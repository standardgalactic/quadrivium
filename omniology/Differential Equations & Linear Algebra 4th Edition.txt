
FOURTH EDITION
Differential Equations
and Linear Algebra
Stephen W. Goode
and
Scott A. Annin
California State University, Fullerton
Boston Columbus Indianapolis New York San Francisco
Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montréal Toronto
Delhi Mexico City São Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo

Editorial Director, Mathematics: Christine Hoag
Editor-in-Chief: Deirdre Lynch
Acquisitions Editor: William Hoffman
Project Team Lead: Christina Lepre
Project Manager: Lauren Morse
Editorial Assistant: Jennifer Snyder
Program Team Lead: Karen Wernholm
Program Manager: Danielle Simbajon
Cover and Illustration Design: Studio Montage
Program Design Lead: Beth Paquin
Product Marketing Manager: Claire Kozar
Product Marketing Coordiator: Brooke Smith
Field Marketing Manager: Evan St. Cyr
Senior Author Support/Technology Specialist: Joe Vetere
Senior Procurement Specialist: Carol Melville
Interior Design, Production Management, Answer Art, and Composition:
iEnergizer Aptara®, Ltd.
Cover Image: Light trails on modern building background in Shanghai, China – hxdyl/123RF
Copyright ©2017, 2011, 2005 Pearson Education, Inc. or its afﬁliates. All Rights Reserved. Printed in the United States of
America. This publication is protected by copyright, and permission should be obtained from the publisher prior to any prohibited
reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying,
recording, or otherwise. For information regarding permissions, request forms and the appropriate contacts within the Pearson
Education Global Rights & Permissions department, please visit www.pearsoned.com/permissions/.
PEARSON and ALWAYS LEARNING are exclusive trademarks in the U.S. and/or other countries owned by Pearson Education, Inc.
or its afﬁliates.
Unless otherwise indicated herein, any third-party trademarks that may appear in this work are the property of their respective owners
and any references to third-party trademarks, logos or other trade dress are for demonstrative or descriptive purposes only. Such
references are not intended to imply any sponsorship, endorsement, authorization, or promotion of Pearson’s products by the owners
of such marks or any relationship between the owner and Pearson Education, Inc. or its afﬁliates, authors, licensees or distributors.
Library of Congress Cataloging-in-Publication Data
Goode, Stephen W.,
Differential equations and linear algebra / Stephen W. Goode and Scott A. Annin,
California State University, Fullerton. — 4th edition.
pages cm
Includes index
ISBN 978-0-321-96467-0 — ISBN 0-321-96467-5
1. Differential equations. 2. Algebras, Linear. I. Annin, Scott. II. Title.
QA371.G644 2015
515’.35—dc23
2014006015
1 2 3 4 5 6 7 8 9 10—V031—19 18 17 16 15
www.pearsonhighered.com
ISBN 10: 0-321-96467-5
ISBN 13: 978-0-321-96467-0

Contents
Preface vii
1
First-Order Differential Equations
1
1.1
Differential Equations Everywhere
1
1.2
Basic Ideas and Terminology
13
1.3
The Geometry of First-Order Differential Equations
23
1.4
Separable Differential Equations
34
1.5
Some Simple Population Models
45
1.6
First-Order Linear Differential Equations
53
1.7
Modeling Problems Using First-Order Linear
Differential Equations
61
1.8
Change of Variables
71
1.9
Exact Differential Equations
82
1.10
Numerical Solution to First-Order Differential
Equations
93
1.11
Some Higher-Order Differential Equations 101
1.12
Chapter Review 106
2
Matrices and Systems of Linear
Equations
114
2.1
Matrices: Definitions and Notation 115
2.2
Matrix Algebra 122
2.3
Terminology for Systems of Linear Equations 138
2.4
Row-Echelon Matrices and Elementary Row
Operations 146
2.5
Gaussian Elimination 156
2.6
The Inverse of a Square Matrix 168
2.7
Elementary Matrices and the LU Factorization 179
2.8
The Invertible Matrix Theorem I 188
2.9
Chapter Review 190
3
Determinants
196
3.1
The Definition of the Determinant 196
3.2
Properties of Determinants 209
3.3
Cofactor Expansions 222
3.4
Summary of Determinants 235
3.5
Chapter Review 242
iii

iv
Contents
4
Vector Spaces
246
4.1
Vectors in Rn 248
4.2
Definition of a Vector Space 252
4.3
Subspaces 263
4.4
Spanning Sets 274
4.5
Linear Dependence and Linear Independence 284
4.6
Bases and Dimension 298
4.7
Change of Basis 311
4.8
Row Space and Column Space 319
4.9
The Rank-Nullity Theorem 325
4.10
Invertible Matrix Theorem II 331
4.11
Chapter Review 332
5
Inner Product Spaces
339
5.1
Definition of an Inner Product Space 340
5.2
Orthogonal Sets of Vectors and Orthogonal
Projections 352
5.3
The Gram-Schmidt Process 362
5.4
Least Squares Approximation 366
5.5
Chapter Review 376
6
Linear Transformations
379
6.1
Definition of a Linear Transformation 380
6.2
Transformations of R2 391
6.3
The Kernel and Range of a Linear Transformation 397
6.4
Additional Properties of Linear Transformations 407
6.5
The Matrix of a Linear Transformation 419
6.6
Chapter Review 428
7
Eigenvalues and Eigenvectors
433
7.1
The Eigenvalue/Eigenvector Problem 434
7.2
General Results for Eigenvalues and Eigenvectors 446
7.3
Diagonalization 454
7.4
An Introduction to the Matrix Exponential Function 462
7.5
Orthogonal Diagonalization and Quadratic Forms 466
7.6
Jordan Canonical Forms 475
7.7
Chapter Review 488
8
Linear Differential Equations of
Order n
493
8.1
General Theory for Linear Differential Equations 495
8.2
Constant Coefficient Homogeneous Linear
Differential Equations 505
8.3
The Method of Undetermined Coefficients:
Annihilators 515
8.4
Complex-Valued Trial Solutions 526
8.5
Oscillations of a Mechanical System 529

Contents v
8.6
RLC Circuits 542
8.7
The Variation of Parameters Method 547
8.8
A Differential Equation with Nonconstant Coefficients 557
8.9
Reduction of Order 568
8.10
Chapter Review 573
9
Systems of Differential Equations
580
9.1
First-Order Linear Systems 582
9.2
Vector Formulation 588
9.3
General Results for First-Order Linear Differential
Systems 593
9.4
Vector Differential Equations: Nondefective
Coefficient Matrix 599
9.5
Vector Differential Equations: Defective Coefficient
Matrix 608
9.6
Variation-of-Parameters for Linear Systems 620
9.7
Some Applications of Linear Systems of Differential
Equations 625
9.8
Matrix Exponential Function and Systems of
Differential Equations 635
9.9
The Phase Plane for Linear Autonomous Systems 643
9.10
Nonlinear Systems 655
9.11
Chapter Review 663
10 The Laplace Transform and Some
Elementary Applications
670
10.1
Definition of the Laplace Transform 670
10.2
The Existence of the Laplace Transform and the
Inverse Transform 676
10.3
Periodic Functions and the Laplace Transform 682
10.4
The Transform of Derivatives and Solution of
Initial-Value Problems 685
10.5
The First Shifting Theorem 690
10.6
The Unit Step Function 695
10.7
The Second Shifting Theorem 699
10.8
Impulsive Driving Terms: The Dirac Delta Function 706
10.9
The Convolution Integral 711
10.10 Chapter Review 717
11 Series Solutions to Linear Differential
Equations
722
11.1
A Review of Power Series 723
11.2
Series Solutions about an Ordinary Point 731
11.3
The Legendre Equation 741
11.4
Series Solutions about a Regular Singular Point 750
11.5
Frobenius Theory 759
11.6
Bessel’s Equation of Order p 773
11.7
Chapter Review 785

vi
Contents
A Review of Complex Numbers
791
B
Review of Partial Fractions
797
C
Review of Integration Techniques
804
D Linearly Independent Solutions to
x2y′′ + xp(x)y′ + q(x)y = 0
811
Answers to Odd-Numbered
Exercises
814
Index 849
S. W. Goode dedicates this book to Megan and Tobi
S. A. Annin dedicates this book to Arthur and Juliann, the best
parents anyone could ask for

Preface
Like the ﬁrst three editions of Differential Equations and Linear Algebra, this fourth
edition is intended for a sophomore level course that covers material in both differential
equations and linear algebra. In writing this text we have endeavored to develop the stu-
dent’s appreciation for the power of the general vector space framework in formulating
and solving linear problems. The material is accessible to science and engineering stu-
dents who have completed three semesters of calculus and who bring the maturity of that
success with them to this course. This text is written as we would naturally teach, blend-
ing an abundance of examples and illustrations, but not at the expense of a deliberate
and rigorous treatment. Most results are proven in detail. However, many of these can be
skipped in favor of a more problem-solving oriented approach depending on the reader’s
objectives. Some readers may like to incorporate some form of technology (computer
algebra system (CAS) or graphing calculator) and there are several instances in the text
where the power of technology is illustrated using the CAS Maple. Furthermore, many
exercise sets have problems that require some form of technology for their solution.
These problems are designated with a ⋄.
In developing the fourth edition we have once more kept maximum ﬂexibility of
the material in mind. In so doing, the text can effectively accommodate the different
emphases that can be placed in a combined differential equations and linear algebra
course, the varying backgrounds of students who enroll in this type of course, and the
fact that different institutions have different credit values for such a course. The whole
text can be covered in a ﬁve credit-hour course. For courses with a lower credit-hour
value, some selectivity will have to be exercised. For example, much (or all) of Chapter
1 may be omitted since most students will have seen many of these differential equations
topics in an earlier calculus course, and the remainder of the text does not depend on
the techniques introduced in this chapter. Alternatively, while one of the major goals
of the text is to interweave the material on differential equations with the tools from
linear algebra in a symbiotic relationship as much as possible, the core material on linear
algebra is given in Chapters 2–7 so that it is possible to use this book for a course that
focuses solely on the linear algebra presented in these six chapters. The material on
differential equations is contained primarily in Chapters 1 and 8–11, and readers who
have already taken a ﬁrst course in linear algebra can choose to proceed directly to these
chapters.
There are other means of eliminating sections to reduce the amount of material to
be covered in a course. Section 2.7 contains material that is not required elsewhere in
the text, Chapter 3 can be condensed to a single section (Section 3.4) for readers needing
only a cursory overview of determinants, and Sections 4.7, 5.4, and the later sections of
Chapters 6 and 7 could all be reserved for a second course in linear algebra. In Chapter 8,
Sections 8.4, 8.8, and 8.9 can be omitted, and, depending on the goals of the course, Sec-
tions 8.5 and 8.6 could either be de-emphasized or omitted completely. Similar remarks
apply to Sections 9.7–9.10. At California State University, Fullerton we have a four
credit-hour course for sophomores that is based around the material in Chapters 1–9.
vii

viii
Preface
Major Changes in the Fourth Edition
Several sections of the text have been modiﬁed to improve the clarity of the presentation
and to provide new examples that reﬂect insightful illustrations we have used in our own
courses at California State University, Fullerton. Other signiﬁcant changes within the
text are listed below.
1. The chapter on vector spaces in the previous edition has been split into two chapters
(Chapters 4 and 5) in the present edition, in order to focus separate attention on
vector spaces and inner product spaces. The shorter length of these two chapters
is also intended to make each of them less daunting.
2. The chapter on inner product spaces (Chapter 5) includes a new section providing
an application of linear algebra to the subject of least squares approximation.
3. The chapter on linear transformations in the previous edition has been split into
two chapters (Chapters 6 and 7) in the present edition. Chapter 6 is focused on
linear transformations, while Chapter 7 places direct emphasis on the theory of
eigenvalues and eigenvectors. Once more, readers should ﬁnd the shorter chapters
covering these topics more approachable and focused.
4. Most exercise sets have been enlarged or rearranged. Over 3,000 problems are now
contained within the text, and more than 600 concept-oriented true/false items are
also included in the text.
5. Every chapter of the book includes one or more optional projects that allow for
more in-depth study and application of the topics found in the text.
6. The back of the book now includes the answer to every True-False Review item
contained in the text.
Acknowledgments
We would like to acknowledge the thoughtful input from the following reviewers of
the fourth edition: Jamey Bass of City College of San Francisco, Tamar Friedmann of
University of Rochester, and Linghai Zhang of Lehigh University.
All of their comments were considered carefully in the preparation of the text.
S.A. Annin: I once more thank my parents, Arthur and Juliann Annin, for their love
and encouragement in all of my professional endeavors. I also gratefully acknowledge
the many students who have taken this course with me over the years and, in so doing,
have enhanced my love for these topics and deeply enriched my career as a professor.

1
First-Order Differential
Equations
1.1
Differential Equations Everywhere
A differential equation is any equation that involves one or more derivatives of an
unknown function. For example,
d2y
dx2 + x2 dy
dx + y2 = 5 sin x
(1.1.1)
and
dS
dt = e3t(S −1)
(1.1.2)
are differential equations. In the differential equation (1.1.1) the unknown function or
dependent variable is y, and x is the independent variable; in the differential equation
(1.1.2) the dependent and independent variables are S and t, respectively. Differential
equations such as (1.1.1) and (1.1.2) in which the unknown function depends only on
a single independent variable are called ordinary differential equations. By contrast,
the differential equation (Laplace’s equation)
∂2u
∂x2 + ∂2u
∂y2 = 0
involvespartial derivativesoftheunknownfunctionu(x, y)oftwoindependentvariables
x and y. Such differential equations are called partial differential equations.
One way in which differential equations can be characterized is by the order of the
highest derivative that occurs in the differential equation. This number is called the order
of the differential equation. Thus, (1.1.1) has order two, whereas (1.1.2) is a ﬁrst-order
differential equation.
1

2
CHAPTER 1
First-Order Differential Equations
The major reason why it is important to study differential equations is that these types
of equations pervade all areas of science, technology, engineering, and mathematics. In
this section we will illustrate some of the multitude of applications that are described
mathematically by differential equations and then, in the remainder of the chapter, in-
troduce several techniques that can be used to study the properties and solutions of
differential equations.
Population Models
The Malthusian model for the growth of a population of bacteria assumes that the rate
at which the culture grows is proportional to the number of bacteria present at that time.
If P(t) denotes the number of bacteria in the culture at time t, then this growth model is
described mathematically by the ﬁrst-order differential equation
d P
dt = kP,
(1.1.3)
where k is a constant. Since the culture grows in time, k is positive. Here the unknown
function is P(t). In elementary calculus it is shown that all functions that satisfy (1.1.3)
are of the form1
P(t) = Cekt,
(1.1.4)
where C is an arbitrary constant. The formula for P(t) given in (1.1.4) is called the
general solution to the differential equation (1.1.3), since every solution to (1.1.3) can
be obtained from (1.1.4) by appropriate choice of C. To determine a particular solution
to the differential equation, we must be given some extra information that speciﬁes the
appropriate value of C corresponding to the solution we require. For example, if P0
denotes the number of bacteria present at time t = 0, then in addition to the differential
equation (1.1.3) we also have the initial condition
P(0) = P0.
(1.1.5)
But, according to (1.1.4),
P(0) = Cek·0 = C.
Therefore, in order to satisfy the initial condition (1.1.5), we must choose C = P0, in
which case the particular solution that is relevant for our problem is
P(t) = P0ekt.
Since k is a positive constant, we see that this model predicts that the bacteria population
grows exponentially in time. This is consistent with observations of bacteria populations
but does not give an accurate description of the growth of populations in other species
(people, insects, ﬁsh, aardvarks, …). More general population models arise under the
assumption that the rate of growth of the population at time t is a more general function
of P than simply kP. For instance, the logistic population model corresponds to the
case when we assume that there is a constant birthrate B0 per individual, and that the
death rate per individual is proportional to the instantaneous population. The resulting
ﬁrst-order differential equation is
d P
dt = (B0 −D0P)P,
1Alternatively, this can be derived by writing (1.1.3) as P−1 d P
dt = k or, equivalently, d(ln P)
dt
= k, which
can be integrated directly to yield ln P = kt + c, so that P(t) = ekt · ec = Cekt, where C = ec.

1.1
Differential Equations Everywhere 3
where B0 and D0 are positive constants (the Malthusian model considered previously
corresponds to B0 = k, D0 = 0). This differential equation is often written in the
equivalent form
d P
dt = k
!
1 −P
C
"
P,
(1.1.6)
wherek = B0 andC = B0/D0.InSection1.5wewillstudythelogisticpopulationmodel
in detail, and show that, in contrast to the Malthusian model, it predicts that the population
does not increase without bound, but rather approaches a limiting population given by the
constant C in Equation (1.1.6). This limiting population is called the carrying capacity
of the population and represents the maximum population that is sustainable with the
given resources. The graph of a typical solution to the differential equation (1.1.6) is
given in Figure 1.1.1.
t
P
Carrying capacity
Figure 1.1.1: Behavior of a typical solution to the logistic differential equation (1.1.6).
Newton’s Law of Cooling
We now build a mathematical model describing the cooling (or heating) of an object.
Suppose that we bring an object into a room. If the temperature of the object is hotter
than that of the room, then the object will begin to cool. Further, we might expect that the
major factor governing the rate at which the object cools is the temperature difference
between it and the room.
Newton’s Law of Cooling: The rate of change of temperature of an object is propor-
tional to the difference between the temperature of the object and the temperature of the
surrounding medium.
To formulate this law mathematically, we let T (t) denote the temperature of the ob-
ject at time t, and let Tm(t) denote the temperature of the surrounding medium. Newton’s
law of cooling can then be expressed as the ﬁrst-order differential equation
dT
dt = −k(T −Tm),
(1.1.7)
where k is a constant. The minus sign in front of the constant k is traditional. It ensures
that k will always be positive.2 Once we have studied Section 1.4 it will be easy to show
that, when Tm is constant, the solution to this differential equation is
T (t) = Tm + ce−kt,
(1.1.8)
2If T > Tm, then the object will cool, so that dT/dt < 0. Hence, from Equation (1.1.7), k must be positive.
Similarly, if T < Tm, then dT/dt > 0, and once more Equation (1.1.7) implies that k must be positive.

4
CHAPTER 1
First-Order Differential Equations
where c is a constant (see also Problem 8). Newton’s law of cooling therefore predicts
that as t approaches inﬁnity (t →∞) the temperature of the object approaches that
of the surrounding medium (T →Tm). This is certainly consistent with our everyday
experience (see Figure 1.1.2).
T0
T0
T(t)
T(t)
t
t
Object that is heating
Object that is cooling
Tm
Tm
Figure 1.1.2: According to Newton’s law of cooling, the temperature of an object approaches
room temperature exponentially. In these ﬁgures T0( = T (0)) represents the initial temperature
of the object.
The Orthogonal Trajectory Problem
Next we consider a geometric problem that has many interesting and important applica-
tions. Suppose
F(x, y, c) = 0
(1.1.9)
deﬁnes a family of curves in the xy-plane, where the constant c labels the different
curves. For instance if c is a real constant, the equation
x2 + y2 −c2 = 0
describes a family of concentric circles with center at the origin, whereas
−x2 + y −c = 0
describes a family of parabolas that are vertical shifts of the standard parabola y = x2.
We assume that every curve in the family F(x, y, c) = 0 has a well-deﬁned tangent
line at each point. Associated with this family is a second family of curves, say,
G(x, y, k) = 0
(1.1.10)
with the property that whenever a curve from the family (1.1.9) intersects a curve from
the family (1.1.10) it does so at right angles.3 We say that the curves in the family
(1.1.10) are orthogonal trajectories of the family (1.1.9), and vice versa. For example,
from elementary geometry, it follows that the lines y = kx in the family G(x, y, k) =
y −kx = 0 are orthogonal trajectories of the family of concentric circles x2 + y2 = c2.
(See Figure 1.1.3.)
y
x
Figure 1.1.3: The family of
curves x2 + y2 = c2 and the
orthogonal trajectories y = kx.
Orthogonal trajectories arise in various applications. For example, a family of curves
and its orthogonal trajectories can be used to deﬁne an orthogonal coordinate system in
the xy-plane. In Figure 1.1.3 the families x2 + y2 = c2 and y = kx are the coordinate
curves of a polar coordinate system (that is, the curves r = constant and θ = constant,
3That is, the tangent lines to each curve are perpendicular at any point of intersection.

1.1
Differential Equations Everywhere 5
respectively). In physics, the lines of electric force of a static conﬁguration are the
orthogonal trajectories of the family of equipotential curves. As a ﬁnal example, if we
consider a two-dimensional heated plate, then the heat energy ﬂows along the orthogonal
trajectories to the constant temperature curves (isotherms).
Statement of the Problem: Given the equation of a family of curves, ﬁnd the equation of
the family of orthogonal trajectories.
Mathematical Formulation: We recall that curves that intersect at right angles satisfy the
following:
The product of the slopes4 at the point of intersection is −1.
Thus if the given family F(x, y, c) = 0 has slope m1 = f (x, y) at the point (x, y), then
the slope of the family of orthogonal trajectories G(x, y, k) = 0 at the point (x, y) is
m2 = −1/f (x, y), and therefore the orthogonal trajectories are obtained by solving the
ﬁrst-order differential equation
dy
dx = −
1
f (x, y).
Example 1.1.1
Determinetheequationofthefamilyoforthogonaltrajectoriestothecurveswithequation
y2 = cx.
(1.1.11)
Solution:
According to the preceding discussion, the differential equation determin-
ing the orthogonal trajectories is
dy
dx = −
1
f (x, y),
where f (x, y) denotes the slope of the given family at the point (x, y). To determine
f (x, y), we differentiate Equation (1.1.11) implicitly with respect to x to obtain
2y dy
dx = c.
(1.1.12)
We must now eliminate c from the previous equation to obtain an expression that gives
the slope at the point (x, y). From Equation (1.1.11) we have
c = y2
x ,
which, when substituted into Equation (1.1.12), yields
dy
dx = y
2x .
Consequently, the slope of the given family at the point (x, y) is
f (x, y) = y
2x
so that the orthogonal trajectories are obtained by solving the differential equation
dy
dx = −2x
y .
4By the slope of a curve at a given point, we mean the slope of the tangent line to the curve at that point.

6
CHAPTER 1
First-Order Differential Equations
A key point to notice is that we cannot solve this differential equation by simply inte-
grating with respect to x, since the function on the right-hand side of the differential
equation depends on both x and y. However, multiplying by y we see that
y dy
dx = −2x
or equivalently,
d
dx
!1
2 y2
"
= −2x.
Since the right-hand side of this equation depends only on x whereas the term on the
left-hand side is a derivative with respect to x, we can integrate both sides of the equation
with respect to x to obtain
1
2 y2 = −x2 + c1,
which we write as
2x2 + y2 = k
(1.1.13)
x
y
2x2 1 y2 5 k
y2 5 cx
Figure 1.1.4: The family of curves y2 = cx and its orthogonal trajectories 2x2 + y2 = k.
where k = 2c1. We see that the curves in the given family (1.1.11) are parabolas, and the
orthogonal trajectories (1.1.13) are a family of ellipses. This is illustrated in Figure 1.1.4.
□
Newton’s Second Law of Motion
Newton’s second law of motion states that, for an object of constant mass m, the sum
of the applied forces that are acting on the object is equal to the mass of the object
multiplied by the acceleration of the object. If the object is moving in one dimension
under the inﬂuence of a force F, then the mathematical statement of this law is the
ﬁrst-order differential equation
m dv
dt = F,
(1.1.14)
where v(t) denotes the velocity of the object at time t. We let y(t) denote the displacement
of the object at time t. Then, using the fact that velocity and displacement are related via
v = dy
dt

1.1
Differential Equations Everywhere 7
it follows that (1.1.14) can be written as the second-order differential equation
m d2y
dt2 = F.
(1.1.15)
mg
Positive y-direction
Figure 1.1.5: Object falling
under the inﬂuence of gravity.
Vertical Motion under Gravity: As a speciﬁc example, consider the case of an object
falling freely under the inﬂuence of gravity (see Figure 1.1.5). In this case the only force
acting on the object is F = mg, where g denotes the (constant) acceleration due to
gravity. It follows from Equation (1.1.15) that the motion of the object is governed by
the differential equation5
m d2y
dt2 = mg,
(1.1.16)
or equivalently,
d2y
dt2 = g.
Since g is a (positive) constant, we can integrate this equation to determine y(t). Per-
forming one integration yields
dy
dt = gt + c1,
where c1 is an arbitrary integration constant. Integrating once more with respect to t
we obtain
y(t) = 1
2gt2 + c1t + c2,
(1.1.17)
where c2 is a second integration constant. We see that the differential equation has an
inﬁnite number of solutions parameterized by the constants c1 and c2. In order to uniquely
specify the motion, we must augment the differential equation with initial conditions that
specify the initial position and initial velocity of the object. For example, if the object
is released at t = 0 from y = y0 with a velocity v0, then, in addition to the differential
equation, we have the initial conditions
y(0) = y0,
dy
dt (0) = v0.
(1.1.18)
These conditions must be imposed on the solution (1.1.17) in order to determine the
values of c1 and c2 that correspond to the particular problem under investigation. Setting
t = 0 in (1.1.17) and using the ﬁrst initial condition from (1.1.18) we ﬁnd that
y0 = c2.
Substituting this into Equation (1.1.17), we get
y(t) = 1
2gt2 + c1t + y0.
(1.1.19)
In order to impose the second initial condition from (1.1.18), we ﬁrst differentiate Equa-
tion (1.1.19) to obtain
dy
dt = gt + c1.
Consequently the second initial condition in (1.1.18) requires
c1 = v0.
5Note that we are choosing the positive direction as downward, hence the + sign in front of mg.

8
CHAPTER 1
First-Order Differential Equations
From (1.1.19), it follows that the position of the object at time t is
y(t) = 1
2gt2 + v0t + y0.
The differential equation (1.1.16) together with the initial conditions (1.1.18) is an ex-
ample of an initial-value problem.
A more realistic model of vertical motion under gravity would have to take account
of the force due to air resistance. Since increasing velocity generally has the effect of
increasing the resistive force, it is reasonable to assume that the force due to air resistance
is a function of the instantaneous velocity of the object. A particular model that is often
used is to assume that the resistive force is directly proportional to a positive power n
(not necessarily integer) of the velocity. Therefore, the total force acting on the object is
F = mg −kvn,
where k is a positive constant, so that (1.1.14) can be written as
m dv
dt = mg −kvn.
(1.1.20)
In Section 1.3 we will develop qualitative techniques for analyzing ﬁrst-order differential
equations that can be used to show that all solutions to Equation (1.1.20) approach a so-
called terminal velocity, vT deﬁned by
vT = lim
t→∞v(t) =
#mg
k
$ 1
n .
This is a very reassuring result for parachutists!
Spring Force: As a second application of Newton’s law of motion, consider the spring-
mass system depicted in Figure 1.1.6, where, for simplicity, we are neglecting frictional
and external forces. In this case, the only force acting on the mass is the restoring force (or
spring force), Fs, due to the displacement of the spring from its equilibrium (unstretched)
position. We use Hooke’s law to model this force:
Positive y-direction
y 5 0
y(t)
Mass in its
equilibrium position
Figure 1.1.6: A simple harmonic oscillator.
Hooke’s Law: The restoring force of a spring is directly proportional to the displacement
of the spring from its equilibrium position and is directed toward the equilibrium position.
If y(t) denotes the displacement of the spring from its equilibrium position at time
t (see Figure 1.1.6), then according to Hooke’s law, the restoring force is
Fs = −ky,

1.1
Differential Equations Everywhere 9
where k is a positive constant called the spring constant. Consequently, Newton’s second
law of motion implies that the motion of the spring-mass system is governed by the
differential equation
m d2y
dt2 = −ky
which we write in the equivalent form
d2y
dt2 + ω2y = 0,
(1.1.21)
where ω = √k/m. At present we cannot solve this differential equation. However, we
leave it as an exercise (Problem 30) to verify by direct substitution that
y(t) = A cos(ωt −φ)
is a solution to the differential equation (1.1.21), where A and φ are constants (determined
from the initial conditions for the problem). We see that the resulting motion is periodic
with amplitude A. This is consistent with what we might expect physically, since no
frictional forces or external forces are acting on the system. This type of motion is
referred to as simple harmonic motion, and the physical system is called a simple
harmonic oscillator.
Ontogenetic Growth
Ontogeny is the study of the growth of an individual organism (human, orangutan, snake,
ﬁsh, …) from embryo to maximum body size. A general growth equation based purely on
fundamental metabolic principles (as opposed to assumptions about birth rates and death
rates) has been developed by West, Brown, and Enquist6 that is applicable to all multicel-
lular animals. The model is derived from the following conservation of energy equation:
B(t) = NcBc + Ec
dNc
dt ,
(1.1.22)
where B(t) is the resting metabolic rate of the whole organism at time t, Bc is the
metabolic rate of a single cell, Ec is the metabolic energy required to create a cell and
Nc is the total number of cells. If m and mc denote the total body mass and average cell
mass respectively, then m = mcNc so that Nc = m/mc. Substituting this expression for
Nc into Equation (1.1.22) yields
B =
!m
mc
"
Bc +
! Ec
mc
" dm
dt
or, equivalently,
dm
dt =
!mc
Ec
"
B −
!m
Ec
"
Bc.
(1.1.23)
Assuming the allometric relationship7
B = B0m3/4,
where B0 is a constant, yields
dm
dt =
!mc
Ec
"
B0m3/4 −
!m
Ec
"
Bc,
(1.1.24)
6West, G.B., Brown, J.H., and Enquist, B.J. (2001), Nature 400, 467.
7Perhaps surprisingly, this simple relationship between resting metabolic rate and total body mass accurately
ﬁts the data across species.

10
CHAPTER 1
First-Order Differential Equations
which we write in the equivalent form:
dm
dt = am3/4
%
1 −
# m
M
$1/4&
,
(1.1.25)
where a = B0mc/Ec and M = (B0mc/Bc)4. Once we have studied Section 1.4 it will
be straightforward to derive the following solution to the differential equation (1.1.25):
m(t) = M
'
1 −
%
1 −
#m0
M
$1/4&
e−at/(4M1/4)
(4
.
(1.1.26)
This solution gives the mass of the animal t days after its birth. We note that
lim
t→∞m(t) = M,
which indicates that the model predicts that organisms do not grow indeﬁnitely but reach
a maximum body size, which is represented by the constant M.
Exercises for 1.1
Key Terms
Differential equation, Order of a differential equation,
Malthusian population model, Logistic population model,
Initial conditions, Newton’s law of cooling, Orthogonal tra-
jectories, Newton’s second law of motion, Hooke’s law,
Spring constant, Simple harmonic motion, Simple harmonic
oscillator, Ontogenetic growth model.
Skills
• Be able to determine the order of a differential
equation.
• Given a differential equation, be able to check whether
or not a given function y = f (x) is indeed a solution
to the differential equation.
• Be able to describe qualitatively how the temperature
of an object changes as a function of time according
to Newton’s law of cooling.
• Be able to ﬁnd the equation of the orthogonal trajec-
tories to a given family of curves. In simple geometric
cases, be prepared to provide rough sketches of some
representative orthogonal trajectories.
• Be able to ﬁnd the distance, velocity, and accelera-
tion functions for an object moving freely under the
inﬂuence of gravity.
• Be able to determine the motion of an object in a
spring-mass system with no frictional or external
forces.
True-False Review
For items (a)–(n), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A differential equation for a function y = f (x) must
contain the ﬁrst derivative y′ = f ′(x).
(b) The order of a differential equation is the order of
the lowest derivative appearing in the differential
equation.
(c) The differential equation y′′ + ex(y′)3 + y = sin x
has order 3.
(d) In the logistic population model the initial population
is called the carrying capacity.
(e) The numerical value y(0) accompanying a ﬁrst-order
differential equation for a function y = f (x) is called
an initial condition for the differential equation.
(f) If room temperature is 70◦F, then an object whose
temperature is 100◦F at a particular time cools faster
at that time than an object whose temperature at that
time is 90◦F.
(g) According to Newton’s law of cooling, the tempera-
ture of an object eventually becomes the same as the
temperature of the surrounding medium.

1.1
Differential Equations Everywhere 11
(h) A hot cup of coffee that is put into a cold room cools
more in the ﬁrst hour than the second hour.
(i) At a point of intersection of a curve and one of its or-
thogonal trajectories, the slopes of the two curves are
reciprocals of one another.
(j) The family of orthogonal trajectories for a family of
parallel lines is another family of parallel lines.
(k) The family of orthogonal trajectories for a family of
circles that are centered at the origin is another family
of circles centered at the origin.
(l) The relationship between the velocity and the acceler-
ation of an object falling under the inﬂuence of grav-
ity can be expressed mathematically as a differential
equation.
(m) Hooke’s law states that the restoring force of a spring is
directly proportional to the displacement of the spring
from its equilibrium position and is directed in the
direction of the displacement from the equilibrium
position.
(n) According to the ontogenetic growth model the rest-
ing metabolic rate of an aardvark is proportional to its
mass to the power of three-quarters.
Problems
For Problems 1–4 determine the order of the differential
equation.
1. d2y
dx2 + x dy
dx + y = ex.
2.
!dy
dx
"3
+ y2 = sin x.
3. y′′ + xy′ + ex y = y′′′.
4. sin(y′′) + x2y′ + xy = ln x.
5. Verify that, for t > 0, y(t) = ln t is a solution to the
differential equation
2
!dy
dt
"3
= d 3y
dt3 .
6. Verify that y(x) = x/(x + 1) is a solution to the dif-
ferential equation
y + d2y
dx2 = dy
dx + x3 + 2x2 −3
(1 + x)3
.
7. Verify that y(x) = ex sin x is a solution to the differ-
ential equation
2y cot x −d2y
dx2 = 0.
8. By writing Equation (1.1.7) in the form
1
T −Tm
dT
dt = −k
and using u−1 du
dt = d
dt (ln u), derive (1.1.8).
9. A glass of water whose temperature is 50◦F is taken
outside at noon on a day whose temperature is con-
stant at 70◦F. If the water’s temperature is 55◦F at
2 p.m., do you expect the water’s temperature to reach
60◦F before 4 p.m. or after 4 p.m.? Use Newton’s law
of cooling to explain your answer.
10. On a cold winter day (10◦F), an object is brought out-
side from a 70◦F room. If it takes 40 minutes for the
object to cool from 70◦F to 30◦F, did it take more or
less than 20 minutes for the object to reach 50◦F ? Use
Newton’s law of cooling to explain your answer.
For Problems 11–16, ﬁnd the equation of the orthogonal tra-
jectories to the given family of curves. In each case, sketch
some curves from each family.
11. x2 + 9y2 = c.
12. y = cx2.
13. y = c/x.
14. y = cx5.
15. y = cex.
16. y2 = 2x + c.
For Problems 17–20, m denotes a ﬁxed nonzero constant,
and c is the constant distinguishing the different curves
in the given family. In each case, ﬁnd the equation of the
orthogonal trajectories.
17. y = cxm.
18. y = mx + c.
19. y2 = mx + c.
20. y2 + mx2 = c.

12
CHAPTER 1
First-Order Differential Equations
21. Consider the family of circles x2 + y2 = 2cx. Show
that the differential equation for determining the fam-
ily of orthogonal trajectories is
dy
dx =
2xy
x2 −y2 .
22. We call a coordinate system (u, v) orthogonal if its
coordinate curves (the two families of curves u = con-
stant and v = constant) are orthogonal trajectories (for
example, a Cartesian coordinate system or a polar co-
ordinatesystem).Let(u, v)beorthogonalcoordinates,
where u = x2+2y2, and x and y are Cartesian coordi-
nates. Find the Cartesian equation of the v-coordinate
curves, and sketch the (u, v) coordinate system.
23. Any curve with the property that whenever it inter-
sects a curve of a given family it does so at an angle
a ̸= π/2 is called an oblique trajectory of the given
family. (See Figure 1.1.7.) Let m1 (equal to tan a1)
denote the slope of the required family at the point
(x, y), and let m2 (equal to tan a2) denote the slope of
the given family. Show that
m1 = m2 −tan a
1 + m2 tan a .
[Hint: From Figure 1.1.7, tan a1 = tan(a2−a)]. Thus,
the equation of the family of oblique trajectories is
obtained by solving
dy
dx = m2 −tan a
1 + m2 tan a .
Curve of given family
m1 5 tan a1 5 slope of required family
m2 5 tan a2 5 slope of given family
a1
a2
a
Curve of required
family
Figure 1.1.7: Oblique trajectories intersect at an angle a.
24. An object is released from rest at a height of 100 me-
ters above the ground. Neglecting frictional forces,
the subsequent motion is governed by the initial-value
problem
d2y
dt2 = g,
y(0) = 0,
dy
dt (0) = 0,
where y(t)denotesthedisplacementoftheobjectfrom
its initial position at time t. Solve this initial-value
problem and use your solution to determine the time
when the object hits the ground.
25. A ﬁve-foot-tall boy tosses a tennis ball straight up from
the level of the top of his head. Neglecting frictional
forces, the subsequent motion is governed by the dif-
ferential equation
d2y
dt2 = g.
If the object hits the ground 8 seconds after the boy
releases it, ﬁnd
(a) the time when the tennis ball reaches its maxi-
mum height.
(b) the maximum height of the tennis ball.
26. A pyrotechnic rocket is to be launched vertically up-
wards from the ground. For optimal viewing, the
rocket should reach a maximum height of 90 meters
above the ground. Ignore frictional forces.
(a) How fast must the rocket be launched in order to
achieve optimal viewing?
(b) Assuming the rocket is launched with the speed
determined in part (a), how long after the rocket
is launched will it reach its maximum height?
27. Repeat Problem 26 under the assumption that the
rocket is launched from a platform ﬁve meters above
the ground.
28. An object that is initially thrown vertically upward
with a speed of 2 meters/second from a height of h
meters takes 10 seconds to reach the ground. Set up
and solve the initial-value problem that governs the
motion of the object, and determine h.
29. An object that is released from a height h meters above
the ground with a vertical velocity of v0 meters/second
hits the ground after t0 seconds. Neglecting frictional
forces, set up and solve the initial-value problem gov-
erning the motion, and use your solution to show that
v0 = 1
2t0
(2h −gt2
0).
30. Verify that y(t) = A cos(ωt −φ) is a solution to
the differential equation (1.1.21), where A and ω are
nonzero constants. Determine the constants A and φ
(with |φ| < π radians) in the particular case when the

1.2
Basic Ideas and Terminology 13
initial conditions are
y(0) = a,
dy
dt (0) = 0.
31. Verify that
y(t) = c1 cos ωt + c2 sin ωt
is a solution to the differential equation (1.1.21). Show
that the amplitude of the motion is
A =
)
c2
1 + c2
2.
32. A heron has a birth mass of 3 g, and when fully
grown its mass is 2700 g. Using equation (1.1.26)
with a = 1.5 determine the mass of the heron after
30 days.
33. A rat has a birth mass of 8 g, and when fully grown its
mass is 280 g. Using equation (1.1.26) with a = 0.25
determine how many days it will take for the rat to
reach 75% of its fully grown size.
1.2
Basic Ideas and Terminology
In the previous section we gave several examples of problems that are described math-
ematically by differential equations. We now formalize many of the ideas introduced
through those examples.
Any differential equation of order n can be written in the form
G(x, y, y′, y′′, . . . , y(n)) = 0,
(1.2.1)
where we have introduced the prime notation to denote derivatives, and y(n) denotes the
nth derivative of y with respect to x (not y to the power of n). Of particular interest to us
throughout the text will be linear differential equations. These arise as the special case of
Equation (1.2.1) when y, y′, . . . , y(n) occur to the ﬁrst degree only, and not as products
or arguments of other functions. The general form for such a differential equation is
given in the next deﬁnition.
DEFINITION
1.2.1
A differential equation that can be written in the form
a0(x)y(n) + a1(x)y(n−1) + · · · + an(x)y = F(x),
where a0, a1, . . . , an and F are functions of x only, is called a linear differential
equation of order n. Such a differential equation is linear in y, y′, y′′, . . . , y(n).
A differential equation that does not satisfy this deﬁnition is called a nonlinear
differential equation.
Example 1.2.2
The equations
y′′′ + e3x y′′ + x3y′ + (cos x)y = ln x
and
xy′ −
2
1 + x2 y = 0
are linear differential equations of order 3 and order 1, respectively, whereas
y′′ + x4 cos(y′) −xy = ex2
and
y′′ + y2 = 0
are both second-order nonlinear differential equations. In the ﬁrst case the nonlinearity
arises from the cos(y′) term, whereas in the second differential equation the nonlinearity
is due to the y2 term.
□

14
CHAPTER 1
First-Order Differential Equations
Example 1.2.3
The general forms for ﬁrst- and second-order linear differential equations are
a0(x)dy
dx + a1(x)y = F(x)
and
a0(x)d2y
dx2 + a1(x)dy
dx + a2(x)y = F(x)
respectively.
□
The differential equation (1.1.3) arising in the Malthusian population model can be
written in the form
d P
dt −kP = 0
and so is a ﬁrst-order linear differential equation. Similarly, writing the Newton’s law
of cooling differential equation (1.1.7) as
dT
dt + kT = kTm
reveals that it also is a ﬁrst-order linear differential equation. In contrast, the logistic
differential equation (1.1.6), when written as
d P
dt −kP +
! k
C
"
P2 = 0,
is seen to be a ﬁrst-order nonlinear differential equation. The differential equation
(1.1.21) governing the simple harmonic oscillator, namely,
d2y
dt2 + ω2y = 0,
is a second-order linear differential equation. In this case the linearity was imposed in the
modeling process when we assumed that the restoring force was directly proportional to
thedisplacementfromequilibrium(Hooke’slaw).Notallspringssatisfythisrelationship.
For example, Dufﬁng’s Equation
m d2y
dt2 + k1y + k2y3 = 0
gives a mathematical model of a nonlinear spring-mass system. If k2 = 0, this reduces
to the simple harmonic oscillator equation.
Solutions to Differential Equations
We now deﬁne precisely what is meant by a solution to a differential equation.
DEFINITION
1.2.4
A function y = f (x) that is (at least) n times differentiable on an interval I is called
a solution to the differential equation (1.2.1) on I if the substitution y = f (x), y′ =
f ′(x), . . . , y(n) = f (n)(x) reduces the differential equation (1.2.1) to an identity valid
for all x in I. In this case we say that y = f (x) satisﬁes the differential equation.
Example 1.2.5
Verify that for all constants c1 and c2, y(x) = c1e2x + c2e−2x is a solution to the linear
differential equation y′′ −4y = 0 for x in the interval (−∞, ∞).

1.2
Basic Ideas and Terminology 15
Solution:
The function y(x) is certainly twice differentiable for all real x.
Furthermore,
y′(x) = 2c1e2x −2c2e−2x
and
y′′(x) = 4c1e2x + 4c2e−2x = 4
#
c1e2x + c2e−2x$
.
Consequently,
y′′ −4y = 4
#
c1e2x + c2e−2x$
−4
#
c1e2x + c2e−2x$
= 0
so that y′′ −4y = 0 for every x in (−∞, ∞). It follows from Deﬁnition 1.2.4 that the
given function is a solution to the differential equation on (−∞, ∞).
□
In the preceding example, x could assume all real values. Often, however, the inde-
pendent variable will be restricted in some manner. For example, the differential equation
dy
dx =
1
2√x (y −1)
is undeﬁned when x ≤0 and so any solution would be deﬁned only for x > 0. In fact
this linear differential equation has solution
y(x) = ce
√x + 1,
x > 0,
where c is a constant. (The reader can check this by plugging in to the given differential
equation, as was done in Example 1.2.5. In Section 1.4 we will introduce a technique that
will enable us to derive this solution.) We now distinguish two different ways in which
solutions to a differential equation can be expressed. Often, as in Example 1.2.5, we will
be able to obtain a solution to a differential equation in the explicit form y = f (x),
for some function f . However, when dealing with nonlinear differential equations, we
usually have to be content with a solution written in implicit form
F(x, y) = 0,
where the function F deﬁnes the solution, y(x), implicitly as a function of x. This is
illustrated in Example 1.2.6.
Example 1.2.6
Verify that the relation x2 + y2 −4 = 0 deﬁnes an implicit solution to the nonlinear
differential equation
dy
dx = −x
y .
Solution:
We regard the given relation as deﬁning y as a function of x. Differentiating
this relation with respect to x yields8
2x + 2y dy
dx = 0.
That is,
dy
dx = −x
y
8Note that we have used implicit differentiation in obtaining d(y2)/dx = 2y · (dy/dx).

16
CHAPTER 1
First-Order Differential Equations
as required. In this example we can obtain y explicitly in terms of x since x2+y2−4 = 0
implies that
y = ±
*
4 −x2.
The implicit relation therefore contains the two explicit solutions
y(x) =
*
4 −x2,
y(x) = −
*
4 −x2,
which correspond graphically to the two semi-circles sketched in Figure 1.2.1.
Both solutions are undefined
when x 5 62
y
x
y(x) 5 (4 2 x2)1/2
y(x) 5 2(4 2 x2)1/2
Figure 1.2.1: Two solutions to the differential equation y′ = −x/y.
Since x = ±2 correspond to y = 0 in both of these equations, whereas the differential
equation is only deﬁned for y ̸= 0, we must omit x = ±2 from the domains of the
solutions. Consequently, both of the foregoing solutions to the differential equation are
valid for −2 < x < 2.
□
In the previous example the solutions to the differential equation are more simply
expressed in implicit form although, as we have shown, it is quite easy to obtain the
corresponding explicit solutions. In the following example the solution must be expressed
in implicit form, since it is impossible to solve the implicit relation (analytically) for y
as a function of x.
Example 1.2.7
Verify that the relation sin(xy) + y2 −x = 0 deﬁnes a solution to
dy
dx = 1 −y cos(xy)
x cos(xy) + 2y .
Solution:
Differentiating the given relationship implicitly with respect to x yields
cos(xy)
!
y + x dy
dx
"
+ 2y dy
dx −1 = 0.
That is,
dy
dx [x cos(xy) + 2y] = 1 −y cos(xy),
which implies that
dy
dx = 1 −y cos(xy)
x cos(xy) + 2y
as required.
□

1.2
Basic Ideas and Terminology 17
Now consider the differential equation
d2y
dx2 = 12x.
From elementary calculus we know that all functions whose second derivative is 12x can
be obtained by performing two integrations. Integrating the given differential equation
once yields
dy
dx = 6x2 + c1,
where c1 is an arbitrary constant. Integrating again we obtain
y(x) = 2x3 + c1x + c2,
(1.2.2)
where c2 is another arbitrary constant. The point to notice about this solution is that
it contains two arbitrary constants. Further, by assigning appropriate values to these
constants, we can determine all solutions to the differential equation. We call (1.2.2)
the general solution to the differential equation. In this example the given differential
equation is of second-order, and the general solution contains two arbitrary constants,
which arise due to the fact that two integrations are required to solve the differential
equation. In the case of an nth-order differential equation we might suspect that the most
general form of solution that can arise would contain n arbitrary constants. This is indeed
the case and motivates the following deﬁnition.
DEFINITION
1.2.8
A solution to an nth-order differential equation on an interval I is called the general
solution on I if it satisﬁes the following conditions:
1. The solution contains n constants c1, c2, . . . , cn.
2. All solutions to the differential equation can be obtained by assigning appropriate
values to the constants.
Remark
Not all differential equations have a general solution. For example, consider
(y′)2 + (y −1)2 = 0.
The only solution to this differential equation is y(x) = 1, and hence the differential
equation does not have a solution containing an arbitrary constant.
Example 1.2.9
Determine the general solution to the differential equation y′′ = 18 cos 3x.
Solution:
Integrating the given differential equation with respect to x yields
y′ = 6 sin 3x + c1,
where c1 is an integration constant. Integrating this equation we obtain
y(x) = −2 cos 3x + c1x + c2,
(1.2.3)
where c2 is another integration constant. Consequently, all solutions to y′′ = 18 cos 3x
are of the form (1.2.3), and therefore, according to Deﬁnition 1.2.8, this is the general
solution to y′′ = 18 cos 3x on any interval.
□

18
CHAPTER 1
First-Order Differential Equations
As the previous example illustrates, we can, in principle, always ﬁnd the general
solution to a differential equation of the form
dny
dxn = f (x)
(1.2.4)
by performing n integrations. However, if the function on the right-hand side of the
differential equation is not a function of x only, this procedure cannot be used. Indeed,
one of the major aims of this text is to determine solution techniques for differential
equations that are more complicated than Equation (1.2.4).
A solution to a differential equation is called a particular solution if it does not
contain any arbitrary constants not present in the differential equation itself. One way in
which particular solutions arise is by assigning speciﬁc values to the arbitrary constants
occurring in the general solution to a differential equation. For example, from (1.2.3),
y(x) = −2 cos 3x + 5x −7
is a particular solution to the differential equation d2y/dx2 = 18 cos 3x (the solution
corresponding to c1 = 5, c2 = −7).
Initial-Value Problems
As discussed in the previous section, the unique speciﬁcation of an applied problem
requires more than just a differential equation. We must also give appropriate auxiliary
conditions that characterize the problem under investigation. Of particular interest to
us is the case of the initial-value problem deﬁned for an nth-order differential equation
as follows.
DEFINITION
1.2.10
An nth-order differential equation together with n auxiliary conditions of the form
y(x0) = y0,
y′(x0) = y1,
. . . ,
y(n−1)(x0) = yn−1,
where y0, y1, . . . , yn−1 are constants, is called an initial-value problem.
Example 1.2.11
Solve the initial-value problem
y′′ = 18 cos 3x
(1.2.5)
y(0) = 1,
y′(0) = 4.
(1.2.6)
Solution:
From Example 1.2.9, the general solution to Equation (1.2.5) is
y(x) = −2 cos 3x + c1x + c2.
(1.2.7)
We now impose the auxiliary conditions (1.2.6). Setting x = 0 in (1.2.7) we see that
y(0) = 1
if and only if
1 = −2 + c2.
So c2 = 3. Using this value for c2 in (1.2.7) and differentiating the result yields
y′(x) = 6 sin 3x + c1.

1.2
Basic Ideas and Terminology 19
Consequently
y′(0) = 4
if and only if
4 = 0 + c1
and hence c1 = 4. Thus the given auxiliary conditions pick out the particular solution
to the differential equation (1.2.5) with c1 = 4, and c2 = 3, so that the initial-value
problem has the unique solution
y(x) = −2 cos 3x + 4x + 3.
□
Initial-value problems play a fundamental role in the theory and applications of
differential equations. In the previous example, the initial-value problem had a unique
solution. More generally, suppose we have a differential equation that can be written in
the normal form
y(n) = f (x, y, y′, . . . , y(n−1)).
According to Deﬁnition 1.2.10, the initial-value problem for such an nth-order dif-
ferential equation is the following:
Statement of the Initial-Value Problem: Solve
y(n) = f (x, y, y′, . . . , y(n−1))
subject to
y(x0) = y0,
y′(x0) = y1,
. . . ,
y(n−1)(x0) = yn−1,
where y0, y1, . . . , yn−1 are constants.
It can be shown that this initial-value problem always has a unique solution pro-
vided f and its partial derivatives with respect to y, y′, . . . , y(n−1), are continuous in an
appropriate region. This is a fundamental result in the theory of differential equations.
In Chapter 8 we will show how the following special case can be used to develop the
theory for linear differential equations.
Theorem 1.2.12
Let a1, a2, . . . , an, F be functions that are continuous on an interval I. Then, for any x0
in I, the initial-value problem
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x)
y(x0) = y0,
y′(x0) = y1,
. . . ,
y(n−1)(x0) = yn−1
has a unique solution on I.
The next example, which we will refer back to on many occasions throughout the
remainder of the text, illustrates the power of the preceding theorem.
Example 1.2.13
Prove that the general solution to the differential equation
y′′ + ω2y = 0,
−∞< x < ∞,
(1.2.8)
where ω is a nonzero constant, is
y(x) = c1 cos ωx + c2 sin ωx,
(1.2.9)
where c1, c2 are arbitrary constants.
Solution:
It is a routine computation to verify that y(x) = c1 cos ωx + c2 sin ωx
is a solution to the differential equation (1.2.8) on (−∞, ∞). According to Deﬁnition
1.2.8 we must now establish that every solution to (1.2.8) is of the form (1.2.9). To that

20
CHAPTER 1
First-Order Differential Equations
end, suppose that y = f (x) is any solution to (1.2.8). Then according to the preceding
theorem, y = f (x) is the unique solution to the initial-value problem
y′′ + ω2y = 0,
y(0) = f (0),
y′(0) = f ′(0).
(1.2.10)
However, consider the function
y(x) = f (0) cos ωx + f ′(0)
ω
sin ωx.
(1.2.11)
This is of the form y(x) = c1 cos ωx +c2 sin ωx, where c1 = f (0) and c2 = f ′(0)
ω
, and
therefore solves the differential equation (1.2.8). Further, evaluating (1.2.11) at x = 0
yields
y(0) = f (0) and
y′(0) = f ′(0).
Consequently, (1.2.11) solves the initial-value problem (1.2.10). But, by assumption,
y(x) = f (x) solves the same initial-value problem. Due to the uniqueness of solution to
this initial-value problem it follows that these two solutions must coincide. Therefore,
f (x) = f (0) cos ωx + f ′(0)
ω
sin ωx = c1 cos ωx + c2 sin ωx.
Since f (x) was an arbitrary solution to the differential equation (1.2.8) we can conclude
that every solution to (1.2.8) is of the form
y(x) = c1 cos ωx + c2 sin ωx
and therefore this is the general solution on (−∞, ∞).
□
For the remainder of this chapter, we will focus our attention primarily on ﬁrst-order
differential equations and some of their elementary applications. We will investigate such
differential equations qualitatively, analytically, and numerically.
Exercises for 1.2
Key Terms
Linear differential equation, Nonlinear differential equation,
General solution to a differential equation, Particular solu-
tion to a differential equation, Initial-value problem.
Skills
• Be able to determine whether a given differential equa-
tion is linear or nonlinear.
• Be able to determine whether or not a given func-
tion y(x) is a particular solution to a given differential
equation.
• Be able to determine whether or not a given implicit
relation deﬁnes a particular solution to a given differ-
ential equation.
• Be able to ﬁnd the general solution to differential equa-
tions of the form y(n) = f (x) via n integrations.
• Be able to use initial conditions to ﬁnd the solution to
an initial-value problem.
True-False Review
For items (a)–(e), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The general solution to a third-order differential equa-
tion must contain three constants.
(b) An initial-value problem always has a unique solu-
tion if the functions and partial derivatives involved
are continuous.
(c) The general solution to y′′ + y = 0 is y(x) =
c1 cos x + 5c2 cos x.

1.2
Basic Ideas and Terminology 21
(d) The general solution to y′′ + y = 0 is y(x) =
c1 cos x + 5c1 sin x.
(e) The general solution to a differential equation of the
form y(n) = F(x) can be obtained by n consecutive
integrations of the function F(x).
Problems
For Problems 1–6, determine whether the differential equa-
tion is linear or nonlinear.
1. d2y
dx2 + ex dy
dx = x2.
2. d3y
dx3 + 4d2y
dx2 + sin x dy
dx = xy2 + tan x.
3. yy′′ + x(y′) −y = 4x ln x.
4. sin x · y′′ + y′ −tan y = cos x.
5. d4y
dx4 + 3d2y
dx2 = x.
6. √x y′′ + 1
y′ ln x = 3x3.
For Problems 7–21, verify that the given function is a solu-
tion to the given differential equation (c1 and c2 are arbitrary
constants), and state the maximum interval over which the
solution is valid.
7. y(x) = c1e−5x + c2e5x,
y′′ −25y = 0.
8. y(x) = c1 cos 2x + c2 sin 2x,
y′′ + 4y = 0.
9. y(x) = c1ex + c2e−2x,
y′′ + y′ −2y = 0.
10. y(x) =
1
x + 4,
y′ = −y2.
11. y(x) = c1x1/2,
y′ = y
2x .
12. y(x) = e−x sin 2x,
y′′ + 2y′ + 5y = 0.
13. y(x) = c1 cosh 3x + c2 sinh 3x,
y′′ −9y = 0.
14. y(x) = c1x−3 + c2x−1,
x2y′′ + 5xy′ + 3y = 0.
15. y(x) = c1x2 ln x,
x2y′′ −3xy′ + 4y = 0.
16. y(x) = c1x2 cos(3 ln x),
x2y′′ −3xy′ + 13y = 0.
17. y(x) = c1x1/2 + 3x2,
2x2y′′ −xy′ + y = 9x2.
18. y(x) = c1x2 + c2x3 −x2 sin x,
x2y′′ −4xy′ + 6y = x4 sin x.
19. y(x) = c1eax + c2ebx,
y′′ −(a + b)y′ + aby = 0,
where a and b are constants and a ̸= b.
20. y(x) = eax(c1 +c2x),
y′′ −2ay′ +a2y = 0, where
a is a constant.
21. y(x) = eax(c1 cos bx + c2 sin bx),
y′′ −2ay′ + (a2 + b2)y = 0, where a and b are
constants.
For Problems 22–25, determine all values of the constant
r such that the given function solves the given differential
equation.
22. y(x) = erx,
y′′ −y′ −6y = 0.
23. y(x) = erx,
y′′ + 6y′ + 9y = 0.
24. y(x) = xr,
x2y′′ + xy′ −y = 0.
25. y(x) = xr,
x2y′′ + 5xy′ + 4y = 0.
26. When N is a positive integer, the Legendre equation
(1 −x2)y′′ −2xy′ + N(N + 1)y = 0,
with −1 < x < 1, has a solution that is a polynomial
of degree N. Show by substitution into the differential
equation that in the case N = 3 such a solution is
y(x) = 1
2 x(5x2 −3).
27. Determine a solution to the differential equation
(1 −x2)y′′ −xy′ + 4y = 0
of the form y(x) = a0 + a1x + a2x2 satisfying the
normalization condition y(1) = 1.
For Problems 28–32, show that the given relation deﬁnes an
implicit solution to the given differential equation, where c
is an arbitrary constant.
28. x sin y −ex = c,
y′ = ex −sin y
x cos y
.
29. xy2 + 2y −x = c,
y′ =
1 −y2
2(1 + xy).
30. exy −x = c,
y′ = 1 −yexy
xexy
.
Determine the solution with y(1) = 0.
31. ey/x + xy2 −x = c,
y′ = x2(1 −y2) + yey/x
x(ey/x + 2x2y)
.

22
CHAPTER 1
First-Order Differential Equations
32. x2y2 −sin x = c,
y′ = cos x −2xy2
2x2y
. Determine
the explicit solution that satisﬁes y(π) = 1/π.
For Problems 33–36, ﬁnd the general solution to the given
differential equation and the maximum interval on which the
solution is valid.
33. y′ = sin x.
34. y′ = x−2/3.
35. y′′ = xex.
36. y′′ = xn, n an integer.
For Problems 37–40, solve the given initial-value problem.
37. y′ = x2 ln x, y(1) = 2.
38. y′′ = cos x, y(0) = 2, y′(0) = 1.
39. y′′′ = 6x, y(0) = 1, y′(0) = −1, y′′(0) = 4.
40. y′′ = xex, y(0) = 3, y′(0) = 4.
41. Prove that the general solution to y′′ −y = 0 on any
interval I is y(x) = c1ex + c2e−x.
A second-order differential equation together with two
auxiliary conditions imposed at different values of the
independent variable is called a boundary-value prob-
lem. For Problems 42–43, solve the given boundary-value
problem.
42. y′′ = e−x, y(0) = 1, y(1) = 0.
43. y′′ = −2(3 + 2 ln x), y(1) = y(e) = 0.
44. The differential equation y′′ + y = 0 has the general
solution y(x) = c1 cos x + c2 sin x.
(a) Show that the boundary-value problem y′′ + y =
0, y(0) = 0, y(π) = 1 has no solutions.
(b) Show that the boundary-value problem y′′ + y =
0, y(0) = 0, y(π) = 0 has an inﬁnite number
of solutions.
For Problems 45–50, verify that the given function is a so-
lution to the given differential equation. In these problems,
c1 and c2 are arbitrary constants.
45. ⋄y(x) = c1e2x + c2e−3x, y′′ + y′ −6y = 0.
46. ⋄y(x) = c1x4+c2x−2, x2y′′−xy′−8y = 0, x > 0.
47. ⋄y(x) = c1x2 + c2x2 ln x + 1
6x2(ln x)3,
x2y′′ −3xy′ + 4y = x2 ln x, x > 0.
48. ⋄y(x) = xa[c1 cos(b ln x) + c2 sin(b ln x)],
x2y′′ +(1−2a)xy′ +(a2 +b2)y = 0, x > 0, where
a and b are arbitrary constants.
49. ⋄y(x) = c1ex + c2e−x(1 + 2x + 2x2),
xy′′ −2y′ + (2 −x)y = 0, x > 0.
50. ⋄y(x) =
10
+
k=0
1
k!xk, xy′′ −(x + 10)y′ + 10y = 0,
x > 0.
51. ⋄
(a) Derive the polynomial of degree ﬁve that satisﬁes
both the Legendre equation
(1 −x2)y′′ −2xy′ + 30y = 0
and the normalization condition y(1) = 1.
(b) ⋄Sketch your solution from (a) and determine ap-
proximations to all zeros and local maxima and
local minima on the interval (−1, 1).
52. ⋄One solution to the Bessel equation of (nonnegative)
integer order N
x2y′′ + xy′ + (x2 −N 2)y = 0
is
y(x) = JN(x) =
∞
+
k=0
(−1)k
k!(N + k)!
#x
2
$2k+N
.
(a) Write the ﬁrst three terms of J0(x).
(b) Let J(0, x, m) denote the mth partial sum
J(0, x, m) =
m
+
k=0
(−1)k
(k!)2
#x
2
$2k
.
Plot J(0, x, 4) and use your plot to approximate
the ﬁrst positive zero of J0(x). Compare your
value against a tabulated value or one generated
by a computer algebra system.
(c) Plot J0(x) and J(0, x, 4) on the same axes over
the interval [0, 2]. How well do they compare?
(d) If your system has built-in Bessel functions, plot
J0(x) and J(0, x, m) on the same axes over the
interval [0, 10] for various values of m. What is
the smallest value of m that gives an accurate ap-
proximation to the ﬁrst three positive zeros of
J0(x)?

1.3
The Geometry of First-Order Differential Equations 23
1.3
The Geometry of First-Order Differential Equations
The primary aim of this chapter is to study the ﬁrst-order differential equation
dy
dx = f (x, y),
(1.3.1)
where f (x, y) is a given function of x and y. In this section we focus our attention mainly
on the geometric aspects of the differential equation and its solutions. The graph of any
solution to the differential equation (1.3.1) is called a solution curve. If we recall the
geometric interpretation of the derivative dy/dx as giving the slope of the tangent line
at any point on the curve with equation y = y(x), we see that the function f (x, y) in
(1.3.1) gives the slope of the tangent line to the solution curve passing through the point
(x, y). Consequently when we solve Equation (1.3.1), we are ﬁnding all curves whose
slope at the point (x, y) is given by the function f (x, y). According to our deﬁnition in
the previous section, the general solution to the differential equation (1.3.1) will involve
one arbitrary constant, and therefore, geometrically, the general solution gives a family
of solution curves in the xy-plane, one solution curve corresponding to each value of the
arbitrary constant.
Example 1.3.1
Find the general solution to the differential equation dy/dx = 2x, and sketch the corre-
sponding solution curves.
Solution:
The differential equation can be integrated directly to obtain y(x) = x2+c.
Consequently the solution curves are a family of parabolas in the xy-plane. This is
illustrated in Figure 1.3.1.
□
x
y
Figure 1.3.1: Some solution curves for the differential equation dy
dx = 2x.
Figure 1.3.2 gives a Mathematica plot of some solution curves to the differential
equation
dy
dx = y −x2.
This illustrates that generally the solution curves of a differential equation are quite
complicated. Upon completion of the material in this section, the reader will be able to
obtain Figure 1.3.2 without the necessity of a computer algebra system.

24
CHAPTER 1
First-Order Differential Equations
y
x
(x0, y0)
y9(x0) 5 f(x0, y0) 5 y0 2 x0
2
Figure 1.3.2: Some solution curves for the differential equation dy
dx = y −x2.
Existence and Uniqueness of Solutions
It is useful for the further analysis of the differential equation (1.3.1) to at this point
give a brief discussion of the existence and uniqueness of solutions to the corresponding
initial-value problem
dy
dx = f (x, y),
y(x0) = y0.
(1.3.2)
Geometrically, we are interested in ﬁnding the particular solution curve to the differential
equation that passes through the point in the xy-plane with coordinates (x0, y0). The
following questions arise regarding the initial-value problem:
1. Existence: Does the initial-value problem have any solutions?
2. Uniqueness: If the answer to (1) is yes, does the initial-value problem have only
one solution?
Certainly in the case of an applied problem we would be interested only in initial-value
problems that have precisely one solution. The following theorem establishes condi-
tions on f that guarantee the existence and uniqueness of a solution to the initial-value
problem (1.3.2).
Theorem 1.3.2
(Existence and Uniqueness Theorem)
Let f (x, y) be a function that is continuous on the rectangle
R = {(x, y) : a ≤x ≤b, c ≤y ≤d}.
Suppose further that ∂f
∂y is continuous in R. Then for any interior point (x0, y0) in the
rectangle R, there exists an interval I containing x0 such that the initial-value problem
(1.3.2) has a unique solution for x in I.
Proof A complete proof of this theorem can be found, for example, in G.F. Simmons,
Differential Equations, McGraw-Hill, 1972. Figure 1.3.3 gives a geometric illustration
of the result.
Remark
From a geometric viewpoint, if f (x, y) satisﬁes the hypotheses of the exis-
tence and uniqueness theorem in a region R of the xy-plane, then throughout that region
the solution curves of the differential equation dy/dx = f (x, y) cannot intersect. For if

1.3
The Geometry of First-Order Differential Equations 25
c
a
b
d
I
y
x
Unique solution on I
Rectangle, R
(x0, y0)
Figure 1.3.3: Illustration of the existence and uniqueness theorem for ﬁrst-order differential
equations.
two solution curves did intersect at (x0, y0) in R, then that would imply that there was
more than one solution to the initial-value problem
dy
dx = f (x, y),
y(x0) = y0,
which would contradict the existence and uniqueness theorem.
Thefollowingexampleillustrateshowtheprecedingtheoremcanbeusedtoestablish
the existence of a unique solution to a differential equation, even though at present we
do not know how to determine the solution.
Example 1.3.3
Prove that the initial-value problem
dy
dx = 3xy1/3,
y(0) = a
has a unique solution whenever a ̸= 0.
Solution:
In this case the initial point is x0 = 0, y0 = a, and f (x, y) = 3xy1/3.
Hence, ∂f/∂y = xy−2/3. Consequently, f is continuous at all points in the xy-plane,
whereas ∂f/∂y is continuous at all points not lying on the x-axis (y ̸= 0). Provided
a ̸= 0, we can certainly draw a rectangle containing (0, a) that does not intersect the
x-axis. (See Figure 1.3.4.) In any such rectangle the hypotheses of the existence and
uniqueness theorem are satisﬁed, and therefore the initial-value problem does indeed
have a unique solution.
□
y
x
(0, a)
Figure 1.3.4: The initial-value problem in Example 1.3.3 satisﬁes the hypotheses of the
existence and uniqueness theorem in the small rectangle, but not in the large rectangle.

26
CHAPTER 1
First-Order Differential Equations
Example 1.3.4
Discuss the existence and uniqueness of solutions to the initial-value problem
dy
dx = 3xy1/3,
y(0) = 0.
Solution:
The differential equation is the same as in the previous example, but the
initial condition is imposed on the x-axis. Since ∂f/∂y = xy−2/3 is not continuous
along the x-axis there is no rectangle containing (0, 0) in which the hypotheses of the
existence and uniqueness theorem are satisﬁed. We can therefore draw no conclusion
from the theorem itself. We leave it as an exercise to verify by direct substitution that the
given initial-value problem does in fact have the following two solutions:
y(x) = 0
and
y(x) = x3.
Consequently, in this case the initial-value problem does not have a unique solution.
□
Slope Fields
We now return to our discussion of the geometry of solutions to the differential equation
dy
dx = f (x, y).
The fact that the function f (x, y) gives the slope of the tangent line to the solution
curves of this differential equation leads to a simple and important idea for determining
the overall shape of the solution curves. We compute the value of f (x, y) at several
points and draw through each of the corresponding points in the xy-plane small line
segments having f (x, y) as their slopes. The resulting sketch is called the slope ﬁeld
for the differential equation. The key point is that each solution curve must be tangent to
the line segments that we have drawn, and therefore by studying the slope ﬁeld we can
obtain the general shape of the solution curves.
Example 1.3.5
Sketch the slope ﬁeld for the differential equation dy
dx = 2x2.
Solution:
The slope of the solution curves to the differential equation at each point in
the xy-plane depends on x only. Consequently, the slopes of the solution curves will be
the same at every point on any line parallel to the y-axis (on such a line, x is constant).
Table 1.3.1 contains the values of the slope of the solution curves at various points in the
interval [−1, 1].
x
Slope = 2x2
0
0
±0.2
0.08
±0.4
0.32
±0.6
0.72
±0.8
1.28
±1.0
2
Table 1.3.1: Values of the slope
for the differential equation in
Example 1.3.5.
Using this information we obtain the slope ﬁeld shown in Figure 1.3.5. In this
example, we can integrate the differential equation to obtain the general solution
y(x) = 2
3x3 + c.
Some solution curves and their relation to the slope ﬁeld are also shown in Figure 1.3.5.
□
In the previous example, the slope ﬁeld could be obtained fairly easily due to the fact
that the slope of the solution curves to the differential equation were constant on lines
parallel to the y-axis. For more complicated differential equations, further analysis is
generally required if we wish to obtain an accurate plot of the slope ﬁeld and the behavior
of the corresponding solution curves. Below we have listed three useful procedures.

1.3
The Geometry of First-Order Differential Equations 27
y
x
Figure 1.3.5: Slope ﬁeld and some representative solution curves for the differential equation
dy
dx = 2x2.
1. Isoclines: For the differential equation
dy
dx = f (x, y),
(1.3.3)
the function f (x, y) determines the regions in the xy-plane where the slope of
the solution curves is positive, as well as those regions where it is negative.
Furthermore, each solution curve will have the same slope k along the family
of curves
f (x, y) = k.
These curves are called the isoclines of the differential equation, and they can be
very useful in determining slope ﬁelds. When sketching a slope ﬁeld we often start
by drawing several isoclines and the corresponding line segments with slope k at
various points along them.
2. Equilibrium Solutions: Any solution to the differential equation (1.3.3) of the
form y(x) = y0 where y0 is a constant is called an equilibrium solution to the
differential equation. The corresponding solution curve is a line parallel to the x-
axis. From Equation (1.3.3), equilibrium solutions are given by any constant values
of y for which f (x, y) = 0, and therefore can often be obtained by inspection.
For example, the differential equation
dy
dx = (y −x)(y + 1)
has the equilibrium solution y(x) = −1. One of the reasons that equilibrium
solutions are useful in sketching slope ﬁelds and determining the general behavior
of the full family of solution curves is that, from the existence and uniqueness
theorem, we know that no other solution curves can intersect the solution curve
corresponding to an equilibrium solution. Consequently, equilibrium solutions
serve to divide the xy-plane into different regions.
3. Concavity Changes: By differentiating Equation (1.3.3) (implicitly) with respect
to x we can obtain an expression for d2y/dx2 in terms of x and y. This can be
useful in determining the behavior of the concavity of the solution curves to the
differential equation (1.3.3).

28
CHAPTER 1
First-Order Differential Equations
The remaining examples illustrate the application of the foregoing procedures.
Example 1.3.6
Sketch the slope ﬁeld and some approximate solution curves for the differential equation
dy
dx = y(2 −y).
(1.3.4)
Solution:
We ﬁrst note that the given differential equation has the two equilibrium
solutions
y(x) = 0
and
y(x) = 2.
Consequently, from Theorem 1.3.2, the xy-plane can be divided into the three distinct
regions y < 0, 0 < y < 2, and y > 2. From Equation (1.3.4) the behavior of the sign
of the slope of the solution curves in each of these regions is given in the following
schematic.
sign of slope:
−−−−|+ + ++ |−−−−
y-interval:
0
2
The isoclines are determined from
y(2 −y) = k.
That is,
y2 −2y + k = 0,
so that the solution curves have slope k at all points of intersection with the horizontal
lines
y = 1 ±
√
1 −k.
(1.3.5)
Table 1.3.2 contains some of the isocline equations. Note from Equation (1.3.5) that the
largest possible positive slope is k = 1. We see that the slope of the solution curves
quickly become very large and negative for y outside the interval [0, 2]. Finally, differ-
entiating Equation (1.3.4) implicitly with respect to x yields
d2y
dx2 = 2dy
dx −2y dy
dx = 2(1 −y)dy
dx = 2y(1 −y)(2 −y).
Slope of
Equation of
Solution Curves
Isocline
k = 1
y = 1
k = 0
y = 2 and
y = 0
k = −1
y = 1 ±
√
2
k = −2
y = 1 ±
√
3
k = −3
y = 3 and
y = −1
k = −n, n ≥1
y = 1 ± √n + 1
Table 1.3.2: Slope and isocline information for the differential equation in Example 1.3.6.
The sign of d2y/dx2 is given in the following schematic.
sign of y′′:
−−−−|+ + ++ |−−−−|+ + ++
y-interval:
0
1
2

1.3
The Geometry of First-Order Differential Equations 29
2
y
x
Figure 1.3.6: Hand-drawn slope ﬁeld, isoclines, and some solution curves for the differential
equation dy
dx = y(2 −y).
Using this information leads to the slope ﬁeld sketched in Figure 1.3.6. We have also
included some approximate solution curves. We see from the slope ﬁeld that for any
initial condition y(x0) = y0, with 0 ≤y0 ≤2, the corresponding unique solution to
the differential equation will be bounded. In contrast, if y0 > 2, the slope ﬁeld suggests
that all corresponding solutions approach y = 2 as x →∞, whereas if y0 < 0, then all
corresponding solutions approach y = 0 as x →−∞. Furthermore, the behavior of the
slope ﬁeld also suggests that the solution curves that do not lie in the region 0 < y < 2
may diverge at ﬁnite values of x. We leave it as an exercise to verify (by substitution into
Equation (1.3.4)) that for all values of the constant c,
y(x) =
2ce2x
ce2x −1
is a solution to the given differential equation. We see that any initial condition that
yields a positive value for c will indeed lead to a solution that has a vertical asymptote
at x = 1
2 ln(1/c).
□
Example 1.3.7
Sketch the slope ﬁeld for the differential equation
dy
dx = y −x.
(1.3.6)
Solution:
By inspection we see that the differential equation has no equilibrium
solutions. The isoclines of the differential equation are the family of straight lines y−x =
k. Thus each solution curve of the differential equation has slope k at all points along
the line y −x = k. Table 1.3.3 contains several values for the slopes of the solution
curves, and the equations of the corresponding isoclines. We note that the slope at all
points along the isocline y = x + 1 is unity, which, from Table 1.3.3, coincides with
the slope of any solution curve that meets it. This implies that the isocline must in fact
coincide with a solution curve. Hence, one solution to the differential equation (1.3.6)
is y(x) = x + 1 and, by the existence and uniqueness theorem, no other solution curve
can intersect this one.

30
CHAPTER 1
First-Order Differential Equations
Slope of
Equation of
Solution Curves
Isocline
k = −2
y = x −2
k = −1
y = x −1
k = 0
y = x
k = 1
y = x + 1
k = 2
y = x + 2
Table 1.3.3: Slope and isocline information for the differential equation in Example 1.3.7.
In order to determine the behavior of the concavity of the solution curves, we dif-
ferentiate the given differential equation implicitly with respect to x to obtain
d2y
dx2 = dy
dx −1 = y −x −1,
where we have used (1.3.6) to substitute for dy/dx in the second step. We see that the
solution curves are concave up (y′′ > 0) at all points above the line
y = x + 1
(1.3.7)
and concave down (y′′ < 0) at all points beneath this line. We also note that Equa-
tion (1.3.7) coincides with the particular solution already identiﬁed. Putting all of this
information together we obtain the slope ﬁeld sketched in Figure 1.3.7.
□
y
x
y 5 x 1 1
Isoclines
Figure 1.3.7: Hand-drawn slope ﬁeld, isoclines, and some approximate solution curves for the
differential equation in Example 1.3.7.
Generating Slope Fields Using Technology
Many computer algebra systems (CAS) and graphing calculators have built-in programs
to generate slope ﬁelds. As an example, in the CAS Maple the command
diffeq := diff(y(x), x) = y(x) −x;
assigns the name diffeq to the differential equation considered in the previous example.
The further command
DEplot(diffeq, y(x), x = −3..3, y = −3..3, arrows=line);
then produces a sketch of the slope ﬁeld for the differential equation on the square
−3 ≤x ≤3, −3 ≤y ≤3. Initial conditions such as y(0) = 0, y(0) = 1,

1.3
The Geometry of First-Order Differential Equations 31
1
1
2
3
2
21
21
3
y
x
22
23
22
23
Figure 1.3.8: Maple plot of the slope ﬁeld and some approximate solution curves for the
differential equation in Example 1.3.7.
y(0) = 2, y(0) = −1 can be speciﬁed using the command
IC := {[0, 0], [0, 1], [0, 2], [0, −1]};
Then the command
DEplot(diffeq, y(x), x = −3..3, IC, y = −3..3, arrows=line);
not only plots the slope ﬁeld, but also gives a numerical approximation to each of the
solution curves satisfying the speciﬁed initial conditions. Some of the methods that can
be used to generate such numerical approximations will be discussed in Section 1.10.
The preceding sequence of Maple commands was used to generate the Maple plot given
in Figure 1.3.8. Clearly the generation of slope ﬁelds and approximate solution curves
is one area where technology can be extremely helpful.
Exercises for 1.3
Key Terms
Solution curve, Existence and Uniqueness Theorem, Slope
ﬁeld, Isocline, Equilibrium solution.
Skills
• Be able to ﬁnd isoclines for a differential equation
dy
dx = f (x, y).
• Be able to determine equilibrium solutions for a dif-
ferential equation dy
dx = f (x, y).
• Be able to sketch the slope ﬁeld for a differential equa-
tion, using isoclines, equilibrium solutions, and con-
cavity changes.
• Be able to sketch solution curves to a differential
equation.
• Be able to apply the Existence and Uniqueness Theo-
rem to ﬁnd unique solutions to initial-value problems.
True-False Review
For items (a)–(g), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,

32
CHAPTER 1
First-Order Differential Equations
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If f (x, y) satisﬁes the hypotheses of the Existence
and Uniqueness Theorem in a region R of the xy-
plane, then the solution curves to a differential equa-
tion dy
dx = f (x, y) cannot intersect in R.
(b) Every differential equation dy
dx = f (x, y) has at least
one equilibrium solution.
(c) The differential equation dy
dx = x(y2−4) has no equi-
librium solutions.
(d) The circle x2+y2 = 4 is an isocline for the differential
equation dy
dx = x2 + y2.
(e) The equilibrium solutions of a differential equation are
always parallel to one another.
(f) The isoclines for the differential equation dy
dx
=
x2 + y2
2y
are the family of circles x2 + (y −k)2 = k2.
(g) No solution to the differential equation dy
dx = f (x, y)
can intersect with equilibrium solutions of the differ-
ential equation.
Problems
For Problems 1–8, determine the differential equation giving
the slope of the tangent line at the point (x, y) for the given
family of curves.
1. y = ce2x.
2. y = ecx.
3. y = cx2.
4. y = c/x.
5. y2 = cx.
6. x2 + y2 = 2cx.
7. (x −c)2 + (y −c)2 = 2c2.
8. 2cy = x2 −c2.
For Problems 9–12, verify that the given function (or rela-
tion) deﬁnes a solution to the given differential equation and
sketch some of the solution curves. If an initial condition is
given, label the solution curve corresponding to the resulting
unique solution. (In these problems, c denotes an arbitrary
constant.)
9. x2 + y2 = c, y′ = −x/y.
10. y = cx3, y′ = 3y/x,
y(2) = 8.
11. y2 = cx, 2x dy −y dx = 0,
y(1) = 2.
12. (x −c)2 + y2 = c2, y′ = y2 −x2
2xy
,
y(2) = 2.
13. Prove that the initial-value problem
y′ = x sin(x + y),
y(0) = 1
has a unique solution.
14. Use the existence and uniqueness theorem to prove
that y(x) = 3 is the only solution to the initial-value
problem
y′ =
x
x2 + 1(y2 −9),
y(0) = 3.
15. Do you think that the initial-value problem
y′ = xy1/2,
y(0) = 0
has a unique solution? Justify your answer.
16. Even simple looking differential equations can have
complicated solution curves. In this problem, we study
the solution curves of the differential equation
y′ = −2xy2.
(1.3.8)
(a) Verify that the hypotheses of the existence and
uniqueness theorem (Theorem 1.3.2) are satisﬁed
for the initial-value problem
y′ = −2xy2,
y(x0) = y0
for every (x0, y0). This establishes that the initial-
value problem always has a unique solution on
some interval containing x0.
(b) Verify that for all values of the constant c, y(x) =
1
(x2 + c) is a solution to (1.3.8).
(c) Use the solution to (1.3.8) given in (b) to solve the
following initial-value problem. For each case,
sketch the corresponding solution curve, and state
the maximum interval on which your solution
is valid.

1.3
The Geometry of First-Order Differential Equations 33
(i) y′ = −2xy2,
y(0) = 1.
(ii) y′ = −2xy2,
y(1) = 1.
(iii) y′ = −2xy2,
y(0) = −1.
(d) What is the unique solution to the initial-value
problem
y′ = −2xy2,
y(0) = 0?
17. Consider the initial-value problem:
y′ = y(y −1),
y(x0) = y0.
(a) Verify that the hypotheses of the existence and
uniqueness theorem are satisﬁed for this initial-
value problem for any x0, y0. This establishes that
the initial-value problem always has a unique so-
lution on some interval containing x0.
(b) By inspection, determine all equilibrium solu-
tions to the differential equation.
(c) Determine the regions in the xy-plane where the
solution curves are concave up, and determine
those regions where they are concave down.
(d) Sketch the slope ﬁeld for the differential equa-
tion, and determine all values of y0 for which the
initial-value problem has bounded solutions. On
your slope ﬁeld, sketch representative solution
curves in the three cases y0 < 0, 0 < y0 < 1,
and y0 > 1.
For Problems 18–21:
(a) Determine all equilibrium solutions.
(b) Determine the regions in the xy-plane where the so-
lutions are increasing, and determine those regions
where they are decreasing.
(c) Determine the regions in the xy-plane where the so-
lution curves are concave up, and determine those re-
gions where they are concave down.
(d) Sketch representative solution curves in each region
of the xy-plane identiﬁed in (c).
18. y′ = (y + 2)(y −1).
19. y′ = (y −2)2.
20. y′ = y2(y −1).
21. y′ = y(y −1)(y + 1).
For Problems 22–29, sketch the slope ﬁeld and some repre-
sentative solution curves for the given differential equation.
22. y′ = 4x.
23. y′ = 1/x.
24. y′ = x + y.
25. y′ = x/y.
26. y′ = −4x/y.
27. y′ = x2y.
28. y′ = x2 cos y.
29. y′ = x2 + y2.
30. According to Newton’s law of cooling (see Section
1.1), the temperature of an object at time t is governed
by the differential equation
dT
dt = −k(T −Tm),
where Tm is the temperature of the surrounding
medium, and k is a constant. Consider the case when
Tm = 70 and k = 1/80. Sketch the corresponding
slope ﬁeld and some representative solution curves.
What happens to the temperature of the object as
t →∞. Note that this result is independent of the
initial temperature of the object.
For Problems 31–36, determine the slope ﬁeld and some rep-
resentativesolutioncurvesforthegivendifferentialequation.
31. ⋄y′ = −2xy.
32. ⋄y′ = x sin x
1 + y2 .
33. ⋄y′ = 3x −y.
34. ⋄y′ = 2x2 sin y.
35. ⋄y′ =
2 + y2
3 + 0.5x2 .
36. ⋄y′ =
1 −y2
2 + 0.5x2 .
37. ⋄
(a) Determine the slope ﬁeld for the differential
equation
y′ = x−1(3 sin x −y)
on the interval (0, 10].
(b) Plot the solution curves corresponding to each of
the following initial conditions:
y(0.5) = 0,
y(1) = −1,
y(1) = 2,
y(3) = 0.

34
CHAPTER 1
First-Order Differential Equations
What do you conclude about the behavior as
x →0+ of solutions to the differential equation?
(c) Plot the solution curve corresponding to the ini-
tial condition y(π/2) = 6/π. How does this ﬁt
in with your answer to part (b)?
(d) Describe the behavior of the solution curves for
large positive x.
38. ⋄Consider the family of curves y = kx2, where k is
a constant.
(a) Show that the differential equation of the family
of orthogonal trajectories is
dy
dx = −x
2y .
(b) On the same axes sketch the slope ﬁeld for the
preceding differential equation and several mem-
bers of the given family of curves. Describe the
family of orthogonal trajectories.
39. ⋄Consider the differential equation
di
dt + ai = b,
where a and b are constants. By drawing the slope
ﬁelds corresponding to various values of a and b, for-
mulate a conjecture regarding the value of
lim
t→∞i(t).
1.4
Separable Differential Equations
In the previous section we analyzed ﬁrst-order differential equations using qualitative
techniques. We now begin an analytical study of these differential equations by devel-
oping some solution techniques that enable us to determine the exact solution to certain
types of differential equations. The simplest differential equations for which a solu-
tion technique can be obtained are the so-called separable equations, which are deﬁned
as follows:
DEFINITION
1.4.1
A ﬁrst-order differential equation is called separable if it can be written in the form
p(y)dy
dx = q(x).
(1.4.1)
The solution technique for a separable differential equation is given in Theorem 1.4.2.
Theorem 1.4.2
If p(y) and q(x) are continuous, then Equation (1.4.1) has the general solution
,
p(y) dy =
,
q(x) dx + c
(1.4.2)
where c is an arbitrary constant.
Proof We use the chain rule for derivatives to rewrite Equation (1.4.1) in the equivalent
form
d
dx
!,
p(y) dy
"
= q(x).
Integrating both sides of this equation with respect to x yields Equation (1.4.2).
Remark
In differential form, Equation (1.4.1) can be written as
p(y) dy = q(x) dx

1.4
Separable Differential Equations 35
and the general solution (1.4.2) is obtained by integrating the left-hand side with respect
to y and the right-hand side with respect to x. This is the general procedure for solving
separable equations.
Example 1.4.3
Solve (ey + 4y2)dy
dx = 9xe3x.
Solution:
By inspection we see that the differential equation is separable. Integrating
both sides of the differential equation yields
,
(ey + 4y2)dy =
,
9xe3x dx + c.
Using integration by parts to evaluate the integral on the right-hand side we obtain
ey + 4
3 y3 = 3xe3x −e3x + c,
or equivalently,
3ey + 4y3 = 3e3x(3x −1) + c1,
where c1 = 3c. As often happens with separable differential equations, the solution is
given in implicit form.
□
In general, the differential equation dy
dx = f (x)g(y) is separable, since it can be
written as
1
g(y)
dy
dx = f (x),
which is of the form of Equation (1.4.1) with p(y) = 1/g(y). It is important to note,
however, that in writing the given differential equation in this way, we have assumed
that g(y) ̸= 0. Thus the general solution to the resulting differential equation may not
include solutions of the original equation corresponding to any values of y for which
g(y) = 0. (These are the equilibrium solutions for the original differential equation.)
We will illustrate with an example.
Example 1.4.4
Find all solutions to
y′ = −2xy2.
(1.4.3)
Solution:
Separating the variables yields
y−2dy = −2x dx.
(1.4.4)
Integrating both sides we obtain
−y−1 = −x2 + c,
so that
y(x) =
1
x2 −c.
(1.4.5)
This is the general solution to Equation (1.4.4). It is not the general solution to Equation
(1.4.3), since there is no value of the constant c for which y(x) = 0, whereas by inspec-
tion, we see y(x) = 0 is a solution to Equation (1.4.3). This solution is not contained in
(1.4.5), since in separating the variables, we divided by y and hence assumed implicitly
that y ̸= 0. Thus the solutions to Equation (1.4.3) are
y(x) =
1
x2 −c
and
y(x) = 0.
The slope ﬁeld for the given differential equation is depicted in Figure 1.4.1, together
with some representative solution curves.
□

36
CHAPTER 1
First-Order Differential Equations
2
1
21
21
22
22
2
1
y
x
Figure 1.4.1: The slope ﬁeld and some solution curves for the differential equation
dy
dx = −2xy2.
Many of the difﬁculties that students encounter with ﬁrst-order differential equations
arise not from the solution techniques themselves, but in the algebraic simpliﬁcations that
are used to obtain a simple form for the resulting solution. We will explicitly illustrate
some of the standard simpliﬁcations using the differential equation
dy
dx = −2xy.
First notice that y(x) = 0 is an equilibrium solution to the differential equation. Con-
sequently, no other solution curves can cross the x-axis. For y ̸= 0 we can separate the
variables to obtain
1
y dy = −2x dx.
(1.4.6)
Integrating this equation yields
ln |y| = −x2 + c.
Exponentiating both sides of this solution gives
|y| = e−x2+c
or equivalently,
|y| = ece−x2.
We now introduce a new constant c1 deﬁned by c1 = ec. Then the preceding expression
for |y| reduces to
|y| = c1e−x2.
(1.4.7)
Notice that c1 is a positive constant. This is a perfectly acceptable form for the solution.
However, a redeﬁnition of the integration constant can be used to eliminate the absolute
value bars as follows. According to (1.4.7), the solution to the differential equation is
y(x) =
-
c1e−x2, if y > 0,
−c1e−x2, if y < 0.
(1.4.8)

1.4
Separable Differential Equations 37
We can now deﬁne a new constant c2, by
c2 =
-
c1, if y > 0,
−c1, if y < 0,
in terms of which the solutions given in (1.4.8) can be combined into the single formula
y(x) = c2e−x2.
(1.4.9)
The appropriate sign for c2 will be determined from the initial conditions. For example,
the initial condition y(0) = 1 would require that c2 = 1, with corresponding unique
solution
y(x) = e−x2.
Similarly the initial condition y(0) = −1 leads to c2 = −1, so that
y(x) = −e−x2.
We make one further point about the solution (1.4.9). In obtaining the separable form
(1.4.6), we divided the given differential equation by y, and so, the derivation of the
solution obtained assumes that y ̸= 0. However, as we have already noted, y(x) = 0 is
indeed a solution to this differential equation. Formally this solution is the special case
c2 = 0 in (1.4.9), and corresponds to the initial condition y(0) = 0. Thus (1.4.9) does
give the general solution to the differential equation, provided we allow c2 to assume the
value zero. The slope ﬁeld for the differential equation, together with some particular
solution curves, is shown in Figure 1.4.2.
3
21
22
23
2
1
1
2
21
22
23
3
y
x
Figure 1.4.2: Slope ﬁeld and some solution curves for the differential equation dy
dx = −2xy.
Example 1.4.5
An object of mass m falls from rest, starting at a point near the earth’s surface. Assum-
ing that the air resistance is proportional to the velocity of the object, determine the
subsequent motion.
Solution:
Let y(t) be the distance travelled by the object at time t from the point it
was released, and let the positive y direction be downward. Then, y(0) = 0, and the
velocity of the object is v(t) = dy/dt. Since the object was dropped from rest, we have
v(0) = 0. The forces acting on the object are those due to gravity, Fg = mg, and the

38
CHAPTER 1
First-Order Differential Equations
force due to air resistance, Fr = −kv, where k is a positive constant (see Figure 1.4.3).
According to Newton’s second law, the differential equation describing the motion of
the object is
m dv
dt = Fg + Fr = mg −kv.
Wearealsogiventheinitialconditionv(0) = 0.Thustheinitial-valueproblemgoverning
mg
ky
Positive y
Figure 1.4.3: Object falling
under the inﬂuence of gravity and
air resistance.
the behavior of v is
⎧
⎨
⎩
m dv
dt
= mg −kv,
v(0)
= 0.
(1.4.10)
Separating the variables in Equation (1.4.10) yields
m
mg −kv dv = dt,
which can be integrated directly to obtain
−m
k ln |mg −kv| = t + c.
Multiplying both sides of this equation by −k/m and exponentiating the result yields
|mg −kv| = c1e−(k/m)t,
where c1 = e−ck/m. By redeﬁning the constant c1, we can write this in the equivalent
form
mg −kv = c2e−(k/m)t.
Hence,
v(t) = mg
k
−c3e−(k/m)t,
(1.4.11)
where c3 = c2/k. Imposing the initial condition v(0) = 0 yields
c3 = mg
k .
So the solution to the initial-value problem (1.4.10) is
v(t) = mg
k
1
1 −e−(k/m)t2
.
(1.4.12)
As noted in Section 1.1, the velocity of the object does not increase indeﬁnitely, but
approaches the terminal velocity
vT = lim
t→∞v(t) = lim
t→∞
mg
k
1
1 −e−(k/m)t2
= mg
k .
The behavior of the velocity as a function of time is shown in Figure 1.4.4. Since dy/dt =
v, it follows from (1.4.12) that the position of the object at time t can be determined by
solving the initial-value problem
dy
dt = mg
k
1
1 −e−(k/m)t2
,
y(0) = 0.
The differential equation can be integrated directly to obtain
y(t) = mg
k
1
t + m
k e−(k/m)t2
+ c.

1.4
Separable Differential Equations 39
mg/k
y
t
Figure 1.4.4: The behavior of the velocity of the object in Example 1.4.5.
Imposing the initial condition y(0) = 0 yields
c = −m2g
k2 ,
so that
y(t) = mg
k
3
t + m
k
1
e−(k/m)t −1
24
.
□
Example 1.4.6
A hot metal bar whose temperature is 350◦F is placed in a room whose temperature is
constant at 70◦F. After two minutes, the temperature of the bar is 210◦F. Using Newton’s
law of cooling, determine
1. the temperature of the bar after four minutes.
2. the time required for the bar to cool to 100◦F.
Solution:
According to Newton’s law of cooling (see Section 1.1), the temperature
of the object at time t measured in minutes is governed by the differential equation
dT
dt = −k(T −Tm),
(1.4.13)
where, from the statement of the problem,
Tm = 70◦F,
T (0) = 350◦F,
T (2) = 210◦F.
Substituting for Tm in Equation (1.4.13), we have the separable equation
dT
dt = −k(T −70).
Separating the variables yields
1
T −70 dT = −k dt,
which we can integrate immediately to obtain
ln |T −70| = −kt + c.
Exponentiating both sides and solving for T yields
T (t) = 70 + c1e−kt,
(1.4.14)

40
CHAPTER 1
First-Order Differential Equations
where we have redeﬁned the integration constant. The two constants c1 and k can be
determined from the given auxiliary conditions as follows. The condition T (0) = 350◦F
requires that 350 = 70 + c1. Hence, c1 = 280. Substituting this value for c1 into
(1.4.14) yields
T (t) = 70(1 + 4e−kt).
(1.4.15)
Consequently, T (2) = 210◦F if and only if
210 = 70(1 + 4e−2k),
so that e−2k = 1
2. Hence, k = 1
2 ln 2, and so, from (1.4.15),
T (t) = 70
1
1 + 4e−(t/2) ln 22
.
(1.4.16)
We can now answer the questions (1) and (2).
1. We have T (4) = 70(1 + 4e−2 ln 2) = 70
!
1 + 4 · 1
22
"
= 140◦F.
2. From (1.4.16), T (t) = 100◦F when
100 = 70
1
1 + 4e−(t/2) ln 22
;
that is, when
e−(t/2) ln 2 = 3
28.
Taking the natural logarithm of both sides and solving for t yields
t = 2 ln(28/3)
ln 2
≈6.4 minutes.
□
Example 1.4.7
According to the ontogenetic growth model discussed in Section 1.1, the mass of an
organism t days after birth is governed by the initial-value problem
dm
dt = am3/4
%
1 −
# m
M
$1/4&
,
m(0) = m0
(1.4.17)
where M grams is the organism’s maximum body size, and a is a dimensionless constant
for a given taxon. A guinea pig has a birth mass of 4 g (grams), and when fully grown its
mass is 850 g. Given that a = 0.2, determine the mass of the guinea pig after 200 days.
Solution:
Substituting the given values for a, M, and m0 into the initial-value problem
(1.4.17) yields
dm
dt = 0.2m3/4
%
1 −
# m
850
$1/4&
,
m(0) = 4.
(1.4.18)
Separating the variables in the preceding differential equation gives
1
m3/4
%
1 −
# m
850
$1/4&dm = 0.2 dt
so that
,
1
m3/4
%
1 −
# m
850
$1/4&dm = 0.2t + c.

1.4
Separable Differential Equations 41
To evaluate the integral on the left-hand side of the preceding equation, we make the
change of variable
w =
# m
850
$1/4
,
dw = 1
4 ·
1
850
# m
850
$−3/4
dm.
Simplifying, we obtain
4 · (850)1/4
,
1
1 −wdw = 0.2t + c,
which can be integrated directly to obtain
−4 · (850)1/4 ln(1 −w) = 0.2t + c.
Exponentiating both sides of the preceding equation, and solving for w yields
w = 1 −c1e−0.05t/(850)1/4
or equivalently,
# m
850
$1/4
= 1 −c1e−0.05t/(850)1/4.
Consequently,
m(t) = 850
1
1 −c1e−0.05t/(850)1/424
.
(1.4.19)
Imposing the initial condition m(0) = 4 yields
4 = 850 (1 −c1)4
so that
c1 = 1 −
! 2
425
"1/4
≈0.74.
Inserting this expression for c1 into Equation (1.4.19) gives
m(t) = 850
1
1 −0.74e−0.05t/(850)1/424
.
(1.4.20)
Consequently,
m(200) = 850
1
1 −0.74e−10/(850)1/424
≈519 g.
□
Consider a chemical reaction in which two chemicals A and B combine to form a
third chemical C. Let Q(t) denote the amount of C that has been formed at time t, and
assume that the reaction is such that A and B combine in the ratio a : b (that is, any sample
of C is made up of a parts of A and b parts of B). According to the Law of Mass Action:
The rate of change of Q at time t is proportional to the product of the amounts of A and
B that are unconverted at that time.
The following example illustrates the use of this law.
Example 1.4.8
In a certain chemical reaction 5 g of chemical C are formed when 2 g of chemical A
react with 3 g of chemical B. Initially there are 10 g of A and 24 g of B present, and after
5 min, 10 g of C has been produced. Determine the amount of C that is produced in 15 min.

42
CHAPTER 1
First-Order Differential Equations
Solution:
Since the chemicals A and B combine in the ratio 2:3, when Q grams of
C are formed, it consists of 2
5 Q grams of A and 3
5 Q grams of B. Consequently, the
amounts of A and B that are unconverted at time t are (10 −2
5 Q) grams and (24 −3
5 Q)
grams, respectively. Thus, according to the law of mass action, the differential equation
governing the behavior of Q(t) is
dQ
dt = k1
!
10 −2
5 Q
" !
24 −3
5 Q
"
or, equivalently,
dQ
dt = k(25 −Q)(40 −Q),
where k = 6k1/25. Separating the variables yields
1
(25 −Q)(40 −Q)dQ = k dt,
which can be written by using a partial fractions decomposition, as
1
15
!
1
25 −Q −
1
40 −Q
"
dQ = k dt.
Integrating and simplifying we obtain
ln
!40 −Q
25 −Q
"
= 15kt + ln c.
Exponentiating both sides of the preceding equation yields
40 −Q
25 −Q = ce15kt.
The initial condition Q(0) = 0 implies that c = 8
5, so that
40 −Q
25 −Q = 8
5e15kt.
(1.4.21)
Further, Q(5) = 10 requires that
e75k = 5
4
so that
k = 1
75 ln
!5
4
"
.
Substituting into (1.4.21) yields
40 −Q
25 −Q = 8
5e(t/5) ln(5/4).
Solving for Q we obtain
Q(t) = 200
5
e(t/5) ln(5/4) −1
8e(t/5) ln(5/4) −5
6
.
Consequently,
Q(15) = 200
5
e3 ln(5/4) −1
8e3 ln(5/4) −5
6
≈17.94 g.
□

1.4
Separable Differential Equations 43
Exercises for 1.4
Skills
• Be able to recognize whether or not a given differential
equation is separable.
• Be able to solve separable differential equations.
True-False Review
For items (a)–(i), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every differential equation of the form dy
dx
=
f (x)g(y) is separable.
(b) The general solution to a separable differential equa-
tion contains one constant whose value can be de-
termined from an initial condition for the differential
equation.
(c) Newton’s law of cooling is a separable differential
equation.
(d) The differential equation dy
dx = x2 + y2 is separable.
(e) The differential equation
dy
dx
=
x sin(xy) is
separable.
(f) The differential equation dy
dx = ex+y is separable.
(g) The differential equation dy
dx
=
1
x2(1 + y2) is
separable.
(h) The differential equation dy
dx = x + 4y
4x + y is separable.
(i) The differential equation dy
dx
=
x3y + x2y2
x2 + xy
is
separable.
Problems
For Problems 1–11, solve the given differential equation.
1. dy
dx = 2xy.
2. dy
dx =
y2
x2 + 1.
3. ex+ydy −dx = 0.
4. dy
dx =
y
x ln x .
5. ydx −(x −2)dy = 0.
6. dy
dx = 2x(y −1)
x2 + 3
.
7. y −x dy
dx = 3 −2x2 dy
dx .
8. dy
dx = cos(x −y)
sin x sin y −1.
9. dy
dx =
x(y2 −1)
2(x −2)(x −1).
10. dy
dx = x2y −32
16 −x2 + 2.
11. (x −a)(x −b)y′ −(y −c) = 0, where a, b, c are
constants, with a ̸= b.
In Problems 12–15, solve the given initial-value problem.
12. (x2 + 1)y′ + y2 = −1,
y(0) = 1.
13. (1 −x2)y′ + xy = ax,
y(0) = 2a, where a is a
constant.
14. dy
dx = 1 −sin(x + y)
sin y cos x ,
y(π/4) = π/4.
15. y′ = y3 sin x,
y(0) = 0.
16. One solution to the initial-value problem
dy
dx = 2
3(y −1)1/2,
y(1) = 1
is y(x) = 1. Determine another solution to this initial-
value problem. Does this contradict the existence and
uniqueness theorem (Theorem 1.3.2)? Explain.
17. An object of mass m falls from rest, starting at a point
near the earth’s surface. Assuming that the air resis-
tance varies as the square of the velocity of the object,
a simple application of Newton’s second law yields
the initial-value problem for the velocity, v(t), of the
object at time t:
m dv
dt = mg −kv2,
v(0) = 0,
where k, m, g are positive constants.

44
CHAPTER 1
First-Order Differential Equations
(a) Solve the foregoing initial-value problem for v in
terms of t.
(b) Does the velocity of the object increase indeﬁ-
nitely? Justify.
(c) Determine the position of the object at time t.
18. Find the equation of the curve that passes through the
point (0, 1/2) and whose slope at each point (x, y)
is −x
4y .
19. Find the equation of the curve that passes through
the point (3, 1) and whose slope at each point (x, y)
is ex−y.
20. Find the equation of the curve that passes through the
point (−1, 1) and whose slope at each point (x, y)
is x2y2.
21. At time t, the velocity v(t) of an object moving in a
straight line satisﬁes
dv
dt = −(1 + v2).
(1.4.22)
(a) Show that
tan−1(v) = tan−1(v0) −t,
where v0 denotes the velocity of the object at time
t = 0 (and we assume v0 > 0). Hence prove
that the object comes to rest after a ﬁnite time
tan−1(v0). Does the object remain at rest?
(b) Use the chain rule to show that (1.4.22) can be
written as v dv
dx = −(1+v2), where x(t) denotes
the distance travelled by the object at time t, from
its position at t = 0. Determine the distance trav-
elled by the object when it ﬁrst comes to rest.
22. The differential equation governing the velocity of an
object is
dv
dt = −kvn,
where k > 0 and n are constants. At t = 0, the object
is set in motion with velocity v0. Assume v0 > 0.
(a) Show that the object comes to rest in a ﬁnite time
if and only if n < 1, and determine the maximum
distance travelled by the object in this case.
(b) If 1 ≤n < 2, show that the maximum dis-
tance travelled by the object in a ﬁnite time is less
than
v2−n
0
(2 −n)k .
(c) If n ≥2, show that there is no limit to the distance
that the object can travel.
23. The pressure p, and density, ρ, of the atmosphere at a
height y above the earth’s surface are related by
dp = −gρdy.
Assuming that p and ρ satisfy the adiabatic equation
of state p = p0
! ρ
ρ0
"γ
, where γ ̸= 1 is a constant
and p0 and ρ0 denote the pressure and density at the
earth’s surface, respectively, show that
p = p0
#
1 −(γ −1)
γ
· ρ0gy
p0
$γ/(γ −1)
.
24. An object whose temperature is 615◦F is placed in
a room whose temperature is 75◦F. At 4 p.m., the
temperature of the object is 135◦F, and an hour later
its temperature is 95◦F. At what time was the object
placed in the room?
25. A ﬂammable substance whose initial temperature is
50◦F is inadvertently placed in a hot oven whose tem-
perature is 450◦F. After 20 minutes, the substance’s
temperature is 150◦F. Find the temperature of the sub-
stance after 40 minutes. Assuming that the substance
ignites when its temperature reaches 350◦F, ﬁnd the
time of combustion.
26. At 2 p.m. on a cool (34◦F) afternoon in March, Sher-
lock Holmes measured the temperature of a dead body
to be 38◦F. One hour later, the temperature was 36◦F.
After a quick calculation using Newton’s law of cool-
ing, and taking the normal temperature of a living body
to be 98◦F, Holmes concluded that the time of death
was 10 a.m. Was Holmes right?
27. At 4 p.m., a hot coal was pulled out of a furnace and
allowed to cool at room temperature (75◦F). If, after
10 minutes, the temperature of the coal was 415◦F, and
after 20 minutes, its temperature was 347◦F, ﬁnd the
following:
(a) The temperature of the furnace.
(b) The time when the temperature of the coal was
100◦F.
28. A hot object is placed in a room whose temperature is
72◦F. After one minute, the temperature of the object
is 150◦F and its rate of change of temperature is 20◦F

1.5
Some Simple Population Models 45
per minute. Find the initial temperature of the object
and the rate at which its temperature is changing after
10 minutes.
29. A hen has a birth mass of 4 g, and when fully grown
its mass is 2000 g. Using the ontogenetic model with
a = 0.5, determine the mass of the hen after 100 days.
30. A guppy has a birth mass of 0.008 g, and when fully
grown its mass is 0.15 g. Given that a = 0.10, deter-
mine the mass of the guppy after 30 days. After how
many days will its mass have reached 90% of its fully
grown mass?
31. In a certain chemical reaction 9 g of C are formed
when 6 g of A combine with 3 g of B. Initially there
are 20 g of both A and B, and after 10 min, 15 g of C
has been produced. Determine the amount of C that is
produced in 20 min.
32. Chemicals A and B combine in the ratio 2:3. Initially
there are 10 g of A and 15 g of B present, and after
5 min, 10 g of C has been produced. Determine the
amount of C that has been produced in 30 min. How
long will it take for the reaction to be 50% complete?
33. Chemicals A and B combine in the ratio 3:5 in pro-
ducing the chemical C. If we have 15 g of A, use the
law of mass action to determine the minimum amount
of B required to produce 30 g of C.
34. Chemicals A and B combine in the ratio a : b to form
a third chemical C.
(a) Show that, according to the law of mass ac-
tion, the differential equation that governs the
reaction is
dQ
dt = k
!
A0 −
a
a + b Q
" !
B0 −
b
a + b Q
"
(1.4.23)
where k is a constant of proportionality, and A0
and B0 denote the initial amounts of A and B,
respectively.
(b) Show that Equation (1.4.23) can be written in the
equivalent form
dQ
dt = r(α −Q)(β −Q)
(1.4.24)
where
r = (a + b)2
ab
k, α = a + b
a
A0, β = a + b
a
B0.
35. Solve the differential equation (1.4.24) with the ini-
tial condition Q(0) = 0, when α ̸= β. If α > β,
determine limt→∞Q(t).
36. Solve the differential equation (1.4.24) with the ini-
tial condition Q(0) = 0, when α = β. Determine
limt→∞Q(t).
37. The differential equation governing a trimolecular re-
action is
dQ
dt = k(α −Q)(β −Q)(γ −Q)
where k, α, β, γ are constants. Solve this differential
equation if α, β, γ are all distinct and Q(0) = 0.
1.5
Some Simple Population Models
In this section we consider in detail the models of population growth that were introduced
in Section 1.1. Other models are explored in the exercises.
Malthusian Growth
The simplest mathematical model of population growth is obtained by assuming that the
rate of increase of the population at any time is proportional to the size of the population
at that time. If we let P(t) denote the population at time t, then
d P
dt = kP,
where k is a positive constant. Separating the variables and integrating yields
P(t) = P0ekt,
(1.5.1)
where P0 denotes the population at t = 0. This law predicts an exponential increase in
the population with time, which gives a reasonably accurate description of the growth

46
CHAPTER 1
First-Order Differential Equations
of certain algae, bacteria, and cell cultures. It is called the Malthusian growth model.
The time taken for such a culture to double in size is called the doubling time. This is
the time, td, when P(td) = 2P0. Substituting into (1.5.1) yields
2P0 = P0ektd.
Dividing both sides by P0 and taking logarithms, we ﬁnd
ktd = ln 2,
so that the doubling time is
td = 1
k ln 2.
Example 1.5.1
The number of bacteria in a certain culture grows at a rate that is proportional to the
number present. If the number increased from 500 to 2000 in 2 hours, determine
1. The number present after 12 hours.
2. The doubling time.
Solution:
The behavior of the system is governed by the differential equation
d P
dt = kP,
so that
P(t) = P0ekt,
where the time t is measured in hours. Taking t = 0 as the time when the population
was 500, we have P0 = 500. Thus,
P(t) = 500ekt.
Further, P(2) = 2000 implies that
2000 = 500e2k,
so that
k = 1
2 ln 4 = ln 2.
Consequently,
P(t) = 500et ln 2.
1. The number of bacteria present after twelve hours is therefore
P(12) = 500e12 ln 2 = 500(212) = 2,048,000.
2. The doubling time of the system is
td = 1
k ln 2 = 1 hour.
□

1.5
Some Simple Population Models 47
Logistic Population Model
The Malthusian growth law (1.5.1) does not provide an accurate model for the growth
of a population over a long time period. To obtain a more realistic model we need to
take account of the fact that as the population increases several factors will begin to
have an effect on the growth rate. For example, there will be increased competition for
the limited resources that are available, increases in disease, and overcrowding of the
limited available space, all of which would serve to slow the growth rate. In order to
model this situation mathematically, we modify the differential equation leading to the
simple exponential growth law by adding in a term that slows the growth down as the
population increases. If we consider a closed environment (neglecting factors such as
immigration and emigration), then the rate of change of population can be modeled by
the differential equation
d P
dt = [B(t) −D(t)]P,
where B(t) and D(t) denote the birth rate and death rate per individual, respectively.
The simple exponential law corresponds to the case when B(t) = k and D(t) = 0. In
the more general situation of interest now, the increased competition as the population
grows would result in a corresponding increase in the death rate per individual. Perhaps
the simplest way to take account of this is to assume that the death rate per individual is
directly proportional to the instantaneous population, and that the birth rate per individual
remains constant. The resulting initial-value problem governing the population growth
can then be written as
d P
dt = (B0 −D0P)P,
P(0) = P0,
where B0 and D0 are positive constants. It is useful to write the differential equation in
the equivalent form
d P
dt = r
!
1 −P
C
"
P,
(1.5.2)
where r = B0, and C = B0/D0. Equation (1.5.2) is called the logistic equation, and the
corresponding population model is called the logistic model. The differential equation
(1.5.2) is separable, and can be solved without difﬁculty. Before doing that, however, we
give a qualitative analysis of the differential equation.
The constant C in Equation (1.5.2) is called the carrying capacity of the population.
We see from Equation (1.5.2) that if P < C, then d P/dt > 0 and the population
increases, whereas if P > C, then d P/dt < 0 and the population decreases. We can
therefore interpret C as representing the maximum population that the environment can
sustain. We note that P(t) = C is an equilibrium solution to the differential equation,
as is P(t) = 0. The isoclines for Equation (1.5.2) are determined from
r
!
1 −P
C
"
P = k,
where k is a constant. This can be written as
P2 −C P + kC
r
= 0,
so that the isoclines are the lines
P = 1
2
7
C ±
8
C2 −4kC
r
9
.

48
CHAPTER 1
First-Order Differential Equations
This tells us that the slopes of the solution curves satisfy
C2 −4kC
r
≥0,
so that
k ≤rC/4.
Furthermore, the largest value that the slope can assume is k = rC/4, which corresponds
to P = C/2. We also note that the slope approaches zero as the solution curves approach
theequilibriumsolutions P(t) = 0 and P(t) = C.DifferentiatingEquation(1.5.2)yields
d2P
dt2 = r
%!
1 −P
C
" d P
dt −P
C
d P
dt
&
= r
!
1 −2 P
C
" d P
dt = r2
C2 (C −2P)(C −P)P,
where we have substituted for d P/dt from (1.5.2) and simpliﬁed the result. Since P = C
and P = 0 are solutions to the differential equation (1.5.2), the only points of inﬂection
occur along the line P = C/2. The behavior of the concavity is therefore given by the
following schematic:
sign of P′′:
| + + ++ |−−−−|+ + ++
P-interval:
0
C/2
C
This information determines the general behavior of the solution curves to the differential
equation (1.5.2). Figure 1.5.1 gives a Maple plot of the slope ﬁeld and some representative
solution curves. Of course, such a ﬁgure could have been constructed by hand using the
information we have obtained. From Figure 1.5.1, we see that if the initial population is
less than the carrying capacity, then the population increases monotonically toward the
carrying capacity. Similarly, if the initial population is bigger than the carrying capacity,
then the population monotonically decreases toward the carrying capacity. Once more
this illustrates the power of the qualitative techniques that have been introduced for
analyzing ﬁrst-order differential equations.
C/2
C
t
P
Figure 1.5.1: Representative slope ﬁeld and some approximate solution curves for the logistic
equation.
We turn now to obtaining an analytical solution to the differential equation (1.5.2).
Separating the variables in Equation (1.5.2) and integrating yields
,
C
P(C −P) d P = rt + c1,

1.5
Some Simple Population Models 49
where c1 is an integration constant. Using a partial fraction decomposition on the left-
hand side, we ﬁnd
, ! 1
P +
1
C −P
"
d P = rt + c1,
which upon integration gives
ln
::::
P
C −P
:::: = rt + c1.
Exponentiating, and redeﬁning the integration constant yields
P
C −P = c2ert,
which can be solved algebraically for P to obtain
P(t) =
c2Cert
1 + c2ert ,
or equivalently,
P(t) =
c2C
c2 + e−rt .
Imposing the initial condition P(0) = P0, we ﬁnd that c2 = P0/(C −P0). Inserting this
value of c2 into the preceding expression for P(t) yields
P(t) =
C P0
P0 + (C −P0)e−rt .
(1.5.3)
We make two comments regarding this formula. Firstly, we see that due to the nega-
tive exponent of the exponential term in the denominator, as t →∞the population
does indeed tend to the carrying capacity C independently of the initial population P0.
Secondly, by writing (1.5.3) in the equivalent form
P(t) =
P0
P0/C + (1 −P0/C)e−rt ,
it follows that if P0 is very small compared to the carrying capacity, then for small t the
terms involving P0 in the denominator can be neglected, leading to the approximation
P(t) ≈P0ert.
Consequently, in this case, the Malthusian population model does approximate the lo-
gistic model for small time intervals.
Althoughwenowhaveaformulaforthesolutiontothelogisticpopulationmodel,the
qualitative analysis is certainly very enlightening as far as the general overall properties
of the solution are concerned. Of course, if we want to investigate speciﬁc details of a
particular model, then we would use the corresponding exact solution (1.5.3).
Example 1.5.2
The initial population (measured in thousands) of a city is 20. After 10 years, this has
increased to 50.87, and after 15 years the population is 78.68. Use the logistic model to
predict the population after 30 years.
Solution:
In this problem, we have P0 = P(0) = 20, P(10) = 50.87, P(15) =
78.68, and we wish to ﬁnd P(30). Substituting for P0 into Equation (1.5.3) yields
P(t) =
20C
20 + (C −20)e−rt .
(1.5.4)

50
CHAPTER 1
First-Order Differential Equations
Imposing the two remaining auxiliary conditions leads to the following pair of equations
for determining r and C:
50.87 =
20C
20 + (C −20)e−10r ,
78.68 =
20C
20 + (C −20)e−15r .
This is a pair of nonlinear equations that are tedious to solve by hand. We therefore turn
to technology. Using the algebraic capabilities of Maple, we ﬁnd that
r ≈0.1,
C ≈500.37.
Substituting these values of r and C into Equation (1.5.4) yields
P(t) =
10,007.4
20 + 480.37e−0.1t .
Accordingly, the predicted value of the population after 30 years is
P(30) =
10,007.4
20 + 480.37e−3 = 227.87.
A sketch of P(t) is given in Figure 1.5.2.
□
0
100
200
300
400
500
20
40
60
80
100
t
P
Carrying capacity
Figure 1.5.2: Solution curve corresponding to the population model in Example 1.5.2. The
population is measured in thousands of people.
Exercises for 1.5
Key Terms
Malthusian growth model, Doubling time, Logistic growth
model, Carrying capacity.
Skills
• Be able to solve the basic differential equations
describing the Malthusian and Logistic population
growth models.
• Be able to solve word problems involving initial con-
ditions, doubling time, etc. for the Malthusian and
Logistic population growth models.
• Be able to compute the carrying capacity for a Logistic
population model.
• Be able to discuss the qualitative behavior of a pop-
ulation governed by a Malthusian or Logistic model,
based on initial values, doubling time, etc., as a func-
tion of time.

1.5
Some Simple Population Models 51
True-False Review
For items (a)–(j), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A population whose growth rate at any given time is
proportional to its size at that time obeys the Malthu-
sian growth model.
(b) If a population obeys the Logistic growth model, then
its size can never exceed the carrying capacity of the
population.
(c) The differential equations which describe population
growth according to the Malthusian model and the Lo-
gistic model are both separable.
(d) The rate of change of a population whose growth is
described with the Logistic model eventually tends
toward zero, regardless of the initial population.
(e) If the doubling time of a population governed by the
Malthusian growth model is ﬁve minutes, then the ini-
tial population increases 64-fold in a half-hour.
(f) If a population whose growth is based on the Malthu-
sian growth model has a doubling time of 10 years,
then it takes approximately 30–40 years in order for
the initial population size to increase ten-fold.
(g) The population growth rate according to the Malthu-
sian growth model is always constant.
(h) The Logistic population model always has exactly two
equilibrium solutions.
(i) The concavity of the graph of population governed by
the Logistic model changes if and only if the initial
population is less than the carrying capacity.
(j) The concavity of the graph of a population governed
by the Malthusian growth model never changes, re-
gardless of the initial population.
Problems
1. The number of bacteria in a culture grows at a rate that
is proportional to the number present. Initially there
were 10 bacteria in the culture. If the doubling time of
the culture is 3 hours, ﬁnd the number of bacteria that
were present after 24 hours.
2. The number of bacteria in a culture grows at a rate that
is proportional to the number present. After 10 hours,
there were 5000 bacteria present, and after 12 hours,
there were 6000 bacteria present. Determine the ini-
tial size of the culture and the doubling time of the
population.
3. A certain cell culture has a doubling time of 4 hours.
Initially there were 2000 cells present. Assuming an
exponential growth law, determine the time it takes for
the culture to contain 106 cells.
4. At time t, the population P(t) of a certain city is in-
creasing at a rate that is proportional to the number of
residents in the city at that time. In January 2000, the
population of the city was 10,000 and by 2005 it had
risen to 20,000.
(a) What will the population of the city be at the be-
ginning of the year 2020?
(b) In what year will the population reach one
million?
In the logistic population model (1.5.3), if P(t1) = P1 and
P(2t1) = P2, then it can be shown (through some tedious al-
gebra to derive by hand, although easy on a computer algebra
system) that
r = 1
t1
ln
% P2(P1 −P0)
P0(P2 −P1)
&
,
(1.5.5)
C = P1[P1(P0 + P2) −2P0P2]
P2
1 −P0P2
.
(1.5.6)
These formulas will be used in Problems 5–7.
5. The initial population in a small village is 500. After
5 years, this has grown to 800, while after 10 years
the population is 1000. Using the logistic population
model, determine the population after 15 years.
6. An animal sanctuary had an initial population of 50 an-
imals. After two years, the population was 62, while
after four years it was 76. Using the logistic popula-
tion model, determine the carrying capacity and the
number of animals in the sanctuary after 20 years.
7. (a) Using Equations (1.5.5) and (1.5.6), and the fact
that r and C are positive, derive two inequalities
that P0, P1, P2 must satisfy in order for there to
be a solution to the logistic equation satisfying
the conditions
P(0) = P0,
P(t1) = P1,
P(2t1) = P2.
(b) The initial population in a town is 10,000. Af-
ter ﬁve years, this has grown to be 12,000, while

52
CHAPTER 1
First-Order Differential Equations
after ten years, the population is 18,000. Is there a
solution to the logistic equation that ﬁts this data?
8. Of the 1500 passengers, crew, and staff that board a
cruise ship, 5 have the ﬂu. After one day of sailing,
the number of infected people has risen to 10. As-
suming that the rate at which the ﬂu virus spreads is
proportional to the product of the number of infected
individuals and the number not yet infected, determine
how many people will have the ﬂu at the end of the
14-day cruise. Would you like to be a member of the
customer relations department for the cruise line the
day after the ship docks?
9. Consider the population model
d P
dt = r(P −T )P,
P(0) = P0,
(1.5.7)
where r, T , and P0 are positive constants.
(a) Perform a qualitative analysis of the differential
equation in the initial-value problem (1.5.7) fol-
lowing the steps used in the text for the logistic
equation. Identify the equilibrium solutions, the
isoclines, and the behavior of the slope and con-
cavity of the solution curves.
(b) Using the information obtained in (a), sketch the
slope ﬁeld for the differential equation and in-
clude representative solution curves.
(c) What predictions can you make regarding the
behavior of the population? Consider the cases
P0 < T and P0 > T . The constant T is called
the threshold level. Based on your predictions,
why is this an appropriate term to use for T ?
10. In the previous problem, a qualitative analysis of the
differential equation in (1.5.7) was carried out. In this
problem, we determine the exact solution to the dif-
ferential equation and verify the predictions from the
qualitative analysis.
(a) Solve the initial-value problem (1.5.7).
(b) Using your solution from (a), verify that if P0 <
T , then lim
t→∞P(t) = 0. What does this mean for
the population?
(c) Using your solution from (a), verify that if P0 >
T , then each solution curve has a vertical asymp-
tote at t = te, where
te = 1
rT ln
!
P0
P0 −T
"
.
How do you interpret this result in terms of pop-
ulation growth? Note that this was not obvious
from thequalitativeanalysis performedinthepre-
vious problem.
11. As a modiﬁcation to the population model considered
in the previous two problems, suppose that P(t) sat-
isﬁes the initial-value problem
d P
dt = r(C −P)(P −T )P,
P(0) = P0,
where r, C, T, P0 are positive constants, and 0 < T <
C. Perform a qualitative analysis of this model. Sketch
theslopeﬁeld,andsomerepresentativesolutioncurves
in the three cases 0 < P0 < T , T < P0 < C, and
P0 > C. Describe the behavior of the corresponding
solutions.
The next two problems consider the Gompertz
population model, which is governed by the initial-
value problem
d P
dt = r P(ln C −ln P),
P(0) = P0,
(1.5.8)
where r, C, and P0 are positive constants.
12. Determine all equilibrium solutions for the differen-
tial equation in (1.5.8), and the behavior of the slope
and concavity of the solution curves. Use this informa-
tion to sketch the slope ﬁeld and some representative
solution curves.
13. Solve the initial-value problem (1.5.8) and verify that
all solutions satisfy lim
t→∞P(t) = C.
Problems 14–16 consider the phenomenon of exponential
decay. This occurs when a population P(t) is governed by
the differential equation
d P
dt = kP,
where k is a negative constant.
14. A population of swans in a wildlife sanctuary is de-
clining due to the presence of dangerous chemicals in
the water. If the population of swans is experiencing
exponential decay, and if there were 400 swans in the
park at the beginning of the summer and 340 swans
30 days later,
(a) how many swans are in the park 60 days after
the start of summer? 100 days after the start of
summer?
(b) how long does it take for the population of swans
to be cut in half? (This is known as the half-life
of the population.)

1.6
First-Order Linear Differential Equations 53
15. At the conclusion of the Super Bowl, the number of
fans remaining in the stadium decreases at a rate pro-
portional to the number of fans in the stadium. Assume
that there are 100,000 fans in the stadium at the end of
the Super Bowl and ten minutes later there are 80,000
fans in the stadium.
(a) Thirty minutes after the Super Bowl will there be
more or less than 40,000 fans? How do you know
this without doing any calculations?
(b) What is the half-life (see the previous problem)
for the fan population in the stadium?
(c) When will there be only 15,000 fans left in the
stadium?
(d) Explain why the exponential decay model for the
population of fans in the stadium is not realistic
from a qualitative perspective.
16. Cobalt-60, an isotope used in cancer therapy, decays
exponentially with a half-life of 5.2 years (i.e., half
the original sample remains after 5.2 years). How long
does it take for a sample of Cobalt-60 to disintegrate
to the extent that only 4% of the original amount re-
mains?
17. ⋄Use some form of technology to solve the pair of
equations
P1 =
C P0
P0 + (C −P0)e−rt1 ,
P2 =
C P0
P0 + (C −P0)e−2rt1 ,
for r and C, and thereby derive the expressions given
in Equations (1.5.5) and (1.5.6).
18. ⋄According to data from the U.S. Bureau of the Cen-
sus, the population (measured in millions of people)
of the U.S. in 1950, 1960, and 1970 was, respectively,
151.3, 179.4, and 203.3.
(a) Usingthe1950and1960populationﬁgures,solve
the corresponding Malthusian population model.
(b) Determine the logistic model corresponding to
the given data.
(c) On the same set of axes, plot the solution curves
obtained in (a) and (b). From your plots, deter-
mine the values the different models would have
predicted for the population in 1980 and 1990,
and compare these predictions to the actual val-
ues of 226.54 and 248.71, respectively.
19. ⋄In a period of ﬁve years, the population of a city
doubles from its initial size of 50 (measured in thou-
sands of people). After ten more years, the population
has reached 250. Determine the logistic model corre-
sponding to this data. Sketch the solution curve and
use your plot to estimate the time it will take for the
population to reach 95% of the carrying capacity.
1.6
First-Order Linear Differential Equations
In this section we derive a technique for determining the general solution to any ﬁrst-order
linear differential equation. This is the most important technique in the chapter.
DEFINITION
1.6.1
A differential equation that can be written in the form
a(x)dy
dx + b(x)y = r(x),
(1.6.1)
where a(x), b(x), and r(x) are functions deﬁned on an interval (a, b), is called a
ﬁrst-order linear differential equation.
We assume that a(x) ̸= 0 on (a, b) and divide both sides of (1.6.1) by a(x) to obtain
the standard form
dy
dx + p(x)y = q(x),
(1.6.2)

54
CHAPTER 1
First-Order Differential Equations
where p(x) = b(x)/a(x) and q(x) = r(x)/a(x). The idea behind the solution technique
for (1.6.2) is to rewrite the differential equation in the form
d
dx [g(x, y)] = F(x)
for an appropriate function g(x, y). The general solution to the differential equation can
then be obtained by an integration with respect to x. First consider an example.
Example 1.6.2
Solve the differential equation
dy
dx + 1
x y = ex,
x > 0.
(1.6.3)
Solution:
If we multiply (1.6.3) by x we obtain
x dy
dx + y = xex.
But, from the product rule for differentiation, the left-hand side of this equation is just
the expanded form of d
dx (xy). Thus (1.6.3) can be written in the equivalent form
d
dx (xy) = xex.
Integrating both sides of this equation with respect to x we obtain
xy = xex −ex + c.
Dividing by x yields the general solution to (1.6.3) as
y(x) = x−1[ex(x −1) + c],
where c is an arbitrary constant.
□
In the previous example we multiplied the given differential equation by the function
I (x) = x. This had the effect of reducing the left-hand side of the resulting differential
equation to the integrable form
d
dx (xy).
Motivated by this example, we now consider the possibility of multiplying the general
linear differential equation
dy
dx + p(x)y = q(x)
(1.6.4)
by a nonzero function I (x), chosen in such a way that the left-hand side of the resulting
differential equation is
d
dx [I (x)y].
Henceforth we will assume that the functions p and q are continuous on (a, b). Multi-
plying the differential equation (1.6.4) by I (x) yields
I dy
dx + p(x)I y = Iq(x).
(1.6.5)

1.6
First-Order Linear Differential Equations 55
Furthermore, from the product rule for derivatives, we know that
d
dx (I y) = I dy
dx + dI
dx y.
(1.6.6)
Comparing Equations (1.6.5) and (1.6.6), we see that Equation (1.6.5) can indeed be
written in the integrable form
d
dx (I y) = Iq(x)
provided the function I (x) is a solution to9
I dy
dx + p(x)I y = I dy
dx + dI
dx y.
This will hold whenever I (x) satisﬁes the separable differential equation
dI
dx = p(x)I.
(1.6.7)
Separating the variables and integrating yields
ln |I| =
,
p(x) dx + c,
so that
I (x) = c1e
;
p(x) dx,
where c1 is an arbitrary constant. Since we only require one solution to Equation (1.6.7)
we set c1 = 1, in which case
I (x) = e
;
p(x) dx.
We can therefore draw the following conclusion.
Multiplying the linear differential equation
dy
dx + p(x)y = q(x)
(1.6.8)
by I (x) = e
;
p(x) dx reduces it to the integrable form
d
dx
1
e
;
p(x) dx y
2
= q(x)e
;
p(x) dx.
(1.6.9)
The general solution to (1.6.8) can now be obtained from (1.6.9) by integration.
Formally we have
y(x) = e−
;
p(x) dx
%,
q(x)e
;
p(x) dx dx + c
&
.
(1.6.10)
Remarks
1. The function I (x) = e
;
p(x) dx is called an integrating factor for the differential
equation (1.6.8) since it enables us to reduce the differential equation to a form
that is directly integrable.
9This is obtained by equating the left-hand side of Equation (1.6.5) to the right-hand side of Equation (1.6.6).

56
CHAPTER 1
First-Order Differential Equations
2. It is not necessary to memorize (1.6.10). In a speciﬁc problem, we ﬁrst evaluate
the integrating factor e
;
p(x) dx and then use (1.6.9).
Example 1.6.3
Solve the initial-value problem
dy
dx + xy = xex2/2,
y(0) = 1.
Solution:
An appropriate integrating factor in this case is
I (x) = e
;
x dx = ex2/2.
Multiplying the given differential equation by I and using (1.6.9) yields
d
dx (ex2/2y) = xex2.
Integrating both sides with respect to x, we obtain
ex2/2y = 1
2ex2 + c.
Hence,
y(x) = e−x2/2(1
2ex2 + c).
Imposing the initial condition y(0) = 1 yields
1 = 1
2 + c,
so that c = 1
2. Thus the required particular solution is
y(x) = 1
2e−x2/2(ex2 + 1) = 1
2(ex2/2 + e−x2/2) = cosh(x2/2).
□
Example 1.6.4
Solve x dy
dx −2y = 2x2 ln x, x > 0.
Solution:
We ﬁrst write the given differential equation in standard form. Dividing by
x yields
dy
dx −2x−1y = 2x ln x.
(1.6.11)
An integrating factor is
I (x) = e
;
(−2/x) dx = e−2 ln x = x−2.
Multiplying Equation (1.6.11) by I we obtain
d
dx (x−2y) = 2x−1 ln x.
Integrating and rearranging gives
y(x) = x2 1
(ln x)2 + c
2
.
□

1.6
First-Order Linear Differential Equations 57
Example 1.6.5
Solve the initial-value problem
y′ −y = f (x),
y(0) = 0,
where f (x) =
'
1, if x < 1,
2 −x,
if x ≥1.
Solution:
We have sketched f (x) in Figure 1.6.1. An integrating factor for the dif-
ferential equation is I (x) = e−x.
x
f(x)
1
1
2
Figure 1.6.1: A sketch of the function f (x) from Example 1.6.5.
Upon multiplication by the integrating factor, the differential equation reduces to
d
dx (e−x y) = e−x f (x).
We now integrate this differential equation over the interval [0, x]. To do so we need to
use a dummy integration variable, which we denote by w. We therefore obtain
1
e−wy(w)
2x
0 =
, x
0
e−w f (w) dw,
or equivalently,
e−x y(x) −y(0) =
, x
0
e−w f (w) dw.
Multiplying by ex and substituting for y(0) = 0 yields
y(x) = ex
, x
0
e−w f (w) dw.
(1.6.12)
Due to the form of f (x), the value of the integral on the right-hand side will depend on
whether x < 1 or x ≥1. If x < 1, then f (w) = 1 and so (1.6.12) can be written as
y(x) = ex
, x
0
e−w dw = ex(1 −e−x),
so that
y(x) = ex −1,
x < 1.
If x ≥1, then the interval of integration [0, x] must be split into two parts. From (1.6.12)
we have
y(x) = ex
%, 1
0
e−w dw +
, x
1
(2 −w)e−w
&
dw.

58
CHAPTER 1
First-Order Differential Equations
A straightforward integration leads to
y(x) = ex 3
1 −e−1 +
1
−2e−w + we−w + e−w2x
1
4
,
which simpliﬁes to
y(x) = ex(1 −e−1) + x −1.
The solution to the initial-value problem can therefore be written as
y(x) =
-
ex −1,
if x < 1,
ex(1 −e−1) + x −1, if x ≥1.
A sketch of the corresponding solution curve is given in Figure 1.6.2.
3
5
10
15
x
y
2
1
21
22
Figure 1.6.2: The solution curve for the initial-value problem in Example 1.6.5. The dashed
curve is the continuation of y(x) = ex −1 for x > 1.
Differentiating both branches of this function we ﬁnd
y′(x) =
-
ex,
if x < 1,
ex(1 −e−1) + 1, if x ≥1.
y′′(x) =
-
ex,
if x < 1,
ex(1 −e−1), if x ≥1.
We see that even though the function f in the original differential equation was not dif-
ferentiable at x = 1, the solution to the initial-value problem has a continuous derivative
at that point. The discontinuity in the derivative of the driving term does show up in the
second derivative of the solution as indeed it must.
□
Exercises for 1.6
Key Terms
First-order linear differential equations, Integrating factor.
Skills
• Be able to recognize a ﬁrst-order linear differential
equation.
• Be able to ﬁnd an integrating factor for a given ﬁrst-
order linear differential equation.
• Be able to solve a ﬁrst-order linear differential equa-
tion.
True-False Review
For items (a)–(e), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.

1.6
First-Order Linear Differential Equations 59
(a) There is a unique integrating factor for a differential
equation of the form y′ + p(x)y = q(x).
(b) An integrating factor for the differential equation
y′ + p(x)y = q(x) is e
;
p(x)dx.
(c) Upon multiplying the differential equation y′ +
p(x)y = q(x) by an integrating factor I (x), the dif-
ferential equation becomes (I (x) · y)′ = q(x)I.
(d) An integrating factor for the differential equation
dy
dx = x2y + sin x is I (x) = e
;
x2dx.
(e) An integrating factor for the differential equation
dy
dx = x −y
x is I (x) = x + 5.
Problems
For Problems 1–15, solve the given differential equation.
1. dy
dx + y = 4ex.
2. dy
dx + 2
x y = 5x2,
x > 0.
3. x2y′ −4xy = x7 sin x,
x > 0.
4. y′ + 2xy = 2x3.
5. dy
dx +
2x
1 −x2 y = 4x,
−1 < x < 1.
6. dy
dx +
2x
1 + x2 y =
4
(1 + x2)2 .
7. 2(cos2 x)y′ + y sin 2x = 4 cos4 x,
0 ≤x < π/2.
8. y′ +
1
x ln x y = 9x2.
9. y′ −y tan x = 8 sin3 x.
10. t dx
dt + 2x = 4et,
t > 0.
11. y′ = sin x(y sec x −2).
12. (1 −y sin x)dx −(cos x)dy = 0.
13. y′ −x−1y = 2x2 ln x.
14. y′ + αy = eβx, where α, β are constants.
15. y′ + mx−1y = ln x, where m is constant.
In Problems 16–21, solve the given initial-value problem.
16. y′ + 2x−1y = 4x,
y(1) = 2.
17. (sin x)y′ −y cos x = sin 2x,
y(π/2) = 2.
18. dx
dt +
2
4 −t x = 5, x(0) = 4.
19. (y −ex)dx + dy = 0, y(0) = 1.
20. y′ + y = f (x), y(0) = 3, where
f (x) =
' 1, if x ≤1,
0, if x > 1.
21. y′ −2y = f (x), y(0) = 1, where
f (x) =
' 1 −x, if x < 1,
0,
if x ≥1.
22. Solve the initial-value problem in Example 1.6.5 as
follows. First determine the general solution to the
differential equation on each interval separately. Then
use the given initial condition to ﬁnd the appropriate
integration constant for the interval (−∞, 1). To deter-
mine the integration constant on the interval [1, ∞),
use the fact that the solution must be continuous at
x = 1.
23. Find the general solution to the second-order differen-
tial equation
d2y
dx2 + 1
x
dy
dx = 9x,
x > 0.
[Hint: Let u = dy
dx .]
24. Solve the differential equation for Newton’s law of
cooling by viewing it as a ﬁrst-order linear differential
equation.
25. Suppose that an object is placed in a medium whose
temperature is increasing at a constant rate of α◦F
per minute. Show that, according to Newton’s law
of cooling, the temperature of the object at time t is
given by
T (t) = α(t −k−1) + c1 + c2e−kt,
where c1 and c2 are constants.
26. Between 8 a.m. and 12 p.m. on a hot summer day, the
temperature rose at a rate of 10◦F per hour from an
initial temperature of 65◦F. At 9 a.m., the temperature

60
CHAPTER 1
First-Order Differential Equations
of an object was measured to be 35◦F and was, at that
time, increasing at a rate of 5◦F per hour. Show that
the temperature of the object at time t was
T (t) = 10t −15 + 40e(1−t)/8,
0 ≤t ≤4.
27. It is known that a certain object has constant of pro-
portionality k = 1/40 in Newton’s law of cooling.
When the temperature of this object is 0◦F, it is placed
in a medium whose temperature is changing in time
according to
Tm(t) = 80e−t/20.
(a) Using Newton’s law of cooling, show that the
temperature of the object at time t is
T (t) = 80(e−t/40 −e−t/20).
(b) What happens to the temperature of the object as
t →+∞? Is this reasonable?
(c) Determine the time, tmax, when the temperature
of the object is a maximum. Find T (tmax) and
Tm(tmax).
(d) Make a sketch to depict the behavior of T (t) and
Tm(t).
28. The differential equation
dT
dt = −k1[T −Tm(t)] + A0,
(1.6.13)
where k1 and A0 are positive constants, can be used to
model the temperature variation T (t) in a building. In
this equation, the ﬁrst term on the right-hand side gives
the contribution due to the variation in the outside tem-
perature, and the second term on the right-hand side
gives the contribution due to the heating effect from
internal sources such as machinery, lighting, people,
etc. Consider the case when
Tm(t) = A −B cos ωt,
ω = π/12,
(1.6.14)
where A and B are constants, and t is measured in
hours.
(a) Make a sketch of Tm(t). Taking t = 0 to corre-
spond to midnight, describe the variation of the
external temperature over a 24-hour period.
(b) With Tm given in (1.6.14), solve (1.6.13) subject
to the initial condition T (0) = T0.
29. This
problem
demonstrates
the
variation-of-
parameters method for ﬁrst-order linear differential
equations. Consider the ﬁrst-order linear differential
equation
y′ + p(x)y = q(x).
(1.6.15)
(a) Show that the general solution to the associated
homogeneous equation
y′ + p(x)y = 0
is
yH(x) = c1e−
;
p(x)dx.
(b) Determine the function u(x) such that
y(x) = u(x)e−
;
p(x)dx
is a solution to (1.6.15), and hence derive the gen-
eral solution to (1.6.15).
For Problems 30–33, use the technique derived in the previ-
ous problem to solve the given differential equation.
30. y′ + x−1y = cos x,
x > 0.
31. y′ + y = e−2x.
32. y′ + y cot x = 2 cos x,
0 < x < π.
33. xy′ −y = x2 ln x.
For Problems 34–39, use a differential equations solver to
determine the solution to each of the initial-value problems
and sketch the corresponding solution curve.
34. ⋄The initial-value problem in Problem 16.
35. ⋄The initial-value problem in Problem 17.
36. ⋄The initial-value problem in Problem 18.
37. ⋄The initial-value problem in Problem 19.
38. ⋄The initial-value problem in Problem 20.
39. ⋄The initial-value problem in Problem 21.

1.7
Modeling Problems Using First-Order Linear Differential Equations 61
1.7
Modeling Problems Using First-Order Linear Differential Equations
There are many examples of applied problems whose mathematical formulation leads
to a ﬁrst-order linear differential equation. In this section we analyze two in detail.
Mixing Problems
Statement of the Problem: Consider the situation depicted in Figure 1.7.1. A tank initially
contains V0 liters of a solution in which is dissolved A0 grams of a certain chemical. A
solution containing c1 grams/liter of the same chemical ﬂows into the tank at a constant
rate of r1 liters/minute, and the mixture ﬂows out at a constant rate of r2 liters/minute. We
assume that the mixture is kept uniform by stirring. Then at any time t the concentration
of chemical in the tank, c2(t), is the same throughout the tank and is given by
c2 = A(t)
V (t),
(1.7.1)
where V (t) denotes the volume of solution in the tank at time t and A(t) denotes the
amount of chemical in the tank at time t.
Solution of concentration c1 grams/liter
flows in at a rate of r1 liters/minute
A(t) 5 amount of chemical in the tank at time t
V(t) 5 volume of solution in the tank at time t
c2(t) 5 A(t)/V(t) 5 concentration of chemical in the tank at time t
Solution of concentration
c2 grams/liter flows out at
a rate of r2 liters/minute
Figure 1.7.1: A mixing problem.
Mathematical Formulation: The two functions in the problem are V (t) and A(t). In
order to determine how they change with time, we ﬁrst consider their change during a
short time interval, )t minutes. In time )t, r1 )t liters of solution ﬂow into the tank,
whereas r2 )t liters ﬂow out. Thus during the time interval )t, the change in the volume
of solution in the tank is
)V = r1 )t −r2 )t = (r1 −r2) )t.
(1.7.2)
Since the concentration of chemical in the inﬂow is c1 grams/liter (assumed constant),
it follows that in the time interval )t the amount of chemical that ﬂows into the tank is
c1r1 )t. Similarly, the amount of chemical that ﬂows out in this same time interval is
approximately10 c2r2 )t. Thus, the total change in the amount of chemical in the tank
during the time interval )t, denoted by )A, is approximately
)A ≈c1r1 )t −c2r2 )t = (c1r1 −c2r2) )t.
(1.7.3)
Dividing Equations (1.7.2) and (1.7.3) by )t yields
)V
)t = r1 −r2
and
)A
)t ≈c1r1 −c2r2,
10This is only an approximation, since c2 is not constant over the time interval )t. The approximation will
become more accurate as )t →0.

62
CHAPTER 1
First-Order Differential Equations
respectively. These equations describe the rates of change of V and A over the short, but
ﬁnite, time interval )t. In order to determine the instantaneous rates of change of V and
A, we take the limit as )t →0 to obtain
dV
dt = r1 −r2
(1.7.4)
and
d A
dt = c1r1 −A
V r2,
(1.7.5)
where we have substituted for c2 from Equation (1.7.1). Since r1 and r2 are constants,
we can integrate Equation (1.7.4) directly, to obtain
V (t) = (r1 −r2)t + V0,
where V0 is an integration constant. Substituting for V into Equation (1.7.5) and rear-
ranging terms yields the linear equation for A(t)
d A
dt +
r2
(r1 −r2)t + V0
A = c1r1.
(1.7.6)
This differential equation can be solved, subject to the initial condition A(0) = A0, to
determine the behavior of A(t).
Remark
The reader need not memorize Equation (1.7.6), since it is better to derive
it for each speciﬁc example.
Example 1.7.1
A tank contains 8 L (liters) of water in which is dissolved 32 g (grams) of chemical. A
solution containing 2 g/L of the chemical ﬂows into the tank at a rate of 4 L/min, and
the well-stirred mixture ﬂows out at a rate of 2 L/min.
1. Determine the amount of chemical in the tank after 20 minutes.
2. What is the concentration of chemical in the tank at that time?
Solution:
We are given
r1 = 4 L/min,r2 = 2 L/min, c1 = 2 g/L, V (0) = 8 L, and A(0) = 32 g.
For parts (1) and (2), we must ﬁnd A(20) and A(20)/V (20), respectively. Now,
)V = r1 )t −r2 )t
implies that
dV
dt = 2.
Integrating this equation and imposing the initial condition that V (0) = 8 yields
V (t) = 2(t + 4).
(1.7.7)
Further,
)A ≈c1r1 )t −c2r2 )t
implies that
d A
dt = 8 −2c2.

1.7
Modeling Problems Using First-Order Linear Differential Equations 63
That is, since c2 = A/V ,
d A
dt = 8 −2 A
V .
Substituting for V from (1.7.7), we must solve
d A
dt +
1
t + 4 A = 8.
(1.7.8)
This ﬁrst-order linear equation has integrating factor
I = e
;
1/(t+4) dt = t + 4.
Consequently (1.7.8) can be written in the equivalent form
d
dt [(t + 4)A] = 8(t + 4),
which can be integrated directly to obtain
(t + 4)A = 4(t + 4)2 + c.
Hence
A(t) =
1
t + 4[4(t + 4)2 + c].
Imposing the given initial condition A(0) = 32 g implies that c = 64. Consequently
A(t) =
4
t + 4[(t + 4)2 + 16].
Setting t = 20 gives us the answer for (1) and (2):
1. We have
A(20) = 1
6[(24)2 + 16] = 296
3
g.
2. Furthermore, using (1.7.7),
A(20)
V (20) = 1
48 · 296
3
= 37
18 g/L.
□
Electric Circuits
An important application of differential equations arises from the analysis of simple
electric circuits. The most basic electric circuit is obtained by connecting the ends of
a wire to the terminals of a battery or generator. This causes a ﬂow of charge, q(t),
measured in coulombs (C), through the wire, thereby producing a current, i(t), measured
in amperes (A), deﬁned to be the rate of change of charge. Thus,
i(t) = dq
dt .
(1.7.9)
In practice a circuit will contain several components that oppose the ﬂow of charge. As
current passes through these components work has to be done, and so there is a loss of
energy, which is described by the resulting voltage drop across each component. For the
circuits that we will consider, the behavior of the current in the circuit is governed by
Kirchhoff’s second law, which can be stated as follows.

64
CHAPTER 1
First-Order Differential Equations
Kirchhoff’s Second Law: The sum of the voltage drops around a closed circuit is zero.
In order to apply this law we need to know the relationship between the current
passing through each component in the circuit and the resulting voltage drop. The com-
ponents of interest to us are resistors, capacitors, and inductors. We brieﬂy describe each
of these next.
1. Resistors: As its name suggests, a resistor is a component that, due to its con-
stituency, directly resists the ﬂow of charge through it. According to Ohm’s law,
the voltage drop, )VR, between the ends of a resistor is directly proportional to
the current that is passing through it. This is expressed mathematically as
)VR = i R,
(1.7.10)
where the constant of proportionality, R, is called the resistance of the resistor.
The units of resistance are ohms (*).
2. Capacitors: A capacitor can be thought of as a component that stores charge and
thereby opposes the passage of current. If q(t) denotes the charge on the capacitor
at time t, then the drop in voltage, )VC, as current passes through it is directly
proportional to q(t). It is usual to express this law in the form
)VC = 1
C q,
(1.7.11)
where the constant C is called the capacitance of the capacitor. The units of
capacitance are farads (F).
3. Inductors: The third component that is of interest to us is an inductor. This can be
considered as a component that opposes any change in the current ﬂowing through
it. The drop in voltage as current passes through an inductor is directly proportional
to the rate at which the current is changing. We write this as
)VL = L di
dt ,
(1.7.12)
where the constant L is called the inductance of the inductor, measured in units
of henrys (H).
4. EMF: The ﬁnal component in our circuits will be a source of voltage that produces
an electromotive force (EMF). We can think of this as providing the force that
drives the charge through the circuit. As current passes through the voltage source,
there is a voltage gain, which we denote by E(t) volts (that is, a voltage drop of
−E(t) volts).
A circuit containing all of these components is shown in Figure 1.7.2. Such a circuit
is called an RLC circuit. According to Kirchhoff’s second law, the sum of the voltage
Resistance, R
Switch
Capacitance, C
Inductance, L
E(t)
i(t)
Figure 1.7.2: A simple RLC circuit.

1.7
Modeling Problems Using First-Order Linear Differential Equations 65
drops at any instant must be zero. Applying this to the RLC circuit in Figure 1.7.2 we
obtain
)VR + )VC + )VL −E(t) = 0.
(1.7.13)
Substituting into Equation (1.7.13) from (1.7.10)–(1.7.12) and rearranging yields the
basic differential equation for an RLC circuit; namely,
L di
dt + Ri + q
C = E(t).
(1.7.14)
There are three cases that are of importance in applications, two of which are gov-
erned by ﬁrst-order linear differential equations.
Case 1: An RL CIRCUIT. In the case when there is no capacitor present, we have
what is referred to as an RL circuit. The differential equation (1.7.14) then reduces to
di
dt + R
L i = 1
L E(t).
(1.7.15)
This is a ﬁrst-order linear differential equation for the current in the circuit at any time t.
Case 2: An RC CIRCUIT. Now consider the case when there is no inductor present
in the circuit. Setting L = 0 in Equation (1.7.14) yields
i +
1
RC q = E
R .
In this equation we have two unknowns, q(t) and i(t). Substituting from (1.7.9) for
i(t) = dq/dt we obtain the following differential equation for q(t):
dq
dt +
1
RC q = E
R .
(1.7.16)
In this case, the ﬁrst-order linear differential equation (1.7.16) can be solved for the
charge q(t) on the plates of the capacitor. The current in the circuit can then be obtained
from
i(t) = dq
dt
by differentiation.
Case3:AnRLCCIRCUIT. Inthegeneralcase,wemustconsiderallthreecomponents
to be present in the circuit. Substituting from Equation (1.7.9) into Equation (1.7.14)
yields the following differential equation for determining the charge on the capacitor:
d2q
dt2 + R
L
dq
dt +
1
LC q = 1
L E(t).
We will develop techniques in Chapter 8 that enable us to solve this differential equation
without difﬁculty.
For the remainder of this section we restrict our attention to RL and RC circuits.
Since these are both ﬁrst-order linear differential equations, we can solve them using the
technique derived in the previous section once the applied EMF, E(t), has been speciﬁed.
The two most important forms for E(t) are
E(t) = E0
and
E(t) = E0 cos ωt,

66
CHAPTER 1
First-Order Differential Equations
where E0 and ω are constants. The ﬁrst of these corresponds to a source of EMF such
as a battery. The resulting current is called a direct current (DC). The second form of
EMF oscillates between ±E0 and is called an alternating current (AC).
Example 1.7.2
Determine the current in an RL circuit if the applied EMF is E(t) = E0 cos ωt, where
E0 and ω are constants, and the initial current is zero.
Solution:
Substituting into Equation (1.7.15) for E(t) yields the differential equation
di
dt + R
L i = E0
L cos ωt,
which we write as
di
dt + ai = E0
L cos ωt,
(1.7.17)
where a = R
L . An integrating factor for (1.7.17) is I (t) = eat, so that the equation can
be written in the equivalent form
d
dt (eati) = E0
L eat cos ωt.
Integrating this equation using the standard integral
,
eat cos ωt dt =
1
a2 + ω2 eat(a cos ωt + ω sin ωt) + c,
we obtain
eati =
E0
L(a2 + ω2)eat(a cos ωt + ω sin ωt) + c,
where c is an integration constant. Consequently,
i(t) =
E0
L(a2 + ω2)(a cos ωt + ω sin ωt) + ce−at.
Imposing the initial condition i(0) = 0, we ﬁnd
c = −
E0a
L(a2 + ω2),
so that
i(t) =
E0
L(a2 + ω2)(a cos ωt + ω sin ωt −ae−at).
(1.7.18)
This solution can be written in the form
i(t) = iS(t) + iT (t),
where
iS(t) =
E0
L(a2 + ω2)(a cos ωt + ω sin ωt),
iT (t) = −
aE0
L(a2 + ω2)e−at.

1.7
Modeling Problems Using First-Order Linear Differential Equations 67
(a2 1 v2)1/2
f
a
v
Figure 1.7.3: Deﬁning the
phase angle for an RL circuit.
The term iT (t) decays exponentially with time and is referred to as the transient part
of the solution. As t →∞the solution (1.7.18) approaches the steady-state solution,
iS(t). The steady-state solution can be written in a more illuminating form as follows.
If we construct the right-angled triangle (see Figure 1.7.3) with sides a and ω, then the
hypotenuse of the triangle is
√
a2 + ω2. Consequently, there exists a unique angle φ in
(0, π/2), such that
cos φ =
a
√
a2 + ω2 ,
sin φ =
ω
√
a2 + ω2 .
Equivalently,
a =
*
a2 + ω2 cos φ,
ω =
*
a2 + ω2 sin φ.
Substituting for a and ω into the expression for iS yields
iS(t) =
E0
L
√
a2 + ω2 (cos ωt cos φ + sin ωt sin φ),
which can be written, using an appropriate trigonometric identity, as
iS(t) =
E0
L
√
a2 + ω2 cos(ωt −φ).
This is referred to as the phase-amplitude form of the solution. Comparing this with the
original driving term, E0 cos ωt, we see that the system has responded with a steady-
state solution having the same periodic behavior, but with a phase shift of φ radians.
Furthermore the amplitude of the response is
A =
E0
L
√
a2 + ω2 =
E0
√
R2 + ω2L2 ,
(1.7.19)
where we have substituted for a = R/L. This is illustrated in Figure 1.7.4. The general
picture that we have, therefore, is that the transient part of the solution affects i(t) for a
short period of time, after which the current settles into a steady state. In the case when
the driving EMF has the form E(t) = E0 cos ωt, the steady state is a phase shift of
this driving EMF with an amplitude given in Equation (1.7.19). This general behavior is
illustrated in Figure 1.7.5.
iS(t), E(t)
t
iS(t) 5 A cos (vt — f)
E(t) 5 E0 cos vt
Figure 1.7.4: The response of an RL circuit to the driving term E(t) = E0 cos ωt.

68
CHAPTER 1
First-Order Differential Equations
iS(t)
i(t) 5 iS(t) 1 iT(t)
i(t), iS(t)
t
Figure 1.7.5: The transient part of the solution for an RL circuit dies out as t increases.
□
Our next example illustrates the procedure for solving the differential equation
(1.7.16) governing the behavior of an RC circuit.
Example 1.7.3
Consider the RC circuit in which R = 0.5 *, C = 0.1 F, and E0 = 20 V. Given that the
capacitor has zero initial charge, determine the current in the circuit after 0.25 seconds.
Solution:
In this case we ﬁrst solve Equation (1.7.16) for q(t) and then determine
the current in the circuit by differentiating the result. Substituting for R, C, and E into
Equation (1.7.16) yields
dq
dt + 20q = 40,
which has general solution
q(t) = 2 + ce−20t,
where c is an integration constant. Imposing the initial condition q(0) = 0 yields c = −2,
so that
q(t) = 2(1 −e−20t).
Differentiating this expression for q gives the current in the circuit
i(t) = dq
dt = 40e−20t.
Consequently,
i(0.25) = 40e−5 ≈0.27 A.
□
Exercises for 1.7
Key Terms
Mixing problem, Concentration, Electric circuit, Kirchhoff’s
Second Law, Resistor, Capacitor, Inductor, Electromotive
force (EMF), RL Circuit, RC Circuit, RLC Circuit, Direct
current, Alternating current, Transient solution, Steady-state
solution, Phase, Amplitude.
Skills
• Be able to use information about a mixing problem to
provide the correct mathematical formulation of the
problem.

1.7
Modeling Problems Using First-Order Linear Differential Equations 69
• Beabletosolvemixingproblemsbyderivingandsolv-
ing the differential equation (1.7.6) for a speciﬁc mix-
ing problem and using initial conditions.
• Know the relationship between the charge and the cur-
rent in an electric circuit.
• Be familiar with the basic components of an electric
circuit, such as electromotive force, resistors, capaci-
tors, and inductors.
• Be able to write down and solve the differential equa-
tion for the current in an RL circuit and for the charge
in an RC circuit, for either a direct current or an alter-
nating current.
• Be able to identify the transient and steady-state com-
ponents of current in an electric circuit with an alter-
nating current.
• Be able to put the steady-state component of the cur-
rent in an RL circuit in phase-amplitude form, and
identify the phase shift and the amplitude.
True-False Review
For items (a)–(h), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The amount of chemical A(t) in a tank at time t is ob-
tained by multiplying the concentration of chemical
c(t) in the tank at time t by the volume of the solution,
V (t), at time t.
(b) If r1 and r2 denote the rates at which ﬂuid is ﬂowing
into a tank and out of the tank, respectively, then the
rate of change of the volume of the tank is r2 −r1.
(c) For the mixing problems described in this section, we
assume that the concentration of the chemical entering
the tank is independent of time.
(d) For the mixing problems described in this section, we
assume that the concentration of the chemical leaving
the tank is independent of time.
(e) Kirchhoff’s second law states the sum of the voltage
drops around a closed circuit is independent of time.
(f) The larger the resistance in a resistor, the greater the
voltage drop between the ends of the resistor.
(g) Given an alternating current in an RL circuit, the tran-
sient part of the current decays to zero with time, while
the steady-state part of the current oscillates with the
same frequency as the applied EMF.
(h) The higher the frequency of an applied EMF in an
RL circuit, the lower the amplitude of the steady-state
current.
Problems
1. A tank initially contains 600 L of solution in which
there is dissolved 1500 grams of chemical. A solution
containing 5 g/L of the chemical ﬂows into the tank at
a rate of 6 L/min, and the well-stirred mixture ﬂows
out at a rate of 3 L/min. Determine the concentration
of chemical in the tank after one hour.
2. A container initially contains 10 L of water in which
there is 20 g of salt dissolved. A solution containing
4 g/L of salt is pumped into the container at a rate
of 2 L/min, and the well-stirred mixture runs out at
a rate of 1 L/min. How much salt is in the tank after
40 min?
3. A tank whose volume is 200 L is initially half full of
a solution that contains 100 g of chemical. A solution
containing 0.5 g/L of the same chemical ﬂows into the
tank at a rate of 6 L/min, and the well-stirred mixture
ﬂows out at a rate of 4 L/min. Determine the concen-
tration of chemical in the tank just before the solution
overﬂows.
4. A tank whose volume is 40 L initially contains 20
L of water. A solution containing 10 g/L of salt is
pumped into the tank at a rate of 4 L/min, and the
well-stirred mixture ﬂows out at a rate of 2 L/min.
How much salt is in the tank just before the solution
overﬂows?
5. A tank initially contains 20 L of water. A solution
containing 1 g/L of chemical ﬂows into the tank at a
rate of 3 L/min, and the mixture ﬂows out at a rate of
2 L/min.
(a) Set up and solve the initial-value problem for
A(t), the amount of chemical in the tank at
time t.
(b) When does the concentration of chemical in the
tank reach 0.5 g/L?
6. A tank initially contains 10 L of a salt solution. Water
ﬂows into the tank at a rate of 3 L/min, and the
well-stirred mixture ﬂows out at a rate of 2 L/min.

70
CHAPTER 1
First-Order Differential Equations
After 5 minutes, the concentration of salt in the tank
is 0.2 g/L. Find:
(a) The amount of salt in the tank initially.
(b) The volume of solution in the tank when the con-
centration of salt is 0.1 g/L.
7. A tank initially contains w liters of a solution in which
is dissolved A0 grams of chemical. A solution contain-
ing k g/L of this chemical ﬂows into the tank at a rate
of r L/min, and the mixture ﬂows out at the same rate.
(a) Show that the amount of chemical, A(t), in the
tank at time t is
A(t) = e−(rt)/w[kw(e(rt)/w −1) + A0].
(b) Show that as t →∞, the concentration of chem-
ical in the tank approaches k g/L. Is this result
reasonable? Explain.
8. Consider the double mixing problem depicted in
Figure 1.7.6.
r1, c1
r3, c3
r2, c2
A1
A2
Figure 1.7.6: Double mixing problem.
(a) Show that the following are differential equations
for A1(t) and A2(t):
d A1
dt
+
r2
(r1 −r2)t + V1
A1 = c1r1,
d A2
dt
+
r3
(r2 −r3)t + V2
A2 =
r2 A1
(r1 −r2)t + V1
,
where V1 and V2 are constants.
(b) Let r1 = 6 L/min, r2 = 4 L/min, r3 = 3 L/min,
and c1 = 0.5 g/L. If the ﬁrst tank initially holds
40 L of water in which 4 g of chemical is
dissolved, whereas the second tank initially con-
tains 20 g of chemical dissolved in 20 L of water,
determine the amount of chemical in the second
tank after 10 min.
9. Consider the RL circuit in which R = 4 *, L = 0.1
H, and E(t) = 20 V. If there is no current ﬂowing
initially, determine the current in the circuit for t ≥0.
10. Consider the RC circuit which has R = 5 *, C = 1
50
F, and E(t) = 100 V. If the capacitor is uncharged
initially, determine the current in the circuit for t ≥0.
11. An RL circuit has EMF E(t) = 10 sin 4t V. If R =
2 *, L = 2
3 H, and there is no current ﬂowing initially,
determine the current for t ≥0.
12. Consider the RC circuit with R = 2 *, C = 1
8 F,
and E(t) = 10 cos 3t V. If q(0) = 1 C, determine the
current in the circuit for t ≥0.
13. Consider the general RC circuit with E(t) = 0. Sup-
pose that q(0) = 5 C. Determine the charge on the
capacitor for t > 0. What happens as t →∞? Is this
reasonable? Explain.
14. Determine the current in an RC circuit if the capac-
itor has zero charge initially and the driving EMF is
E = E0, where E0 is a constant. Make a sketch show-
ing the change in the charge q(t) on the capacitor with
time and show that q(t) approaches a constant value
as t →∞. What happens to the current in the circuit
as t →∞?
15. Determine the current ﬂowing in an RL circuit if the
applied EMF is E(t) = E0 sin ωt, where E0 and ω are
constants. Identify the transient part of the solution and
the steady-state solution.
16. Determine the current ﬂowing in an RL circuit if the
applied EMF is constant and the initial current is zero.
17. Determine the current ﬂowing in an RC circuit if the
capacitor is initially uncharged and the driving EMF
is given by E(t) = E0e−at, where E0 and a are con-
stants.
18. Consider the special case of the RLC circuit in which
the resistance is negligible and the driving EMF is
zero. The differential equation governing the charge
on the capacitor in this case is
d2q
dt2 +
1
LC q = 0.
If the capacitor has an initial charge of q0 coulombs,
and there is no current ﬂowing initially, determine the

1.8
Change of Variables 71
charge on the capacitor for t > 0, and the correspond-
ing current in the circuit. [Hint: Let u = dq
dt and
use the chain rule to show that this implies du
dt
=
u
!du
dq
"
.]
19. Repeat the previous problem for the case in which the
driving EMF is E(t) = E0, a constant.
1.8
Change of Variables
So far we have introduced techniques for solving separable and ﬁrst-order linear dif-
ferential equations. Clearly, most ﬁrst-order differential equations are not of these two
types. In this section, we consider two further types of differential equations that can
be solved by using a change of variables to reduce them to one of the types we know
how to solve. The key point to grasp in this section, however, is not the speciﬁc changes
of variables that we discuss, but the general idea of changing variables in a differential
equation. Further examples are considered in the exercises. We ﬁrst require a preliminary
deﬁnition.
DEFINITION
1.8.1
A function f (x, y) is said to be homogeneous of degree zero11 if
f (tx, ty) = f (x, y)
for all positive values of t for which (tx, ty) is in the domain of f .
Remark
Equivalently, we can say that f is homogeneous of degree zero if it is
invariant under a re-scaling of the variables x and y.
Thesimplestnonconstantfunctionsthatarehomogeneousofdegreezeroare f (x, y) =
y
x , and f (x, y) = x
y .
Example 1.8.2
If f (x, y) = x2 + 3xy −y2
x2 + 4y2
, then
f (tx, ty) = (tx)2 + 3(tx)(ty) −(ty)2
(tx)2 + 4(ty)2
= t2(x2 + 3xy −y2)
t2(x2 + 4y2)
= f (x, y),
so that f is homogeneous of degree zero.
□
Inthepreviousexample,ifwefactoran x2 termfromthenumeratoranddenominator,
then the function f can be written in the form
f (x, y) = x2 <
1 + 3(y/x) −(y/x)2=
x2 <
1 + 4(y/x)2=
.
That is,
f (x, y) = 1 + 3(y/x) −(y/x)2
1 + 4(y/x)2
.
11More generally, f (x, y) is said to be homogeneous of degree m if f (tx, ty) = tm f (x, y).

72
CHAPTER 1
First-Order Differential Equations
Thus f can be considered to depend on the single variable V = y/x. The following
theorem establishes that this is a basic property of all functions that are homogeneous of
degree zero.
Theorem 1.8.3
A function f (x, y) is homogeneous of degree zero if and only if it depends on y/x only.
Proof Suppose that f is homogeneous of degree zero. We must consider two cases
separately.
Case (a): If x > 0, we can take t = 1/x in Deﬁnition 1.8.1 to obtain
f (x, y) = f (1, y/x),
which is a function of V = y/x only.
Case (b): If x < 0, then we can take t = −1/x in Deﬁnition 1.8.1. In this case we obtain
f (x, y) = f (−1, −y/x),
which once more depends on y/x only.
Conversely, suppose that f (x, y) depends only on y/x. If we replace x by
tx and y by ty then f is unaltered, since y/x = (ty)/(tx), and hence is
homogeneous of degree zero.
Remark
Do not memorize the formulas in the preceding theorem. Just remember that
a function f (x, y) that is homogeneous of degree zero depends only on the combination
y/x and hence can be considered as a function of a single variable, say, V , where
V = y/x.
We now consider solving differential equations that satisfy the following deﬁnition.
DEFINITION
1.8.4
If f (x, y) is homogeneous of degree zero, then the differential equation
dy
dx = f (x, y)
is called a homogeneous ﬁrst-order differential equation.
In general, if
dy
dx = f (x, y)
is a homogeneous ﬁrst-order differential equation, then we cannot solve it directly. How-
ever, our preceding discussion implies that such a differential equation can be written in
the equivalent form
dy
dx = F(y/x)
(1.8.1)
for an appropriate function F. This suggests that instead of using the variables x and y,
we should use the variables x and V , where V = y/x, or equivalently,
y = xV (x).
(1.8.2)

1.8
Change of Variables 73
Substitution of (1.8.2) into the right-hand side of Equation (1.8.1) has the effect of
reducing it to a function of V only. We must also determine how the derivative term
dy
dx transforms. Differentiating (1.8.2) with respect to x using the product rule yields the
following relationship between dy
dx and dV
dx :
dy
dx = x dV
dx + V.
Substituting into Equation (1.8.1), we therefore obtain
x dV
dx + V = F(V ),
or, equivalently,
x dV
dx = F(V ) −V.
The variables can now be separated to yield
1
F(V ) −V dV = 1
x dx,
which can be solved directly by integration. We have therefore established the next
theorem.
Theorem 1.8.5
The change of variables y = xV (x) reduces a homogeneous ﬁrst-order differential
equation dy/dx = f (x, y) to the separable equation
1
F(V ) −V dV = 1
x dx.
Remark
The separable equation that results in the previous technique can be inte-
grated to obtain a relationship between V and x. We then obtain the solution to the given
differential equation by substituting y/x for V in this relationship.
Example 1.8.6
Find the general solution to
dy
dx = 4x + y
x −4y .
(1.8.3)
Solution:
The function on the right-hand side of Equation (1.8.3) is homogeneous of
degree zero, so that we have a ﬁrst-order homogeneous differential equation. Substituting
y = xV into the equation yields
d
dx (xV ) = 4 + V
1 −4V .
That is,
x dV
dx + V = 4 + V
1 −4V ,
or equivalently,
x dV
dx = 4(1 + V 2)
1 −4V
.

74
CHAPTER 1
First-Order Differential Equations
Separating the variables gives
1 −4V
4(1 + V 2) dV = 1
x dx.
We write this as
%
1
4(1 + V 2) −
V
1 + V 2
&
dV = 1
x dx,
which can be integrated directly to obtain
1
4 arctan V −1
2 ln(1 + V 2) = ln |x| + c.
Substituting V = y/x and multiplying through by 2 yields
1
2 arctan(y/x) −ln
1
(x2 + y2)/x22
= ln(x2) + c1,
which simpliﬁes to
1
2 arctan(y/x) −ln(x2 + y2) = c1.
(1.8.4)
Although this technically gives the answer, the solution is more easily expressed in terms
of polar coordinates:
x = r cos θ
and
y = r sin θ
⇐⇒
r =
)
x2 + y2
and
θ = arctan(y/x).
Substituting into Equation (1.8.4) yields
1
2θ −ln(r2) = c1
or equivalently,
ln r = 1
4θ + c2.
Exponentiating both sides of this equation gives
r = c3eθ/4.
For each value of c3, this is the equation of a logarithmic spiral. The particular spiral
with equation r = 1
2eθ/4 is shown in Figure 1.8.1.
□
28
26
24
22
2
24
22
x
y
4
2
Figure 1.8.1: Graph of the logarithmic spiral with polar equation r = 1
2eθ/4,
−5π
6 ≤θ ≤22π
6 .

1.8
Change of Variables 75
Example 1.8.7
Find the equation of the orthogonal trajectories to the family
x2 + y2 −2cx = 0.
(1.8.5)
(Completing the square in x, we obtain (x −c)2 + y2 = c2, which represents the family
of circles centered at (c, 0), with radius c.)
Solution:
First we need an expression for the slope of the given family at the point
(x, y). Differentiating Equation (1.8.5) implicitly with respect to x yields
2x + 2y dy
dx −2c = 0,
which simpliﬁes to
dy
dx = c −x
y
.
(1.8.6)
This is not the differential equation of the given family, since it still contains the constant
c, and hence is dependent on the individual curves in the family. Therefore, we must
eliminate c to obtain an expression for the slope of the family that is independent of any
particular curve in the family. From Equation (1.8.5) we have
c = x2 + y2
2x
.
Substituting this expression for c into Equation (1.8.6) and simplifying gives
dy
dx = y2 −x2
2xy
.
Therefore, the differential equation for the family of orthogonal trajectories is
dy
dx = −
2xy
y2 −x2 .
(1.8.7)
This differential equation is ﬁrst-order homogeneous. Substituting y = xV (x) into
Equation (1.8.7) yields
d
dx (xV ) =
2V
1 −V 2 ,
so that
x dV
dx + V =
2V
1 −V 2 .
Hence
x dV
dx = V + V 3
1 −V 2 ,
or in separated form,
1 −V 2
V (1 + V 2) dV = 1
x dx.
Decomposing the left-hand side into partial fractions yields
! 1
V −
2V
1 + V 2
"
dV = 1
x dx,

76
CHAPTER 1
First-Order Differential Equations
which can be integrated directly to obtain
ln |V | −ln(1 + V 2) = ln |x| + c,
or equivalently,
ln
!
|V |
1 + V 2
"
= ln |x| + c.
Exponentiating both sides and redeﬁning the constant yields
V
1 + V 2 = c1x.
Substituting back for V = y/x, we obtain
xy
x2 + y2 = c1x.
That is,
x2 + y2 = c2y,
where c2 = 1/c1. Completing the square in y yields
x2 + (y −k)2 = k2,
(1.8.8)
where k = c2/2. Equation (1.8.8) is the equation of the family of orthogonal trajectories.
This is the family of circles centered at (0, k) with radius k (circles along the y-axis).
(See Figure 1.8.2.)
□
y
x
x2 1 (y 2 k)2 5 k2
(x 2 c)2 1 y2 5 c2
Figure 1.8.2: The family (x −c)2 + y2 = c2 and its orthogonal trajectories
x2 + (y −k)2 = k2.
Bernoulli Equations
We now consider a special type of nonlinear differential equation that can be reduced to
a linear equation by a change of variables.
DEFINITION
1.8.8
A differential equation that can be written in the form
dy
dx + p(x)y = q(x)yn,
(1.8.9)
where n is a real constant, is called a Bernoulli equation.

1.8
Change of Variables 77
If n = 0 or n = 1, Equation (1.8.9) is linear, but otherwise it is nonlinear. We can
reduce it to a linear equation as follows. We ﬁrst divide Equation (1.8.9) by yn to obtain
y−n dy
dx + y1−n p(x) = q(x).
(1.8.10)
We now make the change of variables
u(x) = y1−n,
(1.8.11)
which implies that
du
dx = (1 −n)y−n dy
dx .
That is,
y−n dy
dx =
1
1 −n
du
dx .
Substituting into Equation (1.8.10) for y1−n and y−n dy
dx yields the linear differential
equation
1
1 −n
du
dx + p(x)u = q(x),
or in standard form,
du
dx + (1 −n)p(x)u = (1 −n)q(x).
(1.8.12)
The linear equation (1.8.12) can now be solved for u as a function of x. The solution to
the original equation is then obtained from (1.8.11).
Example 1.8.9
Solve
dy
dx + 3
x y = 27y1/3 ln x,
x > 0.
Solution:
The differential equation is a Bernoulli equation, with n = 1/3. Dividing
both sides of the differential equation by y1/3 yields
y−1/3 dy
dx + 3
x y2/3 = 27 ln x.
(1.8.13)
We make the change of variables
u = y2/3,
(1.8.14)
which implies that
du
dx = 2
3 y−1/3 dy
dx .
Substituting into Equation (1.8.13) yields
3
2
du
dx + 3
x u = 27 ln x
or, in standard form,
du
dx + 2
x u = 18 ln x.
(1.8.15)
An integrating factor for this linear equation is
I (x) = e
;
(2/x) dx = e2 ln x = x2,

78
CHAPTER 1
First-Order Differential Equations
so that Equation (1.8.15) can be written as
d
dx (x2u) = 18x2 ln x.
Integrating, we obtain
x2u = 18
!1
3x3 ln x −1
9x3
"
+ c,
so that
u(x) = 2x (3 ln x −1) + cx−2,
and so, from (1.8.14), the solution to the original differential equation is
y2/3 = 2x (3 ln x −1) + cx−2.
□
Exercises for 1.8
Key Terms
Homogeneous of degree zero, Homogeneous ﬁrst-order dif-
ferential equations, Bernoulli equation.
Skills
• Be able to recognize whether or not a function f (x, y)
is homogeneous of degree zero, and whether or not
a given differential equation is a homogeneous ﬁrst-
order differential equation.
• Know how to change the variables in a homogeneous
ﬁrst-order differential equation in order to get a dif-
ferential equation that is separable and thus can be
solved.
• Be able to recognize whether or not a given ﬁrst-order
differential equation is a Bernoulli equation.
• Know how to change the variables in a Bernoulli equa-
tion in order to get a differential equation that is ﬁrst-
order linear and thus can be solved.
• Be able to make other changes of variables to differ-
ential equations in order to turn them into differential
equations that can be solved by methods from earlier
in this chapter.
True-False Review
For items (a)–(i), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The function f (x, y) = 3y2 −5xy
2xy + y2 is homogeneous
of degree zero.
(b) The function f (x, y) =
y2 + x
x2 + 2y2 is homogeneous of
degree zero.
(c) The differential equation dy
dx = x3 + xy2
y3 + 1
is a ﬁrst-
order homogeneous differential equation.
(d) The differential equation dy
dx =
x4y−2
x2 + y2 is a ﬁrst-
order homogeneous differential equation.
(e) The change of variables y = xV (x) always turns
a ﬁrst-order homogeneous differential equation into
a separable differential equation for V as a function
of x.
(f) The change of variables u = y−n always turns a
Bernoulli differential equation into a ﬁrst-order linear
differential equation for u as a function of x.
(g) The differential equation dy
dx = √x y + √xy is a
Bernoulli differential equation.
(h) The differential equation dy
dx −exy y = 5x√y is a
Bernoulli differential equation.
(i) The differential equation y dy
dx + xy2 = x2y5/3 is a
Bernoulli differential equation.

1.8
Change of Variables 79
Problems
For Problems 1–8, determine whether the given function is
homogeneous of degree zero. Rewrite those that are as func-
tions of the single variable V = y/x.
1. f (x, y) = 5x + 2y
9x −4y .
2. f (x, y) = 2x −5y.
3. f (x, y) = x sin(x/y) −y cos(y/x)
y
.
4. f (x, y) =
*
3x2 + 5y2
2x + 5y
,
x > 0.
5. f (x, y) = x + 7
2y .
6. f (x, y) = x −2
2y
+ 5y + 3
3y
.
7. f (x, y) =
*
x2 + y2
x
,
x < 0.
8. f (x, y) =
*
x2 + 4y2 −x + y
x + 3y
,
x, y ̸= 0.
For Problems 9–23, solve the given differential equation.
9. dy
dx = y2 + xy + x2
x2
.
10. (3x −2y)dy
dx = 3y.
11. y′ = (x + y)2
2x2
.
12. sin
# y
x
$
(xy′ −y) = x cos
# y
x
$
.
13. xy′ =
*
16x2 −y2 + y,
x > 0.
14. xy′ −y =
*
9x2 + y2,
x > 0.
15. y(x2 −y2)dx −x(x2 + y2)dy = 0.
16. xy′ + y ln x = y ln y.
17. dy
dx = y2 + 2xy −2x2
x2 −xy + y2 .
18. 2xydy −(x2e−y2/x2 + 2y2)dx = 0.
19. x2 dy
dx = y2 + 3xy + x2.
20. yy′ =
*
x2 + y2 −x,
x > 0.
21. 2x(y + 2x)y′ = y(4x −y).
22. x dy
dx = x tan(y/x) + y.
23. dy
dx = x
*
x2 + y2 + y2
xy
,
x > 0.
24. Solve the differential equation in Example 1.8.6 by
ﬁrst transforming it into polar coordinates. (Hint:
Write the differential equation in differential form and
then express dx and dy in terms of r and θ.)
For Problems 25–27, solve the given initial-value problem.
25. dy
dx = 2(2y −x)
x + y
,
y(0) = 2.
26. dy
dx = 2x −y
x + 4y ,
y(1) = 1.
27. dy
dx = y −
*
x2 + y2
x
,
y(3) = 4.
28. Find all solutions to
x dy
dx −y =
)
4x2 −y2,
x > 0.
29. (a) Show that the general solution to the differential
equation
dy
dx = x + ay
ax −y
can be written in polar form as r = keaθ.
(b) For the particular case when a = 1/2, deter-
mine the solution satisfying the initial condition
y(1) = 1, and ﬁnd the maximum x-interval on
which this solution is valid. (Hint: When does
the solution curve have a vertical tangent?)
(c) ⋄On the same set of axes, sketch the spiral cor-
responding to your solution in (b), and the line
y = x/2. Thus verify the x-interval obtained in
(b) with the graph.
For Problems 30–31, determine the orthogonal trajectories
to the given family of curves. Sketch some curves from each
family.
30. x2 + y2 = 2cy.
31. (x −c)2 + (y −c)2 = 2c2.
32. Fix a real number m. Let S1 denote the family of cir-
cles, centered on the line y = mx, each member of
which passes through the origin.

80
CHAPTER 1
First-Order Differential Equations
(a) Show that the equation of S1 can be written in the
form
(x −a)2 + (y −ma)2 = a2(m2 + 1),
where a is a constant that labels particular mem-
bers of the family.
(b) Determine the equation of the family of orthog-
onal trajectories to S1, and show that it con-
sists of the family of circles centered on the line
x = −my that pass through the origin.
(c) ⋄Sketch some curves from both families when
m =
√
3/3.
Let F1 and F2 be two families of curves with the prop-
erty that whenever a curve from the family F1 inter-
sects one from the family F2, it does so at an angle
a ̸= π/2. If we know the equation of F2, then it can
be shown (see Problem 23 in Section 1.1) that the dif-
ferential equation for determining F1 is
dy
dx = m2 −tan a
1 + m2 tan a ,
(1.8.16)
where m2 denotes the slope of the family F2 at the
point (x, y).
For Problems 33–35, use Equation (1.8.16) to determine the
equation of the family of curves that cuts the given family at
an angle α = π/4.
33. x2 + y2 = c.
34. y = cx6.
35. x2 + y2 = 2cx.
36. (a) Use Equation (1.8.16) to ﬁnd the equation of
the family of curves that intersects the family of
hyperbolas y = c/x at an angle a = a0. (̸= π/2)
(b) ⋄When α0 = π/4, sketch several curves from
each family.
37. (a) Use Equation (1.8.16) to show that the family of
curves that intersects the family of concentric cir-
cles x2 + y2 = c at an angle a = tan−1 m has
polar equation r = kemθ.
(b) ⋄When α = π/6, sketch several curves from
each family.
For Problems 38–50, solve the given differential equation.
38. y′ −x−1y = 4x2y−1 cos x,
x > 0.
39. dy
dx + 1
2(tan x)y = 2y3 sin x.
40. dy
dx −3
2x y = 6y1/3x2 ln x.
41. y′ + 2x−1y = 6
√
1 + x2√y,
x > 0.
42. y′ + 2x−1y = 6y2x4.
43. 2x(y′ + y3x2) + y = 0.
44. (x −a)(x −b)(y′ −√y) = 2(b −a)y, where a, b are
constants.
45. y′ + 6x−1y = 3x−1y2/3 cos x,
x > 0.
46. y′ + 4xy = 4x3y1/2.
47. dy
dx −
1
2x ln x y = 2xy3.
48. dy
dx −
1
(π −1)x y =
3
1 −π xyπ.
49. 2y′ + y cot x = 8y−1 cos3 x.
50. (1 −
√
3)y′ + y sec x = y
√
3 sec x.
For Problems 51–52, solve the given initial-value problem.
51. dy
dx +
2x
1 + x2 y = xy2,
y(0) = 1.
52. y′ + y cot x = y3 sin3 x,
y(π/2) = 1.
53. Consider the differential equation
y′ = F(ax + by + c),
(1.8.17)
where a, b ̸= 0, and c are constants. Show that the
change of variables from x, y to x, V , where
V = ax + by + c
reduces Equation (1.8.17) to the separable form
1
bF(V ) + a dV = dx.
For Problems 54–56, use the result from the previous prob-
lem to solve the given differential equation. For Problem 54,
impose the given initial condition as well.
54. y′ = (9x −y)2,
y(0) = 0.
55. y′ = (4x + y + 2)2.
56. y′ = sin2(3x −3y + 1).

1.8
Change of Variables 81
57. Show that the change of variables V = xy transforms
the differential equation
dy
dx = y
x F(xy)
into the separable differential equation
1
V [F(V ) + 1]
dV
dx = 1
x .
58. Use the result from the previous problem to solve
dy
dx = y
x [ln(xy) −1].
59. Consider the differential equation
dy
dx = 2x(x + y)2 −1.
(1.8.18)
(a) Show that the change of variables
y(x) = w(x) −x
reduces (1.8.18) to the separable equation
dw
dx = 2xw2.
(1.8.19)
(b) Solve the differential equation (1.8.19) and then
determine the particular solution to the differen-
tial equation (1.8.18) that satisﬁes the initial con-
dition y(0) = 1.
60. Consider the differential equation
dy
dx = x + 2y −1
2x −y + 3.
(1.8.20)
(a) Show that the change of variables deﬁned by
x = u −1,
y = v + 1
transforms Equation (1.8.20) into the homoge-
neous equation
dv
du = u + 2v
2u −v .
(1.8.21)
(b) Find the general solution to Equation (1.8.21),
and hence, solve Equation (1.8.20).
61. A differential equation of the form
y′ + p(x)y + q(x)y2 = r(x)
(1.8.22)
is called a Riccati equation.
(a) If y = Y(x) is a known solution to Equation
(1.8.22), show that the substitution
y = Y(x) + v−1(x)
reduces it to the linear equation
v′ −[p(x) + 2Y(x)q(x)]v = q(x).
(b) Find the general solution to the Riccati equation
x2y′ −xy −x2y2 = 1,
x > 0,
given that y = −x−1 is a solution.
62. Consider the Riccati equation
y′ + 2x−1y −y2 = −2x−2,
x > 0.
(1.8.23)
(a) Determine the values of the constants a and r
such that y(x) = axr is a solution to Equation
(1.8.23).
(b) Use the result from part (a) of the previous prob-
lem to determine the general solution to Equation
(1.8.23).
63. (a) Show that the change of variables y = x−1 + w
transforms the Riccati differential equation
y′ + 7x−1y −3y2 = 3x−2
(1.8.24)
into the Bernoulli equation
w′ + x−1w = 3w2.
(1.8.25)
(b) Solve Equation (1.8.25), and hence determine the
general solution to (1.8.24).
64. Consider the differential equation
y−1y′ + p(x) ln y = q(x),
(1.8.26)
where p(x) and q(x) are continuous functions on
some interval (a, b). Show that the change of vari-
ables u = ln y reduces Equation (1.8.26) to the linear
differential equation
u′ + p(x)u = q(x),

82
CHAPTER 1
First-Order Differential Equations
and hence show that the general solution to Equation
(1.8.26) is
y(x) = exp
'
I −1
%,
I (x)q(x)dx + c
&(
,
where
I = e
;
p(x)dx
(1.8.27)
and c is an arbitrary constant.
65. Use the technique derived in the previous problem to
solve the initial-value problem
y−1y′ −2x−1 ln y = x−1(1 −2 ln x),
y(1) = e.
66. Consider the differential equation
f ′(y)dy
dx + p(x) f (y) = q(x),
(1.8.28)
where p and q are continuous functions on some in-
terval (a, b), and f is an invertible function. Show that
Equation (1.8.28) can be written as
du
dx + p(x)u = q(x),
where u = f (y) and hence show that the general so-
lution to Equation (1.8.28) is
y(x) = f −1
'
I −1
%,
I (x)q(x)dx + c
&(
,
where I is given in (1.8.27), f −1 is the inverse of f ,
and c is an arbitrary constant.
67. Solve
sec2 y dy
dx +
1
2√1 + x tan y =
1
2√1 + x .
1.9
Exact Differential Equations
For the next technique it is best to consider ﬁrst-order differential equation written in
differential form
M(x, y)dx + N(x, y)dy = 0,
(1.9.1)
where M and N are given functions, assumed to be sufﬁciently smooth.12 The method
that we will consider is based on the idea of a differential. Recall from a previous calculus
course that if φ = φ(x, y) is a function of two variables, x and y, then the differential
of φ, denoted dφ, is deﬁned by
dφ = ∂φ
∂x dx + ∂φ
∂y dy.
(1.9.2)
Example 1.9.1
Solve
tan y dx + x sec2 y dy = 0.
(1.9.3)
Solution:
This equation is separable; however, we will use a different technique to
solve it. By inspection, we notice that
tan y dx + x sec2 y dy = d(x tan y).
Consequently, Equation (1.9.3) can be written as d(x tan y) = 0, which implies that
x tan y is constant and hence, the general solution to Equation (1.9.3) is
tan y = c
x
where c is an arbitrary constant.
□
12This means we assume that the functions M and N have continuous derivatives of sufﬁciently high order.

1.9
Exact Differential Equations 83
In the foregoing example we were able to write the given differential equation in the
form dφ(x, y) = 0, and hence obtain its solution. However, we cannot always do this.
Indeed we see by comparing Equation (1.9.1) with (1.9.2) that the differential equation
M(x, y) dx + N(x, y) dy = 0
can be written as dφ = 0 if and only if
M = ∂φ
∂x
and
N = ∂φ
∂y
for some function φ. This motivates the following deﬁnition:
DEFINITION
1.9.2
The differential equation
M(x, y) dx + N(x, y) dy = 0
is said to be exact in a region R of the xy-plane if there exists a function φ(x, y) such
that
∂φ
∂x = M,
∂φ
∂y = N,
(1.9.4)
for all (x, y) in R.
Any function φ satisfying (1.9.4) is called a potential function for the differential
equation
M(x, y) dx + N(x, y) dy = 0.
We emphasize that if such a function exists, then the preceding differential equation can
be written as
dφ = 0.
This is why such a differential equation is called an exact differential equation. From the
previous example, a potential function for the differential equation
tan y dx + x sec2 y dy = 0
is
φ(x, y) = x tan y.
We now show that if a differential equation is exact and we can ﬁnd a potential function
φ, its solution can be written down immediately.
Theorem 1.9.3
The general solution to an exact equation
M(x, y) dx + N(x, y) dy = 0
is deﬁned implicitly by
φ(x, y) = c,
where φ satisﬁes (1.9.4) and c is an arbitrary constant.

84
CHAPTER 1
First-Order Differential Equations
Proof We rewrite the differential equation in the form
M(x, y) + N(x, y)dy
dx = 0.
Since the differential equation is exact, there exists a potential function φ (see (1.9.4))
such that
∂φ
∂x + ∂φ
∂y
dy
dx = 0.
But this implies that d
dx [φ(x, y(x))] = 0, so that φ(x, y) = c, where c is a constant.
Remarks
1. The potential function φ is a function of two variables x and y, and we interpret the
relationship φ(x, y) = c as deﬁning y implicitly as a function of x. The preceding
theorem states that this relationship deﬁnes the general solution to the differential
equation for which φ is a potential function.
2. Geometrically, Theorem 1.9.3 says that the solution curves of an exact differential
equation are the family of curves φ(x, y) = k, where k is a constant. These are
called the level curves of the function φ(x, y).
The following two questions now arise:
1. How can we tell whether a given differential equation is exact?
2. If we have an exact equation, how do we ﬁnd a potential function?
The answers are given in the next theorem and its proof.
Theorem 1.9.4
(Test for Exactness)
Let M, N, and their ﬁrst partial derivatives My and Nx, be continuous in a (simply
connected13) region R of the xy-plane. Then the differential equation
M(x, y) dx + N(x, y) dy = 0
is exact for all x, y in R if and only if
∂M
∂y = ∂N
∂x .
(1.9.5)
Proof We ﬁrst prove that exactness implies the validity of Equation (1.9.5). If the
differential equation is exact, then by deﬁnition there exists a potential function φ(x, y)
such that φx = M and φy = N. Thus, taking partial derivatives, φxy = My and
φyx = Nx. Since My and Nx are continuous in R, it follows that φxy and φyx are
continuous in R. But, from multivariable calculus, this implies that φxy = φyx and
hence that My = Nx.
13Roughly speaking, simply connected means that the interior of any closed curve drawn in the region also
lies in the region. For example, the interior of a circle is a simply connected region, although the region between
two concentric circles is not.

1.9
Exact Differential Equations 85
We now prove the converse. Thus we assume that Equation (1.9.5) holds and must
prove that there exists a potential function φ such that
∂φ
∂x = M
(1.9.6)
and
∂φ
∂y = N.
(1.9.7)
The proof is constructional. That is, we will actually ﬁnd a potential function φ. We
begin by integrating Equation (1.9.6) with respect to x, holding y ﬁxed (this is a partial
integration) to obtain
φ(x, y) =
, x
M(s, y) ds + h(y),
(1.9.8)
where h(y) is an arbitrary function of y (this is the integration “constant" that we must
allow to depend on y, since we held y ﬁxed in performing the integration14). We now
show how to determine h(y) so that the function f deﬁned in (1.9.8) also satisﬁes
Equation (1.9.7). Differentiating (1.9.8) partially with respect to y yields
∂φ
∂y = ∂
∂y
, x
M(s, y) ds + dh
dy .
In order that φ satisfy Equation (1.9.7) we must choose h(y) to satisfy
∂
∂y
, x
M(s, y) ds + dh
dy = N(x, y).
That is,
dh
dy = N(x, y) −∂
∂y
, x
M(s, y) ds.
(1.9.9)
Since the left-hand side of this expression is a function of y only, we must show, for
consistency, that the right-hand side also depends only on y. Taking the derivative of the
right-hand side with respect to x yields
∂
∂x
!
N −∂
∂y
, x
M(s, y) ds
"
= ∂N
∂x −
∂2
∂x∂y
, x
M(s, y) ds
= ∂N
∂x −∂
∂y
! ∂
∂x
, x
M(s, y) ds
"
= ∂N
∂x −∂M
∂y .
Thus, using (1.9.5), we have
∂
∂x
!
N −∂
∂y
, x
M(s, y) ds
"
= 0,
so that the right-hand side of Equation (1.9.9) does just depend on y. It follows that
(1.9.9) is a consistent equation, and hence, we can integrate both sides with respect to y
to obtain
h(y) =
, y
N(x, t) dt −
, y ∂
∂t
!, x
M(s, t) ds
"
dt.
14Throughout the text,
, x
f (t) dt means “evaluate the indeﬁnite integral
,
f (t) dt and replace t with x in
the result.”

86
CHAPTER 1
First-Order Differential Equations
Finally, substituting into (1.9.8) yields the potential function
φ(x, y) =
, x
M(s, y)ds +
, y
N(x, t) dt −
, y ∂
∂t
!, x
M(s, t) ds
"
dt.
Remark
There is no need to memorize the ﬁnal result for φ. For each particular
problem, one can construct an appropriate potential function from ﬁrst principles. This
is illustrated in Examples 1.9.6 and 1.9.7.
Example 1.9.5
Determine whether the given differential equation is exact.
1. 2xey dx + (x2ey + cos y) dy = 0.
2. x2y dx −(xy2 + y3) dy = 0.
Solution:
1. In this case, M = 2xey and N = x2ey + cos y, so that My = 2xey = Nx. It
follows from the previous theorem that the differential equation is exact.
2. In this case, we have M = x2y and N = −(xy2 + y3), so that My = x2, whereas
Nx = −y2. Since My ̸= Nx, the differential equation is not exact.
□
Example 1.9.6
Determine the general solution to (y/x) dx + [1 + ln(xy)] dy = 0,
x > 0.
Solution:
We have
M(x, y) = y/x,
N(x, y) = 1 + ln(xy),
so that
My = 1/x = Nx.
Hence the given differential equation is exact, and so there exists a potential function φ
such that (see Deﬁnition 1.9.2)
∂φ
∂x = y/x,
(1.9.10)
∂φ
∂y = 1 + ln(xy).
(1.9.11)
Integrating Equation (1.9.10) with respect to x, holding y ﬁxed, yields
φ(x, y) = y ln x + h(y),
(1.9.12)
where h is an arbitrary function of y. We now determine h(y) such that (1.9.12) also
satisﬁes Equation (1.9.11). Taking the derivative of (1.9.12) with respect to y yields
∂φ
∂y = ln x + dh
dy .
(1.9.13)
Equations (1.9.11) and (1.9.13) give two expressions for ∂φ
∂y . This allows us to de-
termine h. Subtracting Equation (1.9.11) from Equation (1.9.13) gives the consistency
requirement
ln x + dh
dy −1 −ln(xy) = 0,

1.9
Exact Differential Equations 87
which simpliﬁes to
dh
dy = 1 + ln y.
Integrating the preceding equation yields
h(y) = y ln y,
where we have set the integration constant equal to zero without loss of generality since
we only require one potential function. Substitution into (1.9.12) yields the potential
function
φ(x, y) = y ln x + y ln y = y ln(xy).
Consequently, the given differential equation can be written as
d[y ln(xy)] = 0,
and so, from Theorem 1.9.3, the general solution is
y ln(xy) = c.
□
Notice that the solution obtained in the preceding example is an implicit solution.
Due to the nature of the way in which a potential function for an exact equation is
obtained, this is usually the case.
Example 1.9.7
Find the general solution to
[sin(xy) + xy cos(xy) + 2x] dx +
1
x2 cos(xy) + 2y
2
dy = 0.
Solution:
We have
M(x, y) = sin(xy) + xy cos(xy) + 2x
and
N(x, y) = x2 cos(xy) + 2y.
Thus,
My = 2x cos(xy) −x2y sin(xy) = Nx,
and so the differential equation is exact. Hence there exists a potential function φ(x, y)
such that
∂φ
∂x = sin(xy) + xy cos(xy) + 2x,
(1.9.14)
∂φ
∂y = x2 cos(xy) + 2y.
(1.9.15)
In this case, Equation (1.9.15) is the simpler equation, and so we integrate it with respect
to y, holding x ﬁxed, to obtain
φ(x, y) = x sin(xy) + y2 + g(x),
(1.9.16)
where g(x) is an arbitrary function of x. We now determine g(x), and hence φ, from
(1.9.14) and (1.9.16). Differentiating (1.9.16) partially with respect to x yields
∂φ
∂x = sin(xy) + xy cos(xy) + dg
dx .
(1.9.17)

88
CHAPTER 1
First-Order Differential Equations
Equations (1.9.14) and (1.9.17) are consistent if and only if
dg
dx = 2x.
Hence, upon integrating,
g(x) = x2,
where we have once more set the integration constant to zero without loss of generality,
since we only require one potential function. Substituting into (1.9.16) gives the potential
function
φ(x, y) = x sin xy + x2 + y2.
The original differential equation can therefore be written as
d(x sin xy + x2 + y2) = 0,
and hence the general solution is
x sin xy + x2 + y2 = c.
□
Remark
At ﬁrst sight the above procedure appears to be quite complicated. However,
with a little bit of practice, the steps are seen to be, in fact, fairly straightforward. As we
have shown in Theorem 1.9.4, the method works in general, provided one starts with an
exact differential equation.
Integrating Factors
Usually a given differential equation will not be exact. However, sometimes it is possible
to multiply the differential equation by a nonzero function to obtain an exact equation
that can then be solved using the technique we have described in this section. Notice
that the solution to the resulting exact equation will be the same as that of the original
equation, since we multiply by a nonzero function.
DEFINITION
1.9.8
Anonzerofunction I (x, y)iscalledan integratingfactor forthedifferentialequation
M(x, y) dx + N(x, y) dy = 0 if the differential equation
I (x, y)M(x, y) dx + I (x, y)N(x, y) dy = 0
is exact.
Example 1.9.9
Show that I = x2y is an integrating factor for the differential equation
(3y2 + 5x2y) dx + (3xy + 2x3) dy = 0.
(1.9.18)
Solution:
Multiplying the given differential equation (which is not exact) by x2y
yields
(3x2y3 + 5x4y2) dx + (3x3y2 + 2x5y) dy = 0.
(1.9.19)
Thus,
My = 9x2y2 + 10x4y = Nx,

1.9
Exact Differential Equations 89
so that the differential equation (1.9.19) is exact, and hence I = x2y is an integrating
factor for Equation (1.9.18). Indeed we leave it as an exercise to verify that (1.9.19) can
be written as
d(x3y3 + x5y2) = 0,
so that the general solution to Equation (1.9.19) (and hence the general solution to
Equation (1.9.18)) is deﬁned implicitly by
x3y3 + x5y2 = c.
That is,
x3y2(y + x2) = c.
□
As shown in the next theorem, using the test for exactness it is straightforward to
determine the conditions that a function I (x, y) must satisfy in order to be an integrating
factor for the differential equation M(x, y) dx + N(x, y) dy = 0.
Theorem 1.9.10
The function I (x, y) is an integrating factor for
M(x, y) dx + N(x, y) dy = 0
(1.9.20)
if and only if it is a solution to the partial differential equation
N ∂I
∂x −M ∂I
∂y =
!∂M
∂y −∂N
∂x
"
I.
(1.9.21)
Proof Multiplying Equation (1.9.20) by I yields
I M dx + I N dy = 0.
This equation is exact if and only if
∂
∂y (I M) = ∂
∂x (I N),
that is, if and only if
∂I
∂y M + I ∂M
∂y = ∂I
∂x N + I ∂N
∂x .
Rearranging the terms in this equation yields Equation (1.9.21).
The preceding theorem is not too useful in general, since it is usually no easier to
solve the partial differential equation (1.9.21) to ﬁnd I than it is to solve the original
Equation (1.9.20). However, it sometimes happens that an integrating factor exists that
dependsonlyononevariable.WenowshowthatTheorem1.9.10canbeusedtodetermine
whensuchanintegratingfactorexistsandalsotoactuallyﬁndacorrespondingintegrating
factor.
Theorem 1.9.11
Consider the differential equation M(x, y) dx + N(x, y) dy = 0.
1. There exists an integrating factor that depends only on x if and only if
(My −Nx)/N = f (x), a function of x only. In such a case, an integrating factor is
I (x) = e
;
f (x) dx.

90
CHAPTER 1
First-Order Differential Equations
2. There exists an integrating factor that depends only on y if and only if
(My −Nx)/M = g(y), a function of y only. In such a case, an integrating factor is
I (y) = e−
;
g(y) dy.
Proof For (1), we begin by assuming that I = I (x) is an integrating factor for
M(x, y) dx + N(x, y) dy = 0. Then ∂I
∂y = 0, and so, from (1.9.21), I is a solution
to
dI
dx N = (My −Nx)I.
That is,
1
I
dI
dx = My −Nx
N
.
Since, by assumption, I is a function of x only, it follows that the left-hand side of this
expression depends only on x and hence also the right-hand side.
Conversely, suppose that (My−Nx)/N = f (x), a function of x only. Then, dividing
(1.9.21) by N, it follows that I is an integrating factor for M(x, y) dx + N(x, y) dy = 0
if and only if it is a solution to
∂I
∂x −M
N
∂I
∂y = I f (x).
(1.9.22)
We must show that this differential equation has a solution I that depends on x only.
We do this by explicitly integrating the differential equation under the assumption that
I = I (x). Indeed, if I = I (x), then Equation (1.9.22) reduces to
dI
dx = I f (x),
which is a separable equation with solution
I (x) = e
;
f (x) dx
The proof of (2) is similar, and so we leave it as an exercise (see Problem 33).
Example 1.9.12
Solve
(2x −y2) dx + xy dy = 0,
x > 0.
(1.9.23)
Solution:
The equation is not exact (My ̸= Nx). However,
My −Nx
N
= −2y −y
xy
= −3
x ,
which is a function of x only. It follows from (1) of the preceding theorem that an
integrating factor for Equation (1.9.23) is
I (x) = e−
;
(3/x) dx = e−3 ln x = x−3.
Multiplying Equation (1.9.23) by I yields the exact equation
(2x−2 −x−3y2) dx + x−2y dy = 0.
(1.9.24)

1.9
Exact Differential Equations 91
(The reader should check that this is exact, although it must be, by the previous theorem.)
We leave it as an exercise to verify that a potential function for Equation (1.9.24) is
φ(x, y) = 1
2 x−2y2 −2x−1,
and hence the general solution to (1.9.23) is given implicitly by
1
2 x−2y2 −2x−1 = c,
or equivalently,
y2 −4x = c1x2.
□
Exercises for 1.9
Key Terms
Exact differential equation, Potential function, Integrating
factor.
Skills
• Be able to determine whether or not a given differential
equation is exact.
• Given the partial derivatives ∂φ
∂x and ∂φ
∂y of a potential
function φ(x, y), be able to determine φ(x, y).
• Be able to ﬁnd the general solution to an exact differ-
ential equation.
• When circumstances allow, be able to use an integrat-
ing factor to convert a given differential equation into
an exact differential equation with the same solution
set.
True-False Review
For items (a)–(i), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Thedifferentialequation M(x, y)dx+N(x, y)dy = 0
is exact in a simply connected region R if Mx and Ny
are continuous partial derivatives with Mx = Ny.
(b) The solution to an exact differential equation is called
a potential function.
(c) If M(x) and N(y) are continuous functions, then the
differential equation M(x)dx + N(y)dy = 0 is exact.
(d) If (My −Nx)/N(x, y) is a function of x only, then the
differential equation M(x, y)dx + N(x, y)dy = 0
becomes exact when it is multiplied through by
I (x) = exp
!,
(My −Nx)/N(x, y)dx
"
.
(e) There is a unique potential function for an exact dif-
ferential equation M(x, y)dx + N(x, y)dy = 0.
(f) The differential equation
(2ye2x −sin y)dx + (e2x −x cos y)dy = 0
is exact.
(g) The differential equation
−2xy
(x2 + y)2 dx +
x2
(x2 + y)2 dy = 0
is exact.
(h) The differential equation
(y2 + cos x)dx + 2xy2dy = 0
is exact.
(i) The differential equation
(ex sin y sin y)dx + (ex sin y cos y)dy = 0
is exact.
Problems
For Problems 1–4, determine whether the given differential
equation is exact.
1. yexy dx + (2y −xexy) dy = 0.

92
CHAPTER 1
First-Order Differential Equations
2. [cos(xy) −xy sin(xy)] dx −x2 sin(xy) dy = 0.
3. (y + 3x2) dx + x dy = 0.
4. 2xey dx + (3y2 + x2ey) dy = 0.
For Problems 5–15, solve the given differential equation.
5. 2xy dx + (x2 + 1) dy = 0.
6. (y2 −2x) dx + 2xy dy = 0.
7. (4e2x + 2xy −y2) dx + (x −y)2 dy = 0.
8.
! 1
x −
y
x2 + y2
"
dx +
x
x2 + y2 dy = 0.
9. [y cos(xy) −sin x] dx + x cos(xy) dy = 0.
10. (2y2e2x + 3x2) dx + 2ye2x dy = 0.
11. (y2 + cos x) dx + (2xy + sin y) dy = 0.
12. (sin y + y cos x) dx + (x cos y + sin x) dy = 0.
13. [1 + ln (xy)] dx + xy−1 dy = 0.
14. x−1(xy −1) dx + y−1(xy + 1) dy = 0.
15. (2xy + cos y) dx + (x2 −x sin y −2y) dy = 0.
For Problems 16–18, solve the given initial-value problem.
16. 2x2y′ + 4xy = 3 sin x, y(2π) = 0.
17. (3x2 ln x + x2 −y) dx −x dy = 0, y(1) = 5.
18. (yexy + cos x) dx + xexy dy = 0, y(π/2) = 0.
19. Show that if φ(x, y) is a potential function for
M(x, y) dx + N(x, y) dy = 0, then so is φ(x, y) + c,
where c is an arbitrary constant. This shows that po-
tential functions are only uniquely deﬁned up to an
additive constant.
For Problems 20–22, determine whether the given function
is an integrating factor for the given differential equation.
20. I (x, y) = cos(xy), [tan(xy) + xy] dx + x2 dy = 0.
21. I (x, y) = y−2e−x/y, y(x2 −2xy) dx −x3 dy = 0.
22. I (x) = sec x, [2x −(x2 + y2) tan x] dx + 2y dy = 0.
For Problems 23–29, determine an integrating factor for
the given differential equation, and hence ﬁnd the general
solution.
23. (y −x2) dx + 2x dy = 0,
x > 0.
24. (3xy −2y−1) dx + x(x + y−2) dy = 0.
25. x2y dx + y(x3 + e−3y sin y) dy = 0.
26. (xy −1) dx + x2 dy = 0.
27. dy
dx +
2x
1 + x2 y =
1
(1 + x2)2 .
28. xy[2 ln(xy) + 1] dx + x2 dy = 0,
x > 0.
29. y dx −(2x + y4) dy = 0.
For Problems 30–32, determine the values of the constants
r and s such that I (x, y) = xr ys is an integrating factor for
the given differential equation.
30. (y−1 −x−1) dx + (xy−2 −2y−1) dy = 0.
31. 2y(y + 2x2) dx + x(4y + 3x2) dy = 0.
32. y(5xy2 + 4) dx + x(xy2 −1) dy = 0.
33. Prove that if (My −Nx)/M = g(y), a function of y
only, then an integrating factor for
M(x, y) dx + N(x, y) dy = 0
is I (y) = e−
;
g(y)dy.
34. Consider the general ﬁrst-order linear differential
equation
dy
dx + p(x)y = q(x),
(1.9.25)
where p(x)andq(x)arecontinuousfunctionsonsome
interval (a, b).
(a) Rewrite Equation (1.9.25) in differential form,
and show that an integrating factor for the result-
ing equation is
I (x) = e
;
p(x) dx.
(1.9.26)
(b) Show that the general solution to Equation
(1.9.25) can be written in the form
y(x) = I −1
', x
I (t)q(t)dt + c
(
,
where I is given in Equation (1.9.26), and c is an
arbitrary constant.

1.10
Numerical Solution to First-Order Differential Equations 93
1.10
Numerical Solution to First-Order Differential Equations
So far in this chapter we have investigated ﬁrst-order differential equations geometrically
via slope ﬁelds, and analytically, by trying to construct exact solutions to certain types of
differential equations. Certainly, for most ﬁrst-order differential equations, it simply is
notpossibletoﬁndanalyticsolutions,sincetheywillnotfallintothefewclassesforwhich
solution techniques are available. Our ﬁnal approach to analyzing ﬁrst-order differential
equations is to look at the possibility of constructing a numerical approximation to the
unique solution to the initial-value problem
dy
dx = f (x, y),
y(x0) = y0.
(1.10.1)
We consider three techniques that give varying levels of accuracy. In each case, we
generate a sequence of approximations y1, y2, . . . to the value of the exact solution at
the points x1, x2, . . . , where xn+1 = xn + h, n = 0, 1, . . . , and h is a real number.
We emphasize that numerical methods do not generate a formula for the solution to the
differential equation. Rather they generate a sequence of approximations to the value of
the solution at speciﬁed points. Furthermore, if we use a sufﬁcient number of points, then
by plotting the points (xi, yi) and joining them with straight line segments we are able to
obtain an overall approximation to the solution curve corresponding to the solution of the
given initial-value problem. This is how the approximate solution curves were generated
in the preceding sections via the computer algebra system Maple. There are many subtle
ideas associated with constructing numerical solutions to initial-value problems that are
beyond the scope of this text. Indeed, a full discussion of the application of numerical
methods to differential equations is best left for a future course in numerical analysis.
Euler’s Method
Suppose we wish to approximate the solution to the initial-value problem (1.10.1) at
x = x1 = x0 +h, where h is small. The idea behind Euler’s Method is to use the tangent
line to the solution curve through (x0, y0) to obtain such an approximation. (See Figure
1.10.1.) The equation of the tangent line through (x0, y0) is
y(x) = y0 + m(x −x0),
h
h
h
x0
x1
x2
x3
y0
y1
y2
y3
y
x
Exact solution to IVP
Solution curve through (x1, y1)
Tangent line to the
solution curve passing
through (x1, y1)
Tangent line at the point
(x0, y0) to the exact 
solution to the IVP
(x0, y0)
(x1, y1)
(x1, y(x1))
(x2, y(x2))
Figure 1.10.1: Euler’s method for approximating the solution to the initial-value problem
dy
dx = f (x, y),
y(x0) = y0.

94
CHAPTER 1
First-Order Differential Equations
where m is the slope of the curve at (x0, y0). From Equation (1.10.1), m = f (x0, y0), so
y(x) = y0 + f (x0, y0)(x −x0).
Setting x = x1 in this equation yields the Euler approximation to the exact solution at
x1, namely,
y1 = y0 + f (x0, y0)(x1 −x0),
which we write as
y1 = y0 + h f (x0, y0).
Now suppose we wish to obtain an approximation to the exact solution to the initial-
value problem (1.10.1) at x2 = x1 + h. We can use the same idea, except we now use
the tangent line to the solution curve through (x1, y1). From (1.10.1), the slope of this
tangent line is f (x1, y1), so that the equation of the required tangent line is
y(x) = y1 + f (x1, y1)(x −x1).
Setting x = x2 yields the approximation
y2 = y1 + h f (x1, y1),
where we have substituted for x2 −x1 = h, to the solution to the initial-value problem
at x = x2. Continuing in this manner, we determine the sequence of approximations
yn+1 = yn + h f (xn, yn),
n = 0, 1, . . .
to the solution to the initial-value problem (1.10.1) at the points xn+1 = xn + h.
In summary, Euler’s Method for approximating the solution to the initial-value problem
y′ = f (x, y),
y(x0) = y0
at the points xn+1 = x0 + nh (n = 0, 1, . . . ) is
yn+1 = yn + h f (xn, yn),
n = 0, 1, . . . .
(1.10.2)
Example 1.10.1
Consider the initial-value problem
y′ = y −x,
y(0) = 1/2.
Use Euler’s method with (a) h = 0.1 and (b) h = 0.05 to obtain an approximation to
y(1). Given that the exact solution to the initial-value problem is
y(x) = x + 1 −1
2ex,
compare the errors in the two approximations to y(1).
Solution:
In this problem we have
f (x, y) = y −x,
x0 = 0,
y0 = 1
2.
(a) Setting h = 0.1 in (1.10.2) yields
yn+1 = yn + 0.1(yn −xn).

1.10
Numerical Solution to First-Order Differential Equations 95
Hence,
y1 = y0 + 0.1(y0 −x0) = 0.5 + 0.1(0.5 −0) = 0.55,
y2 = y1 + 0.1(y1 −x1) = 0.55 + 0.1(0.55 −0.1) = 0.595.
Continuing in this manner, we generate the approximations listed in Table 1.10.1,
where we have rounded the calculations to six decimal places. We have also listed
the values of the exact solution and the absolute value of the error. In this case,
the approximation to y(1) is y10 = 0.703129, with an absolute error of
|y(1) −y10| = 0.062270.
(1.10.3)
n
xn
yn
Exact Solution
Absolute Error
1
0.1
0.55
0.547414
0.002585
2
0.2
0.595
0.589299
0.005701
3
0.3
0.65345
0.625070
0.009430
4
0.4
0.66795
0.654088
0.013862
5
0.5
0.694745
0.675639
0.019106
6
0.6
0.714219
0.688941
0.025278
7
0.7
0.725641
0.693124
0.032518
8
0.8
0.728205
0.687229
0.040976
9
0.9
0.721026
0.670198
0.050828
10
1.0
0.703129
0.640859
0.062270
Table 1.10.1: The results of applying Euler’s method with h = 0.1 to the initial-value problem
in Example 1.10.1.
(b) When h = 0.05, Euler’s method gives
yn+1 = yn + 0.05(yn −xn),
n = 0, 1, . . . , 19,
which generates the approximations given in Table 1.10.2, where we have only
listed every other intermediate approximation. We see that the approximation to
y(1) is
y20 = 0.673351
and that the absolute error in this approximation is
|y(1) −y20| = 0.032492.
n
xn
yn
Exact Solution
Absolute Error
2
0.1
0.54875
0.547414
0.001335
4
0.2
0.592247
0.589299
0.002948
6
0.3
0.629952
0.625070
0.004881
8
0.4
0.661272
0.654088
0.007185
10
0.5
0.685553
0.675639
0.009913
12
0.6
0.702072
0.688941
0.013131
14
0.7
0.710034
0.693124
0.016910
16
0.8
0.708563
0.687229
0.021333
18
0.9
0.696690
0.670198
0.026492
20
1.0
0.686525
0.640859
0.032492
Table 1.10.2: The results of applying Euler’s method with h = 0.05 to the initial-value
problem in Example 1.10.1.

96
CHAPTER 1
First-Order Differential Equations
Comparing this with (1.10.3), we see that the smaller step size has led to a better
approximation. In fact, it has almost halved the error at y(1). In Figure 1.10.2 we have
plotted the exact solution and the Euler approximations just obtained.
0.2
0.4
0.6
0.8
1
0.55
0.6
0.65
0.7
x
y
Figure 1.10.2: The exact solution to the initial-value problem considered in Example 1.10.1
and the two approximations obtained using Euler’s method.
□
In the preceding example we saw that halving the step size had the effect of essen-
tially halving the error. However, even then the accuracy was not as good as we probably
would have liked. Of course we could just keep decreasing the step size (provided we do
not take h to be so small that round-off errors start to play a role) to increase the accuracy,
but then the number of steps we would have to take would make the calculations very
cumbersome. A better approach is to derive methods that have a higher order of accuracy.
We will consider two such methods.
Modified Euler Method (Heun’s Method)
The method that we consider here is an example of what is called a predictor-corrector
method. The idea is to use the formula from Euler’s method to obtain a ﬁrst approxima-
tion to the solution y(xn+1). We denote this approximation by y∗
n+1, so that
y∗
n+1 = yn + h f (xn, yn).
We now improve (or “correct") this approximation by once more applying Euler’s
method. But this time, we use the average of the slopes of the solution curves through
(xn, yn) and (xn+1, y∗
n+1). This gives
yn+1 = yn + 1
2h[ f (xn, yn) + f (xn+1, y∗
n+1)].
As illustrated in Figure 1.10.3 for the case n = 1, we can interpret the modiﬁed Euler
approximations as arising from ﬁrst stepping to the point
P
!
xn + h
2 , yn + h f (xn, yn)
2
"
along the tangent line to the solution curve through (xn, yn) and then stepping from P
to (xn+1, yn+1) along the line through P whose slope is f (xn, y∗
n).
In summary, the modiﬁed Euler method for approximating the solution to the
initial-value problem
y′ = f (x, y), y(x0) = y0

1.10
Numerical Solution to First-Order Differential Equations 97
(x1, y(x1))
(x1, y1)
(x1, y*1)
(x0, y0)
P
Exact solution
to the IVP
(x0 1 h/2, y0 1 hf(x0, y0)/2)
h/2
x0
x0 1 h/2
x1
x
Modified Euler
approximation at x 5 x1
Euler approximation
at x 5 x1
Tangent line to solution
curve through (x1, y*1)
y
Figure 1.10.3: Derivation of the ﬁrst step in the modiﬁed Euler Method.
at the points xn+1 = x0 + nh (n = 0, 1, . . . ) is
yn+1 = yn + 1
2h[ f (xn, yn) + f (xn+1, y∗
n+1)],
where
y∗
n+1 = yn + h f (xn, yn),
n = 0, 1, . . . .
Example 1.10.2
Apply the modiﬁed Euler method with h = 0.1 to determine an approximation to the
solution to the initial-value problem
y′ = y −x,
y(0) = 1/2
at x = 1.
Solution:
Taking h = 0.1, and f (x, y) = y −x in the modiﬁed Euler method yields
y∗
n+1 = yn + 0.1(yn −xn)
yn+1 = yn + 0.05(yn −xn + y∗
n+1 −xn+1).
Hence,
yn+1 = yn + 0.05{yn −xn + [yn + 0.1(yn −xn)] −xn+1}.
That is,
yn+1 = yn + 0.05(2.1yn −1.1xn −xn+1),
n = 0, 1, . . . , 9.
When n = 0,
y1 = y0 + 0.05(2.1y0 −1.1x0 −x1) = 0.5475,
and when n = 1,
y2 = y1 + 0.05(2.1y1 −1.1x1 −x2) = 0.5894875.
Continuing in this manner, we generate the results displayed in Table 1.10.3. From this
table, we see that the approximation to y(1) according to the modiﬁed Euler method is
y10 = 0.642960.
As seen in the previous example, the value of the exact solution at x = 1 is
y(1) = 0.640859.

98
CHAPTER 1
First-Order Differential Equations
n
xn
yn
Exact Solution
Absolute Error
1
0.1
0.5475
0.547414
0.000085
2
0.2
0.589487
0.589299
0.000189
3
0.3
0.625384
0.625070
0.000313
4
0.4
0.654549
0.654088
0.000461
5
0.5
0.676277
0.675639
0.000637
6
0.6
0.689786
0.688941
0.000845
7
0.7
0.694213
0.693124
0.001089
8
0.8
0.688605
0.687229
0.001376
9
0.9
0.671909
0.670198
0.001711
10
1.0
0.642959
0.640859
0.002100
Table 1.10.3: The results of applying the modiﬁed Euler method with h = 0.1 to the
initial-value problem in Example 1.10.2.
Consequently, the absolute error in the approximation at x = 1 using the modiﬁed Euler
approximation with h = 0.1 is
|y(1) −y10| = 0.002100.
Comparing this with the results of the previous example we see that the modiﬁed Euler
method has picked up approximately one decimal place of accuracy when using a step
size h = 0.1. This is indicative of the general result that the error in the modiﬁed Euler
method behaves as order h/2 as compared to the order h behavior of the Euler method.
In Figure 1.10.4 we have sketched the exact solution to the differential equation and the
modiﬁed Euler approximation with h = 0.1.
0.2
0.4
0.6
0.8
1
0.55
0.6
0.65
y
x
Figure 1.10.4: The exact solution to the initial-value problem in Example 1.10.2 and the
approximations obtained using the modiﬁed Euler method with h = 0.1.
□
Runge-Kutta Method of Order Four
The ﬁnal method that we consider is somewhat more tedious to use in hand calculations,
but is very easily programmed into a calculator or computer. It is a fourth-order method,
which, in the case of a differential equation of the form y′ = f (x), reduces to Simp-
son’s Rule (which the reader has probably studied in a calculus course) for numerically
evaluating deﬁnite integrals. Without justiﬁcation, we state the algorithm.

1.10
Numerical Solution to First-Order Differential Equations 99
Fourth-Order Runge-Kutta Method for approximating the solution to the initial-value
problem
y′ = f (x, y),
y(x0) = y0
at the points xn+1 = x0 + nh (n = 0, 1, . . . ) is
yn+1 = yn + 1
6(k1 + 2k2 + 2k3 + k4),
where
k1 = h f (xn, yn), k2 = h f (xn + 1
2h, yn + 1
2k1), k3 = h f (xn + 1
2h, yn + 1
2k2),
k4 = h f (xn+1, yn + k3),
n = 0, 1, 2, . . . .
Remark
In the previous sections, we used Maple to generate slope ﬁelds and approx-
imate solution curves for ﬁrst-order differential equations. The solution curves were in
fact generated using a Runge-Kutta approximation.
Example 1.10.3
Apply the Fourth-Order Runge-Kutta Method with h = 0.1 to determine an approxima-
tion to the solution to the initial-value problem below at x = 1:
y′ = y −x,
y(0) = 1/2
Solution:
We take h = 0.1, and f (x, y) = y −x in the Fourth-Order Runge-Kutta
Method, and need to determine y10. First we determine k1, k2, k3, k4.
k1 = 0.1 f (xn, yn) = 0.1(yn −xn),
k2 = 0.1 f (xn + 0.05, yn + 0.5k1) = 0.1(yn + 0.5k1 −xn −0.05),
k3 = 0.1 f (xn + 0.05, yn + 0.5k2) = 0.1(yn + 0.5k2 −xn −0.05),
k4 = 0.1 f (xn+1, yn + k3) = 0.1(yn + k3 −xn+1).
When n = 0,
k1 = 0.1(0.5) = 0.05,
k2 = 0.1[0.5 + (0.5)(0.05) −0.05] = 0.0475,
k3 = 0.1[0.5 + (0.5)(0.0475) −0.05] = 0.047375,
k4 = 0.1(0.5 + 0.047375 −0.1) = 0.0447375,
so that
y1 = y0 + 1
6(k1 + 2k2 + 2k3 + k4) = 0.5 + 1
6(0.2844875) = 0.54741458,
rounded to eight decimal places. Continuing in this manner, we obtain the results dis-
played in Table 1.10.4. In particular, we see that the Fourth-Order Runge-Kutta Method
approximation to y(1) is
y10 = 0.64086013,
so that
|y(1) −y10| = 0.00000104.

100
CHAPTER 1
First-Order Differential Equations
n
xn
yn
Exact Solution
Absolute Error
1
0.1
0.54741458
0.54741454
0.00000004
2
0.2
0.58929871
0.58929862
0.00000009
3
0.3
0.62507075
0.62507060
0.00000015
4
0.4
0.65408788
0.65408788
0.00000022
5
0.5
0.67563968
0.67563968
0.00000032
6
0.6
0.68894102
0.68894102
0.00000042
7
0.7
0.69312419
0.69312365
0.00000054
8
0.8
0.68723022
0.68722954
0.00000068
9
0.9
0.67019929
0.67019844
0.00000085
10
1.0
0.64086013
0.64085909
0.00000104
Table 1.10.4: The results of applying the Fourth-Order Runge-Kutta Method with h = 0.1 to
the initial-value problem in Example 1.10.3.
Clearly this is an excellent approximation. If we increase the step size to h = 0.2, the
corresponding approximation to y(1) becomes
y5 = 0.640874,
with absolute error
|y(1) −y5| = 0.000015,
which is still very impressive.
□
Exercises for 1.10
Key Terms
Euler’s
method,
Predictor-corrector
method,
Modiﬁed
Euler method (Heun’s method), Fourth-order Runge-Kutta
method.
Skills
• Be able to apply Euler’s method to approximate the
solution to an initial-value problem at a point near the
initial value x0.
• Be able to use the modiﬁed Euler method (Heun’s
method) to approximate the solution to an initial-value
problem at a point near the initial value x0.
• Be able to use the Fourth-order Runge Kutta method
to approximate the solution to an initial-value problem
at a point near the initial value x0.
True-False Review
For items (a)–(d), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Generally speaking, the smaller the stepsize in Euler’s
method, the more accurate the approximation to the
solution of an initial-value problem at a point near the
initial value x0.
(b) Euler’s method is based on the equation of a tangent
line to a curve at a given point (x0, y0).
(c) With each additional step that is taken in Euler’s
method, the error in the approximation obtained from
the method can only grow in size.
(d) At each step of length h, Heun’s method requires two
applications of Euler’s method with step size h/2.
Problems
For Problems 1–5, use Euler’s method with the speciﬁed
step size to determine the solution to the given initial-value
problem at the speciﬁed point.
1. y′ = 4y −1,
y(0) = 1,
h = 0.05,
y(0.5).

1.11
Some Higher-Order Differential Equations 101
2. y′ = −2xy
1 + x2 ,
y(0) = 1,
h = 0.1,
y(1).
3. y′ = x −y2,
y(0) = 2,
h = 0.05,
y(0.5).
4. y′ = −x2y,
y(0) = 1,
h = 0.2,
y(1).
5. y′ = 2xy2,
y(0) = 0.5,
h = 0.1,
y(1).
For Problems 6–10, use the modiﬁed Euler method with the
speciﬁed step size to determine the solution to the given
initial-value problem at the speciﬁed point. In each case,
compare your answer to that obtained using Euler’s method.
6. The initial-value problem in Problem 1.
7. The initial-value problem in Problem 2.
8. The initial-value problem in Problem 3.
9. The initial-value problem in Problem 4.
10. The initial-value problem in Problem 5.
For Problems 11–15, use the Fourth-Order Runge-Kutta
Method with the speciﬁed step size to determine the solu-
tion to the given initial-value problem at the speciﬁed point.
In each case, compare your answer to that obtained using
Euler’s method.
11. The initial-value problem in Problem 1.
12. The initial-value problem in Problem 2.
13. The initial-value problem in Problem 3.
14. The initial-value problem in Problem 4.
15. The initial-value problem in Problem 5.
16. ⋄Use the Fourth-Order Runge-Kutta Method with
h = 0.5 to approximate the solution to the initial-
value problem
y′ + 1
10 y = e−x/10 cos x,
y(0) = 0
at the points x
=
0.5, 1.0, . . . , 25. Plot these
points and describe the behavior of the corresponding
solution.
1.11
Some Higher-Order Differential Equations
So far we have developed analytical techniques only for solving special types of ﬁrst-
order differential equations. The methods that we have discussed do not apply directly to
higher-order differential equations and so the solution to such equations usually requires
the derivation of new techniques. One approach is to replace a higher-order differential
equation by an equivalent system of ﬁrst-order equations. (This will be developed further
in Chapter 9.) For example, any second-order differential equation that can be written in
the form
d2y
dx2 = F
!
x, y, dy
dx
"
,
(1.11.1)
where F is a known function, can be replaced by an equivalent pair of ﬁrst-order differ-
ential equations as follows. We let v = dy/dx. Then d2y/dx2 = dv/dx, and so solv-
ing Equation (1.11.1) is equivalent to solving the following two ﬁrst-order differential
equations
dy
dx = v,
(1.11.2)
dv
dx = F(x, y, v).
(1.11.3)
In general the differential equation (1.11.3) cannot be solved directly, since it involves
three variables, namely, x, y, and v. However, for certain forms of the function F,
Equation (1.11.3) will involve only two variables and then can sometimes be solved for
v using one of our previous techniques. Having obtained v, we can then substitute into
Equation (1.11.2) to obtain a ﬁrst-order differential equation for y. We now discuss two
forms of F for which this is certainly the case.

102
CHAPTER 1
First-Order Differential Equations
Case 1: Second-Order Equations with the Dependent Variable Missing
If y does not occur explicitly in the function F, then Equation (1.11.1) assumes the
form
d2y
dx2 = F
!
x, dy
dx
"
.
(1.11.4)
Substituting v = dy/dx and dv/dx = d2y/dx2 into this equation allows us to replace
it with the two ﬁrst-order equations
dy
dx = v,
(1.11.5)
dv
dx = F(x, v).
(1.11.6)
Thus, to solve Equation (1.11.4), we ﬁrst solve Equation (1.11.6) for v in terms of x and
then solve Equation (1.11.5) for y as a function of x.
Example 1.11.1
Find the general solution to
d2y
dx2 = 1
x
!dy
dx + x2 cos x
"
,
x > 0.
(1.11.7)
Solution:
In Equation (1.11.7), the dependent variable is missing, and so we let
v = dy/dx, which implies that d2y/dx2 = dv/dx. Substituting into Equation (1.11.7)
yields the following equivalent ﬁrst-order system
dy
dx = v,
(1.11.8)
dv
dx = 1
x (v + x2 cos x).
(1.11.9)
Equation (1.11.9) is a ﬁrst-order linear differential equation with standard form
dv
dx −x−1v = x cos x.
(1.11.10)
An appropriate integrating factor is
I (x) = e−
;
x−1 dx = e−ln x = x−1.
Multiplying Equation (1.11.10) by x−1 reduces it to
d
dx (x−1v) = cos x,
which can be integrated directly to obtain
x−1v = sin x + c.
Thus,
v = x sin x + cx.
(1.11.11)
Substituting the expression for v from (1.11.11) into Equation (1.11.8) gives
dy
dx = x sin x + cx,

1.11
Some Higher-Order Differential Equations 103
which we can integrate to obtain
y(x) = −x cos x + sin x + c1x2 + c2.
□
Case 2: Second-Order Equations with the Independent Variable Missing
If x does not occur explicitly in the function F in Equation (1.11.1), then we must
solve a differential equation of the form
d2y
dx2 = F
!
y, dy
dx
"
.
(1.11.12)
In this case, we still let
v = dy
dx ,
as previously, but now we use the chain rule to express d2y/dx2 in terms of dv/dy.
Speciﬁcally, we have
d2y
dx2 = dv
dx = dv
dy
dy
dx = v dv
dy .
Substituting for dy/dx and d2y/dx2 into Equation (1.11.12) reduces the second-order
equation to the equivalent ﬁrst-order system
dy
dx = v,
(1.11.13)
dv
dy = F(y, v).
(1.11.14)
In this case, we ﬁrst solve Equation (1.11.14) for v as a function of y and then solve
Equation (1.11.13) for y as a function of x.
Example 1.11.2
Find the general solution to
d2y
dx2 = −
2
1 −y
!dy
dx
"2
.
(1.11.15)
Solution:
In this differential equation, the independent variable does not occur ex-
plicitly. Therefore, we let v = dy/dx and use the chain rule to obtain
d2y
dx2 = dv
dx = dv
dy
dy
dx = v dv
dy .
Substituting into Equation (1.11.15) results in the equivalent system
dy
dx = v,
(1.11.16)
v dv
dy = −
2
1 −y v2.
(1.11.17)
Separating the variables in the differential equation (1.11.17) gives
1
v dv = −
2
1 −y dy,
(1.11.18)
which can be integrated to obtain
ln |v| = 2 ln |1 −y| + c.

104
CHAPTER 1
First-Order Differential Equations
Combining the logarithm terms and exponentiating yields
v(y) = c1(1 −y)2,
(1.11.19)
where we have set c1 = ±ec. Notice that in solving Equation (1.11.17), we implicitly
assumed that v ̸= 0, since we divided by it to obtain Equation (1.11.18). However, the
general form (1.11.19) does include the solution v = 0, provided we allow c1 to equal
zero. Substituting for v into Equation (1.11.16) yields
dy
dx = c1(1 −y)2.
Separating the variables and integrating, we obtain
(1 −y)−1 = c1x + d1.
That is,
1 −y =
1
c1x + d1
.
Solving for y gives
y(x) = c1x + (d1 −1)
c1x + d1
,
(1.11.20)
which can be written in the simpler form
y(x) = x + a
x + b ,
(1.11.21)
where the constants a and b are deﬁned by a = (d1 −1)/c1, and b = d1/c1. Notice
that the form (1.11.21) does not include the solution y = constant, which is contained
in (1.11.20) (set c1 = 0). This is because in dividing by c1, we implicitly assumed that
c1 ̸= 0. Thus in specifying the solution in the form (1.11.21), we should also include the
statement that any constant function y = k (k a constant) is a solution.
□
Example 1.11.3
Determine the displacement at time t of a simple harmonic oscillator that is extended a
distance A units from its equilibrium position and released from rest at t = 0.
Solution:
According to the derivation in Section 1.1, the motion of the simple har-
monic oscillator is governed by the initial-value problem
d2y
dt2 = −ω2y,
(1.11.22)
y(0) = A,
dy
dt (0) = 0,
(1.11.23)
where ω is a positive constant. The differential equation (1.11.22) has the independent
variable t missing. We therefore let v = dy/dt and use the chain rule to write
d2y
dt2 = v dv
dy .
It then follows that Equation (1.11.22) can be replaced by the equivalent ﬁrst-order
system
dy
dt = v,
(1.11.24)
v dv
dy = −ω2y.
(1.11.25)

1.11
Some Higher-Order Differential Equations 105
Separating the variables and integrating Equation (1.11.25) yields
1
2v2 = −1
2ω2y2 + c,
which implies that
v = ±
)
c1 −ω2y2,
where c1 = 2c. Substituting for v into Equation (1.11.24) yields
dy
dt = ±
)
c1 −ω2y2.
(1.11.26)
Setting t = 0 in this equation and using the initial conditions (1.11.23), we ﬁnd that
c1 = ω2 A2. Equation (1.11.26) therefore gives
dy
dt = ±ω
)
A2 −y2.
By separating the variables and integrating, we obtain
arcsin(y/A) = ±ωt + b,
where b is an integration constant. Thus,
y(t) = A sin(b ± ωt).
The initial condition y(0) = A implies that sin b = 1, and so we can choose b = π/2.
We therefore have
y(t) = A sin(π/2 ± ωt).
That is,
y(t) = A cos ωt.
Consequently the predicted motion is that the mass oscillates between ±A for all t. This
solution makes sense physically since the simple harmonic oscillator does not include
dissipative forces that would slow the motion.
□
Remark
In Chapter 8 we will see how to solve the initial-value problem (1.11.22),
(1.11.23) in just a few lines of work without requiring any integration!
Exercises for 1.11
Skills
• Be familiar with the strategy of solving a higher-order
differential equation by replacing it with an equivalent
system of ﬁrst-order differential equations, and be able
to carry out this strategy in particular instances.
Problems
For Problems 1–14, solve the given differential equation.
1. y′′ −2y′ = 6e3x.
2. y′′ = 2x−1y′ + 4x2.
3. (x −1)(x −2)y′′ = y′ −1.
4. y′′ + 2y−1(y′)2 = y′.
5. y′′ = (y′)2 tan y.
6. y′′ + y′ tan x = (y′)2.
7. d2x
dt2 =
!dx
dt
"2
+ 2dx
dt .
8. y′′ −2x−1y′ = 6x4.
9. t d2x
dt2 = 2(t + dx
dt ).
10. y′′ −α(y′)2 −βy′ = 0, where α and β are nonzero
constants.

106
CHAPTER 1
First-Order Differential Equations
11. y′′ −2x−1y′ = 18x4.
12. (1 + x2)y′′ = −2xy′.
13. y′′ + y−1(y′)2 = ye−y(y′)3.
14. y′′ −y′ tan x = 1,
0 ≤x < π/2.
In Problems 15–16, solve the given initial-value problem.
15. yy′′ = 2(y′)2 + y2,
y(0) = 1,
y′(0) = 0.
16. y′′ = ω2y,
y(0) = a,
y′(0) = 0, where ω, a are
positive constants.
17. The following initial-value problem arises in the anal-
ysis of a cable suspended between two ﬁxed points
y′′ = 1
a
)
1 + (y′)2, y(0) = a, y′(0) = 0,
where a is a nonzero constant. Solve this initial-value
problem for y(x). The corresponding solution curve
is called a catenary.
18. Consider the general second-order linear differential
equation with dependent variable missing:
y′′ + p(x)y′ = q(x).
Replace this differential equation with an equivalent
pair of ﬁrst-order equations and express the solution
in terms of integrals.
19. Consider the general third-order differential equation
of the form
y′′′ = F(x, y′′).
(1.11.27)
(a) Show that Equation (1.11.27) can be replaced by
the equivalent ﬁrst-order system
du1
dx = u2, du2
dx = u3, du3
dx = F(x, u3),
where the variables u1, u2, u3 are deﬁned by
u1 = y,
u2 = y′,
u3 = y′′.
(b) Solve y′′′ = x−1(y′′ −1).
20. A simple pendulum consists of a particle of mass m
supported by a piece of string of length L. Assuming
that the pendulum is displaced through an angle θ0
radians from the vertical and then released from rest,
the resulting motion is described by the initial-value
problem
d2θ
dt2 + g
L sin θ = 0,
θ(0) = θ0,
dθ
dt (0) = 0.
(1.11.28)
(a) For small oscillations, θ << 1, we can use the
approximation sin θ ≈θ in Equation (1.11.28) to
obtain the linear equation
d2θ
dt2 + g
L θ = 0,
θ(0) = θ0,
dθ
dt (0) = 0.
Solve this initial-value problem for θ as a function
of t. Is the predicted motion reasonable?
(b) Obtain the following ﬁrst integral of (1.11.28):
dθ
dt = ±
8
2g
L (cos θ −cos θ0).
(1.11.29)
(c) Show from Equation (1.11.29) that the time T
(equal to one-fourth of the period of motion) re-
quired for θ to go from 0 to θ0 is given by the
elliptic integral of the ﬁrst kind
T =
>
L
2g
, θ0
0
1
√cos θ −cos θ0
dθ. (1.11.30)
(d) Show that (1.11.30) can be written as
T =
>
L
g
, π/2
0
1
*
1 −k2 sin2 u
du,
where k = sin(θ0/2). [Hint: First express cos θ
and cos θ0 in terms of sin2(θ/2) and sin2(θ0/2).]
1.12
Chapter Review
Basic Theory of Differential Equations
This chapter has provided an introduction to the theory of differential equations. A
differential equation is an equation involving one or more derivatives of an unknown
function, and the highest order derivative appearing in the equation is the order of the
differential equation.

1.12
Chapter Review 107
For an nth-order differential equation, the general solution to the differential equa-
tion contains n arbitrary constants, and all solutions to the differential equation can be
obtained by assigning appropriate values to the constants. This chapter is mainly con-
cerned with ﬁrst-order differential equations, which may be written in the form
dy
dx = f (x, y),
(1.12.1)
for some given function f . If we impose an initial condition specifying the value of a
solution y(x) to the differential equation (1.12.1) at a particular point x0, say y0 = y(x0),
then we have an initial-value problem:
dy
dx = f (x, y),
y(x0) = y0.
(1.12.2)
To solve an initial-value problem of the form (1.12.2), the ﬁrst step is to determine the
general solution to the differential equation (1.12.1), and then use the initial condition to
determine the speciﬁc value of the arbitrary constant appearing in the general solution.
Solution Techniques for First-Order Differential Equations
One of our main goals in this chapter is to ﬁnd solutions to ﬁrst-order differential equa-
tions of the form (1.12.1). There are various ways in which we can seek to ﬁnd these
solutions:
1. Geometrically: The function f (x, y) gives the slope of the tangent line to the
solution curves of the differential equation (1.12.1) at the point (x, y). Thus, by
computing f (x, y) for various points (x, y), we can draw small line segments
through the point (x, y) with slope f (x, y) to depict how a solution curve would
pass through (x, y). The resulting picture of line segments is called the slope ﬁeld
of the differential equation, and any solution curves to the differential equation in
the xy-plane must be tangent to the slope ﬁeld at all points.
For example, the differential equation dy
dx = −x
y determines a slope ﬁeld con-
sisting of small line segments that encircle the origin. Indeed, the solutions to this
differential equation consist of concentric circles centered at the origin.
One piece of theory is that different solution curves for the same differential equa-
tion can never cross (this essentially tells us that an initial-value problem cannot
have multiple solutions). Thus, for example, if we ﬁnd a solution to the differential
equation (1.12.1) of the form y(x) = y0, for some constant y0 (recall that such a
solution, is called an equilibrium solution), then all other solution curves to the
differential equation must lie entirely above the line y = y0 or entirely below the
line y = y0.
2. Numerically: Suppose we wish to approximate the solution to the initial-value
problem (1.12.2) at the point x = x1 = x0 + h, where h is small. Euler’s method
uses the slope of the solution at (x0, y0), which is f (x0, y0), to use a tangent line
approximation to the solution:
y(x) = y0 + f (x0, y0)(x −x0).
Therefore, we approximate
y(x1) = y0 + f (x0, y0)(x1 −x0) = y0 + h f (x0, y0).
Now, starting from the point (x1, y(x1)), we can repeat the process to ﬁnd ap-
proximations to the solutions at other points x2, x3, . . . . The conclusion is that the

108
CHAPTER 1
First-Order Differential Equations
approximation to the solution to the initial value problem (1.12.2) at the points
xn+1 = x0 + nh (n = 0, 1, . . . ) is
yn+1 = yn + h f (xn, yn),
n = 0, 1, . . .
In Section 1.10, other modiﬁcations to Euler’s method are also discussed.
3. Analytically: In some situations, we can explicitly obtain an equation for the
general solution to the differential equation (1.12.1). These include situations in
which the differential equation is separable, ﬁrst-order linear, homogeneous of
degree zero, Bernoulli, and/or exact. The table below shows the types of differential
equations we can solve analytically and a summary of the solution technique. If a
given differential equation cannot be written in one of these forms, then the next
step is to try to determine an integrating factor. If that fails, then we might try to
ﬁnd a change of variables that would reduce the differential equation to one of the
above types.
Type
Standard Form
Technique
Separable
p(y)y′ = q(x)
Separate the variables and integrate.
First-order
y′ + p(x)y = q(x)
Rewrite as d
dx (I · y) = I · q(x), where I = e
;
p(x) dx,
linear
and integrate with respect to x.
First-order
y′ = f (x, y) where
Change variables: y = xV (x) and reduce to a
homogeneous
f (tx, ty) = f (x, y)
separable equation.
Bernoulli
y′ + p(x)y = q(x)yn
Divide by yn and make the change of variables u = y1−n.
This reduces the differential equation to a linear equation.
Exact
M dx + N dy = 0,
The solution is φ(x, y) = c, where φ is determined by
with My = Nx.
integrating φx = M, φy = N.
Table 1.12.1: A summary of the basic solution techniques for y′ = f (x, y).
Example 1.12.1
Determine which of the above types, if any, the following differential equation falls into
dy
dx = −(8x5 + 3y4)
4xy3
.
Solution:
Since the given differential equation is written in the form dy/dx =
f (x, y), we ﬁrst check whether it is separable or homogeneous. By inspection, we
see that it is neither of these. We next check to see whether it is a linear or a Bernoulli
equation. We therefore rewrite the equation in the equivalent form
dy
dx + 3
4x y = −2x4y−3,
(1.12.3)
which we recognize as a Bernoulli equation with n = −3. We could therefore solve the
equation using the appropriate technique. Due to the y−3 term in Equation (1.12.3) it
follows that the equation is not a linear equation. Finally, we check for exactness. The
natural differential form to try for the given differential equation is
(8x5 + 3y4) dx + 4xy3 dy = 0.
(1.12.4)

1.12
Chapter Review 109
In this form, we have
My = 12y3,
Nx = 4y3,
so that the equation is not exact. However, we see that
(My −Nx)/N = 2x−1,
so that according to Theorem 1.9.11, I (x) = x2 is an integrating factor. Therefore, we
could multiply Equation (1.12.4) by x2 and then solve it as an exact equation.
□
Examples of First-Order Differential Equations
There are numerous real-world examples of ﬁrst-order differential equations. Among
the applications discussed in this chapter are Malthusian and logistic population models,
Newton’s Law of Cooling, families of orthogonal trajectories, ontogenetic growth model,
chemical reactions, mixing problems, electric circuits, and others.
Additional Problems
1. A boy 2 meters tall shoots a toy rocket straight up
from head level at 10 meters per second. Assume the
acceleration of gravity is 9.8 meters/sec2.
(a) What is the highest point above the ground
reached by the rocket?
(b) When does the rocket hit the ground?
2. A racquetball player standing at the back wall of the
court hits the ball from a height of 2 feet horizontally
toward the front wall at 80 miles per hour. The length
of a regulation racquetball court is 40 feet. Does the
ball reach the front wall before hitting the ground?
Neglect air resistance, and assume the acceleration of
gravity is 32 feet/sec2.
In Problems 3–6, ﬁnd the equation of the orthogonal trajec-
tories to the given family of curves.
3. y = cx3.
4. y = ln(cx).
5. y2 = cx3.
6. x4 + y4 = c.
7. Consider the family of curves
x2 + 3y2 = 2cy.
(1.12.5)
(a) Show that the differential equation of this family
is
dy
dx =
2xy
x2 −3y2 .
(b) Determine the orthogonal trajectories to the
family (1.12.5).
In Problems 8–12, sketch the slope ﬁeld and some represen-
tative solution curves for the given differential equation.
8. y′ = y(y −1)2.
9. y′ = (y −3)(y + 1).
10. y′ = y(2 −y)(1 −y).
11. y′ = y/x2.
12. y′ = 2xy.
13. At time t the velocity, v(t), of an object is governed
by the differential equation
dv
dt = 1
2(25 −v),
t > 0.
(a) Verify that v(t) = 25 is a solution to this differ-
ential equation.
(b) Sketch the slope ﬁeld for 0 ≤v ≤25. What
happens to v(t) as t →∞?

110
CHAPTER 1
First-Order Differential Equations
14. Consider the ontogenetic model
dm
dt = am3/4
%
1 −
# m
M
$1/4&
.
(a) Determine all equilibrium solutions.
(b) Explain why dm
dt > 0.
(c) Determine where in the region of physical interest
the solution curves are concave up, and where they are
concave down.
(d) Determine the slope of the solution curves at the
point of inﬂection.
(e) Sketch the slope ﬁeld and include some represen-
tative solution curves.
15. An object of mass m is released from rest in a medium
in which the frictional forces are proportional to the
square of the velocity. The initial-value problem that
governs the subsequent motion is
mv dv
dy = mg −kv2,
v(0) = 0,
(1.12.6)
where v(t) denotes the velocity of the object at time
t, y(t) denotes the distance travelled by the object at
time t as measured from the point at which the object
was released, and k is a positive constant.
(a) Solve (1.12.6) and show that
v2 = mg
k (1 −e−2ky/m).
(b) Make a sketch of v2 as a function of y.
In Problems 16–42, determine which of the ﬁve types of dif-
ferential equations we have studied the given equation falls
into (see Table 1.12.1), and use an appropriate technique to
ﬁnd the general solution.
16. dy
dx = x2y(y −1).
17. dy
dx = 2 ln x
xy .
18. xy′ −2y = 2x2 ln x.
19. dy
dx = −
2xy
x2 + 2y .
20. (y2 + 3xy −x2) dx −x2 dy = 0.
21. y′ + y(tan x + y sin x) = 0.
22. dy
dx +
2e2x
1 + e2x y =
1
e2x −1.
23. y′ −x−1y = x−1*
x2 −y2, × > 0.
24. dy
dx = sin y + y cos x + 1
1 −x cos y −sin x .
25. dy
dx + 1
x y = 25x2 ln x
2y
.
26. e2x+ydy −ex−ydx = 0.
27. y′ + y cot x = sec x.
28. dy
dx +
2ex
1 + ex y = 2√ye−x.
29. y[ln(y/x) + 1]dx −xdy = 0.
30. (1 + 2xey)dx −(ey + x)dy = 0.
31. y′ + y sin x = sin x.
32. (3y2 + x2)dx −2xydy = 0.
33. 2x(ln x)y′ −y = −9x3y3 ln x.
34. (1 + x)y′ = y(2 + x).
35. (x2 −1)(y′ −1) + 2y = 0.
36. x sec2(xy)dy = −
<
y sec2(xy) + 2x
=
dx.
37. dy
dx =
x2
x2 −y2 + y
x .
38. dy
dx = x2 + y2
x2 −y2 .
39. dy
dx + y
x = (25 ln x)
(2x3y) .
40. y′ + x2y = (xy)3.
41. (1 + y)y′ = xe(x−y).
42. y′ = cos x(y csc x −1).
For Problems 43–46, determine which of the ﬁve types of
differential equations we have studied the given differential
equation falls into, and use an appropriate technique to ﬁnd
the solution to the initial-value problem.
43. y′ −x2y = x2, y(0) = 5.
44. e−3x+2ydx + ex−4ydy = 0, y(0) = 0.

1.12
Chapter Review 111
45. (3x2 + 2xy2) dx + (2x2y) dy = 0, y(1) = 3.
46. dy
dx −(sin x)y = e−cos x, y(0) = 1
e .
47. Determine all values of the constants m and n, if there
are any, for which the differential equation
(x5 + ym) dx −xny3dy = 0
is each of the following:
(a) Exact.
(b) Separable.
(c) Homogeneous.
(d) Linear.
(e) Bernoulli.
48. A man’s sandals are moved from poolside (80◦F) to
a sauna (180◦F) to warm and dry them. If they are
100◦F after 3 minutes in the sauna, how much time
is required in the sauna to increase the temperature of
the sandals to 140◦F, according to Newton’s Law of
Cooling?
49. A hot plate (150◦F) is placed on a countertop in a
room kept at 70◦F. If the plate cools 25◦F in the ﬁrst
10 minutes, when does the plate reach 100◦F accord-
ing to Newton’s Law of Cooling?
50. A simple nonlinear law of cooling states that the rate
of change of temperature of an object is proportional
to the square of the temperature difference between
the object and its surrounding medium (you may as-
sume that the temperature of the surrounding medium
is constant). Set up and solve the initial-value problem
that governs this cooling process if the initial temper-
ature is T0. What happens to the temperature of the
object as t →∞?
51. The velocity (meters/second) of an object at time t
(seconds) is governed by the differential equation
dv
dt + kv = 80ke−kt
with initial conditions
v(0) = 20,
dv
dt (0) = 2.
(a) How is the velocity of the object changing at
t = 0?
(b) Determine the value of the constant k.
(c) Determine the velocity of the object at time t.
(d) Is there a ﬁnite time t > 0 at which the object is
at rest? Explain.
(e) What happens to the velocity of the object as
t →∞?
52. The temperature of an object at time t is governed by
the linear differential equation
dT
dt = −k(T −5 cos 2t).
At t = 0, the temperature of the object is 0◦F and is,
at that time, increasing at a rate of 5◦F/min.
(a) Determine the value of the constant k.
(b) Determine the temperature of the object at
time t.
(c) Describe the behavior of the temperature of the
object for large values of t.
53. Each spring, sandhill cranes migrate through the Platte
River valley in central Nebraska. An estimated maxi-
mum of a half-million of these birds reach the region
by April 1 each year. If there are only 100,000 sandhill
cranes 15 days later and the sandhill cranes leave the
Platte River valley at a rate proportional to the number
of sandhill cranes still in the valley at the time,
(a) How many sandhill cranes remain in the valley
30 days after April 1?
(b) How many sandhill cranes remain in the valley
35 days after April 1?
(c) How many days after April 1 will there be less
than 1000 sandhill cranes in the valley?
54. A city’s population in the year 2008 was 200,000, in
2011itwas230,000,andin2014itwas250,000.Using
the logistic model of population, predict the popula-
tion in 2018 and 2028.
55. Consider an RC circuit with R = 4*, C = 1
5 F, and
E(t) = 6 cos 2t V. If q(0) = 3 C, determine the cur-
rent in the circuit for t ≥0.
56. Consider the RL circuit with R = 3*, L = 0.3 H, and
E(t) = 10 V. If i(0) = 3 A, determine the current in
the circuit for t ≥0.
57. A solution containing 3 grams/L of a salt solution
pours into a tank, initially half full of water, at a
rate of 6 L/min. The well-stirred mixture ﬂows out at

112
CHAPTER 1
First-Order Differential Equations
arateof4L/min.Ifthetankholds60L,ﬁndtheamount
of salt (in grams) in the tank when the solution over-
ﬂows.
In Problems 58–59, use Euler’s method with the speciﬁed
step size to determine the solution to the given initial-value
problem at the speciﬁed point.
58. y′ = x2 + 2y2,
y(0) = −3, h = 0.1,
y(1).
59. y′ = 3x
y + 2,
y(1) = 2, h = 0.05,
y(1.5).
In Problems 60–61, use the modiﬁed Euler method with the
speciﬁed step size to determine the solution to the given
initial-value problem at the speciﬁed point. In each case,
compare your answer to that determined by using Euler’s
method.
60. The initial-value problem in Problem 58.
61. The initial-value problem in Problem 59.
In Problems 62–63, use the Fourth-Order Runge-Kutta
method with the speciﬁed step size to determine the solu-
tion to the given initial-value problem at the speciﬁed point.
In each case, compare your answer to that determined by
using Euler’s method.
62. The initial-value problem in Problem 58.
63. The initial-value problem in Problem 59.
Project: A Cylindrical Tank Problem
Consider an open cylindrical tank of height h0 meters and radius r meters that is ﬁlled
with water. A circular hole of radius l meters in the bottom of the tank allows the water
to ﬂow out under the inﬂuence of gravity. According to Torricelli’s law, the water ﬂows
out with the same speed that it would acquire in falling freely from the water level in the
tank to the hole.
1. Use Torricelli’s law to derive the following equation for the rate of change of
volume of water in the tank
dV
dt = −a
*
2gh
where h(t) denotes the height of water in the tank at time t, a denotes the area of
the hole, and g denotes the acceleration due to gravity. [Hint: First show that an
object that is released from rest at a height h hits the ground with a speed √2gh.
Then consider the change in the volume of water in the tank in a time interval )t.]
2. Show that the rate of change of volume of water in the tank is also given by
dV
dt = πr2 dh
dt .
3. Using the results from problems (1) and (2), determine the height of the water in
the tank at time t, and show that the tank will empty when t = te, where
te = πr2
a
>
2h0
g .
4. Suppose now that starting at t = 0 chemical is added to the water in the tank at a
rate of w grams/second. Derive the following differential equation governing the
amount of chemical, A(t), in the tank at time t:
d A
dt −
2
t −te
A = w,
0 < t < te.
(1.12.7)
5. Solve the differential equation (1.12.7). Determine the time when A(t) is a maxi-
mum.

1.12
Chapter Review 113
6. By making an appropriate change of variables in the differential equation (1.12.7),
derive a differential equation for the concentration c(t) of chemical in the tank at
time t. Solve your differential equation and verify that you get the same expression
for c(t) as you do by dividing the expression for A(t) obtained in the previous
problem by V (t).
7. In the particular case when h0 = 16 m, r = 5 m, l = 0.1 m, and w = 15 g/s, determine
te, and the time when the concentration of chemical in the tank reaches 1 g/L.

2
Matrices and Systems of
Linear Equations
In the theory of equations, the simplest equations are the linear ones. Examples of linear
equations include 3x −8y = −5 and 4x1 −x2 + 3x3 + x4 = 6. More generally, any
equation of the form
a1x1 + a2x2+ · · · + anxn = b
in constants a1, a2, . . . , an and b and unknowns x1, x2, . . . , xn is called a linear equation.
We will see in the later chapters that many problems in linear algebra can be reduced to
studying such equations. Often, several linear equations need to be considered at once,
in which case we can refer to a system of linear equations. The next two chapters are
concerned with giving a detailed introduction to the theory and solution techniques for
such systems. An example of a linear system of equations in the unknowns x1, x2, x3 is
5x1 −x2 −3x3 = −1,
−x1 + 4x2 −8x3 =
2,
6x1
+ 8x3 = −5.
We see that this system is completely determined by the array of numbers
⎡
⎣
5
−1
−3
−1
−1
4
−8
2
6
0
8
−5
⎤
⎦,
which contains the coefﬁcients of the unknowns on the left-hand side of the system and
the numbers appearing on the right-hand side of the system. Such an array is an example
of a matrix. In this chapter we see that, in general, linear systems of equations are best
represented in terms of matrices and that, once such a representation has been made, the
set of all solutions to the system can be easily determined. In the ﬁrst few sections of
114

2.1
Matrices: Definitions and Notation 115
this chapter we therefore introduce the basics of matrix algebra. We then apply matrices
to solve systems of linear equations. In Chapter 9 we will see how matrices also give a
natural framework for formulating and solving systems of linear differential equations.
2.1
Matrices: Definitions and Notation
We begin our discussion of matrices with a deﬁnition.
DEFINITION
2.1.1
An m × n (read “m by n”) matrix is a rectangular array of numbers arranged in m
horizontal rows and n vertical columns. Matrices are usually denoted by upper case
letters, such as A and B. The entries in the matrix are called the elements of the
matrix.
Example 2.1.2
The following are examples of a 3 × 3 and a 4 × 2 matrix, respectively:
A =
⎡
⎣
9
3
−2
−5
2
0
0
−7
8
⎤
⎦,
B =
⎡
⎢⎢⎣
−1
0
3
−5
−6
7/2
−1
−3
⎤
⎥⎥⎦.
□
We will use the index notation to denote the elements of a matrix. According to this
notation, the element in the ith row and jth column of the matrix A will be denoted ai j.
Thus, for the matrices in the previous example we have
a13 = −2,
a22 = 2,
b32 = 7
2,
etc.
Using the index notation, a general m × n matrix A is written
A =
⎡
⎢⎢⎢⎣
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
⎤
⎥⎥⎥⎦,
or, in a more abbreviated form, A = [ai j].
Remark
The expression m × n representing the number of rows and columns of a
general matrix A is sometimes informally called the size of the matrix A. The numbers
m and n themselves are sometimes called the dimensions1 of the matrix A.
Next we deﬁne what is meant by equality of matrices.
DEFINITION
2.1.3
Two matrices A and B are equal, written A = B, if
1. They both have the same size, m × n.
2. All corresponding elements in the matrices are equal: ai j
= bi j for all
i and j with 1 ≤i ≤m and 1 ≤j ≤n.
1Be careful not to confuse this usage of the term with the dimension of a vector space, which will be
introduced in Chapter 4.

116
CHAPTER 2
Matrices and Systems of Linear Equations
According to Deﬁnition 2.1.3, even though the matrices
A =
'3
−1
0
2
−6
−2
(
and
B =
⎡
⎣
−6
2
0
3
−2
−1
⎤
⎦
contain the same six numbers, and therefore store the same basic information, they are
not equal as matrices.
Row Vectors and Column Vectors
Of particular interest to us in the future will be 1 × n and n × 1 matrices. For this reason
we give them special names.
DEFINITION
2.1.4
A 1×n matrix is called a row n-vector. An n ×1 matrix is called a column n-vector.
The elements of a row or column n-vector are called the components of the vector.
Remarks
1. We can refer to the objects just deﬁned simply as row vectors and column vectors
if the value of n is clear from the context.
2. We will see later in this chapter that when a system of linear equations is written
using matrices, the basic unknown in the reformulated system is a column vector.
A similar formulation will also be given in Chapter 9 for systems of differential
equations.
Example 2.1.5
The matrix a =
'
−2
1
3
5
(
is a row 3-vector and b =
⎡
⎢⎢⎣
3
0
−1
−1
⎤
⎥⎥⎦is a column
4-vector.
□
As indicated in the above example, we usually denote a row or column vector by a
lowercase letter in bold print.
Associated with any m × n matrix are m row n-vectors and n column m-vectors.
These are referred to as the row vectors of the matrix and the column vectors of the
matrix, respectively.
Example 2.1.6
Associated with the matrix A =
⎡
⎣
2
0
−4
9
−3
−1
4
1
8
−3
−3
2
⎤
⎦are the row 4-vectors
)2
0
−4
9*
,
)−3
−1
4
1*
,
and
)8
−3
−3
2*
,
and the column 3-vectors
⎡
⎣
2
−3
8
⎤
⎦,
⎡
⎣
0
−1
−3
⎤
⎦,
⎡
⎣
−4
4
−3
⎤
⎦,
and
⎡
⎣
9
1
2
⎤
⎦.
□

2.1
Matrices: Definitions and Notation 117
Conversely, if a1, a2, . . . , an are each column m-vectors, then we let [a1, a2, . . . , an]
denote the m × n matrix whose column vectors are a1, a2, . . . , an. Similarly, if
b1, b2, . . . , bm are each row n-vectors, then we write
⎡
⎢⎢⎢⎣
b1
b2
...
bm
⎤
⎥⎥⎥⎦
for the m × n matrix with row vectors b1, b2, . . . , bm. The reader should observe that
a list of vectors arranged in a row will always consist of column vectors, while a list of
vectors arranged in a column will always consist of row vectors.
Example 2.1.7
If a1 =
'−1
−7
(
, a2 =
' 0
−5
(
, and a3 =
'−2
4
(
, then
[a1, a2, a3] =
'−1
0
−2
−7
−5
4
(
.
□
DEFINITION
2.1.8
If we interchange the row vectors and column vectors in an m ×n matrix A, we obtain
an n × m matrix called the transpose of A. We denote this matrix by AT . In index
notation, the (i, j)-th element of AT , denoted aT
i j, is given by
aT
i j = a ji.
Example 2.1.9
If A =
'−5
3
0
−4
1
8
−4
−4
2
3
(
and B =
⎡
⎣
2
6
−2
0
−3
3
−5
−1
1
⎤
⎦, then
AT =
⎡
⎢⎢⎢⎢⎣
−5
8
3
−4
0
−4
−4
2
1
3
⎤
⎥⎥⎥⎥⎦
and
BT =
⎡
⎣
2
0
−5
6
−3
−1
−2
3
1
⎤
⎦.
□
Square Matrices
An n × n matrix is called a square matrix, since it has the same number of rows as
columns. If A is a square matrix, then the elements aii, 1 ≤i ≤n, make up the main
diagonal, or leading diagonal, of the matrix. (See Figure 2.1.1 for the 3 × 3 case.)
a31
a32
a33
a21
a22
a23
a11
a12
a13
Figure 2.1.1: The main diagonal of a 3 × 3 matrix.

118
CHAPTER 2
Matrices and Systems of Linear Equations
The sum of the main diagonal elements of an n × n matrix A is called the trace of
A and is denoted tr(A). Thus,
tr(A) = a11 + a22 + · · · + ann.
DEFINITION
2.1.10
An n × n matrix A = [ai j] is said to be lower triangular if ai j = 0 whenever
i < j (zeros everywhere above (i.e., “northeast of") the main diagonal), and it is said
to be upper triangular if ai j = 0 whenever i > j (zeros everywhere below (i.e.,
“southwest of”) the main diagonal). An n×n matrix D = [di j] is said to be a diagonal
matrix if di j = 0 whenever i ̸= j (zeros everywhere off the main diagonal).
Observe that the transpose of a lower (upper) triangular matrix is an upper (lower)
triangular matrix.
Example 2.1.11
The matrices
A =
⎡
⎣
−3
3
4
0
−5
1
0
0
9
⎤
⎦
and
B =
⎡
⎣
−5
0
0
0
4
0
2
−2
−7
⎤
⎦
are upper triangular and lower triangular, respectively. The matrix
D =
⎡
⎣
0
0
0
0
−2
0
0
0
5
⎤
⎦
is a diagonal matrix.
□
We will see many times later in the text that diagonal matrices hold an important place
in linear algebra, in large part because of their computational simplicity with respect
to matrix multiplication (see Section 2.2). Note that a matrix D is a diagonal matrix if
and only if D is simultaneously upper and lower triangular. Since a diagonal matrix is
completely determined by giving its main diagonal elements, we can specify a diagonal
matrix in the compact form
D = diag(d1, d2, . . . , dn),
where di denotes the diagonal element dii.
Example 2.1.12
The 4 × 4 diagonal matrix D = diag(−5, 0, −9, 4) is
D =
⎡
⎢⎢⎣
−5
0
0
0
0
0
0
0
0
0
−9
0
0
0
0
4
⎤
⎥⎥⎦.
This example illustrates that while any entries of a diagonal matrix that are off the main
diagonal must be zero, entries that lie on the main diagonal may also be zero.
□
If every element on the main diagonal of a lower (upper) triangular matrix is a 1,
the matrix is called a unit lower (upper) triangular matrix.

2.1
Matrices: Definitions and Notation 119
The transpose naturally picks out two important types of square matrices as follows.
DEFINITION
2.1.13
1. A square matrix A satisfying AT = A is called a symmetric matrix.
2. If A = [ai j],thenwelet−A denotethematrixwithelements−ai j.Asquarema-
trix A satisfying AT = −A, is called a skew-symmetric (or anti-symmetric)
matrix.
Example 2.1.14
The matrices
A1 =
!−7
2
2
4
"
and
A2 =
⎡
⎢⎢⎣
3
−6
0
−1
−6
4
1
−2
0
1
8
−9
−1
−2
−9
2
⎤
⎥⎥⎦
are both symmetric, whereas the matrices
B1 =
⎡
⎣
0
1
2
−1
0
3
−2
−3
0
⎤
⎦
and
B2 =
⎡
⎢⎢⎣
0
5
−1
−7
−5
0
4
1
1
−4
0
2
7
−1
−2
0
⎤
⎥⎥⎦
are both skew-symmetric.
□
Notice that the main diagonal elements of the skew-symmetric matrices in the pre-
ceding example are all zero. This is true in general, since if A is a skew-symmetric
matrix, then ai j = −a ji, which implies that when i = j, aii = −aii, so that aii = 0.
On the other hand, the main diagonal entries of a symmetric matrix are arbitrary. They
need not be zero, nor need they all be identical to one another.
It will be useful as we move through this text to have the form of a symmetric or
skew-symmetric matrix in mind. For instance,
(a): the general form of a 3 × 3 symmetric matrix is
⎡
⎣
a
b
c
b
d
e
c
e
f
⎤
⎦;
(b): the general form of a 3 × 3 skew-symmetric matrix is
⎡
⎣
0
x
y
−x
0
z
−y
−z
0
⎤
⎦.
Matrix and Vector Functions
Later in the text we will be concerned with systems of two or more differential equations.
The most effective way to study such systems, as it turns out, is to represent the system
using matrices and vectors. However, we will need to allow the elements of the matrices
and vectors that arise to contain functions of a single variable, not just real or complex
numbers. This leads to the following deﬁnition, reminiscent of Deﬁnition 2.1.1.
DEFINITION
2.1.15
An m ×n matrix function A is a rectangular array with m rows and n columns whose
elements are functions of a single real variable t. The matrix function is only deﬁned
for real values of t such that all elements in A(t) assume a well-deﬁned value.

120
CHAPTER 2
Matrices and Systems of Linear Equations
Example 2.1.16
Here are two examples of matrix functions, A and B, with formulas given by:
A(t) =
'
t3
e2t
−3
ln t
1 −et
sin t
(
and
B(t) =
⎡
⎣
tan t
esin t
−2
6 −t
−5
1 + 2t2
⎤
⎦.
The function A is only deﬁned for real values of t with t > 0 since ln t is only deﬁned
for t > 0. The reader should determine the values of t for which the matrix function B
is deﬁned.
□
Remark
It is possible, of course, to consider matrix functions of more than one
variable. However, this will not be particularly relevant for our purposes in this text.
Finally in this section, we have the following special type of matrix function.
DEFINITION
2.1.17
An n × 1 matrix function is called a column n-vector function.
For instance,
'
t2
−6tet
(
is a column 2-vector function.2
Exercises for 2.1
Key Terms
Matrices, Elements, Size (dimensions) of a matrix, Row
vector, Column vector, Square matrix, Main diagonal,
Trace, Lower (Upper) triangular matrix, Unit lower (up-
per) triangular matrix, Diagonal matrix, Symmetric matrix,
Skew-symmetric matrix, Matrix function, Column n-vector
function.
Skills
• Be able to determine the elements of a matrix.
• Be able to identify the size (i.e., dimensions) of a
matrix.
• Be able to identify the row and column vectors of a
matrix.
• Be able to determine the components of a row or col-
umn vector.
• Be able to say whether or not two given matrices are
equal.
• Be able to ﬁnd the transpose of a matrix.
• Be able to compute the trace of a square matrix.
• Be able to recognize square matrices that are upper
triangular, lower triangular, or diagonal.
• Be able to recognize square matrices that are symmet-
ric or skew-symmetric.
• Be able to determine the values of the variable t such
that a matrix function A is deﬁned.
True-False Review
For items (a)–(m), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A diagonal matrix must be both upper triangular and
lower triangular.
(b) An m × n matrix has m column vectors and n row
vectors.
(c) The matrix
⎡
⎣
0
0
0
0
0
0
0
0
0
⎤
⎦is a diagonal matrix.
(d) The matrix
' 4
−2
2
0
(
is skew-symmetric.
2We could, of course, also speak of row n-vector functions as the 1 × n matrix functions, but we will not
need them in this text.

2.1
Matrices: Definitions and Notation 121
(e) The general form of a 4 × 4 symmetric matrix is
⎡
⎢⎢⎣
a
b
c
d
b
a
e
f
c
e
a
g
d
f
g
a
⎤
⎥⎥⎦.
(f) If A is a symmetric matrix, then so is AT .
(g) The trace of a matrix is the product of the elements
along the main diagonal.
(h) A skew-symmetric matrix must have zeros along the
main diagonal.
(i) A matrix that is both symmetric and skew-symmetric
cannot contain any nonzero elements.
(j) The
matrix
functions
⎡
⎣
√t
3t2
1
|t|
sin 2t
⎤
⎦
and
' −2 + t
ln t
esin t
−3
(
are deﬁned for exactly the same
values of t.
(k) The matrix function
⎡
⎢⎢⎣
cos t
t2
−2
−t
et
1
√t −3
⎤
⎥⎥⎦is deﬁned
for all positive real numbers t.
(l) Any matrix of numbers is a matrix function deﬁned
for all real values of the variable t.
(m) If A and B are matrix functions such that the matrices
A(0) and B(0) are the same, then A and B are the
same matrix function.
Problems
1. If A =
⎡
⎣
1
−2
3
2
7
−6
5
−1
0
2
−3
4
⎤
⎦, determine
(a) a31, a24, a14, a32, a21, and a34,
(b) all pairs (i, j) such that ai j = 2.
2. If B =
⎡
⎢⎢⎢⎢⎣
7
−1
−1
−1
0
3
−5
−1
4
0
6
8
−1
9
1
⎤
⎥⎥⎥⎥⎦
, determine
(a) b12, b33, b41, b43, b51, and b52,
(b) all pairs (i, j) such that bi j = −1.
For Problems 3–9, write the matrix with the given elements.
In each case, specify the dimensions of the matrix.
3. a11 = 1, a21 = −1, a12 = 5, a22 = 3.
4. a11 = 2, a12 = 1, a13 = −1, a21 = 0, a22 = 4,
a23 = −2.
5. a11 = −1, a41 = −5, a31 = 1, a21 = 1.
6. a11 = 1, a31 = 2, a42 = −1, a32 = 7, a13 = −2,
a23 = 0, a33 = 4, a21 = 3, a41 = −4, a12 = −3,
a22 = 6, a43 = 5.
7. a12 = −1, a13 = 2, a23 = 3, a ji = −ai j, 1 ≤i ≤3,
1 ≤j ≤3.
8. ai j = i −j, 1 ≤i ≤4, 1 ≤j ≤4.
9. ai j = i + j, 1 ≤i ≤4, 1 ≤j ≤4.
For Problems 10–12, determine tr(A) for the given matrix.
10. A =
' 1
0
2
3
(
.
11. A =
⎡
⎣
1
2
−1
3
2
−2
7
5
−3
⎤
⎦.
12. A =
⎡
⎣
2
0
1
3
2
5
0
1
−5
⎤
⎦.
For Problems 13–15, write the column vectors and row vec-
tors of the given matrix.
13. A =
' 1
−1
3
5
(
.
14. A =
⎡
⎣
1
3
−4
−1
−2
5
2
6
7
⎤
⎦.
15. A =
' 2
10
6
5
−1
3
(
.
16. If a1 = [1
2], a2 = [3
4], and a3 = [5
1], write
the matrix
A =
⎡
⎣
a1
a2
a3
⎤
⎦,
and determine the column vectors of A.
17. If a1 = [−2
0
4
−1
−1] and
a2 = [9
−4
−4
0
8], write the matrix
A =
' a1
a2
(
,
and determine the column vectors of A.

122
CHAPTER 2
Matrices and Systems of Linear Equations
18. If
b1 =
⎡
⎢⎢⎢⎢⎣
−2
−6
3
−1
−2
⎤
⎥⎥⎥⎥⎦
and
b2 =
⎡
⎢⎢⎢⎢⎣
−4
−6
0
0
1
⎤
⎥⎥⎥⎥⎦
,
write the matrix B = [b1, b2], and determine the row
vectors of B.
19. If
b1 =
⎡
⎣
2
−1
4
⎤
⎦,
b2 =
⎡
⎣
5
7
−6
⎤
⎦,
b3 =
⎡
⎣
0
0
0
⎤
⎦,
b4 =
⎡
⎣
1
2
3
⎤
⎦,
write the matrix B = [b1, b2, b3, b4], and determine
the row vectors of B.
20. If a1, a2, . . . , ap are each column q-vectors, what are
the dimensions of the matrix that has a1, a2, . . . , ap
as its column vectors?
For Problems 21–26, give an example of a matrix of the spec-
iﬁed form. (In some cases, many examples may be possible.)
21. 3 × 3 diagonal matrix.
22. 4 × 4 upper triangular matrix.
23. 4 × 4 skew-symmetric matrix.
24. 3 × 3 upper triangular symmetric matrix.
25. 3 × 3 lower triangular skew-symmetric matrix.
26. 3 × 3 symmetric and skew-symmetric matrix.
For Problems 27–30, given an example of a matrix function
of the speciﬁed form. (Many examples may be possible.)
27. 4 × 2 matrix function A such that
A(0) = A(1) ̸= A(2).
28. 2×3 matrix function deﬁned only for values of t with
−2 ≤t < 3.
29. 2 × 1 matrix function A that is nonconstant such that
all elements of A(t) are in [0, 1] for every t in R.
30. 1 × 5 matrix function A that is nonconstant such that
all elements of A(t) are positive for all t in R.
31. Construct distinct matrix functions A and B deﬁned
on all of R such that A(0) = B(0) and A(1) = B(1).
32. Show that an n ×n symmetric upper triangular matrix
is diagonal. [Hint: This amounts to showing that if
i ̸= j, then ai j = 0.]
33. Show that if A is an n×n matrix that is both symmetric
and skew-symmetric, then every element of A is zero.
(Such a matrix is called a zero matrix.)
2.2
Matrix Algebra
In the previous section we introduced the general idea of a matrix. The next step is to
develop the algebra of matrices. Unless otherwise stated, we assume that all elements of
the matrices that appear are real or complex numbers.
Addition and Subtraction of Matrices and
Multiplication of a Matrix by a Scalar
Addition and subtraction of matrices is only deﬁned for matrices with the same dimen-
sions. We begin with addition.
DEFINITION
2.2.1
If A and B are both m × n matrices, then we deﬁne addition (or the sum) of A and
B, denoted by A + B, to be the m × n matrix whose elements are obtained by adding
corresponding elements of A and B. In index notation, if A = [ai j] and B = [bi j],
then A + B = [ai j + bi j].

2.2
Matrix Algebra 123
Example 2.2.2
If
A =
'−2
1
0
4
−1
−3
2
2
(
and
B =
' 6
3
−3
3
−3
3
−2
−5
(
,
we have
A + B =
' 4
4
−3
7
−4
0
0
−3
(
.
On the other hand, if C =
' 3
0
7
−4
1
1
(
, then A + C is not deﬁned since A and C have
different sizes.
□
Properties of Matrix Addition: If A and B are both m × n matrices, then
A + B = B + A
(Matrix addition is commutative),
A + (B + C) = (A + B) + C
(Matrix addition is associative).
Both of these properties follow directly from Deﬁnition 2.2.1.
In order that we can model oscillatory physical phenomena, in much of the later
work we will need to use complex as well as real numbers. Throughout the text we will
use the term scalar to mean a real or complex number.
DEFINITION
2.2.3
If A is an m ×n matrix and s is a scalar, then we let s A denote the matrix obtained by
multiplying every element of A by s. This procedure is called scalar multiplication.
In index notation, if A = [ai j], then s A = [sai j].
Example 2.2.4
If A =
' 4
0
−3
−1
1
−5
(
and B =
'−2 + i
3
4i
−5 + 2i
(
, determine −7A and (−1+2i)B.
Solution:
We have
−7A =
'−28
0
21
7
−7
35
(
and
(−1 + 2i)B = (−1 + 2i)
'−2 + i
3
4i
−5 + 2i
(
=
'
−5i
−3 + 6i
−8 −4i
1 −12i
(
.
□
Further properties satisﬁed by the operations of matrix addition and multiplication
of a matrix by a scalar are as follows:
Properties of Scalar Multiplication: For any scalars s and t, and for any matrices A
and B of the same size,
1A = A
(Unit property),
s(A + B) = s A + sB
(Distributivity of scalars over matrix addition),
(s + t)A = s A + t A
(Distributivity of scalar addition over matrices),
s(t A) = (st)A = (ts)A = t(s A)
(Associativity of scalar multiplication).
Next we turn our attention to subtraction of matrices, which is deﬁned by using both
addition and scalar multiplication of matrices.

124
CHAPTER 2
Matrices and Systems of Linear Equations
DEFINITION
2.2.5
If A and B are both m ×n matrices, then we deﬁne subtraction of these two matrices
by
A −B = A + (−1)B.
In index notation A −B = [ai j −bi j]. That is, we subtract corresponding elements.
Example 2.2.6
With
A =
'−2
1
0
4
−1
−3
2
2
(
and
B =
' 6
3
−3
3
−3
3
−2
−5
(
,
we have
A −B =
'−8
−2
3
1
2
−6
4
7
(
and
A −A =
'0
0
0
0
0
0
0
0
(
.
□
The matrix A −A in the previous example has a special name. Formally, the m × n
zero matrix, denoted 0m×n (or simply 0, if the dimensions are clear), is the m ×n matrix
whose elements are all zeros. In the case of the n × n zero matrix, we may write 0n. We
now collect a few properties of the zero matrix. The ﬁrst of these below indicates that
the zero matrix plays a similar role in matrix addition to that played by the number zero
in the addition of real numbers.
Properties of the Zero Matrix: For all matrices A and the zero matrix of the same size,
we have
A + 0 = A,
A −A = 0,
and
0A = 0.
Note that in the last property here, the zero on the left side of the equation is a scalar,
while the zero on the right side of the equation is a matrix.
Multiplication of Matrices
The deﬁnition we introduced above for how to multiply a matrix by a scalar is essentially
the only possibility if, in the case when s is a positive integer, we want s A to be the same
matrix as the one obtained when A is added to itself s times. We now deﬁne how to
multiply two matrices together. In this case the multiplication operation is by no means
obvious. However, in Chapter 6 when we study linear transformations, the motivation for
the matrix multiplication procedure we are deﬁning here will become quite transparent
(see Theorem 6.5.7).
We will build up to the general deﬁnition of matrix multiplication in three stages.
CASE 1: Product of a row n-vector and a column n-vector. We begin by generalizing
a concept from elementary calculus. If a and b are either row or column n-vectors, with
components a1, a2, . . . , an, and b1, b2, . . . , bn, respectively, then their dot product,
denoted a · b, is the number
a · b = a1b1 + a2b2 + · · · + anbn.
As we will see, this is the key formula in deﬁning the product of two matrices. Now
let a be a row n-vector, and let x be a column n-vector. Then their matrix product ax is

2.2
Matrix Algebra 125
deﬁned to be the 1×1 matrix whose single element is obtained by taking the dot product
of the row vectors a and xT . Thus,
ax =
)a1
a2
. . .
an
*
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦= [a1x1 + a2x2 + · · · + anxn].
Example 2.2.7
If a =
)−8
3
−1
2*
and x =
⎡
⎢⎢⎣
1
4
7
−5
⎤
⎥⎥⎦, then
ax =
)−8
3
−1
2*
⎡
⎢⎢⎣
1
4
7
−5
⎤
⎥⎥⎦=
'
(−8)(1) + (3)(4) + (−1)(7) + (2)(−5)
(
= [−13].
□
CASE 2: Product of an m ×n matrix and a column n-vector. If A is an m ×n matrix
and x is a column n-vector, then the product Ax is deﬁned to be the m × 1 matrix whose
ith element is obtained by taking the dot product of the ith row vector of A with x. (See
Figure 2.2.1.)
a11
a21
ai1
am1
a12
a22
ai2
am2
(Ax)1
(Ax)2
(Ax)i
(Ax)m
a1n
a2n
ain
amn
5
Row i
ith element of Ax
...
...
...
...
x1
x2
xn
...
...
...
...
...
...
...
...
Figure 2.2.1: Multiplication of an m × n matrix with a column n-vector.
The ith row vector of A, ai, is
ai =
)ai1
ai2
. . .
ain
*
,
so that Ax has ith element
(Ax)i = ai1x1 + ai2x2 + · · · + ainxn.
Consequently the column vector Ax has elements
(Ax)i =
n
+
k=1
aikxk,
1 ≤i ≤m.
(2.2.1)
Example 2.2.8
If A =
⎡
⎣
−3
1
−2
0
5
−2
−4
−2
5
⎤
⎦and x =
⎡
⎣
−2
1
6
⎤
⎦, ﬁnd Ax.
Solution:
We have
⎡
⎣
−3
1
−2
0
5
−2
−4
−2
5
⎤
⎦
⎡
⎣
−2
1
6
⎤
⎦=
⎡
⎣
−5
−7
36
⎤
⎦.
□

126
CHAPTER 2
Matrices and Systems of Linear Equations
The following result regarding multiplication of a column vector by a matrix will
be used repeatedly in the later chapters.
Theorem 2.2.9
If A =
)a1, a2, . . . , an
*
is an m × n matrix and c =
⎡
⎢⎢⎢⎣
c1
c2
...
cn
⎤
⎥⎥⎥⎦is a column n-vector, then
Ac = c1a1 + c2a2 + · · · + cnan.
(2.2.2)
Proof In this proof, we adopt the notation (x)i to denote the ith element of the column
vector x. The element aik of A is the ith component of the column m-vector ak, so
aik = (ak)i.
Applying formula (2.2.1) for multiplication of a column vector by a matrix yields
(Ac)i =
n
+
k=1
aikck =
n
+
k=1
(ak)ick =
n
+
k=1
(ckak)i.
Consequently,
Ac =
n
+
k=1
ckak = c1a1 + c2a2 + · · · + cnan
as required.
If a1, a2, . . . , an are column m-vectors and c1, c2, . . . , cn are scalars, then an ex-
pression of the form
c1a1 + c2a2 + · · · + cnan
is called a linear combination of the column vectors. Therefore, from Equation (2.2.2),
we see that the vector Ac is obtained by taking a linear combination of the column vectors
of A.
Example 2.2.10
If
A =
⎡
⎣
−3
1
−2
0
5
−2
−4
−2
5
⎤
⎦
and
c =
⎡
⎣
−2
1
6
⎤
⎦,
then
Ac = c1a1 + c2a2 + c3a3 = (−2)
⎡
⎣
−3
0
−4
⎤
⎦+ (1)
⎡
⎣
1
5
−2
⎤
⎦+ (6)
⎡
⎣
−2
−2
5
⎤
⎦=
⎡
⎣
−5
−7
36
⎤
⎦.
□
CASE 3: Product of an m × n matrix and an n × p matrix. If A is an m × n matrix
and B is an n × p matrix, then the product AB has columns deﬁned by multiplying
the matrix A by the respective column vectors of B, as described in Case 2. That is, if
B = [b1, b2, . . . , bp], then AB is the m × p matrix deﬁned by
AB = [Ab1, Ab2, . . . , Abp].
Example 2.2.11
If A =
'−2
1
3
4
−2
6
(
and B =
⎡
⎣
−4
1
3
−1
−9
2
⎤
⎦, determine AB.

2.2
Matrix Algebra 127
Solution:
We have
AB =
!−2
1
3
4
−2
6
" ⎡
⎣
−4
1
3
−1
−9
2
⎤
⎦
=
!!−2
1
3
4
−2
6
" ⎡
⎣
−4
3
−9
⎤
⎦,
!−2
1
3
4
−2
6
" ⎡
⎣
1
−1
2
⎤
⎦
"
=
![(−2)(−4) + (1)(3) + (3)(−9)]
[(−2)(1) + (1)(−1) + (3)(2)]
[(4)(−4) + (−2)(3) + (6)(−9)]
[(4)(1) + (−2)(−1) + (6)(2)]
"
=
!−16
3
−76
18
"
.
□
Example 2.2.12
If A =
⎡
⎢⎢⎣
6
−4
0
1
⎤
⎥⎥⎦and B =
)1
−3*
, determine AB.
Solution:
We have
AB =
⎡
⎢⎢⎣
6
−4
0
1
⎤
⎥⎥⎦
)1
−3*
=
!
⎡
⎢⎢⎣
6
−4
0
1
⎤
⎥⎥⎦[1],
⎡
⎢⎢⎣
6
−4
0
1
⎤
⎥⎥⎦[−3]
"
=
⎡
⎢⎢⎣
(6)(1)
(6)(−3)
(−4)(1)
(−4)(−3)
(0)(1)
(0)(−3)
(1)(1)
(1)(−3)
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
6
−18
−4
12
0
0
1
−3
⎤
⎥⎥⎦.
□
Another way to describe AB is to note that the element (AB)i j is obtained by
computing the matrix product of the ith row vector of A and the jth column vector of
B. That is,
(AB)i j = ai1b1 j + ai2b2 j + · · · + ainbnj.
Expressing this using the summation notation yields the following important result:
DEFINITION
2.2.13
If A = [ai j] is an m × n matrix, B = [bi j] is an n × p matrix, and C = AB, then
ci j =
n
+
k=1
aikbkj
1 ≤i ≤m,
1 ≤j ≤p.
(2.2.3)
This is called the index form of the matrix product.
The formula (2.2.3) for the (ij)-element of AB is very important and will often be
required in the future. The reader should memorize it.
In order for the product AB to be deﬁned, we see that A and B must satisfy
number of columns of A = number of rows of B.

128
CHAPTER 2
Matrices and Systems of Linear Equations
In such a case, if C represents the product matrix AB, then the relationship between the
dimensions of the matrices is
SAME
RESULT
Am × n
Bn × p = Cm ×  p
Now we give some further examples of matrix multiplication.
Example 2.2.14
If A =
)−2
−1*
and B =
'4
−6
−3
2
6
3
(
, then
AB =
)−2
−1* '4
−6
−3
2
6
3
(
=
)−10
6
3*
.
□
Example 2.2.15
If A =
⎡
⎣
2
−2
−4
1
1
−5
⎤
⎦and B =
' 4
−3
0
−3
1
8
(
, then
AB =
⎡
⎣
2
−2
−4
1
1
−5
⎤
⎦
' 4
−3
0
−3
1
8
(
=
⎡
⎣
14
−8
−16
−19
13
8
19
−8
−40
⎤
⎦.
□
Example 2.2.16
If A =
⎡
⎢⎢⎣
8
−9
2
3
⎤
⎥⎥⎦and B =
)−3
1*
, then
AB =
⎡
⎢⎢⎣
8
−9
2
3
⎤
⎥⎥⎦
)−3
1*
=
⎡
⎢⎢⎣
−24
8
27
−9
−6
2
−9
3
⎤
⎥⎥⎦.
□
Example 2.2.17
If A =
'5 + 2i
−1
1 + 3i
2i
(
and B =
'1 −2i
2 −3i
2
2i
(
, then
AB =
'5 + 2i
−1
1 + 3i
2i
( '1 −2i
2 −3i
2
2i
(
=
'7 −8i
16 −13i
7 + 5i
7 + 3i
(
.
□
Notice that in Examples 2.2.14 and 2.2.16 above, the product B A is not deﬁned,
since the number of columns of the matrix B does not agree with the number of rows of
the matrix A.
We can now establish some basic properties of matrix multiplication.
Theorem 2.2.18
If A, B, and C have appropriate dimensions for the operations to be performed, then
A(BC) = (AB)C
(Associativity of matrix multiplication),
(2.2.4)
A(B + C) = AB + AC
(Left distributivity of matrix multiplication),
(2.2.5)
(A + B)C = AC + BC
(Right distributivity of matrix multiplication).
(2.2.6)
Proof The idea behind the proof of each of these results is to use the deﬁnition of matrix
multiplication to show that the (i, j)-element of the matrix on the left-hand side of each

2.2
Matrix Algebra 129
equation is equal to the (i, j)-element of the matrix on the right-hand side. We illustrate
by proving (2.2.6), but we leave the proofs of (2.2.4) and (2.2.5) as exercises. Suppose
that A and B are m × n matrices and that C is an n × p matrix. Then, from Equation
(2.2.3),
[(A + B)C]i j =
n
+
k=1
(aik + bik)ckj =
n
+
k=1
aikckj +
n
+
k=1
bikckj
= (AC)i j + (BC)i j
= (AC + BC)i j,
1 ≤i ≤m,
1 ≤j ≤p.
Consequently,
(A + B)C = AC + BC.
Theorem 2.2.18 states that matrix multiplication is associative and distributive (over
addition). We now consider the question of commutativity of matrix multiplication. If
A is an m × n matrix and B is an n × m matrix, we can form both of the products
AB and B A, which are m × m and n × n, respectively. In the ﬁrst of these, we say
that B has been premultiplied by A, whereas in the second product, we say that B has
been postmultiplied by A. If m ̸= n, then the matrices AB and B A will have different
dimensions, and so, they cannot be equal. It is important to realize, however, that even
if m = n, in general (that is, except for special cases)
AB ̸= B A.
This is the statement that
matrix multiplication is not commutative.
With a little bit of thought this should not be too surprising in view of the fact that
the (i j) element of AB is obtained by taking the matrix product of the ith row vector
of A with the jth column vector of B, whereas the (i j) element of B A is obtained by
taking the matrix product of the ith row vector of B with the jth column vector of A.
We illustrate with an example.
Example 2.2.19
If A =
' 3
−1
−2
4
(
and B =
'−5
2
3
−2
(
, ﬁnd AB and B A.
Solution:
We have
AB =
' 3
−1
−2
4
( '−5
2
3
−2
(
=
'−18
8
22
−12
(
,
and
B A =
'−5
2
3
−2
( ' 3
−1
−2
4
(
=
'−19
13
13
−11
(
.
Thus we see that in this example, AB ̸= B A.
□
As an exercise, the reader can calculate the matrix B A in Examples 2.2.15 and
2.2.17 and again see that AB ̸= B A.
For an n × n matrix we use the usual power notation to denote the operation of
multiplying A by itself. Thus,
A2 = AA,
A3 = AAA,
etc.
As the next example illustrates, the lack of commutativity of matrix multiplication
requires one to exercise caution while computing powers of matrices.

130
CHAPTER 2
Matrices and Systems of Linear Equations
Example 2.2.20
If A and B are n × n matrices, ﬁnd expressions for (A + B)2 and (A + B)3.
Solution:
By using Equations (2.2.5) and (2.2.6), we can write
(A + B)2 = (A + B)(A + B)
= A(A + B) + B(A + B)
= A2 + AB + B A + B2.
(2.2.7)
Similarly, we have
(A + B)3 = (A + B)2(A + B)
= (A2 + AB + B A + B2)(A + B)
= A3 + AB A + B A2 + B2 A + A2B + AB2 + B AB + B3.
(2.2.8)
It may be tempting to try to simplify the expressions (2.2.7) and (2.2.8) further by
combining terms, but this is not possible since AB ̸= B A in general. Here we see a
signiﬁcant departure from the algebra of real or complex numbers.
□
The identity matrix, In (or just I if the dimensions are obvious), is the n ×n matrix
with ones on the main diagonal and zeros elsewhere. For example,
I2 =
'1
0
0
1
(
and
I3 =
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦.
DEFINITION
2.2.21
The elements of In can be represented by the Kronecker delta symbol, δi j, deﬁned
by
δi j =
,
1, if i = j,
0, if i ̸= j.
Then,
In = [δi j].
The following properties of the identity matrix indicate that it plays the same role
in matrix multiplication as the number 1 does in the multiplication of real numbers.
Properties of the Identity Matrix:
1. Am×n In = Am×n.
2. Im Am×p = Am×p.
Proof We establish (1) and leave the proof of (2) as an exercise (Problem 24). Using
the index form of the matrix product we have
(AI)i j =
n
+
k=1
aikδkj = ai1δ1 j + ai2δ2 j + · · · + ai jδ j j + · · · + ainδnj.
But, from the deﬁnition of the Kronecker delta symbol, we see that all terms in the
summation with k ̸= j vanish, so that we are left with
(AI)i j = ai jδ j j = ai j,
1 ≤i ≤m,
1 ≤j ≤n.

2.2
Matrix Algebra 131
The next example illustrates the properties of the identity matrix.
Example 2.2.22
If A =
'−1
4
0
7
−3
2
(
, verify the properties (1) and (2) of the identity matrix.
Solution:
We have
AI3 =
'−1
4
0
7
−3
2
( ⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦= A
and
I2 A =
'1
0
0
1
( '−1
4
0
7
−3
2
(
= A.
□
Properties of the Transpose
The operation of taking the transpose of a matrix was introduced in the previous section.
The next theorem gives three important properties satisﬁed by the transpose. These
should be memorized.
Theorem 2.2.23
Let A and C be m × n matrices, and let B be an n × p matrix. Then
1. (AT )T = A.
2. (A + C)T = AT + CT .
3. (AB)T = BT AT .
Proof For all three statements, our strategy is again to show that the (i, j)-elements of
each side of the equation are the same. We prove (3) and leave the proofs of (1) and (2)
for the exercises (Problem 23). From the deﬁnition of the transpose and the index form
of the matrix product we have
[(AB)T ]i j = (AB) ji
(deﬁnition of the transpose)
=
n
+
k=1
a jkbki
(index form of the matrix product)
=
n
+
k=1
bkia jk =
n
+
k=1
bT
ikaT
kj
= (BT AT )i j
Consequently,
(AB)T = BT AT .
Results for Triangular Matrices
Upper and lower triangular matrices play a signiﬁcant role in the analysis of linear
systems of algebraic equations. The following theorem and its corollary will be needed
in Section 2.7.
Theorem 2.2.24
The product of two lower (upper) triangular matrices is a lower (upper) triangular
matrix.

132
CHAPTER 2
Matrices and Systems of Linear Equations
Proof Suppose that A and B are n × n lower triangular matrices. Then, aik = 0
whenever i < k, and bkj = 0 whenever k < j. If we let C = AB, then we must prove
that
ci j = 0 whenever i < j.
Using the index form of the matrix product, we have
ci j =
n
+
k=1
aikbkj =
n
+
k= j
aikbkj
(since bkj = 0 if k < j).
(2.2.9)
We now impose the condition that i < j. Then, since k ≥j in (2.2.9), it follows that
k > i. However, this implies that aik = 0 (since A is lower triangular), and hence, from
(2.2.9), that
ci j = 0 whenever i < j.
as required.
To establish the result for upper triangular matrices, we can either give a similar
argument to that presented above for lower triangular matrices, or alternatively, we can
use the fact that the transpose of a lower triangular matrix is an upper triangular matrix,
and vice versa. Hence, if A and B are n × n upper triangular matrices, then AT and BT
are lower triangular, and therefore by what we proved above, (AB)T = BT AT remains
lower triangular. Thus, AB is upper triangular.
Corollary 2.2.25
The product of two unit lower (upper) triangular matrices is a unit lower (upper)
triangular matrix.
Proof Let A and B be unit lower triangular n × n matrices. We know from Theo-
rem 2.2.24 that C = AB is a lower triangular matrix. We must establish that cii = 1
for each i. The elements on the main diagonal of C can be obtained by setting j = i in
(2.2.9):
cii =
n
+
k=i
aikbki.
(2.2.10)
Since aik = 0 whenever k > i, the only nonzero term in the summation in (2.2.10)
occurs when k = i. Consequently,
cii = aiibii = 1 · 1 = 1,
i = 1, 2, . . . , n.
The proof for unit upper triangular matrices is similar and left as an exercise.
The Algebra and Calculus of Matrix Functions
By and large, the algebra of matrix and vector functions is the same as that for matrices
and vectors of real or complex numbers. Since vector functions are a special case of
matrix functions, we focus here on matrix functions. The main comment here pertains to
scalar multiplication. In the description of scalar multiplication of matrices of numbers,
the scalars were required to be real or complex numbers. However, for matrix functions,
we can scalar multiply by any scalar function s(t).
Example 2.2.26
If s(t) = t2 and A(t) =
'
t2 −1
−6
t
3t + 1
(
, then
s(t)A(t) =
' t2(t2 −1)
−6t2
t3
t2(3t + 1)
(
=
' t4 −t2
−6t2
t3
3t3 + t2
(
.
□

2.2
Matrix Algebra 133
Example 2.2.27
Referring to A and B from Example 2.1.16, ﬁnd et AT −2t B.
Solution:
We have
et AT −2t B = et
⎡
⎣
t3
ln t
e2t
1 −et
−3
sin t
⎤
⎦−2t
⎡
⎣
tan t
esin t
−2
6 −t
−5
1 + 2t2
⎤
⎦
=
⎡
⎣
ett3 −2t tan t
et ln t −2tesin t
e3t + 4t
et −e2t −2t(6 −t)
−3et + 10t
et sin t −2t(1 + 2t2)
⎤
⎦.
□
We can also perform calculus operations on matrix functions. In particular we can
differentiate and integrate them. The rules for doing so are as follows:
1. The derivative of a matrix function is obtained by differentiating every element of
the matrix. Thus, if A(t) = [ai j(t)], then
d A
dt =
'dai j(t)
dt
(
,
provided that each of the ai j are differentiable.
2. It follows from (1) and the index form of the matrix product that if A and B are
both differentiable and the product AB is deﬁned, then
d
dt (AB) = Ad B
dt + d A
dt B.
The key point to notice is that the order of the multiplication must be preserved.
3. If A(t) = [ai j(t)], where each ai j(t) is integrable on an interval [a, b], then
- b
a
A(t)dt =
'- b
a
ai j(t)dt
(
.
Example 2.2.28
If A(t) =
' 2t
1
6t2
4e2t
(
, determine d A
dt and
. 1
0 A(t)dt.
Solution:
We have
d A
dt =
'
2
0
12t
8e2t
(
,
whereas
- 1
0
A(t)dt =
⎡
⎣
. 1
0 2tdt
. 1
0 1dt
. 1
0 6t2dt
. 1
0 4e2tdt
⎤
⎦=
' 1
1
2
2(e2 −1)
(
.
□
Exercises for 2.2
Key Terms
Matrix addition and subtraction, Scalar multiplication, Ma-
trix multiplication, Dot product, Linear combination of col-
umn vectors, Index form, Premultiplication, Postmultiplica-
tion, Zero matrix, Identity matrix, Kronecker delta symbol.
Skills
• Know the basic relationships between the dimensions
of two matrices A and B in order for A + B to be
deﬁned, and in order for AB to be deﬁned.

134
CHAPTER 2
Matrices and Systems of Linear Equations
• Be able to perform matrix addition, subtraction, and
multiplication.
• Be able to multiply a matrix by a scalar.
• Be able to express the product Ax of a matrix A and a
columnvectorx asalinearcombinationofthecolumns
of A.
• Be familiar with all of the basic properties of matrix
addition, matrix multiplication, and scalar multiplica-
tion.
• Be familiar with the basic properties of the zero matrix
and the identity matrix.
• Know the basic technique for showing formally that
two matrices are equal.
• Be able to perform algebra and calculus operations on
matrix functions.
True-False Review
For items (a)–(l), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) For all matrices A, B, and C of the appropriate dimen-
sions, we have
(AB)C = (C A)B.
(b) If A is an m × n matrix, B is an n × p matrix, and C
is a p × q matrix, then ABC is an m × q matrix.
(c) If A and B are symmetric n × n matrices, then so is
A + B.
(d) If A and B are skew-symmetric n × n matrices, then
AB is a symmetric matrix.
(e) For n × n matrices A and B, we have
(A + B)2 = A2 + 2AB + B2.
(f) If AB = 0, then either A = 0 or B = 0.
(g) If A and B are square matrices such that AB is upper
triangular, then A and B must both be upper triangular.
(h) If A is a square matrix such that A2 = A, then A must
be the zero matrix or the identity matrix.
(i) If A is a matrix of numbers, then if we consider A as
a matrix function, its derivative is the zero matrix.
(j) If A and B are matrix functions whose product AB is
deﬁned, then d
dt (AB) = Ad B
dt + B d A
dt .
(k) If A is an n × n matrix function such that A and
d A
dt are the same function, then A = cet In for some
constant c.
(l) If A and B are matrix functions whose product AB is
deﬁned, then the matrix functions (AB)T and BT AT
are the same.
Problems
For Problems 1–2, let
A =
' −2
6
1
−1
0
−3
(
, B =
' 2
1
−1
0
4
−4
(
,
C =
⎡
⎣
1 + i
2 + i
3 + i
4 + i
5 + i
6 + i
⎤
⎦, D =
⎡
⎣
4
0
1
1
2
5
3
1
2
⎤
⎦,
E =
⎡
⎣
2
−5
−2
1
1
3
4
−2
−3
⎤
⎦, F =
⎡
⎣
6
2 −3i
i
1 + i
−2i
0
−1
5 + 2i
3
⎤
⎦.
In these problems, i denotes √−1.
1. Compute each of the following:
(a) 5A
(b) −3B
(c) iC
(d) 2A −B
(e) A + 3CT
(f) 3D −2E
(g) D + E + F
(h) the matrix G such that 2A+3B−2G = 5(A+B)
(i) the matrix H such that D + 2F + H = 4E
(j) the matrix K such that K T + 3A −2B = 02×3
2. Compute each of the following:
(a) −D
(b) 4BT
(c) −2AT + C
(d) 5E + D
(e) 4AT −2BT + iC
(f) 4E −3DT

2.2
Matrix Algebra 135
(g) (1 −6i)F + i D
(h) the matrix G such that 2A −B + (1 −i)CT =
G + A −B
(i) the matrix H such that 3D−3E +6I3−2H = 03
(j) the matrix K such that K T + FT = DT + ET
For Problems 3–4, let
A =
' 1
−1
2
3
1
4
(
, B =
⎡
⎣
2
−1
3
5
1
2
4
6
−2
⎤
⎦,
C =
⎡
⎣
1
−1
2
⎤
⎦, D =
) 2
−2
3 *
,
E =
' 2 −i
1 + i
−i
2 + 4i
(
, F =
' i
1 −3i
0
4 + i
(
.
In these problems, i denotes √−1.
3. For each item, decide whether or not the given expres-
sion is deﬁned. For each item that is deﬁned, compute
the result.
(a) AB
(b) BC
(c) C A
(d) AT E
(e) C D
(f) CT AT
(g) F2
(h) BDT
(i) AT A
(j) FE
4. For each item, decide whether or not the given expres-
sion is deﬁned. For each item that is deﬁned, compute
the result.
(a) AC
(b) DC
(c) DB
(d) AD
(e) E F
(f) AT B
(g) C2
(h) E2
(i) ADT
(j) ET A
5. Let A =
'−3
2
7
−1
6
0
−3
−5
(
, B =
⎡
⎢⎢⎣
−2
8
8
−3
−1
−9
0
2
⎤
⎥⎥⎦,
and C =
'−6
1
1
5
(
. Compute ABC and C AB.
For Problems 6–9, determine Ac by computing an appropri-
ate linear combination of the column vectors of A.
6. A =
'
1
3
−5
4
(
, c =
'
6
−2
(
.
7. A =
⎡
⎣
3
−1
4
2
1
5
7
−6
3
⎤
⎦, c =
⎡
⎣
2
3
−4
⎤
⎦.
8. A =
⎡
⎣
−1
2
4
7
5
−4
⎤
⎦, c =
'
5
−1
(
.
9. A =
' a
b
c
d
e
f
g
h
(
, c =
⎡
⎢⎢⎣
x
y
z
w
⎤
⎥⎥⎦.
10. Suppose A is an m ×n matrix and C is an r ×s matrix.
(a) What must the dimensions of a matrix B be in
order for the product ABC to be deﬁned?
(b) Writeanexpressionforthe(i, j)elementof ABC
in terms of the elements of A, B, and C.
11. Find A2, A3, and A4 if
(a) A =
' 1
−1
2
3
(
.
(b) A =
⎡
⎣
0
1
0
−2
0
1
4
−1
0
⎤
⎦.
12. If A, B, and C are n × n matrices, show that
(a) (A + 2B)2 = A2 + 2AB + 2B A + 4B2.
(b) (A + B + C)2 = A2 + B2 + C2 + AB + B A +
AC + C A + BC + C B.
(c) (A −B)3 = A3 −AB A −B A2 + B2 A −A2B +
AB2 + B AB −B3.
13. If A =
' 2
−5
6
−6
(
, calculate A2 and verify that A
satisﬁes A2 + 4A + 18I2 = 02.

136
CHAPTER 2
Matrices and Systems of Linear Equations
14. If A =
⎡
⎣
−1
0
4
1
1
2
−2
3
0
⎤
⎦, calculate A2 and A3 and
verify that A satisﬁes A3 + A −26I3 = 03.
15. Find numbers x, y, and z such that the matrix A =
⎡
⎣
1
x
z
0
1
y
0
0
1
⎤
⎦satisﬁes
A2 +
⎡
⎣
0
−1
0
0
0
−1
0
0
0
⎤
⎦= I3.
16. If A =
'
x
1
−2
y
(
, determine all values of x and y
for which A2 = A.
17. The Pauli spin matrices σ1, σ2, and σ3 are deﬁned by
σ1 =
' 0
1
1
0
(
, σ2 =
' 0
−i
i
0
(
,
and
σ3 =
' 1
0
0
−1
(
.
Verify that they satisfy
σ1σ2 = iσ3, σ2σ3 = iσ1, σ3σ1 = iσ2.
If A and B are n × n matrices, we deﬁne their com-
mutator, denoted [A, B], by
[A, B] = AB −B A.
Thus, [A, B] = 0 if and only if A and B commute.
That is, AB = B A. Problems 19–22 require the com-
mutator.
18. If A =
' 1
−1
2
1
(
, B =
' 3
1
4
2
(
, ﬁnd [A, B].
19. If A1 =
' 1
0
0
1
(
, A2 =
' 0
1
0
0
(
, and A3 =
' 0
0
1
0
(
, compute all of the commutators [Ai, A j],
and determine which of the matrices commute.
20. If A1 = 1
2
' 0
i
i
0
(
, A2 = 1
2
' 0
−1
1
0
(
, and
A3 = 1
2
' i
0
0
−i
(
, verify that
[A1, A2] = A3, [A2, A3] = A1, [A3, A1] = A2.
21. If A, B, and C are n×n matrices, ﬁnd [A, [B, C]] and
prove the Jacobi identity
[A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0.
22. Use the index form of the matrix product to prove
properties (2.2.4) and (2.2.5).
23. Prove parts (1) and (2) of Theorem 2.2.23.
24. Prove property (2) of the identity matrix.
25. If A and B are n × n matrices, prove that tr(AB) =
tr(B A).
For Problems 26–27, let
A =
) −3
−1
6 *
,
B =
⎡
⎣
0
−4
−7
1
−1
−3
⎤
⎦,
C =
' −9
0
3
−2
1
1
5
−2
(
, D =
⎡
⎣
−2
1
5
0
0
7
1
−2
−1
⎤
⎦.
26. For each item, decide whether or not the given expres-
sion is deﬁned. For each item that is deﬁned, compute
the result.
(a) BT AT
(b) CT BT
(c) DT A
27. For each item, decide whether or not the given expres-
sion is deﬁned. For each item that is deﬁned, compute
the result.
(a) ADT
(b) (CT C)2
(c) DT B
28. Let A =
⎡
⎣
2
2
1
2
5
2
1
2
2
⎤
⎦, and let S be the matrix with
column vectors s1 =
⎡
⎣
−x
0
x
⎤
⎦, s2 =
⎡
⎣
−y
y
−y
⎤
⎦, and
s3 =
⎡
⎣
z
2z
z
⎤
⎦, where x, y, z are constants.
(a) Show that AS = [s1, s2, 7s3].
(b) Find all values of x, y, z such that ST AS =
diag(1, 1, 7).

2.2
Matrix Algebra 137
29. Let A =
⎡
⎣
1
−4
0
−4
7
0
0
0
5
⎤
⎦, and let S be the matrix
with column vectors s1 =
⎡
⎣
0
0
z
⎤
⎦, s2 =
⎡
⎣
2x
x
0
⎤
⎦, and
s3 =
⎡
⎣
y
−2y
0
⎤
⎦, where x, y, z are constants.
(a) Show that AS = [5s1, −s2, 9s3].
(b) Find all values of x, y, z such that ST AS =
diag(5, −1, 9).
30. A matrix that is a scalar multiple of In is called an
n × n scalar matrix.
(a) Determine the 4 × 4 scalar matrix whose trace is
8.
(b) Determine the 3 × 3 scalar matrix such that the
product of the elements on the main diagonal is
343.
31. Prove that for each positive integer n, there is a unique
scalar matrix whose trace is a given constant k.
If A is an n × n matrix, then the matrices B and C deﬁned
by
B = 1
2(A + AT ),
C = 1
2(A −AT )
are referred to as the symmetric and skew-symmetric parts
of A respectively. Problems 32–36 investigate properties of
B and C.
32. Use the properties of the transpose to show that B and
C are symmetric and skew-symmetric, respectively.
33. Show that A = B + C. Together with the previous
exercise, this shows that every n × n matrix A can
be written as the sum of a symmetric matrix and a
skew-symmetric matrix.
34. Find B and C for the matrix
A =
⎡
⎣
4
−1
0
9
−2
3
2
5
5
⎤
⎦.
35. Find B and C for the matrix
A =
⎡
⎣
1
−5
3
3
2
4
7
−2
6
⎤
⎦.
36. (a) If A is an n × n symmetric matrix, what are B
and C?
(b) If A is an n ×n skew-symmetric matrix, what are
B and C?
37. Prove that if A is an n × p matrix and D
=
diag(d1, d2, . . . , dn), then DA is the matrix obtained
by multiplying the i-th row vector of A by di,
where 1 ≤i ≤n.
38. Prove that if A is an m × n matrix and D
=
diag(d1, d2, . . . , dn), then AD is the matrix obtained
by multiplying the j-th column vector of A by d j,
where 1 ≤j ≤n.
39. If A and B are n × n symmetric matrices such that
AB = B A, then AB is symmetric.
40. Use the properties of the transpose to prove that
(a) AAT is a symmetric matrix.
(b) (ABC)T = CT BT AT .
For Problems 41–44, determine the derivative of the given
matrix function.
41. A(t) =
%
t
sin t
cos t
4t
&
.
42. A(t) =
%
e−2t
sin t
&
.
43. A(t) =
⎡
⎣
sin t
cos t
0
−cos t
sin t
t
0
3t
1
⎤
⎦.
44. A(t) =
% et
e2t
t2
2et
4e2t
5t2
&
.
45. Let A = [ai j(t)] be an m × n matrix function and let
B = [bi j(t)] be an n × p matrix function. Use the
deﬁnition of matrix multiplication to prove that
d
dt (AB) = Ad B
dt + d A
dt B.
For Problems 46–49, determine
' b
a A(t)dt for the given
matrix function.
46. A(t) =
% et
e−t
2et
5e−t
&
, a = 0, b = 1.

138
CHAPTER 2
Matrices and Systems of Linear Equations
47. A(t) =
' cos t
sin t
(
, a = 0, b = π/2.
48. The matrix function A(t) in Problem 44, with a = 0
and b = 1.
49. A(t) =
⎡
⎣
e2t
sin 2t
t2 −5
tet
sec2 t
3t −sin t
⎤
⎦, a = 0, b = 1.
Integration of matrix functions given in the text was done
with deﬁnite integrals, but one can naturally compute indef-
inite integrals of matrix functions as well, by performing in-
deﬁnite integrals for each element of the matrix function. For
each element of the matrix
.
A(t)dt, an arbitrary constant of
integration must be included, and the arbitrary constants for
different elements should be different. In Problems 50–54,
evaluate the indeﬁnite integral
.
A(t)dt for the given matrix
function. You may assume that the constants of all indeﬁnite
integrations are zero.
50. A(t) =
'
−5
1
t2 + 1
e3t
(
51. A(t) =
' 2t
3t2
(
.
52. The matrix function A(t) in Problem 43.
53. The matrix function A(t) in Problem 46.
54. The matrix function A(t) in Problem 49.
2.3
Terminology for Systems of Linear Equations
As we mentioned in Section 2.1, one of the main aims of this chapter is to apply matrices
to determine the solution properties of any system of linear equations. We are now in a
position to pursue that aim. We begin by introducing some notation and terminology.
DEFINITION
2.3.1
The general m × n system of linear equations is of the form
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
(2.3.1)
am1x1 + am2x2 + · · · + amnxn = bm,
where the system coefﬁcients ai j and the system constants b j are given scalars and
x1, x2, . . . , xn denote the unknowns in the system. If bi = 0 for all i, then the system
is called homogeneous; otherwise it is called nonhomogeneous.
DEFINITION
2.3.2
By a solution to the system (2.3.1) we mean an ordered n-tuple of scalars, (c1, c2, . . . ,
cn), which, when substituted for x1, x2, . . . , xn into the left-hand side of system
(2.3.1), yield the values on the right-hand side. The set of all solutions to system
(2.3.1) is called the solution set to the system.
Example 2.3.3
Verify that for all real numbers a and b, the 4-tuple (24+5a −10b, −8−4a +3b, a, b)
satisﬁes the system
x1 + 2x2+ 3x3 + 4x4= 8,
2x1 + 5x2+10x3 + 5x4= 8.

2.3
Terminology for Systems of Linear Equations 139
Solution:
Plugging the substitutions x1 = 24 + 5a −10b, x2 = −8 −4a + 3b,
x3 = a, and x4 = b into both equations in the given system, we have
x1 + 2x2 + 3x3 + 4x4 = (24 + 5a −10b) + 2(−8 −4a + 3b) + 3a + 4b = 8
and
2x1 + 5x2 + 10x3 + 5x4 = 2(24 + 5a −10b) + 5(−8 −4a + 3b) + 10a + 5b = 8,
as required.
□
Remarks
1. Usually the ai j and b j will be real numbers, and we will then be interested in deter-
mining only the real solutions to system (2.3.1). However, many of the problems
that arise in the later chapters will require the solution to systems with complex
coefﬁcients, in which case the corresponding solutions will also be complex.
2. If (c1, c2, . . . , cn) is a solution to the system (2.3.1), we will sometimes specify
this solution by writing x1 = c1, x2 = c2, . . . , xn = cn. For example, the ordered
pair of numbers (1, 2) is a solution to the system
x1 + x2 =
3,
(2.3.2)
3x1 −2x2 = −1,
and we could express this solution in the equivalent form x1 = 1, x2 = 2.
Returning to the general discussion of system (2.3.1), there are some fundamental
questions that we will consider:
1. Does the system (2.3.1) have a solution?
2. If the answer to (1) is yes, then how many solutions are there?
3. How do we determine all of the solutions?
To obtain an idea of the answer to questions (1) and (2), consider the special case of
a system of three equations in three unknowns. The linear system (2.3.1) then reduces to
a11x1 + a12x2 + a13x3 = b1,
a21x1 + a22x2 + a23x3 = b2,
a31x1 + a32x2 + a33x3 = b3,
which can be interpreted as deﬁning three planes in space. An ordered triple (c1, c2, c3)
is a solution to this system if and only if it corresponds to the coordinates of a point of
intersection of the three planes. There are precisely four possibilities:
1. The planes have no intersection point.
2. The planes intersect in just one point.
3. The planes intersect in a line.
4. The planes are all identical.
In (1), the corresponding system has no solution, whereas in (2), the system has just
one solution. Finally, in (3) and (4), every point on the line or plane (respectively) is a
solution to the linear system and hence the system has an inﬁnite number of solutions.
Cases (1)–(3) are illustrated in Figure 2.3.1.

140
CHAPTER 2
Matrices and Systems of Linear Equations
Planes intersect at a point: a
unique solution
Planes intersect in a line: an
infinite number of solutions
Three parallel planes (no
intersection): no solution
No common intersection:
no solution
Figure 2.3.1: Possible intersection points for three planes in space.
We have therefore proved, geometrically, that there are precisely three possibilities
for the solutions of a linear system of three equations in three unknowns. The system
either has no solution, it has just one solution, or it has an inﬁnite number of solutions.
In Section 2.5, we will establish that these are the only possibilities for the general m ×n
system (2.3.1).
DEFINITION
2.3.4
A system of equations that has at least one solution is said to be consistent, whereas
a system that has no solution is called inconsistent.
Our problem will be to determine whether a given system is consistent and then, in
the case when it is, to ﬁnd its solution set.
DEFINITION
2.3.5
Naturally associated with the system (2.3.1) are the following two matrices:
1. The matrix of coefﬁcients A =
⎡
⎢⎢⎢⎣
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
am1
am2
. . .
amn
⎤
⎥⎥⎥⎦.
2. The augmented matrix A# =
⎡
⎢⎢⎢⎣
a11
a12
· · ·
a1n
b1
a21
a22
· · ·
a2n
b2
...
...
am1
am2
· · ·
amn
bm
⎤
⎥⎥⎥⎦.
The bar just to the left of the rightmost column is useful for visually separating
the matrix of coefﬁcients from the constants given on the right side of the linear
system.

2.3
Terminology for Systems of Linear Equations 141
The augmented matrix completely characterizes a system of equations since it con-
tains all of the system coefﬁcients and system constants. We will see in the following
sections that it is the relationship between A and A# that determines the solution prop-
erties of a linear system. Notice that the matrix of coefﬁcients is the matrix consisting
of the ﬁrst n columns of A#.
Example 2.3.6
Write the system of equations with the following augmented matrix:
⎡
⎣
−2
0
5
−1
6
4
−1
2
2
−2
−7
−6
0
4
−8
⎤
⎦.
Solution:
The appropriate system is
−2x1
+ 5x3 −x4 =
6,
4x1 −x2 + 2x3 + 2x4 = −2,
−7x1 −6x2
+ 4x4 = −8.
□
Vector Formulation
We next show that the matrix product described in the preceding section can be used to
write a linear system as a single equation involving the matrix of coefﬁcients and column
vectors. For example, the system
6x1
−2x3 = −2,
x1 −7x2 + 9x3 = −9,
5x1 −3x2 + x3 = −1
can be written as the vector equation
⎡
⎣
6
0
−2
1
−7
9
5
−3
1
⎤
⎦
⎡
⎣
x1
x2
x3
⎤
⎦=
⎡
⎣
−2
−9
−1
⎤
⎦,
since this vector equation is satisﬁed if and only if
⎡
⎣
6x1
−
2x3
x1
−
7x2
+
9x3
5x1
−
3x2
+
x3
⎤
⎦=
⎡
⎣
−2
−9
−1
⎤
⎦;
that is, if and only if each equation of the given system is satisﬁed.
Similarly, the general m × n system of linear equations
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
can be written as the vector equation
Ax = b,

142
CHAPTER 2
Matrices and Systems of Linear Equations
where A is the m × n matrix of coefﬁcients and
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦
and
b =
⎡
⎢⎢⎢⎣
b1
b2
...
bm
⎤
⎥⎥⎥⎦.
We will refer to the column n-vector x as the vector of unknowns, and the column
m-vector b will be called the right-hand side vector.
Notation: The set of all ordered n-tuples of real numbers (c1, c2, . . . , cn) will be denoted
by Rn. Therefore, the set of all real solutions to the linear system (2.3.1) forms a subset
of Rn. In like manner, the set of all ordered n-tuples of complex numbers will be denoted
by Cn, and the solution set for a linear system (2.3.1) containing complex coefﬁcients
can be viewed as a subset of Cn.
When we restrict all scalar values to be real, we have a natural correspondence
between elements of Rn, row n-vectors, and column n-vectors:
(x1, x2, . . . , xn) ←→[x1 x2 . . . xn] ←→
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦.
(2.3.3)
Therefore,wemayusetheoperationsofaddition,subtraction,andscalarmultiplicationof
row n-vectors and column n-vectors to naturally equip Rn with these same operations.
Hence, just as we can perform addition and scalar multiplication of row or column
vectors, so too can we perform these operations on n-tuples of scalars. In fact, we will
often treat ordered n-tuples of scalars, row n-vectors, and column n-vectors as if they
are just different representations of the same basic object.
Notice in (2.3.3) that it requires more vertical space to render a vector as a column
than it does to render it as a row. For this reason, we will sometimes save space by
expressing vectors as row vectors even when column representations may seem more
natural. For instance, we noted above that one solution to (2.3.2) is (1, 2), even though
the system could be written as
' 1
1
3
−2
( ' x1
x2
(
=
'
3
−1
(
,
in which the unknown vector is expressed as a column.
Of course, if we allow all scalars in question to assume complex values, then the
correspondence is between elements of Cn, row n-vectors, and column n-vectors. We
will have much more to say about the sets Rn and Cn in Chapter 4.
Returning to the linear system Ax = b, where A is an m × n coefﬁcient matrix, we
observe that if all elements in the system are real, then we can view x as an element of
Rn and b as an element of Rm. We can denote these statements by x ∈Rn and b ∈Rm,
respectively.3 Therefore, the set of all real solutions to the system Ax = b is
S = {x ∈Rn : Ax = b},
which is a subset of Rn.
3The symbol ∈is the set-theoretic notation declaring membership in a set, and will be often encountered in
the text.

2.3
Terminology for Systems of Linear Equations 143
Example 2.3.7
It can be shown, using the techniques of the next two sections, that for
A =
⎡
⎣
1
1
2
−1
3
−2
1
2
5
3
3
−2
⎤
⎦and b =
⎡
⎣
0
0
0
⎤
⎦, the set of all real-valued solutions x
to the linear system Ax = b can be expressed by
S = {x ∈R4 : x = (−t, 4t, t, 5t) for some t ∈R}.
We observe that S can be alternatively expressed more simply as
S = {(−t, 4t, t, 5t) : t ∈R}.
By factoring the scalar t out of the 4-tuple representation of the elements of S, note
that S consists precisely of the vectors in R4 that are scalar multiples of the vector
(−1, 4, 1, 5).
□
A similar vector formulation for systems of differential equations can be used not
only in developing the theory for such systems, but also in deriving solution techniques.
As an example of this formulation, consider the system of differential equations
dx1
dt = 5t2x1 −3etx2 + 2 sin t,
dx2
dt =
6x1 −4x2 + 3t4.
Using matrix and vector functions, this system can be written as the vector equation
dx
dt = A(t)x(t) + b(t),
where
x(t) =
'x1(t)
x2(t)
(
,
dx
dt =
'dx1/dt
dx2/dt
(
,
A =
'
5t2
−3et
6
−4
(
,
and b(t) =
'2 sin t
3t4
(
.
In this formulation, the basic unknown is the column 2-vector function x(t).
Example 2.3.8
Give the vector formulation for the system of equations
x′
1 = (tan t)x1 + e5tx2 + 8t5,
x′
2 =
x1 + 3t3x2.
Solution:
We have
' x′
1
x′
2
(
=
'
tan t
e5t
1
3t3
( ' x1
x2
(
+
'
8t5
0
(
.
That is,
x′(t) = A(t)x(t) + b(t),
where
x(t) =
' x1(t)
x2(t)
(
, A(t) =
'
tan t
e5t
1
3t3
(
, b(t) =
'
8t5
0
(
.
□

144
CHAPTER 2
Matrices and Systems of Linear Equations
Exercises for 2.3
Key Terms
System coefﬁcients, System constants, Homogeneous sys-
tem, Nonhomogeneous system, Solution, Solution set, Con-
sistent system, Inconsistent system, Matrix of coefﬁcients,
Augmented matrix, Vector of unknowns, Right-hand side
vector.
Skills
• Be able to identify the matrix of coefﬁcients, the right-
hand side vector, and the augmented matrix for a given
linear system of equations.
• Given a matrix of coefﬁcients and a right-hand side
vector, or an augmented matrix, be able to write the
corresponding linear system.
• Be able to write a linear system of equations as a vector
equation.
• Understand the geometric difference between a con-
sistent linear system and an inconsistent one.
• Be able to verify that the components of a given vector
provide a solution to a linear system.
• Be able to give the vector formulation for a system of
differential equations.
True-False Review
For items (a)–(g), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If a linear system of equations has an m × n aug-
mented matrix, then the system has m equations and
n unknowns.
(b) A linear system that contains three distinct planes can
have at most one solution.
(c) If the matrix of coefﬁcients of a linear system is an
m × n matrix, then the right-hand side vector must
have n components.
(d) It is impossible for a linear system of equations to have
exactly two solutions.
(e) If a linear system has an m × n coefﬁcient matrix,
then the augmented matrix for the linear system is
m × (n + 1).
(f) The row vector (x1, x2, x3, 0, 0) is an element of both
R3 and R5.
(g) The column vectors
⎡
⎢⎢⎣
1
2
3
0
⎤
⎥⎥⎦and
⎡
⎣
1
2
3
⎤
⎦are the same.
Problems
For Problems 1–2, verify that the given triple of real numbers
is a solution to the given system.
1. (1, −1, 2);
2x1 −3x2 + 4x3 = 13,
x1 + x2 −x3 = −2,
5x1 + 4x2 + x3 =
3.
2. (2, −3, 1);
x1 + x2 −2x3 = −3,
3x1 −x2 −7x3 =
2,
x1 + x2 + x3 =
0,
2x1 + 2x2 −4x3 = −6.
3. Verify that for all values of t,
(1 −t, 2 + 3t, 3 −2t)
is a solution to the linear system
x1 + x2 + x3 =
6,
x1 −x2 −2x3 = −7,
5x1 + x2 −x3 =
4.
4. Verify that for all values of s and t,
(s, s −2t, 2s + 3t, t)
is a solution to the linear system
x1 + x2−x3 + 5x4 = 0,
2x2−x3 + 7x4 = 0,
4x1 + 2x2 −3x3 + 13x4 = 0.
In Problem 5–8, make a sketch in the xy-plane in order to
determine the number of solutions to the given linear system.
5. 3x −4y = 12,
6x −8y = 24.

2.3
Terminology for Systems of Linear Equations 145
6. 2x + 3y = 1,
2x + 3y = 2.
7.
x + 4y = 8,
3x + y = 3.
8.
x + 4y = 8,
3x + y = 3,
2x + 8y = 8.
For Problems 9–11, determine the coefﬁcient matrix, A, the
right-hand side vector, b, and the augmented matrix, A#, of
the given system.
9.
x1 + 2x2 −3x3 = 1,
2x1 + 4x2 −5x3 = 2,
7x1 + 2x2 −x3 = 3.
10.
x + y + z −w = 3,
2x + 4y −3z + 7w = 2.
11.
x1 + 2x2 −x3 = 0,
2x1 + 3x2 −2x3 = 0,
5x1 + 6x2 −5x3 = 0.
For Problems 12–15, write the system of equations with the
given coefﬁcient matrix and right-hand side vector.
12. A =
⎡
⎣
1
−1
2
3
1
1
−2
6
3
1
4
2
⎤
⎦, b =
⎡
⎣
1
−1
2
⎤
⎦.
13. A =
⎡
⎣
2
1
3
4
−1
2
7
6
3
⎤
⎦, b =
⎡
⎣
3
1
−5
⎤
⎦.
14. A =
)4
−2
−2
0
−3*
, b =
)−9*
.
15. A =
⎡
⎣
0
−3
2
−7
5
5
⎤
⎦, b =
⎡
⎣
−1
6
7
⎤
⎦.
16. Consider the m × n homogeneous system of linear
equations
Ax = 0.
(2.3.4)
(a) If x = [x1 x2 . . . xn]T and y = [y1 y2 . . . yn]T
are solutions to (2.3.4), show that
z = x + y and w = cx
are also solutions, where c is an arbitrary scalar.
(b) Is the result of (a) true when x and y are solu-
tions to the nonhomogeneous system Ax = b?
Explain.
For Problems 17–20, write the vector formulation for the
given system of differential equations.
17. x′
1 = −4x1 + 3x2 + 4t, x′
2 = 6x1 −4x2 + t2.
18. x′
1 = t2x1 −tx2, x′
2 = (−sin t)x1 + x2.
19. x′
1 = e2tx2, x′
2 + (sin t)x1 = 1.
20. x′
1 = (−sin t)x2 + x3 + t, x′
2 = −etx1 + t2x3 + t3,
x′
3 = −tx1 + t2x2 + 1.
For Problems 21–24, verify that the given vector function x
deﬁnes a solution to x′ = Ax + b for the given A and b.
21. x(t) =
'
e4t
−2e4t
(
, A =
'
2
−1
−2
3
(
,
b(t) =
' 0
0
(
.
22. x(t) =
' 4e−2t + 2 sin t
3e−2t −cos t
(
, A =
'
1
−4
−3
2
(
,
b(t) =
' −2(cos t + sin t)
7 sin t + 2 cos t
(
.
23. x(t) =
' 2tet + et
2tet −et
(
, A =
'
2
−1
−1
2
(
,
b(t) =
'
0
4et
(
.
24. x(t) =
⎡
⎣
−tet
9e−t
tet + 6e−t
⎤
⎦, A =
⎡
⎣
1
0
0
2
−3
2
1
−2
2
⎤
⎦,
b(t) =
⎡
⎣
−et
6e−t
et
⎤
⎦.

146
CHAPTER 2
Matrices and Systems of Linear Equations
2.4
Row-Echelon Matrices and Elementary Row Operations
In the next section, we will develop methods for solving a system of linear equations.
These methods will consist of reducing a given system of equations to a new system
that has the same solution set as the given system, but is easier to solve. For example,
consider the system of equations
x1 + x2 + x3 =
6,
(2.4.1)
x2 −4x3 = −4,
(2.4.2)
x3 =
3.
(2.4.3)
This system can be solved quite easily as follows. From Equation (2.4.3), x3 = 3.
Substituting this value into Equation (2.4.2) and solving for x2 yields x2 = −4+12 = 8.
Finally, substituting for x3 and x2 into Equation (2.4.1) and solving for x1, we obtain
x1 = −5. Thus, the solution to the given system of equations is (−5, 8, 3), a single
vector in R3. This technique is called back substitution and could be used because the
given system has a simple form.
In this section, we describe the characteristics of a linear system that can be readily
solved by back substitution, and we explain how to reduce a given system of equations
to a form that exhibits these characteristics.
Row-Echelon Matrices
The augmented matrix of the system of equations (2.4.1), (2.4.2), and 2.4.3) is
⎡
⎣
1
1
1
6
0
1
−4
−4
0
0
1
3
⎤
⎦.
We see that the submatrix consisting of the ﬁrst three columns (which corresponds
to the matrix of coefﬁcients) is an upper triangular matrix with the leftmost nonzero
entry in each row equal to 1. The back substitution method will work on any system of
linear equations with an augmented matrix of this form. Unfortunately, not all systems
of equations have augmented matrices that can be reduced to such a form. However, as
we shall see later in this section, there is a simple type of matrix that any matrix can be
reduced to, and which also represents a system of equations that can be solved (if it has
a solution) by back substitution. This type of matrix is called a row-echelon matrix and
is deﬁned as follows:
DEFINITION
2.4.1
An m × n matrix is called a row-echelon matrix if it satisﬁes the following three
conditions:
1. If there are any rows consisting entirely of zeros, they are grouped together
at the bottom of the matrix.
2. The ﬁrst nonzero element in any nonzero row4 is a 1 (called a leading 1).
3. The leading 1 of any row below the ﬁrst row is to the right of the leading 1 of
the row above it.
4A nonzero row (nonzero column) is any row (column) that does not consist entirely of zeros.

2.4
Row-Echelon Matrices and Elementary Row Operations 147
Example 2.4.2
Examples of row-echelon matrices are
⎡
⎣
1
−8
−3
7
0
1
5
9
0
0
0
1
⎤
⎦,
⎡
⎣
0
0
1
0
0
0
0
0
0
⎤
⎦,
and
⎡
⎢⎢⎣
1
−3
−6
5
7
0
0
1
3
−5
0
0
0
1
0
0
0
0
0
0
⎤
⎥⎥⎦,
whereas
⎡
⎣
1
0
−1
0
1
2
0
1
−1
⎤
⎦,
⎡
⎢⎢⎣
1
0
0
0
0
0
0
1
−1
0
0
1
⎤
⎥⎥⎦,
and
⎡
⎣
1
0
0
0
0
0
2
3
0
0
0
1
⎤
⎦
are not row-echelon matrices.
□
Elementary Row Operations
Our next aim is to demonstrate that every linear system of equations can be reduced to
a system whose corresponding augmented matrix is in row-echelon form. This can be
done in such a way that the solution set to the linear system remains unaltered throughout
the modiﬁcation. Let us begin with an example.
Example 2.4.3
Consider the system of equations
x1 + 2x2 + 4x3 = 2,
(2.4.4)
2x1 −5x2 + 3x3 = 6,
(2.4.5)
4x1 + 6x2 −7x3 = 8.
(2.4.6)
If we permute (i.e., interchange), say, Equations (2.4.4) and (2.4.5), the resulting
system is
2x1 −5x2 + 3x3 = 6,
x1 + 2x2 + 4x3 = 2,
4x1 + 6x2 −7x3 = 8,
which certainly has the same solution set as the original system. Returning to the original
system, if we multiply, say, Equation (2.4.5) by 5, we obtain the system
x1 + 2x2 + 4x3 = 2,
10x1 −25x2 + 15x3 = 30,
4x1 + 6x2 −7x3 = 8,
which again has the same solution set as the original system. Finally, if we add, say,
twice Equation (2.4.4) to Equation (2.4.6) we obtain the system
x1 + 2x2 + 4x3
= 2,
(2.4.7)
2x1 −5x2 + 3x3
= 6,
(2.4.8)
(4x1 + 6x2 −7x3) + 2(x1 + 2x2 + 4x3) = 8 + 2(2).
(2.4.9)
We can verify that, if (2.4.7)–(2.4.9) are satisﬁed, then so are (2.4.4)–(2.4.6), and
vice versa. It follows that the system of equations (2.4.7)–(2.4.9) has the same solution
set as the original system of equations (2.4.4)–(2.4.6).
□

148
CHAPTER 2
Matrices and Systems of Linear Equations
More generally, similar reasoning can be used to show that the following three
operations can be performed on any m × n system of linear equations without altering
the solution set:
1. Permute equations.
2. Multiply an equation by a nonzero constant.
3. Add a multiple of one equation to another equation.
Since the operations (1)–(3) involve changes only in the system coefﬁcients and constants
(and not changes in the variables), they can be represented by the following operations
on the rows of the augmented matrix of the system:
1. Permute rows.
2. Multiply a row by a nonzero constant.
3. Add a multiple of one row to another row.
These three operations are called elementary row operations and will be a basic com-
putational tool throughout the text even in cases when the matrix under consideration is
not derived from a system of linear equations. The following notation will be used to
describe elementary row operations performed on a matrix A.
1. Pi j:
Permute the ith and jth rows of A.
2. Mi(k): Multiply every element of the ith row of A by a nonzero scalar k.
3. Ai j(k): Add to the elements of the jth row of A the scalar k times the corresponding
elements of the ith row of A.
Furthermore, the notation A ∼B will mean that matrix B has been obtained from
matrix A by a sequence of elementary row operations. To reference a particular elemen-
tary row operation used in, say, the nth step of the sequence of elementary row operations,
we will write n∼B.
Example 2.4.4
The one step operations performed on the system in Example 2.4.3 can be described as
follows using elementary row operations on the augmented matrix of the system:
⎡
⎣
1
2
4
2
2
−5
3
6
4
6
−7
8
⎤
⎦1∼
⎡
⎣
2
−5
3
6
1
2
4
2
4
6
−7
8
⎤
⎦
1. P12.
Permute (2.4.4) and (2.4.5).
⎡
⎣
1
2
4
2
2
−5
3
6
4
6
−7
8
⎤
⎦1∼
⎡
⎣
1
2
4
2
10
−25
15
30
4
6
−7
8
⎤
⎦1. M2(5).
Multiply (2.4.5) by 5.
⎡
⎣
1
2
4
2
2
−5
3
6
4
6
−7
8
⎤
⎦1∼
⎡
⎣
1
2
2
2
2
−5
3
6
6
10
1
12
⎤
⎦
1. A13(2).
Add 2 times (2.4.4) to (2.4.6).
□
It is important to realize that each elementary row operation is reversible; we can “un-
do” a given elementary row operation by another elementary row operation to bring the
modiﬁed linear system back into its original form. Speciﬁcally, in terms of the notation

2.4
Row-Echelon Matrices and Elementary Row Operations 149
introduced above, the reverse operations are determined as follows (ERO refers here to
“elementary row operation”):
ERO APPLIED TO A
REVERSE ERO APPLIED TO B
A ∼B
B ∼A
Pi j
P ji: permute row j and i in B.
Mi(k)
Mi(1/k): multiply the ith row of B by 1/k.
Ai j(k)
Ai j(−k): Add to the elements of the jth row
of B the scalar −k times the corresponding
elements of the ith row of B
We introduce a special term for matrices that are related via elementary row
operations.
DEFINITION
2.4.5
Let A be an m × n matrix. Any matrix obtained from A by a ﬁnite sequence of
elementary row operations is said to be row-equivalent to A.
Thus, all of the matrices in the previous example are row-equivalent. Since elemen-
tary row operations do not alter the solution set of a linear system, we have the next
theorem.
Theorem 2.4.6
Systems of linear equations with row-equivalent augmented matrices have the same
solution sets.
The basic result that will allow us to determine the solution set to any system of
linear equations is stated in the next theorem.
Theorem 2.4.7
Every matrix is row-equivalent to a row-echelon matrix.
According to Theorem 2.4.7, by applying an appropriate sequence of elementary row
operations to any m ×n matrix, we can always reduce it to row-echelon form. The proof
of Theorem 2.4.7 consists of giving an algorithm that will reduce an arbitrary m × n
matrix to a row-echelon matrix after a ﬁnite sequence of elementary row operations.
Before presenting such an algorithm we ﬁrst illustrate the result with an example.
When a matrix A has been reduced to a row-echelon matrix, we say that it has been
reduced to row-echelon form and refer to the resulting matrix as a row-echelon form
of A.
Example 2.4.8
Use elementary row operations to reduce
⎡
⎢⎢⎣
2
1
−1
3
1
−1
2
1
−4
6
−7
1
2
0
1
3
⎤
⎥⎥⎦to row-echelon form.
Solution:
We show each step in detail.
Step 1: Put a leading 1 in the (1,1) position.
This can be most easily accomplished by permuting rows 1 and 2.
⎡
⎢⎢⎣
2
1
−1
3
1
−1
2
1
−4
6
−7
1
2
0
1
3
⎤
⎥⎥⎦
1∼
⎡
⎢⎢⎣
1
−1
2
1
2
1
−1
3
−4
6
−7
1
2
0
1
3
⎤
⎥⎥⎦

150
CHAPTER 2
Matrices and Systems of Linear Equations
We could also accomplish this by multiplying row 1 by 1/2. However, this would intro-
duce fractions into the matrix and thereby complicate the remaining computations. In
hand calculations, fewer algebraic errors result if we avoid the use of fractions. In this
case, we can obtain a leading 1 without the use of fractions by permuting rows 1 and 2.
Step 2: Use the leading 1 to put zeros beneath it in column 1.
This is accomplished by adding appropriate multiples of row 1 to the remaining
rows.
2∼
⎡
⎢⎢⎣
1
−1
2
1
0
3
−5
1
0
2
1
5
0
2
−3
1
⎤
⎥⎥⎦
Step 2 row operations:
⎧
⎪⎨
⎪⎩
Add –2 times row 1 to row 2.
Add 4 times row 1 to row 3.
Add –2 times row 1 to row 4.
Step 3: Put a leading 1 in the (2,2) position.
We could accomplish this by multiplying row 2 by 1/3. However, once more this
wouldintroducefractionsintothematrixandmakesubsequentcalculationsmoretedious.
3∼
⎡
⎢⎢⎣
1
−1
2
1
0
1
−6
−4
0
2
1
5
0
2
−3
1
⎤
⎥⎥⎦
Step 3 row operation: Add –1 times row 3 to row 2.
Step 4: Use the leading 1 in the (2,2) position to put zeros beneath it in column 2.
We now add appropriate multiples of row 2 to the rows beneath it. For row-echelon
form, we need not be concerned about the row above it, however.
4∼
⎡
⎢⎢⎣
1
−1
2
1
0
1
−6
−4
0
0
13
13
0
0
9
9
⎤
⎥⎥⎦
Step 4 row operations:
,
Add –2 times row 2 to row 3.
Add –2 times row 2 to row 4.
Step 5: Put a leading 1 in the (3,3) position.
This can be accomplished by multiplying row 3 by 1/13.
5∼
⎡
⎢⎢⎣
1
−1
2
1
0
1
−6
−4
0
0
1
1
0
0
9
9
⎤
⎥⎥⎦
Step 6: Use the leading 1 in the (3,3) position to put zeros beneath it in column 3.
The appropriate row operation is to add –9 times row three to row four.
6∼
⎡
⎢⎢⎣
1
−1
2
1
0
1
−6
−4
0
0
1
1
0
0
0
0
⎤
⎥⎥⎦
This is a row-echelon matrix, and hence, the given matrix has been reduced to row-
echelon form. The speciﬁc operations used at each step are given next using the notation
introduced previously in this section. In future examples, we will simply indicate brieﬂy
the elementary row operation used at each step. The following shows this description
for the present example.
1. P12
2. A12(−2), A13(4), A14(−2)
3. A32(−1)
4. A23(−2), A24(−2)
5. M3(1/13)
6. A34(−9)
□

2.4
Row-Echelon Matrices and Elementary Row Operations 151
Remarks on performing elementary row operations.
1. The reader may have noticed that the particular steps taken to reduce a matrix to
row-echelon form are not uniquely determined. Therefore, we may have multiple
strategies for reducing a matrix to row-echelon form, and indeed, many possible
row-echelon forms for a given matrix A.
2. Notice in steps 2 and 4 of Example 2.4.8, we performed multiple elementary row
operations of the type Ai j(k) in a single step. With this one exception, the reader is
strongly advised not to combine multiple elementary row operations into a single
step, particularly when the elementary row operations are of different types. This
is a common source of calculation errors.
3. To reiterate an important point we made earlier, note that we could have achieved
a leading 1 in the (1, 1) position in step 1 of Example 2.4.8 by multiplying the ﬁrst
row by 1/2, rather than permuting the ﬁrst two rows. We chose not to multiply
the ﬁrst row by 1/2 in order to avoid introducing fractions into the subsequent
calculations.
The reader is urged to study the foregoing example very carefully, since it illustrates
the general procedure for reducing an m×n matrix to row-echelon form using elementary
row operations. This is a procedure that will be used repeatedly throughout the text. The
idea behind reduction to row-echelon form is to start at the upper left-hand corner of the
matrix and proceed downward and to the right in the matrix. The following algorithm
formalizes the steps that reduce any m × n matrix to row-echelon form using a ﬁnite
number of elementary row operations and thereby provides a proof of Theorem 2.4.7.
An illustration of this algorithm is given in Figure 2.4.1.
~
~
* * *
zeros
Nonzero elements
0 
0
*
*
*
*
*
*
*
Pivot position
Submatrix
Pivot column
Elementary row
operations applied
to pivot column
in submatrix
Elementary row
operations applied
to rows beneath
pivot position
New submatrix
. . .
* * *
zeros
0 
0
*
*
*
*
*
*
1
. . .
* * *
zeros
0 
0
0
.
0.
*
*
*
*
*
*
1
. . .
Figure 2.4.1: Illustration of an algorithm for reducing an m × n matrix to row-echelon form.
Algorithm for Reducing an m × n Matrix A to Row-Echelon Form
1. Start with an m × n matrix A. If A = 0, go to (7).
2. Determine the leftmost nonzero column (this is called a pivot column
and the topmost position in this column is called a pivot position).
3. Use elementary row operations to put a 1 in the pivot position.
4. Use elementary row operations to put zeros below the pivot position.
5. If there are no more nonzero rows below the pivot position go to (7),
otherwise go to (6).
6. Apply (2)–(5) to the submatrix consisting of the rows that lie
below the pivot position.
7. The matrix is a row-echelon matrix.

152
CHAPTER 2
Matrices and Systems of Linear Equations
Remark
In order to obtain a row-echelon matrix, we put a 1 in each pivot position.
However, many algorithms for solving systems of linear equations numerically are based
around the preceding algorithm except in step (3) we place a nonzero number (not
necessarily a 1) in the pivot position. Of course, the matrix resulting from an application
of this algorithm differs from a row-echelon matrix since it will have arbitrary nonzero
elements in the pivot positions.
Example 2.4.9
Reduce
⎡
⎣
3
2
−5
2
1
1
−1
1
1
0
−3
4
⎤
⎦to row-echelon form.
Solution:
Applying the row-reduction algorithm leads to the following sequence of
elementary row operations. The speciﬁc row operations used at each step are given at
the end of the process.
⎡
⎣
3
2
−5
2
1
1
−1
1
1
0
−3
4
⎤
⎦1∼
⎡
⎣
1
1
−1
1
3
2
−5
2
1
0
−3
4
⎤
⎦2∼
⎡
⎣
1
1
−1
1
0
−1
−2
−1
0
−1
−2
3
⎤
⎦
✲
✻
Pivot column
Pivot position
❤
♠
Pivot column
✻
#
#
✠
Pivot position
3∼
⎡
⎣
1
1
−1
1
0
1
2
1
0
−1
−2
3
⎤
⎦4∼
⎡
⎣
1
1
−1
1
0
1
2
1
0
0
0
4
⎤
⎦5∼
⎡
⎣
1
1
−1
1
0
1
2
1
0
0
0
1
⎤
⎦
❤
Pivot position
✻
❅❅
❘
Pivot column
This is a row-echelon matrix and hence we are done. The row operations used are
summarized here:
1. P12
2. A12(−3), A13(−1)
3. M2(−1)
4. A23(1)
5. M3(1/4)
□
The Rank of a Matrix
We now derive some further results on row-echelon matrices that will be required in the
next section to develop the theory for solving systems of linear equations.
As we stated earlier, a row-echelon form for a matrix A is not unique. Given one
row-echelon form for A, we can always obtain a different row-echelon form for A by
taking the ﬁrst row-echelon form for A and adding some multiple of a given row to any
rows above it. The result is still in row-echelon form.
However, even though the row-echelon form of A is not unique, we do have the
following theorem (in Chapter 4 we will see how the proof of this theorem arises naturally
from the more sophisticated ideas from linear algebra yet to be introduced).
Theorem 2.4.10
Let A be an m × n matrix. All row-echelon matrices that are row-equivalent to A have
the same number of nonzero rows.
Theorem 2.4.10 associates a number with any m × n matrix A; namely, the number
of nonzero rows in any row-echelon form of A. As we will see in the next section, this
number is fundamental in determining the solution properties of linear systems, and it

2.4
Row-Echelon Matrices and Elementary Row Operations 153
indeed plays a central role in linear algebra in general. For this reason, we give it a special
name.
DEFINITION
2.4.11
The number of nonzero rows in any row-echelon form of a matrix A is called the
rank of A and is denoted rank(A).
Example 2.4.12
Determine rank(A) if A =
⎡
⎣
3
−1
4
2
1
−1
2
3
7
−1
8
0
⎤
⎦.
Solution:
In order to determine rank(A), we must ﬁrst reduce A to row-echelon form.
⎡
⎣
3
−1
4
2
1
−1
2
3
7
−1
8
0
⎤
⎦1∼
⎡
⎣
1
−1
2
3
3
−1
4
2
7
−1
8
0
⎤
⎦2∼
⎡
⎣
1
−1
2
3
0
2
−2
−7
0
6
−6
−21
⎤
⎦
3∼
⎡
⎣
1
−1
2
3
0
2
−2
−7
0
0
0
0
⎤
⎦4∼
⎡
⎣
1
−1
2
3
0
1
−1
−7
2
0
0
0
0
⎤
⎦.
Sincetherearetwononzerorowsinthisrow-echelonformof A,itfollowsfromDeﬁnition
2.4.11 that rank(A) = 2.
1. P12
2. A12(−3), A13(−7)
3. A23(−3)
4. M2(1/2)
□
In the preceding example, the original matrix A had three nonzero rows, whereas
any row-echelon form of A has only two nonzero rows. We can interpret this as follows.
The three row vectors of A can be considered as vectors in R4 with components
a1 = (3, −1, 4, 2),
a2 = (1, −1, 2, 3),
a3 = (7, −1, 8, 0).
In performing elementary row operations on A, we are taking combinations of these
vectors in the following way:
c1a1 + c2a2 + c3a3,
and thus the rows of a row-echelon form of A are all of this form. We have been combining
the vectors linearly. The fact that we obtained a row of zeros in the row-echelon form
means that one of the vectors can be written in terms of the other two vectors. Reducing
the matrix to row-echelon form has uncovered this relationship among the three vectors.
We shall have much more to say about this in Chapter 4.
Remark
If A is an m×n matrix, then rank(A) ≤m and rank(A) ≤n. This is because
the number of nonzero rows in a row-echelon form of A is equal to the number of pivots
in a row-echelon form of A, which cannot exceed the number of rows or columns of A,
since there can be at most one pivot per row and per column.
Reduced Row-Echelon Matrices
It will be necessary in the future to consider the special row-echelon matrices that arise
when zeros are placed above, as well as beneath, each leading 1. Any such matrix is
called a reduced row-echelon matrix and is deﬁned precisely as follows.

154
CHAPTER 2
Matrices and Systems of Linear Equations
DEFINITION
2.4.13
An m ×n matrix is called a reduced row-echelon matrix if it satisﬁes the following
conditions:
1. It is a row-echelon matrix.
2. Any column that contains a leading 1 has zeros everywhere else.
Example 2.4.14
The following are examples of reduced row-echelon matrices:
⎡
⎣
1
3
0
0
0
0
1
0
0
0
0
1
⎤
⎦,
'1
−1
7
0
0
0
0
1
(
,
⎡
⎣
1
0
5
3
0
1
2
1
0
0
0
0
⎤
⎦,
and
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦.
□
Although an m × n matrix A does not have a unique row-echelon form, in reducing
A to a reduced row-echelon matrix we are making a particular choice of row-echelon
matrix since we arrange that all elements above each leading 1 are zeros. In view of this,
the following theorem should not be too surprising.
Theorem 2.4.15
An m × n matrix is row-equivalent to a unique reduced row-echelon matrix.
The unique reduced row-echelon matrix to which a matrix A is row-equivalent will
be called the reduced row-echelon form of A. As illustrated in the next example, the row
reduction algorithm is easily extended to determine the reduced row-echelon form of
A—we just put zeros above and beneath each leading 1.
Example 2.4.16
Determine the reduced row-echelon form of A =
⎡
⎣
3
−2
−1
17
2
2
−4
8
−1
4
−3
1
⎤
⎦.
Solution:
We apply the row reduction algorithm, but put zeros above and below the
leading 1s. In so doing, it is immaterial whether we ﬁrst reduce A to row-echelon form
and then arrange zeros above the leading 1s, or arrange zeros both above and below the
leading 1s as we proceed from left to right.
A =
⎡
⎣
3
−2
−1
17
2
2
−4
8
−1
4
−3
1
⎤
⎦1∼
⎡
⎣
−1
4
−3
1
2
2
−4
8
3
−2
−1
17
⎤
⎦2∼
⎡
⎣
1
−4
3
−1
2
2
−4
8
3
−2
−1
17
⎤
⎦
3∼
⎡
⎣
1
−4
3
1
0
10
−10
10
0
10
−10
20
⎤
⎦4∼
⎡
⎣
1
−4
3
−1
0
1
−1
1
0
1
−1
2
⎤
⎦
5∼
⎡
⎣
1
−4
3
−1
0
1
−1
1
0
0
0
1
⎤
⎦6∼
⎡
⎣
1
−4
3
0
0
1
−1
0
0
0
0
1
⎤
⎦7∼
⎡
⎣
1
0
−1
0
0
1
−1
0
0
0
0
1
⎤
⎦
which is the reduced row-echelon form of A.
1. P13
2. M1(−1) 3. A12(−2), A13(−3)
4. M2(1/10), M3(1/10)
5. A23(−1)
6. A31(1), A32(−1)
7. A21(4)
□

2.4
Row-Echelon Matrices and Elementary Row Operations 155
Exercises for 2.4
Key Terms
Elementary row operations, Row-equivalent matrices, Back
substitution, Row-echelon matrix, Row-echelon form, Lead-
ing 1, Pivot, Rank of a matrix, Reduced row-echelon matrix.
Skills
• Be able to identify if a matrix is in row-echelon form
or reduced row-echelon form.
• Be able to perform elementary row operations on a
matrix.
• Be able to determine a row-echelon form for a matrix.
• Be able to ﬁnd the rank of a matrix.
• Be able to determine a reduced row-echelon form for
a matrix.
True-False Review
For items (a)–(i), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A matrix A can have many row-echelon forms, but
only one reduced row-echelon form.
(b) Any upper triangular n × n matrix is in row-echelon
form.
(c) Any n × n matrix in row-echelon form is upper trian-
gular.
(d) If a matrix A has more rows than a matrix B, then
rank(A) ≥rank(B).
(e) For any matrices A and B of the same dimensions,
rank(A + B) = rank(A) + rank(B).
(f) For any matrices A and B of the appropriate dimen-
sions,
rank(AB) = rank(A) · rank(B).
(g) If a matrix has rank zero, then it must be the zero ma-
trix.
(h) The matrices A and 2A must have the same rank.
(i) The matrices A and 2A must have the same reduced
row-echelon form.
Problems
For Problems 1–8, determine whether the given matrices
are in reduced row-echelon form, row-echelon form but not
reduced row-echelon form, or neither.
1.
' 0
1
1
0
(
.
2.
' 1
1
0
0
(
.
3.
⎡
⎣
1
0
2
5
1
0
0
2
0
1
1
0
⎤
⎦.
4.
⎡
⎣
1
0
−1
0
0
0
1
2
0
0
0
0
⎤
⎦.
5.
⎡
⎢⎢⎣
1
0
1
2
0
0
1
1
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦.
6.
' 1
0
0
0
0
0
0
1
(
.
7.
⎡
⎣
0
1
0
0
0
0
1
0
0
0
0
0
⎤
⎦.
8.
⎡
⎣
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎦.
For Problems 9–18, use elementary row operations to reduce
the given matrix to row-echelon form, and hence determine
the rank of each matrix.
9.
'
2
−4
−4
8
(
.
10.
' 2
1
1
−3
(
.
11.
⎡
⎣
0
1
3
0
1
4
0
3
5
⎤
⎦.

156
CHAPTER 2
Matrices and Systems of Linear Equations
12.
⎡
⎣
2
1
4
2
−3
4
3
−2
6
⎤
⎦.
13.
⎡
⎣
2
−1
3
3
1
−2
2
−2
1
⎤
⎦.
14.
⎡
⎣
2
−1
3
2
2
5
⎤
⎦.
15.
⎡
⎢⎢⎣
2
−2
−1
3
3
−2
3
1
1
−1
1
0
2
−1
2
2
⎤
⎥⎥⎦.
16.
⎡
⎣
2
−1
3
4
1
−2
1
3
1
−5
0
5
⎤
⎦.
17.
⎡
⎣
2
1
3
4
2
1
0
2
1
3
2
3
1
5
7
⎤
⎦.
18.
⎡
⎢⎢⎣
4
7
4
7
3
5
3
5
2
−2
2
−2
5
−2
5
−2
⎤
⎥⎥⎦.
For Problems 19–26, reduce the given matrix to reduced row-
echelon form and hence determine the rank of each matrix.
19.
' −4
2
−6
3
(
.
20.
' 3
2
1
−1
(
.
21.
⎡
⎣
3
7
10
2
3
−1
1
2
1
⎤
⎦.
22.
⎡
⎣
3
−3
6
2
−2
4
6
−6
12
⎤
⎦.
23.
⎡
⎣
3
5
−12
2
3
−7
−2
−1
1
⎤
⎦.
24.
⎡
⎢⎢⎣
1
−1
−1
2
3
−2
0
7
2
−1
2
4
4
−2
3
8
⎤
⎥⎥⎦.
25.
⎡
⎣
1
−2
1
3
3
−6
2
7
4
−8
3
10
⎤
⎦.
26.
⎡
⎣
0
1
2
1
0
3
1
2
0
2
0
1
⎤
⎦.
Many forms of technology have commands for performing
elementary row operations on a matrix A. For example, in
the linear algebra package of Maple, the three elementary
row operations are
swaprow(A, i, j) : permute rows i and j
mulrow(A, i, k) : multiply row i by k
addrow(A, i, j) : add k times row i to row j
⋄For Problems 27–29, use some form of technology to de-
termine a row-echelon form of the given matrix.
27. The matrix in Problem 13.
28. The matrix in Problem 16.
29. The matrix in Problem 17.
⋄Many forms of technology also have built in functions
for directly determining the reduced row-echelon form of a
given matrix A. For example, in the linear algebra package
of Maple, the appropriate command is rref(A). In Problems
30–32,usetechnologytodeterminedirectlythereducedrow-
echelon form of the given matrix.
30. The matrix in Problem 22.
31. The matrix in Problem 25.
32. The matrix in Problem 26.
2.5
Gaussian Elimination
We now illustrate how elementary row operations applied to the augmented matrix of a
system of linear equations can be used ﬁrst to determine whether the system is consistent,
and second, if the system is consistent, to ﬁnd all of its solutions. In doing so, we will
develop the general theory for solving linear systems of equations.

2.5
Gaussian Elimination 157
The key observation was already made in Theorem 2.4.6. Namely, no solutions
to a linear system are gained or lost when elementary row operations are applied to
it. Therefore, starting with the augmented matrix for any linear system, we may apply
elementary row operations to bring it to row-echelon form, and then solve the resulting
simpler linear system. Let us illustrate with an example.
Example 2.5.1
Determine the solution set to
4x1 −3x2 + 6x3 =
2,
x1 −3x2 + 6x3 =
5,
(2.5.1)
−2x1 + 3x2 −8x3 = −6.
Solution:
We ﬁrst use elementary row operations to reduce the augmented matrix of
the system to row-echelon form.
⎡
⎣
4
−3
6
2
1
−3
6
5
−2
3
−8
−6
⎤
⎦1∼
⎡
⎣
1
−3
6
5
4
−3
6
2
−2
3
−8
−6
⎤
⎦2∼
⎡
⎣
1
−3
6
5
0
9
−18
−18
0
−3
4
4
⎤
⎦
3∼
⎡
⎣
1
−3
6
5
0
1
−2
−2
0
−3
4
4
⎤
⎦4∼
⎡
⎣
1
−3
6
5
0
1
−2
−2
0
0
−2
−2
⎤
⎦5∼
⎡
⎣
1
−3
6
5
0
1
−2
−2
0
0
1
1
⎤
⎦.
1. P12
2. A12(−4), A13(2)
3. M2(1/9)
4. A23(3)
5. M3(−1/2)
The system corresponding to this row-echelon form of the augmented matrix is
x1 −3x2 + 6x3 =
5,
(2.5.2)
x2 −2x3 = −2,
(2.5.3)
x3 =
1,
(2.5.4)
which can be solved by back substitution. From Equation (2.5.4), x3 = 1. Substituting
into Equation (2.5.3) and solving for x2, we ﬁnd that x2 = 0. Finally, substituting into
Equation (2.5.2) for x3 and x2 and solving for x1 yields x1 = 1. Thus, our original system
of equations has the unique solution (−1, 0, 1), and the solution set to the system is
S = {(−1, 0, 1)},
which is a subset of R3.
□
The process of reducing the augmented matrix to row-echelon form and then using
back substitution to solve the equivalent system is called Gaussian elimination. The
particular case of Gaussian elimination that arises when the augmented matrix is reduced
to reduced row-echelon form is called Gauss-Jordan elimination.
Example 2.5.2
Use Gauss-Jordan elimination to determine the solution set to
x1 −x2 −5x3 = −3,
3x1 + 2x2 −3x3 =
5,
2x1
−5x3 =
1.

158
CHAPTER 2
Matrices and Systems of Linear Equations
Solution:
In this case, we ﬁrst reduce the augmented matrix of the system to reduced
row-echelon form.
⎡
⎣
1
−1
−5
−3
3
2
−3
5
2
0
−5
1
⎤
⎦1∼
⎡
⎣
1
−1
−5
−3
0
5
12
14
0
2
5
7
⎤
⎦2∼
⎡
⎣
1
−1
−5
−3
0
1
2
0
0
2
5
7
⎤
⎦
3∼
⎡
⎣
1
−1
−5
−3
0
1
2
0
0
0
1
7
⎤
⎦4∼
⎡
⎣
1
−1
0
32
0
1
0
−14
0
0
1
7
⎤
⎦5∼
⎡
⎣
1
0
0
18
0
1
0
−14
0
0
1
7
⎤
⎦.
1. A12(−3), A13(−2)
2. A32(−2)
3. A23(−2)
4. A32(−2), A31(5)
5. A21(1)
The augmented matrix is now in reduced row-echelon form. The equivalent system is
x1
=
18,
x2
= −14,
x3 =
7.
and the solution can be read off directly as (18, −14, 7). Consequently, the given system
has solution set
S = {(18, −14, 7)}
in R3.
□
We see from the previous two examples that the advantage of Gauss-Jordan elimi-
nation over Gaussian elimination is that it does not require back substitution. However,
the disadvantage is that reducing the augmented matrix to reduced row-echelon form
requires more elementary row operations than reduction to row-echelon form. It can be
shown, in fact, that in general, Gaussian elimination is the more computationally efﬁcient
technique. As we will see in the next section, the main reason for introducing the Gauss-
Jordan method is its application to the computation of the inverse of an n × n matrix.
Remark
The Gaussian elimination method is so systematic that it can be programmed
easily on a computer. Indeed, many large-scale programs for solving linear systems are
based on the row reduction method.
In both of the preceding examples,
rank(A) = rank(A#) = number of unknowns in the system
and the system had a unique solution. More generally, we have the following lemma:
Lemma 2.5.3
Consider the m × n linear system Ax = b. Let A# denote the augmented matrix of the
system. If rank(A) = rank(A#) = n, then the system has a unique solution.
Proof If rank(A) = rank(A#) = n, then there are n leading ones in any row-echelon
form of A, and hence, back substitution gives a unique solution. The form of the row-
echelon form of A# is shown below, with m −n rows of zeros at the bottom of the matrix
omitted, and where the ∗’s denote unknown elements of the row-echelon form.
⎡
⎢⎢⎢⎢⎢⎣
1
∗
∗
∗
. . .
∗
∗
0
1
∗
∗
. . .
∗
∗
0
0
1
∗
. . .
∗
∗
...
...
...
...
. . .
...
...
0
0
0
0
. . .
1
∗
⎤
⎥⎥⎥⎥⎥⎦

2.5
Gaussian Elimination 159
Note that rank(A) cannot exceed rank(A#). Thus, there are only two possibilities
for the relationship between rank(A) and rank(A#): rank(A) < rank(A#) or rank(A) =
rank(A#). We now consider what happens in these cases.
Example 2.5.4
Determine the solution set to
x1 + x2 −
x3 + x4 = 1,
2x1 + 3x2 +
x3
= 4,
3x1 + 5x2 + 3x3 −x4 = 5.
Solution:
We use elementary row operations to reduce the augmented matrix:
⎡
⎣
1
1
−1
1
1
2
3
1
0
4
3
5
3
−1
5
⎤
⎦1∼
⎡
⎣
1
1
−1
1
1
0
1
3
−2
2
0
2
6
−4
2
⎤
⎦2∼
⎡
⎣
1
1
−1
1
1
0
1
3
−2
2
0
0
0
0
−2
⎤
⎦
1. A12(−2), A13(−3)
2. A23(−2)
The last row tells us that the system of equations has no solution (that is, it is inconsistent),
since it requires
0x1 + 0x2 + 0x3 + 0x4 = −2,
which is clearly impossible. The solution set to the system is thus the empty set ∅.
□
In the previous example, rank(A) = 2, whereas rank(A#) = 3. Thus, rank(A) <
rank(A#), and the corresponding system has no solution. Next we establish that this
result is true in general.
Lemma 2.5.5
Consider the m × n linear system Ax = b. Let A# denote the augmented matrix of the
system. If rank(A) < rank(A#), then the system is inconsistent.
Proof If rank(A) < rank(A#), then there will be one row in the reduced row-echelon
form of the augmented matrix whose ﬁrst nonzero element arises in the last column.
Such a row corresponds to an equation of the form
0x1 + 0x2 + · · · + 0xn = 1,
which has no solution. Consequently, the system is inconsistent.
Finally, we consider the case when rank(A) = rank(A#). If rank(A) = n, we have
already seen in Lemma 2.5.3 that the system has a unique solution. We now consider an
example in which rank(A) < n.
Example 2.5.6
Determine the solution set to
5x1 −6x2 + x3 = 4,
2x1 −3x2 + x3 = 1,
(2.5.5)
4x1 −3x2 −x3 = 5.

160
CHAPTER 2
Matrices and Systems of Linear Equations
Solution:
We begin by reducing the augmented matrix of the system.
⎡
⎣
5
−6
1
4
2
−3
1
1
4
−3
−1
5
⎤
⎦1∼
⎡
⎣
1
−3
2
−1
2
−3
1
1
4
−3
−1
5
⎤
⎦2∼
⎡
⎣
1
−3
2
−1
0
3
−3
3
0
9
−9
9
⎤
⎦
3∼
⎡
⎣
1
−3
2
−1
0
1
−1
1
0
9
−9
9
⎤
⎦4∼
⎡
⎣
1
−3
2
−1
0
1
−1
1
0
0
0
0
⎤
⎦
1. A31(−1)
2. A12(−2), A13(−4)
3. M2(1/3)
4. A23(−9)
The augmented matrix is now in row-echelon form, and the equivalent system is
x1 −3x2 + 2x3 = −1,
(2.5.6)
x2 −
x3 =
1.
(2.5.7)
Since we have three variables, but only two equations relating them, we are free to
specify one of the variables arbitrarily. The variable that we choose to specify is called
a free variable or free parameter. The remaining variables are then determined by the
system of equations and are called bound (or leading) variables or bound parameters.
In the foregoing system, we take x3 as the free variable and set
x3 = t,
where t can assume any real value.5 It follows from (2.5.7) that
x2 = 1 + t.
Further, from Equation (2.5.6),
x1 = −1 + 3(1 + t) −2t = 2 + t.
Thus the solution set to the given system of equations is the following subset of R3:
S = {(2 + t, 1 + t, t) : t ∈R}.
The system has an inﬁnite number of solutions obtained by allowing the parameter t to
assume all real values. For example, two particular solutions of the system are
(2, 1, 0)
and
(0, −1, −2),
corresponding to t = 0 and t = −2, respectively. Note that we can also write the solution
set S above in the form
S = {(2, 1, 0) + t(1, 1, 1) : t ∈R}.
□
Remark
The geometry of the foregoing solution is as follows. The given system
(2.5.5) can be interpreted as consisting of three planes in 3-space. Any solution to the
system gives the coordinates of a point of intersection of the three planes. In the preceding
example the planes intersect in a line whose parametric equations are
x1 = 2 + t,
x2 = 1 + t,
x3 = t.
(See Figure 2.3.1.)
5When considering systems of equations with complex coefﬁcients, we allow free variables to assume
complex values as well.

2.5
Gaussian Elimination 161
In general, the solution to a consistent m ×n system of linear equations may involve
more than one free variable. Indeed, the number of free variables will depend on how
many nonzero rows arise in any row-echelon form of the augmented matrix, A#, of the
system;thatis,itwilldependontherankof A#.Moreprecisely,ifrank(A#) = r#,thenthe
equivalent system will have only r# relationships between the n variables. Consequently,
provided the system is consistent,
Number of free variables = n −r#.
We therefore have the following lemma.
Lemma 2.5.7
Consider the m × n linear system Ax = b. Let A# denote the augmented matrix of the
system and let r# = rank(A#). If r# = rank(A) < n, then the system has an inﬁnite
number of solutions, indexed by n −r# free variables.
Proof As discussed before, any row-echelon equivalent system will have only r# equa-
tions involving the n variables, and so, there will be n −r# > 0 free variables. If we
assign arbitrary values to these free variables, then the remaining r# variables will be
uniquely determined, by back substitution, from the system. Since the free variables can
each assume inﬁnitely many values, in this case there are an inﬁnite number of solutions
to the system.
Example 2.5.8
Use Gaussian elimination to solve
x1 −2x2 + 2x3 −
x4 = 3,
3x1 + x2 + 6x3 + 11x4 = 16,
2x1 −x2 + 4x3 + 4x4 = 9.
Solution:
A row-echelon form of the augmented matrix of the system is
⎡
⎣
1
−2
2
−1
3
0
1
0
2
1
0
0
0
0
0
⎤
⎦,
so that we have two free variables. The equivalent system is
x1 −2x2 + 2x3 −x4 = 3,
(2.5.8)
x2
+ 2x4 = 1.
(2.5.9)
Notice that we cannot choose any two variables freely. For example, from Equation
(2.5.9), we cannot specify both x2 and x4 independently. The bound variables should be
taken as those that correspond to leading 1s in the row-echelon form of A#, since these
are the variables that can always be determined by back substitution (they appear as the
leftmost variable in some equation of the system corresponding to the row-echelon form
of the augmented matrix).
Choose as free variables those variables that
do not correspond to a leading 1 in a row-echelon form of A#.

162
CHAPTER 2
Matrices and Systems of Linear Equations
Applying this rule to Equations (2.5.8) and (2.5.9), we choose x3 and x4 as free variables
and therefore set
x3 = s,
x4 = t.
It then follows from Equation (2.5.9) that
x2 = 1 −2t.
Substitution into (2.5.8) yields
x1 = 5 −2s −3t,
so that the solution set to the given system is the following subset of R4:
S = {(5 −2s −3t, 1 −2t, s, t) : s, t ∈R}.
= {(5, 1, 0, 0) + s(−2, 0, 1, 0) + t(−3, −2, 0, 1) : s, t ∈R}.
□
Lemmas 2.5.3, 2.5.5, and 2.5.7 completely characterize the solution properties of
an m × n linear system. Combining the results of these three lemmas gives the next
theorem.
Theorem 2.5.9
Consider the m × n linear system Ax = b. Let r denote the rank of A, and let r# denote
the rank of the augmented matrix of the system. Then
1. If r < r#, the system is inconsistent.
2. If r = r#, the system is consistent and
(a) There exists a unique solution if and only if r# = n.
(b) There exists an inﬁnite number of solutions if and only if r# < n.
Homogeneous Linear Systems
Many of the problems that we will meet in the future will require the solution to a
homogeneous system of linear equations. The general form for such a system is
a11x1 + a12x2 + · · · + a1nxn = 0,
a21x1 + a22x2 + · · · + a2nxn = 0,
...
(2.5.10)
am1x1 + am2x2 + · · · + amnxn = 0,
or, in matrix form, Ax = 0, where A is the coefﬁcient matrix of the system and 0 denotes
the m-vector whose elements are all zeros.
Corollary 2.5.10
The homogeneous linear system Ax = 0 is consistent for any coefﬁcient matrix A, with
a solution given by x = 0.
Proof We can see immediately from (2.5.10) that if x = 0, then Ax = 0, so x = 0 is a
solution to the homogeneous linear system.
Alternatively, we can deduce the consistency of this system from Theorem 2.5.9 as
follows. The augmented matrix A# of a homogeneous linear system only differs from
that of the coefﬁcient matrix A by the addition of a column of zeros, a feature that does
not affect the rank of the matrix. Consequently, for a homogeneous system, we have
rank(A#) = rank(A), and therefore, from Theorem 2.5.9, such a system is necessarily
consistent.

2.5
Gaussian Elimination 163
Remarks
1. The solution x = 0 is referred to as the trivial solution. Consequently, from
Theorem 2.5.9, a homogeneous system either has only the trivial solution or it has
an inﬁnite number of solutions (one of which must be the trivial solution).
2. Once more it is worth mentioning the geometric interpretation of Corollary 2.5.10
in the case of a homogeneous system with three unknowns. We can regard each
equation of such a system as deﬁning a plane. Due to the homogeneity, each plane
passes through the origin, and hence, the planes intersect at least at the origin.
Often we will be interested in determining whether a given homogeneous system
has an inﬁnite number of solutions, and not in actually obtaining the solutions. The
following corollary to Theorem 2.5.9 can sometimes be used to determine by inspection
whether a given homogeneous system has nontrivial solutions:
Corollary 2.5.11
A homogeneous system of m linear equations in n unknowns, with m < n, has an inﬁnite
number of solutions.
Proof Letr andr# be as in Theorem 2.5.9. Using the fact thatr = r# for a homogeneous
system, we see that since r# ≤m < n, Theorem 2.5.9 implies that the system has an
inﬁnite number of solutions.
Remark
If m ≥n, then we may or may not have nontrivial solutions, depending on
whether the rank of the augmented matrix, r#, satisﬁes r# < n, or r# = n, respectively.
We encourage the reader to construct linear systems that illustrate each of these two
possibilities.
Example 2.5.12
Determine the solution set to Ax = 0, if A =
⎡
⎣
0
2
3
0
1
−1
0
3
7
⎤
⎦.
Solution:
The augmented matrix of the system is
⎡
⎣
0
2
3
0
0
1
−1
0
0
3
7
0
⎤
⎦,
with reduced row-echelon form
⎡
⎣
0
1
0
0
0
0
1
0
0
0
0
0
⎤
⎦.
The equivalent system is
x2 = 0
x3 = 0.
It is tempting, but incorrect, to conclude from this that the solution to the system
is x1 = x2 = x3 = 0. Since x1 does not occur in the system, it is a free variable and
therefore not necessarily zero. Consequently, the correct solution to the foregoing system
is (t, 0, 0), where t is a free variable, and the solution set is {(t, 0, 0) : t ∈R}.
□

164
CHAPTER 2
Matrices and Systems of Linear Equations
The linear systems that we have so far encountered have all had real coefﬁcients, and
we have considered corresponding real solutions. The techniques that we have developed
for solving linear systems are also applicable to the case when our system has complex
coefﬁcients. The corresponding solutions will also be complex.
Remark
In general, the simplest method of putting a leading 1 in a position that con-
tains the complex number a + ib is to multiply the corresponding row by
1
a2 + b2 (a −ib). This is illustrated in steps 1 and 4 in the next example. If difﬁ-
culties are encountered, then this is an indication that consultation of Appendix A is
in order.
Example 2.5.13
Determine the solution set to
(1 + 2i)x1 +
4x2 +
(3 + i)x3 = 0,
(2 −i)x1 + (1 + i)x2 +
3x3 = 0,
5ix1 + (7 −i)x2 + (3 + 2i)x3 = 0.
Solution:
We reduce the augmented matrix of the system.
⎡
⎣
1 + 2i
4
3 + i
0
2 −i
1 + i
3
0
5i
7 −i
3 + 2i
0
⎤
⎦1∼
⎡
⎢⎣
1
4
5(1 −2i)
1 −i
0
2 −i
1 + i
3
0
5i
7 −i
3 + 2i
0
⎤
⎥⎦
2∼
⎡
⎢⎢⎢⎣
1
4
5(1 −2i)
1 −i
0
0
(1 + i) −4
5(1 −2i)(2 −i)
3 −(1 −i)(2 −i)
0
0
(7 −i) −4i(1 −2i)
(3 + 2i) −5i(1 −i)
0
⎤
⎥⎥⎥⎦
=
⎡
⎢⎣
1
4
5(1 −2i)
1 −i
0
0
1 + 5i
2 + 3i
0
0
−1 −5i
−2 −3i
0
⎤
⎥⎦
3∼
⎡
⎢⎣
1
4
5(1 −2i)
1 −i
0
0
1 + 5i
2 + 3i
0
0
0
0
0
⎤
⎥⎦
4∼
⎡
⎢⎢⎢⎣
1
4
5(1 −2i)
1 −i
0
0
1
1
26(17 −7i)
0
0
0
0
0
⎤
⎥⎥⎥⎦.
1. M1((1 −2i)/5)
2. A12(−(2 −i)), A13(−5i)
3. A23(1)
4. M2((1 −5i)/26)
This matrix is now in row-echelon form. The equivalent system is
x1 + 4
5(1 −2i)x2 +
(1 −i)x3 = 0,
x2 + 1
26(17 −7i)x3 = 0.
There is one free variable, which we take to be x3 = t, where t can assume any complex
value. Applying back substitution yields
x2 = 1
26t(−17 + 7i)
x1 = −2
65t(1 −2i)(−17 + 7i) −t(1 −i) = −1
65t(59 + 17i)

2.5
Gaussian Elimination 165
so that the solution set to the system is the subset of C3
!"
−1
65t(59 + 17i), 1
26t(−17 + 7i), t
#
: t ∈C
$
.
□
Exercises for 2.5
Key Terms
Gaussian elimination, Gauss-Jordan elimination, Free vari-
ables, Bound (or leading) variables, Trivial solution.
Skills
• Be able to solve a linear system of equations by Gaus-
sian elimination and by Gauss-Jordan elimination.
• Be able to identify free variables and bound variables
and know how they are used to construct the solution
set to a linear system.
• Understand the relationship between the ranks of A
and A#, and how this affects the number of solutions
to a linear system.
True-False Review
For items (a)–(f), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The process by which a matrix is brought via elemen-
tary row operations to row-echelon form is known as
Gauss-Jordan elimination.
(b) A homogeneous linear system of equations is always
consistent.
(c) For a linear system Ax = b, every column of the
row-echelon form of A corresponds either to a lead-
ing variable or to a free variable, but not both, of the
linear system.
(d) A linear system Ax = b is consistent if and only if the
last column of the row-echelon form of the augmented
matrix [A | b] is not a pivoted column.
(e) A linear system is consistent if and only if there are
free variables in the row-echelon form of the corre-
sponding augmented matrix.
(f) The columns of the row-echelon form of A# that con-
tain the leading 1s correspond to the free variables.
Problems
For Problems 1–12, use Gaussian elimination to determine
the solution set to the given system.
1.
x1 −5x2 = 3,
3x1 −9x2 = 15.
2. 4x1 −x2 = 8,
2x1 + x2 = 1.
3.
7x1 −3x2 = 5,
14x1 −6x2 = 10.
4.
x1 + 2x2 + x3 = 1,
3x1 + 5x2 + x3 = 3,
2x1 + 6x2 + 7x3 = 1.
5.
3x1 −x2
=
1,
2x1 + x2 + 5x3 =
4,
7x1 −5x2 −8x3 = −3.
6.
3x1 + 5x2 −
x3 = 14,
x1 + 2x2 +
x3 = 3,
2x1 + 5x2 + 6x3 = 2.
7.
6x1 −3x2 + 3x3 = 12,
2x1 −x2 +
x3 = 4,
−4x1 + 2x2 −2x3 = −8.
8.
2x1 −x2 + 3x3 = 14,
3x1 + x2 −2x3 = −1,
7x1 + 2x2 −3x3 =
3,
5x1 −x2 −2x3 =
5.
9.
2x1 −x2 −4x3 = 5,
3x1 + 2x2 −5x3 = 8,
5x1 + 6x2 −6x3 = 20,
x1 + x2 −3x3 =−3.
10.
x1 + 2x2 −
x3 + x4 = 1,
2x1 + 4x2 −2x3 + 2x4 = 2,
5x1 + 10x2 −5x3 + 5x4 = 5.

166
CHAPTER 2
Matrices and Systems of Linear Equations
11.
x1 + 2x2 −
x3 + x4 = 1,
2x1 −3x2 +
x3 −x4 = 2,
x1 −5x2 + 2x3 −2x4 = 1,
4x1 + x2 −
x3 + x4 = 3.
12.
x1 + 2x2 + x3 +
x4 −2x5 = 3,
x3 + 4x4 −3x5 = 2,
2x1 + 4x2 −x3 −10x4 + 5x5 = 0.
For Problems 13–18, use Gauss-Jordan elimination to deter-
mine the solution set to the given system.
13.
2x1 −x2 −
x3 =
2,
4x1 + 3x2 −2x3 = −1,
x1 + 4x2 +
x3 =
4.
14.
3x1 + x2 + 5x3 = 2,
x1 + x2 −
x3 = 1,
2x1 + x2 + 2x3 = 3.
15.
x1
−2x3 = −3,
3x1 −2x2 −4x3 = −9,
x1 −4x2 + 2x3 = −3.
16.
2x1 −x2 + 3x3 −x4 =
3,
3x1 + 2x2 +
x3 −5x4 = −6,
x1 −2x2 + 3x3 + x4 =
6.
17.
x1 + x2 + x3 −x4 =
4,
x1 −x2 −x3 −x4 =
2,
x1 + x2 −x3 + x4 = −2,
x1 −x2 + x3 + x4 = −8.
18.
2x1 −x2 + 3x3 + x4 −
x5 = 11,
x1 −3x2 −2x3 −x4 −2x5 =
2,
3x1 + x2 −2x3 −x4 +
x5 = −2,
x1 + 2x2 +
x3 + 2x4 + 3x5 = −3,
5x1 −3x2 −3x3 + x4 + 2x5 =
2.
For Problems 19–23, determine the solution set to the sys-
tem Ax = b for the given coefﬁcient matrix A and right-hand
side vector b.
19. A =
⎡
⎣
1
−3
1
5
−4
1
2
4
−3
⎤
⎦, b =
⎡
⎣
8
15
−4
⎤
⎦.
20. A =
⎡
⎣
1
0
5
3
−2
11
2
−2
6
⎤
⎦, b =
⎡
⎣
0
2
2
⎤
⎦.
21. A =
⎡
⎣
0
1
−1
0
5
1
0
2
1
⎤
⎦, b =
⎡
⎣
−2
8
5
⎤
⎦.
22. A =
⎡
⎣
1
−1
0
−1
2
1
3
7
3
−2
1
0
⎤
⎦, b =
⎡
⎣
2
2
4
⎤
⎦.
23. A =
⎡
⎢⎢⎣
1
1
0
1
3
1
−2
3
2
3
1
2
−2
3
5
−2
⎤
⎥⎥⎦, b =
⎡
⎢⎢⎣
2
8
3
−9
⎤
⎥⎥⎦.
24. Determine all values of the constant k for which the
following system has (a) no solution, (b) an inﬁnite
number of solutions, and (c) a unique solution.
x1 + 2x2 −
x3 =
3,
2x1 + 5x2 +
x3 =
7,
x1 + x2 −k2x3 = −k.
25. Determine all values of the constant k for which the
following system has (a) no solution, (b) an inﬁnite
number of solutions, and (c) a unique solution.
2x1 + x2 −x3 + x4 = 0,
x1 + x2 + x3 −x4 = 0,
4x1 + 2x2 −x3 + x4 = 0,
3x1 −x2 + x3 + kx4 = 0.
26. Determine all values of the constants a and b for which
the following system has (a) no solution, (b) an inﬁnite
number of solutions, and (c) a unique solution.
x1 + x2 +
x3 = 4,
3x1 + 5x2 −4x3 = 16,
2x1 + 3x2 −ax3 = b.
27. Determine all values of the constants a and b for which
the following system has (a) no solution, (b) an inﬁnite
number of solutions, and (c) a unique solution.
x1 −
ax2 = 3,
2x1 +
x2 = 6,
−3x1 + (a + b)x2 = 1.

2.5
Gaussian Elimination 167
28. Show that the system
x1 + x2 + x3 = y1,
2x1 + 3x2 + x3 = y2,
3x1 + 5x2 + x3 = y3,
has an inﬁnite number of solutions provided that
(y1, y2, y3) lies on the plane with equation y1 −2y2 +
y3 = 0.
29. Consider the system of linear equations
a11x1 + a12x2 = b1,
a21x1 + a22x2 = b2.
Deﬁne $, $1, and $2 by
$ = a11a22 −a12a21,
$1 = a22b1 −a12b2, $2 = a11b2 −a12b1.
(a) Show that the given system has a unique solution
if and only if $ ̸= 0, and that the unique solution
in this case is x1 = $1/$, x2 = $2/$.
(b) If $ = 0 and a11 ̸= 0, determine the conditions
on $2 that would guarantee that the system has (i)
no solution, (ii) an inﬁnite number of solutions.
(c) Interpret your results in terms of intersections of
straight lines.
Gaussian elimination with partial pivoting uses the follow-
ing algorithm to reduce the augmented matrix:
(1) Start with augmented matrix A#.
(2) Determine the leftmost nonzero column.
(3) Permute rows to put the element of largest ab-
solute value in the pivot position.
(4) Use elementary row operations to put zeros be-
neath the pivot position.
(5) If there are no more nonzero rows below the
pivot position, go to 7, otherwise go to 6.
(6) Apply (2)–(5) to the submatrix consisting of the
rows that lie below the pivot position.
(7) The matrix is in reduced form.6
In Problems 30–33, use the preceding algorithm to reduce A#
and then apply back substitution to solve the equivalent sys-
tem. Technology might be useful in performing the required
row operations.
30. The system in Problem 4.
31. The system in Problem 8.
6Notice that this reduced form is not a row-echelon matrix.
32. The system in Problem 9.
33. The system in Problem 13.
34. (a) An n × n system of linear equations whose ma-
trix of coefﬁcients is a lower triangular matrix is
called a lower triangular system. Assuming that
aii ̸= 0 for each i, devise a method for solving
such a system that is analogous to the back sub-
stitution method.
(b) Use your method from (a) to solve
x1
= 2,
2x1 −3x2
= 1,
3x1 + x2 −x3 = 8.
35. Find all solutions to the following nonlinear system of
equations:
4x3
1 + 2x2
2 + 3x3 = 12,
x3
1 −x2
2 +
x3 = 2,
3x3
1 + x2
2 −
x3 = 2.
Does your answer contradict Theorem 2.5.9? Explain.
For Problems 36–46, determine the solution set to the given
system.
36.
3x1 + 2x2 −x3 = 0,
2x1 + x2 + x3 = 0,
5x1 −4x2 + x3 = 0.
37.
2x1 + x2 −
x3 = 0,
3x1 −x2 + 2x3 = 0,
x1 −x2 −
x3 = 0,
5x1 + 2x2 −2x3 = 0.
38.
2x1 −x2 −
x3 = 0,
5x1 −x2 + 2x3 = 0,
x1 + x2 + 4x3 = 0.
39.
(1 + 2i) x1 + (1 −i)x2 +
x3 = 0,
ix1 + (1 + i)x2 −
ix3 = 0,
2ix1 +
x2 + (1 + 3i)x3 = 0.
40.
3x1 + x2 +
x3 = 0,
6x1 −x2 + 2x3 = 0,
12x1 + 6x2 + 4x3 = 0.

168
CHAPTER 2
Matrices and Systems of Linear Equations
41.
2x1 + x2 −8x3 = 0,
3x1 −2x2 −5x3 = 0,
5x1 −6x2 −3x3 = 0,
3x1 −5x2 +
x3 = 0.
42.
x1 + (1 + i)x2 +
(1 −i)x3 = 0,
ix1 +
x2 +
ix3 = 0,
(1 −2i)x1 −(1 −i)x2 + (1 −3i)x3 = 0.
43.
x1 −x2 +
x3 = 0,
3x2 + 2x3 = 0,
3x1
−
x3 = 0,
5x1 + x2 −
x3 = 0.
44.
2x1 −4x2 + 6x3 = 0,
3x1 −6x2 + 9x3 = 0,
x1 −2x2 + 3x3 = 0,
5x1 −10x2 + 15x3 = 0.
45.
4x1 −2x2 −
x3 −x4 = 0,
3x1 + x2 −2x3 + 3x4 = 0,
5x1 −x2 −2x3 + x4 = 0.
46.
2x1 + x2 −x3 + x4 = 0,
x1 + x2 + x3 −x4 = 0,
3x1 −x2 + x3 −2x4 = 0,
4x1 + 2x2 −x3 + x4 = 0.
For Problems 47–57, determine the solution set to the system
Ax = 0 for the given matrix A.
47. A =
' 2
−1
3
4
(
.
48. A =
' 1 −i
2i
1 + i
−2
(
.
49. A =
'
1 + i
1 −2i
−1 + i
2 + i
(
.
50. A =
⎡
⎣
1
2
3
2
−1
0
1
1
1
⎤
⎦.
51. A =
⎡
⎣
1
1
1
−1
−1
0
−1
2
1
3
2
2
⎤
⎦.
52. A =
⎡
⎣
2 −3i
1 + i
i −1
3 + 2i
−1 + i
−1 −i
5 −i
2i
−2
⎤
⎦.
53. A =
⎡
⎣
1
3
0
−2
−3
0
1
4
0
⎤
⎦.
54. A =
⎡
⎢⎢⎢⎢⎣
1
0
3
3
−1
7
2
1
8
1
1
5
−1
1
−1
⎤
⎥⎥⎥⎥⎦
.
55. A =
⎡
⎣
1
−1
0
1
3
−2
0
5
−1
2
0
1
⎤
⎦.
56. A =
⎡
⎣
1
0
−3
0
3
0
−9
0
−2
0
6
0
⎤
⎦.
57. A =
⎡
⎣
2 + i
i
3 −2i
i
1 −i
4 + 3i
3 −i
1 + i
1 + 5i
⎤
⎦.
2.6
The Inverse of a Square Matrix
In this section, we investigate the situation when, for a given n ×n matrix A, there exists
a matrix B satisfying
AB = In
and
B A = In
(2.6.1)
and derive an efﬁcient method for determining B (when it does exist). As a possible
application of the existence of such a matrix B, consider the n × n linear system
Ax = b.
(2.6.2)
Premultiplying both sides of (2.6.2) by an n × n matrix B yields
(B A)x = Bb.

2.6
The Inverse of a Square Matrix 169
Assuming that B A = In, this reduces to
x = Bb.
(2.6.3)
Thus, we have determined a solution to the system (2.6.2) by a matrix multiplication. Of
course, this depends on the existence of a matrix B satisfying (2.6.1), and even if such a
matrix B does exist, it will turn out that using (2.6.3) to solve n × n systems is not very
efﬁcient computationally. Therefore it is generally not used in practice to solve n × n
systems. However, from a theoretical point of view, a formula such as (2.6.3) is very
useful. We begin the investigation by establishing that there can be at most one matrix
B satisfying (2.6.1) for a given n × n matrix A.
Theorem 2.6.1
Let A be an n × n matrix. Suppose B and C are both n × n matrices satisfying
AB = B A = In
(2.6.4)
AC = C A = In
(2.6.5)
respectively. Then B = C.
Proof From (2.6.4), it follows that
C = C In = C(AB).
That is,
C = (C A)B = In B = B,
where we have used (2.6.5) to replace C A by In in the second step.
Since the identity matrix In plays the role of the number 1 in the multiplication of
matrices, the properties given in (2.6.1) are the analogs for matrices of the properties
xx−1 = 1,
x−1x = 1,
which hold for all (nonzero) numbers x. It is therefore natural to denote the matrix B
in (2.6.1) by A−1 and to call it the inverse of A. The following deﬁnition introduces the
appropriate terminology.
DEFINITION
2.6.2
Let A be an n × n matrix. If there exists an n × n matrix A−1 satisfying
AA−1 = A−1 A = In,
then we call A−1 the matrix inverse to A, or just the inverse of A. We say that A is
invertible if A−1 exists.
Invertible matrices are sometimes called nonsingular, while matrices that are not
invertible are sometimes called singular.
Remark
It is important to realize that A−1 denotes the matrix that satisﬁes
AA−1 = A−1 A = In.
It does not mean 1
A, which has no meaning whatsoever.

170
CHAPTER 2
Matrices and Systems of Linear Equations
Example 2.6.3
If A =
⎡
⎣
1
−1
2
2
−3
3
1
−1
1
⎤
⎦, verify that B =
⎡
⎣
0
−1
3
1
−1
1
1
0
−1
⎤
⎦is the inverse of A.
Solution:
By direct multiplication, we ﬁnd that
AB =
⎡
⎣
1
−1
2
2
−3
3
1
−1
1
⎤
⎦
⎡
⎣
0
−1
3
1
−1
1
1
0
−1
⎤
⎦=
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦= I3
and
B A =
⎡
⎣
0
−1
3
1
−1
1
1
0
−1
⎤
⎦
⎡
⎣
1
−1
2
2
−3
3
1
−1
1
⎤
⎦=
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦= I3.
Consequently, (2.6.1) is satisﬁed, and hence, B is indeed the inverse of A. We
therefore write
A−1 =
⎡
⎣
0
−1
3
1
−1
1
1
0
−1
⎤
⎦.
□
As we will see throughout this text, invertible matrices are of tremendous theoretical
importance. The next example, which is more theoretical in nature, aims to focus the
reader’s attention on checking the requirement in Deﬁnition 2.6.2 for two matrices to be
inverses of one another.
Example 2.6.4
Suppose A is an n × n matrix such that A3 = 0n. Verify that In −2A is invertible
and that
(In −2A)−1 = In + 2A + 4A2.
Solution:
We should be careful not to write the expression (In −2A)−1 prematurely,
since we must ﬁrst establish that In−2A is indeed invertible. To do this, we use properties
of the identity matrix to observe that
(In −2A)(In + 2A + 4A2) = In + 2A + 4A2 −2A −4A2 −8A3
= In + 2A + 4A2 −2A −4A2
= In,
and
(In + 2A + 4A2)(In −2A) = In −2A + 2A −4A2 + 4A2 −8A3
= In −2A + 2A −4A2 + 4A2
= In,
which shows that In + 2A + 4A2 satisﬁes the requirements of (In −2A)−1 set forth in
Deﬁnition 2.6.2.
□
We now return to the n × n system of equations (2.6.2).
Theorem 2.6.5
If A−1 exists, then the n × n system of linear equations
Ax = b

2.6
The Inverse of a Square Matrix 171
has the unique solution
x = A−1b
for every b in Rn.
Proof We can verify by direct substitution that x = A−1b is indeed a solution to the
linear system. To see why this solution is unique, observe that for any solution x1 to the
system Ax = b, we have Ax1 = b. Premultiplying both sides by A−1 and using the fact
that A−1 A = In, we conclude that x1 = A−1b.
Our next theorem establishes when A−1 exists, and it also uncovers an efﬁcient
method for computing A−1.
Theorem 2.6.6
An n × n matrix A is invertible if and only if rank(A) = n.
Proof If A−1 exists, then by Theorem 2.6.5, any n × n linear system Ax = b has a
unique solution. Hence, Theorem 2.5.9 implies that rank(A) = n.
Conversely, suppose rank(A) = n. We must establish that there exists an n × n
matrix X satisfying
AX = In = X A.
Let e1, e2, . . . , en denote the column vectors of the identity matrix In. Since rank(A) = n,
Theorem 2.5.9 implies that each of the linear systems
Axi = ei,
i = 1, 2, . . . , n
(2.6.6)
hasauniquesolution7 xi.Consequently,ifwelet X = [x1, x2, . . . , xn],wherex1, x2, . . . , xn
are the unique solutions of the systems in (2.6.6), then
A[x1, x2, . . . , xn] = [Ax1, Ax2, . . . , Axn] = [e1, e2, . . . , en];
that is,
AX = In.
(2.6.7)
We must also show that, for the same matrix X,
X A = In.
Postmultiplying both sides of (2.6.7) by A yields
(AX)A = A.
That is,
A(X A −In) = 0n.
(2.6.8)
We claim that X A −In = 0n. To see this, let y1, y2, . . . , yn denote the column
vectors of the n × n matrix X A −In. Equating corresponding column vectors on either
side of (2.6.8) implies that
Ayi = 0,
i = 1, 2, . . . , n.
(2.6.9)
But, by assumption, rank(A) = n, and so each of the systems in (2.6.9) has a unique solu-
tion that, since the systems are homogeneous, must be the trivial solution. Consequently,
7Notice that for an n × n system Ax = b, if rank(A) = n, then rank(A#) = n.

172
CHAPTER 2
Matrices and Systems of Linear Equations
each yi is the zero vector, and thus
X A −In = 0n.
Therefore,
X A = In.
(2.6.10)
Equations (2.6.7) and (2.6.10) imply that X = A−1.
We now have the following converse to Theorem 2.6.5.
Corollary 2.6.7
Let A be an n × n matrix. If Ax = b has a unique solution for some column n-vector b,
then A−1 exists.
Proof If Ax = b has a unique solution, then from Theorem 2.5.9, rank(A) = n, and
so from the previous theorem, A−1 exists.
Remark
In particular, the above corollary tells us that if the homogeneous linear
system Ax = 0 has only the trivial solution x = 0, then A−1 exists.
Other criteria for deciding whether or not an n × n matrix A has an inverse will be
developed in the next several chapters, but our goal at present is to develop a method for
ﬁnding A−1, should it exist.
Assuming that rank (A) = n, let x1, x2, . . . , xn denote the column vectors of A−1.
Then, from (2.6.6), these column vectors can be obtained by solving each of the n × n
systems
Axi = ei,
i = 1, 2, . . . , n.
As we now show, some computation can be saved if we employ the Gauss-Jordan method
in solving these systems. We ﬁrst illustrate the method when n = 3. In this case, from
(2.6.6), the column vectors of A−1 are determined by solving the three linear systems
Ax1 = e1,
Ax2 = e2,
Ax3 = e3.
The augmented matrices of these systems can be written as
⎡
⎣
1
A
0
0
⎤
⎦,
⎡
⎣
0
A
1
0
⎤
⎦,
⎡
⎣
0
A
0
1
⎤
⎦,
respectively. Furthermore, since rank(A) = 3 by assumption, the reduced row-echelon
form of A is I3. Consequently, using elementary row operations to reduce the augmented
matrix of the ﬁrst system to reduced row-echelon form will yield, schematically,
⎡
⎣
1
A
0
0
⎤
⎦∼ERO
. . . ∼
⎡
⎣
1
0
0
a1
0
1
0
a2
0
0
1
a3
⎤
⎦,
which implies that the ﬁrst column vector of A−1 is
x1 =
⎡
⎣
a1
a2
a3
⎤
⎦.

2.6
The Inverse of a Square Matrix 173
Similarly, for the second system, the reduction
⎡
⎣
0
A
1
0
⎤
⎦∼ERO
. . . ∼
⎡
⎣
1
0
0
b1
0
1
0
b2
0
0
1
b3
⎤
⎦,
implies that the second column vector of A−1 is
x2 =
⎡
⎣
b1
b2
b3
⎤
⎦.
Finally, for the third system, the reduction
⎡
⎣
0
A
0
1
⎤
⎦∼ERO
. . . ∼
⎡
⎣
1
0
0
c1
0
1
0
c2
0
0
1
c3
⎤
⎦,
implies that the third column vector of A−1 is
x3 =
⎡
⎣
c1
c2
c3
⎤
⎦.
Consequently,
A−1 = [x1, x2, x3] =
⎡
⎣
a1
b1
c1
a2
b2
c2
a3
b3
c3
⎤
⎦.
The key point to notice is that in solving for x1, x2, x3 we use the same elementary
row operations to reduce A to I3. We can therefore save a signiﬁcant amount of work by
combining the foregoing operations as follows:
⎡
⎣
1
0
0
A
0
1
0
0
0
1
⎤
⎦∼ERO
. . . ∼
⎡
⎣
1
0
0
a1
b1
c1
0
1
0
a2
b2
c2
0
0
1
a3
b3
c3
⎤
⎦.
The generalization to the n × n case is immediate. We form the n × 2n matrix [A | In]
and reduce A to In using elementary row operations. Schematically,
[A | In] ∼ERO
... ∼[In | A−1].
This method of ﬁnding A−1 is called the Gauss-Jordan technique.
Remark
Notice that if we are given an n × n matrix A, we likely will not know
from the outset whether rank(A) = n, and hence, we will not know whether A−1 exists.
However, if at any stage in the row reduction of [A | In] we ﬁnd that rank(A) < n, then
it will follow from Theorem 2.6.6 that A is not invertible.
Example 2.6.8
Find A−1 if A =
⎡
⎣
1
1
3
0
1
2
3
5
−1
⎤
⎦.

174
CHAPTER 2
Matrices and Systems of Linear Equations
Solution:
Using the Gauss-Jordan technique we proceed as follows.
⎡
⎣
1
1
3
1
0
0
0
1
2
0
1
0
3
5
−1
0
0
1
⎤
⎦1∼
⎡
⎣
1
1
3
1
0
0
0
1
2
0
1
0
0
2
−10
−3
0
1
⎤
⎦
2∼
⎡
⎣
1
0
1
1
−1
0
0
1
2
0
1
0
0
0
−14
−3
−2
1
⎤
⎦3∼
⎡
⎣
1
0
1
1
−1
0
0
1
2
0
1
0
0
0
1
3
14
1
7
−1
14
⎤
⎦
4∼
⎡
⎢⎢⎣
1
0
0
11
14
−8
7
1
14
0
1
0
−3
7
5
7
1
7
0
0
1
3
14
1
7
−1
14
⎤
⎥⎥⎦
Thus,
A−1 =
⎡
⎢⎢⎢⎢⎣
11
14
−8
7
1
14
−3
7
5
7
1
7
3
14
1
7
−1
14
⎤
⎥⎥⎥⎥⎦
.
We leave it as an exercise to conﬁrm that AA−1 = A−1A = I3.
1. A13(−3)
2. A21(−1), A23(−2)
3. M3(−1/14)
4. A31(−1), A32(−2)
□
Example 2.6.9
Continuing the previous example, use A−1 to solve the system
x1 + x2 + 3x3 = 2,
x2 + 2x3 = 1,
3x1 + 5x2 −
x3 = 4.
Solution:
The system can be written as
Ax = b,
where A is the matrix in the previous example, and b =
⎡
⎣
2
1
4
⎤
⎦. Since A is invertible, the
system has a unique solution that can be written as x = A−1b. Thus, from the previous
example we have
x =
⎡
⎢⎢⎢⎢⎣
11
14
−8
7
1
14
−3
7
5
7
1
7
3
14
1
7
−1
14
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
2
1
4
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
5
7
3
7
2
7
⎤
⎥⎥⎥⎥⎦
.
Consequently, x1 = 5
7, x2 = 3
7, and x3 = 2
7, so that the solution to the system is
35
7, 3
7, 2
7
4
.
□
We now return to more theoretical information pertaining to the inverse of a matrix.

2.6
The Inverse of a Square Matrix 175
Properties of the Inverse
The inverse of an n × n matrix satisﬁes the properties stated in the following theorem,
which should be committed to memory:
Theorem 2.6.10
Let A and B be invertible n × n matrices. Then
1. A−1 is invertible and (A−1)−1 = A.
2. AB is invertible and (AB)−1 = B−1 A−1.
3. AT is invertible and (AT )−1 = (A−1)T .
Proof The proof of each result consists of verifying that the appropriate matrix products
yield the identity matrix.
1. We must verify that
A−1A = In
and
AA−1 = In.
Both of these follow directly from Deﬁnition 2.6.2.
2. We must verify that
(AB)(B−1A−1) = In
and
(B−1A−1)(AB) = In.
We establish the ﬁrst equality, leaving the second equation as an exercise. We have
(AB)(B−1)(A−1) = A(BB−1)A−1 = AIn A−1 = AA−1 = In.
3. We must verify that
AT (A−1)T = In
and
(A−1)T AT = In.
Again, we prove the ﬁrst part, leaving the second part as an exercise. First recall
from Theorem 2.2.23 that AT BT = (B A)T . Using this property with B = A−1
yields
AT (A−1)T = (A−1A)T = I T
n = In.
The proof of (2) above can easily be extended to a statement about invertibility of a
product of an arbitrary ﬁnite number of matrices. More precisely, we have the following.
Corollary 2.6.11
Let A1, A2, . . . , Ak be invertible n × n matrices. Then A1A2 · · · Ak is invertible, and
(A1A2 · · · Ak)−1 = A−1
k A−1
k−1 · · · A−1
1 .
Proof The proof is left as an exercise (Problem 32).
Some Further Theoretical Results
Finally in this section, we establish two results that will be required in Section 2.7 and
also in one of the proofs that arises in Section 3.2.
Theorem 2.6.12
Let A and B be n × n matrices. If AB = In, then both A and B are invertible and
B = A−1.

176
CHAPTER 2
Matrices and Systems of Linear Equations
Proof Let b be an arbitrary column n-vector. Then, since AB = In, we have
A(Bb) = Inb = b.
Consequently, for every b, the system Ax = b has the solution x = Bb. But this implies
that rank(A) = n. To see why, suppose that rank(A) < n, and let A∗denote a row-
echelon form of A. Note that the last row of A∗is zero. Choose b∗to be any column
n-vector whose last component is nonzero. Then, since rank(A) < n, it follows that
the system
A∗x = b∗
is inconsistent. But, applying to the augmented matrix [A∗| b∗] the inverse row opera-
tions that reduced A to row-echelon form yields [A | b] for some b. Since Ax = b has
the same solution set as A∗x = b∗, it follows that Ax = b is inconsistent. We therefore
have a contradiction, and so it must be the case that rank(A) = n, and therefore that A
is invertible by Theorem 2.6.6.
We now establish that8 A−1 = B. Since AB = In by assumption, we have
A−1 = A−1In = A−1(AB) = (A−1 A)B = In B = B,
as required. It now follows directly from property (1) of Theorem 2.6.10 that B is
invertible with inverse A.
Corollary 2.6.13
Let A and B be n × n matrices. If AB is invertible, then both A and B are invertible.
Proof If we let C = B(AB)−1 and D = AB, then
AC = AB(AB)−1 = DD−1 = In.
It follows from Theorem 2.6.12 that A is invertible. Similarly, if we let C = (AB)−1A,
then
C B = (AB)−1 AB = In.
Once more we can apply Theorem 2.6.12 to conclude that B is invertible.
Exercises for 2.6
Key Terms
Inverse, Invertible, Singular, Nonsingular, Gauss-Jordan
technique.
Skills
• Be able to check directly whether or not two matrices
A and B are inverses of each other.
• Be able to ﬁnd the inverse of an invertible matrix via
the Gauss-Jordan technique.
• Be able to use the inverse of a coefﬁcient matrix of a
linear system in order to solve the system.
• Know the basic properties related to how the inverse
operation behaves with respect to itself, multiplica-
tion, and transpose (Theorem 2.6.10).
True-False Review
For items (a)–(j), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Aninvertiblematrixis alsoknownas asingular matrix.
(b) Every square matrix that does not contain a row of
zeros is invertible.
8Note that it now makes sense to speak of A−1, whereas prior to proving in the preceding paragraph that A
is invertible, it would not have been legal to use the notation A−1.

2.6
The Inverse of a Square Matrix 177
(c) A linear system Ax = b with an n × n invertible co-
efﬁcient matrix A has a unique solution.
(d) If A is a matrix such that there exists a matrix B with
AB = In, then A is invertible.
(e) If A and B are invertible n × n matrices, then so is
A + B.
(f) If A and B are invertible n × n matrices, then so is
AB.
(g) If A is an invertible matrix such that A2 = A, then A
is the identity matrix.
(h) If A is an n×n invertible matrix and B and C are n×n
matrices such that AB = AC, then B = C.
(i) If A is a 5×5 matrix of rank 4, then A is not invertible.
(j) If A is a 6 × 6 matrix of rank 6, then A is invertible.
Problems
For Problems 1–4, verify by direct multiplication that the
given matrices are inverses of one another.
1. A =
' 4
9
3
7
(
, A−1 =
'
7
−9
−3
4
(
.
2. A =
' 2
−1
3
−1
(
, A−1 =
' −1
1
−3
2
(
.
3. A =
' a
b
c
d
(
, A−1 =
1
ad −bc
'
d
−b
−c
a
(
,
provided ad −bc ̸= 0.
4. A =
⎡
⎣
3
5
1
1
2
1
2
6
7
⎤
⎦, A−1 =
⎡
⎣
8
−29
3
−5
19
−2
2
−8
1
⎤
⎦.
For Problems 5–18, determine A−1, if possible, using the
Gauss-Jordan method. If A−1 exists, check your answer by
verifying that AA−1 = In.
5. A =
' 1
2
1
3
(
.
6. A =
'
1
1 + i
1 −i
1
(
.
7. A =
'
1
−i
−1 + i
2
(
.
8. A =
' 0
0
0
0
(
.
9. A =
⎡
⎣
1
−1
2
2
1
11
4
−3
10
⎤
⎦.
10. A =
⎡
⎣
3
5
1
1
2
1
2
6
7
⎤
⎦.
11. A =
⎡
⎣
0
1
0
0
0
1
0
1
2
⎤
⎦.
12. A =
⎡
⎣
4
2
−13
2
1
−7
3
2
4
⎤
⎦.
13. A =
⎡
⎣
1
2
−3
2
6
−2
−1
1
4
⎤
⎦.
14. A =
⎡
⎣
1
i
2
1 + i
−1
2i
2
2i
5
⎤
⎦.
15. A =
⎡
⎣
2
1
3
1
−1
2
3
3
4
⎤
⎦.
16. A =
⎡
⎢⎢⎣
1
−1
2
3
2
0
3
−4
3
−1
7
8
1
0
3
5
⎤
⎥⎥⎦.
17. A =
⎡
⎢⎢⎣
0
−2
−1
−3
2
0
2
1
1
−2
0
2
3
−1
−2
0
⎤
⎥⎥⎦.
18. A =
⎡
⎢⎢⎣
1
2
0
0
3
4
0
0
0
0
5
6
0
0
7
8
⎤
⎥⎥⎦.
19. Let A =
⎡
⎣
−1
−2
3
−1
1
1
−1
−2
−1
⎤
⎦. Find the third column
vector of A−1 without determining the other columns
of the inverse matrix.
20. Let A =
⎡
⎣
2
−1
4
5
1
2
1
−1
3
⎤
⎦. Find the second column
vector of A−1 without determining the other columns
of the inverse matrix.

178
CHAPTER 2
Matrices and Systems of Linear Equations
For Problems 21–26, use A−1 to ﬁnd the solution to the given
system.
21.
6x1 + 20x2= −8,
2x1 + 7x2=
2.
22.
x1 + 3x2= 1,
2x1 + 5x2= 3.
23.
x1 + x2 −2x3 = −2,
x2 +
x3 =
3,
2x1 + 4x2 −3x3 =
1.
24.
x1 −2ix2=
2,
(2 −i)x1 + 4ix2 = −i.
25.
3x1 + 4x2 + 5x3 = 1,
2x1 + 10x2 +
x3 = 1,
4x1 +
x2 + 8x3 = 1.
26.
x1 + x2 + 2x3 =
12,
x1 + 2x2 −
x3 =
24,
2x1 −x2 +
x3 = −36.
An n × n matrix A is called orthogonal if AT = A−1.
For Problems 27–30, show that the given matrices are
orthogonal.
27. A =
'
0
1
−1
0
(
.
28. A =
' √
3/2
1/2
−1/2
√
3/2
(
.
29. A =
'
cos α
sin α
−sin α
cos α
(
.
30. A =
1
1 + 2x2
⎡
⎣
1
−2x
2x2
2x
1 −2x2
−2x
2x2
2x
1
⎤
⎦.
31. Complete the proof of Theorem 2.6.10 by verifying
the remaining properties in parts (2) and (3).
32. Prove Corollary 2.6.11.
For Problems 33–34, use properties of the inverse to prove
the given statement.
33. If A is an n×n invertible skew-symmetric matrix, then
A−1 is skew-symmetric.
34. If A is an n ×n invertible symmetric matrix, then A−1
is symmetric.
35. Let A be an n × n matrix with A12 = 0. Prove that
In −A3 is invertible with
(In −A3)−1 = In + A3 + A6 + A9.
36. Let A be an n × n matrix with A4 = 0. Prove that
In −A is invertible with
(In −A)−1 = In + A + A2 + A3.
37. Suppose the inverse of the matrix A5 is B3. What is
the inverse of A15? Prove your answer.
38. Suppose the inverse of the matrix A3 is B−1. What is
the inverse of A9? Prove your answer.
39. Prove that if A, B, C are n × n matrices satisfying
B A = In and AC = In, then B = C.
40. If A, B, C are n × n matrices satisfying B A = In and
C A = In, does it follow that B = C? Justify your
answer.
41. Consider the general 2 × 2 matrix
A =
' a11
a12
a21
a22
(
and let $ = a11a22 −a12a21 with a11 ̸= 0. Show that
if $ ̸= 0,
A−1 = 1
$
'
a22
−a12
−a21
a11
(
.
The quantity $ deﬁned above is referred to as the de-
terminant of A. We will investigate determinants in
more detail in the next chapter.
42. Let A be an n × n matrix, and suppose that we have
to solve the p linear systems
Axi = bi,
i = 1, 2, . . . , p
where the bi are given. Devise an efﬁcient method for
solving these systems.
43. Use your method from the previous problem to solve
the three linear systems
Axi = bi,
i = 1, 2, 3
if
A =
⎡
⎣
1
−1
1
2
−1
4
1
1
6
⎤
⎦, b1 =
⎡
⎣
1
1
−1
⎤
⎦,
b2 =
⎡
⎣
−1
2
5
⎤
⎦, b3 =
⎡
⎣
2
3
2
⎤
⎦.

2.7
Elementary Matrices and the LU Factorization 179
44. Let A be an m × n matrix with m ≤n.
(a) If rank(A) = m, prove that there exists a matrix
B satisfying AB = Im. Such a matrix is called a
right inverse of A.
(b) If A =
' 1
3
1
2
7
4
(
, determine all right in-
verses of A.
⋄For Problems 45–46, reduce the matrix [A In] to reduced
row-echelon form and thereby determine, if possible, the in-
verse of A.
45. A =
⎡
⎣
5
9
17
7
21
13
27
16
8
⎤
⎦.
46. A is a randomly generated 4 × 4 matrix.
⋄For Problems 47–49, use built-in functions of some form
of technology to determine rank(A) and, if possible, A−1.
47. A =
⎡
⎣
3
5
−7
2
5
9
13
−11
22
⎤
⎦.
48. A =
⎡
⎢⎢⎣
7
13
15
21
9
−2
14
23
17
−27
22
31
19
−42
21
33
⎤
⎥⎥⎦.
49. A is a randomly generated 5 × 5 matrix.
50. ⋄For the system in Problem 25, determine A−1 and
use it to solve the system.
51. ⋄Consider the n × n Hilbert matrix
Hn =
'
1
i + j −1
(
,
1 ≤i, j ≤n.
(a) Determine H4 and show that it is invertible.
(b) Find H−1
4
and use it to solve H4x = b if b =
[2, −1, 3, 5]T .
2.7
Elementary Matrices and the LU Factorization
We now introduce some matrices that can be used to perform elementary row operations
on a matrix. Although they are of limited computational use, they do play a signiﬁcant
role in linear algebra and its applications.
DEFINITION
2.7.1
Any matrix obtained by performing a single elementary row operation on the identity
matrix is called an elementary matrix.
In particular, an elementary matrix is always a square matrix. In general we will denote
elementary matrices by E. If we are describing a speciﬁc elementary matrix, then in
keeping with the notation introduced previously for elementary row operations, we will
use the following notation for the three types of elementary matrices:
Type 1: Pi j — permute rows i and j in In
Type 2: Mi(k) — multiply row i of In by the nonzero scalar k
Type 3: Ai j(k) — add k times row i of In to row j of In
Example 2.7.2
Write all 2 × 2 elementary matrices.
Solution:
From Deﬁnition 2.7.1 and using the notation introduced above, we have
1. Permutation matrix:
P12 =
' 0
1
1
0
(
.

180
CHAPTER 2
Matrices and Systems of Linear Equations
2. Scaling matrices:
M1(k) =
' k
0
0
1
(
, M2(k) =
' 1
0
0
k
(
.
3. Row combinations:
A12(k) =
' 1
0
k
1
(
, A21(k) =
' 1
k
0
1
(
.
□
We leave it as an exercise to verify that the n × n elementary matrices have the
following structure:
Pi j: ones along main diagonal except (i, i) and ( j, j), ones in the (i, j) and ( j, i)
positions, and zeros elsewhere
Mi(k): the diagonal matrix diag(1, 1, . . . , k, . . . , 1), where k appears in the (i, i)
position
Ai j(k): ones along the main diagonal, k in the ( j, i) position, and zeros elsewhere.
One of the key points to note about elementary matrices is the following:
Premultiplying an n × p matrix A by an n × n elementary matrix E has the effect
of performing the corresponding elementary row operation on A.
Rather than proving this statement, which we leave as an exercise, we illustrate with
an example.
Example 2.7.3
If A =
' −1
6
−4
2
−8
−1
(
, then for example,
M1(k)A =
' k
0
0
1
( ' −1
6
−4
2
−8
−1
(
=
' −k
6k
−4k
2
−8
−1
(
.
Similarly,
A21(k)A =
' 1
k
0
1
( ' −1
6
−4
2
−8
−1
(
=
' −1 + 2k
6 −8k
−4 −k
2
−8
−1
(
. □
Since elementary row operations can be performed on a matrix by premultiplication
by an appropriate elementary matrix, it follows that any matrix A can be reduced to row-
echelon form by multiplication by a sequence of elementary matrices. Schematically we
can therefore write
Ek Ek−1 . . . E2E1A = U,
where U denotes a row-echelon form of A and the Ei are elementary matrices.
Example 2.7.4
Determine elementary matrices that reduce A =
' 2
3
1
4
(
to row-echelon form.
Solution:
We can reduce A to row-echelon form using the following sequence of
elementary row operations:
' 2
3
1
4
(
1∼
' 1
4
2
3
(
2∼
' 1
4
0
−5
(
3∼
' 1
4
0
1
(
.
1. P12
2. A12(−2)
3. M2(−1/5)

2.7
Elementary Matrices and the LU Factorization 181
Consequently,
M2
!
−1
5
"
A12(−2)P12 A =
# 1
4
0
1
$
,
which we can verify by direct multiplication:
M2
!
−1
5
"
A12(−2)P12 A =
# 1
0
0
−1
5
$ #
1
0
−2
1
$ # 0
1
1
0
$ # 2
3
1
4
$
=
# 1
0
0
−1
5
$ #
1
0
−2
1
$ # 1
4
2
3
$
=
# 1
0
0
−1
5
$ # 1
4
0
−5
$
=
# 1
4
0
1
$
.
□
Since any elementary row operation is reversible, it follows that each elementary
matrix is invertible. Indeed, in the 2 × 2 case it is easy to see that
P−1
12 =
# 0
1
1
0
$
,
M1(k)−1 =
# 1/k
0
0
1
$
,
M2(k)−1 =
# 1
0
0
1/k
$
,
A12(k)−1 =
#
1
0
−k
1
$
,
A21(k)−1 =
# 1
−k
0
1
$
.
We leave it as an exercise to verify that in the n × n case, we have:
Mi(k)−1 = Mi(1/k),
P−1
i j = Pi j,
Ai j(k)−1 = Ai j(−k).
Now consider an invertible n × n matrix A. Since the unique reduced row-echelon
form of such a matrix is the identity matrix In, it follows from the preceding discussion
that there exist elementary matrices E1, E2, . . . , Ek such that
Ek Ek−1 . . . E2E1A = In.
But this implies that
A−1 = Ek Ek−1 . . . E2E1,
and hence,
A = (A−1)−1 = (Ek · · · E2E1)−1 = E−1
1 E−1
2
· · · E−1
k ,
which is a product of elementary matrices. So any invertible matrix is a product of
elementary matrices. Conversely, since elementary matrices are invertible, a product of
elementary matrices is a product of invertible matrices, hence is invertible by Corollary
2.6.11. Therefore, we have established the following.
Theorem 2.7.5
Let A be an n ×n matrix. Then A is invertible if and only if A is a product of elementary
matrices.
The LU Decomposition of an Invertible Matrix9
For the remainder of this section, we restrict our attention to invertible n ×n matrices. In
reducing such a matrix to row-echelon form, we have always placed leading ones on the
main diagonal in order that we obtain a row-echelon matrix. We now lift the requirement
that the main diagonal of the row-echelon form contain ones. As a consequence, the
matrix that results from row reduction will be an upper triangular matrix, but will not
necessarily be in row-echelon form. Furthermore, reduction to such an upper triangular
form can be accomplished without the use of Type 2 row operations.
9The material in the remainder of this section is not used elsewhere in the text.

182
CHAPTER 2
Matrices and Systems of Linear Equations
Example 2.7.6
Use elementary row operations to reduce the matrix A =
⎡
⎣
2
5
3
3
1
−2
−1
2
1
⎤
⎦to upper
triangular form.
Solution:
The given matrix can be reduced to upper triangular form using the fol-
lowing sequence of elementary row operations:
⎡
⎣
2
5
3
3
1
−2
−1
2
1
⎤
⎦1∼
⎡
⎢⎢⎢⎢⎣
2
5
2
0
−13
2
−13
2
0
9
2
5
2
⎤
⎥⎥⎥⎥⎦
2∼
⎡
⎢⎢⎢⎢⎣
2
5
3
0
−13
2
−13
2
0
0
−2
⎤
⎥⎥⎥⎥⎦
.
1. A12(−3/2), A13(1/2)
2. A23(9/13)
□
When using elementary row operations of Type 3, the multiple of a speciﬁc row that
is subtracted from row i to put a zero in the (i, j) position is called a multiplier, and
denoted mi j. Thus, in the preceding example, there are three multipliers; namely,
m21 = 3
2,
m31 = −1
2,
m32 = −9
13.
The multipliers will be used in the forthcoming discussion.
In Example 2.7.6 we were able to reduce A to upper triangular form using only row
operations of Type 3. This is not always the case. For example, the matrix
' 0
5
3
2
(
requires that the two rows be permuted to obtain an upper triangular form. For the
moment, however, we will restrict our attention to invertible matrices A for which the
reduction to upper triangular form can be accomplished without permuting rows. In this
case, we can therefore reduce A to upper triangular form using row operations of Type
3 only. Furthermore, throughout the reduction process, we can restrict ourselves to Type
3 operations that add multiples of a row to rows beneath that row, by simply performing
the row operations column by column, from left to right. According to our description
of the elementary matrices Ai j(k), our reduction process therefore uses only elementary
matrices that are unit lower triangular. More speciﬁcally, in terms of elementary matrices
we have
Ek Ek−1 . . . E2E1A = U,
where Ek, Ek−1, . . . , E2, E1 are unit lower triangular Type 3 elementary matrices and U
is an upper triangular matrix. Since each elementary matrix is invertible, we can write
the preceding equation as
A = E−1
1 E−1
2
. . . E−1
k U.
(2.7.1)
But, as we have already argued, each of the elementary matrices in (2.7.1) is a unit lower
triangular matrix, and we know from Corollary 2.2.25 that the product of two unit lower
triangular matrices is also a unit lower triangular matrix. Consequently, (2.7.1) can be
written as
A = LU,
(2.7.2)
where
L = E−1
1 E−1
2
. . . E−1
k
(2.7.3)

2.7
Elementary Matrices and the LU Factorization 183
is a unit lower triangular matrix and U is an upper triangular matrix. Equation (2.7.2)
is referred to as the LU factorization of A. It can be shown (Problem 30) that this LU
factorization is unique.
Example 2.7.7
Determine the LU factorization of the matrix
A =
⎡
⎣
2
5
3
3
1
−2
−1
2
1
⎤
⎦.
Solution:
Using the results of Example 2.7.6, we can write
E3E2E1A =
⎡
⎣
2
5
3
0
−13
2
−13
2
0
0
−2
⎤
⎦,
where
E1 = A12
%
−3
2
&
,
E2 = A13
%1
2
&
,
and
E3 = A23
% 9
13
&
.
Therefore,
U =
⎡
⎢⎣
2
5
3
0
−13
2
−13
2
0
0
−2
⎤
⎥⎦
and from (2.7.3),
L = E−1
1 E−1
2
. . . E−1
k .
(2.7.4)
Computing the inverses of the elementary matrices, we have
E−1
1
= A12
%3
2
&
,
E−1
2
= A13
%
−1
2
&
,
and
E−1
3
= A23
%
−9
13
&
.
Substituting these results into (2.7.4) yields
L =
⎡
⎣
1
0
0
3
2
1
0
0
0
1
⎤
⎦
⎡
⎣
1
0
0
0
1
0
−1
2
0
1
⎤
⎦
⎡
⎣
1
0
0
0
1
0
0
−9
13
1
⎤
⎦=
⎡
⎢⎣
1
0
0
3
2
1
0
−1
2
−9
13
1
⎤
⎥⎦.
Consequently,
A =
⎡
⎢⎣
1
0
0
3
2
1
0
−1
2
−9
13
1
⎤
⎥⎦
⎡
⎢⎣
2
5
3
0
−13
2
−13
2
0
0
−2
⎤
⎥⎦,
which is easily veriﬁed by a matrix multiplication.
□
Computing the lower triangular matrix L in the LU factorization of A using (2.7.3)
can require a signiﬁcant amount of work. However, if we look carefully at the matrix
L in Example 2.7.7, we see that the elements beneath the leading diagonal are just the
corresponding multipliers. That is, if li j denotes the (i, j) element of the matrix L, then
li j = mi j,
i > j.
(2.7.5)
Furthermore, it can be shown that this relationship holds in general. Consequently, we
do not need to use (2.7.3) to obtain L. Instead we use row operations of Type 3 to reduce
A to upper triangular form and then we can use (2.7.5) to obtain L directly.

184
CHAPTER 2
Matrices and Systems of Linear Equations
Example 2.7.8
Determine the LU decomposition for the matrix
A =
⎡
⎢⎢⎣
−1
1
3
2
2
−3
1
2
5
−1
2
−24
3
2
46
−15
⎤
⎥⎥⎦.
Solution:
To determine U, we reduce A to upper triangular form using only row
operations of Type 3 in which we add multiples of a given row only to rows below the
given row.
A 1∼
⎡
⎢⎢⎣
−1
1
3
2
0 −1
7
6
0
4 17 −14
0
5 55
−9
⎤
⎥⎥⎦
2∼
⎡
⎢⎢⎣
−1
1
3
2
0 −1
7
6
0
0 45 10
0
0 90 21
⎤
⎥⎥⎦
3∼
⎡
⎢⎢⎣
−1
1
3
2
0 −1
7
6
0
0 45 10
0
0
0
1
⎤
⎥⎥⎦= U.
Row Operations
Corresponding Multipliers
(1) A12(2),
A13(5),
A14(3)
m21 = −2,
m31 = −5,
m41 = −3
(2) A23(4),
A24(5)
m32 = −4,
m42 = −5
(3) A34(−2)
m43 = 2
Consequently, from (2.7.4),
L =
⎡
⎢⎢⎣
1
0
0
0
−2
1
0
0
−5
−4
1
0
−3
−5
2
1
⎤
⎥⎥⎦.
We leave it as an exercise to verify that LU = A.
□
The question that is undoubtedly in the reader’s mind is: what is the use of the LU
decomposition? In order to answer this question, consider the n × n system of linear
equation Ax = b, where A = LU. If we write the system as
LUx = b
and let Ux = y, then solving Ax = b is equivalent to solving the pair of equations
Ly = b,
Ux = y.
Due to the triangular form of each of the coefﬁcient matrices L and U, these systems
can be solved easily — the ﬁrst one by “forward" substitution, and the second one by
back substitution. In the case when we have a single right-hand side vector b there
is no advantage to using the LU factorization for solving the system over Gaussian
elimination. However, if we require the solution of several systems of equations with the
same coefﬁcient matrix A, say
Axi = bi,
i = 1, 2, . . . , p

2.7
Elementary Matrices and the LU Factorization 185
then it is more efﬁcient to compute the LU factorization of A once, and then successively
solve the triangular systems
Lyi = bi,
Uxi = yi.
5
i = 1, 2, . . . , p.
Example 2.7.9
Use the LU decomposition of
A =
⎡
⎢⎢⎣
−1
1
3
2
2
−3
1
2
5
−1
2
−24
3
2
46
−15
⎤
⎥⎥⎦
to solve the system Ax = b if b =
⎡
⎢⎢⎣
−7
39
−70
−110
⎤
⎥⎥⎦.
Solution:
We have shown in the previous example that A = LU where
L =
⎡
⎢⎢⎣
1
0
0
0
−2
1
0
0
−5
−4
1
0
−3
−5
2
1
⎤
⎥⎥⎦
and
U =
⎡
⎢⎢⎣
−1
1
3
2
0
−1
7
6
0
0
45
10
0
0
0
1
⎤
⎥⎥⎦.
Wenowsolvethetwotriangularsystems Ly = bandUx = y.Usingforwardsubstitution
on the ﬁrst of these systems, we have
y1 = −7,
y2 = 39 + 2y1 = 25,
y3 = −70 + 5y1 + 4y2 = −5,
y4 = −110 + 3y1 + 5y2 −2y3 = 4.
Solving Ux = y via back substitution yields
x4 = y4 = 4,
x3 = −1
45(5 + 10x4) = −1,
x2 = 7x3 + 6x4 −25 = −8,
x1 = x2 + 3x3 + 2x4 + 7 = 4.
Consequently,
x = (4, −8, −1, 4).
□
In the more general case when row interchanges are required to reduce an invertible
matrix A to upper triangular form, it can be shown that A has a factorization of the form
A = PLU,
(2.7.6)
where P is an appropriate product of elementary permutation matrices, L is a unit
lower triangular matrix, and U is an upper triangular matrix. From the properties of the
elementary permutation matrices, it follows (see Problem 28) that P−1 = PT . Using
(2.7.6) the linear system Ax = b can be written as
PLUx = b,

186
CHAPTER 2
Matrices and Systems of Linear Equations
or equivalently,
LUx = PT b.
Consequently, to solve Ax = b in this case we can solve the two triangular systems
6
Ly = PT b,
Ux = y.
For a full discussion of this and other factorizations of n × n matrices, and their
applications, the reader is referred to more advanced texts on linear algebra or numerical
analysis (for example, B. Noble and J.W. Daniel, Applied Linear Algebra, Prentice Hall,
1988; J.Ll. Morris, Computational Methods in Elementary Numerical Analysis, Wiley,
1983).
Exercises for 2.7
Key Terms
Elementary matrix, Multiplier, LU Factorization of a matrix.
Skills
• Be able to determine whether or not a given matrix is
an elementary matrix.
• Know the form for the permutation matrices, scaling
matrices, and row combination matrices.
• Be able to write down the inverse of an elementary
matrix without any computation.
• Be able to determine elementary matrices that reduce
a given matrix to row-echelon form.
• Be able to express an invertible matrix as a product of
elementary matrices.
• Be able to determine the multipliers of a matrix.
• Be able to determine the LU factorization of a matrix.
• Be able to use the LU factorization of a matrix A to
solve a linear system Ax = b.
True-False Review
For items (a)–(j), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every elementary matrix is invertible.
(b) A product of elementary matrices is an elementary
matrix.
(c) Every matrix can be expressed as a product of elemen-
tary matrices.
(d) If A is an m × n matrix and E is an m × m elementary
matrix, then the matrices A and E A have the same
rank.
(e) If Pi j is a permutation matrix, then P2
i j = Pi j.
(f) If E1 and E2 are n × n elementary matrices, then
E1E2 = E2E1.
(g) If E1 and E2 are n×n elementary matrices of the same
type, then E1E2 = E2E1.
(h) Every matrix has an LU factorization.
(i) In the LU factorization of a matrix A, the matrix L is a
unit lower triangular matrix and the matrix U is a unit
upper triangular matrix.

2.7
Elementary Matrices and the LU Factorization 187
(j) A 4 × 4 matrix A that has an LU factorization has 10
multipliers.
Problems
1. Write all 3×3 elementary matrices and their inverses.
For Problems 2–6, determine elementary matrices that re-
duce the given matrix to row-echelon form.
2.
⎡
⎣
−4
−1
0
3
−3
7
⎤
⎦.
3.
' 3
5
1
−2
(
.
4.
' 5
8
2
1
3
−1
(
.
5.
⎡
⎣
3
−1
4
2
1
3
1
3
2
⎤
⎦.
6.
⎡
⎣
1
2
3
4
2
3
4
5
3
4
5
6
⎤
⎦.
For Problems 7–13, express the matrix A as a product of
elementary matrices.
7. A =
' 1
2
1
3
(
.
8. A =
' −2
−3
5
7
(
.
9. A =
'
3
−4
−1
2
(
.
10. A =
' 4
−5
1
4
(
.
11. A =
⎡
⎣
1
−1
0
2
2
2
3
1
3
⎤
⎦.
12. A =
⎡
⎣
0
−4
−2
1
−1
3
−2
2
2
⎤
⎦.
13. A =
⎡
⎣
1
2
3
0
8
0
3
4
5
⎤
⎦.
14. Determine elementary matrices E1, E2, . . . , Ek that
reduce
A =
' 2
−1
1
3
(
to reduced row-echelon form. Verify by direct multi-
plication that E1E2 . . . Ek A = I2.
15. Determine a Type 3 lower triangular elementary ma-
trix E1 that reduces A =
'
3
−2
−1
5
(
to upper trian-
gular form. Use Equation (2.7.3) to determine L and
verify Equation (2.7.2).
For Problems 16–21, determine the LU factorization of
the given matrix. Verify your answer by computing the
product LU.
16. A =
' 2
3
5
1
(
.
17. A =
' 3
1
5
2
(
.
18. A =
⎡
⎣
3
−1
2
6
−1
1
−3
5
2
⎤
⎦.
19. A =
⎡
⎣
5
2
1
−10
−2
3
15
2
−3
⎤
⎦.
20. A =
⎡
⎢⎢⎣
1
−1
2
3
2
0
3
−4
3
−1
7
8
1
3
4
5
⎤
⎥⎥⎦.
21. A =
⎡
⎢⎢⎣
2
−3
1
2
4
−1
1
1
−8
2
2
−5
6
1
5
2
⎤
⎥⎥⎦.
For Problems 22–25, use the LU factorization of A to solve
the system Ax = b.
22. A =
' 1
2
2
3
(
, b =
'
3
−1
(
.
23. A =
⎡
⎣
1
−3
5
3
2
2
2
5
2
⎤
⎦, b =
⎡
⎣
1
5
−1
⎤
⎦.
24. A =
⎡
⎣
2
2
1
6
3
−1
−4
2
2
⎤
⎦, b =
⎡
⎣
1
0
2
⎤
⎦.

188
CHAPTER 2
Matrices and Systems of Linear Equations
25. A =
⎡
⎢⎢⎣
4
3
0
0
8
1
2
0
0
5
3
6
0
0
−5
7
⎤
⎥⎥⎦, b =
⎡
⎢⎢⎣
2
3
0
5
⎤
⎥⎥⎦.
26. Use the LU factorization of A =
'
2
−1
−8
3
(
to
solve each of the systems Axi = bi if
b1 =
'
3
−1
(
,
b2 =
' 2
7
(
,
b3 =
'
5
−9
(
.
27. Use the LU factorization of
A =
⎡
⎣
−1
4
2
3
1
4
5
−7
1
⎤
⎦
to solve each of the systems Axi = ei and thereby
determine A−1.
28. If P = P1P2 . . . Pk, where each Pi is an elementary
permutation matrix, show that P−1 = PT .
29. Prove that
(a) the inverse of an invertible upper triangular ma-
trix is upper triangular. Repeat for an invertible
lower triangular matrix.
(b) the inverse of a unit upper triangular matrix is unit
upper triangular. Repeat for a unit lower triangu-
lar matrix.
30. In this problem, we prove that the LU decomposition
of an invertible n × n matrix is unique in the sense
that, if A = L1U1 and A = L2U2, where L1, L2 are
unit lower triangular matrices and U1,U2 are upper
triangular matrices, then L1 = L2 and U1 = U2.
(a) Apply Corollary 2.6.13 to conclude that L2 and
U1 are invertible, and then use the fact that
L1U1
=
L2U2 to establish that L−1
2 L1
=
U2U −1
1 .
(b) Use the result from (a) together with Theo-
rem 2.2.24 and Corollary 2.2.25 to prove that
L−1
2 L1 = In and U2U −1
1
= In, from which the
required result follows.
31. QR Factorization: It can be shown that any invertible
n × n matrix has a factorization of the form
A = QR,
where Q and R are invertible, R is upper triangular,
and Q satisﬁes QT Q = In (i.e., Q is orthogonal).
Determine an algorithm for solving the linear system
Ax = b using this QR factorization.
⋄For Problems 32–34, use some form of technology to de-
termine the LU factorization of the given matrix. Verify the
factorization by computing the product LU.
32. A =
⎡
⎣
3
5
−2
2
7
9
−5
5
11
⎤
⎦.
33. A =
⎡
⎣
27
−19
32
15
−16
9
23
−13
51
⎤
⎦.
34. A =
⎡
⎢⎢⎣
34
13
19
22
53
17
−71
20
21
37
63
59
81
93
−47
39
⎤
⎥⎥⎦.
2.8
The Invertible Matrix Theorem I
In Section 2.6, we deﬁned an n × n invertible matrix A to be a matrix such that there
exists an n × n matrix B satisfying AB = B A = In. There are, however, many other
important and useful viewpoints on invertibility of matrices. Some of these have already
been encountered in the preceding two sections, while others await us in later chapters of
the text. It is worthwhile to begin collecting a list of conditions on an n ×n matrix A that
are mathematically equivalent to its invertibility. We refer to this theorem as the Invertible
Matrix Theorem. As we have indicated, this result is somewhat a “work-in-progress,”
and we shall return to it later in Sections 3.2 and 4.10.

2.8
The Invertible Matrix Theorem I 189
Theorem 2.8.1
(Invertible Matrix Theorem)
Let A be an n × n matrix. The following conditions on A are equivalent:
(a) A is invertible.
(b) The equation Ax = b has a unique solution for every b in Rn.
(c) The equation Ax = 0 has only the trivial solution x = 0.
(d) rank(A) = n.
(e) A can be expressed as a product of elementary matrices.
(f) A is row-equivalent to In.
Proof The equivalence of (a), (b), and (d) has already been established in Section 2.6
in Theorems 2.6.5 and 2.6.6, as well as Corollary 2.6.7. Moreover, the equivalence of
(a) and (e) was already established in Theorem 2.7.5.
Next we establish that (c) is an equivalent statement by proving that (b) "⇒(c) "⇒
(d). Assuming that (b) holds, we can conclude that the linear system Ax = 0 has a unique
solution. However, one solution is evidently x = 0, and hence, this is the unique solution
to Ax = 0, which establishes (c). Next, assume that (c) holds. The fact that Ax = 0 has
only the trivial solution means that, in reducing A to row-echelon form, we ﬁnd no free
parameters.Thus,everycolumn(andhenceeveryrow)of A containsapivot,whichmeans
that the row-echelon form of A has n nonzero rows; that is, rank(A) = n, which is (d).
Finally, we prove that (e) "⇒(f) "⇒(a). If (e) holds, we can left multiply In by
a product of elementary matrices (corresponding to a sequence of elementary row op-
erations applied to In) to obtain A. This means that A is row-equivalent to In, which
is (f). Lastly, if A is row-equivalent to In, we can write A as a product of elementary
matrices, each of which is invertible. Since a product of invertible matrices is invertible
(by Corollary 2.6.11), we conclude that A is invertible, as needed.
Exercises for 2.8
Skills
• Know the list of characterizations of invertible matri-
ces given in the Invertible Matrix Theorem.
• Be able to use the Invertible Matrix Theorem to draw
conclusions related to the invertibility of a matrix.
True-False Review
For items (a)–(d), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If the linear system Ax = 0 has a nontrivial solution,
then A can be expressed as a product of elementary
matrices.
(b) A 4 × 4 matrix A with rank(A) = 4 is row-equivalent
to I4.
(c) If A is a 3×3 matrix with rank(A) = 2, then the linear
system Ax = b must have inﬁnitely many solutions.
(d) Any n × n upper triangular matrix is row-equivalent
to In.
Problems
1. Use part (c) of the Invertible Matrix Theorem to prove
that if A is an invertible matrix and B and C are ma-
trices of the same size as A such that AB = AC, then
B = C. [Hint: Consider AB −AC = 0.]
2. Give a direct proof of the fact that (d) "⇒(c) in the
Invertible Matrix Theorem.
3. Give a direct proof of the fact that (c) "⇒(b) in the
Invertible Matrix Theorem.
4. Use the equivalence of (a) and (e) in the Invertible Ma-
trix Theorem to prove that if A and B are invertible
n × n matrices, then so is AB.
5. Use the equivalence of (a) and (c) in the Invertible Ma-
trix Theorem to prove that if A and B are invertible
n × n matrices, then so is AB.

190
CHAPTER 2
Matrices and Systems of Linear Equations
2.9
Chapter Review
In this chapter we have investigated linear systems of equations. Matrices provide a
convenient mathematical representation for linear systems, and whether or not a lin-
ear system has a solution (and if so, how many) can be determined entirely from the
augmented matrix for the linear system.
An m × n matrix A = [ai j] is a rectangular array of numbers arranged in m rows
and n columns. The entry in the i-th row and j-th column is written ai j. More generally,
such an array whose entries are allowed to depend on an indeterminate t is known as
a matrix function. Matrix functions can be used to formulate systems of differential
equations.
If m = n, the matrix (or matrix function) is called a square matrix.
Concepts Related to Square Matrices
• Main diagonal: this consists of the entries a11, a22, . . . , ann in the matrix.
• Trace: the sum of the entries on the main diagonal.
• Upper triangular matrix: ai j = 0 for i > j.
• Lower triangular matrix: ai j = 0 for i < j.
• Diagonal matrix: ai j = 0 for i ̸= j.
• Transpose: this applies to any m × n matrix A and it is the n × m matrix AT
obtained from A by interchanging its rows and columns
• Symmetric matrix: AT = A; that is, ai j = a ji.
• Skew-symmetric matrix: AT = −A; that is, ai j = −a ji. In particular, aii = 0
for each i.
Matrix Algebra
Given two matrices A and B of the same size m × n, we can perform the following
operations:
• Addition/Subtraction A ± B: add/subtract the corresponding elements of A
and B.
• Scalar Multiplication r A: multiply each entry of A by the real (or complex)
scalar r.
If A is m × n and B is n × p, we can form their product AB, which is an m × p matrix
whose (i, j)-entry is computed by taking the dot product of the i-th row vector of A with
the j-th column vector of B. Note that, in general, AB ̸= B A.
Linear Systems
The general m × n system of linear equations is of the form
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
am1x1 + am2x2 + · · · + amnxn = bm.

2.9
Chapter Review 191
Ifeach bi = 0,thesystemiscalled homogeneous. Therearetwousefulwaystoformulate
the above linear system:
1. Augmented matrix:
A# =
⎡
⎢⎢⎢⎣
a11
a12
. . .
a1n
b1
a21
a22
. . .
a2n
b2
...
...
am1
am2
. . .
amn
bm
⎤
⎥⎥⎥⎦.
2. Vector form:
Ax = b,
where
A =
⎡
⎢⎢⎢⎣
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
am1
am2
. . .
amn
⎤
⎥⎥⎥⎦,
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦,
b =
⎡
⎢⎢⎢⎣
b1
b2
...
bm
⎤
⎥⎥⎥⎦.
Elementary Row Operations and Row-Echelon Form
There are three types of elementary row operations on a matrix A:
1. Pi j: Permute the ith and jth rows of A.
2. Mi(k): Multiply the entries in the ith row of A by the nonzero scalar k.
3. Ai j(k): Add to the elements of the jth row of A the scalar k times the corresponding
elements of the ith row of A.
By performing elementary row operations on the augmented matrix above, we can
determine solutions, if any, to the linear system. The strategy is to apply elementary row
operations in such a way that A is transformed into row-echelon form. This process is
known as Gaussian elimination. The resulting row-echelon form is solved by applying
back-substitution, with the use of free parameters if necessary, to the linear system
corresponding to the row-echelon form. The solutions, if any, to the original linear
system are the same. A leading one in the far right-hand column of the row-echelon form
indicates that the system has no solution.
A row-echelon form matrix is one in which
• all rows consisting entirely of zeros are placed at the bottom of the matrix.
• all other rows begin with a (leading) “1”, which occurs in a pivot position.
• the leading ones occur in columns strictly to the right of the leading ones in the
rows above.
Invertible Matrices
An n×n matrix A is invertible if there exists an n×n matrix B such that AB = In = B A,
where In is the n × n identity matrix (ones on the main diagonal, zeros elsewhere). We
write A−1 for the (unique) inverse B of A. One procedure for determining A−1, if it
exists, is the Gauss-Jordan Technique:
[A | In] ∼ERO
. . . ∼[In | A−1].

192
CHAPTER 2
Matrices and Systems of Linear Equations
Invertible matrices A share all of the following equivalent properties:
• A can be reduced to In via a sequence of elementary row operations.
• the linear system Ax = b has a unique solution x.
• the linear system Ax = 0 has only the trivial solution x = 0.
• A can be expressed as a product of elementary matrices that are obtained from
the identity matrix by applying exactly one elementary row operation.
Additional Problems
Let A =
' −2
4
2
6
−1
−1
5
0
(
, B =
⎡
⎢⎢⎣
−3
0
2
2
1
−3
0
1
⎤
⎥⎥⎦, C =
⎡
⎢⎢⎣
−5
−6
3
1
⎤
⎥⎥⎦. For Problems 1–9, compute the given expression,
if possible.
1. AT −5B.
2. CT B.
3. A2.
4. −4A −BT .
5. AB and tr(AB).
6. (AC)(AC)T .
7. (−4B)A.
8. (AB)−1.
9. CT C and tr(CT C).
10. Let A =
'1
2
3
2
5
7
(
and
B =
⎡
⎣
3
b
−4
a
a
b
⎤
⎦.
(a) Compute AB and determine the values of a and
b such that AB = I2.
(b) Using the values of a and b obtained in (a), com-
pute BA.
11. Let A be an m ×n matrix and let B be an p×n matrix.
Use the index form of the matrix product to prove that
(ABT )T = B AT .
12. Let A be an n × n matrix.
(a) Use the index form of the matrix product to write
the i jth element of A2.
(b) In the case when A is a symmetric matrix, show
that A2 is also symmetric.
13. Let A and B be n ×n matrices. If A is skew-symmetric
use properties of the transpose to establish that BT AB
also is skew-symmetric.
An n × n matrix A is called nilpotent if Ap = 0 for some
positive integer p. For Problems 14–15, show that the given
matrix is nilpotent.
14. A =
'
3
9
−1
−3
(
.
15. A =
⎡
⎣
0
1
1
0
0
1
0
0
0
⎤
⎦.
For Problems 16–19, let A(t) =
⎡
⎣
e−3t
−sec2 t
2t3
cos t
6 ln t
36 −5t
⎤
⎦and
B(t) =
⎡
⎢⎢⎣
−7
t2
6 −t
3t3 + 6t2
1 + t
cos(πt/2)
et
1 −t3
⎤
⎥⎥⎦. Compute the given expres-
sion, if possible.
16. A′(t).
17.
. 1
0 B(t) dt.
18. t3 · A(t) −sin t · B(t).
19. B′(t) −et A(t).

2.9
Chapter Review 193
For Problems 20–26, determine the solution set to the given
linear system of equations.
20.
x1 + 5x2 + 2x3 = −6,
4x2 −7x3 =
2,
5x3 =
0.
21.
5x1 −x2 + 2x3 =
7,
−2x1 + 6x2 + 9x3 =
0,
−7x1 + 5x2 −3x3 = −7.
22.
x + 2y −z = 1,
x
+ z = 5,
4x + 4y
= 12.
23.
x1 −2x2 −x3 + 3x4 = 0,
−2x1 + 4x2 + 5x3 −5x4 = 3,
3x1 −6x2 −6x3 + 8x4 = 2.
24.
3x1
−x3 + 2x4 −x5 =
1,
x1 + 3x2 + x3 −3x4 + 2x5 = −1,
4x1 −2x2 −3x3 + 6x4 −x5 =
5.
x4 + 4x5 = −2.
25.
x1 + x2 + x3 + x4 −3x5 = 6,
x1 + x2 + x3 + 2x4 −5x5 = 8,
2x1 + 3x2 + x3 + 4x4 −9x5 = 17,
2x1 + 2x2 + 2x3 + 3x4 −8x5 = 14.
26.
x1 −3x2 + 2ix3 =
1,
−2ix1 + 6x2 + 2x3 = −2.
For Problems 27–30, determine all values of k for which the
given linear system has (a) no solution, (b) a unique solution,
and (c) inﬁnitely many solutions.
27.
x1 −kx2 = 6,
2x1 + 3x2 = k.
28.
kx1 + 2x2 −x3 = 2,
kx2 + x3 = 2.
29.
10x1 + kx2 −x3 = 0,
kx1 + x2 −x3 = 0,
2x1 + x2 −x3 = 0.
30.
x1 −kx2 + k2x3 = 0,
x1
+
kx3 = 0,
x2 −
x3 = 1.
31. Do the three planes x1 + 2x2 + x3 = 4, x2 −x3 = 1,
and x1 + 3x2 = 0 have at least one common point of
intersection? Explain.
For Problems 32–37, (a) ﬁnd a row-echelon form of the given
matrix A, (b) determine rank(A), and (c) use the Gauss-
Jordan Technique to determine the inverse of A, if it exists.
32. A =
!
4
7
−2
5
"
.
33. A =
!
2
−7
−4
14
"
.
34. A =
⎡
⎣
3
−1
6
0
2
3
3
−5
0
⎤
⎦.
35. A =
⎡
⎢⎢⎣
2
1
0
0
1
2
0
0
0
0
3
4
0
0
4
3
⎤
⎥⎥⎦.
36. A =
⎡
⎣
3
0
0
0
2
−1
1
−1
2
⎤
⎦.
37. A =
⎡
⎣
−2
−3
1
1
4
2
0
5
3
⎤
⎦.
38. Let A =
⎡
⎣
1
−1
3
4
−3
13
1
1
4
⎤
⎦. Solve each of the systems
Axi = ei,
i = 1, 2, 3
where ei denote the column vectors of the identity
matrix I3.
39. Solve each of the systems Axi = bi if
A =
!2
5
7
−2
"
, b1 =
!1
2
"
, b2 =
!4
3
"
, b3 =
!−2
5
"
.
40. Let A and B be invertible matrices.
(a) By computing an appropriate matrix product, ver-
ify that (A−1B)−1 = B−1A.
(b) Use
properties
of
the
inverse
to
derive
(A−1B)−1 = B−1 A.

194
CHAPTER 2
Matrices and Systems of Linear Equations
41. Let S be an invertible n × n matrix, and let A and B
be n × n matrices such that B = S−1 AS.
(a) Show that B4 = S−1 A4S.
(b) Generalizing part (a), show that for any positive
integer k, we have Bk = S−1AkS.
For Problems 42–45, (a) express the given matrix as a
product of elementary matrices, and (b) determine the LU
decomposition of the matrix.
42. The matrix in Problem 32.
43. The matrix in Problem 35.
44. The matrix in Problem 36.
45. The matrix in Problem 37.
46. (a) Prove that if A and B are n × n matrices, then
(A + 2B)3 = A3 + 2A2B + 2AB A + 2B A2 + 4AB2
+ 4 B AB + 4B2 A + 8B3.
(b) How does the formula change for (A −2B)3?
47. How many terms are there in the expansion of
(A + B)k, in terms of k? Verify your answer explicitly
for k = 4.
48. Suppose that A and B are invertible matrices. Prove
that the block matrix
3 A
0
0
B−1
4
is invertible.
49. How many different positions can two leading ones of
a row-echelon form of a 2 × 4 matrix occur in? How
about three leading ones for a 3×4 matrix? How about
four leading ones for a 4 × 6 matrix? How about m
leading ones for an m × n matrix with m ≤n?
50. If the inverse of A2 is the matrix B, what is the inverse
of the matrix A10? Prove your answer.
51. If the inverse of A3 is the matrix B2, what is the inverse
of the matrix A9? Prove your answer.
Project: Circles and Spheres via Gaussian Elimination
Part 1: Circles In this part, we shall see that any three noncollinear points in the plane
can be found on a unique circle, and we will use Gaussian Elimination to ﬁnd the center
and radius of this circle.
(a) Show geometrically that three noncollinear points in the plane must lie on a unique
circle. [Hint: The radius must lie on the line that passes through the midpoint of
two of the three points and that is perpendicular to the segment connecting the two
points.]
(b) A circle in the plane has an equation that can be given in the form
(x −a)2 + (y −b)2 = r2,
where (a, b) is the center and r is the radius. By expanding the formula, we may
write the equation of the circle in the form
x2 + y2 + cx + dy = k,
for constants c, d, and k. Using this latter formula together with Gaussian
Elimination, determine c, d, and k for each set of points below. Then solve for
(a, b) and r to write the equation of the circle.
(i) (2, −1), (3, 3), (4, −1).
(ii) (−1, 0), (1, 2), (2, 2).

2.9
Chapter Review 195
Part 2: Spheres In this part, we shall extend the ideas of Part (1) and consider four
noncoplanar points in 3-space. Any three of these four points lie in a plane but are
noncollinear (why?). A sphere in 3-space has an equation that can be given in the form
(x −a)2 + (y −b)2 + (z −c)2 = r2,
where (a, b, c) is the center and r is the radius. By expanding the formula, we may write
the equation of the sphere in the form
x2 + y2 + z2 + ux + vy + wz = k,
for constants u, v, w, and k.
(a) Using the latter formula above together with Gaussian Elimination, determine
u, v, w, and k for each set of points below. Then solve for (a, b, c) and r to write
the equation of the sphere.
(i) (1, −1, 2), (2, −1, 4), (−1, −1, −1), (1, 4, 1).
(ii) (2, 0, 0), (0, 3, 0), (0, 0, 4), (0, 0, 6).
(b) What goes wrong with the procedure in (a) if the points lie on a single plane?
Choose four points of your own and carry out the procedure in part (a) to see what
happens? Can you describe circumstances under which the four coplanar points
will lie on a sphere?

3
Determinants
In this chapter, we introduce a basic tool in applied mathematics, namely the determinant
of a square matrix. The determinant is a number, associated with an n×n matrix A, whose
valuecharacterizeswhenthelinearsystem Ax = bhasauniquesolution(or,equivalently,
when A−1 exists). Determinants enjoy a wide range of applications, including coordinate
geometry and function theory.
Sections 3.1–3.3 give a detailed introduction to determinants, their properties, and
their applications. Alternatively, the summary Section 3.4 can be used to give a nonrig-
orous and much more abbreviated introduction to the fundamental results required in the
remainder of the text. We will see in later chapters that determinants are invaluable in
the theory of eigenvalues and eigenvectors of a matrix, as well as in solution techniques
for linear systems of differential equations.
3.1
The Definition of the Determinant
We will give a criterion shortly (Theorem 3.2.5) for the invertibility of a square matrix A
in terms of the determinant of A, written det(A), which is a number determined directly
from the elements of A. This criterion will provide a ﬁrst extension of the Invertible
Matrix Theorem introduced in Section 2.8.
To motivate the deﬁnition of the determinant of an n × n matrix A, we begin with
the special cases n = 1, n = 2, and n = 3.
Case 1: n = 1. According to Theorem 2.6.6, the 1 × 1 matrix A = [a11] is invertible
if and only if rank(A) = 1 if and only if the 1 × 1 determinant, det(A), deﬁned by
det(A) = a11
is nonzero.
196

3.1
The Definition of the Determinant 197
Case 2: n = 2. According to Theorem 2.6.6, the 2 × 2 matrix A =
! a11
a12
a21
a22
"
is
invertible if and only if rank(A) = 2, if and only if the row-echelon form of A has two
nonzero rows. Provided that a11 ̸= 0, we can reduce A to row-echelon form as follows:
! a11
a12
a21
a22
"
1∼
⎡
⎣
a11
a12
0
a22 −a12a21
a11
⎤
⎦.
1. A12
'
−a21
a11
(
For A to be invertible, it is necessary that a22 −a12a21
a11
̸= 0, or that a11a22 −a12a21 ̸= 0.
Thus, for A to be invertible, it is necessary that the 2 × 2 determinant, det(A), deﬁned
by
det(A) = a11a22 −a12a21
(3.1.1)
is nonzero. We will see in the next section that this condition is also sufﬁcient for the
2 × 2 matrix A to be invertible.
Case 3: n = 3. According to Theorem 2.6.6, the 3 × 3 matrix
A =
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦
is invertible if and only if rank(A) = 3, if and only if the row-echelon form of A has
three nonzero rows. Reducing A to row-echelon form as in Case 2, we ﬁnd that it is
necessary that the 3 × 3 determinant deﬁned by
det(A) = a11a22a33 + a12a23a31 + a13a21a32 −a11a23a32 −a12a21a33 −a13a22a31
(3.1.2)
is nonzero. Again, in the next section we will prove that this condition on det(A) is also
sufﬁcient for the 3 × 3 matrix A to be invertible.
To generalize the foregoing formulas for the determinant of an n × n matrix A,
we take a closer look at their structure. Each determinant above consists of a sum of
n! products, where each product term contains precisely one element from each row
and each column of A. Furthermore, each possible choice of one element from each
row and each column of A does in fact occur as a term of the summation. Finally,
each term is assigned a plus or a minus sign. Based on these observations, the appro-
priate way in which to deﬁne det(A) for an n × n matrix would seem to be to add
up all possible products consisting of one element from each row and each column
of A, with some condition on which products are taken with a plus sign and which
products are taken with a minus sign. To describe this condition, we digress to discuss
permutations.
Permutations
Consider the ﬁrst n positive integers 1, 2, 3, . . . , n. Any arrangement of these integers
in a speciﬁc order, say, (p1, p2, . . . , pn), is called a permutation.

198
CHAPTER 3
Determinants
Example 3.1.1
There are precisely six distinct permutations of the integers 1,2, and 3:
(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1).
□
More generally, we have the following result:
Theorem 3.1.2
There are precisely n! distinct permutations of the integers 1, 2, . . . , n.
The proof of this result is left as an exercise.
The elements in the permutation (1, 2, . . . , n) are said to be in their natural increas-
ing order. We now introduce a number that describes how far a given permutation is
from its natural order. For i ̸= j, the pair of elements pi and p j in the permutation
(p1, p2, . . . , pn) are said to be inverted if they are out of their natural order; that is,
if pi > p j with i < j. If this is the case, we say that (pi, p j) is an inversion. For
example, in the permutation (4, 2, 3, 1), the pairs (4, 2), (4, 3), (4, 1), (2, 1), and (3, 1)
are all out of their natural order, and so, there are a total of ﬁve inversions in this permu-
tation. In general we let N(p1, p2, . . . , pn) denote the total number of inversions in the
permutation (p1, p2, . . . , pn).
Example 3.1.3
Find the number of inversions in the permutations (1, 3, 2, 4, 5) and (2, 4, 5, 3, 1).
Solution:
The only pair of elements in the permutation (1, 3, 2, 4, 5) that is out of
natural order is (3, 2), so that N(1, 3, 2, 4, 5) = 1.
The permutation (2, 4, 5, 3, 1) has the following pairs of elements out of natural
order: (2, 1), (4, 3), (4, 1), (5, 3), (5, 1), and (3, 1). Thus, N(2, 4, 5, 3, 1) = 6.
□
It can be shown that the number of inversions gives the minimum number of adjacent
interchanges of elements in the permutation that are required to restore the permutation to
itsnaturalincreasingorder.Thisjustiﬁestheclaimthatthenumberofinversionsdescribes
how far from natural order a given permutation is. For example, N(3, 2, 1) = 3 and the
permutation (3, 2, 1) can be restored to its natural order by the following sequence of
adjacent interchanges:
(3, 2, 1) →(3, 1, 2) →(1, 3, 2) →(1, 2, 3).
The number of inversions enables us to distinguish two different types of permuta-
tions as follows.
DEFINITION
3.1.4
1. If N(p1, p2, . . . , pn) is an even integer (or zero), we say (p1, p2, . . . , pn)
is an even permutation. We also say that (p1, p2, . . . , pn) has even parity.
2. If N(p1, p2, . . . , pn) is an odd integer, we say (p1, p2, . . . , pn) is an
odd permutation. We also say that (p1, p2, . . . , pn) has odd parity.
Example 3.1.5
The permutation (4, 1, 3, 2) has even parity, since we have N(4, 1, 3, 2) = 4, whereas
(3, 2, 1, 4) is an odd permutation since N(3, 2, 1, 4) = 3.
□
We associate a plus or a minus sign with a permutation, depending on whether it has
even or odd parity, respectively. The sign associated with the permutation (p1, p2, . . . , pn)
can be speciﬁed by the indicator σ(p1, p2, . . . , pn) deﬁned in terms of the number of

3.1
The Definition of the Determinant 199
inversions as follows:
σ(p1, p2, . . . , pn) =
) +1 if (p1, p2, . . . , pn) has even parity,
−1 if (p1, p2, . . . , pn) has odd parity.
Hence,
σ(p1, p2, . . . , pn) = (−1)N(p1,p2,...,pn).
Example 3.1.6
It follows from Example 3.1.3 that
σ(1, 3, 2, 4, 5) = (−1)1 = −1,
whereas
σ(2, 4, 5, 3, 1) = (−1)6 = 1.
□
The proofs of some of our later results will depend upon the next theorem.
Theorem 3.1.7
If any two elements in a permutation are interchanged, then the parity of the resulting
permutation is opposite to that of the original permutation.
Proof We ﬁrst show that interchanging two adjacent terms in a permutation changes
its parity. Consider an arbitrary permutation (p1, . . . , pk, pk+1, . . . , pn), and suppose
we interchange the adjacent elements pk and pk+1. Then
• If pk > pk+1, then
N(p1, p2, . . . , pk+1, pk, . . . , pn) = N(p1, p2, . . . , pk, pk+1, . . . , pn) −1,
• If pk < pk+1, then
N(p1, p2, . . . , pk+1, pk, . . . , pn) = N(p1, p2, . . . , pk, pk+1, . . . , pn) + 1,
so that the parity is changed in both cases.
Now suppose we interchange the elements pi and pk in the permutation (p1, p2, . . . ,
pi, . . . , pk, . . . , pn). Note that k −i > 0. We can accomplish this by successively
interchanging adjacent elements. In moving pk to the i-th position, we perform k −i
interchanges involving adjacent terms, and the resulting permutation is
(p1, p2, . . . , pk, pi, . . . , pk−1, pk+1, . . . , pn).
Next we move pi to the k-th position. A moment’s thought shows that this requires
(k −i) −1 interchanges of adjacent terms. Thus, the total number of adjacent interchanges
involved in interchanging the elements pi and pk is 2(k −i) −1, which is always an odd
integer. Since each adjacent interchange changes the parity, the permutation resulting from
an odd number of adjacent interchanges has opposite parity to the original permutation.
At this point, we are ready to see how permutations can facilitate the deﬁnition of the
determinant. From the expression (3.1.2) for the 3 × 3 determinant, we see that the row
indices of the factors in each term have been arranged in their natural increasing order
and that the column indices are each a permutation (p1, p2, p3) of 1,2,3. Further, the sign
attached to each term coincides with the sign of the permutation of the corresponding

200
CHAPTER 3
Determinants
column indices; that is, σ(p1, p2, p3). These observations motivate the following general
deﬁnition of the determinant of an n × n matrix:
DEFINITION
3.1.8
Let A = [ai j] be an n × n matrix. The determinant of A, denoted det(A), is deﬁned
as follows:
det(A) =
*
σ(p1, p2, · · · , pn)a1p1a2p2a3p3 · · · anpn,
(3.1.3)
where the summation is over the n! distinct permutations (p1, p2, . . . , pn) of the
integers 1, 2, 3, . . . , n. The determinant of an n × n matrix is said to have order n.
We sometimes denote det(A) by
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
an1
an2
. . .
ann
.
Thus, for example, from (3.1.1), we have
a11
a12
a21
a22 = a11a22 −a12a21.
Example 3.1.9
Use Deﬁnition 3.1.8 to derive the expression for the determinant of order 3.
Solution:
When n = 3, (3.1.3) reduces to
det(A) =
*
σ(p1, p2, p3)a1p1a2p2a3p3,
where the summation is over the 3! = 6 permutations of 1, 2, 3. It follows that the six
terms in this summation are
a11a22a33,
a11a23a32,
a12a21a33,
a12a23a31,
a13a21a32,
a13a22a31,
so that
det(A) = σ(1, 2, 3)a11a22a33 + σ(1, 3, 2)a11a23a32 + σ(2, 1, 3)a12a21a33
+ σ(2, 3, 1)a12a23a31 + σ(3, 1, 2)a13a21a32 + σ(3, 2, 1)a13a22a31.
To obtain the values of each σ(p1, p2, p3), we determine the parity for each permutation
(p1, p2, p3). We ﬁnd that
σ(1, 2, 3) = +1,
σ(1, 3, 2) = −1,
σ(2, 1, 3) = −1,
σ(2, 3, 1) = +1,
σ(3, 1, 2) = +1,
σ(3, 2, 1) = −1.
Hence,
det(A) =
a11
a12
a13
a21
a22
a23
a31
a32
a33
= a11a22a33 + a12a23a31 + a13a21a32 −a11a23a32 −a12a21a33 −a13a22a31.
□
a11
a12
a13
a11
a12
a21
a22
a23
a21
a22
a31
a32
a33
a31
a32
2
2
2
1
1
1
Figure 3.1.1: A schematic for
obtaining the determinant of a
3 × 3 matrix A = [ai j].
A simple schematic for obtaining the terms in the determinant of order 3 is given in
Figure 3.1.1. By taking the product of the elements joined by each arrow and attaching

3.1
The Definition of the Determinant 201
the indicated sign to the result, we obtain the six terms in the determinant of the 3 × 3
matrix A = [ai j]. Note that this technique for obtaining the terms in a determinant does
not generalize to determinants of n × n matrices with n > 3.
Example 3.1.10
Evaluate
(a) | −6 |.
(b)
−4 6
−2 5 .
(c)
2 −5 2
6
1 0
−3 −1 4
.
(d)
0 0 −2 3
0 0
1 6
−2 1
0 0
−7 3
0 0
.
Solution:
(a) |−6| = −6. In the case of a 1 × 1 matrix, the reader is cautioned not to confuse
the vertical bars notation for the determinant with absolute value bars.
(b)
−4 6
−2 5 = (−4)(5) −(6)(−2) = −8.
(c) In this case, the schematic in Figure 3.1.1 is
2 −5 2
2 −5
6
1 0
6
1
−3 −1 4 −3 −1
so that
2 −5 2
6
1 0
−3 −1 4
= (2)(1)(4) + (−5)(0)(−3) + (2)(6)(−1)
−(−3)(1)(2) −(−1)(0)(2) −(4)(6)(−5)
= 8 + 0 + (−12) −(−6) −0 −(−120)
= 122.
(d) To evaluate this 4×4 determinant, we must use Deﬁnition 3.1.8. There are 4! = 24
terms in the summation; however, in this case many of these terms are zero
due to the large number of zeros in the given matrix. The only nonzero terms
a1p1a2p2a3p3a4p4 occur when 3 ≤p1 ≤4, 3 ≤p2 ≤4, 1 ≤p3 ≤2, and
1 ≤p4 ≤2. The following table summarizes the permutations (p1, p2, p3, p4)
whose corresponding term in the computation of the determinant via Deﬁnition
3.1.8 is nonzero, together with the number of inversions and the sign of those
permutations.
Permutation
Number of Inversions
Sign
(p1, p2, p3, p4)
N(p1, p2, p3, p4)
σ(p1, p2, p3, p4)
(3, 4, 1, 2)
4
+
(3, 4, 2, 1)
5
−
(4, 3, 1, 2)
5
−
(4, 3, 2, 1)
6
+

202
CHAPTER 3
Determinants
Therefore,
0 0 −2 3
0 0
1 6
−2 1
0 0
−7 3
0 0
= a13a24a31a42 −a13a24a32a41 −a14a23a31a42 + a14a23a32a41
= (−2)(6)(−2)(3) −(−2)(6)(1)(−7) −(3)(1)(−2)(3)
+ (3)(1)(1)(−7)
= 72 −84 −(−18) + (−21)
= −15.
□
Example 3.1.11
Find all x satisfying
(a)
x2
x
1
1
1
1
4
2
1
= 0.
(b)
2 −4x
−4
2
5 + 3x
3 −3
1 −2x
−2
1
= 0.
Solution:
(a) Using the schematic in Figure 3.1.1 to evaluate this determinant, we have
x2
x
1
1
1
1
4
2
1
= x2 + 4x + 2 −4 −2x2 −x = −x2 + 3x −2 = −(x −2)(x −1).
Consequently, the two values of x satisfying the given equation are x = 2 and
x = 1.
(b) Proceeding as in part (a), we have
2 −4x
−4
2
5 + 3x
3 −3
1 −2x
−2
1
= 3(2 −4x) + 12(1 −2x) −4(5 + 3x) −6(1 −2x)
−6(2 −4x) −(−4)(5 + 3x) = 0.
Therefore, regardless of the value of x, the required equation is satisﬁed. Hence,
every real number x satisﬁes the equation.
□
We now turn to some geometric applications of the determinant.
Geometric Interpretation of the Determinants
of Orders Two and Three
If a and b are two vectors in space, we recall that their dot product is the scalar
a · b = ||a|| ||b|| cos θ,
(3.1.4)
where θ is the angle between a and b, and ||a|| and ||b|| denote the lengths of a and b,
respectively. On the other hand, the cross product of a and b is the vector
a × b = ||a|| ||b|| sin θ n,
(3.1.5)

3.1
The Definition of the Determinant 203
where n denotes a unit vector1 that is perpendicular to the plane of a and b and chosen in
such a way that {a, b, n} is a right-handed set of vectors. If i, j, k denote the unit vectors
pointing along the positive x-, y-, and z-axes, respectively, of a rectangular Cartesian
coordinate system and a = a1i + a2j + a3k, b = b1i + b2j + b3k, then Equation (3.1.5)
can be expressed in component form as
a × b = (a2b3 −a3b2)i + (a3b1 −a1b3)j + (a1b2 −a2b1)k.
(3.1.6)
This can be remembered most easily in the compact form
a × b =
i
j
k
a1
a2
a3
b1
b2
b3
,
which is compatible with the schematic in Figure 3.1.1. We will use the equations above
to establish the following theorem.
Theorem 3.1.12
1. The area of a parallelogram with sides determined by the vectors a = a1i + a2j
and b = b1i + b2j is
Area = |det(A)|,
where A =
! a1
a2
b1
b2
"
.
2. The volume of a parallelepiped determined by the vectors a = a1i + a2j + a3k,
b = b1i + b2j + b3k, c = c1i + c2j + c3k is
Volume = |det(A)|,
where A =
⎡
⎣
a1
a2
a3
b1
b2
b3
c1
c2
c3
⎤
⎦.
Before presenting the proof of this theorem, we make some remarks and give two
examples.
Remarks
1. The vertical bars appearing in the formulas in Theorem 3.1.12 denote the absolute
value of the number det(A).
2. It follows from results in the next section that it does not matter in what order the
given vectors are placed in the rows of the matrix A appearing in Theorem 3.1.12.
Moreover, it is also permissible to arrange the given vectors into the columns of A
instead of the rows of A.
3. We see from the expression for the volume of a parallelepiped that the condition
for three vectors to lie in the same plane (i.e., the parallelepiped has zero volume)
is that the matrix A whose rows (or columns) are comprised of the three vectors
has det(A) = 0. This will be a useful result in the next chapter.
1A unit vector is a vector of length 1.

204
CHAPTER 3
Determinants
Example 3.1.13
Find the area of the parallelogram containing the points (−2, 1), (1, 5), (3, 2), and (6, 6).
Solution:
A sketch locating these four points in the plane reveals that the parallelo-
gram is determined by the adjacent sides that meet at, say, (−2, 1). These two vectors
are a = 3i + 4j and b = 5i + j. According to part 1 of Theorem 3.1.12, the area of the
parallelogram is
++++det
! 3 4
5 1
"++++ = |(3)(1) −(4)(5)| = | −17| = 17.
□
Example 3.1.14
Determine whether or not the vectors a = −4i + j and b = 2i + 3j + k and c =
−8i + 9j + 2k lie in a single plane in 3-space.
Solution:
By Remark 3 above, it sufﬁces to determine whether or not the volume of
the parallelepiped determined by the three vectors is zero or not. To do this, we use part
(2) of Theorem 3.1.12:
Volume =
++++++
det
⎡
⎣
−4 1 0
2 3 1
−8 9 2
⎤
⎦
++++++
= |(−4)(3)(2) + (1)(1)(−8) + (0)(2)(9)
−(−8)(3)(0) −(9)(1)(−4) −(2)(2)(1)|
= 0,
which shows that the three vectors do lie in a single plane.
□
Now we turn to the
Proof of Theorem 3.1.12:
1. The area of the parallelogram is
Area = (length of base) × (perpendicular height).
From Figure 3.1.2, this can be written as
Area = ||a||h = ||a|| ||b|| | sin θ| = ||a × b||.
(3.1.7)
y
x
b
h
a
u
Figure 3.1.2: Determining the area of a parallelogram.
Since the k components of a and b are both zero (since the vectors lie in the
xy-plane), substitution from Equation (3.1.6) yields
Area = ||(a1b2 −a2b1)k|| = |a1b2 −a2b1| = |det(A)|.

3.1
The Definition of the Determinant 205
2. The volume of the parallelepiped is
Volume = (area of base) × (perpendicular height).
The base is determined by the vectors b and c (see Figure 3.1.3), and its area can
be written as ||b × c||, in similar fashion to that done in (3.1.7). From Figure 3.1.3
and Equation (3.1.4), we therefore have
Volume = ||b × c||h = ||b × c|| ||a||| cos ψ| = ||b × c|| |a · n|,
where n is a unit vector that is perpendicular to the plane containing b and c. We
can now use Equations (3.1.5) and (3.1.6) to obtain
Volume = ||b × c|| ||a|| | cos ψ| = |a · (b × c)|
=
++++(a1i+a2j+a3k)·[(b2c3 −b3c2)i + (b3c1−b1c3)j + (b1c2−b2c1)k]
++++
= |a1(b2c3 −b3c2) + a2(b3c1 −b1c3) + a3(b1c2 −b2c1)|
= |det(A)|,
as required.
z
y
x
b
c
h
a
u
c
Figure 3.1.3: Determining the volume of a parallelepiped.
Exercises for 3.1
Key Terms
Permutation, Inversion, Parity, Determinant, Order, Dot
product, Cross product.
Skills
• Be able to list permutations of 1, 2, . . . , n.
• Be able to compute determinants by using Deﬁni-
tion 3.1.8.
• Be able to ﬁnd the number of inversions of a given
permutation and thus determine its parity.
• Be able to compute the area of a parallelogram with
sides determined by vectors in R2.
• Be able to compute the volume of a parallelogram with
sides determined by vectors in R3.
• Be able to use the determinant to determine whether
or not a set of three vectors in 3-space lie in a single
plane.
True-False Review
For items (a)–(j), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If A is a 2 × 2 lower triangular matrix, then det(A) is
the product of the elements on the main diagonal of A.

206
CHAPTER 3
Determinants
(b) If A is a 3 × 3 upper triangular matrix, then det(A)
is the product of the elements on the main diagonal
of A.
(c) The volume of the parallelepiped whose sides are de-
termined by the vectors a, b, and c is given by det(A),
where A = [a, b, c].
(d) There are the same number of permutations of
{1, 2, 3, 4} of even parity as there are of odd parity.
(e) If A and B are 2 × 2 matrices, then det(A + B) =
det(A)+ det(B).
(f) The determinant of a matrix whose elements are all
positive must be positive.
(g) A matrix containing a row of zeros must have zero
determinant.
(h) Three vectors v1, v2, and v3 in R3 are coplanar if and
only if the determinant of the 3×3 matrix [v1, v2, v3]
is zero.
(i)
++++++++
a1
a2
0
0
a3
a4
0
0
0
0
b1
b2
0
0
b3
b4
++++++++
=
++++
a1
a2
a3
a4
++++
++++
b1
b2
b3
b4
++++ .
(j)
++++++++
0
0
a1
a2
0
0
a3
a4
b1
b2
0
0
b3
b4
0
0
++++++++
=
++++
a1
a2
a3
a4
++++
++++
b1
b2
b3
b4
++++ .
Problems
For Problems 1–6, determine the number of inversions and
the parity of the given permutation.
1. (3, 1, 4, 2).
2. (2, 4, 3, 1).
3. (5, 4, 3, 2, 1).
4. (2, 4, 1, 5, 3).
5. (6, 1, 4, 2, 5, 3).
6. (6, 5, 4, 3, 2, 1).
7. Use Deﬁnition 3.1.8 to derive the general expression
for the determinant of A if
A =
! a11
a12
a21
a22
"
.
For Problems 8–11, determine whether the given expression
is a term in the determinant of order 5. If it is, determine
whether the permutation of the column indices has even or
odd parity and hence ﬁnd whether the term has a plus or a
minus sign attached to it.
8. a11a23a34a43a52.
9. a11a25a33a42a54.
10. a11a32a24a43a55.
11. a13a25a31a44a42.
For Problems 12–15, determine the values of the indices p
and q such that the following are terms in a determinant of
order 4. In each case, determine the number of inversions
in the permutation of the column indices and hence ﬁnd the
appropriate sign that should be attached to each term.
12. a21a3qap2a43.
13. a13ap4a32a2q.
14. apqa34a13a42.
15. a3qap4a13a42.
For Problems 16–42, evaluate the determinant of the given
matrix.
16. A =
! 0 −2
5
1
"
.
17. A =
!
6 −3
−5 −1
"
.
18. A =
! −4 7
1 7
"
.
19. A =
! 2 −3
1
5
"
.
20. A =
!
9 −8
−7 −3
"
.
21. A =
!
2 −4
−1
0
"
.
22. A =
!
1 4
−4 3
"
.
23. A =
! e−3
3e10
2e−5
6e8
"
.
24. A =
! π
π2
√
2 2π
"
.

3.1
The Definition of the Determinant 207
25. A =
⎡
⎣
6 −1 2
−4
7 1
0
3 1
⎤
⎦.
26. A =
⎡
⎣
5 −3
0
1
4 −1
−8
2 −2
⎤
⎦.
27. A =
⎡
⎣
−2 −4 1
6
1 1
−2 −1 3
⎤
⎦.
28. A =
⎡
⎣
0 0 −3
0 4
3
−2 1
5
⎤
⎦.
29. A =
⎡
⎣
9 1 −7
6 2
1
−4 0 −2
⎤
⎦.
30. A =
⎡
⎣
2 −10
3
1
1
1
0
8 −3
⎤
⎦.
31. A =
⎡
⎣
5
4
3
−2
9 12
1 −1
0
⎤
⎦.
32. A =
⎡
⎣
5 0 4
0 3 0
2 0 1
⎤
⎦.
33. A =
⎡
⎣
√π
e2
e−1
√
67 1/30 2001
π
π2
π3
⎤
⎦.
34. A =
⎡
⎣
2 3 −1
1 4
1
3 1
6
⎤
⎦.
35. A =
⎡
⎢⎢⎣
0 0
0 −3
0 0 −7 −1
0 2
6
9
1 8 −8 −9
⎤
⎥⎥⎦.
36. A =
⎡
⎢⎢⎣
4
1
8
6
0 −2
13
5
0
0 −6
1
0
0
0 −3
⎤
⎥⎥⎦.
37. A =
⎡
⎢⎢⎣
−2 0
1
6
−1 3 −1 −4
2 1
0
3
0 5 −4 −2
⎤
⎥⎥⎦.
38. A =
⎡
⎢⎢⎣
−2 −1 4 −6
0
1 0
2
0 −6 3
2
0
8 5
1
⎤
⎥⎥⎦.
39. A =
⎡
⎢⎢⎣
−1
2
0
0
2 −8
0
0
0
0
2
3
0
0 −1 −1
⎤
⎥⎥⎦.
40. A =
⎡
⎢⎢⎢⎢⎣
1
2 3
0
0
2 −1 4
0
0
6
1 1
0
0
0
0 0
4
3
0
0 0 −1 −2
⎤
⎥⎥⎥⎥⎦
.
41. A =
⎡
⎢⎢⎢⎢⎣
1 2 0 0 0
3 4 0 0 0
0 0 5 0 0
0 0 0 6 7
0 0 0 8 9
⎤
⎥⎥⎥⎥⎦
.
42. A =
⎡
⎢⎢⎢⎢⎣
0
0 0
8 4
0
0 0 −1 1
0
0 2
0 0
2 −3 0
0 0
4 −2 0
0 0
⎤
⎥⎥⎥⎥⎦
.
For Problems 43–46, evaluate the determinant of the given
matrix function.
43. A(t) =
! e6t
e4t
6e6t
4e4t
"
.
44. A(t) =
⎡
⎣
sin t
cos t
1
cos t
−sin t
0
sin t
−cos t
0
⎤
⎦.
45. A(t) =
⎡
⎣
e2t
e3t
e−4t
2e2t
3e3t
−4e−4t
4e2t
9e3t
16e−4t
⎤
⎦.
46. A(t) =
⎡
⎣
e−t
e−5t
e2t
−e−t
−5e−5t
2e2t
e−t
25e−5t
4e2t
⎤
⎦.
In Problems 47–48, we explore a relationship between de-
terminants and solutions to a differential equation. The 3×3
matrix consisting of solutions to a differential equation and
their derivatives is called the Wronskian and, as we will
see in later chapters, plays a pivotal role in the theory of
differential equations.

208
CHAPTER 3
Determinants
47. Verify that y1(x) = cos 2x, y2(x) = sin 2x, and
y3(x) = ex are solutions to the differential equation
y′′′ −y′′ + 4y′ −4y = 0,
and show that
y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3
is nonzero on any
interval.
48. Verify that y1(x) = ex, y2(x) = cosh x, and y3(x) =
sinh x are solutions to the differential equation
y′′′ −y′′ −y′ + y = 0,
and show that
y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3
is identically zero.
49. (a) Write all 24 distinct permutations of the integers
1, 2, 3, 4.
(b) Determine the parity of each permutation in
part (a).
(c) Use parts (a) and (b) to derive the expression for
a determinant of order 4.
50. Use Problem 49 to evaluate det(A), where
A =
⎡
⎢⎢⎣
2
0 −1
5
0
6
1
2
1 −1 −2
3
0
2
0 −4
⎤
⎥⎥⎦.
51. Use Problem 49 to evaluate det(A), where
A =
⎡
⎢⎢⎣
1 4 −7
0
3 0
1 −1
−2 1
3 −3
0 2
2
4
⎤
⎥⎥⎦.
52. Use Problems 49 and 50 to evaluate det(B), where
B =
⎡
⎢⎢⎢⎢⎣
2 1
5
5
0
0 2
0 −1
5
0 0
6
1
2
0 1 −1 −2
3
0 0
2
0 −4
⎤
⎥⎥⎥⎥⎦
.
53. Use Problems 49 and 51 to evaluate det(B), where
B =
⎡
⎢⎢⎢⎢⎣
1 4 −7
0
0
3 0
1 −1
0
−2 1
3 −3
0
0 2
2
4
0
0 0
0
0 −3
⎤
⎥⎥⎥⎥⎦
.
54. (a) If A =
! a11
a12
a21
a22
"
and c is a constant, verify
that det(cA) = c2det(A).
(b) Use the deﬁnition of a determinant to prove that
if A is an n × n matrix and c is a constant, then
det(cA) = cndet(A).
55. The alternating symbol ϵi jk is deﬁned by
ϵi jk =
⎧
⎨
⎩
1, if (i jk) is an even permutation of 1, 2, 3,
−1, if (i jk) is an odd permutation of 1, 2, 3,
0, otherwise.
(a) Write all nonzero ϵi jk, for 1 ≤i ≤3, 1 ≤j ≤3,
1 ≤k ≤3.
(b) If A = [ai j] is a 3 × 3 matrix, verify that
det(A) =
3
*
i=1
3
*
j=1
3
*
k=1
ϵi jka1ia2 ja3k.
56. If A is the general n × n matrix, determine the sign
attached to the term
a1na2 n−1a3 n−2 · · · an1,
which arises in det(A).
57. ⋄Use some form of technology to evaluate the deter-
minants in Problems 40–46.
58. ⋄Let A be an arbitrary 4×4 matrix. By experimenting
with various elementary row operations, conjecture
how elementary row operations applied to A affect
the value of det(A).
59. ⋄Verify that y1(x) = e−2x cos 3x, y2(x) = e−2x
sin 3x, and y3(x) = e−4x are solutions to the differ-
ential equation
y′′′ + 8y′′ + 29y′ + 52y = 0,
and show that
y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3
is nonzero on any
interval.
60. ⋄Verify that y1(x) = ex cos 2x, y2 = ex sin 2x, and
y3 = e3x are solutions to the differential equation
y′′′ −5y′′ + 11y′ −15y = 0
and show that
++++++
y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3
++++++
is nonzero on any
interval.

3.2
Properties of Determinants 209
3.2
Properties of Determinants
Forlargevaluesofn,evaluatingadeterminantofordernusingthedefinitiongivenintheprevious
section is not very practical since the number of terms isn!(for example, a determinant of order
10 contains 3,628,800 terms). In the next two sections, we develop alternative techniques for
evaluating determinants. The following theorem suggests one way to proceed.
Theorem 3.2.1
If A is an n × n upper or lower triangular matrix, then
det(A) = a11a22a33 · · · ann =
n
1
i=1
aii.
Proof We use the deﬁnition of the determinant to prove the result in the upper triangular
case. From Equation (3.1.3),
det(A) =
*
σ(p1, p2, . . . , pn)a1p1a2p2a3p3 · · · anpn.
(3.2.1)
If A is upper triangular, then ai j = 0 whenever i > j, and therefore, the only nonzero
terms in the preceding summation are those with pi ≥i for all i. Since all the pi must
be distinct, the only possibility is (by applying pi ≥i to i = n, n −1, . . . , 2, 1 in turn)
pi = i,
i = 1, 2, . . . , n,
and so Equation (3.2.1) reduces to the single term
det(A) = σ(1, 2, . . . , n)a11a22 · · · ann.
Since σ(1, 2, . . . , n) = 1, it follows that
det(A) = a11a22 · · · ann.
The proof in the lower triangular case is left as an exercise (Problem 52).
Example 3.2.2
According to the previous theorem, if A =
⎡
⎢⎢⎣
−6 4
9 −2
0 2
3
8
0 0 −5
6
0 0
0 −3
⎤
⎥⎥⎦, then
det(A) = (−6)(2)(−5)(−3) = −180.
□
Theorem 3.2.1 shows that it is easy to compute the determinant of an upper or lower
triangular matrix. Recall from Chapter 2 that any matrix can be reduced to row-echelon form
by a sequence of elementary row operations. In the case of an n × n matrix, any row-echelon
form will be upper triangular. Theorem 3.2.1 suggests, therefore, that we should consider how
elementary row operations performed on a matrix A alter the value of det(A).
Elementary Row Operations and Determinants
Let A be an n × n matrix.
P1. If B is the matrix obtained by permuting two rows of A, then
det(B) = −det(A).

210
CHAPTER 3
Determinants
P2. If B is the matrix obtained by multiplying one row of A by any2 scalar k, then
det(B) = k det(A).
P3. If B is the matrix obtained by adding a multiple of any row of A to a different row
of A, then
det(B) = det(A).
The proofs of these properties are given at the end of this section.
Example 3.2.3
Suppose that A =
⎡
⎣
a
b
c
d
e
f
g
h
i
⎤
⎦is some 3 × 3 matrix with det(A) = 7. Compute
det
⎡
⎣
4g
4h
4i
a + d
b + e
c + f
a −2g
b −2h
c −2i
⎤
⎦.
Solution:
From the form of the entries of the matrix in question, we need to ascertain
what elementary row operations have been applied to the matrix A in order to apply
P1–P3 correctly. For instance, the third row of A appears to have been permuted to the
ﬁrst row and multiplied by 4. The ﬁrst row of A, on the other hand, has been permuted
to the third row, and −2 times the third row of A was added to it. Finally, the ﬁrst row of
A was added to the second row of A. Therefore, the only two effects that change det(A)
are the multiplication of the last row of A by 4 and the permutation of the ﬁrst and third
rows. The determinant of A is therefore multiplied by 4 and −1, respectively. Hence,
det
⎡
⎣
4g
4h
4i
a + d
b + e
c + f
a −2g
b −2h
c −2i
⎤
⎦= 7(−1)(4) = −28.
□
Remark
The main use of P2 is that it enables us to factor a common multiple of the
entries of a particular row out of the determinant. For example, if
A =
! −3
6
4 −2
"
and
B =
! −3
6
12 −6
"
,
where B is obtained from A by multiplying the second row of A by 3, then we have
det(B) = 3 · det(A) = 3 [(−3)(−2) −(6)(4)] = 3(−18) = −54.
On the other hand, if we multiply the entire matrix A by a scalar multiple of 3, then
in effect, each row of A has been multiplied by 3, and we must apply P2 once for each
of the two rows. Therefore,
det(3A) = 32det(A) = 9(−18) = −162.
2This statement is even true if k = 0.

3.2
Properties of Determinants 211
In general, repeated application of property P2 to each row of an n × n matrix A leads
to the following:
P4. For any scalar k and n × n matrix A, we have
det(k A) = kndet(A).
We now illustrate how the foregoing properties P1–P3, together with Theorem 3.2.1,
can be used to evaluate a determinant. The basic idea is the same as that for Gaussian
elimination. We use elementary row operations to reduce the determinant to upper trian-
gular form and then use Theorem 3.2.1 to evaluate the resulting determinant. Care must
be exercised in this process, because one must remember with each step how the ele-
mentary row operations being applied affect the determinant of the subsequent matrices
in the process. Let us illustrate with an example.
Example 3.2.4
Evaluate
!!!!!!!!
3
4 −1
5
1
2 −1
3
−2 −2
2 −7
−4 −3 −2 −8
!!!!!!!!
.
Solution:
We have
!!!!!!!!
3
4 −1
5
1
2 −1
3
−2 −2
2 −7
−4 −3 −2 −8
!!!!!!!!
1= −
!!!!!!!!
1
2 −1
3
3
4 −1
5
−2 −2
2 −7
−4 −3 −2 −8
!!!!!!!!
2= −
!!!!!!!!
1
2 −1
3
0 −2
2 −4
0
2
0 −1
0
5 −6
4
!!!!!!!!
3= 2
!!!!!!!!
1 2 −1
3
0 1 −1
2
0 2
0 −1
0 5 −6
4
!!!!!!!!
4= 2
!!!!!!!!
1 2 −1
3
0 1 −1
2
0 0
2 −5
0 0 −1 −6
!!!!!!!!
5= −2
!!!!!!!!
1 2 −1
3
0 1 −1
2
0 0 −1 −6
0 0
2 −5
!!!!!!!!
6= −2
!!!!!!!!
1 2 −1
3
0 1 −1
2
0 0 −1
−6
0 0
0 −17
!!!!!!!!
= −2(1)(1)(−1)(−17) = −34,
where we have used Theorem 3.2.1 at the end.
1. P12
2. A12(−3), A13(2), A14(4)
3. M2(−1/2)
4. A23(−2), A24(−5)
5. P34
6. A34(2)
Warning: Notice in step 3 that we applied the elementary row operation M2(−1/2).
According to P2, the resulting matrix after this step has a determinant that is −1/2 the value
of the determinant of the matrix before this step. Therefore, to account for this difference,
one must place a factor of −2 (the reciprocal of −1/2) in front of the determinant that results
after applying P2. One way to think of this is that we have literally factored a multiple of −2
out of one row of the matrix, and with respect to the determinant, property P2 states that this
factor of −2 must be placed in front of the resulting determinant.
□
Theoretical Results for n × n Matrices and n × n Linear Systems
In Section 2.8, we established several conditions on an n × n matrix A that are equiv-
alent to saying that A is invertible. At this point, we are ready to give one additional
characterization of invertible matrices in terms of determinants.

212
CHAPTER 3
Determinants
Theorem 3.2.5
Let A be an n × n matrix. The following conditions on A are equivalent.
(a) A is invertible.
(g) det(A) ̸= 0.
Proof Let A∗denote the reduced row-echelon form of A, and note that A is invertible
if and only if A∗= In. Since A∗is obtained from A by performing a sequence of
elementary row operations, properties P1–P3 of determinants imply that det(A) is just
a nonzero multiple of det(A∗). If A is invertible, then det(A∗) = det(In) = 1, so that
det(A) is nonzero.
Conversely, if det(A) ̸= 0, then det(A∗) ̸= 0. This implies that A∗= In, and hence,
A is invertible.
According to Theorem 2.5.9 in the previous chapter, any linear system Ax = b has
either no solution, exactly one solution, or inﬁnitely many solutions. Recall from the
Invertible Matrix Theorem that the linear system Ax = b has a unique solution for every
b in Rn if and only if A is invertible. Thus, for an n × n linear system, Theorem 3.2.5
tells us that, for each b in Rn, the system Ax = b has a unique solution x if and only if
det(A) ̸= 0.
Next, we consider the homogeneous n × n linear system Ax = 0.
Corollary 3.2.6
The homogeneous n ×n linear system Ax = 0 has an inﬁnite number of solutions if and
only if det(A) = 0, and has only the trivial solution if and only if det(A) ̸= 0.
Proof The system Ax = 0 clearly has the trivial solution x = 0 under any circum-
stances. By our remarks above, this must be the unique solution if and only if det(A) ̸= 0.
The only other possibility, which occurs if and only if det(A) = 0, is that the system has
inﬁnitely many solutions.
Remark
The preceding corollary is very important, since we are often interested only
in determining the solution properties of a homogeneous linear system and not actually in
ﬁnding the solutions themselves. We will refer back to this corollary on many occasions
throughout the remainder of the text.
Example 3.2.7
Verify that the matrix A =
⎡
⎣
3 −1 2
7
0 1
−2
3 9
⎤
⎦is invertible. What can be concluded about
the solution to Ax = 0?
Solution:
It is easily shown that det(A) = 98 ̸= 0. Consequently, A is invertible. It
follows from Corollary 3.2.6 that the homogeneous system Ax = 0 has only the trivial
solution (0, 0, 0).
□
Example 3.2.8
Verify that the matrix A =
⎡
⎣
1 0
1
0 1
0
−3 0 −3
⎤
⎦is not invertible and determine a set of real
solutions to the system Ax = 0.
Solution:
By the row operation A13(3), we see that A is row equivalent to the upper
triangular matrix B =
⎡
⎣
1 0 1
0 1 0
0 0 0
⎤
⎦. By Theorem 3.2.1, det(B) = 0, and hence B and A

3.2
Properties of Determinants 213
are not invertible. We illustrate Corollary 3.2.6 by ﬁnding an inﬁnite number of solutions
(x1, x2, x3) to Ax = 0. Working with the upper triangular matrix B, we may set x3 = t,
a free parameter. The second row of the matrix system requires that x2 = 0 and the
ﬁrst row requires that x1 + x3 = 0, so x1 = −x3 = −t. Hence, the set of solutions is
{(−t, 0, t) : t ∈R}.
□
Further Properties of Determinants
In addition to elementary row operations, the following properties can also be useful in
evaluating determinants.
Let A and B be n × n matrices.
P5. det(AT ) = det(A).
P6. Let a1, a2, . . . , an denote the row vectors of A. If the ith row vector of A is the
sum of two row vectors, say ai = bi + ci, then det(A) = det(B)+ det(C), where
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a1
...
ai−1
bi
ai+1
...
an
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
and
C =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a1
...
ai−1
ci
ai+1
...
an
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The corresponding property is also true for columns.
P7. If A has a row (or column) of zeros, then det(A) = 0.
P8. If two rows (or columns) of A are scalar multiples of one another, then det(A) = 0.
P9. det(AB) = det(A)det(B).
P10. If A is an invertible matrix, then det(A) ̸= 0 and det(A−1) =
1
det(A).
The proofs of these properties are given at the end of the section. The main im-
portance of P5 is the implication that any results regarding determinants that hold for
the rows of a matrix also hold for the columns of a matrix. In particular, the properties
P1–P3 regarding the effects that elementary row operations have on the determinant
can be translated to corresponding statements on the effects that “elementary column
operations" have on the determinant. We will use the notations
CPi j,
CMi(k),
and
CAi j(k)
to denote the three types of elementary column operations.
Example 3.2.9
Let A =
⎡
⎢⎢⎣
4
12 −5 −2
−1 −18
0
3
2
−6
3
1
7
6 −1 −1
⎤
⎥⎥⎦. Evaluate det(A).

214
CHAPTER 3
Determinants
Solution:
Although we can carry out elementary row operations as we have in past
examples, in this case if we simply apply the elementary column operation CA42(6) to
A, we arrive at the answer much more quickly:
A ∼
⎡
⎢⎢⎣
4 0 −5 −2
−1 0
0
3
2 0
3
1
7 0 −1 −1
⎤
⎥⎥⎦.
The latter matrix has determinant zero by property P7, so we conclude that det(A) = 0.
Alternatively, we can simply note that the second and fourth columns of A are scalar
multiples of each other, so P8 implies that det(A) = 0.
□
Example 3.2.10
Use property P6 to express
++++
a1 + b1
c1 + d1
a2 + b2
c2 + d2
++++
as a sum of four determinants.
Solution:
Applying P6 to row 1 yields:
++++
a1 + b1
c1 + d1
a2 + b2
c2 + d2
++++ =
++++
a1
c1
a2 + b2
c2 + d2
++++ +
++++
b1
d1
a2 + b2
c2 + d2
++++ .
Now we apply P6 to row 2 of both of the determinants on the right-hand side to obtain
++++
a1 + b1
c1 + d1
a2 + b2
c2 + d2
++++ =
++++
a1
c1
a2
c2
++++ +
++++
a1
c1
b2
d2
++++ +
++++
b1
d1
a2
c2
++++ +
++++
b1
d1
b2
d2
++++ .
Notice that we could also have applied P6 to the columns of the given determinant.
□
Warning. In view of P6, it may be tempting to believe that if A, B, and C are n × n
matrices such that A = B+C, then det(A) = det(B)+ det(C). This is not true! Examples
abound to show the failure of this equation. For instance, if we take B = I2 and C = −I2,
then A = 02 so that det(A) = det(02) = 0, while det(B) = det(C) = 1. Thus, det(B)+
det(C) = 1 + 1 = 2 ̸= 0.
Let us consider a few more examples of applications of P1–P10, and then we will
proceed to the proofs of these properties to conclude this section. We ﬁrst pause to show
how P6 and P8 can be used together to quickly solve an example we ﬁrst studied from
a more elementary viewpoint in Section 3.1 (see part (b) of Example 3.1.11).
Example 3.2.11
Show that for all values of x, we have
2 −4x
−4
2
5 + 3x
3 −3
1 −2x
−2
1
= 0.
Solution:
Applying P6 to the ﬁrst column, we have
2 −4x
−4
2
5 + 3x
3 −3
1 −2x
−2
1
=
2 −4
2
5
3 −3
1 −2
1
+
−4x
−4
2
3x
3 −3
−2x
−2
1
= 2
1 −2
1
5
3 −3
1 −2
1
+ x
−4 −4
2
3
3 −3
−2 −2
1
= 0 + 0 = 0,

3.2
Properties of Determinants 215
since the ﬁrst and third rows of the ﬁrst matrix are the same, and the ﬁrst and second
columns of the second matrix are the same.
□
Example 3.2.12
If A =
!
sin φ
cos φ
−cos φ
sin φ
"
and B =
! cos θ
−sin θ
sin θ
cos θ
"
, show that det(AB) = 1.
Solution:
Using P9, we have
det(AB) = det(A)det(B) = (sin2 φ + cos2 φ)(cos2 θ + sin2 θ) = 1 · 1 = 1.
We note that it is a useful exercise in matrix multiplication and trigonometric identities
for the reader to verify by direct computation of AB that det(AB) = 1.
□
Example 3.2.13
Suppose that A and B are 3 × 3 matrices with det(A) = −2 and det(B) = 5, and let
D = diag(−2, 1, 3). (Note in view of Theorem 3.2.5 that A and B are both invertible.)
Compute the following:
(a) det(B−1 AT ).
(b) det(2B).
(c) det(D2 A−1B)2.
Solution:
(a) Using properties P9, P10, and P5, we have
det(B−1 AT ) = det(B−1)det(AT ) =
1
det(B) · det(A) = −2
5.
(b) Using property P4, we have
det(2B) = 23 · det(B) = 8 · 5 = 40.
(c) Note that det(D) = (−2)(1)(3) = −6. Thus, det(D2 A−1B) = (−6)2 ·
2
−1
2
3
·5 =
−90. Thus,
det(D2 A−1B)2 = (−90)2 = 8100.
□
Proofs of the Properties of Determinants
We now prove the properties P1–P10.
Proof of P1: Let B be the matrix obtained by interchanging row r with row s in A,
where, say r < s. Then the elements of B are related to those of A as follows:
bi j =
⎧
⎨
⎩
ai j if i ̸= r, s,
asj if i = r,
ar j if i = s.
Thus, from Deﬁnition 3.1.8,
det(B) =
*
σ(p1, p2, . . . , pr, . . . , ps, . . . , pn)b1p1b2p2 · · · brpr · · · bsps · · · bnpn
=
*
σ(p1, p2, . . . , pr, . . . , ps, . . . , pn)a1p1a2p2 · · · aspr · · · arps · · · anpn.

216
CHAPTER 3
Determinants
Interchanging pr and ps in σ(p1, p2, . . . , pr, . . . , ps, . . . , pn) and recalling from Theo-
rem 3.1.7 that such an interchange has the effect of changing the parity of the permutation,
we obtain
det(B) = −
*
σ(p1, p2, . . . , ps, . . . , pr, . . . , pn)a1p1a2p2 · · · arps · · · aspr · · · anpn,
where we have also rearranged the terms so that the row indices are in their natural
increasing order. The sum on the right-hand side of this equation is just det(A), so that
det(B) = −det(A).
Proof of P2: Let B be the matrix obtained by multiplying the ith row of A through
by any scalar k. Then bi j = kai j for each j. Then
det(B) =
*
σ(p1, p2, . . . , pn)b1p1b2p2 · · · bnpn
=
*
σ(p1, p2, . . . , pn)a1p1a2p2 · · · (kaipi ) · · · anpn = k det(A).
Proof of P4: This follows at once by applying property P2 to each row of the n × n
matrix A.
We prove properties P6, P7, and P8 next, since they simplify the proof of P3.
Proof of P6: The elements of A are
akj =
)
akj,
if k ̸= i,
bi j + ci j,
if k = i.
Thus, from Deﬁnition 3.1.8,
det(A) = 4 σ(p1, p2, . . . , pn)a1p1a2p2 · · · anpn
= 4 σ(p1, p2, . . . , pn)a1p1a2p2 · · · ai−1pi−1(bipi + cipi )ai+1pi+1 · · · anpn
= 4 σ(p1, p2, . . . , pn)a1p1a2p2 · · · ai−1pi−1bipi ai+1pi+1 · · · anpn
+ 4 σ(p1, p2, . . . , pn)a1p1a2p2 · · · ai−1pi−1cipi ai+1pi+1 · · · anpn
= det(B) + det(C).
Proof of P7: Since each term σ(p1, p2, . . . , pn)a1p1a2p2 · · · anpn in the formula for
det(A) contains a factor from the row (or column) of zeros, each such term is zero. Thus,
det(A) = 0.
Proof of P8: By P7, if A contains a row or column of zeros, then already we have
det(A) = 0.Therefore,wemaynowassumethattherowsandcolumnsofthematrix A are
all nonzero. Suppose rowsi and j in A are scalar multiples of one another. More precisely,
assume that row j is k times row i for some real number k ̸= 0. Let A′ denote the matrix
obtained by multiplying row i of the matrix A by k. By P2, we have det(A′) = k ·det(A).
Looking at the matrix A′, observe that row i and row j are identical. Therefore, if we
interchange these rows, the matrix, and hence its determinant, are unaltered. However,
according to P1, the determinant of the resulting matrix is −det(A′). Therefore,
det(A′) = −det(A′),
which implies that
det(A′) = 0.
Therefore, det(A) = 0.
The case of two columns that are scalar multiples of one another will follow once
we prove P5 below.

3.2
Properties of Determinants 217
Proof of P3: Let A = [a1, a2, . . . , an]T , and let B be the matrix obtained from A
when k times row j of A is added to row i of A. Then
B = [a1, a2, . . . , ai + ka j, . . . , an]T
so that, using P6,
det(B) = det([a1, a2, . . . , ai + ka j, . . . , an]T )
= det([a1, a2, . . . , an]T ) + det([a1, a2, . . . , kaj, . . . , an]T ).
Rows i and j of the second matrix appearing on the right-hand side are multiples of one
another, and so by property P8, the value of the second determinant is zero. Thus,
det(B) = det([a1, a2, . . . , an]T ) = det(A),
as required.
Proof of P5: Using Deﬁnition 3.1.8, we have
det(AT ) =
*
σ(p1, p2, . . . , pn)ap11ap22ap33 · · · apnn.
(3.2.2)
Since (p1, p2, . . . , pn) is a permutation of 1, 2, . . . , n, it follows that, by rearranging
terms,
ap11ap22ap33 · · · apnn = a1q1a2q2a3q3 · · · anqn,
(3.2.3)
for appropriate values of q1, q2, . . . , qn. Furthermore,
N(p1, . . . , pn) = # of interchanges in changing (1, 2, . . . , n) to (p1, p2, . . . , pn)
= # of interchanges in changing (p1, p2, . . . , pn) to (1, 2, . . . , n)
and by (3.2.3), this number is
= # of interchanges in changing (1, 2, . . . , n) to (q1, q2, . . . , qn)
= N(q1, . . . , qn).
Thus,
σ(p1, p2, . . . , pn) = σ(q1, q2, . . . , qn).
(3.2.4)
Substituting Equations (3.2.3) and (3.2.4) into Equation (3.2.2), we have:
det(AT ) =
*
σ(q1, q2, . . . , qn)a1q1a2q2a3q3 · · · anqn
= det(A).
Proof of P9: Let E denote an elementary matrix. We leave it as an exercise (Prob-
lem 56) to verify that
det(E) =
⎧
⎨
⎩
−1
if E permutes rows,
+1
if E adds a multiple of one row to another row,
k
if E scales a row by k.
It then follows from properties P1–P3 that in each case
det(E A) = det(E)det(A).
(3.2.5)
Now consider a general product AB. We need to distinguish two cases.

218
CHAPTER 3
Determinants
1. If A is not invertible, then from Corollary 2.6.13, so is AB. Consequently, applying
Theorem 3.2.5,
det(AB) = 0 = det(A)det(B).
2. If A is invertible, then from Theorem 2.7.5, we know that it can be expressed
as the product of elementary matrices, say, A = E1E2 · · · Er. Hence, repeatedly
applying (3.2.5) gives
det(AB) = det(E1E2 · · · Er B) = det(E1)det(E2 · · · Er B)
= det(E1)det(E2) · · · det(Er)det(B)
= det(E1E2 · · · Er)det(B) = det(A)det(B).
Proof of P10: Since A is invertible, det(A) ̸= 0 by Theorem 3.2.5. We can write
AA−1 = In. Recalling that det(In) = 1, we use P9 to derive that
det(A) · det(A−1) = det(AA−1) = det(In) = 1,
from which P10 immediately follows.
Exercises for 3.2
Skills
• Be able to compute the determinant of an upper or
lower triangular matrix “at a glance" (Theorem 3.2.1).
• Know the effects that elementary row operations have
on the determinant of a matrix.
• Likewise, be comfortable with the effects that column
operations have on the determinant of a matrix.
• Be able to use the determinant to decide if a matrix is
invertible (Theorem 3.2.5).
• Know how the determinant behaves with respect to
matrix multiplication, matrix transpose, and taking the
inverse of a matrix.
• Be able to compute determinants by using elementary
row operations to bring the matrix to upper triangular
form and then applying Theorem 3.2.1.
• Be able to use the determinant to address whether or
not a linear system with a square coefﬁcient matrix
has a unique solution.
True-False Review
For items (a)–(f), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If each element of an n ×n matrix is doubled, then the
determinant of the matrix also doubles.
(b) Multiplying a row of an n × n matrix through by a
scalar c has the same effect on the determinant as mul-
tiplying a column of the matrix through by c.
(c) If A is an n × n matrix, then det(A5) = (det A)5.
(d) If A is an n × n matrix with real entries, then det(A2)
cannot be negative.
(e) The matrix
! x2
x
y2
y
"
is not invertible if and only if
x = 0 or y = 0.
(f) If A and B are n × n matrices, then det(AB) =
det(B A).
Problems
For Problems 1–14, evaluate the determinant of the given
matrix by ﬁrst using elementary row operations to reduce it
to upper triangular form.
1.
−2
5
5 −4 .
2.
1
2 3
2
6 4
3 −5 2
.
3.
2 −1 4
3
2 1
−2
1 4
.

3.2
Properties of Determinants 219
4.
2 1
3
−1 2
6
4 1 12
.
5.
0
1 −2
−1
0
3
2 −3
0
.
6.
3 7
1
5 9 −6
2 1
3
.
7.
1 −1 2 4
3
1 2 4
−1
1 3 2
2
1 4 2
.
8.
2
32
1
4
26 104 26 −13
2
56
2
7
1
40
1
5
.
9.
0
1 −1 1
−1
0
1 1
1 −1
0 1
−1 −1 −1 0
.
10.
2 1 3 5
3 0 1 2
4 1 4 3
5 2 5 3
.
11.
2 −1
3
4
7
1
2
3
−2
4
8
6
6 −6 18 −24
.
12.
7 −1 3 4
14
2 4 6
21
1 3 4
−7
4 5 8
.
13.
3
7
1 2
3
1
1 −1 0
1
4
8 −1 6
6
3
7
0 9
4
8 16 −1 8 12
.
14.
1 2 3 4 5
5 4 3 2 1
2 3 4 5 6
6 5 4 3 2
0 2 4 6 8
.
For Problems 15–21, use Theorem 3.2.5 to determine
whether the given matrix is invertible or not.
15.
! −1
1
1 −1
"
.
16.
! 2 1
3 2
"
.
17.
⎡
⎣
−1
2 3
5 −2 1
8 −2 5
⎤
⎦.
18.
⎡
⎣
2 6 −1
3 5
1
2 0
1
⎤
⎦.
19.
⎡
⎢⎢⎣
1 1
1
1
−1 1 −1
1
1 1 −1 −1
−1 1
1 −1
⎤
⎥⎥⎦.
20.
⎡
⎢⎢⎣
1
0 2 −1
3 −2 1
4
2
1 6
2
1 −3 4
0
⎤
⎥⎥⎦.
21.
⎡
⎢⎢⎣
1
2 −3
5
−1
2 −3
6
2
3 −1
4
1 −2
3 −6
⎤
⎥⎥⎦.
22. Determine all values of the constant k for which the
given system has an inﬁnite number of solutions.
x1 + 2x2 + kx3 = 0,
2x1 −kx2 +
x3 = 0,
3x1 + 6x2 +
x3 = 0.
23. Determine all values of the constant k for which the
given system has a unique solution
x1 + kx2 = b1,
kx1 + 4x2 = b2.
24. Determine all values of k for which the given system
has a unique solution.
x1 + kx2
= 2,
kx1 + x2 + x3 = 1,
x1 + x2 + x3 = 1.

220
CHAPTER 3
Determinants
25. Determine all values of k for which the given system
has an inﬁnite number of solutions.
x1 + 2x2 +
x3 = kx1,
2x1 + x2 +
x3 = kx2
x1 + x2 + 2x3 = kx3.
26. If A =
⎡
⎣
1 −1 2
3
1 4
0
1 3
⎤
⎦, ﬁnd det(A), and use properties
of determinants to ﬁnd det(A−1) and det(−3A).
27. Verify property P9 for the matrices A =
! 1 −1
2
3
"
and B =
!
1 2
−2 4
"
.
28. Verify property P9 for the matrices
A =
! cosh x
sinh x
sinh x
cosh x
"
and B =
! cosh y
sinh y
sinh y
cosh y
"
.
For Problems 29–32, let A
=
! a
b
c
d
"
and assume
det(A) = 1. Find det(B).
29. B =
! −2a
−2c
3a + b 3c + d
"
.
30. B =
! 3c
3d
4a
4b
"
.
31. B =
! −6d
−6c
3b
3a
"
.
32. B =
!
−b
−a
d −4b c −4a
"
.
For Problems 33–36, let A =
⎡
⎣
a
b
c
d
e
f
g
h
i
⎤
⎦and assume
det(A) = −6. Find det(B).
33. B =
⎡
⎣
g
h
i
−2d
−2e −2 f
−a
−b
−c
⎤
⎦.
34. B =
⎡
⎣
−4d
−4e
−4 f
g + 5a
h + 5b i + 5c
a
b
c
⎤
⎦.
35. B =
⎡
⎣
d
e
f
−3a
−3b
−3c
g −4d
h −4e i −4 f
⎤
⎦.
36. B =
⎡
⎣
2a
2d
2g
b −c
e −f
h −i
c −a
f −d
i −g
⎤
⎦.
For Problems 37–44, let A and B be 4×4 matrices such that
det(A) = 5 and det(B) = 3. Compute the determinant of
the given matrix.
37. ABT .
38. A2B5.
39. (A−1B2)3.
40. (2B)−1(AB)T .
41. (5A)(2B).
42. B−1A−1.
43. B−1(2A)BT .
44. (4B)3.
45. Let A, B, and S be n × n matrices. If S−1 AS = B,
must A = B? Must det(A) = det(B)? Justify your
answers.
46. Let
A =
⎡
⎣
1 2 4
3 1 6
k
3 2
⎤
⎦.
(a) Intermsofk,ﬁndthevolumeoftheparallelepiped
determined by the row vectors of the matrix A.
(b) Does your answer to (a) change if we instead con-
sider the volume of the parallelepiped determined
by the column vectors of the matrix A? Why or
why not?
(c) For what value(s) of k, if any, is A invertible?
47. Without expanding the determinant, determine all val-
ues of x for which det(A) = 0 if
A =
⎡
⎣
1 −1
x
2
1
x2
4 −1
x3
⎤
⎦.
48. Use only properties P6, P1, and P2 to show that
αx −βy
βx −αy
βx + αy
αx + βy = (x2 + y2) α
β
β
α .

3.2
Properties of Determinants 221
49. Use only properties P6, P1, and P2 to ﬁnd the value of
αβγ such that
a1 + βb1
b1 + γ c1
c1 + αa1
a2 + βb2
b2 + γ c2
c2 + αa2
a3 + βb3
b3 + γ c3
c3 + αa3
= 0
for all values of ai, bi, ci.
50. Use only properties P3 and P8 to prove property P7.
51. An n×n matrix A that satisﬁes AT = A−1 is called an
orthogonal matrix. Show that if A is an orthogonal
matrix, then det(A) = ±1.
52. (a) Use the deﬁnition of a determinant to prove that
if A is an n × n lower triangular matrix, then
det(A) = a11a22a33 · · · ann =
n
1
i=1
aii.
(b) Evaluate the following determinant by ﬁrst reduc-
ing it to lower triangular form and then using the
result from (a):
2 −1 3 5
1
2 2 1
3
0 1 4
1
2 0 1
.
53. Use determinants to prove that if A is invertible and B
and C are matrices with AB = AC, then B = C.
54. If A and S are n × n matrices with S invertible, show
that det((S−1 AS)2) =[det(A)]2.
[Hint: Write out (S−1 AS)2.]
55. If det(A3) = 0, is it possible for A to be invertible?
Justify your answer.
56. Let E be an elementary matrix. Verify the formula for
det(E) given in the text at the beginning of the proof
of P9.
57. Show that
x
y
1
x1
y1
1
x2
y2
1
= 0 represents the equation of
the straight line through the distinct points (x1, y1) and
(x2, y2).
58. Without expanding the determinant, show that
1
x
x2
1
y
y2
1
z
z2
= (y −z)(z −x)(x −y).
59. If A is an n × n skew-symmetric matrix and n is odd,
prove that det(A) = 0.
60. Let A = [a1, a2, . . . , an] be an n × n matrix, and let
b = c1a1 + c2a2 + · · · + cnan, where c1, c2, . . . , cn
are constants. If Bk denotes the matrix obtained from
A by replacing the kth column vector by b, prove that
det(Bk) = ckdet(A),
k = 1, 2, . . . , n.
61. ⋄Let A be the general 4 × 4 matrix.
(a) Verify property P1 of determinants in the case
when the ﬁrst two rows of A are permuted.
(b) Verify property P2 of determinants in the case
when row 1 of A is divided by k.
(c) Verify property P3 of determinants in the case
when k times row 2 is added to row 1.
62. ⋄For a randomly generated 5 × 5 matrix, verify that
det(AT ) = det(A).
63. ⋄Determine all values of a for which
⎡
⎢⎢⎢⎢⎣
1
2 3 4 a
2
1 2 3
4
3
2 1 2
3
4
3 2 1
2
a
4 3 2
1
⎤
⎥⎥⎥⎥⎦
is invertible.
64. ⋄If A =
⎡
⎣
1 4
1
3 2
1
3 4 −1
⎤
⎦, determine all values of the
constant k for which the linear system (A−kI3)x = 0
has an inﬁnite number of solutions, and ﬁnd the cor-
responding solutions.
65. ⋄Use the determinant to show that
A =
⎡
⎢⎢⎣
1 2 3 4
2 1 2 3
3 2 1 2
4 3 2 1
⎤
⎥⎥⎦
is invertible, and use A−1 to solve Ax = b if b =
[3, 7, 1, −4]T .

222
CHAPTER 3
Determinants
3.3
Cofactor Expansions
We now obtain an alternative method for evaluating determinants. The basic idea is that
we can reduce a determinant of order n to a sum of determinants of order n−1. Continuing
in this manner, it is possible to express any determinant as a sum of determinants of
order 2. This method is the one most frequently used to evaluate a determinant by hand,
although the procedure introduced in the previous section whereby we use elementary
row operations toreducethematrixtoupper triangular form involves less workingeneral.
When A is invertible, the technique we derive leads to formulas for both A−1 and the
unique solution to Ax = b. We ﬁrst require two preliminary deﬁnitions.
DEFINITION
3.3.1
Let A be an n × n matrix. The minor, Mi j, of the element ai j is the determinant of
the matrix obtained by deleting the ith row vector and jth column vector of A.
Remark
Notice that if A is an n ×n matrix, then Mi j is a determinant of order n −1.
By convention, if n = 1, we deﬁne the “empty" determinant M11 to be 1.
Example 3.3.2
If A =
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦, then, for example,
M23 = a11
a12
a31
a32
and
M31 = a12
a13
a22
a23 .
□
Example 3.3.3
Determine the minors M11, M21, and M32 for
A =
⎡
⎣
−4
9 1
−2 −2 5
6
3 1
⎤
⎦.
Solution:
Using Deﬁnition 3.3.1, we have:
M11 = −2 5
3 1 = −17,
M21 = 9 1
3 1 = 6,
M32 = −4 1
−2 5 = −18.
□
DEFINITION
3.3.4
Let A be an n × n matrix. The cofactor, Ci j, of the element ai j is deﬁned by
Ci j = (−1)i+ j Mi j,
where Mi j is the minor of ai j.
From Deﬁnition 3.3.4, we see that the cofactor of ai j and the minor of ai j are the
same if i + j is even, and they differ by a minus sign if i + j is odd. The appropriate

3.3
Cofactor Expansions 223
sign in the cofactor Ci j is easy to remember, since it alternates in the following manner:
+ −+ −+ . . .
−+ −+ −. . .
+ −+ −+ . . .
...
...
...
...
...
.
The positions indicated with plus signs require no sign change from Mi j to Ci j, whereas
the positions with minus signs do.
Example 3.3.5
Determine the cofactors C11, C21, and C32 for the matrix in Example 3.3.3.
Solution:
We have already obtained the minors M11, M21, and M32 in Example 3.3.3,
so it follows that
C11 = +M11 = −17,
C21 = −M21 = −6,
C32 = −M32 = 18.
□
Example 3.3.6
If A =
! a11
a12
a21
a22
"
, verify that det(A) = a11C11 + a12C12.
Solution:
In this case,
C11 = +det[a22] = a22,
C12 = −det[a12] = −a12,
so that
a11C11 + a12C12 = a11a22 + a12(−a21) = det(A).
□
Example 3.3.7
If A =
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦, verify that det(A) = a13C13 + a23C23 + a33C33.
Solution:
In this case,
C13 = +det
! a21
a22
a31
a32
"
,
C23 = −det
! a11
a12
a31
a32
"
,
C33 = +det
! a11
a12
a21
a22
"
,
and using the formula for the determinant of a 2 × 2 matrix, we ﬁnd that
a13C13 + a23C23 + a33C33 = a13(a21a32 −a22a31) −a23(a11a32 −a12a31)
+ a33(a11a22 −a12a21)
= a11a22a33 + a12a23a31 + a13a21a32 −a13a22a31
−a11a23a32 −a12a21a33
= det(A).
□
The preceding two examples are special cases of the following important theorem.
Theorem 3.3.8
(Cofactor Expansion Theorem)
Let A be an n × n matrix. If we multiply the elements in any row (or column) of A by
their cofactors, then the sum of the resulting products is det(A). Thus,
1. If we expand along row i,
det(A) = ai1Ci1 + ai2Ci2 + · · · + ainCin =
n
*
k=1
aikCik.

224
CHAPTER 3
Determinants
2. If we expand along column j,
det(A) = a1 jC1 j + a2 jC2 j + · · · + anjCnj =
n
*
k=1
akjCkj.
The expressions for det(A) appearing in this theorem are known as cofactor ex-
pansions. Notice that a cofactor expansion can be formed along any row or column of
A; regardless of the chosen row or column, the cofactor expansion will always yield the
determinant of A. However, sometimes the calculation is simpler if the row or column
of expansion is wisely chosen. We will illustrate this in the examples below. The proof
of the Cofactor Expansion Theorem will be presented after some examples.
Example 3.3.9
UsetheCofactorExpansionTheoremalong(a)row1,(b)column2toﬁnd
−8
7
0
−1 −3
6
4 −2 −2
.
Solution:
(a) We have
−8
7
0
−1 −3
6
4 −2 −2
= −8 −3
6
−2 −2 −7 −1
6
4 −2 + 0 = −8(18) −7(−22) = 10.
(b) We have
−8
7
0
−1 −3
6
4 −2 −2
= −7 −1
6
4 −2 + (−3) −8
0
4 −2 −(−2) −8 0
−1 6
= −7(−22) −3(16) + 2(−48) = 10.
□
Notice that (a) was easier than (b) in the previous example, because of the zero in
row 1. Whenever one uses the cofactor expansion method to evaluate a determinant, it
is usually best to select a row or column containing as many zeros as possible in order
to minimize the amount of computation required.
Example 3.3.10
Evaluate
−6
2
1 5
7 −3
0 2
9 −4
0 8
1
0 −2 0
.
Solution:
In this case, it is easiest to use either row 4 or column 3 for cofactor
expansion. Choosing row 4, we have
−6
2
1 5
7 −3
0 2
9 −4
0 8
1
0 −2 0
= −
2 1 5
−3 0 2
−4 0 8
−(−2)
−6
2 5
7 −3 2
9 −4 8
.
The reader can verify by any of the methods in this chapter that
2 1 5
−3 0 2
−4 0 8
= 16
and
−6
2 5
7 −3 2
9 −4 8
= 15.

3.3
Cofactor Expansions 225
Therefore,
−6
2
1 5
7 −3
0 2
9 −4
0 8
1
0 −2 0
= −16 + 2(15) = 14.
For additional practice, the reader may wish to verify our result here by cofactor expan-
sion along a different row or column.
□
Now we turn to the
Proof of the Cofactor Expansion Theorem: It follows from the deﬁnition that
det(A) can be written in the form
det(A) = ai1 ˆCi1 + ai2 ˆCi2 + · · · + ain ˆCin
(3.3.1)
where the coefﬁcients ˆCi j contain no elements from row i or column j. We must show
that
ˆCi j = Ci j
where Ci j is the cofactor of ai j.
Consider ﬁrst a11. From Deﬁnition 3.1.8, the terms of det(A) that contain a11 are
given by
a11
*
σ(1, p2, p3, . . . , pn)a2p2a3p3 · · · anpn,
where the summation is over the (n −1)! distinct permutations of 2, 3, . . . , n. Thus,
ˆC11 =
*
σ(1, p2, p3, . . . , pn)a2p2a3p3 · · · anpn.
However, this summation is just the minor M11, and since C11 = M11, we have shown
the coefﬁcient of a11 in det(A) is indeed the cofactor C11.
Now consider the element ai j. By successively interchanging adjacent rows and
columns of A, we can move ai j into the (1,1)-position without altering the relative
positions of the other rows and columns of A. We let A′ denote the resulting matrix.
Obtaining A′ from A requires i −1 row interchanges and j −1 column interchanges.
Therefore, the total number of interchanges required to obtain A′ from A is i + j −2.
Consequently,
det(A) = (−1)i+ j−2det(A′) = (−1)i+ jdet(A′).
Now for the key point. The coefﬁcient of ai j in det(A) must be (−1)i+ j times the
coefﬁcient of ai j in det(A′). But, ai j occurs in the (1,1)-position of A′, and so, as we
have previously shown, its coefﬁcient in det(A′) is M′
11. Since the relative positions
of the remaining rows in A has not altered, it follows that M′
11 = Mi j, and therefore
the coefﬁcient of ai j in det(A′) is Mi j. Consequently, the coefﬁcient of ai j in det(A) is
(−1)i+ j Mi j = Ci j. Applying this result to the elements ai1, ai2, . . . , ain and comparing
with (3.3.1) yields
ˆCi j = Ci j,
j = 1, 2, . . . , n,
which establishes the theorem for expansion along a row. The result for expansion along
a column follows directly, since det(AT ) = det(A).
We now have two computational methods for evaluating determinants: the use of
elementary row operations given in the previous section to reduce the matrix in question
to upper triangular form, and the Cofactor Expansion Theorem. In evaluating a given
determinant by hand, it is usually most efﬁcient (and least error prone) to use a com-
bination of the two techniques. More speciﬁcally, we use elementary row operations to

226
CHAPTER 3
Determinants
set all except one element in a row or column equal to zero and then use the Cofactor
Expansion Theorem on that row or column. We illustrate with an example.
Example 3.3.11
Evaluate
2 1
8 6
1 4
1 3
−1 2
1 4
1 3 −1 2
.
Solution:
We have
2 1
8 6
1 4
1 3
−1 2
1 4
1 3 −1 2
1=
0 −7
6
0
1
4
1
3
0
6
2
7
0 −1 −2 −1
2= −
−7
6
0
6
2
7
−1 −2 −1
3= −
−7
6
0
−1 −12
0
−1
−2 −1
4= 90.
1. A21(−2), A23(1), A24(−1)
2. Cofactor expansion along column 1
3. A32(7)
4. Cofactor expansion along column 3.
□
Example 3.3.12
Determine all values of k for which the system
10x1 + kx2−x3 = 0,
kx1 + x2−x3 = 0,
2x1 + x2−3x3 = 0,
has nontrivial solutions.
Solution:
We will apply Corollary 3.2.6. The determinant of the matrix of coefﬁcients
of the system is
det(A) =
10 k
−1
k
1 −1
2
1 −3
1=
10
k
−1
k −10
1 −k
0
−28
1 −3k
0
2= −k −10
1 −k
−28
1 −3k
= −[(k −10)(1 −3k) −(−28)(1 −k)]
= 3k2 −3k −18 = 3(k2 −k −6)
= 3(k −3)(k + 2).
1. A12(−1), A13(−3)
2. Cofactor expansion along column 3.
From Corollary 3.2.6, the system has nontrivial solutions if and only if det(A) = 0; that
is, if and only if k = 3 or k = −2.
□
Later in the text, we will study the important theory of eigenvalues and eigenvec-
tors of an n × n matrix A. Among their many applications, eigenvalues play a crucial
role in ﬁnding solutions to linear systems of differential equations, as we shall see in
Chapter 9. By deﬁnition, the eigenvalues of A are precisely those scalars λ for which
det(A −λIn) = 0. Let us illustrate with an example.
Example 3.3.13
Find the eigenvalues of the matrix A =
⎡
⎣
−1 1 1
0 3 4
2 0 2
⎤
⎦.

3.3
Cofactor Expansions 227
Solution:
In the matrix A −λI, we subtract λ from the main diagonal of A. Using
cofactor expansion along the ﬁrst column of A −λI, we obtain
det(A −λI) = det
⎡
⎣
−1 −λ
1
1
0
3 −λ
4
2
0
2 −λ
⎤
⎦
= (−1 −λ) [(3 −λ)(2 −λ)] + 2 [4 −(3 −λ)]
= −λ3 + 4λ2 + λ −4.
Because the matrix A is 3 × 3, observe that det(A −λI) is a polynomial of degree 3.
Finding roots of a polynomial equation when the degree is 3 or more can be difﬁcult.
Most examples and exercises in this text have been constructed to have integer roots,
but even so, great care must be used to factor the polynomials that arise correctly. In this
case, we can factor by grouping as follows:
det(A −λI) = −λ3 + 4λ2 + λ −4 = −λ(λ2 −1) + 4(λ2 −1) = (−λ + 4)(λ2 −1)
= (−λ + 4)(λ −1)(λ + 1),
whose roots are λ = 4, λ = 1, and λ = −1. Thus, these are the eigenvalues of A.
□
The Adjoint Method for A−1
We next establish two corollaries to the Cofactor Expansion Theorem that, in the case
of an invertible matrix A, lead to a method for expressing the elements of A−1 in terms
of determinants.
Corollary 3.3.14
If the elements in the ith row (or column) of an n × n matrix A are multiplied by the
cofactors of a different row (or column), then the sum of the resulting products is zero.
That is,
1. If we use the elements of row i and the cofactors of row j,
n
*
k=1
aikC jk = 0,
i ̸= j.
(3.3.2)
2. If we use the elements of column i and the cofactors of column j,
n
*
k=1
akiCkj = 0,
i ̸= j.
(3.3.3)
Proof We prove (3.3.2). Let B be the matrix obtained from A by adding row i to row
j (i ̸= j) in the matrix A. By P3, det(B) = det(A). Cofactor expansion of B along row
j gives
det(A) = det(B) =
n
*
k=1
(a jk + aik)C jk =
n
*
k=1
a jkC jk +
n
*
k=1
aikC jk.
That is,
det(A) = det(A) +
n
*
k=1
aikC jk,

228
CHAPTER 3
Determinants
since the ﬁrst summation on the right-hand side is simply det(A) by the Cofactor Ex-
pansion Theorem. It follows immediately that
n
*
k=1
aikC jk = 0,
i ̸= j.
Equation (3.3.3) can be proved similarly (Problem 69).
The Cofactor Expansion Theorem and the above corollary can be combined into the
following corollary.
Corollary 3.3.15
Let A be an n × n matrix. If δi j is the Kronecker delta symbol (see Deﬁnition 2.2.21),
then
n
*
k=1
aikC jk = δi j det(A),
n
*
k=1
akiCkj = δi j det(A).
(3.3.4)
The formulas in (3.3.4) should be reminiscent of the index form of the matrix
product. Combining this with the fact that the Kronecker delta gives the elements of the
identity matrix, we might suspect that (3.3.4) is telling us something about the inverse
of A. Before establishing that this suspicion is indeed correct, we need a deﬁnition.
DEFINITION
3.3.16
If every element in an n × n matrix A is replaced by its cofactor, the resulting matrix
is called the matrix of cofactors and is denoted MC. The transpose of the matrix of
cofactors, MT
C , is called the adjoint of A and is denoted adj(A). Thus, the elements
of adj(A) are
adj(A)i j = C ji.
Example 3.3.17
Determine adj(A) if A =
⎡
⎣
6 −1
0
2 −2
1
3
0 −3
⎤
⎦.
Solution:
We ﬁrst determine the cofactors of A:
C11 = 6,
C12 = 9,
C13 = 6,
C21 = −3,
C22 = −18,
C23 = −3,
C31 = −1,
C32 = −6,
C33 = −10.
Thus,
MC =
⎡
⎣
6
9
6
−3 −18
−3
−1
−6 −10
⎤
⎦,
so that
adj(A) = MT
C =
⎡
⎣
6
−3
−1
9 −18
−6
6
−3 −10
⎤
⎦.
□
We can now prove the next theorem.

3.3
Cofactor Expansions 229
Theorem 3.3.18
(The Adjoint Method for Computing A−1)
If det(A) ̸= 0, then
A−1 =
1
det(A) adj(A).
Proof Let B =
1
det(A) adj(A). Then we must establish that AB = In = B A. But,
using the index form of the matrix product,
(AB)i j =
n
*
k=1
aikbkj =
n
*
k=1
aik ·
1
det(A) · adj(A)kj =
1
det(A)
n
*
k=1
aikC jk = δi j,
where we have used Equation (3.3.4) in the last step. Consequently, AB = In. We leave
it as an exercise (Problem 75) to verify that B A = In also.
Example 3.3.19
For the matrix in Example 3.3.17,
det(A) = 27,
so that
A−1 = 1
27
⎡
⎣
6
−3
−1
9 −18
−6
6
−3 −10
⎤
⎦.
□
For square matrices of relatively small size, the adjoint method for computing A−1
is often easier than using elementary row operations to reduce A to upper triangular form.
In Chapter 9, we will ﬁnd that the solution of a system of differential equations can
be expressed naturally in terms of matrix functions. Certain problems will require us
to ﬁnd the inverse of such matrix functions. For 2 × 2 systems, the adjoint method is
very quick.
Example 3.3.20
Find A−1 if A =
! −sin t
e−3t cos t
cos t
e−3t sin t
"
.
Solution:
In this case,
det(A) = −(sin2 t)e−3t −(cos2 t)e−3t = −e−3t(sin2 t + cos2 t) = −e−3t,
and
adj(A) =
!
e−3t sin t
−e−3t cos t
−cos t
−sin t
"
,
so that
A−1 = −e3t
!
e−3t sin t
−e−3t cos t
−cos t
−sin t
"
=
! −sin t
cos t
e3t cos t
e3t sin t
"
.
□
Cramer’s Rule
We now derive a technique that enables us, in the case when det(A) ̸= 0, to express the
unique solution of an n × n linear system
Ax = b

230
CHAPTER 3
Determinants
directly in terms of determinants. Let Bk denote the matrix obtained by replacing the kth
column vector of A with b. Thus,
Bk =
⎡
⎢⎢⎢⎣
a11
a12
. . .
b1
. . .
a1n
a21
a22
. . .
b2
. . .
a2n
...
...
...
...
an1
an2
. . .
bn
. . .
ann
⎤
⎥⎥⎥⎦.
The key point to notice is that the cofactors of the elements in the kth column of Bk
coincide with the corresponding cofactors of A. Thus, expanding det(Bk) along the kth
column using the Cofactor Expansion Theorem yields
det(Bk) = b1C1k + b2C2k + · · · + bnCnk =
n
*
i=1
biCik,
k = 1, 2, . . . , n,
(3.3.5)
where the Ci j are the cofactors of A. We can now prove Cramer’s rule.
Theorem 3.3.21
(Cramer’s Rule)
If det(A) ̸= 0, the unique solution to the n ×n system Ax = b is (x1, x2, . . . , xn), where
xk = det(Bk)
det(A) ,
k = 1, 2, . . . , n.
(3.3.6)
Proof If det(A) ̸= 0, then the system Ax = b has the unique solution
x = A−1b,
(3.3.7)
where, from Theorem 3.3.18, we can write
A−1 =
1
det(A) adj(A).
(3.3.8)
If we let
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦
and b =
⎡
⎢⎢⎢⎣
b1
b2
...
bn
⎤
⎥⎥⎥⎦
and recall that adj(A)i j = C ji, then substitution from (3.3.8) into (3.3.7) and use of the
index form of the matrix product yields
xk =
n
*
i=1
(A−1)kibi =
n
*
i=1
1
det(A)adj(A)kibi
=
1
det(A)
n
*
i=1
Cikbi,
k = 1, 2, . . . , n.
Using (3.3.5), this can be written as
xk = det(Bk)
det(A) ,
k = 1, 2, . . . , n
as required.
Remark
In general, Cramer’s rule requires more work than the Gaussian Elimination
method, and in addition, it is restricted to n × n systems whose coefﬁcient matrix is

3.3
Cofactor Expansions 231
invertible. However, it is a powerful theoretical tool, since it gives us a formula for the
solution of an n × n system provided det(A) ̸= 0.
Example 3.3.22
Use Cramer’s rule to solve the system
−5x1 −x2 + 2x3 =
9,
x1 −2x2 + 7x3 = −2,
3x1 −x2 +
x3 = −6.
Solution:
Using the notation in Theorem 3.3.21, the following determinants are easily
evaluated:
det(A) =
−5 −1 2
1 −2 7
3 −1 1
= −35,
det(B1) =
9 −1 2
−2 −2 7
−6 −1 1
= 65,
det(B2) =
−5
9 2
1 −2 7
3 −6 1
= −20,
det(B3) =
−5 −1
9
1 −2 −2
3 −1 −6
= −5.
Inserting these results into (3.3.6) yields x1 = −65
35 = −13
7 , x2 = 20
35 = 4
7, and
x3 = 5
35 = 1
7, so that the solution to the system is (−13
7 , 4
7, 1
7).
□
Exercises for 3.3
Key Terms
Minor, Cofactor, Cofactor expansion, Matrix of cofactors,
Adjoint, Cramer’s rule.
Skills
• Be able to compute the minors and cofactors of a
matrix.
• Understand the difference between Mi j and Ci j.
• Be able to compute the determinant of a matrix via
cofactor expansion.
• Be able to compute the matrix of cofactors and the
adjoint of a matrix.
• Be able to use the adjoint of an invertible matrix A to
compute A−1.
• Be able to use Cramer’s rule to solve a linear system
of equations.
True-False Review
For items (a)–(j), decide if the given statement is true or
false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The (2, 3)-minor of a matrix is the same as the (2, 3)-
cofactor of the matrix.
(b) If A = diag(a1, a2, . . . , an), then for all i ̸=
j,
Mi j = 0.
(c) If
A
=
diag(a1, a2, . . . , an),
then
Mii
=
diag(a1, . . . , ai−1, ai+1, . . . , an).
(d) If every entry of a square matrix A is doubled, then
each of its minors doubles.
(e) We have A·adj(A) = det(A)· In for all n ×n matrices
A.
(f) Cofactor expansion of a matrix along any row or col-
umn will yield the same result, although the individual
terms in the expansion along different rows or columns
can vary.
(g) If A is an n × n matrix and c is a scalar, then
adj(cA) = c · adj(A).

232
CHAPTER 3
Determinants
(h) If A and B are 2 × 2 matrices, then
adj(A + B) = adj(A) + adj(B).
(i) If A and B are 2 × 2 matrices, then
adj(AB) = adj(A) · adj(B).
(j) For every positive integer n, adj(In) = In.
Problems
For Problems 1–4, determine all minors and cofactors of the
given matrix.
1. A =
! −9 2
0 5
"
.
2. A =
! 1 −3
2
4
"
.
3. A =
⎡
⎣
1 −1 2
3 −1 4
2
1 5
⎤
⎦.
4. A =
⎡
⎣
2
10 3
0 −1 0
4
1 5
⎤
⎦.
5. If A =
⎡
⎢⎢⎣
1 3 −1 2
3 4
1 2
7 1
4 6
5 0
1 2
⎤
⎥⎥⎦,
determine the minors M12, M31, M23, M42, and the
corresponding cofactors.
6. If
A =
⎡
⎢⎢⎣
−2
9
0 −1
4 −6
8
8
0 −1 −3
4
7 −7
3
1
⎤
⎥⎥⎦,
determine the minors M41, M22, M23, M43, and the
corresponding cofactors.
For Problems 7–14, use the cofactor expansion theorem to
evaluate the given determinant along the speciﬁed row or
column.
7.
−8 6
−4 9 , column 2.
8.
1 −2
1
3 , row 1.
9.
−1 2
3
1 4 −2
3 1
4
, column 3.
10.
2 1 −4
7 1
3
1 5 −2
, row 2.
11.
3 1
4
7 1
2
2 3 −5
, column 1.
12.
0
2 −3
−2
0
5
3 −5
0
, row 3.
13.
1 −2
3
0
4
0
7 −2
0
1
3
4
1
5 −2
0
, column 4.
14.
−3 0 −1 0
0 4
0 2
1 4 −4 2
0 2
5 0
, column 1.
For Problems 15–22, evaluate the given determinant by using
the Cofactor Expansion Theorem. Do not apply elementary
row operations.
15.
−4
2 −1
7 −3
2
−6
6
2
.
16.
1 0 −2
3 1 −1
7 2
5
.
17.
−1
2 3
0
1 4
2 −1 3
.
18.
2 −1 3
5
2 1
3 −3 7
.
19.
0 −2
1
2
0 −3
−1
3
0
.
20.
1 0 −1
0
0 1
0 −1
−1 0 −1
0
0 1
0
1
.

3.3
Cofactor Expansions 233
21.
2 −1
3 1
1
4 −2 3
0
2 −1 0
1
3 −2 4
.
22.
−4 1 −3 −2
1 2 −1 −6
3 0
2
3
0 0 −5
2
.
For Problems 23–28, use elementary row operations together
with the Cofactor Expansion Theorem to evaluate the given
determinant.
23.
−1
3
3
4 −6 −3
2 −1
4
.
24.
3
5
2
6
2
3
5
−5
7
5 −3 −16
9 −6
27 −12
.
25.
2 −7
4 3
5
5 −3 7
6
2
6 3
4
2 −4 5
.
26.
−2 0 1 1
1 2 2 0
−4 4 6 1
−1 1 0 5
.
27.
−3 2 0 −1 −4
1 3 1
3
5
0 2 1 −3 −1
2 0 4 −2 −3
−1 2 6
0 −1
.
28.
2 0 −1
3 0
0 3
0
1 2
0 1
3
0 4
1 0
1 −1 0
3 0
2
0 5
.
29. If A =
⎡
⎢⎢⎣
0
x
y
z
−x
0
1 −1
−y
−1
0
1
−z
1 −1
0
⎤
⎥⎥⎦, show that det(A) =
(x + y + z)2.
30. (a) Consider the 3 × 3 Vandermonde determinant
V (r1,r2,r3) deﬁned by
V (r1,r2,r3) =
1
1
1
r1
r2
r3
r2
1
r2
2
r2
3
.
Show that
V (r1,r2,r3) = (r2 −r1)(r3 −r1)(r3 −r2).
(b) More generally, show that the n×n Vandermonde
determinant
V (r1,r2, . . . ,rn) =
1
1
. . .
1
r1
r2
. . .
rn
r2
1
r2
2
. . .
r2
n
...
...
...
rn−1
1
rn−1
2
. . .
rn−1
n
has value
V (r1,r2, . . . ,rn) =
1
1≤i<m≤n
(rm −ri).
For Problems 31–38, determine the eigenvalues of the
given matrix A. That is, determine the scalars λ such that
det(A −λI) = 0.
31. A =
! 2 −1
2
4
"
.
32. A =
! 2
4
3 13
"
.
33. A =
! −1 2
−4 7
"
.
34. A =
⎡
⎣
2
0 0
−1 −6 0
3
3 7
⎤
⎦.
35. A =
⎡
⎣
2 0 0
7 7 7
7 7 7
⎤
⎦.
36. A =
⎡
⎣
6 0 −2
0 7
0
−5 0 −3
⎤
⎦.
37. A =
⎡
⎣
−5 −5 0
−8
1 0
−5
3 7
⎤
⎦.

234
CHAPTER 3
Determinants
38. A =
⎡
⎣
−5 −1 1
−2 −1 0
−5
2 3
⎤
⎦.
For Problems 39–48, ﬁnd (a) det(A), (b) the matrix of co-
factors MC, (c) adj(A), and, if possible, (d) A−1.
39. A =
% 3 1
4 5
&
.
40. A =
% −1 −2
4
1
&
.
41. A =
%
5
2
−15 −6
&
.
42. A =
⎡
⎣
2 −3 0
2
1 5
0 −1 2
⎤
⎦.
43. A =
⎡
⎣
−2 3 −1
2 1
5
0 2
3
⎤
⎦.
44. A =
⎡
⎣
1 −1 2
3 −1 4
5
1 7
⎤
⎦.
45. A =
⎡
⎣
0
1 2
−1 −1 3
1 −2 1
⎤
⎦.
46. A =
⎡
⎣
2 −3
5
1
2
1
0
7 −1
⎤
⎦.
47. A =
⎡
⎢⎢⎣
1 1
1
1
−1 1 −1
1
1 1 −1 −1
−1 1
1 −1
⎤
⎥⎥⎦.
48. A =
⎡
⎢⎢⎣
1 0 3
5
−2 1 1
3
3 9 0
2
2 0 3 −1
⎤
⎥⎥⎦.
49. Let A =
⎡
⎣
1
−2x
2x2
2x
1 −2x2
−2x
2x2
2x
1
⎤
⎦.
(a) Show that det(A) = (1 + 2x2)3.
(b) Use the adjoint method to ﬁnd A−1.
In Problems 50–53, ﬁnd the speciﬁed element in the inverse
of the given matrix. Do not use elementary row operations.
50. A =
⎡
⎣
−1 −2
4
0
2 −1
3 −2
1
⎤
⎦; (1, 1)-element.
51. A =
⎡
⎣
1 1 1
1 2 2
1 2 3
⎤
⎦; (3, 2)-element.
52. A =
⎡
⎣
2
0 −1
2
1
1
3 −1
0
⎤
⎦; (3, 1)-element.
53. A =
⎡
⎢⎢⎣
1
0
1 0
2 −1
1 3
0
1 −1 2
−1
1
2 0
⎤
⎥⎥⎦; (2, 3)-element.
In Problems 54–57, ﬁnd A−1.
54. A =
% et sin 2t
−e−t cos 2t
et cos 2t
e−t sin 2t
&
.
55. A =
% 3et
e2t
2et
2e2t
&
.
56. A =
⎡
⎣
e3t
9te3t
−e−2t
−te3t
e3t
e−2t
−te3t
e3t
0
⎤
⎦.
57. A =
⎡
⎣
et
tet
e−2t
et
2tet
e−2t
et
tet
2e−2t
⎤
⎦.
58. If A =
⎡
⎣
1 2 3
3 4 5
4 5 6
⎤
⎦, compute the matrix product
A · adj(A). What can you conclude about det(A)?
For Problems 59–64, use Cramer’s rule to solve the given
linear system.
59. 2x1 −3x2 = 2,
x1 + 2x2 = 4.
60.
x1 + 5x2 =
1,
−3x1 + 6x2 = −4.
61.
−2x1 + 4x2 + x3 = −5,
3x1 −2x2 −x3 =
2,
4x1 −3x2 + 2x3 =
1.
62.
3x1 −2x2 + x3 = 4,
x1 + x2 −x3 = 2,
x1 +
x3 = 1.

3.4
Summary of Determinants 235
63.
x1 −3x2 + x3 = 0,
x1 + 4x2 −x3 = 0,
2x1 + x2 −3x3 = 0.
64.
x1 −2x2 + 3x3 −x4 = 1,
2x1 +
x3
= 2,
x1 + x2 −
x4 = 0,
x2 −2x3 + x4 = 3.
65. Use Cramer’s rule to determine x1 and x2 if
etx1 + e−2tx2 = 3 sin t,
etx1 −2e−2tx2 = 4 cos t.
66. Determine the value of x2 such that
x1 + 4x2 −2x3 + x4 = 2,
2x1 + 9x2 −3x3 −2x4 = 5,
x1 + 5x2 +
x3 −x4 = 3,
3x1 + 14x2 + 7x3 −2x4 = 6.
67. Determine the value of x4 such that
−3x1 + x2 −3x3 −9x4 = −3,
x1 −2x2
+ 4x4 =
1,
2x3 + x4 = −1,
x1 + x2
+ x4 =
0.
68. Find all solutions to the system
(b + c)x1 + a(x2 + x3) = a,
(c + a)x2 + b(x3 + x1) = b,
(a + b)x3 + c(x1 + x2) = c,
where a, b, c are constants. Make sure you consider all
cases (that is, those when there is a unique solution,
an inﬁnite number of solutions, and no solutions).
69. Prove Equation (3.3.3).
70. ⋄Let A be a randomly generated invertible 4 × 4 ma-
trix. Verify the cofactor expansion theorem for expan-
sion along row 1.
71. ⋄Let A be a randomly generated 4 × 4 matrix. Verify
Equation (3.3.3) when i = 2 and j = 4.
72. ⋄Let A be a randomly generated 5 × 5 matrix. Deter-
mine adj(A) and compute A · adj(A). Use your result
to determine det(A).
73. ⋄Solve the system of equations
1.21x1 + 3.42x2 + 2.15x3 = 3.25,
5.41x1 + 2.32x2 + 7.15x3 = 4.61,
21.63x1 + 3.51x2 + 9.22x3 = 9.93.
Round answers to two decimal places.
74. ⋄Use Cramer’s rule to solve the system Ax = b if
A =
⎡
⎢⎢⎢⎢⎣
1 2 3 4 4
2 1 2 3 4
3 2 1 2 3
4 3 2 1 2
4 4 3 2 1
⎤
⎥⎥⎥⎥⎦
and b =
⎡
⎢⎢⎢⎢⎣
68
−72
−87
79
43
⎤
⎥⎥⎥⎥⎦
.
75. Verify that B A = In in the proof of Theorem 3.3.18.
3.4
Summary of Determinants
The primary aim of this section is to serve as a stand-alone introduction to determinants
for readers who desire only a cursory review of the major facts pertaining to determinants.
Formulas for the Determinant
The determinant of an n × n matrix A, denoted det(A), is a scalar whose value can be
obtained in the following manner.
1. If A = [a11], then det(A) = a11.
2. If A =
! a11
a12
a21
a22
"
, then det(A) = a11a22 −a12a21.

236
CHAPTER 3
Determinants
3. For n > 2, the determinant of A can be computed using either of the following
formulas:
det(A) = ai1Ci1 + ai2Ci2 + · · · + ainCin,
(3.4.1)
det(A) = a1 jC1 j + a2 jC2 j + · · · + anjCnj,
(3.4.2)
where Ci j = (−1)i+ j Mi j, and Mi j is the determinant of the matrix obtained by deleting
the ith row and jth column of A. The formulas (3.4.1) and (3.4.2) are referred to as
cofactor expansion along the ith row and cofactor expansion along the jth column,
respectively. The determinants Mi j and Ci j are called the minors and cofactors of A,
respectively. We also denote det(A) by
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
an1
an2
. . .
ann
.
As an example, consider the general 3 × 3 matrix A =
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦. Using
cofactor expansion along row 1, we have
det(A) = a11C11 + a12C12 + a13C13.
(3.4.3)
We next compute the required cofactors:
C11 = +M11 = a22
a23
a32
a33 = a22a33 −a23a32,
C12 = −M12 = −a21
a23
a31
a33 = −(a21a33 −a23a31),
C13 = +M13 = a21 a22
a31 a32 = a21a32 −a22a31.
Inserting these expressions for the cofactors into Equation (3.4.3) yields
det(A) = a11(a22a33 −a23a32) −a12(a21a33 −a23a31) + a13(a21a32 −a22a31),
which can be written as
det(A) = a11a22a33 + a12a23a31 + a13a21a32 −a11a23a32 −a12a21a33 −a13a22a31.
Although we chose to use cofactor expansion along the ﬁrst row to obtain the preceding
formula, according to (3.4.1) and (3.4.2), the same result would have been obtained if we
had chosen to expand along any row or column of A. A simple schematic for obtaining
the terms in the determinant of a 3 × 3 matrix is given in Figure 3.4.1. By taking the
product of the elements joined by each arrow and attaching the indicated sign to the
result, we obtain the six terms in the determinant of the 3 × 3 matrix A = [ai j]. Note
that this technique for obtaining the terms in a 3 × 3 determinant does not generalize to
determinants of larger matrices.
a11
a12
a13
a11
a12
a21
a22
a23
a21
a22
a31
a32
a33
a31
a32
2
2
2
1
1
1
Figure 3.4.1: A schematic for
obtaining the determinant of a 3 × 3
matrix A = [ai j].

3.4
Summary of Determinants 237
Example 3.4.1
Evaluate
6
7 −3
1 −1
2
5 −2
5
.
Solution:
In this case, the schematic given in Figure 3.4.1 is
6
7 −3 6
7
1 −1
2 1 −1
5 −2
5 5 −2
so that
6
7 −3
1 −1
2
5 −2
5
= (6)(−1)(5) + (7)(2)(5) + (−3)(1)(−2) −(5)(−1)(−3)
−(−2)(2)(6) −(5)(1)(7)
= 20.
□
Properties of Determinants
Let A and B be n × n matrices. The determinant has the following properties:
P1. If B is obtained by permuting two rows (or columns) of A, then
det(B) = −det(A).
P2. If B is obtained by multiplying any row (or column) of A by a scalar k, then
det(B) = k det(A).
P3. If B is obtained by adding a multiple of any row (or column) of A to another row
(or column) of A, then
det(B) = det(A).
P4. For any scalar k, we have
det(k A) = kndet(A).
P5. We have
det(AT ) = det(A).
P6. Let a1, a2, . . . , an denote the row vectors of A. If the ith row vector of A is the
sum of two row vectors, say ai = bi + ci, then
det(A) = det(B) + det(C),
where
B = [a1, a2, . . . , ai−1, bi, ai+1, . . . , an]T
and
C = [a1, a2, . . . , ai−1, ci, ai+1, . . . , an]T .
The corresponding property for columns is also true.
P7. If A has a row (or column) of zeros, then det(A) = 0.
P8. If two rows (or columns) of A are scalar multiples of one another, then det(A) = 0.

238
CHAPTER 3
Determinants
P9. We have
det(AB) = det(A)det(B).
P10. If A is an invertible matrix, then det(A) ̸= 0 and
det(A−1) =
1
det(A).
The ﬁrst three properties tell us how elementary row operations performed on a
matrix A alter the value of det(A). They can be very helpful in reducing the amount of
work required to evaluate a determinant, since we can use elementary row operations to
put several zeros in a row or column of A and then use cofactor expansion along that
row or column. We illustrate with an example.
Example 3.4.2
Evaluate
2 1
3 2
−1 1 −2 2
5 1 −2 1
−2 3
1 1
.
Solution:
Before performing a cofactor expansion, we ﬁrst use elementary row op-
erations to simplify the determinant:
⎡
⎢⎢⎣
2 1
3 2
−1 1 −2 2
5 1 −2 1
−2 3
1 1
⎤
⎥⎥⎦
1∼
⎡
⎢⎢⎣
0 3
−1
6
−1 1
−2
2
0 6 −12
11
0 1
5 −3
⎤
⎥⎥⎦
According to P3, the determinants of the two matrices above are the same. To evaluate
the determinant of the matrix on the right, we use cofactor expansion along the ﬁrst
column.
0 3
−1
6
−1 1
−2
2
0 6 −12
11
0 1
5 −3
= −(−1)
3
−1
6
6 −12
11
1
5 −3
To evaluate the determinant of the 3 × 3 matrix on the right, we can use the schematic
given in Figure 3.4.1, or alternatively, we can continue to use elementary row operations
to introduce zeros into the matrix:
3
−1
6
6 −12
11
1
5 −3
2=
0 −16
15
0 −42
29
1
5 −3
= −16 15
−42 29 = 166.
Here, we have reduced the 3 × 3 determinant to a 2 × 2 determinant by using cofactor
expansion along the ﬁrst column of the 3 × 3 matrix.
1. A21(2), A23(5), A24(−2)
2. A31(−3), A32(−6).
□

3.4
Summary of Determinants 239
Basic Theoretical Results
The determinant is a useful theoretical tool in linear algebra. We list next the major
results that will be needed in the remainder of the text.
1. The volume of the parallelepiped determined by the vectors
a = a1i + a2j + a3k,
b = b1i + b2j + b3k,
c = c1i + c2j + c3k
in 3-space is
Volume = |det(A)|,
where A =
⎡
⎣
a1
a2
a3
b1
b2
b3
c1
c2
c3
⎤
⎦. Thus, the vectors a, b, and c lie in a common plane
(thus determining a parallelepiped of volume zero) if and only if det(A) = 0.
2. An n × n matrix is invertible if and only if det(A) ̸= 0.
3. An n × n linear system Ax = b has a unique solution if and only if det(A) ̸= 0.
4. An n × n homogeneous linear system Ax = 0 has an inﬁnite number of solutions
if and only if det(A) = 0.
We see, for example, that according to (2), the matrices in Examples 3.4.1 and 3.4.2 are
both invertible.
If A is an n × n matrix with det(A) ̸= 0, then the following two methods can be
derived for obtaining the inverse of A and for ﬁnding the unique solution to the linear
system Ax = b, respectively.
1. Adjoint Method for A−1: If A is invertible, then
A−1 =
1
det(A)adj(A),
where adj(A) denotes the transpose of the matrix obtained by replacing each element in
A by its cofactor.
2. Cramer’s Rule: If det(A) ̸= 0, then the unique solution to Ax = b is x =
(x1, x2, . . . , xn), where
xk = det(Bk)
det(A) ,
k = 1, 2, . . . , n,
and Bk denotes the matrix obtained when the kth column vector of A is replaced by b.
Example 3.4.3
Use the adjoint method to ﬁnd A−1 if A =
⎡
⎣
6
7 −3
1 −1
2
5 −2
5
⎤
⎦.
Solution:
We have already shown in Example 3.4.1 that det(A) = 20, so that A is
invertible. Replacing each element in A with its cofactor yields the matrix of cofactors
MC =
⎡
⎣
−1
5
3
−29
45
47
11 −15 −13
⎤
⎦,

240
CHAPTER 3
Determinants
so that
adj(A) = MT
C =
⎡
⎣
−1 −29
11
5
45 −15
3
47 −13
⎤
⎦.
Consequently,
A−1 =
1
det(A)adj(A) =
⎡
⎢⎢⎣
−1
20
−29
20
11
20
1
4
9
4
−3
4
3
20
47
20
−13
20
⎤
⎥⎥⎦.
□
Example 3.4.4
Use Cramer’s rule to solve the linear system
6x1 + 7x2 −3x3 = −1,
x1 −x2 + 2x3 =
0,
5x1 −2x2 + 5x3 =
8.
Solution:
The matrix of coefﬁcients is A =
⎡
⎣
6
7 −3
1 −1
2
5 −2
5
⎤
⎦. We have already shown
in Example 3.4.1 that det(A) = 20. Consequently, Cramer’s rule can indeed be applied.
In this problem, we have
det(B1) =
−1
7 −3
0 −1
2
8 −2
5
= 89,
det(B2) =
6 −1 −3
1
0
2
5
8
5
= −125,
det(B3) =
6
7 −1
1 −1
0
5 −2
8
= −107.
It therefore follows from Cramer’s rule that
x1 = det(B1)
det(A) = 89
20,
x2 = det(B2)
det(A) = −125
20 = −25
4 ,
x3 = det(B3)
det(A) = −107
20 .
□
Exercises for 3.4
Skills
• Be able to compute the determinant of an n×n matrix.
• Know the properties P1–P10 of determinants.
• Be able to compute the matrix of cofactors and the
adjoint of a given matrix.
• Be able to compute the inverse of a matrix by using
the adjoint.
• Be able to use Cramer’s rule to solve a system of linear
equations.
Problems
For Problems 1–8, evaluate the given determinant.
1.
−3 .
2.
5 −1
3
7 .

3.4
Summary of Determinants 241
3.
3 5
7
−1 2
4
6 3 −2
.
4.
5 1 4
6 1 3
14 2 7
.
5.
2.3 1.5 7.9
4.2 3.3 5.1
6.8 3.6 5.7
.
6.
a
b
c
b
c
a
c
a
b
.
7.
3
5 −1 2
2
1
5 2
3
2
5 7
1 −1
2 1
.
8.
7
1
2
3
2 −2
4
6
3 −1
5
4
18
9 27 54
.
For Problems 9–14, ﬁnd det(A). If A is invertible, use the
adjoint method to ﬁnd A−1.
9. A =
! 3 5
2 7
"
.
10. A =
⎡
⎣
1 2 3
2 3 1
3 1 2
⎤
⎦.
11. A =
⎡
⎣
3
4
7
2
6
1
3 14 −1
⎤
⎦.
12. A =
⎡
⎣
2
5
7
4 −3
2
6
9 11
⎤
⎦.
13. A =
⎡
⎢⎢⎣
5 −1
2 1
3 −1
4 5
1 −1
2 1
5
9 −3 2
⎤
⎥⎥⎦.
14. A =
⎡
⎢⎢⎣
−1 0
0
4
1 1
0 −2
6 0 −1 −2
−3 1
3
2
⎤
⎥⎥⎦.
For Problems 15–20, use Cramer’s rule to determine the
unique solution for x to the system Ax = b for the given
matrix A and vector b.
15. A =
!
2 8
−2 4
"
, b =
!
0
−3
"
.
16. A =
! 3 5
6 2
"
, b =
! 4
9
"
.
17. A =
! cos t
sin t
sin t
−cos t
"
, b =
! e−t
3e−t
"
.
18. A =
⎡
⎣
4
1 3
2 −1 5
2
3 1
⎤
⎦, b =
⎡
⎣
5
7
2
⎤
⎦.
19. A =
⎡
⎣
5 3
6
2 4 −7
2 5
9
⎤
⎦, b =
⎡
⎣
3
−1
4
⎤
⎦.
20. A =
⎡
⎣
3.1 3.5 7.1
2.2 5.2 6.3
1.4 8.1 0.9
⎤
⎦, b =
⎡
⎣
3.6
2.5
9.3
⎤
⎦.
21. If A is an invertible n × n matrix, prove property P9:
det(A−1) =
1
det(A).
22. If A is an arbitrary 3×3 matrix, use cofactor expansion
to show that property P5 holds:
det(AT ) = det(A).
For Problems 23–29, assume that A and B be 3×3 matrices
with det(A) = 3 and det(B) = −4. Compute the speciﬁed
determinant.
23. det(2A)
24. det(A−1)
25. det(AT B)
26. det(B5)
27. det(B−1AB)2
28. det(C), where C is obtained from matrix B by inter-
changing the last two columns and multiplying the ﬁrst
column by 4.
29. det(C), where C is obtained from matrix A by multi-
plying the second row of A by 3, and adding the ﬁrst
row of A to the last row of A.

242
CHAPTER 3
Determinants
3.5
Chapter Review
This chapter has laid out a basic introduction to the theory of determinants.
Determinants and Elementary Row Operations
For a square matrix A, one approach for computing the determinant of A, det(A), is
to use elementary row operations to reduce A to row-echelon form. The effects of the
various types of elementary row operations on det(A) are as follows:
• Pi j: permuting two rows of A alters the determinant by a factor of −1.
• Mi(k): multiplying the ith row of A by k multiplies the determinant of the matrix
by a factor of k.
• Ai j(k): adding a multiple of one row of A to another has no effect whatsoever on
det(A).
A crucial fact in this approach is the following:
Theorem 3.5.1
If A is an n × n upper (or lower) triangular matrix, its determinant is
det(A) = a11a22 · · · ann.
Therefore, since the row-echelon form of A is upper triangular, we can compute det(A)
by using Theorem 3.5.1 and by keeping track of the elementary row operations involved
in the row reduction process.
Cofactor Expansion
Another way to compute det(A) is via the Cofactor Expansion Theorem: For n ≥2, the
determinant of A can be computed using either of the following formulas:
det(A) = ai1Ci1 + ai2Ci2 + · · · + ainCin,
(3.5.1)
det(A) = a1 jC1 j + a2 jC2 j + · · · + anjCnj,
(3.5.2)
where Ci j = (−1)i+ j Mi j, and Mi j is the determinant of the matrix obtained by deleting
the ith row and jth column of A. The formulas (3.5.1) and (3.5.2) are referred to as
cofactor expansion along the ith row, and cofactor expansion along the jth column,
respectively. The determinants Mi j and Ci j are called the minors and cofactors of A,
respectively.
Adjoint Method and Cramer’s Rule
If A is an n × n matrix with det(A) ̸= 0, then the following two methods can be derived
for obtaining the inverse of A, and for ﬁnding the unique solution to the linear system
Ax = b, respectively.
1. Adjoint Method for A−1: If A is invertible, then
A−1 =
1
det(A)adj(A),
where adj(A) denotes the transpose of the matrix obtained by replacing each ele-
ment in A by its cofactor.

3.5
Chapter Review 243
2. Cramer’s Rule: If det(A) ̸= 0, then the unique solution to Ax = b is x =
(x1, x2, . . . , xn), where
xk = det(Bk)
det(A) ,
k = 1, 2, . . . , n,
and Bk denotes the matrix obtained when the kth column vector of A is replaced
by b.
Additional Problems
For Problems 1–6, evaluate the determinant of the given ma-
trix A by using (a) Deﬁnition 3.1.8, (b) elementary row op-
erations to reduce A to an upper triangular matrix, and (c)
the Cofactor Expansion Theorem.
1. A =
!
6 6
−2 1
"
.
2. A =
! −7 −2
1 −5
"
.
3. A =
⎡
⎣
2
3 −5
−4
0
2
6 −3
3
⎤
⎦.
4. A =
⎡
⎣
−1 4
1
0 2
2
2 2 −3
⎤
⎦.
5. A =
⎡
⎢⎢⎣
0
0
0 −2
0
0 −5
1
0
1 −4
1
−3 −3 −3 −3
⎤
⎥⎥⎦.
6. A =
⎡
⎢⎢⎣
3 −1 −2
1
0
0
1
4
0
2
1 −1
0
0
0 −4
⎤
⎥⎥⎦.
For Problems 7–10, suppose that
A =
⎡
⎣
a
b
c
d
e
f
g
h
i
⎤
⎦and det(A) = 4.
Compute the determinant of each matrix below.
7.
⎡
⎣
a −5d
b −5e
c −5 f
3g
3h
3i
−d + 3g
−e + 3h
−f + 3i
⎤
⎦.
8.
⎡
⎣
g
h
i
−4a
−4b
−4c
2d
2e
2 f
⎤
⎦.
9. 3 ·
⎡
⎣
a −d
b −e c −f
2g
2h
2i
−d
−e
−f
⎤
⎦.
10.
⎡
⎣
3b
3e
3h
c −2a
f −2d
i −2g
−a
−d
−g
⎤
⎦.
For Problems 11–14, suppose that A and B are 4 × 4 in-
vertible matrices. If det(A) = −2 and det(B) = 3, compute
each determinant below.
11. det(B2 A−1).
12. det(AB).
13. det((−A)3(2B2)).
14. det(((A−1B)T )(2B−1)).
For Problems 15–26, let
A =
! 1 2 −1
2 1
4
"
,
B =
⎡
⎣
2
1
5 −2
4
7
⎤
⎦,
C =
⎡
⎣
1
0 5
3 −1 4
2 −2 6
⎤
⎦.
Compute the determinants, where possible.
15. det(A)
16. det(B)
17. det(C)
18. det(CT )
19. det(AB)
20. det(B A)
21. det(BT AT )

244
CHAPTER 3
Determinants
22. det(B AC)
23. det(AC B)
24. det((AAT )2)
25. det(C−1B A)
26. det(BCCT )
27. Let A =
!1 2
3 4
"
, and let B =
!5 4
1 1
"
. Use the adjoint
method to ﬁnd B−1 and then determine (A−1BT )−1.
For Problems 28–32, use the adjoint method to determine
A−1 for the given matrix A.
28. A =
⎡
⎣
2 −1
1
0
5 −1
1
1
3
⎤
⎦.
29. A =
⎡
⎢⎢⎣
0 −3 2
2
0
1 1
1
1
2 3 −4
1
0 0
5
⎤
⎥⎥⎦.
30. A =
⎡
⎢⎢⎣
0
0
0
1
0
1
3 −3
−2 −3 −5
2
4 −4
4
6
⎤
⎥⎥⎦.
31. A =
⎡
⎣
5
8
16
4
1
8
−4 −4 −11
⎤
⎦.
32. A =
⎡
⎣
2 6 6
2 7 6
2 7 7
⎤
⎦.
33. Add one row to the matrix A =
! 4 −1 0
5
1 4
"
so as to
create a 3 × 3 matrix B with det(B) = 10.
34. True or False: Given any real number r and any 3×3
matrix A whose entries are all nonzero, it is always
possible to change at most one entry of A to get a
matrix B with det(B) = r.
35. Let A =
⎡
⎣
1 2 4
3 1 6
k
3 2
⎤
⎦.
(a) Find all value(s) of k for which the matrix A fails to
be invertible.
(b) In terms of k, determine the volume of the paral-
lelepiped determined by the row vectors of the matrix
A. Is that the same as the volume of the parallelepiped
determined by the column vectors of the matrix A?
Explain how you know this without any calculation.
36. Repeat the preceding problem for the matrix
A =
⎡
⎣
k + 1 2 1
0
3 k
1
1 1
⎤
⎦.
37. Repeat the preceding problem for the matrix
A =
⎡
⎣
2 k −3 k2
2
1
4
1
k
0
⎤
⎦.
38. Let A and B be n ×n matrices such that AB = −B A.
Use determinants to prove that if n is odd, then A and
B cannot both be invertible.
39. A real n × n matrix A is called orthogonal if AAT =
AT A = In. If A is an orthogonal matrix prove that
det(A) = ±1.
For Problems 40–42, use Cramer’s rule to solve the given
linear system.
40. −3x1 + x2 = 3,
x1 + 2x2 = 1.
41.
2x1 −x2 + x3 = 2,
4x1 + 5x2 + 3x3 = 0,
4x1 −3x2 + 3x3 = 2.
42.
3x1 + x2 + 2x3 = −1,
2x1 −x2 + x3 = −1,
5x2 + 5x3 = −5.
Project: Volume of a Tetrahedron
In this project, we use determinants and vectors to derive the formula for the volume
of a tetrahedron with vertices A = (x1, y1, z1), B = (x2, y2, z2), C = (x3, y3, z3), and
D = (x4, y4, z4).

3.5
Chapter Review 245
Let h denote the distance from A to the plane determined by B, C, and D. From
geometry, the volume of the tetrahedron is given by
Volume = 1
3h(Area of Triangle BC D).
(3.5.3)
(a) Express the area of triangle BC D in terms of a cross product of vectors.
(b) Use trigonometry to express h in terms of the distance from A to B and the angle
between the vector
→
AB and the segment connecting A to the base BC D at a right
angle.
(c) Combining (a) and (b) with the volume of the tetrahedron given above, express the
volume of the tetrahedron in terms of dot products and cross products of vectors.
(d) Following the proof of part (2) of Theorem 3.1.12, express the volume of the
tetrahedron in terms of a determinant with entries in terms of the xi, yi, and zi for
1 ≤i ≤4.
(e) Show that the expression in part (d) is the same as
Volume = 1
6
x1
y1
z1
1
x2
y2
z2
1
x3
y3
z3
1
x4
y4
z4
1
.
(3.5.4)
(f) For each set of four points below, determine the volume of the tetrahedron with
those points as vertices by using (3.5.3) and by using (3.5.4). Both formulas should
yield the same answer.
(i) (0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1).
(ii) (−1, 1, 2), (0, 3, 3), (1, −1, 2), (0, 0, 1).

4
Vector Spaces
To motivate what we will be discussing in the next several chapters, let us return once
more to the familiar problem of solving a differential equation. Suppose we wish to ﬁnd
solutions to the differential equation
y′ + 2y = 0,
(4.0.1)
for example. This ﬁrst-order linear differential equation can be solved via methods in
Chapter 1, and the result is that every solution to (4.0.1) can be expressed in the form
y(x) = ce−2x, for some constant c. That is, each solution is a scalar multiple of the basic
solution y1(x) = e−2x. We will observe a similar phenomenon later in Chapter 8 when
we establish that every solution to the homogeneous second-order differential equation
y′′ + a1y′ + a2y = 0
can be written in the form
y(x) = c1y1(x) + c2y2(x),
where y1(x) and y2(x) are any two nonproportional solutions to the differential equation
on the interval of interest. Whenever we study linear problems such as this, it is possible
to express all solutions of the problem in terms of a privileged set of basic solutions.
As another illustration, the set of solutions to a system of linear equations, as studied
in Chapter 2, can also be expressed in terms of a ﬁnite list of particular solutions. Suppose,
for instance, that we consider the homogeneous linear system Ax = 0, where
A =
⎡
⎣
1 −1 2
2 −2 4
3 −3 6
⎤
⎦.
246

247
It is straightforward to show that this system has solution set
S = {(r −2s,r, s) : r, s ∈R}.
Geometrically, we can interpret each solution as deﬁning the coordinates of a point in
space or, equivalently, as the geometric vector with components
v = (r −2s,r, s).
Using the standard operations of vector addition and multiplication of a vector by a real
number, it follows that v can be written in the form
v = r(1, 1, 0) + s(−2, 0, 1).
We see that every solution to the given linear problem can be expressed as a linear
combination of the two basic solutions (see Figure 4.0.1):
v1 = (1, 1, 0)
and
v2 = (−2, 0, 1).
x3
v2 5 (22, 0, 1)
x2
x1
v1 5 (1, 1, 0)
v 5 rv1 + sv2
Figure 4.0.1: Two basic solutions to Ax = 0 and an example of an arbitrary solution to
the system.
The theory underlying the solution to a linear differential equation and the theory
underlying the solution of a system of linear equations can both be considered as special
cases of a general mathematical framework for linear problems. In both of these illus-
trations, we have a set of “vectors” V (in the case of linear differential equations, the
vectors are sufﬁciently differentiable functions, whereas in the case of linear systems of
equations, the vectors consist of n-tuples of real or complex numbers). In both cases,
all solutions to the given problem can be expressed in terms of some particular vectors
from V .
In the next few chapters, we develop this way of formulating linear problems in
terms of an abstract set of vectors, V , and a linear vector equation with solutions in
V . We will ﬁnd that many problems ﬁt into this framework and that the solutions to
these problems can be expressed as linear combinations of a certain number of basic
solutions. The importance of this result cannot be overemphasized. It reduces the search
for all solutions to a given problem to that of ﬁnding a ﬁnite number of solutions. As
speciﬁc applications, we will derive the theory underlying linear differential equations
and linear systems of differential equations as special cases of the general framework.
Before proceeding any further, we give a word of encouragement to the more applied
oriented reader. It will probably seem at times that the ideas we are introducing are rather
esoteric and that the formalism is pure mathematical abstraction. However, in addition
to the inherent mathematical beauty of the formalism, the ideas that it incorporates
pervade many areas of applied mathematics, particularly engineering mathematics and

248
CHAPTER 4
Vector Spaces
mathematical physics, where the problems under investigation are very often linear
in nature. Indeed, the linear algebra introduced in the next four chapters should be
considered an extremely important addition to one’s mathematical repertoire, certainly
on a par with the ideas of elementary calculus.
4.1
Vectors in Rn
In this section, we use some familiar ideas about geometric vectors from elementary
calculus to motivate the more general and abstract idea of a vector space, which will
be introduced in the next section. We begin here by recalling that a geometric vector
can be considered mathematically as a directed line segment (or arrow) that has both a
magnitude (length) and a direction attached to it. In calculus courses, we deﬁne vector
addition according to the parallelogram law (see Figure 4.1.1); namely, the sum of the
vectors x and y is the diagonal of the parallelogram formed by x and y. We denote the
sum by x + y. It can then be shown geometrically that for all vectors x, y, z,
x
y
x 1 y
Figure 4.1.1: Parallelogram law
of vector addition.
x + y = y + x
(4.1.1)
and
x + (y + z) = (x + y) + z.
(4.1.2)
These are the statements that the vector addition operation is commutative and associa-
tive. The zero vector, denoted 0, is deﬁned as the vector satisfying
x + 0 = x,
(4.1.3)
for all vectors x. We consider the zero vector as having zero magnitude and arbitrary
direction. Geometrically, we picture the zero vector as corresponding to a point in space.
Let −x denote the vector that has the same magnitude as x, but the opposite direction.
Then according to the parallelogram law of addition,
x + (−x) = 0.
(4.1.4)
The vector −x is called the additive inverse of x. Properties (4.1.1)–(4.1.4) are the
fundamental properties of vector addition.
Next, we deﬁne the operation of multiplication of a vector by a real number. Ge-
ometrically, if x is a vector and k is a real number, then kx is deﬁned to be the vector
whose magnitude is |k| times the magnitude of x and whose direction is the same as x if
k > 0, and opposite to x if k < 0. (See Figure 4.1.2.) If k = 0, then kx = 0. This scalar
multiplication operation has several important properties that we now list. Once more,
each of these can be established geometrically using only the foregoing deﬁnitions of
vector addition and scalar multiplication.
x
kx, k . 0
kx, k , 0
Figure 4.1.2: Scalar
multiplication of x by k.
For all vectors x and y, and all real numbers r, s, and t,
1x = x,
(4.1.5)
(st)x = s(tx),
(4.1.6)
r(x + y) = rx + ry,
(4.1.7)
(s + t)x = sx + tx.
(4.1.8)
It is important to realize that, in the foregoing development, we have not deﬁned
a “multiplication of vectors”. In Chapter 3 we discussed the idea of a dot product and
cross product of two vectors in space (see Equations (3.1.4) and (3.1.5)), but for the

4.1
Vectors in Rn
249
purposes of discussing abstract vector spaces we will essentially ignore the dot product
andcrossproduct.WewillrevisitthedotproductlaterinChapter5whenwedevelopinner
product spaces.
We will see in the next section how the concept of a vector space arises as a direct
generalization of the ideas associated with geometric vectors. Before performing this
abstraction, we want to recall some further features of geometric vectors and give one
speciﬁc and important extension.
We begin by considering vectors in the plane. Recall that R2 denotes the set of all
ordered pairs of real numbers; thus,
R2 = {(x, y) : x ∈R, y ∈R}.
The elements of this set are called vectors in R2, and we use the usual vector notation
to denote these elements. Geometrically we identify the vector v = (x, y) in R2 with
the geometric vector v directed from the origin of a Cartesian coordinate system to
the point with coordinates (x, y). This identiﬁcation is illustrated in Figure 4.1.3. The
numbers x and y are called the components of the geometric vector v. The geometric
vector addition and scalar multiplication operations are consistent with the addition and
scalar multiplication operations deﬁned in Chapter 2 via the correspondence with row
(or column) vectors for R2:
(x, y)
v
x
y
(x, 0)
(0, y)
Figure 4.1.3: Identifying vectors
in R2 with geometric vectors in
the plane.
If v = (x1, y1) and w = (x2, y2), and k is an arbitrary real number, then
v + w = (x1, y1) + (x2, y2) = (x1 + x2, y1 + y2),
(4.1.9)
kv = k(x1, y1) = (kx1, ky1).
(4.1.10)
These are the algebraic statements of the parallelogram law of vector addition and the
scalar multiplication law, respectively. (See Figure 4.1.4.)
v
w
x
y
kv
(x2, y2)
(x1, y1)
(kx1, ky1)
v 1 w
(x1 1 x2, y1 1 y2)
Figure 4.1.4: Vector addition and scalar multiplication in R2.
Using Equations (4.1.9) and (4.1.10), it follows that any vector v = (x, y) can be
written as
v = xi + yj = x(1, 0) + y(0, 1),
where i = (1, 0) and j = (0, 1) are the unit vectors pointing along the positive x- and
y-coordinate axes, respectively.
The properties (4.1.1)–(4.1.8) are now easily veriﬁed for vectors in R2. In particular,
the zero vector in R2 is the vector
0 = (0, 0).
Furthermore, Equation (4.1.9) implies that
(x, y) + (−x, −y) = (0, 0) = 0,
so that the additive inverse of the general vector v = (x, y) is −v = (−x, −y).

250
CHAPTER 4
Vector Spaces
It is straightforward to extend these ideas to vectors in 3-space. We recall that
R3 = {(x, y, z) : x ∈R, y ∈R, z ∈R}
is the set of all ordered triples of real numbers. As illustrated in Figure 4.1.5, each vector
v = (x, y, z) in R3 can be identiﬁed with the geometric vector v that joins the origin of
a Cartesian coordinate system to the point with coordinates (x, y, z). We call x, y, and
z the components of v.
z
y
x
(x, y, z)
(x, 0, 0)
(0, y, 0)
(0, 0, z)
v
(x, y, 0)
Figure 4.1.5: Identifying vectors in R3 with geometric vectors in space.
Extending the formulas (4.1.9) and (4.1.10) to the case v = (x1, y1, z1), w =
(x2, y2, z2), and an arbitrary real number k, we have
v + w = (x1, y1, z1) + (x2, y2, z2) = (x1 + x2, y1 + y2, z1 + z2),
(4.1.11)
kv = k(x1, y1, z1) = (kx1, ky1, kz1).
(4.1.12)
Once more, these are, respectively, the component forms of the laws of vector
addition and scalar multiplication for geometric vectors. It follows that an arbitrary
vector v = (x, y, z) can be written as
v = xi + yj + zk = x(1, 0, 0) + y(0, 1, 0) + z(0, 0, 1),
where i = (1, 0, 0), j = (0, 1, 0), and k = (0, 0, 1) denote the unit vectors which point
along the positive x-, y-, and z-coordinate axes, respectively.
We leave it as an exercise to check that the properties (4.1.1)–(4.1.8) are satisﬁed
by vectors in R3, where
0 = (0, 0, 0),
and the additive inverse of v = (x, y, z) is −v = (−x, −y, −z).
We now come to our ﬁrst major abstraction. Whereas the sets R2 and R3 and their
associated algebraic operations arise naturally from our experience with Cartesian ge-
ometry, the motivation behind the algebraic operations in Rn for larger values of n does
not come from geometry. Rather, we can view the addition and scalar multiplication
operations in Rn for n > 3 as the natural extension of the component forms of addition
and scalar multiplication in R2 and R3 in (4.1.9)–(4.1.12). Therefore, in Rn we have that
if v = (x1, x2, . . . , xn), w = (y1, y2, . . . , yn), and k is an arbitrary real number, then
v + w = (x1 + y1, x2 + y2, . . . , xn + yn),
(4.1.13)
kv = (kx1, kx2, . . . , kxn).
(4.1.14)

4.1
Vectors in Rn
251
The reader should view (4.1.13) and (4.1.14) as deﬁnitions motivated by the alge-
braic deﬁnitions (4.1.9)–(4.1.12) for R2 and R3. However, as we indicated, there is no
geometric perspective on (4.1.13) and (4.1.14) for n > 3.
It is easily established that these operations satisfy properties (4.1.1)–(4.1.8), where
the zero vector in Rn is
0 = (0, 0, . . . , 0),
and the additive inverse of the vector v = (x1, x2, . . . , xn) is
−v = (−x1, −x2, . . . , −xn).
The veriﬁcation of this is left as an exercise.
Example 4.1.1
If v = (−7.1, 2.4, −0.1, 6, −8.3, 5.4) and w = (9.6, −3.3, 4, −8.1, 0, −1.7) are vec-
tors in R6, then
v + w = (−7.1, 2.4, −0.1, 6, −8.3, 5.4) + (9.6, −3.3, 4, −8.1, 0, −1.7)
= (2.5, −0.9, 3.9, −2.1, −8.3, 3.7)
and
−3v = (21.3, −7.2, 0.3, −18, 24.9, −16.2).
□
Exercises for 4.1
Key Terms
Vectors in Rn, Vector addition, Scalar multiplication, Zero
vector, Additive inverse, Components of a vector.
Skills
• Be able to perform vector addition and scalar multi-
plication for vectors in Rn given in component form.
• Understand the geometric perspective on vector ad-
dition and scalar multiplication in the cases of R2
and R3.
• Be able to formally verify the axioms (4.1.1)–(4.1.8)
for vectors in Rn.
True-False Review
For Questions (a)–(l), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The vector (x, y) in R2 is the same as the vector
(x, y, 0) in R3.
(b) Each vector (x, y, z) in R3 has exactly one additive
inverse.
(c) The solution set to a linear system of 4 equations and
6 unknowns consists of a collection of vectors in R6.
(d) For every vector (x1, x2, . . . , xn) in Rn, the vector
(−1) · (x1, x2, . . . , xn) is an additive inverse.
(e) A vector whose components are all positive is called
a “positive vector”.
(f) If s and t are scalars and x and y are vectors in Rn,
then (s + t)(x + y) = sx + ty.
(g) For every vector x in Rn, the vector 0x is the zero
vector of Rn.
(h) The parallelogram whose sides are determined by vec-
tors x and y in R2 have diagonals determined by the
vectors x + y and x −y.
(i) If x is a vector in the ﬁrst quadrant of R2, then any
scalar multiple kx of x is still a vector in the ﬁrst quad-
rant of R2.
(j) The vector 5i −6j +
√
2k in R3 is the same as
(5, −6,
√
2).
(k) Three vectors x, y, and z in R3 always determine a
3-dimensional solid region in R3.
(l) If x and y are vectors in R2 whose components are even
integers and k is a scalar, then x + y and kx are also
vectors in R2 whose components are even integers.

252
CHAPTER 4
Vector Spaces
Problems
1. If x = (−1, −4) and y = (−5, 1), determine the vec-
tors v1 = 3x, v2 = −4y, v3 = 3x+(−4)y. Sketch the
corresponding points in the xy-plane and the equiva-
lent geometric vectors.
2. If x = (3, 1) and y = (−1, 2), determine the vectors
v1 = 2x, v2 = 3y, v3 = 2x + 3y. Sketch the cor-
responding points in the xy-plane and the equivalent
geometric vectors.
3. If x = (5, −2, 9) and y = (−1, 6, 4), determine the
additive inverse of the vector v = −2x + 10y.
4. If x = (3, −1, 2, 5) and y = (−1, 2, 9, −2), deter-
mine v = 5x + (−7)y and its additive inverse.
5. If x = (1, 2, 3, 4, 5) and z = (−1, 0, −4, 1, 2), ﬁnd y
in R5 such that 2x + (−3)y = −z.
6. If x = (−3, 9, 9) and y = (3, 0, −5), ﬁnd a vector z
in R3 such that 4x −y + 2z = 0.
7. If x = (−2 + i, 3i) and y = (5, 2 −2i) in C2, ﬁnd a
vector z in C2 such that (1 + i)x −2y = 2iz.
8. Ifx = (5+i, 0, −1−2i, 1+8i)andy = (−3, i, i, 3)in
C4, ﬁnd a vector z in C4 such that 2x+3iy = (1+i)z.
9. Verify the commutative law of addition for vectors
in R4.
10. Verify the associative law of addition for vectors
in R4.
11. Verify properties (4.1.5)–(4.1.8) for vectors in R3.
12. Verify properties (4.1.5)–(4.1.8) for vectors in R5.
13. Show with examples that if x is a vector in the ﬁrst
quadrant of R2 (i.e., both coordinates of x are posi-
tive) and y is a vector in the third quadrant of R2 (i.e.,
both coordinates of y are negative), then the sum x+y
could occur in any of the four quadrants.
4.2
Definition of a Vector Space
In the previous section, we showed how the set Rn of all ordered n-tuples of real numbers,
together with the addition and scalar multiplication operations deﬁned on it, has the
same algebraic properties as the familiar algebra of geometric vectors. We now push this
abstraction one step further and introduce the idea of a vector space. Such an abstraction
will enable us to develop a mathematical framework for studying a broad class of linear
problems, such as systems of linear equations, linear differential equations, and systems
oflineardifferentialequations,whichhavefarreachingapplicationsinallareasofapplied
mathematics, science and engineering.
Let V be a nonempty set. For our purposes, it is useful to call the elements of V
vectors and use the usual (bold-face) vector notation u, v, . . . to denote these elements.
In handwriting, one should be careful to distinguish vectors from scalars. Two common
ways to do this are to use the symbols
→v or v∼. If V is the set of all 2 × 2 matrices, then
the vectors in V are 2 × 2 matrices, whereas if V is the set of all positive integers, then
the vectors in V are positive integers. We will only be interested in the case when the set
V has an addition operation and a scalar multiplication operation deﬁned on its elements
in the following senses:
Vector Addition: A rule for combining any two vectors in V . We will use the usual +
sign to denote an addition operation, and the result of adding the vectors u and v will
be denoted u + v.
Real (or complex) scalar multiplication: A rule for combining each vector in V with
any real (or complex) number. We will use the notation kv or, for emphasis, k · v, to
denote the result of scalar multiplying the vector v by the real (or complex) number k.
We let F denote the set of scalars for which the operation is deﬁned. Thus, for us,
F is either the set of all real numbers or the set of all complex numbers. For example,
if V is the set of all 2 × 2 matrices with complex elements and F denotes the set of all

4.2
Definition of a Vector Space 253
complex numbers, then the usual operation of matrix addition is an addition operation
on V , and the usual method of multiplying a matrix by a scalar is a scalar multiplication
operation on V . Notice that the result of applying either of these operations is always
another vector (2 × 2 matrix) in V .
As a further example, let V be the set of positive integers, and let F be the set of all
real numbers. Then the usual operations of addition and multiplication within the real
numbers deﬁne addition and scalar multiplication operations on V . Note in this case,
however, that the scalar multiplication operation, in general, will not yield another vector
in V , since when we multiply a positive integer by a real number, the result is not always
a positive integer.
We are now in a position to give a precise deﬁnition of a vector space.
DEFINITION
4.2.1
Let V be a nonempty set (whose elements are called vectors) on which is deﬁned an
addition operation and a scalar multiplication operation with scalars in F. We call V
a vector space over F, provided the following ten conditions are satisﬁed:
A1. Closure under addition: For each pair of vectors u and v in V , the sum u + v
is also in V . We say that V is closed under addition.
A2. Closure under scalar multiplication: For each vector v in V and each scalar k
in F, the scalar multiple kv is also in V . We say that V is closed under scalar
multiplication.
A3. Commutativity of addition: For all u, v ∈V , we have
u + v = v + u.
A4. Associativity of addition: For all u, v, w ∈V , we have
(u + v) + w = u + (v + w).
A5. Existence of a zero vector in V : In V there is a vector, denoted 0, satisfying
v + 0 = v,
for all v ∈V.
A6. Existence of additive inverses in V : For each vector v ∈V , there is a vector,
denoted −v, in V such that
v + (−v) = 0.
A7. Unit property: For all v ∈V ,
1v = v.
A8. Associativity of scalar multiplication: For all v ∈V and all scalars r, s ∈F,
(rs)v = r(sv).
A9. Distributive property of scalar multiplication over vector addition: For all
u, v ∈V and all scalars r ∈F,
r(u + v) = ru + rv.
A10. Distributive property of scalar multiplication over scalar addition: For all
v ∈V and all scalars r, s ∈F,
(r + s)v = rv + sv.

254
CHAPTER 4
Vector Spaces
Remarks
1. A key point to note is that in order to deﬁne a vector space, we must start with all
of the following:
(a) A nonempty set of vectors V .
(b) A set of scalars F (either R or C).
(c) An addition operation deﬁned on V .
(d) A scalar multiplication operation deﬁned on V .
Then we must check that the axioms A1–A10 are satisﬁed.
2. Terminology: A vector space over the real numbers will be referred to as a real
vector space, whereas a vector space over the complex numbers will be called a
complex vector space.
3. To reiterate a point we made earlier in this section, we will use bold print in this
text to denote vectors in a general vector space, but in handwriting it is strongly
advised that vectors be denoted either as
→v or as v∼. This will avoid any confusion
between vectors in V and scalars in F.
4. When we deal with a familiar vector space, we will use the usual notation for
vectors in the space. For example, as seen below, the set Rn of ordered n-tuples
is a vector space, and we will denote vectors here in the form (x1, x2, . . . , xn), as
done in the previous section. As another illustration, it is shown in Example 4.2.3
below that the set of all real-valued functions deﬁned on an interval is a vector
space, and we will denote the vectors in this vector space by f, g, . . . .
Examples of Vector Spaces
1. The set of all real numbers, R, together with the usual operations of addition and
multiplication, is a real vector space.
2. The set of all complex numbers is a complex vector space when we use the usual
operations of addition and multiplication by a complex number. It is also possible
to restrict the set of scalars to R, in which case the set of complex numbers becomes
a real vector space.
3. The set Rn, together with the operations of addition and scalar multiplication
deﬁned in (4.1.13) and (4.1.14), is a real vector space. As we saw in the previous
section, the zero vector in Rn is the n-tuple of zeros (0, 0, . . . , 0), and the additive
inverse of the vector v = (x1, x2, . . . , xn) is −v = (−x1, −x2, . . . , −xn).
Strictly speaking, for each of the examples above it is necessary to verify all of
the axioms A1–A10 of a vector space. However, in these examples, the axioms hold
immediately as well-known properties of real and complex numbers and n-tuples.
Example 4.2.2
Let V be the set of all 2 × 2 matrices with real elements. Show that V , together with the
usual operations of matrix addition and multiplication of a matrix by a real number, is a
real vector space.
Solution:
We must verify the axioms A1–A10. If A and B are in V (that is, A and B
are 2 × 2 matrices with real elements), then A + B and k A are in V for all real numbers
k. Consequently, V is closed under addition and scalar multiplication, and therefore,
Axioms A1 and A2 of the vector space deﬁnition hold.

4.2
Definition of a Vector Space 255
A3. Given two 2 × 2 matrices A =
% a1
a2
a3
a4
&
and B =
% b1
b2
b3
b4
&
, we have
A + B =
% a1
a2
a3
a4
&
+
% b1
b2
b3
b4
&
=
% a1 + b1
a2 + b2
a3 + b3
a4 + b4
&
=
% b1 + a1
b2 + a2
b3 + a3
b4 + a4
&
=
% b1
b2
b3
b4
&
+
% a1
a2
a3
a4
&
= B + A.
Note that, in the key step in the middle of the chain of equalities, we used the fact
that the commutative property for addition of real numbers ai + bi = bi + ai (for
i = 1, 2, 3, 4) is already known to hold.
A4. Given three 2 × 2 matrices A =
% a1
a2
a3
a4
&
, B =
% b1
b2
b3
b4
&
, and C =
% c1
c2
c3
c4
&
,
we use the fact that the associative property for addition of real numbers (ai +
bi) + ci = ai + (bi + ci) (for i = 1, 2, 3, 4) is already known to hold as follows:
(A + B) + C =
'% a1
a2
a3
a4
&
+
% b1
b2
b3
b4
&(
+
% c1
c2
c3
c4
&
=
% a1 + b1
a2 + b2
a3 + b3
a4 + b4
&
+
% c1
c2
c3
c4
&
=
% (a1 + b1) + c1
(a2 + b2) + c2
(a3 + b3) + c3
(a4 + b4) + c4
&
=
% a1 + (b1 + c1) a2 + (b2 + c2)
a3 + (b3 + c3) a4 + (b4 + c4)
&
=
% a1
a2
a3
a4
&
+
% b1 + c1
b2 + c2
b3 + c3
b4 + c4
&
=
% a1
a2
a3
a4
&
+
'% b1
b2
b3
b4
&
+
% c1
c2
c3
c4
&(
= A + (B + C).
A5. If A is any matrix in V , then
A +
% 0 0
0 0
&
= A.
Thus, 02 is the zero vector in V .
A6. The additive inverse of A =
% a
b
c
d
&
is −A =
% −a
−b
−c −d
&
, since
A + (−A) =
% a + (−a)
b + (−b)
c + (−c)
d + (−d)
&
=
% 0 0
0 0
&
= 02.
A7. If A is any matrix in V , then
1A = A,
thus verifying the unit property.
As we verify the remaining properties A8–A10, the reader should once more
observe how the corresponding property of real numbers is being utilized within
theelementsofthematricestoprovethesamepropertyforthematricesthemselves.

256
CHAPTER 4
Vector Spaces
A8. Given a matrix A =
% a
b
c
d
&
and scalars r and s, we have
(rs)A =
% (rs)a
(rs)b
(rs)c
(rs)d
&
=
%r(sa) r(sb)
r(sc) r(sd)
&
= r
% sa
sb
sc
sd
&
= r(s A),
as required.
A9. Given matrices A =
% a1
a2
a3
a4
&
and B =
% b1
b2
b3
b4
&
and a scalar r, we have
r(A + B) = r
'% a1
a2
a3
a4
&
+
% b1
b2
b3
b4
&(
= r
% a1 + b1
a2 + b2
a3 + b3
a4 + b4
&
=
%r(a1 + b1) r(a2 + b2)
r(a3 + b3) r(a4 + b4)
&
=
%ra1 + rb1
ra2 + rb2
ra3 + rb3
ra4 + rb4
&
=
%ra1
ra2
ra3
ra4
&
+
%rb1
rb2
rb3
rb4
&
= r A+r B.
A10. Given A, r, and s as in A8 above, we have
(r + s)A =
% (r + s)a
(r + s)b
(r + s)c
(r + s)d
&
=
%ra + sa
rb + sb
rc + sc
rd + sd
&
=
%ra
rb
rc
rd
&
+
% sa
sb
sc
sd
&
= r A + s A,
as required.
Thus V , together with the given operations, is a real vector space.
□
Remark
In a manner similar to the previous example, it is easily established that the
set of all m × n matrices with real elements is a real vector space when we use the usual
operations of addition of matrices and multiplication of matrices by a real number. We
will denote the vector space of all m × n matrices with real elements by Mm×n(R), and
we denote the vector space of all n × n matrices with real elements by Mn(R).
Example 4.2.3
Let V be the set of all real-valued functions deﬁned on an interval I. Deﬁne addition and
scalar multiplication in V as follows. If f and g are in V and k is any real number, then
f + g and k f are deﬁned by
( f + g)(x) = f (x) + g(x)
for all x ∈I,
(k f )(x) = k f (x)
for all x ∈I.
Show that V , together with the given operations of addition and scalar multiplication, is
a real vector space.
Solution:
It follows from the given deﬁnitions of addition and scalar multiplication
that if f and g are in V , and k is any real number, then f + g and k f are both real-valued
functions on I and are therefore in V . Consequently, the closure axioms A1 and A2 hold.
We now check the remaining axioms. Note that to show that two functions in V
are equal, such as f + g and g + f for Axiom A3, we are required to show that both
functions yield the same output for each input x ∈I.
A3. Let f and g be arbitrary functions in V . From the deﬁnition of function addition,
we have
( f + g)(x) = f (x) + g(x) = g(x) + f (x) = (g + f )(x),

4.2
Definition of a Vector Space 257
for all x ∈I. (The middle step here follows from the fact that f (x) and g(x) are
real numbers associated with evaluating f and g at the input x, and real numbers
commute.) Consequently, f + g = g + f (since the values of f + g and g + f
agree for every x ∈I), and so addition in V is commutative.
A4. Let f, g, h ∈V . Then for all x ∈I, we have
[( f + g) + h](x) = ( f + g)(x) + h(x) = [ f (x) + g(x)] + h(x)
= f (x) + [g(x) + h(x)] = f (x) + (g + h)(x)
= [ f + (g + h)](x).
Consequently, ( f +g)+h = f +(g+h), so that addition in V is indeed associative.
A5. If we deﬁne the zero function, O, by O(x) = 0, for all x ∈I, then
( f + O)(x) = f (x) + O(x) = f (x) + 0 = f (x),
for all f ∈V and all x ∈I, which implies that f + O = f . Hence, O is the zero
vector in V . (See Figure 4.2.1.)
y 5 f(x)
x
y
I
y 5 –f(x)
y 5 O(x)
Figure 4.2.1: In the vector space
of all functions deﬁned on an
interval I, the additive inverse of a
function f is obtained by reﬂecting
the graph of f about the x-axis. The
zero vector is the zero function
O(x).
A6. If f ∈V , then −f is deﬁned by (−f )(x) = −f (x) for all x ∈I, since
[ f + (−f )](x) = f (x) + (−f )(x) = f (x) −f (x) = 0
for all x ∈I. This implies that f + (−f ) = O.
A7. Let f ∈V . Then, by deﬁnition of the scalar multiplication operation, for all x ∈I,
we have
(1 f )(x) = 1 f (x) = f (x).
Consequently, 1 f = f .
A8. Let f ∈V , and let r, s ∈R. Then, for all x ∈I,
[(rs) f ](x) = (rs) f (x) = r[s f (x)] = r[(s f )(x)] = [r(s f )](x).
Hence, the functions (rs) f and r(s f ) agree on every x ∈I, and hence, (rs) f =
r(s f ) as required.
A9. Let f, g ∈V and let r ∈R. Then, for all x ∈I,
[r( f + g)](x) = r[( f + g)(x)] = r[ f (x) + g(x)] = r f (x) + rg(x)
= (r f )(x) + (rg)(x) = (r f + rg)(x).
Hence, r( f + g) = r f + rg.
A10. Let f ∈V , and let r, s ∈R. Then for all x ∈I,
[(r+s) f ](x) = (r+s) f (x) = r f (x)+s f (x) = (r f )(x)+(s f )(x) = [r f +s f ](x),
which proves that (r + s) f = r f + s f .
Since all parts of Deﬁnition 4.2.1 are satisﬁed, it follows that V , together with the
given operations of addition and scalar multiplication, is a real vector space.
□
Remark
As the previous two examples indicate, a full veriﬁcation of the vector space
deﬁnition can be somewhat tedious and lengthy, although it is usually straightforward.
Be careful to not leave out any important steps in such a veriﬁcation.

258
CHAPTER 4
Vector Spaces
Example 4.2.4
Let V be the set of all polynomials with real coefﬁcients and of degree 2 or less, together
with the usual operations of polynomial addition and multiplication of a polynomial by
a real number. Show that V is a real vector space.
Solution:
A typical vector in V can be expressed in the form p(x) = ax2 + bx + c,
wherea, b, and c are real scalars. Note that since the coefﬁcients of p(x) could potentially
be zero, the degree of p(x) could be less than 2.
A1. Suppose that p1(x) = a1x2 + b1x + c1 and p2(x) = a2x2 + b2x + c2, where
a1, b1, c1, a2, b2, c2 ∈R. (Note that we use the same indeterminate, x, for both
polynomials. It would be an error to write the second polynomial as p(y). The
characteristics that determine the polynomial are the coefﬁcients, not the choice
of indeterminate.) Then
p1(x) + p2(x) = (a1x2 + b1x + c1) + (a2x2 + b2x + c2)
= (a1 + a2)x2 + (b1 + b2)x + (c1 + c2),
which is another polynomial of the form required for membership in V .
A2. Let k be a real number, and let p(x) be as above. Then
k · p(x) = k(ax2 + bx + c) = (ka)x2 + (kb)x + (kc),
which once more belongs to V .
The reader may have noticed that the polynomials in V can be viewed as real-valued
functions deﬁned on any interval. Therefore, all of the work done in Example 4.2.3
still applies in the present context. We will therefore forego a detailed veriﬁcation of
A3–A10, except to note that the zero vector in this case is the polynomial p(x) =
0x2 + 0x + 0 = 0. In the next section, we will discuss at length how one can often
use an already-established vector space to considerably shorten the work required to
engender other closely related vector spaces.
□
Remark
The vector space V in the previous example will be denoted by P2(R)
throughout this text. In fact, one can more generally establish that the collection of all
polynomials of degree n or less, which can be denoted by Pn(R), is a vector space.
The Vector Space Cn
We now introduce the most important complex vector space. Let Cn denote the set of all
ordered n-tuples of complex numbers. Thus,
Cn = {(z1, z2, . . . , zn) : z1, z2, . . . , zn ∈C}.
We refer to the elements of Cn as vectors in Cn. A typical vector in Cn is (z1, z2, . . . , zn),
where each zk is a complex number.
Example 4.2.5
The following are examples of vectors in C2 and C4, respectively:
u = (−6i, −3 + 2i),
v = (−3i, 8, −12 + i, −3 −11i).
□
In order to obtain a vector space, we must deﬁne appropriate operations of “vector
addition” and “multiplication by a scalar” on the set of vectors in question. In the case of
Cn, we are motivated by the corresponding operations in Rn and thus deﬁne the addition

4.2
Definition of a Vector Space 259
and scalar multiplication operations componentwise. Thus, if u = (u1, u2, . . . , un) and
v = (v1, v2, . . . , vn) are vectors in Cn and k is an arbitrary complex number, then
u + v = (u1 + v1, u2 + v2, . . . , un + vn),
ku = (ku1, ku2, . . . , kun).
Example 4.2.6
If u = (−5i, 2 + 3i), v = (−5, 1 + 4i), and k = −6 + 2i, ﬁnd u + kv.
Solution:
We have
u + kv = (−5i, 2 + 3i) + (−6 + 2i)(−5, 1 + 4i)
= (−5i, 2 + 3i) + (30 −10i, −14 −22i) = (30 −15i, −12 −19i).
□
It is straightforward to show that Cn, together with the given operations of addition and
scalar multiplication, is a complex vector space.
Further Properties of Vector Spaces
The main reason for formalizing the deﬁnition of an abstract vector space is that any
results that we can prove based solely on the deﬁnition will then apply to all vector
spaces we care to examine; that is, we do not have to prove separate results for geometric
vectors, m × n matrices, vectors in Rn or Cn, real-valued functions, and so on. The next
theorem lists some results that can be proved using the vector space axioms.
Theorem 4.2.7
Let V be a vector space over F.
1. The zero vector is unique.
2. 0v = 0 for all v ∈V .
3. k0 = 0 for all scalars k ∈F.
4. The additive inverse of each element in V is unique.
5. For all v ∈V , −v = (−1)v.
6. If k is a scalar and v ∈V such that kv = 0, then either k = 0 or v = 0.
Remark
In light of the results listed in Theorem 4.2.7, the importance of distinguish-
ing the scalar 0 from the vector 0 bears repeating.
Proof
1. Suppose that there are two zero vectors in V , denoted 01 and 02. Then, for
any v ∈V , we would have
v + 01 = v
(4.2.1)
and
v + 02 = v.
(4.2.2)
We must prove that 01 = 02. But, applying (4.2.1) with v = 02, we have
02 = 02 + 01
= 01 + 02
(Axiom A3)
= 01
(from (4.2.2) with v = 01).
Consequently, 01 = 02, and so, the zero vector is unique in a vector space.

260
CHAPTER 4
Vector Spaces
2. Let v be an arbitrary element in a vector space V . Since 0 = 0 + 0, we have
0v = (0 + 0)v = 0v + 0v,
by Axiom A10. Now Axiom A6 implies that the vector −(0v) exists, and adding
it to both sides of the previous equation yields
0v + [−(0v)] = (0v + 0v) + [−(0v)].
Thus, since addition in a vector space is associative (Axiom A4),
0v + [−(0v)] = 0v + (0v + [−(0v)]).
Applying Axiom A6 on both sides and then using Axiom A5, this becomes
0 = 0v + 0 = 0v,
and this completes the veriﬁcation of (2).
3. Using the fact that 0 = 0 + 0 (by Axiom A5), the proof here proceeds along the
same lines as the proof of (2). We leave the veriﬁcation to the reader as an exercise
(Problem 30).
4. Let v ∈V be an arbitrary vector, and suppose that there are two additive inverses,
say w1 and w2, for v. According to Axiom A6, this implies that
v + w1 = 0
(4.2.3)
and
v + w2 = 0.
(4.2.4)
We wish to show that w1 = w2. Now Axiom A6 implies that a vector −w1 exists,
so adding it on the right to both sides of (4.2.3) yields
[v + w1] + (−w1) = 0 + (−w1) = −w1.
Applying Axioms A4–A6 on the left side, we simplify this to
v = −w1.
Substituting this into (4.2.4) yields
−w1 + w2 = 0.
Adding w1 on the left of both sides and applying Axioms A4–A6 once more yields
w1 = w2, as desired.
5. To verify that −v = (−1)v for all v ∈V , we note that
0 = 0v = (1 + (−1))v = 1v + (−1)v = v + (−1)v,
where we have used part (2) of Theorem 4.2.7 and Axioms A10 and A7. The
equation above proves that (−1)v is an additive inverse of v, and by the uniqueness
of additive inverses that we just proved in part (4), we conclude that (−1)v = −v,
as desired.
Finally, we leave the proof of (6) in Theorem 4.2.7 as an exercise (Problem 31).

4.2
Definition of a Vector Space 261
Remark
The proof of Theorem 4.2.7 involved a number of tedious and seemingly
obvious steps. It is important to remember, however, that in an abstract vector space,
we are not allowed to rely on past experience in deriving results for the ﬁrst time.
For instance, the statement “0 + 0 = 0” may seem intuitively clear, but in our newly
developed mathematical structure, we must appeal speciﬁcally to the rules A1–A10 given
for a vector space. Hence, the statement 0 + 0 = 0 should be viewed as a consequence
of Axiom A5 and nothing else. Once we have proved these basic results, of course, then
we are free to use them in any vector space context where they are needed. This is the
whole advantage to working in the general vector space setting.
We end this section with a list of the most important vector spaces that will be
required throughout the remainder of the text. In each case the addition and scalar mul-
tiplication operations are the usual ones associated with the set of vectors.
• Rn, the (real) vector space of all ordered n-tuples of real numbers.
• Cn, the (complex) vector space of all ordered n-tuples of complex numbers.
• Mm×n(R), the (real) vector space of all m × n matrices with real elements.
• Mn(R), the (real) vector space of all n × n matrices with real elements.
• Ck(I), the vector space of all real-valued functions that are continuous and have
(at least) k continuous derivatives on an interval I in R. We will show that this set
of vectors is a (real) vector space in the next section.
• Pn(R), the (real) vector space of all real-valued polynomials of degree ≤n with
real coefﬁcients. That is,
Pn(R) = {a0 + a1x + a2x2 + · · · + anxn : a0, a1, . . . , an ∈R}.
We leave the veriﬁcation that Pn(R) is a (real) vector space as an exercise
(Problem 32).
Exercises for 4.2
Key Terms
Vector space (real or complex), Closure under addition, Clo-
sure under scalar multiplication, Commutativity of addition,
Associativity of addition, Existence of zero vector, Existence
of additive inverses, Unit property, Associativity of scalar
multiplication, Distributive properties, Examples: Rn, Cn,
Mm×n(R), Mn(R), Ck(I), Pn(R).
Skills
• Be able to deﬁne a vector space. Speciﬁcally, be able to
identify and list the ten axioms (A1)–(A10) governing
the vector space operations.
• Know each of the standard examples of vector spaces
given at the end of the section, and know how to per-
form the vector operations in these vector spaces.
• Be able to check whether or not each of the axioms
(A1)–(A10) holds for speciﬁc examples V . This in-
cludes, if possible, closure of V under vector addition
and scalar multiplication, as well as identiﬁcation of
the zero vector and the additive inverse of each vector
in the set V .
• Be able to prove basic properties that hold generally
for vector spaces V (see Theorem 4.2.7).
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The zero vector in a vector space V is unique.
(b) If v is a vector in a vector space V and r and s are
scalars such that rv = sv, then r = s.

262
CHAPTER 4
Vector Spaces
(c) If v is a nonzero vector in a vector space V , and r and
s are scalars such that rv = sv, then r = s.
(d) The set Z of integers, together with the usual oper-
ations of addition and scalar multiplication, forms a
vector space.
(e) If x and y are vectors in a vector space V , then the
additive inverse of x + y is (−x) + (−y).
(f) The additive inverse of a vector v in a vector space V
is unique.
(g) The set {0}, with the usual operations of addition and
scalar multiplication, forms a vector space.
(h) The set {0, 1}, with the usual operations of addition
and scalar multiplication, forms a vector space.
(i) The set of positive real numbers, with the usual op-
erations of addition and scalar multiplication, forms a
vector space.
(j) The set of nonnegative real numbers (i.e., {x ∈R :
x ≥0}), with the usual operations of addition and
scalar multiplication, forms a vector space.
Problems
For Problems 1–14, determine whether the given set S of
vectors is closed under addition and closed under scalar mul-
tiplication. In each case, take the set of scalars to be the set
of all real numbers.
1. The set S := Q of all rational numbers.1
2. The set S := Un(R) of all upper triangular n × n
matrices with real elements.
3. The set
S := {A ∈Mn(R) : A is upper or lower triangular}.
4. The set S := {a0 + a1x + a2x2 : a0 + a1 + a2 = 0}.
5. The set S := {a0 + a1x + a2x2 : a0 + a1 + a2 = 1}.
6. The set S of all solutions to the differential equation
y′ + 3y = 6x3 + 5. (Do not solve the differential
equation.)
7. The set S of all solutions to the differential equation
y′ + 3y = 0. (Do not solve the differential equation.)
1A rational number is any real number that can be expressed as a fraction a/b of two whole numbers a and b,
with b ̸= 0. This set includes the set of integers as a subset, since any integer n can be written as the fraction
n/1.
8. For a ﬁxed m × n matrix A, the set
S := {x ∈Rn : Ax = 0}.
(This is the set of all solutions to the homogeneous
linear system of equations Ax = 0 and is often called
the null space of A.)
9. The set S := {A ∈M2(R) : det(A) = 0}.
10. The set S := {(x, y) ∈R2 : y = x2}.
11. The set S := {(x, y) ∈R2 : y = x + 1}.
12. The set N := {1, 2, . . . } of all positive integers.
13. The set S of all polynomials of degree exactly 2.
14. Theset S ofallpolynomialsoftheforma + bx3 + cx4,
where a, b, c ∈R.
15. We have deﬁned the set R2 = {(x, y) : x, y ∈R},
together with the addition and scalar multiplication
operations as follows:
(x1, y1) + (x2, y2) = (x1 + x2, y1 + y2),
k(x1, y1) = (kx1, ky1).
Give a complete veriﬁcation that each of the vector
space axioms is satisﬁed.
16. Determine the zero vector in the vector space V =
M4×2(R), and write down a general element A in V
along with its additive inverse −A.
17. Generalize the previous exercise to ﬁnd the zero vec-
tor and the additive inverse of a general element of
Mm×n(R).
18. Determine the zero vector in the vector space V =
P3(R), and write down a general element p(x) in V
along with its additive inverse −p(x).
19. Generalize the previous exercise to ﬁnd the zero vec-
tor and the additive inverse of a general element of
Pn(R).
20. On R+, the set of positive real numbers, deﬁne the op-
erations of addition, ⊕, and scalar multiplication, ⊙,
as follows:
x ⊕y = xy
c ⊙x = xc.

4.3
Subspaces 263
Note that the multiplication and exponentiation ap-
pearing on the right side of these formulas refer to
the ordinary operations on real numbers. Determine
whether R+, together with these algebraic operations,
is a vector space.
21. On R2, deﬁne the operations of addition and scalar
multiplication as follows:
(x1, y1) ⊕(x2, y2) = (x1 + x2, y1 + y2),
k ⊙(x1, y1) = (kx1, y1).
Which of the axioms for a vector space are satisﬁed
by R2 with these algebraic operations? Is this a vector
space structure?
22. On R2, deﬁne the operations of addition and scalar
multiplication by a real number as follows:
(x1, y1) ⊕(x2, y2) = (x1 −x2, y1 −y2),
k ⊙(x1, y1) = (−kx1, −ky1).
Which of the axioms for a vector space are satisﬁed
by R2 with these algebraic operations? Is this a vector
space structure?
23. On R2, deﬁne the operation of addition by
(x1, y1) ⊕(x2, y2) = (x1x2, y1y2).
Do axioms (A5) and (A6) in the deﬁnition of a vector
space hold? Justify your answer.
24. On M2(R), deﬁne the operation of addition by
A ⊕B = AB,
and use the usual scalar multiplication operation. De-
termine which axioms for a vector space are satisﬁed
by M2(R) with the above operations.
25. On M2(R), deﬁne the operations of addition and scalar
multiplication by a real number (⊕and ⊙, respec-
tively) as follows:
A ⊕B = −(A + B),
k ⊙A = −k A,
where the operations on the right-hand sides of these
equations are the usual ones associated with M2(R).
Determine which of the axioms for a vector space are
satisﬁed by M2(R) with the operations ⊕and ⊙.
For Problems 26–27, verify that the given set of objects to-
gether with the usual operations of addition and scalar mul-
tiplication is a complex vector space.
26. C2.
27. M2(C), the set of all 2 × 2 matrices with complex
elements.
28. Is C3 a real vector space? Explain.
29. Is R3 a complex vector space? Explain.
30. Prove part (3) of Theorem 4.2.7.
31. Prove part (6) of Theorem 4.2.7.
32. Prove that Pn(R) is a vector space.
4.3
Subspaces
In the preceding section, we introduced a number of examples of vector spaces. These
examples are important not only as motivation for the concept of a vector space, but
also because the solution to many of the applied problems in this text can be regarded
as a vector or a collection of vectors in one of these vector spaces. For instance, in
Example 2.5.6, we saw that the system of linear equations
5x1 −6x2 + x3 = 4,
2x1 −3x2 + x3 = 1,
4x1 −3x2 −x3 = 5.
has a set of solutions consisting of vectors in the vector space R3. More generally, given
any system of linear equations, Ax = b, where A is an m × n matrix with real elements,
the basic unknown in this system, x, is a vector in Rn. Consequently, the solution set to
the system is a subset of the vector space Rn.
Similarly,thedifferentialequation y′ + 3y = 0 canbequicklysolvedbythemethods
of Chapter 1, and the resulting collection of solutions are precisely the functions in the
vector space C1(R) of the form y(x) = ce−3x.

264
CHAPTER 4
Vector Spaces
Vector space of unknowns
Solution set of applied problem:
Is S a vector space?
V
Figure 4.3.1: The solution set S of an applied problem is a subset of the vector space V of
unknowns in the problem.
As these examples illustrate, the solution set of an applied problem is generally
a subset of vectors from an appropriate vector space (schematically represented in
Figure 4.3.1). The question that we will need to answer in the future is whether this
subset of vectors is a vector space in its own right. The following deﬁnition introduces
the terminology we will use:
DEFINITION
4.3.1
Let S be a nonempty subset of a vector space V . If S is itself a vector space under the
same operations of addition and scalar multiplication as used in V , then we say that
S is a subspace of V .
In establishing that a given subset S of vectors from a vector space V is a subspace
of V , it would appear as though we must check that each of the axioms in the vector
space deﬁnition are satisﬁed when we restrict our attention to vectors lying only in S.
The ﬁrst and most important theorem of the section tells us that all we need do, in fact,
is check the closure axioms A1 and A2, and if these are satisﬁed, then the remaining
axioms necessarily hold in S. This is a very useful theorem that will be applied on many
occasions throughout the remainder of the text and in the exercises.
Theorem 4.3.2
Let S be a nonempty subset of a vector space V . Then S is a subspace of V if and only
if S is closed under the operations of addition and scalar multiplication in V .
Proof If S is a subspace of V , then it is a vector space, and hence, it is certainly
closed under addition and scalar multiplication. Conversely, assume that S is closed
under addition and scalar multiplication. We must prove that Axioms A3–A10 of Deﬁ-
nition 4.2.1 hold when we restrict to vectors in S. Consider ﬁrst the axioms A3, A4, and
A7–A10. These are properties of the addition and scalar multiplication operations, and
hence, since we use the same operations in S as in V , these axioms are all inherited from
V by the subset S. Finally, we establish A5 and A6: Choose any vector2 v in S. Since S
is closed under scalar multiplication, both 0v and (−1)v are in S. But by Theorem 4.2.7,
0v = 0 and (−1)v = −v, and hence, 0 and −v are both in S. Therefore, A5 and A6
are satisﬁed.
2This is possible since S is assumed to be nonempty.

4.3
Subspaces 265
The idea behind Theorem 4.3.2 is that once we have a vector space V in place,
then any nonempty subset S, equipped with the same addition and scalar multiplication
operations, will inherit all of the axioms that involve those operations. The only possible
concern we have for S is whether or not it satisﬁes the closure axioms A1 and A2. Of
course, we presumably had to carry out the full veriﬁcation of A1–A10 for the vector
space V in the ﬁrst place, before gaining the shortcut of Theorem 4.3.2 for the subset S.
In determining whether a subset S of a vector space V is a subspace of V , we must
keep clear in our minds what the given vector space is and what conditions on the vectors
in V restrict them to lie in the subset S. This is most easily done by expressing S in set
notation as follows:
S = {v ∈V : conditions on v}.
(4.3.1)
We illustrate with an example.
Example 4.3.3
Let S denote the set of all real solutions to the following linear system of equations:
x1 −4x2 + 6x3 = 0,
−3x1 + 10x2 −10x3 = 0.
Express S in set notation and verify that S is a subspace of R3.
Solution:
Following the model in (4.3.1), we can express S in set notation as follows:
S = {x ∈R3 : x satisﬁes the two equations given in the linear system},
or
S = {(x1, x2, x3) ∈R3 : x1 −4x2 + 6x3 = 0
and
−3x1 + 10x2 −10x3 = 0}.
The reduced row-echelon form of the augmented matrix of the system is
% 1 0 −10 0
0 1
−4 0
&
,
so that the solution set of the system is the nonempty set
S = {(x1, x2, x3) ∈R3 : x1 = 10t, x2 = 4t, x3 = t
for some t ∈R}.
We can also write this as
S = {x ∈R3 : x = (10t, 4t, t), t ∈R}.
We now use Theorem 4.3.2 to verify that S is a subspace of R3: If x = (10r, 4r,r) and
y = (10s, 4s, s) are any two vectors in S, then
x + y = (10r, 4r,r) + (10s, 4s, s) = (10(r + s), 4(r + s),r + s) = (10t, 4t, t),
where t = r + s. Thus, x + y has the required form for elements of S, and conse-
quently, if we add two vectors in S, the result is another vector in S. Similarly, if we
multiply an arbitrary vector x = (10r, 4r,r) in S by a real number k, the resulting
vector is
kx = k(10r, 4r,r) = (10kr, 4kr, kr) = (10t, 4t, t),
where t = kr. Hence, kx again has the proper form for membership in the subset S,
and so S is closed under scalar multiplication. By Theorem 4.3.2, S is a subspace of R3.

266
CHAPTER 4
Vector Spaces
y
x
z
(10, 4, 1)
x = (10t, 4t, t) = t(10, 4, 1)
Figure 4.3.2: The solution set to the homogeneous system of linear equations in Example 4.3.3
is a subspace of R3.
Note, of course, that our application of Theorem 4.3.2 hinges on our prior knowledge
that R3 is a vector space.
Geometrically, the vectors in S lie along the line of intersection of the planes with
the given equations. This is the line through the origin in the direction of the vector
v = (10, 4, 1). (See Figure 4.3.2.)
□
Example 4.3.4
Verify that S = {x ∈R2 : x = (r, −3r + 1), r ∈R} is not a subspace of R2.
Solution:
One approach here, according to Theorem 4.3.2, is to demonstrate the
failure of closure under addition or scalar multiplication. For example, if we start with
two vectors in S, say x = (r, −3r + 1) and y = (s, −3s + 1), then
x + y = (r, −3r + 1) + (s, −3s + 1) = (r + s, −3(r + s) + 2) = (w, −3w + 2),
where w = r + s. Owing to the form of the second component of x + y, we see that
x +y does not have the required form for membership in S. Hence, S is not closed under
addition, and hence fails to be a subspace of R2. Alternatively, we can show similarly
that S is not closed under scalar multiplication.
Observant readers may have noticed another reason that S cannot form a subspace.
Geometrically, the points in S correspond to those points that lie on the line with Cartesian
equation y = −3x+1. Since this line does not pass through the origin, S does not contain
the zero vector 0 = (0, 0), and therefore, we know S cannot be a subspace.
□
Remark
In general, we have the following important observation. It could be referred
to as the
Zero Vector Check
If a subset S of a vector space V fails to contain the zero vector 0,
then it cannot form a subspace.
This observation can often be made more quickly than deciding whether or not S is
closed under addition and closed under scalar multiplication. However, we caution that
if the zero vector does belong to S, then the observation is inconclusive and further
investigation is required to determine whether or not S forms a subspace of V .
Example 4.3.5
Let V = R2, and let
S1 = {(x, x −1) : x ∈R}
and
S2 = {(x, x2) : x ∈R}.

4.3
Subspaces 267
Note that (0, 0) does not belong to S1, so S1 fails to be a subspace of V by the Zero
Vector Check. On the other hand, (0, 0) does belong to S2. However, in spite of this, S2
still fails to be a subspace of V . The reader can check that both closure under addition
and closure under scalar multiplication do not hold for S2. See Figure 4.3.3.
□
y = x2
y = x – 1 
x
y
S1 = {(x, x – 1) : x P R}
S2 = {(x, x2) : x P R}
Figure 4.3.3: The subsets S1 and S2 in Example 4.3.5 each fail to form a subspace of R2.
Example 4.3.6
Let S denote the set of all real skew-symmetric n×n matrices. Verify that S is a subspace
of Mn(R).
Solution:
The subset of interest is
S = {A ∈Mn(R) : AT = −A}.
Note that S is nonempty since, for example, it contains the zero matrix 0n. We now verify
closure of S under addition and scalar multiplication. Let A and B be in S. Then
AT = −A
and
BT = −B.
Using these conditions and the properties of the transpose yields
(A + B)T = AT + BT = (−A) + (−B) = −(A + B)
and
(k A)T = k AT = k(−A) = −(k A)
for all real values of k. Consequently A + B and k A are both skew-symmetric matri-
ces, and so, they are elements of S. Hence S is closed under both addition and scalar
multiplication, and so, by Theorem 4.3.2, it is indeed a subspace of Mn(R).
□
Remark
Notice in Example 4.3.6 that it was not necessary to actually write out the
matrices A and B in terms of their elements [ai j] and [bi j], respectively. This shows the
advantage of using simple abstract notation to describe the elements of the subset S in
some situations. In other circumstances, the nature of the description of the elements of
S demands that the elements of the matrices be explicitly written. Here is an example.
Example 4.3.7
Let V = M2×3(R), and let S denote the set of all elements of V for which the entries in
each column sum to zero. Show that S is a subspace of V .

268
CHAPTER 4
Vector Spaces
Solution:
The entries in each column of any matrix A belonging to S must sum
to zero, although the various columns of A are themselves unrelated to one another.
Therefore, a typical element of S has the form
A =
!
a
b
c
−a
−b −c
"
.
Unlike our previous example, in this case it is necessary for us to explicitly write A by
showing its entries. By choosing a = b = c = 0, we see that the zero vector of M2×3(R)
belongs to S. Therefore, we proceed now to show that S is closed under addition and
scalar multiplication.
Let A and B be in S, and for real constants a, b, c, x, y, and z, write
A =
!
a
b
c
−a
−b −c
"
and
B =
!
x
y
z
−x
−y
−z
"
.
Noting that
A + B =
!
a + x
b + y
c + z
−(a + x) −(b + y) −(c + z)
"
,
we conclude from the fact that the columns of A + B sum to zero that A + B still resides
in S. Moreover, for all real values of k, we see that
k A =
!
ka
kb
kc
k(−a) k(−b) k(−c)
"
=
!
ka
kb
kc
−ka
−kb −kc
"
,
which shows that k A belongs to S. Hence, S is closed under both addition and scalar
multiplication. Thus, S is a subspace of M2×3(R).
□
Example 4.3.8
Let V be the vector space of all real-valued functions deﬁned on an interval [a, b], and
let S denote the set of all functions f in V that satisfy f (a) = f (b). Verify that S is a
subspace of V .
Solution:
We have
S = { f ∈V : f (a) = f (b)},
which is nonempty since it contains, for example, the zero function
O(x) = 0 for all x in [a, b].
Assume that f and g are in S, so that f (a) = f (b) and g(a) = g(b). We now check for
closure of S under addition and scalar multiplication. We have
( f + g)(a) = f (a) + g(a) = f (b) + g(b) = ( f + g)(b),
which implies that f + g ∈S. Hence, S is closed under addition. Further, if k is any real
number,
(k f )(a) = k f (a) = k f (b) = (k f )(b),
so that S is also closed under scalar multiplication. Theorem 4.3.2 therefore implies
that S is a subspace of V . Some representative functions from S are sketched in
Figure 4.3.4.
□

4.3
Subspaces 269
a
b
f(x)
x
Figure 4.3.4: Representative functions in the subspace S given in Example 4.3.8. Each
function in S satisﬁes f (a) = f (b).
Example 4.3.9
Let V be the vector space P2(R), ﬁx r ∈R, and let S denote the set of polynomials
p(x) ∈V such that p(r) = 0. Express S in set notation and verify that S is a subspace
of V .
Solution:
We have
S = {p(x) ∈V : p(r) = 0}.
To verify that S is a subspace of V , it is actually not necessary to write the polynomials
in question explicitly in the form p(x) = a0 + a1x + a2x2 for real scalars a0, a1, a2.
Rather, we can proceed as follows. To show closure under addition, suppose that p(x) and
q(x) belong to S. That is, p(r) = q(r) = 0. Then (p+q)(r) = p(r)+q(r) = 0+0 = 0,
so that p + q ∈S. Similarly, for closure under scalar multiplication, if k is a real scalar,
then (kp)(r) = k · p(r) = k · 0 = 0, so that kp ∈S, as needed.
This example shows that the set of polynomials in P2(R) that have a given root
r ∈R forms a subspace of P2(R).
□
In the next theorem, we establish that the subset {0} of a vector space V is in fact a
subspace of V . We call this subspace the trivial subspace of V .
Theorem 4.3.10
Let V be a vector space with zero vector 0. Then S = {0} is a subspace of V .
Proof Note that S is nonempty. Further, the closure of S under addition and scalar
multiplication follows, respectively, from
0 + 0 = 0
and
k0 = 0,
where the second statement follows from Theorem 4.2.7.
We now use Theorem 4.3.2 to establish an important result pertaining to homoge-
neous systems of linear equations that has already been illustrated in Example 4.3.3.
Theorem 4.3.11
Let A be an m×n matrix. The solution set of the homogeneous system of linear equations
Ax = 0 is a subspace of Cn (or Rn if the solutions are real).

270
CHAPTER 4
Vector Spaces
Proof Let S denote the solution set of the homogeneous linear system. Then we can
write
S = {x ∈Cn : Ax = 0},
a subset of Cn. Since a homogeneous system always admits the trivial solution x = 0,
we know that S is nonempty. If x1 and x2 are in S, then
Ax1 = 0
and
Ax2 = 0.
Using properties of the matrix product, we have
A(x1 + x2) = Ax1 + Ax2 = 0 + 0 = 0,
so that x1 + x2 also solves the system and therefore is in S. Furthermore, if k is any
complex scalar, then
A(kx) = k Ax = k0 = 0,
so that kx is also a solution of the system and therefore is in S. Since S is closed under both
addition and scalar multiplication, it follows from Theorem 4.3.2 that S is a subspace of
Cn. Of course, if the solutions to the system Ax = 0 in Theorem 4.3.11 are all real, then
the solution set is actually a subspace of Rn.
The preceding theorem has established that the solution set to any homogeneous
linear system of equations is a vector space. Because of the importance of this vector
space, it is given a special name.
DEFINITION
4.3.12
Let A be an m × n matrix. The solution set to the corresponding homogeneous linear
system Ax = 0 is called the null space of A and is denoted nullspace(A). Thus,
nullspace(A) = {x ∈Cn : Ax = 0}.
Remarks
1. If the matrix A has real elements, then we will consider only the corresponding
real solutions to Ax = 0. Consequently, in this case,
nullspace(A) = {x ∈Rn : Ax = 0},
a subspace of Rn.
2. The previous theorem does not hold for the solution set of a nonhomogeneous
linear system Ax = b, for b ̸= 0, since x = 0 is not in the solution set of the
system.
Next we introduce the vector space of primary importance in the study of linear
differential equations. This vector space arises as a subspace of the vector space of all
functions that are deﬁned on an interval I.
Example 4.3.13
Let V denote the vector space of all real-valued functions that are deﬁned on an interval
I, and let Ck(I) denote the set of all functions that are continuous and have (at least) k
continuous derivatives on the interval I, for a ﬁxed positive integer k. Show that Ck(I)
is a subspace of V .

4.3
Subspaces 271
Solution:
In this case
Ck(I) = { f ∈V : f, f ′, f ′′, . . . , f (k) exist and are continuous on I}.
This set is nonempty, as the zero function O(x) = 0 for all x ∈I is an element of Ck(I).
Moreover, it follows from the properties of derivatives that if we add two functions in
Ck(I), the result is a function in Ck(I). Similarly, if we multiply a function in Ck(I) by
a scalar, then the result is a function in Ck(I). Thus, Theorem 4.3.2 implies that Ck(I)
is a subspace of V .
□
Our ﬁnal result in this section ties together the ideas introduced here with the theory
of differential equations.
Theorem 4.3.14
The set of all solutions to the homogeneous linear differential equation
y′′ + a1(x)y′ + a2(x)y = 0
(4.3.2)
on an interval I is a vector space.
Proof Let S denote the set of all solutions to the given differential equation. Then S is
a nonempty subset of C2(I), since the identically zero function y = 0 is a solution to the
differentialequation.ByusingTheorem4.3.2,wewillestablishthat S isinfactasubspace
of Ck(I).3 Let y1 and y2 be in S, and let k be a scalar. Then we have the following:
y′′
1 + a1(x)y′
1 + a2(x)y1 = 0
and
y′′
2 + a1(x)y′
2 + a2(x)y2 = 0.
(4.3.3)
Now, if y(x) = y1(x) + y2(x), then
y′′ + a1y′ + a2y = (y1 + y2)′′ + a1(x)(y1 + y2)′ + a2(x)(y1 + y2)
= [y′′
1 + a1(x)y′
1 + a2(x)y1] + [y′′
2 + a1(x)y′
2 + a2(x)y2]
= 0 + 0 = 0,
where we have used (4.3.3). Consequently, y(x) = y1(x) + y2(x) is a solution to the
differential equation (4.3.2). Moreover, if y(x) = ky1(x), then
y′′ + a1(x)y′ + a2(x)y = (ky1)′′ + a1(x)(ky1)′ + a2(x)(ky1)
= k[y′′
1 + a1(x)y′
1 + a2(x)y1] = 0,
where we have once more used (4.3.3). This establishes that y(x) = ky1(x) is a solution
to Equation (4.3.2). Therefore, S is closed under both addition and scalar multiplication.
Consequently, the set of all solutions to Equation (4.3.2) is a subspace of C2(I).
We will refer to the set of all solutions to a differential equation of the form (4.3.2)
as the solution space of the differential equation. A key theoretical result that we will
establish in Chapter 8 regarding the homogeneous linear differential equation (4.3.2) is
that every solution to the differential equation has the form
y(x) = c1y1(x) + c2y2(x),
(4.3.4)
where y1, y2 are any two nonproportional solutions. The form (4.3.4) for y(x) is called a
linear combination of y1(x) and y2(x). The power of this result is impressive: It reduces
3It is important at this point that we have already established Example 4.3.13, so that S is a subset of a set
that is indeed a vector space.

272
CHAPTER 4
Vector Spaces
the search for all solutions to Equation (4.3.2) to the search for just two nonproportional
solutions. In vector space terms, the result can be restated as follows:
Every vector in the solution space to the differential equation (4.3.2) can be written
as a linear combination of any two nonproportional solutions y1 and y2.
We say that the solution space is spanned by y1 and y2. For example, we saw in Example
1.2.13 that the set of all solutions to the differential equation
y′′ + ω2y = 0
is spanned by y1(x) = cos ωx and y2(x) = sin ωx. We now begin our investigation as
to whether this type of idea will work more generally when the solution set to a problem
is a vector space. For example, what about the solution set to a homogeneous linear
system Ax = 0? We might suspect that if there are k free variables deﬁning the vectors
in nullspace(A), then every solution to Ax = 0 can be expressed as a linear combination
of k basic solutions. We will establish that this is indeed the case in Section 4.9. The two
key concepts we need to generalize are (1) spanning a general vector space with a set of
vectors, and (2) linear independence in a general vector space. These will be addressed
in turn in the next two sections.
Exercises for 4.3
Key Terms
Subspace, Zero vector check, Trivial subspace, Null space
of a matrix A.
Skills
• Be able to express typical vectors from a subset S of
a vector space V in set notation.
• Be able to check whether or not a subset S of a vector
space V is a subspace of V .
• Be able to compute the null space of an m×n matrix A.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The null space of an m×n matrix A with real elements
is a subspace of Rm.
(b) The solution set of any linear system of m equations
in n variables forms a subspace of Cn.
(c) The points in R2 that lie on the line y = mx + b form
a subspace of R2 if and only if b = 0.
(d) If m < n, then Rm is a subspace of Rn.
(e) A nonempty subset S of a vector space V that is closed
under scalar multiplication contains the zero vector
of V .
(f) If V = R is a vector space under the usual operations
of addition and scalar multiplication, then the subset
R+ of positive real numbers, together with the oper-
ations deﬁned in Problem 20 of Section 4.2, forms a
subspace of V .
(g) If V = R3 and S consists of all points on the xy-plane,
the xz-plane, and the yz-plane, then S is a subspace
of V .
(h) If V is a vector space, then two different subspaces of
V can contain no common vectors other than 0.
Problems
1. Let S = {x ∈R3 : x = (r −2s, 3r +s, s),r, s ∈R}.
(a) Show that S is a subspace of R3.
(b) Show that the vectors in S lie on the plane with
equation 3x −y + 7z = 0.
2. Let S = {x ∈R2 : x = (2k, −3k), k ∈R}.
(a) Show that S is a subspace of R2.
(b) Make a sketch depicting the subspace S in the
Cartesian plane.

4.3
Subspaces 273
For Problems 3–22, express S in set notation and determine
whether it is a subspace of the given vector space V .
3. V = R3, and S is the set of all vectors (x, y, z) in V
such that z = 3x and y = 2x.
4. V = R2, and S is the set of all vectors (x, y) in V
satisfying 3x + 2y = 0.
5. V = R4, and S is the set of all vectors of the form
(x1, 0, x3, 2).
6. V = R3, and S is the set of all vectors (x, y, z) in V
satisfying x + y + z = 1.
7. V = Rn, and S is the set of all solutions to the nonho-
mogeneous linear system Ax = b, where A is a ﬁxed
m × n matrix and b (̸= 0) is a ﬁxed vector.
8. V = R2, and S consists of all vectors (x, y) satisfying
x2 −y2 = 0.
9. V = M2(R), and S is the subset of all 2 × 2 matrices
with det(A) = 1.
10. V = Mn(R), and S is the subset of all n × n lower
triangular matrices.
11. V = Mn(R), and S is the subset of all n ×n invertible
matrices.
12. V = M2(R), and S is the subset of all 2 × 2 matrices
whose four elements sum to zero.
13. V = M3×2(R), and S is the subset of all 3 × 2
matrices such that the elements in each column sum
to zero.
14. V = M2×3(R), and S is the subset of all 2×3 matrices
such that the elements in each row sum to 10.
15. V = M2(R), and S is the subset of all 2 × 2 real
symmetric matrices.
16. V is the vector space of all real-valued functions de-
ﬁned on the interval [a, b], and S is the subset of V
consisting of all real-valued functions [a, b] satisfying
f (a) = 5 · f (b).
17. V is the vector space of all real-valued functions de-
ﬁned on the interval [a, b], and S is the subset of V
consisting of all real-valued functions [a, b] satisfying
f (a) = 1.
18. V is the vector space of all real-valued functions de-
ﬁned on the interval (−∞, ∞), and S is the subset
of V consisting of all real-valued functions satisfying
f (−x) = f (x) for all x ∈(−∞, ∞).
19. V = P2(R), and S is the subset of P2(R) consisting
of all polynomials of the form p(x) = ax2 + b.
20. V = P2(R), and S is the subset of P2(R) consisting
of all polynomials of the form p(x) = ax2 + 1.
21. V = C2(I), and S is the subset of V consisting of
those functions satisfying the differential equation
y′′ + 2y′ −y = 0
on I.
22. V = C2(I), and S is the subset of V consisting of
those functions satisfying the differential equation
y′′ + 2y′ −y = 1
on I.
For Problems 23–29, determine the null space of the given
matrix A.
23. A =
! 1 4 "
.
24. A =
! 1 −3 2 "
.
25. A =
⎡
⎣
2 −4
1
2
−3 −5
⎤
⎦.
26. A =
' 1 2 3 4
5 6 7 8
(
.
27. A =
⎡
⎣
1 −2
1
4 −7 −2
−1
3
4
⎤
⎦.
28. A =
⎡
⎣
1
3 −2
1
3 10 −4
6
2
5 −6 −1
⎤
⎦.
29. A =
⎡
⎣
1
i
−2
3
4i
−5
−1 −3i
i
⎤
⎦.
30. Show that the set of all solutions to the nonhomoge-
neous differential equation
y′′ + a1y′ + a2y = F(x),
where F(x) is nonzero on an interval I, is not a sub-
space of C2(I).

274
CHAPTER 4
Vector Spaces
31. Let S1 and S2 be subspaces of a vector space V . Let
S1 ∪S2 = {v ∈V : v ∈S1 or v ∈S2},
S1 ∩S2 = {v ∈V : v ∈S1 and v ∈S2},
and let
S1 + S2 = {v ∈V : v = x + y for some x ∈S1 and y ∈S2}.
(a) Show that, in general, S1 ∪S2 is not a subspace
of V .
(b) Show that S1 ∩S2 is a subspace of V .
(c) Show that S1 + S2 is a subspace of V .
4.4
Spanning Sets
The only algebraic operations that are deﬁned in a vector space V are those of addition
and scalar multiplication. Consequently, the most general way in which we can combine
the vectors v1, v2, . . . , vk in V is
c1v1 + c2v2 + · · · + ckvk,
(4.4.1)
where c1, c2, . . . , ck are scalars. An expression of the form (4.4.1) is called a linear com-
bination of v1, v2, . . . , vk. Since V is closed under addition and scalar multiplication, it
follows that the foregoing linear combination is itself a vector in V . One of the questions
that we wish to answer is whether every vector in a vector space can be obtained by
taking linear combinations of a ﬁnite set of vectors. The following terminology is used
in the case when the answer to this question is afﬁrmative:
DEFINITION
4.4.1
If every vector in a vector space V can be written as a linear combination of v1,
v2, . . . , vk, we say that V is spanned or generated by v1, v2, . . . , vk and call the
set of vectors {v1, v2, . . . , vk} a spanning set for V . In this case, we also say that
{v1, v2, . . . , vk} spans V .
This spanning idea was introduced at the end of the preceding section within the
frameworkof differential equations. Inaddition, weareall usedtorepresentinggeometric
vectors in R3 in terms of their components as (see Section 4.1)
v = ai + bj + ck,
where i, j, and k denote the unit vectors pointing along the positive x-,y-, and z-axes,
respectively, of a rectangular Cartesian coordinate system. Using the above terminology,
we say that v has been expressed as a linear combination of the vectors i, j, and k, and
that the vector space R3 is spanned by i, j, and k.
We now consider several examples to illustrate the spanning concept in different
vector spaces.
Example 4.4.2
Show that R2 is spanned by the vectors
v1 = (1, 1)
and
v2 = (2, −1).
Solution:
We must establish that for every v = (x1, x2) in R2, there exist constants
c1 and c2 such that
v = c1v1 + c2v2.
(4.4.2)

4.4
Spanning Sets 275
That is, in component form,
(x1, x2) = c1(1, 1) + c2(2, −1).
Equating corresponding components in this equation yields the following linear system:
c1 + 2c2 = x1,
c1 −c2 = x2.
In this system, we view x1 and x2 as ﬁxed, while the variables we must solve for are c1
and c2. The determinant of the matrix of coefﬁcients of this system is
1
2
1 −1 = −3.
Since this is nonzero regardless of the values of x1 and x2, the matrix of coefﬁcients is
invertible, and hence for all (x1, x2) ∈R2, the system has a (unique) solution according
to Theorem 2.6.5. Thus, Equation (4.4.2) can be satisﬁed for any vector v ∈R2, and so,
the given vectors do span R2.
□
Remark
In Example 4.4.2 we are not actually required to solve the system obtained
for the unknowns c1 and c2. We simply need to show that the system has a solution. This
is not an uncommon scenario in theoretical mathematics. It is possible, of course, to go
further and solve the linear system explicitly. For the interested reader, solving the linear
system yields
c1 = 1
3(x1 + 2x2),
c2 = 1
3(x1 −x2).
Hence,
(x1, x2) = 1
3(x1 + 2x2)v1 + 1
3(x1 −x2)v2.
For example, if v = (2, 1), then c1 = 4
3 and c2 = 1
3, so that v = 4
3v1 + 1
3v2. This is
illustrated in Figure 4.4.1.
v
x
y
v1
v2
(4/3, 4/3)
(2, 1)
(2,21)
(2/3,21/3)
(1, 1)
Figure 4.4.1: The vector
v = (2, 1) expressed as a linear
combination of v1 = (1, 1) and
v2 = (2, −1).
More generally, any two nonzero and noncollinear vectors v1 and v2 in R2 span R2,
since, as illustrated geometrically in Figure 4.4.2, every vector in R2 can be written as a
linear combination of v1 and v2.
v 5 c1v1 1 c2v2
x
y
v1
c2v2
c1v1
v2
Figure 4.4.2: Any two
noncollinear vectors in R2
span R2.
Example 4.4.3
Determine whether the vectors v1 = (1, −3, 6), v2 = (1, −4, 2), and v3 = (−2, 10, 4)
span R3.
Solution:
Let v = (x1, x2, x3) be an arbitrary vector in R3. We must determine
whether there are real numbers c1, c2, c3 such that
v = c1v1 + c2v2 + c3v3
(4.4.3)
or, in component form,
(x1, x2, x3) = c1(1, −3, 6) + c2(1, −4, 2) + c3(−2, 10, 4).
Equating corresponding components on either side of this vector equation yields
c1 + c2 −2c3 = x1,
−3c1 −4c2 + 10c3 = x2,
6c1 + 2c2 + 4c3 = x3.

276
CHAPTER 4
Vector Spaces
Reducing the augmented matrix for this system to row-echelon form, we obtain
⎡
⎣
1 1 −2
x1
0 1 −4
−3x1 −x2
0 0
0 −18x1 −4x2 + x3
⎤
⎦.
It follows that the system is consistent if and only if x1, x2, x3 satisfy
−18x1 −4x2 + x3 = 0.
(4.4.4)
Consequently, Equation (4.4.3) holds only for those vectors v = (x1, x2, x3) in R3
whose components satisfy Equation (4.4.4). Hence, v1, v2, and v3 do not span R3.
Geometrically, Equation (4.4.4) is the equation of a plane through the origin in space,
and so by taking linear combinations of the given vectors v1, v2, and v3, we can obtain
only those vectors that lie on this plane. We leave it as an exercise to verify that indeed
the three given vectors lie in the plane with Equation (4.4.4). It is worth noting that this
plane forms a subspace S of R3, and that while V is not spanned by the vectors v1, v2,
and v3, S is.
□
The reason that the vectors in the previous example did not span R3 was because
they were coplanar. In general, any three noncoplanar vectors v1, v2, and v3 in R3
span R3 since, as illustrated in Figure 4.4.3, every vector in R3 can be written as a
linear combination of v1, v2, and v3. In the coming sections, we will make this same
observation from a more algebraic point of view.
z
y
x
c1v1
c2v2
c3v3
v1
v2
v3
v 5 c1v1 1 c2v2 1 c3v3
Figure 4.4.3: Any three noncoplanar vectors in R3 span R3.
Notice in the previous example that the linear combination (4.4.3) can be written as
the matrix equation
Ac = v,
where c is the column vector of unknowns c =
⎡
⎣
c1
c2
c3
⎤
⎦and the columns of A are the
given vectors v1, v2, and v3: A = [v1, v2, v3]. Thus, the question of whether or not the
vectors v1, v2, and v3 span R3 can be formulated as follows: Does the system Ac = v
have a solution c for every v in R3? If so, then the column vectors of A span R3, and
if not, then the column vectors of A do not span R3. This reformulation applies more
generally to vectors in Rn, and we state it here for the record.

4.4
Spanning Sets 277
Theorem 4.4.4
Let v1, v2, . . . , vk be vectors in Rn. Then {v1, v2, . . . , vk} spans Rn if and only if, for
the matrix A = [v1, v2, . . . , vk], the linear system Ac = v is consistent for every
v in Rn.
Proof Rewriting the system Ac = v as the linear combination
c1v1 + c2v2 + · · · + ckvk = v,
we see that the existence of a solution (c1, c2, . . . , ck) to this vector equation for each v
in Rn is equivalent to the statement that {v1, v2, . . . , vk} spans Rn.
Next, we consider a couple of examples involving vector spaces other than Rn.
Example 4.4.5
Verify that A1 =
% 1 0
0 0
&
, A2 =
% 1 1
0 0
&
, A3 =
% 1 1
1 0
&
, and A4 =
% 1 1
1 1
&
span
M2(R).
Solution:
An arbitrary vector in M2(R) is of the form A =
% a
b
c
d
&
. If we write
c1 A1 + c2 A2 + c3A3 + c4 A4 = A, then equating the elements of the matrices on each
side of the equation yields the system
c1 + c2 + c3 + c4 = a,
c2 + c3 + c4 = b,
c3 + c4 = c,
c4 = d.
Solving this by back substitution gives
c1 = a −b,
c2 = b −c,
c3 = c −d,
c4 = d.
(4.4.5)
Hence, we have
A = (a −b)A1 + (b −c)A2 + (c −d)A3 + d A4.
Consequently every vector in M2(R) can be written as a linear combination of A1, A2,
A3, and A4, and therefore, these matrices do indeed span M2(R). Of course, we note once
more that the actual solution (4.4.5) for c1, c2, c3, and c4 is not required here—merely
the observation that the system can be solved is sufﬁcient.
□
Remark
The most natural spanning set for M2(R) is
)% 1 0
0 0
&
,
% 0 1
0 0
&
,
% 0 0
1 0
&
,
% 0 0
0 1
&*
,
a fact that we leave to the reader as an exercise.
Example 4.4.6
Determine a spanning set for P2(R), the vector space of all polynomials of degree 2
or less.
Solution:
The general polynomial in P2(R) is
p(x) = a0 + a1x + a2x2.

278
CHAPTER 4
Vector Spaces
If we let
p0(x) = 1,
p1(x) = x,
p2(x) = x2,
then
p(x) = a0 p0(x) + a1 p1(x) + a2 p2(x).
Thus, every vector in P2(R) is a linear combination of 1, x, and x2, and so a spanning set
for P2(R) is {1, x, x2}. For practice, the reader might show that {x2, x + x2, 1 + x + x2}
is another spanning set for P2(R), by making the appropriate modiﬁcations to the cal-
culations in this example.
□
We will look at a couple more examples of a similar nature to Example 4.4.6 shortly,
but to facilitate this, it is useful at this point to discuss in more detail the concept of the
span of a ﬁnite set of vectors.
The Linear Span of a Set of Vectors
Let v1, v2, . . . , vk be vectors in a vector space V . Forming all possible linear combinations
of v1, v2, . . . , vk generates a subset of V called the linear span of {v1, v2, . . . , vk},
denoted span{v1, v2, . . . , vk}. We have
span{v1, v2, . . . , vk} = {v ∈V : v = c1v1 + c2v2 + · · · + ckvk , c1, c2, . . . , ck ∈F}.
(4.4.6)
For example, suppose V = C2(I), and let y1(x) = sin x and y2(x) = cos x. Then
span{y1, y2} = {y ∈C2(I) : y(x) = c1 cos x + c2 sin x,
for some c1, c2 ∈R}.
From Example 1.2.13, we recognize y1 and y2 as being nonproportional solutions to
the differential equation y′′ + y = 0. Consequently, in this example, the linear span of
the given functions coincides with the set of all solutions to the differential equation
y′′ + y = 0 and therefore is a subspace of V . Our next theorem generalizes this to show
that any linear span of vectors in any vector space forms a subspace.
Theorem 4.4.7
Let v1, v2, . . . , vk be vectors in a vector space V . Then span{v1, v2, . . . , vk} is a subspace
of V .
Proof Let S = span{v1, v2, . . . , vk}. Then 0 ∈S (corresponding to c1 = c2 = · · · =
ck = 0 in (4.4.6)), so S is nonempty. We now verify closure of S under addition and
scalar multiplication. If v and w are in S, then, from Equation (4.4.6),
v = a1v1 + a2v2 + · · · + akvk
and
w = b1v1 + b2v2 + · · · + bkvk,
for some scalars ai, bi. Thus,
v + w = (a1v1 + a2v2 + · · · + akvk) + (b1v1 + b2v2 + · · · + bkvk)
= (a1 + b1)v1 + (a2 + b2)v2 + · · · + (ak + bk)vk
= c1v1 + c2v2 + · · · + ckvk,
where ci = ai + bi for each i = 1, 2, . . . , k. Consequently, v + w has the proper form
for membership in S according to (4.4.6), so S is closed under addition. Further, if r is
any scalar, then
rv = r(a1v1 + a2v2 + · · · + akvk) = (ra1)v1 + (ra2)v2 + · · · + (rak)vk
= d1v1 + d2v2 + · · · + dkvk,

4.4
Spanning Sets 279
where di = rai for each i = 1, 2, . . . , k. Consequently, rv ∈S, and so S is also closed
under scalar multiplication. Hence, S = span{v1, v2, . . . , vk} is a subspace of V .
Remarks
1. As a special case, we will declare that span(∅) = {0}.
2. We will also refer to span{v1, v2, . . . , vk} as the subspace of V spanned by
v1, v2, . . . , vk.
3. In general, Theorem 4.4.7 provides an effective way to produce subspaces of
a given vector space V that is already in place. One simply needs to take any
ﬁnite set of vectors in V and form their linear span. While a given set of vectors
{v1, v2, . . . , vk} in a vector space V may or may not span the whole of V , the
deﬁnition of the linear span of a set of vectors given above makes it clear that this
set does span the subspace S = span{v1, v2, . . . , vk}.
Example 4.4.8
If V = R2 and v1 = (−1, 1), determine span{v1}.
Solution:
We have
span{v1} = {v ∈R2 : v = c1v1, c1 ∈R} = {v ∈R2 : v = c1(−1, 1), c1 ∈R}
= {v ∈R2 : v = (−c1, c1), c1 ∈R}.
Geometrically, this is the line through the origin with parametric equations x = −c1,
y = c1, so that the Cartesian equation of the line is y = −x. (See Figure 4.4.4.)
y
x
(2c1, c1)
(21, 1)v1
The subspace of R2 spanned
by the vector v1 5 (21, 1)
c1v1
Figure 4.4.4: The subspace of R2 spanned by v1 = (−1, 1).
□
Example 4.4.9
If V = R3 and v1 = (1, 0, 1) and v2 = (0, 1, 1), determine the subspace of R3 spanned
by v1 and v2. Does w = (1, 1, −1) lie in this subspace?
Solution:
We have
span{v1, v2} = {v ∈R3 : v = c1v1 + c2v2, c1, c2 ∈R}
= {v ∈R3 : v = c1(1, 0, 1) + c2(0, 1, 1), c1, c2 ∈R}
= {v ∈R3 : v = (c1, c2, c1 + c2), c1, c2 ∈R}.
The vector w = (1, 1, −1) is not of the form (c1, c2, c1 + c2), since the system c1 = 1,
c2 = 1, and c1 + c2 = −1 clearly has no solutions. Therefore, w does not lie in
span{v1, v2}. Geometrically, span{v1, v2} is the plane through the origin determined
by the two given vectors v1 and v2. It has parametric equations x = c1, y = c2,
z = c1 + c2, which implies that its Cartesian equation is z = x + y. Thus, the fact that
w is not in span{v1, v2} means that w does not lie in this plane. The subspace is depicted
in Figure 4.4.5.
□

280
CHAPTER 4
Vector Spaces
z
y
x
v1
v2
w 5 (1, 1, 21) does not
lie in span{v1, v2}
The subspace of R3 spanned
by v1 5 (1, 0, 1), v2 5 (0, 1, 1)
Figure 4.4.5: The subspace of R3 spanned by v1 = (1, 0, 1) and v2 = (0, 1, 1) is the plane
with Cartesian equation z = x + y.
Example 4.4.10
Let A1 =
! 1 0
0 0
"
, A2 =
! 0 1
1 0
"
, A3 =
! 0 0
0 1
"
in M2(R).Determinespan{A1, A2, A3}.
Solution:
By deﬁnition we have
span{A1, A2, A3}
= {A ∈M2(R) : A = c1 A1 + c2 A2 + c3A3, c1, c2, c3 ∈R}
= {A ∈M2(R) : A = c1
! 1 0
0 0
"
+ c2
! 0 1
1 0
"
+ c3
! 0 0
0 1
"
, c1, c2, c3 ∈R}
= {A ∈M2(R) : A =
! c1
c2
c2
c3
"
, c1, c2, c3 ∈R}.
This is the set of all real 2 × 2 symmetric matrices.
□
Example 4.4.11
Determine the subspace of P2(R) spanned by
p1(x) = 1 + 3x,
p2(x) = x + x2,
and decide whether {p1(x), p2(x)} is a spanning set for P2(R).
Solution:
We have
span{p1(x), p2(x)} = {p(x) ∈P2(R) : p(x) = c1 p1(x) + c2 p2(x), c1, c2 ∈R}
= {p(x) ∈P2(R) : p(x) = c1(1 + 3x) + c2(x + x2), c1, c2 ∈R}
= {p(x) ∈P2(R) : p(x) = c1 + (3c1 + c2)x + c2x2, c1, c2 ∈R}.
Next, we will show that {p1(x), p2(x)} is not a spanning set for P2(R). A general element
in P2(R) has the form p(x) = a +bx +cx2 for some real scalars a, b, and c. We can see
from the last expression for span{p1(x), p2(x)} above that unless b = 3a + c, p(x) will
not belong to span{p1(x), p2(x)}. Therefore, since some polynomials in P2(R) will not
have this form, {p1(x), p2(x)} is not a spanning set for P2(R). In Section 4.6, we will see
from a theoretical point of view how we could have drawn this conclusion immediately,
without even computing what span{p1(x), p2(x)} is.
□
In the last few examples, we have started with some vectors in a vector space V
and computed their linear span. Let us conclude this section with a couple of examples
where we turn this process around and instead seek a set of vectors that will span V .

4.4
Spanning Sets 281
Example 4.4.12
Find a spanning set for the vector space V of all 3 × 3 skew-symmetric matrices.
Solution:
Recall from Example 4.3.6 that V does indeed form a vector space. A
typical element of V can be written as
A =
⎡
⎣
0
a
b
−a
0
c
−b −c
0
⎤
⎦.
To ﬁnd a spanning set for V , we express A as a linear combination as follows:
A = a
⎡
⎣
0 1 0
−1 0 0
0 0 0
⎤
⎦+ b
⎡
⎣
0 0 1
0 0 0
−1 0 0
⎤
⎦+ c
⎡
⎣
0
0 0
0
0 1
0 −1 0
⎤
⎦.
(4.4.7)
Therefore, we can write every general element in V as a linear combination of the three
matrices appearing on the right side of (4.4.7). Thus, one spanning set for V is given by
⎧
⎨
⎩
⎡
⎣
0 1 0
−1 0 0
0 0 0
⎤
⎦,
⎡
⎣
0 0 1
0 0 0
−1 0 0
⎤
⎦,
⎡
⎣
0
0 0
0
0 1
0 −1 0
⎤
⎦
⎫
⎬
⎭.
□
Example 4.4.13
Find a spanning set for the null space of the matrix A =
+ −1
5
3
2 −10 −6
,
.
Solution:
We begin by computing the null space of A, as described in Section
4.3. A short calculation gives us the following row-echelon form for the matrix A#:
+ 1 −5 −3 0
0
0
0 0
,
. Therefore, the null space of A is precisely the set of points (x, y, z)
in R3 such that x −5y −3z = 0. Setting y = s and z = t, we have x = 5s + 3t. Thus,
elements in the null space of A can be written in the form (5s + 3t, s, t), where s, t ∈R.
To ﬁnd a spanning set, we write
(5s + 3t, s, t) = s(5, 1, 0) + t(3, 0, 1),
so that {(5, 1, 0), (3, 0, 1)} is a spanning set for the null space of A.
□
In both of the last two examples, the key step is to take a general element of the
vector space in question, expressed using arbitrary parameters, and then “bust out” those
parameters as scalars in a linear combination in order to obtain the vectors in a spanning
set. This is an important skill that the reader should practice and master, as it will be
needed many times in what lies ahead.
Exercises for 4.4
Key Terms
Linear combination, Linear span, Spanning set.
Skills
• Be able to determine whether a given set of vectors
S spans a vector space V , and be able to prove your
answer mathematically.
• Be able to determine the linear span of a set of vec-
tors. For vectors in R, R2, and R3 be able to give a
geometric description of the linear span.
• If S is a spanning set for a vector space V , be able to
write any vector in V as a linear combination of the
elements of S.

282
CHAPTER 4
Vector Spaces
• Be able to construct a spanning set for a vector space
V . As a special case, be able to determine a spanning
set for the null space of an m × n matrix.
• Be able to determine whether a particular vector v in
a vector space V lies in the linear span of a set S of
vectors in V .
True-False Review
For Questions (a)–(l), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The linear span of a set of vectors in a vector space V
forms a subspace of V .
(b) If some vector v in a vector space V is a linear com-
bination of vectors in a set S, then S spans V .
(c) If S is a spanning set for a vector space V and W is a
subspace of V , then S is a spanning set for W.
(d) If S is a spanning set for a vector space V , then every
vector v in V must be uniquely expressible as a linear
combination of the vectors in S.
(e) A set S of vectors in a vector space V spans V if and
only if the linear span of S is V .
(f) The linear span of two vectors in R3 must be a plane
through the origin.
(g) Every vector space V has a ﬁnite spanning set.
(h) If S is a spanning set for a vector space V , then any
proper subset S′ of S (i.e., S′ ̸= S) not a spanning set
for V .
(i) The vector space of 3 × 3 upper triangular matrices is
spanned by the matrices Ei j where 1 ≤i ≤j ≤3.
(j) Aspanningsetforthevectorspace P2(R)mustcontain
a polynomial of each degree 0,1, and 2.
(k) If m < n, then any spanning set for Rn must contain
more vectors than any spanning set for Rm.
(l) The vector space P(R) of all polynomials with real
coefﬁcients cannot be spanned by a ﬁnite set S.
Problems
For Problems 1–4, determine whether the given set of vectors
spans R2.
1. {(5, −1)}
2. {(1, −1), (2, −2), (2, 3)}.
3. {(2, 5), (0, 0)}.
4. {(6, −2), (−2, 2/3), (3, −1)}.
Recall that three vectors v1, v2, v3 in R3 are coplanar if and
only if
det([v1, v2, v3]) = 0.
For Problems 5–8, use this result to determine whether the
given set of vectors spans R3.
5. {(1, −1, 1), (2, 5, 3), (4, −2, 1)}.
6. {(1, −2, 1), (2, 3, 1), (4, −1, 2)}.
7. {(2, −1, 4), (3, −3, 5), (1, 1, 3)}.
8. {(1, 2, 3), (4, 5, 6), (7, 8, 9)}.
9. Show that the set of vectors
{(−4, 1, 3), (5, 1, 6), (6, 0, 2)}
does not span R3, but that it does span the subspace
of R3 consisting of all vectors lying in the plane with
equation x + 13y −3z = 0.
10. Show that the set of vectors
{(1, 2, 3), (3, 4, 5), (4, 5, 6)}
does not span R3, but that it does span the subspace
of R3 consisting of all vectors lying in the plane with
equation x −2y + z = 0.
11. Show that v1 = (2, −1), v2 = (3, 2) span R2 and ex-
press the vector v = (5, −7) as a linear combination
of v1, v2.
12. Show that v1 = (1, −5), v2 = (6, 3) span R2, and
express the vector v = (x, y) as a linear combination
of v1, v2.
13. Show that v1 = (1, −3, 2), v2 = (1, 0, −1), v3 =
(1, 2, −4) span R3, and express v = (9, 8, 7) as a
linear combination of v1, v2, v3.
14. Show that v1 = (−1, 3, 2), v2 = (1, −2, 1), v3 =
(2, 1, 1) span R3, and express v = (x, y, z) as a linear
combination of v1, v2, v3.
15. Show that v1 = (1, 1), v2 = (−1, 2), v3 = (1, 4) span
R2. Do v1, v2 alone span R2 also?
16. Let S be the subspace of R3 consisting of all vectors
of the form v = (c1, c2, c2 −2c1). Determine a set of
vectors that spans S.

4.4
Spanning Sets 283
17. Let S be the subspace of R4 consisting of all vectors
of the form v = (c1, c2, c2 −c1, c1 −2c2). Determine
a set of vectors that spans S.
18. Let S be the subspace of M2(R) consisting of all skew-
symmetric 2 × 2 matrices with real elements. Deter-
mine a matrix that spans S.
19. Let S be the subset of M2(R) consisting of all upper
triangular 2 × 2 matrices.
(a) Verify that S is a subspace of M2(R).
(b) Determine a set of 2 × 2 matrices that span S.
20. Let S be the subspace of M2(R) consisting of all
2 × 2 matrices whose four elements sum to zero (see
Problem 12 in Section 4.3). Find a set of vectors that
spans S.
21. Let S be the subspace of M3(R) consisting of all
3 × 3 matrices such that the elements in each row and
each column sum to zero. Find a set of vectors that
spans S.
22. Let S be the subspace of M3(R) consisting of all 3×3
symmetric matrices. Find a set of vectors that spans S.
23. Let S be the subspace of R3 consisting of all solutions
to the linear system
x −2y −z = 0.
Determine a set of vectors that spans S.
24. Let S be the subspace of P3(R) consisting of all poly-
nomials p(x) in P3(R) such that p′(x) = 0. Find a set
of vectors that spans S.
For Problems 25–33, determine a spanning set for the null
space of the given matrix A.
25. The matrix A deﬁned in Problem 23 in Section 4.3.
26. The matrix A deﬁned in Problem 24 in Section 4.3.
27. The matrix A deﬁned in Problem 25 in Section 4.3.
28. The matrix A deﬁned in Problem 26 in Section 4.3.
29. The matrix A deﬁned in Problem 27 in Section 4.3.
30. The matrix A deﬁned in Problem 28 in Section 4.3.
31. The matrix A deﬁned in Problem 29 in Section 4.3.
32. A =
⎡
⎣
1 2 3
5
1 3 4
2
2 4 6 −1
⎤
⎦.
33. A =
⎡
⎣
1 2 3
3 4 5
5 6 7
⎤
⎦.
For Problems 34–35, determine span{v1, v2} for the given
vectors in R3, and describe it geometrically.
34. v1 = (1, −1, 2), v2 = (2, −1, 3).
35. v1 = (1, 2, −1), v2 = (−2, −4, 2).
36. Let S be the subspace of R3 spanned by the vectors
v1 = (1, 1, −1), v2 = (2, 1, 3), v3 = (−2, −2, 2).
Show that S is also spanned by v1 and v2 only.
For Problems 37–39, determine whether the given vector v
lies in span{v1, v2}.
37. v = (3, 3, 4), v1 = (1, −1, 2), v2 = (2, 1, 3) in R3.
38. v = (5, 3, −6), v1 = (−1, 1, 2), v2 = (3, 1, −4)
in R3.
39. v = (1, 1, −2), v1 = (3, 1, 2), v2 = (−2, −1, 1)
in R3.
40. If p1(x) = x −4 and p2(x) = x2 −x + 3, determine
whether p(x) = 2x2 −x + 2 lies in span{p1, p2}.
41. Consider the vectors
A1 =
% 1 −1
2
0
&
, A2 =
%
0 1
−2 1
&
, A3 =
% 3 0
1 2
&
in M2(R). Determine span{A1, A2, A3}.
42. Consider the vectors
A1 =
%
1 2
−1 3
&
, A2 =
% −2
1
1 −1
&
in M2(R). Find span{A1, A2}, and determine whether
or not B =
%
3 1
−2 4
&
lies in this subspace.
43. Let V = C∞(I) and let S be the subspace of V
spanned by the functions
f (x) = cosh x, g(x) = sinh x.
(a) Give an expression for a general vector in S.
(b) Show that S is also spanned by the functions
h(x) = ex,
j(x) = e−x.

284
CHAPTER 4
Vector Spaces
For Problems 44–47, give a geometric description of the sub-
space of R3 spanned by the given set of vectors.
44. {0}.
45. {v1}, where v1 is any nonzero vector in R3.
46. {v1, v2}, where v1, v2 are nonzero and noncollinear
vectors in R3.
47. {v1, v2}, where v1, v2 are collinear vectors in R3.
48. Prove that if S and S′ are subsets of a vector space V
such that S is a subset of S′, then span(S) is a subset
of span(S′).
49. Prove that
span{v1, v2, v3} = span{v1, v2}
if and only if v3 can be written as a linear combination
of v1 and v2.
4.5
Linear Dependence and Linear Independence
As indicated in the previous section, in analyzing a vector space we will be interested in
determining a spanning set. The reader has perhaps already noticed that a vector space
V can have many such spanning sets.
Example 4.5.1
Observe that {(1, 0), (0, 1)}, {(1, 0), (1, 1)}, and {(1, 0), (0, 1), (1, 2)} are three different
spanning sets for R2.
□
As another illustration, two different spanning sets for V = M2(R) were given in Exam-
ple 4.4.5 and the remark that followed. Given the abundance of spanning sets available
for a given vector space V , we are faced with a natural question: Is there a “best class”
of spanning sets to use? The answer, to a large degree, is yes. For instance, in Example
4.5.1, the spanning set {(1, 0), (0, 1), (1, 2)} contains an “extra” vector, (1, 2), which
seems to be unnecessary for spanning R2, since {(1, 0), (0, 1)} is already a spanning
set. In some sense, {(1, 0), (0, 1)} is a more efﬁcient spanning set. It is what we call a
minimal spanning set, since it contains the minimum number of vectors needed to span
the vector space.4
But how will we know if we have found a minimal spanning set (assuming one
exists)? Returning to the example above, we have seen that
span{(1, 0), (0, 1)} = span{(1, 0), (0, 1), (1, 2)} = R2.
Observe that the vector (1, 2) is already a linear combination of (1, 0) and (0, 1), and
therefore, it does not add any new vectors to the linear span of {(1, 0), (0, 1)}.
As a second example, consider the vectors v1 = (1, 1, 1), v2 = (3, −2, 1), and
v3 = 4v1 + v2 = (7, 2, 5). It is easily veriﬁed that det([v1, v2, v3]) = 0. Consequently,
the three vectors lie in a plane (see Figure 4.5.1) and therefore, since they are not collinear,
the linear span of these three vectors is the whole of this plane. Furthermore, the same
plane is generated if we consider the linear span of v1 and v2 alone. As in the previous
example, the reason that v3 does not add any new vectors to the linear span of {v1, v2} is
that it is already a linear combination of v1 and v2. It is not possible, however, to generate
all vectors in the plane by taking linear combinations of just one of the given vectors, as
we could only generate a line lying in the plane in that case. Consequently, {v1, v2} is a
minimal spanning set for the subspace of R3 consisting of all points lying on the plane.
As a ﬁnal example, recall from Example 1.2.13 that the solution space to the differ-
ential equation
y′′ + y = 0
4Since a single (nonzero) vector in R2 only spans the line through the origin along which it points, it cannot
span all of R2; hence, the minimum number of vectors required to span R2 is 2.

4.5
Linear Dependence and Linear Independence 285
z
y
x
(7, 2, 5)
(3,22, 1)
(3,22, 0)
(1, 1, 0)
(1, 1, 1)
(7, 2, 0)
v3 5 4v1 1 v2
v1
v2
Figure 4.5.1: v3 = 4v1 + v2 lies in the plane through the origin containing v1 and v2, and so,
span{v1, v2, v3} = span{v1, v2}.
can be written as span{y1, y2}, where y1(x) = cos x and y2(x) = sin x. However, if we
let y3(x) = 3 cos x −2 sin x, for instance, then {y1, y2, y3} is also a spanning set for the
solution space of the differential equation, since
span{y1, y2, y3} = {c1 cos x + c2 sin x + c3(3 cos x −2 sin x) : c1, c2, c3 ∈R}
= {(c1 + 3c3) cos x + (c2 −2c3) sin x : c1, c2, c3 ∈R}
= {d1 cos x + d2 sin x : d1, d2 ∈R}
= span{y1, y2}.
The reason that {y1, y2, y3} is not a minimal spanning set for the solution space is that
y3 is a linear combination of y1 and y2, and therefore, as we have just shown, it does not
add any new vectors to the linear span of {cos x, sin x}.
More generally, it is not too difﬁcult to extend the argument used in the preceding
examples to establish the following general result.
Theorem 4.5.2
Let {v1, v2, . . . , vk} be a set of at least two vectors in a vector space V . If one of the
vectors in the set is a linear combination of the other vectors in the set, then that vector
can be deleted from the given set of vectors and the linear span of the resulting set of
vectors will be the same as the linear span of {v1, v2, . . . , vk}.
Proof The proof of this result is left for the exercises (Problem 51).
For instance, if v1 is a linear combination of v2, v3, . . . , vk, then Theorem 4.5.2 says
that
span{v1, v2, . . . , vk} = span{v2, v3, . . . , vk}.
In this case, the set {v1, v2, . . . , vk} is not a minimal spanning set.
To determine a minimal spanning set, the problem that we are faced with in view of
Theorem 4.5.2 is that of determining when a vector in {v1, v2, . . . , vk} can be expressed
as a linear combination of the remaining vectors in the set. The correct formulation for
solving this problem requires the concepts of linear dependence and linear independence,
which we are now ready to introduce. First we consider linear dependence.

286
CHAPTER 4
Vector Spaces
DEFINITION
4.5.3
A ﬁnite nonempty set of vectors {v1, v2, . . . , vk} in a vector space V is said to be
linearly dependent if there exist scalars c1, c2, . . . , ck, not all zero, such that
c1v1 + c2v2 + · · · + ckvk = 0.
Such a nontrivial linear combination of vectors is sometimes referred to as a linear
dependency among the vectors v1, v2, . . . , vk.
A set of vectors that is not linearly dependent is called linearly independent. This can be
stated mathematically as follows:
DEFINITION
4.5.4
A ﬁnite, nonempty set of vectors {v1, v2, . . . , vk} in a vector space V is said to be
linearly independent if the only values of the scalars c1, c2, . . . , ck for which
c1v1 + c2v2 + · · · + ckvk = 0
are c1 = c2 = · · · = ck = 0.
Remarks
1. It follows immediately from the preceding two deﬁnitions that a nonempty set of
vectors in a vector space V is linearly independent if and only if it is not linearly
dependent.
2. If{v1, v2, . . . , vk}isalinearlyindependentsetofvectors,wesometimesinformally
say that the vectors v1, v2, . . . , vk are themselves linearly independent. The same
remark applies to the linearly dependent condition as well.
Consider the simple case of a set containing a single vector v. If v = 0, then {v} is
linearly dependent, since for any nonzero scalar c1,
c10 = 0.
On the other hand, if v ̸= 0, then the only value of the scalar c1 for which
c1v = 0
is c1 = 0. Consequently, {v} is linearly independent. We can therefore state the next
theorem.
Theorem 4.5.5
A set consisting of a single vector v in a vector space V is linearly dependent if and only
if v = 0. Therefore, any set consisting of a single nonzero vector is linearly independent.
We next establish that linear dependence of a set containing at least two vectors is
equivalent to the property that we are interested in; namely, that at least one vector in
the set can be expressed as a linear combination of the remaining vectors in the set.
Theorem 4.5.6
Let {v1, v2, . . . , vk} be a set of at least two vectors in a vector space V . Then {v1, v2, . . . ,
vk} is linearly dependent if and only if at least one of the vectors in the set can be expressed
as a linear combination of the others.

4.5
Linear Dependence and Linear Independence 287
Proof If {v1, v2, . . . , vk} is linearly dependent, then according to Deﬁnition 4.5.3, there
exist scalars c1, c2, . . . , ck, not all zero, such that
c1v1 + c2v2 + · · · + ckvk = 0.
Suppose that ci ̸= 0. Then we can express vi as a linear combination of the other vectors
as follows:
vi = −1
ci
(c1v1 + c2v2 + · · · + ci−1vi−1 + ci+1vi+1 + · · · + ckvk).
Conversely, suppose that one of the vectors, say, v j, can be expressed as a linear combi-
nation of the remaining vectors. That is,
v j = c1v1 + c2v2 + · · · + c j−1v j−1 + c j+1v j+1 + · · · + ckvk.
Adding (−1)v j to both sides of this equation yields
c1v1 + c2v2 + · · · + c j−1v j−1 −v j + c j+1v j+1 + · · · + ckvk = 0.
Since the coefﬁcient of v j is −1 ̸= 0, the set of vectors {v1, v2, . . . , vk} is linearly
dependent.
As far as the minimal spanning set idea is concerned, Theorems 4.5.6 and 4.5.2 tell
us that a linearly dependent spanning set for a (nontrivial) vector space V cannot be a
minimal spanning set. On the other hand, we will see in the next section that a linearly
independent spanning set for V must be a minimal spanning set for V . For the remainder
of this section, however, we focus more on the mechanics of determining whether a given
set of vectors is linearly independent or linearly dependent. Sometimes this can be done
by inspection. For example, Figure 4.5.2 illustrates that any set of three vectors in R2 is
linearly dependent.
x
y
v1
v2
v3
Figure 4.5.2: The set of vectors
{v1, v2, v3} is linearly dependent in
R2 since v3 is a linear combination
of v1 and v2.
Example 4.5.7
Let V be the vector space of all functions deﬁned on an interval I. If
f1(x) = 1,
f2(x) = 2 sin2 x,
f3(x) = −5 cos2 x,
then { f1, f2, f3} is linearly dependent in V , since the trigonometric identity sin2 x +
cos2 x = 1 implies that for all x ∈I,
f1(x) = 1
2 f2(x) −1
5 f3(x).
We can therefore conclude from Theorem 4.5.2 that
span{1, 2 sin2 x, −5 cos2 x} = span{2 sin2 x, −5 cos2 x}.
□
In relatively simple examples, the following general results can be applied. They
are a direct consequence of the deﬁnition of linearly dependent vectors and are left for
the exercises (Problem 52).

288
CHAPTER 4
Vector Spaces
Proposition 4.5.8
Let V be a vector space.
1. Any set of two vectors in V is linearly dependent if and only if the vectors are
proportional.
2. Any set of vectors in V containing the zero vector is linearly dependent.
Example 4.5.9
If v1 = (1, 2, −9) and v2 = (−2, −4, 18), then {v1, v2} is linearly dependent in R3,
since v2 = −2v1. Geometrically, v1 and v2 lie on the same line.
□
Remark
We emphasize that the ﬁrst result in Proposition 4.5.8 holds only for the
case of two vectors. It cannot be applied to sets containing more than two vectors.
Example 4.5.10
If v1 = (2, 4), v2 = (−3, 1), and v3 = (−1, 5), then {v1, v2, v3} is linearly dependent
in R2 since v1 + v2 −v3 = 0, but no two of these three vectors are proportional.
□
Example 4.5.11
If A1 =
% 2 1
3 4
&
, A2 =
% 0 0
0 0
&
, and A3 =
%
2 5
−3 2
&
, then {A1, A2, A3} is linearly
dependent in M2(R), since it contains the zero vector from M2(R).
□
For more complicated situations, we must resort to Deﬁnitions 4.5.3 and 4.5.4,
although conceptually it is always helpful to keep in mind that the essence of the problem
that we are solving is to determine whether a vector in a given set can be expressed as a
linear combination of the remaining vectors in the set. We now give some examples to
illustrate the use of Deﬁnitions 4.5.3 and 4.5.4.
Example 4.5.12
If v1 = (3, −1, 2), v2 = (−9, 5, −2), and v3 = (−9, 9, 6), show that {v1, v2, v3} is
linearly dependent in R3, and determine the linear dependency relationship.
Solution:
We must ﬁrst establish that there are values of the scalars c1, c2, c3, not all
zero, such that
c1v1 + c2v2 + c3v3 = 0.
(4.5.1)
Substituting for the given vectors yields
c1(3, −1, 2) + c2(−9, 5, −2) + c3(−9, 9, 6) = (0, 0, 0).
That is,
(3c1 −9c2 −9c3, −c1 + 5c2 + 9c3, 2c1 −2c2 + 6c3) = (0, 0, 0).
Equating corresponding components on either side of this equation yields
3c1 −9c2 −9c3 = 0,
−c1 + 5c2 + 9c3 = 0,
2c1 −2c2 + 6c3 = 0.
The reduced row-echelon form of the augmented matrix of this system is
⎡
⎣
1 0 6 0
0 1 3 0
0 0 0 0
⎤
⎦.

4.5
Linear Dependence and Linear Independence 289
Consequently, the system has an inﬁnite number of solutions for c1, c2, c3, and so, the
vectors are linearly dependent.
In order to determine a speciﬁc linear dependency relationship, we proceed to ﬁnd
c1, c2, and c3. Setting c3 = t, we have c2 = −3t and c1 = −6t. Taking t = 1 and
substituting these values for c1, c2, c3 into (4.5.1), we obtain the linear dependency
relationship
−6v1 −3v2 + v3 = 0,
(4.5.2)
which can be easily veriﬁed using the given expressions for v1, v2, and v3. It follows
from Theorem 4.5.2 that
span{v1, v2, v3} = span{v1, v2}.
Geometrically, we can conclude that v3 lies in the plane determined by the vectors v1
and v2. The choice to focus on v3 here is somewhat arbitrary. We could alternatively
draw similar conclusions about any one of the vectors that occurs in the linear depen-
dency (4.5.2).
□
Example 4.5.13
Determine whether the set of polynomials {p1(x), p2(x), p3(x), p4(x)} is linearly de-
pendent or linearly independent in P3(R), where
p1(x) = 1 −4x3,
p2(x) = 2 + 2x,
p3(x) = 1−x2 + 2x3,
p4(x) = 2x −x3.
Solution:
The condition for determining whether these vectors are linearly dependent
or linearly independent,
c1 p1(x) + c2 p2(x) + c3 p3(x) + c4 p4(x) = 0,
is equivalent in this case to
c1(1 −4x3) + c2(2 + 2x) + c3(1 −x2 + 2x3) + c4(2x −x3) = 0,
which is satisﬁed if and only if
c1 + 2c2 +
c3
= 0,
2c2
+ 2c4 = 0,
−c3
= 0,
−4c1
+ 2c3 −c4 = 0.
The augmented matrix for this linear system is
⎡
⎢⎢⎣
1 2
1
0 0
0 2
0
2 0
0 0 −1
0 0
−4 0
2 −1 0
⎤
⎥⎥⎦.
Notice how the coefﬁcients of the given polynomials appear in this matrix. Their coef-
ﬁcients are arranged by columns. The reduced row-echelon form for this matrix is
⎡
⎢⎢⎣
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
⎤
⎥⎥⎦,
which implies that the system has only the trivial solution c1 = c2 = c3 = c4 = 0. It
follows from Deﬁnition 4.5.4 that {p1(x), p2(x), p3(x), p4(x)} is linearly independent.
□

290
CHAPTER 4
Vector Spaces
As a corollary to Theorem 4.5.2, we establish the following result.
Corollary 4.5.14
Any nonempty, ﬁnite set of linearly dependent vectors in a vector space V contains a
linearly independent subset that has the same linear span as the given set of vectors.
Proof Since the given set is linearly dependent, at least one of the vectors in the set
is a linear combination of the remaining vectors, by Theorem 4.5.6. Thus, by Theorem
4.5.2, we can delete that vector from the set, and the resulting set of vectors will span the
same subspace of V as the original set. If the resulting set is linearly independent, then
we are done. If not, then we can repeat the procedure to eliminate another vector in the
set. Continuing in this manner (with a ﬁnite number of iterations), we will obtain a
linearly independent set that spans the same subspace of V as the subspace spanned by
the original set of vectors.
Remark
Corollary 4.5.14 is actually true even if the set of vectors in question is
inﬁnite, but we shall not need to consider that case in this text. In the case of an inﬁnite
set of vectors, other techniques are required for the proof.
Note that the linearly independent set that is obtained using the procedure given in
the previous theorem is not unique, and therefore one question that arises is whether the
number of vectors in any resulting linearly independent set is the same regardless of the
manner in which the procedure is applied. We will give an afﬁrmative answer to this
question in Section 4.6.
Example 4.5.15
Let v1 = (1, 2, 3), v2 = (−1, 1, 4), v3 = (3, 3, 2), and v4 = (−2, −4, −6). Deter-
mine a linearly independent set of vectors that spans the same subspace of R3 that
{v1, v2, v3, v4} does.
Solution:
Setting
c1v1 + c2v2 + c3v3 + c4v4 = 0
requires that
c1(1, 2, 3) + c2(−1, 1, 4) + c3(3, 3, 2) + c4(−2, −4, −6) = (0, 0, 0),
leading to the linear system
c1 −c2 + 3c3 −2c4 = 0,
2c1 + c2 + 3c3 −4c4 = 0,
3c1 + 4c2 + 2c3 −6c4 = 0.
The augmented matrix of this system is
⎡
⎣
1 −1 3 −2 0
2
1 3 −4 0
3
4 2 −6 0
⎤
⎦,
and the reduced row-echelon form of the augmented matrix of this system is
⎡
⎣
1 0
2 −2 0
0 1 −1
0 0
0 0
0
0 0
⎤
⎦.

4.5
Linear Dependence and Linear Independence 291
The system has two free variables, c3 = s and c4 = t, and so {v1, v2, v3, v4} is linearly
dependent. Free variables arise in the solution to this linear system because the third and
fourth columns of the reduced row-echelon form are not pivoted. Thus, we may view the
third and fourth vectors in the list we started with as the impediments to linear indepen-
dence. Indeed, if we remove the third and fourth columns of the augmented matrix, we
will no longer acquire free variables in the solution. Thus, {v1, v2} is the linearly inde-
pendent set of vectors we are seeking. Geometrically, the subspace span{v1, v2, v3, v4}
of R3 is the plane spanned by v1 and v2, and the vectors v3 and v4 lie in this plane.
□
Linear Dependence and Linear Independence in Rn
Let {v1, v2, . . . , vk} be a set of vectors in Rn, and let A denote the matrix that has
v1, v2, . . . , vk as column vectors. Thus,
A = [v1, v2, . . . , vk].
(4.5.3)
Since each of the given vectors is in Rn, it follows that A has n rows and is therefore an
n × k matrix.
The linear combination c1v1 + c2v2 + · · · + ckvk = 0 can be written in matrix form
as (see Theorem 2.2.9)
Ac = 0,
(4.5.4)
where A is given in Equation (4.5.3) and c = [c1 c2 . . . ck]T . Consequently, we can
state the following theorem and corollary:
Theorem 4.5.16
Let v1, v2, . . . , vk be vectors in Rn and A = [v1, v2, . . . , vk]. Then {v1, v2, . . . , vk} is
linearly dependent if and only if the linear system Ac = 0 has a nontrivial solution for c.
Corollary 4.5.17
Let v1, v2, . . . , vk be vectors in Rn and A = [v1, v2, . . . , vk].
1. If k > n, then {v1, v2, . . . , vk} is linearly dependent.
2. If k = n, then {v1, v2, . . . , vk} is linearly dependent if and only if det(A) = 0.
Proof If k > n, the system (4.5.4) has an inﬁnite number of solutions (see Corollary
2.5.11), and hence, the vectors are linearly dependent by Theorem 4.5.16.
On the other hand, if k = n, the system (4.5.4) is n × n, and hence, from Corollary
3.2.6, it has an inﬁnite number of solutions if and only if det(A) = 0.
In the case k < n, further investigation is generally required to assess whether the
given set of vectors is linearly dependent or linearly independent.
Example 4.5.18
Determine whether the given vectors are linearly dependent or linearly independent
in R4.
(a) v1 = (1, 3, −1, 0), v2 = (2, 9, −1, 3), v3 = (4, 5, 6, 11), v4 = (1, −1, 2, 5),
v5 = (3, −2, 6, 7).
(b) v1 = (1, 4, 1, 7), v2 = (3, −5, 2, 3), v3 = (2, −1, 6, 9), v4 = (−2, 3, 1, 6).

292
CHAPTER 4
Vector Spaces
Solution:
(a) Since we have ﬁve vectors in R4, Corollary 4.5.17 implies that {v1, v2, v3, v4, v5}
is necessarily linearly dependent.
(b) In this case, we have four vectors in R4, and therefore, we can use the determinant:
det(A) = det[v1, v2, v3, v4] =
1
3
2 −2
4 −5 −1
3
1
2
6
1
7
3
9
6
= −462.
Since the determinant is nonzero, it follows from Corollary 4.5.17 that the given
set of vectors is linearly independent.
□
Linear Independence of Functions
We now consider the general problem of determining whether or not a given set of
functions is linearly independent or linearly dependent. We begin by specializing the
general Deﬁnition 4.5.4 to the case of a set of functions deﬁned on an interval I.
DEFINITION
4.5.19
The set of functions { f1, f2, . . . , fk} is linearly independent on an interval I if and
only if the only values of the scalars c1, c2, . . . , ck such that
c1 f1(x) + c2 f2(x) + · · · + ck fk(x) = 0,
for all x ∈I,
(4.5.5)
are c1 = c2 = · · · = ck = 0.
The main point to notice is that the condition (4.5.5) must hold for all x in I.
A key tool in deciding whether or not a collection of functions is linearly independent
on an interval I is the Wronskian. As we will see in Chapter 8, we can draw particularly
sharp conclusions from the Wronskian about the linear dependence or independence of
a family of solutions to a differential equation.
DEFINITION
4.5.20
Let f1, f2, . . . , fk be functions in Ck−1(I). The Wronskian of these functions is the
order k determinant deﬁned by
W[ f1, f2, . . . , fk](x) =
f1(x)
f2(x)
. . .
fk(x)
f ′
1(x)
f ′
2(x)
. . .
f ′
k(x)
...
...
...
f (k−1)
1
(x)
f (k−1)
2
(x) . . .
f (k−1)
k
(x)
.
Remark
Notice that the Wronskian is a function deﬁned on I. Also note that this
function depends on the order of the functions in the Wronskian. For example, using
properties of determinants,
W[ f2, f1, . . . , fk](x) = −W[ f1, f2, . . . , fk](x).

4.5
Linear Dependence and Linear Independence 293
Example 4.5.21
If f1(x) = sin x and f2(x) = cos x on (−∞, ∞), then
W[ f1, f2](x) = sin x
cos x
cos x
−sin x = (sin x)(−sin x) −(cos x)(cos x)
= −(sin2 x + cos2 x) = −1.
□
Example 4.5.22
If f1(x) = x, f2(x) = 1
x2 , and f3(x) = 1
x4 on (0, ∞), then for all x > 0, we have
W[ f1, f2, f3](x) =
x
1
x2
1
x4
1
−2
x3
−4
x5
0
6
x4
20
x6
= x
'
−40
x9 + 24
x9
(
−
'20
x8 −6
x8
(
= −30
x8 .
We have used cofactor expansion along the ﬁrst column in evaluating the determinant in
this case.
□
We can now state and prove the main result about the Wronskian.
Theorem 4.5.23
Let f1, f2, . . . , fk be functions in Ck−1(I). If W[ f1, f2, . . . , fk] is nonzero at some
point x0 in I, then { f1, f2, . . . , fk} is linearly independent on I.
Proof To apply Deﬁnition 4.5.19, assume that
c1 f1(x) + c2 f2(x) + · · · + ck fk(x) = 0,
for all x in I. Then, differentiating k −1 times yields the linear system
c1 f1(x)
+ c2 f2(x)
+ · · · + ck fk(x)
= 0,
c1 f ′
1(x)
+ c2 f ′
2(x)
+ · · · + ck f ′
k(x)
= 0,
...
c1 f (k−1)
1
(x) + c2 f (k−1)
2
(x) + · · · + ck f (k−1)
k
(x) = 0,
where the unknowns in the system are c1, c2, . . . , ck. We wish to show that c1 = c2 =
· · · = ck = 0. The determinant of the matrix of coefﬁcients of this system is just
W[ f1, f2, . . . , fk](x). Consequently, if W[ f1, f2, . . . , fk](x0) ̸= 0 for some x0 in I,
thenthedeterminantofthematrixofcoefﬁcientsofthesystemisnonzeroatthatpoint,and
therefore the only solution to the system is the trivial solution c1 = c2 = · · · = ck = 0.
That is, the given set of functions is linearly independent on I.
Remarks
1. Notice that it is only necessary for W[ f1, f2, . . . , fk](x) to be nonzero at one point
in I for { f1, f2, . . . , fk} to be linearly independent on I.
2. Theorem 4.5.23 does not say that if W[ f1, f2, . . . , fk](x) = 0 for every x in I,
then { f1, f2, . . . , fk} is linearly dependent on I. As we will see in the next example
below, the Wronskian of a linearly independent set of functions on an interval I can
be identically zero on I. Instead, the logical equivalent of the preceding theorem
is: If { f1, f2, . . . , fk} is linearly dependent on I, then W[ f1, f2, . . . , fk](x) = 0
at every point x of I.

294
CHAPTER 4
Vector Spaces
If W[ f1, f2, . . . , fk](x) = 0 for all x in I, Theorem 4.5.23 gives no information
as to the linear dependence or independence of { f1, f2, . . . , fk} on I.
Example 4.5.24
Determine whether the following functions are linearly dependent or linearly indepen-
dent on I = (−∞, ∞).
(a) f1(x) = ex, f2(x) = x2ex.
(b) f1(x) = x, f2(x) = x + x2, f3(x) = 2x −x2.
(c) f1(x) = x2, f2(x) =
) 2x2,
if x ≥0,
−x2,
if x < 0.
Solution:
(a) W[ f1, f2](x) = ex
x2ex
ex
ex(x2 + 2x) = e2x(x2 + 2x) −x2e2x = 2xe2x. Since
W[ f1, f2](x) ̸= 0 (except at x = 0), the functions are linearly independent on
(−∞, ∞).
(b)
W[ f1, f2, f3](x) =
x
x + x2
2x −x2
1
1 + 2x
2 −2x
0
2
−2
= x [(−2)(1 + 2x) −2(2 −2x)] −
+
(−2)(x + x2) −2(2x −x2)
,
= 0.
Thus, no conclusion can be drawn from Theorem 4.5.23. However, a closer in-
spection of the functions reveals, for example, that
f2 = 3 f1 −f3.
Consequently, the functions are linearly dependent on (−∞, ∞).
(c) If x ≥0, then
W[ f1, f2](x) = x2
2x2
2x
4x
= 0,
whereas if x < 0, then
W[ f1, f2](x) = x2
−x2
2x
−2x = 0.
Thus, W[ f1, f2](x) = 0 for all x in (−∞, ∞), and so, no conclusion can be drawn
from Theorem 4.5.23. Again we take a closer look at the given functions. They
are sketched in Figure 4.5.3. In this case, we see that on the interval (−∞, 0), the
functions are linearly dependent, since
f1 + f2 = 0.
They are also linearly dependent on [0, ∞), since on this interval, we have
2 f1 −f2 = 0.

4.5
Linear Dependence and Linear Independence 295
y
x
y 5 f1(x) 5 x2
y 5 f1(x) 5 x2
y 5 f2(x) 5 2x2
y 5 f2(x) 5 2x2
f2(x) 5 2f1(x) on [0, `)
f2(x) 5 2f1(x) on (2`, 0)
Figure 4.5.3: Two functions that are linearly independent on (−∞, ∞), but whose Wronskian
is identically zero on that interval.
The key point is to realize that there is no set of nonzero constants c1, c2 for which
c1 f1 + c2 f2 = 0
holds for all x in (−∞, ∞). Hence, the given functions are linearly independent on
(−∞, ∞).ThisillustratesoursecondremarkfollowingTheorem4.5.23,anditem-
phasizes the importance of the role played by the interval I when discussing linear
dependence and linear independence of functions. A collection of functions may be
linearlyindependentonaninterval I1,butlinearlydependentonanotherinterval I2.
□
It might appear at this stage that the usefulness of the Wronskian is questionable,
since if W[ f1, f2, . . . , fk] vanishes on an interval I, then no conclusion can be drawn
as to the linear dependence or linear independence of the functions f1, f2, . . . , fk on
I. However, the real power of the Wronskian is in its application to solutions of linear
differential equations of the form
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0.
(4.5.6)
In Chapter 8, we will establish that if we have n functions that are solutions of an equation
of the form (4.5.6) on an interval I, then if the Wronskian of these functions is identically
zero on I, the functions are indeed linearly dependent on I. Thus, the Wronskian does
completely characterize the linear dependence or linear independence of solutions of
such equations. This is a fundamental result in the theory of linear differential equations.
Exercises for 4.5
Key Terms
Linearly dependent set, Linear dependency, Linearly inde-
pendent set, Minimal spanning set, Wronskian of a set of
functions.
Skills
• For a set of one or two vectors, be able to determine
at a glance whether it is linearly dependent or linearly
independent.
• Be able to determine whether any given ﬁnite set of
vectors is linearly dependent or linearly independent.
• For linearly dependent sets of vectors, be able to de-
termine a linear dependency relationship among the
vectors.
• Be able to take a linearly dependent set of vectors and
remove vectors until it becomes a linearly independent
set of vectors with the same linear span as the original
set.
• Be able to produce a linearly independent set of vec-
tors that spans a given subspace of a vector space V .
• Be able to conclude immediately that a set of k vectors
in Rn is linearly dependent if k > n, and know what
can be said in the case where k = n as well.

296
CHAPTER 4
Vector Spaces
• Know what information the Wronskian does (and does
not) give about the linear dependence or linear inde-
pendence of a set of functions on an interval I.
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every vector space V possesses a unique minimal
spanning set.
(b) The set of column vectors of a 5×7 matrix A must be
linearly dependent.
(c) The set of column vectors of a 7×5 matrix A must be
linearly independent.
(d) Any nonempty subset of a linearly independent set of
vectors is linearly independent.
(e) If the Wronskian of a set of functions is nonzero at
some point x0 in an interval I, then the set of func-
tions is linearly independent.
(f) If it is possible to express one of the vectors in a set
S as a linear combination of the others, then S is a
linearly dependent set.
(g) If a set of vectors S in a vector space V contains a
linearly dependent subset, then S is itself a linearly
dependent set.
(h) A set of three vectors in a vector space V is linearly de-
pendent if and only if all three vectors are proportional
to one another.
(i) If the Wronskian of a set of functions is identically
zero at every point of an interval I, then the set of
functions is linearly dependent.
Problems
For Problems 1–10, determine whether the given set of
vectors is linearly independent or linearly dependent in
Rn. In the case of linear dependence, ﬁnd a dependency
relationship.
1. {(3, 6, 9)}.
2. {(1, −1), (1, 1)}.
3. {(2, −1), (3, 2), (0, 1)}.
4. {(1, −1, 0), (0, 1, −1), (1, 1, 1)}.
5. {(1, 2, 3), (1, −1, 2), (1, −4, 1)}.
6. {(−2, 4, −6), (3, −6, 9)}.
7. {(1, −1, 2), (2, 1, 0)}.
8. {(−1, 1, 2), (0, 2, −1), (3, 1, 2), (−1, −1, 1)}.
9. {(1, −1, 2, 3), (2, −1, 1, −1), (−1, 1, 1, 1)}.
10. {(2, −1, 0, 1), (1, 0, −1, 2), (0, 3, 1, 2),
(−1, 1, 2, 1)}.
11. Let v1 = (1, 2, 3), v2 = (4, 5, 6), v3 = (7, 8, 9). De-
termine whether {v1, v2, v3} is linearly independent in
R3. Describe
span{v1, v2, v3}
geometrically.
12. Consider the vectors v1 = (2, −1, 5), v2 = (1, 3, −4),
v3 = (−3, −9, 12) in R3.
(a) Show that {v1, v2, v3} is linearly dependent.
(b) Is v1 ∈span{v2, v3}? Draw a picture illustrating
your answer.
13. Determine all values of the constant k for which the
vectors (1, 1, k), (0, 2, k) and (1, k, 6) are linearly de-
pendent in R3.
For Problems 14–15, determine all values of the constant k
for which the given set of vectors is linearly independent
in R4.
14. {(1, 0, 1, k), (−1, 0, k, 1), (2, 0, 1, 3)}.
15. {(1, 1, 0, −1), (1, k, 1, 1), (2, 1, k, 1), (−1, 1, 1, k)}.
For Problems 16–18, determine whether the given set of vec-
tors is linearly independent in M2(R).
16. A1 =
% 1 1
0 1
&
, A2 =
% 2 −1
0
1
&
, A3 =
% 3 6
0 4
&
.
17. A1 =
% 2 −1
3
4
&
, A2 =
% −1 2
1 3
&
.
18. A1 =
% 1 0
1 2
&
, A2 =
% −1 1
2 1
&
, A3 =
% 2 1
5 7
&
.

4.5
Linear Dependence and Linear Independence 297
For Problems 19–22, determine whether the given set of vec-
tors is linearly independent in P2(R).
19. p1(x) = 1 −x,
p2(x) = 1 + x.
20. p1(x) = 2 + 3x,
p2(x) = 4 + 6x.
21. p1(x) = 1 −3x2,
p2(x) = 2x + x2,
p3(x) = 5.
22. p1(x) = 3x + 5x2,
p2(x) = 1 + x + x2,
p3(x) = 2 −x,
p4(x) = 1 + 2x2.
23. Show that the vectors
p1(x) = a + bx
and
p2(x) = c + dx
are linearly independent in P1(R) if and only if the
constants a, b, c, d satisfy ad −bc ̸= 0.
24. If f1(x) = cos 2x, f2(x) = sin2 x, f3(x) = cos2 x,
determine whether { f1, f2, f3} is linearly dependent
or linearly independent in C∞(−∞, ∞).
For Problems 25–31, determine a linearly independent set of
vectors that spans the same subspace of V as that spanned
by the original set of vectors.
25. V = R3, {(1, 2, 3), (−3, 4, 5), (1, −4/3, −5/3)}.
26. V = R3, {(3, 1, 5), (0, 0, 0), (1, 2, −1), (−1, 2, 3)}.
27. V = R3, {(1, 1, 1), (1, −1, 1), (1, −3, 1), (3, 1, 2)}.
28. V = R4,
{(1, 1, −1, 1), (2, −1, 3, 1), (1, 1, 2, 1), (2, −1, 2, 1)}.
29. V = M2(R),
)% 1 2
3 4
&
,
% −1 2
5 7
&
,
% 3 2
1 1
&*
.
30. V = P1(R), {2 −5x, 3 + 7x, 4 −x}.
31. V = P2(R), {2 + x2, 4 −2x + 3x2, 1 + x}.
For Problems 32–36, use the Wronskian to show that
the given functions are linearly independent on the given
interval I.
32. f1(x) = 1, f2(x) = x, f3(x) = x2, I = (−∞, ∞).
33. f1(x) = sin x, f2(x) = cos x, f3(x) = tan x,
I = (−π/2, π/2).
34. f1(x) = 1, f2(x) = 3x, f3(x) = x2 −1,
I = (−∞, ∞).
35. f1(x) = e2x, f2(x) = e3x, f3(x) = e−x,
I = (−∞, ∞).
36.
f1(x) =
) x2,
if x ≥0,
3x3,
if x < 0,
f2(x) = 7x2, I = (−∞, ∞).
For Problems 37–39, show that the Wronskian of the
given functions is identically zero on (−∞, ∞). Determine
whether the functions are linearly independent or linearly
dependent on that interval.
37. f1(x) = 1, f2(x) = x, f3(x) = 2x −1.
38. f1(x) = ex, f2(x) = e−x, f3(x) = cosh x.
39. f1(x) = 2x3,
f2(x) =
)
5x3,
if x ≥0,
−3x3,
if x < 0,
40. Consider the functions f1(x) = x,
f2(x) =
)
x,
if x ≥0,
−x,
if x < 0.
(a) Show that f2 is not in C1(−∞, ∞).
(b) Show that { f1, f2} is linearly dependent on the in-
tervals(−∞, 0)and[0, ∞),whileitislinearlyin-
dependent on the interval (−∞, ∞). Justify your
results by making a sketch showing both of the
functions.
41. Determine whether the functions f1(x) = x,
f2(x) =
) x,
if x ̸= 0,
1,
if x = 0.
are linearly dependent or linearly independent on I =
(−∞, ∞).
42. Show that the functions
f1(x) =
)
x −1,
if x ≥1,
2(x −1),
if x < 1,
f2(x) = 2x, f3(x) = 3 form a linearly independent
set on (−∞, ∞). Determine all intervals on which
{ f1, f2, f3} is linearly dependent.
43. (a) Show that {1, x, x2, x3} is linearly independent
on every interval.
(b) If fk(x) = xk for k = 0, 1, . . . , n, show that
{ f0, f1, . . . , fn} is linearly independent on every
interval for all ﬁxed n.

298
CHAPTER 4
Vector Spaces
44. (a) Show that the functions
f1(x) = er1x, f2(x) = er2x, f3(x) = er3x
have Wronskian
W[ f1, f2, f3](x) = e(r1+r2+r3)x
1
1
1
r1
r2
r3
r2
1
r2
2
r2
3
= e(r1+r2+r3)x(r3 −r1)(r3 −r2)(r2 −r1),
and hence determine the conditions on r1,r2,r3
such that { f1, f2, f3} is linearly independent on
every interval.
(b) More generally, show that the set of functions
{er1x, er2x, . . . , ernx}
is linearly independent on every interval if and
only if all of the ri are distinct. [Hint: Show that
the Wronskian of the given functions is a multiple
of the n × n Vandermonde determinant, and then
use Problem 30 in Section 3.3.]
45. Let {v1, v2} be a linearly independent set in a vector
space V , and let v = αv1 + v2, w = v1 + αv2, where
α is a constant. Use Deﬁnition 4.5.4 to determine all
values of α for which {v, w} is linearly independent.
46. If v1 and v2 are vectors in a vector space V , and
u1, u2, u3 are each linear combinations of them, prove
that {u1, u2, u3} is linearly dependent.
47. Letv1, v2, . . . , vm beasetoflinearlyindependentvec-
tors in a vector space V and suppose that the vectors
u1, u2, . . . , un are each linear combinations of them.
It follows that we can write
uk =
m
!
i=1
aikvi,
k = 1, 2, . . . , n,
for appropriate constants aik.
(a) If n > m, prove that {u1, u2, . . . , un} is linearly
dependent on V .
(b) If n = m, prove that {u1, u2, . . . , un} is linearly
independent in V if and only if det[ai j] ̸= 0.
(c) If n < m, prove that {u1, u2, . . . , un} is linearly
independent in V if and only if rank(A) = n,
where A = [ai j].
(d) Which result from this section do these results
generalize?
48. Prove from Deﬁnition 4.5.4 that if {v1, v2, . . . , vn} is
linearly independent and if A is an invertible n × n
matrix, then the set {Av1, Av2, . . . , Avn} is linearly
independent.
49. Prove that if {v1, v2} is linearly independent and v3 is
not in span{v1, v2}, then {v1, v2, v3} is linearly inde-
pendent.
50. Generalizing the previous exercise, prove that if
{v1, v2, . . . , vk} is linearly independent and vk+1 is
not in span{v1, v2, . . . , vk}, then {v1, v2, . . . , vk+1} is
linearly independent.
51. Prove Theorem 4.5.2.
52. Prove Proposition 4.5.8.
53. Prove that if {v1, v2, . . . , vk} spans a vector space V ,
then for every vector v in V , {v, v1, v2, . . . , vk} is lin-
early dependent.
54. Prove that if V = Pn(R) and S = {p1, p2, . . . , pk} is
a set of vectors in V each of a different degree, then S
is linearly independent. [Hint: Assume without loss of
generalitythatthepolynomialsareorderedindescend-
ing degree: deg(p1) > deg(p2) > · · · > deg(pk).
Assuming that c1 p1 + c2 p2 + · · · + ck pk = 0, ﬁrst
show that c1 is zero by examining the highest degree.
Then repeat for lower degrees to show successively
that c2 = 0, c3 = 0, and so on.]
4.6
Bases and Dimension
In the preceding two sections, we have encountered the concept of a spanning set for a
vector space and the concept of a linearly independent set of vectors. In this section, we
put the two concepts together to arrive at one of the most important deﬁnitions in this
entire text and a cornerstone of linear algebra.

4.6
Bases and Dimension 299
DEFINITION
4.6.1
A set of vectors {v1, v2, . . . , vk} in a vector space V is called a basis5 for V if
(a) The vectors are linearly independent.
(b) The vectors span V .
In the preceding section, we saw that a minimal spanning set must be linearly
independent. Thus, every minimal spanning set is a basis. Corollary 4.6.6 below will
establish that every basis is a minimal spanning set.
Notice that if we have a ﬁnite spanning set for a vector space, then we can always,
in principle, determine a basis for V by using the technique of Corollary 4.5.14. Fur-
thermore, the computational aspects of determining a basis have been covered in the
previous two sections, since all we are really doing is combining the two concepts of
linear independence and linear span. Consequently, this section is somewhat more the-
oretically oriented than the preceding ones. The reader is encouraged to not gloss over
the theoretical aspects, as these really are fundamental results in linear algebra.
There do exist vector spaces V for which it is impossible to ﬁnd a ﬁnite set of linearly
independent vectors that span V . The vector space Cn(I), n ≥1, is such an example
(Example 4.6.19). Such vector spaces are called inﬁnite-dimensional vector spaces.
Our primary interest in this text, however, will be vector spaces that contain a ﬁnite span-
ning set of linearly independent vectors. These are known as ﬁnite-dimensional vector
spaces, and we will encounter numerous examples of them throughout the remainder of
this section.
We begin with the vector space Rn. In R2, the most natural basis, denoted {e1, e2},
consists of the two vectors
e1 = (1, 0),
e2 = (0, 1),
(4.6.1)
and in R3, the most natural basis, denoted {e1, e2, e3}, consists of the three vectors
e1 = (1, 0, 0),
e2 = (0, 1, 0),
e3 = (0, 0, 1).
(4.6.2)
The veriﬁcation that the sets (4.6.1) and (4.6.2) are indeed bases of R2 and R3, respec-
tively, is straightforward and left as an exercise.6 These bases are referred to as the
standard basis on R2 and R3, respectively. In the case of the standard basis for R3 given
in (4.6.2), we recognize the vectors e1, e2, e3 as the familiar unit vectors i, j, k pointing
along the positive x-, y-, and z-axes of the rectangular Cartesian coordinate system.
More generally, consider the set of vectors {e1, e2, . . . , en} in Rn deﬁned by
e1 = (1, 0, . . . , 0),
e2 = (0, 1, . . . , 0),
. . . ,
en = (0, 0, . . . , 1).
These vectors are linearly independent by Corollary 4.5.17 since
det([e1, e2, . . . , en]) = det(In) = 1 ̸= 0.
Furthermore, the vectors span Rn, since an arbitrary vector v = (x1, x2, . . . , xn) in Rn
can be written as
v = x1(1, 0, . . . , 0) + x2(0, 1, . . . , 0) + · · · + xn(0, 0, . . . , 1)
= x1e1 + x2e2 + · · · + xnen.
5 The plural of basis is bases.
6Alternatively, the veriﬁcation is a special case of that given shortly for the general case of Rn.

300
CHAPTER 4
Vector Spaces
Consequently, {e1, e2, . . . , en} is a basis for Rn. We refer to this basis as the standard
basis for Rn.
The general vector in Rn has n components, and the standard basis vectors arise as
the n vectors that are obtained by sequentially setting one component to the value 1 and
the other components to 0. In general, this is how we obtain standard bases in vector
spaces whose vectors are determined by the speciﬁcation of n constants. We illustrate
with some examples.
Example 4.6.2
Determine the standard basis for M2(R).
Solution:
The general matrix in M2(R) is
% a
b
c
d
&
. Consequently, there are four
parameters that give rise to four special vectors in M2(R). Sequentially setting one of
these parameters to the value 1 and the others to 0 generates the following four matrices:
E11 =
% 1 0
0 0
&
,
E12 =
% 0 1
0 0
&
,
E21 =
% 0 0
1 0
&
,
E22 =
% 0 0
0 1
&
.
We see that {E11, E12, E21, E22} is a spanning set for M2(R). Furthermore,
c1E11 + c2E12 + c3E21 + c4E22 = 02
holds if and only if
c1
% 1 0
0 0
&
+ c2
% 0 1
0 0
&
+ c3
% 0 0
1 0
&
+ c4
% 0 0
0 1
&
=
% 0 0
0 0
&
;
that is, if and only if c1 = c2 = c3 = c4 = 0. Consequently, {E11, E12, E21, E22} is a
linearly independent spanning set for M2(R), and hence it is a basis. This is the standard
basis for M2(R).
□
Remark
More generally, consider the vector space of all m × n matrices with real
entries, Mm×n(R). If we let Ei j denote the m×n matrix with value 1 in the (i, j)-position
and zeros elsewhere, then one can show routinely that
{Ei j : 1 ≤i ≤m, 1 ≤j ≤n}
is a basis for Mm×n(R), and it is the standard basis for Mm×n(R).
Example 4.6.3
Determine a basis for P2(R).
Solution:
We have
P2(R) = {a0 + a1x + a2x2 : a0, a1, a2 ∈R},
so that the vectors in P2(R) are determined by specifying values for the three parameters
a0, a1, and a2. Sequentially setting one of these parameters to the value 1 and the other
two to the value 0 yields the following vectors in P2(R):
p0(x) = 1,
p1(x) = x,
p2(x) = x2.
We have shown in Example 4.4.6 that {p0(x), p1(x), p2(x)} is a spanning set for P2(R).
Furthermore,
W[p0, p1, p2](x) =
1
x
x2
0
1
2x
0
0
2
= 2 ̸= 0,

4.6
Bases and Dimension 301
which implies that {p0(x), p1(x), p2(x)} is linearly independent on any interval.7 Con-
sequently, {p0(x), p1(x), p2(x)} is a basis for P2(R). This is the standard basis for
P2(R).
□
Remark
More generally, the reader can check that a basis for the vector space of all
polynomials of degree n or less, Pn(R), is
{1, x, x2, . . . , xn}.
This is the standard basis for Pn(R).
Dimension of a Finite-Dimensional Vector Space
The reader has probably realized that there can be many different bases for a given
vector space V . In addition to the standard basis {e1, e2, e3} on R3, for example, it can
be checked8 that {(1, 2, 3), (4, 5, 6), (7, 8, 8)} and {(1, 0, 0), (1, 1, 0), (1, 1, 1)} are also
bases for R3. And there are countless others as well.
Despite the multitude of different bases available for a vector space V , they all share
one common feature: the number of vectors in each basis for V is the same. This fact
will be deduced as a corollary of our next theorem, a fundamental result in the theory of
vector spaces.
Theorem 4.6.4
If a ﬁnite-dimensional vector space has a basis consisting of n vectors, then any set of
more than n vectors is linearly dependent.
Proof Let {v1, v2, . . . , vn} be a basis for V , and consider an arbitrary set of vectors in V ,
say, {u1, u2, . . . , um}, with m > n. We wish to prove that {u1, u2, . . . , um} is necessarily
linearly dependent. Since {v1, v2, . . . , vn} is a basis for V , it follows that each u j can
be written as a linear combination of v1, v2, . . . , vn. Thus, there exist constants ai j
such that
u1 = a11v1 + a21v2 + · · · + an1vn,
u2 = a12v1 + a22v2 + · · · + an2vn,
...
um = a1mv1 + a2mv2 + · · · + anmvn.
To prove that {u1, u2, . . . , um} is linearly dependent, we must show that there exist
scalars c1, c2, . . . , cm, not all zero, such that
c1u1 + c2u2 + · · · + cmum = 0.
(4.6.3)
Inserting the expressions for u1, u2, . . . , um into Equation (4.6.3) yields
c1(a11v1 + a21v2 + · · · + an1vn) + c2(a12v1 + a22v2 + · · · + an2vn)
+ · · · + cm(a1mv1 + a2mv2 + · · · + anmvn) = 0.
7Alternatively, we can start with the equation c0 p0(x) + c1 p1(x) + c2 p2(x) = 0 for all x in R and show
readily that c0 = c1 = c2 = 0.
8The reader desiring extra practice at the computational aspects of verifying a basis is encouraged to pause
here to check these examples.

302
CHAPTER 4
Vector Spaces
Rearranging terms, we have
(a11c1 + a12c2 + · · · + a1mcm)v1 + (a21c1 + a22c2 + · · · + a2mcm)v2
+ · · · + (an1c1 + an2c2 + · · · + anmcm)vn = 0.
Since {v1, v2, . . . , vn} is linearly independent, we can conclude that
a11c1 + a12c2 + · · · + a1mcm = 0,
a21c1 + a22c2 + · · · + a2mcm = 0,
...
an1c1 + an2c2 + · · · + anmcm = 0.
This is an n × m homogeneous system of linear equations with n < m, and hence,
from Corollary 2.5.11, it has nontrivial solutions for c1, c2, . . . , cm. It therefore follows
from Equation (4.6.3) that {u1, u2, . . . , um} is linearly dependent.
Corollary 4.6.5
All bases in a ﬁnite-dimensional vector space V contain the same number of vectors.
Proof Suppose {v1, v2, . . . , vn} and {u1, u2, . . . , um} are two bases for V . From The-
orem 4.6.4 we know that we cannot have m > n (otherwise {u1, u2, . . . , um} would be
a linearly dependent set and hence could not be a basis for V ). Nor can we have n > m
(otherwise {v1, v2, . . . , vn} would be a linearly dependent set and hence could not be a
basis for V ). Thus, it follows that we must have m = n.
We can now prove that any basis provides a minimal spanning set for V .
Corollary 4.6.6
If a ﬁnite-dimensional vector space V has a basis consisting of n vectors, then every
spanning set for V must contain at least n vectors.
Proof If the spanning set contained fewer than n vectors, then there would be a subset
of less than n linearly independent vectors that spanned V ; that is, there would be a basis
consisting of less than n vectors. But this would contradict Corollary 4.6.5.
The number of vectors in a basis for a ﬁnite-dimensional vector space is clearly a
fundamental property of the vector space, and by Corollary 4.6.5, it is independent of
the particular chosen basis. We call this number the dimension of the vector space.
DEFINITION
4.6.7
The dimension of a ﬁnite-dimensional vector space V , written dim[V ], is the number
of vectors in any basis for V . If V is the trivial vector space, V = {0}, then we deﬁne
its dimension to be zero.
Remark
We say that the dimension of the world we live in is three for the very reason
that the maximum number of independent directions that we can perceive is three. If a
vector space has a basis containing n vectors, then from Theorem 4.6.4, the maximum
number of vectors in any linearly independent set is n. Thus, we see that the terminology
dimension used in an arbitrary vector space is a generalization of a familiar idea.
Example 4.6.8
It follows from our examples earlier in this section that dim[R3] = 3, dim[M2(R)] = 4,
and dim[P2(R)] = 3.
□

4.6
Bases and Dimension 303
More generally, the following dimensions should be remembered:
dim[Rn] = n, dim[Mm×n(R)] = mn, dim[Mn(R)] = n2, dim[Pn(R)] = n + 1.
These values have essentially been established previously in our discussion of standard
bases. The standard basis for Rn is {e1, e2, . . . , en}, where ei is the n-tuple with value
1 in the ith position and value 0 elsewhere. Thus, this basis contains n vectors. The
standard basis for Mm×n(R) is the set of matrices Ei j (1 ≤i ≤m, 1 ≤j ≤n) with
value 1 in the (i, j)-position and value 0 elsewhere. There are mn such matrices in this
standard basis. The case of Mn(R) is just a special case of Mm×n(R) in which m = n.
Finally, the standard basis for Pn(R) is {1, x, x2, . . . , xn}, a set of n + 1 vectors.
Next, let us return to Example 1.2.13 once more to cast its results in terms of the
basis concept.
Example 4.6.9
Determine a basis for the solution space to the differential equation
y′′ + y = 0
on any interval I.
Solution:
Our results from Example 1.2.13 tell us that all solutions to the given
differential equation are of the form
y(x) = c1 cos x + c2 sin x.
Consequently, {cos x, sin x} is a linearly independent spanning set for the solution space
of the differential equation and therefore is a basis.
□
More generally, we will show in Chapter 8 that all solutions to the differential
equation
y′′ + a1(x)y′ + a2(x)y = 0
on the interval I have the form
y(x) = c1y1(x) + c2y2(x),
where {y1, y2} is any linearly independent set of solutions to the differential equation.
Using the terminology introduced in this section, it will therefore follow that:
The set of all solutions to y′′ + a1(x)y′ + a2(x)y = 0 on
an interval I is a vector space of dimension two.
If a vector space has dimension n, then from Theorem 4.6.4, the maximum number of
vectors in any linearly independent set is n. On the other hand, from Corollary 4.6.6,
the minimum number of vectors that can span V is also n. Thus, a basis for V must be
a linearly independent set of n vectors. Our next theorem establishes that any set of n
linearly independent vectors is a basis for V .
Theorem 4.6.10
If dim[V ] = n, then any set of n linearly independent vectors in V is a basis for V .

304
CHAPTER 4
Vector Spaces
Proof Let v1, v2, . . . , vn be n linearly independent vectors in V . We need to show that
they span V . To do this, let v be an arbitrary vector in V . From Theorem 4.6.4, the set of
vectors {v, v1, v2, . . . , vn} is linearly dependent, and so there exist scalars c0, c1, . . . , cn,
not all zero, such that
c0v + c1v1 + · · · + cnvn = 0.
(4.6.4)
If c0 = 0, then the linear independence of {v1, v2, . . . , vn} and (4.6.4) would imply that
c0 = c1 = · · · = cn = 0, a contradiction. Hence, c0 ̸= 0, and so, from Equation (4.6.4),
v = −1
c0
(c1v1 + c2v2 + · · · + cnvn).
Thusv,andhenceanyvectorin V ,canbewrittenasalinearcombinationofv1, v2, . . . , vn,
and hence, {v1, v2, . . . , vn} spans V , in addition to being linearly independent. Hence it
is a basis for V , as required.
Theorem 4.6.10 is one of the most important results of the section. In Chapter 8, we
will explicitly construct a basis for the solution space to the differential equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
consisting of n vectors. That is, we will show that the solution space to this differential
equation is n-dimensional. It will then follow immediately from Theorem 4.6.10 that
every solution to this differential equation is of the form
y(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x),
where {y1, y2, . . . , yn} is any linearly independent set of n solutions to the differential
equation. Therefore, determining all solutions to the differential equation will be reduced
to determining any linearly independent set of n solutions. A similar application of the
theorem will be used to develop the theory for systems of differential equations in
Chapter 9.
More generally, Theorem 4.6.10 says that if we know in advance that the dimension
of the vector space V is n, then n linearly independent vectors in V are already guaranteed
to form a basis for V without needing to explicitly verify that these n vectors also span
V . This represents a useful reduction in the work required to verify a basis. Here is
an example:
Example 4.6.11
Verify that {1 + x, 2 −2x + x2, 1 + x2} is a basis for P2(R).
Solution:
We know that dim[P2(R)] = 3. Therefore, if we can show that the three
given vectors are linearly independent, then Theorem 4.6.10 will guarantee that the three
given vectors are a basis for P2(R). The polynomials
p1(x) = 1 + x,
p2(x) = 2 −2x + x2,
p3(x) = 1 + x2
have Wronskian
W[p1, p2, p3](x) =
1 + x
2 −2x + x2
1 + x2
1
−2 + 2x
2x
0
2
2
= −6 ̸= 0.
Since the Wronskian is nonzero, the given set of vectors is linearly independent on any
interval. Consequently, {1 + x, 2 −2x + x2, 1 + x2} is indeed a basis for P2(R).
□

4.6
Bases and Dimension 305
There is a notable parallel result to Theorem 4.6.10 which can also cut down the
work required to verify that a set of vectors in V is a basis for V , provided that we know
the dimension of V in advance.
Theorem 4.6.12
If dim[V ] = n, then any set of n vectors in V that spans V is a basis for V .
Proof Let v1, v2, . . . , vn be n vectors in V that span V . To conﬁrm that {v1, v2, . . . , vn}
is a basis for V , we need only show that this is a linearly independent set of vectors.
Suppose, to the contrary, that {v1, v2, . . . , vn} is a linearly dependent set. By Corollary
4.5.14, there is a linearly independent subset of {v1, v2, . . . , vn}, with fewer than n
vectors, which also spans V . But this implies that V contains a basis with fewer than n
vectors, a contradiction.
Putting the results of Theorems 4.6.10 and 4.6.12 together, the following result is
immediate.
Corollary 4.6.13
If dim[V ] = n and S = {v1, v2, . . . , vn} is a set of n vectors in V , the following
statements are equivalent:
1. S is a basis for V .
2. S is linearly independent.
3. S spans V .
We emphasize once more the importance of this result. It means that if we have a
set S of dim[V ] vectors in V , then to determine whether or not S is a basis for V , we
need only check if S is linearly independent or if S spans V , not both.
We have already navigated through a number of important theoretical results in this
section. Therefore, before proceeding further, it will be helpful to pause and summarize
some important relationships in a table:
Suppose that V is a vector space with dim[V ] = n,
and let S = {v1, v2, . . . , vk} be a subset of V .
S
k < n
k > n
k = n
is
Maybe
No
Maybe
linearly independent?
(Theorem 4.6.4)
(Corollary 4.6.13)
spans
No
Maybe
Maybe
V ?
(Corollary 4.6.6)
(Corollary 4.6.13)
is a
No
No
Maybe
basis?
(Corollary 4.6.5)
(Corollary 4.6.5)
(Corollary 4.6.13)
Of course, to be able to use the foregoing table information, we must already know
the value of dim[V ].
In keeping with the intuitive idea that the dimension of a vector space in some way
measures its size, the next result relating the dimension of a vector space V to that of
any of its subspaces should not be surprising.
Corollary 4.6.14
Let S be a subspace of a ﬁnite-dimensional vector space V . If dim[V ] = n, then
dim[S] ≤n.
Furthermore, if dim[S] = n, then S = V .

306
CHAPTER 4
Vector Spaces
Proof Suppose that dim[S] > n. Then any basis for S would contain more than n
linearly independent vectors, and therefore, we would have a linearly independent set of
more than n vectors in V . This would contradict Theorem 4.6.4. Thus, dim[S] ≤n.
Now consider the case when dim[S] = n = dim[V ]. In this case, any basis for S
consists of n linearly independent vectors in S, and hence n linearly independent vectors
in V . Thus, by Theorem 4.6.10, these vectors also form a basis for V . Hence, every
vector in V is spanned by the basis vectors for S, and hence, every vector in V lies in S.
Thus, V = S.
Example 4.6.15
Give a geometric description of the subspaces of R3 of dimensions 0, 1, 2, 3.
Solution:
Zero-dimensional subspace: This corresponds to the subspace {(0, 0, 0)},
and therefore, it is represented geometrically by the origin of a Cartesian coordinate
system.
One-dimensional subspace: These are subspaces generated by a single (nonzero) basis
vector. Consequently, they correspond geometrically to lines through the origin.
Two-dimensional subspace: These are the subspaces generated by any two noncollinear
vectors and correspond geometrically to planes through the origin.
Three-dimensional subspace: Since dim[R3] = 3, it follows from Corollary 4.6.14 that
the only three-dimensional subspace of R3 is R3 itself.
□
Example 4.6.16
Determine a basis for the subspace of R3 consisting of all solutions to the equation
x1 + 2x2 −x3 = 0.
Solution:
We can solve this problem geometrically. The given equation is that of a
plane through the origin and therefore is a two-dimensional subspace of R3. In order to
determine a basis for this subspace, we need only choose two linearly independent (i.e.,
noncollinear) vectors that lie in the plane. A simple choice of vectors is9 v1 = (1, 0, 1)
and v2 = (2, −1, 0). Thus, a basis for the subspace is {(1, 0, 1), (2, −1, 0)}.
□
Corollary 4.6.14 has shown that if S is a subspace of a ﬁnite-dimensional vector
space V with dim[S] = dim[V ], then S = V . Our next result establishes that, in general,
a basis for a subspace of a ﬁnite-dimensional vector space V can be extended to a basis
for V . This result will be required in the next section and also in Chapter 6.
Theorem 4.6.17
Let S be a subspace of a ﬁnite-dimensional vector space V . Any basis for S is part of a
basis for V .
Proof Suppose dim[V ] = n and dim[S] = k. By Corollary 4.6.14, k ≤n. If k = n,
then S = V , so that any basis for S is a basis for V . Suppose now that k < n, and let
{v1, v2, . . . , vk} be a basis for S. These basis vectors are linearly independent, but they
fail to span V (otherwise they would form a basis for V , contradicting k < n). Thus,
there is at least one vector, say vk+1, in V that is not in span{v1, v2, . . . , vk}. Hence,
{v1, v2, . . . , vk, vk+1} is linearly independent. If k+1 = n, then we have a basis for V by
Theorem 4.6.10, and we are done. Otherwise, we can repeat the procedure to obtain the
linearly independent set {v1, v2, . . . , vk, vk+1, vk+2}. The process will terminate when
we have a linearly independent set containing n vectors, including the original vectors
v1, v2, . . . , vk in the basis for S. This proves the theorem.
9There are many others, of course.

4.6
Bases and Dimension 307
Remark
The process used in proving the previous theorem is referred to as extending
a basis.
Example 4.6.18
Let S denotethesubspaceof M2(R)consistingofallsymmetric2×2 matrices.Determine
a basis for S, and ﬁnd dim[S]. Extend this basis for S to obtain a basis for M2(R).
Solution:
We ﬁrst express S in set notation as
S = {A ∈M2(R) : AT = A}.
In order to determine a basis for S, we need to obtain the element form of the matrices
in S. We can write
S =
)% a
b
b
c
&
: a, b, c ∈R
*
.
Since
% a
b
b
c
&
= a
% 1 0
0 0
&
+ b
% 0 1
1 0
&
+ c
% 0 0
0 1
&
,
it follows that
S = span
)% 1 0
0 0
&
,
% 0 1
1 0
&
,
% 0 0
0 1
&*
.
Furthermore, it is easily shown that the matrices in this spanning set are linearly indepen-
dent. Consequently, a basis for S is
)% 1 0
0 0
&
,
% 0 1
1 0
&
,
% 0 0
0 1
&*
, so that dim[S] = 3.
Since dim[M2(R)] = 4, in order to extend the basis for S to a basis for M2(R), we
need to add one additional matrix from M2(R) such that the resulting set is linearly
independent. We must choose a nonsymmetric matrix, for any symmetric matrix can be
expressed as a linear combination of the three basis vectors for S, and this would create
a linear dependency among the matrices. A simple choice of nonsymmetric matrix (al-
though this is certainly not the only choice) is
% 0 1
0 0
&
. Adding this vector to the basis
for S yields the linearly independent set
)% 1 0
0 0
&
,
% 0 1
1 0
&
,
% 0 0
0 1
&
,
% 0 1
0 0
&*
.
(4.6.5)
Since dim[M2(R)] = 4, Theorem 4.6.10 implies that (4.6.5) is a basis for M2(R).
□
It is important to realize that not all vector spaces are ﬁnite-dimensional. Some are
inﬁnite-dimensional. In an inﬁnite-dimensional vector space, we can ﬁnd an arbitrarily
large number of linearly independent vectors. We now give an example of an inﬁnite-
dimensional vector space that is of primary importance in the theory of differential
equations, Cn(I).
Example 4.6.19
Show that the vector space Cn(I) is an inﬁnite-dimensional vector space.
Solution:
Consider the functions 1, x, x2, . . . , xk in Cn(I). Of course, each of these
functions is in Ck(I) as well, and for each ﬁxed k, the Wronskian of these functions
is nonzero (the reader can check that the matrix involved in this calculation is upper
triangular, with nonzero entries on the main diagonal). Hence, the functions are linearly
independent on I by Theorem 4.5.23. Since we can choose k arbitrarily, it follows that
there is an arbitrarily large number of linearly independent vectors in Cn(I), and hence,
Cn(I) is inﬁnite-dimensional.
□

308
CHAPTER 4
Vector Spaces
In this example, we showed that Cn(I) is an inﬁnite-dimensional vector space.
Consequently, the use of our ﬁnite-dimensional vector space theory in the analysis of
differential equations appears questionable. However, the key theoretical result that we
will establish in Chapter 8 is that the solution set of certain linear differential equations is
a ﬁnite-dimensional subspace of Cn(I), and therefore, our basis results will be applicable
to this solution set.
Exercises for 4.6
Key Terms
Basis, Standard basis, Inﬁnite-dimensional, Finite-dimen-
sional, Dimension, Extending a basis.
Skills
• Be able to determine whether a given set of vectors
forms a basis for a vector space V .
• Be able to construct a basis for a given vector
space V .
• Be able to extend a basis for a subspace of V to V
itself.
• Be familiar with the standard bases on Rn, Mm×n(R),
and Pn(R).
• Be able to give the dimension of a vector space V .
• Be able to draw conclusions about the properties of a
set of vectors in a vector space (i.e., spanning or linear
independence) based solely on the size of the set.
• Understand the usefulness of Theorems 4.6.10 and
4.6.12.
True-False Review
For Questions (a)–(k), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A basis for a vector space V is a set S of vectors that
spans V .
(b) If V and W are vector spaces of dimensions n and m,
respectively, and if n > m, then W is a subspace of V .
(c) A vector space V can have many different bases.
(d) dim[Pn(R)] = dim[Rn].
(e) If V is an n-dimensional vector space, then any set S
of m vectors with m > n must span V .
(f) Five vectors in P3(R) must be linearly dependent.
(g) Two vectors in P3(R) must be linearly independent.
(h) The set of all solutions to any nth order linear differ-
ential equation forms an n-dimensional vector space.
(i) If V is an n-dimensional vector space, then every set
S with fewer than n vectors can be extended to a basis
for V .
(j) Every set of vectors that spans a ﬁnite-dimensional
vector space V contains a subset which forms a basis
for V .
(k) The set of all 3 × 3 upper triangular matrices forms a
3-dimensional subspace of M3(R).
Problems
For Problems 1–7, determine whether the given set S of vec-
tors is a basis for Rn.
1. S = {(−6, −1)}.
2. S = {(1, 1), (−1, 1)}.
3. S = {(1, 2, 1), (3, −1, 2), (1, 1, −1)}.
4. S = {(1, −1, 1), (2, 5, −2), (3, 11, −5)}.
5. S = {(1, 1, −1, 2), (1, 0, 1, −1), (2, −1, 1, −1)}.
6. S = {(1, 1, 0, 2), (2, 1, 3, −1), (−1, 1, 1, −2),
(2, −1, 1, 2)}.
7. S = {(7, 1, −3), (6, 1, 0), (−5, −1, −2), (0, −3, 8)}.
8. Determine all values of the constant k for which
the set of vectors S = {(0, −1, 0, k), (1, 0, 1, 0),
(0, 1, 1, 0), (k, 0, 2, 1)} is a basis for R4.
For Problems 9–14, determine whether the given set S of
vectors is a basis for Pn(R).
9. n = 1: S = {2 −5x, 3x, 7 + x}.
10. n = 2: S = {1 −3x2, 2x + 5x2, 1 −x + 3x2}.

4.6
Bases and Dimension 309
11. n = 2: S = {5x2, 1 + 6x, −3 −x2}.
12. n = 2: S = {−2x + x2, 1 + 2x + 3x2, −1 −x2,
5x + 5x2}.
13. n = 3: S = {1 + x3, x + x3, x2 + x3}.
14. n = 3: S = {1 + x + 2x3, 2 + x + 3x2 −x3,
−1 + x + x2 −2x3, 2 −x + x2 + 2x3}.
For Problems 15–18, determine whether the given set S of
vectors is a basis for Mm×n(R).
15. m = n = 2: S =
!" −3 1
0 2
#
,
" 3 −5
6
1
#
,
" −1 −2
1
0
#
,
" 0
3
1 −4
#
,
"
6 −2
−3 −4
#$
.
16. m = n = 2: S =
!" −2 −8
1
4
#
,
"
0 1
−1 1
#
,
" −5
0
5 −4
#
,
" 3 −2
4 −1
#$
.
17. m = 3, n = 2: S =
⎧
⎨
⎩
⎡
⎣
6 −3
1
4
4 −4
⎤
⎦,
⎡
⎣
0 −2
9
1
−3 −5
⎤
⎦,
⎡
⎣
2 −9
1
1
−3
0
⎤
⎦,
⎡
⎣
1 −5
2
0
−4
0
⎤
⎦,
⎡
⎣
−7
5
0 −1
3
1
⎤
⎦
⎫
⎬
⎭.
18. m = 2, n = 3: S =
!" 1 1 1
1 1 1
#
,
"
0 −5 1
−1
0 2
#
,
" 8 −3 1
0
1 4
#
,
" 2 −6 3
1
0 1
#
,
" 2 2 2
2 2 2
#
,
" 1
0 −2
2 −2
1
#$
.
For Problems 19–23, ﬁnd the dimension of the null space of
the given matrix A.
19. A =
/ 8 −9 3 3 −5 0
.
20. A =
"
1
3
−2 −6
#
.
21. A =
⎡
⎣
0 0 0
0 0 0
0 1 0
⎤
⎦.
22. A =
⎡
⎣
1 −1
4
2
3 −2
1
2 −2
⎤
⎦.
23. A =
⎡
⎢⎢⎣
1 −1 2 3
2 −1 3 4
1
0 1 1
3 −1 4 5
⎤
⎥⎥⎦.
24. Let S be the subspace of R3 that consists of all solu-
tions to the equation x −3y + z = 0. Determine a
basis for S, and hence, ﬁnd dim[S].
25. Let S be the subspace of R3 consisting of all vectors
of the form (r,r −2s, 3s −5r), where r and s are
real numbers. Determine a basis for S, and hence, ﬁnd
dim[S].
26. Determine a basis S for P3(R), and hence, prove that
dim[P3(R)] = 4. Be sure to prove that S is a basis.
27. Determine a basis S for P3(R) whose elements all have
the same degree. Be sure to prove that S is a basis.
28. Let S be the subspace of M2(R) consisting of all 2×2
upper triangular matrices. Determine a basis for S, and
hence, ﬁnd dim[S].
29. Let S be the subspace of M2(R) consisting of all 2×2
matrices with trace zero. Determine a basis for S, and
hence, ﬁnd dim[S].
30. Let S be the subspace of R3 spanned by the vectors
v1 = (1, 0, 1), v2 = (0, 1, 1), v3 = (2, 0, 2). Deter-
mine a basis for S, and hence, ﬁnd dim[S].
31. Let S be the vector space consisting of the set of
all linear combinations of the functions f1(x) =
ex, f2(x) = e−x, f3(x) = sinh(x). Determine a basis
for S, and hence, ﬁnd dim[S].
32. Determine a basis for the subspace of M2(R) spanned
by
"
1 3
−1 2
#
,
" 0 0
0 0
#
,
" −1 4
1 1
#
,
"
5 −6
−5
1
#
.
33. Let v1 = (1, 1) and v2 = (−1, 1).
(a) Show that {v1, v2} spans R2.
(b) Show that {v1, v2} is linearly independent.
(c) Conclude from (a) or (b) that {v1, v2} is a ba-
sis for R2. What theorem in this section allows
you to draw this conclusion from either (a) or (b),
without proving both?
34. Let v1 = (2, 1) and v2 = (3, 1).
(a) Show that {v1, v2} spans R2.
(b) Show that {v1, v2} is linearly independent.

310
CHAPTER 4
Vector Spaces
(c) Conclude from (a) or (b) that {v1, v2} is a ba-
sis for R2. What theorem in this section allows
you to draw this conclusion from either (a) or (b),
without proving both?
35. Let v1
=
(0, 6, 3), v2
=
(3, 0, 3), and v3
=
(6, −3, 0). Show that {v1, v2, v3} is a basis for R3.
[Hint: You need not show that the set is both linearly
independent and a spanning set for R3. Use a theorem
from this section to shorten your work.]
36. Determine all values of the constant α for which
{1 + αx2, 1 + x + x2, 2 + x} is a basis for P2(R).
37. Let p1(x) = 1 + x, p2(x) = −x + x2, p3(x) =
1 + 2x2. Show that {p1, p2, p3} is a basis for P2(R).
[Hint: You need not show that the set is both linearly
independent and a spanning set for P2(R). Use a the-
orem from this section to shorten your work.]
38. The Legendre polynomial of degree n, pn(x), is de-
ﬁned to be the polynomial solution of the differential
equation
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0,
which has been normalized so that pn(1) = 1. The ﬁrst
three Legendre polynomials are p0(x) = 1, p1(x) =
x, and p2(x) = 1
2(3x2 −1). Show that {p0, p1, p2} is
a basis for P2(R). [The hint for the previous problem
applies again.]
39. Let A1 =
! −1 1
0 1
"
, A2 =
!
1 3
−1 0
"
,
A3 =
! 1 0
1 2
"
, A4 =
! 0 −1
2
3
"
.
(a) Show that {A1, A2, A3, A4} is a basis for M2(R).
[Adapt the hints on Problems 35 and 37.]
(b) Express the vector
! 5 6
7 8
"
as a linear combina-
tion of the basis {A1, A2, A3, A4}.
40. Let A
=
⎡
⎣
1
1 −1
1
2 −3
5 −6
5
0
2 −3
⎤
⎦, and let v1
=
(−2, 7, 5, 0) and v2 = (3, −8, 0, 5).
(a) Show that {v1, v2} is a basis for the null space
of A.
(b) Using the basis in part (a), write an expression for
an arbitrary vector (x, y, z, w) in the null space
of A.
41. Let V = M3(R) and let S be the subset of all vectors
in V such that the sum of the entries in each row and
in each column is zero.
(a) Find a basis and the dimension of S.
(b) Extend the basis in (a) to a basis for V .
42. Let V = M3(R) and let S be the subset of all vectors
in V such that the sum of the entries in each column
is zero.
(a) Find a basis and the dimension of S.
(b) Extend the basis in (a) to a basis for V .
For Problems 43–44, Symn(R) and Skewn(R) denote the
vector spaces consisting of all real n × n matrices that are
symmetric and skew-symmetric, respectively.
43. Find a basis for Sym2(R) and Skew2(R), and show
that
dim[Sym2(R)] + dim[Skew2(R)] = dim[M2(R)].
44. Determine
the
dimensions
of
Symn(R)
and
Skewn(R), and show that
dim[Symn(R)] + dim[Skewn(R)] = dim[Mn(R)].
For Problems 45–47, a subspace S of a vector space V is
given. Determine a basis for S and extend your basis for S
to obtain a basis for V .
45. V = R3, S is the subspace consisting of all points
lying on the plane with Cartesian equation
x + 4y −3z = 0.
46. V = M2(R), S is the subspace consisting of all ma-
trices of the form
! a
b
b
a
"
.
47. V = P2(R), S is the subspace consisting of all poly-
nomials of the form
(2a1 + a2)x2 + (a1 + a2)x + (3a1 −a2).
For Problems 48–50, determine a basis for the solution space
of the given differential equation by seeking solutions of the
form y(x) = erx.
48. y′′ + 2y′ −3y = 0.
49. y′′ + 6y′ = 0.

4.7
Change of Basis 311
50. y′′ −2y = 0.
51. Let S denote the subspace of the solution space to
the differential equation y′′ + 16y = 0, with basis
{sin 4x + 5 cos 4x}. Write the general vector in S and
extend the basis for S to a basis for the full solution
space of the differential equation.
52. Let S be a basis for Pn−1(R). Prove that S ∪{xn} is a
basis for Pn(R). [Hint: Problem 50 in Section 4.5 is
useful here.]
53. Generalize the previous problem as follows. Let S be a
basis for Pn−1(R), and let p be any polynomial of de-
gree n. Prove that S ∪{p} is a basis for Pn(R). [Hint:
Problem 50 in Section 4.5 is useful here.]
54. (a) What is the dimension of Cn as a real vector
space? Determine a basis.
(b) What is the dimension of Cn as a complex vector
space? Determine a basis.
4.7
Change of Basis
Throughout this section, we restrict our attention to vector spaces that are ﬁnite-
dimensional. If we have a (ﬁnite) basis for such a vector space V , then, since the vectors
in a basis span V , any vector in V can be expressed as a linear combination of the basis
vectors. The next theorem establishes that there is only one way in which we can do this.
Theorem 4.7.1
If V is a vector space with basis {v1, v2, . . . , vn}, then every vector v ∈V can be written
uniquely as a linear combination of v1, v2, . . . , vn.
Proof Since v1, v2, . . . , vn span V , every vector v ∈V can be expressed as
v = a1v1 + a2v2 + · · · + anvn,
(4.7.1)
for some scalars a1, a2, . . . , an. Suppose also that
v = b1v1 + b2v2 + · · · + bnvn,
(4.7.2)
for some scalars b1, b2, . . . , bn. We will show that ai = bi for each i, which will prove
the uniqueness assertion of this theorem. Subtracting Equation (4.7.2) from Equation
(4.7.1) yields
(a1 −b1)v1 + (a2 −b2)v2 + · · · + (an −bn)vn = 0.
(4.7.3)
But {v1, v2, . . . , vn} is linearly independent, and so Equation (4.7.3) implies that
a1 −b1 = 0,
a2 −b2 = 0,
. . . ,
an −bn = 0.
That is, ai = bi for each i = 1, 2, . . . , n.
Remark
The converse of Theorem 4.7.1 is also true. That is, if every vector v in
a vector space V can be written uniquely as a linear combination of the vectors in
{v1, v2, . . . , vn}, then {v1, v2, . . . , vn} is a basis for V . The proof of this fact is left as
an exercise (Problem 39).
Up to this point, we have not paid particular attention to the order in which the
vectors of a basis are listed. However, in the remainder of this section, this will become
an important consideration. By an ordered basis for a vector space, we mean a basis in
which we are keeping track of the order in which the basis vectors are listed.

312
CHAPTER 4
Vector Spaces
DEFINITION
4.7.2
If B = {v1, v2, . . . , vn} is an ordered basis for V and v is a vector in V , then the
scalars c1, c2, . . . , cn in the unique n-tuple (c1, c2, . . . , cn) such that
v = c1v1 + c2v2 + · · · + cnvn
are called the components of v relative to the ordered basis B = {v1, v2, . . . , vn}.
We denote the column vector consisting of the components of v relative to the ordered
basis B by [v]B, and we call [v]B the component vector of v relative to B.
Example 4.7.3
Determine the component vector of the vector v = (1, 7) in R3 relative to the ordered
basis B = {(1, 2), (3, 1)}.
Solution:
If we let v1 = (1, 2) and v2 = (3, 1), then since these vectors are not
collinear, B = {v1, v2} is indeed a basis for R2. We must determine constants c1, c2
such that
c1v1 + c2v2 = v.
We write
c1(1, 2) + c2(3, 1) = (1, 7).
This requires that
c1 + 3c2 = 1
and
2c1 + c2 = 7.
The solution to this system is (4, −1), which gives the components of v relative to the
ordered basis B = {v1, v2}. (See Figure 4.7.1.) Thus,
v = 4v1 −v2.
Therefore, we have
[v]B =
%
4
−1
&
.
□
x
y
(4, 8)
(1, 7)
4v1
v1
v2
2v2
v = 4v12 v2
(1, 2)
(3, 1)
Figure 4.7.1: The components of
the vector v = (1, 7) relative to the
basis {(1, 2), (3, 1)}.
Remark
In the preceding example, the component vector of v = (1, 7) relative to
the ordered basis B′ = {(3, 1), (1, 2)} is
[v]B′ =
% −1
4
&
.
Thus, even though the bases B and B′ contain the same vectors, the fact that the vectors
are listed in different order affects the components of the vectors in the vector space.
Example 4.7.4
In P2(R), determine the component vector of p(x) = 1 −2x + 5x2 relative to the
following:
(a) The standard (ordered) basis B = {1, x, x2}.
(b) The ordered basis C = {1 −2x, 2 + x, 2 + x + x2}.
Solution:
(a) The given polynomial is already written as a linear combination of the standard
basis vectors. Consequently, the components of p(x) = 1 −2x + 5x2 relative to

4.7
Change of Basis 313
the standard basis B are 1, −2, and 5. We write
[p(x)]B =
⎡
⎣
1
−2
5
⎤
⎦.
(b) The components of p(x) = 1 −2x + 5x2 relative to the ordered basis
C = {1 −2x, 2 + x, 2 + x + x2}
are c1, c2, and c3, where
c1(1 −2x) + c2(2 + x) + c3(2 + x + x2) = 1 −2x + 5x2.
That is,
(c1 + 2c2 + 2c3) + (−2c1 + c2 + c3)x + c3x2 = 1 −2x + 5x2.
Hence, c1, c2, and c3 satisfy
c1 + 2c2 + 2c3 =
1,
−2c1 + c2 +
c3 = −2,
c3 =
5.
The augmented matrix of this system has reduced row-echelon form
⎡
⎣
1 0 0
1
0 1 0 −5
0 0 1
5
⎤
⎦,
so that the system has solution (1, −5, 5), which gives the required components.
Hence, we can write
1 −2x + 5x2 = 1(1 −2x) −5(2 + x) + 5(2 + x + x2).
Therefore,
[p(x)]C =
⎡
⎣
1
−5
5
⎤
⎦.
□
Change-of-Basis Matrix
The preceding example naturally motivates the following question: If we are given two
different ordered bases for an n-dimensional vector space V , say
B = {v1, v2, . . . , vn}
and
C = {w1, w2, . . . , wn},
(4.7.4)
and a vector v in V , how are [v]B and [v]C related? In practical terms, we may know
the components of v relative to B and wish to know the components of v relative to
a different ordered basis C. This question actually arises quite often, since different
bases are advantageous in different circumstances, so it is useful to be able to convert
components of a vector relative to one basis to components relative to another basis.
The tool we need in order to do this efﬁciently is the change-of-basis matrix. Before
we describe this matrix, we pause to record the linearity properties satisﬁed by the
components of a vector. These properties will facilitate the discussion that follows.

314
CHAPTER 4
Vector Spaces
Lemma 4.7.5
Let V be a vector space with basis B = {v1, v2, . . . , vn}, let x and y be vectors in V ,
and let c be a scalar. Then we have
(a) [x + y]B = [x]B + [y]B.
(b) [cx]B = c[x]B.
Proof Write
x = a1v1 + a2v2 + · · · + anvn
and
y = b1v1 + b2v2 + · · · + bnvn,
so that
x + y = (a1 + b1)v1 + (a2 + b2)v2 + · · · + (an + bn)vn.
Hence,
[x + y]B =
⎡
⎢⎢⎢⎣
a1 + b1
a2 + b2
...
an + bn
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
a1
a2
...
an
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
b1
b2
...
bn
⎤
⎥⎥⎥⎦= [x]B + [y]B,
which establishes (a). The proof of (b) is left as an exercise (Problem 38).
DEFINITION
4.7.6
Let V be an n-dimensional vector space with ordered bases B and C given in (4.7.4).
We deﬁne the change-of-basis matrix from B to C by
PC←B =
%
[v1]C, [v2]C, . . . , [vn]C
&
.
(4.7.5)
In words, we determine the components of each vector in the “old basis” B with
respect the “new basis” C and write the component vectors in the columns of the
change-of-basis matrix.
Remark
Of course, there is also a change of basis matrix from C to B, given by
PB←C =
%
[w1]B, [w2]B, . . . , [wn]B
&
.
We will see shortly that the matrices PB←C and PC←B, which are both n × n
matrices, are closely related.
Our ﬁrst order of business at this point is to see why the matrix in (4.7.5) converts
the components of a vector relative to B into components relative to C. Let v be a vector
in V and write
v = a1v1 + a2v2 + · · · + anvn.
Then
[v]B =
⎡
⎢⎢⎢⎣
a1
a2
...
an
⎤
⎥⎥⎥⎦.

4.7
Change of Basis 315
Hence, using Theorem 2.2.9 and Lemma 4.7.5, we have
PC←B[v]B = a1[v1]C +a2[v2]C +· · ·+an[vn]C = [a1v1+a2v2+· · ·+anvn]C = [v]C.
This calculation shows that premultiplying the component vector of v relative to B by
the change of basis matrix PC←B yields the component vector of v relative to C:
[v]C = PC←B[v]B.
(4.7.6)
Example 4.7.7
Let V = R2, B = {(1, 2), (3, 4)}, C = {(7, 3), (4, 2)}, and v = (1, 0). It is routine to
verify that B and C are bases for V .
(a) Determine [v]B and [v]C.
(b) Find PC←B and PB←C.
(c) Use (4.7.6) to compute [v]C, and compare your answer with (a).
Solution:
(a) Solving (1, 0) = a1(1, 2) + a2(3, 4), we ﬁnd a1 = −2 and a2 = 1. Hence,
[v]B =
% −2
1
&
. Likewise, setting (1, 0) = b1(7, 3)+b2(4, 2), we ﬁnd b1 = 1 and
b2 = −1.5. Hence, [v]C =
%
1
−1.5
&
.
(b) A short calculation shows that [(1, 2)]C =
% −3
5.5
&
and [(3, 4)]C =
% −5
9.5
&
. Thus,
we have
PC←B =
% −3 −5
5.5 9.5
&
.
Likewise, another short calculation shows that [(7, 3)]B
=
% −9.5
5.5
&
and
[(4, 2)]B =
% −5
3
&
. Hence,
PB←C =
% −9.5 −5
5.5
3
&
.
(c) We compute as follows:
PC←B[v]B =
% −3 −5
5.5 9.5
& % −2
1
&
=
%
1
−1.5
&
= [v]C,
as we found in part (a).
□
The reader may have noticed a close resemblance between the two matrices PC←B
and PB←C computed in part (b) of the preceding example. In fact, a brief calculation
shows that
PC←B PB←C = I2 = PB←C PC←B.
The two change-of-basis matrices are inverses of each other. This turns out to be always
true. To see why, consider again Equation (4.7.6). If we premultiply both sides of (4.7.6)

316
CHAPTER 4
Vector Spaces
by the matrix PB←C, we get
PB←C[v]C = PB←C PC←B[v]B.
(4.7.7)
Rearranging the roles of B and C in (4.7.6), the left side of (4.7.7) is simply [v]B. Thus,
PB←C PC←B[v]B = [v]B.
Since this is true for any vector [v]B in Rn, this implies that
PB←C PC←B = In,
the n × n identity matrix. Likewise, a similar calculation with the roles of B and C
reversed shows that
PC←B PB←C = In.
Thus, we have proved that
The matrices PC←B and PB←C are inverses of one another.
Example 4.7.8
Let V = P2(R), and let B = {2 + 4x + x2, 2 + 7x + 2x2, 6 + 4x + 5x2}, and C =
{2 + x + x2, x + x2, x}. It is routine to verify that B and C are bases for V . Find the
change-of-basis matrix from B to C, and use it to calculate the change-of-basis matrix
from C to B.
Solution:
We set 2 + 4x + x2 = a1(2 + x + x2) + a2(x + x2) + a3x. With a quick
calculation, we ﬁnd that a1 = 1, a2 = 0, and a3 = 3. Next, we set 2 + 7x + 2x2 =
b1(2+x +x2)+b2(x +x2)+b3x, and we ﬁnd that b1 = 1, b2 = 1, and b3 = 5. Finally,
we set 6 + 4x + 5x2 = c1(2 + x + x2) + c2(x + x2) + c3x, from which it follows that
c1 = 3, c2 = 2, and c3 = −1. Hence, we have
PC←B =
⎡
⎣
a1
b1
c1
a2
b2
c2
a3
b3
c3
⎤
⎦=
⎡
⎣
1 1
3
0 1
2
3 5 −1
⎤
⎦.
This matrix arose in Example 2.6.8. Referring to the calculations shown there, we
have
PB←C = (PC←B)−1 =
⎡
⎢⎣
11
14
−8
7
1
14
−3
7
5
7
1
7
3
14
1
7
−1
14
⎤
⎥⎦.
□
In much the same way that we showed above that the matrices PC←B and PB←C
are inverses of one another, we can make the following observation.
Theorem 4.7.9
Let V be a vector space with ordered bases A, B, and C. Then
PC←A = PC←B PB←A.
(4.7.8)

4.7
Change of Basis 317
Proof Using (4.7.6), for every v in V , we have
PC←B PB←A[v]A = PC←B[v]B = [v]C = PC←A[v]A,
so that premultiplication of [v]A by either matrix in (4.7.8) yields the same result. Hence,
the matrices on either side of (4.7.8) are the same.
We conclude this section by using Theorem 4.7.9 to show how an arbitrary change-
of-basis matrix PC←B in Rn can be expressed as a product of change-of-basis matrices
involving the standard basis E = {e1, e2, . . . , en} of Rn. Let B = {v1, v2, . . . , vn} and
C = {w1, w2, . . . , wn} be arbitrary ordered bases for Rn. Since [v]E = v for all column
vectors v in Rn, the matrices
PE←B = [[v1]E, [v2]E, . . . , [vn]E] = [v1, v2, . . . , vn]
and
PE←C = [[w1]E, [w2]E, . . . , [wn]E] = [w1, w2, . . . , wn]
can be written down immediately. Using these matrices, together with Theorem 4.7.9,
we can compute the arbitrary change-of-basis matrix PC←B with ease:
PC←B = PC←E PE←B = (PE←C)−1PE←B.
Exercises for 4.7
Key Terms
Ordered basis, Components of a vector relative to an ordered
basis, Change-of-basis matrix.
Skills
• Be able to ﬁnd the components of a vector relative to
a given ordered basis for a vector space V .
• Be able to compute the change-of-basis matrix for a
vector space V from one ordered basis B to another
ordered basis C.
• Be able to use the change-of-basis matrix from B to
C to determine the components of a vector relative to
C from the components of the vector relative to B.
• Be familiar with the relationship between the two
change-of-basis matrices PC←B and PB←C.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every vector in a ﬁnite-dimensional vector space V
can be expressed uniquely as a linear combination of
vectors comprising a basis for V .
(b) Premultiplying the component vector of v relative to
C by the change-of-basis matrix PB←C produces the
component vector of v relative to the basis B.
(c) A change-of-basis matrix is always a square matrix.
(d) A change-of-basis matrix is always invertible.
(e) For any vectors v and w in a ﬁnite-dimensional vector
space V withbasis B,wehave[v−w]B = [v]B−[w]B.
(f) If the bases B and C for a vector space V contain the
same set of vectors, then [v]B = [v]C for every vector
v in V .
(g) If B and C are bases for a ﬁnite-dimensional vector
space V , and v and w are in V such that [v]B = [w]C,
then v = w.

318
CHAPTER 4
Vector Spaces
(h) For every basis B for V , the matrix PB←B is the iden-
tity matrix.
Problems
For Problems 1–14, determine the component vector of the
given vector in the vector space V relative to the given or-
dered basis B.
1. V = R2; B = {(7, −1), (−9, −2)}; v = (27, 6).
2. V = R2; B = {(2, −2), (1, 4)}; v = (5, −10).
3. V = R2; B = {(−1, 3), (3, 2)}; v = (8, −2).
4. V = R3; B = {(1, 0, 1), (1, 1, −1), (2, 0, 1)};
v = (−9, 1, −8).
5. V = R3; B = {(1, −6, 3), (0, 5, −1), (3, −1, −1)};
v = (1, 7, 7).
6. V = R3; B = {(3, −1, −1), (1, −6, 3), (0, 5, −1)};
v = (1, 7, 7).
7. V = R3; B = {(−1, 0, 0), (0, 0, −3), (0, −2, 0)};
v = (5, 5, 5).
8. V = P2(R); B = {x2 + x, 2 + 2x, 1};
p(x) = −4x2 + 2x + 6.
9. V = P2(R); B = {5 −3x, 1, 1 + 2x2};
p(x) = 15 −18x −30x2.
10. V = P3(R); B = {1, 1+x, 1+x+x2, 1+x+x2+x3};
p(x) = 4 −x + x2 −2x3.
11. V = P3(R); B = {x3 + x2, x3 −1, x3 + 1, x3 + x};
p(x) = 8 + x + 6x2 + 9x3.
12. V = M2(R);
B =
)% 1 1
1 1
&
,
% 1 1
1 0
&
,
% 1 1
0 0
&
,
% 1 0
0 0
&*
;
A =
% −3 −2
−1
2
&
.
13. V = M2(R);
B =
)% 2 −1
3
5
&
,
%
0 4
−1 1
&
,
% 1 1
1 1
&
,
% 3 −1
2
5
&*
;
A =
% −10
16
−15 −14
&
.
14. V = M2(R);
B =
)% −1 1
0 1
&
,
%
1 3
−1 0
&
,
% 1 0
1 2
&
,
% 0 −1
2
3
&*
;
A =
% 5 6
7 8
&
.
15. Let v1
=
(0, 6, 3), v2
=
(3, 0, 3), and v3
=
(6, −3, 0). Determine the component vector of an
arbitrary vector v = (x, y, z) relative to the basis
{v1, v2, v3}.
16. Let p1(x) = 1 + x, p2(x) = −x + x2, and p3(x) =
1 + 2x2. Determine the component vector of an arbi-
trary polynomial p(x) = a0 + a1x + a2x2 relative to
the basis {p1, p2, p3}.
For Problems 17–26, ﬁnd the change-of-basis matrix PC←B
from the given ordered basis B to the given ordered basis C
of the vector space V .
17. V = R2; B = {(9, 2), (4, −3)};
C = {(2, 1), (−3, 1)}.
18. V = R2; B = {(−5, −3), (4, 28)};
C = {(6, 2), (1, −1)}.
19. V = R3; B = {(2, −5, 0), (3, 0, 5), (8, −2, −9)};
C = {(1, −1, 1), (2, 0, 1), (0, 1, 3)}.
20. V = R3; B = {(−7, 4, 4), (4, 2, −1), (−7, 5, 0)};
C = {(1, 1, 0), (0, 1, 1), (3, −1, −1)}.
21. V = P1(R); B = {7 −4x, 5x}; C = {1 −2x, 2 + x}.
22. V = P2(R);
B = {−4 + x −6x2, 6 + 2x2, −6 −2x + 4x2};
C = {1 −x + 3x2, 2, 3 + x2}.
23. V = P3(R);
B = {−2 + 3x + 4x2 −x3, 3x + 5x2 + 2x3,
−5x2 −5x3, 4 + 4x + 4x2};
C = {1 −x3, 1 + x, x + x2, x2 + x3}.
24. V = P2(R);
B = {2 + x2, −1 −6x + 8x2, −7 −3x −9x2};
C = {1 + x, −x + x2, 1 + 2x2}.
25. V = M2(R);
B =
)% 1
0
−1 −2
&
,
%0 −1
3
0
&
,
%3 5
0 0
&
,
%−2 −4
0
0
&*
;
C =
)% 1 1
1 1
&
,
% 1 1
1 0
&
,
% 1 1
0 0
&
,
% 1 0
0 0
&*
.
26. V = M2(R); B = {E12, E22, E21, E11};
C = {E22, E11, E21, E12}.

4.8
Row Space and Column Space 319
For Problems 27–32, ﬁnd the change-of-basis matrix PB←C
from the given basis C to the given basis B of the vector
space V .
27. V , B, and C from Problem 17.
28. V , B, and C from Problem 18.
29. V , B, and C from Problem 19.
30. V , B, and C from Problem 21.
31. V , B, and C from Problem 23.
32. V , B, and C from Problem 26.
For Problems 33–37, verify Equation (4.7.6) for the given
vector.
33. v = (−5, 3); V , B, and C from Problem 17.
34. v = (−1, 2, 0); V , B, and C from Problem 20.
35. p(x) = 6 −4x; V , B, and C from Problem 21.
36. p(x) = 5 −x + 3x2; V , B, and C from Problem 22.
37. A =
% −1 −1
−4
5
&
; V , B, and C from Problem 25.
38. Prove part (b) of Lemma 4.7.5.
39. Prove that if every vector v in a vector space V can
be written uniquely as a linear combination of the vec-
tors in {v1, v2, . . . , vn}, then {v1, v2, . . . , vn} is a basis
for V .
40. Show that if B is a basis for a ﬁnite-dimensional vec-
tor space V , and C is a basis obtained by reordering
the vectors in B, then the matrices PC←B and PB←C
each contain exactly one 1 in each row and column,
and zeros elsewhere.
4.8
Row Space and Column Space
In this section, we consider two vector spaces that can be associated with any m × n
matrix. For simplicity, we will assume that the matrices have real entries, although the
results that we establish can easily be extended to matrices with complex entries.
Row Space
Let A = [ai j] be an m × n real matrix. The row vectors of this matrix are row n-vectors,
and therefore they can be associated with vectors in Rn. The subspace of Rn spanned
by these vectors is called the row space of A and denoted rowspace(A). For example, if
A =
% 2 −1
3
5
9 −7
&
, then
rowspace(A) = span{(2, −1, 3), (5, 9, −7)}.
For a general m × n matrix A, how can we obtain a basis for rowspace(A)? By its
very deﬁnition, the row space of A is spanned by the row vectors of A, but these may not
be linearly independent, and hence the row vectors of A do not necessarily form a basis
for rowspace(A). We wish to determine a systematic and efﬁcient method for obtaining
a basis for the row space. Perhaps not surprisingly, it involves the use of elementary row
operations.
If we perform elementary row operations on A, then we are merely taking linear
combinations of vectors in rowspace(A), and we therefore might suspect that the row
space of the resulting matrix coincides with the row space of A. This is the content of
the following theorem.
Theorem 4.8.1
If A and B are row-equivalent matrices, then
rowspace(A) = rowspace(B).
Proof We establish that the matrix that results from performing any of the three ele-
mentary row operations on a matrix A has the same row space as the row space of A. If

320
CHAPTER 4
Vector Spaces
we interchange two rows of A, then clearly we have not altered the row space, since we
still have the same set of row vectors (listed in a different order).
Now let a1, a2, . . . , am denote the row vectors of A. We combine the remaining
two types of elementary row operations by considering the result of replacing ai by the
vector rai + sa j, where r (̸= 0) and s are real numbers. If s = 0, then this corresponds
to scaling ai by a factor of r, whereas if r = 1 and s ̸= 0, this corresponds to adding a
multiple of row j to row i. If B denotes the resulting matrix, then
rowspace(B) = {c1a1 + c2a2 + · · · + ci(rai + sa j) + · · · + cmam}
= {c1a1 + c2a2 + · · · + (rci)ai + · · · + (c j + sci)a j + · · · + cmam}
= {c1a1 + c2a2 + · · · + diai + · · · + d ja j + · · · + cmam},
where di = rci and d j = c j + sci. Note that di and d j can take on arbitrary values, and
hence, the vectors in rowspace(B) consist precisely of arbitrary linear combinations of
a1, a2, . . . , am. That is,
rowspace(B) = span{a1, a2, . . . , am} = rowspace(A).
The previous theorem is the key to determining a basis for rowspace(A). The idea
we use is to reduce A to row echelon form. If d1, d2, . . . , dk denote the nonzero row
vectors in this row-echelon form, then from the previous theorem,
rowspace(A) = span{d1, d2, . . . , dk}.
We now establish that {d1, d2, . . . , dk} is linearly independent. Consider
c1d1 + c2d2 + · · · + ckdk = 0.
(4.8.1)
Due to the positioning of the leading ones in a row-echelon matrix, each of the row
vectors d1, d2, . . . , dk−1 will have a leading one in a position where each succeeding
row vector in the row-echelon form has a zero. Hence, Equation (4.8.1) is satisﬁed
only if
c1 = c2 = · · · = ck−1 = 0,
and therefore, it reduces to
ckdk = 0.
However, dk is a nonzero vector, and so we must have ck = 0. Consequently, all of
the constants in Equation (4.8.1) must be zero, and therefore {d1, d2, . . . , dk} not only
spans rowspace(A), but is also linearly independent. Hence, {d1, d2, . . . , dk} is a basis
for rowspace(A). We have therefore established the next theorem.
Theorem 4.8.2
The set of nonzero row vectors in any row-echelon form of an m × n matrix A is a basis
for rowspace(A).
As a consequence of the preceding theorem, we can conclude that all row-echelon
forms of A have the same number of nonzero rows. For if this were not the case, then we
could ﬁnd two bases for rowspace(A) containing a different number of vectors, which
would contradict Corollary 4.6.5. We can therefore consider Theorem 2.4.10 as a direct
consequence of Theorem 4.8.2.
Example 4.8.3
Determine a basis for the row space of
A =
⎡
⎢⎢⎣
1
4 −1
2
3
5
−2 −7
5 −5 −6 −9
−2 −6
8 −6 −6 −8
1
5
2
1
3
6
⎤
⎥⎥⎦.

4.8
Row Space and Column Space 321
Solution:
We ﬁrst reduce A to row-echelon form:
A 1∼
⎡
⎢⎢⎣
1 4 −1
2 3 5
0 1
3 −1 0 1
0 2
6 −2 0 2
0 1
3 −1 0 1
⎤
⎥⎥⎦
2∼
⎡
⎢⎢⎣
1 4 −1
2 3 5
0 1
3 −1 0 1
0 0
0
0 0 0
0 0
0
0 0 0
⎤
⎥⎥⎦.
1. A12(2), A13(2), A14(−1)
2. A23(−2), A24(−1)
Consequently, a basis for rowspace(A) is {(1, 4, −1, 2, 3, 5), (0, 1, 3, −1, 0, 1)}, and
therefore, rowspace(A) is a two-dimensional subspace of R6.
□
Theorem 4.8.2 also gives an efﬁcient method for determining a basis for the subspace
of Rn spanned by a given set of vectors. If we let A be the matrix whose row vectors are
the given vectors from Rn, then rowspace(A) coincides with the subspace of Rn spanned
by those vectors. Consequently, the nonzero row vectors in any row-echelon form of A
will be a basis for the subspace spanned by the given set of vectors.
Example 4.8.4
Determine a basis for the subspace of R4 spanned by
{(1, 2, 3, 4), (4, 5, 6, 7), (7, 8, 9, 10)}.
Solution:
We ﬁrst let A denote the matrix that has the given vectors as row vectors.
Thus,
A =
⎡
⎣
1 2 3
4
4 5 6
7
7 8 9 10
⎤
⎦.
We now reduce A to row-echelon form:
A 1∼
⎡
⎣
1
2
3
4
0 −3
−6
−9
0 −6 −12 −18
⎤
⎦2∼
⎡
⎣
1
2
3
4
0
1
2
3
0 −6 −12 −18
⎤
⎦3∼
⎡
⎣
1 2 3 4
0 1 2 3
0 0 0 0
⎤
⎦.
1. A12(−4), A13(−7)
2. M2(−1
3)
3. A23(6)
Consequently, a basis for the subspace of R4 spanned by the given vectors is {(1, 2, 3, 4),
(0, 1, 2, 3)}. We see that the given vectors span a two-dimensional subspace of R4.
□
Column Space
If A is an m × n matrix, the column vectors of A are column m-vectors and therefore
can be associated with vectors in Rm. The subspace of Rm spanned by these vectors is
called the column space of A and denoted colspace(A).
Example 4.8.5
For the matrix A =
⎡
⎣
6
2
−1
0
4 −4
⎤
⎦, we have
colspace(A) = span
)⎡
⎣
6
−1
4
⎤
⎦,
⎡
⎣
2
0
−4
⎤
⎦
*
,

322
CHAPTER 4
Vector Spaces
which is the subspace of R3 consisting of the plane10 spanned by the vectors (6, −1, 4)
and (2, 0, −4).
□
We now consider the problem of determining a basis for the column space of an
m × n matrix A. Since the column vectors of A coincide with the row vectors of AT , it
follows that
colspace(A) = rowspace(AT ).
Hence one way to obtain a basis for colspace(A) would be to reduce AT to row-echelon
form and then the nonzero row vectors in the resulting matrix would form a basis for
colspace(A).
There is, however, a better method for determining a basis for colspace(A) directly
from any row-echelon form of A. The derivation of this technique is somewhat involved
and will require full attention.
We begin by determining the column space of an m×n reduced row-echelon matrix.
In order to introduce the basic ideas, consider the particular reduced row-echelon matrix
E =
⎡
⎢⎢⎣
1 2 0 3 0
0 0 1 5 0
0 0 0 0 1
0 0 0 0 0
⎤
⎥⎥⎦.
In this case, we see that the ﬁrst, third, and ﬁfth column vectors, which are the column
vectors containing the leading ones, coincide with the ﬁrst three standard basis vectors
in R4 (written as column vectors):
e1 =
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦,
e2 =
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦,
e3 =
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦.
Consequently, these column vectors are linearly independent. Furthermore, the re-
maining column vectors in E (those that do not contain leading ones) are both linear
combinations of e1 and e2, columns that do contain leading ones. Therefore {e1, e2, e3}
is a linearly independent set of vectors that spans colspace(E), and so a basis for
colspace(E) is
{(1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0)}.
Clearly, the same arguments apply to any reduced row-echelon matrix E. Thus, if
E contains k (necessarily ≤n) leading ones, a basis for colspace(E) is {e1, e2, . . . , ek}.
Now consider an arbitrary m×n matrix A, and let E denote the reduced row-echelon
form of A. Recall from Chapter 2 that performing elementary row operations on a linear
system does not alter its solution set. Hence, the two homogeneous systems of equations
Ac = 0
and
Ec = 0
(4.8.2)
have the same solution sets. If we write A and E in column vector form as A =
[a1, a2, . . . , an] and E = [d1, d2, . . . , dn], respectively, then the two systems in (4.8.2)
can be written as
c1a1 + c2a2+ · · · + cnan = 0,
c1d1 + c2d2+ · · · + cndn = 0,
10To ﬁnd the equation of this plane, we can use methods from multivariate calculus. Speciﬁcally, we obtain
a normal vector n to this plane by computing the cross product of the two given vectors in the plane: n =
(6, −1, 4) × (2, 0, −4) = (4, 32, 2). Thus, the equation of the plane can be written as 4x + 32y + 2z = 0.

4.8
Row Space and Column Space 323
respectively. Thus, the fact that these two systems have the same solution set means that
a linear dependence relationship will hold between the column vectors of E if and only if
precisely the same linear dependence relation holds between the corresponding column
vectors of A. In particular, since our previous work shows that the column vectors in
E that contain leading ones give a basis for colspace(E), they give a maximal linearly
independent set in colspace(E). Therefore, the corresponding column vectors in A will
also be a maximal linearly independent set in colspace(A). Consequently, this set of
vectors from A will be a basis for colspace(A).
We have therefore shown that the set of column vectors of A corresponding to those
column vectors containing leading ones in the reduced row-echelon form of A is a basis
for colspace(A). But do we have to reduce A to reduced row-echelon form? The answer
is no. We only need to reduce A to row-echelon form. The reason is that going further
to reduce a matrix from row-echelon form to reduced row-echelon form does not alter
the position or number of leading ones in a matrix, and therefore, the column vectors
containing leading ones in any row-echelon form of A will correspond to the column
vectors containing leading ones in the reduced row-echelon form of A. Consequently,
we have established the following result.
Theorem 4.8.6
Let A be an m ×n matrix. The set of column vectors of A corresponding to those column
vectors containing leading ones in any row-echelon form of A is a basis for colspace(A).
Example 4.8.7
Determine a basis for colspace(A) if
A =
⎡
⎢⎢⎣
1
2 −1 −2 −1
2
4 −2 −3 −1
5
10 −5 −3 −1
−3 −6
3
2
1
⎤
⎥⎥⎦.
Solution:
We ﬁrst reduce A to row-echelon form:
A 1∼
⎡
⎢⎢⎣
1 2 −1 −2 −1
0 0
0
1
1
0 0
0
7
4
0 0
0 −4 −2
⎤
⎥⎥⎦
2∼
⎡
⎢⎢⎣
1 2 −1 −2 −1
0 0
0
1
1
0 0
0
0 −3
0 0
0
0
2
⎤
⎥⎥⎦
3∼
⎡
⎢⎢⎣
1 2 −1 −2 −1
0 0
0
1
1
0 0
0
0
1
0 0
0
0
2
⎤
⎥⎥⎦
4∼
⎡
⎢⎢⎣
1 2 −1 −2 −1
0 0
0
1
1
0 0
0
0
1
0 0
0
0
0
⎤
⎥⎥⎦.
1. A12(−2), A13(−5), A14(3)
2. A23(−7), A24(4)
3. M3(−1
3)
4. A34(−2)
Since the ﬁrst, fourth, and ﬁfth column vectors in this row-echelon form of A contain
the leading ones, it follows from Theorem 4.8.6 that the set of corresponding column
vectors in A is a basis for colspace(A). Consequently, a basis for colspace(A) is
{(1, 2, 5, −3), (−2, −3, −3, 2), (−1, −1, −1, 1)}.

324
CHAPTER 4
Vector Spaces
Hence, colspace(A) is a three-dimensional subspace of R4. Notice from the row-echelon
form of A that a basis for rowspace(A) is {(1, 2, −1, −2, −1), (0, 0, 0, 1, 1),
(0, 0, 0, 0, 1)} so that rowspace(A) is a three-dimensional subspace of R5.
□
We now summarize the discussion of row space and column space.
Summary: Let A be an m × n matrix. In order to determine a basis for rowspace(A)
and a basis for colspace(A), we reduce A to row-echelon form.
1. The row vectors containing the leading ones in the row-echelon form
give a basis for rowspace(A) (a subspace of Rn).
2. The column vectors of A corresponding to the column vectors containing
the leading ones in the row-echelon form give a basis for colspace(A)
(a subspace of Rm).
Since the number of vectors in a basis for rowspace(A) or in a basis for colspace(A)
is equal to the number of leading ones in any row-echelon form of A, it follows that
dim[rowspace(A)] = dim[colspace(A)].
However, we emphasize that rowspace(A) and colspace(A) are, in general, subspaces
of different vector spaces. In Example 4.8.7, for instance, rowspace(A) is a subspace
of R5, while colspace(A) is a subspace of R4. For an m × n matrix, rowspace(A) is a
subspace of Rn, whereas colspace(A) is a subspace of Rm.
Exercises for 4.8
Key Terms
Row space, Column space.
Skills
• Be able to compute a basis for the row space of a
matrix.
• Be able to compute a basis for the column space of a
matrix.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If A is an m × n matrix such that rowspace(A) =
colspace(A), then m = n.
(b) A basis for the row space of a matrix A consists of the
row vectors of any row-echelon form of A.
(c) The nonzero column vectors of a row echelon form of
a matrix A form a basis for colspace(A).
(d) The sets rowspace(A) and colspace(A) have the same
dimension.
(e) If A is an n ×n invertible matrix, then rowspace(A) =
Rn.
(f) If A is an n × n invertible matrix, then colspace(A) =
Rn.
Problems
For Problems 1–2, (a) determine a basis for rowspace(A)
and make a sketch of it in the xy-plane; (b) Repeat part (a)
for colspace(A).
1. A =
% 6 −1
12 −2
&
.
2. A =
%
1 −2
−3
6
&
.

4.9
The Rank-Nullity Theorem 325
For Problems 3–9, (a) ﬁnd n such that rowspace(A) is a sub-
space of Rn, and determine a basis for rowspace(A); (b) ﬁnd
m such that colspace(A) is a subspace of Rm, and determine
a basis for colspace(A).
3. A =
/ 1 2 3 4 5 0
.
4. A =
⎡
⎣
−3
1
7
⎤
⎦.
5. A =
% 1 1
−3 2
3 4 −11 7
&
.
6. A =
⎡
⎣
1
2
3
5
6
7
9 10 11
⎤
⎦.
7. A =
⎡
⎣
0
3
1
0 −6 −2
0
12
4
⎤
⎦.
8. A =
⎡
⎢⎢⎣
1
2 −1
3
3
6 −3
5
1
2 −1 −1
5 10 −5
7
⎤
⎥⎥⎦.
9. A =
⎡
⎣
1 −1
2 3
1
1 −2 6
3
1
4 2
⎤
⎦.
For Problems 10–13, determine a basis for the subspace of
Rn spanned by the given set of vectors by (a) using the con-
cept of the row space of a matrix, and (b) using the concept
of the column space of a matrix.
10. {(1, −1, 2), (5, −4, 1), (7, −5, −4)}.
11. {(1, 3, 3), (1, 5, −1), (2, 7, 4), (1, 4, 1)}.
12. {(1, 1, −1, 2), (2, 1, 3, −4), (1, 2, −6, 10)}.
13. {(1, 4, 1, 3), (2, 8, 3, 5), (1, 4, 0, 4), (2, 8, 2, 6)}.
14. Let A =
⎡
⎣
1
2
4
5 11 21
3
7 13
⎤
⎦.
(a) Find a basis for rowspace(A) and colspace(A).
(b) Show that rowspace(A) corresponds to the plane
with Cartesian equation 2x + y −z = 0, whereas
colspace(A) corresponds to the plane with Carte-
sian equation 2x −y + z = 0.
15. Give an example of a square matrix A whose row space
andcolumnspacehavenononzerovectorsincommon.
16. Give examples to show how each type of elementary
row operation applied to a matrix can change the col-
umn space of the matrix.
17. Let A be an m × n matrix with colspace(A) =
nullspace(A). Prove that m = n.
18. Let A be an n × n matrix with rowspace(A) =
nullspace(A). Prove that A cannot be invertible.
4.9
The Rank-Nullity Theorem
In this section, we make connections between the null space of a real m × n matrix
A introduced in Section 4.3 and the row space and column space of A introduced in
the previous section. Recall that the null space of A is deﬁned to be the set of all real
solutions to the associated homogeneous linear system Ax = 0. Thus,
nullspace(A) = {x ∈Rn : Ax = 0}.
The dimension of nullspace(A) is referred to as the nullity of A and is denoted nullity(A).
In order to ﬁnd nullity(A), we need to determine a basis for nullspace(A). Recall that if
rank(A) = r, then any row echelon form of A contains r leading ones, which correspond
to the bound variables in the linear system. Thus, there are n−r columns without leading
ones, which correspond to free variables in the solution of the system Ax = 0. Hence,
there are n −r free variables in the solution of the system Ax = 0. We might therefore
suspect that nullity(A) = n −r. Our next theorem, which is often referred to as the
Rank-Nullity Theorem, establishes that this is indeed the case.
Theorem 4.9.1
(Rank-Nullity Theorem)
For any m × n matrix A,
rank(A) + nullity(A) = n.
(4.9.1)

326
CHAPTER 4
Vector Spaces
Proof If rank(A) = n, then by the Invertible Matrix Theorem, the only solution to
Ax = 0 is the trivial solution x = 0. Hence, in this case, nullspace(A) = {0}, so
nullity(A) = 0 and Equation (4.9.1) holds.
Now suppose rank(A) = r < n. In this case, there are n −r > 0 free variables
in the solution to Ax = 0. Let t1, t2, . . . , tn−r denote these free variables (chosen as
those variables not attached to a leading one in any row-echelon form of A), and let
x1, x2, . . . , xn−r denote the solutions obtained by sequentially setting each free variable
to 1 and the remaining free variables to zero. Note that {x1, x2, . . . , xn−r} is linearly inde-
pendent. Moreover, every solution to Ax = 0 is a linear combination of x1, x2, . . . , xn−r:
x = t1x1 + t2x2 + · · · + tn−rxn−r,
which shows that {x1, x2, . . . , xn−r} spans nullspace(A). Thus, {x1, x2, . . . , xn−r} is a
basis for nullspace(A), and nullity(A) = n −r.
Example 4.9.2
If A =
⎡
⎢⎢⎣
2
−6
−8
−1
3
4
5 −15 −20
−2
6
8
⎤
⎥⎥⎦, ﬁnd a basis for nullspace(A) and verify Theorem 4.9.1.
Solution:
We must ﬁnd all solutions to Ax = 0. Reducing the augmented matrix of
this system yields
A# 1∼
⎡
⎢⎢⎣
1
−3
−4 0
−1
3
4 0
5 −15 −20 0
−2
6
8 0
⎤
⎥⎥⎦
2∼
⎡
⎢⎢⎣
1 −3 −4 0
0
0
0 0
0
0
0 0
0
0
0 0
⎤
⎥⎥⎦.
1. M1(1/2)
2. A12(1), A13(−5), A14(2)
Consequently, there are two free variables, x2 = s and x3 = t, so that
x1 = 3s + 4t.
Hence,
nullspace(A) = {(3s + 4t, s, t) : s, t ∈R}
= {s(3, 1, 0) + t(4, 0, 1) : s, t ∈R}
= span{(3, 1, 0), (4, 0, 1)}.
Since the two vectors in this spanning set are not proportional, they are linearly indepen-
dent. Consequently, a basis for nullspace(A) is {(3, 1, 0), (4, 0, 1)}, so that nullity(A) =
2. In this problem, A is a 4 × 3 matrix, and so, in the Rank-Nullity Theorem, n = 3.
Further, from the foregoing row-echelon form of the augmented matrix of the system
Ax = 0, we see that rank(A) = 1. Hence,
rank(A) + nullity(A) = 1 + 2 = 3 = n,
and the Rank-Nullity Theorem is veriﬁed.
□
For an m × n matrix A with real entries, let us summarize in the table below the
essential information relating nullspace(A), rowspace(A), colspace(A), Rm, and Rn:

4.9
The Rank-Nullity Theorem 327
Description
Subspace of
Dimension
nullspace(A)
set of vectors x with Ax = 0
Rn
nullity(A)
rowspace(A)
span of the row vectors of A
Rn
rank(A)
colspace(A)
span of the column vectors of A
Rm
rank(A)
Notice that rowspace(A) and colspace(A) both have the same dimension, rank(A),
but they occur as subspaces of different vectors, namely Rn and Rm, respectively.
Systems of Linear Equations
We now examine the linear structure of the solution set to the linear system Ax =
b in terms of the concepts introduced in the last few sections. First we consider the
homogeneous case b = 0.
Corollary 4.9.3
Let A be an m × n matrix, and consider the corresponding homogeneous linear system
Ax = 0.
1. Ifrank(A) = n,then Ax = 0hasonlythetrivialsolution,andso,nullspace(A) = {0}.
2. If rank(A) = r < n, then Ax = 0 has an inﬁnite number of solutions, all of which
can be expressed in the form
x = c1x1 + c2x2 + · · · + cn−rxn−r,
(4.9.2)
where {x1, x2, . . . , xn−r} is any linearly independent set of n −r solutions to
Ax = 0.
Proof Note that (1) is a restatement of previous results, or can be quickly deduced from
the Rank-Nullity Theorem. Now for (2), assume that rank(A) = r < n. By the Rank-
Nullity Theorem, nullity(A) = n −r. Thus, from Theorem 4.6.10, if {x1, x2, . . . , xn−r}
is any set of n−r linearly independent solutions to Ax = 0, it is a basis for nullspace(A),
and so all vectors in nullspace(A) can be written as
x = c1x1 + c2x2 + · · · + cn−rxn−r,
for appropriate values of the constants c1, c2, . . . , cn−r.
Remark
The expression (4.9.2) is referred to as the general solution to the system
Ax = 0.
We now turn our attention to nonhomogeneous linear systems. We begin by formu-
lating Theorem 2.5.9 in terms of colspace(A).
Theorem 4.9.4
Let A be an m × n matrix and consider the linear system Ax = b.
1. If b is not in colspace(A), then the system is inconsistent.
2. If b ∈colspace(A), then the system is consistent and has the following:
(a) a unique solution if and only if dim[colspace(A)] = n.
(b) an inﬁnite number of solutions if and only if dim[colspace(A)] < n.

328
CHAPTER 4
Vector Spaces
Proof If we write A in terms of its column vectors as A = [a1, a2, . . . , an], then the
linear system Ax = b can be written as
x1a1 + x2a2 + · · · + xnan = b,
where x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦. Consequently, the linear system is consistent if and only if the vector
b is a linear combination of the column vectors of A. Thus, the system is consistent if
and only if b ∈colspace(A). This proves (1). Parts (2a) and (2b) follow directly from
Theorem 2.5.9, since rank(A) = dim[colspace(A)].
The set of all solutions to a nonhomogeneous linear system is not a vector space,
since, for example, it does not contain the zero vector, but the linear structure of
nullspace(A) can be used to determine the general form of the solution of a nonho-
mogeneous system.
Theorem 4.9.5
Let A be an m × n matrix. If rank(A) = r < n and b ∈colspace(A), then all solutions
to Ax = b are of the form
x = c1x1 + c2x2 + · · · + cn−rxn−r + xp,
(4.9.3)
where xp is any particular solution to Ax = b, and {x1, x2, . . . , xn−r} is a basis for
nullspace(A).
Proof Since xp is a solution to Ax = b, we have
Axp = b.
(4.9.4)
Let x = u be an arbitrary solution to Ax = b. Then we also have
Au = b.
(4.9.5)
Subtracting (4.9.4) from (4.9.5) yields
Au −Axp = 0,
or equivalently,
A(u −xp) = 0.
Consequently, the vector u −xp is in nullspace(A), and, therefore, there exist scalars
c1, c2, . . . , cn−r such that
u −xp = c1x1 + c2x2 + · · · + cn−rxn−r,
since {x1, x2, . . . , xn−r} is a basis for nullspace(A). Hence,
u = c1x1 + c2x2 + · · · + cn−rxn−r + xp,
as required.
Remark
The expression given in Equation (4.9.3) is called the general solution to
Ax = b. It has the structure
x = xc + xp,

4.9
The Rank-Nullity Theorem 329
where
xc = c1x1 + c2x2 + · · · + cn−rxn−r
is the general solution of the associated homogeneous system and xp is one particular
solution of the nonhomogeneous system. In later chapters, we will see that this structure
is also apparent in the solution of all linear differential equations and in all linear systems
of differential equations. It is a result of the linearity inherent in the problem, rather than
the speciﬁc problem that we are studying. The unifying concept, in addition to the vector
space, is the idea of a linear transformation, which we will study in Chapter 6.
Example 4.9.6
Let A =
⎡
⎢⎢⎣
2
−6
−8
−1
3
4
5 −15 −20
−2
6
8
⎤
⎥⎥⎦and b =
⎡
⎢⎢⎣
−18
9
−45
18
⎤
⎥⎥⎦. Verify that xp = (5, 2, 2) is a particular
solution to Ax = b, and use Theorem 4.9.5 to determine the general solution to the
system.
Solution:
For the given xp, we have
Axp =
⎡
⎢⎢⎣
2
−6
−8
−1
3
4
5 −15 −20
−2
6
8
⎤
⎥⎥⎦
⎡
⎣
5
2
2
⎤
⎦=
⎡
⎢⎢⎣
−18
9
−45
18
⎤
⎥⎥⎦= b.
Consequently, xp = (5, 2, 2) is a particular solution to Ax = b. Further, from Example
4.9.2, a basis for nullspace(A) is {x1, x2}, where x1 = (3, 1, 0) and x2 = (4, 0, 1). Thus,
the general solution to Ax = 0 is
xc = c1x1 + c2x2,
and therefore, from Theorem 4.9.5, the general solution to Ax = b is
x = c1x1 + c2x2 + xp = c1(3, 1, 0) + c2(4, 0, 1) + (5, 2, 2),
which can be written as
x = (3c1 + 4c2 + 5, c1 + 2, c2 + 2).
□
Exercises for 4.9
Skills
• For a given matrix A, be able to determine the rank
from the nullity, or the nullity from the rank.
• Know the relationship between the rank of a matrix A
and the consistency of a linear system Ax = b.
• Know the relationship between the column space of
a matrix A and the consistency of a linear system
Ax = b.
• Be able to formulate the solution set to a linear system
Ax = b in terms of the solution set to the correspond-
ing homogeneous linear equation.
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) For an m × n matrix A, the nullity of A must be at
least |m −n|.
(b) If A is a 7 × 9 matrix with nullity(A) = 2, then
rowspace(A) = R7.
(c) If A is a 9 × 7 matrix with nullity(A) = 0, then
rowspace(A) = R7.

330
CHAPTER 4
Vector Spaces
(d) The nullity of an n × n upper triangular matrix A
is simply the number of zeros appearing on the main
diagonal of A.
(e) An n × n matrix A for which nullspace(A)
=
colspace(A) cannot be invertible.
(f) For all m × n matrices A and B, nullity(A + B) =
nullity(A)+ nullity(B).
(g) For all n × n matrices A and B, nullity(AB) =
nullity(A)· nullity(B).
(h) For all n × n matrices A and B, nullity(AB) ≥
nullity(B).
(i) If xp is a solution to the linear system Ax = b, then
y + xp is also a solution for any y in nullspace(A).
Problems
For Problems 1–5, determine the null space of A and verify
the Rank-Nullity Theorem.
1. A =
⎡
⎣
6
−1
9
⎤
⎦.
2. A =
/ 1 0 −6 −1 0
.
3. A =
%
2 −1
−4
2
&
.
4. A =
⎡
⎣
1 1 −1
3 4
4
1 1
0
⎤
⎦.
5. A =
⎡
⎣
1 4 −1 3
2 9 −1 7
2 8 −2 6
⎤
⎦.
For Problems 6–9, determine the nullity of A “by inspection”
by appealing to the Rank-Nullity Theorem. Avoid computa-
tions.
6. A =
⎡
⎢⎢⎣
2
−3
0
0
−4
6
22 −33
⎤
⎥⎥⎦.
7. A =
⎡
⎢⎢⎣
1
3 −3
2
5
−4 −12
12 −8 −20
0
0
0
0
0
1
3 −3
2
6
⎤
⎥⎥⎦.
8. A =
⎡
⎢⎢⎣
0 1 0
0 1 0
0 0 1
0 0 1
⎤
⎥⎥⎦.
9. A =
/ 0 0 0 −2 0
.
For Problems 10–13, determine the solution set to Ax = b,
and show that all solutions are of the form (4.9.3).
10. A =
⎡
⎣
1 3 −1
2 7
9
1 5
21
⎤
⎦, b =
⎡
⎣
4
11
10
⎤
⎦.
11. A =
⎡
⎣
2 −1 1 4
1 −1 2 3
1 −2 5 5
⎤
⎦, b =
⎡
⎣
5
6
13
⎤
⎦.
12. A =
⎡
⎢⎢⎣
1
1 −2
3 −1 −7
1
1
1
2
2 −4
⎤
⎥⎥⎦, b =
⎡
⎢⎢⎣
−3
2
0
−6
⎤
⎥⎥⎦.
13. A =
⎡
⎣
1 1 −1
5
0 2 −1
7
4 2 −3 13
⎤
⎦, b =
⎡
⎣
0
0
0
⎤
⎦.
14. Show that a 3 × 7 matrix A with nullity(A) = 4 must
have colspace(A) = R3. Is rowspace(A) = R3?
15. Show that a 6 × 4 matrix A with nullity(A) = 0 must
have rowspace(A) = R4. Is colspace(A) = R4?
16. Prove that if rowspace(A) = nullspace(A), then A
contains an even number of columns.
17. Show that a 5 × 7 matrix A must have 2
≤
nullity(A) ≤7. Give an example of a 5 × 7 matrix
A with nullity(A) = 2 and a 5 × 7 matrix A with
nullity(A) = 7.
18. Show that 3×8 matrix A must have 5 ≤nullity(A) ≤
8. Give an example of a 3 × 8 matrix A with
nullity(A)
=
5 and a 3 × 8 matrix A with
nullity(A) = 8.
19. Prove that if A and B are n × n matrices and A is
invertible, then
nullity(AB) = nullity(B) = nullity(B A).
[Hint: Bx = 0 if and only if ABx = 0.]

4.10
Invertible Matrix Theorem II 331
4.10
Invertible Matrix Theorem II
In Section 2.8, we gave a list of characterizations of invertible matrices (Theorem 2.8.1).
In view of the concepts introduced in this chapter, we are now in a position to add to the
list that was begun there.
Theorem 4.10.1
(Invertible Matrix Theorem)
Let A be an n × n matrix. The following conditions on A are equivalent:
(a) A is invertible.
(h) nullity(A) = 0.
(i) nullspace(A) = {0}.
(j) The columns of A form a linearly independent set of vectors in Rn.
(k) colspace(A) = Rn (that is, the columns of A span Rn).
(l) The columns of A form a basis for Rn.
(m) The rows of A form a linearly independent set of vectors in Rn.
(n) rowspace(A) = Rn (that is, the rows of A span Rn).
(o) The rows of A form a basis for Rn.
(p) AT is invertible.
Proof The equivalence of (a) and (h) follows at once from Theorem 2.8.1(d) and the
Rank-Nullity Theorem (Theorem 4.9.1). The equivalence of (h) and (i) is immediately
clear. The equivalence of (a) and (j) is immediate from Theorem 2.8.1(c) and Theorem
4.5.16. Since the dimension of colspace(A) is simply rank(A), the equivalence of (a)
and (k) is immediate from Theorem 2.8.1(d). Next, from the deﬁnition of a basis, we see
that (j) and (k) are logically equivalent to (l). Moreover, since the row space and column
space of A have the same dimension, (k) and (n) are equivalent. Since rowspace(A) =
colspace(AT ), the equivalence of (k) and (n) proves that (a) and (p) are equivalent.
Finally, the equivalence of (a) and (p) proves that (j) is equivalent to (m) and that (l) is
equivalent to (o).
Example 4.10.2
Do the rows of the matrix below span R4?
A =
⎡
⎢⎢⎣
−2 −2
1
3
3
3
0 −1
−1 −1 −2 −5
2
2
1
1
⎤
⎥⎥⎦
Solution:
We see by inspection that the columns of A are linearly dependent, since
the ﬁrst two columns are identical. Therefore, by the equivalence of (j) and (n) in the
Invertible Matrix Theorem, the rows of A do not span R4.
□
Example 4.10.3
If A is an n × n matrix such that the linear system AT x = 0 has no nontrivial solution
x, then nullspace(AT ) = {0}, and thus AT is invertible by the equivalence of (a) and (i)

332
CHAPTER 4
Vector Spaces
in the Invertible Matrix Theorem. Thus, by the same theorem, we can conclude that the
columns of A form a linearly independent set.
□
Despite the lengthy list of characterizations of invertible matrices that we have been
able to develop so far, this list is still by no means complete. In Chapters 6 and 7, we
will use linear transformations and eigenvalues to provide further characterizations of
invertible matrices.
Exercises for 4.10
Skills
• Be well-familiar with all of the conditions (a)–(p) in
the Invertible Matrix Theorem that characterize invert-
ible matrices.
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An invertible matrix has linearly independent rows.
(b) An m × n matrix can have linearly independent rows
and linearly dependent columns.
(c) An n × n matrix can have linearly independent rows
and linearly dependent columns.
(d) If A is an n × n matrix with det(A) = 0, then the
columns of A must form a basis for Rn.
(e) If A and B are row-equivalent n×n matrices such that
rowspace(A) ̸= Rn, then colspace(B) ̸= Rn.
(f) If E is an n × n elementary matrix and A is an n × n
matrix with nullspace(A) = {0}, then det(E A) = 0.
(g) If A and B are n × n invertible matrices, then
nullity([A|B]) = 0, where [A|B] is the n × 2n matrix
with the blocks A and B as shown.
(h) A matrix of the form
⎡
⎣
0
a
0
b
0
c
0 d
0
⎤
⎦
cannot be invertible.
(i) A matrix of the form
⎡
⎢⎢⎣
0
a
0
b
c
0
d
0
0
e
0
f
g
0
h
0
⎤
⎥⎥⎦
cannot be invertible.
(j) A matrix of the form
⎡
⎣
a
b
c
d
e
f
g
h
i
⎤
⎦
such that ae −bd = 0 cannot be invertible.
4.11
Chapter Review
In this chapter we have derived some basic results in linear algebra regarding vector
spaces. These results form the framework for much of linear mathematics. Following
are listed some of the highlights of the chapter.
The Definition of a Vector Space
A vector space consists of the following four different components:
1. A set of vectors V .
2. A set of scalars F (either the set of real numbers R, or the set of complex
numbers C).

4.11
Chapter Review 333
3. A rule, +, for adding vectors in V .
4. A rule, ·, for multiplying vectors in V by scalars in F.
Then (V, +, ·) is a vector space over F if and only if axioms A1–A10 of Deﬁnition 4.2.1
are satisﬁed. If F is the set of all real numbers, then (V, +, ·) is called a real vector
space, whereas if F is the set of all complex numbers, then (V, +, ·) is called a complex
vector space. Since it is usually quite clear what the addition and scalar multiplication
operations are, we usually specify a vector space by giving only the set of vectors V .
The major vector spaces we have dealt with are the following:
Rn, the (real) vector space of all ordered n-tuples of real numbers.
Cn, the (complex) vector space of all ordered n-tuples of complex numbers.
Mn(R), the (real) vector space of all n × n matrices with real elements.
Ck(I), the vector space of all real-valued functions that are continuous and have
(at least) k continuous derivatives on I.
Pn(R), the vector space of all polynomials of degree ≤n with real coefﬁcients.
Subspaces
Usually the vector space V that underlies a given problem is known. It is often one
that appears in the list above. However, the solution of a given problem in general only
involvesasubsetofvectorsfromthisvectorspace.Thequestionthatthenarisesiswhether
this subset of vectors is itself a vector space under the same operations of addition and
scalar multiplication as in V . In order to answer this question, Theorem 4.3.2 tells us
that a nonempty subset of a vector space V is a subspace of V if and only if the subset
is closed under addition and closed under scalar multiplication.
Spanning Sets
A set of vectors {v1, v2, . . . , vk} in a vector space V is said to span V if every vector in
V can be written as a linear combination of v1, v2, . . . , vk; that is, if for every v ∈V ,
there exist scalars c1, c2, . . . , ck such that
v = c1v1 + c2v2 + · · · + ckvk.
Given a set of vectors {v1, v2, . . . , vk} in a vector space V , we can form the set of all
vectors that can be written as a linear combination of v1, v2, . . . , vk. This collection of
vectors is a subspace of V called the subspace spanned by {v1, v2, . . . , vk}, and denoted
span{v1, v2, . . . , vk}. Thus,
span{v1, v2, . . . , vk} = {v ∈V : v = c1v1 + c2v2 + · · · + ckvk}.
Linear Dependence and Linear Independence
Let {v1, v2, . . . , vk} be a set of vectors in a vector space V , and consider the vector
equation
c1v1 + c2v2 + · · · + c,vk = 0.
(4.11.1)
Clearly this equation will hold if c1 = c2 = · · · = ck = 0. The question of interest is
whether there are nonzero values of some or all of the scalars c1, c2, . . . , ck such that
(4.11.1) holds. This leads to the following two ideas:
Linear dependence:
There exist scalars c1, c2, . . . , ck, not all zero, such that
(4.11.1) holds.
Linear independence:
The only values of the scalars c1, c2, . . . , ck such that (4.11.1)
holds are c1 = c2 = · · · = ck = 0.

334
CHAPTER 4
Vector Spaces
To determine whether a set of vectors is linearly dependent or linearly independent we
usually have to use (4.11.1). However, if the vectors are from Rn, then we can use
Corollary 4.5.17, whereas for vectors in Ck−1(I) the Wronskian can be useful.
Bases and Dimension
A linearly independent set of vectors that spans a vector space V is called a basis for V .
If {v1, v2, . . . , vk} is a basis for V , then any vector in V can be written uniquely as
v = c1v1 + c2v2 + · · · + ckvk,
for appropriate values of the scalars c1, c2, . . . , ck. We call (c1, c2, . . . , ck) the compo-
nents of v relative to {v1, v2, . . . , vn}.
1. All bases in a ﬁnite-dimensional vector space V contain the same number of
vectors, and this number is called the dimension of V , denoted dim[V ].
2. We can view the dimension of a ﬁnite-dimensional vector space V in two different
ways. First, it gives the minimum number of vectors that span V . Alternatively, we
can regard dim[V ] as determining the maximum number of vectors that a linearly
independent set in V can contain.
3. If dim[V ] = n, then any linearly independent set of n vectors in V is a basis for
V . Alternatively, any set of n vectors that spans V is a basis for V . Here is a useful
summary:
Suppose that V is a vector space with dim[V ] = n,
and let S = {v1, v2, . . . , vk} be a subset of V .
S
k < n
k > n
k = n
is
Maybe
No
Maybe
linearly independent?
(Theorem 4.6.4)
(Corollary 4.6.13)
spans
No
Maybe
Maybe
V ?
(Corollary 4.6.6)
(Corollary 4.6.13)
is a
No
No
Maybe
basis?
(Corollary 4.6.5)
(Corollary 4.6.5)
(Corollary 4.6.13)
Three Subspaces Associated to a Matrix
In this chapter, we have examined three subspaces that can be associated with an m × n
matrix A. They are the null space, row space, and column space of A.
The null space of A, written nullspace(A), is {x ∈Rn : Ax = 0}. It is important
because it represents the solution set to the homogeneous linear system of equations
Ax = 0. Its dimension, by deﬁnition, is nullity(A), and this is simply the number of
unpivoted columns in the reduced row-echelon form of A. One can quickly deduce,
therefore, that rank(A) + nullity(A) = n. This is an important equation known as the
Rank-Nullity Theorem.
The row space of A, written rowspace(A), is simply the span of the row vectors
of the matrix A. As such, it forms a subspace of Rn. Likewise, the column space of A
is written colspace(A), and this is the span of the column vectors of A, a subspace of
Rm. It is not hard to show that both rowspace(A) and colspace(A) have dimension equal
to rank(A). A basis for rowspace(A) is provided by the nonzero rows of the reduced

4.11
Chapter Review 335
row-echelon form of A, while a basis for colspace(A) is obtained by taking the column
vectors of A that correspond to the pivoted columns of the reduced row-echelon form
of A.
The table below provides a convenient summary of relationships between the three
subspaces associated to the m × n matrix A:
Description
Subspace of
Dimension
nullspace(A)
set of vectors x with Ax = 0
Rn
nullity(A)
rowspace(A)
span of the row vectors of A
Rn
rank(A)
colspace(A)
span of the column vectors of A
Rm
rank(A)
More Characterizations of Invertible Matrices
We can add to the list of characterizations of invertible matrices by using the concepts
of the null space, row space, and column space of an n × n matrix A. Theorem 4.10.1
records these additional characterizations.
Additional Problems
For Problems 1–2, let r and s denote scalars and let v and w
denote vectors in R5.
1. Prove that (r + s)v = rv + sv.
2. Prove that r(v + w) = rv + rw.
For Problems 3–12, determine whether the given set (to-
gether with the usual operations on that set) forms a vector
space over R. In all cases, justify your answer carefully.
3. V = {p(x) ∈P2(R) : p(3) = 0 and p′(5) = 0}.
4. The set of polynomials of degree 5 or less whose co-
efﬁcients are even integers.
5. The set of all polynomials of degree 5 or less whose
coefﬁcients of x2 and x3 are zero.
6. The set of solutions to the linear system
−2x2 + 5x3 = 7
4x1 −6x2 + 3x3 = 0.
7. The set of solutions to the linear system
4x1 −7x2 + 2x3 = 0
5x1 −2x2 + 9x3 = 0.
8. The set of 2 × 2 real matrices whose entries are either
all zero or all nonzero.
9. The set of 2 × 2 real matrices that commute with the
matrix
! 1 2
0 2
"
.
10. The set of all functions f : [0, 1] →[0, 1] such that
f (0) = f ( 1
4) = f ( 1
2) = f ( 3
4) = f (1) = 0.
11. The set of all functions f : [0, 1] →[0, 1] such that
| f (x)| ≤x for all x in [0, 1].
12. The set of n ×n matrices A such that A2 is symmetric.
13. Let
V = {(a1, a2) : a1, a2 ∈R, a2 > 0}.
Deﬁne addition and scalar multiplication on V as fol-
lows:
(a1, a2) ⊕(b1, b2) = (a1 + b1, a2b2),
k ⊗(a1, a2) = (ka1, ak
2),
k ∈R.
Explicitly verify that V is a vector space over R.
14. Show that
S = {(a, 2a) : a ∈R}
is a subspace of the vector space V given in Problem
13.

336
CHAPTER 4
Vector Spaces
15. Show that {(1, 2), (3, 8)} is a linearly dependent set in
the vector space V in Problem 13.
16. Show that {(1, 4), (2, 1)} is a basis for the vector space
V in Problem 13.
17. What is the dimension of the subspace of P2(R) given
by
S = span{2 + x2, 4 −2x + 3x2, 1 + x}?
For Problems 18–23, decide (with justiﬁcation) whether S is
a subspace of V .
18. V = R2, S = {(x, y) : x2 −y = 0}.
19. V = R2, S = {(x, x3) : x ∈R}.
20. V = M2(R), S = {2 × 2 orthogonal matrices}.
21. V = C[a, b], S = { f ∈V : f (a) = 2 f (b)}.
22. V = C[a, b], S = { f ∈V :
! b
a f (x) dx = 0}.
23. V = M3×2(R),
S =
"⎡
⎣
a
b
c
d
e
f
⎤
⎦: a+b = c+ f and a−c = e−f −d
'
.
For Problems 24–31, decide (with justiﬁcation) whether or
not the given set S of vectors (a) spans V , and (b) is linearly
independent.
24. V = R3, S = {(5, −1, 2), (7, 1, 1)}.
25. V = R3, S = {(6, −3, 2), (1, 1, 1), (1, −8, −1)}.
26. V = R4, S = {(6, −3, 2, 0), (1, 1, 1, 0),
(1, −8, −1, 0)}.
27. V = R3,
S = {(10, −6, 5), (3, −3, 2), (0, 0, 0), (6, 4, −1),
(7, 7, −2)}.
28. V = P3(R), S = {2x −x3, 1 + x + x2, 3, x}.
29. V = P4(R), S = {x4 + x2 + 1, x2 + x + 1, x + 1,
x4 + 2x + 3}.
30. V = M2×3(R),
S =
"( −1 0 0
0 1 1
)
,
( 3 2 1
1 2 3
)
,
( −1 −2 −3
3
2
1
)
,
( −11 −6 −5
1 −2 −5
)'
.
31. V = M2(R),
S =
"( 1 2
2 1
)
,
( 3 4
4 3
)
,
( −2 −1
−1 −2
)
,
( −3 0
0 3
)
,
( 2 0
0 0
)'
.
32. Prove that if {v1, v2, v3} is linearly independent and
v4 is not in span {v1, v2, v3}, then {v1, v2, v3, v4} is
linearly independent.
33. Let A be an m × n matrix. Show that the columns
of A are linearly independent if and only if AT A is
invertible.
34. Let S denote the set of all 4 × 4 skew-symmetric ma-
trices.
(a) Show that S is a subspace of M3(R).
(b) Find a basis and the dimension of S.
(c) Extend the basis you constructed in part (b) to a
basis for M4(R).
35. Let S denote the set of all 4 × 4 matrices such that the
entries in each row and each column add up to zero.
(a) Show that S is a subspace of M4(R).
(b) Find a basis and the dimension of S.
(c) Extend the basis you constructed in part (b) to a
basis for M4(R).
36. Let (V, +V , ·V ) and (W, +W, ·W) be vector spaces
and deﬁne
V ⊕W = {(v, w) : v ∈V and w ∈W}.
Prove that
(a) V ⊕W is a vector space, under componentwise
operations.
(b) if S = {(v, 0) : v ∈V } and S′ = {(0, w) : w ∈
W}, then S and S′ are subspaces of V ⊕W.
(c) if dim[V ]
=
n and dim[W]
=
m, then
dim[V ⊕W] = m + n.
[Hint: Write a basis for V ⊕W in terms of bases
for V and W.]
37. Show that a basis for P3(R) need not contain a poly-
nomial of each degree 0, 1, 2, 3.

4.11
Chapter Review 337
38. Prove that if A is a matrix whose nullspace and column
space are the same, then A must have an even number
of columns.
39. Let B =
⎡
⎢⎢⎢⎣
b1
b2
...
bn
⎤
⎥⎥⎥⎦and C =
/ c1
c2
. . .
cn
0
. Prove
that if all entries b1, b2, . . . , bn and c1, c2, . . . , cn are
nonzero, then the n × n matrix A = BC has nullity
n −1.
For Problems 40–43, ﬁnd a basis and the dimension for
the row space, column space, and null space of the given
matrix A.
40. A =
% −3
−6
−6 −12
&
.
41. A =
⎡
⎣
−1
6 2
0
3
3 1
5
7 21 7 15
⎤
⎦.
42. A =
⎡
⎢⎢⎣
−4
0
3
0 10 13
6
5
2
−2
5 10
⎤
⎥⎥⎦.
43. A =
⎡
⎢⎢⎣
3 5
5
2
0
1 0
2
2
1
1 1
1 −2 −2
−2 0 −4 −2 −2
⎤
⎥⎥⎦.
44. State as many conditions as you can on an n×n matrix
A that are equivalent to its invertibility.
Project: Lattices
In this chapter, we have begun our treatment of the abstract mathematical concept of a
vector space. In this project, we will study a mathematical structure that is similar to a
vector space and which has become increasingly important to the modern-day theory of
cryptography: lattices.
Part I: Background and Basic Exercises
Likevectorspaces,latticesconsistofasetofvectors L thatareclosedundertheoperations
of addition and scalar multiplication, but the main difference between a lattice and a
vector space is that the scalars used in lattice scalar multiplication are restricted to be
integers. Thus, the only linear combinations that can be made in a lattice have the form
v = a1v1 + a2v2 + · · · + akvk,
where v1, v2, . . . , vk belong to L and a1, a2, . . . , ak are integers. A basis for a lattice
L is deﬁned in like manner to what is done in this chapter for vector spaces (Deﬁnition
4.6.1), with the key deﬁnitions being Deﬁnitions 4.4.1 (spanning set) and 4.5.4 (linear
independence). We now explore several examples of lattices in R2.
Let L denote the lattice in R2 with basis B = {(1, 0), (0, 1)}. Using the notation Z
to denote the set of integers, we note that L = Z2 = {(x, y) : x, y ∈Z}.
(a) Draw a sketch of the lattice L.
(b) Show that for all integers k, Bk := {(k, 1), (k + 1, 1)} is a basis for L. Therefore,
L has inﬁnitely many different bases.
(c) Show that for all positive integers n ≥2, Zn is a lattice with inﬁnitely many
different bases.
Let L′ denote the lattice in R2 with basis B = {(1, 2), (2, 0)}.
(a’) Draw a sketch of the lattice L′.
(b’) Show that for all integers k, Bk := {(2k + 1, 2), (2k + 3, 2)} is a basis for L.
Therefore, L′ has inﬁnitely many different bases.

338
CHAPTER 4
Vector Spaces
Let L′′ denote the lattice in R2 with basis B = {(1, 0), ( 1
2,
√
3
2 )}.
(a”) Draw a sketch of the lattice L′′.
(b”) Show that L′′ has inﬁnitely many different bases.
The lattice L is an example of a square lattice, while L′ is an example of a parallelo-
grammic lattice, and L′′ is an example of a hexagonal lattice (also called an equilateral
triangular lattice).
Part II: Extensions
(a) In Part I, we studied three of the ﬁve lattice types in R2. The other two types
are known as rectangular lattices and rhombic (or isosceles triangular) lattices.
Research information on these other two types of lattices, draw a sketch of an
example of each, and show that each of these lattices has inﬁnitely many different
bases.
(b) To what extent can the theory of vector spaces developed throughout this chapter
be carried over to lattices? Study especially the proofs of the results in Sections
4.5 and 4.6 of this chapter carefully and examine whether they can be used or
modiﬁed for the theory of lattices.

5
Inner Product Spaces
In the last chapter, we studied vector spaces — mathematical structures consisting of
a set of vectors together with two operations, addition and scalar multiplication, that
satisfy a lengthy list of algebraic rules. As such, we can treat vector spaces purely as
algebraic objects without making an appeal to any underlying geometric notions that may
be available. However, in the cases of the vector spaces R2 and R3, we already know
from elementary calculus that the geometric aspects of these spaces, such as magnitude of
vectors, angle between vectors, parallel and perpendicular lines, and so on, are extensive
and crucial to many applications in disciplines ranging from the sciences to engineering
and economics. As we will see in this chapter, it is possible to endow other vector
spaces with geometric structure that creates opportunities for important and illuminating
applications in those vector spaces as well. Vector spaces that are so equipped will be
known as inner product spaces.
One of the key tools in the geometry of R2 and R3 is the dot product of two vectors,
which we will primarily refer to in this chapter as the inner product of two vectors.
Given two vectors in R3, x = (x1, x2, x3) and y = (y1, y2, y3), recall from Equation
(3.1.4) that x · y = ||x|| ||y|| cos θ, where θ is the angle between x and y, and ||x|| and
||y|| denote the lengths of the vectors x and y, respectively. This formula presupposes
that notions of length and angle have already been established. However, the alternate
well-known formula from elementary calculus
x · y = x1y1 + x2y2 + x3y3
(5.0.1)
relies only on the components of the vectors x and y. The veriﬁcation that Equations
(3.1.4) and (5.0.1) are consistent with one another can be found in elementary calculus
texts, and we will not take the space here to review it. One advantage that Equation
(5.0.1) has over Equation (3.1.4) is that it offers a natural generalization to vectors with
n components, and we will do this in Section 5.1. We will also ﬁnd ways to equip
339

340
CHAPTER 5
Inner Product Spaces
other vector spaces we considered in Chapter 4 with the additional inner product space
structure that will facilitate the development of geometric ideas in those spaces.
For inner product spaces, we will focus our study in Section 5.2 on sets of orthog-
onal1 vectors. There are nice advantages to working with such sets. In particular, given
an orthogonal basis for an inner product space, which is a basis consisting of mutually
orthogonal vectors, calculations with respect to that basis are often relatively simple.
One notable example is the calculation of component vectors that we examined in
Section 4.7. This will be explained in full by Theorem 5.2.7 and Corollary 5.2.9.
Because of the beneﬁts of working with sets of orthogonal vectors, our attention will
turn in Section 5.3 to a procedure by which an arbitrary basis for an inner product space
can be replaced by an alternative basis for the same space which consists of orthogonal
vectors. Carrying out this procedure, which is known as the Gram-Schmidt Process, one
can reap the beneﬁts of working with a basis consisting of orthogonal vectors.
Finally in this chapter, Section 5.4 will explore one of the many important appli-
cations of inner product spaces: least squares approximation. To give a brief preview,
suppose that data points (x1, y1), (x2, y2), . . . , (xn, yn) in the xy-plane have been col-
lected in an experiment. Even if a linear relationship is expected between the xi values
and the yi values, in an experimental problem where measurement errors are likely, it
is rarely the case that a single line containing all of the data points can be found. What
we wish to do is to ﬁnd a line, commonly known as a least squares line, that best ﬁts
the data. We will explain how this line is deﬁned and discuss a procedure for ﬁnding it.
In addition, we discuss the more general problem of ﬁnding a best approximation for a
solution to an inconsistent linear system of equations.
5.1
Definition of an Inner Product Space
In this section, we extend the familiar idea of a dot product for geometric vectors to an
arbitrary vector space V . This enables us to associate a magnitude with each vector in V
and also to deﬁne the angle between two vectors in V . The major reason that we want to
do this is that, as we will see in the next two sections, it enables us to construct orthogonal
bases in a vector space, and use of such a basis often simpliﬁes the representation of
vectors. We begin with a brief review of the dot product.
Let x = (x1, x2, x3) and y = (y1, y2, y3) be two arbitrary vectors in R3, and consider
the corresponding geometric vectors
x = x1i + x2j + x3k,
y = y1i + y2j + y3k.
As mentioned previously, the dot product of x and y can be deﬁned in terms of the
components of these vectors as
x · y = x1y1 + x2y2 + x3y3,
(5.1.1)
whereas an equivalent geometric deﬁnition of the dot product is
x · y = ||x|| ||y|| cos θ,
(5.1.2)
where ||x||, ||y|| denote the lengths of x and y respectively, and 0 ≤θ ≤π is the angle
between them. (See Figure 5.1.1.)
u
z
y
x
x
y
(x1, x2, x3)
(y1, y2, y3)
Figure 5.1.1: Deﬁning the dot
product in R3.
Taking y = x in Equations (5.1.1) and (5.1.2) yields
||x||2 = x · x = x2
1 + x2
2 + x2
3,
1Orthogonal vectors are commonly called perpendicular vectors.

5.1
Definition of an Inner Product Space 341
so that the length of a geometric vector is given in terms of the dot product by
||x|| = √x · x =
!
x2
1 + x2
2 + x2
3.
Furthermore, from Equation (5.1.2), the angle between any two nonzero vectors x and
y is
cos θ =
x · y
||x|| ||y||,
(5.1.3)
which implies that x and y are orthogonal (perpendicular) if and only if
x · y = 0.
In a general vector space, we do not have a geometrical picture to guide us in
deﬁning the dot product, and hence, our deﬁnitions must be purely algebraic. We begin
by considering the vector space Rn, since there is a natural way to extend Equation (5.1.1)
in this case. Before proceeding, we note that from now on we will use the standard terms
inner product and norm in place of dot product and length, respectively.
DEFINITION
5.1.1
Let x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) be vectors in Rn. We deﬁne the
standard inner product in Rn, denoted ⟨x, y⟩, by
⟨x, y⟩= x1y1 + x2y2 + · · · + xnyn.
The norm of x is
||x|| =
"
⟨x, x⟩=
!
x2
1 + x2
2 + · · · + x2n.
Example 5.1.2
If x = (−2, 0, 4, 1, −2, 0) and y = (5, −1, −1, 6, −3, 3) in R6, then
⟨x, y⟩= (−2)(5) + (0)(−1) + (4)(−1) + (1)(6) + (−2)(−3) + (0)(3) = −2,
||x|| =
"
(−2)2 + 02 + 42 + 12 + (−2)2 + 02 =
√
25 = 5,
||y|| =
"
52 + (−1)2 + (−1)2 + 62 + (−3)2 + 32 =
√
81 = 9.
□
Basic Properties of the Standard Inner Product in Rn
In the case of Rn, the deﬁnition of the standard inner product was a natural extension of
the familiar dot product in R3. To generalize this deﬁnition further to an arbitrary vector
space, we isolate the most important properties of the standard inner product in Rn and
use them as the deﬁning criteria for a general notion of an inner product. Let us examine
the inner product in Rn more closely. We view it as a mapping that associates with any
two vectors x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) in Rn the real number
⟨x, y⟩= x1y1 + x2y2 + · · · + xnyn.
(5.1.4)
This mapping has the following four properties:
For all x, y, and z in Rn and all real numbers k,
1. ⟨x, x⟩≥0. Furthermore, ⟨x, x⟩= 0 if and only if x = 0.
2. ⟨y, x⟩= ⟨x, y⟩.

342
CHAPTER 5
Inner Product Spaces
3. ⟨kx, y⟩= k⟨x, y⟩.
4. ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.
These properties are easily established using Deﬁnition 5.1.1. For example, to prove (1),
we proceed as follows. From Deﬁnition 5.1.1,
⟨x, x⟩= x2
1 + x2
2 + · · · + x2
n.
Since this is a sum of squares of real numbers, it is necessarily nonnegative. Further,
⟨x, x⟩= 0 if and only if x1 = x2 = · · · = xn = 0; that is, if and only if x = 0. Similarly,
for (2), we have
⟨y, x⟩= y1x1 + y2x2 + · · · + ynxn = x1y1 + x2y2 + · · · + xnyn = ⟨x, y⟩.
We leave the veriﬁcation of properties (3) and (4) for the reader.
Definition of a Real Inner Product Space
We now use properties (1)–(4) as the basic deﬁning properties of an inner product in a
real vector space.
DEFINITION
5.1.3
Let V be a real vector space. A mapping that associates with each pair of vectors u
and v in V a real number, denoted ⟨u, v⟩, is called an inner product in V provided
it satisﬁes the following properties. For all u, v, and w in V , and all real numbers k,
1. ⟨v, v⟩≥0. Furthermore, ⟨v, v⟩= 0 if and only if v = 0.
2. ⟨v, u⟩= ⟨u, v⟩.
3. ⟨ku, v⟩= k⟨u, v⟩.
4. ⟨u + v, w⟩= ⟨u, w⟩+ ⟨v, w⟩.
The norm of v is deﬁned in terms of an inner product by
||v|| =
!
⟨v, v⟩.
A real vector space together with an inner product deﬁned in it is called a real inner
product space.
Remarks
1. Observe that ||v|| = √⟨v, v⟩takes a well-deﬁned nonnegative real value, since
property (1) of an inner product guarantees that the norm evaluates the square root
of a nonnegative real number.
2. It follows from the discussion above that Rn together with the inner product deﬁned
in Deﬁnition 5.1.1 is an example of a real inner product space.
3. By using properties (2) and (3) in Deﬁnition 5.1.1, we see that for all u and v in
V and all real numbers k,
⟨u, kv⟩= ⟨kv, u⟩= k⟨v, u⟩= k⟨u, v⟩.
Thus, property (3) has a parallel result that enables us to pull scalars out from the
second entry of the inner product mapping.

5.1
Definition of an Inner Product Space 343
4. Caution: For all u and v in V and all real numbers k, we have
⟨ku, kv⟩= k2⟨u, v⟩
by using the previous remark and property (3). This should not be confused with
scalar multiplication in the vector space R2.
a
x
y
y 5 [f(x)]2
b
Figure 5.1.2: ⟨f, f ⟩gives the area
between the graph of y = [ f (x)]2
and the x-axis, lying over the
interval [a, b].
One of the fundamental inner products arises in the vector space C0[a, b] of all
real-valued functions that are continuous on the interval [a, b]. In this vector space, we
deﬁne the mapping ⟨f, g⟩by
⟨f, g⟩=
" b
a
f (x)g(x) dx,
(5.1.5)
for all f and g in C0[a, b]. We establish that this mapping deﬁnes an inner product in
C0[a, b] by verifying properties (1)–(4) of Deﬁnition 5.1.3. If f is in C0[a, b], then
⟨f, f ⟩=
" b
a
[ f (x)]2 dx.
Since the integrand, [ f (x)]2, is a nonnegative continuous function, it follows that ⟨f, f ⟩
measures the area between the graph y = [ f (x)]2 and the x-axis on the interval [a, b].
(See Figure 5.1.2.) Consequently, ⟨f, f ⟩≥0. Furthermore, ⟨f, f ⟩= 0 if and only if
there is zero area between the graph y = [ f (x)]2 and the x-axis; that is, if and only if
[ f (x)]2 = 0
for all x in [a, b].
Hence, ⟨f, f ⟩= 0 if and only if f (x) = 0, for all x in [a, b], so f must be the zero
function. (See Figure 5.1.3.) Consequently, property (1) of Deﬁnition 5.1.3 is satisﬁed.
Now let f, g, and h be in C0[a, b], and let k be an arbitrary real number. Then
⟨g, f ⟩=
" b
a
g(x) f (x) dx =
" b
a
f (x)g(x) dx = ⟨f, g⟩.
Hence, property (2) of Deﬁnition 5.1.3 is satisﬁed.
a
x
y
f(x) 5 0 for all x in [a, b]
b
Figure 5.1.3: ⟨f, f ⟩= 0 if and
only if f is the zero function.
For property (3), we have
⟨k f, g⟩=
" b
a
(k f )(x)g(x) dx =
" b
a
k f (x)g(x) dx = k
" b
a
f (x)g(x) dx = k⟨f, g⟩,
as needed. Finally,
⟨f + g, h⟩=
" b
a
( f + g)(x)h(x) dx =
" b
a
[ f (x) + g(x)]h(x) dx
=
" b
a
f (x)h(x) dx +
" b
a
g(x)h(x) dx = ⟨f, h⟩+ ⟨g, h⟩,
so that property (4) of Deﬁnition 5.1.3 is satisﬁed. We can now conclude that Equation
(5.1.5) does deﬁne an inner product in the vector space C0[a, b].
Example 5.1.4
UseEquation(5.1.5)todeterminetheinnerproductofthefollowingfunctionsinC0[0, 1]:
f (x) = 8x,
g(x) = x2 −1.
Also ﬁnd || f || and ||g||.

344
CHAPTER 5
Inner Product Spaces
Solution:
From Equation (5.1.5),
⟨f, g⟩=
" 1
0
8x(x2 −1) dx =
#
2x4 −4x2
$1
0
= −2.
Moreover, we have
|| f || =
%" 1
0
64x2 dx =
8
√
3
and
||g|| =
%" 1
0
(x2 −1)2 dx =
%" 1
0
(x4 −2x2 + 1) dx =
&
8
15.
□
Example 5.1.5
Let V = P2(R) and for p(x) = a0 + a1x + a2x2 and q(x) = b0 + b1x + b2x2 in V ,
consider the mapping
⟨p(x), q(x)⟩= a0b1 + a1b2 + a2b0.
Decide with proof whether or not this mapping deﬁnes a valid inner product on V .
Solution:
There are many places here where we could choose to begin an analysis.
For one thing, the proposed mapping gives
⟨q(x), p(x)⟩= b0a1 + b1a2 + b2a0,
which is clearly not the same expression as ⟨p(x), q(x)⟩, thus showing that property (2)
will fail. To give a speciﬁc illustration, note that
⟨1, x⟩= (1)(1)+(0)(0)+(0)(0) = 1
while
⟨x, 1⟩= (0)(0)+(1)(0)+(0)(1) = 0.
Alternatively, one can focus on property (1) and observe, as a speciﬁc example, that
⟨x, x⟩= (0)(1) + (1)(0) + (0)(0) = 0,
which shows a nonzero vector p(x) = x with ⟨p(x), p(x)⟩= 0, a violation of
property (1).
The reader is invited to check that properties (3) and (4) in the deﬁnition of an
inner product hold for this example. However, since we have already demonstrated that
properties (1) and (2) fail here, the proposed mapping does not deﬁne a valid inner
product on V .
□
As the reader has perhaps already realized, a given vector space V can be equipped in
many different ways with a valid inner product. In Example 5.1.4, simply by changing the
given interval [0, 1] to any other interval [a, b] would change the calculations of integrals
and result in a different inner product. In the case of Rn, we have already studied the
standard inner product (5.1.4). This is indeed the most important inner product on Rn,
and unless stated otherwise, the reader should assume we are using the standard inner
product when working with Rn. However, there are other ways to endow Rn with an inner
product. To understand why we might want to do this, consider the following scenario.
Suppose we have these vectors in R2:
v1 =
#
.00003
−20, 000, 000
$
,
v2 =
#
−.00001
17, 500, 000
$
,
v3 =
#
.00015
22, 000, 000
$
.

5.1
Definition of an Inner Product Space 345
If we use the standard inner product on R2 to measure the norms of these vectors,
we will ﬁnd that their norms will be approximately equal to the absolute value of the
second component, meaning that the ﬁrst component will essentially be lost:
||v1|| ≈20, 000, 000,
||v2|| ≈17, 500, 000,
||v3|| ≈22, 000, 000.
Therefore, it might be useful to consider an alternative inner product on R2 that is able
to detect changes in the two components of the vectors in R2 roughly equally. Consider
the following deﬁnition, where v = (v1, v2) and w = (w1, w2):
⟨v, w⟩= (100, 000)2v1w1 +
'
1
10, 000, 000
(2
v2w2.
(5.1.6)
We will see momentarily in Example 5.1.6 that this weighted inner product does
indeed satisfy the requirements of Deﬁnition 5.1.3. With this modiﬁcation to the standard
inner product on R2, small relative changes in the ﬁrst components of these vectors will
impact the norm on roughly the same scale as small relative changes in the second
components of these vectors. For instance, using Equation (5.1.6), we ﬁnd that
||v1|| =
%
(100, 000)2(.00003)2 +
'
1
10, 000, 000
(2
(−20, 000, 000)2
=
!
32 + (−2)2 =
√
13,
whereas if we deﬁne
v11 =
#
.00006
−20, 000, 000
$
and
v12 =
#
.00003
−40, 000, 000
$
by doubling the ﬁrst and second components of v1, respectively, we can quickly compute
that ||v11|| =
√
40 and ||v12|| =
√
25 = 5, changes to the norm of roughly equal
magnitude.
Example 5.1.6
If v = (v1, v2, . . . , vn) and w = (w1, w2, . . . , wn) are vectors in Rn, then show that the
mapping ⟨, ⟩deﬁned by
⟨v, w⟩= k1v1w1 + k2v2w2 + · · · + knvnwn
(5.1.7)
is a valid inner product on Rn if and only if the constants k1, k2, . . . , kn are all positive
real numbers. (The constants k1, k2, . . . , kn are called the weights associated with the
inner product.)
Solution:
Using (5.1.7), we have
⟨v, v⟩= k1v2
1 + k2v2
2 + · · · + knv2
n.
(5.1.8)
Therefore, for i = 1, 2, . . . , n, we have ⟨ei, ei⟩= ki. (Recall that ei denotes the ith
standard basis vector in Rn.) Therefore, if (5.1.7) is a valid inner product on Rn, property
(1) requires that ki > 0 for i = 1, 2, . . . , n.
Conversely, assume that all of the constants k1, k2, . . . , kn are positive. We see at
once from (5.1.8) that ⟨v, v⟩≥0 for all v in Rn, and that ⟨v, v⟩= 0 only if v1 = v2 =
· · · = vn = 0; that is, only if v = 0. This conﬁrms that property (1) holds. The remaining
properties (2)–(4) are routine to verify and are left for the exercises (Problem 28).
□
We have already seen that the norm concept generalizes the length of a geometric
vector. Our next goal is to show how an inner product enables us to deﬁne the angle

346
CHAPTER 5
Inner Product Spaces
between two vectors in an abstract vector space. The key result is the Cauchy-Schwarz
inequality established in the next theorem.
Theorem 5.1.7
(Cauchy-Schwarz Inequality)
Let u and v be arbitrary vectors in a real inner product space V . Then
|⟨u, v⟩| ≤||u|| ||v||.
(5.1.9)
Proof Let k be an arbitrary real number. For the vector u + kv, we have
0 ≤||u + kv||2 = ⟨u + kv, u + kv⟩.
(5.1.10)
But, using the properties of a real inner product,
⟨u + kv, u + kv⟩= ⟨u, u + kv⟩+ ⟨kv, u + kv⟩
= ⟨u + kv, u⟩+ ⟨u + kv, kv⟩
= ⟨u, u⟩+ ⟨kv, u⟩+ ⟨u, kv⟩+ ⟨kv, kv⟩
= ⟨u, u⟩+ 2⟨kv, u⟩+ k⟨v, kv⟩
= ⟨u, u⟩+ 2⟨kv, u⟩+ k⟨kv, v⟩
= ⟨u, u⟩+ 2⟨kv, u⟩+ k2⟨v, v⟩
= ||u||2 + 2k⟨v, u⟩+ k2||v||2.
Consequently, (5.1.10) implies that
||v||2k2 + 2⟨u, v⟩k + ||u||2 ≥0.
(5.1.11)
The left-hand side of this inequality deﬁnes the quadratic expression
P(k) = ||v||2k2 + 2⟨u, v⟩k + ||u||2.
The discriminant of this quadratic is
" = 4(⟨u, v⟩)2 −4||u||2||v||2.
If " > 0, then P(k) has two real and distinct roots. This would imply that the graph of P
crosses the k-axis and, therefore, P would assume negative values, contrary to (5.1.11).
Consequently, we must have " ≤0. That is,
4(⟨u, v⟩)2 −4||u||2||v||2 ≤0,
or equivalently,
(⟨u, v⟩)2 ≤||u||2||v||2.
Hence,
|⟨u, v⟩| ≤||u|| ||v||.
If u and v are arbitrary vectors in a real inner product space V , then ⟨u, v⟩is a real
number, and therefore, (5.1.9) can be written in the equivalent form
−||u|| ||v|| ≤⟨u, v⟩≤||u|| ||v||.
Consequently, provided that u and v are nonzero vectors, we have
−1 ≤
⟨u, v⟩
||u|| ||v|| ≤1.

5.1
Definition of an Inner Product Space 347
Thus, each pair of nonzero vectors in a real inner product space V determines a unique
angle θ by
cos θ =
⟨u, v⟩
||u|| ||v||,
0 ≤θ ≤π.
(5.1.12)
We call θ the angle between u and v. In the case when u and v are geometric vectors,
the formula (5.1.12) coincides with Equation (5.1.3).
Example 5.1.8
Determine the angle between the vectors u = (1, −1, 2, 3) and v = (−2, 1, 2, −2)
in R4.
Solution:
Using the standard inner product in R4 yields
⟨u, v⟩= −5,
||u|| =
√
15,
||v|| =
√
13,
so that the angle between u and v is given by
cos θ = −
5
√
15
√
13
= −
√
195
39 ,
0 ≤θ ≤π.
Hence,
θ = arccos
'
−
√
195
39
(
≈1.937 radians ≈110◦58′.
□
Example 5.1.9
Usetheinnerproduct(5.1.5)todeterminetheanglebetweenthefunctions f1(x) = sin 2x
and f2(x) = cos 2x on the interval [−π, π].
Solution:
Using the inner product (5.1.5), we have
⟨f1, f2⟩=
" π
−π
sin 2x cos 2x dx = 1
2
" π
−π
sin 4x dx = 1
8 (−cos 4x)
))π
−π = 0.
Consequently, the angle between the two functions satisﬁes
cos θ = 0,
0 ≤θ ≤π,
which implies that θ = π/2. We say that the functions are orthogonal on the interval
[−π, π], relative to the inner product (5.1.5). In the next section, we will have much
more to say about orthogonality of vectors.
□
Complex Inner Products2
The preceding discussion has been concerned with real vector spaces. Let us consider
generalizing the deﬁnition of an inner product to the complex vector space Cn. By
analogy with Deﬁnition 5.1.1, one might think that the natural inner product in Cn
would be obtained by summing the products of corresponding components of vectors
in Cn in exactly the same manner that was done in the standard inner product for Rn.
However, one of the reasons for introducing an inner product is so that we can obtain
a concept of “length” of a vector. In order for a quantity to be considered a reasonable
measure of length, we would want it to be a nonnegative real number that vanishes if
and only if the vector itself is the zero vector (property (1) of a real inner product). But,
if we apply the inner product in Rn given in Deﬁnition 5.1.1 to vectors in Cn, then since
2In the remainder of the text, the only complex inner product that we will require is the standard inner
product in Cn, and this is only needed in Section 7.5.

348
CHAPTER 5
Inner Product Spaces
the components of vectors in Cn are complex numbers, it follows that the resulting norm
of a vector in Cn would be a complex number also. Furthermore, applying the R2 inner
product to, for example, the vector v = (1 −i, 1 + i), we obtain
||v||2 = (1 −i)2 + (1 + i)2 = 0,
which means that a nonzero vector would have zero “length.” To rectify this situation,
we must deﬁne an inner product in Cn more carefully. We take advantage of complex
conjugation to do this, as the next deﬁnition shows.
DEFINITION
5.1.10
If u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn) are vectors in Cn, we deﬁne the
standard inner product in Cn by3
⟨u, v⟩= u1v1 + u2v2 + · · · + unvn.
The norm of v is deﬁned to be the real number
||v|| =
!
⟨v, v⟩=
!
|v1|2 + |v2|2 + · · · + |vn|2.
The preceding inner product is a mapping that associates with the two vectors
u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn) in Cn the scalar
⟨u, v⟩= u1v1 + u2v2 + · · · + unvn.
In general, ⟨u, v⟩will be nonreal (i.e., it will have a nonzero imaginary part). The key
point to notice is that the norm of v is always a real number, even though the separate
components of v are complex numbers.
Example 5.1.11
If u = (5 −i, −3i, 6 + 2i) and v = (3, 2i, −1 −4i) in C3, ﬁnd ⟨u, v⟩, ||u||, and ||v||.
Solution:
Using Deﬁnition 5.1.10,
⟨u, v⟩= (5 −i)(3) + (−3i)(−2i) + (6 + 2i)(−1 + 4i)
= (15 −3i) + (−6) + (−14 + 22i)
= −5 + 19i,
||u|| =
!
⟨u, u⟩=
!
(5 −i)(5 + i) + (−3i)(3i) + (6 + 2i)(6 −2i)
=
√
26 + 9 + 40 =
√
75 = 5
√
3,
and
||v|| =
!
⟨v, v⟩=
!
(3)(3) + (2i)(−2i) + (−1 −4i)(−1 + 4i)
=
√
9 + 4 + 17 =
√
30.
□
The standard inner product in Cn satisﬁes properties (1),(3), and (4) in Deﬁnition
5.1.3, but not property (2). We now derive the appropriate generalization of property
(2) when using the standard inner product in Cn. Let u = (u1, u2, . . . , un) and v =
(v1, v2, . . . , vn) be vectors in Cn. Then, from Deﬁnition 5.1.10,
⟨v, u⟩= v1u1 + v2u2 + · · · + vnun = u1v1 + u2v2 + · · · + unvn = ⟨u, v⟩.
3Recall that if z = a + ib, then z = a −ib and |z|2 = zz = (a + ib)(a −ib) = a2 + b2.

5.1
Definition of an Inner Product Space 349
Thus,
⟨v, u⟩= ⟨u, v⟩.
We now use the properties satisﬁed by the standard inner product in Cn to deﬁne an inner
product in an arbitrary (that is, real or complex) vector space.
DEFINITION
5.1.12
Let V be a (real or complex) vector space. A mapping that associates with each pair
of vectors u, v in V a scalar, denoted ⟨u, v⟩, is called an inner product in V provided
it satisﬁes the following properties. For all u, v and w in V and all scalars k,
1. ⟨v, v⟩≥0. Furthermore, ⟨v, v⟩= 0 if and only if v = 0.
2. ⟨v, u⟩= ⟨u, v⟩.
3. ⟨ku, v⟩= k⟨u, v⟩.
4. ⟨u + v, w⟩= ⟨u, w⟩+ ⟨v, w⟩.
The norm of v is deﬁned in terms of the inner product by
||v|| =
!
⟨v, v⟩.
Remark
Notice that the properties in the preceding deﬁnition reduce to those in
Deﬁnition 5.1.3 in the case that V is a real vector space, since in such a case, the
complex conjugates are unnecessary. Thus, this deﬁnition is a consistent extension of
Deﬁnition 5.1.3.
Example 5.1.13
Use properties (2) and (3) of Deﬁnition 5.1.12 to prove that in an inner product space
⟨u, kv⟩= k⟨u, v⟩
for all vectors u, v and all scalars k.
Solution:
From properties (2) and (3), we have
⟨u, kv⟩= ⟨kv, u⟩= k⟨v, u⟩= k ⟨v, u⟩= k ⟨u, v⟩.
Notice that in the particular case of a real vector space, the foregoing result reduces to
⟨u, kv⟩= k⟨u, v⟩,
since in such a case the scalars are real numbers.
□
Exercises for 5.1
Key Terms
Inner product, Axioms of an inner product, Real (com-
plex) inner product space, Norm, Angle, Cauchy-Schwarz
Inequality.
Skills
• Know the four inner product space axioms.
• Be able to check whether or not a proposed inner prod-
uctonavectorspace V satisﬁestheinnerproductspace
axioms.
• Be able to compute the inner product of two vectors
in an inner product space.
• Be able to ﬁnd the norm of a vector in an inner product
space.
• Be able to ﬁnd the angle between two vectors in an
inner product space.

350
CHAPTER 5
Inner Product Spaces
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If v and w are linearly independent vectors in an inner
product space V , then ⟨v, w⟩= 0.
(b) In any inner product space V , we have
⟨kv, kw⟩= k⟨v, w⟩.
(c) If ⟨v1, w⟩= ⟨v2, w⟩= 0 in an inner product space V ,
then
⟨c1v1 + c2v2, w⟩= 0.
(d) In any inner product space V , ⟨x + y, x −y⟩< 0 if
and only if ||x|| < ||y||.
(e) In any vector space V , there is at most one valid inner
product ⟨, ⟩that can be deﬁned on V .
(f) The angle between the vectors v and w in an inner
product space V is the same as the angle between the
vectors −2v and −2w.
(g) If p(x) = a0+a1x+a2x2 andq(x) = b0+b1x+b2x2,
then we can deﬁne an inner product on P2(R) via
⟨p, q⟩= a0b0.
Problems
1. Use the standard inner product in R5 to determine the
angle between the vectors v = (0, −2, 1, 4, 1) and
w = (−3, 1, −1, 0, 3).
2. Use the standard inner product in R4 to determine
the angle between the vectors v = (1, 3, −1, 4) and
w = (−1, 1, −2, 1).
3. If f (x) = sin x and g(x) = x on [0, π], use the func-
tion inner product (5.1.5) to determine the angle be-
tween f and g.
4. If f (x) = sin x and g(x) = 2 cos x + 4 on [0, π/2],
use the function inner product (5.1.5) to determine the
angle between f and g.
5. Let m and n be positive real numbers. If f (x) = xm
and g(x) = xn on an arbitrary interval [a, b], use the
function inner product (5.1.5) to determine the angle
between f and g in terms of a, b, m, and n.
6. If v = (2 + i, 3 −2i, 4 + i) and w = (−1 + i, 1 −3i,
3 −i), use the standard inner product in C3 to deter-
mine, ⟨v, w⟩, ||v||, and ||w||.
7. If v = (6 −3i, 4, −2 + 5i, 3i) and w = (i, 2i, 3i, 4i),
use the standard inner product in C4 to determine,
⟨v, w⟩, ||v||, and ||w||.
8. Let A =
# a11
a12
a21
a22
$
and B =
# b11
b12
b21
b22
$
be vectors
in M2(R). Show that the mapping
⟨A, B⟩= a11b11
does not deﬁne a valid inner product on M2(R). Which
of the four properties of an inner product, if any, do
hold? Justify your answer.
9. Referring to A and B in Problem 8, show that the
mapping
⟨A, B⟩= det(AB)
does not deﬁne a valid inner product on M2(R). Which
of the four properties of an inner product, if any, do
hold? Justify your answer.
10. Referring to A and B in Problem 8, show that the
mapping
⟨A, B⟩= a11b22 + a12b21 + a21b12 + a22b11
does not deﬁne a valid inner product on M2(R). Which
of the four properties of an inner product, if any, do
hold? Justify your answer.
11. Referring to A and B in Problem 8, show that the
mapping
⟨A, B⟩= a11b11 + a12b12 + a21b21 + a22b22
(5.1.13)
deﬁnes a valid inner product in M2(R).
For Problems 12–13, use the inner product (5.1.13) in Prob-
lem 11 to determine ⟨A, B⟩, ||A||, and ||B||. Also, determine
the angle between the given matrices.
12. A =
#
3 2
−2 4
$
, B =
#
1 1
−2 1
$
.
13. A =
# 2 −1
3
5
$
, B =
#
3 1
−1 2
$
.
14. Let p1(x) = a + bx and p2(x) = c + dx be vectors
in P1(R). Determine a mapping ⟨p1, p2⟩that deﬁnes
an inner product on P1(R).

5.1
Definition of an Inner Product Space 351
15. Let V = C0[0, 1] and for f and g in V , consider the
mapping
⟨f, g⟩=
" 1/2
0
f (x)g(x)dx.
Does this deﬁne a valid inner product on V ? Show
why or why not.
16. Let V = C0[0, 1] and for f and g in V , consider the
mapping
⟨f, g⟩=
" 1
0
x f (x)g(x)dx.
Does this deﬁne a valid inner product on V ? Show
why or why not.
17. Let V = C0[−1, 0] and for f and g in V , consider
the mapping
⟨f, g⟩=
" 0
−1
x f (x)g(x)dx.
Does this deﬁne a valid inner product on V ? Show
why or why not.
18. Consider the vector space R2. Deﬁne the mapping
⟨, ⟩by
⟨v, w⟩= 2v1w1 + v1w2 + v2w1 + 2v2w2 (5.1.14)
for all vectors v = (v1, v2) and w = (w1, w2) in R2.
Verify that Equation (5.1.14) deﬁnes an inner product
on R2.
ForProblems19–21,determinetheinnerproductofthegiven
vectors using (a) the inner product (5.1.14) in Problem 18,
(b) the standard inner product in R2.
19. v = (1, 0), w = (−1, 2).
20. v = (2, −1), w = (3, 6).
21. v = (1, −2), w = (2, 1).
22. Consider the vector space R2. Deﬁne the mapping
⟨, ⟩by
⟨v, w⟩= v1w1 −v2w2,
(5.1.15)
for all vectors v = (v1, v2) and w = (w1, w2). Verify
that all of the properties in Deﬁnition 5.1.3 except (1)
are satisﬁed by (5.1.15).
The mapping (5.1.15) is called a pseudo-inner prod-
uct in R2 and, when generalized to R4, is of fundamen-
tal importance in Einstein’s special relativity theory.
23. Using Equation (5.1.15) in Problem 22, determine all
nonzero vectors satisfying ⟨v, v⟩= 0. Such vectors
are called null vectors.
24. Using Equation (5.1.15) in Problem 22, determine all
vectors satisfying ⟨v, v⟩< 0. Such vectors are called
timelike vectors.
25. Using Equation (5.1.15) in Problem 22, determine all
vectors satisfying ⟨v, v⟩> 0. Such vectors are called
spacelike vectors.
26. Make a sketch of R2 and indicate the position of the
null, timelike, and spacelike vectors.
27. Consider the vector space R2. Deﬁne the mapping
⟨, ⟩by
⟨v, w⟩= v1w1 −v1w2 −v2w1 + 4v2w2
for all vectors v = (v1, v2) and w = (w1, w2). Verify
that ⟨, ⟩deﬁnes a valid inner product on R2.
28. Consider the vector space Rn, and let v
=
(v1, v2, . . . , vn) and w = (w1, w2, . . . , wn) be vec-
tors in Rn. Complete the proof begun in Example 5.1.6
that the mapping ⟨, ⟩deﬁned by
⟨v, w⟩= k1v1w1 + k2v2w2 + · · · + knvnwn
is a valid inner product on Rn if and only if the con-
stants k1, k2, . . . , kn are all positive.
29. Prove from the inner product axioms that, in any inner
product space V , ⟨v, 0⟩= 0 for all v in V .
30. Prove from the inner product axioms that for all vec-
tors u, v, and w in an inner product space V , we have
⟨u, v + w⟩= ⟨u, v⟩+ ⟨u, w⟩.
31. Use the previous exercise together with the inner prod-
uct space axioms to derive a formula for ⟨u+v, w+x⟩
for vectors u, v, w, and x in an inner product space V .
32. Let V be an inner product space with vectors v and w
with ||v|| = 3, ||w|| = 4, and ⟨v, w⟩= −2. Compute
the following:
(a) ||v + w||.
(b) ⟨3v + w, −2v + 3w⟩.
(c) ⟨−w, 5v + 2w⟩.
33. Let V be an inner product space with vectors v and w
with ||v|| = 2, ||w|| = 6, and ⟨v, w⟩= 3. Compute
the following:
(a) ||w −2v||.
(b) ||2v + 5w||.
(c) ⟨2v −w, 4w⟩.

352
CHAPTER 5
Inner Product Spaces
34. Let V be an inner product space with vectors u, v, and
w with ||u|| = 1, ||v|| = 2, ||w|| = 3, ⟨u, v⟩= 4,
⟨u, w⟩= 5, and ⟨v, w⟩= 0. Compute the following:
(a) ||u + v + w||.
(b) ||2u −3v −w||.
(c) ⟨u + 2w, 3u −3v + w⟩.
35. Let V be a real inner product space.
(a) Prove that for all v, w ∈V ,
||v + w||2 = ||v||2 + 2⟨v, w⟩+ ||w||2.
[Hint: ||v + w||2 = ⟨v + w, v + w⟩.]
(b) Two vectors v and w in an inner product space
V are called orthogonal if ⟨v, w⟩= 0. Use (a)
to prove the general Pythagorean Theorem: If
v and w are orthogonal in an inner product space
V , then
||v + w||2 = ||v||2 + ||w||2.
36. Let V be a real inner product space. Prove that for all
v, w in V ,
(a) ||v + w||2 −||v −w||2 = 4⟨v, w⟩.
(b) ||v + w||2 + ||v −w||2 = 2(||v||2 + ||w||2).
37. Let V be a complex inner product space. Prove that
for all v, w in V ,
||v + w||2 = ||v||2 + 2Re(⟨v, w⟩) + ||v||2,
where Re denotes the real part of a complex number.
5.2
Orthogonal Sets of Vectors and Orthogonal Projections
The discussion in the previous section has shown how an inner product can be used
to deﬁne the angle between two nonzero vectors. In particular, if the inner product of
two nonzero vectors is zero, then the angle between those two vectors is π/2 radians,
and therefore it is natural to call such vectors orthogonal (perpendicular). The following
deﬁnition extends the idea of orthogonality into an arbitrary inner product space.
DEFINITION
5.2.1
Let V be an inner product space.
1. Two vectors u and v in V are said to be orthogonal if ⟨u, v⟩= 0.
2. A set of nonzero vectors {v1, v2, . . . , vk} in V is called an orthogonal set of
vectors if
⟨vi, v j⟩= 0,
whenever i ̸= j.
(That is, every vector is orthogonal to every other vector in the set.)
3. A vector v in V is called a unit vector if ||v|| = 1.
4. An orthogonal set of unit vectors is called an orthonormal set of vectors. Thus,
{v1, v2, . . . , vk} in V is an orthonormal set if and only if
(a) ⟨vi, v j⟩= 0 whenever i ̸= j.
(b) ⟨vi, vi⟩= 1 for all i = 1, 2, . . . , k.
Remarks
1. The conditions in (4a) and (4b) can be written compactly in terms of the Kronecker
delta symbol as
⟨vi, v j⟩= δi j,
i, j = 1, 2, . . . , k.

5.2
Orthogonal Sets of Vectors and Orthogonal Projections 353
2. Note that the inner products occurring in Deﬁnition 5.2.1 will depend upon which
inner product space we are working in. Two vectors could be orthogonal to one
another with respect to one inner product but not orthogonal with respect to a
different inner product on the same vector space.
3. If v is any nonzero vector, then
1
||v|| v is a unit vector, since the properties of an
inner product imply that
* 1
||v|| v,
1
||v|| v
+
=
1
||v||2 ⟨v, v⟩=
1
||v||2 ||v||2 = 1.
Using Remark (3) above, we can take an orthogonal set of vectors {v1, v2, . . . , vk}
and create a new set {u1, u2, . . . , uk}, where ui =
1
||vi||vi is a unit vector for each i.
Using the properties of an inner product, it is easy to see that the new set {u1, u2, . . . , uk}
is an orthonormal set (see Problem 34). The process of replacing the vi by the ui is called
normalization.
Example 5.2.2
Verify that {(−2, 1, 3, 0), (0, −3, 1, −6), (−2, −4, 0, 2)} is an orthogonal set of vectors
in R4, and use it to construct an orthonormal set of vectors in R4.
Solution:
Let v1 = (−2, 1, 3, 0), v2 = (0, −3, 1, −6), and v3 = (−2, −4, 0, 2).
Then
⟨v1, v2⟩= 0,
⟨v1, v3⟩= 0,
⟨v2, v3⟩= 0,
so that the given set of vectors is an orthogonal set. Scalar multiplying each vector in the
set by the reciprocal of its norm yields the following orthonormal set:
, 1
√
14
v1,
1
√
46
v2,
1
2
√
6
v3
-
.
□
Example 5.2.3
Verify that the set of functions {1, sin x, cos x} is orthogonal in C0[−π, π] with in-
ner product given in (5.1.5), and use it to construct an orthonormal set of functions in
C0[−π, π].
Solution:
Let f1(x) = 1, f2(x) = sin x, and f3(x) = cos x. We have
⟨f1, f2⟩=
" π
−π
sin x dx = 0,
⟨f1, f3⟩=
" π
−π
cos x dx = 0,
⟨f2, f3⟩=
" π
−π
sin x cos x dx =
#1
2 sin2 x
$π
−π
= 0,
so that the functions are indeed orthogonal on [−π, π]. Taking the norm of each function,
we obtain
|| f1|| =
%" π
−π
1 dx =
√
2π,
|| f2|| =
%" π
−π
sin2 x dx =
%" π
−π
1
2(1 −cos 2x) dx = √π,
|| f3|| =
%" π
−π
cos2 x dx =
%" π
−π
1
2(1 + cos 2x) dx = √π.

354
CHAPTER 5
Inner Product Spaces
Thus an orthonormal set of functions on [−π, π] is
,
1
√
2π
,
1
√π sin x,
1
√π cos x
-
.
□
Orthogonal and Orthonormal Bases
In the analysis of geometric vectors in elementary calculus courses, it is usual to use
the standard basis {i, j, k}. Notice that this set of vectors is in fact an orthonormal set.
The introduction of an inner product in a vector space opens up the possibility of using
similar bases in a general ﬁnite-dimensional vector space. The next deﬁnition introduces
the appropriate terminology.
DEFINITION
5.2.4
A basis {v1, v2, . . . , vn} for a (ﬁnite-dimensional) inner product space is called an
orthogonal basis if
⟨vi, v j⟩= 0
whenever i ̸= j,
and it is called an orthonormal basis if
⟨vi, v j⟩= δi j,
i, j = 1, 2, . . . , n.
There are two natural questions to be asked at this point: (1) Why is it beneﬁcial
to work with an orthogonal or orthonormal basis of vectors? (2) How can we obtain
an orthogonal or orthonormal basis for an inner product space V ? We address the ﬁrst
question in the remainder of this section, and then we will address the second question
in the next section.
In light of our work in Chapter 4, the importance of our next theorem should be
self-evident.
Theorem 5.2.5
If {v1, v2, . . . , vk} is an orthogonal set of nonzero vectors in an inner product space V ,
then {v1, v2, . . . , vk} is linearly independent.
Proof Assume that
c1v1 + c2v2 + · · · + ckvk = 0.
(5.2.1)
We will show that c1 = c2 = · · · = ck = 0. Taking the inner product of each side of
(5.2.1) with vi, we ﬁnd that
⟨vi, c1v1 + c2v2 + · · · + ckvk⟩= ⟨vi, 0⟩= 0,
where we have used Problem 29 of Section 5.1 in the last step. Using the inner product
properties on the left side, we have
c1⟨vi, v1⟩+ c2⟨vi, v2⟩+ · · · + ck⟨vi, vk⟩= 0.
Finally, using the fact that for all j ̸= i, we have ⟨vi, v j⟩= 0, we conclude that
ci⟨vi, vi⟩= 0.
Since vi ̸= 0, it follows that ci = 0, and this holds for each i with 1 ≤i ≤k.

5.2
Orthogonal Sets of Vectors and Orthogonal Projections 355
Example 5.2.6
Let V = M2(R), let S be the subspace of all 2 × 2 symmetric matrices, and let
B =
,#
2 −1
−1
0
$
,
# 1 1
1 2
$
,
# 2
2
2 −3
$-
.
Deﬁne an inner product on V via4
*# a11
a12
a21
a22
$
,
# b11
b12
b21
b22
$+
= a11b11 + a12b12 + a21b21 + a22b22.
Show that B is an orthogonal basis for S.
Solution:
According to Example 4.6.18, we already know that dim[S] = 3. Using
the given inner product, it can be directly shown that B is an orthogonal set, and hence,
Theorem 5.2.5 implies that B is linearly independent. Therefore, by Theorem 4.6.10, B
is a basis for the subspace S of V .
□
Let V be a (ﬁnite-dimensional) inner product space, and suppose that we have an
orthogonal basis {v1, v2, . . . , vn} for V . As we saw in Section 4.7, any vector v in V
can be written uniquely in the form
v = c1v1 + c2v2 + · · · + cnvn,
(5.2.2)
where the unique n-tuple (c1, c2, . . . , cn) consists of the components of v relative to the
given basis. It is easier to determine the components ci in the case of an orthogonal basis
than it is for other bases, because we can simply form the inner product of both sides of
(5.2.2) with vi as follows:
⟨v, vi⟩= ⟨c1v1 + c2v2 + · · · + cnvn, vi⟩
= c1⟨v1, vi⟩+ c2⟨v2, vi⟩+ · · · + cn⟨vn, vi⟩
= ci||vi||2,
where the last step follows by using the orthogonality properties of the basis {v1, v2, . . . ,
vn}. Therefore, we have proved the following theorem.
Theorem 5.2.7
Let V be a (ﬁnite-dimensional) inner product space with orthogonal basis {v1, v2, . . . ,
vn}. Then any vector v ∈V may be expressed in terms of the basis as
v =
'⟨v, v1⟩
||v1||2
(
v1 +
'⟨v, v2⟩
||v2||2
(
v2 + · · · +
'⟨v, vn⟩
||vn||2
(
vn.
Theorem 5.2.7 gives a simple formula for writing an arbitrary vector in an inner
product space V as a linear combination of vectors in an orthogonal basis for V . Let us
illustrate with an example.
Example 5.2.8
Let V , S, and B be as in Example 5.2.6. Find the components of the vector v =
#
0 −1
−1
2
$
relative to B.
4This deﬁnes a valid inner product on V by Problem 11 in Section 5.1.

356
CHAPTER 5
Inner Product Spaces
Solution:
From the formula given in Theorem 5.2.7, we have
v = 2
6
#
2 −1
−1
0
$
+ 2
7
# 1 1
1 2
$
−10
21
# 2
2
2 −3
$
,
so the component vector of v relative to B is
[v]B =
⎡
⎣
1/3
2/7
−10/21
⎤
⎦.
□
If the orthogonal basis {v1, v2, . . . , vn} for V is in fact orthonormal, then since
||vi|| = 1 for each i, we immediately deduce the following corollary of Theorem 5.2.7.
Corollary 5.2.9
Let V be a (ﬁnite-dimensional) inner product space with orthonormal basis {v1, v2, . . . ,
vn}. Then any vector v ∈V may be expressed in terms of the basis as
v = ⟨v, v1⟩v1 + ⟨v, v2⟩v2 + · · · + ⟨v, vn⟩vn.
Remark
Corollary 5.2.9 tells us that the components of a given vector v relative to
the orthonormal basis {v1, v2, . . . , vn} are precisely the numbers ⟨v, vi⟩, for 1 ≤i ≤n.
Thus, by working with an orthonormal basis for a vector space, we have a simple method
for getting the components of any vector in the vector space.
Example 5.2.10
We can write an arbitrary vector in Rn, v = (a1, a2, . . . , an), in terms of the standard
basis {e1, e2, . . . , en} by noting that ⟨v, ei⟩= ai. Thus, v = a1e1 + a2e2 + · · · + anen.
□
Example 5.2.11
We can equip the vector space P1(R) with the inner product
⟨p, q⟩=
" 1
−1
p(x)q(x) dx,
thus making P1(R) into an inner product space. Let
p0 =
1
√
2
and
p1 =
√
3x
√
2
.
Verify that B = {p0, p1} forms an orthonormal basis for P1(R) and use Corollary 5.2.9
to compute the coordinate vector [q]B if q = 1 + x.
Solution:
We have
⟨p0, p1⟩=
" 1
−1
1
√
2
·
√
3x
√
2
dx = 0,
||p0|| =
!
⟨p0, p0⟩=
%" 1
−1
p2
0 dx =
%" 1
−1
1
2 dx =
√
1 = 1,
and
||p1|| =
!
⟨p1, p1⟩=
%" 1
−1
p2
1 dx =
%" 1
−1
3
2 x2 dx =
&
1
2 x3))1
−1 =
√
1 = 1.
Thus, {p0, p1} is an orthonormal (and hence linearly independent) set of vectors in
P1(R). Since dim[P1(R)] = 2, Theorem 4.6.10 shows that {p0, p1} is an (orthonormal)
basis for P1(R).

5.2
Orthogonal Sets of Vectors and Orthogonal Projections 357
Finally, we wish to write q = 1 + x as a linear combination of p0 and p1, by using
Corollary 5.2.9. We leave it to the reader to verify that ⟨q, p0⟩=
√
2 and ⟨q, p1⟩=
2
2
3.
Thus, we have
1 + x =
√
2 p0 +
&
2
3 p1 =
√
2 · 1
√
2
+
&
2
3 ·
'&
3
2 x
(
.
So the component vector of 1 + x relative to B is
⎡
⎣
√
2
2
2
3
⎤
⎦.
□
Orthogonal Projections
We conclude the present section by developing an important tool that will be used in the
next section to produce orthogonal and orthonormal bases for inner product spaces. This
tool is orthogonal projection, and it represents another beautiful geometric application
of inner products. We can already present this application in the inner product space R3
(with standard inner product) by posing the following problem:
Problem 5.2.12
Given a point P(x0, y0, z0) in R3 and a line L in R3, what is the distance from P to L?
The line L can be described parametrically through the use of a point Q(x1, y1, z1)
on L and a parallel vector v = (a, b, c) to L as follows:
x(t) = (x1 + at, y1 + bt, z1 + ct),
where t ∈R. By deﬁnition, the distance from P to L is deﬁned as the distance from P
to the point on the line L that is closest to P. Determining this point becomes crucial,
since in Problem 5.2.12, it is not supplied. Because the problem at hand involves R3, we
can visualize what is going on by using a picture in R3, but the formulas we will derive
will be valid and applicable in any inner product space.
The idea is to use the points P and Q described above to form the vector w from Q
to P via vector subtraction:
w = (x0 −x1, y0 −y1, z0 −z1).
Unless the point P actually lies on L (in which case, the distance from P to the line
L is trivially zero), the vectors w and v are noncollinear. The situation is pictured in
Figure 5.2.1.
L
W
P(x0, y0, z0)
Q(x1, y1, z1)
v 5 (a, b, c)
Figure 5.2.1: The problem of ﬁnding the distance from a point P(x0, y0, z0) to a line L in R3.

358
CHAPTER 5
Inner Product Spaces
In general, if v and w are two linearly independent (noncollinear) vectors, then the
orthogonal projection of w on v is the vector P(w, v) shown in Figure 5.2.2.
u
z
y
v2
w
w 2 P(w, v)
P(w, v)
v 5 v1
x
Figure 5.2.2: Obtaining an
orthogonal basis for a
two-dimensional subspace of R3.
We note that the distance from the point P to the line L in Problem 5.2.12 is simply
the norm of the vector w −P(w, v), which is ||w −P(w, v)||.
In order to complete the solution to Problem 5.2.12, we must therefore determine a
formula for P(w, v). Our expression will be derived in terms of the inner product, and it
will be generalizable to an arbitrary inner product space.
We see from Figure 5.2.2 that an orthogonal basis for the subspace (plane) of three-
space spanned by v and w is {v1, v2}, where
v1 = v
and
v2 = w −P(w, v).
Note that the norm of P(w, v) is
||P(w, v)|| = ||w|| cos θ,
where θ is the angle between v and w. Thus
P(w, v) = ||w|| cos θ v
||v||,
which we can write as
P(w, v) =
'||w|| ||v||
||v||2
cos θ
(
v.
(5.2.3)
Recalling that the dot product of the vectors w and v is deﬁned by
w · v = ||w|| ||v|| cos θ,
it follows from Equation (5.2.3) that
P(w, v) = (w · v)
||v||2 v,
orequivalently,usingthenotationfortheinnerproductintroducedintheprevioussection,
P(w, v) = ⟨w, v⟩
||v||2 v.
(5.2.4)
Armed with (5.2.4), we can ﬁnd the distance from a point P to a line L in R3. Let
us work out an explicit example.
Example 5.2.13
Find the distance from the point P(9, 0, −4) to the line L with parametric vector equation
x(t) = (−3t, 1 + 6t, −1).
Solution:
By setting t = 0, note that the point Q(0, 1, −1) lies on the line L, and
note that L has parallel vector v = (−3, 6, 0). We form the vector from Q to P, which
is w = (9, −1, −3). Moreover, according to Equation (5.2.4), we have
P(w, v) = −33
45(−3, 6, 0) =
'11
5 , −22
5 , 0
(
.
Thus, the distance from P to L is given by
||w −P(w, v)|| = ||
'34
5 , 17
5 , −3
(
|| = 1
5||(34, 17, −15)|| =
√
1670
5
≈8.173.
□

5.2
Orthogonal Sets of Vectors and Orthogonal Projections 359
In the exercises, we will ask the reader to consider how to ﬁnd the distance from a
point P(x0, y0) in R2 to a line L with equation y = mx + b, and we will also consider
how to ﬁnd the distance from a point P(x0, y0, z0) in R3 to a speciﬁed plane. (See
Problems 21 and 28.) In both cases, modiﬁcations of the procedure we have described
here for the distance from a point to a line in R3 can be utilized.
We can see geometrically in Figure 5.2.2 that the vector w −P(w, v) is orthogonal
to v. In the next section, we will prove that this is the case in any inner product space. In
fact, we will see that a stronger result is true: If {v1, v2, . . . , vk} is an orthogonal set of
vectors in an inner product space V and w ∈V , then the vector
w −P(w, v1) −P(w, v2) −· · · −P(w, vk)
is orthogonal to the vector vi for each i.
Exercises for 5.2
Key Terms
Orthogonal vectors, Orthogonal set, Unit vector, Orthonor-
mal vectors, Orthonormal sets, Normalization, Orthogonal
basis, Orthonormal basis, Orthogonal projection.
Skills
• Be able to determine whether a given set of vectors are
orthogonal or orthonormal.
• Be able to determine whether a given set of vectors
forms an orthogonal or orthonormal basis for an inner
product space.
• Be able to replace an orthogonal set with an orthonor-
mal set via normalization.
• Be able to readily compute the components of a vector
v in an inner product space V relative to an orthogonal
(or orthonormal) basis for V .
• Be able to compute the orthogonal projection of one
vector w along another vector v: P(w, v).
• Be able to use orthogonal projection to compute the
distance from a point to a line in R2 or in R3.
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every orthonormal basis for an inner product space V
is also an orthogonal basis for V .
(b) Every linearly independent set of vectors in an inner
product space V is orthogonal.
(c) With the inner product ⟨f, g⟩=
! π
0 f (t)g(t)dt, the
functions f (x) = cos x and g(x) = sin x are an or-
thogonal basis for span{cos x, sin x}.
(d) If v and w are nonzero vectors in an inner product
space V , then the vector w −P(w, v) is orthogonal
to w.
(e) In expressing the vector v as a linear combination of
theorthogonalbasis{v1, v2, . . . , vn}foraninnerprod-
uct space V , the coefﬁcient of vi is
ci = ⟨v, vi⟩
||vi||2 .
(f) If u and v are orthogonal vectors and w is any vector,
then
P(P(w, v), u) = 0.
(g) If w1, w2, and v are vectors in an inner product space
V , then
P(w1 + w2, v) = P(w1, v) + P(w2, v).
Problems
For Problems 1–5, determine whether the given set of vec-
tors is an orthogonal set in Rn. For those that are, determine
a corresponding orthonormal set of vectors.
1. {(1, 2), (−4, 2)}.
2. {(2, −1, 1), (1, 1, −1), (0, 1, 1)}.
3. {(1, 3, −1, 1), (−1, 1, 1, −1), (1, 0, 2, 1)}

360
CHAPTER 5
Inner Product Spaces
4. {(1, 2, −1, 0), (1, 0, 1, 2), (−1, 1, 1, 0),
(1, −1, −1, 0)}.
5. {(1, 2, −1, 0, 3), (1, 1, 0, 2, −1), (4, 2, −4, −5, −4)}
6. Let v = (7, −2). Determine all nonzero vectors w in
R2 such that {v, w} is an orthogonal set.
7. Let v = (−3, 6, 1). Determine all vectors in R3 that
are orthogonal to v. Use this to ﬁnd an orthogonal basis
for R3 that includes the vector v.
8. Let v1 = (1, 2, 3), v2 = (1, 1, −1). Determine all
nonzero vectors w in R3 such that {v1, v2, w} is an
orthogonal set. Hence obtain an orthonormal set of
vectors in R3.
9. Let v1 = (−4, 0, 0, 1), v2 = (1, 2, 3, 4). Determine
all vectors in R4 that are orthogonal to both v1 and
v2. Use this to ﬁnd an orthogonal basis for R4 that
includes the vectors v1 and v2.
For Problems 10–12, show that the given set of vectors is an
orthogonal set in Cn, and hence obtain an orthonormal set of
vectors in Cn in each case.
10. {(2 + i, −5i), (−2 −i, −i)}.
11. {(1 −i, 3 + 2i), (2 + 3i, 1 −i)}.
12. {(1 −i, 1 + i, i), (0, i, 1 −i), (−3 + 3i, 2 + 2i, 2i)}.
13. Consider the vectors v = (1−i, 1+2i), w = (2+i, z)
in C2. Determine the complex number z such that
{v, w} is an orthogonal set of vectors, and hence obtain
an orthonormal set of vectors in C2.
For Problems 14–16, show that the given functions in
C0[−1, 1] are orthogonal, and use them to construct an or-
thonormal set of functions in C0[−1, 1].
14. f1(x) = 1, f2(x) = sin πx, f3(x) = cos πx.
15. f1(x) = 1, f2(x) = x, f3(x) = 1
2(3x2 −1).
[Note: These are the Legendre polynomials that arise
as solutions of the Legendre differential equation
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0,
when n = 0, 1, 2, respectively.]
16. f1(x) = 2x, f2(x) = 1 + 2x2, f3(x) = x3 −3
5x.
For Problems 17–18, show that the given functions are or-
thonormal on [−1, 1].
17. f1(x) = cos πx, f2(x) = cos 2πx,
f3(x) = cos 3πx.
18. f1(x) = sin πx, f2(x) = sin 2πx, f3(x) = sin 3πx.
[Hint: The trigonometric identity
sin a sin b = 1
2[cos(a + b) −cos(a −b)]
will be useful.]
19. Let p(x) = 2 −x −x2 and q(x) = 1 + x + x2. Using
the inner product
⟨a0 + a1x + a2x2, b0 + b1x + b2x2⟩
= a0b0 + a1b1 + a2b2,
ﬁnd all polynomials r(x) = a + bx + cx2 in P2(R)
such that {p(x), q(x),r(x)} is an orthogonal set.
20. Let A1 =
#
1 1
−1 2
$
, A2 =
# −1 1
2 1
$
, and A3 =
# −1 −3
0
2
$
. Use the inner product
⟨A, B⟩= a11b11 + a12b12 + a21b21 + a22b22
to ﬁnd all matrices A4
=
# a
b
c
d
$
such that
{A1, A2, A3, A4} is an orthogonal set of matrices in
M2(R).
21. Consider the problem of ﬁnding the distance from
a point P(x0, y0) in R2 to a line L with equation
y = mx + b.
(a) Show that v = ⟨1, m⟩is a parallel to the line L.
(b) Using part (a) and following the approach used to
solve Problem 5.2.12 in the text, derive a formula
for the distance from P to L.
For Problems 22–27, ﬁnd the distance from the given point
P to the given line L.
22. P(−8, 0); Line L with equation y = 3x −4.
23. P(1, −1); Line L with equation 4x + 5y = 1.
24. P(−6, 4); Line L with equation x −y = 3.
25. P(4, 1, −1); Line L with equation
x(t) = (2t, −4 −t, 3t).
26. P(9, 0, 0); Line L with equation
x(t) = (4 + 3t, 6, −t).

5.2
Orthogonal Sets of Vectors and Orthogonal Projections 361
27. P(1, 2, 3, 2, 1); Line L with equation
x(t) = (−3, −2t, 1 + 2t, −t, 2 + 5t).
28. In this problem, we use the ideas of this section
to derive a formula for the distance from a point
P(x0, y0, z0) in R3 to a plane P with equation ax +
by + cz + d = 0. Unlike the situation of the distance
from a point to a line in which we performed orthog-
onal projection onto a vector pointing along the line
followed by vector subtraction, here we can project di-
rectly onto a normal vector n = (a, b, c) for the plane
without the need for vector subtraction.
(a) Draw a picture of this situation, including the
point P, the plane P, and the normal vector n
to the plane.
(b) Choosing a point Q(x1, y1, z1) on the plane, con-
struct the vector w from Q to P and show geo-
metrically that the distance from P(x0, y0, z0) to
the plane is ||P(w, n)||.
(c) Find a formula for ||P(w, n)|| in terms of
a, b, c, d, x0, y0, and z0.
For Problems 29–32, use the result of Problem 28 to ﬁnd the
distance from the given point P to the given plane P.
29. P(−4, 7, −2); Plane P with equation
x + 2y −4z −2 = 0.
30. P(0, −1, 3); Plane P with equation 3x −y −z = 5.
31. P(−1, 1, −1); Plane P with equation z = 2x.
32. P(8, 8, −1); Plane P with equation y = 4z + 2.
33. Let {u1, u2, v} be linearly independent vectors in an
inner product space V , and suppose that u1 and u2 are
orthogonal. Deﬁne the vector u3 in V by
u3 = v + λu1 + µu2,
where λ, µ are scalars. Derive the values of λ and µ
such that {u1, u2, u3} is an orthogonal basis for the
subspace of V spanned by {u1, u2, v}.
34. Prove that if {v1, v2, . . . , vk} is an orthogonal set of
vectorsinaninnerproductspace V andifui =
1
||vi||vi
for each i, then {u1, u2, . . . , uk} form an orthonormal
set of vectors.
35. The subject of Fourier series is concerned with the
representation of a 2π-periodic function f as the
following inﬁnite linear combination of the set of
functions
{1, sin nx, cos nx}∞
n=1 :
f (x) = 1
2a0 +
∞
3
n=1
(an cos nx + bn sin nx). (5.2.5)
In this problem, we investigate the possibility of per-
forming such a representation.
(a) Use appropriate trigonometric identities, or some
form of technology, to verify that the set of func-
tions
{1, sin nx, cos nx}∞
n=1
is orthogonal on the interval [−π, π].
(b) By multiplying (5.2.5) by cos mx and integrating
over the interval [−π, π], show that
a0 = 1
π
" π
−π
f (x) dx
and
am = 1
π
" π
−π
f (x) cos mx dx.
[Hint: You may assume that interchange of the
inﬁnite summation with the integral is permissi-
ble.]
(c) Use a similar procedure to show that
bm = 1
π
" π
−π
f (x) sin mx dx.
It can be shown that if f is in C1(−π, π), then
Equation (5.2.5) holds for each x ∈(−π, π). The
series appearing on the right-hand side of (5.2.5)
is called the Fourier series of f , and the con-
stants in the summation are called the Fourier
coefﬁcients for f .
(d) Show that the Fourier coefﬁcients for the function
f (x) = x, −π < x ≤π, f (x + 2π) = f (x),
are
an = 0,
n = 0, 1, 2, . . .
bn = −2
n cos nπ,
n = 1, 2, . . .
and thereby determine the Fourier series of f .
(e) ⋄Using some form of technology, sketch the
approximations to f (x) = x on the interval
(−π, π) obtained by considering the ﬁrst three
terms, ﬁrst ﬁve terms, and ﬁrst ten terms in the
Fourier series for f . What do you conclude?

362
CHAPTER 5
Inner Product Spaces
5.3
The Gram-Schmidt Process
In this section, we return to address the second question we raised in the last section:
How can we obtain an orthogonal or orthonormal basis for an inner product space V ?
The idea behind the process is to begin with any basis for V , say {x1, x2, . . . , xn}, and
to successively replace these vectors with vectors v1, v2, . . . , vn that are orthogonal to
one another, and to ensure that, throughout the process, the space spanned by the vectors
remains unchanged. This process is known as the Gram-Schmidt process.
Let V be an arbitrary inner product space. We begin by considering just two linear
linearly independent vectors x1 and x2 in V . We can obtain an orthogonal basis {v1, v2}
for the subspace of V spanned by {x1, x2} as follows. Let
v1 = x1
and
v2 = x2 −P(x2, v1) = x2 −⟨x2, v1⟩
||v1||2 v1.
(5.3.1)
Note from (5.3.1) that v2 can be written as a linear combination of {x1, x2}, and
hence, v2 ∈span{x1, x2}. Since we also have that x2 ∈span{v1, v2}, it follows that
span{v1, v2} = span{x1, x2}. Next we claim that v2 is orthogonal to v1. We have
⟨v2, v1⟩=
4
x2 −⟨x2, v1⟩
||v1||2 v1, v1
5
= ⟨x2, v1⟩−
4⟨x2, v1⟩
||v1||2 v1, v1
5
= ⟨x2, v1⟩−⟨x2, v1⟩
||v1||2 ⟨v1, v1⟩= 0,
which veriﬁes our claim. We have shown that {v1, v2} is an orthogonal set of vectors that
spans the same subspace of V as {x1, x2}.
The calculations just presented can be generalized to prove the following useful
result (see Problem 25).
Lemma 5.3.1
Let {v1, v2, . . . , vk} be an orthogonal set of vectors in an inner product space V . If x ∈V ,
then the vector
x −P(x, v1) −P(x, v2) −· · · −P(x, vk)
(5.3.2)
is orthogonal to vi for each i.
Equation (5.3.2) is really saying that if we take a vector x in an inner product space
and subtract off its orthogonal projections along a series of other mutually orthogonal
vectors, we are left with a vector that is orthogonal to all of the vectors along whose
directions we have projected. This result is centrally important to the Gram-Schmidt
Process.
Now suppose we are given a linearly independent set of vectors {x1, x2, . . . , xm} in
an inner product space V . Using Lemma 5.3.1, we can construct an orthogonal basis for
the subspace of V spanned by these vectors. We begin with the vector v1 = x1 as above,
and we deﬁne vi by subtracting off appropriate projections of xi on v1, v2, . . . , vi−1.
The resulting procedure is called the Gram-Schmidt orthogonalization procedure.
The formal statement of the result is as follows.

5.3
The Gram-Schmidt Process 363
Theorem 5.3.2
(Gram-Schmidt Process)
Let {x1, x2, . . . , xm} be a linearly independent set of vectors in an inner product space V .
Then an orthogonal basis for the subspace of V spanned by these vectors is
{v1, v2, . . . , vm}, where
v1 = x1
v2 = x2 −⟨x2, v1⟩
||v1||2 v1
v3 = x3 −⟨x3, v1⟩
||v1||2 v1 −⟨x3, v2⟩
||v2||2 v2
...
vi = xi −
i−1
3
k=1
⟨xi, vk⟩
||vk||2 vk
...
vm = xm −
m−1
3
k=1
⟨xm, vk⟩
||vk||2 vk.
Proof Lemma 5.3.1 shows that {v1, v2, . . . , vm} is an orthogonal set of vectors. Thus,
both {v1, v2, . . . , vm} and {x1, x2, . . . , xm} are linearly independent sets, and hence
span{v1, v2, . . . , vm}
and
span{x1, x2, . . . , xm}
are m-dimensional subspaces of V . (Why?) Moreover, from the formulas given in Theo-
rem5.3.2,weseethateachxi ∈span{v1, v2, . . . , vm},andthereforespan{x1, x2, . . . , xm}
is a subset of span{v1, v2, . . . , vm}. Thus, by Corollary 4.6.14,
span{v1, v2, . . . , vm} = span{x1, x2, . . . , xm}.
We conclude that {v1, v2, . . . , vm} is a basis for the subspace of V spanned by
x1, x2, . . . , xm.
Example 5.3.3
Obtain an orthogonal basis for the subspace of R4 spanned by
x1 = (1, 0, 1, 0),
x2 = (1, 1, 1, 1),
x3 = (−1, 2, 0, 1).
Solution:
Following the Gram-Schmidt process, we set v1 = x1 = (1, 0, 1, 0). Next,
we have
v2 = x2 −⟨x2, v1⟩
||v1||2 v1 = (1, 1, 1, 1) −2
2(1, 0, 1, 0) = (0, 1, 0, 1)
and
v3 = x3 −⟨x3, v1⟩
||v1||2 v1 −⟨x3, v2⟩
||v2||2 v2 = (−1, 2, 0, 1) + 1
2(1, 0, 1, 0) −3
2(0, 1, 0, 1)
=
'
−1
2, 1
2, 1
2, −1
2
(
.

364
CHAPTER 5
Inner Product Spaces
The orthogonal basis so obtained is
,
(1, 0, 1, 0), (0, 1, 0, 1),
'
−1
2, 1
2, 1
2, −1
2
(-
.
□
Of course, once an orthogonal basis {v1, v2, . . . , vm} is obtained for a subspace
of V , we can normalize this basis by setting ui =
vi
||vi|| to obtain an orthonormal
basis {u1, u2, . . . , um}. For instance, an orthonormal basis for the subspace of R4 in the
preceding example is
,' 1
√
2
, 0, 1
√
2
, 0
(
,
'
0, 1
√
2
, 0, 1
√
2
(
,
'
−1
2, 1
2, 1
2, −1
2
(-
.
Example 5.3.4
Determine an orthogonal basis for the subspace of C0[−1, 1] spanned by the functions
f1(x) = x, f2(x) = x3, f3(x) = x5, using the inner product in (5.1.5).
Solution:
In this case, we let {g1, g2, g3} denote the orthogonal basis, and we apply
the Gram-Schmidt process. Thus, g1(x) = x, and
g2(x) = f2(x) −⟨f2, g1⟩
||g1||2 g1(x).
(5.3.3)
We have
⟨f2, g1⟩=
" 1
−1
f2(x)g1(x)dx =
" 1
−1
x4dx = 2
5
and
||g1||2 = ⟨g1, g1⟩=
" 1
−1
x2dx = 2
3.
Substituting into Equation (5.3.3) yields
g2(x) = x3 −3
5x = 1
5x(5x2 −3).
We now compute g3(x). According to the Gram-Schmidt process,
g3(x) = f3(x) −⟨f3, g1⟩
||g1||2 g1(x) −⟨f3, g2⟩
||g2||2 g2(x).
(5.3.4)
We ﬁrst evaluate the required inner products:
⟨f3, g1⟩=
" 1
−1
f3(x)g1(x)dx =
" 1
−1
x6dx = 2
7,
⟨f3, g2⟩=
" 1
−1
f3(x)g2(x)dx = 1
5
" 1
−1
x6(5x2 −3)dx = 1
5
'10
9 −6
7
(
= 16
315,
||g2||2 =
" 1
−1
[g2(x)]2dx = 1
25
" 1
−1
x2(5x2 −3)2dx
= 1
25
" 1
−1
(25x6 −30x4 + 9x2)dx =
8
175.
Substituting into Equation (5.3.4) yields
g3(x) = x5 −3
7x −2
9x(5x2 −3) = 1
63(63x5 −70x3 + 15x).

5.3
The Gram-Schmidt Process 365
Thus, an orthogonal basis for the subspace of C0[−1, 1] spanned by f1, f2, and f3 is
,
x, 1
5x(5x2 −3), 1
63x(63x4 −70x2 + 15)
-
.
□
Exercises for 5.3
Key Terms
Gram-Schmidt process.
Skills
• Be able to carry out the Gram-Schmidt process to re-
placeabasisfor V withanorthogonal(ororthonormal)
basis for V .
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If {x1, x2, . . . , xn} is already an orthogonal basis for
an inner product space V , then applying the Gram-
Schmidt process to this set results in this same set of
vectors.
(b) If x1 and x2 are orthogonal, then applying the Gram-
Schmidt Process to the set {x1, x1 + x2} will result in
the orthogonal set {x1, x2}.
(c) The Gram-Schmidt process applied to the vectors
{x1, x2, x3} yields the same basis as the Gram-Schmidt
process applied to the vectors {x3, x2, x1}.
(d) The Gram-Schmidt process can only be applied to a
linearly independent set of vectors.
(e) If B1 = {x1, x2} and B2 = {y1, y2} are two bases
for an inner product V such that the Gram-Schmidt
process applied to each of them results in the same
orthogonal basis {v1, v2} for V , then B1 = B2.
(f) If the Gram-Schmidt Process applied to B1 = {x1, x2}
yields the orthogonal set {v1, v2}, then applying the
Gram-Schmidt Process to B2 = {2x1, 2x2} will yield
the orthogonal set {2v1, 2v2}.
Problems
For Problems 1–10, use the Gram-Schmidt process to deter-
mine an orthonormal basis for the subspace of Rn spanned
by the given set of vectors.
1. {(1, 2, 3), (6, −3, 0)}.
2. {(1, −1, −1), (2, 1, −1)}.
3. {(2, 1, −2), (1, 3, −1)}.
4. {(1, −5, −3), (0, −1, 3), (−6, 0, −2)}.
5. {(2, 0, 1), (−3, 1, 1), (1, −3, 8)}.
6. {(−1, 1, 1, 1), (1, 2, 1, 2)}.
7. {(1, 0, −1, 0), (1, 1, −1, 0), (−1, 1, 0, 1)}
8. {(1, 2, 0, 1), (2, 1, 1, 0), (1, 0, 2, 1)}.
9. {(1, 1, −1, 0), (−1, 0, 1, 1), (2, −1, 2, 1)}.
10. {(1, 2, 3, 4, 5), (−7, 0, 1, −2, 0)}.
For Problems 11–14, determine orthogonal bases for
rowspace(A) and colspace(A).
11. A =
# 1 −3
2 0 −1
4 −9 −1 1
2
$
.
12. A =
⎡
⎢⎢⎢⎢⎣
1 5
2 4
3 3
4 2
5 1
⎤
⎥⎥⎥⎥⎦
.
13. A =
⎡
⎣
3
1 4
1 −2 1
1
5 2
⎤
⎦.
14. A =
⎡
⎣
1 −4
7
−2
6 −8
−1
0
5
⎤
⎦.
For Problems 15–16, determine an orthonormal basis for the
subspace of C3 spanned by the given set of vectors. Make
sure that you use the appropriate inner product in C3.
15. {(1 + i, i, 2 −i), (1 + 2i, 1 −i, i)}.
16. {(1 −i, 0, i), (1, 1 + i, 0)}.

366
CHAPTER 5
Inner Product Spaces
For Problems 17–20, determine an orthogonal basis for the
subspace of C0[a, b] spanned by the given vectors, for the
given interval [a, b]. Use the inner product given in Equation
(5.1.5).
17. f1(x) = 1+2x, f2(x) = −2−x + x2, a = 0, b = 1.
18. f1(x) = 1, f2(x) = x, f3(x) = x2, a = 0, b = 1.
19. f1(x) = 1, f2(x) = x2, f3(x) = x4, a = −1, b = 1.
20. f1(x) = 1,
f2(x) = sin x,
f3(x) = cos x,
a = −π/2,
b = π/2.
On M2(R) deﬁne the inner product ⟨A, B⟩by
⟨A, B⟩= 5a11b11 + 2a12b12 + 3a21b21 + 5a22b22
for all matrices A = [ai j] and B = [bi j]. For Problems
21–22, use this inner product in the Gram-Schmidt proce-
dure to determine an orthogonal basis for the subspace of
M2(R) spanned by the given matrices.
21. A1 =
# 1 −1
2
1
$
, A2 =
# 2 −3
4
1
$
.
22. A1 =
# 0 1
1 0
$
, A2 =
# 0 1
1 1
$
,
A3 =
# 1 1
1 0
$
. Also identify the subspace of M2(R)
spanned by {A1, A2, A3}.
On Pn(R), deﬁne the inner product ⟨p1, p2⟩by
⟨p1, p2⟩= a0b0 + a1b1 + · · · + anbn,
for all polynomials
p1(x) = a0 + a1x + · · · + anxn,
p2(x) = b0 + b1x + · · · + bnxn.
For Problems 23–24, use this inner product to determine an
orthogonal basis for the subspace of Pn(R) spanned by the
given polynomials.
23. p1(x) = 1 −2x + 2x2, p2(x) = 2 −x −x2.
24. p1(x) = 1+x2, p2(x) = 2−x+x3, p3(x) = 2x2−x.
25. Prove Lemma 5.3.1.
5.4
Least Squares Approximation
In this section, we return to the general linear system of equations that we ﬁrst studied
in Chapter 2 in order to provide an important application of the material we have been
learning in this chapter:
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
(5.4.1)
am1x1 + am2x2 + · · · + amnxn = bm,
where the system coefﬁcients ai j and the system constants b j are given scalars and
x1, x2, . . . , xn denote the unknowns in the system. As usual, we abbreviate this system
in vector form as Ax = b. We saw in Chapter 2 that systems such as this have either no
solutions, one unique solution, or inﬁnitely many solutions, for the vector x of unknowns.
In earlier chapters, we were primarily interested in systems (5.4.1) that are consistent
and what we can say about the set of solutions. By contrast, in this section we will focus
on the situation in which (5.4.1) has no solutions. This is not an uncommon occurrence
in many real-world problems that relate two or more quantities. Here is an example.
Example 5.4.1
In economics, the cost incurred by a company that is manufacturing a product is generally
expected to be an increasing function of the quantity produced, and is expected to depend
on both ﬁxed costs and variable costs of production. As a concrete example, assume that
a company produces x thousands of pillows in a particular month at a cost of C(x)
dollars. Suppose the following data are recorded by the company for the ﬁrst six months
of the year:

5.4
Least Squares Approximation 367
x Thousand
Cost C(x) of Production
Month
Pillows Produced
(thousands of dollars)
January
5
11.5
February
8
13.5
March
10
15
April
7
13
May
9
14
June
3
6
If we assume that the cost C(x) is a linear function of x, say
C(x) = ax + b,
(5.4.2)
we can seek to determine the values of a and b. In this notation, the quantity a repre-
sents the variable cost (per thousand pillows produced) and b represents the monthly
ﬁxed cost.
□
The data points associated with the data in Example 5.4.1 can be substituted into
(5.4.2) to obtain the following equations relating a and b:
Data Point
Equation Relating a and b
(5, 11.5)
5a + b = 11.5
(8, 13.5)
8a + b = 13.5
(10, 15)
10a + b = 15
(7, 13)
7a + b = 13
(9, 14)
9a + b = 14
(3, 6)
3a + b = 6
Inspection of the list of equations appearing in the right column quickly reveals that
there is no single choice of a and b that satisﬁes them all. Equivalently, there is no single
line that passes through all of the data points. The data is plotted in Figure 5.4.1.
3
4
14
12
10
8
6
5
6
7
8
9
10
C(x)
x
Figure 5.4.1: Data showing cost C(x) of production (in thousands of dollars) of x thousand
pillows.

368
CHAPTER 5
Inner Product Spaces
This list of equations constitutes an inconsistent linear system in the unknowns a
and b. We can express this linear system in the form Ax = b, where
A =
⎡
⎢⎢⎢⎢⎢⎢⎣
5
1
8
1
10 1
7
1
9
1
3
1
⎤
⎥⎥⎥⎥⎥⎥⎦
,
x =
# a
b
$
,
and
b =
⎡
⎢⎢⎢⎢⎢⎢⎣
11.5
13.5
15
13
14
6
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(5.4.3)
Since there is no single line that ﬁts all of the data points in this example, we settle
for asking:
Question: What line “best ﬁts” the given set of data points?
We will later answer this question for the data given in Example 5.4.1.
The line referred to in the question above is called a least squares line or least
squares approximation for the data points. It has countless applications throughout
a variety of disciplines. As a result, it has garnered a lot of attention and has led to
numerous variations and generalizations. We will touch on this brieﬂy at the end of this
section. Our question above raises an instance of the general problem of least squares:
Problem of Least Squares: Let A be an m × n matrix, and let b be a ﬁxed vector in
Rm. If Ax = b is a linear system of m equations in n unknowns, seek to minimize the
quantity ϵ(x) = ||Ax −b|| by an appropriate choice of x, say x0. The vector x0 is called
a least squares solution to the system Ax = b.
In other words, a least squares solution x0 to the linear system Ax = b is any vector
x0 in Rn such that ϵ(x0) ≤ϵ(x) for all x in Rn. If we write
Ax −b =
⎡
⎢⎢⎢⎣
e1
e2
...
em
⎤
⎥⎥⎥⎦,
then note that ϵ(x) =
2
e2
1 + e2
2 + · · · + e2m, from which the name of this topic, “least
squares,” becomes apparent. A least squares solution to Ax = b is one for which this
sum of squares is minimized.
Thereadermaywellwonderiftheleastsquaressolutionx0 introducedintheproblem
statement above is uniquely determined for every linear system Ax = b. For instance, if
Ax = b is actually a consistent linear system, then we know that the system has either
one solution, or it has inﬁnitely many solutions. Clearly, any such solution x0 in this case
will have ϵ(x0) = 0, so in this case the uniqueness of the least squares solution rests on
whether the system Ax = b has a unique solution or not. We will further address the
issue of uniqueness of least squares solutions as we proceed further through this section.
Derivation of the Least Squares Solution
Let A be an m × n matrix, x a vector of unknowns in Rn, and b a vector given in Rm.
The approach we take5 to ﬁnding a least squares solution x0 to Ax = b begins with the
5It is also possible to obtain the least squares solution by using optimization techniques from multivariable
calculus, but the goal here is to illustrate the use of the geometry in inner product spaces.

5.4
Least Squares Approximation 369
geometric observation that a vector x0 in Rn will be a least squares solution to Ax = b
if and only if b −Ax0 is orthogonal to every vector in the subspace
colspace(A) = {Ax : x ∈Rn}
of Rm. (We can simply say that b −Ax0 is orthogonal to the whole subspace.) This is
visualized in Figure 5.4.2.
colspace(A)
Ax0
Ax2
Ax1
ε(x0)
ε(x1)
ε(x2)
b
colspace(A)
Ax0
Ax1
Ax2
b
ε(x1)
ε(x2)
minimum error
ε(x0) 
Figure 5.4.2: The least squares solution x0 to the inconsistent linear system Ax = b is
obtained when b −Ax0 is orthogonal to every vector in colspace(A).
That is, for every x ∈Rn, we must have
⟨Ax, b −Ax0⟩= 0.
(5.4.4)
Since the inner product of vectors u and v in Rn can be written as a matrix multiplication,
⟨u, v⟩= uT v,
we can rewrite (5.4.4) as
(Ax)T (b −Ax0) = 0,
or, using Theorem 2.2.23,
xT AT (b −Ax0) = 0.
Since this equation must hold for all x in Rn, it follows that
AT (b −Ax0) = 0.
That is,
AT Ax0 = AT b.
(5.4.5)
The equations arising in the system (5.4.5) are known as normal equations. Conversely,
beginning with a vector x0 in Rn that satisﬁes Equation (5.4.5), tracing through the
reversible algebraic steps above easily proves that x0 is a least squares solution to Ax = b.
Therefore, we have proven:
Theorem 5.4.2
Consider the linear system of equations Ax = b, where A is a ﬁxed m × n matrix and b
is a ﬁxed column vector in Rm. Then x0 in Rn is a least squares solution to this system
if and only if
AT Ax0 = AT b.

370
CHAPTER 5
Inner Product Spaces
According to Theorem 5.4.2, we can determine all least squares solutions for Ax = b
by ﬁnding all solutions x to the n × n linear system AT Ax = AT b. Provided that A
has linearly independent columns, the matrix AT A is invertible (see Problem 33 in
Section 4.11), and in this case, we can solve Equation (5.4.5) uniquely for the least
squares solution:
x0 = (AT A)−1AT b.
(5.4.6)
Thus,
Ax0 = A(AT A)−1AT b
is the point in the column space of A that is closest to b, and we call it the projection of
b onto the column space of A, and we write
Ax0 = A(AT A)−1 AT b = Pb,
where
P = A(AT A)−1AT
(5.4.7)
is called a projection matrix.
Example 5.4.3
If A does not have linearly independent columns, then the least squares solution x0
will not be uniquely determined. For example, consider the linear system Ax = b,
where A =
! 1 1
1 1
"
and b =
! 0
1
"
. This is the inconsistent system whose equations
are x + y = 0 and x + y = 1. In this case, Equation (5.4.5) reads
! 2 2
2 2
"
x0 =
! 1
1
"
.
One quickly ﬁnds that there are inﬁnitely many solutions for x0 =
! x
y
"
consisting of
all points on the line x + y = 1
2.
□
Example 5.4.4
Consider the system of equations 3x −5y = 0, −2x + 4y = −2, and x −3y = 1. Find
all least squares solutions to this linear system.
Solution:
We have
A =
⎡
⎣
3 −5
−2
4
1 −3
⎤
⎦,
x =
! x
y
"
,
and
b =
⎡
⎣
0
−2
1
⎤
⎦.
In this case, the columns of A are linearly independent, so we know that AT A is invertible.
In fact,
AT A =
!
14 −26
−26
50
"
,
so that (AT A)−1 = 1
24
! 50 26
26 14
"
.
Hence, according the Equation (5.4.6), we have
x0 = 1
24
! 50 26
26 14
" !
3 −2
1
−5
4 −3
" ⎡
⎣
0
−2
1
⎤
⎦=
'
−3
2
−1
(
.
Although the point x0 = (−3
2, −1) does not solve any of the three equations in the
original system, it is sufﬁciently close to each of the three lines that the value of ϵ(x0) is
as small as possible.
□

5.4
Least Squares Approximation 371
Let us now return to Example 5.4.1 and compute the least squares line. We have
already computed the matrix A and vector b in (5.4.3) above. A quick computation
shows that
AT A =
! 328 42
42
6
"
,
and thus,
(AT A)−1 =
1
102
!
3 −21
−21
164
"
.
Continuing with the computation, we now ﬁnd
x0 = (AT A)−1AT b =
1
102
!
3 −21
−21
164
" ! 5 8 10 7 9 3
1 1
1
1 1 1
"
⎡
⎢⎢⎢⎢⎢⎢⎣
11.5
13.5
15
13
14
6
⎤
⎥⎥⎥⎥⎥⎥⎦
≈
! 1.162
4.034
"
.
Therefore, the least squares line that “best ﬁts” the data points in Example 5.4.1 is
C(x) = 1.162x + 4.034.
The monthly ﬁxed cost for the company is approximately $4,034, and the variable cost
per 1000 pillows manufactured is $1,162.
In general, to ﬁnd the least squares line y = ax + b for a set of data points (x1, y1),
(x2, y2), . . . , (xm, ym), we form
A =
⎡
⎢⎢⎢⎣
x1
1
x2
1
...
...
xm
1
⎤
⎥⎥⎥⎦,
x =
! a
b
"
,
and
b =
⎡
⎢⎢⎢⎣
y1
y2
...
ym
⎤
⎥⎥⎥⎦
and compute the least squares solution to the linear system Ax = b. The reader can
practice this method on several exercises at the end of the section.
Example 5.4.5
Recall from Section 1.1 that a spring-mass system in which frictional and external forces
are ignored behaves according to Hooke’s Law, which states that the restoring force (or
spring force) Fs on the spring is linearly proportional and opposite in direction to its
displacement y from equilibrium:
Fs = −ky,
for some positive spring constant k. Suppose we have a spring with unknown spring
constant that we want to estimate. We can stretch the spring by various amounts y and
measure the restoring force felt by the spring in each case. Experimental errors can
result in different values of k being estimated with each measurement, and so we will
use a least squares approximation of the spring constant. The table below collects some
sample data.
Displacement y
Restoring Force Fs
(in inches)
(in pounds)
0
0
1
−2
2
−3
3
−6
5
−10
7
−13

372
CHAPTER 5
Inner Product Spaces
These data points, (0, 0), (1, −2), (2, −3), (3, −6), (5, −10), (7, −13), do not
satisfy a linear relationship. Unlike Example 5.4.1, in this case Hooke’s Law predicts
that the linear relationship y = ax + b should be expected to yield b = 0. Therefore,
we can seek directly a least squares line of the form y = ax. In this notation the spring
constant k in Hooke’s Law is given by k = −a. Given our data, the following equations
involving a would have to simultaneously hold:
Data Point
Equation Involving a
(0, 0)
0 = 0
(1, −2)
−2 = a
(2, −3)
−3 = 2a
(3, −6)
−6 = 3a
(5, −10)
−10 = 5a
(7, −13)
−13 = 7a
The list of equations in the right column of the table above leads to the linear system
Ax = b, where
A =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
1
2
3
5
7
⎤
⎥⎥⎥⎥⎥⎥⎦
,
x =
' a (
,
and
b =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
−2
−3
−6
−10
−13
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(5.4.8)
Using (5.4.6), we have
x0 = (AT A)−1AT b = 1
88
' 0 1 2 3 5 7 (
⎡
⎢⎢⎢⎢⎢⎢⎣
0
−2
−3
−6
−10
−13
⎤
⎥⎥⎥⎥⎥⎥⎦
= 1
88(−167) ≈−1.898.
Therefore, the spring constant k for the spring with the data above can be estimated via
the method of least squares as k = −a ≈1.898 pounds per inch.
□
Linear Least Squares for Nonlinear Models
The linear approximation achieved by the least squares method described above is nat-
ural in problems where a linear relationship is expected. However, problems involving
quantities where a linear relationship is not expected might be better served with an al-
ternative to the least squares line. For general nonlinear models, the linear least squares
is not applicable, but in the remainder of this section we will study two special cases
where we can use linear least squares for nonlinear problems. For example, if an object
is tossed through the air subject to the force of gravity (see Section 1.1), we would expect
a plot of the object’s position as a function of time to take on a parabolic nature. In an
actual experiment, this parabolic motion could be disturbed by inﬂuences such as air
resistance, but still, we would be most interested in ﬁnding a “best ﬁt” parabola to a
set of observed data points along the trajectory of the object’s path. Fortunately, very
little of the theory discussed above for a least squares line needs to be modiﬁed in this

5.4
Least Squares Approximation 373
case, because the problem of ﬁnding the coefﬁcients of the parabola is linear. Writing
the equation of the proposed parabola in the form
y = ax2 + bx + c,
(5.4.9)
we can substitute the data points obtained from observing positions yi of the object’s
motion at times xi in an experiment, say (x1, y1), (x2, y2), . . . , (xm, ym), into Equation
(5.4.9). Once more we form the vector equation Ax = b. In this case,
A =
⎡
⎢⎢⎢⎢⎢⎣
x2
1
x1
1
x2
2
x2
1
...
...
x2
m
xm
1
⎤
⎥⎥⎥⎥⎥⎦
, x =
⎡
⎣
a
b
c
⎤
⎦,
and b =
⎡
⎢⎢⎢⎣
y1
y2
...
ym
⎤
⎥⎥⎥⎦.
We obtain exactly the same least squares solution as we did above, so that x0 can be
found via Equation (5.4.6). We will provide some examples of least squares parabolic
approximations in the exercises.
Finally in this section, let us return to the Malthusian model for the growth of a
bacteria population that was discussed in Section 1.5. We saw in Equation (1.5.1) that
the theory predicts that the population will grow as an exponential function of time:
P(t) = Cekt, where C and k are constants. If a scientist measures the size of the
population at a variety of times, it is unlikely that a single pair of constants C and k
will accurately match the data points. In this case, we would like to ﬁnd the “best ﬁt”
exponential function. To proceed, we take the natural logarithm of each side of the
exponential function P(t) to obtain a linear equation:
ln P(t) = kt + ln C.
(5.4.10)
Setting y(t) = ln P(t) and b = ln C, we have y(t) = kt + b, and we can once
more use the data points (ti, y(ti)) to form the linear system of equations Ax = y, where
x =
# k
b
$
. Once the least squares solution x0 =
# k0
b0
$
is obtained, we can set C0 = eb0 to
obtain the best ﬁt exponential function P(t) = C0ek0t. Let us illustrate with an example.
Example 5.4.6
Suppose that the size of a bacteria culture (measured in thousands of bacteria) is measured
at various time intervals (measured in hours) and yields the following data:
time t (in hours)
0
1
2
4
5
number of bacteria P(t) (in thousands)
1
1.5
2
3.5
5
Determine an exponential function of the form P(t) = Cekt, where t is measured in
hours and P(t) is the number of bacteria (measured in thousands).
Solution:
Using the notation y(t) = ln P(t), we compute the following table from
the given data (all values are approximated to three decimal places):
t
0
1
2
4
5
y(t)
0
.405
.693
1.253
1.609

374
CHAPTER 5
Inner Product Spaces
We can now formulate this least squares problem via the vector equation Ax = y, where
A =
⎡
⎢⎢⎢⎢⎣
0 1
1 1
2 1
4 1
5 1
⎤
⎥⎥⎥⎥⎦
, x =
' k
b
(
,
and y =
⎡
⎢⎢⎢⎢⎣
0
.405
.693
1.253
1.609
⎤
⎥⎥⎥⎥⎦
.
We use (5.4.6) to ﬁnd
x0 = (AT A)−1 AT y =
' 0.311
0.046
(
.
Thus, k0 = 0.311 and b0 = 0.046. Hence, C0 = e0.046 ≈1.047. Therefore, our best ﬁt
exponentialfunctiondescribingthegrowthofthisbacteriacultureis P(t) = 1.047e0.311t.
This equation predicts, for instance, that after ﬁve hours, the size of the bacteria cul-
ture should be P(5) = 4.960 thousands of bacteria, within 1 percent of the observed
5000 bacteria.
□
1.5
1.0
0.5
0.0
5
4
3
2
1
0
ln(P(t))
t
Figure 5.4.3: A plot of the data points ln P(t) as a function of t in Example 5.4.6.
Exercises for 5.4
Key Terms
Least squares approximation, least squares solution, least
squares line, normal equations, projection matrix.
Skills
• Be able to compute a least squares line for a collection
of data points in the xy-plane.
• Be able to compute the projection matrix associated
with a given linear system Ax = y.
• Understand the least squares approximation technique
and be able to modify it to handle nonlinear models.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If x0 is a least squares solution to the linear system
Ax = b for an m × n matrix, then ||b −Ax0|| ≤
||b −Ax|| for each x in Rn.
(b) The least squares solution x0 to the linear system
Ax = b is given by x0 = (AAT )−1AT b.

5.4
Least Squares Approximation 375
(c) If A is an n×n invertible matrix, then the least squares
solution to the linear system Ax = b is given by
x0 = A−1b.
(d) Every linear system of equations has a unique least
squares solution x0.
(e) For every vector b in Rm, applying the projection ma-
trix P for the linear system Ax = b to the vector b
results in a vector that lies in colspace(A).
(f) If A is an invertible n × n matrix, then the projection
matrix is P = In.
Problems
For Problems 1–7, ﬁnd the equation of the least squares line
associated with the given set of data points.
1. (6, 3), (−2, 0).
2. (1, 10), (2, 20).
3. (1, 10), (2, 20), (3, 10).
4. (2, −2), (1, −3), (0, 0).
5. (2, 5), (0, −1), (5, 3), (1, −3).
6. (0, 3), (1, −1), (2, 6), (4, 6).
7. (−7, 3), (−4, 0), (2, −1), (3, 6), (6, −1).
For Problems 8–9, ﬁnd the equation of the least squares
parabola (sometimes called the quadratic regression equa-
tion) associated with the given set of data points.
8. The data points given in Problem 3.
9. The data points given in Problem 4.
⋄For Problems 10–12, use some form of technology to ﬁnd
the equation of the least squares parabola (sometimes called
the quadratic regression equation) associated with the given
set of data points.
10. The data points given in Problem 5.
11. The data points given in Problem 6.
12. The data points given in Problem 7.
13. A physicist wishes to estimate the spring constant k for
a spring whose restoring force is measured through ex-
perimentation for various displacements of the spring
from equilibrium. The table below gathers the data
collected by this experiment.
Displacement y
Restoring Force Fs
(in inches)
(in pounds)
0
0
3
−10
5
−14
6
−19
Find a least squares solution for k.
14. If the size P(t) of a culture of bacteria (measured
in thousands of bacteria) is measured at various time
intervals (measured in hours) with the data in the
table below, use least squares to estimate P(t) at any
time t:
time t (in hours)
0
4
5
8
10
number of bacteria
3.5
17 25.5
85.5
189.5
P(t) (in thousands)
15. If the size P(t) of a culture of bacteria (measured
in thousands of bacteria) is measured at various time
intervals (measured in minutes) with the data in the
table below, use least squares to estimate P(t) at any
time t:
time t (in minutes)
0
12
20
24
number of bacteria
1.5
5
11
16
P(t) (in thousands)
16. Suppose that a radioactive sample of a radioisotope de-
cays exponentially according to the formula A(t) =
A0ekt where k is a negative constant, A0 is the initial
amount of the sample, t is measured in hours, and A(t)
is measured in grams. Use a least squares technique for
this nonlinear model to estimate a formula for A(t),
given the following measured data:
time t (in hours)
0
1
3
4
6
amount of sample
100
24.5
1.5
0.4
0.1
A(t) (in grams)
17. Recall the projection matrix P = A(AT A)−1AT asso-
ciated with the least squares approximation technique.
Assume that A is an m × n matrix.
(a) What is the size of P?
(b) Show that P A = A and P2 = P.
(c) Show that P is a symmetric matrix.
18. Let A be an n × n invertible matrix. Show that the
unique solution to the linear system Ax = b, namely,
x = A−1b, is also the least squares solution for this
system.

376
CHAPTER 5
Inner Product Spaces
5.5
Chapter Review
Inner Product Spaces
An inner product is a mapping that associates with any two vectors u and v in a vector
space V a scalar that we denote by ⟨u, v⟩. This mapping must satisfy the properties given
in Deﬁnition 5.1.12. The main reason for introducing the idea of an inner product is that
it enables us to extend the familiar idea of orthogonality and length of vectors in R3 to a
general vector space. Thus, u and v are said to be orthogonal in an inner product space
if and only if ⟨u, v⟩= 0.
The Gram-Schmidt Orthonormalization Process
The Gram-Schmidt procedure is a process that takes a linearly independent set of vec-
tors {x1, x2, . . . , xm} in an inner product space V and returns an orthogonal basis
{v1, v2, . . . , vm} for span{x1, x2, . . . , xm}.
Least Squares Approximation
For many linear systems of the form Ax = b, where A and b are given and x is a vector of
unknowns, there is no solution for x. This is particularly true of real-world experimental
problems in which measurement errors can lead to data points that do not obey a linear
relationship, even when theory suggests that there should be such a relationship. The
method of least squares constructs a vector x0 which is as close as possible to being
a solution in the sense that the value of ϵ(x0) = ||Ax0 −b|| is as small as possible.
This vector, which may not be unique, is any solution to the associated linear system
AT Ax = AT b. Least squares is especially important in the ﬁeld of statistics, in which a
collection of data points is found that must be “ﬁt” with a linear (or other) function. A
line that is constructed to “ﬁt” a collection of data points (x1, y1), (x2, y2), . . . , (xm, ym)
is called a least squares line and, if we write this line in the form y = ax + b, then we
can obtain the constants a and b from the formula
x0 =
# a
b
$
= (AT A)−1AT b.
where
A =
⎡
⎢⎢⎢⎣
x1
1
x2
1
...
...
xm
1
⎤
⎥⎥⎥⎦
and b =
⎡
⎢⎢⎢⎣
y1
y2
...
ym
⎤
⎥⎥⎥⎦.
Additional Problems
For Problems 1–2, determine the angle between the
given vectors u and v using the standard inner product
on Rn.
1. u = (2, 3) and v = (4, −1).
2. u = (−2, −1, 2, 4) and v = (−3, 5, 1, 1).
3. Repeat Problems 1–2 for the inner product on Rn given
by
⟨u, v⟩= 2u1v1 + u2v2 + u3v3 + · · · + unvn.
For Problems 4–5, ﬁnd an orthonormal basis for the row
space, column space, and null space of the given matrix A.
4. A =
⎡
⎢⎢⎣
1 2 6
2 1 6
0 1 2
1 0 2
⎤
⎥⎥⎦.
5. A =
⎡
⎢⎢⎢⎢⎣
1
3 5
−1 −3 1
0
2 3
1
5 2
1
5 8
⎤
⎥⎥⎥⎥⎦
.

5.5
Chapter Review 377
For Problems 6–9, ﬁnd an orthogonal basis for the span of
the set S in the vector space V .
6. V = R3, S = {(5, −1, 2), (7, 1, 1)}.
7. V = R3, S = {(6, −3, 2), (1, 1, 1), (1, −8, −1)}.
8. V = P3(R), S = {2x −x3, 1 + x + x2, 3, x}, using
p · q =
! 1
0 p(t)q(t)dt.
9. V = M2(R), S =
"# 1 2
2 1
$
,
# 3 4
4 3
$
,
# −2 −1
−1 −2
$
,
# −3 0
0 3
$
,
# 2 0
0 0
$%
, using the inner product deﬁned
in Problem 11 of Section 5.1.
10. Let t0, t1, . . . , tn be real numbers. For p and q in
Pn(R), deﬁne
⟨p, q⟩= p(t0)q(t0) + p(t1)q(t1) + · · · + p(tn)q(tn).
(a) Prove that ⟨, ⟩deﬁnes a valid inner product on
Pn(R).
(b) Let t0 = −3, t1 = −1, t2 = 1, and t3 = 3. Let
p0(t) = 1, p1(t) = t, and p2(t) = t2. Show that
p0 and p1 are orthogonal in this inner product
space.
(c) Find a polynomial q that is orthogonal to p0 and
p1, such that {p0, p1, q} is an orthogonal basis
for span{p0, p1, p2}.
11. Find the distance from the point (2, 3, 4) to the line in
R3 passing through (0, 0, 0) and (6, −1, −4).
12. Find the distance from the point P(0, 0, 0) to the plane
with equation 2x −y + 3z = 6.
13. Find the distance from the point P(−1, 3, 5) to the
plane with equation −x + 3y + 3z = 8.
14. Find the distance from the point P(−2, 8, 0) to the
plane in R3 that contains the vectors v = (−1, 0, 5)
and w = (3, 3, −1).
In Problems 15–18, ﬁnd the equation of the least squares line
to the given data points.
15. (0, −2), (1, −1), (2, 1), (3, 2), (4, 2).
16. (−1, 5), (1, 1), (2, 1), (3, −3).
17. (−4, −1), (−3, 1), (−2, 3), (0, 7).
18. (−3, 1), (−2, 0), (−1, 1), (0, −1), (2, −1).
19. ⋄Usesomeformoftechnologytoﬁndtheleastsquares
parabola in each of Problems 15–18.
20. Let
V
be an inner product space with basis
{v1, v2, . . . , vn}. If x and y are vectors in V such that
⟨x, vi⟩= ⟨y, vi⟩for each i = 1, 2, . . . , n, prove that
x = y.
Project: Orthogonal Complement
Let V be an inner product space and let W be a subspace of V .
Part 1: Deﬁnition
Let
W ⊥= {v ∈V : ⟨v, w⟩= 0 for all w ∈W}.
Show that:
(a) W ⊥is a subspace of V .
(b) W ⊥∩W = {0}. That is, W ⊥and W share only the zero vector.
Part 2: Examples
(a) Let V = M2(R) with inner product
&# a11
a12
a21
a22
$
,
# b11
b12
b21
b22
$'
= a11b11 + a12b12 + a21b21 + a22b22.
Find the orthogonal complement of the set W of 2 × 2 symmetric matrices.

378
CHAPTER 5
Inner Product Spaces
(b) Let V = M3(R) with inner product deﬁned analogously to the inner product used
in part (a) on M2(R). Find the orthogonal complement of the set W of 3 × 3
symmetric matrices.
(c) Generalize parts (a) and (b) to the set V = Mn(R) and prove your conclusions.
(d) Let A be an m × n matrix. Show that
(rowspace(A))⊥= nullspace(A)
and
(colspace(A))⊥= nullspace(AT ).
Use this to ﬁnd the orthogonal complement of the row space and column space of
the matrices below:
(i) A =
# 3 1 −1
6 0 −4
$
.
(ii) A =
⎡
⎣
−1
0 6
2
3 −1 0
4
1
1 1 −1
⎤
⎦.
(e) Find the orthogonal complement of
(i) the line in R3 containing the points (0, 0, 0) and (2, −1, 3).
(ii) the plane 2x + 3y −4z = 0 in R3.
Part 3: Some Theoretical Results
Let W be a subspace of a ﬁnite dimensional inner product space V .
(a) Show that every vector in V can be written uniquely in the form w + w⊥, where
w ∈W and w⊥∈W ⊥.
[Hint: By Gram-Schmidt, v can be projected onto the subspace W as, say
projW(v), and so v = projW(v) + w⊥, where w⊥∈W ⊥. For the uniqueness,
use the fact that W ∩W ⊥= {0}.]
(b) Use part (a) to show that
dim[V ] = dim[W] + dim[W ⊥].
(c) Show that
(W ⊥)⊥= W.
(d) Prove that if W1 is a subspace of W2, then (W2)⊥is a subspace of (W1)⊥.

6
Linear Transformations
In Chapter 4, we began building a general framework for studying linear problems.
This framework was the mathematical concept of a vector space. Then in Chapter 5, we
introduced the concept of an inner product. By endowing a vector space with an inner
product, geometric insight became available in the vector space. In both of these past
two chapters, our attention at any given moment has been focused on the properties of a
single vector space (or inner product space) V . However, a rich mastery of linear algebra
also requires a working knowledge of the relationships between different vector spaces.
Many problems in linear algebra involve the simultaneous consideration of more
than one vector space. For example, let A be an m × n matrix and consider the linear
system
Ax = 0.
In this case, note that x is a vector in Rn, while the right-hand side vector 0 is a vector in
Rm. In fact, the matrix A can be viewed as a mapping T that accepts inputs x from the
vector space V = Rn and yields outputs
T (x) = Ax
in the vector space W = Rm. In terms of this mapping, the solution set to the homo-
geneous linear system Ax = 0, for example, consists of all vectors x in Rn with the
property that T (x) = 0.
We can use the general mapping notation T in other problems as well. For example,
consider the second-order differential equation
y′′ + y = 0
(6.0.1)
and the associated “mapping of functions” T deﬁned by
T (y) = y′′ + y.
379

380
CHAPTER 6
Linear Transformations
Given a function y, T maps y to the function y′′ + y. For example,
T (x2) = (x2)′′ + (x2) = 2 + x2,
T (ln x) = (ln x)′′ + (ln x) = −1
x2 + ln x.
In terms of the mapping T , the solution set S to the differential equation (6.0.1) consists
of all those functions y that are mapped to the zero function:
S = {y : T (y) = 0}.
The point that we are making is that a variety of problems we have studied to this point
in the text, both in linear algebra and in differential equations, can be viewed as special
cases of the general problem of ﬁnding all vectors v in a vector space with the property
that T (v) = 0, where T is a mapping from a vector space V into a vector space W.
Observe that the mapping T satisﬁes the linearity properties
T (u + v) = T (u) + T (v)
for all u, v ∈V
T (c v) = c T (v)
for all v ∈V and all scalars c.
Any mapping that satisﬁes these properties is called a linear function, or a linear transfor-
mation. We will see in this chapter that the general linear framework that we have been
aiming for is indeed completed once an appropriate linear transformation is deﬁned with
inputs from the vector space of unknowns in the problem we are studying. We will show
that the set of all solutions to the corresponding homogeneous linear problem T (v) = 0
is a subspace of the vector space from which we are mapping. Consequently, once we
have determined the dimension of that solution space, we will know how many linearly
independent solutions to T (v) = 0 are required to determine all of its solutions.
6.1
Definition of a Linear Transformation
We begin with a precise deﬁnition of a mapping between two vector spaces.
DEFINITION
6.1.1
Let V and W be vector spaces. A mapping T from V into W is a rule that assigns to
each vector v in V precisely one vector w = T (v) in W. We denote such a mapping
by T : V →W.
Example 6.1.2
The following are examples of mappings between vector spaces:
1. T : Mn(R) →Mn(R) deﬁned by T (A) = AT .
2. T : Mn(R) →R deﬁned by T (A) = det(A).
3. T : P1(R) →P2(R) deﬁned by T (a0 + a1x) = 2a0 + a1 + (a0 + 3a1)x + 4a1x2.
4. T : C0[a, b] →R deﬁned by T ( f ) =
! b
a f (x) dx.
□
The basic operations of addition and scalar multiplication in a vector space V enable
us to form only linear combinations of vectors in V . In keeping with the aim of studying
linear mathematics, it is natural to restrict attention to mappings T : V →W that
preserve such linear combinations of vectors in the sense that
T (c1v1 + c2v2) = c1T (v1) + c2T (v2),

6.1
Definition of a Linear Transformation 381
for all vectors v1, v2 in V and all scalars c1, c2. The most general type of mapping that
does this is called a linear transformation.
DEFINITION
6.1.3
Let V and W be vector spaces.1 A mapping T : V →W is called a linear transfor-
mation from V to W if it satisﬁes the following properties:
1. T (u + v) = T (u) + T (v) for all u, v ∈V .
2. T (c v) = c T (v) for all v ∈V and all scalars c.
We refer to these properties as the linearity properties. The vector space V is
called the domain of T , while the vector space W is called the codomain of T .
Observe that the additive operation appearing on the left side of (1) in Deﬁnition
6.1.3 refers to addition in V , while the additive operation appearing on the right side of
(1) in Deﬁnition 6.1.3 refers to addition in W. Although we use the same symbol for
addition in each vector space, it is important to realize that they need not be the same
operation. The same remarks apply to the scalar multiplication operations appearing in
(2) in Deﬁnition 6.1.3.
A mapping T : V →W that does not satisfy Deﬁnition 6.1.3 is called a nonlinear
transformation. For instance, in Example 6.1.2, (2) is a nonlinear transformation if
n > 1 since, for example
T (2A) = det(2A) = 2ndet(A) ̸= 2 det(A) = 2 T (A),
unless A is not invertible. On the other hand, (1) in Example 6.1.2 is a linear transfor-
mation, since for all n × n matrices A, B and scalars c, we have
T (A + B) = (A + B)T = AT + BT = T (A) + T (B),
and
T (cA) = (cA)T = cAT = c T (A).
Likewise, for (3), in Example 6.1.2, we can consider two polynomials a0 + a1x and
b0 + b1x, and a scalar c. Then we have
T ((a0 + a1x) + (b0 + b1x)) = T ((a0 + b0) + (a1 + b1)x)
= 2(a0 + b0) + (a1 + b1) + ((a0 + b0)
+ 3(a1 + b1))x + 4(a1 + b1)x2
= ((2a0 + a1) + (a0 + 3a1)x + 4a1x2)
+ ((2b0 + b1) + (b0 + 3b1)x + 4b1x2)
= T (a0 + a1x) + T (b0 + b1x),
and
T (c(a0 + a1x)) = T (ca0 + ca1x) = 2ca0 + ca1 + (ca0 + 3ca1)x + 4ca1x2
= c(2a0 + a1 + (a0 + 3a1)x + 4a1x2)
= c T (a0 + a1x).
1The vector spaces V and W must either be both real vector spaces or both complex vector spaces in order
that we use the same scalars in both spaces.

382
CHAPTER 6
Linear Transformations
Similarly, it can be shown that the mapping in (4) in Example 6.1.2 is a linear
transformation. We now give some further examples.
Example 6.1.4
Deﬁne T : C1(I) →C0(I) by T ( f ) = f ′. Verify that T is a linear transformation.
Solution:
If f and g are in C1(I) and c is a real number, then
T ( f + g) = ( f + g)′ = f ′ + g′ = T ( f ) + T (g)
and
T (cf ) = (cf )′ = cf ′ = c T ( f ).
Thus, T satisﬁes both properties of Deﬁnition 6.1.3 and is a linear transformation.
□
Example 6.1.5
Deﬁne T : C2(I) →C0(I) by T (y) = y′′ + y. Verify that T is a linear transformation.
Solution:
If y1 and y2 are in C2(I), then
T (y1 + y2) = (y1 + y2)′′ + (y1 + y2)
= y′′
1 + y′′
2 + y1 + y2
= (y′′
1 + y1) + (y′′
2 + y2)
= T (y1) + T (y2).
Furthermore, if c is an arbitrary real number, then
T (cy1) = (cy1)′′ + (cy1) = cy′′
1 + cy1 = c(y′′
1 + y1) = c T (y1).
Consequently, since both properties of Deﬁnition 6.1.3 are satisﬁed, T is a linear
transformation.
□
Example 6.1.6
Deﬁne T : M23(R) →M2(R) by
T
!" a
b
c
d
e
f
#$
=
" c + 3 f
−b
−b
4a −3d
#
.
Verify that T is a linear transformation.
Solution:
Let A =
" a
b
c
d
e
f
#
and B =
" a′
b′
c′
d′
e′
f ′
#
. Then
T (A + B) = T
!" a + a′
b + b′
c + c′
d + d′
e + e′
f + f ′
#$
=
" (c + c′) + 3( f + f ′)
−(b + b′)
−(b + b′)
4(a + a′) −3(d + d′)
#
=
" c + 3 f
−b
−b
4a −3d
#
+
" c′ + 3 f ′
−b′
−b′
4a′ −3d′
#
= T (A) + T (B).
Moreover, for any scalar k, we have
T (k A) = T
!" ka
kb
kc
kd
ke
k f
#$
=
" kc + 3k f
−kb
−kb
4ka −3kd
#
= k
" c + 3 f
−b
−b
4a −3d
#
= kT (A).

6.1
Definition of a Linear Transformation 383
Once more, both conditions of Deﬁnition 6.1.3 are satisﬁed, and hence, T is a linear
transformation.
□
Remark
The codomain of the linear transformation T given in Example 6.1.6 is
M2(R). However, since all output values T (A) are symmetric 2 × 2 matrices, we can
view the transformation as T ′ : M23(R) →W, where W is the vector space of all 2 × 2
symmetric matrices with entries in R.
Deﬁnition 6.1.3 indicates two steps that must be veriﬁed to show that a mapping
T : V →W is linear. Our next result shows that this two-step process can be combined
into a single step if desired.
Theorem 6.1.7
A mapping T : V →W is a linear transformation if and only if
T (c1v1 + c2v2) = c1T (v1) + c2T (v2),
(6.1.1)
for all v1, v2 in V and all scalars c1, c2.
Proof Suppose T satisﬁes Equation (6.1.1). Then property (1) of Deﬁnition 6.1.3 arises
as the special case c1 = c2 = 1, v1 = u, v2 = v. Further, property (2) of Deﬁnition
6.1.3 is the special case c1 = c, c2 = 0, v1 = v. Since (1) and (2) of Deﬁnition 6.1.3 are
both satisﬁed, T is a linear transformation.
Conversely, if T is a linear transformation, then, using properties (1) and (2) from
Deﬁnition 6.1.3 yields
T (c1v1 + c2v2) = T (c1v1) + T (c2v2) = c1T (v1) + c2T (v2),
so that Equation (6.1.1) is satisﬁed.
Let us illustrate the use of Theorem 6.1.7 with one example.
Example 6.1.8
Deﬁne T : P2(R) →R2 via
T (p(x)) = (p(2), p′(4)).
Verify that T is a linear transformation.
Solution:
To apply Theorem 6.1.7, suppose that p(x) and q(x) belong to P2(R) and
let c1 and c2 be real numbers. Then
T (c1 p(x) + c2q(x)) = (c1 p(2) + c2q(2), c1 p′(4) + c2q′(4))
= (c1 p(2), c1 p′(4)) + (c2q(2), c2q′(4))
= c1(p(2), p′(4)) + c2(q(2), q′(4))
= c1T (p(x)) + c2T (q(x)),
so that T is a linear transformation.
□
Repeated application of the linearity properties can now be used to establish that if
T : V →W is a linear transformation, then for all v1, v2, . . . , vk in V and all scalars
c1, c2, . . . , ck, we have
T (c1v1 + c2v2 + · · · + ckvk) = c1T (v1) + c2T (v2) + · · · + ckT (vk).
(6.1.2)
In particular, if {v1, v2, . . . , vk} is a basis for V , then any vector in V can be written as
v = c1v1 + c2v2 + · · · + ckvk,

384
CHAPTER 6
Linear Transformations
for appropriate scalars c1, c2, . . . , ck, so that from Equation (6.1.2),
T (v) = T (c1v1 + c2v2 + · · · + ckvk) = c1T (v1) + c2T (v2) + · · · + ckT (vk).
Consequently, if we know T (v1), T (v2), . . . , T (vk), then we know how every vector in
V transforms. This once more emphasizes the importance of the basis in studying vector
spaces.
Example 6.1.9
If T : R3 →R2 is a linear transformation such that
T (1, 0, 0) = (7, −2),
T (0, 1, 0) = (1, 5),
T (0, 0, 1) = (0, −8),
then we can compute
T (4, 3, 2) = 4 T (1, 0, 0) + 3 T (0, 1, 0) + 2 T (0, 0, 1)
= 4(7, −2) + 3(1, 5) + 2(0, −8) = (31, −9).
In general,
T (a, b, c) = a T (1, 0, 0) + b T (0, 1, 0) + c T (0, 0, 1)
= a(7, −2) + b(1, 5) + c(0, −8) = (7a + b, −2a + 5b −8c).
□
Example 6.1.10
Let T : P2(R) →P2(R) be a linear transformation satisfying
T (1) = 2 −3x,
T (x) = 2x + 5x2,
T (x2) = 3 −x + x2.
For an arbitrary vector p(x) = a0 + a1x + a2x2 in P2(R), determine T (p(x)).
Solution:
Since we have been given the transformation of the (standard) basis
{1, x, x2} for P2(R), we can determine the transformation of all vectors in P2(R).
Using the linearity properties in Deﬁnition 6.1.3, it follows that
T (a0 + a1x + a2x2) = T (a0) + T (a1x) + T (a2x2)
= a0T (1) + a1T (x) + a2T (x2)
= a0(2 −3x) + a1(2x + 5x2) + a2(3 −x + x2)
= 2a0 + 3a2 + (−3a0 + 2a1 −a2)x + (5a1 + a2)x2.
□
The next theorem lists two basic properties of linear transformations. In this theorem,
we distinguish the zero vector in V , denoted 0V , from the zero vector in W, denoted 0W.
Theorem 6.1.11
Let T : V →W be a linear transformation. Then
1. T (0V ) = 0W,
2. T (−v) = −T (v) for all v ∈V .
Proof
1. If v is a vector in V , then 0 · v = 0V , by Theorem 4.2.7 (2). Consequently,
T (0V ) = T (0 · v) = 0 · T (v) = 0W.
2. We know that −v = (−1)v for all v ∈V , by Theorem 4.2.7 (5). Consequently,
T (−v) = T ((−1)v) = (−1)T (v) = −T (v).

6.1
Definition of a Linear Transformation 385
Linear Transformations from Rn to Rm
Linear transformations between the vector spaces Rn and Rm play a very fundamental
role in linear algebra and its applications. We now investigate some of their properties.
Example 6.1.12
Deﬁne T : R2 →R4 as follows: If x = (x1, x2), then
T (x) = (2x1 + x2, 3x1 −x2, −5x1 + 3x2, −4x2).
Verify that T is a linear transformation from R2 to R4.
Solution:
Let x = (x1, x2) and y = (y1, y2) be arbitrary vectors in R2. Then, using
vector addition in R2, we have x + y = (x1 + y1, x2 + y2). Consequently,
T (x + y) = T (x1 + y1, x2 + y2)
= (2(x1 + y1) + (x2 + y2), 3(x1 + y1) −(x2 + y2),
−5(x1 + y1) + 3(x2 + y2), −4(x2 + y2))
= (2x1 + x2, 3x1 −x2, −5x1 + 3x2, −4x2)
+ (2y1 + y2, 3y1 −y2, −5y1 + 3y2, −4y2)
= T (x) + T (y).
Further, if c is any real number, then cx = (cx1, cx2), so that
T (cx) = T (cx1, cx2) = (2cx1 + cx2, 3cx1 −cx2, −5cx1 + 3cx2, −4cx2)
= c(2x1 + x2, 3x1 −x2, −5x1 + 3x2, −4x2)
= cT (x).
Hence, properties (1) and (2) of Deﬁnition 6.1.3 are satisﬁed, and so T is a linear
transformation from R2 to R4.
□
Our next theorem introduces how linear transformations from Rn to Rm arise.
Theorem 6.1.13
Let A be an m × n real matrix, and deﬁne T : Rn →Rm by T (x) = Ax. Then T is a
linear transformation.
Proof We need only verify the two linearity properties in Deﬁnition 6.1.3. Let x and y
be arbitrary vectors in Rn, and let c be an arbitrary real number. Then
T (x + y) = A(x + y) = Ax + Ay = T (x) + T (y),
T (cx) = A(cx) = cAx = c T (x).
A linear transformation T : Rn →Rm deﬁned by T (x) = Ax, where A is an m × n
matrix, is called a matrix transformation.
Example 6.1.14
Determine the matrix transformation T : R2 →R4 if
A =
⎡
⎢⎢⎣
2
1
3 −1
−5
3
0 −4
⎤
⎥⎥⎦.

386
CHAPTER 6
Linear Transformations
Solution:
We have
T (x) = Ax =
⎡
⎢⎢⎣
2
1
3 −1
−5
3
0 −4
⎤
⎥⎥⎦
( x1
x2
)
=
⎡
⎢⎢⎣
2x1 + x2
3x1 −x2
−5x1 + 3x2
−4x2
⎤
⎥⎥⎦,
which we write as
T (x1, x2) = (2x1 + x2, 3x1 −x2, −5x1 + 3x2, −4x2).
□
If we compare Examples 6.1.12 and 6.1.14, we see that they contain the same linear
transformation, but deﬁned in two different ways. This leads to the question as to whether
we can always describe a linear transformation from Rn to Rm as a matrix transformation.
The following theorem answers this in the afﬁrmative:
Theorem 6.1.15
Let T : Rn →Rm be a linear transformation. Then T is described by the matrix
transformation
T (x) = Ax,
where A is the m × n matrix
A = [T (e1), T (e2), . . . , T (en)]
and e1, e2, . . . , en denote the standard basis vectors in Rn.
Proof Any vector x = (x1, x2, . . . , xn) in Rn can be expressed in terms of the standard
basis as
x = x1e1 + x2e2 + · · · + xnen.
Thus, since T : Rn →Rm is a linear transformation, properties (1) and (2) of Deﬁnition
6.1.3 imply that
T (x) = T (x1e1 + x2e2 + · · · + xnen)
= x1T (e1) + x2T (e2) + · · · + xnT (en).
Consequently, T (x) is a linear combination of the vectors T (e1),T (e2),. . . ,T (en), each
of which is a vector in Rm. Therefore, the preceding expression for T (x) can be written
as the matrix product
T (x) = [T (e1), T (e2), . . . , T (en)]
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦= Ax,
where A = [T (e1), T (e2), . . . , T (en)].
DEFINITION
6.1.16
If T : Rn →Rm is a linear transformation, then the m × n matrix
A = [T (e1), T (e2), . . . , T (en)]
is called the matrix of T .

6.1
Definition of a Linear Transformation 387
Example 6.1.17
Determine the matrix of the linear transformation T : R3 →R4 deﬁned by
T (x1, x2, x3) = (−x1 + 3x3, −2x3, 2x1 + 5x2 −9x3, −7x1 + 5x2).
(6.1.3)
Solution:
The standard basis vectors in R3 are
e1 = (1, 0, 0),
e2 = (0, 1, 0),
e3 = (0, 0, 1).
Consequently, from (6.1.3),
T (e1) = (−1, 0, 2, −7),
T (e2) = (0, 0, 5, 5),
T (e3) = (3, −2, −9, 0),
so that the matrix of the transformation is
A = [T (e1), T (e2), T (e3)] =
⎡
⎢⎢⎣
−1 0
3
0 0 −2
2 5 −9
−7 5
0
⎤
⎥⎥⎦.
□
As we have seen, linear transformations T : Rn →Rm are completely determined
from the matrix of T . Therefore, to answer questions regarding such linear transforma-
tions, it is almost always desirable to calculate the matrix of T . Here is one more example.
Example 6.1.18
Suppose T : R3 →R2 is a linear transformation with
T (1, 0, 0) = (4, 5),
T (0, 1, 0) = (−1, 1),
T (2, 1, −3) = (7, −1).
Find T (x1, x2, x3).
Solution:
We need to compute the matrix of T , which is A =
* T (e1), T (e2), T (e3)+
.
We are already given T (1, 0, 0) and T (0, 1, 0), but to ﬁnd T (0, 0, 1), we must write
(0, 0, 1) as a linear combination of the vectors (1, 0, 0), (0, 1, 0), and (2, 1, −3). A short
calculation shows that
(0, 0, 1) = 2
3(1, 0, 0) + 1
3(0, 1, 0) −1
3(2, 1, −3).
Hence,
T (0, 0, 1) = 2
3T (1, 0, 0) + 1
3T (0, 1, 0) −1
3T (2, 1, −3)
= 2
3(4, 5) + 1
3(−1, 1) −1
3(7, −1)
= (0, 4).
Thus, the matrix of T is
A =
( 4 −1 0
5
1 4
)
,
so that
T (x1, x2, x3) =
( 4 −1 0
5
1 4
) ⎡
⎣
x1
x2
x3
⎤
⎦= (4x1 −x2, 5x1 + x2 + 4x3).
□
Finally in this section, consider the mapping T : R2 →R2, which rotates each point
in the plane through an angle θ in the counterclockwise direction, where 0 ≤θ ≤2π.
As Figure 6.1.1 illustrates, T is a linear transformation. In order to determine the matrix

388
CHAPTER 6
Linear Transformations
x
y
r
ot
a
t
e
y
x
y
x
T(x + y) 5 T(x) 1 T(y)
x + y
T(x)
T(y)
T(cx) 5 cT(x)
T(x)
cx
x
Figure 6.1.1: Rotation in the plane satisﬁes the basic linearity properties and therefore is a
linear transformation.
of this transformation, all we need to do is obtain T (e1) and T (e2). From Figure 6.1.2,
we see that
T (e1) = (cos θ, sin θ),
T (e2) = (−sin θ, cos θ).
Consequently, the matrix of the transformation is
T (θ) =
( cos θ
−sin θ
sin θ
cos θ
)
.
(6.1.4)
u
u
1
1
x
y
e2
e1
(2sin u, cos u)
(cos u, sin u)
Figure 6.1.2: Determining the transformation matrix corresponding to a rotation in the
xy-plane.
Exercises for 6.1
Key Terms
Mapping,
Linear
transformation,
Linearity
properties,
Nonlinear transformation, Matrix transformation, Matrix
of T .
Skills
• Be able to determine and verify whether a given map-
ping T : V →W is a linear or nonlinear transfor-
mation.
• Be able to determine the matrix of a linear transfor-
mation T : Rn →Rm.
• Given a linear transformation T : V →W and values
T (v1), T (v2), . . . , T (vk) for a basis {v1, v2, . . . , vk}
of V , be able to ﬁnd T (v) for any vector v in V .
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A linear transformation T : V →W is a mapping that
satisﬁes the conditions T (u + v) = T (u) + T (v) and
T (c · v) = c · T (v) for some vectors u, v in V and for
some scalar c.
(b) A linear transformation T : Rn →Rm can be rep-
resented by the formula T (x) = Ax for some n × m
matrix A.
(c) The formula T (0V ) = 0W holds for any mapping
T : V →W.

6.1
Definition of a Linear Transformation 389
(d) The matrix of a linear transformation T : Rn →Rm
is the matrix A =
* T (e1), T (e2), . . . , T (en)+
.
(e) If T : V →W is a linear transformation, the formula
T (−v) = −T (v) holds for every v in V .
(f) A linear transformation T : V →W must satisfy
T ((c + d)v) = cT (v) + dT (v)
for every vector v in V and for all scalars c and d.
Problems
For Problems 1–8, verify directly from Deﬁnition 6.1.3 that
the given mapping is a linear transformation.
1. T : R3 →R2 deﬁned by
T (x1, x2, x3) = (x1 + 3x2 + x3, x1 −x2).
2. T : R2 →R2 deﬁned by
T (x1, x2) = (x1 + 2x2, 2x1 −x2).
3. T : C2(I) →C0(I) deﬁned by
T (y) = y′′ + a1y′ + a2y,
where a1 and a2 are functions deﬁned on I.
4. T : C2(I) →C0(I) deﬁned by
T (y) = y′′ −16y.
5. T : Mn(R) →Mn(R) deﬁned by
T (A) = AB −B A,
where B is a ﬁxed n × n matrix.
6. T : C0[a, b] →R deﬁned by
T ( f ) =
, b
a
f (x) dx.
7. T : Mn(R) →R deﬁned by T (A) = tr(A), where
tr(A) denotes the trace of A.
8. S : Mn(R) →Mn(R) deﬁned by
S(A) = A + AT .
For Problems 9–13, show that the given mapping is a
nonlinear transformation.
9. T : P2(R) →R deﬁned by
T (a + bx + cx2) = a + b + c + 1.
10. T : M2(R) →M2(R) deﬁned by T (A) = A2.
11. T : C0[a, b] →C0[a, b] deﬁned by T ( f (x)) = x.
12. T : R2 →R2 deﬁned by
T (x1, x2) = (x1 + x2, 2).
13. T : M2(R) →R deﬁned by
T (A) = det(A).
For Problems 14–18, determine the matrix of the given trans-
formation
T : Rn →Rm.
14. T (x1, x2) = (3x1 −2x2, x1 + 5x2).
15. T (x1, x2) = (x1 + 3x2, 2x1 −7x2, x1).
16. T (x1, x2, x3) = (x1 −x2 + x3, x3 −x1).
17. T (x1, x2, x3) = x1 + 5x2 −3x3.
18. T (x1, x2, x3) = (x3 −x1, −x1, 3x1 + 2x3, 0).
For Problems 19–23, determine the linear transformation
T : Rn →Rm that has the given matrix.
19. A =
(
1 3
−4 7
)
.
20. A =
( 2 −1
5
3
1 −2
)
.
21. A =
⎡
⎣
2
2 −3
4 −1
2
5
7 −8
⎤
⎦.
22. A =
⎡
⎢⎢⎣
−3
−2
0
1
⎤
⎥⎥⎦.
23. A =
* 1 −4 −6 0 2 +
.
24. Let V be a real inner product space, and let u be a
ﬁxed (nonzero) vector in V . Deﬁne T : V →R by
T (v) = ⟨u, v⟩.
Use properties of the inner product to show that T is
a linear transformation.
25. Let V bearealinnerproductspace,andletu1 andu2 be
ﬁxed (nonzero) vectors in V . Deﬁne T : V →R2 by
T (v) = (⟨u1, v⟩, ⟨u2, v⟩).
Use properties of the inner product to show that T is
a linear transformation.

390
CHAPTER 6
Linear Transformations
26. (a) Let v1 = (1, 1) and v2 = (1, −1). Show that
{v1, v2} is a basis for R2.
(b) Let T : R2 →R2 be the linear transformation
satisfying
T (v1) = (2, 3),
T (v2) = (−1, 1),
where v1 and v2 are the basis vectors given in (a).
Find T (x1, x2) for an arbitrary vector (x1, x2) in
R2. What is T (4, −2)?
For Problems 27–30, assume that T deﬁnes a linear trans-
formation and use the given information to ﬁnd the matrix
of T .
27. T : R2 →R4 such that T (−1, 1) = (1, 0, −2, 2) and
T (1, 2) = (−3, 1, 1, 1).
28. T : R4 →R2 such that T (1, 0, 0, 0) = (3, −2),
T (1, 1, 0, 0) = (5, 1), T (1, 1, 1, 0) = (−1, 0), and
T (1, 1, 1, 1) = (2, 2).
29. T : R3 →R3 such that T (1, 2, 0) = (2, −1, 1),
T (0, 1, 1) = (3, −1, −1) and T (0, 2, 3) = (6, −5, 4).
30. T : R3 →R4 such that T (0, −1, 4) = (2, 5, −2, 1),
T (0, 3, 3)
=
(−1, 0, 0, 5), and T (4, 4, −1)
=
(−3, 1, 1, 3).
31. Let T : P2(R) →P2(R) be the linear transformation
satisfying
T (1) = x + 1,
T (x) = x2 −1,
T (x2) = 3x + 2.
Determine T (ax2 + bx + c), where a, b, and c are
arbitrary real numbers.
32. Let T : V →V be a linear transformation, and sup-
pose that
T (2v1 + 3v2) = v1 + v2,
T (v1 + v2) = 3v1 −v2.
Find T (v1) and T (v2).
33. Let T : P2(R) →P2(R) be the linear transformation
satisfying:
T (x2 −1) = x2 + x −3,
T (2x) = 4x,
T (3x + 2) = 2(x + 3).
Find T (1), T (x), T (x2), and hence show that
T (ax2 + bx + c) = ax2 −(a −2b + 2c)x + 3c,
where a, b, and c are arbitrary real numbers.
34. Let {v1, v2} be a basis for the vector space V . If
T : V →V is the linear transformation satisfying
T (v1) = 3v1 −v2,
T (v2) = v1 + 2v2,
ﬁnd T (v) for an arbitrary vector in V .
35. Let T : V →W and S : V →W be linear trans-
formations, and assume that {v1, v2, . . . , vk} spans V .
Prove that if T (vi) = S(vi) for each i = 1, 2, . . . , k,
then T = S; that is, T (v) = S(v) for each v ∈V .
36. Let V be a vector space with basis {v1, v2, . . . , vk} and
suppose T : V →W is a linear transformation such
that T (vi) = 0 for each i = 1, 2, . . . , k. Prove that T
is the zero transformation; that is, T (v) = 0 for each
v ∈V .
Let T1 : V →W and T2 : V →W be linear transforma-
tions, and let c be a scalar. We deﬁne the sum T1 + T2 and
the scalar product cT1 by
(T1 + T2)(v) = T1(v) + T2(v)
and
(cT1)(v) = cT1(v)
for all v ∈V . The remaining problems in this section con-
sider the properties of these mappings.
37. Verify that T1+T2 and cT1 are linear transformations.
38. Let T1 : R2 →R2 and T2 : R2 →R2 be the linear
transformations with matrices
A =
(
3 1
−1 2
)
,
B =
( 2
5
3 −4
)
.
Find T1 + T2 and cT1.
39. Let T1 : Rn →Rm and T2 : Rn →Rm be the linear
transformations with matrices A and B respectively.
Show that T1 + T2 and cT1 are the linear transforma-
tions with matrices A + B and cA respectively.
40. Let V and W be vector spaces, and let L(V, W) de-
note the set of all linear transformations from V into
W. Verify that L(V, W) together with the operations
of addition and scalar multiplication just deﬁned for
linear transformations is a vector space.

6.2
Transformations of R2
391
∗6.2
Transformations of R2
To gain some geometric insight into linear transformations, we consider the particular
case of linear transformations T : R2 →R2. Any such transformation is called a trans-
formation of R2. Geometrically, the action of a transformation of R2 can be represented
by its effect on an arbitrary point in the Cartesian plane. We ﬁrst establish that trans-
formations of R2 map lines into lines. Recall that the parametric equations of a line
passing through the point x1 = (x1, y1) in the direction of the vector v with components
(a, b) are
x = x1 + at,
y = y1 + bt,
which can be written as
x = x1 + tv.
(6.2.1)
The transformation T (x) = Ax therefore transforms the points along this line into
T (x) = A(x1 + tv) = Ax1 + t Av.
(6.2.2)
Consequently, the transformed points lie along the line with parametric equations
T (x) = y1 + tw,
where y1 = Ax1 and w = Av. This is illustrated in Figure 6.2.1. It follows that if we
know how two points in R2 transform, then we can determine the transformation of all
points along the line joining those two points. Further, the linearity properties (1) and
(2) of Deﬁnition 6.1.3 are the statement that the parallelogram law for vector addition is
preserved under the transformation. This is illustrated in Figure 6.2.2. From Equations
(6.2.1) and (6.2.2), we see that any line with direction vector v is mapped into a line with
direction vector Av, so that parallel lines are mapped to parallel lines by a transformation
of R2. In describing speciﬁc transformations of R2, it is often useful to determine the
effect of such a transformation on the points lying inside a rectangle. Due to the fact that
a transformation of R2 maps parallel lines into parallel lines, it follows that the transform
of a rectangle will be the parallelogram whose vertices are the transforms of the vertices
of the rectangle. This is illustrated in Figure 6.2.3.
v
x
y
x1 5 (x1, y1)
w 5 Av
y1 5 Ax1
Figure 6.2.1: A transformation of R2 maps the line with vector parametric equation
x = x1 + tv into the line T (x) = Ax1 + t Av = y1 + tw.
∗This section can be omitted without loss of continuity.

392
CHAPTER 6
Linear Transformations
x
y
T(c1x + c2y) = c1T(x) 1 c2T(y) 
c1x
c2y
T(c2y) 5 c2T(y)
T(c1x) 5 c1T(x)
c1 x 1 c2y
Figure 6.2.2: A transformation of R2 preserves the parallelogram law of vector addition in the
sense that T (c1x + c2y) = c1T (x) + c2T (y).
y
x
y
x
x
T(x)
Figure 6.2.3: Transformations of R2 map rectangles into parallelograms.
Simple Transformations
We next introduce some simple transformations of R2 and their geometrical interpre-
tation. We then show how more complicated transformations can be considered as a
combination of these simple ones.
I: Reﬂections. Consider the transformation of R2 with matrix
Rx =
( 1
0
0 −1
)
.
If v = (x, y) is an arbitrary point in R2, then
T (v) = Rx(v) =
( 1
0
0 −1
) ( x
y
)
=
(
x
−y
)
.
Thus,
T (x, y) = (x, −y).
We see that each point in the Cartesian plane is mapped to its mirror image in the x-axis.
Hence, Rx describes the reﬂection in the x-axis. This can be illustrated by considering the
rectangle in the xy-plane through the points (0, 0), (a, 0), (0, b), and (a, b). In order to
determine the transform of this rectangle, all we need to do is determine the transform of
the vertices. (See Figure 6.2.4.) We leave it as an exercise to show that the transformation
y
x
a
b
(a, b)
(a, 2b)
Figure 6.2.4: Reﬂection in the
x-axis.
of R2 with matrix
Ry =
( −1 0
0 1
)

6.2
Transformations of R2
393
describes a reﬂection in the y-axis. Now consider the transformation of R2 with matrix
Rxy =
( 0 1
1 0
)
.
If v = (x, y), then
T (v) = Rxyv =
( 0 1
1 0
) ( x
y
)
=
( y
x
)
.
Thus,
T (x, y) = (y, x).
We see that the x- and y-coordinates have been interchanged. Geometrically, all points
along the line y = x remain ﬁxed, and all other points are transformed into their mirror
image in the line y = x. Consequently Rxy describes a reﬂection in the line y = x. This
is illustrated in Figure 6.2.5.
a
b
x
y
(a, b)
T(a, b) 5 (b, a)
T(a, 0) 5 (0, a)
T(0, b) 5 (b, 0)
(a, a)
y 5 x
Figure 6.2.5: Reﬂection in the line y = x. The rectangle with vertices (0, 0), (a, 0), (0, b),
(a, b) is transformed into the rectangle with vertices (0, 0), (0, a), (b, 0), (b, a).
II: Stretches. The next transformations we analyze are those with matrices
(a)
LSx =
( k
0
0 1
)
,
k > 0
(b)
LSy =
( 1 0
0 k
)
,
k > 0.
Consider ﬁrst (a). If v = (x, y), then
T (x) = LSxv =
( k
0
0 1
) ( x
y
)
=
( kx
y
)
.
Thus,
T (x, y) = (kx, y).
We see that the x-coordinate of each point in the plane is scaled by a factor k, whereas the
y-coordinate is unaltered. Thus, points along the y-axis (x = 0) remain ﬁxed, whereas
points with positive (negative) x-coordinate are moved horizontally to the right (left).
This transformation is called a linear stretch in the x-direction (hence, the notation
LSx). The effect of this transformation is illustrated in Figure 6.2.6. If k > 1, then the
transformation is an expansion, whereas if 0 < k < 1, we have a compression. If k = 1,
then LSx = I2, and all points remain ﬁxed. This is the identity transformation. In a
similar manner, it is easily veriﬁed that the transformation with matrix LSy corresponds
to a linear stretch in the y-direction.

394
CHAPTER 6
Linear Transformations
stretch
x
y
stretch
y
x
22a
22ka
2ka
2a
2a
2ka
ka
a
(a, b)
(ka, b)
Figure 6.2.6: The effect of a linear stretch in the x-direction. Points along the y-axis remain
ﬁxed. All other points are moved parallel to the x-axis.
III: Shears.
Now consider the transformations of R2 with matrices
(a)
Sx =
( 1 k
0 1
)
,
(b)
Sy =
( 1 0
k
1
)
.
For the matrix in (a), if v = (x, y), then
T (v) = Sxv =
( 1 k
0 1
) ( x
y
)
=
( x + ky
y
)
,
so that
T (x, y) = (x + ky, y).
In this case, each point in the plane is moved parallel to the x-axis a distance proportional
to its y-coordinate (points along the x-axis remain ﬁxed). This is referred to as a shear
parallel to the x-axis and is illustrated in Figure 6.2.7 for the case k > 0. We leave it
as an exercise to verify that the transformation of R2 with matrix Sy corresponds to a
shear parallel to the y-axis.
y
x
(a, b)
Figure 6.2.7: A shear parallel to the x-axis. Points on the x-axis remain ﬁxed. Rectangles are
transformed into parallelograms.
Invertible Transformations of R2
We now show that any transformation of R2 with an invertible matrix can be obtained
by combining the basic transformations I–III just described. To do so, we recall from
Section 2.7 that any matrix A can be reduced to reduced row-echelon form through multi-
plication by an appropriate sequence of elementary matrices. In the case of an invertible

6.2
Transformations of R2
395
2 × 2 matrix, the reduced row-echelon form is I2, so that, denoting the elementary
matrices by E1, E2, . . . , En, we can write
En En−1 · · · E2E1A = I2.
Equivalently, since each of the elementary matrices is invertible,
A = E−1
1 E−1
2
· · · E−1
n .
(6.2.3)
Now let T be any transformation of R2 with invertible matrix A. It follows from
Equation (6.2.3) that
T (v) = Av = E−1
1 E−1
2
· · · E−1
n v.
(6.2.4)
Since, as we have shown in Section 2.7, the inverse of an elementary matrix is also an
elementary matrix, (6.2.4) implies that a general transformation of R2 with invertible
matrix can be obtained by applying a sequence of transformations corresponding to
appropriate elementary matrices. Now for the key point. A closer look at the elementary
matrices shows that the following relationships hold between them and the matrices of
the simple transformations introduced previously in this section.
(1) P12 =
( 0 1
1 0
)
corresponds to a reﬂection in the line y = x.
(2a) M1(k) =
( k
0
0 1
)
,
k > 0 corresponds to a linear stretch in the x-direction.
(2b) M1(k) =
( k
0
0 1
)
,
k < 0. In this case, we can write
( k
0
0 1
)
=
( −k
0
0 1
) ( −1 0
0 1
)
,
which corresponds to a reﬂection in the y-axis followed by a linear stretch (stretch
factor −k > 0) in the x-direction.
(3a) M2(k) =
( 1 0
0 k
)
,
k > 0 corresponds to a linear stretch in the y-direction.
(3b) M2(k) =
( 1 0
0 k
)
,
k < 0. This corresponds to a reﬂection in the x-axis followed
by a linear stretch in the y-direction.
(4) A12(k) =
( 1 0
k
1
)
corresponds to a shear parallel to the y-axis.
(5) A21(k) =
( 1 k
0 1
)
corresponds to a shear parallel to the x-axis.
We can therefore conclude that any transformation of R2 with invertible matrix can be
obtained by applying an appropriate sequence of reﬂections, shears, and stretches.
Example 6.2.1
Let T : R2 →R2 be the transformation of R2 with invertible matrix A =
( 3 9
1 2
)
.
Describe T as a combination of reﬂections, shears, and stretches.

396
CHAPTER 6
Linear Transformations
Solution:
Reducing A to reduced row-echelon form in the usual manner yields
( 3 9
1 2
)
1∼
( 1 2
3 9
)
2∼
( 1 2
0 3
)
3∼
( 1 2
0 1
)
4∼
( 1 0
0 1
)
.
1. P12
2. A12(−3) 3. M2(1/3) 4. A21(−2)
The corresponding elementary matrices that accomplish this reduction are
P12 =
( 0 1
1 0
)
,
A12(−3) =
(
1 0
−3 1
)
,
M2(1/3) =
( 1
0
0 1/3
)
,
A21(−2) =
( 1 −2
0
1
)
,
with inverses
P12,
A12(3),
M2(3),
A21(2),
respectively. Consequently,
A =
( 0 1
1 0
) ( 1 0
3 1
) ( 1 0
0 3
) ( 1 2
0 1
)
.
We can therefore write
T (v) = Av =
( 0 1
1 0
) ( 1 0
3 1
) ( 1 0
0 3
) ( 1 2
0 1
)
v.
We see that T consists of a shear parallel to the x-axis, followed by a stretch in the
y-direction, followed by a shear parallel to the y-axis, followed by a reﬂection in y = x.
□
In the previous section, we derived the matrix of the transformation of R2 that
corresponds to a rotation through an angle θ in the counterclockwise direction, namely,
T (θ) =
( cos θ
−sin θ
sin θ
cos θ
)
.
Since this is an invertible matrix for any value of θ, it follows from the analysis in this
section that a rotation is an appropriate combination of reﬂections, shears, and stretches.
Indeed, we leave it as an exercise to verify that, for θ ̸= π/2, 3π/2,
T (θ) =
( cos θ
0
0
1
) (
1
0
sin θ
1
) ( 1
0
0 sec θ
) ( 1 −tan θ
0
1
)
.
(6.2.5)
Exercises for 6.2
Key Terms
Transformation of R2, Reﬂection, Stretch (expansion and
compression), Shear, Invertible transformation of R2.
Skills
• Be able to describe the relationships between elemen-
tary matrices and the matrices representing reﬂections,
stretches, and shears.
• Be able to express any transformation of R2 with
invertible matrix as a composition of reﬂections,
stretches, and shears.
• Be able to recognize reﬂections, stretches, and shears
of R2 by looking at the matrix of the transformation.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Any transformation of R2 maps a line in the plane onto
another line.

6.3
The Kernel and Range of a Linear Transformation 397
(b) The matrix of a reﬂection, stretch, or shear of R2 is an
elementary matrix.
(c) A composition of two shears is a shear.
(d) Every invertible transformation of R2 is a composition
of reﬂections, stretches, and shears.
(e) A composition of two reﬂections is a stretch.
(f) A composition of two stretches is a stretch.
Problems
For Problems 1–4, for the transformation of R2 with the
given matrix, sketch the transform of the square with ver-
tices (1, 1), (2, 1), (2, 2), and (1, 2).
1. A =
( 1 −1
1
2
)
.
2. A =
(
0 1
−1 0
)
.
3. A =
( −2 −2
−2
0
)
.
4. A =
(
1 1
−1 1
)
.
For Problems 5–12, describe the transformation of R2 with
the given matrix as a product of reﬂections, stretches, and
shears.
5. A =
( 0 2
2 0
)
.
6. A =
( 1 2
0 1
)
.
7. A =
( −1
0
0 −1
)
.
8. A =
( 1 0
3 1
)
.
9. A =
( 1 2
3 4
)
.
10. A =
(
1 −3
−2
8
)
.
11. A =
( −1 −1
−1
0
)
.
12. A =
( 1
0
0 −2
)
.
13. Express the transformation of R2 corresponding to a
counterclockwise rotation through an angle θ = π/2
as a product of reﬂections, stretches, and shears. Re-
peat for the case θ = 3π/2.
14. Consider the transformation of R2 corresponding to a
counterclockwise rotation through angle θ (0 ≤θ <
2π). For θ ̸= π/2, 3π/2, verify that the matrix of the
transformation is given by (6.2.5), and describe it in
terms of reﬂections, stretches, and shears.
6.3
The Kernel and Range of a Linear Transformation
If T : V →W is any linear transformation, there is an associated homogeneous linear
vector equation, namely,
T (v) = 0.
The solution set to this vector equation is a subset of V called the kernel of T .
DEFINITION
6.3.1
Let T : V →W be a linear transformation. The set of all vectors v ∈V such that
T (v) = 0 is called the kernel of T and is denoted Ker(T ). Thus,
Ker(T ) = {v ∈V : T (v) = 0}.
Example 6.3.2
Determine Ker(T ) for the linear transformation T : C2(I) →C0(I) in Example 6.1.5
deﬁned by T (y) = y′′ + y.
Solution:
We have
Ker(T ) = {y ∈C2(I) : T (y) = 0} = {y ∈C2(I) : y′′ + y = 0 for all x ∈I}.

398
CHAPTER 6
Linear Transformations
Hence, in this case, Ker(T ) is the solution set to the differential equation
y′′ + y = 0.
Since, from Example 1.2.13, this differential equation has general solution y(x) =
c1 cos x + c2 sin x, we have
Ker(T ) = {y ∈C2(I) : y(x) = c1 cos x + c2 sin x}.
This is the subspace of C2(I) spanned by {cos x, sin x}.
□
The set of all vectors in W that we map onto when T is applied to all vectors in V is
called the range of T . We can think of the range of T as being the set of function output
values. A formal deﬁnition follows.
DEFINITION
6.3.3
The range of the linear transformation T : V →W is the subset of W consisting of
all transformed vectors from V . We denote the range of T by Rng(T ). Thus,
Rng(T ) = {T (v) : v ∈V }.
A schematic representation of Ker(T ) and Rng(T ) is given in Figure 6.3.1.
Rng(T)
0
V
W
Ker(T)
0
Every vector in Ker(T) is mapped
to the zero vector in W
Figure 6.3.1: Schematic representation of the kernel and range of a linear transformation.
Let us now focus on matrix transformations, say T : Rn →Rm. In this particular
case,
Ker(T ) = {x ∈Rn : T (x) = 0}.
If we let A denote the matrix of T , then T (x) = Ax, so that
Ker(T ) = {x ∈Rn : Ax = 0}.
Consequently,
If T : Rn →Rm is the linear transformation with matrix A then Ker(T ) is
the solution set to the homogeneous linear system Ax = 0.
In Section 4.3, we deﬁned the solution set to Ax = 0 to be nullspace(A). Therefore,
we have
Ker(T ) = nullspace(A),
(6.3.1)
from which it follows directly that2 Ker(T ) is a subspace of Rn. Furthermore, for a linear
transformation T : Rn →Rm,
Rng(T ) = {T (x) : x ∈Rn}.
2It is also easy to verify this fact directly by using Deﬁnition 6.3.1 and Theorem 4.3.2.

6.3
The Kernel and Range of a Linear Transformation 399
If A = [a1, a2, . . . , an] denotes the matrix of T , then
Rng(T ) = {Ax : x ∈Rn}
= {x1a1 + x2a2 + · · · + xnan : x1, x2, . . . , xn ∈R}
= colspace(A).
Consequently, Rng(T ) is a subspace of Rm. We illustrate these results with an example.
Example 6.3.4
Let T : R3 →R2 be the linear transformation with matrix A =
!
1 −2
5
−2
4 −10
"
.
Determine Ker(T ) and Rng(T ).
Solution:
To determine Ker(T ), (6.3.1) implies that we need to ﬁnd the solution set
to the system Ax = 0. The reduced row-echelon form of the augmented matrix of this
system is
! 1 −2 5 0
0
0 0 0
"
,
sothattherearetwofreevariables.Setting x2 = r and x3 = s,itfollowsthat x1 = 2r−5s,
so that x = (2r −5s,r, s). Hence,
Ker(T ) = {x ∈R3 : x = (2r −5s,r, s) : r, s ∈R}
= {x ∈R3 : x = r(2, 1, 0) + s(−5, 0, 1), r, s ∈R}.
We see that Ker(T ) is the two-dimensional subspace of R3 spanned by the linearly
independent vectors (2, 1, 0) and (−5, 0, 1), and therefore, it consists of all points lying
on the plane through the origin that contains these vectors. The equation of this plane is
x1 −2x2 + 5x3 = 0. The linear transformation T maps all points lying on this plane to
the zero vector in R2. (See Figure 6.3.2.)
Turning our attention to Rng(T ), recall that, since T is a matrix transformation,
Rng(T ) = colspace(A).
From the foregoing reduced row-echelon form of A we see that colspace(A) is generated
by the ﬁrst column vector in A. Consequently,
Rng(T ) = {y ∈R2 : y = r(1, −2), r ∈R}.
Hence, the points in Rng(T ) lie along the line through the origin in R2 whose
direction is determined by v = (1, −2). The Cartesian equation of this line is
y2 = −2y1. Consequently, T maps all points in R3 onto this line, and therefore Rng(T )
is a one-dimensional subspace of R2. This is illustrated in Figure 6.3.2.
□
x
x3
x2
y2
y1
x1
x12 2x2 1 5x3 5 0
y2 5 22y1
Rng(T)
T(x)
Figure 6.3.2: The kernel and range of the linear transformation in Example 6.3.4.

400
CHAPTER 6
Linear Transformations
To summarize, any matrix transformation T : Rn →Rm with m × n matrix A has
natural subspaces
Ker(T ) = nullspace(A)
(subspace of Rn)
Rng(T ) = colspace(A)
(subspace of Rm)
Now let us return to arbitrary linear transformations. The preceding discussion has
shown that both the kernel and range of any linear transformation from Rn to Rm are
subspaces of Rn and Rm, respectively. Our next result, which is fundamental, establishes
that this is true in general.
Theorem 6.3.5
If T : V →W is a linear transformation, then
1. Ker(T ) is a subspace of V .
2. Rng(T ) is a subspace of W.
Proof In this proof, we once more denote the zero vector in W by 0W. Both Ker(T )
and Rng(T ) are necessarily nonempty, since, as we veriﬁed in Section 6.1, any linear
transformation maps the zero vector in V to the zero vector in W. We must now es-
tablish that Ker(T ) and Rng(T ) are both closed under addition and closed under scalar
multiplication in the appropriate vector space.
1. If v1 and v2 are in Ker(T ), then T (v1) = 0W and T (v2) = 0W. We must show
that v1 + v2 is in Ker(T ); that is, T (v1 + v2) = 0. But we have
T (v1 + v2) = T (v1) + T (v2) = 0W + 0W = 0W,
so that Ker(T ) is closed under addition. Further, if c is any scalar,
T (cv1) = cT (v1) = c0W = 0W,
which shows that cv1 is in Ker(T ), and so Ker(T ) is also closed under scalar
multiplication. Thus, Ker(T ) is a subspace of V .
2. If w1 and w2 are in Rng(T ), then w1 = T (v1) and w2 = T (v2) for some v1 and
v2 in V . Thus,
w1 + w2 = T (v1) + T (v2) = T (v1 + v2).
This says that w1 +w2 arises as an output of the transformation T ; that is, w1 +w2
is in Rng(T ). Thus, Rng(T ) is closed under addition. Further, if c is any scalar,
then
cw1 = cT (v1) = T (cv1),
so that cw1 is the transform of cv1, and therefore cw1 is in Rng(T ). Consequently,
Rng(T ) is a subspace of W.
Remark
We can interpret the ﬁrst part of the preceding theorem as telling us that if T
is a linear transformation, then the solution set to the corresponding linear homogeneous
problem
T (v) = 0
is a vector space. Consequently, if we can determine the dimension of this vector space,
then we know how many linearly independent solutions are required to build every
solution to the problem. This is the formulation for linear problems that we have been
looking for.

6.3
The Kernel and Range of a Linear Transformation 401
Example 6.3.6
Find Ker(S), Rng(S), and their dimensions for the linear transformation S : M2(R) →
M2(R) deﬁned by
S(A) = A −AT .
Solution:
In this case,
Ker(S) = {A ∈M2(R) : S(A) = 0} = {A ∈M2(R) : A −AT = 02}.
Thus, Ker(S) is the solution set of the matrix equation
A −AT = 02,
so that the matrices in Ker(S) satisfy
AT = A.
Hence, Ker(S) is the subspace of M2(R) consisting of all symmetric 2 × 2 matrices. We
have shown previously that a basis for this subspace is
-( 1 0
0 0
)
,
( 0 1
1 0
)
,
( 0 0
0 1
).
,
so that dim[Ker(S)] = 3. We now determine the range of S:
Rng(S) = {S(A) : A ∈M2(R)} = {A −AT : A ∈M2(R)}
=
-( a
b
c
d
)
−
( a
c
b
d
)
: a, b, c, d ∈R
.
=
-(
0
b −c
−(b −c)
0
)
: b, c ∈R
.
.
Thus,
Rng(S) =
-(
0
e
−e 0
)
: e ∈R
.
= span
-(
0 1
−1 0
).
.
Consequently, Rng(S) consists of all skew-symmetric 2×2 matrices with real elements.
Since Rng(S) is generated by the single nonzero matrix
(
0 1
−1 0
)
, it follows that a basis
for Rng(S) is
-(
0 1
−1 0
).
, so that dim[Rng(S)] = 1.
□
Example 6.3.7
Let T : P1(R) →P2(R) be the linear transformation deﬁned by
T (a + bx) = (2a −3b) + (b −5a)x + (a + b)x2.
Find Ker(T ), Rng(T ), and their dimensions.
Solution:
From Deﬁnition 6.3.1,
Ker(T ) = {p ∈P1(R) : T (p) = 0}
= {a + bx : (2a −3b) + (b −5a)x + (a + b)x2 = 0 for all x}
= {a + bx : a + b = 0,
b −5a = 0,
2a −3b = 0}.
But the only values of a and b that satisfy the conditions
a + b = 0,
b −5a = 0,
2a −3b = 0

402
CHAPTER 6
Linear Transformations
are
a = b = 0.
Consequently, Ker(T ) contains only the zero polynomial. Hence, we write
Ker(T ) = {0}.
It follows from Deﬁnition 4.6.7 that dim[Ker(T )] = 0. Furthermore,
Rng(T ) = {T (a +bx) : a, b ∈R} = {(2a −3b)+(b −5a)x +(a +b)x2 : a, b ∈R}.
To determine a basis for Rng(T ), we write this as
Rng(T ) = {a(2 −5x + x2) + b(−3 + x + x2) : a, b ∈R}
= span{2 −5x + x2, −3 + x + x2}.
Thus, Rng(T ) is spanned by the polynomials p1(x) = 2 −5x + x2 and p2(x) = −3 +
x +x2. Since p1 and p2 are not proportional to one another, they are linearly independent
on any interval. Consequently, a basis for Rng(T ) is {2 −5x + x2, −3 + x + x2}, so that
dim[Rng(T )] = 2.
□
The General Rank-Nullity Theorem
In concluding this section, we consider a fundamental theorem for linear transformations
T : V →W that gives a relationship between the dimensions of Ker(T ), Rng(T ), and
V . This is a generalization of the Rank-Nullity Theorem considered in Section 4.9. The
theorem here, Theorem 6.3.8, reduces to the previous result, Theorem 4.9.1, in the case
when T is a linear transformation from Rn to Rm with m × n matrix A. Suppose that
dim[V ] = n and that dim[Ker(T )] = k. Then k-dimensions worth of the vectors in V
are all mapped onto the zero vector in W. Consequently, we only have n −k dimensions
worth of vectors left to map onto the remaining vectors in W. This suggests that
dim[Rng(T )] = dim[V ] −dim[Ker(T )].
This is indeed correct, although a rigorous proof is somewhat involved. We state the
result as a theorem here.
Theorem 6.3.8
(General Rank-Nullity Theorem)
If T : V →W is a linear transformation and V is ﬁnite-dimensional, then
dim[Ker(T )] + dim[Rng(T )] = dim[V ].
Before presenting the proof of this theorem, we give a few applications and exam-
ples. The general Rank-Nullity Theorem is useful for checking that we have the correct
dimensions when determining the kernel and range of a linear transformation. Further-
more, it can also be used to determine the dimension of Rng(T ), once we know the
dimension of Ker(T ), or vice versa. For example, consider the linear transformation
discussed in Example 6.3.6. Theorem 6.3.8 tells us that
dim[Ker(S)] + dim[Rng(S)] = dim[M2(R)],
so that once we had determined that dim[Ker(S)] = 3, it immediately follows that
3 + dim[Rng(S)] = 4,

6.3
The Kernel and Range of a Linear Transformation 403
so that
dim[Rng(S)] = 1.
As another illustration, consider the matrix transformation in Example 6.3.4 with A =
(
1 −2
5
−2
4 −10
)
.Byinspection,wecanseethatdim[Rng(T )]=dim[colspace(A)]= 1,
so that the Rank-Nullity Theorem implies that
dim[Ker(T )] = 3 −1 = 2,
with no additional calculation. Of course, to obtain a basis for Ker(T ), it becomes
necessary to carry out the calculations presented in Example 6.3.4.
Example 6.3.9
Let T : P2(R) →R2 be the linear transformation given in Example 6.1.8 by the formula
T (p(x)) = (p(2), p′(4)). Find Ker(T ), Rng(T ), and their dimensions.
Solution:
It will be necessary to explicitly write p(x) = a + bx + cx2. Then
T (p(x)) = T (a + bx + cx2) = (a + 2b + 4c, b + 8c).
To compute Ker(T ), we set T (p(x)) = (0, 0), which leads to the system of equations
a + 2b + 4c = 0
and
b + 8c = 0.
The augmented matrix for this linear system is
( 1 2 4 0
0 1 8 0
)
. Solving this system, we
ﬁnd that
a = 12t,
b = −8t,
c = t,
where t is a free variable. Hence,
p(x) = 12t −8tx + tx2 = t(12 −8x + x2).
Consequently, a basis for Ker(T ) is given by {12 −8x + x2}, and dim[Ker(T )] = 1.
Next, we consider Rng(T ). By taking advantage of Theorem 6.3.8, we can obtain
this quickly. Since dim[P2(R)] = 3, we have
dim[Rng(T )] = 3 −dim[Ker(T )] = 3 −1 = 2.
However, Rng(T ) is a subspace of R2, and the only two-dimensional subspace of R2 is
R2 itself. Therefore, Rng(T ) = R2, and any basis for R2 serves as a basis for Rng(T )
as well.
□
We close this section with a proof of Theorem 6.3.8.
Proof of Theorem 6.3.8: Suppose that dim[V ] = n. We consider three cases:
Case 1: If dim[Ker(T )] = n, then by Corollary 4.6.14, we conclude that Ker(T ) = V .
This means that T (v) = 0 for every vector v ∈V . In this case
Rng(T ) = {T (v) : v ∈V } = {0},

404
CHAPTER 6
Linear Transformations
and hence, dim[Rng(T )] = 0. Thus, we have
dim[Ker(T )] + dim[Rng(T )] = n + 0 = n = dim[V ],
as required.
Case 2: Assume dim[Ker(T )] = k, where 0 < k < n. Let {v1, v2, . . . , vk} be a basis
for Ker(T ). Then, using Theorem 4.6.17, we can extend this basis to a basis for V , which
we denote by {v1, v2, . . . , vk, vk+1, . . . , vn}.
We prove that {T (vk+1), T (vk+2), . . . , T (vn)} is a basis for Rng(T ). To do this,
we ﬁrst prove that {T(vk+1), T (vk+2), . . . , T (vn)} spans Rng(T ). Let w be any
vector in Rng(T ). Then w = T (v), for some v ∈V . Using the basis for V , we have
v = c1v1 + c2v2 + · · · + cnvn for some scalars c1, c2, . . . , cn. Hence,
w = T (v) = T (c1v1 + c2v2 + · · · + cnvn) = c1T (v1) + c2T (v2) + · · · + cnT (vn).
Since v1, v2, . . . , vk are in Ker(T ), this reduces to
w = ck+1T (vk+1) + ck+2T (vk+2) + · · · + cnT (vn).
Thus,
Rng(T ) = span{T(vk+1), T (vk+2), . . . , T (vn)}.
Next we show that {T (vk+1), T (vk+2), . . . , T (vn)} is linearly independent. Suppose
that
dk+1T (vk+1) + dk+2T (vk+2) + · · · + dnT (vn) = 0,
(6.3.2)
where dk+1, dk+2, . . . , dn are scalars. Then, using the linearity of T ,
T (dk+1vk+1 + dk+2vk+2 + · · · + dnvn) = 0,
which implies that the vector dk+1vk+1 + dk+2vk+2 + · · · + dnvn is in Ker(T ). Conse-
quently, there exist scalars d1, d2, . . . , dk such that
dk+1vk+1 + dk+2vk+2 + · · · + dnvn = d1v1 + d2v2 + · · · + dkvk,
which means that
d1v1 + d2v2 + · · · + dkvk −(dk+1vk+1 + dk+2vk+2 + · · · + dnvn) = 0.
Since the set of vectors {v1, v2, . . . , vk, vk+1, . . . , vn} is linearly independent, we must
have
d1 = d2 = · · · = dk = dk+1 = · · · = dn = 0.
Thus, from Equation (6.3.2), {T(vk+1), T (vk+2), . . . , T (vn)} is linearly independent.
By the work in the last two paragraphs, {T (vk+1), T (vk+2), . . . , T (vn)} is a basis
for Rng(T ). Since there are n −k vectors in this basis, it follows that dim[Rng(T )] =
n −k. Consequently,
dim[Ker(T )] + dim[Rng(T )] = k + (n −k) = n = dim[V ],
as required.
Case 3: If dim[Ker(T )] = 0, then Ker(T ) = {0}. Let {v1, v2, . . . , vn} be any basis
for V . By a similar argument to that used in Case 2 above, it can be shown that (see
Problem 22) {T (v1), T (v2), . . . , T (vn)} is a basis for Rng(T ), and so again we have
dim[Ker(T )] + dim[Rng(T )] = n.

6.3
The Kernel and Range of a Linear Transformation 405
Exercises for 6.3
Key Terms
Kernel and range of a linear transformation, Rank-Nullity
Theorem.
Skills
• Be able to ﬁnd the kernel of a linear transformation
T : V →W and give a basis and the dimension of
Ker(T ).
• Be able to ﬁnd the range of a linear transformation
T : V →W and give a basis and the dimension of
Rng(T ).
• Be able to show that the kernel (resp. range) of a lin-
ear transformation T : V →W is a subspace of V
(resp. W).
• Be able to verify the Rank-Nullity Theorem for a given
linear transformation T : V →W.
• Be able to utilize the Rank-Nullity Theorem to help
ﬁnd the dimensions of the kernel and range of a linear
transformation T : V →W.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If T : V →W is a linear transformation and W is
ﬁnite-dimensional, then
dim[Ker(T )] + dim[Rng(T )] = dim[W].
(b) If T : P4(R) →R7 is a linear transformation, then
Ker(T ) must be at least two-dimensional.
(c) If T : Rn →Rm is a linear transformation with matrix
A, then Rng(T ) is the solution set to the homogeneous
linear system Ax = 0.
(d) The range of a linear transformation T : V →W is a
subspace of V .
(e) If T : M23(R) →P7(R) is a linear transformation
with T
( 1 1 1
0 0 0
)
= 0 and T
( 1 2 3
4 5 6
)
= 0, then
Rng(T ) is at most four-dimensional.
(f) If T : Rn →Rm is a linear transformation with matrix
A, then Rng(T ) is the column space of A.
Problems
1. Consider T : R2 →R4 deﬁned by T (x) = Ax,
where A =
⎡
⎢⎢⎣
1
2
2
4
4
8
8 16
⎤
⎥⎥⎦. For each x below, ﬁnd T (x)
and thereby determine whether x is in Ker(T ).
(a) x = (−10, 5).
(b) x = (1, −1).
(c) x = (2, −1).
2. Consider T : R3 →R2 deﬁned by T (x) = Ax, where
A =
( 1 −1
2
1 −2 −3
)
. For each x below, ﬁnd T (x) and
thereby determine whether x is in Ker(T ).
(a) x = (7, 5, −1).
(b) x = (−21, −15, 2).
(c) x = (35, 25, −5).
For Problems 3–7, ﬁnd Ker(T ) and Rng(T ), and give a ge-
ometrical description of each. Also, ﬁnd dim[Ker(T )] and
dim[Rng(T )], and verify Theorem 6.3.8.
3. T : R2 →R2 deﬁned by T (x) = Ax, where
A =
( 3 6
1 2
)
.
4. T : R3 →R3 deﬁned by T (x) = Ax, where
A =
⎡
⎣
1 −1 0
0
1 2
2 −1 1
⎤
⎦.
5. T : R3 →R3 deﬁned by T (x) = Ax, where
A =
⎡
⎣
1 −2
1
2 −3 −1
5 −8 −1
⎤
⎦.
6. T : R3 →R2 deﬁned by T (x) = Ax, where
A =
(
1 −1
2
−3
3 −6
)
.

406
CHAPTER 6
Linear Transformations
7. T : R3 →R2 deﬁned by T (x) = Ax, where
A =
! 1 3 2
2 6 5
"
.
For Problems 8–11, compute Ker(T ) and Rng(T ).
8. The linear transformation T deﬁned in Problem 27 in
Section 6.1.
9. The linear transformation T deﬁned in Problem 28 in
Section 6.1.
10. The linear transformation T deﬁned in Problem 29 in
Section 6.1.
11. The linear transformation T deﬁned in Problem 30 in
Section 6.1.
12. Consider the linear transformation T : R3 →R de-
ﬁned by
T (v) = ⟨u, v⟩,
where u is a ﬁxed nonzero vector in R3.
(a) Find Ker(T ) and dim[Ker(T )], and interpret this
geometrically.
(b) Find Rng(T ) and dim[Rng(T )].
13. Consider the linear transformation S : Mn(R) →
Mn(R) deﬁned by S(A) = A + AT , where A is a
ﬁxed n × n matrix.
(a) Find
Ker(S)
and
describe
it.
What
is
dim[Ker(S)]?
(b) In the particular case when A is a 2 × 2 matrix,
determine a basis for Ker(S), and hence, ﬁnd its
dimension.
14. Consider the linear transformation T : Mn(R) →
Mn(R) deﬁned by
T (A) = AB −B A,
where B is a ﬁxed n × n matrix. Describe Ker(T ) in
words.
15. Consider the linear transformation T : P2(R) →
P2(R) deﬁned by
T (ax2 + bx + c) = ax2 + (a + 2b + c)x + (3a −2b −c),
where a, b, and c are arbitrary constants.
(a) Show that Ker(T ) consists of all polynomials of
the form b(x −2), and hence, ﬁnd its dimension.
(b) Find Rng(T ) and its dimension.
16. Consider the linear transformation T : P2(R) →
P1(R) deﬁned by
T (ax2 + bx + c) = (a + b) + (b −c)x,
where a, b, and c are arbitrary real numbers. Deter-
mine Ker(T ), Rng(T ), and their dimensions.
17. Consider the linear transformation T : P1(R) →
P2(R) deﬁned by
T (ax + b) = (b −a) + (2b −3a)x + bx2.
Determine Ker(T ), Rng(T ), and their dimensions.
18. Consider the linear transformation T : M2(R) →
P2(R) deﬁned by
T
#! a
b
c
d
"$
= (a −b + d) + (−a + b −d)x2.
Determine Ker(T ), Rng(T ), and their dimensions.
19. Consider the linear transformation T : R2 →M23(R)
deﬁned by
T (x, y) =
! −x −y
0
2x + 2y
0
3x + 3y
−9x −9y
"
.
Determine Ker(T ), Rng(T ), and their dimensions.
20. Consider the linear transformation T : M24(R) →
M42(R) deﬁned by
T (A) = AT .
Determine Ker(T ), Rng(T ), and their dimensions.
21. Let {v1, v2, v3} and {w1, w2} be bases for real vector
spaces V and W, respectively, and let T : V →W be
the linear transformation satisfying
T (v1) = 2w1 −w2,
T (v2) = w1 −w2,
T (v3) = w1 + 2w2.
Find Ker(T ), Rng(T ), and their dimensions.

6.4
Additional Properties of Linear Transformations 407
22. (a) Let T : V →W be a linear transformation, and
suppose that dim[V ] = n. If Ker(T ) = {0} and
{v1, v2, . . . , vn} is any basis for V , prove that
{T (v1), T (v2), . . . , T (vn)}
is a basis for Rng(T ). (This ﬁlls in the missing
details in the proof of Theorem 6.3.8.)
(b) Show that the conclusion from part (a) fails to
hold if Ker(T ) ̸= {0}.
23. Let T : V →W and S : V →W be linear trans-
formations, and assume that {v1, v2, . . . , vk} spans V .
Prove that if T (vi) = S(vi) for each i = 1, 2, . . . , k,
then T = S; that is, T (v) = S(v) for each v ∈V .
24. Let V be a vector space with basis {v1, v2, . . . , vk} and
suppose T : V →W is a linear transformation such
that T (vi) = 0 for each i = 1, 2, . . . , k. Prove that T
is the zero transformation; that is, T (v) = 0 for each
v ∈V .
6.4
Additional Properties of Linear Transformations
One of the aims of this section is to establish that all real vector spaces of (ﬁnite)
dimension n are essentially the same as Rn. In order to do so, we need to consider the
composition of linear transformations.
DEFINITION
6.4.1
Let T1 : U →V and T2 : V →W be two linear transformations.3 We deﬁne the
composition, or product, T2T1 : U →W by
(T2T1)(u) = T2(T1(u))
for all u ∈U.
The composition is illustrated in Figure 6.4.1. Our ﬁrst result establishes that T2T1
is a linear transformation.
u
U
V
W
T2T1
T1
T2
T1(u)
(T2T1)(u) 5T2(T1(u))
Figure 6.4.1: The composition of two linear transformations.
Theorem 6.4.2
Let T1 : U →V and T2 : V →W be linear transformations. Then T2T1 : U →W is a
linear transformation.
Proof Let u1, u2 be arbitrary vectors in U, and let c be a scalar. We must prove that
(T2T1)(u1 + u2) = (T2T1)(u1) + (T2T1)(u2)
(6.4.1)
3We assume that U, V and W are either all real vector spaces or all complex vector spaces.

408
CHAPTER 6
Linear Transformations
and that
(T2T1)(cu1) = c(T2T1)(u1).
(6.4.2)
Consider ﬁrst Equation (6.4.1). We have
(T2T1)(u1 + u2) = T2(T1(u1 + u2))
(deﬁnition of T2T1)
= T2(T1(u1) + T1(u2))
(using the linearity of T1)
= T2(T1(u1)) + T2(T1(u2))
(using the linearity of T2)
= (T2T1)(u1) + (T2T1)(u2),
(deﬁnition of T2T1)
so that (6.4.1) is satisﬁed. We leave the proof of (6.4.2) as an exercise (Problem 39).
ObserveinDeﬁnition6.4.1thattheoutputsfromthelineartransformation T1 become
the inputs for the linear transformation T2 when computing the composition T2T1. For this
reason, we observe that forming the composition T1T2 may not even be possible, since
the outputs of T2 may not be acceptable inputs for T1. Even when both compositions T1T2
and T2T1 make sense, they may not be the same linear transformation. These observations
will be apparent in the examples that follow.
Example 6.4.3
Let T1 : Rn →Rm and T2 : Rm →Rp be linear transformations with matrices A and
B, respectively. Determine the linear transformation T2T1 : Rn →Rp.
Solution:
From Deﬁnition 6.4.1, for any vector x in Rn, we have
(T2T1)(x) = T2(T1(x)) = T2(Ax) = B(Ax) = (B A)x.
Consequently, T2T1 is the linear transformation with matrix B A. Note that A is an m ×n
matrix and B is a p × m matrix, so that the matrix product B A is deﬁned, with size
p × n.
□
Example 6.4.4
Let T1 : Mn(R) →Mn(R) and T2 : Mn(R) →R be the linear transformations deﬁned
by
T1(A) = A + AT ,
T2(A) = tr(A).
Determine T2T1.
Solution:
In this case, T2T1 : Mn(R) →R is deﬁned by
(T2T1)(A) = T2(T1(A)) = T2(A + AT ) = tr(A + AT ).
This can be written in the equivalent form
(T2T1)(A) = 2tr(A).
As a speciﬁc example, consider the matrix A =
(
2 −1
−3
6
)
. We have
T1(A) = A + AT =
(
4 −4
−4
12
)
so that
T2(T1(A)) = tr
/(
4 −4
−4
12
)0
= 16.
□

6.4
Additional Properties of Linear Transformations 409
Example 6.4.5
Let T1 : M2(R) →P2(R) be deﬁned by
T1
/( a
b
c
d
)0
= (a + b) + (b + c)x + (c + d)x2
and let T2 : P2(R) →P1(R) be deﬁned by
T2(p(x)) = p′(x).
Determine Ker(T2T1), Rng(T2T1), and their dimensions.
Solution:
In this case, T2T1 : M2(R) →P1(R) is deﬁned by
(T2T1)
/( a
b
c
d
)0
= T2((a + b) + (b + c)x + (c + d)x2) = (b + c) + 2(c + d)x.
To compute Ker(T2T1), we must set (T2T1)
/( a
b
c
d
)0
= 0. That is,
(b + c) + (c + d)x = 0,
which implies that
b + c = 0
and
c + d = 0.
Solving this system, we set d = t, then c = −t, b = t, and a = s, where s and t are free
variables. Hence,
Ker(T2T1) =
-(
s
t
−t
t
)
: s, t ∈R
.
.
Therefore, a basis for Ker(T2T1) is given by
-( 1 0
0 0
)
,
(
0 1
−1 1
).
.
Hence, dim[Ker(T2T1)] = 2.
For Rng(T2T1), we can appeal to the Rank-Nullity Theorem (Theorem 6.3.8) to
observe that since M2(R) is four-dimensional, we have
dim[Rng(T2T1)] = 4 −dim[Ker(T2T1)] = 4 −2 = 2.
However, since P1(R) is 2-dimensional and contains Rng(T2T1) as a subspace, we con-
clude that Rng(T2T1) = P1(R). Hence, any basis for P1(R), such as {1, x}, is a suitable
basis for Rng(T2T1).
□
We now extend the deﬁnitions of one-to-one and onto, which should be familiar in
the case of a function of a single variable f : R →R, to the case of arbitrary linear
transformations.
DEFINITION
6.4.6
A linear transformation T : V →W is said to be
1. one-to-one if distinct elements in V are mapped via T to distinct elements in
W; that is, whenever v1 ̸= v2 in V , we have T (v1) ̸= T (v2).
2. onto if the range of T is the whole of W; that is, if every w ∈W is the image
under T of at least one vector v ∈V .

410
CHAPTER 6
Linear Transformations
Example 6.4.7
The linear transformation T : M2(R) →R deﬁned by T (A) = tr(A) is not one-to-one,
since it is possible to ﬁnd two distinct matrices A and B for which T (A) = T (B). For
instance, we can take A =
( 0 1
0 0
)
and B =
( 0 2
0 0
)
, but tr(A) = tr(B) = 0. Many
other choices of A and B can also be used to illustrate this.
On the other hand, T is onto, since every real number w ∈R is the image of some
matrix in M2(R); for example, take A =
( w
0
0
0
)
. Then T (A) = w, as desired. Again,
many other choices for A are possible here.
□
The following theorem can be helpful in determining whether a given linear trans-
formation is one-to-one.
Theorem 6.4.8
Let T : V →W be a linear transformation. Then T is one-to-one if and only if
Ker(T ) = {0}.
Proof Since T is a linear transformation, we have T (0) = 0. Thus, if T is one-to-one,
there can be no other vector v in V satisfying T (v) = 0, and so, Ker(T ) = {0}.
Conversely, suppose that Ker(T ) = {0}. If v1 ̸= v2, then v1 −v2 ̸= 0, and therefore
since Ker(T ) = {0}, T (v1 −v2) ̸= 0. Hence, by the linearity of T , T (v1) −T (v2) ̸= 0,
or equivalently, T (v1) ̸= T (v2). Thus, if Ker(T ) = {0}, then T is one-to-one.
For instance, our calculations in Example 6.3.7 showed that Ker(T ) = {0}, so the
linear transformation T in that example is one-to-one.
We now have the following characterization of one-to-one and onto in terms of the
kernel and range of T :
The linear transformation T : V →W is
1. one-to-one if and only if Ker(T ) = {0}.
2. onto if and only if Rng(T ) = W.
Example 6.4.9
Since the linear transformation T2T1 in Example 6.4.5 has Ker(T2T1) ̸= {0} and
Rng(T2T1) = P1(R), we conclude that T2T1 is onto, but not one-to-one.
□
Example 6.4.10
If T : R7 →R5 is a linear transformation and Ker(T ) is two-dimensional, then by
Theorem 6.3.8, we have that Rng(T ) is ﬁve-dimensional, and hence Rng(T ) = W. That
is, T is onto.
□
Example 6.4.11
If T : R3 →R6 is a linear transformation and Rng(T ) is three-dimensional, then by
Theorem 6.3.8, Ker(T ) = {0}, so T must be one-to-one.
□
Example 6.4.12
Consider the linear transformation T : P2(R) →P2(R) deﬁned by
T (a + bx + cx2) = (2a −b + c) + (b −2a)x + cx2.
Determine whether T is one-to-one, onto, both, or neither.

6.4
Additional Properties of Linear Transformations 411
Solution:
To determine whether T is one-to-one, we ﬁnd Ker(T ). For the given
transformation, we have
Ker(T ) = {p ∈P2(R) : T (p) = 0}
= {a + bx + cx2 : T (a + bx + cx2) = 0 for all x}
= {a + bx + cx2 : (2a −b + c) + (b −2a)x + cx2 = 0 for all x}.
But,
(2a −b + c) + (b −2a)x + cx2 = 0,
for all real x, if and only if
c = 0,
b −2a = 0,
2a −b + c = 0.
These equations are satisﬁed if and only if
c = 0,
b = 2a,
so that
Ker(T ) = {a + bx + cx2 : c = 0, b = 2a}
= {a(1 + 2x) : a ∈R}.
Since the kernel of T contains nonzero vectors, Theorem 6.4.8 implies that T is not one-
to-one. To determine whether T is onto, we can check whether or not Rng(T ) = P2(R).
However, there is a shorter method, using Theorem 6.3.8. Since the vectors in Ker(T )
consist of all scalar multiples of the nonzero polynomial p1(x) = 1 + 2x, Ker(T ) =
span{1 + 2x}, and so dim[Ker(T )] = 1. Further, since dim[P2(R)] = 3, from Theorem
6.3.8, we have
1 + dim[Rng(T )] = 3,
which implies that
dim[Rng(T )] = 2.
Thus, Rng(T ) is a two-dimensional subspace of the three-dimensional vector space
P2(R), and so Rng(T ) ̸= P2(R). Hence, T is not onto. Thus, T is neither one-to-one
nor onto.
□
Let us explore the relationship between the one-to-one and onto properties of a linear
transformation T : V →W and the dimensions of V and W a bit further. We have the
following useful result.
Corollary 6.4.13
Let T : V →W be a linear transformation, and assume that V and W are both ﬁnite-
dimensional. Then
1. If T is one-to-one, then dim[V ] ≤dim[W].
2. If T is onto, then dim[V ] ≥dim[W].
3. If T is one-to-one and onto, then dim[V ] = dim[W].

412
CHAPTER 6
Linear Transformations
Proof We appeal once more to Theorem 6.3.8. To prove (1), assume that T : V →W
is one-to-one. Then by Theorem 6.4.8, we see that dim[Ker(T )] = 0. Thus, Theorem
6.3.8 implies that dim[V ] = dim[Rng(T )]. But Rng(T ) is a subspace of W, and so by
Corollary 4.6.14, dim[Rng(T )] ≤dim[W]. Hence, dim[V ] ≤dim[W].
To prove (2), assume that T is onto. Then Rng(T ) = W, so Theorem 6.3.8 can be
rewritten as
dim[V ] = dim[Ker(T )] + dim[W],
which immediately shows that dim[W] ≤dim[V ].
Finally, (3) is an immediate consequence of (1) and (2).
In many situations, Corollary 6.4.13 can be used in contrapositive form to eliminate
the possibility of a one-to-one or onto linear transformation from V to W. For instance, if
dim[V ] < dim[W], then part (2) of Corollary 6.4.13 implies at once that there can be no
onto linear transformation from V to W. For a speciﬁc example, no linear transformation
T : M2(R) →R5 can be onto, since this would imply that dim[M2(R)] ≥dim[R5],
a contradiction.
In a similar way, if dim[V ] > dim[W], then part (1) of Corollary 6.4.13 implies at
once that there can be no one-to-one linear transformation from V to W. For a speciﬁc
example, no linear transformation T : M3(R) →R5 can be one-to-one, since this would
imply that dim[M3(R)] ≤dim[R5], a contradiction. Likewise, we could have used this
type of reasoning to arrive at the conclusion that the linear transformation in Example
6.4.7 cannot be one-to-one.
From part (3) of Corollary 6.4.13, it follows that a necessary condition on T : V →
W for T to be both one-to-one and onto is that dim[V ] = dim[W]. The reader must be
careful not to take this conclusion too far. In particular, if two vector spaces V and W do
have the same dimension, this does not guarantee that any given linear transformation
T : V →W is both one-to-one and onto. This point is well-illustrated by Example
6.4.12. However, if we know in advance that two ﬁnite-dimensional vector spaces V
and W have the same dimension, then we can draw the following conclusion regarding
a linear transformation between them.
Proposition 6.4.14
Assume V and W are ﬁnite-dimensional vector spaces with dim[V ] = dim[W]. If
T : V →W is a linear transformation, then T is one-to-one if and only if T is onto.
The proof of Proposition 6.4.14 uses Theorem 6.3.8 in like manner to that done
in the proof of Corollary 6.4.13 and is left as an exercise (Problem 40). The utility of
Proposition 6.4.14 is that, for a linear transformation T between vector spaces V and W
of the same dimension, if we show that T is one-to-one, it automatically follows that T
is also onto. Alternatively, if we show that T is onto, then it automatically follows that
T is one-to-one. In this way, only one of the two properties, one-to-one or onto, needs
to be explicitly veriﬁed.
The following table summarizes our discussion above.
Suppose V and W are vector spaces of dimensions n and m, respectively,
and suppose T : V →W is a linear transformation.
Property of T
n > m
n < m
n = m
T is onto
Maybe
No
Maybe
T is one-to-one
No
Maybe
Maybe
If T : V →W is both one-to-one and onto, then for each w ∈W, there is a unique
v ∈V such that T (v) = w. We can therefore deﬁne a mapping T −1 : W →V by
T −1(w) = v if and only if w = T (v).

6.4
Additional Properties of Linear Transformations 413
This mapping satisﬁes the basic properties of an inverse; namely,
T −1(T (v)) = v for all v ∈V
and
T (T −1(w)) = w for all w ∈W.
We call T −1 the inverse transformation to T . Again we stress that T −1 exists if and
only if T is both one-to-one and onto, in which case we call T an invertible linear
transformation. We leave it as an exercise to verify that T −1 is a linear transformation
(Problem 41).
DEFINITION
6.4.15
Let T : V →W be a linear transformation. If T is both one-to-one and onto, then
the linear transformation T −1 : W →V deﬁned by
T −1(w) = v if and only if w = T (v)
is called the inverse transformation to T .
For linear transformations T : Rn →Rn, the following theorem characterizes the
existence of an inverse transformation.
Theorem 6.4.16
Let T : Rn →Rn be a linear transformation with matrix A. Then T −1 exists if and only
if det(A) ̸= 0. Furthermore, T −1 : Rn →Rn is a linear transformation with matrix
A−1.
Proof We have
Ker(T ) = {x ∈Rn : Ax = 0}.
Hence, T is one-to-one if and only if the homogeneous linear system Ax = 0 has only
the trivial solution. But this is true if and only if det(A) ̸= 0. Furthermore,
Rng(T ) = {Ax : x ∈Rn} = colspace(A).
Consequently,
T is onto ⇐⇒colspace(A) = Rn
⇐⇒the column vectors of A span Rn
⇐⇒the columns of A are linearly independent
⇐⇒det(A) ̸= 0,
where we have used the Invertible Matrix Theorem (Theorem 4.10.1) for the last two
equivalences. Hence, T is both one-to-one and onto if and only if det(A) ̸= 0. That is,
T −1 exists if and only if det(A) ̸= 0.
Finally, if det(A) ̸= 0, then A−1 exists, so that
T (x) = y ⇐⇒Ax = y ⇐⇒x = A−1y.
Consequently, the inverse transformation is
T −1(y) = A−1y,
from which it follows that T −1 is itself a linear transformation with matrix A−1.

414
CHAPTER 6
Linear Transformations
Example 6.4.17
If T : R3 →R3 has matrix A =
⎡
⎣
−1
6
3
0
3 −3
1 −1
2
⎤
⎦, show that T −1 exists and ﬁnd it.
Solution:
It is easily shown that det(A) = −30 ̸= 0, so that A is invertible. Conse-
quently, T −1 exists and is given by
T −1(x) = A−1x.
Using the Gauss-Jordan technique or the adjoint method, it is found that
A−1 =
⎡
⎢⎢⎣
−1
10
1
2
9
10
1
10
1
6
1
10
1
10
−1
6
1
10
⎤
⎥⎥⎦.
Hence,
T −1(x1, x2, x3)=
/
−1
10 x1 + 1
2 x2 + 9
10 x3, 1
10 x1 + 1
6x2 + 1
10 x3, 1
10 x1 −1
6x2 + 1
10 x3
0
.
□
Isomorphism
Now let V be a (real) vector space of ﬁnite dimension n, and let {v1, v2, . . . , vn} be a
basis for V . We deﬁne a mapping T : Rn →V via
T (c1, c2, . . . , cn) = c1v1 + c2v2 + · · · + cnvn,
where c1, c2, . . . , cn ∈R. It is easy to check that T is a linear transformation. Moreover,
since {v1, v2, . . . , vn} is linearly independent, it follows that Ker(T ) = {0}, so that T is
one-to-one. Furthermore, Rng(T ) = span{v1, v2, . . . , vn} = V , so that T is also onto.
Since T is both one-to-one and onto, every vector in V occurs as the image of exactly one
vector in Rn. This transformation has therefore matched up vectors in Rn with vectors
in V in such a manner that linear combinations are preserved under the mapping. Such
a transformation is called an isomorphism between Rn and V . We say that Rn and V
are isomorphic.
DEFINITION
6.4.18
Let V and W be vector spaces.4 If there exists a linear transformation T : V →W
that is both one-to-one and onto, we call T an isomorphism, and we say that V and
W are isomorphic vector spaces, written V ∼= W.
Remarks
1. If V and W are vector spaces with dim[V ] ̸= dim[W], then V and W cannot be
isomorphic, by Corollary 6.4.13.
2. Our discussion above proves that all n-dimensional (real) vector spaces are iso-
morphic to Rn. Hence, if V and W are vector spaces with dim[V ] = dim[W] = n,
then
V ∼= Rn ∼= W.
4We assume that V and W are either both real vector spaces or both complex vector spaces.

6.4
Additional Properties of Linear Transformations 415
Thus, all (real) n-dimensional vector spaces are isomorphic to one another. Hence,
in studying properties of the vector space Rn, we are really studying all (real) vector
spaces
of
dimension
n.
This
illustrates
the
importance
of
the
vector
space Rn.
Example 6.4.19
Determine an isomorphism T : R3 →P2(R).
Solution:
An arbitrary vector in P2(R) can be expressed relative to the standard
basis as
a0 + a1x + a2x2.
Consequently, an isomorphism between R3 and P2(R) is T : R3 →P2(R) deﬁned by
T (a0, a1, a2) = a0 + a1x + a2x2.
It is straightforward to verify that T is one-to-one, onto, and a linear transformation.
□
Example 6.4.20
Determine an isomorphism T : R4 →M2(R).
Solution:
An arbitrary vector A =
( a
b
c
d
)
in M2(R) can be written relative to the
standard basis as
A = a
( 1 0
0 0
)
+ b
( 0 1
0 0
)
+ c
( 0 0
1 0
)
+ d
( 0 0
0 1
)
.
Hence, we can deﬁne an isomorphism T : R4 →M2(R) by
T (a, b, c, d) =
( a
b
c
d
)
.
□
Remark
In the preceding two examples, note that the given isomorphism T is not
unique. For instance, in Example 6.4.20, we could also deﬁne an isomorphism S : R4 →
M2(R) via
S(a, b, c, d) =
( d
c
b
a
)
.
In general, isomorphisms are not unique.
Example 6.4.21
For what positive integer n is the vector space V of 3 × 3 symmetric matrices over R
isomorphic to Rn ?
Solution:
A typical element of V can be written in the form
⎡
⎣
a
b
c
b
d
e
c
e
f
⎤
⎦, so that we
can easily construct an isomorphism T : V →R6 via
T
⎛
⎝
⎡
⎣
a
b
c
b
d
e
c
e
f
⎤
⎦
⎞
⎠= (a, b, c, d, e, f ).
The reader can verify that T is indeed linear. Moreover, T is clearly one-to-one and
onto, hence an isomorphism. Therefore, we conclude that V is isomorphic to Rn
for n = 6.
□

416
CHAPTER 6
Linear Transformations
Finally in this section, we can extend our list of criteria for an n × n matrix A to be
invertible as follows.
Theorem 6.4.22
Let A be an n × n matrix with real elements, and let T : Rn →Rn be the matrix
transformation deﬁned by T (x) = Ax. The following conditions are equivalent:
(a) A is invertible.
(q) T is one-to-one.
(r) T is onto.
(s) T is an isomorphism.
Proof By the Invertible Matrix Theorem, A is invertible if and only if nullspace(A) =
{0}. According to Equation (6.3.1), this is equivalent to the statement that Ker(T ) = {0},
and Theorem 6.4.8 shows that this is equivalent to the statement that T is one-to-one.
Hence, (a) and (q) are equivalent. Now (q) and (r) are equivalent by Proposition 6.4.14,
and (q) and (r) together are equivalent to (s) by the deﬁnition of an isomorphism.
Exercises for 6.4
Key Terms
Composition of linear transformations, One-to-one and onto
properties, Inverse transformation, Invertible linear transfor-
mation, Isomorphism, Isomorphic vector spaces.
Skills
• Be able to determine the composition of two (or more)
given linear transformations.
• Be able to determine whether a given linear transfor-
mation T : V →W is one-to-one, onto, both, or
neither.
• Be able to use the one-to-one and onto properties of
a linear transformation T : V →W to draw con-
clusions about the relationship between dim[V ] and
dim[W].
• Conversely, given information about the dimensions
of vector spaces V and W, decide quickly whether it
is possible for a linear transformation T : V →W to
be one-to-one and/or onto.
• Be able to determine whether a given linear transfor-
mation T : V →W has an inverse transformation,
and if so, be able to ﬁnd it.
• Be able to determine whether two vector spaces V and
W are isomorphic, and if so, be able to construct an
isomorphism T : V →W.
True-False Review
For Questions (a)–(l), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) There is no one-to-one linear transformation T
:
P3(R) →M32(R).
(b) If V denotes the set of all 3 × 3 upper triangular ma-
trices, then V ∼= M32(R).
(c) If T1 : V1 →V2 and T2 : V2 →V3, then Ker(T1) is a
subspace of Ker(T2T1).
(d) If T : M2(R) →P2(R) is a linear transformation
and Ker(T ) is one-dimensional, then T is onto but not
one-to-one.
(e) There is no onto linear transformation T : M2(R) →
P4(R).
(f) If T1 : V1 →V2 and T2 : V2 →V3 are both one-to-
one, then so is the composition T2T1 : V1 →V3.
(g) The linear transformation T : C1[a, b] →C0[a, b]
given by T ( f ) = f ′ is both one-to-one and onto.
(h) If T : P3(R) →M23(R) is a linear transformation and
Rng(T ) is four-dimensional, then T is one-to-one but
not onto.

6.4
Additional Properties of Linear Transformations 417
(i) Thereisnoisomorphism T : Rn →Rm unless m = n.
(j) Every real vector space is isomorphic to Rn for some
positive integer n.
(k) If T1 : V1 →V2 and T2 : V2 →V3 are linear trans-
formations such that T2 is onto, then T2T1 is onto.
(l) If T : R8 →R3 is an onto linear transformation, then
Ker(T ) is ﬁve-dimensional.
Problems
1. Let T1 : R2 →R2 and T2 : R2 →R be the linear
transformations with matrices
A =
( 1 −1
3
2
)
,
B = [−1 1],
respectively. Find T2T1. Does T1T2 exist? Explain.
2. Let T1 : R2 →R2 and T2 : R2 →R2 be the linear
transformations with matrices
A =
( −1 2
3 1
)
,
B =
(
1 5
−2 0
)
,
respectively. Find T1T2 and T2T1. Does T1T2 = T2T1?
3. Let T1 : R2 →R3 and T2 : R3 →R2 be the linear
transformations with matrices
A =
⎡
⎣
2 −1
0
1
1
1
⎤
⎦,
B =
( 0 −4 3
1
0 1
)
,
respectively. Find T1T2, Ker(T1T2), Rng(T1T2), T2T1,
Ker(T2T1), and Rng(T2T1).
4. Let T1 : R2 →R2 and T2 : R2 →R2 be the linear
transformations with matrices
A =
( 1 −1
2 −2
)
,
B =
( 2
1
3 −1
)
,
respectively. Find Ker(T1), Ker(T2), Ker(T1T2), and
Ker(T2T1).
5. Let T1 : Mn(R) →Mn(R) and T2 : Mn(R) →
Mn(R) be the linear transformations deﬁned by
T1(A) = A −AT and T2(A) = A + AT . Show that
T2T1 is the zero transformation.
6. Deﬁne T1
:
C1[a, b]
→
C0[a, b] and T2
:
C0[a, b] →C1[a, b] by
T1( f ) = f ′,
[T2( f )](x) =
, x
a
f (t) dt,
a ≤x ≤b.
(a) If f (x) = sin(x −a), ﬁnd
[T1( f )](x)
and
[T2( f )](x),
and show that, for the given function,
[T1T2]( f ) = [T2T1]( f ) = f.
(b) Show that for general functions f and g,
[T1T2]( f ) = f, {[T2T1](g)}(x) = g(x) −g(a).
7. Let {v1, v2} be a basis for the vector space V , and sup-
pose that T1 : V →V and T2 : V →V are the linear
transformations satisfying
T1(v1) = v1 −v2,
T1(v2) = 2v1 + v2
T2(v1) = v1 + 2v2,
T2(v2) = 3v1 −v2.
Determine (T2T1)(v) for an arbitrary vector v in V .
8. Repeat Problem 7 under the assumption
T1(v1) = 3v1 + v2,
T1(v2) = 0
T2(v1) = −5v2,
T2(v2) = −v1 + 6v2.
9. Isthelineartransformation T2T1 inExample6.4.4one-
to-one, onto, both, or neither? Explain your answer.
For Problems 10–14, ﬁnd Ker(T ) and Rng(T ), and hence,
determine whether the given transformation is one-to-one,
onto, both, or neither. If T −1 exists, ﬁnd it.
10. T (x) = Ax, where A =
⎡
⎣
1
2
3
⎤
⎦.
11. T (x) = Ax, where A =
( 4 2
1 3
)
.
12. T (x) = Ax, where A =
(
1
2
−2 −4
)
.
13. T (x) = Ax, where A =
( 1 2 −1
2 5
1
)
.
14. T (x) = Ax, where A =
⎡
⎢⎢⎣
0 1 2
3 4 5
5 4 3
2 1 0
⎤
⎥⎥⎦.

418
CHAPTER 6
Linear Transformations
15. Suppose T : R3 →R2 is a linear transformation such
that T (1, 0, 0) = (4, 5), T (0, 1, 0) = (−1, 1), and
T (2, 1, −3) = (7, −1).
(a) Find the matrix of T .
(b) Is T one-to-one, onto, both, or neither? Explain
brieﬂy.
16. Let V be a vector space and deﬁne T : V →V by
T (x) = λx, where λ is a nonzero scalar. Show that T
is a linear transformation that is one-to-one and onto,
and ﬁnd T −1.
17. Deﬁne T : P1(R) →P1(R) by
T (ax + b) = (2b −a)x + (b + a).
Show that T is both one-to-one and onto, and ﬁnd T −1.
18. Deﬁne T : P2(R) →P1(R) by
T (ax2 + bx + c) = (a −b)x + c.
Determine whether T is one-to-one, onto, both, or nei-
ther. Find T −1 or explain why it does not exist.
19. Deﬁne T : P2(R) →R2 by
T (ax2 + bx + c) = (a −3b + 2c, b −c).
Determine whether T is one-to-one, onto, both, or nei-
ther. Find T −1 or explain why it does not exist.
20. Let V denote the vector space of 2 × 2 symmetric
matrices and deﬁne T : V →P2(R) by
T
/( a
b
b
c
)0
= ax2 + bx + c.
Determine whether T is one-to-one, onto, both, or nei-
ther. Find T −1 or explain why it does not exist.
21. Deﬁne T : R3 →M2(R) by
T (a, b, c) =
( −a + 3c a −b −c
2a + b
0
)
.
Determine whether T is one-to-one, onto, both, or nei-
ther. Find T −1 or explain why it does not exist.
22. Deﬁne T : M2(R) →P3(R) by
T
/( a
b
c
d
)0
= (a −b + d) + (2a + b)x + cx2
+ (4a −b + 2d)x3.
Determine whether T is one-to-one, onto, both, or nei-
ther. Find T −1 or explain why it does not exist.
23. Let {v1, v2} be a basis for the vector space V , and sup-
pose that T : V →V is a linear transformation. If
T (v1) = v1 + 2v2 and T (v2) = 2v1 −3v2, determine
whether T is one-to-one, onto, both, or neither. Find
T −1 or explain why it does not exist.
24. Let v1 and v2 be a basis for the vector space V , and
suppose that T1 : V →V and T2 : V →V are the
linear transformations satisfying
T1(v1) = v1 + v2,
T1(v2) = v1 −v2,
T2(v1) = 1
2(v1 + v2),
T2(v2) = 1
2(v1 −v2).
Find (T1T2)(v) and (T2T1)(v) for an arbitrary vector
in V and show that T2 = T −1
1
.
25. Determine an isomorphism between R2 and the vector
space P1(R).
26. Determine an isomorphism between R3 and the sub-
space of M2(R) consisting of all upper triangular
matrices.
27. Determine an isomorphism between R and the sub-
space of M2(R) consisting of all skew-symmetric
matrices.
28. Determine an isomorphism between R3 and the sub-
space of M2(R) consisting of all symmetric matrices.
29. Let V denote the vector space of all 4 × 4 upper trian-
gular matrices. Find n such that V ∼= Rn, and construct
an isomorphism.
30. Let V denote the subspace of P8(R) consisting of all
polynomials whose coefﬁcients of odd powers of x are
all zero. Find n such that V ∼= Rn, and construct an
isomorphism.
31. Let V denote the vector space of all 3 × 3 skew-
symmetric matrices over R. For what positive integer
n is V isomorphic to Rn. Construct an isomorphism.
For Problems 32–35, an invertible linear transformation
Rn →Rn is given. Find a formula for the inverse linear
transformation.
32. T1 : R2 →R2 deﬁned by T1(x) = Ax, where
A =
( −4 −1
2
2
)
.

6.5
The Matrix of a Linear Transformation 419
33. T2 : R2 →R2 deﬁned by T2(x) = Ax, where
A =
( 1 1
2 3
)
.
34. T3 : R3 →R3 deﬁned by T3(x) = Ax, where
A =
⎡
⎣
3 5 1
1 2 1
2 6 7
⎤
⎦.
35. T4 : R3 →R3 deﬁned by T4(x) = Ax, where
A =
⎡
⎣
1 1
3
0 1
2
3 5 −1
⎤
⎦.
36. Referring to Problems 32–33, compute the matrix of
T2T1 and the matrix of T1T2.
37. Referring to Problems 34–35, compute the matrix of
T4T3 and the matrix of T3T4.
38. Let T1 : V1 →V2 and T2 : V2 →V3 be linear trans-
formations.
(a) Prove that if T1 and T2 are both one-to-one, then
so is T2T1 : V1 →V3.
(b) Prove that if T1 and T2 are both onto, then so is
T2T1 : V1 →V3.
(c) Prove that if T1 and T2 are both isomorphisms,
then so is T2T1 : V1 →V3.
39. Complete the proof of Theorem 6.4.2 by verifying
Equation (6.4.2).
40. Prove Proposition 6.4.14.
41. If T : V →W is an invertible linear transformation
(that is, T −1 exists), show that T −1 : W →V is also
a linear transformation.
42. Prove that if T : V →V is a one-to-one linear trans-
formation, and V is ﬁnite-dimensional, then T −1 ex-
ists.
43. Prove that if T : V →W is a one-to-one linear trans-
formation and {v1, v2, . . . , vk} is a linearly indepen-
dent set of vectors in V , then
{T (v1), T (v2), . . . , T (vk)}
is a linearly independent set of vectors in W.
44. Suppose T : V →W is a linear transformation and
{w1, w2, . . . , wm} spans W. If there exist vectors
v1, v2, . . . , vm in V such that T (vi) = wi for each
i = 1, 2, . . . , m, prove that T is onto.
45. Prove that if T : V →W is a linear transformation
with dim[W] = n = dim[Rng(T )], then T is onto.
46. Let T1 : V →V and T2 : V →V be linear transfor-
mations and suppose that T2 is onto. If
(T1T2)(v) = v for all v in V,
prove that
(T2T1)(v) = v for all v in V.
47. Prove that if T : V →V is a linear transformation
such that T 2 = 0 (that is, T (T (v)) = 0 for all v ∈V ),
then Rng(T ) is a subspace of Ker(T ).
∗6.5
The Matrix of a Linear Transformation
In Section 6.1 we associated an m × n matrix with any linear transformation T : Rn →
Rm (see Deﬁnition 6.1.16). In an effort to generalize this, let V and W be vector spaces
with dim[V ] = n and dim[W] = m. If we ﬁx ordered bases B and C on V and
W, respectively, we will see that we can uniquely associate an m × n matrix to any
linear transformation T : V →W. All of the essential information about the linear
transformation T : V →W can be found in the associated matrix, and therefore all of
the ideas we have been developing for linear transformations between ﬁnite-dimensional
vector spaces can be expressed entirely in the language of matrices. Before we begin,
recall from Section 4.7 that the component vector of a vector v relative to an ordered
basis B is denoted [v]B. We proceed as follows.
∗This section can be omitted without loss of continuity.

420
CHAPTER 6
Linear Transformations
DEFINITION
6.5.1
Let V and W be vector spaces with ordered bases B = {v1, v2, . . . , vn} and C =
{w1, w2, . . . , wm}, respectively, and let T : V →W be a linear transformation. The
m × n matrix
[T]C
B =
(
[T (v1)]C, [T (v2)]C, . . . , [T (vn)]C
)
is called the matrix representation of T relative to the bases B and C. In case
V = W and B = C, we refer to [T ]B
B simply as the matrix representation of T
relative to the basis B.
Remarks
1. If V = Rn and W = Rm are each equipped with the standard bases, then [T ]C
B is
the same as the matrix of T introduced in Deﬁnition 6.1.16.
2. If V = W and T (v) = v for all v in V (i.e., T is the identity transformation), then
[T ]C
B is just the change-of-basis matrix from B to C described in Section 4.7.
Example 6.5.2
Recall the linear transformation T : P1(R) →P2(R) deﬁned by
T (a + bx) = (2a −3b) + (b −5a)x + (a + b)x2
in Example 6.3.7. Determine the matrix representation of T relative to the given ordered
bases B and C.
(a) B = {1, x} and C = {1, x, x2}.
(b) B = {1, x + 5} and C = {1, 1 + x, 1 + x2}.
Solution:
(a) We have T (1) = 2 −5x + x2 and T (x) = −3 + x + x2, so
[T (1)]C =
⎡
⎣
2
−5
1
⎤
⎦
and [T (x)]C =
⎡
⎣
−3
1
1
⎤
⎦.
Thus,
[T ]C
B =
⎡
⎣
2 −3
−5
1
1
1
⎤
⎦.
(b) We have T (1) = 2 −5x + x2 and T (x + 5) = 7 −24x + 6x2. Writing
2 −5x + x2 = a1(1) + a2(1 + x) + a3(1 + x2) = (a1 + a2 + a3) + a2x + a3x2,
we ﬁnd that a3 = 1, a2 = −5, and a1 = 6. Thus, we have [T(1)]C =
⎡
⎣
6
−5
1
⎤
⎦.
Next, we write
7 −24x + 6x2 = (b1 + b2 + b3) + b2x + b3x2,

6.5
The Matrix of a Linear Transformation 421
from which it follows that b3 = 6, b2 = −24, and b1 = 25. Thus, we have
[T (x + 5)]C =
⎡
⎣
25
−24
6
⎤
⎦. Thus,
[T]C
B =
⎡
⎣
6
25
−5 −24
1
6
⎤
⎦.
□
Example 6.5.3
Let T : R3 →P1(R) be deﬁned by
T (a, b, c) = (a −3c) + (2b + c)x.
Find [T ]C
B, where
B = {(1, −1, 0), (0, 1, 2), (−1, 2, 0)}
and
C = {1 + x, 2 + x}.
Solution:
The reader can verify these calculations:
[T(1, −1, 0)]C = [1 −2x]C =
( −5
3
)
,
[T(0, 1, 2)]C = [−6 + 4x]C =
(
14
−10
)
,
[T (−1, 2, 0)]C = [−1 + 4x]C =
(
9
−5
)
.
Thus,
[T ]C
B =
( −5
14
9
3 −10 −5
)
.
□
Given the matrix [T ]C
B representing T : V →W relative to the bases B and C, we
can completely recover the formula for T (v), for any vector v in V . The next theorem
gives us a way of doing this.
Theorem 6.5.4
Let V and W be vector spaces with ordered bases B and C, respectively. If T : V →W
is a linear transformation and v is any vector in V , then we have
[T(v)]C = [T ]C
B[v]B.
(6.5.1)
Proof Let B = {v1, v2, . . . , vn}, and consider a vector v in V . Writing v = a1v1 +
a2v2 + · · · + anvn, we have
[T ]C
B[v]B = a1[T (v1)]C + a2[T (v2)]C + · · · + an[T (vn)]C
= [T(a1v1 + a2v2 + · · · + anvn)]C = [T(v)]C,
where we have used the linearity of the components of a vector (see Lemma 4.7.5).
Example 6.5.5
Let us verify that the matrix [T]C
B found in Example 6.5.3 above does indeed contain all
of the information needed to compute T (v) for any vector v in V . Our ﬁrst step is to ﬁnd
the components of a general vector v = (a, b, c) relative to the basis B above. Writing
(a, b, c) = k1(1, −1, 0) + k2(0, 1, 2) + k3(−1, 2, 0),

422
CHAPTER 6
Linear Transformations
we have a system of linear equations for k1, k2, and k3:
k1 −k3 = a,
−k1 + k2 + 2k3 = b,
2k2 = c.
Solving this system of equations yields
k1 = 2a + b −c
2,
k2 = c
2,
k3 = a + b −c
2.
Thus, [v]B =
⎡
⎢⎣
2a + b −c
2
c
2
a + b −c
2
⎤
⎥⎦. Applying Theorem 6.5.4, we obtain the components of
T (v) relative to C:
[T (v)]C =
( −5
14
9
3 −10 −5
)
⎡
⎢⎣
2a + b −c
2
c
2
a + b −c
2
⎤
⎥⎦
=
5
−5(2a + b −c
2) + 7c + 9(a + b −c
2)
3(2a + b −c
2) −5c −5(a + b −c
2)
6
=
( −a + 4b + 5c
a −2b −4c
)
.
Hence, we have
T (v) = (−a + 4b + 5c)(1 + x) + (a −2b −4c)(2 + x) = (a −3c) + (2b + c)x,
so that we have recovered the formula for the linear transformation T .
□
We next consider the special case of a linear transformation from the n-dimensional
vector space V to itself, T : V →V . If we let B and C denote two ordered bases on
V , we pose the following natural question: Does there exist any relationship between
the matrices [T ]B
B and [T]C
C? This is really a question of change-of-basis for linear
transformations, and our next theorem provides the answer. Recall from Section 4.7 that
PC←B denotes the change-of-basis matrix from B to C.
Theorem 6.5.6
Let V be a vector space with ordered bases B and C. If T : V →V is a linear
transformation, then we have
[T]C
C = PC←B[T ]B
B PB←C.
Proof Let v be an arbitrary vector in V . On the one hand, we have
[T]C
C[v]C = [T (v)]C
by Theorem 6.5.4. On the other hand,
PC←B[T ]B
B PB←C[v]C = PC←B[T]B
B[v]B = PC←B[T(v)]B = [T (v)]C.
Thus, the two matrices [T ]C
C and PC←B[T ]B
B PB←C have the same effect on the com-
ponent vector [v]C. Since this holds for every v in V , the matrices must be identical.
We have seen that the matrix representation of a linear transformation between
vector spaces with ﬁxed ordered bases fosters a natural relationship between linear
transformations and matrices. The added perspective we have gained in this section
now enables us to better understand both of these types of objects. This is one of many
instances in mathematics in which one’s knowledge and understanding of two or more
topics can be considerably enhanced through an understanding of how those topics
are connected to one another. This remarkably important realization about the nature

6.5
The Matrix of a Linear Transformation 423
of mathematics cannot be too strongly emphasized. Therefore, for the remainder of
this section we look for additional insight about linear transformations that is available
through the use of matrix representations.
Linear Transformation Concepts in Terms
of Matrix Representations
In the preceding sections of this chapter, we have explored several key concepts pertain-
ing to linear transformations, including composition of linear transformations, kernels
and ranges of linear transformations, and one-to-one, onto, and invertible linear trans-
formations. Let us see what additional light can be brought to each of these concepts
through the machinery of matrix representations. We begin with composition of linear
transformations.
Composition of Linear Transformations: Let U, V , and W be vector spaces with
ordered bases A, B, and C. Let T1 : U →V and T2 : V →W be linear transformations.
The next theorem determines the matrix representation of T2T1 : U →W relative to the
bases A and C in terms of the matrix representations of T1 and T2 (with respect to the
appropriate bases) separately. The theorem is a generalization of Theorem 4.7.9.
Theorem 6.5.7
If U, V, and W are vector spaces with ordered bases A,B, and C, respectively, and
T1 : U →V and T2 : V →W are linear transformations, then we have
[T2T1]C
A = [T2]C
B[T1]B
A.
(6.5.2)
Proof It sufﬁces to show that when we premultiply any column vector of the form [u]A,
where u is a vector in U, by the matrices appearing on either side of (6.5.2), we obtain
the same result. Using Equation (6.5.1), we have
[T2]C
B[T1]B
A[u]A = [T2]C
B[T1(u)]B = [T2(T1(u))]C = [(T2T1)u]C = [T2T1]C
A[u]A,
as required.
Theorem 6.5.7 is the extremely powerful statement that multiplication of matrix
representations corresponds to composition of the associated linear transformations. At
ﬁrst glance, the procedure for multiplying matrices that was presented in Section 2.2
may have seemed somewhat unmotivated, but now we see that it is precisely that matrix
multiplication scheme that allows Theorem 6.5.7 to work. Therefore, we now have clear
evidence and motivation for why we multiply matrices the way we do.
Example 6.5.8
Deﬁne
T1 : R4 →P2(R)
via
T1(a, b, c, d) = (−a + 5d) + (b −c −d)x2
and
T2 : P2(R) →M2(R)
via
T2(a + bx + cx2) =
( −2a + 5c
0
2a + 2b −c b + 4c
)
.
Use the matrix representation of T2T1 relative to the standard bases on R4 and M2(R)
to ﬁnd the formula for the composition T2T1 : R4 →M2(R).

424
CHAPTER 6
Linear Transformations
Solution:
Let A, B, and C be the standard bases on the vector spaces R4, P2(R), and
M2(R), respectively. We have
[T2T1]C
A = [T2]C
B[T1]B
A
=
⎡
⎢⎢⎣
−2 0
5
0 0
0
2 2 −1
0 1
4
⎤
⎥⎥⎦
⎡
⎣
−1 0
0
5
0 0
0
0
0 1 −1 −1
⎤
⎦=
⎡
⎢⎢⎣
2
5 −5 −15
0
0
0
0
−2 −1
1
11
0
4 −4
−4
⎤
⎥⎥⎦.
From this matrix, we conclude that
(T2T1)(a, b, c, d) =
( 2a + 5b −5c −15d
0
−2a −b + c + 11d
4b −4c −4d
)
.
This can also be veriﬁed directly by composing the two linear transformations.
□
Kernels and Ranges of Linear Transformations: Because of the close relationship
between the matrix representation of a linear transformation and the linear transformation
itself, the following theorem is to be expected.
Theorem 6.5.9
Let T : V →W be a linear transformation, and let B and C be ordered bases for V and
W, respectively. Then
(a) For all v in V , v belongs to Ker(T ) if and only if [v]B belongs to nullspace([T ]C
B).
(b) For all w in W, w belongs to Rng(T ) if and only if [w]C belongs to colspace([T ]C
B).
Proof We prove part (a) and leave (b) as an exercise (Problem 21). Given v in V , we
have T (v) = 0 if and only if [T (v)]C is the zero vector in Rn, where n = dim[W]. But
since [T (v)]C = [T ]C
B[v]B by Equation (6.5.1), this is equivalent to saying that [v]B is
in nullspace([T ]C
B), as required.
One-to-one, Onto, and Invertible Linear Transformations: We saw in the previous
section that a linear transformation T : V →W is one-to-one if and only if Ker(T ) =
{0}, and T is onto if and only if Rng(T ) = W. Combining this with Theorem 6.5.9, we
have the following.
Corollary 6.5.10
Let T : V →W be a linear transformation, and let B and C be ordered bases for V and
W, respectively. Then
(a) T is one-to-one if and only if nullspace([T ]C
B) = {0}.
(b) T is onto if and only if colspace([T ]C
B) = Rn, where n = dim[W].
Finally, a linear transformation T : V →W is invertible if and only if T is both
one-to-one and onto. If dim[V ] = dim[W] = n, then the matrix [T]C
B is an n ×n matrix,
and from Corollary 6.5.10 we see that
T is invertible
if and only if
[T ]C
B is an invertible matrix for all ordered bases B and C of V and W, respectively,
if and only if
[T]C
B is an invertible matrix for some ordered bases B and C of V and W respectively.

6.5
The Matrix of a Linear Transformation 425
This is the natural generalization of Theorem 6.4.16. It says that, in order to check that
T is invertible, we need only show that [T]C
B is an invertible matrix for one choice of
ordered bases B and C. However, if we know already that T is invertible, then the matrix
[T ]C
B will be invertible for all choices of ordered bases B and C.
Given an invertible linear transformation T with matrix representation [T ]C
B, we
can now use matrices to determine the inverse linear transformation T −1. According to
Theorem 6.5.7, we have
[T −1]B
C[T ]C
B = [I]B
B,
(6.5.3)
and the latter matrix is simply the identity matrix. Likewise,
[T ]C
B[T −1]B
C = [I]C
C
(6.5.4)
is the identity matrix. Therefore, (6.5.3) and (6.5.4) imply that
7
[T ]C
B
8−1
= [T −1]B
C.
Therefore, we can use the matrix representation [T ]C
B of T to determine the matrix
representation [T −1]B
C of T −1, thus enabling us to determine T −1 directly from matrix
algebra. We illustrate with an example.
Example 6.5.11
Let T : P2(R) →P2(R) be deﬁned via
T (a + bx + cx2) = (3a −b + c) + (a −c)x + (4b + c)x2.
(a) Find the matrix representation of T relative to the standard basis B = {1, x, x2}
on P2(R).
(b) Use the matrix in part (a) to prove that T is invertible.
(c) Determine the linear transformation T −1 : P2(R) →P2(R) by using the matrix
representation of T −1 relative to B = {1, x, x2}.
Solution:
(a) We have T (1) = 3 + x, T (x) = −1 + 4x2, and T (x2) = 1 −x + x2. Finding the
components of these polynomials relative to the standard basis on P2(R) enables
us to quickly write down
[T ]B
B =
⎡
⎣
3 −1
1
1
0 −1
0
4
1
⎤
⎦.
(b) We can verify easily that [T ]B
B is invertible (for example, its determinant is
nonzero), and hence T is invertible.
(c) We have that
[T −1]B
B =
7
[T ]B
B
8−1
=
⎡
⎣
3 −1
1
1
0 −1
0
4
1
⎤
⎦
−1
= 1
17
⎡
⎣
4
5 1
−1
3 4
4 −12 1
⎤
⎦.
Thus,
T −1(a +bx +cx2) = 1
17[(4a +5b +c)+(−a +3b +4c)x +(4a −12b +c)x2].
□

426
CHAPTER 6
Linear Transformations
Exercises for 6.5
Key Terms
Matrix representation of T relative to bases B and C.
Skills
• Be able to determine the matrix representation of a
linear transformation T : V →W relative to bases B
and C for vector spaces V and W, respectively.
• Be able to use the matrix representation of a linear
transformation T : V →W and components of vec-
tors v in V to determine the action of T on v; that is,
to determine T (v).
• Be aware of the rich relationship between composition
of linear transformations and multiplication of corre-
sponding matrix representations.
• Be able to rephrase the concepts of kernel and range of
a linear transformation in terms of matrix representa-
tions, and likewise for one-to-one, onto, and invertible
linear transformations.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If dim[V ] = n and dim[W] = m, then any matrix rep-
resentation [T ]C
B of T : V →W relative to ordered
bases B and C (for V and W, respectively) is an n ×m
matrix.
(b) If T : V →W is any linear transformation, and [T ]C
B
is a matrix representation of T , then we have another
matrix representation for T given by [T ]B
C.
(c) If U, V, and W are vector spaces with ordered bases
A, B, and C, respectively, and T1
: U
→
V
and T2 : V →W are linear transformations, then
[T2T1]C
A = [T2]B
C[T1]A
B.
(d) If T : V →V is an invertible linear transforma-
tion, and B and C are ordered bases for V , then
([T ]C
B)−1 = [T −1]C
B.
(e) Two different linear transformations T1 : V1 →W1
and T2 : V2 →W2 (with bases B1, C1, B2, C2 for
V1, W1, V2, W2, respectively) can have the same ma-
trix representations: [T]C1
B1 = [T ]C2
B2.
(f) If T : V →W is an onto linear transformation
(with dim[V ]
=
n and dim[W]
=
m), then
colspace([T ]C
B) = Rm for any choice of ordered bases
B and C for V and W, respectively.
Problems
For Problems 1–8, determine the matrix representation [T ]C
B
for the given linear transformation T and ordered bases B
and C.
1. T : M2(R) →P3(R) given by
T
/( a
b
c
d
)0
= (a −d) + 3bx2 + (c −a)x3,
(a) B = {E11, E12, E21, E22}; C = {1, x, x2, x3}.
(b) B = {E21, E11, E22, E12}; C = {x, 1, x3, x2}.
2. T : P2(R) →R2 given by
T (a + bx + cx2) = (a −3c, 2a + b −2c),
(a) B = {1, x, x2}; C = {(1, 0), (0, 1)}.
(b) B = {1, 1+x, 1+x+x2}; C = {(1, −1), (2, 1)}.
3. T : P2(R) →P3(R) given by
T (p(x)) = (x + 1)p(x),
(a) B = {1, x, x2}; C = {1, x, x2, x3}.
(b) B = {1, x −1, (x −1)2};
C = {1, x −1, (x −1)2, (x −1)3}.
4. T : R3 →span{cos x, sin x} given by
T (a, b, c) = (a −2c) cos x + (3b + c) sin x,
(a) B = {(1, 0, 0), (0, 1, 0), (0, 0, 1)};
C = {cos x, sin x}.
(b) B = {(2, −1, −1), (1, 3, 5), (0, 4, −1)};
C = {cos x −sin x, cos x + sin x}.
5. T : M2(R) →R2 given by
T (A) = (tr(A), tr(A)),
(a) B = {E11, E12, E21, E22}; C = {(1, 0), (0, 1)}.
(b) B =
-(
−1 −2
−2 −3
)
,
(
1 1
2 2
)
,
(
0 −3
2 −2
)
,
(
0 4
1 0
).
;
C = {(1, 0), (0, 1)}.

6.5
The Matrix of a Linear Transformation 427
6. T : P3(R) →P2(R) given by T (p(x)) = p′(x),
(a) B = {1, x, x2, x3}; C = {1, x, x2}.
(b) B = {x3, x3 + 1, x3 + x, x3 + x2};
C = {1, 1 + x, 1 + x + x2}.
7. T : V →V (where V = span{e2x, e−3x}) given by
T ( f ) = f ′,
(a) B = C = {e2x, e−3x}.
(b) B = {e2x −3e−3x, 2e−3x};
C = {e2x + e−3x, −e2x}.
8. T : M2(R) →M2(R) given by
T (A) = 2A −AT ,
(a) B = C = {E11, E12, E21, E22}.
(b) B =
-(
−1 −2
−2 −3
)
,
(
1 1
2 2
)
,
(
0 −3
2 −2
)
,
(
0 4
1 0
).
;
C = {E11, E12, E21, E22}.
ForProblems9–15,determine T (v)forthegivenlineartrans-
formation T and vector in V by
(a) Computing [T ]C
B and [v]B and using Theorem 6.5.4.
(b) Direct calculation.
9. T : R3 →P3(R) via
T (a, b, c) = 2a −(a + b −c)x + (2c −a)x3,
relative to the standard bases B and C; v = (2, −1, 5).
10. T : P1(R) →M2(R) via
T (a + bx) =
( a −b
0
−2b
−a + b
)
,
relative to the standard bases B and C; p(x) =
−2 + 3x.
11. T : P2(R) →P4(R) via T (p(x)) = x2 p(x), relative
to the standard bases B and C; p(x) = −1+5x −6x2.
12. T : M2(R) →M2(R) via
T
/( a
b
c
d
)0
=
( 2a −b + d
−a + 3d
0
−a −b + 3c
)
,
relative to the standard basis B = C;
A =
( −7
2
1 −3
)
.
13. T : P4(R) →P3(R) via T (p(x)) = p′(x), relative to
the standard bases B and C; p(x) = 3 −4x + 6x2 +
6x3 −2x4.
14. T : M3(R) →R via T (A) = tr(A), relative to the
standard bases B and C;
A =
⎡
⎣
2 −6
0
1
4 −4
0
0 −3
⎤
⎦.
15. T : P3(R) →R via T (p(x)) = p(2), relative to the
standard bases B and C; p(x) = 2x −3x2.
16. Let T1 be the linear transformation from Problem
13 and let T2 be the linear transformation from
Problem 15.
(a) Find the matrix representation of T2T1 relative to
the standard bases.
(b) Verify Theorem 6.5.7 by comparing part (a) with
the product of the matrices in Problems 13 and 15.
(c) Use the matrix representation found in (a) to de-
termine (T2T1)(2 + 5x −x2 + 3x4). Verify your
answer by computing this directly.
17. Let T1 be the linear transformation from Problem
10 and let T2 be the linear transformation from
Problem 5.
(a) Find the matrix representation of T2T1 relative to
the standard bases.
(b) Verify Theorem 6.5.7 by comparing part (a) with
the product of the matrices in Problems 5 and 10.
(c) Use the matrix representation found in (a) to de-
termine (T2T1)(−3+8x). Verify your answer by
computing this directly.
18. Let T1 be the linear transformation from Problem
3 and let T2 be the linear transformation from
Problem 6.
(a) Find the matrix representation of T2T1 relative to
the standard bases.
(b) Verify Theorem 6.5.7 by comparing part (a) with
the product of the matrices in Problems 3 and 6.
(c) Use the matrix representation found in (a) to de-
termine (T2T1)(7−x +2x2). Verify your answer
by computing it directly.
(d) Is T2T1 invertible? Use matrix representations to
explain your answer.

428
CHAPTER 6
Linear Transformations
19. Is the linear transformation in Problem 1 invertible?
Use matrix representations to explain your answer.
20. Let T : P2(R) →R3 be given by
T (p(x)) = (p(0), p(1), p(2)).
Is T invertible? Find the matrix representation of T
with respect to the standard bases and use it to support
your answer.
21. Supply a proof of part (b) of Theorem 6.5.9.
6.6
Chapter Review
Linear Transformations
In this chapter, we have considered mappings T : V →W between vector spaces V and
W that satisfy the basic linearity properties
T (u + v) = T (u) + T (v), for all u and v in V ,
T (cv) = cT (v), for all v in V and all scalars c.
We now list some of the key deﬁnitions and theorems for linear transformations.
We have identiﬁed the following two important subsets of vectors associated with
a linear transformation:
1. The kernel of T , denoted Ker(T ). This is the set of all vectors in V that are mapped
to 0W, the zero vector in W.
2. The range of T , denoted Rng(T ). This is the set of vectors in W that we obtain
when we allow T to act on every vector in V . Equivalently, Rng(T ) is the set of
all transformed vectors.
The key results about Ker(T ) and Rng(T ) are as follows.
Let T : V →W be a linear transformation. Then,
1. Ker(T ) is a subspace of V .
2. Rng(T ) is a subspace of W.
3. If V is ﬁnite-dimensional, dim[Ker(T )] + dim[Rng(T )] = dim[V ].
4. T is one-to-one if and only if Ker(T ) = {0}.
5. T is onto if and only if Rng(T ) = W.
Finally, if T : V →W is a linear transformation, then the inverse transformation
T −1 : W →V exists if and only if T is both one-to-one and onto, in which case V and
W are isomorphic and T is called an isomorphism.
Note that a linear transformation T : V →W cannot be onto if dim[V ] < dim[W]
(and it might be one-to-one), and T : V →W cannot be one-to-one if dim[V ] > dim[W]
(and it might be onto). Therefore, a necessary condition for T : V →W to be an
isomorphism is that dim[V ] = dim[W].
Matrix Representation of a Linear Transformation
Starting with an m × n matrix A, we can deﬁne a linear transformation T : Rn →Rm
via T (x) = Ax. This is sometimes referred to as a matrix transformation, and in this
case, Ker(T ) = nullspace(A) and Rng(T ) = colspace(A). Therefore, to a given matrix,
we can naturally associate a linear transformation from Rn to Rm. Conversely, given any

6.6
Chapter Review 429
linear transformation T : Rn →Rm, we can construct the matrix of T given by the
formula
A = [T (e1), T (e2), . . . , T (en)],
where ei are the standard basis vectors on Rn. In this case, we can explicitly write the
formula for T : Rn →Rm as T (x) = Ax.
More generally, given any linear transformation T : V →W, where dim[V ] =
n and dim[W] = m with bases B = {v1, v2, . . . , vn} and C = {w1, w2, . . . , wm},
respectively, we can construct the m × n matrix representation of T relative to the
bases B and C via
[T ]C
B = [[T (v1)]C, [T (v2)]C, . . . , [T (vn)]C].
In the case V = W and B = C, we refer to [T ]B
B simply as the matrix representation of
T relative to the basis B. An important application of the matrix [T ]C
B is the computation
of coordinate vectors for vectors transformed under T :
[T(v)]C = [T ]C
B[v]B.
Many concepts relevant for linear transformations T : V →W can be understood
by ﬁnding a matrix representation [T ]C
B and looking at the corresponding concepts for
matrix. Here is a list of some examples:
1. For all v in V , v belongs to Ker(T ) if and only if [v]B belongs to nullspace([T ]C
B).
2. For all w in W, w belongs to Rng(T ) if and only if [w]C belongs to colspace([T ]C
B).
3. T is one-to-one if and only if nullspace([T ]C
B) = {0}.
4. T is onto if and only if colspace([T ]C
B) = Rn, where n = dim[W].
5. T is invertible if and only if [T]C
B is an invertible matrix for all ordered bases B
and C for V and W, respectively, if and only if [T]C
B is an invertible matrix for
some ordered bases B and C for V and W, respectively. The inverse transformation
T −1 : W →V has ([T ]C
B)−1 = [T −1]B
C.
6. If U, V , and W are vector spaces with ordered bases A, B, and C, respectively,
and T1 : U →V and T2 : V →W are linear transformations, then we have
[T2T1]C
A = [T2]C
B[T1]B
A.
Additional Problems
In Problems 1–10, decide whether or not the given mapping
T is a linear transformation. Justify your answers. For each
mapping that is a linear transformation, decide whether or
not T is one-to-one, onto, both, or neither, and ﬁnd a basis
and dimension for Ker(T ) and Rng(T ).
1. T : R2 →R4 deﬁned by
T (x, y) = (x + y, 0, x −y, xy).
2. T : R3 →R2 deﬁned by
T (x, y, z) = (2x −3y, −x).
3. T : R3 →R2 deﬁned by
T ((x, y, z)) = (−3z, 2x −y + 5z).
4. T : C[0, 1] →R2 deﬁned by T (g) = (g(0), g(1)).
5. T : R2 →R deﬁned by T (x, y) = x + y
5
.
6. T : M2(R) →R2 deﬁned by T
( a
b
c
d
)
= (ac, bd).
7. T : P2(R) →M2(R) deﬁned by
T (a + bx + cx2) =
( −a −b
0
3c −a
−2b
)
.
8. T : M2(R) →M2(R) deﬁned by
T (A) = A + AT .
9. T : R3 →P2(R) deﬁned by
T ((a, b, c)) = ax2 + (2b −c)x + (a −2b + c).
10. T : R3 →M2(R) deﬁned by
T ((x1, x2, x3)) =
(
0
x1 −x2 + x3
−x1 + x2 −x3
0
)

430
CHAPTER 6
Linear Transformations
In Problems 11–15, determine a formula for the linear trans-
formation meeting the given conditions.
11. T : R3 →R2 given by T (x) = Ax, where
A =
( −1
8
0
2 −2 −5
)
.
12. T : R2 →R5 given by T (x) = Ax, where
A =
⎡
⎢⎢⎢⎢⎣
−1
4
0
2
3 −3
3 −3
2 −6
⎤
⎥⎥⎥⎥⎦
.
13. T : R →R4 such that T (2) = (−1, 5, 0, −2).
14. T : M2(R) →R2 such that
T
( 1 0
0 1
)
= (2, −5), T
( 0 1
1 0
)
= (0, −3),
T
( 1 0
0 0
)
= (1, 1), T
( 1 1
0 0
)
= (−6, 2).
15. T : P2(R) →M2(R) such that
T (x2−x−3) =
( −2
1
−4 −1
)
, T (2x+5) =
( 0
1
2 −2
)
,
and
T (6) =
( 12
6
6
18
)
.
16. If T : P5(R) →M2(R) is an onto linear transforma-
tion, what is dim[Ker(T )]?
17. If T
: M23(R) →P6(R) is one-to-one, what is
dim[Rng(T )]?
18. If T : M42(R) →Rn is an isomorphism, what is n?
19. If T : P4(R) →P3(R) is not onto, what are the pos-
sible values for the dimension of Ker(T )?
In Problems 20–23, determine the matrix representation
[T ]C
B for the given linear transformation T and ordered bases
B and C, and for the given vector v in V , use Theorem 6.5.4
to compute T (v).
20. T : P3(R) →R2 given by
T (p(x)) = (p(0), p(1)),
with B = {1, x, x2, x3}, C = {(1, 0), (0, 1)}, and
v = 2 −x2 + 2x3.
21. T : R3 →M2(R) given by
T (a, b, c) =
(
0
−a + b + 3c
5a −c
−3b
)
,
with B = {(0, 1, 0), (0, 0, 1), (1, 0, 0)},
C = {E21, E22, E11, E12}, and v = (−2, 1, −2).
22. T : R2 →R4 given by
T (x) = Ax,
with A =
⎡
⎢⎢⎣
1 3
−1 1
−2 0
5 2
⎤
⎥⎥⎦, B = {(1, 1), (1, 0)},
C = {(0, 0, 0, 1), (0, 0, 1, 0), (0, 1, 0, 0), (1, 0, 0, 0)},
and v = (−2, 4).
23. T : P2(R) →P1(R) given by
T (p(x)) = p′(x),
with B = {x2 + x, 1, x}, C = {1, 1 + x}, and
v = −3 + x2.
If T1 and T2 are both linear transformations from V to W,
then we can deﬁne a mapping T1 + T2 : V →W, given by
(T1 + T2)(v) = T1(v) + T2(v) for all v in V . The next three
problems concern the mapping T1 + T2.
24. Let T1 and T2 be linear transformations from V to W.
Prove that T1 + T2 is a linear transformation. Must
there be any relationship between Ker(T1), Ker(T2),
and Ker(T1 + T2)?
25. True or False: Let T1 and T2 be linear transformations
from V to W. If T1 and T2 are both onto, then T1 + T2
is onto.
26. True or False: Let T1 and T2 be linear transformations
from V to W. If T1 and T2 are both one-to-one, then
T1 + T2 is one-to-one.
27. Let V and W be vector spaces, and let T : V →W
be a linear transformation with Ker(T ) = {0}. If
{v1, v2, . . . , vn} is a linearly independent subset of V ,
show that {T(v1), T (v2), . . . , T (vn)} is a linearly in-
dependent subset of W.
28. Prove that if V1 is isomorphic to V2 and V2 is isomor-
phic to V3, then V1 is isomorphic to V3.
29. Fix an invertible n×n matrix S. Show that the function
T : Mn(R) →Mn(R) deﬁned by T (A) = S−1 AS is
an isomorphism.

6.6
Chapter Review 431
Project: More transformations in two and three dimensions
A primary application of linear transformations is in the ﬁeld of computer graphics. A
displayed image occupies a set of points on a computer screen, which can be assigned to
an xy-coordinate system R2. The points of the image have coordinates in this coordinate
system. In the ﬁrst two sections of this chapter, we considered various transformations
of R2. For example, to rotate the points of R2 counterclockwise about the origin through
an angle of θ, one can apply the matrix T (θ) given in Equation (6.1.4). In Section 6.2,
we considered other simple transformations including reﬂections, stretches, and shears.
All of these actions on R2 are useful in applications such as computer animations. In
this project, we will extend some of the basic transformations we have looked at before,
and we will also explore a few examples of transformations in R3 as well.
Part I: Reﬂection about a line through the origin in R2. In the context of linear
transformations, the only lines we are permitted to reﬂect across in R2 must go through
the origin. (Why?) Simple examples such as reﬂections across the lines x = 0, y = 0,
and y = x were presented in Section 6.2. In this part of the project, we aim to determine
in general the matrix representation for the linear transformation T that reﬂects points of
R2 across an arbitrary line y = mx passing through the origin, where m is an arbitrary,
ﬁxed real number.
(a) Determine a nonzero vector v1 in R2 such that T (v1) = v1.
(b) Determine a nonzero vector v2 in R2 such that T (v2) = −v2.
(c) Explain why B = {v1, v2} is a basis for R2.
(d) Compute the matrix [T ]B
B.
(e) Let I : R2 →R2 denote the identity linear transformation deﬁned by I (x) = x
for all x in R2, and let C be any basis for R2. Use matrix representations for I to
ﬁnd a formula for [T ]C
C in terms of [T ]B
B.
(f) Let C = {(1, 0), (0, 1)} denote the standard ordered basis on R2. Use part (e) to
determine [T ]C
C.
(g) Use part (f) to derive a formula for T (x, y) for an arbitrary point (x, y) in R2.
Part II: Reﬂection about an arbitrary line in R2. Now we are ready to generalize
what we accomplished in Part I to arbitrary lines in R2. The line y = mx + b does not
pass through the origin when b ̸= 0, and the transformation that reﬂects points across this
line is not linear. (Why?) Therefore, we need a new strategy for how to derive a formula
for reﬂecting a point (x, y) across this line. The basic idea we will use is to translate the
problem so that the line in question once again passes through the origin, then perform
the reﬂection as conducted in Part I, and then translate back to its original position. Let
S(x, y) denote the result of reﬂecting the point (x, y) across the line y = mx + b.
(a) Show that
S(x, y) = T (x, y −b) + b.
(b) Test your formula for S(x, y) for various lines y = mx + b. Plot such a line
using computer software, choose various points in R2, plot these points and their
reﬂections across y = mx + b, and then use your formula for S(x, y) to verify
that the correct point was found.

432
CHAPTER 6
Linear Transformations
Part III: Reﬂection across a plane in R3. In this part of the project, we generalize what
we accomplished in Parts I and II to reﬂection across planes. To begin with, we assume
that the reﬂecting plane passes through the origin and determine the formula for a linear
transformation T : R3 →R3 that performs reﬂection across the plane ax +by +cz = 0
in R3.
(a) Determine two nonzero and nonproportional vectors v1 and v2 in R3 such that
T (v1) = v1 and T (v2) = v2.
(b) Determine a nonzero vector v3 in R3 such that T (v3) = −v3.
(c) Explain why B = {v1, v2, v3} is a basis for R3.
(d) Compute the matrix [T ]B
B.
(e) Let I : R3 →R3 denote the identity linear transformation deﬁned by I (x) = x
for all x in R3, and let C be any basis for R3. Use matrix representations for I to
ﬁnd a formula for [T]C
C in terms of [T ]B
B.
(f) Let C = {(1, 0, 0), (0, 1, 0), (0, 0, 1)} denote the standard ordered basis on R3.
Use part (e) to determine [T ]C
C.
(f) Use part (f) to derive a formula for T (x, y, z) for an arbitrary point (x, y, z) in R3.
(g) As done in Part II, we can use the result in part (g) to ﬁnd a formula S(x, y, z) for the
image obtained by reﬂecting the point (x, y, z) across the plane ax +by +cz = d.
Assuming that c ̸= 0, shift the plane in the z-direction and apply part (g) to ﬁnd a
formula for S(x, y, z). Find an alternative expression for S(x, y, z) if c = 0.
(h) Test your formula in part (h) for S(x, y, z) for various planes ax + by + cz = d.
Part IV: Open-ended investigation of transformations in R3. We have seen in Sec-
tion 6.2 that the fundamental transformations available in R2 are the reﬂections, stretches,
and shears. Moreover, we saw that rotations about the origin in R2 can be viewed as a suit-
able combination of these three fundamental transformations. In Part III, we discussed
how to reﬂect points in R3 across a plane in R3. Next, the reader is invited to investigate
other possible transformations in R3, including stretches and shears. In particular, the
reader may want to make a list of 3 × 3 matrices that accomplish stretches and shears in
the same way that the 2 × 2 matrices for stretches and shears were given in Section 6.2.

7
Eigenvalues and
Eigenvectors
In order to motivate the problem to be studied in this chapter, we recall from Chapter 1
that the differential equation
dx
dt = ax,
(7.0.1)
where a is a constant, has general solution
x(t) = ceat.
(7.0.2)
Then, in Section 2.3, we considered a system of two differential equations
dx1
dt
= a11x1 + a12x2,
dx2
dt
= a21x1 + a22x2,
where x1 and x2 are functions of the independent variable t, and the ai j are constants.
We wrote this more succinctly as the vector equation
dx
dt = Ax(t),
(7.0.3)
where
x(t) =
! x1(t)
x2(t)
"
,
A = [ai j],
dx
dt =
! dx1/dt
dx2/dt
"
.
Based on the solution (7.0.2) to the differential equation (7.0.1), we might suspect that
the system (7.0.3) may have solutions of the form
x(t) =
! eλtv1
eλtv2
"
= eλtv,
433

434
CHAPTER 7
Eigenvalues and Eigenvectors
where λ, v1, and v2 are constants and v =
! v1
v2
"
. Indeed, substituting this expression
for x(t) into (7.0.3) yields
λeλtv = A(eλtv),
or equivalently,
Av = λv.
(7.0.4)
We have therefore shown that
x(t) = eλtv
is a solution to the system of differential equations (7.0.3), provided that λ and v satisfy
Equation (7.0.4).
In Chapter 9, we will pursue this technique for determining solutions to general
linear systems of differential equations. In the present chapter, however, we focus our
attention on the mathematical problem of ﬁnding all scalars λ and all nonzero vectors v
satisfying Equation (7.0.4) for a given n × n matrix A.
Although we used systems of differential equations to motivate the study of this
problem, it is important to realize that this problem arises also in many other areas of
mathematics and statistics, as well as in applications in physics, chemistry, biology, and
computer science. We will study some of these applications in the pages to come.
7.1
The Eigenvalue/Eigenvector Problem
We begin this section by introducing the cornerstone terminology on which everything
in the remainder of this chapter will depend.
DEFINITION
7.1.1
Let A be an n × n matrix. Any values of λ for which
Av = λv
(7.1.1)
has nontrivial solutions v are called eigenvalues of A. The corresponding nonzero
vectors v are called eigenvectors of A.
Remark
Eigenvalues and eigenvectors are also sometimes referred to as character-
istic values and characteristic vectors of A.
In order to formulate the eigenvalue/eigenvector problem within the vector space
framework, we will interpret A as the matrix of a linear transformation T : Cn →Cn in
the usual manner; that is, T (v) = Av. In many of our problems, A and λ will both be real,
which will enable us to restrict attention to Rn, although we will often require the complex
vector space Cn. Indeed, we will see in the later chapters that complex eigenvalues and
eigenvectors are required to describe linear physical systems that exhibit oscillatory
behavior (in a similar manner that oscillatory behavior in a spring-mass system arises
only if the auxiliary equation has complex conjugate roots). Readers can ﬁnd a cursory
review of the essential mechanics and properties of complex numbers in Appendix A.
It is always helpful to have a geometric interpretation of the problem under consid-
eration. According to Equation (7.1.1), the eigenvectors of A are those nonzero vectors
that are mapped into a constant scalar multiple of themselves by the linear transformation
T (v) = Av. Geometrically, this means that the linear transformation leaves the direction

7.1
The Eigenvalue/Eigenvector Problem 435
of v unchanged1 and stretches the vector v by a factor of λ. This is illustrated for the
case R2 in Figure 7.1.1. Note that if Av = λv and c is an arbitrary scalar, then
A(cv) = cAv = c(λv) = λ(cv).
Consequently, if v is an eigenvector of A, then so is cv for any nonzero scalar c.
v
v
v2
v2
v1
v1
Av = lv
Av 5 lv
v is an eigenvector of A
v is not an eigenvector of A
/
Figure 7.1.1: A geometrical description of the eigenvalue/eigenvector problem in R2.
Example 7.1.2
Let A =
! −2
5
6 −1
"
. Show that v1 = (−1, 1) is an eigenvector of A corresponding to
the eigenvalue λ1 = −7, and show that v2 = (5, 6) is an eigenvector of A corresponding
to the eigenvalue λ2 = 4.
Solution:
We have the following:2
Av1 =
! −2
5
6 −1
" ! −1
1
"
=
!
7
−7
"
= −7
! −1
1
"
= −7v1.
Consequently, v1 is an eigenvector of A corresponding to the eigenvalue λ = −7.
Similarly, we have
Av2 =
! −2
5
6 −1
" ! 5
6
"
=
! 20
24
"
= 4
! 5
6
"
= 4v2.
Consequently, v2 is an eigenvector of A corresponding to the eigenvalue λ = 4.
□
Remark
In the example above, notice that v1 + v2 = (−1, 1) + (5, 6) = (4, 7), and
! −2
5
6 −1
" ! 4
7
"
=
! 27
17
"
,
and since (27, 17) is not a scalar multiple of (4, 7), we see that v1 + v2 is not an
eigenvector of A. Therefore, in general, the sum of two eigenvectors of a matrix is not
an eigenvector. As we shall see soon, however, any nonzero sum of two eigenvectors
corresponding to the same eigenvalue λ will in fact be another eigenvector corresponding
to λ.
Solution of the Problem
The solution of the eigenvalue/eigenvector problem hinges on the observation that (7.1.1)
can be written in the equivalent form
(A −λI)v = 0,
(7.1.2)
1If λ < 0, then the transformed vector points in the direction opposite to v, due to the minus sign.
2Notice that once more we will switch between vectors in Rn and column n-vectors.

436
CHAPTER 7
Eigenvalues and Eigenvectors
where I denotes the identity matrix. Consequently, the eigenvalues of A are those values
of λ for which the n×n linear system (7.1.2) has nontrivial solutions, and the eigenvectors
are the corresponding solutions. But, according to Corollary 3.2.6, the system (7.1.2)
has nontrivial solutions if and only if
det(A −λI) = 0.
To solve the eigenvalue/eigenvector problem, we therefore proceed as follows:
Solution to the Eigenvalue/Eigenvector Problem
1. Find all scalars λ with det(A −λI) = 0. These are the eigenvalues of A.
2. If λ1, λ2, . . . , λk are the distinct eigenvalues obtained in (1), then solve
the k systems of linear equations
(A −λi I)vi = 0,
i = 1, 2, . . . , k
to ﬁnd all eigenvectors vi corresponding to each eigenvalue.
DEFINITION
7.1.3
For a given n × n matrix A, the polynomial p(λ) deﬁned by
p(λ) = det(A −λI)
is called the characteristic polynomial of A, and the equation
p(λ) = 0
is called the characteristic equation of A.
From the deﬁnition of the determinant, Deﬁnition 3.1.8, we can verify that3 p(λ)
is a polynomial in λ of degree n. Also, it follows from (1) that the eigenvalues of A are
the roots of the characteristic equation. Using this fact, the following fundamental result
can be deduced (Problem 40).
Proposition 7.1.4
An n × n matrix A is invertible if and only if 0 is not an eigenvalue of A.
Some Examples
We now consider three examples to illustrate some of the possibilities that can arise in
the eigenvalue/eigenvector problem and also to motivate some of the theoretical results
that will be established in the next section.
Example 7.1.5
Find all eigenvalues and eigenvectors of A =
!
3 −1
−5 −1
"
.
Solution:
The linear system for determining the eigenvalues and eigenvectors is
(A −λI)v = 0; that is,
! 3 −λ
−1
−5
−1 −λ
" ! v1
v2
"
=
! 0
0
"
.
(7.1.3)
3Alternatively, this can be veriﬁed by induction on n, using the Cofactor Expansion Theorem.

7.1
The Eigenvalue/Eigenvector Problem 437
This system has nontrivial solutions if and only if λ satisﬁes the characteristic equation
3 −λ
−1
−5
−1 −λ = 0.
Expanding the determinant yields
(3 −λ)(−1 −λ) −5 = 0.
That is,
λ2 −2λ −8 = 0,
which has factorization
(λ + 2)(λ −4) = 0.
Consequently, the eigenvalues of A are
λ1 = −2,
λ2 = 4.
The corresponding eigenvectors are obtained by successively substituting the fore-
going eigenvalues into (7.1.3) and solving the resulting system.
Eigenvalue λ1 = −2: We have A −λ1I =
!
5 −1
−5
1
"
, so the augmented matrix of the
system (A −λ1I)v = 0 is
!
5 −1 0
−5
1 0
"
,
with reduced row-echelon form
#
1 −1
5
0
0
0
0
$
.
The solution to the system can therefore be written in the form v = (r, 5r), where r
is a free variable. It follows that the eigenvectors corresponding to λ1 = −2 are those
vectors in R2 of the form
v = r(1, 5),
where r is any nonzero real number. Notice that there is only one linearly indepen-
dent eigenvector corresponding to the eigenvalue λ1 = −2, which we may choose as
v1 = (1, 5). All other eigenvectors corresponding to λ1 = −2 are scalar multiples of v1.
Eigenvalue λ2 = 4: The augmented matrix of the system (A −λ2I)v = 0 is
! −1 −1 0
−5 −5 0
"
,
with reduced row-echelon form
! 1 1 0
0 0 0
"
.
Consequently, the system has solutions v = (−s, s), where s is a free variable. It follows
that the eigenvectors corresponding to λ2 = 4 are those vectors in R2 of the form
v = s(−1, 1),
where s is any nonzero real number. Once more there is only one linearly indepen-
dent eigenvector corresponding to the eigenvalue λ2 = 4, which we may choose as
v2 = (−1, 1). All other eigenvectors corresponding to λ2 = 4 are scalar multiples of v2.

438
CHAPTER 7
Eigenvalues and Eigenvectors
Notice that the eigenvectors v1 = (1, 5) and v2 = (−1, 1) (which correspond
to the two different eigenvalues here) are nonproportional and therefore are linearly
independent in R2. The matrix A has therefore picked out a basis for R2. This is illustrated
in Figure 7.1.2.
□
v1 5 (1, 5)
v2
v1
v2 5 (21, 1)
Figure 7.1.2: Two linearly independent eigenvectors for the matrix in Example 7.1.5.
Example 7.1.6
Find all eigenvalues and eigenvectors of
A =
⎡
⎣
5
12 −6
−3 −10
6
−3 −12
8
⎤
⎦.
Solution:
The system (A −λI)v = 0 has nontrivial solutions if and only if
det(A −λI) = 0. For the given matrix,
det(A −λI) =
5 −λ
12
−6
−3
−10 −λ
6
−3
−12
8 −λ
.
Using the Cofactor Expansion Theorem along row 1 yields
det(A −λI) = (5 −λ)[(λ −8)(λ + 10) + 72] −12[3λ −6] −6[6 −3λ]
= (5 −λ)(λ2 + 2λ −8) + 18(2 −λ)
= (5 −λ)(λ −2)(λ + 4) + 18(2 −λ)
= (2 −λ)[(λ −5)(λ + 4) + 18] = (2 −λ)(λ2 −λ −2)
= (2 −λ)(λ −2)(λ + 1) = −(λ −2)2(λ + 1).
Consequently, A has eigenvalues
λ1 = 2
(repeated twice),
λ2 = −1.
To determine the corresponding eigenvectors of A, we must solve each of the
homogeneous linear systems
(A −λ1I)v = 0,
(A −λ2I)v = 0.
Eigenvalue λ1 = 2: The augmented matrix of the homogeneous linear system
(A −λ1)v = 0 is
⎡
⎣
3
12 −6 0
−3 −12
6 0
−3 −12
6 0
⎤
⎦,

7.1
The Eigenvalue/Eigenvector Problem 439
with reduced row-echelon form
⎡
⎣
1 4 −2 0
0 0
0 0
0 0
0 0
⎤
⎦.
Hence, the eigenvectors of A corresponding to the eigenvalue λ1 = 2 are
v = (−4r + 2s,r, s),
where r and s are free variables that cannot be simultaneously zero. (Why not?) Writing
v in the equivalent form
v = r(−4, 1, 0) + s(2, 0, 1),
we see that there are two linearly independent eigenvectors corresponding to λ1 = 2,
which we may choose as v1 = (−4, 1, 0) and v2 = (2, 0, 1). All other eigenvectors
corresponding to λ1 = 2 are obtained by taking nontrivial linear combinations of v1, v2.
Therefore, geometrically, these eigenvectors all lie in the plane through the origin of a
Cartesian coordinate system that contains v1 and v2.
Eigenvalue λ2 = −1: The augmented matrix of the linear system (A −λ2I)v = 0 is
⎡
⎣
6
12 −6 0
−3
−9
6 0
−3 −12
9 0
⎤
⎦,
with reduced row-echelon form
⎡
⎣
1 0
1 0
0 1 −1 0
0 0
0 0
⎤
⎦.
Consequently, the eigenvectors of A corresponding to the eigenvalue λ2 = −1 are those
nonzero vectors in R3 of the form
v = t(−1, 1, 1),
where t is a nonzero free variable. We see that there is only one linearly independent
eigenvector corresponding to λ2 = −1, which we may take to be v3 = (−1, 1, 1). All
other eigenvectors of A corresponding to λ2 = −1 are scalar multiples of v3. Geomet-
rically, these eigenvectors lie on the line with direction vector v3, which passes through
the origin of a Cartesian coordinate system.
We note that the eigenvectors of A have given rise to the set of vectors {(−4, 1, 0),
(2, 0, 1), (−1, 1, 1)}. Furthermore, since
det([v1, v2, v3]) =
−4 2 −1
1 0
1
0 1
1
= 1 ̸= 0,
the set of eigenvectors {(−4, 1, 0), (2, 0, 1), (−1, 1, 1)} is linearly independent and
therefore is a basis for R3.
□
The reader may have noticed a couple of common features of the two examples just
given. First, the number of linearly independent eigenvectors associated with a given
eigenvalue λ is equal to the number of times λ occurs as a root of the characteristic
equation. Second, the total number of linearly independent eigenvectors obtained from

440
CHAPTER 7
Eigenvalues and Eigenvectors
all of the eigenvalues agrees with the number of rows and columns of the matrix A.
Are these observations always to be expected? The answer is no. The next example
illustrates this, and in the next section, we will have much more to say about this issue
from a theoretical perspective.
Example 7.1.7
Find all eigenvalues and eigenvectors of A =
! 1 1
0 1
"
.
Solution:
The system (A −λI)v = 0 has nontrivial solutions if and only if
det(A −λI) = 0. For the given matrix,
det(A −λI) = 1 −λ
1
0
1 −λ = (λ −1)2.
Hence, A has eigenvalue λ1 = 1 (repeated twice).
To determine the corresponding eigenvectors of A, we must solve the homogeneous
linear system
(A −λ1I)v = 0
which, for λ1 = 1, has augmented matrix
! 0 1 0
0 0 0
"
.
Hence, the eigenvectors of A corresponding to λ1 = 1 are
v = (r, 0),
where r is a nonzero free variable. It follows that the eigenvectors corresponding to
λ1 = 1 are those vectors in R2 of the form
v = r(1, 0).
Notice that there is only one linearly independent eigenvector here,
v1 = (1, 0).
In this case, therefore, the number of linearly independent eigenvectors associated with
λ falls short of the number of times λ occurs as a root of the characteristic equation.
In the next section, we will refer to a matrix such as this as defective, since one of its
eigenvalues fails to have “enough” linearly independent eigenvectors.
□
The matrices and eigenvalues in the previous examples have all been real, and this
enabled us to regard the eigenvectors as vectors in Rn. We now consider the case when
some or all of the eigenvalues and eigenvectors are complex. The steps in determining
the eigenvalues and eigenvectors do not change, although they can be more complicated
algebraically, since the equations determining the eigenvectors will have complex coef-
ﬁcients. For the majority of matrices that we consider, the matrix A will have only real
elements. In these cases, the following theorem can save some work in determining any
complex eigenvectors.
Theorem 7.1.8
Let A be an n × n matrix with real elements. If λ is a complex eigenvalue of A
with corresponding eigenvector v, then λ is an eigenvalue of A with corresponding
eigenvector v.
Proof If Av = λv, then Av = λv, which implies that Av = λv, since A has real
entries.

7.1
The Eigenvalue/Eigenvector Problem 441
Remark
According to the previous theorem, if we ﬁnd the eigenvectors of a real
matrix A corresponding to a complex eigenvalue λ, then we can obtain the eigenvectors
corresponding to the eigenvalue λ without having to solve a linear system.
Example 7.1.9
Find all eigenvalues and eigenvectors of A =
!
9
37
−1 −3
"
.
Solution:
The characteristic polynomial of A is
p(λ) = 9 −λ
37
−1
−3 −λ = λ2 −6λ + 10.
Consequently, the eigenvalues of A are
λ1 = 3 + i,
λ2 = λ1 = 3 −i.
Since these are complex eigenvalues, we take the underlying vector space as C2, and
hence, any scalars that arise in the solution of the problem will be complex.
Eigenvalue λ1 = 3 + i: The augmented matrix of the system (A−λ1I)v = 0 reduces to
! 6 −i
37
0
−1
−6 −i 0
"
1∼
!
0
0
0
−1 −6 −i 0
"
2∼
! −1 −6 −i 0
0
0
0
"
3∼
! 1 6 + i 0
0
0
0
"
,
1. A21(6 −i)
2. P12
3. M1(−1)
so that the eigenvectors of A corresponding to λ1 = 3 + i are those vectors in C2 of
the form
v = r(−(6 + i), 1),
where r is an arbitrary nonzero complex number.
Eigenvalue λ2 = 3 −i: From Theorem 7.1.8, the eigenvectors in this case are those
vectors in C2 of the form v = s(−(6 −i), 1), where s is an arbitrary nonzero complex
number.
Notice that the eigenvectors corresponding to different eigenvalues are linearly in-
dependent vectors in C2. For example, a linearly independent set of eigenvectors is
{(−(6 + i), 1), (−(6 −i), 1)}. Once more the eigenvectors have determined a basis in
the underlying vector space (in this case C2).
□
Remark
Performing elementary row operations on matrices with complex entries is
often more cumbersome than when only real entries are present. An important fact that
is helpful to remember when ﬁnding eigenvectors corresponding to the eigenvalue λ is
that the matrix A −λI is always not invertible. This means that performing elementary
row operations on the augmented matrix for the system (A −λI)v = 0 must produce
a row of zeros at the bottom. Therefore, in the case when A is only 2 × 2, then the two
rows of A −λI must be proportional to one another for each eigenvalue λ of A. Hence,
it is permissible to forego tedious elementary row operations, and instead, simply ignore
one of the two rows. We caution the reader that this shortcut is generally not applicable
in the case of a matrix A of size 3 × 3 or larger.
When ﬁrst encountering the eigenvalue/eigenvector problem, students often focus so
much attention on the computational aspects of ﬁnding the eigenvalues and eigenvectors

442
CHAPTER 7
Eigenvalues and Eigenvectors
that they lose sight of the original equation that deﬁnes the eigenvalues and eigenvectors
of A. The following examples illustrate the importance of the deﬁning equation when
establishing theoretical results.
Example 7.1.10
Let λ be an eigenvalue of the matrix A with corresponding eigenvector v. Prove that λ2
is an eigenvalue of A2 with corresponding eigenvector v.
Solution:
We are given that
Av = λv,
(7.1.4)
and we must establish that
A2v = λ2v.
From Equation (7.1.4), we have
A2v = A(Av) = A(λv) = λ(Av) = λ(λv) = λ2v,
and the result is established.
□
Example 7.1.11
Let λ and v be an eigenvalue/eigenvector pair for the n × n matrix A. If k is an arbitrary
real number, prove that v is also an eigenvector of the matrix A −kI corresponding to
the eigenvalue λ −k.
Solution:
Once more, the only information that we are given is that
Av = λv.
If we let B = A −kI, then we must establish that
Bv = (λ −k)v.
But,
Bv = (A −kI)v = Av −kv = λv −kv = (λ −k)v,
as required.
□
Exercises for 7.1
Key Terms
Eigenvalue, Eigenvector, Characteristic polynomial, Char-
acteristic equation.
Skills
• Be able to determine whether a given scalar λ and vec-
tor v form an eigenvalue/eigenvector pair for a given
matrix A.
• Be able to determine eigenvectors that correspond to
a given eigenvalue of A.
• Be able to determine the eigenvalue that corresponds
to a given eigenvector of A.
• For 2 × 2 and 3 × 3 matrices, be able to provide a
geometric interpretation of the eigenvalue/eigenvector
problem. For special cases, you should be able to
determine eigenvalue/eigenvector pairs by arguing
geometrically.
• Be able to compute the characteristic polynomial for
a given matrix A and use it to ﬁnd the eigenvalues
of A.
• Be able to prove basic facts about eigenvalue-
eigenvector pairs from the deﬁnitions in this section.
(See Examples 7.1.10 and 7.1.11.)
• Be able to ﬁnd all eigenvalues and corresponding
eigenvectors for a given matrix A.

7.1
The Eigenvalue/Eigenvector Problem 443
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An eigenvector corresponding to the eigenvalue λ of
a matrix A is any vector v such that Av = λv.
(b) The eigenvalues of an upper or lower-triangular ma-
trix A are the entries appearing on the main diagonal
of A.
(c) If two matrices A and B have the same characteristic
polynomial, then A and B must have exactly the same
set of eigenvalues.
(d) If A is an n × n matrix, then A has n eigenvalues,
including possible repeated eigenvalues and complex
eigenvalues.
(e) If A is the 2 × 2 matrix of the linear transformation
T : R2 →R2 that rotates points of the xy-plane
counterclockwise by 90 degrees, then A has no real
eigenvalues.
(f) If two matrices A and B have exactly the same char-
acteristic polynomial, then A and B must have exactly
the same set of eigenvectors.
(g) A linear combination of a set of eigenvectors of a ma-
trix A is again an eigenvector of A.
(h) If λ = a + ib (b ̸= 0) is a complex eigenvalue of a
matrix A, then so is λ = a −ib.
(i) If λ is an eigenvalue of the matrix A, then λ3 is an
eigenvalue of A3.
Problems
For Problems 1–5, use Equation (7.1.1) to verify that λ and
v are an eigenvalue/eigenvector pair for the given matrix A.
1. λ = 2, v = (2, 9), A =
! −7 2
−9 4
"
.
2. λ = 4, v = (1, 1), A =
! 1 3
2 2
"
.
3. λ = 3, v = (2, 1, −1), A =
⎡
⎣
1 −2 −6
−2
2 −5
2
1
8
⎤
⎦.
4. λ = −2, v = c1(1, 0, −3) + c2(4, −3, 0),
A =
⎡
⎣
1 4
1
3 2
1
3 4 −1
⎤
⎦, where c1 and c2 are constants.
5. λ = 10, v = c1(1, −4, 0) + c2(0, 0, 1),
A =
⎡
⎣
6 −1
0
−16
6
0
−4 −1 10
⎤
⎦, where c1 and c2 are con-
stants.
6. Given that v1 = (−2, 1) and v2 = (1, 1) are eigen-
vectors of
A =
! −5
2
1 −4
"
,
determine the eigenvalues of A.
7. Given that v1 = (1, −2) and v2 = (1, 1) are eigen-
vectors of A =
! 4 1
2 3
"
, determine the eigenvalues
of A.
8. The effect of the linear transformation T : R2 →R2
with matrix A =
! 1
0
0 −1
"
is to reﬂect each vector
in the x-axis. By arguing geometrically, determine all
eigenvalues and eigenvectors of A.
9. The effect of the linear transformation T : R2 →R2
with matrix A =
! 0 1
1 0
"
is to reﬂect each vector
across the line y = x. By arguing geometrically, de-
termine all eigenvalues and eigenvectors of A.
10. The linear transformation T : R2 →R2 with matrix
A =
! cos θ
−sin θ
sin θ
cos θ
"
rotates vectors in the xy-plane
counterclockwise through an angle θ, where 0 ≤θ <
2π. By arguing geometrically, determine all values
of θ for which A has real eigenvalues. Find the real
eigenvalues and the corresponding eigenvectors.
11. The linear transformation T : R3 →R3 with matrix
A =
⎡
⎣
0 0 0
0 1 0
0 0 0
⎤
⎦takes vectors (x, y, z) in R3 and
moves them to the corresponding point (0, y, 0) on
the y-axis. By arguing geometrically, determine all
eigenvalues and eigenvectors of A.

444
CHAPTER 7
Eigenvalues and Eigenvectors
For Problems 12–32, determine all eigenvalues and corre-
sponding eigenvectors of the given matrix.
12.
! 5 −4
8 −7
"
.
13.
! 1
6
2 −3
"
.
14.
!
7 4
−1 3
"
.
15.
! 2 0
0 2
"
.
16.
!
7 3
−6 1
"
.
17.
! −2 −6
3
4
"
.
18.
! 3 −2
4 −1
"
.
19.
!
2 3
−3 2
"
.
20.
⎡
⎣
10 −12
8
0
2
0
−8
12 −6
⎤
⎦.
21.
⎡
⎣
3
0
0
0
2 −1
1 −1
2
⎤
⎦.
22.
⎡
⎣
1
0
0
0
3
2
2 −2 −1
⎤
⎦.
23.
⎡
⎣
6
3 −4
−5 −2
2
0
0 −1
⎤
⎦.
24.
⎡
⎣
7 −8
6
8 −9
6
0
0 −1
⎤
⎦.
25.
⎡
⎣
0
1 −1
0
2
0
2 −1
3
⎤
⎦.
26.
⎡
⎣
1
0 0
0
0 1
0 −1 0
⎤
⎦.
27.
⎡
⎣
−2
1
0
1 −1 −1
1
3 −3
⎤
⎦.
28.
⎡
⎣
2 −1 3
3
1 0
2 −1 3
⎤
⎦.
29.
⎡
⎣
5 0 0
0 5 0
0 0 5
⎤
⎦.
30.
⎡
⎣
0 2 2
2 0 2
2 2 0
⎤
⎦.
31.
⎡
⎢⎢⎣
1 2 3 4
4 3 2 1
4 5 6 7
7 6 5 4
⎤
⎥⎥⎦.
32.
⎡
⎢⎢⎣
0 1 0
0
−1 0 0
0
0 0 0 −1
0 0 1
0
⎤
⎥⎥⎦.
33. Find all eigenvalues and corresponding eigenvectors
of
A =
⎡
⎣
1 + i
0
0
2 −2i
1 −3i
0
2i
0
1
⎤
⎦.
Note that the eigenvectors do not occur in complex
conjugate pairs. Does this contradict Theorem 7.1.8?
Explain.
34. Consider the matrix A =
! 1 −1
2
4
"
.
(a) Show that the characteristic polynomial of A is
p(λ) = λ2 −5λ + 6.
(b) Show that A satisﬁes its characteristic equation.
That is, A2 −5A + 6I2 = 02. (This result is
known as the Cayley-Hamilton Theorem and is
true for general n × n matrices.)
(c) Use the result from (b) to ﬁnd A−1.
[Hint: Multiply the equation in (b) by A−1.]
35. Let A =
! 1
2
2 −2
"
.
(a) Determine all eigenvalues of A.
(b) Reduce A to row-echelon form, and determine
the eigenvalues of the resulting matrix. Are these
the same as the eigenvalues of A?

7.1
The Eigenvalue/Eigenvector Problem 445
36. If v1 = (1, −1) and v2 = (2, 1) are eigenvectors
of the matrix A corresponding to the eigenvalues
λ1 = 2, λ2 = −3, respectively, ﬁnd A(3v1 −v2).
37. Let v1
= (1, −1, 1), v2
= (2, 1, 3), and v3
=
(−1, −1, 2) be eigenvectors of the matrix A corre-
sponding to the eigenvalues λ1 = 2, λ2 = −2, and
λ3 = 3, respectively, and let v = (5, 0, 3).
(a) Express v as a linear combination of v1, v2,
and v3.
(b) Find Av.
38. If v1, v2, and v3 are linearly independent eigenvec-
tors of A corresponding to the eigenvalue λ, and
c1, c2, and c3 are scalars (not all zero), show that
c1v1 + c2v2 + c3v3 is also an eigenvector of A corre-
sponding to the eigenvalue λ.
39. Prove that the eigenvalues of an upper (or lower) tri-
angular matrix are just the diagonal elements of the
matrix.
40. Prove Proposition 7.1.4.
41. Let A be an n × n invertible matrix. Prove that if λ is
an eigenvalue of A, then 1/λ is an eigenvalue of A−1.
[Note: By Proposition 7.1.4, λ ̸= 0 here.]
42. Let A and B be n×n matrices, and assume that v in Rn
is an eigenvector of A corresponding to the eigenvalue
λ and also an eigenvector of B corresponding to the
eigenvalue µ.
(a) Prove that v is an eigenvector of the matrix AB.
What is the corresponding eigenvalue?
(b) Prove that v is an eigenvector of the matrix A+B.
What is the corresponding eigenvalue?
43. Let A be an n × n matrix. Prove that A and AT have
the same eigenvalues.
[Hint: Show that det(AT −λI) = det(A −λI).]
44. Let A be an n×n real matrix with complex eigenvalue
λ = a + ib, where b ̸= 0, and let v = r + is be a
corresponding eigenvector of A.
(a) Prove that r and s are nonzero vectors in Rn.
(b) Prove that {r, s} is linearly independent in Rn.
For Problems 45–50, use some form of technology to deter-
mine the eigenvalues and eigenvectors of A in the following
manner:
(1) Form the matrix A −λI.
(2) Solve the characteristic equation det(A −λI) = 0 to
determine the eigenvalues of A.
(3) For each eigenvalue λi found in (2), solve the system
(A −λi I)v = 0 to determine the eigenvectors of A.
45. ⋄A =
! 3 1
2 4
"
.
46. ⋄A =
⎡
⎣
5 34 −41
4 17 −23
5 24 −31
⎤
⎦.
47. ⋄A =
⎡
⎣
4 1 1
1 4 1
1 1 4
⎤
⎦.
48. ⋄A =
⎡
⎣
1
1 1
3 −1 2
3
1 4
⎤
⎦.
49. ⋄A =
⎡
⎣
0
1 −2
−1
0
2
2 −2
0
⎤
⎦.
50. ⋄A =
⎡
⎢⎢⎢⎢⎣
0 1 1 1 1
1 0 1 1 1
1 1 0 1 1
1 1 1 0 1
1 1 1 1 0
⎤
⎥⎥⎥⎥⎦
.
For Problems 51–56, use some form of technology to directly
determine the eigenvalues and eigenvectors of the given
matrix.
51. ⋄The matrix in Problem 45.
52. ⋄The matrix in Problem 46.
53. ⋄The matrix in Problem 47.
54. ⋄The matrix in Problem 48.
55. ⋄The matrix in Problem 49.
56. ⋄The matrix in Problem 50.

446
CHAPTER 7
Eigenvalues and Eigenvectors
7.2
General Results for Eigenvalues and Eigenvectors
In this section, we look more closely at the relationship between the eigenvalues and
eigenvectors of an n × n matrix. Our aim is to formalize several of the ideas introduced
via the examples of the previous section.
For a given n × n matrix A = [ai j], the characteristic polynomial p(λ) assumes
the form
p(λ) = det(A −λI) =
a11 −λ
a12
. . .
a1n
a21
a22 −λ . . .
a2n
...
...
...
an1
an2
. . .
ann −λ
.
Expanding this determinant yields a polynomial of degree n in λ with leading coefﬁcient
(−1)n. It follows that p(λ) can be written in the form
p(λ) = (−1)nλn + b1λn−1 + b2λn−2 + · · · + bn,
where b1, b2, . . . , bn are scalars. Since we consider the underlying vector space to be Cn,
the Fundamental Theorem of Algebra guarantees that p(λ) will have precisely n zeros
(not necessarily distinct), and hence, A will have n eigenvalues. If we let λ1, λ2, . . . , λk
denote the distinct eigenvalues of A, then p(λ) can be factored as
p(λ) = (−1)n(λ −λ1)m1(λ −λ2)m2(λ −λ3)m3 . . . (λ −λk)mk,
where, since p(λ) has degree n,
m1 + m2 + · · · + mk = n.
Thus, associated with each eigenvalue λi is a number mi, called the multiplicity of λi.
We now focus our attention on the eigenvectors of A.
DEFINITION
7.2.1
Let A be an n ×n matrix. For a given eigenvalue λi, let Ei denote the set of all vectors
v satisfying Av = λiv. Then Ei is called the eigenspace of A corresponding to the
eigenvalue λi. Thus, Ei is the solution set to the linear system (A −λi I)v = 0.
Remarks
1. Equivalently, we can say that the eigenspace Ei is the kernel of the linear trans-
formation Ti : Cn →Cn deﬁned by Ti(v) = (A −λi I)v.
2. It is important to notice that there is one eigenspace associated with each eigenvalue
of A.
3. The only difference between the eigenspace corresponding to a speciﬁc eigen-
value and the set of all eigenvectors corresponding to that eigenvalue is that the
eigenspace includes the zero vector.
Example 7.2.2
Determine all eigenspaces for the matrix A =
!
3 −1
−5 −1
"
.
Solution:
We have already computed the eigenvalues and eigenvectors of A in Exam-
ple 7.1.5. The eigenvalues of A are λ1 = −2 and λ2 = 4. The eigenvectors corresponding

7.2
General Results for Eigenvalues and Eigenvectors 447
to λ1 = −2 are all nonzero vectors of the form v = r(1, 5), where r is a constant. Thus,
the eigenspace corresponding to λ1 = −2 is
E1 = {v ∈R2 : v = r(1, 5), r ∈R}.
The eigenvectors corresponding to the eigenvalue λ2 = 4 are of the form v = s(−1, 1),
where s ̸= 0, so that the eigenspace corresponding to λ2 = 4 is
E2 = {v ∈R2 : v = s(−1, 1), s ∈R}.
□
We have one main result for eigenspaces.
Theorem 7.2.3
Let λi be an eigenvalue of A of multiplicity mi and let Ei denote the corresponding
eigenspace. Then
1. For each i, Ei is a subspace of Cn.
2. If ni denotes the dimension of Ei, then 1 ≤ni ≤mi for each i. In words, the
dimension of the eigenspace corresponding to λi is at most the multiplicity of λi.
Proof
1. From Deﬁnition 7.2.1, Ei is the null space of the matrix A −λi I and hence is
a subspace of Cn. Alternatively, Remark (1) above, coupled with Theorem 6.3.5,
provides an immediate proof.
2. The proof of this result requires some more advanced ideas about linear trans-
formations than we have developed and is therefore omitted. (See, for example,
Shilov, G.E. Linear Algebra, Dover Publications, 1977.)
Remark
The numbers mi and ni are called the algebraic multiplicity and the
geometric multiplicity of the eigenvalue λi, respectively. In what follows, the alge-
braic multiplicity of an eigenvalue is sometimes referred to simply as the multiplicity of
the eigenvalue.
Example 7.2.4
Determine all eigenspaces and their dimensions for the matrix
A =
⎡
⎣
3 −1 0
0
2 0
−1
1 2
⎤
⎦.
Solution:
A straightforward calculation yields the characteristic polynomial
p(λ) = −(λ −2)2(λ −3),
so that the eigenvalues of A are λ1 = 2 (with algebraic multiplicity 2) and λ2 = 3 (with
algebraic multiplicity 1). The eigenvectors corresponding to λ1 = 2 are determined by
solving the linear system (A −λ1I)v = 0 for v. The augmented matrix for the system is
⎡
⎣
1 −1 0 0
0
0 0 0
−1
1 0 0
⎤
⎦,

448
CHAPTER 7
Eigenvalues and Eigenvectors
which has reduced row-echelon form
⎡
⎣
1 −1 0 0
0
0 0 0
0
0 0 0
⎤
⎦.
The general solution to this system is therefore
v = (r,r, s) = r(1, 1, 0) + s(0, 0, 1),
where r and s are scalars. Thus, the eigenspace corresponding to λ1 = 2 is
E1 = {v ∈R3 : v = r(1, 1, 0) + s(0, 0, 1), r, s ∈R} = span{(1, 1, 0), (0, 0, 1)}.
We see that the linearly independent set {(1, 1, 0), (0, 0, 1)} is a basis for the eigenspace
E1, and hence, dim[E1] = 2; that is, n1 = 2.
It is easily shown that the eigenvectors corresponding to the eigenvalue λ2 = 3 are
of the form
v = (t, 0, −t) = t(1, 0, −1),
where t is a nonzero real number. Thus, the eigenspace corresponding to λ2 = 3 is
E2 = {v ∈R3 : v = t(1, 0, −1), t ∈R} = span{(1, 0, −1)}.
Therefore, {(1, 0, −1)} is a basis for this eigenspace E2, and hence, dim[E2] = 1; that
is, n2 = 1. The eigenspaces E1 and E2 are sketched in Figure 7.2.1.
□
z
v3
y
v2
x
v1
The eigenspace E2 in Example 7.2.4 is the
subspace of R3 spanned by v3 5 (1, 0, 21).
The eigenspace E1 in Example 7.2.4
is the subspace of R3 spanned by
v1 5 (1, 1, 0), v2 5 (0, 0, 1).
Figure 7.2.1: Geometrical description of the eigenspaces determined in Example 7.2.4.
We now consider the relationship between eigenvectors corresponding to distinct
eigenvalues. There is one key theorem which has already been illustrated by the examples
in the previous section.
Theorem 7.2.5
Eigenvectors corresponding to distinct eigenvalues are linearly independent.
Proof Weuseinductiontoprovetheresult. Letλ1, λ2, . . . , λm bedistincteigenvaluesof
A with corresponding eigenvectors v1, v2, . . . , vm. It is certainly true that {v1} is linearly
independent. Now suppose that {v1, v2, . . . , vk} is linearly independent for some k < m,
and consider the set {v1, v2, . . . , vk, vk+1}. We wish to show that this set of vectors is
linearly independent. Consider
c1v1 + c2v2 + · · · + ckvk + ck+1vk+1 = 0.
(7.2.1)

7.2
General Results for Eigenvalues and Eigenvectors 449
Premultiplying both sides of this equation by A and using Avi = λivi yields
c1λ1v1 + c2λ2v2 + · · · + ckλkvk + ck+1λk+1vk+1 = 0.
(7.2.2)
But, from Equation (7.2.1),
ck+1vk+1 = −(c1v1 + c2v2 + · · · + ckvk),
so that Equation (7.2.2) can be written as
c1λ1v1 + c2λ2v2 + · · · + ckλkvk −λk+1(c1v1 + c2v2 + · · · + ckvk) = 0.
That is,
c1(λ1 −λk+1)v1 + c2(λ2 −λk+1)v2 + · · · + ck(λk −λk+1)vk = 0.
Since v1, v2, . . . , vk are linearly independent, this implies that
c1(λ1 −λk+1) = 0,
c2(λ2 −λk+1) = 0,
. . . ,
ck(λk −λk+1) = 0,
and hence, since the λi are distinct, c1 = c2 = · · · = ck = 0. But now, since vk+1 ̸= 0,
it follows from Equation (7.2.1) that ck+1 = 0 also, and so, {v1, v2, . . . , vk, vk+1}
is linearly independent. We have therefore shown that the desired result is true for
{v1, v2, . . . , vk, vk+1} whenever it is true for {v1, v2, . . . , vk}, and, since the result is
true for a single eigenvector, it is true for {v1, v2, . . . , vk}, 1 ≤k ≤m.
Corollary 7.2.6
Let E1, E2, . . . , Ek denote the eigenspaces of an n × n matrix A. In each eigenspace,
choose a set of linearly independent eigenvectors, and let {v1, v2, . . . , vr} denote the
union of the linearly independent sets. Then {v1, v2, . . . , vr} is linearly independent.
Proof We argue by contradiction. Suppose that {v1, v2, . . . , vr} is linearly dependent.
Then there exist scalars c1, c2, . . . , cr, not all zero, such that
c1v1 + c2v2 + · · · + crvr = 0,
(7.2.3)
which can be written as
w1 + w2 + · · · + wk = 0,
where wi is the sum of those terms in (7.2.3) that involve vectors in Ei. Note that some
wi ̸= 0; otherwise since some c j ̸= 0, we would have a nontrivial linear combination of
the vectors in E j resulting in w j = 0, which contradicts our choice of the set of linearly
independent eigenvectors. But if wi ̸= 0, then this implies that {w1, w2, . . . , wk} is
linearly dependent, which contradicts Theorem 7.2.5. Consequently, all of the scalars in
Equation (7.2.3) must be zero, and so {v1, v2, . . . , vr} is indeed linearly independent.
Since the dimension of Rn (or Cn) is n, the maximum number of linearly independent
eigenvectors that A can have is n. In such a case, we say that A is nondefective. The
following deﬁnition introduces the appropriate terminology.
DEFINITION
7.2.7
An n×n matrix A that has n linearly independent eigenvectors is called nondefective.
In such a case, we say that A has a complete set of eigenvectors. If A has less than
n linearly independent eigenvectors, it is called defective.

450
CHAPTER 7
Eigenvalues and Eigenvectors
If A is nondefective, then any set of n linearly independent eigenvectors of A is a
basis for Rn (or Cn). Such a basis is referred to as an eigenbasis of A.
Example 7.2.8
For the matrix in Example 7.2.4, {(1, 1, 0), (0, 0, 1), (1, 0, −1)} is a complete set of
eigenvectors. Consequently, the matrix is nondefective. Likewise, the matrices in
Examples 7.1.5, 7.1.6, and 7.1.9 are all nondefective, while the matrix in Example 7.1.7
is defective.
□
Example 7.2.9
Determine whether A =
! −1
1
−1 −3
"
is defective or not.
Solution:
The characteristic polynomial of A is
p(λ) = (−1 −λ)(−3 −λ) + 1 = λ2 + 4λ + 4 = (λ + 2)2.
Thus, λ1 = −2 is an eigenvalue of multiplicity 2. The eigenvectors of A are easily found
from the augmented matrix of the system (A −λ1I)v = 0, which is
!
1
1 0
−1 −1 0
"
. We
ﬁnd that the set of eigenvectors take the form
v = (−r,r) = r(−1, 1),
where r is a nonzero real number. So the eigenspace corresponding to λ1 = −2 is
E1 = {v ∈R2 : v = r(−1, 1), r ∈R} = span{(−1, 1)}.
Thus, dim[E1] = 1 < 2, and hence A is defective.
□
The next result is a direct consequence of Theorem 7.2.5.
Corollary 7.2.10
If an n × n matrix A has n distinct eigenvalues, then it is nondefective.
Proof Denote the n distinct eigenvalues of A by λ1, λ2, . . . , λn, and denote the corre-
spondingeigenvectorsbyv1, v2, . . . , vn,respectively.ByTheorem7.2.5,{v1, v2, . . . , vn}
is linearly independent. Thus, A is nondefective.
Note that if A does not have n distinct eigenvalues, it may still be nondefective. For
instance, in Example 7.1.6, there are only two distinct eigenvalues, but there are three
linearly independent eigenvectors, which gives a complete set. The general result is
as follows.
Theorem 7.2.11
An n × n matrix A is nondefective if and only if the dimension of each eigenspace is
the same as the algebraic multiplicity mi of the corresponding eigenvalue; that is, if and
only if dim[Ei] = mi for each i.
Proof Suppose that A is nondefective, with eigenspaces E1, E2, . . . , Ek of dimensions
n1, n2, . . . , nk, respectively. Since A is nondefective, n1 +n2 +· · ·+nk = n. If ni < mi
for some i, then since ni ≤mi for each i by Theorem 7.2.3, we have
n = n1 + n2 + · · · + nk < m1 + m2 + · · · + mk = n,
a contradiction. Thus, ni = mi for each i; that is, the dimension of each eigenspace is
the same as the algebraic multiplicity of the corresponding eigenvalue.

7.2
General Results for Eigenvalues and Eigenvectors 451
Conversely, if ni = mi for each i, then
n = m1 + m2 + · · · + mk = n1 + n2 + · · · + nk,
which means that the union of the linearly independent eigenvectors that span each
eigenspace consists of n eigenvectors of A, and this union is linearly independent by
Corollary 7.2.6. Thus, A has n linearly independent eigenvectors.
Theorem 7.2.11 really says that, in order for a matrix A to be nondefective, each
eigenvalue of A must “pull its weight” in the sense that each eigenvalue must have a
corresponding eigenspace that is “large enough” in terms of dimension to match the
multiplicity of the eigenvalue. In Example 7.1.7, for instance, the eigenvalue λ1 = 1 has
multiplicity 2, but the corresponding eigenspace is only one-dimensional. Therefore, the
matrix in that example is defective.
Exercises for 7.2
Key Terms
Algebraic multiplicity, Geometric multiplicity, Eigenspace
of A corresponding to λ, Defective matrix, Nondefective ma-
trix, Complete set of eigenvectors.
Skills
• Be able to compute the algebraic and geometric mul-
tiplicities of an eigenvalue λ of a square matrix A.
• For a square matrix A, be able to compute its
eigenspaces and ﬁnd bases and the dimension of each.
• Be able to determine if a given square matrix is defec-
tive or nondefective.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An n × n matrix A is nondefective if it has n
eigenvectors.
(b) Each eigenspace of an n × n matrix is a subspace
of Rn.
(c) If A has an eigenvalue λ of algebraic multiplic-
ity 3, then the eigenspace Eλ cannot be more than
three-dimensional.
(d) If S is a set consisting of exactly one nonzero vector
from each eigenspace of a matrix A, then S is linearly
independent.
(e) If the eigenvalues of a 3×3 matrix A are λ = −1, 2, 6,
then A is nondefective.
(f) If a matrix A has a repeated eigenvalue, then it is
defective.
Problems
For Problems 1–16, determine the multiplicity of each eigen-
value and a basis for each eigenspace of the given matrix A.
Hence, determine the dimension of each eigenspace and state
whether the matrix is defective or nondefective.
1. A =
! −7
0
−3 −7
"
.
2. A =
! 1 4
2 3
"
.
3. A =
! 3 0
0 3
"
.
4. A =
!
1 2
−2 5
"
.
5. A =
!
5
5
−2 −1
"
.
6. A =
⎡
⎣
3 −4 −1
0 −1 −1
0 −4
2
⎤
⎦.
7. A =
⎡
⎣
4
0
0
0
2 −3
0 −2
1
⎤
⎦.
8. A =
⎡
⎣
3 1 0
−1 5 0
0 0 4
⎤
⎦.
9. A =
⎡
⎣
3 0
0
2 0 −4
1 4
0
⎤
⎦.

452
CHAPTER 7
Eigenvalues and Eigenvectors
10. A =
⎡
⎣
4 1
6
−4 0 −7
0 0 −3
⎤
⎦.
11. A =
⎡
⎣
2 0 0
0 2 0
0 0 2
⎤
⎦.
12. A =
⎡
⎣
7 −8
6
8 −9
6
0
0 −1
⎤
⎦.
13. A =
⎡
⎣
2 2 −1
2 1 −1
2 3 −1
⎤
⎦.
14. A =
⎡
⎣
1 −1 2
1 −1 2
1 −1 2
⎤
⎦.
15. A =
⎡
⎣
2
3 0
−1
0 1
−2 −1 4
⎤
⎦.
16. A =
⎡
⎣
0 −1 −1
−1
0 −1
−1 −1
0
⎤
⎦.
For Problems 17–22, determine whether the given matrix is
defective or nondefective.
17. A =
! 2 3
2 1
"
.
18. A =
!
6
5
−5 −4
"
.
19. A =
! 1 −2
5
3
"
.
20. A =
⎡
⎣
1 −3 1
−1 −1 1
−1 −3 3
⎤
⎦;
characteristic polynomial p(λ) = −(λ −2)2(λ + 1).
21. A =
⎡
⎣
−1 2 2
−4 5 2
−4 2 5
⎤
⎦;
characteristic polynomial p(λ) = (3 −λ)3.
22. A =
⎡
⎢⎢⎢⎢⎣
4 1 0 0 0
0 4 1 0 0
0 0 4 0 0
0 0 0 0 1
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
.
For Problems 23–28, determine a basis for each eigenspace
of A and sketch the eigenspaces.
23. A =
! −7
0
−3 −7
"
.
24. A =
! 2 1
3 4
"
.
25. A =
! 2 3
0 2
"
.
26. A =
! 5 0
0 5
"
.
27. A =
⎡
⎣
3
1 −1
1
3 −1
−1 −1
3
⎤
⎦;
characteristic polynomial p(λ) = (5 −λ)(λ −2)2.
28. A =
⎡
⎣
−3
1
0
−1 −1
2
0
0 −2
⎤
⎦.
29. The matrix
A =
⎡
⎣
2 −2 3
1 −1 3
1 −2 4
⎤
⎦
has eigenvalues λ1 = 1 and λ2 = 3.
(a) Determine a basis for the eigenspace E1 corre-
sponding to λ1 = 1 and then use the Gram-
Schmidt procedure to obtain an orthogonal basis
for E1.
(b) Are the vectors in E1 orthogonal to the vectors in
E2, the eigenspace corresponding to λ2 = 3?
30. Repeat the previous question for
A =
⎡
⎣
1 −1 1
−1
1 1
1
1 1
⎤
⎦,
assuming that A has eigenvalues λ1 = 2, λ2 = −1.
31. The matrix
A =
⎡
⎣
a
b c
a
b c
a
b c
⎤
⎦
has eigenvalues 0, 0, and a + b + c. Determine all
values of the constants a, b, and c for which A is non-
defective.

7.2
General Results for Eigenvalues and Eigenvectors 453
32. Consider the characteristic polynomial of an n × n
matrix A; namely,
p(λ) = det(A −λI) =
a11 −λ
a12
. . .
a1n
a21
a22 −λ . . .
a2n
...
...
...
an1
an2
. . .
ann −λ
(7.2.4)
which can be written in either of the following equiv-
alent forms:
p(λ) = (−1)nλn + b1λn−1 + · · · + bn,(7.2.5)
p(λ) = (λ1 −λ)(λ2 −λ) . . . (λn −λ), (7.2.6)
where λ1, λ2, . . . , λn (not necessarily distinct) are the
eigenvalues of A.
(a) Use Equations (7.2.4) and (7.2.5) to show that
b1 = (−1)n−1(a11 + a22 + · · · + ann),
bn = det(A).
Recall that the quantity a11 + a22 + · · · + ann is
called the trace of the matrix A, denoted tr(A).
(b) Use Equations (7.2.5) and (7.2.6) to show that
b1 = (−1)n−1(λ1 + λ2 + · · · + λn),
bn = λ1λ2 . . . λn.
(c) Use your results from (a) and (b) to show that
det(A) = product of the eigenvalues of A
tr(A) = sum of the eigenvalues of A
In Problems 33–36, use the result of Problem 32 to deter-
mine the sum and the product of the eigenvalues of the given
matrix A.
33. A =
⎡
⎣
−1 −2
0
6 −3 −8
−2
2
1
⎤
⎦.
34. A =
⎡
⎣
2
0 5
0 −1 1
3 −4 2
⎤
⎦.
35. A =
⎡
⎢⎢⎣
0 −3
1
1
0
2 −1
3
−1
1
1
1
1
0
5 −2
⎤
⎥⎥⎦.
36. A =
⎡
⎢⎢⎣
12 11
9 −7
2
3 −5
6
10
8
5
4
1
0
3
4
⎤
⎥⎥⎦.
37. Let Ei denote the eigenspace of A corresponding to
the eigenvalue λi. Use Theorem 4.3.2 to prove that Ei
is a subspace of Cn.
38. Let v1 and v2 be eigenvectors of A corresponding
to the distinct eigenvalues λ1 and λ2, respectively.
Prove that v1 and v2 are linearly independent. [Hint:
Model your proof on the general case considered in
Theorem 7.2.5.]
39. Let Ei denote the eigenspace of A corresponding to
the eigenvalue λi. If {vi} is a basis for E1 and {v2, v3}
is a basis for E2, prove that {v1, v2, v3} is linearly in-
dependent. [Hint: Model your proof on the general
case considered in Corollary 7.2.6.]
For Problems 40–44, use some form of technology to de-
termine the eigenvalues and a basis for each eigenspace
of the given matrix. Hence, determine the dimension of
each eigenspace and state whether the matrix is defective or
nondefective.
40. ⋄A =
⎡
⎣
1 −3 3
−1 −2 3
−1 −3 4
⎤
⎦.
41. ⋄A =
⎡
⎣
1 1 1
1 1 1
1 1 1
⎤
⎦.
42. ⋄A =
⎡
⎣
3
√
2
3
√
2
3
√
2
3
√
2
3
⎤
⎦.
43. ⋄A =
⎡
⎣
25 −6
12
11
0
6
−44
12 −21
⎤
⎦.
44. ⋄A =
⎡
⎢⎢⎣
1 2 1 2
2 1 2 1
1 2 1 2
2 1 2 1
⎤
⎥⎥⎦.
For Problems 45–46, use some form of technology to show
that the given matrix is nondefective.
45. ⋄A =
⎡
⎣
a
b
a
b
a
b
a
b
a
⎤
⎦.
46. ⋄A =
⎡
⎣
a
a
b
a
2a + b a
b
a
a
⎤
⎦.

454
CHAPTER 7
Eigenvalues and Eigenvectors
7.3
Diagonalization
A powerful application of the theory of eigenvalues and eigenvectors is diagonalization.
As motivation for this application, we consider the linear system of differential equations
dx1
dt = a11x1 + a12x2,
(7.3.1)
dx2
dt = a21x1 + a22x2,
(7.3.2)
which we write as a vector equation
x′ = Ax,
(7.3.3)
where
x =
! x1
x2
"
,
x′ =
#
x′
1
x′
2
$
,
A = [ai j].
The prime symbol denotes differentiation with respect to the independent variable t. In
general, we cannot integrate the given system directly, because each equation involves
both unknown functions x1 and x2. We say that the equations are coupled. Suppose,
however, we make a linear change of variables deﬁned by
x = Sy,
(7.3.4)
where S is an invertible matrix of constants. Then
x′ = Sy′,
so that (7.3.3) is transformed to the equivalent system
Sy′ = ASy.
Premultiplying by S−1 yields
y′ = By,
(7.3.5)
where B = S−1 AS. The question that now arises is whether it is possible to choose
S such that the system (7.3.5) can be integrated. For if this is the case, then, upon
performing the integration to ﬁnd y, the solution x to (7.3.3) can be determined from
(7.3.4).Theresultsofthissectionwillestablishthat,provided A isnondefective,allofthis
is possible.
The aim of the section therefore is to investigate matrices that are related via
B = S−1 AS. Of particular interest to us is the possibility of choosing S so that S−1 AS
has a simple structure. Of course, the question that needs answering is how simple a
form should we aim for. First we introduce some terminology and a helpful result.
DEFINITION
7.3.1
Let A and B be n × n matrices. We say A is similar to B if there exists an invertible
matrix S such that B = S−1 AS.

7.3
Diagonalization 455
Example 7.3.2
If A =
!
2 0
−1 1
"
and B =
!
22
6
−70 −19
"
, verify that B = S−1 AS, where S =
! 7 2
3 1
"
.
Solution:
It is easily shown that S−1 =
!
1 −2
−3
7
"
, so that
S−1 AS =
!
1 −2
−3
7
" !
2 0
−1 1
" ! 7 2
3 1
"
=
!
4 −2
−13
7
" ! 7 2
3 1
"
=
!
22
6
−70 −19
"
;
that is, S−1 AS = B.
□
Theorem 7.3.3
Similar matrices have the same eigenvalues (including multiplicities).
Proof If A is similar to B, then B = S−1 AS for some invertible matrix S. Thus,
det(B −λI) = det(S−1 AS −λI) = det(S−1 AS −λS−1S)
= det(S−1(A −λI)S) = det(S−1)det(A −λI)det(S)
= det(A −λI),
where we have used the fact that det(S−1) =
1
det(S) in the ﬁnal step. Consequently,
A and B have the same characteristic polynomial and hence the same eigenvalues (and
multiplicities).
We now know from Theorem 7.3.3 that A and S−1 AS have the same eigenvalues
λ1, λ2, . . . , λn. Furthermore, the simplest matrix that has these eigenvalues is the diago-
nal matrix D = diag(λ1, λ2, . . . , λn). Consequently, the simplest possible structure for
S−1 AS is
S−1 AS = diag(λ1, λ2, . . . , λn).
We have therefore been led to the question:
For an n × n matrix A, when does an invertible matrix S exist such that
S−1 AS = diag(λ1, λ2, . . . , λn)?
The answer is provided in the next crucial theorem.
Theorem 7.3.4
An n × n matrix A is similar to a diagonal matrix if and only if A is nondefective. In
such a case, if v1, v2, . . . , vn denote n linearly independent eigenvectors of A and
S = [v1, v2, . . . , vn],
then
S−1 AS = diag(λ1, λ2, . . . , λn),
where λ1, λ2, . . . , λn are the eigenvalues of A (not necessarily distinct) corresponding
to the eigenvectors v1, v2, . . . , vn.

456
CHAPTER 7
Eigenvalues and Eigenvectors
Proof If A is similar to a diagonal matrix, then there exists an invertible matrix
S = [v1, v2, . . . , vn] such that
S−1 AS = D,
(7.3.6)
where D = diag(λ1, λ2, . . . , λn) and, from Theorem 7.3.3, λ1, λ2, . . . , λn are the eigen-
values of A. Premultiplying both sides of (7.3.6) by S yields
AS = SD,
or equivalently,
[Av1, Av2, . . . , Avn] = [λ1v1, λ2v2, . . . , λnvn].
Equating corresponding column vectors we must have
Av1 = λ1v1,
Av2 = λ2v2,
. . . ,
Avn = λnvn.
Consequently, v1, v2, . . . , vn are eigenvectors of A corresponding to the eigenvalues
λ1, λ2, . . . , λn. Further, since det(S) ̸= 0, the eigenvectors are linearly independent.
Conversely, suppose A is nondefective, and let S = [v1, v2, . . . , vn], where {v1, v2,
. . . , vn} is any complete set of eigenvectors of A. Then
AS = A[v1, v2, . . . , vn] = [Av1, Av2, . . . , Avn] = [λ1v1, λ2v2, . . . , λnvn].
This can be written in the equivalent form
AS = SD,
(7.3.7)
where D = diag(λ1, λ2, . . . , λn). Since the columns of S form a linearly independent
set, det(S) ̸= 0, and hence S is invertible. Premultiplying both sides of (7.3.7) by
S−1 yields
S−1 AS = D,
so that A is indeed similar to a diagonal matrix.
DEFINITION
7.3.5
An n × n matrix that is similar to a diagonal matrix is said to be diagonalizable.
Remarks
1. Since every matrix is similar to itself (why?), note that every diagonal matrix is
diagonalizable.
2. By Theorem 7.3.4, the term “diagonalizable” is synonymous with “nondefective.”
The matrices in Examples 7.1.5, 7.1.6, 7.1.9, and 7.2.4 are diagonalizable, while
the matrix in Example 7.1.7 is not. As an exercise, the reader should write down
an appropriate matrix S in each of Examples 7.1.5, 7.1.6, 7.1.9, and 7.2.4, together
with diagonal matrices to which the given matrices are similar. To illustrate this,
we offer the following example.
Example 7.3.6
Verify that A =
⎡
⎣
3 −2 −2
−3 −2 −6
3
6
10
⎤
⎦is diagonalizable and ﬁnd a matrix S such that
S−1 AS = diag(λ1, λ2, λ3).

7.3
Diagonalization 457
Solution:
The characteristic polynomial for A is p(λ) = −(λ −4)2(λ −3) (this can
be determined, for example, by cofactor expansion of A −λI), so that the eigenvalues
of A are λ = 4, 4, 3. Corresponding linearly independent eigenvectors are
λ = 4 : v1 = (−2, 0, 1), v2 = (−2, 1, 0),
λ = 3 : v3 = (1, 3, −3).
Consequently, A is nondefective and therefore diagonalizable. If we set
S =
⎡
⎣
−2 −2
1
0
1
3
1
0 −3
⎤
⎦,
then, according to Theorem 7.3.4,
S−1 AS = diag(4, 4, 3).
It is important to note that the ordering of the eigenvalues in the diagonal matrix must be
in correspondence with the ordering of the eigenvectors in the matrix S. For example,
permuting columns 2 and 3 in S yields the matrix
S1 =
⎡
⎣
−2
1 −2
0
3
1
1 −3
0
⎤
⎦.
Since the column vectors of S1 are eigenvectors of A, Theorem 7.3.4 implies that
S−1
1
AS1 = diag(4, 3, 4).
□
Now we return to the system of differential equations (7.3.1) and (7.3.2) that moti-
vated our discussion. We assume that A is nondefective and choose S = [v1, v2] such
that S−1 AS = diag(λ1, λ2). Then the system of differential equations (7.3.5) reduces to
y′ = diag(λ1, λ2)y.
That is,
! y′
1
y′
2
"
=
! λ1
0
0
λ2
" ! y1
y2
"
,
so that
y′
1 = λ1y1,
y′
2 = λ2y2.
We see that the system of differential equations has decoupled, and both of the resulting
differential equations are easily integrated to obtain
y1(t) = c1eλ1t,
y2(t) = c2eλ2t.
From (7.3.4) we see that the solution in the original variables is
x(t) = Sy(t),
which can be written in the equivalent form
x(t) = Sy(t) = [v1, v2]
! c1eλ1t
c2eλ2t
"
.
That is,
x(t) = c1eλ1tv1 + c2eλ2tv2,
(7.3.8)

458
CHAPTER 7
Eigenvalues and Eigenvectors
where λ1, λ2 are the eigenvalues of A corresponding to the eigenvectors v1, v2. The
formula (7.3.8) looks suspiciously like a statement about the set of all solutions to the
system of differential equations being generated by taking all linear combinations of a
certain set of basic solutions (in this case, two such solutions). As mentioned previously,
a full vector space formulation for systems of linear differential equations will be given
in Chapter 9.
Example 7.3.7
Use the ideas introduced in this section to determine all solutions to
x′
1 = 9x1 + 6x2,
x′
2 = −10x1 −7x2.
Solution:
The given system can be written as
x′ = Ax,
where A =
!
9
6
−10 −7
"
. The transformed system is
y′ = (S−1AS)y,
(7.3.9)
where
x = Sy.
(7.3.10)
To determine S, we need the eigenvalues and eigenvectors of A. The characteristic
polynomial of A is
p(λ) = 9 −λ
6
−10
−7 −λ = (9 −λ)(−7 −λ) + 60 = λ2 −2λ −3 = (λ −3)(λ + 1).
Hence, A is nondefective by Corollary 7.2.10. The eigenvectors are easily computed:
λ1 =
3 : v = r(−1, 1),
λ2 = −1 : v = s(−3, 5).
We could now substitute into (7.3.8) to obtain the solution to the system, but it is more
instructive to go through the steps that led to that equation. If we set
S =
! −1 −3
1
5
"
,
then from Theorem 7.3.4,
S−1 AS = diag(3, −1),
so that the system (7.3.9) is
! y′
1
y′
2
"
=
! 3
0
0 −1
" ! y1
y2
"
.
Hence,
y′
1 = 3y1,
y′
2 = −y2.
Both of these equations can be integrated to obtain
y1(t) = c1e3t,
y2(t) = c2e−t.

7.3
Diagonalization 459
Using (7.3.10) to return to the original variables, we have
x = Sy =
! −1 −3
1
5
" !
c1e3t
c2e−t
"
=
! −c1e3t −3c2e−t
c1e3t + 5c2e−t
"
.
Consequently,
x1(t) = −c1e3t −3c2e−t,
x2(t) = c1e3t + 5c2e−t.
□
In concluding this section, we note that if a matrix A is defective, then from Theorem
7.3.4, there does not exist a matrix S such that S−1 AS = diag(λ1, λ2, . . . , λn). To handle
defective matrices, we try to ﬁnd an invertible matrix S such that S−1 AS is “close” to a
diagonal matrix. This will be pursued in Section 7.6.
Exercises for 7.3
Key Terms
Similar matrices, Diagonalizable matrix.
Skills
• Be able to ﬁnd matrices that are similar to a given
matrix A.
• Be able to list properties shared by similar matrices
and use these to help decide whether two given matri-
ces are similar or not.
• Be able to determine if a given matrix is diagonalizable
or not.
• Be able to ﬁnd n linearly independent eigenvectors for
an n×n diagonalizable matrix A and thus construct an
n × n matrix S such that S−1 AS is a diagonal matrix.
• Beabletosolvelinearsystemsofdifferentialequations
in which the coefﬁcient matrix A is diagonalizable.
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A square matrix A is diagonalizable if and only if it is
nondefective.
(b) If A is an invertible, diagonalizable matrix, then so
is A−1.
(c) If two matrices A and B have the same set of eigen-
values (including multiplicities), then they are similar.
(d) An n × n matrix is diagonalizable if and only if it has
n eigenvectors.
(e) If the characteristic polynomial p(λ) of a matrix A has
no repeated roots, then A is diagonalizable.
(f) If A is a diagonalizable matrix, then so is A2.
(g) A square matrix A is always similar to itself.
(h) If A is an n × n matrix with n odd whose eigenspaces
are all even-dimensional, then A is not diagonalizable.
(i) If (λ1, v1) and (λ2, v2) are two eigenvalue-eigenvector
pairs of a matrix A with λ1 ̸= λ2, then {v1, v2} is lin-
early independent.
Problems
For Problems 1–15, determine whether the given matrix A
is diagonalizable. Where possible, ﬁnd a matrix S such that
S−1 AS = diag(λ1, λ2, . . . , λn).
1. A =
! −9
0
4 −9
"
.
2. A =
! −1 −2
−2
2
"
.
3. A =
! −7 4
−4 1
"
.
4. A =
! 1 −8
2 −7
"
.

460
CHAPTER 7
Eigenvalues and Eigenvectors
5. A =
!
0 4
−4 0
"
.
6. A =
⎡
⎣
1 0
0
0 3
7
1 1 −3
⎤
⎦.
7. A =
⎡
⎣
1 −2
0
2 −3
0
2 −2 −1
⎤
⎦.
8. A =
⎡
⎣
0 −2 −2
−2
0 −2
−2 −2
0
⎤
⎦.
9. A =
⎡
⎣
−2 1 4
−2 1 4
−2 1 4
⎤
⎦.
10. A =
⎡
⎣
2
0 0
0
1 0
2 −1 1
⎤
⎦.
11. A =
⎡
⎣
4
0
0
3 −1 −1
0
2
1
⎤
⎦.
12. A =
⎡
⎣
0 2 −1
−2 0 −2
1 2
0
⎤
⎦.
13. A =
⎡
⎣
1 −2 0
−2
1 0
0
0 3
⎤
⎦.
14. A =
⎡
⎢⎢⎣
−1
1
0 0
0 −1
0 0
0
0 −1 0
0
0
0 1
⎤
⎥⎥⎦.
15. A =
⎡
⎢⎢⎣
1
2
3
4
−1 −2 −3 −4
2
4
6
8
−2 −4 −6 −8
⎤
⎥⎥⎦.
[Hint: Problem 32 in Section 7.2 may be useful.]
⋄For Problems 16–17, use some form of technology to de-
termine a complete set of eigenvectors for the given matrix
A. Construct a matrix S that diagonalizes A and explicitly
verify that S−1 AS = diag(λ1, λ2, . . . , λn).
16. A =
⎡
⎣
1 −3 3
−2 −4 6
−2 −6 8
⎤
⎦.
17. A =
⎡
⎢⎢⎣
3 −2
3 −2
−2
3 −2
3
3 −2
3 −2
−2
3 −2
3
⎤
⎥⎥⎦.
For Problems 18–22, use the ideas introduced in this section
to solve the given system of differential equations.
18. x′
1 = x1 + 4x2,
x′
2 = 2x1 + 3x2.
19. x′
1 = 6x1 −2x2,
x′
2 = −2x1 + 6x2.
20. x′
1 = 6x1 −x2,
x′
2 = −5x1 + 2x2.
21. x′
1 = −12x1 −7x2,
x′
2 = 16x1 + 10x2.
22. x′
1 = x2,
x′
2 = −x1.
For Problems 23–24, ﬁrst write the given system of differ-
ential equations in matrix form, and then use the ideas from
this section to determine all solutions.
23. x′
1 = 3x1 −4x2 −x3,
x′
2 = −x2 −x3,
x′
3 = −4x2 + 2x3.
24. x′
1 = x1 + x2 −x3,
x′
2 = x1 + x2 + x3,
x′
3 = −x1 + x2 + x3.
25. Let A be a nondefective matrix. Then
S−1 AS = D,
where D is a diagonal matrix. This can be written as
A = SDS−1.
Use this result to show that
A2 = SD2S−1,
and that for every positive integer k,
Ak = SDkS−1.
26. If D = diag(λ1, λ2, . . . , λn), show that for every pos-
itive integer k,
Dk = diag(λk
1, λk
2, . . . , λk
n).
27. Use the results of the preceding two problems to de-
termine A3 and A5, given that A =
! −7 −4
18
11
"
.

7.3
Diagonalization 461
28. We call a matrix B a square root of A if B2 = A.
(a) Show that if D = diag(λ1, λ2, . . . , λn), then the
matrix
√
D = diag(
+
λ1,
+
λ2, . . . ,
+
λn)
is a square root of D.
(b) Show that if A is a nondefective matrix with
S−1 AS = D for some invertible matrix S and di-
agonal matrix D, then S
√
DS−1 is a square root
of A.
(c) Find a square root for the matrix
A =
!
6 −2
−3
7
"
.
29. Prove the following properties for similar matrices:
(a) A matrix A is always similar to itself.
(b) If A is similar to B, then B is similar to A.
(c) If A is similar to B and B is similar to C, then A
is similar to C.
30. If A is similar to B, prove that AT is similar to BT .
31. InTheorem7.3.3,weprovedthatsimilarmatriceshave
the same eigenvalues. This problem investigates the
relationship between their eigenvectors. Let v be an
eigenvector of A corresponding to the eigenvalue λ.
Prove that if B = S−1 AS, then S−1v is an eigenvec-
tor of B corresponding to the eigenvalue λ.
32. Let A be a nondefective matrix and let S be a matrix
such that S−1 AS = diag(λ1, λ2, . . . , λn), where all
λi are nonzero.
(a) Prove that A is invertible.
(b) Prove that
S−1 A−1S = diag
, 1
λ1
, 1
λ2
, . . . , 1
λn
-
.
33. Let A be a nondefective matrix and let S be a matrix
such that S−1AS = diag(λ1, λ2, . . . , λn).
(a) Prove that if Q = (ST )−1, then
Q−1AT Q = diag(λ1, λ2, . . . , λn).
This establishes that AT is also nondefective.
(b) If MC denotes the matrix of cofactors of S, prove
that the column vectors of MC are linearly inde-
pendent eigenvectors of AT . [Hint: Use the ad-
joint method to determine S−1.]
34. If A =
! −2 4
1 1
"
, determine S such that S−1 AS =
diag(−3, 2), and use the result from the previous prob-
lem to determine all eigenvectors of AT .
Problems 35–37 deal with the generalization of the diagonal-
ization problem to defective matrices. A complete discussion
of this topic can be found in Section 7.6.
35. Let A be a 2×2 defective matrix. It follows from The-
orem 7.3.4 that A is not diagonalizable. However, it
can be shown that A is similar to the Jordan canonical
form matrix Jλ =
! λ
1
0
λ
"
. Thus, there exists a matrix
S = [v1, v2], such that
S−1AS = Jλ.
Prove that v1 and v2 must satisfy
(A −λI)v1 = 0
(7.3.11)
(A −λI)v2 = v1.
(7.3.12)
Equation (7.3.11) is the statement that v1 must be an
eigenvector of A corresponding to the eigenvalue λ.
Any vectors that satisfy (7.3.12) are called generalized
eigenvectors of A. The subject of generalized eigen-
vectors and Jordan canonical form matrices will be
taken up in detail in Section 7.6.
36. Show that A =
!
2 1
−1 4
"
is defective and use the
previous problem to determine a matrix S such that
S−1 AS =
! 3 1
0 3
"
.
37. Let λ be an eigenvalue of the 3 × 3 matrix A of mul-
tiplicity 3, and suppose the corresponding eigenspace
has dimension 1. It can be shown that, in this case,
there exists a matrix S = [v1, v2, v3] such that
S−1 AS =
⎡
⎣
λ
1
0
0
λ
1
0
0
λ
⎤
⎦.
Prove that v1, v2, v3 must satisfy
(A −λI)v1 = 0
(A −λI)v2 = v1
(A −λI)v3 = v2.

462
CHAPTER 7
Eigenvalues and Eigenvectors
38. In this problem, we establish that similar matri-
ces describe the same linear transformation relative
to different bases. Assume that {e1, e2, . . . , en} and
{f1, f2, . . . , fn} are bases for a vector space V and let
T : V →V be a linear transformation. Deﬁne the
n × n matrices A = [aik] and B = [bik] by
T (ek) =
n
.
i=1
ailei,
k = 1, 2, . . . , n,
(7.3.13)
T (fk) =
n
.
i=1
bikfi,
k = 1, 2, . . . , n.
(7.3.14)
If we express each of the basis vectors f1, f2, . . . , fn in
terms of the basis vectors e1, e2, . . . , en, we have that
fi =
n
.
j=1
s jie j,
i = 1, 2, . . . , n,
(7.3.15)
for appropriate scalars s ji. Thus, the matrix S = [s ji]
describes the relationship between the two bases.
(a) Prove that S is nonsingular. [Hint: Use the fact
that f1, f2, . . . , fn are linearly independent.]
(b) Use (7.3.14) and (7.3.15) to show that, for
k = 1, 2, . . . , n, we have
T (fk) =
n
.
j=1
/ n
.
i=1
s jibik
0
e j,
or equivalently,
T (fk) =
n
.
i=1
⎛
⎝
n
.
j=1
si jb jk
⎞
⎠ei.
(7.3.16)
(c) Use (7.3.13) and (7.3.15) to show that, for k =
1, 2, . . . n, we have
T (fk) =
n
.
i=1
⎛
⎝
n
.
j=1
ai js jk
⎞
⎠ei.
(7.3.17)
(d) Use (7.3.16) and (7.3.17) together with the fact
that {e1, e2, . . . , en} is linearly independent to
show that
n
.
j=1
si jb jk =
n
.
j=1
ai js jk,
1 ≤i, k ≤n,
and hence that
SB = AS.
Finally, conclude that A and B are related by
B = S−1 AS.
7.4
An Introduction to the Matrix Exponential Function
This section provides a brief introduction to the matrix exponential function. In Chapter
9, we will see that this function plays a valuable role in the analysis and solution of
systems of differential equations.
DEFINITION
7.4.1
Let A be an n × n matrix of constants. We deﬁne the matrix exponential function,
denoted eAt, by
eAt = In + At + 1
2!(At)2 + 1
3!(At)3 + · · · + 1
k!(At)k + · · · .
(7.4.1)
It can be shown that for all n × n matrices A and all values of t ∈(−∞, ∞), the
inﬁnite series appearing on the right-hand side of (7.4.1) converges to an n × n matrix.
Consequently, eAt is a well-deﬁned n × n matrix.
Properties of the Matrix Exponential Function
1. If A and B are n × n matrices satisfying AB = B A, then
e(A+B)t = eAteBt.

7.4
An Introduction to the Matrix Exponential Function 463
2. For all n × n matrices A, eAt is invertible and
(eAt)−1 = e(−A)t = e−At.
That is,
eAte−At = In.
The proofs of these results require a precise deﬁnition of convergence of an inﬁnite
series of matrices. This would take us too far astray from the main focus of this text, and
hence, the proofs are omitted. (See, for example, M.W. Hirsch and S. Smale, Differential
Equations, Dynamical Systems, and Linear Algebra, Academic Press, 1974.)
We now turn to the issue of computing eAt.
Example 7.4.2
Compute eAt if A =
! 2
0
0 −1
"
.
Solution:
In this case, we see that At =
! 2t
0
0
−t
"
, so that for any positive integer
k, we have
(At)k =
! (2t)k
0
0
(−t)k
"
.
Thus,
eAt =
! 1 0
0 1
"
+
! 2t
0
0
−t
"
+ 1
2!
! (2t)2
0
0
(−t)2
"
+ · · · + 1
k!
! (2t)k
0
0
(−t)k
"
+ · · ·
=
⎡
⎢⎢⎢⎢⎣
∞
.
k=0
1
k!(2t)k
0
0
∞
.
k=0
1
k!(−t)k
⎤
⎥⎥⎥⎥⎦
.
Hence,
eAt =
!
e2t
0
0
e−t
"
.
□
More generally, it can be shown (Problem 8) that
If A = diag(d1, d2, . . . , dn), then eAt = diag(ed1t, ed2t, . . . , ednt).
If A is not a diagonal matrix, then the computation of eAt is more involved. The
next simplest case that can arise is when A is nondefective. In this case, as we have
shown in the previous section, A is similar to a diagonal matrix, and we might suspect
that this would lead to a simpliﬁcation in the evaluation of eAt. We now show that this is
indeed the case. Suppose that A has n linearly independent eigenvectors v1, v2, . . . , vn,
and deﬁne the n × n matrix S by
S = [v1, v2, . . . , vn].
Then from Theorem 7.3.4,
S−1 AS = diag(λ1, λ2, . . . , λn),
(7.4.2)

464
CHAPTER 7
Eigenvalues and Eigenvectors
where λ1, λ2, . . . , λn are the eigenvalues of A corresponding to the eigenvectors v1,
v2, . . . , vn. Premultiplying (7.4.2) by S and postmultiplying by S−1 yields
A = SDS−1,
where
D = diag(λ1, λ2, . . . , λn).
We now compute eAt. From Deﬁnition 7.4.1,
eAt = In + At + 1
2!(At)2 + 1
3!(At)3 + · · · + 1
k!(At)k + · · ·
= SS−1 + (SDS−1)t + 1
2!(SDS−1)2t2 + · · · + 1
k!(SDS−1)ktk + · · · .
A short exercise (see Problem 25 in Section 7.3) shows that for every positive integer k,
we have (SDS−1)k = SDkS−1. Substituting this into the preceding expression for eAt
above, we get
eAt = S
!
I + Dt + 1
2!(Dt)2 + · · · + 1
k!(Dt)k + · · ·
"
S−1.
That is,
eAt = SeDt S−1.
Consequently, we have established the next theorem.
Theorem 7.4.3
Let A be a nondefective n × n matrix with linearly independent eigenvectors v1,
v2, . . . , vn, and corresponding eigenvalues λ1, λ2, . . . , λn. Then
eAt = SeDt S−1,
where S = [v1, v2, . . . , vn] and D = diag(λ1, λ2, . . . , λn).
Example 7.4.4
Determine eAt if A =
! 3 3
5 1
"
.
Solution:
The eigenvalues of A are λ1 = 6 and λ2 = −2, and therefore A is nondefec-
tive. A straightforward computation yields the following eigenvectors, which correspond
respectively to λ1 and λ2:
v1 = (1, 1)
and
v2 = (−3, 5).
It follows from Theorem 7.4.3 that if we set
S =
! 1 −3
1
5
"
and
D = diag(6, −2),
then
eAt = SeDt S−1.
That is,
eAt = S
! e6t
0
0
e−2t
"
S−1.
(7.4.3)
It is easily shown that
S−1 = 1
8
!
5 3
−1 1
"
,

7.4
An Introduction to the Matrix Exponential Function 465
so that, substituting into Equation (7.4.3),
eAt = 1
8
! 1 −3
1
5
" ! e6t
0
0
e−2t
" !
5 3
−1 1
"
= 1
8
! 1 −3
1
5
" ! 5e6t
3e6t
−e−2t
e−2t
"
.
Consequently,
eAt = 1
8
! 5e6t + 3e−2t
3(e6t −e−2t)
5(e6t −e−2t)
3e6t + 5e−2t
"
.
□
The computation of eAt when A is a defective matrix is best accomplished by
relating eAt to the solution of a linear, homogeneous system of differential equations (see
Section 9.8). Alternatively, one can use a generalization of the diagonalization procedure
to defective matrices via the machinery of Jordan canonical forms (see Section 7.6).
Exercises for 7.4
Key Terms
Matrix exponential function.
Skills
• Be able to compute the matrix exponential function
eAt for any nondefective matrix A.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The matrix exponential function eAt is only deﬁned
for a square matrix A.
(b) If A3 = 0, then eA = I + A + A2.
(c) The matrix exponential function eAt is invertible if and
only if A is invertible.
(d) The matrix exponential function eAt is an inﬁnite se-
ries that converges for all values of t.
(e) For any diagonal matrix D and invertible matrix S,
we have (SDS−1)k = Sk Dk(S−1)k for all positive
integers k.
(f) For all n × n matrices A, eA2t = (eAt)2.
Problems
For Problems 1–7, show that A is nondefective and use The-
orem 7.4.3 to ﬁnd eAt.
1. A =
! 1 2
0 3
"
.
2. A =
! 3 1
1 3
"
.
3. A =
!
0 2
−2 0
"
.
4. A =
! −1
3
−3 −1
"
.
5. A =
!
a
b
−b a
"
.
6. A =
⎡
⎣
3 −2 −2
1
0 −2
0
0
3
⎤
⎦.
7. A =
⎡
⎣
6 −2 −1
8 −2 −2
4 −2
1
⎤
⎦,
and you may assume that p(λ) = −(λ −2)2(λ −1).
8. If A = diag(d1, d2, . . . , dn), prove that
eAt = diag(ed1t, ed2t, ed3t, . . . , ednt).
9. If A =
! −3 0
0 5
"
, determine eAt and e−At.
10. Prove that for all values of the constant λ,
eλInt = eλt In.

466
CHAPTER 7
Eigenvalues and Eigenvectors
11. Consider the matrix A =
! a
b
0
a
"
. We can write
A = B + C, where B =
! a
0
0
a
"
and C =
! 0 b
0 0
"
.
(a) Verify that BC = C B.
(b) Verify that C2 = 02, and determine eCt.
(c) Use property (1) of the matrix exponential func-
tion to ﬁnd eAt.
12. If A =
!
a
b
−b a
"
, use property (1) of the matrix ex-
ponential function and Deﬁnition 7.4.1 to show that
eAt = eat
!
cos bt
sin bt
−sin bt
cos bt
"
.
An n × n matrix A that satisﬁes Ak = 0 for some k is called
nilpotent. For Problems 13–17, show that the given matrix
is nilpotent, and use Deﬁnition 7.4.1 to determine eAt.
13. A =
!
1
1
−1 −1
"
.
14. A =
! −3 9
−1 3
"
.
15. A =
⎡
⎣
−1 −6 −5
0 −2 −1
1
2
3
⎤
⎦.
16. A =
⎡
⎣
0 0 0
1 0 0
0 1 0
⎤
⎦.
17. A =
⎡
⎢⎢⎣
0 1 0 0
0 0 1 0
0 0 0 1
0 0 0 0
⎤
⎥⎥⎦.
18. Let A be the n×n matrix whose only nonzero elements
are
ai+1 i = 1, i = 1, 2, . . . , n −1.
Determine eAt. (See Problem 16 for the case n = 3.)
19. If A =
, A0
0
0
B0
-
is a block diagonal matrix with
diagonal block matrices A0 and B0, prove that
eAt =
, eA0t
0
0
eB0t
-
.
7.5
Orthogonal Diagonalization and Quadratic Forms
Symmetric matrices with real elements play an important role in many applications of linear
algebra. In particular, they arise in the study of quadratic forms (defined below), and these
appear in applications including geometry, statistics, mechanics, and electrical engineering.
In this section, we study the special properties satisﬁed by the eigenvectors of a real
symmetric matrix and show how this simpliﬁes the diagonalization problem introduced
earlier in this chapter. We also give a brief application of the theoretical results that are
obtained to quadratic forms. We begin with a deﬁnition.
DEFINITION
7.5.1
A real n × n invertible matrix A is called orthogonal if
A−1 = AT .
Example 7.5.2
Verify that the following matrix is an orthogonal matrix:
A =
⎡
⎣
1/3 −2/3
2/3
2/3 −1/3 −2/3
2/3
2/3
1/3
⎤
⎦.
Solution:
For the given matrix, we have
AAT =
⎡
⎣
1/3 −2/3
2/3
2/3 −1/3 −2/3
2/3
2/3
1/3
⎤
⎦
⎡
⎣
1/3
2/3 2/3
−2/3 −1/3 2/3
2/3 −2/3 1/3
⎤
⎦= I3.
Similarly, AT T = I3 so that AT = A−1. Consequently, A is an orthogonal matrix.
□

7.5
Orthogonal Diagonalization and Quadratic Forms 467
If we look more closely at the preceding example, we see that the column vectors
of A form an orthonormal set of vectors. The same can be said of the set of row vectors
of A. The next theorem establishes that this is a basic characterizing property of all
orthogonal matrices.4
Theorem 7.5.3
A real n × n matrix A is an orthogonal matrix if and only if the row (or column) vectors
of A form an orthonormal set of vectors.
Proof We leave this proof as an exercise (Problem 27).
We can now state the main results of this section.
Basic Results for Real Symmetric Matrices
Theorem 7.5.4
Let A be a real symmetric matrix. Then
1. All eigenvalues of A are real.
2. Real eigenvectors of A that correspond to distinct eigenvalues are orthogonal.
3. A is nondefective.
4. A has a complete set of orthonormal eigenvectors.
5. A can be diagonalized with an orthogonal matrix S such that S−1 AS is diagonal.
The proof of Theorem 7.5.4 is deferred to the end of the section. We emphasize that the
orthogonal diagonalization of a real symmetric matrix A alluded to in (5) requires that
we use a complete set of orthonormal eigenvectors in constructing S.
Example 7.5.5
Find a complete set of orthonormal eigenvectors of
A =
⎡
⎣
2 2 1
2 5 2
1 2 2
⎤
⎦,
and determine an orthogonal matrix that diagonalizes A.
Solution:
Since A is real and symmetric, a complete orthonormal set of eigenvectors
exists. To ﬁnd it, we ﬁrst seek the eigenvalues of A. The characteristic polynomial of
A is
det(A −λI) =
2 −λ
2
1
2
5 −λ
2
1
2
2 −λ
= −(λ −1)2(λ −7).
Thus, A has eigenvalues
λ1 = 1 (with multiplicity 2),
λ2 = 7 (with multiplicity 1).
Eigenvalue λ1 = 1: In this case, the system (A −λ1I)v = 0 reduces to the single
equation
v1 + 2v2 + v3 = 0,
4Because of this characterization of orthogonal matrices, it might perhaps be more appropriate to call
them orthonormal matrices. However, this would run contrary to the vast literature that has established the
fundamental term orthogonal matrix as we have done in Deﬁnition 7.5.1.

468
CHAPTER 7
Eigenvalues and Eigenvectors
which has solution (−2r −s,r, s), so that the corresponding eigenvectors are
x = (−2r −s,r, s) = r(−2, 1, 0) + s(−1, 0, 1).
Two linearly independent eigenvectors corresponding to the eigenvalue λ1 = 1 are
x1 = (−1, 0, 1),
x2 = (−2, 1, 0).
Although {x1, x2} is not an orthonormal set, we can apply the Gram-Schmidt process in
Section 5.3 to replace this set with an orthonormal set {u1, u2} as follows.
First, let v1 = x1 and v2 = x2 −P(x2, v1) = x2 −v1 = (−1, 1, −1). Now {v1, v2}
is an orthogonal set, and we can normalize each vector to get
u1 =
,
−1
√
2
, 0, 1
√
2
-
and
u2 =
,
−1
√
3
, 1
√
3
, −1
√
3
-
.
Thus, {u1, u2} is an orthonormal set of eigenvectors of A corresponding to λ1 = 1.
Eigenvalue λ2 = 7: The reader can check that Gaussian elimination applied to the sys-
tem (A −λ2I)v = 0 yields the eigenvector
v3 = (1, 2, 1).
Replacing this with a unit vector, we have
u3 =
, 1
√
6
, 2
√
6
, 1
√
6
-
.
Notice that u3 is orthogonal to both u1 and u2, as guaranteed by Theorem 7.5.4 (2). It
follows that a complete set of orthonormal eigenvectors for the matrix A is {u1, u2, u3};
that is,
5,
−1
√
2
, 0, 1
√
2
-
,
,
−1
√
3
, 1
√
3
, −1
√
3
-
,
, 1
√
6
, 2
√
6
, 1
√
6
-6
.
If we set
S =
⎡
⎢⎢⎣
−1
√
2
−1
√
3
1
√
6
0
1
√
3
2
√
6
1
√
2
−1
√
3
1
√
6
⎤
⎥⎥⎦,
then S is an orthogonal matrix satisfying
ST AS = S−1 AS = diag(1, 1, 7).
□
Notice that in this example, we took the linearly independent eigenvectors associated
with each eigenvalue separately and applied Gram-Schmidt to them to get an orthonormal
basis for each eigenspace separately. It is important to realize that this is a legal process
to apply to any n × n matrix. However, the advantage of a real symmetric matrix is
that, for such a matrix, the eigenvectors from one eigenspace and the eigenvectors from
another eigenspace are already orthogonal. This is not true for an arbitrary matrix.
When we apply Gram-Schmidt to a set of eigenvectors within a single eigenspace,
the vectors resulting from the process are still eigenvectors belonging to that eigenspace.
However,applyingGram-Schmidttovectorsarisingindifferenteigenspaceswillresultin
vectors wi and ui that are not even eigenvectors at all! Hence, for an arbitrary matrix, we
have no possibility of “orthogonalizing” eigenvectors occurring in distinct eigenspaces.

7.5
Orthogonal Diagonalization and Quadratic Forms 469
Quadratic Forms
If A is a symmetric n × n real matrix and x is a column n-vector, then an expression of
the form
xT Ax
is called a quadratic form. We can consider a quadratic form as deﬁning a mapping
from Rn to R. For example, if n = 2, then
xT Ax = [x1 x2]
! a11
a12
a12
a22
" ! x1
x2
"
= [x1 x2]
! a11x1 + a12x2
a12x1 + a22x2
"
= a11x2
1 + 2a12x1x2 + a22x2
2.
We see that this is indeed quadratic in x1 and x2. Quadratic forms arise in many appli-
cations. For example, in geometry, the conic sections have Cartesian equations that can
be expressed as
xT Ax = c,
whereas quadric surfaces have equations of this same form, where now A is a 3 × 3
matrix and x is a vector in R3. In mechanics, the kinetic energy K of a physical system
with n degrees of freedom (and time independent constraints) can be written as
K = xT Ax,
where A is an n × n matrix and x is a vector of (generalized) velocities. The question
that we are going to address is whether it is possible to make a linear change of variables
x = Sy
(7.5.1)
that enables us to simplify a quadratic form by eliminating the cross terms. Equivalently,
we want to reduce a quadratic form to a sum of squares. Making the change of variables
(7.5.1) in the general quadratic form yields
xT Ax = (Sy)T A(Sy) = yT (ST AS)y.
(7.5.2)
If we choose S = [w1, w2, . . . , wn], where {w1, w2, . . . , wn} is any complete set of
orthonormal eigenvectors for A, then Theorem 7.5.4 implies that
ST AS = diag(λ1, λ2, . . . , λn),
where λ1, λ2, . . . , λn are the eigenvalues of A corresponding to w1, w2, . . . , wn. With
this choice of S, the right-hand side of Equation (7.5.2) reduces to
[y1 y2 . . . yn]
⎡
⎢⎢⎢⎣
λ1
λ2
...
λn
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
y1
y2
...
yn
⎤
⎥⎥⎥⎦= λ1y2
1 + λ2y2
2 + · · · + λny2
n,
and we have accomplished our goal of reducing the quadratic form to a sum of squares.
This result is summarized in the following theorem.

470
CHAPTER 7
Eigenvalues and Eigenvectors
Theorem 7.5.6
(Principal Axes Theorem)
Let A be an n × n real symmetric matrix. Then the quadratic form xT Ax can be reduced
to a sum of squares by the change of variables x = Sy, where S is an orthogonal
matrix whose column vectors {w1, w2, . . . , wn} are any complete orthonormal set of
eigenvectors for A. The transformed quadratic form is
λ1y2
1 + λ2y2
2 + · · · + λny2
n,
where λ1, λ2, . . . , λn are the eigenvalues of A corresponding to w1, w2, . . . , wn.
The column vectors of the matrix S in the Principal Axes Theorem are called principal
axes for the quadratic form xT Ax.
Example 7.5.7
By transforming to principal axes, reduce the quadratic form xT Ax for
A =
⎡
⎣
2 2 1
2 5 2
1 2 2
⎤
⎦
to a sum of squares.
Solution:
In the previous example, we found an orthogonal matrix that diagonalizes
A; namely,
S =
⎡
⎢⎢⎣
−1
√
2
−1
√
3
1
√
6
0
1
√
3
2
√
6
1
√
2
−1
√
3
1
√
6
⎤
⎥⎥⎦.
Furthermore, the corresponding eigenvalues of A are (1, 1, 7). Consequently, the change
of variables x = Sy reduces the quadratic form to
y2
1 + y2
2 + 7y2
3.
This is quite a simpliﬁcation from the quadratic form expressed in the original variables;
namely,
xT Ax = [x1 x2 x3]
⎡
⎣
2 2 1
2 5 2
1 2 2
⎤
⎦
⎡
⎣
x1
x2
x3
⎤
⎦
= 2x2
1 + 5x2
2 + 2x2
3 + 4x1x2 + 2x1x3 + 4x2x3.
□
Example 7.5.8
By transforming to principal axes, identify the conic section with Cartesian equation
5x2
1 + 2x1x2 + 5x2
2 = 1.
Solution:
We ﬁrst write the quadratic form appearing on the left-hand side of the
given equation in matrix form. If we set A =
! 5 1
1 5
"
, then the given equation is
xT Ax = 1.
(7.5.3)

7.5
Orthogonal Diagonalization and Quadratic Forms 471
Observe that A has characteristic polynomial p(λ) = (λ −6)(λ −4), so that the eigen-
values of A are λ1 = 6 and λ2 = 4, with corresponding orthonormal eigenvectors
w1 =
! 1
√
2
, 1
√
2
"
and
w2 =
!
−1
√
2
, 1
√
2
"
.
Therefore, if we set S =
⎡
⎣
1
√
2
−1
√
2
1
√
2
1
√
2
⎤
⎦, then under the change of variables x = Sy,
Equation (7.5.3) reduces to
6y2
1 + 4y2
2 = 1,
(7.5.4)
which is the equation of an ellipse. Geometrically, vectors w1 and w2 are obtained by
rotating the standard basis vectors e1 and e2, respectively, counterclockwise through an
angle π/4 radians. Relative to the Cartesian coordinate system (y1, y2) corresponding to
the principal axes w1, w2, the ellipse has the simple equation (7.5.4). Figure 7.5.1 shows
how the conic looks relative to the standard Cartesian axes and the rotated principal axes.
□
20.5
0.5
20.5
0.5
y1
y2
x1
x2
w1
w2
21
1
21
1
Figure 7.5.1: Relative to Cartesian axes (x1, x2), cross terms come into the equation for the
ellipse. Rotating to the principal axes (y1, y2) eliminates these cross terms.
We conclude this section with a proof of Theorem 7.5.4. The following preliminary
lemma will be useful for the proof.
Lemma 7.5.9
Let v1 and v2 be vectors in Rn (or Cn), and let A be an n × n real symmetric matrix.
Then
⟨Av1, v2⟩= ⟨v1, Av2⟩.
Proof The key to the proof is to note that the inner product of two column vectors in
Rn or Cn can be written in matrix form as
[⟨x, y⟩] = xT y.
Applying this to the vectors Av1 and v2 yields
[⟨Av1, v2⟩] = (Av1)T v2 = vT
1 AT v2 = vT
1 (Av2) = [⟨v1, Av2⟩],
from which the result follows directly.

472
CHAPTER 7
Eigenvalues and Eigenvectors
Using this lemma, we can now give the
Proof of Theorem 7.5.4:
1. We must prove that every eigenvalue of a real symmetric matrix A is real. Let
(λ1, v1) be an eigenvalue/eigenvector pair for A; that is
Av1 = λ1v1.
(7.5.5)
We will show that λ1 = λ1, which will prove (1). Taking the inner product of both
sides of (7.5.5) with the vector v1 yields
⟨Av1, v1⟩= ⟨λ1v1, v1⟩.
Using the properties of the inner product, we obtain
⟨Av1, v1⟩= λ1||v1||2.
(7.5.6)
Taking the complex conjugate of (7.5.6) yields (remember that ||v1|| is a real
number)
⟨Av1, v1⟩= λ1||v1||2.
Using the fact that ⟨u, v⟩= ⟨v, u⟩,
⟨v1, Av1⟩= λ1||v1||2.
(7.5.7)
Subtracting (7.5.7) from (7.5.6) and using Lemma 7.5.9 yields
0 = (λ1 −λ1)||v1||2.
(7.5.8)
However, v1 ̸= 0 since it is an eigenvector of A. Consequently, from Equation
(7.5.8), we must have
λ1 = λ1.
2. It follows from part (1) that all eigenvalues of A are necessarily real. We can there-
fore take the underlying vector space as Rn, so that the corresponding eigenvectors
of A will also be real. Let (λ1, v1) and (λ2, v2) be two such eigenvalue/eigenvector
pairs with λ1 ̸= λ2. Then, in addition to (7.5.5) we also have
Av2 = λ2v2.
(7.5.9)
We must prove that ⟨v1, v2⟩= 0. Taking the inner product of (7.5.5) with v2 and
the inner product of (7.5.9) with v1 and pulling the scalars out of the inner products,
we get, respectively,
⟨Av1, v2⟩= λ1⟨v1, v2⟩,
⟨v1, Av2⟩= λ2⟨v1, v2⟩.
Subtracting the second equation from the ﬁrst and using Lemma 7.5.9, we obtain
0 = (λ1 −λ2)⟨v1, v2⟩.
Since λ1 −λ2 ̸= 0 by assumption, it follows that ⟨v1, v2⟩= 0.
3. The proof of this part utilizes some ideas from the next section, so we postpone
the proof of this fact until the end of the next section.

7.5
Orthogonal Diagonalization and Quadratic Forms 473
4. Eigenvectors corresponding to distinct eigenvalues are orthogonal from (2). Now
suppose that the eigenvalue λi has multiplicity mi. Then, since A is nondefec-
tive (from (3)), it follows from Theorem 7.2.11 that we can ﬁnd mi linearly in-
dependent eigenvectors corresponding to λi. These vectors span the eigenspace
corresponding to λi, and hence, we can use the Gram-Schmidt process to ﬁnd mi
orthonormal eigenvectors(correspondingtoλi)thatspanthiseigenspace.Proceed-
ing in this manner for each eigenvalue, we obtain a complete set of orthonormal
eigenvectors.
5. If we denote the orthonormal set of eigenvectors in (4) by {w1, w2, . . . , wn} and let
S = [w1, w2, . . . , wn], then Theorem 7.5.3 implies that S is an orthogonal matrix.
Consequently,
ST AS = S−1 AS = diag(λ1, λ2, . . . , λn),
where we have used Theorem 7.3.4.
Exercises for 7.5
Key Terms
Orthogonal matrix, Complete set of orthonormal eigenvec-
tors, Quadratic forms, Principal axes.
Skills
• Be able to determine whether a given matrix A is or-
thogonal or not.
• Be able to call upon the properties of real symmetric
matrices given in Theorem 7.5.4.
• Be able to construct a complete set of orthonormal
eigenvectors for a given matrix A and construct an
orthogonal matrix S such that ST AS is a diagonal
matrix.
• Determine a set of principal axes for a given quadratic
form and reduce it to a sum of squares by an appropri-
ate change of variables.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If A is an n ×n orthogonal matrix, then A is invertible
and AAT = AT A = In.
(b) A real matrix A whose characteristic polynomial is
p(λ) = λ3 + λ cannot be symmetric.
(c) A real matrix A with eigenvectors v1 =
! 1
2
"
and
v2 =
! 1
3
"
cannot be symmetric.
(d) If A is an n×n real, symmetricmatrixwitheigenvalues
λ1, λ2, . . . , λn corresponding to a complete orthonor-
mal set of eigenvectors for A, then the quadratic form
xT Ax is transformed into λ1y2
1 + λ2y2
2 + · · · + λny2
n.
(e) For any n × n real, symmetric matrix A and vectors v
and w in Rn, we have ⟨Av, w⟩= ⟨v, Aw⟩.
(f) If A and B are n × n orthogonal matrices, then AB is
also an orthogonal matrix.
(g) A real n × n matrix A is orthogonal if and only if its
row (or column) vectors are orthogonal unit vectors.
(h) Any real matrix with a complete set of orthonormal
eigenvectors is symmetric.
Problems
For Problems 1–13, determine an orthogonal matrix S such
that ST AS = diag(λ1, λ2, . . . , λn), where A denotes the
given matrix.
1.
! 1
4
4 −5
"
.

474
CHAPTER 7
Eigenvalues and Eigenvectors
2.
! 2
2
2 −1
"
.
3.
! 4 6
6 9
"
.
4.
! 1 2
2 1
"
.
5.
⎡
⎣
0
0 3
0 −2 0
3
0 0
⎤
⎦.
6.
⎡
⎣
1 2 1
2 4 2
1 2 1
⎤
⎦.
7.
⎡
⎣
2 0 0
0 3 1
0 1 3
⎤
⎦.
8.
⎡
⎣
0 1 0
1 0 0
0 0 1
⎤
⎦.
9.
⎡
⎣
1 1 −1
1 1
1
−1 1
1
⎤
⎦.
10.
⎡
⎣
1 0 −1
0 1
1
−1 1
0
⎤
⎦.
11.
⎡
⎣
3 3 4
3 3 0
4 0 3
⎤
⎦.
You may assume that p(λ) = (λ + 2)(λ −3)(8 −λ).
12.
⎡
⎣
−3
2
2
2 −3
2
2
2 −3
⎤
⎦.
You may assume that p(λ) = (1 −λ)(λ + 5)2.
13.
⎡
⎣
0 1 1
1 0 1
1 1 0
⎤
⎦.
You may assume that p(λ) = (λ + 1)2(2 −λ).
For Problems 14–17, determine a set of principal axes for
the given quadratic form, and reduce the quadratic form to a
sum of squares.
14. xT Ax,
A =
! 1 3
3 1
"
.
15. xT Ax,
A =
! 5 2
2 5
"
.
16. xT Ax,
A =
⎡
⎣
1 1 −1
1 1
1
−1 1
1
⎤
⎦.
17. ⋄xT Ax, A =
⎡
⎢⎢⎣
3 1 3 1
1 3 1 3
3 1 3 1
1 3 1 3
⎤
⎥⎥⎦.
18. Consider the general 2 × 2 real symmetric matrix
A =
! a
b
b
c
"
. Prove that A has an eigenvalue of mul-
tiplicity two if and only if it is a scalar matrix (that is,
a matrix of the form r I2, where r is a constant).
19. (a) Let A be an n × n real symmetric matrix. Prove
that if λ is an eigenvalue of A of multiplicity
n, then A is a scalar matrix. [Hint: Prove that
there exists an orthogonal matrix S such that
ST AS = λIn, and then solve for A.]
(b) State and prove the corresponding result for gen-
eral n × n matrices.
20. The 2 × 2 real symmetric matrix A has two distinct
eigenvalues λ1 and λ2. If v1 = (1, 2) is an eigenvector
of A corresponding to the eigenvalue λ1, determine an
eigenvector corresponding to λ2.
21. The 2 × 2 real symmetric matrix A has two distinct
eigenvalues λ1 and λ2.
(a) If v1 = (a, b) is an eigenvector of A correspond-
ing to the eigenvalue λ1, determine an eigenvector
corresponding to λ2, and hence ﬁnd an orthogonal
matrix S such that ST AS = diag(λ1, λ2).
(b) Use your result from part (a) to ﬁnd A. [Your
answer will involve λ1, λ2, a, and b.]
22. The 3×3 real symmetric matrix A has eigenvalues λ1
and λ2 (multiplicity 2).
(a) If v1 = (1, −1, 1) spans the eigenspace E1, deter-
mine a basis for E2 and hence ﬁnd an orthogonal
matrix S, such that ST AS = diag(λ1, λ2, λ2).
(b) Use your result from part (a) to ﬁnd A.

7.6
Jordan Canonical Forms 475
Problems 23–26 deal with the eigenvalue/eigenvector prob-
lem for n × n real skew-symmetric matrices.
23. Let A be an n × n real skew-symmetric matrix.
(a) Prove that for all v1 and v2 in Cn,
⟨Av1, v2⟩= −⟨v1, Av2⟩,
where ⟨, ⟩denotes the standard inner product in
Cn. [Hint: See Lemma 7.5.9.]
(b) Prove that all nonzero eigenvalues of A are pure
imaginary (λ = −λ). [Hint: Model your proof
after that of (1) in Theorem 7.5.4.]
24. It follows from the previous problem that the only real
eigenvalue that a real skew-symmetric matrix can pos-
sess is λ = 0. Use this to prove that if A is an n × n
real skew-symmetric matrix, with n odd, then A nec-
essarily has zero as one of its eigenvalues.
25. Determine all eigenvalues and corresponding eigen-
vectors of the matrix
A =
⎡
⎣
0 4 −4
−4 0 −2
4 2
0
⎤
⎦.
26. Repeat Problem 25 for the matrix
A =
⎡
⎣
0 −1 −6
1
0
5
6 −5
0
⎤
⎦.
27. Prove Theorem 7.5.3.
7.6
Jordan Canonical Forms
The diagonalization of an n × n matrix A described in Section 7.3 is not possible when
the matrix A is defective. With this in mind, the question at hand naturally becomes: For
a defective matrix A, is an “approximation” to the diagonalization procedure available?
The answer is afﬁrmative, provided we allow the matrices arising in the theory to have
complex entries. The approximation we are alluding to here gives rise to the Jordan
canonical form of A, and the present section aims to give a few examples and introduce
the theory of Jordan canonical forms.
For nondefective matrices A, one can construct an invertible matrix S and a diagonal
matrix D such that S−1 AS = D. The matrix S is constructed by placing n linearly
independent eigenvectors as its columns. For a defective matrix, we simply do not have
“enough” linearly independent eigenvectors to form such a matrix S. The strategy then
becomes to search for additional linearly independent vectors that are “close” to being
eigenvectors of A. The deﬁnition below makes this more precise.
DEFINITION
7.6.1
Let A be an n × n matrix. A nonzero vector v is called a generalized eigenvector of
A corresponding to the eigenvalue λ if
(A −λI)p v = 0
for some positive integer p. That is, v belongs to the null space of the matrix (A−λI)p.
Remarks
1. By setting p = 1, we see that every eigenvector of A is a generalized eigenvector
of A.
2. If p is the smallest positive integer such that (A −λI)p v = 0, then the vector
(A −λI)p−1 v is a (regular) eigenvector of A corresponding to λ, since it belongs
to the null space of A −λI.

476
CHAPTER 7
Eigenvalues and Eigenvectors
Example 7.6.2
Returning to the defective matrix A =
! −1
1
−1 −3
"
in Example 7.2.9, note that (A+2I)2
= 02, which implies that every nonzero vector v is a generalized eigenvector of A
corresponding to λ1 = −2. For instance, along with the eigenvector v1 = (−1, 1), we
may choose the vector v2 = (1, 0) as a generalized eigenvector such that {v1, v2} is
linearly independent. Since {v1, v2} is a linearly independent set of two vectors in R2, it
is a basis of R2.
□
Example 7.6.3
Determine generalized eigenvectors for the matrix
B =
⎡
⎣
1 1 0
0 1 2
0 0 3
⎤
⎦.
Solution:
Direct calculation shows that the characteristic polynomial for B is
p(λ) = (3 −λ)(1 −λ)2,
so the eigenvalues of B are λ1 = 3 and λ2 = 1 (with multiplicity 2). We now look for
eigenvectors corresponding to each of these eigenvalues.
Eigenvalue λ1 = 3: The augmented matrix of the linear system (B −λ1I)v = 0 is
⎡
⎣
−2
1 0 0
0 −2 2 0
0
0 0 0
⎤
⎦,
and we quickly ﬁnd a solution v1 = (1, 2, 2), which is an eigenvector of B corresponding
to λ1 = 3.
Eigenvalue λ2 = 1: The augmented matrix of the linear system (B −λ2I)v = 0 is
⎡
⎣
0 1 0 0
0 0 2 0
0 0 2 0
⎤
⎦,
so only one linearly independent eigenvector, v2 = (1, 0, 0), is obtained by solving
this system. Hence, B is defective. We therefore seek to use a generalized eigenvector
that is not parallel to v2 as a substitute for a second linearly independent eigenvector
corresponding to λ2. To do this, we compute
(B −λ2I)2 =
⎡
⎣
0 0 2
0 0 4
0 0 4
⎤
⎦,
which, in addition to multiples of v2, contains the vector v3 = (0, 1, 0) in its null
space. Therefore, v3 is a generalized eigenvector of B corresponding to λ2 = 1. It is
worth noting, however, that in this case, any nonzero vector of the form (a, b, 0) is a
legitimate generalized eigenvector of B corresponding to λ2 = 1. Computing the powers
(B −λ2I)3, (B −λ2I)4, . . . , we see that no further generalized eigenvectors of B can be
found. Observe that {v2, v3} is linearly independent and that any generalized eigenvector
of B corresponding to λ2 = 1 is a linear combination of v2 and v3. Note therefore that
{v1, v2, v3} is a basis of R3 consisting of generalized eigenvectors.
□

7.6
Jordan Canonical Forms 477
In Examples 7.6.2 and 7.6.3, we see that by using generalized eigenvectors, we are
able to obtain n linearly independent vectors. This is not merely a coincidence, and it
turns out that for any n ×n matrix A, one can always ﬁnd a linearly independent set of n
vectors in Cn (and hence a basis for Cn over C) consisting of generalized eigenvectors.5
For instance, it can be shown that a union of linearly independent sets of generalized
eigenvectors for different eigenvalues results in a linearly independent set of vectors.
In view of the diagonalization procedure described in Section 7.3, we are tempted
in Examples 7.6.2 and 7.6.3 above to construct a matrix S whose columns consist of
a linearly independent set of generalized eigenvectors of A and compute6 S−1 AS and
S−1BS, respectively. Unlike the case of a diagonalizable matrix, where we can use any
linearly independent set of eigenvectors, in this case the set of generalized eigenvectors
must be carefully chosen. We will see how to make this choice below. In Example 7.6.2,
for instance, we can construct the matrix S =
! −1 1
1 0
"
. Computing S−1 AS gives
J1 = S−1 AS =
! −2
1
0 −2
"
.
(7.6.1)
Likewise, in Example 7.6.3, we can construct the matrix S =
⎡
⎣
1 1 0
2 0 1
2 0 0
⎤
⎦. Computing
S−1BS gives
J2 = S−1BS =
⎡
⎣
3 0 0
0 1 1
0 0 1
⎤
⎦.
(7.6.2)
The matrices J1 and J2 appearing in (7.6.1) and (7.6.2) are almost diagonal matrices,
with the eigenvalues along the main diagonal. The only discrepancy is the appearance of
some 1’s along the superdiagonals of these matrices, where the superdiagonal consists
of the elements ai,i+1 appearing on the diagonal line directly above and parallel to the
main diagonal. This is a characteristic feature of matrices in Jordan canonical form. A
formal description will be given momentarily, but one feature we observe here is that a
matrix in Jordan canonical form consists of block matrices inside it, each containing an
eigenvalue on the main diagonal and 1’s on the superdiagonal. These block matrices are
so important in this theory that they enjoy a special name.
DEFINITION
7.6.4
If λ is a real number, then a square matrix of the form
Jλ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
λ
1
0
0
. . .
0
0
λ
1
0
. . .
0
0
0
λ
1
. . .
0
...
...
...
...
...
...
0
0
. . .
0
λ
1
0
0
0
. . .
0
λ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
is called a Jordan block corresponding to λ.
5The proof of this fact is beyond the scope of this text, but can be found in more advanced texts on linear
algebra, such as S. Friedberg, A. Insel, L. Spence, Linear Algebra, Prentice Hall, 4th edition (2002).
6Here again note that such S is invertible, since its columns are linearly independent.

478
CHAPTER 7
Eigenvalues and Eigenvectors
Example 7.6.5
The matrices
! −5
1
0 −5
"
, [2], and
⎡
⎢⎢⎢⎢⎣
8 1 0 0 0
0 8 1 0 0
0 0 8 1 0
0 0 0 8 1
0 0 0 0 8
⎤
⎥⎥⎥⎥⎦
are Jordan blocks.
□
DEFINITION
7.6.6
Any square matrix consisting of Jordan blocks centered along the main diagonal and
zeros elsewhere is said to be in Jordan canonical form.
Example 7.6.7
The matrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−4
1
0
0 −4
1
0
0 −4
6 1
0 6
8 1
0 8
8
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
wheretheemptyelementsareallzero,isinJordancanonicalform.Moreover,thematrices
J1 and J2 in (7.6.1) and (7.6.2) are in Jordan canonical form.
□
In general, since each Jordan block is an upper triangular matrix, every matrix in
Jordan canonical form is upper triangular. Notice, too, that the size of the Jordan blocks
in a Jordan canonical form can vary. For instance, the matrix in Example 7.6.7 contains
one 3 × 3 block, two 2 × 2 blocks, and two 1 × 1 blocks.
It turns out that, by a careful selection and arrangement of n linearly indepen-
dent generalized eigenvectors of an n × n matrix A in the columns of a matrix S,
we will have S−1 AS = J, a matrix in Jordan canonical form. Therefore, we have
the following.
Theorem 7.6.8
Every square matrix A is similar to a matrix J that is in Jordan canonical form .
Proof We refer the reader to the text by S. Friedberg, A. Insel, L. Spence, Linear
Algebra, Prentice Hall (2002).
Remark
It can be shown that the matrix J to which the square matrix A in Theorem
7.6.8 is similar is uniquely determined, up to a rearrangement of the Jordan blocks. Thus,
we can write JCF(A) for the unique Jordan canonical form of A.
As we indicated above, considerable care must be exercised in selecting and arrang-
ing the generalized eigenvectors in the columns of S in order to ensure that S−1 AS is in
Jordan canonical form. To describe this, we need the following terminology.

7.6
Jordan Canonical Forms 479
DEFINITION
7.6.9
Let A be an n × n matrix, and let v be a generalized eigenvector of A correspond-
ing to λ. If p is the smallest positive integer such that (A −λI)pv = 0, then the
ordered set
{(A −λI)p−1v, (A −λI)p−2v, . . . , (A −λI)v, v}
(7.6.3)
is called a cycle of generalized eigenvectors of A corresponding to λ. The integer
p is called the length of the cycle.
We often refer to the vector v in (7.6.3) as the initial vector of the cycle, and the
vector (A −λI)p−1v as the terminal vector of the cycle.
Theorem 7.6.10
The cycle of generalized eigenvectors in (7.6.3) is linearly independent.
Proof Assume that
a0v + a1(A −λI)v + a2(A −λI)2v + · · · + ap−1(A −λI)p−1v = 0.
(7.6.4)
Multiplying (7.6.4) through by (A −λI)p−1 and using the fact that (A −λI)pv = 0,
we have
a0(A −λI)p−1v = 0.
Thus, a0 = 0. Substituting this into (7.6.4) yields
a1(A −λI)v + a2(A −λI)2v + · · · + ap−1(A −λI)p−1v = 0.
(7.6.5)
Now multiply (7.6.5) through by (A −λI)p−2 to get
a1(A −λI)p−1v = 0,
which implies a1 = 0.
Continuing in this way, we deduce that a0 = a1 = a2 = · · · = ap−1 = 0, as needed.
Notice that the terminal vector of (7.6.3) is the one (and only) vector in the cycle that
is an eigenvector of A corresponding to λ. Hence, every cycle of generalized eigenvectors
of A can be associated with exactly one eigenvector of A. In fact, if λ is an eigenvalue of A
of multiplicity m, then it can be shown (S. Friedberg, A. Insel, L. Spence, Linear Algebra,
Prentice Hall (2002)) that there are m linearly independent generalized eigenvectors of
A corresponding to λ.
The basic idea in constructing the matrix S so that S−1 AS is in Jordan canonical
form is to put vectors occurring in various cycles of the type (7.6.3) in adjacent columns
and in the same order as listed in (7.6.3). For each cycle of generalized eigenvectors of
length p corresponding to λ, the matrix S−1 AS will contain a Jordan block of size p × p
corresponding to λ.
We summarize the above observations as follows:
1. The number of Jordan blocks in JCF(A) is the number of linearly independent
eigenvectors of A.
2. The size of a Jordan block is equal to the number of vectors in the
corresponding cycle of generalized eigenvectors of A.

480
CHAPTER 7
Eigenvalues and Eigenvectors
Rather than prove these observations, which is best left for another course on linear
algebra, we content ourselves with some examples. The observations listed above can
often considerably simplify the process of determining JCF(A).
Example 7.6.11
Suppose A is a 4×4 matrix with eigenvalues λ, λ, λ, λ. List the possible Jordan canonical
forms of A.
Solution:
Since JCF(A) is a 4 × 4 matrix, the possible Jordan block decompositions
are as follows:
(a) one 4 × 4 block,
(b) one 3 × 3 block and one 1 × 1 block,
(c) two 2 × 2 blocks,
(d) one 2 × 2 block and two 1 × 1 blocks,
(e) four 1 × 1 blocks.
Note that in arranging the blocks, we will generally choose for each λ to place larger
blocks above smaller blocks.
The Jordan canonical forms associated with these ﬁve cases are, respectively:
⎡
⎢⎢⎣
λ
1
0
0
0
λ
1
0
0
0
λ
1
0
0
0
λ
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
λ
1
0
0
0
λ
1
0
0
0
λ 0
0
0
0
λ
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
λ
1
0
0
0
λ 0
0
0
0
λ
1
0
0
0
λ
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
λ
1
0
0
0
λ 0
0
0
0
λ 0
0
0
0
λ
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
λ 0
0
0
0
λ 0
0
0
0
λ 0
0
0
0
λ
⎤
⎥⎥⎦.
By calculating a set of linearly independent eigenvectors of A, we can narrow down
which of the ﬁve matrices listed above can be the JCF(A). For example, if A has only
one linearly independent eigenvector, then JCF(A) is the ﬁrst matrix, which has only one
Jordan block. Likewise, if A has three linearly independent eigenvectors, then JCF(A)
is the fourth matrix, and if A has four linearly independent eigenvectors (i.e., if A is
nondefective), then JCF(A) is the last matrix, which is the same matrix we would obtain
via the diagonalization procedure described in Section 7.3.
On the other hand, if A has two linearly independent eigenvectors, then either the
second or third matrix could be JCF(A), since both of these matrices contain two Jordan
blocks. To distinguish these two matrices, we examine the sizes of the Jordan blocks.
This information comes from the length of the cycles of generalized eigenvectors. If we
can ﬁnd a cycle of generalized eigenvectors of length 3, {(A −λI)2v, (A −λI)v, v},
then JCF(A) would have to contain a 3×3 block. Otherwise it would not contain a 3×3
block. In this way, we could distinguish cases (b) and (c) above.
□
More generally, suppose the n ×n matrix A has a single eigenvalue λ of multiplicity
n. To determine whether or not A contains a cycle of generalized eigenvectors corre-
sponding to λ of length p or more, it is necessary and sufﬁcient to determine whether
or not (A −λI)p−1 = 0n. If so, then no cycle of length p is possible. Otherwise,
(A −λI)p−1 ̸= 0n, and so we can ﬁnd a vector v with (A −λI)p−1v ̸= 0, and hence
we may use v as an initial vector for a cycle of generalized eigenvectors of length p
or more.
Example 7.6.12
Suppose that A is a 9 × 9 matrix with eigenvalue λ (repeated nine times), and suppose
that dim[Eλ] = 3. If (A −λI)3 ̸= 09 and (A −λI)4 = 09, what are the possible Jordan
canonical forms for A (assuming that the Jordan blocks are ordered so that larger blocks
are placed above smaller blocks)?

7.6
Jordan Canonical Forms 481
Solution:
An enumeration of all possible combinations of Jordan block sizes for
a 9 × 9 matrix with a single eigenvalue λ would be quite lengthy. However, since
dim[Eλ] = 3, we know JCF(A) has precisely three Jordan blocks. Moreover, since
(A −λI)3 ̸= 0, there must be at least one cycle of generalized eigenvectors of A corre-
sponding to λ of length 4, so that at least one of the Jordan blocks has at least 4 × 4 size.
Since (A −λI)4 = 09, we see that no Jordan blocks of size 5×5 (or larger) are possible
in JCF(A). There are only two block size combinations that are compatible with all of
these constraints: (a) two blocks of size 4 × 4 and one block of size 1 × 1, and (b) one
block each of sizes 4 × 4, 3 × 3, and 2 × 2:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
λ
1
λ
1
λ
1
λ
λ
1
λ
1
λ
1
λ
λ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
and
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
λ
1
λ
1
λ
1
λ
λ
1
λ
1
λ
λ
1
λ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
□
Example 7.6.13
Let A =
⎡
⎣
0 0 1
0 0 0
0 0 0
⎤
⎦. Compute JCF(A).
Solution:
The eigenvalues of A are λ = 0, 0, 0, so JCF(A) contains either (a) one
3 × 3 block, (b) one 2 × 2 block and one 1 × 1 block, or (c) three 1 × 1 blocks. To
determine which, we simply need to know how many linearly independent eigenvectors
A has. It is easily seen that the null space of the matrix A −λI is two-dimensional.
Therefore, JCF(A) contains two Jordan blocks, which is case (b). Thus, we have
JCF(A) =
⎡
⎣
0 1 0
0 0 0
0 0 0
⎤
⎦.
□
Example 7.6.14
Let A =
⎡
⎢⎢⎢⎢⎢⎢⎣
7 0 0 4 0 0
0 7 0 0 5 0
0 0 7 0 0 6
0 0 0 7 0 0
0 0 0 0 7 0
0 0 0 0 0 7
⎤
⎥⎥⎥⎥⎥⎥⎦
. Compute JCF(A).
Solution:
The eigenvalues of A are λ = 7, 7, 7, 7, 7, 7. There are a number of pos-
sibilities for JCF(A). However, easy computation (or inspection) shows that the null
space of A −7I is three-dimensional, so JCF(A) must contain three Jordan blocks. The
possibilities are thus
(a) one 4 × 4 block and two 1 × 1 blocks,
(b) one 3 × 3 block, one 2 × 2 block, and one 1 × 1 block,
(c) three 2 × 2 blocks.

482
CHAPTER 7
Eigenvalues and Eigenvectors
Observe, however, that (A −7I)2 = 0, so that it is impossible to build a cycle
of generalized eigenvectors of length greater than 2. Thus, JCF(A) cannot have Jordan
blocks of size larger than 2 × 2, and hence, case (c) is the answer:
JCF(A) =
⎡
⎢⎢⎢⎢⎢⎢⎣
7 1
0 7
7 1
0 7
7 1
0 7
⎤
⎥⎥⎥⎥⎥⎥⎦
.
□
Example 7.6.15
Let A =
⎡
⎣
0
1 0
0
0 1
1 −3 3
⎤
⎦. Find an invertible matrix S such that S−1 AS is in Jordan
canonical form, and determine JCF(A).
Solution:
It is routine to check that the eigenvalues of A are λ = 1, 1, 1. Now
A −I =
⎡
⎣
−1
1 0
0 −1 1
1 −3 2
⎤
⎦
row reduces to
⎡
⎣
1 −1
0
0
1 −1
0
0
0
⎤
⎦and has only a one-dimensional null space. Thus, JCF(A)
consists of just one 3 × 3 Jordan block: JCF(A) =
⎡
⎣
1 1 0
0 1 1
0 0 1
⎤
⎦. To ﬁnd an invertible
matrix S, we must produce a cycle of generalized eigenvectors of length 3 and place
them into the columns of S. The cycle will take the form
{(A −I)2v, (A −I)v, v},
so we simply need to ﬁnd a vector v such that (A −I)2v ̸= 0. Direct computation shows
that (A −I)2 =
⎡
⎣
1 −2 1
1 −2 1
1 −2 1
⎤
⎦, and we may take v to be any vector that is not killed by
this matrix; say v =
⎡
⎣
1
0
0
⎤
⎦. Then (A −I)v =
⎡
⎣
−1
0
1
⎤
⎦and (A −I)2v =
⎡
⎣
1
1
1
⎤
⎦. Hence,
we have
S =
⎡
⎣
1 −1 1
1
0 0
1
1 0
⎤
⎦.
□
Example 7.6.16
Let A =
⎡
⎣
−2 −1
1
2 −5
2
1 −1 −2
⎤
⎦. Find an invertible matrix S such that S−1 AS is in Jordan
canonical form, and determine JCF(A).
Solution:
The reader can check that the eigenvalues of A are λ = −3, −3, −3. Since
A + 3I =
⎡
⎣
1 −1 1
2 −2 2
1 −1 1
⎤
⎦,

7.6
Jordan Canonical Forms 483
nullspace(A + 3I) contains two linearly independent solutions. Thus, JCF(A) consists
of two Jordan blocks (one of them of size 2 × 2 and the other of size 1 × 1):
JCF(A) =
⎡
⎣
−3
1
0
0 −3
0
0
0 −3
⎤
⎦.
To ﬁnd an invertible matrix S, let us ﬁrst determine a cycle of generalized eigenvectors
of length 2 (no such cycle of length 3 is possible in this case, since there is no 3 × 3
Jordan block in JCF(A)). A cycle of generalized eigenvectors of length 2 will take
the form
{(A + 3I)v, v},
so we seek a vector v such that (A + 3I)v ̸= 0 and (A + 3I)2v = 0. This latter equation
is guaranteed to hold for every v since (A + 3I)2 = 03. We quickly see that the vector
v =
⎡
⎣
1
0
0
⎤
⎦
will serve nicely, and thus
(A + 3I)v =
⎡
⎣
1
2
1
⎤
⎦.
So we have a cycle of generalized eigenvectors
⎧
⎨
⎩
⎡
⎣
1
2
1
⎤
⎦,
⎡
⎣
1
0
0
⎤
⎦
⎫
⎬
⎭.
Finally, we need to ﬁnd an additional eigenvector that is nonproportional to the eigen-
vector (A + 3I)v. There are many possibilities, and the reader can check that
w =
⎡
⎣
1
1
0
⎤
⎦
is workable. Hence, we have
S =
⎡
⎣
1 1 1
2 0 1
1 0 0
⎤
⎦.
□
The Jordan canonical form concept is a powerful tool in the study of matrices (and
corresponding linear transformations). To reiterate, every n × n matrix has a unique
Jordan canonical form (up to the order of the Jordan blocks) that it is similar to. Since
similar matrices share many properties, such as characteristic polynomial, eigenvalues,
determinant, trace, dimension of eigenspaces, and so on, many questions that one can ask
about matrices in general can be reduced to questions about matrices that are in Jordan
canonical form, a much more specialized class. An excellent illustration of this can be
seen in the second project at the end of this chapter.
As a ﬁnal application of the concepts in this section, we return once more to linear
systems of differential equations. We saw in Section 7.3 that a linear system of differential
equations with a diagonalizable coefﬁcient matrix can be solved by transforming the
given system to a diagonal system. Now, through the machinery of Jordan canonical

484
CHAPTER 7
Eigenvalues and Eigenvectors
forms, we can attempt a similar strategy on linear systems of differential equations in
which the coefﬁcient matrix is not diagonalizable. Consider once again the linear system
x′ = Ax. Given an invertible matrix S of constants, recall the linear change of variables
x = Sy from (7.3.4). This yields the transformed linear system
y′ = By,
(7.6.6)
where B = S−1 AS. In this case, we wish to choose S so that B is the Jordan canon-
ical form of A. As such, the system (7.6.6) will consist of ﬁrst-order linear differen-
tial equations, which we can solve by the technique of Section 1.6. We illustrate with
an example.
Example 7.6.17
Solve the linear system
x′
1 = −9x1 + 9x2
x′
2 = −16x1 + 15x2
.
Solution:
The coefﬁcient matrix for the system is A =
! −9
9
−16 15
"
. The eigenvalues
of A are λ = 3, 3, and we ﬁnd that the eigenspace E3 is only one-dimensional, with
basis vector
! 3
4
"
. We can ﬁnd a generalized eigenvector v of A corresponding to λ = 3
by ﬁnding a vector v such that (A −3I)v ̸= 0. Since A −3I =
! −12
9
−16 12
"
, we see by
inspection that the vector
! 0
1
"
is a generalized eigenvector of A. Since
! −12
9
−16 12
" ! 0
1
"
=
! 9
12
"
,
we have the cycle of generalized eigenvectors
5! 9
12
"
,
! 0
1
"6
,
and thus we construct the matrices
S =
! 9 0
12 1
"
and
B =
! 3 1
0 3
"
.
Via the substitution x = Sy, the original system is transformed into
y′ =
! 3 1
0 3
"
y,
which corresponds to the equations
y′
1 = 3y1 + y2
y′
2 =
3y2.
The solution to the second equation is
y2(t) = c1e3t.
(7.6.7)

7.6
Jordan Canonical Forms 485
Substituting (7.6.7) into the ﬁrst equation and rearranging, it becomes
y′
1 −3y1 = c1e3t.
(7.6.8)
This is a ﬁrst-order linear equation, with integrating factor I (t) = e−3t. Multiplying
(7.6.8) through by I (t), it becomes
(y1 · e−3t)′ = c1.
Integrating both sides yields
y1 · e−3t = c1t + c2,
and thus,
y1(t) = c1te3t + c2e3t.
Thus, we have
y =
! y1(t)
y2(t)
"
=
! c1te3t + c2e3t
c1e3t
"
.
(7.6.9)
Finally, we must substitute (7.6.9) into the equation x = Sy to solve for x. We obtain
x =
! 9 0
12 1
"
y =
!
9(c1te3t + c2e3t)
12(c1te3t + c2e3t) + c1e3t
"
= c1e3t
!
9t
12t + 1
"
+ c2e3t
! 9
12
"
.
Thus, the general solution to the linear system of differential equations is
x1(t) = 9c1te3t + 9c2e3t,
x2(t) = c1e3t(12t + 1) + 12c2e3t.
□
Concluding this section, we return to prove part (3) of Theorem 7.5.4, which states
that every real symmetric matrix is diagonalizable.
Proof that every real symmetric matrix is nondefective
(Theorem 7.5.4, (3)):
Suppose to the contrary that A is a real symmetric matrix that is defective. There exists an
eigenvalue λ of A and a corresponding generalized eigenvector v with (A −λI)2v = 0,
but (A −λI)v ̸= 0. Then
0 ̸= ⟨(A −λI)v, (A −λI)v⟩
since (A −λI)v ̸= 0
= ⟨v, (A −λI)2v⟩
by Lemma 7.5.9
= ⟨v, 0⟩= 0,
a contradiction.
Exercises for 7.6
Key Terms
Generalized eigenvector, Superdiagonal, Jordan block corre-
sponding to λ, Jordan canonical form, Cycle of generalized
eigenvectors corresponding to λ, Length of a cycle, Initial
and terminal vectors.
Skills
• For a given matrix A and eigenvalue λ, be able to
determine whether a vector v is an eigenvector, a gen-
eralized eigenvector, or neither.
• Be able to construct a cycle of generalized eigenvec-
tors corresponding to an eigenvalue λ of a matrix A.
• Be able to compute the Jordan canonical form of a
given matrix A, along with an invertible matrix S
whose columns are comprised of the cycles of gen-
eralized eigenvectors.
• Be able to list the possible Jordan canonical forms for
a matrix A, given only the multiplicities of its eigen-
values.
• Beabletosolvelinearsystemsofdifferentialequations
in which the coefﬁcient matrix A is not necessarily di-
agonalizable.

486
CHAPTER 7
Eigenvalues and Eigenvectors
True-False Review
For Questions (a)–(l), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Every eigenvector is a generalized eigenvector.
(b) The number of Jordan blocks in the Jordan canonical
form of a matrix A is the number of linearly indepen-
dent eigenvectors of A.
(c) For every square matrix A, there is a unique invertible
matrix S such that S−1AS is in Jordan canonical form.
(d) If J1 and J2 are n × n matrices in Jordan canonical
form, then the matrix J1 + J2 is in Jordan canonical
form.
(e) A generalized eigenvector of A corresponding to an
eigenvalue λ is a member of the null space of (A−λI)p
for some positive integer p.
(f) The dimension of Kλ, the vector space of general-
ized eigenvectors corresponding to an eigenvalue λ, is
equal to the number of Jordan blocks corresponding
to λ in the Jordan canonical form of A.
(g) Every square matrix A is similar to a matrix J in Jor-
dan canonical form.
(h) If J1 and J2 are n × n matrices in Jordan canonical
form, then the matrix J1J2 is in Jordan canonical form.
(i) The size of a Jordan block is equal to the number
of vectors in the corresponding cycle of generalized
eigenvectors of A.
(j) If A is an n × n matrix with no cycles of generalized
eigenvectors of length p ≥2, then A is diagonalizable.
(k) Similar matrices must have the same Jordan canonical
form, up to rearrangement of the Jordan blocks.
(l) If J is in Jordan canonical form and r is a scalar, then
the matrix r J is in Jordan canonical form.
Problems
For Problems 1–5, determine how many Jordan canonical
formsarewiththegiveneigenvalues(notcountingrearrange-
ments of the Jordan blocks) and list each of them.
1. A 3 × 3 matrix with eigenvalues λ = −4, 0, 9.
2. A 3 × 3 matrix with eigenvalues λ = 1, 1, 1.
3. A 4 × 4 matrix with eigenvalues λ = 1, 1, 3, 3.
4. A 5 × 5 matrix with eigenvalues λ = 2, 2, 2, 2, 2.
5. A 6 × 6 matrix with eigenvalues λ = 3, 3, 3, 3, 9, 9.
For Problems 6–7, determine how many Jordan canonical
forms are possible with the given eigenvalues (not counting
rearrangements of the Jordan blocks). You do not need to
list them.
6. An 11 × 11 matrix with eigenvalues λ = 2, 2, 2,
2, 6, 6, 6, 6, 8, 8, 8.
7. A 10 × 10 matrix with eigenvalues λ = 2, 2, 2,
2, 5, 5, 5, 5, 5, 5.
8. If it is known that (A −5I)2 = 0 for the matrix in
Problem 7, how many Jordan canonical form struc-
tures are possible for the matrix A?
9. Let A be a 5 × 5 matrix with eigenvalues λ1, λ1,
λ1, λ2, λ2, where λ1 ̸= λ2.
(a) Determine the complete list of possible Jordan
canonical forms of A.
(b) Assume further that (A −λ1I)2 = 05. Among
the matrices listed in part (a), which of them are
the possible Jordan canonical form of A in light
of this new information?
10. Suppose A is a 6 × 6 matrix with eigenvalue λ (of
multiplicity 6). If it is known that (A −λI)3 = 0
but (A −λI)2 ̸= 0, write down all possible Jordan
canonical forms of A.
For Problems 11–14, the characteristic polynomial p(λ) for
a square matrix A is given. Write down a set S of matrices
such that every square matrix with characteristic polynomial
p(λ) is guaranteed to be similar to exactly one of the matrices
in the set S.
11. p(λ) = (4 −λ)2(−6 −λ).
12. p(λ) = (4 −λ)3(−1 −λ)2.
13. p(λ) = (3 −λ)2(−2 −λ)3λ2.
14. p(λ) = (−2 −λ)2(6 −λ)5.
15. Which of the matrices in the set S in Problem 14 have
a set of exactly ﬁve (and not more than ﬁve) linearly
independent eigenvectors? Explain.

7.6
Jordan Canonical Forms 487
16. Give an example of a 2×2 matrix A that has a general-
ized eigenvector that is not an eigenvector, and exhibit
such a generalized eigenvector.
17. Give an example of a 3×3 matrix A that has a general-
ized eigenvector that is not an eigenvector, and exhibit
such a generalized eigenvector.
For Problems 18–29, ﬁnd the Jordan canonical form J for
the matrix A, and determine an invertible matrix S such that
S−1 AS = J.
18. A =
! −1
0
0 −2
"
.
19. A =
!
4
4
−4 12
"
.
20. A =
!
1 1
−1 3
"
.
21. A =
⎡
⎣
1 1 1
0 1 1
0 1 1
⎤
⎦.
22. A =
⎡
⎣
5 0 −1
1 4 −1
1 0
3
⎤
⎦.
23. A =
⎡
⎣
4 −4 5
−1
4 2
−1
2 4
⎤
⎦.
24. A =
⎡
⎣
−6
1
0
−1/2 −9/2
1/2
−1/2
1/2 −11/2
⎤
⎦.
[Hint: The eigenvalues of A are λ = −5 and λ = −6.]
25. A =
⎡
⎣
2 −2
14
0
3 −7
0
0
2
⎤
⎦.
26. A =
⎡
⎣
7 −2
2
0
4 −1
−1
1
4
⎤
⎦.
27. A =
⎡
⎣
−1 −1
0
0 −1 −2
0
0 −1
⎤
⎦.
28. A =
⎡
⎢⎢⎣
2 −1
0 1
0
3 −1 0
0
1
1 0
0 −1
0 3
⎤
⎥⎥⎦.
29. A =
⎡
⎢⎢⎣
2 −4 2 2
−2
0 1 3
−2 −2 3 3
−2 −6 3 7
⎤
⎥⎥⎦. [The characteristic polyno-
mial is p(λ) = (2 −λ)2(4 −λ)2.]
For Problems 30–32, ﬁnd the Jordan canonical form J for
the matrix A. You need not determine an invertible matrix S
such that S−1 AS = J.
30. A =
⎡
⎢⎢⎢⎢⎣
2 1 1 1 1
0 2 0 0 1
0 0 2 0 1
0 0 0 2 1
0 0 0 0 2
⎤
⎥⎥⎥⎥⎦
.
31. A =
⎡
⎢⎢⎢⎢⎣
0 0 0 1 4
0 0 1 1 1
0 0 0 6 0
0 0 0 0 0
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
.
32. A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 1 0 1 0 1 0 1
0 1 0 0 0 0 0 0
0 0 1 1 0 1 0 1
0 0 0 1 0 0 0 0
0 0 0 0 1 1 0 1
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 1
0 0 0 0 0 0 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
For Problems 33–35, use Jordan canonical forms to deter-
mine whether the given pair of matrices are similar.
33. A =
⎡
⎣
7 1 0
−1 5 0
1 0 6
⎤
⎦; B =
⎡
⎣
6 −1 1
0
6 0
0
0 6
⎤
⎦.
34. A =
⎡
⎣
7 −2
2
0
4 −1
−1
1
4
⎤
⎦; B =
⎡
⎣
3 −1 −2
1
6
1
1
0
6
⎤
⎦.
35. A =
⎡
⎣
3 0
4
0 2
0
−4 0 −5
⎤
⎦; B =
⎡
⎣
−1 −1 3
0 −1 1
0
0 2
⎤
⎦.
For Problems 36–41, determine the general solution to the
system x′ = Ax for the given matrix A.
36. A =
! −4
1
−1 −6
"
.
37. A =
! −3 −2
2
1
"
.

488
CHAPTER 7
Eigenvalues and Eigenvectors
38. A =
⎡
⎣
0 1
0
0 0
1
1 1 −1
⎤
⎦.
39. A =
⎡
⎣
−2
0
0
1 −3 −1
−1
1 −1
⎤
⎦.
40. A =
⎡
⎣
4 0 0
1 4 0
0 1 4
⎤
⎦.
41. A =
⎡
⎣
3 0
4
0 2
0
−4 0 −5
⎤
⎦.
42. Solve the initial-value problem x′
=
Ax, where
A =
! −2 −1
1 −4
"
, x(0) =
!
0
−1
"
.
43. Prove that if A are B are n ×n matrices with the same
Jordan canonical form, then A is similar to B.
44. Let A be a square matrix with characteristic polyno-
mial p(λ) = −λ3. Use Jordan canonical forms to
prove that A is a nilpotent7 matrix.
45. (a) Let J be a Jordan block. Prove that the Jordan
canonical form of the matrix J T is J.
(b) Let A be an n × n matrix. Prove that A and AT
have the same Jordan canonical form.
7.7
Chapter Review
The Algebraic Eigenvalue/Eigenvector Problem
We summarize here the main results obtained in this chapter regarding the algebraic
eigenvalue/eigenvector problem.
1. For a given n × n matrix A, the eigenvalue/eigenvector problem consists of deter-
mining all scalars λ and all nonzero vectors v such that
Av = λv.
2. The eigenvalues of A are the roots of the characteristic equation
p(λ) = det(A −λI) = 0,
(7.7.1)
and the eigenvectors of A are obtained by solving the linear systems
(A −λI)v = 0,
(7.7.2)
when λ assumes the values obtained in (7.7.1).
3. If A is a matrix with real elements, then complex eigenvalues and eigenvectors
occur in conjugate pairs.
4. Associated with each eigenvalue λ there is a vector space, called the eigenspace
of λ. This is the set of all eigenvectors corresponding to λ together with the zero
vector. Equivalently, it can be considered as the set of all solutions to the linear
system (7.7.2).
5. If m denotes the multiplicity of the eigenvalue λ and n denotes the dimension of
the corresponding eigenspace, then
1 ≤n ≤m.
6. Eigenvectors corresponding to distinct eigenvalues are linearly independent.
7. An n × n matrix that has n linearly independent eigenvectors is said to have a
complete set of eigenvectors, and we call such a matrix nondefective.
7Recall that an n × n matrix A is called nilpotent if Ap = 0n for some positive integer p.

7.7
Chapter Review 489
8. Two n × n matrices A and B are said to be similar if there exists a matrix S such
that
B = S−1 AS.
A matrix that is similar to a diagonal matrix is said to be diagonalizable. We have
shown that A is diagonalizable if and only if it is nondefective.
9. If A is nondefective and S = [v1, v2, . . . , vn], where v1, v2, . . . , vn are linearly
independent eigenvectors of A, then
S−1 AS = diag(λ1, λ2, . . . , λn),
where λ1, λ2, . . . , λn are the eigenvalues of A corresponding to v1, v2, . . . , vn.
10. If A is a real symmetric matrix, then
(a) All eigenvalues of A are real.
(b) Eigenvectors corresponding to different eigenspaces are orthogonal.
(c) A is nondefective.
(d) A has a complete set of orthonormal eigenvectors, say w1, w2, . . . , wn.
(e) If we let S = [w1, w2, . . . , wn], where w1, w2, . . . , wn are the orthonormal
eigenvectors determined in (d), then S is an orthogonal matrix (S−1 = ST ),
and hence, from (9), for this matrix,
ST AS = diag(λ1, λ2, . . . , λn).
11. Every matrix A is similar to a matrix J in Jordan canonical form. Here are some
facts about JCF(A):
(a) The JCF(A) consists of Jordan blocks arranged along the main diagonal, and
each Jordan block consists of an eigenvalue placed along its main diagonal
with ones appearing on the superdiagonal and zeros elsewhere.
(b) The number of Jordan blocks in JCF(A) is equal to the number of linearly
independent eigenvectors of A.
(c) Each Jordan block coincides with a cycle of generalized eigenvectors. A
generalized eigenvector v satisﬁes the equation (A −λI)pv = 0 for some
p ≥1.
(d) The length of the cycle of generalized eigenvectors is the same as the size of
the corresponding Jordan block.
Additional Problems
In Problems 1–6, decide whether or not the given matrix A
is diagonalizable. If so, ﬁnd an invertible matrix S and a
diagonal matrix D such that S−1 AS = D.
1. A =
! 3
0
16 −1
"
.
2. A =
! 13
−9
25 −17
"
.
3. A =
⎡
⎣
−4
3
0
−6
5
0
3 −3 −1
⎤
⎦.
4. A =
⎡
⎣
1
1
0
−4
5
0
17 −11 −2
⎤
⎦.
5. A =
⎡
⎣
−1 −1
3
4
4 −4
−1
0
3
⎤
⎦.
[Hint: The only eigenvalue of A is λ = 2.]
6. A =
⎡
⎣
9
5 −5
0
−1
0
10
5 −6
⎤
⎦.
[Hint: The eigenvalues of A are λ = 4 and λ = −1.]

490
CHAPTER 7
Eigenvalues and Eigenvectors
⋄In Problems 7–10, use some form of technology to ﬁnd
a complete set of orthonormal eigenvectors for A and an
orthogonal matrix S and a diagonal matrix D such that
S−1 AS = D.
7. A =
⎡
⎣
2 2 1
2 5 2
1 2 2
⎤
⎦.
8. A =
⎡
⎣
0 −1 4
−1
5 2
4
2 2
⎤
⎦.
9. A =
⎡
⎣
−2 1
1
1 3
6
1 6 −1
⎤
⎦.
10. A =
⎡
⎣
1 0 1
0 1 1
1 1 1
⎤
⎦.
Find the Jordan canonical form of each matrix in
Problems 11–12.
11. A =
⎡
⎣
5
8
16
4
1
8
−4 −4 −11
⎤
⎦.
[Hint: The eigenvalues of A are λ = 1 and λ = −3.]
12. A =
⎡
⎣
2 1
1
2 1 −2
−1 0 −2
⎤
⎦.
[Hint: The eigenvalues of A are λ = −1 and λ = 3.]
In Problems 13–16, write down all of the possible Jordan
canonical form structures, up to a rearrangement of the
blocks, for matrices of the speciﬁed type. For each Jordan
canonical form structure, list the number of linearly inde-
pendent eigenvectors of a matrix with that Jordan canonical
form, and list the maximum length of a cycle of generalized
eigenvectors of the matrix.
13. 4 × 4 matrices with eigenvalues λ = −1, −1, −1, 2.
14. 5 × 5 matrices with eigenvalues λ = 4, 4, 4, 4, 4.
15. 6 × 6 matrices with eigenvalues
λ = 6, 6, 6, 6, −3, −3.
16. 7 × 7 matrices with eigenvalues
λ = 2, 2, 2, 2, −4, −4, −4.
17. True or False: If A and B are n × n matrices with
eigenvalues λA and λB, respectively, then λA −λB is
an eigenvalue of A −B. Explain.
18. True or False: If A and B are square matrices such
that A2 is similar to B2, then A is similar to B. Explain.
19. Assume that A1, A2, . . . , Ak are n × n matrices and,
for each i, a vector v is an eigenvector of Ai with
corresponding eigenvalue λi. Show that v is also an
eigenvector of the matrix A1 A2 · · · Ak. What is the
corresponding eigenvalue?
20. Let A and B be n×n matrices. Let v be an eigenvector
of A corresponding to λ1 and let v also be an eigen-
vector of B corresponding to eigenvalue λ2. Show that
AB −B A is not invertible.
Project I: The Hungry Knights
King Arthur and his knights are sitting at the round table for a hearty breakfast of porridge.
Each knight is served a portion of porridge (the servings are not necessarily equal portions).
But the knights are greedy, and in the first minute of the meal, each knight (including King
Arthur) steals half of the porridge from the knight on his left, and half of the porridge from
the knight on his right. In the second minute, each knight again steals half the porridge
from each neighbor. This process continues indefinitely. The knights are so busy stealing
each others’ porridge that nothing ever actually gets eaten.
Assume there are n knights (including King Arthur), and they receive initial distribu-
tion of porridge (a1, a2, ..., an). The problem is to determine the long-term distribution
of porridge. [Note that knight 1 and knight n are seated next to each other at the round
table, and so they steal from each other.]
Part I: Solve the above problem for n = 3 as follows:
(a) Regarding the above porridge distribution as a column vector v, ﬁnd a matrix A
such that Av represents the porridge distribution after one minute, A2v represents
the porridge distribution after two minutes, and so on.

7.7
Chapter Review 491
(b) Diagonalize A.
(c) Determine the distribution of porridge as time →∞.
(d) Do the amounts of porridge stabilize, oscillate, or does it depend on the initial
distribution of porridge? If so, how?
(e) Discuss the impact that the eigenvalues of A have on the limiting behavior. Explain
how and why certain eigenvalues play a larger role in shaping the limit behavior
of the distribution than others.
(f) Discuss any special cases that you think are interesting (such as what happens
in the case where all knights start with equal porridge, or in the case where one
knight starts with all the porridge).
Part II: Redo Part I for n = 4. [Hint: You can save yourself some trouble of computing
eigenvalues if you ﬁnd a couple eigenvectors by inspection (trial and error) and use facts
about tr(A) and det(A). In fact, with a little clever experimenting, you may be able to
get all of the eigenvalues and eigenvectors by “inspection”.]
Part III: Based on your work above, try to guess what happens for larger values of n.
Explore with larger values of n on a calculator or computer to check out your guess.
Project II: Square Roots of Matrices
In this project, we will study which n × n matrices possess square roots. We begin with
the formal deﬁnition.
DEFINITION
A square root for an n × n matrix A is an n × n matrix B such that B2 = A.
Throughout this project, we allow the matrices under consideration to have complex
entries. We begin with diagonalizable matrices.
Part I: Diagonalizable matrices
(a) In Problem 25 of Section 7.3, it was shown that (SDS−1)k = SDkS−1 for all
positive integers k. Extend this result to all positive fractions k = p
q , where p and
q are positive integers.
(b) Applying part (a) with k = 1
2, prove that all diagonalizable matrices have square
roots.
(c) Find four different square roots for the matrix
A =
! −7 −32
16
41
"
.
(d) Generalizing part (b) above, prove that if A and B are similar matrices, then A has
a square root if and only if B has a square root.
(e) Conclude that the classiﬁcation of matrices that possess square roots is reduced to
the classiﬁcation of Jordan canonical form structures that have square roots.

492
CHAPTER 7
Eigenvalues and Eigenvectors
Part II: Invertible matrices
(a) Show that the Jordan canonical form of an invertible matrix A is an upper triangular
matrix J with all entries along the main diagonal nonzero.
(b) Use the fact that the Jordan canonical form for a 2×2 matrix A is either a diagonal
matrix, or a single 2 × 2 Jordan block to prove that any invertible 2 × 2 matrix has
a square root.
(c) Perform a similar analysis for the case of a 3 × 3 matrix A to prove that any
invertible 3 × 3 matrix has a square root.8
Part III: Matrices with all zero eigenvalues By Part I, the determination of which
matrices with all zero eigenvalues have square roots is reduced to the determination of
which Jordan canonical form structures whose blocks all correspond to the eigenvalue
λ = 0 have square roots.
(a) Prove that the Jordan block J =
! 0 1
0 0
"
does not possess a square root. [Hint: As-
sume to the contrary that such a square root B does exist, and ﬁnd the relationship
between the Jordan canonical form of B and J to obtain a contradiction.]
(b) On the other hand, construct a square root for the 4 × 4 matrix J ′ consisting of
two 2 × 2 Jordan blocks J as in (a).
8In fact, it can be shown that any n × n invertible matrix has a square root.

8
Linear Differential
Equations of Order n
In Chapter 1 we developed a technique that enabled us to solve any ﬁrst-order linear
differential equation. As we illustrated in that chapter there are many applied problems
that can be modeled by ﬁrst-order differential equations, but most applications are gov-
erned by differential equations of order higher than ﬁrst-order. For example, second-order
differential equations are of extreme importance since they arise in any application of
Newton’s second law of motion. As a speciﬁc example, consider the damped spring-mass
system depicted in Figure 8.0.1. In Section 8.5 we will derive the following second-order
differential equation governing the displacement, y(t), of the mass from its equilibrium
position at time t:
m d2y
dt2 + cdy
dt + ky = F(t).
Damping mechanism
Mass, m kilograms
y(t)
y 5 0, equilibrium
Positive y
Figure 8.0.1: A simple model of a damped spring-mass system.
493

494
CHAPTER 8
Linear Differential Equations of Order n
Here c and k are positive constants, and F(t) represents any external forces that may be
acting on the mass m. Such a force may arise, for example, due to a person moving the
top of the spring in a vertical direction thereby driving, or forcing the motion.
Asasecondexample,wehaveshowninSection1.7thatanapplicationofKirchhoff’s
law yields the following differential equation governing the behavior of the RLC circuit
shown in Figure 8.0.2:
L d2q
dt2 + R dq
dt + 1
C q = E(t).
R
Switch
C
i(t)
L
E(t)
Figure 8.0.2: An RLC circuit.
Here q(t) gives the charge on the capacitor at time t, R, L, and C are positive constants,
and E(t) provides the driving force (electromotive force) that maintains the ﬂow of
charge through the circuit. The current, i(t), in the circuit is related to q(t) via
i(t) = dq
dt .
Differential equations of order greater than two also arise in applications. For exam-
ple, the mathematical analysis of certain beam conﬁgurations gives rise to fourth-order
differential equations of the form
d4Y
dx4 −kY = 0,
where k is a positive constant.
The differential equations appearing in the preceding examples are all linear; fur-
thermore, the coefﬁcients of the dependent variables (y, q, Y, respectively) and their
derivatives are all constants. Whereas many applications are governed by such con-
stant coefﬁcient differential equations, there are a multitude of physical problems whose
mathematical analysis leads to linear differential equations whose coefﬁcients are not
constant. Examples of such equations include Legendre’s equation,
(1 −x2)d2y
dx2 −2x dy
dx + k(k + 1)y = 0,
and Bessel’s equation,
x2 d2y
dx2 + x dy
dx + (x2 −p2)y = 0,
where k and p are constants. The solutions to both of these differential equations will
be considered in Chapter 11.
In order to give a uniﬁed approach to studying linear differential equations of arbi-
trary order n, we recall that any such differential equation can be written in the form
a0(x)y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x),
(8.0.1)

8.1
General Theory for Linear Differential Equations 495
where a0, a1, . . . an, F are functions deﬁned on an interval I. In this chapter we will
apply the results obtained in Chapters 4 and 6 to develop the underlying theory for the
solution of (8.0.1). This will be accomplished in three steps.
1. Reformulate the problem of solving (8.0.1) in the equivalent form
Ly = F,
where L is an appropriate linear transformation.
2. Establish that the set of all solutions to the associated homogeneous differential
equation
Ly = 0
is a vector space of dimension n, so that every solution to the homogeneous dif-
ferential equation can be expressed as
y(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x),
where {y1, y2, . . . , yn} is any linearly independent set of n solutions to Ly = 0.
3. Establish that every solution to the nonhomogeneous problem Ly = F is of the
form
y(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x) + yp(x),
(8.0.2)
where yp(x) is any particular solution to the nonhomogeneous equation.
The hard work put into understanding the material in Chapters 4 and 6 is really seen
to pay off in Section 8.1, where the power of the vector space methods enables us to
build this general theory very quickly.
Having obtained the general theory for linear differential equations of order n, the
remainder of the chapter is primarily concerned with deriving techniques for obtaining
the requisite number of solutions needed to build (8.0.2) in the case when the differ-
ential equation has constant coefﬁcients. We will then give a complete discussion of
both the spring-mass system and the RLC circuit. Finally, some techniques that can
be used to solve certain linear differential equations with nonconstant coefﬁcients will
be introduced. A more general discussion of such differential equations will be given
in Chapter 11.
8.1
General Theory for Linear Differential Equations
Recall from Chapter 6 that the mapping D : C1(I) →C0(I) deﬁned by D( f ) = f ′
is a linear transformation. We call D the derivative operator. Higher-order derivative
operators can be deﬁned by composition. Thus, Dk : Ck(I) →C0(I) is deﬁned by
Dk = D(Dk−1),
k = 2, 3, . . . ,
so that
Dk( f ) = dk f
dxk .

496
CHAPTER 8
Linear Differential Equations of Order n
By taking a linear combination of the basic derivative operators, we obtain the general
linear differential operator of order n,
L = Dn + a1Dn−1 + · · · + an−1D + an,
(8.1.1)
deﬁned by
Ly = y(n) + a1y(n−1) + · · · + an−1y′ + any,
where the ai are, in general, functions of x. We leave it as an exercise (Problem 43) to
verify that for all y1, y2 ∈Cn(I), and all scalars c,
L(y1 + y2) = Ly1 + Ly2
L(cy1) = cL(y1).
Consequently, L is a linear transformation from Cn(I) into C0(I).
Example 8.1.1
If L = D2 + 4x D −3x, then
Ly = y′′ + 4xy′ −3xy,
so that, for example,
L(sin x) = −sin x + 4x cos x −3x sin x,
whereas
L(x2) = 2 + 8x2 −3x3.
□
Example 8.1.2
Determine the kernel of the linear differential operator L = D −2x.
Solution:
The kernel of L consists of all functions that satisfy Ly = 0; that is, all
solutions to the differential equation
y′ −2xy = 0.
An integrating factor for this ﬁrst-order linear differential equation (see Section 1.6) is
I = e
!
−2x dx = e−x2. Consequently, the differential equation can be written in the
equivalent form
"
e−x2 y
#′
= 0
which, upon integration, yields e−x2 y = c, so that y(x) = cex2. Therefore,
Ker(L) =
$
cex2 : c ∈R
%
□
Now consider the general nth-order linear differential equation
a0(x)y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x),
(8.1.2)
where a0(̸= 0), a1, . . . , an and F are functions speciﬁed on an interval I. If F(x)
is identically zero on I, then the differential equation (8.1.2) is called homogeneous.
Otherwise, it is called nonhomogeneous. We will assume that a0(x) is nonzero on I, in
which case we can divide Equation (8.1.2) by a0 and redeﬁne the remaining functions
to obtain the following standard form:
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x).
(8.1.3)

8.1
General Theory for Linear Differential Equations 497
This can be written in the equivalent form
Ly = F(x),
where L is given in Equation (8.1.1). The key result that we require in developing the
theory for linear differential equations is the following existence and uniqueness theorem.
Theorem 8.1.3
Let a1, a2, . . . , an, and F be functions that are continuous on an interval I. Then, for
any x0 in I, the initial-value problem
Ly = F(x)
y(x0) = y0,
y′(x0) = y1,
. . . ,
y(n−1)(x0) = yn−1
has a unique solution on I.
Proof The proof of this theorem requires concepts from advanced calculus and is best
left for a second course in differential equations. See, for example, Coddington, E.A.
and Levinson, N., Theory of Differential Equations, McGraw-Hill, 1955.
The differential equation (8.1.3) is said to be regular on I if the functions a1, a2, . . . ,
an, and F are continuous on I. In developing the theory for linear differential equations,
we will always assume that our differential equations are regular on the interval of interest
so that the existence and uniqueness theorem can be applied on that interval.
Homogeneous Linear Differential Equations
We ﬁrst consider the nth-order linear homogeneous differential equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
(8.1.4)
on an interval I. This differential equation can be written as the operator equation
Ly = 0,
where L : Cn(I) →C0(I) is the nth-order linear differential operator
L = Dn + a1Dn−1 + · · · + an−1D + an.
If we let S denote the set of all solutions to the differential equation (8.1.4), then
S = {y ∈Cn(I) : Ly = 0}.
That is,
S = Ker(L).
In Chapter 6, we proved that the kernel of any linear transformation T : V →W
is a subspace of V . It follows directly from this result that S, the set of all solutions to
(8.1.4), is a subspace of Cn(I). We will refer to this subspace as the solution space of
the differential equation. If we can determine the dimension of S, then we will know
how many linearly independent solutions are required to span the solution space. This
is dealt with in the following theorem.
Theorem 8.1.4
The set of all solutions to the regular nth-order homogeneous linear differential equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
on an interval I is a vector space of dimension n.

498
CHAPTER 8
Linear Differential Equations of Order n
Proof The given differential equation can be written in operator form as
Ly = 0.
We have already shown that the set of all solutions to this differential equation is a vector
space. To prove that the dimension of the solution space is n, we must establish the
existence of a basis consisting of n solutions. For simplicity, we provide the details only
for the case n = 2.
Let y1, y2 be the unique solutions to the initial-value problems
Ly1 = 0,
y1(x0) = 1,
y′
1(x0) = 0,
(8.1.5)
Ly2 = 0,
y2(x0) = 0,
y′
2(x0) = 1,
(8.1.6)
where
L = D2 + a1(x)D + a2(x).
The Wronskian of these solutions at x0 ∈I is W[y1, y2](x0) = det(I2) = 1 ̸= 0, so that
the solutions are linearly independent on I. To be a basis for the solution space, y1 and
y2 must also span the solution space. Let y = u(x) be any solution to the differential
equation Ly = 0 on I, and suppose u(x0) = c1 and u′(x0) = c2. Then y = u(x) is the
unique solution to the initial-value problem
Ly = 0,
y(x0) = c1,
y′(x0) = c2.
(8.1.7)
However, if we deﬁne
w(x) = c1y1(x) + c2y2(x),
then, using the linearity of L,
Lw = L(c1y1 + c2y2) = c1L(y1) + c2L(y2) = 0.
Further, using the initial values in (8.1.5) and (8.1.6),
w(x0) = c1y1(x0) + c2y2(x0) = c1
and
w′(x0) = c1y′
1(x0) + c2y′
2(x0) = c2.
Consequently, w(x) also satisﬁes the initial-value problem (8.1.7). Thus, by uniqueness,
we must have
u(x) = w(x).
That is,
u(x) = c1y1(x) + c2y2(x).
Therefore, we have shown that any solution to Ly = 0 can be written as a linear
combination of the linearly independent solutions y1, y2, and hence, these solutions
do span the solution space. It follows that {y1, y2} is a basis for the solution space and,
since the basis consists of two vectors, the dimension of this solution space is two. The
extension of the foregoing proof to arbitrary n is left as an exercise (Problem 44).
It follows from the previous theorem and Theorem 4.6.10 that any set of n linearly
independent solutions, say {y1, y2, . . . , yn}, to
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
(8.1.8)

8.1
General Theory for Linear Differential Equations 499
is a basis for the solution space of this differential equation. Consequently, every solution
to the differential equation can be written as
y(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x),
(8.1.9)
for appropriate constants c1, c2, . . . , cn. We refer to (8.1.9) as the general solution to
the differential equation (8.1.8).
Example 8.1.5
Determine all solutions to the differential equation y′′ −2y′ −15y = 0 of the form
y(x) = erx, where r is a constant. Use your solutions to determine the general solution
to the differential equation.
Solution:
If y(x) = erx then y′(x) = rerx, and y′′(x) = r2erx. Substituting these
results into the given differential equation and simplifying yields
erx(r2 −2r −15) = 0,
or equivalently,
(r + 3)(r −5) = 0.
Hence, two solutions to the differential equation are
y1(x) = e−3x
and
y2(x) = e5x.
Furthermore, the Wronskian of these solutions is W[y1, y2](x) = 8e2x ̸= 0, so that they
are linearly independent on any interval. It follows directly from Theorem 8.1.4 that a
basis for the set of all solutions to the differential equation is {e−3x, e5x}, so that the
general solution to the differential equation is
y(x) = c1e−3x + c2e5x.
□
In the previous example, we were able to determine that the solutions y1 and y2
are linearly independent on any interval since their Wronskian was nonzero. What
would have happened if their Wronskian had been identically zero? Based on Theo-
rem 4.5.23, we would not have been able to draw any conclusion as to the linear depen-
dence or linear independence of the solutions. We now show, however, that when dealing
with solutions of an nth-order homogeneous linear differential equation, if the Wron-
skian of the solutions is zero for at least one point in I, then the solutions are linearly
dependent on I.
Theorem 8.1.6
Let y1, y2, . . . , yn be solutions to the regular nth-order differential equation Ly = 0
on
an
interval
I,
and
let
W[y1, y2, . . . , yn](x)
denote
their
Wronskian.
If
W[y1, y2, . . . , yn](x0) = 0 at some point x0 in I, then {y1, y2, . . . , yn} is linearly de-
pendent on I.
Proof We provide details for the case n = 2 and leave the extension to arbitrary n as
an exercise (Problem 45). Once more, the proof depends on the existence-uniqueness
theorem. Let x0 be a point in I at which W[y1, y2](x0) = 0, and consider the linear
system
c1y1(x0) + c2y2(x0) = 0,
c1y′
1(x0) + c2y′
2(x0) = 0,

500
CHAPTER 8
Linear Differential Equations of Order n
where the unknowns are c1, c2. The determinant of the matrix of coefﬁcients of this
system is W[y1, y2](x0) = 0, so that the system has nontrivial solutions. Let (α1, α2) be
one such nontrivial solution, and deﬁne the function u(x) by
u(x) = α1y1(x) + α2y2(x).
Then y = u(x) satisﬁes the initial-value problem
Ly = 0,
y(x0) = 0,
y′(x0) = 0.
However, y(x) = 0 also satisﬁes the initial-value problem, and hence, by uniqueness,
we must have u(x) = 0; that is,
α1y1(x) + α2y2(x) = 0,
where at least one of α1, α2 is nonzero. Consequently, {y1, y2} is linearly dependent
on I.
To summarize:
The vanishing or nonvanishing of the Wronskian on an interval I completely
characterizes whether solutions to Ly = 0 are linearly dependent or
linearly independent on I.
Example 8.1.7
Verify that y1(x) = cos 2x and y2(x) = 3(1 −2 sin2 x) are solutions to the differential
equation y′′ + 4y = 0 on (−∞, ∞). Determine whether they are linearly independent
on (−∞, ∞).
Solution:
It is easily veriﬁed by direct substitution that
y′′
1 + 4y1 = 0
and
y′′
2 + 4y2 = 0
so that y1 and y2 are solutions to the given differential equation on (−∞, ∞). To deter-
mine whether they are linearly independent on (−∞, ∞), we compute their Wronskian.
W[y1, y2](x) =
cos 2x
3(1 −2 sin2 x)
−2 sin 2x
−12 sin x cos x
= −12 cos 2x sin x cos x + 6 sin 2x(1 −2 sin2 x)
= −6 cos 2x sin 2x + 6 sin 2x cos 2x
= 0,
so that, from Theorem 8.1.6, the solutions are linearly dependent on (−∞, ∞). Indeed,
since cos 2x = 1 −2 sin2 x, the dependency relation is 3y1 −y2 = 0. We leave it as
an exercise to verify that a second linearly independent solution to the given differential
equation is y3(x) = sin 2x, so that the general solution to y′′ + 4y = 0 is
y(x) = c1 cos 2x + c2 sin 2x.
□
Nonhomogeneous Linear Differential Equations
We now consider the nonhomogeneous linear differential equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x),
(8.1.10)

8.1
General Theory for Linear Differential Equations 501
where F(x) is not identically zero on the interval of interest. If we set F(x) = 0 in
(8.1.10), we obtain the associated homogeneous equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0.
(8.1.11)
Equations (8.1.10) and (8.1.11) can be written in operator form as
Ly = F
and
Ly = 0,
respectively, where
L = Dn + a1(x)Dn−1 + · · · + an−1(x)D + an(x).
The main theoretical result for nonhomogeneous linear differential equation is given in
the following theorem:
Theorem 8.1.8
Let {y1, y2, . . . , yn} be a linearly independent set of solutions to Ly = 0 on an interval
I, and let y = yp be any particular solution to Ly = F on I. Then every solution to
Ly = F on I is of the form
y = c1y1 + c2y2 + · · · + cnyn + yp,
for appropriate constants c1, c2, . . . , cn.
Proof Since y = yp satisﬁes Equation (8.1.10), we have
Lyp = F.
(8.1.12)
Let y = u be any solution to Equation (8.1.10). Then we also have
Lu = F.
(8.1.13)
Subtracting (8.1.12) from (8.1.13) and using the linearity of L yields
L(u −yp) = 0.
Thus, y = u −yp is a solution to the associated homogeneous equation Ly = 0 and
therefore can be written as
u −yp = c1y1 + c2y2 + · · · + cnyn,
for appropriately chosen constants c1, c2, . . . , cn. Consequently,
u = c1y1 + c2y2 + · · · + cnyn + yp.
According to Theorem 8.1.8, the general solution to the nonhomogeneous differen-
tial equation Ly = F is of the form
y(x) = yc(x) + yp(x),
where
yc(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x)
is the general solution to the associated homogeneous equation Ly = 0, and yp is a
particular solution to Ly = F. We refer to yc as the complementary function for
Ly = F.

502
CHAPTER 8
Linear Differential Equations of Order n
Example 8.1.9
Verify that yp(x) = 2e6x is a particular solution to the differential equation
y′′ −2y′ −15y = 18e6x
(8.1.14)
and determine the general solution.
Solution:
For the given function, we have
y′′
p −2y′
p −15yp = 72e6x −24e6x −30e6x = 18e6x.
Hence, yp(x) = 2e6x is a solution to the given differential equation. We have seen
in Example 8.1.5 that the general solution to the associated homogeneous differential
equation is
yc(x) = c1e−3x + c2e5x.
So, Theorem 8.1.8 tells us that the general solution to the differential equation (8.1.14) is
y(x) = c1e−3x + c2e5x + 2e6x.
□
We need one ﬁnal result regarding solutions to nonhomogeneous differential
equations.
Theorem 8.1.10
If y = u p and y = vp are particular solutions to Ly = f (x) and Ly = g(x), respectively,
then y = u p + vp is a solution to Ly = f (x) + g(x).
Proof We have L(u p + vp) = L(u p) + L(vp) = f (x) + g(x).
We have now derived the fundamental theory for linear differential equations. For
the remainder of the chapter, we focus our attention on developing techniques for ﬁnding
the solutions whose existence is guaranteed by our theory.
Exercises for 8.1
Key Terms
Derivative operator, Linear differential operator of order n,
Regular differential equation, nth-order linear homogeneous
differential equation, Solution space of a differential equa-
tion, General solution to a differential equation, nth-order
linear nonhomogeneous differential equation, Complemen-
tary function.
Skills
• Be able to evaluate a given linear differential operator
L on a function y.
• Be able to compute the kernel of a given linear differ-
ential operator L.
• Be able to write an nth-order linear differential equa-
tion as an operator equation.
• Be able to ﬁnd solutions to a given nth-order lin-
ear homogeneous differential equation of a speciﬁed
form.
• Be able to use basic solutions to an nth-order linear
homogeneous differential equation to ﬁnd the general
solution to the differential equation.
• Be able to use the Wronskian to determine whether a
collection of solutions to Ly = 0 are linearly depen-
dent or linearly independent.
• Given a particular solution to an nth-order linear non-
homogeneous differential equation, be able to ﬁnd the
general solution to the differential equation.
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A regular nth-order linear homogeneous differential
equation deﬁned on an interval I always has n solu-
tions that are linearly independent on the interval I.

8.1
General Theory for Linear Differential Equations 503
(b) If y1, y2, . . . , yn are solutions to a regular nth-order
linear homogeneous differential equation such that
W[y1, y2, . . . , yn](x) is zero at some points of I and
nonzero at other points of I, then {y1, y2, . . . , yn} is a
linearly independent set of functions.
(c) If L1 and L2 are linear differential operators, then
L1L2 = L2L1.
(d) If L1 and L2 are linear differential operators, then so
is L1 + L2.
(e) If L is a linear differential operator, then so is cL for
all constants c.
(f) If yp is a particular solution to the differential equation
Ly = F, then yp + u is also a solution to Ly = F for
every solution u of the corresponding homogeneous
differential equation Ly = 0.
(g) If y1 is a solution to Ly = F1 and y2 is a solution to
Ly = F2, then y1 + y2 is a solution to Ly = F1 + F2.
(h) If L = D2 −1, then a basis for Ker(L) is {ex, e−x}.
(i) A basis for the solution space of the differential equa-
tion y′′ −3y′ + 2y = 0 is {ex, e2x}.
(j) A basis for the solution space of the differential equa-
tion y′′′ + 2y′′ −y′ −2y = 0 is {ex, e−2x}.
Problems
For Problems 1–4, ﬁnd Ly for the given differential oper-
ator if (a) y(x) = 2e3x, (b) y(x) = 3 ln x, (c) y(x) =
2e3x + 3 ln x.
1. L = D −x.
2. L = D2 −x2D + x.
3. L = D3 −2x D2.
4. L = D3 −D + 4.
For Problems 5–9, verify that the given function is in the
kernel of L.
5. y(x) = xe2x,
L = D2 −4D + 4.
6. y(x) = x−2,
L = x2D2 + 2x D −2.
7. y(x) = sin(x2),
L = D2 −x−1D + 4x2.
8. y(x) = sin x + cos x,
L = D3 + D2 + D + 1.
9. y(x) = xex,
L = −D2 + 2D −1.
For Problems 10–13, compute Ker(L).
10. L = D −3x2.
11. L = D2 + 1.
12. L = D2 + 2D −15. [Hint: Try for two solutions to
Ly = 0 of the form erx.]
13. L = x2D + x.
For Problems 14–15, ﬁnd L1L2 and L2L1 for the given dif-
ferential operators, and determine whether L1L2 = L2L1.
14. L1 = D + 1,
L2 = D −2x2.
15. L1 = D + x,
L2 = D + (2x −1).
16. If L1 = D+a1(x), determine all differential operators
of the form L2 = D +b1(x) such that L1L2 = L2L1.
For Problems 17–18, write the given nonhomogeneous dif-
ferential equation as an operator equation, and give the as-
sociated homogeneous differential equation.
17. y′′′ + x2y′′ −(sin x)y′ + ex y = x3.
18. y′′ + 4xy′ −6x2y = x2 sin x.
19. Use the existence and uniqueness theorem to prove
that the only solution to the initial-value problem
y′′ + x2y + ex y = 0
y(0) = 0, y′(0) = 0
is the trivial solution y(x) = 0.
20. Use the existence and uniqueness theorem to formu-
late and prove a general theorem regarding the solution
to the initial-value problem
Ly = 0
y(x0) = 0, y′(x0) = 0, . . . , y(n−1)(x0) = 0.
21. Determine which of the following sets of vectors is a
basis for the solution space to the differential equation
y′′ −16y = 0:
S1 = {e4x}, S2 = {e2x, e4x, e−4x}, S3 = {e4x, e2x},
S4 = {e4x, e−4x}, S5 = {e4x, 7e4x},
S6 = {cosh 4x, sinh 4x}.
22. Determine which of the following sets of vectors is a
basis for the solution space to the differential equation
x2y′′ −3xy′ + 4y = 0 on the interval (0, ∞):
S1 = {x2}, S2 = {x2, x2 ln x}, S3 = {2x2, 3x2 ln x},
S4 = {x2(2 + 3 ln x), x2(2 −3 ln x)}.

504
CHAPTER 8
Linear Differential Equations of Order n
For Problems 23–26, determine two linearly independent
solutions to the given differential equation of the form
y(x) = erx, and thereby determine the general solution to
the differential equation.
23. y′′ −2y′ −3y = 0.
24. y′′ + 7y′ + 10y = 0.
25. y′′ −36y = 0.
26. y′′ + 4y′ = 0.
For Problems 27–31, determine three linearly independent
solutions to the given differential equation of the form
y(x) = erx, and thereby determine the general solution to
the differential equation.
27. y′′′ −3y′′ −y′ + 3y = 0.
28. y′′′ + 3y′′ −4y′ −12y = 0.
29. y′′′ + 3y′′ −18y′ −40y = 0.
30. y′′′ −y′′ −2y′ = 0.
31. y′′′ + y′′ −10y′ + 8y = 0.
For Problems 32–33, determine four linearly independent
solutions to the given differential equation of the form
y(x) = erx, and thereby determine the general solution to
the differential equation.
32. y(iv) −2y′′′ −y′′ + 2y′ = 0.
33. y(iv) −13y′′ + 36y = 0.
[Hint: Factor the fourth-degree equation you get as a
product of two quadratic polynomials ﬁrst.]
For Problems 34–35, determine two linearly independent
solutions to the given differential equation of the form
y(x) = xr, and thereby determine the general solution to
the differential equation on (0, ∞).
34. x2y′′ + 3xy′ −8y = 0, x > 0.
35. 2x2y′′ + 5xy′ + y = 0, x > 0.
For Problems 36–37, determine three linearly independent
solutions to the given differential equation of the form
y(x) = xr, and thereby determine the general solution to
the differential equation on (0, ∞).
36. x3y′′′ + x2y′′ −2xy′ + 2y = 0, x > 0.
37. x3y′′′ + 3x2y′′ −6xy′ = 0, x > 0.
38. Determine a particular solution to the given dif-
ferential equation of the form yp(x)
=
A0e5x.
Also ﬁnd the general solution to the differential
equation:
y′′ + y′ −6y = 18e5x.
39. Determine a particular solution to the given differen-
tial equation of the form
yp(x) = A0 + A1x + A2x2.
Also ﬁnd the general solution to the differential
equation:
y′′ + y′ −2y = 4x2 + 5.
40. Determine a particular solution to the given differen-
tial equation of the form yp(x) = A0e2x. Also ﬁnd the
general solution to the differential equation:
y′′′ + 2y′′ −y′ −2y = 4e2x.
41. Determine a particular solution to the given differen-
tial equation of the form yp(x) = A0e−3x. Also ﬁnd
the general solution to the differential equation:
y′′′ + y′′ −10y′ + 8y = 24e−3x.
42. Determine a particular solution to the given differen-
tial equation of the form yp(x) = A0e−x. Also ﬁnd
the general solution to the differential equation:
y′′′ + 5y′′ + 6y′ = 6e−x.
43. Prove that the linear differential operator of order n,
L = Dn + a1Dn−1 + · · · + an−1D + an
is a linear transformation from Cn(I) to C0(I).
44. Extend the proof of Theorem 8.1.4 to an arbitrary
positive integer n.
45. Extend the proof of Theorem 8.1.6 to an arbitrary
positive integer n.
46. Let T : V →W be a linear transformation, and sup-
pose that {v1, v2, . . . , vn} is a basis for Ker(T ). Prove
that every solution to the operator equation
T (v) = w
(8.1.15)
is of the form
v = c1v1 + c2v2 + · · · + cnvn + vp,
where vp is any particular solution to Equation
(8.1.15).

8.2
Constant Coefficient Homogeneous Linear Differential Equations 505
8.2
Constant Coefficient Homogeneous Linear Differential Equations
In the next few sections we develop techniques for solving linear differential equations
of order n that have constant coefﬁcients. These are differential equations that can be
written in the form
y(n) + a1y(n−1) + · · · + an−1y′ + any = F(x),
where a1, a2, . . . , an are constants. To determine the general solution to this differen-
tial equation we will require the complementary function. Consequently, we begin by
analyzing the associated homogeneous equation
y(n) + a1y(n−1) + · · · + an−1y′ + any = 0.
In operator form, we write this as
P(D)y = 0,
where
P(D) = Dn + a1Dn−1 + · · · + an−1D + an.
The operator P(D) is called a polynomial differential operator. It is the special case
of the general linear differential operator introduced in the previous section that arises
when the coefﬁcients are constant. Associated with any polynomial differential operator
is the real polynomial
P(r) = rn + a1rn−1 + · · · + an−1r + an,
referred to as the auxiliary polynomial. The corresponding polynomial equation
P(r) = 0
is called the auxiliary equation.
Example 8.2.1
Write the differential equation y′′ + 5y′ −7y = 0 as P(D)y = 0 for an appropriate
polynomial differential operator P(D). Determine the auxiliary polynomial and the
auxiliary equation.
Solution:
The given differential equation can be written as
(D2 + 5D −7)y = 0.
That is,
P(D)y = 0,
where P(D) = D2 + 5D −7. Consequently, the auxiliary polynomial is
P(r) = r2 + 5r −7
and the auxiliary equation is
r2 + 5r −7 = 0.
□
In general, the composition of two linear transformations is not commutative, so
that, in particular, if L1 and L2 are two linear differential operators, then, in general,
L1L2 ̸= L2L1. According to the next theorem, however, commutativity does hold for
polynomial differential operators. This is a key result in determining all solutions to
P(D)y = 0.
Theorem 8.2.2
If P(D) and Q(D) are polynomial differential operators, then
P(D)Q(D) = Q(D)P(D).

506
CHAPTER 8
Linear Differential Equations of Order n
Proof The proof consists of a straightforward veriﬁcation and is left for the exercises
(Problem 45).
Example 8.2.3
If P(D) = D −5 and Q(D) = D + 7, verify that
P(D)Q(D) = Q(D)P(D).
Solution:
For any twice differentiable function f , we have
P(D)Q(D) f = (D −5)(D + 7) f
= (D −5)( f ′ + 7 f ) = f ′′ + 2 f ′ −35 f = (D2 + 2D −35) f.
Consequently,
P(D)Q(D) = D2 + 2D −35.
Similarly,
Q(D)P(D) f = (D + 7)(D −5) f
= (D + 7)( f ′ −5 f )
= f ′′ + 2 f ′ −35 f = (D2 + 2D −35) f,
so that
Q(D)P(D) = D2 + 2D −35 = P(D)Q(D).
□
The importance of the preceding theorem is that it enables us to factor polyno-
mial differential operators in the same way that we can factor real polynomials. More
speciﬁcally, if P(D) is a polynomial differential operator of degree n, then the auxiliary
polynomial P(r) can be factored as
P(r) = (r −r1)m1(r −r2)m2 · · · (r −rk)mk,
where mi denotes the multiplicity of the root ri, and
m1 + m2 + · · · + mk = n.
Consequently, P(D) has the corresponding factorization
P(D) = (D −r1)m1(D −r2)m2 · · · (D −rk)mk,
and Theorem 8.2.2 tells us that the ordering of the terms in this factored form of P(D)
does not matter. It follows that the differential equation P(D)y = 0 can be written as
(D −r1)m1(D −r2)m2 · · · (D −rk)mk y = 0.
(8.2.1)
The next step is to establish the following theorem.
Theorem 8.2.4
If P(D) = P1(D)P2(D) · · · Pk(D), where each Pi(D) is a polynomial differential
operator, then, for each i, 1 ≤i ≤k, any solution to Pi(D)y = 0 is also a solution to
P(D)y = 0.
Proof Suppose Pi(D)u = 0 for some i satisfying 1 ≤i ≤k. Then, since we can change
the order of the factors in a polynomial differential operator (with constant coefﬁcients),
it follows that the expression for P(D) can be rearranged as follows:
P(D) = P1(D) · · · Pi−1(D)Pi+1(D) · · · Pk(D)Pi(D).

8.2
Constant Coefficient Homogeneous Linear Differential Equations 507
Hence,
P(D)u = P1(D) · · · Pi−1(D)Pi+1(D) · · · Pk(D)Pi(D)u = 0.
Applying the preceding theorem to Equation (8.2.1), we see that any solutions to
(D −ri)mi y = 0
(8.2.2)
will also be solutions to the full differential equation (8.2.1). Therefore, we ﬁrst focus our
attention on differential equations of the form (8.2.2). Before determining the solution to
this differential equation, we recall two properties of the complex exponential function.
(See Appendix A for a fuller discussion of complex exponential functions.)
1. Euler’s formula:
e(a+ib)x = eax(cos bx + i sin bx).
(8.2.3)
2. If r = a + ib, then
d
dx (erx) = rerx.
We also need the following lemma concerning differential operators of the form
(D −r)m.
Lemma 8.2.5
Consider the differential operator (D −r)m, where m is a positive integer, and r is a real
or complex number. For any u ∈Cm(I),
(D −r)m(erxu) = erx Dm(u).
(8.2.4)
Proof When m = 1 we have
(D −r)(erxu) = erxu′ + rerxu −rerxu.
Thus,
(D −r)(erxu) = erxu′.
Repeating this procedure yields
(D −r)2(erxu) = (D −r)(erxu′) = erxu′′
so that in general,
(D −r)m(erxu) = erx Dm(u)
(8.2.5)
as required.
We can now determine a set of linearly independent solutions to the differential
equation (8.2.2).
Theorem 8.2.6
The differential equation (D −r)m y = 0, where m is a positive integer and r is a
real or complex number, has the following m solutions that are linearly independent on
any interval:
erx, xerx, x2erx, . . . , xm−1erx.
Proof Since
Dm(xk) = 0,
k = 0, 1, . . . , m −1,

508
CHAPTER 8
Linear Differential Equations of Order n
an application of the preceding lemma with u(x) = xk yields
(D −r)m(erxxk) = erx Dm(xk) = 0,
k = 0, 1, . . . , m −1,
and hence, erx, xerx, x2erx, . . . , xm−1erx are solutions to the differential equation
(D −r)m y = 0. We now prove that these solutions are linearly independent on any
interval. We must show that
c1erx + c2xerx + c3x2erx + · · · + cmxm−1erx = 0,
for x in any interval if and only if c1 = c2 = · · · = cm = 0. Dividing by erx, we obtain
the equivalent expression
c1 + c2x + c3x2 + · · · + cmxm−1 = 0.
Since the set of functions {1, x, x2, . . . , xm−1} is linearly independent on any interval
(see Problem 43 in Section 4.5), it follows that c1 = c2 = · · · = cm = 0. Hence, the
given functions are indeed linearly independent on any interval.
We now apply the results of the previous two theorems to the differential equation
(D −r1)m1(D −r2)m2 · · · (D −rk)mk y = 0.
(8.2.6)
The solutions that are obtained due to a term of the form (D −r)m depend on whether
r is a real or complex number. We consider the two cases separately.
1. Real Roots of the Auxiliary Equation: Each factor of the form (D −r)m, where r
is real, contributes the m linearly independent solutions
erx, xerx, . . . , xm−1erx.
2. Complex Roots of the Auxiliary Equation: Since complex roots of the auxiliary
equation occur in conjugate pairs, each factor of the form (D −r)m, where
r = a + ib (b ̸= 0), occurring in Equation (8.2.6) must be accompanied by a
term (D −r)m. These complex conjugate terms contribute the complex-valued
solutions
e(a±ib)x, xe(a±ib)x, x2e(a±ib)x, . . . , xm−1e(a±ib)x.
Real-valued solutions can be obtained in the following manner. Consider the two
complex conjugate solutions
w1(x) = xke(a+ib)x = xkeax(cos bx + i sin bx),
w2(x) = xke(a−ib)x = xkeax(cos bx −i sin bx),
where 0 ≤k ≤m −1 and we have used Euler’s formula (8.2.3). Since these are
both solutions to a linear homogeneous equation, any linear combination of them
is also a solution to the same equation. In particular, deﬁning y1(x) and y2(x) by
y1(x) = 1
2[w1(x) + w2(x)] = xkeax cos bx,
y2(x) = 1
2i [w1(x) −w2(x)] = xkeax sin bx,
respectively, yields two corresponding real-valued solutions. Repeating this pro-
cedure for each value of k, we obtain the following 2m real-valued solutions to
(D −r)m(D −r)m y = 0:
eax cos bx, eax sin bx, xeax cos bx, xeax sin bx, . . . , xm−1eax cos bx, xm−1eax sin bx

8.2
Constant Coefficient Homogeneous Linear Differential Equations 509
We leave the veriﬁcation that these solutions are linearly independent on any interval
for the exercises (Problems 46 and 47).
By considering each factor in Equation (8.2.6) successively, we can therefore obtain
n real-valued solutions to P(D)y = 0. The proof that the resulting set of solutions is
linearly independent on any interval is tedious and not particularly instructive. Conse-
quently, this proof is omitted (see, for example, Kaplan, W.L. Differential Equations,
Addison-Wesley, 1958).
We now summarize our results.
Theorem 8.2.7
Consider the differential equation
P(D)y = 0.
(8.2.7)
Let r1,r2, . . . ,rk be the distinct roots of the auxiliary equation, so that
P(r) = (r −r1)m1(r −r2)m2 · · · (r −rk)mk,
where mi denotes the multiplicity of the root r = ri.
1. If ri is real, then the functions eri x, xeri x, . . . , xmi−1eri x are linearly independent
solutions to Equation (8.2.7) on any interval.
2. If r j is complex, say r j = a + ib (a and b are real, with b ̸= 0), then the functions
eax cos bx, xeax cos bx, . . . , xm j−1eax cos bx
eax sin bx, xeax sin bx, . . . , xm j−1eax sin bx
corresponding to the conjugate roots r = a ±ib are linearly independent solutions
to Equation (8.2.7) on any interval.
3. The n real-valued solutions y1, y2, . . . , yn to Equation (8.2.7) that are obtained
by considering the distinct roots r1,r2, . . . ,rk are linearly independent on any
interval. Consequently, the general solution to Equation (8.2.7) is
y(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x).
Example 8.2.8
Determine the general solution to y′′ −y′ −2y = 0.
Solution:
The auxiliary polynomial is
P(r) = r2 −r −2 = (r −2)(r + 1).
Therefore, the auxiliary equation has roots r1 = 2, r2 = −1, so that two linearly
independent solutions to the given differential equation are
y1(x) = e2x
and
y2(x) = e−x.
Hence, the general solution to the differential equation is
y(x) = c1e2x + c2e−x.
Some representative solution curves are sketched in Figure 8.2.1.
□

510
CHAPTER 8
Linear Differential Equations of Order n
21
20.5
0.5
1
1.5
2
25
5
10
y
x
Figure 8.2.1: Representative solution curves for the differential equation in Example 8.2.8.
Example 8.2.9
Determine the general solution to y′′ + 6y′ + 25y = 0.
Solution:
The auxiliary equation is
r2 + 6r + 25 = 0
with roots r = −3 ± 4i. Consequently, two linearly independent real-valued solutions
to the differential equation are
y1(x) = e−3x cos 4x
and
y2(x) = e−3x sin 4x
and the general solution to the differential equation is
y(x) = e−3x(c1 cos 4x + c2 sin 4x).
We see that, due to the presence of the trigonometric functions, the solutions are oscilla-
tory. The negative exponential term implies that the amplitude of the oscillations decays
as x increases. Some representative solution curves are given in Figure 8.2.2.
□
20.5
0.5
1
1.5
2
2.5
21.5
21
20.5
0.5
1
1.5
y
x
Figure 8.2.2: Representative solution curves for the differential equation in Example 8.2.9.

8.2
Constant Coefficient Homogeneous Linear Differential Equations 511
Example 8.2.10
Solve the initial-value problem
y′′ + 4y′ + 4y = 0,
y(0) = 1,
y′(0) = 4.
Solution:
The auxiliary polynomial is
P(r) = r2 + 4r + 4 = (r + 2)2.
Thus, r = −2 is a repeated root of the auxiliary equation, and therefore two linearly
independent solutions to the given differential equation are
y1(x) = e−2x,
y2(x) = xe−2x.
Consequently, the general solution is
y(x) = e−2x(c1 + c2x).
Due to the presence of the negative exponential term, it follows that all solutions approach
zero as x →∞. The initial condition y(0) = 1 implies that c1 = 1. Thus,
y(x) = e−2x(1 + c2x).
Differentiating this expression yields
y′(x) = −2e−2x(1 + c2x) + c2e−2x
so that the second initial condition requires c2 = 6. Hence, the unique solution to the
given initial-value problem is
y(x) = e−2x(1 + 6x).
Some solution curves are sketched in Figure 8.2.3. Which one corresponds to the given
initial conditions?
0.5
1
1.5
2
2.5
3
21
20.5
0.5
1
1.5
x
y
Figure 8.2.3: Representative solution curves for the differential equation in Example 8.2.10.
□
Example 8.2.11
Determine the general solution to y′′′ + 3y′′ + 3y′ −7y = 0.
Solution:
The auxiliary polynomial is P(r) = r3 + 3r2 + 3r −7, which can be
factored as
P(r) = (r −1)(r2 + 4r + 7).

512
CHAPTER 8
Linear Differential Equations of Order n
The roots of the auxiliary equation are therefore
r = 1
and
r = −2 ± i
√
3.
Hence, three linearly independent solutions to the given differential equation are
y1(x) = ex,
y2(x) = e−2x cos
√
3x,
y3(x) = e−2x sin
√
3x,
so that the general solution is
y(x) = c1ex + c2e−2x cos
√
3x + c3e−2x sin
√
3x.
□
Example 8.2.12
Determine the general solution to
(D −3)(D2 + 2D + 2)2y = 0.
(8.2.8)
Solution:
The auxiliary polynomial is
P(r) = (r −3)(r2 + 2r + 2)2,
so that the roots of the auxiliary equation are r = 3, and r = −1 ± i (multiplicity 2).
The corresponding linearly independent solutions to Equation (8.2.8) are
y1(x) = e3x, y2(x) = e−x cos x, y3(x) = xe−x cos x,
y4(x) = e−x sin x, y5(x) = xe−x sin x,
and hence, the general solution to (8.2.8) is
y(x) = c1e3x + e−x(c2 cos x + c3x cos x + c4 sin x + c5x sin x).
□
Example 8.2.13
Determine the general solution to
D3(D −2)2(D2 + 1)2y = 0.
(8.2.9)
Solution:
The auxiliary polynomial is P(r) = r3(r −2)2(r2 +1)2, with zeros r = 0
(multiplicity 3), r = 2 (multiplicity 2), and r = ±i (multiplicity 2). We therefore obtain
the following linearly independent solutions to the given differential equation:
y1(x) = 1,
y2(x) = x,
y3(x) = x2,
y4(x) = e2x,
y5(x) = xe2x,
y6(x) = cos x,
y7(x) = x cos x,
y8(x) = sin x,
y9(x) = x sin x.
Hence, the general solution to (8.2.9) is
y(x) = c1 + c2x + c3x2 + c4e2x + c5xe2x + (c6 + c7x) cos x + (c8 + c9x) sin x.
□
Exercises for 8.2
Key Terms
Polynomial differential operator, Auxiliary polynomial,
Auxiliary equation.
Skills
• Be able to express an nth-order constant coefﬁcient
homogeneous linear differential equation in polyno-
mial differential operator form.
• Be able to ﬁnd the auxiliary polynomial and equation
associated with an nth-order constant coefﬁcient ho-
mogeneous linear differential equation.
• Be able to use the distinct roots (and their multiplici-
ties) of the auxiliary equation to ﬁnd n linearly inde-
pendent solutions to an nth-order constant coefﬁcient
homogeneous linear differential equation.
• Be able to ﬁnd the general solution to an nth-order
constant coefﬁcient homogeneous linear differential
equation.

8.2
Constant Coefficient Homogeneous Linear Differential Equations 513
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An nth-order constant coefﬁcient homogeneous lin-
ear differential equation has n linearly independent
solutions if and only if the corresponding auxiliary
polynomial as n distinct roots.
(b) Anytwodifferentialoperators L1(D) and L2(D) com-
mute; that is, L1(D)L2(D) = L2(D)L1(D).
(c) The roots of the auxiliary polynomial of an nth-order
constant coefﬁcient homogeneous linear differential
equation have multiplicities that sum to n.
(d) If 0 is a root of multiplicity four of the auxiliary poly-
nomial of an nth-order constant coefﬁcient homoge-
neouslineardifferentialequation,thenanypolynomial
of degree three or less is a solution to the differential
equation.
(e) The general solution to the differential equation
D(D2 −4)2y = 0
is y(x) = c1 + c2x + c3e2x + c4e−2x + c5xe2x +
c6xe−2x.
(f) The general solution to the differential equation
(D + 3)2(D2 + 25)y = 0
is y(x) = c1e−3x + c2xe−3x + c3 cos 5x + c4 sin 5x.
(g) The general solution to the differential equation
(D2 −4D + 5)2y = 0
is y(x) = c1e2x cos x + c2e2x sin x + c3xe2x cos x +
c4xe2x sin x.
(h) If y(x) is the general solution to the differential equa-
tion P(D)y = 0, then xy(x) is the general solution to
the differential equation (P(D))2y = 0.
Problems
For Problems 1–3, determine a basis for the solution space
of the given differential equation.
1. y′′ + 2y′ −3y = 0.
2. y′′ + 6y′ + 9y = 0.
3. y′′ −6y′ + 25y = 0.
4. Let S denote the subspace of the solution space to
the differential equation y′′ + 9y = 0, with basis
{2 sin 3x −7 cos 3x}. Write the general vector in S and
extend the basis for S to a basis for the full solution
space of the differential equation.
For problems 5–7 you will need to use the function inner
product < f, g > =
& b
a
f (x)g(x) dx in C2[a, b].
5. Determine a basis for the solution space to y′′ + y′ −
2y = 0 that is orthogonal on the interval [0, 1].
6. Determineabasisforthesolutionspaceto y′′+ 4y = 0
that is orthogonal on the interval [0, π/4].
7. Determine a basis for the solution space to y′′ + y = 0
that is orthonormal on the interval [−π, π].
For Problems 8–34, determine the general solution to the
given differential equation.
8. y′′ −y′ −2y = 0.
9. y′′ −6y′ + 9y = 0.
10. (D2 + 6D + 25)y = 0.
11. (D + 1)(D −5)y = 0.
12. (D + 2)2y = 0.
13. y′′ −6y′ + 34y = 0.
14. y′′ + 10y′ + 25y = 0.
15. (D2 −2)y = 0.
16. y′′ + 8y′ + 20y = 0.
17. y′′ + 2y′ + 2y = 0.
18. (D −4)(D + 2)y = 0.
19. y′′ −14y′ + 58y = 0.
20. y′′′ −y′′ + y′ −y = 0.
21. y′′′ −2y′′ −4y′ + 8y = 0.
22. (D −2)(D2 −16)y = 0.
23. (D2 + 2D + 10)2y = 0.
24. (D2 + 4)2(D + 1)y = 0.

514
CHAPTER 8
Linear Differential Equations of Order n
25. (D2 + 3)(D + 1)2y = 0.
26. D2(D −1)y = 0.
27. y(iv) −8y′′ + 16y = 0.
28. y(iv) −16y = 0.
29. y′′′ + 8y′′ + 22y′ + 20y = 0.
30. y(iv) −16y′′ + 40y′ −25y = 0.
31. (D −1)3(D2 + 9)y = 0.
32. (D2 −2D + 2)2(D2 −1)y = 0.
33. (D + 3)(D −1)(D + 5)3y = 0.
34. (D2 + 9)3y = 0.
For Problems 35–38, solve the given initial-value problem.
35. y′′ −8y′ + 16y = 0, y(0) = 2, y′(0) = 7.
36. y′′ −4y′ + 5y = 0, y(0) = 3, y′(0) = 5.
37. y′′′ −y′′ + y′ −y = 0,
y(0) = 0,
y′(0) = 1, y′′(0) = 2.
38. y′′′ + 2y′′ −4y′ −8y = 0,
y(0) = 0, y′(0) = 6, y′′(0) = 8.
39. Solve the initial-value problem
y′′ −2my′ + (m2 + k2)y = 0, y(0) = 0, y′(0) = k,
where m and k are positive constants.
40. Determine the general solution to
y′′ −2my′ + (m2 −k2)y = 0,
where m and k are positive constants. Show that the
solution can be written in the form:
y(x) = emx(c1 cosh kx + c2 sinh kx).
41. An object of mass m is attached to one end of a spring,
and the other end of the unstretched spring is attached
to a ﬁxed wall. (See Figure 8.2.4.)
Positive y-direction
Damping
mechanism
Mass, m
y 5 0
y(t)
Figure 8.2.4: The spring-mass system considered in
Problem 41.
The object is pulled to the right a distance y0 and re-
leased from rest. Assuming that there is a damping
force that is proportional to the velocity of the object,
an application of Hooke’s law and Newton’s second
law of motion yields an initial-value problem that can
be written in the form (using appropriate units)
d2y
dt2 + 2cdy
dt + k2y = 0,
y(0) = y0,
dy
dt (0) = 0,
where y(t)denotesthedisplacementofthespringfrom
its equilibrium position at time t, and c, k are positive
constants.
(a) Assuming that c2 < k2, solve the preceding
initial-value problem to obtain
y(t) =
" y0
ω
#
e−ct(ω cos ωt + c sin ωt),
where ω =
√
k2 −c2.
(b) Show that the solution in (a) can be written in the
form
y(t) =
'ky0
ω
(
e−ct sin(ωt + φ),
where φ = tan−1(ω/c), and then sketch the graph
of y against t. Is the predicted motion reasonable?
Explain.
42. Consider the partial differential equation (Laplace’s
equation)
∂2u
∂x2 + ∂2u
∂y2 = 0.
(8.2.10)
(a) Show that the substitution
u(x, y) = ex/α f (ξ),
where ξ = βx −αy, (and α, β are positive con-
stants) reduces Equation (8.2.10) to the differen-
tial equation
d2 f
dξ2 + 2p d f
dξ + q
α2 f = 0,
(8.2.11)
where
p =
β
α(α2 + β2),
q =
1
α2 + β2 . (8.2.12)
[Hint: Use the chain rule, for example:
∂f
∂x = d f
dξ · ∂ξ
∂x .]

8.3
The Method of Undetermined Coefficients: Annihilators 515
(b) Solve Equation (8.2.11), and hence, ﬁnd the cor-
responding solution to (8.2.10). [Hint: In solving
Equation (8.2.11), you will need to use (8.2.12)
in order to obtain a simple form of solution.]
43. Consider the differential equation
y′′ + a1y′ + a2y = 0,
(8.2.13)
where a1, a2 are constants.
(a) If the auxiliary equation has real roots r1 and r2,
what conditions on these roots would guarantee
that every solution to Equation (8.2.13) satisﬁes
lim
x→+∞y(x) = 0?
(b) If the auxiliary equation has complex conjugate
roots r = a ± ib, what conditions on these roots
would guarantee that every solution to Equation
(8.2.13) satisﬁes
lim
x→+∞y(x) = 0?
(c) If a1, a2 are positive, prove that
lim
x→+∞y(x) = 0,
for every solution to Equation (8.2.13).
(d) If a1 > 0 and a2 = 0, prove that all solutions
to Equation (8.2.13) approach a constant value as
x →+∞.
(e) If a1 = 0 and a2 > 0, prove that all solutions to
Equation (8.2.13) remain bounded as x →+∞.
44. Consider P(D)y = 0. What conditions on the roots
of the auxiliary equation would guarantee that every
solution to the differential equation satisﬁes
lim
x→+∞y(x) = 0?
45. Prove Theorem 8.2.2.
46. For all constants a and b, prove that the set of functions
{eax cos bx, eax sin bx, xeax cos bx, xeax sin bx} is
linearly independent on (−∞, ∞).
47. Generalizing the previous exercise, prove that the set
of functions
{eax cos bx, xeax cos bx, x2eax cos bx, . . . ,
xmeax cos bx, eax sin bx, xeax sin bx,
x2eax sin bx, . . . , xmeax sin bx}
islinearlyindependenton(−∞, ∞).[Hint:Showthat
the condition for determining linear dependence or lin-
ear independence can be written as
P(x) cos bx + Q(x) sin bx = 0,
where P(x) = c0 + c1x + · · · + cmxm and Q(x) =
d0 + d1x + · · · + dmxm. Then show that this implies
P(nπ/b) = 0 and Q((2n + 1)π/b) = 0 for all inte-
gers n, which means that P and Q must both be the
zero polynomial.]
For Problems 48–52, use some form of technology to factor
the auxiliary polynomial of the given differential equation.
Write the general solution to the differential equation.
48. ⋄y′′′ −7y′′ −193y′ −665y = 0.
49. ⋄y(iv) + 4y′′′ −3y′′ −64y′ −208y = 0.
50. ⋄y(iv) + 8y′′′ + 28y′′ + 47y′ + 36y = 0.
51. ⋄y(v) + 4y(iv) + 50y′′′ + 200y′′ + 625y′ +
2500y = 0.
52. ⋄y(vii) + 3y(vi) + 3y(v) + 9y(iv) + 3y′′′ + 9y′′ +
y′ + 3y = 0.
For Problems 53–54, use some form of technology to solve
the given initial-value problem. Also sketch the solution
curve.
53. ⋄Problem 37.
54. ⋄Problem 38.
8.3
The Method of Undetermined Coefficients: Annihilators
According to Theorem 8.1.8, the general solution to the nonhomogeneous differential
equation
P(D)y = F(x)
(8.3.1)
is of the form
y(x) = yc(x) + yp(x),

516
CHAPTER 8
Linear Differential Equations of Order n
where yc is the general solution to the associated homogeneous differential equation and
yp is one particular solution to (8.3.1). We have seen in the previous section how yc
can be obtained. We now turn our attention to determining a particular solution yp. In
this section we develop a method that can be applied whenever F(x) has certain special
forms. This technique can be introduced quite simply as follows. Consider the differen-
tial equation (8.3.1), and suppose that there is a polynomial differential operator A(D)
such that
A(D)F = 0.
Then, operating on (8.3.1) with A(D) yields the homogeneous differential equation
A(D)P(D)y = 0.
(8.3.2)
This is a constant coefﬁcient homogeneous linear differential equation and therefore can
be solved using the technique of the previous section. The key point is the following.
Any solution to (8.3.1) must also solve (8.3.2). Consequently, by choosing the arbitrary
constants in the general solution to (8.3.2) appropriately, we must be able to obtain a
particular solution to (8.3.1). We note that the general solution to (8.3.2) will contain
the complementary function for (8.3.1), since P(D) is part of the composed differential
operator A(D)P(D) in (8.3.2). Hence, we must be able to obtain a particular solution
to (8.3.1) from that part of the general solution to (8.3.2) that does not include the
complementary function.
Example 8.3.1
Determine the general solution to
(D + 3)(D −3)y = 10e2x.
(8.3.3)
Solution:
We ﬁrst obtain the complementary function. The auxiliary polynomial is
P(r) = (r + 3)(r −3),
so that
yc(x) = c1e−3x + c2e3x.
The nonhomogeneous term in (8.3.3) is F(x) = 10e2x, and so we need a polynomial dif-
ferential operator A(D) such that A(D)F = 0. It is easily veriﬁed that (D−2)(e2x) = 0,
so that we can choose
A(D) = D −2.
Operating on (8.3.3) with A(D) yields the homogeneous differential equation
(D −2)(D + 3)(D −3)y = 0,
which has general solution
y(x) = c1e−3x + c2e3x + A0e2x.
This solution must contain a particular solution to (8.3.3) for appropriate values of the
constants c1, c2, and A0. However, the ﬁrst two terms coincide with the complementary
function to (8.3.3) and therefore satisfy
(D + 3)(D −3)(c1e−3x + c2e3x) = 0.
Consequently, (8.3.3) must have a solution of the form
yp(x) = A0e2x.
(8.3.4)

8.3
The Method of Undetermined Coefficients: Annihilators 517
We call yp(x) a trial solution for the differential equation (8.3.3). It contains one unde-
termined coefﬁcient, A0. In order to determine the appropriate value for A0, we substitute
the trial solution into (8.3.3). We have
(D + 3)(D −3)(A0e2x) = 10e2x.
That is,
(D2 −9)(A0e2x) = 10e2x,
or equivalently,
A0(4e2x −9e2x) = 10e2x.
We must therefore choose A0 to satisfy
−5A0e2x = 10e2x,
so that A0 = −2. Substituting this value for A0 into (8.3.4) yields the following particular
solution to (8.3.3):
yp(x) = −2e2x.
Consequently, the general solution to (8.3.3) is
y(x) = yc(x) + yp(x) = c1e−3x + c2e3x −2e2x.
□
This technique for obtaining a particular solution is called the method of undeter-
mined coefﬁcients. It is applicable only to linear differential equations that satisfy the
following two conditions:
1. The differential equation has constant coefﬁcients and therefore is of the form
P(D)y = F(x).
(8.3.5)
2. There exists a polynomial differential operator A(D) such that
A(D)F(x) = 0.
(8.3.6)
Any polynomial differential operator A(D) that satisﬁes (8.3.6) is said to annihilate
F(x). The polynomial differential operator of lowest order that satisﬁes Equation (8.3.6)
is called the annihilator of F.
Example 8.3.2
Show that A(D) = D2 + 4 annihilates F(x) = 5 cos 2x.
Solution:
We have
A(D)(5 cos 2x) = (D2 + 4)(5 cos 2x) = D2(5 cos 2x) + 20 cos 2x
= −20 cos 2x + 20 cos 2x = 0.
□
More generally, a polynomial differential operator A(D) annihilates F(x) if and
only if y = F(x) is a solution to
A(D)y = 0.
Thus, the only types of functions that can be annihilated by a polynomial differential
operator are those that arise as solutions to a homogeneous constant coefﬁcient linear
differential equation. From our results of the previous section it follows that F(x) must
be one of the following forms:
1. F(x) = cxkeax,
2. F(x) = cxkeax sin bx,

518
CHAPTER 8
Linear Differential Equations of Order n
3. F(x) = cxkeax cos bx,
4. sums of (1)–(3),
where a, b, c are real numbers and k is a nonnegative integer. We next derive appropriate
annihilators to cover any case that might arise. Consider ﬁrst F(x) = xkeax, where a is
a real number. Since the differential equation
(D −a)k+1y = 0,
where a is a real number and k is a nonnegative integer, has the real-valued solutions
eax, xeax, . . . , xkeax,
it follows that
1. A(D) = (D −a)k+1 annihilates each of the functions
eax, xeax, . . . , xkeax,
and therefore, it also annihilates
F(x) = (a0 + a1x + · · · + akxk)eax,
for all values of the constants a0, a1, . . . , ak.
Remark
Note the special case of (1) that arises when a = 0, namely
A(D) = Dk+1 annihilates F(x) = a0 + a1x + · · · + akxk.
Now consider the functions eax cos bx and eax sin bx, where a and b are real num-
bers. These functions arise as linearly independent (real-valued) solutions to the differ-
ential equation
(D −α)(D −α)y = 0,
where α = a + ib. Expanding the polynomial differential operator, we have
(D2 −2aD + a2 + b2)y = 0.
Consequently,
2. A(D) = D2 −2aD + a2 + b2 annihilates both of the functions
eax cos bx and eax sin bx,
and therefore, it also annihilates
F(x) = eax(a0 cos bx + b0 sin bx),
for all values of the constants a0, b0. In particular,
A(D) = D2 + b2
annihilates the functions cos bx and sin bx.
Further, the functions
eax cos bx, xeax cos bx, x2eax cos bx, . . . , xkeax cos bx,
eax sin bx, xeax sin bx, x2eax sin bx, . . . , xkeax sin bx,
arise as linearly independent (real-valued) solutions to the differential equation
(D2 −2aD + a2 + b2)k+1y = 0.

8.3
The Method of Undetermined Coefficients: Annihilators 519
Equivalently, we can state that
3. A(D) = (D2 −2aD + a2 + b2)k+1 annihilates each of the functions
eax cos bx, xeax cos bx, x2eax cos bx, . . . , xkeax cos bx,
eax sin bx, xeax sin bx, x2eax sin bx, . . . , xkeax sin bx,
and hence, for all values of the constants a0, a1, . . . , ak, b0, b1, . . . , bk,
it annihilates
F(x) = (a0 + a1x + · · · + akxk)eax cos bx + (b0 + b1x + · · · + bkxk)eax sin bx.
Finally, if A1(D)F1 = 0 and A2(D)F2 = 0, then
A1(D)A2(D)(F1 + F2) = A1(D)A2(D)F1 + A1(D)A2(D)F2
= A2(D)A1(D)F1 + A1(D)(0)
= A2(D)(0) = 0.
We can therefore state:
4. If F(x) is a sum of functions of the forms given in (1)–(3), then
F(x) F(x) is annihilated by the corresponding product of the annihilators
in (1)–(3).
The following examples give further illustrations of the annihilator technique.
Example 8.3.3
Determine the general solution to
(D −4)(D + 1)y = 15e4x.
(8.3.7)
Solution:
The auxiliary polynomial for the given differential equation is P(r) =
(r −4)(r + 1), so that the complementary function is
yc(x) = c1e−x + c2e4x.
In this case, F(x) = 15e4x, which has annihilator A(D) = D −4. Operating on the
given differential equation with A(D) yields the homogeneous differential equation
(D −4)2(D + 1)y = 0
with general solution
y(x) = c1e−x + c2e4x + A0xe4x.
Since the ﬁrst two terms coincide with the complementary function, an appropriate trial
solution for (8.3.7) is
yp(x) = A0xe4x.
To determine A0 we substitute this trial solution into (8.3.7). Differentiating yp twice
yields
y′
p(x) = A0e4x(4x + 1),
y′′
p = A0e4x(16x + 8).
Thus, substituting yp into (8.3.7) it follows that A0 must satisfy
A0e4x[(16x + 8) −3(4x + 1) −4x] = 15e4x.

520
CHAPTER 8
Linear Differential Equations of Order n
Simplifying,
5A0 = 15,
so that
A0 = 3.
Consequently, a particular solution to (8.3.7) is
yp(x) = 3xe4x.
Hence, the general solution to (8.3.7) is
y(x) = c1e−x + c2e4x + 3xe4x.
□
Example 8.3.4
Solve the initial-value problem
y′′ −y′ −2y = 10 sin x,
y(0) = 0,
y′(0) = 1.
(8.3.8)
Solution:
The auxiliary polynomial is
P(r) = r2 −r −2 = (r −2)(r + 1)
so that the complementary function is
yc(x) = c1e2x + c2e−x.
The annihilator for F(x) = 10 sin x is A(D) = D2 +1. Writing the differential equation
in (8.3.8) in operator form (D −2)(D + 1)y = 10 sin x and operating on this equation
with A(D) therefore yields
(D2 + 1)(D2 −D −2)y = 0,
which has general solution
y(x) = c1e2x + c2e−x + A0 sin x + A1 cos x.
The ﬁrst two terms coincide with the complementary function, so that an appropriate
trial solution is
yp(x) = A0 sin x + A1 cos x.
Substituting this trial solution into Equation (8.3.8) yields
(−A0 sin x −A1 cos x) −(A0 cos x −A1 sin x) −2(A0 sin x + A1 cos x) = 10 sin x.
That is,
(−3A0 + A1) sin x −(A0 + 3A1) cos x = 10 sin x.
This equation is satisﬁed for all x if and only if
−3A0 + A1 = 10
and
A0 + 3A1 = 0.
The unique solution to this system of equations is
A0 = −3
and
A1 = 1,
so that a particular solution to the differential equation in Equation (8.3.8) is
yp(x) = −3 sin x + cos x.

8.3
The Method of Undetermined Coefficients: Annihilators 521
Consequently the general solution is
y(x) = c1e2x + c2e−x −3 sin x + cos x.
(8.3.9)
We now impose the initial conditions given in Equation (8.3.8). From Equation (8.3.9),
y(0) = 0 if and only if
c1 + c2 = −1,
(8.3.10)
whereas y′(0) = 1 if and only if
2c1 −c2 = 4.
(8.3.11)
Solving Equations (8.3.10) and (8.3.11) yields
c1 = 1
and
c2 = −2,
so that, from Equation (8.3.9), the unique solution to the given initial-value problem is
y(x) = e2x −2e−x −3 sin x + cos x.
□
Example 8.3.5
Determine the general solution to
(D2 + 1)y = 3 cos x + 4 sin x.
(8.3.12)
Solution:
The complementary function is
yc(x) = c1 cos x + c2 sin x.
Furthermore, the annihilator for F(x) = 3 cos x + 4 sin x is A(D) = D2 + 1. Operating
on the differential equation (8.3.12) with A(D) yields
(D2 + 1)2y = 0,
which has general solution
y(x) = c1 cos x + c2 sin x + x(A0 cos x + B0 sin x).
Hence, a trial solution for (8.3.12) is
yp(x) = x(A0 cos x + B0 sin x).
Consequently,
y′
p(x) = x(−A0 sin x + B0 cos x) + A0 cos x + B0 sin x
and
y′′
p(x) = −x(A0 cos x + B0 sin x) + 2(−A0 sin x + B0 cos x).
Substituting these results into Equation (8.3.12) and simplifying yields
2(−A0 sin x + B0 cos x) = 3 cos x + 4 sin x,
so that A0 = −2, and B0 = 3/2. Therefore, a particular solution to Equation (8.3.12) is
yp(x) = x
'
−2 cos x + 3
2 sin x
(
= 1
2 x(3 sin x −4 cos x),

522
CHAPTER 8
Linear Differential Equations of Order n
and the general solution is
y(x) = c1 cos x + c2 sin x + 1
2 x(3 sin x −4 cos x).
□
Example 8.3.6
Determine the general solution to
(D2 −4D + 5)y = 8xe2x cos x
(8.3.13)
Solution:
The auxiliary equation is r2 −4r +5 = 0, with roots r = 2±i. Therefore,
yc(x) = e2x(c1 cos x + c2 sin x).
An annihilator for F(x) = 8xe2x cos x is A(D) = (D2 −4D + 5)2. Operating on
Equation (8.3.13) with A(D) yields
(D2 −4D + 5)3y = 0,
which has general solution
y(x) = e2x(c1 cos x+c2 sin x)+xe2x(A0 cos x+B0 sin x)+x2e2x(A1 cos x+B1 sin x).
Neglectingthecontributionfromthecomplementaryfunction,weobtainthetrialsolution
yp(x) = xe2x(A0 cos x + B0 sin x) + x2e2x(A1 cos x + B1 sin x).
Substituting into Equation (8.3.13) and simplifying yields
(−2A0 + 2B1 −4x A1) sin x + (2B0 + 2A1 + 4x B1) cos x = 8x cos x,
so that A0, A1, B0, B1 must satisfy
−2A0 + 2B1 = 0,
2A1 + 2B0 = 0,
−4A1 = 0,
4B1 = 8.
We see that A1 = 0 = B0 and A0 = 2 = B1, so that a particular solution to (8.3.13) is
yp(x) = 2xe2x(x sin x + cos x),
and the general solution is
y(x) = e2x[c1 cos x + c2 sin x + 2x(x sin x + cos x)].
□
Example 8.3.7
Use the annihilator technique to determine a trial solution for
(D + 1)(D2 + 9)y = 4xe−x + 5e2x cos 3x.
Solution:
The complementary function is
yc(x) = c1e−x + c2 cos 3x + c3 sin 3x.
An annihilator for F1(x) = 4xe−x is
A1(D) = (D + 1)2,

8.3
The Method of Undetermined Coefficients: Annihilators 523
whereas an annihilator for F2(x) = 5e2x cos 3x is
A2(D) = D2 −4D + 13.
Hence,operatingonthegivendifferentialequationwith A(D) = (D2−4D+13)(D+1)2
yields the homogeneous differential equation
(D2 −4D + 13)(D + 1)3(D2 + 9)y = 0,
which has general solution
y(x) = c1e−x +c2 cos 3x+c3 sin 3x+ A0xe−x + A1x2e−x +e2x(B0 cos 3x+B1 sin 3x).
Consequently, a trial solution for the given differential equation is
yp(x) = A0xe−x + A1x2e−x + e2x(B0 cos 3x + B1 sin 3x).
□
In the preceding examples, we have used annihilators to determine appropriate
trial solutions on a case-by-case basis for the given nonhomogeneous linear constant
coefﬁcient differential equation with F(x) of one of the forms (1)–(4). As we now show,
it is actually possible to derive generally the appropriate trial solutions without the need
to make reference to annihilators. For example, consider
P(D)y = cxkeax,
(8.3.14)
and let yc denote the complementary function. The appropriate annihilator for Equation
(8.3.14) is A(D) = (D −a)k+1, and so a trial solution for Equation (8.3.14) can be
determined from the general solution to
A(D)P(D)y = 0.
(8.3.15)
The following two cases arise:
Case 1: r = a is not a root of P(r) = 0: Then the general solution to Equation (8.3.15)
will be of the form
y(x) = yc(x) + eax(A0 + A1x + · · · + Akxk),
so that an appropriate trial solution is
yp(x) = eax(A0 + A1x + · · · + Akxk).
This is the “usual” trial solution.
Case 2: r = a is a root of multiplicity m of P(r) = 0: Then the complementary
function yc(x) will contain the terms
eax(c0 + c1x + · · · + cm−1xm−1).
The operator A(D)P(D) will therefore contain the factor (D −a)m+k+1, so that the
terms in the general solution to Equation (8.3.15) that do not arise in the complementary
function are
yp(x) = eaxxm(A0 + A1x + · · · + Akxk),
which is the “modiﬁed” trial solution.
The derivation of appropriate trial solutions for P(D)y = F(x) in the case when
P(x) = cxkeax cos bx
and
F(x) = cxkeax sin bx
is left as an exercise (Problem 48). We summarize the results in a table.

524
CHAPTER 8
Linear Differential Equations of Order n
F(x)
Usual Trial Solution
Modiﬁed Trial Solution
If a is a root of P(r) = 0
cxkeax
If P(a) ̸= 0:
of multiplicity m: yp(x) =
yp(x) = eax(A0 + · · · + Akxk)
xmeax(A0 + · · · + Akxk)
If P(a + ib) ̸= 0:
If a + ib is a root of P(r)
cxkeax cos bx
yp(x) =
of multiplicity m: yp(x) =
or
eax[A0 cos bx + B0 sin bx
xmeax[A0 cos bx + B0 sin bx
cxkeax sin bx
+ x(A1 cos bx + B1 sin bx)
+ x(A1 cos bx + B1 sin bx)
+ · · · +
+ · · · +
xk(Ak cos bx + Bk sin bx)]
xk(Ak cos bx + Bk sin bx)]
If F(x) is the sum of functions of the preceding form, then the appropriate trial solution
is the corresponding sum.
In the following table, we have specialized the foregoing trial solutions to the cases
that arise most often in applications:
F(x)
Usual Trial Solution
Modiﬁed Trial Solution
If a is a root of P(r) = 0
ceax
If P(a) ̸= 0:
of multiplicity m:
yp(x) = A0eax
yp(x) = A0xmeat
c cos bx
If ib is a root of P(r) = 0
or
If P(ib) ̸= 0:
of multiplicity m: yp(x) =
c sin bx
yp(x) = A0 cos bx + B0 sin bx
xm(A0 cos bx + B0 sin bx)
If zero is a root of P(r) = 0
cxk
If P(0) ̸= 0:
of multiplicity m: yp(x) =
yp(x) = A0 + A1x + · · · + Akxk
xm(A0 + A1x + · · · + Akxk)
Exercises for 8.3
Key Terms
Trial
solution,
Undetermined
coefﬁcients,
Annihilate,
Annihilator.
Skills
• Be able to determine the annihilator of a given
function.
• Be able to use annihilators to derive an appropriate
trial solution for the constant coefﬁcient differential
equation P(D)y = F(x).
• Be able to determine the general solution to
an nth-order constant coefﬁcient nonhomogeneous
differential equation P(D)y = F(x) by using an ap-
propriate trial solution.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If A1(D) annihilates F1(x) and A2(D) annihilates
F2(x),then A1(D)+A2(D)annihilates F1(x)+F2(x).
(b) The annihilator of F(x) = a0 + a1x + · · · + akxk is
A(D) = Dk.
(c) The annihilator of F(x) = xeax is A(D) = (D −a)2.
(d) Every function F(x) has a unique annihilator A(D).

8.3
The Method of Undetermined Coefficients: Annihilators 525
(e) If A1(D)A2(D) annihilates F(x), then either A1(D)
annihilates F(x) or A2(D) annihilates F(x).
(f) The appropriate trial solution for the 4th-order differ-
ential equation D2(D2 + 4)y = 3 −5x is yp(x) =
A0 + A1x + A2x2 + A3x3.
(g) The appropriate trial solution for the 5th-order dif-
ferential equation D3(D2 + 1)y = x4 is yp(x) =
A0 + A1x + A2x2 + A3x3 + A4x4.
(h) The appropriate trial solution for the 5th-order differ-
ential equation D3(D2 + 1)y = cos x is yp(x) =
x(A0 cos x + B0 sin x).
Problems
For Problems 1–16, determine the annihilator of the given
function.
1. F(x) = 5e−3x.
2. F(x) = 2ex −3x.
3. F(x) = sin x + 3xe2x.
4. F(x) = x3e7x + 5 sin 4x.
5. F(x) = 4e−2x sin x.
6. F(x) = ex sin 2x + 3 cos 2x.
7. F(x) = (1 −3x)e4x + 2x2.
8. F(x) = e5x(2 −x2) cos x.
9. F(x) = e−3x(2 sin x + 7 cos x).
10. F(x) = e4x(x −2 sin 5x) + 3x −x2e−2x cos x.
11. F(x) = x2 sin x.
12. F(x) = x cos 3x.
13. F(x) = cos2 x. [Hint: Write cos2 x = 1 + cos 2x
2
.]
14. F(x) = sin4 x. [Hint: Write sin2 x = 1 −cos 2x
2
.]
15. F(x) = sin x cos3 x.
16. F(x) = sin2 x cos2 x cos2 2x.
For Problems 17–32, determine the general solution to the
given differential equation. Derive your trial solution using
the annihilator technique.
17. (D −1)(D + 2)y = 5e3x.
18. (D + 5)(D −2)y = 14e2x.
19. (D2 + 16)y = 4 cos x.
20. (D −1)2y = 6ex.
21. (D −2)(D + 1)y = 4x(x −2).
22. (D2 −1)y = 3e2x −8e3x.
23. (D + 1)(D −3)y = 4(e−x −2 cos x).
24. D(D + 3)y = x(5 + ex).
25. y′′ + y = 6ex.
26. y′′ + 4y′ + 4y = 5xe−2x.
27. y′′ + 4y = 8 sin 2x.
28. y′′ −y′ −2y = 5e2x.
29. y′′ + 2y′ + 5y = 3 sin 2x.
30. y′′′ + 2y′′ −5y′ −6y = 4x2.
31. y′′′ −y′′ + y′ −y = 9e−x.
32. y′′′ + 3y′′ + 3y′ + y = 2e−x + 3e2x.
For Problems 33–37, solve the given initial-value problem:
33. y′′ + 9y = 5 cos 2x, y(0) = 2, y′(0) = 3.
34. y′′ −y = 9xe2x, y(0) = 0, y′(0) = 7.
35. y′′ + y′ −2y = −10 sin x, y(0) = 2, y′(0) = 1.
36. (D2 + D −2)y = 4 cos x −2 sin x,
y(0) = −1, y′(0) = 4.
37. (D −1)(D −2)(D −3)y = 6e4x, y(0) = 4,
y′(0) = 10, y′′(0) = 30.
38. At time t the displacement from equilibrium, y(t), of
an undamped spring-mass system of mass m is gov-
erned by the initial-value problem
d2y
dt2 + ω2y = F0
m cos ωt, y(0) = 1, dy
dt (0) = 0,
where F0 and ω are positive constants. Solve this
initial-value problem to determine the motion of the
system. What happens as t →∞?

526
CHAPTER 8
Linear Differential Equations of Order n
For Problems 39–47, determine an appropriate trial solution
for the given differential equation. Do not solve for the con-
stants that arise in your trial solution.
39. (D −2)(D −3)y = 7e2x.
40. (D + 1)(D2 + 1)y = 4xex.
41. (D2 + 4D + 13)2y = 5e−2x cos 3x.
42. (D2 + 4)(D −2)3y = 4x + 9xe2x.
43. D2(D −1)(D2 + 4)2y = 11ex −sin 2x.
44. (D2 −2D +2)3(D −2)2(D +4)y = ex cos x −3e2x.
45. D(D2 −9)(D2 −4D + 5)y = 2e3x + e2x sin x.
46. (D + 3)(D −1)y = sin2 x.
47. (D2 + 6)y = sin2 x cos2 x.
48. Derive an appropriate trial solution for the differential
equation
P(D)y = cxkeax cos bx.
8.4
Complex-Valued Trial Solutions
We now introduce an alternative method for solving constant coefﬁcient differential
equations of the form
y′′ + a1y′ + a2y = F(x)
when F(x) contains terms of the form xkeax sin bx or xkeax cos bx. The technique is
based on the observation that
xkeax cos bx = Re{xke(a+ib)x} and
xkeax sin bx = Im{xke(a+ib)x},
where “Re” and “Im” denote the real part and the imaginary part of a complex-valued
function, respectively. To see why this observation is useful, we need the next theorem.
Theorem 8.4.1
If y(x) = u(x) + iv(x) is a complex-valued solution to
y′′ + a1y′ + a2y = F(x) + iG(x)
(8.4.1)
then
u′′ + a1u′ + a2u = F(x) and v′′ + a1v′ + a2v = G(x).
Proof If y(x) = u(x) + iv(x), then
y′′ + a1y′ + a2y = [u(x) + iv(x)]′′ + a1[u(x) + iv(x)]′ + a2[u(x) + iv(x)]
= (u′′ + a1u′ + a2u) + i(v′′ + a1v′ + a2v).
Since y solves Equation (8.4.1) we must have
(u′′ + a1u′ + a2u) + i(v′′ + a1v′ + a2v) = F(x) + iG(x).
Equating real and imaginary parts on either side of this equation yields the desired result.
Consequently, if we solve the complex equation
y′′ + a1y′ + a2y = cxke(a+ib)x,
(8.4.2)
then by taking the real and imaginary parts of the resulting complex-valued solution, we
can directly determine solutions to
y′′ + a1y′ + a2y = cxkeax cos bx
and
y′′ + a1y′ + a2y = cxkeax sin bx. (8.4.3)

8.4
Complex-Valued Trial Solutions 527
The key point is that Equation (8.4.2) is a simpler equation to solve than its real coun-
terparts given in (8.4.3). We illustrate the technique with some examples.
Example 8.4.2
Solve
y′′ + y′ −6y = 4 cos 2x.
(8.4.4)
Solution:
The complementary function for Equation (8.4.4) is
yc(x) = c1e−3x + c2e2x.
In determining a particular solution, we consider the complex differential equation
z′′ + z′ −6z = 4e2ix.
(8.4.5)
An appropriate complex-valued trial solution for this differential equation is
z p(x) = A0e2ix,
(8.4.6)
where A0 is a complex constant. The ﬁrst two derivatives of z p are
z′
p(x) = 2i A0e2ix
and z′′
p(x) = −4A0e2ix,
so that z p is a solution to Equation (8.4.5) if and only if
(−4A0 + 2i A0 −6A0)e2ix = 4e2ix.
This is the case if and only if
A0 =
2
−5 + i = −1
13(5 + i).
Substituting this value of A0 into (8.4.6) yields
z p(x) = −1
13(5 + i)e2ix = −1
13(5 + i)(cos 2x + i sin 2x)
= 1
13(sin 2x −5 cos 2x) −1
13i(cos 2x + 5 sin 2x).
Consequently, a particular solution to Equation (8.4.4) is
yp(x) = Re{z p} = 1
13(sin 2x −5 cos 2x),
so that the general solution to Equation (8.4.4) is
y(x) = yc(x) + yp(x) = c1e−3x + c2e2x + 1
13(sin 2x −5 cos 2x).
Notice that we can also write down the general solution to the differential equation
y′′ + y′ −6y = 4 sin 2x,
since a particular solution will just be Im{z p}.
□

528
CHAPTER 8
Linear Differential Equations of Order n
Example 8.4.3
Solve
y′′ −2y′ + 5y = 8ex sin 2x.
(8.4.7)
Solution:
The complementary function is
yc(x) = ex(c1 cos 2x + c2 sin 2x).
In order to determine a particular solution to Equation (8.4.7), we consider the complex
counterpart
z′′ −2z′ + 5z = 8e(1+2i)x.
(8.4.8)
Since 1+2i is a root of the auxiliary equation, an appropriate trial solution for Equation
(8.4.8) is
z p(x) = A0xe(1+2i)x.
(8.4.9)
Differentiating with respect to x yields
) z′
p(x) = A0e(1+2i)x[(1 + 2i)x + 1],
z′′
p(x) = A0e(1+2i)x[(1 + 2i)2x + 2(1 + 2i)] = A0e(1+2i)x[(−3 + 4i)x + 2(1 + 2i)].
Substituting into Equation (8.4.8) leads to the following condition on A0:
A0 [(−3 + 4i)x + 2(1 + 2i) −2(1 + 2i)x −2 + 5x] = 8.
Hence,
A0 = 2
i = −2i.
It follows from Equation (8.4.9) that a complex-valued solution to Equation (8.4.8) is
z p(x) = −2ixe(1+2i)x = −2ixex(cos 2x + i sin 2x),
and so a particular solution to the original differential equation is
yp(x) = Im{z p} = −2xex cos 2x.
Consequently, Equation (8.4.7) has general solution
y(x) = ex(c1 cos 2x + c2 sin 2x) −2xex cos 2x.
□
Exercises for 8.4
Skills
• Be able to use the method of Section 8.3 to solve
differential equations of the form (8.4.3) by using a
complex-valued trial solution.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An appropriate complex-valued trial solution for the
differential equation y′′ + y′ = e−x cos 2x is yp(x) =
A0e(−1+2i)x.
(b) An appropriate complex-valued trial solution for the
differential equation y′′ + 2y′ −15y = xe2x sin 3x is
yp(x) = A0xe(2+3i)x.
(c) An appropriate complex-valued trial solution for the
differential equation y′′ + y = −cos x is yp(x) =
A0eix.
(d) An appropriate complex-valued trial solution for the
differential equation y′′ + y = 3 sin 2x is yp(x) =
A0e2ix.
(e) An appropriate complex-valued trial solution for the
differential equation y′′ −4y′ + 29y = xe2x sin 5x is
yp(x) = A0xe(2+5i)x.
(f) An appropriate complex-valued trial solution for the
differential equation y′′ + 9y = cos 3x + sin 4x is
yp(x) = Axe3ix + Be4ix.

8.5
Oscillations of a Mechanical System 529
Problems
For all problems below, use a complex-valued trial solution
to determine a particular solution to the given differential
equation.
1. y′′ −16y = 20 cos 4x.
2. y′′ + 2y′ + y = 50 sin 3x.
3. y′′ −y = 10e2x cos x.
4. y′′ + 4y′ + 4y = 169 sin 3x.
5. y′′ −y′ −2y = 40 sin2 x.
6. y′′ + y = 3ex cos 2x.
7. y′′ + 2y′ + 2y = 2e−x sin x.
8. y′′ −4y = 100xex sin x.
9. y′′ + 2y′ + 5y = 4e−x cos 2x.
10. y′′ −2y′ + 10y = 24ex cos 3x.
11. y′′ + 16y = 34ex + 16 cos 4x −8 sin 4x.
12. d2y
dt2 +ω2
0y = F0 cos ωt, where ω0, ω are positive con-
stants, and F0 is an arbitrary constant. You will need
to consider the cases ω ̸= ω0 and ω = ω0 separately.
8.5
Oscillations of a Mechanical System
In this section we analyze in some detail the motion of a mechanical system consisting
of a mass attached to a spring. We will see that even such a simple physical system has
interesting and varied behavior. We begin by constructing an appropriate mathematical
model of the physical situation under consideration.
Mathematical Formulation
Statement of the Problem: A mass of m kilograms is attached to the end of a spring
whose natural length is l0 meters. At t = 0, the mass is displaced a distance y0 meters
from its equilibrium position and released with a velocity v0 meters/second. We wish to
determine the initial-value problem that governs the resulting motion.
Mathematical Formulation of the Problem: We assume that the motion takes place ver-
tically and adopt the convention that distances are measured positive in the downward
direction. In order to formulate the problem mathematically, we need to determine the
forces acting on the mass. Consider ﬁrst the static equilibrium position, in which the
mass hangs freely from the spring with no motion. (See Figure 8.5.1.) The forces acting
on the mass in this equilibrium position are
1. The force due to gravity
Fg = mg.
Natural
length, l0
L0
Positive
direction
Figure 8.5.1: Spring-mass system in static equilibrium.

530
CHAPTER 8
Linear Differential Equations of Order n
2. The spring force, Fs. According to Hooke’s law (see Section 1.1),
Fs = −kL0,
where k is the (positive) spring constant and L0 is the displacement of the spring
from its equilibrium position.
Since the system is in static equilibrium, these forces must exactly balance, so that
Fs + Fg = 0. Hence,
mg = kL0.
(8.5.1)
Now consider the situation when the mass has been set in motion. (See Figure 8.5.2.)
We let y(t) denote the position of the mass at time t and take y = 0 to coincide with
the equilibrium position of the system. The equation of motion of the mass can then be
obtained from Newton’s second law. The forces that now act on the mass are as follows:
Dashpot
Mass, m kilograms
y(t)
y 5  0, equilibrium
Positive y
Figure 8.5.2: A simple model of a damped spring-mass system.
1. The force due to gravity Fg. Once more this is
Fg = mg.
(8.5.2)
2. The spring force Fs. At time t the total displacement of the spring from its natural
length is L0 + y(t), so that, according to Hooke’s law,
Fs = −k[L0 + y(t)].
(8.5.3)
3. A damping force Fd. In general, the motion will be damped due, for example,
to air resistance or, as shown in Figure 8.5.2, an external damping system, such
as a dashpot. We assume that any damping forces that are present are directly
proportional to the velocity of the mass. Under this assumption, we have
Fd = −cdy
dt ,
(8.5.4)
where c is a positive constant called the damping constant. Note that the negative
sign is inserted in Equation (8.5.4), since Fd always acts in the opposite direction
to that of the motion.
4. Any external driving forces, F(t), that are present. For example, the top of the
spring or the mass itself may be subjected to an external force.

8.5
Oscillations of a Mechanical System 531
The total force acting on the system will be the sum of the preceding forces. Thus,
using Newton’s second law of motion, the differential equation governing the motion of
the mass is
m d2y
dt2 = Fg + Fs + Fd + F(t).
Substituting from Equations (8.5.2)–(8.5.4) yields
m d2y
dt2 = mg −k(L0 + y) −cdy
dt + F(t).
That is, using Equation (8.5.1), and rearranging terms,
d2y
dt2 + c
m
dy
dt + k
m y = 1
m F(t).
In addition, we also have the initial conditions
y(0) = y0
and
dy
dt (0) = v0,
where y0 denotes the initial displacement of the mass from its equilibrium position, and
v0 denotes the initial velocity of the mass. The motion of the spring-mass system is
therefore governed by the initial-value problem
d2y
dt2 + c
m
dy
dt + k
m y = 1
m F(t),
y(0) = y0,
dy
dt (0) = v0.
(8.5.5)
Free Oscillations of a Mechanical System
We ﬁrst consider the case when there are no external forces acting on the system. In
the preceding formulation this corresponds to setting F(t) = 0, so that the initial-value
problem (8.5.5) reduces to
d2y
dt2 + c
m
dy
dt + k
m y = 0,
y(0) = y0,
dy
dt (0) = v0.
For most of the discussion we will concentrate on the differential equation alone, since
the initial conditions do not signiﬁcantly affect the behavior of its solutions. We must
therefore solve the constant coefﬁcient homogeneous differential equation
d2y
dt2 + c
m
dy
dt + k
m y = 0.
(8.5.6)
We divide the discussion of the solution to Equation (8.5.6) into several subcases.
Case 1: No Damping. This is the simplest case that can arise and is of importance for
understanding the more general situation. Setting c = 0 in Equation (8.5.6) yields
d2y
dt2 + ω2
0y = 0,
(8.5.7)
where
ω0 =
*
k/m.
Equation (8.5.7) has general solution
y(t) = c1 cos ω0t + c2 sin ω0t.
(8.5.8)

532
CHAPTER 8
Linear Differential Equations of Order n
It is instructive to introduce two new constants A0 and φ deﬁned in terms of c1 and c2
by (see Figure 8.5.3)
A0 cos φ = c1,
A0 sin φ = c2.
(8.5.9)
That is,
A0 =
+
c2
1 + c2
2,
φ = arctan
'c2
c1
(
.
c1
2 1 c2
2
c1
c2
f
Figure 8.5.3: The deﬁnition of
the phase angle φ.
Substituting from Equation (8.5.9) into Equation (8.5.8) yields
y(t) = A0(cos ω0t cos φ + sin ω0t sin φ).
Consequently,
y(t) = A0 cos(ω0t −φ).
(8.5.10)
Clearly, the motion described by Equation (8.5.10) is periodic. We refer to such
motion as simple harmonic motion (SHM). Figure 8.5.4 depicts this motion for typical
values of the constants A0, ω0, and φ. The standard names for the constants arising in
the solution are as follows:
A0: the amplitude of the motion.
ω0: the circular frequency of the system.
φ: the phase of the motion.
2A0
A0
y(t)
t
Figure 8.5.4: Simple harmonic motion. The mass continues to oscillate with a constant
amplitude A0.
The fundamental period of oscillation (that is, the time for the system to undergo
one complete cycle), T , is
T = 2π
ω0
= 2π
,m
k .
Consequently, the frequency of oscillation (number of oscillations per unit of time), f ,
is given by
f = 1
T = ω0
2π = 1
2π
,
k
m .
Notice that this is independent of the initial conditions. It is truly a property of the system.

8.5
Oscillations of a Mechanical System 533
Case 2: Damping. We now discuss the motion of the spring-mass system when the
damping constant, c, is nonzero. In this case, the auxiliary polynomial for Equation
(8.5.6) is
P(r) = r2 + c
m r + k
m
with roots
r = −c ±
√
c2 −4km
2m
.
As we might expect, the behavior of the system is dependent on whether the auxiliary
polynomial has distinct real roots, a repeated real root, or complex conjugate roots.
These three situations will arise, depending on the magnitude of the (dimensionless)
combination of the system variables c2/(4km). For a given spring and mass, only the
damping can be altered, which leads to the following terminology. We say that the
system is
(a) Underdamped if c2/(4km) < 1
(complex conjugate roots),
(b) Critically damped if c2/(4km) = 1
(repeated real root),
(c) Overdamped if c2/(4km) > 1
(two distinct real roots).
The corresponding solutions to Equation (8.5.6) are
y(t) = e−ct/(2m)(c1 cos µt + c2 sin µt),
µ =
√
4km −c2
2m
,
(8.5.11)
y(t) = e−ct/(2m)(c1 + c2t),
(8.5.12)
y(t) = e−ct/(2m)(c1eµt + c2e−µt),
µ =
√
c2 −4km
2m
.
(8.5.13)
As we shall discuss below, in all three cases (8.5.11)–(8.5.13) we have lim
t→∞y(t) = 0,
which implies that the motion dies out for large t. This is certainly consistent with our
everyday experience. We will discuss the different cases separately.
Case 2a: Underdamping. In this case, the position of the mass at time t is given in
(8.5.11), which reduces to SHM when c = 0. Once more it is convenient to introduce
constants A0 and φ deﬁned by
A0 cos φ = c1
and
A0 sin φ = c2.
Now (8.5.11) can be written in the equivalent form
y(t) = A0e−ct/(2m) cos(µt −φ).
(8.5.14)
We see that the mass oscillates between ±A0e−ct/(2m). The corresponding motion is
depicted in Figure 8.5.5 for the case when y(0) > 0 and dy
dt (0) > 0.
In general the motion is oscillatory, but it is not periodic. The amplitude of the
motion dies out exponentially with time, although the time interval, T , between succes-
sive maxima (or minima) of y(t) has the constant value (see Problem 15)
T = 2π
µ =
4πm
√
4km −c2 .
This is called the quasiperiod.

534
CHAPTER 8
Linear Differential Equations of Order n
y(t)
t
Figure 8.5.5: Underdamping: The mass oscillates between ±A0e−ct/(2m).
Case 2b: Critical Damping. This case arises when c2/(4km) = 1. From Equation
(8.5.6), the motion is governed by the differential equation
d2y
dt2 + c
m
dy
dt + c2
4m2 y = 0
with general solution
y(t) = e−ct/(2m)(c1 + c2t).
(8.5.15)
Now the damping is so severe that the system can pass through the equilibrium po-
sition at most once, and so we do not have oscillatory behavior. If we impose the
initial conditions
y(0) = y0
and
dy
dt (0) = v0,
then it is easily shown (see Problem 16) that (8.5.15) can be written in the form
y(t) = e−ct/(2m) -
y0 + t
"
v0 + c
2m y0
#.
.
Consequently, the system will pass through the equilibrium position, provided y0 and
v0 + c
2m y0 have opposite signs. A sketch of the motion described by (8.5.15) is given
in Figure 8.5.6.
System does not pass 
through equilibrium
System does pass
through equilibrium
t
y(t)
Figure 8.5.6: Critical damping: The system can pass through equilibrium at most once.
Case 2c: Overdamping. In this case we have c2/(4km) > 1. The roots of the auxiliary
equation corresponding to Equation (8.5.6) are
r1 = −c +
√
c2 −4km
2m
and r2 = −c −
√
c2 −4km
2m
,

8.5
Oscillations of a Mechanical System 535
so that the general solution to Equation (8.5.6) is
y(t) = e−ct/(2m)(c1eµt + c2e−µt),
µ =
√
c2 −4km
2m
.
Since c, k, and m are positive, it follows that both of the roots of the auxiliary equation
are negative, which implies that both terms in y(t) decay in time. Once more, we do
not have oscillatory behavior. The motion is very similar to that of the critically damped
case. The system can pass through the equilibrium position at most once. (The graphs
given in Figure 8.5.6 are representative of this case also.)
Forced Oscillations
We now consider the case when an external force acts on the spring-mass system. As
shown at the beginning of the section, the appropriate differential equation describing
the motion of the system is
d2y
dt2 + c
m
dy
dt + k
m y = F(t)
m .
The situation of most interest arises when the applied force is periodic in time, and we
therefore restrict attention to a driving term of the form
F(t) = F0 cos ωt,
where F0 and ω are constants. Then the differential equation governing the motion is
d2y
dt2 + c
m
dy
dt + k
m y = F0
m cos ωt.
(8.5.16)
Once more we will divide our discussion into several cases.
Case 1: No Damping. Setting c = 0 in Equation (8.5.16) yields
d2y
dt2 + ω2
0y = F0
m cos ωt,
(8.5.17)
where
ω0 =
*
k/m
denotes the circular frequency of the system. The complementary function for Equation
(8.5.17) is
yc(t) = c1 cos ω0t + c2 sin ω0t,
which can be written in the form
yc(t) = A0 cos(ω0t −φ),
for appropriate constants A0 and φ. We therefore need to ﬁnd a particular solution to
Equation (8.5.17). The right-hand side of Equation (8.5.17) is of an appropriate form to
use the method of undetermined coefﬁcients, although the trial solution will depend on
whether ω ̸= ω0 or ω = ω0.
Case 1a: ω ̸= ω0. In this case, the appropriate trial solution is
yp(t) = A cos ωt + B sin ωt.

536
CHAPTER 8
Linear Differential Equations of Order n
A straightforward calculation yields the particular solution for Equation (8.5.17) (see
Problem 27)
yp(t) =
F0
m(ω2
0 −ω2) cos ωt,
(8.5.18)
so that the general solution to Equation (8.5.17) is
y(t) = A0 cos(ω0t −φ) +
F0
m(ω2
0 −ω2) cos ωt.
(8.5.19)
Comparing this with (8.5.10), we see that the resulting motion consists of a superpo-
sition of two simple harmonic oscillation modes. One of these modes has the circular
frequency, ω0, of the system, whereas the other mode has the frequency of the driving
force. Consequently, the motion is oscillatory and bounded for all time, but, in general,
it is not periodic. Indeed, it can be shown that the motion is periodic whenever the ratio
ω/ω0 is a rational number, say,
ω
ω0
= p
q ,
(8.5.20)
where p and q are positive integers (see Problem 28). In such a case, the fundamental
period of the motion is
T = 2πq
ω0
= 2πp
ω ,
where p and q are the smallest integers satisfying Equation (8.5.20). A typical (nonpe-
riodic) motion of the form (8.5.19) is sketched in Figure 8.5.7.
y(t)
t
Figure 8.5.7: Forced harmonic oscillation.
An interesting occurrence arises when the driving frequency ω is close to (but not
equal to) the natural frequency of the system. To investigate this situation, we ﬁrst impose
the initial conditions y(0) = 0 and dy
dt (0) = 0 on the general solution (8.5.19). These
conditions imply that A0 and φ must satisfy
A0 cos φ +
F0
m(ω2
0 −ω2) = 0,
ω0 A0 sin φ = 0.
Hence,
A0 = −
F0
m(ω2
0 −ω2),
φ = 0.
Substituting these values into the general solution (8.5.19) gives
y(t) =
F0
m(ω2
0 −ω2)(cos ωt −cos ω0t).

8.5
Oscillations of a Mechanical System 537
We next use the trigonometric identity 2 sin A sin B = cos(A −B) −cos(A + B) with
A = (ω0 −ω)t/2 and B = (ω0 + ω)t/2 to obtain
y(t) =
2F0
m(ω2
0 −ω2) sin
/'ω0 −ω
2
(
t
0
sin
/'ω0 + ω
2
(
t
0
.
If ω and ω0 are nearly equal, then sin[(ω0 −ω)t/2] is slowly varying compared to
sin[(ω0 + ω)t/2]. Thus, y(t) behaves like a rapidly oscillating SHM mode whose am-
plitude is slowly varying in time. (See Figure 8.5.8.) One of the simplest occurrences
of this phenomenon is when two tuning forks whose frequencies are nearly equal are
struck simultaneously.
t
y(t)
Figure 8.5.8: When ω0 ≈ω, the
resulting motion can be interpreted
as being simple harmonic with a
slowly varying amplitude.
Case 1b: ω = ω0 (Resonance). When the frequency of the driving term coincides with
the frequency of the system, we must solve
d2y
dt2 + ω2
0y = F0
m cos ω0t.
(8.5.21)
The complementary function can be written as
yc(t) = A0 cos(ω0t −φ),
and an appropriate trial solution is
yp(t) = t(A cos ω0t + B sin ω0t).
A straightforward application of the method of undetermined coefﬁcients yields the
particular solution (see Problem 27)
yp(t) =
F0
2mω0
t sin ω0t,
(8.5.22)
so that the general solution to Equation (8.5.21) is
y(t) = A0 cos(ω0t −φ) +
F0
2mω0
t sin ω0t.
We see that the motion is oscillatory, but we also see that the amplitude increases without
bound as t →∞. This phenomenon, which occurs when the driving and natural frequen-
cies coincide, is called resonance. Its physical consequences cannot be overemphasized.
For example, the occurrence of resonance in the present situation would eventually lead
to the spring’s elastic limit being exceeded, and hence, the system would be destroyed.
This situation is depicted in Figure 8.5.9.
t
y(t)
Figure 8.5.9: Resonance: The
amplitude of the oscillation
increases without bound as t →∞.
Case 2: Damping. We now consider the general damped equation
d2y
dt2 + c
m
dy
dt + k
m y = F0
m cos ωt,
(8.5.23)
where c ̸= 0. An appropriate trial solution for this equation is
yp(t) = A cos ωt + B sin ωt.
A fairly lengthy, but straightforward, computation yields the particular solution (see
Problem 30)
yp(t) =
F0
(k −mω2)2 + c2ω2 [(k −mω2) cos ωt + cω sin ωt],
(8.5.24)

538
CHAPTER 8
Linear Differential Equations of Order n
which can be written in the form
yp(t) = F0
H cos(ωt −η),
(8.5.25)
where
cos η = m(ω2
0 −ω2)
H
,
sin η = cω
H ,
H =
+
m2(ω2
0 −ω2)2 + c2ω2,
and
ω0 =
*
k/m.
Consider ﬁrst the case of underdamping. Using the homogeneous solution from (8.5.14)
and the foregoing particular solution, it follows that the general solution to Equation
(8.5.23) in this case is
y(t) = A0e−ct/(2m) cos(µt −φ) + F0
H cos(ωt −η).
(8.5.26)
For large t, we see that yp is dominant. For this reason, we refer to the complementary
functionasthetransientpartofthesolutionand yp iscalledthesteady-statesolution.We
recognize Equation (8.5.26) as consisting of a superposition of two harmonic oscillations,
one damped and the other undamped. The motion is eventually simple harmonic with a
frequency coinciding with that of the driving term.
The cases for critical damping and overdamping are similar, since in both cases
the complementary function (transient part of the solution) dies out exponentially and
the steady-state solution (8.5.25) dominates. A typical motion of a forced mechanical
system with damping is shown in Figure 8.5.10.
t
y(t)
Figure 8.5.10: An example of
forced motion with damping.
Exercises for 8.5
Key Terms
Spring-mass system, Static equilibrium, Spring force,
Damping force, Damping constant, External driving force,
Free oscillations, Simple harmonic motion, Amplitude, Cir-
cular frequency, Phase, Period, Underdamping, Quasiperiod,
Critical
damping,
Overdamping,
Forced
oscillations,
Resonance.
Skills
• Understand the statement and mathematical formula-
tion of the problem of determining the motion of a
spring in a mechanical system.
• Be familiar with the different forces that are present
in the spring-mass system described in this section.
• Be able to write down and solve the initial-value prob-
lem governing the motion of the spring-mass system
in the case when no external forces are present.
• Be able to determine the simple harmonic motion as-
sociated with the free oscillations of a system with no
damping, including its amplitude, frequency, phase,
and period.
• Be able to determine whether a damped spring-mass
systemwithnoexternalforcesisunderdamped,critically
damped, or overdamped, and be able to determine the
motion of the spring-mass system in each case.
• Be able to determine the motion of a spring-mass sys-
tem subject to external forces, whether or not there is
damping.
• Be able to determine the steady-state and transient so-
lutions for the motion of a spring-mass system subject
to a periodic external force and damping.
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The spring force on a mass acts in a direction opposite
to the displacement of the mass from equilibrium.
(b) The circular frequency of a spring-mass system is the
spring constant k divided by the mass m.

8.5
Oscillations of a Mechanical System 539
(c) For simple harmonic motion, the product of the fre-
quency of oscillation and the period of oscillation is 1.
(d) An underdamped spring-mass system tends to rest as
t →∞.
(e) Underdamped, critically damped, and overdamped
spring-mass systems can all exhibit periodic motion.
(f) Resonance occurs when the circular frequency of a
spring-mass system agrees with the frequency of the
driving force.
(g) The air resistance experienced by a spring-mass sys-
tem is an example of a damping force.
(h) The larger the mass, the shorter the period of a spring-
mass system that is undergoing simple harmonic
motion.
(i) In order for the amplitude of a spring-mass system to
increase without bound, an external driving force must
be present.
Problems
For Problems 1–2, consider the spring-mass system whose
motion is governed by the given initial-value problem. Deter-
mine the circular frequency of the system and the amplitude,
phase, and period of the motion.
1. d2y
dt2 + 4y = 0, y(0) = 2, dy
dt (0) = 4.
2. d2y
dt2 + ω2
0y = 0, y(0) = y0, dy
dt (0) = v0,
where ω0, y0, v0 are constants.
3. A force of 3 N stretches a spring by 1 m.
(a) Find the spring constant k.
(b) A mass of 4 kg is attached to the spring. At t = 0,
the mass is pulled down a distance 1 meter from
equilibrium and released with a downward veloc-
ity of 0.5 meters/second. Assuming that damping
is negligible, determine an expression for the po-
sition of the mass at time t. Find the circular fre-
quency of the system and the amplitude, phase,
and period of the motion.
For Problems 4–10, determine the motion of the spring-mass
system governed by the given initial-value problem. In each
case, state whether the motion is underdamped, critically
damped, or overdamped, and make a sketch depicting the
motion.
4. d2y
dt2 + 2dy
dt + y = 0,
y(0) = −1,
dy
dt (0) = 2.
5. 4d2y
dt2 + 4dy
dt + y = 0,
y(0) = 4,
dy
dt (0) = −1.
6. d2y
dt2 + 4dy
dt + 7y = 0,
y(0) = 2,
dy
dt (0) = 6.
7. d2y
dt2 + 2dy
dt + 5y = 0,
y(0) = 1,
dy
dt (0) = 3.
8. d2y
dt2 + 3dy
dt + 2y = 0,
y(0) = 1,
dy
dt (0) = 0.
9. 4d2y
dt2 +12dy
dt +5y = 0,
y(0) = 1,
dy
dt (0) = −3.
10. d2y
dt2 + 5dy
dt + 6y = 0,
y(0) = −1,
dy
dt (0) = 4.
11. In the previous problem, ﬁnd the time at which the
mass passes through the equilibrium position, and de-
termine the maximum positive displacement of the
mass from equilibrium. Make a sketch depicting the
motion.
12. Consider the spring-mass system whose motion is
governed by the differential equation
d2y
dt2 + 2α dy
dt + y = 0.
Determine all values of the (positive) constant α for
which the system is (i) underdamped, (ii) critically
damped, and (iii) overdamped. In the case of over-
damping, solve the system fully. If the initial velocity
of the system is zero, determine if the mass passes
through equilibrium.
13. Consider the spring-mass system whose motion is
governed by the initial-value problem
d2y
dt2 + 1
5
dy
dt + 1
100 y = 0,
y(0) = 1,
dy
dt (0) = 5.
(a) Determine the position of the mass at time t.
(b) Determine the maximum displacement of the
mass.
(c) Make a sketch depicting the general motion of the
system.

540
CHAPTER 8
Linear Differential Equations of Order n
14. Consider the spring-mass system whose motion is
governed by the initial-value problem
d2y
dt2 + 3dy
dt + 2y = 0,
y(0) = 1,
dy
dt (0) = −3.
(a) Determine the position of the mass at time t.
(b) Determine the time when the mass passes through
the equilibrium position.
(c) Make a sketch depicting the general motion of the
system.
15. Consider the general solution for an underdamped
spring-mass system.
(a) Show that the time between successive maxima
(or minima) of y(t) is
T = 2π
µ =
4πm
√
4km −c2 .
(b) Show that if
c2
4km << 1, then
T = 2π
,m
k .
Is this result reasonable? Explain.
16. Show that the general solution for the motion of a
critically damped spring-mass system, with initial dis-
placement y0 and initial velocity v0, can be written in
the form
y(t) = e−ct/(2m) -
y0 + t
"
v0 + c
2m y0
#.
and that the system can pass through the equilibrium
position at most once.
17. A cylinder of side L meters lies one quarter submerged
and upright in a certain ﬂuid. At t = 0, the cylinder
is pushed down a distance of L/2 meters and released
from rest. Show that the resulting motion is simple
harmonic, and determine the circular frequency and
period of the motion.1
Asimplependulum consists of amass, m kilograms, attached
to the end of a light rod of length L meters, whose other end
is ﬁxed. (See Figure 8.5.11.)
If we let θ radians denote the angle the rod is displaced
from the vertical at time t, then the component of the ve-
locity in the direction of motion is v = L · dθ
dt , so that
mg
mg sin u
u
L
Figure 8.5.11: The simple pendulum
the component of the acceleration in the direction of motion
is L · d2θ
dt2 . Further, the tangential component of the force
is FT = −mg sin θ, so that, from Newton’s second law, the
equation of motion of the pendulum is
mL d2θ
dt2 = −mg sin θ.
That is,
d2θ
dt2 + g
L sin θ = 0.
(8.5.27)
This is a nonlinear differential equation. However, if we re-
call the Maclaurin expansion for sin θ, namely,
sin θ = θ −1
3!θ3 + 1
5!θ5 −· · · ,
it follows that for small oscillations, we can approximate
sin θ by θ. Then Equation (8.5.27) can be replaced to rea-
sonable accuracy by the simple linear differential equation
d2θ
dt2 + g
L θ = 0.
(8.5.28)
Problems 18–21 deal with the simple pendulum whose mo-
tion is described by Equation (8.5.28).
18. A pendulum of length 0.5 meters is displaced an angle
0.1 radians from the equilibrium position and released
from rest. Determine the resulting motion.
19. A pendulum of length L meters is displaced an an-
gle α radians from the vertical and released with an
angular velocity of β radians/second. Determine the
amplitude, phase, and period of the resulting motion.
1According to Archimedes’ principle, when an object is partially or wholly immersed in a ﬂuid, it experiences
an upward force equal to the weight of ﬂuid displaced.

8.5
Oscillations of a Mechanical System 541
20. Show that the period of the simple pendulum is T =
2π√L/g. Determine the length of a pendulum that
takes one second to swing from its extreme position
on the right to its extreme position on the left. Let
g = 9.8 meters/second2.
21. A clock has a pendulum of length 90 centimeters. If
the clock ticks each time the pendulum swings from
its extreme position on the right to its extreme position
on the left, determine the number of times the clock
ticks in one minute. Let g = 9.8 meters/second2.
22. An object of mass m is attached to the midpoint of
a light elastic string of natural length 6a. When the
ends of the string are ﬁxed at the same level a distance
6a apart and the mass is allowed to hang in equilib-
rium, the length of the stretched string is 10a. (See
Figure 8.5.12.) The mass is pulled down a small ver-
tical distance from equilibrium and released.
5a
6a
mg
Figure 8.5.12: The static equilibrium position.
Show that, for small oscillations, the period of the re-
sulting motion is
T = 20π
7
·
,a
g .
23. Repeat the previous problem if the string has natural
length 2L0 and in equilibrium, the stretched string has
length 2L.
24. Consider the damped spring-mass system whose mo-
tion is governed by
d2y
dt2 + 2dy
dt + 5y = 17 sin 2t, y(0) = −2,
dy
dt (0) = 0.
(a) Determine whether the motion is underdamped,
overdamped, or critically damped.
(b) Find the solution to the given initial-value prob-
lem and identify the steady-state and transient
parts.
25. Consider the spring-mass system whose motion is
governed by
d2y
dt2 + ω2
0y = F0 cos ωt, y(0) = 0, dy
dt (0) = 0.
Determine the solution if the system is resonating.
26. Consider the spring-mass system whose motion is
governed by
d2y
dt2 + 3dy
dt + 2y = 10 sin t.
Determine the steady-state solution, yp, and express
your answer in the form
yp(t) = A0 sin(t −φ),
for appropriate constants A0 and φ.
27. Consider the forced undamped spring-mass system
whose motion is governed by
d2y
dt2 + ω2
0y = F0
m cos ωt.
Derive the particular solutions given in Equations
(8.5.18) and (8.5.22). (You will need to consider ω ̸=
ω0 and ω = ω0 separately.)
28. The general solution to the forced undamped (non-
resonating) spring-mass system is
y(t) = A0 cos(ω0t −φ) +
F0 cos ωt
m(ω2
0 −ω2).
If ω/ω0 = p/q, where p and q are integers, show that
the motion is periodic with period T = 2πq
ω0
.
29. Determinetheperiodofthemotionforthespring-mass
system governed by the differential equation
d2y
dt2 + 9
16 y = 55 cos 2t.
30. Consider the damped forced motion described by
d2y
dt2 + c
m
dy
dt + k
m y = F0
m cos ωt.
Derive the steady-state solution (8.5.24) given in the
text.

542
CHAPTER 8
Linear Differential Equations of Order n
31. Consider the damped forced motion described by
d2y
dt2 + c
m
dy
dt + k
m y = F0
m cos ωt.
We have shown that the steady-state solution can be
written in the form
yp(t) = F0
H cos(ωt −ν),
where
cos ν = m(ω2
0 −ω2)
H
,
sin ν = cω
H ,
ω0 =
,
k
m ,
H =
+
m2(ω2
0 −ω2)2 + c2ω2.
Assuming that c2/(2m2ω2
0) < 1, show that the ampli-
tude of the steady-state solution is a maximum when
ω =
1
ω2
0 −c2
2m2 .
[Hint: The maximum occurs at the value of ω that
makes H a minimum. Assume that H is a function of
ω, and determine the value of ω that minimizes H.]
32. Consider the damped spring-mass system with m =
1, k = 5, c = 2, and F(t) = 8 cos ωt.
(a) Determine the transient part of the solution and
the steady-state solution.
(b) Determine the value of ω that maximizes the am-
plitude of the steady-state solution and express
the corresponding solution in the form
yp(t) = A0 cos(ωt −ν),
for appropriate constants A0, ω, and ν.
33. Considerthespring-masssystemwhosemotionisgov-
erned by the differential equation
d2y
dt2 + 2dy
dt + 5y = 4e−t cos 2t.
(a) Describe the variation with time of the applied
external force.
(b) Determine the motion of the mass. What happens
as t →∞?
34. Considerthespring-masssystemwhosemotionisgov-
erned by the differential equation
d2y
dt2 + 16y = 130e−t cos t.
Determine the resulting motion, and identify any tran-
sient and steady-state parts of your solution.
8.6
RLC Circuits
In Section 1.7, we used Kirchhoff’s second law to derive the differential equation
di
dt + R
L i +
1
LC q = 1
L E(t),
(8.6.1)
which governs the behavior of the RLC circuit shown in Figure 8.6.1. Here, q is the
charge on the capacitor at time t, the constants R, L, and C are the resistance, induc-
tance, and capacitance of the circuit elements respectively, and E(t) denotes the driving
electromotive force (EMF). The current in the circuit is related to the charge on the
capacitor via
i(t) = dq
dt .
R
Switch
C
i(t)
L
E(t)
Figure 8.6.1: An RLC circuit.

8.6
RLC Circuits 543
Substituting this expression for i(t) into Equation (8.6.1) yields the second-order
constant-coefﬁcient differential equation
d2q
dt2 + R
L
dq
dt +
1
LC q = 1
L E(t).
(8.6.2)
A comparison of Equation (8.6.2) with the basic differential equation governing the
motion of a spring-mass system, namely
d2y
dt2 + c
m
dy
dt + k
m y = 1
m F(t),
reveals that, although the two problems are distinct physically, from a purely mathe-
matical standpoint they are identical. The correspondence between the variables and
parameters in an RLC circuit and a spring-mass system is given in Table 8.6.1. It follows
that the results derived in the previous section for a spring-mass system can be translated
into corresponding results for RLC circuits. Rather than repeating these results, we will
make some general observations and then consider one illustrative example. The full
investigation of the behavior of an RLC circuit is left for the exercises.
RLC Circuit
Spring-Mass System
q(t)
y(t)
L
m
R
c
1/C
k
E(t)
F(t)
Table 8.6.1: Comparison of an RLC circuit and a spring-mass system.
Consider ﬁrst the homogeneous differential equation
d2q
dt2 + R
L
dq
dt +
1
LC q = 0.
(8.6.3)
This has auxiliary equation
r2 + R
L r + 1
L C = 0
with roots
r = −R ±
*
R2 −4L/C
2L
.
Three familiar cases arise. The circuit is said to be
1. Underdamped if R2 < 4L/C.
2. Critically damped if R2 = 4L/C.
3. Overdamped if R2 > 4L/C.
The corresponding solutions to Equation (8.6.3) are
q(t) = e−Rt/(2L)(c1 cos µt + c2 sin µt),
µ =
*
4L/C −R2
2L
,
q(t) = e−Rt/(2L)(c1 + c2t),
(8.6.4)
q(t) = e−Rt/(2L)(c1eµt + c2e−µt),
µ =
*
R2 −4L/C
2L
.

544
CHAPTER 8
Linear Differential Equations of Order n
In all cases of physical relevance, R/L > 0, so that
lim
t→∞q(t) = 0.
Equivalently, we can state that the complementary function qc(t) for Equation (8.6.2)
satisﬁes
lim
t→∞qc(t) = 0.
We refer to qc as the transient part of the solution to Equation (8.6.2), since it decays
exponentially with time. As a speciﬁc example, we consider the case of a periodic driving
EMF in an underdamped circuit.
Example 8.6.1
Determine the current in the RLC circuit
d2q
dt2 + R
L
dq
dt +
1
LC q = E0
L cos ωt,
(8.6.5)
where E0 and ω are positive constants and R2 < 4L/C.
Solution:
The complementary function given in Equation (8.6.4) can be written in
phase-amplitude form as
qc(t) = A0e−Rt/(2L) cos(µt −φ),
where A0 and φ are deﬁned in the usual manner. A particular solution to Equation (8.6.5)
can be obtained by using Table 8.6.1 to make the appropriate replacements in the solution
(8.5.18) for the corresponding spring-mass system. The result is
qp(t) = E0
H cos(ωt −η),
where
H =
+
L2(ω2
0 −ω2)2 + R2w2
and
cos η = L(ω2
0 −ω2)
H
,
sin η = Rω
H ,
ω0 =
1
√
LC
.
Consequently, the charge on the capacitor at time t is
q(t) = A0e−Rt/(2L) cos(µt −φ) + E0
H cos(ωt −η),
and the corresponding current in the circuit can be determined from
i(t) = dq
dt .
Rather than compute this derivative, we consider the behavior of q(t) and the corre-
sponding behavior of the current as t →∞. Since qc tends to zero as t →∞, for large
t, the particular solution qp will be the dominant part of q(t). For this reason, we refer to

8.6
RLC Circuits 545
qp as the steady-state solution. The corresponding steady-state current in the circuit,
denoted iS, is given by
iS(t) = dqp
dt
= −ωE0
H
sin(ωt −η).
We see that this is periodic and that the frequency of the oscillation coincides with the
frequency of the driving EMF. The amplitude of the oscillation is
A = ωE0
H .
That is, upon substituting for H,
A =
ωE0
+
L2(ω2
0 −ω2)2 + R2w2
.
(8.6.6)
It is often required to determine the value of ω that maximizes this amplitude. In
order to do so, we rewrite (8.6.6) in the equivalent form
A =
E0
+
ω−2L2(ω2
0 −ω2)2 + R2
.
This will be a maximum when the term in parentheses vanishes, which occurs when
ω2 = ω2
0.
Substituting for ω2
0 = 1/(LC), it follows that the amplitude of the steady-state current
will be a maximum when ω = ωmax, where
ωmax =
1
√
LC
.
The corresponding value of A is
Amax = E0
R .
The behavior of A as a function of ω for typical values of E0, R, L, and C is shown in
Figure 8.6.2.
□
vmax
Amax
A
v
Figure 8.6.2: The behavior of the amplitude of the steady-state current as a function of the
driving frequency.

546
CHAPTER 8
Linear Differential Equations of Order n
Exercises for 8.6
Key Terms
RLC Circuit: Underdamped, Critically damped, Overdamped,
Transient solution, Steady-state solution.
Skills
• Be able to solve the differential equation arising
from Kirchhoff’s second law in order to determine
the charge on a capacitor or the current in an RLC
circuit.
• Be able to determine the transient and steady-state
parts of the solution q(t) of Equation (8.6.2) and the
solution i(t) of Equation (8.6.1).
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If there is no driving electromotive force in an RLC
circuit, then the charge on the capacitor and the current
in the circuit tend to zero as t →∞.
(b) If R = 4 +, L = 4 H, and C =
1
17 F, then the RLC
is underdamped.
(c) An external driving force E(t) = E0 cos ωt produces
a steady-state current of maximum amplitude when
ω =
1
√
LC
.
(d) The amplitude of the steady-state current in an RLC
circuit is proportional to the amplitude E0 of the ex-
ternal driving force.
(e) If the resistance R in an RLC circuit is doubled, then
the current i(t) in the circuit is reduced by one-half.
(f) The charge on a capacitor in an RL circuit with no
driving force varies periodically with time.
Problems
1. Determine the charge on the capacitor at time t in an
RLC circuit that has R = 4 +, L = 4 H, C =
1
17 F,
and E = E0 V, where E0 is constant. What happens
to the charge on the capacitor as t →+∞? Describe
the behavior of the current in the circuit.
2. Determine the steady-state current in the RLC cir-
cuit that has R =
3
2 +, L =
1
2 H, C =
2
3 F, and
E(t) = 13 cos 3t V.
3. Consider the RLC circuit with E(t) = E0 cos ωt V,
where E0 and ω are constants. If there is no resistor
in the circuit, show that the charge on the capacitor
satisﬁes
lim
t→∞q(t) = +∞
if and only if ω =
1
√
LC
. What happens to the current
in the circuit as t →+∞?
4. Consider the RLC circuit with R = 3 +, L = 1
2 H,
C = 1
5 F, and E(t) = 2 cos ωt V. Determine the cur-
rent in the circuit at time t, and ﬁnd the value of ω that
maximizes the amplitude of the steady-state current.
5. Consider the RLC circuit with R = 16 +, L = 8 H,
C =
1
40 F, and E(t) = 17 cos 2t V. Determine the
current in the circuit for t > 0, given that at t = 0, the
capacitor is uncharged and there is no current ﬂowing.
6. Show that the differential equation governing the be-
havior of an RLC circuit can be written directly in
terms of the current i(t) has
d2i
dt2 + R
L
di
dt +
1
LC i = 1
L
dE
dt .
7. Determine the current in the general RLC circuit with
R2 < 4L/C, if E(t) = E0e−at, where E0, a are
constants.
8. Consider the RLC circuit with R = 2 +, L = 1
2 H,
C =
2
5 F. Initially, the capacitor is uncharged, and
there is no current ﬂowing in the circuit. Determine
the current for t > 0, if the applied EMF is (see
Figure 8.6.3)
E(t) =
) 50t,
0 ≤t < π,
50π, t ≥π
p
50p
t
E(t)
Figure 8.6.3: EMF for Problem 8.

8.7
The Variation of Parameters Method 547
8.7
The Variation of Parameters Method
The method of undetermined coefﬁcients has two severe limitations. Firstly, it is only
applicable to differential equations with constant coefﬁcients, and secondly, it can only be
applied to differential equations whose nonhomogeneous terms are of the form described
in Section 8.3. For example, we could not use the method of undetermined coefﬁcients
to ﬁnd a particular solution to the differential equation
y′′ + 4y′ −6y = x2 ln x.
In this section, we introduce a very powerful technique, called the variation-of-
parameters method, for obtaining particular solutions to second-order linear nonho-
mogeneous differential equations, assuming that we know the general solution to the
associated homogeneous equation. Unlike the method of undetermined coefﬁcients, the
variation-of-parameters method is not restricted to differential equation with constant
coefﬁcients, and, at least in theory, the actual form of the nonhomogeneous term is im-
material. We will begin by considering the second-order case, since the generalization
to nth-order will then be fairly straightforward.
Consider the second-order linear nonhomogeneous differential equation
y′′ + a1y′ + a2y = F,
(8.7.1)
where we assume that a1, a2, and F are continuous on an interval I. Suppose that
y = y1(x) and y = y2(x) are two linearly independent solutions to the associated
homogeneous equation
y′′ + a1y′ + a2y = 0
(8.7.2)
on I, so that the general solution to Equation (8.7.2) on I is
yc(x) = c1y1(x) + c2y2(x).
The variation-of-parameters method consists of replacing the constants c1 and c2 by
functions u1(x) and u2(x) (that is, we allow the parameters c1 and c2 to vary) determined
in such a way that the resulting function
yp(x) = u1(x)y1(x) + u2(x)y2(x)
(8.7.3)
is a particular solution to Equation (8.7.1). Differentiating Equation (8.7.3) with respect
to x yields
y′
p = u′
1y1 + u1y′
1 + u′
2y2 + u2y′
2.
It is tempting to differentiate this expression once more and then substitute into Equation
(8.7.1) to determine u1 and u2. However, if we did this, the resulting expression for y′′
p
would involve second derivatives of u1 and u2, and hence, we would have complicated
our problem. Since yp contains two unknown functions, whereas Equation (8.7.1) gives
only one condition for determining them, we have the freedom to impose a further
constraint on u1 and u2. In order to eliminate second derivatives of u1 and u2 arising in
y′′
p, we try for solutions of the form (8.7.3) satisfying the constraint
u′
1y1 + u′
2y2 = 0.
(8.7.4)
The expression for y′
p then reduces to
y′
p = u1y′
1 + u2y′
2,

548
CHAPTER 8
Linear Differential Equations of Order n
so that
y′′
p = u′
1y′
1 + u1y′′
1 + u′
2y′
2 + u2y′′
2.
Substituting into Equation (8.7.1) and collecting terms yields
u1(y′′
1 + a1y′
1 + a2y1) + u2(y′′
2 + a1y′
2 + a2y2) + (u′
1y′
1 + u′
2y′
2) = F(x).
The terms multiplying u1 and u2 vanish, since y1 and y2 each solve y′′ +a1y′ +a2y = 0.
We therefore require that
u′
1y′
1 + u′
2y′
2 = F.
(8.7.5)
Consequently, yp(x) = u1(x)y1(x) + u2(x)y2(x) is a solution to Equation (8.7.1),
provided that u1 and u2 satisfy Equations (8.7.4) and (8.7.5). That is,
y1u′
1 + y2u′
2 = 0 and
y′
1u′
1 + y′
2u′
2 = F.
(8.7.6)
This is a linear system of equations for the unknowns u′
1 and u′
2. The matrix of coefﬁcients
of this system has determinant
y1
y2
y′
1
y′
2
,
which is the Wronskian, W[y1, y2](x), of y1 and y2. Since y1 and y2 are linearly inde-
pendent on I, W[y1, y2](x) is nonzero on I and hence the system (8.7.6) has a unique
solution for u′
1 and u′
2. Indeed, applying Cramer’s rule to (8.7.6) yields
u′
1(x) = −y2(x)F(x)
W[y1, y2](x),
u′
2(x) =
y1(x)F(x)
W[y1, y2](x),
which can be integrated directly to obtain
u1(x) = −
& x
x0
y2(t)F(t)
W[y1, y2](t) dt,
u2(x) =
& x
x0
y1(t)F(t)
W[y1, y2](t) dt,
(8.7.7)
where x0 ∈I. We have therefore established the next theorem.
Theorem 8.7.1
(Variation-of-Parameters Method)
Consider
y′′ + a1y′ + a2y = F,
(8.7.8)
where a1, a2, and F are assumed to be (at least) continuous on the interval I. Let y1 and
y2 be linearly independent solutions to the associated homogeneous equation
y′′ + a1y′ + a2y = 0
on I. Then a particular solution to Equation (8.7.8) is
yp = u1y1 + u2y2,
where u1 and u2 satisfy
y1u′
1 + y2u′
2 = 0 and
y′
1u′
1 + y′
2u′
2 = F.

8.7
The Variation of Parameters Method 549
Example 8.7.2
Solve y′′ + y = sec x.
Solution:
Two linearly independent solutions to the associated homogeneous equa-
tion are y1(x) = cos x and y2(x) = sin x. Thus, a particular solution to the given
differential equation is
yp(x) = u1y1 + u2y2 = u1 cos x + u2 sin x,
(8.7.9)
where u1 and u2 satisfy
cos x u′
1 + sin x u′
2 = 0,
−sin x u′
1 + cos x u′
2 = sec x.
Applying Cramer’s rule, the solution to this system is
u′
1 =
2222
0
sin x
sec x
cos x
2222
2222
cos x
sin x
−sin x
cos x
2222
= −sin x sec x,
u′
2 =
2222
cos x
0
−sin x
sec x
2222
2222
cos x
sin x
−sin x
cos x
2222
= cos x sec x = 1.
Consequently,
u1(x) = −
&
sin x sec x dx = −
&
sin x
cos x dx = ln | cos x|
and
u2(x) =
&
1 dx = x,
where we have set the integration constants to zero, since we only require one particular
solution. Substitution into Equation (8.7.9) yields
yp(x) = cos x · ln | cos x| + x sin x
so that the general solution to the given differential equation is
y(x) = c1 cos x + c2 sin x + cos x · ln | cos x| + x sin x.
□
Example 8.7.3
Solve y′′ + 4y′ + 4y = e−2x ln x,
x > 0.
Solution:
In this case, two linearly independent solutions to the associated homoge-
neous equation are y1(x) = e−2x and y2(x) = xe−2x, and hence, we seek a particular
solution to the given differential equation of the form
yp(x) = u1e−2x + u2xe−2x,
where u1 and u2 satisfy
e−2xu′
1 + xe−2xu′
2 = 0,
−2e−2xu′
1 + e−2x(1 −2x)u′
2 = e−2x ln x.
The solution to this system is
u′
1 = −x ln x,
u′
2 = ln x.

550
CHAPTER 8
Linear Differential Equations of Order n
Integrating both of these expressions by parts (and setting the integration constants to
zero), we obtain
u1(x) = 1
4x2(1 −2 ln x),
u2(x) = x(ln x −1).
Thus,
yp(x) = 1
4x2e−2x(1 −2 ln x) + x2e−2x(ln x −1).
That is,
yp(x) = 1
4x2e−2x(2 ln x −3).
Consequently, the general solution to the given differential equation is
y(x) = e−2x
/
c1 + c2x + 1
4x2(2 ln x −3)
0
.
□
Sometimes we can use a combination of the variation-of-parameters method and
the method of undetermined coefﬁcients to obtain a particular solution to a differential
equation. We illustrate this with an example.
Example 8.7.4
Determine the general solution to the differential equation
y′′ + 9y = 6 cot2 3x + 5e2x,
0 < x < π/6.
(8.7.10)
Solution:
The complementary function for the given differential equation is
yc(x) = c1 cos 3x + c2 sin 3x.
Application of the variation-of-parameters technique directly to the differential equation
(8.7.10) leads to some rather nasty integrals arising from the e2x term in the right-hand
side of the differential equation (8.7.10). However, if we determine a particular solution
to each of the differential equations
y′′ + 9y = 6 cot2 3x
(8.7.11)
and
y′′ + 9y = 5e2x,
(8.7.12)
then Theorem 8.1.10 can be applied to conclude that the sum of these two solutions
will itself be a particular solution to Equation (8.7.10). The key point is that Equation
(8.7.12) can be solved easily using the method of undetermined coefﬁcients, and there-
fore, we have alleviated the problem of evaluating the integrals mentioned previously.
Consider ﬁrst Equation (8.7.11). According to the variation-of-parameters method, there
is a particular solution to this differential equation of the form
yp1(x) = u1 cos 3x + u2 sin 3x,
where u1 and u2 satisfy
cos 3xu′
1 + sin 3xu′
2 = 0,
−sin 3xu′
1 + cos 3xu′
2 = 2 cot2 3x.

8.7
The Variation of Parameters Method 551
Solving this system of equations yields
u′
1 = −2 cot2 3x sin 3x,
u′
2 = 2 cot2 3x cos 3x.
Consequently, using the trigonometric identity cot2 θ = csc2 θ −1 and integration
formulas,
u1 = −2
&
cot2 3x sin 3x dx = −2
&
(csc 3x −sin 3x) dx
= −2
3[ln(csc 3x −cot 3x) + cos 3x]
and
u2 = 2
&
cot2 3x cos 3x dx = 2
& ' cos 3x
sin2 3x −cos 3x
(
dx
= −2
3(csc 3x + sin 3x).
Therefore,
yp1(x) = −2
3 cos 3x[ln(csc 3x −cot 3x) + cos 3x] −2
3 sin 3x(csc 3x + sin 3x),
which simpliﬁes to
yp1(x) = −2
3[cos 3x ln(csc 3x −cot 3x) + 2].
Next consider Equation (8.7.12). An appropriate trial solution for this differential equa-
tion is
yp2(x) = A0e2x,
and substitution into Equation (8.7.12) yields A0 = 5/13. Consequently, a particular
solution to Equation (8.7.12) is
yp2(x) = 5
13e2x.
It follows directly from Theorem 8.1.10 that a particular solution to Equation (8.7.10) is
yp(x) = yp1(x) + yp2(x) = −2
3[cos 3x ln(csc 3x −cot 3x) + 2] + 5
13e2x.
The general solution to Equation (8.7.10) is therefore
y(x) = c1 cos 3x + c2 sin 3x −2
3[cos 3x ln(csc 3x −cot 3x) + 2] + 5
13e2x.
□
Green’s Functions
According to Theorem 8.7.1, a particular solution to the differential equation
y′′ + a1y′ + a2y = F,
x ∈I,
(8.7.13)
where a1, a2, F are continuous on I, is
yp(x) = u1(x)y1(x) + u2(x)y2(x).
(8.7.14)

552
CHAPTER 8
Linear Differential Equations of Order n
Substituting the expressions for u1 and u2 obtained in (8.7.7) gives
yp(x) = −y1(x)
& x
x0
y2(t)F(t)
W[y1, y2](t) dt + y2(x)
& x
x0
y1(t)F(t)
W[y1, y2](t) dt.
The two terms on the right-hand side of the preceding equation can be combined to obtain
yp(x) =
& x
x0
) y1(t)y2(x) −y2(t)y1(x)
W[y1, y2](t)
3
F(t) dt,
which we write as
yp(x) =
& x
x0
K(x, t)F(t) dt,
(8.7.15)
where
K(x, t) = y1(t)y2(x) −y2(t)y1(x)
W[y1, y2](t)
.
(8.7.16)
The function K(x, t) is called a Green’s function for the problem. We see that it de-
pends only on the solutions to the associated homogeneous problem and not on the
nonhomogeneous term F(x).
Example 8.7.5
Use a Green’s function to determine a particular solution to the differential equation
y′′ + 16y = F(x).
Solution:
Two linearly independent solutions to the associated homogeneous differ-
ential equation are
y1(x) = cos 4x
and
y2(x) = sin 4x
with Wronskian
W[y1, y2](x) = (cos 4x)(4 cos 4x) −(sin 4x)(−4 sin 4x) = 4.
Substitution into (8.7.16) yields
K(x, t) = 1
4(cos 4t sin 4x −sin 4t cos 4x) = 1
4 sin [4(x −t)] .
Consequently, from (8.7.15),
yp(x) = 1
4
& x
x0
sin [4(x −t)] F(t) dt.
The general solution to the given differential equation can therefore be expressed as
y(x) = c1 cos 4x + c2 sin 4x + 1
4
& x
x0
sin [4(x −t)] F(t) dt.
□
Generalization to Higher Order
We now consider the generalization of the variation-of-parameters method to linear non-
homogeneous differential equations of arbitrary order n. In this case, the basic equation is
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = F(x),
(8.7.17)
where we assume that the functions a1, a2, . . . , an and F are at least continuous on the
interval I. Let {y1(x), y2(x), . . . , yn(x)} be a linearly independent set of solutions to the

8.7
The Variation of Parameters Method 553
associated homogeneous equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
(8.7.18)
on I, so that the general solution to Equation (8.7.18) on I is
yc(x) = c1y1(x) + c2y2(x) + · · · + cnyn(x).
We now look for a particular solution to Equation (8.7.17) of the form
yp(x) = u1(x)y1(x) + u2(x)y2(x) + · · · + un(x)yn(x).
(8.7.19)
The idea is to substitute this expression for yp into Equation (8.7.17) and choose the
functions u1, u2, . . . , un so that the resulting yp is indeed a solution. However, Equation
(8.7.17) will only give one constraint on the functions u1, u2, . . . , un and their deriva-
tives. Since we have n functions, we might expect that we can impose n −1 further
constraints on these functions. Following the steps taken in the second-order case, we
differentiate yp a total of n times, while imposing the constraint that the sum of the
terms involving derivatives of the u1, u2, . . . , un that arise at each stage (except the last)
should equal zero. For example, at the ﬁrst stage, we obtain
y′
p = u1y′
1 + u′
1y1 + u2y′
2 + u′
2y2 + · · · + uny′
n + u′
nyn,
and so, we impose the constraint
u′
1y1 + u′
2y2 + · · · + u′
nyn = 0,
in which case the foregoing expression for y′
p reduces to
y′
p = u1y′
1 + u2y′
2 + · · · + uny′
n.
Continuing in this manner leads to the following expressions for yp and its derivatives:
yp = u1y1 + u2y2
+ · · · + unyn,
y′
p = u1y′
1 + u2y′
2
+ · · · + uny′
n,
...
(8.7.20)
y(n)
p
= u1y(n)
1
+ u2y(n)
2
+ · · · + uny(n)
n
+
-
u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
.
,
together with the corresponding constraint conditions
u′
1y1
+ u′
2y2
+ · · · + u′
nyn
= 0,
u′
1y′
1
+ u′
2y′
2
+ · · · + u′
ny′
n
= 0,
...
(8.7.21)
u′
1y(n−2)
1
+ u′
2y(n−2)
2
+ · · · + u′
ny(n−2)
n
= 0.
Substitution from (8.7.20) into (8.7.17) yields the following condition in order for yp to
be a solution [simply multiply each equation in (8.7.20) by the appropriate ai and add
the elements in each column]:
u1
-
y(n)
1
+ a1y(n−1)
1
+ · · · + an−1y′
1 + any1
.
+ u2
-
y(n)
2
+ a1y(n−1)
2
+ · · · + an−1y′
2 + any2
.
+ . . .
+ un
-
y(n)
n
+ a1y(n−1)
n
+ · · · + an−1y′
n + anyn
.
+
-
u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
.
= F(x).

554
CHAPTER 8
Linear Differential Equations of Order n
The terms in each of the brackets, except the last, vanish, since y1, y2, . . . , yn are solu-
tions of Equation (8.7.18). We are therefore left with the condition
u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
= F(x).
Combining this with the constraints given in (8.7.21) leads to the following linear system
of equations for determining u′
1, u′
2, . . . , u′
n:
y1u′
1
+ y2u′
2
+ · · · + ynu′
n
= 0,
y′
1u′
1
+ y′
2u′
2
+ · · · + y′
nu′
n
= 0,
...
(8.7.22)
y(n−2)
1
u′
1 + y(n−2)
2
u′
2 + · · · + y(n−2)
n
u′
n = 0,
y(n−1)
1
u′
1 + y(n−1)
2
u′
2 + · · · + y(n−1)
n
u′
n = F(x).
The determinant of the matrix of coefﬁcients of this system is the Wronskian of the
functions y1, y2, . . . , yn, which is necessarily nonzero on I since y1, y2, . . . , yn are lin-
early independent on I. Consequently, the system (8.7.22) has a unique solution for
the derivatives u′
1, u′
2, . . . , u′
n, from which we can determine u1, u2, . . . , un by integra-
tion. Having found the functions u1, u2, . . . , un, we can obtain yp by substitution into
Equation (8.7.19).
Theorem 8.7.6
Variation-of-Parameters
Consider
y(n) + a1(x)y(n−1) + · · · + an−1y′ + an(x)y = F(x),
(8.7.23)
where a1, a2, . . . , an, F are assumed to be (at least) continuous on the interval I. Let
{y1, y2, . . . , yn} be a linearly independent set of solutions to the associated homogeneous
equation
y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y = 0
on I. Then a particular solution to Equation (8.7.23) is
yp = u1y1 + u2y2 + · · · + unyn,
where the functions u1, u2, . . . , un satisfy (8.7.22).
Example 8.7.7
Determine the general solution to
y′′′ −3y′′ + 3y′ −y = 36ex ln x.
(8.7.24)
Solution:
In this case, the auxiliary polynomial of the associated homogeneous equa-
tion is
P(r) = r3 −3r2 + 3r −1 = (r −1)3
so that three linearly independent solutions are
y1(x) = ex,
y2(x) = xex,
y3(x) = x2ex.
According to the variation-of-parameters method, there is a particular solution to Equa-
tion (8.7.24) of the form
yp(x) = exu1(x) + xexu2(x) + x2exu3(x),
(8.7.25)

8.7
The Variation of Parameters Method 555
where u1, u2, and u3 satisfy (8.7.22), which, in this case (after division by ex), assumes
the form
u′
1 +
xu′
2
+
x2u′
3
= 0,
u′
1 + (x + 1)u′
2 +
(x2 + 2x)u′
3 = 0,
u′
1 + (x + 2)u′
2 + (x2 + 4x + 2)u′
3= 36 ln x.
Since we have more than two variables in the system it is more efﬁcient to use Gaussian
elimination, rather than Cramer’s rule, to determine the solution. We therefore reduce
the augmented matrix of the system to row-echelon form:
⎡
⎣
1
x
x2
0
1
x + 1
x2 + 2x
0
1
x + 2
x2 + 4x + 2 36 ln x
⎤
⎦∼
⎡
⎣
1
x
x2
0
0
1
2x
0
0
2
4x + 2 36 ln x
⎤
⎦
∼
⎡
⎣
1
x
x2
0
0
1
2x
0
0
0
2
36 ln x
⎤
⎦∼
⎡
⎣
1
x
x2
0
0
1
2x
0
0
0
1
18 ln x
⎤
⎦.
Consequently,
u′
1 = 18x2 ln x,
u′
2 = −36x ln x,
u′
3 = 18 ln x.
By integrating, we obtain
u1(x) = 18
&
x2 ln xdx = 2x3(3 ln x −1),
u2(x) = −36
&
x ln xdx = 9x2(1 −2 ln x),
u3(x) = 18
&
ln xdx = 18x(ln x −1),
where we have set the integration constants to zero without loss of generality. Substituting
these expressions for u1, u2, and u3 into Equation (8.7.25) yields the particular solution
yp(x) = x3ex(6 ln x −11).
The general solution to the given differential equation is therefore
y(x) = ex8
c1 + c2x + c3x2 + x3(6 ln x −11)
9
.
□
Exercises for 8.7
Key Terms
Variation-of-parameters, Green’s function.
Skills
• Be able to use the variation-of-parameters method to
ﬁnd the general solution to an nth-order linear differ-
ential equation.
• Be able to use a Green’s function to determine a par-
ticular solution to a given differential equation.
True-False Review
For Questions (a)–(c), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The functions u1(x) and u2(x) arising in the particular
solution (8.7.3) to (8.7.1) satisfy
u′
1(x)y′
1(x) + u′
2(x)y′
2(x) = F(x),
where y1 and y2 are linearly independent solutions to
(8.7.2).

556
CHAPTER 8
Linear Differential Equations of Order n
(b) The variation-of-parameters method seeks a particular
solution to (8.7.17) of the form
yp(x) = u1(x)y1(x)+u2(x)y2(x)+· · ·+un(x)yn(x),
where y1, y2, . . . , yn are any n solutions to the asso-
ciated homogeneous differential equation (8.7.18).
(c) The functions u1(x), u2(x), . . . , un(x) arising in the
particular solution (8.7.19) to the differential equation
(8.7.17) are uniquely determined by the variation-of-
parameters method.
Problems
For Problems 1–22, use the variation-of-parameters method
to ﬁnd the general solution to the given differential equation.
1. y′′ −6y′ + 9y = 4e3x ln x,
x > 0.
2. y′′ + 4y′ + 4y = x−2e−2x,
x > 0.
3. y′′ + 9y = 18 sec3(3x),
|x| < π/6.
4. y′′ + 6y′ + 9y = 2e−3x
x2 + 1.
5. y′′ −4y =
8
e2x + 1.
6. y′′ −4y′ + 5y = e2x tan x,
0 < x < π/2.
7. y′′ + 9y =
36
4 −cos2(3x).
8. y′′ −10y′ + 25y =
2e5x
4 + x2 .
9. y′′ −6y′ + 13y = 4e3x sec2(2x),
|x| < π/4.
10. y′′ + y = sec x + 4ex,
|x| < π/2.
11. y′′ + y = csc x + 2x2 + 5x + 1,
0 < x < π.
12. y′′ −y = 2 tanh x.
13. y′′ −2my′ + m2y = emx/(1 + x2), m constant.
14. y′′ −2y′ + y = 4exx−3 ln x,
x > 0.
15. y′′ + 2y′ + y =
e−x
√
4 −x2 ,
|x| < 2.
16. y′′ + 2y′ + 17y =
64e−x
3 + sin2(4x).
17. y′′ + 4y′ + 4y = 4e−2x
1 + x2 + 2x2 −1.
18. y′′ + 4y′ + 4y = 15e−2x ln x + 25 cos x,
x > 0.
19. y′′′ −3y′′ + 3y′ −y = 2x−2ex, x > 0.
20. y′′′ −6y′′ + 12y′ −8y = 36e2x ln x.
21. y′′′ + 3y′′ + 3y′ + y = 2e−x
1 + x2 .
22. y′′′ −6y′′ + 9y′ = 12e3x. Suggest a better method for
solving this problem.
For Problems 23–26, use a Green’s function to determine a
particular solution to the given differential equation.
23. y′′ −y = F(x).
24. y′′ + 5y′ + 4y = F(x).
25. y′′ + y′ −2y = F(x).
26. y′′ + 4y′ −12y = F(x).
For Problems 27–28, use a Green’s function to solve the
given initial-value problem.
[Hint: Choose x0 = 0.]
27. y′′ −4y′ + 4y = 5xe2x,
y(0) = 1,
y′(0) = 0.
28. y′′ + y = sec x,
y(0) = 0,
y′(0) = 1.
29. Determine a Green’s function for
y′′ −2ay′ + a2y = F(x),
(8.7.26)
where a is a constant, and use it to ﬁnd a particu-
lar solution to (8.7.26) when (in each case, α, β are
constants):
(a) F(x) =
αeax
x2 + β2 .
(b) F(x) =
αeax
*
β2 −x2 ,
|x| < β.
(c) F(x) = eaxxα ln x,
x > 0.

8.8
A Differential Equation with Nonconstant Coefficients 557
30. Consider the differential equation y′′ + y = F(x),
where F is continuous on the interval [a, b]. If
x0 ∈(a, b), show that the solution to the initial-value
problem
) y′′ + y = F(x),
y(x0) = y0,
y′(x0) = y1
is
y(x) = y0 cos(x −x0) + y1 sin(x −x0)
+
& x
x0
F(t) sin(x −t)dt.
31. If F is continuous on the interval [a, b], use the
variation-of-parameters technique to show that a par-
ticular solution to
(D −r)3y = F(x),
r constant
is
yp(x) = 1
2
& x
a
F(t)(x −t)2er(x−t)dt.
32. (a) Use Cramer’s rule to show that the solution to the
system (8.7.22) can be written in the form
u′
k =
F(x)Wk(x)
W[y1, y2, . . . , yn](x),
k = 1, 2, . . . , n,
where Wk(x) denotes the determinant that is
obtained when the kth column of
W[y1, y2, . . . , yn](x) is replaced by
⎡
⎢⎢⎢⎣
0
0
...
1
⎤
⎥⎥⎥⎦.
(b) Use your result from (a) to show that a particular
solution to Equation (8.7.23) is
yp(x) =
& x
x0
K(x, t)F(t)dt,
where K(x, t) is given by
K(x, t) =
y1(x)W1(t) + y2(x)W2(t) + · · · + yn(x)Wn(t)
W[y1, y2, . . . , yn](t)
,
and x0 is an arbitrary point in the interval of in-
terest.
For Problems 33–37, use the result of the preceding problem
to determine a particular solution to the given differential
equation.
33. (D2 −4D + 13)(D −3)y = F(x).
34. (D2 + 8D + 16)(D −2)y = F(x).
35. (D + 1)(D2 + 9)y = F(x).
36. (D + 3)(D −3)(D + 5)y = F(x).
37. (D −1)(D −2)(D + 4)y = F(x).
8.8
A Differential Equation with Nonconstant Coefficients
In this section we consider a particular type of homogeneous differential equation that
has nonconstant coefﬁcients. The solution to this differential equation will be useful
in Chapter 11 and also will enable us to give a further illustration of the power of the
variation-of-parameters technique introduced in the preceding section.
DEFINITION
8.8.1
A differential equation of the form
xn dny
dxn + a1xn−1 dn−1y
dxn−1 + · · · + an−1x dy
dx + any = 0,
where a1, a2, . . . , an are constants, is called a Cauchy-Euler equation.
Notice that if we replace x by kx, where k is a constant, then the form of a Cauchy-Euler
equation is unaltered. Such a re-scaling of x can be interpreted as a dimensional change
(for example, inches to centimeters) and so Cauchy-Euler equations are sometimes called
equi-dimensional equations.

558
CHAPTER 8
Linear Differential Equations of Order n
We begin our analysis by restricting attention to the second-order case and will
assume that x > 0 (the extension to the interval x < 0 is left for the exercises). Thus,
consider the differential equation
x2y′′ + a1xy′ + a2y = 0,
x > 0,
(8.8.1)
where a1 and a2 are constants. The solution technique is based on the observation that
if we substitute y(x) = xr into Equation (8.8.1), then each of the resulting terms on the
left-hand side will be multiplied by the same power of x, which suggests that there may
be solutions of the form
y(x) = xr
(8.8.2)
for an appropriately chosen constant r. In order to investigate this possibility we differ-
entiate (8.8.2) twice to obtain
y′ = rxr−1,
y′′ = r(r −1)xr−2.
Substituting these expressions into Equation (8.8.1) yields the condition
xr[r(r −1) + a1r + a2] = 0,
so that (8.8.2) is indeed a solution to Equation (8.8.1) provided that r satisﬁes
r(r −1) + a1r + a2 = 0.
That is,
r2 + (a1 −1)r + a2 = 0.
(8.8.3)
This is referred to as the indicial equation associated with Equation (8.8.1). The roots
of (8.8.3) are
r1 = −(a1 −1) +
*
(a1 −1)2 −4a2
2
,
r2 = −(a1 −1) −
*
(a1 −1)2 −4a2
2
,
so that there are three cases to consider.
1. r1, r2 real and distinct: In this case two solutions to Equation (8.8.1) are
y1(x) = xr1
and
y2(x) = xr2.
Further, since by assumption in this case r1 ̸= r2,
y2
y1
= xr2−r1 ̸= constant
so that y1 and y2 are linearly independent on (0, ∞). Consequently, the general
solution to Equation (8.8.1) in this case is
y(x) = c1xr1 + c2xr2.
(8.8.4)
2. r1 = r2 = −(a1 −1)
2
: In this case, we obtain only one solution to Equation
(8.8.1); namely
y1(x) = xr1.

8.8
A Differential Equation with Nonconstant Coefficients 559
We try for a second linearly independent solution to Equation (8.8.1) of the form
y2(x) = xr1u(x),
(8.8.5)
where u′ ̸= 0 (this ensures linear independence of {y1, y2}). Differentiating (8.8.5)
with respect to x yields
y′
2(x) = xr1u′ + r1xr1−1u,
y′′
2(x) = xr1u′′ + 2r1xr1−1u′ + r1(r1 −1)xr1−2u.
Substituting these expressions into Equation (8.8.1) we obtain the following equa-
tion for u:
x28
xr1u′′+2r1xr1−1u′+r1(r1−1)xr1−2u
9
+a1x(xr1u′+r1xr1−1u)+a2xr1u = 0.
Equivalently,
xr1+2u′′ + (2r1 + a1)xr1+1u′ + xr1[r1(r1 −1) + a1r1 + a2]u = 0.
The last term on the left-hand side vanishes, since r1 is a root of the indicial
Equation (8.8.3). Thus, substituting r1 = −(a1 −1)
2
and dividing through by
xr1+2, we see that u must satisfy
u′′ + x−1u′ = 0.
This separable equation can be written as
u′′
u′ = −1
x ,
which can be integrated directly to obtain
ln |u′| = −ln x + c.
We can therefore choose (by setting c = 0)
u′ = x−1.
Integrating once more yields
u(x) = ln x,
where we have set the integration constant to zero again. Consequently, a second
solution to Equation (8.8.1) in this case is
y2(x) = xr1 ln x.
Since
y2
y1
= ln x ̸= constant,
it follows that y1 and y2 are linearly independent on (0, ∞). The general solution
to Equation (8.8.1) is therefore given by
y(x) = c1xr1 + c2xr1 ln x = xr1(c1 + c2 ln x).
(8.8.6)

560
CHAPTER 8
Linear Differential Equations of Order n
3. Complex conjugate roots, r1 = a + ib, r2 = a −ib, where b ̸= 0: In this case,
two complex-valued solutions to Equation (8.8.1) are
w1(x) = xa+ib = e(a+ib) ln x = xa[cos(b ln x) + i sin(b ln x)],
w2(x) = xa−ib = e(a−ib) ln x = xa[cos(b ln x) −i sin(b ln x)],
where we have used Euler’s formula. Two corresponding real-valued solutions are
y1(x) = 1
2[w1(x) + w2(x)] = xa cos(b ln x),
y2(x) = 1
2i [w1(x) −w2(x)] = xa sin(b ln x).
Further,
y2
y1
= tan(b ln x) ̸= constant
since b ̸= 0. Consequently, y1 and y2 are linearly independent on (0, ∞), and so
the general solution to Equation (8.8.1) in this case is
y(x) = xa[c1 cos(b ln x) + c2 sin(b ln x)].
(8.8.7)
The preceding discussion is summarized in Table 8.8.1.
Linearly Independent
Roots of Indicial Equation
Solutions to Differential Equation
Real distinct: r1 ̸= r2
y1(x) = xr1, y2(x) = xr2.
Real repeated: r1 = r2
y1(x) = xr1, y2(x) = xr1 ln x.
Complex conjugate:
y1(x) = xa cos(b ln x),
r1 = a + ib, r2 = a −ib
y2(x) = xa sin(b ln x).
Table 8.8.1: Linearly independent solutions to a Cauchy-Euler equation.
Example 8.8.2
Solve
x2y′′ −xy′ −8y = 0,
x > 0.
(8.8.8)
Solution:
Inserting y = xr into Equation (8.8.8) yields the indicial equation
r(r −1) −r −8 = 0.
That is,
r2 −2r −8 = (r −4)(r + 2) = 0.
Hence, two linearly independent solutions to Equation (8.8.8) are
y1(x) = x4
and
y2(x) = x−2.
Consequently, Equation (8.8.8) has general solution
y(x) = c1x4 + c2x−2.
□

8.8
A Differential Equation with Nonconstant Coefficients 561
Example 8.8.3
Solve the initial-value problem
x2y′′ −3xy′ + 13y = 0
(8.8.9)
y(1) = 2,
y′(1) = −5.
(8.8.10)
Solution:
Substituting y = xr into Equation (8.8.9) yields the indicial equation
r2 −4r + 13 = 0,
which has the complex conjugate roots
r = 2 ± 3i.
It follows that two linearly independent solutions to Equation (8.8.9) are
y1(x) = x2 cos(3 ln x)
and
y2(x) = x2 sin(3 ln x),
so that the general solution is
y(x) = c1x2 cos(3 ln x) + c2x2 sin(3 ln x),
which we write as
y(x) = x2[c1 cos(3 ln x) + c2 sin(3 ln x)].
The ﬁrst initial condition in (8.8.10) requires that
c1 cos 0 + c2 sin 0 = 2,
so that c1 = 2. Inserting this value of c1 into the general solution and differentiating
with respect to x yields
y′(x) = 2x
8
2 cos(3 ln x)+c2 sin(3 ln x)
9
+x28
−6x−1 sin(3 ln x)+3x−1c2 cos(3 ln x)
9
.
The second initial condition in (8.8.10) therefore requires
2(2 + 0) + (0 + 3c2) = −5,
so that c2 = −3. Consequently the solution to the initial-value problem is
y(x) = x2[2 cos(3 ln x) −3 sin(3 ln x)].
A sketch of the corresponding solution curve is given in Figure 8.8.1. Due to the trigono-
metric terms, the solution is oscillatory. The amplitude of the oscillation is growing
rapidly with x due to the multiplicative factor x2. Furthermore, as x →0+, the ampli-
tude also approaches zero.
□
2
4
6
8
25
50
75
100
125
150
y
x
Figure 8.8.1: The solution to the initial-value problem in Example 8.8.3.

562
CHAPTER 8
Linear Differential Equations of Order n
Now consider the nonhomogeneous equation
x2y′′ + a1xy′ + a2y = g(x),
(8.8.11)
where a1 and a2 are constants. Since the associated homogeneous equation is a Cauchy-
Euler equation, we can determine the complementary function, and then the variation-of-
parameters method can be used to determine a particular solution. We must remember,
however, that the formulas derived in the variation-of-parameters technique are based
around a differential equation written in the standard form
y′′ + a1(x)y′ + a2(x)y = F(x).
Consequently, when applying the method to a differential equation of the form (8.8.11),
the appropriate formulas for determining u1 and u2 are
y1u′
1 + y2u′
2 = 0,
y′
1u′
1 + y′
2u′
2 = x−2g(x).
We illustrate with an example.
Example 8.8.4
Determine the general solution to
x2y′′ −3xy′ + 4y = x2 ln x,
x > 0.
(8.8.12)
Solution:
The associated homogeneous equation is the Cauchy-Euler equation
x2y′′ −3xy′ + 4y = 0.
(8.8.13)
Substituting y = xr into this equation yields the indicial equation
r2 −4r + 4 = 0.
That is,
(r −2)2 = 0.
Hence two linearly independent solutions to Equation (8.8.13) are
y1(x) = x2
and
y2(x) = x2 ln x.
According to the variation-of-parameters technique, a particular solution to Equation
(8.8.12) is
yp(x) = y1(x)u1(x) + y2(x)u2(x) = x2u1 + x2 ln x u2,
(8.8.14)
where u1 and u2 are determined from
x2u′
1 + x2 ln x u′
2 = 0,
2xu′
1 + (2x ln x + x)u′
2 = ln x.
Hence,
u′
1 = −x−1(ln x)2
and
u′
2 = x−1 ln x,
which upon integration gives
u1(x) = −1
3(ln x)3
and
u2(x) = 1
2(ln x)2,

8.8
A Differential Equation with Nonconstant Coefficients 563
where we have set the integration constants to zero without loss of generality. Substitution
into (8.8.14) yields
yp(x) = −1
3x2(ln x)3 + 1
2 x2(ln x)3 = 1
6x2(ln x)3.
Thus, Equation (8.8.12) has general solution
y(x) = c1x2 + c2x2 ln x + 1
6x2(ln x)3,
which can be written as
y(x) = 1
6x2[c3 + c4 ln x + (ln x)3],
where c3 = 6c1 and c4 = 6c2.
□
Generalization to Higher Order
Now consider the general Cauchy-Euler equation of order n,
xny(n) + a1xn−1y(n−1) + · · · + an−1xy′ + any = 0,
(8.8.15)
where a1, a2, . . . , an are constants, on the interval x > 0. We begin by substituting
y(x) = xr into (8.8.15). The result is the indicial equation
r(r −1)(r −2) · · · (r −n + 1) + a1r(r −1) · · · (r −n + 2) + · · · + an−1r + an = 0.
(8.8.16)
In the case that (8.8.16) has n distinct roots, say r1,r2, . . . ,rn, we directly obtain the n
solutions
xr1, xr2, . . . , xrn,
and it can be shown that these solutions are linearly independent on (0, ∞). Of course,
if some of the roots are complex, then we must take the real and imaginary parts of
the corresponding complex-valued solutions in order to determine real-valued solutions.
If, however, r = r1 is a root of multiplicity k, then we cannot directly determine the
appropriate number of solutions. Based on our experience in the second-order case, we
might suspect that there are k solutions of the form
xr1, xr1 ln x, xr1(ln x)2, . . . , xr1(ln x)k−1.
We now show that this is indeed correct. The key idea is that the change of variables
x = ez, or equivalently z = ln x, transforms Equation (8.8.15) into a constant coefﬁcient
equation, which can be solved via the technique of Section 8.2. To establish this we need
the following lemma.
Lemma 8.8.5
If y is a sufﬁciently smooth function of x and x = ez, then
xk dk y
dxk = D(D −1)(D −2) · · · (D −k + 1)y,
k = 1, 2, . . . ,
(8.8.17)
where D = d/dz.

564
CHAPTER 8
Linear Differential Equations of Order n
Proof The proof of the result requires the chain rule and mathematical induction. Since
x = ez, we have z = ln x, so that dz/dx = 1/x. Thus, by the chain rule,
dy
dx = dy
dz
dz
dx = 1
x
dy
dz ,
which implies that
x dy
dx = Dy,
(8.8.18)
where D = d/dz. Thus, (8.8.17) is true when k = 1. Now suppose that the result is true
when k = m. That is,
xm dm y
dxm = D(D −1)(D −2) · · · (D −m + 1)y.
(8.8.19)
We must show that this implies its validity when k = m + 1. We proceed as follows.
Using the product rule we have
x d
dx
'
xm dm y
dxm
(
= xm+1 dm+1y
dxm+1 + mxm dm y
dxm ,
which can be rearranged to obtain
xm+1 dm+1y
dxm+1 = x d
dx
'
xm dm y
dxm
(
−mxm dm y
dxm .
Substituting from (8.8.18) for x(d/dx) = D and from (8.8.19) for xm(dm y/dxm) into
this equation yields
xm+1 dm+1y
dxm+1 = D [D(D −1)(D −2) · · · (D −m + 1)y]
−m [D(D −1)(D −2) · · · (D −m + 1)y] ;
that is, since we can interchange the order of the factors in a polynomial differential
operator,
xm+1 dm+1y
dxm+1 = D(D −1)(D −2) · · · (D −m + 1)(D −m)y.
We have therefore established that the validity of (8.8.17) when k = m implies its validity
when k = m + 1. Since the result is true when k = 1 it follows, by induction, that it is
valid for all positive integers k.
Remark
Although the rule for transforming derivatives given in (8.8.17) looks quite
formidable, it is quite easy to remember. We write out the ﬁrst three derivatives in order
to elucidate this:
x dy
dx = Dy,
x2 d2y
dx2 = D(D −1)y,
x3 d3y
dx3 = D(D −1)(D −2)y,
where D = d/dz.
We can now establish the main result.

8.8
A Differential Equation with Nonconstant Coefficients 565
Theorem 8.8.6
The change of variables x = ez transforms the Cauchy-Euler equation
xn dny
dxn + a1xn−1 dn−1y
dxn−1 + · · · + an−1x dy
dx + any = 0,
x > 0,
(8.8.20)
into the constant coefﬁcient equation
[D(D −1)(D −2) · · · (D −n + 1) + a1D(D −1)
· · · (D −n + 2) + · · · + an−1D + an]y = 0.
(8.8.21)
Proof Equation (8.8.21) follows directly by substituting for each of the terms
xk(dk y/dxk) in (8.8.20) using the previous lemma.
The auxiliary equation for the constant coefﬁcient equation (8.8.21) is
r(r −1)(r −2) · · · (r −n + 1) + a1r(r −1) · · · (r −n + 2) + · · · + an−1r + an = 0,
which coincides with the indicial equation (8.8.16) of the original differential equation.
If r = r1 is a root of multiplicity k, then it follows from our results of Section 8.2 that
the corresponding linearly independent solutions to (8.8.21), and hence (8.8.20), are
er1z,
zer1z,
. . . ,
zk−1er1z;
that is, since z = ln x,
xr1,
xr1 ln x,
. . . ,
xr1(ln x)k−1.
If r = a + ib is complex, then we can obtain the appropriate real-valued solutions by
extracting the real and imaginary parts of the corresponding complex-valued solutions.
Thus, corresponding to complex conjugate roots of the indicial equation of multiplicity
k, we obtain the 2k real-valued solutions:
xa cos(b ln x),
xa sin(b ln x),
xa ln x cos(b ln x),
xa ln x sin(b ln x),
. . . ,
xa(ln x)k−1 cos(b ln x),
xa(ln x)k−1 sin(b ln x).
In summary, to solve a Cauchy-Euler equation we can substitute y(x) = xr into the
differential equation to obtain the indicial equation. Corresponding to each root, ri,
we can then determine the appropriate number of linearly independent solutions as
indicated above.
Example 8.8.7
Determine the general solution to
x3y′′′ + 2x2y′′ + 4xy′ −4y = 0,
x > 0.
(8.8.22)
Solution:
Substituting y(x) = xr into (8.8.22) yields the indicial equation
r(r −1)(r −2) + 2r(r −1) + 4r −4 = 0;
that is,
r3 −r2 + 4r −4 = 0,
which can be factored as
(r −1)(r2 + 4) = 0.

566
CHAPTER 8
Linear Differential Equations of Order n
The roots of the indicial equation are, therefore,
r = 1,
r = ±2i,
so that three linearly independent solutions to (8.8.22) on (0, ∞) are
y1(x) = x,
y2(x) = cos(2 ln x),
y3(x) = sin(2 ln x).
Consequently, the general solution is
y(x) = c1x + c2 cos(2 ln x) + c3 sin(2 ln x).
□
Example 8.8.8
Determine the general solution to
x3y′′′ −3x2y′′ + 7xy′ −8y = 0,
x > 0.
Solution:
In this case the substitution y(x) = xr yields the indicial equation
r(r −1)(r −2) −3r(r −1) + 7r −8 = 0;
that is,
r3 −6r2 + 12r −8 = 0.
By inspection we see that r = 2 is a root, and therefore the indicial equation can be
written as
(r −2)(r2 −4r + 4) = 0;
that is,
(r −2)3 = 0.
It follows that three linearly independent solutions to the given differential equation are
y1(x) = x2,
y2(x) = x2 ln x,
y3(x) = x2(ln x)2,
so that the general solution is
y(x) = x2[c1 + c2 ln x + c3(ln x)2].
□
Finally we mention that on the interval (−∞, 0), the substitution y(x) = (−x)r can
be used to obtain the indicial equation, and the solutions of the differential equation can
be obtained by replacing x with −x in the solutions obtained previously in this section.
Consequently, if we use | x | in place of x, we will obtain solutions to a Cauchy-Euler
equation that are valid for all x ̸= 0.
Exercises for 8.8
Key Terms
Cauchy-Euler equation, Equi-dimensional equation, Indicial
equation.
Skills
• Be able to determine whether or not a given differential
equation is a Cauchy-Euler equation.
• Be able to determine the indicial equation associated
with a Cauchy-Euler equation.
• Be able to determine two linearly independent solu-
tions to a Cauchy-Euler equation according to whether
the associated indicial equation has real distinct roots,
real repeated roots, or complex conjugate roots.
• Be able to ﬁnd the general solution to a Cauchy-Euler
equation.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A Cauchy-Euler equation is a differential equation of
the form
x2y′′ + a1y′ + a2y = 0,
where a1 and a2 are constants.

8.8
A Differential Equation with Nonconstant Coefficients 567
(b) The Cauchy-Euler equation
x2y′′ −2xy′ −18y = 0
has two linearly independent solutions of the form
y(x) = xr.
(c) The Cauchy-Euler equation
x2y′′ + 9xy′ + 16y = 0
has two linearly independent solutions of the form
y(x) = xr.
(d) All solutions y(x) to the Cauchy-Euler equation
x2y′′ + 6xy′ + 6y = 0
tend to 0 as x →+∞.
(e) If y(x) = ln x
x
is obtained by the method of this sec-
tion as a solution to a Cauchy-Euler equation, then the
differential equation is
x2y′′ + 3xy′ + y = 0.
(f) All nontrivial solutions to the Cauchy-Euler equation
x2y′′ −5xy′ −7y = 0
are oscillatory as a function of x.
Problems
ForProblems1–8,determinethegeneralsolutiontothegiven
differential equation on (0, ∞).
1. x2y′′ −4xy′ + 4y = 0.
2. x2y′′ + 3xy′ + y = 0.
3. x2y′′ + 5xy′ + 13y = 0.
4. x2y′′ −xy′ + 5y = 0.
5. x2y′′ −6y = 0.
6. x2y′′ −3xy′ + 4y = 0.
7. x2y′′ + xy′ + 16y = 0.
8. x2y′′ −xy′ −35y = 0.
For Problems 9–11, ﬁnd the solution to the Cauchy-Euler
equation on the interval (0, ∞). In each case, m and k are
positive constants.
9. x2y′′ + xy′ −m2y = 0.
10. x2y′′ −x(2m −1)y′ + m2y = 0.
11. x2y′′ −x(2m −1)y′ + (m2 + k2)y = 0.
12. Consider the Cauchy-Euler equation
x2y′′ + xa1y′ + a2y = 0,
x > 0.
(8.8.23)
(a) Show that the change of independent variable de-
ﬁned by x = ez transforms Equation (8.8.23) into
the constant coefﬁcient equation
d2y
dz2 + (a1 −1)dy
dz + a2y = 0.
(8.8.24)
(b) Show that if y1(z), y2(z) are linearly inde-
pendent solutions to Equation (8.8.24), then
y1(ln x), y2(ln x) are linearly independent solu-
tions to Equation (8.8.23).
[Hint: From (a), we already know that y1, y2 are
solutions to Equation (8.8.23). To show that they
are linearly independent, verify that
W[y1, y2](x) = dz
dx W[y1, y2](z).]
13. Consider the Cauchy-Euler equation
x2y′′ + axy′ + by = 0,
x < 0.
(8.8.25)
Show that the substitution y = (−x)r yields the indi-
cial equation
r2 + (a −1)r + b = 0.
Thus, linearly independent solutions to Equation
(8.8.25) on (−∞, 0) can be determined by replacing
x with −x, in (8.8.4), (8.8.6), and (8.8.7). As a conse-
quence, if we replace x by |x| in these solutions, we
will obtain solutions to Equation (8.8.1) that are valid
for all x ̸= 0.
For Problems 14–21, solve the given differential equation
on the interval x > 0. [Remember to put the equation in
standard form.]
14. x2y′′ + 4xy′ + 2y = 4 ln x.
15. x2y′′ + 4xy′ + 2y = cos x.
16. x2y′′ + xy′ + 9y = 9 ln x.
17. x2y′′ −xy′ + 5y = 8x(ln x)2.
18. x2y′′ −4xy′ + 6y = x4 sin x.
19. x2y′′ + 6xy′ + 6y = 4e2x.

568
CHAPTER 8
Linear Differential Equations of Order n
20. x2y′′ −3xy′ + 4y = x2
ln x .
21. x2y′′ −(2m −1)xy′ + m2y = xm(ln x)k, where m, k
are constants.
22. (a) Solve the initial-value problem
x2y′′ −xy′ + 5y = 0,
y(1) =
√
2,
y′(1) = 3
√
2
and show that your solution can be written in the
form
y(x) = 2x cos(2 ln x −π/4).
(b) Determine all zeros of y(x).
(c) ⋄Sketch the corresponding solution curve on the
interval [0.001, 16], and verify the zeros in the
interval [3, 16].
23. The motion of a physical system is governed by the
initial-value problem
t2 d2y
dt2 + t dy
dt + 25y = 0,
y(1) = 3
√
3/2,
y′(1) = 15/2.
(a) Solve the given initial-value problem, and show
that your solution can be written in the form
y(t) = 3 cos(5 ln t −π/6).
(b) Determine all zeros of y(t).
(c) ⋄Sketch the corresponding solution curve on the
interval [0.01, 2].
(d) Is the system performing simple harmonic mo-
tion? Justify your answer.
24. We have shown that in the case of complex conjugate
roots, r = a ± ib, b ̸= 0, of the indicial equation, the
general solution to the Cauchy-Euler equation
y′′ + a1y′ + a2y = 0,
x > 0
is
y(x) = xa[c1 cos(b ln x) + c2 sin(b ln x)]. (8.8.26)
(a) Show that (8.8.26) can be written in the form
y(x) = Axa cos(b ln x −φ)
for appropriate constants A and φ.
(b) Determine all zeros of y(x), and the distance be-
tween successive zeros. What happens to this dis-
tance as x →∞, and as x →0+?
(c) Describe the behavior of the solution as x →∞
and as x
→0+ in each of the three cases
a > 0, a < 0, and a = 0. In each case, give
a general sketch of a generic solution curve.
8.9
Reduction of Order
Finally, in this chapter, we consider a powerful technique for determining the general
solution to any second-order linear differential equation, assuming that we know just
one solution to the associated homogeneous equation. This technique is usually referred
to as reduction of order.
Consider
y′′ + a1(x)y′ + a2(x)y = F(x),
(8.9.1)
where we assume that the functions a1, a2, F(x) are continuous on an interval J. We
know that the general solution to Equation (8.9.1) is of the form
y(x) = c1y1(x) + c2y2(x) + yp(x),
where y1 and y2 are linearly independent (i.e., nonproportional) solutions to
y′′ + a1(x)y′ + a2(x)y = 0
(8.9.2)
on J. Suppose that we have found one solution, say, y = y1(x) to (8.9.2). We now
replace the constant c with an arbitrary function u(x), and try for a solution to (8.9.1) of
the form
y(x) = u(x)y1(x).
(8.9.3)

8.9
Reduction of Order 569
The following derivation establishes that u(x) can, in theory, always be determined from
Equation (8.9.1). Differentiating (8.9.3) twice with respect to x yields
y′ = u′y1 + uy′
1,
y′′ = u′′y1 + 2u′y′
1 + uy′′
1.
Substituting into Equation (8.9.1) gives
(u′′y1 + 2u′y′
1 + uy′′
1) + a1(x)(u′y1 + uy′
1) + a2(x)(uy1) = F(x),
so that (8.9.3) solves Equation (8.9.1) provided u satisﬁes
u[y′′
1 + a1(x)y′
1 + a2(x)y1] + u′′y1 + u′[2y′
1 + a1(x)y1] = F(x).
(8.9.4)
Since y = y1(x) is a solution to Equation (8.9.2) the coefﬁcient of u in this expression
vanishes. Consequently Equation (8.9.4) reduces to
u′′y1 + u′[2y′
1 + a1(x)y1] = F(x),
which is a ﬁrst-order linear differential equation for u′. We have therefore reduced the
order of the differential equation, hence the name of the technique. If we let w = u′,
then the preceding differential equation can be written as
w′ +
'2y′
1
y1
+ a1
(
w = F(x)
y1
.
(8.9.5)
An integrating factor for (8.9.5) is
I (x) = e
& x '2y′
1(s)
y1(s) + a1(s)
(
ds
= y2
1(x)e
! x a1(s)ds.
According to the technique developed in Section 1.6, multiplying Equation (8.9.5) by
I (x) reduces it to the integrable form
d
dx [I (x)w(x)] = I (x)F(x)
y1(x)
.
Integrating the preceding equation and dividing by I (x) yields
w(x) = 1
I
& x I (s)F(s)
y1(s)
ds +
c1
I (x),
where c1 is an integration constant. Since w = u′, we have
u′(x) =
1
I (x)
& x I (s)F(s)
y1(s)
ds +
c1
I (x).
One more integration yields
u(x) =
& x
1
I (t)
& t I (s)F(s)
y1(s)
ds dt + c1
& x
1
I (s)ds + c2,
so that
y(x) = u(x)y1(x)
= c1y1(x)
& x
1
I (s)ds + c2y1(x) + y1(x)
& x
1
I (t)
& t I (s)F(s)
y1(s)
ds dt.
(8.9.6)

570
CHAPTER 8
Linear Differential Equations of Order n
From this formula we can identify two linearly independent solutions to (8.9.2), namely
y(x) = y1(x)
and
y(x) = y1(x)
& x
1
I (s)ds,
and the following particular solution to (8.9.1):
yp(x) = y1(x)
& x
1
I (t)
& t I (s)F(s)
y1(s)
ds dt.
Consequently, Equation (8.9.6) gives the general solution to Equation (8.9.1). We have
therefore established the following theorem.
Theorem 8.9.1
If y = y1(x) is a solution to
y′′ + a1(x)y′ + a2(x)y = 0
on an interval J, then substituting y(x) = y1(x)u(x) into
y′′ + a1(x)y′ + a2(x)y = F(x)
yields its general solution.
Example 8.9.2
Determine the general solution to
xy′′ −2y′ + (2 −x)y = 0,
x > 0
(8.9.7)
given that one solution is y1(x) = ex.
Solution:
To determine the general solution we substitute
y(x) = y1(x)u(x) = exu(x)
(8.9.8)
into (8.9.7). We ﬁrst compute the appropriate derivatives:
y′ = ex(u′ + u),
y′′ = ex(u′′ + 2u′ + u).
Substituting these expressions into Equation (8.9.7), we ﬁnd that u must satisfy
x(u′′ + 2u′ + u) −2(u′ + u) + (2 −x)u = 0,
which simpliﬁes to
xu′′ + 2u′(x −1) = 0,
or equivalently,
w′ + 2(x −1)
x
w = 0,
where w = u′. We can solve the preceding differential equation either as a linear equation
or a separable equation. Separating the variables yields
w′
w = 2(x−1 −1).
By integrating, we obtain
ln |w| = 2(ln x −x) + c,

8.9
Reduction of Order 571
which, upon exponentiation, can be written as
w = c1x2e−2x.
Therefore,
u′ = c1x2e−2x.
Integration by parts gives
u(x) = −1
4c1e−2x(1 + 2x + 2x2) + c2.
Substituting into (8.9.8) yields the general solution to (8.9.7), namely,
y(x) = c1e−x(1 + 2x + 2x2) + c2ex,
where we have absorbed the factor of −1/4 into c1.
□
Example 8.9.3
Determine the general solution to
x2y′′ + 3xy′ + y = 4 ln x,
x > 0
(8.9.9)
given that one solution to the associated homogeneous equation is y(x) = x−1.
Solution:
We let
y(x) = x−1u(x),
(8.9.10)
where u(x) is to be determined. Differentiating y twice yields
y′ = x−1u′ −x−2u,
y′′ = x−1u′′ −2x−2u′ + 2x−3u.
Substituting into Equation (8.9.9) and collecting terms, we obtain the following differ-
ential equation for u:
u′′ + x−1u′ = 4x−1 ln x,
or equivalently,
w′ + x−1w = 4x−1 ln x,
(8.9.11)
where w = u′. An integrating factor for (8.9.11) is I (x) = e
!
x−1 dx = x, so that
Equation (8.9.11) can be written in the equivalent form
d
dx (xw) = 4 ln x.
Integrating both sides with respect to x yields
xw = 4x(ln x −1) + c1,
where c1 is a constant. Thus,
w(x) = 4(ln x −1) + c1x−1.
Consequently,
u′(x) = 4(ln x −1) + c1x−1,
which can be integrated directly to obtain
u(x) = 4x(ln x −2) + c1 ln x + c2,

572
CHAPTER 8
Linear Differential Equations of Order n
where c2 is another integration constant. Inserting this expression for u into (8.9.10)
yields
y(x) = 4(ln x −2) + c1x−1 ln x + c2x−1,
which is the general solution to Equation (8.9.9).
□
Exercises for 8.9
Skills
• Be able to carry out the reduction of order technique
to ﬁnd the general solution to a second-order linear
differential equation.
Problems
For Problems 1–6, y1 is a solution to the given differential
equation. Use the method of reduction of order to determine
a second linearly independent solution.
1. x2y′′ −3xy′ + 4y = 0,
x > 0,
y1(x) = x2.
2. xy′′ + (1 −2x)y′ + (x −1)y = 0,
x > 0,
y1(x) = ex.
3. x2y′′ −2xy′ + (x2 + 2)y = 0,
x > 0,
y1(x) = x sin x.
4. (1 −x2)y′′ −2xy′ + 2y = 0,
−1 < x < 1,
y1(x) = x.
5. y′′ −x−1y′ + 4x2y = 0,
x > 0,
y1(x) = sin(x2).
6. 4x2y′′ + 4xy′ + (4x2 −1)y = 0,
x > 0,
y1(x) = x−1/2 sin x.
7. Consider the Cauchy-Euler equation
x2y′′ −(2m −1)xy′ + m2y = 0,
x > 0, (8.9.12)
where m is a constant.
(a) Determine a particular solution to Equation
(8.9.12) of the form y1(x) = xr.
(b) Use your solution from (a) and the method of
reduction of order to obtain a second linearly in-
dependent solution.
8. Determine the values of the constants a0, a1, and a2
such that
y(x) = a0 + a1x + a2x2
is a solution to
(4 + x2)y′′ −2y = 0,
and use the reduction of order technique to ﬁnd a sec-
ond linearly independent solution.
9. Consider the differential equation
xy′′ −(αx + β)y′ + αβy = 0,
x > 0,
(8.9.13)
where α and β are constants.
(a) Show that y1(x) = eαx is a solution to Equation
(8.9.13).
(b) Use reduction of order to derive the second lin-
early independent solution
y2(x) = eαx
&
xβe−αxdx.
(c) In the particular case when α = 1 and β is a
nonnegative integer, show that a second linearly
independent solution to Equation (8.9.13) is
y2(x) = 1 + x + 1
2!x2 + · · · + 1
β!xβ.
For Problems 10–15, y1 is a solution to the associated ho-
mogeneous equation. Use the method of reduction of order
to determine the general solution to the given differential
equation.
10. y′′ + y = csc x,
0 < x < π,
y1(x) = sin x.
11. xy′′ −(2x + 1)y′ + 2y = 8x2e2x,
x > 0,
y1(x) = e2x.
12. x2y′′ −3xy′ + 4y = 8x4,
x > 0,
y1(x) = x2.

8.10
Chapter Review 573
13. y′′ −6y′ + 9y = 15e3x√x,
x > 0,
y1(x) = e3x.
14. y′′ −4y′ + 4y = 4e2x ln x,
x > 0,
y1(x) = e2x.
15. 4x2y′′ + y = √x ln x,
x > 0,
y1(x) = x1/2.
16. Consider the differential equation
y′′ + p(x)y′ + q(x)y = r(x),
(8.9.14)
where p, q, and r are continuous on an interval I. If
y = y1(x) is a solution to the associated homogeneous
equation, show that y2(x) = u(x)y1(x) is a solution
to Equation (8.9.14) provided v = u′ is a solution to
the linear differential equation
v′ +
'
2 y′
1
y1
+ p
(
v = r
y1
.
Express the solution to Equation (8.9.14) in terms of
integrals. Identify two linearly independent solutions
to the associated homogeneous equation and a partic-
ular solution to Equation (8.9.14).
8.10
Chapter Review
This chapter has studied the general theory for linear differential equations of arbitrary
order n. To do so, we use the linear differential operator of order n,
L = Dn + a1(x)Dn−1 + · · · + an−1(x)D + an(x),
where D(y) = y′ for a differentiable function y on an interval I. Thus,
Ly = y(n) + a1(x)y(n−1) + · · · + an−1(x)y′ + an(x)y.
A homogeneous linear differential equation of order n can thus be expressed as
Ly = 0,
and the solution set is an n-dimensional vector space. Therefore, the general solution to
Ly = 0 can be constructed from any n linearly independent solutions to the differential
equation.
If L is a polynomial differential operator P(D) (with constant coefﬁcients), then a
real root r of the equation P(D) = 0 of multiplicity m contributes linearly independent
solutions
erx,
xerx,
. . . ,
xm−1erx.
On the other hand, a complex root r = a + bi (b ̸= 0) of multiplicity m contributes 2m
solutions
eax cos bx,
xeax cos bx,
. . . ,
xm−1eax cos bx,
eax sin bx,
xeax sin bx,
. . . ,
xm−1eax sin bx.
Note that r = a −bi necessarily will occur with multiplicity m as well, but contributes
the same 2m solutions as r = a + bi.
The general solution to a nonhomogeneous differential equation L y = F(x) has the
form
y(x) = yc(x) + yp(x),
where yc(x) is the complementary function (which solves the corresponding homoge-
neous differential equation) and yp(x) is any particular solution to the original nonho-
mogeneous differential equation. We have discussed two basic methods for determining
a particular solution yp(x), undetermined coefﬁcients and variation-of-parameters.

574
CHAPTER 8
Linear Differential Equations of Order n
Method of Undetermined Coefficients: Annihilators
When F(x) has one of the forms
F(x) =
⎧
⎨
⎩
cxkeax,
cxkeax sin bx,
cxkeax cos bx,
or sums of these forms, we can ﬁnd an annihilator A(D) of F (i.e., such that A(D)F =
0). For instance, A(D) = (D −a)k+1 annihilates cxkeax, while A(D) = (D2 −2aD +
a2 + b2)k+1 annihilates cxkeax cos bx and cxkeax sin bx. If F(x) is given by a sum of
the forms above, then it is annihilated by the corresponding product of annihilators.
By operating on the differential equation P(D)y = F(x) by the annihilator A(D)
of F, we obtain the homogeneous differential equation A(D)P(D)y = 0, whose general
solutioncanbereadilyobtainedbydeterminingtherootsoftheequation A(D)P(D) = 0.
By removing terms of the general solution to A(D)P(D)y = 0 that coincide with the
complementary function for the differential equation P(D)y = F(x), the remaining
terms provide the form of a particular solution yp, also known as a trial solution, to
the nonhomogeneous differential equation. This trial solution provides the correct form
for a particular solution, but contains undetermined coefﬁcients that can be computed
by substituting the trial solution into the differential equation. The table at the end of
Section 8.3 shows the appropriate trial solution to use if F(x) is of any of the forms
mentioned above.
Method of Variation-of-Parameters
Here we attempt a particular solution to L y = F(x) of the form
yp(x) = u1(x)y1(x) + u2(x)y2(x) + · · · + un(x)yn(x),
where n is the degree of P(D), y1, y2, . . . , yn are n linearly independent solutions to
the corresponding homogeneous differential equation L y = 0, and u1, u2, . . . , un are
unknown functions satisfying Equations (8.7.22).
Applications
In Sections 8.5 and 8.6, we have considered some applications of second-order nonho-
mogeneous linear differential equations. First, we explored the motion of a mechanical
system consisting of a mass attached to a spring under the inﬂuence of a variety of pos-
sible forces. Next we saw that the mathematics of a spring-mass system coincides with
that of an RLC circuit, given an appropriate renaming of the variables.
Cauchy-Euler Equations
A second-order Cauchy-Euler differential equation has the form
x2y′′ + a1xy′ + a2y = 0,
x > 0
and is an important example of a differential equation with nonconstant coefﬁcients. As
with the method of undetermined coefﬁcients, the solution technique relies on using a
trial solution, this time of the form y(x) = xr, to determine two linearly independent
solutions to the Cauchy-Euler equation. The text also discusses a natural generalization
to higher-order Cauchy-Euler equations.

8.10
Chapter Review 575
Reduction of Order
In the ﬁnal section of this chapter, a powerful method is used to generate a second
linearly independent solution to a second-order nonhomogeneous differential equation,
once one solution to the associated homogeneous differential equation has been obtained.
The procedure described actually reduces the problem to solving a ﬁrst-order linear
differential equation.
Additional Problems
In Problems 1–6, ﬁnd Ly for the given differential operator
L and the given function y.
1. L = D2 + 3,
y(x) = ex3.
2. L = 5,
y(x) =
1
1 + x2 .
3. L = 1
x D2 + x D −2,
y(x) = 4 sin x.
4. L = x2D3 −sin x D,
y(x) = e2x + cos x.
5. L = (x2 + 1)D3 −(cos x)D + 5x2,
y(x) = ln x + 8x5.
6. L = 4x2D,
y(x) = sin2(x2 + 1).
In Problems 7–13, determine the general solution to the given
differential equation.
7. y′′′ + 3y′′ −4y = 0.
8. y′′′ + 11y′′ + 36y′ + 26y = 0.
[Hint: r = −1 is a root of the auxiliary polynomial.]
9. y(iv) + 13y′′ + 36y = 0.
10. y′′′ + 10y′′ + 25y′ = 0.
11. (D + 3)3(D2 −4D + 13)y = 0.
12. (D2 −2D + 2)3y = 0.
13. (D2 + 4D + 4)(D −3)y = 0.
In Problems 14–17, determine the annihilator of the given
function.
14. F(x) = 5e−x + 4x.
15. F(x) = 17e3x sin x.
16. F(x) = 2x5 cos 4x.
17. F(x) = 4x sin x −3e−2x.
In Problems 18–23, determine a trial solution for the given
nonhomogeneous differential equation. In each case, check
that you obtain the same trial solution with or without the
use of annihilators.
18. y′′ + 6y′ + 9y = 4e−3x.
19. y′′ + 6y′ + 9y = 4e−2x.
20. y′′′ −6y′′ + 25y′ = x2.
21. y′′′ −6y′′ + 25y′ = sin 4x.
22. y′′′ + 9y′′ + 24y′ + 16y = 8e−x + 1.
23. y(vi) + 3y(iv) + 3y′′ + y = 2 sin x.
For Problems 24–29, solve the given nonhomogeneous dif-
ferential equation by using (a) the method of undetermined
coefﬁcients, and (b) the variation-of-parameters method.
24. The differential equation in Problem 19.
25. The differential equation in Problem 20.
26. The differential equation in Problem 21.
27. y′′ −4y = 5ex.
28. y′′ + 2y′ + y = 2xe−x.
29. y′′ −y = 4ex.
For Problems 30–39, state whether the annihilator method
can be used to determine a particular solution to the given dif-
ferential equation. If the technique cannot be used, state why
not. If the technique can be used, then give an appropriate
trial solution.
30. y′′ + xy = sin x.
31. y′′ + 4y = ln x.
32. y′′ + 2y′ −3y = 5ex.
33. y′′ + y = tan x.
34. y′′ + y = 4 cos 2x + 3ex.
35. y′′ −8y′ + 16y = 7e4x.
36. x2y′′ + 5xy′ + 7y = 3ex.
37. y′′ −2y′ + 5y = 7ex cos x + sin x.

576
CHAPTER 8
Linear Differential Equations of Order n
38. y′′ + 4y = 7 cos2 x.
39. d2y
dt2 −2a dy
dt + (a2 + b2)y = eat(4t + cos bt), where
a and b are positive constants.
For Problems 40–45, use the annihilator method to solve the
given differential equation.
40. y′′ + 4y = 7ex.
41. y′′ + 2y′ −3y = 2xe−3x.
42. y′′ + 4y′ = 4x2.
43. y′′ + 4y = 8 cos 2x.
44. y′′ −8y′ + 16y = 5e4x.
45. y′′ −y = 3e2x + sin x.
46. Solve the initial-value problem:
y′′ −y′ −2y = 15e2x,
y(0) = 0,
y′(0) = 8.
For Problems 47–51, use the variation-of-parameters method
to solve the given differential equation.
47. y′′ + y =
1
sin x .
48. y′′ + y = tan x.
49. y′′ −2my′ + m2y = emx ln x,
x > 0, where m is a
real constant.
50. y′′ + 2y′ + y = x−1ex.
51. y′′ −2y′ + y = ex ln x,
x > 0.
52. Solve Problem 49 by the reduction of order method,
given that y1(x) = emx is a solution to the associated
homogeneous differential equation.
For Problems 53–58, ﬁnd the general solution to the given
differential equation on the interval (0, ∞).
53. x2y′′ + 9xy′ + 16y = 0.
54. x2y′′ + 9xy′ + 15y = 0.
55. x2y′′ −11xy′ + 37y = 0.
56. x2y′′ + xy′ + 25y = 0.
57. x2y′′ −2xy′ −18y = 0.
58. x2y′′ −xy′ = 0.
For Problems 59–61, solve the given differential equation
on the interval x > 0. Use the variation-of-parameters tech-
nique to obtain a particular solution.
59. x2y′′ + 9xy′ + 16y = x−3.
60. x2y′′ −3xy′ −12y = x4 + 5x2.
61. x2y′′ −5xy′ + 10y = x3.
For Problems 62–66, determine a particular solution to the
given differential equation.
62. y′′ −4y′ −5y = e3x sin 2x.
63. y′′ + 4y′ + 4y = ln x
xe2x .
64. y′′ −2y′ + 26y = ex cos 5x.
65. y′′ −9y′ + 20y = x3e5x.
66. y′′ −8y′ + 17y = e4x csc x.
Project: Motion of a Stretched String with Fixed End-Points
Consider a taut string of length L whose ends are ﬁxed. Let u(x, t) denote the dis-
placement from its equilibrium position of the point on the string at distance x from the
left-hand end at time t. Then, the motion of the string is governed by the following initial
boundary-value problem (IBVP):
∂2u
∂t2 −c2 ∂2u
∂x2 = 0,
(8.10.1)
u(0, t) = 0,
u(L, t) = 0,
t > 0,
(8.10.2)
u(x, 0) = f (x),
∂u
∂t (x, 0) = g(x),
0 < x < L.
(8.10.3)
The partial differential equation (8.10.1) is called the wave equation. In this equation
c isapositiveconstant.TheconditionsgiveninEquation(8.10.2)areboundaryconditions

8.10
Chapter Review 577
u(x, t0) 
u(x0, t0) 
x0
string
no displacement
no displacement
x
L
Figure 8.10.1: Snapshot of string at time t = t0
that correspond to there being no displacement of the end-points of the string for all t > 0,
whereas Equation (8.10.3) gives the initial conditions corresponding to the problem.
More speciﬁcally, the functions f and g denote the initial shape of the string, and the
initial velocity at each point along the string at t = 0, respectively. In this project we
will derive the Fourier series solution to this initial boundary-value problem. This will
require almost all of the concepts that we have been studying in Chapters 4 through 8.
1. (Separation of variables) Begin by showing that any solutions to (8.10.1) of the
form u(x, t) = X(x) · T (t) must satisfy
X′′
X =
¨T
c2T ,
(8.10.4)
where the prime and dot denote differentiation with respect to x and t, respectively.
Since the only way that a function of x only can equal a function of t only is for
both functions to equal the same constant, we can conclude that
X′′
X = k
and
¨T
c2T = k
where k is a constant.
2. Show that imposing the boundary conditions (8.10.2) on any function of the form
u(x, t) = X(x) · T (t) requires that
X(0) = 0
and
X(L) = 0.
3. Combine the results from Parts 1 and 2 to conclude that solutions to (8.10.1) and
(8.10.2) of the form u(x, t) = X(x) · T (t) are obtained by solving
X′′ −kX = 0,
X(0) = 0,
X(L) = 0
(8.10.5)
¨T −c2kT = 0.
(8.10.6)
4. Show that if k > 0 or k = 0 then the only solution to (8.10.5) is u(x, t) = 0.
5. Now consider the case when k < 0, and set k = −λ2, for some real number λ.
Show that in order for there to be nonzero solutions to (8.10.5) we must have
λ2 = λ2
n = n2π2
L2 ,
n = 1, 2, . . . .
Without loss of generality we can choose
λ = λn = nπ
L ,
n = 1, 2, . . . .
Show that with this choice the corresponding solutions to (8.10.5) are
X = Xn(x) = sin nπx
L ,
n = 1, 2, . . .
(8.10.7)
The scalars, λn, are called the eigenvalues of the problem, and the corresponding
functions, Xn, are called the eigenfunctions of the problem.

578
CHAPTER 8
Linear Differential Equations of Order n
6. With the choice of λn given in (8.10.7), derive the corresponding general solution to
(8.10.6), and thereby conclude that the separable solutions to (8.10.5) and (8.10.6)
are given by
un(x, t) = (An cos nπct
L
+ Bn
nπct
L
) sin nπx
L ,
n = 1, 2, . . . .
(8.10.8)
7. Now let
Fk(x, t) =
k
?
n=1
un(x, t) =
k
?
n=1
'
An cos nπct
L
+ Bn sin nπct
L
(
sin nπx
L .
Verify by direct substitution that u = Fk(x, t) is a solution to (8.10.1) and (8.10.2).
In Part 7, you have shown that any ﬁnite linear combination of solutions of the
form (8.10.8) is also a solution to (8.10.1) and (8.10.2). However, in order to satisfy
the initial conditions (8.10.3), we must now take a leap of faith and consider taking
a linear combination of all of the eigenfunctions and set
u(x, t) =
∞
?
n=1
un(x, t) =
∞
?
n=1
'
An cos nπct
L
+ Bn sin nπct
L
(
sin nπx
L .
For the remainder of this project we will ignore convergence issues associated
with taking such an inﬁnite sum, and proceed in the knowledge that under suitable
conditions all of our steps can be rigorously justiﬁed.
8. (Satisfying the initial conditions) In order to determine the solution for the string
problem, we need to choose the constants An and Bn so that the initial conditions
given in (8.10.3) are satisﬁed. Show that these conditions lead to the following
equations for determining An and Bn:
∞
?
n=1
An sin nπx
L
= f (x),
∞
?
n=1
nBn sin nπx
L
= L
πc g(x).
(8.10.9)
9. (Orthogonality of the eigenfuctions) Use the function inner product
⟨Xm, Xn⟩=
& L
0
Xm(x) · Xn(x) dx
to establish that the set of eigenfuctions
$
sin nπx
L
%∞
n=1 is an orthogonal set on the
interval [0, L], that is,
⟨Xm, Xn⟩= 0,
m ̸= n.
Also verify that ||Xn(x)||2 = L/2.
10. Nowwecanreturntothedeterminationofthe An and Bn.Multiplytheﬁrstequation
in (8.10.9) by sin mπx
L
, integrate the resulting equation over the interval [0, L]
(assume it is valid to switch the integral and inﬁnite sum), and use the results of
Part 8 to conclude that
An = 2
L
& L
0
f (x) sin nπx
L
dx.
Use a similar strategy to establish that
Bn =
2
nπc
& L
0
g(x) sin nπx
L
dx.

8.10
Chapter Review 579
To summarize, we have shown that the solution to the IBVP (8.10.1), (8.10.2),
(8.10.3) is
u(x, t) =
∞
?
n=1
'
An cos nπct
L
+ Bn sin nπct
L
(
sin nπx
L
(8.10.10)
where
An = 2
L
& L
0
f (x) sin nπx
L
dx
(8.10.11)
and
Bn =
2
nπc
& L
0
g(x) sin nπx
L
dx.
(8.10.12)
11. Consider the particular case of the ﬁxed ﬁnite string of length 1 unit that is governed
by the following IBVP:
∂2u
∂t2 −∂2u
∂x2 = 0
u(0, t) = 0,
u(1, t) = 0
t > 0
u(x, 0) = f (x)
=
⎧
⎪⎨
⎪⎩
0, 0 ≤x ≤1/3
4(3x −1)(2 −3x), 1/3 ≤x ≤2/3,
0, 2/3 ≤x ≤1
∂u
∂t (x, 0) = 0,
0 < x < 1.
(a) Sketch the initial proﬁle of the string.
(b) Use (8.10.10)–(8.10.12) to derive the following Fourier series solution to the
IBVP:
u(x, t) = 48
π3
∞
?
k=0
(−1)k+1
(2k + 1)3
/
(2k + 1)π cos (2k + 1)π
6
−6 sin (2k + 1)π
6
0
sin(2k + 1)πx cos(2k + 1)πt
(8.10.13)
(c) Finally, consider the ﬁve-term approximation to (8.10.13) given by the partial
sum
S5(x, t) = 48
π3
4
?
k=0
(−1)k+1
(2k + 1)3
/
(2k + 1)π cos (2k + 1)π
6
−6 sin (2k + 1)π
6
0
sin(2k + 1)πx cos(2k + 1)πt
Use some form of technology to plot the following approximations to the
solution to the IBVP:
S5(x, 0), S5(x, 1/3), S5(x, 1/2), S5(x, 2/3), S5(x, 1), S5(x, 4/3),
S5(x, 3/2), S5(x, 5/3), S5(x, 3/2),
and use your plots to describe the displacement of the string as a function
of time.

9
Systems of Differential
Equations
In practice, most applied problems involve more than one unknown function for their
formulation and hence require the solution to a system of differential equations. Perhaps
the simplest way to see how systems naturally arise is to consider the motion of an
object in space. If this object has mass m and is moving under the inﬂuence of a force
F(t) = (F1(t), F2(t), F3(t)), then, according to Newton’s second law of motion, the
position of the object at time t, (x(t), y(t), z(t)), is obtained by solving the system
m d2x
dt2 = F1,
m d2y
dt2 = F2,
m d2z
dt2 = F3.
In this chapter, we consider the formulation and solution of systems of differential
equations. The majority of the chapter is concerned with linear systems of differential
equations. In this case, the following familiar questions need addressing:
Question 1: How can we formulate problems in a way suitable for solution?
Question 2: How many solutions, if any, does our linear system of differential equa-
tions possess?
Question 3: How do we ﬁnd the solutions that arise in Question 2?
For Question 1, we have already seen some examples in Chapters 2 and 3 of how to
formulate applied problems in a fruitful way by using vectors and matrices. Answering
Question 2 will once more require the vector space techniques from Chapters 4 through
7, whereas, in the case when our linear systems have constant coefﬁcients, we will ﬁnd an
elegant answer to Question 3 using eigenvalues and eigenvectors of appropriate matrices.
Before beginning the general development of the theory for systems of differential
equations, we consider two physical problems that can be formulated mathematically in
terms of such systems.
580

581
Consider the coupled spring-mass system that consists of two masses m1, m2 con-
nected by two springs whose spring constants are k1 and k2, respectively. (See Fig-
ure 9.0.1.)
k1
k2
m1
m2
x 5 0
y 5 0
x(t)
y(t)
Positive x, y
Figure 9.0.1: A coupled spring-mass system.
Let x(t) and y(t) denote the displacement of m1 and m2, respectively, from their
positions when the system is in the static equilibrium position. Then, using Hooke’s law
and Newton’s second law, it follows that the motion of the masses is governed by the
system of differential equations
m1
d2x
dt2 = −k1x + k2(y −x),
m2
d2y
dt2 = −k2(y −x).
We would expect the problem to have a unique solution once we have speciﬁed the initial
positions and velocities of the masses.
As a second example, consider the mixing problem depicted in Figure 9.0.2. Two
tanks contain a solution consisting of chemical dissolved in water. A solution containing
c g/L of the chemical ﬂows into tank 1 at a rate of r L/min, and the solution in tank 2
ﬂows out at the same rate. In addition, the solution ﬂows into tank 1 from tank 2 at a rate
of r12 L/min and into tank 2 from tank 1 at a rate of r21 L/min.
r
c
A1
A2
r21
rout
r12
Tank 2
Tank 1
Figure 9.0.2: A mixing problem.

582
CHAPTER 9
Systems of Differential Equations
We wish to determine the amounts of chemical A1(t) and A2(t) in tanks 1 and 2 at
any time t. A similar analysis to that used in Section 1.7 yields the following system of
differential equations governing the behavior of A1 and A2:
d A1
dt
= −r21
V1
A1 + r12
V2
A2 + cr,
d A2
dt
= r21
V1
A1 −(r12 + r)
V2
A2,
where V1 and V2 denote the volume of solution in each tank at time t.
We will give a full discussion of both of the foregoing problems in Section 9.7 once
we have developed the theory and solution techniques for linear systems of differential
equations.
9.1
First-Order Linear Systems
We ﬁrst focus our attention on linear systems of differential equations, sometimes called
linear differential systems. Once such a system has been appropriately formulated,
vectorspacemethodscanbeappliedtoderivethecompletetheoryregardingtheirsolution
properties.
DEFINITION
9.1.1
A system of differential equations of the form
dx1
dt = a11(t)x1(t) + a12(t)x2(t) + · · · + a1n(t)xn(t) + b1(t),
dx2
dt = a21(t)x1(t) + a22(t)x2(t) + · · · + a2n(t)xn(t) + b2(t),
...
dxn
dt = an1(t)x1(t) + an2(t)x2(t) + · · · + ann(t)xn(t) + bn(t),
(9.1.1)
where the ai j(t) and bi(t) are speciﬁed functions on an interval I, is called a ﬁrst-
order linear system. If b1 = b2 = · · · = bn = 0, then the system is called homoge-
neous. Otherwise, it is called nonhomogeneous.
Remarks
1. It is important to notice the structure of a ﬁrst-order linear system. The highest
derivative occurring in such a system is a ﬁrst derivative. Further, there is precisely
one equation involving the derivative of each separate unknown function. Finally,
the terms that appear on the right-hand side of the equations do not involve any
derivatives and are linear in the unknown functions x1, x2, . . . , xn.
2. We will usually denote dxi
dt by x′
i.
Example 9.1.2
An example of a nonhomogeneous ﬁrst-order linear system is
x′
1 = etx1 + t2x2 + sin t,
x′
2 = tx1 + 3x2 −cos t.

9.1
First-Order Linear Systems 583
The associated homogeneous system is
x′
1 = etx1 + t2x2,
x′
2 = tx1 + 3x2.
□
DEFINITION
9.1.3
By a solution to the system (9.1.1) on an interval I we mean an ordered n-tuple
of functions x1(t), x2(t), . . . , xn(t), which, when substituted into both sides of the
system, yield the same result for all t in I.
Example 9.1.4
Verify that
x1(t) = −2e5t + 4e−t,
x2(t) = e5t + e−t
(9.1.2)
is a solution to the linear system of differential equations
x′
1 =
x1 −8x2,
(9.1.3)
x′
2 = −x1 + 3x2,
(9.1.4)
on (−∞, ∞).
Solution:
From (9.1.2) it follows that the left-hand side of Equation (9.1.3) is
x′
1(t) = −10e5t −4e−t,
whereas the right-hand side is
x1(t) −8x2(t) = (−2e5t + 4e−t) −8(e5t + e−t) = −10e5t −4e−t.
Consequently,
x′
1 = x1 −8x2,
so that Equation (9.1.3) is satisﬁed by the given functions for all t ∈(−∞, ∞). Similarly,
it is easily shown that, for all t ∈(−∞, ∞),
x′
2 = −x1 + 3x2,
so that Equation (9.1.4) is also satisﬁed. It follows that x1 and x2 do deﬁne a solution to
the given system on (−∞, ∞).
□
We now derive a simple technique for solving the system (9.1.1) that can be used
when the coefﬁcients ai j(t) in the system are constants. Although we will develop a
superior technique for such systems in the later sections, the method introduced here
does have importance and will be useful in motivating some of the subsequent results.
For simplicity, we will only consider n = 2. Under the assumption that all ai j are
constants, the system (9.1.1) reduces to
x′
1 = a11x1 + a12x2 + b1(t),
x′
2 = a21x1 + a22x2 + b2(t).
This system can be written in the equivalent form
(D −a11)x1−
a12x2 = b1(t),
(9.1.5)
−a21x1 + (D −a22)x2 = b2(t),
(9.1.6)

584
CHAPTER 9
Systems of Differential Equations
where D is the differential operation d/dt. The idea behind the solution technique is that
we can now easily eliminate x2 between these two equations by operating on Equation
(9.1.5) with D −a22, multiplying Equation (9.1.6) by a12, and adding the resulting
equations. This yields a second-order constant coefﬁcient linear differential equation
for x1 only, which can be solved using the techniques of Chapter 8. Substituting the
expression thereby obtained for x1 into Equation (9.1.5) will then yield x2.1 We illustrate
the technique with an example.
Example 9.1.5
Solve the system
x′
1 =
x1 + 2x2,
(9.1.7)
x′
2 = 2x1 −2x2.
(9.1.8)
Solution:
We begin by rewriting the system in operator form as
(D −1)x1 −
2x2 = 0,
(9.1.9)
−2x1 + (D + 2)x2 = 0.
(9.1.10)
To eliminate x2 between these two equations, we ﬁrst operate on Equation (9.1.9) with
D + 2 to obtain
(D + 2)(D −1)x1 −2(D + 2)x2 = 0.
Adding twice Equation (9.1.10) to this equation eliminates x2 and yields
(D + 2)(D −1)x1 −4x1 = 0.
That is,
(D2 + D −6)x1 = 0.
This constant coefﬁcient differential equation has auxiliary polynomial
P(r) = r2 + r −6 = (r + 3)(r −2).
Consequently,
x1(t) = c1e−3t + c2e2t.
(9.1.11)
We now determine x2. From Equation (9.1.9), we have
x2(t) = 1
2(D −1)x1.
Inserting the expression for x1 from (9.1.11) into the previous equation yields
x2(t) = 1
2(Dx1 −x1) = 1
2(−4c1e−3t + c2e2t).
Hence, the solution to the system of differential equations (9.1.7) and (9.1.8) is
x1(t) = c1e−3t + c2e2t,
x2(t) = 1
2(−4c1e−3t + c2e2t),
where c1 and c2 are arbitrary constants.
□
1If a12 = 0, we can determine x1 directly from Equation (9.1.5), and then x2 can be determined from
Equation (9.1.6).

9.1
First-Order Linear Systems 585
In solving an applied problem that is governed by a system of differential equations,
we usually require the particular solution to the system that corresponds to the speciﬁc
problem of interest. Such a particular solution is obtained by specifying appropriate
auxiliary conditions. This leads to the idea of an initial-value problem for linear systems.
DEFINITION
9.1.6
Solving the system (9.1.1) subject to n auxiliary conditions imposed at the same value
of the independent variable is called an initial-value problem. Thus, the general form
of the auxiliary conditions for an initial-value problem is:
x1(t0) = α1,
x2(t0) = α2,
. . . ,
xn(t0) = αn,
where α1, α2, . . . , αn are constants.
Example 9.1.7
Solve the initial-value problem
x′
1 = x1 + 2x2,
x′
2 = 2x1 −2x2,
x1(0) = 1,
x2(0) = 0.
Solution:
We have already seen in the previous example that the solution to the given
system of differential equations is
x1(t) = c1e−3t + c2e2t,
x2(t) = 1
2(−4c1e−3t + c2e2t),
(9.1.12)
where c1 and c2 are arbitrary constants. Imposing the two initial conditions yields the
following equations for determining c1 and c2:
c1 + c2 = 1,
−4c1 + c2 = 0.
Consequently,
c1 = 1
5
and c2 = 4
5.
Substituting for c1 and c2 into (9.1.12) yields the unique solution
x1(t) = 1
5(e−3t + 4e2t),
x2(t) = 2
5(e2t −e−3t).
□
It might appear that restricting to ﬁrst-order linear systems means that we are only
considering very special types of linear differential equations. In fact this is incorrect,
since most systems of k differential equations that are linear in k unknown functions
and their derivatives can be rewritten as equivalent ﬁrst-order systems by redeﬁning the
dependent variables. We illustrate with an example.
Example 9.1.8
Rewrite the linear system
d2x
dt2 −
4y = et
(9.1.13)
d2y
dt2 + t2 dx
dt = sin t,
(9.1.14)
as an equivalent ﬁrst-order system.

586
CHAPTER 9
Systems of Differential Equations
Solution:
We introduce new dependent variables relative to which Equations (9.1.13)
and (9.1.14) reduce to ﬁrst-order differential equations. Let
x1 = x,
x2 = dx
dt ,
x3 = y,
x4 = dy
dt .
(9.1.15)
Then Equations (9.1.13) and (9.1.14) can be replaced by
dx2
dt −4x3 = et,
dx4
dt + t2x2 = sin t.
These equations must also be supplemented with equations for x1 and x3. From (9.1.15),
we see that
dx1
dt = x2,
dx3
dt = x4.
Consequently, the given system of differential equations is equivalent to the ﬁrst-order
linear system
dx1
dt = x2,
dx2
dt = 4x3 + et,
dx3
dt = x4,
dx4
dt = −t2x2 + sin t.
□
Finally, consider the general nth-order linear differential equation
x(n) + a1(t)x(n−1) + · · · + an−1(t)x′ + an(t)x = F(t).
(9.1.16)
If we introduce the new variables x1, x2, . . . , xn deﬁned by
x1 = x,
x2 = x′,
. . . ,
xn = x(n−1),
then Equation (9.1.16) can be replaced by the equivalent ﬁrst-order linear system
x′
1 = x2,
x′
2 = x3,
. . . ,
x′
n−1 = xn,
x′
n = −an(t)x1 −an−1(t)x2 −· · · −a1(t)xn + F(t).
Consequently, any nth-order linear differential equation can be replaced by an equivalent
system of ﬁrst-order differential equations.
Example 9.1.9
Write the following differential equation as an equivalent ﬁrst-order system:
d2x
dt2 + 4et dx
dt −9t2x = 7t2.
Solution:
We introduce new variables x1 and x2 deﬁned by
x1 = x,
x2 = dx
dt .
Then the given differential equation can be replaced by the ﬁrst-order system
dx1
dt = x2,
dx2
dt = 9t2x1 −4etx2 + 7t2.
□

9.1
First-Order Linear Systems 587
Exercises for 9.1
Key Terms
First-order linear system, Homogeneous linear system, Non-
homogeneous linear system, Solution to a ﬁrst-order linear
system, Initial-value problem.
Skills
• Be able to use differential operators to solve a ﬁrst-
order linear system of differential equations.
• Be able to use differential operators together with ini-
tial conditions to solve an initial-value problem con-
sisting of a ﬁrst-order linear system.
• Be able to convert higher order linear systems of dif-
ferential equations into a ﬁrst-order linear system by
introducing new variables.
• Be able to convert a ﬁrst-order linear system of differ-
ential equations into a single higher-order differential
equation that can be solved by the techniques of the
previous chapter.
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The system of differential equations x′ = e2tx +e−t y,
y′ = tx + 5(cos t)y is a ﬁrst-order linear system of
differential equations.
(b) Thesystemofdifferentialequations x′ = t4x−et y+4,
y′ = (t2 + 3)x −t2 is a ﬁrst-order linear system of
differential equations.
(c) The system of differential equations x′
= ty +
5(sin t)y −3et, y′ = t2x −7xy −1 is a ﬁrst-order
linear system of differential equations.
(d) The system of differential equations x′ = −2x, x′ =
eyx + et y, is a ﬁrst-order linear system of differential
equations.
(e) A third-order linear differential equation can be re-
placed by a ﬁrst-order linear system consisting of three
differential equations.
(f) A ﬁrst-order linear system of two differential equa-
tions can be solved by converting it into a second-order
linear differential equation.
(g) The ﬁrst-order linear system x′ = x + y, y′ = x −y,
with auxiliary conditions x(0) = 0, y(1) = 1 is an
initial-value problem.
(h) The ﬁrst-order linear system x′ = −2x −3y, y′ =
5x −y, with auxiliary conditions x(0) = 2, y(0) = 6
is an initial-value problem.
(i) The ﬁrst-order linear system x′ = etx −t3y, y′ =
4x −5t2y, with auxiliary condition x(2) = 5, is an
initial-value problem.
(j) The ﬁrst-order linear system x′ = t2x +3y, y′ = 3x −
ty, with auxiliary condition x(3) = 7, y(−3) = −7 is
an initial-value problem.
Problems
For Problems 1–8, solve the given system of differential
equations.
1. x′
1 = 2x1 + x2,
x′
2 = 2x1 + 3x2.
2. x′
1 = 2x1 −3x2,
x′
2 = x1 −2x2.
3. x′
1 = 4x1 + 2x2,
x′
2 = −x1 + x2.
4. x′
1 = 2x1 + 4x2,
x′
2 = −4x1 −6x2.
5. x′
1 = 2x2,
x′
2 = −2x1.
6. x′
1 = x1 −3x2,
x′
2 = 3x1 + x2.
7. x′
1 = 2x1,
x′
2 = x2 −x3,
x′
3 = x2 + x3.
8. x′
1 = −2x1 + x2 + x3,
x′
2 = x1 −x2 + 3x3,
x′
3 = −x2 −3x3.
For Problems 9–11, solve the given initial-value problem.
9. x′
1 = 2x2,
x′
2 = x1 + x2,
x1(0) = 3,
x2(0) = 0.
10. x′
1 = 2x1 + 5x2,
x′
2 = −x1 −2x2,
x1(0) = 0,
x2(0) = 1.
11. x′
1 = 2x1 + x2,
x′
2 = −x1 + 4x2,
x1(0) = 1,
x2(0) = 3.
For Problems 12–14, solve the given nonhomogeneous
system.
12. x′
1 = x1 + 2x2 + 5e4t,
x′
2 = 2x1 + x2.
13. x′
1 = −2x1 + x2 + t,
x′
2 = −2x1 + x2 + 1.
14. x′
1 = x1 + x2 + e2t,
x′
2 = 3x1 −x2 + 5e2t.

588
CHAPTER 9
Systems of Differential Equations
For Problems 15–16, convert the given system of differential
equations to a ﬁrst-order linear system.
15. dx
dt −ty = cos t,
d2y
dt2 −dx
dt + x = et.
16. d2x
dt2 −3dy
dt + x = sin t,
d2y
dt2 −t dx
dt −et y = t2.
For Problems 17–19, convert the given linear differential
equations to a ﬁrst-order linear system.
17. y′′ + 2ty′ + y = cos t.
18. y′′ + ay′ + by = F(t),
a, b constants.
19. y′′′ + t2y′ −et y = t.
20. The initial-value problem that governs the behavior of
a coupled spring-mass system is (see the introduction
to this chapter)
m1
d2x
dt2 = −k1x + k2(y −x),
m2
d2y
dt2 = −k2(y −x).
x(0) = α1,
x′(0) = α2,
y(0) = α3,
y′(0) = α4,
where α1, α2, α3, and α4 are constants. Convert this
problem into an initial-value problem for an equiva-
lent ﬁrst-order linear system. (You must give the ap-
propriate initial conditions in the new variables.)
21. Solve the initial-value problem:
x′
1 = −(tan t)x1 + 3 cos2 t,
x′
2 = x1 + (tan t)x2 + 2 sin t,
x1(0) = 4,
x2(0) = 0.
9.2
Vector Formulation
The ﬁrst step in developing the general theory for ﬁrst-order linear systems is to formulate
the problem of solving such a system as an appropriate vector space problem. The key
to this formulation is the realization that the scalar system of equations
x′
1 = a11(t)x1(t) + a12(t)x2(t) + · · · + a1n(t)xn(t),
x′
2 = a21(t)x1(t) + a22(t)x2(t) + · · · + a2n(t)xn(t),
...
x′
n = an1(t)x1(t) + an2(t)x2(t) + · · · + ann(t)xn(t),
(9.2.1)
can be written as the equivalent vector equation
x′(t) = A(t)x(t) + b(t),
(9.2.2)
where
x(t) =
⎡
⎢⎢⎢⎣
x1(t)
x2(t)
...
xn(t)
⎤
⎥⎥⎥⎦,
x′(t) =
⎡
⎢⎢⎢⎣
x′
1(t)
x′
2(t)
...
x′
n(t)
⎤
⎥⎥⎥⎦,
and
A(t) =
⎡
⎢⎢⎢⎣
a11(t)
a12(t)
. . .
a1n(t)
a21(t)
a22(t)
. . .
a2n(t)
...
...
...
an1(t)
an2(t)
. . .
ann(t)
⎤
⎥⎥⎥⎦,
b(t) =
⎡
⎢⎢⎢⎣
b1(t)
b2(t)
...
bn(t)
⎤
⎥⎥⎥⎦.

9.2
Vector Formulation 589
Notice that x, x′, and b in (9.2.2) are column n-vector functions. We let Vn(I) denote
the set of all column n-vector functions deﬁned on an interval I, and deﬁne addition
and scalar multiplication within this set in the same manner as for column vectors. The
following result concerning Vn(I) will be needed in the remaining sections:
Theorem 9.2.1
The set Vn(I) is a vector space.
Proof Verifying that Vn(I) together with the operations of addition and scalar multi-
plication just deﬁned satisﬁes Deﬁnition 4.2.1 is left as an exercise (Problem 10).
Since Vn(I) is a vector space, we can discuss linear dependence and linear indepen-
dence of column vector functions. We ﬁrst need a deﬁnition.
DEFINITION
9.2.2
Let x1(t), x2(t), . . . , xn(t) be vectors in Vn(I). Then the Wronskian of these vector
functions, denoted W[x1, x2, . . . , xn](t), is deﬁned by
W[x1, x2, . . . , xn](t) = det([x1(t), x2(t), . . . , xn(t)]).
Remark
Notice that the Wronskian introduced in this deﬁnition refers to column
vector functions in the vector space Vn(I), whereas the Wronskian deﬁned previously
in the text refers to functions in Cn(I). The relationship between these two Wronskians
is investigated in Problem 13.
Example 9.2.3
Determine the Wronskian of the column vector functions
x1(t) =
' et
2et
(
,
x2(t) =
'3 sin t
cos t
(
.
Solution:
From Deﬁnition 9.2.2, we have
W[x1, x2](t) =
et
3 sin t
2et
cos t
= et(cos t −6 sin t).
□
Our next theorem indicates that the Wronskian plays a familiar role in determining
the linear independence of a set of vectors in Vn(I).
Theorem 9.2.4
Let x1(t), x2(t), . . . , xn(t) be vectors in Vn(I). If W[x1, x2, . . . , xn](t0) is nonzero at
some point t0 in I, then {x1(t), x2(t), . . . , xn(t)} is linearly independent on I.
Proof Consider
c1x1(t) + c2x2(t) + · · · + cnxn(t) = 0,
where c1, c2, . . . , cn are scalars. Using Theorem 2.2.9, we can write this as the vector
equation
X(t)c = 0,

590
CHAPTER 9
Systems of Differential Equations
where c = [c1 c2 . . . cn]T and X(t) = [x1(t), x2(t), . . . , xn(t)]. Let t0 be in I. If
we assume that det([X(t0)]) = W[x1, x2, . . . , xn](t0) ̸= 0, Corollary 3.2.6 implies
that the only solution to this n × n system of linear equations is c = 0. Consequently,
{x1(t), x2(t), . . . , xn(t)} is linearly independent on I, as required.
Example 9.2.5
The vector functions
x1(t) =
' et
2et
(
,
x2 =
'3 sin t
cos t
(
are linearly independent on (−∞, ∞) since W[x1, x2](t) = et(cos t−6 sin t) is nonzero,
for example, when t = 0.
□
Example 9.2.6
Given an n × n matrix function A(t), the function T : Vn(I) →Vn(I) deﬁned by
T (x(t)) = A(t)x(t)
is a linear transformation. To see this, let x(t) and y(t) be column n-vector functions,
and let c be a scalar. For clarity, we suppress the variable t from the functions A, x, and
y in the calculations below. We have
T (x + y) = A[x + y] = Ax + Ay = T (x) + T (y)
and
T (c x) = A[c x] = c[Ax] = c T (x).
Likewise, the reader can verify that the function D : Vn(I) →Vn(I) deﬁned by
D(x(t)) = x′(t)
is a linear transformation.
□
Vector Differential Equations
A system of linear differential equations written in the vector form
x′(t) = A(t)x(t) + b(t)
will be called a vector differential equation. We emphasize that within this formu-
lation the primary unknown is the column vector function x(t) whose components,
x1(t), x2(t), . . . , xn(t) are the unknowns in the corresponding linear system (9.2.1). The
problem of determining all solutions to the general ﬁrst-order linear system of differential
equations (9.2.1) can now be formulated as the vector space problem
Find all column vector functions x(t) ∈Vn(I) satisfying the vector differential
equation
x′(t) = A(t)x(t) + b(t).
The vector space Vn(I) is not ﬁnite-dimensional, since there is no ﬁnite set of linearly
independent vectors that span Vn(I). The key to solving linear differential systems comes
from the realization that, if A(t) is an n × n matrix function, then the set of all solutions

9.2
Vector Formulation 591
to the homogeneous vector differential equation
x′(t) = A(t)x(t)
is an n-dimensional subspace of Vn(I). This is illustrated in the next example and estab-
lished in general in the next section.
Example 9.2.7
Consider the homogeneous linear system of differential equations
x′
1 =
x1 + 2x2
x′
2 = 2x1 −2x2.
In Example 9.1.5, we derived the following solution to this scalar system:
x1(t) = c1e−3t + c2e2t,
x2(t) = 1
2(−4c1e−3t + c2e2t).
We can reformulate this problem as an equivalent vector differential equation:
x′ = Ax,
A =
'1
2
2
−2
(
.
(9.2.3)
The solution vector to the vector differential equation (9.2.3) is
x(t) =
'
c1e−3t + c2e2t
1
2(−4c1e−3t + c2e2t)
(
,
which can be written in the equivalent form
x(t) = c1
' e−3t
−2e−3t
(
+ c2
' e2t
1
2e2t
(
.
Consequently, in this particular example, the set of all solutions to the vector dif-
ferential equation is the two-dimensional subspace of V2(I) spanned by the linearly
independent column vector functions2
x1(t) =
' e−3t
−2e−3t
(
,
x2(t) =
' e2t
1
2e2t
(
.
□
Exercises for 9.2
Key Terms
Vector formulation of a linear system, Wronskian of vector
functions, Vector differential equation.
Skills
• Be able to write a system of ﬁrst-order linear differ-
ential equations as an equivalent vector differential
equation.
• Be able to determine the Wronskian of a collection of
column vector functions.
• Understand how the Wronskian of a collection of col-
umn vector functions relates to the linearly indepen-
dence of those functions.
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
2The linear independence can be readily veriﬁed by using the Wronskian introduced in this section.

592
CHAPTER 9
Systems of Differential Equations
(a) In the vector equation x′(t) = A(t)x(t) + b(t), the
matrix function A(t) must always be a square matrix.
(b) The Wronskians W[x1, x2](t) and W[x2, x1](t) are
the same.
(c) Three column vector functions in V2(I) must be lin-
early dependent.
(d) In order for a set of n column vector functions in Vn(I)
tobelinearlyindependent,theWronskianofthesevec-
tor functions must be nonzero for all x in I.
(e) If A is a 2 × 2 matrix of constants whose deter-
minant is zero, then the vector differential equation
x′(t) = Ax(t) cannot have two linearly independent
solutions.
(f) A single fourth-order linear differential equation can
be rewritten as a 4 × 4 linear system of differential
equations.
(g) If x0(t) is a solution to the homogeneous vector differ-
ential equation x′(t) = A(t)x(t), then x0(t) + b(t) is
a solution to the nonhomogeneous vector differential
equation x′(t) = A(t)x(t) + b(t).
Problems
For Problems 1–5, show that the given vector functions are
linearly independent on (−∞, ∞).
1. x1(t) =
' et
−et
(
, x2(t) =
'et
et
(
.
2. x1(t) =
't
t
(
, x2(t) =
' t
t2
(
.
3. x1(t) =
⎡
⎣
t + 1
t −1
2t
⎤
⎦, x2(t) =
⎡
⎣
et
e2t
e3t
⎤
⎦,
x3(t) =
⎡
⎣
1
sin t
cos t
⎤
⎦.
4. x1(t) =
't
t
(
, x2(t) =
'|t|
t
(
.
Is there an interval on which x1(t) and x2(t) in this exercise
are not linearly independent?
5. x1(t) =
⎡
⎣
sin t
cos t
1
⎤
⎦,
x2(t) =
⎡
⎣
t
1 −t
1
⎤
⎦,
x3(t) =
⎡
⎣
sinh t
cosh t
1
⎤
⎦.
For Problems 6–9, show that the given vector functions are
linearly dependent on (−∞, ∞).
6. x1(t) =
' et
2e2t
(
,
x2(t) =
' 4et
8e2t
(
.
7. x1(t) =
'
t2
6 −t + t3
(
,
x2(t) =
'
−3t2
−18t + 3t2 −3t3
(
.
8. x1(t) =
⎡
⎣
t
t2
−t3
⎤
⎦,
x2(t) =
⎡
⎣
2t
3t2
0
⎤
⎦,
x3(t) =
⎡
⎣
−t
0
3t3
⎤
⎦.
9. x1(t) =
⎡
⎣
sin2 t
cos2 t
2
⎤
⎦,
x2(t) =
⎡
⎣
2 cos2 t
2 sin2 t
1
⎤
⎦,
x3(t) =
⎡
⎣
2
2
5
⎤
⎦.
10. Prove that Vn(I) is a vector space.
11. Let A(t) be an n × n matrix function. Prove that the
set of all solutions x to the system x′(t) = A(t)x(t) is
a subspace of Vn(I).
12. If A =
'2
−4
1
−3
(
, determine two linearly independent
solutions to x′ = Ax on (−∞, ∞).
Problem 13 investigates the relationship between the
Wronskian deﬁned in this section for column vector func-
tions in Vn(I) and the Wronskian deﬁned previously for
functions in Cn(I).
13. Consider the differential equation
d2y
dt2 + a dy
dt + by = 0,
(9.2.4)
where a and b are arbitrary functions of t.
(a) Show that Equation (9.2.4) can be replaced by the
equivalent linear system
x′ = Ax,
(9.2.5)

9.3
General Results for First-Order Linear Differential Systems 593
where
A =
' 0
1
−b
−a
(
and
x1 = y, x2 = y′.
(b) If y1 = f1(t) and y2 = f2(t) are solutions to
Equation (9.2.4) on an interval I, show that the
corresponding solutions to the system (9.2.5) are
x1(t) =
' f1(t)
f ′
1(t)
(
,
x2(t) =
' f2(t)
f ′
2(t)
(
.
(c) Show that
W[x1, x2](t) = W[y1, y2](t).
9.3
General Results for First-Order Linear Differential Systems
We now show how the formulation of a linear system of differential equations as a
single vector differential equation enables us to derive the underlying theory for linear
differential systems as an application of the vector space results from Chapter 4. We
emphasize that although the derivation of the results is based on the vector differential
equation formulation, the results themselves apply to any ﬁrst-order linear system, since
such a system can always be formulated as a vector differential equation.
The fundamental theoretical result that will be used in deriving the underlying theory
for the solution of vector differential equations is the following existence and uniqueness
theorem:
Theorem 9.3.1
The initial-value problem
x′(t) = A(t)x(t) + b(t),
x(t0) = x0,
where A(t) and b(t) are continuous on an interval I, has a unique solution on I.
Proof The proof is omitted. (See, for example, F.J. Murray and K.S. Miller, Existence
Theorems, New York University Press, 1954.)
Homogeneous Vector Differential Equations
Just as for a single nth-order linear differential equation, the solution to a nonhomo-
geneous linear differential system can, in theory, be obtained once we have solved the
associated homogeneous differential system. Consequently, we begin by developing the
theory for homogeneous vector differential equations:
x′(t) = A(t)x(t),
(9.3.1)
where A is an n × n matrix function. This is where the vector space techniques are
required. We ﬁrst show that the set of all solutions to (9.3.1) is an n-dimensional subspace
of the vector space of all column n-vector functions.
Theorem 9.3.2
The set of all solutions to x′(t) = A(t)x(t), where A(t) is an n × n matrix function that
is continuous on an interval I, is a vector space of dimension n.
Proof Let S denote the set of all solutions to x′ = A(t)x(t). By Example 9.2.6, the
functions T (x) = Ax and D(x) = x′ are linear transformations, and hence, so is
(D −T )(x) = x′ −Ax.
Therefore, since S is simply the kernel of the linear transformation D−T , it is a subspace
of Vn(I) by Theorem 6.3.5.

594
CHAPTER 9
Systems of Differential Equations
We now prove that the dimension of S is n by constructing a basis for S containing
n vectors. We ﬁrst show that there exist n linearly independent solutions to x′ = Ax. Let
ei denote the ith column vector of the identity matrix In. Then, from Theorem 9.3.1, for
every t0 in I and every i, the initial-value problem
⎧
⎨
⎩
x′
i(t) = A(t)xi(t),
i = 1, 2, . . . , n
xi(t0) = ei,
has a unique solution xi(t). Further, W[x1, x2, . . . , xn](t0) = det(In) = 1 ̸= 0 for any
t0 in I, so that {x1(t), x2(t), . . . , xn(t)} is linearly independent on I. Next we establish
that these solutions span the solution space. Let x(t) be any real solution to x′ = Ax on
I. Then, since {x1(t0), x2(t0), . . . , xn(t0)} is the standard basis for Rn, we can write
x(t0) = c1x1(t0) + c2x2(t0) + · · · + cnxn(t0)
for some scalars c1, c2, . . . , cn. It follows that x(t) is the unique solution to the initial-
value problem
,x′(t) = A(t)x(t),
x(t0) = c1x1(t0) + c2x2(t0) + · · · + cnxn(t0),
(9.3.2)
by Theorem 9.3.1. But
u(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t)
also satisﬁes the initial-value problem (9.3.2), and so, by uniqueness, we must have
x(t) = u(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t).
We have therefore shown that any solution to x′ = Ax on I can be written as a linear
combination of the n linearly independent solutions x1(t), x2(t), . . . , xn(t), and hence,
{x1(t), x2(t), . . . , xn(t)} is a basis for the solution space. Consequently, the dimension
of the solution space is n.
It follows from Theorem 9.3.2 that if {x1, x2, . . . , xn} is any set of n linearly inde-
pendent solutions to (9.3.1), then every solution to the system can be written as
x(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t),
(9.3.3)
for appropriate constants c1, c2, . . . , cn. In keeping with the terminology that we have
used throughout the text, we will refer to (9.3.3) as the general solution to the vector
differential equation (9.3.1).
The following deﬁnition introduces some important terminology for homogeneous
vector differential equations.
DEFINITION
9.3.3
Let A(t) be an n × n matrix function that is continuous on an interval I. Any set of
n solutions, {x1, x2, . . . , xn}, to x′ = Ax that is linearly independent on I is called a
fundamental solution set on I. The corresponding matrix X(t) deﬁned by
X(t) = [x1, x2, . . . , xn]
is called a fundamental matrix for the vector differential equation x′ = Ax.

9.3
General Results for First-Order Linear Differential Systems 595
Remarks
1. Since the vector space of all solutions to x′ = Ax is n-dimensional by Theo-
rem 9.3.2, the fundamental solution set for x′ = Ax is a basis for the space of
solutions to the system.
2. If X(t) is a fundamental matrix for (9.3.1) then, applying Theorem 2.2.9, the
general solution (9.3.3) can be written in vector form as x(t) = X(t)c, where c =
[c1 c2 . . . cn]T . This will be our starting point when the variation-of-parameters
method is derived in Section 9.6.
Now suppose that x1, x2, . . . , xn are solutions to x′ = Ax on an interval I. We have
shown in the previous section that if W[x1, x2, . . . , xn](t) ̸= 0 at some point in I, then
the solutions are linearly independent on I. We now prove a converse statement.
Theorem 9.3.4
Let A(t) be an n×n matrix function that is continuous on an interval I. If {x1, x2, . . . , xn}
is a linearly independent set of solutions to x′ = Ax on I, then
W[x1, x2, . . . , xn](t) ̸= 0
at every point in t in I.
Proof We prove the equivalent statement that if W[x1, x2, . . . , xn](t0) = 0 at some
point t0 in I, then {x1, x2, . . . , xn} is linearly dependent on I. We proceed as fol-
lows. If W[x1, x2, . . . , xn](t0) = 0, then from Corollary 4.5.17, the set of vectors
{x1(t0), x2(t0), . . . , xn(t0)} is linearly dependent in Rn. Thus, there exist scalars
c1, c2, . . . , cn, not all zero, such that
c1x1(t0) + c2x2(t0) + · · · + cnxn(t0) = 0.
(9.3.4)
Now let
x(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t).
(9.3.5)
It follows from Equations (9.3.4) and (9.3.5) and Theorem 9.3.1 that x(t) is the unique
solution to the initial-value problem
x′ = A(t)x(t),
x(t0) = 0.
However, this initial-value problem has the solution x(t) = 0, and so, by uniqueness,
we must have
c1x1(t) + c2x2(t) + · · · + cnxn(t) = 0.
Since not all of the ci are zero, it follows that the set of vector functions {x1, x2, . . . , xn}
is indeed linearly dependent on I.
Thus, to determine whether {x1, x2, . . . , xn} is a fundamental solution set for x′ =
Ax on an interval I, we can compute the Wronskian of x1, x2, . . . , xn at any convenient
point t0 in I. If W[x1, x2, . . . , xn](t0) ̸= 0, then the solutions are linearly independent on
I, whereas if W[x1, x2, . . . , xn](t0) = 0, then the solutions are linearly dependent on I.
Example 9.3.5
Consider the vector differential equation
x′ = Ax,
where
A =
' 1
2
−2
1
(
,
and let
x1(t) =
'−et cos 2t
et sin 2t
(
,
x2(t) =
'et sin 2t
et cos 2t
(
.

596
CHAPTER 9
Systems of Differential Equations
(a) Verify that {x1, x2} is a fundamental set of solutions for the vector differential
equation on any interval, and write the general solution to the vector differential
equation.
(b) Solve the initial-value problem
x′ = Ax,
x(0) =
'3
2
(
,
and write the corresponding scalar solutions.
Solution:
(a) Differentiating the given vector functions with respect to t yields, respectively,
x′
1 =
'et(−cos 2t + 2 sin 2t)
et(sin 2t + 2 cos 2t)
(
,
x′
2 =
'et(sin 2t + 2 cos 2t)
et(cos 2t −2 sin 2t)
(
,
whereas
Ax1 =
' 1
2
−2
1
( '−et cos 2t
et sin 2t
(
=
'et(−cos 2t + 2 sin 2t)
et(sin 2t + 2 cos 2t)
(
,
and
Ax2 =
' 1
2
−2
1
( 'et sin 2t
et cos 2t
(
=
'et(sin 2t + 2 cos 2t)
et(cos 2t −2 sin 2t)
(
.
Hence,
x′
1 = Ax1
and
x′
2 = Ax2,
so that x1 and x2 are indeed solutions to the given vector differential equation.
Furthermore, the Wronskian of these solutions is
W[x1, x2](t) = −et cos 2t
et sin 2t
et sin 2t
et cos 2t
= −e2t.
Since the Wronskian is never zero, it follows that {x1, x2} is linearly independent on
any interval and so is a fundamental set of solutions for the given vector differential
equation. Therefore, the general solution to the system is
x(t) = c1x1(t) + c2x2(t) = c1
'−et cos 2t
et sin 2t
(
+ c2
'et sin 2t
et cos 2t
(
.
Combining the two column vector functions on the right-hand side yields
x(t) =
'et(−c1 cos 2t + c2 sin 2t)
et(c1 sin 2t + c2 cos 2t)
(
.
(b) Imposing the given initial condition x(0) =
'3
2
(
on the general solution above
requires that
'−c1
c2
(
=
'3
2
(
,

9.3
General Results for First-Order Linear Differential Systems 597
so that c1 = −3 and c2 = 2. Hence,
x(t) =
' et(3 cos 2t + 2 sin 2t)
et(−3 sin 2t + 2 cos 2t)
(
.
The corresponding scalar solutions are
x1(t) = et(3 cos 2t + 2 sin 2t),
x2(t) = et(−3 sin 2t + 2 cos 2t).
□
Nonhomogeneous Vector Differential Equations
The preceding results have dealt with the case of a homogeneous vector differential
equation. We end this section with the main theoretical result that will be needed for
nonhomogeneous vector differential equations. In view of our previous experience with
nonhomogeneous linear problems, the following theorem should not be too surprising.
Theorem 9.3.6
Let A(t) be a matrix function that is continuous on an interval I, and let {x1, x2, . . . , xn}
be a fundamental solution set on I for the vector differential equation x′(t) = A(t)x(t).
If xp(t) is any particular solution to the nonhomogeneous vector differential equation
x′(t) = A(t)x(t) + b(t)
(9.3.6)
on I, then every solution to (9.3.6) on I is of the form
x(t) = c1x1 + c2x2 + · · · + cnxn + xp.
Proof Since xp is a solution to x′(t) = A(t)x(t) + b(t) on I, we have
x′
p(t) = A(t)xp(t) + b(t).
(9.3.7)
Now let u(t) be any other solution to x′(t) = A(t)x(t) + b(t) on I. We then also have
u′(t) = A(t)u(t) + b(t).
(9.3.8)
Subtracting (9.3.7) from (9.3.8) yields
(u −xp)′ = A(u −xp).
Thus, the vector function x = u−xp is a solution to the associated homogeneous system
x′ = Ax on I. Since {x1, x2, . . . , xn} spans the solution space of this system, it follows
that
u −xp = c1x1 + c2x2 + · · · + cnxn,
for some scalars c1, c2, . . . , cn. Consequently,
u = c1x1 + c2x2 + · · · + cnxn + xp,
and the result is proved.
Theorem 9.3.6 implies that in order to solve a nonhomogeneous vector differential
equation, we must ﬁrst ﬁnd the general solution to the associated homogeneous system. In
the next two sections, we will concentrate on homogeneous vector differential equations,
and then, in Section 9.6, we will see how the variation-of-parameters technique can be
used to determine a particular solution to a nonhomogeneous vector differential equation.

598
CHAPTER 9
Systems of Differential Equations
Exercises for 9.3
Key Terms
Homogeneous vector differential equations, General solu-
tion to vector differential equations, Fundamental solution
set, Fundamental matrix, Nonhomogeneous vector differen-
tial equations, Particular solution.
Skills
• Know the existence and uniqueness theorem (Theo-
rem 9.3.1) for a vector differential equation together
with an initial condition.
• Understand the theory underlying fundamental solu-
tion sets and fundamental matrices for a given vector
differential equation.
• Be able to use a fundamental solution set to write down
the general solution to a homogeneous vector differ-
ential equation.
• Know the relationship between the Wronskian of n
column n-vector functions that solve x′ = Ax and
their linear dependence/independence.
• Be able to write down the general solution to a non-
homogeneous vector differential equation by using a
fundamental solution set for the corresponding homo-
geneous vector differential equation and a particular
solution to the vector differential equation.
True-False Review
For Questions (a)–(d), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The set of all solutions to the vector differential equa-
tion x′(t) = A(t)x(t) + b(t) (where A(t) is an n × n
matrix function and b(t) ̸= 0) is a vector space of
dimension n.
(b) Any fundamental matrix for the vector differential
equation x′ = Ax is invertible.
(c) A fundamental solution set to a vector differential
equation x′ = Ax forms a spanning set for the vector
space of all solutions to x′ = Ax.
(d) If the general solution to the homogeneous vector dif-
ferential equation x′(t) = A(t)x(t) is xc(t) and xp(t)
is a particular solution to the nonhomogeneous vector
differential equation x′(t) = A(t)x(t) + xp(t), then
the general solution to the nonhomogeneous vector
differential equation is xc(t) + xp(t).
Problems
ForProblems1–7, showthatthegivenfunctionsaresolutions
of the system x′(t) = A(x)x(t) for the given matrix A, and
hence, ﬁnd the general solution to the system (remember to
check linear independence). If auxiliary conditions are given,
ﬁnd the particular solution that satisﬁes these conditions.
1. x1(t) =
'sin 3t
cos 3t
(
, x2(t) =
'−cos 3t
sin 3t
(
,
A =
' 0
3
−3
0
(
.
2. x1(t) =
' e4t
2e4t
(
, x2(t) =
'3e−t
e−t
(
,
A =
'−2
3
−2
5
(
,
x(0) =
'−2
1
(
.
3. x1(t) =
'e−t cos 2t
e−t sin 2t
(
, x2(t) =
'−e−t sin 2t
e−t cos 2t
(
,
A =
'1
−2
2
1
(
, x(0) =
'1
3
(
.
4. x1(t) =
' e2t
−e2t
(
, x2(t) =
'e2t(1 + t)
−te2t
(
,
A =
' 3
1
−1
1
(
.
5. x1(t) =
⎡
⎣
−3
9
5
⎤
⎦, x2(t) =
⎡
⎣
e2t
3e2t
e2t
⎤
⎦,
x3(t) =
⎡
⎣
e4t
e4t
e4t
⎤
⎦,
A =
⎡
⎣
2
−1
3
3
1
0
2
−1
3
⎤
⎦.
6. x1(t) =
'2t
et
(
, x2(t) =
' 0
3et
(
,
A =
'1/t
0
0
1
(
.
7. x1(t) =
't sin t
cos t
(
,
x2(t) =
'−t cos t
sin t
(
,
A =
' 1/t
t
−1/t
0
(
.
8. If x1, x2, . . . , xn are solutions to x′ = A(t)x and
X = [x1, x2, . . . , xn], prove that
X′ = A(t)X.

9.4
Vector Differential Equations: Nondefective Coefficient Matrix 599
9. Let X(t) be a fundamental matrix for x′ = A(t)x on
the interval I.
(a) Show that the general solution to the linear system can
be written as
x = X(t)c,
where c is a vector of constants.
(b) If t0 ∈I, show that the solution to the initial-value
problem
x′ = Ax, x(t0) = x0,
can be written as
x = X(t)X−1(t0)x0.
9.4
Vector Differential Equations: Nondefective Coefficient Matrix
The theory that we have developed in the previous section is valid for any ﬁrst-order
linear system. However, in practice, obtaining a fundamental solution set for a given
vector differential equation x′ = Ax on an interval I can be a difﬁcult task, and so, we
must make a simplifying assumption in order to develop solution techniques applicable
to a broad class of linear systems. The assumption that we will make is that the coefﬁcient
matrix is a constant matrix.3 In the next two sections, we will consider only homogeneous
linear systems
x′ = Ax,
where A is an n × n matrix of real constants.
To motivate the new solution technique to be developed, we recall from Exam-
ple 9.2.7 that two linearly independent solutions to the vector differential equation
x′ = Ax,
A =
'1
2
2
−2
(
are
x1(t) =
' e−3t
−2e−3t
(
,
x2(t) =
' e2t
1
2e2t
(
,
which we write as
x1(t) = e−3t
' 1
−2
(
,
x2(t) = e2t
' 1
1/2
(
.
The key point to notice is that both of these solutions are of the form
x(t) = eλtv,
(9.4.1)
where λ is a scalar and v is a constant vector. This suggests that the general vector
differential equation
x′ = Ax
(9.4.2)
may also have solutions of the form (9.4.1). We now investigate this possibility. Differ-
entiating (9.4.1) with respect to t yields
x′ = λeλtv.
Thus, x(t) = eλtv is a solution to (9.4.2) if and only if
λeλtv = eλt Av;
3This assumption should not be too surprising in view of the discussion of linear nth-order differential
equations in Chapter 8.

600
CHAPTER 9
Systems of Differential Equations
that is, if and only if λ and v satisfy
Av = λv.
But this is the statement that λ and v must be an eigenvalue/eigenvector pair for A.
Consequently, we have established the following fundamental result:
Theorem 9.4.1
Let A be an n × n matrix of real constants, and let λ be an eigenvalue of A with
corresponding eigenvector v. Then
x(t) = eλtv
is a solution to the constant coefﬁcient vector differential equation x′ = Ax on any
interval.
Remark
Notice that we have not assumed that the eigenvalues and eigenvectors of
A are real; the preceding result holds in the complex case also.
We now illustrate how Theorem 9.4.1 can be used to ﬁnd the general solution to
constant coefﬁcient vector differential equations.
Example 9.4.2
Find the general solution to
x′
1 =
2x1 + x2,
x′
2 = −3x1 −2x2.
(9.4.3)
Solution:
The corresponding vector differential equation is
x′ = Ax, where A =
' 2
1
−3
−2
(
.
(9.4.4)
A straightforward calculation yields
det(A −λI) = 2 −λ
1
−3
−2 −λ = λ2 −1,
so that A has eigenvalues λ = ±1.
Eigenvalue λ1 = 1: In this case, the system (A −λ1I)v = 0 is
ν1 + ν2 = 0,
−3ν1 −3ν2 = 0,
with solution v = r(1, −1). Therefore,
x1(t) = et
' 1
−1
(
is a solution to the vector differential equation (9.4.4).
Eigenvalue λ2 = −1: In this case, the system (A −λ2I)v = 0 is
3ν1 + ν2 = 0,
−3ν1 −ν2 = 0,

9.4
Vector Differential Equations: Nondefective Coefficient Matrix 601
with solution v = s(1, −3). Consequently,
x2(t) = e−t
' 1
−3
(
is also a solution to the vector differential equation (9.4.4).
The Wronskian of the solutions x1 and x2 obtained is
W[x1, x2](t) =
et
e−t
−et
−3e−t
= −2 ̸= 0,
so that {x1, x2} is linearly independent on any interval by Theorem 9.2.4. Hence, the
general solution to (9.4.4) is
x(t) = c1x1 + c2x2 = c1et
' 1
−1
(
+ c2e−t
' 1
−3
(
.
Combining the column vectors on the right-hand side yields the solution vector
x(t) =
'
c1et + c2e−t
−(c1et + 3c2e−t)
(
.
Therefore, the solution to the linear system of differential equations (9.4.3) is
x1(t) = c1et + c2e−t,
x2(t) = −(c1et + 3c2e−t).
□
To ﬁnd the general solution to an n ×n constant coefﬁcient vector differential equa-
tion, we need to ﬁnd n linearly independent solutions (see Theorem 9.3.2). The preceding
example together with our experience with eigenvalues and eigenvectors suggests that
we will be able to ﬁnd n such linearly independent solutions provided the matrix A has n
linearly independent eigenvectors, that is; provided that A is nondefective. This is indeed
the case, although if the eigenvalues and eigenvectors are complex, we must do some
work to obtain real-valued solutions to the system. We ﬁrst give the result for the case
of real eigenvalues.
Theorem 9.4.3
Let A be an n ×n matrix of real constants. If A has n real linearly independent eigenvec-
tors v1, v2, . . . , vn, with corresponding real eigenvalues λ1, λ2, . . . , λn (not necessarily
distinct), then the vector functions {x1, x2, . . . , xn} deﬁned by
xk(t) = eλktvk,
k = 1, 2, . . . , n,
for all t, are linearly independent solutions to x′ = Ax on any interval. The general
solution to this vector differential equation is
x(t) = c1x1 + c2x2 + · · · + cnxn.
Proof We have already shown in Theorem 9.4.1 that each xk(t) satisﬁes x′ = Ax for
all t. Further, using properties of determinants,
W[x1, x2, . . . , xn] = det([eλ1tv1, eλ2tv2, . . . , eλktvn])
= e(λ1+λ2+ ... +λn)tdet([v1, v2, . . . , vn])
̸= 0,

602
CHAPTER 9
Systems of Differential Equations
since the eigenvectors are linearly independent by assumption, and hence, the solutions
are linearly independent on any interval. Thus, {x1, x2, . . . , xn} is a fundamental solu-
tion set to the vector differential equation, from which we immediately deduce the last
statement.
Example 9.4.4
Find the general solution to x′ = Ax if A =
⎡
⎣
0
2
−3
−2
4
−3
−2
2
−1
⎤
⎦.
Solution:
We ﬁrst determine the eigenvalues and eigenvectors of A. For the given
matrix, we have
det(A −λI) =
−λ
2
−3
−2
4 −λ
−3
−2
2
−1 −λ
= −(λ + 1)(λ −2)2,
so that the eigenvalues are
λ1 = −1 (with multiplicity 1) ,
λ2 = 2 (with multiplicity 2) .
Eigenvalue λ1 = −1: It is easily shown that all eigenvectors corresponding to this
eigenvalue are of the form v = r(1, 1, 1), so that we can take
v1 = (1, 1, 1).
Eigenvalue λ2 = 2: In this case, the system for the eigenvectors reduces to the single
equation
2ν1 −2ν2 + 3ν3 = 0,
which has solution v = r(1, 1, 0) + s(−3, 0, 2). Therefore, two linearly independent
eigenvectors corresponding to λ = 2 are
v2 = (1, 1, 0),
v3 = (−3, 0, 2).
It follows from Theorem 9.4.3 that three linearly independent solutions to the given
vector differential equation are
x1(t) = e−t
⎡
⎣
1
1
1
⎤
⎦,
x2(t) = e2t
⎡
⎣
1
1
0
⎤
⎦,
x3(t) = e2t
⎡
⎣
−3
0
2
⎤
⎦.
Consequently, the general solution to the given system is
x(t) = c1e−t
⎡
⎣
1
1
1
⎤
⎦+ c2e2t
⎡
⎣
1
1
0
⎤
⎦+ c3e2t
⎡
⎣
−3
0
2
⎤
⎦,
which can be combined to obtain the solution vector
x(t) =
⎡
⎣
c1e−t + c2e2t −3c3e2t
c1e−t + c2e2t
c1e−t + 2c3e2t
⎤
⎦.
□

9.4
Vector Differential Equations: Nondefective Coefficient Matrix 603
Theorem 9.4.3 constructs the general solution to the vector differential equation
x′ = Ax in the case of a nondefective matrix A real eigenvectors. For such a matrix,
there is another way to arrive at the general solution
x(t) = c1x1 + c2x2 + · · · + cnxn
to x′ = Ax that we brieﬂy introduced in Section 7.3. Namely, we can write
A = SDS−1,
where the columns of S consist of n linearly independent eigenvectors of A,
S = [v1, v2, . . . , vn],
and D is a diagonal matrix containing the corresponding eigenvalues:
D = diag(λ1, λ2, . . . , λn).
We can use the linear change of variables x = Sy to replace x′ = Ax by
Sy′ = (SDS−1)Sy = SDy,
or, using the invertibility of S,
y′ = Dy.
This is an uncoupled system of differential equations that is easy to solve:
y = [c1eλ1t c2eλ2t . . . cneλnt]T .
Hence, we ﬁnd that
x = Sy = c1eλ1tv1 + c2eλ2tv2 + · · · + cneλntvn,
precisely the general solution guaranteed by Theorem 9.4.3.
We now consider the case when some (or all) of the eigenvalues are complex.
Since we are restricting attention to systems of equations with real constant coefﬁ-
cients, it follows that the matrix of the system will have real entries, and hence, from
Theorem 7.1.8, the eigenvalues and eigenvectors will occur in conjugate pairs. The cor-
responding solutions to x′ = Ax guaranteed by Theorem 9.4.1 will also be complex
conjugate. However, as we now show, each conjugate pair gives rise to two real-valued
solutions.
Theorem 9.4.5
Let u(t) and v(t) be real-valued vector functions. If
w1(t) = u(t) + iv(t)
and
w2(t) = u(t) −iv(t)
are complex conjugate solutions to x′ = Ax, then
x1(t) = u(t)
and
x2(t) = v(t)
are themselves real-valued solutions of x′ = Ax.

604
CHAPTER 9
Systems of Differential Equations
Proof Since w1 and w2 are solutions to the vector differential equation, so is any linear
combination of them. In particular,
x1(t) = 1
2[w1(t) + w2(t)] = u(t)
and
x2(t) = 1
2i [w1(t) −w2(t)] = v(t)
are solutions to the vector differential equation.
We now explicitly derive two appropriate real-valued solutions corresponding to a
complex conjugate pair of eigenvalues. Suppose that λ = a+ib (b ̸= 0) is an eigenvalue
of A with corresponding eigenvector v = r + is. Then, applying Theorem 9.4.1, a
complex-valued solution to x′ = Ax is
w(t) = e(a+ib)t(r + is) = eat(cos bt + i sin bt)(r + is),
which can be written as
w(t) = eat(cos bt r −sin bt s) + ieat(sin bt r + cos bt s).
Theorem 9.4.5 implies that two real-valued solutions to x′ = Ax are
x1(t) = eat(cos bt r −sin bt s),
x2(t) = eat(sin bt r + cos bt s).
It can further be shown that the set of all real-valued solutions obtained in this manner
is linearly independent on any interval.
Remark
Notice that we do not have to derive the solution corresponding to the
conjugate eigenvalue λ = a −ib, since it does not yield any new linearly independent
solutions to x′ = Ax.
Example 9.4.6
Find the general solution to the vector differential equation x′ = Ax if
A =
' 0
2
−2
0
(
.
Solution:
The characteristic polynomial of A is
p(λ) = −λ
2
−2
−λ = λ2 + 4.
Consequently, A has complex conjugate eigenvalues λ = ±2i. The eigenvectors corre-
sponding to the eigenvalues λ = 2i are obtained by solving
−2iν1 + 2ν2 = 0
and are therefore of the form v = r(−i, 1). Hence, a complex-valued solution to the
given differential equation is
w(t) = e2it
'−i
1
(
.

9.4
Vector Differential Equations: Nondefective Coefficient Matrix 605
We must now do some algebra to obtain the corresponding real-valued solutions. Using
Euler’s formula, we can write
w(t) = (cos 2t + i sin 2t)
'−i
1
(
=
'sin 2t −i cos 2t
cos 2t + i sin 2t
(
=
'sin 2t
cos 2t
(
+ i
'−cos 2t
sin 2t
(
.
Applying Theorem 9.4.5, we directly obtain the two real-valued functions
x1(t) =
'sin 2t
cos 2t
(
and x2(t) =
'−cos 2t
sin 2t
(
.
Consequently, the general solution to the given vector differential equation is
x(t) = c1
'sin 2t
cos 2t
(
+ c2
'−cos 2t
sin 2t
(
=
'c1 sin 2t −c2 cos 2t
c1 cos 2t + c2 sin 2t
(
.
We note that, in this case, the eigenvalues of A were pure imaginary and that the com-
ponents of the corresponding solution vector are oscillatory. Once more, this illustrates
the importance of complex scalars when modelling oscillatory physical behavior.
□
As illustrated in the next example, when the real part of a complex eigenvalue is
nonzero, the algebra can become a little bit more tedious.
Example 9.4.7
Find the general solution to the vector differential equation x′ = Ax if
A =
'2
−1
2
4
(
.
Solution:
The characteristic polynomial of A is
p(λ) = 2 −λ
−1
2
4 −λ = λ2 −6λ + 10,
so that the eigenvalues are λ = 3±i. We need only to ﬁnd the eigenvectors corresponding
to one of these conjugate eigenvalues. When λ = 3 + i, the eigenvectors are obtained
by solving
−(1 + i)ν1 −
ν2 = 0,
2ν1 + (1 −i)ν2 = 0,
which yield the complex eigenvectors v = r(1, −(1 + i)). Hence a complex-valued
solution to the given system is
w(t) = e3t(cos t + i sin t)
'
1
−(1 + i)
(
= e3t
'
cos t + i sin t
−(1 + i)(cos t + i sin t)
(
= e3t
'
cos t + i sin t
(sin t −cos t) −i(sin t + cos t)
(
= e3t
,'
cos t
sin t −cos t
(
+ i
'
sin t
−(sin t + cos t)
(-
.

606
CHAPTER 9
Systems of Differential Equations
From Theorem 9.4.5, the real and imaginary parts of this complex-valued solution yield
the following two real-valued linearly independent solutions:
x1(t) = e3t
!
cos t
sin t −cos t
"
and x2(t) = e3t
!
sin t
−(sin t + cos t)
"
.
Hence, the general solution to the given system is
x(t) = c1e3t
!
cos t
sin t −cos t
"
+ c2e3t
!
sin t
−(sin t + cos t)
"
= e3t
#
c1
!
cos t
sin t −cos t
"
+ c2
!
sin t
−(sin t + cos t)
"$
=
!
e3t(c1 cos t + c2 sin t)
e3t[c1(sin t −cos t) −c2(sin t + cos t)]
"
.
□
The results of this section are summarized in the next theorem.
Theorem 9.4.8
Let A be an n × n matrix of real constants.
1. Suppose λ is a real eigenvalue of A with corresponding linearly independent eigen-
vectors v1, v2, . . . , vk. Then k linearly independent solutions to x′ = Ax are
x j(t) = eλtv j,
j = 1, 2, . . . , k.
2. Suppose λ = a + ib is a complex eigenvalue of A with corresponding linearly
independent eigenvectors v1, v2, . . . , vk, where v j = r j + is j. Then k complex-
valued solutions to x′ = Ax are
u j(t) = eλtv j,
j = 1, 2, . . . , k
and 2k real-valued linearly independent solutions to x′ = Ax are
x11(t) = eat(cos bt r1 −sin bt s1),
x12(t) = eat(sin bt r1 + cos bt s1)
x21(t) = eat(cos bt r2 −sin bt s2),
x22(t) = eat(sin bt r2 + cos bt s2)
...
...
xk1(t) = eat(cos bt rk −sin bt sk),
xk2(t) = eat(sin bt rk + cos bt sk)
Further, the set of all solutions to x′ = Ax obtained in this manner is linearly
independent on any interval.
Corollary 9.4.9
If A is a nondefective n × n matrix, then the solutions obtained from parts (1) and (2)
of Theorem 9.4.8 yield a fundamental set of solutions {x1, x2, . . . , xn} to x′ = Ax, and
the general solution to this vector differential equation is
x(t) = c1x1 + c2x2 + · · · + cnxn.

9.4
Vector Differential Equations: Nondefective Coefficient Matrix 607
Exercises for 9.4
Skills
• Be able to ﬁnd the general solution to the vector dif-
ferential equation x′ = Ax in the case where A is
a constant, nondefective matrix (Theorem 9.4.8 and
Corollary 9.4.9).
• In the case of a complex eigenvalue-eigenvector pair
(λ, v), be able to obtain two real-valued solutions for
the solution x(t) = eλtv to x′ = Ax.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If (λ, v) is an eigenvalue-eigenvector pair for A, then
x(t) = eλtv is a solution to the vector differential
equation x′ = Ax.
(b) Each pair of complex eigenvalues λ = a ±ib (b ̸= 0)
of a matrix A gives rise to a pair of real-valued solu-
tions to the vector differential equation x′ = Ax.
(c) If A and B are n × n matrices with the same charac-
teristic equation, then the solution sets to the vector
differential equations x′ = Ax and x′ = Bx are the
same.
(d) If A and B are n ×n matrices with the same collection
of eigenvalue-eigenvector pairs, then the solutions to
the vector differential equations x′ = Ax and x′ = Bx
are the same.
(e) If A is a 2 × 2 nondefective matrix with eigenvalues
λ = a±ib with a, b > 0, then all solutions of the vec-
tor differential equation x′ = Ax satisfy ||x(t)|| →∞
as t →∞.
(f) If the eigenvalues λ1 and λ2 of a 2×2 matrix A are real
with λ1 < 0 < λ2, then a solution x(t) to the vector
differential equation x′ = Ax tends towards the origin
as t →∞if and only if x(0) is a vector parallel to an
eigenvector v2 that corresponds to λ2.
Problems
For Problems 1–16, determine the general solution to the
system x′ = Ax for the given matrix A.
1.
'−1
2
2
2
(
.
2.
'−2
−7
−1
4
(
.
3.
'0
−4
4
0
(
.
4.
'1
−2
5
−5
(
.
5.
'−1
2
−2
−1
(
.
6.
⎡
⎣
2
0
0
0
5
−7
0
2
−4
⎤
⎦.
7.
⎡
⎣
−1
0
0
1
5
−1
1
6
−2
⎤
⎦.
8.
⎡
⎣
0
1
0
−1
0
0
0
0
5
⎤
⎦.
9.
⎡
⎣
2
0
3
0
−4
0
−3
0
2
⎤
⎦.
10.
⎡
⎣
3
2
6
−2
1
−2
−1
−2
−4
⎤
⎦.
11.
⎡
⎣
0
−3
1
−2
−1
1
0
0
2
⎤
⎦.
12.
⎡
⎣
3
0
−1
0
−3
−1
0
2
−1
⎤
⎦.
13.
⎡
⎣
1
1
−1
1
1
1
−1
1
1
⎤
⎦.
14.
⎡
⎣
2
−1
3
2
−1
3
2
−1
3
⎤
⎦.
15.
⎡
⎢⎢⎣
1
2
3
4
4
3
2
1
4
5
6
7
7
6
5
4
⎤
⎥⎥⎦.

608
CHAPTER 9
Systems of Differential Equations
16.
⎡
⎢⎢⎣
0
1
0
0
−1
0
0
0
0
0
0
−1
0
0
1
0
⎤
⎥⎥⎦.
For Problems 17–19, solve the initial-value problem x′ =
Ax, x(0) = x0.
17. A =
'−1
4
2
−3
(
,
x0 =
'3
0
(
.
18. A =
'−1
−6
3
5
(
,
x0 =
'2
2
(
.
19. A =
⎡
⎣
2
−1
3
3
1
0
2
−1
3
⎤
⎦,
x0 =
⎡
⎣
−4
4
4
⎤
⎦.
20. Solve the initial-value problem
x′ = Ax,
x(0) =
'1
1
(
,
when A =
' 0
4
−4
0
(
. Sketch the solution in the x1x2-
plane.
21. Consider the differential equation
d2x
dt2 + bdx
dt + cx = 0,
(9.4.5)
where b and c are constants.
(a) Show that Equation (9.4.5) can be replaced by
the equivalent ﬁrst-order linear system x′ = Ax,
where A =
' 0
1
−c
−b
(
.
(b) Show that the characteristic polynomial of A co-
incides with the auxiliary polynomial of Equation
(9.4.5).
22. Let λ = a + ib, b ̸= 0, be an eigenvalue of the
n × n (real) matrix A with corresponding eigenvec-
tor v = r + is. Then we have shown in the text that
two real-valued solutions to x′ = Ax are
x1(t) = eat[cos btr −sin bts],
x2(t) = eat[sin btr + cos bts].
Prove that x1 and x2 are linearly independent on any
interval. (You may assume that r and s are linearly
independent in Rn.)
The remaining problems in this section investigate
general properties of solutions to x′ = Ax, where A is
a nondefective matrix.
23. Let A be a 2×2 nondefective matrix. If all eigenvalues
of A have negative real part, prove that every solution
to x′ = Ax satisﬁes
lim
t→∞x(t) = 0.
(9.4.6)
24. Let A be a 2×2 nondefective matrix. If every solution
to x′ = Ax satisﬁes (9.4.6), prove that all eigenvalues
of A have negative real part.
25. Determine the general solution to x′ = Ax if A =
' 0
b
−b
0
(
, where b > 0. Describe the behavior of the
solutions.
26. Describe the behavior of the solutions to x′ = Ax, if
A =
' a
b
−b
a
(
, where a < 0 and b > 0.
27. What conditions on the eigenvalues of an n ×n matrix
A would guarantee that the system x′ = Ax has at
least one solution satisfying
x(t) = x0,
for all t, where x0 is a constant vector?
28. The motion of a certain physical system is described
by the system of differential equations
x′
1 = x2,
x′
2 = −bx1 −ax2,
where a and b are positive constants and a ̸= 2b. Show
that the motion of the system dies out as t →+∞.
9.5
Vector Differential Equations: Defective Coefficient Matrix
The results of the previous section enable us to solve any constant coefﬁcient linear
system of differential equations x′ = Ax, provided that A is nondefective. We recall from
Chapter 7 that if m denotes the multiplicity of an eigenvalue of A, then the dimension r
of the corresponding eigenspace satisﬁes the inequality
1 ≤r ≤m,

9.5
Vector Differential Equations: Defective Coefficient Matrix 609
and the condition for A to be nondefective is that r = m for each eigenvalue; that is, the
dimension of each eigenspace equals the multiplicity of the corresponding eigenvalue.
(See Theorem 7.2.11.) We now turn our attention to the case when A is defective. Then,
for at least one eigenvalue λ, the dimension r of the corresponding eigenspace is strictly
less than the multiplicity m of the eigenvalue. In this case, there are only r linearly
independent eigenvectors corresponding to λ, and so Theorem 9.4.8 will only yield r
linearly independent solutions to the vector differential equation x′ = Ax. We must
therefore ﬁnd an additional m −r linearly independent solutions. In order to motivate
the results of this section, we consider a particular example.
Example 9.5.1
Find the general solution to
x′ = Ax,
A =
' 0
1
−9
6
(
.
(9.5.1)
Solution:
We try using the technique from the previous section. The coefﬁcient matrix
A has the single eigenvalue λ = 3 of multiplicity 2, and it is straightforward to show
that there is just one corresponding linearly independent eigenvector, which we may
take to be v0 = (1, 3). Consequently, A is defective, and we obtain only one linearly
independent solution to the vector differential equation, namely,
x0(t) = e3t
'1
3
(
.
(9.5.2)
In order to obtain a second linearly independent solution x1(t) to the vector differential
equation (9.5.1), we explicitly consider the equations of the linear system:
x′
1 = x2,
x′
2 = −9x1 + 6x2.
Applying the solution technique introduced in Section 9.1 yields
x1(t) = c1e3t + c2te3t,
x2(t) = 3c1e3t + c2e3t(3t + 1).
Consequently, the general solution to (9.5.1) is
x(t) =
'
c1e3t + c2te3t
3c1e3t + c2e3t(3t + 1)
(
,
which can be written in the equivalent form
x(t) = c1e3t
'1
3
(
+ c2e3t
,'0
1
(
+ t
'1
3
(-
.
We see that two linearly independent solutions to the given system are
x0(t) = e3t
'1
3
(
and
x1(t) = e3t
,'0
1
(
+ t
'1
3
(-
.
The solution x0(t) coincides with the solution (9.5.2), which was derived using the
eigenvalue/eigenvector technique of the previous section. The key point to notice is that,
in this particular example, there is a second linearly independent solution of the form
x1(t) = eλt(v1 + tv0).
□

610
CHAPTER 9
Systems of Differential Equations
As a ﬁrst step towards generalizing the result of the preceding example, suppose
that the n × n matrix A has precisely one eigenvalue, λ, of multiplicity n, and that
the dimension of the corresponding eigenspace is n −1. If v(1)
0 , v(2)
0 , . . . , v(n−1)
0
denote
linearly independent eigenvectors corresponding to λ, then n −1 linearly independent
solutions to x′ = Ax are
x(i)
0 (t) = eλtv(i)
0 ,
i = 1, 2, . . . , n −1.
Based on the previous example, we look for a further linearly independent solution to
x′ = Ax of the form
x1(t) = eλt(v1 + tv0),
for some eigenvector v0. Differentiating this proposed solution with respect to t yields
x′
1 = eλt[(λv1 + v0) + λtv0].
Consequently, x′
1 = Ax1 provided v1 and v0 satisfy
eλt[(λv1 + v0) + λtv0] = eλt(Av1 + t Av0);
that is,
(λv1 + v0) + λtv0 = Av1 + t Av0.
Since this equation must hold for all values of t in the interval of interest, we can equate
the corresponding coefﬁcients of powers of t to obtain
Av0 = λv0,
Av1 = λv1 + v0,
which can be rearranged to read
(A −λI)v0 = 0,
(9.5.3)
(A −λI)v1 = v0.
(9.5.4)
Equation (9.5.3) requires that v0 be an eigenvector of A corresponding to the eigenvalue
λ as expected. Consequently, one approach to determining v0 and v1 would be to substi-
tute the general expression for the eigenvectors v0 into the right-hand side of Equation
(9.5.4) and solve the resulting system of equations for v1. Since Equation (9.5.4) is a
nonhomogeneous system of equations there is no a priori reason why the system should
have a solution. However, it is possible to establish that, by an appropriate choice of
v0, consistency can be obtained and therefore v1 can indeed be determined. Whereas
this method for obtaining v0 and v1 works well in the present situation, we prefer to
introduce an equivalent method that is computationally more efﬁcient and transparent.
We proceed as follows. Left multiplying (9.5.4) by A −λI and using (9.5.3) yields
(A −λI)2v1 = 0.
(9.5.5)
Any vector v1 satisfying (9.5.5) is called a generalized eigenvector of A (see Deﬁnition
7.6.1).Thekeypointisthat,inthecaseunderconsideration,itisalwayspossibletochoose
a vector v1 satisfying (A −λI)2v1 = 0 and (A −λI)v1 ̸= 0. For any such v1, we can
use Equation (9.5.4) to deﬁne the corresponding v0 by
v0 = (A −λI)v1.
Then v0 is an eigenvector of A, since
(A −λI)v0 = (A −λI)2v1 = 0.

9.5
Vector Differential Equations: Defective Coefficient Matrix 611
Consequently, Equation (9.5.3) is also satisﬁed. To summarize,
x1(t) = eλt(v1 + tv0)
is a solution to the system x′ = Ax whenever v0 and v1 satisfy
(A −λI)2v1 = 0,
(A −λI)v1 ̸= 0
v0 = (A −λI)v1.
Furthermore, the resulting n solutions x(1)
0 , x(2)
0 , . . . , x(n−1)
0
, x1 are linearly independent
on any interval. We illustrate this solution technique with some examples.
Example 9.5.2
Solve the vector differential equation x′ = Ax if A =
'6
−8
2
−2
(
.
Solution:
We ﬁrst determine the eigenvalues and eigenvectors of A. We have
det(A −λI) = 6 −λ
−8
2
−2 −λ = (λ −2)2,
so that there is only one eigenvalue λ = 2, with multiplicity 2. The corresponding
eigenvectors are of the form v = r(2, 1) so that the corresponding eigenspace is one-
dimensional and we only have a single eigenvalue/eigenvector solution to the vector
differential equation. It follows from the preceding discussion that there is a second
linearly independent solution to x′ = Ax of the form
x1(t) = e2t(v1 + tv0),
where v1 and v0 are determined from
(A −2I)2v1 = 0,
(A −2I)v1 ̸= 0
(9.5.6)
v0 = (A −2I)v1.
(9.5.7)
In this case, we have
A −2I =
'4
−8
2
−4
(
and (A −2I)2 = 02.
Consequently, the equations in (9.5.6) will be satisﬁed by any vector v1 such that
(A −2I)v1 ̸= 0. For simplicity, we take
v1 =
'1
0
(
.
Then from Equation (9.5.7)
v0 =
'4
−8
2
−4
( '1
0
(
=
'4
2
(
.
From the expressions here for v0 and v1, we can write down two linearly independent
solutions to the vector differential equation:
x0(t) = c1e2t
'4
2
(
and x1(t) = e2t
,'1
0
(
+ t
'4
2
(-
= e2t
'1 + 4t
2t
(
.

612
CHAPTER 9
Systems of Differential Equations
Consequently, the general solution to the vector differential equation is
x(t) = c1e2t
'4
2
(
+ c2e2t
'1 + 4t
2t
(
=
'e2t[4c1 + c2(1 + 4t)]
e2t(2c1 + 2c2t)
(
.
□
Example 9.5.3
Determine the general solution to x′ = Ax if A =
⎡
⎣
5
2
−1
1
6
−1
3
6
1
⎤
⎦.
Solution:
A short calculation shows that
det(A −λI) =
5 −λ
2
−1
1
6 −λ
−1
3
6
1 −λ
= −(λ −4)3.
We see that A has a single eigenvalue, λ = 4, of multiplicity 3. It is easily shown that
the associated eigenvectors are of the form
v0 = (−2a + b, a, b) = a(−2, 1, 0) + b(1, 0, 1),
(9.5.8)
which means that only two linearly independent solutions x(1)
0
and x(2)
0
can be con-
structed solely from the eigenvectors of A. Consequently, we must seek a third linearly
independent solution x1(t) to the vector differential equation of the form
x1(t) = e4t(v1 + tv0),
where v1 and v0 are determined from
(A −4I)2v1 = 0,
(A −4I)v1 ̸= 0.
(9.5.9)
v0 = (A −4I)v1.
(9.5.10)
In this example, we have
A −4I =
⎡
⎣
1
2
−1
1
2
−1
3
6
−3
⎤
⎦
and (A −4I)2 = 03.
By (9.5.9), we can therefore choose v1 to be any vector such that (A −4I)v1 ̸= 0. For
simplicity, we take
v1 =
⎡
⎣
1
0
0
⎤
⎦.
Then, from Equation (9.5.10)
v0 =
⎡
⎣
1
2
−1
1
2
−1
3
6
−3
⎤
⎦
⎡
⎣
1
0
0
⎤
⎦=
⎡
⎣
1
1
3
⎤
⎦.
Therefore, we obtain two linearly independent solutions to the vector differential equa-
tion, namely,
x(1)
0 (t) = e4t
⎡
⎣
1
1
3
⎤
⎦
and x(1)
1 (t) = e4t
⎧
⎨
⎩
⎡
⎣
1
0
0
⎤
⎦+ t
⎡
⎣
1
1
3
⎤
⎦
⎫
⎬
⎭= e4t
⎡
⎣
1 + t
t
3t
⎤
⎦.

9.5
Vector Differential Equations: Defective Coefficient Matrix 613
Finally, we obtain a third linearly independent solution to x′ = Ax by choosing a
second eigenvector v(2)
0
that is nonproportional to v0. From (9.5.8), we may choose
v(2)
0
= (−2, 1, 0). Thus, x(2)
0 (t) = e4t
⎡
⎣
−2
1
0
⎤
⎦. Consequently, the general solution to the
given vector differential equation is
x(t) = c1e4t
⎡
⎣
1
1
3
⎤
⎦+ c2e4t
⎡
⎣
1 + t
t
3t
⎤
⎦+ c3e4t
⎡
⎣
−2
1
0
⎤
⎦,
or equivalently,
x(t) = e4t
⎡
⎣
c1 + c2(1 + t) −2c3
c1 + c2t + c3
3c1 + 3c2t
⎤
⎦.
□
Our next theorem tells us how the preceding technique generalizes to the case of an
arbitrary defective coefﬁcient matrix.
Theorem 9.5.4
Let A be an n × n matrix.
1. Suppose the eigenvalue λ has multiplicity m and that the dimension of the corre-
sponding eigenspace is r ≤m. Then there exist m linearly independent solutions
to x′ = Ax that can be constructed in r cycles (one for each i with 1 ≤i ≤r) of
the form
x(i)
0 (t) = eλtv(i)
0 ,
(9.5.11)
x(i)
1 (t) = eλt 1
v(i)
1 + tv(i)
0
2
,
(9.5.12)
x(i)
2 (t) = eλt
3
v(i)
2 + tv(i)
1 + 1
2!t2v(i)
0
4
,
(9.5.13)
...
x(i)
ki (t) = eλt
3
v(i)
ki + tv(i)
ki−1 + · · · + 1
ki!tki v(i)
0
4
,
(9.5.14)
where each ki ≥0, and k1 + k2 + · · · + kr = m −r. For each i, the vec-
tors v(i)
0 , v(i)
1 , . . . , v(i)
ki can be determined as follows. Choose vki to be any vector
satisfying
(A −λI)ki+1v(i)
ki = 0,
(A −λI)ki v(i)
ki ̸= 0.
(9.5.15)
Then:
v(i)
ki−1 = (A −λI)v(i)
ki ,
(9.5.16)
v(i)
ki−2 = (A −λI)2v(i)
ki ,
(9.5.17)
...
v(i)
1
= (A −λI)ki−1v(i)
ki ,
(9.5.18)
v(i)
0
= (A −λI)ki v(i)
ki ,
(9.5.19)

614
CHAPTER 9
Systems of Differential Equations
The collection of all solutions x(i)
j (t) (for 1 ≤i ≤r and 0 ≤j ≤ki) is a linearly
independent set of m solutions for x′ = Ax.
2. Applying the results of (1) to each eigenvalue of A generates a set of n solutions
to x′ = Ax that are linearly independent on any interval.
Remarks
1. Note that if ki = 0 for each i and for each eigenvalue λ, then m = r for each
eigenvalue λ, which is the case in which the matrix A is nondefective, and the
result here is compatible with the results in the previous section.
2. A proof of Theorem 9.5.4 based on the Jordan canonical form of an n × n
matrix A is given in Problem 20. For readers who have studied this concept
(Section 7.6), observe that r is the number of Jordan blocks corresponding to
λ in the Jordan canonical form of A, the numbers ki + 1 are the sizes of the Jordan
blocks corresponding to the eigenvalue λ, and each ordered collection of vectors
{v(i)
0 , v(i)
1 , v(i)
2 , . . . , v(i)
ki } is a cycle of generalized eigenvectors of A corresponding
to λ.
We conclude this section with examples to illustrate Theorem 9.5.4.
Example 9.5.5
Find the general solution to x′ = Ax if A =
⎡
⎣
1
2
0
1
1
2
0
−1
1
⎤
⎦.
Solution:
The given matrix has characteristic polynomial
det(A −λI) =
1 −λ
2
0
1
1 −λ
2
0
−1
1 −λ
= −(λ −1)3.
Hence, λ = 1 is the only eigenvalue of A, of multiplicity 3. The corresponding eigen-
vectors are the nonzero multiples of
v0 =
⎡
⎣
−2
0
1
⎤
⎦
so that the eigenspace has dimension 1. Therefore, r = 1 in Theorem 9.5.4, and the only
value of i we must consider is i = 1. Therefore, we will omit the superscript (i) from
the notation used in theorem. We obtain a solution to x′ = Ax of the form
x0(t) = etv0.
(9.5.20)
According to Theorem 9.5.4, there exist two further linearly independent solutions to
x′ = Ax of the form
x1(t) = et(v1 + tv0),
(9.5.21)
x2(t) = et
3
v2 + tv1 + 1
2!t2v0
4
,
(9.5.22)
where
(A −I)3v2 = 0,
(A −I)2v2 ̸= 0,

9.5
Vector Differential Equations: Defective Coefficient Matrix 615
and
v1 = (A −I)v2,
v0 = (A −I)2v2.
A short calculation shows that
A −I =
⎡
⎣
0
2
0
1
0
2
0
−1
0
⎤
⎦,
(A −I)2 =
⎡
⎣
2
0
4
0
0
0
−1
0
−2
⎤
⎦,
(A −I)3 = 03,
so we may take
v2 =
⎡
⎣
1
0
0
⎤
⎦,
v1 = (A −I)v2 =
⎡
⎣
0
1
0
⎤
⎦,
v0 = (A −I)2v2 =
⎡
⎣
2
0
−1
⎤
⎦.
Substituting these vectors into the expressions (9.5.20), (9.5.21), and (9.5.22) for x0(t),
x1(t), and x2(t) gives us
x0(t) = et
⎡
⎣
2
0
−1
⎤
⎦,
x1(t) = et
⎧
⎨
⎩
⎡
⎣
0
1
0
⎤
⎦+ t
⎡
⎣
2
0
−1
⎤
⎦
⎫
⎬
⎭=
⎡
⎣
2tet
et
−tet
⎤
⎦,
and
x2(t) = et
⎧
⎨
⎩
⎡
⎣
1
0
0
⎤
⎦+ t
⎡
⎣
0
1
0
⎤
⎦+ 1
2!t2
⎡
⎣
2
0
−1
⎤
⎦
⎫
⎬
⎭=
⎡
⎢⎣
et(1 + t2)
tet
−1
2t2et
⎤
⎥⎦.
Hence, the general solution to the vector differential equation is
x(t) = c1et
⎡
⎣
2
0
−1
⎤
⎦+ c2
⎡
⎣
2tet
et
−tet
⎤
⎦+ c3
⎡
⎢⎣
et(1 + t2)
tet
−1
2t2et
⎤
⎥⎦
=
⎡
⎢⎣
et[2c1 + 2c2t + c3(1 + t2)]
et(c2 + c3t)
−et(c1 + c2t + 1
2c3t2)
⎤
⎥⎦.
□
Example 9.5.6
Find the general solution to x′ = Ax if A =
⎡
⎢⎢⎣
2
1
0
0
0
2
0
0
0
0
2
1
0
0
0
2
⎤
⎥⎥⎦.
Solution:
The matrix A has characteristic polynomial
det(A −λI) =
2 −λ
1
0
0
0
2 −λ
0
0
0
0
2 −λ
1
0
0
0
2 −λ
= (2 −λ)4.

616
CHAPTER 9
Systems of Differential Equations
Hence, λ = 2 is the only eigenvalue with multiplicity 4. The corresponding eigenvectors
are of the form
v0 = a(1, 0, 0, 0) + b(0, 0, 1, 0).
Consequently, since the eigenspace corresponding to λ = 2 is two-dimensional,
Theorem 9.5.4 guarantees that we have two cycles of linearly independent solutions to
x′ = Ax. In general, two cycles that produce four generalized eigenvectors must consist
either of one cycle of length one and one cycle of length three, or two cycles of length
two. In this case, however, we ﬁnd that
A −2I =
⎡
⎢⎢⎣
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦,
(A −2I)2 = 04,
(9.5.23)
so no cycles of length 3 are possible (a cycle of length 3 would have to contain a nonzero
vector of the form (A −2I)2v2, which cannot exist in this case). Therefore, we must
seek two cycles of length two. From (9.5.23) we see that the two linearly independent
vectors
v(1)
1
=
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦
and v(2)
1
=
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦
satisfy
(A −2I)2v(1)
1
= 0,
(A −2I)v(1)
1
̸= 0 and (A −2I)2v(2)
1
= 0,
(A −2I)v(2)
1
̸= 0.
Consequently, we have
v(1)
0
= (A −2I)v(1)
1
=
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
and
v(2)
0
= (A −2I)v(2)
1
=
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦.
Hence, using Theorem 9.5.4, we obtain the solutions
x(1)
0
= e2t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦,
x(1)
1
= e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
= e2t
⎡
⎢⎢⎣
t
1
0
0
⎤
⎥⎥⎦
(9.5.24)
and
x(2)
0
= e2t
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦,
x(2)
1
= e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
= e2t
⎡
⎢⎢⎣
0
0
t
1
⎤
⎥⎥⎦
(9.5.25)
to x′ = Ax. Equations (9.5.24) and (9.5.25) provide four linearly independent solutions
to x′ = Ax. Therefore, the general solution to this system of differential equations is
x(t) = c1e2t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦+ c2e2t
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦+ c3e2t
⎡
⎢⎢⎣
t
1
0
0
⎤
⎥⎥⎦+ c4e2t
⎡
⎢⎢⎣
0
0
t
1
⎤
⎥⎥⎦= e2t
⎡
⎢⎢⎣
c1 + c3t
c3
c2 + tc4
c4
⎤
⎥⎥⎦.
□

9.5
Vector Differential Equations: Defective Coefficient Matrix 617
Example 9.5.7
Find the general solution to x′ = Ax if A =
⎡
⎢⎢⎣
2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2
⎤
⎥⎥⎦.
Solution:
Here, A has the same characteristic polynomial as in the previous example,
with λ = 2 occurring as an eigenvalue of multiplicity 4. The corresponding eigenvectors
in this case are of the form
v0 = a(1, 0, 0, 0) + b(0, 0, 0, 1),
(9.5.26)
and so once more we have two cycles of linearly independent solutions to x′ = Ax. In
this case, we observe that
(A −2I)2 =
⎡
⎢⎢⎣
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦̸= 04,
(A −2I)3 = 04.
Therefore, in contrast to the preceding example, the matrix (A −2I)2 here is nonzero.
Consequently, it is possible to ﬁnd a vector v(1)
2
such that
(A −2I)3v(1)
2
= 0,
and (A −2I)2v(1)
2
̸= 0.
An appropriate choice is
v(1)
2
=
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦.
Therefore, we have
v(1)
2
=
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦,
v(1)
1
= (A −2I)v(1)
2
=
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦,
v(1)
0
= (A −2I)2v(1)
2
=
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦.
Using (9.5.26), we can select an eigenvector that is nonproportional to v(1)
0 , say
v(2)
0
=
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦.
Theorem 9.5.4 supplies four linearly independent solutions to x′ = Ax, namely,
x(1)
0 (t) = e2t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦,
x(1)
1 (t) = e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
,
x(1)
2 (t) = e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦+ t2
2!
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
,

618
CHAPTER 9
Systems of Differential Equations
and
x(2)
0 (t) = e2t
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦.
We can now give the general solution:
x(t) = c1e2t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦+ c2e2t
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦+ c3e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
+ c4e2t
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦+ t
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦+ t2
2!
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
= e2t
⎡
⎢⎢⎢⎣
c1 + c3t + 1
2!c4t2
c3 + c4t
c4
c2
⎤
⎥⎥⎥⎦.
□
Remark
The difference between the last two examples is somewhat subtle. In both
cases, the eigenspace corresponding to λ = 2 is two-dimensional, so according to The-
orem 9.5.4, two cycles of (linearly independent) generalized eigenvectors can be con-
structed. In Example 9.5.6, however, both cycles have length two, and in Example 9.5.7,
one cycle has length three while the other has length one. From a computational point
of view, the reason for this difference is that (A −λI)2 is the zero matrix in the former
example (so that no cycles of length greater than 2 are possible), but nonzero in the latter
example (so that a cycle of length 3 is possible). Readers who have studied the Jordan
canonical form (Section 7.6) should observe that JCF(A) contains the information about
what the lengths of the cycles of generalized eigenvectors of A are. These lengths are
precisely the sizes of the Jordan blocks comprising JCF(A). Therefore, if we are given
what JCF(A) is in advance, then the task of determining proper cycles of generalized
eigenvectors of A is substantially simpliﬁed.
Exercises for 9.5
Key Terms
Cycle of generalized eigenvectors.
Skills
• Be able to solve the vector differential equation
x′ = Ax in the case where A is a defective ma-
trix by constructing cycles of generalized eigenvec-
tors and forming corresponding solutions xk(t) as in
Theorem 9.5.4.
True-False Review
For Questions (a)–(d), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If A is an n × n defective matrix, then the vector dif-
ferential equation x′ = Ax cannot have n linearly in-
dependent solutions.

9.5
Vector Differential Equations: Defective Coefficient Matrix 619
(b) A cycle of generalized eigenvectors of A correspond-
ing to an eigenvalue λ consisting of k vectors yields k
linearly independent solutions to the vector differen-
tial equation x′ = Ax.
(c) The number of linearly independent solutions to
x′ = Ax corresponding to λ is equal to the dimen-
sion of the eigenspace Eλ.
(d) If v0 is an eigenvector of A corresponding to λ and
v1 is a vector satisfying (A −λI)v1 = v0, then
x(t) = eλtv1 is a solution to the vector differential
equation x′ = Ax.
Problems
For Problems 1–14, determine the general solution to the
system x′ = Ax for the given matrix A.
1.
' 1
1
−1
3
(
.
2.
'0
−2
2
4
(
.
3.
'−3
−2
2
1
(
.
4.
⎡
⎣
0
1
0
0
0
1
1
1
−1
⎤
⎦.
5.
⎡
⎣
2
2
−1
2
1
−1
2
3
−1
⎤
⎦.
6.
⎡
⎣
−2
0
0
1
−3
−1
−1
1
−1
⎤
⎦.
7.
⎡
⎣
15
−32
12
8
−17
6
0
0
−1
⎤
⎦.
8.
⎡
⎣
4
0
0
1
4
0
0
1
4
⎤
⎦.
9.
⎡
⎣
1
0
0
0
3
2
2
−2
−1
⎤
⎦.
10.
⎡
⎣
3
1
0
−1
5
0
0
0
4
⎤
⎦.
11.
⎡
⎣
−1
1
0
−2
−3
1
1
1
−2
⎤
⎦.
12.
⎡
⎢⎢⎣
0
−1
0
0
1
0
0
0
1
0
2
1
0
1
0
2
⎤
⎥⎥⎦.
13.
⎡
⎢⎢⎣
−2
3
0
0
3
−2
0
0
1
0
1
−1
0
1
0
1
⎤
⎥⎥⎦.
14.
⎡
⎢⎢⎣
0
−1
0
0
1
0
0
0
1
0
0
−1
0
1
1
0
⎤
⎥⎥⎦.
For Problems 15–16, solve the initial-value problem.
15. x′ = Ax, x(0) = x0, where
A =
'−2
−1
1
−4
(
,
x0 =
' 0
−1
(
.
16. x′ = Ax, x(0) = x0, where
A =
⎡
⎣
−2
−1
4
0
−1
0
−1
−3
2
⎤
⎦,
x0 =
⎡
⎣
−2
1
1
⎤
⎦.
17. Show that if the vector differential equation x′ = Ax
has a solution of the form
x(t) = eλt
3
v2 + tv1 + t2
2!v2
4
,
then
(A −λI)v0 = 0,
(A −λI)v1 = v0,
and
(A −λI)v2 = v1.
18. Let A be a 2 × 2 real matrix. Prove that all solutions
to x′ = Ax satisfy
lim
t→∞x(t) = 0
if and only if all eigenvalues of A have negative real
part.
19. Extend the result of the previous exercise to the system
x′ = Ax, where A is an arbitrary (real) n × n matrix.

620
CHAPTER 9
Systems of Differential Equations
20. This problem outlines a proof of Theorem 9.5.4 using
results from the optional section on Jordan canonical
forms, Section 7.6.
(a) Conclude from the summary preceding Example
7.6.11 that there arer cycles of generalized eigen-
vectors of A corresponding to λ. Let the lengths
of these cycles be l1,l2, . . . ,lr, respectively.
(b) How are ki and li related for each i? Show that
ki ≥0 for each i and that k1 + k2 + · · · + kr =
m −r.
(c) Conclude that for each i we have a cycle of gen-
eralized eigenvectors {v(i)
0 , v(i)
1 , . . . , v(i)
ki } satisfy-
ing (9.5.15)–(9.5.19).
(d) By Theorem 7.6.10, the vectors in the cycle in
part (c) are linearly independent. Conclude that
the corresponding vector functions in (9.5.11)–
(9.5.14) are linearly independent.
(e) Show that the functions deﬁned in (9.5.11)–
(9.5.14) whose terms satisfy (9.5.15)–(9.5.19)
are solutions to the vector differential equation
x′ = Ax. This proves part (1) of Theorem 9.5.4.
(f) Deduce part (2) of Theorem 9.5.4.
9.6
Variation-of-Parameters for Linear Systems
We now consider solving the nonhomogeneous vector differential equation
x′(t) = A(t)x(t) + b(t),
(9.6.1)
where A is an n × n matrix function and b is a column n-vector function. The homoge-
neous equation associated with Equation (9.6.1) is
x′(t) = A(t)x(t).
(9.6.2)
According to Theorem 9.3.6, every solution to the system (9.6.1) is of the form
x(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t) + xp(t),
where x1, x2, . . . , xn are n linearly independent solutions to the associated homogeneous
system (9.6.2), and xp is a particular solution to (9.6.1). In this section, we derive the
variation-of-parameters method for determining xp, assuming that we know n linearly
independent solutions to (9.6.2).
Theorem 9.6.1
(Variation-of-Parameters Method)
Let A(t) be an n × n matrix function and let b(t) be a column n-vector function, both of
which are continuous on an interval I. If {x1, x2, . . . , xn} is any linearly independent set
of solutions to x′(t) = A(t)x(t) and X(t) = [x1(t), x2(t), . . . , xn(t)], then a particular
solution to
x′(t) = A(t)x(t) + b(t)
(9.6.3)
is
xp(t) = X(t)u(t),
where u(t) satisﬁes
X(t)u′(t) = b(t).
Explicitly,
xp(t) = X(t)
6 t
X−1(s)b(s)ds.

9.6
Variation-of-Parameters for Linear Systems 621
Proof The general solution to x′(t) = A(t)x(t) is
xc(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t),
which can be written in the form
xc(t) = X(t)c,
where X(t) = [x1(t), x2(t), . . . , xn(t)]andc = [c1 c2 . . . cn]T .Wetryforaparticular
solution to Equation (9.6.3) of the form
xp(t) = X(t)u(t),
(9.6.4)
where4 u(t) = [u1(t) u2(t) . . . un(t)]T . Substituting (9.6.4) into (9.6.3), it follows
that xp is a solution to (9.6.3) provided that u satisﬁes
(Xu)′ = A(Xu) + b.
(9.6.5)
Applying the product rule for differentiation to the left-hand side of Equation (9.6.5),
we obtain
X′u + Xu′ = A(Xu) + b.
(9.6.6)
By deﬁnition, we have
X = [x1, x2, . . . , xn],
so that
X′ = [x′
1, x′
2, . . . , x′
n].
(9.6.7)
Since each of the vector functions xi is a solution to the associated homogeneous equation
x′ = Ax, we can write (9.6.7) in the form
X′ = [Ax1, Ax2, . . . , Axn].
That is,
X′ = AX.
Substituting this expression for X′ into (9.6.6) yields
(AX)u + Xu′ = A(Xu) + b,
so that
Xu′ = b.
This implies that5
u′ = X−1b.
Consequently,
u(t) =
6 t
X−1(s)b(s)ds,
(we have set the integration constant to zero without loss of generality) and hence, from
(9.6.4), a particular solution to (9.6.3) is
xp(t) = X(t)
6 t
X−1(s)b(s)ds.
4That is, we replace the constants in xc by arbitrary functions.
5Note that X−1 exists, since det(X) ̸= 0. (Why?)

622
CHAPTER 9
Systems of Differential Equations
Remarks
1. There is no need to memorize the formula for xp given in the previous theorem.
Rather, it should be remembered that a particular solution to x′ = Ax + b is
xp(t) = Xu,
where X is a fundamental matrix for the associated homogeneous vector differen-
tial equation and u′ is determined by solving the linear system
Xu′ = b.
(9.6.8)
2. Whereas the proof of the previous theorem used X−1 to obtain a simple formula
for the solution to (9.6.8), in practice any of the methods for solving systems
of linear equations that we have derived in the text can be applied. For 2 × 2
systems, Cramer’s rule is quite effective. Alternatively, the inverse of X can be
determined very quickly using the adjoint method. For systems bigger than 2 × 2,
it is computationally more efﬁcient to use Gaussian elimination to solve (9.6.8)
for u′ and then integrate the resulting vector to determine u.
Example 9.6.2
Solve the initial-value problem
x′(t) = A(t)x(t) + b(t),
x(0) =
'3
0
(
,
if
A =
'1
2
4
3
(
and
b(t) =
'12e3t
18e2t
(
.
Solution:
We ﬁrst solve the associated homogeneous equation x′(t) = A(t)x(t). For
the given matrix A, we ﬁnd that
det(A −λI) = (λ −5)(λ + 1),
so that the eigenvalues of A are λ1 = −1 and λ2 = 5. A quick calculation shows that
the corresponding eigenvectors are, respectively,
v1 =
'−1
1
(
and v2 =
'1
2
(
.
Consequently, two linearly independent solutions to x′ = Ax are
x1(t) = e−t
'−1
1
(
,
x2(t) = e5t
'1
2
(
,
and therefore a fundamental matrix for x′ = Ax is
X(t) =
'−e−t
e5t
e−t
2e5t
(
.
It follows from Theorem 9.6.1 that a particular solution to x′ = Ax + b is
xp = Xu,
where
Xu′ = b.
Since this is a 2 × 2 system, we will solve for the components of u′ using Cramer’s rule.
We have
det(X(t)) = −3e4t,

9.6
Variation-of-Parameters for Linear Systems 623
so that
u′
1(t) =
12e3t
e5t
18e2t
2e5t
−3e4t
= −8e4t + 6e3t
and
u′
2(t) =
−e−t
12e3t
e−t
18e2t
−3e4t
= 6e−3t + 4e−2t.
Integrating these two expressions yields
u1(t) = −2e4t + 2e3t
and
u2(t) = −2e−3t −2e−2t,
where we have set the integration constants to zero. Hence,
u(t) =
' −2e4t + 2e3t
−2e−2t −2e−3t
(
.
It follows that a particular solution to the given vector differential equation is
xp(t) = X(t)u(t) =
'−e−t
e5t
e−t
2e5t
( ' −2e4t + 2e3t
−2e−2t −2e−3t
(
=
'
−4e2t
−2e2t −6e3t
(
.
Consequently, the general solution to the given nonhomogeneous vector differential
equation is
x(t) = c1e−t
'−1
1
(
+ c2e5t
'1
2
(
−
'
4e2t
2e2t + 6e3t
(
,
or equivalently,
x(t) =
'
−c1e−t + c2e5t −4e2t
c1e−t + 2c2e5t −2(e2t + 3e3t)
(
.
(9.6.9)
The initial condition x(0) =
'3
0
(
requires that
c1
'−1
1
(
+ c2
'1
2
(
−
'4
8
(
=
'3
0
(
,
which yields the equations
−c1 + c2 = 7
and
c1 + 2c2 = 8.
Thus, c1 = −2 and c2 = 5. Substituting these values into (9.6.9) yields
x(t) =
'
2e−t + 5e5t −4e2t
−2e−t + 10e5t −2(e2t + 3e3t)
(
.
□

624
CHAPTER 9
Systems of Differential Equations
Exercises for 9.6
Key Terms
Variation-of-parameters, Particular solution to a nonhomo-
geneous vector differential equation.
Skills
• Be able to use the variation-of-parameters technique
to ﬁnd a particular solution xp to the vector differential
equation x′(t) = A(t)x(t) + b(t) for a given matrix
function A and column vector function b(t).
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) To apply the variation-of-parameters technique for de-
terminingaparticularsolutiontothenonhomogeneous
vector differential equation x′ = Ax + b, it is not nec-
essary to determine any solutions to the corresponding
homogeneous vector differential equation x′ = Ax.
(b) If X(t) is a fundamental matrix for x′ = Ax, then a
particular solution to the nonhomogeneous vector dif-
ferential equation x′ = Ax + b is xp(t) = X(t)u(t),
where u(t) is any arbitrary vector function.
(c) If X(t) is a fundamental matrix for x′ = Ax, then
X′(t) = A(t)X(t).
(d) If xp is a particular solution to the vector differential
equation x′ = Ax+b, then so is c·xp for any nonzero
constant c.
(e) There is only one particular solution xp that satis-
ﬁes the nonhomogeneous vector differential equation
x′ = Ax + b.
(f) Theparametersu(t)intheequationxp(t) = X(t)u(t),
where X(t) is a fundamental matrix for x′ = Ax, are
determined by solving the system X(t)u′(t) = b(t)
for u′(t) and integrating the resulting vector function.
Problems
For Problems 1–9, use the variation-of-parameters technique
to ﬁnd a particular solution xp to x′ = Ax + b, for the given
A and b. Also obtain the general solution to the system of
differential equations.
1. A =
'4
−3
2
−1
(
,
b =
'
e2t
et
(
.
2. A =
' 2
−1
−1
2
(
,
b =
' 0
4et
(
.
3. A =
'3
1
0
3
(
,
b =
'te3t
e3t
(
.
4. A =
'−1
1
3
1
(
,
b =
'
20e3t
12et
(
.
5. A =
'−1
2
−2
4
(
,
b =
'54te3t
9e3t
(
.
6. A =
' 2
4
−2
−2
(
,
b =
'8 sin 2t
8 cos 2t
(
.
7. A =
' 3
2
−2
−1
(
,
b =
'−3et
6tet
(
.
8. A =
⎡
⎣
1
0
0
2
−3
2
1
−2
2
⎤
⎦,
b =
⎡
⎣
−et
6e−t
et
⎤
⎦.
9. A =
⎡
⎣
−1
−2
2
2
4
−1
0
0
3
⎤
⎦,
b =
⎡
⎣
−e3t
4e3t
3e3t
⎤
⎦.
10. Let X(t) be a fundamental matrix for the system
x′ = A(t)x(t), where A(t) is an n×n matrix function.
Show that the solution to the initial-value problem
x′(t) = A(t)x(t) + b(t),
x(t0) = x0
can be written as
x(t) = X(t)X−1(t0)x0 + X(t)
6 t
t0
X−1(s)b(s)ds.
11. Consider the nonhomogeneous system
x′
1 =
2x1 −3x2 + 34 sin t,
x′
2 = −4x1 −2x2 + 17 cos t.
Find the general solution to this system by ﬁrst solv-
ing the associated homogeneous system, and then us-
ing the method of undetermined coefﬁcients to obtain
a particular solution.
[Hint: The form of the nonhomogeneous term sug-
gests a trial solution of the form
xp(t) =
'A1 cos t + B1 sin t
A2 cos t + B2 sin t
(
,
where the constants A1, A2, B1, and B2 can be deter-
mined by substituting into the given system.]

9.7
Some Applications of Linear Systems of Differential Equations 625
9.7
Some Applications of Linear Systems of Differential Equations
In this section, we analyze the two problems that were brieﬂy introduced at the beginning
of this chapter. We begin with the coupled spring-mass system that consists of two masses
m1 and m2 connected by two springs whose spring constants are k1 and k2, respectively.
(See Figure 9.7.1.) Let x(t) and y(t) denote the displacement of m1 and m2 from their
equilibrium positions, respectively. When the system is in motion, the extension of
spring 1 is
L1(t) = x(t),
whereas the net extension of spring 2 is
L2(t) = y(t) −x(t).
Consequently, using Hooke’s law, the net forces acting on masses m1 and m2 at time t
are
F1(t) = −k1x(t) + k2[y(t) −x(t)] and
F2(t) = −k2[y(t) −x(t)],
k1
k2
m1
m2
x 5 0
y 5 0
x(t)
y(t)
Positive x, y
Figure 9.7.1: A coupled spring-mass system.
respectively. Thus, applying Newton’s second law to each mass yields the system of
differential equations
m1
d2x
dt2 = −k1x + k2(y −x),
(9.7.1)
m2
d2y
dt2 = −k2(y −x).
(9.7.2)
The motion of the spring-mass system will be fully determined once we have speciﬁed
appropriate initial conditions of the form
x(t0) = α1,
dx
dt (t0) = α2,
y(t0) = α3,
dy
dt (t0) = α4,
(9.7.3)
where α1, α2, α3, and α4 are constants.
To apply the techniques that we have developed in this chapter for solving systems
of differential equations, we must convert Equations (9.7.1) and (9.7.2) into a ﬁrst-order

626
CHAPTER 9
Systems of Differential Equations
system. We introduce new variables x1, x2, x3, and x4 deﬁned by
x1 = x,
x2 = x′,
x3 = y,
x4 = y′.
(9.7.4)
Using (9.7.4), we can replace Equations (9.7.1) and (9.7.2) by the equivalent system
x′
1 = x2,
x′
2 = −k1
m1
x1 + k2
m1
(x3 −x1),
x′
3 = x4,
x′
4 = −k2
m2
(x3 −x1).
Rearranging terms yields the ﬁrst-order linear system
x′
1 = x2,
(9.7.5)
x′
2 = −
3 k1
m1
+ k2
m1
4
x1 + k2
m1
x3,
(9.7.6)
x′
3 = x4,
(9.7.7)
x′
4 = k2
m2
x1 −k2
m2
x3.
(9.7.8)
In the new variables, the initial conditions (9.7.3) are
x1(t0) = α1,
x2(t0) = α2,
x3(t0) = α3,
x4(t0) = α4.
This initial-value problem for x1, x2, x3, x4 can be written in vector form as
x′ = Ax,
x(t0) = x0,
where
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦,
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
0
−1
m1
(k1 + k2)
0
k2
m1
0
0
0
0
1
k2
m2
0
−k2
m2
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
x0 =
⎡
⎢⎢⎢⎣
α1
α2
α3
α4
⎤
⎥⎥⎥⎦.
We leave the analysis of the general system for the exercises and consider a particular
example.
Example 9.7.1
Consider the spring-mass system with
k1 = 4 Nm−1,
k2 = 2 Nm−1,
m1 = 2 kg,
m2 = 1 kg.
At t = 0, both masses are pulled down a distance 1 meter from equilibrium and released
from rest. Determine the subsequent motion of the system.
Solution:
The motion of the system is governed by the initial-value problem
2d2x
dt2 = −4x + 2(y −x),
d2y
dt2 = −2(y −x),
x(0) = 1,
dx
dt (0) = 0,
y(0) = 1,
dy
dt (0) = 0.

9.7
Some Applications of Linear Systems of Differential Equations 627
Introducing new variables x1 = x, x2 = x′, x3 = y, and x4 = y′ yields the equivalent
initial-value problem
x′
1 = x2,
x′
2 = −3x1 + x3,
x′
3 = x4,
x′
4 = 2x1 −2x3,
x1(0) = 1,
x2(0) = 0,
x3(0) = 1,
x4(0) = 0.
In vector form, we have
x′ = Ax,
x(0) = x0,
(9.7.9)
where
A =
⎡
⎢⎢⎣
0
1
0
0
−3
0
1
0
0
0
0
1
2
0
−2
0
⎤
⎥⎥⎦
and x0 =
⎡
⎢⎢⎣
1
0
1
0
⎤
⎥⎥⎦.
The characteristic polynomial of A is6
det(A −λI) =
−λ
1
0
0
−3
−λ
1
0
0
0
−λ
1
2
0
−2
−λ
= λ4 + 5λ2 + 4
= (λ2 + 1)(λ2 + 4).
Thus, the eigenvalues of A are
λ = ±i, ±2i.
We now determine the eigenvectors.
Eigenvalue λ = i: The system (A −λI)v1 = 0 has augmented matrix
⎡
⎢⎢⎣
−i
1
0
0
0
−3
−i
1
0
0
0
0
−i
1
0
2
0
−2
−i
0
⎤
⎥⎥⎦
with reduced row-echelon form
⎡
⎢⎢⎣
1
0
0
i/2
0
0
1
0
−1/2
0
0
0
1
i
0
0
0
0
0
0
⎤
⎥⎥⎦.
Consequently, the eigenvectors are
v1 = r(−i, 1, −2i, 2),
so that a complex-valued solution to x′ = Ax is
u1(t) = eit
⎡
⎢⎢⎣
−i
1
−2i
2
⎤
⎥⎥⎦= (cos t + i sin t)
⎡
⎢⎢⎣
−i
1
−2i
2
⎤
⎥⎥⎦.
6In Problem 1, the reader is asked to ﬁll in the missing details of this computation.

628
CHAPTER 9
Systems of Differential Equations
Taking the real and imaginary parts of this complex-valued solution yields the two
linearly independent real-valued solutions
x1(t) =
⎡
⎢⎢⎣
sin t
cos t
2 sin t
2 cos t
⎤
⎥⎥⎦
and
x2(t) =
⎡
⎢⎢⎣
−cos t
sin t
−2 cos t
2 sin t
⎤
⎥⎥⎦.
Eigenvalue λ = 2i: The augmented matrix of the system (A −λI)v2 = 0 in this case
is
⎡
⎢⎢⎣
−2i
1
0
0
0
−3
−2i
1
0
0
0
0
−2i
1
0
2
0
−2
−2i
0
⎤
⎥⎥⎦
with reduced row-echelon form
⎡
⎢⎢⎣
1
0
0
−i/2
0
0
1
0
1
0
0
0
1
i/2
0
0
0
0
0
0
⎤
⎥⎥⎦.
The corresponding eigenvectors are therefore of the form
v2 = s(i, −2, −i, 2),
so that a complex-valued solution to the system x′ = Ax is
u2(t) = e2it
⎡
⎢⎢⎣
i
−2
−i
2
⎤
⎥⎥⎦= (cos 2t + i sin 2t)
⎡
⎢⎢⎣
i
−2
−i
2
⎤
⎥⎥⎦.
Taking the real and imaginary parts of this complex-valued solution yields the additional
real-valued linearly independent solutions
x3(t) =
⎡
⎢⎢⎣
−sin 2t
−2 cos 2t
sin 2t
2 cos 2t
⎤
⎥⎥⎦
and
x4(t) =
⎡
⎢⎢⎣
cos 2t
−2 sin 2t
−cos 2t
2 sin 2t
⎤
⎥⎥⎦.
Consequently, the vector differential equation in (9.7.9) has general solution
x(t) = c1
⎡
⎢⎢⎣
sin t
cos t
2 sin t
2 cos t
⎤
⎥⎥⎦+ c2
⎡
⎢⎢⎣
−cos t
sin t
−2 cos t
2 sin t
⎤
⎥⎥⎦+ c3
⎡
⎢⎢⎣
−sin 2t
−2 cos 2t
sin 2t
2 cos 2t
⎤
⎥⎥⎦+ c4
⎡
⎢⎢⎣
cos 2t
−2 sin 2t
−cos 2t
2 sin 2t
⎤
⎥⎥⎦.
Combining the vector functions yields the solution vector
x(t) =
⎡
⎢⎢⎢⎣
c1 sin t −c2 cos t −c3 sin 2t + c4 cos 2t
c1 cos t + c2 sin t −2c3 cos 2t −2c4 sin 2t
2c1 sin t −2c2 cos t + c3 sin 2t −c4 cos 2t
2(c1 cos t + c2 sin t + c3 cos 2t + c4 sin 2t)
⎤
⎥⎥⎥⎦.

9.7
Some Applications of Linear Systems of Differential Equations 629
Imposing the initial condition x(0) =
⎡
⎢⎢⎣
1
0
1
0
⎤
⎥⎥⎦, we ﬁnd that c1 = 0, c2 = −2
3, c3 = 0,
c4 = 1
3. Thus,
x(t) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
3(2 cos t + cos 2t)
−2
3(sin t + sin 2t)
1
3(4 cos t −cos 2t)
2
3(−2 sin t + sin 2t)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Since x = x1 and y = x3, it follows that the motion of the spring-mass system is given
by
x(t) = 1
3(2 cos t + cos 2t),
y(t) = 1
3(4 cos t −cos 2t).
The motion of both masses is periodic, with period 2π. Looking at the second and fourth
components of the solution vector x (or by differentiating the previous expressions for
x and y), we see that
x′(t) = −2
3(sin t + sin 2t) = −2
3(1 + 2 cos t) sin t,
y′(t) = 2
3(−2 sin t + sin 2t) = 4
3(cos t −1) sin t.
Consequently, on the interval [0, 2π], x′ has zeros when t = 0, 2π/3, π, 4π/3, and 2π,
whereas the only zeros of y′ are 0, π, and 2π. Notice that both y′′ and y′′′ vanish at t = 0
and t = 2π. Hence, the graph of y is very ﬂat in the neighborhood of these points. This
motion is depicted in Figure 9.7.2.
2
4
6
8
10
0.5
1
—0.5
—1
—1.5
x(t), y(t)
t
y(t)
x(t)
Figure 9.7.2: The solutions for the spring-mass system in Example 9.7.1.
□
Next consider the mixing problem depicted in Figure 9.7.3. Two tanks contain a
solution consisting of chemical dissolved in water. A solution containing cin g/L of
chemical ﬂows into tank 1 at a rate of rin L/min and solution of concentration cout g/L

630
CHAPTER 9
Systems of Differential Equations
rin
cin
A1
A2
r21
c21
rout
cout
r12
c12
Tank 2
Tank 1
Figure 9.7.3: A general mixing problem.
ﬂows out of tank 2 at a rate of rout L/min. In addition, solution of concentration c12 g/L
ﬂows into tank 1 from tank 2 at a rate of r12 L/min and solution of concentration
c21 g/L ﬂows into tank 2 from tank 1 at a rate of r21 L/min. We wish to determine
A1(t) and A2(t), the amounts of chemical in tank 1 and tank 2, respectively. The analy-
sis is similar to that used in Section 9.7. Assuming that the solution in each tank is well
mixed, it follows immediately that
c12 = cout = A2
V2
,
c21 = A1
V1
,
where Vi denotes the volume of solution in tank i at time t. Consider a short time interval
%t. The total amount of chemical entering tank 1 in this time interval is approximately
(cinrin + c12r12)%t grams,
whereas approximately
c21r21%t grams
of chemical leave tank 1 in the same time interval. Consequently, the change in the
amount of chemical in tank 1 in the time interval %t, denoted %A1, is approximately
%A1 ≈[(cinrin + c12r12) −c21r21]%t;
that is,
%A1 ≈
'
cinrin + r12
A2
V2
−r21
A1
V1
(
%t.
(9.7.10)
Similarly, the change in the amount of chemical in tank 2 in the time interval %t, denoted
%A2, is approximately
%A2 ≈[r21c21 −(r12c12 + routcout)]%t
or, equivalently,
%A2 ≈
'
r21
A1
V1
−(r12 + rout) A2
V2
(
%t.
(9.7.11)
Dividing Equations (9.7.10) and (9.7.11) by %t and taking the limit as %t →0+ yields
the following system of differential equations for A1 and A2:
d A1
dt
= −r21
A1
V1
+ r12
A2
V2
+ cinrin,
d A2
dt
=
r21
A1
V1
−(r12 + rout) A2
V2
.

9.7
Some Applications of Linear Systems of Differential Equations 631
We will now assume that V1 and V2 are constant. This imposes the conditions
rin + r12 −r21 = 0,
r21 −r12 −rout = 0.
(See Problem 7.) Consequently, the foregoing system of differential equations reduces to
d A1
dt
= −r21
V1
A1 + r12
V2
A2 + cinrin,
d A2
dt
=
r21
V1
A1 −r21
V2
A2.
This is a constant coefﬁcient system for A1 and A2, and therefore it can be solved using
the techniques that we have developed in this chapter.
Example 9.7.2
Two tanks each contain 20 L of a solution consisting of salt dissolved in water. (See
Figure 9.7.4.) A solution containing 4 g/L of salt ﬂows into tank 1 at a rate of 3 L/min
and the solution in tank 2 ﬂows out at the same rate. In addition, solution ﬂows into tank
1 from tank 2 at a rate of 1 L/min and into tank 2 from tank 1 at a rate of 4 L/min. Initially
tank 1 contained 40 g of salt and tank 2 contained 20 g of salt. Find the amount of salt
in each tank at time t.
A1
A2
Tank 2
Tank 1
3 L/min
4 g/L
4 L/min
1 L/min
3 L/min
Figure 9.7.4: The mixing problem considered in Example 9.7.2.
Solution:
The inﬂow and outﬂow rates from each tank are indicated in Figure 9.7.4.
We notice that the total amount of solution ﬂowing into tank 1 is 4 L/min, and the
same volume of solution ﬂows out of tank 1 per minute. Consequently, the volume of
solution in tank 1 remains constant at 20 L. The same is true for tank 2. Let A1(t) and
A2(t) denote the amounts of salt in tanks 1 and 2, respectively, and let ci j denote the
concentration of salt in the solution ﬂowing into tank i from tank j. Now consider a
short time interval %t. The overall change in the amount of salt in tank 1 in this time
interval %t is approximately
%A1 ≈(12 + 1 · c12)%t −4c21%t;
that is,
%A1 ≈
3
12 + 1
20 A2 −1
5 A1
4
%t.
(9.7.12)
A similar analysis of the change in the amount of salt in tank 2 in the time interval %t
yields
%A2 ≈
31
5 A1 −1
20 A2 −3
20 A2
4
%t;

632
CHAPTER 9
Systems of Differential Equations
that is,
%A2 ≈
31
5 A1 −1
5 A2
4
%t.
(9.7.13)
Dividing Equations (9.7.12) and (9.7.13) by %t and taking the limit as %t →0+ yields
the system of differential equations
d A1
dt
= −1
5 A1 + 1
20 A2 + 12,
d A2
dt
=
1
5 A1 −1
5 A2.
We are also given the initial conditions
A1(0) = 40 and
A2(0) = 20.
In vector form, we must therefore solve the initial-value problem
x′ = Ax + b,
x(0) = x0,
where
x =
'A1
A2
(
,
A =
'−1/5
1/20
1/5
−1/5
(
,
b =
'12
0
(
,
x0 =
'40
20
(
.
The characteristic polynomial of A is
det(A −λI) = −1/5 −λ
1/20
1/5
−1/5 −λ =
3
λ + 1
5
42
−
1
100.
Consequently, the eigenvalues of A are
λ = −1
5 ± 1
10.
That is,
λ1 = −1
10
and λ2 = −3
10.
The corresponding eigenvectors are
v1 = (1, 2) and v2 = (1, −2),
respectively, so that two linearly independent solutions to x′ = Ax are
x1(t) = e−t/10
'1
2
(
and x2(t) = e−3t/10
' 1
−2
(
.
Thus,
xc(t) = c1e−t/10
'1
2
(
+ c2e−3t/10
' 1
−2
(
.
We now need a particular solution to x′ = Ax + b. According to the variation-of-
parameters technique, a particular solution is
xp = Xu,

9.7
Some Applications of Linear Systems of Differential Equations 633
where
Xu′ = b
(9.7.14)
and
X(t) =
' e−t/10
e−3t/10
2e−t/10
−2e−3t/10
(
.
The system (9.7.14) is
e−t/10u′
1 + e−3t/10u′
2 = 12,
e−t/10u′
1 −e−3t/10u′
2 = 0,
which has solution
u′
1 = 6et/10
and
u′
2 = 6e3t/10.
By integrating, we obtain
u1(t) = 60et/10
and
u2(t) = 20e3t/10,
where we have set the integration constants to zero without loss of generality. Conse-
quently,
xp(t) =
' e−t/10
e−3t/10
2e−t/10
−2e−3t/10
( ' 60et/10
20e3t/10
(
=
'80
80
(
.
Hence, the general solution to the system x′ = Ax + b is
x(t) = c1e−t/10
'1
2
(
+ c2e−3t/10
' 1
−2
(
+
'80
80
(
.
That is,
x(t) =
' c1e−t/10 + c2e−3t/10 + 80
2c1e−t/10 −2c2e−3t/10 + 80
(
.
Imposing the initial condition x(0) =
'40
20
(
requires
' c1 + c2 + 80
2c1 −2c2 + 80
(
=
'40
20
(
.
We quickly solve this for c1 and c2: c1 = −35 and c2 = −5. Thus, the solution to the
initial-value problem is
x(t) = −35e−t/10
'1
2
(
−5e−3t/10
' 1
−2
(
+
'80
80
(
.
Consequently, the amounts of salt in tanks 1 and 2 at time t are, respectively,
A1(t) = 80 −35e−t/10 −5e−3t/10,
A2(t) = 80 −70e−t/10 + 10e−3t/10.
We see that both A1 and A2 approach the constant value of 80 grams as t →∞. Why
is this a reasonable result?
□

634
CHAPTER 9
Systems of Differential Equations
Exercises for 9.7
Key Terms
Coupled spring-mass system, Mixing problem.
Skills
• Be able to determine the motion of a coupled spring-
mass system as a function of time.
• Be able to solve mixing problems to determine the
amounts (or concentrations) of chemical present in a
system of tanks as a function of time.
True-False Review
For Questions (a)–(e), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A coupled spring-mass system consisting of two
masses and two springs can be solved as a ﬁrst-order
linear system x′ = Ax, where A is a 2 × 2 matrix
whose entries are determined by the masses and spring
constants.
(b) The units of the spring constant in the metric system
are Newtons per meter.
(c) In the metric system, the concentration of chemical in
a tank of solution is measured in grams.
(d) In a chemical mixing problem involving two tanks, the
rate at which ﬂuid moves from tank 1 to tank 2 must
be the same as the rate at which ﬂuid moves from tank
2 to tank 1.
(e) In a chemical mixing problem with two tanks in which
no solution ﬂows in or out of the system from the out-
side and the rates of ﬂow between the two tanks r12
and r21 are equal, the amount of chemical in each tank
remains constant over time, regardless of the initial
conditions.
Problems
1. Derive the eigenvalues and eigenvectors given in
Example 9.7.1.
2. Determine the motion of the coupled spring-mass sys-
tem which has
k1 = 3 Nm−1,
k2 = 1
2 Nm−1,
m1 = 1
2 kg,
m2 = 1
12 kg
given that at t = 0 both masses are set in motion from
their equilibrium positions with a velocity of 1 m/s.
3. Determine the general motion of the coupled spring-
mass system that has
k1 = 3 Nm−1,
k2 = 4 Nm−1,
m1 = 1 kg,
m2 = 4/3 kg.
4. Determine the general motion of the coupled spring-
mass system which has
k1 = 2k2,
m1 = 2m2.
[Hint: Let ω2 = k2/(2m2).]
5. Consider the general coupled spring-mass system
whose motion is governed by the system (9.7.5)–
(9.7.8). Show that the coefﬁcient matrix of the system
has characteristic equation
λ4 +
' k2
m2
+ (k1 + k2)
m1
(
λ2 + k1k2
m1m2
= 0
and that the corresponding eigenvalues are of the form
λ = ±iω1, ±iω2,
where ω1 and ω2 are positive real numbers.
6. Two masses m1 and m2 rest on a horizontal friction-
less plane. The masses are attached to ﬁxed walls by
springs whose spring constants are k1 and k3. (See
Figure 9.7.5.) The masses are connected by a spring
whose spring constant is k2. Determine a ﬁrst-order
system of differential equations that governs the mo-
tion of the system.
k1
m1
k2
k3
m2
x
y
Figure 9.7.5: Three-spring system.
7. Show that the assumption that V1 and V2 are constant
in the general mixing problem considered in the text
imposes the conditions
rin + r12 = r21,
r21 −r12 = rout.

9.8
Matrix Exponential Function and Systems of Differential Equations 635
8. Solve the initial-value problem arising in Exam-
ple 9.7.2 using the technique derived in Section 9.1.
9. Solve the mixing problem depicted in Figure 9.7.6,
A1
A2
Tank 2
Tank 1
6 L/min
2 g/L
8 L/min
2 L/min
6 L/min
Figure 9.7.6: Mixing problem in Problem 9.
given that at t = 0, the volume of solution in both
tanks is 60 L, and tank 1 contains 60 grams of chemi-
cal whereas tank 2 contains 200 grams of chemical.
10. In the mixing problem shown in Figure 9.7.7, there is
no inﬂow from or outﬂow to the outside. For this rea-
son, the system is said to be closed. If tank 1 contains
6 L of solution and tank 2 contains 12 L of solution,
determine the amount of chemical in each tank at time
t, given that initially tank 1 contains 5 grams of chem-
ical and tank 2 contains 25 grams of chemical.
A1
A2
2 L/min
2 L/min
Tank 1
Tank 2
Figure 9.7.7: Mixing problem in Problem 10.
11. Consider the general closed system depicted in Fig-
ure 9.7.8.
A1
A2
r21
r21
Tank 1
Tank 2
Figure 9.7.8: Mixing problem in Problem 11.
(a) Derive the system of differential equations that
governs the behavior of A1 and A2.
(b) Deﬁne the constant β by V2 = βV1, where V1
and V2 denote the volume of solution in tank 1
and tank 2, respectively. Show that the eigenval-
ues of the coefﬁcient matrix of the system derived
in (a) are
λ1 = 0 and λ2 = −1 + β
βV1
r21.
(c) Determine A1 and A2, given that A1(0) = α1
and A2(0) = α2, where α1 and α2 are positive
constants.
(d) Show that
lim
t→+∞
A1
V1
=
lim
t→+∞
A2
V2
=
α1 + α2
(1 + β)V1
.
Is this result reasonable?
9.8
Matrix Exponential Function and Systems of Differential Equations
The matrix exponential function was ﬁrst introduced in Section 7.4. Recall that for an
n × n matrix A, the matrix exponential function is deﬁned by
eAt = In + At + 1
2!(At)2 + 1
3!(At)3 + · · · + 1
k!(At)k + · · · ,
(9.8.1)
where the inﬁnite series here can be shown to converge for all real numbers t. In this
section, we investigate the relationship between the matrix exponential function eAt and
the solutions to the corresponding vector differential equation
x′ = Ax.
We begin by deﬁning the derivative of eAt. It can be shown that the inﬁnite series (9.8.1)
deﬁning eAt can be differentiated term by term and the resulting series converges for all

636
CHAPTER 9
Systems of Differential Equations
t ∈(−∞, ∞). Thus, through differentiating (9.8.1), we have
d
dt (eAt) = A + A2t + 1
2! A3t2 + · · · +
1
(k −1)! Aktk−1 + · · · .
That is,
d
dt (eAt) = A
'
I + At + 1
2!(At)2 + 1
3!(At)3 + · · · + 1
k!(At)k + · · ·
(
.
Hence,
d
dt (eAt) = AeAt.
(9.8.2)
Now recall from Chapter 1 that for all values of the constants a and x0, the unique
solution to the initial-value problem
dx
dt = ax,
x(0) = x0
is
x(t) = x0eat.
Our next theorem shows that the same formula holds for the vector differential equation
x′ = Ax,
provided we replace eat by eAt. This is a very elegant result that has far-reaching conse-
quences in both the computation of eAt and the analysis of vector differential equations.
Theorem 9.8.1
Let x0 be an arbitrary vector. Then the unique solution to the initial-value problem
x′ = Ax,
x(0) = x0
is
x(t) = eAtx0.
Proof If x(t) = eAtx0, then by (9.8.2), we have x′(t) = AeAtx0. That is,
x′ = Ax.
Further, setting t = 0,
x(0) = e0·Ax0 = Ix0 = x0.
Consequently, x(t) = eAtx0 is a solution to the given initial-value problem. The unique-
ness of the solution follows from Theorem 9.3.1.
We now investigate how the result of Theorem 9.8.1, combined with our previ-
ous techniques for solving x′ = Ax, can be used to determine eAt. To this end, let
x1, x2, . . . , xn be linearly independent solutions to the vector differential equation
x′ = Ax,
(9.8.3)
where A is an n×n matrix of constants. We recall from Section 9.3 that the corresponding
matrix function
X(t) = [x1, x2, . . . , xn]

9.8
Matrix Exponential Function and Systems of Differential Equations 637
is called a fundamental matrix for (9.8.3) and that the general solution to (9.8.3) can be
written as
x(t) = X(t)c,
where c is a column vector of arbitrary constants. If X(t) is any fundamental matrix for
(9.8.3) and B is any invertible matrix of constants, then the matrix function
Y(t) = X(t)B
is also a fundamental matrix for (9.8.3), since its columns are linear combinations of the
column vectors of X and hence are linearly independent solutions of (9.8.3). (The linear
independence follows since Y(t) is invertible.) We focus our attention on a particular
fundamental matrix.
DEFINITION
9.8.2
The unique fundamental matrix for x′ = Ax that satisﬁes
X(0) = In
is called the transition matrix for x′ = Ax based at t = 0, and it is denoted by X0(t).
In terms of the transition matrix, the solution to the initial-value problem
x′ = Ax,
x(0) = x0
is just
x(t) = X0(t)x0,
so that the transition matrix does indeed describe the transition of the system from its
state at time t = 0 to its state at time t. Further, if X(t) is any fundamental matrix for
x′ = Ax, then the transition matrix can be determined from (see Problem 1)
X0(t) = X(t)X−1(0).
(9.8.4)
We now prove that X0(t) is in fact eAt. From Equation (9.8.2), we have
d
dt (eAt) = AeAt,
so that the column vectors of eAt are solutions to x′ = Ax. Further, setting t = 0 yields
e0·A = In,
(9.8.5)
which implies that
det(e0·A) = 1 ̸= 0.
Hence, the column vectors of eAt are linearly independent on any interval. Consequently,
eAt is a fundamental matrix for x′ = Ax. Finally, combining (9.8.5) with the uniqueness
of the transition matrix leads to the required conclusion, namely
eAt = X0(t).
(9.8.6)
Thus,if A isann×n matrixand X(t)isanyfundamentalmatrixforthecorresponding
vector differential equation x′ = Ax, then Equations (9.8.4) and (9.8.6) imply that
eAt = X(t)X−1(0).
(9.8.7)

638
CHAPTER 9
Systems of Differential Equations
Consequently, to determine eAt, we can use the techniques from Sections 9.3 and 9.4
to ﬁnd a fundamental matrix for x′ = Ax, and then eAt can be obtained directly from
Equation (9.8.7).
Example 9.8.3
Determine eAt if A =
'6
−8
2
−2
(
.
Solution:
We ﬁrst ﬁnd a fundamental matrix for
x′ = Ax.
This system has been solved in Example 9.5.2, where it was found that two linearly
independent solutions are7
x1(t) = e2t
'2
1
(
and x2(t) = e2t
'1 + 4t
2t
(
.
Thus, a fundamental matrix for x′ = Ax is
X(t) =
'2e2t
(1 + 4t)e2t
e2t
2te2t
(
,
whose inverse at t = 0 is
X−1(0) =
'0
1
1
−2
(
.
Consequently,
eAt = X(t)X−1(0) =
'2e2t
(1 + 4t)e2t
e2t
2te2t
( '0
1
1
−2
(
.
That is,
eAt =
'(1 + 4t)e2t
−8te2t
2te2t
(1 −4t)e2t
(
,
which can be written as
eAt = e2t
'1 + 4t
−8t
2t
1 −4t
(
.
□
We have seen how to take a set of n linearly independent solutions to x′ = Ax,
where A is an n × n matrix, and compute the matrix exponential function eAt. We now
indicate how this process can be reversed. That is, given the matrix exponential function
eAt, we derive n linearly independent solutions to x′ = Ax as follows.
We begin by writing
A = λI + (A −λI),
where λ is a (possibly complex) scalar. By writing B = λI and C = A −λI and noting
that BC = C B, it follows from property (1) of the matrix exponential function (see
Section 7.4) that for every v in Cn
eAtv = e[λIt+(A−λI)t]v = eλIte(A−λI)tv.
7Notice that in this example A is defective.

9.8
Matrix Exponential Function and Systems of Differential Equations 639
Further, by Problem 8 in Section 7.4,
eλIt = diag(eλt, eλt, . . . , eλt) = eλt I,
so that
eAtv = eλt
'
v + t(A −λI)v + t2
2!(A −λI)2v + . . .
(
.
(9.8.8)
Theorem 9.8.1 guarantees that eAtv is a solution to x′ = Ax, but in general, the preceding
series for eAtv contains an inﬁnite number of terms and hence is intractable. However,
if we can ﬁnd vectors v such that
(A −λI)kv = 0
(9.8.9)
for some positive integer k, then the series will terminate after a ﬁnite number of terms.
Nonzero vectors v satisfying (9.8.9) for some positive k were introduced in Section 7.6 as
generalized eigenvectors of A, and as was indicated there, if λ occurs with multiplicity
m in the characteristic polynomial of A, then A has m linearly independent generalized
eigenvectors corresponding to λ. Proceeding with each of these generalized eigenvectors
v, we get a solution to x′ = Ax in the form
eAtv = eλt
'
v + t(A −λI)v + t2
2!(A −λI)2v + · · · +
tk−1
(k −1)!(A −λI)k−1v
(
,
each of which is a solution with a ﬁnite number of terms. Therefore, we have obtained m
linearly independent solutions to x′ = Ax corresponding to λ. Proceeding in this manner
for each eigenvalue, we can obtain n linearly independent solutions to x′ = Ax. This
will determine a fundamental matrix for x′ = Ax from which eAt can be determined in
the usual manner.
Remark
Note, too, that for each v ̸= 0 such that (9.8.9) holds with k = 1, v is an
eigenvector of A and the series (9.8.8) has only one term. In that case, we obtain the
result of Theorem 9.4.1, namely, that eAtv = eλtv is a solution to x′ = Ax whenever λ
and v are an eigenvalue-eigenvector pair for A. Hence, if A is nondefective, Equation
(9.8.8) yields n linearly independent solutions to x′ = Ax in the usual manner.
Example 9.8.4
Let A =
⎡
⎣
6
8
1
−1
−3
3
−1
−1
1
⎤
⎦.
(a) Determine a fundamental matrix for x′ = Ax, and thereby determine the general
solution to the vector differential equation.
(b) Determine eAt.
Solution:
(a) The characteristic polynomial of A is
p(λ) = −(λ −3)2(λ + 2).
Hence, A has eigenvalues λ1 = 3 (multiplicity 2), and λ2 = −2.
Eigenvalue λ1 = 3: In this case, we determine two linearly independent solu-
tions to
(A −3I)2v = 0.
(9.8.10)

640
CHAPTER 9
Systems of Differential Equations
The coefﬁcient matrix of this system is
(A −3I)2 =
⎡
⎣
3
8
1
−1
−6
3
−1
−1
−2
⎤
⎦
⎡
⎣
3
8
1
−1
−6
3
−1
−1
−2
⎤
⎦=
⎡
⎣
0
−25
25
0
25
−25
0
0
0
⎤
⎦,
so that the system (9.8.10) reduces to the single equation
v2 −v3 = 0,
which has two free variables. We set v1 = r and v3 = s, in which case v2 = s.
Hence, Equation (9.8.10) has solution
v = r(1, 0, 0) + s(0, 1, 1).
Consequently, two linearly independent solutions to Equation (9.8.10) are
v1 = (1, 0, 0) and v2 = (0, 1, 1).
Since (A −3I)2v1 = 0, eAtv1 reduces to
eAtv1 = e3t[v1 + t(A −3I)v1]
= e3t
⎧
⎨
⎩
⎡
⎣
1
0
0
⎤
⎦+ t
⎡
⎣
3
8
1
−1
−6
3
−1
−1
−2
⎤
⎦
⎡
⎣
1
0
0
⎤
⎦
⎫
⎬
⎭
= e3t
⎧
⎨
⎩
⎡
⎣
1
0
0
⎤
⎦+ t
⎡
⎣
3
−1
−1
⎤
⎦
⎫
⎬
⎭.
Hence, one solution to the given system is
x1(t) = eAtv1 = e3t
⎡
⎣
1 + 3t
−t
−t
⎤
⎦.
Similarly,
eAtv2 = e3t[v2 + t(A −3I)v2]
= e3t
⎧
⎨
⎩
⎡
⎣
0
1
1
⎤
⎦+ t
⎡
⎣
3
8
1
−1
−6
3
−1
−1
−2
⎤
⎦
⎡
⎣
0
1
1
⎤
⎦
⎫
⎬
⎭
= e3t
⎧
⎨
⎩
⎡
⎣
0
1
1
⎤
⎦+ t
⎡
⎣
9
−3
−3
⎤
⎦
⎫
⎬
⎭.
Thus, a second linearly independent solution to the given system is
x2(t) = eAtv2 = e3t
⎡
⎣
9t
1 −3t
1 −3t
⎤
⎦.
Eigenvalue λ2 = −2: It is easily shown that the eigenvectors corresponding to
λ2 = −2 are all scalar multiples of
v3 = (−1, 1, 0).

9.8
Matrix Exponential Function and Systems of Differential Equations 641
Hence, a third linearly independent solution to the given system is
x3(t) = eAtv3 = e−2t
⎡
⎣
−1
1
0
⎤
⎦.
Consequently, a fundamental matrix for x′ = Ax is
X(t) = [eAtv1, eAtv2, eAtv3] =
⎡
⎣
e3t(1 + 3t)
9te3t
−e−2t
−te3t
e3t(1 −3t)
e−2t
−te3t
e3t(1 −3t)
0
⎤
⎦,
(9.8.11)
so that the given vector differential equation has general solution
x(t) = X(t)c =
⎡
⎣
e3t(1 + 3t)
9te3t
−e−2t
−te3t
e3t(1 −3t)
e−2t
−te3t
e3t(1 −3t)
0
⎤
⎦
⎡
⎣
c1
c2
c3
⎤
⎦.
(b) From Equation (9.8.11), we have
X(0) =
⎡
⎣
1
0
−1
0
1
1
0
1
0
⎤
⎦,
and, using the Gauss-Jordan method, we ﬁnd that
X−1(0) =
⎡
⎣
1
1
−1
0
0
1
0
1
−1
⎤
⎦.
Consequently,
eAt = X(t)X−1(0) =
⎡
⎣
e3t(1 + 3t)
9te3t
−e−2t
−te3t
e3t(1 −3t)
e−2t
−te3t
e3t(1 −3t)
0
⎤
⎦
⎡
⎣
1
1
−1
0
0
1
0
1
−1
⎤
⎦.
That is,
eAt =
⎡
⎣
e3t(1 + 3t)
e3t(1 + 3t) −e−2t
e3t(6t −1) + e−2t
−te3t
e−2t −te3t
e3t(1 −2t) −e−2t
−te3t
−te3t
e3t(1 −2t)
⎤
⎦.
□
Remark
The main use of the matrix exponential is theoretical, but as the examples
in this section show, it is also a useful computational tool.
We end this section by showing that the results obtained in this chapter are a gener-
alization of those from Chapter 1. Consider the initial-value problem
dx
dt −ax = b(t),
x(0) = x0,
where a is a constant. Using the technique developed in Section 1.6 for solving linear
differential equations, it is easily shown that the solution to the initial-value problem is
x(t) = eat
'6 t
0
e−asb(s)dx + x0
(
.
(9.8.12)

642
CHAPTER 9
Systems of Differential Equations
Now consider the corresponding initial-value problem for vector differential equations,
namely,
x′ = Ax + b(t),
x(0) = x0.
(9.8.13)
According to the variation-of-parameters method, a particular solution to the system is
xp(t) = X(t)
6 t
0
X−1(s)b(s)ds,
where X(t) is any fundamental matrix for x′ = Ax. Further, the complementary function
for (9.8.13) is
xc(t) = X(t)c.
If we use the matrix exponential function eAt as the fundamental matrix, then combining
xc and xp, the general solution to the system (9.8.13) assumes the form
x(t) = eAtc + eAt
6 t
0
e−Asb(s)ds.
(9.8.14)
Imposing the initial condition x(0) = x0 yields
c = x0.
Substituting into (9.8.14) and simplifying, we ﬁnally obtain
x(t) = eAt
'
x0 +
6 t
0
e−Asb(s)ds
(
,
which is a generalization of (9.8.12) to systems.
Exercises for 9.8
Key Terms
Transition matrix.
Skills
• Be able to compute the derivative of the matrix expo-
nential function.
• Be able to use a fundamental matrix for the vector
differential equation x′ = Ax to compute the matrix
exponential function.
• Be able to use the matrix exponential function to ﬁnd
a fundamental matrix and the solution to the vector
differential equation x′ = Ax.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The derivative of the matrix exponential function eAt
with respect to the variable t is AeAt.
(b) The transition matrix X0(t) for the vector differential
equation x′ = Ax is precisely the same as the matrix
exponential function eAt.
(c) If the matrix exponential function eAt is known, then
one can explicitly solve the initial-value problem
x′ = Ax, x(0) = x0.
(d) The transition matrix for a linear system is always
invertible.
(e) The matrix exponential function eAt can be written as
eAt = X(t)X−1(0) for any fundamental matrix X(t)
for the vector differential equation x′ = Ax.

9.9
The Phase Plane for Linear Autonomous Systems 643
(f) If {v1, v2, . . . , vn} is linearly independent in Rn,
then so is {eAtv1, eAtv2, . . . , eAtvn} for all n × n
matrices A.
Problems
1. If X(t) is any fundamental matrix for x′ = Ax, show
that the transition matrix based at t = 0 is given by
X0 = X(t)X−1(0).
For Problems 2–4, use the techniques from Section 9.4 and
Section 9.5 to determine a fundamental matrix for x′ = Ax,
and hence, ﬁnd eAt.
2. A =
'2
1
0
2
(
.
3. A =
'1
2
0
−1
(
.
4. A =
⎡
⎣
3
0
0
0
3
−1
0
1
1
⎤
⎦.
For Problems 5–7, ﬁnd n linearly independent solutions to
x′ = Ax of the form eAtv, and hence ﬁnd eAt.
5. A =
'−3
−2
2
1
(
.
6. A =
'3
−1
4
−1
(
.
7. A =
⎡
⎣
2
0
0
0
1
−8
0
2
−7
⎤
⎦.
For Problems 8–10, solve x′ = Ax by determining n linearly
independent solutions of the form x(t) = eAtv.
8. A =
⎡
⎣
0
1
3
2
3
−2
1
1
2
⎤
⎦. You may assume that p(λ) =
−(λ + 1)(λ −3)2.
9. A =
⎡
⎣
−8
6
−3
−12
10
−3
−12
12
−2
⎤
⎦. You may assume that p(λ) =
−(λ + 2)2(λ −4).
10. A =
⎡
⎢⎢⎣
1
0
0
0
0
6
−7
3
0
0
3
−1
0
−4
9
−3
⎤
⎥⎥⎦. You may assume that
p(λ) = (λ −1)(λ −2)3.
11. The matrix A =
⎡
⎢⎢⎣
0
−1
0
0
1
0
0
0
1
0
0
−1
0
1
1
0
⎤
⎥⎥⎦has character-
istic polynomial p(λ) = (λ2 + 1)2. Determine two
complex-valued solutions to x′ = Ax of the form
x = eAtv, and hence, ﬁnd four linearly independent
real-valued solutions to the differential system.
9.9
The Phase Plane for Linear Autonomous Systems
So far in this chapter, we have developed the general theory for linear systems of dif-
ferential equations, and we have derived particular solution techniques for solving such
systems in the case of constant coefﬁcients. If we drop either the constant coefﬁcient
assumption or the linearity assumption, in general, it is not possible to explicitly solve
the resulting systems. Consequently, we need to resort either to a qualitative analysis of
the system or to numerical techniques. In the ﬁnal two sections of this chapter, we give
a brief introduction to the qualitative approach in the case of systems of the form
dx
dt = F(x, y),
(9.9.1)
dy
dt = G(x, y),
(9.9.2)
where F and G depend only on x and y. Such a system, in which t does not explicitly
occur in F and G, is called an autonomous system. We can interpret the two equations
in the system as determining the components of the velocity of a particle that is moving
in the xy-plane. As t increases, the particle moves along a curve in the xy-plane called

644
CHAPTER 9
Systems of Differential Equations
a trajectory.8 The xy-plane itself is referred to as the phase plane, and the totality of
all trajectories gives the phase portrait. Note that each trajectory has a natural direction
associated with it, namely, the direction that the particle moves along a trajectory as
t increases. From Equations (9.9.1) and (9.9.2), we see that the differential equation
determining the trajectories is
dy
dx = G(x, y)
F(x, y).
Even if we cannot solve this differential equation, it is possible to obtain much qualitative
information about the behavior of the trajectories by constructing, either by hand or using
technology, the slope ﬁeld associated with it.
For the system of equations (9.9.1) and (9.9.2), any values of x and y for which
both F and G vanish are called equilibrium points. If (x0, y0) is an equilibrium point,
then x(t) = x0, y(t) = y0 is a solution to the system (9.9.1) and (9.9.2) and is called an
equilibrium solution. We will see that equilibrium points play a key role in the analysis
of the phase plane.
Example 9.9.1
Determine all equilibrium points for the system
x′ = x + y,
y′ = 2x −3y.
Solution:
To determine any equilibrium points, we must solve
x + y = 0,
2x −3y = 0.
Since the determinant of the matrix of coefﬁcients of this homogeneous linear system is
nonzero, the only solution is (0, 0). Hence, (0, 0) is the only equilibrium point.
□
Example 9.9.2
Determine all equilibrium points for the system
x′ = 2x + y,
y′ = 4x + 2y.
Solution:
Here, we must solve
2x + y = 0,
4x + 2y = 0.
In this case, any point (x, y) in the plane that lies along the line y = −2x will be an
equilibrium point, since the second equation is simply a multiple of the ﬁrst one.
□
Remark
As the preceding example illustrates, it is possible for a system of differential
equations to have an inﬁnite number of equilibrium points. However, we will restrict our
attention from now on to the case when there is a unique equilibrium point.
Before analyzing the general autonomous system (9.9.1) and (9.9.2), we need to
look at the simpler case when F and G are linear functions of x and y. The system then
reduces to the general homogeneous constant coefﬁcient system
dx
dt = ax + by,
dy
dt = cx + dy,
(9.9.3)
8Other terms used for trajectories are phase paths or orbits.

9.9
The Phase Plane for Linear Autonomous Systems 645
where a, b, c, and d are constants. Consequently, the differential equation for determin-
ing the trajectories (or slope ﬁeld) is
dy
dx = cx + dy
ax + by ,
which falls into the ﬁrst-order homogeneous type that we studied in Chapter 1. Whereas
in many cases it is possible to solve this differential equation using the change of variables
y = xV , we can more easily determine the general behavior in the phase plane by work-
ing with the equivalent vector differential equation and using results already obtained in
this chapter. Consequently, we write (9.9.3) as the vector differential equation
x′ = Ax,
A =
'a
b
c
d
(
.
(9.9.4)
The goal is to determine the general qualitative behavior of the solution curves x(t) =
(x(t), y(t)) to (9.9.4).
The equilibrium points of the system (9.9.4) are solutions to the 2×2 homogeneous
linear system Ax = 0. The Invertible Matrix Theorem therefore guarantees that there
will be a unique equilibrium point if and only if A is invertible, in which case the
corresponding equilibrium point is x = (0, 0). Therefore,
we will assume for the remainder of this section that the matrix A in
question is invertible.
As we now show, the eigenvalues and eigenvectors of A play a basic role in the
structure of the phase plane. Note that since A is invertible, all eigenvalues of A are
nonzero. Suppose that λ ̸= 0 is a real eigenvalue of A, with corresponding eigenvector
v. Then a solution to (9.9.4) is
x(t) = eλtv.
Since v is a constant vector, the corresponding trajectories are two half-lines that emanate
from the equilibrium point (0, 0) and are parallel to the eigenvector v. The initial con-
ditions would determine which half-line corresponded to a particular motion. If λ > 0,
then due to the eλt term, the direction along the trajectory is away from the origin. (See
Figure 9.9.1.) Interpreting (x(t), y(t)) as the coordinates of a particle at time t, these
trajectories correspond to a particle emitted from the equilibrium point at t = −∞and
moving outwards along the appropriate half-line. If λ < 0, then the direction along
the trajectory is toward the origin. (See Figure 9.9.2.) In this case, we can interpret the
trajectory as corresponding to a point particle moving along the half-line towards the
origin, but which does not reach the origin in a ﬁnite time.
x
Equilibrium point
y
Direction of
Eigenvector
Figure 9.9.1: Trajectories corresponding to a positive eigenvalue and real eigenvector solution
to the system x′ = Ax.

646
CHAPTER 9
Systems of Differential Equations
x
Equilibrium point
y
Direction of
Eigenvector
Figure 9.9.2: Trajectories corresponding to a negative eigenvalue and real eigenvector solution
to the system x′ = Ax.
As the above discussion suggests, the eigenvalue-eigenvector pairs for A play a
fundamental role in the general analysis of the phase plane. The eigenvalues of A are
the solutions to the equation
0 = det(A −λI) = (a −λ)(d −λ) −bc = λ2 −(a + d)λ + (ad −bc),
which can be written as
λ2 −tr(A)λ + det(A) = 0,
where we recall that tr(A) is the trace of A, the sum of the elements on the main diagonal
of A. This quadratic equation has roots λ1, λ2 where
λ1 = tr(A) +
7
[tr(A)]2 −4 det(A)
2
,
λ2 = tr(A) −
7
[tr(A)]2 −4 det(A)
2
.
The following three cases arise:
1. λ1, λ2 are real and distinct. This occurs if and only if [tr(A)]2 > 4 det(A).
2. λ1 = λ2. This occurs if and only if [tr(A)]2 = 4 det(A).
3. λ1 and λ2 are complex conjugates. This occurs if and only if [tr(A)]2 < 4 det(A).
We now analyze the phase plane in each case.
Case 1: λ1 and λ2 are real and distinct.
Let v1 and v2 denote corresponding eigenvectors.9 Then we have the basic solutions
x1 =
'x1(t)
y1(t)
(
= eλ1tv1
and
x2 =
'x2(t)
y2(t)
(
= eλ2tv2
(9.9.5)
to the system (9.9.4). That is, x′
1 = Ax1 and x′
2 = Ax2, so that any vector function of
the form
'x(t)
y(t)
(
= c1eλ1tv1 + c2eλ2tv2
(9.9.6)
is also a solution to (9.9.4). In fact, as the next theorem indicates, every solution to (9.9.4)
has this form:
Theorem 9.9.3
If x1(t) and x2(t) are two linearly independent solutions to (9.9.4), then every solution
to (9.9.4) is of the form
x(t) = c1x1(t) + c2x2(t)
where c1 and c2 are arbitrary constants.
9By Theorem 7.2.5 or Problem 38 in Section 7.2, v1 and v2 must be linearly independent.

9.9
The Phase Plane for Linear Autonomous Systems 647
Proof This follows at once from Theorem 9.3.2.
The two solutions in (9.9.5) give rise to four half-line trajectories, as previously
discussed. Since trajectories cannot intersect, the phase plane is divided into four regions.
The speciﬁc behavior of the remaining trajectories depends on the relationship between
λ1 and λ2.
(a) λ2 < λ1 < 0: The general properties of the trajectories are summarized as follows:
1. Since λ1 and λ2 are both negative, lim
t→∞
'x(t)
y(t)
(
= 0, so that as t →∞, all
trajectories approach the equilibrium point (0, 0).
2. Writing (9.9.6) as
'x(t)
y(t)
(
= eλ1t[c1v1 + c2v2e(λ2−λ1)t]
and using the fact that λ2 −λ1 < 0 in this case, we see that for large t,
and c1 ̸= 0, the second term in the brackets is negligible compared to the
ﬁrst term. Consequently, apart from the trajectories corresponding to the
eigenvector solution
'x2(t)
y2(t)
(
= eλ2tv2, all trajectories are parallel to v1 as
t →∞.
3. Writing (9.9.6) as
'x(t)
y(t)
(
= eλ2t[c1v1e(λ1−λ2)t + c2v2]
and using the fact that λ1 −λ2 > 0, we see that apart from the trajectories
corresponding to the eigenvector solution
'x1(t)
y1(t)
(
= eλ1tv1,
all trajectories are parallel to v2 as t →−∞.
A generic sketch of the phase plane in this case is given in Figure 9.9.3. The
equilibrium point (0, 0) is called a node. It is stable, since all solutions approach
the node as t →∞.
y
x
v2
v1
Direction of
eigenvectors
Figure 9.9.3: Typical phase portrait in the case λ2 < λ1 < 0.

648
CHAPTER 9
Systems of Differential Equations
(b) 0 < λ1 < λ2: The general behavior in this case is the same as that in Case 1(a),
except the arrows are reversed on each trajectory. The equilibrium point is still
called a node, but in this case it is unstable.
(c) λ2 < 0 < λ1: The following general behavior can be identiﬁed.
1. The only trajectories that approach the equilibrium point as t →∞are
those corresponding to the eigenvector solution
'x2(t)
y2(t)
(
= eλ2tv2.
2. Writing (9.9.6) as
'x(t)
y(t)
(
= eλ1t[c1v1 + c2v2e(λ2−λ1)t]
and using the fact that λ2−λ1 < 0, it follows that, apart from the trajectories
corresponding to the eigenvector solution
'x2(t)
y2(t)
(
= eλ2tv2,
all trajectories are parallel to v1 as t →∞.
3. Writing (9.9.6) as
'x(t)
y(t)
(
= eλ2t[c1v1e(λ1−λ2)t + c2v2]
and using the fact that λ1 −λ2 > 0, we see that, apart from the trajectories
corresponding to the eigenvector solution
'x1(t)
y1(t)
(
= eλ1tv1,
all trajectories are parallel to v2 as t →−∞.
A typical phase portrait is sketched in Figure 9.9.4. In this case, the equilibrium
point is called a saddle point.
y
x
Direction of Eigenvectors
v1
v2
Figure 9.9.4: Typical phase portrait in the case λ2 < 0 < λ1. The equilibrium point is a saddle
point and is unstable.

9.9
The Phase Plane for Linear Autonomous Systems 649
Case 2: λ1 = λ2 = λ ̸= 0.
Two main subcases can be distinguished, depending on the structure of the matrix A.
(a) If A is a scalar multiple of the identity matrix, that is, A = aI where a is a nonzero
constant, then
det(A −λI) = (a −λ)2
so that λ = a is a repeated eigenvalue. Furthermore,
A −λI = aI −aI = 0,
so that every nonzero vector is an eigenvector. Consequently, if we choose two
nonproportional eigenvectors v1 and v2 of A it follows from Theorem 9.9.3 that
every solution to the system is
'x(t)
y(t)
(
= eλt(c1v1 + c2v2),
which, for each pair of values for c1 and c2, is the equation of a line through
the origin. Moreover, since v1 and v2 are nonproportional, all directions in the
xy-plane are obtained as c1 and c2 assume all possible values. Consequently, the
trajectories consist of all half-lines through the origin. The equilibrium point is
called a proper node in this case. If λ < 0, then all trajectories approach the
equilibrium point as t →∞, whereas if λ > 0, the direction along the trajectories
is away from the equilibrium point. A representative sketch of the phase portraits
is given in Figure 9.9.5.
x
y
Figure 9.9.5: Typical phase
portrait when A = aI, a < 0. The
equilibrium point is a proper node
and is stable.
(b) If A is not a scalar multiple of the identity matrix, then as shown in Problem 29, A
is defective, and so all eigenvectors are proportional to one another. If v0 denotes
any such eigenvector, then, from our preceding discussion,
x0 =
'x0(t)
y0(t)
(
= eλtv0
is a solution to the system (9.9.4). Using the material in Section 9.5, we know that
a second linearly independent solution to the system in this case is
x1 =
'x1(t)
y1(t)
(
= eλt(v1 + tv0),
where v1 is a vector satisfying the condition (A −λI)v1 = v0. Consequently,
applying Theorem 9.9.3, all solutions to the system are of the form
'x(t)
y(t)
(
= eλt[c0v0 + c1(v1 + tv0)].
For c1 = 0, we have the trajectories corresponding to the eigenvector solution. If
c1 ̸= 0, then the dominant term in the general solution as t →±∞is
'x(t)
y(t)
(
= c1teλtv0.

650
CHAPTER 9
Systems of Differential Equations
Consequently, if λ < 0, we have the following results.
1. As t →∞, all trajectories approach the equilibrium point (0, 0) tangent to
the eigenvector v0.
2. As t →−∞, all trajectories are parallel to v0.
See Figure 9.9.6 for a typical phase portrait. If λ > 0, then the direction
along the trajectories is reversed. In this case, the equilibrium point (0, 0) is
called a degenerate node and is said to be stable or unstable depending on
whether λ is negative or positive respectively.
x
y
Direction of eigenvector
v1
Figure 9.9.6: Typical phase portrait when A has only one linearly independent eigenvector and
λ < 0. The equilibrium point is called a degenerate node.
Case 3: Complex conjugate eigenvalues λ = a ± ib.
If we let v = r + is denote a complex eigenvector (with r and s real-valued) corre-
sponding to the eigenvalue λ = a + ib, then according to our results from Section 9.4,
two linearly independent solutions to the system of differential equations are of the
form
'x1(t)
y1(t)
(
= eat(cos btr −sin bts) and
'x2(t)
y2(t)
(
= eat(sin btr + cos bts).
(9.9.7)
Consequently, the general solution in this case is
'x(t)
y(t)
(
= eat[c1(cos bt r −sin bt s) + c2(sin bt r + cos bt s)],
or equivalently,
x(t) = eatv(t),
(9.9.8)
where
v(t) = c1(cos bt r −sin bt s) + c2(sin bt r + cos bt s).
The key point to notice is that
v(t + 2π/b) = v(t).

9.9
The Phase Plane for Linear Autonomous Systems 651
Consequently, v(t) has period T = 2π/b, and we can therefore draw the following
conclusions.
(a) If a = 0, (9.9.8) implies that x(t + 2π/b) = x(t). All trajectories are therefore
closed curves (see Figure 9.9.7) and so the corresponding solutions are periodic.
In this case, the equilibrium point (0, 0) is called a center and is stable.
x
y
Figure 9.9.7: Typical phase
portrait for the case of pure
imaginary eigenvalues. The
equilibrium point is called a
center and is stable.
(b) If a ̸= 0, then the trajectories spiral around the origin. (See Figure 9.9.8.) The
equilibrium point (0, 0) is called a spiral point. Furthermore,
1. If a > 0, the trajectories spiral away from the equilibrium point, and there-
fore, it is called an unstable spiral point.
2. If a < 0, the trajectories spiral towards the origin, and therefore, it is called
a stable spiral point.
y
x
Figure 9.9.8: Typical phase
portrait for the case of complex
conjugate eigenvalues λ = a ± ib,
with a > 0. The equilibrium point
is called a spiral point.
This completes the classiﬁcation of the equilibrium point (0, 0) associated with
the system (9.9.4) in the case where det(A) ̸= 0. The results when λ1 ̸= λ2 are
summarized in Table 9.9.1.
Eigenvalues
Type of Equilibrium Point
Real and negative
Stable node
Real and positive
Unstable node
Opposite sign
Saddle
Pure imaginary
Stable center
Complex with positive real part
Unstable spiral
Complex with negative real part
Stable spiral
Table 9.9.1: Classiﬁcation of equilibrium point in terms of eigenvalues.
Based on the preceding analysis, it is not too difﬁcult to obtain a general sketch of the
phase plane once we have determined the eigenvalues and eigenvectors of A. However,
as in the case of slope ﬁelds considered in Chapter 1, this is an area where technology is
a deﬁnite beneﬁt. In the examples that follow we have provided Maple plots of the phase
planes.
Example 9.9.4
Characterize the equilibrium point for the linear systems x′ = Ax and sketch the phase
portrait.
(a) A =
'−1
−2
−2
−1
(
.
(b) A =
'−1
−2
2
−1
(
.
(c) A =
' 1
3
−2
−4
(
.
Solution:
(a) The matrix A has eigenvalues λ1 = 1 and λ2 = −3 with corresponding non-
proportional eigenvectors v1 = (1, −1) and v2 = (1, 1). Since the eigenvalues
have different signs, the equilibrium point (0, 0) is a saddle point. A Maple sketch
including the slope ﬁeld is given in Figure 9.9.9. Notice that we have appended
arrow heads to each line segment in the slope ﬁeld to indicate the direction that

652
CHAPTER 9
Systems of Differential Equations
trajectories are traversed. The resulting slope ﬁeld is usually called a direction
ﬁeld.
y
x
Direction of Eigenvectors
v2
v1
Figure 9.9.9: Phase portrait for system in part (a) of Example 9.9.4.
(b) In this case, the matrix has complex conjugate eigenvalues λ = −1 ± 2i. Since
the real part of the eigenvalues is negative, the equilibrium point is a stable spiral.
To determine whether the trajectories spiral clockwise or counterclockwise toward
the origin, we check the sign of dx
dt at points where the trajectories intersect the
positive y-axis. From the given system, when x = 0 and y > 0, we see that
dx
dt
< 0, so that the trajectories spiral counterclockwise around the origin. A
Maple plot of the phase plane is given in Figure 9.9.10.
y
x
Figure 9.9.10: Phase portrait for system in part (b) of Example 9.9.4.
(c) In this case, the matrix A has eigenvalues λ1 = −1 and λ2 = −2 with corre-
sponding nonproportional eigenvectors v1 = (3, −2) and v2 = (1, −1). We see
that the equilibrium point is a stable node. All trajectories approach (0, 0) tangent
to the eigenvector v1 (see Case 1(a)). The phase plane for this system is given in
Figure 9.9.11.

9.9
The Phase Plane for Linear Autonomous Systems 653
y
x
Direction of Eigenvectors
v1
v2
Figure 9.9.11: Phase portrait for system in part (c) of Example 9.9.4.
□
Exercises for 9.9
Key Terms
Autonomous system, Trajectory, Phase plane, Phase portrait,
Equilibrium points, Equilibrium solution, Stable node, Un-
stable node, Saddle point, Proper node, Degenerate node,
Stable spiral point, Unstable spiral point, Direction ﬁeld.
Skills
• Be able to determine the equilibrium point(s) for a
linear system of two differential equations.
• Be able to classify the equilibrium point(s) as stable
or unstable nodes, saddle points, proper nodes, degen-
erate nodes, or stable or unstable spiral points.
• Be able to use the eigenvalue-eigenvector pairs for a
linear system of two differential equations to predict
the qualitative behavior of the trajectories in the phase
plane.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) An equilibrium point is a point toward which all tra-
jectories of a linear system of differential equations
approach as t →∞.
(b) A linear system x′ = Ax for which A has complex
eigenvalues λ = ±ib (for some b ∈R) gives rise to
elliptical trajectories in the phase plane.
(c) The equilibrium point of a linear system x′ = Ax that
hastwopositive(real)eigenvaluesiscalledanunstable
node.
(d) If all solutions to the linear system x′ = Ax approach
(0, 0), then (0, 0) is stable.
(e) The type of equilibrium point for the linear system
x′ = Ax is the same as the type of equilibrium point
for the linear system x′ = (2A)x.
(f) For a saddle point, the only trajectories that approach
(0, 0) as t →∞are those pointing along the eigen-
vector v corresponding to the positive eigenvalue λ.
Problems
For Problems 1–4, determine all equilibrium points of the
given system.
1. x′ = x(y −3),
y′ = y(x + 1).
2. x′ = x(x −y + 1),
y′ = y(y + 2x).
3. x′ = x(2x + y),
y′ = y(x −2y + 4).
4. x′ = x(x2 + y2 −1),
y′ = 2y(xy −1).
For Problems 5–20, characterize the equilibrium point for
the system x′ = Ax and sketch the phase portrait.
5. A =
'1
3
1
−1
(
.
6. A =
' 0
2
−2
0
(
.

654
CHAPTER 9
Systems of Differential Equations
7. A =
'1
0
3
1
(
.
8. A =
' 2
3
−1
−2
(
.
9. A =
'−2
3
−3
−2
(
.
10. A =
'−2
1
1
−2
(
.
11. A =
'5
4
4
5
(
.
12. A =
'0
−1
1
0
(
.
13. A =
'3
−2
2
−1
(
.
14. A =
'2
−1
1
2
(
.
15. A =
'2
−5
4
−7
(
.
16. A =
'2
1
3
4
(
.
17. A =
'3
4
4
−3
(
.
18. A =
' 1
1
−9
−5
(
.
19. A =
'1
−1
1
2
(
.
20. A =
'3
0
0
3
(
.
21. Characterize the equilibrium point (0, 0) for the sys-
tem x′ = Ax if A =
'−1
2
−2
−1
(
. Solve the system of
differential equations, and show that the components
of the solution vector satisfy
x2 + y2 = e−4t(c2
1 + c2
2),
(9.9.9)
where c1 and c2 are constants. As t varies in Equa-
tion (9.9.9), describe the curve that is generated in the
phase plane.
For Problems 22–25, convert the given differential equation
to a ﬁrst-order system using the substitution u = y, v = dy
dt ,
and determine the phase portrait for the resulting system.
22. d2y
dt2 + 6dy
dt + 9y = 0.
23. d2y
dt2 + 16y = 0.
24. d2y
dt2 + 4dy
dt + 5y = 0.
25. d2y
dt2 −25y = 0.
26. Consider the differential equation
d2y
dt2 + 2cdy
dt + ky = 0,
where c and k are positive constants, that governs the
behavior of a spring-mass system. Convert the dif-
ferential equation to a ﬁrst-order linear system and
sketch the corresponding phase portraits. (You will
need to distinguish the three cases c2 > k, c2 < k,
and c2 = k.) In each case, use your phase por-
trait to describe the behavior of y for various initial
conditions.
27. Verify that the solutions to x′ = Ax appearing in
(9.9.7) are not proportional.
[Hint: Evaluate each solution at t = 0 and at t =
2π/b, and use the fact that both r and s are real-valued
vectors.]
28. Let A =
'a
b
c
d
(
be a 2 × 2 matrix of real constants
with a repeated eigenvalue: λ1 = λ2 = λ.
(a) Show that λ = a + d
2
.
(b) Show that if A has two linearly independent
eigenvectors corresponding to λ, then A = aI
for some scalar a.
[Hint: Under this assumption, both e1 and
e2 must be eigenvectors. Now consider (A −
λI)e1 = 0 and (A −λI)e2 = 0.]
(c) Conclude from part (b) that if A is a 2 × 2 matrix
of real constants with a repeated eigenvalue that
is not a scalar multiple of the identity matrix, then
A is defective.

9.10
Nonlinear Systems 655
9.10
Nonlinear Systems
We now brieﬂy discuss the qualitative analysis of general autonomous systems of the
form
dx
dt = F(x, y),
dy
dt = G(x, y),
(9.10.1)
where, throughout the remainder of the discussion, we will assume that F and G have
continuous partial derivatives up to order at least two. We are interested in making
a similar classiﬁcation of the equilibrium points for this system as we were able to
do in the linear case. The approach that we will take is to approximate (9.10.1) with
a corresponding linear system. To see how the approximation arises, we recall from
elementary calculus that if we are given a function f (x, y) deﬁned in some region of the
xy-plane, then the equation z = f (x, y) deﬁnes a surface in space. Further, the tangent
plane to this surface at any point (x0, y0) has equation
z = f (x0, y0) + (x −x0)∂f
∂x (x0, y0) + (y −y0)∂f
∂y (x0, y0),
and this plane gives the best linear approximation to f (x, y) at (x0, y0). Returning to
the system (9.10.1), we deﬁne the linear approximation to this system at (x0, y0) by
dx
dt = F(x0, y0) + (x −x0)∂F
∂x (x0, y0) + (y −y0)∂F
∂y (x0, y0),
dy
dt = G(x0, y0) + (x −x0)∂G
∂x (x0, y0) + (y −y0)∂G
∂y (x0, y0).
In the case when (x0, y0) is an equilibrium point of (9.10.1), the approximate system
reduces to
dx
dt = (x −x0)∂F
∂x (x0, y0) + (y −y0)∂F
∂y (x0, y0),
dy
dt = (x −x0)∂G
∂x (x0, y0) + (y −y0)∂G
∂y (x0, y0).
The Jacobian matrix, J(x, y), is deﬁned by
J(x, y) =
⎡
⎢⎢⎣
∂F
∂x
∂F
∂y
∂G
∂x
∂G
∂y
⎤
⎥⎥⎦.
Using this matrix, the linear approximation to (9.10.1) at an equilibrium point (x0, y0)
can be written as
'x′(t)
y′(t)
(
= J(x0, y0)
'x −x0
y −y0
(
,
or, equivalently, as
u′ = J(x0, y0)u,
where u =
'x −x0
y −y0
(
.
Example 9.10.1
Determine the linear approximation to the system
x′ = cos x + 3y −1,
y′ = 2x + sin y
at the equilibrium point (0, 0).

656
CHAPTER 9
Systems of Differential Equations
Solution:
For the given system, we have
F(x, y) = cos x + 3y −1
and
G(x, y) = 2x + sin y,
so that
J(x, y) =
'−sin x
3
2
cos y
(
.
Hence,
J(0, 0) =
'0
3
2
1
(
,
and the linear approximation to the given system at (0, 0) is
x′ =
'0
3
2
1
( 'x
y
(
,
or, equivalently,
dx
dt = 3y,
dy
dt = 2x + y.
□
Example 9.10.2
Determine all equilibrium points for the system
x′ = x(1 −y)
and
y′ = y(2 −x),
and determine the linear approximation to the system at each equilibrium point.
Solution:
The system has the two equilibrium points (0, 0) and (2, 1). In this case,
the Jacobian matrix is
J(x, y) =
'1 −y
−x
−y
2 −x
(
.
Thus,
J(0, 0) =
'1
0
0
2
(
,
so that the linear approximation at the equilibrium point (0, 0) is
x′ = J(0, 0)x.
That is,
x′ = x
and
y′ = 2y.
Similarly, the linear approximation at the equilibrium point (2, 1) is
u′ = J(2, 1)u =
' 0
−2
−1
0
(
u,
where u =
'x −2
y −1
(
. In scalar form, we have
x′ = −2(y −1)
and
y′ = −(x −2).
□

9.10
Nonlinear Systems 657
It perhaps seems reasonable to expect that the behavior of the trajectories to the
nonlinearsystem(9.10.1)iscloselyapproximatedbythetrajectoriesofthecorresponding
linearsystem,providedthatwedonotmovetoofarawayfrom (x0, y0).Thisisindeedtrue
in most cases. In Table 9.10.1 we summarize the relationship between the behavior at an
equilibriumpointofanonlinearsystemandthebehavioratthecorrespondingequilibrium
point of the linear approximation in the case of distinct eigenvalues. This indicates
that apart from the case of pure imaginary eigenvalues (or a repeated eigenvalue), the
phase portrait for a nonlinear system looks similar to the linear approximation in the
neighborhood of an equilibrium point.
Eigenvalues
Linear Approximation
Nonlinear System
Real and negative
Stable node
Stable node
Real and positive
Unstable node
Unstable node
Opposite signs
Saddle
Saddle
Complex with negative real part
Stable spiral
Stable spiral
Complex with positive real part
Unstable spiral
Unstable spiral
Pure imaginary
Stable center
Center or spiral point,
stability indeterminate
Table 9.10.1: Nonlinear system and linear approximation behavior in terms of eigenvalues.
Example 9.10.3
Determine and classify all equilibrium points for the given system
(a) x′ = x(x −1),
y′ = y(2 + xy2).
(b) x′ = x −y,
y′ = y(2x + y −3).
Solution:
(a) The equilibrium points are obtained by solving
x(x −1) = 0,
y(2 + xy2) = 0.
The ﬁrst of these equations implies that x = 0 or x = 1. In both cases, substitution
into the second equation yields y = 0. Consequently, the only equilibrium points
are (0, 0) and (1, 0). The Jacobian for the given system is
J(x, y) =
'2x −1
0
y3
2 + 3xy2
(
.
Hence,
J(0, 0) =
'−1
0
0
2
(
,
which has eigenvalues λ1 = 2 and λ2 = −1. Since the eigenvalues have different
signs, the equilibrium point (0, 0) is a saddle point in both the linear approximation
to the given system and the given system itself. At the equilibrium point (1, 0), we
have
J(1, 0) =
'1
0
0
2
(
.

658
CHAPTER 9
Systems of Differential Equations
This matrix has eigenvalues λ1 = 2 and λ2 = 1, which implies that the equilibrium
point is an unstable node. Figure 9.10.1 gives a Maple plot of the phase plane for
the given nonlinear system.
1
x
y
Figure 9.10.1: Phase portrait for system in part (a) of Example 9.10.3.
(b) To determine the equilibrium points, we must solve
x −y = 0,
y(2x + y −3) = 0.
Substituting y = x from the ﬁrst equation into the second yields the condition
3x(x −1) = 0.
Hence, the equilibrium points are (0, 0) and (1, 1). The Jacobian for the given
system is
J(x, y) =
8
1
−1
2y
2x + 2y −3
9
.
At the equilibrium point (0, 0), we have
J(0, 0) =
'1
−1
0
−3
(
,
which has eigenvalues λ1 = 1 and λ2 = −3, with corresponding eigenvectors
v1 = (1, 0) and v2 = (1, 4). Since the eigenvalues have different signs, the
equilibrium point (0, 0) is a saddle point. At the equilibrium point (1, 1), we have
J(1, 1) =
'1
−1
2
1
(
,
which has eigenvalues λ = 1 ± i
√
2. Hence, the equilibrium point is an unstable
spiral point. A Maple plot of the phase plane is given in Figure 9.10.2.
□
A Predator Prey Model
As an example of an applied problem that is modeled by a nonlinear system of differential
equations, we consider the interaction of two species. One species is a predator, and the
other is the prey. Let x(t) denote the prey population at time t, and let y(t) denote the

9.10
Nonlinear Systems 659
y
x
Figure 9.10.2: Phase portrait for system in part (b) of Example 9.10.3.
predator population. Then the model equations that we discuss are
dx
dt = x(a −by),
(9.10.2)
dy
dt = y(cx −d),
(9.10.3)
where a, b, c, and d are positive constants. To interpret these equations, we see that
in the absence of any predators (i.e., y = 0), Equation (9.10.2) reduces to the simple
Malthusian exponential growth law. The inclusion of predators is taken account of by
subtracting a term proportional to the number of predators present from the growth rate
of the prey. Similarly, from Equation (9.10.3), in the absence of prey (i.e., x = 0), a
predator population would decay exponentially. To account for the inclusion of prey, a
term proportional to the number of prey has been added to the growth rate of the predator.
This model is called the Lotka-Volterra system. We see that the system is nonlinear with
equilibrium points at (0, 0) and (d/c, a/b). Computing the Jacobian of the system yields
J(x, y) =
'a −by
−bx
cy
cx −d
(
,
so that
J(0, 0) =
'a
0
0
−d
(
with eigenvalues λ1 = a and λ2 = −d. Consequently, the equilibrium point (0, 0) is a
saddle point. We note that nonproportional eigenvectors in this case are v1 = (1, 0) and
v2 = (0, 1). At the equilibrium point (d/c, a/b), we have
J(d/c, a/b) =
' 0
−bd/c
ca/b
0
(
,
with eigenvalues λ = ±i
√
ad. Consequently, in the linear approximation the equi-
librium point at (d/c, a/b) is a center. Therefore, according to the results given in
Table 9.10.1, the nonlinear system either has a center or a spiral point. In Figure 9.10.3
we give a Maple plot of the phase plane using typical values for the constants a, b, c,
and d. This indicates that the equilibrium point in the nonlinear model is also a center.
Consequently, the corresponding solutions for both x and y are periodic in time. The
model therefore predicts that the population of both species is periodic, and hence both
species would survive. The general qualitative behavior starting at small values for both

660
CHAPTER 9
Systems of Differential Equations
the predator and prey can be seen from the trajectories. The prey initially increases, and
the predator population remains approximately constant. Then, since there is plenty of
food (prey), the predator population increases with a corresponding decrease in the prey
population. This gives rise to a situation where there are too many predators for the prey
population, and therefore, the predator population decreases while the prey population
remains approximately constant. Then the cycle repeats itself. We see from the three
different trajectories in Figure 9.10.3 that the speciﬁc behavior varies quite signiﬁcantly
depending on the initial conditions.
1
2
3
4
5
1
3
5
x
y
2
4
Figure 9.10.3: Representative phase portrait for a predator-prey system.
The Van Der Pol Equation
Finally in this section, we use the nonlinear differential equation
d2y
dt2 + µ(y2 −1)dy
dt + y = 0,
µ > 0,
(9.10.4)
to illustrate a new type of behavior that does not arise in linear systems. The differential
equation (9.10.4) is called the Van der Pol Equation and arises in the study of nonlinear
circuits. If the parameter µ is zero, then Equation (9.10.4) reduces to that of the simple
harmonic oscillator, which has periodic solutions and circular trajectories. For µ small
and positive and |y| > 1, we can presumably interpret the dy/dt term as a damping term
and expect the system to behave somewhat like a damped harmonic oscillator. However,
for −1 < y < 1, the coefﬁcient of dy/dt is negative, and therefore, this term would
tend to amplify, rather than dampen, any oscillations. This suggests that there may be
an isolated periodic solution (closed trajectory) with the property that all trajectories
that start within it approach the closed path as t increases, and all trajectories that start
outside the closed path spiral toward it as t →∞. Such a closed path, if it exists, is
called a limit cycle. To analyze the Van der Pol Equation, we introduce the phase plane
variables
u = y,
v = dy
dt ,
thereby obtaining the equivalent ﬁrst-order system
du
dt = v,
dv
dt = −u −µ(u2 −1)v.
(9.10.5)
The only equilibrium point of the system (9.10.5) is (0, 0), and the Jacobian of this
system is
J(u, v) =
'
0
1
−1 −2µv
−µ(u2 −1)
(
.

9.10
Nonlinear Systems 661
Hence,
J(0, 0) =
' 0
1
−1
µ
(
.
This matrix has characteristic polynomial
p(λ) = λ2 −µλ + 1,
so that the eigenvalues are
λ = 1
2(µ ±
7
µ2 −4).
v
u
Figure 9.10.4: Maple plot of the phase plane for the Van der Pol equation with µ = 0.1.
For µ > 2, there are two positive eigenvalues, and the equilibrium point is an unstable
node. For µ < 2, however, the equilibrium point is an unstable spiral. Hence, the tra-
jectories close to the equilibrium point do indeed spiral outwards. However, this local
analysis does not give us information about the global behavior of the trajectories. Al-
though we do not have the tools to prove that the Van der Pol equation does indeed have a
limit cycle, further convincing evidence for its existence can be obtained by studying the
phase portraits associated with the differential equation. Figure 9.10.4 contains a Maple
plot in the case when µ = 0.1. The limit cycle is clearly visible and is almost circular, as
we would expect with a small µ value. Figure 9.10.5 contains a similar plot with µ = 1.
The limit cycle is still visible, but it no longer resembles a circle.
u
v
Figure 9.10.5: The phase plane for the Van der Pol equation with µ = 1.

662
CHAPTER 9
Systems of Differential Equations
Exercises for 9.10
Key Terms
Jacobian matrix, Lotka-Volterra system, Van der Pol
equation, Limit cycle.
Skills
• Be able to determine the linear approximation to a
nonlinear system by using the Jacobian matrix.
• Be able to ﬁnd and classify the equilibrium points for
a nonlinear system, as well as for a linear system.
• Be familiar with the Lotka-Volterra system for mod-
elling predator-prey interactions.
• Be familiar with the Van der Pol Equation, how to
convert it into a ﬁrst-order non-linear system, and the
qualitative behavior of the solutions to the system, de-
pending on the parameter µ.
True-False Review
For Questions (a)–(e), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The Jacobian matrix for a linear system of the form
(9.10.1) is
J(x, y) =
⎡
⎢⎢⎣
∂F
∂x
∂F
∂y
∂G
∂x
∂G
∂y
⎤
⎥⎥⎦.
(b) If we replace a nonlinear system of differential equa-
tions with a linear approximation to the system at an
equilibrium point, then the behavior of the trajecto-
ries of the nonlinear system and the behavior of the
trajectories of the corresponding linear system are ap-
proximately the same throughout the xy-plane.
(c) In the Lotka-Volterra predator-prey model, the origin
is a saddle point.
(d) The only equilibrium point of the linear system of dif-
ferential equations arising from the Van der Pol Equa-
tion is the origin.
(e) The equilibrium point of the linear system arising from
the Van der Pol Equation d2y
dt2 +3(y2 −1)dy
dt + y = 0
is an unstable spiral.
Problems
For Problems 1–9, determine all equilibrium points of the
given system and, if possible, characterize them as centers,
spirals, saddles, or nodes.
1. x′ = y(3x −2),
y′ = 2x + 9y2.
2. x′ = y(3x −2),
y′ = 2x −9y2.
3. x′ = x −y2,
y′ = y(9x −4).
4. x′ = x + 3y2,
y′ = y(x −2).
5. x′ = 2x + 5y2,
y′ = y(3 −4x).
6. x′ = 2y + sin x,
y′ = x(cos y −2).
7. x′ = x −2y + 5xy,
y′ = 2x + y.
8. x′ = x(1 −y),
y′ = y(x + 1).
9. x′ = 4x −y −y sin x,
y′ = x + 2y.
The remaining problems require the use of some form of
technology to generate the phase plane for the system of
differential equations.
10. ⋄Sketch the phase portrait of the system in Problem 1
for −1 ≤x ≤1, −1 ≤y ≤1, and thereby determine
whether the equilibrium point (0, 0) is a center or a
spiral.
11. ⋄Sketch the phase portrait of the system in Problem 6
for −2 ≤x ≤2, −2 ≤y ≤2, and thereby determine
whether the equilibrium point (0, 0) is a center or a
spiral.
12. ⋄Sketch the phase portrait of the system in Problem
8 for −2 ≤x ≤2, −2 ≤y ≤2. By inspection, guess
the equation of one particular trajectory.
For Problems 13–18, sketch the phase portrait of the given
system for −2 ≤x ≤2, −2 ≤y ≤2. Comment on the
types of equilibrium points.
13. ⋄The system in Problem 2.
14. ⋄The system in Problem 3.
15. ⋄The system in Problem 4.
16. ⋄The system in Problem 5.
17. ⋄The system in Problem 7.
18. ⋄The system in Problem 9.

9.11
Chapter Review 663
19. ⋄Consider the predator-prey model
dx
dt = x(2 −y), dy
dt = y(x −2).
Sketch the phase plane for 0 ≤x ≤10, 0 ≤y ≤10.
Compare the behavior of the two speciﬁc cases corre-
sponding to the initial conditions x(0) = 1, y(0) =
0.1, and x(0) = 1, y(0) = 1.
20. ⋄Consider the predator-prey model
dx
dt = x(3 −x −y), dy
dt = y(x −1).
Sketch the phase plane for 0 ≤x ≤4, 0 ≤y ≤4.
What happens to the populations of both species as
t →+∞?
21. ⋄Consider the differential equation
d2y
dt2 + 0.1(y −4)(y + 1)dy
dt + y = 0.
(a) Convert the differential equation to a ﬁrst-order
system using the substitution u = y, v = dy
dt ,
and characterize the equilibrium point (0, 0).
(b) Sketch the phase plane for the system on the
square −2 ≤u ≤2, −2 ≤v ≤2. Based on
the resulting sketch, do you think the differential
equation has a limit cycle?
(c) Repeat (b) using the square −8 ≤u ≤8,
−8 ≤v ≤8, and include the trajectories cor-
responding to the initial conditions u(0) = 1,
v(0) = 0, and u(0) = 6, v(0) = 0.
9.11
Chapter Review
Many problems in applied mathematics involve two or more unknown functions, as
well as their derivatives, and therefore require the solution of a system of differential
equations. Two such problems that we have considered in this chapter (Section 9.7)
are (a) coupled spring-mass systems and (b) mixing problems involving chemicals in a
system of two connected tanks.
A ﬁrst-order linear system of differential equations for n unknown functions
x1(t), x2(t), . . . , xn(t) may be written in the form
x′(t) = A(t)x(t) + b(t),
(9.11.1)
where
x(t) =
⎡
⎢⎢⎢⎣
x1(t)
x2(t)
...
xn(t)
⎤
⎥⎥⎥⎦,
x′(t) =
⎡
⎢⎢⎢⎣
x′
1(t)
x′
2(t)
...
x′
n(t)
⎤
⎥⎥⎥⎦,
and
A(t) =
⎡
⎢⎢⎢⎣
a11(t)
a12(t)
. . .
a1n(t)
a21(t)
a22(t)
. . .
a2n(t)
...
...
...
an1(t)
an2(t)
. . .
ann(t)
⎤
⎥⎥⎥⎦,
b(t) =
⎡
⎢⎢⎢⎣
b1(t)
b2(t)
...
bn(t)
⎤
⎥⎥⎥⎦.
Equation (9.11.1) is called a vector differential equation. A primary goal of this chapter
has been to develop techniques for solving Equation (9.11.1) for the unknown vector
function x(t). To do this, we have assumed throughout much of the chapter that the
matrix A(t) is constant.
Homogeneous First-Order Linear Systems
In Sections 9.4 and 9.5, we have developed solution techniques for (9.11.1) in the case
when this system is homogeneous (i.e., b(t) = 0). In this case, we often abbreviate

664
CHAPTER 9
Systems of Differential Equations
Equation (9.11.1) as
x′ = Ax,
(9.11.2)
where A is an n × n matrix function. It turns out that the solution set to the homoge-
neous system (9.11.2) is a vector space of dimension n, and consequently, the goal is to
determine n linearly independent solutions x1(t), x2(t), . . . , xn(t), called a fundamental
solution set, for the linear system (9.11.2). The fundamental solution set is therefore a
basis for the space of all solutions to (9.11.2), and thus, allows us to write the general
solution to (9.11.2) in the form
x(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t).
(9.11.3)
This general solution is sometimes written in the form
x(t) = X(t)c,
where
X(t) = [x1(t) x2(t) . . . xn(t)]
(9.11.4)
is called a fundamental matrix for the linear system (9.11.2) and c = (c1 c2 . . . cn)T .
If values
x1(t0),
x2(t0),
. . . ,
xn(t0)
(9.11.5)
are speciﬁed, then we can solve for the values of c1, c2, . . . , cn, thereby determining the
solution to the initial-value problem (9.11.2) with initial conditions given by (9.11.5).
In the case when A is an n ×n matrix of constants, the eigenvalues and eigenvectors
of A play a crucial role in ﬁnding a fundamental solution set for (9.11.2). In particular,
if λ is an eigenvalue of A with corresponding eigenvector v, then
x(t) = eλtv
is a solution to the system (9.11.2). Moreover, if v1, v2, . . . , vn are linearly independent
eigenvectors, corresponding to (not necessarily distinct) eigenvalues λ1, λ2, . . . , λn of
A, then the vector functions
xi(t) = eλitvi,
i = 1, 2, . . . , n
(9.11.6)
are linearly independent. In the case where A is nondefective (Section 9.4), we there-
fore already have a natural way, by using the vector functions in (9.11.6), to obtain a
fundamental solution set to x′ = Ax.
The case in which A is a defective matrix is discussed in Section 9.5. In this case,
we do not have n linearly independent eigenvectors of A at our disposal, so the strategy
becomes to manufacture so-called generalized eigenvectors that enable us to gener-
ate additional solutions to the system (9.11.2). Although these solutions take a more
complicated form, it can be shown that any system of the form (9.11.2) has n linearly
independent solutions x1(t), x2(t), . . . , xn(t) that can be built using eigenvectors and
generalized eigenvectors.
Matrix Exponential Function
Alternatively, we can use the matrix exponential function eAt, ﬁrst introduced in
Chapter 7, to directly derive n linearly independent solutions to x′ = Ax. To do this, we
observed in Section 9.8 that if v1, v2, . . . , vn are any n linearly independent vectors in
Rn (or Cn), then each of the vector functions
x1(t) = eAtv1,
x2(t) = eAtv2,
. . . ,
xn(t) = eAtvn
(9.11.7)
is a solution to x′ = Ax. Moreover, the n solutions in (9.11.7) are linearly independent.

9.11
Chapter Review 665
Nonhomogeneous First-Order Linear Systems
In Section 9.6, we considered the nonhomogeneous linear system (9.11.1). In this case,
the general solution takes the form
x(t) = c1x1 + c2x2 + · · · + cnxn + xp,
where {x1, x2, . . . , xn} is a fundamental solution set for the corresponding homoge-
neous system (9.11.2) and xp is one particular solution to (9.11.1). In the text, we used
the variation-of-parameters technique for linear systems to derive a particular solution.
Explicitly,
xp(t) = X(t)
6 t
X−1(s)b(s) ds,
where X(t) is the fundamental matrix given in Equation (9.11.4).
Qualitative Analysis
If the matrix A in the system x′ = Ax is not constant, or if the system of differential equa-
tions is nonlinear, we must resort to either a qualitative analysis or numerical techniques
to study the system. In Sections 9.9 and 9.10, we considered the qualitative aspects of
linear systems in the case of two differential equations and two unknown functions. If
A =
'a
b
c
d
(
is a 2 × 2 invertible matrix, the system
x′(t) = Ax(t)
can be solved for the trajectory x(t) in the xy-plane (called the phase plane). The
collection of all valid trajectories in the phase plane is known as the phase portrait. Once
more, the analysis hinges on the eigenvalues λ and corresponding eigenvectors v of A.
Various cases arise according to whether the values of λ are real or complex, repeated
or distinct, positive or negative, and so on. The equilibrium point may be a stable node,
unstable node, proper node, degenerate node, saddle point, stable spiral point, or unstable
spiral point.
Nonlinear Systems of Differential Equations
A system of two differential equations that is not linear can in most cases be approximated
at an equilibrium point by a linear system whose behavior is similar near the equilibrium
point to the original system.
Additional Problems
1. Verify that x1(t) =
'
et2−t
−1
(
and x2(t) =
' 0
2et
(
are
linearly independent solutions to x′ = Ax, where
A =
'2t −1
0
et−t2
1
(
.
Write the general solution to the system x′ = Ax.
2. Consider the linear system x′ = Ax + b, where
A =
⎡
⎢⎣
t cot(t2)
0
t cos(t2)/2
0
1/t
−1
csc(t2)
1
−1
⎤
⎥⎦,
b =
⎡
⎢⎣
0
2 −t sin t
1 −t cos t
⎤
⎥⎦.

666
CHAPTER 9
Systems of Differential Equations
(a) Verify that x(t) =
⎡
⎣
sin(t2)
t cos t
2
⎤
⎦is a solution to this
system.
(b) Is it possible for a constant vector x0 to solve the
system? Justify your answer.
For Problems 3–24, determine the general solution to the
linear system x′ = Ax for the given matrix A.
3.
'−6
1
6
−5
(
.
4.
'9
−2
5
−2
(
.
5.
'10
−4
4
2
(
.
6.
'−8
5
−5
2
(
.
7.
⎡
⎣
3
0
4
0
2
0
−4
0
−5
⎤
⎦.
8.
⎡
⎣
−3
−1
0
4
−7
0
6
6
4
⎤
⎦.
9.
' 3
13
−1
−3
(
.
10.
'−3
−10
5
11
(
.
11.
⎡
⎣
−1
−5
1
4
−9
−1
0
0
3
⎤
⎦.
12.
⎡
⎣
−4
0
0
2
5
−9
0
5
−1
⎤
⎦.
13.
⎡
⎣
2
−2
1
1
−4
1
2
2
−3
⎤
⎦.
[Hint: The eigenvalues of A are λ = 2, −2, −5.]
14.
⎡
⎣
2
−4
3
−9
−3
−9
4
4
3
⎤
⎦.
[Hint: The eigenvalues of A are λ = 6, −3, −1.]
15.
⎡
⎣
−17
0
−42
−7
4
−14
7
0
18
⎤
⎦.
[Hint: The eigenvalues of A are λ = 4, −3.]
16.
⎡
⎣
−16
30
−18
−8
8
16
8
−15
9
⎤
⎦.
[Hint: The eigenvalues of A are λ = 8, −7, 0.]
17.
⎡
⎣
−7
−6
−7
−3
−3
−3
7
6
7
⎤
⎦.
[Hint: The eigenvalues of A are λ = 0, −3.]
18.
⎡
⎣
3
−1
−2
1
6
1
1
0
6
⎤
⎦.
[Hint: The only eigenvalue of A is λ = 5.]
19.
⎡
⎣
−1
−4
−2
−4
−5
−6
4
8
7
⎤
⎦.
[Hint: The eigenvalues of A are λ = −1, 1 ± 2i.]
20.
⎡
⎣
7
−2
2
0
4
−1
−1
1
4
⎤
⎦.
[Hint: The only eigenvalue of A is λ = 5.]
21.
⎡
⎣
−3
−1
−2
1
0
1
1
0
0
⎤
⎦.
22.
⎡
⎣
−2
0
−1
0
−1
0
1
0
0
⎤
⎦.
23.
⎡
⎢⎢⎣
2
13
0
0
−1
−2
0
0
0
0
2
4
0
0
0
2
⎤
⎥⎥⎦.
24.
⎡
⎢⎢⎣
7
0
0
−1
0
6
0
0
0
0
−1
0
2
0
0
5
⎤
⎥⎥⎦.

9.11
Chapter Review 667
For Problems 25–29, use the variation-of-parameters method
to determine a particular solution to the nonhomogeneous
linear system x′ = Ax + b. Also ﬁnd the general solution to
the system.
25. A =
'−6
1
6
−5
(
, b =
' 1
e−t
(
.
26. A =
'9
−2
5
−2
(
, b =
'9t
0
(
.
27. A =
'10
−4
4
2
(
, b =
8 0
1
t e6t
9
.
28. A =
⎡
⎣
2
−4
3
−9
−3
−9
4
4
3
⎤
⎦, b =
⎡
⎣
e6t
1
0
⎤
⎦.
[Hint: The eigenvalues of A are λ = 6, −3, −1.]
29. A =
⎡
⎣
2
−2
1
1
−4
1
2
2
−3
⎤
⎦, b =
⎡
⎣
t
0
1
⎤
⎦.
[Hint: The eigenvalues of A are λ = 2, −2, −5.]
30. True or False: If X(t) is a fundamental matrix for the
linear system x′ = Ax, then X(t)T is a fundamental
matrix for the linear system x′ = AT x.
31. True or False: If x0 is a solution to the linear system
x′ = Ax, then x0 is also a solution to the linear system
x′′ = A2x.
32. Show that the function D : Vn(I) →Vn(I) deﬁned
by
D(x(t)) = x′(t)
is a linear transformation.
33. Consider the differential equation
d3y
dt3 + a d2y
dt2 + bdy
dt + cy = 0,
(9.11.8)
where a, b, and c are arbitrary functions of t.
(a) Replace Equation (9.11.8) by an equivalent linear
system x′ = Ax for an appropriate 3 × 3 matrix
A.
(b) If y1 = f1(t), y2 = f2(t), and y3 = f3(t) are
solutions to Equation (9.11.8) on an interval I,
show that the corresponding solutions to the sys-
tem you derived in part (a) are
x1(t) =
⎡
⎣
f1(t)
f ′
1(t)
f ′′
1 (t)
⎤
⎦,
x2(t) =
⎡
⎣
f2(t)
f ′
2(t)
f ′′
2 (t)
⎤
⎦,
x3(t) =
⎡
⎣
f3(t)
f ′
3(t)
f ′′
3 (t)
⎤
⎦.
(c) Show that
W[x1, x2, x3](t) = W[y1, y2, y3](t).
For Problems 34–41, characterize the equilibrium point for
the system x′ = Ax and sketch the phase portrait.
34. A =
'−3
4
8
1
(
.
35. A =
'0
−6
1
−5
(
.
36. A =
' 5
9
−2
−1
(
.
37. A =
'−4
0
0
−4
(
.
38. A =
'7
−2
1
4
(
.
39. A =
'−3
−5
1
−7
(
.
40. A =
'−2
−1
1
−4
(
.
41. A =
'10
−8
2
2
(
.
For Problems 42–43, convert the given DE to a ﬁrst-order
system using the substitution u = y, v = dy
dt , and determine
the phase portrait for the resulting system.
42. d2y
dt2 + dy
dt −12y = 0.
43. d2y
dt2 + 25y = 0.

668
CHAPTER 9
Systems of Differential Equations
Project: Coupled Springs Revisited
Recall from Section 9.7 (See Equations (9.7.1) and (9.7.2) with x replaced by x1 and
y replaced by x2) that the system of differential equations that governs the motion of a
coupled spring-mass system is:
m1
d2x1
dt2 = −k1x1 + k2(x2 −x1),
(9.11.9)
m2
d2x2
dt2 = −k2(x2 −x1),
(9.11.10)
where x1 and x2 denote the displacement of the masses m1 and m2 from their equilibrium
positions, respectively, and k1 and k2 are the (positive) spring constants. Our approach
to solving systems of linear differential equations of order higher than ﬁrst order has
been to replace the system with an equivalent ﬁrst-order system. In the present case this
gave rise to a system of four equations (see Equations (9.7.5)–(9.7.7)). In this project we
develop an equivalent solution method that can be applied directly to the higher-order
system. We begin by writing the system (9.11.9), (9.11.10) in the equivalent form
x′′ = Ax,
(9.11.11)
where
A =
⎡
⎢⎢⎢⎣
−1
m1
(k1 + k2)
k2
m1
k2
m2
−k2
m2
⎤
⎥⎥⎥⎦.
(9.11.12)
1. Since we know that the motion is oscillatory, it makes sense to look for solutions
to (9.11.11) of the form
x(t) = v cos ωt
or x(t) = v sin ωt,
(9.11.13)
where v is a constant vector and ω is a positive constant.
(a) Verify, by direct substitution, that each of the vector functions given in
(9.11.13) is a solution to (9.11.11) provided λ = −ω2 is an eigenvalue
of A and v is a corresponding eigenvector.
(b) Derive the following characteristic equation for A:
λ2 +
' 1
m1
(k1 + k2) + k2
m2
(
λ + k1k2
m1m2
= 0,
(9.11.14)
and show that it has two distinct negative real roots.
(c) Let λ1 and λ2 be the eigenvalues determined in (b), and let v1 and v2 be the
corresponding linearly independent eigenvectors. Use your result from (a) to
derive the following four solutions to (9.11.11):
x1 = v1 cos ω1t,
x2 = v1 sin ω1t,
x3 = v2 cos ω2t,
x4 = v2 sin ω2t,
where ω1 = √−λ1, and ω2 = √−λ2.
(d) Verify that the solutions determined in (c) are linearly independent on any
interval, and write the general solution to (9.11.11).

9.11
Chapter Review 669
(e) Consider the couple spring-mass system with
k1 = 4 Nm−1,
k2 = 2 Nm−1,
m1 = 2 kg,
m2 = 1 kg.
At t = 0, both masses are pulled down a distance 1 m from equilibrium and
released from rest. Use the solution technique developed above to determine
the subsequent motion of the system. Compare your solution technique to
that used in Example 9.7.1 (page 626).
2. Now suppose that the spring-mass system is subjected to a periodic external force.
In this case the system (9.11.9) and (9.11.10) can be replaced by
x′′ = Ax + F cos ω0t
(9.11.15)
where ω0 is a positive constant, and F is a constant vector.
(a) Use a trial solution of the form
xp(t) = B cos ω0t
to derive the following particular solution to (9.11.15):
xp(t) = −
1
det(A + ω2
0I2)adj(A + ω2
0I2)F cos ω0t.
(b) Write the general solution to (9.11.15) in the case when A is given in
(9.11.12). Comment on the behavior of the spring-mass system if ω0 is close
to either ω1 or ω2.

10
The Laplace Transform
and Some Elementary
Applications
10.1
Definition of the Laplace Transform
In this chapter, we introduce another technique for solving linear, constant coefﬁcient
ordinary differential equations. Actually, the technique has a much broader usage than
this. For example, it is used in the solution of linear systems of differential equations,
partial differential equations, and also integral equations (see Section 10.9). The reader’s
immediate reaction may be to question the need for introducing a new method for solving
linear, constant coefﬁcient, ordinary differential equations, since our results from Chapter
8 can be applied to any such equation. To answer this question, consider the differential
equation
y′′ + ay′ + by = F,
where a and b are constants. We have seen how to solve this equation when F is a
continuous function on some interval I. However, in many problems that arise in en-
gineering, physics, and applied mathematics, F represents an external force that is act-
ing on the system under investigation, and often this force acts intermittently or even
instantaneously.1 Whereas the techniques from Chapter 8 can be extended to cover these
cases, the computations involved are tedious. In contrast, the approach introduced here
can handle such problems quite easily.
1For example, the switch in an RLC circuit may be turned on and off several times, or the mass in a
spring-mass system may be dealt an instantaneous blow at t = t0.
670

10.1
Definition of the Laplace Transform 671
First we need a deﬁnition.
DEFINITION
10.1.1
Let f be a function deﬁned on an interval [0, ∞). The Laplace transform of f is
the function F(s) deﬁned by
F(s) =
! ∞
0
e−st f (t)dt,
(10.1.1)
provided that the improper integral converges. We will usually denote the Laplace
transform of f by L[ f ].
Recall that the improper integral appearing in (10.1.1) is deﬁned by
! ∞
0
e−st f (t)dt = lim
N→∞
! N
0
e−st f (t)dt,
and that this improper integral converges if and only if the limit on the right-hand side
exists and is ﬁnite. It follows that not all functions deﬁned on [0, ∞) have a Laplace
transform. In the next section, we will address some of the theoretical aspects associated
with determining the types of functions for which (10.1.1) converges. For the remainder
of this section, we focus our attention on gaining familiarity with Deﬁnition 10.1.1, and
we derive some basic Laplace transforms.
Example 10.1.2
Determine the Laplace transform of the following functions:
(a) f (t) = 1.
(b) f (t) = t.
(c) f (t) = eat, where a is constant.
(d) f (t) = cos bt, where b is constant.
Solution:
(a) From the foregoing deﬁnition, we have
L[1] =
! ∞
0
e−stdt = lim
N→∞
"
−1
s e−st
#N
0
= lim
N→∞
"1
s −1
s e−sN
#
= 1
s ,
s > 0.
Notice that the restriction s > 0 is required for the improper integral to converge,
and hence, the Laplace transform F of f (t) = 1 is only deﬁned on the set of
positive real numbers.
(b) In this case, we use integration by parts to obtain
L[t] =
! ∞
0
e−sttdt = lim
N→∞
"
−te−st
s
#N
0
+
! ∞
0
1
s e−stdt.
But,
lim
N→∞Ne−sN = 0,
s > 0,

672
CHAPTER 10
The Laplace Transform and Some Elementary Applications
so that
L[t] =
! ∞
0
1
s e−stdt = lim
N→∞
"
−e−st
s2
#N
0
= 1
s2 ,
s > 0.
It is left as an exercise to show that more generally, for all positive integers n,
L[tn] =
n!
sn+1 ,
s > 0.
(10.1.2)
(c) In this case, we have
L[eat] =
! ∞
0
e−steatdt =
! ∞
0
e(a−s)tdt = lim
N→∞
"
1
a −s e(a−s)t
#N
0
=
1
s −a ,
provided that s > a. Thus,
L[eat] =
1
s −a ,
s > a.
(10.1.3)
(d) From the deﬁnition of the Laplace transform,
L[cos bt] =
! ∞
0
e−st cos bt dt.
Using the standard integral
!
eat cos bt dt =
eat
a2 + b2 (a cos bt + b sin bt) + c,
it follows that
L[cos bt] = lim
N→∞
" e−st
s2 + b2 (b sin bt −s cos bt)
#N
0
=
s
s2 + b2
provided that s > 0. Thus,
L[cos bt] =
s
s2 + b2 ,
s > 0.
(10.1.4)
Similarly, it can be shown that
L[sin bt] =
b
s2 + b2 ,
s > 0.
(10.1.5)
□
As illustrated by the preceding examples, the range of values s can assume must
often be restricted to ensure the convergence of the improper integral (10.1.1). For the
remainder of the chapter, we will often take for granted without mention that the Laplace
transforms we compute have a restricted domain.

10.1
Definition of the Laplace Transform 673
Linearity of the Laplace Transform
Suppose that the Laplace transform of both f and g exist for s > α, where α is a constant.
Then, using properties of convergent improper integrals, it follows that for s > α,
L[ f + g] =
! ∞
0
e−st[ f (t) + g(t)]dt =
! ∞
0
e−st f (t)dt +
! ∞
0
e−stg(t)dt
= L[ f ] + L[g].
Further, if c is any real number, then
L[cf ] =
! ∞
0
e−stcf (t)dt = c
! ∞
0
e−st f (t)dt = cL[ f ].
Consequently,
1. L[ f + g] = L[ f ] + L[g]
2. L[cf ] = cL[ f ]
so that the Laplace transform satisﬁes the basic properties of a linear transformation. This
linearity of L enables us to determine the Laplace transform of complicated functions
from a knowledge of the Laplace transform of some basic functions. This will be used
continuously throughout this chapter.
Example 10.1.3
Determine the Laplace transform of
f (t) = 4e3t + 2 sin 5t −7t3.
Solution:
Since the Laplace transform is linear, it follows that
L[4e3t + 2 sin 5t −7t3] = 4L[e3t] + 2L[sin 5t] −7L[t3].
Using the results of the previous example, we therefore obtain
L[4e3t + 2 sin 5t −7t3] =
4
s −3 +
10
s2 + 25 −42
s4 ,
s > 3.
□
Piecewise Continuous Functions
Thefunctionsthatwehaveconsideredintheforegoingexampleshaveallbeencontinuous
on [0, ∞). As we will see in later sections, the real power of the Laplace transform comes
from the fact that piecewise continuous functions can be transformed. Before illustrating
this point, we recall the deﬁnition of a piecewise continuous function.
DEFINITION
10.1.4
A function f is called piecewise continuous on the interval [a, b] if we can divide
[a, b] into a ﬁnite number of subintervals in such a manner that
1.
f is continuous on each subinterval, and
2.
f approaches a ﬁnite limit as the endpoints of each subinterval are approached
from within.
If f is piecewise continuous on every interval of the form [0, b], where b is a constant,
then we say that f is piecewise continuous on [0, ∞).

674
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Example 10.1.5
The function f deﬁned by
f (t) =
⎧
⎪⎨
⎪⎩
t2 + 1, 0 ≤t ≤1,
2 −t,
1 < t ≤2,
1,
2 < t ≤3,
is piecewise continuous on [0, 3], whereas
f (t) =
(
1
1−t , 0 ≤t < 1,
t,
1 ≤t ≤3,
is not piecewise continuous on [0, 3]. The graphs of these functions are shown in
Figure 10.1.1.
2
3
3
t
1
1
t
1
1
f(t)
f(t)
(a)
(b)
Figure 10.1.1: (a) An example of a piecewise continuous function. (b) An example of a
function that is not piecewise continuous.
□
Example 10.1.6
Determine the Laplace transform of the piecewise continuous function
f (t) =
)
t, 0 ≤t < 1,
−1,
t ≥1.
Solution:
The function is sketched in Figure 10.1.2. To determine the Laplace trans-
form of f , we use Deﬁnition 10.1.1.
L[ f ] =
! ∞
0
e−st f (t)dt =
! 1
0
e−st f (t)dt +
! ∞
1
e−st f (t)dt
=
! 1
0
te−stdt −
! ∞
1
e−stdt =
"
−1
s te−st −1
s2 e−st
#1
0
+ lim
N→∞
"1
s e−st
#N
1
= −1
s e−s −1
s2 e−s + 1
s2 −1
s e−s,
provided that s > 0. Thus,
L[ f ] = 1
s2 [1 −e−s(2s + 1)],
s > 0.
□
t
1
21
1
f(t)
Figure 10.1.2: The piecewise
continuous function in Example
10.1.6.

10.1
Definition of the Laplace Transform 675
Exercises for 10.1
Key Terms
Laplace transform, Piecewise continuous function.
Skills
• Be able to determine the Laplace transform of a given
function f .
• Be able to use the linearity of the Laplace transform
to assist in computing Laplace transforms.
• Be able to determine whether or not a given function
is piecewise continuous.
True-False Review
For Questions (a)–(i), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The function
f (t) =
) t,
if t is an integer,
t2,
if t is not an integer,
is piecewise continuous on [0, ∞).
(b) The function
g(t) =
⎧
⎨
⎩
1
sin t , if t is not a multiple of π,
0,
if t is a multiple of π,
is piecewise continuous on [0, ∞).
(c) If f and g are piecewise continuous functions on an
interval I, then so is f + g.
(d) If f is piecewise continuous on the interval [a, b] and
on the interval [b, c], then f is piecewise continuous
on the interval [a, c].
(e) The Laplace transform of a function f is a function
F(s) deﬁned by
F(s) =
! ∞
1
e−st f (t)dt,
provided that this improper integral converges.
(f) The Laplace transform F(s) of f (t) = et is only
deﬁned for s > 1.
(g) The Laplace transform F(s) of f (t) = 2 cos 3t is only
deﬁned for s > 3.
(h) For any function f such that f (x) > 0 for all x in
[0, ∞), L
" 1
f
#
=
1
L[ f ].
(i) For any function f such that f (x) > 0 for all x in
[0, ∞), L[ f 2] = L[ f ]2.
Problems
For Problems 1–12, use (10.1.1) to determine L[ f ].
1. f (t) = t −1.
2. f (t) = e2t.
3. f (t) = tet.
4. f (t) = sin bt, where b is constant.
5. f (t) = sinh bt, where b is constant.
6. f (t) = cosh bt, where b is constant.
7. f (t) = 3e2t.
8. f (t) = 2t.
9. f (t) =
)
t2, 0 ≤t ≤1,
1,
t > 1.
10. f (t) =
)
1, 0 ≤t < 2,
−1,
t ≥2.
11. f (t) = e2t cos 3t.
12. f (t) = et sin t.
For Problems 13–22, use the linearity of L and the formulas
derived in this section to determine L[ f ].
13. f (t) = 2 sin 3t + 4t3.
14. f (t) = 2t −e2t.
15. f (t) = sinh bt, where b is constant.
16. f (t) = cosh bt, where b is constant.
17. f (t) = 7e−2t + 1.
18. f (t) = 3t2 −5 cos 2t + sin 3t.
19. f (t) = 4 cos(t −π/4).
20. f (t) = 2e−3t + 4et −5 sin t.

676
CHAPTER 10
The Laplace Transform and Some Elementary Applications
21. f (t) = 2 sin2 4t −3.
22. f (t) = 4 cos2 bt, where b is constant.
ForProblems23–30,sketchthegivenfunctionanddetermine
whether it is piecewise continuous on [0, ∞).
23. f (t) =
⎧
⎨
⎩
1,
0 ≤t ≤1,
1 −t,
t > 2,
1,
t > 2.
24. f (t) =
⎧
⎨
⎩
3, 0 ≤t ≤1,
0, 1 ≤t < 3,
−1,
t ≥3.
25. f (t) =
)
t,
0 ≤t ≤1,
1/t2,
t > 1.
26. f (t) =
)
1,
0 ≤t ≤1,
1/(t −1),
t > 1.
27. f (t) = t, 0 ≤t < 1,
f (t + 1) = f (t).
28. f (t) = n, n ≤t < n + 1, n = 0, 1, 2, . . . .
29. f (t) =
2
t + 1.
30. f (t) =
1
t −2.
ForProblems31–34,sketchthegivenfunctionanddetermine
its Laplace transform.
31. f (t) =
)
1, 0 ≤t ≤2,
−1,
t > 2.
32. f (t) =
) t,
0 ≤t ≤1,
0,
t ≥1.
33. f (t) =
⎧
⎨
⎩
t,
0 ≤t < 1,
1,
1 ≤t < 3,
et−3,
t > 3.
34. f (t) =
⎧
⎨
⎩
0,
0 ≤t ≤1,
t,
1 < t ≤2,
0,
t > 2.
35. Recall that according to Euler’s formula
eibt = cos bt + i sin bt.
Since the Laplace transform is linear, it follows that
L[cos bt] = Re(L[eibt]),
L[sin bt] = Im(L[eibt]).
Find L[eibt], and hence, derive Equations (10.1.4)
and (10.1.5).
36. Use the technique introduced in the previous problem
to determine
L[eat cos bt] and L[eat sin bt],
where a and b are arbitrary constants.
37. Use mathematical induction to prove that for every
positive integer n,
L[tn] =
n!
sn+1 .
38. (a) By making the change of variables t = x2
s (s > 0)
in the integral that deﬁnes the Laplace transform,
show that
L[t−1/2] = 2s−1/2
! ∞
0
e−x2dx.
(b) Use your result in (a) to show that
(L[t−1/2])2 = 4s−1
! ∞
0
! ∞
0
e−(x2+y2)dx dy.
(c) By changing to polar coordinates, evaluate the
double integral in (b), and hence, show that
L[t−1/2] =
*π
s , s > 0.
10.2
The Existence of the Laplace Transform and the Inverse Transform
Intheprevioussection,wederivedtheLaplacetransformofseveralelementaryfunctions.
In this section, we address some of the more theoretical aspects of the Laplace transform.
The ﬁrst question that we wish to answer is the following:
What types of functions have a Laplace transform?
We will not be able to answer this question completely, since it requires a deeper mathe-
matical background than we assume of the reader. However, we can identify a very large
class of functions that are Laplace transformable.

10.2
The Existence of the Laplace Transform and the Inverse Transform 677
By deﬁnition, the Laplace transform of a function f is
L[ f ] =
! ∞
0
e−st f (t)dt,
(10.2.1)
provided that the integral converges. If f is piecewise continuous on an interval [a, b],
then it is a standard result from calculus that f is also integrable over [a, b]. Thus, if
we restrict attention to functions that are piecewise continuous on [0, ∞), it follows that
the integral
! b
0
e−st f (t)dt
exists for all positive (and ﬁnite) b. However, it does not follow that the Laplace trans-
form of f exists, since the improper integral in (10.2.1) may still diverge. To guarantee
convergence of the integral, we must ensure that the integrand in (10.2.1) approaches
zero rapidly enough as t →∞. As we show next, this will be the case provided that, in
addition to being piecewise continuous, f also satisﬁes the following deﬁnition:
DEFINITION
10.2.1
A function f is said to be of exponential order if there exist constants M and α
such that
| f (t)| ≤Meαt,
for all t > 0.
Example 10.2.2
The function f (t) = 10e7t cos 5t is of exponential order, since
| f (t)| = 10e7t| cos 5t| ≤10e7t.
□
Now let E(0, ∞) denote the set of all functions that are both piecewise continuous
on [0, ∞) and of exponential order. If we add two functions that are in E(0, ∞), the
result is a new function that is also in E(0, ∞). Similarly, if we multiply a function in
E(0, ∞) by a constant, the result is once more a function in E(0, ∞). It follows from
Theorem 4.3.2 that E(0, ∞) is a subspace of the vector space of all functions deﬁned on
[0, ∞). We will show next that the functions in the vector space E(0, ∞) have a Laplace
transform. Before doing so, we need to state a basic result about the convergence of
improper integrals.
Lemma 10.2.3
(The Comparison Test for Improper Integrals)
Suppose that 0 ≤G(t) ≤H(t) for 0 ≤t < ∞. If
! ∞
0
H(t)dt converges, then
! ∞
0
G(t)dt converges.
Proof This can be found in any textbook on advanced calculus.
We also recall that if
! ∞
0
|F(t)|dt converges, then so does
! ∞
0
F(t)dt. We can
now establish a key existence theorem for the Laplace transform.

678
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Theorem 10.2.4
If f is in E(0, ∞), then there exists a constant α such that
L[ f ] =
! ∞
0
e−st f (t)dt
exists for all s > α.
Proof Since f is piecewise continuous on [0, ∞), e−st f (t) is integrable over any ﬁnite
interval. Further, since f is in E(0, ∞), there exist constants M and α such that
| f (t)| ≤Meαt,
for all t > 0. We now use the comparison test for integrals to establish that the improper
integral deﬁning the Laplace transform converges on the domain (α, ∞). Let
F(t) = |e−st f (t)|.
Then,
F(t) = e−st| f (t)| ≤Me(α−s)t.
But, for s > α,
! ∞
0
Me(α−s)tdt = lim
N→∞
! N
0
Me(α−s)tdt =
M
s −α .
Applying the comparison test for improper integrals with F(t) as just deﬁned and G(t) =
Me(α−s)t, it follows that
! ∞
0
|e−st f (t)|dt
converges for s > α, and hence, so also does
! ∞
0
e−st f (t)dt.
Thus, we have shown that L[ f ] exists for s > α, as required.
Remark
The preceding theorem gives only sufﬁcient conditions that guarantee the
existence of the Laplace transform. There are functions that are not in E(0, ∞), but that
do have a Laplace transform. For example, f (t) = t−1/2 is certainly not in E(0, ∞),
but L[t−1/2] =
*π
s . (See Problem 38 in the previous section.)
The Inverse Laplace Transform
Let V denote the subspace of E(0, ∞) consisting of all continuous functions of expo-
nential order. We have seen in the previous section that the Laplace transform satisﬁes
L[ f + g] = L[ f ] + L[g],
L[cf ] = cL[ f ].
Consequently, L deﬁnes a linear transformation of V onto Rng(L). Further, it can be
shown (Problem 22) that L is also one-to-one, and therefore, from the results of Sec-
tion 6.4, the inverse transformation, L−1, exists and is deﬁned as follows:

10.2
The Existence of the Laplace Transform and the Inverse Transform 679
DEFINITION
10.2.5
The linear transformation L−1 : Rng(L) →V deﬁned by
L−1[F](t) = f (t) if and only if L[ f ](s) = F(s)
(10.2.2)
is called the inverse Laplace transform.
Remark
We emphasize the fact that L−1 is a linear transformation, so that
L−1[F + G] = L−1[F] + L−1[G],
and
L−1[cF] = cL−1[F],
for all F and G in Rng(L) and all real numbers c. This can either be seen directly from
(10.2.2) or from the general theory of inverse linear transformations.
In Section 10.1, we derived the transforms
L[tn] =
n!
sn+1 ,
L[eat] =
1
s −a ,
L[cos bt] =
s
s2 + b2 ,
L[sin bt] =
b
s2 + b2 ,
from which we directly obtain the inverse transforms
L−1
"
1
sn+1
#
= 1
n!tn,
L−1
"
1
s −a
#
= eat,
L−1
"
s
s2 + b2
#
= cos bt,
L−1
"
b
s2 + b2
#
= sin bt.
Example 10.2.6
Find L−1[F](t) if
(a) F(s) = 2
s2 .
(b) F(s) =
3s
s2 + 4.
(c) F(s) =
3s + 2
(s −1)(s −2).
Solution:
(a) L−1
" 2
s2
#
= 2L−1
" 1
s2
#
= 2t.
(b) L−1
"
3s
s2 + 4
#
= 3L−1
"
s
s2 + 4
#
= 3 cos 2t.

680
CHAPTER 10
The Laplace Transform and Some Elementary Applications
(c) In this case, it is not obvious at ﬁrst sight what the appropriate inverse transform
is. However, decomposing F(s) into partial fractions yields2
F(s) =
3s + 2
(s −1)(s −2) =
8
s −2 −
5
s −1.
Consequently, using linearity of L−1,
L−1
"
3s + 2
(s −1)(s −2)
#
= L−1
"
8
s −2
#
−L−1
"
5
s −1
#
= 8L−1
"
1
s −2
#
−5L−1
"
1
s −1
#
= 8e2t −5et.
□
If we relax the assumption that V contain only continuous functions of exponential
order, then it is no longer true that L is one-to-one, and so, for a given F(s), there will
be (inﬁnitely) many piecewise continuous functions f with the property that
L[ f ] = F(s).
Thus, we lose the uniqueness of L−1[F]. However, it can be shown (see, for example,
R.V. Churchill, Modern Operational Mathematics in Engineering, McGraw-Hill, 1944)
that if two functions have the same Laplace transform, then they can only differ in their
values at points of discontinuity. This does not affect the solution to our problems, and
therefore, we will use (10.2.2) to determine the inverse Laplace transform, even if f is
piecewise continuous.
Example 10.2.7
In the previous section, we have shown that the Laplace transform of the piecewise
continuous function
f (t) =
)
t, 0 ≤t < 1,
−1,
t ≥1.
is
L[ f ] = 1
s2 [1 −e−s(2s + 1)],
s > 0.
Consequently,
L−1
) 1
s2 [1 −e−s(2s + 1)]
+
= f (t).
□
It is possible to give a general formula for determining the inverse Laplace transform
of F(s) in terms of a contour integral in the complex plane. However, this is beyond the
scope of the present treatment of the Laplace transform. In practice, as in the previous
examples, we determine inverse Laplace transforms by recognizing F(s) as being the
Laplace transform of an appropriate function f (t). In order for this approach to work,
we need to memorize a few basic transforms and then be able to use these transforms to
determine the inverse Laplace transform of more complicated functions. This is similar
to the way that we learn how to integrate. The transform pairs that will be needed for
the remainder of the text are listed in Table 10.2.1. Several of the transforms given in
this table will be derived in the following sections. More generally, very large tables
2It is very important in this chapter to be able to perform partial fractions decompositions. A review of this
technique is given in Appendix B.

10.2
The Existence of the Laplace Transform and the Inverse Transform 681
of Laplace transforms have been compiled for use in applications, and most current
computer algebra systems (such as Maple, Mathematica, and so on) have the built-in
capability to determine Laplace transforms.
Table 10.2.1
Function f (t)
Laplace Transform F(s)
f (t) = tn,
n a nonnegative integer
F(s) =
n!
sn+1 , s > 0.
f (t) = eat,
a constant
F(s) =
1
s −a , s > a.
f (t) = sin bt,
b constant
F(s) =
b
s2 + b2 , s > 0.
f (t) = cos bt,
b constant
F(s) =
s
s2 + b2 , s > 0.
f (t) = t−1/2
F(s) = (π/s)1/2, s > 0.
f (t) = ua(t) (see Section 10.7)
F(s) = 1
s e−as.
f (t) = δ(t −a) (see Section 10.8)
F(s) = e−as.
Transform of Derivatives (see Section 10.4)
f ′
L[ f ′] = sL[ f ] −f (0).
f ′′
L[ f ′′] = s2L[ f ] −s f (0) −f ′(0).
Shifting Theorems (see Sections 10.5 and 10.7)
eat f (t)
F(s −a).
ua(t) f (t −a)
e−as F(s).
Exercises for 10.2
Key Terms
Exponential order, Comparison test for improper integrals,
Inverse Laplace transform.
Skills
• Be able to decide whether or not a given function is of
exponential order.
• Be able to determine the inverse Laplace transform of
a given function.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If f is a function of exponential order, and |g(x)| <
f (x) for all x, then g is of exponential order.
(b) If f and g are functions of exponential order, then so
is f + g.
(c) If 0 ≤G(t) ≤H(t) for 0 ≤t < ∞and
! ∞
0
G(t)dt
converges, then so does
! ∞
0
H(t)dt.
(d) The inverse Laplace transform operator is a linear
transformation.
(e) The inverse Laplace transform of the function F(s) =
s
s2 + 9 is f (t) = sin 3t.
(f) The inverse Laplace transform of the function F(s) =
1
s + 3 is the function f (t) = e3t.

682
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Problems
For Problems 1–5, show that the given function is of expo-
nential order.
1. f (t) = e2t.
2. f (t) = cos 2t.
3. f (t) = te−2t.
4. f (t) = e3t sin 4t.
5. f (t) = tneat, where a and n are positive integers.
6. Show that if f and g are in E(0, ∞), then so are f +g
and cf for any scalar c.
For Problems 7–21, determine the inverse Laplace transform
of the given function.
7. F(s) =
3
s −2.
8. F(s) = 2
s .
9. F(s) =
1
s2 + 4.
10. F(s) =
5
s + 3.
11. F(s) = 4
s3 .
12. F(s) =
2s
s2 + 9.
13. F(s) = 2s + 1
s2 + 16.
14. F(s) = s + 6
s2 + 1.
15. F(s) = 4
s2 −s + 2
s2 + 9.
16. F(s) = 2
s −
3
s + 1.
17. F(s) =
s −2
(s + 1)(s2 + 4).
18. F(s) =
1
s(s + 1).
19. F(s) =
s + 4
(s −1)(s + 2)(s −3).
20. F(s) =
2s + 3
(s −2)(s2 + 1).
21. F(s) =
2s + 3
(s2 + 4)(s2 + 1).
22. This exercise veriﬁes the claim in the text that the
Laplace transform deﬁnes a one-to-one linear trans-
formation from V to Rng(L). Let f be a continuous
function of exponential order. It sufﬁces to prove that
if f is not identically zero, then L[ f ] ̸= 0.
[Hint: Show that L[| f |] > 0 if f is not identically
zero.]
10.3
Periodic Functions and the Laplace Transform
Many of the functions that arise in engineering applications are periodic on some interval.
Due to the symmetry associated with a periodic function, we might suspect that the
evaluation of the Laplace transform of such a function can be reduced to an integration
overoneperiodofthefunction.Beforeestablishingthisresult,weﬁrstrecallthedeﬁnition
of a periodic function.
DEFINITION
10.3.1
A function f deﬁned on an interval [0, ∞) is said to be periodic with period T if T
is the smallest positive real number that satisﬁes the equation
f (t + T ) = f (t),
for all t ≥0.
The most familiar examples of periodic functions are the trigonometric functions
sine and cosine, which have period 2π.

10.3
Periodic Functions and the Laplace Transform 683
Example 10.3.2
The function f deﬁned by
f (t) =
( 2,
0 ≤t ≤1,
f (t + 2) = f (t),
1, 1 < t < 2,
is periodic on [0, ∞) with period 2. (See Figure 10.3.1.)
t
1
2
1
3
5
f(t)
2
4
6
Figure 10.3.1: A function that is periodic on [0, ∞) with period 2.
□
The following theorem can be used to simplify the evaluation of the Laplace trans-
form of a periodic function.
Theorem 10.3.3
Let f be in E(0, ∞). If f is periodic on [0, ∞) with period T , then
L[ f ] =
1
1 −e−sT
! T
0
e−st f (t)dt.
Proof By deﬁnition of the Laplace transform, we have
L[ f ] =
! ∞
0
e−st f (t)dt
=
! T
0
e−st f (t)dt +
! 2T
T
e−st f (t)dt + · · · +
! (n+1)T
nT
e−st f (t)dt + · · · .
Now consider the general integral
I =
! (n+1)T
nT
e−st f (t)dt.
If we let x = t −nT , then dx = dt. Further, t = nT corresponds to x = 0, whereas
t = (n + 1)T corresponds to x = T . Hence, I can be written in the equivalent form
I =
! T
0
e−s(x+nT ) f (x + nT )dx = e−snT
! T
0
e−sx f (x)dx,
where we have used the fact that f is periodic of period T to replace f (x + nT ) by
f (x). All of the integrals that arise in the expression for L[ f ] are of the preceding form
for an appropriate value of n. It follows, therefore, that we can write
L[ f ] = (1 + e−sT + e−2sT + · · · + e−nsT + · · · )
! T
0
e−sx f (x)dx.
(10.3.1)

684
CHAPTER 10
The Laplace Transform and Some Elementary Applications
However, the series multiplying the integral is just a geometric series with common ratio3
e−sT . Consequently, the sum of the geometric series is
1
1 −e−sT , so that
L[ f ] =
1
1 −e−sT
! T
0
e−sT f (t)dt,
where we have replaced the dummy variable x in (10.3.1) by t without loss of generality.
Example 10.3.4
Determine the Laplace transform of
f (t) =
(
sin t,
0 ≤t ≤π,
f (t + 2π) = f (t).
0,
π ≤t < 2π,
Solution:
Since the given function is periodic on [0, ∞) with period 2π (see
Figure 10.3.2), we can use Theorem 10.3.3 to determine L[ f ]. We have
L[ f ] =
1
1 −e−2πs
! 2π
0
e−st f (t)dt =
1
1 −e−2πs
! π
0
e−st sin t dt.
t
1
p
2p
3p
f(t)
Figure 10.3.2: The periodic function deﬁned in Example 10.3.4.
Using the standard integral
!
eat sin bt dt =
1
a2 + b2 eat(a sin bt −b cos bt) + c,
it follows that
L[ f ] =
1
1 −e−2πs
)
−
1
s2 + 1
,
e−st(cos t + s sin t)
-π
0
+
=
1
1 −e−2πs
.e−sπ + 1
s2 + 1
/
.
Substituting for
1 −e−2πs = (1 −e−πs)(1 + e−πs)
yields
L[ f ] =
1
(s2 + 1)(1 −e−πs).
□
3Recall that an inﬁnite series of the form a + ar + ar2 + ar3 + · · · is called a geometric series with common
ratio r. If |r| < 1, then the sum of such a series is a/(1 −r).

10.4
The Transform of Derivatives and Solution of Initial-Value Problems 685
Exercises for 10.3
Key Terms
Periodic function, Period.
Skills
• Be able to determine the Laplace transform of a given
periodic function.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Any periodic function f has more than one period.
(b) If f is a periodic function and g is a function such
that g(t) = f (t + c) for some constant c, then g is a
periodic function with the same period as f .
(c) The function f (t) = cos 2t is periodic with period
π/2.
(d) The function f (t) = sin(t2) is periodic.
(e) Every piecewise continuous function f is periodic.
(f) If m and n are positive integers and f is a periodic
function with period m and g is a periodic function
with period n, then f + g is a periodic function with
period m + n.
(g) If m, n, f , and g are as in the previous question, then
f g is a periodic function with period mn.
(h) The Laplace transform of a periodic function is a pe-
riodic function.
Problems
For Problems 1–9, determine the Laplace transform of the
given function.
1. f (t) = t2, 0 ≤t < 2,
f (t + 2) = f (t).
2. f (t) = t, 0 ≤t < 1,
f (t + 1) = f (t).
3. f (t) = cos t, 0 ≤t < π,
f (t + π) = f (t).
4. f (t) = sin t, 0 ≤t < π,
f (t + π) = f (t).
5. f (t) = et, 0 ≤t < 1,
f (t + 1) = f (t).
6. f (t) =
) 2t/π,
0 ≤t < π/2,
sin t,
π/2 ≤t < π, ,
where f (t + π) = f (t).
7. f (t) =
)
1, 0 ≤t < 1,
−1, 1 ≤t ≤2, , where f (t + 2) = f (t).
8. f (t) = | cos t|, 0 ≤t < π, f (t + π) = f (t).
9. The triangular wave function (see Figure 10.3.3)
f (t) =
)
t/a,
0 ≤t < a,
(2a −t)/a, a ≤t < 2a,
where f (t + 2a) = f (t), for a positive constant a.
1
a
2a
4a
t
f(t)
Figure 10.3.3: A triangular wave function.
10. Use Theorem 10.3.3, together with the fact that f (t) =
sin at is periodic on the interval [0, 2π/a], to deter-
mine L[ f ].
11. Repeat the previous problem for the function f (t) =
cos at.
10.4
The Transform of Derivatives and Solution of Initial-Value Problems
The reason that we have introduced the Laplace transform is that it provides an alternative
technique for solving differential equations. To see how this technique arises, we must
ﬁrst consider how the derivative of a function transforms.

686
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Theorem 10.4.1
Suppose that f is of exponential order on [0, ∞) and that f ′ exists and is piecewise
continuous on [0, ∞). Then L[ f ′] exists and is given by
L[ f ′] = sL[ f ] −f (0).
Proof For simplicity we consider the case when f ′ is continuous on [0, ∞). The
extension to the case of piecewise continuity is straightforward. Since f is differen-
tiable and of exponential order on [0, ∞), it follows that it belongs to E(0, ∞), and
hence its Laplace transform exists. By deﬁnition of the Laplace transform, we have
L[ f ′] =
! ∞
0
e−st f ′(t)dt =
,
e−st f (t)
-∞
0 + s
! ∞
0
e−st f (t)dt.
That is, since f is of exponential order on [0, ∞),
L[ f ′] = sL[ f ] −f (0).
Example 10.4.2
Solve the initial-value problem
dy
dt = t,
y(0) = 1.
Solution:
This problem can be solved by a direct integration. However, we will use
the Laplace transform. Taking the Laplace transform of both sides of the given differential
equation and using the result of the previous theorem, we obtain
sY(s) −y(0) = 1
s2 .
This is an algebraic equation for Y(s). Substituting in the initial condition and solving
algebraically for Y(s) yields
Y(s) = 1
s3 + 1
s .
To determine the solution of the original problem, we now take the inverse Laplace
transform of both sides of this equation. The result is
y(t) = L−1
" 1
s3 + 1
s
#
.
That is, since L−1
"
1
sn+1
#
= 1
n!tn,
y(t) = 1
2t2 + 1.
□
The foregoing example illustrates the basic steps in solving an initial-value problem
using the Laplace transform. We proceed as follows:
1. Take the Laplace transform of the given differential equation, and substitute
in the given initial conditions.
2. Solve the resulting equation algebraically for Y(s).
3. Take the inverse Laplace transform of Y(s) to determine the solution
y(t) of the given initial-value problem.

10.4
The Transform of Derivatives and Solution of Initial-Value Problems 687
These steps are illustrated in Figure 10.4.1.
Solve algebraically
for Y(s)
Initial value
problem
Algebraic equation
for Y(s)
Y(s) = F(s)
Take inverse
Laplace transform
Solution
y(t) = L21[F(s)]
Take Laplace
transform
Figure 10.4.1: A schematic representation of the Laplace transform method for solving
initial-value problems.
To extend the technique introduced in the previous example to higher-order differen-
tial equations, we need to determine how the higher-order derivatives transform. This can
be derived quite easily from Theorem 10.4.1. We illustrate for the case of second-order
derivatives and leave the derivation of the general case as an exercise.
Assuming that f ′′ is sufﬁciently smooth, it follows from Theorem 10.4.1 that
L[ f ′′] = sL[ f ′] −f ′(0).
Thus, applying Theorem 10.4.1 once more yields
L[ f ′′] = s2L[ f ] −s f (0) −f ′(0).
We leave it as an exercise to establish that more generally, we have
L[ f (n)] = snL[ f ] −sn−1 f (0) −sn−2 f ′(0) −· · · −s f (n−2)(0) −f (n−1)(0).
Example 10.4.3
Use the Laplace transform to solve the initial-value problem
y′′ −y′ −6y = 0,
y(0) = 1,
y′(0) = 2.
Solution:
We take the Laplace transform of both sides of the differential equation
to obtain
[s2Y(s) −sy(0) −y′(0)] −[sY(s) −y(0)] −6Y(s) = 0.
Substituting in the given initial values and rearranging terms yields
(s2 −s −6)Y(s) = s + 1.
That is,
Y(s) =
s + 1
(s −3)(s + 2).
Thus, we have solved for the Laplace transform of y(t). To ﬁnd y itself, we must take the
inverse Laplace transform. We ﬁrst decompose the right-hand side into partial fractions
to obtain
Y(s) =
4
5(s −3) +
1
5(s + 2).

688
CHAPTER 10
The Laplace Transform and Some Elementary Applications
We recognize the terms on the right-hand side as being the Laplace transform of appro-
priate exponential functions. Taking the inverse Laplace transform yields
y(t) = 4
5e3t + 1
5e−2t,
and the initial-value problem is solved.
□
Example 10.4.4
Solve the initial-value problem
y′′ + y = e2t,
y(0) = 0,
y′(0) = 1.
Solution:
Once more we take the Laplace transform of both sides of the differential
equation to obtain
[s2Y(s) −sy(0) −y′(0)] + Y(s) =
1
s −2.
That is, upon substituting for the given initial conditions and simplifying,
Y(s) =
s −1
(s −2)(s2 + 1).
We must now determine the partial fractions decomposition of the right-hand side.
We have
s −1
(s −2)(s2 + 1) =
A
s −2 + Bs + C
s2 + 1 ,
for appropriate constants A, B, and C. Multiplying both sides of this equality by
(s −2)(s2 + 1) yields
s −1 = A(s2 + 1) + (Bs + C)(s −2).
Equating coefﬁcients of s0, s1, and s2 results in the three conditions
A −2C = −1,
−2B + C = 1,
A + B = 0.
Solving for A, B, and C, we obtain
A = 1
5,
B = −1
5,
C = 3
5.
Thus,
Y(s) =
1
5(s −2) −
s −3
5(s2 + 1).
That is,
Y(s) =
1
5(s −2) −
s
5(s2 + 1) +
3
5(s2 + 1).
Taking the inverse Laplace transform of both sides of this equation yields
y(t) = 1
5e2t −1
5 cos t + 3
5 sin t.
□
The structure of the solution obtained in the previous example has a familiar form.
The ﬁrst term represents a particular solution to the differential equation that could have
been obtained by the method of undetermined coefﬁcients, whereas the last two terms
come from the complementary function. There are no arbitrary constants in the solution,

10.4
The Transform of Derivatives and Solution of Initial-Value Problems 689
since we have solved an initial-value problem. Notice the difference between solving
an initial-value problem using the Laplace transform and our previous techniques. In
the Laplace transform technique, we impose the initial values at the beginning of the
problem and just solve the initial-value problem. In our previous techniques, we ﬁrst
found the general solution to the differential equation and then imposed the initial values
to solve the initial-value problem. We note, however, that the Laplace transform can also
be used to determine the general solution of a differential equation (see Problem 28).
It should be apparent from the results of the previous two sections that the main
difﬁculty in applying the Laplace transform technique to the solution of initial-value
problems is in steps 1 and 3. In order for the technique to be useful, we need to know
the transform and the inverse transform for a large number of functions. So far, we have
only determined the Laplace transform of some very basic functions, namely, tn, eat,
sin bt, and cos bt. We will show in the remaining sections how these basic transforms
can be used to determine the Laplace transform of almost any function that is likely
to arise in the applications. The reader is once more strongly advised to memorize the
basic transforms.
Exercises for 10.4
Skills
• For functions f of exponential order on [0, ∞) whose
derivatives f ′ exist and are piecewise continuous on
[0, ∞), be able to compute L[ f ′].
• Be able to use the Laplace transform to solve initial-
value problems.
• Where applicable, be able to compute the Laplace
transform of the nth derivative of f by repeated appli-
cation of Theorem 10.4.1.
True-False Review
For Questions (a)–(d), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) For every function f with a continuous derivative on
[0, ∞), the Laplace transform of the derivative is given
by L[ f ′] = sL[ f ] −f (0).
(b) In solving an initial-value problem using the Laplace
transform method, the general solution of the differen-
tial equation is not explicitly found. Rather, we impose
the initial conditions immediately in the procedure.
(c) The Laplace transform method for solving an initial-
value problem can also be used to ﬁnd the general
solution of the differential equation.
(d) The initial conditions of an initial-value problem do
not affect the expression Y(s) for the Laplace trans-
form of the solution y(t).
Problems
For Problems 1–27, use the Laplace transform to solve the
given initial-value problem.
1. y′ −2y = 6e5t,
y(0) = 3.
2. y′ + y = 8e3t,
y(0) = 2.
3. y′ + 3y = 2e−t,
y(0) = 3.
4. y′ + 2y = 4t,
y(0) = 1.
5. y′ −y = 6 cos t,
y(0) = 2.
6. y′ −y = 5 sin 2t,
y(0) = −1.
7. y′ + y = 5et sin t,
y(0) = 1.
8. y′′ + y′ −2y = 0,
y(0) = 1,
y′(0) = 4.
9. y′′ + 4y = 0,
y(0) = 5,
y′(0) = 1.
10. y′′ −3y′ + 2y = 4,
y(0) = 0,
y′(0) = 1.
11. y′′ −y′ −12y = 36,
y(0) = 0,
y′(0) = 12.
12. y′′ + y′ −2y = 10e−t,
y(0) = 0,
y′(0) = 1.
13. y′′ −3y′ + 2y = 4e3t,
y(0) = 0,
y′(0) = 0.
14. y′′ −2y′ = 30e−3t,
y(0) = 1,
y′(0) = 0.
15. y′′ −y = 12e2t,
y(0) = 1,
y′(0) = 1.

690
CHAPTER 10
The Laplace Transform and Some Elementary Applications
16. y′′ + 4y = 10e−t,
y(0) = 4,
y′(0) = 0.
17. y′′ −y′ −6y = 6(2 −et),
y(0) = 5,
y′(0) = −3.
18. y′′ −y = 6 cos t,
y(0) = 0,
y′(0) = 4.
19. y′′ −9y = 13 sin 2t,
y(0) = 3,
y′(0) = 1.
20. y′′ −y = 8 sin t −6 cos t,
y(0) = 2,
y′(0) = −1.
21. y′′ −y′ −2y = 10 cos t,
y(0) = 0,
y′(0) = −1.
22. y′′ + 5y′ + 4y = 20 sin 2t,
y(0) = −1,
y′(0) = 2.
23. y′′ + 5y′ + 4y = 20 sin 2t,
y(0) = 1,
y′(0) = −2.
24. y′′−3y′+2y = 3 cos t+sin t,
y(0) = 1,
y′(0) = 1.
25. y′′ + 4y = 9 sin t,
y(0) = 1,
y′(0) = −1.
26. y′′ + y = 6 cos 2t,
y(0) = 0,
y′(0) = 2.
27. y′′+9y = 7 sin 4t+14 cos 4t,
y(0) = 1,
y′(0) = 2.
28. Use the Laplace transform to ﬁnd the general solution
to y′′ −y = 0.
29. Use the Laplace transform to solve the initial-value
problem
y′′ + ω2y = A sin ω0t + B cos ω0t
y(0) = y0,
y′(0) = y1,
where A, B, ω, and ω0 are positive constants and
ω ̸= ω0.
30. The current i(t) in an RL circuit is governed by the
differential equation
di
dt + R
L i = 1
L E(t),
where R and L are constants.
(a) Use the Laplace transform to determine i(t) if
E(t) = E0, a constant. There is no current ﬂow-
ing initially.
(b) Repeat part (a) in the case when E(t) = E0
sin ωt, where ω is a constant.
The Laplace transform can also be used to solve initial-value
problems for systems of linear differential equations. The re-
maining problems deal with this.
31. Consider the initial-value problem
x′
1 = a11x1 + a12x2 + b1(t),
x′
2 = a21x1 + a22x2 + b2(t),
x1(0) = α1,
x2(0) = α2,
where the ai j, α1, and α2 are constants. Show that the
Laplace transforms of x1(t) and x2(t) must satisfy the
linear system
(s −a11)X1(s) −
a12X2(s) = α1 + B1(s)
−a12X1(s) + (s −a22)X2(s) = α2 + B2(s).
This system can be solved quite easily (for example,
by Cramer’s Rule) to determine X1(s) and X2(s), and
then x1(t) and x2(t) can be obtained by taking the
inverse Laplace transform.
For Problems 32–33, solve the given initial-value problem.
32. x′
1 = −4x1 −2x2,
x′
2 = x1 −x2,
x1(0) = 0, x2(0) = 1.
33. x′
1 = −3x1 + 4x2,
x′
2 = −x1 + 2x2,
x1(0) = 2, x2(0) = 1.
34. Establish the formula for L[ f (n)], the Laplace trans-
form of the nth derivative of f , given in the text.
[Hint: Use induction on n.]
10.5
The First Shifting Theorem
In order for the Laplace transform to be a useful tool for solving differential equations,
we need to be able to ﬁnd L[ f ] for a large class of functions f . Trying to apply the
deﬁnition of the Laplace transform to determine L[ f ] for every function we encounter
is not an appropriate way to proceed. Instead, we derive some general theorems that
will enable us to obtain the Laplace transform of most elementary functions from a
knowledge of the transforms of the functions given in Table 10.2.1. For the remainder
of this section, we will be assuming that all of the functions that we encounter do have
a Laplace transform.

10.5
The First Shifting Theorem 691
Theorem 10.5.1
(First Shifting Theorem)
If L[ f ] = F(s), then
L[eat f (t)] = F(s −a).
Conversely, if L−1[F(s)] = f (t), then
L−1[F(s −a)] = eat f (t).
Proof From the deﬁnition of the Laplace transform, we have
L[eat f (t)] =
! ∞
0
e−steat f (t)dt =
! ∞
0
e−(s−a)t f (t)dt.
(10.5.1)
But,
F(s) =
! ∞
0
e−st f (t)dt,
so that
F(s −a) =
! ∞
0
e−(s−a)t f (t)dt.
(10.5.2)
Comparing (10.5.1) and (10.5.2), we obtain
L[eat f (t)] = F(s −a),
(10.5.3)
as required. Taking the inverse Laplace transform of both sides of (10.5.3) yields
L−1[F(s −a)] = eat f (t).
We illustrate the use of the preceding theorem with several examples. (See also
Figure 10.5.1.)
F(s)
F(s — a)
F(s — a)
a
s
s
Figure 10.5.1: An illustration of the ﬁrst shifting theorem. Multiplying f (t) by eat has the
effect of shifting F(s) by a units in s-space.
Example 10.5.2
Find L[ f ] for each f (t).
(a) f (t) = e5t cos 4t.
(b) f (t) = eat sin bt, where a, b are constants.
(c) f (t) = eattn, where a is a constant and n is a positive integer.

692
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Solution:
(a) From Table 10.2.1, we have
L[cos 4t] =
s
s2 + 16,
so that applying the ﬁrst shifting theorem with a = 5 yields
L[e5t cos 4t] =
s −5
(s −5)2 + 16.
(b) Since L[sin bt] =
b
s2 + b2 ,
L[eat sin bt] =
b
(s −a)2 + b2 .
Similarly, it follows from Table 10.2.1 and the ﬁrst shifting theorem that
L[eat cos bt] =
s −a
(s −a)2 + b2 .
(c) From Table 10.2.1, we have
L[tn] =
n!
sn+1 ,
so that
L[eattn] =
n!
(s −a)n+1 .
□
The previous example dealt with the direct use of the ﬁrst shifting theorem to obtain
the Laplace transform of a function. Of equal importance is its use in determining inverse
transforms. Once more, we illustrate with several examples.
Example 10.5.3
Determine L−1[F(s)] for the given F.
(a) F(s) =
3
(s −2)2 + 9.
(b) F(s) =
6
(s −4)3 .
(c) F(s) =
s + 4
s2 + 6s + 13.
(d) F(s) =
s −2
s2 + 2s + 3.
Solution:
(a) From Table 10.2.1,
L−1
"
3
s2 + 9
#
= sin 3t,
so that, by the ﬁrst shifting theorem,
L−1
"
3
(s −2)2 + 9
#
= e2t sin 3t.

10.5
The First Shifting Theorem 693
(b) From Table 10.2.1,
L−1
" 6
s3
#
= 3t2.
Thus, applying the ﬁrst shifting theorem yields
L−1
"
6
(s −4)3
#
= 3t2e4t.
(c) In this case,
F(s) =
s + 4
s2 + 6s + 13,
which we do not recognize as being a shift of the transform of any of the functions
given in Table 10.2.1. However, completing the square in the denominator of F(s)
yields4
F(s) =
s + 4
(s + 3)2 + 4.
(10.5.4)
We still cannot write down the inverse transform directly, but, by the ﬁrst shifting
theorem, we have
L−1
"
s + 3
(s + 3)2 + 4
#
= e−3t cos 2t,
(10.5.5)
L−1
"
2
(s + 3)2 + 4
#
= e−3t sin 2t.
(10.5.6)
This suggests that we rewrite (10.5.4) in the equivalent form
F(s) =
s + 3
(s + 3)2 + 4 +
1
(s + 3)2 + 4,
so that, using the linearity of L−1 and Equations (10.5.5) and (10.5.6),
L−1[F(s)] = L−1
"
s + 3
(s + 3)2 + 4
#
+ L−1
"
1
(s + 3)2 + 4
#
= e−3t cos 2t + 1
2e−3t sin 2t.
(d) We proceed as in the previous example. In this case, we have
F(s) =
s −2
(s + 1)2 + 2,
which can be written as
F(s) =
s + 1
(s + 1)2 + 2 −
3
(s + 1)2 + 2.
Then, using Table 10.2.1 and the ﬁrst shifting theorem, it follows that
L−1[F(s)] = e−t cos
√
2t −3
√
2
e−t sin
√
2t.
□
4Recall that we can always write x2 + ax + b = (x + a/2)2 + b −a2/4. This procedure is known as
completing the square.

694
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Exercises for 10.5
Key Terms
First Shifting Theorem.
Skills
• Be able to use the First Shifting Theorem to compute
the Laplace transform and inverse Laplace transform
of the applicable “shifted” functions in this section.
True-False Review
For Questions (a)–(h), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If L[ f ] = F(s), then we have L[e−at f (t)] =
F(s + a).
(b) For every function f , we have f (t −1) = f (t) −1
for every t.
(c) If f (t + 2) =
et
√t + 3
, then f (t) =
et−2
√t + 1
.
(d) If f and g areintegrablefunctionssuchthat f (x−3) =
g(x), then
! 1
0
f (t)dt =
! 1
0
g(t)dt −3.
(e) We have L[e−t sin 2t] =
2
(s −1)2 + 4.
(f) We have L[e2tt3] =
6
(s −2)4 .
(g) We have L−1
"
s + 4
(s + 4)2 + 9
#
= e−4t cos 3t.
(h) We have L−1
"
3
(s + 1)2 + 36
#
= 2e−t sin 6t.
Problems
For Problems 1–11, determine f (t−a) for the given function
f and the given constant a.
1. f (t) = 2t, a = 1.
2. f (t) = e−2t, a = −1.
3. f (t) = 1, a = 3.
4. f (t) = t2 −2t, a = −2.
5. f (t) = e3t, a = 2.
6. f (t) = e2t cos t, a = π.
7. f (t) = te2t, a = −1.
8. f (t) = e−t sin 2t, a = π/6.
9. f (t) =
t
t2 + 4, a = 1.
10. f (t) =
t + 1
t2 −2t + 2, a = 2.
11. f (t) = e−t(sin 2t + cos 2t), a = π/4.
For Problems 12–17, determine f (t).
12. f (t −1) = (t −1)2.
13. f (t −1) = (t −2)2.
14. f (t −2) = (t −2)e3(t−2).
15. f (t −1) = t sin[3(t −1)].
16. f (t −3) = te−(t−3).
17. f (t −4) =
t + 1
(t −1)2 + 4.
For Problems 18–27, determine the Laplace transform of f .
18. f (t) = e3t cos 4t.
19. f (t) = e−4t sin 5t.
20. f (t) = te2t.
21. f (t) = 3te−t.
22. f (t) = t3e−4t.
23. f (t) = et −te−2t.
24. f (t) = 2e3t sin t + 4e−t cos 3t.
25. f (t) = e2t(1 −sin2 t).
26. f (t) = t2(et −3).
27. f (t) = e−2t sin(t −π/4).

10.6
The Unit Step Function 695
For Problems 28–42, determine L−1[F].
28. F(s) =
1
(s −3)2 .
29. F(s) =
4
(s + 2)3 .
30. F(s) =
2
√s + 3.
31. F(s) =
2
(s −1)2 + 4.
32. F(s) =
s + 2
(s + 2)2 + 9.
33. F(s) =
s
(s −3)2 + 4.
34. F(s) =
5
(s −2)2 + 16.
35. F(s) =
6
s2 + 2s + 2.
36. F(s) =
s −2
s2 + 2s + 26.
37. F(s) =
2s
s2 −4s + 13.
38. F(s) =
s
(s + 1)2 + 4.
39. F(s) =
2s + 3
(s + 5)2 + 49.
40. F(s) =
4
s(s + 2)2 .
41. F(s) =
2s + 1
(s −1)2(s + 2).
42. F(s) =
2s + 3
s(s2 −2s + 5).
For Problems 43–53, solve the given initial-value problem.
43. y′′ −y = 8et,
y(0) = 0,
y′(0) = 0.
44. y′′ −4y = 12e2t,
y(0) = 2,
y′(0) = 3.
45. y′′ −y′ −2y = 6e−t,
y(0) = 0,
y′(0) = 1.
46. y′′ + y′ −2y = 3e−2t,
y(0) = 3,
y′(0) = −1.
47. y′′ −4y′ + 4y = 6e2t,
y(0) = 1,
y′(0) = 0.
48. y′′ + 2y′ + y = 2e−t,
y(0) = 2,
y′(0) = 1.
49. y′′ −4y = 2tet,
y(0) = 0,
y′(0) = 0.
50. y′′ + 3y′ + 2y = 12te2t,
y(0) = 0,
y′(0) = 1.
51. y′′ + y = 5te−3t,
y(0) = 2,
y′(0) = 0.
52. y′′ −y = 8et sin 2t,
y(0) = 2,
y′(0) = −2.
53. y′′ +2y′ −3y = 26e2t cos t,
y(0) = 1,
y′(0) = 0.
54. Solve the initial-value problem
x′
1 = 2x1 −x2,
x′
2 = x1 + 2x2,
x1(0) = 1,
x2(0) = 0.
55. Solve the initial-value problem
x′
1 = 3x1 + 2x2,
x′
2 = −x1 + 4x2,
x1(0) = −1,
x2(0) = 1.
10.6
The Unit Step Function
In applications of differential equations such as
y′′ + by′ + cy = F(t)
to engineering problems, it often arises that the forcing term F(t) is either piece-
wise continuous or even discontinuous. In such a situation, the Laplace transform is
ideally suited for determining the solution of the differential equation as compared
with the techniques developed in Chapter 8. To specify piecewise continuous func-
tions in an appropriate manner, it is useful to introduce the unit step function, deﬁned
as follows:

696
CHAPTER 10
The Laplace Transform and Some Elementary Applications
DEFINITION
10.6.1
The unit step function or Heaviside step function, ua(t), is deﬁned by
ua(t) =
) 0, 0 ≤t < a,
1,
t ≥a,
where a is any positive number. (See Figure 10.6.1.)
ua(t)
t
1
a
Figure 10.6.1: The unit step function f (t) = ua(t).
Example 10.6.2
Sketch the function f (t) = ua(t) −ub(t), where b > a.
Solution:
By deﬁnition of the unit step function, we have
f (t) =
⎧
⎨
⎩
0, 0 ≤t < a,
1, a ≤t < b,
0,
t ≥b,
so that the graph of f is given as in Figure 10.6.2.
f(t)
1
a
t
b
Figure 10.6.2: A sketch of the function given in Example 10.6.2.
□
The real power of the unit step function is that it enables us to model the situation
when a force acts intermittently or in a nonsmooth manner. For example, the function f
in Figure 10.6.2 can be interpreted as representing a force of unit magnitude that begins
to act at t = a and that stops acting at t = b. More generally, it is useful to regard the
unit step function ua(t) as giving a mathematical description of a switch that is turned
on at t = a.
The remaining examples in this section indicate how ua(t) can be useful for repre-
senting functions that are piecewise continuous.
Example 10.6.3
Express the following function in terms of the unit step function:
f (t) =
⎧
⎨
⎩
0,
0 ≤t < 1,
t −1, 1 ≤t < 2,
1,
t ≥2.

10.6
The Unit Step Function 697
Solution:
We view the given function in the following way. The contribution f1(t) =
t −1 is “switched on” at t = 1 and is “switched off” again at t = 2. Mathematically this
can be described by
f1(t) = u1(t)(t −1)
0
12
3
switch on at t = 1
−u2(t)(t −1)
0
12
3
switch off at t = 2
.
At t = 2, the contribution f2(t) = 1 switches on and remains on for all t ≥2.
Mathematically this is described by
f2(t) = u2(t).
The function f is then given by
f (t) = f1(t) + f2(t) = (t −1)u1(t) −(t −1)u2(t) + u2(t),
which can be written in the equivalent form
f (t) = (t −1)u1(t) −(t −2)u2(t).
(10.6.1)
A sketch of f (t) is given in Figure 10.6.3. Notice that this sketch is more easily deter-
mined from the original deﬁnition of f , rather than from (10.6.1).
f(t)
1
1
t
2
Figure 10.6.3: A sketch of the function given in Example 10.6.3.
□
Example 10.6.4
Make a sketch of the function f (t) deﬁned by
f (t) =
⎧
⎪⎪⎨
⎪⎪⎩
t,
0 ≤t < 2,
−1,
2 ≤t < 4,
t −4,
4 ≤t < 5,
e(5−t),
t ≥5,
and express f in terms of the unit step function.
Solution:
The function is sketched in Figure 10.6.4. Using the unit step function, we
see that f consists of the following different parts:
f1(t) = t[1 −u2(t)],
f2(t) = −[u2(t) −u4(t)],
f3(t) = (t −4)[u4(t) −u5(t)],
f4(t) = e(5−t)u5(t).
Thus,
f (t) = t[1 −u2(t)] −[u2(t) −u4(t)] + (t −4)[u4(t) −u5(t)] + e(5−t)u5(t).
□

698
CHAPTER 10
The Laplace Transform and Some Elementary Applications
f(t)
2
2
4
1
—1
1
t
3
5
Figure 10.6.4: A sketch of the function deﬁned in Example 10.6.4.
Exercises for 10.6
Key Terms
Unit (Heaviside) step function.
Skills
• Be able to sketch functions that involve the unit step
function.
• Be able to express appropriate functions in terms of
unit step functions.
True-False Review
For Questions (a)–(d), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The unit step function is deﬁned by
) 0, 0 ≤t ≤a,
1,
t ≥a,
where a is any positive number.
(b) If a < b, the function f (t) = ua(t) −ub(t) has value
1 on the interval [a, b] and value 0 elsewhere.
(c) If a and b are positive integers with a < b, then
ua(t) ≤ub(t) for all t ≥0.
(d) The function
⎧
⎨
⎩
0, 0 ≤t < a,
1, a ≤t < b,
0,
t ≥b
can be expressed as f (t) = ub(t) −ua(t).
Problems
For Problems 1–7, make a sketch of the given function on
the interval [0, ∞).
1. f (t) = 3(u2(t) −u4(t)).
2. f (t) = 2u1(t) −4u3(t).
3. f (t) = 1 + (t −1)u1(t).
4. f (t) = t(1 −u1(t)).
5. f (t) = u1(t) + u2(t) + u3(t) + u4(t).
6. f (t) = u1(t) + u2(t) + · · · =
∞
4
i=1
ui(t).
7. f (t) = u1(t) −u2(t) + u3(t) −· · ·
=
∞
4
i=1
(−1)i+1ui(t).
For Problems 8–15, make a sketch of the given function on
[0, ∞) and express it in terms of the unit step function.
8. f (t) =
)
3, 0 ≤t < 1,
−1,
t ≥1.
9. f (t) =
)
t2, 0 ≤t < 1,
1,
t ≥1.

10.7
The Second Shifting Theorem 699
10. f (t) =
⎧
⎨
⎩
2, 0 ≤t < 2,
1, 2 ≤t < 4,
−1,
t ≥4.
11. f (t) =
)
2,
0 ≤t < 1,
2e(t−1),
t > 1.
12. f (t) =
⎧
⎨
⎩
t,
0 ≤t < 3,
6 −t, 3 ≤t < 6,
0,
t ≥6.
13. f (t) =
⎧
⎨
⎩
0,
0 ≤t < 2,
3 −t, 2 ≤t < 4,
−1,
t ≥4.
14. f (t) =
⎧
⎨
⎩
1,
0 ≤t < π/2,
sin t, π/2 ≤t < 3π/2,
−1,
t ≥3π/2.
15. f (t) =
⎧
⎨
⎩
sin t, 2nπ ≤t < (2n + 1)π,
(n = 0, 1, 2, 3, . . . ),
0,
otherwise.
10.7
The Second Shifting Theorem
In the previous section, we saw how the unit step function can be used to represent func-
tions that are piecewise continuous. In this section, we show that the Laplace transform
provides a straightforward method for solving constant coefﬁcient linear differential
equations that have such functions as driving terms. We ﬁrst need to determine how the
unit step function transforms.
Theorem 10.7.1
(Second Shifting Theorem)
Let L[ f (t)] = F(s). Then
L[ua(t) f (t −a)] = e−as F(s).
(10.7.1)
Conversely,
L−1[e−as F(s)] = ua(t) f (t −a).
(10.7.2)
Proof Once more we must return to the deﬁnition of the Laplace transform. We have
L[ua(t) f (t −a)] =
! ∞
0
e−stua(t) f (t −a)dt =
! ∞
a
e−st f (t −a)dt,
where we have used the deﬁnition of the unit step function. We now make a change of
variable in the integral. Let x = t −a. Then dx = dt, and the lower limit of integration
t = a corresponds to x = 0, whereas the upper limit of integration is unchanged. Thus,
L[ua(t) f (t −a)] =
! ∞
0
e−s(x+a) f (x)dx = e−as
! ∞
0
e−sx f (x)dx = e−as L[ f ],
as required. Taking the inverse Laplace transform of both sides of (10.7.1) yields (10.7.2).
This theorem is illustrated in Figure 10.7.1.
Corollary 10.7.2
If L[ f (t)] = F(s), then
L[ua(t) f (t)] = e−as L[ f (t + a)].
Proof This is a direct consequence of the previous theorem.

700
CHAPTER 10
The Laplace Transform and Some Elementary Applications
L—1[F(s)]
f(t)
t
L—1[e—asF(s)]
ua(t)f(t — a)
t
a
Figure 10.7.1: An illustration of the second shifting theorem. Multiplying F(s) by e−as has
the effect of shifting f (t) by a units to the right in t-space.
We illustrate the use of the preceding theorem and corollary with several examples.
Example 10.7.3
Determine L[ f ] if
f (t) =
⎧
⎨
⎩
0,
0 ≤t < 1,
t −1,
1 ≤t < 2
1,
t ≥2.
Solution:
We have already shown in Example 10.6.3 that the given function can be
expressed in terms of the unit step function as
f (t) = (t −1)u1(t) −(t −2)u2(t).
If we let g(t) = t, then
f (t) = g(t −1)u1(t) −g(t −2)u2(t).
Using Theorem 10.7.1, it follows that
L[ f ] = e−s L[g] −e−2s L[g] = 1
s2 (e−s −e−2s).
□
Example 10.7.4
Find L[ f ] if
f (t) =
)
1,
0 ≤t < 2,
e−(t−2),
t ≥2.
Solution:
To determine L[ f ], we ﬁrst express f in terms of the unit step function.
In this case, we have
f (t) = [1 −u2(t)] + e−(t−2)u2(t).
That is,
f (t) = 1 + u2(t)[e−(t−2) −1].
If we let g(t) = e−t −1, then
f (t) = 1 + u2(t)g(t −2),
so that, from Theorem 10.7.1,
L[ f ] = 1 −e−2s
s
+ e−2s
s + 1.
□

10.7
The Second Shifting Theorem 701
Example 10.7.5
Determine L−1
" 2e−s
s2 + 4
#
.
Solution:
From Table 10.2.1, we have
L[sin 2t] =
2
s2 + 4.
Consequently,
L−1
" 2e−s
s2 + 4
#
= L−1{e−s L[sin 2t]}
= u1(t) sin[2(t −1)],
using Theorem 10.7.1.
□
Example 10.7.6
Determine L−1
"(s −4)e−3s
s2 −4s + 5
#
.
Solution:
Let
G(s) = (s −4)e−3s
s2 −4s + 5.
We ﬁrst rewrite G in a form more suitable for determining L−1[G]. Completing the
square in the denominator yields
G(s) = (s −4)e−3s
(s −2)2 + 1,
which can be written in the equivalent form
G(s) = e−3s
"
s −2
(s −2)2n + 1 −
2
(s −2)2 + 1
#
.
Thus,
L−1[G] = L−1 5
e−3s L[e2t cos t −2e2t sin t]
6
= u3(t)
7
e2(t−3) cos(t −3) −2e2(t−3) sin(t −3)
8
= e2(t−3)u3(t)[cos(t −3) −2 sin(t −3)].
□
We now illustrate how the unit step function can be useful in the solution of initial-
value problems. For simplicity, we will start with a ﬁrst-order differential equation.
Example 10.7.7
Solve the initial-value problem
y′ −y = 1 −(t −1)u1(t),
y(0) = 0.
Solution:
In this case, the forcing term on the right-hand side of the differential
equation is sketched in Figure 10.7.2. Taking the Laplace transform of both sides of the
differential equation yields
sY(s) −Y(s) −y(0) = 1
s −e−s
s2 .

702
CHAPTER 10
The Laplace Transform and Some Elementary Applications
f(t)
t
1
1
2
Figure 10.7.2: The forcing
function in Example 10.7.7.
By imposing the given initial conditions and simplifying, we obtain
Y(s) =
1
s(s −1) −e−s
"
1
s2(s −1)
#
.
Decomposing the terms on the right-hand side into partial fractions yields
Y(s) =
1
s −1 −1
s −e−s
.
1
s −1 −1
s −1
s2
/
.
Taking the inverse Laplace transform of both sides of this equation, we obtain
y(t) = et −1 −u1(t)[e(t−1) −1 −(t −1)].
That is,
y(t) = et −1 −u1(t)[e(t−1) −t].
This problem was solved in Chapter 1 (Example 1.6.5). A comparison of the two solution
methods indicates the power of the Laplace transform.
□
Example 10.7.8
Solve the initial-value problem
y′′ + 2y′ + 5y = f (t),
y(0) = 0,
y′(0) = 0
if
f (t) =
⎧
⎨
⎩
10, 0 ≤t < 4,
−10, 4 ≤t < 8,
0,
t ≥8.
Solution:
The forcing function f (t) is sketched in Figure 10.7.3. We ﬁrst express f
in terms of the unit step function. In this case, we have
f (t) = 10[1 −2u4(t) + u8(t)],
so that the differential equation can be written as
y′′ + 2y′ + 5y = 10[1 −2u4(t) + u8(t)],
and we can now proceed in the usual manner. Taking the Laplace transform of both sides
of the differential equation yields
[s2Y(s) −sy(0) −y′(0)] + 2[sY(s) −y(0)] + 5Y(s) = 10
s (1 −2e−4s + e−8s).
That is, by imposing the given initial conditions and simplifying,
Y(s) = 10(1 −2e−4s + e−8s)
s(s2 + 2s + 5)
.
The right-hand side of this equation has the following partial fractions decomposition:
1
s(s2 + 2s + 5) = 1
5s −
s + 2
5(s2 + 2s + 5),
f(t)
t
10
—10
4
8
Figure 10.7.3: The forcing
function in Example 10.7.8.

10.7
The Second Shifting Theorem 703
so that
Y(s) = 2(1 −2e−4s + e−8s)
.1
s −
s + 2
s2 + 2s + 5
/
.
(10.7.3)
Now,
L−1
"
s + 2
s2 + 2s + 5
#
= L−1
"
s + 1
(s + 1)2 + 4 +
1
(s + 1)2 + 4
#
= e−t cos 2t + 1
2e−t sin 2t.
Taking the inverse Laplace transform of both sides of (10.7.3) and using Theorem 10.7.1
yields
y(t) = 2
)
1 −e−t cos 2t −1
2e−t sin 2t
−2u4(t)
"
1 −e−(t−4) cos 2(t −4) −1
2e−(t−4) sin 2(t −4)
#
+ u8(t)
"
1 −e−(t−8) cos 2(t −8) −1
2e−(t−8) sin 2(t −8)
#+
.
We can express this solution in the simpler form
y(t) = g(t) −2u4(t)g(t −4) + u8(t)g(t −8),
where
g(t) = 2
.
1 −e−t cos 2t −1
2e−t sin 2t
/
.
The given initial-value problem can be interpreted as governing the motion of a damped
spring-mass system. Due to the form of the initial conditions, if the forcing function were
the same constant, F0, for all time, the oscillations would quickly be damped out, and
y(t) would approach F0/5. In this problem, the forcing term is constant over different
time intervals. Consequently, the mass ﬁrst performs damped oscillations approaching
y = 2. Then the second part of the driving term comes into effect and the subsequent
oscillations are about y = −2. After t = 8, the forcing function vanishes and the physical
system performs damped oscillations about the equilibrium. These features can be seen
in Figure 10.7.4.
y(t)
2
1
—1
—2
2
6
10
12
14
t
4
8
Figure 10.7.4: The response of a damped spring-mass system to the driving term given in
Example 10.7.8.
□

704
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Exercises for 10.7
Key Terms
Second Shifting Theorem.
Skills
• Be able to apply the Second Shifting Theorem to com-
puteLaplacetransformsoffunctionsinvolvingtheunit
step function.
• Be able to use the Second Shifting Theorem to com-
pute inverse Laplace transforms that result in unit step
functions.
• Be able to solve initial-value problems that involve
unit step functions.
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If L[ f (t)] = F(s), then the inverse Laplace transform
of the function eas F(s) is ua(t) f (t −a).
(b) If L[ f (t)] = F(s), then the Laplace transform of
ua(t) f (t) is e−as L[ f (t + a)].
(c) If L[ f (t)] = F(s), then multiplying F(s) by e−as
has the effect of shifting f (t) by a units to the right in
t-space.
(d) We have L[u2(t) cos 4t] =
se−2s
s2 + 16.
(e) We have L[u3(t)et] =
1
e3s(s −1).
(f) We have L−1
"
es
s2 + 9
#
= 1
3u1(t) sin[3(t −1)].
(g) We have L−1
" 1
se2s
#
= u2(t)(t −2).
Problems
For Problems 1–11, determine the Laplace transform of the
given function f .
1. f (t) = u2(t) −u3(t).
2. f (t) = (t −1)u1(t).
3. f (t) = e3(t−2)u2(t).
4. f (t) = sin(t −π/4)uπ/4(t).
5. f (t) = cos t uπ(t).
6. f (t) = (t −2)2u2(t).
7. f (t) = tu3(t).
8. f (t) = (t −1)2u2(t).
9. f (t) = e(t−4)(t −4)3u4(t).
10. f (t) = e−2(t−1) sin 3(t −1)u1(t).
11. f (t) = ea(t−c) cos b(t −c)uc(t), where a, b, and c are
positive constants.
For Problems 12–26, determine the inverse Laplace trans-
form of F.
12. F(s) = e−2s
s2 .
13. F(s) = e−s
s + 1.
14. F(s) = e−3s
s + 4.
15. F(s) = se−s
s2 + 4.
16. F(s) = e−3s
s2 + 1.
17. F(s) = e−2s
s + 2.
18. F(s) =
e−s
(s + 1)(s −4).
19. F(s) =
e−2s
s2 + 2s + 2.
20. F(s) = e−s(s + 6)
s2 + 9
.
21. F(s) =
e−5s
s2 + 16.

10.7
The Second Shifting Theorem 705
22. F(s) =
e−2s
(s −3)3 .
23. F(s) = e−4s(s + 3)
s2 −6s + 13.
24. F(s) = e−s(2s −1)
s2 + 4s + 5 .
25. F(s) =
2e−2s
(s −1)(s2 + 1).
26. F(s) =
50e−3s
(s + 1)2(s2 + 4).
For Problems 27–41, solve the given initial-value problem.
27. y′ + 2y = 2u1(t),
y(0) = 1.
28. y′ −2y = u2(t)et−2,
y(0) = 2.
29. y′ −y = 4uπ/4(t) cos(t −π/4),
y(0) = 1.
30. y′ + 2y = uπ(t) sin 2t,
y(0) = 3.
31. y′ + 3y = f (t),
y(0) = 1, where
f (t) =
) 1, 0 ≤t < 1,
0,
t ≥1.
32. y′ −3y = f (t),
y(0) = 2, where
f (t) =
) sin t, 0 ≤t < π/2,
1,
t ≥π/2.
33. y′ −3y = 10e−(t−a) sin[2(t −a)]ua(t),
y(0) = 5,
where a is a positive constant.
34. y′′ −y = u1(t),
y(0) = 2,
y′(0) = 0.
35. y′′−y′−2y = 1−3u2(t),
y(0) = 1,
y′(0) = −2.
36. y′′ −4y = u1(t) −u2(t),
y(0) = 0,
y′(0) = 4.
37. y′′ + y = t −u1(t)(t −1),
y(0) = 2,
y′(0) = 1.
38. y′′ + 3y′ + 2y = 10uπ/4(t) sin(t −π/4),
y(0) = 1,
y′(0) = 0.
39. y′′ + y′ −6y = 30u1(t)e−(t−1),
y(0) = 3,
y′(0) = −4.
40. y′′ + 4y′ + 5y = 5u3(t),
y(0) = 2,
y′(0) = 1.
41. y′′ −2y′ + 5y = 2 sin t + uπ/2(t)[1 −sin(t −π/2)],
y(0) = 0,
y′(0) = 0.
For Problems 42–45, solve the given initial-value problem.
42. y′ + y = f (t),
y(0) = 2, where f (t) is given in
Figure 10.7.5.
1
3
1
f(t)
t
2
Figure 10.7.5: Forcing term for Problem 42.
43. y′ + 2y = f (t),
y(0) = 0, where f (t) is given in
Figure 10.7.6.
1
1
t
f(t)
Figure 10.7.6: Forcing term for Problem 43.
44. y′ −y = f (t),
y(0) = 2, where f (t) is given in
Figure 10.7.7.
1
f(t)
f(t) 5 e—(t—1)
t
1
Figure 10.7.7: Forcing term for Problem 44.
45. y′ −2y = f (t),
y(0) = 0, where f (t) is given in
Figure 10.7.8.
1
3
1
f(t)
t
2
Figure 10.7.8: Forcing term for Problem 45.

706
CHAPTER 10
The Laplace Transform and Some Elementary Applications
46. Solve the initial-value problem
y′ −y = f (t),
y(0) = 1,
where
f (t) =
)
2, 0 ≤t < 1,
−1,
t ≥1,
in the following two ways:
(a) Directly using the Laplace transform.
(b) Using the technique for solving ﬁrst-order linear
equations developed in Section 1.6.
47. The current i(t) in an RL circuit is governed by the
differential equation
di
dt + R
L i = 1
L E(t),
where R and L are constants and E(t) represents the
applied EMF. At t = 0, the switch in the circuit is
closed, and the applied EMF increases linearly from
0 V to 10 V in a time interval of 5 seconds. The EMF
then remains constant for t ≥5. Determine the current
in the circuit for t ≥0.
48. The differential equation governing the charge q(t) on
the capacitor in an RC circuit is
dq
dt +
1
RC q = 1
R E(t),
where R and C are constants and E(t) represents
the applied EMF. Over a time interval of 10 sec-
onds, the applied EMF has the constant value 20 V.
Thereafter, the EMF decays exponentially according
to E(t) = 20e−(t−10). If the capacitor is initially un-
charged and RC ̸= 1, determine the current in the
circuit for t > 0. [Recall that the current i(t) is related
to the charge on the capacitor by i(t) = dq
dt .]
10.8
Impulsive Driving Terms: The Dirac Delta Function
Consider the differential equation
y′′ + by′ + cy = f (t).
We have seen in the previous sections that the Laplace transform is useful in the case
when the forcing term f (t) is piecewise continuous. We now consider another type of
forcing term; namely, that describing an impulsive force. Such a force arises when an
object is dealt an instantaneous blow, for example, when an object is hit by a hammer.
(See Figure 10.8.1.) The aim of this section is to develop a way of representing impulsive
forces mathematically and then to show how the Laplace transform can be used to solve
differential equations when the driving term is due to an impulsive force.
Impulsive force
M
Figure 10.8.1: An example of an
impulsive force.
Suppose that a force of magnitude F acts on an object over the time interval [t1, t2].
The impulse of this force, I, is deﬁned by5
I =
! t2
t1
F(t)dt.
F(t)
t1
t2
t
Figure 10.8.2: When a force of
magnitude F newtons acts on an
object over a time interval [t1, t2]
seconds, the impulse of the force is
given by the area under the curve.
Since F(t) is zero for t outside the interval [t1, t2], we can write
I =
! ∞
−∞
F(t)dt.
Mathematically, I gives the area under the curve y = F(t) lying over the t-axis. (See
Figure 10.8.2.)
5This represents the change in momentum of the object due to the applied force.

10.8
Impulsive Driving Terms: The Dirac Delta Function 707
We now introduce a mathematical description of a force that instantaneously imparts
an impulse of unit magnitude to an object at t = a. Thus, the two properties that we
wish to characterize are the following:
1. The force acts instantaneously.
2. The force has unit impulse.
We proceed in the following manner. Deﬁne the function dϵ(t −a) by
dϵ(t −a) = ua(t) −ua+ϵ(t)
ϵ
,
(10.8.1)
where ua is the unit step function. (See Figure 10.8.3.) We can interpret dϵ(t −a) as
representing a force of magnitude 1/ϵ that acts for a time interval of ϵ starting at t = a.
Notice that this force does have unit impulse, since
I =
! ∞
−∞
dϵ(t −a)dt =
! ∞
−∞
ua(t) −ua+ϵ(t)
ϵ
dt =
! a+ϵ
a
1
ϵ dt = 1.
de(t — a)
1/e
1/e
e
a
a 1 e
t
Area of 1 unit
Figure 10.8.3: The function dϵ(t −a).
To capture the idea of an instantaneous force we take the limit as ϵ →0+. It follows
from (10.8.1) that
lim
ϵ→0+ dϵ(t −a) = 0 whenever t ̸= a.
Also, since I = 1 for all t,
lim
ϵ→0+ I = 1.
These properties characterize mathematically the idea of a force of unit impulse acting
instantaneously at t = a. We use them to deﬁne the unit impulse function.
DEFINITION
10.8.1
The unit impulse function, or Dirac delta function, δ(t −a) is the (generalized)
function that satisﬁes
1. δ(t −a) = 0, t ̸= a
2.
! ∞
−∞
δ(t −a) dt = 1.
Remark
The unit impulse function is not a function in the usual sense. It is an example
of what is called a generalized function. The detailed study of such functions is beyond

708
CHAPTER 10
The Laplace Transform and Some Elementary Applications
the scope of the present text. However, all that we will require are the properties (1) and
(2) of Deﬁnition 10.8.1.
Thus, to summarize,
δ(t −a) describes a force that instantaneously
imparts a unit impulse to a system at t = a.
We now consider the possibility of determining the Laplace transform of δ(t −a).
The natural way to do this is to return to the function dϵ(t −a) and deﬁne the Laplace
transform of δ(t −a) in the following manner.
L[δ(t −a)] = lim
ϵ→0+ L[dϵ(t −a)] = lim
ϵ→0+
! ∞
0
e−st
"ua(t) −ua+ϵ(t)
ϵ
#
dt
= lim
ϵ→0+
1
ϵ
! a+ϵ
a
e−stdt = lim
ϵ→0+
1
ϵ
)
−1
s [e−s(a+ϵ) −e−sa]
+
= 1
s e−sa lim
ϵ→0+
.1 −e−ϵs
ϵ
/
.
Using L’Hopital’s rule to evaluate the preceding limit yields
L[δ(t −a)] = e−sa.
In particular,
L[δ(t)] = 1.
It can be shown more generally that if g is a continuous function on (−∞, ∞), then
! ∞
−∞
g(t)δ(t −a)dt = g(a).
Example 10.8.2
Solve the initial-value problem
y′′ + 4y′ + 13y = δ(t −π),
y(0) = 2,
y′(0) = 1.
Solution:
Taking the Laplace transform of both sides of the given differential equation
and imposing the initial conditions yields
s2Y −2s −1 + 4(sY −2) + 13Y = e−πs,
which implies that
Y(s) = e−πs + 2s + 9
s2 + 4s + 13 = e−πs + 2s + 9
(s + 2)2 + 9
=
e−πs
(s + 2)2 + 9 +
2(s + 2)
(s + 2)2 + 9 +
5
(s + 2)2 + 9.
Taking the inverse Laplace transform of both sides gives
y(t) = L−1
)1
3e−πt L[e−2t sin 3t]
+
+ 2e−2t cos 3t + 5
3e−2t sin 3t
= 1
3uπ(t)e−2(t−π) sin[3(t −π)] + 2e−2t cos 3t + 5
3e−2t sin 3t.

10.8
Impulsive Driving Terms: The Dirac Delta Function 709
Since sin[3(t −π)] = −sin 3t, we ﬁnally obtain
y(t) = −1
3uπ(t)e−2(t−π) sin 3t + e−2t
.
2 cos 3t + 5
3 sin 3t
/
.
□
Example 10.8.3
Consider the spring-mass system depicted in Figure 10.8.4. At t = 0, the mass is pulled
down a distance 1 unit from equilibrium and released from rest. After 3 seconds, the
mass is dealt an instantaneous blow that imparts ﬁve units of impulse in the upward
direction. The initial-value problem governing the motion of the mass is
d2y
dt2 + 4y = −5δ(t −3),
y(0) = 1,
dy
dt (0) = 0,
where 5 δ(t −3) describes the impulsive force that acts on the mass at t = 3, and the
positive y-direction is downward. Determine the motion of the mass for all t > 0.
Positive y-
direction
Impulsive force
acts after 3 s
Figure 10.8.4: A spring-mass
system in which friction is
neglected and the only external
force acting on the system is an
impulsive force that imparts 5
units of impulse at t = 3.
Solution:
Taking the Laplace transform of the differential equation and imposing the
initial conditions yields
s2Y(s) −s + 4Y(s) = −5e−3s,
so that
Y(s) = −5e−3s + s
s2 + 4
= −5e−3s
s2 + 4 +
s
s2 + 4.
We can now take the inverse Laplace transform of both sides of this equation to obtain
y(t) = L−1
)
−5
2e−3s L[sin 2t]
+
+ cos 2t.
Thus,
y(t) = −5
2u3(t) sin[2(t −3)] + cos 2t.
The ﬁrst term on the right-hand side represents the contribution from the impulsive force.
Obviously this does not affect the motion of the mass until t = 3, but then contributes
for all t ≥3. More explicitly, we can write the solution as
y(t) =
)
cos 2t,
0 ≤t < 3,
cos 2t −5
2 sin 2(t −3),
t ≥3.
The resulting motion is depicted in Figure 10.8.5, where the effect of the impulsive force
is apparent. We see that y(t) is continuous at t = 3, but that y′(t) is discontinuous
at t = 3.
2
1
—1
—2
2
8
10
y(t)
t
4
6
Figure 10.8.5: The motion of the spring-mass system in Example 10.8.3.
□

710
CHAPTER 10
The Laplace Transform and Some Elementary Applications
Exercises for 10.8
Key Terms
Impulsive force, Unit impulse (Dirac delta) function.
Skills
• Be able to solve initial-value problems involving the
Dirac delta function.
True-False Review
For Questions (a)–(e), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The impulse of a force F(t) acting on an object is
obtained by integrating F(t) over all values of t.
(b) A unit impulseforceis aforcethat acts instantaneously
and has unit impulse.
(c) The Laplace transform of the unit impulse function
δ(t −a) is eas.
(d) The initial-value problem governing the motion of a
spring-mass system that is dealt an instantaneous blow
at t = a is a second-order nonhomogeneous differen-
tial equation involving the unit impulse function.
(e) The instantaneous blow delivered to a spring-mass
system determines the initial conditions for the nonho-
mogeneous differential equation governing the motion
of the mass as a function of time.
Problems
For Problems 1–13, solve the given initial-value problem.
1. y′ + y = δ(t −5),
y(0) = 3.
2. y′ −2y = δ(t −2),
y(0) = 1.
3. y′ + 4y = 3 δ(t −1),
y(0) = 2.
4. y′ −5y = 2e−t + δ(t −3),
y(0) = 0.
5. y′′ −3y′ + 2y = δ(t −1),
y(0) = 1,
y′(0) = 0.
6. y′′ −4y = δ(t −3),
y(0) = 0,
y′(0) = 1.
7. y′′ +2y′ +5y = δ(t −π/2),
y(0) = 0,
y′(0) = 2.
8. y′′−4y′ + 13y = δ(t−π/4),
y(0) = 3,
y′(0) = 0.
9. y′′ + 4y′ + 3y = δ(t −2),
y(0) = 1,
y′(0) = −1.
10. y′′+6y′+13y = δ(t−π/4),
y(0) = 5,
y′(0) = 5.
11. y′′ + 9y = 15 sin 2t + δ(t −π/6),
y(0) = 0,
y′(0) = 0.
12. y′′ + 16y = 4 cos 3t + δ(t −π/3),
y(0) = 0,
y′(0) = 0.
13. y′′ + 2y′ + 5y = 4 sin t + δ(t −π/6),
y(0) = 0,
y′(0) = 1.
14. The motion of a spring-mass system is governed by
the initial-value problem
d2y
dt2 + 4y = F0 cos 3t,
y(0) = 0,
dy
dt (0) = 0,
where F0 is a constant. At t = 5 seconds, the mass is
dealt a blow in the upward (negative) direction that in-
stantaneously imparts 4 units of impulse to the system.
Determine the resulting motion of the mass.
15. The motion of a spring-mass system is governed by
d2y
dt2 + 4dy
dt + 13y = 10 sin 5t,
y(0) = 0,
dy
dt (0) = 0.
At t = 10 seconds, the mass is dealt a blow in
the downward (positive) direction that instantaneously
imparts 2 units of impulse to the system. Determine
the resulting motion of the mass.
16. Considerthespring-masssystemwhosemotionisgov-
erned by the initial-value problem
d2y
dt2 + ω2
0y = F0 sin ωt + Aδ(t −t0),
y(0) = 0,
dy
dt (0) = 0,
where ω0, ω, F0, A, and t0 are positive constants and
ω ̸= ω0. Solve the initial-value problem to determine
the position of the mass at time t.

10.9
The Convolution Integral 711
10.9
The Convolution Integral
Very often in solving a differential equation using the Laplace transform method we
require the inverse Laplace transform of an expression of the form
H(s) = F(s)G(s),
where F(s) and G(s) are functions whose inverse Laplace transform is known. It is
important to note that
L−1[H(s)] ̸= L−1[F(s)]L−1[G(s)].
For example,
L−1
"
1
(s −1)(s2 −1)
#
= L−1
"
1
2(s −1) −
s + 1
2(s2 + 1)
#
= 1
2et −1
2 cos t −1
2 sin t,
whereas
L−1
"
1
s −1
#
L−1
"
1
s2 + 1
#
= et sin t.
Consequently,
L−1
"
1
(s −1)(s2 + 1)
#
̸= L−1
"
1
s −1
#
L−1
"
1
s2 + 1
#
.
However, it is possible, at least in theory, to determine L−1[F(s)G(s)] directly in
terms of an integral involving f (t) and g(t). Before showing this, we require a deﬁnition.
DEFINITION
10.9.1
Suppose that f and g are continuous on the interval [0, b]. Then for t in (0, b], the
convolution product, f ∗g, of f and g is deﬁned by
( f ∗g)(t) =
! t
0
f (t −τ)g(τ)dτ.
Notice that f ∗g is indeed a function of t. The integral
! t
0
f (t −τ)g(τ)dτ
is called a convolution integral.
Example 10.9.2
If f (t) = t and g(t) = sin t, determine f ∗g.
Solution:
From Deﬁnition 10.9.1, we have
( f ∗g)(t) =
! t
0
(t −τ) sin τdτ = t
! t
0
sin τdτ −
! t
0
τ sin τdτ
= t(1 −cos t) −(sin t −t cos t)
= t −sin t.
□

712
CHAPTER 10
The Laplace Transform and Some Elementary Applications
The convolution product satisﬁes the three basic properties of the ordinary multi-
plicative product, namely,
1.
f ∗g = g ∗f .
(Commutative)
2.
f ∗(g ∗h) = ( f ∗g) ∗h.
(Associative)
3.
f ∗(g + h) = f ∗g + f ∗h.
(Distributive over addition)
The proofs of these properties are left as exercises.
We now show how the convolution product can be useful in evaluating inverse
Laplace transforms.
Theorem 10.9.3
(The Convolution Theorem)
If f and g are in E(0, ∞), then
L[ f ∗g] = L[ f ]L[g].
(10.9.1)
Conversely,
L−1[F(s)G(s)] = ( f ∗g)(t).
(10.9.2)
Proof We must use the deﬁnition of the Laplace transform and the convolution product:
L[ f ∗g] =
! ∞
0
e−st
)! t
0
f (t −τ)g(τ)dτ
+
dt
=
! ∞
0
! t
0
e−st f (t −τ)g(τ)dτdt.
It is not clear how to proceed at this point. However, when dealing with an iterated double
integral, it is often worth changing the order of integration to see if any simpliﬁcation
arises. In this case, the limits of integration are 0 ≤τ ≤t, 0 ≤t < ∞, so that the region
of integration is that part of the tτ-plane that lies above the t-axis and below the line
τ = t. This region is shown in Figure 10.9.1.
t
t5t
t ranges from t to ` and then t
ranges from 0 to ` to cover the
region of integration
t
Figure 10.9.1: Changing the order of integration in Theorem 10.9.3.
Reversing the order of integration, the new limits are τ ≤t < ∞, 0 ≤τ < ∞.
Thus, we can write
L[( f ∗g)(t)] =
! ∞
0
! ∞
τ
e−st f (t −τ)g(τ)dtdτ.
We now make the change of variable u = t −τ in the ﬁrst iterated integral. Then du = dt
(remember that τ is treated as a constant when performing the inside integration) and

10.9
The Convolution Integral 713
the new u-limits are 0 ≤u < ∞. Consequently,
L[( f ∗g)(t)] =
! ∞
0
! ∞
0
e−s(u+τ)g(τ) f (u)dudτ
=
! ∞
0
e−sτ g(τ)
"! ∞
0
e−su f (u)du
#
dτ.
=
"! ∞
0
e−su f (u)du
# "! ∞
0
e−sτ g(τ)dτ
#
= L[ f ]L[g],
as required. The converse, (10.9.2), is obtained in the usual manner by taking the inverse
Laplace transform of (10.9.1).
Remark
It can be shown more generally that
L−1[F1(s)F2(s) . . . Fn(s)] = ( f1 ∗f2 ∗· · · ∗f n)(t).
Example 10.9.4
Determine L[ f ] if
f (t) =
! t
0
sin(t −τ)e−τdτ.
Solution:
In this case, we recognize that
f (t) = sin t ∗e−t,
so that, by the convolution theorem,
L[ f ] = L[sin t]L[e−t] =
1
(s2 + 1)(s + 1).
□
Example 10.9.5
Find L−1
"
1
s2(s −1)
#
.
Solution:
We could determine the inverse Laplace transform in the usual manner
by ﬁrst using a partial fractions decomposition. However, we will use the convolution
theorem:
L−1
"
1
s2(s −1)
#
= L−1
" 1
s2
#
∗L−1
"
1
s −1
#
=
! t
0
(t −τ)eτdτ.
By integrating by parts, we obtain
L−1
"
1
s2(s −1)
#
=
,
teτ −(τeτ −eτ)
- 99t
0 = et −1.
□
Example 10.9.6
Find L−1
"
G(s)
(s −1)2 + 1
#
.
Solution:
Using the convolution theorem, we see that
L−1
"
G(s)
(s −1)2 + 1
#
= L−1
"
1
(s −1)2 + 1
#
∗L−1[G(s)]
= et sin t ∗g(t).

714
CHAPTER 10
The Laplace Transform and Some Elementary Applications
That is,
L−1
"
G(s)
(s −1)2 + 1
#
=
! t
0
et−τ sin(t −τ)g(τ)dτ.
□
Example 10.9.7
Solve the initial-value problem
y′′ + ω2y = f (t),
y(0) = α,
y′(0) = β,
where α, β, and ω are constants with ω ̸= 0, and f (t) is an arbitrary function in E(0, ∞).
Solution:
Taking the Laplace transform of the differential equation and imposing the
given initial conditions yields
s2Y(s) −αs −β + ω2Y(s) = F(s),
where F(s) denotes the Laplace transform of f . Simplifying, we obtain
Y(s) =
F(s)
s2 + ω2 +
αs
s2 + ω2 +
β
s2 + ω2 .
Taking the inverse Laplace transform of both sides of this equation and using the con-
volution theorem yields
y(t) = 1
ω
! t
0
sin ω(t −τ) f (τ)dτ + α cos ωt + β
ω sin ωt.
□
Volterra Integral Equations
The applications of the Laplace transform that we have so far considered have been for
solving differential equations. We now brieﬂy discuss another type of equation whose
solution can often be obtained by using the Laplace transform.
An equation of the form
x(t) = f (t) +
! t
0
k(t −τ)x(τ)dτ
(10.9.3)
is called a Volterra integral equation. In this equation, the unknown function is x(t).
The functions f and k are speciﬁed, and k is called the kernel of the equation. For
example,
x(t) = 2 sin t +
! t
0
cos(t −τ)x(τ)dτ
is a Volterra integral equation. We now show how the convolution theorem for Laplace
transforms can be used to determine, up to the evaluation of an inverse transform, the
function x(t) that satisﬁes Equation (10.9.3). The key to solving Equation (10.9.3) is to
notice that the integral that appears in this equation is, in fact, a convolution integral.
Thus, taking the Laplace transform of both sides of Equation (10.9.3) we obtain
X(s) = F(s) + L[k(t) ∗x(t)].
That is, using the convolution theorem,
X(s) = F(s) + K(s)X(s).
Solving algebraically for X(s) yields
X(s) =
F(s)
1 −K(s),

10.9
The Convolution Integral 715
so that
x(t) = L−1
"
F(s)
1 −K(s)
#
.
This technique can be used to solve a wide variety of Volterra integral equations.
Example 10.9.8
Solve the Volterra integral equation
x(t) = 3 cos t + 5
! t
0
sin(t −τ)x(τ)dτ.
Solution:
Taking the Laplace transform of the given integral equation and using the
convolution theorem yields
X(s) =
3s
s2 + 1 +
5
s2 + 1 X(s).
That is,
X(s)
.s2 −4
s2 + 1
/
=
3s
s2 + 1,
so that
X(s) =
3s
s2 −4.
Decomposing the right-hand side into partial fractions, we obtain
X(s) = 3
2
.
1
s −2 +
1
s + 2
/
.
Taking the inverse Laplace transform yields
x(t) = 3
2(e2t + e−2t) = 3 cosh 2t.
□
Exercises for 10.9
Key Terms
Convolution product, Convolution integral, Convolution
Theorem, Volterra integral equation, Kernel of the Volterra
integral equation.
Skills
• Be able to compute the convolution product of two
functions f and g.
• Be able to prove the basic properties of the convolution
product.
• Be able to use the Convolution Theorem to compute
the Laplace transform of a convolution product.
• Be able to use the Convolution Theorem to compute
the inverse Laplace transform of a product of func-
tions.
• Be able to solve initial-value problems up to the eval-
uation of a convolution integral.
• Be able to solve Volterra integral equations.
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) For all continuous functions f and g, f ∗g = g ∗f .
(b) If the functions f and g are continuous and positive
on [0, b], then the convolution product f ∗g is an
increasing function of t.
(c) If f and g are in E(0, ∞), then L[ f g] = L[ f ]∗L[g].
(d) Any equation of the form
x(t) = f (t) +
! t
0
k(t −τ)x(τ)dτ
is called a Volterra integral equation.

716
CHAPTER 10
The Laplace Transform and Some Elementary Applications
(e) If f, g, and h are continuous functions on [0, b] and
if f ∗g = f ∗h, then g = h.
(f) The Convolution Theorem states (in part) that the in-
verse Laplace transform of the product F(s)G(s) is
the convolution product ( f ∗g)(t), where f and g are
in E(0, ∞).
(g) If a is a constant and f and g are continuous on the
interval [0, b], then
a( f ∗g) = (af ) ∗g = f ∗(ag).
Problems
For Problems 1–6, determine f ∗g.
1. f (t) = t,
g(t) = 1.
2. f (t) = 6t2,
g(t) = 5t3.
3. f (t) = cos t,
g(t) = t.
4. f (t) = et,
g(t) = t.
5. f (t) = t2,
g(t) = et.
6. f (t) = et,
g(t) = et sin t.
7. Prove that f ∗g = g ∗f .
8. Prove that f ∗(g ∗h) = ( f ∗g) ∗h.
9. Prove that f ∗(g + h) = f ∗g + f ∗h.
For Problems 10–14, determine L[ f ∗g].
10. f (t) = t,
g(t) = sin t.
11. f (t) = e2t,
g(t) = 1.
12. f (t) = sin t,
g(t) = cos 2t.
13. f (t) = et,
g(t) = te2t.
14. f (t) = t2,
g(t) = e2t sin 2t.
For Problems 15–20, determine L−1[F(s)G(s)] in the fol-
lowing two ways: (a) using the Convolution Theorem,
(b) using partial fractions.
15. F(s) = 1
s ,
G(s) =
1
s −2.
16. F(s) =
1
s + 1,
G(s) = 1
s .
17. F(s) =
s
s2 + 4,
G(s) = 2
s .
18. F(s) =
1
s + 2,
G(s) =
s + 2
s2 + 4s + 13.
19. F(s) =
1
s2 + 9,
G(s) = 2
s3 .
20. F(s) = 1
s2 ,
G(s) = e−πs
s2 + 1.
For Problems 21–25, express L−1[F(s)G(s)] in terms of a
convolution integral.
21. F(s) = 4
s3 ,
G(s) =
s −1
s2 −2s + 5.
22. F(s) =
s + 1
s2 + 2s + 2,
G(s) =
1
(s + 3)2 .
23. F(s) =
2
s2 + 6s + 10,
G(s) =
2
s −4.
24. F(s) =
s + 4
s2 + 8s + 25,
G(s) = se−π/2
s2 + 16.
25. F(s) =
1
s −4,
G(s) arbitrary.
For Problems 26–32, solve the given initial-value problem
up to the evaluation of a convolution integral.
26. y′′ + y = e−t,
y(0) = 0,
y′(0) = 1.
27. y′′ −2y′ + 10y = cos 2t,
y(0) = 0,
y′(0) = 1.
28. y′′ + 16y = f (t),
y(0) = α,
y′(0) = β, where α
and β are constants.
29. y′ −ay = f (t),
y(0) = α, where a and α are con-
stants.
30. y′′ −a2y = f (t),
y(0) = α,
y′(0) = β, where
a, α, and β are constants and a ̸= 0.
31. y′′ −(a+b)y′ + aby = f (t),
y(0) = α,
y′(0) =
β, where a, b, α, and β are constants and a ̸= b.
32. y′′−2ay′+(a2+b2)y = f (t),
y(0) = α,
y′(0) =
β, where a, b, α, and β are constants, and b ̸= 0.
For Problems 33–38, solve the given Volterra integral
equation.
33. x(t) = e−t + 4
: t
0(t −τ)x(τ)dτ.
34. x(t) = 2e3t −
: t
0 e2(t−τ)x(τ)dτ.

10.10
Chapter Review 717
35. x(t) = 4et + 3
: t
0 e−(t−τ)x(τ)dτ.
36. x(t) = 1 + 2
: t
0 sin(t −τ)x(τ)dτ.
37. x(t) = e2t + 5
: t
0 cos[2(t −τ)]x(τ)dτ.
38. x(t) = 2{1 +
: t
0 cos[2(t −τ)]x(τ)dτ}.
39. Show that the initial-value problem
y′′ + y = f (t),
y(0) = 0,
y′(0) = 0
can be reformulated as the integral equation
x(t) = f (t) −
! t
0
(t −τ)x(τ)dτ,
where y′′(t) = x(t).
10.10
Chapter Review
Laplace transforms are a powerful tool in solving differential equations. In particular,
the real power of the Laplace transform comes from the simpliﬁcation of differential
equations with a forcing term that is piecewise continuous or periodic in nature. The
Laplace transform of a function f deﬁned on an interval [0, ∞) is the function F(s)
deﬁned by
F(s) =
! ∞
0
e−st f (t)dt,
(10.10.1)
provided that the improper integral converges. Throughout the chapter, we have devel-
oped formulas for the Laplace transforms of a number of basic functions. These results
are summarized in the table below.
Summary of Laplace Transforms
Function f (t)
Laplace Transform F(s)
f (t) = tn, n a nonnegative integer
F(s) =
n!
sn+1 , s > 0.
f (t) = eat, a constant
F(s) =
1
s −a , s > a.
f (t) = sin bt, b constant
F(s) =
b
s2 + b2 , s > 0.
f (t) = cos bt, b constant
F(s) =
s
s2 + b2 , s > 0.
f (t) = t−1/2
F(s) = (π/s)1/2, s > 0.
f (t) = ua(t) (see Section 10.7)
F(s) = 1
s e−as.
f (t) = δ(t −a) (see Section 10.8)
F(s) = e−as.
Transform of Derivatives (see Section 10.4)
f ′
L[ f ′] = sL[ f ] −f (0).
f ′′
L[ f ′′] = s2L[ f ] −s f (0) −f ′(0).
Shifting Theorems (see Sections 10.5 and 10.7)
eat f (t)
F(s −a).
ua(t) f (t −a).
e−as F(s).

718
CHAPTER 10
The Laplace Transform and Some Elementary Applications
The Laplace transform satisﬁes the linearity properties
L[ f + g] = L[ f ] + L[g]
and
L[cf ] = cL[ f ]
for all transformable functions f and g and constants c. Therefore, we can take linear
combinations of the functions from the above table and compute their Laplace transforms
as well.
In order to transform a ﬁrst- or second-order differential equation with unknown
function y(t), we use the formulas given in the table above for L[ f ′] or L[ f ′′], re-
spectively, and obtain the resulting algebraic equation for the Laplace transform Y(s)
of y(t). After solving this equation for Y(s), we ﬁnd the solution y(t) of the initial-
value problem by taking the inverse Laplace transform: y(t) = L−1[Y(s)]. Note that
y(t) = L−1[Y(s)] if and only if L[y](s) = Y(s). Higher-order differential equations
can also be handled by this technique, using a generalization of the formulas for Laplace
transforms of derivatives in the table above. The ﬁgure below summarizes this technique.
Solve algebraically
for Y(s)
Initial value
problem
Algebraic equation
for Y(s)
Y(s) 5 F(s)
Take inverse
Laplace transform
Solution
y(t) 5 L21[F(s)]
Take Laplace
transform
Figure 10.10.1: Using the Laplace transform to solve an initial-value problem.
Additional Problems
For Problems 1–10, use (10.10.1) to determine L[ f ].
1. f (t) = 3t −4.
2. f (t) = sin 2t.
3. f (t) = 4t2.
4. f (t) = 5e−3t.
5. f (t) = 7te−t.
6. f (t) = sin at cos bt, wherea, b arepositiveconstants.
7. f (t) = sin2 at, where a is a positive constant.
8. f (t) =
) 2, 0 ≤t ≤1,
t,
t > 1.
9. f (t) =
) t + 1, 0 ≤t < 3,
t2 −1,
t > 3.
10. f (t) =
⎧
⎨
⎩
2,
0 ≤t ≤1,
1 −t, 1 < t ≤2,
0,
t > 2.
For Problems 11–19, use properties of the Laplace transform
and the table of Laplace transforms to determine L[ f ].
11. f (t) = 5 cos 2t −7e−t −3t6.
12. f (t) = e−5t/√t.
13. f (t) = e3t cos 5t −e−t sin 2t.
14. f (t) = 6t4e−2t −2tet+1 +
√
10t.
15. f (t) = e−5t/√t.
16. f (t) = 2(t −5)u5(t).
17. f (t) = 2 + 2(e−t −1)u1(t).
18. f (t) =
! t
0
(t −w) cos 2w dw.
19. f (t) =
! t
0
(t −w)2ew dw.

10.10
Chapter Review 719
For Problems 20–25, determine a function f (t) that has the
given Laplace transform F(s).
20. F(s) = 3
s2 .
21. F(s) = 4s + 5
s2 + 9 .
22. F(s) =
s −2
s2 + 2s + 2.
23. F(s) =
2
s(s2 + 16).
24. F(s) =
2s + 5
s(s2 + 4s + 20).
25. F(s) =
2s + 5
s(s2 + 4s + 20).
For Problems 26–28, sketch f (t), express f (t) in terms of
ua(t), and determine L{ f (t)}.
26. f (t) =
)
2,
0 ≤t < 1,
3 −t,
t ≥1.
27. f (t) =
)
1,
0 ≤t < ln 2,
2e−t,
t ≥ln 2.
28. f (t) =
⎧
⎪⎪⎨
⎪⎪⎩
t,
0 ≤t < 1,
1
1 < t ≤2,
3 −t, 2 < t ≤3,
0,
t > 3.
29. Let f ∈E(0, ∞) and let a be a positive real number.
Deﬁne the function fa as follows
fa(t) =
) f (t −a),
if t ≥a,
0,
if 0 ≤t < a.
Show that L{ fa(t)} = f (s + a).
30. Use the Convolution Theorem and the table of Laplace
transforms to show that
! x
0
(x −w)awbdw =
a! b!
(a + b + 1)!xa+b+1,
a > −1, b > −1, x > 0.
31. Let f (x) = xeax, where a is a constant.
(a) Show that
L{ f ′(x)} = aL{ f (x)} +
1
s −a .
(b) Use the result from (a) together with the ex-
pression for the Laplace transform of the deriva-
tive of a function, to determine L{xeax} without
integrating.
(c) Use mathematical induction to establish that
L{xneax} =
n!
(s −a)n+1 ,
s > a,
n = 1, 2, . . . .
32. Let y(t) be the solution to the initial-value problem
y′ + ay = f (t), y(0) = y0, where a and y0 are con-
stants. Verify that
L[y] = L[ f ]
s + a +
y0
s + a ,
and show that
y(t) = y0e−at +
! t
0
e−a(t−w) f (w)dw.
33. Show that the general solution to the initial-value
problem
y(n) + a1y(n−1) + · · · + any = f (t),
y(0) = 0, y′(0) = 0, . . . , y(n−1)(0) = 0,
is
y(t) =
! t
0
K(t −w) f (w)dw,
for an appropriate function K(t) that should be
determined.
For Problems 34–40, use the Laplace transform to solve the
given initial-value problem.
34. y′′ −3y′ −4y = 4e−t,
y(0) = 1,
y′(0) = 1.
35. y′′ −2y′ −8y = 5,
y(0) = 1,
y′(0) = 0.
36. y′′ + 9y = 8 cos 3t,
y(0) = 1,
y′(0) = 0.
37. y′′ + y = f (t),
y(0) = 0,
y′(0) = 1, where
f (t) =
) 1, 0 ≤t < π/2,
0,
t ≥π/2.
38. y′′+4y = 4 sin t+3δ(t−2),
y(0) = 2,
y′(0) = 1.

720
CHAPTER 10
The Laplace Transform and Some Elementary Applications
39. y′′ + 2y′ + y = δ(t −4),
y(0) = 0,
y′(0) = 0.
40. y′′ + 4y′ + 4y = δ(t −4),
y(0) = 1,
y′(0) = 2.
For Problems 41–44, use the Laplace transform to solve the
given system of differential equations subject to the given
initial conditions.
41. dx1
dt = x1 + 2x2,
dx2
dt = 2x1 + x2,
x1(0) = 1,
dx1
dt (0) = 0.
42. dx1
dt = 2x2,
dx2
dt = −2x1,
x1(0) = 0,
x2(0) = 1.
43. dx1
dt = −2x2,
dx2
dt = 2x1 + 4x2,
x1(0) = 1,
x2(0) = 1.
44. dx1
dt = 2x1 + 4x2 + 16 sin 2t,
dx2
dt = −2x1 −2x2 + 16 cos 2t,
x1(0) = 0,
x2(0) = 1.
For Problems 45–48, use the Laplace transform to solve the
given integral equation.
45. x(t) = 2t +
! t
0
sin(t −τ)x(τ)dτ.
46. x(t) = 2t2 +
! t
0
(t −τ)x(τ)dτ.
47. x(t) = 2t2 +
! t
0
sin[2(t −τ)]x(τ)dτ.
48. x(t) = 3 + 4
! t
0
x(t −τ) cos τdτ.
Project: Population Growth with Harvesting
For many species of animals, population is controlled by allowing a harvesting period
during the year. In order to guard against overharvesting, restrictions are placed on the
length of the harvesting period and the harvesting rate. For example, in parts of Southern
California the duck hunting season lasts for about 100 days each year with the restriction
that a hunter may harvest a maximum of seven ducks per day during that period. In
this project you will analyze a modiﬁcation to the Malthusian population growth model
considered in Chapter 1 that takes into account such harvesting.
Let P(t) denote the population at time t, with t measured in days; let r denote the
harvesting rate, the number of animals harvested/day; and let a days denote the length
of the harvesting period. Then, the behavior of P(t) can be modeled by the following
initial-value problem
d P
dt = kP −r [1 −ua(t)] ,
(k > 0)
P(0) = P0,
and the harvesting period corresponds to the time interval 0 ≤t ≤a.
1. Use the Laplace transform to solve the preceding initial-value problem to obtain
P(t) = 1
k (kP0 −r) ekt + r
k
5
1 +
7
ek(t−a) −1
8
ua(t)
6
,
t ≥0.
2. Show that the population is increasing, constant, or decreasing during the har-
vesting period depending on whether r is less than, equal to, or greater than kP0,
respectively, and sketch representative solution curves in each case.
3. An extreme case of overharvesting occurs if all members of the population are
harvested during the harvesting period. This would correspond to extinction of the
population. According to your results from Part 2, this could only occur ifr > kP0.
In this case, show that extinction will indeed occur if
r ≥
kP0
1 −e−ka ,

10.10
Chapter Review 721
and that the time to extinction is
t0 = 1
k ln
.
r
r −kP0
/
≤a.
For the remainder of the project, assume that
kP0 < r <
kP0
1 −e−ka ,
so that the population is decreasing during the harvesting period, but extinction does not
occur.
4. Determine the harvesting rate that would ensure that at the beginning of the fol-
lowing harvesting period (t = 365) the population will have recovered to its initial
value, P0.
5. Show that in order for no more than α% of the initial population to be harvested,
the length of the harvesting period must satisfy
eka ≤100(r −kP0) + αkP0
100(r −kP0)
.
6. Consider a population that has
k = 1/1000,
P0 = 100,000,
r = 400.
(a) Determine the maximum length of the harvesting period in order to ensure
that no more than 20% of the initial population are harvested.
(b) Using the result obtained in (a), determine the initial population in the sub-
sequent harvesting period.

11
Series Solutions to
Linear Differential
Equations
So far, the techniques that we have developed for solving differential equations have
involved determining a closed form solution for a given equation (or system) in terms of
familiar elementary functions. Essentially, the only differential equations of order two
or more that we can derive such solutions for are as follows:
1. Constant coefﬁcient equations.
2. Cauchy-Euler equations.
For example, we cannot at the present time determine the solution to the seemingly
simple differential equation
y′′ + ex y = 0.
In this chapter, we consider the possibility of representing solutions to linear differential
equations in the form of some type of inﬁnite series. We begin in Section 11.2 with the
simplest case, namely, differential equations whose solutions can be represented as a
convergent power series,
y(x) =
∞
!
n=0
anxn,
where an are constants. This can be considered as a generalization of the method of
undetermined coefﬁcients to the case when we have an inﬁnite number of constants.
We will determine the appropriate values of these constants by substitution into the
differential equation.
Not all differential equations have solutions that can be represented by a convergent
power series. We will ﬁnd that the next simplest type of series solution that is applicable
722

11.1
A Review of Power Series 723
to a broad class of linear differential equations is one of the form
y(x) = xr
∞
!
n=0
anxn,
called a Frobenius series. Here, in addition to the coefﬁcients an, we must also determine
the value of the constantr (which in general will not be a positive integer). The analysis of
this problem is quite involved, and the computations can be extremely tedious. However,
the technique is an important and useful addition to the applied mathematician’s tools
for solving differential equations.
Before beginning the development of the theory, we note that for simplicity we
will restrict our attention in this chapter to second-order linear homogeneous differential
equations whose standard form is
y′′ + p(x)y′ + q(x)y = 0,
where p and q are functions that are speciﬁed on some interval I. The techniques can
be extended easily to higher order, and also to systems of linear differential equations.
11.1
A Review of Power Series
We begin with a very brief review of the main facts about power series, which should be
familiar from a previous calculus course. They will be required throughout the remainder
of the chapter.
DEFINITION
11.1.1
An inﬁnite series of the form
∞
!
n=0
an(x −x0)n,
(11.1.1)
where an and x0 are constants, is called a power series centered at x = x0.
The substitution u = x −x0 has the effect of transforming (11.1.1) to
∞
!
n=0
anun,
so that we can, without loss of generality, restrict attention to power series of the form
∞
!
n=0
anxn,
(11.1.2)
whose center is x = 0. The series (11.1.2) is said to converge at x = x1 if
lim
k→∞
k
!
n=0
anxn
1
exists and is ﬁnite. The set of all x for which (11.1.2) converges is called the interval of
convergence.

724
CHAPTER 11
Series Solutions to Linear Differential Equations
Theorem 11.1.2
(Basic Convergence Theorem)
For the power series (11.1.2), precisely one of the following is true:
1.
∞
!
n=0
anxn converges only at x = 0.
2.
∞
!
n=0
anxn converges for all real x.
3. There is a positive number R such that
∞
!
n=0
anxn converges (absolutely) for |x| < R
and diverges for |x| > R.
Remark
The number R occurring in possibility (3) is called the radius of conver-
gence. (See Figure 11.1.1.) The convergence or divergence of the series at the endpoints
x = ±R must be treated separately. In possibility (2), we deﬁne the radius of convergence
to be R = ∞.
DIVERGENCE
DIVERGENCE
CONVERGENCE
x 5  2R
 x = R
x 5  0
Figure 11.1.1: The radius of convergence of a power series.
Ratio Test
For the power series
∞
!
n=0
anxn, if
lim
n→∞
""""
an+1
an
"""" = L,
then the radius of convergence of the power series is R = 1
L . If L = 0, the series
converges for all x, whereas if L = ∞, the power series converges only at x = 0.
Example 11.1.3
Determine the radius of convergence of
∞
!
n=0
n2xn
3n .
Solution:
In this case, we have
lim
n→∞
""""
an+1
an
"""" = lim
n→∞
(n + 1)2
3n+1
· 3n
n2 = lim
n→∞
(n + 1)2
3n2
= 1
3.
Thus, L = 1
3, so that the radius of convergence is R = 3. It is easy to see that the series
diverges at the endpoints1 x = ±3, so that the interval of convergence is (−3, 3).
□
1Recall that a necessary (but not sufﬁcient) condition for the convergence of the inﬁnite series
∞
!
n=0
anxn is
that lim
n→∞an = 0.

11.1
A Review of Power Series 725
The Algebra of Power Series
Two power series
∞
!
n=0
anxn and
∞
!
n=0
bnxn are equal if and only if each corresponding
coefﬁcient is equal; that is, an = bn for all n. In particular,
∞
!
n=0
anxn = 0
if and only if
an = 0 for every n.
We will use this result repeatedly throughout the chapter.
Now let
∞
!
n=0
anxn and
∞
!
n=0
bnxn be power series with radii of convergence R1 and
R2, respectively, and let R = min{R1, R2}. For |x| < R, deﬁne the functions f and g
by
f (x) =
∞
!
n=0
anxn,
g(x) =
∞
!
n=0
bnxn.
Then, for |x| < R,
1.
f (x) + g(x) =
∞
!
n=0
(an + bn)xn (addition of power series).
2. cf (x) =
∞
!
n=0
(can)xn (multiplication of a power series by a real number c).
3.
f (x)g(x) =
∞
!
n=0
cnxn, where cn =
n
!
k=0
an−kbk (multiplication of power series).
The coefﬁcients cn appearing in this formula can be written in the equivalent form
cn =
n
!
k=0
akbn−k.
Example 11.1.4
Assume that the coefﬁcients in the expansion
f (x) =
∞
!
n=0
anxn
satisfy
∞
!
n=1
nanxn −
∞
!
n=0
anxn+1 = 0.
(11.1.3)
Express all an in terms of a0.
Solution:
We replace n by n −1 in the second summation in (11.1.3) (and alter the
range of n appropriately) to obtain a common power of xn in both sums. The result is
∞
!
n=1
nanxn −
∞
!
n=1
an−1xn = 0,
which can be written as
∞
!
n=1
(nan −an−1)xn = 0.

726
CHAPTER 11
Series Solutions to Linear Differential Equations
It follows that the an must satisfy the recurrence relation2
nan −an−1 = 0,
n = 1, 2, 3, . . . ;
that is,
an = 1
n an−1,
n = 1, 2, 3, . . . .
Substituting for successive values of n, we obtain
n = 1:
a1 = a0,
n = 2:
a2 = 1
2a1 = 1
2a0,
n = 3:
a3 = 1
3a2 =
1
3 · 2a0,
n = 4:
a4 = 1
4a3 =
1
4 · 3 · 2a0.
Continuing in this manner, we see that
an = 1
n!a0.
Consequently, we can write3
f (x) = a0
∞
!
n=0
1
n!xn.
□
Differentiation of Power Series
Suppose that #∞
n=0 anxn has radius of convergence R, and let
f (x) =
∞
!
n=0
anxn,
|x| < R.
Then f (x) can be differentiated an arbitrary number of times on the interval |x| < R.
Furthermore, the derivatives can be obtained by termwise differentiation. Thus,
f ′(x) =
∞
!
n=0
nanxn−1 =
∞
!
n=1
nanxn−1,
f ′′(x) =
∞
!
n=1
n(n −1)anxn =
∞
!
n=2
n(n −1)anxn,
and so on for higher-order derivatives. Similar statements can be made for integration of
power series, but these will not be needed in this text.
2A recurrence relation is an equation that expresses an element an in a sequence of numbers in terms of the
previous term(s) an−1, an−2, . . . of the sequence.
3This power series is the Maclaurin expansion of ex, so that we can write f (x) = a0ex.

11.1
A Review of Power Series 727
Analytic Functions and Taylor Series
We now introduce one of the main deﬁnitions of the section.
DEFINITION
11.1.5
A function is said to be analytic at x = x0 if it can be represented by a convergent
power series centered at x = x0 with nonzero radius of convergence.
In a previous calculus course the reader should have seen that if a function is analytic
at x = x0, then the power series representation of that function is unique and is given by
f (x) =
∞
!
n=0
f (n)(x0)
n!
(x −x0)n.
(11.1.4)
This is the Taylor series expansion of f (x) about x = x0. If x0 = 0, then (11.1.4)
reduces to
f (x) =
∞
!
n=0
f (n)(0)
n!
xn,
which is called the Maclaurin series expansion of f (x). Many of the familiar elementary
functions are analytic at all points. In particular, the Maclaurin expansions of ex, sin x,
and cos x are, respectively,
ex = 1 + x + 1
2!x2 + 1
3!x3 + · · · + 1
n!xn + · · · =
∞
!
n=0
1
n!xn,
sin x = x −1
3!x3 + 1
5!x5 −· · · +
(−1)n
(2n + 1)!x2n+1 + · · · =
∞
!
n=0
(−1)n
(2n + 1)!x2n+1,
cos x = 1 −1
2!x2 + 1
4!x4 −· · · + (−1)n
(2n)! x2n + · · · =
∞
!
n=0
(−1)n
(2n)! x2n.
We can determine many other analytic functions using the next theorem, whose
proof is omitted.
Theorem 11.1.6
If f (x) and g(x) are analytic at x = x0, then so also are f (x) ± g(x), f (x)g(x), and
f (x)
g(x) (provided that g(x0) ̸= 0).
Of particular importance to us throughout this chapter will be polynomial functions;
that is, functions of the form
p(x) = a0 + a1x + a2x2 + · · · + anxn,
(11.1.5)
where a0, a1, . . . , an are real numbers. Such a function is analytic at all points. In par-
ticular, (11.1.5) can be considered as the Maclaurin series expansion of p about x = 0.
Since the series has only a ﬁnite number of terms, it converges for all real x. Now suppose
that p(x) and q(x) are polynomials, and hence are analytic at all points. According to
Theorem 11.1.6, the rational function deﬁned by r(x) = p(x)
q(x) is analytic at all points
x = x0 such that q(x0) ̸= 0. However, Theorem 11.1.6 does not give us any indication

728
CHAPTER 11
Series Solutions to Linear Differential Equations
of the radius of convergence of the series representation of r(x). The next theorem deals
with this issue.
Theorem 11.1.7
If p(x) and q(x) are polynomials and q(x0) ̸= 0, then the power series representation of
p/q has radius of convergence R, where R is the distance, in the complex plane, from
x0 to the nearest root of q.
Once more, we omit the proof of this result; it can be found in texts on advanced calculus.
If z = a +ib is a root of q, then the distance from the center, x = x0, of the power series
to z is (see Figure 11.1.2)
|z −x0| =
$
(a −x0)2 + b2.
z2x0
Im(z)
b
x0
a
Re(z)
z 5 a 1 ib
Figure 11.1.2: Determining the radius of convergence of the power series representation of a
rational function centered at x = x0.
Example 11.1.8
Determine the radius of convergence of the power series representation of the function
f (x) = 1 −x
x2 −4
centered at (a) x = 0, (b) x = 1.
Solution:
Taking p(x) = 1 −x and q(x) = x2 −4, we have
f (x) = p(x)
q(x) ,
and the roots of q are x = ±2.
(a) In this case, the center of the power series is x = 0, so that the distance to the
nearest root of q is 2. (See Figure 11.1.3.) Consequently, the radius of convergence
of the power series representation of f centered at x = 0 is R = 2.
(b) If the center of the power series is x = 1, then the nearest root of q is at x = 2
(see Figure 11.1.3), and hence, the radius of convergence of the power series
representation of f is R = 1.
□
zeros of q
zeros of q
—2
0
2
—2
1
center
center
2
Figure 11.1.3: Determining the radius of convergence of the power series representation of the
function given in Example 11.1.8.

11.1
A Review of Power Series 729
Example 11.1.9
Determine the radius of convergence of the power series expansion of
f (x) =
1 −x
(x2 + 2x + 2)(x −2)
centered at x = 0.
Solution:
We take p(x) = 1 −x and q(x) = (x2 + 2x + 2)(x −2). According to
Theorem 11.1.7, the radius of convergence of the required power series will be given
by the distance from x = 0 to the nearest root of q. It is easily seen that the roots of q
are x1 = −1 + i, x2 = −1 −i, and x3 = 2. The corresponding distances from x = 0
are d1 =
√
2, d2 =
√
2, and d3 = 2, so the radius of convergence is R =
√
2. (See
Figure 11.1.4.)
□
1
2
21
21
d1
d2
21 1 i
21 2 i
d3
center
Re(z)
Im(z)
Figure 11.1.4: Determination of the radius of convergence of the power series representation
of the rational function given in Example 11.1.9.
Exercises for 11.1
Key Terms
Power series, Converge, Interval of convergence, Radius
of convergence, Ratio test, Analytic function, Analytic at
x = x0, Taylor series, Maclaurin series.
Skills
• Be able to determine the radius and interval of conver-
gence of a power series, using for example, the Ratio
Test or Theorem 11.1.2.
• Be able to determine all points at which a function is
analytic.
• Be familiar with the basic algebra and calculus of
power series.
• Know the relationship between the roots of a function
g(x) and the points of analyticity of a function of the
form f (x)
g(x) .
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The radius of convergence of the power series repre-
sentation of a function f (x) depends on the point x0
about which the power series is centered.
(b) If
∞
!
n=0
anxn and
∞
!
n=0
bnxn both converge at x = x1,
then so does
∞
!
n=0
(an + bn)xn.
(c) If
∞
!
n=0
anxn and
∞
!
n=0
bnxn both fail to converge at
x = x1, then so does
∞
!
n=0
(an + bn)xn.

730
CHAPTER 11
Series Solutions to Linear Differential Equations
(d) If the radius of convergence of the power series
∞
!
n=0
anxn is R, then the power series converges at
x = ±R, as well as |x| < R.
(e) The product of two analytic functions at x = x0 re-
mains analytic at x = x0.
(f) Every inﬁnitely differentiable function f can be rep-
resented by the formula
f (x) =
∞
!
n=0
f (n)(x0)
n!
(x −x0)n,
which holds for all real values of x in the domain
of f .
(g) Every polynomial has a power series representation
about any point x0 with an inﬁnite radius of conver-
gence.
(h) If f and g have power series representations centered
at x = 0 of radii R1 and R2, respectively, then the
product f g has a power series representation centered
at x = 0 of radius R, where R = min{R1, R2}.
(i) The coefﬁcient of xn in the product
∞
!
n=0
anxn
∞
!
n=0
bnxn
is anbn.
(j) A Maclaurin series is a Taylor series that is centered
at x = 0.
Problems
For Problems 1–6, determine the radius of convergence of
the given power series.
1.
∞
!
n=0
xn
22n .
2.
∞
!
n=0
(3x)n
53n .
3.
∞
!
n=0
xn
n2 .
4.
∞
!
n=0
2nxn
n
.
5.
∞
!
n=0
n!xn.
6.
∞
!
n=0
5nxn
n! .
For Problems 7–11, determine the radius of convergence of
the power series representation of the given function with
center x0.
7. f (x) = x2 −1
x + 2 ,
x0 = 0.
8. f (x) =
x
x2 + 1,
x0 = 0.
9. f (x) =
2x
x2 + 16,
x0 = 1.
10. f (x) =
x2 −3
x2 −2x + 5,
x0 = 0.
11. f (x) =
x
(x2 + 4x + 13)(x −3),
x0 = −1.
12. (a) Determine all values of x at which the function
f (x) =
1
x2 −1
(11.1.6)
is analytic.
(b) Determine the radius of convergence of a power
series representation of the function (11.1.6) cen-
tered at x = x0. (You will need to consider the
cases −1 < x0 < 1 and |x0| > 1 separately.)
13. By redeﬁning the ranges of the summations appearing
on the left-hand side, show that
∞
!
n=2
n(n −1)an−1xn−2 +
∞
!
n=1
nanxn−1
=
∞
!
n=0
(n + 1)(n + 3)an+1xn.
14. If f (x) =
∞
!
n=0
anxn, where the coefﬁcients in the ex-
pansion satisfy
∞
!
n=0
n(n + 2)anxn +
∞
!
n=1
(n −3)an−1xn = 0,
determine f (x).

11.2
Series Solutions about an Ordinary Point 731
15. Suppose it is known that the coefﬁcients in the
expansion
f (x) =
∞
!
n=0
anxn
satisfy
∞
!
n=0
(n + 2)an+1xn −
∞
!
n=0
anxn = 0.
Show that
f (x) = a0
x
∞
!
n=0
1
(n + 1)!xn+1,
and express this in terms of familiar elementary
functions.
16. If
∞
!
n=1
(n + 1)(n + 2)an+1xn −
∞
!
n=1
nan−1xn = 0,
show that for k = 1, 2, 3, . . . , we have
a2k = 1 · 3 · 5 · · · (2k −1)
(2k + 1)!
a0,
a2k+1 =
2k+1k!
(2k + 2)!a1.
11.2
Series Solutions about an Ordinary Point
We now consider the second-order linear homogeneous differential equation written in
standard form:
y′′ + p(x)y′ + q(x)y = 0.
Our aim is to determine a series representation of the general solution to this differential
equation centered at x = x0. We will see that the existence and form of the solution is
dependent on the behavior of the functions p and q at x = x0.
DEFINITION
11.2.1
The point x = x0 is called an ordinary point of the differential equation
y′′ + p(x)y′ + q(x)y = 0
(11.2.1)
if p and q are both analytic at x = x0. Any point that is not an ordinary point of
(11.2.1) is called a singular point of the differential equation.
Example 11.2.2
The differential equation
y′′ +
1
x2 −4 y′ +
1
x + 1 y = 0
has
p(x) =
1
x2 −4
and
q(x) =
1
x + 1.
We see by inspection that the only points at which p fails to be analytic are x = ±2,
whereas q is analytic at all points except x = −1. Consequently, the only singular points
of the differential equation are x = −1, ±2. All other points are ordinary points.
□
In this section, we restrict our attention to ordinary points. Since the functions p and
q are analytic at an ordinary point, we might suspect that any solution to (11.2.1) valid at
x = x0 is also analytic there and hence can be represented as a convergent power series
y(x) =
∞
!
n=0
an(x −x0)n,

732
CHAPTER 11
Series Solutions to Linear Differential Equations
for appropriate constants an. This is indeed the case, but before stating the general result,
we consider a familiar example.
Example 11.2.3
Determine two linearly independent power series solutions to the differential equation
y′′ + y = 0
(11.2.2)
centered at x = 0. Identify the solutions in terms of familiar elementary functions.
Solution:
Since x = 0 is an ordinary point of the differential equation, we try for a
power series solution of the form
y(x) =
∞
!
n=0
anxn.
(11.2.3)
We proceed in a similar manner to the method of undetermined coefﬁcients by substi-
tuting (11.2.3) into (11.2.2) and determining the values of the an such that (11.2.3) is
indeed a solution. Differentiating (11.2.3) twice with respect to x yields
y′(x) =
∞
!
n=1
nanxn−1,
y′′(x) =
∞
!
n=2
n(n −1)anxn−2,
where we have shifted the starting point on the summations without loss of generality.
Substituting into (11.2.2), it follows that (11.2.3) does deﬁne a solution, provided that
∞
!
n=2
n(n −1)anxn−2 +
∞
!
n=0
anxn = 0.
If we replace n by k + 2 in the ﬁrst summation, and replace n by k in the second
summation, the result is
∞
!
k=0
(k + 2)(k + 1)ak+2xk +
∞
!
k=0
akxk = 0.
Combining the summations yields
∞
!
k=0
[(k + 2)(k + 1)ak+2 + ak]xk = 0.
This implies that the coefﬁcients of xk must vanish for k = 0, 1, 2, . . . . Consequently,
we obtain the recurrence relation
(k + 2)(k + 1)ak+2 + ak = 0,
k = 0, 1, 2, . . . .
Since (k +2)(k +1) is never zero, we can write this recurrence relation in the equivalent
form
ak+2 = −
1
(k + 2)(k + 1)ak,
k = 0, 1, 2, . . . .
(11.2.4)
We now use this relation to determine the appropriate values of the coefﬁcients. It is
convenient to consider separately the two cases (1) k is even and (2) k is odd.

11.2
Series Solutions about an Ordinary Point 733
Case 1: k is even. Substituting successively into (11.2.4), we obtain the coefﬁcients as
follows:
When k = 0, we have
a2 = −1
2a0.
When k = 2, we have
a4 = −1
4 · 3a2 =
1
4 · 3 · 2a0.
That is,
a4 = 1
4!a0.
When k = 4, we have
a6 = −1
6 · 5a4,
so that
a6 = −1
6!a0.
Continuing in this manner, we soon recognize the pattern that is emerging; namely,
a2n = (−1)n
(2n)! a0.
(11.2.5)
Thus, all of the even coefﬁcients are determined in terms of a0, but a0 itself is arbitrary.
Case 2: k is odd. Now consider the recurrence relation (11.2.4) when k is an odd
positive integer:
When k = 1, we have
a3 = −1
2 · 3a1 = −1
3!a1.
When k = 3, we have
a5 = −1
4 · 5a3 = 1
5!a1.
When k = 5, we have
a7 = −1
6 · 7a5 = −1
7!a1.
Continuing in this manner, we see that the general odd coefﬁcient is
a2n+1 =
(−1)n
(2n + 1)!a1.
(11.2.6)
Thus, using Equations (11.2.5) and (11.2.6), we have shown that for all values of a0, a1,
a solution to the given differential equation is
y(x) = a0
%
1 −1
2!x2 + 1
4!x4 −· · ·
&
+ a1
%
x −1
3!x3 + 1
5!x5 −· · ·
&
.

734
CHAPTER 11
Series Solutions to Linear Differential Equations
That is,
y(x) = a0
∞
!
n=0
(−1)2n
(2n)! x2n + a1
∞
!
n=0
(−1)n
(2n + 1)!x2n+1.
(11.2.7)
Setting a0 = 1 and a1 = 0 yields the solution
y1(x) =
∞
!
n=0
(−1)2n
(2n)! x2n,
whereas setting a0 = 0 and a1 = 1 yields the solution
y2(x) =
∞
!
n=0
(−1)n
(2n + 1)!x2n+1.
Applying the ratio test, it is straightforward to show that both of the foregoing series
converge for all real x. Finally, since y1 and y2 are not proportional, they are linearly
independent on any interval. It follows that (11.2.7) is the general solution of the given
differential equation. Indeed, the power series representing y1 is just the Maclaurin series
expansion of cos x, whereas the series deﬁning y2 is the Maclaurin series expansion of
sin x. Thus, we can write (11.2.7) in the more familiar form
y(x) = a0 cos x + a1 sin x.
□
The solution of the previous example consisted of the following four steps.
1. Assume that a power series solution of the form y(x) =
∞
!
n=0
anxn exists.
2. Determine the values of the coefﬁcients, an, such that y is a formal solution of the
differential equation. This led to two distinct solutions, one determined in terms
of the constant a0, and the other in terms of the constant a1.
3. Use the ratio test to determine the radius of convergence of the solutions, and
hence, the interval over which the solutions are valid.
4. Check that the solutions are linearly independent on the interval of existence.
The next theorem justiﬁes the preceding steps and shows that the technique can be
applied about any ordinary point of a differential equation.
Theorem 11.2.4
Let p and q be analytic at x = x0, and suppose that their power series expansions are
valid for |x −x0| < R. Then the general solution to the differential equation
y′′ + p(x)y′ + q(x)y = 0
(11.2.8)
can be represented as a power series centered at x = x0, with radius of convergence at
least R. The coefﬁcients in this series solution can be determined in terms of a0 and a1
by directly substituting y(x) =
∞
!
n=0
an(x −x0)n into (11.2.8). The resulting solution is
of the form
y(x) = a0y1(x) + a1y2(x),
where y1 and y2 are linearly independent solutions to (11.2.8) on the interval of existence.
If the initial conditions y(x0) = α, y′(x0) = β are imposed, then a0 = α, a1 = β.

11.2
Series Solutions about an Ordinary Point 735
Idea Behind Proof We outline the steps required to prove this theorem but do not
give details. The ﬁrst step is to expand p and q in a power series centered at x = x0.
Then we assume a solution exists of the form
y(x) =
∞
!
n=0
an(x −x0)n
and substitute this into the differential equation. Upon collecting the coefﬁcients of like
powers of x −x0, a recurrence relation is obtained, and it can be shown that this relation
determines all of the coefﬁcients in terms of a0 and a1. These steps are computationally
tedious, but quite straightforward. The hard part is to show that the power series solution
that has been obtained has a radius of convergence at least equal to R. This requires
some ideas from advanced calculus. Having determined a power series solution, y1 and
y2 arise as the special cases a0 = 1, a1 = 0, and a0 = 0, a1 = 1, respectively. The
Wronskian of these functions satisﬁes W[y1, y2](x0) = 1, so that they are linearly inde-
pendent on their interval of existence. Finally, it is easy to show that y(x0) = a0 and that
y′(x0) = a1.
We now illustrate the use of the above theorem with some examples.
Example 11.2.5
Show that
(1 + x2)y′′ + 3xy′ + y = 0
(11.2.9)
has two linearly independent series solutions centered at x = 0, and determine a lower
bound on the radius of convergence of these solutions.
Solution:
We ﬁrst rewrite (11.2.9) in the standard form
y′′ +
3x
1 + x2 y′ +
1
1 + x2 y = 0,
from which we conclude that x = 0 is an ordinary point, and hence, (11.2.9) does indeed
have two linearly independent series solutions centered at x = 0. In this case,
p(x) =
3x
1 + x2
and
q(x) =
1
1 + x2 .
According to Theorem 11.2.4, the radius of convergence of the power series solutions
will be at least equal to the smaller of the radii of convergence of the power series repre-
sentations of p and q. Using Theorem 11.1.7, we see directly that the series expansions
of both p and q about x = 0 have radius of convergence R = 1, so that a lower bound
on the radius of convergence of the power series solutions to (11.2.9) is also R = 1.
□
Example 11.2.6
Determine two linearly independent series solutions in powers of x to
y′′ −2xy′ −4y = 0,
(11.2.10)
and ﬁnd the radius of convergence of these solutions.
Solution:
The point x = 0 is an ordinary point of the differential equation, and
therefore Theorem 11.2.4 can be applied with x0 = 0. In this case, we have
p(x) = −2x,
q(x) = −4.

736
CHAPTER 11
Series Solutions to Linear Differential Equations
Since these are both polynomials, their power series expansions about x = 0 are valid for
all x, and hence, from Theorem 11.2.4, the general solution to (11.2.10) can be expressed
in the form
y(x) =
∞
!
n=0
anxn,
(11.2.11)
and this power series solution will converge for all real x. Differentiating (11.2.11) we
obtain
y′(x) =
∞
!
n=1
nanxn−1,
y′′(x) =
∞
!
n=2
n(n −1)anxn−2.
Substitution into (11.2.10) yields
∞
!
n=2
n(n −1)anxn−2 −2
∞
!
n=1
nanxn −4
∞
!
n=0
anxn = 0.
We now redeﬁne the ranges in the summation in order to obtain a common xk in all
terms. This is accomplished by replacing n with k + 2 in the ﬁrst summation, and, for
consistency in notation, we replace n with k in the other summations. The result is
∞
!
k=0
'
(k + 2)(k + 1)ak+2 −2kak −4ak
(
xk = 0.
This equation requires that the coefﬁcient of xk vanish, and hence, we obtain the recur-
rence relation
(k + 2)(k + 1)ak+2 −2kak −4ak = 0,
k = 0, 1, 2, . . . ,
which can be written in the equivalent form
ak+2 =
2(k + 2)
(k + 1)(k + 2)ak,
k = 0, 1, 2, . . . ;
that is,
ak+2 =
2
k + 1ak,
k = 0, 1, 2, . . . .
(11.2.12)
We see from this relation that, as in Example 11.2.3, all of the even coefﬁcients can be
expressed in terms of a0, whereas all of the odd coefﬁcients can be expressed in terms
of a1. We now determine the exact form of these coefﬁcients.
Case 1: k is even. From (11.2.12), we have the following:
When k = 0,
a2 = 2a0.
When k = 2,
a4 = 2
3a2 = 22
3 a0.
When k = 4,
a6 = 2
5a4 =
23
1 · 3 · 5a0.
The general even coefﬁcient is thus
a2n =
2n
1 · 3 · 5 · (2n −1)a0.

11.2
Series Solutions about an Ordinary Point 737
Case 2: k is odd. Substituting successively into (11.2.12) yields the following:
When k = 1,
a3 = a1.
When k = 3,
a5 = 2
4a3 =
1
1 · 2a1.
When k = 5,
a7 = 2
6a5 =
1
1 · 2 · 3a1.
When k = 7,
a9 = 2
8a7 =
1
1 · 2 · 3 · 4a1.
The general odd coefﬁcient can therefore be written as
a2n+1 = 1
n!a1.
Substituting back into (11.2.11), we obtain the solution
y(x) = a0
)
1 + 2x2 + 22
1 · 3x4 +
23
1 · 3 · 5x6 + · · · +
2n
1 · 3 · 5 · · · (2n −1)x2n + · · ·
*
+ a1
)
x + x3 + 1
2!x5 + 1
3!x7 + · · · + 1
n!x2n+1 + · · ·
*
.
That is,
y(x) = a0
+
1 +
∞
!
n=1
2n
1 · 3 · 5 · · · (2n −1)x2n
,
+ a1
+ ∞
!
n=0
1
n!x2n+1
,
.
Consequently, from Theorem 11.2.4, two linearly independent solutions to (11.2.10) on
(−∞, ∞) are
y1(x) = 1 +
∞
!
n=1
2n
1 · 3 · · · · (2n −1)x2n,
y2(x) =
∞
!
n=0
1
n!x2n+1.
□
In Examples 11.2.3 and 11.2.6, we were able to solve the recurrence relation that
arose from the power series technique. In general, this will not be possible, and hence,
we must be satisﬁed with obtaining just a ﬁnite number of terms of each power series
solution.
Example 11.2.7
Determine the terms up to x5 in each of the two linearly independent power series
solutions to
y′′ + (2 −4x2)y′ −8xy = 0
centered at x = 0. Also ﬁnd the radius of convergence of these solutions.
Solution:
The functions p(x) = 2 −4x2 and q(x) = −8x are polynomials, and
hence, from Theorem 11.2.4, the power series solutions will converge for all real x. We
now determine the solutions. Substituting
y(x) =
∞
!
n=0
anxn
(11.2.13)

738
CHAPTER 11
Series Solutions to Linear Differential Equations
into the given differential equation yields
∞
!
n=2
n(n −1)anxn−2 + 2
∞
!
n=1
nanxn−1 −4
∞
!
n=1
nanxn+1 −8
∞
!
n=0
anxn+1 = 0.
Replacing n by k + 2 in the ﬁrst summation, k + 1 in the second summation, and k −1
in the third and fourth summations, we obtain
∞
!
k=0
(k + 2)(k + 1)ak+2xk+2
∞
!
k=0
(k + 1)ak+1xk
−4
∞
!
k=2
(k −1)ak−1xk −8
∞
!
k=1
ak−1xk = 0.
Separating out the terms corresponding to k = 0 and k = 1, it follows that this can be
written as
(2a2 + 2a1) + (6a3 + 4a2 −8a0)x
+
∞
!
k=2
-
(k + 2)(k + 1)ak+2 + 2(k + 1)ak+1 −[4(k −1) + 8]ak−1
.
xk = 0.
Setting the coefﬁcients of all powers xk to zero yields the following:
For k = 0 and k = 1, we have
2a2 + 2a1 = 0
and
6a3 + 4a2 −8a0 = 0,
(11.2.14)
and for k ≥2, we have
(k + 2)(k + 1)ak+2 + 2(k + 1)ak+1 −[4(k −1) + 8]ak−1 = 0.
(11.2.15)
It follows from (11.2.14) that
a2 = −a1,
a3 = 2
3(2a0 + a1),
(11.2.16)
and (11.2.15) yields the general recurrence relation
ak+2 = 4ak−1 −2ak+1
k + 2
,
k = 2, 3, 4, . . .
(11.2.17)
In this case, the recurrence relation is quite difﬁcult to solve. However, we were only
asked to determine terms up to x5 in the series solution, and so we proceed to do so. We
already have a2 and a3 expressed in terms of a0 and a1. Setting k = 2 in (11.2.17) yields
a4 = 1
4(4a1 −2a3) = 1
4
)
4a1 −4
3(2a0 + a1)
*
,
where we have substituted from (11.2.16) for a3. Simplifying this expression, we obtain
a4 = 2
3(a1 −a0).
We still require one more term. Setting k = 3 in (11.2.17) yields
a5 = 1
5(4a2 −2a4) = 1
5
)
−4a1 −4
3(a1 −a0)
*
,
so that
a5 = 4
15(a0 −4a1).

11.2
Series Solutions about an Ordinary Point 739
Substituting for the coefﬁcients a2, a3, a4, and a5 into (11.2.13), we obtain
y(x) = a0 + a1x −a1x2 + 2
3(2a0 + a1)x3 + 2
3(a1 −a0)x4 + 4
15(a0 −4a1)x5 + · · · .
That is,
y(x) = a0
%
1+ 4
3x3 −2
3x4 + 4
15x5 +· · ·
&
+a1
%
x −x2 + 2
3x3 + 2
3x4 −16
15x5 +· · ·
&
.
Thus, two linearly independent solutions to the given differential equation on (−∞, ∞)
are
y1(x) = 1 + 4
3x3 −2
3x4 + 4
15x5 + · · ·
and
y2(x) = x −x2 + 2
3x3 + 2
3x4 −16
15x5 + · · · .
□
Exercises for 11.2
Key Terms
Ordinary point, Singular point.
Skills
• Be able to decide if a given point x = x0 is an ordinary
point or a singular point of a differential equation of
the form (11.2.1).
• Be able to determine two linearly independent power
series solutions to a differential equation of the form
(11.2.1) centered at an ordinary point x = x0.
• In cases where the recurrence relation for the coefﬁ-
cients of a power series solution to a differential equa-
tion cannot be explicitly solved, be able to obtain a
speciﬁed ﬁnite number of terms of two linearly inde-
pendent power series solutions.
• Be able to determine a lower bound on the radius of
convergence of a power series solution to a differential
equation.
• Be able to determine the general solution to a differ-
ential equation of the form (11.2.1) as a linear combi-
nation of two linearly independent power series solu-
tions, and be able to ﬁnd its radius of convergence.
• Be able to solve initial-value problems via the tech-
nique of power series solutions.
True-False Review
For Questions (a)–(j), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) If p(x) and q(x) are polynomials, then the differential
equation y′′ + p(x)y′ + q(x)y = 0 has no singular
points.
(b) The radius of convergence of a power series solution
to the differential equation y′′ + 1
x y′ +
1
x + 1 y = 0
centered at x = −3 is at most 2.
(c) The radius of convergence of a power series solution to
the differential equation y′′+
1
x2 −1 y′+
1
x + 2 y = 0
centered at x = 2 is at least 2.
(d) The coefﬁcients a0 and a1 in the power series solution
to an initial-value problem y′′ + p(x)y′ + q(x)y = 0,
where p(x) and q(x) are analytic at x = x0, are the
values y(x0) and y′(x0), respectively.
(e) A power series solution to y′′ + p(x)y′ + q(x) = 0
centered at an ordinary point x = x0 always exists and
has a positive radius of convergence.
(f) Two linearly independent power series solutions to the
differential equation y′′ + p(x)y′ +q(x)y = 0 cannot
contain any of the same powers of x.
(g) If
∞
!
n=0
anxn is a power series solution to the differen-
tial equation y′′ + p(x)y′ + q(x)y = 0, then so is
∞
!
n=0
anxn+1.

740
CHAPTER 11
Series Solutions to Linear Differential Equations
(h) The recurrence relation ak = ak−2 has a unique solu-
tion a0, a1, a2, a3, . . . provided that the value of a0 is
speciﬁed.
(i) The recurrence relation ak = 3ak−1 −2ak−3 has a
unique solution a0, a1, a2, a3, . . . provided that the
values of a0, a1, and a2 are speciﬁed.
(j) If the recurrence relation arising in the power series
method of solution of a differential equation can-
not be solved, then the differential equation has no
solution.
Problems
ForProblems1–8,determinetwolinearlyindependentpower
series solutions to the given differential equation centered at
x = 0. Also determine the radius of convergence of the series
solutions.
1. y′′ −y = 0.
2. y′′ + 2xy′ + 4y = 0.
3. y′′ −2xy′ −2y = 0.
4. y′′ −x2y′ −2xy = 0.
5. y′′ + xy = 0.
6. y′′ + xy′ + 3y = 0.
7. y′′ −x2y′ −3xy = 0.
8. y′′ + 2x2y′ + 2xy = 0.
For Problems 9–12, determine two linearly independent
power series solutions to the given differential equation cen-
tered at x = 0. Give a lower bound on the radius of conver-
gence of the series solutions obtained.
9. (x2 −3)y′′ −3xy′ −5y = 0.
10. (1 + x2)y′′ + 4xy′ + 2y = 0.
11. (1 −4x2)y′′ −20xy′ −16y = 0.
12. (x2 −1)y′′ −6xy′ + 12y = 0.
For Problems 13–16, determine terms up to and including
x5 in two linearly independent power series solutions of the
given differential equation. State the radius of convergence
of the series solutions.
13. y′′ + 2y′ + 4xy = 0.
14. y′′ + xy′ + (2 + x)y = 0.
15. y′′−ex y = 0. [Hint: ex = 1+x + 1
2!x2+ 1
3!x3+· · · .]
16. y′′ + (sin x)y′ + y = 0.
17. Consider the differential equation
xy′′ −(x −1)y′ −xy = 0.
(11.2.18)
(a) Is x = 0 an ordinary point?
(b) Determine the ﬁrst three nonzero terms in each of
two linearly independent series solutions to Equa-
tion (11.2.18) centered at x = 1.
[Hint: Make the change of variables z = x −1
and obtain a series solution in powers of z.]
Give a lower bound on the radius of convergence
of each of your solutions.
18. Determine a series solution to the initial-value
problem
(1 + 2x2)y′′ + 7xy′ + 2y = 0,
y(0) = 0,
y′(0) = 1.
19. (a) Determine a series solution to the initial-value
problem
4y′′+xy′+4y = 0,
y(0) = 1,
y′(0) = 0.
(11.2.19)
(b) Find a polynomial that approximates the solution
to Equation (11.2.19) with an error less than 10−5
on the interval [−1, 1].
[Hint: The series obtained is a convergent alter-
nating series.]
The power series technique can also be used to solve nonho-
mogeneous differential equations of the form
y′′ + p(x)y′ + q(x)y = r(x),
provided that p, q, andr are analytic at the point about which
we are expanding. For Problems 20–21, determine terms up
to x6 in the power series representation of the general so-
lution to the given differential equation centered at x = 0.
Identify those terms in your solution that correspond to the
complementary function and those that correspond to a par-
ticular solution to the differential equation.
20. y′′ + 2x2y′ + xy = 2 cos x.
21. y′′ + xy′ −4y = 6ex.

11.3
The Legendre Equation 741
11.3
The Legendre Equation
Thereareseverallineardifferentialequationsthatarisefrequentlyinappliedmathematics
and whose solutions can only be obtained using a power series technique. Among the
most important of these are the following:
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0,
(Legendre Equation)
y′′ −2xy′ + 2αy = 0,
(Hermite Equation)
(1 −x2)y′′ −xy′ + α2y = 0,
(Chebyshev Equation)
where α is an arbitrary constant. Since x = 0 is an ordinary point of these equations, we
can obtain a series solution in powers of x. We will consider only the Legendre equation
and leave the analysis of the remaining equations for the exercises.
The Legendre equation is
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0,
(11.3.1)
where α is an arbitrary constant. To determine a lower bound on the radius of convergence
of the series solution about x = 0 to this equation, we divide by 1 −x2 to obtain
y′′ −
2x
1 −x2 y′ + α(α + 1)
1 −x2 y = 0.
Since the power series expansion of
1
1 −x2 about x = 0 is valid for |x| < 1, it follows
that a lower bound on the radius of convergence of the power series solutions to Equation
(11.3.1) about x = 0 is 1. We now determine the series solutions. Substituting
y(x) =
∞
!
n=0
anxn
into Equation (11.3.1) yields
∞
!
n=2
n(n −1)anxn−2 −
∞
!
n=2
n(n −1)anxn −2
∞
!
n=1
nanxn +
∞
!
n=0
α(α + 1)anxn = 0.
That is, upon redeﬁning the ranges of the summations,
∞
!
n=0
'
(n + 2)(n + 1)an+2 −n(n −1)an −2nan + α(α + 1)an
(
xn = 0.
Thus, we obtain the recurrence relation
an+2 = n(n + 1) −α(α + 1)
(n + 1)(n + 2)
an,
n = 0, 1, 2, . . . ,
(11.3.2)
which can be written as
an+2 = −(α −n)(α + n + 1)
(n + 1)(n + 2)
an,
n = 0, 1, 2, . . . .
Even values of n:
n = 0 (⇒a2 = −α(α + 1)
2
a0,
n = 2 (⇒a4 = −(α −2)(α + 3)
3 · 4
a2 = (α −2)α(α + 1)(α + 3)
4!
a0,
n = 4 (⇒a6 = −(α −4)(α + 5)
5 · 6
a4 = −(α −4)(α −2)α(α + 1)(α + 3)(α + 5)
6!
a0.

742
CHAPTER 11
Series Solutions to Linear Differential Equations
In general, for k = 1, 2, 3 . . . , we have
a2k = (−1)k (α −2k + 2)(α −2k + 4) · · · (α −2)α(α + 1) · · · (α + 2k −1)
(2k)!
a0.
Odd values of n:
n = 1 (⇒a3 = −(α −1)(α + 2)
2 · 3
a1,
n = 3 (⇒a5 = −(α −3)(α + 4)
4 · 5
a3 = (α −3)(α −1)(α + 2)(α + 4)
5!
a1,
n = 5 (⇒a7 = −(α −5)(α + 6)
6 · 7
a5
= −(α −5)(α −3)(α −1)(α + 2)(α + 4)(α + 6)
7!
a1.
In general, for k = 1, 2, 3, . . . , we have
a2k+1 = (−1)k (α −2k + 1) · · · (α −3)(α −1)(α + 2)(α + 4) · · · (α + 2k)
(2k + 1)!
a1.
Consequently, for a0 ̸= 0 and a1 ̸= 0, two linearly independent solutions to the
Legendre equation are
y1(x) = a0
)
1 −α(α + 1)
2
x2 + (α −2)α(α + 1)(α + 3)
4!
x4
−(α −4)(α −2)α(α + 1)(α + 3)(α + 5)
6!
x6 + · · ·
*
(11.3.3)
and
y2(x) = a1
)
x −(α −1)(α + 2)
2!
x3 + (α −3)(α −1)(α + 2)(α + 4)
5!
x5 + · · ·
*
,
(11.3.4)
and both of these solutions are valid for |x| < 1.
The Legendre Polynomials
Of particular importance in applications is the special case of the Legendre equation
(1 −x2)y′′ −2xy′ + N(N + 1)y = 0
in which N is a nonnegative integer. In this case, the recurrence relation (11.3.2) is
an+2 = n(n + 1) −N(N + 1)
(n + 1)(n + 2)
an,
n = 0, 1, 2, . . . ,
which implies that
aN+2 = aN+4 = · · · = 0.

11.3
The Legendre Equation 743
Consequently, one of the solutions (11.3.3) or (11.3.4) to Legendre’s equation, depend-
ing on whether N is even or odd, is a polynomial of degree N. (Notice that such a
solution converges for all x, and hence, we have a radius of convergence greater than
that guaranteed by Theorem 11.2.4.)
DEFINITION
11.3.1
Let N be a nonnegative integer. The Legendre polynomial of degree N, denoted
PN(x), is deﬁned to be the polynomial solution to
(1 −x2)y′′ −2xy′ + N(N + 1)y = 0,
which has been normalized so that PN(1) = 1.
Example 11.3.2
Determine P0, P1, and P2.
Solution:
Substituting α = N into (11.3.3) and (11.3.4) yields
N = 0: y1(x) = a0, which implies that P0(x) = 1;
N = 1: y2(x) = a1x, which implies that P1(x) = x;
N = 2: y1(x) = a0(1 −3x2), and imposing the normalizing condition y1(1) = 1, we
require that a0 = −1
2, so that P2(x) = 1
2(3x2 −1).
□
In general, it is tedious to determine PN(x) directly from (11.3.3) and (11.3.4), and
various other methods have been derived. Among the most useful are the following:
Rodrigues’ Formula
PN(x) =
1
2N N!
d N
dx N (x2 −1)N,
N = 0, 1, 2, . . . .
Recurrence Relation
PN+1(x) = (2N + 1)x PN(x) −N PN−1(x)
N + 1
,
N = 1, 2, 3, . . . .
We can use Rodrigues’ formula to obtain PN directly. Alternatively, starting with
P0 and P1, we can use the recurrence relation to generate all PN.
Example 11.3.3
According to Rodrigues’ formula,
P2(x) = 1
8
d2
dx2 (x2 −1)2 = 1
8
d
dx [4x(x2 −1)] = 1
2(3x2 −1),
which does indeed coincide with that given in the previous example.
□

744
CHAPTER 11
Series Solutions to Linear Differential Equations
The ﬁrst ﬁve Legendre polynomials are given in Table 11.3.1.
N
Legendre Polynomial of Degree N
0
P0(x) = 1
1
P1(x) = x
2
P2(x) = 1
2(3x2 −1)
3
P3(x) = 1
2 x(5x2 −3)
4
P4(x) = 1
8(35x4 −30x2 + 3)
Table 11.3.1: The First Five Legendre Polynomials
Orthogonality of the Legendre Polynomials
In Section 5.1, we deﬁned an inner product on the vector space C0[a, b] by
⟨f, g⟩=
/ b
a
f (x)g(x)dx,
for all f and g in C0[a, b]. We now show that the Legendre polynomials are orthogonal
relative to the above inner product on the interval [−1, 1].
Theorem 11.3.4
The set of Legendre polynomials {P0, P1, P2, . . . } is an orthogonal set of functions on
the interval [−1, 1]. That is,
/ 1
−1
PM(x)PN(x)dx = 0
whenever M ̸= N.
Proof Using the product rule for differentiation, we see easily that Legendre’s equation
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0
can be written in the form
[(1 −x2)y′]′ + α(α + 1)y = 0.
Consequently, the Legendre polynomials PN(x) and PM(x) satisfy
[(1 −x2)P′
N]′ + N(N + 1)PN = 0,
(11.3.5)
[(1 −x2)P′
M]′ + M(M + 1)PM = 0,
(11.3.6)
respectively. Multiplying Equation (11.3.5) by PM and Equation (11.3.6) by PN and
subtracting yields
[(1 −x2)P′
N]′PM −[(1 −x2)P′
M]′PN + [N(N + 1) −M(M + 1)]PM PN = 0,

11.3
The Legendre Equation 745
which can be written as
01
(1 −x2)P′
N PM
2′
−(1 −x2)P′
N P′
M
3
−
01
(1 −x2)P′
M PN
2′
−(1 −x2)P′
N P′
M
3
+
1
N(N + 1) −M(M + 1)
2
PM PN = 0.
That is,
1
(1 −x2)(P′
N PM −P′
M PN)
2′
+
1
N(N + 1) −M(M + 1)
2
PM PN = 0.
Integrating over the interval [−1, 1], we obtain
1
(1 −x2)(P′
N PM −P′
M PN)
21
−1+
1
N(N + 1) −M(M + 1)
2 / 1
−1
PM(x)PN(x)dx = 0.
The ﬁrst term vanishes at x = ±1, and the term multiplying the integral can be factorized
to yield
(N −M)(N + M + 1)
/ 1
−1
PM(x)PN(x)dx = 0.
Since M and N are nonnegative integers, the previous formula implies that
/ 1
−1
PM(x)PN(x)dx = 0
whenever M ̸= N.
It can also be shown, although it is more difﬁcult (see N.N. Lebedev, Special Func-
tions and their Applications, Dover, 1972), that
/ 1
−1
P2
N(x)dx =
2
2N + 1.
(11.3.7)
Consequently,
04
2N+1
2
PN(x)
3
is an orthonormal set of polynomials on [−1, 1].
Since the set of Legendre polynomials {P0, P1, . . . , PN} is linearly independent on
any interval,4 it is a basis for the vector space of all polynomials of degree less than or
equal to N. Thus, if p(x) is any polynomial, there exist scalars a0, a1, . . . , aN such that
p(x) =
N
!
k=0
ak Pk(x).
(11.3.8)
We can use orthogonality of the Legendre polynomials to determine the coefﬁcients ak in
this expansion as follows. Multiplying (11.3.8) by Pj(x) for 0 ≤j ̸= N, and integrating
over the interval [−1, 1] yields
/ 1
−1
p(x)Pj(x)dx =
/ 1
−1
N
!
k=0
ak Pk(x)Pj(x)dx =
N
!
k=0
ak
/ 1
−1
Pk(x)Pj(x)dx.
4This is true, for example, from the fact that the polynomials in the set each have a different degree.

746
CHAPTER 11
Series Solutions to Linear Differential Equations
However, due to the orthogonality of the Legendre polynomials, all of the terms in the
summation with k ̸= j vanish, so that
/ 1
−1
p(x)Pj(x)dx = a j
/ 1
−1
Pj(x)Pj(x)dx.
Consequently, using (11.3.7), we obtain
/ 1
−1
p(x)Pj(x)dx =
2
2 j + 1a j,
which implies that
a j = 2 j + 1
2
/ 1
−1
p(x)Pj(x)dx.
(11.3.9)
Example 11.3.5
Expand f (x) = x2 −x + 2 as a series of Legendre polynomials.
Solution:
Since f (x) has degree 2, we can write
x2 −x + 2 = a0P0 + a1P1 + a2P2,
where, from (11.3.9), the coefﬁcients are given by
a j = 2 j + 1
2
/ 1
−1
(x2 −x + 2)Pj(x)dx.
From Table 11.3.1,
P0(x) = 1,
P1(x) = x,
P2(x) = 1
2(3x2 −1),
so that
a0 = 1
2
/ 1
−1
(x2 −x + 2)dx = 7
3,
a1 = 3
2
/ 1
−1
(x2 −x + 2)xdx = −1,
a2 = 5
2
/ 1
−1
1
2(x2 −x + 2)(3x2 −1)dx = 2
3.
Consequently,
x2 −x + 2 = 7
3 P0 −P1 + 2
3 P2.
□
More generally, the following expansion theorem plays a fundamental role in many
applications of mathematics to physics, chemistry, engineering, and so on.
Theorem 11.3.6
Let f and f ′ be continuous on the interval (−1, 1). Then, for −1 < x < 1,
f (x) = a0P0(x) + a1P1(x) + · · · + an Pn(x) + · · · =
∞
!
n=0
an Pn(x),
(11.3.10)
where
an = 2n + 1
2
/ 1
−1
f (x)Pn(x)dx.
(11.3.11)

11.3
The Legendre Equation 747
Proof Establishing the existence of a convergent series of the form (11.3.10) is best
left for a course on Fourier analysis or partial differential equations. The derivation that
the coefﬁcients in such an expansion must be given by (11.3.11) follows similar steps to
those leading to Equation (11.3.9) and is left as an exercise.
Example 11.3.7
Determine the terms leading up to and including P3(x) in the Legendre series expansion
of f (x) = sin πx, −1 < x < 1.
Solution:
Accordingtotheprevioustheorem,thegivenfunctiondoeshaveaLegendre
series expansion with coefﬁcients given by
an = 2n + 1
2
/ 1
−1
sin(πx)Pn(x)dx.
Thus,
a0 = 1
2
/ 1
−1
sin(πx)dx = 0,
a1 = 3
2
/ 1
−1
x sin(πx)dx = 3
π ,
a2 = 5
2
/ 1
−1
1
2(3x2 −1) sin(πx)dx = 0,
a3 = 7
2
/ 1
−1
1
2 x(5x2 −3) sin(πx)dx = 7
π3 (π2 −15).
Consequently, Theorem 11.3.6 implies that, for −1 < x < 1,
sin πx = 3
x P1(x) + 7
π3 (π2 −15)P3(x) + · · · .
(11.3.12)
To illustrate how good this approximation is, in Figure 11.3.1 we have sketched the
functions
f (x) = sin πx
and
g(x) = 3
π P1(x) + 7
π3 (π2 −15)P3(x).
0.5
0.5
1
1
—0.5
—0.5
—1
—1
x
f(x), g(x)
Figure 11.3.1: Comparison of f (x) = sin πx and its Legendre polynomial expansion of
degree 3.
For comparison, in Figure 11.3.2 we sketch f (x) and the ﬁfth-order Taylor approx-
imation
h(x) = πx −1
3!(πx)3 + 1
5!(πx)5.

748
CHAPTER 11
Series Solutions to Linear Differential Equations
1
—1
—1
1
—0.5
—0.5
0.5
0.5
x
f(x), h(x)
Figure 11.3.2: Comparison of f (x) = sin πx and its Taylor polynomial expansion of degree 5.
In Figure 11.3.3, we sketch f (x) together with the Legendre polynomial approximation
k(x) = 3
π P1(x) + 7
π3 (π2 −15)P3(x) + 11
π5 (945 −105π2 + π4)P5(x)
that arises when we include the next nonzero term in (11.3.12). We see that k(x) gives
an excellent approximation to f (x) = sin πx at all points in [−1, 1] (including the
endpoints).
□
1
—1
—1
1
0.5
—0.5
—0.5
0.5
x
f(x), k(x)
Figure 11.3.3: Comparison of f (x) = sin πx and its Legendre polynomial expansion of
degree 5.
Remark
ComputationbyhandofthecoefﬁcientsinaLegendrepolynomialexpansion
can be very tedious. However, computer algebra systems, such as Maple or Mathematica,
have the Legendre polynomials as built-in functions, and therefore can be useful in
computing the coefﬁcients. For example, in Maple the command P(n, x) generates
PN(x).
Exercises for 11.3
Key Terms
Legendre equation, Legendre polynomial of degree N,
Rodrigues’ formula.
Skills
• Be able to compute the Legendre polynomials PN(x)
directly for small n, either from (11.3.3) and (11.3.4),
or from Rodrigues’ formula, or from the recurrence
relation for the Legendre polynomials.
• Provided that a function f has a continuous derivative
on (−1, 1), be able to use the orthogonality of the Leg-
endre polynomials to expand f as a linear combination
of Legendre polynomials.

11.3
The Legendre Equation 749
True-False Review
For Questions (a)–(d), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The radius of convergence of the power series solu-
tions to Legendre’s equation about x = 0 is 1.
(b) The Legendre polynomials deﬁne an orthogonal basis
for the vector space Pn(R).
(c) For an integer value of α, one (but not both) of the
formulas (11.3.3) and (11.3.4) is a polynomial.
(d) Each of the Legendre polynomials contains terms with
all odd powers of x or with all even powers of x.
Problems
1. Use Equations (11.3.3) and (11.3.4) to determine poly-
nomial solutions to Legendre’s equation when α = 3
and α = 4. Hence, determine the Legendre polynomi-
als P3(x) and P4(x).
2. Starting with P0(x) = 1 and P1(x) = x, use the re-
currence relation
(n+1)Pn+1+nPn−1 = (2n+1)x Pn,
n = 1, 2, 3, . . .
to determine P2, P3, and P4.
3. Use Rodrigues’ formula to determine the Legendre
polynomial of degree 3.
4. Determine all values of the constants a0, a1, a2, and
a3 such that
x3 + 2x = a0P0 + a1P1 + a2P2 + a3P3.
5. Express p(x) = 2x3 + x2 + 5 as a linear combination
of Legendre polynomials.
6. Let Q(x) be a polynomial of degree less than N. Prove
that
5 1
−1 Q(x)PN(x)dx = 0.
7. Show that
d2Y
dφ2 + cot φ dY
dφ + α(α + 1)Y = 0,
0 < φ < π,
is transformed into Legendre’s equation by the change
of variables x = cos φ.
Problems 8–10 deal with Hermite’s equation:
y′′ −2xy′ + 2αy = 0,
−∞< x < ∞. (11.3.13)
8. Determine two linearly independent series solutions
to Hermite’s equation centered at x = 0.
9. Show that if α = N, a positive integer, then Equa-
tion (11.3.13) has a polynomial solution. Determine
the polynomial solutions when α = 0, 1, 2, and 3.
10. When suitably normalized, the polynomial solutions
to Equation (11.3.13) are called the Hermite polyno-
mials, and are denoted by HN(x).
(a) Use Equation (11.3.13) to show that HN(x)
satisﬁes
(e−x2 H′
N)′ + 2Ne−x2 HN = 0.
(11.3.14)
[Hint: Replace α with N in Equation (11.3.13)
and multiply the resulting equation by e−x2.]
(b) Use Equation (11.3.14) to prove that the Hermite
polynomials satisfy
/ ∞
−∞
e−x2 HN(x)HM(x)dx = 0,
M ̸= N.
(11.3.15)
[Hint: Follow the steps taken in proving orthogo-
nality of the Legendre polynomials. You will need
to recall that
lim
x→±∞e−x2 p(x) = 0,
for any polynomial p.]
(c) Let p(x) be a polynomial of degree N. Then we
can write
p(x) =
N
!
k=1
ak Hk(x).
(11.3.16)
Given that
/ ∞
−∞
e−x2 H2
N(x)dx = 2N N!√π,
use (11.3.15) to prove that the constants in
(11.3.16) are given by
a j =
1
2 j j!√π
/ ∞
−∞
e−x2 Hj(x)p(x)dx.

750
CHAPTER 11
Series Solutions to Linear Differential Equations
11. ⋄Use some form of technology to determine the coef-
ﬁcients in the Legendre expansion of the polynomial
p(x) = 3x3 −1.
For Problems 12–13, use some form of technology to deter-
mine the ﬁrst four terms in the Legendre series expansion
of the given function on the interval (−1, 1). Plot the given
function and the approximations
S1 = a0P0,
S3 = a0P0 + a1P1 + a2P2,
S5 = a0P0 + a1P1 + a2P2 + a3P3 + a4P4
on the interval (−1, 1). Comment on the convergence of the
Legendre series to the given function for −1 < x < 1.
12. ⋄f (x) = cos πx.
13. ⋄f (x) = x(1 −x2)ex.
11.4
Series Solutions about a Regular Singular Point
The power series technique for solving
y′′ + P(x)y′ + Q(x)y = 0
(11.4.1)
developed in the previous section is directly applicable only at ordinary points; that is,
points where P and Q are both analytic. According to Deﬁnition 11.2.1, any points at
which P or Q fail to be analytic are called singular points of Equation (11.4.1), and the
general analysis of the behavior of solutions to Equation (11.4.1) in the neighborhood of
a singular point is quite complicated. However, singular points often turn out to be the
points of major interest in an applied problem, and so it is of some importance that
we pursue this analysis. In the next two sections, we will show that, provided that
the functions P and Q are not too badly behaved at a singular point, the power series
technique can be extended to obtain solutions of the corresponding differential equations
that are valid in the neighborhood of the singular point. We will restrict our attention to
differential equations whose singular points satisfy the following deﬁnition.
DEFINITION
11.4.1
The point x = x0 is called a regular singular point of the differential equation
(11.4.1) if and only if the following two conditions are satisﬁed:
1. x0 is a singular point of Equation (11.4.1).
2. Both of the functions
p(x) = (x −x0)P(x)
and
q(x) = (x −x0)2Q(x)
are analytic at x = x0.
A singular point of Equation (11.4.1) that does not satisfy condition (2) is called an
irregular singular point.
Example 11.4.2
Determine the ordinary points, regular singular points, and irregular singular points of
the differential equation
y′′ +
1
x(x −1)2 y′ +
x + 1
x(x −1)3 y = 0.
(11.4.2)

11.4
Series Solutions about a Regular Singular Point 751
Solution:
In this case, we have
P(x) =
1
x(x −1)2
and
Q(x) =
x + 1
x(x −1)3 ,
and, by inspection, P and Q are analytic at all points except x = 0 and x = 1. Hence the
only singular points of Equation (11.4.2) are x = 0 and x = 1. Consequently, every x
with x ̸= 0 and x ̸= 1 is an ordinary point of the differential equation. We now determine
whether the singular points x = 0 and x = 1 are regular or irregular.
Consider the singular point x = 0. The functions
p(x) = x P(x) =
1
(x −1)2
and
q(x) = x2Q(x) = x(x + 1)
(x −1)3
are both analytic at x = 0, so that x = 0 is a regular singular point of Equation (11.4.2).
Now consider the singular point x = 1. Since
p(x) = (x −1)P(x) =
1
x(x −1)
is nonanalytic at x = 1, it follows that x = 1 is an irregular singular point of Equation
(11.4.2).
□
Now suppose that x = x0 is a regular singular point of the differential equation
y′′ + P(x)y′ + Q(x)y = 0.
Multiplying this equation by (x −x0)2 yields
(x −x0)2y′′ + (x −x0)[(x −x0)P(x)]y′ + (x −x0)2Q(x)y = 0,
which we can write as
(x −x0)2y′′ + (x −x0)p(x)y′ + q(x)y = 0,
where
p(x) = (x −x0)P(x)
and
q(x) = (x −x0)2Q(x).
Since, by assumption, x = x0 is a regular singular point, it follows that the functions p
and q are analytic at x = x0. By the change of variables z = x −x0, we can always
transform a regular singular point to x = 0, and so we will restrict our attention to
differential equations that can be written in the form
x2y′′ + xp(x)y′ + q(x)y = 0,
(11.4.3)
where p and q are analytic at x = 0. This is the standard form of any differential equation
that has a regular singular point at x = 0. The simplest type of equation that falls into
this category is the second-order Cauchy-Euler equation
x2y′′ + p0xy′ + q0y = 0,
(11.4.4)
where p0 and q0 are constants. The solution techniques that we will develop for solving
Equation (11.4.3) will be motivated by the solutions to Equation (11.4.4). Recall from
Section 8.8 that (11.4.4) has solutions on the interval (0, ∞) of the form y(x) = xr,
where r is a root of the indicial equation
r(r −1) + p0r + q0 = 0.

752
CHAPTER 11
Series Solutions to Linear Differential Equations
Now consider Equation (11.4.3). Since, by assumption, p and q are analytic at x = 0,
we can write
p(x) = p0 + p1x + p2x2 + · · · ,
q(x) = q0 + q1x + q2x2 + · · · ,
(11.4.5)
for x in some interval of the form (−R, R). It follows that Equation (11.4.3) can be
written as
x2y′′ + x(p0 + p1x + p2x2 + · · · )y′ + (q0 + q1x + q2x2 + · · · )y = 0.
If |x| << 1, this is approximately the Cauchy-Euler equation (11.4.4), and so it is
reasonable to expect that for x in the interval (0, R), Equation (11.4.3) has solutions of
the form
y(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
(11.4.6)
where r is a root of the indicial equation
r(r −1) + p0r + q0 = 0.
(11.4.7)
A series of the form (11.4.6) is called a Frobenius series. We can assume without loss
of generality that a0 ̸= 0, since if this were not the case, we could always factor the
leading power of x out of the series and combine it into xr.
The following theorem conﬁrms our expectations:
Theorem 11.4.3
Consider the differential equation
x2y′′ + xp(x)y′ + q(x)y = 0,
x > 0,
(11.4.8)
where p and q are analytic at x = 0. Suppose that
p(x) =
∞
!
n=0
pnxn,
q(x) =
∞
!
n=0
qnxn,
for |x| < R. Let r1 and r2 denote the roots of the indicial equation
r(r −1) + p0r + q0 = 0,
and assume that r1 ≥r2 if these roots are real. Then Equation (11.4.8) has a solution of
the form
y1(x) = xr1
∞
!
n=0
anxn,
a0 ̸= 0.
This solution is valid (at least) for 0 < x < R. Further, provided that r1 and r2 are
distinct and do not differ by an integer, then there exists a second solution to (11.4.8)
that is valid (at least) for 0 < x < R of the form
y2(x) = xr2
∞
!
n=0
bnxn,
b0 ̸= 0.
The solutions y1 and y2 are linearly independent on their intervals of existence.

11.4
Series Solutions about a Regular Singular Point 753
Proof The proof of this theorem, as well as its extension to the case when the roots of
the indicial equation do differ by an integer, will be discussed fully in the next section.
Remark
Using the formula for the Maclaurin expansion of p and q, it follows that
the constants p0 and q0 appearing in (11.4.5) are given by
p0 = p(0)
and
q0 = q(0).
Consequently, the indicial equation (11.4.7) for
x2y′′ + xp(x)y′ + q(x)y = 0
can be written directly as
r(r −1) + p(0)r + q(0) = 0.
We conclude this section with some examples that illustrate the implementation of
the above theorem.
Example 11.4.4
Show that the differential equation
x2y′′ + xe2x y′ −2(cos x)y = 0,
x > 0
has two linearly independent Frobenius series solutions, and determine the interval on
which these solutions are valid.
Solution:
Comparing the given differential equation with the standard form (11.4.3),
we see that
p(x) = e2x
and
q(x) = −2 cos x.
Consequently,
p(0) = 1
and
q(0) = −2,
and so the indicial equation is
r(r −1) + r −2 = 0.
That is,
r2 −2 = 0.
Thus, the roots of the indicial equation are r1 =
√
2 and r2 = −
√
2. Since r1 and r2 are
distinct and do not differ by an integer, it follows from Theorem 11.4.3 that the given
differential equation has two Frobenius series solutions of the form
y1(x) = x
√
2
∞
!
n=0
anxn,
y2(x) = x−
√
2
∞
!
n=0
bnxn.
Further, since the power series expansions of p and q about x = 0 are valid for all x,
the preceding solutions will be deﬁned and linearly independent on (0, ∞).
□
Example 11.4.5
Find the general solution to
2x2y′′ + xy′ −(1 + x)y = 0,
x > 0.
(11.4.9)
Solution:
In this case, p(x) = 1
2 and q(x) = −1
2(1 + x), both of which are analytic
at x = 0. Thus, x = 0 is a regular singular point of Equation (11.4.9). The indicial
equation r(r −1) + 1
2r −1
2 = 0 can be written as 2r2 −r −1 = 0, which factors as

754
CHAPTER 11
Series Solutions to Linear Differential Equations
(2r + 1)(r −1) = 0. Hence, the roots of the indicial equation are
r1 = 1
and
r2 = −1
2.
(11.4.10)
Since these roots are distinct and do not differ by an integer, it follows from Theorem
11.4.3 that the given differential equation has two Frobenius series solutions of the form
y1(x) = x
∞
!
n=0
anxn,
y2(x) = x−1/2
∞
!
n=0
bnxn.
Now we wish to determine the coefﬁcients ai and bi. To do this, we let
y(x) = xr
∞
!
n=0
anxn =
∞
!
n=0
anxr+n,
a0 ̸= 0,
so that
y′(x) =
∞
!
n=0
(r + n)anxr+n−1,
y′′(x) =
∞
!
n=0
(r + n)(r + n −1)anxr+n−2.
Substituting into (11.4.9) yields
∞
!
n=0
2(r +n)(r +n −1)anxr+n +
∞
!
n=0
(r +n)anxr+n −
∞
!
n=0
anxr+n −
∞
!
n=0
anxr+n+1 = 0.
That is, combining the ﬁrst three terms and replacing n with n −1 in the fourth sum,
∞
!
n=0
'
2(r + n)(r + n −1) + (r + n) −1
(
anxn −
∞
!
n=1
an−1xn = 0.
(11.4.11)
Thus, the coefﬁcients of xn must vanish for n = 0, 1, 2, . . . . The constant term in
(11.4.11), corresponding to n = 0, leads directly to the indicial equation and the roots
given in (11.4.10). On the other hand, when n = 1, 2, . . . , in (11.4.11), we obtain the
recurrence relation
(r + n −1)(2r + 2n + 1)an −an−1 = 0.
(11.4.12)
We now substitute the values of r obtained in (11.4.10) into (11.4.12) to determine the
corresponding Frobenius series solutions.
Let r = 1: Substituting into the recurrence relation (11.4.12) yields
an =
1
n(2n + 3)an−1,
n = 1, 2, 3, . . . .
Thus,
n = 1:
a1 =
1
1 · 5a0,
n = 2:
a2 =
1
2 · 7a1 =
1
(2!)(5 · 7)a0,
n = 3:
a3 =
1
3 · 9a2 =
1
(3!)(5 · 7 · 9)a0,
n = 4:
a4 =
1
4 · 11a3 =
1
(4!)(5 · 7 · 9 · 11)a0.

11.4
Series Solutions about a Regular Singular Point 755
It follows that, in general,
an =
1
(n!)[5 · 7 · 9 · · · (2n + 3)]a0,
n = 1, 2, 3, . . . ,
so that the corresponding Frobenius series solution is
y1(x) = x
)
1 + 1
5x +
1
(2!)(5 · 7)x2 + · · · +
1
(n!)[5 · 7 · 9 · · · (2n + 3)]xn + · · ·
*
,
where we have set a0 = 1. We can write this solution as
y1(x) = x
)
1 +
∞
!
n=1
1
(n!)[5 · 7 · 9 · · · (2n + 3)]xn
*
,
x > 0.
Let r = −1/2: For the second Frobenius series solution, we replace the coefﬁcients ai
in the preceding work by bi. In this case, the recurrence relation (11.4.12) reduces to
bn =
1
n(2n −3)bn−1,
n = 1, 2, . . . .
We therefore obtain
n = 1:
b1 = −b0,
n = 2:
b2 =
1
2 · 1b1 = −1
2!b0,
n = 3:
b3 =
1
3 · 3b2 = −
1
(3!)(1 · 3)b0,
n = 4:
b4 =
1
4 · 5b3 = −
1
(4!)(1 · 3 · 5)b0.
In general, we have
bn = −
1
(n!)[1 · 3 · 5 · · · (2n −3)]b0,
n = 1, 2, 3, . . . .
It follows that a second linearly independent Frobenius series solution to the differential
equation (11.4.9) on (0, ∞) is
y2(x) = x−1/2
)
1−x −1
2!x2 −
1
(3!)(1 · 3)x3 −· · ·−
1
(n!)[1 · 3 · · · (2n −3)]xn −· · ·
*
,
where we have set b0 = 1. This can be written as
y2(x) = x−1/2
)
1 −
∞
!
n=1
1
(n!)[1 · 3 · 5 · · · (2n −3)]xn
*
,
x > 0.
Consequently, the general solution to the differential equation (11.4.9) on (0, ∞) is
y(x) = c1y1(x) + c2y2(x).
□
The differential equation in the previous example had two linearly independent
Frobeniusseriessolutions,andwewerethereforeabletodetermineitsgeneralsolution.In
the following example, there is only one linearly independent Frobenius series solution.

756
CHAPTER 11
Series Solutions to Linear Differential Equations
Example 11.4.6
Determine a Frobenius series solution to
x2y′′ + x(3 + x)y′ + (1 + 3x)y = 0,
x > 0.
(11.4.13)
Solution:
Byinspection,weseethat x = 0isaregularsingularpointofthedifferential
equation (11.4.13), and so from Theorem 11.4.3, the differential equation has at least
one Frobenius series solution. Further, since p(x) = 3 + x and q(x) = 1 + 3x are both
polynomials, their power series expansions about x = 0 are valid for all x. It follows
that any Frobenius series solution will be valid for 0 < x < ∞. To determine a solution,
we let
y(x) = xr
∞
!
n=0
anxn =
∞
!
n=0
anxr+n.
Differentiating twice with respect to x yields
y′(x) =
∞
!
n=0
(r + n)anxr+n−1,
y′′(x) =
∞
!
n=0
(r + n)(r + n −1)anxr+n−2,
so that y is a solution to (11.4.13) provided that an and r satisfy
∞
!
n=0
(r + n)(r + n −1)anxr+n + 3
∞
!
n=0
(r + n)anxr+n +
∞
!
n=0
(r + n)anxr+n+1
+
∞
!
n=0
anxr+n + 3
∞
!
n=0
anxr+n+1 = 0.
Dividing by xr and replacing n by n −1 in the third and ﬁfth sums yields
∞
!
n=0
'
(r + n)(r + n −1) + 3(r + n) + 1
(
anxn +
∞
!
n=1
(r + n + 2)an−1xn = 0. (11.4.14)
This implies that the coefﬁcients of xn must vanish for n = 0, 1, 2, . . . . When n = 0,
we obtain the indicial equation
r(r −1) + 3r + 1 = 0.
That is,
(r + 1)2 = 0.
Thus, the only value of r for which a Frobenius series solution exists is
r = −1.
For n ≥1, (11.4.14) yields the recurrence relation
'
(r + n)(r + n −1) + 3(r + n) + 1
(
an + (r + n + 2)an−1 = 0.
Setting r = −1, we obtain
'
(n −1)(n −2) + 3(n −1) + 1
(
an + (n + 1)an−1 = 0,

11.4
Series Solutions about a Regular Singular Point 757
which can be written as
an = −n + 1
n2
an−1,
n = 1, 2, . . . .
Solving this recurrence relation, we have
n = 1:
a1 = −2a0,
n = 2:
a2 = −3
4a1 = 3 · 2
4 a0,
n = 3:
a3 = −4
9a2 = −4!
4 · 9a0.
The general coefﬁcient is
an = (−1)n
(n + 1)!
22 · 32 · · · n2 a0,
n = 1, 2, 3, . . . ,
which can be written as
an = (−1)n (n + 1)!
(n!)2
a0.
That is,
an = (−1)n n + 1
n!
a0,
n = 1, 2, 3, . . . .
Consequently, the corresponding Frobenius series solution is
y(x) = x−1
)
1 +
∞
!
n=1
(−1)n n + 1
n!
xn
*
,
x > 0,
where we set a0 = 1.
□
Notice that in this problem, there is only one linearly independent Frobenius series
solution to the given differential equation. To determine a second linearly independent
solution, we could, for example, use the reduction of order method introduced in Sec-
tion 8.9. We will have more to say about this in the next section.
Exercises for 11.4
Key Terms
Regular singular point, Irregular singular point, Frobenius
series.
Skills
• Be able to classify singular points as regular or
irregular.
• Be familiar with the indicial equation and be able to
ﬁnd its roots.
• Be able to use the roots of the indicial equation to
determine a Frobenius series solution to Equation
(11.4.3).
• In the case where the indicial equation has two distinct
roots that do not differ by an integer, be able to ﬁnd
the general solution to Equation (11.4.3).
True-False Review
For Questions (a)–(e), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) A point x = x0 is a regular singular point of the differ-
ential equation y′′ + P(x)y′ + Q(x)y = 0 provided
that P(x) and Q(x) are not analytic at x = x0, but
(x −x0)P(x) and (x −x0)Q(x) are analytic at x = x0.
(b) If r1 and r2 are the roots of the indicial equation asso-
ciated with the differential equation x2y′′+xp(x)y′ +
q(x)y = 0, then xr1
∞
!
n=0
anxn and xr2
∞
!
n=0
bnxn (where
a0, b0 ̸= 0) are two linearly independent solutions to
the differential equation.

758
CHAPTER 11
Series Solutions to Linear Differential Equations
(c) The coefﬁcients in a Frobenius series solution to the
differential equation x2y′′ + xp(x)y′ + q(x)y = 0
are obtained by substituting the series solution and its
derivatives into the differential equation and match-
ing coefﬁcients of the powers of x on each side of the
equation.
(d) It is possible for a given differential equation of the
form y′′ + P(x)y′ + Q(x)y = 0 to have ordinary
points, regular singular points, and irregular singular
points.
(e) It is possible for all of the singular points of a given
differential equation of the form y′′ + P(x)y′ +
Q(x)y = 0 to be irregular.
Problems
For Problems 1–5, determine all singular points of the given
differential equation and classify them as regular or irregular
singular points.
1. y′′ +
1
1 −x y′ + xy = 0.
2. x2y′′ +
x
x2 −4 y′ +
1
x(x −2)(x + 2)2 y = 0.
3. x2y′′ +
x
(1 −x2)2 y′ + y = 0.
4. (x −2)2y′′ + (x −2)ex y′ + 4
x y = 0.
5. y′′ +
2
x(x −3) y′ −
1
x3(x + 3) y = 0.
For Problems 6–9, determine the roots of the indicial equa-
tion of the given differential equation.
6. x2y′′ + x(1 −x)y′ −7y = 0.
7. 4x2y′′ + xex y′ −y = 0.
8. 4xy′′ −xy′ + 2y = 0.
9. x2y′′ −x(cos x)y′ + 5e2x y = 0.
For Problems 10–17, show that the indicial equation of the
given differential equation has distinct roots that do not dif-
fer by an integer and ﬁnd two linearly independent Frobenius
series solutions on (0, ∞).
10. 4x2y′′ + 3xy′ + xy = 0.
11. 6x2y′′ + x(1 + 18x)y′ + (1 + 12x)y = 0.
12. x2y′′ + xy′ −(2 + x)y = 0.
13. 2xy′′ + y′ −2xy = 0.
14. 3x2y′′ −x(x + 8)y′ + 6y = 0.
15. 2x2y′′ −x(1 + 2x)y′ + 2(4x −1)y = 0.
16. x2y′′ + x(1 −x)y′ −(5 + x)y = 0.
17. 3x2y′′ + x(7 + 3x)y′ + (1 + 6x)y = 0.
18. Consider the differential equation
x2y′′ + xy′ + (1 −x)y = 0,
x > 0.
(11.4.15)
(a) Find the indicial equation, and show that the roots
are r = ±i.
(b) Determine the ﬁrst three terms in a complex-
valued Frobenius series solution to Equation
(11.4.15).
(c) Use the solution in (b) to determine two lin-
early independent real-valued solutions to Equa-
tion (11.4.15).
19. Determine the ﬁrst ﬁve nonzero terms in each of two
linearly independent Frobenius series solutions to
3x2y′′ + x(1 + 3x2)y′ −2xy = 0,
x > 0.
20. Consider the differential equation
4x2y′′ −4x2y′ + (1 + 2x)y = 0.
(a) Show that the indicial equation has only one
root, and ﬁnd the corresponding Frobenius series
solution.
(b) Use the reduction of order technique to ﬁnd a sec-
ond linearly independent solution on (0, ∞).
[Hint: To evaluate
/ ex
x dx, expand ex in a
Maclaurin series.]
21. Find two linearly independent solutions to
x2y′′ + x(3 −2x)y′ + (1 −2x)y = 0
on (0, ∞).

11.5
Frobenius Theory 759
11.5
Frobenius Theory
In the previous section, we saw how Frobenius series solutions can be obtained to the
differential equation
x2y′′ + xp(x)y′ + q(x)y = 0,
x > 0.
(11.5.1)
In this section, we give some justiﬁcation for Theorem 11.4.3 and extend this theorem
to the case when the roots of the indicial equation for (11.5.1) differ by an integer. We
will ﬁrst assume that x > 0 since our results can easily be extended to x < 0. We begin
by establishing the existence of at least one Frobenius series solution.
Assuming that x = 0 is a regular singular point of (11.5.1), it follows that p and q
are analytic at x = 0, and hence, we can write
p(x) =
∞
!
n=0
pnxn,
q(x) =
∞
!
n=0
qnxn,
for |x| < R. Consequently, (11.5.1) can be written as
x2y′′ + x
∞
!
n=0
pnxny′ +
∞
!
n=0
qnxny = 0.
(11.5.2)
We try for a Frobenius series solution and therefore let
y(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
where r and an are constants to be determined. Differentiating y twice yields
y′ =
∞
!
n=0
(r + n)anxr+n−1,
y′′ =
∞
!
n=0
(r + n)(r + n −1)anxr+n−2.
We now substitute into Equation (11.5.2) to obtain
∞
!
n=0
(r + n)(r + n −1)anxn +
) ∞
!
n=0
pnxn
*) ∞
!
n=0
(r + n)anxn
*
+
) ∞
!
n=0
qnxn
*) ∞
!
n=0
anxn
*
= 0.
Using the formula given in Section 11.1 for the product of two inﬁnite series gives
∞
!
n=0
(r + n)(r + n −1)anxn +
∞
!
n=0
) n
!
k=0
pn−kxn−k(k + r)akxk
*
+
∞
!
n=0
) n
!
k=0
qn−kxn−kakxk
*
= 0,
which can be written as
∞
!
n=0
0
(r + n)(r + n −1)an +
n
!
k=0
[pn−k(k + r) + qn−k]ak
3
xn = 0.

760
CHAPTER 11
Series Solutions to Linear Differential Equations
Thus, an must satisfy the recurrence relation
(r + n)(r + n −1)an +
n
!
k=0
'
pn−k(k + r) + qn−k
(
ak = 0,
n = 1, 2, . . . . (11.5.3)
Evaluating Equation (11.5.3) when n = 0 yields
[r(r −1) + p0r + q0]a0 = 0,
so that, since a0 ̸= 0 (by assumption), r must satisfy
r(r −1) + p0r + q0 = 0,
(11.5.4)
which we recognize as being the indicial equation for (11.5.1). When n ≥1, we combine
the coefﬁcients of an in (11.5.3) to obtain
'
(r + n)(r + n −1) + p0(r + n) + q0
(
an +
n−1
!
k=0
'
pn−k(k + r) + qn−k
(
ak = 0.
That is,
'
(r +n)(r +n−1)+ p0(r +n)+q0
(
an = −
n−1
!
k=0
'
pn−k(k+r)+qn−k
(
ak,
n = 1, 2, . . .
(11.5.5)
If we deﬁne F(r) by
F(r) = r(r −1) + p0r + q0,
then
F(r + n) = (r + n)(r + n −1) + p0(r + n) + q0,
so that the indicial equation (11.5.4) is
F(r) = 0,
whereas the recurrence relation (11.5.5) can be written as
F(r + n)an = −
n−1
!
k=0
[pn−k(k + r) + qn−k]ak,
n = 1, 2, 3, . . . .
(11.5.6)
It is tempting to divide (11.5.6) by F(r + n), thereby determining an in terms of
a0, a1, . . . , an−1. However, we can do this only if F(r + n) ̸= 0. Let r1 and r2 de-
note the roots of Equation (11.5.4). The following three familiar cases arise:
1. r1 and r2 are real and distinct.
2. r1 and r2 are real and coincident.
3. r1 and r2 are complex conjugates.
If r1 and r2 are real, we assume without loss of generality that r1 ≥r2. Consider (11.5.6)
when r = r1. We have
F(r1 + n)an = −
n−1
!
k=0
'
pn−k(k + r1) + qn−k
(
ak,
n = 1, 2, 3, . . . .
(11.5.7)

11.5
Frobenius Theory 761
In cases (1) and (2), it follows, since r = r1 is the largest root of F(r) = 0, that
F(r1 + n) ̸= 0, for any n. Also, in case (3), F(r1 + n) ̸= 0, and so, in all three cases we
can write (11.5.7) as
an = −
1
F(r1 + n)
n−1
!
k=0
'
pn−k(k + r1) + qn−k
(
ak,
n = 1, 2, 3, . . . .
(11.5.8)
Starting from n = 1, we can therefore determine all of the an in terms of a0, and so we
formally obtain the Frobenius series solution
y1(x) = xr1
)
1 +
∞
!
n=1
an(r1)xn
*
,
where an(r1) denotes the coefﬁcients obtained from (11.5.8) upon setting a0 = 1. A fairly
delicate analysis shows that this series solution converges for (at least) 0 < x < R. This
justiﬁes the steps in the preceding derivation and establishes the ﬁrst part of Theorem
11.4.3 stated in the previous section.
We now consider the problem of determining a second linearly independent solution
to Equation (11.5.1). We must consider the three cases separately.
Case 1: r1 and r2 are real and distinct. Setting r = r2 in (11.5.6) yields
F(r2 + n)an = −
n−1
!
k=0
'
pn−k(k + r2) + qn−k
(
ak.
(11.5.9)
Thus, provided that F(r2 + n) ̸= 0 for any positive integer n, the same procedure that
we used when r = r1 will yield a second Frobenius series solution. But, since r1 and r2
are the only zeros of F, it follows that F(r2 +n) = 0 if and only if there exists a positive
integer n such that r2 + n = r1. Consequently, provided that r1 −r2 is not a positive
integer, there exists a second Frobenius series solution of the form
y2(x) = xr2
)
1 +
∞
!
n=1
an(r2)xn
*
,
where an(r2) denotes the values of the coefﬁcients obtained from (11.5.6) when r = r2,
and once more, we have set a0 = 1. Since r1 ̸= r2, it follows that the Frobenius series
solutions y1 and y2 are linearly independent on (at least) 0 < x < R.
Now suppose that r1 −r2 = N, where N is a positive integer. Then substituting for
r2 = r1 −N in (11.5.9), we obtain
F(r1 + (n −N))an = −
n−1
!
k=0
'
pn−k(k + r2) + qn−k
(
ak,
n = 1, 2, . . . ,
which, when n = N, leads to the consistency condition
0 · aN = −
N−1
!
k=0
'
pN−k(k + r2) + qN+k
(
ak.
(11.5.10)
Since all of the coefﬁcients a1, a2, . . . , aN−1 will already have been determined in terms
of a0, (11.5.10) will be of the form
0 · aN = αa0,
(11.5.11)

762
CHAPTER 11
Series Solutions to Linear Differential Equations
where α is a constant. By assumption, a0 is nonzero, and therefore two possibilities arise.
(Notice that (11.5.11) is not an equation for determining α; rather, it is a consistency
condition for the validity of the recurrence relation.)
(a) First it may happen that α = 0. If this occurs, then from (11.5.11), aN can be
speciﬁed arbitrarily, and (11.5.9) determines all of the remaining Frobenius co-
efﬁcients. We therefore do obtain a second linearly independent Frobenius series
solution.
(b) In the more general case, α will be nonzero. Then, since a0 ̸= 0 (by assumption),
(11.5.11) cannot be satisﬁed, and so we cannot compute the Frobenius coefﬁcients.
Hence, there does not exist a Frobenius series solution corresponding to r = r2.
The reduction of order technique from Section 8.9 can be used, however, to prove
that there exists a second linearly independent solution to (11.5.1) on (0, R), of
the form
y2(x) = Ay1(x) ln x + xr2
∞
!
n=0
bnxn,
where the constants A and bn can be determined by direct substitution into the
differential equation (11.5.1). The derivation is straightforward, but quite long-
winded, and so the details have been relegated to Appendix D. Notice that the
foregoing form includes case (a), which arises when A = 0.
Case 2: r1 = r2. In this case, there certainly cannot exist a second linearly independent
Frobenius series solution. However, once more the reduction of order technique can be
used to establish the existence of a second linearly independent solution of the form
y2(x) = y1(x) ln x + xr1
∞
!
n=1
bnxn
valid for (at least) 0 < x < R. (See Appendix D.) The coefﬁcients bn can be obtained
by substituting this expression for y2 into the differential equation (11.5.1).
Case 3: r1 and r2 are complex conjugates. In this case, the solution just obtained
when r = r1 will be a complex-valued solution. Due to the linearity of the differential
equation, it follows that the real and imaginary parts of this complex-valued solution
will themselves be real-valued solutions. It can be shown that these real-valued solutions
are linearly independent on their intervals of existence. Thus, we can always, in theory,
obtain the general solution in this case.
Finally, we mention that the validity of the above solutions can be extended to
−R < x < 0 by the replacement
xr1 →|x|r1,
xr2 →|x|r2.
The preceding discussion is summarized in the next theorem.
Theorem 11.5.1
Consider the differential equation
x2y′′ + xp(x)y′ + q(x)y = 0,
(11.5.12)

11.5
Frobenius Theory 763
where p and q are analytic at x = 0. Suppose that
p(x) =
∞
!
n=0
pnxn,
q(x) =
∞
!
n=0
qnxn,
for |x| < R. Let r1 and r2 denote the roots of the indicial equation and assume that
r1 ≥r2 if these roots are real. Then (11.5.12) has two linearly independent solutions
valid (at least) on the interval (0, R). The form of the solution is determined as follows:
1. r1−r2 not an integer:
y1(x) = xr1
∞
!
n=0
anxn,
a0 ̸= 0,
(11.5.13)
y2(x) = xr2
∞
!
n=0
bnxn,
b0 ̸= 0.
(11.5.14)
2. r1 = r2 = r:
y1(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
(11.5.15)
y2(x) = y1(x) ln x + xr
∞
!
n=1
bnxn.
(11.5.16)
3. r1 −r2 a positive integer:
y1(x) = xr1
∞
!
n=0
anxn,
a0 ̸= 0,
(11.5.17)
y2(x) = Ay1(x) ln x + xr2
∞
!
n=0
bnxn,
b0 ̸= 0.
(11.5.18)
The coefﬁcients in each of these solutions can be determined by direct substitution into
Equation (11.5.12). Finally, if xr1 and xr2 are replaced by |x|r1 and |x|r2, respectively,
we obtain linearly independent solutions that are valid for (at least) 0 < |x| < R.
Remark
Sinceasolutionofahomogeneouslineardifferentialequationisonlydeﬁned
up to a multiplicative constant, we can use this freedom to set a0 = 1 in (11.5.13),
(11.5.15), and (11.5.17) and to set b0 = 1 in (11.5.14) and (11.5.18). It is often convenient
to make these choices in solving our problems.
We now consider several examples to illustrate the use of the preceding theorem.
Example 11.5.2
Consider the differential equation
x2y′′ −x(3 + x)y′ + (4 −x)y = 0.
(11.5.19)
Determine the general form of two linearly independent series solutions in the neigh-
borhood of the regular singular point x = 0.

764
CHAPTER 11
Series Solutions to Linear Differential Equations
Solution:
By inspection we see that x = 0 is a regular singular point of Equation
(11.5.19) and that the indicial equation is
r(r −1) −3r + 4 = r2 −4r + 4 = (r −2)2 = 0.
Since r = 2 is a repeated root, it follows from Theorem 11.5.1 that there exist two
linearly independent solutions to Equation (11.5.19) of the form
y1(x) = x2
∞
!
n=0
anxn,
y2(x) = y1(x) ln |x| + x2
∞
!
n=1
bnxn.
The coefﬁcients in each of these series solutions could be obtained by direct substitution
into Equation (11.5.19). In this problem, we have
p(x) = −(3 + x),
q(x) = 4 −x.
Since these are polynomials, their power series expansions about x = 0 converge for
all real x. It follows from Theorem 11.5.1 that the series solutions to Equation (11.5.19)
will be valid for all x ̸= 0.
□
Example 11.5.3
Consider the differential equation
x2y′′ + x(1 + 2x)y′ −1
4(1 −γ x)y = 0,
x > 0,
(11.5.20)
where γ is a constant. Determine the form of two linearly independent series solutions
to this differential equation about the regular singular point x = 0.
Solution:
In this case, we have p0 = 1 and q0 = −1
4, so that
F(r) = r(r −1) + r −1
4 = r2 −1
4.
Thus, the roots of the indicial equation are r = ± 1
2. Setting r1 = 1
2 and r2 = −1
2, it
follows that r1 −r2 = 1, so that we are in Case 3 of Theorem 11.5.1. Thus, there exist
two linearly independent solutions to Equation (11.5.20) on (0, ∞) of the form
y1(x) = x1/2
∞
!
n=0
anxn,
y2(x) = Ay1(x) ln x + x−1/2
∞
!
n=0
bnxn.
(11.5.21)
To determine whether the constant A is zero or nonzero, we need the general recurrence
relation. Substituting
y1(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
into (11.5.20) yields
)4(r + n)2 −1
4
*
an = −
'
2(r + n −1) + γ
(
an−1,
n = 1, 2, . . . .
When r = −1
2, this reduces to
n(n −1)an = −(2n −3 + γ )an−1.
(11.5.22)

11.5
Frobenius Theory 765
As predicted from our general theory, the coefﬁcient of an is zero when n = r1 −r2 = 1.
Thus, a second Frobenius series solution exists [A = 0 in (11.5.21)] if and only if the
term on the right-hand side also vanishes when n = 1. Setting n = 1 in (11.5.22) yields
the consistency condition
0 · a1 = (1 −γ )a0.
(11.5.23)
Since a0 ̸= 0, it follows from (11.5.23) that when γ ̸= 1, there does not exist a second
linearly independent Frobenius series solution and so the constant A in (11.5.21) is nec-
essarily nonzero. If γ = 1, however, then (11.5.23) is identically satisﬁed independently
of the value of a1. In this case, we can specify a1 arbitrarily and then the recurrence rela-
tion (11.5.22) will determine the remaining coefﬁcients in a second linearly independent
Frobenius series solution. To summarize:
1. If γ ̸= 1, there exist two linearly independent solutions of the form (11.5.21) with
A necessarily nonzero.
2. If γ = 1, then there exist two linearly independent Frobenius series solutions
y1(x) = x1/2
∞
!
n=0
anxn,
y2(x) = x−1/2
∞
!
n=0
bnxn.
In both cases, the series solutions will be valid for 0 < x < ∞.
□
Example 11.5.4
Determine two linearly independent series solutions to
x2y′′ + x(3 −x)y′ + y = 0,
x > 0.
(11.5.24)
Solution:
We see by inspection that x = 0 is a regular singular point. Furthermore,
since the functions
p(x) = 3 −x
and
q(x) = 1
are polynomials, the linearly independent series solutions obtained in Theorem 11.5.1
will be valid on (0, ∞). We begin by obtaining a Frobenius series solution. Substituting
y(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
into (11.5.24) yields
∞
!
n=0
(r + n)(r + n −1)anxr+n + 3
∞
!
n=0
(r + n)anxr+n
−
∞
!
n=0
(r + n)anxr+n+1 +
∞
!
n=0
anxr+n = 0,
which, upon collecting coefﬁcients of xr+n and replacing n by n −1 in the third sum,
can be written as
∞
!
n=0
'
(r + n)(r + n + 2) + 1
(
anxr+n −
∞
!
n=1
(r + n −1)an−1xr+n = 0.

766
CHAPTER 11
Series Solutions to Linear Differential Equations
Dividing by xr yields
∞
!
n=0
'
(r + n)(r + n + 2) + 1
(
anxn −
∞
!
n=1
(r + n −1)an−1xn = 0.
(11.5.25)
We determine the an in the usual manner. When n = 0, we obtain
[r(r + 2) + 1]a0 = 0,
so that the indicial equation is
r2 + 2r + 1 = 0;
that is,
(r + 1)2 = 0.
Hence, there is only one root; namely,
r = −1.
From (11.5.25), the remaining coefﬁcients must satisfy the recurrence relation
[(r + n)(r + n + 2) + 1]an −(r + n −1)an−1 = 0,
n = 1, 2, . . . .
Setting r = −1, this simpliﬁes to
an = n −2
n2
an−1,
n = 1, 2, 3, . . . .
Consequently,
a1 = −a0,
a2 = a3 = a4 = · · · = 0.
Thus, a Frobenius series solution to (11.5.24) is
y1(x) = x−1(1 −x),
(11.5.26)
where we set a0 = 1.
Since the indicial equation for (11.5.24) has only one root, there does not exist a
second linearly independent Frobenius series solution. However, according to Theorem
11.5.1, there is a second linearly independent solution of the form
y2(x) = y1(x) ln x + x−1
∞
!
n=1
bnxn,
(11.5.27)
where the coefﬁcients bn can be determined by substitution into (11.5.24). We now
determine such a solution. The computations are quite straightforward, but they are long
and tedious. The reader is encouraged to pay full attention and not to be overwhelmed by
the formidable look of the equations. Differentiating (11.5.27) with respect to x yields
y′
2 = y′
1 ln x + x−1y1 +
∞
!
n=1
(n −1)bnxn−2,
y′′
2 = y′′
1 ln x + 2x−1y′
1 −x−2y1 +
∞
!
n=1
(n −2)(n −1)bnxn−3.

11.5
Frobenius Theory 767
Substituting into (11.5.24), we obtain the following equation for the bn:
x2
)
y′′
1 ln x + 2x−1y′
1 −x−2y1 +
∞
!
n=1
(n −2)(n −1)bnxn−3
*
+ x(3 −x)
)
y′
1 ln x + x−1y1 +
∞
!
n=1
(n −1)bnxn−2
*
+ y1 ln x + x−1
∞
!
n=1
bnxn = 0,
or, equivalently,
1
x2y′′
1 + x(3 −x)y′
1 + y1
2
ln x + 2xy′
1 + 2y1 −xy1 +
∞
!
n=1
(n −1)(n −2)bnxn−1
+ 3
∞
!
n=1
(n −1)bnxn−1 −
∞
!
n=1
(n −1)bnxn +
∞
!
n=1
bnxn−1 = 0.
Since y1 is a solution to Equation (11.5.24), the combination of terms multiplying ln x
vanish.5 Combining the coefﬁcients of xn−1, we obtain
2xy′
1 + 2y1 −xy1 +
∞
!
n=1
'
(n −1)(n −2) + 3(n −1) + 1
(
bnxn−1 −
∞
!
n=1
(n −1)bnxn = 0.
Simplifying the terms in the ﬁrst sum and replacing n by n −1 in the second sum yields
2xy′
1 + 2y1 −xy1 +
∞
!
n=1
n2bnxn−1 −
∞
!
n=2
(n −2)bn−1xn−1 = 0.
(11.5.28)
From (11.5.26), we have
y1(x) = x−1(1 −x),
y′
1(x) = −x−2.
Substituting these expressions into (11.5.28) gives
−2x−1 + 2x−1(1 −x) −(1 −x) +
∞
!
n=1
n2bnxn−1 −
∞
!
n=2
(n −2)bn−1xn−1 = 0.
That is,
−3 + x +
∞
!
n=1
n2bnxn−1 −
∞
!
n=2
(n −2)bn−1xn−1 = 0.
Equating the corresponding coefﬁcients of xn−1 to zero for n = 1, 2, . . . , we obtain the
bn in a familiar manner.
n = 1:
−3 + b1 = 0, which implies that b1 = 3.
n = 2:
1 + 4b2 = 0, which implies that b2 = −1
4.
n ≥3:
n2bn −(n −2)bn−1 = 0.
5Note that this is not just a fortuitous result for this particular example. The combination of terms multiplying
ln x will always vanish at this stage of the computation.

768
CHAPTER 11
Series Solutions to Linear Differential Equations
That is,
bn = n −2
n2
bn−1,
n = 3, 4, . . . .
(11.5.29)
When n = 3, we have
b3 = 1
32 b2.
Substituting for b2 = −1
4 = −1
22 yields
b3 = −
1
22 · 32 .
When n = 4, (11.5.29) implies that
b4 = 2
42 b3 = −
1 · 2
22 · 32 · 42 .
We see that in general, for n ≥3,
bn = −
(n −2)!
22 · 32 · · · n2 ,
which can be written as
bn = −(n −2)!
(n!)2
,
n = 3, 4, . . . .
Finally,
substituting
for
bn
into
(11.5.27)
yields
the
following
solution
to
Equation (11.5.24):
y2(x) = x−1(1 −x) ln x + x−1
)
3x −1
4x2 −
∞
!
n=3
(n −2)!
(n!)2
xn
*
.
Theorem 11.5.1 guarantees that y1 and y2 are linearly independent on (0, ∞).
□
We give one ﬁnal example to illustrate the case when the roots of the indicial equation
differ by an integer.
Example 11.5.5
Determine two linearly independent solutions to
x2y′′ + xy′ −(4 + x)y = 0,
x > 0.
(11.5.30)
Solution:
Since x = 0isaregularsingularpoint,wetryforFrobeniusseriessolutions.
Substituting
y(x) =
∞
!
n=0
anxr+n
into (11.5.30) and simplifying yields the indicial equation
r2 −4 = 0
and the recurrence relation
[(r + n)2 −4]an = an−1,
n = 1, 2, . . . .
(11.5.31)

11.5
Frobenius Theory 769
It follows that the roots of the indicial equation are
r1 = 2
and
r2 = −2,
which differ by an integer. Substituting r = 2 into (11.5.31) yields
an =
1
n(n + 4)an−1,
n = 1, 2, . . . .
This is easily solved to obtain
an =
4!
n!(n + 4)!a0.
Consequently, one Frobenius series solution to (11.5.30) is
y1(x) = a0
∞
!
n=0
4!
n!(n + 4)!xn+2.
Choosing a0 = 1
4!, this solution reduces to
y1(x) =
∞
!
n=0
1
n!(n + 4)!xn+2.
(11.5.32)
We now determine whether there exists a second linearly independent Frobenius
series solution. Substituting r = −2 into (11.5.31) yields
n(n −4)an = an−1,
n = 1, 2, . . . .
(11.5.33)
Thus, when n = 1, n = 2, or n = 3, we obtain
a1 = −1
3a0,
a2 = 1
12a0,
a3 = −1
36a0.
However, when n = 4, (11.5.33) requires
0 · a4 = a3 = −1
36a0,
which is clearly impossible, since a0 ̸= 0. It follows that a second linearly independent
Frobenius series solution does not exist. However, according to Theorem 11.5.1, there
is a second linearly independent solution of the form
y2(x) = Ay1(x) ln x + x−2
∞
!
n=0
bnxn,
(11.5.34)
where the constants A and bn can be determined by substitution into (11.5.30). Differ-
entiating y2 yields
y′
2 = Ay′
1 ln x + Ax−1y1 +
∞
!
n=0
(n −2)bnxn−3,
y′′
2 = Ay′′
1 ln x + 2Ax−1y′
1 −Ax−2y1 +
∞
!
n=0
(n −3)(n −2)bnxn−4.

770
CHAPTER 11
Series Solutions to Linear Differential Equations
By substituting into (11.5.30) and simplifying, we obtain
A
1
x2y′′
1 + xy′
1 −(4 + x)y
2
ln x + 2Axy′
1 +
∞
!
n=0
(n −3)(n −2)bnxn−2
+
∞
!
n=0
(n −2)bnxn−2 −4
∞
!
n=0
bnxn−2 −
∞
!
n=0
bnxn−1 = 0.
The combination of terms multiplying ln x vanish,6 since y1 is a solution of (11.5.30).
Combining the coefﬁcients of xn−2 and simplifying yields
2Axy′
1 +
∞
!
n=1
n(n −4)bnxn−2 −
∞
!
n=0
bnxn−1 = 0.
(11.5.35)
We must now determine y′
1. Differentiating (11.5.32), we obtain
y′
1(x) =
∞
!
n=0
n + 2
n!(n + 4)!xn+1.
Substituting into (11.5.35) yields
2A
∞
!
n=0
n + 2
n!(n + 4)!xn+2 +
∞
!
n=1
n(n −4)bnxn−2 −
∞
!
n=0
bnxn−1 = 0.
We now replace n with n −4 in the ﬁrst sum and replace n with n −1 in the third sum
to obtain a common power of x in all sums. The result is
2A
∞
!
n=4
n −2
(n −4)!n!xn−2 +
∞
!
n=1
n(n −4)bnxn−2 −
∞
!
n=1
bn−1xn−2 = 0.
Upon multiplying through by x2, this becomes
2A
∞
!
n=4
n −2
(n −4)!n!xn +
∞
!
n=1
n(n −4)bnxn −
∞
!
n=1
bn−1xn = 0.
We can now determine the appropriate values of the constants by setting successive
coefﬁcients of xn to zero in the usual manner.
n = 1:
−3b1 −b0 = 0; hence
b1 = −1
3b0.
n = 2:
−4b2 −b1 = 0; hence
b2 = −1
4b1 = 1
12b0.
n = 3:
−3b3 −b2 = 0; hence
b3 = −1
3b2 = −1
36b0.
6Once more, we note that this will always happen at this stage of the computation.

11.5
Frobenius Theory 771
n = 4:
1
6 A −b3 = 0; hence
A = 6b3 = −1
6b0.
n ≥5:
bn =
1
n(n −4)
)
bn−1 −2A(n −2)
(n −4)!n!
*
.
Using this recurrence relation, we could continue to determine values of the bn. Notice
that b4 is unconstrained in this problem and so can be set equal to any convenient value.
For example, we could set b4 = 0. Substituting the values of the coefﬁcients so far
obtained into (11.5.34) gives
y2(x) = b0
)
−1
6 y1(x) ln x + x−2
%
1 −1
3x + 1
12 x2 −1
36x3 + · · ·
&*
.
(11.5.36)
Thus, two linearly independent solutions to the given differential equation on (0, ∞) are
y1(x) =
∞
!
n=0
1
n!(n + 4)!xn+2
and
y2(x) = y1(x) ln x −6x−2
%
1 −1
3x + 1
12 x2 −1
36x3 + · · ·
&
,
where we set b0 = −6 in (11.5.36).
□
Remark
In the previous example, we had to be content with determining only a few
terms in the second linearly independent solution, since we could not solve the recurrence
relation that arose for the bn. This is usually the case in these types of problems.
Exercises for 11.5
Skills
• Understand the three cases arising from the roots
of the indicial equation of the differential equation
x2y′′ + xp(x)y′ + q(x)y = 0, where p and q are
analytic at x = 0. The results are summarized in The-
orem 11.5.1.
• Be able to determine the general form for two linearly
independent series solutions to the differential equa-
tion x2y′′ + xp(x)y′ + q(x)y = 0
• Be able to solve for the coefﬁcients in the general
form of a series solution to the differential equation
x2y′′ + xp(x)y′ + q(x)y = 0 by substituting the se-
ries into the differential equation.
True-False Review
For Questions (a)–(f), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) The roots of the indicial equation for the differential
equation
x2y′′ −(x −x2)y′ + (1 + x3)y = 0
are distinct and differ by an integer.
(b) The roots of the indicial equation for the differential
equation
x2y′′ −(2
√
5 −1)xy′ +
%19
4 −3x2
&
y = 0
are distinct and differ by an integer.
(c) The roots of the indicial equation for the differential
equation
x2y′′ + (9x −2x5)y′ + (25 + 5x2 + 10x4)y = 0
are distinct and differ by an integer.

772
CHAPTER 11
Series Solutions to Linear Differential Equations
(d) The roots of the indicial equation for the differential
equation
x2y′′ +
%
4x + 1
2 x2 −1
3x3
&
y′ −7
4 y = 0
are distinct and differ by an integer.
(e) The roots of the indicial equation for the differential
equation
x2y′′ + x2y′ + xy = 0
are distinct and differ by an integer.
(f) Two linearly independent Frobenius series solutions
exist for the differential equation x2y′′ + xp(x)y′ +
q(x)y = 0 (where p and q are analytic at x = 0) only
if the roots r1 and r2 of the indicial equation differ by
an integer.
Problems
For Problems 1–8, determine the roots of the indicial equa-
tionofthegivendifferentialequation.Alsoobtainthegeneral
form of two linearly independent solutions to the differen-
tial equation on an interval (0, R). Finally, if r1 −r2 equals a
positive integer, obtain the recurrence relation and determine
whether the constant A in
y2(x) = Ay1(x) ln x + xr2
∞
!
n=0
bnxn
is zero or nonzero.
1. x2y′′ + x(x −3)y′ + (4 −x)y = 0.
2. 4x2y′′ + 2x2y′ + y = 0.
3. x2y′′ + x(cos x)y′ −2ex y = 0.
4. x2y′′ + x2y′ −(2 + x)y = 0.
5. x2y′′ + 2x2y′ + (x −3
4)y = 0.
6. x2y′′ + xy′ + (2x −1)y = 0.
7. x2y′′ + x3y′ −(2 + x)y = 0.
8. x2(x2 + 1)y′′ + 7xex y′ + 9(1 + tan x)y = 0.
9. Determine all values of the constant α for which
x2y′′ + x(1 −2x)y′ + [2(α −1)x −α2]y = 0
has two linearly independent Frobenius series solu-
tions on (0, ∞).
10. The indicial equation and recurrence relation for the
differential equation
x2y′′ + x[(2 −b)+ x]y′ −(b −γ x)y = 0 (11.5.37)
are, respectively,
(r + 1)(r −b) = 0,
(r + n + 1)(r + n −b)an = −[(r + n −1) + γ ]an−1,
n = 1, 2, 3, . . . ,
in the usual notation, where b and γ are constants.
Determine the form of two linearly independent se-
ries solutions to Equation (11.5.37) on (0, ∞) in the
following cases:
(a) b not an integer.
(b) b = −1.
(c) b = N, a nonnegative integer. [For solutions con-
taining a term of the form Ay1(x) ln x, you must
determine whether A is zero or nonzero.]
11. Show that
x2(1 + x)y′′ + x2y′ −2y = 0
has two linearly independent Frobenius series solu-
tions on (−1, 1) and ﬁnd them.
12. Consider the differential equation
x2y′′ + 3xy′ + (1 −x)y = 0,
x > 0. (11.5.38)
(a) Determine the indicial equation and show that it
has a repeated root r = −1.
(b) Obtain
the
corresponding
Frobenius
series
solution.
(c) It follows from Theorem 11.5.1 that Equation
(11.5.38) has a second linearly independent so-
lution of the form
y2(x) = y1(x) ln x + 1
x
∞
!
n=1
bnxn.
Show that b1 = −2 and that in general,
bn = 1
n2
)
bn−1 −
2n
n!(n −1)!
*
,
n = 2, 3, 4, . . .
Use this to ﬁnd the ﬁrst three terms of y2.

11.6
Bessel’s Equation of Order p
773
13. Consider the differential equation
xy′′ −y = 0,
x > 0.
(11.5.39)
(a) Show that the roots of the indicial equation are
r1 = 1 and r2 = 0, and determine the Frobenius
series solutions corresponding to r1 = 1.
(b) Show that there does not exist a second linearly
independent Frobenius series solution.
(c) According to Theorem 11.5.1, Equation (11.5.39)
has a second linearly independent solution of
the form
y2(x) = Ay1(x) ln x +
∞
!
n=0
bnxn.
Show that A = b0, and determine the ﬁrst three
terms in y2.
For Problems 14–27, determine two linearly independent so-
lutions to the given differential equation on (0, ∞).
14. x2y′′ + x(6 + x2)y′ + 6y = 0.
15. x2y′′ + x(1 −x)y′ −y = 0.
16. 4x2y′′ + (1 −4x)y = 0.
17. xy′′ + y′ −2y = 0.
18. x2y′′ + xy′ −(1 + x)y = 0.
19. x2y′′ −x(x + 3)y′ + 4y = 0.
20. x2y′′ −x2y′ −2y = 0.
21. x2y′′ −x2y′ −(3x + 2)y = 0.
22. x2y′′ + x(5 −x)y′ + 4y = 0.
23. 4x2y′′ + 4x(1 −x)y′ + (2x −9)y = 0.
24. x2y′′ + 2x(2 + x)y′ + 2(1 + x)y = 0.
25. x2y′′ −x(1 −x)y′ + (1 −x)y = 0.
26. 4x2y′′ + 4x(1 + 2x)y′ + (4x −1)y = 0.
27. 4x2y′′ −(3 + 4x)y = 0.
For Problems 28–29, determine a Frobenius series solution
to the given differential equation and use the reduction of or-
der technique to ﬁnd a second linearly independent solution
on (0, ∞).
28. xy′′ −xy′ + y = 0.
29. x2y′′ + x(4 + x)y′ + (2 + x)y = 0.
30. Consider the Laguerre differential equation
x2y′′ + x(1 −x)y′ + Nxy = 0,
(11.5.40)
where N is a constant. Show that in the case when
N is a positive integer, Equation (11.5.40) has a so-
lution that is a polynomial of degree N, and ﬁnd it.
When properly normalized, these solutions are called
the Laguerre polynomials.
31. Consider the differential equation
x2y′′ + x(1 + 2N + x)y′ + N 2y = 0,
(11.5.41)
where N is a positive integer.
(a) Show that there is only one Frobenius series so-
lution and that it terminates after N + 1 terms.
Find this solution.
(b) ShowthatthechangeofvariablesY = x N y trans-
forms Equation (11.5.41) into the Laguerre dif-
ferential equation (11.5.40).
11.6
Bessel’s Equation of Order p
One of the most important differential equations in applied mathematics and mathemat-
ical physics is Bessel’s equation of order p, deﬁned by
x2y′′ + xy′ + (x2 −p2)y = 0,
(11.6.1)
where p is a nonnegative constant. In general, it is not possible to obtain closed form
solutions to this equation. However, since x = 0 is a regular singular point, we can apply
the Frobenius series technique to obtain series solutions. We will assume that x > 0.
The indicial equation for (11.6.1) is
r(r −1) + r −p2 = 0

774
CHAPTER 11
Series Solutions to Linear Differential Equations
with roots
r = ±p.
Consequently, provided that 2p is not an integer, there will exist two linearly independent
Frobenius series solutions. In order to obtain these solutions, we let
y(x) = xr
∞
!
n=0
anxn
(11.6.2)
so that
y′(x) =
∞
!
n=0
(r + n)anxr+n−1,
y′′(x) =
∞
!
n=0
(r + n)(r + n −1)anxr+n−2.
Substituting into (11.6.1) and rearranging yields
∞
!
n=0
[(r + n)2 −p2]anxr+n +
∞
!
n=2
an−2xr+n = 0.
(11.6.3)
When n = 0, we obtain the indicial equation whose roots are, as we have seen above,
r = ±p.
When n = 1, (11.6.3) implies that
[(r + 1)2 −p2]a1 = 0
(11.6.4)
and for n ≥2, we obtain the general recurrence relation
[(r + n)2 −p2]an = −an−2,
n = 2, 3, . . . .
(11.6.5)
Consider the root r = p. In this case, (11.6.4) reduces to
(2p + 1)a1 = 0
so that, since p ≥0,
a1 = 0.
(11.6.6)
Setting r = p in (11.6.5) yields
an = −
1
n(2p + n)an−2,
n = 2, 3, . . . .
(11.6.7)
It follows from (11.6.6) and (11.6.7) that all of the odd coefﬁcients are zero; that is,
a2k+1 = 0,
k = 0, 1, 2, . . . .
(11.6.8)
Now consider the even coefﬁcients. From (11.6.7), we obtain
a2 = −
1
2(2p + 2)a0,
a4 = −
1
4(2p + 4)a2 =
1
2 · 4(2p + 4)(2p + 2)a0,
and so on. The general even coefﬁcient is
a2k =
(−1)k
2 · 4 · · · (2k)(2p + 2)(2p + 4) · · · (2p + 2k)a0,
k = 1, 2, . . . ,

11.6
Bessel’s Equation of Order p
775
which can be written as
a2k =
(−1)k
22kk!(p + 1)(p + 2) · · · (p + k)a0,
k = 1, 2, 3, . . . .
Consequently, the corresponding Frobenius series solution to Bessel’s equation is
y1(x) = a0x p
)
1 +
∞
!
k=1
(−1)k
22kk!(p + 1)(p + 2) · · · (p + k)x2k
*
.
(11.6.9)
This solution is valid for all x > 0.
Bessel Functions of the First Kind7
In order to study the solutions of Bessel’s equation obtained above, it is convenient to
ﬁrst rewrite (11.6.9) in a different, but equivalent form. The analysis splits into two cases:
Case 1: p = N, a nonnegative integer: In this case, the solution (11.6.9) can be
written as
y1(x) = a0x N
)
1 +
∞
!
k=1
(−1)k
22kk!(N + 1)(N + 2) · · · (N + k)x2k
*
,
(11.6.10)
where the constant a0 can be chosen arbitrarily. It is convenient to make the choice
a0 =
1
N!2N .
The corresponding solution of Bessel’s equation is denoted JN(x) and is called the Bessel
function of the ﬁrst kind of integer order N. Substituting for a0 into (11.6.10), we
obtain
JN(x) =
∞
!
k=0
(−1)k
k!(N + k)!
%x
2
&2k+N
.
(11.6.11)
The most important Bessel functions of integer order are J0(x) and J1(x) since, as we
shall see shortly, all other integer-order Bessel functions of the ﬁrst kind can be expressed
in terms of these two. Writing out the ﬁrst few terms in (11.6.11) when N = 0, 1 yields,
respectively,
J0(x) = 1 −1
4x2 + 1
64x4 −· · ·
J1(x) = 1
2 x
%
1 −1
8x2 +
1
192 x4 −· · ·
&
.
An analysis of these functions shows that they both oscillate with decaying amplitude.
Further, each has an inﬁnite number of nonnegative zeros. A sketch of J0 and J1 on the
interval (0, 10] is given in Figure 11.6.1.
Case 2: p a noninteger: In order to obtain a formula analogous to (11.6.11) for the
Frobenius series solution (11.6.9) when p is not an integer, we need to introduce the
gamma function. This function can be considered as the generalization of the factorial
function to the case of noninteger real numbers.
7The remainder of this section includes only a brief introduction to Bessel functions. For more details and
the proofs of the results stated in this section, the reader is referred to N.N. Lebedev, Special Functions and
their Applications, Dover (1972).

776
CHAPTER 11
Series Solutions to Linear Differential Equations
2
4
6
8
10
—0.2
0.2
—0.4
0.4
0.6
0.8
1
x
J0(x), J1(x)
J0(x)
J1(x)
Figure 11.6.1: The Bessel functions of the ﬁrst kind J0(x), J1(x).
DEFINITION
11.6.1
The gamma function, &, is deﬁned by
&(p) =
/ ∞
0
t p−1e−tdt,
p > 0.
It can be shown that the above improper integral converges for all p > 0, so that the
gamma function is well-deﬁned for all such p. To show that the gamma function is a
generalization of the factorial function, we ﬁrst require the following result.
Lemma 11.6.2
For all p > 0,
&(p + 1) = p&(p).
(11.6.12)
Proof The proof consists of integrating the expression for &(p + 1) by parts:
&(p + 1) =
/ ∞
0
t pe−tdt = [−t pe−t]∞
0 + p
/ ∞
0
t p−1e−tdt
= p&(p).
We also require
&(1) =
/ ∞
0
e−tdt = 1.
(11.6.13)
Equations (11.6.12) and (11.6.13) imply that
&(2) = 1&(1) = 1,
&(3) = 2&(2) = 2!,
&(4) = 3&(3) = 3!,
and in general, for all nonnegative integers N,
&(N + 1) = N!.
This justiﬁes the claim that the gamma function generalizes the factorial function. We
now extend the deﬁnition of the gamma function to p < 0. From (11.6.12),
&(p) = &(p + 1)
p
(11.6.14)

11.6
Bessel’s Equation of Order p
777
for p > 0. We use this expression to deﬁne &(p) for p < 0 as follows. If p is in the
interval (−1, 0), then p + 1 is in the interval (0, 1) and so &(p) given in (11.6.14) is
well-deﬁned. We continue in this manner to successively deﬁne &(p) in the intervals
(−2, −1), (−3, −2), . . . . From (11.6.13) and (11.6.14), it follows that
lim
p→0+ &(p) = +∞,
lim
p→0−&(p) = −∞
so that the graph of the gamma function has the general form given in Figure 11.6.2.
—1
—2
—3
G(p)
p
Figure 11.6.2: The gamma function.
We note that the gamma function is continuous and in fact inﬁnitely differentiable at
all points of its domain, which consists of all real numbers with the exception of the
collectionofintegers≤0.Finally,beforereturningtoourdiscussionofBessel’sequation,
we require the following formula:
&(p + 1)[(p + 1)(p + 2) · · · (p + k)] = &(p + k + 1).
(11.6.15)
The proof of this follows by repeated application of (11.6.14), in the form
p&(p) = &(p + 1),
to the left-hand side of the above equality and is left as an exercise.
Now let us return to the solution (11.6.9) of Bessel’s equation. Once more we make
a speciﬁc choice for a0. We set
a0 =
1
2p&(p + 1).
Substituting this value for a0 into (11.6.9) and using (11.6.15) yields the Bessel function
of the ﬁrst kind of order p, Jp(x), deﬁned by
Jp(x) =
∞
!
k=0
(−1)k
&(k + 1)&(p + k + 1)
%x
2
&2k+p
.
(11.6.16)
Notice that this does reduce to JN(x) when N is a nonnegative integer.

778
CHAPTER 11
Series Solutions to Linear Differential Equations
Bessel Functions of the Second Kind
Now consider determining the general solution of Bessel’s equation. For all p ≥0, we
have shown above that one solution to Bessel’s equation on (0, ∞) is given in (11.6.16).
Wethereforerequireasecondlinearlyindependentsolution.Sincetherootsoftheindicial
equation are r = ±p, it follows from our general Frobenius theory that, provided 2p is
not equal to an integer, there will exist a second linearly independent Frobenius series
solution corresponding to the root r = −p. It is not too difﬁcult to see that this solution
can be obtained by replacing p by −p in (11.6.16). Thus, we obtain
J−p(x) =
∞
!
k=0
(−1)k
&(k + 1)&(k −p + 1)
%x
2
&2k−p
.
(11.6.17)
Consequently, the general solution to Bessel’s equation of order p when 2p is not equal
to an integer is
y(x) = c1Jp(x) + c2J−p(x).
(11.6.18)
When 2p is equal to an integer, two subcases arise depending on whether p is itself
an integer or a half-integer (that is, 1
2, 3
2, . . . ). In the latter case, with p of the form
(2 j + 1)/2 ( j a nonnegative integer), a straightforward analysis of the recurrence relation
(11.6.5) with r = −p shows that a second Frobenius series also exists in this case, and it
is, in fact, given by (11.6.17). Thus, (11.6.18) represents the general solution of Bessel’s
equation, provided p is not equal to an integer. In practice, rather than using J−p as the
second linearly independent solution of Bessel’s equation, it is usual to use the following
linear combination of these two solutions:
Yp(x) = Jp(x) cos pπ −J−p(x)
sin pπ
.
(11.6.19)
The function deﬁned in (11.6.19) is called the Bessel function of the second kind of
order p. Using Yp we can therefore write the general solution of Bessel’s equation,
when p is not equal to a positive integer, in the form
y(x) = c1Jp(x) + c2Yp(x).
(11.6.20)
The determination of a second linearly independent solution to Bessel’s equation when
p is a positive integer, n, is quite a bit more complicated. From our general Frobenius
theory, we certainly know the form of the second linearly independent solution; namely,
y2(x) = AJn(x) ln x + x−n
∞
!
k=0
bkxk,
and the coefﬁcients A, bn could be determined by direct substitution into (11.6.1). How-
ever, if we extend the deﬁnition of the Bessel function of the second kind to the case of
positive integers by
Yn(x) = lim
p→n
) Jp(x) cos pπ −J−p(x)
sin pπ
*
,
(11.6.21)
it can be shown that the above limit exists and that the resulting function is indeed a
second linearly independent solution of Bessel’s equation. We could derive the series
representation of (11.6.21) by evaluating the limit explicitly. The calculations are lengthy

11.6
Bessel’s Equation of Order p
779
and tedious and so we omit them. The result of these calculations is
Yn(x) = 2
π Jn(x)
1
ln
6x
2
7
+ γ
2
−1
π
)n−1
!
k=0
(n −k −1)!
k!
6x
2
72k−n
+ sn
n!
6x
2
7n
+
∞
!
k=1
(−1)k sk + sn+k
k!(n + k)!
6x
2
72k+n*
,
where
sk = 1 + 1
2 + 1
3 + · · · + 1
k
and
γ = lim
k→∞(sk −ln k) ≈0.577215664 . . .
The value γ is called Euler’s constant.
Thus (11.6.20) represents the general solution to Bessel’s equation of arbitrary
order p.
Properties of Bessel Functions of the First Kind
In practice, we are usually interested in solutions of Bessel’s equation that are bounded
at x = 0. However, the Bessel functions of the second kind are always unbounded at
x = 0. (When p is not an integer, this follows from the fact that x−p has a negative
exponent, and when p is an integer, it is due to the second term in the expansion of Yn
given previously.) Thus we usually only require the Bessel functions of the ﬁrst kind. In
this section, we list various properties of the Bessel functions of the ﬁrst kind that help
in either tabulating values of the functions or in working with the functions themselves.
We ﬁrst recall from (11.6.16) the deﬁnition of the Bessel functions of the ﬁrst kind
of order p, namely
Jp(x) =
∞
!
k=0
(−1)k
&(k + 1)&(p + k + 1)
6x
2
72k+p
.
(11.6.22)
Property 1:
d
dx [x p Jp(x)] = x p Jp−1(x).
(11.6.23)
Property 2:
d
dx [x−p Jp(x)] = −x−p Jp+1(x).
(11.6.24)
Proof of Property 1: Multiplying (11.6.22) by x p and differentiating the result with
respect to x yields
d
dx [x p Jp(x)] =
∞
!
k=0
(−1)k
&(k + 1)&(p + k + 1)(2k + 2p)2p−1 6x
2
72k+2p−1
= x p
∞
!
k=0
(−1)k
&(k + 1)&(p + k + 1)(k + p)2p−1 6x
2
72k+p−1
.

780
CHAPTER 11
Series Solutions to Linear Differential Equations
But, from (11.6.14), &(p + k + 1) = (p + k)&(p + k), so that
d
dx [x p Jp(x)] = x p
∞
!
k=0
(−1)k
&(k + 1)&(p + k)
6x
2
72k+p−1
= x p Jp−1(x).
Property 2 is proved similarly.
We now derive two identities satisﬁed by the derivatives of Jp. Expanding the
derivatives on the right-hand sides of (11.6.23) and (11.6.24) and dividing the resulting
equations by x p and x−p respectively yields
J ′
p(x) + x−1 pJp(x) = Jp−1(x)
(11.6.25)
J ′
p(x) −x−1 pJp(x) = −Jp+1(x).
(11.6.26)
Subtracting (11.6.26) from (11.6.25) and rearranging terms we obtain:
Property 3:
Jp+1(x) = 2x−1 pJp(x) −Jp−1(x).
(11.6.27)
Similarly, adding (11.6.25) and (11.6.26) and rearranging yields:
Property 4:
J ′
p(x) = 1
2[Jp−1(x) −Jp+1(x)].
(11.6.28)
These formulas allow us to express high-order Bessel functions and their derivatives in
terms of lower-order functions. For example, all integer-order Bessel functions can be
expressed in terms of J0(x) and J1(x).
Example 11.6.3
Express J2(x) and J3(x) in terms of J0(x) and J1(x).
Solution:
Applying (11.6.27) with p = 1, we obtain
J2(x) = 2x−1J1(x) −J0(x).
Similarly, when p = 2,
J3(x) = 4x−1J2(x) −J1(x) = x−2(8 −x2)J1(x) −4x−1J0(x).
□
A Bessel Function Expansion Theorem
It can be shown that every Bessel function of the ﬁrst kind has an inﬁnite number of
positive zeros. We now show how this can be used to obtain a Bessel function expansion
of an arbitrary function.
Let λ1, λ2, . . . denote the positive zeros of the Bessel function Jp(x), where p ≥0
is ﬁxed, and consider the corresponding functions
um(x) = Jp(λmx),
un(x) = Jp(λnx)

11.6
Bessel’s Equation of Order p
781
for x in the interval [0, 1]. We begin by deriving the orthogonality relation for the
functions um and un. Since Jp(x) solves Bessel’s equation
y′′ + 1
x y′ +
%
1 −p2
x2
&
y = 0,
it is not too difﬁcult to show (see Problem 19) that um and un satisfy
u′′
m + 1
x u′
m +
%
λ2
m −p2
x2
&
um = 0,
(11.6.29)
u′′
n + 1
x u′
n +
%
λ2
n −p2
x2
&
un = 0.
(11.6.30)
Multiplying (11.6.29) by un and (11.6.30) by um, and subtracting, we get
(u′′
mun −u′′
num) + 1
x (u′
mun −u′
num) + (λ2
m −λ2
n)umun = 0,
which can be written as
(u′
mun −u′
num)′ + 1
x (u′
mun −u′
num) + (λ2
m −λ2
n)umun = 0.
Multiplyingthis equationby x andcombiningtheﬁrst twoterms of theresultingequation,
we obtain
d
dx [x(u′
mun −u′
num)] + x(λ2
m −λ2
n)umun = 0.
Integrating from 0 to 1 and using the fact that um(1) = un(1) = 0 (since λm and λn are
zeros of Jp(x)) yields
(λ2
m −λ2
n)
/ 1
0
xumundx = 0;
that is, since λm and λn are distinct and positive,
/ 1
0
xumundx = 0.
Substituting for um and un, we ﬁnally obtain
/ 1
0
x Jp(λmx)Jp(λnx)dx = 0,
whenever m ̸= n.
(11.6.31)
In this case, we say that the set of functions {Jp(λnx)}∞
n=1 is orthogonal on (0,1) relative
to the weight function w(x) = x. It can be further shown (see Problem 20) that when
m = n,
/ 1
0
x[Jp(λnx)]2dx = 1
2[Jp+1(λn)]2.
(11.6.32)
We can now state a Bessel function expansion theorem.

782
CHAPTER 11
Series Solutions to Linear Differential Equations
Theorem 11.6.4
If f and f ′ are continuous on the interval [0, 1], then for 0 < x < 1,
f (x) = a1Jp(λ1x) + a2Jp(λ2x) + · · · + an Jp(λnx) + · · · =
∞
!
n=1
an Jp(λnx),
(11.6.33)
where the coefﬁcients can be determined from
an =
2
[Jp+1(λn)]2
/ 1
0
x f (x)Jp(λnx)dx.
(11.6.34)
Proof We show only that if a series of the form (11.6.33) exists, then the coefﬁcients
are given by (11.6.34). The proof of convergence is omitted. Multiplying (11.6.33) by
x Jp(λmx) and integrating the resulting equation with respect to x from 0 to 1, we obtain
/ 1
0
x f (x)Jp(λmx)dx = am
/ 1
0
x[Jp(λmx)]2dx,
where we have used (11.6.31). Substituting from (11.6.32) for the integral on the right-
hand side of the preceding equation and rearranging terms yields
am =
2
[Jp+1(λm)]2
/ 1
0
x f (x)Jp(λmx)dx,
which is what we wished to show.
Remark
An expansion of the form (11.6.33) is called a Fourier-Bessel expansion
of f .
Example 11.6.5
Determine the Fourier-Bessel expansion of f (x) = 1 in terms of the functions J0(λnx).
Solution:
According to Theorem 11.6.4, for 0 < x < 1 we can write
1 =
∞
!
n=1
an J0(λnx),
where
an =
2
[J1(λn)]2
/ 1
0
x J0(λnx)dx.
(11.6.35)
But,
/ 1
0
x J0(λnx)dx = 1
λ2n
/ λn
0
uJ0(u)du,
where u = λnx. Applying (the integrated form of) (11.6.23) with p = 1 yields
/ 1
0
x J0(λnx)dx = 1
λ2n
[uJ1(u)]λn
0 = 1
λn
J1(λn).
Substitution into (11.6.35) gives
an =
2
λn J1(λn),
so that the appropriate Fourier-Bessel expansion is
1 = 2
∞
!
n=1
1
λn J1(λn) J0(λnx),
0 < x < 1.
□

11.6
Bessel’s Equation of Order p
783
Exercises for 11.6
Key Terms
Bessel’s equation of order p, Bessel functions of the ﬁrst
kind, Gamma function, Bessel functions of the second kind,
Fourier-Bessel expansion of a function.
Skills
• Be able to determine two linearly independent solu-
tions to Bessel’s equation.
• Be able to evaluate the gamma function for all p at
which the gamma function is deﬁned.
• Be familiar with Properties 1–4 of the Bessel functions
of the ﬁrst kind.
• If f is a continuous function with a continuous deriva-
tive on [0, 1], be able to compute a Fourier-Bessel ex-
pansion of f on the interval (0, 1).
True-False Review
For Questions (a)–(g), decide if the given statement is true
or false, and give a brief justiﬁcation for your answer. If true,
you can quote a relevant deﬁnition or theorem from the text.
If false, provide an example, illustration, or brief explanation
of why the statement is false.
(a) Provided that p is a positive noninteger, two linearly
independent Frobenius series solutions can be ob-
tained to Bessel’s equation of order p.
(b) Equations (11.6.18) and (11.6.20) are both valid gen-
eral solutions to Bessel’s equation of order p in the
case when 2p is not an integer.
(c) The gamma function is deﬁned for all real numbers p.
(d) &(p) < 0 if and only if the greatest integer less than
or equal to p is odd and negative.
(e) The Bessel function Jp(x) can be written as a lin-
ear combination of the Bessel functions Jp−1(x) and
Jp−2(x).
(f) The Bessel functions are differentiable with J ′
p(x) =
pJp−1(x).
(g) If λ1, λ2, . . . , denote the positive zeros of the Bessel
function Jp(x), then the functions Jp(λnx) and
Jp(λmx) are orthogonal on (0, 1) relative to the in-
ner product ⟨f, g⟩=
5 1
0 f (x)g(x)dx.
Problems
1. Use the relations (11.6.4) and (11.6.5) to show that if p
is a half-integer, then Bessel’s equation of order p has
two linearly independent Frobenius series solutions.
2. Determine two linearly independent solutions to
x2y′′ + xy′ +
%
x2 −9
4
&
y = 0
on the interval (0, ∞).
3. Verify that y(x) = x J1(x) is a solution to the differ-
ential equation
xy′′ −y′ + xy = 0,
x > 0.
4. Let &(p) denote the gamma function. Show that
&(p +1)[(p +1)(p +2) · · · (p +k)] = &(p +k +1).
5. For p > 0 and a > 0 express the following integral in
terms of the gamma function:
/ ∞
0
t p−1e−at dt.
6. (a) By making the change of variables t = x2 in the
integral that deﬁnes the gamma function, show
that
&(1/2) = 2
/ ∞
0
e−x2dx.
(b) Use your result from (a) to show that
[&(1/2)]2 = 4
/ ∞
0
/ ∞
0
e−(x2+y2)dxdy.
(c) By changing to polar coordinates, evaluate the
double integral in (b) and hence show that
&(1/2) = √π.
7. (a) Given that &(1/2) = √π by Problem 6, deter-
mine &(3/2) and &(−1/2).
(b) Show that for positive integer n:
&
%
n + 1
2
&
=
(2n)!
22n · n!
√π.
(c) Show that for positive integer n:
&
%1
2 −n
&
= (−1)n · 22n · n!
(2n)!
√π.

784
CHAPTER 11
Series Solutions to Linear Differential Equations
8. Let Jp(x) denote the Bessel function of the ﬁrst kind
of order p. Show that
d
dx (x−p Jp(x)) = −x−p Jp+1(x).
9. By manipulating the general expression for J1/2(x),
show that it can be written in closed form as
J1/2(x) =
8
2
πx sin x.
10. Given that
J1/2(x) =
8
2
πx sin x,
J−1/2(x) =
8
2
πx cos x,
express J3/2(x) and J−3/2(x) in closed form. Con-
vince yourself that all half-integer order Bessel func-
tions of the ﬁrst kind can be expressed as a ﬁnite sum
of terms involving products of sin x, cos x, and powers
of x.
11. By integrating the recurrence relation for derivatives
of the Bessel functions of the ﬁrst kind, show that
(a)
5
x p Jp−1(x)dx = x p Jp(x) + C.
(b)
5
x−p Jp+1(x)dx = −x−p Jp(x) + C.
12. Show that
(a) J ′′
0 (x) = −J0(x) −x−1J ′
0(x).
(b) J ′′′
0 (x) = x−1J0(x) + x−2(2 −x2)J ′
0(x).
13. Show that
J4(x) = 8x−3(6 −x2)J1(x) −x−2(24 −x2)J0(x).
14. Show that
J ′
2(x) = 2x−1J0(x) + x−2(4 −x2)J ′
0(x).
15. Show that
(a) J2(x) = J0(x) + 2J ′′
0 (x).
(b) J3(x) = 3J1(x) + 4J ′′
1 (x).
16. (a) Verify that y(x) = x J0(x) is a solution to the
differential equation
y′′ + y = −J1(x),
x > 0.
(11.6.36)
(b) Use the result from (a) to establish that
/ x
0
cos(x −t)J0(t) dt = x J0(x),
x > 0.
[Hint: Use Equation (8.7.15) and (8.7.16) to
construct a particular solution to (11.6.36)
17. Determine the Fourier-Bessel expansion in the func-
tions Jp(λnx) for f (x) = x p, on the interval (0, 1).
[Here the λn denote the positive zeros of Jp(x). Hint:
You will need to use one of the results from Prob-
lem 11.]
18. Determine the Fourier-Bessel expansion in the func-
tions J0(λkx) of f (x) = x2 on the interval (0, 1).
[Here the λk denote the positive zeros of J0(x).]
19. Let Jp(x) denote the Bessel function of the ﬁrst kind of
order p, and let λ be a positive real number. If u(x) =
Jp(λx), show that u satisﬁes the differential equation
d2u
dx2 + 1
x
du
dx +
%
λ2 −p2
x2
&
u = 0.
20. Let λ and µ be positive real numbers. Then Jp(λx)
and Jp(µx) satisfy
d
dx
0
x d
dx [Jp(λx)]
3
+
%
λ2x −p2
x
&
Jp(λx) = 0,
(11.6.37)
d
dx
0
x d
dx [Jp(µx)]
3
+
%
µ2x −p2
x
&
Jp(µx) = 0,
(11.6.38)
respectively.
(a) Show that for λ ̸= µ,
/ 1
0
x Jp(λx)Jp(µx)dx
=
µJp(λ)J ′
p(µ) −λJp(µ)J ′
p(λ)
λ2 −µ2
.
(11.6.39)
[Hint: Multiply (11.6.37) by Jp(µx), (11.6.38)
by Jp(λx), subtract the resulting equations and
integrate over (0, 1).] If λ and µ are distinct zeros
of Jp(x), what does your result imply?
(b) In order to compute
5 1
0 x[Jp(µx)]2dx, we take
the limit as λ →µ in (11.6.39). Use L’Hopital’s
rule to compute this limit and thereby show that
/ 1
0
x[Jp(µx)]2dx
=
µ[J ′
p(µ)]2 −Jp(µ)J ′
p(µ) −µJp(µ)J ′′
p(µ)
2µ
.
(11.6.40)

11.7
Chapter Review 785
Substituting from Bessel’s equation for J ′′
p(µ),
show that (11.6.40) can be written as
/ 1
0
x[Jp(µx)]2dx
= 1
2
0
[J ′
p(µ)]2 +
%
1 −p2
µ2
&
[Jp(µ)]2
3
.
(c) In the case when µ is a zero of Jp(x), use
(11.6.26) to show that your result in (b) can be
written as
/ 1
0
x[Jp(µx)]2dx = 1
2[Jp+1(µ)]2.
11.7
Chapter Review
In this chapter our focus has been on second-order homogeneous nonconstant coefﬁcient
differential equations of the form
y′′ + p(x)y′ + q(x)y = 0,
(11.7.1)
for x ∈I. In general it is not possible to determine two linearly independent solutions
to such a differential equation in closed form, and therefore, our approach has been to
try to represent solutions in the form of some type of convergent inﬁnite series.
Series Solution about an Ordinary Point
The simplest situation occurs when both of the functions p and q in (11.7.1) are analytic
at x0 ∈I, which means that they can each be represented as power series centered at
x = x0 with a radius of convergence denoted by R. We say that x0 is an ordinary point
of the differential equation (11.7.1). In such a case it is possible to expand the solutions
to (11.7.1) in terms of a convergent power series of the form
y(x) =
∞
!
n=0
an(x −x0)n.
(11.7.2)
Substitution of this expression for y into (11.7.1) and matching coefﬁcients on each side
of the differential equation leads to a recurrence relation on the coefﬁcients. Therefore,
all coefﬁcients an can be expressed in terms of the initial coefﬁcients a0, a1, . . . . In fact,
assuming that a0 and a1 are nonzero, we obtain the general solution in the form
y(x) = a0y1(x) + a1y2(x).
where y1 and y2 are linearly independent solutions to (11.7.1). This solution is valid for
(at least) all x with |x −x0| < R.
Legendre’s Equation
A particularly important example of a differential equation of the form (11.7.1) is the
Legendre equation
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0,
−1 < x < 1.
(11.7.3)
Since the functions
p(x) = −
2x
1 −x2 ,
q(x) = α(α + 1)
1 −x2
both have power series expansions about x = 0 with radius of convergence R = 1,
it follows that we can obtain two linearly independent power series solutions to the

786
CHAPTER 11
Series Solutions to Linear Differential Equations
Legendre equation which are also valid for (at least) |x| < 1. The key results that we
have established about the solutions to the Legendre equation are as follows:
1. If α = N a nonnegative integer then the Legendre equation has solutions (unique
up to a multiplicative constant) that are polynomials of degree N. Imposing the
conditon that y(1) = 1 yields the degree N Legendre polynomial denoted PN(x).
2. The set of all Legendre polynomials is an orthogonal set of functions on the interval
[−1, 1]; that is,
/ 1
−1
PM(x)PN(x) dx = 0,
whenever M ̸= N.
3. If f and f ′ are continuous on the interval (−1, 1), then for −1 < x < 1,
f (x) =
∞
!
n=0
an Pn(x),
where
an = 2n + 1
2
/ 1
−1
f (x)Pn(x) dx.
Series Solution about a Regular Singular Point
We now rewrite (11.7.1) in the form
y′′ + P(x)y′ + Q(x)y = 0.
(11.7.4)
If P(x) and/or Q(x) fail to be analytic at x0 ∈I then x0 is called a singular point of
the differential equation (11.7.1). In this chapter we have considered the special type of
singular point that arises when both of the functions
p(x) = (x −x0)P(x)
and
q(x) = (x −x0)2Q(x)
are analytic at x = x0. In such a case the point x0 is called a regular singular point.
Without loss of generality we can restrict our attention to the case when x0 = 0, in
which case (11.7.4) can be written as
x2y′′ + xp(x)y′ + q(x)y = 0,
(11.7.5)
where p and q are assumed to be analytic at x = 0. We let R denote the smaller of the
radii of convergence of the power series expansions of p(x) and q(x) about x = 0. If r1
and r2 denote the roots of the indicial equation
r(r −1) + p(0)r + q(0) = 0,
(11.7.6)
with r1 ≥r2 if these roots are real, then we have seen in this chapter that two linearly
independent solutions to (11.7.5) on the interval (0, R) can be determined as follows.
1. If r1 −r2 ̸= integer, then
y1(x) = xr1
∞
!
n=0
anxn,
a0 ̸= 0,
y2(x) = xr2
∞
!
n=0
bnxn,
b0 ̸= 0.
2. If r1 = r2 = r, then
y1(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
y2(x) = y1(x) ln x + xr
∞
!
n=1
bnxn.

11.7
Chapter Review 787
3. If r1 −r2 = positive integer, then
y1(x) = xr
∞
!
n=0
anxn,
a0 ̸= 0,
y2(x) = Ay1(x) ln x+xr2
∞
!
n=0
bnxn,
b0 ̸= 0.
The coefﬁcients in each of these solutions can be determined by direct substitution
into (11.7.5).
Bessel’s Equation of Order p
An important differential equation that has a regular singular point at x = 0 is Bessel’s
equation of order p:
x2y′′ + xy′ + (x2 −p2)y = 0,
x > 0,
(11.7.7)
where p is a nonnegative constant. This differential equation has general solution
y(x) = c1Jp(x) + c2Yp(x),
where Jp and Yp denote the Bessel functions of the ﬁrst and second kinds of order
p respectively. Our major interest is in the Bessel functions of the ﬁrst kind. We let
λ1, λ2, . . . , denote the positive zeros of Jp(x), p ≥0, and consider the corresponding
functions Jp(λnx). The key results about these functions are as follows:
1. The set of functions
9
Jp(λnx)
:∞
n=1 is orthogonal on the interval (0, 1) relative to
the weight function w(x) = x; that is,
/ 1
0
x Jp(λmx)Jp(λnx) dx = 0,
whenever m ̸= n.
2. If f and f ′ are continuous on the interval [0, 1] then for 0 < x < 1,
f (x) =
∞
!
n=1
an Jp(λnx),
where
an =
2
'
Jp+1(λnx)
(2
/ 1
0
x f (x)Jp(λnx) dx.
Additional Problems
For Problems 1–13 determine whether x = 0 is an ordi-
nary point or a regular singular point of the given differential
equation. Then obtain two linearly independent solutions to
the differential equation and state the maximum interval on
which your solutions are valid.
1. y′′ + xy = 0.
2. y′′ −x2y = 0.
3. (1 −x2)y′′ −6xy′ −4y = 0.
4. xy′′ + y′ + 2y = 0.
5. xy′′ + 2y′ + xy = 0.
6. 2xy′′ + 5(1 −2x)y′ −5y = 0.
7. xy′′ + y′ + xy = 0.
8. (1 + 4x2)y′′ −8y = 0.
9. x2y′′ + xy′ +
;
x2 −1
4
<
y = 0.
10. 4xy′′ + 3y′ + 3y = 0.
11. x2y′′ + 3
2 xy′ −1
2(1 + x)y = 0.
12. x2y′′ −x(2 −x)y′ + (2 + x2)y = 0.

788
CHAPTER 11
Series Solutions to Linear Differential Equations
13. x2y′′ −3xy′ + 4(x + 1)y = 0.
14. Consider the hypergeometric equation
x(1 −x)y′′ + [c −(a + b + 1)x]y′ −aby = 0,
(11.7.8)
where a, b, c are constants.
(a) Verify that x = 0 is a regular singular point of
(11.7.8).
(b) Verify that the roots of the indicial equation are
r1 = 0,r2 = 1 −c.
(c) Show that the coefﬁcients in a series solution to
Equation (11.7.8) centered at x = 0 must satisfy
the recurrence relation:
an+1 = (n + r + a)(n + r + b)
(n + r + 1)(n + r + c) an,
n = 0, 1, . . . .
(d) Assuming that c is not an integer, derive the fol-
lowing solutions to (11.7.8):
y1(x) = F(a, b, c; x) = 1 + ab
c x + a(a + 1)b(b + 1)
c(c + 1)
x2
2!
+ a(a + 1) · · · (a + n −1)b(b + 1) · · · (b + n −1)
(c + 1) · · · (c + n −1)
xn
n!
+ · · · , y2(x) = x1−cF(a + 1 −c, b + 1 −c, 2 −c; x).
15. Consider the differential equation
(x2 −1)y′′ + [1 −(a + b)]xy′ + aby = 0, (11.7.9)
where a and b are constants.
(a) Show that the coefﬁcients in a series solution to
Equation (11.7.9) centered at x = 0 must satisfy
the recurrence relation
an+2 = (n −a)(n −b)
(n + 2)(n + 1)an, n = 0, 1, . . . ,
and determine two linearly independent series
solutions.
(b) Show that if either a or b is a nonnegative inte-
ger, then one of the solutions obtained in (a) is a
polynomial.
(c) Show that if a is an odd positive integer and b is
an even positive integer, then both of the solutions
deﬁned in (a) are polynomials.
(d) If a = 5 and b = 4, determine two linearly
independent polynomial solutions to Equation
(11.7.9). Notice that in this case the radius of
convergence of the solutions obtained is R = ∞,
whereas Theorem 11.2.1 only guarantees a radius
of convergence R ≥1.
16. Consider the Chebyshev equation
(1 −x2)y′′ −xy′ + a2y = 0,
(11.7.10)
where a is a constant.
(a) Show that if a = N, a nonnegative integer, then
Equation (11.7.10) has a polynomial solution of
degree N. When suitably normalized, these poly-
nomials are called the Chebyshev polynomials
and are denoted by TN(x).
(b) Use Equation (11.7.10) to show that TN(x)
satisﬁes
[
$
1 −x2T ′
N]′ +
N
√
1 −x2 TN = 0.
(c) Use the result from (b) to prove that
/ 1
−1
TN(x)TM(x)
√
1 −x2
dx = 0,
M ̸= N.
17. Consider the differential equation
x2y′′ + x(1 + 2N −x)y′ + N 2y = 0,
x > 0.
(11.7.11)
(a) Find the indicial equation, and show that it has
only one root r = −N.
(b): If N is a nonnegative integer, show that the
Frobenius series solution to Equation (11.7.11)
terminates after N terms.
(c) Determine the Frobenius series solutions when
N = 0, 1, 2, 3.
(d) Show that if N is a positive integer, then the
Frobenius series solution to Equation (11.7.11)
can be written as
y(x) =
x−N
)
1 +
N
!
k=1
(−1)k N(N −1) · · · (N + 1 −k)
12 · 22 · · · k2
xk
*
= x−N
)
1 +
N
!
k=1
(−1)k
k=
i=1
(N + 1 −i)
i2
xk
*
.

11.7
Chapter Review 789
18. Consider
the
general
“perturbed”
Cauchy-Euler
equation
x2y′′ + x[1 −(a + b) + cx]y′ + (ab + dx)y = 0, x > 0,
(11.7.12)
where a, b, c, d are constants. Assuming that a and b
are distinct and do not differ by an integer, determine
two linearly independent Frobenius series solutions to
Equation (11.7.12). [Hint: Use symmetry to get the
second solution.]
19. Consider the differential equation
x2y′′ + x(1 + bx)y′ + [b(1 −N)x −N 2]y = 0,
x > 0,
(11.7.13)
where N is a positive integer and b is a constant.
(a) Show that the roots of the indicial equation are
r = ±N.
(b) Show that the Frobenius series solution corre-
sponding to r = N is
y1(x) = a0x N
∞
!
n=0
(2N)!(−b)n
(2N + n)! xn
and that by an appropriate choice of a0, one so-
lution to (11.7.13) is
y1(x) = x−N
)
e−bx −
2N−1
!
n=0
(−bx)n
n!
*
.
(c) Show that Equation (11.7.13) has a second lin-
early independent Frobenius series solution that
can be taken as
y2(x) = x−N
2N−1
!
n=0
(−bx)n
n!
.
Hence, conclude that Equation (11.7.13) has lin-
early independent solutions
y1(x) = x−Ne−bx, y2(x) = x−N
2N−1
!
n=0
(−bx)n
n!
.
20. Show that the change of variables y = x1/2u trans-
forms the differential equation
y′′ +
%
1 −
3
4x2
&
y = 0
into the Bessel equation
x2u′′ + xu′ + (x2 −1)u = 0
and thereby write the general solution to the given dif-
ferential equation.
Project: The Aging Spring
Recall from Equation (8.5.7) that the differential equation governing the free oscillations
of a spring mass system without friction is
d2y
dt2 + ω2
0y = 0,
where
ω0 =
8
k
m ,
k (spring constant) and m (mass) are positive constants, and y(t) represents the displace-
ment of the mass from equilibrium at time t. As a spring ages it will lose its “springiness”
and the restoring force will be a decreasing function of time. In order to model this behav-
ior we assume that the spring’s restoring force is proportional to a decaying exponential
function of time multiplied by the displacement of the spring from its equilibrium. This
leads to the differential equation
d2y
dt2 + ω2
0e−at y = 0,
(11.7.14)
where a is a positive constant. One approach to solving this differential equation is to
replace the exponential term by its Maclaurin series representation and then look for two

790
CHAPTER 11
Series Solutions to Linear Differential Equations
linearly independent power series solutions. However, the exponential time dependence
in the coefﬁcient of y implies that if we change the independent variable to
τ = αe−βt,
(11.7.15)
where α and β are positive constants, then the resulting differential equation will contain
powers of τ, rather than exponential functions. This may enable an appropriate choice
of α and β to be made that will result in a differential equation that can more easily
be solved.
1. Using the chain rule, show that the change of variables (11.7.15) transforms the
differential equation (11.7.14) into
τ 2 d2y
dτ 2 + τ dy
dτ +
%ω0
β
&2 6τ
α
7a/β
y = 0.
(11.7.16)
2. Determine the values of α and β that reduce (11.7.16) to the Bessel equation of
order zero.
With the choice of α and β determined in Part 2, the general solution to (11.7.16)
is given in (11.6.20) by
y(τ) = c2J0(τ) + c2Y0(τ),
or, equivalently,
y(t) = c1J0(Ae−at/2) + c2Y0(Ae−at/2),
A = 2ω0
a .
3. If c2 ̸= 0, describe the behavior of the spring-mass system as t →∞.
4. Assume c2 = 0, and let λ1 < λ2 < . . . denote the positive zeros of J0.
(a) Show that the system will pass through the equilibrium at times t = tn where
tn = 2
a ln
% A
λn
&
,
n = 1, 2, . . . , p,
and λp is the largest zero of J0 such that
λp < A.
(b) Show that
lim
t→∞y(t) = c1.
(c) In the particular case when A = 14, a = 1
5, c1 = 2, make a sketch of y as
a function of t. Use the following approximations to the ﬁrst ﬁve positive
zeros of J0:
λ1 = 2.405,
λ2 = 5.520,
λ3 = 8.654,
λ4 = 11.792,
λ5 = 14.931.

A
Review of Complex
Numbers
Any number z of the form z = a + ib, where a and b are real numbers and i = √−1,
is called a complex number. If z = a + ib, then we refer to a as the real part of z,
denoted Re(z), and we refer to b as the imaginary part of z, denoted Im(z). Thus
If z = a + ib,
then Re(z) = a and Im(z) = b.
Example A.1
If z = 2 −3i, then Re(z) = 2 and Im(z) = −3.
□
Complex numbers can be added, subtracted, and multiplied in the usual manner,
and the result is once more a complex number. Further, these operations satisfy all of
the basic properties satisﬁed by the real numbers. All that we need to remember is that
whenever we encounter the term i2, it must be replaced by −1.
Example A.2
If z1 = 3 + 4i and z2 = −1 + 2i, ﬁnd z1 −3z2 and z1z2.
Solution:
We have
z1 −3z2 = (3 + 4i) −3(−1 + 2i) = 6 −2i = 2(3 −i)
and
z1z2 = (3 + 4i)(−1 + 2i) = −3 + 6i −4i + 8i2 = −11 + 2i.
□
Example A.3
If z1 = 4 + 3i and z2 = 4 −3i, determine z1z2.
Solution:
In this case, z1z2 = (4+3i)(4−3i) = 16−12i +12i −9i2 = 16+9 = 25.
□
Notice that in the previous example the product z1z2 turned out to be a real number.
This was not an accident. If we look at the deﬁnition of z2, we see that it can be obtained
from z1 by replacing the imaginary part of z1 by its negative. Complex numbers that are
related in this manner are called conjugates of one another.
791

792
APPENDIX A
Review of Complex Numbers
DEFINITION
A.4
If z = a + ib, then the complex number z deﬁned by
z = a −ib
is called the conjugate of z.
Example A.5
If z = 2 + 5i and z = 2 −5i, whereas if z = 3 −4i, then z = 3 + 4i.
□
Properties of the Conjugate
1. z = z.
2. zz = zz = a2 + b2.
Proof
1. If z = a + ib, then z = a −ib, so that z = a + ib = z.
2. We have zz = (a + ib)(a −ib) = a2 −iab + iab −(ib)2 = a2 + b2.
If z = a + ib, then the real number
√
a2 + b2 is often called the modulus of z or
the absolute value of z and is denoted |z|. It follows from Property 2 that
|z|2 = zz.
Example A.6
Determine |z| if z = 2 −3i.
Solution:
By deﬁnition,
|z| =
!
22 + (−3)2 =
√
13.
□
We now recall from elementary algebra that an expression of the form
1
a +
√
b
can
always be written with the radical in the numerator. To accomplish this, we multiply by
a −
√
b
a −
√
b
,
and the result is
a −
√
b
a2 −b .
The reason that this works is because
(a +
√
b)(a −
√
b) = a2 −b.
This is similar to (a + ib)(a −ib) = a2 + b2. Now consider an expression of the form
1
a + ib.
As this is written, we cannot say that it is a complex number, since it is not of the form
a + ib. However, if we multiply by
a −ib
a −ib

Appendix A
Review of Complex Numbers 793
and use Property 2 of the conjugate, we obtain
1
a + ib =
1
(a + ib)
(a −ib)
(a −ib) = a −ib
a2 + b2 ,
which is a complex number.
Example A.7
Express z =
1
2 + 5i in the form a + ib.
Solution:
We have
z =
1
2 + 5i =
1
(2 + 5i)
(2 −5i)
(2 −5i) = 2
29 −5
29i.
□
More generally, if z1 = a + ib and z2 = x + iy, then
z1
z2
= a + ib
x + iy = (a + ib)
(x + iy)
(x −iy)
(x −iy) =
1
x2 + y2 [(ax + by) + i(ay + bx)].
This illustrates that we can divide two complex numbers, and the result is once more a
complex number.
Example A.8
If z1 = 2 + 3i and z2 = 3 + 4i, determine z1
z2
.
Solution:
In this case, we have
z1
z2
= 2 + 3i
3 + 4i = (2 + 3i)
(3 + 4i)
(3 −4i)
(3 −4i) = 1
25(18 + i).
□
Complex-Valued Functions
A function w(x) of the form
w(x) = u(x) + iv(x),
where u and v ar real-valued functions of a real variable x (and i2 = −1), is called a
complex-valued function of a real variable. An example of such a function is
w(x) = 3 cos 2x + 4i sin 3x.
The Complex Exponential Function: Recall that for all real x, the function ex has the
Maclaurin expansion
ex =
∞
"
n=0
1
n!xn.
It is also possible to discuss convergence of inﬁnite series of complex numbers. We
deﬁne eib, where b is a real number, by
eib =
∞
"
n=0
1
n!(ib)n = 1 + ib + 1
2!(ib)2 + 1
3!(ib)3 + · · · + 1
n!(ib)n + · · · .
Factoring the even and odd powers of b and using the formulas
i2k = (−1)k
and
i2k+1 = (−1)ki

794
APPENDIX A
Review of Complex Numbers
yields
eib =
#
1 −1
2!b2 + 1
4!b4 + · · · + (−1)k
(2k)! b2k + · · ·
$
+ i
#
b −1
3!b3 + 1
5!b5 −· · · +
(−1)k
(2k + 1)!b2k+1 + · · ·
$
.
That is,
eib =
∞
"
n=0
(−1)n b2n
(2n)! + i
∞
"
n=0
(−1)n
b2n+1
(2n + 1)!.
The two series appearing the foregoing equation are, respectively, the Maclaurin series
expansions of cos b and sin b, both of which converge for all real b. Thus, we have shown
that
eib = cos b + i sin b,
(A.1)
which is called Euler’s formula. It is now natural to deﬁne ea+ib by
ea+ib = ea · eib = ea(cos b + i sin b),
(A.2)
where a and b are any real numbers.
A function of the form f (x) = erx, where r = a + ib and x is a real variable, is
called a complex exponential function. Replacing ib with ibx in (A.1) and a +ib with
(a + ib)x in (A.2) yields the following important formulas:
eibx = cos bx + i sin bx,
e(a+ib)x = eax(cos bx + i sin bx).
By replacing i with −i in the foregoing formulas, we obtain
e−ibx = cos bx −i sin bx, e(a−ib)x = eax(cos bx −i sin bx).
Example A.9
Express e(3−5i)x in terms of trigonometric functions.
Solution:
We have
e(3−5i)x = e3x(cos 5x −i sin 5x).
□
The preceding deﬁnition of e(a+ib)x also enables us to attach a meaning to x(a+ib).
We recall that for a nonrational number r and x > 0, xr is deﬁned by xr = er ln x. We
now extend this deﬁnition to the case when r is complex and therefore deﬁne
xa+ib = e(a+ib) ln x.
Using Euler’s formula, this can be written as
xa+ib = xaeib ln x = xa[cos(b ln x) + i sin(b ln x)].
For example,
x2+3i = x2[cos(3 ln x) + i sin(3 ln x)].

Appendix A
Review of Complex Numbers 795
Differentiation of Complex-Valued Functions
We now return to the general complex-valued function w(x) = u(x) + iv(x). If u′(x)
and v′(x) exist, then we deﬁne the derivative of w by
w′(x) = u′(x) + iv′(x).
Higher-order derivatives are deﬁned similarly. In particular, we have the following im-
portant result:
d
dx (erx) = rerx
when r is complex.
This coincides with the usual formula for the derivative of erx when r is a real number.
To establish the above formula, we proceed as follows. If r = a + ib, then
erx = e(a+ib)x = eax(cos bx + i sin bx).
Differentiating with respect to x using the product rule yields
d
dx (erx) = aeax(cos bx + i sin bx) + beax(−sin bx + i cos bx)
= aeax(cos bx + i sin bx) + ibeax(cos bx + i sin bx)
= (a + ib)eax(cos bx + i sin bx) = rerx,
as required.
Similarly, it can be shown that
d
dx (xr) = rxr−1
when r is complex.
Exercises for A
Problems
For Problems 1–5, determine z and |z| for the given complex
number.
1. z = 2 + 5i
2. z = 3 −4i
3. z = 5 −2i
4. z = 7 + i
5. z = 1 + 2i
For Problems 6–10, express z1z2 and z1/z2 in the form a+ib.
6. z1 = 1 + i, z2 = 3 + 2i.
7. z1 = −1 + 3i, z2 = 2 −i.
8. z1 = 2 + 3i, z2 = 1 −i.
9. z1 = 4 −i, z2 = 1 + 3i.
10. z1 = 1 −2i, z2 = 3 + 4i.
11. Show that if z1 and z2 are complex numbers, then
z1 + z2 = z1 + z2.
12. Generalize the previous example to the case when
z1, z2, . . . , zn are complex numbers.
13. Show that if z1 and z2 are complex numbers then
z1z2 = z1z2.
14. Show that if z1 and z2 are complex numbers then
(z1/z2) = z1/z2.

796
APPENDIX A
Review of Complex Numbers
ForProblems15–22,expressthegivencomplex-valuedfunc-
tion in the form u(x) + iv(x) for appropriate real-valued
functions u and v.
15. e2ix.
16. e(3+4i)x.
17. e−5ix.
18. e−(2+i)x.
19. x2−i.
20. x3i.
21. x−1+2i.
22. x2ie(3+4i)x.
23. Derive the famous mathematical formula
eiπ + 1 = 0.
24. Show that
cos bx = 1
2(eibx + e−ibx)
and
sin bx = 1
2i (eibx −e−ibx).
(A comparison of these formulas with the correspond-
ing formulas
cosh bx = 1
2(ebx + e−bx)
and
sinh bx = 1
2(ebx −e−bx)
indicates why the trigonometric and hyperbolic func-
tions satisfy similar identities.)
For Problems 25–27, use the result of Problem 24 to express
the given functions in terms of complex exponential func-
tions.
25. sin 4x.
26. cos 8x.
27. tan x.
28. Use the result of Problem 24 to verify the identity
sin2 x + cos2 x = 1.

B
Review of Partial
Fractions
In this appendix, we review the partial fraction decomposition of rational functions.
Recall that a function of the form
p(x) = anxn + an−1xn−1 + · · · + a1x + a0
(B.1)
with an ̸= 0 is called a polynomial of degree n. According to the fundamental theorem
of algebra, the equation p(x) = 0 has precisely n roots (not all necessarily distinct). If
we let x1, x2, . . . , xn denote these roots, then p(x) can be factored as
p(x) = K(x −x1)(x −x2) · · · (x −xn),
(B.2)
where K is a constant. Some of the roots may be complex. We will assume that the
coefﬁcients in (B.1) are real numbers, in which case any complex roots must occur in
conjugate pairs.
A quadratic factor of the form ax2 + bx + c (where a ̸= 0) that has no real linear
factors is said to be irreducible.
Theorem B.1
Any real polynomial1 can be factored into linear and irreducible quadratic terms with
real coefﬁcients.
Proof Let p(x) be a real polynomial and suppose that x = α is a complex root of
p(x) = 0. Then x = α is also a root. Thus, (B.2) will contain the terms (x −α)(x −α).
These linear terms have complex coefﬁcients. However, if we expand the product the
result is
(x −α)(x −α) = x2 −(α + α)x + αα.
But,
α + α = 2Re(α) and αα = |α|2
are both real, so that the irreducible quadratic term does indeed have real coefﬁcients.
1By a “real polynomial” we mean a polynomial with real coefﬁcients.
797

798
APPENDIX B
Review of Partial Fractions
If p(x) and q(x) are two polynomials (not necessarily of the same degree) then a function
of the form
R(x) = p(x)
q(x)
is called a rational function. Suppose that q(x) has been factored into linear and irre-
ducible quadratic terms. Then q(x) will consist of a product of terms of the form
(ax −b)k
or
(ax2 + bx + c)k,
(B.3)
where a, b, c, and k are constants. For example,
x2 −1
(x + 2)(x2 + 3)
is a factorized rational function. The idea behind a partial fraction decomposition is to
express a rational function as a sum of terms whose denominators are of the form (B.3).
The following rules tell us the form that such a decomposition must take.
1. Each factor of the form (ax −b)k in q(x) contributes the following terms to the
partial fraction decomposition of p(x)/q(x):
A1
(ax −b) +
A2
(ax −b)2 + · · · +
Ak
(ax −b)k ,
where A1, A2, . . . , Ak are constants.
2. Each irreducible quadratic factor of the form (ax2 + bx + c)k contributes the
following terms to the partial fraction decomposition of p(x)/q(x):
A1x + B1
ax2 + bx + c +
A2x + B2
(ax2 + bx + c)2 + · · · +
Akx + Bk
(ax2 + bx + c)k .
Thus, for example,
x2 + 1
x(x −1)(x2 + 4) = A
x +
B
x −1 + Cx + D
x2 + 4
for appropriate values of the constants A, B, C, D. Similarly,
x −2
(x + 2)2(x2 + 2x + 2) =
A
x + 2 +
B
(x + 2)2 +
Cx + D
(x2 + 2x + 2)
for appropriate values of the constants A, B, C, D.
The preceding rules only give the form of a partial fraction decomposition. The next
question that needs answering is the following: How do we determine the constants that
arise in the partial fraction decomposition? A standard way to proceed is as follows:
1. Determine the general form of the partial fraction decomposition of
p(x)/q(x).
2. Multiply both sides of the resulting decomposition by q(x).
3. Equate the coefﬁcients of like powers of x on both sides of the
resulting equation in order to determine the constants in the partial
fraction decomposition.

Appendix B
Review of Partial Fractions 799
We illustrate the procedure with several examples.
Example B.2
Determine the partial fraction decomposition of
2x
(x −1)(x + 3).
Solution:
The general form of the partial fraction decomposition is
2x
(x −1)(x + 3) =
A
x −1 +
B
x + 3.
Multiplying both sides of this equation by (x −1)(x + 3) yields
2x = A(x + 3) + B(x −1).
We now equate coefﬁcients of like powers of x on both sides of this equation to obtain
A + B = 2
and
3A −B = 0.
Consequently,
A = 1
2
and
B = 3
2,
so that
2x
(x −1)(x + 3) =
1
2(x −1) +
3
2(x + 3).
□
Example B.3
Determine the partial fraction decomposition of
x2 + 1
(x + 1)(x2 + 4).
Solution:
In this case the general form of the partial fraction decomposition is
x2 + 1
(x + 1)(x2 + 4) =
A
x + 1 + Bx + C
x2 + 4 .
Multiplying both sides by (x + 1)(x2 + 4) we obtain:
x2 + 1 = A(x2 + 4) + (Bx + C)(x + 1).
Equating coefﬁcients of like powers of x on both sides of this equality yields
A + B = 1, B + C = 0, 4A + C = 1.
Solving this system of equations, we obtain
A = 2
5,
B = 3
5,
C = −3
5,
so that
x2 + 1
(x + 1)(x2 + 4) =
2
5(x + 1) + 3(x −1)
5(x2 + 4).
□

800
APPENDIX B
Review of Partial Fractions
Example B.4
Determine the partial fraction decomposition of
2x −1
(x + 2)2(x2 + 2x + 2).
Solution:
The term x2 + 2x + 2 is irreducible. Thus, the partial fraction decompo-
sition has the general form:
2x −1
(x + 2)2(x2 + 2x + 2) =
A
x + 2 +
B
(x + 2)2 +
Cx + D
x2 + 2x + 2.
Clearing the fractions yields
2x −1 = A(x + 2)(x2 + 2x + 2) + B(x2 + 2x + 2) + (Cx + D)(x + 2)2.
Equating the coefﬁcients of like powers of x we obtain
A +
C
=
0,
4A +
B
+ 4C +
D
=
0,
6A + 2B
+ 4C + 4D
=
2,
4A + 2B
+ 4D
= −1.
Solving this system yields
A = −3
2, B = −5
2, C = 3
2, D = 5
2.
Consequently,
2x −1
(x + 2)2(x2 + 2x + 2) =
3x + 5
2(x2 + 2x + 2) −
3
2(x + 2) −
5
2(x + 2)2 .
□
Some Shortcuts
The preceding technique for determining the constants that arise in the partial fraction
decomposition of a rational function will always work. However, in practice it is often
tedious to apply. We now present, without justiﬁcation, some shortcuts that can circum-
vent many of the computations.
1. Linear Factors: The Cover-up Rule
If q(x) contains a linear factor x −a, then this factor contributes a term of the form
A
x −a
to the partial fraction decomposition of p(x)/q(x). Let P(x) denote the expres-
sion obtained by omitting the x −a term in p(x)/q(x). Then the constant A is
given by
A = P(a).
Example B.5
Determine the partial fraction decomposition of
3x −1
(x −3)(x + 2).

Appendix B
Review of Partial Fractions 801
Solution:
The general form of the decomposition is
3x −1
(x −3)(x + 2) =
A
x −3 +
B
x + 2.
To determine A, we neglect the x −3 term in the given rational function and set
P(x) = 3x −1
x + 2 .
Then, according to the preceding rule,
A = P(3) = 8
5.
Similarly, to determine B we neglect the x + 2 term in the given function, and set
P(x) = 3x −1
x −3 .
Using the cover-up rule, it then follows that
B = P(−2) = −7
−5 = 7
5.
Consequently,
3x −1
(x −3)(x + 2) =
8
5(x −3) +
7
5(x + 2).
The idea behind the technique is to cover up the linear factor x −a in the given
rational function and set x = a in the remaining part of the function. The result
will be the constant A in the contribution A/(x −a) to the partial fraction decom-
position of the rational function.
□
2. Repeated Linear Factors: The cover-up rule can be extended to the case of repeated
linear factors also. Suppose that q(x) contains a factor of the form (x −a)k. Then
this contributes the terms
A1
(ax −b) +
A2
(ax −b)2 + · · · +
Ak
(ax −b)k
to the partial fraction decomposition of p(x)/q(x). Let P(x) be the expression
obtained when the (x −a)k term is neglected in p(x)/q(x). Then the constants
A1, A2, . . . , Ak are given by
Ak = P(a),
Ak−1 = P′(a),
Ak−2 = 1
2! P′′(a),
. . . ,
A1 =
1
(k −1)! P(k−1)(a)
where a ′ symbol denotes differentiation with respect to x.
Remarks
1. The above formulae look rather formidable to begin with. However, they are
easy to apply in practice.
2. Notice that in the case k = 1 we are back to the cover-up rule.

802
APPENDIX B
Review of Partial Fractions
Example B.6
Determine the partial fraction decomposition of
x
(x −1)(x + 2)2 .
Solution:
The general form of the partial fraction decomposition is
x
(x −1)(x + 2)2 =
A1
x + 2 +
A2
(x + 2)2 +
A3
x −1.
To determine A1 and A2 we omit the term (x + 2)2 in the given function to obtain
P(x) =
x
x −1.
Applying the above rule with k = 2 yields
A2 = P(−2) = 2
3, A1 = P′(−2) = −
1
(x −1)2
!!!!
x=−2
= −1
9.
We now use the cover-up rule to determine A3. Neglecting the x −1 term in the
given function and setting x = 1 in the resulting expression yields A3 = 1
9. Thus,
x
(x −1)(x + 2)2 = −
1
9(x + 2) +
2
3(x + 2)2 +
1
9(x −1).
3. Irreducible Quadratic Factors of the Form x2 + a2: The ﬁnal case that we will
consider is when q(x) contains a factor of the form x2 + a2. This will contribute
a term
Ax + B
x2 + a2
to the partial fraction decomposition of p(x)/q(x). Let P(x) be the expression
obtained by deleting the term x2 + a2 in p(x)/q(x). Then the constants A and B
are given by
A = 1
a Im[P(ia)],
B = Re[P(ia)].
□
Example B.7
Determine the partial fraction decomposition of
x −1
(x + 2)(x2 + 4).
Solution:
In this case the general form of the partial fraction decomposition is
x −1
(x + 2)(x2 + 4) = Ax + B
x2 + 4 +
C
x + 2.
In order to determine A and B, we delete the x2 + 4 term from the given function
to obtain
P(x) = x −1
x + 2.

Appendix B
Review of Partial Fractions 803
Since in this case a = 2i, we ﬁrst compute
P(2i) = 2i −1
2i + 2 = 1
4(1 + 3i).
Thus,
A = 1
2Im
"1
4(1 + 3i)
#
= 3
8, B = Re
"1
4(1 + 3i)
#
= 1
4.
In order to determine C, we use the cover-up rule. Neglecting the x + 2 factor in
the given function and setting x = −2 in the result yields C = −3
8. Thus,
x −1
(x + 2)(x2 + 4) =
3x + 2
8(x2 + 4) −
3
8(x + 2).
□
Remark
These techniques can be extended to the case of irreducible factors of
the form (ax2 + bx + c)k.
Exercises for B
Problems
For Problems 1–18, determine the partial fraction decompo-
sition of the given rational function.
1.
2x −1
(x + 1)(x + 2).
2.
x −2
(x −1)(x + 4).
3.
x + 1
(x −3)(x + 2).
4.
x2 −x + 4
(x + 3)(x −1)(x + 2).
5.
2x −1
(x + 4)(x −2)(x + 1).
6.
3x2 −2x + 14
(2x −1)(x + 5)(x + 2).
7.
2x + 1
(x + 2)(x + 1)2 .
8.
5x2 + 3
(x + 1)(x −1)2 .
9.
3x + 4
x2(x2 + 4).
10.
3x −2
(x −5)(x2 + 1).
11.
x2 + 6
(x −2)(x2 + 16).
12.
10
(x −1)(x2 + 9).
13.
7x + 2
(x −2)(x + 2)2 .
14.
7x2 −20
(x −2)(x2 + 4).
15.
7x + 4
(x + 1)3(x −2).
16.
x(2x + 3)
(x + 1)(x2 + 2x + 2).
17.
3x + 4
(x −3)(x2 + 4x + 5).
18.
7 −2x2
(x −1)(x2 + 4).

C
Review of Integration
Techniques
In this appendix, we review some of the basic integration techniques that are required
throughout the text. This is a very brief refresher and should not be considered as a
substitute for a calculus text.
1. Integration by Parts. The basic formula for integration by parts can be written in
the form
!
u dv = uv −
!
v du.
To derive this, we start with the product rule for differentiation, namely
d
dx [u(x)v(x)] = dv
dx + v du
dx .
Integrating both sides of this equation with respect to x yields
u(x)v(x) =
! "
u dv
dx + v du
dx
#
dx
or, upon rearranging terms,
!
u dv
dx dx = uv −
!
v du
dx dx.
Consequently,
!
u dv = uv −
!
v du.
Example C.1
Evaluate
!
xe2x dx.
Solution:
Choosing
u = x
and
dv = e2xdx,
804

Appendix C
Review of Integration Techniques 805
it follows that
du
dx = 1
and
v = 1
2e2x,
so that
!
xe2x dx = 1
2 xe2x −1
2
!
e2x dx
= 1
2 xe2x −1
4e2x + c,
where c is an integration constant.
□
Example C.2
Evaluate
!
x2 sin x dx.
Solution:
In this case we take
u = x2
and
dv = sin x dx,
so that
du
dx = 2x
and
v = −cos x.
Thus,
!
x2 sin x dx = −x2 cos x + 2
!
x cos x dx.
We must now evaluate the second integral. Once more, integration by parts is
appropriate. This time we take
u = x
and
dv = cos x dx.
Then
du
dx = 1
and
v = sin x,
so that
!
x2 sin x dx = −x2 cos x + 2(x sin x −
!
sin x dx)
= −x2 cos x + 2(x sin x + cos x) + c.
□
As the previous two examples illustrate, the integration by parts technique is
extremely useful for evaluating integrals of the form
!
xk f (x) dx
when k is a positive integer. Such an integral can often be evaluated by successively
applying the integration by parts formula until the power of x is reduced to zero.
However, this will not always work.
Example C.3
Evaluate
!
x ln x dx.
Solution:
In this case, if we set u = x and dv = ln x dx, then we require the
integral of ln x. Instead we choose
u = ln x
and
dv = x dx.

806
APPENDIX C
Review of Integration Techniques
Then
du
dx = 1
x
and
v = 1
2 x2.
Applying the integration by parts formula, we obtain
!
x ln x dx = 1
2 x2 ln x −1
2
!
x dx
= 1
2 x2 ln x −1
4x2 + c
= 1
4x2(2 ln x −1) + c.
□
2. Integration by Substitution. This is one of the most important integration tech-
niques. Many of the standard integrals can be derived using a substitution. We
illustrate with some examples.
Example C.4
Evaluate the following integrals:
(a)
!
xex2dx. (b)
!
1
√
1 −x2 dx. (c)
! 1
x (ln x)2dx.
Solution:
(a) If we let u = x2, then du = 2x dx, so that
!
xex2dx = 1
2
!
eudu = 1
2eu + c = 1
2ex2 + c.
(b) Recalling that 1−sin2 θ = cos2 θ, the form of the integrand suggests that we
let x = sin θ. Then dx = cos θ dθ and the given integral can be written as
!
1
√
1 −x2 dx =
!
cos θ
$
1 −sin2 θ
dθ =
!
dθ = θ + c.
Substituting back for θ = sin−1 x, we obtain
!
1
√
1 −x2 dx = sin−1 x + c.
(c) In this case we recognize that the derivative of ln x is 1/x. This suggests that
we make the substitution
u = ln x,
so that
du = 1
x dx.
Then the given integral can be written in the form
! 1
x (ln x)2dx =
!
u2du = 1
3u3 + c.
Substituting back for u = ln x yields
! 1
x (ln x)2dx = 1
3(ln x)3 + c.
□

Appendix C
Review of Integration Techniques 807
Now consider the general integral
!
f ′(x)
f (x) dx.
If we let u = f (x), then du = f ′(x)dx, so that
!
f ′(x)
f (x) dx = 1
u du = ln |u| + c.
Substituting back for u = f (x) yields the important formula
!
f ′(x)
f (x) dx = ln | f (x)| + c.
Example C.5
Evaluate
!
x −2
x2 −4x + 3dx.
Solution:
If we rewrite the integral in the equivalent form
!
x −2
x2 −4x + 3dx = 1
2
!
2(x −2)
x2 −4x + 3dx,
then we see that the numerator in the second integral is the derivative of the
denominator. Thus
!
x −2
x2 −4x + 3dx = 1
2 ln |x2 −4x + 3| + c.
□
3. Integration by Partial Fractions. We can always evaluate an integral of the form
!
p(x)
q(x) dx
(C.1)
when p(x) and q(x) are polynomials in x. Consider ﬁrst the case when the degree
of p(x) is less than the degree of q(x). To evaluate (C.1) we ﬁrst determine the
partial fraction decomposition of the integrand (a review of partial fractions is
given in Appendix B). The result will always be integrable, although we might
need a substitution to carry out this integration.
Example C.6
Evaluate
!
3x + 2
(x + 1)(x + 2)dx.
Solution:
In this case we require the partial fraction decomposition of the in-
tegrand. Using the rules for partial fractions it follows that
3x + 2
(x + 1)(x + 2) =
A
x + 1 +
B
x + 2.
Multiplying both sides of this equality by (x + 1)(x + 2) yields
3x + 2 = A(x + 2) + B(x + 1).
Equating coefﬁcients of like powers of x on both sides of this equality, we obtain
A + B = 3
and
2A + B = 2.

808
APPENDIX C
Review of Integration Techniques
Solving for A and B yields
A = −1
and
B = 4.
Thus,
3x + 2
(x + 1)(x + 2) = −
1
x + 1 +
4
x + 2
so that
!
3x + 2
(x + 1)(x + 2)dx = −ln |x + 1| + 4 ln |x + 2| + c.
□
Example C.7
Evaluate
!
2x −3
(x −2)(x2 + 1)dx.
Solution:
Weﬁrstdeterminethepartialfractiondecompositionoftheintegrand.
From the general rules of partial fractions it follows that there are constants A, B,
and C such that
2x −3
(x −2)(x2 + 1) =
A
x −2 + Bx + C
x2 + 1 .
(C.2)
In order to determine the values of A, B, and C, we multiply both sides of (C.2)
by (x −2)(x2 + 1). This yields
2x −3 = A(x2 + 1) + (Bx + C)(x −2).
Equating coefﬁcients of like powers of x on both sides of this equality we obtain
A + B = 0
and
−2B + C = 2
and
A −2C = −3.
Solving for A, B, and C yields
A = 1
5
and
B = −1
5
and
C = 8
5,
so that
2x −3
(x −2)(x2 + 1) =
1
5(x −2) +
8 −x
5(x2 + 1).
Thus,
!
2x −3
(x −2)(x2 + 1)dx = 1
5
!
1
x −2dx + 8
5
!
1
x2 + 1dx −1
5
!
x
x2 + 1dx
= 1
5 ln |x −2| + 8
5 tan−1 x −1
10 ln(x2 + 1) + c.
□
Now return to the integral (C.1). If the degree of p(x) is greater than or equal to
the degree of q(x), then we ﬁrst divide q(x) into p(x). The resulting expression
will be integrable, although in general we will need to perform a partial fraction
decomposition.
Example C.8
Evaluate
! 2x −1
x + 3 dx.
Solution:
We ﬁrst divide the denominator into the numerator to obtain
2x −1
x + 3 = 2 −
7
x + 3.

Appendix C
Review of Integration Techniques 809
Thus,
! 2x −1
x + 3 dx =
!
2 dx −7
!
1
x + 3dx
= 2x −7 ln |x + 3| + c.
□
Example C.9
Evaluate
!
3x2 + 5
x2 −3x + 2dx.
Solution:
Once more, we must ﬁrst divide the denominator into the numerator.
It is easily shown that
3x2 + 5
x2 −3x + 2 = 3 +
9x −1
x2 −3x + 2.
(C.3)
The next step is to determine the partial fraction decomposition of the second term
on the right-hand side. We ﬁrst notice that the denominator can be factored as
(x −2)(x −1). Using the rules for partial fraction decomposition, it follows that
9x −1
(x −2)(x −1) =
A
x −2 +
B
x −1.
Clearing the fractions yields
9x −1 = A(x −1) + B(x −2).
Equating coefﬁcients of like powers of x on both sides of this equation, we obtain
A + B = 9
and
A + 2B = 1.
Solving for A and B yields
A = 17
and
B = −8.
Substitution into (C.3) gives
3x2 + 5
x2 −3x + 2 = 3 +
17
x −2 −
8
x −1,
so that
!
3x2 + 5
x2 −3x + 2dx = 3x + 17 ln |x −2| −8 ln |x −1| + c.
□
A short list of important integrals is given on the end pages of this text.
Exercises for C
Problems
For Problems 1–22, evaluate the given integral.
1.
!
x cos x dx.
2.
!
x2e−xdx.
3.
!
ln x dx.
4.
!
tan−1 x dx.
5.
!
x3ex2dx.

810
APPENDIX C
Review of Integration Techniques
6.
!
x
x2 + 1dx.
7.
! x −1
x + 2dx.
8.
!
x + 2
(x −1)(x + 3)dx.
9.
!
2x + 1
x(x2 + 4)dx.
10.
!
x2 + 5
(x −1)(x + 4)dx.
11.
!
x + 3
2x −1dx.
12.
!
2x + 3
x2 + 3x + 4dx.
13.
!
3x + 2
x(x + 1)2 dx.
14.
!
1
√
4 −x2 dx.
15.
!
1
x2 + 2x + 2dx.
16.
!
1
x ln x dx.
17.
!
tan x dx.
18.
!
x + 1
x2 −x −6dx.
19.
!
cos2 x dx.
20.
! $
1 −x2dx.
21.
!
e3x sin 2x dx.
22.
!
ex sin2 x dx.

D
Linearly Independent
Solutions to
x2y′′ + xp(x)y′ + q(x)y = 0
Consider the differential equation
x2y′′ + xp(x)y′ + q(x)y = 0,
(D.1)
where p and q are analytic at x = 0. Writing
p(x) =
∞
!
n=0
pnxn
and q(x) =
∞
!
n=0
qnxn
(D.2)
on (0, R), it follows that the indicial equation for (D.1) is
r(r −1) + p0r + q0.
(D.3)
If the roots of the indicial equation are distinct and do not differ by an integer, then there
exist two linearly independent Frobenius series solutions in the neighborhood of x = 0.
In this appendix, we derive the general form for two linearly independent solutions to
(D.1) in the case that the roots of the indicial equation differ by an integer. The analysis
of the case when the roots of the indicial equation coincide is left as an exercise. Let r1
and r2 denote the roots of the indicial equation and suppose that
r1 −r2 = N,
(D.4)
where N is a positive integer. Then, we know that one solution to (D.1) on (0, R) is
given by the Frobenius series
y1(x) = xr1(1 + a1x + a2x2 + · · · ).
(D.5)
According to the reduction of order technique, a second linearly independent solution to
(D.1) on (0, R) is given by
y2(x) = u(x)y1(x),
(D.6)
where the function u can be determined by substitution into (D.1). Differentiating (D.6)
twice and substituting into (D.1) yields
x2(y1u′′ + 2y′
1u′ + y′′
1u) + xp(x)(y1u′ + y′
1u) + q(x)uy1 = 0;
811

812
APPENDIX D
Linearly Independent Solutions to x2y′′ + xp(x)y′ + q(x)y = 0
that is, since y1 is a solution to (D.1),
x2(y1u′′ + 2y′
1u′) + xp(x)(y1u′) = 0.
Consequently, u can be determined by solving
u′′
u′ = −
" p(x)
x
+ 2 y′
1
y1
#
which, upon integrating, yields
u′ = y−2e−
$
[p(x)/x]dx.
(D.7)
We now determine the two terms that appear on the right-hand side. From (D.2) we can
write
p(x)
x
= p0
x + p1 + p2x + · · · ,
so that
%
p(x)
x
dx = p0 ln x + P(x),
where
P(x) = p1x + 1
2 p2x2 + 1
3 p3x3 + · · · .
Consequently,
e−
$
[p(x)/x]dx = x−p0eP(x).
(D.8)
Since P(x) is analytic at x = 0, it follows that
eP(x) = α0 + α1x + α2x2 + · · ·
for appropriate constants α0, α1, α2, . . . . Hence, from (D.8),
e−
$
[p(x)/x]dx = x−p0(α0 + α1x + α2x2 + · · · ).
(D.9)
Now consider y−2
1 . From (D.5),
y−2
1
=
x−2r1
(1 + a1x + a2x2 + · · · )2 .
(D.10)
Further, since the series
1 + a1x + a2x2 + · · ·
converges for 0 ≤x < R and is nonzero at x = 0, it follows that (1+a1x+a2x2+· · · )−2
is analytic at x = 0 and hence there exist constants β1, β2, . . . , such that
(1 + a1x + a2x2 + · · · )−2 = 1 + β1x + β2x2 + · · · .
We can therefore write (D.10) in the form
y−2
1
= x−2r1(1 + β1x + β2x2 + · · · ).
(D.11)
Substituting from (D.9) and (D.11) into (D.7) yields
u′ = x−(p0+2r1)(α0 + α1x + α2x2 + · · · )(1 + β1x + β2x2 + · · · ),
which can be written as
u′ = x−(p0+2r1)(A0 + A1x + A2x2 + · · · )
(D.12)

Appendix D
Linearly Independent Solutions to x2y′′ + xp(x)y′ + q(x)y = 0 813
for appropriate constants A0, A1, A2, . . . . Now, since the roots of the indicial equation
are r1 and r2 = r1 −N, it follows that the indicial equation has factored form
(r −r1)(r −r1 + N) = 0,
which upon expansion gives
r2 −(2r1 −N)r + r1(r1 −N) = 0.
Comparison with (D.3) reveals that
p0 −1 = −(2r1 −N),
so that
p0 + 2r1 = N + 1.
Consequently, (D.12) can be written as
u′ = x−(N+1)(A0 + A1x + A2x2 + · · · );
that is,
u′ = A0x−(N+1) + A1x−N + · · · + AN x−1 + AN+1 + AN+2 + · · · ,
which can be integrated directly to yield
u(x) = −A0
N x−N +
A1
1 −N x1−N + · · · −AN−1x−1 + AN ln x + AN+1x + · · · .
Rearranging terms, we can write this as
u(x) = A ln x + x−N(B0 + B1x + B2x2 + · · · ),
where we have redeﬁned the coefﬁcients. Substituting this expression for u into (D.6)
gives
y2(x) = [A ln x + x−N(B0 + B1x + B2x2 + · · · )]y1(x).
Hence,
y2(x) = Ay1(x) ln x + x−N y1(x)(B0 + B1x + B2x2 + · · · ).
Substituting for y1 from (D.5) into the second term on the right-hand side yields
y2(x) = Ay1(x) ln x + xr1−N(1 + a1x + a2x2 + · · · )(B0 + B1x + B2x2 + · · · ).
Finally, multiplying the two power series together and substituting r2 = r1 −N from
(D.4), we obtain
y2(x) = Ay1(x) ln x + xr2(b0 + b1x + b2x2 + · · · )
for appropriate constants b0, b1, b2, . . . , and so we have
y2(x) = Ay1(x) ln x + xr2
∞
!
n=0
bnxn.
The derivation of a second solution to the differential equation (D.1) in the case when
the indicial equation has two equal roots follows exactly the same lines as that above
and is left as an exercise.

Answers
to Odd-Numbered Exercises
Chapter 1
Section 1.1
True-False Review
(a) False
(b) False
(c) False
(d) False
(e) True
(f) True
(g) True
(h) True
(i) False
(j) True
(k) False
(l) True
(m) False
(n) True
Problems
1. Second order.
3. Third order.
9. After 4 p.m.
11. y = cx9.
13. y2 −x2 = c.
15. y2 = −2x + c.
17. y2 = −1
m x2 + c.
19. y = ce−2x
m .
25.
(a) ≈3.98 s.
(b) 258.51 ft.
27.
(a) ≈40.84 ms−1.
(b) ≈4.16 s.
33. ≈140 days.
Section 1.2
True-False Review
(a) True
(b) True
(c) False
(d) False
(e) True
Problems
1. Linear.
3. Nonlinear
5. Linear
7. (−∞, ∞).
9. (−∞, ∞).
11. (0, ∞).
13. (−∞, ∞).
15. (0, ∞).
814

Answers to Odd-Numbered Exercises 815
17. (0, ∞).
19. (−∞, ∞).
21. (−∞, ∞).
23. r = −3.
25. r = −2.
27. y(x) = 2x2 −1.
33. y(x) = −cos x + c for x ∈(−∞, ∞).
35. y(x) = ex(x −2) + c1x + c2 for x ∈(−∞, ∞).
37. y(x) = 1
9[x3(3 ln x −1) + 19].
39. y(x) = 1
4 x4 + 2x2 −x + 1.
43. y(x) = 2e2
e −1(x −1) −2x2 ln x.
51.
(a) y(x) = 1
8 (15 −70x2 + 63x4).
Section 1.3
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) True
(f) True
Problems
1. y′ = 2y.
3. y′ = 2y
x .
5. y′ = y
2x .
7. y′ = y2 −2xy −x2
y2 + 2xy −x2 .
17.
(b) y(x) = 0 and y(x) = 1.
(c) Concave up for 0 < y < 1
2 and y > 1. Concave down for
y < 0 and 1
2 < y < 1.
19.
(a) y(x) = 2.
(b) Increasing for y ̸= 2.
(c) Concave up for y > 2. Concave down for y < 2.
21.
(a) y(x) = −1, y(x) = 0, y(x) = 1.
(b) Increasing for −1 < y < 0 and y > 1. Decreasing for
0 < y < 1 and y < −1.
(c) Concave up for y > 1, 0 < y < 1/
√
3, and −1 < y <
−1/
√
3. Concave down for 1/
√
3 < y < 1, −1/
√
3 < y <
0, and y < 1.
Section 1.4
True-False Review
(a) True
(b) True
(c) True
(d) False
(e) False
(f) True
(g) True
(h) False
(i) True
Problems
1. y(x) = cex2.
3. y(x) = ln(c −e−x).
5. y(x) = c(x −2).
7. y(x) = cx −3
2x −1.
9. y(x) = (x −1) + c(x −2)2
(x −1) −c(x −2)2 and y(x) = −1.
11. y(x) = c + c1
! x −a
x −b
"1/(a−b)
.
13. y(x) = a(1 +
#
1 −x2).
15. y(x) = 0.
17.
(a) v(t) = a
$
egt/a −e−gt/a
egt/a + e−gt/a
%
= a tanh(gt/a), where a =
√mg/k.
(b) No.
(c) y(t) = a2
g ln[cosh(gt/a)].
19. y(x) = ln(ex −e3 + e).
21.
(a) No.
(b) 1
2 ln(1 + v2
0).
25. t ≈96.4 minutes.
27.
(a) 500◦F.
(b) t ≈6 : 07 P.M.
29. ≈1190.5 g.
31. 150/7 g.
33. 18.75 g.
35. Q(t) =
αβ
&
ek(β−α)t −1
'
βek(β−α)t −α
.
37.
(
1 −Q
α
)β−γ (
1 −Q
β
)γ −α (
1 −Q
γ
)α−β
= e(α−β)(β−γ )(γ −α)t.
Section 1.5
True-False Review
(a) True
(b) False
(c) True
(d) True
(e) True
(f) True
(g) False

816
Answers to Odd-Numbered Exercises
(h) True
(i) False
(j) True
Problems
1. 2560.
3. t ≈35.86 hours.
5. 1091.
7.
(a) P1 > 2P0 P2
P0 + P2
(b) No.
9. (a) Equilibrium solns: P(t) = 0, P(t) = T . Isoclines: P =
0.5
!
T ±
*
rT 2+4k
r
"
. Concave up for P > T/2; Concave down
for 0 < P < T/2. (c) For 0 < P0 < T , population dies out; For
P0 > T , population grows.
13. P(t) = Celn(P0/k)e−rt .
Section 1.6
True-False Review
(a) False
(b) True
(c) True
(d) False
(e) False
Problems
1. y(x) = 2ex + ce−x.
3. y(x) = x4(sin x −x cos x + c).
5. y(x) = (1 −x2)[−ln(1 −x2)2 + c].
7. y(x) = sin 2x + c cos x.
9. y(x) =
1
cos x (2 sin4 x + c).
11. y(x) =
1
cos x
! 1
2 cos 2x + c
"
.
13. y(x) = 1
2 x3(2 ln x −1) + cx.
15. y(x) =
x
m + 1 ln x −
x
(m + 1)2 + c
xm .
17. y(x) = 2 sin x[ln(sin x) + 1].
19. y(x) = cosh x.
21. y(x) =
⎧
⎪⎪⎨
⎪⎪⎩
1
4 (5e2x + 2x −1) if x < 1,
1
4 (5e2x + e2(x−1)) if x ≥1.
.
23. y(x) = x3 + c1 ln x + c2.
27. (b) limt→∞T (t) = 0. (c) tmax = 40 ln 2,
T (tmax) = Tm(tmax) = 20◦F.
29. (b) u(x) =
/
e
/
p(x) dxq(x) dx + c.
General solution: y(x) = e−
/
p(x) dx &/
e
/
p(x) dxq(x) dx + c
'
.
31. y(x) = e−x(−e−x + c).
33. y(x) = x(x ln x −x + c).
Section 1.7
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) False
(f) True
(g) True
(h) True
Problems
1.
595
169 g/L.
3.
9
16g/L.
5.
(a) A(t) = (t + 20)3 −8000
(t + 20)2
.
(b) 20( 3√
2 −1) minutes.
9. i(t) = 5(1 −e−40t).
11. i(t) = 3
5 (3 sin 4t −4 cos 4t + 4e−3t).
15. i(t) =
E0
R2 + L2ω2 (R sin ωt −ωL cos ωt) + Ae−Rt/L,
iS(t) =
E0
R2 + L2ω2 (R sin ωt −ωL cos ωt), iT (t) = Ae−Rt/L.
17. i(t) =
E0C
1 −aRC ( 1
RC e−t/RC −ae−at).
Section 1.8
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) True
(f) False
(g) True
(h) False
(i) True
Problems
1. F(V ) = 5 + 2V
9 −4V .
3. F(V ) = (sin 1
V −V cos V )/V .
5. Not homogeneous of degree zero.
7. F(V ) = −
#
1 + V 2.

Answers to Odd-Numbered Exercises 817
9. y(x) = x tan(ln cx).
11. tan−1 ( y
x
)
= 1
2 ln |x| + c.
13. sin−1 ( y
4x ) = ln |x| + c.
15. ln |xy| −x2
2y2 = c1.
17. (y −2x)2(y + x)
y −x
= c.
19. y(x) = −x
0
1 +
1
ln (cx)
1
.
21. y2 = c
x e4x/y.
23. y2 = x2[(ln cx)2 −1].
25. (x −y)2 = 1
2 (y −2x)3.
27. y(x) = 1
18(81 −x2).
29.
(b) r =
√
2e(θ−π/4)/2. Maximum interval ≈(−5.18, 1.08).
31. (x −k)2 + (y + k)2 = 2k2.
33. ln (x2 + y2) −2 tan−1 (y/x) = k.
35. (x −k)2 + (y −k)2 = 2k2.
39. y−2 = 2 cos x +
c
cos x .
41. y1/2 = 1
x (1 + x2)3/2 + c
x .
43. y−2 = x3 + cx.
45. y1/3 = cos x + x sin x + c
x2
.
47. y2 =
ln x
x2(1 −2 ln x) + c .
49. y2 = −2 cos4 x + c
sin x
.
51. y(x) =
2
(1 + x2)[2 −ln(1 + x2)] .
55. y(x) = 2[tan (2x + c) −2x −1].
59. (b) y(x) =
1
1−x2 −x.
61. y(x) = x−1
!
1
c −ln x −1
"
.
63. (b) y(x) = x−1
!
1 +
1
c −3 ln x
"
.
65. y(x) = xex2.
67. y(x) = tan−1 (
1 + ce−√1+x)
.
Section 1.9
True-False Review
(a) False
(b) False
(c) True
(d) True
(e) False
(f) True
(g) False
(h) False
(i) False
Problems
1. Not exact.
3. Exact.
5. y(x) =
c
x2 + 1 .
7. y(x) = x + (c −x3 −6e2x)1/3.
9. sin xy + cos x = c.
11. xy2 −cos y + sin x = c.
13. y(x) = 1
x ec/x.
15. x2y + x cos y −y2 = c.
17. y(x) = x−1(x3 ln x + 5).
21. Yes
23. y(x) = c + 2x5/2
10√x
.
25.
x3e3y
3
−cos y = c.
27. y(x) = c + tan−1 x
1 + x2
.
29. 2x −y4 = cy2.
31. r = 1, s = 2.
Section 1.10
True-False Review
(a) True
(b) True
(c) False
(d) False
Problems
1. y(0.5) ≈4.8938.
3. y(0.5) ≈1.0477.
5. y(x) =
c
x2+1 .
7. y(x)x + (c −x3 −6e2x)1/3.
9. y(1) ≈0.7115.
11. y(0.5) ≈5.79167.
13. y(x) 1
x ec/x.
15. y(1) ≈0.9999.
Section 1.11
Problems
1. y(x) = 2e3x + c1e2x + c2.
3. y(x) = (1 −c1) ln |x −1| + c1x + c2.
5. y(x) = sin−1 (c1x + c2).

818
Answers to Odd-Numbered Exercises
7. x(t) = c2 −ln |c1 −e2t|.
9. x(t) = c1t3 −t2 + c2.
11. y(x) = x6 + c1x3 + c2.
13. c1y2 −e−y(y + 1) = c2 + x.
15. y(x) = sec x.
17. y(x) = a cosh (x/a).
19.
(b) y(x) = u1 = c1x3 + 1
2 x2 + c2x + c3.
Section 1.12
Problems
1.
(a) ≈7.1 m.
(b) ≈1.82 s.
3. x2 + 3y2 = k.
5. 2x2 + 3y2 = k.
7.
(b) y2 −x2 = kx3.
17. y2 = 2(ln x)2.
19. x2y + y2 = c.
21. y(x) = −2 cos x
cos2 x + c .
23. y(x) = x sin(c + ln |x|).
25. y2 = x3(5 ln x −1) + cx−2.
27. y(x) = c −ln(cos x)
sin x
.
29. y(x) = xecx.
31. y(x) = 1 −cecos x.
33. y2 =
ln x
x3(3 ln x −1) + c .
35. y(x) =
! x + 1
x −1
"
[x −2 ln(x + 1) + c].
37. 3x2y + y3 = 3x3(ln |x| + c)
39. y2 = 25(ln x)2 + c
2x2
.
41. yey = ex(x −1) + c.
43. y(x) = 6e
1
3 x3 −1.
45. y(x) =
#
10 −x3
x
.
47.
(a) m = n = 0.
(b) m = 0, n any real number.
(c) m = 5, n = 2.
(d) No values.
(e) m = 4, n any real number.
49. ≈26.18 min.
51.
(a) The velocity is increasing at a rate of 2 ms−2.
(b) k = 1
30 .
(c) v(t) = 4
3 e−t/30(2t + 15).
(d) No.
(e) limt→∞v(t) = 0.
53.
(a) 20,000.
(b) 11696.
(c) ≈58 days after April 1.
55. i(t) = 12
89 (8 cos 2t −5 sin 2t) −1185
356 e−5
4 t.
57. ≈157.5 g.
59. y(1.5) ≈3.67185.
61. y(1.5) ≈3.66576.
63. y(1.5) ≈3.66568.
Chapter 2
Section 2.1
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) False
(f) True
(g) False
(h) True
(i) True
(j) True
(k) False
(l) True
(m) False
Problems
1.
(a) a31 = 0, a24 = −1, a14 = 2, a32 = 2, a21 = 7, a34 = 4.
(b) (1, 4) and (3, 2).
3.
0
1 5
−1 3
1
; 2 × 2 matrix.
5.
⎡
⎢⎢⎣
−1
1
1
−5
⎤
⎥⎥⎦; 4 × 1 matrix.
7.
⎡
⎣
0 −1 2
1
0 3
−2 −3 0
⎤
⎦; 3 × 3 matrix.
9.
⎡
⎢⎢⎣
2 3 4 5
3 4 5 6
4 5 6 7
5 6 7 8
⎤
⎥⎥⎦; 4 × 4 matrix.
11. 0.
13. Column vectors:
0 1
3
1
,
0 −1
5
1
.
Row vectors: [1
−1], [3
5].
15. Column vectors:
0 2
5
1
,
0 10
−1
1
,
0 6
3
1
.
Row vectors [2
10
6], [5
−1
3].
17. A =
0 −2
0
4 −1 −1
9 −4 −4
0
8
1
. Column vectors:
0 −2
9
1
,
0
0
−4
1
,
0
4
−4
1
,
0 −1
0
1
,
0 −1
8
1
.

Answers to Odd-Numbered Exercises 819
19. B =
⎡
⎣
2
5 0 1
−1
7 0 2
4 −6 0 3
⎤
⎦. Row vectors: [2
5
0
1],
[−1
7
0
2], [4
−6
0
3].
21. One example:
⎡
⎣
2 0
0
0 3
0
0 0 −1
⎤
⎦.
23. One example:
⎡
⎢⎢⎣
0
3 −1
2
−3
0
4 −3
1 −4
0
1
−2
3 −1
0
⎤
⎥⎥⎦.
25. The only possibility here is the zero matrix:
⎡
⎣
0 0 0
0 0 0
0 0 0
⎤
⎦.
27. One example: A(t) =
⎡
⎢⎢⎣
t2 −t
0
0
0
0
0
0
0
⎤
⎥⎥⎦.
29. One example: A(t) =
8
1
t2+1
0
9
.
31. One example: A(t) = [t] and B(t) = [t2].
Section 2.2
True-False Review
(a) False
(b) True
(c) True
(d) False
(e) False
(f) False
(g) False
(h) False
(i) True
(j) False
(k) False
(l) True
Problems
1.
(a)
0 −10 30
5
−5
0
−15
1
.
(b)
0 −6
−3
3
0 −12 12
1
.
(c)
⎡
⎣
−1 + i
−1 + 2i
−1 + 3i
−1 + 4i
−1 + 5i
−1 + 6i
⎤
⎦.
(d)
0 −6
11
3
−2 −4 −2
1
.
(e)
0 1 + 3i
15 + 3i
16 + 3i
5 + 3i
12 + 3i
15 + 3i
1
.
(f)
⎡
⎣
8 10
7
1
4
9
1
7
12
⎤
⎦.
(g)
⎡
⎣
12
−3 −3i
−1 + i
3 + i
3 −2i
8
6
4 + 2i
2
⎤
⎦.
(h)
0 1
−10 −1/2
3/2
−4
17/2
1
.
(i)
⎡
⎣
−8
−24 + 6i
−9 −2i
1 −2i
2 + 4i
7
15
−19 −4i
−20
⎤
⎦.
(j)
⎡
⎣
10 3
−16 8
−5 1
⎤
⎦.
3.
(a)
0 5
10 −3
27 22
3
1
.
(b) BC =
⎡
⎣
9
8
−6
⎤
⎦.
(c) Not deﬁned.
(d)
⎡
⎣
2 −4i
7 + 13i
−2
1 + 3i
4 −6i
10 + 18i
⎤
⎦.
(e)
⎡
⎣
2 −2
3
−2
2 −3
4 −4
6
⎤
⎦.
(f)
:
6 10
;
.
(g)
0 −1 10 −10i
0
15 + 8i
1
.
(h)
⎡
⎣
15
14
−10
⎤
⎦.
(i)
⎡
⎣
10 2 14
2
2
2
14 2 20
⎤
⎦.
(j)
0 −2 + i
13 −i
1 −4i
4 + 18i
1
.
5. ABC =
0 −185 −460
119
316
1
. C AB =
0 −99 635
−30 230
1
.
7.
⎡
⎣
−13
−13
−16
⎤
⎦
9.
0 ax + by + cz + dw
ex + f y + gz + hw
1
.
11.
(a) A2 =
0 −1 −4
8
7
1
, A3 =
0 −9 −11
22
13
1
,
A4 =
0 −31 −24
48
17
1
.
(b) A2 =
⎡
⎣
−2
0
1
4 −3
0
2
4 −1
⎤
⎦, A3 =
⎡
⎣
4 −3
0
6
4 −3
−12
3
4
⎤
⎦,
A4 =
⎡
⎣
6
4 −3
−20
9
4
10 −16
3
⎤
⎦.
13. A2 =
0 −26 20
−24
6
1
.

820
Answers to Odd-Numbered Exercises
15. x = 1/2, y = 1/2, z = −1/8.
19. [A1, A2] = 02, [A1, A3] = 02, [A2, A3] =
! 1
0
0 −1
"
. A1 com-
mutes with A2, A1 commutes with A3, but A2 does not commute
with A3.
27. (a)
#
35 42 −7
$
.
(b)
⎡
⎢⎢⎣
7465 −59 −2803
1790
−59
31
185
−82
−2803
185
1921 −1034
1790 −82 −1034
580
⎤
⎥⎥⎦. (c):
⎡
⎣
−1
5
2
2
−48 −10
⎤
⎦.
29. (b) x = ±1/
√
5, y = ±1/
√
5, z = ±1.
35. B =
⎡
⎣
1 −1 5
−1
2 1
5
1 6
⎤
⎦, C =
⎡
⎣
0 −4 −2
4
0
3
2 −3
0
⎤
⎦.
41.
!
1
cos t
−sin t
4
"
.
43.
⎡
⎣
cos t
−sin t
0
sin t
cos t
1
0
3 0
⎤
⎦.
47.
! 1
1
"
.
49.
⎡
⎢⎢⎣
e2−1
2
1−cos 2
2
−14
3
1
tan 1
1
2 + cos 1
⎤
⎥⎥⎦.
51.
! t2
t3
"
.
53.
! et
−e−t
2et
−5e−t
"
.
Section 2.3
True-False Review
(a) False
(b) False
(c) False
(d) True
(e) True
(f) False
(g) False
Problems
5. Inﬁnitely many solutions.
7. One solution.
9. A =
⎡
⎣
1 2 −3
2 4 −5
7 2 −1
⎤
⎦, b =
⎡
⎣
1
2
3
⎤
⎦, A# =
⎡
⎣
1 2 −3 1
2 4 −5 2
7 2 −1 3
⎤
⎦.
11. A =
⎡
⎣
1 2 −1
2 3 −2
5 6 −5
⎤
⎦, b =
⎡
⎣
0
0
0
⎤
⎦, A# =
⎡
⎣
1 2 −1 0
2 3 −2 0
5 6 −5 0
⎤
⎦.
13. 2x1 + x2 + 3x3 = 3, 4x1 −x2 + 2x3 = 1, 7x1 + 6x2 + 3x3 = −5.
15. −3x2 = −1, 2x1 −7x2 = 6, 5x1 + 5x2 = 7.
17.
+
x′
1
x′
2
,
=
! −4
3
6 −4
" ! x1
x2
"
+
! 4t
t2
"
.
19.
! x′
1
x′
2
"
=
!
0 e2t
−sin t
0
" ! x1
x2
"
+
! 0
1
"
.
Section 2.4
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) False
(f) False
(g) True
(h) True
(i) True
Problems
1. Neither.
3. Neither.
5. Row-echelon form.
7. Reduced row-echelon form.
9.
! 1 −2
0
0
"
, rank(A) = 1.
11.
⎡
⎣
0 1 3
0 0 1
0 0 0
⎤
⎦, rank(A) = 2.
13.
⎡
⎣
1 2 −5
0 1
2
0 0
1
⎤
⎦, rank(A) = 3.
15.
⎡
⎢⎢⎣
1 −1 1
0
0
1 0
1
0
0 1 −1
0
0 0
1
⎤
⎥⎥⎦, rank(A) = 4.
17.
⎡
⎣
1 0
2 1
3
0 1 −1 2
−4
0 0
0 1 −13
3
⎤
⎦, rank(A) = 3.
19.
!
1 −1
2
0
0
"
, rank(A) = 1.
21. I3, rank(A) = 3.
23.
⎡
⎣
1 0
1
0 1 −3
0 0
0
⎤
⎦, rank(A) = 2.
25.
⎡
⎣
1 −2 0 1
0
0 1 2
0
0 0 0
⎤
⎦, rank(A) = 2.
Section 2.5
True-False Review
(a) False
(b) True

Answers to Odd-Numbered Exercises 821
(c) True
(d) True
(e) False
(f) False
Problems
1. {(8, 1)}.
3. { 3
7t + 5
7 , t) : t ∈R}.
5. {(1 −t, 2 −3t, t) : t ∈R}.
7. {(2 + s
2 −t
2 , s, t) : s, t ∈R}.
9. {(10, −1, 4)}.
11. No solution.
13. {(3, −1, 5)}.
15. {(2t −3, t, t) : t ∈R}.
17. {(−1, 2, −1, −4)}.
19. {(1, −3, −2)}.
21. {(t, 1, 3) : t ∈R}.
23. {(3 + r −t, −r −1,r, t) : r, t ∈R}.
25.
(a) Not possible.
(b) k = −1.
(c) k ̸= −1.
27.
(a) a ̸= −1
2 or (a, b) = (−1
2 , −1).
(b) Not possible.
(c) a = −1
2 and b ̸= −1.
29.
(b) No solution if '2 ̸= 0; inﬁnite number of solutions if
'2 = 0.
(c) Inﬁnite number of solutions: one line; No solution: two dis-
tinct, parallel lines; Unique solution: Two distinct lines that
intersect at one point.
35. (1, 1, 2) and (1, −1, 2).
37. {(0, 0, 0)}.
39. {(0, 0, 0)}.
41. {(3t, 2t, t) : t ∈R}.
43. {(0, 0, 0)}.
45. {(s −t, s −3t, 2s, 2t) : s, t ∈R}.
47. {(0, 0)}.
49. {( 1+3i
2
t, t) : t ∈C}.
51. {(3t, −t, −t, t) : t ∈R}.
53. {(0, 0, t) : t ∈R}.
55. {(−3s, −2s, t, s) : s, t ∈R}.
57. {(0, 0, 0)}.
Section 2.6
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) False
(f) True
(g) True
(h) True
(i) True
(j) True
Problems
5. A−1 =
0
3 −2
−1
1
1
.
7. A−1 =
8
1 + i
−1+i
2
1
1+i
2
9
.
9. A−1 =
⎡
⎣
−43 −4
13
−24 −2
7
10
1 −3
⎤
⎦.
11. Not invertible.
13. A−1 =
⎡
⎢⎢⎣
−13
5
11
10
−7
5
3
5
−1
10
2
5
−4
5
3
10
−1
5
⎤
⎥⎥⎦.
15. Not invertible.
17. A−1 =
⎡
⎢⎢⎢⎢⎢⎣
0
2
9
−1
9
2
9
−2
9
0 −1
3
1
9
1
9
1
3
0 −2
9
−2
9
−1
9
2
9
0
⎤
⎥⎥⎥⎥⎥⎦
.
19.
⎡
⎣
−5/12
−1/6
−1/4
⎤
⎦.
21. {(−48, 14)}.
23. {(−2, 2, 1)}.
25. {(−6, 1, 3)}.
37. B9.
43. x1 = (0, −1, 0), x2 = (9, 8, −2), x3 = (−5, −5, 2).
Section 2.7
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) False
(f) False
(g) False
(h) False
(i) False
(j) False

822
Answers to Odd-Numbered Exercises
Problems
3. M2( 1
11 ), A12(−3), P12.
5. M2(−1
5), A23(−2), A13(−3), A12(−2), P13.
7. A12(1)A21(2).
9. P12M1(−1)A12(3)M2(2)A21(−2).
11. A12(2)A13(3)A23(1)M2(4)A32( 1
2 )A21(−1).
13. M2(8)A12(3)A21(2)A23(−2)M3(−4)A31(3).
15. E1 =
0 1
0
1
3
1
1
, L =
0
1 0
−1
3
1
1
.
17. L =
0 1
0
5
3
1
1
, U =
0 3
1
0
1
3
1
.
19. L =
⎡
⎣
1
0 0
−2
1 0
3 −2 1
⎤
⎦, U =
⎡
⎣
5 2 1
0 2 5
0 0 4
⎤
⎦.
21. L =
⎡
⎢⎢⎣
1
0 0 0
2
1 0 0
−4 −2 1 0
3
2 1 1
⎤
⎥⎥⎦, U =
⎡
⎢⎢⎣
2 −3
1
2
0
5 −1 −3
0
0
4 −3
0
0
0
5
⎤
⎥⎥⎦.
23. (3, −1, −1).
25.
! 677
1300 , −9
325, −37
65, 4
13
"
.
27. A−1 =
⎡
⎢⎢⎣
−29
13
18
13
−14
13
−17
13
11
13
−10
13
2 −1
1
⎤
⎥⎥⎦.
Section 2.8
True-False Review
(a) False
(b) True
(c) False
(d) False.
Section 2.9
Additional Problems
1.
⎡
⎢⎢⎣
13
−1
−6 −11
−3
20
6
−5
⎤
⎥⎥⎦.
3. Not possible.
5. AB =
0 16
8
6
−17
1
, tr(AB) = −1.
7.
⎡
⎢⎢⎣
−24
48
24
72
24 −24 −56 −48
−4 −28
52 −24
4
4 −20
0
⎤
⎥⎥⎦.
9. CT C = [71], tr(CT C) = 71.
17.
⎡
⎢⎢⎣
−7
1/3
11/2
11/4
3/2
2/π
e −1
3/4
⎤
⎥⎥⎦.
19. Impossible.
21.
<
( 21
13 , 10
13 , −2
13)
=
.
23. No solution.
25. {(2t −2s + 3, s −t + 1, s, 2t + 2, t) : s, t ∈R}.
27.
(a) k = −3
2
(b) k ̸= −3
2
(c) Not possible.
29.
(a) Not possible
(b) k ̸= 1, 2
(c) k = 1 or k = 2.
31. No.
33.
(a)
0
1 −7
2
0
0
1
(b) rank(A) = 1
(c) Does not exist.
35.
(a)
⎡
⎢⎢⎣
1 2 0
0
0 1 0
0
0 0 1 −1
0 0 0
1
⎤
⎥⎥⎦
(b) rank(A) = 4
(c) A−1 =
⎡
⎢⎢⎢⎢⎢⎣
2
3
−1
3
0
0
−1
3
2
3
0
0
0
0 −3
7
4
7
0
0
4
7
−3
7
⎤
⎥⎥⎥⎥⎥⎦
.
37.
(a)
⎡
⎣
1 4 2
0 1 1
0 0 1
⎤
⎦
(b) rank(A) = 3
(c) A−1 =
⎡
⎢⎢⎣
1
5
7
5
−1
−3
10
−3
5
1
2
1
2
1 −1
2
⎤
⎥⎥⎦.
39. x1 = 1
13
0 4
1
1
, x2 = 1
39
0 23
22
1
, x3 = 1
13
0
7
−8
1
.
43.
(a) P12A12(2)M2(−3)A21(2)M3(3)A34(4)M4(−7/3)A43(4/3).
(b) L =
⎡
⎢⎢⎣
1
0
0
0
1
2
1
0
0
0
0
1
0
0
0
4
3
1
⎤
⎥⎥⎦, U =
⎡
⎢⎢⎣
2
1
0
0
0
3
2
0
0
0
0
3
4
0
0
0 −7
3
⎤
⎥⎥⎦.
45.
(a) P12A12(−2)A23(1)M3(−2)M2(5)A21(4)A31(−2)A32(1)
(b) L =
⎡
⎣
1 0 0
−1
2
1 0
0 2 1
⎤
⎦, U =
⎡
⎣
−2 −3
1
0
5
2
5
2
0
0 −2
⎤
⎦.
47. 2k.
49.
(a) 6
(b) 4
(c) 15
(d)
n!
m!(n−m)!
51. B6.

Answers to Odd-Numbered Exercises 823
Chapter 3
Section 3.1
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) False
(g) True
(h) True
(i) True
(j) True
Problems
1. 3, odd.
3. 10, even.
5. 8, even.
9. Yes, even, plus sign.
11. No.
13. p = 4, q = 1, N(3, 1, 2, 4) = 2, plus sign.
15. p = 2, q = 1, N(3, 4, 1, 2) = 4, plus sign.
17. −21.
19. 13.
21. −4.
23. 0.
25. −4.
27. 68.
29. −84.
31. 87.
33. det(A) ≈9602.
35. 42.
37. −189.
39. 4.
41. 20.
43. −2e10t.
45. 42et.
51. −508.
53. 1524.
55.
(a) ϵ123 = 1, ϵ132 = −1, ϵ213 = −1, ϵ231 = 1, ϵ312 = 1,
ϵ321 = −1.
Section 3.2
True-False Review
(a) False
(b) True
(c) True
(d) True
(e) False
(f) True
Problems
1. −17.
3. 56.
5. 0.
7. −72.
9. 9.
11. −7020.
13. 24.
15. Not invertible.
17. Invertible.
19. Invertible.
21. Not invertible.
23. All k ̸= ±2.
25. k = −1, 1, or 4.
29. −2.
31. −18.
33. 12.
35. −18.
37. 15.
39.
729
125 .
41. 150, 000.
43. 80.
45. Only det(A) = det(B) must hold.
47. x = −1, 0 or 2.
49. −1.
55. No.
Section 3.3
True-False Review
(a) False
(b) True
(c) True
(d) False
(e) True
(f) True
(g) False
(h) False
(i) False
(j) True
Problems
1. M11 = 5, M12 = 0, M21 = 2, M22 = −9, C11 = 5, C12 = 0,
C21 = −2, C22 = −9.

824
Answers to Odd-Numbered Exercises
3. M11 = −9, M12 = 7, M13 = 5, M21 = −7, M22 = 1, M23 = 3,
M31 = −2, M32 = −2, M33 = 2,C11 = −9,C12 = −7,C13 = 5,
C21 = 7, C22 = 1, C23 = −3, C31 = −2, C32 = 2, C33 = 2.
5. M12 = −4, M31 = 16, M23 = 40, M42 = 12, C12 = 4, C31 = 16,
C23 = −40, C42 = 12.
7. −48.
9. −71.
11. 82.
13. 72.
15. −4.
17. 3.
19. 0.
21. 0.
23. −15.
25. −892.
27. −1151.
31. 3 ± i.
33. 3 ± 2
√
2.
35. 0, 2, 14.
37. 5, 7, −9.
39.
(a) det(A) = 11
(b) MC =
0
5 −4
−1
3
1
(c) adj(A) =
0
5 −1
−4
3
1
(d) A−1 = 1
11
0
5 −1
−4
3
1
.
41.
(a) det(A) = 0
(b) MC =
0 −6 15
−2
5
1
(c) adj(A) =
0 −6 −2
15
5
1
(d) A−1 does not exist.
43.
(a) det(A) = −8
(b) MC =
⎡
⎣
−7 −6
4
−11 −6
4
16
8 −8
⎤
⎦
(c) adj(A) =
⎡
⎣
−7 −11
16
−6
−6
8
4
4 −8
⎤
⎦
(d) A−1 = −1
8
⎡
⎣
−7 −11
16
−6
−6
8
4
4 −8
⎤
⎦.
45.
(a) det(A) = 10
(b) MC =
⎡
⎣
5
4 3
−5 −2 1
5 −2 1
⎤
⎦
(c) adj(A) =
⎡
⎣
5 −5
5
4 −2 −2
3
1
1
⎤
⎦
(d) A−1 = 1
10
⎡
⎣
5 −5
5
4 −2 −2
3
1
1
⎤
⎦.
47.
(a) det(A) = 16
(b) MC =
⎡
⎢⎢⎣
4 4
4
4
−4 4 −4
4
4 4 −4 −4
−4 4
4 −4
⎤
⎥⎥⎦
(c) adj(A) =
⎡
⎢⎢⎣
4 −4
4 −4
4
4
4
4
4 −4 −4
4
4
4 −4 −4
⎤
⎥⎥⎦
(d) A−1 = 1
16
⎡
⎢⎢⎣
4 −4
4 −4
4
4
4
4
4 −4 −4
4
4
4 −4 −4
⎤
⎥⎥⎦.
49.
(b) A−1 =
1
(1+2x2)2
⎡
⎣
1
2x
2x2
−2x
1 −2x2
2x
2x2
−2x
1
⎤
⎦.
51. −1.
53.
9
16.
55. A−1 =
1
4e3t
0
2e2t
−e2t
−2et
3et
1
.
57. A−1 = 1
t
⎡
⎣
3te−t
−te−t
−te−t
−e−t
e−t
0
−te2t
0
te2t
⎤
⎦.
59. ( 16
7 , 6
7).
61. (−11
27, −35
27, −17
27).
63. (0, 0, 0).
65. ( 2
3 e−t(3 sin t + 2 cos t), 1
3 e2t(3 sin t −4 cos t)).
67.
19
3 .
Section 3.4
Problems
1. −3.
3. −43.
5. −40.683.
7. −124.
9. det(A) = 11, A−1 = 1
11
0
7 −5
−2
3
1
.
11. det(A) = 30, A−1 = 1
30
⎡
⎣
−20
102 −38
5 −24
11
10 −30
10
⎤
⎦.
13. det(A) = −152, A−1 = −1
152
⎡
⎢⎢⎣
−38
0
38
0
32
28 −124 −24
34
44 −222 −16
2 −60
130
8
⎤
⎥⎥⎦.
15. (1, −1
4 ).
17. (e−t(cos t + 3 sin t), e−t(sin t −3 cos t)).
19. ( 30
271 , 59
271 , 81
271 ).
23. 24.
25. −12.
27. 9.
29. 9.

Answers to Odd-Numbered Exercises 825
Section 3.5
Problems
1. 18.
3. 24.
5. −30.
7. 12.
9. 216.
11. −9
2 .
13. 1152.
15. Not possible.
17. −18.
19. 474.
21. 474.
23. 4104.
25. 0.
27. B−1 =
0
1 −4
−1
5
1
, (A−1BT )−1 =
0 −2 −2
11
12
1
.
29. A−1 = −1
60
⎡
⎢⎢⎣
5
65 −25 −35
12 −24
0
0
−11 −23
−5
5
−1 −13
5
−5
⎤
⎥⎥⎦.
31. A−1 = 1
3
⎡
⎣
7
8
16
4
3
8
−4 −4 −9
⎤
⎦.
33. Many answers possible, such as [0 0
10
9 ].
35.
(a) k = −1
(b) |8k + 8|.
37.
(a) k ≈2.39 (b) |2k3 −k2 −4k −12|.
41. (2, −1
4 , −9
4 ).
Chapter 4
Section 4.1
True-False Review
(a) False
(b) True
(c) True
(d) True
(e) False
(f) False
(g) True
(h) True
(i) False
(j) True
(k) False
(l) False.
Problems
1. v1 = (−3, −12), v2 = (20, −4), v3 = (17, −16).
3. −v = (20, −64, −22).
5. y = ( 1
3, 4
3 , 2
3 , 3, 4).
7. z = (−1
2 + 13
2 i, 7
2 + 7
2i).
Section 4.2
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) True
(f) True
(g) True
(h) False
(i) False
(j) False
Problems
1. (A1) holds, (A2) fails.
3. (A1) fails, (A2) holds.
5. (A1), (A2) both fail.
7. (A1), (A2) both hold.
9. (A1) fails, (A2) holds.
11. (A1), (A2) both fail.
13. (A1), (A2) both fail.
17. 0m×n is the zero vector, and −A = (−ai j) is the additive inverse
of A = (ai j).
19. p(x) = 0 is the zero vector, and −p(x) = −a0 −a1x −· · ·−anxn
is the additive inverse of p(x) = a0 + a1x + · · · + anxn.
21. All axioms are satisﬁed.
23. (A5) holds, (A6) fails.
25. Only axioms (A1), (A2), and (A3) hold.
29. No.
Section 4.3
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) True
(f) False
(g) False
(h) False

826
Answers to Odd-Numbered Exercises
Problems
3. S = {(x, y, z) ∈R3 : z = 3x and y = 2x}, Yes.
5. S = {(x1, 0, x3, 2) : x1, x3 ∈R}, No.
7. S = {x ∈Rn : Ax = b}, No.
9. S = {A ∈M2(R) : det(A) = 1}, No.
11. S = {A ∈Mn(R) : A is invertible}, No.
13. S =
⎧
⎨
⎩
⎡
⎣
a
c
b
d
−(a + b) −(c + d)
⎤
⎦: a, b, c, d ∈R
⎫
⎬
⎭, Yes.
15. S = {A ∈M2(R) : AT = A}, Yes.
17. S = { f ∈V : f (a) = 1}, No.
19. S = {ax2 + b : a, b ∈R}, Yes.
21. S = {y ∈C2(I) : y′′ + 2y′ −y = 0}, Yes.
23. nullspace(A) = {(−4t, t) : t ∈R}.
25. nullspace(A) = {(0, 0)}.
27. nullspace(A) = {(0, 0, 0)}.
29. nullspace(A) = {(0, 0, 0)}.
Section 4.4
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) True
(f) False
(g) False
(h) False
(i) True
(j) False
(k) False
(l) True
Problems
1. No.
3. No.
5. Yes.
7. No.
11. (5, −7) = 31
7 v1 −9
7v2.
13. (9, 8, 7) = −56
3 v1 + 155
3 v2 −24v3.
15. Yes.
17. {(1, 0, −1, 1), (0, 1, 1, −2)}.
19. (b)
A0 1 0
0 0
1
,
0 0 1
0 0
1
,
0 0 0
0 1
1B
.
21.
⎧
⎨
⎩
⎡
⎣
1 0 −1
0 0
0
−1 0
1
⎤
⎦,
⎡
⎣
0
1 −1
0
0
0
0 −1
1
⎤
⎦,
⎡
⎣
0 0
0
1 0 −1
−1 0
1
⎤
⎦,
⎡
⎣
0
0
0
0
1 −1
0 −1
1
⎤
⎦
⎫
⎬
⎭.
23. {(2, 1, 0), (1, 0, 1)}.
25. {(−4, 1)}.
27. ∅
29. ∅
31. ∅
33. {(1, −2, 1)}.
35. span(v1, v2) consists of the line in R3 passing through the origin
and with parallel vector (1, 2, −1).
37. Yes.
39. No.
41. span{A1, A2, A3} =
A
A ∈M2(R) :
A =
0
c1 + 3c3
−c1 + c2
2c1 −2c2 + c3
c2 + 2c3
1
with c1, c2, c3 ∈R
B
.
43. (a) h(x) = c1 cosh x + c2 sinh x.
45. All points lying on the line through the origin with direction v1.
47. If v1 = v2 = 0, the subspace is {(0, 0, 0)}; otherwise, it consists
of all points lying on the line through the origin with direction v1
(or v2).
Section 4.5
True-False Review
(a) False
(b) True
(c) False
(d) True
(e) True
(f) True
(g) True
(h) False
(i) False
Problems
1. Linearly independent.
3. Linearly dependent. 3(2, −1) −2(3, 2) + 7(0, 1) = (0, 0).
5. Linearly dependent. (1, 2, 3)−2(1, −1, 2)+(1, −4, 1) = (0, 0, 0).
7. Linearly independent.
9. Linearly independent.
11. Linearly dependent. The vectors span a plane in R3.
13. k = 3 or k = −4.
15. k ̸= 3, 1, or −2.
17. Linearly independent.
19. Linearly independent.
21. Linearly independent.
25. Many answers possible. {(1, 2, 3), (−3, 4, 5)}.
27. Many answers possible. {(1, −1, 1), (1, −3, 1), (3, 1, 2)}.
29. Many answers possible.
A0 1 2
3 4
1
,
0 −1 2
5 7
1B
.

Answers to Odd-Numbered Exercises 827
31. Many answers possible. {2 + x2, 1 + x}.
37. Linearly dependent.
39. Linearly independent.
41. Linearly independent.
45. α ̸= ±1.
Section 4.6
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) False
(f) True
(g) False
(h) False
(i) False
(j) True
(k) False
Problems
1. No.
3. Yes.
5. No.
7. No.
9. No.
11. Yes.
13. No.
15. No.
17. No.
19. dim[nullspace(A)] = 4.
21. dim[nullspace(A)] = 2.
23. dim[nullspace(A)] = 2.
25. One possible basis: {(1, 1, −5), (0, −2, 3)}; dim[S] = 2.
27. One possible basis: {x3, x3 + 1, x3 + x, x3 + x2}.
29. One possible basis:
A0 1
0
0 −1
1
,
0 0 1
0 0
1
,
0 0 0
1 0
1B
; dimS] = 3.
31. One possible basis: { f1, f2}; dim[S] = 2.
39.
(b)
0 5 6
7 8
1
= −34
3 A1 + 12A2 −55
3 A3 + 56
3 A4.
41.
(a) One possible basis: {E11 −E13 −E31 + E33, E12
−E13 −E32 + E33, E21 −E23 −E31 + E33,
E22 −E23 −E32 + E33}.
(b) One extension includes E11, E12, E13, E21, and E31.
43. Basis for Sym2(R): {E11, E12 + E21, E22}. Basis for Skew2(R):
{E12 −E21}.
45. One possible basis for S: {(4, −1, 0), (3, 0, 1)}. One possible ex-
tension: include (0, 0, 1).
47. One possible basis for S: {2x2 + x + 3, x2 + x −1}. One possible
extension: include 1.
49. {e−3x, xe−3x}.
51. General vector: f (x) = c(sin 4x + 5 cos 4x), where c ∈R. One
possible extension: include cos 4x.
Section 4.7
True-False Review
(a) True
(b) True
(c) True
(d) True
(e) True
(f) False
(g) False
(h) True
Problems
1. [v]B = (0, −3).
3. [v]B = (−2, 2).
5. [v]B = (4, 6, −1).
7. [v]B = (−5, −5/3, −5/2).
9. [p(x)]B = (6, 0, −15).
11. [p(x)]B = (6, −3, 5, 1).
13. [A]B = (−2, 4, −3, −1).
15. [v]B = ( 1
9 x + 2
9 y −1
9 z, −1
9 x −2
9 y + 4
9 z, 2
9 x + 1
9 y −2
9 z).
17. PC←B =
0
3 −1
−1 −2
1
.
19. PC←B =
⎡
⎣
4 1 −2
−1 1
5
−1 1 −4
⎤
⎦.
21. PC←B =
0 3 −2
2
1
1
.
23. PC←B =
⎡
⎢⎢⎣
0 0
0 2
−2 0
0 2
5 3
0 2
−1 2 −5 2
⎤
⎥⎥⎦.
25. PC←B =
⎡
⎢⎢⎣
−2
0
0
0
1
3
0
0
1 −4
5 −4
1
1 −2
2
⎤
⎥⎥⎦.
27. PB←C = 1
7
0
2 −1
−1 −3
1
.
29. PB←C = 1
45
⎡
⎣
9 −2 −7
9
18
18
0
5 −5
⎤
⎦.
31. PB←C = 1
30
⎡
⎢⎢⎣
15 −15
0
0
−35
25 10
0
−11
13
4
−6
15
0
0
0
⎤
⎥⎥⎦.

828
Answers to Odd-Numbered Exercises
Section 4.8
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) True
(f) True
Problems
1.
(a) {6, −1)
(b) {(6, 12)}.
3.
(a) n = 5; Basis for rowspace(A) = {(1, 2, 3, 4, 5)}
(b) m = 1; Basis for colspace(A) = {1}.
5.
(a) n = 4; Basis for rowspace(A) = {(1, 1, −3, 2),
(0, 1, −2, 1)}
(b) m = 2; Basis for colspace(A) = {(1, 3), (1, 4)}.
7.
(a) n = 3; Basis for rowspace(A) = {(0, 3, 1)}
(b) m = 3; Basis for colspace(A) = {(3, −6, 12)}.
9.
(a) n = 4; Basis for rowspace(A) = {(1, −1, 2, 3),
(0, 2, −4, 3), (0, 0, 6, −13)}
(b) m = 3; Basis for colspace(A) = {(1, 1, 3), (−1, 1, 1),
(2, −2, 4)}.
11.
(a) {(1, 3, 3), (0, 1, −2)}
(b) {(1, 3, 3), (1, 5, −1)}.
13.
(a) {(1, 4, 1, 3), (0, 0, 1, −1)}
(b) {(1, 4, 1, 3), (2, 8, 3, 5)}.
15. Many examples possible, such as 0n,
! 0 1
0 0
"
.
Section 4.9
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) True
(f) False
(g) False
(h) True
(i) True
Problems
1. nullspace(A) = {0}.
3. nullspace(A) = {(t, 2t) : t ∈R}.
5. nullspace(A) = {(5t + s, −t −s, t, s) : t, s ∈R}.
7. nullity(A) = 3.
9. nullity(A) = 3.
11. {(−1 + t −s, −7 + 3t + 2s, t, s) : t, s ∈R}.
13. {( 1
2 t −3
2 s, 1
2 t −7
2 s, t, s) : t, s ∈R}.
15. No.
17. Many possible examples: A =
⎡
⎢⎢⎢⎢⎣
1 0 0 0 0 0 0
0 1 0 0 0 0 0
0 0 1 0 0 0 0
0 0 0 1 0 0 0
0 0 0 0 1 0 0
⎤
⎥⎥⎥⎥⎦
and
A = 05×7.
Section 4.10
True-False Review
(a) True
(b) False
(c) False
(d) False
(e) True
(f) False
(g) False
(h) True
(i) False
(j) False
Section 4.11
Additional Problems
3. Yes.
5. Yes.
7. Yes.
9. Yes.
11. No.
17. dim[S] = 2.
19. No.
21. Yes.
23. Yes.
25. Both.
27. Neither.
29. Neither.
31. Neither.
35. (b) dim[S] = 9.
37. As an example, here is a basis: {x3, x3 + 1, x3 + x, x3 + x2}.
41. Basis for rowspace(A) = {(1, −6, −2, 0), (0, 1, 1
3, 5
21)}; Basis for
colspace(A) = {(−1, 3, 7), (6, 3, 21)}; Basis for nullspace(A) =
{(−10
7 , −5
21, 0, 1), (0, −1
3, 1, 0)}.
43. Basis for rowspace(A) = {(1, 0, 2, 2, 1), (0, 1, −1, −4, −3),
(0, 0, 1, 4, 3), (0, 0, 0, 1, 0)};
Basis for colspace(A) = {(3, 1, 1, −2), (5, 0, 1, 0), (5, 2, 1, −4),
(2, 2, −2, −2)};
Basis for nullspace(A) = {(5, 0, −3, 0, 1)}.

Answers to Odd-Numbered Exercises 829
Chapter 5
Section 5.1
True-False Review
(a) False
(b) False
(c) True
(d) True
(e) False
(f) True
(g) False
Problems
1. θ = π/2 rad.
3. cos θ =
√
6/π or θ ≈0.68 rad.
5. cos θ =
√2m+1√2n+1(bm+n+1−am+n+1)
√
b2m+1−a2m+1√
b2n+1−a2n+1(m+n+1) .
7. ⟨v, w⟩= 24 −8i, ||v|| =
√
99, ||w|| =
√
30.
9. Only property 2 holds.
13. ⟨A, B⟩= 12, ||A|| =
√
39, ||B|| =
√
15, θ ≈1.05 rad.
15. No.
17. No.
19.
(a) 0
(b) −1.
21.
(a) −3
(b) 0.
23. v = r(1, 1) or v = s(1, −1), where r, s ∈R.
25. {(v1, v2) ∈R2 : v2
1 > v2
2}.
31. ⟨u + v, w + x⟩= ⟨u, w⟩+ ⟨u, x⟩+ ⟨v, w⟩+ ⟨v, x⟩.
33.
(a)
√
40
(b)
√
976
(c) −120.
Section 5.2
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) True
(f) True
(g) True
Problems
1. Orthonormal set: { 1
√
5(1, 2), 1
√
5 (−2, 1)}.
3. Orthonormalset:{
√
3
6 (1, 3, −1, 1), 1
2 (−1, 1, 1, −1),
√
6
6 (1, 0, 2, 1)}.
5. Orthonormal set: {
1
√
15(1, 2, −1, 0, 3), 1
√
7 (1, 1, 0, 2, −1),
1
√
77 (4, 2, −4, −5, −4)}.
7. Orthogonal vectors (a, b, c) ∈R3 satisfy −3a + 6b + c = 0.
Orthogonal basis: {(−3, 6, 1), (1, 0, 3), (0, −1, 6)}.
9. Orthogonal vectors (a, b, c, d) ∈R4 satisfying a = 1
4 d and
b = −3
2 c −17
8 d. Orthogonal basis: {(−4, 0, 0, 1), (1, 2, 3, 4),
(1, 0, 0, 4), (0, −3, 2, 0)}.
11. Orthonormal set: {
1
√
15 (1 −i, 3 + 2i),
1
√
15(2 + 3i, 1 −i)}.
13. z = 1 −i. Orthonormal set: { 1
√
7(1 −i, 1 + 2i), 1
√
7(2 + i, 1 −i)}.
15. Orthonormal set: { 1
√
2 , 3
√
6 x,
5
2
√
10 (3x2 −1)}.
19. Polynomials of the form r(x) = t(−x + x2).
23.
2
√
41.
25.
6
√
35
7
.
27.
√
31858
34
.
29.
16
√
21.
31.
1
√
5.
33. λ = −⟨v,u1⟩
||u1||2 , µ = −⟨v,u2⟩
||u2||2
Section 5.3
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) True
Problems
1. Orthonormal basis: {
1
√
14 (1, 2, 3), 1
√
5(2, −1, 0)}.
3. Orthonormal basis: { 1
3(2, 1, −2),
√
2
6 (−1, 4, 1)}.
5. Orthonormal basis: { 1
√
5(2, 0, 1), 1
√
6(−1, 1, 2),
1
√
30 (−1, −5, 2)}.
7. Orthonormal basis: { 1
√
2 (1, 0, −1, 0), (0, 1, 0, 0),
1
√
6(−1, 0, −1, 2)}.
9. Orthonormal basis: { 1
√
3 (1, 1, −1, 0),
1
√
15 (−1, 2, 1, 3),
1
√
15(3, −1, 2, 1)}.
11. Orthogonal basis for rowspace(A): {(1, −3, 2, 0, −1),
(11, −18, −23, 5, 19)};
orthogonal
basis
for
colspace(A):
{(1, 0), (0, 1)}.
13. Orthogonal basis for rowspace(A): {(1, −2, 1), (13, 16, 19)};
orthogonal basis for colspace(A): {(3, 1, 1), (−1, −4, 7)}.
15. Orthonormal basis: {
√
2
4 (1 + i, i, 2 −i),
√
118
236 (9 + 13i, 10 −9i,
−4 + 5i)}.
17. Orthogonal basis: {1 + 2x, −1 + x + x2}.

830
Answers to Odd-Numbered Exercises
19. Orthogonal basis: {1, 1
3(3x2 −1), 1
35(35x4 −30x2 + 3)}.
21. Orthogonal basis:
A0 1 −1
2
1
1
, 1
8
0 1 −9
2 −7
1B
.
23. Orthogonal basis: {1 −2x + 2x2, 16 −5x −13x2}.
Section 5.4
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) True
(f) True
Problems
1. y = 3
8 x + 3
4 .
3. y = 40
3 .
5. y = x −1.
7. y = −.096x + 1.4.
9. y = 2x2 −5x.
13. k = 107
35 pounds per square inch.
15. P(t) = 1.510e0.099t.
17. (a) m × m.
Section 5.5
Additional Problems
1. θ = cos−1 (
5
√
221
)
≈1.23 rad.
3. θ = cos−1 (
13
√
561
)
≈0.99 rad.
5. nullspace(A) = {0}, so the basis is empty; orthonormal basis for
rowspace(A) = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}; orthonormal basis
for colspace(A) = { 1
2 (1, −1, 0, 1, 1), 1
√
8 (−1, 1, 2, 1, 1),
1
6(3, 3, 0, −3, 3)}.
7. Orthogonal basis: {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.
9. Orthogonal basis:
A0 1 0
0 0
1
,
0 0 1
1 0
1
,
0 0 0
0 1
1B
.
11.
√
78864
53
.
13.
17
√
19 .
15. y = 1.1x −1.8.
17. y = 2x + 7.
Chapter 6
Section 6.1
True-False Review
(a) False
(b) False
(c) False
(d) True
(e) True
(f) True
Problems
15. A =
⎡
⎣
1
3
2 −7
1
0
⎤
⎦.
17. A =
:
1 5 −3
;
.
19. T (x1, x2) = (x1 + 3x2, −4x1 + 7x2).
21. T (x1, x2, x3) = (2x1 + 2x2 −3x3, 4x1 −x2 + 2x3,
5x1 + 7x2 −8x3).
23. T (x1, x2, x3, x4, x5) = x1 −4x2 −6x3 + 2x5.
27. A =
⎡
⎢⎢⎢⎢⎢⎣
−5
3
−2
3
1
3
1
3
5
3
−1
3
−1
1
⎤
⎥⎥⎥⎥⎥⎦
.
29. A =
⎡
⎣
−4
3
0
−5
2 −3
15 −7
6
⎤
⎦.
31. T (ax2 + bx + c) = bx2 + (3a + c)x + (2a −b + c).
33. T (1) = −2x + 3, T (x) = 2x, T (x2) = x2 −x.
Section 6.2
True-False Review
(a) False
(b) True
(c) False
(d) True
(e) False
(f) False
Problems
5. Stretch in the y-direction, followed by a stretch in the x-direction,
followed by reﬂection across line y = x.
7. Reﬂection in the x-axis, followed by a reﬂection in the y-axis.
9. Reﬂection in the x-axis, followed by a stretch in the y-direction, fol-
lowed by a shear parallel to the x-axis, followed by a shear parallel
to the y-axis.
11. Shear parallel to the x-axis, followed by a reﬂection in the y-axis,
followed by a shear parallel to the y-axis.
13. For θ = π/2, this corresponds to a reﬂection in the x-axis followed
by a reﬂection across the line y = x. For θ = 3π/2, this corre-
sponds to a reﬂection in the y-axis followed by a reﬂection across
the line y = x.

Answers to Odd-Numbered Exercises 831
Section 6.3
True-False Review
(a) False
(b) False
(c) False
(d) False
(e) True
(f) True
Problems
1.
(a) T (x) = (0, 0, 0, 0), Yes
(b) T (x) = (−1, −2, −4, −8), No
(c) T (x) = (0, 0, 0, 0), Yes.
3. Ker(T ) = {x ∈R2 : x = (−2t, t), t ∈R}, dim[Ker(T )] = 1;
Rng(T ) = {y ∈R2 : y = (3r,r),r ∈R}, dim[Rng(T )] = 1.
5. Ker(T ) = {x ∈R3 : x = (5t, 3t, t), t ∈R}, dim[Ker(T )] = 1;
Rng(T ) = {y ∈R3 : y = (r −2s, 2r −3s, 5r −8s),r, s ∈R},
dim[Rng(T )] = 2.
7. Ker(T ) = {x ∈R3 : x = (−3r,r, 0),r ∈R}, dim[Ker(T )] = 1;
Rng(T ) = R2, dim[Rng(T )] = 2.
9. Ker(T ) = span
<(
16
13, 15
13, 1, 0
)
,
(
−5
13, −12
13 , 0, 1
)=
;
Rng(T ) = R2.
11. Ker(T ) = {0};Rng(T ) = span {(2, 5, −2, 1), (−1, 0, 0, 5), (−3, 1, 1, 3)}.
13.
(a) Ker(S) consists of the set of n × n symmetric matrices with
entries in R; dim[Ker(S)] = n(n+1)
2
.
(b) One possible basis: {E11, E12 + E21, E22};
dim[Ker(S)] = 3.
15.
(a) dim[Ker(T )] = 1
(b) Rng(T ) = span{x2 + x + 3, x −1}; dim[Rng(T )] = 2.
17. Ker(T ) = {0}, dim[Ker(T )] = 0;
Rng(T ) = span{−3x −1, x2 + 2x + 1}, dim[Rng(T )] = 2.
19. Ker(T ) = {x ∈R2 : x = (t, t), t ∈R}, dim[Ker(T )] = 1;
Rng(T ) = span
A0 −1 0
2
0 3 −9
1B
, dim[Rng(T )] = 1.
21. Ker(T ) = {v ∈V : v = r(−v1 + v2 + v3 : r ∈R},
dim[Ker(T )] = 1; Rng(T ) = W, dim[Rng(T )] = 2.
Section 6.4
True-False Review
(a) False
(b) True
(c) True
(d) True
(e) True
(f) True
(g) False
(h) True
(i) True
(j) False
(k) False
(l) True
Problems
1. (T2T1)(x, y) = 2x + 3y. T1T2 does not exist.
3. (T1T2)(x, y, z) = (−x −8y + 5z, x + z, x −4y + 4z),
Ker(T1T2) = span{(−4, 3, 4)}, Rng(T1T2) = span{(−1, 1, 1),
(−8, 0, −4)}; (T2T1)(x, y) = (3x −y, 3x), Ker(T2T1) = {(0, 0)},
Rng(T2T1) = R2.
7. Writing v = av1+bv2, then (T2T1)(v) = (5b−2a)v1+3(a+b)v2.
9. Onto only.
11. Ker(T ) = {(0, 0)}, Rng(T ) = R2; T is both one-to-one and onto.
T −1(x) = A−1x = 1
10
0
3 −2
−1
4
1
.
13. Ker(T ) = span{(7, −3, 1)}, Rng(T ) = R2; T is onto only.
15.
(a) A =
0 4 −1 0
5
1 4
1
(b) T is onto only.
17. T −1(ax + b) = 1
3 (2b −a)x + 1
3(a + b).
19. Neither; T −1 does not exist.
21. T is one-to-one only; T −1 does not exist.
23. T is both; T −1(av1 + bv2) = 1
7(3a + 2b)v1 + 1
7(2a −b)v2.
25. One example is T (a, b) = ax + b.
27. One example is T (a) =
0 0
−a
a
0
1
.
29. n = 10; One example is T
⎛
⎜⎜⎝
⎡
⎢⎢⎣
a
b
c
d
0
e
f
g
0
0
h
i
0
0
0
j
⎤
⎥⎥⎦
⎞
⎟⎟⎠
= (a, b, c, d, e, f, g, h, i, j).
31. n = 3; One example is T
⎛
⎝
⎡
⎣
0
−a
−b
a
0
−c
b
c
0
⎤
⎦
⎞
⎠= (a, b, c).
33. T −1(x) =
0
3 −1
−2
1
1
x.
35. T −1(x) = 1
14
⎡
⎣
11 −16
1
−6
10
2
3
2 −1
⎤
⎦x.
37. Matrix of T4T3:
⎡
⎣
10 25 23
5
14 15
12 19
1
⎤
⎦; Matrix of T3T4:
⎡
⎣
6
13 18
4
8
6
23 43 11
⎤
⎦.
Section 6.5
True-False Review
(a) False
(b) False
(c) False
(d) False
(e) True
(f) True

832
Answers to Odd-Numbered Exercises
Problems
1.
(a) [T ]C
B =
⎡
⎢⎢⎣
1 0 0 −1
0 0 0
0
0 3 0
0
−1 0 1
0
⎤
⎥⎥⎦
(b) [T ]C
B =
⎡
⎢⎢⎣
0
0
0 0
0
1 −1 0
1 −1
0 0
0
0
0 3
⎤
⎥⎥⎦.
3.
(a) [T ]C
B =
⎡
⎢⎢⎣
1 0 0
1 1 0
0 1 1
0 0 1
⎤
⎥⎥⎦
(b) [T ]C
B =
⎡
⎢⎢⎣
2 0 0
1 2 0
0 1 2
0 0 1
⎤
⎥⎥⎦.
5.
(a) [T ]C
B =
0 1 0 0 1
1 0 0 1
1
(b) [T ]C
B =
0 −4 3 −2 0
−4 3 −2 0
1
.
7.
(a) [T ]C
B =
0 2
0
0 −3
1
(b) [T ]C
B =
0 9 −6
7 −6
1
.
9. T (v) = 4 + 4x + 8x3.
11. T (p(x)) = −x2 + 5x3 −6x4.
13. T (p(x)) = −4 + 12x + 18x2 −8x3.
15. T (p(x)) = −8.
17.
(a) A = 02
(c) (T2T1)(−3 + 8x) = (0, 0).
19. No.
Section 6.6
Additional Problems
1. Not a linear transformation.
3. Linear; Onto only; Basis for Ker(T ): {(1, 2, 0)}; dim[Ker(T )] = 1;
Basis for Rng(T ): {(1, 0), (0, 1)}; dim[Rng(T )] = 2.
5. Linear; Onto only; Basis for Ker(T ): {(−1, 1)}; dim[Ker(T )] = 1;
Basis for Rng(T ): {1}; dim[Rng(T )] = 1.
7. Linear; One-to-one only; Basis for Ker(T ): ∅; dim[Ker(T )] = 0;
Basis for Rng(T ):
A0 −1 0
−1 0
1
,
0 −1
0
0 −2
1
,
0 0 0
3 0
1B
;
dim[Rng(T )] = 3.
9. Linear; Neither; Basis for Ker(T ): {(0, 1, 2)}; dim[Ker(T )]= 1;
Basis for Rng(T ): {x2 + 1, 2x −2}; dim[Rng(T )] = 2.
11. T (x, y, z) = (−x + 8y, 2x −2y −5z).
13. T (x) = (−1
2 x, 5
2 x, 0, −x).
15. T (a + bx + cx2) =
0 2a −5b −c
a −2b + 2c
a −3
2 b −5
2 c 3a −17
2 b −1
2 c
1
.
17. 6.
19. 2, 3, 4, or 5.
21. [T ]C
B =
⎡
⎢⎢⎣
0 −1
5
−3
0
0
0
0
0
1
3 −1
⎤
⎥⎥⎦; T (v) =
0
0 −3
−8 −3
1
.
23. [T ]C
B =
0 −1 0 1
2 0 0
1
; T (v) = 2x.
25. False.
Chapter 7
Section 7.1
True-False Review
(a) False
(b) True
(c) True
(d) True
(e) True
(f) False
(g) False
(h) True
(i) True
Problems
7. λ = 2, 5.
9. Vectors (t, t) with t ̸= 0, corresponding to λ = 1; Vectors (t, −t)
with t ̸= 0, corresponding to λ = −1.
11. Vectors (0, y, 0) with y ̸= 0, corresponding to λ = 1; Vectors
(x, 0, z) with x, z not both zero, corresponding to λ = 0.
13. λ1 = 3, v1 = r(3, 1); λ2 = −5, v2 = s(−1, 1).
15. λ = 2, v = s(1, 0) + t(0, 1).
17. λ1 = 1 + 3i, v1 = r(1 −i, −1); λ2 = 1 −3i, v2 = s(1 + i, −1).
19. λ1 = 2 + 3i, v1 = r(1, i); λ2 = 2 −3i, v2 = s(1, −i).
21. λ1 = 1, v1 = r(0, 1, 1); λ2 = 3, v2 = s(0, −1, 1).
23. λ1 = −1, v1 = r(1, 3, 4); λ2 = 1, v2 = s(3, −5, 0); λ3 = 3,
v3 = t(−1, 1, 0).
25. λ1 = 1, v1 = r(−1, 0, 1); λ2 = 2, v2 = s(1, 2, 0) + t(−1, 0, 2).
27. λ1 = −2, v1 = r(1, 0, 1); λ2 = −2 + i, v2 = s(2 −i, 1 + 2i, 5);
λ3 = −2 −i, v3 = t(2 + i, 1 −2i, 5).
29. λ = 5, v = r(1, 0, 0) + s(0, 1, 0) + t(0, 0, 1).
31. λ1 = 16, v1 = r(17, 13, 35, 31); λ2 = −2, v2 = s(−1, 1, −1, 1);
λ3 = 0, v3 = a(1, −2, 1, 0) + b(2, −3, 0, 1).
33. λ1 = 1 + i, v1 = r(−2, 1 + i, −4); λ2 = 1 −3i, v2 = s(0, 1, 0);
λ3 = 1, v3 = t(0, 0, 1).
35.
(a) λ = −3, 2
(b) No.
37.
(a) v = 2v1 + v2 −v3
(b) (3, −3, −8).

Answers to Odd-Numbered Exercises 833
Section 7.2
True-False Review
(a) True
(b) True
(c) True
(d) True
(e) True
(f) False
Problems
1. λ1 = −7, m1 = 2, basis for E1: {(0, 1)}, n1 = 1;
defective.
3. λ1 = 3, m1 = 2, basis for E1: {(1, 0), (0, 1)}, n1 = 2;
nondefective.
5. λ1 = 2+i, m1 = 1, basis for E1: {(−3−i, 2)}, n1 = 1; λ2 = 2−i,
m2 = 1, basis for E2: {(−3 + i, 2)}, n2 = 1; nondefective.
7. λ1 = −1, m1 = 1, basis for E1: {(0, 1, 1)}, n1 = 1; λ2 = 4,
m2 = 2, basis for E2: {(1, 0, 0), (0, 3, −2)}, n2 = 2; nondefective.
9. λ1 = 3, m1 = 1, basis for E1: {(25, 2, 11)}, n1 = 1; λ2 = 4i,
m2 = 1, basis for E2: {(0, 1, −i)}, n2 = 1; λ3 = −4i, m3 = 1,
basis for E3: {(0, 1, i)}, n3 = 1; nondefective.
11. λ1 = 2, m1 = 3, basis for E1: {(1, 0, 0), (0, 1, 0), (0, 0, 1)},
n1 = 3; nondefective.
13. λ1 = 2, m1 = 1, basis for E1: {(3, 2, 4)}, n1 = 1; λ2 = 0, m2 = 2,
basis for E2: {(1, 0, 2)}, n2 = 1; defective.
15. λ1 = 2, m1 = 3, basis for E1: {(1, 0, 1)}, n1 = 1; defective.
17. Nondefective.
19. Nondefective.
21. Defective.
23. λ1 = −7, basis for E1: {(0, 1)}.
25. λ1 = 2, basis for E1: {(1, 0)}.
27. λ1
= 5, basis for E1: {(1, 1, −1)}; λ2
= 2, basis for E2:
{(−1, 1, 0), (1, 0, 1)}.
29.
(a) {(−3, 0, 1), (1, 5, 3)}
(b) No.
31. Either a + b + c ̸= 0 or a = b = c = 0.
33. Sum is −3; Product is −33.
35. Sum is 1; Product is −69.
Section 7.3
True-False Review
(a) True
(b) True
(c) False
(d) False
(e) True
(f) True
(g) True
(h) True
(i) True
Problems
1. Not diagonalizable.
3. Not diagonalizable.
5. S =
0 i
−i
1
1
1
; S−1 AS = diag(−4i, 4i).
7. Not diagonalizable.
9. S =
⎡
⎣
1 1 2
1 2 0
1 0 1
⎤
⎦; S−1 AS = diag(3, 0, 0).
11. S =
⎡
⎣
17
0
0
9
1 −i
1 + i
6
−2
−2
⎤
⎦; S−1 AS = diag(4, i, −i).
13. S =
⎡
⎣
1 −1 0
1
1 0
0
0 1
⎤
⎦; S−1 AS = diag(−1, 3, 3).
15. S =
⎡
⎢⎢⎣
−2 −3 −4 −1
1
0
0
1
0
1
0 −2
0
0
1
2
⎤
⎥⎥⎦; S−1 AS = diag(0, 0, 0, −3).
19. x1(t) = c1e4t −c2e8t, x2(t) = c1e4t + c2e8t.
21. x1(t) = c1e2t + 7c2e−4t, x2(t) = −2c1e2t −8c2e−4t.
23. x1(t) = c1e−2t + c2e3t, x2(t) = c1e−2t + c3e3t,
x3(t) = c1e−2t −4c3e3t.
27. A3 =
0 −127 −84
378
251
1
; A5 =
0 −3127 −2084
9378
6251
1
.
Section 7.4
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) False
Problems
1. eAt =
8
et
e3t −et
0
e3t
9
.
3. eAt =
8
cos 2t
sin 2t
−sin 2t
cos 2t
9
.
5. eAt = eat
8
cos bt
sin bt
−sin bt
cos bt
9
.
7. eAt = et
⎡
⎢⎢⎣
5et −4
2(1 −et)
1 −et
8(et −1)
4 −3et
2(1 −et)
4(et −1) 2(1 −et)
1
⎤
⎥⎥⎦.

834
Answers to Odd-Numbered Exercises
9. eAt =
8
e−3t
0
0
e5t
9
; e−At =
8
e3t
0
0
e−5t
9
.
11.
(b) eCt =
8
1 bt
0
1
9
(c) eAt =
8
eat
bteat
0
eat
9
.
13. eAt =
8
1 + t
t
−t
1 −t
9
.
15. eAt =
⎡
⎢⎢⎣
1 −t −2t2
−6t + 4t2
−5t −2t2
−t2
2
1 −2t + t2
−t −t2
2
t + t2
2t −2t2
1 + 3t + t2
⎤
⎥⎥⎦.
17. eAt =
⎡
⎢⎢⎢⎢⎣
1
t
1
2 t2
1
6t3
0 1
t
1
2 t2
0 0
1
t
0 0
0
1
⎤
⎥⎥⎥⎥⎦
.
Section 7.5
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) True
(f) True
(g) True
(h) True
Problems
1. S =
⎡
⎣
1
√
5
2
√
5
−2
√
5
1
√
5
⎤
⎦; ST AS = diag(−7, 3).
3. S =
⎡
⎣
−
3
√
13
2
√
13
2
√
13
3
√
13
⎤
⎦; ST AS = diag(0, 13).
5. S =
⎡
⎢⎢⎢⎣
−1
√
2
0
1
√
2
0 1
0
1
√
2
0
1
√
2
⎤
⎥⎥⎥⎦; ST AS = diag(−3, −2, 3).
7. S =
⎡
⎢⎢⎢⎣
1
0
0
0 −1
√
2
1
√
2
0
1
√
2
1
√
2
⎤
⎥⎥⎥⎦; ST AS = diag(2, 2, 4).
9. S =
⎡
⎢⎢⎢⎢⎣
1
√
2
−1
√
6
1
√
3
1
√
2
1
√
6
−1
√
3
0
2
√
6
1
√
3
⎤
⎥⎥⎥⎥⎦
; ST AS = diag(2, 2, −1).
11. S =
⎡
⎢⎢⎢⎢⎣
−1
√
2
0
1
√
2
3
5
√
2
−4
5
3
5
√
2
4
5
√
2
3
5
4
5
√
2
⎤
⎥⎥⎥⎥⎦
; ST AS = diag(−2, 3, 8).
13. S =
⎡
⎢⎢⎢⎢⎣
−1
√
2
−1
√
6
1
√
3
0
2
√
6
1
√
3
1
√
2
−1
√
6
1
√
3
⎤
⎥⎥⎥⎥⎦
; ST AS = diag(−1, −1, 2).
15. Principal axes:
<(
1
√
2 , 1
√
2
)
; Reduced quadratic form: 7y2
1 + 3y2
2.
21.
(a) S =
⎡
⎢⎣
a
√
a2+b2
−
b
√
a2+b2
b
√
a2+b2
a
√
a2+b2
⎤
⎥⎦.
(b) A =
1
a2+b2
0 λ1a2 + λ2b2
ab(λ1 −λ2)
ab(λ1 −λ2) λ1b2 + λ2a2
1
.
25. λ1 = 0, v1 = r(−1, 2, 2); λ2 = 6i, v2 = s(2 + 6i, −4 + 3i, 5);
λ3 = −6i, v3 = t(2 −6i, −4 −3i, 5).
Section 7.6
True-False Review
(a) True
(b) True
(c) False
(d) False
(e) True
(f) False
(g) True
(h) False
(i) True
(j) True
(k) True
(l) False
Problems
1. 1. Block sizes: (1, 1, 1).
3. 4. Block sizes: (2; 2), (2; 1, 1), (1, 1; 2), (1, 1; 1, 1).
5. 10. Block sizes: (4; 2), (3, 1; 2), (2, 2; 2), (2, 1, 1; 2), (1, 1, 1, 1; 2),
(4; 1, 1), (3, 1; 1, 1), (2, 2; 1, 1), (2, 1, 1; 1, 1), (1, 1, 1, 1; 1, 1).
7. 55.
9.
(a) Block sizes: (3; 2), (2, 1; 2), (1, 1, 1; 2), (3; 1, 1),
(2, 1; 1, 1), (1, 1, 1; 1, 1).
(b) (2, 1; 2), (1, 1, 1; 2), (2, 1; 1, 1), (1, 1, 1; 1, 1).
11.
⎡
⎣
4 0
0
0 4
0
0 0 −6
⎤
⎦,
⎡
⎣
4 1
0
0 4
0
0 0 −6
⎤
⎦.

Answers to Odd-Numbered Exercises 835
13. There are 12 matrices in S. Block sizes: (3; 2; 2), (2, 1; 2; 2),
(1, 1, 1; 2; 2),
(3; 2; 1, 1),
(2, 1; 2; 1, 1),
(1, 1, 1; 2; 1, 1),
(3; 1, 1; 2),
(2, 1; 1, 1; 2),
(1, 1, 1; 1, 1; 2),
(3; 1, 1; 1, 1),
(2, 1; 1, 1; 1, 1), (1, 1, 1; 1, 1; 1, 1).
15. Use two Jordan blocks of size 2 × 2 and three of size 1 × 1, or use
one Jordan block of size 3 × 3 and four blocks of size 1 × 1.
17. One possible example: A =
⎡
⎣
0 1 0
0 0 0
0 0 0
⎤
⎦; Generalized eigenvector
v = (0, 1, 0).
19. J =
0 8 1
0 8
1
, S =
0 −4 1
−4 0
1
.
21. J =
⎡
⎣
0 0 0
0 1 0
0 0 2
⎤
⎦, S =
⎡
⎣
0 1 2
−1 0 1
1 0 1
⎤
⎦.
23. J =
⎡
⎣
2 0 0
0 5 1
0 0 5
⎤
⎦, S =
⎡
⎣
2 −1 1
1 −1 0
0 −1 0
⎤
⎦.
25. J =
⎡
⎣
2 0 0
0 2 0
0 0 3
⎤
⎦, S =
⎡
⎣
1 0 −2
0 7
1
0 1
0
⎤
⎦.
27. J =
⎡
⎣
−1
1
0
0 −1
1
0
0 −1
⎤
⎦, S =
⎡
⎣
2
0 0
0 −2 0
0
0 1
⎤
⎦.
29. J =
⎡
⎢⎢⎣
2 0 0 0
0 2 0 0
0 0 4 1
0 0 0 4
⎤
⎥⎥⎦, S =
⎡
⎢⎢⎣
2 0 0 1
1 1 1 0
0 2 1 0
2 0 1 1
⎤
⎥⎥⎦.
31. J =
⎡
⎢⎢⎢⎢⎣
0 1 0 0 0
0 0 1 0 0
0 0 0 0 0
0 0 0 0 1
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
.
33. No.
35. Yes.
37. x1(t) = −2(c1te−t +c2e−t)+c1e−t; x2(t) = 2(c1te−t +c2e−t).
39. x1(t) = c2e−2t + c3e−2t; x2(t) = c2te−2t + c1e−2t + c3e−2t;
x3(t) = −(c2te−2t + c1e−2t).
41. x1(t) = 4e−t(c2t + c1) + c2e−t;
x2(t) = c3e2t; x3(t) = −4e−t(c2t + c1).
Section 7.7
Additional Problems
1. S =
0 1 0
4 1
1
, D =
0 3
0
0 −1
1
.
3. S =
⎡
⎣
−1 1 0
−2 1 0
1 0 1
⎤
⎦, D =
⎡
⎣
2
0
0
0 −1
0
0
0 −1
⎤
⎦.
5. Not diagonalizable.
11. J =
⎡
⎣
1
0
0
0 −3
0
0
0 −3
⎤
⎦.
17. False.
19. λ1λ2 · · · λk.
Chapter 8
Section 8.1
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) True
(f) True
(g) True
(h) True
(i) True
(j) False
Problems
1.
(a) 2e3x(3 −x).
(b)
3
x (1 −x2 ln x).
(c) 2e3x(3 −x) + 3
x (1 −x2 ln x).
3.
(a) 18e3x(3 −2x).
(b)
6
x3 (1 + x2).
(c) 18e3x(3 −2x) + 6
x3 (1 + x2).
11. Ker(L) = {a cos x + b sin x : a, b ∈R}.
13. Ker(L) =
A c
|x| : c ∈R
B
.
15. L1L2 = D2 + (3x −1)D + (2x2 −x + 2).
L2L1 = D2 + (3x −1)D + (2x2 −x + 1).
17. (D3 + x2D2 −(sin x)D + ex)y = x3.
y′′′ + x2y′′ −(sin x)y′ + ex y = 0.
21. S4 and S6.
23. y(x) = c1e−x + c2e3x.
25. y(x) = c1e−6x + c2e6x.
27. y(x) = c1ex + c2e−x + c3e3x.
29. y(x) = c1e−5x + c2e−2x + c3e4x.
31. y(x) = c1e−4x + c2ex + c3e2x.
33. y(x) = c1e−3x + c2e−2x + c3e2x + c4e3x.
35. y(x) = c1x−1
2 + c2x−1.
37. y(x) = c1 + c2x
√
7 + c3x−
√
7.

836
Answers to Odd-Numbered Exercises
39. yp(x) = −11
2 −2x −2x2.
General solution: y(x) = c1ex + c2e−2x −11
2 −2x −2x2.
41. yp(x) = 6
5e−3x.
General solution: y(x) = c1ex + c2e2x + c3e−4x + 6
5 e−3x.
Section 8.2
True-False Review
(a) False
(b) False
(c) True
(d) True
(e) False
(f) True
(g) True
(h) False
Problems
1. {e−3x, ex}.
3. {e3x cos 4x, e3x sin 4x}.
5. Orthogonal basis:
<
ex, e−2x −
2
e(e+1) ex=
.
7. Orthonormal basis:
<
1
√π cos x,
1
√π sin x
=
.
9. y(x) = c1e3x + c2xe3x.
11. y(x) = c1e−x + c2e5x.
13. y(x) = c1e3x cos 5x + c2e3x sin 5x.
15. y(x) = c1e−
√
2x + c2e
√
2x.
17. y(x) = c1e−x cos x + c2e−x sin x.
19. y(x) = c1e7x cos 3x + c2e7x sin 3x.
21. y(x) = c1e−2x + (c2 + c3x)e2x.
23. y(x) = e−x[c1 cos 3x + c2 sin 3x + x(c3 cos 3x + c4 sin 3x)].
25. y(x) = c1e−x + c2xe−x + c3 cos (
√
3x) + c4 sin (
√
3x).
27. y(x) = e2x(c1 + c2x) + e−2x(c3 + c4x).
29. y(x) = c1e−2x + e−3x(c2 cos x + c3 sin x).
31. y(x) = ex(c1 + c2x + c3x2) + c4 cos 3x + c5 sin 3x.
33. y(x) = c1e−3x + c2ex + e−5x(c3 + c4x + c5x2).
35. y(x) = 2e4x −xe4x.
37. y(x) = ex −cos x.
39. y(x) = emx sin kx.
49. y(x) = c1e4x + c2e−4x + e−2x(c3 cos 3x + c4 sin 3x).
51. y(x) = c1e−4x + c2 cos 5x + c3 sin 5x + x(c4 cos 5x + c5 sin 5x).
Section 8.3
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) False
(f) False
(g) False
(h) True
Problems
1. A(D) = D + 3.
3. A(D) = (D2 + 1)(D −2)2.
5. A(D) = D2 + 4D + 5.
7. A(D) = D3(D −4)2.
9. A(D) = D2 + 6D + 10.
11. A(D) = (D2 + 1)3.
13. A(D) = A1(D)A2(D) = D3 + 4D.
15. A(D) = A1(D)A2(D) = (D2 +4)(D2 +16) = D4 +20D2 +64.
17. y(x) = c1ex + c2e−2x + 1
2 e3x.
19. y(x) = c1 cos 4x + c2 sin 4x + 4
15 cos x.
21. y(x) = c1e2x + c2e−x −5 + 6x −2x2.
23. y(x) = c1e−x + c2e3x −xe−x + 8
5 cos x + 4
5 sin x.
25. y(x) = c1 cos x + c2 sin x + 3ex.
27. y(x) = c1 cos 2x + c2 sin 2x −2x cos 2x.
29. y(x) = e−x(c1 cos 2x + c2 sin 2x) −12
17 cos 2x + 3
17 sin 2x.
31. y(x) = c1ex + c2 cos x + c3 sin x −9
4 e−x.
33. y(x) = cos 3x + sin 3x + cos 2x.
35. y(x) = e−2x + cos x + 3 sin x.
37. y(x) = ex + e2x + e3x + e4x.
39. yp(x) = A0xe2x.
41. yp(x) = x2e−2x(A0 cos 3x + B0 sin 3x).
43. yp(x) = A0xex + x2(A1 cos 2x + A2 sin 2x).
45. yp(x) = A0xe3x + xe2x(A1 cos x + A2 sin x).
47. yp(x) = x(A0 cos 4x + B0 sin 4x) + A1.
Section 8.4
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) True

Answers to Odd-Numbered Exercises 837
Problems
1. yp(x) = −5
8 cos 4x
3. yp(x) = e2x(cos x + 2 sin x).
5. yp(x) = −10 + (3 cos 2x + sin 2x).
7. yp(x) = −xe−x cos x.
9. yp(x) = xe−x sin 2x.
11. yp(x) = Re(z p) = 4xex sin 3x.
Section 8.5
True-False Review
(a) True
(b) False
(c) True
(d) True
(e) False
(f) True
(g) True
(h) False
(i) True
Problems
1. ω0 = 2, A0 = 2
√
2, φ = π/4, T = π.
3.
(a) k = 3.
(b) ω0 =
√
3/2, A0 = 2
√
3/3, φ = 5π/6, T = 4π
√
3/3.
5. Critically damped, y(t) = 4e−t/2 + te−t/2.
7. Underdamped, y(t) = e−t(cos 2t + 2 sin 2t).
9. Overdamped, y(t) = −1
4 e−t/2 + 5
4 e−5t/2.
11. t = ln 2; max. displacement: 1/27.
13.
(a) y(t) = 1
10 e−t/10(51t + 10).
(b) ≈19.13.
17. ω0 = 2
I g
L , T = π
I L
g .
19. Amplitude =
J
α2g + β2L
g
; phase = tan−1
$
β
α
J
L
g
%
;
period = 2π
J
L
g .
21. ≈63.
23. T = 2π
ω where ω2 = g(L2 + L0L + L2
0)
L2
*
L2 −L2
0
.
25. yp(t) =
√
10 sin (t −tan−1 3).
29. T = 8π.
Section 8.6
True-False Review
(a) True
(b) True
(c) True
(d) True
(e) False
(f) False
Problems
1. q(t) = e−t/2(c1 cos 2t +c2 sin 2t)+ E0
17 ; limt→∞q(t) = E0/17.
5. i(t) = e−t(−cos 2t + 13
16 sin 2t) + cos 2t −1
4 sin 2t.
7. i p(t) = −
aC E0
a2LC −aC R + 1 e−at and
i(t) = A0e−Rt
2L cos (µt −φ) −
aC E0
a2LC −aC R + 1 e−at,
where µ =
#
4L/C −R2
2L
.
Section 8.7
True-False Review
(a) True
(b) False
(c) False
Problems
1. y(x) = c1e3x + c2xe3x + x2e3x(2 ln x −3).
3. y(x) = c1 cos 3x + c2 sin 3x −cos 3x tan2 3x + 2 sin 3x tan 3x.
5. y(x) = c1e2x + c2e−2x +
[ln(e2x + 1) −2x −e−2x]e2x −ln(e2x + 1)e−2x.
7. y(x) = [c1 + 2 tanh−1 (cos 3x/2)] cos 3x +
&
c2 + 4
√
3
3
tan−1 (sin 3x/
√
3)
'
sin 3x.
9. y(x) = e3x(c1 cos 2x + c2 sin 2x + sin 2x ln | sec 2x+
tan 2x| −1).
11. y(x) = c1 cos x + c2 sin x −x cos x + ln (sin x) sin x −3 +
5x + 2x2.
13. y(x) = emx(c1 + c2x + x tan−1 (x) −ln
#
1 + x2).
15. y(x) = e−x(c1 + c2x + x sin−1 K x
2
L
+
#
4 −x2).
17. y(x) = c1e−2x + c2xe−2x −2e−2x ln (1 + x2) +
4xe−2x tan−1 x + 1
2 (x −1)2.
19. y(x) = c1ex + c2xex + c3x2ex −2xex ln x.
21. y(x) = c1e−x + c2xe−x + c3x2e−x + (x −tan−1 x)e−x −
xe−x ln (x2 + 1) + x2e−x tan−1 x.
23. yp(x) =
/ x
x0 sinh (x −t)F(t)dt.
25. yp(x) = 1
3
/ x
x0 [ex−t −e2(t−x)]F(t)dt.
27. y(x) = e2x(1 −2x + 5
6 x3).

838
Answers to Odd-Numbered Exercises
29.
(a) yp(x) = αeax
8
x
β tan−1
! x
β
"
−1
2 ln
$
x2 + β2
β2
%9
.
(b) yp(x) = αeax
0
x sin−1
! x
β
"
+ (β2 −x2)1/2 −β2
1
.
(c) α = −1: yp(x) = xeax
2
[(ln x)2 −2 ln x −2].
α = −2:
yp(x) = −eax
8
(ln x)2
2
+ ln x + 1
9
.
α ̸= −1, −2:
yp(x) =
eax xα+2
(α + 1)(α + 2)
0
ln x −
2α + 3
(α + 1)(α + 2)
1
.
33. yp(x) =
/ x
x0[ 1
30 e2(x−t)[sin (3(t −x)) −
3 cos (3(t −x))] + 1
10 e3(x−t)]F(t)dt.
35. yp(x) = 1
30
/ x
x0[3et−x + sin(3(x −t)) −
3 cos(3(x −t))]F(t)dt.
37. yp(x) = 1
30
/ x
x0[e−4(x−t) + 5e2(x−t) −6ex−t]F(t)dt.
Section 8.8
True-False Review
(a) False
(b) True
(c) False
(d) True
(e) True
(f) False
Problems
1. y(x) = c1x + c2x4.
3. y(x) = x−2[c1 cos (3 ln x) + c2 sin (3 ln x)].
5. y(x) = c1x−2 + c2x3.
7. y(x) = c1 cos (4 ln x) + c2 sin (4 ln x).
9. y(x) = c1xm + c2x−m.
11. y(x) = xm[c1 cos (k ln x) + c2 sin (k ln x)].
15. y(x) = c1x−1 + c2x−2 −1
x2 cos x.
17. y(x) = x[c1 cos (2 ln x) + c2 sin (2 ln x) + 2(ln x)2 −1].
19. y(x) = c1x−2 + c2x−3 + e2x(x −1)
x3
.
21. y(x) = c1xm + c2xm ln x + yp(x), where
yp(x) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
xm(ln x)k+2
(k + 1)(k + 2) ,
if k ̸= −1, −2,
xm(ln | ln x| −1) ln x, if k = −1,
−xm(1 + ln | ln x|),
if k = −2.
23.
(a) t = eπ(6n+5)/30.
(d) No.
Section 8.9
Problems
1. y2(x) = x2 ln x.
3. y2(x) = x cos x.
5. y2(x) = cos(x2).
7.
(a) y1(x) = xm.
(b) y2(x) = xm ln x.
11. y(x) = 2x2e2x + c1(2x + 1) + c2e2x.
13. y(x) = e3x(4x5/2 + c1x + c2).
15. y(x) = √x
0 1
24 (ln x)3 + c1 ln x + c2
1
.
Section 8.10
Additional Problems
1. 3ex3(3x4 + 2x + 1).
3. −4
x sin x + 4x cos x −8 sin x.
5. (x2 + 1)
(
2
x3 + 480x2)
−(cos x)
(
1
x + 40x4)
+
5x2(ln x + 8x5).
7. y(x) = c1ex + c2e−2x + c3xe−2x.
9. y(x) = c1 cos 2x + c2 sin 2x + c3 cos 3x + c4 sin 3x.
11. y(x) = c1e−3x + c2xe−3x + e2x(c3 cos 3x + c4 sin 3x).
13. y(x) = c1e−2x + c2xe−2x + c3e3x.
15. A(D) = D2 −6D + 10.
17. A(D) = (D2 + 1)2(D + 2).
19. yp(x) = A0e−2x.
21. yp(x) = A0 cos 4x + A1 sin 4x.
23. yp(x) = x3(A0 cos x + A1 sin x).
25. y(x) = e3x(c1 cos 4x +c2 sin 4x)+c3 +
22
15625 x +
6
625 x2 + 1
75 x3.
27. y(x) = c1e2x + c2e−2x −5
3ex.
29. y(x) = c1ex + c2e−x + 2xex.
31. Annihilators cannot be used.
33. Annihilators cannot be used.
35. yp(x) = A0x2e4x.
37. yp(x) = ex(A0 cos x + A1 sin x) + A2 cos x + A3 sin x.
39. yp(t) = eat(A0 + A1t) + teat(A0 cos 2t + B0 sin bt).
41. y(x) = c1e−3x + c2ex −1
8 xe−3x(2x + 1).
43. y(x) = c1 cos 2x + c2 sin 2x + 2x sin 2x.
45. y(x) = c1ex + c2e−x + e2x −1
2 sin x.
47. y(x) = c1 cos x + c2 sin x −x cos x + sin x ln | sin x|.
49. y(x) = c1emx + c2xemx + 1
4 x2emx(2 ln x −3).
51. y(x) = c1ex + c2xex + 1
4 x2ex(2 ln x −3)
53. y(x) = c1x−4 + c2x−4 ln x.
55. y(x) = c1x6 cos(ln x) + c2x6 sin(ln x).
57. y(x) = c1x−3 + c2x6.
59. y(x) = c1x−4 + c2x−4 ln x + x−3.
61. y(x) = c1x3 cos(ln x) + c2x3 sin(ln x) + x3.
63. y(x) = 1
2 xe−2x[(ln x)2 −2 ln x].
65. yp(x) = 1
4 xe5x(x3 −4x2 + 12x −24).

Answers to Odd-Numbered Exercises 839
Chapter 9
Section 9.1
True-False Review
(a) True
(b) True
(c) False
(d) False
(e) True
(f) False
(g) False
(h) True
(i) False
(j) False
Problems
1. x1(t) = c1e4t + c2et,
x2(t) = 2c1e4t −c2et.
3. x1(t) = c1e2t + c2e3t,
x2(t) = −c1e2t −1
2 c2e3t.
5. x1(t) = c1 cos 2t + c2 sin 2t,
x2(t) = −c1 sin 2t + c2 cos 2t.
7. x1(t) = c1e2t, x2(t) = −c2et sin t + c3et cos t,
x3(t) = c2et cos t + c3et sin t.
9. x1(t) = e2t + 2e−t, x2(t) = e2t −e−t.
11. x1(t) = e3t + 2te3t, x2(t) = e3t(3 + 2t).
13. x1(t) = c1 + c2e−t +
(
3 −1
2 t
)
t,
x2(t) = 2c1 + c2e−t −t2 + 4t + 3.
15. dx1
dt
= tx2 + cos t, dx2
dt
= x3, dx3
dt
= −x1 + tx2 + et + cos t.
17. dy1
dt
= y2, dy2
dt
= −2ty2 −y1 + cos t.
19. dy1
dt
= y2, dy2
dt
= y3, dy3
dt
= et y1 −t2y2 + t.
21. x1(t) = cos t(3 sin t + 4),
x2(t) = sec t
&
sin2 t −cos3 t + 2( 1
2 sin 2t + t) + 1
'
.
Section 9.2
True-False Review
(a) True
(b) False
(c) False
(d) False
(e) False
(f) True
(g) False
Section 9.3
True-False Review
(a) False
(b) True
(c) True
(d) True
Problems
1. x(t) = c1
0 sin 3t
cos 3t
1
+ c2
0 −cos 3t
sin 3t
1
.
3. General solution: x(t) = c1
0 e−t cos 2t
e−t sin 2t
1
+ c2
0 −e−t sin 2t
e−t cos 2t
1
.
Particular solution: x(t) =
0 e−t cos 2t −3e−t sin 2t
e−t sin 2t + 3e−t cos 2t
1
.
5. x(t) =
⎡
⎣
−3
e2t
e4t
9 3e2t
e4t
5
e2t
e4t
⎤
⎦
⎡
⎣
c1
c2
c3
⎤
⎦.
7. x(t) = c1
0 t sin t
cos t
1
+ c2
0 −t cos t
sin t
1
.
Section 9.4
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) True
(f) False
Problems
1. x(t) = c1e3t
0 1
2
1
+ c2e−2t
0 −2
1
1
.
3. x(t) = c1
0 cos 4t
sin 4t
1
+ c2
0
sin 4t
−cos 4t
1
.
5. x(t) = c1e−t
0
cos 2t
−sin 2t
1
+ c2e−t
0 sin 2t
cos 2t
1
.
7. x(t) = c1e−t
⎡
⎣
1
0
1
⎤
⎦+ c2e−t
⎡
⎣
−6
1
0
⎤
⎦+ c3e4t
⎡
⎣
0
1
1
⎤
⎦.
9. x(t) = c1e−4t
⎡
⎣
0
1
0
⎤
⎦+ c2e2t
⎡
⎣
cos 3t
0
−sin 3t
⎤
⎦+ c3e2t
⎡
⎣
sin 3t
0
cos 3t
⎤
⎦.
11. x(t) = c1e2t
⎡
⎣
1
0
2
⎤
⎦+ c2e2t
⎡
⎣
0
1
3
⎤
⎦+ c3e−3t
⎡
⎣
1
1
0
⎤
⎦.
13. x(t) = c1e−t
⎡
⎣
1
−1
1
⎤
⎦+ c2e2t
⎡
⎣
1
1
0
⎤
⎦+ c3e2t
⎡
⎣
−1
0
1
⎤
⎦.

840
Answers to Odd-Numbered Exercises
15. x(t) = c1
⎡
⎢⎢⎣
2
−3
0
1
⎤
⎥⎥⎦+c2
⎡
⎢⎢⎣
1
−2
1
0
⎤
⎥⎥⎦+c3e−2t
⎡
⎢⎢⎣
−1
1
−1
1
⎤
⎥⎥⎦+c4e16t
⎡
⎢⎢⎣
17
13
35
31
⎤
⎥⎥⎦.
17. x(t) = et
0 2
1
1
+ e−5t
0
1
−1
1
.
19. x(t) =
⎡
⎣
−3
9
5
⎤
⎦−2e2t
⎡
⎣
1
3
1
⎤
⎦+ e4t
⎡
⎣
1
1
1
⎤
⎦.
Section 9.5
True-False Review
(a) False
(b) True
(c) False
(d) False
Problems
1. x(t) = c1e2t
0 1
1
1
+ c2e2t
0 1 −t
−t
1
.
3. x(t) = c1e−t
0 −2
2
1
+ c2e−t
0 1 −2t
2t
1
.
5. x(t) = c1e2t
⎡
⎣
3
2
4
⎤
⎦+ c2
⎡
⎣
1
0
2
⎤
⎦+ c3
⎛
⎝
⎡
⎣
0
1
1
⎤
⎦+ t
⎡
⎣
1
0
2
⎤
⎦
⎞
⎠.
7. x(t) = e−t
⎧
⎨
⎩c1
⎡
⎣
2
1
0
⎤
⎦+ c2
⎡
⎣
−3
0
4
⎤
⎦+ c3
⎡
⎣
1 + 16t
8t
0
⎤
⎦
⎫
⎬
⎭.
9. x(t) = et
⎧
⎨
⎩c1
⎡
⎣
0
−1
1
⎤
⎦+ c2
⎡
⎣
0
4t
2 −4t
⎤
⎦+ c3
⎡
⎣
1
2t2
2t −2t2
⎤
⎦
⎫
⎬
⎭.
11. x(t) = e−2t
⎧
⎨
⎩c1
⎡
⎣
−1
1
−1
⎤
⎦+ c2
⎡
⎣
1 −t
−2 + t
1 −t
⎤
⎦+ c3
⎡
⎣
1 + t −t2/2
−2t + t2/2
t −t2/2
⎤
⎦
⎫
⎬
⎭.
13. x(t) = c1e−5t
⎡
⎢⎢⎣
36
−36
−5
6
⎤
⎥⎥⎦+ c2et
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦+ c3et
⎡
⎢⎢⎣
0
0
1 −t
1
⎤
⎥⎥⎦
+ c4et
⎡
⎢⎢⎣
1
1
t −t2/2
t
⎤
⎥⎥⎦.
15. x(t) = e−3t
0
t
t −1
1
.
Section 9.6
True-False Review
(a) False
(b) False
(c) True
(d) False
(e) False
(f) True
Problems
1. x(t) = c1et
0 1
1
1
+ c2e2t
0 3
2
1
+
0 3tet −2e2t + 3te2t + 3et
3tet −2e2t + 2te2t + 2et
1
.
3. x(t) = c1e3t
0 1
0
1
+ c2e3t
0 t
1
1
+ e3t
0
t2
t
1
.
5. x(t) = c1
0 2
1
1
+ c2e3t
0 1
2
1
−e3t
0 9t2 −30t + 10
18t2 −24t + 5
1
.
7. x(t) = c1et
0 −1
1
1
+ c2et
0 1 + 2t
−2t
1
+ tet
0 2t2 −3t −3
6t −2t2
1
.
9. x(t) = c1
⎡
⎣
−2
1
0
⎤
⎦+ c2e3t
⎡
⎣
1
0
2
⎤
⎦+ c3e3t
⎡
⎣
0
1
1
⎤
⎦
+ e3t
⎡
⎣
−(3t + 2)/9
(33t + 1)/9
3t
⎤
⎦.
11. x(t) = c1e4t
0
3
−2
1
+ c2e−4t
0 1
2
1
+
0 cos t −4 sin t
2 cos t + 9 sin t
1
.
Section 9.7
True-False Review
(a) False
(b) True
(c) False
(d) False
(e) False
Problems
3. x(t) = 2(c1 sin t −c2 cos t −c3 sin 3t + c4 cos 3t),
y(t) = 3c1 sin t −3c2 cos t + c3 sin 3t −c4 cos 3t.
9. A1(t) = 120 −50e−t/5 −10e−t/15,
A2(t) = 120 + 100e−t/5 −20e−t/15.
11. A1(t) = α1 + α2
1 + β
−α2 −βα1
1 + β
eλ2t,
A2(t) = β(α1 + α2)
1 + β
+ α2 −βα1
1 + β
eλ2t.
Section 9.8
True-False Review
(a) True
(b) True
(c) True

Answers to Odd-Numbered Exercises 841
(d) True
(e) True
(f) True
Problems
3. eAt =
0 et
et −e−t
0
e−t
1
.
5. x1(t) =
0 −e−t
e−t
1
, x2(t) =
0 (1 −2t)e−t
2te−t
1
.
eAt = e−t
0 1 −2t
−2t
2t
1 + 2t
1
.
7. x1(t) =
⎡
⎣
e2t
0
0
⎤
⎦, x2(t) =
⎡
⎣
0
2e−3t
e−3t
⎤
⎦, x3(t) =
⎡
⎣
0
−8te−3t
(1 −4t)e−3t
⎤
⎦.
eAt =
⎡
⎣
e2t
0
0
0
e−3t(1 + 4t)
−8te−3t
0
2te−3t
(1 −4t)e−3t
⎤
⎦.
9. x(t) = c1
⎡
⎣
0
e4t
2e4t
⎤
⎦+ c2
⎡
⎣
e−2t
e−2t
0
⎤
⎦+ c3
⎡
⎣
−3te−2t
−3te−2t
e−2t
⎤
⎦.
11. x1(t) =
⎡
⎢⎢⎣
−sin t
cos t
−t sin t
t cos t
⎤
⎥⎥⎦, x2(t) =
⎡
⎢⎢⎣
cos t
sin t
t cos t
t sin t
⎤
⎥⎥⎦, x3(t) =
⎡
⎢⎢⎣
0
0
−sin t
cos t
⎤
⎥⎥⎦,
x4(t) =
⎡
⎢⎢⎣
0
0
cos t
sin t
⎤
⎥⎥⎦.
Section 9.9
True-False Review
(a) False
(b) True
(c) True
(d) True
(e) True
(f) False
Problems
1. (0, 0), (−1, 3).
3. (0, 0), (0, 2), (−4
5 , 8
5 ).
5. Saddle.
7. Degenerate unstable node.
9. Stable spiral.
11. Unstable node.
13. Degenerate unstable node.
15. Stable node.
17. Saddle
19. Unstable proper node.
21. Stable spiral.
23. Stable center.
25. Saddle
Section 9.10
True-False Review
(a) True
(b) False
(c) True
(d) True
(e) False
Problems
1. (0, 0), center or spiral.
3. (0, 0), saddle; ( 4
9 , 2
3 ), unstable spiral; ( 4
9, −2
3 ), unstable spiral.
5. (0, 0), unstable node.
7. (0, 0), unstable spiral; ( 1
2 , −1), saddle.
9. (0, 0), unstable degenerate node.
Section 9.11
Additional Problems
3. x(t) = c1e−3t
0 1
3
1
+ c2e−8t
0 −1
2
1
.
5. x(t) = c1e6t
0 1
1
1
+ c2e6t
0 1 + 4t
4t
1
.
7. x(t) = c1e2t
⎡
⎣
0
1
0
⎤
⎦+ c2e−t
⎡
⎣
−1
0
1
⎤
⎦+ c3e−t
⎡
⎣
1 + 4t
0
−4t
⎤
⎦.
9. x(t) = c1
0 3 cos 2t −2 sin 2t
−cos 2t
1
+ c2
0 2 cos 2t + 3 sin 2t
−sin 2t
1
.
11. x(t) = c1e3t
⎡
⎣
1
0
4
⎤
⎦+ c2e−5t
⎡
⎣
4 cos 2t −2 sin 2t
4 cos 2t
0
⎤
⎦
+ c3e−5t
⎡
⎣
2 cos 2t + 4 sin 2t
4 sin 2t
0
⎤
⎦.
13. x(t) = c1e2t
⎡
⎣
4
1
2
⎤
⎦+ c2e−2t
⎡
⎣
0
1
2
⎤
⎦+ c3e−5t
⎡
⎣
1
2
−3
⎤
⎦.
15. x(t) = c1e4t
⎡
⎣
0
1
0
⎤
⎦+ c2e4t
⎡
⎣
−2
0
1
⎤
⎦+ c3e3t
⎡
⎣
−3
−1
1
⎤
⎦.
17. x(t) = c1
⎡
⎣
−1
0
1
⎤
⎦+ c2
⎡
⎣
−t
−1
1 + t
⎤
⎦+ c3e−3t
⎡
⎣
−2
−1
2
⎤
⎦.
19. x(t) = c1e−t
⎡
⎣
−2
−1
2
⎤
⎦+ c2et
⎡
⎣
−cos 2t
−sin 2t
cos 2t + sin 2t
⎤
⎦
+ c3et
⎡
⎣
−sin 2t
cos 2t
−cos 2t + sin 2t
⎤
⎦.

842
Answers to Odd-Numbered Exercises
21. x(t) = e−t
⎧
⎨
⎩c1
⎡
⎣
1
0
−1
⎤
⎦+ c2
⎡
⎣
−2 + t
1
1 −t
⎤
⎦+ c3
⎡
⎣
1 −2t + t2/2
t
t −t2/2
⎤
⎦
⎫
⎬
⎭.
23. x(t) = c1e2t
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦+ c2e2t
⎡
⎢⎢⎣
0
0
4t
1
⎤
⎥⎥⎦+ c3
⎡
⎢⎢⎣
2 cos 3t −3 sin 3t
−cos 3t
0
0
⎤
⎥⎥⎦
+ c4
⎡
⎢⎢⎣
3 cos 3t + 2 sin 3t
−sin 3t
0
0
⎤
⎥⎥⎦.
25. x(t) = c1e−3t
0 1
3
1
+ c2e−8t
0 −1
2
1
+
8 5
24 + 1
14 e−t
1
4 + 5
14 e−t
9
.
27. x(t) = c1e6t
01
1
1
+ c2e6t
01 + 4t
4t
1
+ e6t
0
4t(1 −ln |t|)
ln |t| + 4t(1 −ln |t|)
1
.
29. x(t) = c1e2t
⎡
⎣
4
1
2
⎤
⎦+ c2e−2t
⎡
⎣
0
1
2
⎤
⎦+ c3e−5t
⎡
⎣
1
2
−3
⎤
⎦
+
⎡
⎢⎢⎣
−1
2 t −7
20
−1
4 t + 1
20
−1
2 t + 3
10
⎤
⎥⎥⎦.
Chapter 10
Section 10.1
True-False Review
(a) False
(b) False
(c) True
(d) True
(e) False
(f) True
(g) False
(h) False
(i) False
Problems
1. 1 −s
s2
.
3.
1
(1 −s)2 .
5.
b
s2 −b2 .
7.
3
s −2 .
9.
2
s3 [1 −e−s(1 + s)].
11.
s −2
(s −2)2 + 9.
13. 6(s4 + 4s2 + 36)
(s2 + 9)s4
.
15.
b
s2 −b2 .
17. 2(4s + 1)
s(s + 2) .
19. 2
√
2(s + 1)
s2 + 1
.
21. −128 + 3s2
s(s2 + 64).
23. Piecewise continuous.
25. Piecewise continuous.
27. Piecewise continuous.
29. Piecewise continuous.
31.
1−2e−2s
s
.
33.
1
s2 −e−s
s2 −
e−3s
s(1 −s) .
35. L[eibt] =
s
s2 + b2 +
b
s2 + b2 i.
Section 10.2
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) False
Problems
7. 3e2t.
9.
1
2 sin 2t.
11. 2t2.
13. 2 cos 4t + 1
4 sin 4t.
15. 4t −cos 3t −2
3 sin 3t.
17.
1
5(−3e−t + 3 cos 2t + sin 2t).
19.
1
30 (−25et + 4e−2t + 21e3t).
21.
1
6(4 cos t + 6 sin t −4 cos 2t −3 sin 2t).
Section 10.3
True-False Review
(a) False
(b) True
(c) False
(d) False
(e) False

Answers to Odd-Numbered Exercises 843
(f) False
(g) False
(h) False
Problems
1. F(s) =
2
s3(1 −e−2s) [1 −e−2s(2s2 + 2s + 1)].
3. F(s) =
s(1 + e−πs)
(1 −e−πs)(s2 + 1).
5. F(s) =
e1−s −1
(1 −s)(1 −e−s) .
7. F(s) =
1 −e−s
s(1 + e−s) = 1
s tanh
( s
2
)
.
9. F(s) =
eas/2 −e−as/2
as2(eas/2 + e−as/2) =
1
as2 tanh
(as
2
)
.
11. F(s) =
s
s2 + a2 .
Section 10.4
True-False Review
(a) False
(b) True
(c) True
(d) False
Problems
1. y(t) = 2e5t + e2t.
3. y(t) = e−t + 2e−3t.
5. y(t) = 5et −3 cos t + 3 sin t.
7. y(t) = −et cos t + 2e−t sin t + 2e−t.
9. y(t) = 5 cos 2t + 1
2 sin 2t.
11. y(t) = 3(e4t −1).
13. y(t) = 2e3t −4e2t + 2et.
15. y(t) = 4e2t −5et + 2e−t.
17. y(t) = 22
5 e−2t + 8
5e3t −2 + et.
19. y(t) = 2e3t + e−3t −sin 2t.
21. y(t) = −sin t −3 cos t + e2t + 2e−t.
23. y(t) = 10
3 e−t −1
3e−4t −2 cos 2t.
25. y(t) = cos 2t −2 sin 2t + 3 sin t.
27. y(t) = 3 cos 3t + 2 sin 3t −2 cos 4t −sin 4t.
29. y(t) = 1
ω
$
Aω0
ω2
0 −ω2 + y1
%
sin ωt +
$
B
ω2
0 −ω2 + y0
%
cos ωt −
$
A
ω2
0 −ω2
%
sin ω0t −
$
B
ω2
0 −ω2
%
cos ω0t.
33. x1(t) = 2
3 (2e−2t + et), x2(t) = 1
3(e−2t + 2et).
Section 10.5
True-False Review
(a) True
(b) False
(c) True
(d) False
(e) False
(f) True
(g) True
(h) False
Problems
1. 2(t −1).
3. 1.
5. e3(t−2).
7. (t + 1)e2(t+1).
9.
t −1
(t −1)2 + 4 .
11. e−(t−π/4)[sin(2(t −π/4)) + cos(2(t −π/4))] =
e−(t−π/4)(sin 2t −cos 2t).
13.
f (t) = (t −1)2.
15.
f (t) = (t + 1) sin 3t.
17.
f (t) =
t + 5
(t + 3)2 + 4 .
19.
5
(s + 4)2 + 25.
21.
3
(s + 1)2 .
23.
s2 + 3s + 5
(s −1)(s + 2)2 .
25.
s2 −4s + 6
(s −2)[(s −2)2 + 4] .
27. −
√
2(s + 1)
2[(s + 2)2 + 1].
29. 2t2e−2t.
31. et sin 2t.
33.
1
2 e3t(2 cos 2t + 3 sin 2t).
35. 6e−t sin t.
37. e2t(2 cos 3t + 4
3 sin 2t).
39. e−5t(2 cos 7t −sin 7t).
41.
1
3et + tet −1
3e−2t.
43. y(t) = 4tet −2et + 2e−t.
45. y(t) = e2t −2te−t −e−t.
47. y(t) = e2t(1 −2t + 3t2).
49. y(t) = 1
2 e2t −1
18e−2t −2
9 et(2 + 3t).
51. y(t) = 17
10 cos t + 2
5 sin t + 1
10 e−3t(3 + 5t).

844
Answers to Odd-Numbered Exercises
53. y(t) = 3
2 e−3t −5
2 et + e2t(2 cos t + 3 sin t).
55. x1(t) = −e7t/2 cos(
√
7t/2) +
5
√
7 e7t/2 sin(
√
7t/2),
x2(t) = e7t/2 cos(
√
7t/2) +
3
√
7 e7t/2 sin(
√
7t/2).
Section 10.6
True-False Review
(a) False
(b) False
(c) False
(d) False
Problems
9.
f (t) = t2 + (1 −t2)u1(t).
11.
f (t) = 2[1 + (et−1 −1)u1(t)].
13.
f (t) = (3 −t)u2(t) + (t −4)u4(t).
15.
f (t) = sin t
∞
M
i=0
[u2iπ(t) −u(2i+1)π(t)].
Section 10.7
True-False Review
(a) False
(b) True
(c) True
(d) False
(e) False
(f) False
(g) False
Problems
1. F(s) = e−2s −e−3s
s
.
3. F(s) = e−2s
s −3.
5. F(s) = −se−πs
s2 + 1 .
7. F(s) = e−3s
s2
+ 3e−3s
s
.
9. F(s) =
6e−4s
(s −1)4 .
11. F(s) = e−cs
s −a
(s −a)2 + b2 .
13.
f (t) = u1(t)e−(t−1).
15.
f (t) = u1(t) cos[2(t −1)].
17.
f (t) = u2(t)e−2(t−2).
19.
f (t) = u2(t)e−(t−2) sin(t −2).
21.
f (t) = 1
4 sin[4(t −5)]u5(t).
23.
f (t) = u4(t)e3(t−4)(cos[2(t −4)] + 3 sin[2(t −4)]).
25.
f (t) = u2(t)[et−2 −cos(t −2) −sin(t −2)].
27. y(t) = u1(t)(1 −e−2(t−1)) + e−2t.
29. y(t) = et + 2[et−π/4 −cos(t −π/4) + sin(t −π/4)]uπ/4(t).
31. y(t) = 1
3(1 + 2e−3t) −1
3u1(t)[1 −e−3(t−1)].
33. y(t) = 5e3t + ua(t){e3(t−a) −e−(t−a)[cos 2(t −a) +
2 sin 2(t −a)]}.
35. y(t) = −1
2 + 5
3e−t −1
6e2t + u2(t)
2
[3 −e2(t−2) −2e−(t−2)].
37. y(t) = t + 2 cos t −u1(t)[t −1 −sin(t −1)].
39. y(t) = 2e−3t + e2t + u1(t)[2e2(t−1) + 3e−3(t−1) −5e−(t−1)].
41. y(t) = 1
5[cos t + 2 sin t −et(cos 2t + 1
2 sin 2t)]
+ 1
20 uπ/2(t)(2 cos 2t −3 sin 2t)et−π/2
+ 1
10 (2 −sin t + 2 cos t)uπ/2(t).
43. y(t) = 1
4 [3 −3e−2t −2t + u1(t)(e−2(t−1) + 2t −3)].
45. y(t) = 1
4 [e2t −1 −2t −u1(t)(e2(t−1) −2t + 1) −
u2(t)(e2(t−1) −2t + 3) + u3(t)(e2(t−3) −2t + 5)].
47. i(t) = g(t) −u5(t)g(t −5), where g(t) = 2
R t + 2L
R2 (e−Rt/L −1).
Section 10.8
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
Problems
1. y(t) = 3e−t + u5(t)e−(t−5).
3. y(t) = 2e−4t + 3u1(t)e−4(t−1).
5. y(t) = 2et −e2t + u1(t)[e2(t−1) −et−1].
7. y(t) = e−t sin 2t −1
2 uπ/2(t)e−(t−π/2) sin 2t.
9. y(t) = e−t + 1
2 u2(t)[e−(t−2) −e−3(t−2)].
11. y(t) = 3 sin 2t −2 sin 3t −1
3uπ/6(t) cos 3t.
13. y(t) = 3
10 e−t sin 2t + 4
5 sin t −2
5 cos t + 2
5 e−t cos 2t +
1
2 uπ/6(t)e−(t−π/6) sin[2(t −π/6)].
15. y(t) = −25
68 cos 5t −15
68 sin 5t + 25
68 e−2t cos 3t + 125
204 e−2t sin 3t +
2
3 u10(t)e−2(t−10) sin[3(t −10)].
Section 10.9
True-False Review
(a) True
(b) True
(c) False
(d) True
(e) False
(f) True
(g) True

Answers to Odd-Numbered Exercises 845
Problems
1.
1
2 t2.
3. 1 −cos t.
5. 2et −t2 −2t −2.
11.
1
s(s −2) .
13.
1
(s −1)(s −2)2 .
15. 1
2 (e2t −1).
17. sin 2t.
19.
1
81 [2(cos 3t −1) + 9t2].
21. 2
/ t
0(t −τ)2eτ cos 2τdτ.
23. 4
/ t
0 e−(3t−7τ) sin(t −τ)dτ.
25.
/ t
0 e4(t−τ)g(τ)dτ.
27. y(t) = 1
3et sin 3t + 1
3
/ t
0 cos[2(t −τ)]eτ sin 3τdτ.
29. y(t) = αeat +
/ t
0 f (t −τ)eaτ dτ.
31. y(t) = β −bα
a −b eat + aα −β
a −b ebt +
1
a −b
N t
0
f (t −τ)(eaτ −ebτ )dτ.
33. x(t) = 1
3e2t + e−2t −1
3e−t.
35. x(t) = 12e2t −8et.
37. x(t) = 1
3(5et + 10e4t −12e2t).
Section 10.10
Additional Problems
1. F(s) = 3
s2 −4
s .
3. F(s) = 8
s3 .
5. F(s) =
7
(s + 1)2 .
7. F(s) = 1
2s −
s
2(s2 + 4a2) .
9. F(s) = (4s2 + 5s + 2)e−3s
s3
+ s + 1
s2
.
11. F(s) =
5s
s2 + 4 −
7
s + 1 −360
s7 .
13. F(s) =
s −3
(s −3)2 + 25 −
2
(s + 1)2 + 4 .
15. F(s) =
I
π
s + 5 .
17. F(s) = 2
s (1 −e−s) + 2e−(s+1)
s + 1
.
19. F(s) =
2
s3(s −1) .
21.
f (t) = 4 cos 3t + 5
3 sin 3t.
23.
f (t) = 1
8[1 −cos 4t].
25.
f (t) = 1
8[2 + 3e−2t sin 4t −2e−2t cos 4t].
27.
f (t) = 1 + uln 2(t)[2e−t −1]; L[ f ] = 1
s −
1
s(s + 1)2s .
35. y(t) = 13
24 e4t + 13
12 e−2t −5
8.
37. y(t) = 1 + sin t −cos t −uπ/2(t)[1 −cos(t −π/2)].
39. y(t) = u4(t)(t −4)e−(t−4).
41. x1(t) = 1
4 e3t + 3
4 e−t, x2(t) = 1
4 e3t −3
4 e−t.
43. x1(t) = e2t −4te2t, x2(t) = e2t + 4te2t.
45. x(t) = 2t + t3/3.
47. x(t) = −2 + 4t2 + 2 cos
√
2t.
Chapter 11
Section 11.1
True-False Review
(a) True
(b) True
(c) False
(d) False
(e) True
(f) False
(g) True
(h) True
(i) False
(j) True
Problems
1. R = 4.
3. R = 1.
5. R = 0.
7. R = 2.
9. R =
√
17.
11. R =
√
10.
15.
f (x) = a0
x (ex −1).
Section 11.2
True-False Review
(a) True
(b) False
(c) False
(d) True
(e) True
(f) False
(g) False
(h) False
(i) True
(j) False

846
Answers to Odd-Numbered Exercises
Problems
1. y1(x) =
∞
M
n=0
1
(2n)! x2n, y2(x) =
∞
M
n=0
1
(2n + 1)! x2n+1, R = ∞.
3. y1(x) =
∞
M
n=0
1
n! x2n, y2(x) =
∞
M
n=0
2n
1 · 3 · 5 · · · (2n + 1) x2n+1,
R = ∞.
5. y1(x) = 1 +
∞
M
n=1
4 · 7 · · · (3n −2)(−1)n
(3n)!
x3n,
y2(x) = x +
∞
M
n=1
2 · 5 · · · (3n −1)(−1)n
(3n + 1)!
x3n+1, R = ∞.
7. y1(x) = 1 +
∞
M
n=1
1
2 · 5 · · · (3n −1) x3n,
y2(x) = x +
∞
M
n=1
1
3 · 6 · · · (3n) x3n+1, R = ∞.
9. y1(x) =
∞
M
n=0
(−5)(−3)(−1) · · · (2n −7)
3n(2 · 4 · 6 · · · 2n)
x2n,
y2(x) = x −4
9 x3 +
8
135 x5, R =
√
3 (lower bound).
11. y1(x) = 1 +
∞
M
n=1
4n(2 · 4 · 6 · · · 2n)
1 · 3 · 5 · · · (2n −1) x2n,
y2(x) = x +
∞
M
n=1
4n(1 · 3 · 5 · · · (2n −1)
2 · 4 · 6 · · · 2n
x2n+1,
R = 1
2 (lower bound).
13. y1(x) = 1 −2
3 x3 + 1
3 x4 −2
15 x5 + · · · ,
y2(x) = x −x2 + 2
3 x3 −2
3 x4 + 7
15 x5 + · · · , R = ∞.
15. y1(x) = 1 + 1
2 x2 + 1
6 x3 + 1
12 x4 + 1
24 x5 + · · · ,
y2(x) = x + 1
6 x3 + 1
12 x4 + 1
30 x5 + · · · , R = ∞.
17.
(a) No
(b) y1(x) = 1 + 1
2 (x −1)2 + 1
8(x −1)4 + · · · ,
y2(x) = (x −1) + 1
3(x −1)3 −1
12 (x −1)4 + · · · , R ≥1.
19.
(a) y(x) =
∞
M
n=0
(−1)n (n + 1)!
2n(2n)! x2n
(b) Polynomial approximation: y8(x) = 1 −1
2 x2 + 1
16 x4 −
1
240 x6 +
1
5376 x8; error < 6.3 × 10−6 on [−1, 1].
21. y(x) = a0(1 + 2x2 + 1
3 x4 + · · · ) + a1(x + 1
2 x3 + 1
40 x5 + · · · ) +
(3x2 + x3 + 3
4 x4 + 1
10 x5 +
1
120 x6 + · · · ).
Section 11.3
True-False Review
(a) False
(b) True
(c) True
(d) True
Problems
1. For α = 3, y2(x) = a1(x −5
3 x3), P3(x) = −3
2 x(1 −5
3 x2),
and for α = 4, y1(x) = a0(1 −10x2 + 35
3 x4), P4(x) =
3
8(1 −10x2 + 35
3 x4).
5. p(x) = 16
3 P0 + 6
5 P1 + 2
3 P2 + 4
5 P3.
9. α = 0: y1(x) = 1; α = 1: y2(x) = x; α = 2: y1(x) = 1 −2x2;
α = 3: y2(x) = x(1 −2
3 x2).
Section 11.4
True-False Review
(a) False
(b) False
(c) True
(d) True
(e) True
Problems
1. x = 1 is a regular singular point. All other points are ordinary points.
3. x = 0 is a regular singular point, and x = ±1 are irregular singular
points.
5. x = ±3 are regular singular points, and x = 0 is an irregular sin-
gular point.
7. r = −1
4 and r = 1.
9. r = 1 ± 2i
11. y1(x) = x1/2e−3x, y2(x) = x1/3
8
1 +
∞
M
n=1
(−18)n
5 · 11 · · · (6n −1) xn
9
.
13. y1(x) = 1 +
∞
M
n=1
x2n
n![3 · 7 · · · (4n −1)] ,
y2(x) = x1/2
8
1 +
∞
M
n=1
x2n
n![5 · 9 · · · (4n + 1)]
9
.
15. y1(x) = x2(1 −4
7 x + 4
63 x2),
y2(x) = x−1/2
8
1 −
∞
M
n=1
315
n!(2n −5)(2n −7)(2n −9) xn
9
.
17. y1(x) = x−1
8
1 +
∞
M
n=1
(−3)n
1 · 4 · · · (3n −2) xn
9
,
y2(x) = x−1/3e−x.
19. y1(x) = 1 + 2x + 1
2 x2 −5
21 x3 −73
840 x4 + · · · ,
y2(x) = x2/3 &
1 + 2
5 x −3
40 x2 −43
660 x3 +
31
3696 x4 + · · ·
'
.
21. y1(x) = x−1, y2(x) = x−1
8
ln x +
∞
M
n=1
(2x)n
n · n!
9
.
Section 11.5
True-False Review
(a) False
(b) True
(c) False
(d) True
(e) True
(f) False

Answers to Odd-Numbered Exercises 847
Problems
1. y1(x) = x2
∞
M
n=0
anxn, y2(x) = y1(x) ln x + x2
∞
M
n=1
bnxn.
3. y1(x) = x
√
2
∞
M
n=0
anxn, y2(x) = x−
√
2
∞
M
n=0
bnxn.
5. y1(x) = x3/2
∞
M
n=0
anxn, y2(x) = x−1/2
∞
M
n=0
bnxn.
7. y1(x) = x2
∞
M
n=0
anxn, y2(x) = Ay1(x) ln x + x−1
∞
M
n=0
bnxn,
A ̸= 0.
9. All α ̸= 0.
11. y1(x) = x2
8
1 + 6
∞
M
n=1
(−1)n(n + 1)
(n + 2)(n + 3) xn
9
,
y2(x) = x−1 &
1 + 1
2 x
'
.
13.
(a) y1(x) = x
∞
M
n=0
xn
n!(n + 1)!
(c) y2(x) = y1(x) ln x +
&
1 −3
4 x2 −7
36 x3 + · · ·
'
.
15. y1(x) = x
∞
M
n=0
1
(n + 2)! xn = x−1(ex −x −1),
y2(x) = x−1(1 + x).
17. y1(x) =
∞
M
n=0
(2x)n
(n!)2 ,
y2(x) = y1(x) ln x −(4x + 3x2 + 22
27 x3 + · · · ).
19. y1(x) = x2
∞
M
n=0
(n + 1)
n!
xn,
y2(x) = y1(x) ln x −x2(3x + 13
4 x2 + 31
18 x3 + · · · ).
21. y1(x) = x2
8
1 +
∞
M
n=1
(n + 4)
4(n!) xn
9
,
y2(x) = 2y1(x) ln x + x−1(1 −x + 3
2 x2 −21
8 x4 + · · · ).
23. y1(x) = x3/2
∞
M
n=0
1
(n + 3)! xn = x−3/2(ex −1 −x −1
2 x2),
y2(x) = x−3/2(1 + x + 1
2 x2).
25. y1(x) = x, y2(x) = y1(x) ln x +
∞
M
n=1
(−1)n
n · n! xn+1.
27. y1(x) = x3/2
∞
M
n=0
1
n!(n + 2)! xn,
y2(x) = y1(x) −x−1/2 (
1 −x + 2
9 x3 + 25
576 x4 + · · ·
)
.
29. y1(x) = x−1, y2(x) = x−1 ln x + x−2
8
1 +
∞
M
n=2
(−1)n+1
n!(n −1) xn
9
.
31.
(a) y(x) = N!x−N
N
M
n=0
1
(N −n)!(n!)2 xn.
Section 11.6
True-False Review
(a) False
(b) True
(c) False
(d) True
(e) False
(f) False
(g) False
Problems
5.
/ ∞
0
t p−1e−atdt =
1
a p ,(p).
7.
(a) ,(3/2) = √π/2 and ,(−1/2) = −2√π.
17. x p =
∞
M
n=1
2
λn Jp+1(λn) Jp(λnx).
Section 11.7
Additional Problems
1. Ordinary point.
y1(x) = 1 +
∞
M
n=1
(−1)n(3n −2)(3n −5) · · · 7 · 4 · 1
(3n)!
x3n,
y2(x) = x +
∞
M
n=1
(−1)n(3n −1)(3n −4) · · · 5 · 2
(3n + 1)!
x3n+1.
Solutions valid on (−∞, ∞).
3. Ordinary point. y1(x) = 1 + 2x2 + 3x4 + 4x5 + · · · , y2(x) =
x + 5
3 x3 + 7
3 x5 + 9
3 x7 + · · · . Solutions valid on (−1, 1).
5. Regular singular point. y1(x) = 1 −1
3! x2 + 1
5! x4 −1
7! x6 + · · · ,
y2(x) = x−1 &
1 −1
2 x2 + 1
4! x4 + · · ·
'
. Solutions valid on (0, ∞).
7. Regular singular point. y1(x) =
∞
M
n=0
(−1)n
22n · (n!)2 x2n,
y2(x) = y1(x) ln x +
&
1
4 x2 −
3
128 x4 +
11
13824 x6 + · · ·
'
.
9. Regular singular point. y1(x) = J1/2(x) =
*
2
πx sin x,
y2(x) = J−1/2(x) =
*
2
πx cos x.
11. Regular singular point. y1(x) = x1/2
∞
M
n=0
1
n![3 · 5 · · · (2n + 3)] xn,
y2(x) = x−1
8
1 −x −
∞
M
n=2
1
n![1 · 3 · · · (2n −3)] xn
9
.
13. Regular singular point. y1(x)
=
x2
∞
M
n=0
(−4)n
(n!)2 xn, y2(x)
=
y1(x) ln x + x3 &
8 −12x + 176
27 x2 −50
27 x3 + 1096
3375 x4 + · · ·
'
.

848
Answers to Odd-Numbered Exercises
15.
(a) y1(x) = 1 + ab
2! x2 + ab(2 −a)(2 −b)
4!
x4 + · · · ,
y2(x) = x + (1 −a)(1 −b)
3!
x3
+ (1 −a)(1 −b)(3 −a)(3 −b)
5!
x5 + · · ·
(d) y1(x) = 1 + 10x2 + 5x4, y2(x) = x + 2x3 + 1
5 x5.
17.
(c) N = 0: y(x) = 1; N = 1: y(x) = x−1(1 −x);
N = 2: y(x) = x−2[1 −2x + 1
2 x2];
N = 3: y(x) = x−3[1 −3x + 3
2 x2 −1
6 x3].
Appendix A
1. z = 2 −5i, |z| =
√
29.
3. z = 5 + 2i, |z| =
√
29.
5. z = 1 −2i, |z| =
√
5.
7. z1z2 = 1 + 7i, z1
z2
= −1 + i.
9. z1z2 = 7 + 11i, z1
z2
= 1
10 (1 −13i).
Appendix B
1.
5
x + 2 −
3
x + 1.
3.
4
5(x −3) +
1
5(x + 2).
5.
1
3(x + 1) +
1
6(x −2) −
1
2(x + 4) .
7.
3
x + 1 −
1
(x + 1)2 +
3
x + 2 .
9.
3
4x + 1
x2 −
3x + 4
4(x2 + 4).
11.
1
2(x −2) +
x + 2
2(x2 + 16).
13.
1
x −2 −
1
x + 2 +
3
(x + 2)2 .
15.
2
3(x −2) −
2
3(x + 1) −
2
(x + 1)2
+
1
(x + 1)3 .
17.
1
2(x −3) −
x + 1
2(x2 + 4x + 5).
Appendix C
Note that we have omitted the integration constants.
1. cos x + x sin x.
3. x ln x −x.
5. 1
2 ex2(x2 −1).
7. x −3 ln |x + 2|.
9. 1
4 ln |x| −1
8 ln(x2 + 4) + tan−1 x
2 .
11. 1
2 x + 7
4 ln |2x −1|.
13. 2 ln |x| −
1
x + 1 −2 ln |x + 1|.
15. tan−1(x + 1).
17. −ln | cos x|.
19. 1
4 (2x + sin 2x).
21.
1
13e3x(3 sin 2x −2 cos 2x).

Index
A
Absolute value
around a matrix, 200–201
of a complex number, 792
Addition
associativity of, 248, 253
closure under, 253, 264
commutativity of, 248, 253
in Rn, 249–250
of geometric vectors, 247
of matrices, 122–123
Additive inverse
existence of, 253
in Rn, 248–251
properties of, 259
Adjoint method, 227–229, 239–240, 242–243
for computing matrix inverse, 228
Algebra of power series, 725–726
Alternating current (AC), 66
phase-amplitude form of a solution, 67
steady-state, 67
transient, 67
Amplitude, 532
of a steady-state current, 544–545
Analytic functions, 727–729
Angle between vectors
in R3, 340
in a real inner product space, 347
Annihilators, 515–524, 574
Anti-symmetric matrix, 119
Applied Linear Algebra (Noble/Daniel), 186
Archimedes’ principle, 540 fn
Area of a parallelogram, 203–204
Associated homogeneous differential equation, 501,
505, 516, 547, 552–553, 568
Augmented matrix, 140–141, 191
Autonomous system, 643–653
Auxiliary equation, 505, 509
Auxiliary polynomial, 505
B
Back substitution, 146
Basis, 298–308, 334
change of, 311–317
deﬁned, 299
for lattices, 337
of eigenvectors, 450
ordered, 312
orthogonal, 354–357, 363
orthonormal, 354–357, 365
standard basis, 299–301
Bernoulli equations, 76–78
Bessel’s equation of order p, 494, 773–782, 787
Fourier-Bessel expansion, 782
function expansion theorem, 780–782
function of the ﬁrst kind, 775–777
properties of, 779–780
function of the second kind, 778–779
Bound parameters, 160
Bound variables, 160
Boundary-value problem, 22
C
Capacitance, 64
Capacitor, 64
Carrying capacity, 3, 47
CAS (computer algebra systems), 30
Catenary, 106
Cauchy-Euler equation, 557–566, 574, 751–752
Cauchy-Schwarz inequality, 346–347
Center, 651, 659
Change of basis, 311–317
Change of basis matrix, 313–317
Change of variables for differential equations, 71–78
for Bernoulli equations, 76–78
for homogeneous equations of degree zero, 71–76
Characteristic equation, 436
Characteristic polynomial, 436
Characteristic values, 434
Characteristic vector, 434
Chebyshev equation, 741, 788
Chebyshev polynomials, 788
Circuit. See Electric circuit
Circular frequency, 532
Closure under scalar multiplication, 253, 264
Closure under vector addition, 253, 264
Codomain of a linear transformation, 381
Coefﬁcient matrix, 140
Coefﬁcients of a linear system, 138
Cofactor
matrix of cofactors, 228, 239–240
of a matrix, 222–223, 236, 242
Cofactor expansion, 222–231, 238, 242
adjointmethodfor A−1,227–229,239–240,242–243
Cramer’s rule, 229–231, 239–240, 242–243, 549
Cofactor Expansion Theorem, 223–226
Column n-vector functions, 120
Column n-vectors, 116
Column space of a matrix, 321–324, 334–335
as the range of a linear transformation, 400
Column vectors, 116–117
Comparison Test for Improper Integrals, 677
Complementary function, 501, 574
Complete set of eigenvectors, 449, 488
Completing the square, 693fn
Complex n-space, 139
Complex eigenvalue, 440–441, 488, 606
Complex eigenvector, 440–441, 488, 606
Complex exponential function, 507, 793–795
Complex inner product spaces, 347–349
Complex numbers, 777, 791–796
absolute value of, 792
conjugate, 792
Euler’s formula, 794–795
imaginary part, 791
modulus of, 792
real part, 791
Complex scalar multiplication, 252–253
Complex-valued functions, 793–795
Complex-valued trial solutions, 526–528
Complex vector space, 254, 258–259, 333
Components of a vector, 116
relative to an ordered basis, 312
relative to an orthogonal basis, 355
relative to an orthonormal basis, 355–356
Component vector relative to an ordered basis, 312
Composition of linear transformations, 407–408,
423–424
Computational Methods in Elementary Numerical
Analysis (Morris), 186
Computer algebra systems (CAS), 30
Conjugate of a complex number, 792
Consistent system of equations, 140
Constant coefﬁcient homogeneous linear differential
equations, 505–512
complex-valued trial solutions, 526–528
Constants of a linear system, 138
Convergence of power series, 723–724
Convolution integral, 711–715
Convolution product, 711
Convolution Theorem, 712
Coupled spring–mass system, 581, 625–629, 663,
703, 709
Cover-up rule, 800
Cramer’s rule, 229–231, 239–240, 242–243, 549
Critical damping, 534, 543
Cross product, 202
Cycle of generalized eigenvectors, 479–480, 489, 614
Cylindrical tank problem, 112–113
849

850
Index
D
Damping, 530, 533–535, 537–538
Damping constant, 530
Defective matrix, 440, 449
as coefﬁcient matrix for a linear system, 608–618
Degenerate node, 650
Dependent variables, 1
Derivative
of a matrix function, 133
operator, 495
Determinants, 196–245
computed from cofactor expansion, 222–231
condition for invertibility, 211–212, 239–240
effect from elementary row operations,
209–211, 238
of orders two and three (geometric interpretation),
202–205, 238
order of, 200
permutations, 197–202
properties of, 209–218, 237–238
relationship to linear systems, 211–213,
229–230, 239
Vandermonde, 233
Wronskian, 292–295, 499–500, 589, 596
Diagonalizable matrix, 456–459
square roots of, 461, 491–492
Diagonal matrix, 117–119, 190
Diagonalization, 454–459
orthogonal, 466–473
Differential equations, 1, 106
Bernoulli, 76–78
Chebyshev, 741
constant coefﬁcient homogeneous linear, 505–512
applications (second-order), 529–538,
542–545
exact, 82–91
ﬁrst-order, 1–100
applications, 61–68
homogeneous of degree zero, 71–76
linear, 53–58
general solution to, 2, 17, 107, 499, 501–502,
509–510, 574, 593–595, 601, 606, 637,
664, 665
for Bessel’s equation of order p, 778–779, 787
for Cauchy-Euler equations, 557–560
for exact differential equations, 83–84
for ﬁrst-order linear differential equations, 55
for nth order linear differential equations, 499,
501–502, 509–510, 515–524
via power series, 734
via reduction of order, 568–570
Hermite, 741
higher-order, 101–105
homogeneous linear differential equations of
order n, 496–500, 573
homogeneous of degree zero, 71–76, 108
hypergeometric, 788
Legendre, 741–748, 785–786
linear, 13–14, 493–579
applications, 61–68, 529–538, 542–545
ﬁrst-order, 53–58
homogeneous, 496–500
of order n, 493–579
linear systems of. See Systems of differential
equations
nonlinear, 13–14
normal form, 19
order of, 1, 106
ordinary, 1
ordinary point of, 731, 785
partial, 1
particular solution to, 2, 18, 495, 501–502,
515–528, 547–555, 574
reduction of order, 568–572
regular, 497
Riccati, 81
separable, 34–42
singular point of, 731, 786
solutions of, 14–18
systems of. See Systems of differential equations
Differential Equations (Kaplan), 509
Differential Equations, Dynamical Systems, and
Linear Algebra (Hirsch and Smale), 463
Differential operator, 496, 505
Differentiation of power series, 726
Dimension of a space of solutions to a linear
differential equation, 495, 498
Dimension of a vector space, 302
Dimensions of a matrix, 115
matrix multiplication requirements, 128
Dirac delta function, 706–709
Direct current (DC), 66
Direction ﬁeld, 652
Domain of a linear transformation, 381
Dot product, 124, 340, 358
Doubling time, 46
Dufﬁng’s equation, 14
E
Eigenbasis, 450
Eigenspace, 446
Eigenvalue/eigenvector, 433–492
diagonalization, 454–459
eigenvalue/eigenvector problem, 434–442,
488–489
Jordan canonical forms, 475–485
matrix exponential function, 462–465
orthogonal diagonalization, 466–473
Eigenvalues, 434
complex, 440–441, 606
Jordan block corresponding to, 477–478
multiplicity of an, 446–447
Eigenvectors, 434
basis of, 450
complete set of, 449, 488
complex, 440–441, 606
cycle of generalized, 479–480, 489
generalized, 475, 610–611, 639, 664
Electric circuit, 63–68, 542–545
capacitor, 64
current
alternating (AC), 66
direct (DC), 66
electromotive force, 64, 67
inductor, 64
RC circuit, 65
resistor, 64
RL circuit, 65
RLC circuit, 64–66, 542–545
steady-state solution, 67, 544–545
transient solution, 67, 544
Electromotive force (EMF), 64, 542
Elementary column operations, 213
Elementary matrices, 179–186, 192
Elementary row operation (ERO), 147–152, 191
effect on the determinant, 209–211, 213, 242
Elements of a matrix, 115
Equality of matrices, 115–116
Equidimensional equations, 557
Equilibrium points, 644, 665
classiﬁcation of, 645–651
Equilibrium solutions
for ﬁrst-order differential equations, 27, 107
for linear autonomous systems, 643–644
Euler’s constant, 779
Euler’s formula, 507, 794–795
Euler’s method, 93–96, 107
Modiﬁed (Heun’s method), 96–98
Even parity, 198
Even permutation, 198
Exact differential equations, 82–91
integrating factor for, 88–91
potential function, 83–86
level curves of, 84
test for exactness, 84
Existence and Uniqueness Theorem
for ﬁrst-order differential equations, 24–26
for higher-order differential equations, 497
for vector differential equations, 593
Existence Theorems (Murray and Miller), 593
Exponential function, complex, 507, 793–795
Exponential order, 677
Extending a basis, 307
F
Farad (F), 64
Finite-dimensional vector spaces, 299
examples of, 298–308
First-order differential equations, 1–100
applications
electric circuits, 63–68
mixing problems, 61–63
Newton’s law of cooling, 3–4, 14, 39–40, 60
Newton’s second law of motion, 6–9
orthogonal trajectories, 4–6
Bernoulli equation, 76–78
exact equation, 82–91
geometry of, 24
homogeneous of degree zero, 71–76, 108
linear. See First-order linear differential equations
numerical solution to, 93–100
Euler’s method, 93–96, 107
fourth-order Runge-Kutta method, 98–100
modiﬁed Euler’s method (Heun’s method),
96–98
separable, 34–42
solution techniques summary, 107–108
First-order linear differential equations, 14, 53–58
applications, 61–68
integrating factor for, 55
standard form, 53–54
system of. See Systems of differential equations
First-order linear systems. See Systems of differential
equations
First shifting theorem, 690–693

Index 851
Forced oscillations of a mechanical system, 535–538
Fourier-Bessel expansion, 782
Fourier coefﬁcients, 361
Fourier series, 361
Fourth-order Runge-Kutta method, 98–100
Free oscillations of a mechanical system, 531–535
Free parameter, 160
Free variable, 160
Frequency of oscillation, 532
Frobenius series, 752
Frobenius theory, 759–771, 778
Fundamental matrix, 594–595, 637–638, 664–665
Fundamental solution set, 594–595, 597, 664–665
G
Gamma function, 776–777
Gaussian elimination, 156–165, 191
with partial pivoting, 167
Gauss-Jordan elimination, 157–158
technique for ﬁnding A−1, 173, 191–192
General solution
to a differential equation. See Differential
equations, general solution to
to a linear system of equations. See Linear
systems of equations, general solution to
to a vector differential equation. See Vector
differential equations, general solution to
Generalized eigenvector, 475, 610–611, 639, 664
cycle of, 479–480, 489, 614
Generated by a set of vectors, 274, 306
Geometry
level curves of a potential function, 84
of ﬁrst-order differential equations, 23–31
of vectors, 246–247
transformations of R2, 391–396
Gompertz population model, 52
Gram-Schmidt orthogonalization procedure, 362–363
Gram-Schmidt process, 362–365, 376
Green’s functions, 551–552
H
Half-life, 52
Heaviside step function, 695–698
Henry (H), 64
Hermite equation, 741, 749
Hermite polynomials, 749
Heun’s method, 96–98
Hexagonal lattices, 338
Higher-order differential equations, 101–105,
493–579
Hilbert matrix, 179
Homogeneous
associated to a nonhomogeneous differential
equation, 501, 505, 516, 568, 574
constant coefﬁcient differential equations,
505–512
ﬁrst-order differential equation, 71–76
ﬁrst-order linear system, 582, 663–664
linear differential equations, 496–500
linear system of equations, 138, 162–164, 191,
211–213, 239, 246–247, 269–270, 327, 398
of degree zero, 71–76
vector differential equation, 593–597, 665
Hooke’s law, 8, 14, 371, 581, 625
Hypergeometric equation, 788
I
Identity matrix, 130
Identity transformation, 393
Impulsive driving terms, 706–709
Impulsive force, 706
Inconsistent system of equations, 140
Independent variables, 1
Index form of the matrix product, 127
Indicial equation, 558, 563, 752, 811
Inductance, 64
Inductor, 64
Inﬁnite-dimensional vector spaces, 299
Initial conditions for a differential equation, 2, 7, 107
Initial conditions for a linear system of differential
equations, 664
Initial-value problems, 8, 18–20, 107, 585, 664
for systems of differential equations, 585,
593, 664
nth order differential equation, 18–20
numerical solution for ﬁrst-order, 93
solution technique via Laplace transforms,
685–689
Initial vector, 479
Inner product, 342, 349
in an arbitrary inner product space, 349
pseudo, 351
standard in Cn, 347–349
standard in Rn, 340–341
Inner product spaces, 339–378
Cauchy-Schwarz inequality, 346–347
complex inner product spaces, 347–349
Gram-Schmidt process, 362–365
least squares approximation, 366–374
linear least squares for nonlinear models, 372–374
orthogonal and orthonormal bases for, 354–357
orthogonal projection, 357–359
real inner product spaces, 342–347
in Rn, 341–342
Instantaneous blow, 706
Integral of a matrix function, 133
Integrating factor
for an exact differential equation, 88–91
for a ﬁrst-order linear differential equation, 55
Integration of a matrix function, 133
Integration techniques, 804–810
integration by partial fractions, 807–809
integration by parts, 804–806
integration by substitution, 806–807
Interval of convergence, 723
Inverse Laplace transform, 678–681
Inverse of a matrix, 168–176
properties of, 175
Inverse transformation, 413–414
Inversion of a permutation, 198
Inverted elements of a permutation, 198
Invertible linear transformations, 413–414, 424–425
Invertible matrices, 169–170, 191–192, 335. See also
Invertible Matrix Theorem
LU decomposition of, 184–186
properties of, 175
square roots of, 491–492
Invertible Matrix Theorem, 188–189, 331–332,
413, 416
Invertible transformations of R2, 394–396
Irreducible quadratic factors, 797
Irregular singular point, 750–751
Isoclines, 27
Isomorphic vector spaces, 414–415, 428
Isomorphism, 414–416, 428
J
Jacobi identity, 136
Jacobian matrix, 655–656, 659
Jordan block, 477–478
number of, 479
size of, 479
Jordan canonical forms, 475–485, 489
deﬁned, 478
K
Kernel
of a Volterra integral equation, 714
of linear transformations, 397–404, 409–410,
424, 428
Kirchoff’s second law, 63–64, 542
Kronecker delta symbol, 130
L
Laguerre equation, 773
Laguerre polynomials, 773
Laplace’s equation, 1, 514
Laplace transforms, 670–721
existence of, 676–681
ﬁrst shifting theorem, 690–693
for solving initial-value problems, 685–689, 718
for solving systems of linear differential
equations, 690
inverse, 678–681
linearity properties of, 673, 717
of a convolution product, 712
of a derivative, 685–689
of a periodic function, 682–684
of the Dirac delta function, 706–709
of the Volterra integral equation, 714–715
second shifting theorem, 699–703
summary of (table), 717
Lattices, 337–338
Leading diagonal of a matrix, 117
Leading one, 146, 153
Leading variable, 160
Least squares approximation, 366–374, 376
Legendre equation, 21, 360, 494, 741–748, 785–786
Legendre polynomials, 310, 360, 742–748
orthogonality of, 744–748
recurrence relation for, 743–744
Length of a cycle, 479
Level curves of a potential function, 84
Limit cycle, 660
Linear Algebra (Friedberg/Insel/Spence), 477fn,
478, 479
Linear Algebra (Shilov), 447
Linear autonomous systems, 643–653
Linear combination, 126, 271–272, 274, 291,
311, 333
of derivative operators, 496
Linear dependence, 284–295, 333–334
in Rn, 291–292
of vector functions, 596

852
Index
Linear dependency, 286
Linear differential equations, 13–14, 53–58, 493–579
applications, 61–68, 529–538, 542–545
Cauchy-Euler, 557–566, 574
constant coefﬁcient, 505–512
ﬁrst-order, 14, 53–58
homogeneous, 496–500
nonhomogeneous, 500–502
nth order, 493–579
power series technique, 722–790
reduction of order technique, 568–572, 575
second-order, 14, 246, 543, 547, 568–572, 574
undetermined coefﬁcients technique, 515–524
Linear differential operators, 496
Linear differential systems, 582
Linear equation, 114
Linear independence, 284–295, 333–334
deﬁnition, 286
in Rn, 291–292
of functions on an interval I, 292–295
of vector functions, 589
Linear least squares, for nonlinear models, 372–374
Linear span of a set of vectors, 278–281
Linear stretch, 393
Linear systems of differential equations. See Systems
of differential equations
Linear systems of equations, 114–195, 190–191,
327–329
augmented matrix of, 140–141, 191
back substitution to solve, 146
bound parameter, 160
bound variable, 160
coefﬁcients, 138
consistent, 140
constants, 138
deﬁnition of a linear system, 138
free parameter, 160
free variable, 160
Gaussian elimination for, 156–165, 191, 194–195
Gauss-Jordan elimination for, 157–158, 173,
194–195
general solution to, 327–329
homogeneous, 138
inconsistent, 140
matrix of coefﬁcients, 140
nonhomogeneous, 138
of differential equations. See Systems of
differential equations
right-hand side vector, 142
solution set, 138
solution to, 138
terminology for, 138–143
trivial solution, 163, 172, 189, 190
vector equation for, 141–142
vector formulation, 141–143
vector of unknowns, 142
Linear transformations, 379–432, 495
codomain, 381
composition of, 407–408, 423–424
domain, 381
from Rn to Rm, 385–388
from R2 to R2, 391–396
compression, 393
expansion, 393
invertible, 394–396
reﬂection, 392–393
shear, 394
stretch, 393
inverse, 413–414
invertible, 394–396, 413–414, 424–425
isomorphism, 414–416, 428
kernel of, 397–404, 409–410, 424, 428
linearity properties of, 381
matrix of, 386, 419–425
matrix representation of, 420–423, 428–429
one-to-one, 409–413, 424–425, 428
onto, 409–413, 424–425, 428
product of, 407–408, 423–424
range, 397–404, 409–410, 424, 428
Rank-Nullity Theorem, 402–404
scalar product of, 390
simple transformations, 392–394
sum of, 390
Linearity properties
of components of a vector, 313
of derivative operators, 496
of Laplace transforms, 673
of linear transformations, 381
Linearly dependent, 286. See also Linear dependence
Linearly independent, 286. See also Linear
independence
Logistic equation, 47
Logistic population model, 2–3, 47–50
Lotka-Volterra system, 659
Lower triangular matrix, 118, 190
determinant of, 209, 221
products of, 131–132
unit lower triangular matrix, 118, 131–132,
182–183
Lower triangular system, 167
LU decomposition of an invertible matrix, 184–186
M
m × n matrix, 115, 190
m × n zero matrix, 124
Maclaurin series expansion, 727, 734
Main diagonal of a matrix, 117, 190
Malthusian population model, 2, 45–46
as ﬁrst-order linear differential equation, 14
doubling time, 46
half-life, 52
Maple (computer algebra system), 30, 48, 50, 93, 99,
156, 651–652, 659, 661, 681, 748
Mapping, 379
Mathematica (computer algebra system), 681, 748
Matrices, 115–133
addition/subtraction of, 122–124, 190
adjoint, 228. See also Adjoint
algebra of, 122–133
anti-symmetric, 119
augmented, 140–141, 192
change of basis, 313–317
characteristic equation of, 436
characteristic polynomial of, 436
cofactor of, 222–223, 236, 242. See also Cofactor
column space, 321–324
column vectors, 116–117
defective, 440, 449
as coefﬁcient matrix of a linear system of
differential equations, 608–618
determinant of. See Determinants
diagonal, 117–119, 190
diagonalizable, 456–459
dimensions, 115
eigenbasis for, 450
eigenspace of, 446
eigenvalues of. See Eigenvalues
eigenvectors of. See Eigenvectors
elementary, 179–186, 192
elements of, 115
equality of, 115–116
exponential function, 462–465
for systems of differential equations,
635–642, 664
function. See Matrix functions
fundamental, 637–638, 664, 665
Hilbert, 179
identity, 130
inverse, 168–176
properties of, 175
invertible, 169–170, 191–192, 335
properties of, 175
square roots of, 491–492
Jacobian, 655–656, 659
leading diagonal, 117
lower triangular, 118, 190
LU decomposition of, 184–186
main diagonal, 117
minor of, 222, 242
multiplication of, 124–131, 190
by a scalar, 123
noncommutativity of, 129
nilpotent, 466
nondefective, 449, 488
as coefﬁcient matrix of a linear system,
599–606
nonsingular, 169
null space of, 270
nullity of, 325
of coefﬁcients, 140, 142
of linear transformations, 386, 419–425
orthogonal, 178, 221, 244, 466–467
product of, 124–131. See also Matrix
multiplication
projection, 370
properties of matrix addition, 123
rank of, 152–153
real skew-symmetric, 267
real symmetric, 467–468, 488–489
reduced row-echelon, 153–154
representation of a linear transformation,
420–423, 428–429
right inverse of, 179
row-echelon, 146–154
algorithm for reducing an m × n matrix A to,
151–152
reduced, 153–154
row-equivalent, 149
row space, 319–321
row vectors, 116–117
scalar matrix, 137
scalar multiplication, 123
scaling matrix, 180
similar, 454–456, 489
singular, 169
size of, 115
skew-symmetric, 119, 190

Index 853
square, 117–119, 190
square roots of, 461, 491–492
subspaces associated to, 334–335
superdiagonals of, 477
symmetric, 119, 190
trace of, 118, 190
transition, 637
transpose of, 117
triangular, 131–132
unit lower triangular, 118, 131–132, 182–183
unit upper triangular, 118, 131–132
upper triangular, 118, 131–132, 190
Wronskian of. See Wronskian zero,
122, 124
Matrix algebra, 122–133, 190
Matrix exponential function, 462–465, 664
systems of differential equations, 635–642, 664
fundamental matrix for, 637–638, 664
transition matrix, 637
Matrix functions, 119–120, 190
algebra and calculus of, 132–133
Matrix inverse, 168–176
Matrix multiplication
examples of, 124–131
index form of the matrix product, 127
noncommutativity of, 129
product of an m × n matrix and a column n-vector
125–126
product of an m × n matrix and an n × p matrix,
126–127
product of a row n-vector and a column n-vector,
124–125
Matrix of coefﬁcients, 140
Matrix of cofactors, 228, 236
Matrix product, 124–131
Matrix representation of a linear transformation
relative to the bases B and C, 420
relative to the basis B, 420
Matrix transformation, 400
Minimal spanning set, 284–287
Minor of a matrix, 222, 242
Mixing problems, 61–63, 581–582, 629–633
concentration, 61
one tank, 61–63
two tanks, 70, 581–582, 629–633
Modern Operational Mathematics in Engineering
(Churchill), 680
Modiﬁed Euler’s method (Heun’s method), 96–98
Modulus of a complex number, 792
Multiplication of matrices, 124–131
Multiplicity of an eigenvalue, 446–447
algebraic, 447
geometric, 447
Multiplier, 182
N
n × n identity matrix, 130
n × n scalar matrix, 137
Newton’s law of cooling, 3–4, 14, 39–40, 60
as a ﬁrst-order linear differential equation, 14, 60
as a separable differential equation, 39–40
slope ﬁeld of, 33
Newton’s second law of motion, 6–9, 531, 581, 625
Nilpotent matrix, 192, 466, 488
Node, 647–653
degenerate, 650
proper, 649
stable, 647–648
unstable, 648
Nondefective matrix, 449, 488
as coefﬁcient matrix for a linear system of
differential equations, 599–606
Nonhomogeneous
linear differential equation, 496, 500–502
general solution to, 515
linear system of differential equations, 582, 665
system of linear equations, 138
vector differential equation, 597
Nonlinear
differential equation, 13–14
models, linear least squares for, 372–374
predator/prey model, 658–660
spring-mass system, 660–661
system of differential equations, 655–661
linear approximation of, 655
transformation, 381, 388, 389
Nonsingular matrices, 169
Norm of a vector, 342, 348, 349
Normal equations, 374
Normal form of a differential equation, 19
Normalization, 353
nth-order differential equation, 18. See also Linear
differential equations, nth order
Null space of a matrix, 270
as a kernel, 400
Null spaces, 334–335
Null vectors, 351
Nullity of a matrix, 325
Numerical solution to differential equations, 93–100,
107
Euler’s method, 93–96, 107
Heun’s method, 96–98
Runge-Kutta method, 98–100
O
Oblique trajectory, 12
Odd parity, 198
Odd permutation, 198
Off-diagonal elements, 118
Ohms (!), 64
Ohm’s law, 64
One-to-one linear transformations, 409–413,
424–425, 428, 443
Ontogenetic growth model, 9–10, 40–41
Onto linear transformations, 409–413, 424–425,
428, 443
Orbit. see Trajectories
Order
of a determinant, 200
of a differential equation, 1, 106
Ordered basis, 312
Ordinary differential equations, 1
Ordinary point, 731
Orthogonal
basis, 354–357, 363
complement, 377–378
coordinate system, 4–5, 12
diagonalization, 466–473
matrix, 178, 221, 244, 466–467
projection, 357–359
set of vectors, 352–354
trajectories, 4–6
vectors, 341
Orthogonality of the Legendre polynomials,
744–748
Orthonormal
basis, 354–357, 365
set of vectors, 352–354
Orthonormalization. See Gram-Schmidt process
Overdamped, 534–535, 543
P
Parallelepiped, volume of, 203–205, 239
Parallelogram
area of, 203–204
law of vector addition, 248–249
Parallelogrammic lattices, 338
Parameter
bound, 160
free, 160
Parity
even, 198
odd, 198
Partial differential equations, 1
Partial fractions decomposition, 797–803, 807
Partial pivoting, 167
Particular solution
to a differential equation, 2, 18, 501–502, 574
derivation via the method of undetermined
coefﬁcients, 515–524
derivation via the method of
variation-of-parameters, 547–555
to a nonhomogeneous linear system of equations,
328–329
to a nonhomogeneous vector differential
equation, 597, 665
derivation via the method of
variation-of-parameters, 620–623
Pendulum, 106, 540
Period of a function, 682–684
Period of oscillation, 532
Periodic function, 682–684
Permutation(s), 197–202
even, 198
inversion of, 198
inverted elements of, 198
odd, 198
Permutation matrix, 180
Phase
angle, 532
paths. see Trajectories
plane, for linear autonomous systems,
643–653
portrait, 644, 648–653
Phase-amplitude form of a solution, 67
Piecewise continuous functions, 673–674
Pivot
column, 151–152
partial, 167
position, 151–152, 191
Polynomial(s)
degree n, 261, 301, 333
irreducible quadratic, 797
Legendre, 310, 360, 742–748
vector space of, 261, 301, 333
Polynomial differential operators, 505–506

854
Index
Population models, 2–3, 45–50
Gompertz, 52
logistic, 2–3, 47–50
Malthusian, 2, 14, 45–46
Postmultiply, 129
Potential function, 83–86
level curves of, 84
Power series, 723–729
algebra of, 725–726
analytic functions, 727–729
basic convergence theorem, 724
convergence of, 723–724
differentiation of, 726
Frobenius series, 752
interval of convergence, 723–724
radius of convergence, 724, 728
ratio test, 724
Taylor series expansion, 727–729
theory of, 759–771, 778
Predator/prey model, 658–660
Predictor-corrector method, 96
Premultiply, 129
Principal Axes Theorem, 470
Product of linear transformations, 407–408, 423–424
Projection, orthogonal, 357–359
Projection matrix, 370
Proper node, 649
Pseudo-inner product, 351
Pythagorean Theorem, 352
Q
QR Factorization, 188
Qualitative analysis of system of differential
equations, 643, 655, 665
Quasi period, 533
R
Radius of convergence, 724
Range of a linear transformation, 397–404, 409–410,
424, 428, 443
Rank-Nullity Theorem, 325–329, 402–404
Rank of a matrix, 152–153
condition for invertibility, 171
for solutions to linear systems of equations, 159–162
Ratio test, 724
RC circuit, 65
Real n-space, 139, 248–251
Real inner product space, 342–347
Real skew-symmetric matrices, 267
Real symmetric matrices, 467–468, 488–489
Real vector space, 254, 333
Reduced row-echelon form, 153–154
Reduction of order, 568–572, 575
Reﬂections in R2, 392–393
Regular
differential equation, 497
singular point, 750–757
Resistance, 64
Resistor, 64
Resonance, 537
Riccati equation, 81
Right-hand side vector, 142
Right inverse of a matrix, 179
RLC circuits, 64–65, 542–545
RL circuit, 65
phase angle, 67
Rodrigues’ formula, 743–744
Row combination, 180
Row-echelon form, 146–154
algorithm for reducing an m × n matrix A to,
151–152
reduced, 153–154
Row-echelon matrices, 146–154
Row-equivalent matrices, 149
Row n-vectors, 116
Row space of a matrix, 319–321, 334–335
Row vectors, 116–117
Runge-Kutta method, 98–100
S
Saddle point, 648
Scalar, 123
Scalar matrix, 137, 190
Scalar multiplication, 123, 190, 252–254, 333
associativity of, 253
closure under, 253, 264, 333
distributive property over addition, 253
in a vector space, 252–254
in Rn, 248
of a linear transformation, 390
of a matrix, 123
properties of, 123
Scalar product, of linear transformations, 390
Scaling matrix, 180
Second-order differential equations, 102–104, 246
linear, 14, 246
solution technique for nonhomogeneous
via the method of reduction of order, 568–572
via the method of variation-of-parameters,
547–552
with the dependent variable missing, 102–103
with the independent variable missing, 103–104
Second shifting theorem, 699–703
Separable differential equations, 34–42
Series solutions, 722–790
about an ordinary point, 731–739, 786
about a regular singular point, 750–757, 786–787
Legendre equations, 741–742, 785–786
Legendre polynomials, 742–748
Shear in R2, 394
Shifting theorem
ﬁrst, 690–693
second, 699–703
Similar matrices, 454–456, 489
Simple harmonic motion (SHM), 9, 532
amplitude, 532
circular frequency, 532
frequency of oscillation, 532
period, 532
phase, 532
Simple harmonic oscillator, 9
Simple transformations of R2, 392–394
Singular matrices, 169
Singular point of a differential equation, 731
irregular, 750–751
regular, 750–751
Size of a matrix, 115
Skew-symmetric matrix, 119, 190
Slope ﬁelds, 26–30, 107
direction ﬁeld, 652
generating using technology, 30–31
Solution
complex-valued trial, 526–528
curves, 23
equilibrium, 27, 107, 643–644
existence and uniqueness of. See Existence and
Uniqueness Theorem
numerical. See Numerical solution to differential
equations
particular. See Particular solution
set. See Solution set
steady-state, 67, 538, 544–545
to a differential equation, 14–18
to a homogeneous linear system of equations,
162–164
to a homogeneous vector differential equation, 601
to a linear system, 138, 169, 172, 229, 239,
242–243, 328
homogeneous, 162–164
number of solutions, 159–162
of differential equations, 583
uniqueness of, 159, 162
to a nonhomogeneous vector differential
equation, 620
to a system of differential equations, 583
to the eigenvalue/eigenvector problem, 436
transient, 544
trial, 517, 574
Solution set
fundamental, 594–595, 664, 665
of a linear system of equations, 138, 269–270,
327, 398
homogeneous, 327, 398
nonhomogeneous, 328–329
Solution space of a differential equation, 271–272,
497–498
Spacelike vectors, 351
Span of a set of vectors, 278–281
Spanning sets, 274–281, 333
linear span of a set of vectors, 278–281
minimal, 284–287
Spans, 274
Special Functions and Their Applications (Lebedev),
775fn
Spiral point, 651, 659
Spring constant, 9, 530, 625
Spring force, 8–9
Spring–mass system, 371, 514, 529–538, 543, 574
amplitude, 532
circular frequency, 532
coupled. See Coupled spring–mass system
critically damped, 534, 543
damping force, 530
external driving force, 530
frequency, 532
nonlinear, 14
overdamped, 534–535, 543
period, 532
phase, 532
quasi period, 533
spring force, 530
in static equilibrium, 529–530
steady-state solution, 538

Index 855
transient solution, 538
underdamped, 533–534, 543
quasi period, 533
Square lattices, 338
Square matrices, 117–119, 190
Square roots of matrices, 461, 491–492
Stable center, 651
Stable node, 647–648. See also Node
Stable spiral point, 651
Standard basis, 299–301
on Mm×n (R), 300
on Pn(R), 301
on Rn, 300
Standard inner product, 341
deﬁned, 348
in Cn, 347–349
in Rn, 340–341
Static equilibrium, 529–530
Steady-state current, 544–545
Steady-state solution, 67, 538, 544–545
Step function, 695–698
Stretches in R2, 393
Subspaces, 263–272, 333
associated to matrices, 334–335
spanned by {V1, V2, . . . , Vk}, 274–279, 333
trivial, 269
Substitution technique for integration, 806–807
Subtraction of matrices, 123–124
Sum, of linear transformations, 390
Superdiagonals of matrices, 477
Symmetric matrix, 119, 190
System coefﬁcients, 138
System constants, 138
Systems of differential equations, 580–669
applications of, 580–582, 625–633
defective coefﬁcient matrix, 608–618
fundamental matrix for, 594–595
fundamental solution set, 594–595
general solution to, 593–595, 601, 606, 637,
664, 665
homogeneous, 582
solution techniques, 599–606, 608–618
nondefective coefﬁcient matrix, 599–606
nonhomogeneous, 582
solution via variation-of-parameters, 620–623
nonlinear, 655–661
linear approximation of, 655
particular solution to, 597, 620–623, 665
qualitative analysis of solution, 643–653
solution of, 583
transition matrix for, 637
Systems of linear equations. See Linear systems
of equations
T
Taylor series expansion, 727–729
Terminal vector, 479
Terminal velocity, 38
Tetrahedron, volume of, 244–245
Theory of Differential Equations
(Coddington/Levinson), 497
Threshold level, 52
Timelike vectors, 351
Torricelli’s law, 112
Trace of a matrix, 118, 190
Trajectories, 643–648, 665
orthogonal, 4–6
Transformations. See Linear transformations
Transient part
charge in a circuit, 544
of an alternating current, 67
of spring-mass motion, 538
Transient part of a solution, 67
Transition matrix, 637
Transpose of a matrix, 117, 190
properties of, 131
Trial solution, 517, 574
complex-valued, 526–528
Triangular matrices, 118, 131–132, 190, 209, 221
Triangular wave function, 685
Trivial solution of a linear system, 163, 172, 189, 190
Trivial subspace, 269
Trivial vector space, 302
U
Underdamped, 533–534, 543
Undetermined coefﬁcients, 517, 574
method of, 515–524, 574
Uniqueness of
additive inverse, 259
solutions to differential equations. See Existence
and Uniqueness Theorem
zero vector, 259
Unit impulse, 707
impulse function, 707
lower triangular matrix, 118, 131–132, 182–183
property, 253
step function, 695–698
upper triangular matrix, 118, 131–132
vector, 203, 352–353
Unstable node, 648. See also Node
Unstable spiral point, 651
Upper triangular matrix, 118, 131–132, 190
determinant of, 209
products of, 131–132
unit upper triangular matrix, 118, 131–132
V
Van der Pol equation, 660–661
Vandermonde determinant, 233
Variable
bound, 160
dependent, 1
free, 160
independent, 1
Variation-of-parameters method, 60, 547–555, 574,
620–623, 665
for ﬁrst-order linear differential equations, 60
for higher-order differential equations, 552–555
for linear systems of differential equations,
620–623
for second-order linear differential equations,
547–552
Vector(s)
addition, 252
components, 116
components relative to an ordered basis, 312
formulation
of a linear system of equations, 141–143
of a system of differential equations, 588–591
geometric, 248–250
in Rn, 248–251
initial, 479
linearly dependent set of, 286
linearly independent set of, 286
linear span of, 278–281
null, 351
orthogonal, 341
orthogonal set of, 352–354
orthonormal set of, 352–354
spacelike, 351
terminal, 479
timelike, 351
unit, 203, 352–353
Vector addition, 252
distributive property of scalar multiplication
over, 253
Vector differential equations, 590–591, 593–597,
599–606, 608–618, 663–665
defective coefﬁcient matrix, 608–618
fundamental matrix for, 594–595, 664, 665
fundamental solution set, 594–595, 664, 665
general solution to
homogeneous vector differential equation, 594
nonhomogeneous vector differential
equation, 597
homogeneous, 593–597, 663–664
nondefective coefﬁcient matrix, 599–606
nonhomogeneous, 597, 665
particular solution to, 597, 665
Vector equation. See Vector formulation
Vector formulation
of a linear system of equations, 141–143
of a system of differential equations, 588–591
Vector functions, 119–120. See also Column n-vector
functions
Vector of unknowns, 142
Vector spaces, 246–338
axioms, 253
basis for, 299
complex, 254
dimension of, 302
ﬁnite-dimensional, 299
inﬁnite-dimensional, 299
isomorphic spaces, 414–415, 428
isomorphism, 414–416, 428
real, 254
subspaces of, 263–272, 333
deﬁned, 264
trivial, 302
Volterra integral equations, 714–715
Volume of a parallelepiped, 203–205, 239
W
Weighted inner product, 345
Wronskian, 292–295, 499–500
and determinants, 207
of vector functions, 589, 596
Z
Zero matrix, 122, 124
Zero transformation, 407
Zero vector
existence of, 253
in Rn, 251
properties of, 259
Zero Vector Check, 266–267

Basic Integrals
Function F(x)
Integral
!
F(x) dx
xn
(n ̸= −1)
1
n + 1xn+1 + c
x−1
ln |x| + c
eax
(a ̸= 0)
1
a eax + c
xeax
1
a2 (ax −1)eax + c
sin x
−cos x + c
cos x
sin x + c
x sin x
sin x −x cos x + c
x cos x
cos x + x sin x + c
sec2 x
tan x + c
csc2 x
−cot x + c
sec x · tan x
sec x + c
csc x · cot x
csc x + c
tan x
ln | sec x| + c
sec x
ln | sec x + tan x| + c
csc x
ln | csc x −cot x| + c
eax sin bx
1
a2 + b2 eax(a sin bx −b cos bx) + c
eax cos bx
1
a2 + b2 eax(a cos bx + b sin bx) + c
ln x
x ln x −x + c
xn ln x
xn+1
(n + 1)2 [(n + 1) ln x −1] + c
1
a2 + x2
1
a tan−1(x/a) + c
1
√
a2 −x2
(a > 0)
sin−1(x/a) + c
1
√
a2 + x2
ln |x +
√
a2 + x2| + c
f ′(x)
f (x)
ln | f (x)| + c
eu(x) du
dx
eu(x) + c
856

Index of Symbols
Below is a collection of mathematical symbols and notations that occur throughout the text.
Symbol
Meaning
y(n)
nth derivative of the function y
δi j
Kronecker delta symbol
∈
denotes membership in a set
∪
set-theoretic union
∩
set-theoretic intersection
ai j
element in the ith row and jth column of a matrix
AT
transpose of A
A−1
inverse of A
tr(A)
trace of A
det(A)
determinant of A
|A|
determinant of A
Mi j
i j-minor of a matrix
Ci j
i j-cofactor of a matrix
adj(A)
adjoint of A
diag(d1, d2, . . . , dn)
diagonal matrix with diagonal elements di
0m×n
m × n zero matrix
0n
n × n zero matrix
In
n × n identity matrix
A#
augmented matrix
A ∼B
A is row-equivalent to B
rank(A)
rank of A
rowspace(A)
row space of A
colspace(A)
column space of A
nullspace(A)
null space of A
nullity(A)
nullity of A
JCF(A)
Jordan canonical form of A
Jλ
Jordan block
Pi j
permute rows i and j
Mi(k)
multiply row i by k
Ai j(k)
add k times row i to row j
C Pi j
permute columns i and j
C Mi(k)
multiply column i by k
C Ai j(k)
add k times column i to column j
N(p1, p2, . . . , pn)
number of inversions of the permutation (p1, p2, . . . , pn)
σ(p1, p2, . . . , pn)
parity of the permutation (p1, p2, . . . , pn)
T : V →W
mapping or linear transformation from V to W
Ker(T )
kernel of the linear transformation T
Rng(T )
range of the linear transformation T
T2T1
composition of linear transformations T1 and T2
T −1
inverse of the linear transformation T
V ∼= W
V and W are isomorphic vector spaces
[T ]C
B
matrix representation of T relative to bases B and C

Symbol
Meaning
R
set of real numbers
R+
set of positive real numbers
C
set of complex numbers
z
complex conjugate of z
Rn
set of ordered n-tuples of real numbers
ei
ith standard basis vector in Rn
Cn
set of ordered n-tuples of complex numbers
Pn(R)
set of polynomials with real coefﬁcients of degree ≤n
Mn(R)
set of n × n matrices with real elements
Un(R)
set of n × n upper triangular matrices with real elements
Ln(R)
set of n × n lower triangular matrices with real elements
Mm×n(R)
set of m × n matrices with real elements
C[a, b]
set of continuous functions on the interval [a, b]
C(k)(I)
set of all continuous functions with at least k continuous derivatives on I
Vn(I)
set of all column n-vector functions on I
span{v1, v2, . . . , vk}
linear span of {v1, v2, . . . , vk}
W[ f1, f2, . . . , fk]
Wronskian of functions f1, f2, . . . , fk
W[x1, x2, . . . , xk]
Wronskian of vector functions x1, x2, . . . , xk
dim[V ]
dimension of the vector space V
[v]B
components of v relative to basis B
PC←B
change of basis matrix from B to C
⟨, ⟩
inner product
a · b
dot product of vectors a and b
a × b
cross product of vectors a and b
i, j, k
unit vectors pointing along coordinate axes in 3-space
0
zero vector
−v
additive inverse of v
||v||
norm of the vector v
P(w, v)
orthogonal projection of w on v
W ⊥
orthogonal complement of the subspace W
eAt
matrix exponential function
D : C1(I) →C0(I)
derivative operator
P(D)
polynomial differential operator
K(x, t)
Green’s function
X(t)
fundamental matrix
X0(t)
transition matrix
J(x, y)
Jacobian matrix
L[ f ]
Laplace transform of f
L−1[F]
inverse Laplace transform of F
ua(t)
Heaviside (unit) step function
δ(t −a)
Dirac-delta function
f ∗g
convolution product of f and g
$(p)
gamma function
Jp(x)
Bessel function of order p

Some Solution Techniques for y′ = f (x, y)
Type
Standard Form
Technique
Separable (Section 1.4)
p(y)y′ = q(x)
Separate the variables and integrate directly:
Equation
!
p(y)dy =
!
q(x)dx
First-Order Linear
y′ + p(x)y = q(x)
Rewrite as d
dx (I · y) = qI, where I = e
!
p(x)dx,
Equation (Section 1.6)
and integrate with respect to x
First-Order Homogeneous
y′ = f (x, y) with f
Change variables: y = xV (x)
(Section 1.8)
homogeneous of degree
and reduce to a separable equation
zero: f (tx, ty) = f (x, y)
Bernoulli Equation
y′ + p(x)y = q(x)yn
Divide by yn and make the change of
(Section 1.8)
variables u = y1−n and reduce to a
linear equation
Exact Equation
M(x, y)dx + N(x, y)dy = 0,
Solution is φ(x, y) = c, where φ is
(Section 1.9)
with My = Nx
determined by integration φx = M, φy = N
The Method of Undetermined Coefﬁcients
The following table lists trial solutions for the DE P(D)y = F(x), where P(D) is a polynomial differential operator (see
Section 8.3).
F(x)
Usual trial solution
Modiﬁed trial solution
If P(a) ̸= 0:
If a is a root of P(r) = 0 of multiplicity m:
cxkeax
yp(x) = xmeax(A0 + A1x + · · · + Akxk)
yp(x) = eax(A0 + A1x + · · · + Akxk)
cxkeax cos bx or
cxkeax sin bx
If P(a + ib) ̸= 0:
If a + ib is a root of P(r) = 0 of multiplicity m:
yp(x) = eax
" k
#
i=0
xi(Ai cos bx + Bi sin bx)
$
yp(x) = xmeax
" k
#
i=0
xi(A0 cos bx + B0 sin bx)
$
Variation of Parameters
Consider y′′ + a1(x)y′ + a2(x)y = F(x), where a1, a2, F are continuous. If y1 and y2 are linearly independent solutions
to y′′ + a1(x)y′ + a2(x)y = 0, then a particular solution to the nonhomogeneous DE is yp = u1y1 + u2y2, where
u1(x) = −
%
y2F
y1y′
2 −y′
1y2
dx and u2(x) =
%
y1F
y1y′
2 −y′
1y2
. Section 8.7 contains a generalization to DE of order > 2.

A Short Table of Laplace Transforms
Function f (t)
Laplace Transform F(s)
f (t) = tn
(n = nonnegative integer)
F(s) =
n!
sn+1 , s > 0
f (t) = eat
(a = constant)
F(s) =
1
s −a , s > a
f (t) = sin bt
(b = constant)
F(s) =
b
s2 + b2 , s > 0
f (t) = cos bt
(b = constant)
F(s) =
s
s2 + b2 , s > 0
f (t) = t−1/2
F(s) = √π/s, s > 0
f (t) = ua(t)
F(s) = 1
s e−as
f (t) = δ(t −a)
F(s) = e−as
Transform of Derivatives (Section 10.4)
f ′
L[ f ′] = sL[ f ] −f (0)
f ′′
L[ f ′′] = s2L[ f ] −s f (0) −f ′(0)
Shifting Theorems (Sections 10.5 and 10.7)
eat f (t)
F(s −a)
ua(t) f (t −a)
e−as F(s)
Invertible Matrix Theorem
(Theorems 2.8.1, 3.2.5, 4.10.1, and 6.4.22)
For any n × n matrix A, the following conditions are equivalent:
(a) A is invertible (i.e., there exists a matrix A−1 with AA−1 = A−1A = In).
(b) The equation Ax = b has a unique solution for every b in Rn.
(c) The equation Ax = 0 has only the trivial solution x = 0.
(d) rank(A) = n.
(e) A can be expressed as a product of elementary matrices.
(f) A is row-equivalent to In.
(g) det(A) ̸= 0.
(h) nullity(A) = 0.
(i) nullspace(A) = {0}.
(j) The columns of A form a linearly independent set of vectors in Rn.
(k) colspace(A) = Rn (that is, the columns of A span Rn).
(l) The columns of A form a basis for Rn.
(m) The rows of A form a linearly independent set of vectors in Rn.
(n) rowspace(A) = Rn (that is, the rows of A span Rn).
(o) The rows of A form a basis for Rn.
(p) AT is invertible.
(q) The transformation T : Rn →Rn given by T (x) = Ax is one-to-one (i.e., Ker(T ) = {0}).
(r) The transformation in (q) is onto (i.e., Rng(T ) = Rn).
(s) The transformation in (q) is an isomorphism.

• If the conditions in the Invertible Matrix Theorem hold, the inverse of A is given by the formula A−1 =
1
det(A)adj(A),
where adj(A) denotes the adjoint of A. (Theorem 3.3.18)
• (Cramer’s Rule) If the conditions in the Invertible Matrix Theorem hold, the unique solution x in (b) is (x1, x2, . . . ,
xn), where xk = det(Bk)
det(A) (k = 1, 2, . . . , n), and Bk denotes the matrix obtained by replacing the kth column vector
of A by b. (Theorem 3.3.21)
Matrices and Systems of Linear Equations
A linear system of m equations in n unknowns can be written Ax = b, where
A is an m × n matrix,
x is in Rn,
b is in Rm.
rank(A) = number of nonzero rows in any row-echelon form of A.
If r = rank(A) and r# = rank(A#), where A# is the augmented matrix [A | b], then (see Theorem 2.5.9)
1. If r < r#, the system Ax = b is inconsistent.
2. If r = r#, the system is consistent and:
(a) There exists a unique solution if and only if r# = n.
(b) There exists an inﬁnite number of solutions if and only if r# < n.
Vector Spaces
A vector space V is a collection of vectors, together with operations of addition (+) and scalar multiplication (·) on the
vectors. In this text, the set of scalars = R or C. In addition, the following vector space axioms must be satisﬁed for all
scalars r, s and all vectors u, v, w in V (see Section 4.2):
(A1) Closure under addition: u + v ∈V .
(A2) Closure under scalar multiplication: r · u ∈V .
(A3) Commutativity of addition: u + v = v + u.
(A4) Associativity of addition: (u + v) + w = u + (v + w).
(A5) Existence of zero vector: There exists 0 ∈V with v + 0 = 0 + v = v for all v.
(A6) Existence of additive inverses: For each v, there exists −v with v + (−v) = 0.
(A7) Unit property: 1 · v = v.
(A8) Associativity of scalar multiplication: (rs) · v = r · (s · v)
(A9) Associativity of scalar multiplication over vector addition: r · (u + v) = r · u + r · v.
(A10) Associativity of scalar multiplication over scalar addition: (r + s) · v = r · v + s · v.
Examples: (In each case, the usual operations of addition and scalar multiplication are used.)
• Rn, the collection of ordered n-tuples of real numbers.
• Cn, the collection of ordered n-tuples of complex numbers.
• Mm×n(R), the collection of all m × n matrices with real entries.
• Mn(R), the collection of all n × n matrices with real entries.
• Ck(I), the collection of real-valued functions with (at least) k continuous derivatives on the interval I.
• Pn(R), the collection of all polynomials of degree ≤n with real coefﬁcients.


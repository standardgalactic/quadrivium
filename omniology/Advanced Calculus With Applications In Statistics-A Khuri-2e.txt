
Advanced Calculus with
Applications in Statistics
Second Edition
Revised and Expanded
Andre I. Khuri
´
University of Florida
Gainesville, Florida


Advanced Calculus with
Applications in Statistics
Second Edition


Advanced Calculus with
Applications in Statistics
Second Edition
Revised and Expanded
Andre I. Khuri
´
University of Florida
Gainesville, Florida


Copyright  2003 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive,
Ž
.
Ž
.
Danvers, MA 01923, 978 750-8400, fax 978 750-4470, or on the web at www.copyright.com.
Requests to the Publisher for permission should be addressed to the Permissions Department,
Ž
.
Ž
.
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, 201 748-6011, fax 201
748-6008, e-mail: permreq@wiley.com.
Limit of LiabilityrDisclaimer of Warranty: While the publisher and author have used their best
efforts in preparing this book, they make no representations or warranties with respect to
the accuracy or completeness of the contents of this book and specifically disclaim any
implied warranties of merchantability or fitness for a particular purpose. No warranty may be
created or extended by sales representatives or written sales materials. The advice and
strategies contained herein may not be suitable for your situation. You should consult with
a professional where appropriate. Neither the publisher nor author shall be liable for any
loss of profit or any other commercial damages, including but not limited to special,
incidental, consequential, or other damages.
For general information on our other products and services please contact our Customer
Care Department within the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or
fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears
in print, however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data
Khuri, Andre I., 1940-
´
Advanced calculus with applications in statistics r Andre I. Khuri. -- 2nd ed. rev. and
´
expended.
Ž
.
p. cm. -- Wiley series in probability and statistics
Includes bibliographical references and index.
Ž
.
ISBN 0-471-39104-2 cloth : alk. paper
1. Calculus. 2. Mathematical statistics. I. Title. II. Series.
QA303.2.K48 2003
515--dc21
2002068986
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

To Ronnie, Marcus, and Roxanne
and
In memory of my sister Ninette

Contents
Preface
xv
Preface to the First Edition
xvii
1.
An Introduction to Set Theory
1
1.1.
The Concept of a Set, 1
1.2.
Set Operations, 2
1.3.
Relations and Functions, 4
1.4.
Finite, Countable, and Uncountable Sets, 6
1.5.
Bounded Sets, 9
1.6.
Some Basic Topological Concepts, 10
1.7.
Examples in Probability and Statistics, 13
Further Reading and Annotated Bibliography, 15
Exercises, 17
2.
Basic Concepts in Linear Algebra
21
2.1.
Vector Spaces and Subspaces, 21
2.2.
Linear Transformations, 25
2.3.
Matrices and Determinants, 27
2.3.1.
Basic Operations on Matrices, 28
2.3.2.
The Rank of a Matrix, 33
2.3.3.
The Inverse of a Matrix, 34
2.3.4.
Generalized Inverse of a Matrix, 36
2.3.5.
Eigenvalues and Eigenvectors of a Matrix, 36
2.3.6.
Some Special Matrices, 38
2.3.7.
The Diagonalization of a Matrix, 38
2.3.8.
Quadratic Forms, 39
vii

CONTENTS
viii
2.3.9.
The Simultaneous Diagonalization
of Matrices, 40
2.3.10.
Bounds on Eigenvalues, 41
2.4.
Applications of Matrices in Statistics, 43
2.4.1.
The Analysis of the Balanced Mixed Model, 43
2.4.2.
The Singular-Value Decomposition, 45
2.4.3.
Extrema of Quadratic Forms, 48
2.4.4.
The Parameterization of Orthogonal
Matrices, 49
Further Reading and Annotated Bibliography, 50
Exercises, 53
3.
Limits and Continuity of Functions
57
3.1.
Limits of a Function, 57
3.2.
Some Properties Associated with Limits of Functions, 63
3.3.
The o, O Notation, 65
3.4.
Continuous Functions, 66
3.4.1.
Some Properties of Continuous Functions, 71
3.4.2.
Lipschitz Continuous Functions, 75
3.5.
Inverse Functions, 76
3.6.
Convex Functions, 79
3.7.
Continuous and Convex Functions in Statistics, 82
Further Reading and Annotated Bibliography, 87
Exercises, 88
4.
Differentiation
93
4.1.
The Derivative of a Function, 93
4.2.
The Mean Value Theorem, 99
4.3.
Taylor’s Theorem, 108
4.4.
Maxima and Minima of a Function, 112
4.4.1.
A Sufficient Condition for a Local Optimum, 114
4.5.
Applications in Statistics, 115
Functions of Random Variables, 116
4.5.2.
Approximating Response Functions, 121
4.5.3.
The Poisson Process, 122
4.5.4.
Minimizing the Sum of Absolute Deviations, 124
Further Reading and Annotated Bibliography, 125
Exercises, 127
4.5.1.

CONTENTS
ix
5.
Infinite Sequences and Series
132
5.1.
Infinite Sequences, 132
5.1.1.
The Cauchy Criterion, 137
5.2.
Infinite Series, 140
5.2.1.
Tests of Convergence for Series
of Positive Terms, 144
5.2.2.
Series of Positive and Negative Terms, 158
5.2.3.
Rearrangement of Series, 159
5.2.4.
Multiplication of Series, 162
5.3.
Sequences and Series of Functions, 165
5.3.1.
Properties of Uniformly Convergent Sequences
and Series, 169
5.4.
Power Series, 174
5.5.
Sequences and Series of Matrices, 178
5.6.
Applications in Statistics, 182
5.6.1.
Moments of a Discrete Distribution, 182
5.6.2.
Moment and Probability Generating
Functions, 186
5.6.3.
Some Limit Theorems, 191
5.6.3.1.
The Weak Law of Large Numbers
Ž
.
Khinchine’s Theorem , 192
5.6.3.2.
The Strong Law of Large Numbers
Ž
.
Kolmogorov’s Theorem , 192
5.6.3.3.
The Continuity Theorem for Probability
Generating Functions, 192
5.6.4.
Power Series and Logarithmic Series
Distributions, 193
5.6.5.
Poisson Approximation to Power Series
Distributions, 194
5.6.6.
A Ridge Regression Application, 195
Further Reading and Annotated Bibliography, 197
Exercises, 199
6.
Integration
205
6.1.
Some Basic Definitions, 205
6.2.
The Existence of the Riemann Integral, 206
6.3.
Some Classes of Functions That Are Riemann
Integrable, 210
6.3.1.
Functions of Bounded Variation, 212

CONTENTS
x
6.4.
Properties of the Riemann Integral, 215
6.4.1.
Change of Variables in Riemann Integration, 219
6.5.
Improper Riemann Integrals, 220
6.5.1.
Improper Riemann Integrals of the Second
Kind, 225
6.6.
Convergence of a Sequence of Riemann Integrals, 227
6.7.
Some Fundamental Inequalities, 229
6.7.1.
The CauchySchwarz Inequality, 229
6.7.2.
Holder’s Inequality, 230
¨
6.7.3.
Minkowski’s Inequality, 232
6.7.4.
Jensen’s Inequality, 233
6.8.
RiemannStieltjes Integral, 234
6.9.
Applications in Statistics, 239
6.9.1.
The Existence of the First Negative Moment of a
Continuous Distribution, 242
6.9.2.
Transformation of Continuous Random
Variables, 246
6.9.3.
The RiemannStieltjes Representation of the
Expected Value, 249
6.9.4.
Chebyshev’s Inequality, 251
Further Reading and Annotated Bibliography, 252
Exercises, 253
7.
Multidimensional Calculus
261
7.1.
Some Basic Definitions, 261
7.2.
Limits of a Multivariable Function, 262
7.3.
Continuity of a Multivariable Function, 264
7.4.
Derivatives of a Multivariable Function, 267
7.4.1.
The Total Derivative, 270
7.4.2.
Directional Derivatives, 273
7.4.3.
Differentiation of Composite Functions, 276
7.5.
Taylor’s Theorem for a Multivariable Function, 277
7.6.
Inverse and Implicit Function Theorems, 280
7.7.
Optima of a Multivariable Function, 283
7.8.
The Method of Lagrange Multipliers, 288
7.9.
The Riemann Integral of a Multivariable Function, 293
7.9.1.
The Riemann Integral on Cells, 294
7.9.2.
Iterated Riemann Integrals on Cells, 295
7.9.3.
Integration over General Sets, 297
7.9.4.
Change of Variables in n-Tuple Riemann
Integrals, 299

CONTENTS
xi
7.10.
Differentiation under the Integral Sign, 301
7.11.
Applications in Statistics, 304
7.11.1.
Transformations of Random Vectors, 305
7.11.2.
Maximum Likelihood Estimation, 308
7.11.3.
Comparison of Two Unbiased
Estimators, 310
7.11.4.
Best Linear Unbiased Estimation, 311
7.11.5.
Optimal Choice of Sample Sizes in Stratified
Sampling, 313
Further Reading and Annotated Bibliography, 315
Exercises, 316
8.
Optimization in Statistics
327
8.1.
The Gradient Methods, 329
8.1.1.
The Method of Steepest Descent, 329
8.1.2.
The NewtonRaphson Method, 331
8.1.3.
The DavidonFletcherPowell Method, 331
8.2.
The Direct Search Methods, 332
8.2.1.
The NelderMead Simplex Method, 332
8.2.2.
Price’s Controlled Random Search
Procedure, 336
8.2.3.
The Generalized Simulated Annealing
Method, 338
8.3.
Optimization Techniques in Response Surface
Methodology, 339
8.3.1.
The Method of Steepest Ascent, 340
8.3.2.
The Method of Ridge Analysis, 343
8.3.3.
Modified Ridge Analysis, 350
8.4.
Response Surface Designs, 355
8.4.1.
First-Order Designs, 356
8.4.2.
Second-Order Designs, 358
8.4.3.
Variance and Bias Design Criteria, 359
8.5.
Alphabetic Optimality of Designs, 362
8.6.
Designs for Nonlinear Models, 367
8.7.
Multiresponse Optimization, 370
8.8.
Maximum Likelihood Estimation and the
EM Algorithm, 372
8.8.1.
The EM Algorithm, 375
8.9.
Minimum Norm Quadratic Unbiased Estimation of
Variance Components, 378

CONTENTS
xii
8.10.
Scheffe’s Confidence Intervals, 382
´
8.10.1.
The Relation of Scheffe’s Confidence Intervals
´
to the F-Test, 385
Further Reading and Annotated Bibliography, 391
Exercises, 395
9.
Approximation of Functions
403
9.1.
Weierstrass Approximation, 403
9.2.
Approximation by Polynomial Interpolation, 410
9.2.1.
The Accuracy of Lagrange Interpolation, 413
9.2.2.
A Combination of Interpolation and
Approximation, 417
9.3.1.
Properties of Spline Functions, 418
9.3.2.
Error Bounds for Spline Approximation, 421
9.4.
Applications in Statistics, 422
9.4.1.
Approximate Linearization of Nonlinear Models
by Lagrange Interpolation, 422
9.4.2.
Splines in Statistics, 428
9.4.2.1.
The Use of Cubic Splines in
Regression, 428
9.4.2.2.
Designs for Fitting Spline Models, 430
9.4.2.3.
Other Applications of Splines in
Statistics, 431
Further Reading and Annotated Bibliography, 432
Exercises, 434
10.
Orthogonal Polynomials
437
10.1.
Introduction, 437
10.2.
Legendre Polynomials, 440
10.2.1.
Expansion of a Function Using Legendre
Polynomials, 442
10.3.
Jacobi Polynomials, 443
10.4.
Chebyshev Polynomials, 444
10.4.1.
Chebyshev Polynomials of the First Kind, 444
10.4.2.
Chebyshev Polynomials of the Second Kind, 445
10.5.
Hermite Polynomials, 447
10.6.
Laguerre Polynomials, 451
10.7.
Least-Squares Approximation with Orthogonal
Polynomials, 453
9.3
Approximation by Spline Functions, 418
.

CONTENTS
xiii
10.8.
Orthogonal Polynomials Defined on a Finite Set, 455
10.9.
Applications in Statistics, 456
10.9.1.
Applications of Hermite Polynomials, 456
10.9.1.1.
Approximation of Density Functions
and Quantiles of Distributions, 456
10.9.1.2.
Approximation of a Normal
Integral, 460
10.9.1.3.
Estimation of Unknown
Densities, 461
10.9.2.
Applications of Jacobi and Laguerre
Polynomials, 462
10.9.3.
Calculation of Hypergeometric Probabilities
Using Discrete Chebyshev Polynomials, 462
Further Reading and Annotated Bibliography, 464
Exercises, 466
11.
Fourier Series
471
11.1.
Introduction, 471
11.2.
Convergence of Fourier Series, 475
11.3.
Differentiation and Integration of Fourier Series, 483
11.4.
The Fourier Integral, 488
11.5.
Approximation of Functions by Trigonometric
Polynomials, 495
11.5.1.
Parseval’s Theorem, 496
11.6.
The Fourier Transform, 497
11.6.1.
Fourier Transform of a Convolution, 499
11.7.
Applications in Statistics, 500
Applications in Time Series, 500
11.7.2.
Representation of Probability Distributions, 501
11.7.3.
Regression Modeling, 504
11.7.4.
The Characteristic Function, 505
11.7.4.1.
Some Properties of Characteristic
Functions, 510
Further Reading and Annotated Bibliography, 510
Exercises, 512
12.
Approximation of Integrals
517
12.1.
The Trapezoidal Method, 517
12.1.1.
Accuracy of the Approximation, 518
12.2.
Simpson’s Method, 521
12.3.
NewtonCotes Methods, 523
11.7.1.

CONTENTS
xiv
12.4.
Gaussian Quadrature, 524
12.5.
Approximation over an Infinite Interval, 528
12.6.
The Method of Laplace, 531
12.7.
Multiple Integrals, 533
12.8.
The Monte Carlo Method, 535
12.8.1.
Variation Reduction, 537
12.8.2.
Integrals in Higher Dimensions, 540
12.9.
Applications in Statistics, 541
12.9.1.
The GaussHermite Quadrature, 542
12.9.2.
Minimum Mean Squared Error
Quadrature, 543
12.9.3.
Moments of a Ratio of Quadratic Forms, 546
12.9.4.
Laplace’s Approximation in Bayesian
Statistics, 548
12.9.5.
Other Methods of Approximating Integrals
in Statistics, 549
Further Reading and Annotated Bibliography, 550
Exercises, 552
Appendix. Solutions to Selected Exercises
557
Chapter 1, 557
Chapter 2, 560
Chapter 3, 565
Chapter 4, 570
Chapter 5, 577
Chapter 6, 590
Chapter 7, 600
Chapter 8, 613
Chapter 9, 622
Chapter 10, 627
Chapter 11, 635
Chapter 12, 644
General Bibliography
652
Index
665

Preface
This edition provides a rather substantial addition to the material covered in
the first edition. The principal difference is the inclusion of three new
chapters, Chapters 10, 11, and 12, in addition to an appendix of solutions to
exercises.
Chapter 10 covers orthogonal polynomials, such as Legendre, Chebyshev,
Jacobi, Laguerre, and Hermite polynomials, and discusses their applications
in statistics. Chapter 11 provides a thorough coverage of Fourier series. The
presentation is done in such a way that a reader with no prior knowledge of
Fourier series can have a clear understanding of the theory underlying the
subject. Several applications of Fouries series in statistics are presented.
Chapter 12 deals with approximation of Riemann integrals. It gives an
exposition of methods for approximating integrals, including those that are
multidimensional. Applications of some of these methods in statistics
are discussed. This subject area has recently gained prominence in several
fields of science and engineering, and, in particular, Bayesian statistics. The
material should be helpful to readers who may be interested in pursuing
further studies in this area.
A significant addition is the inclusion of a major appendix that gives
detailed solutions to the vast majority of the exercises in Chapters 112. This
supplement was prepared in response to numerous suggestions by users of
the first edition. The solutions should also be helpful in getting a better
understanding of the various topics covered in the book.
In addition to the aforementioned material, several new exercises were
added to some of the chapters in the first edition. Chapter 1 was expanded by
the inclusion of some basic topological concepts. Chapter 9 was modified to
accommodate Chapter 10. The changes in the remaining chapters, 2 through
8, are very minor. The general bibliography was updated.
The choice of the new chapters was motivated by the evolution of the field
of statistics and the growing needs of statisticians for mathematical tools
beyond the realm of advanced calculus. This is certainly true in topics
concerning approximation of integrals and distribution functions, stochastic
xv

PREFACE
xvi
processes, time series analysis, and the modeling of periodic response func-
tions, to name just a few.
The book is self-contained. It can be used as a text for a two-semester
course in advanced calculus and introductory mathematical analysis. Chap-
ters 17 may be covered in one semester, and Chapters 812 in the other
semester. With its coverage of a wide variety of topics, the book can also
serve as a reference for statisticians, and others, who need an adequate
knowledge of mathematics, but do not have the time to wade through the
myriad mathematics books. It is hoped that the inclusion of a separate
section on applications in statistics in every chapter will provide a good
motivation for learning the material in the book. This represents a continua-
tion of the practice followed in the first edition.
As with the first edition, the book is intended as much for mathematicians
as for statisticians. It can easily be turned into a pure mathematics book by
simply omitting the section on applications in statistics in a given chapter.
Mathematicians, however, may find the sections on applications in statistics
to be quite useful, particularly to mathematics students seeking an interdisci-
plinary major. Such a major is becoming increasingly popular in many circles.
In addition, several topics are included here that are not usually found in a
typical advanced calculus book, such as approximation of functions and
integrals, Fourier series, and orthogonal polynomials. The fields of mathe-
matics and statistics are becoming increasingly intertwined, making any
separation of the two unpropitious. The book represents a manifestation of
the interdependence of the two fields.
The mathematics background needed for this edition is the same as for
the first edition. For readers interested in statistical applications, a back-
ground in introductory mathematical statistics will be helpful, but not abso-
lutely essential. The annotated bibliography in each chapter can be consulted
for additional readings.
I am grateful to all those who provided comments and helpful suggestions
concerning the first edition, and to my wife Ronnie for her help and support.
ANDRE I. KHURI
´
Gaines®ille, Florida

Preface to the First Edition
The most remarkable mathematical achievement of the seventeenth century
Ž
.
was the invention of calculus by Isaac Newton
16421727 and Gottfried
Ž
.
Wilhelm Leibniz
16461716 . It has since played a significant role in all
fields of science, serving as its principal quantitative language. There is hardly
any scientific discipline that does not require a good knowledge of calculus.
The field of statistics is no exception.
Advanced calculus has had a fundamental and seminal role in the devel-
opment of the basic theory underlying statistical methodology. With the rapid
growth of statistics as a discipline, particularly in the last three decades,
knowledge of advanced calculus has become imperative for understanding
the recent advances in this field. Students as well as research workers in
statistics are expected to have a certain level of mathematical sophistication
in order to cope with the intricacies necessitated by the emerging of new
statistical methodologies.
This book has two purposes. The first is to provide beginning graduate
students in statistics with the basic concepts of advanced calculus. A high
percentage of these students have undergraduate training in disciplines other
than mathematics with only two or three introductory calculus courses. They
are, in general, not adequately prepared to pursue an advanced graduate
degree in statistics. This book is designed to fill the gaps in their mathemati-
cal training and equip them with the advanced calculus tools needed in their
graduate work. It can also provide the basic prerequisites for more advanced
courses in mathematics.
One salient feature of this book is the inclusion of a complete section in
each chapter describing applications in statistics of the material given in the
chapter. Furthermore, a large segment of Chapter 8 is devoted to the
important problem of optimization in statistics. The purpose of these applica-
tions is to help motivate the learning of advanced calculus by showing its
relevance in the field of statistics. There are many advanced calculus books
designed for engineers or business majors, but there are none for statistics
xvii

PREFACE TO THE FIRST EDITION
xviii
majors. This is the first advanced calculus book to emphasize applications in
statistics.
The scope of this book is not limited to serving the needs of statistics
graduate students. Practicing statisticians can use it to sharpen their mathe-
matical skills, or they may want to keep it as a handy reference for their
research work. These individuals may be interested in the last three chapters,
particularly Chapters 8 and 9, which include a large number of citations of
statistical papers.
The second purpose of the book concerns mathematics majors. The book’s
thorough and rigorous coverage of advanced calculus makes it quite suitable
as a text for juniors or seniors. Chapters 1 through 7 can be used for this
purpose. The instructor may choose to omit the last section in each chapter,
which pertains to statistical applications. Students may benefit, however,
from the exposure to these additional applications. This is particularly true
given that the trend today is to allow the undergraduate student to have a
major in mathematics with a minor in some other discipline. In this respect,
the book can be particularly useful to those mathematics students who may
be interested in a minor in statistics.
Other features of this book include a detailed coverage of optimization
Ž
.
techniques and their applications in statistics Chapter 8 , and an introduc-
Ž
.
tion to approximation theory Chapter 9 . In addition, an annotated bibliog-
raphy is given at the end of each chapter. This bibliography can help direct
the interested reader to other sources in mathematics and statistics that are
relevant to the material in a given chapter. A general bibliography is
provided at the end of the book. There are also many examples and exercises
in mathematics and statistics in every chapter. The exercises are classified by
Ž
.
discipline mathematics and statistics for the benefit of the student and the
instructor.
The reader is assumed to have a mathematical background that is usually
obtained in the freshmansophomore calculus sequence. A prerequisite for
understanding the statistical applications in the book is an introductory
statistics course. Obviously, those not interested in such applications need
not worry about this prerequisite. Readers who do not have any background
in statistics, but are nevertheless interested in the application sections, can
make use of the annotated bibliography in each chapter for additional
reading.
The book contains nine chapters. Chapters 17 cover the main topics in
advanced calculus, while chapters 8 and 9 include more specialized subject
areas. More specifically, Chapter 1 introduces the basic elements of set
theory. Chapter 2 presents some fundamental concepts concerning vector
spaces and matrix algebra. The purpose of this chapter is to facilitate the
understanding of the material in the remaining chapters, particularly, in
Chapters 7 and 8. Chapter 3 discusses the concepts of limits and continuity of
functions. The notion of differentiation is studied in Chapter 4. Chapter 5
covers the theory of infinite sequences and series. Integration of functions is

PREFACE TO THE FIRST EDITION
xix
the theme of Chapter 6. Multidimensional calculus is introduced in Chapter
7. This chapter provides an extension of the concepts of limits, continuity,
Ž
differentiation, and integration to functions of several variables multivaria-
.
ble functions . Chapter 8 consists of two parts. The first part presents an
overview of the various methods of optimization of multivariable functions
whose optima cannot be obtained explicitly by standard advanced calculus
techniques. The second part discusses a variety of topics of interest to
statisticians. The common theme among these topics is optimization. Finally,
Chapter 9 deals with the problem of approximation of continuous functions
with polynomial and spline functions. This chapter is of interest to both
mathematicians and statisticians and contains a wide variety of applications
in statistics.
I am grateful to the University of Florida for granting me a sabbatical
leave that made it possible for me to embark on the project of writing this
book. I would also like to thank Professor Rocco Ballerini at the University
of Florida for providing me with some of the exercises used in Chapters, 3, 4,
5, and 6.
ANDRE I. KHURI
´
Gaines®ille, Florida


C H A P T E R
1
An Introduction to Set Theory
The origin of the modern theory of sets can be traced back to the Russian-born
Ž
.
German mathematician Georg Cantor 18451918 . This chapter introduces
the basic elements of this theory.
1.1. THE CONCEPT OF A SET
A set is any collection of well-defined and distinguishable objects. These
objects are called the elements, or members, of the set and are denoted by
lowercase letters. Thus a set can be perceived as a collection of elements
united into a single entity. Georg Cantor stressed this in the following words:
‘‘A set is a multitude conceived of by us as a one.’’
If x is an element of a set A, then this fact is denoted by writing xgA.
If, however, x is not an element of A, then we write xA. Curly brackets
are usually used to describe the contents of a set. For example, if a set A
consists of the elements x , x , . . . , x , then it can be represented as As
1
2
n

4
x , x , . . . , x . In the event membership in a set is determined by the
1
2
n
satisfaction of a certain property or a relationship, then the description of the
same can be given within the curly brackets. For example, if A consists of all
2
 
2
4
real numbers x such that x 1, then it can be expressed as As x x 1 ,

where the bar is used simply to mean ‘‘such that.’’ The definition of sets in
this manner is based on the axiom of abstraction, which states that given any
property, there exists a set whose elements are just those entities having that
property.
Definition 1.1.1.
The set that contains no elements is called the empty set
and is denoted by .

Definition 1.1.2.
A set A is a subset of another set B, written symboli-
cally as A;B, if every element of A is an element of B. If B contains at
least one element that is not in A, then A is said to be a proper subset of B.

1

AN INTRODUCTION TO SET THEORY
2
Definition 1.1.3.
A set A and a set B are equal if A;B and B;A.
Thus, every element of A is an element of B and vice versa.

Definition 1.1.4.
The set that contains all sets under consideration in a
certain study is called the universal set and is denoted by .

1.2. SET OPERATIONS
There are two basic operations for sets that produce new sets from existing
ones. They are the operations of union and intersection.
Definition 1.2.1.
The union of two sets A and B, denoted by AjB, is
the set of elements that belong to either A or B, that is,


4
AjBs x xgA or xgB .

This definition can be extended to more than two sets. For example, if
A , A , . . . , A are n given sets, then their union, denoted by  n
A , is a set
1
2
n
is1
i
such that x is an element of it if and only if x belongs to at least one of the
Ž
.
A
is1, 2, . . . , n .
i
Definition 1.2.2.
The intersection of two sets
A and B, denoted by
AlB, is the set of elements that belong to both A and B. Thus


4
AlBs x xgA and xgB .

This definition can also be extended to more than two sets. As before, if
A , A , . . . , A
are n given sets, then their intersection, denoted by  n
A ,
1
2
n
is1
i
Ž
.
is the set consisting of all elements that belong to all the A
is1, 2, . . . , n .
i
Definition 1.2.3.
Two sets A and B are disjoint if their intersection is the
empty set, that is, AlBs.

Definition 1.2.4.
The complement of a set A, denoted by A, is the set
consisting of all elements in the universal set that do not belong to A. In
other words, xgA if and only if xA.
The complement of A with respect to a set B is the set ByA which
consists of the elements of B that do not belong to A. This complement is
called the relative complement of A with respect to B.

From Definitions 1.1.11.1.4 and 1.2.11.2.4, the following results can be
concluded:
RESULT 1.2.1.
The empty set  is a subset of every set. To show this,
suppose that A is any set. If it is false that ;A, then there must be an

SET OPERATIONS
3
element in  which is not in A. But this is not possible, since  is empty. It
is therefore true that ;A.
RESULT 1.2.2.
The empty set  is unique. To prove this, suppose that 1
and 
are two empty sets. Then, by the previous result,  ;
and
2
1
2
 G . Hence,  s .
2
1
1
2
RESULT 1.2.3.
The complement of  is . Vice versa, the complement
of  is .
RESULT 1.2.4.
The complement of A is A.
RESULT 1.2.5.
For any set A, AjAs and AlAs.
RESULT 1.2.6.
AyBsAyAlB.
Ž
.
Ž
.
RESULT 1.2.7.
Aj BjC s AjB jC.
Ž
.
Ž
.
RESULT 1.2.8.
Al BlC s AlB lC.
Ž
.
Ž
.
Ž
.
RESULT 1.2.9.
Aj BlC s AjB l AjC .
Ž
.
Ž
.
Ž
.
RESULT 1.2.10.
Al BjC s AlB j AlC .
n
n
RESULT 1.2.11.
AjB sAlB. More generally, 
A s
A .
Ž
.
is1
i
is1
i
n
n
RESULT 1.2.12.
AlB sAjB. More generally, 
A s
A .
Ž
.
is1
i
is1
i
Definition 1.2.5.
Let
A and B be two sets. Their Cartesian product,
Ž
.
denoted by AB, is the set of all ordered pairs
a, b such that agA and
bgB, that is,

ABs
a, b
agA and bgB .

4
Ž
.
The word ‘‘ordered’’ means that if a and c are elements in A and b and d
Ž
.
Ž
.
are elements in B, then a, b s c, d if and only if asc and bsd.

The preceding definition can be extended to more than two sets. For
example, if A , A , . . . , A
are n given sets, then their Cartesian product is
1
2
n
denoted by n
A and defined by
i
is1
n
A s
a , a , . . . , a
a gA , is1, 2, . . . , n .

4
Ž
.

i
1
2
n
i
i
is1

AN INTRODUCTION TO SET THEORY
4
Ž
.
Here,
a , a , . . . , a , called an ordered n-tuple, represents a generaliza-
1
2
n
tion of the ordered pair. In particular, if the
A
are equal to
A for
i
is1, 2, . . . , n, then one writes An for n
A.
is1
The following results can be easily verified:
RESULT 1.2.13.
ABs if and only if As or Bs.
Ž
.
Ž
.
Ž
.
RESULT 1.2.14.
AjB Cs AC j BC .
Ž
.
Ž
.
Ž
.
RESULT 1.2.15.
AlB Cs AC l BC .
Ž
.
Ž
.
Ž
.
Ž
.
RESULT 1.2.16.
AB l CD s AlC  BlD .
1.3. RELATIONS AND FUNCTIONS
Let AB be the Cartesian product of two sets, A and B.
Definition 1.3.1.
A relations  from A to B is a subset of AB, that is,
Ž
.
 consists of ordered pairs a, b such that agA and bgB. In particular, if
AsB, then  is said to be a relation in A.

4

4
Ž
.
For example, if As 7, 8, 9
and Bs 7, 8, 9, 10 , then s
a, b ab,
4
agA, bgB is a relation from A to B that consists of the six ordered pairs
Ž
. Ž
. Ž
. Ž
. Ž
.
Ž
.
7, 8 , 7, 9 , 7, 10 , 8, 9 , 8, 10 , and 9, 10 .
Ž
.
Whenever  is a relation and
x, y g, then x and y are said to be
-related. This is denoted by writing x  y.

Definition 1.3.2.
A relation  in a set A is an equivalence relation if the
following properties are satisfied:
1.  is reflexive, that is, a a for any a in A.
2.  is symmetric, that is, if a b, then b a for any a, b in A.
3.  is transitive, that is, if a b and b c, then a c for any a, b, c in A.
If  is an equivalence relation in a set A, then for a given a
in A, the set
0

C a
s agA a  a ,

4
Ž
.
0
0
which consists of all elements of A that are -related to a , is called an
0
equivalence class of a .

0
Ž .
RESULT 1.3.1.
agC a for any a in A. Thus each element of A is an
element of an equivalence class.

RELATIONS AND FUNCTIONS
5
Ž
.
Ž
.
RESULT 1.3.2.
If C a
and C a
are two equivalence classes, then
1
2
Ž
.
Ž
.
Ž
.
Ž
.
either C a
sC a , or C a
and C a
are disjoint subsets.
1
2
1
2
It follows from Results 1.3.1 and 1.3.2 that if A is a nonempty set, the
collection of distinct -equivalence classes of A forms a partition of A.
As an example of an equivalence relation, consider that a  b if and only if
a and b are integers such that ayb is divisible by a nonzero integer n. This
is the relation of congruence modulo n in the set of integers and is written
Ž
.
Ž
.
symbolically as ab
mod n . Clearly, aa
mod n , since ayas0 is
Ž
.
Ž
.
divisible by n. Also, if ab mod n , then ba mod n , since if ayb is
Ž
.
divisible by n, then so is bya. Furthermore, if ab
mod n
and bc
Ž
.
Ž
.
mod n , then ac mod n . This is true because if ayb and byc are both
Ž
.
Ž
.
divisible by n, then so is
ayb q byc sayc. Now, if a
is a given
0
integer, then a -equivalence class of a
consists of all integers that can be
0
Ž
.
written as asa qkn, where k is an integer. This in this example C a
is
0
0


4
the set a qkn kgJ , where J denotes the set of all integers.
0
Definition 1.3.3.
Let  be a relation from A to B. Suppose that  has
the property that for all x in A, if x y and x z, where y and z are elements
in B, then ysz. Such a relation is called a function.

Thus a function is a relation  such that any two elements in B that are
-related to the same x in A must be identical. In other words, to each
element x in A, there corresponds only one element y in B. We call y the
Ž .
value of the function at x and denote it by writing ysf x . The set A is
Ž .
called the domain of the function f, and the set of all values of f x for x in
A is called the range of f, or the image of A under f, and is denoted by
Ž
.
f A . In this case, we say that f is a function, or a mapping, from A into B.
Ž
.
We express this fact by writing f: A™B. Note that f A is a subset of B. In
Ž
.
particular, if Bsf A , then f is said to be a function from A onto B. In this
case, every element b in B has a corresponding element a in A such that
Ž .
bsf a .
Definition 1.3.4.
A function
f defined on a set
A is said to be a
Ž
.
Ž
.
one-to-one function if whenever f x
sf x
for x , x
in
A, one has
1
2
1
2
x sx . Equivalently, f is a one-to-one function if whenever x x , one has
1
2
1
2
Ž
.
Ž
.
f x
f x
.

1
2
Ž
.
Thus a function f: A™B is one-to-one if to each y in f A , there
Ž .
corresponds only one element x in A such that ysf x . In particular, if f is
a one-to-one and onto function, then it is said to provide a one-to-one
correspondence between A and B. In this case, the sets A and B are said to
be equivalent. This fact is denoted by writing AB.
Note that whenever AB, there is a function g: B™A such that if
Ž .
Ž .
ysf x , then xsg y . The function g is called the inverse function of f and

AN INTRODUCTION TO SET THEORY
6
is denoted by fy1. It is easy to see that
AB defines an equivalence
relation. Properties 1 and 2 in Definition 1.3.2 are obviously true here. As for
property 3, if A, B, and C are sets such that AB and BC, then AC.
To show this, let f: A™B and h: B™C be one-to-one and onto functions.
Ž .
w Ž .x
Then, the composite function h f, where h f x sh f x , defines a one-
to-one correspondence between A and C.
EXAMPLE 1.3.1.
The relation a b, where a and b are real numbers such
2
Ž
.
that asb , is not a function. This is true because both pairs
a, b
and
Ž
.
a,yb belong to .
EXAMPLE 1.3.2.
The relation a b, where a and b are real numbers such
that bs2a2q1, is a function, since for each a, there is only one b that is
-related to a.
 
4
 
4
EXAMPLE 1.3.3.
Let
A s x y1 F x F 1 , B s x 0 F x F 2 .
Define
Ž .
2
f: A™B such that f x sx . Here, f is a function, but is not one-to-one
Ž .
Ž
.
because f 1 sf y1 s1. Also, f does not map A onto B, since ys2 has no
corresponding x in A such that x 2s2.
EXAMPLE 1.3.4.
Consider the relation x y, where ysarcsin x, y1F
xF1. Here, y is an angle measured in radians whose sine is x. Since there
are infinitely many angles with the same sine,  is not a function. However, if
 
4
we restrict the range of y to the set Bs y yr2FyFr2 , then 
becomes a function, which is also one-to-one and onto. This function is the
inverse of the sine function xssin y. We refer to the values of y that belong
to the set B as the principal values of arcsin x, which we denote by writing
ysArcsin x. Note that other functions could have also been defined from
the arcsine relation. For example, if r2FyF3r2, then xssin ysysin z,
where zsyy. Since yr2FzFr2, then zsyArcsin x. Thus ys
 
4
yArcsin x maps the set As x y1FxF1 in a one-to-one manner onto
 
4
the set Cs y r2FyF3r2 .
1.4. FINITE, COUNTABLE, AND UNCOUNTABLE SETS

4
Let J s 1, 2, . . . , n be a set consisting of the first n positive integers, and let
n
Jq denote the set of all positive integers.
Definition 1.4.1.
A set A is said to be:
1. Finite if AJ
for some positive integer n.
n
2. Countable if AJq. In this case, the set Jq, or any other set equiva-
lent to it, can be used as an index set for A, that is, the elements of A
Ž
.
q
are assigned distinct indices
subscripts
that belong to J . Hence,

4
A can be represented as As a , a , . . . , a , . . . .
1
2
n

FINITE, COUNTABLE, AND UNCOUNTABLE SETS
7
3. Uncountable if A is neither finite nor countable. In this case, the
elements of A cannot be indexed by J
for any n, or by Jq.

n

2
4
EXAMPLE 1.4.1.
Let As 1, 4, 9, . . . , n , . . . . This set is countable, since
q
Ž .
2
the function f: J ™A defined by f n sn
is one-to-one and onto. Hence,
AJq.
EXAMPLE 1.4.2.
Let AsJ be the set of all integers. Then A is count-
able. To show this, consider the function f: Jq™A defined by
nq1 r2,
n odd,
Ž
.
f n s
Ž . ½ 2yn r2,
n even.
Ž
.
It can be verified that f is one-to-one and onto. Hence, AJq.
 
4
EXAMPLE 1.4.3.
Let As x 0FxF1 . This set is uncountable. To show
this, suppose that there exists a one-to-one correspondence between Jq and

4
A. We can then write As a , a , . . . , a , . . . . Let the digit in the nth decimal
1
2
n
Ž
.
place of a be denoted by b
ns1, 2, . . . . Define a number c as cs0c c
n
n
1
2
 c  such that for each n, c s1 if b 1 and c s2 if b s1. Now, c
n
n
n
n
n
belongs to A, since 0FcF1. However, by construction, c is different from
Ž
.
every a in at least one decimal digit is1, 2, . . . and hence cA, which is a
i
contradiction. Therefore, A is not countable. Since A is not finite either,
then it must be uncountable.
This result implies that any subset of R, the set of real numbers, that
contains A, or is equivalent to it, must be uncountable. In particular, R is
uncountable.
Theorem 1.4.1.
Every infinite subset of a countable set is countable.
Proof. Let A be a countable set, and B be an infinite subset of A. Then

4
As a , a , . . . , a , . . . , where the a ’s are distinct elements. Let n
be the
1
2
n
i
1
smallest positive integer such that a gB. Let n n be the next smallest
n
2
1
1
integer such that a
gB. In general, if n n   n
have been
n
1
2
ky1
2
chosen, let n
be the smallest integer greater than n
such that a
gB.
k
ky1
nk
q
Ž .
Define the function f: J ™B such that f k sa , ks1, 2, . . . . This func-
nk
tion is one-to-one and onto. Hence, B is countable.

Theorem 1.4.2.
The union of two countable sets is countable.
Proof. Let A and B be countable sets. Then they can be represented as

4

4
As a , a , . . . , a , . . . , Bs b , b , . . . , b , . . . . Define CsAjB. Consider
1
2
n
1
2
n
the following two cases:
i. A and B are disjoint.
ii. A and B are not disjoint.

AN INTRODUCTION TO SET THEORY
8

4
In case i, let us write C as Cs a , b , a , b , . . . , a , b , . . . . Consider the
1
1
2
2
n
n
function f: Jq™C such that
a
,
n odd,
Žnq1. r2
f n s
Ž . ½ b
,
n even.
nr2
It can be verified that f is one-to-one and onto. Hence, C is countable.
Let us now consider case ii. If AlB, then some elements of C,
namely those in AlB, will appear twice. Hence, there exists a set E;Jq
such that EC. Thus C is either finite or countable. Since C>A and A is
infinite, C must be countable.

Corollary 1.4.1.
If A , A , . . . , A , . . . , are countable sets, then 
A
1
2
n
is1
i
is countable.
Proof. The proof is left as an exercise.

Theorem 1.4.3.
Let A and B be two countable sets. Then their Cartesian
product AB is countable.
Ž
4
Proof. Let us write
A as
As a , a , . . . , a , . . . . For a given agA,
1
2
n
Ž
.
define
a, B as the set

a, B s
a, b
bgB .

4
Ž
.
Ž
.
Ž
.
Ž
.
Then a, B B and hence a, B is countable.
However,

ABs
a , B .
Ž
.

i
is1
Thus by Corollary 1.4.1, AB is countable.

Corollary 1.4.2.
If A , A , . . . , A
are countable sets, then their Carte-
1
2
n
sian product n
A is countable.
i
is1
Proof. The proof is left as an exercise.

Corollary 1.4.3.
The set Q of all rational numbers is countable.
Proof. By definition, a rational number is a number of the form mrn,
˜
where m and n are integers with n0. Thus QQ, where
˜

Qs
m, n
m, n are integers and n0 .

4
Ž
.

BOUNDED SETS
9
˜
Since Q is an infinite subset of JJ, where J is the set of all integers, which
is countable as was seen in Example 1.4.2, then by Theorems 1.4.1 and 1.4.3,
˜
Q is countable and so is Q.

REMARK 1.4.1.
Any real number that cannot be expressed as a rational
'
number is called an irrational number. For example,
2 is an irrational
number. To show this, suppose that there exist integers, m and n, such that
'2 smrn. We may consider that mrn is written in its lowest terms, that is,
m and n have no common factors other than unity. In particular, m and n,
cannot both be even. Now, m2s2n2. This implies that m2 is even. Hence, m
is even and can therefore be written as ms2m. It follows that n2sm2r2s
2m2. Consequently, n2, and hence n, is even. This contradicts the fact that
'
m and n are not both even. Thus
2 must be an irrational number.
1.5. BOUNDED SETS
Let us consider the set R of real numbers.
Definition 1.5.1.
A set A;R is said to be:
1. Bounded from above if there exists a number q such that xFq for all
x in A. This number is called an upper bound of A.
2. Bounded from below if there exists a number p such that xGp for all
x in A. The number p is called a lower bound of A.
3. Bounded if A has an upper bound q and a lower bound p. In this case,
there exists a nonnegative number r such that yrFxFr for all x in
Ž    .
A. This number is equal to max p , q .

Definition 1.5.2.
Let A;R be a set bounded from above. If there exists
a number l that is an upper bound of A and is less than or equal to any
other upper bound of A, then l is called the least upper bound of A and is
Ž
.
Ž
.
denoted by lub A . Another name for lub A is the supremum of A and is
Ž
.
denoted by sup A .

Definition 1.5.3.
Let A;R be a set bounded from below. If there exists
a number g that is a lower bound of A and is greater than or equal to any
other lower bound of A, then g is called the greatest lower bound and is
Ž
.
Ž
.
denoted by glb A . The infimum of A, denoted by inf A , is another name
Ž
.
for glb A .

The least upper bound of A, if it exists, is unique, but it may or may not
Ž
.
belong to A. The same is true for glb A . The proof of the following theorem
Ž
.
is omitted and can be found in Rudin 1964, Theorem 1.36 .

AN INTRODUCTION TO SET THEORY
10
Theorem 1.5.1.
Let A;R be a nonempty set.
Ž
.
1. If A is bounded from above, then lub A exists.
Ž
.
2. If A is bounded from below, then glb A exists.
 
4
Ž
.
EXAMPLE 1.5.1.
Let
As x x0 . Then lub A s0, which does not
belong to A.


4
Ž
.
Ž
.
EXAMPLE 1.5.2.
Let As 1rn ns1, 2, . . . . Then lub A s1 and glb A
Ž
.
Ž
.
s0. In this case, lub A belongs to A, but glb A does not.
1.6. SOME BASIC TOPOLOGICAL CONCEPTS
The field of topology is an abstract study that evolved as an independent
discipline in response to certain problems in classical analysis and geometry.
It provides a unifying theory that can be used in many diverse branches of
mathematics. In this section, we present a brief account of some basic
definitions and results in the so-called point-set topology.

4
Definition 1.6.1.
Let A be a set, and let FFs B
be a family of subsets

of A. Then FF is a topology in A if it satisfies the following properties:
1. The union of any number of members of FF is also a member of FF.
2. The intersection of a finite number of members of FF is also a member
of FF.
3. Both A and the empty set  are members of FF.

Ž
.
Definition 1.6.2.
Let FF be a topology in a set A. Then the pair
A, FF
is
called a topological space.

Ž
.
Definition 1.6.3.
Let
A, FF be a topological space. Then the members of
FF are called the open sets of the topology FF.

Ž
.
Definition 1.6.4.
Let
A, FF
be a topological space. A neighborhood of a
Ž
.
point pgA is any open set that is, a member of FF
that contains p. In
particular, if AsR, the set of real numbers, then a neighborhood of pgR
Ž .
  

4
is an open set of the form N
p s q
qyp r for some r0.

r
Ž
.

4
Definition 1.6.5.
Let
A, FF be a topological space. A family Gs B
;FF

Ž
.
is called a basis for FF if each open set that is, member of FF
is the union of
members of G.

On the basis of this definition, it is easy to prove the following theorem.

SOME BASIC TOPOLOGICAL CONCEPTS
11
Ž
.
Theorem 1.6.1.
Let
A, FF
be a topological space, and let G be a basis
Ž
.
for FF. Then a set B;A is open that is, a member of FF
if and only if for
each pgB, there is a UgG such that pgU;B.

Ž .
4
For example, if AsR, then Gs N
p pgR, r0 is a basis for the
r
topology in R. It follows that a set B;R is open if for every point p in B,
Ž .
Ž .
there exists a neighborhood N
p such that N
p ;B.
r
r
Ž
.
Definition 1.6.6.
Let
A, FF
be a topological space. A set B;A is closed
if B, the complement of B with respect to A, is an open set.

Ž
.
It is easy to show that closed sets of a topological space
A, FF
satisfy the
following properties:
1. The intersection of any number of closed sets is closed.
2. The union of a finite number of closed sets is closed.
3. Both A and the empty set  are closed.
Ž
.
Definition 1.6.7.
Let
A, FF
be a topological space. A point pgA is said
to be a limit point of a set B;A if every neighborhood of p contains at least
Ž .
one element of B distinct from p. Thus, if U p is any neighborhood of p,
Ž .
then U p lB is a nonempty set that contains at least one element besides
p. In particular, if AsR, the set of real numbers, then p is a limit point of a
Ž .
w
 4x
 4
set B;R if for any r0, N
p l By p , where
p denotes a set
r
consisting of just p.

Theorem 1.6.2.
Let
p be a limit point of a set B;R. Then every
neighborhood of p contains infinitely many points of B.
Proof. The proof is left to the reader.

The next theorem is a fundamental theorem in set theory. It is originally
Ž
.
due to Bernhard Bolzano
17811848 , though its importance was first
Ž
.
recognized by Karl Weierstrass 18151897 . The proof is omitted and can be
Ž
.
found, for example, in Zaring 1967, Theorem 4.62 .
Ž
.
Theorem 1.6.3 BolzanoWeierstrass .
Every bounded infinite subset of
R, the set of real numbers, has at least one limit point.
Note that a limit point of a set B may not belong to B. For example, the


4
set Bs 1rn ns1, 2, . . .
has a limit point equal to zero, which does not
belong to B. It can be seen here that any neighborhood of 0 contains
infinitely many points of B. In particular, if r is a given positive number, then
Ž .
all elements of B of the form 1rn, where n1rr, belong to N 0 . From
r
Theorem 1.6.2 it can also be concluded that a finite set cannot have limit
points.

AN INTRODUCTION TO SET THEORY
12
Limit points can be used to describe closed sets, as can be seen from the
following theorem.
Theorem 1.6.4.
A set B is closed if and only if every limit point of B
belongs to B.
Proof. Suppose that B is closed. Let p be a limit point of B. If pB,
Ž .
then pgB, which is open. Hence, there exists a neighborhood U p of p
Ž .
contained inside B by Theorem 1.6.1. This means that U p lBs, a
Ž
.
contradiction, since p is a limit point of B see Definition 1.6.7 . Therefore,
p must belong to B. Vice versa, if every limit point of B is in B, then B must
be closed. To show this, let p be any point in B. Then, p is not a limit point
Ž .
Ž .
of B. Therefore, there exists a neighborhood U p such that U p ;B. This
means that B is open and hence B is closed.

It should be noted that a set does not have to be either open or closed; if
it is closed, it does not have to be open, and vice versa. Also, a set may be
both open and closed.
 
4
EXAMPLE 1.6.1.
Bs x 0x1
is an open subset of R, but is not
closed, since both 0 and 1 are limit points of B, but do not belong to it.
 
4
EXAMPLE 1.6.2.
Bs x 0FxF1
is closed, but is not open, since any
neighborhood of 0 or 1 is not contained in B.
 
4
EXAMPLE 1.6.3.
Bs x 0xF1 is not open, because any neighborhood
of 1 is not contained in B. It is also not closed, because 0 is a limit point that
does not belong to B.
EXAMPLE 1.6.4.
The set R is both open and closed.
EXAMPLE 1.6.5.
A finite set is closed because it has no limit points, but is
obviously not open.
Ž
.
Definition 1.6.8.
A subset B of a topological space
A, FF is disconnected
if there exist open subsets C and D of A such that BlC and BlD are
disjoint nonempty sets whose union is B. A set is connected if it is not
disconnected.

'
 
4
The set of all rationals Q is disconnected, since
x x 2 lQ and
'
 
4
x x 2 lQ are disjoint nonempty sets whose union is Q. On the other
Ž
.
hand, all intervals in R open, closed, or half-open are connected.

4
Definition 1.6.9.
A collection of sets B
is said to be a co®ering of a set


4
A if the union  B
contains A. If each B
is an open set, then
B
is




called an open co®ering.

EXAMPLES IN PROBABILITY AND STATISTICS
13
Definition 1.6.10.
A set A in a topological space is compact if each open

4
covering
B
of
A has a finite subcovering, that is, there is a finite


4
n
subcollection B , B , . . . , B
of B
such that A;
B .





is1

1
2
n
i
The concept of compactness is motivated by the classical HeineBorel
theorem, which characterizes compact sets in R, the set of real numbers, as
closed and bounded sets.
Ž
.
Theorem 1.6.5 HeineBorel .
A set B;R is compact if and only if it is
closed and bounded.
Ž
.
Proof. See, for example, Zaring 1967, Theorem 4.78 .

Thus, according to the HeineBorel theorem, every closed and bounded
w
x
interval a, b is compact.
1.7. EXAMPLES IN PROBABILITY AND STATISTICS
EXAMPLE 1.7.1.
In probability theory, events are considered as subsets in
a sample space , which consists of all the possible outcomes of an experi-
Ž
.
ment. A Borel field of events also called a 	-field in  is a collection B
B of
events with the following properties:
i. gB
B.
ii. If EgB
B, then EgB
B, where E is the complement of E.
iii. If E , E , . . . , E , . . . is a countable collection of events in B
B, then
1
2
n

E belongs to B
B.
is1
i
Ž
.
The probability of an event E is a number denoted by P E
that has the
following properties:
Ž
.
i. 0FP E F1.
Ž
.
ii. P  s1.
iii. If E , E , . . . , E , . . . is a countable collection of disjoint events in B
B,
1
2
n
then


P
E
s
P E
.
Ž
.

Ý
i
i
ž
/
is1
is1
Ž
.
By definition, the triple , B
B, P is called a probability space.
EXAMPLE 1.7.2 .
A random variable X defined on a probability space
Ž
.
, B
B, P
is a function X: ™A, where A is a nonempty set of real


Ž
.
4
numbers. For any real number x, the set
Es 
g X 
 Fx
is an

AN INTRODUCTION TO SET THEORY
14
element of
B
B. The probability of the event E is called the cumulative
Ž .
distribution function of X and is denoted by F x . In statistics, it is custom-
Ž
.
ary to write just X instead of X 
 . We thus have
F x sP XFx .
Ž .
Ž
.
This concept can be extended to several random variables: Let X , X , . . . , X
1
2
n


Ž
.
4
be n random variables. Define the event
A s 
g X 
 Fx , is
i
i
i
Ž
n
.
1, 2, . . . , n. Then, P 
A , which can be expressed as
is1
i
F x , x , . . . , x
sP X Fx , X Fx , . . . , X Fx
,
Ž
.
Ž
.
1
2
n
1
1
2
2
n
n
is called the joint cumulative distribution function of X , X , . . . , X . In this
1
2
n
Ž
.
case, the n-tuple
X , X , . . . , X
is said to have a multivariate distribution.
1
2
n
A random variable
X is said to be discrete, or to have a discrete
distribution, if its range is finite or countable. For example, the binomial
random variable is discrete. It represents the number of successes in a
sequence of n independent trials, in each of which there are two possible
outcomes: success or failure. The probability of success, denoted by p , is the
n
same in all the trials. Such a sequence of trials is called a Bernoulli sequence.
Thus the possible values of this random variable are 0, 1, . . . , n.
Another example of a discrete random variable is the Poisson, whose
possible values are 0, 1, 2, . . . . It is considered to be the limit of a binomial
random variable as n™ in such a way that np ™0. Other examples of
n
discrete random variables include the discrete uniform, geometric, hypergeo-
Ž
metric, and negative binomial
see, for example, Fisz, 1963; Johnson and
.
Kotz, 1969; Lindgren 1976; Lloyd, 1980 .
A random variable X is said to be continuous, or to have a continuous
distribution, if its range is an uncountable set, for example, an interval. In
Ž .
this case, the cumulative distribution function F x
of X is a continuous
Ž .
function of x on the set R of all real numbers. If, in addition, F x
is
differentiable, then its derivative is called the density function of X. One of
the best-known continuous distributions is the normal. A number of continu-
ous distributions are derived in connection with it, for example, the chi-
squared, F, Rayleigh, and t distributions. Other well-known continuous
distributions include the beta, continuous uniform, exponential, and gamma
Ž
.
distributions see, for example, Fisz, 1963; Johnson and Kotz, 1970a, b .
Ž
.
EXAMPLE 1.7.3.
Let f x, 
denote the density function of a continuous
random variable X, where  represents a set of unknown parameters that
identify the distribution of X. The range of X, which consists of all possible
values of X, is referred to as a population and denoted by P . Any subset of
X
n elements from P
forms a sample of size n. This sample is actually an
X
element in the Cartesian product P n. Any real-valued function defined on
X
n
Ž
.
P
is called a statistic. We denote such a function by g X , X , . . . , X ,
X
1
2
n
where each X has the same distribution as X. Note that this function is a
i
random variable whose values do not depend on . For example, the sample
n
2
n
2
Ž
.
Ž
.
mean XsÝ
X rn and the sample variance S sÝ
X yX
r ny1
is1
i
is1
i

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
15
are statistics. We adopt the convention that whenever a particular sample of
Ž
.
size n is chosen
or observed
from P , the elements in that sample are
X
written using lowercase letters, for example, x , x , . . . , x . The correspond-
1
2
n
Ž
.
ing value of a statistic is written as g x , x , . . . , x
.
1
2
n
EXAMPLE 1.7.4.
Two random variables, X and Y, are said to be equal in
distribution if they have the same cumulative distribution function. This fact
d
is denoted by writing XsY. The same definition applies to random variables
d
with multivariate distributions. We note that s is an equivalence relation,
since it satisfies properties 1, 2, and 3 in Definition 1.3.2. The first two
d
d
properties are obviously true. As for property 3, if XsY and YsZ, then
d
XsZ, which implies that all three random variables have the same cumula-
tive distribution function. This equivalence relation is useful in nonparamet-
Ž
.
ric statistics
see Randles and Wolfe, 1979 . For example, it can be shown
that if X has a distribution that is symmetric about some number , then
d
XysyX. Also, if X , X , . . . , X
are independent and identically dis-
1
2
n
Ž
.
tributed random variables, and if m , m , . . . , m
is any permutation of the
1
2
n
d
Ž
.
Ž
.
Ž
.
n-tuple 1, 2, . . . , n , then
X , X , . . . , X
s X
, X
, . . . , X
. In this case,
1
2
n
m
m
m
1
2
n
we say that the collection of random variables X , X , . . . , X
is exchange-
1
2
n
able.
EXAMPLE 1.7.5.
Consider the problem of testing the null hypothesis H :
0
F
versus the alternative hypothesis H :  , where  is some un-
0
a
0
known parameter that belongs to a set A. Let T be a statistic used in making
a decision as to whether H
should be rejected or not. This statistic is
0
appropriately called a test statistic.
Suppose that H
is rejected if Tt, where t is some real number. Since
0
Ž
.
the distribution of T depends on , then the probability P Tt
is a
Ž .
w
x
function of , which we denote by   . Thus  : A™0,1 . Let B
be a
0


4
subset of A defined as B s gA F
. By definition, the size of the test
0
0
Ž
.
is the least upper bound of the set  B . This probability is denoted by 
0
and is also called the level of significance of the test. We thus have
s sup   .
Ž .
F 0
To learn more about the above examples and others, the interested reader
may consider consulting some of the references listed in the annotated
bibliography.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Ž
Bronshtein, I. N., and K. A. Semendyayev 1985 . Handbook of Mathematics English
.
Ž
translation edited by K. A. Hirsch . Van Nostrand Reinhold, New York. Section
4.1 in this book gives basic concepts of set theory; Chap. 5 provides a brief
.
introduction to probability and mathematical statistics.

AN INTRODUCTION TO SET THEORY
16
Ž
.
Ž
Dugundji, J. 1966 . Topology. Allyn and Bacon, Boston. Chap. 1 deals with elemen-
tary set theory; Chap. 3 presents some basic topological concepts that comple-
.
ments the material given in Section 1.6.
Ž
.
Fisz, M.
1963 . Probability Theory and Mathematical Statistics, 3rd ed. Wiley, New
Ž
York. Chap. 1 discusses random events and axioms of the theory of probability;
Chap. 2 introduces the concept of a random variable; Chap. 5 investigates some
.
probability distributions.
Ž
.
Hardy, G. H. 1955 . A Course of Pure Mathematics, 10th ed. The University Press,
Ž
Cambridge, England. Chap. 1 in this classic book is recommended reading for
.
understanding the real number system.
Ž
.
Harris, B.
1966 . Theory of Probability. Addison-Wesley, Reading, Massachusetts.
ŽChaps. 2 and 3 discuss some elementary concepts in probability theory as well as
.
in distribution theory. Many exercises are provided.
Ž
.
Hogg, R. V., and A. T. Craig 1965 . Introduction to Mathematical Statistics, 2nd ed.
Ž
Macmillan, New York. Chap. 1 is an introduction to distribution theory; exam-
ples of some special distributions are given in Chap. 3; Chap. 10 considers some
.
aspects of hypothesis testing that pertain to Example 1.7.5.
Ž
.
Johnson, N. L., and S. Kotz 1969 . Discrete Distributions. Houghton Mifflin, Boston.
ŽThis is the first volume in a series of books on statistical distributions. It is an
excellent source for getting detailed accounts of the properties and uses of these
distributions. This volume deals with discrete distributions, including the bino-
mial in Chap. 3, the Poisson in Chap. 4, the negative binomial in Chap. 5, and the
.
hypergeometric in Chap. 6.
Ž
.
Johnson, N. L., and S. Kotz 1970a . Continuous Uni®ariate Distributions1. Houghton
Ž
Mifflin, Boston. This volume covers continuous distributions, including the nor-
mal in Chap. 13, lognormal in Chap. 14, Cauchy in Chap. 16, gamma in Chap. 17,
.
and the exponential in Chap. 18.
Ž
.
Johnson, N. L., and S. Kotz 1970b . Continuous Uni®ariate Distributions2. Houghton
Ž
Mifflin, Boston.
This is a continuation of Vol. 2 on continuous distributions.
Chaps. 24, 25, 26, and 27 discuss the beta, continuous uniforms, F, and t
.
distributions, respectively.
Ž
.
Johnson, P. E. 1972 . A History of Set Theory. Prindle, Weber, and Schmidt, Boston.
ŽThis book presents a historical account of set theory as was developed by Georg
.
Cantor.
Ž
.
Ž
Lindgren, B. W. 1976 . Statistical Theory, 3rd ed. Macmillan, New York. Sections 1.1,
1.2, 2.1, 3.1, 3.2, and 3.3 present introductory material on probability models and
.
distributions; Chap. 6 discusses test of hypothesis and statistical inference.
Ž
.
Lloyd, E.
1980 . Handbook of Applicable Mathematics, Vol. II. Wiley, New York.
ŽThis is the second volume in a series of six volumes designed as texts of
mathematics for professionals. Chaps. 1, 2, and 3 present expository material on
.
probability; Chaps. 4 and 5 discuss random variables and their distributions.
Ž
.
Randles, R. H., and D. A. Wolfe 1979 . Introduction to the Theory of Nonparametric
Ž
Statistics. Wiley, New York.
Section 1.3 in this book discusses the ‘‘equal in
.
distribution’’ property mentioned in Example 1.7.4.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis, 2nd ed. McGraw-Hill, New
Ž
York. Chap. 1 discusses the real number system; Chap. 2 deals with countable,
.
uncountable, and bounded sets and pertains to Sections 1.4, 1.5, and 1.6.

EXERCISES
17
Ž
.
Ž
Stoll, R. R. 1963 . Set Theory and Logic. W. H. Freeman, San Francisco. Chap. 1 is
an introduction to set theory; Chap. 2 discusses countable sets; Chap. 3 is useful
.
in understanding the real number system.
Ž
.
Tucker, H. G. 1962 . Probability and Mathematical Statistics. Academic Press, New
Ž
York. Chaps. 1, 3, 4, and 6 discuss basic concepts in elementary probability and
.
distribution theory.
Ž
.
Ž
Vilenkin, N. Y.
1968 . Stories about Sets. Academic Press, New York. This is an
interesting book that presents various notions of set theory in an informal and
delightful way. It contains many unusual stories and examples that make the
.
learning of set theory rather enjoyable.
Ž
.
Ž
Zaring, W. M. 1967 . An Introduction to Analysis. Macmillan, New York. Chap. 2
.
gives an introduction to set theory; Chap. 3 discusses functions and relations.
EXERCISES
In Mathematics
1.1. Verify Results 1.2.31.2.12.
1.2. Verify Results 1.2.131.2.16.
1.3. Let A, B, and C be sets such that AlB;C and AjC;B. Show
that A and C are disjoint.
Ž
.
Ž
.
1.4. Let A, B, and C be sets such that Cs AyB j ByA . The set C is
called the symmetric difference of A and B and is denoted by AB.
Show that
( )
a
A^ BsAjByAlB
( )
Ž
.
Ž
.
b
A^ B^ D s A^ B ^ D, where D is any set.
( )
Ž
.
Ž
.
Ž
.
c
Al B^ D s AlB ^ AlD , where D is any set.
1.5. Let AsJqJq, where Jq is the set of positive integers. Define a
Ž
.
Ž
.
relation  in A as follows: If m , n
and m , n
are elements in A,
1
1
2
2
Ž
.
Ž
.
then m , n
 m , n
if m n sn m . Show that  is an equivalence
1
1
2
2
1
2
1
2
relation and describe its equivalence classes.
1.6. Let A be the same set as in Exercise 1.5. Show that the following
Ž
.
Ž
.
relation is an equivalence relation:
m , n
 m , n
if m qn sn
1
1
2
2
1
2
1
Ž
.
qm . Draw the equivalence class of 1, 2 .
2
Ž
. Ž
. Ž
. Ž
.4
1.7. Consider the set As y2,y5 , y1,y3 , 1, 2 , 3, 10 . Show that A
defines a function.
1.8. Let A and B be two sets and f be a function defined on A such that
Ž
.
f A ;B. If A , A , . . . , A
are subsets of A, then show that:
1
2
n
( )
Ž
n
.
n
Ž
.
a
f 
A s
f A .
is1
i
is1
i

AN INTRODUCTION TO SET THEORY
18
( )
Ž
n
.
n
Ž
.
b
f 
A ;
f A .
is1
i
is1
i
Ž .
Under what conditions are the two sides in b equal?
1.9. Prove Corollary 1.4.1.
1.10. Prove Corollary 1.4.2.

4
1.11. Show that the set As 3, 9, 19, 33, 51, 73, . . .
is countable.
'
1.12. Show that
3 is an irrational number.
'
'
1.13. Let a, b, c, and d be rational numbers such that aq b scq d .
Then, either
( )
a
asc, bsd, or
( )
b
b and d are both squares of rational numbers.
1.14. Let A;R be a nonempty set bounded from below. Define yA to be


4
Ž
.
Ž
.
the set yx xgA . Show that inf A sysup yA .
Ž
.
1.15. Let A;R be a closed and bounded set, and let sup A sb. Show that
bgA.
1.16. Prove Theorem 1.6.2.
Ž
.
1.17. Let
A, FF
be a topological space. Show that G;FF is a basis for FF in
and only if for each BgFF and each pgB, there is a UgG such that
pgU;B.
1.18. Show that if A and B are closed sets, then AjB is a closed set.
1.19. Let B;A be a closed subset of a compact set A. Show that B is
compact.
1.20. Is a compact subset of a compact set necessarily closed?
In Statistics
1.21. Let X be a random variable. Consider the following events:

yn
A s 
g X 
 xq3
,
ns1, 2, . . . ,

4
Ž
.
n

yn
B s 
g X 
 Fxy3
,
ns1, 2, . . . ,

4
Ž
.
n

As 
g X 
 Fx ,

4
Ž
.

Bs 
g X 
 x ,

4
Ž
.

EXERCISES
19
where x is a real number. Show that for any x,
( )

a

A sA;
ns1
n
( )

b

B sB.
ns1
n
Ž
.
1.22. Let X be a nonnegative random variable such that E X s is finite,
Ž
.
where E X
denotes the expected value of X. The following inequal-
ity, known as Marko®’s inequality, is true:

P XGh F
,
Ž
.
h
where h is any positive number. Consider now a Poisson random
variable with parameter .
( )
Ž
.
a
Find an upper bound on the probability P XG2 using Markov’s
inequality.
( )
Ž .
b
Obtain the exact probability value in a , and demonstrate that it is
smaller than the corresponding upper bound in Markov’s inequal-
ity.
1.23. Let X be a random variable whose expected value  and variance 	 2
exist. Show that for any positive constants c and k,
( )
Ž

.
2
2
a
P
Xy Gc F	 rc ,
( )
Ž

.
2
b
P
Xy Gk	 F1rk ,
( )
Ž

.
2
c
P
Xy k	 G1y1rk .
The preceding three inequalities are equivalent versions of the so-called
Chebyshe®’s inequality.
1.24. Let X be a continuous random variable with the density function
 
1y x ,
y1x1,
f x s
Ž . ½ 0
elsewhere.
By definition, the density function of X is a nonnegative function such
Ž .
x
Ž .
Ž .
that F x sH
f t dt, where F x is the cumulative distribution func-
y
tion of X.
( )
a
Apply Markov’s inequality to finding upper bounds on the following
1
1
Ž .
Ž

. Ž .
Ž

.
probabilities: i P
X G
; ii
P
X 
.
2
3
1
( )
Ž

.
b
Compute the exact value of P
X G
, and compare it against the
2
Ž .Ž .
upper bound in a i .
1.25. Let
X , X , . . . , X
be n continuous random variables. Define the
1
2
n
random variables X
and X
as
Ž1.
Žn.

4
X
s min
X , X , . . . , X
,
Ž1.
1
2
n
1FiFn

4
X
s max
X , X , . . . , X
.
Žn.
1
2
n
1FiFn

AN INTRODUCTION TO SET THEORY
20
Show that for any x,
( )
Ž
.
Ž
.
a
P X
Gx sP X Gx, X Gx, . . . , X Gx ,
Ž1.
1
2
n
( )
Ž
.
Ž
.
b
P X
Fx sP X Fx, X Fx, . . . , X Fx .
Žn.
1
2
n
In particular, if
X , X , . . . , X
form a sample of size n from a
1
2
n
Ž .
population with a cumulative distribution function F x , show that
( )
Ž
.
w
Ž .xn
c
P X
Fx s1y 1yF x
,
Ž1.
( )
Ž
.
w
Ž .xn
d
P X
Fx s F x
.
Žn.
The statistics X
and X
are called the first-order and nth-order
Ž1.
Žn.
statistics, respectively.
1.26. Suppose that we have a sample of size ns5 from a population with an
exponential distribution whose density function is
2ey2 x,
x0,
f x s
Ž . ½ 0
elsewhere.
Ž
.
Find the value of P 2FX
F3 .
Ž1.

C H A P T E R
2
Basic Concepts in Linear Algebra
In this chapter we present some fundamental concepts concerning vector
spaces and matrix algebra. The purpose of the chapter is to familiarize the
reader with these concepts, since they are essential to the understanding of
some of the remaining chapters. For this reason, most of the theorems in this
chapter will be stated without proofs. There are several excellent books on
Ž
linear algebra that can be used for a more detailed study of this subject see
.
the bibliography at the end of this chapter .
In statistics, matrix algebra is used quite extensively, especially in linear
Ž
.
models and multivariate analysis. The books by Basilevsky 1983 , Graybill
Ž
.
Ž
.
Ž
.
1983 , Magnus and Neudecker
1988 , and Searle
1982
include many
applications of matrices in these areas.
In this chapter, as well as in the remainder of the book, elements of the
set of real numbers, R, are sometimes referred to as scalars. The Cartesian
product n
R is denoted by Rn, which is also known as the n-dimensional
is1
Euclidean space. Unless otherwise stated, all matrix elements are considered
to be real numbers.
2.1. VECTOR SPACES AND SUBSPACES
A vector space over R is a set V of elements called vectors together with two
operations, addition and scalar multiplication, that satisfy the following
conditions:
1. uqv is an element of V for all u, v in V.
2. If  is a scalar and ugV, then  ugV.
3. uqvsvqu for all u, v in V.
Ž
.
Ž
.
4. uq vqw s uqv qw for all u, v, w in V.
5. There exists an element 0gV such that 0qusu for all u in V. This
element is called the zero vector.
21

BASIC CONCEPTS IN LINEAR ALGEBRA
22
6. For each ugV there exists a vgV such that uqvs0.
Ž
.
7.  uqv s uq v for any scalar  and any u and v in V.
Ž
.
8.
q us uq u for any scalars  and  and any u in V.
Ž
.
Ž
.
9.   u s  u for any scalars  and  and any u in V.
10. 1usu for any ugV.
EXAMPLE 2.1.1.
A familiar example of a vector space is the n-dimen-
sional Euclidean space Rn. Here, addition and multiplication are defined as
Ž
.
Ž
.
n
follows: If
u , u , . . . , u
and
® , ® , . . . , ®
are two elements in R , then
1
2
n
1
2
n
Ž
.
their sum is defined as
u q® , u q® , . . . , u q® . If  is a scalar, then
1
1
2
2
n
n
Ž
.
Ž
.
 u , u , . . . , u
s  u ,  u , . . . ,  u
.
1
2
n
1
2
n
EXAMPLE 2.1.2.
Let V be the set of all polynomials in x of degree less
than or equal to k. Then V is a vector space. Any element in V can be
expressed as Ýk
a x i, where the a ’s are scalars.
is0
i
i
EXAMPLE 2.1.3.
Let V be the set of all functions defined on the closed
w
x
Ž .
Ž .
interval y1, 1 . Then V is a vector space. It can be seen that f x qg x and
Ž .
Ž .
Ž .
 f x
belong to V, where f x
and g x
are elements in V and  is any
scalar.
w
x
EXAMPLE 2.1.4.
The set V of all nonnegative functions defined on y1, 1
Ž .
is not a vector space, since if f x gV and  is a negative scalar, then
Ž .
 f x V.
Ž
.
EXAMPLE 2.1.5.
Let V be the set of all points
x, y
on a straight line
given by the equation 2 xyyq1s0. Then V is not a vector space. This is
Ž
.
Ž
.
Ž
.
because if
x , y
and
x , y
belong to V, then
x qx , y qy
V, since
1
1
2
2
1
2
1
2
Ž
.
Ž
.
2 x qx
y y qy
q1sy10. Alternatively, we can state that V is not
1
2
1
2
Ž
.
a vector space because the zero element 0, 0 does not belong to V. This
violates condition 5 for a vector space.
A subset W of a vector space V is said to form a vector subspace if W
itself is a vector space. Equivalently, W is a subspace if whenever u, vgW
and  is a scalar, then uqvgW and  ugW. For example, the set W of all
w
x
continuous functions defined on y1, 1 is a vector subspace of V in Example
2.1.3. Also, the set of all points on the straight line yy2 xs0 is a vector
subspace of R2. However, the points on any straight line in R2 not going
Ž
.
through the origin
0, 0
do not form a vector subspace, as was seen in
Example 2.1.5.
Definition 2.1.1.
Let V be a vector space, and u , u , . . . , u
be a collec-
1
2
n
tion of n elements in V. These elements are said to be linearly dependent if
there exist n scalars  ,  , . . . ,  , not all equal to zero, such that Ýn
 u
1
2
n
is1
i
i
s0. If, however, Ýn
 u s0 is true only when all the  ’s are zero, then
is1
i
i
i

VECTOR SPACES AND SUBSPACES
23
u , u , . . . , u are linearly independent. It should be noted that if u , u , . . . , u
1
2
n
1
2
n
are linearly independent, then none of them can be zero. If, for example,
u s0, then  u q0u q  q0u s0 for any 0, which implies that the
1
1
2
n
u ’s are linearly dependent, a contradiction.

i
From the preceding definition we can say that a collection of n elements
in a vector space are linearly dependent if at least one element in this
collection can be expressed as a linear combination of the remaining ny1
elements. If no element, however, can be expressed in this fashion, then the
3 Ž
. Ž
.
n elements are linearly independent. For example, in R , 1, 2,y2 , y1, 0, 3 ,
Ž
.
Ž
.
Ž
.
and
1, 4,y 1
are
linearly
dependent,
since
2 1, 2,y 2 q y1, 0, 3 y
Ž
.
Ž
. Ž
.
1, 4, y1 s0. On the other hand, it can be verified that 1, 1, 0 , 1, 0, 2 , and
Ž
.
0, 1, 3 are linearly independent.
Definition 2.1.2.
Let u , u , . . . , u
be n elements in a vector space V.
1
2
n
The collection of all linear combinations of the form Ýn
 u , where the  ’s
is1
i
i
i
are scalars, is called a linear span of u , u , . . . , u
and is denoted by
1
2
n
Ž
.
L u , u , . . . , u
.

1
2
n
Ž
.
It is easy to see from the preceding definition that L u , u , . . . , u
is a
1
2
n
vector subspace of V. This vector subspace is said to be spanned by
u , u , . . . , u .
1
2
n
Definition 2.1.3.
Let V be a vector space. If there exist linearly indepen-
Ž
.
dent elements u , u , . . . , u
in V
such that VsL u , u , . . . , u
, then
1
2
n
1
2
n
u , u , . . . , u
are said to form a basis for V. The number n of elements in
1
2
n
this basis is called the dimension of the vector space and is denoted by dim V.

Note that a basis for a vector space is not unique. However, its dimension
Ž
. Ž
.
Ž
.
is unique. For example, the three vectors 1, 0, 0 , 0, 1, 0 , and 0, 0, 1 form a
3
3
Ž
. Ž
.
Ž
.
basis for R . Another basis for R
consists of 1, 1, 0 , 1, 0, 1 , and 0, 1, 1 .
If u , u , . . . , u
form a basis for V and if u is a given element in V, then
1
2
n
there exists a unique set of scalars,  ,  , . . . ,  , such that usÝn
 u . To
1
2
n
is1
i
i
show this, suppose that there exists another set of scalars,  ,  , . . . ,  ,
1
2
n
n
n
Ž
.
such that usÝ
 u. Then Ý
 y u s0, which implies that  s
is1
i
is1
i
i
i
i
i
for all i, since the u ’s are linearly independent.
i
Let us now check the dimensions of the vector spaces for some of the
examples described earlier. For Example 2.1.1, dim Vsn. In Example 2.1.2,

2
k4
1, x, x , . . . , x
is a basis for V; hence dim Vskq1. As for Example 2.1.3,
dim V is infinite, since there is no finite set of functions that can span V.
n
Ž
Definition 2.1.4.
Let u and v be two vectors in R . The dot product also
.
called scalar product or inner product of u and v is a scalar denoted by uv
and is given by
n
uvs
u ® ,
Ý
i i
is1

BASIC CONCEPTS IN LINEAR ALGEBRA
24
Ž
where u
and ®
are the ith components of u and v, respectively
is
i
i
.
Ž
.1r2
Ž
n
2.1r2
1, 2, . . . , n . In particular, if usv, then uu
s Ý
u
is called the
is1
i
Ž
.
	
	
Euclidean norm or length of u and is denoted by
u
. The dot product of
2
	
	 	 	
u and v is also equal to
u
v
cos , where  is the angle between u and v.
2
2

Definition 2.1.5.
Two vectors u and v in Rn are said to be orthogonal if
their dot product is zero.

Definition 2.1.6.
Let U
be a vector subspace of
Rn. The vectors
e , e , . . . , e
form an orthonormal basis for U if they satisfy the following
1
2
m
properties:
1. e , e , . . . , e
form a basis for U.
1
2
m
Ž
.
2. e e s0 for all ij i, js1, 2, . . . , m .
i
j
	
	
3.
e
s1 for is1, 2, . . . , m.
2
i
Any collection of vectors satisfying just properties 2 and 3 are said to be
orthonormal.

Theorem 2.1.1.
Let u , u , . . . , u
be a basis for a vector subspace U of
1
2
m
Rn. Then there exists an orthonormal basis, e , e , . . . , e , for U, given by
1
2
m
v1
e s
,
where v su ,
1
1
1
	
	
v
2
1
v
v u
2
1
2
e s
,
where v su y
v ,
2
2
2
1
2
	
	
v
	
	
v
2
2
2
1
...
my1
v
v u
m
i
m
e s
,
where v su y
v .
Ý
m
m
m
i
2
	
	
v
	
	
v
2
m
2
is1
i
Ž
.
Proof. See Graybill 1983, Theorem 2.6.5 .

The procedure of constructing an orthonormal basis from any given basis
as described in Theorem 2.1.1 is known as the Gram-Schmidt orthonormal-
ization procedure.
Theorem 2.1.2.
Let u and v be two vectors in Rn. Then:


	
	 	 	
1.
uv F u
v
.
2
2
	
	
	
	
	 	
2.
uqv
F u
q v
.
2
2
2
Ž
.
Proof. See Marcus and Minc 1988, Theorem 3.4 .


LINEAR TRANSFORMATIONS
25
The inequality in part 1 of Theorem 2.1.2 is known as the CauchySchwarz
inequality. The one in part 2 is called the triangle inequality.
Definition 2.1.7.
Let U be a vector subspace of Rn. The orthogonal
complement of U, denoted by U H, is the vector subspace of Rn which
consists of all vectors v such that uvs0 for all u in U.

Definition 2.1.8.
Let U , U , . . . , U
be vector subspaces of the vector
1
2
n
space U. The direct sum of these vector subspaces, denoted by [n U ,i
is1
consists of all vectors u that can be uniquely expressed as usÝn
u , where
is1
i
u gU , is1, 2, . . . , n.

i
i
Theorem 2.1.3.
Let U , U , . . . , U be vector subspaces of the vector space
1
2
n
U. Then:
1. [n U is a vector subspace of U.
i
is1
2. If Us[n U , then  n
U consists of just the zero element 0 of U.
i
is1
i
is1
3. dim[n U sÝn
dim U .
i
is1
i
is1
Proof. The proof is left as an exercise.

Theorem 2.1.4.
Let U be a vector subspace of Rn. Then RnsU[U H.
Ž
.
Proof. See Marcus and Minc 1988, Theorem 3.3 .

From Theorem 2.1.4 we conclude that any vgRn can be uniquely written
as vsv qv , where v gU and v gU H. In this case, v and v
are called
1
2
1
2
1
2
the projections of v on U and U H , respectively.
2.2. LINEAR TRANSFORMATIONS
Let U and V be two vector spaces. A function T: U™V is called a linear
Ž
.
Ž
.
Ž
.
transformation if T  u q u
s T u
q T u
for all u , u
in U
1
1
2
2
1
1
2
2
1
2
and any scalars 
and  . For example, let T: R3™R3 be defined as
1
2
T x , x , x
s x yx , x qx , x
.
Ž
.
Ž
.
1
2
3
1
2
1
3
3
Then T is a linear transformation, since
T  x , x , x
q y , y , y
Ž
.
Ž
.
1
2
3
1
2
3
sT  x q y ,  x q y ,  x q y
Ž
.
1
1
2
2
3
3
s  x q y y x y y ,  x q y q x q y ,  x q y
Ž
.
1
1
2
2
1
1
3
3
3
3
s x yx , x qx , x
q y yy , y qy , y
Ž
.
Ž
.
1
2
1
3
3
1
2
1
3
3
sT x , x , x
qT y , y , y
.
Ž
.
Ž
.
1
2
3
1
2
3

BASIC CONCEPTS IN LINEAR ALGEBRA
26
Ž
.
We note that the image of U under T, or the range of T, namely T U , is
Ž
.
a vector subspace of V. This is true because if v , v are in T U , then there
1
2
Ž
.
Ž
.
exist u
and u
in U such that v sT u
and v sT u
. Hence, v qv s
1
2
1
1
2
2
1
2
Ž
.
Ž
.
Ž
.
Ž
.
T u
qT u
sT u qu
, which belongs to T U . Also, if  is a scalar,
1
2
1
2
Ž .
Ž
.
Ž
.
then T u sT  u gT U
for any ugU.
Definition 2.2.1.
Let T: U™V be a linear transformation. The kernel of
Ž .
T, denoted by ker T, is the collection of all vectors u in U such that T u s0,
where 0 is the zero vector in V. The kernel of T is also called the null space
of T.
3
3
Ž
.
As an example of a kernel, let T: R ™R
be defined as T x , x , x
s
1
2
3
Ž
.
x yx , x yx . Then
1
2
1
3

ker Ts
x , x , x
x sx , x sx

4
Ž
.
1
2
3
1
2
1
3
Ž
.
3
In this case, ker T consists of all points
x , x , x
in R that lie on a straight
1
2
3
line through the origin given by the equations x sx sx .

1
2
3
Theorem 2.2.1.
Let T: U™V be a linear transformation. Then we have
the following:
1. ker T is a vector subspace of U.
Ž
.
w Ž
.x
2. dim Usdim ker T qdim T U .
Proof. Part 1 is left as an exercise. To prove part 2 we consider the
Ž
.
w Ž
.x
following.
Let
dim U s n,
dim ker T s p,
and
dim T U
s q.
Let
Ž
.
u , u , . . . , u be a basis for ker T, and v , v , . . . , v be a basis for T U . Then,
1
2
p
1
2
q
Ž
.
Ž
.
there exist vectors w , w , . . . , w in U such that T w sv
is1, 2, . . . , q . We
1
2
q
i
i
need to show that u , u , . . . , u ; w , w , . . . , w
form a basis for U, that is,
1
2
p
1
2
q
they are linearly independent and span U.
Suppose that there exist scalars  ,  , . . . ,  ;  ,  , . . . , 
such that
1
2
p
1
2
q
p
q
 u q
 w s0.
2.1
Ž
.
Ý
Ý
i
i
i
i
is1
is1
Then
p
q
0sT
 u q
 w
,
Ý
Ý
i
i
i
i
ž
/
is1
is1
where 0 represents the zero vector in V
p
q
s
 T u
q
 T w
Ž
.
Ž
.
Ý
Ý
i
i
i
i
is1
is1
q
s
 T w
,
since
u gker T, is1, 2, . . . , p
Ž
.
Ý
i
i
i
is1
q
s
 v .
Ý
i i
is1

MATRICES AND DETERMINANTS
27
Since the v ’s are linearly independent, then  s0 for is1, 2, . . . , q. From
i
i
Ž
.
2.1 it follows that  s0 for is1, 2, . . . , p, since the u ’s are also linearly
i
i
independent. Thus the vectors u , u , . . . , u ; w , w , . . . , w are linearly inde-
1
2
p
1
2
q
pendent.
Let us now suppose that u is any vector in U. To show that it belongs to
Ž
.
Ž .
L u , u , . . . , u ; w , w , . . . , w . Let vsT u . Then
there
exist scalars
1
2
p
1
2
q
a , a , . . . , a
such that vsÝq
a v . It follows that
1
2
q
is1
i i
q
T u s
a T w
Ž .
Ž
.
Ý
i
i
is1
q
sT
a w
.
Ý
i
i
ž
/
is1
Thus,
q
T uy
a w
s0,
Ý
i
i
ž
/
is1
and uyÝq
a w must then belong to ker T. Hence,
is1
i
i
q
p
uy
a w s
b u
2.2
Ž
.
Ý
Ý
i
i
i
i
is1
is1
Ž
.
for some scalars, b , b , . . . , b . From 2.2 we then have
1
2
p
p
q
us
b u q
a w ,
Ý
Ý
i
i
i
i
is1
is1
which shows that u belongs to the linear span of u , u , . . . , u ; w , w , . . . , w .
1
2
p
1
2
q
We conclude that these vectors form a basis for U. Hence, nspqq.

Corollary 2.2.1.
T: U™V is a one-to-one linear transformation if and
Ž
.
only if dim ker T s0.
Proof. If T is a one-to-one linear transformation, then ker T consists of
Ž
.
just one vector, namely, the zero vector. Hence, dim ker T s0. Vice versa, if
Ž
.
dim ker T s0, or equivalently, if ker T consists of just the zero vector, then
T must be a one-to-one transformation. This is true because if u and u
are
1
2
Ž
.
Ž
.
Ž
.
in U and such that T u
sT u
, then T u yu
s0, which implies that
1
2
1
2
u yu gker T and thus u yu s0.

1
2
1
2
2.3. MATRICES AND DETERMINANTS
Matrix algebra was devised by the English mathematician Arthur Cayley
Ž
.
18211895 . The use of matrices originated with Cayley in connection with

BASIC CONCEPTS IN LINEAR ALGEBRA
28
linear transformations of the form
ax qbx sy ,
1
2
1
cx qdx sy ,
1
2
2
where a, b, c, and d are scalars. This transformation is completely deter-
mined by the square array
a
b ,
c
d
which is called a matrix of order 22. In general, let T: U™V be a linear
transformation, where U and V are vector spaces of dimensions m and n,
respectively. Let u , u , . . . , u
be a basis for U and v , v , . . . , v be a basis for
1
2
m
1
2
n
Ž
.
V. For is1, 2, . . . , m, consider T u , which can be uniquely represented as
i
n
T u
s
a v ,
is1, 2, . . . , m,
Ž
.
Ý
i
i j j
js1
where the a ’s are scalars. These scalars completely determine all possible
i j
values of T: If ugU, then usÝm c u for some scalars c , c , . . . , c . Then
is1
i
i
1
2
m
Ž .
m
Ž
.
m
Ž
n
.
T u sÝ
c T u sÝ
c Ý
a v . By definition, the rectangular array
is1
i
i
is1
i
js1
i j j
a
a

a
11
12
1n
a
a

a
21
22
2 n
.
.
.
As
.
.
.
.
.
.
a
a

a
m1
m2
mn
is called a matrix of order mn, which indicates that A has m rows and n
columns. The a ’s are called the elements of A. In some cases it is more
i j
Ž
.
convenient to represent A using the notation As a
. In particular, if
i j
msn, then A is called a square matrix. Furthermore, if the off-diagonal
elements of a square matrix A are zero, then A is called a diagonal matrix and
Ž
.
is written as AsDiag a , a
, . . . , a
. In this special case, if the diagonal
11
22
nn
elements are equal to 1, then A is called the identity matrix and is denoted by
I
to indicate that it is of order nn. A matrix of order m1 is called a
n
column vector. Likewise, a matrix of order 1n is called a row vector.
2.3.1. Basic Operations on Matrices
Ž
.
Ž
.
1. Equality of Matrices. Let As a
and Bs b
be two matrices of the
i j
i j
same order. Then AsB if and only if a sb
for all is1, 2, . . . , m;
i j
i j
js1, 2, . . . , n.

MATRICES AND DETERMINANTS
29
Ž
.
Ž
.
2. Addition of Matrices. Let As a
and Bs b
be two matrices of
i j
i j
Ž
.
order mn. Then AqB is a matrix Cs c
of order mn such that
i j
Ž
.
c sa qb
is1, 2, . . . , m; js1, 2, . . . , n .
i j
i j
i j
Ž
.
3. Scalar Multiplication. Let  be a scalar, and As a
be a matrix of
i j
Ž
.
order mn. Then As a
.
i j
Ž
.
4. The Transpose of a Matrix. Let As a
be a matrix of order mn.
i j
The transpose of A, denoted by A, is a matrix of order nm whose
rows are the columns of A. For example,
2
y1
2
3
1
if
As
,
then
As
.
3
0
y1
0
7
1
7
A matrix A is symmetric if AsA. It is skew-symmetric if AsyA.
A skew-symmetric matrix must necessarily have zero elements along its
diagonal.
Ž
.
Ž
.
5. Product of Matrices. Let As a
and Bs b
be matrices of orders
i j
i j
Ž
.
mn and np, respectively. The product AB is a matrix Cs c
of
i j
n
Ž
.
order mp such that c sÝ
a b
is1, 2, . . . , m; js1, 2, . . . , p .
i j
ks1
ik
k j
It is to be noted that this product is defined only when the number of
columns of A is equal to the number of rows of B.
In particular, if a and b are column vectors of order n1, then their
dot product ab can be expressed as a matrix product of the form ab
or ba.
Ž
.
6. The Trace of a Matrix. Let As a
be a square matrix of order nn.
i j
Ž .
The trace of A, denoted by tr A , is the sum of its diagonal elements,
that is,
n
tr A s
a .
Ž .
Ý
ii
is1
On the basis of this definition, it is easy to show that if A and B are
Ž .
Ž
.
Ž
.
matrices of order nn, then the following hold: i tr AB str BA ;
Ž .
Ž
.
Ž .
Ž .
ii tr AqB str A qtr B .
Ž
.
Definition 2.3.1.
Let As a
be an mn matrix. A submatrix B of A is
i j
a matrix which can be obtained from A by deleting a certain number of rows
and columns.
In particular, if the ith row and jth column of A that contain the element
Ž
a
are deleted, then the resulting matrix is denoted by M
is1, 2, . . . , m;
i j
i j
.
js1, 2, . . . , n .
Let us now suppose that A is a square matrix of order nn. If rows
i , i , . . . , i
and columns i , i , . . . , i
are deleted from A, where pn, then
1
2
p
1
2
p
the resulting submatrix is called a principal submatrix of A. In particular, if
the deleted rows and columns are the last p rows and the last p columns,
respectively, then such a submatrix is called a leading principal submatrix.

BASIC CONCEPTS IN LINEAR ALGEBRA
30
Definition 2.3.2.
A partitioned matrix is a matrix that consists of several
submatrices obtained by drawing horizontal and vertical lines that separate it
into groups of rows and columns.
For example, the matrix
.
.
.
.
1
0
3
4
y5
.
.
.
.
6
2
10
5
0
As
.
.
.
.
3
2
1
0
2
.
.
is partitioned into six submatrices by drawing one horizontal line and two
vertical lines as shown above.
Ž
.
Definition 2.3.3.
Let As a
be an m n matrix and B be an m n
i j
1
1
2
2
Ž
.
matrix. The direct or Kronecker product of A and B, denoted by AmB, is a
matrix of order m m n n defined as a partitioned matrix of the form
1
2
1
2
a B
a B

a
B
11
12
1n1
a B
a
B

a
B
21
22
2 n1
AmBs
.
.
.
.
.
.
.
.
.
.
a
B
a
B

a
B
m 1
m 2
m n
1
2
1
1
w
x
This matrix can be simplified by writing AmBs a B .

i j
Properties of the direct product can be found in several matrix algebra
Ž
.
books and papers. See, for example, Graybill 1983, Section 8.8 , Henderson
Ž
.
Ž
.
and Searle
1981 , Magnus and Neudecker
1988, Chapter 2 , and Searle
Ž
.
1982, Section 10.7 . Some of these properties are listed below:
Ž
.
1.
AmB sAmB.
Ž
.
Ž
.
2. Am BmC s AmB mC.
Ž
.Ž
.
3.
AmB CmD sACmBD, if AC and BD are defined.
Ž
.
Ž . Ž .
4. tr AmB str A tr B , if A and B are square matrices.
Ž
.
The paper by Henderson, Pukelsheim, and Searle
1983
gives a detailed
account of the history associated with direct products.
Ž
Definition 2.3.4.
Let A , A , . . . , A
be matrices of orders m n
is
1
2
k
i
i
.
k
1, 2, . . . , k . The direct sum of these matrices, denoted by [
A , is a
i
is1
Ž
k
.
Ž
k
.
partitioned matrix of order Ý
m  Ý
n
that has the block-diagonal
is1
i
is1
i
form
k
A sDiag A , A , . . . , A
.
Ž
.
[
i
1
2
k
is1

MATRICES AND DETERMINANTS
31
The following properties can be easily shown on the basis of the preceding
definition:
k
k
k Ž
.
1. [
A q[
B s[
A qB , if A and B are of the same order
i
i
i
i
i
i
is1
is1
is1
for is1, 2, . . . , k.
w
k
xw
k
x
k
2. [
A
[
B s[
A B , if A B is defined for is1, 2, . . . , k.
i
i
i
i
i
i
is1
is1
is1
w
k
x
k

3. [
A s[
A .
i
i
is1
is1
Ž
k
.
k
Ž
.
4. tr [
A
sÝ
tr A .

i
is1
i
is1
Ž
.
Definition 2.3.5.
Let As a
be a square matrix of order nn. The
i j
Ž .
determinant of A, denoted by det A , is a scalar quantity that can be
computed iteratively as
n
jq1
det A s
y1
a
det M
,
2.3
Ž .
Ž
.
Ž
.
Ž
.
Ý
1 j
1 j
js1
where M
is a submatrix of A obtained by deleting row 1 and column j
1 j
Ž
.
js1, 2, . . . , n . For each j, the determinant of M
is obtained in terms of
1 j
Ž
.
Ž
.
determinants of matrices of order ny2  ny2 using a formula similar to
Ž
.
2.3 . This process is repeated several times until the matrices on the
Ž
.
right-hand side of 2.3 become of order 22. The determinant of a 22
Ž
.
Ž .
matrix such as bs b
is given by det B sb
b
yb
b . Thus by an
i j
11
22
12
21
Ž
.
Ž .
iterative application of formula
2.3 , the value of det A
can be fully
determined. For example, let A be the matrix
1
2
y1
As
.
5
0
3
1
2
1
Ž .
Ž
.
Ž
.
Ž
.
Then det A sdet A
y2 det A
ydet A
, where A , A , A
are 22 sub-
1
2
3
1
2
3
matrices, namely
0
3
5
3
5
0
A s
,
A s
,
A s
.
1
2
3
2
1
1
1
1
2
Ž .
Ž .
It follows that det A sy6y2 2 y10sy20.

Ž
.
Definition 2.3.6.
Let As a
be a square matrix order of nn. The
i j
determinant of M , the submatrix obtained by deleting row i and column j,
i j
Ž
.iqj
Ž
.
is called a minor of A of order ny1. The quantity y1
det M
is called
i j
Ž
.
a cofactor of the corresponding i, j th element of A. More generally, if A is
an mn matrix and if we strike out all but p rows and the same number of
Ž
.
columns from A, where pFmin m, n , then the determinant of the resulting
submatrix is called a minor of A of order p.

BASIC CONCEPTS IN LINEAR ALGEBRA
32
The determinant of a principal submatrix of a square matrix A is called a
principal minor. If, however, we have a leading principal submatrix, then its
determinant is called a leading principal minor.

NOTE 2.3.1.
The determinant of a matrix A is defined only when A is a
square matrix.
Ž .
Ž
.
NOTE 2.3.2.
The expansion of det A in 2.3 was carried out by multiply-
ing the elements of the first row of A by their corresponding cofactors and
Ž
.
Ž .
then summing over j s1, 2, . . . , n . The same value of det A could have also
been obtained by similar expansions according to the elements of any row of
Ž
.
A instead of the first row , or any column of A. Thus if M
is a submatrix of
i j
Ž .
A obtained by deleting row i and column j, then det A
can be obtained
by using any of the following expansions:
n
iqj
By row i:
det A s
y1
a det M
,
is1, 2, . . . , n.
Ž .
Ž
.
Ž
.
Ý
i j
i j
js1
n
iqj
By column j:
det A s
y1
a det M
,
js1, 2, . . . , n.
Ž .
Ž
.
Ž
.
Ý
i j
i j
is1
NOTE 2.3.3.
Some of the properties of determinants are the following:
Ž
.
Ž .
Ž .
i. det AB sdet A det B , if A and B are nn matrices.
Ž
.
Ž .
ii. If A is the transpose of A, then det A sdet A .
Ž
.
n
Ž .
iii. If A is an nn matrix and  is a scalar, then det A s
det A .
Ž
.
Ž .
iv. If any two rows or columns of A are identical, then det A s0.
Ž
.
Ž .
v. If any two rows or columns of A are interchanged, then det A is
multiplied by y1.
Ž .
vi. If det A s0, then A is called a singular matrix. Otherwise, A is a
nonsingular matrix.
vii. If A and B are matrices of orders mm and nn, respectively, then
Ž .
Ž
.
w
Ž .xnw
Ž .xm Ž .
Ž
.
the following hold: a det AmB s det A
det B
; b det A[B
w
Ž .xw
Ž .x
s det A
det B .
NOTE 2.3.4.
The history of determinants dates back to the fourteenth
Ž
.
century. According to Smith 1958, page 273 , the Chinese had some knowl-
Ž
.
edge of determinants as early as about 1300 A.D. Smith 1958, page 440 also
Ž
.
reported that the Japanese mathematician Seki Kowa
16421708
had
˜
discovered the expansion of a determinant in solving simultaneous equations.
In the West, the theory of determinants is believed to have originated with
Ž
.
the German mathematician Gottfried Leibniz 16461716 in 1693, ten years

MATRICES AND DETERMINANTS
33
after the work of Seki Kowa. However, the actual development of the theory
˜
of determinants did not begin until the publication of a book by Gabriel
Ž
. Ž
.
Cramer 17041752
see Price, 1947, page 85 in 1750. Other mathemati-
cians who contributed to this theory include Alexandre Vandermonde
Ž
.
Ž
.
Ž
.
17351796 , Pierre-Simon Laplace
17491827 , Carl Gauss
17771855 ,
Ž
.
Ž
.
and Augustin-Louis Cauchy 17891857 . Arthur Cayley 18211895 is cred-
ited with having been the first to introduce the common present-day notation
of vertical bars enclosing a square matrix. For more interesting facts about
the history of determinants, the reader is advised to read the article by Price
Ž
.
1947 .
2.3.2. The Rank of a Matrix
Ž
.



Let As a
be a matrix of order mn. Let u , u , . . . , u
denote the row
i j
1
2
m
vectors of A, and let v , v , . . . , v
denote its column vectors. Consider the
1
2
n
Ž



 .
linear spans of the row and column vectors, namely, V sL u ,u , . . . , u
, V
1
1
2
m
2
Ž
.
sL v , v , . . . , v , respectively.
1
2
n
Theorem 2.3.1.
The vector spaces V
and V
have the same dimension.
1
2
Ž
.
Ž
.
Proof. See Lancaster 1969, Theorem 1.15.1 , or Searle 1982, Section 6.6 .

Thus, for any matrix A, the number of linearly independent rows is the
same as the number of linearly independent columns.
Definition 2.3.7.
The rank of a matrix A is the number of its linearly
Ž
.
Ž .
independent rows or columns . The rank of A is denoted by r A .

Theorem 2.3.2.
If a matrix A has a nonzero minor of order r, and if all
Ž
.
minors of order rq1 and higher if they exist are zero, then A has rank r.
Ž
.
Proof. See Lancaster 1969, Lemma 1, Section 1.15 .

For example, if A is the matrix
2
3
y1
As
,
0
1
2
2
4
1
Ž .
Ž .
then r A s2. This is because det A s0 and at least one minor of order 2 is
different from zero.

BASIC CONCEPTS IN LINEAR ALGEBRA
34
There are several properties associated with the rank of a matrix. Some of
these properties are the following:
Ž .
Ž
.
1. r A sr A .
2. The rank of A is unchanged if A is multiplied by a nonsingular matrix.
Thus if A is an mn matrix and P is an nn nonsingular matrix, then
Ž .
Ž
.
r A sr AP .
Ž .
Ž
.
Ž
.
3. r A sr AA sr AA .
w
x
4. If the matrix A is partitioned as As A : A
, where A
and A
are
1
2
1
2
Ž
.
Ž .
Ž
.
Ž
.
submatrices of the same order, then r A qA
Fr A Fr A
qr A
.
1
2
1
2
More generally, if the matrices A , A , . . . , A
are of the same order
1
2
k
w
x
and if A is partitioned as As A : A :  : A
, then
1
2
k
k
k
r
A
Fr A F
r A
.
Ž .
Ž
.
Ý
Ý
i
i
ž
/
is1
is1
Ž .
Ž .
Ž
.
5. If
the
product
AB
is
defined,
then
r A q r B y n F r AB F
 Ž .
Ž .4
Ž
min r A , r B , where n is the number of columns of A or the number
.
of rows of B .
Ž
.
Ž . Ž .
6. r AmB sr A r B .
Ž
.
Ž .
Ž .
7. r A[B sr A qr B .
Definition 2.3.8.
Let A be a matrix of order mn and rank r. Then we
have the following:
1. A is said to have a full row rank if rsmn.
2. A is said to have a full column rank if rsnm.
Ž .
3. A is of full rank if rsmsn. In this case, det A 0, that is, A is a
nonsingular matrix.

2.3.3. The Inverse of a Matrix
Ž
.
Let As a
be a nonsingular matrix of order nn. The inverse of A,
i j
denoted by Ay1, is an nn matrix that satisfies the condition AAy1 sAy1A
sI .
n
The inverse of A can be computed as follows: Let c
be the cofactor of a
i j
i j
Ž
.
Ž
.
see Definition 2.3.6 . Define the matrix C as Cs c
. The transpose of C is
i j
called the adjugate or adjoint of A and is denoted by adj A. The inverse of A
is then given by
adj A
y1
A
s
.
det A
Ž .

MATRICES AND DETERMINANTS
35
It can be verified that
adj A
adj A
A
s
AsI .
n
det A
det A
Ž .
Ž .
For example, if A is the matrix
2
0
1
As
,
y3
2
0
2
1
1
Ž .
then det A sy3, and
2
1
y2
adj As
.
3
0
y3
y7
y2
4
Hence,
2
1
2
y
y
3
3
3
y1
A
s
.
y1
0
1
7
2
4
y
3
3
3
Some properties of the inverse operation are given below:
Ž
.y1
y1
y1
1.
AB
sB
A
.
Ž
.y1
Ž
y1.
2.
A
s A
.
Ž
y1.
Ž .
3. det A
s1rdet A .
Ž
y1.y1
4.
A
sA.
Ž
.y1
y1
y1
5.
AmB
sA
mB
.
Ž
.y1
y1
y1
6.
A[B
sA
[B
.
7. If A is partitioned as
A
A
11
12
As
,
A
A
21
22
Ž
.
where A
is of order n n
i, js1, 2 , then
i j
i
j
det A
det A
yA
Ay1A
if A
is nonsingular,
Ž
.
Ž
.
11
22
21
11
12
11
det A s
Ž .
y1
½ det A
det A
yA
A
A
if A
is nonsingular.
Ž
.
Ž
.
22
11
12
22
21
22

BASIC CONCEPTS IN LINEAR ALGEBRA
36
The inverse of A is partitioned as
B
B
11
12
y1
A
s
,
B
B
21
22
where
y1
y1
B
s A
yA
A
A
,
Ž
.
11
11
12
22
21
B
syB
A
Ay1,
12
11
12
22
B
syAy1A
B
,
21
22
21
11
B
sAy1 qAy1A
B
A
Ay1.
22
22
22
21
11
12
22
2.3.4. Generalized Inverse of a Matrix
This inverse represents a more general concept than the one discussed in the
previous section. Let A be a matrix of order mn. Then, a generalized
inverse of A, denoted by Ay, is a matrix of order nm that satisfies the
condition
AAyAsA.
2.4
Ž
.
Note that Ay is defined even if A is not a square matrix. If A is a square
Ž
.
matrix, it does not have to be nonsingular. Furthermore, condition 2.4 can
Ž
be satisfied by infinitely many matrices
see, for example, Searle, 1982,
.
Ž
.
y1
y1
Chapter 8 . If A is nonsingular, then 2.4 is satisfied by only A
. Thus A
is a special case of Ay.
Theorem 2.3.3.
1. If A is a symmetric matrix, then Ay can be chosen to be symmetric.
Ž
.y
2. A AA
AAsA for any matrix A.
Ž
.y
3. A AA
A is invariant to the choice of a generalized inverse of AA.
Ž
.
Proof. See Searle 1982, pages 221222 .

2.3.5. Eigenvalues and Eigenvectors of a Matrix
Let A be a square matrix of order nn. By definition, a scalar  is said to be
Ž
.
an eigenvalue or characteristic root of A if AyI
is a singular matrix, that
n
is,
det AyI
s0.
2.5
Ž
.
Ž
.
n

MATRICES AND DETERMINANTS
37
Thus an eigenvalue of A satisfies a polynomial equation of degree n called
Ž
.
the characteristic equation of A. If  is a multiple solution
or root
of
Ž
.
Ž
.
equation 2.5 , that is, 2.5 has several roots, say m, that are equal to , then
 is said to be an eigenvalue of multiplicity m.
Ž
.
Since r AyI
n by the fact that AyI
is singular, the columns of
n
n
AyI
must be linearly related. Hence, there exists a nonzero vector v such
n
that
AyI
vs0,
2.6
Ž
.
Ž
.
n
or equivalently,
Avsv.
2.7
Ž
.
Ž
.
Ž
.
A vector satisfying 2.7 is called an eigenvector or a characteristic vector
Ž
.
corresponding to the eigenvalue . From
2.7
we note that the linear
transformation of v by the matrix A is a scalar multiple of v.
The following theorems describe certain properties associated with eigen-
values and eigenvectors. The proofs of these theorems can be found in
Ž
.
standard matrix algebra books see the annotated bibliography .
Theorem 2.3.4.
A square matrix A is singular if and only if at least one of
its eigenvalues is equal to zero. In particular, if A is symmetric, then its rank
is equal to the number of its nonzero eigenvalues.
Theorem 2.3.5.
The eigenvalues of a symmetric matrix are real.
Theorem 2.3.6.
Let A be a square matrix, and let  ,  , . . . ,  denote its
1
2
k
distinct eigenvalues. If v , v , . . . , v
are eigenvectors of A corresponding
1
2
k
to  ,  , . . . ,  , respectively, then v , v , . . . , v
are linearly independent. In
1
2
k
1
2
k
particular, if A is symmetric, then v , v , . . . , v are orthogonal to one another,
1
2
k

Ž
.
that is, v v s0 for ij i, js1, 2, . . . , k .
i
j
Theorem 2.3.7.
Let A and B be two matrices of orders mm and nn,
respectively. Let  ,  , . . . , 
be the eigenvalues of A, and ® , ® , . . . , ® be
1
2
m
1
2
n
the eigenvalues of B. Then we have the following:
Ž
1. The eigenvalues of AmB are of the form  
is1, 2, . . . , m; js
i
j
.
1, 2, . . . , n .
2. The eigenvalues of A[B are  ,  , . . . ,  ;  ,  , . . . ,  .
1
2
m
1
2
n
Theorem 2.3.8.
Let  ,  , . . . , 
be the eigenvalues of a matrix A of
1
2
n
order nn. Then the following hold:
Ž .
n
1. tr A sÝ
 .
is1
i
Ž .
n
2. det A sŁ
 .
is1
i

BASIC CONCEPTS IN LINEAR ALGEBRA
38
Theorem 2.3.9.
Let A and B be two matrices of orders mn and nm
Ž
.
nGm , respectively. The nonzero eigenvalues of BA are the same as those
of AB.
2.3.6. Some Special Matrices
1. The vector 1
is a column vector of ones of order n1.
n
2. The matrix J
is a matrix of ones of order nn.
n
3. Idempotent Matrix. A square matrix A for which A2sA is called an
Ž
.
idempotent matrix. For example, the matrix AsI y 1rn J
is idem-
n
n
potent of order nn. The eigenvalues of an idempotent matrix are
equal to zeros and ones. It follows from Theorem 2.3.8 that the rank of
an idempotent matrix, which is the same as the number of eigenvalues
that are equal to 1, is also equal to its trace. Idempotent matrices are
Ž
.
used in many applications in statistics see Section 2.4 .
4. Orthogonal Matrix. A square matrix A is orthogonal if AAsI. From
Ž .
y1
this definition it follows that i A is orthogonal if and only if AsA
;
Ž . 
Ž .
ii
det A
s1. A special orthogonal matrix is the Householder matrix,
which is a symmetric matrix of the form
HsIy2uuruu,
where u is a nonzero vector. Orthogonal matrices occur in many
applications of matrix algebra and play an important role in statistics,
as will be seen in Section 2.4.
2.3.7. The Diagonalization of a Matrix
Ž
.
Theorem 2.3.10
The Spectral Decomposition Theorem .
Let A be a
symmetric matrix of order nn. There exists an orthogonal matrix P such
Ž
.
that AsP P, where sDiag  ,  , . . . , 
is a diagonal matrix whose
1
2
n
diagonal elements are the eigenvalues of A. The columns of P are the
corresponding orthonormal eigenvectors of A.
Ž
.
Proof. See Basilevsky 1983, Theorem 5.8, page 200 .

w
x
If P is partitioned as Ps p : p :  :p
, where p is an eigenvector of A
1
2
n
i
Ž
.
with eigenvalue 
is1, 2, . . . , n , then A can be written as
i
n

As
 p p .
Ý
i
i
i
is1

MATRICES AND DETERMINANTS
39
For example, if
1
0
y2
As
,
0
0
0
y2
0
4
then A has two distinct eigenvalues,  s0 of multiplicity 2 and  s5. For
1
2
'
Ž
.
 s0 we have two orthonormal eigenvectors, p s 2, 0, 1 r 5 and p s
1
1
2
Ž
.
Ž
.
0, 1, 0 . Note that p
and p
span the kernel
null space
of the linear
1
2
transformation represented by A. For  s5 we have the normal eigenvector
2
'
Ž
.
p s 1, 0,y2 r 5 , which is orthogonal to both p and p . Hence, P and 
3
1
2
in Theorem 2.3.10 for the matrix A are
2
1
0
'
'
5
5
Ps
,
0
1
0
1
y2
0
'
'
5
5
sDiag 0, 0, 5 .
Ž
.
The next theorem gives a more general form of the spectral decomposition
theorem.
Ž
.
Theorem 2.3.11 The Singular-Value Decomposition Theorem .
Let A be
Ž
.
a matrix of order mn mFn and rank r. There exist orthogonal matrices
w
x
Ž
.
P and Q such that AsP D : 0 Q, where DsDiag  ,  , . . . , 
is a diago-
1
2
m
nal matrix with nonnegative diagonal elements called the singular values of
Ž
.
A, and 0 is a zero matrix of order m nym . The diagonal elements of D
are the square roots of the eigenvalues of AA.
Ž
.
Proof. See, for example, Searle 1982, pages 316317 .

2.3.8. Quadratic Forms
Ž
.
Ž
.
Let As a
be a symmetric matrix of order nn, and let xs x , x , . . . , x

i j
1
2
n
be a column vector of order n1. The function
q x sxAx
Ž .
n
n
s
a x x
Ý Ý
i j
i
j
is1 js1
is called a quadratic form in x.

BASIC CONCEPTS IN LINEAR ALGEBRA
40
A quadratic form xAx is said to be the following:
1. Positive definite if xAx0 for all x0 and is zero only if xs0.
2. Positive semidefinite if xAxG0 for all x and xAxs0 for at least one
nonzero value of x.
3. Nonnegative definite if A is either positive definite or positive semi-
definite.
Ž
.
Theorem 2.3.12.
Let As a
be a symmetric matrix of order nn.
i j
Then A is positive definite if and only if either of the following two conditions
is satisfied:
1. The eigenvalues of A are all positive.
2. The leading principal minors of A are all positive, that is,
a
a
11
12
a 0,
det
0, . . . ,
det A 0.
Ž .
11
ž
/
a
a
21
22
Proof. The proof of part 1 follows directly from the spectral decomposi-
Ž
.
tion theorem. For the proof of part 2, see Lancaster 1969, Theorem 2.14.4 .

Ž
.
Theorem 2.3.13.
Let As a
be a symmetric matrix of order nn.
i j
Then A is positive semidefinite if and only if its eigenvalues are nonnegative
with at least one of them equal to zero.
Ž
.
Proof. See Basilevsky 1983, Theorem 5.10, page 203 .

2.3.9. The Simultaneous Diagonalization of Matrices
By simultaneous diagonalization we mean finding a matrix, say Q, that can
reduce several square matrices to a diagonal form. In many situations there
may be a need to diagonalize several matrices simultaneously. This occurs
frequently in statistics, particularly in analysis of variance.
Ž
The proofs of the following theorems can be found in Graybill
1983,
.
Chapter 12 .
Theorem 2.3.14.
Let A and B be symmetric matrices of order nn.
1. If A is positive definite, then there exists a nonsingular matrix Q such
that QAQsI
and QBQsD, where D is a diagonal matrix whose
n
Ž
.
diagonal elements are the roots of the polynomial equation det ByA
s0.

MATRICES AND DETERMINANTS
41
2. If A and B are positive semidefinite, then there exists a nonsingular
matrix Q such that
QAQsD ,
1
QBQsD ,
2
Ž
where D
and D
are diagonal matrices
for a detailed proof of this
1
2
.
result, see Newcomb, 1960 .
Theorem 2.3.15.
Let A , A , . . . , A
be symmetric matrices of order nn.
1
2
k
Then there exists an orthogonal matrix P such that
A sP P,
is1, 2, . . . , k,
i
i
where 
is a diagonal matrix, if and only if A A sA A
for all ij
i
i
j
j
i
Ž
.
i, js1, 2, . . . , k .
2.3.10. Bounds on Eigenvalues
Let A be a symmetric matrix of order nn. We denote the ith eigenvalue of
Ž .
A by e A , is1, 2, . . . , n. The smallest and largest eigenvalues of A are
i
Ž .
Ž .
denoted by e
A and e
A , respectively.
min
max
Ž .
Ž .
Theorem 2.3.16.
e
A FxAxrxxFe
A .
min
max
Proof. This follows directly from the spectral decomposition theorem.

The ratio xAxrxx is called Rayleigh’s quotient for A. The lower and
upper bounds in Theorem 2.3.16 can be achieved by choosing x to be an
Ž .
Ž .
eigenvector associated with e
A and e
A , respectively. Thus Theorem
min
max
2.3.16 implies that
xAx
inf
se
A ,
2.8
Ž .
Ž
.
min
xx
x0
xAx
sup
se
A .
2.9
Ž .
Ž
.
max
xx
x0
Theorem 2.3.17.
If A is a symmetric matrix and B is a positive definite
matrix, both of order nn, then
xAx
y1
y1
e
B
A F
Fe
B
A
Ž
.
Ž
.
min
max
xBx
Proof. The proof is left to the reader.


BASIC CONCEPTS IN LINEAR ALGEBRA
42
Note that the above lower and upper bounds are equal to the infimum and
supremum, respectively, of the ratio xAxrxBx for x0.
Theorem 2.3.18.
If A is a positive semidefinite matrix and B is a positive
Ž
.
definite matrix, both of order nn, then for any i is1, 2, . . . , n ,
e A e
B Fe AB Fe A e
B .
2.10
Ž .
Ž .
Ž
.
Ž .
Ž .
Ž
.
i
min
i
i
max
Ž
.
Furthermore, if A is positive definite, then for any i is1, 2, . . . , n ,
e2 AB
e2 AB
Ž
.
Ž
.
i
i
Fe A e B F
Ž .
Ž .
i
i
e
A e
B
e
A e
B
Ž .
Ž .
Ž .
Ž .
max
max
min
min
Ž
.
Proof. See Anderson and Gupta 1963, Corollary 2.2.1 .

Ž
.
A special case of the double inequality in 2.10 is
e
A e
B Fe AB Fe
A e
B ,
Ž .
Ž .
Ž
.
Ž .
Ž .
min
min
i
max
max
Ž
.
for all i is1, 2, . . . , n .
Theorem 2.3.19.
Let A and B be symmetric matrices of order nn.
Then, the following hold:
Ž .
Ž
.
1. e A Fe AqB , is1, 2, . . . , n, if B is nonnegative definite.
i
i
Ž .
Ž
.
2. e A e AqB , is1, 2, . . . , n, if B is positive definite.
i
i
Ž
.
Proof. See Bellman 1970, Theorem 3, page 117 .

Ž
.
Ž
.
Theorem 2.3.20 Schur’s Theorem .
Let As a
be a symmetric matrix
i j
	
	
of order nn, and let
A
denote its Euclidean norm, defined as
2
1r2
n
n
2
	
	
A
s
a
.
Ý Ý
2
i j
ž
/
is1 js1
Then
n
2
2
	
	
e
A s A
.
Ž .
Ý
2
i
is1
Ž
.
Proof. See Lancaster 1969, Theorem 7.3.1 .

	
	


Since
A
Fn max
a
, then from Theorem 2.3.20 we conclude that
2
i, j
i j




e
A
Fnmax a
.
Ž .
max
i j
i, j

APPLICATIONS OF MATRICES IN STATISTICS
43
Theorem 2.3.21.
Let A be a symmetric matrix of order nn, and let m
and s be defined as
1r2
2
tr A
tr A
Ž .
Ž
.
2
ms
,
ss
ym
.
ž
/
n
n
Then
s
1r2
mys ny1
Fe
A Fmy
,
Ž
.
Ž .
min
1r2
ny1
Ž
.
s
1r2
mq
Fe
A Fmqs ny1
,
Ž .
Ž
.
max
1r2
ny1
Ž
.
1r2
e
A ye
A Fs 2n
.
Ž .
Ž .
Ž
.
max
min
Ž
.
Proof. See Wolkowicz and Styan 1980, Theorems 2.1 and 2.5 .

2.4. APPLICATIONS OF MATRICES IN STATISTICS
The use of matrix algebra is quite prevalent in statistics. In fact, in the areas
of experimental design, linear models, and multivariate analysis, matrix
algebra is considered the most frequently used branch of mathematics.
Applications of matrices in these areas are well documented in several books,
Ž
.
Ž
.
for example, Basilevsky
1983 , Graybill
1983 , Magnus and Neudecker
Ž
.
Ž
.
1988 , and Searle 1982 . We shall therefore not attempt to duplicate the
material given in these books.
Let us consider the following applications:
2.4.1. The Analysis of the Balanced Mixed Model
In analysis of variance, a linear model associated with a given experimental
situation is said to be balanced if the numbers of observations in the
subclasses of the data are the same. For example, the two-way crossed-classi-
fication model with interaction,
y
sq q q 
q
,
2.11
Ž
.
Ž
.
i j
i jk
i
j
i jk
is1, 2, . . . , a; js1, 2, . . . , b; ks1, 2, . . . , n, is balanced, since there are n
observations for each combination of i and j. Here,  and  represent the
i
j
Ž
.
main effects of the factors under consideration,

denotes the interac-
i j
Ž
.
tion effect, and 
is a random error term. Model 2.11 can be written in
i jk
vector form as
ysH  qH  qH  qH  qH  ,
2.12
Ž
.
0
0
1 1
2
2
3
3
4
4

BASIC CONCEPTS IN LINEAR ALGEBRA
44
Ž
.
where y is the vector of observations,  s,  s  ,  , . . . ,  ,  s
0
1
1
2
a
2
Ž
.
wŽ
.
Ž
.
Ž
.
x
 ,  , . . . , 
,
 s

,

, . . . ,

,
and
 s
1
2
b
3
11
12
ab
4
Ž
.
Ž
.

, 
, . . . , 
. The matrices H
is0, 1, 2, 3, 4
can be expressed as
111
112
abn
i
direct products of the form
H s1 m1 m1 ,
0
a
b
n
H sI m1 m1 ,
1
a
b
n
H s1 mI m1 ,
2
a
b
n
H sI mI m1 ,
3
a
b
n
H sI mI mI .
4
a
b
n
In general, any balanced linear model can be written in vector form as

ys
H  ,
2.13
Ž
.
Ý
l
l
ls0
Ž
.
where H
ls0, 1, . . . , 
is a direct product of identity matrices and vectors
l
Ž
.
Ž
.
of ones
see Khuri, 1982 . If  ,  , . . . , 
y1
are fixed unknown
0
1

Ž
.
parameter vectors
fixed effects , and 
, 
, . . . , 
are random vectors
q1
q2

Ž
.
Ž
.
random effects , then model
2.11
is called a balanced mixed model.
Furthermore, if we assume that the random effects are independent and have
Ž
2
.
the normal distributions N 0, 	 I
, where c is the number of columns of
l
c
l
l
Ž
.
H , lsq1, q2, . . . , , then, because model 2.11 is balanced, its statisti-
l
cal analysis becomes very simple. Here, the 	 2’s are called the model’s
l
variance components. A balanced mixed model can be written as
ysXgqZh
2.14
Ž
.
where XgsÝ
H  is the fixed portion of the model, and ZhsÝ
H 
ls0
l
l
lsq1
l
l
is its random portion. The variancecovariance matrix of y is given by

2
s
A 	 ,
Ý
l
l
lsq1

 Ž
.
where A sH H
lsq1, q2, . . . ,  . Note that A A sA A
for all
l
l
l
l
p
p
l
Ž
lp. Hence, the matrices A can be diagonalized simultaneously see Theo-
l
.
rem 2.3.15 .
If yAy is a quadratic form in y, then yAy is distributed as a noncentral

2Ž .
chi-squared variate 
 if and only if A is idempotent of rank m, where
m
Ž
 is the noncentrality parameter and is given by sgXAXg see Searle,
.
1971, Section 2.5 .
The total sum of squares, yy, can be uniquely partitioned as

yys
yP y,
Ý
l
ls0

APPLICATIONS OF MATRICES IN STATISTICS
45
Ž
where the P ’s are idempotent matrices such that P P s0 for all ls see
l
l
s
.
Ž
.
Khuri, 1982 . The quadratic form yP y
ls0, 1, . . . , 
is positive semidefi-
l
Ž
.
nite and represents the sum of squares for the lth effect in model 2.13 .
Ž
.
Theorem 2.4.1.
Consider the balanced mixed model
2.14 , where the
random effects are assumed to be independently and normally distributed
2
Ž
with zero means and variancecovariance matrices 	 I
lsq1, q
l
cl
.
2, . . . ,  . Then we have the following:
1. yP y, yP y, . . . , yP y are statistically independent.
0
1

2. yP yr is distributed as a noncentral chi-squared variate with degrees
l
l
of freedom equal to the rank of P and noncentrality parameter given
l
by  sgXP Xgr
for ls0, 1, . . . , , where 
is a particular linear
l
l
l
l
combination of the variance components 	 2 , 	 2 , . . . , 	 2. However,
q1
q2

for lsq1, q2, . . . , , that is, for the random effects, yP yr
is
l
l
distributed as a central chi-squared variate with m degrees of freedom,
l
Ž
.
where m sr P .
l
l
Ž
.
Proof. See Theorem 4.1 in Khuri 1982 .

Theorem 2.4.1 provides the basis for a complete analysis of any balanced
mixed model, as it can be used to obtain exact tests for testing the signifi-
cance of the fixed effects and the variance components.
Ž
.
A linear function ag, of g in model 2.14 , is estimable if there exists a
Ž
.
linear function, cy, of the observations such that E cy sag. In Searle
Ž
.
1971, Section 5.4 it is shown that ag is estimable if and only if a belongs to
Ž
.
the linear span of the rows of X. In Khuri
1984
we have the following
theorem:
Ž
.
Theorem 2.4.2.
Consider the balanced mixed model in 2.14 . Then we
have the following:
Ž
.
Ž
.
1. r P X sr P , ls0, 1, . . . , .
l
l
Ž .

Ž
.
2. r X sÝ
r P X .
ls0
l
3. P Xg, P Xg, . . . , P Xg are linearly independent and span the space of all
0
1

estimable linear functions of g.
Theorem 2.4.2 is useful in identifying a basis of estimable linear functions
Ž
.
of the fixed effects in model 2.14 .
2.4.2. The Singular-Value Decomposition
The singular-value decomposition of a matrix is far more useful, both in
statistics and in matrix algebra, then is commonly realized. For example, it

BASIC CONCEPTS IN LINEAR ALGEBRA
46
plays a significant role in regression analysis. Let us consider the linear
model
ysXq ,
2.15
Ž
.
Ž
.
where y is a vector of n observations, X is an np nGp matrix consisting
of known constants,  is an unknown parameter vector, and  is a random
error vector. Using Theorem 2.3.11, the matrix X can be expressed as
w
x
XsP D : 0 Q,
2.16
Ž
.
where P and Q are orthogonal matrices of orders pp and nn, respec-
tively, and D is a diagonal matrix of order pp consisting of nonnegative
Ž
.
diagonal elements. These are the singular values of X or of X and are the
Ž
.
positive square roots of the eigenvalues of XX. From 2.16 we get
D
XsQ
P.
2.17
Ž
.
0
If the columns of X are linearly related, then they are said to be
Ž
.
multicollinear. In this case, X has rank r p , and the columns of X belong
to a vector subspace of dimension r. At least one of the eigenvalues of XX,
and hence at least one of the singular values of X, will be equal to zero. In
practice, such exact multicollinearities rarely occur in statistical applications.
Rather, the columns of X may be ‘‘nearly’’ linearly related. In this case, the
rank of X is p, but some of the singular values of X will be ‘‘near zero.’’ We
shall use the term multicollinearity in a broader sense to describe the latter
situation. It is also common to use the term ‘‘ill conditioning’’ to refer to the
same situation.
The presence of multicollinearities in X can have adverse effects on the
ˆ
Ž
.
least-squares estimate, , of  in 2.15 . This can be easily seen from the fact
ˆ
y1
ˆ
y1
2
2
Ž
.
Ž .
Ž
.
that s XX
Xy and Var  s XX
	 , where 	
is the error vari-
ˆ
ance. Large variances associated with the elements of  can therefore be
ˆ
expected when the columns of X are multicollinear. This causes  to become
an unreliable estimate of . For a detailed study of multicollinearity and its
Ž
.
effects, see Belsley, Kuh, and Welsch
1980, Chapter 3 , Montgomery and
Ž
.
Ž
.
Peck 1982, Chapter 8 , and Myers 1990, Chapter 3 .
The singular-value decomposition of X can provide useful information for
detecting multicollinearity, as we shall now see. Let us suppose that the
columns of X are multicollinear. Because of this, some of the singular values
Ž
.
Ž
.
of X, say p
p of them, will be ‘‘near zero.’’ Let us partition D in 2.17 as
2
D
0
1
Ds
,
0
D2

APPLICATIONS OF MATRICES IN STATISTICS
47
Ž
.
where D
and D
are of orders p p
and p p
p spyp
, respec-
1
2
1
1
2
2
1
2
tively. The diagonal elements of D
consist of those singular values of X
2
Ž
.
labeled as ‘‘near zero.’’ Let us now write 2.17 as
D
0
1
XPsQ
.
2.18
Ž
.
0
D2
0
0
w
x
w
x
Let us next partition P and Q as Ps P : P , Qs Q : Q , where P and
1
2
1
2
1
P have p and p columns, respectively, and Q and Q
have p and nyp
2
1
2
1
2
1
1
Ž
.
columns, respectively. From 2.18 we conclude that
XP sQ D ,
2.19
Ž
.
1
1
1
XP f0,
2.20
Ž
.
2
where f represents approximate equality. The matrix XP
is ‘‘near zero’’
2
because of the smallness of the diagonal elements of D .
2
Ž
.
We note from
2.20
that each column of P
provides a ‘‘near’’-linear
2
Ž
.
relationship among the columns of X. If 2.20 were an exact equality, then
the columns of P
would provide an orthonormal basis for the null space
2
of X.
We have mentioned that the presence of multicollinearity is indicated by
the ‘‘smallness’’ of the singular values of X. The problem now is to determine
what ‘‘small’’ is. For this purpose it is common in statistics to use the
Ž .
condition number of X, denoted by  X . By definition
max
 X s
,
Ž .
min
where 
and 
are, respectively, the largest and smallest singular
max
min
values of X. Since the singular values of X are the positive square roots of the
Ž .
eigenvalues of XX, then  X can also be written as
e
XX
Ž
.
max
 X s
.
Ž . ( e
XX
Ž
.
min
Ž .
If  X
is less than 10, then there is no serious problem with multi-
Ž .
collinearity. Values of  X between 10 and 30 indicate moderate to strong
multicollinearity, and if 30, severe multicollinearity is implied.
More detailed discussions concerning the use of the singular-value decom-
Ž
.
position in regression can be found in Mandel
1982 . See also Lowerre
Ž
.
Ž
.
1982 . Good 1969 described several applications of this decomposition in
statistics and in matrix algebra.

BASIC CONCEPTS IN LINEAR ALGEBRA
48
2.4.3. Extrema of Quadratic Forms
Ž
In many statistical problems there is a need to find the extremum maximum
.
or minimum of a quadratic form or a ratio of quadratic forms. Let us, for
example, consider the following problem:
Let X , X , . . . , X
be a collection of random vectors, all having the same
1
2
n
number of elements. Suppose that these vectors are independently and
Ž
.
Ž
.
identically distributed i.i.d. as N ,  , where both  and  are unknown.
Consider testing the hypothesis H : s versus its alternative H :  ,
0
0
a
0
where 
is some hypothesized value of . We need to develop a test
0
statistic for testing H .
0
The multivariate hypothesis H
is true if and only if the univariate
0
hypotheses
H
 : s
Ž .
0
0
Ž
.
are true for all 0. A test statistic for testing H
 is the following:
0
'
 Xy
n
Ž
.
0
t  s
,
Ž
.
'S
n
where XsÝ
X rn and S is the sample variancecovariance matrix, which
is1
i
is an unbiased estimator of , and is given by
n
1
Ss
X yX
X yX .
Ž
.Ž
.
Ý
i
i
ny1 is1
2Ž
.
Ž
.
Large values of t
 indicate falsehood of H
 . Since H is rejected if
0
0
Ž
.
and only if H
 is rejected for at least one , then the condition to reject
0
w 2Ž
.x
H at the -level is sup
t

c , where c
is the upper 100% point
0
  0


w 2Ž
.x
of the distribution of sup
t
 . But
  0
2


n  Xy
Ž
.
0
2
sup t

s sup
Ž
.
S
0
0
 Xy
Xy

Ž
.Ž
.
0
0
sn sup
S
0
y1
sn e
S
Xy
Xy
 ,
Ž
.Ž
.
max
0
0
by Theorem 2.3.17.
Now,
y1
y1
e
S
Xy
Xy
 se
Xy
S
Xy
,
Ž
.Ž
.
Ž
.
Ž
.
max
0
0
max
0
0
y1
s Xy
S
Xy
.
Ž
.
Ž
.
0
0
by Theorem 2.3.9.

APPLICATIONS OF MATRICES IN STATISTICS
49
Hence,
2
y1
sup t

sn Xy
S
Xy
Ž
.
Ž
.
Ž
.
0
0
0
is the test statistic for the multivariate hypothesis H . This is called Hotelling’s
0
T 2-statistic. Its critical values are obtained in terms of the critical values of
Ž
.
the F-distribution see, for example, Morrison, 1967, Chapter 4 .
Another example of using the extremum of a ratio of quadratic forms is in
the determination of the canonical correlation coefficient between two ran-
Ž
.
Ž
.
dom vectors see Exercise 2.26 . The article by Bush and Olkin 1959 lists
several similar statistical applications.
2.4.4. The Parameterization of Orthogonal Matrices
Orthogonal matrices are used frequently in statistics, especially in linear
Ž
models and multivariate analysis see, for example, Graybill, 1961, Chapter
.
11; James, 1954 .
2
Ž
.
The n elements of an nn orthogonal matrix Q are subject to n nq1 r2
constraints because QQsI . These elements can therefore be represented
n
2
Ž
.
Ž
.
by n yn nq1 r2sn ny1 r2 independent parameters. The need for such
a representation arises in several situations. For example, in the design of
experiments, there may be a need to search for an orthogonal matrix that
satisfies a certain optimality criterion. Using the independent parameters of
Ž
.
an orthogonal matrix can facilitate this search. Khuri and Myers
1981
followed this approach in their construction of a response surface design that
is robust to nonnormality of the error distribution associated with the
response function. Another example is the generation of random orthogonal
matrices for carrying out simulation experiments. This was used by Heiberger,
Ž
.
Velleman, and Ypelaar 1983 to construct test data with special properties
Ž
.
for multivariate linear models. Anderson, Olkin, and Underhill 1987 pro-
posed a procedure to generate random orthogonal matrices.
Methods to parameterize an orthogonal matrix were reviewed in Khuri
Ž
.
and Good 1989 . One such method is to use the relationship between an
orthogonal matrix and a skew-symmetric matrix. If Q is an orthogonal matrix
with determinant equal to one, then it can be written in the form
QseT,
Ž
.
where T is a skew-symmetric matrix see, for example, Gantmacher, 1959 .
The elements of T above its main diagonal can be used to parameterize Q.
This exponential mapping is defined by the infinite series
T 2
T 3
T
e sIqTq
q
q  .
2!
3!

BASIC CONCEPTS IN LINEAR ALGEBRA
50
The exponential parameterization was used in a theorem concerning the
asymptotic
joint
density
function
of
the
eigenvalues
of
the
sample
Ž
.
variancecovariance matrix Muirhead, 1982, page 394 .
Another parameterization of Q is given by
y1
Qs IyU
IqU
,
Ž
. Ž
.
where U is a skew-symmetric matrix. This relationship is valid provided that
Q does not have the eigenvalue y1. Otherwise, Q can be written as
y1
QsL IyU
IqU
,
Ž
. Ž
.
where L is a diagonal matrix in which each element on the diagonal is either
Ž
.
1 or y1. Arthur Cayley 18211895 is credited with having introduced the
relationship between Q and U.
Ž
.
Finally, the recent article by Olkin 1990 illustrates the strong interplay
between statistics and linear algebra. The author listed several areas of
statistics with a strong linear algebra component.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Anderson, T. W., and S. D. Gupta 1963 . ‘‘Some inequalities on characteristic roots
of matrices,’’ Biometrika, 50, 522524.
Ž
.
Anderson, T. W., I. Olkin, and L. G. Underhill
1987 . ‘‘Generation of random
orthogonal matrices.’’ SIAM J. Sci. Statist. Comput., 8, 625629.
Ž
.
Basilevsky, A. 1983 . Applied Matrix Algebra in the Statistical Sciences. North-Holland,
Ž
New York. This book addresses topics in matrix algebra that are useful in both
.
applied and theoretical branches of the statistical sciences.
Ž
.
Bellman, R. 1970 . Introduction to Matrix Analysis, 2nd ed. McGraw-Hill, New York.
ŽAn excellent reference book on matrix algebra. The minimummaximum char-
acterization of eigenvalues is discussed in Chap. 7. Kronecker products are
studied in Chap. 12. Some applications of matrices to stochastic processes and
.
probability theory are given in Chap. 14.
Ž
.
Belsley, D. A., E. Kuh, and R. E. Welsch 1980 . Regression Diagnostics. Wiley, New
Ž
York.
This is a good reference for learning about multicollinearity in linear
statistical models that was discussed in Section 2.4.2. Examples are provided
.
based on actual econometric data.
Ž
.
Bush, K. A., and I. Olkin 1959 . ‘‘Extrema of quadratic forms with applications to
statistics.’’ Biometrika, 46, 483486.
Ž
.
Gantmacher, F. R. 1959 . The Theory of Matrices, Vols. I and II. Chelsea, New York.
ŽThese two volumes provide a rather more advanced study of matrix algebra than
standard introductory texts. Methods to parameterize an orthogonal matrix,
.
which were mentioned in Section 2.4.4, are discussed in Vol. I.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
51
Ž
.
Golub, G. H., and C. F. Van Loan
1983 . Matrix Computations. Johns Hopkins
University Press, Baltimore, Maryland.
Ž
.
Good, I. J. 1969 . ‘‘Some applications of the singular decomposition of a matrix.’’
Technometrics, 11, 823831.
Ž
.
Graybill, F. A. 1961 . An Introduction to Linear Statistical Models, Vol. I. McGraw-Hill,
Ž
New York. This is considered a classic textbook in experimental statistics. It is
concerned with the mathematical treatment, using matrix algebra, of linear
.
statistical models.
Ž
.
Graybill, F. A.
1983 . Matrices with Applications in Statistics, 2nd ed. Wadsworth,
Ž
Belmont, California. This frequently referenced textbook contains a great num-
ber of theorems in matrix algebra, and describes many properties of matrices that
.
are pertinent to linear model and mathematical statistics.
Ž
.
Ž
Healy, M. J. R. 1986 . Matrices for Statistics. Clarendon Press, Oxford, England. This
is a short book that provides a brief coverage of some basic concepts in matrix
.
algebra. Some applications in statistics are also mentioned.
Ž
.
Heiberger, R. M., P. F. Velleman, and M. A. Ypelaar 1983 . ‘‘Generating test data
with independently controllable features for multivariate general linear forms.’’
J. Amer. Statist. Assoc., 78, 585595.
Ž
.
Henderson, H. V., F. Pukelsheim, and S. R. Searle
1983 . ‘‘On the history of the
Kronecker product.’’ Linear and Multilinear Algebra, 14, 113120.
Ž
.
Henderson, H. V., and S. R. Searle
1981 . ‘‘The vec-permutation matrix, the vec
operator and Kronecker products: A review.’’ Linear and Multilinear Algebra, 9,
271288.
Ž
.
Hoerl, A. E., and R. W. Kennard
1970 . ‘‘Ridge regression: Applications to
nonorthogonal problems.’’ Technometrics, 12, 6982.
Ž
.
James, A. T. 1954 . ‘‘Normal multivariate analysis and the orthogonal group.’’ Ann.
Math. Statist., 25, 4075.
Ž
.
Khuri, A. I. 1982 . ‘‘Direct products: A powerful tool for the analysis of balanced
data.’’ Comm. Statist. Theory Methods, 11, 29032920.
Ž
.
Khuri, A. I. 1984 . ‘‘Interval estimation of fixed effects and of functions of variance
Ž
components in balanced mixed models.’’ Sankhya, Series B, 46, 1028. Section 5
in this article gives a procedure for the construction of exact simultaneous
confidence intervals on estimable linear functions of the fixed effects in a
.
balanced mixed model.
Ž
.
Khuri, A. I., and I. J. Good 1989 . ‘‘The parameterization of orthogonal matrices:
A review mainly for statisticians.’’ South African Statist. J., 23, 231250.
Ž
.
Khuri, A. I., and R. H. Myers 1981 . ‘‘Design related robustness of tests in regression
models.’’ Comm. Statist. Theory Methods, 10, 223235.
Ž
.
Ž
Lancaster, P. 1969 . Theory of Matrices. Academic Press, New York. This book is
written primarily for students of applied mathematics, engineering, or science
who want to acquire a good knowledge of the theory of matrices. Chap. 7 has an
interesting discussion concerning the behavior of matrix eigenvalues under per-
.
turbation of the elements of the matrix.
Ž
.
Lowerre, J. M. 1982 . ‘‘An introduction to modern matrix methods and statistics.’’
Ž
Amer. Statist., 36, 113115. An application of the singular-value decomposition
.
is given in Section 2 of this article.

BASIC CONCEPTS IN LINEAR ALGEBRA
52
Ž
.
Magnus, J. R., and H. Neudecker 1988 . Matrix Differential Calculus with Applications
Ž
in Statistics and Econometrics. Wiley, New York. This book consists of six parts.
Part one deals with the basics of matrix algebra. The remaining parts are devoted
to the development of matrix differential calculus and its applications to statistics
and econometrics. Part four has a chapter on inequalities concerning eigenvalues
.
that pertains to Section 2.3.10 in this chapter.
Ž
.
Mandel, J. 1982 . ‘‘Use of the singular-value decomposition in regression analysis.’’
Amer. Statist., 36, 1524.
Ž
.
Marcus, M., and H. Minc 1988 . Introduction to Linear Algebra. Dover, New York.
ŽThis book presents an introduction to the fundamental concepts of linear
.
algebra and matrix theory.
Ž
.
Marsaglia, G., and G. P. H. Styan 1974 . ‘‘Equalities and inequalities for ranks of
Ž
matrices.’’ Linear and Multilinear Algebra, 2, 269292.
This is an interesting
collection of results on ranks of matrices. It includes a wide variety of equalities
.
and inequalities for ranks of products, of sums, and of partitioned matrices.
Ž
.
May, W. G. 1970 . Linear Algebra. Scott, Foresman and Company, Glenview, Illinois.
Ž
.
Montgomery, D. C., and E. A. Peck 1982 . Introduction to Linear Regression Analysis.
Ž
Wiley, New York. Chap. 8 in this book has an interesting discussion concerning
multicollinearity. It includes the sources of multicollinearity, its harmful effects in
regression, available diagnostics, and a survey of remedial measures. This chapter
.
provides useful additional information to the material in Section 2.4.2.
Ž
.
Morrison, D. F.
1967 . Multi®ariate Statistical Methods. McGraw-Hill, New York.
Ž
.
This book can serve as an introductory text to multivariate analysis.
Ž
.
Muirhead, R. J. 1982 . Aspects of Multi®ariate Statistical Theory. Wiley, New York.
ŽThis book is designed as a text for a graduate-level course in multivariate
.
analysis.
Ž
.
Myers, R. H.
1990 . Classical and Modern Regression with Applications, 2nd ed.
Ž
PWS-Kent, Boston. Chap. 8 in this book should be useful reading concerning
.
multicollinearity and its effects.
Ž
.
Newcomb, R. W. 1960 . ‘‘On the simultaneous diagonalization of two semidefinite
matrices.’’ Quart. Appl. Math., 19, 144146.
Ž
.
Olkin, I. 1990 . ‘‘Interface between statistics and linear algebra.’’ In Matrix Theory
and Applications, Vol. 40, C. R. Johnson, ed., American Mathematical Society,
Providence, Rhode Island, pp. 233256.
Ž
.
Price, G. B. 1947 . ‘‘Some identities in the theory of determinants.’’ Amer. Math.
Ž
Monthly, 54, 7590. Section 10 in this article gives some history of the theory of
.
determinants.
Ž
.
Rogers, G. S. 1984 . ‘‘Kronecker products in ANOVAa first step.’’ Amer. Statist.,
38, 197202.
Ž
.
Searle, S. R. 1971 . Linear Models. Wiley, New York.
Ž
.
Ž
Searle, S. R. 1982 . Matrix Algebra Useful for Statistics. Wiley, New York. This is a
useful book introducing matrix algebra in a manner that is helpful in the
statistical analysis of data and in statistics in general. Chaps. 13, 14, and 15
.
present applications of matrices in regression and linear models.
Ž
.
Ž
Seber, G. A. F. 1984 . Multi®ariate Obser®ations. Wiley, New York. This is a good
reference on applied multivariate analysis that is suited for a graduate-level
.
course.

EXERCISES
53
Ž
.
Ž
Smith, D. E. 1958 . History of Mathematics. Vol. I. Dover, New York. This interest-
ing book contains, among other things, some history concerning the development
.
of the theory of determinants and matrices.
Ž
.
Wolkowicz, H., and G. P. H. Styan
1980 . ‘‘Bounds for eigenvalues using traces.’’
Linear Algebra Appl., 29, 471506.
EXERCISES
In Mathematics
2.1. Show that a set of n1 vectors, u , u , . . . , u , is always linearly
1
2
m
dependent if mn.
Ž
.
2.2. Let W
be a vector subspace of V such that WsL u , u , . . . , u
,
1
2
n
Ž
.
where the u ’s
is1, 2, . . . , n
are linearly independent. If v is any
i
vector in V that is not in W, then the vectors u , u , . . . , u , v are
1
2
n
linearly independent.
2.3. Prove Theorem 2.1.3.
2.4. Prove part 1 of Theorem 2.2.1.
2.5. Let T: U™V be a linear transformation. Show that T is one-to-one if
and only if whenever u , u , . . . , u
are linearly independent in U, then
1
2
n
Ž
.
Ž
.
Ž
.
T u , T u
, . . . , T u
are linearly independent in V.
1
2
n
2.6. Let T: R ™R
be represented by an nm matrix of rank .
n
m
( )
w Ž
.x
a
Show that dim T R
s.
n
( )
b
Show that if nFm and sn, then T is one-to-one.
Ž
.
2.7. Show that tr AA s0 if and only if As0.
2.8. Let A be a symmetric positive semidefinite matrix of order nn. Show
that vAvs0 if and only if Avs0.
2.9. The matrices A and B are symmetric and positive semidefinite of order
nn such that ABsBA. Show that AB is positive semidefinite.
2.10. If A is a symmetric nn matrix, and B is an nn skew-symmetric
Ž
.
matrix, then show that tr AB s0.
Ž
.
2.11. Suppose that tr PA s0 for every skew-symmetric matrix P. Show that
the matrix A is symmetric.

BASIC CONCEPTS IN LINEAR ALGEBRA
54
2.12. Let A be an nn matrix and C be a nonsingular matrix of order nn.
Show that A, Cy1AC, and CACy1 have the same set of eigenvalues.
2.13. Let A be an nn symmetric matrix, and let  be an eigenvalue of A of
multiplicity k. Then AyI
has rank nyk.
n
2.14. Let A be a nonsingular matrix of order nn, and let c and d be n1
vectors. If dAy1cy1, then
Ay1c
dAy1
Ž
. Ž
.
y1
y1
Aqcd
sA
y
.
Ž
.
y1
1qdA
c
This is known as the Sherman-Morrison formula.
2.15. Show that if A and I qVAy1U are nonsingular, then
k
y1
y1
y1
y1
y1
y1
AqUV
sA
yA
U I qVA
U
VA
,
Ž
.
Ž
.
k
where A is of order nn, and U and V are of order nk. This result
is known as the Sherman-Morrison-Woodbury formula and is a general-
ization of the result in Exercise 2.14.
2.16. Prove Theorem 2.3.17.
2.17. Let A and B be nn idempotent matrices. Show that AyB is
idempotent if and only if ABsBAsB.
2.18. Let A be an orthogonal matrix. What can be said about the eigenvalues
of A?
2.19. Let A be a symmetric matrix of order nn, and let L be a matrix of
order nm. Show that
e
A tr LL Ftr LAL Fe
A tr LL
Ž .
Ž
.
Ž
.
Ž .
Ž
.
min
max
2.20. Let A be a nonnegative definite matrix of order nn, and let L be a
matrix of order nm. Show that
( )
Ž
.
Ž .
Ž
.
a
e
LAL Ge
A e
LL ,
min
min
min
( )
Ž
.
Ž .
Ž
.
b
e
LAL Fe
A e
LL .
max
max
max
2.21. Let A and B be nn symmetric matrices with A nonnegative definite.
Show that
e
B tr A Ftr AB Fe
B tr A .
Ž .
Ž .
Ž
.
Ž .
Ž .
min
max

EXERCISES
55
2.22. Let Ay be a g-inverse of A. Show that
( )
y
a
A A is idempotent,
( )
Ž
y.
Ž .
b
r A
Gr A ,
( )
Ž .
Ž
y .
c
r A sr A A .
In Statistics
2
Ž
.
Ž
.
2.23. Let ys y , y , . . . , y  be a normal random vector N 0, 	 I
. Let y
1
2
n
n
and s2 be the sample mean and sample variance given by
n
1
ys
y ,
Ý
i
n is1
2
n
n
1
Ý
y
Ž
.
is1
i
2
2
s s
y y
.
Ý
i
ny1
n
is1
( )
a
Show that A is an idempotent matrix of rank ny1, where A is an
Ž
.
2
nn matrix such that yAys ny1 s .
( )
Ž
.
2
2
b
What distribution does ny1 s r	
have?
2
( )
c
Show that y and Ay are uncorrelated; then conclude that y and s
are statistically independent.
2.24. Consider the one-way classification model
y sq q ,
is1, 2, . . . , a;
js1, 2, . . . , n ,
i j
i
i j
i
Ž
.
where  and 
is1, 2, . . . , a are unknown parameters and 
is a
i
i j
random error with a zero mean. Show that
( )
Ž
a
 y
is an estimable linear function for all
ii
i, is
i
i
.
1, 2, . . . , a ,
( )
b
 is nonestimable.
2.25. Consider the linear model
ysXq ,
Ž
.
where X is a known matrix of order np and rank r
Fp ,  is an
unknown parameter vector, and  is a random error vector such that
Ž .
Ž .
2
E  s0 and Var  s	 I .
n
( )
Ž
.y
a
Show that X XX
X is an idempotent matrix.
( )
b
Let ly be an unbiased linear estimator of . Show that
ˆ
Var  FVar ly ,
Ž
.
Ž
.
ˆ
y
Ž
.
where s XX
Xy.
Ž .
The result given in part b is known as the GaussMarko® theorem.

BASIC CONCEPTS IN LINEAR ALGEBRA
56
Ž .
2.26. Consider the linear model in Exercise 2.25, and suppose that r X sp.
Ž
.
Hoerl and Kennard
1970
introduced an estimator of  called the
ridge estimator *:
y1
*s XXqkI
Xy,
Ž
.
p
where k is a ‘‘small’’ fixed number. For an appropriate value of k, *
provides improved accuracy in the estimation of  over the least-squares
ˆ
y1
Ž
.
estimator s XX
Xy. Let XXsP P be the spectral decomposi-
ˆ
tion of XX. Show that *sPDP, where D is a diagonal matrix
Ž
.
whose ith diagonal element is  r  qk , is1, 2, . . . , p, and where
i
i
 , , . . . , 
are the diagonal elements of .
1
2
p
2.27. Consider the ratio
2
xAy
Ž
.
2
 s
,
xB x
yB y
Ž
. Ž
.
1
2
where A is a matrix of order mn and B , B
are positive definite of
1
2
orders mm and nn, respectively. Show that
sup  2se
By1ABy1A .
Ž
.
max
1
2
x, y
w Hint: Define C and C
as symmetric nonsingular matrices such that
1
2
C2sB , C2sB . Let C xsu, C ysv. Then  2 can be written as
1
1
2
2
1
2
2
y1
y1
uC
AC
v
Ž
.
2
1
2
2
y1
y1
 s
s C
AC

,
Ž
.
1
2
uu
vv
Ž
. Ž
.
Ž
.1r2
Ž
.1r2
where sur uu
, svr vv
are unit vectors. Verify the result
of this problem after noting that  2 is now the square of a dot
x
product.
Note: This exercise has the following application in multivariate analy-
sis: Let z
and z
be random vectors with zero means and variance
1
2
covariance matrices  , 
, respectively. Let 
be the covariance
11
22
12
matrix of z
and z . On choosing As
, B s , B s
, the
1
2
12
1
11
2
22
positive square root of the supremum of  2 is called the canonical
correlation coefficient between z
and z . It is a measure of the linear
1
2
Ž
association between z
and z
see, for example, Seber, 1984, Section
1
2
.
5.7 .

C H A P T E R
3
Limits and Continuity of Functions
The notions of limits and continuity of functions lie at the kernel of calculus.
The general concept of continuity is very old in mathematics. It had its
Ž
.
inception long ago in ancient Greece. We owe to Aristotle 384322 B.C.
the first known definition of continuity: ‘‘A thing is continuous when of any
two successive parts the limits at which they touch are one and the same and
Ž
.
are, as the word implies, held together’’ see Smith, 1958, page 93 . Our
present definitions of limits and continuity of functions, however, are sub-
Ž
.
stantially those given by Augustin-Louis Cauchy 17891857 .
In this chapter we introduce the concepts of limits and continuity of
real-valued functions, and study some of their properties. The domains of
definition of the functions will be subsets of R, the set of real numbers.
A typical subset of R will be denoted by D.
3.1. LIMITS OF A FUNCTION
Before defining the notion of a limit of a function, let us understand what is
meant by the notation x™a, where a and x are elements in R. If a is finite,
then x™a means that x can have values that belong to a neighborhood
Ž .
Ž
.


N a of a see Definition 1.6.1 for any r0, but xa, that is, 0 xya 
r
r. Such a neighborhood is called a deleted neighborhood of a, that is, a
Ž
neighborhood from which the point a has been removed. If a is infinite y
.
 
or q , then x™a indicates that
x can get larger and larger without any
 
constraint on the extent of its increase. Thus x can have values greater than
any positive number. In either case, whether a is finite or infinite, we say that
x tends to a or approaches a.
Ž .
Let us now study the behavior of a function f x as x™a.
Ž .
Definition 3.1.1.
Suppose that the function f x
is defined in a deleted
Ž .
neighborhood of a point agR. Then f x is said to have a limit L as x™a
57

LIMITS AND CONTINUITY OF FUNCTIONS
58
if for every 0 there exists a 0 such that
f x yL 
3.1
Ž .
Ž
.
for all x for which


0 xya .
3.2
Ž
.
Ž .
In this case, we write f x ™L as x™a, which is equivalent to saying that
Ž .
Ž .
lim
f x sL. Less formally, we say that f x ™L as x™a if, however
x™a
Ž .
small the positive number  might be, f x differs from L by less than  for
values of x sufficiently close to a.

Ž .
NOTE 3.1.1.
When f x has a limit as x™a, it is considered to be finite.
Ž .
Ž
.
If this is not the case, then f x is said to have an infinite limit y or q
as x™a. This limit exists only in the extended real number system, which
consists of the real number system combined with the two symbols, y and
q. In this case, for every positive number M there exists a 0 such that
 Ž .


Ž .
f x
M if 0 xya . If a is infinite and L is finite, then f x ™L as
x™a if for any 0 there exists a positive number N such that inequality
Ž
.
 
3.1 is satisfied for all x for which x N. In case both a and L are infinite,
Ž .
then f x ™L as x™a if for any B0 there exists a positive number A
 Ž .
 
such that f x
B if x A.
Ž .
NOTE 3.1.2.
If f x has a limit L as x™a, then L must be unique. To
Ž .
show this, suppose that L and L are two limits of f x as x™a. Then, for
1
2
any 0 there exist  0,  0 such that
1
2



f x yL 
,
if 0 xya  ,
Ž .
1
1
2



f x yL

,
if 0 xya  .
Ž .
2
2
2
Ž
.
Hence, if smin  , 
, then
1
2


L yL
s L yf x qf x yL
Ž .
Ž .
1
2
1
2
F f x yL q f x yL
Ž .
Ž .
1
2





for all x for which 0 xya . Since L yL
is smaller than , which is
1
2
Ž
.
an arbitrary positive number, we must have L sL
why? .
1
2
Ž .
NOTE 3.1.3.
The limit of f x as described in Definition 3.1.1 is actually
called a two-sided limit. This is because x can approach a from either side.
Ž .
There are, however, cases where f x
can have a limit only when x ap-
proaches a from one side. Such a limit is called a one-sided limit.

LIMITS OF A FUNCTION
59
Ž .
By definition, if
f x
has a limit as
x approaches a from the left,
y
Ž .
symbolically written as x™a , then f x
has a left-sided limit, which we
denote by Ly. In this case we write
lim f x sLy.
Ž .
y
x™a
Ž .
If, however, f x has a limit as x approaches a from the right, symbolically
q
Ž .
q
written as x™a , then f x has a right-sided limit, denoted by L , that is,
lim f x sLq.
Ž .
q
x™a
Ž .
y
From the above definition it follows that f x has a left-sided limit L
as
x™ay if for every 0 there exists a 0 such that
y
f x yL

Ž .
Ž .
q
for all x for which 0ayx. Similarly, f x has a right-sided limit L
as
x™aq if for every 0 there exists a 0 such that
q
f x yL

Ž .
for all x for which 0xya.
Ž .
y
q
Obviously, if f x has a two-sided limit L as x™a, then L
and L
both
y
q
Ž .
exist and are equal to L. Vice versa, if L sL , then f x has a two-sided
y
q Ž
.
limit L as x™a, where L is the common value of L
and L
why? . We
Ž .
can then state that lim
f x sL if and only if
x™a
lim f x s lim f x sL.
Ž .
Ž .
y
q
x™a
x™a
Ž .
Thus to determine if f x has a limit as x™a, we first need to find out if it
has a left-sided limit Ly and a right-sided limit Lq as x™a. If this is the
y
q
Ž .
case and L sL sL, then f x has a limit L as x™a.
Throughout the remainder of the book, we shall drop the characterization
Ž .
‘‘two-sided’’ when making a reference to a two-sided limit L of f x . Instead,
Ž .
we shall simply state that L is the limit of f x .
EXAMPLE 3.1.1.
Consider the function
xy1 r x 2y1 ,
xy1, 1,
Ž
. Ž
.
f x s
Ž . ½ 4,
xs1.
This function is defined everywhere except at xsy1. Let us find its limit as
x™a, where agR. We note that
xy1
1
lim f x s lim
s lim
.
Ž .
2
xq1
x y1
x™a
x™a
x™a

LIMITS AND CONTINUITY OF FUNCTIONS
60
This is true even if as1, because xa as x™a. We now claim that if
ay1, then
1
1
lim
s
.
xq1
aq1
x™a
To prove this claim, we need to find a 0 such that for any 0,
1
1
y

3.3
Ž
.
xq1
aq1


if 0 xya . Let us therefore consider the following two cases:
CASE 1.
ay1. In this case we have


1
1
xya
y
s
.
3.4
Ž
.

 

xq1
aq1
xq1
aq1


If xya , then
ayq1xq1aqq1.
3.5
Ž
.
Since aq10, we can choose 0 such that
ayq10, that is,
Ž
.
Ž
.
aq1. From 3.4 and 3.5 we then get
1
1

y

.
xq1
aq1
aq1
ayq1
Ž
. Ž
.
Let us constrain  even further by requiring that

.
aq1
ayq1
Ž
. Ž
.
This is accomplished by choosing 0 so that
2
aq1

Ž
.

.
1q aq1 
Ž
.
Since
2
aq1

Ž
.
aq1,
1q aq1 
Ž
.
Ž
.


inequality 3.3 will be satisfied by all x for which xya , where
2
aq1

Ž
.
0
.
3.6
Ž
.
1q aq1 
Ž
.

LIMITS OF A FUNCTION
61
CASE 2.
ay1. Here, we choose 0 such that aqq10, that is,
Ž
.
Ž
.
y aq1 . From 3.5 we conclude that


xq1 y aqq1 .
Ž
.
Ž
.
Hence, from 3.4 we get
1
1

y

.
xq1
aq1
aq1
aqq1
Ž
. Ž
.
As before, we further constrain  by requiring that it satisfy the inequality

 ,
aq1
aqq1
Ž
. Ž
.
or equivalently, the inequality
2
aq1

Ž
.

.
1y aq1 
Ž
.
Note that
2
aq1

Ž
.
y aq1 .
Ž
.
1y aq1 
Ž
.
Ž
.
Consequently, inequality 3.3 can be satisfied by choosing  such that
2
aq1

Ž
.
0
.
3.7
Ž
.
1y aq1 
Ž
.
Ž
.
Ž
.
Cases 1 and 2 can be combined by rewriting 3.6 and 3.7 using the single
double inequality

 2
aq1 
0
.


1q aq1 
If asy1, then no limit exists as x™a. This is because
1
lim f x s lim
.
Ž .
xq1
x™y1
x™y1

LIMITS AND CONTINUITY OF FUNCTIONS
62
If x™y1y, then
1
lim
sy,
y xq1
x™y1
and, as x™y1q,
1
lim
s.
q xq1
x™y1
Since the left-sided and right-sided limits are not equal, no limit exists as
x™y1.
Ž .
EXAMPLE 3.1.2.
Let f x be defined as
'
1q x ,
xG0,
f x s
Ž . ½ x,
x0.
This function has no limit as x™0, since
lim f x s lim xs0,
Ž .
y
y
x™0
x™0
'
lim f x s lim
1q x s1.
Ž .
Ž
.
q
q
x™0
x™0
Ž .
However, for any a0, lim
f x exists.
x™a
Ž .
EXAMPLE 3.1.3.
Let f x be given by
x cos x,
x0,
f x s
Ž . ½ 0,
xs0.
Ž .
Figure 3.1. The graph of the function f x .

SOME PROPERTIES ASSOCIATED WITH LIMITS OF FUNCTIONS
63
Ž .
 Ž .
 
Then lim
f x s0. This is true because
f x
F x
in any deleted
x™0
Ž .
neighborhood of as0. As x™, f x oscillates unboundedly, since
yxFx cos xFx.
Ž .
Thus f x has no limit as x™. A similar conclusion can be reached when
Ž
.
x™y see Figure 3.1 .
3.2. SOME PROPERTIES ASSOCIATED WITH LIMITS OF FUNCTIONS
The following theorems give some fundamental properties associated with
function limits.
Ž .
Ž .
Theorem 3.2.1.
Let f x
and g x
be real-valued functions defined on
Ž .
Ž .
D;R. Suppose that lim
f x sL and lim
g x sM. Then
x™a
x™a
w Ž .
Ž .x
1. lim
f x qg x sLqM,
x™a
w Ž . Ž .x
2. lim
f x g x sLM,
x™a
w
Ž .x
3. lim
1rg x s1rM if M0,
x™a
w Ž .
Ž .x
4. lim
f x rg x sLrM if M0.
x™a
Proof. We shall only prove parts 2 and 3. The proof of part 1 is straight-
forward, and part 4 results from applying parts 2 and 3.
Proof of Part 2. Consider the following three cases:
CASE 1.
Both L and M are finite. Let 0 be given and let 0 be
such that




 q L q M
.
3.8
Ž
.
Ž
.
This inequality is satisfied by all values of  for which
2








'
y
L q M
q
L q M
q4
Ž
.
Ž
.
0
.
2
Now, there exist   0 such that
1
2


f x yL 
if 0 xya  ,
Ž .
1


g x yM 
if 0 xya  .
Ž .
2

LIMITS AND CONTINUITY OF FUNCTIONS
64


Ž
.
Then, for any x such that 0 xya  where smin  , 
,
1
2
f x g x yLM s M f x yL qf x
g x yM
Ž . Ž .
Ž .
Ž .
Ž .


F M q f x
Ž .




F M q
L q f x yL
Ž .




F q L q M
Ž
.
 ,
which proves part 2.
CASE 2.
One of L and M is finite and the other is infinite. Without any
loss of generality, we assume that L is finite and Ms. Let us also assume
that L0, since 0 is indeterminate. Let A0 be given. There exists a
 Ž .




Ž
.
 0 such that
f x
 L r2 if 0 xya 
why? . Also, there exists
1
1
 Ž .




Ž
.
a  0 such that g x
2 Ar L if 0 xya  . Let smin  , 
.
2
2
1
2


If 0 xya , then
f x g x
s f x
g x
Ž . Ž .
Ž .
Ž .


L
2

AsA.


2
L
Ž . Ž .
This means that lim
f x g x s, which proves part 2.
x™a
CASE 3.
Both L and M are infinite. Suppose that Ls, Ms. In this
case, for a given B0 there exist  0,  0 such that
1
2
'


f x
 B
if 0 xya  ,
Ž .
1
'


g x
 B
if 0 xya  .
Ž .
2
Then,


f x g x
B,
if 0 xya  ,
Ž . Ž .
Ž
.
Ž . Ž .
where smin  , 
. This implies that lim
f x g x s, which proves
1
2
x™a
part 2.
Proof of Part 3. Let 0 be given. If M0, then there exists a  0
1
 Ž .




such that g x
 M r2 if 0 xya  . Also, there exists a  0 such
1
2
 Ž .

2


that g x yM M r2 if 0 xya  . Then,
2
1
1
g x yM
Ž .
y
s


g x
M
g x
M
Ž .
Ž .
2 g x yM
Ž .

2


M
 ,


Ž
.
if 0 xya , where smin  ,  .

1
2

THE O, O NOTATION
65
Ž .
Theorem 3.2.1 is also true if L and M are one-sided limits of f x
and
Ž .
g x , respectively.
Ž .
Ž .
Ž .
Ž .
Theorem 3.2.2.
If f x Fg x , then lim
f x Flim
g x .
x™a
x™a
Ž .
Ž .
Proof. Let lim
f x sL, lim
g x sM. Suppose that LyM0.
x™a
x™a
Ž .
Ž .
Ž .
By Theorem 3.2.1, LyM is the limit of the function h x sf x yg x .
Therefore, there exists a 0 such that
LyM
h x y LyM

,
3.9
Ž .
Ž
.
Ž
.
2


Ž
.
Ž .
Ž
.
if 0 xya . Inequality 3.9 implies that h x  LyM r20, which
Ž .
Ž .
Ž .
is not possible, since, by assumption, h x sf x yg x F0. We must then
have LyMF0.

3.3. THE o, O NOTATION
These symbols provide a convenient way to describe the limiting behavior of
Ž .
a function f x as x tends to a certain limit.
Ž .
Ž .
Ž .
Let f x and g x be two functions defined on D;R. The function g x
is positive and usually has a simple form such as 1, x, or 1rx. Suppose that
there exists a positive number K such that
f x
FKg x
Ž .
Ž .
Ž .
for all xgE, where E;D. Then, f x is said to be of an order of magnitude
Ž .
not exceeding that of g x . This fact is denoted by writing
f x sO g x
Ž .
Ž .
Ž
.
Ž .
Ž .
for all xgE. In particular, if g x s1, then f x
is necessarily a bounded
function on E. For example,
cos xsO 1
for all x,
Ž .
xsO x 2
for large values of x,
Ž
.
x 2q2 xsO x 2
for large values of x,
Ž
.
 
sin xsO
x
for all x.
Ž
.
The last relationship is true because
sin x
F1
x
for all values of x, where x is measured in radians.

LIMITS AND CONTINUITY OF FUNCTIONS
66
Ž .
Ž .
Let us now suppose that the relationship between f x and g x is such
that
f x
Ž .
lim
s0.
g x
x™a
Ž .
Ž .
Ž .
Then we say that f x
is of a smaller order of magnitude than g x
in a
deleted neighborhood of a. This fact is denoted by writing
f x so g x
as x™a,
Ž .
Ž .
Ž
.
Ž .
Ž .
which is equivalent to saying that f x tends to zero more rapidly than g x
as x™a. The o symbol can also be used when x tends to infinity. In this
case we write
f x so g x
for xA,
Ž .
Ž .
Ž
.
where A is some positive number. For example,
x 2so x
as x™0,
Ž .
tan x 3so x 2
as x™0,
Ž
.
'x so x
as x™.
Ž .
Ž .
Ž .
If f x and g x are any two functions such that
f x
Ž .
™1
as x™a,
g x
Ž .
Ž .
Ž .
then f x and g x are said to be asymptotically equal, written symbolically
Ž .
Ž .
f x g x , as x™a. For example,
x 2x 2q3xq1
as x™,
sin xx
as x™0.
On the basis of the above definitions, the following properties can be
deduced:
Ž Ž .
Ž ..
Ž Ž ..
Ž Ž ..
1. O f x qg x
sO f x
qO g x .
Ž Ž . Ž ..
Ž Ž ..
Ž Ž ..
2. O f x g x
sO f x O g x .
Ž Ž . Ž ..
Ž Ž .. Ž Ž ..
3. o f x g x
sO f x
o g x .
Ž .
Ž .
Ž .
Ž .
Ž Ž ..
4. If f x g x as x™a, then f x sg x qo g x
as x™a.
3.4. CONTINUOUS FUNCTIONS
Ž .
A function f x may have a limit L as x™a. This limit, however, may not be
equal to the value of the function at xsa. In fact, the function may not even

CONTINUOUS FUNCTIONS
67
Ž .
Ž .
Ž .
be defined at this point. If f x is defined at xsa and Lsf a , then f x is
said to be continuous at xsa.
Ž .
Definition 3.4.1.
Let f: D™R, where D;R, and let agD. Then f x
is continuous at xsa if for every 0 there exists a 0 such that
f x yf a

Ž .
Ž .


for all xgD for which xya .
Ž .
It is important here to note that in order for f x
to be continuous at
xsa, it is necessary that it be defined at xsa as well as at all other points
Ž .
inside a neighborhood N a of the point a for some r0. Thus to show
r
Ž .
continuity of f x at xsa, the following conditions must be verified:
Ž .
1. f x is defined at all points inside a neighborhood of the point a.
Ž .
2. f x has a limit from the left and a limit from the right as x™a, and
that these two limits are equal to L.
Ž .
3. The value of f x at xsa is equal to L.
For convenience, we shall denote the left-sided and right-sided limits of
Ž .
Ž
y.
Ž
q.
f x as x™a by f a
and f a
, respectively.
Ž .
If any of the above conditions is violated, then
f x
is said to be
discontinuous at xsa. There are two kinds of discontinuity.

Definition 3.4.2.
A function f: D™R has a discontinuity of the first kind
Ž
y.
Ž
q.
at xsa if f a
and f a
exist, but at least one of them is different from
Ž .
Ž .
f a . The function f x
has a discontinuity of the second kind at the same
Ž
y.
Ž
q.
point if at least one of f a
and f a
does not exist.

Definition 3.4.3.
A function f: D™R is continuous on E;D if it is
continuous at every point of E.
For example, the function
xy1
°
,
xG0, x1,
2
x y1
~
f x s
Ž .
1
,
xs1,
¢2
is defined for all xG0 and is continuous at xs1. This is true because, as
was shown in Example 3.1.1,
1
1
lim f x s lim
s
,
Ž .
xq1
2
x™1
x™1
Ž .
which is equal to the value of the function at xs1. Furthermore, f x
is

LIMITS AND CONTINUITY OF FUNCTIONS
68
Ž .
continuous at all other points of its domain. Note that if f 1 were different
1
Ž .
from
, then f x would have a discontinuity of the first kind at xs1.
2
Let us now consider the function
xq1,
x0,
f x s
Ž .
0,
xs0,
½ xy1,
x0.
This function is continuous everywhere except at xs0, since it has no limit
Ž y.
Ž q.
as x™0 by the fact that f 0
sy1 and f 0
s1. The discontinuity at this
point is therefore of the first kind.
An example of a discontinuity of the second kind is given by the function
1
°cos
,
x0,
~
f x s
Ž .
x
¢0,
xs0,
Ž y.
which has a discontinuity of the second kind at xs0, since neither f 0
nor
Ž q.
f 0
exists.
Definition 3.4.4.
A function
f: D™R is left-continuous at
xsa if
Ž .
Ž .
Ž .
Ž .
y
q
lim
f x sf a . It is right-continuous at xsa if lim
f x sf a .
x™a
x™a

Obviously, a left-continuous or a right-continuous function is not necessar-
Ž .
ily continuous. In order for f x to be continuous at xsa it is necessary and
Ž .
sufficient that f x be both left-continuous and right-continuous at this point.
For example, the function
xy1,
xF0,
f x s
Ž . ½ 1,
x0
Ž y.
Ž .
Ž .
is left-continuous at xs0, since f 0
sy1sf 0 . If f x were defined so
Ž .
Ž .
that f x sxy1 for x0 and f x s1 for xG0, then it would be right-con-
tinuous at xs0.
Definition 3.4.5.
The function f: D™R is uniformly continuous on E;
D if for every 0 there exists a 0 such that
f x
yf x

3.10
Ž
.
Ž
.
Ž
.
1
2


for all x , x gE for which x yx
.

1
2
1
2
This definition appears to be identical to the definition of continuity. That
is not exactly the case. Uniform continuity is always associated with a set such

CONTINUOUS FUNCTIONS
69
as E in Definition 3.4.5, whereas continuity can be defined at a single point
Ž
.
a. Furthermore, inequality 3.10 is true for all pairs of points x , x gE such
1
2


that
x yx
. Hence,  depends only on , not on the particular
1
2
locations of
x , x . On the other hand, in the definition of continuity
1
2
Ž
.
Definition 3.4.1
 depends on  as well as on the location of the point
where continuity is considered. In other words,  can change from one point
to another for the same given 0. If, however, for a given 0, the same 
Ž .
can be used with all points in some set E;D, then f x
is uniformly
Ž .
continuous on E. For this reason, whenever f x is uniformly continuous on
E,  can be described as being ‘‘portable,’’ which means it can be used
everywhere inside E provided that 0 remains unchanged.
Ž .
Obviously, if f x
is uniformly continuous on E, then it is continuous
there. The converse, however, is not true. For example, consider the function
Ž
.
Ž .
Ž .
f: 0, 1 ™R given by f x s1rx. Here, f x
is continuous at all points of
Ž
.
Es 0, 1 , but is not uniformly continuous there. To demonstrate this fact, let
Ž .
us first show that f x is continuous on E. Let 0 be given and let agE.
Ž .
Since a0, there exists a  0 such that the neighborhood N
a
is a
1
1
subset of E. This can be accomplished by choosing 
such that 0 a.
1
1
Ž .
Now, for all xgN
a ,
1


1
1
xya
1
y
s

.
x
a
ax
a ay
Ž
.
1
Let  0 be such that for the given 0,
2
2
 ,
a ay
Ž
.
2
which can be satisfied by requiring that
a2
0 
.
2
1qa
Since
a2
a,
1qa
then
1
1
y

3.11
Ž
.
x
a

LIMITS AND CONTINUITY OF FUNCTIONS
70


if xya , where
a2
a2
min a,
s
.
ž
/
1qa
1qa
Ž .
It follows that f x s1rx is continuous at every point of E. We note here
the dependence of  on both  and a.
Ž .
Let us now demonstrate that f x s1rx is not uniformly continuous on
E. Define G to be the set
2
a 
Gs
agE .
½
5
1qa
Ž .
In order for f x s1rx to be uniformly continuous on E, the infimum of
Ž
.
G must be positive. If this is possible, then for a given 0, 3.11 will be
satisfied by all x for which


xya inf G ,
Ž
.
Ž
.
Ž
.
and for all ag 0, 1 . However, this cannot happen, since inf G s0. Thus it
Ž
.
Ž
.
is not possible to find a single  for which 3.11 will work for all ag 0, 1 .
Ž
.
Let us now try another function defined on the same set Es 0, 1 ,
Ž .
2
namely, the function f x sx . In this case, for a given 0, let 0 be
such that
 2q2ay0.
3.12
Ž
.


Then, for any agE, if xya  we get

2
2

 

x ya
s xya
xqa

 

s xya
xyaq2a
F q2a .
3.13
Ž
.
Ž
.
It is easy to see that this inequality is satisfied by all 0 for which
2
'
0yaq a q .
3.14
Ž
.
If H is the set
2
'

Hs yaq a q agE ,

4
'
Ž
.
then it can be verified that inf H sy1q 1q . Hence, by choosing 
such that
Finf H ,
Ž
.
Ž
.
Ž
.
inequality
3.14 , and hence
3.13 , will be satisfied for all agE. The
Ž .
2
function f x sx
is therefore uniformly continuous on E.

CONTINUOUS FUNCTIONS
71
The above examples demonstrate that continuity and uniform continuity
on a set E are not always equivalent. They are, however, equivalent under
certain conditions on E. This will be illustrated in Theorem 3.4.6 in the next
subsection.
3.4.1. Some Properties of Continuous Functions
Continuous functions have some interesting properties, some of which are
given in the following theorems:
Ž .
Ž .
Theorem 3.4.1.
Let f x
and g x
be two continuous functions defined
on a set D;R. Then:
Ž .
Ž .
Ž . Ž .
1. f x qg x and f x g x are continuous on D.
Ž .
2. af x is continuous on D, where a is a constant.
Ž .
Ž .
Ž .
3. f x rg x is continuous on D provided that g x 0 on D.
Proof. The proof is left as an exercise.

Theorem 3.4.2.
Suppose that
f:
D™R is continuous on
D, and
Ž
.
Ž
.
g: f D ™R is continuous on f D , the image of D under f. Then the
Ž .
w Ž .x
composite function h: D™R defined as h x sg f x
is continuous on D.
Ž .
Proof. Let 0 be given, and let agD. Since g is continuous at f a ,
 w Ž .x
w Ž .x
 Ž .
Ž .
there exists a 0 such that
g f x yg f a
 if
f x yf a
.
Ž .
 Ž .
Ž .
Since f x is continuous at xsa, there exists a 0 such that f x yf a




 if
xya . It follows that by taking
xya 
we must have
 Ž .
Ž .
h x yh a
.

Ž .
Ž .
Theorem 3.4.3.
If f x
is continuous at xsa and f a 0, then there
Ž .
Ž .
exists a neighborhood N a in which f x 0.

Ž .
Proof. Since f x is continuous at xsa, there exists a 0 such that
1
f x yf a
 f a ,
Ž .
Ž .
Ž .
2


if xya . This implies that
1
f x  f a 0
Ž .
Ž .
2
Ž .
for all xgN a .


Ž
.
Theorem 3.4.4
The Intermediate-Value Theorem .
Let f: D™R be
w
x
continuous, and let a, b be a closed interval contained in D. Suppose that

LIMITS AND CONTINUITY OF FUNCTIONS
72
Ž .
Ž .
Ž .
Ž .
f a f b . If  is a number such that f a f b , then there exists a
Ž .
point c, where acb, such that sf c .
Ž .
Ž .
Proof. Let
g: D™R be defined as
g x sf x y. This function is
Ž .
Ž .
continuous and is such that g a 0, g b 0. Consider the set

w
x
Ss xg a, b
g x 0 .

4
Ž .
This set is nonempty, since agS and is bounded from above by b. Hence, by
Ž .
Theorem 1.5.1 the least upper bound of S exists. Let cslub S . Since
w
x
w
x
S; a, b , then cg a, b .
Now, for every positive integer n, there exists a point x gS such that
n
1
cy
x Fc.
n
n
Otherwise, if xFcy1rn for all xgS, then cy1rn will be an upper bound
Ž .
of S, contrary to the definition of c. Consequently, lim
x sc. Since g x
n™
n
w
x
is continuous on a, b , then
g c s lim g x
F0,
3.15
Ž .
Ž
.
Ž
.
n
x ™c
n
Ž
.
Ž
.
by Theorem 3.2.2 and the fact that g x
0. From 3.15 we conclude that
n
Ž .
cb, since g b 0.
Ž .
Let us suppose that g c 0. Then, by Theorem 3.4.3, there exists a
Ž .
Ž .
Ž .
neighborhood N c , for some 0, such that g x 0 for all xgN c l


w
x
w
x
a, b . Consequently, there exists a point x g a, b such that cx cq
0
0
Ž
.
and g x
0. This means that x
belongs to S and is greater than c,
0
0
Ž
.
Ž .
a contradiction. Therefore, by inequality 3.15 we must have g c s0, that
Ž .
is, f c s. We note that ca, since cGa, but ca. This last is true
Ž .
because if asc, then g c 0, a contradiction. This completes the proof of
the theorem.

The direct implication of the intermediate-value theorem is that a continu-
ous function possesses the property of assuming at least once every value
between any two distinct values taken inside its domain.
Theorem 3.4.5.
Suppose that f: D™R is continuous and that D is
Ž .
closed and bounded. Then f x is bounded in D.
Proof. Let a be the greatest lower bound of D, which exists because D is
Ž
.
Ž .
bounded. Since D is closed, then agD
why? . Furthermore, since f x

CONTINUOUS FUNCTIONS
73
is continuous, then for a given 0 there exists a  0 such that
1
f a yf x f a q
Ž .
Ž .
Ž .


Ž .
Ž .
if xya  . The function f x is therefore bounded in N
a . Define AA to
1
1
be the set

AAs xgD f x
is bounded .

4
Ž .
Ž .
This set is nonempty and bounded, and N
a lD;AA. We need to show
1
that DyAA is an empty set.
Ž
.
As before, the least upper bound of AA exists
since it is bounded
and
Ž
.
Ž
.
Ž .
belongs to D since D is closed . Let cslub AA . By the continuity of f x ,
Ž .
Ž .
there exists a neighborhood N
c in which f x is bounded for some  0.

2
2Ž .
Ž
.
w
Ž .
If DyAA is nonempty, then N
c l DyAA is also nonempty if N
c ;AA,


2
2
Ž
.
x
Ž .
Ž
.
then cq  r2 gAA, a contradiction . Let x gN
c l DyAA . Then, on
2
0
 2
Ž
.
Ž
.
one hand, f x
is not bounded, since x g DyAA . On the other hand,
0
0
Ž
.
Ž .
f x
must be bounded, since x gN
c . This contradiction leads us to con-
0
0
 2
Ž .
clude that DyAA must be empty and that f x is bounded in D.

Corollary 3.4.1.
If
f: D™R is continuous, where D is closed and
Ž .
bounded, then f x achieves its infimum and supremum at least once in D,
that is, there exists , gD such that
f 
Ff x
for all xgD,
Ž
.
Ž .
f  Gf x
for all xgD.
Ž .
Ž .
Equivalently,
f 
s inf f x ,
Ž
.
Ž .
xgD
f  s sup f x .
Ž .
Ž .
xgD
Ž
.
Proof. By Theorem 3.4.5, f D
is a bounded set. Hence, its least upper
Ž
.
Ž .
bound exists. Let Mslub f D , which is the same as sup
f x . If there
xg D
Ž .
Ž .
exists no point x in D for which f x sM, then Myf x 0 for all xgD.
w
Ž .x
Consequently, 1r Myf x
is continuous on D by Theorem 3.4.1, and is
hence bounded there by Theorem 3.4.5.
Now, if 0 is any given positive number, we can find a value x for which
Ž .
f x My, or
1
1

.
Myf x

Ž .
w
Ž .x
This implies that 1r Myf x
is not bounded, a contradiction. Therefore,
Ž .
there must exist a point gD at which f  sM.

LIMITS AND CONTINUITY OF FUNCTIONS
74
Ž
.
The proof concerning the existence of a point gD such that f  s
Ž .
inf
f x is similar.

xg D
The requirement that D be closed in Corollary 3.4.1 is essential. For
Ž .
 
4
example, the function f x s2 xy1, which is defined on Ds x 0x1 ,
cannot achieve its infimum, namely y1, in D. For if there exists a gD such
Ž
.
that f  F2 xy1 for all xgD, then there exists a 0 such that 0y.
Hence,
f y s2y2y1f  ,
Ž
.
Ž
.
a contradiction.
Theorem 3.4.6.
Let f: D™R be continuous on D. If D is closed and
bounded, then f is uniformly continuous on D.
Proof. Suppose that f is not uniformly continuous. Then, by using the
logical negation of the statement concerning uniform continuity in Definition
3.4.5, we may conclude that there exists an 0 such that for every 0, we


 Ž
.
Ž
.
can find x , x gD with
x yx
 for which
f x
yf x
G. On this
1
2
1
2
1
2


basis, by choosing s1, we can find u , ® gD with u y®
1 for which
1
1
1
1
1
 Ž
.
Ž
.


f u
yf ®
G. Similarly, we can find u , ® gD with
u y®

for
1
1
2
2
2
2
2
 Ž
.
Ž
.
which f u
yf ®
G. By continuing in this process we can find u , ® gD
2
2
n
n


 Ž
.
Ž
.
with u y®
1rn for which f u
yf ®
G, ns3, 4, . . . .
n
n
n
n
Now, let S be the set

Ss u
ns1, 2, . . .

4
n
This set is bounded, since S;D. Hence, its least upper bound exists. Let
Ž .
cslub S . Since D is closed, then cgD. Thus, as in the proof of Theorem
3.4.4, we can find points u , u , . . . , u , . . . in S such that lim
u
sc.
n
n
n
k ™
n
1
2
k
k
Ž .
Since f x is continuous, there exists a 0 such that

f x yf c

,
Ž .
Ž .
2


if
xyc  for any given 0. Let us next choose k large enough such
that if n N, where N is some large positive number, then
k

1

u
yc 
and

.
3.16
Ž
.
nk
2
n
2
k


Since u
y®
1rn , then
n
n
k
k
k
® yc F u
y®
q u
yc
n
n
n
n
k
k
k
k
1


q

3.17
Ž
.
n
2
k

CONTINUOUS FUNCTIONS
75
Ž
.
Ž
.
Ž .
for n N. From 3.16 and 3.17 and the continuity of f x
we conclude
k
that


f u
yf c

and
f ®
yf c

.
Ž
.
Ž .
Ž
.
Ž .
n
n
k
k
2
2
Thus,
f u
yf ®
F f u
yf c
q f ®
yf c
Ž
.
Ž
.
Ž
.
Ž .
Ž
.
Ž .
n
n
n
n
k
k
k
k
.
3.18
Ž
.
However, as was seen earlier,
f u
yf ®
G ,
Ž
.
Ž
.
n
n
hence,
f u
yf ®
G ,
Ž
.
Ž
.
n
n
k
k
Ž
.
Ž .
which contradicts
3.18 . This leads us to assert that
f x
is uniformly
continuous on D.

3.4.2. Lipschitz Continuous Functions
Lipschitz continuity is a specialized form of uniform continuity.
Definition 3.4.6.
The function f: D™R is said to satisfy the Lipschitz
condition on a set E;D if there exist constants, K and , where K0 and
0F1 such that



f x
yf x
FK x yx
Ž
.
Ž
.
1
2
1
2
for all x , x gE.

1
2
Ž .
Notationally, whenever f x
satisfies the Lipschitz condition with con-
Ž
.
stants K and  on a set E, we say that it is Lip K, 
on E. In this case,
Ž .
f x is called a Lipschitz continuous function. It is easy to see that a Lipschitz
continuous function on E is also uniformly continuous there.
'
Ž .
As an example of a Lipschitz continuous function, consider f x s x ,
1
'
Ž
.
where xG0. We claim that
x is Lip 1,
on its domain. To show this, we
2
first write
x y
x
F
x q
x .
'
'
'
'
1
2
1
2

LIMITS AND CONTINUITY OF FUNCTIONS
76
Hence,
2


x y
x
F x yx
.
'
'
1
2
1
2
Thus,
1r2


x y
x
F x yx
,
'
'
1
2
1
2
which proves our claim.
3.5. INVERSE FUNCTIONS
From Chapter 1 we recall that one of the basic characteristics of a function
Ž .
ysf x is that two values of y are equal if they correspond to the same value
of x. If we were to reverse the roles of x and y so that two values of x are
equal whenever they correspond to the same value of y, then x becomes a
function of y. Such a function is called the inverse function of f and is
denoted by f y1. We conclude that the inverse of f: D™R exists if and only
if f is one-to-one.
y1
Ž
.
Definition 3.5.1.
Let f: D™R. If there exists a function f
: f D ™D
y1w Ž .x
w
y1Ž .x
Ž
.
such that f
f x sx and all xgD and f f
y sy for all ygf D ,
then f y1 is called the inverse function of f.

Definition 3.5.2.
Let f: D™R. Then, f is said to be monotone increas-
w
x
ing
decreasing
on
D
if whenever
x , x gD are such that
x x ,
1
2
1
2
Ž
.
Ž
. w Ž
.
Ž
.x
then f x
Ff x
f x
Gf x
. The function f is strictly monotone in-
1
2
1
2
w
x
Ž
.
Ž
. w Ž
.
Ž
.x
creasing decreasing on D if f x
f x
f x
f x
whenever x x .
1
2
1
2
1
2

If f is either monotone increasing or monotone decreasing on D, then it is
called a monotone function on D. In particular, if it is either strictly
Ž .
monotone increasing or strictly monotone decreasing, then f x
is strictly
monotone on D.
Strictly monotone functions have the property that their inverse functions
exist. This will be shown in the next theorem.
Ž
Theorem 3.5.1.
Let f: D™R be strictly monotone increasing
or de-
.
y1
creasing on D. Then, there exists a unique inverse function f
, which is
Ž
.
Ž
.
strictly monotone increasing or decreasing on f D .
Proof. Let us suppose that f is strictly monotone increasing on D. To
y1
Ž
.
show that f
exists as a strictly monotone increasing function on f D .

INVERSE FUNCTIONS
77
Ž
.
Ž
.
Suppose that x , x gD are such that f x
sf x
sy. If x x , then
1
2
1
2
1
2
Ž
.
Ž
.
x x
or x x . Since f is strictly monotone increasing, then f x
f x
1
2
2
1
1
2
Ž
.
Ž
.
Ž
.
Ž
.
or f x
f x . In either case, f x
f x
, which contradicts the assump-
2
1
1
2
Ž
.
Ž
.
tion that f x
sf x
. Hence, x sx , that is, f is one-to-one and has
1
2
1
2
therefore a unique inverse f y1.
y1
Ž
.
The inverse f
is strictly monotone increasing on f D . To show this,
Ž
.
Ž
.
suppose that f x
f x
. Then, x x . If not, we must have x Gx . In
1
2
1
2
1
2
Ž
.
Ž
.
Ž
.
Ž
.
this case, f x
sf x
when x sx , or f x
f x
when x x , since f
1
2
1
2
1
2
1
2
is strictly monotone increasing. However, this is contrary to the assumption
Ž
.
Ž
.
y1
that f x
f x
. Thus x x
and f
is strictly monotone increasing.
1
2
1
2
The proof of Theorem 3.5.1 when ‘‘increasing’’ is replaced with ‘‘decreas-
ing’’ is similar.

Theorem 3.5.2.
Suppose that f: D™R is continuous and strictly mono-
Ž
.
w
x
y1
tone increasing
decreasing
on
a, b ;D. Then, f
is continuous and
Ž
.
Žw
x.
strictly monotone increasing decreasing on f
a, b .
Proof. By Theorem 3.5.1 we only need to show the continuity of f y1.
Suppose that f is strictly monotone increasing. The proof when f is strictly
monotone decreasing is similar.
Since f is continuous on a closed and bounded interval, then by Corollary
w
x
3.4.1 it must achieve its infimum and supremum on
a, b . Furthermore,
because f is strictly monotone increasing, its infimum and supremum must
be attained at only a and b, respectively. Thus
w
x
f
a, b
s f a , f b
.
Ž .
Ž .
Ž
.
w Ž .
Ž .x
Let dg f a , f b . There exists a unique value c, aFcFb, such that
Ž .
f c sd. For any 0, let  be defined as
smin f c yf cy , f cq yf c
.
Ž .
Ž
.
Ž
.
Ž .
w
x
Then there exists a , 0, such that all the x’s in a, b that satisfy the
inequality
f x yd 
Ž .
must also satisfy the inequality


xyc .
This is true because
f c yf c qf cy dyf x dq
Ž .
Ž .
Ž
.
Ž .
f c qf cq yf c ,
Ž .
Ž
.
Ž .

LIMITS AND CONTINUITY OF FUNCTIONS
78
that is,
f cy f x f cq .
Ž
.
Ž .
Ž
.
y1
Ž
.
Using the fact that f
is strictly monotone increasing by Theorem 3.5.1 ,
we conclude that
cyxcq ,


y1Ž .
w Ž .
Ž .x
that is,
xyc . It follows that xsf
y
is continuous on
f a , f b .

Ž .
Ž .
Note that in general if
ysf x , the equation
yyf x s0 may not
produce a unique solution for x in terms of y. If, however, the domain of f
can be partitioned into subdomains on each of which f is strictly monotone,
then f can have an inverse on each of these subdomains.
Ž .
3
EXAMPLE 3.5.1.
Consider the function f: R™R defined by ysf x sx .
It is easy to see that f is strictly monotone increasing for all xgR. It
y1Ž .
1r3
therefore has a unique inverse given by f
y sy
.
w
x
Ž .
5
EXAMPLE 3.5.2.
Let f: y1, 1 ™R be such that ysf x sx yx. From
Figure 3.2 it can be seen that f is strictly monotone increasing on D s
1
w
y1r4 x
w y1r4
x
y1,y5
and D s 5
, 1 , but is strictly monotone decreasing on
2
w
y1r4
y1r4 x
D s y5
, 5
. This function has therefore three inverses, one on
3
each of D , D , and D . By Theorem 3.5.2, all three inverses are continuous.
1
2
3
w
x
Ž .
EXAMPLE 3.5.3.
Let
f: R™y1, 1
be the function
ysf x ssin x,
where x is measured in radians. There is no unique inverse on R, since the
sine function is not strictly monotone there. If, however, we restrict the
w
x
domain of f to Ds yr2, r2 , then f is strictly monotone increasing
y1Ž .
Ž
.
there and has the unique inverse f
y sArcsin y see Example 1.3.4 . The
w
x
y1Ž .
inverse of f on
r2, 3r2
is given by f
y syArcsin y. We can
w
x w
x
similarly find the inverse of f on 3r2, 5r2 , 5r2, 7r2 , etc.
Ž .
Figure 3.2. The graph of the function f x s
5
x yx.

CONVEX FUNCTIONS
79
3.6. CONVEX FUNCTIONS
Convex functions are frequently used in operations research. They also
happen to be continuous, as will be shown in this section. The natural
domains for such functions are convex sets.
Ž
.
Definition 3.6.1.
A set D;R is convex if x q 1y x gD whenever
1
2
x , x
belong to D and 0FF1. Geometrically, a convex set contains the
1
2
line segment connecting any two of its points. The same definition actually
n
Ž
.
applies to convex sets in R , the n-dimensional Euclidean space nG2 . For
example, each of the following sets is convex:
1. Any interval in R.
2. Any sphere in R3, and in general, any hypersphere in Rn, nG4.
Ž
.
2  
 
4
3. The set
x, y gR
x q y F1 . See Figure 3.3.

Definition 3.6.2.
A function f: D™R is convex if
f x q 1y x
Ff x
q 1y f x
3.19
Ž
.
Ž
.
Ž
. Ž
.
Ž
.
1
2
1
2
for all x , x gD and any  such that 0FF1. The function f is strictly
1
2
Ž
.
convex if inequality 3.19 is strict for x x .
1
2
Ž
.
Geometrically, inequality 3.19 means that if P and Q are any two points
Ž .
on the graph of ysf x , then the portion of the graph between P and Q lies
Ž
.
below the chord PQ see Figure 3.4 . Examples of convex functions include
Ž .
2
Ž .
w
x
Ž .
x
Ž .
f x sx
on R, f x ssin x on , 2 , f x se
on R, f x sylog x for
x0, to name just a few.

Definition 3.6.3.
A function f: D™R is concave if yf is convex.

w
x
We note that if f: a, b ™R is convex and the values of f at a and b are
Ž .
w
x
 Ž .
Ž .4
finite, then f x
is bounded from above on
a, b
by Msmax f a , f b .
w
x
Ž
.
w
x
This is true because if xg a, b , then xsaq 1y b for some g 0, 1 ,
Ž
.
2   
 
4
Figure 3.3. The set
x, y gR
x q y F1 .

LIMITS AND CONTINUITY OF FUNCTIONS
80
Figure 3.4. The graph of a convex function.
w
x
since a, b is a convex set. Hence,
f x Ff a q 1y f b
Ž .
Ž .
Ž
. Ž .
FMq 1y MsM.
Ž
.
Ž .
The function f x
is also bounded from below. To show this, we first note
w
x
that any xg a, b can be written as
aqb
xs
qt,
2
where
aqb
aqb
ay
FtFby
.
2
2
Now,
aqb
1
aqb
1
aqb
f
F
f
qt q
f
yt ,
3.20
Ž
.
ž
/
ž
/
ž
/
2
2
2
2
2
Ž
.
w
x
Ž
.
since if
aqb r2qt belongs to
a, b , then so does
aqb r2yt. From
Ž
.
3.20 we then have
aqb
aqb
aqb
f
qt G2 f
yf
yt .
ž
/
ž
/
ž
/
2
2
2
Since
aqb
f
yt FM,
ž
/
2
then
aqb
aqb
f
qt G2 f
yM,
ž
/
ž
/
2
2

CONVEX FUNCTIONS
81
Ž .
w
x
that is, f x Gm for all xg a, b , where
aqb
ms2 f
yM.
ž
/
2
Another interesting property of convex functions is given in Theorem
3.6.1.
Theorem 3.6.1.
Let f: D™R be a convex function, where D is an open
Ž
.
w
x
interval. Then f is Lip K, 1 on any closed interval
a, b contained in D,
that is,


f x
yf x
FK x yx
3.21
Ž
.
Ž
.
Ž
.
1
2
1
2
w
x
for all x , x g a, b .
1
2
w
x
Proof. Consider the closed interval ay, bq , where 0 is chosen so
that this interval is contained in D. Let m and M be, respectively, the
Ž
.
w
x
lower and upper bounds of f
as was seen earlier
on
ay, bq . Let
w
x
x , x
be any two distinct points in a, b . Define z and  as
1
2
1
 x yx
Ž
.
2
1
z sx q
,
1
2


x yx
1
2


x yx
1
2
s
.


q x yx
1
2
w
x
Ž
.


Then z g ay, bq . This is true because
x yx r x yx
is either
1
1
1
1
2
w
x
equal to 1 or to y1. Since x g a, b , then
2
 x yx
Ž
.
2
1
ayFx yFx q
Fx qFbq.
2
2
2


x yx
1
2
Furthermore, it can be verified that
x sz q 1y x .
Ž
.
2
1
1
We then have
f x
Ff z
q 1y f x
s f z
yf x
qf x
.
Ž
.
Ž
.
Ž
. Ž
.
Ž
.
Ž
.
Ž
.
2
1
1
1
1
1
Thus,
f x
yf x
F f z
yf x
Ž
.
Ž
.
Ž
.
Ž
.
2
1
1
1
w
x
F Mym


x yx
1
2


F
Mym sK x yx
,
3.22
Ž
.
Ž
.
1
2


LIMITS AND CONTINUITY OF FUNCTIONS
82
Ž
.
Ž
.
w
x
where Ks Mym r. Since inequality 3.22 is true for any x , x g a, b ,
1
2
we must also have


f x
yf x
FK x yx
.
3.23
Ž
.
Ž
.
Ž
.
1
2
1
2
Ž
.
Ž
.
From inequalities 3.22 and 3.23 we conclude that


f x
yf x
FK x yx
Ž
.
Ž
.
1
2
1
2
w
x
Ž .
Ž
.
w
x
for any x , x g a, b , which shows that f x is Lip K, 1 on a, b .

1
2
Using Theorem 3.6.1 it is easy to prove the following corollary:
Corollary 3.6.1.
Let f: D™R be a convex function, where D is an open
w
x
Ž .
interval. If a, b is any closed interval contained in D, then f x is uniformly
w
x
continuous on a, b and is therefore continuous on D.
Ž .
Ž
.
Note that if
f x
is convex on
a, b , then it does not have to be
continuous at the end points of the interval. It is easy to see, for example,
w
x
that the function f: y1, 1 ™R defined as
x 2,
y1x1,
f x s
Ž . ½ 2,
xs1,y1
Ž
.
is convex on y1, 1 , but is discontinuous at xsy1, 1.
3.7. CONTINUOUS AND CONVEX FUNCTIONS IN STATISTICS
The most vivid examples of continuous functions in statistics are perhaps the
cumulative distribution functions of continuous random variables. If X is a
continuous random variable, then its cumulative distribution function
F x sP XFx
Ž .
Ž
.
is continuous on R. In this case,
1
1
P Xsa s lim
F aq
yF ay
s0,
Ž
.
ž
/
ž
/
n
n
n™
that is, the distribution of X assigns a zero probability to any single value.
This is a basic characteristic of continuous random variables.
Examples of continuous distributions include the beta, Cauchy, chi-
squared, exponential, gamma, Laplace, logistic, lognormal, normal, t, uni-
form, and the Weibull distributions. Most of these distributions are described

CONTINUOUS AND CONVEX FUNCTIONS IN STATISTICS
83
in introductory statistics books. A detailed account of their properties and
Ž
.
uses is given in the two books by Johnson and Kotz 1970a, 1970b .
Ž
It is interesting to note that if X is any random variable not necessarily
.
Ž .
continuous , then its cumulative distribution function, F x , is right-continu-
Ž
.
ous on R
see, for example, Harris, 1966, page 55 . This function is also
Ž .
monotone increasing on R. If F x is strictly monotone increasing, then by
y1Ž .
Theorem 3.5.1 it has a unique inverse F
y . In this case, if Y has the
Ž
.
uniform distribution over the open interval 0, 1 , then the random variable
y1Ž
.
Ž .
F
Y
has the cumulative distribution function F x . To show this, consider
y1Ž
.
XsF
Y . Then,
y1
P XFx sP F
Y Fx
Ž
.
Ž
.
sP YFF x
Ž .
sF x .
Ž .
This result has an interesting application in sampling. If Y , Y , . . . , Y
1
2
n
Ž
.
form an independent random sample from the uniform distribution U 0, 1 ,
y1Ž
.
y1Ž
.
y1Ž
.
then F
Y , F
Y , . . . , F
Y
will form an independent sample from a
1
2
n
Ž .
distribution with the cumulative distribution function F x . In other words,
samples from any distribution can be generated through sampling from the
Ž
.
uniform distribution U 0, 1 . This result forms the cornerstone of Monte
Carlo simulation in statistics. Such a method provides an artificial way of
collecting ‘‘data.’’ There are situations where the actual taking of a physical
sample is either impossible or too expensive. In such situations, useful
information can often be derived from simulated sampling. Monte Carlo
simulation is also used in the study of the relative performance of test
statistics and parameter estimators when the data come from certain speci-
fied parent distributions.
Another example of the use of continuous functions in statistics is in limit

4
theory. For example, it is known that if
X
is a sequence of random
n ns1
Ž .
variables that converges in probability to c, and if g x
is a continuous
Ž
.
function at xsc, then the random variable g X
converges in probability to
n
Ž .

4
g c
as n™. By definition, a sequence of random variables
Xn ns1
converges in probability to a constant c if for a given 0,


lim P
X yc G s0.
Ž
.
n
n™

4
In particular, if
X
is a sequence of estimators of a parameter c, then
n ns1
X
is said to be a consistent estimator of c if X
converges in probability to
n
n
c. For example, the sample mean
n
1
X s
X
Ý
n
i
n is1

LIMITS AND CONTINUITY OF FUNCTIONS
84
of a sample of size n from a population with a finite mean  is a consistent
Ž
estimator of  according to the law of large numbers
see, for example,
.
Lindgren, 1976, page 155 . Other types of convergence in statistics can be
Ž
found in standard mathematical statistics books see the annotated bibliogra-
.
phy .
Convex functions also play an important role in statistics, as can be seen
from the following examples.
Ž .
If f x is a convex function and X is a random variable with a finite mean
Ž
.
sE X , then
E f X
Gf E X
.
Ž
.
Ž
.
Equality holds if and only if X is constant with probability 1. This inequality
is known as Jensen’s inequality. If f is strictly convex, the inequality is strict
unless X is constant with probability 1. A proof of Jensen’s inequality is given
Ž
.
in Section 6.7.4. See also Lehmann 1983, page 50 .
Jensen’s inequality has useful applications in statistics. For example, it can
be used to show that if x , x , . . . , x
are n positive scalars, then their
1
2
n
arithmetic mean is greater than or equal to their geometric mean, which is
Ž
n
.1r n
equal to 
x
. This can be shown as follows:
is1
i
Ž .
Consider the convex function f x sylog x. Let X be a discrete random
variable that takes the values x , x , . . . , x
with probabilities equal to 1rn,
1
2
n
that is,
1
°
,
xsx , x , . . . , x
1
2
n
~
P Xsx s
Ž
.
n
¢0
otherwise.
Then, by Jensen’s inequality,
E ylog X Gylog E X .
3.24
Ž
.
Ž
.
Ž
.
However,
n
1
E ylog X sy
log x ,
3.25
Ž
.
Ž
.
Ý
i
n is1
and
n
1
ylog E X sylog
x
.
3.26
Ž
.
Ž
.
Ý
i
ž
/
n is1

CONTINUOUS AND CONVEX FUNCTIONS IN STATISTICS
85
Ž
.
Ž
.
Ž
.
By using 3.25 and 3.26 in 3.24 we get
n
n
1
1
log x Flog
x
,
Ý
Ý
i
i
ž
/
n
n
is1
is1
or
1rn
n
n
1
log
x
Flog
x
.
Ł
Ý
i
i
ž
/
ž
/
n
is1
is1
Since the logarithmic function is monotone increasing, we conclude that
1rn
n
n
1
x
F
x .
Ł
Ý
i
i
ž
/
n
is1
is1
Jensen’s inequality can also be used to show that the arithmetic mean is
greater than or equal to the harmonic mean. This assertion can be shown as
follows:
Ž .
y1
Consider the function f x sx
, which is convex for x0. If X is a
Ž
.
random variable with P X0 s1, then by Jensen’s inequality,
1
1
E
G
.
3.27
Ž
.
ž /
X
E X
Ž
.
In particular, if X has the discrete distribution described earlier, then
n
1
1
1
E
s
Ý
ž /
X
n
xi
is1
and
n
1
E X s
x .
Ž
.
Ý
i
n is1
Ž
.
By substitution in 3.27 we get
y1
n
n
1
1
1
G
x
,
Ý
Ý
i
ž
/
n
x
n
i
is1
is1
or
y1
n
n
1
1
1
x G
.
3.28
Ž
.
Ý
Ý
i ž
/
n
n
xi
is1
is1
Ž
.
The quantity on the right of inequality
3.28
is the harmonic mean of
x , x , . . . , x .
1
2
n

LIMITS AND CONTINUITY OF FUNCTIONS
86
Another example of the use of convex functions in statistics is in the
general theory of estimation. Let X , X , . . . , X
be a random sample of size
1
2
n
n from a population whose distribution depends on an unknown parameter
Ž
.
. Let 
 X , X , . . . , X
be an estimator of . By definition, the loss
1
2
n
w
Ž
.x
function L , 
 X , X , . . . , X
is a nonnegative function that measures the
1
2
n
Ž
.
loss incurred when  is estimated by 
 X , X , . . . , X . The expected value
1
2
n
Ž
.
Ž
.
mean of the loss function is called the risk function, denoted by R , 
 ,
that is,
R  , 
 sE L  , 
 X , X , . . . , X
.

4
Ž
.
Ž
.
1
2
n
The loss function is taken to be a convex function of . Examples of loss
functions include the squared error loss,
2
L  , 
 X , X , . . . , X
s y
 X , X , . . . , X
,
Ž
.
Ž
.
1
2
n
1
2
n
and the absolute error loss,
L  , 
 X , X , . . . , X
s y
 X , X , . . . , X
.
Ž
.
Ž
.
1
2
n
1
2
n
The first loss function is strictly convex, whereas the second is convex, but not
strictly convex.
The goodness of an estimator of  is judged on the basis of its risk
function, assuming that a certain loss function has been selected. The smaller
Ž
.
the risk, the more desirable the estimator. An estimator 
* X , X , . . . , X
1
2
n
Ž
.
is said to be admissible if there is no other estimator 
 X , X , . . . , X
of 
1
2
n
such that
R  , 
 FR  , 
*
Ž
.
Ž
.
Ž
.
for all g
 is the parameter space , with strict inequality for at least
Ž
.
one . An estimator 
X , X , . . . , X
is said to be a minimax estimator if it
0
1
2
n
Ž
.
minimizes the supremum with respect to 
of the risk function, that is,
sup R  , 
F sup R  , 
 ,
Ž
.
Ž
.
0
g
g
Ž
.
where 
 X , X , . . . , X
is any other estimator of . It should be noted that
1
2
n
a minimax estimator may not be admissible.
EXAMPLE 3.7.1.
Let X , X , . . . , X
be a random sample of size 20 from
1
2
20
Ž
.
Ž
.
the normal distribution N , 1 ,y. Let 
X , X , . . . , X
sX
1
1
2
20
20
Ž
.
be the sample mean, and let 
X , X , . . . , X
s0. Then, using a squared
2
1
2
20

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
87
error loss function,
2
1
R  , 
sE
X
y
sVar X
s
,
Ž
.
Ž
.
Ž
.
1
20
20
20
2
2
R  , 
sE
0y
s .
Ž
.
Ž
.
2
In this case,
1
sup R  , 
s
,
Ž
.
1
20

whereas
sup R  , 
s.
Ž
.
2

Thus 
 sX
is a better estimator than 
 s0. It can be shown that X
is
1
20
2
20
the minimax estimator of  with respect to a squared error loss function.
Note, however, that X
is not an admissible estimator, since
20
R  , 
FR  , 
Ž
.
Ž
.
1
2
for G20y1r2 or Fy20y1r2. However, for y20y1r2 20y1r2 ,
R  , 
R  , 
.
Ž
.
Ž
.
2
1
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Corwin, L. J., and R. H. Szczarba 1982 . Multi®ariable Calculus. Dekker, New York.
Ž
.
Fisz, M.
1963 . Probability Theory and Mathematical Statistics, 3rd ed. Wiley, New
Ž
York. Some continuous distributions are described in Chap. 5. Limit theorems
.
concerning sequences of random variables are discussed in Chap. 6.
Ž
.
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York.
Ž
.
Hardy, G. H. 1955 . A Course of Pure Mathematics, 10th ed. The University Press,
Ž
Cambridge, England. Section 89 of Chap. 4 gives definitions concerning the o
.
and O symbols introduced in Section 3.3 of this chapter.
Ž
.
Harris, B.
1966 . Theory of Probability. Addison-Wesley, Reading, Massachusetts.
Ž
.
Some continuous distributions are given in Section 3.5.
Ž
.
Henle, J. M., and E. M. Kleinberg
1979 . Infinitesimal Calculus. The MIT Press,
Cambridge, Massachusetts.
Ž
.
Hillier, F. S., and G. J. Lieberman
1967 . Introduction to Operations Research.
Ž
Holden-Day, San Francisco.
Convex sets and functions are described in Ap-
.
pendix 1.
Ž
.
Hogg, R. V., and A. T. Craig 1965 . Introduction to Mathematical Statistics, 2nd ed.
Ž
.
Macmillan, New York. Loss and risk functions are discussed in Section 9.3.
Ž
.
Hyslop, J. M. 1954 . Infinite Series, 5th ed. Oliver and Boyd, Edinburgh, England.
Ž
.
Chap. 1 gives definitions and summaries of results concerning the o, O notation.

LIMITS AND CONTINUITY OF FUNCTIONS
88
Ž
.
Johnson, N. L., and S. Kotz 1970a . Continuous Uni®ariate Distributions1. Houghton
Mifflin, Boston.
Ž
.
Johnson, N. L., and S. Kotz 1970b . Continuous Uni®ariate Distributions2. Houghton
Mifflin, Boston.
Ž
.
Ž
Lehmann, E. L.
1983 . Theory of Point Estimation. Wiley, New York. Section 1.6
.
discusses convex functions and their uses as loss functions.
Ž
.
Ž
Lindgren, B. W.
1976 . Statistical Theory, 3rd ed. Macmillan, New York.
The
concepts of loss and utility, or negative loss, used in statistical decision theory are
.
discussed in Chap. 8.
Ž
.
Randles, R. H., and D. A. Wolfe 1979 . Introduction to the Theory of Nonparametric
Ž
Statistics. Wiley, New York.
Some mathematical statistics results, including
.
Jensen’s inequality, are given in the Appendix.
Ž
.
Rao, C. R. 1973 . Linear Statistical Inference and Its Applications, 2nd ed. Wiley, New
York.
Ž
.
Roberts, A. W., and D. E. Varberg 1973 . Con®ex Functions. Academic Press, New
Ž
York. This is a handy reference book that contains all the central facts about
.
convex functions.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Ž
Reading, Massachusetts. Chap. 12 defines and provides discussions concerning
.
admissible and minimax estimators.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis, 2nd ed. McGraw-Hill, New
Ž
York. Limits of functions and some properties of continuous functions are given
.
in Chap. 4.
Ž
.
Sagan, H. 1974 . Ad®anced Calculus. Houghton Mifflin, Boston.
Ž
.
Smith, D. E. 1958 . History of Mathematics, Vol. 1. Dover, New York.
EXERCISES
In Mathematics
3.1. Determine if the following limits exist:
x 5y1
a
lim
,
Ž .
xy1
x™1
1
b
lim x sin
,
Ž .
x
x™0
1
1
c
lim
sin
sin
,
Ž .
ž
/ ž
/
x
x
x™0

EXERCISES
89
°
2
x y1
,
x0,
xy1
~
d
lim f x , where f x s
Ž .
Ž .
Ž .
3
x y1
x™0
,
x0.
¢2 xy1
Ž
.
3.2. Show that
( )
3
Ž
2.
a
tan x so x
as x™0.
'
( )
Ž
.
b
xso
x
as x™0,
( )
Ž .
Ž .
c
O 1 so x as x™,
( )
Ž . Ž .
y1
Ž .
Ž .
Ž
2.
Ž .
d
f x g x sx
qO 1
as
x™0, where
f x sxqo x
, g x s
y2
Ž
y1.
x
qO x
.
3.3. Determine where the following functions are continuous, and indicate
Ž
.
the points of discontinuity if any :
x sin 1rx ,
x0,
Ž
.
a
f x s
Ž .
Ž . ½ 0,
xs0,
1r2
xy1 r 2yx
,
x2,
Ž
. Ž
.
b
f x s
Ž .
Ž . ½ 1,
xs2,
xym r n,
x0,
c
f x s
Ž .
Ž . ½ 0,
xs0,
where m and n are positive integers,
x 4y2 x 2q3
d
f x s
,
x1.
Ž .
Ž .
3
x y1
Ž .
3.4. Show that f x
is continuous at xsa if and only if it is both left-
continuous and right-continuous at xsa.
3.5. Use Definition 3.4.1 to show that the function
f x sx 2y1
Ž .
is continuous at any point agR.
3.6. For what values of x is
3nx
f x s lim
Ž .
1ynx
n™
continuous?

LIMITS AND CONTINUITY OF FUNCTIONS
90
3.7. Consider the function
 
xy x
f x s
,
y1x1,
x0.
Ž .
x
Ž .
Can f x be defined at xs0 so that it will be continuous there?
Ž .
3.8. Let f x
be defined for all xgR and continuous at xs0. Further-
more,
f aqb sf a qf b ,
Ž
.
Ž .
Ž .
Ž .
for all a, b in R. Show that f x
is uniformly continuous everywhere
in R.
Ž .
3.9. Let f x be defined as
2 xy1,
0FxF1,
f x s
Ž .
3
2
½ x y5x q5,
1FxF2.
Ž .
w
x
Determine if f x is uniformly continuous on 0, 2 .
Ž .
3.10. Show that f x scos x is uniformly continuous on R.
3.11. Prove Theorem 3.4.1.
3.12. A function f: D™R is called upper semicontinuous at agD if for a
given 0 there exists a 0 such that
f x f a q
Ž .
Ž .
Ž .
for all xgN a lD. If the above inequality is replaced with

f x f a y ,
Ž .
Ž .
Ž .
then f x is said to be lower semicontinuous.
Show that if D is closed and bounded, then
( )
Ž .
Ž .
a
f x is bounded from above on D if f x is upper semicontinuous.
( )
Ž .
Ž .
b
f x is bounded from below on D if f x is lower semicontinuous.
w
x
Ž .
3.13. Let f:
a, b ™R be continuous such that f x s0 for every rational
w
x
Ž .
w
x
number in a, b . Show that f x s0 for every x in a, b .
3.14. For what values of x does the function




f x s3q xy1 q xq1
Ž .
have a unique inverse?

EXERCISES
91
3.15. Let f: R™R be defined as
x,
xF1,
f x s
Ž . ½ 2 xy1,
x1.
Find the inverse function f y1,
Ž .
2
Ž .
3.16. Let f x s2 x y8 xq8. Find the inverse of f x for
( )
a
xFy2,
( )
b
x2.
w
x
3.17. Suppose that f: a, b ™R is a convex function. Show that for a given
0 there exists a 0 such that
n
f a
yf b

Ž
.
Ž
.
Ý
i
i
is1
Ž
.4n
for every finite, pairwise disjoint family of open subintervals
a , b
i
i
is1
w
x
n
Ž
.
of a, b for which Ý
b ya .
is1
i
i
Note: A function satisfying this property is said to be absolutely contin-
w
x
uous on a, b .
w
x
3.18. Let f: a, b ™R be a convex function. Show that if a , a , . . . , a
are
1
2
n
w
x
positive numbers and x , x , . . . , x
are points in a, b , then
1
2
n
Ýn
a x
Ýn
a f x
Ž
.
is1
i
i
is1
i
i
f
F
,
ž
/
A
A
where AsÝn
a .
is1
i
Ž .
3.19. Let f x be continuous on D;R. Let S be the set of all xgD such
Ž .
that f x s0. Show that S is a closed set.
Ž .
w Ž .x
3.20. Let f x
be a convex function on D;R. Show that exp f x
is also
convex on D.
In Statistics
3.21. Let X be a continuous random variable with the cumulative distribu-
tion function
F x s1yeyx r,
x0.
Ž .
This is known as the exponential distribution. Its mean and variance
are s, 	 2s 2, respectively. Generate a random sample of five
observations from an exponential distribution with mean 2.

LIMITS AND CONTINUITY OF FUNCTIONS
92
w Hint: Select a ten-digit number from the table of random numbers, for
example, 8389611097. Divide it by 1010 to obtain the decimal number
0.8389611097. This number can be regarded as an observation from the
Ž
.
Ž .
uniform
distribution
U 0, 1 .
Now,
solve
the
equation
F x s
0.8389611097. The resulting value of x is considered as an observation
from the prescribed exponential distribution. Repeat this process four
more times, each time selecting a new decimal number from the table
x
of random numbers.
3.22. Verify Jensen’s inequality in each of the following two cases:
( )
Ž .
 
a
X is normally distributed and f x s x .
( )
Ž .
yx
b
X has the exponential distribution and f x se
.
3.23. Use the definition of convergence in probability to verify that if the

4
sequence of random variables
X
converges in probability to zero,
n ns1

24
then so does the sequence
X
.
n ns1
3.24. Show that
2
2


E X
G E
X
.
Ž
.
Ž
.
w


Ž .
2 x
Hint: Let Ys X . Apply Jensen’s inequality to Y with f x sx .
Deduce that if X has a mean  and a variance 	 2, then


E
Xy
F	 .
Ž
.
3.25. Consider the exponential distribution described in Exercise 3.21. Let
X , X , . . . , X
be a sample of size n from this distribution. Consider
1
2
n
the following estimators of :
( )
Ž
.
a

X , X , . . . , X
sX , the sample mean.
1
1
2
n
n
( )
Ž
.
b

X , X , . . . , X
sX q1,
2
1
2
n
n
( )
Ž
.
c

X , X , . . . , X
sX .
3
1
2
n
n
Determine the risk function corresponding to a squared error loss
function for each one of these estimators. Which estimator has the
smallest risk for all values of  ?

C H A P T E R
4
Differentiation
Differentiation originated in connection with the problems of drawing tan-
gents to curves and of finding maxima and minima of functions. Pierre
Ž
.
de Fermat
16011665 , the founder of the modern theory of numbers, is
credited with having put forth the main ideas on which differential calculus
is based.
In this chapter, we shall introduce the notion of differentiation and study
its applications in the determination of maxima and minima of functions. We
shall restrict our attention to real-valued functions defined on R, the set of
real numbers. The study of differentiation in connection with multivariable
n Ž
.
functions, that is, functions defined on R
nG1 , will be considered in
Chapter 7.
4.1. THE DERIVATIVE OF A FUNCTION
The notion of differentiation was motivated by the need to find the tangent
to a curve at a given point. Fermat’s approach to this problem was inspired by
a geometric reasoning. His method uses the idea of a tangent as the limiting
position of a secant when two of its points of intersection with the curve tend
to coincide. This has lead to the modern notation associated with the
derivative of a function, which we now introduce.
Ž .
Ž
.
Definition 4.1.1.
Let f x be a function defined in a neighborhood N x
r
0
of a point x . Consider the ratio
0
f x qh yf x
Ž
.
Ž
.
0
0
 h s
,
4.1
Ž .
Ž
.
h
Ž .
where h is a nonzero increment of x
such that yrhr. If  h has a
0
Ž .
limit as h™0, then this limit is called the derivative of f x
at x
and is
0
93

DIFFERENTIATION
94
Ž
.
denoted by f x
. It is also common to use the notation
0
df x
Ž .
sf x
.
Ž
.
0
xsx
dx
0
We thus have
f x qh yf x
Ž
.
Ž
.
0
0
f x
s lim
.
4.2
Ž
.
Ž
.
0
h
h™0
Ž
.
By putting xsx qh, formula 4.2 can be written as
0
f x yf x
Ž .
Ž
.
0
f x
s lim
.
Ž
.
0
xyx
x™x 0
0
Ž
.
Ž .
If f x
exists, then f x is said to be differentiable at xsx . Geometri-
0
0
Ž
.
Ž .
cally, f x
is the slope of the tangent to the graph of the function ysf x
0
Ž
.
Ž
.
Ž .
at the point
x , y
, where y sf x
. If f x has a derivative at every point
0
0
0
0
Ž .
of a set D, then f x is said to be differentiable on D.
Ž
.
It is important to note that in order for f x
to exist, the left-sided and
0
Ž .
Ž
.
right-sided limits of  h in formula 4.1 must exist and be equal as h™0,
or as x approaches x
from either side. It is possible to consider only
0
Ž .
one-sided derivatives at xsx . These occur when  h has just a one-sided
0
limit as h™0. We shall not, however, concern ourselves with such deriva-
tives in this chapter.

Functions that are differentiable at a point must necessarily be continuous
there. This will be shown in the next theorem.
Ž .
Theorem 4.1.1.
Let f x be defined in a neighborhood of a point x . If
0
Ž .
f x has a derivative at x , then it must be continuous at x .
0
0
Proof. From Definition 4.1.1 we can write
f x qh yf x
sh h .
Ž
.
Ž
.
Ž .
0
0
Ž .
Ž .
Ž
.
If the derivative of f x exists at x , then  h ™f x
as h™0. It follows
0
0
Ž .
from Theorem 3.2.1 2 that
f x qh yf x
™0
Ž
.
Ž
.
0
0
as h™0. Thus for a given 0 there exists a 0 such that
f x qh yf x

Ž
.
Ž
.
0
0


Ž .
if h . This indicates that f x is continuous at x .

0

THE DERIVATIVE OF A FUNCTION
95
It should be noted that even though continuity is a necessary condition for
differentiability, it is not a sufficient condition, as can be seen from the
Ž .
following example: Let f x be defined as
1
°x sin
,
x0,
~
f x s
Ž .
x
¢0,
xs0.
Ž .
Ž .
This function is continuous at xs0, since f 0 slim
f x s0 by the fact
x™0
that
1
 
x sin
F x
x
Ž .
for all x. However, f x is not differentiable at xs0. This is because when
xs0,
f h yf 0
Ž .
Ž .
 h s
Ž .
h
1
h sin
y0
h
s
,
since h0,
h
1
ssin
,
h
Ž .
which does not have a limit as h™0. Hence, f 0 does not exist.
Ž .
Ž .
If f x is differentiable on a set D, then f x is a function defined on D.
Ž .
In the event f x itself is differentiable on D, then its derivative is called the
Ž .
Ž .
second derivative of f x and is denoted by f x . It is also common to use
the notation
d2f x
Ž .
sf
x .
Ž .
2
dx
Ž
.
Ž .
By the same token, we can define the nth
nG2 derivative of f x as the
Ž
.
Ž .
derivative of the ny1 st derivative of f x . We denote this derivative by
dnf x
Ž .
Žn.
sf
x ,
ns2, 3, . . . .
Ž .
n
dx
We shall now discuss some rules that pertain to differentiation. The
reader is expected to know how to differentiate certain elementary functions
such as polynomial, exponential, and trigonometric functions.

DIFFERENTIATION
96
Ž .
Ž .
Theorem 4.1.2.
Let f x
and g x
be defined and differentiable on a
set D. Then
w
Ž .
Ž .x
Ž .
Ž .
1.
 f x qg x s f x qg x , where  and  are constants.
w Ž . Ž .x
Ž . Ž .
Ž .
Ž .
2.
f x g x sf x g x qf x g x .
w Ž .
Ž .x
w
Ž . Ž .
Ž .
Ž .x
2Ž .
Ž .
3.
f x rg x s f x g x yf x g x rg
x if g x 0.
Ž .
Ž .
Proof. The proof of 1 is straightforward. To prove 2 we write
f xqh g xqh yf x g x
Ž
. Ž
.
Ž . Ž .
lim
h
h™0
f xqh yf x
g xqh qf x
g xqh yg x
Ž
.
Ž .
Ž
.
Ž .
Ž
.
Ž .
s lim
h
h™0
f xqh yf x
Ž
.
Ž .
s lim g xqh
lim
Ž
.
h
h™0
h™0
g xqh yg x
Ž
.
Ž .
qf x
lim
.
Ž .
h
h™0
Ž
.
Ž .
Ž .
Ž
However, lim
g xqh sg x , since g x
is continuous
because it is
h™0
.
differentiable . Hence,
f xqh g xqh yf x g x
Ž
. Ž
.
Ž . Ž .
lim
sg x f x qf x g x .
Ž .
Ž .
Ž .
Ž .
h
h™0
Ž .
Now, to prove 3 we write
f xqh rg xqh yf x rg x
Ž
.
Ž
.
Ž .
Ž .
lim
h
h™0
g x f xqh yf x g xqh
Ž . Ž
.
Ž . Ž
.
s lim
hg x g xqh
h™0
Ž . Ž
.
g x
f xqh yf x
yf x
g xqh yg x
Ž .
Ž
.
Ž .
Ž .
Ž
.
Ž .
s lim
hg x g xqh
h™0
Ž . Ž
.
lim
g x
f xqh yf x
rhyf x
g xqh yg x
rh

4
Ž .
Ž
.
Ž .
Ž .
Ž
.
Ž .
h™0
s
g x lim
g xqh
Ž .
Ž
.
h™0
g x f x yf x g x
Ž .
Ž .
Ž .
Ž .
s
.

2
g
x
Ž .

THE DERIVATIVE OF A FUNCTION
97
Ž
.
Theorem 4.1.3 The Chain Rule .
Let f: D ™R and g: D ™R be two
1
2
Ž
.
Ž .
Ž .
functions. Suppose that f D
;D . If f x is differentiable on D and g x
1
2
1
Ž .
w Ž .x
is differentiable on D , then the composite function h x sg f x
is differ-
2
entiable on D , and
1
dg f x
dg f x
df x
Ž .
Ž .
Ž .
s
.
dx
df x
dx
Ž .
Ž .
Ž
.
Ž .
Proof. Let zsf x and tsf xqh . By the fact that g z is differentiable
we can write
g f xqh
yg f x
sg t yg z
Ž
.
Ž .
Ž .
Ž .
s tyz g z qo tyz ,
4.3
Ž
.
Ž .
Ž
.
Ž
.
where, if we recall, the o-notation was introduced in Section 3.3. We then
have
g f xqh
yg f x
tyz
o tyz
tyz
Ž
.
Ž .
Ž
.
s
g z q

.
4.4
Ž .
Ž
.
h
h
tyz
h
As h™0, t™z, and hence
tyz
f xqh yf x
df x
Ž
.
Ž .
Ž .
lim
s lim
s
.
h
h
dx
h™0
h™0
Ž
.
Now, by taking the limits of both sides of 4.4 as h™0 and noting that
o tyz
o tyz
Ž
.
Ž
.
lim
s lim
s0,
tyz
tyz
h™0
t™z
we conclude that
dg f x
df x
dg f x
Ž .
Ž .
Ž .
s
.

dx
dx
df x
Ž .
Ž .
Ž .
NOTE 4.1.1.
We recall that f x must be continuous in order for f x to
Ž .
exist. However, if f x exists, it does not have to be continuous. Care should
Ž .
be exercised when showing that f x
is continuous. For example, let us
consider the function
1
° 2
x sin
,
x0,
~
f x s
Ž .
x
¢0,
xs0.

DIFFERENTIATION
98
Ž .
Suppose that it is desired to show that f x exists, and if so, to determine if
Ž .
it is continuous. To do so, let us first find out if f x exists at xs0:
f h yf 0
Ž .
Ž .
f 0 s lim
Ž .
h
h™0
1
2
h sin h
s lim
h
h™0
1
s lim h sin
s0.
h
h™0
Ž .
Thus the derivative of f x exists at xs0 and is equal to zero. For x0, it is
Ž .
clear that the derivative of f x exists. By applying Theorem 4.1.2 and using
Ž .
our knowledge of the derivatives of elementary functions, f x
can be
written as
1
1
°2 x sin
ycos
,
x0,
~
f x s
Ž .
x
x
¢0,
xs0.
Ž .
We note that f x exists for all x, but is not continuous at xs0, since
1
1
lim f x s lim
2 x sin
ycos
Ž .
ž
/
x
x
x™0
x™0
Ž
.
does not exist, because cos 1rx
has no limit as x™0. However, for any
Ž .
nonzero value of x, f x is continuous.
Ž .
If f x is a convex function, then we have the following interesting result,
Ž
whose proof can be found in Roberts and Varberg
1973, Theorem C,
.
page 7 :
Ž
.
Ž
.
Theorem 4.1.4.
If f: a, b ™R is convex on the open interval a, b , then
Ž .
the set S where f x
fails to exist is either finite or countable. Moreover,
Ž .
Ž
.
Ž
.
f x is continuous on a, b yS, the complement of S with respect to a, b .
Ž .
 
For example, the function f x s x is convex on R. Its derivatives does
Ž
.
not exist at xs0 why? , but is continuous everywhere else.
Ž .
Ž .
The sign of f x
provides information about the behavior of f x
in a
neighborhood of x. More specifically, we have the following theorem:
Theorem 4.1.5.
Let f: D™R, where D is an open set. Suppose that
Ž .
Ž
.
f x is positive at a point x gD. Then there is a neighborhood N
x
;D
0

0
Ž .
Ž
.
such that for each
x in this neighborhood, f x f x
if
xx , and
0
0
Ž .
Ž
.
f x f x
if xx .
0
0

THE MEAN VALUE THEOREM
99
Ž
.
Proof. Let sf x
r2. Then, there exists a 0 such that
0
f x yf x
Ž .
Ž
.
0
f x
y
f x
q
Ž
.
Ž
.
0
0
xyx0


if xyx
. Hence, if xx ,
0
0
xyx
f x
Ž
.
Ž
.
0
0
f x yf x

,
Ž .
Ž
.
0
2
Ž .
Ž
.
Ž
.
which shows that f x f x
since f x
0. Furthermore, since
0
0
f x yf x
Ž .
Ž
.
0
0,
xyx0
Ž .
Ž
.
then f x f x
if xx .

0
0
Ž
.
Ž .
Ž
.
If f x
0, it can be similarly shown that f x f x
if xx , and
0
0
0
Ž .
Ž
.
f x f x
if xx .
0
0
4.2. THE MEAN VALUE THEOREM
This is one of the most important theorems in differential calculus. It is also
known as the theorem of the mean. Before proving the mean value theorem,
let us prove a special case of it known as Rolle’s theorem.
Ž
.
Ž .
Theorem 4.2.1 Rolle’s Theorem .
Let f x be continuous on the closed
w
x
Ž
.
Ž .
Ž .
interval
a, b and differentiable on the open interval
a, b . If f a sf b ,
Ž .
then there exists a point c, acb, such that f c s0.
Ž .
Ž .
Ž .
Proof. Let d denote the common value of f a and f b . Define h x s
Ž .
Ž .
Ž .
Ž .
Ž
.
Ž .
f x yd. Then h a sh b s0. If h x is also zero on a, b , then h x s0
for axb and the theorem is proved. Let us therefore assume that
Ž .
Ž
.
Ž .
w
x w
Ž .
h x 0 for some xg a, b . Since h x is continuous on a, b
because f x
x
is , then by Corollary 3.4.1 it must achieve its supremum M at a point  in
w
x
w
x
Ž .
a, b , and its infimum m at a point  in
a, b . If h x 0 for some
Ž
.
Ž .
xg a, b , then we must obviously have ab, because h x
vanishes at
Ž
.
Ž
.
both end points. We now claim that h  s0. If h  0 or 0, then by
Ž
.
Ž
.
Theorem 4.1.5, there exists a point x
in a neighborhood N
 ; a, b at
1
1
Ž
.
Ž
.
Ž
.
Ž
.
which h x
h  , a contradiction, since h  sM. Thus h  s0, which
1
Ž
.
Ž .
Ž .
Ž
.
implies that f  s0, since h x sf x for all xg a, b . We can similarly
Ž .
Ž .
Ž
.
arrive at the conclusion that f  s0 if h x 0 for some xg a, b . In this
Ž .
case, if h  0, then by Theorem 4.1.5 there exists a point x
in a neigh-
2

DIFFERENTIATION
100
Ž .
Ž
.
Ž
.
Ž .
borhood N
 ; a, b at which h x
h  sm, a contradiction, since m

2
2
Ž .
w
x
is the infimum of h x over a, b .
Ž .
Ž
.
Thus in both cases, whether h x 0 or 0 for some xg a, b , we must
Ž .
have a point c, acb, such that f c s0.

Ž .
Rolle’s theorem has the following geometric interpretation: If f x
satis-
Ž .
fies the conditions of Theorem 4.2.1, then the graph of ysf x must have a
tangent line that is parallel to the x-axis at some point c between a and b.
Ž
.
Note that there can be several points like c inside
a, b . For example, the
function ysx 3y5x 2q3xy1 satisfies the conditions of Rolle’s theorem on
'
w
x
Ž
.
Ž .
the interval
a, b , where as0 and bs 5q 13 r2. In this case, f a s
1
2
Ž .
Ž .
f b sy1, and f x s3x y10 xq3 vanishes at xs
and xs3.
3
Ž
.
Ž .
Theorem 4.2.2 The Mean Value Theorem .
If f x is continuous on the
w
x
Ž
.
closed interval a, b and differentiable on the open interval a, b , then there
exists a point c, acb, such that
f b sf a q bya f c .
Ž .
Ž .
Ž
.
Ž .
Proof. Consider the function
 x sf x yf a yA xya ,
Ž .
Ž .
Ž .
Ž
.
where
f b yf a
Ž .
Ž .
As
.
bya
Ž .
w
x
Ž
.
The function  x
is continuous on
a, b
and is differentiable on
a, b ,
Ž .
Ž .
Ž .
Ž .
since  x sf x yA. Furthermore,  a s b s0. It follows from
Ž .
Rolle’s theorem that there exists a point c, acb, such that  c s0.
Thus,
f b yf a
Ž .
Ž .
f c s
,
Ž .
bya
which proves the theorem.

The mean value theorem has also a nice geometric interpretation. If the
Ž .
graph of the function ysf x has a tangent line at each point of its length
Ž
.
between two points P and P
see Figure 4.1 , then there must be a point Q
1
2
between P
and P
at which the tangent line is parallel to the secant line
1
2
through P
and P . Note that there can be several points on the curve
1
2
between P and P
that have the same property as Q, as can be seen from
1
2
Figure 4.1.
The mean value theorem is useful in the derivation of several interesting
results, as will be seen in the remainder of this chapter.

THE MEAN VALUE THEOREM
101
Figure 4.1. Tangent lines parallel to the se-
cant line.
Ž .
Ž .
Ž
Corollary 4.2.1.
If f x
has a derivative f x
that is nonnegative non-
.
Ž
.
Ž .
Ž
.
positive on an interval a, b , then f x is monotone increasing decreasing
Ž
.
Ž .
Ž
.
Ž
.
Ž .
on
a, b . If
f x
is positive
negative
on
a, b , then
f x
is strictly
Ž
.
monotone increasing decreasing there.
Ž
.
Proof. Let x
and x
be two points in
a, b
such that x x . By the
1
2
1
2
mean value theorem, there exists a point x , x x x , such that
0
1
0
2
f x
sf x
q x yx
f x
.
Ž
.
Ž
.
Ž
.
Ž
.
2
1
2
1
0
Ž
.
Ž
.
Ž
.
Ž .
If f x
G0, then f x
Gf x
and f x is monotone increasing. Similarly,
0
2
1
Ž
.
Ž
.
Ž
.
Ž .
if f x
F0, then f x
Ff x
and f x
is monotone decreasing. If, how-
0
2
1
Ž .
Ž .
Ž
.
ever, f x 0, or f x 0 on
a, b , then strict monotonicity follows over
Ž
.
a, b .

Ž .
w
x
Theorem 4.2.3.
If f x is monotone increasing decreasing on an interval
Ž
.
Ž .
Ž .
w
Ž .
x
Ž
.
a, b , and if f x is differentiable there, then f x G0
f x F0 on a, b .
Ž
.
Ž
.
Ž
.
Proof. Let x g a, b . There exists a neighborhood N x
; a, b . Then,
0
r
0
Ž
.
for any xgN x
such that xx , the ratio
r
0
0
f x yf x
Ž .
Ž
.
0
xyx0
Ž .
Ž
.
Ž .
Ž
.
is nonnegative. This is true because f x Gf x
if xx
and f x Ff x
if
0
0
0
Ž
.
xx . By taking the limit of this ratio as x™x , we claim that f x
G0. To
0
0
0
Ž
.
prove this claim, suppose that f x
0. Then there exists a 0 such that
0
f x yf x
1
Ž .
Ž
.
0
yf x
y
f x
Ž
.
Ž
.
0
0
xyx
2
0

DIFFERENTIATION
102


if xyx
. It follows that
0
f x yf x
1
Ž .
Ž
.
0

f x
0.
Ž
.
0
xyx
2
0
Ž .
Ž
.
Ž
.
Thus f x f x
if xx , which is a contradiction. Hence, f x
G0.
0
0
0
Ž
.
Ž .
A similar argument can be used to show that
f x
F0 when
f x
is
0
monotone decreasing.

Note that strict monotonicity on a set D does not necessarily imply that
Ž .
Ž .
Ž .
3
f x 0, or f x 0, for all x in D. For example, the function f x sx
is
Ž .
strictly monotone increasing for all x, but f 0 s0.
Ž .
We recall from Theorem 3.5.1 that strict monotonicity of
f x
is a
sufficient condition for the existence of the inverse function fy1. The next
theorem shows that under certain conditions, fy1 is a differentiable function.
Ž .
Ž
Theorem 4.2.4.
Suppose that f x
is strictly monotone increasing
or
.
w
x
Ž .
decreasing
and continuous on an interval
a, b . If
f x
exists and is
Ž
.
y1
different from zero at x g a, b , then the inverse function f
is differen-
0
Ž
.
tiable at y sf x
and its derivative is given by
0
0
y1
df
y
1
Ž .
s
.
dy
f x
Ž
.
ysy
0
0
y1Ž .
Ž
.
Proof. By Theorem 3.5.2, f
y exists and is continuous. Let x g a, b ,
0
Ž
.
Ž
.
Ž
.
and let N x
; a, b for some r0. Then, for any xgN x
,
r
0
r
0
fy1 y yfy1 y
xyx
Ž .
Ž
.
0
0
s
yyy
f x yf x
Ž .
Ž
.
0
0
1
s
,
4.5
Ž
.
f x yf x
r xyx
Ž .
Ž
.
Ž
.
0
0
Ž .
y1
where ysf x . Now, since both f and f
are continuous, then x™x
if
0
Ž
.
and only if y™y . By taking the limits of all sides in formula
4.5 , we
0
conclude that the derivative of fy1 at y exists and is equal to
0
y1
df
y
1
Ž .
s
.

dy
f x
Ž
.
ysy
0
0
The following theorem gives a more general version of the mean value
theorem. It is due to Augustin-Louis Cauchy and has an important applica-
tion in calculating certain limits, as will be seen later in this chapter.

THE MEAN VALUE THEOREM
103
Ž
.
Ž .
Ž .
Theorem 4.2.5 Cauchy’s Mean Value Theorem .
If f x
and g x
are
w
x
continuous on the closed interval
a, b
and differentiable on the open
Ž
.
interval a, b , then there exists a point c, acb, such that
f b yf a
g c s g b yg a
f c ,
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Proof. The proof is based on using Rolle’s theorem in a manner similar to
Ž .
that of Theorem 4.2.2. Define the function  x as

x s f b yf x
g b yg a
y f b yf a
g b yg x
.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
w
x
Ž
.
This function is continuous on a, b and is differentiable on a, b , since
  x syf x
g b yg a
qg x
f b yf a
.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Furthermore,  a s b s0. Thus by Rolle’s theorem, there exists a point
Ž .
c, acb, such that   c s0, that is,
yf c
g b yg a
qg c
f b yf a
s0.
4.6
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž
.
Ž .
Ž .
Ž .
Ž .
In particular, if g b yg a 0 and f x
and g x
do not vanish at the
Ž
.
Ž
.
same point in a, b , then formula 4.6 an be written as
f c
f b yf a
Ž .
Ž .
Ž .
s
.

g c
g b yg a
Ž .
Ž .
Ž .
An immediate application of this theorem is a very popular method in
calculating the limits of certain ratios of functions. This method is attributed
Ž
.
to Guillaume Francois Marquis de l’Hospital 16611704 and is known as
Ž .
Ž .
l’Hospital’s rule. It deals with the limit of the ratio f x rg x as x™a when
both the numerator and the denominator tend simultaneously to zero or to
infinity as x™a. In either case, we have what is called an indeterminate
ratio caused by having 0r0 or r as x™a.
Ž
.
Ž .
Ž .
Theorem 4.2.6 l’Hospital’s Rule .
Let f x
and g x
be continuous on
w
x
Ž
.
the closed interval
a, b
and differentiable on the open interval
a, b .
Suppose that we have the following:
Ž .
Ž .
Ž
.
1. g x and g x are not zero at any point inside
a, b .
Ž .
Ž .
q
2. lim
f x rg x exists.
x™a
Ž .
Ž .
q
Ž .
Ž .
q
3. f x ™0 and g x ™0 as x™a , or f x ™ and g x ™ as x™a .
Then,
f x
f x
Ž .
Ž .
lim
s lim
.
q
q
g x
g x
Ž .
Ž .
x™a
x™a

DIFFERENTIATION
104
Proof. For the sake of simplicity, we shall drop the q sign from aq and
simply write x™a when x approaches a from the right. Let us consider the
following cases:
Ž .
Ž .
Ž
.
CASE 1.
f x ™0 and g x ™0 as x™a, where a is finite. Let xg a, b .
w
x
By applying Cauchy’s mean value theorem on the interval a, x we get
f x
f x yf a
f c
Ž .
Ž .
Ž .
Ž .
s
s
.
g x
g x yg a
g c
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
where acx. Note that f a sg a s0, since f x and g x are continu-
ous and their limits are equal to zero when x™a. Now, as x™a, c™a;
hence
f x
f c
f x
Ž .
Ž .
Ž .
lim
s lim
s lim
.
g x
g c
g x
x™a
c™a
x™a
Ž .
Ž .
Ž .
Ž .
Ž .
CASE 2.
f x ™0 and g x ™0 as x™. Let zs1rx. As x™, z™0.
Then
f x
f
z
Ž .
Ž .
1
lim
s lim
,
4.7
Ž
.
g x
g
z
x™
z™0
Ž .
Ž .
1
Ž .
Ž
.
Ž .
Ž
.
where f
z sf 1rz
and g
z sg 1rz . These functions are continuous
1
1
Ž .
Ž .
Ž
.
since f x and g x are, and z0 as z™0 see Theorem 3.4.2 . Here, we
Ž .
Ž .
Ž .
Ž .
find it necessary to set f 0 sg 0 s0 so that f
z
and g
z
will be
1
1
1
1
continuous at zs0, since their limits are equal to zero. This is equivalent to
Ž .
Ž .
defining f x
and g x
to be zero at infinity in the extended real number
system. Furthermore, by the chain rule of Theorem 4.1.3 we have
1

f
z sf x
y
,
Ž .
Ž .
1
2
ž
/
z
1

g
z sg x
y
.
Ž .
Ž .
1
2
ž
/
z
If we now apply Case 1, we get
f
z
f 
 z
Ž .
Ž .
1
1
lim
s lim

g
z
g
z
z™0
z™0
Ž .
Ž .
1
1
f x
Ž .
s lim
.
4.8
Ž
.
g x
x™
Ž .

THE MEAN VALUE THEOREM
105
Ž
.
Ž
.
From 4.7 and 4.8 we then conclude that
f x
f x
Ž .
Ž .
lim
s lim
.
g x
g x
x™
x™
Ž .
Ž .
Ž .
Ž .
CASE 3.
f x ™ and
g x ™ as
x™a, where
a is finite. Let
w
Ž .
Ž .x
lim
f x rg x sL. Then for a given 0 there exists a 0 such
x™a
that aqb and
f x
Ž .
yL  ,
4.9
Ž
.
g x
Ž .
if axaq. By applying Cauchy’s mean value theorem on the interval
w
x
x,aq
we get
f x yf aq
f d
Ž .
Ž
.
Ž .
s
,
g x yg aq
g d
Ž .
Ž
.
Ž .
Ž
.
where xdaq. From inequality 4.9 we then have
f x yf aq
Ž .
Ž
.
yL 
g x yg aq
Ž .
Ž
.
for all x such that axaq. It follows that
f x yf aq
Ž .
Ž
.
Ls lim g x yg aq
x™a
Ž .
Ž
.
f x
1yf aq rf x
Ž .
Ž
.
Ž .
s lim
lim
g x
1yg aq rg x
x™a
x™a
Ž .
Ž
.
Ž .
f x
Ž .
s lim g x
x™a
Ž .
Ž .
Ž .
since both f x and g x tend to  as x™a.
Ž .
Ž .
CASE 4.
f x ™ and g x ™ as x™. This can be easily shown by
using the techniques applied in Cases 2 and 3.
Ž .
Ž .
CASE 5.
lim
f x rg x s, where a is finite or infinite. Let us
x™a
Ž .
Ž .
consider the ratio g x rf x . We have that
g x
Ž .
lim
s0.
f x
x™a
Ž .

DIFFERENTIATION
106
Hence,
g x
g x
Ž .
Ž .
lim
s lim
s0.
f x
f x
x™a
x™a
Ž .
Ž .
If A is any positive number, then there exists a 0 such that
g x
1
Ž .

,
f x
A
Ž .
if axaq. Thus for such values of x,
f x
Ž .
A,
g x
Ž .
which implies that
f x
Ž .
lim
s.
g x
x™a
Ž .
Ž .
Ž .
When applying l’Hospital’s rule, the ratio f x rg x
may assume the
indeterminate from 0r0 or r as x™a. In this case, higher-order deriva-
Ž .
Ž .
tives of f x
and g x , assuming such derivatives exist, will be needed. In
Ž
.
Ž .
Ž .
general, if the first ny1 nG1 derivatives of f x and g x tend simultane-
Žn.Ž .
Žn.Ž .
ously to zero or to  as x™a, and if the nth derivatives, f
x and g
x ,
Ž .
Ž .
exist and satisfy the same conditions as those imposed on f x and g x in
Theorem 4.2.6, then
f x
f Žn. x
Ž .
Ž .
lim
s lim
.

Žn.
g x
g
x
x™a
x™a
Ž .
Ž .
A Historical Note
Ž
.
According to Eves
1976, page 342 , in 1696 the Marquis de l’Hospital
Ž
.
assembled the lecture notes of his teacher Johann Bernoulli 16671748 into
the world’s first textbook on calculus. In this book, the so-called l’Hospital’s
rule is found. It is perhaps more accurate to refer to this rule as the
Bernoulli-l’Hospital rule. Note that the name l’Hospital follows the old
French spelling and the letter s is not to be pronounced. In modern French
this name is spelled as l’Hopital.
ˆ
sin x
cos x
EXAMPLE 4.2.1.
lim
s lim
s1.
x
1
x™0
x™0
This is a well-known limit. It implies that sin x and x are asymptotically
Ž
.
equal, that is, sin xx as x™0 see Section 3.3 .
1ycos x
sin x
cos x
1
EXAMPLE 4.2.2.
lim
s lim
s lim
s
.
2
2 x
2
2
x
x™0
x™0
x™0
We note here that l’Hospital’s rule was applied twice before reaching the
1
limit
.
2

THE MEAN VALUE THEOREM
107
a x
EXAMPLE 4.2.3.
lim
, where a1.
x
x™
This is of the form r as x™. Since a xse x log a, then
a x
e x log a log a
Ž
.
lim
s lim
s.
x
1
x™
x™
This is also a well-known limit. On the basis of this result it can be shown
Ž
.
that see Exercise 4.12 the following hold:
a x
1.
lim
s, where a1, m0.
m
x
x™
log x
2.
lim
s0, where m0.
m
x
x™
EXAMPLE 4.2.4.
lim
q x x.
x™0
This is of the form 00 as x™0q, which is indeterminate. It can be reduced
to the form 0r0 or r so that l’Hospital’s rule can apply. To do so we write
x x as
x xse x log x.
However,
log x
x log xs
,
1rx
which is of the form yr as x™0q. By l’Hospital’s rule we then have
1rx
lim
x log x s lim
Ž
.
2
q
q y1rx
x™0
x™0
s lim
yx
Ž
.
q
x™0
s0.
It follows that
lim x xs lim e x log x
q
q
x™0
x™0
sexp
lim
x log x
Ž
.
q
x™0
s1.
xq1
EXAMPLE 4.2.5.
lim x log
.
ž
/
xy1
x™
This is of the form 0 as x™, which is indeterminate. But
xq1
logž
/
xq1
xy1
x log
s
ž
/
xy1
1rx

DIFFERENTIATION
108
is of the form 0r0 as x™. Hence,
y2
xq1
xy1
xq1
Ž
. Ž
.
lim x log
s lim
2
ž
/
xy1
y1rx
x™
x™
2
s lim
1y1rx
1q1rx
x™ Ž
. Ž
.
s2.
We can see from the foregoing examples that the use of l’Hospital’s rule
Ž .
Ž .
can facilitate the process of finding the limit of the ratio f x rg x as x™a.
Ž .
Ž .
In many cases, it is easier to work with f x rg x
than with the original
ratio. Many other indeterminate forms can also be resolved by l’Hospital’s
rule by first reducing them to the form 0r0 or r as was shown in
Examples 4.2.4 and 4.2.5.
It is important here to remember that the application of l’Hospital’s rule
Ž .
Ž .
requires that the limit of f x rg x exist as a finite number or be equal to
infinity in the extended real number system as x™a. If this is not the case,
Ž .
Ž .
then it does not necessarily follow that the limit of f x rg x does not exist.
Ž .
2
Ž
.
Ž .
Ž .
Ž .
For example, consider f x sx sin 1rx
and
g x sx. Here, f x rg x
tends to zero as x™0, as was seen earlier in this chapter. However, the ratio
f x
1
1
Ž .
s2 x sin
ycos
g x
x
x
Ž .
has no limit as x™0, since it oscillates inside a small neighborhood of the
origin.
4.3. TAYLOR’S THEOREM
This theorem is also known as the general mean value theorem, since it is
considered as an extension of the mean value theorem. It was formulated by
Ž
.
the English mathematician Brook Taylor 16851731 in 1712 and has since
become a very important theorem in calculus. Taylor used his theorem to
expand functions into infinite series. However, full recognition of the impor-
tance of Taylor’s expansion was not realized until 1755, when Leonhard
Ž
.
Euler 17071783 applied it in his differential calculus, and still later, when
Ž
.
Joseph Louis Lagrange 17361813 used it as the foundation of his theory of
functions.
Ž
.
Ž
.
Ž
.
Theorem 4.3.1 Taylor’s Theorem .
If the
ny1 st
nG1 derivative of
Ž .
Žny1.Ž .
w
x
f x , namely f
x , is continuous on the closed interval a, b and the nth
Žn.Ž .
Ž
.
w
x
derivative f
x exists on the open interval
a, b , then for each xg a, b

TAYLOR’S THEOREM
109
we have
2
xya
Ž
.
f x sf a q xya f a q
f a
Ž .
Ž .
Ž
.
Ž .
Ž .
2!
n
ny1
xya
xya
Ž
.
Ž
.
Žny1.
Žn.
q  q
f
a q
f
 ,
Ž .
Ž
.
ny1 !
n!
Ž
.
where ax.
Proof. The method to prove this theorem is very similar to the one used
w
x
Ž .
for Theorem 4.2.2. For a fixed x in a, b let the function  t be defined as
n
n
xyt

t sg
t y
g
a ,
Ž .
Ž .
Ž .
n
n
n
ž
/
xya
where aFtFb and
g
t sf x yf t y xyt f t
Ž .
Ž .
Ž .
Ž
.
Ž .
n
2
ny1
xyt
xyt
Ž
.
Ž
.
Žny1.
y
f t y  y
f
t .
4.10
Ž .
Ž .
Ž
.
2!
ny1 !
Ž
.
Ž .
The function  t has the following properties:
n
Ž .
Ž .
1. 
a s0 and 
x s0.
n
n
Ž .
w
x
2.  t is a continuous function of t on a, b .
n
Ž .
Ž
.
3. The derivative of  t with respect to t exists on a, b . This derivative
n
is equal to
ny1
n xyt
Ž
.



t sg
t q
g
a
Ž .
Ž .
Ž .
n
n
n
n
xya
Ž
.
syf t qf t y xyt f t q xyt f t
Ž .
Ž .
Ž
.
Ž .
Ž
.
Ž .
ny2
xyt
Ž
.
Žny1.
y  q
f
tŽ .
ny2 !
Ž
.
ny1
ny1
xyt
n xyt
Ž
.
Ž
.
Žn.
y
f
t q
g
a
Ž .
Ž .
n
n
ny1 !
Ž
.
xya
Ž
.
ny1
ny1
xyt
n xyt
Ž
.
Ž
.
Žn.
sy
f
t q
g
a .
Ž .
Ž .
n
n
ny1 !
Ž
.
xya
Ž
.

DIFFERENTIATION
110
Ž .
w
x
By applying Rolle’s theorem to  t on the interval a, x we can assert
n

Ž
.
that there exists a value , ax, such that 
 s0, that is,
n
ny1
ny1
xy
n xy
Ž
.
Ž
.
Žn.
y
f

q
g
a s0,
Ž
.
Ž .
n
n
ny1 !
Ž
.
xya
Ž
.
or
n
xya
Ž
.
Žn.
g
a s
f
 .
4.11
Ž .
Ž
.
Ž
.
n
n!
Ž
.
Ž
.
Using formula 4.10 in 4.11 , we finally get
2
xya
Ž
.
f x sf a q xya f a q
f a
Ž .
Ž .
Ž
.
Ž .
Ž .
2!
n
ny1
xya
xya
Ž
.
Ž
.
Žny1.
Žn.
q  q
f
a q
f
 .
4.12
Ž .
Ž
.
Ž
.
ny1 !
n!
Ž
.
This is known as Taylor’s formula. It can also be expressed as
h2
f aqh sf a qhf a q
f a
Ž
.
Ž .
Ž .
Ž .
2!
hny1
hn
Žny1.
Žn.
q  q
f
a q
f
aq h ,
4.13
Ž .
Ž
.
Ž
.
n
ny1 !
n!
Ž
.
where hsxya and 0 1.

n
Ž .
In particular, if f x
has derivatives of all orders in some neighborhood
Ž .
Ž
.
Ž .
N a of the point a, formula 4.12 can provide a series expansion of f x for
r
Ž .
Ž
.
Ž
.
xgN a
as n™. The last term in formula
4.12 , or formula
4.13 , is
r
called the remainder of Taylor’s series and is denoted by R . Thus,
n
n
xya
Ž
.
Žn.
R s
f

Ž
.
n
n!
hn
Žn.
s
f
aq h .
Ž
.
n
n!

TAYLOR’S THEOREM
111
If R ™0 as n™, then
n
n

xya
Ž
.
Žn.
f x sf a q
f
a ,
4.14
Ž .
Ž .
Ž .
Ž
.
Ý
n!
ns1
or

n
h
Žn.
f aqh sf a q
f
a .
4.15
Ž
.
Ž .
Ž .
Ž
.
Ý n!
ns1
This results in what is known as Taylor’s series. Thus the validity of Taylor’s
series is contingent on having R ™0 as n™, and on having derivatives of
n
Ž .
Ž .
all orders for f x
in N a . The existence of these derivatives alone is not
r
sufficient to guarantee a valid expansion.
A special form of Taylor’s series is Maclaurin’s series, which results when
Ž
.
as0. Formula 4.14 then reduces to

n
x
Žn.
f x sf 0 q
f
0 .
4.16
Ž .
Ž .
Ž .
Ž
.
Ý n!
ns1
In this case, the remainder takes the form
x n
Žn.
R s
f
 x .
4.17
Ž
.
Ž
.
n
n
n!
The sum of the first n terms in Maclaurin’s series provides an approxima-
Ž .
tion for the value of f x . The size of the remainder determines how close
Ž .
the sum is to f x . Since the remainder depends on  , which lies in the
n
Ž
.
interval
0, 1 , an upper bound on R
that is free of 
will therefore be
n
n
needed to assess the accuracy of the approximation. For example, let us
Ž .
consider the function f x scos x. In this case,
n
Žn.
f
x scos xq
,
ns1, 2, . . . ,
Ž .
ž
/
2
and
n
Žn.
f
0 scos
Ž .
ž
/
2
0,
n odd,
s
nr2
½ y1
,
n even.
Ž
.

DIFFERENTIATION
112
Ž
.
Formula 4.16 becomes

2 n
x
n
cos xs1q
y1
Ž
.
Ý
2n !
Ž
.
ns1
x 2
x 4
x 2 n
n
s1y
q
y  q y1
qR
,
Ž
.
2 nq1
2!
4!
2n !
Ž
.
Ž
.
where from formula 4.17 R
is
2 nq1
2 nq1
x
2nq1 
Ž
.
R
s
cos 
xq
.
2 nq1
2 nq1
2nq1 !
2
Ž
.


An upper bound on R
is then given by
2 nq1
  2 nq1
x


R
F
.
2 nq1
2nq1 !
Ž
.
Therefore, the error of approximating cos x with the sum
x 2
x 4
x 2 n
n
s
s1y
q
y  q y1
Ž
.
2 n
2!
4!
2n !
Ž
.
  2 nq1 Ž
.
does not exceed
x
r 2nq1 !, where x is measured in radians. For
example, if xsr3 and ns3, the sum
x 2
x 4
x6
s s1y
q
y
s0.49996
6
2
4!
6!
Ž
.
approximates cos r3 with an error not exceeding
  7
x
s0.00027.
7!
Ž
.
The true value of cos r3 is 0.5.
4.4. MAXIMA AND MINIMA OF A FUNCTION
In this section we consider the problem of finding the extreme values of a
Ž .
Ž .
function ysf x
whose derivative f x
exists in any open set inside its
domain of definition.

MAXIMA AND MINIMA OF A FUNCTION
113
Ž
.
Definition 4.4.1.
A function f: D™R has a local or relative maximum
Ž .
Ž
.
at a point x gD if there exists a 0 such that
f x Ff x
for all
0
0
Ž
.
Ž
.
xgN
x
lD. The function f has a local or relative minimum at x
if

0
0
Ž .
Ž
.
Ž
.
f x Gf x
for all xgN
x
lD. Local maxima and minima are referred to
0

0
Ž
.
as local optima or local extrema .

Ž
Definition 4.4.2.
A function f: D™R has an absolute maximum mini-
.
Ž .
Ž
.w Ž .
mum over D if there exists a point x*gD such that f x Ff x*
f x G
Ž
.x
f x*
for all xgD. Absolute maxima and minima are called absolute optima
Ž
.
or extrema .

Ž .
Ž .
The determination of local optima of f x can be greatly facilitated if f x
is differentiable.
Ž .
Ž
.
Theorem 4.4.1.
Let f x be differentiable on the open interval
a, b . If
Ž .
Ž
.
f x has a local maximum, or a local minimum, at a point x
in
a, b , then
0
Ž
.
f x
s0.
0
Ž .
Ž .
Ž
.
Proof. Suppose that f x has a local maximum at x . Then, f x Ff x
0
0
Ž
.
Ž
.
for all x in some neighborhood N
x
; a, b . It follows that

0
f x yf x
F0
if xx ,
Ž .
Ž
.
0
0
4.18
Ž
.
½ G0
if xx ,
xyx
0
0
Ž
.
q
Ž
.
for all x in N
x
. As x™x , the ratio in 4.18 will have a nonpositive

0
0
y
Ž
.
limit, and if x™x , the ratio will have a nonnegative limit. Since f x
0
0
Ž
.
exists, these two limits must be equal and equal to f x
as x™x . We
0
0
Ž
.
Ž .
therefore conclude that f x
s0. The proof when f x
has a local mini-
0
mum is similar.

Ž
.
It is important here to note that f x
s0 is a necessary condition for a
0
differentiable function to have a local optimum at x . It is not, however, a
0
Ž
.
sufficient condition. That is, if f x
s0, then it is not necessarily true that
0
Ž .
3
x
is a point of local optimum. For example, the function f x sx
has a
0
Ž .
zero derivative at the origin, but f x does not have a local optimum there
Ž
.
Ž
.
why not? . In general, a value x
for which f x
s0 is called a stationary
0
0
value for the function. Thus a stationary value does not necessarily corre-
spond to a local optimum.
Ž .
We should also note that Theorem 4.4.1 assumes that f x
is differen-
tiable in a neighborhood of x . If this condition is not fulfilled, the theorem
0
Ž
.
Ž .
ceases to be true. The existence of f x
is not prerequisite for f x to have
0
Ž .
a local optimum at x . In fact, f x can have a local optimum at x
even if
0
0
Ž
.
Ž .
 
f x
does not exist. For example, f x s x has a local minimum at xs0,
0
Ž .
but f 0 does not exist.

DIFFERENTIATION
114
Ž .
w
x
We recall from Corollary 3.4.1 that if f x is continuous on a, b , then it
w
x
must achieve its absolute optima at some points inside
a, b . These points
Ž
.
can be interior points, that is, points that belong to the open interval
a, b ,
Ž
.
Ž .
Ž
.
or they can be end boundary points. In particular, if f x exists on a, b ,
to determine the locations of the absolute optima we must solve the equation
Ž .
Ž .
f x s0 and then compare the values of f x at the roots of this equation
Ž .
Ž .
Ž
.
with f a
and f b . The largest
smallest
of these values is the absolute
Ž
.
Ž .
Ž
.
Ž .
maximum
minimum . In the event
f x 0 on
a, b , then
f x
must
achieve its absolute optimum at an end point.
4.4.1. A Sufficient Condition for a Local Optimum
We shall make use of Taylor’s expansion to come up with a sufficient
Ž .
condition for f x to have a local optimum at xsx .
0
Ž .
Ž
.
Suppose that f x has n derivatives in a neighborhood N
x
such that

0
Ž
.
Ž
.
Žny1.Ž
.
Žn.Ž
.
f x
sf x
s  sf
x
s0, but f
x
0. Then by Taylor’s the-
0
0
0
0
orem we have
hn
Žn.
f x sf x
q
f
x q h
Ž .
Ž
.
Ž
.
0
0
n
n!
Ž
.
for any x in N
x
, where hsxyx
and 0 1. Furthermore, if we

0
0
n
Žn.Ž .
assume that f
x is continuous at x , then
0
f Žn. x q h sf Žn. x
qo 1 ,
Ž
.
Ž
.
Ž .
0
n
0
Ž .
where, as we recall from Section 3.3, o 1 ™0 as h™0. We can therefore
write
hn
Žn.
n
f x yf x
s
f
x
qo h
.
4.19
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
0
0
n!
Ž .
Ž .
Ž
.
In order for f x to have a local optimum at x , f x yf x
must have the
0
0
Ž
.
same sign positive or negative for small values of h inside a neighborhood
Ž
.
Ž .
Ž
.
of 0. But, from 4.19 , the sign of f x yf x
is determined by the sign of
0
n
Žn.Ž
.
h f
x
. We can then conclude that if n is even, then a local optimum is
0
Žn.Ž
.
achieved at x . In this case, a local maximum occurs at x
if f
x
0,
0
0
0
Žn.Ž
.
whereas
f
x
0 indicates that
x
is a point of local minimum. If,
0
0
Ž .
Ž
.
however, n is odd, then x
is not a point of local optimum, since f x yf x
0
0
Ž .
changes sign around x . In this case, the point on the graph of ysf x
0
whose abscissa is x
is called a saddle point.
0
Ž
.
Ž
.
In particular, if f x
s0 and f x
0, then x
is a point of local
0
0
0
Ž
.
Ž .
optimum. When f x
0, f x
has a local maximum at x , and when
0
0
Ž
.
Ž .
f x
0, f x has a local minimum at x .
0
0

APPLICATIONS IN STATISTICS
115
Ž .
3
2
Ž .
2
EXAMPLE 4.4.1.
Let f x s2 x y3x y12 xq6. Then f x s6 x y6 x
y12s0 at xsy1, 2, and
12 xy6sy18
at xsy1,
f
x s
Ž . ½ 18
at xs2.
We have then a local maximum at xsy1 and a local minimum at xs2.
Ž .
4
EXAMPLE 4.4.2.
f x sx y1. In this case,
f x s4 x 3s0
at xs0,
Ž .
f
x s12 x 2s0
at xs0,
Ž .
f  x s24 xs0
at xs0,
Ž .
f Ž4. x s24.
Ž .
Then, xs0 is a point of local minimum.
Ž .
Ž
.2Ž
3
.
EXAMPLE 4.4.3.
Consider f x s xq5
x y10 . We have
2
f x s5 xq5
xy1
xq2
,
Ž .
Ž
. Ž
. Ž
.
f
x s10 xq2
2 x 2q8 xy1 ,
Ž .
Ž
. Ž
.
f  x s10 6 x 2q24 xq15 .
Ž .
Ž
.
Ž .
Here, f x s0 at xsy5, y2, and 1. At xsy5 there is a local maximum,
Ž
.
Ž .
since f y5 sy2700. At xs1 we have a local minimum, since f 1 s
Ž
.
2700. However, at xsy2 a saddle point occurs, since f y2 s0 and
Ž
.
f  y2 sy900.
Ž .
Ž
. Ž
.
EXAMPLE 4.4.4.
f x s 2 xq1 r xq4 , 0FxF5. Then
2
f x s7r xq4
.
Ž .
Ž
.
Ž .
Ž
.
Ž .
In this case, f x does not vanish anywhere in 0, 5 . Thus f x has no local
w
x
maxima or local minima in that open interval. Being continuous on 0, 5 ,
Ž .
Ž .
Ž .
f x must achieve its absolute optima at the end points. Since f x 0, f x
w
x
is strictly monotone increasing on
0, 5
by Corollary 4.2.1. Its absolute
minimum and absolute maximum are therefore attained at xs0 and xs5,
respectively.
4.5. APPLICATIONS IN STATISTICS
Differential calculus has many applications in statistics. Let us consider some
of these applications.

DIFFERENTIATION
116
4.5.1. Functions of Random Variables
Let Y
be a continuous random variable whose cumulative distribution
Ž .
Ž
.
Ž .
function is F y sP YFy . If F y
is differentiable for all y, then its
Ž .
Ž .
derivative F y is called the density function of Y and is denoted by f y .
Ž .
Continuous random variables for which f y exists are said to be absolutely
continuous.
Let Y be an absolutely continuous random variable, and let W be another
random variable which can be expressed as a function of Y of the form
Ž
.
Ws Y . Suppose that this function is strictly monotone and differentiable
over its domain. By Theorem 3.5.1,  has a unique inverse y1, which is also
Ž
.
differentiable by Theorem 4.2.4. Let G w denote the cumulative distribution
function of W.
If  is strictly monotone increasing, then
y1
y1
G w sP WFw sP YF
w
sF 
w
.
Ž
.
Ž
.
Ž
.
Ž
.
If it is strictly monotone decreasing, then
y1
y1
G w sP WFw sP YG
w
s1yF 
w
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
By differentiating G w using the chain rule we obtain the density function
Ž
.
g w for W, namely,
y1
y1
dF 
w
d
w
Ž
.
Ž
.
g w s
Ž
.
y1
dw
d
w
Ž
.
y1
d
w
Ž
.
y1
sf 
w
4.20
Ž
.
Ž
.
dw
if  is strictly monotone increasing, and
y1
y1
dF 
w
d
w
Ž
.
Ž
.
g w sy
Ž
.
y1
dw
d
w
Ž
.
y1
d
w
Ž
.
y1
syf 
w
4.21
Ž
.
Ž
.
dw
Ž
.
Ž
.
if  is strictly monotone decreasing. By combining
4.20
and
4.21
we
obtain
y1
d
w
Ž
.
y1
g w sf 
w
.
4.22
Ž
.
Ž
.
Ž
.
dw
Ž
.
For example, suppose that Y has the uniform distribution U 0, 1
whose
density function is
1,
0y1,
f y s
Ž . ½ 0
elsewhere.

APPLICATIONS IN STATISTICS
117
Ž
.
Let Wsylog Y. Using formula 4.22 , the density function of W is given by
eyw ,
0w,
g w s
Ž
. ½ 0
elsewhere.
( )
The Mean and Variance of Ws Y
The mean and variance of the random variable W can be obtained by using
its density function:

E W s
wg w dw,
Ž
.
Ž
.
H
y
2
Var W sE WyE W
Ž
.
Ž
.

2
s
wyE W
g w dw.
Ž
.
Ž
.
H
y
In some cases, however, the exact distribution of Y may not be known, or
Ž
.
g w may be a complicated function to integrate. In such cases, approximate
expressions for the mean and variance of W can be obtained by applying
Ž .
Taylor’s expansion around the mean of Y, . If we assume that   y exists,
then

y s
 q yy    qo yy .
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
If o yy is small enough, first-order approximations of E W
and Var W
can be obtained, namely,
2
2
E W f
 ,
since E Yy s0;
Var W f	
  
,
4.23
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
Ž
.
where 	 sVar Y , and the symbol f denotes approximate equality. If
Ž
.
o yy
is not small enough, then higher-order approximations can be
Ž .
Ž .
utilized provided that certain derivatives of  y exist. For example, if   y
exists, then
2
2
1

y s
 q yy    q
yy
 
 qo
yy
.
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
wŽ
.2x
In this case, if o
yy
is small enough, then second-order approximations
Ž
.
Ž
.
can be obtained for E W
and Var W
of the form
2
1
2
2
E W f
 q 	  
 ,
since E Yy
s	 ,
Ž
.
Ž
.
Ž
.
Ž
.
2
2
Var W fE Q Y yE Q Y
,

4
Ž
.
Ž
.
Ž
.

DIFFERENTIATION
118
where
2
1
Q Y s
 q Yy    q
Yy
 
 .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
Thus,
2
2
2
1
2
Var W f	
  
q
 

Var
Yy
Ž
.
Ž
.
Ž
.
Ž
.
4
3
q    
 E
Yy
.
Ž
.
Ž
.
Ž
.
Variance Stabilizing Transformations
One of the basic assumptions of regression and analysis of variance is the
constancy of the variance 	 2 of a response variable Y on which experimental
data are obtained. This assumption is often referred to as the assumption of
homoscedasticity. There are situations, however, in which 	 2 is not constant
for all the data. When this happens, Y
is said to be heteroscedastic.
Heteroscedasticity can cause problems and difficulties in connection with the
Ž
statistical analysis of the data for a survey of the problems of heteroscedas-
.
ticity, see Judge et al., 1980 .
Ž
Some situations that lead to heteroscedasticity are see Wetherill et al.,
.
1986, page 200 :
i. The Use of A®eraged Data. In many experimental situations, the data
used in a regression program consist of averages of samples that are
different in size. This happens sometimes in survey analysis.
ii. Variances Depending on the Explanatory Variables. The variance of an
Ž
.
observation can sometimes depend on the explanatory
or input
variables in the hypothesized model, as is the case with some econo-
metric models. For example, if the response variable is household
expenditure and one explanatory variable is household income, then
the variance of the observations may be a function of household
income.
iii. Variances Depending on the Mean Response. The response variable Y
may have a distribution whose variance is a function of its mean, that
2
Ž
.
is, 	 sh  , where  is the mean of Y. The Poisson distribution, for
2
Ž
example, has the property that 	 s. Thus as  changes
as a
.
2
function of some explanatory variables , then so will 	 . The following
Ž
example illustrates this situation see Chatterjee and Price, 1977, page
.
39 : Let Y
be the number of accidents, and
x be the speed of
operating a lathe in a machine shop. Suppose that a linear relationship
is assumed between Y and x of the form
Ys q xq ,
0
1
where  is a random error with a zero mean. Here, Y has the Poisson
distribution with mean s q x. The variance of Y, being equal
0
1
to , will not be constant, since it depends on x.

APPLICATIONS IN STATISTICS
119
Heteroscedasticity due to dependence on the mean response can be
removed, or at least reduced, by a suitable transformation of the response
2
Ž
.
Ž
.
variable Y. So let us suppose that 	 sh  . Let Ws Y . We need to
find a proper transformation  that causes W to have almost the constant
variance property. If this can be accomplished, then  is referred to as a
variance stabilizing transformation.
Ž
.
If the first-order approximation of Var W
by Taylor’s expansion is ade-
Ž
.
quate, then by formula 4.23 we can select  so that
2
h 
  
sc,
4.24
Ž
.
Ž
.
Ž
.
where c is a constant. Without loss of generality, let cs1. A solution of
Ž
.
4.24 is given by
d

 s
.
Ž
. H'h 
Ž
.
Ž
.
Ž
.
Thus if Ws Y , then Var W
will have a variance approximately equal to
Ž
.
one. For example, if h  s, as is the case with the Poisson distribution,
then
d
'

 s
s2  .
Ž
. H'
'
Ž
Hence, Ws2 Y will have a variance approximately equal to one. In this
'
case, it is more common to use the transformation Ws Y which has a
.
variance approximately equal to 0.25 . Thus in the earlier example regarding
the relationship between the number of accidents and the speed of operating
'
a lathe, we need to regress
Y against x in order to ensure approximate
homosecdasticity.
Ž
.
2
The relationship
if any
between 	
and  may be determined by
theoretical considerations based on a knowledge of the type of data usedfor
example, Poisson data. In practice, however, such knowledge may not be
known a priori. In this case, the appropriate transformation is selected
empirically on the basis of residual analysis of the data. See, for example, Box
Ž
.
Ž
.
and Draper 1987, Chapter 8 , Montgomery and Peck 1982, Chapter 3 . If
Ž
possible, a transformation is selected to correct nonnormality if the original
.
data are not believed to be normally distributed as well as heteroscedasticity.
In this respect, a useful family of transformations introduced by Box and Cox
Ž
.
1964 can be used. These authors considered the power family of transfor-
mations defined by
Y y1 r,
0,
Ž
.
 Y s
Ž
. ½ log Y,
s0.

DIFFERENTIATION
120
This family may only be applied when Y has positive values. Furthermore,
since by l’Hospital’s rule
Y y1

lim
s lim Y
log Yslog Y,

™0
™0
the BoxCox transformation is a continuous function of . An estimate of 
Ž
can be obtained from the data using the method of maximum likelihood see
Montgomery and Peck, 1982, Section 3.7.1; Box and Draper, 1987, Section
.
8.4 .
Asymptotic Distributions
The asymptotic distributions of functions of random variables are of special
interest in statistical limit theory. By definition, a sequence of random

4
variables Y
converges in distribution to the random variable Y if
n ns1
lim F
y sF y
Ž .
Ž .
n
n™
Ž .
Ž .
at each point y where F y
is continuous, where F
y
is the cumulative
n
Ž
.
Ž .
distribution function of Y
ns1, 2, . . .
and F y is the cumulative distribu-
n
Ž
.
tion function of Y see Section 5.3 concerning sequences of functions . This
form of convergence is denoted by writing
d
Y ™Y.
n
An illustration of convergence in distribution is provided by the well-known

4
central limit theorem. It states that if Y
is a sequence of independent and
n ns1
identically distributed random variables with common mean and variance, 
2
n
and 	 , respectively, that are both finite, and if Y sÝ
Yrn is the sample
n
is1
i
mean of a sample size n, then as n™,
Y y
d
n
™Z,
'
	r n
Ž
.
where Z has the standard normal distribution N 0, 1 .
An extension of the central limit theorem that includes functions of
random variables is given by the following theorem:

4
Theorem 4.5.1.
Let Y
be a sequence of independent and identically
n ns1
2 Ž
.
distributed random variables with mean  and variance 	
both finite , and
Ž .
let Y
be the sample mean of a sample of size n. If  y is a function whose
n
Ž .
derivative   y exists and is continuous in a neighborhood of  such that

APPLICATIONS IN STATISTICS
121
Ž
.
   0, then as n™,
 Y
y

Ž
.
Ž
.
d
n
™Z.
'
	   
r n
Ž
.
Ž
.
Proof. See Wilks 1962, page 259 .

On the basis of Theorem 4.5.1 we can assert that when n is large enough,
Ž
.
Ž
.
 Y
is approximately distributed as a normal variate with a mean   and
n
2
'
Ž
.
Ž
.
Ž .
a standard deviation
	r n
   . For example, if  y sy , then as
n™,
2
2
Y y
d
n
™Z.
'


2  	r n
4.5.2. Approximating Response Functions
Perhaps the most prevalent use of Taylor’s expansion in statistics is in the
area of linear models. Let Y denote a response variable, such as the yield of
Ž .
Ž
a product, whose mean  x
is believed to depend on an explanatory
or
.
input
variable x such as temperature or pressure. The true relationship
Ž .
between  and x is usually unknown. However, if  x is considered to have
derivatives of all orders, then it is possible to approximate its values by using
low-order terms of a Taylor’s series over a limited range of interest. In this
Ž .
case,  x
can be represented approximately by a polynomial of degree d
Ž
.
G1 of the form
d
j
 x s q
 x ,
Ž .
Ý
0
j
js1
where  ,  , . . . , 
are unknown parameters. Estimates of these parame-
0
1
d
Ž
.
ters are obtained by running n Gdq1 experiments in which n observa-
tions, y , y , . . . , y , on Y are obtained for specified values of x. This leads us
1
2
n
to the linear model
d
j
y s q
 x q ,
is1, 2, . . . , n,
4.25
Ž
.
Ý
i
0
j
i
i
js1
where  is a random error. The method of least squares can then be used to
i
Ž
.
Ž
.
estimate the unknown parameters in 4.25 . The adequacy of model 4.25 to
Ž .
represent the true mean response  x can be checked using the given data
provided that replicated observations are available at some points inside the
region of interest. For more details concerning the adequacy of fit of linear
models and the method of least squares, see, for example, Box and Draper
Ž
.
Ž
.
1987, Chapters 2 and 3 and Khuri and Cornell 1996, Chapter 2 .

DIFFERENTIATION
122
4.5.3. The Poisson Process
A random phenomenon that arises through a process which continues in time
Ž
.
or space
in a manner controlled by the laws of probability is called a
stochastic process. A particular example of such a process is the Poisson
process, which is associated with the number of events that take place over a
period of timefor example, the arrival of customers at a service counter, or
the arrival of -rays, emitted from a radioactive source, at a Geiger counter.
Ž .
Define p t
as the probability of n arrivals during a time interval of
n
length t. For a Poisson process, the following postulates are assumed to hold:
1. The probability of exactly one arrival during a small time interval of
length h is approximately proportional to h, that is,
p
h shqo h
Ž .
Ž .
1
as h™0, where  is a constant.
2. The probability of more than one arrival during a small time interval of
length h is negligible, that is,
p
h so h
Ž .
Ž .
Ý
n
n1
as h™0.
3. The probability of an arrival occurring during a small time interval
Ž
.
t, tqh does not depend on what happened prior to t. This means that
the events defined according to the number of arrivals occurring during
nonoverlapping time intervals are independent.
Ž .
On the basis of the above postulates, an expression for p t can be found
n
as follows: For nG1 and for small h we have approximately
p
tqh sp
t p
h qp
t p
h
Ž
.
Ž .
Ž .
Ž .
Ž .
n
n
0
ny1
1
sp
t
1yhqo h
qp
t
hqo h
,
4.26
Ž .
Ž .
Ž .
Ž .
Ž
.
n
ny1
Ž
.
since the probability of no arrivals during the time interval
t, tqh
is
Ž .
approximately equal to 1yp h . For ns0 we have
1
p
tqh sp
t p
h
Ž
.
Ž .
Ž .
0
0
0
sp
t
1yhqo h
.
4.27
Ž .
Ž .
Ž
.
0

APPLICATIONS IN STATISTICS
123
Ž
.
Ž
.
From 4.26 and 4.27 we then get for nG1,
p
tqh yp
t
o h
o h
Ž
.
Ž .
Ž .
Ž .
n
n
sp
t
yq
qp
t
q
,
Ž .
Ž .
n
ny1
h
h
h
and for ns0,
p
tqh yp
t
o h
Ž
.
Ž .
Ž .
0
0
sp
t
yq
.
Ž .
0
h
h
By taking the limit as h™0 we obtain the derivatives
p
 t syp
t qp
t ,
nG1
4.28
Ž .
Ž .
Ž .
Ž
.
n
n
ny1
p
t syp
t .
4.29
Ž .
Ž .
Ž
.
0
0
Ž
.
Ž .
From 4.29 the solution for p
t is given by
0
p
t sey t,
4.30
Ž .
Ž
.
0
Ž .
Ž
.
since p
t s1 when ts0
that is, initially there were no arrivals . By
0
Ž
.
Ž
.
substituting 4.30 in 4.28 when ns1 we get
p
 t syp
t qey t.
4.31
Ž .
Ž .
Ž
.
1
1
Ž
.
t
If we now multiply the two sides of 4.31 by e
we obtain
etp
 t qp
t ets,
Ž .
Ž .
1
1
or
t
e p
t
s.
Ž .
1
Hence,
etp
t stqc,
Ž .
1
Ž .
where c is a constant. This constant must be equal to zero, since p 0 s0.
1
We then have
p
t stey t.
Ž .
1
Ž
.
Ž .
By continuing in this process and using equation 4.28 we can find p
t ,
2
Ž .
then p
t , . . . , etc. In general, it can be shown that
3
n
y t
e
t
Ž
.
p
t s
,
ns0, 1, 2, . . . .
4.32
Ž .
Ž
.
n
n!

DIFFERENTIATION
124
Ž
.
In particular, if ts1, then formula 4.32 gives the probability of n arrivals
during one unit of time, namely,
eyn
p
1 s
,
ns0, 1, . . . .
Ž .
n
n!
This gives the probability mass function of a Poisson random variable with
mean .
4.5.4. Minimizing the Sum of Absolute Deviations
Consider a data set consisting of n observations
y , y , . . . , y . For an
1
2
n
Ž .
arbitrary real number a, let D a denote the sum of absolute deviations of
the data from a, that is,
n


D a s
y ya .
Ž .
Ý
i
is1
Ž .
For a given a, D a represents a measure of spread, or variation, for the data
Ž .
set. Since the value of D a varies with a, it may be of interest to determine
Ž .
its minimum. We now show that D a is minimized when as*, where *
denotes the median of the data set. By definition, * is a value that falls in
the middle when the observations are arranged in order of magnitude. It is a
measure of location like the mean. If we write y
Fy
F  Fy
for the
Ž1.
Ž2.
Žn.
ordered y ’s, then when n is odd, * is the unique value y
; whereas
i
Žnr2q1r2.
when n is even, * is any value such that y
F*Fy
. In the latter
Žnr2.
Žnr2q1.
case, * is sometimes chosen as the middle of the interval.
Ž .
There are several ways to show that * minimizes D a . The following
Ž
.
simple proof is due to Blyth 1990 :
On the interval y
ay
, ks1, 2, . . . , ny1, we have
Žk.
Žkq1.
n


D a s
y
ya
Ž .
Ý
Ži.
is1
k
n
s
ayy
q
y
ya
Ž
.
Ž
.
Ý
Ý
Ži.
Ži.
is1
iskq1
k
n
skay
y
q
y
y nyk a.
Ž
.
Ý
Ý
Ži.
Ži.
is1
iskq1
Ž .
The function D a is continuous for all a and is differentiable everywhere
Ž
.
Ž .
except at y , y , . . . , y . For ay
is1, 2, . . . , n , the derivative D a
is
1
2
n
i
given by
D a s2 kynr2 .
Ž .
Ž
.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
125
Ž .
Ž
.
Ž .
If knr2, then D a 0 on
y
, y
, and by Corollary 4.2.1, D a
Žk.
Žkq1.
w
x
must be strictly monotone on
y
, y
, ks1, 2, . . . , ny1.
Žk.
Žkq1.
Ž .
Now, when n is odd, D a
is strictly monotone decreasing for aF
Ž .
Ž
.
y
, because D a 0 over
y
, y
for knr2. It is strictly
Žnr2q1r2.
Žk.
Žkq1.
Ž .
Ž
.
monotone increasing for aGy
, because D a 0 over
y
, y
Žnr2q1r2.
Žk.
Žkq1.
Ž .
for knr2. Hence, *sy
is a point of absolute minimum for D a .
Žnr2q1r2.
Ž .
Furthermore, when n is even, D a
is strictly monotone decreasing for
Ž .
Ž
.
Ž .
aFy
, because D a 0 over
y
, y
for knr2. Also, D a
is
Žnr2.
Žk.
Žkq1.
Ž
.
Ž .
constant over
y
, y
, because D a s0 for ksnr2, and is strictly
Žnr2.
Žnr2q1.
Ž .
Ž
.
monotone increasing for aGy
, because D a 0 over
y
, y
for
Žnr2q1.
Žk.
Žkq1.
Ž .
knr2. This indicates that D a achieves its absolute minimum at any point
* such that y
F*Fy
, which completes the proof.
Žnr2.
Žnr2q1.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Apostol, T. M.
1964 .
Mathematical Analysis. Addison-Wesley, Reading, Mas-
Ž
.
sachusetts. Chap. 5 discusses differentiation of functions of one variable.
Ž
.
Blyth, C. R. 1990 . ‘‘Minimizing the sum of absolute deviations.’’ Amer. Statist., 44,
329.
Ž
.
Box, G. E. P., and D. R. Cox 1964 . ‘‘An analysis of transformations.’’ J. Roy. Statist.
Soc. Ser. B, 26, 211243.
Ž
.
Box, G. E. P., and N. R. Draper
1987 . Empirical Model-Building and Response
Ž
Surfaces. Wiley, New York. Chap. 2 introduces the idea of approximating the
mean of a response variable using low-order polynomials; Chap. 3 discusses the
method of least squares for fitting empirical models; the use of transformations,
.
including those for stabilizing variances, is described in Chap. 8.
Ž
.
Box, G. E. P., W. G. Hunter, and J. S. Hunter
1978 . Statistics for Experimenters.
Ž
Wiley, New York. Various transformations are listed in Chap. 7, which include
.
the BoxCox and variance stabilizing transformations.
Ž
.
Ž
Buck, R. C. 1956 . Ad®anced Calculus, McGraw-Hill, New York. Chap. 2 discusses
.
the mean value theorem and l’Hospital’s rule.
Ž
.
Chatterjee, S., and B. Price 1977 . Regression Analysis by Example. Wiley, New York.
ŽChap. 2 includes a discussion concerning variance stabilizing transformations, in
addition to detection and removal of the effects of heteroscedasticity in regres-
.
sion analysis.
Ž
.
Cooke, W. P. 1988 . ‘‘L’Hopital’srule in a Poisson derivation.’’ Amer. Math. Monthly,
ˆ
95, 253254.
Ž
.
Eggermont, P. P. B.
1988 . ‘‘Noncentral difference quotients and the derivative.’’
Amer. Math. Monthly, 95, 551553.
Ž
.
Eves, H. 1976 . An Introduction to the History of Mathematics, 4th ed. Holt, Rinehart
and Winston, New York.
Ž
.
Ž
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York. Differentiation is the
.
subject of Chap. 4.

DIFFERENTIATION
126
Ž
.
Georgiev, A. A.
1984 . ‘‘Kernel estimates of functions and their derivatives with
applications,’’ Statist. Probab. Lett., 2, 4550.
Ž
.
Hardy, G. H. 1955 . A Course of Pure Mathematics, 10th ed. The University Press,
Ž
Cambridge, England. Chap. 6 covers differentiation and provides some interest-
.
ing examples.
Ž
.
Hogg, R. V., and A. T. Craig 1965 . Introduction to Mathematical Statistics, 2nd ed.
Ž
Macmillan, New York. Chap. 4 discusses distributions of functions of random
.
variables.
Ž
.
James, A. T., and R. A. J. Conyers 1985 . ‘‘Estimation of a derivative by a difference
quotient: Its application to hepatocyte lactate metabolism.’’ Biometrics, 41,
467476.
Ž
.
Judge, G. G., W. E. Griffiths, R. C. Hill, and T. C. Lee
1980 . The Theory and
Practice of Econometrics. Wiley, New York.
Ž
.
Khuri, A. I., and J. A. Cornell 1996 . Response Surfaces, 2nd ed. Dekker, New York.
ŽChaps. 1 and 2 discuss the polynomial representation of a response surface and
.
the method of least squares.
Ž
.
Ž
Lindgren, B. W. 1976 . Statistical Theory, 3rd ed. Macmillan, New York. Section 3.2
.
discusses the development of the Poisson process.
Ž
.
Menon, V. V., B. Prasad, and R. S. Singh 1984 . ‘‘Non-parametric recursive estimates
of a probability density function and its derivatives.’’ J. Statist. Plann. Inference, 9,
7382.
Ž
.
Montgomery, D. C., and E. A. Peck 1982 . Introduction to Linear Regression Analysis.
Ž
Wiley, New York.
Chap. 3 presents several methods useful for checking the
validity of the basic regression assumptions. Several variance stabilizing transfor-
.
mations are also listed.
Ž
.
Ž
Parzen, E. 1962 . Stochastic Processes. Holden-Day, San Francisco.
Chap. 1 intro-
.
duces the definition of stochastic processes including the Poisson process.
Ž
.
Roberts, A. W., and D. E. Varberg 1973 . Con®ex Functions. Academic Press, New
Ž
York. Chap. 1 discusses a characterization of convex functions using derivatives;
.
Chap. 5 discusses maxima and minima of differentiable functions.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Ž
Reading, Massachusetts.
Chap. 3 discusses absolutely continuous random vari-
.
ables.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis. 2nd ed. McGraw-Hill, New
Ž
.
York. Differentiation is discussed in Chap. 5.
Ž
.
Ž
Sagan, H. 1974 . Ad®anced Calculus. Houghton Mifflin, Boston. Chap. 3 discusses
.
differentiation.
Wetherill, G. B., P. Duncombe, M. Kenward, J. Kollerstrom, S. R. Paul, and B. J.
¨
¨
Ž
.
Vowden 1986 . Regression Analysis with Applications. Chapman and Hall, Lon-
Ž
don, England. Section 9.2 discusses the sources of heteroscedasticity in regres-
.
sion analysis.
Ž
.
Ž
Wilks, S. S. 1962 . Mathematical Statistics. Wiley, New York. Chap. 9 considers limit
.
theorems including asymptotic distributions of functions of the sample mean.

EXERCISES
127
EXERCISES
In Mathematics
Ž .
Ž .
4.1. Let f x be defined in a neighborhood of the origin. Show that if f 0
exists, then
f h yf yh
Ž .
Ž
.
lim
sf 0 .
Ž .
2h
h™0
Give a counterexample to show that the converse is not true in general,
Ž .
that is, if the above limit exists, then it is not necessary that f 0 exists.
Ž .
Ž .
w
x
4.2. Let
f x
and
g x
have derivatives up to order n on
a, b . Let
Ž .
Ž . Ž .
h x sf x g x . Show that
n
n
Žn.
Žk.
Žnyk .
h
x s
f
x g
x .
Ž .
Ž .
Ž .
Ý ž /
k
ks0
Ž
.
This is known as Leibniz’s formula.
Ž .
4.3. Suppose that f x has a derivative at a point x , ax b. Show that
0
0
Ž
.
there exists a neighborhood N
x
and a positive number A such that

0


f x yf x
A xyx
Ž .
Ž
.
0
0
Ž
.
for all xgN
x
, xx .

0
0
Ž .
Ž
.
Ž .
4.4. Suppose that f x
is differentiable on 0,  and f x ™0 as x™.
Ž .
Ž
.
Ž .
Ž .
Let g x sf xq1 yf x . Prove that g x ™0 as x™.
Ž .
4.5. Let the function f x be defined as
x 3y2 x,
xG1,
f x s
Ž . ½
2
ax ybxq1,
x1.
Ž .
For what values of a and b does f x have a continuous derivative?
Ž .
Ž
.
4.6. Suppose that f x
is twice differentiable on 0,  . Let m , m , m
be
0
1
2
 Ž .

Ž .

Ž .
the least upper bounds of f x
, f x
, and
f x
, respectively, on
Ž
.
0,  .

DIFFERENTIATION
128
( )
a
Show that
m0
f x
F
qhm
Ž .
2
h
Ž
.
for all x in 0,  and for every h0.
( )
Ž .
b
Deduce from a that
m2F4m m .
1
0
2
Ž .
Ž .
4.7. Suppose that lim
f x exists. Does it follow that f x is differen-
x™x 0
tiable at x ? Give a proof to show that the statement is correct or
0
produce a counterexample to show that it is false.
Ž .
n


4.8. Show that D a sÝ
y ya has no derivatives with respect to a at
is1
i
y , y , . . . , y .
1
2
n
Ž .
Ž .
Ž .
4.9. Suppose that the function
f x
is such that
f x
and
f x
are
Ž .
continuous in a neighborhood of the origin and satisfies f 0 s0. Show
that
d
f x
1
Ž .
lim
s
f 0 .
Ž .
dx
x
2
x™0
Ž .
Ž .
4.10. Show that if
f x
exists and is bounded for all
x, then
f x
is
uniformly continuous on R, the set of real numbers.

Ž .
4.11. Suppose that g: R™R and that g x
M for all xgR, where M is
Ž .
Ž .
a positive constant. Define f x sxqcg x , where c is a positive
constant. Show that it is possible to choose c small enough so that f is
a one-to-one function.
Ž .
w
.
Ž .
Ž
.
Ž .
4.12. Suppose that f x is continuous on 0,  , f x exists on 0,  , f 0 s0,
Ž .
Ž
.
Ž .
and f x
is monotone increasing on 0,  . Show that g x
is mono-
Ž
.
Ž .
Ž .
tone increasing on 0,  where g x sf x rx.
4.13. Show that if a1 and m0, then
( )
Ž
x
m.
a
lim
a rx
s,
x™
( )
wŽ
.
mx
b
lim
log x rx
s0.
x™
4.14. Apply l’Hospital’s rule to find the limit
x
1
lim
1q
.
ž
/
x
x™

EXERCISES
129
( )
Ž
. x
q
4.15.
a
Find lim
sin x
.
x™0
( )
Ž
y1r x
.
q
b
Find lim
e
rx .
x™0
4.16. Show that
1rx
a
lim 1qaxqo x
se ,
Ž .
x™0
Ž .
where a is a constant and o x
is any function whose order of
magnitude is less than that of x as x™0.
Ž .
3
2
Ž .
4
4.17. Consider the functions f x s4 x q6 x y10 xq2 and g x s3x q
4 x 3y5x 2q1. Show that
f x
f 1 yf 0
Ž .
Ž .
Ž .

g x
g 1 yg 0
Ž .
Ž .
Ž .
Ž
.
for any xg 0, 1 . Does this contradict Cauchy’s mean value theorem?
Ž .
Ž .
Ž .
4.18. Suppose that f x is differentiable for aFxFb. If f a f b and 
Ž .
Ž .
is a number such that f a f b , show that there exists a ,
Ž
.
w
Ž .
Ž .x
ab, for which f  s
a similar result holds if f a f b .
w
Ž .
Ž .
Ž
.
Ž .
Hint: Consider the function g x sf x y xya . Show that g x
x
has a minimum at .
Ž .
Ž
.
4.19. Suppose that f x
is differentiable on
a, b . Let x , x , . . . , x
be in
1
2
n
Ž
.
n
a,b , and let  ,  , . . . , 
be positive numbers such that Ý
 s1.
1
2
n
is1
i
Ž
.
Show that there exists a point c in a, b such that
n
 f x
sf c .
Ž
.
Ž .
Ý
i
i
is1
w
x
Note: This is a generalization of the result in Exercise 4.18.
Ž
.
Ž
4.20. Let x , x , . . . , x
and y , y , . . . , y
be in
a, b such that x y
is
1
2
n
1
2
n
i
i
.
Ž .
Ž
.
1, 2, . . . , n . Show that if f x
is differentiable on
a, b , then there
Ž
.
exists a point c in a, b such that
n
n
f y
yf x
sf c
y yx
.
Ž
.
Ž
.
Ž .
Ž
.
Ý
Ý
i
i
i
i
is1
is1
Ž .
Ž
.
4.21. Give a Maclaurin’s series expansion of the function f x slog 1qx .
Ž .
Ž
4
. Ž
2
.
4.22. Discuss the maxima and minima of the function f x s x q3 r x q2 .

DIFFERENTIATION
130
Ž .
3 x
Ž
.
4.23. Determine if f x se rx has an absolute minimum on 0,  .
4.24. For what values of a and b is the function
1
f x s
Ž .
2
x qaxqb
w
x
bounded on the interval y1, 1 ? Find the absolute maximum on that
interval.
In Statistics
4.25. Let Y be a continuous random variable whose cumulative distribution
Ž .
Ž .
function, F y , is strictly monotone. Let G y
be another strictly
monotone, continuous cumulative distribution function. Show that the
y1w
Ž
.x
cumulative distribution function of the random variable G
F Y
is
Ž .
G y .
4.26. Let Y have the cumulative distribution function
1yeyy,
yG0,
F y s
Ž . ½ 0,
y0.
'
Find the density function of Ws Y .
4.27. Let Y be normally distributed with mean 1 and variance 0.04. Let
WsY 3.
( )
a
Find the density function of W.
( )
b
Find the exact mean and variance of W.
( )
c
Find approximate values for the mean and variance of W using
Ž .
Taylor’s expansion, and compare the results with those of b .
4.28. Let Z be normally distributed with mean 0 and variance 1. Let YsZ 2.
Find the density function of Y.
w
Ž .
2
x
Note: The function  z sz
is not strictly monotone for all z.
4.29. Let X be a random variable that denotes the age at failure of a
component. The failure rate is defined as the probability of failure in a
finite interval of time, say of length h, given the age of the component,
say x. This failure rate is therefore equal to

P xFXFxqh XGx .
Ž
.
Consider the following limit:
1

lim
P xFXFxqh XGx .
Ž
.
h
h™0

EXERCISES
131
If this limit exists, then it is called the hazard rate, or instantaneous
failure rate.
( )
Ž .
a
Give an expression for the failure rate in terms of F x , the
cumulative distribution function of X.
( )
b
Suppose that X has the exponential distribution with the cumula-
tive distribution function
F x s1yeyx r	,
xG0,
Ž .
where 	 is a positive constant. Show that X has a constant hazard
rate.
( )
c
Show that any random variable with a constant hazard rate must
have the exponential distribution.
Ž
.
4.30. Consider a Poisson process with parameter  over the interval 0, t .
Divide this interval into n equal subintervals of length hstrn. We
consider that we have a ‘‘success’’ in a given subinterval if one arrival
occurs in that subinterval. If there are no arrivals, then we consider that
we have a ‘‘failure.’’ Let Y denote the number of ‘‘successes’’ in the n
n
subintervals of length h. Then we have approximately
nyr
n
r
P Y sr s
p
1yp
,
rs0, 1, . . . , n,
Ž
.
Ž
.
n
n
n
ž /
r
where p is approximately equal to hstrn. Show that
n
r
y t
e
t
Ž
.
lim P Y sr s
.
Ž
.
n
r!
n™

C H A P T E R
5
Infinite Sequences and Series
The study of the theory of infinite sequences and series is an integral part of
advanced calculus. All limiting processes, such as differentiation and integra-
tion, can be investigated on the basis of this theory.
The first example of an infinite series is attributed to Archimedes, who
showed that the sum
1
1
1q
q  q
n
4
4
4
was less than
for any value of n. However, it was not until the nineteenth
3
century that the theory of infinite series was firmly established by Augustin-
Ž
.
Louis Cauchy 17891857 .
In this chapter we shall study the theory of infinite sequences and series,
and investigate their convergence. Unless otherwise stated, the terms of all
sequences and series considered in this chapter are real-valued.
5.1. INFINITE SEQUENCES
In Chapter 1 we introduced the general concept of a function. An infinite
sequence is a particular function f: Jq™R defined on the set of all positive
q
Ž .
integers. For a given ngJ , the value of this function, namely f n , is called
the nth term of the infinite sequence and is denoted by a . The sequence
n

4
itself is denoted by the symbol a
. In some cases, the integer with which
n ns1
the infinite sequence begins is different from one. For example, it may be
equal to zero or to some other integer. For the sake of simplicity, an infinite
sequence will be referred to as just a sequence.

4
Since a sequence is a function, then, in particular, the sequence
an ns1
can have the following properties:


1. It is bounded if there exists a constant K0 such that
a
FK for
n
all n.
132

INFINITE SEQUENCES
133
2. It is monotone increasing if a Fa
for all n, and is monotone
n
nq1
decreasing if a Ga
for all n.
n
nq1
3. It converges to a finite number c if lim
a sc, that is, for a given
n™
n
0 there exists an integer N such that


a yc 
if nN.
n
In this case, c is called the limit of the sequence and this fact is
denoted by writing a ™c as n™. If the sequence does not converge
n
to a finite limit, then it is said to be divergent.
4. It is said to oscillate if it does not converge to a finite limit, nor to q
or y as n™.
1
2
2
Ž
. Ž
.
EXAMPLE 5.1.1. Let a s n q2n r 2n q3 . Then a ™
as n™,
n
n
2
since
1q2rn
lim a s lim
n
2
2q3rn
n™
n™
1
s
.
2
'
'
EXAMPLE 5.1.2. Consider a s nq1 y n . This sequence converges to
n
zero, since
'
'
'
'
nq1 y n
nq1 q n
Ž
.Ž
.
a s
n
'
'
nq1 q n
1
s
.
'
'
nq1 q n
Hence, a ™0 as n™.
n
EXAMPLE 5.1.3. Suppose that a s2 nrn3. Here, the sequence is diver-
n
gent, since by Example 4.2.3,
2 n
lim
s.
3
n
n™
Ž
.n
EXAMPLE 5.1.4. Let a s y1 . This sequence oscillates, since it is equal
n
to 1 when n is even and to y1 when n is odd.
Theorem 5.1.1.
Every convergent sequence is bounded.

4
Proof. Suppose that a
converges to c. Then, there exists an integer
n ns1
N such that


a yc 1
if nN.
n

INFINITE SEQUENCES AND SERIES
134
For such values of n, we have



 
a
max
cy1 , cq1 .
Ž
.
n
It follows that


a
K
n
for all n, where







 

Ksmax
a
q1, a
q1, . . . , a
q1, cy1 , cq1
.

Ž
.
1
2
N
The converse of Theorem 5.1.1 is not necessarily true. That is, if a
sequence is bounded, then it does not have to be convergent. As a counterex-
ample, consider the sequence given in Example 5.1.4. This sequence is
bounded, but is not convergent. To guarantee converge of a bounded
sequence we obviously need an additional condition.
Ž
.
Theorem 5.1.2.
Every bounded monotone increasing or decreasing se-
quence converges.

4
Proof. Suppose that
a
is a bounded and monotone increasing se-
n ns1
Ž
.
quence the proof is similar if the sequence is monotone decreasing . Since
the sequence is bounded, it must be bounded from above and hence has a
Ž
.
least upper bound c see Theorem 1.5.1 . Thus a Fc for all n. Furthermore,
n
for any given 0 there exists an integer N such that
cya Fc;
N

4
otherwise cy would be an upper bound of
a
. Now, because the
n ns1
sequence is monotone increasing,
cya Fa
Fa
F  Fc,
N
Nq1
Nq2
that is,
cya Fc
for nGN.
n
We can write
cya cq ,
n
or equivalently,


a yc 
if nGN.
n

4
This indicates that a
converges to c.

n ns1

INFINITE SEQUENCES
135
Using Theorem 5.1.2 it is easy to prove the following corollary.
Corollary 5.1.1.

4
1. If
a
is bounded from above and is monotone increasing, then
n ns1

4
a
converges to cssup
a .
n ns1
nG1
n

4
2. If
a
is bounded from below and is monotone decreasing, then
n ns1

4
a
converges to dsinf
a .
n ns1
nG1
n

'

4
EXAMPLE 5.1.5.
Consider the sequence a
, where a s 2 and a
n ns1
1
nq1
s
2q
a
for nG1. This sequence is bounded, since a 2 for all n, as
'
'
n
n
'
can be easily shown using mathematical induction: We have a s 2 2. If
1
'
'
a 2, then a

2q 2 2. Furthermore, the sequence is monotone
n
nq1
increasing, since a Fa
for ns1, 2, . . . , which can also be shown by
n
nq1

4
mathematical induction. Hence, by Theorem 5.1.2 a
must converge. To
n ns1
find its limit, we note that
lim a
s lim
2q
a'
'
nq1
n
n™
n™
s
2q
lim a
.
n
'
(
n™
If c denotes the limit of a as n™, then
n
'
'
cs
2q c .
'
By solving this equation under the condition cG 2 we find that the only
solution is cs1.831.

4
Definition 5.1.1.
Consider the sequence a
. An infinite collection of
n ns1
its terms, picked out in a manner that preserves the original order of the

4
terms of the sequence, is called a subsequence of a
. More formally, any
n ns1

4
sequence of the form b
, where b sa
such that k k   k 
n ns1
n
k
1
2
n
n

4
 is a subsequence of a
. Note that k Gn for nG1.

n ns1
n

4
Theorem 5.1.3.
A sequence
a
converges to c if and only if every
n ns1

4
subsequence of a
converges to c.
n ns1
Proof. The proof is left to the reader.

It should be noted that if a sequence diverges, then it does not necessarily
follow that every one of its subsequences must diverge. A sequence may fail
to converge, yet several of its subsequences converge. For example, the

INFINITE SEQUENCES AND SERIES
136
Ž
.n
sequence whose nth term is a s y1
is divergent, as was seen earlier.
n

4

4
However, the two subsequences
b
and c
, where b sa
s1 and
n ns1
n ns1
n
2 n
Ž
.
c sa
sy1 ns1, 2, . . . , are both convergent.
n
2 ny1
We have noted earlier that a bounded sequence may not converge. It is
possible, however, that one of its subsequences is convergent. This is shown
in the next theorem.
Theorem 5.1.4.
Every bounded sequence has a convergent subsequence.

4
Proof. Suppose that
a
is a bounded sequence. Without loss of
n ns1
generality we can consider that the number of distinct terms of the sequence
Ž
is infinite. If this is not the case, then there exists an infinite subsequence of

4
a
that consists of terms that are equal. Obviously, such a subsequence
n ns1
.
converges.
Let G denote the set consisting of all terms of the sequence.
Then G is a bounded infinite set. By Theorem 1.6.2, G must have a limit
point, say c. Also, by Theorem 1.6.1, every neighborhood of c must contain
infinitely many points of G. It follows that we can find integers k k k
1
2
3
  such that
1
a
yc 
for ns1, 2, . . . .
k n
n


Thus for a given 0 there exists an integer N1r such that a
yc 
k n

4
if nN. This indicates that the subsequence
a
converges to c.
k
ns1
n

We conclude from Theorem 5.1.4 that a bounded sequence can have
several convergent subsequences. The limit of each of these subsequences is
called a subsequential limit. Let E denote the set of all subsequential limits

4
Ž
.
of a
. This set is bounded, since the sequence is bounded why? .
n ns1

4
Definition 5.1.2.
Let
a
be a bounded sequence, and let E be the
n ns1
set of all its subsequential limits. Then the least upper bound of E is called

4
the upper limit of
a
and is denoted by lim sup
a . Similarly, the
n ns1
n™
n

4
greatest lower bound of E is called the lower limit of a
and is denoted
n ns1

4
Ž
.nw
by lim inf
a . For example, the sequence
a
, where a s y1
1q
n™
n
n ns1
n
Ž
.x

4
1rn , has two subsequential limits, namely y1 and 1. Thus Es y1, 1 ,
and lim sup
a s1, lim inf
a sy1.

n™
n
n™
n

4
Theorem 5.1.5.
The sequence a
converges to c if any only if
n ns1
lim inf a s lim sup a sc.
n
n
n™
n™
Proof. The proof is left to the reader.


INFINITE SEQUENCES
137
Theorem 5.1.5 implies that when a sequence converges, the set of all its
subsequential limits consists of a single element, namely the limit of the
sequence.
5.1.1. The Cauchy Criterion

4
We have seen earlier that the definition of convergence of a sequence an ns1
requires finding the limit of a as n™. In some cases, such a limit may be
n
difficult to figure out. For example, consider the sequence whose nth term is
ny1
1
1
1
y1
Ž
.
a s1y
q
y
q  q
,
ns1, 2, . . . .
5.1
Ž
.
n
3
5
7
2ny1
It is not easy to calculate the limit of a
in order to find out if the sequence
n
converges. Fortunately, however, there is another convergence criterion for
Ž
sequences, known as the Cauchy criterion after Augustin-Louis Cauchy it was
known earlier to Bernhard Bolzano, 17811848, a Czechoslovakian priest
whose mathematical work was undeservedly overlooked by his lay and cleri-
.
cal contemporaries; see Boyer, 1968, page 566 .
Ž
.

4
Theorem 5.1.6 The Cauchy Criterion .
The sequence
a
converges
n ns1
if and only if it satisfies the following condition, known as the -condition:
For each 0 there is an integer N such that


a ya

for all mN, nN.
m
n
Proof. Necessity: If the sequence converges, then it must satisfy the -con-

4
dition. Let 0 be given. Since the sequence a
converges, then there
n ns1
exists a number c and an integer N such that



a yc 
if nN.
n
2
Hence, for mN, nN we must have




a ya
s a ycqcya
m
n
m
n




F a yc q a yc .
m
n
Sufficiency: If the sequence satisfies the -condition, then it must converge.
If the -condition is satisfied, then there is an integer N such that for any
given 0,


a ya

n
Nq1
for all values of nGNq1. Thus for such values of n,
a
ya a
q.
5.2
Ž
.
Nq1
n
Nq1

INFINITE SEQUENCES AND SERIES
138

4
The sequence a
is therefore bounded, since from the double inequality
n ns1
Ž
.
5.2 we can assert that









 

a
max
a
q1, a
q1, . . . , a
q1, a
y , a
q
Ž
.
n
1
2
N
Nq1
Nq1

4

4
for all n. By Theorem 5.1.4, a
has a convergent subsequence
a
.
n ns1
k
ns1
n
Let c be the limit of this subsequence. If we invoke again the -condition, we
can find an integer N such that


a ya

if mN, k GnGN,
m
k
n
n
where . By fixing m and letting k ™ we get
n


a yc F
if mN.
m

4
This indicates that the sequence a
is convergent and has c as its limit.
n ns1


4
Definition 5.1.3.
A sequence a
that satisfies the -condition of the
n ns1
Cauchy criterion is said to be a Cauchy sequence.

EXAMPLE 5.1.6.
With the help of the Cauchy criterion it is now possible

4
Ž
.
to show that the sequence a
whose nth term is defined by formula 5.1
n ns1
is a Cauchy sequence and is therefore convergent. To do so, let mn. Then,
py1
1
1
y1
Ž
.
n
a ya s y1
y
q  q
,
5.3
Ž
.
Ž
.
m
n
2nq1
2nq3
2nq2 py1
Ž
.
where psmyn. We claim that the quantity inside brackets in formula 5.3
is positive. This can be shown by grouping successive terms in pairs. Thus if p
is even, the quantity is equal to
1
1
1
1
y
q
y
q 
ž
/ ž
/
2nq1
2nq3
2nq5
2nq7
1
1
q
y
,
ž
/
2nq2 py3
2nq2 py1
which is positive, since the difference inside each parenthesis is positive. If
Ž
.
ps1, the quantity is obviously positive, since it is then equal to 1r 2nq1 .
If pG3 is an odd integer, the quantity can be written as
1
1
1
1
y
q
y
q 
ž
/ ž
/
2nq1
2nq3
2nq5
2nq7
1
1
1
q
y
q
,
ž
/
2nq2 py5
2nq2 py3
2nq2 py1

INFINITE SEQUENCES
139
which is also positive. Hence, for any p,
py1
1
1
y1
Ž
.


a ya
s
y
q  q
.
5.4
Ž
.
m
n
2nq1
2nq3
2nq2 py1
We now claim that
1


a ya

.
m
n
2nq1
To prove this claim, let us again consider two cases. If p is even, then
1
1
1


a ya
s
y
y
y 
m
n
ž
/
2nq1
2nq3
2nq5
1
1
1
y
y
y
ž
/
2nq2 py5
2nq2 py3
2nq2 py1
1

,
5.5
Ž
.
2nq1
Ž
.
since all the quantities inside parentheses in 5.5 are positive. If p is odd,
then
1
1
1


a ya
s
y
y
y 
m
n
ž
/
2nq1
2nq3
2nq5
1
1
1
y
y

,
ž
/
2nq2 py3
2nq2 py1
2nq1
which proves our claim. On the basis of this result we can assert that for a
given 0,


a ya

if mnN,
m
n
where N is such that
1
 ,
2 Nq1
or equivalently,
1
1
N
y
.
2
2

4
This shows that a
is a Cauchy sequence.
n ns1

INFINITE SEQUENCES AND SERIES
140

4
EXAMPLE 5.1.7.
Consider the sequence a
, where
n ns1
1
n
a s y1
1q
.
Ž
.
n
ž
/
n
We have seen earlier that lim inf
a sy1 and lim sup
a s1. Thus
n™
n
n™
n
by Theorem 5.1.5 this sequence is not convergent. We can arrive at the same
conclusion using the Cauchy criterion by showing that the -condition is not
satisfied. This occurs whenever we can find an 0 such that for however N
may be chosen,


a ya
G
m
n
for some mN, nN. In our example, if N is any positive integer, then the
inequality


a ya
G2
5.6
Ž
.
m
n
can be satisfied by choosing ms and nsq1, where  is an odd integer
greater than N.
5.2. INFINITE SERIES

4
Let a
be a given sequence. Consider the symbolic expression
n ns1

a sa qa q  qa q  .
5.7
Ž
.
Ý
n
1
2
n
ns1
By definition, this expression is called an infinite series, or just a series for
simplicity, and a is referred to as the nth term of the series. The finite sum
n
n
s s
a ,
ns1, 2, . . . ,
Ý
n
i
is1
is called the nth partial sum of the series.
Definition 5.2.1.
Consider the series Ý
a . Let s
be its nth partial
ns1
n
n
Ž
.
sum ns1, 2, . . . .

4
1. The series is said to be convergent if the sequence s
converges. In
n ns1
this case, if lim
s ss, where s is finite, then we say that the series
n™
n
converges to s, or that s is the sum of the series. Symbolically, this is

INFINITE SERIES
141
expressed by writing

ss
a .
Ý
n
ns1
2. If s
does not tend to a finite limit, then the series is said to be
n
divergent.

Definition 5.2.1 formulates convergence of a series in terms of conver-
gence of the associated sequence of its partial sums. By applying the Cauchy
Ž
.
criterion Theorem 5.1.6 to the latter sequence, we arrive at the following
condition of convergence for a series:
Theorem 5.2.1.
The series Ý
a , converges if and only if for a given
ns1
n
0 there is an integer N such that
n
a 
for all nmN.
5.8
Ž
.
Ý
i
ismq1
Ž
.
Inequality
5.8
follows from applying Theorem 5.1.6 to the sequence

4
s
of partial sums of the series and noting that
n ns1
n


s ys
s
a
for nm.
Ý
n
m
i
ismq1
Ž
.
In particular, if nsmq1, then inequality 5.8 becomes


a

5.9
Ž
.
mq1
for all mN. This implies that lim
a
s0, and hence lim
a s0.
m™
mq1
n™
n
We therefore conclude the following result:
RESULT 5.2.1.
If Ý
a
is a convergent series, then lim
a s0.
ns1
n
n™
n
It is important here to note that the convergence of the nth term of a
series to zero as n™ is a necessary condition for the convergence of the
series. It is not, however, a sufficient condition, that is, if lim
a s0, then
n™
n
it does not follow that Ý
a converges. For example, as we shall see later,
ns1
n

Ž
.
the series Ý
1rn is divergent, and its nth term goes to zero as n™. It
ns1
is true, however, that if lim
a 0, then Ý
a is divergent. This follows
n™
n
ns1
n
from applying the law of contraposition to the necessary condition of conver-
gence. We conclude the following:
1. If a ™0 as n™, then no conclusion can be reached regarding
n
convergence or divergence of Ý
a .
ns1
n

INFINITE SEQUENCES AND SERIES
142
2. If a 0 as n™, then Ý
a
is divergent. For example, the series
n
ns1
n

w
Ž
.x
Ý
nr nq1
is divergent, since
ns1
n
lim
s10.
nq1
n™
EXAMPLE 5.2.1.
One of the simplest series is the geometric series, Ý
an.
ns1
 
n
This series is divergent if
a G1, since lim
a 0. It is convergent if
n™
 a 1 by the Cauchy criterion: Let nm. Then
s ys samq1 qamq2 q  qan.
5.10
Ž
.
n
m
Ž
.
By multiplying the two sides of 5.10 by a, we get
a s ys
samq2 qamq3 q  qanq1 .
5.11
Ž
.
Ž
.
n
m
Ž
.
Ž
.
If we now subtract 5.11 from 5.10 , we obtain
amq1 yanq1
s ys s
.
5.12
Ž
.
n
m
1ya
 
Since a 1, we can find an integer N such that for mN, nN,
 1ya
Ž
.
mq1
 a

,
2
 1ya
Ž
.
nq1
 a

.
2
Hence, for a given 0,


s ys

if nmN.
n
m
Ž
.
Formula 5.12 can actually be used to find the sum of the geometric series
 
Ž
.
when a 1. Let ms1. By taking the limits of both sides of 5.12 as n™
we get
a2
nq1
lim s ss q
,
since lim a
s0,
n
1
1ya
n™
n™
a2
saq 1ya
a
s
.
1ya

INFINITE SERIES
143

Ž
.
EXAMPLE 5.2.2.
Consider the series Ý
1rn! . This series converges by
ns1
the Cauchy criterion. To show this, we first note that
n!sn ny1
ny2   321
Ž
. Ž
.
G2 ny1
for ns1, 2, . . . .
Hence, for nm,
1
1
1


s ys
s
q
q  q
n
m
mq1 !
mq2 !
n!
Ž
.
Ž
.
1
1
1
F
q
q  q
m
mq1
ny1
2
2
2
n
1
s2
.
Ý
i2
ismq1
1
w
This is a partial sum of a convergent geometric series with as 1 see
2
Ž
.x


formula 5.10 . Consequently, s ys
can be made smaller than any given
n
m
0 by choosing m and n large enough.
Theorem 5.2.2.
If Ý
a and Ý
b are two convergent series, and if c
ns1
n
ns1
n
is a constant, then the following series are also convergent:

Ž
.

1. Ý
ca
scÝ
a .
ns1
n
ns1
n

Ž
.


2. Ý
a qb
sÝ
a qÝ
b .
ns1
n
n
ns1
n
ns1
n
Proof. The proof is left to the reader.





Definition 5.2.2.
The series Ý
a
is absolutely convergent if Ý
a
ns1
n
ns1
n
is convergent.


wŽ
.n
x
For example, the series Ý
y1 rn! is absolutely convergent, since
ns1

Ž
.
Ý
1rn! is convergent, as was seen in Example 5.2.2.
ns1
Theorem 5.2.3.
Every absolutely convergent series is convergent.




Proof. Consider the series Ý
a , and suppose that Ý
a
is conver-
ns1
n
ns1
n
gent. We have that
n
n


a F
a .
5.13
Ž
.
Ý
Ý
i
i
ismq1
ismq1

INFINITE SEQUENCES AND SERIES
144



By applying the Cauchy criterion to Ý
a
we can find an integer N such
ns1
n
that for a given 0,
n


a 
if nmN.
5.14
Ž
.
Ý
i
ismq1
Ž
.
Ž
.

From 5.13 and 5.14 we conclude that Ý
a
satisfies the Cauchy crite-
ns1
n
rion and is therefore convergent by Theorem 5.2.1.





Note that it is possible that Ý
a
is convergent while Ý
a
is
ns1
n
ns1
n
divergent. In this case, the series Ý
a
is said to be conditionally conver-
ns1
n
gent. Examples of this kind of series will be seen later.
In the next section we shall discuss convergence of series whose terms are
positive.
5.2.1. Tests of Convergence for Series of Positive Terms
Suppose that the terms of the series Ý
a are such that a 0 for nK,
ns1
n
n
where K is a constant. Without loss of generality we shall consider that
Ks1. Such a series is called a series of positive terms.
Series of positive terms are interesting because the study of their conver-
gence is comparatively simple and can be used in the determination of
convergence of more general series whose terms are not necessarily positive.
It is easy to see that a series of positive terms diverges if and only if its sum
is q.
In what follows we shall introduce techniques that simplify the process of
determining whether or not a given series of positive terms is convergent. We
refer to these techniques as tests of convergence. The advantage of these
tests is that they are in general easier to apply than the Cauchy criterion.
This is because evaluating or obtaining inequalities involving the expression
Ýn
a in Theorem 5.2.1 can be somewhat difficult. The tests of conver-
ismq1
i
gence, however, have the disadvantage that they can sometime fail to
determine convergence or divergence, as we shall soon find out. It should be
remembered that these tests apply only to series of positive terms.
The Comparison Test
This test is based on the following theorem:
Theorem 5.2.4.
Let Ý
a
and Ý
b
be two series of positive terms
ns1
n
ns1
n
such that a Fb for nN , where N is a fixed integer.
n
n
0
0
i. If Ý
b converges, then so does Ý
a .
ns1
n
ns1
n
ii. If Ý
a
is divergent, then Ý
b is divergent too.
ns1
n
ns1
n

INFINITE SERIES
145
Proof. We have that
n
n
a F
b
for nmN .
5.15
Ž
.
Ý
Ý
i
i
0
ismq1
ismq1
If Ý
b is convergent, then for a given 0 there exists an integer N such
ns1
n
1
that
n
b 
for nmN .
5.16
Ž
.
Ý
i
1
ismq1
Ž
.
Ž
.
Ž
.
From 5.15 and 5.16 it follows that if nmN, where Nsmax N , N ,
0
1
then
n
a  ,
Ý
i
ismq1
Ž .
which proves i .
Ž .
Ž .
The proof of ii follows from applying the law of contraposition to i .

To determine convergence or divergence of Ý
a
we thus need to have
ns1
n
in our repertoire a collection of series of positive terms whose behavior
Ž
.
with regard to convergence or divergence is known. These series can then
be compared against Ý
a . For this purpose, the following series can be
ns1
n
useful:
a. Ý
1rn. This is a divergent series called the harmonic series.
ns1
b. Ý
1rnk. This is divergent if k1 and is convergent if k1.
ns1
To prove that the harmonic series is divergent, let us consider its nth
partial sum, namely,
n
1
s s
.
Ý
n
i
is1
Let A0 be an arbitrary positive number. Choose n large enough so that
n2 m, where m2 A. Then for such values of n,
1
1
1
1
1
1
1
s  1q
q
q
q
q
q
q
q 
n ž
/ ž
/ ž
/
2
3
4
5
6
7
8
1
1
q
q  q
m
my1
ž
/
2
2
q1
1
2
4
2 my1
m

q
q
q  q
s
A.
5.17
Ž
.
m
2
4
8
2
2

INFINITE SEQUENCES AND SERIES
146
Since A is arbitrary and s is a monotone increasing function of n, inequality
n
Ž
.
5.17 implies that s ™ as n™. This proves divergence of the harmonic
n
series.
Ž .
k
Let us next consider the series in
b . If k1, then 1rn 1rn and

Ž
k.
Ž .
Ý
1rn
must be divergent by Theorem 5.2.4 ii . Suppose now that k1.
ns1
Consider the nth partial sum of the series, namely,
n
1

s s
.
Ý
n
ki
is1
Then, by choosing m large enough so that 2 mn we get
2 my1 1

s F Ý
n
ki
is1
1
1
1
1
1
1
s1q
q
q
q
q
q
q 
k
k
k
k
k
k
ž
/ ž
/
2
3
4
5
6
7
1
1
q
q  q
k
k
m
my1
2 y1
Ž
.
2
Ž
.
1
1
1
1
1
1
F1q
q
q
q
q
q
q 
k
k
k
k
k
k
ž
/ ž
/
2
2
4
4
4
4
1
1
q
q  q
k
k
my1
my1
2
2
Ž
.
Ž
.
2
4
2 my1
s1q
q
q  q
k
k
k
my1
2
4
2
Ž
.
m
iy1
s
a
,
5.18
Ž
.
Ý
is1
ky1
Ž
.
where as1r2
. But the right-hand side of
5.18
represents the mth
Ž
.
partial sum of a convergent geometric series since a1 . Hence, as m™,
Ž
.
the right-hand side of 5.18 converges to

1
iy1
a
s
see Example 5.2.1 .
Ž
.
Ý
1ya
is1
 
 4
Thus the sequence s
is bounded. Since it is also monotone increasing, it
n ns1
Ž
.
must be convergent
see Theorem 5.1.2 . This proves convergence of the

Ž
k.
series Ý
1rn
for k1.
ns1

INFINITE SERIES
147
Another version of the comparison test in Theorem 5.2.4 that is easier to
implement is given by the following theorem:
Theorem 5.2.5.
Let Ý
a and Ý
b be two series of positive terms. If
ns1
n
ns1
n
there exists a positive constant l such that a
and lb
are asymptotically
n
n
Ž
.
equal, a lb as n™ see Section 3.3 , that is,
n
n
an
lim
sl,
b
n™
n
then the two series are either both convergent or both divergent.
Proof. There exists an integer N such that
a
l
n yl 
if nN,
b
2
n
or equivalently,
l
a
3l
n


whenever nN.
2
b
2
n
If Ý
a
is convergent, then Ý
b
is convergent by a combination of
ns1
n
ns1
n
Ž .
Ž .
Ž
.

Theorem 5.2.2 1 and 5.2.4 i , since b  2rl a . Similarly, if Ý
b
con-
n
n
ns1
n

Ž
.

verges, then so does Ý
a , since a  3lr2 b . If Ý
a
is divergent,
ns1
n
n
n
ns1
n

Ž .
then Ý
b
is divergent too by a combination of Theorems 5.2.2 1
and
ns1
n
Ž .
Ž
.

5.2.4 ii , since b  2r3l a . Finally, Ý
a
diverges if the same is true of
n
n
ns1
n

Ž
.
Ý
b , since a  lr2 b .

ns1
n
n
n

Ž
. Ž
3
.
EXAMPLE 5.2.3. The series Ý
nq2 r n q2nq1 is convergent, since
ns1
nq2
1

as n™,
3
2
n q2nq1
n
w

Ž
k.
which is the nth term of a convergent series
recall that Ý
1rn
is
ns1
x
convergent if k1 .

'
EXAMPLE 5.2.4. Ý
1r
n nq1
is divergent, because
Ž
.
ns1
1
1

as n™,
n
'n nq1
Ž
.
which is the nth term of the divergent harmonic series.

INFINITE SEQUENCES AND SERIES
148
The Ratio or d’Alembert’sTest
This test is usually attributed to the French mathematician Jean Baptiste
Ž
.
d’Alembert
17171783 , but is also known as Cauchy’s ratio test after
Ž
.
Augustin-Louis Cauchy 17891857 .
Theorem 5.2.6.
Let Ý
a
be a series of positive terms. Then the
ns1
n
following hold:
Ž
.
Ž
.
1. The series converges if lim sup
a
ra
1 see Definition 5.1.2 .
n™
nq1
n
Ž
.
Ž
.
2. The series diverges if lim inf
a
ra
1 see Definition 5.1.2 .
n™
nq1
n
Ž
.
Ž
.
3. If lim inf
a
ra
F1Flim sup
a
ra , no conclusion can
n™
nq1
n
n™
nq1
n
Ž
be made regarding convergence or divergence of the series that is, the
.
ratio test fails .
Ž
.
In particular, if lim
a
ra
sr exists, then the following hold:
n™
nq1
n
1. The series converges if r1.
2. The series diverges if r1.
3. The test fails if rs1.
Ž
.
Ž
.
Proof. Let pslim inf
a
ra , qslim sup
a
ra .
n™
nq1
n
n™
nq1
n
Ž
.
1. If q1, then by the definition of the upper limit Definition 5.1.2 ,
there exists an integer N such that
anq1 q
for nGN,
5.19
Ž
.
an
Ž
where q is chosen such that qq1. If a
ra Gq for infinitely
nq1
n

4
many values of n, then the sequence a
ra
has a subsequential
nq1
n ns1
limit greater than or equal to q, which exceeds q. This contradicts the
.
Ž
.
definition of q. From 5.19 we then get
a
a q
Nq1
N
a
a
qa q2,
Nq2
Nq1
N
...
a
a
qa qm,
Nqm
Nqmy1
N
where mG1. Thus for nN,
yN
ŽnyN .
n
a a q
sa
q
q .
Ž
.
n
N
N

INFINITE SERIES
149
Hence, the series converges by comparison with the convergent geomet-
ric series Ý
qn, since q1.
ns1
2. If p1, then in an analogous manner we can find an integer N such
that
anq1 p
for nGN,
5.20
Ž
.
an
where p is chosen such that pp1. But this implies that a cannot
n
tend to zero as n™, and the series is therefore divergent by Result
5.2.1.
3. If pF1Fq, then we can demonstrate by using an example that the

Ž
.
ratio
test
is
inconclusive:
Consider
the
two
series
Ý
1rn ,
ns1

Ž
2.
Ý
1rn . For both series, psqs1 and hence pF1Fq, since
ns1
Ž
.
lim
a
ra
s1. But the first series is divergent while the second
n™
nq1
n
is convergent, as was seen earlier.

EXAMPLE 5.2.5.
Consider the same series as in Example 5.2.2. This series
was shown to be convergent by the Cauchy criterion. Let us now apply the
ratio test. In this case,
a
1
1
nq1
lim
s lim
a
nq1 !
n!
n™
n™ Ž
.
n
1
s lim
s01,
nq1
n™
Ž .
which indicates convergence by Theorem 5.2.6 1 .
Ž
.
Nurcombe 1979 stated and proved the following extension of the ratio
test:
Theorem 5.2.7.
Let Ý
a be a series of positive terms, and k be a fixed
ns1
n
positive integer.
Ž
.
1. If lim
a
ra
1, then the series converges.
n™
nqk
n
Ž
.
2. If lim
a
ra
1, then the series diverges.
n™
nqk
n
This test reduces to the ratio test when ks1.
The Root or Cauchy’s Test
This is a more powerful test than the ratio test. It is based on the following
theorem:

INFINITE SEQUENCES AND SERIES
150
Theorem
5.2.8.
Let
Ý
a
be
a
series
of
positive
terms.
Let
ns1
n
lim sup
a1r ns. Then we have the following:
n™
n
1. The series converges if 1.
2. The series diverges if 1.
3. The test is inconclusive if s1.
In particular, if lim
a1r ns exists, then we have the following:
n™
n
1. The series converges if 1.
2. The series diverges if 1.
3. The test is inconclusive if s1.
Proof.
Ž .
1. As in Theorem 5.2.6 1 , if 1, then there is an integer N such that
a1r n
for nGN,
n
where  is chosen such that 1. Thus
a n
for nGN.
n
The series is therefore convergent by comparison with the convergent
geometric series Ý
n, since 1.
ns1
2. Suppose that 1. Let 0 be such that y1. Then
a1r ny1
n
Ž
.
for infinitely many values of n why? . Thus for such values of n,
n
a 
y
,
Ž
.
n
which implies that a
cannot tend to zero as n™ and the series is
n
therefore divergent by Result 5.2.1.

Ž
.

Ž
2.
3. Consider again the two series Ý
1rn , Ý
1rn . In both cases
ns1
ns1
Ž
.
s1 see Exercise 5.18 . The test therefore fails, since the first series is
divergent and the second is convergent.

NOTE 5.2.1.
We have mentioned earlier that the root test is more
powerful than the ratio test. By this we mean that whenever the ratio test
shows convergence or divergence, then so does the root test; whenever the
root test is inconclusive, the ratio test is inconclusive too. However, there are
Ž
situations where the ratio test fails, but the root test doe not see Example
.
5.2.6 . This fact is based on the following theorem:

INFINITE SERIES
151
Theorem 5.2.9.
If a 0, then
n
a
a
nq1
nq1
1r n
1r n
lim inf
F lim inf a
F lim sup a
F lim sup
.
n
n
a
a
n™
n™
n™
n™
n
n
Proof. It is sufficient to prove the two inequalities
anq1
1r n
lim sup a
F lim sup
,
5.21
Ž
.
n
a
n™
n™
n
anq1
1r n
lim inf
F lim inf a
.
5.22
Ž
.
n
a
n™
n™
n
Ž
.
Ž
.
Inequality 5.21 : Let qslim sup
a
ra . If qs, then there is noth-
n™
nq1
n
ing to prove. Let us therefore consider that q is finite. If we choose q such
Ž .
that qq, then as in the proof of Theorem 5.2.6 1 , we can find an integer
N such that
yN
n
a a
q
q
for nN.
Ž
.
n
N
Hence,
1rn
yN
1r n
a
 a
q
q.
5.23
Ž
.
Ž
.
n
N
Ž
.
As n™, the limit of the right-hand side of inequality 5.23 is q. It follows
that
lim sup a1r nFq.
5.24
Ž
.
n
n™
Ž
.
Since 5.24 is true for any qq, then we must also have
lim sup a1r nFq.
n
n™
Ž
.
Ž
.
Inequality
5.22 : Let pslim inf
a
ra . We can consider p to be
n™
nq1
n
Ž
finite if ps, then qs and the proof of the theorem will be complete; if
.
psy, then there is nothing to prove . Let p be chosen such that pp.
Ž .
As in the proof of Theorem 5.2.6 2 , we can find an integer N such that
anq1 p
for nGN.
5.25
Ž
.
an
Ž
.
From 5.25 it is easy to show that
yN
n
a a
p
p
for nGN.
Ž
.
n
N

INFINITE SEQUENCES AND SERIES
152
Hence, for such values of n,
1rn
yN
1r n
a
 a
p
p.
Ž
.
n
N
Consequently,
lim inf a1r nGp.
5.26
Ž
.
n
n™
Ž
.
Since 5.26 is true for any pp, then
lim inf a1r nGp.
n
n™
From Theorem 5.2.9 we can easily see that whenever
q1, then
lim sup
a1r n1; whenever p1, then lim sup
a1r n1. In both cases,
n™
n
n™
n
if convergence or divergence of the series is resolved by the ratio test, then it
Ž
can also be resolved by the root test. If, however, the root test fails when
1r n
.
Ž .
lim sup
a
s1 , then the ratio test fails too by Theorem 5.2.6 3 . On the
n™
n
other hand, it is possible for the ratio test to be inconclusive whereas the root
test is not. This occurs when
a
a
nq1
nq1
1r n
1r n
lim inf
F lim inf a
F lim sup a
1F lim sup
.

n
n
a
a
n™
n™
n™
n™
n
n

Ž
n
n.
EXAMPLE 5.2.6.
Consider the series Ý
a qb
, where 0ab1.
ns1
This can be written as Ý
c , where for nG1,
ns1
n
aŽnq1. r2
if n is odd,
c s
n ½
nr2
b
if n is even.
Now,
Ž
.
nq1 r2
c
bra
if n is odd,
Ž
.
nq1 s
nr2
½
c
a arb
if n is even,
n
Ž
.
aŽnq1. rŽ2 n.
if n is odd,
1r n
c
s
n
½
1r2
b
if n is even.

INFINITE SERIES
153
As n™, c
rc
has two limits, namely 0 and ; c1r n has two limits, a1r2
nq1
n
n
and b1r2. Thus
cnq1
lim inf
s0,
c
n™
n
cnq1
lim sup
s,
c
n™
n
lim sup c1r nsb1r2 1.
n
n™
Since 0F1F, we can clearly see that the ratio test is inconclusive, whereas
the root test indicates that the series is convergent.
(
)
Maclaurin’s or Cauchy’s Integeral Test
Ž
.
This test was introduced by Colin Maclaurin 16981746 and then rediscov-
ered by Cauchy. The description and proof of this test will be given in
Chapter 6.
Cauchy’s Condensation Test
Let us consider the following theorem:
Theorem 5.2.10.
Let Ý
a be a series of positive terms, where a
is a
ns1
n
n
Ž
.

monotone decreasing function of n s1, 2, . . . . Then Ý
a
converges or
ns1
n
diverges if and only if the same is true of the series Ý
2 na
n.
ns1
2
Proof. Let s
and t
be the nth and mth partial sums, respectively, of
n
m
Ý
a and Ý
2 na
n. If m is such that n2 m, then
ns1
n
ns1
2
s Fa q a qa
q a qa qa qa
Ž
.
Ž
.
n
1
2
3
4
5
6
7
q  q a
mqa
m
q  qa
m
m
Ž
.
2
2
q1
2
q2
y1
Fa q2a q4a q  q2 ma
mst .
5.27
Ž
.
1
2
4
2
m
Furthermore, if n2 m, then
s Ga qa q a qa
q  q a
my1
q  qa
m
Ž
.
Ž
.
n
1
2
3
4
2
q1
2
a
t
1
m
my1
m
G
qa q2a q  q2
a
s
.
5.28
Ž
.
2
4
2
2
2

n
Ž
.
n
If Ý
2 a
diverges, then t ™ as m™. Hence, from 5.28 , s ™ as
ns1
2
m
n
n™, and the series Ý
a
is also divergent.
ns1
n

INFINITE SEQUENCES AND SERIES
154

n

4
n
Now, if Ý
2 a
converges, then the sequence t
is bounded. From
ns1
2
m ms1
Ž
.

4

5.27 , the sequence
s
is also bounded. It follows that Ý
a
is a
n ns1
ns1
n
Ž
.
convergent series see Exercise 5.13 .


Ž
k.
EXAMPLE 5.2.7.
Consider again the series Ý
1rn
. We have already
ns1
seen that this series converges if k1 and diverges if kF1. Let us now
apply Cauchy’s condensation test. In this case,



1
1
n
n
n
2 a
s
2
s
Ý
Ý
Ý
2
nk
nŽky1.
2
2
ns1
ns1
ns1
is a geometric series Ý
bn, where bs1r2 ky1. If kF1, then bG1 and the
ns1
series diverges. If k1, then b1 and the series converges. It is interesting
to note that in this example, both the ratio and the root tests fail.
The following tests enable us to handle situations where the ratio test fails.
These tests are particular cases on a general test called Kummer’s test.
Kummer’s Test
This test is named after the German mathematician Ernst Eduard Kummer
Ž
.
18101893 .
Theorem 5.2.11.
Let Ý
a and Ý
b be two series of positive terms.
ns1
n
ns1
n
Suppose that the series Ý
b is divergent. Let
ns1
n
1
a
1
n
lim
y
s,
ž
/
b
a
b
n™
n
nq1
nq1
Then Ý
a converges if 0 and diverges if 0.
ns1
n
Proof. Suppose that 0. We can find an integer N such that for nN,
1
a
1

n
y

.
5.29
Ž
.
b
a
b
2
n
nq1
nq1
Ž
.
Inequality 5.29 can also be written as
2
a
a
n
nq1
a

y
.
5.30
Ž
.
nq1
ž
/

b
b
n
nq1

INFINITE SERIES
155

Ž
.
If s
is the nth partial sum of Ý
a , then from 5.30 and for nN,
n
ns1
n
nq1
2
a
a
iy1
i
s
s
q
y
,
Ý
nq1
Nq1
ž
/

b
b
iy1
i
isNq2
that is,
2
a
a
Nq1
nq1
s
s
q
y
,
nq1
Nq1
ž
/

b
b
Nq1
nq1
5.31
Ž
.
2 aNq1
s
s
q
for nN.
nq1
Nq1
 bNq1
Ž
.

4
Inequality 5.31 indicates that the sequence
s
is bounded. Hence, the
n ns1

Ž
.
series Ý
a
is convergent see Exercise 5.13 .
ns1
n
Now, let us suppose that 0. We can find an integer N such that
1
a
1
n
y
0
for nN.
b
a
b
n
nq1
nq1
Thus for such values of n,
an
a

b
.
5.32
Ž
.
nq1
nq1
bn
Ž
.
It is easy to verify that because of 5.32 ,
aNq1
a 
b
5.33
Ž
.
n
n
bNq1

Ž
.
for nGNq2. Since Ý
b is divergent, then from 5.33 and the use of the
ns1
n
comparison test we conclude that Ý
a
is divergent too.

ns1
n
Two particular cases of Kummer’s test are Raabe’s test and Gauss’s test.
Raabe’s Test
This test was established in 1832 by J. L. Raabe.
Theorem 5.2.12.
Suppose that Ý
a
is a series of positive terms and
ns1
n
that
a

1
n
s1q
qo
as n™.
ž /
a
n
n
nq1
Then Ý
a converges if 1 and diverges if 1.
ns1
n

INFINITE SEQUENCES AND SERIES
156
Proof. We have that
a

1
n
s1q
qo
.
ž /
a
n
n
nq1
This means that
a

n
n
y1y
™0
5.34
Ž
.
ž
/
a
n
nq1
Ž
.
as n™. Equivalently, 5.34 can be expressed as
nan
lim
yny1 sy1.
5.35
Ž
.
ž
/
a
n™
nq1
Ž
.
Let b s1rn in 5.35 . This is the nth term of a divergent series. If we now
n
apply Kummer’s test, we conclude that the series Ý
a
converges if
ns1
n
y10 and diverges if y10.

Gauss’s Test
Ž
.
This test is named after Carl Friedrich Gauss
17771855 . It provides a
slight improvement over Raabe’s test in that it usually enables us to handle
the case s1. For such a value of , Raabe’s test is inconclusive.
Theorem 5.2.13.
Let Ý
a
be a series of positive terms. Suppose that
ns1
n
a

1
n
s1q
qO
,
0.
q1
ž
/
a
n
n
nq1
Then Ý
a converges if 1 and diverges if F1.
ns1
n
Proof. Since
1
1
O
so
,
q1
ž /
ž
/
n
n
then by Raabe’s test, Ý
a converges if 1 and diverges if 1. Let us
ns1
n
therefore consider s1. We have
a
1
1
n
s1q
qO
.
q1
ž
/
a
n
n
nq1

INFINITE SERIES
157
Ž
.
Put b s1r n log n , and consider
n
1
a
1
n
lim
y
ž
/
b
a
b
n™
n
nq1
nq1
1
1
s lim
n log n 1q
qO
y nq1 log nq1
Ž
.
Ž
.
q1
½
5
ž
/
n
n
n™
n
1
s lim
nq1 log
q n log n O
sy1.
Ž
.
Ž
.
q1
ž
/
nq1
n
n™
This is true because
n
lim
nq1 log
sy1
by l’Hospital’s rule
Ž
.
Ž
.
nq1
n™
and
1
lim
n log n O
s0
see Example 4.2.3 2
.
Ž
.
Ž .
q1
ž
/
n
n™

w
Ž
.x
Ž
Since Ý
1r n log n
is a divergent series
this can be shown by using
ns1
.

Cauchy’s condensation test , then by Kummer’s test, the series Ý
a
is
ns1
n
divergent.

EXAMPLE 5.2.8.
Gauss established his test in order to determine the
convergence of the so-called hypergeometric series. He managed to do so in
an article published in 1812. This series is of the form 1qÝ
a , where
ns1
n
 q1
q2  qny1 
q1
q2 
qny1
Ž
. Ž
.
Ž
.
Ž
. Ž
.
Ž
.
a s
,
n
n! q1
q2  qny1
Ž
. Ž
.
Ž
.
ns1, 2, . . . ,
where , ,  are real numbers, and none of them is zero or a negative
integer. We have
a
nq1
nq
n2q q1 nq
Ž
. Ž
.
Ž
.
n
s
s
2
a
nq
nq
n q q nq
Ž
. Ž
.
Ž
.
nq1
q1yy
1
s1q
qO
.
2ž /
n
n
In this case, sq1yy and s1. By Gauss’s test, this series is
convergent if 1, or q, and is divergent if F1, or Fq.

INFINITE SEQUENCES AND SERIES
158
5.2.2. Series of Positive and Negative Terms
Consider the series Ý
a , where a
may be positive or negative for nG1.
ns1
n
n
The convergence of this general series can be determined by the Cauchy
Ž
.
criterion
Theorem 5.1.6 . However, it is more convenient to consider the



series Ý
a
of absolute values, to which the tests of convergence in
ns1
n
Section 5.2.1 can be applied. We recall from Definition 5.2.2 that if the latter
series converges, then the series Ý
a
is absolutely convergent. This is a
ns1
n
stronger type of convergence than the one given in Definition 5.2.1, since by




Theorem 5.2.3 convergence of Ý
a
implies convergence of Ý
a . The
ns1
n
ns1
n
converse, however, is not necessarily true, that is, convergence of Ý
a
ns1
n



does not necessarily imply convergence of Ý
a
. For example, consider
ns1
n
the series
ny1

y1
1
1
1
Ž
.
s1y
q
y
q  .
5.36
Ž
.
Ý
2ny1
3
5
7
ns1
This series is convergent by the result of Example 5.1.6. It is not, however,

w
Ž
.x
absolutely convergent, since Ý
1r 2ny1
is divergent by comparison
ns1

Ž
.
with the harmonic series Ý
1rn , which is divergent. We recall that a
ns1
Ž
.
series such as 5.36 that converges, but not absolutely, is called a condition-
ally convergent series.
Ž
.
The series in 5.36 belongs to a special class of series known as alternating
series.

Ž
.ny1
Definition 5.2.3.
The series Ý
y1
a , where a 0 for nG1, is
ns1
n
n
called an alternating series.

The following theorem, which was established by Gottfried Wilhelm
Ž
.
Leibniz
16461716 , can be used to determine convergence of alternating
series:

Ž
.ny1
Theorem 5.2.14.
Let Ý
y1
a
be an alternating series such that
ns1
n

4
the sequence
a
is monotone decreasing and converges to zero as
n ns1
n™. Then the series is convergent.
Proof. Let s be the nth partial sum of the series, and let m be an integer
n
such that mn. Then
n
iy1
s ys s
y1
a
Ž
.
Ý
n
m
i
ismq1
m
nymy1
s y1
a
ya
q  q y1
a
.
5.37
Ž
.
Ž
.
Ž
.
mq1
mq2
n

INFINITE SERIES
159

4
Since
a
is monotone decreasing, it is easy to show that the quantity
n ns1
Ž
.
inside brackets in 5.37 is nonnegative. Hence,
nymy1


s ys
sa
ya
q  q y1
a .
Ž
.
n
m
mq1
mq2
n
Now, if nym is odd, then


s ys
sa
y a
ya
y  y a
ya
Ž
.
Ž
.
n
m
mq1
mq2
mq3
ny1
n
Fa
.
mq1
If nym is even, then


s ys
sa
y a
ya
y  y a
ya
ya
Ž
.
Ž
.
n
m
mq1
mq2
mq3
ny2
ny1
n
Fa
.
mq1
Thus in both cases


s ys
Fa
.
n
m
mq1

4
Since the sequence
a
converges to zero, then for a given 0 there
n ns1
exists an integer N such that for mGN, a
. Consequently,
mq1


s ys

if nmGN.
n
m
By Theorem 5.2.1, the alternating series is convergent.

Ž
.
EXAMPLE
5.2.9. The series given by formula 5.36 was shown earlier to
be convergent. This result can now be easily verified with the help of
Theorem 5.2.14.

Ž
.n
k
EXAMPLE 5.2.10. The series Ý
y1 rn
is absolutely convergent if
ns1
Ž
k1, is conditionally convergent if 0kF1, and is divergent if kF0 since
.
the nth term does not go to zero .

n '
Ž
.
Ž
.
EXAMPLE 5.2.11. The series Ý
y1 r
n log n
is conditionally con-
ns2
vergent, since it converges by Theorem 5.2.14, but the series of absolute
Ž
.
values diverges by Cauchy’s condensation test Theorem 5.2.10 .
5.2.3. Rearrangement of Series
One of the main differences between infinite series and finite series is that
whereas the latter are amenable to the laws of algebra, the former are not
necessarily so. In particular, if the order of terms of an infinite series is
Ž
.
altered, its sum assuming it converges may, in general, change; or worse, the

INFINITE SEQUENCES AND SERIES
160
altered series may even diverge. Before discussing this rather disturbing
phenomenon, let us consider the following definition:
Definition 5.2.4.
Let Jq denote the set of positive integers and Ý
a
ns1
n
be a given series. Then a second series such as Ý
b
is said to be a
ns1
n
rearrangement of Ý
a
if there exists a one-to-one and onto function
ns1
n
f: Jq™Jq such that b sa
for nG1.
n
fŽn.
For example, the series
1
1
1
1
1
1q y q q y q  ,
5.38
Ž
.
3
2
5
7
4
where two positive terms are followed by one negative term, is a rearrange-
ment of the alternating harmonic series
1
1
1
1
1y q y q y  .
5.39
Ž
.
2
3
4
5
Ž
.
Ž
.
The series in
5.39
is conditionally convergent, as is the series in
5.38 .
Ž
.
However, the two series have different sums see Exercise 5.21 .

Fortunately, for absolutely convergent series we have the following
theorem:
Theorem 5.2.15.
If the series Ý
a
is absolutely convergent, then any
ns1
n
rearrangement of it remains absolutely convergent and has the same sum.
Proof. Suppose that Ý
a is absolutely convergent and that Ý
b is a
ns1
n
ns1
n
rearrangement of it. By Theorem 5.2.1, for a given 0, there exists an
integer N such that for all nmN,
n



a 
.
Ý
i
2
ismq1
We then have




a
F
if mN.
Ý
mqk
2
ks1
Now, let us choose an integer M large enough so that

4
1, 2, . . . , Nq1 ; f 1 , f 2 , . . . , f M
.

4
Ž .
Ž .
Ž
.
Ž .
It follows that if nM, then f n GNq2. Consequently, for nmM,
n
n




b s
a
Ý
Ý
i
fŽi.
ismq1
ismq1




F
a
F
.
Ý
Nqkq1
2
ks1

INFINITE SERIES
161



This implies that the series Ý
b
satisfies the Cauchy criterion of Theo-
ns1
n
rem 5.2.1. Therefore, Ý
b is absolutely convergent.
ns1
n
We now show that the two series have the same sum. Let ssÝ
a , and
ns1
n
s
be its nth partial sum. Then, for a given 0 there exists an integer N
n
large enough so that



s
ys 
.
Nq1
2
If t
is the nth partial sum of Ý
b , then
n
ns1
n






t ys F t ys
q s
ys .
n
n
Nq1
Nq1
By choosing M large enough as was done earlier, and by taking nM,
we get
n
Nq1


t ys
s
b y
a
Ý
Ý
n
Nq1
i
i
is1
is1
n
Nq1
s
a
y
a
Ý
Ý
fŽi.
i
is1
is1




F
a
F
,
Ý
Nqkq1
2
ks1
since if nM,

4

4
a , a , . . . , a
; a
, a
, . . . , a
.
1
2
Nq1
fŽ1.
fŽ2.
fŽn.
Hence, for nM,


t ys  ,
n
which shows that the sum of the series Ý
b is s.

ns1
n
Unlike absolutely convergent series, those that are conditionally conver-
gent are susceptible to rearrangements of their terms. To demonstrate this,
let us consider the following alternating series:
ny1


y1
Ž
.
a s
.
Ý
Ý
n
'n
ns1
ns1
This series is conditionally convergent, since it is convergent by Theorem

'
Ž
.
5.2.14 while Ý
1r n
is divergent. Let us consider the following rear-
ns1
rangement:

1
1
1
1
1
b s1q
y
q
q
y
q 
5.40
Ž
.
Ý
n
'
'
'
'
'
3
2
5
7
4
ns1

INFINITE SEQUENCES AND SERIES
162
in which two positive terms are followed by one that is negative. Let s
3n
Ž
.
denote the sum of the first 3n terms of 5.40 . Then
1
1
1
1
1

s
s 1q
y
q
q
y
q 
3n ž
/ ž
/
'
'
'
'
'
3
2
5
7
4
1
1
1
q
q
y
ž
/
'
'
'
4ny3
4ny1
2n
1
1
1
1
1
s 1y
q
y
q  q
y
ž
/ ž
/
ž
/
'
'
'
'
'
2
3
4
2ny1
2n
1
1
1
1
q
q
q  q
q
'
'
'
'
2nq1
2nq3
4ny3
4ny1
1
1
1
ss
q
q
q  q
,
2 n
'
'
'
2nq1
2nq3
4ny1
where s
is the sum of the first 2n terms of the original series. We note that
2 n
n

s
s
q
.
5.41
Ž
.
3n
2 n
'4ny1
Ž
.
If s is the sum of the original series, then lim
s
ss in 5.41 . But, since
n™
2 n
n
lim
s,
'
n™
4ny1
 
 4
Ž
.
the sequence s
is not convergent, which implies that the series in 5.40
3n ns1
is divergent. This clearly shows that a rearrangement of a conditionally
convergent series can change its character. This rather unsettling characteris-
tic of conditionally convergent series is depicted in the following theorem due
Ž
.
to Georg Riemann 18261866 :
Theorem 5.2.16.
A conditionally convergent series can always be rear-
ranged so as to converge to any given number s, or to diverge to q or
to y.
Proof. The proof can be found in several books, for example, Apostol
Ž
.
Ž
.
Ž
.
1964, page 368 , Fulks 1978, page 489 , Knopp 1951, page 318 , and Rudin
Ž
.
1964, page 67 .

5.2.4. Multiplication of Series
Suppose that Ý
a
and Ý
b
are two series. We recall from Theorem
ns1
n
ns1
n
5.2.2 that if these series are convergent, then their sum is a convergent series

INFINITE SERIES
163
obtained by adding the two series term by term. The product of these two
series, however, requires a more delicate operation. There are several ways
to define this product. We shall consider the so-called Cauchy’s product.
Definition 5.2.5.
Let Ý
a
and Ý
b
be two series in which the
ns0
n
ns0
n
summation index starts at zero instead of one. Cauchy’s product of these two
series is the series Ý
c , where
ns0
n
n
c s
a b
,
ns0, 1, 2, . . . ,
Ý
n
k
nyk
ks0
that is,

c sa b q a b qa b
q a b qa b qa b
q  .
Ž
.
Ž
.
Ý
n
0
0
0
1
1
0
0
2
1
1
2
0
ns0
Other products could have been defined by simply adopting different ar-
rangements of the terms that make up the series Ý
c .

ns0
n
The question now is: under what condition will Cauchy’s product of two
series converge? The answer to this question is given in the next theorem.
Theorem 5.2.17.
Let Ý
c be Cauchy’s product of Ý
a and Ý
b .
ns0
n
ns0
n
ns0
n
Suppose that these two series are convergent and have sums equal to s and t,
respectively.
1. If at least one of Ý
a
and Ý
b
converges absolutely, then
ns0
n
ns0
n

Ž
Ý
c
converges and its sum is equal to st
this result is known as
ns0
n
.
Mertens’s theorem .
2. If both series are absolutely convergent, then Ý
c
converges abso-
ns0
n
Ž
.
lutely to the product st this result is due to Cauchy .
Proof.
1. Suppose that Ý
a
is the series that converges absolutely. Let s , t ,
ns0
n
n
n
and u
denote the partial sums Ýn
a , Ýn
b , and Ýn
c , respec-
n
is0
i
is0
i
is0
i
tively. We need to show that u ™st as n™. We have that
n
u sa b q a b qa b
q  q a b qa b
q  qa b
Ž
.
Ž
.
n
0
0
0
1
1
0
0
n
1
ny1
n
0
sa t qa t
q  qa t .
5.42
Ž
.
0 n
1 ny1
n 0
Let 
denote the remainder of the series Ý
b
with respect to t ,
n
ns0
n
n
Ž
.
that is,  styt
ns0, 1, 2, . . . . By making the proper substitution in
n
n

INFINITE SEQUENCES AND SERIES
164
Ž
.
5.42 we get
u sa
ty
qa
ty
q  qa
ty
Ž
.
Ž
.
Ž
.
n
0
n
1
ny1
n
0
sts y a  qa 
q  qa 
.
5.43
Ž
.
Ž
.
n
0
n
1
ny1
n
0
Ž .
Since s ™s as n™, the proof of 1 will be complete if we can show
n
Ž
.
that the sum inside parentheses in 5.43 goes to zero as n™. We
now proceed to show that this is the case.

4
Let 0 be given. Since the sequence

converges to zero,
n ns0
there exists an integer N such that


if nN.
n
Hence,
a  qa 
q  qa 
0
n
1
ny1
n
0
F a  qa
 q  qa

n
0
ny1
1
nyN
N
q a

qa

q  qa 
nyNy1
Nq1
nyNy2
Nq2
0
n
nyNy1
 a  qa
 q  qa

q
a
Ý
n
0
ny1
1
nyN
N
i
is0
n
B
a qs*,
5.44
Ž
.
Ý
i
isnyN
Ž
 


 .
where Bsmax

,  , . . . , 
and s* is the sum of the series
0
1
N


 Ž

.
Ý
a
Ý
a
is absolutely convergent . Furthermore, because of
ns0
n
ns0
n
this and by the Cauchy criterion we can find an integer M such that
n
a 
if nyNMq1.
Ý
i
isnyN
Ž
.
Thus when nNqMq1 we get from inequality 5.44
a  qa 
q  qa 
F Bqs* .
Ž
.
0
n
1
ny1
n
0
Since  can be arbitrarily small, we conclude that
lim
a  qa 
q  qa 
s0.
Ž
.
0
n
1
ny1
n
0
n™

SEQUENCES AND SERIES OF FUNCTIONS
165



2. Let ® denote the nth partial sum of Ý
c . Then
n
is0
i
® s a b
q a b qa b
q  q a b qa b
q  qa b
n
0
0
0
1
1
0
0
n
1
ny1
n
0
F a
b
q a
b
q a
b
0
0
0
1
1
0
q  q a
b q a
b
q  q a
b
0
n
1
ny1
n
0



s a t q a t
q  q a t ,
0
n
1
ny1
n
0

k


where t sÝ
b , ks0, 1, 2, . . . , n. Thus,
k
is0
i







® F
a
q a
q  q a
t
Ž
.
n
0
1
n
n
Fs*t*
for all n,



where t* is the sum of the series Ý
b , which is convergent by
ns0
n

4
assumption. We conclude that the sequence
®
is bounded. Since
n ns0
® G0, then by Exercise 5.12 this sequence is convergent, and therefore
n

Ž .
Ý
c
converges absolutely. By part 1 , the sum of this series is st.
ns0
n

It should be noted that absolute convergence of at least one of Ý
a
ns0
n

Ž .
and Ý
b is an essential condition for the validity of part 1 of Theorem
ns0
n
5.2.17. If this condition is not satisfied, then Ý
c
may not converge. For
ns0
n
example, consider the series Ý
a , Ý
b , where
ns0
n
ns0
n
n
y1
Ž
.
a sb s
,
ns0, 1, . . . .
n
n
'nq1
These two series are convergent by Theorem 5.2.14. They are not, however,
Ž
absolutely convergent, and their Cauchy’s product is divergent see Exercise
.
5.22 .
5.3. SEQUENCES AND SERIES OF FUNCTIONS
All the sequences and series considered thus far in this chapter had constant
terms. We now extend our study to sequences and series whose terms are
functions of x.

Ž .4
Definition 5.3.1.
Let
f
x
be a sequence of functions defined on a
n
ns1
set D;R.

INFINITE SEQUENCES AND SERIES
166
Ž .
1. If there exists a function f x defined on D such that for every x in D,
lim f
x sf x ,
Ž .
Ž .
n
n™

Ž .4
Ž .
then the sequence
f
x
is said to converge to f x on D. Thus for
n
ns1

Ž .
Ž .
a given 0 there exists an integer N such that
f
x yf x
 if
n
nN. In general, N depends on  as well as on x.

Ž .
Ž .
Ž .
2. If Ý
f
x converges for every x in D to s x , then s x is said to be
ns1
n
the sum of the series. In this case, for a given 0 there exists an
integer N such that
s
x ys x

if nN,
Ž .
Ž .
n
Ž .

Ž .
where s
x is the nth partial sum of the series Ý
f
x . The integer
n
ns1
n
N depends on  and, in general, on x also.
Ž .
3. In particular, if N in
1 depends on  but not on xgD, then the

Ž .4
Ž .
sequence
f
x
is said to converge uniformly to
f x
on
D.
n
ns1
Ž .
Similarly, if N in 2 depends on , but not on xgD, then the series

Ž .
Ž .
Ý
f
x converges uniformly to s x on D.

ns1
n
Ž
.
The Cauchy criterion for sequences Theorem 5.1.6 and its application to
Ž
.
series Theorem 5.2.1 apply to sequences and series of functions. In case of
uniform convergence, the integer N described in this criterion depends only
on .

Ž .4
Theorem 5.3.1.
Let
f
x
be a sequence of functions defined on
n
ns1
Ž .
D;R and converging to f x . Define the number 
as
n
 s sup
f
x yf x
.
Ž .
Ž .
n
n
xgD
Ž .
Then the sequence converges uniformly to f x on D if and only if  ™0 as
n
n™.
Ž .
Proof. Sufficiency: Suppose that  ™0 as n™. To show that f
x ™
n
n
Ž .
f x uniformly on D. Let 0 be given. Then there exists an integer N such
that for nN,  . Hence, for such values of n,
n
f
x yf x
F 
Ž .
Ž .
n
n

Ž .4
for all xgD. Since N depends only on , the sequence
f
x
converges
n
ns1
Ž .
uniformly to f x on D.
Ž .
Ž .
Necessity: Suppose that f
x ™f x uniformly on D. To show that  ™0.
n
n
Let 0 be given. There exists an integer N that depends only on  such
that for nN,

f
x yf x

Ž .
Ž .
n
2

SEQUENCES AND SERIES OF FUNCTIONS
167
for all xgD. It follows that

 s sup
f
x yf x
F
.
Ž .
Ž .
n
n
2
xgD
Thus  ™0 as n™.

n
Theorem 5.3.1 can be applied to convergent series of functions by replac-
Ž .
Ž .
Ž .
Ž .
Ž .
ing f
x and f x with s
x and s x , respectively, where s
x is the nth
n
n
n
Ž .
partial sum of the series and s x is its sum.
Ž .
Ž
.
Ž .
EXAMPLE 5.3.1.
Let f
x ssin 2 xrn , 0FxF1. Then
f
x ™0 as
n
n
n™. Furthermore,
2 x
2 x
2
sin
F
F
.
ž
/
n
n
n
In this case,
2 x
2
 s sup
sin
F
.
n
ž
/
n
n
0FxF1

Ž .4
Thus  ™0 as n™, and the sequence
f
x
converges uniformly to
n
n
ns1
Ž .
w
x
f x s0 on 0, 1 .
The next theorem provides a simple test for uniform convergence of series
Ž
.
of functions. It is due to Karl Weierstrass 18151897 .
Ž
.

Ž .
Theorem 5.3.2 Weierstrass’s M-test .
Let Ý
f
x be a series of func-
ns1
n

4
tions defined on D;R. If there exists a sequence M
of constants such
n ns1
that
f
x
FM ,
ns1, 2,, . . . ,
Ž .
n
n


Ž .
for all xgD, and if Ý
M converges, then Ý
f
x converges uniformly
ns1
n
ns1
n
on D.
Ž
.
Proof. Let 0 be given. By the Cauchy criterion Theorem 5.2.1 , there
exists an integer N such that
n
M 
Ý
i
ismq1

INFINITE SEQUENCES AND SERIES
168
for all nmN. Hence, for all such values of m, n, and for all xgD,
n
n
f
x
F
f
x
Ž .
Ž .
Ý
Ý
i
i
ismq1
ismq1
n
F
M .
Ý
i
ismq1

Ž .
This implies that Ý
f
x
converges uniformly on D by the Cauchy
ns1
n
criterion.

We note that Weierstrass’s M-test is easier to apply than Theorem 5.3.1,
Ž .
since it does not require specifying the sum s x of the series.

Ž .4
EXAMPLE 5.3.2.
Let us investigate convergence of the sequence f
x
,
n
ns1
Ž .
where f
x is defined as
n
1
°
2 xq
,
0Fx1,
n
x
~exp
,
1Fx2,
f
x s
Ž .
n
ž /
n
1
1y
,
xG2.
¢
n
This sequence converges to
2 x,
0Fx1,
f x s
Ž . ½ 1,
xG1.
Now,
1rn,
0Fx1,
°~exp xrn y1,
1Fx2,
Ž
.
f
x yf x
s
Ž .
Ž .
n
¢1rn,
xG2.
However, for 1Fx2,
exp xrn y1exp 2rn y1.
Ž
.
Ž
.
Furthermore, by Maclaurin’s series expansion,
k

2
2rn
2
1
Ž
.
exp
y1s


.
Ý
ž /
n
k!
n
n
ks1

SEQUENCES AND SERIES OF FUNCTIONS
169
Thus,
sup
f
x yf x
sexp 2rn y1,
Ž .
Ž .
Ž
.
n
0Fx

Ž .4
which tends to zero as n™. Therefore, the sequence
f
x
converges
n
ns1
Ž .
w
.
uniformly to f x on 0,  .

Ž .
EXAMPLE 5.3.3.
Consider the series Ý
f
x , where
ns1
n
x n
f
x s
,
0FxF1.
Ž .
n
3
n
n qnx
Ž .
The function f
x is monotone increasing with respect to x. It follows that
n
for 0FxF1,
1
f
x
sf
x F
,
ns1, 2, . . . .
Ž .
Ž .
n
n
3
n qn

w
Ž
3
.x

Ž .
But the series Ý
1r n qn
is convergent. Hence, Ý
f
x is uniformly
ns1
ns1
n
w
x
convergent on 0, 1 by Weierstrass’s M-test.
5.3.1. Properties of Uniformly Convergent Sequences and Series
Sequences and series of functions that are uniformly convergent have several
interesting properties. We shall study some of these properties in this section.

Ž .4
Ž .
Theorem 5.3.3.
Let
f
x
be uniformly convergent to f x on a set
n
ns1
Ž .
D. If for each n, f
x has a limit 
as x™x , where x
is a limit point of
n
n
0
0

4
Ž .
D, then the sequence 
converges to  slim
f x . This is equiva-
n ns1
0
x™x 0
lent to stating that
lim
lim f
x
s lim
lim f
x
.
Ž .
Ž .
n
n
n™
x™x
x™x
n™
0
0

4
Proof. Let us first show that

is a convergent sequence. By the
n ns1
Ž
.
Cauchy criterion Theorem 5.1.6 , there exists an integer N such that for a
given 0,

f
x yf
x

for all mN, nN.
5.45
Ž .
Ž .
Ž
.
m
n
2
Ž
.
The integer N depends only on , and inequality 5.45 is true for all xgD,
since the sequence is uniformly convergent. By taking the limit as x™x
in
0
Ž
.
5.45 we get



 y
F
if mN, nN,
m
n
2

INFINITE SEQUENCES AND SERIES
170

4
which indicates that 
is a Cauchy sequence and is therefore conver-
n ns1
Ž .
gent. Let  slim
 . We now need to show that f x has a limit and that
0
n™
n
this limit is equal to  . Let 0 be given. There exists an integer N such
0
1
that for nN ,
1

f x yf
x

Ž .
Ž .
n
4
for all xgD, by the uniform convergence of the sequence. Furthermore,
there exists an integer N such that
2



 y

if nN .
n
0
2
4
Ž
.
Thus for nmax N , N ,
1
2



f x yf
x
q  y

Ž .
Ž .
n
n
0
2
for all xgD. Then
f x y
F f x yf
x
q f
x y
q  y
Ž .
Ž .
Ž .
Ž .
0
n
n
n
n
0

 f
x y
q
5.46
Ž .
Ž
.
n
n
2
Ž
.
Ž
.
if nmax N , N
for all xgD. By taking the limit as x™x
in 5.46 we
1
2
0
get

lim f x y
F
5.47
Ž .
Ž
.
0
2
x™x 0
by the fact that
lim
f
x y
s0
for ns1, 2, . . . .
Ž .
n
n
x™x 0
Ž
.
Since  is arbitrarily small, inequality 5.47 implies that
lim f x s .

Ž .
0
x™x 0

Ž .4
Corollary 5.3.1.
Let
f
x
be a sequence of continuous functions
n
ns1
Ž .
Ž .
that converges uniformly to f x on a set D. Then f x is continuous on D.
Ž
.
Proof. The proof follows directly from Theorem 5.3.3, since  sf
x
for
n
n
0
Ž
.
Ž
.
nG1 and  slim
 slim
f
x
sf x
.

0
n™
n
n™
n
0
0

SEQUENCES AND SERIES OF FUNCTIONS
171

Ž .
Corollary 5.3.2.
Let Ý
f
x
be a series of functions that converges
ns1
n
Ž .
Ž .
uniformly to s x on a set D. If for each n, f
x has a limit 
as x™x ,
n
n
0

Ž .
then the series Ý

converges and has a sum equal to s slim
s x ,
ns1 n
0
x™x 0
that is,


lim
f
x s
lim f
x .
Ž .
Ž .
Ý
Ý
n
n
x™x
x™x
0
0
ns1
ns1
Proof. The proof is left to the reader.

By combining Corollaries 5.3.1 and 5.3.2 we conclude the following corol-
lary:

Ž .
Corollary 5.3.3.
Let Ý
f
x
be a series of continuous functions that
ns1
n
Ž .
Ž .
converges uniformly to s x on a set D. Then s x is continuous on D.
Ž .
2 Ž
2.ny1
w
.
EXAMPLE 5.3.4.
Let f
x sx r 1qx
be defined on 1,  for nG1.
n
Ž .

Ž .
Let s
x be the nth partial sum of the series Ý
f
x . Then,
n
ns1
n
1
1y
n
2
2
n
x
1qx
Ž
.
2
s
x s
sx
,
Ž .
Ý
n
ky1
1
2
1qx
Ž
.
ks1
1y
2
1qx
by using the fact that the sum of the finite geometric series Ýn
aky1 is
ks1
n
n
1ya
ky1
a
s
.
5.48
Ž
.
Ý
1ya
ks1
Ž
2.
Since 1r 1qx
1 for xG1, then as n™,
x 2
2
s
x ™
s1qx .
Ž .
n
1
1y
2
1qx
Thus,

2
x
2
s1qx .
Ý
ny1
2
1qx
Ž
.
ns1

INFINITE SEQUENCES AND SERIES
172
Now, let x s1, then
0

2

x
1
1
lim
s
s
s2
Ý
Ý
1
ny1
ny1
2
1y
2
x™1
2
1qx
Ž
.
ns1
ns1
s lim 1qx 2 ,
Ž
.
x™1
1
Ž
.
which results from applying formula
5.48
with as
and then letting
2
n™. This provides a verification to Corollary 5.3.2. Note that the series

Ž .
Ž
.
Ý
f
x is uniformly convergent by Weierstrass’s M-test why? .
ns1
n
Corollaries 5.3.2 and 5.3.3 clearly show that the properties of the function
Ž .
Ž .

Ž .
f
x
carry over to the sum s x
of the series Ý
f
x
when the series is
n
ns1
n
uniformly convergent.
Ž .
Ž .
Another property that s x shares with the f
x ’s is given by the following
n
theorem:

Ž .
Ž .
Theorem 5.3.4.
Let Ý
f
x
be a series of functions, where f
x
is
ns1
n
n
w
x

Ž .
differentiable on a, b for nG1. Suppose that Ý
f
x converges at least
ns1
n
w
x


Ž .
w
x
at one point x g a, b and that Ý
f
x
converges uniformly on
a, b .
0
ns1
n
Then we have the following:

Ž .
Ž .
w
x
1. Ý
f
x converges uniformly to s x on a, b .
ns1
n
Ž .


Ž .
Ž .
2. s x sÝ
f
x , that is, the derivative of s x
is obtained by a
ns1
n

Ž .
term-by-term differentiation of the series Ý
f
x .
ns1
n
Proof.
w
x
Ž
1. Let xx
be a point in a, b . By the mean value theorem Theorem
0
.
4.2.2 , there exists a point 
between x and x
such that for nG1,
n
0
f
x yf
x
s xyx
f 
 
.
5.49
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
n
n
0
0
n
n


Ž .
w
x
Since Ý
f
x is uniformly convergent on a, b , then by the Cauchy
ns1
n
criterion, there exists an integer N such that
n


f
x

Ž .
Ý
i
bya
ismq1
w
x
Ž
.
for all nmN and for any xg a, b . From 5.49 we get
n
n



f
x yf
x
s xyx
f

Ž .
Ž
.
Ž
.
Ý
Ý
i
i
0
0
i
i
ismq1
ismq1




xyx0
bya


SEQUENCES AND SERIES OF FUNCTIONS
173
w
x
for all nmN and for any xg a, b . This shows that

f
x yf
x
Ž .
Ž
.
Ý
n
n
0
ns1
is uniformly convergent on D. Consequently,


f
x s
f
x yf
x
qs x
Ž .
Ž .
Ž
.
Ž
.
Ý
Ý
n
n
n
0
0
ns1
ns1
Ž .
Ž
.
is uniformly convergent to s x
on D, where s x
is the sum of the
0

Ž
.
series Ý
f
x
, which was assumed to be convergent.
ns1
n
0
Ž .
2. Let 
h denote the ratio
n
f
xqh yf
x
Ž
.
Ž .
n
n

h s
,
ns1, 2, . . . ,
Ž .
n
h
w
x
where both x and xqh belong to a, b . By invoking the mean value
Ž .
theorem again, 
h can be written as
n

h sf 
 xq h ,
ns1, 2, . . . ,
Ž .
Ž
.
n
n
n
where
0    1.
Furthermore,
by
the
uniform
convergence
of
n


Ž .

Ž .
Ý
f
x we can deduce that Ý

h is also uniformly convergent
ns1
n
ns1
n
w
x
on yr, r for some r0. But


f
xqh yf
x
Ž
.
Ž .
n
n

h s
Ž .
Ý
Ý
n
h
ns1
ns1
s xqh ys x
Ž
.
Ž .
s
,
5.50
Ž
.
h
Ž .

Ž .
where s x
is the sum of the series Ý
f
x . Let us now apply
ns1
n

Ž .
Corollary 5.3.2 to Ý

h . We get
ns1
n


lim

h s
lim 
h .
5.51
Ž .
Ž .
Ž
.
Ý
Ý
n
n
h™0
h™0
ns1
ns1
Ž
.
Ž
.
From 5.50 and 5.51 we then have

s xqh ys x
Ž
.
Ž .

lim
s
f
x .
Ž .
Ý
n
h
h™0
ns1
Thus,


s x s
f
x .

Ž .
Ž .
Ý
n
ns1

INFINITE SEQUENCES AND SERIES
174
5.4. POWER SERIES
A power series is a special case of the series of functions discussed in Section
5.3. It is of the form Ý
a x n, where the a ’s are constants. We have
ns0
n
n
already encountered such series in connection with Taylor’s and Maclaurin’s
series in Section 4.3.
Obviously, just as with any series of functions, the convergence of a power
series depends on the values of x. By definition, if there exists a number

n
 
 
0 such that Ý
a x
is convergent if x  and is divergent if x ,
ns0
n
then  is said to be the radius of convergence of the series, and the interval
Ž
.
y, 
is called the interval of convergence. The set of all values of x for
which the power series converges is called its region of convergence.
The definition of the radius of convergence implies that Ý
a x n is
ns0
n
absolutely convergent within its interval of convergence. This is shown in the
next theorem.
Theorem 5.4.1.
Let  be the radius of convergence of Ý
a x n. Sup-
ns0
n
pose that 0. Then Ý
a x n converges absolutely for all x inside the
ns0
n
Ž
.
interval y,  .
 
Ž
.
Proof. Let x be such that x . There exists a point x g y,  such
0
 



n
that
x  x
. Then, Ý
a x
is a convergent series. By Result 5.2.1,
0
ns0
n
0
n

n4
a x ™0 as n™, and hence a x
is a bounded sequence by Theorem
n
0
n
0 ns0
5.1.1. Thus

n
a x
K
for all n.
n
0
Now,
n
x
n
n


a x
s a
x
n
n
0
ž /
x0
K n,
where
x
s
1.
x0
Since the geometric series Ý
 n is convergent, then by the comparison test
ns0
Ž
.


n
see Theorem 5.2.4 , the series Ý
a x
is convergent.

ns0
n
To determine the radius of convergence we shall rely on some of the tests
of convergence given in Section 5.2.1.
Theorem 5.4.2.
Let Ý
a x n be a power series. Suppose that
ns0
n
anq1
lim
sp.
a
n™
n

POWER SERIES
175
Then the radius of convergence of the power series is
1rp,
0p,
°~0,
ps,
s¢,
ps0.
Proof. The proof follows from applying the ratio test given in Theorem


n
5.2.6 to the series Ý
a x
: We have that if
ns0
n
nq1
a
x
nq1
lim
1,
n
a x
n™
n
then Ý
a x n is absolutely convergent. This inequality can be written as
ns0
n
 
p x 1.
5.52
Ž
.
 
If 0p, then absolute convergence occurs if
x 1rp and the series
 
diverges when
x 1rp. Thus s1rp. If ps, the series diverges when-
Ž
.
ever x0. In this case, s0. If ps0, then 5.52 holds for any value of x,
that is, s.

Theorem 5.4.3.
Let Ý
a x n be a power series. Suppose that
ns0
n

 1r n
lim sup a
sq.
n
n™
Then,
1rq,
0q,
°~0,
qs,
s¢,
qs0.
Proof. This result follows from applying the root test in Theorem 5.2.8 to


n
the series Ý
a x
. Details of the proof are similar to those given in
ns0
n
Theorem 5.4.2.

The determination of the region of convergence of Ý
a x n depends on
ns0
n
 
the value of . We know that the series converges if x  and diverges if
 x . The convergence of the series at xs and xsy has to be
Ž
.
determined separately. Thus the region of convergence can be
y,  ,
w
. Ž
x
w
x
y,  , y,  , or y,  .
EXAMPLE 5.4.1. Consider the geometric series Ý
x n. By applying either
ns0
Theorem 5.4.2 or Theorem 5.4.3, it is easy to show that s1. The series
Ž
.
diverges if xs1 or y1. Thus the region of convergence is y1, 1 . The sum

INFINITE SEQUENCES AND SERIES
176
Ž
.
of this series can be obtained from formula 5.48 by letting n go to infinity.
Thus

1
n
x s
,
y1x1.
5.53
Ž
.
Ý
1yx
ns0

Ž
n
.
EXAMPLE 5.4.2. Consider the series Ý
x rn! . Here,
ns0
a
n!
nq1
lim
s lim
a
nq1 !
n™
n™ Ž
.
n
1
s lim
s0.
nq1
n™
Thus s, and the series converges absolutely for any value of x. This
particular series is Maclaurin’s expansion of e x, that is,

n
x
x
e s
.
Ý n!
ns0

Ž
n
.
EXAMPLE 5.4.3. Suppose we have the series Ý
x rn . Then
ns1
a
n
nq1
lim
s lim
s1,
a
nq1
n™
n™
n
and s1. When xs1 we get the harmonic series, which is divergent. When
xsy1 we get the alternating harmonic series, which is convergent by
w
.
Theorem 5.2.14. Thus the region of convergence is y1, 1 .
In addition to being absolutely convergent within its interval of conver-
gence, a power series is also uniformly convergent there. This is shown in the
next theorem.
Theorem 5.4.4.
Let Ý
a x n be a power series with a radius of conver-
ns0
n
Ž
.
gence  0 . Then we have the following:
w
x
1. The series converges uniformly on the interval yr, r , where r.
Ž .

n
Ž . Ž .
w
x
Ž .
2. If s x sÝ
a x , then s x
i
is continuous on
yr, r ;
ii
is
ns0
n
w
x
differentiable on yr, r and has derivative

ny1
s x s
na x
,
yrFxFr;
Ž .
Ý
n
ns1

POWER SERIES
177
Ž
.
w
x
and iii has derivatives of all orders on yr, r and
k

d s x
a n!
Ž .
n
nyk
s
x
,
ks1, 2, . . . , yrFxFr.
Ý
k
nyk !
dx
Ž
.
nsk
Proof.
 

n


n



n
1. If
x Fr, then
a x
F a
r
for nG0. Since Ý
a
r
is conver-
n
n
ns0
n
Ž
.
gent by Theorem 5.4.1, then by the Weierstrass M-test Theorem 5.3.2 ,

n
w
x
Ý
a x
is uniformly convergent on yr, r .
ns0
n
Ž .
Ž .
Ž .
2.
i Continuity of s x follows directly from Corollary 5.3.3. ii To show
this result, we first note that the two series Ý
a x n and Ý
na x ny1
ns0
n
ns1
n
have the same radius of convergence. This is true by Theorem 5.4.3 and
the fact that

 1r n

 1r n
lim sup na
s lim sup a
,
n
n
n™
n™
since lim
n1r ns1 as n™. We can then assert that Ý
na x ny1
n™
ns1
n
w
x
Ž .
is uniformly convergent on yr, r . By Theorem 5.3.4, s x is differen-
w
x
tiable on
yr, r , and its derivative is obtained by a term-by-term

n Ž
.
Ž .
differentiation of Ý
a x . iii This follows from part ii by repeated
ns0
n
Ž .
differentiation of s x .

Under a certain condition, the interval on which the power series con-
verges uniformly can include the end points of the interval of convergence.
This is discussed in the next theorem.
Theorem 5.4.5.
Let Ý
a x n be a power series with a finite nonzero
ns0
n
radius of convergence . If Ý
a  n is absolutely convergent, then the
ns0
n
w
x
power series is uniformly convergent on y,  .
Proof. The proof is similar to that of part 1 of Theorem 5.4.4. In this case,
 

n


n



n

n
for x F, a x
F a
 . Since Ý
a

is convergent, then Ý
a x
n
n
ns0
n
ns0
n
w
x
is uniformly convergent on y,  by the Weierstrass M-test.

EXAMPLE 5.4.4. Consider the geometric series of Example 5.4.1. This
w
x
series is uniformly convergent on
yr, r , where r1. Furthermore, by
Ž
.
differentiating the two sides of 5.53 we get

1
ny1
nx
s
,
y1x1.
Ý
2
1yx
Ž
.
ns1

INFINITE SEQUENCES AND SERIES
178
Ž
.2
Ž
.
This provides a series expansion of 1r 1yx
within the interval y1, 1 . By
repeated differentiation it is easy to show that for y1x1,

1
nqky1
n
s
x ,
ks1, 2, . . . .
Ý
k
ž
/
n
1yx
Ž
.
ns0
The radius of convergence of this series is s1, the same as for the original
series.
EXAMPLE 5.4.5. Suppose we have the series
n
n

2
x
,
Ý
2
ž
/
1yx
2n qn
ns1
which can be written as

n
z
,
Ý
2
2n qn
ns1
Ž
.
where zs2 xr 1yx . This is a power series in z. By Theorem 5.4.2, the
radius of convergence of this series is s1. We note that when zs1 the

w
Ž
2
.x
series Ý
1r 2n qn
is absolutely convergent. Thus by Theorem 5.4.5,
ns1
 
the given series is uniformly convergent for
z F1, that is, for values of x
satisfying
1
x
1
y
F
F
,
2
1yx
2
or equivalently,
1
y1FxF .
3
5.5. SEQUENCES AND SERIES OF MATRICES
In Section 5.3 we considered sequences and series whose terms were scalar
functions of x rather than being constant as was done in Sections 5.1 and 5.2.
In this section we consider yet another extension, in which the terms of the
series are matrices rather than scalars. We shall provide a brief discussion of
this extension. The interested reader can find a more detailed study of this
Ž
.
Ž
.
Ž
.
topic in Gantmacher
1959 , Lancaster
1969 , and Graybill
1983 . As in
Chapter 2, all matrix elements considered here are real.
For the purpose of our study of sequences and series of matrices we first
need to introduce the norm of a matrix.

SEQUENCES AND SERIES OF MATRICES
179
Definition 5.5.1.
Let A be a matrix of order mn. A norm of A, denoted
	
	
by
A , is a real-valued function of A with the following properties:
	
	
	
	
1.
A G0, and
A s0 if and only if As0.
	
	
  	
	
2.
cA s c
A , where c is a scalar.
	
	
	
	
	
	
3.
AqB F A q B , where B is any matrix of order mn.
	
	
	
		
	
4.
AC F A
C , where C is any matrix for which the product AC is
defined.

Ž
.
If As a
, then examples of matrix norms that satisfy properties 1, 2, 3,
i j
and 4 include the following:
	
	
Ž
m
n
2 .1r2
1. The Euclidean norm,
A
s Ý
Ý
a
.
2
is1
js1
i j
	
	
w
Ž
.x1r2
Ž
.
2. The spectral norm,
A
s e
AA
, where e
AA is the largest
s
max
max
eigenvalue of AA.
Ž
.
Definition 5.5.2.
Let A s a
be matrices of orders mn for kG1.
k
i jk

4
Ž
.
The sequence
A
is said to converge to the mn matrix As a
if
k ks1
i j
lim
a
sa
for is1, 2, . . . , m; js1, 2, . . . , n.

k ™
i jk
i j
For example, the sequence of matrices
1
1
ky1
y1
2
k
kq1
k
A s
,
ks1, 2,, . . . ,
k
2
k
2
0
2
2yk
converges to
0
y1
1
As 2
0
y1
as k™. The sequence
1
2
k y2
k
A s
,
ks1, 2, . . . ,
k
k
1
1qk
does not converge, since k 2y2 goes to infinite as k™.

INFINITE SEQUENCES AND SERIES
180

4
From Definition 5.5.2 it is easy to see that A
converges to A if and
k ks1
only if
	
	
lim
A yA s0,
k
k™
	 	
where
 is any matrix norm.

4
Definition 5.5.3.
Let A
be a sequence of matrices of order mn.
k ks1

Ž
.
Then Ý
A
is called an infinite series or just a series of matrices. This
ks1
k
Ž
.
series is said to converge to the mn matrix Ss s
if and only if the series
i j
Ý
a
converges for all is1, 2, . . . , m; js1, 2, . . . , n, where a
is the
ks1
i jk
i jk
Ž
.
i, j th element of A , and
k

a
ss ,
is1, 2, . . . , m; js1, 2, . . . , n.
5.54
Ž
.
Ý
i jk
i j
ks1

Ž
.
The series Ý
A
is divergent if at least one of the series in
5.54
is
ks1
k
divergent.

From Definition 5.5.3 and Result 5.2.1 we conclude that Ý
A
diverges
ks1
k
Ž
.
if lim
a
0 for at least one pair i, j , that is, if lim
A 0.
k ™
i jk
k ™
k
A particular type of infinite series of matrices is the power series

k
Ž
.
0
Ý
 A , where A is a square matrix, 
is a scalar ks0, 1, . . . , and A is
ks0
k
k
by definition the identity matrix I. For example, the power series
1
1
1
2
3
k
IqAq
A q
A q  q
A q 
2!
3!
k!
Ž . Ž
represents an expansion of the exponential matrix function exp A
see
.
Gantmacher, 1959 .
k
	
	
Theorem 5.5.1.
Let A be an nn matrix. Then lim
A s0 if
A 1,
k ™
	 	
where
 is any matrix norm.
Proof. From property 4 in Definition 5.5.1 we can write
	
k	
	
	 k
A
F A
,
ks1, 2, . . . .
	
	
	
k	
k
Ž
.
Since
A 1, then lim
A
s0, which implies that lim
A s0 why? .
k ™
k ™

Theorem 5.5.2.
Let A be a symmetric matrix of order nn such that


Ž
 1 for is1, 2, . . . , n, where 
is the ith eigenvalue of A
all the
i
i
.

k
eigenvalues of A are real by Theorem 2.3.5 . Then Ý
A
converges to
ks0
Ž
.y1
IyA
.

SEQUENCES AND SERIES OF MATRICES
181
Ž
.
Proof. By the spectral decomposition theorem
Theorem 2.3.10
there
exists an orthogonal matrix P such that AsP P, where  is a diagonal
matrix whose diagonal elements are the eigenvalues of A. Then
AksPkP,
ks0, 1, 2, . . . .


k
k
Since
 1 for all i, then  ™0 and hence A ™0 as k™. Further-
i
more, the matrix IyA is nonsingular, since
IyAsP Iy P
Ž
.
and all the diagonal elements of Iy are positive.
Now, for any nonnegative integer k we have the following identity:
IyA
IqAqA2q  qAk sIyAkq1 .
Ž
. Ž
.
Hence,
y1
k
kq1
IqAq  qA s IyA
IyA
.
Ž
.
Ž
.
By letting k go to infinity we get

y1
k
A s IyA
,
Ž
.
Ý
ks0
since lim
Akq1 s0.

k ™
Theorem 5.5.3.
Let A be a symmetric nn matrix and  be any


	
	
	
	
eigenvalue of A. Then  F A , where
A is any matrix norm of A.
Proof. We have that Avsv, where v is an eigenvector of A for the
	
	
eigenvalue . If
A is any matrix norm of A, then
	
	

 	 	
	
	
	
		 	
v s 
v s Av F A
v .
Since v0, we conclude that


	
	
 F A .

Corollary 5.5.1.
Let A be a symmetric matrix of order nn such that
	
	
	
	

k
A 1, where
A
is any matrix norm of A. Then Ý
A
converges to
ks0
Ž
.y1
IyA
.
Proof. This result follows from Theorem 5.5.2, since for is1, 2, . . . , n,


	
	
 F A 1.

i

INFINITE SEQUENCES AND SERIES
182
5.6. APPLICATIONS IN STATISTICS
Sequences and series have many useful applications in statistics. Some of
these applications will be discussed in this section.
5.6.1. Moments of a Discrete Distribution
Perhaps one of the most visible applications of infinite series in statistics is in
the study of the distribution of a discrete random variable that can assume a
countable number of values. Under certain conditions, this distribution can
be completely determined by its moments. By definition, the moments of a
distribution are a set of descriptive constants that are useful for measuring its
properties.
Let
X
be
a
discrete
random
variable
that
takes
on
the
values
Ž .
x , x , . . . , x , . . . , with probabilities p n , nG0. Then, by definition, the kth
0
1
n
central moment of X, denoted by  , is
k

k
k
 sE
Xy
s
x y
p n ,
ks1, 2, . . . ,
Ž
.
Ž
.
Ž .
Ý
k
n
ns0
Ž
.

Ž .
2
where sE X sÝ
x p n is the mean of X. We note that  s	
is
ns0
n
2
the variance of X. The kth noncentral moment of X is given by the series


k
k
 sE X
s
x p n ,
ks1, 2, . . . .
5.55
Ž
.
Ž .
Ž
.
Ý
k
n
ns0



We note that  s. If, for some integer N, x
G1 for nN, and if the
1
n
Ž
.

 Ž
series in
5.55
converges absolutely, then so does the series for 
js
j
.
1, 2, . . . , ky1 . This follows from applying the comparison test:

 j

 k
x
p n F x
p n
if jk and nN.
Ž .
Ž .
n
n
Examples of discrete random variables with a countable number of values
Ž
.
include the Poisson see Section 4.5.3 and the negative binomial. The latter
random variable represents the number n of failures before the rth success
when independent trials are performed, each of which has two probability
outcomes, success or failure, with a constant probability p of success on each
trial. Its probability mass function is therefore of the form
n
nqry1
r
p n s
p
1yp
,
ns0, 1, 2, . . . .
Ž .
Ž
.
ž
/
n

APPLICATIONS IN STATISTICS
183
By contrast, the Poisson random variable has the probability mass function
eyn
p n s
,
ns0, 1, 2, . . . ,
Ž .
n!
where  is the mean of X. We can verify that  is the mean by writing

y n
e

s
n
Ý
n!
ns0

ny1

y
se
Ý
ny1 !
Ž
.
ns1

n

y
se
Ý n!
ns0
sey e ,
by Maclaurin’s expansion of e
Ž
.
s.
The second noncentral moment of the Poisson distribution is

y n
e


2
 s
n
Ý
2
n!
ns0

ny1

y
se

n
Ý
ny1 !
Ž
.
ns1

ny1

y
se

ny1q1
Ž
.
Ý
ny1 !
Ž
.
ns1
ny2
ny1




y
se
 
q
Ý
Ý
ny2 !
ny1 !
Ž
.
Ž
.
ns2
ns1
y w

x
se
 e qe
s2q.
In general, the kth noncentral moment of the Poisson distribution is given by
the series

y n
e


k
 s
n
,
ks1, 2, . . . ,
Ý
k
n!
ns0

INFINITE SEQUENCES AND SERIES
184
which converges for any k. This can be shown, for example, by the ratio test
k
a
nq1

nq1
lim
s lim ž
/
a
n
nq1
n™
n™
n
s01.
Thus all the noncentral moments of the Poisson distribution exist.
Similarly, for the negative binomial distribution we have

n
nqry1
r
s
n
p
1yp
Ž
.
Ý ž
/
n
ns0
r 1yp
Ž
.
s
why? ,
5.56
Ž
.
Ž
.
p

n
nqry1

2
r
 s
n
p
1yp
Ž
.
Ý
2
ž
/
n
ns0
r 1yp
1qryrp
Ž
. Ž
.
s
why?
5.57
Ž
.
Ž
.
2
p
and the kth noncentral moment,

n
nqry1

k
r
 s
n
p
1yp
,
ks1, 2, . . . ,
5.58
Ž
.
Ž
.
Ý
k
ž
/
n
ns0
exists for any k, since, by the ratio test,
k
nq1
nqr
ž
/
ž
/
a
nq1
n
nq1
lim
s lim
1yp
Ž
.
a
nqry1
n™
n™
n
ž
/
n
k
nq1
nqr
s 1yp
lim
Ž
.
ž
/
ž
/
n
nq1
n™
s1yp1,
Ž
.
which proves convergence of the series in 5.58 .
A very important inequality that concerns the mean  and variance 	 2 of
Ž
.
any random variable X not just the discrete ones is Chebyshev’s inequality,

APPLICATIONS IN STATISTICS
185
namely,
1


P
Xy Gr	
F
,
Ž
.
2r
or equivalently,
1


P
Xy r	
G1y
,
5.59
Ž
.
Ž
.
2r
Ž
where r is any positive number see, for example, Lindgren, 1976, Section
.
2.3.2 . The importance of this inequality stems from the fact that it is
independent of the exact distribution of X and connects the variance of X
Ž
.
with the distribution of its values. For example, inequality 5.59 states that at
Ž
2.
least
1y1rr
100% of the values of X fall within r	 from its mean,
2
'
where 	s 	
is the standard deviation of X.
Chebyshev’s inequality is a special case of a more general inequality called
Ž .
Markov’s inequality. If b is a nonzero constant and h x
is a nonnegative
function, then
1
2
P h X Gb
F
E h X
,
Ž
.
Ž
.
2
b
w Ž
.x
provided that E h X
exists. Chebyshev’s inequality follows from Markov’s
Ž
.
Ž
.2
inequality by choosing h X s Xy
.
Another important result that concerns the moments of a distribution is
given by the following theorem, regarding what is known as the Stieltjes
moment problem, which also applies to any random variable:

 Ž
.
Theorem 5.6.1.
Suppose that the moments 
ks1, 2, . . .
of a random
k
variable X exist, and the series


k
k

5.60
Ž
.
Ý k!
ks1
is absolutely convergent for some 0. Then these moments uniquely
Ž .
determine the cumulative distribution function F x of X.
Ž
.
Proof. See, for example, Fisz 1963, Theorem 3.2.1 .

In particular, if


 
k

FM ,
ks1, 2, . . . ,
k
Ž
.
for some constant M, then the series in 5.60 converges absolutely for any

Ž
k
.
k
0 by the comparison test. This is true because the series Ý
M rk! 
ks1
Ž
.
converges for example, by the ratio test for any value of .

INFINITE SEQUENCES AND SERIES
186
Ž
.
It should be noted that absolute convergence of the series in 5.60 is a
Ž .
sufficient condition for the unique determination of F x , but is not a
Ž
.
necessary condition. This is shown in Rao 1973, page 106 . Furthermore,
if some moments of X fail to exist, then the remaining moments that do exist
Ž .
cannot determine F x
uniquely. The following counterexample is given in
Ž
.
Fisz 1963, page 74 :
Let X be a discrete random variable that takes on the values x s2 nrn2,
n
Ž .
n
nG1, with probabilities p n s1r2 , nG1. Then

1
sE X s
,
Ž
.
Ý
2
n
ns1
which exists, because the series is convergent. However, 
 does not exist,
2
because

n
2

2
 sE X
s
Ž
.
Ý
2
4
n
ns1
and this series is divergent, since 2 nrn4™ as n™.
Now, let Y be another discrete random variable that takes on the value
1
nq1
2
zero with probability
and the values y s2
rn , nG1, with probabilities
n
2
Ž .
nq1
q n s1r2
, nG1. Then,

1
E Y s
sE X .
Ž
.
Ž
.
Ý
2
n
ns1
The second noncentral moment of Y does not exist, since

nq1
2

2
 sE Y
s
,
Ž
.
Ý
2
4
n
ns1
and this series is divergent.
Since 
does not exist for both X and Y, none of their noncentral
2
moments of order k2 exist either, as can be seen from applying the
comparison test. Thus X and Y have the same first noncentral moments, but
do not have noncentral moments of any order greater than 1. These two
random variables have obviously different distributions.
5.6.2. Moment and Probability Generating Functions
Let X be a discrete random variable that takes on the values x , x , x , . . .
0
1
2
Ž .
with probabilities p n , nG0.
The Moment Generating Function of X
This function is defined as

t X
t x n
 t sE e
s
e
p n
5.61
Ž .
Ž
.
Ž .
Ž
.
Ý
ns0

APPLICATIONS IN STATISTICS
187
provided that the series converges. In particular, if x sn for nG0, then
n

tn
 t s
e p n ,
5.62
Ž .
Ž .
Ž
.
Ý
ns0
which is a power series in et. If  is the radius of convergence for this series,
Ž .
then by Theorem 5.4.4,  t is a continuous function of t and has derivatives
of all orders inside its interval of convergence. Since
k
d  tŽ .

k
sE X
s ,
ks1, 2, . . . ,
5.63
Ž
.
Ž
.
k
k
dt
ts0
Ž .
 t , when it exists, can be used to obtain all noncentral moments of X,
which can completely determine the distribution of X by Theorem 5.6.1.
Ž
.
Ž .
From 5.63 , by using Maclaurin’s expansion of  t , we can obtain an
expression for this function as a power series in t:

nt
Žn.
 t s 0 q

0
Ž .
Ž .
Ž .
Ý n!
ns1


n
n
s1q
t .
5.64
Ž
.
Ý n!
ns1
Ž
.
Let us now go back to the series in 5.62 . If
p nq1
Ž
.
lim
sp,
p n
n™
Ž .
then by Theorem 5.4.2, the radius of convergence  is
1rp,
0p,
°~0,
ps,
s¢,
ps0.
w Ž .x1r n
Alternatively, if lim sup
p n
sq, then
n™
1rq,
0q,
°~0,
qs,
s¢,
qs0.

INFINITE SEQUENCES AND SERIES
188
For example, for the Poisson distribution, where
eyn
p n s
,
ns0, 1, 2, . . . ,
Ž .
n!
w Ž
.
Ž .x
w
Ž
.x
we have lim
p nq1 rp n slim
r nq1 s0. Hence, s, that
n™
n™
is,

y n
e

tn
 t s
e
Ž .
Ý
n!
ns0
converges uniformly for any value of t for which et, that is, yt.
Ž .
As a matter of fact, a closed-form expression for  t can be found, since
n
t

e
Ž
.
y
 t se
Ž .
Ý
n!
ns0
sey exp et
Ž
.
sexp ety
for all t.
5.65
Ž
.
Ž
.
The kth noncentral moment of X is then given by
k
k
t
d  t
d
e y
Ž .
Ž
.

 s
s
.
k
k
k
ts0
dt
dt
ts0
In particular, the first two noncentral moments are

 ss,
1

 sq2.
2
This confirms our earlier finding concerning these two moments.
Ž
.
It should be noted that formula 5.63 is valid provided that there exists a
Ž .
0 such that the neighborhood N 0 is contained inside the interval of

convergence. For example, let X have the probability mass function
6
p n s
,
ns1, 2, . . . .
Ž .
2
2
 n
Then
p nq1
Ž
.
lim
s1.
p n
n™
Ž .
Hence, by Theorem 5.4.4, the series

6
tn
 t s
e
Ž .
Ý
2
2
 n
ns1

APPLICATIONS IN STATISTICS
189
converges uniformly for values of t satisfying etFr, where rF1, or equiva-
lently, for tFlog rF0. If, however, t0, then the series diverges. Thus
Ž .
there does not exist a neighborhood N 0
that is contained inside the

Ž
.
interval of convergence for any 0. Consequently, formula 5.63 does not
hold in this case.
From the moment generating function we can derive a series of constants
that play a role similar to that of the moments. These constants are called
cumulants. They have properties that are, in certain circumstances, more
useful than those of the moments. Cumulants were originally defined and
Ž
.
studied by Thiele 1903 .
By definition, the cumulants of
X, denoted by  ,  , . . . ,  , . . . are
1
2
n
constants that satisfy the following identity in t:
 t 2
 t n
2
n
exp  tq
q  q
q 
1ž
/
2!
n!


2
n

2
n
s1q tq
t q  q
t q  .
5.66
Ž
.
1
2!
n!
Ž
.
Using formula 5.64 , this identity can be written as

n
nt slog  t ,
5.67
Ž .
Ž
.
Ý n!
ns1
Ž .
provided that  t exists and is positive. By definition, the natural logarithm
of the moment generating function of X is called the cumulant generating
function.
Ž
.
Formula 5.66 can be used to express the noncentral moments in terms of
Ž
.
the cumulants, and vice versa. Kendall and Stuart 1977, Section 3.14 give a
general relationship that can be used for this purpose. For example,
 s
 ,
1
1
 s
 y
2,
2
2
1
 s
 y3
 
 q2
3.
3
3
1
2
1
The cumulants have an interesting property in that they are, except for  ,
1
invariant to any constant shift c in X. That is, for ns2, 3, . . . , 
is not
n
changed if X is replaced by Xqc. This follows from noting that
w
Ž Xqc .tx
ct
E e
se  t ,
Ž .
which is the moment generating function of Xqc. But
ct
log e  t
sctqlog  t .
Ž .
Ž .

INFINITE SEQUENCES AND SERIES
190
Ž
.
By comparison with
5.67
we can then conclude that except for  , the
1
cumulants of Xqc are the same as those of X. This contrasts sharply with
the noncentral moments of X, which are not invariant to such a shift.
Another advantage of using cumulants is that they can be employed to
obtain approximate expressions for the percentile points of the distribution
Ž
.
of X see Section 9.5.1 .
EXAMPLE 5.6.1.
Let X be a Poisson random variable whose moment
Ž
.
Ž
.
generating function is given by formula 5.65 . By applying 5.67 we get

n
n
t
t slog exp e y
Ž
.
Ý n!
ns1
sety

nt
s
.
Ý n!
ns1
Here, we have made use of Maclaurin’s expansion of et. This series converges
for any value of t. It follows that  s for ns1, 2, . . . .
n
The Probability Generating Function
This is similar to the moment generating function. It is defined as

X
x n
 t sE t
s
t
p n .
Ž .
Ž
.
Ž .
Ý
ns0
In particular, if x sn for nG0, then
n

n
 t s
t p n ,
5.68
Ž .
Ž .
Ž
.
Ý
ns0
which is a power series in t. Within its interval of convergence, this series
represents a continuous function with derivatives of all orders. We note that
Ž .
Ž .
 0 sp 0 and that
k
1 d  tŽ .
sp k ,
ks1, 2, . . . .
5.69
Ž .
Ž
.
k
k!
dt
ts0
Thus, the entire probability distribution of X is completely determined by
Ž .
 t .
The probability generating function is also useful in determining the

APPLICATIONS IN STATISTICS
191
moments of X. This is accomplished by using the relation
k

d  tŽ .
s
n ny1  nykq1 p n
Ž
.
Ž
. Ž .
Ý
k
dt
ts1
nsk
sE X Xy1 
Xykq1
.
5.70
Ž
.
Ž
.
Ž
.
Ž
.
The quantity on the right-hand side of
5.70
is called the kth factorial
moment of X, which we denote by  . The noncentral moments of X can be
k
derived from the  ’s. For example,
k

 s ,
1
1

 s q ,
2
2
1

 s q3 q ,
3
3
2
1

 s q6 q7 q .
4
4
3
2
1
Ž
.
Obviously, formula 5.70 is valid provided that ts1 belongs to the interval
Ž
.
of convergence of the series in 5.68 .
If a closed-form expression is available for the moment generating func-
Ž .
tion, then a corresponding expression can be obtained for  t by replacing
t
Ž
.
e
with t. For example, from formula
5.65 , the probability generating
Ž .
Ž
.
function for the Poisson distribution is given by  t sexp ty .
5.6.3. Some Limit Theorems
In Section 3.7 we defined convergence in probability of a sequence of
random variables. In Section 4.5.1 convergence in distribution of the same
sequence was introduced. In this section we introduce yet another type of
convergence.

4
Definition 5.6.1.
A sequence
X
of random variables converges in
n ns1
quadratic mean to a random variable X if
2
lim E X yX
s0.
Ž
.
n
n™
q.m.
6
This convergence is written symbolically as X
X.

n
Convergence in quadratic mean implies convergence in probability. This
q.m.
6
follows directly from applying Markov’s inequality: If X
X, then for any
n
0,
1
2


P
X yX G F
E X yX
™0
Ž
.
Ž
.
n
n
2


4
as n™. This shows that the sequence
X
converges in probability
n ns1
to X.

INFINITE SEQUENCES AND SERIES
192
(
)
5.6.3.1. The Weak Law of Large Numbers Khinchine’s Theorem

4
Let
X
be a sequence of independent and identically distributed random
i is1
variables with a finite mean . Then X
converges in probability to  as
n
n
Ž
.
n™, where X s 1rn Ý
X is the sample mean of a sample of size n.
n
is1
i
Ž
.
Ž
Proof. See, for example, Lindgren
1976, Section 2.5.1
or Rao
1973,
.
Section 2c.3 .


4
Definition 5.6.2.
A sequence
X
of random variables converges
n ns1
strongly, or almost surely, to a random variable X, written symbolically as
a.s.
6
X
X, if for any 0,
n


lim P
sup X yX  s0.

n
ž
/
N™
nGN

4
Theorem 5.6.2.
Let
X
be a sequence of random variables. Then we
n ns1
have the following:
a.s.
6
1. If X
c, where c is constant, then X
converges in probability to c.
n
n
q.m.
a.s.

2
6
6
Ž
.
2. If X
c, and the series Ý
E X yc
converges, then X
c.
n
ns1
n
n
(
)
5.6.3.2. The Strong Law of Large Numbers Kolmogoro©’s Theorem

4
Let
X
be a sequence of independent random variables such that
n ns1
Ž
.
Ž
.
2

2
2
E X
s
and Var X
s	 , ns1, 2, . . . . If the series Ý
	 rn
con-
n
n
n
n
ns1
n
a.s.
n
6
Ž
.
verges, then X
 , where  s 1rn Ý
 .
n
n
n
is1
i
Ž
.
Proof. See Rao 1973, Section 2c.3 .

5.6.3.3. The Continuity Theorem for Probability Generating Functions
Ž
.
See Feller 1968, page 280 .

Ž .4
Suppose that for every kG1, the sequence
p
n
represents a dis-
k
ns0
Ž .

n
Ž .
crete probability distribution. Let 
t sÝ
t p
n be the corresponding
k
ns0
k
Ž
.
probability generating function ks1, 2, . . . . In order for a limit
q s lim p
n
Ž .
n
k
k™
to exist for every ns0, 1, . . . , it is necessary and sufficient that the limit
 t s lim 
t
Ž .
Ž .
k
k™

APPLICATIONS IN STATISTICS
193
Ž
.
exist for every t in the open interval 0, 1 . In this case,

n
 t s
t q .
Ž .
Ý
n
ns0
This theorem implies that a sequence of discrete probability distributions
converges if and only if the corresponding probability generating functions
converge. It is important here to point out that the q ’s may not form a
n
Ž
.
discrete probability distribution
because they may not sum to 1 . The
Ž .
function  t may not therefore be a probability generating function.
5.6.4. Power Series and Logarithmic Series Distributions
Ž
.
The power series distribution, which was introduced by Kosambi
1949 ,
represents a family of discrete distributions, such as the binomial, Poisson,
and negative binomial. Its probability mass function is given by
a  n
n
p n s
,
ns0, 1,2, . . . ,
Ž .
f 
Ž .
Ž .
where a G0, 0, and f 
is the function
n

n
f  s
a  .
5.71
Ž .
Ž
.
Ý
n
ns0
This function is defined provided that  falls inside the interval of conver-
Ž
.
gence of the series in 5.71 .
For example, for the Poisson distribution, s, where  is the mean,
Ž .

a s1rn! for ns0, 1, 2, . . . , and f  se . For the negative binomial, s
n
nq ry 1
1yp and
a s
, ns0, 1, 2, . . . ,
where nsnumber of failures,
n ž
/
n
rsnumber of successes, and psprobability of success on each trial, and
thus

1
n
nqry1
f  s
1yp
s
.
Ž .
Ž
.
Ý
r
ž
/
n
p
ns0
A special case of the power series distribution is the logarithmic series
Ž
.
distribution. It was first introduced by Fisher, Corbet, and Williams 1943
while studying abundance and diversity for insect trap data. The probability
mass function for this distribution is
 n
p n sy
,
ns1, 2, . . . ,
Ž .
n log 1y
Ž
.
where 01.

INFINITE SEQUENCES AND SERIES
194
The logarithmic series distribution is useful in the analysis of various kinds
of data. A description of some of its applications can be found, for example,
Ž
.
in Johnson and Kotz 1969, Chapter 7 .
5.6.5. Poisson Approximation to Power Series Distributions
Ž
.
See Perez-Abreu 1991 .
´
The Poisson distribution can provide an approximation to the distribution
of the sum of random variables having power series distributions. This is
based on the following theorem:
Theorem 5.6.3.
For each kG1, let X , X , . . . , X
be independent non-
1
2
k
negative integer-valued random variables with a common power series distri-
bution
p
n sa  nrf 
,
ns0, 1, . . . ,
Ž .
Ž
.
k
n
k
k
Ž
.
where a G0 ns0, 1, . . .
are independent of k and
n

n
f 
s
a  ,
 0.
Ž
.
Ý
k
n
k
k
ns0
Let a 0, 0 be fixed and S sÝk
X . If k ™ as k™, then
0
k
is1
i
k
lim P S sn sey 0nrn!,
ns0, 1, . . . ,
Ž
.
k
0
k™
where  sa ra .
0
1
0
Ž
.
Proof. See Perez-Abreu 1991, page 43 .

´
By using this theorem we can obtain the well-known Poisson approxima-
tion to the binomial and the negative binomial distributions as shown below.
Ž
.
EXAMPLE
5.6.2
The
Binomial
Distribution .
For
each
k G 1,
let
X , . . . , X
be a sequence of independent Bernoulli random variables with
1
k
success probability p . Let S sÝk
X . Suppose that kp ™0 as k™.
k
k
is1
i
k
Then, for each ns0, 1, . . . ,
k
kyn
n
lim P S sn s lim
p
1yp
Ž
.
Ž
.
k
k
k
ž /
n
k™
k™
seynrn!.
This follows from the fact that the Bernoulli distribution with success
Ž
.
Ž
.
probability p is a power series distribution with  sp r 1yp
and f 
s
k
k
k
k
k

APPLICATIONS IN STATISTICS
195
1q . Since a sa s1, and k ™ as k™, we get from applying
k
0
1
k
Theorem 5.6.3 that
lim P S sn seynrn!.
Ž
.
k
k™
Ž
.
EXAMPLE 5.6.3 The Negative Binomial Distribution .
We recall that a
random variable Y has the negative binomial distribution if it represents the
Ž
.
Ž
.
number of failures n in repeated trials before the kth success
kG1 . Let
p
denote the probability of success on a single trial. Let X , X , . . . , X
be
k
1
2
k
random variables defined as
X snumber of failures occurring before the 1st success,
1
X snumber of failures occurring between the 1st success
2
and the 2nd success,
...
Ž
.
X snumber of failures occurring between the ky1 st
k
success and the kth success.
Such random variables have what is known as the geometric distribution. It is
a special case of the negative binomial distribution with ks1. The common
probability distribution of the X ’s is
i
n
P X sn sp
1yp
,
ns0, 1 . . . ;
is1, 2, . . . , k.
Ž
.
Ž
.
i
k
k
Ž
.
This is a power series distribution with a s1 ns0, 1, . . . ,  s1yp , and
n
k
k

1
1
n
f 
s
1yp
s
s
.
Ž
.
Ž
.
Ý
k
k
1y 1yp
p
Ž
.
k
k
ns0
It is easy to see that X , X , . . . , X
are independent and that YsS s
1
2
k
k
Ýk
X .
is1
i
Ž
.
Let us now assume that k 1yp
™0 as k™. Then from Theorem
k
5.6.3 we obtain the following result:
lim P S sn seynrn!,
ns0, 1. . . . .
Ž
.
k
k™
5.6.6. A Ridge Regression Application
Consider the linear model
ysXq ,

INFINITE SEQUENCES AND SERIES
196
where y is a vector of n response values, X is an np matrix of rank p,  is
a vector of p unknown parameters, and  is a random error vector such that
Ž .
Ž .
2
E  s0 and Var  s	 I . All variables in this model are corrected for
n
their means and scaled to unit length, so that XX and Xy are in correlation
form.
We recall from Section 2.4.2 that if the columns of X are multicollinear,
ˆ
y1
Ž
.
then the least-squares estimator of , namely s XX
Xy, is an unreli-
able estimator due to large variances associated with its elements. There are
several methods that can be used to combat multicollinearity. A review of
Ž
.
such methods can be found in Ofir and Khuri 1986 . Ridge regression is one
of the most popular of these methods. This method, which was developed by
Ž
.
Hoerl and Kennard 1970a, b , is based on adding a positive constant k to the
diagonal elements of XX. This leads to a biased estimator * of  called the
ridge regression estimator and is given by
y1
*s XXqkI
Xy.
Ž
.
n
The elements of * can have substantially smaller variances than the corre-
ˆ Ž
sponding elements of 
see, for example, Montgomery and Peck, 1982,
.
Section 8.5.3 .
Ž
.
Draper and Herzberg
1987
showed that the ridge regression residual
sum of squares can be represented as a power series in k. More specifically,
consider the vector of predicted responses,
y sX*
ˆk
y1
sX XXqkI
Xy,
5.72
Ž
.
Ž
.
n
Ž
.
which is based on using *. Formula 5.72 can be written as
y1
y1
y1
y sX XX
I qk XX
Xy.
5.73
Ž
.
Ž
.
Ž
.
ˆk
n
Ž
.y1
From Theorem 5.5.2, if all the eigenvalues of k XX
are less than one in
absolute value, then

y1
y1
i
yi
i
I qk XX
s
y1
k
XX
.
5.74
Ž
.
Ž
.
Ž
.
Ž
.
Ý
n
is0
Ž
.
Ž
.
From 5.73 and 5.74 we get
y s H ykH qk 2H yk 3H q  y,
ˆ
Ž
.
k
1
2
3
4
Ž
.yi
where H sX XX
X, iG1. Thus the ridge regression residual sum of
i
squares, which is the sum of squares of deviations of the elements of y from
the corresponding elements of y , is
ˆk
yyy
 yyy
syQy,
Ž
. Ž
.
ˆ
ˆ
k
k

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
197
where
2
2
3
Qs I yH qkH yk H qk H y 
.
Ž
.
n
1
2
3
4
Ž
.
It can be shown see Exercise 5.32 that

iy1
yQysSS q
iy2
yk
S ,
5.75
Ž
. Ž
.
Ž
.
Ý
E
i
is3
where SS
is the usual least-squares residual sum of squares, which can be
E
obtained when ks0, that is,
y1
SS sy I yX XX
X y,
Ž
.
E
n
Ž
.
and S syH y, iG3. The terms to the right of 5.75 , other than SS , are
i
i
E
bias sums of squares induced by the presence of a nonzero k. Draper and
Ž
.
Herzberg
1987
demonstrated by means of an example that the series in
Ž
.
5.75 may diverge or else converge very slowly, depending on the value of k.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Apostol, T. M.
1964 .
Mathematical Analysis. Addison-Wesley, Reading, Mas-
Ž
.
sachusetts. Infinite series are discussed in Chap. 12.
Ž
.
Boyer, C. B. 1968 . A History of Mathematics. Wiley, New York.
Ž
.
Draper, N. R., and A. M. Herzberg
1987 . ‘‘A ridge-regression sidelight.’’ Amer.
Statist., 41, 282283.
Ž
.
Draper, N. R., and H. Smith 1981 . Applied Regression Analysis, 2nd ed. Wiley, New
Ž
York. Chap. 6 discusses ridge regression in addition to the various statistical
.
procedures for selecting variables in a regression model.
Ž
.
Feller, W. 1968 . An Introduction to Probability Theory and Its Applications, Vol. I, 3rd
ed. Wiley, New York.
Ž
.
Fisher, R. A., and E. A. Cornish 1960 . ‘‘The percentile points of distribution having
known cumulants.’’ Technometrics, 2, 209225.
Ž
.
Fisher, R. A., A. S. Corbet, and C. B. Williams 1943 . ‘‘The relation between the
number of species and the number of individuals in a random sample of an
animal population.’’ J. Anim. Ecology, 12, 4258.
Ž
.
Fisz, M.
1963 . Probability Theory and Mathematical Statistics, 3rd ed. Wiley, New
Ž
York.
Chap. 5 deals almost exclusively with limit distributions for sums of
.
independent random variables.
Ž
.
Ž
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York. Chap. 2 discusses
limits of sequences; Chap. 13 deals with infinite series of constant terms; Chap.
14 deals with sequences and series of functions; Chap. 15 provides a study of
.
power series.
Ž
.
Gantmacher, F. R. 1959 . The Theory of Matrices, Vol. I. Chelsea, New York.

INFINITE SEQUENCES AND SERIES
198
Ž
.
Graybill, F. A.
1983 . Matrices with Applications in Statistics, 2nd ed. Wadsworth,
Ž
Belmont, California.
Chap. 5 includes a section on sequences and series of
.
matrices.
Ž
.
Hirschman, I. I., Jr. 1962 . Infinite Series. Holt, Rinehart and Winston, New York.
ŽThis book is designed to be used in applied courses beyond the advanced
.
calculus level. It emphasizes applications of the theory of infinite series.
Ž
.
Hoerl, A. E., and R. W. Kennard 1970a . ‘‘Ridge regression: Biased estimation for
non-orthogonal problems.’’ Technometrics, 12, 5567.
Ž
.
Hoerl, A. E., and R. W. Kennard
1970b . ‘‘Ridge regression: Applications to
non-orthogonal problems.’’ Technometrics, 12, 6982; Correction. 12, 723.
Ž
.
Hogg, R. V., and A. T. Craig 1965 . Introduction to Mathematical Statistics, 2nd ed.
Macmillan, New York.
Ž
.
Ž
Hyslop, J. M. 1954 . Infinite Series, 5th ed. Oliver and Boyd, Edinburgh. This book
presents a concise treatment of the theory of infinite series. It provides the basic
.
elements of this theory in a clear and easy-to-follow manner.
Ž
.
Johnson, N. L., and S. Kotz 1969 . Discrete Distributions. Houghton Mifflin, Boston.
ŽChaps. 1 and 2 contain discussions concerning moments, cumulants, generating
.
functions, and power series distributions.
Ž
.
Kendall, M., and A. Stuart 1977 . The Ad®anced Theory of Statistics, Vol. 1, 4th ed.
Ž
Macmillan, New York. Moments, cumulants, and moment generating functions
.
are discussed in Chap. 3.
Ž
.
Knopp, K. 1951 . Theory and Application of Infinite Series. Blackie and Son, London.
ŽThis reference book provides a detailed and comprehensive study of the theory
.
of infinite series. It contains many interesting examples.
Ž
.
Kosambi, D. D. 1949 . ‘‘Characteristic properties of series distributions.’’ Proc. Nat.
Inst. Sci. India, 15, 109113.
Ž
.
Ž
Lancaster, P.
1969 . Theory of Matrices. Academic Press, New York.
Chap. 5
discusses functions of matrices in addition to sequences and series involving
.
matrix terms.
Ž
.
Ž
Lindgren, B. W. 1976 . Statistical Theory, 3rd ed. Macmillan, New York. Chap. 2
contains a section on moments of a distribution and a proof of Markov’s
.
inequality.
Ž
.
Montgomery, D. C., and E. A. Peck 1982 . Introduction to Linear Regression Analysis.
Ž
Wiley, New York.
Chap. 8 discusses the effect of multicollinearity and the
.
methods for dealing with it including ridge regression.
Ž
.
Nurcombe, J. R. 1979 . ‘‘A sequence of convergence tests.’’ Amer. Math. Monthly, 86,
679681.
Ž
.
Ofir, C., and A. I. Khuri 1986 . ‘‘Multicollinearity in marketing models: Diagnostics
Ž
and remedial measures.’’ Internat. J. Res. Market., 3, 181205. This is a review
article that surveys the problem of multicollinearity in linear models and the
.
various remedial measures for dealing with it.
Ž
.
Perez-Abreu, V. 1991 . ‘‘Poisson approximation to power series distributions.’’ Amer.
´
Statist., 45, 4245.
Ž
.
Pye, W. C., and P. G. Webster
1989 . ‘‘A note on Raabe’s test extended.’’ Math.
Comput. Ed., 23, 125128.

EXERCISES
199
Ž
.
Rao, C. R. 1973 . Linear Statistical Inference and Its Applications, 2nd ed. Wiley, New
Ž
York. Chap. 2 contains a section on limit theorems in statistics, including the
.
weak and strong laws of large numbers.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis, 2nd ed. McGraw-Hill, New
Ž
York.
Sequences and series of scalar constants are discussed in Chap. 3;
.
sequences and series of functions are studied in Chap. 7.
Ž
.
Thiele, T. N.
1903 . Theory of Obser®ations. Layton, London. Reprinted in Ann.
Ž
.
Math. Statist., 2, 165307 1931 .
Ž
.
Ž
Wilks, S. S.
1962 . Mathematical Statistics. Wiley, New York.
Chap. 4 discusses
different types of convergence of random variables; Chap. 5 presents several
.
results concerning the moments of a distribution.
Ž
.
Withers, C. S.
1984 . ‘‘Asymptotic expansions for distributions and quantiles with
power series cumulants.’’ J. Roy. Statist. Soc. Ser. B, 46, 389396.
EXERCISES
In Mathematics

4
5.1. Suppose that a
is a bounded sequence of positive terms.
n ns1
( )

4
a
Define b smax a , a , . . . , a , ns1, 2, . . . . Show that the se-
n
1
2
n

4
quence b
converges, and identify its limit.
n ns1
( )
b
Suppose further that a ™c as n™, where c0. Show that
n

4
c ™c, where
c
is the sequence of geometric means, c s
n
n ns1
n
Ž
n
.1r n
Ł
a
.
is1
i

4

4
5.2. Suppose that
a
and
b
are any two Cauchy sequences. Let
n ns1
n ns1



4
d s a yb , ns1, 2, . . . . Show that the sequence d
converges.
n
n
n
n ns1
5.3. Prove Theorem 5.1.3.

4
5.4. Show that if
a
is a bounded sequence, then the set E of all its
n ns1
subsequential limits is also bounded.

4
5.5. Suppose that a ™c as n™ and that a
is a sequence of positive
n
i is1
terms for which Ýn
 ™ as n™.
is1
i
( )
a
Show that
Ýn
 a
is1
i
i ™c
as n™.
n
Ý

is1
i
In particular, if  s1 for all i, then
i
n
1
a ™c
as n™.
Ý
i
n is1
( )
Ž .
b
Show that the converse of the special case in a does not always

INFINITE SEQUENCES AND SERIES
200

4
hold by giving a counterexample of a sequence
a
that does
n ns1
Ž
n
.
not converge, yet Ý
a rn converges as n™.
is1
i

4
5.6. Let a
be a sequence of positive terms such that
n ns1
anq1 ™b
as n™,
an
where 0b1. Show that there exist constants c and r such that
0r1 and c0 for which a cr n for sufficiently large values of n.
n

4
5.7. Suppose that we have the sequence a
, where a s1 and
n ns1
1
a
3bqa2
Ž
.
n
n
a
s
,
b0,
ns1, 2, . . . .
nq1
2
3a qb
n
Show that the sequence converges, and find its limit.

4
5.8. Show that the sequence
a
converges, and find its limit, where
n ns1
a s1 and
1
1r2
a
s 2qa
,
ns1, 2, . . . .
Ž
.
nq1
n

4
n
5.9. Let a
be a sequence and s sÝ
a .
n ns1
n
is1
i
( )
Ž
.
a
Show that lim sup
s rn Flim sup
a .
n™
n
n™
n
( )
b
If s rn converges as n™, then show that a rn™0 as n™.
n
n

4
n
Ž
.
5.10. Show that the sequence a
, where a sÝ
1ri , is not a Cauchy
n ns1
n
is1
sequence and is therefore divergent.

4
5.11. Suppose that the sequence
a
satisfies the following condition:
n ns1
There is an r, 0r1, such that


n
a
ya
br ,
ns1, 2, . . . ,
nq1
n
where b is a positive constant. Show that this sequence converges.
5.12. Show that if a G0 for all n, then Ý
a
converges if and only if
n
ns1
n

4
s
is a bounded sequence, where s
is the nth partial sum of the
n ns1
n
series.
1

w
Ž
.Ž
.x
5.13. Show that the series Ý
1r 3ny1 3nq2
converges to
.
ns1
6

Ž
1r n
. p
5.14. Show that the series Ý
n
y1
is divergent for pF1.
ns1
5.15. Let Ý
a be a divergent series of positive terms.
ns1
n
( )

4

w
Ž
.x
a
If a
is a bounded sequence, then show that Ý
a r 1qa
n ns1
ns1
n
n
diverges.

EXERCISES
201
( )
Ž .

4
b
Show that a is true even if a
is not a bounded sequence.
n ns1
5.16. Let Ý
a be a divergent series of positive terms. Show that
ns1
n
a
1
1
n F
y
,
ns2, 3, . . . ,
2
s
s
s
ny1
n
n
where s
is the nth partial sum of the series; then deduce that
n

Ž
2.
Ý
a rs
converges.
ns1
n
n
5.17. Let Ý
a
be a convergent series of positive terms. Let r sÝ
a .
ns1
n
n
isn
i
Show that for mn,
n
a
r
i
n
1y
,
Ý r
r
i
m
ism

Ž
.
and deduce that Ý
a rr
diverges.
ns1
n
n

Ž
.

Ž
2.
5.18. Given the two series Ý
1rn , Ý
1rn . Show that
ns1
ns1
1rn
1
lim
s1,
ž /
n
n™
1rn
1
lim
s1.
2ž /
n
n™
5.19. Test for convergence of the series Ý
a , where
ns1
n
n
1r n
a
a s n
y1
,
Ž .
Ž
.
n
log 1qn
Ž
.
b
a s
,
Ž .
2
n
n
log 1qe
Ž
.
135   2ny1
1
Ž
.
c
a s

,
Ž .
n
246  2n
2nq1
'
'
'
d
a s
nq n y n ,
Ž .
n
n
n
y1
4
Ž
.
e
a s
,
Ž .
n
n
n
1
f
a ssin
nq
 .
Ž .
n
ž
/
n

INFINITE SEQUENCES AND SERIES
202
5.20. Determine the values of x for which each of the following series
converges uniformly:

nq2
2 n
a
x
,
Ž .
Ý
n
3
ns1

n
10
n
b
x ,
Ž .
Ý
n
ns1

2
n
c
nq1
x ,
Ž .
Ž
.
Ý
ns1

cos nx
Ž
.
d
.
Ž .
Ý
2
n n q1
Ž
.
ns1
5.21. Consider the series
ny1


y1
Ž
.
a s
.
Ý
Ý
n
n
ns1
ns1
Let Ý
b be a certain rearrangement of Ý
a
given by
ns1
n
ns1
n
1
1
1
1
1
1
1
1
1q y q q y q q
y q  ,
3
2
5
7
4
9
11
6
where two positive terms are followed by one negative. Show that the
10
sum of the original series is less than
, whereas that of the rearranged
12
11
Ž
.
series which is convergent exceeds
.
12
5.22. Consider Cauchy’s product of Ý
a
with itself, where
ns0
n
n
y1
Ž
.
a s
,
ns0, 1, 2, . . . .
n
'nq1
w
Show that this product is divergent.
Hint: Show that the nth term of
x
this product does not go to zero as n™.

Ž .4
5.23. Consider the sequence of functions
f
x
, where for ns1, 2, . . .
n
ns1
nx
f
x s
,
xG0.
Ž .
n
2
1qnx
Find the limit of this sequence, and determine whether or not the
w
.
convergence is uniform on 0,  .

EXERCISES
203

Ž
x.
5.24. Consider the series Ý
1rn .
ns1
( )
w
.
a
Show that this series converges uniformly on 1q,  , where  is
w
any positive number. Note: The function represented by this series
Ž . x
is known as Riemann’s -function and is denoted by  x .
( )
Ž .
w
.
b
Is  x
differentiable on 1q,  ? If so, give a series expansion
Ž .
for   x .
In Statistics
Ž
.
Ž
.
5.25. Prove formulas 5.56 and 5.57 .
5.26. Find a series expansion for the moment generating function of the
negative binomial distribution. For what values of t does this series
Ž
.
converge uniformly? In this case, can formula
5.63
be applied to
Ž
.
obtain an expression for the kth noncentral moment
ks1, 2, . . .
of
this distribution? Why or why not?
5.27. Find the first three cumulants of the negative binomial distribution.

Ž
.
5.28. Show that the moments 
ns1, 2, . . .
of a random variable X
n
determine the cumulative distribution functions of X uniquely if


  1r n
n
lim sup
is finite.
n
n™
nq1r2
yn
'
w
x
Hint: Use the fact that n! 2 n
e
as n™.
5.29. Find the moment generating function of the logarithmic series distribu-
tion, and deduce that the mean and variance of this distribution are
given by
sr 1y ,
Ž
.
1
2
	 s
y ,
ž
/
1y
Ž
.
where sy1rlog 1y .

4
5.30. Let
X
be a sequence of binomial random variables where the
n ns1
Ž
.
probability mass function of X
ns1, 2, . . .
is given by
n
n
nyk
k
p
k s
p
1yp
,
ks0, 1,2, . . . , n,
Ž .
Ž
.
n
ž /
k

INFINITE SEQUENCES AND SERIES
204
where 0p1. Further, let the random variable Y
be defined as
n
Xn
Y s
yp.
n
n
( )
Ž
.
Ž
.
Ž
.
a
Show that E X
snp and Var X
snp 1yp .
n
n
( )
b
Apply Chebyshev’s inequality to show that
p 1yp
Ž
.


P
Y
G F
,
Ž
.
n
2
n
where 0.
( )
Ž .
w
c
Deduce from b that Y
converges in probability to zero.
Note:
n
x
This result is known as Bernoulli’s law of large numbers.
5.31. Let X , X , . . . , X
be a sequence of independent Bernoulli random
1
2
n
variables with success probability p . Let S sÝn
X . Suppose that
n
n
is1
i
np ™0 as n™.
n
( )
Ž .
a
Give an expression for 
t , the moment generating function of
n
S .
n
( )
b
Show that
lim 
t sexp ety ,
Ž .
Ž
.
n
n™
which is the moment generating function of a Poisson distribution
with mean .
Ž
.
5.32. Prove formula 5.75 .

C H A P T E R
6
Integration
The origin of integral calculus can be traced back to the ancient Greeks.
They were motivated by the need to measure the length of a curve, the area
of a surface, or the volume of a solid. Archimedes used techniques very
similar to actual integration to determine the length of a segment of a curve.
Ž
.
Democritus 410 B.C. had the insight to consider that a cone was made up of
infinitely many plane cross sections parallel to the base.
The theory of integration received very little stimulus after Archimedes’s
remarkable achievements. It was not until the beginning of the seventeenth
century that the interest in Archimedes’s ideas began to develop. Johann
Ž
.
Kepler
15711630
was the first among European mathematicians to de-
velop the ideas of infinitesimals in connection with integration. The use of
the term ‘‘integral’’ is due to the Swiss mathematician Johann Bernoulli
Ž
.
16671748 .
In the present chapter we shall study integration of real-valued functions
of a single variable x according to the concepts put forth by the German
Ž
.
mathematician Georg Friedrich Riemann 18261866 . He was the first to
establish a rigorous analytical foundation for integration, based on the older
geometric approach.
6.1. SOME BASIC DEFINITIONS
Ž .
w
x
Let f x
be a function defined and bounded on a finite interval
a, b .
Suppose that this interval is partitioned into a finite number of subintervals

4
by a set of points Ps x , x , . . . , x
such that asx x x   x sb.
0
1
n
0
1
2
n
w
x
Ž
.
This set is called a partition of a, b . Let  x sx yx
is1, 2, . . . , n , and
i
i
iy1
 
be the largest of  x ,  x , . . . ,  x . This value is called the norm of P.
p
1
2
n
Consider the sum
n
S P, f s
f t
 x ,
Ž
.
Ž .
Ý
i
i
is1
w
x
where t is a point in the subinterval
x
, x , is1, 2, . . . , n.
i
iy1
i
205

INTEGRATION
206
Ž .
w
x
The function f x is said to be Riemann integrable on a, b if a number
A exists with the following property: For any given 0 there exists a
number 0 such that
AyS P, f

Ž
.
w
x
for any partition P of a, b with a norm  , and for any choice of the
p
w
x
point t in
x
, x , is1, 2, . . . , n. The number A is called the Riemann
i
iy1
i
Ž .
w
x
b Ž .
integral of f x on a, b and is denoted by H f x dx. The integration symbol
a
H was first used by the German mathematician Gottfried Wilhelm Leibniz
Ž
.
Ž
16461716 to represent a sum it was derived from the first letter of the
.
Latin word summa, which means a sum .
6.2. THE EXISTENCE OF THE RIEMANN INTEGRAL
In order to investigate the existence of the Riemann integral, we shall need
the following theorem:
Ž .
Theorem 6.2.1.
Let f x
be a bounded function on a finite interval,
w
x

4
w
x
a, b . For every partition Ps x , x , . . . , x
of
a, b , let m and M
be,
0
1
n
i
i
Ž .
w
x
respectively, the infimum and supremum of f x on
x
, x , is1, 2, . . . , n.
iy1
i
If, for a given 0, there exists a 0 such that
US
f yLS
f 
6.1
Ž .
Ž .
Ž
.
P
P
whenever  , where  
is the norm of P, and
p
p
n
LS
f s
m  x ,
Ž .
Ý
P
i
i
is1
n
US
f s
M  x ,
Ž .
Ý
P
i
i
is1
Ž .
w
x
Ž .
then f x
is Riemann integrable on
a, b . Conversely, if f x
is Riemann
Ž
.
integrable, then inequality 6.1 holds for any partition P such that  .
p
w
Ž .
Ž .
The sums, LS
f
and US
f , are called the lower sum and upper sum,
P
P
Ž .
x
respectively, of f x with respect to the partition P.
In order to prove Theorem 6.2.1 we need the following lemmas:
w
x
Lemma 6.2.1.
Let P and P be two partitions of a, b such that P>P
ŽP is called a refinement of P and is constructed by adding partition points
.
between those that belong to P . Then
US
f FUS
f ,
Ž .
Ž .
P
P
LS
f GLS
f .
Ž .
Ž .
P
P

THE EXISTENCE OF THE RIEMANN INTEGRAL
207

4
Proof. Let Ps x , x , . . . , x . By the nature of the partition P, the ith
0
1
n
subinterval  x sx yx
is divided into k
parts  Ž1.,  Ž2., . . . ,  Žk i., where
i
i
iy1
i
x
x
x
i
i
i
k G1, is1, 2, . . . , n. If mŽ j. and M Ž j. denote, respectively, the infimum and
i
i
i
Ž .
Ž j.
Ž j.
Ž j.
supremum of f x
on  , then m Fm
FM
FM
for js1, 2, . . . , k ;
x
i
i
i
i
i
i
Ž .
is1, 2, . . . , n, where m and M are the infimum and supremum of f x on
i
i
w
x
x
, x , respectively. It follows that
iy1
i
k
n
n
i
Ž j.
Ž j.
LS
f s
m  x F
m  
sLS
f
Ž .
Ž .
Ý
Ý Ý
P
i
i
i
x
P
i
is1
is1 js1
k
n
n
i
Ž j.
Ž j.
US
f s
M
 
F
M  x sUS
f .

Ž .
Ž .
Ý Ý
Ý
P
i
x
i
i
P
i
is1 js1
is1
w
x
Lemma 6.2.2.
Let P and
P be any two partitions of
a, b . Then
Ž .
Ž .
LS
f FUS
f .
P
P
Proof. Let P nsPjP. The partition P is a refinement of both P and
P. Then, by Lemma 6.2.1,
LS
f FLS
f FUS
f FUS
f .

Ž .
Ž .
Ž .
Ž .
P
P
P
P
Proof of Theorem 6.2.1
Ž
.
Let 0 be given. Suppose that inequality 6.1 holds for any partition P
Ž
.
n
Ž .
whose norm  
is less than . Let S P, f sÝ
f t
 x , where t
is a
p
is1
i
i
i
w
x
Ž .
Ž .
point in
x
, x , is1, 2, . . . , n. By the definition of LS
f
and US
f
we
iy1
i
P
P
can write
LS
f FS P, f FUS
f .
6.2
Ž .
Ž
.
Ž .
Ž
.
P
P
Ž .
w
x
Let m and M be the infimum and supremum, respectively, of f x on a, b ;
then
m bya FLS
f FUS
f FM bya .
6.3
Ž
.
Ž .
Ž .
Ž
.
Ž
.
P
P
Ž .
Let us consider two sets of lower and upper sums of f x
with respect to
partitions P, P, P, . . . such that P;P;P ;  . Then, by Lemma 6.2.1,
the set of upper sums is decreasing, and the set of lower sums is increasing.
Ž
.
Furthermore, because of 6.3 , the set of upper sums is bounded from below
Ž
.
Ž
.
by m bya , and the set of lower sums is bounded from above by M bya .
Ž .
Ž .
Hence, the infimum of US
f
and the supremum of LS
f
with respect to
P
P
Ž
.
P do exist see Theorem 1.5.1 .
From Lemma 6.2.2 it is easy to deduce that
sup LS
f F inf US
f .
Ž .
Ž .
P
P
P
P

INTEGRATION
208
Now, suppose that for the given 0 there exists a 0 such that
US
f yLS
f 
6.4
Ž .
Ž .
Ž
.
P
P
for any partition whose norm  
is less than . We have that
p
LS
f F sup LS
f F inf US
f FUS
f .
6.5
Ž .
Ž .
Ž .
Ž .
Ž
.
P
P
P
P
P
P
Hence,
inf US
f y sup LS
f .
Ž .
Ž .
P
P
P
P
Ž
.
Since 0 is arbitrary, we conclude that if 6.1 is satisfied, then
inf US
f s sup LS
f .
6.6
Ž .
Ž .
Ž
.
P
P
P
P
Ž
. Ž
.
Ž
.
Furthermore, from 6.2 , 6.4 , and 6.5 we obtain
S P, f yA  ,
Ž
.
Ž .
Ž .
where A is the common value of inf US
f
and sup
LS
f . This proves
P
P
P
P
Ž .
w
x
that A is the Riemann integral of f x on a, b .
Ž .
Let us now show that the converse of the theorem is true, that is, if f x is
w
x
Ž
.
Riemann integrable on a, b , then inequality 6.1 holds.
Ž .
If f x is Riemann integrable, then for a given 0 there exists a 0
such that
n

f t
 x yA 
6.7
Ž .
Ž
.
Ý
i
i
3
is1
and
n


f t
 x yA 
6.8
Ž .
Ž
.
Ý
i
i
3
is1

4
w
x
for any partition Ps x , x , . . . , x
of
a, b with a norm  , and any
0
1
n
p

w
x
b Ž .
Ž
.
choices of t , t in
x
, x , is1, 2, . . . , n, where AsH f x dx. From 6.7
i
i
iy1
i
a
Ž
.
and 6.8 we then obtain
n
2

f t
yf t
 x 
.
Ž .
Ž .
Ý
i
i
i
3
is1
Ž .
Ž
.
w
x
Now, M ym
is the supremum of f x yf x
for x, x in
x
, x , is
i
i
iy1
i

w
x
1, 2, . . . , n. It follows that for a given 0 we can choose t , t in x
, x
so
i
i
iy1
i
that
f t
yf t
 M ym y,
is1, 2, . . . , n,
Ž .
Ž .
i
i
i
i

THE EXISTENCE OF THE RIEMANN INTEGRAL
209
Ž .
Ž
.
for otherwise M ym y would be an upper bound for f x yf x for all
i
i
w
x
w Ž
.x
x, x in
x
, x , which is a contradiction. In particular, if sr 3 bya ,
iy1
i

w
x
then we can find t , t in
x
, x
such that
i
i
iy1
i
n
US
f yLS
f s
M ym
 x
Ž .
Ž .
Ž
.
Ý
P
P
i
i
i
is1
n


f t
yf t
 x q bya
Ž .
Ž .
Ž
.
Ý
i
i
i
is1
.
Ž
.
This proves the validity of inequality 6.1 .

Ž .
w
x
Ž .
Corollary 6.2.1.
Let f x be a bounded function on a, b . Then f x is
w
x
Ž .
Ž .
Riemann integrable on a, b if and only if inf US
f ssup LS
f , where
P
P
p
P
Ž .
Ž .
Ž .
LS
f
and US
f
are, respectively, the lower and upper sums of f x with
P
P
w
x
respect to a partition P of a, b .
Proof. See Exercise 6.1.

Ž .
w
x
Ž .
2
EXAMPLE 6.2.1.
Let f x :
0, 1 ™R be the function f x sx . Then,
Ž .
w
x

4
f x is Riemann integrable on 0, 1 . To show this, let Ps x , x , . . . , x
be
0
1
n
w
x
any partition of 0, 1 , where x s0, x s1. Then
0
n
n
2
LS
f s
x
 x ,
Ž .
Ý
P
iy1
i
is1
n
2
US
f s
x  x .
Ž .
Ý
P
i
i
is1
Hence,
n
2
2
US
f yLS
f s
x yx
 x
Ž .
Ž .
Ž
.
Ý
P
P
i
iy1
i
is1
n
2
2
F 
x yx
,
Ž
.
Ý
p
i
iy1
is1
where  
is the norm of P. But
p
n
2
2
2
2
x yx
sx yx s1.
Ž
.
Ý
i
iy1
n
0
is1
Thus
US
f yLS
f F .
Ž .
Ž .
P
P
p

INTEGRATION
210
It follows that for a given 0 we can choose s such that for any
partition P whose norm  
is less than ,
p
US
f yLS
f .
Ž .
Ž .
P
P
Ž .
2
w
x
By Theorem 6.2.1, f x sx
is Riemann integrable on 0, 1 .
Ž . w
x
Ž .
EXAMPLE 6.2.2.
Consider the function f x : 0, 1 ™R such that f x s0
Ž .
if x a rational number and f x s1 if x is irrational. Since every subinterval
w
x
of 0, 1 contains both rational and irrational numbers, then for any partition

4
w
x
Ps x , x , . . . , x
of 0, 1 we have
0
1
n
n
n
US
f s
M  x s
 x s1,
Ž .
Ý
Ý
P
i
i
i
is1
is1
n
n
LS
f s
m  x s
0  x s0.
Ž .
Ý
Ý
P
i
i
i
is1
is1
It follows that
inf US
f s1
and
sup LS
f s0.
Ž .
Ž .
P
P
P
P
Ž .
w
x
By Corollary 6.2.1, f x is not Riemann integrable on 0, 1 .
6.3. SOME CLASSES OF FUNCTIONS THAT ARE
RIEMANN INTEGRABLE
There are certain classes of functions that are Riemann integrable. Identify-
ing a given function as a member of such a class can facilitate the determina-
tion of its Riemann integrability. Some of these classes of functions include:
Ž .
Ž .
Ž
.
i continuous functions; ii monotone functions; iii functions of bounded
variation.
Ž .
w
x
Theorem 6.3.1.
If
f x
is continuous on
a, b , then it is Riemann
integrable there.
Ž .
Proof. Since f x is continuous on a closed and bounded interval, then by
w
x
Theorem 3.4.6 it must be uniformly continuous on a, b . Consequently, for a
given 0 there exists a 0 that depends only on  such that for any
w
x
x , x
in a, b we have
1
2

f x
yf x

Ž
.
Ž
.
1
2
bya

SOME CLASSES OF FUNCTIONS THAT ARE RIEMANN INTEGRABLE
211



4
if
x yx
. Let Ps x , x , . . . , x
be a partition of P with a norm
1
2
0
1
n
 . Then
p
n
US
f yLS
f s
M ym
 x ,
Ž .
Ž .
Ž
.
Ý
P
P
i
i
i
is1
Ž .
where m and M are, respectively, the infimum and supremum of f x
on
i
i
w
x
w
x
x
, x , is1, 2, . . . , n. By Corollary 3.4.1 there exist points  ,  in x
, x
iy1
i
i
i
iy1
i
Ž
.
Ž
.


such that m sf  , M sf  , is1, 2, . . . , n. Since
 y F  for
i
i
i
i
i
i
p
is1, 2, . . . , n, then
n
US
f yLS
f s
f 
yf 
 x
Ž .
Ž .
Ž
.
Ž
.
Ý
P
P
i
i
i
is1
n


 x s.
Ý
i
bya is1
Ž .
w
x
By Theorem 6.2.1 we conclude that f x
is Riemann integrable on
a, b .

It should be noted that continuity is a sufficient condition for Riemann
Ž .
integrability, but is not a necessary one. A function f x can have discontinu-
w
x
w
x
ities in
a, b and still remains Riemann integrable on
a, b . For example,
consider the function
y1,
y1Fx0,
f x s
Ž . ½ 1,
0FxF1.
This function is discontinuous at xs0. However, it is Riemann integrable on
w
x

4
y1, 1 . To show this, let 0 be given, and let Ps x , x , . . . , x
be a
0
1
n
w
x
partition of
y1, 1
such that  r2. By the nature of this function,
p
Ž
.
Ž
.
Ž .
w
x
f x yf x
G0, and the infimum and supremum of f x on
x
, x
are
i
iy1
iy1
i
Ž
.
Ž
.
equal to f x
and f x , respectively, is1, 2, . . . , n. Hence,
iy1
i
n
US
f yLS
f s
M ym
 x
Ž .
Ž .
Ž
.
Ý
P
P
i
i
i
is1
n
s
f x
yf x
 x
Ž
.
Ž
.
Ý
i
iy1
i
is1
n
 
f x
yf x
s 
f 1 yf y1
Ž
.
Ž
.
Ž .
Ž
.
Ý
p
i
iy1
p
is1


f 1 yf y1
s.
Ž .
Ž
.
2

INTEGRATION
212
Ž .
w
x
The function f x
is therefore Riemann integrable on y1, 1 by Theorem
6.2.1.
On the basis of this example it is now easy to prove the following theorem:
Ž .
Ž
.
Theorem 6.3.2.
If f x is monotone increasing or monotone decreasing
w
x
on a, b , then it is Riemann integrable there.
Theorem 6.3.2 can be used to construct a function that has a countable
w
x
Ž
number of discontinuities in
a, b
and is also Riemann integrable
see
.
Exercise 6.2 .
6.3.1. Functions of Bounded Variation
Ž .
w
x
Let f x be defined on a, b . This function is said to be of bounded variation
w
x
on
a, b
if there exists a number M0 such that for any partition Ps

4
w
x
x , x , . . . , x
of a, b we have
0
1
n
n


 f FM,
Ý
i
is1
Ž
.
Ž
.
where  f sf x yf x
, is1, 2, . . . , n.
i
i
iy1
Ž
.
w
x
Any function that is monotone increasing or decreasing on a, b is also
Ž .
of bounded variation there. To show this, let f x be monotone increasing on
w
x
a, b . Then
n
n


 f s
f x
yf x
sf b yf a .
Ž
.
Ž
.
Ž .
Ž .
Ý
Ý
i
i
iy1
is1
is1
Ž .
Ž .
Hence, if M is any number greater than or equal to f b yf a , then
n


w
x
Ý
 f FM for any partition P of a, b .
is1
i
Another example of a function of bounded variation is given in the next
theorem.
Ž .
w
x
Ž .
Theorem 6.3.3.
If f x
is continuous on
a, b
and its derivative f x
Ž
.
Ž .
w
x
exists and is bounded on
a, b , then f x is of bounded variation on a, b .

4
w
x
Proof. Let Ps x , x , . . . , x
be a partition of
a, b . By applying the
0
1
n
Ž
.
w
x
mean value theorem
Theorem 4.2.2
on each
x
, x , is1, 2, . . . , n, we
iy1
i
obtain
n
n


 f s
f 
 x
Ž
.
Ý
Ý
i
i
i
is1
is1
n
FK
x yx
sK bya ,
Ž
.
Ž
.
Ý
i
iy1
is1

SOME CLASSES OF FUNCTIONS THAT ARE RIEMANN INTEGRABLE
213

Ž .
where x
 x , is1, 2, . . . , n, and K0 is such that
f x
FK on
iy1
i
i
Ž
.
a, b .

w
x
It should be noted that any function of bounded variation on a, b is also

4
bounded there. This is true because if axb, then Ps a, x, b
is a
w
x
partition of a, b . Hence,
f x yf a
q f b yf x
FM.
Ž .
Ž .
Ž .
Ž .
 Ž .
w
x
for some positive number M. This implies that
f x
is bounded on
a, b
since
1
f x
F
f x yf a
q f x yf b
q f a qf b
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
2
1
F
Mq f a qf b
.
Ž .
Ž .
2
Ž .
The converse of this result, however, is not necessarily true, that is, if f x is
bounded, then it may not be of bounded variation. For example, the function

°x cos
,
0xF1,
~
ž /
f x s
Ž .
2 x
¢0,
xs0
w
x
is bounded on 0, 1 , but is not of bounded variation there. It can be shown
that for the partition
1
1
1 1
Ps 0,
,
, . . . ,
,
, 1 ,
½
5
2n 2ny1
3 2
2 n 

Ý
 f ™ as n™ and hence cannot be bounded by a constant M for
is1
i
Ž
.
all n see Exercise 6.4 .
Ž .
w
x
Theorem 6.3.4.
If
f x
is of bounded variation on
a, b , then it is
Riemann integrable there.

4
Proof. Let 0 be given, and let Ps x , x , . . . , x
be a partition of
0
1
n
w
x
a, b . Then
n
US
f yLS
f s
M ym
 x ,
6.9
Ž .
Ž .
Ž
.
Ž
.
Ý
P
P
i
i
i
is1
Ž .
w
x
where m
and M are the infimum and supremum of f x
on
x
, x ,
i
i
iy1
i
respectively, is1, 2, . . . , n. By the properties of m and M , there exist 
i
i
i

INTEGRATION
214
w
x
and  in
x
, x
such that for is1, 2, . . . , n,
i
iy1
i
m Ff 
m q,
Ž
.
i
i
i
M yf 
FM ,
Ž
.
i
i
i
where  is a small positive number to be determined later. It follows that
M ym y2f 
yf 
FM ym ,
is1, 2, . . . , n.
Ž
.
Ž
.
i
i
i
i
i
i
Hence,
M ym 2qf 
yf 
Ž
.
Ž
.
i
i
i
i
F2q f 
yf 
,
is1, 2, . . . , n.
Ž
.
Ž
.
i
i
Ž
.
From formula 6.9 we obtain
n
n
US
f yLS
f 2
 x q
f 
yf 
 x .
6.10
Ž .
Ž .
Ž
.
Ž
.
Ž
.
Ý
Ý
P
P
i
i
i
i
is1
is1
Now, if  
is the norm of P, then
p
n
n
f 
yf 
 x F 
f 
yf 
Ž
.
Ž
.
Ž
.
Ž
.
Ý
Ý
i
i
i
p
i
i
is1
is1
m
F 
f z
yf z
,
6.11
Ž
.
Ž
.
Ž
.
Ý
p
i
iy1
is1

4
w
x
where
z , z , . . . , z
is a partition Q of a, b , which consists of the points
0
1
m
x , x , . . . , x
as well as the points  ,  ,  ,  , . . . ,  ,  , that is, Q is a
0
1
n
1
1
2
2
n
n
Ž
.
refinement of P obtained by adding the  ’s and  ’s is1, 2, . . . , n . Since
i
i
Ž .
w
x
f x is of bounded variation on a, b , there exists a number M0 such that
m
f z
yf z
FM.
6.12
Ž
.
Ž
.
Ž
.
Ý
i
iy1
is1
Ž
. Ž
.
Ž
.
From 6.10 , 6.11 , and 6.12 it follows that
US
f yLS
f 2 bya qM .
6.13
Ž .
Ž .
Ž
.
Ž
.
P
P
p
Let us now select the partition P such that  , where Mr2. If we
p
Ž
.
Ž
.
also choose  such that 2 bya r2, then from
6.13
we obtain
Ž .
Ž .
Ž .
US
f yLS
f . The function f x is therefore Riemann integrable on
P
P
w
x
a, b by Theorem 6.2.1.


PROPERTIES OF THE RIEMANN INTEGRAL
215
6.4. PROPERTIES OF THE RIEMANN INTEGRAL
The Riemann integral has several properties that are useful at both the
theoretical and practical levels. Most of these properties are fairly simple and
striaghtforward. We shall therefore not prove every one of them in this
section.
Ž .
Ž .
w
x
Theorem 6.4.1.
If f x and g x are Riemann integrable on a, b and if
Ž .
Ž .
c
and c
are constants, then c f x qc g x
is Riemann integrable on
1
2
1
2
w
x
a, b , and
b
b
b
c f x qc g x
dxsc
f x
dxqc
g x
dx.
Ž .
Ž .
Ž .
Ž .
H
H
H
1
2
1
2
a
a
a
Ž .
w
x
Ž .
Theorem 6.4.2.
If f x is Riemann integrable on a, b , and mFf x FM
w
x
for all x in a, b , then
b
m bya F
f x
dxFM bya .
Ž
.
Ž .
Ž
.
H
a
Ž .
Ž .
w
x
Theorem 6.4.3.
If f x and g x are Riemann integrable on a, b , and if
Ž .
Ž .
w
x
f x Fg x for all x in a, b , then
b
b
f x
dxF
g x
dx.
Ž .
Ž .
H
H
a
a
Ž .
w
x
Theorem 6.4.4.
If f x is Riemann integrable on a, b and if acb,
then
c
b
b
f x
dxs
f x
dxq
f x
dx.
Ž .
Ž .
Ž .
H
H
H
a
a
c
Ž .
w
x
 Ž .
Theorem 6.4.5.
If f x is Riemann integrable on a, b , then so is f x
and
b
b
f x
dx F
f x
dx.
Ž .
Ž .
H
H
a
a

4
w
x
Proof. Let Ps x , x , . . . , x
be a partition of a, b . Let m and M be
0
1
n
i
i
Ž .
w
x


the infimum and supremum of f x , respectively, on x
, x ; and let m , M
iy1
i
i
i
 Ž .
be the same for f x
. We claim that
M ym GM
ym
 ,
is1, 2, . . . , n.
i
i
i
i


Ž .
It is obvious that M ym sM ym if f x is either nonnegative or nonposi-
i
i
i
i
w
x
Ž .
tive for all x in x
, x , is1, 2, . . . , n. Let us therefore suppose that f x is
iy1
i

INTEGRATION
216
negative on Dy and nonnegative on Dq, where Dy and Dq are such that
i
i
i
i
y
q
w
x
D
jD
sD s x
, x
for is1, 2, . . . , n. We than have
i
i
i
iy1
i
M ym s sup f x y inf f x
Ž .
Ž .
i
i
y
q
Di
Di
s sup f x
y inf y f x
Ž .
Ž .
Ž
.
y
q
Di
Di
s sup f x
q sup f x
Ž .
Ž .
q
y
D
D
i
i

G sup f x
sM ,
Ž .
i
Di
 Ž .

 Ž .
 Ž . 4

q
y
since sup
f x
smax sup
f x
, sup
f x
. Hence, M ym GM G
D
D
D
i
i
i
i
i
i
M
ym
 for is1, 2, . . . , n, which proves our claim.
i
i
n
n


 
 
US
f
yLS
f
s
M ym
 x F
M ym
 x .
Ž
.
Ž
.
Ž
.
Ž
.
Ý
Ý
P
P
i
i
i
i
i
i
is1
is1
Hence,
 
 
US
f
yLS
f
FUS
f yLS
f .
6.14
Ž .
Ž .
Ž
.
Ž
.
Ž
.
P
P
P
P
Ž .
Ž
.
Since f x is Riemann integrable, the right-hand side of inequality 6.14 can
be made smaller than any given 0 by a proper choice of the norm  
of
p
 Ž .
w
x
P. It follows that
f x
is Riemann integrable on
a, b by Theorem 6.2.1.
Ž .
 Ž .
w
x
b
Ž .
Furthermore, since f x F f x
for all x in
a, b , then H f x dxF
a
b Ž .
H
f x
by Theorem 6.4.3, that is,
a
b
b

f x
dxF
f x
dx.
Ž .
Ž .
H
H
a
a

b Ž .

b Ž .
Thus, H f x dx FH
f x
dx.

a
a
Ž .
w
x
2Ž .
Corollary 6.4.1.
If f x is Riemann integrable on a, b , then so is f
x .
Proof. Using the same notation as in the proof of Theorem 6.4.5, we have

2

2
2Ž .
that m
and M
are, respectively, the infimum and supremum of f
x on
i
i
w
x
x
, x
for is1, 2, . . . , n. Now,
iy1
i
M
2ym
2s M
ym
M
qm
Ž
. Ž
.
i
i
i
i
i
i
F2 M M
ym
Ž
.
i
i
F2 M M ym
,
is1, 2, . . . , n,
6.15
Ž
.
Ž
.
i
i

PROPERTIES OF THE RIEMANN INTEGRAL
217
 Ž .
w
x
where M is the supremum of f x
on a, b . The Riemann integrability of
2Ž .
Ž
.
Ž .
f
x now follows from inequality 6.15 by the Riemann integrability of f x .

Ž .
Ž .
w
x
Corollary 6.4.2.
If f x and g x are Riemann integrable on a, b , then
Ž . Ž .
so is their product f x g x .
Proof. This follows directly from the identity
2
2
4 f x g x s f x qg x
y f x yg x
,
6.16
Ž . Ž .
Ž .
Ž .
Ž .
Ž .
Ž
.
Ž .
Ž .
Ž .
Ž .
and the fact that the squares of f x qg x
and f x yg x
are Riemann
w
x
integrable on a, b by Theorem 6.4.1 and Corollary 6.4.1.

Ž
.
Ž .
Theorem 6.4.6
The Mean Value Theorem for Integrals .
If
f x
is
w
x
w
x
continuous on a, b , then there exists a point cg a, b such that
bf x
dxs bya f c .
Ž .
Ž
. Ž .
H
a
Proof. By Theorem 6.4.2 we have
1
b
mF
f x
dxFM,
Ž .
H
bya
a
Ž .
where m and M are, respectively, the infimum and supremum of f x
on
w
x
Ž .
a, b . Since f x
is continuous, then by Corollary 3.4.1 it must attain the
w
x
values m and M at some points inside a, b . Furthermore, by the intermedi-
Ž
.
Ž .
ate-value theorem Theorem 3.4.4 , f x assumes every value between m and
w
x
M. Hence, there is a point cg a, b such that
1
b
f c s
f x
dx.

Ž .
Ž .
H
bya
a
Ž .
w
x
Definition 6.4.1.
Let f x be Riemann integrable on a, b . The function
x
F x s
f t
dt,
aFxFb,
Ž .
Ž .
H
a
Ž .
is called an indefinite integral of f x .

Ž .
w
x
Ž .
Theorem 6.4.7.
If f x
is Riemann integrable on
a, b , then F x s
x Ž .
w
x
H f t dt is uniformly continuous on a, b .
a

INTEGRATION
218
w
x
Proof. Let x , x
be in a, b , x x . Then,
1
2
1
2
x
x
2
1
F x
yF x
s
f t
dty
f t
dt
Ž
.
Ž
.
Ž .
Ž .
H
H
2
1
a
a
x2
s
f t
dt ,
by Theorem 6.4.4
Ž .
H
x1
x2
F
f t
dt,
by Theorem 6.4.5
Ž .
H
x1
FM x yx
,
Ž
.
2
1
 Ž .
w
x
where M is the supremum of
f x
on
a, b . Thus if 0 is given, then

Ž
.
Ž
.


F x
yF x
 provided that
x yx
rM. This proves uniform
2
1
1
2
Ž .
w
x
continuity of F x on a, b .

The next theorem presents a practical way for evaluating the Riemann
w
x
integral on a, b .
Ž .
w
x
Ž .
Theorem 6.4.8.
Suppose that f x
is continuous on
a, b . Let F x s
x Ž .
H f t dt. Then we have the following:
a
Ž .
Ž .
i. dF x rdxsf x , aFxFb.
b Ž .
Ž .
Ž .
Ž .
Ž .
ii. H f x dxsG b yG a , where G x sF x qc, and c is an arbitrary
a
constant.
Proof. We have
x
x
dF x
d
1
Ž .
xqh
s
f t
dts lim
f t
dty
f t
dt
Ž .
Ž .
Ž .
H
H
H
dx
dx
h
h™0
a
a
a
1
xqh
s lim
f t
dt,
by Theorem 6.4.4
Ž .
H
h
h™0
x
s lim f xqh ,
Ž
.
h™0
by Theorem 6.4.6, where 0FF1. Hence,
dF x
Ž .
s lim f xqh sf x
Ž
.
Ž .
dx
h™0
Ž .
by the continuity of f x . This result indicates that an indefinite integral of
Ž .
Ž .
f x is any function whose derivative is equal to f x . It is therefore unique
Ž .
Ž .
up to a constant. Thus both F x
and F x qc, where c is an arbitrary
constant, are considered to be indefinite integrals.

PROPERTIES OF THE RIEMANN INTEGRAL
219
Ž .
w
x
To prove the second part of the theorem, let G x be defined on a, b as
x
G x sF x qcs
f t
dtqc,
Ž .
Ž .
Ž .
H
a
Ž .
Ž .
Ž .
that is, G x is an indefinite integral of f x . If xsa, then G a sc, since
Ž .
Ž .
Ž .
b Ž .
Ž .
F a s0. Also, if xsb, then G b sF b qcsH f t dtqG a . It follows
a
that
bf t
dtsG b yG a .

Ž .
Ž .
Ž .
H
a
This result is known as the fundamental theorem of calculus. It is generally
Ž
.
attributed to Isaac Barrow
16301677 , who was the first to realize that
differentiation and integration are inverse operations. One advantage of this
Ž .
theorem is that it provides a practical way to evaluate the integral of f x on
w
x
a, b .
6.4.1. Change of Variables in Riemann Integration
There are situations in which the variable x in a Riemann integral is a
function of some other variable, say u. In this case, it may be of interest to
determine how the integral can be expressed and evaluated under the given
transformation. One advantage of this change of variable is the possibility of
simplifying the actual evaluation of the integral, provided that the transfor-
mation is properly chosen.
Ž .
w
x
Ž .
Theorem 6.4.9.
Let f x be continuous on ,  , and let xsg u be a
Ž .
w
x
function whose derivative g u exists and is continuous on c, d . Suppose
w
x
w
x
that the range of g is contained inside
,  . If a, b are points in
, 
Ž .
Ž .
such that asg c and bsg d , then
b
d
f x
dxs
f g u
g u du.
Ž .
Ž .
Ž .
H
H
a
c
Ž .
x Ž .
Ž .
Ž .
Ž .
Proof. Let F x sH f t dt. By Theorem 6.4.8, F x sf x . Let G u be
a
defined as
u
G u s
f g t
g t
dt.
Ž .
Ž .
Ž .
H
c
Since f, g, and g are continuous, then by Theorem 6.4.8 we have
dG u
Ž .
sf g u
g u .
6.17
Ž .
Ž .
Ž
.
du

INTEGRATION
220
Ž
.
However, according to the chain rule Theorem 4.1.3 ,
dF g u
dF g u
dg u
Ž .
Ž .
Ž .
s
du
dg u
du
Ž .
sf g u
g u .
6.18
Ž .
Ž .
Ž
.
Ž
.
Ž
.
From formulas 6.17 and 6.18 we conclude that
G u yF g u
s,
6.19
Ž .
Ž .
Ž
.
w
x
Ž .
where  is a constant. If a and b are points in ,  such that asg c , bs
Ž .
Ž .
w Ž .x
Ž .
g d , then when usc, we have G c s0 and syF g c syF a s0.
Ž .
d w Ž .x
Ž .
Ž
.
Furthermore, when usd, G d sH f g t
g t dt. From 6.19 we then ob-
c
tain
d
G d s
f g t
g t
dtsF g d
q
Ž .
Ž .
Ž .
Ž .
H
c
sF b
Ž .
b
s
f x
dx.
Ž .
H
a
2Ž
2
.1r2
2
For example, consider the integral H
2t y1
t dt. Let
xs2t y1.
1
Then dxs4t dt, and by Theorem 6.4.9,
1
2
7
1r2
2
1r2
2t y1
t dts
x
dx.
Ž
.
H
H
4
1
1
2
1r2
3r2
An indefinite integral of x
is given by
x
. Hence,
3
2
1r2
1
2
1
2
3r2
3r2
2t y1
t dts
7
y1 s
7
y1 .

Ž
.
Ž
.
Ž
.
Ž .
H
4
3
6
1
6.5. IMPROPER RIEMANN INTEGRALS
In our study of the Riemann integral we have only considered integrals of
w
x
functions that are bounded on a finite interval
a, b . We now extend the
scope of Riemann integration to include situations where the integrand can
become unbounded at one or more points inside the range of integration,
which can also be infinite. In such situations, the Riemann integral is called
an improper integral.
Ž .
There are two kinds of improper integrals. If f x is Riemann integrable
w
x
 Ž .
on a, b for any ba, then H f x dx is called an improper integral of the
a
Ž .
first kind, where the range of integration is infinite. If, however, f x

IMPROPER RIEMANN INTEGRALS
221
becomes infinite at a finite number of points inside the range of integration,
b Ž .
then the integral H f x dx is said to be improper of the second kind.
a
Ž .
z Ž .
Ž .
Definition 6.5.1.
Let F z sH f x dx. Suppose that F z
exists for any
a
Ž .
value of z greater than a. If F z
has a finite limit L as z™, then the
 Ž .
improper integral H f x dx is said to converge to L. In this case, L
a
Ž .
w
.
represents the Riemann integral of f x on a,  and we write

Ls
f x
dx.
Ž .
H
a
 Ž .
On the other hand, if Ls, then the improper integral H f x dx is said
a
a
Ž .
to diverge. By the same token, we can define the integral H
f x dx as the
y
a
Ž .

Ž .
limit, if it exists, of H
f x dx as z™. Also, H
f x dx is defined as
yz
y

a
z
f x
dxs lim
f x
dxq lim
f x
dx,
Ž .
Ž .
Ž .
H
H
H
u™
z™
y
yu
a
where a is any finite number, provided that both limits exist.
 Ž .
The convergence of H f x dx can be determined by using the Cauchy
a
criterion in a manner similar to the one used in the study of convergence of
Ž
.
sequences see Section 5.1.1 .
 Ž .
Theorem 6.5.1.
The improper integral H f x dx converges if and only if
a
for a given 0 there exists a z
such that
0
z2f x
dx  ,
6.20
Ž .
Ž
.
H
z1
whenever z and z exceed z .
1
2
0
Ž .
z Ž .
Proof. If F z sH f x dx has a limit L as z™, then for a given 0
a
there exists z
such that for zz .
0
0

F z yL 
.
Ž .
2
Now, if both z and z exceed z , then
1
2
0
z2f x
dx s F z
yF z
Ž .
Ž
.
Ž
.
H
2
1
z1
F F z
yL q F z
yL .
Ž
.
Ž
.
2
1
Ž
.
Ž .
Vice versa, if condition 6.20 is satisfied, then we need to show that F z has

4
a limit as z™. Let us therefore define the sequence
g
, where g
is
n ns1
n

INTEGRATION
222
given by
aqn
g s
f x
dx,
ns1, 2, . . . .
Ž .
H
n
a
It follows that for any 0,
aqn


g yg
s
f x
dx  ,
Ž .
H
n
m
aqm

4
if m and n are large enough. This implies that g
is a Cauchy sequence;
n ns1
hence it converges by Theorem 5.1.6. Let gslim
g . To show that
n™
n
Ž .
lim
F z sg, let us write
z™
F z yg s F z yg qg yg
Ž .
Ž .
n
n


F F z yg
q g yg .
6.21
Ž .
Ž
.
n
n


Suppose 0 is given. There exists an integer N such that g yg r2 if
1
n
nN . Also, there exists an integer N such that
1
2
z

F z yg
s
f x
dx 
6.22
Ž .
Ž .
Ž
.
H
n
2
aqn
Ž
.
if zaqnN . Thus by choosing zaqn, where nmax N , N ya ,
2
1
2
Ž
.
Ž
.
we get from inequalities 6.21 and 6.22
F z yg .
Ž .
This completes the proof.

 Ž .
Definition 6.5.2.
If the improper integral H
f x
dx is convergent, then
a
 Ž .
 Ž .
the integral H f x dx is said to be absolutely convergent. If H f x dx is
a
a
convergent but not absolutely, then it is said to be conditionally convergent.

It is easy to show that an improper integral is convergent if it converges
absolutely.
As with the case of series of positive terms, there are comparison tests that
can be used to test for convergence of improper integrals of the first kind of
nonnegative functions. These tests are described in the following theorems.
Ž .
Theorem 6.5.2.
Let f x
be a nonnegative function that is Riemann
w
x
Ž .
integrable on a, b for every bGa. Suppose that there exists a function g x
Ž .
Ž .
 Ž .
 Ž .
such that f x Fg x for xGa. If H g x dx converges, then so does H f x dx
a
a

IMPROPER RIEMANN INTEGRALS
223
and we have


f x
dxF
g x
dx.
Ž .
Ž .
H
H
a
a
Proof. See Exercise 6.7.

Ž .
Ž .
Theorem 6.5.3.
Let f x
and g x
be nonnegative functions that are
w
x
Riemann integrable on a, b for every bGa. If
f x
Ž .
lim
sk,
g x
x™
Ž .
 Ž .
 Ž .
where k is a positive constant, then H f x dx and H g x dx are either both
a
a
convergent or both divergent.
Proof. See Exercise 6.8.

EXAMPLE 6.5.1.
Consider the integral Heyx x 2 dx. We have that e xs1q
1

Ž
n
.
x
p
Ý
x rn! . Hence, for xG1, e x rp!, where p is any positive integer. If
ns1
p is chosen such that py2G2, then
p!
p!
yx
2
e
x 
F
.
py2
2
x
x
Ž
2.
w
x
However, H
dxrx
s y1rx
s1. Therefore, by Theorem 6.5.2, the inte-
1
1
yx
2
w
.
gral of e
x
on 1,  is convergent.
wŽ
. Ž
.2x
EXAMPLE 6.5.2.
The integral H
sin x r xq1
dx is absolutely conver-
0
gent, since


sin x
1
F
2
2
xq1
xq1
Ž
.
Ž
.
and


dx
1
s y
s1.
H
2
xq1
0
xq1
Ž
.
0
Ž
.
EXAMPLE 6.5.3.
The integral H sin xrx dx is conditionally convergent.
0
Ž
.
We first show that H sin xrx dx is convergent. We have that
0


sin x
sin x
sin x
1
dxs
dxq
dx.
6.23
Ž
.
H
H
H
x
x
x
0
0
1

INTEGRATION
224
Ž
.
w
x
By Exercise 6.3, sin x rx is Riemann integrable on 0, 1 , since it is continu-
Ž
ous there except at xs0, which is a discontinuity of the first kind
see
.
Ž
.
Definition 3.4.2 . As for the second integral in 6.23 , we have for z z 1,
2
1
z2
z
z
sin x
cos x
cos x
2
2
dxs y
y
dx
H
H
2
x
x
x
z
z
z
1
1
1
z
cos z
cos z
cos x
2
1
2
s
y
y
dx.
H
2
z
z
x
z
1
2
1
Thus
z
z
sin x
1
1
dx
2
2
2
dx F
q
q
s
.
H
H
2
x
z
z
z
x
z
z
1
2
1
1
1
Since 2rz can be made arbitrarily small by choosing z
large enough, then
1
1
Ž
.
Ž
.
by Theorem 6.5.1, H sin xrx dx is convergent and so is H sin xrx dx.
1
0
Ž
.
It remains to show that H sin xrx dx is not absolutely convergent. This
0
Ž
.
follows from the fact that see Exercise 6.10
n sin x
lim
dxs.
H
x
n™
0
Convergence of improper integrals of the first kind can be used to
Ž
.
determine convergence of series of positive terms see Section 5.2.1 . This is
based on the next theorem.
Ž
.

Theorem 6.5.4
Maclaurin’s Integral Test .
Let Ý
a
be a series of
ns1
n
Ž .
positive terms such that a
Fa
for nG1. Let f x
be a positive nonin-
nq1
n
w
.
Ž .
creasing function defined on
1, 
such that f n sa , ns1, 2, . . . , and
n
Ž .

f x ™0 as x™. Then, Ý
a
converges if and only if the improper
ns1
n
 Ž .
integral H f x dx converges.
1
Proof. If nG1 and nFxFnq1, then
a sf n Gf x Gf nq1 sa
.
Ž .
Ž .
Ž
.
n
nq1
By Theorem 6.4.2 we have for nG1
nq1
a G
f x
dxGa
.
6.24
Ž .
Ž
.
H
n
nq1
n
n
Ž
.
If s sÝ
a is the nth partial sum of the series, then from inequality 6.24
n
ks1
k
we obtain
nq1
s G
f x
dxGs
ya .
6.25
Ž .
Ž
.
H
n
nq1
1
1

IMPROPER RIEMANN INTEGRALS
225
If the series Ý
a
converges to the sum s, then sGs
for all n. Conse-
ns1
n
n
Ž
.
nq1 Ž .
quently, the sequence whose nth term is F nq1 sH
f x dx is monotone
1
increasing and is bounded by s; hence it must have a limit. Therefore, the
 Ž .
integral H f x dx converges.
1
 Ž .
Now, let us suppose that H f x dx is convergent and is equal to L. Then
1
Ž
.
from inequality 6.25 we obtain
nq1
s
Fa q
f x
dxFa qL,
nG1,
6.26
Ž .
Ž
.
H
nq1
1
1
1
Ž .
Ž
.
since f x is positive. Inequality 6.26 indicates that the monotone increasing

4
sequence
s
is bounded hence it has a limit, which is the sum of the
n ns1
series.

Theorem 6.5.4 provides a test of convergence for a series of positive terms.
Of course, the usefulness of this test depends on how easy it is to integrate
Ž .
the function f x .
As an example of using the integral test, consider the harmonic series

Ž
.
Ž .
Ž .
Ž .
x Ž .
Ý
1rn . If f x
is defined as f x s1rx, xG1, then F x sH f t dts
ns1
1
Ž .
log x. Since F x
goes to infinity as
x™, the harmonic series must
therefore be divergent, as was shown in Chapter 5. On the other hand, the

Ž
2.
Ž .
xŽ
2.
series Ý
1rn
is convergent, since F x sH
dtrt
s1y1rx, which
ns1
1
converges to 1 as x™.
6.5.1. Improper Riemann Integrals of the Second Kind
b Ž .
w
x
Let us now consider integrals of the form H f x dx where
a, b is a finite
a
interval and the integrand becomes infinite at a finite number of points
w
x
inside a, b . Such integrals are called improper integrals of the second kind.
Ž .
q
b Ž .
Suppose, for example, that f x ™ as x™a . Then H f x dx is said to
a
converge if the limit
b
lim
f x
dx
Ž .
H
q
™0
aq
Ž .
y
b Ž .
exists and is finite. Similarly, if
f x ™ as
x™b , then H f x dx is
a
convergent if the limit
by
lim
f x
dx
Ž .
H
q
™0
a
Ž .
b Ž .
exists. Furthermore, if f x ™ as x™c, where acb, then H f x dx is
a
c Ž .
b Ž .
the sum of H f x dx and H f x dx provided that both integrals converge. By
a
c
Ž .
w
x
definition, if f x ™ as x™x , where x g a, b , then x
is said to be a
0
0
0
Ž .
singularity of f x .

INTEGRATION
226
The following theorems can help in determining convergence of integrals
of the second kind. They are similar to Theorems 6.5.1, 6.5.2, and 6.5.3. Their
proofs will therefore be omitted.
Ž .
q
b Ž .
Theorem 6.5.5.
If f x ™ as x™a , then H f x dx converges if and
a
only if for a given 0 there exists a z
such that
0
z2f x
dx  ,
Ž .
H
z1
where z and z
are any two numbers such that az z z b.
1
2
1
2
0
Ž .
b Ž .
Theorem 6.5.6.
Let f x
be a nonnegative function such that H f x dx
c
Ž
x
Ž .
Ž .
Ž .
exists for every c in a, b . If there exists a function g x such that f x Fg x
Ž
x
b Ž .
q
for all x in
a, b , and if H g x dx converges as c™a , then so does
c
b Ž .
H f x dx and we have
c
b
b
f x
dxF
g x
dx.
Ž .
Ž .
H
H
a
a
Ž .
Ž .
Theorem 6.5.7.
Let f x
and g x
be nonnegative functions that are
w
x
Riemann integrable on c, b for every c such that acFb. If
f x
Ž .
lim
sk,
q g x
Ž .
x™a
b Ž .
b Ž .
where k is a positive constant, then H f x dx and H g x dx are either both
a
a
convergent or both divergent.
b Ž .
Definition 6.5.3.
Let H f x dx be an improper integral of the second
a
b Ž .
b Ž .
kind. If H
f x
dx converges, then H f x dx is said to converge absolutely.
a
a
b Ž .
If, however, H f x dx is convergent, but not absolutely, then it is said to be
a
conditionally convergent.

b Ž .
b Ž .
Theorem 6.5.8.
If H
f x
dx converges, then so does H f x dx.
a
a
EXAMPLE 6.5.4.
Consider the integral H1eyx x ny1 dx, where n0. If
0
0n1, then the integral is improper of the second kind, since x ny1 ™ as
x™0q. Thus, xs0 is a singularity of the integrand. Since
eyx x ny1
lim
s1,
ny1
q
x
x™0
then the behavior of H1eyx x ny1 dx with regard to convergence or divergence
0
1
ny1
1
ny1
Ž
.w
nx1
is the same as that of H x
dx. But H x
dxs 1rn
x
s1rn is con-
0
0
0
vergent, and so is H1eyx x ny1 dx.
0

CONVERGENCE OF A SEQUENCE OF RIEMANN INTEGRALS
227
1Ž
2.
EXAMPLE 6.5.5.
H sin xrx
dx. The integrand has a singularity at xs0.
0
Ž .
Ž
. w
2 Ž .x
q
1Ž
.
Let
g x s1rx. Then,
sin x r x g x ™1 as
x™0 . But H
dxrx s
0
w
x1
q
1Ž
2.
log x
is divergent, since log x™y as x™0 . Therefore, H sin xrx
dx
0
0
is divergent.
2Ž
2
. w Ž
.2x
EXAMPLE 6.5.6.
Consider the integral
H
x y3xq1 r x xy1
dx.
0
Here, the integrand has two singularities, namely xs0 and xs1, inside
w
x
0, 2 . We can therefore write
x 2y3xq1
x 2y3xq1
2
1r2
dxs lim
dx
H
H
2
2
q
t™0
0
t
x xy1
x xy1
Ž
.
Ž
.
u
2
x y3xq1
q lim
dx
H
2
y
u™1
1r2 x xy1
Ž
.
x 2y3xq1
2
q lim
dx.
H
2
q
®™1
®
x xy1
Ž
.
We note that
x 2y3xq1
1
1
s
y
.
2
2
x
x xy1
xy1
Ž
.
Ž
.
Hence,
1r2
2
x y3xq1
1
2
dxs lim
log xq
H
2
q
xy1
t™0
0
x xy1
Ž
.
t
u
1
q lim
log xq
y
xy1
u™1
1r2
2
1
q lim
log xq
.
q
xy1
®™1
®
None of the above limits exists as a finite number. This integral is therefore
divergent.
6.6. CONVERGENCE OF A SEQUENCE OF RIEMANN INTEGRALS
In the present section we confine our attention to the limiting behavior of

Ž .4
integrals of a sequence of functions
f
x
.
n
ns1

INTEGRATION
228
Ž .
w
x
Theorem 6.6.1.
Suppose that f
x
is Riemann integrable on
a, b for
n
Ž .
Ž .
w
x
Ž .
nG1. If f
x converges uniformly to f x on a, b as n™, then f x is
n
w
x
Riemann integrable on a, b and
b
b
lim
f
x
dxs
f x
dx.
Ž .
Ž .
H
H
n
n™
a
a
Ž .
w
x
Proof. Let us first show that f x
is Riemann integrable on
a, b . Let
Ž .
Ž .
0 be given. Since f
x converges uniformly to f x , then there exists an
n
integer n that depends only on  such that
0

f
x yf x

6.27
Ž .
Ž .
Ž
.
n
3 bya
Ž
.
w
x
Ž .
if nn for all xg a, b . Let n n . Since f
x is Riemann integrable on
0
1
0
n1
w
x
a, b , then by Theorem 6.2.1 there exists a 0 such that

US
f
yLS
f

6.28
Ž
.
Ž
.
Ž
.
P
n
P
n
1
1
3
w
x
Ž
.
for any partition P of a, b with a norm  . Now, from inequality 6.27
p
we have

f x f
x q
,
Ž .
Ž .
n1
3 bya
Ž
.

f x f
x y
.
Ž .
Ž .
n1
3 bya
Ž
.
We conclude that

US
f FUS
f
q
,
6.29
Ž .
Ž
.
Ž
.
P
P
n1
3

LS
f GLS
f
y
.
6.30
Ž .
Ž
.
Ž
.
P
P
n1
3
Ž
. Ž
.
Ž
.
From inequalities 6.28 , 6.29 , and 6.30 it follows that if  , then
p
2
US
f yLS
f FUS
f
yLS
f
q
.
6.31
Ž .
Ž .
Ž
.
Ž
.
Ž
.
P
P
P
n
P
n
1
1
3
Ž
.
Ž .
w
x
Inequality 6.31 shows that f x
is Riemann integrable on
a, b , again by
Theorem 6.2.1.

SOME FUNDAMENTAL INEQUALITIES
229
Let us now show that
b
b
lim
f
x
dxs
f x
dx.
6.32
Ž .
Ž .
Ž
.
H
H
n
n™
a
a
Ž
.
From inequality 6.27 we have for nn ,
0
b
b
b
f
x
dxy
f x
dx F
f
x yf x
dx
Ž .
Ž .
Ž .
Ž .
H
H
H
n
n
a
a
a


,
3
and the result follows, since  is an arbitrary positive number.

6.7. SOME FUNDAMENTAL INEQUALITIES
In this section we consider certain well-known inequalities for the Riemann
integral.
6.7.1. The Cauchy–Schwarz Inequality
Ž .
Ž .
2Ž .
Theorem 6.7.1.
Suppose that f x
and g x
are such that f
x
and
2Ž .
w
x
g
x are Riemann integrable on a, b . Then
2
b
b
b
2
2
f x g x
dx
F
f
x
dx
g
x
dx .
6.33
Ž . Ž .
Ž .
Ž .
Ž
.
H
H
H
a
a
a
The limits of integration may be finite or infinite.
Proof. Let c and c be constants, not both zero. Without loss of general-
1
2
ity, let us assume that c 0. Then
2
b
2
c f x qc g x
dxG0.
Ž .
Ž .
H
1
2
a
Thus the quadratic form
b
b
b
2
2
2
2
c
f
x
dxq2c c
f x g x
dxqc
g
x
dx
Ž .
Ž . Ž .
Ž .
H
H
H
1
1
2
2
a
a
a

INTEGRATION
230
is nonnegative for all c and c . It follows that its discriminant, namely,
1
2
2
b
b
b
2
2
2
2
c
f x g x
dx
yc
f
x
dx
g
x
dx
Ž . Ž .
Ž .
Ž .
H
H
H
2
2
a
a
a
must be nonpositive, that is,
2
b
b
b
2
2
f x g x
dx
F
f
x
dx
g
x
dx .
Ž . Ž .
Ž .
Ž .
H
H
H
a
a
a
Ž .
Ž .
w
It is easy to see that if f x and g x are linearly related that is, there exist
Ž .
Ž .
x
constants 
and  , not both zero, such that  f x q g x s0 , then
1
2
1
2
Ž
.
inequality 6.33 becomes an equality.

6.7.2. Holder’s Inequality
¨
This is a generalization of the CauchySchwarz inequality due to Otto
Ž
.
Holder
18591937 . To prove Holder’s inequality we need the following
¨
¨
lemmas:
Lemma 6.7.1.
Let a , a , . . . , a ;  ,  , . . . , 
be nonnegative numbers
1
1
n
1
2
n
such that Ýn
 s1. Then
is1
i
n
n
i
a F
 a .
6.34
Ž
.
Ł
Ý
i
i
i
is1
is1
Ž
.
The right-hand side of inequality 6.34 is a weighted arithmetic mean of
the a ’s, and the left-hand side is a weighted geometric mean.
i
Proof. This lemma is an extension of a result given in Section 3.7 concern-
Ž
.
ing the properties of convex functions see Exercise 6.19 .

Ž .
Ž .
Ž .
Lemma 6.7.2.
Suppose that f
x , f
x , . . . , f
x
are nonnegative and
1
2
n
w
x
Riemann integrable on a, b . If  ,  , . . . ,  are nonnegative numbers such
1
2
n
that Ýn
 s1, then
is1
i

n
n
i
b
b
i
f
x
dxF
f
x
dx
.
6.35
Ž .
Ž .
Ž
.
Ł
Ł
H
H
i
i
a
a
is1
is1
b Ž .
Proof. Without loss of generality, let us assume that H f x dx0 for
a
i
w
Ž
.
Ž .
is1, 2, . . . , n
inequality
6.35
is obviously true if at least one f x
is
i

SOME FUNDAMENTAL INEQUALITIES
231
x
identically equal to zero . By Lemma 6.7.1 we have
b
n
i
H
Ł
f
x
dx
Ž .
a
is1
i
i
n
b
Ł
H f
x
dx
Ž .
is1
a
i



1
2
n
f
x
f
x
f
x
Ž .
Ž .
Ž .
b
1
2
n
s

dx
H
b
b
b
H f
x
dx
H f
x
dx
H f
x
dx
Ž .
Ž .
Ž .
a
a
1
a
2
a
n
n
n
 f
x
Ž .
b
i
i
F
dxs
 s1.
Ý
Ý
H
i
b
H f
x
dx
Ž .
a
a
i
is1
is1
Ž
.
Hence, inequality 6.35 follows.

Ž
.
Theorem 6.7.2 Holder’s Inequality .
Let p and q be two positive num-
¨
 Ž . p
 Ž . q
bers such that 1rpq1rqs1. If f x
and g x
are Riemann integrable
w
x
on a, b , then
1rp
1rq
b
b
b
p
q
f x g x
dx F
f x
dx
g x
dx
.
Ž . Ž .
Ž .
Ž .
H
H
H
a
a
a
Proof. Define the functions
p
q
u x s f x
,
® x s g x
.
Ž .
Ž .
Ž .
Ž .
Then, by Lemma 6.7.2,
1rp
1rq
b
b
b
1rp
1rq
u x
® x
dxF
u x
dx
® x
dx
,
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
that is,
1rp
1rq
b
b
b
p
q
f x
g x
dxF
f x
dx
g x
dx
.
6.36
Ž .
Ž .
Ž .
Ž .
Ž
.
H
H
H
a
a
a
Ž
.
The theorem follows from inequality 6.36 and the fact that
b
b
f x g x
dx F
f x
g x
dx.
Ž . Ž .
Ž .
Ž .
H
H
a
a
We note that the Cauchy-Schwarz inequality can be deduced from Theorem
6.7.2 by taking psqs2.


INTEGRATION
232
6.7.3. Minkowski’s Inequality
Ž
.
The following inequality is due to Hermann Minkowski 18641909 .
Ž .
Ž .
Theorem 6.7.3.
Suppose that
f x
and g x
are functions such that
 Ž . p
 Ž . p
w
x
f x
and g x
are Riemann integrable on a, b , where 1Fp. Then
1rp
1rp
1rp
b
b
b
p
p
p
f x qg x
dx
F
f x
dx
q
g x
dx
.
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
Proof. The theorem is obviously true if ps1 by the triangle inequality.
We therefore assume that p1. Let q be a positive number such that
Ž
.
1rpq1rqs1. Hence, psp 1rpq1rq s1qprq. Let us now write
p
prq
f x qg x
s f x qg x
f x qg x
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
prq
prq
F f x
f x qg x
q g x
f x qg x
.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
6.37
Ž
.
By applying Holder’s inequality to the two terms on the right-hand side of
¨
Ž
.
inequality 6.37 we obtain
b
prq
f x
f x qg x
dx
Ž .
Ž .
Ž .
H
a
1rp
1rq
b
b
p
p
F
f x
dx
f x qg x
dx
,
6.38
Ž .
Ž .
Ž .
Ž
.
H
H
a
a
b
prq
g x
f x qg x
dx
Ž .
Ž .
Ž .
H
a
1rp
1rq
b
b
p
p
F
g x
dx
f x qg x
dx
.
6.39
Ž .
Ž .
Ž .
Ž
.
H
H
a
a
Ž
. Ž
.
Ž
.
From inequalities 6.37 , 6.38 , and 6.39 we conclude that
1rq
b
b
p
p
f x qg x
dxF
f x qg x
dx
Ž .
Ž .
Ž .
Ž .
H
H
a
a
1rp
1rp
b
b
p
p

f x
dx
q
g x
dx
.
Ž .
Ž .
H
H
½
5
a
a
6.40
Ž
.

SOME FUNDAMENTAL INEQUALITIES
233
Ž
.
Since 1y1rqs1rp, inequality 6.40 can be written as
1rp
1rp
1rp
b
b
b
p
p
p
f x qg x
dx
F
f x
dx
q
g x
dx
.
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
Minkowski’s inequality can be extended to integrals involving more than
Ž
.
 Ž . p
two functions. It can be shown see Exercise 6.20 that if f x
is Riemann
i
w
x
integrable on a, b for is1, 2, . . . , n, then
1rp
p
1rp
n
n
b
b
p
f
x
dx
F
f
x
dx
.

Ž .
Ž .
Ý
Ý
H
H
i
i
a
a
is1
is1
6.7.4. Jensen’s Inequality
Theorem 6.7.4.
Let X be a random variable with a finite expected value,
Ž
.
Ž .
sE X . If  x is a twice differentiable convex function, then
E  X
G E X
.
Ž
.
Ž
.
Ž .
Ž .
Ž .
Proof. Since  x is convex and  x exists, then we must have  x G0.
Ž
.
By applying the mean value theorem Theorem 4.2.2 around  we obtain
 X s  q Xy  c ,
Ž
.
Ž
.
Ž
.
Ž .
Ž .
where c is between  and X. If Xy0, then c and hence  c G
Ž
.
Ž .
  , since  x is nonnegative. Thus,
 X y  s Xy  c G Xy   .
6.41
Ž
.
Ž
.
Ž
.
Ž .
Ž
.
Ž
.
Ž
.
Ž .
Ž
.
On the other hand, if Xy0, then c and  c F  . Hence,
 X y  s Xy  c G Xy   .
6.42
Ž
.
Ž
.
Ž
.
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
From inequalities 6.41 and 6.42 we conclude that
E  X y 
G  E Xy s0,
Ž
.
Ž
.
Ž
.
Ž
.
which implies that
E  X
G  ,
Ž
.
Ž
.
w
Ž
.x
Ž
.
since E   s  .


INTEGRATION
234
6.8. RIEMANN–STIELTJES INTEGRAL
In this section we consider a more general integral, namely the Riemann
Stieltjes integral. The concept on which this integral is based can be at-
Ž
.
tributed to a combination of ideas by Georg Friedrich Riemann 18261866
Ž
.
and the Dutch mathematician Thomas Joannes Stieltjes 18561894 .
Ž .
Ž .
The Riemann-Stieltjes integral involves two functions f x and g x , both
w
x
b Ž .
Ž .
defined on the interval a, b , and is denoted by H f x dg x . In particular, if
a
Ž .
b Ž .
g x sx we obtain the Riemann integral H f x dx. Thus the Riemann
a
integral is a special case of the Riemann-Stieltjes integral.
Ž .
The definition of the Riemann-Stieltjes integral of f x
with respect to
Ž .
w
x
Ž .
g x on a, b is similar to that of the Riemann integral. If f x is bounded on
w
x
Ž .
w
x

4
a, b , if g x is monotone increasing on a, b , and if Ps x , x , . . . , x
is a
0
1
n
w
x
partition of a, b , then as in Section 6.2, we define the sums
n
LS
f, g s
m  g ,
Ž
.
Ý
P
i
i
is1
n
US
f, g s
M  g ,
Ž
.
Ý
P
i
i
is1
Ž .
where m and M are, respectively, the infimum and supremum of f x
on
i
i
w
x
Ž
.
Ž
.
x
, x ,  g sg x yg x
, is1, 2, . . . , n. If for a given 0 there
iy1
i
i
i
iy1
exists a 0 such that
US
f, g yLS
f, g 
6.43
Ž
.
Ž
.
Ž
.
P
P
Ž .
whenever  , where  
is the norm of P, then f x
is said to be
p
p
Ž .
w
x
RiemannStieltjes integrable with respect to g x on a, b . In this case,
bf x
dg x s inf US
f, g s sup LS
f, g .
Ž .
Ž .
Ž
.
Ž
.
H
P
P
P
a
P
Ž
.
Condition
6.43
is both necessary and sufficient for the existence of the
RiemannStieltjes integral.

4
Equivalently, suppose that for a given partition Ps x , x , . . . , x
we
0
1
n
define the sum
n
S P, f, g s
f t
 g ,
6.44
Ž
.
Ž .
Ž
.
Ý
i
i
is1
w
x
Ž .
where t
is a point in the interval
x
, x , is1, 2, . . . , n. Then f x
is
i
iy1
i
Ž .
w
x
RiemannStieltjes integrable with respect to g x
on
a, b if for any 0

RIEMANNSTIELTJES INTEGRAL
235
there exists a 0 such that
b
S P, f, g y
f x
dg x

6.45
Ž
.
Ž .
Ž .
Ž
.
H
a
w
x
for any partition P of a, b with a norm  , and for any choice of the
p
w
x
point t in
x
, x , is1, 2, . . . , n.
i
iy1
i
Theorems concerning the RiemannStieltjes integral are very similar to
those seen earlier concerning the Riemann integral. In particular, we have
the following theorems:
Ž .
w
x
Ž .
Theorem
6.8.1.
If
f x
is
continuous
on
a, b ,
then
f x
is
w
x
RiemannStieltjes integrable on a, b .
Proof. See Exercise 6.21.

Ž .
Ž
.
Theorem 6.8.2.
If f x is monotone increasing or monotone decreasing
w
x
Ž .
w
x
Ž .
on
a, b , and g x
is continuous on
a, b , then f x
is RiemannStieltjes
Ž .
w
x
integrable with respect to g x on a, b .
Proof. See Exercise 6.22.

The next theorem shows that under certain conditions, the Riemann
Stieltjes integral reduces to the Riemann integral.
Ž .
Theorem 6.8.3.
Suppose that f x
is RiemannStieltjes integrable with
Ž .
w
x
Ž .
Ž .
respect to g x
on
a, b , where g x
has a continuous derivative g x
on
w
x
a, b . Then
b
b
f x
dg x s
f x g x
dx.
Ž .
Ž .
Ž .
Ž .
H
H
a
a

4
w
x
Proof. Let Ps x , x , . . . , x
be a partition of a, b . Consider the sum
0
1
n
n
S P, h s
h t
 x ,
6.46
Ž
.
Ž .
Ž
.
Ý
i
i
is1
Ž .
Ž .
Ž .
where h x sf x g x and x
Ft Fx , is1, 2, . . . , n. Let us also consider
iy1
i
i
the sum
n
S P, f, g s
f t
 g ,
6.47
Ž
.
Ž .
Ž
.
Ý
i
i
is1

INTEGRATION
236
Ž
.
Ž .
If we apply the mean value theorem Theorem 4.2.2 to g x , we obtain
 g sg x
yg x
sg z
 x ,
is1, 2, . . . , n,
6.48
Ž
.
Ž
.
Ž
.
Ž
.
i
i
iy1
i
i
Ž
. Ž
.
Ž
.
where x
z x , is1, 2, . . . , n. From
6.46 ,
6.47 , and
6.48
we can
iy1
i
i
then write
n
S P, f, g yS P, h s
f t
g z
yg t
 x .
6.49
Ž
.
Ž
.
Ž .
Ž
.
Ž .
Ž
.
Ý
i
i
i
i
is1
Ž .
w
x
Ž .
w
x
Since f x is bounded on a, b and g x is uniformly continuous on a, b by
Theorem 3.4.6, then for a given 0 there exists a  0, which depends
1
only on , such that

g z
yg t

,
Ž
.
Ž .
i
i
2 M bya
Ž
.


 Ž .
w
x
Ž
.
if z yt  , where M0 is such that f x
FM on a, b . From 6.49 it
i
i
1
follows that if the partition P has a norm   , then
p
1

S P, f, g yS P, h

.
6.50
Ž
.
Ž
.
Ž
.
2
Ž .
Ž .
Now, since f x
is RiemannStieltjes integrable with respect to g x
on
w
x
a, b , then by definition, for the given 0 there exists a  0 such that
2

b
S P, f, g y
f x
dg x

,
6.51
Ž
.
Ž .
Ž .
Ž
.
H
2
a
Ž
.
Ž
.
if the norm  
of P is less than  . We conclude from 6.50 and 6.51 that
p
2
Ž
.
if the norm of P is less than min  , 
, then
1
2
b
S P, h y
f x
dg x
.
Ž
.
Ž .
Ž .
H
a
b Ž .
Ž .
Since  is arbitrary, this inequality implies that H f x dg x
is in fact the
a
b Ž .
b Ž .
Ž .
Riemann integral H h x dxsH f x g x dx.

a
a
Ž .
Using Theorem 6.8.3, it is easy to see that if, for example, f x s1 and
Ž .
2
b Ž .
Ž .
b Ž .
Ž .
b
2
2
g x sx , then H f x dg x sH f x g x dxsH 2 x dxsb ya .
a
a
a
It should be noted that Theorems 6.8.1 and 6.8.2 provide sufficient
b Ž .
Ž .
conditions for the existence of H f x dg x . It is possible, however, for the
a
Ž .
RiemannStieltjes integral to exist even if g x is a discontinuous function.
Ž .
Ž
.
For example, consider the function g x sI xyc , where  is a nonzero

RIEMANNSTIELTJES INTEGRAL
237
Ž
.
constant, acb, and I xyc is such that
0,
xc,
I xyc s
Ž
. ½ 1,
xGc.
Ž .
The quantity  represents what is called a jump at xsc. If f x is bounded
w
x
on a, b and is continuous at xsc, then
bf x
dg x s f c .
6.52
Ž .
Ž .
Ž .
Ž
.
H
a
Ž
.

4
To show the validity of formula 6.52 , let Ps x , x , . . . , x
be any partition
0
1
n
w
x
Ž
.
Ž
.
of
a, b . Then,  g sg x yg x
will be zero as long as
x c or
i
i
iy1
i
x
Gc. Suppose, therefore, that there exists a k, 1FkFn, such that
iy1
Ž
.
Ž
.
x
cFx . In this case, the sum S P, f, g
in formula
6.44
takes the
ky1
k
form
n
S P, f, g s
f t
 g s f t
.
Ž
.
Ž .
Ž
.
Ý
i
i
k
is1
It follows that


S P, f, g y f c
s 
f t
yf c
.
6.53
Ž
.
Ž .
Ž
.
Ž .
Ž
.
k
Ž .
Now, let 0 be given. Since f x is continuous at xsc, then there exists a
0 such that

f t
yf c

,
Ž
.
Ž .
k





if t yc . Thus if the norm  
of P is chosen so that  , then
k
p
p
S P, f, g y f c
.
6.54
Ž
.
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Equality 6.52 follows from comparing inequalities 6.45 and 6.54 .
It is now easy to show that if
,
aFxb,
g x s
Ž . ½ ,
xsb,
Ž .
and if f x is continuous at xsb, then
bf x
dg x s y f b .
6.55
Ž .
Ž .
Ž
. Ž .
Ž
.
H
a
The previous examples represent special cases of a class of functions
w
x
w
x
defined on a, b called step functions. These functions are constant on a, b

INTEGRATION
238
except for a finite number of jump discontinuities. We can generalize
Ž
.
formula 6.55 to this class of functions as can be seen in the next theorem.
Ž .
w
x
Theorem 6.8.4.
Let g x be a step function defined on a, b with jump
discontinuities at xsc , c , . . . , c , and ac c   c sb, such that
1
2
n
1
2
n
 ,
aFxc ,
° 1
1
 ,
c Fxc ,
2
1
2
.
.
~.
.
g x s
Ž .
.
.
 ,
c
Fxc ,
n
ny1
n
¢
,
xsc .
nq1
n
Ž .
w
x
If f x is bounded on a, b and is continuous at xsc , c , . . . , c , then
1
2
n
n
bf x
dg x s

y
f c
.
6.56
Ž .
Ž .
Ž
. Ž
.
Ž
.
Ý
H
iq1
i
i
a
is1
Proof. The proof can be easily obtained by first writing the integral in
Ž
.
formula 6.56 as
c
c
b
1
2
f x
dg x s
f x
dg x q
f x
dg x
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
c1
cn
q  q
f x
dg x .
6.57
Ž .
Ž .
Ž
.
H
cny1
Ž
.
Ž
.
If we now apply formula 6.55 to each integral in 6.57 we obtain
c1f x
dg x s  y
f c
,
Ž .
Ž .
Ž
. Ž
.
H
2
1
1
a
c2f x
dg x s  y
f c
,
Ž .
Ž .
Ž
. Ž
.
H
3
2
2
c1
...
cn f x
dg x s 
y
f c
.
Ž .
Ž .
Ž
. Ž
.
H
nq1
n
n
cny1
Ž
.
By adding up all these integrals we obtain formula 6.56 .

EXAMPLE 6.8.1.
One example of a step function is the greatest-integer
w x
function
x , which is defined as the greatest integer less than or equal to x.
Ž .
w
x
If f x is bounded on 0, n and is continuous at xs1, 2, . . . , n, where n is a

APPLICATIONS IN STATISTICS
239
positive integer, then by Theorem 6.8.4 we can write
n
n
w x
f x
d x s
f i .
6.58
Ž .
Ž .
Ž
.
Ý
H
0
is1
It follows that every finite sum of the form Ýn
a can be expressed as a
is1
i
w x
Ž .
RiemannStieltjes integral with respect to
x of a function f x continuous
w
x
Ž .
on 0, n such that f i sa , is1, 2, . . . , n. The RiemannStieltjes integral
i
has therefore the distinct advantage of making finite sums expressible as
integrals.
6.9. APPLICATIONS IN STATISTICS
Riemann integration plays an important role in statistical distribution theory.
Perhaps the most prevalent use of the Riemann integral is in the study of the
distributions of continuous random variables.
We recall from Section 4.5.1 that a continuous random variable X with a
Ž .
Ž
.
cumulative distribution function F x sP XFx is absolutely continuous if
Ž .
Ž .
F x
is differentiable. In this case, there exists a function f x
called the
Ž .
Ž .
density function of X such that F x sf x , that is,
x
F x s
f t
dt.
6.59
Ž .
Ž .
Ž
.
H
y
In general, if X is a continuous random variable, it need not be absolutely
continuous. It is true, however, that most common distributions that are
continuous are also absolutely continuous.
The probability distribution of an absolutely continuous random variable is
Ž
.
completely determined by its density function. For example, from 6.59 it
follows that
b
P aXb sF b yF a s
f x
dx.
6.60
Ž
.
Ž .
Ž .
Ž .
Ž
.
H
a
Note that the value of this probability remains unchanged if one or both of
w
x
the end points of the interval
a, b are included. This is true because the
probability assigned to these individual points is zero when X has a continu-
ous distribution. The mean  and variance 	 2 of X are given by

sE X s
xf x
dx,
Ž
.
Ž .
H
y

2
2
	 sVar X s
xy
f x
dx.
Ž
.
Ž
.
Ž .
H
y

INTEGRATION
240
Ž
.
In general, the kth central moment of X, denoted by 
ks1, 2, . . . , is
k
defined as

k
k
 sE
Xy
s
xy
f x
dx.
6.61
Ž
.
Ž
.
Ž .
Ž
.
H
k
y
We note that 	 2s . Similarly, the kth noncentral moment of X, denoted
2

 Ž
.
by 
ks1, 2, . . . , is defined as
k

 sE X k .
6.62
Ž
.
Ž
.
k
The first noncentral moment of X is its mean , while its first central
moment is equal to zero.
Ž .
We note that if the domain of the density function f x is infinite, then k
and 
 are improper integrals of the first kind. Therefore, they may or may
k

  k Ž .

Ž
not exist. If H
x
f x dx exists, then so does 
see Definition 6.5.2
y
k
.
concerning absolute convergence of improper integrals . The latter integral is
called the kth absolute moment and is denoted by  . If 
exists, then the
k
k
noncentral moments of order j for jFk exist also. This follows because of
the inequality

j
  k
x
F x
q1
if jFk,
6.63
Ž
.
  j
  k
 
  j
 
which is true because
x F x
if
x G1 and
x 1 if
x 1. Hence,

j
  k
Ž
.
x
F x
q1 for all x. Consequently, from 6.63 we obtain the inequality
 F q1,
jFk,
j
k
which implies that 
 exists for jFk. Since the central moment 
in
j
j
Ž
.
formula 6.61 is expressible in terms of noncentral moments of order j or
smaller, the existence of  also implies the existence of  .
j
j
EXAMPLE 6.9.1.
Consider a random variable X with the density function
1
f x s
,
yx.
Ž .
2
 1qx
Ž
.
Such a random variable has the so-called Cauchy distribution. Its mean 
does not exist. This follows from the fact that in order of  to exist, the two
limits in the following formula must exist:
1
x dx
1
x dx
0
b
s
lim
q
lim
.
6.64
Ž
.
H
H
2
2


1qx
1qx
a™
b™
ya
0

APPLICATIONS IN STATISTICS
241
1
0
2
2
b
2
Ž
.
Ž
.
Ž
.
But, H
x dxr 1qx
sy log 1qa
™y as a™, and H x dxr 1qx
ya
0
2
1
2

2
Ž
.
Ž
.
s log 1qb
™ as b™. The integral H
x dxr 1qx
is therefore
y
2
divergent, and hence  does not exist.
It should be noted that it would be incorrect to state that
a
1
x dx
s
lim
,
6.65
Ž
.
H
2

1qx
a™
ya
Ž
.
which is equal to zero. This is because the limits in 6.64 must exist for any a
Ž
.
and b that tend to infinity. The limit in formula 6.65 requires that asb.
Such a limit is therefore considered as a subsequential limit.
The higher-order moments of the Cauchy distribution do not exist either.
It is easy to verify that

k
1
x dx

 s
6.66
Ž
.
H
k
2

1qx
y
is divergent for kG1.
EXAMPLE 6.9.2.
Consider a random variable X that has the logistic
distribution with the density function
e x
f x s
,
yx.
Ž .
2
x
1qe
Ž
.
The mean of X is

x
xe
s
dx
H
2
x
y 1qe
Ž
.
u
1
s
log
du,
6.67
Ž
.
H ž
/
1yu
0
x Ž
x.
Ž
.
where use r 1qe
. We recognize the integral in
6.67
as being an
improper integral of the second kind with singularities at us0 and us1.
We therefore write
u
u
1r2
b
s lim
log
duq lim
log
du.
6.68
Ž
.
H
H
ž
/
ž
/
q
y
1yu
1yu
a™0
b™1
a
1r2

INTEGRATION
242
Thus,
1r2
s lim
u log uq 1yu log 1yu
Ž
.
Ž
.
a
q
a™0
b
q lim
u log uq 1yu log 1yu
Ž
.
Ž
.
1r2
y
b™1
s lim
b log bq 1yb log 1yb
Ž
.
Ž
.
y
b™1
y lim
a log aq 1ya log 1ya
.
6.69
Ž
.
Ž
.
Ž
.
q
a™0
Ž
.
By applying l’Hospital’s rule Theorem 4.2.6 we find that
lim
1yb log 1yb s lim a log as0.
Ž
.
Ž
.
y
q
b™1
a™0
We thus have
s lim
b log b y lim
1ya log 1ya
s0.
Ž
.
Ž
.
Ž
.
y
q
b™1
a™0
2
2
Ž
.
The variance 	
of X can be shown to be equal to  r3 see Exercise 6.24 .
6.9.1. The Existence of the First Negative Moment
of a Continuous Distribution
Ž .
Let X be a continuous random variable with a density function f x . By
Ž
y1.
definition, the first negative moment of X is E X
. The existence of such
a moment will be explored in this section.
The need to evaluate a first negative moment can arise in many practical
applications. Here are some examples.
EXAMPLE 6.9.3.
Let PP be a population with a mean  and a variance
	 2. The coefficient of variation is a measure of variation in the population


per unit mean and is equal to 	r  , assuming that 0. An estimate of
 
this ratio is sr y , where s and y are, respectively, the sample standard
deviation and sample mean of a sample randomly chosen from PP. If the
population is normally distributed, then y is also normally distributed and is
Ž
  .
Ž .
Ž
  .
statistically independent of s. In this case, E sr y
sE s E 1r y . The
Ž
  .
question now is whether E 1r y
exists or not.
Ž
.
EXAMPLE 6.9.4 Calibration or Inverse Regression .
Consider the simple
linear regression model
E y s q x.
6.70
Ž .
Ž
.
0
1
In most regression situations, the interest is in predicting the response y for

APPLICATIONS IN STATISTICS
243
ˆ
a given value of x. For this purpose we use the prediction equation ys q
ˆ
0
ˆ
ˆ
ˆ
 x, where 
and 
are the least-squares estimators of 
and  ,
1
0
1
0
1
Ž
. Ž
.
respectively. These are obtained from the data set
x , y , x , y
, . . . ,
1
1
2
2
Ž
.4
x , y
that results from running n experiments in which y is measured for
n
n
specified settings of
x. There are other situations, however, where the
interest is in predicting the value of x, say x , that corresponds to an
0
observed value of y, say y . This is an inverse regression problem known as
0
Ž
the calibration problem
see Graybill, 1976, Section 8.5; Montgomery and
.
Peck, 1982, Section 9.7 .
For example, in calibrating a new type of thermometer, n readings,
y , y , . . . , y ,
are
taken
at
predetermined
known
temperature
values,
1
2
n
Ž
x , x , . . . , x
these values are known by using a standard temperature
1
2
n
.
gauge . Suppose that the relationship between the x ’s and the y ’s is well
i
i
Ž
.
represented by the model in 6.70 . If a new reading y
is observed using the
0
new thermometer, then it is of interest to estimate the correct temperature
Ž
x
that is, the temperature on the standard gauge corresponding to the
0
.
observed temperature reading y
.
0
In another calibration problem, the date of delivery of a pregnant woman
can be estimated by the size y of the head of her unborn child, which can be
Ž
.
determined by a special electronic device
sonogram . If the relationship
between y and the number of days x left until delivery is well represented by
Ž
.
model 6.70 , then for a measured value of y, say y , it is possible to estimate
0
x , the corresponding value of x.
0
Ž
.
Ž
.
In general, from model 6.70 we have E y
s q x . If  0, we
0
0
1
0
1
can solve for x
and obtain
0
E y
y
Ž
.
0
0
x s
.
0
1
Hence, to estimate x
we use
0
ˆ
y y
y yy
0
0
0
x s
sxq
,
ˆ0
ˆ
ˆ


1
1
n
n
ˆ
ˆ
Ž
.
Ž
.
since  syy x, where xs 1rn Ý
x , ys 1rn Ý
y . If the response
0
1
is1
i
is1
i
2
ˆ
y is normally distributed with a variance 	 , then y and 
are statistically
1ˆ
Ž
independent. Since y
is also statistically independent of 
y
does not
0
1
0
.
belong to the data set used to estimate  , then the expected value of x
is
ˆ
1
0
given by
y yy
0
E x
sxqE
Ž
.
ˆ0
ž
/
ˆ1
1
sxqE y yy E
.
Ž
.
0
ž /
ˆ1
ˆ
Ž
.
Here again it is of interest to know if E 1r
exists.
1

INTEGRATION
244
Ž .
Now, suppose that the density function f x
of the continuous random
Ž
.
Ž .
variable X is defined on 0,  . Let us also assume that f x is continuous.
The expected value of Xy1 is
 f x
Ž .
y1
E X
s
dx.
6.71
Ž
.
Ž
.
H
x
0
This is an improper integral with a singularity at xs0. In particular, if
Ž .
Ž
y1.
f 0 0, then E X
does not exist, because
f x rx
Ž .
lim
sf 0 0.
Ž .
q
1rx
x™0
Ž Ž .
.
Ž
.
By Theorem 6.5.7, the integrals H
f x rx dx and H
dxrx are of the same
0
0
Ž .
kind. Since the latter is divergent, then so is the former. Note that if f x is
Ž
.
Ž .
Ž
y1.
defined on y,  and f 0 0, then E X
does not exist either. In this
case,


f x
f x
f x
Ž .
Ž .
Ž .
0
dxs
dxq
dx
H
H
H
x
x
x
y
y
0

f x
f x
Ž .
Ž .
0
sy
dxq
dx.
H
H
 x
x
y
0
Both integrals on the right-hand side are divergent.
Ž
y1.
A sufficient condition for the existence of E X
is given by the following
w
Ž
.x
theorem see Piegorsch and Casella 1985 :
Ž .
Theorem 6.9.1.
Let f x be a continuous density function for a random
Ž
.
variable X defined on 0,  . If
f x
Ž .
lim

for some 0,
6.72
Ž
.

q
x
x™0
Ž
y1.
then E X
exists.
Ž .

q
Proof. Since the limit of f x rx
is finite as x™0 , there exist finite
Ž .

constants M and 0 such that f x rx M if 0x. Hence,
f x
M 
Ž .


y1
dxM
x
dxs
.
H
H
x

0
0

APPLICATIONS IN STATISTICS
245
Thus,


f x
f x
f x
Ž .
Ž .
Ž .

y1
E X
s
dxs
dxq
dx
Ž
. H
H
H
x
x
x
0
0




M
1
M
1

q
f x
dxF
q
.

Ž .
H





It should be noted that the condition of Theorem 6.9.1 is not a necessary
Ž
.
one. Piegorsch and Casella
1985
give an example of a family of density
Ž
.
functions that all violate condition 6.72 , with some members having a finite
Ž
.
first negative moment and others not having one see Exercise 6.25 .
Ž .
Corollary 6.9.1.
Let f x be a continuous density function for a random
Ž
.
Ž .
Ž .
variable X defined on 0,  such that f 0 s0. If f 0 exists and is finite,
Ž
y1.
then E X
exists.
Proof. We have that
f x yf 0
f x
Ž .
Ž .
Ž .
f 0 s lim
s lim
.
Ž .
q
q
x
x
x™0
x™0
Ž
y1.
By applying Theorem 6.9.1 with s1 we conclude that E X
exists.

EXAMPLE 6.9.5.
Let X be a normal random variable with a mean  and
a variance 	 2. Its density function is given by
1
1
2
f x s
exp y
xy
,
yx.
Ž .
Ž
.
2
2
'
2	
2	
Ž .
Ž
y1.
In this example, f 0 0. Hence, E X
does not exist. Consequently,
Ž
  .
E 1r y
in Example 6.9.3 does not exist if the population PP is normally
 
distributed, since the density function of
y
is positive at zero. Also, in
ˆ
ˆ
Ž
.
Example 6.9.4, E 1r
does not exist, because 
is normally distributed if
1
1
the response y satisfies the assumption of normality.
EXAMPLE 6.9.6.
Let X be a continuous random variable with the density
function
1
nr2y1
yx r2
f x s
x
e
,
0x,
Ž .
nr2
! nr2 2
Ž
.

INTEGRATION
246
Ž
.
where n is a positive integer and ! nr2 is the value of the gamma function,
Heyx x nr2y1 dx. This is the density function of a chi-squared random variable
0
with n degrees of freedom.
Let us consider the limit
f x
1
Ž .
nr2yy1
yx r2
lim
s
lim x
e

nr2
q
q
x
! nr2 2
x™0
x™0
Ž
.
for 0. This limit exists and is equal to zero if nr2yy10, that is, if
Ž
.
Ž
y1.
n2 1q 2. Thus by Theorem 6.9.1, E X
exists if the number of
degrees of freedom exceeds 2.
Ž
.
More recently, Khuri and Casella 2002 presented several extensions and
Ž
.
generalizations of the results in Piegorsch and Casella
1985 , including a
Ž
y1.
necessary and sufficient condition for the existence of E X
.
6.9.2. Transformation of Continuous Random Variables
Ž .
Let Y be a continuous random variable with a density function f y . Let
Ž
.
Ž .
Ws Y , where  y is a function whose derivative exists and is continuous
Ž .
Ž .
on a set A. Suppose that   y 0 for all ygA, that is,  y
is strictly
Ž
.
monotone. We recall from Section 4.5.1 that the density function g w of W
is given by
y1
d
w
Ž
.
y1
g w sf 
w
,
wgB,
6.73
Ž
.
Ž
.
Ž
.
dw
y1Ž
.
where B is the image of A under  and ys
w is the inverse function
Ž .
of ws y . This result can be easily obtained by applying the change of
variables technique in Section 6.4.1. This is done as follows: We have that for
any w and w
such that w Fw ,
1
2
1
2
w2
P w FWFw
s
g w dw.
6.74
Ž
.
Ž
.
Ž
.
H
1
2
w1
Ž .
Ž .
If   y 0, then  y is strictly monotone increasing. Hence,
P w FWFw
sP y FYFy
,
6.75
Ž
.
Ž
.
Ž
.
1
2
1
2
Ž
.
Ž
.
where y and y
are such that w s y , w s y
. But
1
2
1
1
2
2
y2
P y FYFy
s
f y dy.
6.76
Ž
.
Ž .
Ž
.
H
1
2
y1
y1Ž
.
Ž
.
Let us now apply the change of variables ys
w to the integral in 6.76 .

APPLICATIONS IN STATISTICS
247
By Theorem 6.4.9 we have
y1
y
w
d
w
Ž
.
2
2
y1
f y dys
f 
w
dw.
6.77
Ž .
Ž
.
Ž
.
H
H
dw
y
w
1
1
Ž
.
Ž
.
From 6.75 and 6.76 we obtain
y1
w
d
w
Ž
.
2
y1
P w FWFw
s
f 
w
dw.
6.78
Ž
.
Ž
.
Ž
.
H
1
2
dw
w1
Ž .
Ž
.
Ž
.
On the other hand, if   y 0, then P w FWFw
sP y FYFy .
1
2
2
1
Consequently,
y1
P w FWFw
s
f y dy
Ž
.
Ž .
H
1
2
y2
y1
w
d
w
Ž
.
1
y1
s
f 
w
dw
Ž
.
H
dw
w2
y1
w
d
w
Ž
.
2
y1
sy
f 
w
dw.
6.79
Ž
.
Ž
.
H
dw
w1
Ž
.
Ž
.
By combining 6.78 and 6.79 we obtain
y1
w
d
w
Ž
.
2
y1
P w FWFw
s
f 
w
dw.
6.80
Ž
.
Ž
.
Ž
.
H
1
2
dw
w1
Ž
.
Ž
.
Ž
.
Formula 6.73 now follows from comparing 6.74 and 6.80 .
( )
The Case Where ws y Has No Unique In®erse
Ž
.
Ž .
Formula 6.73 requires that ws y has a unique inverse, the existence of
Ž .
which is guaranteed by the nonvanishing of the derivative   y . Let us now
Ž .
consider the following extension: The function  y is continuously differen-
tiable, but its derivative can vanish at a finite number of points inside its
Ž .
domain. We assume that the domain of  y can be partitioned into a finite
number, say n, of disjoint subdomains, denoted by I , I , . . . , I , on each of
1
2
n
Ž .
Ž
.
which  y is strictly monotone decreasing or increasing . Hence, on each Ii
Ž
.
Ž .
is1, 2, . . . , n ,  y has a unique inverse. Let  denote the restriction of
i
Ž .
y1Ž
.
the function 
to I , that is, 
y
has a unique inverse,
ys
w ,
i
i
i
is1, 2, . . . , n. Since I , I , . . . , I
are disjoint, for any w
and w
such that
1
2
n
1
2
w Fw
we have
1
2
n
y1
P w FWFw
s
P Yg
w , w
,
6.81
Ž
.
Ž
.
Ž
.
Ý
1
2
i
1
2
is1

INTEGRATION
248
y1Ž
.
w
x
where 
w , w
is the inverse image of w , w , which is a subset of I ,
i
1
2
1
2
i
is1, 2, . . . , n. Now, on the ith subdomain we have
y1
P Yg
w , w
s
f y dy
Ž
.
Ž .
H
i
1
2
y1Ž
.

w , w
i
1
2
y1
d
w
Ž
.
i
y1
s
f 
w
dw,
is1, 2, . . . , n,
Ž
.
H
i
dw
Ti
6.82
Ž
.
y1Ž
.
Ž
.
where T is the image of 
w , w
under  . Formula 6.82 follows from
i
i
1
2
i
Ž
.
Ž .
applying formula 6.80 to the function 
y , is1, 2, . . . , n. Note that T s
i
i
w
y1Ž
.x
w
x
Ž .
Ž
.
 
w , w
s w , w
l I , is1, 2, . . . , n why? . Thus T is a subset
i
i
1
2
1
2
i
i
i
w
x
Ž .
Ž
.
of both w , w
and  I . We can therefore write the integral in 6.82 as
1
2
i
i
y1
y1
w
d
w
d
w
Ž
.
Ž
.
2
i
i
y1
y1
f 
w
dws

w f 
w
dw,
6.83
Ž
.
Ž
.
Ž
.
Ž
.
H
H
i
i
i
dw
dw
T
w
i
1
Ž
.
Ž .
Ž
.
where  w s1 if wg I
and  w s0 otherwise, is1, 2, . . . , n. Using
i
i
i
i
Ž
.
Ž
.
Ž
.
6.82 and 6.83 in formula 6.81 , we obtain
y1
n
w
d
w
Ž
.
2
i
y1
P w FWFw
s

w f 
w
dw
Ž
.
Ž
.
Ž
.
Ý H
1
2
i
i
dw
w1
is1
y1
n
w
d
w
Ž
.
2
i
y1
s

w f 
w
dw,
Ž
.
Ž
.
Ý
H
i
i
dw
w1 is1
from which we deduce that the density function of W is given by
y1
n
d
w
Ž
.
i
y1
g w s

w f 
w
.
6.84
Ž
.
Ž
.
Ž
.
Ž
.
Ý
i
i
dw
is1
EXAMPLE 6.9.7.
Let Y have the standard normal distribution with the
density function
1
y2
f y s
exp y
,
yy.
Ž .
ž
/
'
2
2
Define the random variable W as WsY 2. In this case, the function wsy2

APPLICATIONS IN STATISTICS
249
Ž
.
has two inverse functions on y,  , namely,
'
y w ,
yF0,
ys½ 'w ,
y0.
Ž
x
Ž
.
Ž
.
w
.
Ž
.
Ž
.
Thus
I s y, 0 ,
I s 0,  , and

I
s 0,  , 
I
s 0,  . Hence,
1
2
1
1
2
2
Ž
.
Ž
.
Ž
.
Ž
.
 w s1, 
w s1 if wg 0,  . By applying formula 6.84 we then get
1
2
1
y1
1
1
yw r2
yw r2
g w s
e
q
e
Ž
.
'
'
'
'
2
2 w
2
2 w
1
eyw r2
s
,
w0.
'
'
2
w
This represents the density function of a chi-squared random variable with
one degree of freedom.
6.9.3. The Riemann–Stieltjes Integral Representation of the Expected Value
Ž .
Let X be a random variable with a cumulative distribution function F x .
Ž .
Ž .
Suppose that h x
is RiemannStieltjes integrable with respect to F x
on
Ž
.
Ž
.
y,  . Then the expected value of h X
is defined as

E h X
s
h x
dF x .
6.85
Ž
.
Ž .
Ž .
Ž
.
H
y
Ž
.
Formula 6.85 provides a unified representation of expected values for both
discrete and continuous random variables.
Ž .
If X is a continuous random variable with a density function f x , that is,
Ž .
Ž .
F x sf x , then

E h X
s
h x f x
dx.
Ž
.
Ž . Ž .
H
y
If, however, X has a discrete distribution with a probability mass function
Ž .
p x and takes the values c , c , . . . , c , then
1
2
n
n
E h X
s
h c
p c
.
6.86
Ž
.
Ž
. Ž
.
Ž
.
Ý
i
i
is1
Ž
.
Ž .
Formula 6.86 follows from applying Theorem 6.8.4. Here, F x
is a step

INTEGRATION
250
function with jump discontinuities at c , c , . . . , c
such that
1
2
n
0,
yxc
°
1
p c
,
c Fxc ,
Ž
.
1
1
2
p c
qp c
,
c Fxc ,
Ž
.
Ž
.
1
2
2
3
...~
F x s
Ž .
ny1
p c
,
c
Fxc ,
Ž
.
Ý
i
ny1
n
is1
n
p c
s1,
c Fx.
Ž
.
Ý
i
n
¢is1
Ž
.
Thus, by formula 6.56 we obtain
n

h x
dF x s
p c
h c
.
6.87
Ž .
Ž .
Ž
. Ž
.
Ž
.
Ý
H
i
i
y
is1
For example, suppose that X has the discrete uniform distribution with
Ž .
Ž .
p x s1rn for xsc , c , . . . , c . Its cumulative distribution function F c
1
2
n
can be expressed as
n
1
w
x
F x sP XFx s
I xyc
,
Ž .
Ž
.
Ý
i
n is1
Ž
.
where I xyc
is equal to zero if xc
and is equal to one if xGc
i
i
i
Ž
.
Ž
.
is1, 2, . . . , n . The expected value of h X
is
n
1
E h X
s
h c
.
Ž
.
Ž
.
Ý
i
n is1
Ž .
EXAMPLE 6.9.8.
The moment generating function  t
of a random
Ž .
variable X with a cumulative distribution function F x
is defined as the
Ž
.
t X
expected value of h
X se
, that is,
t

t X
t x
 t sE e
s
e
dF x ,
Ž .
Ž
.
Ž .
H
y
where t is a scalar. If X is a discrete random variable with a probability mass
Ž .
function p x and takes the values c , c , . . . , c , . . . , then by letting n go to
1
2
n
Ž
.
Ž
.
infinity in 6.87 we obtain see also Section 5.6.2

tci
 t s
p c
e
.
Ž .
Ž
.
Ý
i
is1

APPLICATIONS IN STATISTICS
251
The moment generating function of a continuous random variable with a
Ž .
density function f x is

t x
 t s
e f x
dx.
6.88
Ž .
Ž .
Ž
.
H
y
Ž
.
The convergence of the integral in 6.88 depends on the choice of the scalar
Ž
.
t. For example, for the gamma distribution G , 
with the density function
x y1 eyx r 
f x s
,
0,
0,
0x,
Ž .

!  
Ž
.
Ž .
 t is of the form

t x
y1
yx r 
e x
e
 t s
dx
Ž . H

!  
Ž
.
0
y1
 x
exp yx 1yt r
Ž
.
s
dx.
H

!  
Ž
.
0
Ž
.
If we set ysx 1yt r, we obtain
y1


 y
yy
 t s
e
dy.
Ž . H
 ž
/
1yt !  
1yt
Ž
.
Ž
.
0
Thus,

y1
yy
1
y
e
 t s
dy
Ž .
H

! 
Ž
.
1yt
Ž
.
0
y
s 1yt
,
Ž
.

yy
y1
Ž
.
since H e
y
dys!  by the definition of the gamma function. We note
0
Ž .
that  t exists for all values of  provided that 1yt0, that is, t1r.
6.9.4. Chebyshev’s Inequality
In Section 5.6.1 there was a mention of Chebyshev’s inequality. Using the
RiemannStieltjes integral representation of the expected value, it is now
possible to provide a proof for this important inequality.
Ž
.
Theorem 6.9.2.
Let X be a random variable discrete or continuous with
a mean  and a variance 	 2. Then, for any positive constant r,
1


P
Xy Gr	
F
.
Ž
.
2r

INTEGRATION
252
2
Ž
.
Ž
.2
Proof. By definition, 	
is the expected value of h X s Xy
. Thus,

2
2
	 s
xy
dF x ,
6.89
Ž
.
Ž .
Ž
.
H
y
Ž .
where F x is the cumulative distribution function of X. Let us now partition
Ž
.
Ž
x
Ž
.
y, 
into
three
disjoint
intervals:
y, yr	 ,
yr	 , qr	 ,
w
.
Ž
.
qr	 ,  . The integral in 6.89 can therefore be written as
yr	
qr 	
2
2
2
	 s
xy
dF x q
xy
dF x
Ž
.
Ž .
Ž
.
Ž .
H
H
y
yr 	

2
q
xy
dF x
Ž
.
Ž .
H
qr	
yr	

2
2
G
xy
dF x q
xy
dF x .
6.90
Ž
.
Ž .
Ž
.
Ž .
Ž
.
H
H
y
qr 	
Ž
.
We note that in the first integral in 6.90 , xFyr	 , so that xyFyr	 .
Ž
.2
2
2
Hence,
xy
Gr 	 . Also, in the second integral, xyGr	 . Hence,
Ž
.2
2
2
xy
Gr 	 . Consequently,
yr	
yr 	
2
2
2
2
2
xy
dF x Gr 	
dF x sr 	 P XFyr	 ,
Ž
.
Ž .
Ž .
Ž
.
H
H
y
y


2
2
2
2
2
xy
dF x Gr 	
dF x sr 	 P XGqr	 .
Ž
.
Ž .
Ž .
Ž
.
H
H
qr	
qr 	
Ž
.
From inequality 6.90 we then have
2
2
2
	 Gr 	
P XyFyr	
qP XyGr	
Ž
.
Ž
.
2
2


sr 	 P
Xy Gr	 ,
Ž
.
which implies that
1


P
Xy Gr	
F
.

Ž
.
2r
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
DeCani, J. S., and R. A. Stine 1986 . ‘‘A note on deriving the information matrix for
Ž
a logistic distribution.’’ Amer. Statist., 40, 220222.
This article uses calculus
techniques, such as integration and l’Hospital’srule, in determining the mean and
.
variance of the logistic distribution as was seen in Example 6.9.2.
Ž
.
Ž
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York. Chap. 5 discusses the
.
Riemann integral; Chap. 16 provides a study of improper integrals.

EXERCISES
253
Ž
.
Graybill, F. A. 1976 . Theory and Application of the Linear Model. Duxbury Press,
Ž
North Scituate, Massachusetts. Section 8.5 discusses the calibration problem for
.
a simple linear regression model as was seen in Example 6.9.4.
Ž
.
Hardy, G. H., J. E. Littlewood, and G. Polya 1952 . Inequalities, 2nd ed. Cambridge
´
Ž
University Press, Cambridge, England.
This is a classic and often referenced
.
book on inequalities. Chap. 6 is relevant to the present chapter.
Ž
.
Hartig, D.
1991 . ‘‘L’Hopital’s rule via integration.’’ Amer. Math. Monthly, 98,
ˆ
156157.
Ž
.
Khuri, A. I., and G. Casella
2002 . ‘‘The existence of the first negative moment
Ž
revisited.’’ Amer. Statist., 56, 4447. This article demonstrates the utility of the
comparison test given in Theorem 6.5.3 in showing the existence of the first
.
negative moment of a continuous random variable.
Ž
.
Ž
Lindgren, B. W.
1976 . Statistical Theory, 3rd ed. Macmillan, New York. Section
2.2.2 gives the RiemannStieltjes integral representation of the expected value of
.
a random variable as was seen in Section 6.9.3.
Ž
.
Montgomery, D. C., and E. A. Peck 1982 . Introduction to Linear Regression Analysis.
Ž
Wiley, New York. The calibration problem for a simple linear regression model
.
is discussed in Section 9.7.
Ž
.
Moran, P. A. P.
1968 . An Introduction to Probability Theory. Clarendon Press,
Ž
Oxford, England. Section 5.9 defines moments of a random variable using the
RiemannStieltjes integral representation of the expected value; Section 5.10
.
discusses a number of inequalities pertaining to these moments.
Ž
.
Piegorsch, W. W., and G. Casella
1985 . ‘‘The existence of the first negative
Ž
moment.’’ Amer. Statist., 39, 6062. This article gives a sufficient condition for
the existence of the first negative moment of a continuous random variable as
.
was seen in Section 6.9.1.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Ž
Reading, Massachusetts. Chap. 9 is concerned with transformations of continu-
ous random variables as was seen in Section 6.9.2. See, in particular, Theorems 2
.
and 3 in this chapter.
Ž
.
Taylor, A. E., and W. R. Mann 1972 . Ad®anced Calculus, 2nd ed. Wiley, New York.
ŽChap. 18 discusses the Riemann integral as well as the RiemannStieltjes
.
integral; improper integrals are studied in Chap. 22.
Ž
.
Ž
Wilks, S. S.
1962 . Mathematical Statistics. Wiley, New York.
Chap. 3 uses the
RiemannStieltjes integral to define expected values and moments of random
.
variables; functions of random variables are discussed in Section 2.8.
EXERCISES
In Mathematics
Ž .
w
x
6.1. Let f x be a bounded function defined on the interval a, b . Let P be
w
x
Ž .
w
x
a partition of a, b . Show that f x is Riemann integrable on a, b if
and only if
b
inf US
f s sup LS
f s
f x
dx.
Ž .
Ž .
Ž .
H
P
P
P
a
P

INTEGRATION
254
6.2. Construct a function that has a countable number of discontinuities in
w
x
w
x
0, 1 and is Riemann integrable on 0, 1 .
Ž .
w
x
6.3. Show that if f x is continuous on a, b except for a finite number of
Ž
.
Ž .
discontinuities of the first kind
see Definition 3.4.2 , then f x
is
w
x
Riemann integrable on a, b .
6.4. Show that the function
x cos r2 x ,
0xF1,
Ž
.
f x s
Ž . ½ 0,
xs0,
w
x
is not of bounded variation on 0, 1 .
Ž .
Ž .
Ž .
6.5. Let f x and g x have continuous derivatives with g x 0. Suppose
Ž .
Ž .
Ž .
Ž .
that lim
f x s, lim
g x s, and lim
f x rg x sL,
x™
x™
x™
where L is finite.
( )
a
Show that for a given 0 there exists a constant M0 such that
for xM,
f x yLg x
 g x .
Ž .
Ž .
Ž .
Hence, if  and 
are such that M  , then
1
2
1
2


2
2
f x yLg x
dx 
 g x
dx.
Ž .
Ž .
Ž .
H
H


1
1
( )
Ž .
b
Deduce from a that
f 
f 
g 
Ž
.
Ž
.
Ž
.
2
1
1


yL q
q L
.
g 
g 
g 
Ž
.
Ž
.
Ž
.
2
2
2
( )
Ž .
c
Make use of b to show that for a sufficiently large  ,
2
f 
Ž
.
2
yL 3 ,
g 
Ž
.
2
Ž .
Ž .
and hence lim
f x rg x sL.
x™
w Note: This problem verifies l’Hospital’srule for the r indeterminate
form by using integration properties without relying on Cauchy’s mean
Ž
.x
value theorem as in Section 4.2 see Hartig, 1991 .
Ž .
w
x
Ž .
6.6. Show that if f x is continuous on a, b , and if g x is a nonnegative
w
x
b Ž .
Riemann integrable function on
a, b
such that H g x dx0, then
a

EXERCISES
255
there exists a constant c, aFcFb, such that
H bf x g x
dx
Ž . Ž .
a
sf c .
Ž .
b
H g x
dx
Ž .
a
6.7. Prove Theorem 6.5.2.
6.8. Prove Theorem 6.5.3.
Ž .
6.9. Suppose that f x is a positive monotone decreasing function defined
w
.
Ž .
Ž .
on
a, 
such
that
f x ™0
as
x™.
Show
that
if
f x
is
Ž .
w
x
RiemannStieltjes integrable with respect to g x
on
a, b for every
Ž .
w
.
 Ž .
Ž .
bGa, where g x is bounded on a,  , then the integral H f x dg x is
a
convergent.
n Ž
.

6.10. Show that lim
H
sin x rx dxs, where n is a positive integer.
n™
0
w Hint: Show first that
n

sin x
1
1
1
x
dxs
sin x
q
q  q
dx.
H
H
x
x
xq
xq ny1 
Ž
.
0
0
6.11. Apply Maclaurin’s integral test to determine convergence or divergence
of the following series:

log n
a
,
( )
Ý
'
n n
ns1

nq4
b
,
( )
Ý
3
2n q1
ns1

1
c
.
( )
Ý 'nq1 y1
ns1

Ž .4
Ž .
Ž
2.
6.12. Consider the sequence
f
x
, where f
x snxr 1qnx
, xG0.
n
ns1
n
2
Ž .
Find the limit of H f
x dx as n™.
1
n
1
my1Ž
.ny1
6.13. Consider the improper integral H x
1yx
dx.
0

INTEGRATION
256
( )
a
Show that the integral converges if m0, n0. In this case, the
Ž
.
function B m, n defined as
1
ny1
my1
B m, n s
x
1yx
dx
Ž
.
Ž
.
H
0
is called the beta function.
( )
b
Show that
r2
2 my1
2 ny1
B m, n s2
sin
 cos
 d
Ž
.
H
0
( )
c
Show that

my1

ny1
x
x
B m, n s
dxs
dx
Ž
. H
H
mqn
mqn
1qx
1qx
Ž
.
Ž
.
0
0
( )
d
Show that
x my1 qx ny1
1
B m, n s
dx.
Ž
. H
mqn
1qx
Ž
.
0
6.14. Determine whether each of the following integrals is convergent or
divergent:

dx
a
,
( )
H
3
'
0
1qx

dx
b
,
( )
H
1r3
3
0
1qx
Ž
.
dx
1
c
,
( )
H
1r3
3
0
1yx
Ž
.

dx
d
.
( )
H 'x 1q2 x
Ž
.
0
Ž .
Ž .
w
x
Ž .
6.15. Let f
x
and f
x
be bounded on
a, b , and g x
be monotone
1
2
w
x
Ž .
Ž .
increasing on a, b . If f
x and f
x are RiemannStieltjes integrable
1
2
Ž .
w
x
Ž .
Ž .
with respect to g x
on
a, b , then show that
f
x f
x
is also
1
2
Ž .
w
x
RiemannStieltjes integrable with respect to g x on a, b .

EXERCISES
257
Ž .
6.16. Let f x
be a function whose first n derivatives are continuous on
w
x
a, b , and let
ny1
byx
Ž
.
Žny1.
h
x sf b yf x y byx f x y  y
f
x .
Ž .
Ž .
Ž .
Ž
.
Ž .
Ž .
n
ny1 !
Ž
.
Show that
1
b
ny1
Žn.
h
a s
byx
f
x
dx
Ž .
Ž
.
Ž .
H
n
ny1 !
Ž
.
a
and hence
ny1
bya
Ž
.
Žny1.
f b sf a q bya f a q  q
f
a
Ž .
Ž .
Ž
.
Ž .
Ž .
ny1 !
Ž
.
1
b
ny1
Žn.
q
byx
f
x
dx.
Ž
.
Ž .
H
ny1 !
Ž
.
a
Ž .
Ž
This represents Taylor’s expansion of f x
around xsa see Section
.
4.3 with a remainder R
given by
n
1
b
ny1
Žn.
R s
byx
f
x
dx.
Ž
.
Ž .
H
n
ny1 !
Ž
.
a
w Note: This form of Taylor’s theorem has the advantage of providing an
exact formula for R , which does not involve an undetermined number
n
x

as was seen in Section 4.3.
n
Ž .
Ž .
6.17. Suppose that f x
is monotone and its derivative f x
is Riemann
w
x
Ž .
w
x
integrable on a, b . Let g x be continuous on a, b . Show that there
exists a number c, aFcFb, such that
c
b
b
f x g x
dxsf a
g x
dxqf b
g x
dx.
Ž . Ž .
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
c
6.18. Deduce from Exercise 6.17 that for any ba0,
sin x
4
b
dx F
.
H
x
a
a
6.19. Prove Lemma 6.7.1.

INTEGRATION
258
Ž .
Ž .
Ž .
 Ž . p
6.20. Show that if f
x , f
x , . . . , f
x
are such that
f x
is Riemann
1
2
n
i
w
x
integrable on a, b for is1, 2, . . . , n, where 1Fp, then
1rp
p
1rp
n
n
b
b
p
f
x
dx
F
f
x
dx
.
Ž .
Ž .
Ý
Ý
H
H
i
i
a
a
is1
is1
6.21. Prove Theorem 6.8.1.
6.22. Prove Theorem 6.8.2.
In Statistics
Ž
.
6.23. Show that the integral in formula 6.66 is divergent for kG1.
6.24. Consider the random variable X that has the logistic distribution
Ž
.
2
described in Example 6.9.2. Show that Var X s r3.

Ž .4
6.25. Let
f
x
be a family density functions defined by
n
ns1

n
 y1
log
x
f
x s
,
0x,
Ž .
n
y1

n


H
log t
dt
0
Ž
.
where g 0, 1 .
( )
Ž
.
a
Show that condition 6.72 of Theorem 6.9.1 is not satisfied by any
Ž .
f
x , nG1.
n
( )
Ž
y1.
b
Show that when ns1, E X
does not exist, where X is a
Ž .
random variable with the density function f
x .
1
( )
Ž
y1.
c
Show that for n1, E X
exists, where X
is a random variable
n
n
Ž .
with the density function f
x .
n
Ž .
6.26. Let X be a random variable with a continuous density function f x on
Ž
.
Ž .
Ž
y .
0,  . Suppose that f x
is bounded near zero. Then E X
exits,
Ž
.
where g 0, 1 .
Ž .
6.27. Let X be a random variable with a continuous density function f x on
Ž
.
Ž Ž .
.
q
0,  . If lim
f x rx
is equal to a positive constant k for some
x™0
w
yŽ1q .x
0, then E X
does not exist.

EXERCISES
259
6.28. The random variable Y
has the t-distributions with n degrees of
freedom. Its density function is given by
nq1
Ž
.
y nq1 r2
!
2
ž
/
y
2
f y s
1q
,
yy,
Ž .
n ž
/
n
'n !ž /
2
Ž
.
 yx
my1
where ! m
is the gamma function H e
x
dx, m0. Find the
0


density function of Ws Y .
6.29. Let X be a random variable with a mean  and a variance 	 2.
( )
a
Show that Chebyshev’s inequality can be expressed as
	 2


P
Xy Gr F
,
Ž
.
2r
where r is any positive constant.
( )

4
b
Let
X
be a sequence of independent and identically dis-
n ns1
tributed random variables. If the common mean and variance of
the X ’s are  and 	 2, respectively, then show that
i
	 2


P
X y Gr F
,
Ž
.
n
2
nr
n
Ž
.
where X s 1rn Ý
X and r is any positive constant.
n
is1
i
( )
Ž .
c
Deduce from b that X
converges in probability to  as n™,
n
that is, for every 0,


P
X y G ™0
as n™.
Ž
.
n
6.30. Let X be a random variable with a cumulative distribution function
Ž .

F x . Let 
be its kth noncentral moment,
k


k
k
 sE X
s
x dF x .
Ž
.
Ž .
H
k
y
Let 
be the kth absolute moment of X,
k

k
k


 
 sE
X
s
x
dF x .
Ž .
Ž
. H
k
y
Suppose that 
exists for ks1, 2, . . . , n.
k

INTEGRATION
260
( )
2
w
a
Show that  F

, ks1, 2, . . . , ny1.
Hint: For any u
k
ky1
kq1
and ®,

2
Žky1. r2
Žkq1. r2
 
 
0F
u x
q® x
dF x
Ž .
H
y
2
2
x
su 
q2u® q® 
.
ky1
k
kq1
( )
Ž .
b
Deduce from a that
 F 1r2 F 1r3 F  F 1r n.
1
2
3
n

C H A P T E R
7
Multidimensional Calculus
In the previous chapters we have mainly dealt with real-valued functions of a
single variable x. In this chapter we extend the notions of limits, continuity,
differentiation, and integration to multivariable functions, that is, functions
of several variables. These functions can be real-valued or possibly vector-val-
ued. More specifically, if Rn denotes the n-dimensional Euclidean space,
nG1, then we shall in general consider functions defined on a set D;Rn
and have values in Rm, mG1. Such functions are represented symbolically as
m
Ž
.
f: D™R , where for xs x , x , . . . , x
gD,
1
2
n
f x s f
x , f
x , . . . , f
x

Ž .
Ž .
Ž .
Ž .
1
2
m
Ž .
Ž
.
and f x is a real-valued function of x , x , . . . , x
is1, 2, . . . , m .
i
1
2
n
Even though the basic framework of the methodology in this chapter is
general and applies in any number of dimensions, most of the examples are
associated with two- or three-dimensional spaces. At this stage, it would be
helpful to review the basic concepts given in Chapters 1 and 2. This can
facilitate the understanding of the methodology and its development in a
multidimensional environment.
7.1. SOME BASIC DEFINITIONS
Some of the concepts described in Chapter 1 pertained to one-dimensional
Euclidean spaces. In this section we extend these concepts to higher-dimen-
sional Euclidean spaces.
Any point x in Rn can be represented as a column vector of the form
Ž
.
Ž
.
x , x , . . . , x
, where
x
is the ith element of x
is1, 2, . . . , n . The
1
2
n
i
Ž
.
Euclidean norm of x was defined in Chapter 2
see Definition 2.1.4
as
	 	
Ž
n
2.1r2
x
s Ý
x
. For simplicity we shall drop the subindex 2 and denote
2
is1
i
	 	
this norm by x .
261

MULTIDIMENSIONAL CALCULUS
262
n
Ž
.
n
Let x gR . A neighborhood N x
of x
is a set of points in R
that lie
0
r
0
0
within some distance, say r, from x , that is,
0
n 	
	
N x
s xgR
xyx
r .

4
Ž
.
r
0
0
Ž
.
If x
is deleted from N x
, we obtain the so-called deleted neighborhood of
0
r
0
dŽ
.
x , which we denote by N
x
.
0
r
0
A point x
in Rn is a limit point of a set A;Rn if every neighborhood of
0
x
contains an element x of
A such that xx , that is, every deleted
0
0
neighborhood of x
contains points of A.
0
A set A;Rn is closed if every limit point of A belongs to A.
A point x
in Rn is an interior of a set A;Rn if there exists an r0 such
0
Ž
.
that N x
;A.
r
0
A set A;Rn is open if for every point x in A there exists a neighborhood
Ž .
N x that is contained in A. Thus A is open if it consists entirely of interior
r
points.
A point pgRn is a boundary point of a set A;Rn if every neighborhood
of p contains points of A as well as points of A, the complement of A with
respect to Rn. The set of all boundary points of A is called its boundary and
Ž
.
is denoted by Br A .
n
	 	
A set A;R
is bounded if there exists an r0 such that x Fr for all x
in A.
Let g: Jq™Rn be a vector-valued function defined on the set of all
Ž .

4
positive integers. Let g i sa , iG1. Then a
represents a sequence of
i
i is1
n

4

4
points in R . By a subsequence of a
we mean a sequence a
such
i is1
k
is1
i
Ž
.
that k k   k   and k Gi for iG1 see Definition 5.1.1 .
1
2
i
i

4
n
A sequence a
converges to a point cgR
if for a given 0 there
i is1
	
	
exists an integer N such that
a yc  whenever iN. This is written
i
symbolically as lim
a sc, or a ™c as i™.
i™
i
i

4
A sequence a
is bounded if there exists a number K0 such that
i is1
	
	
a
FK for all i.
i
7.2. LIMITS OF A MULTIVARIABLE FUNCTION
We recall from Chapter 3 that for a function of a single variable x, its limit at
a point is considered when x approaches the point from two directions, left
and right. Here, for a function of several variables, say x , x , . . . , x , its limit
1
2
n
Ž
.
Ž
.
at a point as a , a , . . . , a  is considered when xs x , x , . . . , x
 ap-
1
2
n
1
2
n
proaches a in any possible way. Thus when n1 there are infinitely many
ways in which x can approach a.
m
n
Ž .
Definition 7.2.1.
Let f: D™R , where D;R . Then f x is said to have
Ž
.
a limit Ls L , L , . . . , L
 as x approaches a, written symbolically as x™a,
1
2
m
where a is a limit point of D, if for a given 0 there exists a 0 such

LIMITS OF A MULTIVARIABLE FUNCTION
263
	 Ž .
	
dŽ .
dŽ .
that f x yL  for all x in DlN
a , where N
a is a deleted neighbor-


hood of a of radius . If it exists, this limit is written symbolically as
Ž .
lim
f x sL.

x ™a
Ž .
Note that whenever a limit of f x exists, its value must be the same no
matter how x approaches a. It is important here to understand the meaning
of ‘‘x approaches a.’’By this we do not necessarily mean that x moves along a
straight line leading into a. Rather, we mean that x moves closer and closer
to a along any curve that goes through a.
EXAMPLE 7.2.1.
Consider the behavior of the function
x 3yx 3
1
2
f x , x
s
Ž
.
1
2
2
2
x qx
1
2
Ž
.
Ž
.
as xs x , x
™0, where 0s 0, 0 . This function is defined everywhere in
1
2
R2 except at 0. It is convenient here to represent the point x using polar
coordinates, r and , such that x sr cos , x sr sin , r0, 0FF2.
1
2
We then have
r 3 cos3 yr 3 sin3 
f x , x
s
Ž
.
1
2
2
2
2
2
r cos qr sin 
sr cos3 ysin3  .
Ž
.
Ž
.
Since x™0 if and only if r™0, lim
f x , x
s0 no matter how x
x ™0
1
2
approaches 0.
EXAMPLE 7.2.2.
Consider the function
x x
1
2
f x , x
s
.
Ž
.
1
2
2
2
x qx
1
2
Using polar coordinates again, we obtain
f x , x
scos  sin  ,
Ž
.
1
2
which depends on , but not on r. Since  can have infinitely many values,
Ž
.
f x , x
cannot be made close to any one constant L no matter how small r
1
2
is. Thus the limit of this function does not exist as x™0.

MULTIDIMENSIONAL CALCULUS
264
Ž
.
EXAMPLE 7.2.3.
Let f x , x
be defined as
1
2
x
x 2qx 2
Ž
.
2
1
2
f x , x
s
.
Ž
.
1
2
2
2
2
2
x q x qx
Ž
.
2
1
2
2
Ž
.
This function is defined everywhere in R
except at
0, 0 . On the line
Ž
.
3 Ž
2
4.
x s0, f 0, x
sx r x qx
, which goes to zero as x ™0. When x s0,
1
2
2
2
2
2
2
Ž
.
Ž
.
f x , 0 s0 for x 0; hence, f x , 0 ™0 as x ™0. Furthermore, for any
1
1
1
1
Ž
.
other straight line x stx
t0 through the origin we have
2
1
tx
x 2qt 2 x 2
Ž
.
1
1
1
f x , tx
s
,
Ž
.
1
1
2
2
2
2
2
2
t x q x qt x
Ž
.
1
1
1
tx
1qt 2
Ž
.
1
s
,
x 0,
1
2
2
2
2
t qx
1qt
Ž
.
1
which has a limit equal to zero as x ™0. We conclude that the limit of
1
Ž
.
f x , x
is zero as x™0 along any straight line through the origin. However,
1
2
Ž
.
f x , x
does not have a limit as x™0. For example, along the circle
1
2
x sx 2qx 2 that passes through the origin,
2
1
2
2
2
2
x qx
1
Ž
.
1
2
2
2
f x , x
s
s
,
x qx 0.
Ž
.
1
2
1
2
2
2
2
2
2
2
2
x qx
q x qx
Ž
.
Ž
.
1
2
1
2
1
Ž
.
Hence, f x , x
™0.
1
2
2
This example demonstrates that a function may not have a limit as x™a
even though its limit exists for approaches toward a along straight lines.
7.3. CONTINUITY OF A MULTIVARIABLE FUNCTION
The notion of continuity for a function of several variables is much the same
as that for a function of a single variable.
m
n
Ž .
Definition 7.3.1.
Let f: D™R , where D;R , and let agD. Then f x
is continuous at a if
lim f x sf a ,
Ž .
Ž .
x™a
where x remains in D as it approaches a. This is equivalent to stating that for
a given 0 there exits a 0 such that
	
	
f x yf a

Ž .
Ž .
Ž .
for all xgDlN a .


CONTINUITY OF A MULTIVARIABLE FUNCTION
265
Ž .
If f x is continuous at every point x in D, then it is said to be continuous
Ž .
Ž
in D. In particular, if f x is continuous in D and if 
in the definition of
.
Ž
continuity depends only on 
that is,  is the same for all points in D for
.
Ž .
the given  , then f x is said to be uniformly continuous in D.

We now present several theorems that provide some important properties
of multivariable continuous functions. These theorems are analogous to those
Ž
given in Chapter 3. Let us first consider the following lemmas the proofs are
.
left to the reader :
Lemma 7.3.1.
Every bounded sequence in Rn has a convergent subse-
quence.
This lemma is analogous to Theorem 5.1.4.
Lemma 7.3.2.
Suppose that f, g: D™R are real-valued continuous func-
tions, where D;Rn. Then we have the following:
1. fqg, fyg, and fg are continuous in D.
 
2.
f is continuous in D.
Ž .
3. 1rf is continuous in D provided that f x 0 for all x in D.
This lemma is analogous to Theorem 3.4.1.
Lemma 7.3.3.
Suppose that f: D™Rm is continuous, where D;Rn, and
that g: G™R is also continuous, where G;Rm is the image of D under f.

Ž .
w Ž .x
Then the composite function gf: D™R , defined as gf x sg f x , is
continuous in D.
This lemma is analogous to Theorem 3.4.2.
Theorem 7.3.1.
Let
f: D™R be a real-valued continuous function
defined on a closed and bounded set D;Rn. Then there exist points p and q
in D for which
f p s sup f x ,
7.1
Ž .
Ž .
Ž
.
xgD
f q s inf f x .
7.2
Ž .
Ž .
Ž
.
xgD
Ž .
Thus f x attains each of its infimum and supremum at least once in D.
Ž .
Proof. Let us first show that f x is bounded in D. We shall prove this by
Ž .
contradiction. Suppose that f x is not bounded in D. Then we can find a

4
 Ž
.
sequence of points
p
in D such that
f p
Gi for iG1 and hence
i is1
i

MULTIDIMENSIONAL CALCULUS
266
 Ž
.
f p
™ as i™. Since the terms of this sequence are elements in a
i

4
bounded set,
p
must be a bounded sequence. By Lemma 7.3.1, this
i is1

4
sequence has a convergent subsequence p
. Let p
be the limit of this
k
is1
0
i
subsequence, which is also a limit point of D; hence, it belongs to D, since D
 Ž
.
 Ž
.
is closed. Now, on one hand, f p
™f p
as i™, by the continuity of
k
0
i
Ž .
 Ž . w
Ž .x
 Ž
.
f x and hence of f x
see Lemma 7.3.2 2 . On the other hand, f p
™.
k i
Ž .
This contradiction shows that f x must be bounded in D. Consequently, the
Ž .
infimum and supremum of f x in D are finite.
Ž
.
Ž .
Suppose now equality 7.1 does not hold for any pgD. Then, Myf x 0
Ž .
w
Ž .xy1
for all xgD, where Mssup
f x . Consequently, Myf x
is positive
x g D
Ž .
and continuous in D by Lemma 7.3.2 3 and is therefore bounded by the first
half of this proof. However, if 0 is any given positive number, then, by the
Ž
.
definition of M, we can find a point x
in D for which f x
My, or


1
1

.
Myf x

Ž
.

w
Ž .xy1
This implies that
Myf x
is not bounded, a contradiction, which proves
Ž
.
Ž
.
equality 7.1 . The proof of equality 7.2 is similar.

Theorem 7.3.2.
Suppose that D is a closed and bounded set in Rn. If
f: D™Rm is continuous, then it is uniformly continuous in D.
Proof. We shall prove this theorem by contradiction. Suppose that f is not
uniformly continuous in D. Then there exists an 0 such that for every
	
	
	 Ž .
Ž .	
0 we can find a and b in D such that
ayb , but f a yf b
G.
Let us choose
s1ri,
iG1. We can therefore find two sequences

4

4
	
	
a
, b
with a , b gD such that
a yb 1ri, and
i is1
i is1
i
i
i
i
	
	
f a
yf b
G
7.3
Ž
.
Ž
.
Ž
.
i
i

4
for iG1. Now, the sequence a
is bounded. Hence, by Lemma 7.3.1, it
i is1

4
has a convergent subsequence a
whose limit, denoted by a , is in D,
k
is1
0
i
since D is closed. Also, since f is continuous at a , we can find a 0 such
0
	 Ž .
Ž
.	
	
	
that f x yf a
r2 if
xya
, where xgD. By the convergence of
0
0

4
a
to a , we can choose k large enough so that
k
is1
0
i
i
1


7.4
Ž
.
k
2
i
and

	
	
a ya

.
7.5
Ž
.
k
0
i
2

DERIVATIVES OF A MULTIVARIABLE FUNCTION
267
Ž
.
From 7.5 it follows that

	
	
f a
yf a

.
7.6
Ž
.
Ž
.
Ž
.
k
0
i
2
	
	
Furthermore, since
a yb
1rk , we can write
k
k
i
i
i
	
	
	
	
	
	
b ya
F a ya
q a yb
k
0
k
0
k
k
i
i
i
i

1

q
.
2
ki
Hence, by the continuity of f at a ,
0

	
	
f b
yf a

.
7.7
Ž
.
Ž
.
Ž
.
k
0
i
2
Ž
.
Ž
.
From inequalities
7.6
and
7.7
we conclude that whenever k
satisfies
i
Ž
.
Ž
.
inequalities 7.4 and 7.5 ,
	
	
	
	
	
	
f a
yf b
F f a
yf a
q f b
yf a
 ,
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
k
k
k
0
k
0
i
i
i
i
Ž
.
which contradicts inequality 7.3 . This leads us to assert that f is uniformly
continuous in D.

7.4. DERIVATIVES OF A MULTIVARIABLE FUNCTION
In this section we generalize the concept of differentiation given in Chapter 4
to a multivariable function f: D™Rm, where D;Rn.
Ž
.
Let as a , a , . . . , a  be an interior point of D. Suppose that the limit
1
2
n
f a , a , . . . , a qh , . . . , a
yf a , a , . . . , a , . . . , a
Ž
.
Ž
.
1
2
i
i
n
1
2
i
n
lim
h
h ™0
i
i
exists; then f is said to have a partial derivative with respect to x at a. This
i
Ž .
Ž .
derivative is denoted by " f a r" x , or just f
a , is1, 2, . . . , n. Hence, partial
i
x i
differentiation with respect to x is done in the usual fashion while treating
i
all the remaining variables as constants. For example, if f: R3™R is defined

MULTIDIMENSIONAL CALCULUS
268
Ž
.
2
3
3
as f x , x , x
sx x qx x , then at any point xgR
we have
1
2
3
1
2
2
3
" f x
Ž .
2
sx ,
2
" x1
" f x
Ž .
3
s2 x x qx ,
1
2
3
" x2
" f x
Ž .
2
s3x x .
2
3
" x3
Ž
.
In general, if f
is the jth element of f
js1, 2, . . . , m , then the terms
j
Ž .
" f x r" x , for is1, 2, . . . , n; js1, 2, . . . , m, constitute an mn matrix
j
i
Ž
.
called the Jacobian matrix after Carl Gustav Jacobi, 18041851 of f at x and
Ž .
Ž .
is denoted by J x . If msn, the determinant of J x is called the Jacobian
f
f
determinant; it is sometimes represented as
"
f , f , . . . , f
Ž
.
1
2
n
det J
x
s
.
7.8
Ž .
Ž
.
f
" x , x , . . . , x
Ž
.
1
2
n
For example, if f: R3™R2 is such that
f
x , x , x
s x 2 cos x , x 2qx 2 e x1 ,
Ž
.
Ž
.
1
2
3
1
2
2
3
then
2
2 x cos x
yx sin x
0
1
2
1
2
J
x , x , x
s
.
Ž
.
f
1
2
3
2
x
x
1
1
x e
2 x
2 x e
3
2
3
Higher-order partial derivatives of f are defined similarly. For example, the
second-order partial derivative of f with respect to x at a is defined as
i
f
a , a , . . . , a qh , . . . , a
yf
a , a , . . . , a , . . . , a
Ž
.
Ž
.
x
1
2
i
i
n
x
1
2
i
n
i
i
lim
h
h ™0
i
i
2 Ž .
2
Ž .
and is denoted by " f a r" x , or f
a . Also, the second-order partial
i
x x
i
i
derivative of f with respect to x and x , ij, at a is given by
i
j
f
a , a , . . . , a qh , . . . , a
yf
a , a , . . . , a , . . . , a
Ž
.
Ž
.
x
1
2
j
j
n
x
1
2
j
n
i
i
lim
h
h ™0
j
j
2 Ž .
Ž .
and is denoted by " f a r" x " x , or f
a , ij.
j
i
x x
j
i

DERIVATIVES OF A MULTIVARIABLE FUNCTION
269
Under certain conditions, the order in which differentiation with respect
Ž .
Ž .
to x and x
takes place is irrelevant, that is, f
a is identical to f
a ,
i
j
x x
x x
i
j
j
i
ij. This property is known as the commutative property of partial differen-
tiation and is proved in the next theorem.
Theorem 7.4.1.
Let f: D™Rm, where D;Rn, and let a be an interior
point of D. Suppose that in a neighborhood of a the following conditions are
satisfied:
Ž .
Ž .
Ž
.
1. " f x r" x and " f x r" x exist and are finite i, js1, 2, . . . , n, ij .
i
j
2 Ž .
2 Ž .
2. Of the derivatives " f x r" x " x , " f x r" x " x
one exists and is
i
j
j
i
continuous.
Then
" 2f a
" 2f a
Ž .
Ž .
s
.
" x " x
" x " x
i
j
j
i
2 Ž .
Proof. Let us suppose that " f x r" x " x exists and is continuous in a
j
i
neighborhood of a. Without loss of generality we assume that ij. If
2 Ž .
" f a r" x " x exists, then it must be equal to the limit
i
j
f
a , a , . . . , a qh , . . . , a
yf
a , a , . . . , a , . . . , a
Ž
.
Ž
.
x
1
2
i
i
n
x
1
2
i
n
j
j
lim
,
h
h ™0
i
i
that is,
1
1
lim
lim
f a , a , . . . , a qh , . . . , a qh , . . . , a
 Ž
.
1
2
i
i
j
j
n
h
h
h ™0
h ™0
i
j
i
j
yf a , a , . . . , a qh , . . . , a , . . . , a
4
Ž
.
1
2
i
i
j
n
1
y lim
f a , a , . . . , a , . . . , a qh , . . . , a
 Ž
.
1
2
i
j
j
n
h
h ™0
j
j
yf a , a , . . . , a , . . . , a , . . . , a
.
7.9
Ž
.
Ž
.
4
1
2
i
j
n
Ž
.
Ž
.
Let
us
denote
f x , x , . . . , x qh , . . . , x
yf x , x , . . . , x , . . . , x
by
1
2
j
j
n
1
2
j
n
Ž
.
Ž
.
	 x , x , . . . , x
. Then the double limit in 7.9 can be written as
1
2
n
1
lim
lim
	 a , a , . . . , a qh , . . . , a , . . . , a
Ž
.
1
2
i
i
j
n
h h
h ™0 h ™0
i
j
i
j
y	 a , a , . . . , a , . . . , a , . . . , a
Ž
.
1
2
i
j
n
1
" 	 a , a , . . . , a q h , . . . , a , . . . , a
Ž
.
1
2
i
i
i
j
n
s lim
lim
,
7.10
Ž
.
h
" x
h ™0 h ™0
i
j
j
i

MULTIDIMENSIONAL CALCULUS
270
Ž
.
where 0 1. In formula 7.10 we have applied the mean value theorem
i
Ž
.
Ž
Theorem 4.2.2 to 	 as if it were a function of the single variable x
since
i
.
" fr" x , and hence " 	r" x exists in a neighborhood of a . The right-hand
i
i
Ž
.
side of 7.10 can then be written as
1
" f a , a , . . . , a q h , . . . , a qh , . . . , a
Ž
.
1
2
i
i
i
j
j
n
lim
lim h
" x
h ™0 h ™0
i
j
j
i
" f a , a , . . . , a q h , . . . , a , . . . , a
Ž
.
1
2
i
i
i
j
n
y
" xi
"
" f a , a , . . . , a q h , . . . , a q h , . . . , a
Ž
.
1
2
i
i
i
j
j
j
n
s lim
lim
,
" x
" x
h ™0 h ™0
i
j
j
i
7.11
Ž
.
Ž
.
where 0 1. In formula
7.11
we have again made use of the mean
j
2 Ž .
value theorem, since " f x r" x " x exists in the given neighborhood around
j
i
2 Ž .
a. Furthermore, since " f x r" x " x is continuous in this neighborhood, the
j
i
Ž
.
2 Ž .
double limit in 7.11 is equal to " f a r" x " x . This establishes the asser-
j
i
tion that the two second-order partial derivatives of f are equal.

3
Ž
.
EXAMPLE 7.4.1.
Consider the function f: R ™R, where f x , x , x
s
1
2
3
x e x 2qx cos x . Then
1
2
1
" f x , x , x
Ž
.
1
2
3
x 2
se
yx sin x ,
2
1
" x1
" f x , x , x
Ž
.
1
2
3
x 2
sx e
qcos x ,
1
1
" x2
" 2f x , x , x
Ž
.
1
2
3
x 2
se
ysin x ,
1
" x " x
2
1
" 2f x , x , x
Ž
.
1
2
3
x 2
se
ysin x .
1
" x " x
1
2
7.4.1. The Total Derivative
Ž .
n
Let f x
be a real-valued function defined on a set D;R , where xs
Ž
.
x , x , . . . , x
. Suppose that x , x , . . . , x
are functions of a single variable
1
2
n
1
2
n
t. Then f is a function of t. The ordinary derivative of f with respect to t,
namely dfrdt, is called the total derivative of f.

DERIVATIVES OF A MULTIVARIABLE FUNCTION
271
Let us now assume that for the values of t under consideration dx rdt
i
Ž .
exists for is1, 2, . . . , n and that " f x r" x exists and is continuous in the
i
interior of D for is1, 2, . . . , n. Under these considerations, the total deriva-
tive of f is given by
n
df
" f x
dx
Ž .
i
s
.
7.12
Ž
.
Ý
dt
" x
dt
i
is1
To show this we proceed as follows: Let  x ,  x , . . . ,  x
be increments of
1
2
n
x , x , . . . , x
that correspond to an increment  t of t. In turn, f will have
1
2
n
the increment  f. We then have
 fsf x q x , x q x , . . . , x q x
yf x , x , . . . , x
.
Ž
.
Ž
.
1
1
2
2
n
n
1
2
n
This can be written as
 fs f x q x , x q x , . . . , x q x
Ž
.
1
1
2
2
n
n
yf x , x q x , . . . , x q x
Ž
.
1
2
2
n
n
q f x , x q x , . . . , x q x
Ž
.
1
2
2
n
n
yf x , x , x q x , . . . , x q x
Ž
.
1
2
3
3
n
n
q f x , x , x q x , . . . , x q x
Ž
.
1
2
3
3
n
n
yf x , x , x , x q x , . . . , x q x
Ž
.
1
2
3
4
4
n
n
q  q f x , x , . . . , x
, x q x
yf x , x , . . . , x
.
Ž
.
Ž
.
1
2
ny1
n
n
1
2
n
By applying the mean value theorem to the difference in each bracket we
obtain
" f x q  x , x q x , . . . , x q x
Ž
.
1
1
1
2
2
n
n
 fs x1
" x1
" f x , x q  x , x q x , . . . , x q x
Ž
.
1
2
2
2
3
3
n
n
q x2
" x2
" f x , x , x q  x , x q x , . . . , x q x
Ž
.
1
2
3
3
3
4
4
n
n
q x3
" x3
" f x , x , . . . , x
, x q  x
Ž
.
1
2
ny1
n
n
n
q  q x
,
n
" xn

MULTIDIMENSIONAL CALCULUS
272
where 0 1 for is1, 2, . . . , n. Hence,
i
 f
 x
" f x q  x , x q x , . . . , x q x
Ž
.
1
1
1
1
2
2
n
n
s
 t
 t
" x1
 x
" f x , x q  x , x q x , . . . , x q x
Ž
.
2
1
2
2
2
3
3
n
n
q  t
" x2
 x
" f x , x , x q  x , x q x , . . . , x q x
Ž
.
3
1
2
3
3
3
4
4
n
n
q  t
" x3
 x
" f x , x , . . . , x
, x q  x
Ž
.
n
1
2
ny1
n
n
n
q  q
.
7.13
Ž
.
 t
" xn
Ž
.
As  t™0,  x r t™dx rdt, and the partial derivatives in
7.13 , being
i
i
Ž .
continuous, tend to " f x r" x for is1, 2, . . . , n. Thus  fr t tends to the
i
Ž
.
right-hand side of formula 7.12 .
Ž
.
2
3
For example, consider the function
f x , x
sx yx , where
x s
1
2
1
2
1
et cos t, x scos tqsin t. Then,
2
df
t
t
2
s2 x
e cos tye sin t y3x
ysin tqcos t
Ž
.
Ž
.
1
2
dt
2
t
t
t
s2e cos t e cos tye sin t y3 cos tqsin t
ysin tqcos t
Ž
.
Ž
. Ž
.
s cos tysin t
2e2 t cos ty6 sin t cos ty3 .
Ž
. Ž
.
Of course, the same result could have been obtained by expressing f directly
as a function of t via x and x
and then differentiating it with respect to t.
1
2
Ž
.
We can generalize formula 7.12 by assuming that each of x , x , . . . , x
is
1
2
n
a function of several variables including the variable t. In this case, we need
to consider the partial derivative " fr" t, which can be similarly shown to have
the value
n
" f
" f x
" x
Ž .
i
s
.
7.14
Ž
.
Ý
" t
" x
" t
i
is1
In general, the expression
n
" f x
Ž .
dfs
dx
7.15
Ž
.
Ý
i
" xi
is1
is called the total differential of f at x.

DERIVATIVES OF A MULTIVARIABLE FUNCTION
273
Ž
.
EXAMPLE 7.4.2.
Consider the equation f x , x
s0, which in general
1
2
represents a relation between x
and x . It may or may not define x
as a
1
2
2
function of x . In this case, x
is said to be an implicit function of x . If x
1
2
1
2
Ž
.
can be obtained as a function of x , then we write x sg x . Consequently,
1
2
1
w
Ž
.x
w
Ž
.x
f x , g x
will be identically equal to zero. Hence, f x , g x
, being a
1
1
1
1
function of one variable x , will have a total derivative identically equal to
1
Ž
.
zero. By applying formula 7.12 with tsx
we obtain
1
df
" f
" f dx2
s
q
0.
dx
" x
" x
dx
1
1
2
1
If " fr" x 0, then the derivative of x
is given by
2
2
dx
y" fr" x
2
1
s
.
7.16
Ž
.
dx
" fr" x
1
2
Ž
.
Ž
.
In particular, if
f x , x
s0 is of the form
x yh x
s0, and if this
1
2
1
2
equation can be solved uniquely for x
in terms of x , then x
represents the
2
1
2
y1Ž
.
Ž
.
inverse function of h, that is, x sh
x . Thus according to formula 7.16 ,
2
1
dhy1
1
s
.
dx
dhrdx
1
2
This agrees with the formula for the derivative of the inverse function given
in Theorem 4.2.4.
7.4.2. Directional Derivatives
m
n
n Ž
Let f: D™R , where D;R , and let v be a unit vector in R
that is, a
.
vector whose length is equal to one , which represents a certain direction in
the n-dimensional Euclidean space. By definition, the directional derivative
of f at a point x is the interior of D in the direction of v is given by the limit
f xqhv yf x
Ž
.
Ž .
lim
,
h
h™0
if it exists. In particular, if vse , the unit vector in the direction of the ith
i
coordinate axis, then the directional derivative of f in the direction of v is just
Ž
.
the partial derivative of f with respect to x
is1, 2, . . . , n .
i
Lemma 7.4.1.
Let f: D™Rm, where D;Rn. If the partial derivatives
Ž
.
" f r" x
exist at a point xs x , x , . . . , x
 in the interior of D for is
j
i
1
2
n
1, 2, . . . , n; js1, 2, . . . , m, where f
is the jth element of f, then the direc-
j
tional derivative of f at x in the direction of a unit vector v exists and is equal
Ž .
Ž .
to J x v, where J x is the mn Jacobian of f at x.
f
f

MULTIDIMENSIONAL CALCULUS
274
Proof. Let us first consider the directional derivative of f in the direction
j
of v. To do so, we rotate the coordinate axes so that v coincides with the
direction of the  -axis, where  ,  , . . . , 
are the resulting new coordi-
1
1
2
n
nates. By the well-known relations for rotation of axes in analytic geometry of
n dimensions we have
n
x s ® q
  ,
is1, 2, . . . , n,
7.17
Ž
.
Ý
i
1 i
l
li
ls2
Ž
.
where ® is the ith element of v is1, 2, . . . , n and 
is the ith element of
i
li
Ž
.
 , the unit vector in the direction of the  -axis ls2, 3, . . . , n .
l
l
Now, the directional derivative of f in the direction of v can be obtained
j
by first expressing
f
as a function of  ,  , . . . , 
using the relations
j
1
2
n
Ž
.
Ž
.
7.17 and then differentiating it with respect to  . By formula 7.14 , this is
1
equal to
n
" f
" f
" x
j
j
i
s Ý
"
" x "
1
i
1
is1
n
" fj
s
® ,
js1, 2, . . . , m.
7.18
Ž
.
Ý
i
" xi
is1
Ž
.
From formula
7.18
we conclude that the directional derivative of fs
Ž
.
Ž .
f , f , . . . , f
 in the direction of v is equal to J x v.

1
2
m
f
EXAMPLE 7.4.3.
Let f: R3™R2 be defined as
2
2
2
x qx qx
1
2
3
f
x , x , x
s
.
Ž
.
1
2
3
2
2
x yx x qx
1
1
2
3
'
Ž
.
Ž
The directional derivative of f at xs 1, 2, 1  in the direction of vs 1r 2 ,
'
.
y1r 2 , 0  is
1
'2
2 x
2 x
2 x
1
2
3
J
x vs
1
Ž .
f
2 x yx
yx
2 x
y
1
2
1
3
Ž
.
1, 2, 1
'2
0
1
y2
'2
'2
2
4
2
s
1
s
.
1
0
y1
2
y '2
'2
0

DERIVATIVES OF A MULTIVARIABLE FUNCTION
275
Definition 7.4.1.
Let f: D™R, where D;Rn. If the partial derivatives
Ž
.
Ž
.
" fr" x
is1, 2, . . . , n exist at a point xs x , x , . . . , x
 in the interior of
i
1
2
n
Ž
.
D, then the vector " fr" x , " fr" x , . . . , " fr" x
 is called the gradient of f
1
2
n
Ž .
at x and is denoted by 
f x .

Using Definition 7.4.1, the directional derivative of f at x in the direction
Ž .
Ž .
of a unit vector v can be expressed as 
f x v, where 
f x  denotes the
Ž .
transpose of 
f x .
The Geometric Meaning of the Gradient
Let f: D™R, where D;Rn. Suppose that the partial derivatives of f exist
Ž
.
at a point xs x , x , . . . , x
 in the interior of D. Let C denote a smooth
1
2
n
Ž .
curve that lies on the surface of f x sc , where c is a constant, and passes
0
0
through the point x. This curve can be represented by the equations x s
1
Ž .
Ž .
Ž .
Ž
.
g t , x sg
t , . . . , x sg
t , where aFtFb. By formula 7.12 , the total
1
2
2
n
n
derivative of f with respect to t at x is
n
df
" f x
dg
Ž .
i
s
.
7.19
Ž
.
Ý
dt
" x
dt
i
is1
Ž
.
The vector s dg rdt, dg rdt, . . . , dg rdt  is tangent to C at x. Thus from
1
2
n
Ž
.
7.19 we obtain
df
s
f x  .
7.20
Ž .
Ž
.
dt
w
Ž .
Ž .
Ž .x
Now, since f g t , g
t , . . . , g
t
sc along C, then dfrdts0 and hence
1
2
n
0
Ž .

f x s0. This indicates that the gradient vector is orthogonal to , and
hence to C, at xgD. Since this result is true for any smooth curve through x,
Ž .
we conclude that the gradient vector 
f x is orthogonal to the surface of
Ž .
f x sc at x.
0
Definition 7.4.2.
Let f: D™R, where D;Rn. Then 
f: D™Rn. The
Ž .
Jacobian matrix of 
f x is called the Hessian matrix of f and is denoted by
Ž .
Ž .
Ž .
H
x . Thus H
x sJ
x , that is,
f
f

f
2
2
2
" f x
" f x
" f x
Ž .
Ž .
Ž .

2
" x " x
" x " x
" x
2
1
n
1
1
.
.
.
.
.
.
H
x s
.
7.21
Ž .
Ž
.
f
.
.
.
2
2
2
" f x
" f x
" f x
Ž .
Ž .
Ž .

2
" x " x
" x " x
" x
1
n
2
n
n

MULTIDIMENSIONAL CALCULUS
276
Ž .
The determinant of H
x is called the Hessian determinant. If the conditions
f
of Theorem 7.4.1 regarding the commutative property of partial differentia-
Ž .
tion are valid, then H
x is a symmetric matrix. As we shall see in Section
f
7.7, the Hessian matrix plays an important role in the identification of
maxima and minima of a multivariable function.

7.4.3. Differentiation of Composite Functions
Let f: D ™Rm, where D ;Rn, and let g: D ™R p, where D ;Rm. Let x
1
1
2
2
0
Ž
.
be an interior point of D and f x
be an interior point of D . If the mn
1
0
2
Ž
.
w Ž
.x
Jacobian matrix J x
and the pm Jacobian matrix J f x
both exist,
f
0
g
0
Ž
.
then the pn Jacobian matrix J x
for the composite function hsgf
h
0
exists and is given by
J
x
sJ
f x
J
x
.
7.22
Ž
.
Ž
.
Ž
.
Ž
.
h
0
g
0
f
0
Ž
.
Ž
.
Ž
.
To prove formula 7.22 , let us consider the k, i th element of J x
, namely
h
0
Ž
.
Ž
.
w Ž
.x
Ž
.
w Ž
.x
"h x
r" x , where h x
sg f x
is the kth element of h x
sg f x
,
k
0
i
k
0
k
0
0
0
Ž
.
is1, 2, . . . , n; ks1, 2, . . . , p, By applying formula 7.14 we obtain
m
"h
x
" g
f x
" f x
Ž
.
Ž
.
Ž
.
k
0
k
0
j
0
s
,
is1, 2, . . . , n; ks1, 2, . . . , p,
Ý
" x
" f
" x
i
j
i
js1
7.23
Ž
.
Ž
.
Ž
.
w Ž
.x
where f x
is the jth element of f x
, js1, 2, . . . , m. But " g f x
r" f is
j
0
0
k
0
j
Ž
.
w Ž
.x
Ž
.
Ž
.
the
k, j th element of J f x
, and " f x
r" x is the
j, i th element of
g
0
j
0
i
Ž
.
Ž
.
J x
, is1, 2, . . . , n; js1, 2, . . . , m; ks1, 2, . . . , p. Hence, formula
7.22
f
0
Ž
.
follows from formula 7.23 and the rule of matrix multiplication.
Ž
.
In particular, if msnsp, then from formula 7.22 , the determinant of
Ž
.
J x
is given by
h
0
det J
x
sdet J
f x
det J
x
.
7.24
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
h
0
g
0
f
0
Ž
.
Ž
.
Using the notation in formula 7.8 , formula 7.24 can be expressed as
" h , h , . . . , h
" g , g , . . . , g
"
f , f , . . . , f
Ž
.
Ž
.
Ž
.
1
2
n
1
2
n
1
2
n
s
.
7.25
Ž
.
" x , x , . . . , x
"
f , f , . . . , f
" x , x , . . . , x
Ž
.
Ž
.
Ž
.
1
2
n
1
2
n
1
2
n
EXAMPLE 7.4.4.
Let f: R2™R3 be given by
2
x yx cos x
1
2
1
x x
f
x , x
s
.
Ž
.
1
2
1
2
3
3
x qx
1
2

TAYLOR’S THEOREM FOR A MULTIVARIABLE FUNCTION
277
Let g: R3™R be defined as
g  ,  , 
s y 2q ,
Ž
.
1
2
3
1
2
3
where
 sx 2yx cos x ,
1
1
2
1
 sx x ,
2
1
2
 sx 3qx 3.
3
1
2
In this case,
2 x qx sin x
ycos x
1
2
1
1
x
x
J
x s
,
Ž .
2
1
f
2
2
3x
3x
1
2
J
f x
s 1,y2 , 1 .
Ž .
Ž
.
g
2
Ž
.
Hence, by formula 7.22 ,
2 x qx sin x
ycos x
1
2
1
1
x
x
J
x s 1,y2 , 1
Ž .
Ž
.
2
1
h
2
2
2
3x
3x
1
2
s 2 x qx sin x y2 x x 2q3x 2,ycos x y2 x 2 x q3x 2 .
Ž
.
1
2
1
1
2
1
1
1
2
2
7.5. TAYLOR’S THEOREM FOR A MULTIVARIABLE FUNCTION
We shall now consider a multidimensional analogue of Taylor’s theorem,
which was discussed in Section 4.3 for a single-variable function.
Ž
.
Let us first introduce the following notation: Let xs x , x , . . . , x
.
1
2
n
Then x
 denotes a first-order differential operator of the form
n
"
x
s
x
.
Ý
i " xi
is1
The symbol 
, called the del operator, was used earlier to define the
Ž
.m
gradient vector. If m is a positive integer, then x
denotes an mth-order
differential operator. For example, for msns2,
2
"
"
2
x
s
x
qx
Ž
.
1
2
ž
/
" x
" x
1
2
" 2
" 2
" 2
2
2
sx
q2 x x
qx
.
1
1
2
2
2
2
" x " x
" x
" x
1
2
1
2

MULTIDIMENSIONAL CALCULUS
278
Ž
.2
Thus
x
is obtained by squaring
x "r" x qx "r" x
in the usual
1
1
2
2
fashion, except that the squares of "r" x and "r" x are replaced by " 2r" x 2
1
2
1
and " 2r" x 2, respectively, and the product of "r" x
and "r" x
is replaced
2
1
2
2
Ž
by " r" x " x
here we are assuming that the commutative property of
1
2
partial differentiation holds once these differential operators are applied to a
.
Ž
.m
real-valued function . In general, x
is obtained by a multinomial expan-
sion of degree m of the form
" m
m
m
k
k
k
1
2
n
x
s
x
x
 x
,
Ž
.
Ý
1
2
n
k
k
k
1
2
n
k , k , . . . , k
ž
/
" x
" x
 " x
1
2
n
1
2
n
k , k , . . . , k
1
2
n
Ž
.
n
where the sum is taken over all n-tuples k , k , . . . , k
for which Ý
k sm,
1
2
n
is1
i
and
m!
m
s
.
k , k , . . . , k
ž
/
k ! k !  k !
1
2
n
1
2
n
Ž .
If a real-valued function f x has partial derivatives through order m, then
Ž
.m
Ž .
an application of the differential operator x
to f x results in
m
m
k
k
1
2
x
f x s
x
x

Ž
.
Ž .
Ý
1
2
k , k , . . . , k
ž
/
1
2
n
k , k , . . . , k
1
2
n
" mf x
Ž .
k n
x
.
7.26
Ž
.
n
k
k
k
1
2
n
" x
" x
 " x
1
2
n
Ž
.m Ž
.
Ž
.m Ž .
The notation x
f x
indicates that x
f x is evaluated at x .
0
0
n
Ž
.
Theorem 7.5.1.
Let
f: D™R, where D;R , and let
N x
be a

0
Ž
.
neighborhood of x gD such that
N x
;D. If
f and all its partial
0

0
Ž
.
derivatives of order Fr exist and are continuous in N x
, then for any

0
Ž
.
xgN x
,

0
r
i
ry1
xyx

f x
xyx

f z
Ž
.
Ž
.
Ž
.
Ž
.
0
0
0
0
f x sf x
q
q
,
7.27
Ž .
Ž
.
Ž
.
Ý
0
i!
r!
is1
where z
is a point on the line segment from x
to x.
0
0
Ž .
Ž .
Ž
Proof. Let hsxyx . Let the function  t
be defined as  t sf x q
0
0
.
Ž .
Ž
.
Ž .
Ž
.
Ž .
th , where 0FtF1. If ts0, then  0 sf x
and  1 sf x qh sf x , if
0
0

TAYLOR’S THEOREM FOR A MULTIVARIABLE FUNCTION
279
Ž
.
ts1. Now, by formula 7.12 ,
n
d t
" f x
Ž .
Ž .
s
h
Ý
i
dt
" xi
xsx qt h
is1
0
s h
 f x qth ,
Ž
. Ž
.
0
Ž
.
where h is the ith element of h is1, 2, . . . , n . Furthermore, the derivative
i
Ž .
of order m of  t is
dm tŽ .
m
s h
f x qth ,
1FmFr.
Ž
.
Ž
.
0
m
dt
Since the partial derivatives of f through order r are continuous, then the
Ž .
w
x
same order derivatives of  t are also continuous on 0, 1 and
m
d  tŽ .
m
s h
f x
,
1FmFr.
Ž
.
Ž
.
0
m
dt
ts0
If we now apply Taylor’s theorem in Section 4.3 to the single-variable
Ž .
function  t , we obtain
i
i
r
r
ry1 t
d t
t
d t
Ž .
Ž .
 t s 0 q
q
,
7.28
Ž .
Ž .
Ž
.
Ý
r
i
i!
r!
dt
dt
ts
ts0
is1
Ž
.
where 0t. By setting ts1 in formula 7.28 , we obtain
r
i
ry1
xyx

f x
xyx

f z
Ž
.
Ž
.
Ž
.
Ž
.
0
0
0
0
f x sf x
q
q
,
Ž .
Ž
.
Ý
0
i!
r!
is1
where z sx q h. Since 01, the point z
lies on the line segment
0
0
0
between x
and x.

0
Ž .
Ž
.
In particular, if f x has partial derivatives of all orders in N x
, then we

0
have the series expansion
i

xyx

f x
Ž
.
Ž
.
0
0
f x sf x
q
.
7.29
Ž .
Ž
.
Ž
.
Ý
0
i!
is1
Ž
.
In this case, the last term in formula 7.27 serves as a remainder of Taylor’s
series.

MULTIDIMENSIONAL CALCULUS
280
2
Ž
.
EXAMPLE 7.5.1.
Consider the function f: R ™R defined as f x , x
s
1
2
x x qx 2qe x1 cos x . This function has partial derivatives of all orders. Thus
1
2
1
2
Ž
.
in a neighborhood of x s 0, 0  we can write
0
1
1
2
3
f x , x
s1q x
 f 0, 0 q
x
f 0, 0 q
x
f  x ,  x
,
Ž
.
Ž
. Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
1
2
1
2
2!
3!
01.
It can be verified that
x
 f 0, 0 sx ,
Ž
. Ž
.
1
2
2
2
x
f 0, 0 s3x q2 x x yx ,
Ž
.
Ž
.
1
1
2
2
3
3
 x
2
 x
1
1
x
f  x ,  x
sx e
cos  x
y3x x e
sin  x
Ž
.
Ž
.
Ž
.
Ž
.
1
2
1
2
1
2
2
y3x x 2 e x1 cos  x
qx 3 e x1 sin  x
.
Ž
.
Ž
.
1
2
2
2
2
Hence,
f x , x
Ž
.
1
2
1
2
2
s1qx q
3x q2 x x yx
Ž
.
1
1
1
2
2
2!
1
3
2
 x
3
2
 x
1
1
q
x y3x x
e
cos  x
q x y3x x
e
sin  x
.
Ž
.
Ž
.

4
1
1
2
2
2
1
2
2
3!
Ž
.
The first three terms serve as a second-order approximation of f x , x
,
1
2
while the last term serves as a remainder.
7.6. INVERSE AND IMPLICIT FUNCTION THEOREMS
n
n
Ž .
Consider the function f: D™R , where D;R . Let ysf x . The purpose of
this section is to present conditions for the existence of an inverse function
fy1 which expresses x as a function of y. These conditions are given in the
Ž
.
next theorem, whose proof can be found in Sagan 1974, page 371 . See also
Ž
.
Fulks 1978, page 346 .
Ž
.
n
Theorem 7.6.1 Inverse Function Theorem .
Let f: D™R , where D is
an open subset of Rn and f has continuous first-order partial derivatives
Ž
.
in D. If for some x gD, the nn Jacobian matrix J x
is nonsingular,
0
f
0

INVERSE AND IMPLICIT FUNCTION THEOREMS
281
that is,
"
f , f , . . . , f
Ž
.
1
2
n
det J
x
s
0,
Ž
.
f
0
" x , x , . . . , x
Ž
.
1
2
n
xsx 0
Ž
.
where f is the ith element of f is1, 2, . . . , n , then there exist an 0 and
i
y1
w Ž
.x
a 0 such that an inverse function f
exists in the neighborhood N f x

0
Ž
.
y1
and takes values in the neighborhood N x
. Moreover, f
has continuous

0
w Ž
.x
Ž
.
first-order partial derivatives in N f x
, and its Jacobian matrix at f x
is

0
0
Ž
.
the inverse of J x
; hence,
f
0
1
y1
det J
f x
s
.
7.30

4
Ž
.
Ž
.
f
0
det J
x
Ž
.
f
0
EXAMPLE 7.6.1.
Let f: R3™R3 be given by
2 x x yx
1
2
2
2
2
x qx q2 x
f
x , x , x
s
.
Ž
.
1
2
3
1
2
3
x x qx
1
2
2
Here,
2 x
2 x y1
0
2
1
2 x
1
4 x
J
x s
,
Ž .
1
3
f
x
x q1
0
2
1
w
Ž .x
3
and det J x sy12 x x . Hence, all xgR
at which x x 0, f has an
f
2
3
2
3
y1
Ž
.
4
inverse function f
. For example, if Ds
x , x , x
x 0, x 0 , then f is
1
2
3
2
3
invertible in D. From the equations
y s2 x x yx ,
1
1
2
2
y sx 2qx q2 x 2,
2
1
2
3
y sx x qx ,
3
1
2
2
y1Ž .
we obtain the inverse function xsf
y , where
y qy
1
3
x s
,
1
2 y yy
3
1
yy q2 y
1
3
x s
,
2
3
1r2
2
1
y qy
2 y yy
Ž
.
1
3
3
1
x s
y y
y
.
3
2
2
'
3
2
2 y yy
Ž
.
3
1

MULTIDIMENSIONAL CALCULUS
282
Ž
.
Ž
.
Ž
.
If, for example, we consider x s 1, 1, 1 , then y sf x
s 1, 4, 2 , and
0
0
0
w
Ž
.x
y1
det J x
sy12. The Jacobian matrix of f
at y
is
f
0
0
2
1
0
y
3
3
1
2
y
0
y1
J
y
s
.
Ž
.
3
3
f
0
1
1
y
0
4
4
Its determinant is equal to
1
y1
det J
y
sy
.
Ž
.
f
0
12
w
Ž
.x
We note that this is the reciprocal of det J x
, as it should be according to
f
0
Ž
.
formula 7.30 .
The inverse function theorem can be viewed as providing a unique
Ž .
solution to a system of n equations given by ysf x . There are, however,
situations in which y is not explicitly expressed as a function of x. In general,
we may have two vectors, x and y, of orders n1 and m1, respectively,
that satisfy the relation
g x, y s0,
7.31
Ž
.
Ž
.
where g: Rmqn ™Rn. In this more general case, we have n equations
involving mqn variables, namely, the elements of x and those of y. The
Ž
.
question now is what conditions will allow us to solve equations
7.31
uniquely for x in terms of y. The answer to this question is given in the next
Ž
.
theorem, whose proof can be found in Fulks 1978, page 352 .
Ž
.
n
Theorem 7.6.2 Implicit Function Theorem .
Let g: D™R , where D is
an open subset of Rmqn, and g has continuous first-order partial derivatives
Ž 

 .
n
m
in D. If there is a point z gD, where z s x , y  with x gR , y gR
0
0
0
0
0
0
Ž
.
such that g z
s0, and if at z ,
0
0
" g , g , . . . , g
Ž
.
1
2
n
0,
" x , x , . . . , x
Ž
.
1
2
n
Ž
.
where g is the ith element of g is1, 2, . . . , n , then there is a neighborhood
i
Ž
.
Ž
.
N y
of y in which the equation g x, y s0 can be solved uniquely for x as a

0
0
continuously differentiable function of y.

OPTIMA OF A MULTIVARIABLE FUNCTION
283
EXAMPLE 7.6.2.
Let g: R3™R2 be given by
2
x qx qy y18
1
2
g x , x , y s
.
Ž
.
1
2
x yx x qyy4
1
1
2
We have
" g , g
Ž
.
1
1
1
2
sdet
sx yx y1.
2
1
1yx
yx
ž
/
" x , x
2
1
Ž
.
1
2
Ž
.
Ž
.
Ž
.
Let zs x , x , y . At the point z s 1, 1, 4 , for example, g z
s0 and
1
2
0
0
Ž
.
Ž
.
" g , g
r" x , x
sy10. Hence, by Theorem 7.6.2, we can solve the
1
2
1
2
equations
x qx qy2y18s0,
7.32
Ž
.
1
2
x yx x qyy4s0
7.33
Ž
.
1
1
2
uniquely in terms of y in some neighborhood of y s4. For example, if D in
0
Theorem 7.6.2 is of the form

Ds
x , x , y
x 0, x 0, y4.06 ,

4
Ž
.
1
2
1
2
Ž
.
Ž
.
then from equations 7.32 and 7.33 we obtain the solution
1r2
2
1
2
2
x s
y y y17 q
y y17
y4 yq16
,
Ž
.
Ž
.
1
2½
5
1r2
2
1
2
2
x s
19yy y
y y17
y4 yq16
.
Ž
.
2
2½
5
We note that the sign preceding the square root in the formula for x
was
1
chosen as q, so that
x sx s1 when
ys4. It can be verified that
1
2
Ž
2
.2
y y17
y4 yq16 is positive for y4.06.
7.7. OPTIMA OF A MULTIVARIABLE FUNCTION
Ž .
n
Let f x be a real-valued function defined on a set D;R . A point x gD is
0
said to be a point of local maximum of f if there exists a neighborhood
Ž
.
Ž .
Ž
.
Ž
.
Ž .
Ž
.
N x
;D such that f x Ff x
for all xgN x
. If f x Gf x
for all

0
0

0
0
Ž
.
xgN x
, then x
is a point of local minimum. If one of these inequalities

0
0
holds for all x in D, then x
is called a point of absolute maximum, or a point
0
of absolute minimum, respectively, of f in D. In either case, x
is referred to
0
Ž
.
Ž .
as a point of optimum or extremum , and the value of f x at xsx
is called
0
Ž .
an optimum value of f x .

MULTIDIMENSIONAL CALCULUS
284
Ž .
In this section we shall discuss conditions under which f x attains local
optima in D. Then, we shall investigate the determination of the optima of
Ž .
f x over a constrained region of D.
Ž .
As in the case of a single-variable function, if f x has first-order partial
derivatives at a point x
in the interior of D, and if x
is a point of local
0
0
optimum, then " fr" x s0 for is1, 2, . . . , n at x . The proof of this fact is
i
0
similar to that of Theorem 4.4.1. Thus the vanishing of the first-order partial
Ž .
derivatives of f x at x
is a necessary condition for a local optimum at x ,
0
0
but is obviously not sufficient. The first-order partial derivatives can be zero
without necessarily having a local optimum at x .
0
In general, any point at which " fr" x s0 for is1, 2, . . . , n is called a
i
stationary point. It follows that any point of local optimum at which f has
first-order partial derivatives is a stationary point, but not every stationary
point is a point of local optimum. If no local optimum is attained at a
stationary point x , then x
is called a saddle point. The following theorem
0
0
gives the conditions needed to have a local optimum at a stationary point.
Theorem 7.7.1.
Let f: D™R, where D;Rn. Suppose that f has contin-
uous second-order partial derivatives in D. If x
is a stationary point of f,
0
then at x
f has the following:
0
Ž
.2 Ž
.
Ž
.
i. A local minimum if
h
f x
0 for all hs h , h , . . . , h
 in a
0
1
2
n
neighborhood of 0, where the elements of h are not all equal to zero.
Ž
.2 Ž
.
Ž .
ii. A local maximum if h
f x
0, where h is the same as in i .
0
Ž
.2 Ž
.
iii. A saddle point if
h
f x
changes sign for values of h in a
0
neighborhood of 0.
Ž .
Proof. By applying Taylor’s theorem to f x in a neighborhood of x
we
0
obtain
1
2
f x qh sf x
q h
 f x
q
h
f z
,
Ž
.
Ž
.
Ž
. Ž
.
Ž
.
Ž
.
0
0
0
0
2!
where h is a nonzero vector in a neighborhood of 0 and z
is a point on the
0
line segment from x
to x qh. Since x
is a stationary point, then
0
0
0
Ž
. Ž
.
h
 f x
s0. Hence,
0
1
2
f x qh yf x
s
h
f z
.
Ž
.
Ž
.
Ž
.
Ž
.
0
0
0
2!
Also, since the second-order partial derivatives of f are continuous at x ,
0
then we can write
1
2
	
	
f x qh yf x
s
h
f x
qo
h
,
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
0
0
0
2!

OPTIMA OF A MULTIVARIABLE FUNCTION
285
	
	
Ž
.1r2
Ž	
	.
where
h s hh
and o
h
™0 as h™0. We note that for small values
	
	
Ž
.
Ž
.
Ž
.2 Ž
.
of
h , the sign of f x qh yf x
depends on the value of h
f x
. It
0
0
0
follows that if
Ž
.2 Ž
.
Ž
.
Ž
.
i.
h
f x
0, then f x qh f x
for all nonzero values of h in
0
0
0
some neighborhood of 0. Thus x
is a point of local minimum of f.
0
Ž
.2 Ž
.
Ž
.
Ž
.
ii.
h
f x
0, then f x qh f x
for all nonzero values of h in
0
0
0
some neighborhood of 0. In this case, x
is a point of local maximum
0
of f.
Ž
.2 Ž
.
iii.
h
f x
changes sign inside a neighborhood of 0, then x
is neither
0
0
a point of local maximum nor a point of local minimum. Therefore, x 0
must be a saddle point.

Ž
.2 Ž
.
We note that h
f x
can be written as a quadratic form of the form
0
Ž
.
hAh, where AsH
x
is the nn Hessian matrix of f evaluated at x , that
f
0
0
is,
f
f

f
11
12
1n
f
f

f
21
22
2 n
.
.
.
As
,
7.34
Ž
.
.
.
.
.
.
.
f
f

f
n1
n2
nn
2 Ž
.
where for simplicity we have denoted " f x
r" x " x by f , i, js1, 2, . . . , n
0
i
j
i j
w
Ž
.x
see formula 7.21 .
Corollary 7.7.1.
Let f be the same function as in Theorem 7.7.1, and let
Ž
.
A be the matrix given by formula 7.34 . If x
is a stationary point of f, then
0
at x
f has the following:
0
i. A local minimum if A is positive definite, that is, the leading principal
Ž
.
minors of A see Definition 2.3.6 are all positive,
f
f
11
12
f
0,
det
0, . . . ,
det A 0.
7.35
Ž .
Ž
.
11
ž
/
f
f
21
22
ii. A local maximum if A is negative definite, that is, the leading principal
minors of A have alternating signs as follows:
f
f
n
11
12
f
0,
det
0, . . . , y1
det A 0.
7.36
Ž
.
Ž .
Ž
.
11
ž
/
f
f
21
22
iii. A saddle point if A is neither positive definite nor negative definite.

MULTIDIMENSIONAL CALCULUS
286
Proof.
Ž
.2 Ž
.
i. By Theorem 7.7.1, f has a local minimum at x
if h
f x
shAh
0
0
is positive for all h0, that is, if A is positive definite. By Theorem
Ž .
2.3.12 2 , A is positive definite if and only if its leading principal
Ž
.
minors are all positive. The conditions stated in 7.35 are therefore
sufficient for a local minimum at x .
0
Ž
.2 Ž
.
ii.
h
f x
0 if and only if A is negative definite, or yA is positive
0
Ž
.
definite. Now, a leading principal minor of order m s1, 2, . . . , n of
Ž
.m
yA is equal to y1
multiplied by the corresponding leading princi-
Ž
.
pal minor of A. This leads to conditions 7.36 .
Ž
.2 Ž
.
iii. If A is neither positive definite nor negative definite, then h
f x 0
must change sign inside a neighborhood of x . This makes x
a saddle
0
0
point.

A Special Case
Ž
.
If f is a function of only ns2 variables, x
and x , then conditions 7.35
1
2
Ž
.
and 7.36 can be written as:
i. f
0, f
f
yf 2 0 for a local minimum at x .
11
11
22
12
0
ii. f
0, f
f
yf 2 0 for a local maximum at x .
11
11
22
12
0
If f
f
yf 2 0, then x
is a saddle point, since in this case
11
22
12
0
" 2f x
" 2f x
" 2f x
Ž
.
Ž
.
Ž
.
0
0
0
2
2
hAhsh
q2h h
qh
1
1
2
2
2
2
" x " x
" x
" x
1
2
1
2
" 2f x
Ž
.
0
s
h yah
h ybh
,
Ž
. Ž
.
1
2
1
2
2
" x1
where ah
and bh
are the real roots of the equation hAhs0 with respect
2
2
to h . Hence, hAh changes sign in a neighborhood of 0.
1
If f
f
yf 2 s0, then hAh can be written as
11
22
12
2
2
2
" f x
" f x
r" x " x
Ž
.
Ž
.
0
0
1
2
hAhs
h qh
1
2
2
2
2
" x
" f x
r" x
Ž
.
1
0
1
2 Ž
.
2
provided that " f x
r" x 0. Thus hAh has the same sign as that of
0
1
2 Ž
.
2
Ž
.
" f x
r" x
except for those values of hs h , h
 for which
0
1
1
2
" 2f x
r" x " x
Ž
.
0
1
2
h qh
s0,
1
2
2
2
" f x
r" x
Ž
.
0
1

OPTIMA OF A MULTIVARIABLE FUNCTION
287
2 Ž
.
2
2 Ž
.
in which case it is zero. In the event " f x
r" x s0, then " f x
r" x " x
0
1
0
1
2
2
2 Ž
.
2
s0, and hAhsh " f x
r" x , which has the same sign as that of
2
0
2
2 Ž
.
2
" f x
r" x , if it is different from zero, except for those values of hs
0
2
Ž
.
2
h , h
 for which h s0, where it is zero. It follows that when f
f
yf
s
1
2
2
11
22
12
0, hAh has a constant sign for all h inside a neighborhood of 0. However, it
can vanish for some nonzero values of h. For such values of h, the sign of
Ž
.
Ž
.
f x qh yf x
depends on the signs of higher-order partial derivatives
0
0
Ž
.
higher than second order of f at x . These can be obtained from Taylor’s
0
expansion. In this case, no decision can be made regarding the nature of the
Ž
.
stationary point until these higher-order partial derivatives if they exist have
been investigated.
2
Ž
.
2
2
EXAMPLE 7.7.1.
Let f: R ™R be the function f x , x
sx q2 x yx .
1
2
1
2
1
Consider the equations
" f
s2 x y1s0,
1
" x1
" f
s4 x s0.
2
" x2
Ž
.
The only solution is x s 0.5, 0 . The Hessian matrix is
0
2
0
As
,
0
4
Ž .
which is positive definite, since 20 and det A s80. The point x
is
0
therefore a local minimum. Since it is the only one in R2, it must also be the
absolute minimum.
EXAMPLE 7.7.2.
Consider the function f: R3™R, where
1
3
2
2
f x , x , x
s x q2 x qx y2 x x q3x x qx x
Ž
.
1
2
3
1
2
3
1
2
1
3
2
3
3
y10 x q4 x y6 x q1.
1
2
3
A stationary point must satisfy the equations
" f
2
sx y2 x q3x y10s0,
7.37
Ž
.
1
2
3
" x1
" f
sy2 x q4 x qx q4s0,
7.38
Ž
.
1
2
3
" x2
" f
s3x qx q2 x y6s0.
7.39
Ž
.
1
2
3
" x3

MULTIDIMENSIONAL CALCULUS
288
Ž
.
Ž
.
From 7.38 and 7.39 we get
x sx y2,
2
1
x s4y2 x .
3
1
Ž
.
By substituting these expressions in equation 7.37 we obtain
x 2y8 x q6s0.
1
1
'
'
This equation has two solutions, namely, 4y 10 and 4q 10 . We therefore
have two stationary points,
Ž1.
'
'
'
x
s 4q 10 , 2q 10 ,y4y2 10 ,
Ž
.
0
Ž2.
'
'
'
x
s 4y 10 , 2y 10 ,y4q2 10 .
Ž
.
0
Now, the Hessian matrix is
2 x
y2
3
1
As
.
y2
4
1
3
1
2
Its leading principal minors are 2 x , 8 x y4, and 14 x y56. The last one is
1
1
1
the determinant of A. At xŽ1. all three are positive. Therefore, xŽ1. is a point
0
0
of local minimum. At xŽ2. the values of the leading principal minors are 1.675,
0
2.7018, and y44.272. In this case, A is neither positive definite over negative
definite. Thus xŽ2. is a saddle point.
0
7.8. THE METHOD OF LAGRANGE MULTIPLIERS
Ž
.
This method, which is due to Joseph Louis de Lagrange 17361813 , is used
Ž
.
to optimize a real-valued function f x , x , . . . , x
, where x , x , . . . , x
are
1
2
n
1
2
n
Ž
.
subject to m n equality constraints of the form
g
x , x , . . . , x
s0,
Ž
.
1
1
2
n
g
x , x , . . . , x
s0,
Ž
.
2
1
2
n
...
7.40
Ž
.
g
x , x , . . . , x
s0,
Ž
.
m
1
2
n
where g , g , . . . , g
are differentiable functions.
1
2
m
The determination of the stationary points in this constrained optimization
problem is done by first considering the function
m
F x sf x q
 g
x ,
7.41
Ž .
Ž .
Ž .
Ž
.
Ý
j
j
js1

THE METHOD OF LAGRANGE MULTIPLIERS
289
Ž
.
where xs x , x , . . . , x
 and  ,  , . . . , 
are scalars called Lagrange
1
2
n
1
2
m
Ž
.
multipliers. By differentiating 7.41 with respect to x , x , . . . , x
and equat-
1
2
n
ing the partial derivatives to zero we obtain
m
"F
" f
" gj
s
q

s0,
is1, 2, . . . , n.
7.42
Ž
.
Ý
j
" x
" x
" x
i
i
i
js1
Ž
.
Ž
.
Equations 7.40 and 7.42 consist of mqn equations in mqn unknowns,
namely, x , x , . . . , x ;  ,  , . . . ,  . The solutions for x , x , . . . , x
deter-
1
2
n
1
2
m
1
2
n
mine the locations of the stationary points. The following argument explains
why this is the case:
Ž
.
Suppose that in equation
7.40
we can solve for m x ’s, for example,
i
x , x , . . . , x , in terms of the remaining nym variables. By Theorem 7.6.2,
1
2
m
this is possible whenever
" g , g , . . . , g
Ž
.
1
2
m
0.
7.43
Ž
.
" x , x , . . . , x
Ž
.
1
2
m
In this case, we can write
x sh
x
, x
, . . . , x
,
Ž
.
1
1
mq1
mq2
n
x sh
x
, x
, . . . , x
,
Ž
.
2
2
mq1
mq2
n
...
7.44
Ž
.
x sh
x
, x
, . . . , x
.
Ž
.
m
m
mq1
mq2
n
Ž .
Thus f x is a function of only nym variables, namely, x
, x
, . . . , x .
mq1
mq2
n
If the partial derivatives of f with respect to these variables exist and if f has
a local optimum, then these partial derivatives must necessarily vanish,
that is,
m
" f
" f "hj
q
s0,
ismq1, mq2, . . . , n.
7.45
Ž
.
Ý
" x
"h " x
i
j
i
js1
Ž
.
Now, if equations 7.44 are used to substitute h , h , . . . , h
for x , x , . . . , x ,
1
2
m
1
2
m
Ž
.
respectively, in equation 7.40 , then we obtain the identities
g
h , h , . . . , h , x
, x
, . . . , x
0,
Ž
.
1
1
2
m
mq1
mq2
n
g
h , h , . . . , h , x
, x
, . . . , x
0,
Ž
.
2
1
2
m
mq1
mq2
n
...
g
h , h , . . . , h , x
, x
, . . . , x
0.
Ž
.
m
1
2
m
mq1
mq2
n

MULTIDIMENSIONAL CALCULUS
290
By differentiating these identities with respect to x
, x
, . . . , x
we
mq1
mq2
n
obtain
m
" g
" g
"h
k
k
j
q
s0,
ismq1, mq2, . . . , n; ks1, 2, . . . , m.
7.46
Ž
.
Ý
" x
"h " x
i
j
i
js1
Let us now define the vectors

" g
" g
" g
k
k
k
 s
,
, . . . ,
,
ks1, 2, . . . , m,
k ž
/
" x
" x
" x
mq1
mq2
n

" g
" g
" g
k
k
k
 s
,
, . . . ,
,
ks1, 2, . . . , m,
k ž
/
"h
"h
"h
1
2
m

"h
"h
"h
j
j
j
 s
,
, . . . ,
,
js1, 2, . . . , m,
j ž
/
" x
" x
" x
mq1
mq2
n

" f
" f
" f
	s
,
, . . . ,
,
ž
/
" x
" x
" x
mq1
mq2
n

" f
" f
" f
s
,
, . . . ,
.
ž
/
"h
"h
"h
1
2
m
Ž
.
Ž
.
Equations 7.45 and 7.46 can then be written as
w
x
w
x
 :  :  : m q  :  :  : 
s0,
7.47
Ž
.
1
2
1
2
m
w
x
	q  :  :  : 
s0,
7.48
Ž
.
1
2
m
w
x
where s  :  :  : 
, which is a nonsingular mm matrix if condition
1
2
m
Ž
.
Ž
.
7.43 is valid. From equation 7.47 we have
w
x
w
x
y1
 :  :  : 
sy  :  :  : 

.
1
2
m
1
2
m
Ž
.
By making the proper substitution in equation 7.48 we obtain
w
x
	q  :  :  : 
s0,
7.49
Ž
.
1
2
m
where
syy1.
7.50
Ž
.

THE METHOD OF LAGRANGE MULTIPLIERS
291
Ž
.
Equations 7.49 can then be expressed as
m
" f
" gj
q

s0,
ismq1, mq2, . . . , n.
7.51
Ž
.
Ý
j
" x
" x
i
i
js1
Ž
.
From equation 7.50 we also have
m
" f
" gj
q

s0,
is1, 2, . . . , m.
7.52
Ž
.
Ý
j
" x
" x
i
i
js1
Ž
.
Ž
.
Equations
7.51
and
7.52
can now be combined into a single vector
equation of the form
m

f x q
 
g s0,
Ž .
Ý
j
j
js1
Ž
.
which is the same as equation 7.42 . We conclude that at a stationary point
of f, the values of x , x , . . . , x and the corresponding values of  ,  , . . . , 
1
2
n
1
2
m
Ž
.
Ž
.
must satisfy equations 7.40 and 7.42 .
Sufficient Conditions for a Local Optimum
in the Method of Lagrange Multipliers
Ž
.
Equations 7.42 are only necessary for a stationary point x
to be a point of
0
Ž
.
local optimum of f subject to the constraints given by equations
7.40 .
Ž
Sufficient conditions for a local optimum are given in Gillespie 1954, pages
.
9798 . The following is a reproduction of these conditions:
Ž
.
Let x be a stationary point of f whose coordinates satisfy equations 7.40
0
Ž
.
and 7.42 , and let  ,  , . . . , 
be the corresponding Lagrange multipliers.
1
2
m
Ž
.
Let F denote the second-order partial derivative of F in formula 7.41 with
i j
Ž
.
Ž
.
respect to x , and x , i, js1, 2, . . . , n; ij. Consider the
mqn  mqn
i
j
matrix
Ž1.
Ž1.
Ž1.
F
F

F
g
g

g
11
12
1n
1
2
m
Ž2.
Ž2.
Ž2.
F
F

F
g
g

g
21
22
2 n
1
2
m
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Žn.
Žn.
Žn.
F
F

F
g
g

g
n1
n2
nn
1
2
m
B s
,
7.53
Ž
.
1
Ž1.
Ž2.
Žn.
g
g

g
0
0

0
1
1
1
Ž1.
Ž2.
Žn.
g
g

g
0
0

0
2
2
2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Ž1.
Ž2.
Žn.
g
g

g
0
0

0
m
m
m

MULTIDIMENSIONAL CALCULUS
292
where g Ži.s" g r" x , is1, 2, . . . , n; js1, 2, . . . , m. Let  denote the deter-
j
j
i
1
minant of B . Furthermore, let  ,  , . . . ,  
denote a set of principal
1
2
3
nym
Ž
.
minors of B
see Definition 2.3.6 , namely, the determinants of the principal
1
submatrices B , B , . . . , B
, where B is obtained by deleting the first iy1
2
3
nym
i
Ž
.
rows and the first iy1 columns of B
is2, 3, . . . , nym . All the partial
1
derivatives used in B , B , . . . , B
are evaluated at x . Then sufficient
1
2
nym
0
conditions for x
to be a point of local minimum of f are the following:
0
i. If m is even,
 0,
 0, . . . ,
 
0.
1
2
nym
ii. If m is odd,
 0,
 0, . . . ,
 
0.
1
2
nym
However, sufficient conditions for x
to be a point of local maximum are the
0
following:
i. If n is even,
nym
 0,
 0, . . . ,
y1
 
0.
Ž
.
1
2
nym
ii. If n is odd,
nym
 0,
 0, . . . ,
y1
 
0.
Ž
.
1
2
nym
EXAMPLE 7.8.1.
Let us find the minimum and maximum distances from
the origin to the curve determined by the intersection of the plane x qx s0
2
3
2
2
2
Ž
.
with the ellipsoid x q2 x qx q2 x x s1. Let f x , x , x
be the squared
1
2
3
2
3
1
2
3
distance function from the origin, that is,
f x , x , x
sx 2qx 2qx 2.
Ž
.
1
2
3
1
2
3
The equality constraints are
g
x , x , x
x qx s0,
Ž
.
1
1
2
3
2
3
g
x , x , x
x 2q2 x 2qx 2q2 x x y1s0.
Ž
.
2
1
2
3
1
2
3
2
3
Then
F x , x , x
sx 2qx 2qx 2q
x qx
Ž
.
Ž
.
1
2
3
1
2
3
1
2
3
q
x 2q2 x 2qx 2q2 x x y1 ,
Ž
.
2
1
2
3
2
3
"F
s2 x q2 x s0,
7.54
Ž
.
1
2
1
" x1

THE RIEMANN INTEGRAL OF A MULTIVARIABLE FUNCTION
293
"F
s2 x q q2
2 x qx
s0,
7.55
Ž
.
Ž
.
2
1
2
2
3
" x2
"F
s2 x q q2
x qx
s0.
7.56
Ž
.
Ž
.
3
1
2
2
3
" x3
Ž
. Ž
.
Ž
.
Equations 7.54 , 7.55 , and 7.56 and the equality constraints are satisfied
by the following sets of solutions:
I. x s0, x s1, x sy1,  s2,  sy2.
1
2
3
1
2
II. x s0, x sy1, x s1,  sy2,  sy2.
1
2
3
1
2
III. x s1, x s0, x s0,  s0,  sy1.
1
2
3
1
2
IV. x sy1, x s0, x s0,  s0,  sy1.
1
2
3
1
2
To determine if any of these four sets correspond to local maxima or
minima, we need to examine the values of  ,  , . . . ,  
. Here, the matrix
1
2
nym
Ž
.
B in formula 7.53 has the value
1
2q2
0
0
0
2 x
2
1
0
2q4
2
1
4 x q2 x
2
2
2
3
B s
.
0
2
2q2
1
2 x q2 x
1
2
2
2
3
0
1
1
0
0
2 x
4 x q2 x
2 x q2 x
0
0
1
2
3
2
3
Since ns3 and ms2, only one  , namely,  , the determinant of B , is
i
1
1
needed. Furthermore, since m is even and n is odd, a sufficient condition for
a local minimum is  0, and for a local maximum the condition is  0.
1
1
It can be verified that  sy8 for solution sets I and II, and  s8 for
1
1
solution sets III and IV. We therefore have local maxima at the points
Ž
.
Ž
.
0, 1,y1 and
0,y1, 1 with a common maximum value f
s2. We also
max
Ž
.
Ž
.
have local minima at the points
1, 0,0
and
y1, 0, 0
with a common
minimum value f
s1. Since these are the only local optima on the curve of
min
intersection, we conclude that the minimum distance from the origin to this
'
curve is 1 and the maximum distance is
2 .
7.9. THE RIEMANN INTEGRAL OF A MULTIVARIABLE FUNCTION
In Chapter 6 we discussed the Riemann integral of a real-valued function of
a single variable x. In this section we extend the concept of Riemann
integration to real-valued functions of n variables, x , x , . . . , x .
1
2
n

MULTIDIMENSIONAL CALCULUS
294
Definition 7.9.1.
The set of points in Rn whose coordinates satisfy the
inequalities
a Fx Fb ,
is1, 2, . . . , n,
7.57
Ž
.
i
i
i
Ž
.
where a b , is1, 2, . . . , n, form an n-dimensional cell denoted by c
a, b .
i
i
n
Ž
.
n
Ž
.
The content
or volume
of this cell is Ł
b ya
and is denoted by
is1
i
i
w
Ž
.x
 c
a,b .
n
w
x
Suppose that P is a partition of the interval
a , b , is1, 2, . . . , n. The
i
i
i
n
Ž
.
Cartesian product Ps
P
is a partition of c
a, b
and consists of
is1
i
n
Ž
.
n-dimensional subcells of c
a, b . We denote these subcells by S , S , . . . , S .
n
1
2

Ž
.
The content of S is denoted by  S , is1, 2, . . . , , where  is the number
i
i
of subcells.

Ž .
We shall first define the Riemann integral of a real-valued function f x
on an n-dimensional cell; then we shall extend this definition to any bounded
region in Rn.
7.9.1. The Riemann Integral on Cells
n
Ž
.
Let f: D™R, where D;R . Suppose that c
a, b is an n-dimensional cell
n
Ž
.
contained in D and that f is bounded on c
a, b . Let P be a partition of
n
Ž
.
c
a, b consisting of the subcells S , S , . . . , S . Let m and M be, respec-
n
1
2

i
i
tively, the infimum and supremum of f on S , is1, 2, . . . , . Consider the
i
sums

LS
f s
m  S
,
7.58
Ž .
Ž
.
Ž
.
Ý
P
i
i
is1

US
f s
M  S
.
7.59
Ž .
Ž
.
Ž
.
Ý
P
i
i
is1
We note the similarity of these sums to the ones defined in Section 6.2. As
Ž .
Ž .
before, we refer to LS
f
and US
f
as the lower and upper sums,
P
P
respectively, of f with respect to the partition P.
The following theorem is an n-dimensional analogue of Theorem 6.2.1.
The proof is left to the reader.
Theorem 7.9.1.
Let f: D™R, where D;Rn. Suppose that f is bounded
Ž
.
Ž
.
on c
a, b ;D. Then f is Riemann integrable on c
a, b if and only if for
n
n
Ž
.
every 0 there exists a partition P of c
a, b such that
n
US
f yLS
f .
Ž .
Ž .
P
P
Ž
.
Definition 7.9.2.
Let P and P
be two partitions of c
a, b . Then P is
1
2
n
2
a refinement of P if every point in P is also a point in P , that is, P ;P .
1
1
2
1
2


THE RIEMANN INTEGRAL OF A MULTIVARIABLE FUNCTION
295
Using this definition, it is possible to prove results similar to those of
Lemmas 6.2.1 and 6.2.2. In particular, we have the following lemma:
Lemma 7.9.1.
Let f: D™R, where D;Rn. Suppose that f is bounded
Ž
.
Ž .
Ž .
on c
a, b ;D. Then sup
LS
f
and inf US
f
exist, and
n
P
p
P
P
sup LS
f F inf US
f .
Ž .
Ž .
P
P
P
P
Ž
.
Definition 7.9.3.
Let f: c
a, b ™R be a bounded function. Then f is
n
Ž
.
Riemann integrable on c
a, b if and only if
n
sup LS
f s inf US
f .
7.60
Ž .
Ž .
Ž
.
P
P
P
P
Ž
.
Their common value is called the Riemann integral of f on c
a, b and is
n
Ž .
b1
b2
denoted by H
f x dx. This is equivalent to the expression H H

c Ža, b.
a
a
n
1
2
bn Ž
.
H
f x , x , . . . , x
dx dx  dx . For example, for ns2, 3 we have
a
1
2
n
1
2
n
n
b
b
1
2
f x dxs
f x , x
dx dx ,
7.61
Ž .
Ž
.
Ž
.
H
H H
1
2
1
2
Ž
.
c
a, b
a
a
2
1
2
b
b
b
1
2
3
f x dxs
f x , x , x
dx dx dx .
7.62
Ž .
Ž
.
Ž
.
H
H H H
1
2
3
1
2
3
Ž
.
c
a, b
a
a
a
3
1
2
3
Ž
.
The integral in formula 7.61 is called a double Riemann integral, and the
Ž
.
one in formula
7.62
is called a triple Riemann integral. In general, for
Ž .
nG2, H
f x dx is called an n-tuple Riemann integral.

c Ža, b.
n
Ž .
The integral H
f x dx has properties similar to those of a single-varia-
c Ža, b.
n
ble Riemann integral in Section 6.4. The following theorem is an extension of
Theorem 6.3.1.
Ž
.
Theorem 7.9.2.
If f is continuous on an n-dimensional cell c
a, b , then
n
it is Riemann integrable there.
7.9.2. Iterated Riemann Integrals on Cells
The definition of the n-tuple Riemann integral in Section 7.9.1 does not
provide a practicable way to evaluate it. We now show that the evaluation of
this integral can be obtained by performing n Riemann integrals each of
which is carried out with respect to one variable. Let us first consider the
Ž
.
double integral as in formula 7.61 .
Ž
.
Lemma 7.9.2.
Suppose that f is real-valued and continuous on c
a, b .
2
Ž
.
Define the function g x
as
2
b1
g x
s
f x , x
dx .
Ž
.
Ž
.
H
2
1
2
1
a1
Ž
.
w
x
Then g x
is continuous on a , b .
2
2
2

MULTIDIMENSIONAL CALCULUS
296
Ž
.
Proof. Let 0 be given. Since f is continuous on c
a, b , which is
2
closed and bounded, then by Theorem 7.3.2, f is uniformly continuous on
Ž
.
c
a, b . We can therefore find a 0 such that
2

f  yf 

Ž .
Ž
.
b ya
1
1
	
	
Ž
.
Ž
.
w
x
if
y , where s x , x
, s y , y
, and x , y g a , b , x , y
1
2
1
2
1
1
1
1
2
2
w
x


g a , b . It follows that if y yx
, then
2
2
2
2
b1
g y
yg x
s
f x , y
yf x , x
dx
Ž
.
Ž
.
Ž
.
Ž
.
H
2
2
1
2
1
2
1
a1
b1
F
f x , y
yf x , x
dx
Ž
.
Ž
.
H
1
2
1
2
1
a1

b1

dx ,
7.63
Ž
.
H
1
b ya
a
1
1
1
	Ž
.
Ž
. 	


Ž
.
since
x , y
y x , x
 s y yx
. From inequality
7.63
we con-
1
2
1
2
2
2
clude that
g y
yg x

Ž
.
Ž
.
2
2


Ž
.
w
x
if
y yx
. Hence, g x
is continuous on a , b . Consequently, from
2
2
2
2
2
Ž
.
w
x
b2 Ž
.
Theorem 6.3.1, g x
is Riemann integrable on a , b , that is, H
g x
dx
2
2
2
a
2
2
2
exists. We call the integral
b
b
b
2
2
1
g x
dx s
f x , x
dx
dx
7.64
Ž
.
Ž
.
Ž
.
H
H H
2
2
1
2
1
2
a
a
a
2
2
1
an iterated integral of order 2.

Ž
.
The next theorem states that the iterated integral 7.64 is equal to the
Ž .
double integral H
f x dx.
c Ža, b.
2
Ž
.
Theorem 7.9.3.
If f is continuous on c
a, b , then
2
b
b
2
1
f x dxs
f x , x
dx
dx .
Ž .
Ž
.
H
H H
1
2
1
2
Ž
.
c
a, b
a
a
2
2
1
Proof. Exercise 7.22.

Ž
.
We note that the iterated integral in 7.64 was obtained by integrating
first with respect to x , then with respect to x . This order of integration
1
2

THE RIEMANN INTEGRAL OF A MULTIVARIABLE FUNCTION
297
could have been reversed, that is, we could have integrated f with respect to
x
and then with respect to x . The result would be the same in both cases.
2
1
Ž
.
This is based on the following theorem due to Guido Fubini 18791943 .
Ž
.
Ž
.
Theorem 7.9.4 Fubini’s Theorem .
If f is continuous on c
a, b , then
2
b
b
b
b
2
1
1
2
f x dxs
f x , x
dx
dx s
f x , x
dx
dx
Ž .
Ž
.
Ž
.
H
H H
H H
1
2
1
2
1
2
2
1
Ž
.
c
a, b
a
a
a
a
2
2
1
1
2
Ž
.
Proof. See Corwin and Szczarba 1982, page 287 .

A generalization of this theorem to multiple integrals of order n is given
w
Ž
.x
by the next theorem see Corwin and Szczarba 1982, Section 11.1 .
Ž
.
Theorem 7.9.5
Generalized Fubini’s Theorem .
If f is continuous on
Ž
.
 
4
c
a, b s x a Fx Fb , is1, 2, . . . , n , then
n
i
i
i
bi
f x dxs
f x dx
dx
,
is1, 2, . . . , n,
Ž .
Ž .
H
H
H
i
Ži.
Ži.
Ž
.
Ž
.
c
a, b
c
a, b
a
n
ny1
i
Ži. Ž
.
Ž
.
where
dx
sdx dx  dx
dx
 dx
and
c
a, b
is an
ny1 -
Ži.
1
2
iy1
iq1
n
ny1
dimensional
cell
such
that
a F x F b , a F x F b , . . . , a
F x
F
1
1
1
2
2
2
iy1
iy1
b
, a
Fx
Fb
, . . . , a Fx Fb .
iy1
iq1
iq1
iq1
n
n
n
7.9.3. Integration over General Sets
We now consider n-tuple Riemann integration over regions in Rn that are
not necessarily cell shaped as in Section 7.9.1.
Let f: D™R be a bounded and continuous function, where D is a
n
Ž
.
bounded region in R . There exists an n-dimensional cell c
a, b such that
n
Ž
.
Ž
.
D;c
a, b . Let g: c
a, b ™R be defined as
n
n
f x ,
xgD,
Ž .
g x s
Ž . ½ 0,
xD.
Then
g x dxs
f x dx.
7.65
Ž .
Ž .
Ž
.
H
H
Ž
.
c
a, b
D
n
Ž
.
The integral on the right-hand side of 7.65 is independent of the choice of
Ž
.
Ž .
c
a, b provided that it contains D. It should be noted that the function g x
n
Ž
.
may not be continuous on Br D , the boundary of D. This, however, should
Ž
.
not affect the existence of the integral on the left-hand side of 7.65 . The
reason for this is given in Theorem 7.9.7. First, we need to define the
so-called Jordan content of a set.

MULTIDIMENSIONAL CALCULUS
298
n
Ž
.
Definition 7.9.4.
Let D;R
be a bounded set such that D;c
a, b for
n
some n-dimensional cell. Let the function  : Rn™R be defined as
D
1,
xgD,

x s
Ž .
D
½ 0,
xD.
This is called the characteristic function of D. Suppose that
sup LS

s inf US

,
7.66
Ž
.
Ž
.
Ž
.
P
D
P
D
P
P
Ž
.
Ž
.
where LS

and US

are, respectively, the lower and upper sums of
P
D
P
D
Ž .
Ž
.

x with respect to a partition P of c
a, b . Then, D is said to have an
D
n
Ž
.
Ž
.
n-dimensional Jordan content denoted by  D , where  D is equal to the
j
j
Ž
.
common value of the terms in equality 7.66 . In this case, D is said to be
Jordan measurable.

Ž
The proofs of the next two theorems can be found in Sagan 1974, Chapter
.
11 .
Theorem 7.9.6.
A bounded set D;Rn is Jordan measurable if and only
Ž
.
if its boundary Br D has a Jordan content equal to zero.
Theorem 7.9.7.
Let f: D™R, where D;Rn is bounded and Jordan
measurable. If f is bounded and continuous in D except on a set that has a
Ž .
Jordan content equal to zero, then H f x dx exists.
D
Ž
.
It follows from Theorems 7.9.6 and 7.9.7 that the integral in equality 7.75
Ž .
Ž
.
must exist even though g x may not be continuous on the boundary Br D
Ž
.
of D, since Br D has a Jordan content equal to zero.
Ž
.
EXAMPLE 7.9.1.
Let f x , x
sx x
and D be the region
1
2
1
2

2
2
Ds
x , x
x qx F1, x G0, x G0 .
Ž
.

4
1
2
1
2
1
2
It is easy to see that D is contained inside the two-dimensional cell

c
0, 1 s
x , x
0Fx F1, 0Fx F1 .

4
Ž
.
Ž
.
2
1
2
1
2
Then
2 1r2
1
Ž
.
1yx 1
x x dx dx s
x x dx
dx .
HH
H H
1
2
1
2
1
2
2
1
D
0
0

THE RIEMANN INTEGRAL OF A MULTIVARIABLE FUNCTION
299
w
x
Ž
.
We note that for a fixed x
in 0, 1 , the part of the line through
x , 0 that
1
1
lies inside D and is parallel to the x -axis is in fact the interval 0Fx F
2
2
Ž
2.1r2
Ž
2.1r2
1yx
. For this reason, the limits of x
are 0 and
1yx
. Conse-
1
2
1
quently,
2 1r2
2 1r2
1
1
Ž
.
Ž
.
1yx
1yx
1
1
x x dx
dx s
x
x dx
dx
H H
H
H
1
2
2
1
1
2
2
1
0
0
0
0
1
1
2
s
x 1yx
dx
Ž
.
H
1
1
1
2
0
1
s .
8
Ž
.
In practice, it is not always necessary to make reference to c
a, b that
n
encloses D in order to evaluate the integral on D. Rather, we only need to
recognize that the limits of integration in the iterated Riemann integral
depend in general on variables that have not yet been integrated out, as was
seen in Example 7.9.1. Care should therefore be exercised in correctly
identifying the limits of integration. By changing the order of integration
Ž
.
according to Fubini’s theorem , it is possible to facilitate the evaluation of
the integral.
EXAMPLE 7.9.2.
Consider H H e x 2
2 dx dx , where D is the region in the
D
1
2
first quadrant bounded by x s1 and x sx . In this example, it is easier to
2
1
2
integrate first with respect to x and then with respect to x . Thus
1
2
x
1
2
2
2
x
x
2
2
e
dx dx s
e
dx
dx
HH
H H
1
2
1
2
D
0
0
1
2
x 2
s
x e
dx
H
2
2
0
1
s
ey1 .
Ž
.
2
Ž
2
3.
EXAMPLE 7.9.3.
Consider the integral H H
x qx
dx dx , where D is a
D
1
2
1
2
region in the first quadrant bounded by x sx 2 and x sx 4. Hence,
2
1
1
2
1
x' 2
2
3
2
3
x qx
dx dx s
x qx
dx
dx
Ž
.
Ž
.
HH
H H
1
2
1
2
1
2
1
2
4
D
0
x2
1
1
3r2
12
4
3
s
x
yx
q
x yx
x
dx
'
Ž
.
H
ž
/
2
2
2
2
2
2
3
0
959
s
.
4680
7.9.4. Change of Variables in n-Tuple Riemann Integrals
In this section we give an extension of the change of variables formula in
Section 6.4.1 to n-tuple Riemann integrals.

MULTIDIMENSIONAL CALCULUS
300
Theorem 7.9.8.
Suppose that D is a closed and bounded set in Rn. Let
f: D™R be continuous. Suppose that h: D™Rn is a one-to-one function
with continuous first-order partial derivatives such that the Jacobian determi-
nant,
" h , h , . . . , h
Ž
.
1
2
n
det J
x
s
,
Ž .
h
" x , x , . . . , x
Ž
.
1
2
n
Ž
.
is different from zero for all x in D, where xs x , x , . . . , x
 and h is the
1
2
n
i
Ž
.
ith element of h is1, 2, . . . , n . Then
f x dxs
f g u
det J
u
du,
7.67
Ž .
Ž .
Ž .
Ž
.
H
H
g
D
D
Ž
.
Ž .
where Dsh D , ush x , g is the inverse function of h, and
" g , g , . . . , g
Ž
.
1
2
n
det J
u s
,
7.68
Ž .
Ž
.
g
" u , u , . . . , u
Ž
.
1
2
n
Ž
.
where g and u are, respectively, the ith elements of g and u is1, 2, . . . , n .
i
i
Ž
.
Proof. See, for example, Corwin and Szczarba
1982, Theorem 6.2 , or
Ž
.
Sagan 1974, Theorem 115.1 .

EXAMPLE 7.9.4.
Consider the integral
H H x x 2 dx dx , where
D is
D
1
2
1
2
bounded by the four parabolas, x 2sx , x 2s3x , x 2sx , x 2s4 x . Let u s
2
1
2
1
1
2
1
2
1
x 2rx , u sx 2rx . The inverse transformation is given by
2
1
2
1
2
1r3
1r3
2
2
x s u u
,
x s u u
.
Ž
.
Ž
.
1
1
2
2
1
2
Ž
.
From formula 7.68 we have
" g , g
" x , x
1
Ž
.
Ž
.
1
2
1
2
s
sy
.
" u , u
" u , u
3
Ž
.
Ž
.
1
2
1
2
Ž
.
By applying formula 7.67 we obtain
1
2
5r3
4r3
x x dx dx s
u
u
du du ,
HH
HH
1
2
1
2
1
2
1
2
3
D
D
where D is a rectangular region in the u u
space bounded by the lines
1
2
u s1, 3; u s1, 4. Hence,
1
2
3
4
1
2
5r3
4r3
x x dx dx s
u
du
u
du
HH
H
H
1
2
1
2
1
1
2
2
3
D
1
1
3
8r3
7r3
s
3
y1
4
y1 .
Ž
. Ž
.
56

DIFFERENTIATION UNDER THE INTEGRAL SIGN
301
7.10. DIFFERENTIATION UNDER THE INTEGRAL SIGN
Ž
.
n
Suppose that f x , x , . . . , x
is a real-valued function defined on D;R . If
1
2
n
Ž
.
some of the x ’s, for example, x
, x
, . . . , x
nm , are integrated out,
i
mq1
mq2
n
we obtain a function that depends only on the remaining variables. In this
section we discuss conditions under which the latter function is differen-
tiable. For simplicity, we shall only consider functions of ns2 variables.
Theorem 7.10.1.
Let f: D™R, where D;R2 contains the two-dimen-
Ž
.
Ž
.
4
sional cell c
a, b s
x , x
a Fx Fb , a Fx Fb . Suppose that f is
2
1
2
1
1
1
2
2
2
continuous and has a continuous first-order partial derivative with respect to
x
in D. Then, for a x b ,
2
2
2
2
d
" f x , x
Ž
.
b
b
1
2
1
1
f x , x
dx s
dx .
7.69
Ž
.
Ž
.
H
H
1
2
1
1
dx
" x
a
a
2
2
1
1
Ž
.
w
x
Proof. Let h x
be defined on a , b
as
2
2
2
" f x , x
Ž
.
b
1
2
1
h x
s
dx ,
a Fx Fb .
Ž
. H
2
1
2
2
2
" x
a
2
1
Ž
.
Since " fr" x
is continuous, then by Lemma 7.9.2, h x
is continuous on
2
2
w
x
Ž
.
a , b . Now, let t be such that a tb . By integrating h x
over the
2
2
2
2
2
w
x
interval a , t we obtain
2
" f x , x
Ž
.
t
t
b
1
2
1
h x
dx s
dx
dx .
7.70
Ž
.
Ž
.
H
H H
2
2
1
2
" x
a
a
a
2
2
2
1
Ž
.
The order of integration in 7.70 can be reversed by Theorem 7.9.4. We than
have
" f x , x
Ž
.
t
b
t
1
2
1
h x
dx s
dx
dx
Ž
.
H
H H
2
2
2
1
" x
a
a
a
2
2
1
2
b1
s
f x , t yf x , a
dx
Ž
.
Ž
.
H
1
1
2
1
a1
b
b
1
1
s
f x , t
dx y
f x , a
dx
Ž
.
Ž
.
H
H
1
1
1
2
1
a
a
1
1
sF t yF a
,
7.71
Ž .
Ž
.
Ž
.
2

MULTIDIMENSIONAL CALCULUS
302
Ž .
b1 Ž
.
where F y sH
f x , y dx . If we now apply Theorem 6.4.8 and differenti-
a
1
1
1
Ž
.
Ž .
Ž .
ate the two sides of 7.71 with respect to t we obtain h t sF t , that is,
" f x , t
d
Ž
.
b
b
1
1
1
dx s
f x , t
dx .
7.72
Ž
.
Ž
.
H
H
1
1
1
" t
dt
a
a
1
1
Ž
.
Ž
.
Formula 7.69 now follows from formula 7.72 on replacing t with x .

2
Theorem 7.10.2.
Let
f and
D be the same as in Theorem 7.10.1.
Ž
.
Ž
.
Furthermore, let  x
and  x
be functions defined and having continu-
2
2
w
x
Ž
.
Ž
.
ous derivatives on
a , b
such that a F x
F x
Fb
for all x
in
2
2
1
2
2
1
2
w
x
w
x
a , b . Then the function G: a , b
™R defined by
2
2
2
2
Ž
.
 x2
G x
s
f x , x
dx
Ž
.
Ž
.
H
2
1
2
1
Ž
.
 x2
is differentiable for a x b , and
2
2
2
dG
" f x , x
Ž
.
Ž
.
 x
1
2
2
s
dx q x
f  x
, x
y x
f  x
, x
.
Ž
.
Ž
.
Ž
.
Ž
.
H
1
2
2
2
2
2
2
dx
" x
Ž
.
 x
2
2
2
Ž
.
Ž
.
Proof. Let us write G x
as H , , x
. Since both of  and  depend on
2
2
w
Ž
.x
x , then by applying the total derivative formula see formula 7.12
to H we
2
obtain
dH
"H d
"H d
"H
s
q
q
.
7.73
Ž
.
dx
" dx
" dx
" x
2
2
2
2
Now, by Theorem 6.4.8,
"H
"

s
f x , x
dx sf  , x
,
Ž
.
Ž
.
H
1
2
1
2
"
"

"H
"

s
f x , x
dx
Ž
.
H
1
2
1
"
"

"

sy
f x , x
dx syf , x
.
Ž
.
Ž
.
H
1
2
1
2
"

Furthermore, by Theorem 7.10.1,
"H
"
" f x , x
Ž
.


1
2
s
f x , x
dx s
dx .
Ž
.
H
H
1
2
1
1
" x
" x
" x


2
2
2

DIFFERENTIATION UNDER THE INTEGRAL SIGN
303
Ž
.
By making the proper substitution in formula 7.73 we finally conclude that
d
" f x , x
Ž
.
Ž
.
Ž
.
 x
 x
1
2
2
2
f x , x
dx s
dx q x
f  x
, x
Ž
.
Ž
.
Ž
.
H
H
1
2
1
1
2
2
2
dx
" x
Ž
.
Ž
.
 x
 x
2
2
2
2
y x
f  x
, x
.

Ž
.
Ž
.
2
2
2
EXAMPLE 7.10.1.
cos x
cos x
d
2
2
2
yx
yx
1
1
x x y1 e
dx s
2 x x e
dx
Ž
.
H
H
1
2
1
1
2
1
2
2
dx
x
x
2
2
2
ysin x
x 2 cos x y1 eycos x 2
Ž
.
2
2
2
y2 x
x 4y1 eyx 2
2.
Ž
.
2
2
Theorems 7.10.1 and 7.10.2 can be used to evaluate certain integrals of the
b Ž .
form H f x dx. For example, consider the integral
a

2
Is
x cos x dx.
H
0
Define the function

F x
s
cos x x
dx ,
Ž
.
Ž
.
H
2
1
2
1
0
where x G1. Then
2
x s
1
1
1
F x
s
sin x x
s
sin  x
.
Ž
.
Ž
.
Ž
.
2
1
2
2
x
x
2
2
x s0
1
Ž
.
If we now differentiate F x
two times, we obtain
2
2 sin  x
y2 x cos  x
y 2 x 2 sin  x
Ž
.
Ž
.
Ž
.
2
2
2
2
2
F
x
s
.
Ž
.
2
3
x2
Thus

2
2
2 x cos  x
q x sin  x
y2 sin  x
Ž
.
Ž
.
Ž
.
2
2
2
2
2
2
x cos x x
dx s
.
Ž
.
H
1
1
2
1
3
x
0
2

MULTIDIMENSIONAL CALCULUS
304
By replacing x
with 1 we obtain
2

2
Is
x cos x dx sy2 .
H
1
1
1
0
7.11. APPLICATIONS IN STATISTICS
Multidimensional calculus provides a theoretical framework for the study of
multivariate distributions, that is, joint distributions of several random vari-
ables. It can also be used to estimate the parameters of a statistical model.
We now provide details of some of these applications.
Ž
.
Let Xs X , X , . . . , X  be a random vector. The distribution of X is
1
2
n
characterized by its cumulative distribution function, namely,
F x sP X Fx , X Fx , . . . , X Fx
,
7.74
Ž .
Ž
.
Ž
.
1
1
2
2
n
n
Ž
.
Ž .
where xs x , x , . . . , x
. If F x is continuous and has an nth-order mixed
1
2
n
partial derivative with respect to x , x , . . . , x , then the function
1
2
n
" nF x
Ž .
f x s
,
Ž .
" x " x  " x
1
2
n
Ž
.
is called the density function of X. In this case, formula 7.74 can be written
in the form
x
x
x
1
2
n
F x s

f z dz.
Ž .
Ž .
H H
H
y y
y
Ž
.
Ž
.
where zs z , z , . . . , z
. If the random variable X
is1, 2, . . . , n is con-
1
2
n
i
sidered separately, then its distribution function is called the ith marginal
Ž
.
distribution of X. Its density function f x , called the ith marginal density
i
i
function, can be obtained by integrating out the remaining ny1 variables
Ž .
Ž
.
from f x . For example, if Xs X , X
, then the marginal density function
1
2
of X
is
1

f
x
s
f x , x
dx .
Ž
.
Ž
.
H
1
1
1
2
2
y
Similarly, the marginal density function of X
is
2

f
x
s
f x , x
dx .
Ž
.
Ž
.
H
2
2
1
2
1
y

APPLICATIONS IN STATISTICS
305
In particular, if X , X , . . . , X
are independent random variables, then the
1
2
n
Ž
.
density function of Xs X , X , . . . , X  is the product of all the associated
1
2
n
Ž .
n
Ž
.
marginal density functions, that is, f x sŁ
f x .
is1
i
i
Ž .
If only ny2 variables are integrated out from
f x , we obtain the
so-called bivariate density function of the remaining two variables. For
Ž
.
example, if Xs X , X , X , X , the bivariate density function of x
and
1
2
3
4
1
x
is
2


f
x , x
s
f x , x , x , x
dx dx .
Ž
.
Ž
.
H H
12
1
2
1
2
3
4
3
4
y y
Ž
.
Ž
.
Now, the mean of Xs X , X , . . . , X  is s  ,  , . . . ,  , where
1
2
n
1
2
n

 s
x f
x
dx ,
is1, 2, . . . , n.
Ž
.
H
i
i
i
i
i
y
Ž
.
The variancecovariance matrix of X is the nn matrix s 	
, where
i j


	 s
x y
x y
f
x , x
dx dx ,
Ž
. Ž
.
Ž
.
H H
i j
i
i
j
j
i j
i
j
i
j
y y
Ž
.
where  and  are the means of X and X , respectively, and f
x , x
is
i
j
i
j
i j
i
j
the bivariate density function of X and X , ij. If isj, then 	
is the
i
j
ii
variance of X , where
i

2
	 s
x y
f
x
dx ,
is1, 2, . . . , n.
Ž
.
Ž
.
H
ii
i
i
i
i
i
y
7.11.1. Transformations of Random Vectors
Ž
.
In this section we consider a multivariate extension of formula 6.73 regard-
ing the density function of a function of a single random variable. This is
given in the next theorem.
Theorem 7.11.1.
Let X be a random vector with a continuous density
Ž .
n
n
function f x . Let g: D™R , where D is an open subset of R
such that
Ž
.
P XgD s1. Suppose that g satisfies the conditions of the inverse function
Ž
.
theorem Theorem 7.6.1 , namely the following:
i. g has continuous first-order partial derivatives in D.
Ž .
ii. The Jacobian matrix J x is nonsingular in D, that is,
g
" g , g , . . . , g
Ž
.
1
2
n
det J
x
s
0
Ž .
g
" x , x , . . . , x
Ž
.
1
2
n
Ž
.
for all xgD, where g is the ith element of g is1, 2, . . . , n .
i

MULTIDIMENSIONAL CALCULUS
306
Ž .
Then the density function of Ysg X is given by
y1
y1
h y sf g
y
det J
y
,
Ž .
Ž .
Ž .
g
where gy1 is the inverse function of g.
Proof. By Theorem 7.6.1, the inverse function of g exists. Let us therefore
y1Ž .
write Xsg
Y . Now, the cumulative distribution function of Y is
H y sP g
X Fy ,
g
X Fy , . . . ,
g
X Fy
Ž .
Ž .
Ž .
Ž .
1
1
2
2
n
n
s
f x dx,
7.75
Ž .
Ž
.
H
An


Ž .
4
where A s xgD g x Fy , is1, 2, . . . , n . If we make the change of vari-
n
i
i
Ž .
Ž
.
y1Ž .
able wsg x in formula 7.75 , then, by applying Theorem 7.9.8 with g
w
Ž .
used instead of g u , we obtain
y1
y1
f x dxs
f g
w
det J
w
dw,
Ž .
Ž .
Ž .
H
H
g
A
B
n
n
Ž
.
 Ž .
Ž .
4
where B sg A
s g x g x Fy , is1, 2, . . . , n . Thus
n
n
i
i
y
y
y
1
2
n
y1
y1
H y s

f g
w
det J
w
dw.
Ž .
Ž .
Ž .
H H
H
g
y y
y
It follows that the density function of Y is
y1
y1
y1
" g
, g
, . . . , g
Ž
.
1
2
n
y1
h y sf g
y
,
7.76
Ž .
Ž .
Ž
.
"
y , y , . . . , y
Ž
.
1
2
n
y1
y1 Ž
.
where g
is the ith element of g
is1, 2, . . . , n .

i
Ž
.
EXAMPLE 7.11.1.
Let Xs X , X
, where X
and X
are independent
1
2
1
2
random variables that have the standard normal distribution. Here, the
density function of X is the product of the density functions of X
and X .
1
2
Thus
1
1
2
2
f x s
exp y
x qx
,
yx , x .
Ž .
Ž
.
1
2
1
2
2
2
Ž
.
Let Ys Y , Y  be defined as
1
2
Y sX qX ,
1
1
2
Y sX y2 X .
2
1
2

APPLICATIONS IN STATISTICS
307
2
Ž .
Ž .
In this case, the set D in Theorem 7.11.1 is R , g x sx qx , g x sx y
1
1
2
2
1
1
1
y1
y1
Ž .
Ž
.
Ž .
Ž
.
2 x , g
y sx s
2 y qy
, g
y sx s
y yy
, and
2
1
1
1
2
2
2
1
2
3
3
2
1
y1
y1
" g
, g
1
Ž
.
1
2
3
3
sdet
sy
.
1
1
"
y , y
3
y
Ž
.
1
2
3
3
Ž
.
Hence, by formula 7.76 , the density function of y is
2
2
1
1
2 y qy
1
y yy
1
1
2
1
2
h y s
exp y
y

Ž .
ž
/
ž
/
2
2
3
2
3
3
1
1
2
2
s
exp y
5y q2 y y q2 y
,
yy , y .
Ž
.
1
1
2
2
1
2
6
18
EXAMPLE 7.11.2.
Suppose that it is desired to determine the density
function of the random variable VsX qX , where X G0, X G0, and
1
2
1
2
Ž
.
Ž
.
Xs X , X
 has a continuous density function
f x , x
. This can be
1
2
1
2
accomplished in two ways:
Ž .
Ž .
i. Let Q ® denote the cumulative distribution function of V and let q ®
be its density function. Then
Q ® sP X qX F®
Ž .
Ž
.
1
2
s
f x , x
dx dx ,
Ž
.
HH
1
2
1
2
A
Ž
.
4
Ž .
where As
x , x
x G0, x G0, x qx F® . We can write Q ® as
1
2
1
2
1
2
®
®yx2
Q ® s
f x , x
dx
dx .
Ž .
Ž
.
H H
1
2
1
2
0
0
If we now apply Theorem 7.10.2, we obtain
®
®yx
dQ
"
2
q ® s
s
f x , x
dx
dx
Ž .
Ž
.
H
H
1
2
1
2
d®
" ®
0
0
®
s
f ®yx , x
dx .
7.77
Ž
.
Ž
.
H
2
2
2
0
ii. Consider the following transformation:
Y sX qX ,
1
1
2
Y sX .
2
2

MULTIDIMENSIONAL CALCULUS
308
Then
X sY yY ,
1
1
2
X sY .
2
2
Ž
.
By Theorem 7.11.1, the density function of Ys Y , Y  is
1
2
" x , x
Ž
.
1
2
h y , y
sf y yy , y
Ž
.
Ž
.
1
2
1
2
2
"
y , y
Ž
.
1
2
1
y1
sf y yy , y
det
Ž
.
1
2
2
0
1
sf y yy , y
, y Gy G0.
Ž
.
1
2
2
1
2
By integrating
y
out we obtain the marginal density function of
2
Y sV, namely,
1
®
®
q ® s
f y yy , y
dy s
f ®yx , x
dx .
Ž .
Ž
.
Ž
.
H
H
1
2
2
2
2
2
2
0
0
Ž
.
This is identical to the density function given in formula 7.77 .
7.11.2. Maximum Likelihood Estimation
Let X , X , . . . , X
be a sample of size n from a population whose distribu-
1
2
n
tion depends on a set of p parameters, namely  ,  , . . . ,  . We can regard
1
2
p
Ž
.
this sample as forming a random vector Xs X , X , . . . , X . Suppose that
1
2
n
Ž
.
Ž
.
X has the density function
f x,  , where xs x , x , . . . , x
 and s
1
2
n
Ž
.
 ,  , . . . ,  . This density function is usually referred to as the likelihood
1
2
p
Ž
.
function of X; we denote it by L x,  .
ˆ
For a given sample, the maximum likelihood estimate of , denoted by ,
Ž
.
Ž
.
is the value of  that maximizes L x,  . If L x,  has partial derivatives with
ˆ
respect to  ,  , . . . ,  , then  is often obtained by solving the equations
1
2
p
ˆ
"L x, 
Ž
.
s0,
is1, 2, . . . , p.
"i
In most situations, it is more convenient to work with the natural logarithm
Ž
.
Ž
.
of L x,  ; its maxima are attained at the same points as those of L x,  .
ˆ
Thus  satisfies the equation
ˆ
" log L x, 
Ž
.
s0,
is1, 2, . . . , p.
7.78
Ž
.
"i
Ž
.
Equations 7.78 are known as the likelihood equations.

APPLICATIONS IN STATISTICS
309
EXAMPLE 7.11.3.
Suppose that X , X , . . . , X
form a sample of size n
1
2
n
from a normal distribution with an unknown mean  and a variance 	 2.
Ž
2.
Here, s , 	
, and the likelihood function is given by
n
1
1
2
L x,  s
exp y
x y
,
Ž
.
Ž
.
Ý
i
nr2
2
2
2	
2	
Ž
.
is1
Ž
.
Ž
.
Let L* x,  slog L x,  . Then
n
1
n
2
2
L* x,  sy
x y
y
log 2	
.
Ž
.
Ž
.
Ž
.
Ý
i
2
2
2	
is1
Ž
.
The likelihood equations in formula 7.78 are of the form
n
"L*
1
s
x y s0
7.79
Ž
.
Ž
.
ˆ
Ý
i
2
"
	
is1
n
"L*
1
n
2
s
x y
y
s0.
7.80
Ž
.
Ž
.
ˆ
Ý
i
2
4
2
"	
2	
2	
ˆ
ˆ
is1
Ž
.
Ž
.
Equations 7.79 and 7.80 can be written as
n xy s0,
7.81
Ž
.
Ž
.
ˆ
n
2
2
x y
yn	 s0,
7.82
Ž
.
Ž
.
ˆ
ˆ
Ý
i
is1
n
Ž
.
Ž
.
Ž
.
where xs 1rn Ý
x . If nG2, then equations 7.81 and 7.82 have the
is1
i
solution
sx,
ˆ
n
1
2
2
	 s
x yx
.
Ž
.
ˆ
Ý
i
n is1
These are the maximum likelihood estimates of  and 	 2, respectively.
It can be verified that  and 	 2 are indeed the values of  and 	 2 that
ˆ
ˆ
Ž
.
maximize L* x,  . To show this, let us consider the Hessian matrix A of
Ž
.
second-order partial derivatives of L* see formula 7.34 ,
2
2
" L*
" L*
2
2
"
" "	
As
.
2
2
" L*
" L*
2
4
" "	
"	

MULTIDIMENSIONAL CALCULUS
310
Hence, for s and 	 2s	 2,
ˆ
ˆ
" 2L*
n
sy
,
2
2
"
	ˆ
2
n
" L*
1
sy
x y s0,
Ž
.
ˆ
Ý
i
2
4
" "	
	ˆ
is1
" 2L*
n
sy
.
4
4
"	
2	ˆ
2
2
Ž .
2
6
Thus " L*r" 0 and det A sn r2	 0. Therefore, by Corollary 7.7.1,
ˆ
Ž
2.
, 	
is a point of local maximum of L*. Since it is the only maximum, it
ˆ ˆ
must also be the absolute maximum.
Maximum likelihood estimators have interesting asymptotic properties.
For more information on these properties, see, for example, Bickel and
Ž
.
Doksum 1977, Section 4.4 .
7.11.3. Comparison of Two Unbiased Estimators
Let X
and X
be two unbiased estimators of a parameter . Suppose that
1
2
Ž
.
Ž
.
Xs X , X
 has the density function f x , x
, yx , x . To com-
1
2
1
2
1
2
pare these estimators, we may consider the probability that one estimator, for
example, X , is closer to  than the other, X , that is,
1
2




psP
X y  X y
.
1
2
This probability can be expressed as
ps
f x , x
dx dx ,
7.83
Ž
.
Ž
.
HH
1
2
1
2
D
Ž
. 


 4
where Ds
x , x
x y  x y . Let us now make the following
1
2
1
2
change of variables using polar coordinates:
x ysr cos  ,
x ysr sin .
1
2
Ž
.
Ž
.
By applying formula 7.67 , the integral in 7.83 can be written as
" x , x
Ž
.
1
2
ps
g r, 
drd
Ž
.
˜
HH
" r, 
Ž
.
D
s
g r,  r drd ,
Ž
.
˜
HH
D

APPLICATIONS IN STATISTICS
311
Ž
.
Ž
.
where g r,  sf qr cos , qr sin 
and
˜

3 5
7

D
r, 
0Fr,
FF
,
FF
.
Ž
.
½
5
4
4
4
4
In particular, if X has the bivariate normal density, then
1
f x , x
s
Ž
.
1
2
1r2
2
2	 	
1y
Ž
.
1
2
2
1
x y
2 x y
x y
Ž
.
Ž
. Ž
.
1
1
2
exp y
y
2
2
½
	 	
	
2 1y
Ž
.
1
2
1
2
x y
Ž
.
2
q
,
yx , x ,
1
2
2
5
	2
and
1
g r,  s
Ž
.
˜
1r2
2
2	 	
1y
Ž
.
1
2
2
2
2
r
cos 
2 cos  sin 
sin 
exp y
y
q
,
2
2
2
½
5
	 	
	
	
2 1y
Ž
.
1
2
1
2
where 	 2 and 	 2 are the variances of X
and X , respectively, and  is
1
2
1
2
their correlation coefficient. In this case,

3r4
ps2
g r,  r dr
d.
Ž
.
˜
H
H
r4
0
Ž
.
It can be shown see Lowerre, 1983 that
1r2
2
1
2	 	
1y
Ž
.
1
2
ps1y
Arctan
7.84
Ž
.
2
2

	 y	
2
1
if 	 	 . A large value of p indicates that X is closer to  than X , which
2
1
1
2
means that X
is a better estimator of  than X .
1
2
7.11.4. Best Linear Unbiased Estimation
Let X , X , . . . , X
be independent and identically distributed random vari-
1
2
n
ables with a common mean  and a common variance 	 2. An estimator of

MULTIDIMENSIONAL CALCULUS
312
ˆ
n
the form sÝ
a X , where the a ’s are constants, is said to be a linear
is1
i
i
i
ˆ
n
Ž
.
estimator of . This estimator is unbiased if E  s, that is, if Ý
a s1,
is1
i
ˆ
Ž
.
since E X s for is1, 2, . . . , n. The variance of  is given by
i
n
2
2
ˆ
Var  s	
a .
Ž .
Ý
i
is1
ˆ
ˆ
The smaller the variance of , the more efficient  is as an estimator of .
ˆ
Ž
.
In particular, if a , a , . . . , a
are chosen so that Var 
attains a minimum
1
2
n
ˆ
value, then  will have the smallest variance among all unbiased linear
ˆ
estimators of . In this case,  is called the best linear unbiased estimator
Ž
.
BLUE of .
Thus to find the BLUE of  we need to minimize the function fsÝn
a2
is1
i
subject to the constraint Ýn
a s1. This minimization problem can be solved
is1
i
w
using the method of Lagrange multipliers. Let us therefore write F
see
Ž
.x
formula 7.41
as
n
n
2
Fs
a q
a y1 ,
Ý
Ý
i
i
ž
/
is1
is1
"F
s2a qs0,
is1, 2, . . . , n.
i
" ai
Ž
.
n
Hence, a syr2 is1, 2, . . . , n . Using the constraint Ý
a s1, we con-
i
is1
i
clude that sy2rn. Thus a s1rn, is1, 2, . . . , n. To verify that this
i
solution minimizes f, we need to consider the signs of  ,  , . . . ,  
,
1
2
ny1
Ž
.
Ž
.
where  is the determinant of B
see Section 7.8 . Here, B is an nq1 
i
i
1
Ž
.
nq1 matrix of the form
2I
1
n
n
B s
.

1
1
0
n
It follows that
n2 n
 sdet B
sy
0,
Ž
.
1
1
2
ny1 2 ny1
Ž
.
 sy
0,
2
2
...
 
sy220.
ny1
Since the number of constraints, ms1, is odd, then by the sufficient

APPLICATIONS IN STATISTICS
313
conditions described in Section 7.8 we must have a local minimum when
a s1rn, is1, 2, . . . , n. Since this is the only local minimum in Rn, it must be
i
ˆ
the absolute minimum. Note that for such values of a , a , . . . , a ,  is the
1
2
n
Ž
sample mean X . We conclude that the sample mean is the most efficient in
n
.
terms of variance unbiased linear estimator of .
7.11.5. Optimal Choice of Sample Sizes in Stratified Sampling
In stratified sampling, a finite population of N units is divided into r
subpopulations, called strata, of sizes N , N , . . . , N . From each stratum a
1
2
r
random sample is drawn, and the drawn samples are obtained independently
in the different strata. Let n be the size of the sample drawn from the ith
i
Ž
.
stratum is1, 2, . . . , r . Let y
denote the response value obtained from the
i j
Ž
.
jth unit within the ith stratum is1, 2, . . . , r; js1, 2, . . . , n . The population
i
mean Y is
N
r
r
i
1
1
Ys
y s
N Y ,
Ý Ý
Ý
i j
i
i
N
N
is1 js1
is1
Ž
.
where Y is the true mean for the ith stratum
is1, 2, . . . , r . A stratified
i
Ž
.
estimate of Y is y
st for stratified , where
st
r
1
y s
N y ,
Ý
st
i
i
N is1
ni
Ž
.
in which y s 1rn Ý
y
is the mean of the sample from the ith stratum
i
i
js1
i j
Ž
.
is1, 2, . . . , r . If, in every stratum, y is unbiased for Y , then y
is an
i
i
st
unbiased estimator of Y. The variance of y
is
st
r
1
2
Var y
s
N
Var y
.
Ž
.
Ž
.
Ý
st
i
i
2
N
is1
Since y is the mean of a random sample from a finite population, then its
i
Ž
.
variance is given by see Cochran, 1963, page 22
S2
i
Var y
s
1yf
,
is1, 2, . . . , r,
Ž
.
Ž
.
i
i
ni
where f sn rN , and
i
i
i
Ni
1
2
2
S s
y yY
.
Ý ž
/
i
i j
i
N y1
i
js1

MULTIDIMENSIONAL CALCULUS
314
Hence,
r
1
2
2
Var y
s
L S
1yf
,
Ž
.
Ž
.
Ý
st
i
i
i
ni
is1
Ž
.
where L sNrN is1, 2, . . . , r .
i
i
The sample sizes n , n , . . . , n can be chosen by the sampler in an optimal
1
2
r
Ž
.
way, the optimality criterion being the minimization of Var y
for a speci-
st
fied cost of taking the samples. Here, the cost is defined by the formula
r
costsc q
c n ,
Ý
0
i
i
is1
Ž
.
where c is the cost per unit in the ith stratum is1, 2, . . . , r and c
is the
i
0
overhead cost. Thus the optimal choice of the sample sizes is reduced to
r
Ž
.
2
2Ž
.
finding the values of n , n , . . . , n
that minimize Ý
1rn L S
1yf
1
2
r
is1
i
i
i
i
subject to the constraint
r
c n sdyc ,
7.85
Ž
.
Ý
i
i
0
is1
where d is a constant. Using the method of Lagrange multipliers, we write
r
r
1
2
2
Fs
L S
1yf
q
c n qc yd
Ž
.
Ý
Ý
i
i
i
i
i
0
ž
/
ni
is1
is1
r
r
r
1
1
2
2
2
2
s
L S y
L S q
c n qc yd .
Ý
Ý
Ý
i
i
i
i
i
i
0
ž
/
n
N
i
i
is1
is1
is1
Ž
.
Differentiating with respect to n
is1, 2, . . . , r , we obtain
i
"F
1
2
2
sy
L S qc s0,
is1, 2, . . . , r,
i
i
i
2
" n
n
i
i
Thus
y1r2
n s c
L S ,
is1, 2, . . . , r.
Ž
.
i
i
i
i
Ž
.
By substituting n in the equality constraint 7.85 we get
i
r
Ý
c L S
'
is1
i
i
i
' s
.
dyc0

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
315
Therefore,
dyc
N S
Ž
.
0
i
i
n s
,
is1, 2, . . . , r.
7.86
Ž
.
i
r
c Ý
c N S
'
'
i
js1
j
j
j
Ž
.
It is easy to verify using the sufficient conditions in Section 7.8 that the
Ž
.
Ž
.
values of n , n , . . . , n given by equation 7.86 minimize Var y
under the
1
2
r
st
Ž
.
Ž
.
constraint of equality 7.85 . We conclude that Var y
is minimized when n
st
i
Ž
.
Ž
.
is proportional to
1r
c
N S
is1, 2, . . . , r . Consequently, n
must be
' i
i
i
i
large if the corresponding stratum is large, if the cost of sampling per unit in
that stratum is low, or if the variability within the stratum is large.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Bickel, P. J., and K. A. Doksum
1977 . Mathematical Statistics, Holden-Day, San
Ž
Francisco.
Chap. 1 discusses distribution theory for transformation of random
.
vectors.
Ž
.
Brownlee, K. A. 1965 . Statistical Theory and Methodology, 2nd ed. Wiley, New York.
Ž
.
See Section 9.8 with regard to the BehrensFisher test.
Ž
.
Ž
Cochran, W. G. 1963 . Sampling Techniques, 2nd ed. Wiley, New York. This is a
.
classic book on sampling theory as developed for use in sample surveys.
Ž
.
Corwin, L. J., and R. H. Szczarba 1982 . Multi®ariate Calculus. Marcel Dekker, New
Ž
York.
This is a useful book that provides an introduction to multivariable
calculus. The topics covered include continuity, differentiation, multiple inte-
.
grals, line and surface integrals, differential forms, and infinite series.
Ž
.
Ž
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York. Chap. 8 discusses
limits and continuity for a multivariable function; Chap. 10 covers the inverse
.
function theorem; Chap. 11 discusses multiple integration.
Ž
.
Gillespie, R. P. 1954 . Partial Differentiation. Oliver and Boyd, Edinburgh, Scotland.
ŽThis concise book provides a brief introduction to multivariable calculus. It
covers partial differentiation, Taylor’s theorem, and maxima and minima of
.
functions of several variables.
Ž
.
Kaplan, W.
1991 .
Ad®anced Calculus, 4th ed. Addison-Wesley, Redwood City,
Ž
California.
Topics pertaining to multivariable calculus are treated in several
.
chapters including Chaps. 2, 3, 4, 5, and 6.
Ž
.
Kaplan, W., and D. J. Lewis 1971 . Calculus and Linear Algebra, Vol. II. Wiley, New
Ž
York. Chap. 12 gives a brief introduction to differential calculus of a multivari-
.
able function; Chap. 13 covers multiple integration.
Ž
.
Ž
Lindgren, B. W. 1976 . Statistical Theory, 3rd ed. Macmillan, New York. Multivariate
.
transformations are discussed in Chap. 10.
Ž
.
Lowerre, J. M.
1983 . ‘‘An integral of the bivariate normal and an application.’’
Amer. Statist., 37, 235236.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis, 2nd ed. McGraw-Hill, New
Ž
.
York. Chap. 9 includes a study of multivariable functions.

MULTIDIMENSIONAL CALCULUS
316
Ž
.
Ž
Sagan, H.
1974 . Ad®anced Calculus. Houghton Mifflin, Boston.
Chap. 9 covers
differential calculus of a multivariable function; Chap. 10 deals with the inverse
.
function and implicit function theorems; Chap. 11 discusses multiple integration.
Ž
.
Satterthwaite, F. E.
1946 . ‘‘An approximate distribution of estimates of variance
components.’’ Biometrics Bull., 2, 110114.
Ž
.
Taylor, A. E., and W. R. Mann 1972 . Ad®anced Calculus, 2nd ed. Wiley, New York.
ŽThis book contains several chapters on multivariable calculus with many helpful
.
exercises.
Ž
.
Thibaudeau, Y., and G. P. H. Styan 1985 . ‘‘Bounds for Chakrabarti’s measure of
imbalance in experimental design.’’ In Proceedings of the First International Tam-
pere Seminar on Linear Statistical Models and Their Applications, T. Pukkila and S.
Puntanen, eds. University of Tampere, Tampere, Finland, pp. 323347.
Ž
.
Wen, L. 2001 . ‘‘A counterexample for the two-dimensional density function.’’ Amer.
Math. Monthly, 108, 367368.
EXERCISES
In Mathematics
Ž
.
2
7.1. Let f x , x
be a function defined on R
as
1
2




° x
x
1
1
exp y
,
x 0,
2
2
2
~
ž
/
x
x
f x , x
s
Ž
.
2
2
1
2
¢0,
x s0.
2
( )
Ž
.
Ž
.
a
Show that f x , x
has a limit equal to zero as xs x , x
™0
1
2
1
2
along any straight line through the origin.
( )
Ž
.
b
Show that f x , x
does not have a limit as x™0.
1
2
7.2. Prove Lemma 7.3.1.
7.3. Prove Lemma 7.3.2.
7.4. Prove Lemma 7.3.3.
7.5. Consider the function
x x
1
2
°
,
x , x
 0, 0 ,
Ž
.
Ž
.
1
2
2
2
~ x qx
1
2
f x , x
s
Ž
.
1
2
¢0,
x , x
s 0, 0 .
Ž
.
Ž
.
1
2
( )
Ž
.
a
Show that f x , x
is not continuous at the origin.
1
2

EXERCISES
317
( )
Ž
.
b
Show that the partial derivatives of f x , x
with respect to x and
1
2
1
x
exist at the origin.
2
w Note: This exercise shows that a multivariable function does not have
to be continuous at a point in order for its partial derivatives to exist at
x
that point.
Ž
.
7.6. The function f x , x , . . . , x
is said to be homogeneous of degree n in
1
2
k
x , x , . . . , x
if for any nonzero scalar t,
1
2
k
f tx , tx , . . . , tx
st nf x , x , . . . , x
Ž
.
Ž
.
1
2
k
1
2
k
Ž
.
for
all
x s x , x , . . . , x

in
the
domain
of
f.
Show
that
if
1
2
k
Ž
.
f x , x , . . . , x
is homogeneous of degree n, then
1
2
k
k
" f
x
snf
Ý
i " xi
is1
w Note: This result is known as Euler’s theorem for homogeneous
x
functions.
7.7. Consider the function
°
2
x x
1
2
,
x , x
 0, 0 ,
Ž
.
Ž
.
1
2
4
2
~ x qx
f x , x
s
Ž
.
1
2
1
2
¢0,
x , x
s 0, 0 .
Ž
.
Ž
.
1
2
( )
a
Is f continuous at the origin? Why or why not?
( )
b
Show that f has a directional derivative in every direction at the
origin.
Ž .
7.8. Let S be a surface defined by the equation f x sc , where xs
0
Ž
.
x , x , . . . , x
 and c
is a constant. Let C denote a curve on S given
1
2
k
0
Ž .
Ž .
Ž .
by
the
equations
x s g
t , x s g
t , . . . , x s g
t ,
where
1
1
2
2
k
k
g , g , . . . , g
are differentiable functions. Let s be the arc length of C
1
2
k
measured from some fixed point in such a way that s increases with t.
The curve can then be parameterized, using s instead of t, in the form
Ž .
Ž .
Ž .
x sh s , x sh
s , . . . , x sh
s . Suppose that f has partial deriva-
1
1
2
2
k
k
tives with respect to x , x , . . . , x .
1
2
k
Show that the directional derivative of f at a point x on C in the
Ž
direction of v, where v is a unit tangent vector to C at x
in the
.
direction of increasing s , is equal to dfrds.
7.9. Use Taylor’s expansion in a neighborhood of the origin to obtain a
second-order approximation for each of the following functions:

MULTIDIMENSIONAL CALCULUS
318
( )
Ž
.
Ž
.
a
f x , x
sexp x sin x .
1
2
2
1
( )
Ž
.
Ž
x1
2
3.
b
f x , x , x
ssin e
qx qx
.
1
2
3
2
3
( )
Ž
.
Ž
.
c
f x , x
scos x x
.
1
2
1
2
Ž
.
Ž
.
7.10. Suppose that f x , x
and g x , x
are continuously differentiable
1
2
1
2
Ž
.
functions in a neighborhood of a point x s x
, x
. Consider the
0
10
20
Ž
.
equation u sf x , x
. Suppose that " fr" x 0 at x .
1
1
2
1
0
( )
a
Show that
" x
" f
" f
1 sy
" x
" x
" x
2
2
1
in a neighborhood of x .
0
( )
b
Suppose that in a neighborhood of x ,
0
"
f, g
Ž
.
s0.
" x , x
Ž
.
1
2
Show that
" g " x
" g
1 q
s0,
" x
" x
" x
1
2
2
that is, g is actually independent of x
in a neighborhood of x .
2
0
( )
Ž .
c
Deduce from
b
that there exists a function : D™R, where
Ž
.
D;R is a neighborhood of f x
, such that
0
g x , x
s f x , x
Ž
.
Ž
.
1
2
1
2
throughout a neighborhood of x . In this case, the functions f and
0
g are said to be functionally dependent.
( )
d
Show that if f and g are functionally dependent, then
"
f, g
Ž
.
s0.
" x , x
Ž
.
1
2
w
Ž . Ž .
Ž .
Note: From b , c , and d we conclude that f and g are functionally
2
Ž
.
Ž
.
x
dependent on a set  ;R
if and only if " f, g r" x , x
s0 in  .
1
2
7.11. Consider the equation
" u
" u
" u
x
qx
qx
snu.
1
2
3
" x
" x
" x
1
2
3

EXERCISES
319
Let  sx rx ,  sx rx ,  sx . Use this change of variables to show
1
1
3
2
2
3
3
3
that the equation can be written as
" u

snu.
3 " 3
Deduce that u is of the form
x
x
1
2
n
usx F
,
.
3 ž
/
x
x
3
3
7.12. Let u and u be defined as
1
2
1r2
1r2
2
2
u sx 1yx
qx
1yx
,
Ž
.
Ž
.
1
1
2
2
1
1r2
1r2
2
2
u s 1yx
1yx
yx x .
Ž
.
Ž
.
2
1
2
1
2
Show that u and u
are functionally dependent.
1
2
7.13. Let f: R3™R3 be defined as
usf x ,
xs x , x , x
,
us u , u , u
,
Ž .
Ž
.
Ž
.
1
2
3
1
2
3
where u sx 3, u sx 3, u sx 3.
1
1
2
2
3
3
( )
a
Show that the Jacobian matrix of f is not nonsingular in any subset
D;R3 that contains points on ay of the coordinate planes.
( )
3
b
Show that f has a unique inverse everywhere in R
including any
Ž .
subset D of the type described in a .
w Note: This exercise shows that the nonvanishing of the Jacobian
Ž
.
determinant in Theorem 7.6.1 inverse function theorem is a sufficient
x
condition for the existence of an inverse function, but is not necessary.
7.14. Consider the equations
g
x , x , y , y
s0,
Ž
.
1
1
2
1
2
g
x , x , y , y
s0,
Ž
.
2
1
2
1
2
where g
and g
are differentiable functions defined on a set D;R4.
1
2
Ž
.
Ž
.
Suppose that " g , g
r" x , x
0 in D. Show that
1
2
1
2
" x
" g , g
" g , g
Ž
.
Ž
.
1
1
2
1
2
sy
,
" y
"
y , x
" x , x
Ž
.
Ž
.
1
1
2
1
2
" x
" g , g
" g , g
Ž
.
Ž
.
2
1
2
1
2
sy
.
" y
" x , y
" x , x
Ž
.
Ž
.
1
1
1
1
2

MULTIDIMENSIONAL CALCULUS
320
Ž
.
Ž
.
7.15. Let f x , x , x
s0, g x , x , x
s0, where f and g are differentiable
1
2
3
1
2
3
functions defined on a set D;R3. Suppose that
"
f, g
"
f, g
"
f, g
Ž
.
Ž
.
Ž
.
0,
0,
0
" x , x
" x , x
" x , x
Ž
.
Ž
.
Ž
.
2
3
3
1
1
2
in D. Show that
dx
dx
dx
1
2
3
s
s
.
"
f, g r" x , x
"
f, g r" x , x
"
f, g r" x , x
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
3
3
1
1
2
7.16. Determine the stationary points of the following functions and check
for local minima and maxima:
( )
2
2
a
fsx qx qx qx qx x .
1
2
1
2
1
2
( )
2
2
b
fs2 x yx x qx qx yx q1, where  is a scalar. Can  be
1
1
2
2
1
2
Ž .
chosen so that the stationary point is i a point of local minimum;
Ž .
Ž
.
ii a point of local maximum; iii a saddle point?
( )
3
2
c
fsx y6 x x q3x y24 x q4.
1
1
2
2
1
( )
4
4
Ž
.2
d
fsx qx y2 x yx
.
1
2
1
2
7.17. Consider the function
1qpqÝm
p
is1
i
fs
,
1r2
2
m
2
1qp qÝ
p
Ž
.
is1
i
which is defined on the region

Cs
p , p , . . . , p
0pFp F1, is1, 2, . . . , m ,

4
Ž
.
1
2
m
i
where p is a known constant. Show that
( )
a
" fr" p , for is1, 2, . . . , m, vanish at exactly one point in C.
i
( )
Ž
.
b
The gradient vector 
fs " fr" p , " fr" p , . . . , " fr" p
 does not
1
2
m
vanish anywhere on the boundary of C.
( )
c
f attains its absolute maximum in the interior of C at the point
Ž
o
o
o .
p , p , . . . , p
, where
1
2
m
1qp2
o
p s
,
is1, 2, . . . , m.
i
1qp
w Note: The function f was considered in an article by Thibaudeau and
Ž
.
Styan
1985
concerning a measure of imbalance for experimental
x
designs.
Ž
2.Ž
2.
7.18. Show that the function fs x yx
x y2 x
does not have a local
2
1
2
1
maximum or minimum at the origin, although it has a local minimum
for ts0 along every straight line given by the equations x sat, x sbt,
1
2
where a and b are constants.

EXERCISES
321
7.19. Find the optimal values of the function fsx 2q12 x x q2 x 2 subject
1
1
2
2
to 4 x 2qx 2s25. Determine the nature of the optima.
1
2
7.20. Find the minimum distance from the origin to the curve of intersection
Ž
.
of the surfaces, x
x qx
sy2 and x x s1.
3
1
2
1
2
7.21. Apply the method of Lagrange multipliers to show that
1r3
1
2
2
2
2
2
2
x x x
F
x qx qx
Ž
.
Ž
.
1
2
3
1
2
3
3
for all values of x , x , x .
1
2
3
w
2
2
2
2
2
2
Hint: Find the maximum value of fsx x x
subject to x qx qx s
1
2
3
1
2
3
2
x
c , where c is a constant.
7.22. Prove Theorem 7.9.3.
7.23. Evaluate the following integrals:
( )
a
H H x
x dx dx , where
'
D
2
1
1
2

2
2
Ds
x , x
x 0, x x , x 2yx
.
Ž
.

4
1
2
1
2
1
2
1
2
2
4
1
1yx
'
2
( )
w
Ž
.
x
b
H
H
x q x
dx
dx .
0
0
1
2
1
2
3
3
Ž
.
7.24. Show that if f x , x
is continuous, then
1
2
2
2
4
x
4 x yx
' 2
1
1f x , x
dx
dx s
f x , x
dx
dx .
Ž
.
Ž
.
H H
H H
1
2
2
1
1
2
1
2
2
0
x
0
2y
4yx
'
1
2
7.25. Consider the integral
2
1
1yx 1
Is
f x , x
dx
dx .
Ž
.
H H
1
2
2
1
0
1yx 1
( )
a
Write an equivalent expression for I by reversing the order of
integration.
( )
Ž
.
1yx 2
1 Ž
.
b
If g x
sH
f x , x
dx , find dgrdx .
1
1yx
1
2
2
1
1
7.26. Evaluate H H x x dx dx , where D is a region enclosed by the four
D
1
2
1
2
parabolas x 2sx , x 2s2 x , x 2sx , x 2s2 x .
2
1
2
1
1
2
1
2
w
x
Hint: Use a proper change of variables.

MULTIDIMENSIONAL CALCULUS
322
Ž
2
2.
7.27. Evaluate H H H
x qx
dx dx dx , where D is a sphere of radius 1
D
1
2
1
2
3
centered at the origin.
w Hint: Make a change of variables using spherical polar coordinates of
the form
x sr sin  cos ,
1
x sr sin  sin ,
2
x sr cos  ,
3
x
0FrF1, 0FF, 0FF2.
7.28. Find the value of the integral
dx
'3
Is
.
H
3
2
0
1qx
Ž
.
'3
2
w
Ž
.
x
Hint: Consider the integral H
dxr aqx
, where a0.
0
In Statistics
Ž
.
7.29. Suppose that the random vector Xs X , X
 has the density function
1
2
x qx ,
0x 1, 0x 1,
1
2
1
2
f x , x
s
Ž
.
1
2
½ 0
elsewhere.
( )
a
Are the random variables X
and X
independent?
1
2
( )
b
Find the expected value of X X .
1
2
Ž
.
Ž
.
7.30. Consider the density function f x , x
of Xs X , X
, where
1
2
1
2
1,
yx x x , 0x 1,
2
1
2
2
f x , x
s
Ž
.
1
2
½ 0
elsewhere.
w
Show that X
and X
are uncorrelated random variables
that is,
1
2
Ž
.
Ž
.
Ž
.x
E X X
sE X
E X
, but are not independent.
1
2
1
2
Ž
.
7.31. The density function of Xs X , X
 is given by
1
2
1
°
y1
y1
yx yx
1
2
x
x
e
,
0x , x ,
1
2
1
2
~
f x , x
s
Ž
.
!  ! 
Ž
.
Ž
.
1
2
¢0
elsewhere.

EXERCISES
323
Ž
.
Ž
.
where
0,
0, and
! m
is the gamma function
! m s
H eyx x my1 dx, m0. Suppose that Y
and Y
are random variables
0
1
2
defined as
X1
Y s
,
1
X qX
1
2
Y sX qX .
2
1
2
( )
a
Find the joint density function of Y and Y .
1
2
( )
b
Find the marginal densities of Y and Y .
1
2
( )
c
Are Y and Y
independent?
1
2
Ž
.
7.32. Suppose that Xs X , X
 has the density function
1
2
10 x x 2,
0x x , 0x 1,
1
2
1
2
2
f x , x
s
Ž
.
1
2
½ 0
elsewhere.
Find the density function of WsX X .
1
2
Ž
2
2.1r2
Ž
.
7.33. Find the density function of Ws X qX
given that Xs X , X

1
2
1
2
has the density function
4 x x eyx 2
1yx 2
2 ,
x 0, x 0,
1
2
1
2
f x , x
s
Ž
.
1
2
½ 0
elsewhere.
7.34. Let X , X , . . . , X
be independent random variables that have the
1
2
n
Ž .
yx
exponential density f x se
, x0. Let Y , Y , . . . , Y
be n random
1
2
n
variables defined as
Y sX ,
1
1
Y sX qX ,
2
1
2
...
Y sX qX q  qX .
n
1
2
n
Ž
.
Find the density of Ys Y , Y , . . . , Y , and then deduce the marginal
1
2
n
density of Y .
n
Ž
.
7.35. Prove formula 7.84 .
7.36. Let X
and X
be independent random variables such that W s
1
2
1
Ž
2.
Ž
2.
6r	
X
and W s 8r	
X
have the chi-squared distribution with
1
1
2
2
2

MULTIDIMENSIONAL CALCULUS
324
six and eight degrees of freedom, respectively, where 	 2 and 	 2 are
1
2
1
1
2
2
unknown parameters. Let s 	 q 	 . An unbiased estimator of 
1
2
7
9
1
1
ˆ
is given by s X q X , since X and X are unbiased estimators of
1
2
1
2
7
9
	 2 and 	 2, respectively.
1
2
Ž
.
Using Satterthwaite’s approximation see Satterthwaite, 1946 , it can
ˆ
be shown that r is approximately distributed as a chi-squared
variate with  degrees of freedom, where  is given by
2
1
1
2
2
	 q 	
Ž
.
1
2
7
9
s
,
2
2
1
1
1
1
2
2
	
q
	
Ž
.
Ž
.
1
2
6
7
8
9
which can be written as
2
8 9q7
Ž
.
s
,
2
108q49
where s	 2r	 2. It follows that the probability
2
1
ˆ
ˆ


psP

,
2
2
ž
/


0.025 , 
0.975 , 
where  2
denotes the upper 100% point of the chi-squared distri-
, 
bution with  degrees of freedom, is approximately equal to 0.95.
Compute the exact value of p using double integration, given that
s2. Compare the result with the 0.95 value.
w
Ž .
Notes: 1 The density function of a chi-squared random variable with
Ž .
n degrees of freedom is given in Example 6.9.6.
2 In general,  is
unknown. It can be estimated by  which results from replacing  with
ˆ
ˆ
ˆ
Ž .
sX rX
in the formula for .
3 The estimator  is used in the
2
1
BehrensFisher test statistic for comparing the means of two popula-
tions with unknown variances 	 2 and 	 2, which are assumed to be
1
2
unequal. If Y
and Y
are the means of two independent samples of
1
2
sizes n s7 and n s9, respectively, randomly chosen from these
1
2
populations, then  is the variance of Y yY . In this case, X and X
1
2
1
2
represent the corresponding sample variances. The BehrensFisher
t-statistic is then given by
Y yY
1
2
ts
.
'ˆ
If the two population means are equal, t has approximately the t-distri-
bution with  degrees of freedom. For more details about the
Ž
. x
BehrensFisher test, see for example, Brownlee 1965, Section 9.8 .

EXERCISES
325
7.37. Suppose that a parabola of the form s q xq x 2 is fitted to a
0
1
2
Ž
. Ž
.
Ž
.
set of paired data,
x , y , x , y
, . . . , x , y . Obtain estimates of  ,
1
1
2
2
n
n
0
n
w
Ž
2.x2
 , and 
by minimizing Ý
y y  q x q x
with respect
1
2
is1
i
0
1
i
2
i
to  ,  , and  .
0
1
2
w Note: The estimates obtained in this manner are the least-squares
x
estimates of  ,  , and  .
0
1
2
7.38. Suppose that we have k disjoint events A , A , . . . , A
such that the
1
2
k
Ž
.
k
probability of A
is p
is1, 2, . . . , k
and Ý
p s1. Furthermore,
i
i
is1
i
suppose that among n independent trials there are X , X , . . . , X
1
2
k
outcomes associated with A , A , . . . , A , respectively. The joint proba-
1
2
k
bility that X sx , X sx , . . . , X sx
is given by the likelihood func-
1
1
2
2
k
k
tion
n!
x
x
x
1
2
k
L x, p s
p
p
 p
,
Ž
.
1
2
k
x !x !  x !
1
2
k
where
x s0, 1, 2, . . . , n for is1, 2, . . . , k such that Ýk
x sn, xs
i
is1
i
Ž
.
Ž
.
x , x , . . . , x
, ps p , p , . . . , p . This defines a joint distribution
1
2
k
1
2
k
for X , X , . . . , X
known as the multinomial distribution.
1
2
k
Find the maximum likelihood estimates of p , p , . . . , p by maximiz-
1
2
k
Ž
.
k
ing L x, p subject to Ý
p s1.
is1
i
w
x1
x 2
xk
Hint: Maximize the natural logarithm of
p
p
 p
subject to
1
2
k
k
x
Ý
p s1.
is1
i
Ž .
Ž
.
7.39. Let  y be a positive, even, and continuous function on y,  such
Ž .
Ž
.

Ž .
that  y is strictly decreasing on 0,  , and H
 y dys1. Consider
y
the following bivariate density function:
°1qxr y ,
y y Fx0,
Ž .
Ž .
~
f x, y s
Ž
.
1yxr y ,
0FxF y ,
Ž .
Ž .
¢0
otherwise.
( )
Ž
.
a
Show that f x, y is continuous for yx, y.
( )
Ž
.
b
Let F x, y be the corresponding cumulative distribution function,
x
y
F x, y s
f s, t
dsdt.
Ž
.
Ž
.
H H
y y
Ž .
Show that if 0 x 0 , then
s
y1
 x
Ž
.

 x
F  x, 0 yF 0, 0 G
1y
dsdt
Ž
.
Ž
. H
H
 tŽ .
0
0
1
y1
G  x 
 x ,
Ž
.
2
y1
Ž .
where 
is the inverse function of  y for 0Fy.

MULTIDIMENSIONAL CALCULUS
326
( )
Ž .
c
Use part b to show that
F  x, 0 yF 0, 0
Ž
.
Ž
.
lim
s.
q
 x
 x™0
Ž
.
Ž
.
Hence, "F x, y r" x does not exist at 0, 0 .
( )
Ž .
d
Deduce from part c that the equality
" 2F x, y
Ž
.
f x, y s
Ž
.
" x " y
does not hold in this example.
w
Ž
.
Note: This example was given by Wen
2001
to demonstrate that
Ž
.
continuity of f x, y is not sufficient for the existence of "Fr" x, and
Ž . x
hence for the validity of the equality in part d .

C H A P T E R
8
Optimization in Statistics
Optimization is an essential feature in many problems in statistics. This is
apparent in almost all fields of statistics. Here are few examples, some of
which will be discussed in more detail in this chapter.
1. In the theory of estimation, an estimator of an unknown parameter is
sought that satisfies a certain optimality criterion such as minimum
Ž
variance, maximum likelihood, or minimum average risk as in the case
.
of a Bayes estimator . Some of these criteria were already discussed in
Section 7.11. For example, in regression analysis, estimates of the
parameters of a fitted model are obtained by minimizing a certain
expression that measures the closeness of the fit of the model. One
common example of such an expression is the sum of the squared
Ž
residuals
these are deviations of the predicted response values, as
specified by the model, from the corresponding observed response
.
values . This particular expression is used in the method of ordinary
least squares. A more general class of parameter estimators is the class
Ž
.
of M-estimators. See Huber
1973, 1981 . The name ‘‘M-estimator’’
comes from ‘‘generalized maximum likelihood.’’ They are based on the
idea of replacing the squared residuals by another symmetric function
of the residuals that has a unique minimum at zero. For example,
minimizing the sum of the absolute values of the residuals produces the
Ž
.
so-called least absolute ®alues LAV estimators.
2. Estimates of the variance components associated with random or mixed
models are obtained by using several methods. In some of these
methods, the estimates are given as solutions to certain optimization
Ž
.
problems as in maximum likelihood
ML
estimation and minimum
Ž
.
norm quadratic unbiased estimation MINQUE . In the former method,
the likelihood function is maximized under the assumption of normally
w
Ž
.x
distributed data see Hartley and Rao 1967 . A completely different
approach is used in the latter method, which was proposed by Rao
Ž
.
1970, 1971 . This method does not require the normality assumption.
327

OPTIMIZATION IN STATISTICS
328
For a review of methods of estimating variance components, see Khuri
Ž
.
and Sahai 1985 .
3. In statistical inference, tests are constructed so that they are optimal in
Ž
a certain sense. For example, in the NeymanPearson lemma see, for
.
example, Roussas, 1973, Chapter 13 , a test is obtained by minimizing
the probability of Type II error while holding the probability of Type I
error at a certain level.
4. In the field of response surface methodology, design settings are chosen
to minimize the prediction variance inside a region of interest, or to
minimize the bias that occurs from fitting the ‘‘wrong’’ model. Other
optimality criteria can also be considered. For example, under the
D-optimality criterion, the determinant of the variancecovariance ma-
trix of the least-squares estimator of the vector of unknown parameters
Ž
.
of a fitted model is minimized with respect to the design settings.
5. Another objective of response surface methodology is the determina-
tion of optimum operating conditions on the input variables that
produce maximum, or minimum, response values inside a region of
interest. For example, in a particular chemical reaction setting, it may
be of interest to determine the reaction temperature and the reaction
time that maximize the percentage yield of a product. Optimum seeking
methods in response surface methodology will be discussed in detail in
Section 8.3.
6. Several response variables may be observed in an experiment for each
setting of a group of input variables. Such an experiment is called a
multiresponse experiment. In this case, optimization involves a number
Ž
of response functions and is therefore referred to as simultaneous or
.
multiresponse
optimization. For example, it may be of interest to
maximize the yield of a certain chemical compound while reducing the
production cost. Multiresponse optimization will be discussed in Sec-
tion 8.7.
7. In multivariate analysis, a large number of measurements may be
available as a result of some experiment. For convenience in the
analysis and interpretation of such data, it would be desirable to work
with fewer of the measurements, without loss of much information.
This problem of data reduction is dealt with by choosing certain linear
functions of the measurements in an optimal manner. Such linear
functions are called principal components.
Optimization of a multivariable function was discussed in Chapter 7.
However, there are situations in which the optimum cannot be obtained
explicitly by simply following the methods described in Chapter 7.
Instead, iterative procedures may be needed. In this chapter, we shall
first discuss some commonly used iterative optimization methods. A
number of these methods require the explicit evaluation of the partial
Ž
.
derivatives of the function to be optimized objective function . These

THE GRADIENT METHODS
329
are referred to as the gradient methods. Three other optimization
techniques that rely solely on the values of the objective function will
also be discussed. They are called direct search methods.
8.1. THE GRADIENT METHODS
Ž .
Let
f x
be a real-valued function of k variables
x , x , . . . , x , where
1
2
k
Ž
.
Ž .
xs x , x , . . . , x
. The gradient methods are based on approximating f x
1
2
k
with a low-degree polynomial, usually of degree one or two, using Taylor’s
Ž .
expansion. The first- and second-order partial derivatives of f x are there-
fore assumed to exist at every point x in the domain of f. Without loss of
generality, we shall consider that f is to be minimized.
8.1.1. The Method of Steepest Descent
Ž .
This method is based on a first-order approximation of f x with a polyno-
Ž
.
mial of degree one using Taylor’s theorem see Section 7.5 . Let x
be an
0
Ž .
initial point in the domain of f x . Let x qth
be a neighboring point,
0
0
Ž
where th
represents a small change in the direction of a unit vector h
that
0
0
.
Ž .
Ž
.
Ž
.
is, t0 . The corresponding change in f x is f x qth
yf x
. A first-
0
0
0
order approximation of this change is given by
f x qth
yf x
fth
 
f x
,
8.1
Ž
.
Ž
.
Ž
.
Ž
.
0
0
0
0
0
Ž
.
as can be seen from applying formula 7.27 . If the objective is to minimize
Ž .

Ž
.
f x , then h
must be chosen so as to obtain the largest value for yth 
f x
.
0
0
0
This is a constrained maximization problem, since h
has unit length. For this
0
purpose we use the method of Lagrange multipliers. Let F be the function
Fsyth
 
f x
q h
 h y1 .
Ž
.
Ž
.
0
0
0
0
By differentiating F with respect to the elements of h
and equating the
0
derivatives to zero we obtain
t
h s

f x
.
8.2
Ž
.
Ž
.
0
0
2
Using the constraint h
 h s1, we find that  must satisfy the equation
0
0
t 2
2
2
	
	
 s

f x
,
8.3
Ž
.
Ž
.
2
0
4
	
Ž
.	
Ž
.

Ž
.
where

f x
is the Euclidean norm of 
f x
. In order for yth 
f x
2
0
0
0
0

OPTIMIZATION IN STATISTICS
330
Ž
.
to have a maximum,  must be negative. From formula 8.3 we then have
t 	
	
sy

f x
.
Ž
.
2
0
2
Ž
.
By substituting this expression in formula 8.2 we get

f x
Ž
.
0
h sy
.
8.4
Ž
.
0
	
	

f x
Ž
.
2
0
Ž
.
Thus for a given t0, we can achieve a maximum reduction in f x
by
0
Ž
.
moving from x
in the direction specified by h
in formula 8.4 . The value of
0
0
t is now determined by performing a linear search in the direction of h . This
0
Ž
.
is accomplished by increasing the value of t
starting from zero
until no
further reduction in the values of f is obtained. Let such a value of t be
denoted by t . The corresponding value of x is given by
0

f x
Ž
.
0
x sx yt
.
1
0
0 	
	

f x
Ž
.
2
0
Since the direction of h
is in general not toward the location x* of the
0
true minimum of f, the above process must be performed iteratively. Thus if
at stage i we have an approximation x for x*, then at stage iq1 we have the
i
approximation
x
sx qt h ,
is0, 1, 2, . . . ,
iq1
i
i
i
where

f x
Ž
.
i
h sy
,
is0, 1, 2, . . . ,
i
	
	

f x
Ž
.
2
i
and t is determined by a linear search in the direction of h , that is, t is the
i
i
i
Ž
.
value of t that minimizes f x qth . Note that if it is desired to maximize f,
i
i
Ž
.
then for each i G0 we need to move in the direction of yh . In this case,
i
the method is called the method of steepest ascent.
Convergence of the method of steepest descent can be very slow, since
frequent changes of direction may be necessary. Another reason for slow
convergence is that the direction of h
at the ith iteration may be nearly
i
perpendicular to the direction toward the minimum. Furthermore, the method
becomes inefficient when the first-order approximation of f is no longer
adequate. In this case, a second-order approximation should be attempted.
This will be described in the next section.

THE GRADIENT METHODS
331
8.1.2. The Newton–Raphson Method
Ž .
Let x
be an initial point in the domain of f x . By a Taylor’s expansion of f
0
Ž
.
in a neighborhood of x
see Theorem 7.5.1 , it is possible to approximate
0
Ž .
Ž .
f x with the quadratic function  x given by
1
 x sf x
q xyx

f x
q
xyx
H
x
xyx
,
8.5
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
. Ž
.
Ž
.
0
0
0
0
f
0
0
2!
Ž
.
where H
x
is the Hessian matrix of f evaluated at x .
f
0
0
Ž
.
On the basis of formula 8.5 we can obtain a reasonable approximation to
Ž .
Ž .
Ž .
the minimum of f x by using the minimum of  x . If  x attains a local
Ž
.
Ž
.
minimum at x , then we must necessarily have 
 x
s0 see Section 7.7 ,
1
1
that is,

f x
qH
x
x yx
s0.
8.6
Ž
.
Ž
. Ž
.
Ž
.
0
f
0
1
0
Ž
.
Ž
.
If H
x
is nonsingular, then from equation 8.6 we obtain
f
0
x sx yHy1 x

f x
.
Ž
.
Ž
.
1
0
f
0
0
Ž .
If we now approximate
f x
with another quadratic function, by again
applying Taylor’s expansion in a neighborhood of x , and then repeat the
1
same process as before with x used instead of x , we obtain the point
1
0
x sx yHy1 x

f x
.
Ž
.
Ž
.
2
1
f
1
1
Further
repetitions
of
this
process
lead
to
a
sequence
of
points,
x , x , x , . . . , x , . . . , such that
0
1
2
i
x
sx yHy1 x

f x
,
is0, 1, 2, . . . .
8.7
Ž
.
Ž
.
Ž
.
iq1
i
f
i
i
The NewtonRaphson method requires finding the inverse of the Hessian
matrix H
at each iteration. This can be computationally involved, especially
f
if the number of variables, k, is large. Furthermore, the method may fail to
Ž
.
converge if H
x
is not positive definite. This can occur, for example, when
f
i
x
is far from the location x* of the true minimum. If, however, the initial
i
point x
is close to x*, then convergence occurs at a rapid rate.
0
8.1.3. The Davidon–Fletcher–Powell Method
This method is basically similar to the one in Section 8.1.1 except that at the
ith iteration we have
x
sx y G 
f x
,
is0, 1, 2, . . . ,
Ž
.
iq1
i
i
i
i

OPTIMIZATION IN STATISTICS
332
where G is a positive definite matrix that serves as the ith approximation to
i
Ž
.
the inverse of the Hessian matrix H
x , and  is a scalar determined by a
f
i
i
Ž
.
linear search from x in the direction of yG 
f x , similar to the one for the
i
i
i
steepest descent method. The initial choice G
of the matrix G can be any
0
positive definite matrix, but is usually taken to be the identity matrix. At the
Ž
.
iq1 st iteration, G is updated by using the formula
i
G
sG qL qM ,
is0, 1, 2, . . . ,
iq1
i
i
i
where
G

f x
y
f x

f x
y
f x
G
Ž
.
Ž
.
Ž
.
Ž
.
i
iq1
i
iq1
i
i
L sy
,
i

f x
y
f x
G

f x
y
f x
Ž
.
Ž
.
Ž
.
Ž
.
iq1
i
i
iq1
i
 G 
f x
G 
f x

Ž
.
Ž
.
i
i
i
i
i
M sy
.
i
G 
f x
 
f x
y
f x
Ž
.
Ž
.
Ž
.
i
i
iq1
i
Ž
.
The justification for this method is given in Fletcher and Powell 1963 .
Ž
.
See also Bunday 1984, Section 4.3 . Note that if G is initially chosen as the
i
Ž
.
identity, then the first increment is in the steepest descent direction y
f x
.
0
This is a powerful optimization method and is considered to be very
efficient for most functions.
8.2. THE DIRECT SEARCH METHODS
The direct search methods do not require the evaluation of any partial
derivatives of the objective function. For this reason they are suited for
situations in which it is analytically difficult to provide expressions for the
partial derivatives, such as the minimization of the maximum absolute devia-
tion. Three such methods will be discussed here, namely, the NelderMead
simplex method, Price’s controlled random search procedure, and general-
ized simulated annealing.
8.2.1. The Nelder–Mead Simplex Method
Ž .
Ž
.
Let f x , where xs x , x , . . . , x
, be the function to be minimized. The
1
2
k
simplex method is based on a comparison of the values of f at the kq1
vertices of a general simplex followed by a move away from the vertex with
the highest function value. By definition, a general simplex is a geometric
figure formed by a set of kq1 points called vertices in a k-dimensional
space. Originally, the simplex method was proposed by Spendley, Hext, and
Ž
.
Himsworth 1962 , who considered a regular simplex, that is, a simplex with
mutually equidistant points such as an equilateral triangle in a two-dimen-
Ž
.
Ž
.
sional space
ks2 . Nelder and Mead
1965
modified this method by

THE DIRECT SEARCH METHODS
333
allowing the simplex to be nonregular. This modified version of the simplex
method will be described here.
The simplex method follows a sequential search procedure. As was men-
tioned earlier, it begins by evaluating f at the kq1 points that form a
general simplex. Let these points be denoted by x , x , . . . , x
. Let f
and
1
2
kq1
h
f
denote,
respectively,
the
largest
and
the
smallest
of
the
values
l
Ž
.
Ž
.
Ž
.
f x
, f x
, . . . , f x
. Let us also denote the points where f
and f are
1
2
kq1
h
l
attained by x
and x , respectively.
h
l
Obviously, if we are interested in minimizing f, then a move away from x h
will be in order. Let us therefore define x
as the centroid of all the points
c
with the exclusion of x . Thus
h
1
x s
x .
Ý
c
i
k ih
In order to move away from x , we reflect x with respect to x to obtain the
h
h
c
point x. More specifically, the latter point is defined by the relation
h
xyx sr x yx
,
Ž
.
h
c
c
h
or equivalently,
xs 1qr x yrx ,
Ž
.
h
c
h
where r is a positive constant called the reflection coefficient and is given by
	 
	
x yx
2
h
c
rs
.
	
	
x yx
2
c
h
The points x , x , and x are depicted in Figure 8.1. Let us consider the
h
c
h
Ž .
Ž  .
Figure 8.1. A two-dimensional simplex with the reflection x
, expansion x
, and contraction
h
he
Ž  .
x
points.
hc

OPTIMIZATION IN STATISTICS
334
following cases:
Ž .

a. If f f x
f , replace x
by x
and start the process again with the
l
h
h
h
h
Ž
new simplex that is, evaluate f at the vertices of the simplex which has

.
the same points as the original simplex, but with x
substituted for x
.
h
h
Ž .

b. If f x
f , then the move from x
to x
is in the right direction and
h
l
c
h
should therefore be expanded. In this case, x is expanded to x
h
he
defined by the relation
x yx s xyx
,
Ž
.
he
c
h
c
that is,
x s xq 1y x ,
Ž
.
he
h
c
Ž
.
where 
1 is an expansion coefficient given by
	

	
x
yx
2
he
c
s

	
	
x yx
2
h
c
Ž
.
Ž  .
see Figure 8.1 . This operation is called expansion. If
f x
f ,
he
l

Ž  .
replace x
by x
and restart the process. However, if f x
f , then
h
he
he
l
expansion is counterproductive. In this case, x
is dropped, x
is
he
h
replaced by x, and the process is restarted.
h

Ž .
Ž
.
c. If upon reflecting x
to x
we discover that f x
f x
for all ih,
h
h
h
i

Ž .
then replacing x
by x
would leave f x
as the maximum in the new
h
h
h
simplex. In this case, a new x
is defined to be either the old x
or x,
h
h
h
whichever has the lower value. A point x is then found such that
hc
x yx s x yx
,
Ž
.
hc
c
h
c
that is,
x s x q 1y x ,
Ž
.
hc
h
c
Ž
.
where  01 is a contraction coefficient given by
	

	
x
yx
2
hc
c
s
.
	
	
x yx
2
h
c
Next, x
is substituted for x
and the process is restarted unless
hc
h
Ž  .
w
Ž .x
f x
min f , f x
, that is, the contracted point is worse than the
hc
h
h
Ž .
better of f
and f x
. When such a contraction fails, the size of the
h
h
simplex is reduced by halving the distance of each point of the simplex
from x , where, if we recall, x
is the point generating the lowest
l
l
1
1
Ž
.
Ž
function value. Thus x is replaced by x q
x yx , that is, by
x q
i
l
i
l
i
2
2
.
x . The process is then restarted with the new reduced simplex.
l

THE DIRECT SEARCH METHODS
335
Figure 8.2. Flow diagram for the NelderMead simplex method. Source: Nelder and Mead
Ž
.
1965 . Reproduced with permission of Oxford University Press.
Thus at each stage in the minimization process, x , the point at
h
which f has the highest value, is replaced by a new point according to
one of three operations, namely, reflection, contraction, and expansion.
As an aid to illustrating this step-by-step procedure, a flow diagram is
shown in Figure 8.2. This flow diagram is similar to one given by Nelder
Ž
.
and Mead 1965, page 309 . Figure 8.2 lists the explanations of steps 1
through 6.
The criterion used to stop the search procedure is based on the
variation in the function values over the simplex. At each step, the

OPTIMIZATION IN STATISTICS
336
standard error of these values in the form
1r2
2
kq1
Ý
f yf
Ž
.
is1
i
ss
k
is calculated and compared with some preselected value d, where
f , f , . . . , f
denote the function values at the vertices of the simplex
1
2
kq1
kq1
Ž
.
at hand and fsÝ
f r kq1 . The search is halted when sd. The
is1
i
reasoning behind this criterion is that when sd, all function values
are very close together. This hopefully indicates that the points of the
simplex are near the minimum.
Ž
.
Bunday 1984 provided the listing of a computer program which can be
used to implement the steps described in the flow diagram.
Ž
.
Olsson and Nelson 1975 demonstrated the usefulness of this method by
using it to solve six minimization problems in statistics. The robustness of the
method itself and its advantages relative to other minimization techniques
Ž
.
were reported in Nelson 1973 .
8.2.2. Price’s Controlled Random Search Procedure
Ž
.
The controlled random search procedure was introduced by Price 1977 . It is
Ž
.
capable of finding the absolute or global minimum of a function within a
constrained region R. It is therefore well suited for a multimodal function,
that is, a function that has several local minima within the region R.
The essential features of Price’s algorithm are outlined in the flow diagram
of Figure 8.3. A predetermined number, N, of trial points are randomly
chosen inside the region R. The value of N must be greater than k, the
number of variables. The corresponding function values are obtained and
stored in an array A along with the coordinates of the N chosen points. At
each iteration, kq1 distinct points, x , x , . . . , x
, are chosen at random
1
2
kq1
from the N points in storage. These kq1 points form a simplex in a
Ž
k-dimensional space. The point x
is arbitrarily taken as the pole desig-
kq1
.
nated vertex of the simplex, and the next trial point x
is obtained as the
t
Ž
.
image reflection
point of the pole with respect to the centroid x
of the
c
remaining k points. Thus
x s2x yx
.
t
c
kq1
The point x
must satisfy the constraints of the region R. The value of the
t
function f at x
is then compared with f
, the largest function value in
t
max
Ž
.
storage. Let x
denote the point at which f
is achieved. If f x
f
,
max
max
t
max
then x
is replaced in the array A by x . If x fails to satisfy the constraints
max
t
t
Ž
.
of the region R, or if f x
f
, then x
is discarded and a new point is
t
max
t

THE DIRECT SEARCH METHODS
337
Ž
.
Figure 8.3. A flow diagram for Price’s procedure. Source: Price
1977 . Reproduced with
permission of Oxford University Press.

OPTIMIZATION IN STATISTICS
338
chosen by following the same procedure as the one used to obtain x . As the
t
algorithm proceeds, the N points in storage tend to cluster around points at
which the function values are lower than the current value of f
. Price did
max
not specify a particular stopping rule. He left it to the user to do so. A
possible stopping criterion is to terminate the search when the N points in
storage cluster in a small region of the k-dimensional space, that is, when
f
and f
are close together, where f
is the smallest function value in
max
min
min
storage. Another possibility is to stop after a specified number of function
evaluations have been made. In any case, the rate of convergence of the
procedure depends on the value of N, the complexity of the function f, the
nature of the constraints, and the way in which the set of trial points is
chosen.
Price’s procedure is simple and does not necessarily require a large value
of N. It is sufficient that N should increase linearly with k. Price chose, for
example, the value Ns50 for ks2. The value Ns10k has proved useful
for many functions. Furthermore, the region constraints can be quite com-
plex. A FORTRAN program for the implementation of Price’s algorithm was
Ž
.
written by Conlon 1991 .
8.2.3. The Generalized Simulated Annealing Method
This method derives its name from the annealing of metals, in which many
Ž
.
final crystalline configurations corresponding to different energy states are
Ž
possible, depending on the rate of cooling
see Kirkpartrick, Gelatt, and
.
Ž
.
Vechhi, 1983 . The method can be applied to find the absolute or global
optimum of a multimodal function f within a constrained region R in a
k-dimensional space.
Ž
.
Bohachevsky, Johnson, and Stein 1986 presented a generalization of the
method of simulated annealing for function optimization. The following is a
Ž
description of their algorithm for function minimization a similar one can be
.
used for function maximization : Let f
be some tentative estimate of the
m
minimum of f over the region R. The method proceeds according to the
Ž
following steps
reproduced with permission of the American Statistical
.
Association :
1. Select an initial point x
in R. This point can be chosen at random or
0
specified depending on available information.
Ž
.


2. Calculate
f sf x
. If
f yf
, where  is a specified small
0
0
0
m
constant, then stop.
3. Choose a random direction of search by generating k independent
standard normal variates z , z , . . . , z ; then compute the elements of
1
2
k
Ž
.
the random vector us u , u , . . . , u
, where
1
2
k
zi
u s
,
is1, 2, . . . , k,
i
1r2
2
2
2
z qz q  qz
Ž
.
1
2
k
and k is the number of variables in the function.

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
339
4. Set x sx q r u, where  r is the size of a step to be taken in the
1
0
direction of u. The magnitude of  r depends on the properties of the
objective function and on the desired accuracy.
5. If x
does not belong to R, return to step 3. Otherwise, compute
1
Ž
.
f sf x
and  fsf yf .
1
1
1
0


6. if f Ff , set x sx and f sf . If f yf
, stop. Otherwise, go to
1
0
0
1
0
1
0
m
step 3.
Ž
g
.
7. If f f , set a probability value given by psexp y f  f , where 
1
0
0
Ž
.
is a positive number such that 0.50exp y  f 0.90, and g is an
arbitrary negative number. Then, generate a random number ® from
Ž
.
the uniform distribution U 0, 1 . If ®Gp, go to step 3. Otherwise, if
®p, set x sx , f sf , and go to step 3.
0
1
0
1
Ž
.
From steps 6 and 7 we note that beneficial steps
that is, f Ff
are
1
0
Ž
.
accepted unconditionally, but detrimental steps
f f
are accepted accord-
1
0
ing to a probability value p described in step 7. If ®p, then the step leading
to x
is accepted; otherwise, it is rejected and a step in a new random
1
direction is attempted. Thus the probability of accepting an increment of f
depends on the size of the increment: the larger the increment, the smaller
the probability of its acceptance.
Several possible values of the tentative estimate f
can be attempted. For
m
a given f
we proceed with the search until fyf
becomes negative. Then,
m
m
we decrease f , continue the search, and repeat the process when necessary.
m
Bohachevsky, Johnson, and Stein gave an example in optimal design theory
to illustrate the application of their algorithm.
Ž
.
Price’s 1977 controlled random search algorithm produces results compa-
rable to those of simulated annealing, but with fewer tuning parameters. It is
also better suited for problems with constrained regions.
8.3. OPTIMIZATION TECHNIQUES IN RESPONSE
SURFACE METHODOLOGY
Ž
.
Response surface methodology RSM is an area in the design and analysis of
experiments. It consists of a collection of techniques that encompasses:
1. Conducting a series of experiments based on properly chosen settings
of a set of input variables, denoted by x , x , . . . , x , that influence a
1
2
k
response of interest y. The choice of these settings is governed by
certain criteria whose purpose is to produce adequate and reliable
information about the response. The collection of all such settings
constitutes a matrix D of order nk, where n is the number of
experimental runs. The matrix D is referred to as a response surface
design.

OPTIMIZATION IN STATISTICS
340
2. Determining a mathematical model that best fits the data collected
Ž .
under the design chosen in 1 . Regression techniques can be used to
evaluate the adequacy of fit of the model and to conduct appropriate
tests concerning the model’s parameters.
3. Determining optimal operating conditions on the input variables that
Ž
.
produce maximum
or minimum
response value within a region of
interest R.
This last aspect of RSM can help the experimenter in determining the best
combinations of the input variables that lead to desirable response values.
For example, in drug manufacturing, two drugs are tested with regard to
reducing blood pressure in humans. A series of clinical trials involving a
certain number of high blood pressure patients is set up, and each patient
is given some predetermined combination of the two drugs. After a period of
time the patient’s blood pressure is checked. This information can be used to
find the specific combination of the drugs that results in the greatest
reduction in the patient’s blood pressure within some specified time interval.
In this section we shall describe two well-known optimum-seeking proce-
Ž
.
dures in RSM. These include the method of steepest ascent or descent and
ridge analysis.
8.3.1. The Method of Steepest Ascent
This is an adaptation of the method described in Section 8.1.1 to a response
surface environment; here the objective is to increase the value of a certain
response function.
The method of steepest ascent requires performing a sequence of sets of
trials. Each set is obtained as a result of proceeding sequentially along a path
of maximum increase in the values of a given response y, which can be
observed in an experiment. This method was first introduced by Box and
Ž
.
Wilson 1951 for the general area of RSM.
The procedure of steepest ascent depends on approximating a response
surface with a hyperplane in some restricted region. The hyperplane is
represented by a first-order model which can be fitted to a data set obtained
as a result of running experimental trials using a first-order design such as a
complete 2 k factorial design, where k is the number of input variables in the
w
model. A fraction of this design can also be used if k is large
see, for
Ž
.x
example, Section 3.3.2 in Khuri and Cornell
1996 . The fitted first-order
model is then used to determine a path along which one may initially observe
increasing response values. However, due to curvature in the response
surface, the initial increase in the response will likely be followed by a
leveling off, and then a decrease. At this stage, a new series of experiments is
Ž
.
performed using again a first-order design and the resulting data are used
to fit another first-order model. A new path is determined along which

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
341
increasing response values may be observed. This process continues until it
becomes evident that little or no additional increase in the response can be
gained.
Let us now consider more specific details of this sequential procedure. Let
Ž .
y x be a response function that depends on k input variables, x , x , . . . , x ,
1
2
k
which form the elements of a vector x. Suppose that in some restricted region
Ž .
y x is adequately represented by a first-order model of the form
k
y x s q
 x q ,
8.8
Ž .
Ž
.
Ý
0
i
i
is1
where  ,  , . . . , 
are unknown parameters and  is a random error. This
0
1
k
Ž
model is fitted using data collected under a first-order design for example, a
k
.
2
factorial design or a fraction thereof . The data are utilized to calculate
ˆ
ˆ
ˆ
the least-squares estimates  ,  , . . . , 
of the model’s parameters. These
0
1
k
ˆ
y1
Ž
.
w
x
are elements of s XX
Xy, where Xs 1 : D with 1
being a vector of
n
n
ones of order n1, D is the design matrix of order nk, and y is the
corresponding vector of response values. It is assumed that the random
errors associated with the n response values are independently distributed
with means equal to zero and a common variance 	 2. The predicted
Ž .
response y x is then given by
ˆ
k
ˆ
ˆ
y x s q
 x .
8.9
Ž .
Ž
.
ˆ
Ý
0
i
i
is1
The input variables are coded so that the design center coincides with the
origin of the coordinates system.
The next step is to move a distance of r units away from the design center
Ž
.
or the origin
such that a maximum increase in y can be obtained. To
ˆ
determine the direction to be followed to achieve such an increase, we need
Ž .
k
2
2
to maximize y x subject to the constraint Ý
x sr
using the method of
ˆ
is1
i
Lagrange multipliers. Consider therefore the function
k
k
2
2
ˆ
ˆ
Q x s q
 x y
x yr
,
8.10
Ž .
Ž
.
Ý
Ý
0
i
i
i
ž
/
is1
is1
where  is a Lagrange multiplier. Setting the partial derivatives of Q equal to
zero produces the equations
1 ˆ
x s
 ,
is1, 2, . . . , k.
i
i
2
For a maximum,  must be positive. Using the equality constraint, we
conclude that
1r2
k
1
2
ˆ
s

.
Ý
i
ž
/
2r
is1

OPTIMIZATION IN STATISTICS
342
A local maximum is then achieved at the point whose coordinates are given
by
ˆ
ri
x s
,
is1, 2, . . . , k,
i
1r2
k
2
ˆ
Ý

Ž
.
is1
i
which can be written as
x sre ,
is1, 2, . . . , k,
8.11
Ž
.
i
i
ˆ
k
ˆ2 y1r2
Ž
.
Ž
.
where e s Ý

, is1, 2, . . . , k. Thus es e , e , . . . , e
 is a unit
i
i
is1
i
1
2
k
ˆ
ˆ
ˆ
Ž
.
Ž
.
vector in the direction of
 ,  , . . . , 
. Equations 8.11 indicate that at a
1
2
k
distance of r units away from the origin, a maximum increase in y occurs
ˆ
along a path in the direction of e. Since this is the only local maximum on the
hypersphere of radius r, it must be the absolute maximum.
Ž
.
If the actual response value that is, the value of y
at the point xsre
exceeds its value at the origin, then a move along the path determined by e is
in order. A series of experiments is then conducted to obtain response values
at several points along the path until no additional increase in the response is
evident. At this stage, a new first-order model is fitted using data collected
under a first-order design centered at a point in the vicinity of the point at
which that first drop in the response was observed along the path. This model
Ž
.
leads to a new direction similar to the one given by formula 8.11 . As before,
a series of experiments are conducted along the new path until no further
increase in the value of y can be observed. The process of moving along
different paths continues until it becomes evident that little or no additional
increase in y can be gained. This usually occurs when the first-order model
becomes inadequate as the method progresses, due to curvature in the
response surface. It is therefore necessary to test each fitted first-order
model for lack of fit at every stage of the process. This can be accomplished
by taking repeated observations at the center of each first-order design and
at possibly some other design points in order to obtain an independent
w
estimate of the error variance that is needed for the lack of fit test see, for
Ž
.x
example, Sections 2.6 and 3.4 in Khuri and Cornell 1996 . If the lack of fit
test is significant, indicating an inadequate model, then the process is
stopped and a more elaborate experiment must be conducted to fit a
higher-order model, as will be seen in the next section.
Examples that illustrate the application of the method of steepest ascent
Ž
.
Ž
.
can be found in Box and Wilson 1951 , Bayne and Rubin 1986, Section 5.2 ,
Ž
.
Ž
.
Khuri and Cornell 1996, Chapter 5 , and Myers and Khuri 1979 . In the last
reference, the authors present a stopping rule along a path that takes into
account random error variation in the observed response. We recall that a
search along a path is discontinued as soon as a drop in the response is first

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
343
observed. Since response values are subject to random error, the decision to
stop can be premature due to a false drop in the observed response. The
Ž
.
stopping rule by Myers and Khuri 1979 protects against taking too many
Ž
observations along a path when in fact the true mean response that is, the
.
mean of y is decreasing. It also protects against stopping prematurely when
the true mean response is increasing.
It should be noted that the procedure of steepest ascent is not invariant
with respect to the scales of the input variables x , x , . . . , x . This is evident
1
2
k
from the fact that a path taken by the procedure is determined by the
ˆ
ˆ
ˆ w
Ž
.x
least-squares estimates  ,  , . . . , 
see equations 8.11 , which depend on
1
2
k
the scales of the x ’s.
i
There are situations in which it is of interest to determine conditions that
lead to a decrease in the response, instead of an increase. For example, in a
chemical investigation it may be desired to decrease the level of impurity or
the unit cost. In this case, a path of steepest descent will be needed. This can
be accomplished by changing the sign of the response y, followed by an
application of the method of steepest ascent. Thus any steepest descent
problem can be handled by the method of steepest ascent.
8.3.2. The Method of Ridge Analysis
The method of steepest ascent is most often used as a maximum-region-seek-
ing procedure. By this we mean that it is used as a preliminary tool to get
quickly to the region where the maximum of the mean response is located.
Since the first-order approximation of the mean response will eventually
break down, a better estimate of the maximum can be obtained by fitting a
second-order model in the region of the maximum. The method of ridge
Ž
.
analysis, which was introduced by Hoerl 1959 and formalized by Draper
Ž
.
1963 , is used for this purpose.
Let us suppose that inside a region of interest R, the true mean response
is adequately represented by the second-order model
k
ky1
k
k
2
y x s q
 x q
 x x q
 x q ,
8.12
Ž .
Ž
.
Ý
Ý Ý
Ý
0
i
i
i j
i
j
ii
i
is1
is1 js2
is1
ij
where the  ’s are unknown parameters and  is a random error with mean
2
Ž
.
zero and variance 	 . Model 8.12 can be written as
y x s qxqxBxq ,
8.13
Ž .
Ž
.
0

OPTIMIZATION IN STATISTICS
344
Ž
.
where s  ,  , . . . , 
 and B is a symmetric kk matrix of the form
1
2
k
1
1
1





11
12
13
1k
2
2
2
1
1




22
23
2 k
2
2 .
. .
.
.
.
.
.
Bs
.
.
1
.
ky1 , k
2
. .
symmetric
kk
Ž
.
Least-squares estimates of the parameters in model 8.13 can be obtained by
using data collected according to a second-order design. A description of
Ž
potential second-order designs can be found in Khuri and Cornell
1996,
.
Chapter 4 .
ˆ
ˆ
ˆ
Let  , , and B denote the least-squares estimates of  , , and B,
0
0
Ž .
respectively. The predicted response y x inside the region R is then given by
ˆ
ˆ
ˆ
ˆ
y x s qxqxBx.
8.14
Ž .
Ž
.
ˆ
0
The input variables are coded so that the design center coincides with the
origin of the coordinates system.
Ž
The method of ridge analysis is used to find the optimum maximum or
.
Ž .
minimum
of y x
on concentric hyperspheres of varying radii inside the
ˆ
region R. It is particularly useful in situations in which the unconstrained
Ž .
optimum of y x
falls outside the region R, or if a saddle point occurs
ˆ
inside R.
Ž .
Let us now proceed to optimize y x subject to the constraint
ˆ
k
2
2
x sr ,
8.15
Ž
.
Ý
i
is1
where r is the radius of a hypersphere centered at the origin and is contained
inside the region R. Using the method of Lagrange multipliers, let us
consider the function
k
2
2
Fsy x y
x yr
,
8.16
Ž .
Ž
.
ˆ
Ý
i
ž
/
is1
where  is a Lagrange multiplier. Differentiating
F with respect to xi

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
345
Ž
.
is1, 2, . . . , k and equating the partial derivatives to zero, we obtain
"F
ˆ
ˆ
ˆ
ˆ
s2 
y x q
x q  q
x q s0,
Ž
.
11
1
12
2
1k
k
1
" x1
"F
ˆ
ˆ
ˆ
ˆ
s
x q2 
y x q  q
x q s0,
Ž
.
12
1
22
2
2 k
k
2
" x2
...
"F
ˆ
ˆ
ˆ
ˆ
s
x q
x q  q2 
y x q s0.
Ž
.
1k
1
2 k
2
kk
k
k
" xk
These equations can be expressed as
1
ˆ
ˆ
ByI
xsy .
8.17
Ž
.
Ž
.
k
2
Ž
.
Ž
.
Equations 8.15 and 8.17 need to be solved for x , x , . . . , x
and . This
1
2
k
traditional approach, however, requires calculations that are somewhat in-
Ž
.
volved. Draper 1963 proposed the following simpler, yet equivalent proce-
dure:
i. Regard r as a variable, but fix  instead.
Ž
.
ii. Insert the selected value of  in equation 8.17 and solve for x. The
solution is used in steps iii and iv.
Ž 
 .1r2
iii. Compute rs x x
.
Ž .
iv. Evaluate y x .
ˆ
Several values of  can give rise to several stationary points which lie on
the same hypersphere of radius r. This can be seen from the fact that if  is
ˆ
Ž
.
chosen to be different from any eigenvalue of B, then equation 8.17 has a
unique solution given by
y1
1 ˆ
ˆ
xsy
ByI
.
8.18
Ž
.
Ž
.
k
2
Ž
.
By substituting x in equation 8.15 we obtain
y2
2
ˆ ˆ
ˆ
 ByI
s4r .
8.19
Ž
.
Ž
.
k
Hence, each value of r gives rise to at most 2k corresponding values of .
The choice of  has an effect on the nature of the stationary point. Some
values of  produce points at each of which y has a maximum. Other values
ˆ
of  cause y to have minimum values. More specifically, suppose that  and
ˆ
1

OPTIMIZATION IN STATISTICS
346
Ž
.
 are two values substituted for  in equation 8.18 . Let x , x
and r , r
be
2
1
2
1
2
the corresponding values of x and r, respectively. The following results,
Ž
.
which were established in Draper 1963 , can be helpful in selecting the value
of  that produces a particular type of stationary point:
RESULT 1.
If r sr
and   , then y y , where y and y
are the
ˆ
ˆ
ˆ
ˆ
1
2
1
2
1
2
1
2
Ž .
values of y x at x and x , respectively.
ˆ
1
2
This result means that for two stationary points that have the same
distance from the origin, y will be larger at the stationary point with the
ˆ
larger value of .
RESULT 2.
Let M be the matrix of second-order partial derivatives of F
Ž
.
in formula 8.16 , that is,
ˆ
Ms2 ByI
.
8.20
Ž
.
Ž
.
k
Ž
If r sr , and if M is positive definite for x and is indefinite that is, neither
1
2
1
.
positive definite nor negative definite for x , then y y .
ˆ
ˆ
2
1
2
ˆ
RESULT 3.
If 
is larger than the largest eigenvalue of B, then the
1
Ž
.
corresponding solution x
in formula 8.18 is a point of absolute maximum
1
Ž 
.1r2
for y on a hypersphere of radius r s x x
. If, on the other hand,  is
ˆ
1
1
1
1
ˆ
smaller than the smallest eigenvalue of B, then x
is a point of absolute
1
minimum for y on the same hypersphere.
ˆ
On the basis of Result 3 we can select several values of  that exceed the
ˆ
largest eigenvalue of B. The resulting values of the k elements of x and y can
ˆ
be plotted against the corresponding values of r. This produces kq1 plots
Ž
.
called ridge plots see Myers, 1976, Section 5.3 . They are useful in that an
experimenter can determine, for a particular r, the maximum of y within a
ˆ
Ž
.
region R and the operating conditions that is, the elements of x that give
rise to the maximum. Similar plots can be obtained for the minimum of yˆ
ˆ
Žhere, values of  that are smaller than the smallest eigenvalue of B must be
.
chosen . Obviously, the portions of the ridge plots that fall outside R should
not be considered.
EXAMPLE 8.3.1.
An experiment was conducted to investigate the effects
of three fertilizer ingredients on the yield of snap beans under field condi-
tions. The fertilizer ingredients and actual amounts applied were nitrogen
Ž .
Ž
.
N , from 0.94 to 6.29 lbrplot; phosphoric acid
P O , from 0.59 to 2.97
2
5
Ž
.
lbrplot; and potash
K O , from 0.60 to 4.22 lbrplot. The response of
2
interest is the average yield in pounds per plot of snap beans.

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
347
Five levels of each fertilizer were used. The levels are coded using the
following linear transformations:
X y3.62
X y1.78
X y2.42
1
2
3
x s
,
x s
,
x s
.
1
2
3
1.59
0.71
1.07
Here, X , X , and X
denote the actual levels of nitrogen, phosphoric acid,
1
2
3
and potash, respectively, used in the experiment, and x , x , x
the corre-
1
2
3
sponding coded values. In this particular coding scheme, 3.62, 1.78, and 2.42
are the averages of the experimental levels of X , X , and X , respectively,
1
2
3
that is, they represent the centers of the values of nitrogen, phosphoric acid,
and potash, respectively. The denominators of x , x , and x
were chosen so
1
2
3
that the second and fourth levels of each X correspond to the values y1
i
Ž
.
and 1, respectively, for x
is1, 2, 3 . One advantage of such a coding
i
Ž
scheme is to make the levels of the three fertilizers scale free
this is
necessary in general, since the input variables can have different units of
.
measurement . The measured and coded levels for the three fertilizers are
shown below:
Ž
.
Levels of x
is1, 2, 3
i
Fertilizer
y1.682
y1.000
0.000
1.000
1.682
N
0.94
2.03
3.62
5.21
6.29
P O
0.59
1.07
1.78
2.49
2.97
2
5
K O
0.60
1.35
2.42
3.49
4.22
2
Combinations of the levels of the three fertilizers were applied according
to the experimental design shown in Table 8.1, in which the design settings
are given in terms of the coded levels. Six center-point replications were run
in order to obtain an estimate of the experimental error variance. This
w
particular design is called a central composite design for a description of this
Ž
.x
design and its properties, see Khuri and Cornell 1996, Section 4.5.3 , which
has the rotatability property. By this we mean that the prediction variance,
w Ž .x
that is, Var y x , is constant at all points that are equidistant from the design
ˆ
w
Ž
.
center see Khuri and Cornell 1996, Section 2.8.3 for more detailed infor-
x
Ž
.
mation concerning rotatability . The corresponding response
yield
values
are given in Table 8.1.
Ž
.
A second-order model of the form given by formula 8.12 was fitted to the
data set in Table 8.1. Thus in terms of the coded variables we have the model
3
3
2
y x s q
 x q
x x q
x x q
x x q
 x q.
8.21
Ž .
Ž
.
Ý
Ý
0
i
i
12
1
2
13
1
3
23
2
3
ii
i
is1
is1

OPTIMIZATION IN STATISTICS
348
Table 8.1. The Coded and Actual Settings of the Three Fertilizers
and the Corresponding Response Values
x
x
x
N
P O
K O
Yield y
1
2
3
2
5
2
y1
y1
y1
2.03
1.07
1.35
11.28
1
y1
y1
5.21
1.07
1.35
8.44
y1
1
y1
2.03
2.49
1.35
13.19
1
1
y1
5.21
2.49
1.35
7.71
y1
y1
1
2.03
1.07
3.49
8.94
1
y1
1
5.21
1.07
3.49
10.90
y1
1
1
2.03
2.49
3.49
11.85
1
1
1
5.21
2.49
3.49
11.03
y1.682
0
0
0.94
1.78
2.42
8.26
1.682
0
0
6.29
1.78
2.42
7.87
0
y1.682
0
3.62
0.59
2.42
12.08
0
1.682
0
3.62
2.97
2.42
11.06
0
0
y1.682
3.62
1.78
0.60
7.98
0
0
1.682
3.62
1.78
4.22
10.43
0
0
0
3.62
1.78
2.42
10.14
0
0
0
3.62
1.78
2.42
10.22
0
0
0
3.62
1.78
2.42
10.53
0
0
0
3.62
1.78
2.42
9.50
0
0
0
3.62
1.78
2.42
11.53
0
0
0
3.62
1.78
2.42
11.02
Ž
.
Source: A. I. Khuri and J. A. Cornell 1996 . Reproduced with permission of Marcel Dekker, Inc.
The resulting prediction equation is given by
y x s10.462y0.574 x q0.183x q0.456 x y0.678 x x q1.183x x
Ž .
ˆ
1
2
3
1
2
1
3
q0.233x x y0.676 x 2q0.563x 2y0.273x 2.
8.22
Ž
.
2
3
1
2
3
Ž .
Ž
.
Ž
.
Here, y x is the predicted yield at the point xs x , x , x . Equation 8.22
ˆ
1
2
3
ˆ
Ž
.
can be expressed
in matrix form as in equation
8.14 , where
s
ˆ
Ž
.
y0.574, 0.183, 0.456  and B is the matrix
y0.676
y0.339
0.592
ˆBs
.
8.23
Ž
.
y0.339
0.563
0.117
0.592
0.117
y0.273

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
349
Ž .
The coordinates of the stationary point x
of y x satisfy the equation
ˆ
0
" yˆ sy0.574q2 y0.676 x y0.339x q0.592 x
s0,
Ž
.
1
2
3
" x1
" yˆ s0.183q2 y0.339x q0.563x q0.117x
s0,
Ž
.
1
2
3
" x2
" yˆ s0.456q2 0.592 x q0.117x y0.273x
s0,
Ž
.
1
2
3
" x3
which can be expressed as
ˆ
ˆ
q2Bx s0.
8.24
Ž
.
0
Hence,
1
y1
ˆ
ˆ
x sy B
s y0.394,y0.364,y0.175 .
Ž
.
0
2
ˆ
The eigenvalues of B are  s0.6508,  s0.1298,  sy1.1678. The matrix
1
2
3
ˆB is therefore neither positive definite nor negative definite, that is, x
is a
0
Ž
.
saddle point
see Corollary 7.7.1 . This point falls inside the experimental
region R, which, in the space of the coded variables x , x , x , is a sphere
1
2
3
'
centered at the origin of radius
3 .
Let us now apply the method of ridge analysis to maximize y inside the
ˆ
w
region R. For this purpose we choose values of  the Lagrange multiplier in
ˆ
Ž
.x
equation 8.16
larger than  s0.6508, the largest eigenvalue of B. For each
1
Ž
.
such value of , equation 8.17 has a solution for x that represents a point of
Ž .
Ž
.1r2
absolute maximum of y x on a sphere of radius rs xx
inside R. The
ˆ
results
are
displayed
in
Table
8.2.
We
note
that
at
the
point
Ž
.
y0.558, 1.640, 0.087 , which is located near the periphery of the region R,
the maximum value of y is 13.021. By expressing the coordinates of this point
ˆ
in terms of the actual values of the three fertilizers we obtain X s2.733
1
lbrplot, X s2.944 lbrplot, and X s2.513 lbrplot. We conclude that a
2
3
combination of nitrogen, phosphoric acid, and potash fertilizers at the rates
Table 8.2. Ridge Analysis Values

1.906
1.166
0.979
0.889
0.840
0.808
0.784
0.770
0.754
0.745
0.740
x
y0.106
y0.170
y0.221
y0.269
y0.316
y0.362
y0.408
y0.453
y0.499
y0.544
y0.558
1
x
0.102
0.269
0.438
0.605
0.771
0.935
1.099
1.263
1.426
1.589
1.640
2
x
0.081
0.110
0.118
0.120
0.117
0.113
0.108
0.102
0.096
0.089
0.087
3
r
0.168
0.337
0.505
0.673
0.841
1.009
1.177
1.346
1.514
1.682
1.734
y
10.575
10.693
10.841
11.024
11.243
11.499
11.790
12.119
12.484
12.886
13.021
ˆ

OPTIMIZATION IN STATISTICS
350
of 2.733, 2.944, and 2.513 lbrplot, respectively, results in an estimated
maximum yield of snap beans of 13.021 lbrplot.
8.3.3. Modified Ridge Analysis
Ž .
Optimization of y x on a hypersphere S by the method of ridge analysis is
ˆ
justified provided that the prediction variance on S is relatively small.
Furthermore, it is desirable that this variance remain constant on S. If not,
then it is possible to obtain poor estimates of the optimum response,
especially when the dispersion in the prediction variances on S is large. Thus
the reliability of ridge analysis as an optimum-seeking procedure depends
very much on controlling the size and variability of the prediction variance. If
w Ž .x
the design is rotatable, then the prediction variance, Var y x , is constant on
ˆ
S. It is then easy to attain small prediction variances by restricting the
procedure to hyperspheres of small radii. However, if the design is not
w Ž .x
rotatable, then Var y x
may vary widely on S, which, as was mentioned
ˆ
earlier, can adversely affect the quality of estimation of the optimum re-
sponse. This suggests that the prediction variance should be given serious
consideration in the strategy of ridge analysis if the design used is not
rotatable.
Ž
.
Khuri and Myers 1979 proposed a certain modification to the method of
Ž .
ridge analysis: one that optimizes y x subject to a particular constraint on
ˆ
the prediction variance. The following is a description of their proposed
modification:
Ž
.
Consider model 8.12 , which can be written as
y x sf x q ,
8.25
Ž .
Ž .
Ž
.
where
f x s 1, x , x , . . . , x , x x , x x , . . . , x
x , x 2, x 2, . . . , x 2 ,
Ž .
Ž
.
1
2
k
1
2
1
3
ky1
k
1
2
k
s
 ,  ,  , . . . ,  , 
, 
, . . . , 
, 
, 
, . . . , 
.
Ž
.
0
1
2
k
12
13
ky1 , k
11
22
kk
The predicted response is given by
y x sf x ,
8.26
Ž .
Ž .
Ž
.
ˆ
ˆ
where  is the least-squares estimator of , namely,
ˆ
y1
s XX
Xy,
8.27
Ž
.
Ž
.
ˆ
w Ž
.
Ž
.
Ž
.x
where Xs f x
, f x
, . . . , f x
 with x being the vector of design settings
1
2
n
i
Ž
at the ith experimental run
is1, 2, . . . , n, where n is the number of runs
.
used in the experiment , and y is the corresponding vector of n observations.
Since
y1
2
Var  s XX
	 ,
8.28
Ž
.
Ž
.
Ž
.
ˆ

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
351
2
Ž
.
where 	
is the error variance, then from equation 8.26 , the prediction
variance is of the form
y1
2
Var y x
s	 f x
XX
f x .
8.29
Ž .
Ž . Ž
.
Ž .
Ž
.
ˆ
Ž
.
Ž
.Ž
.
The number of unknown parameters in model 8.25 is ps kq1 kq2 r2,
where k is the number of input variables. Let  ,  , . . . , 
denote the
1
2
p
Ž
.
eigenvalues of XX. Then from equation 8.29 and Theorem 2.3.16 we have
	 2f x f x
	 2f x f x
Ž . Ž .
Ž . Ž .
FVar y x
F
,
Ž .
ˆ


max
min
where 
and 
are, respectively, the smallest and largest of the  ’s.
min
max
i
This double inequality shows that the prediction variance can be inflated if
XX has small eigenvalues. This occurs when the columns of X are multi-
Ž
.
collinear
see, for example, Myers, 1990, pages 125126 and Chapter 8 .
Ž
.
Now, by the spectral decomposition theorem Theorem 2.3.10 , XXsVV,
where V is an orthogonal matrix of orthonormal eigenvectors of XX and
Ž
.
sDiag  ,  , . . . , 
is a diagonal matrix of eigenvalues of XX. Equation
1
2
p
Ž
.
8.29 can then be written as
2
p
f x v
Ž .
j
2
Var y x
s	
,
8.30
Ž .
Ž
.
ˆ
Ý
j
js1
Ž
.
where v is the jth column of V
js1, 2, . . . , p . If we denote the elements of
j
Ž .
v by ® , ® , . . . , ® , ®
, ®
, . . . , ®
, ®
, ®
, . . . , ®
, then f x v
can
j
0 j
1 j
k j
12 j
13 j
ky1 k j
11 j
22 j
kk j
j
be expressed as
f x v s® qx qxT x,
js1, 2, . . . , p,
8.31
Ž .
Ž
.
j
0 j
j
j
Ž
.
where  s ® , ® , . . . , ®
 and
j
1 j
2 j
k j
1
1
1
®
®
®

®
11 j
12 j
13 j
1k j
2
2
2
1
1
®
®

®
22 j
23 j
2 k j
2
2 .
. .
.
.
.
.
T s
,
js1, 2, . . . , p.
.
j
.
1
.
®ky1 k j
2
. .
symmetric
®kk j
Ž .
Ž
.
We note that the form of f x v , as given by formula 8.31 , is identical to
j
Ž
.
that of a second-order model. Formula 8.30 can then be written as
2
p
® qx qxT x
Ž
.
0 j
j
j
2
Var y x
s	
.
8.32
Ž .
Ž
.
ˆ
Ý
j
js1

OPTIMIZATION IN STATISTICS
352
Ž
.
Ž .
As was noted earlier, small values of 
js1, 2, . . . , p cause y x to have
ˆ
j
large variances.
To reduce the size of the prediction variance within the region explored by
w Ž .x
ridge analysis, we can consider putting constraints on the portion of Var y x
ˆ
Ž .
that corresponds to 
. It makes sense to optimize y x
subject to the
ˆ
min
constraints
xxsr 2,
8.33
Ž
.
®
qx qxT x Fq,
8.34
Ž
.
0m
m
m
where ®
,  , and T
are the values of ® , , and T that correspond to 
.
0m
m
m
0
min
Here, q is a positive constant chosen small enough to offset the small value
Ž
.
of 
. Khuri and Myers
1979 suggested that q be equal to the largest
min


value taken by
®
qx qxT x
at the n design points. The rationale
0m
m
m
behind this rule of thumb is that the prediction variance is smaller at the
design points than at other points in the experimental region.
Ž
.
The modification suggested by Khuri and Myers 1979 amounts to adding
Ž
.
the constraint
8.34 to the usual procedure of ridge analysis. In this way,
some control can be maintained on the size of prediction variance during the
optimization process. The mathematical algorithm needed for this con-
strained optimization is based on a technique introduced by Myers and
Ž
.
Carter 1973 for a dual response system in which a primary second-order
response function is optimized subject to the condition that a constrained
second-order response function takes on some specified or desirable values.
Ž .
Here, the primary response is y x and the constrained response is ®
q
ˆ
0m
x qxT x.
m
m
Ž
.
Myers and Carter’s 1973 procedure is based on the method of Lagrange
multipliers, which uses the function
ˆ
ˆ
ˆ
2
Ls qxqxBxy ®
qx qxT xy
 y xxyr
,
Ž
.
Ž
.
0
0m
m
m
ˆ
ˆ
ˆ
Ž
.
where  , , and B are the same as in model 8.14 ,  and  are Lagrange
0


w
Ž
.x
multipliers, 
 is such that 
 Fq see inequality 8.34 , and r is the radius
of a hypersphere centered at the origin and contained inside a region of
interest R. By differentiating L with respect to x , x , . . . , x
and equating
1
2
k
the derivatives to zero, we obtain
1
ˆ
ˆ
ByT yI
xs
 y .
8.35
Ž
.
Ž
.
Ž
.
m
k
m
2
Ž
.
As in the method of ridge analysis, to solve equation 8.35 , values of  and
 are chosen directly in such a way that the solution represents a point of
Ž
.
Ž .
maximum or minimum for y x . Thus for a given value of , the matrix of
ˆ
ˆ
Ž
.
second-order partial derivatives of L, namely 2 ByT yI
, is made
m
k
w
Ž .
x
negative definite and hence a maximum of y x is achieved by selecting 
ˆ

OPTIMIZATION TECHNIQUES IN RESPONSE SURFACE METHODOLOGY
353
ˆ
larger than the largest eigenvalue of ByT . Values of  smaller than the
m
ˆ
Ž .
smallest eigenvalue of ByT
should be considered in order for y x to
ˆ
m
attain a minimum. It follows that for such an assignment of values for  and
Ž
.
, the corresponding solution of equation 8.35 produces an optimum for yˆ
Ž
.1r2
subject to a fixed rs xx
and a fixed value of ®
qx qxT x.
0m
m
m
EXAMPLE 8.3.2.
An attempt was made to design an experiment from
which one could find conditions on concentration of three basic substances
that maximize a certain mechanical modular property of a solid propellant.
Ž
The initial intent was to construct and use a central composite design see
.
Khuri and Cornell, 1996, Section 4.5.3
for the three components in the
system. However, certain experimental difficulties prohibited the use of the
design as planned, and the design used led to problems with multicollinearity
as far as the fitting of the second-order model is concerned. The design
settings and corresponding response values are given in Table 8.3.
In this example, the smallest eigenvalue of XX is 
s0.0321. Corre-
min
Ž
.
spondingly, the values of ®
,  , and T
in inequality
8.34
are ®
s
0m
m
m
0m
Ž
.
y0.2935,  s 0.0469, 0.4081, 0.4071 , and
m
0.1129
0.0095
0.2709
T s
.
0.0095
y0.1382
y0.0148
m
0.2709
y0.0148
0.6453
Ž
.


As for q in inequality 8.34 , values of ®
qx qxT x were computed
0m
m
m
at each of the 15 design points in Table 8.3. The largest value was found to
Table 8.3. Design Settings and Response Values for Example 8.3.2
x
x
x
y
1
2
3
y1.020
y1.402
y0.998
13.5977
0.900
0.478
y0.818
12.7838
0.870
y1.282
0.882
16.2780
y0.950
0.458
0.972
14.1678
y0.930
y1.242
y0.868
9.2461
0.750
0.498
y0.618
17.0167
0.830
y1.092
0.732
13.4253
y0.950
0.378
0.832
16.0967
1.950
y0.462
0.002
14.5438
y2.150
y0.402
y0.038
20.9534
y0.550
0.058
y0.518
11.0411
y0.450
1.378
0.182
21.2088
0.150
1.208
0.082
25.5514
0.100
1.768
y0.008
33.3793
1.450
y0.342
0.182
15.4341
Ž
.
Source: Khuri and Myers 1979 . Reproduced with permission of the Ameri-
can Statistical Association.

OPTIMIZATION IN STATISTICS
354
Table 8.4. Results of Modified Ridge Analysis
r
0.848
1.162
1.530
1.623
1.795
1.850
1.904
1.935
2.000



0.006
0.074
0.136
1.139
0.048
0.086
0.126
0.146
0.165
2
w Ž .x
Var y x r	
1.170
1.635
2.922
3.147
1.305
2.330
3.510
4.177
5.336
ˆ
x
0.410
0.563
0.773
0.785
0.405
0.601
0.750
0.820
0.965
1
x
0.737
1.015
1.320
1.422
1.752
1.751
1.750
1.752
1.752
2
x
0.097
0.063
0.019
0.011
0.000
0.000
0.012
0.015 y0.030
3
Ž .
y x
22.420 27.780 35.242 37.190 37.042 40.222 42.830 44.110
46.260
ˆ
Ž
.
Source: Khuri and Myers
1979 . Reproduced with permission of the American Statistical
Association.
Table 8.5. Results of Standard Ridge Analysis
r
0.140
0.379
0.698
0.938
1.146
1.394
1.484
1.744
1.944
1.975
2.000



0.241
0.124
0.104
0.337
0.587
0.942
1.085
1.553
1.958
2.025
2.080
2

 r
1.804
0.477
0.337
3.543
10.718
27.631
36.641
75.163
119.371
127.815
134.735
min
2
w Ž .x
Var y x r	
2.592
1.554
2.104
6.138
14.305
32.787
42.475
83.38
129.834
138.668
145.907
ˆ
x
0.037
0.152
0.352
0.515
0.660
0.835
0.899
1.085
1.227
1.249
1.265
1
x
0.103
0.255
0.422
0.531
0.618
0.716
0.749
0.845
0.916
0.927
0.936
2
x
0.087
0.235
0.431
0.577
0.705
0.858
0.912
1.074
1.197
1.217
1.232
3
Ž .
y x
12.796
16.021
21.365
26.229
31.086
37.640
40.197
48.332
55.147
56.272
57.176
ˆ
Ž
.
Source: Khuri and Myers
1979 . Reproduced with permission of the American Statistical
Association.


be 0.087. Hence, the value of
®
qx qxT x
should not grow much
0m
m
m
larger than 0.09 in the experimental region. Furthermore, r in equation
Ž
.
8.33
must not exceed the value 2, since most of the design points are
Ž .
contained inside a sphere of radius 2. The results of maximizing y x subject
ˆ
to this dual constraint are given in Table 8.4. For the sake of comparison, the
Ž
results of applying the standard procedure of ridge analysis that is, without
.
the additional constraint concerning ®
qx qxT x
are displayed in
0m
m
m
Table 8.5.
It is clear from Tables 8.4 and 8.5 that the extra constraint concerning
®
qx qxT x has profoundly improved the precision of y at the esti-
ˆ
0m
m
m
mated maxima. At a specified radius, the value of y obtained under standard
ˆ
ridge analysis is higher than the one obtained under modified ridge analysis.
However, the prediction variance values under the latter procedure are much
smaller, as can be seen from comparing Tables 8.4 and 8.5. While the
tradeoff that exists between a high response value and a small prediction
variance is a bit difficult to cope with from a decision making standpoint,
there is a clear superiority of the results displayed in Table 8.4. For example,
one would hardly choose any operating conditions in Table 8.5 that indicate
yG50, due to the accompanying large prediction variances. On the other
ˆ
w Ž .x
2
hand, Table 8.4 reveals that at radius rs2.000, ys46.26 with Var y x r	
ˆ
ˆ

RESPONSE SURFACE DESIGNS
355
s5.336, while a rival set of coordinates at rs1.744 for standard ridge
w Ž .x
2
analysis gives ys48.332 with Var y x r	 s83.38.
ˆ
ˆ
Row 3 of Table 8.5 gives values of 
 2r
, which should be compared
min
with the corresponding values in row 4 of the same table. One can easily see
2
w Ž .x
2
that in this example, 
 r
accounts for a large portion of Var y x r	 .
ˆ
min
8.4. RESPONSE SURFACE DESIGNS
We recall from Section 8.3 that one of the objectives of response surface
methodology is the selection of a response surface design according to a
certain optimality criterion. The design selection entails the specification of
the settings of a group of input variables that can be used as experimental
runs in a given experiment.
The proper choice of a response surface design can have a profound effect
on the success of a response surface exploration. To see this, let us suppose
that the fitted model is linear of the form
ysXq ,
8.36
Ž
.
where y is an n1 vector of observations, X is an np known matrix that
depends on the design settings,  is a vector of p unknown parameters, and
 is a vector of random errors in the elements of y. Typically,  is assumed to
Ž
2
.
2
have the normal distribution N 0, 	 I
, where 	
is unknown. In this case,
n
ˆ
the vector  is estimated by the least-squares estimator , which is given by
y1
ˆs XX
Xy.
8.37
Ž
.
Ž
.
If x , x , . . . , x
are the input variables for the model under consideration,
1
2
k
Ž
.
then the predicted response at a point xs x , x , . . . , x
 in a region of
1
2
k
interest R is written as
ˆ
y x sf x ,
8.38
Ž .
Ž .
Ž
.
ˆ
Ž .
where f x is a p1 vector whose first element is equal to one and whose
remaining py1 elements are functions of x , x , . . . , x . These functions are
1
2
k
in the form of powers and cross products of powers of the x ’s up to degree d.
i
In this case, the model is said to be of order d. At the uth experimental run,
Ž
.
Ž
x s x
, x
, . . . , x
 and the corresponding response value is
y
us
u
u1
u1
uk
u
.
w
x
1, 2, . . . , n . Then nk matrix Ds x : x :  :x
 is the design matrix. Thus
1
2
n
by a choice of design we mean the specification of the elements of D.
ˆ
Ž
.
If model 8.36 is correct, then  is an unbiased estimator of  and its
variancecovariance matrix is given by
y1
2
ˆ
Var  s XX
	 .
8.39
Ž
.
Ž
.
Ž .

OPTIMIZATION IN STATISTICS
356
Ž
.
Hence, from formula 8.38 , the prediction variance can be written as
y1
2
Var y x
s	 f x
XX
f x .
8.40
Ž .
Ž . Ž
.
Ž .
Ž
.
ˆ
w Ž .x
The design D is rotatable if Var y x
remains constant at all points that are
ˆ
equidistant from the design center, as we may recall from Section 8.3. The
input variables are coded so that the center of the design coincides with the
Ž
.
origin of the coordinates system see Khuri and Cornell, 1996, Section 2.8 .
8.4.1. First-Order Designs
Ž
.
Ž
.
If model 8.36 is of the first order that is, ds1 , then the matrix X is of the
w
x
w
form Xs 1 : D , where 1
is a vector of ones of order n1 see model
n
n
Ž
.x
8.8 . The input variables can be coded in such a way that the sum of the
elements in each column of D is equal to zero. Consequently, the prediction
Ž
.
variance in formula 8.40 can be written as
1
y1
2
Var y x
s	
qx DD
x .
8.41
Ž .
Ž
.
Ž
.
ˆ
n
Ž
.
Formula 8.41 clearly shows the dependence of the prediction variance on
the design matrix.
w Ž .x
A reasonable criterion for the choice of D is the minimization of Var y x ,
ˆ
Ž
.y1
or equivalently, the minimization of x DD
x within the region R. To
accomplish this we first note that for any x in the region R,
y1
y1
2
	 	 	
	
x DD
xF x
DD
,
8.42
Ž
.
Ž
.
Ž
.
2
2
	 	
Ž
.1r2
	Ž
.y1 	
w
k
k
Ž
i j.2x1r2
where
x
s xx
and
DD
s Ý
Ý
d
is the Euclidean
2
2
is1
js1
Ž
.y1
i j
Ž
.
Ž
.
norm of
DD
with d
being its
i, j th element
i, js1, 2, . . . , k . In-
Ž
.
equality
8.42 follows from applying Theorems 2.3.16 and 2.3.20. Thus by
	Ž
.y1 	
choosing the design D so that it minimizes
DD
, the quantity
2
Ž
.y1
x DD
x, and hence the prediction variance, can be reduced throughout
the region R.
	Ž
.y1 	
Theorem 8.4.1.
For a given number n of experimental runs,
DD
2
attains its minimum if the columns d , d , . . . , d
of D are such that d
 d s0,
1
2
k
i
j
ij, and d
 d is as large as possible inside the region R.
i
i
w
x
Proof. We have that Ds d : d :  : d
. The elements of d, are the n
1
2
k
Ž
.
design settings of the input variable x
is1, 2, . . . , k . Suppose that the
i
region R places the following restrictions on these settings:
d
 d Fc2,
is1, 2, . . . , k,
8.43
Ž
.
i
i
i

RESPONSE SURFACE DESIGNS
357
where c is some fixed constant. This means that the spread of the design in
i
2 Ž
.
the direction of the ith coordinate axis is bounded by c
is1, 2, . . . , k .
i

Now, if d
denotes the ith diagonal element of DD, then d sd
 d
ii
ii
i
i
Ž
.
is1, 2, . . . , k . Furthermore,
1
ii
d G
,
is1, 2, . . . , k,
8.44
Ž
.
dii
ii
Ž
.y1
where d
is the ith diagonal element of DD
.
Ž
.
Ž
.
To prove inequality 8.44 , let D be a matrix of order n ky1 obtained
i
Ž
.
from D by removing its ith column d
is1, 2, . . . , k . The cofactor of d
in
i
ii
Ž

.
DD is then det D D . Hence, from Section 2.3.3,
i
i
det D
D
Ž
.
i
i
ii
d s
,
is1, 2, . . . , k.
8.45
Ž
.
det DD
Ž
.
Ž
There exists an orthogonal matrix E of order kk whose determinant has
i
.
an absolute value of one such that the first column of DE
is d
and the
i
i
remaining columns are the same as those of D , that is,
i
w
x
DE s d : D ,
is1, 2, . . . , k.
i
i
i
w
x
It follows that see property 7 in Section 2.3.3
det DD sdet E
 DDE
Ž
.
Ž
.
i
i
y1





sdet D D
d d yd D D D
D d
.
Ž
.
Ž
.
i
i
i
i
i
i
i
i
i
i
Ž
.
Hence, from 8.45 we obtain
y1
y1



ii
d s d yd D D D
D d
,
is1, 2, . . . , k.
8.46
Ž
.
Ž
.
ii
i
i
i
i
i
i
Ž
.
Ž
.

Ž

.y1

Inequality 8.44 now follows from formula 8.46 , since d D D D
D d G0.
i
i
i
i
i
i
We can therefore write
1r2
1r2
k
k
1
2
y1
ii
DD
G
d
G
.
Ž
.
Ž
.
Ý
Ý
2
2
dii
is1
is1

OPTIMIZATION IN STATISTICS
358
Ž
.
Using the restrictions 8.43 we then have
1r2
k
1
y1
DD
G
.
Ž
.
Ý
2
4
ci
is1
Equality is achieved if the columns of D are orthogonal to one another and
2 Ž
.
ii
d sc
is1, 2, . . . , k . This follows from the fact that d s1rd
if and only
ii
i
ii

Ž
.
Ž
.
if d D s0 is1, 2, . . . , k , as can be seen from formula 8.46 .
i
i
Definition 8.4.1.
A design for fitting a fist-order model is said to be
orthogonal if its columns are orthogonal to one another.

ˆ
Ž
.
Corollary 8.4.1.
For a given number n of experimental runs, Var i
ˆ
attains a minimum if and only if the design is orthogonal, where  is the
i
Ž
.
least-squares estimator of  in model 8.8 , is1, 2, . . . , k.
i
ˆ
Ž
.
Proof. This follows directly from Theorem 8.4.1 and the fact that Var i
2
ii Ž
.
Ž
.
s	 d
is1, 2, . . . , k , as can be seen from formula 8.39 .

From Theorem 8.4.1 and Corollary 8.4.1 we conclude that an orthogonal
design for fitting a first-order model has optimal variance properties. An-
other advantage of orthogonal first-order designs is that the effects of the k
Ž
.
Ž
input variables in model 8.8 , as measured by the values of the  ’s
is
i
.
1, 2, . . . , k , can be estimated independently. This is because the off-diagonal
ˆ
Ž
.
elements of the variancecovariance matrix of  in formula 8.39 are zero.
ˆ
This means that the elements of  are uncorrelated and hence statistically
independent under the assumption of normality of the random error vector 
Ž
.
in model 8.36 .
Examples of first-order orthogonal designs are given in Khuri and Cornell
Ž
.
k
1996, Chapter 3 . Prominent among these designs are the 2
factorial design
Žeach input variable has two levels, and the number of all possible combina-
k.
tions of these levels is 2
and the PlackettBurman design, which was
Ž
.
introduced in Plackett and Burman 1946 . In the latter design, the number
of design points is equal to kq1, which must be a multiple of 4.
8.4.2. Second-Order Designs
Ž
.
These designs are used to fit second-order models of the form given by 8.12 .
Ž
.Ž
.
Since the number of parameters in this model is ps kq1 kq2 r2, the
Ž
.
number of experimental runs
or design points
in a second-order design
must at least be equal to p. The most frequently used second-order designs
k
Ž
include the 3 design each input variable has three levels, and the number of
k.
all possible combinations of these levels is 3
, the central composite design
Ž
.
CCD , and the BoxBehnken design.

RESPONSE SURFACE DESIGNS
359
Ž
.
The CCD was introduced by Box and Wilson 1951 . It is made up of a
factorial portion consisting of a 2 k factorial design, an axial portion of k
pairs of points with the ith pair consisting of two symmetric points on the ith
Ž
.
Ž
.
coordinate axis
is1, 2, . . . , k
at a distance of 
0
from the design
Ž
center
which coincides with the center of the coordinates system by the
.
Ž
.
coding scheme , and n
G1 center-point runs. The values of  and n can
0
0
Ž
be chosen so that the CCD acquires certain desirable features
see, for
.
1r4
example, Khuri and Cornell, 1996, Section 4.5.3 . In particular, if sF
,
where F denotes the number of points in the factorial portion, then the CCD
is rotatable. The choice of n
can affect the stability of the prediction
0
variance.
Ž
.
The BoxBehnken design, introduced in Box and Behnken
1960 , is a
subset of a 3k factorial design and, in general, requires many fewer points. It
also compares favorably with the CCD. A thorough description of this design
Ž
.
is given in Box and Draper 1987, Section 15.4 .
Other examples of second-order designs are given in Khuri and Cornell
Ž
.
1996, Chapter 4 .
8.4.3. Variance and Bias Design Criteria
We have seen that the minimization of the prediction variance represents an
important criterion for the selection of a response surface design. This
criterion, however, presumes that the fitted model is correct. There are many
situations in which bias in the predicted response can occur due to fitting the
wrong model. We refer to this as model bias.
Ž
.
Box and Draper 1959, 1963 presented convincing arguments in favor of
recognizing bias as an important design criterionin certain cases, even
more important than the variance criterion.
Ž
.
Consider again model
8.36 . The response
value at a point xs
Ž
.
x , x , . . . , x
 in a region R is represented as
1
2
k
y x sf x q ,
8.47
Ž .
Ž .
Ž
.
Ž .
Ž
.
Ž
.
where f x is the same as in model 8.38 . While it is hoped that model 8.47
is correct, there is always a fear that the true model is different. Let us
therefore suppose that in reality the true mean response at x, denoted by
Ž .
 x , is given by
 x sf x qg x 
8.48
Ž .
Ž .
Ž .
Ž
.
Ž .
where the elements of g x depend on x and consist of powers and cross
products of powers of x , x , . . . , x
of degree dd, with d being the order
1
2
k
Ž
.
of model 8.47 , and  is a vector of q unknown parameters. For a given

OPTIMIZATION IN STATISTICS
360
design D of n experimental runs, we then have the model
sXqZ,
Ž
.
where  is the vector of true means or expected values of the elements of y
Ž
.
at the n design points, X is the same as in model 8.36 , and Z is a matrix of
Ž
.

order nq whose uth row is equal to g x
. Here, x
denotes the uth row
u
u
Ž
.
of D us1, 2, . . . , n .
Ž
.
Ž .
Ž .
At each point x in R, the mean squared error MSE of y x , where y x is
ˆ
ˆ
Ž
.
the predicted response as given by formula 8.38 , is defined as
2
MSE y x
sE y x y x
.
Ž .
Ž .
Ž .
ˆ
ˆ
This can be expressed as
2
MSE y x
sVar y x
qBias
y x
,
8.49
Ž .
Ž .
Ž .
Ž
.
ˆ
ˆ
ˆ
w Ž .
w Ž .x
Ž .
where Bias y x sE y x y x . The fundamental philosophy of Box and
ˆ
ˆ
Ž
.
Draper 1959, 1963 is centered around the consideration of the integrated
Ž
.
Ž .
mean squared error IMSE of y x . This is denoted by J and is defined in
ˆ
terms of a k-tuple Riemann integral over the region R, namely,
n
Js
MSE y x
dx,
8.50
Ž .
Ž
.
ˆ
H
2
	
R
where y1 sH dx and 	 2
is the error variance. The partitioning of
R
w Ž .x
Ž
.
MSE y x
as in formula 8.49 enables us to separate J into two parts:
ˆ
n
n
2
Js
Var y x
dxq
Bias
y x
dxsVqB.
8.51
Ž .
Ž .
Ž
.
ˆ
ˆ
H
H
2
2
	
	
R
R
The quantities V and B are called the average variance and average squared
Ž .
bias of y x , respectively. Both V and B depend on the design D. Thus a
ˆ
Ž .
Ž .
reasonable choice of design is one that minimizes 1 V alone, 2 B alone, or
Ž .
3
JsVqB.
Ž
.
Now, using formula 8.40 , V can be written as
y1
Vsn
f x
XX
f x dx
Ž . Ž
.
Ž .
H
R
y1
str n XX

f x f x dx
Ž
.
Ž .
Ž .
H
½
5
R
y1
str n XX

,
8.52
Ž
.
Ž
.
11

RESPONSE SURFACE DESIGNS
361
where
 s
f x f x dx.
Ž .
Ž .
H
11
R
Ž
.
As for B, we note from formula 8.37 that
y1
ˆ
E  s XX
X
Ž
.
Ž .
sqA,
Ž
.y1
Ž
.
where As XX
XZ. Thus from formula 8.38 we have
E y x
sf x
qA .
Ž .
Ž . Ž
.
ˆ
Ž .
Ž
.
Using the expression for  x in formula 8.48 , B can be written as
n
2
Bs
f x qf x Ayf x yg x 
dx
Ž .
Ž .
Ž .
Ž .
H
2
	
R
n
2
s
f x Ayg x 
dx
Ž .
Ž .
H
2
	
R
n
s
 Af x yg x
f x Ayg x
 dx
Ž .
Ž .
Ž .
Ž .
H
2
	
R
n
s
 Af x f x Ayg x f x AyAf x g x qg x g x
 dx
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
2
	
R
n
s
,
8.53
Ž
.
2
	
where
sA Ay
 AyA q
,
11
12
12
22
 s
f x g x dx,
Ž .
Ž .
H
12
R

s
g x g x dx.
Ž .
Ž .
H
22
R
The matrices  ,  , and 
are called region moments. By adding and
11
12
22

y1
Ž
.
subtracting the matrix  

from  in formula
8.53 , B can be
12
11
12
expressed as
n

y1
y1
y1
Bs


y 

q Ay


Ay

.
8.54
Ž
.
Ž
.
Ž
.
Ž
.
22
12
11
12
11
12
11
11
12
2
	

OPTIMIZATION IN STATISTICS
362
We note that the design D affects only the second expression inside brackets
Ž
.
on the right-hand side of formula 8.54 . Thus to minimize B, the design D
should be chosen such that
Ayy1 s0.
8.55
Ž
.
11
12
Ž
.y1
Ž
.
Since As XX
XZ, a sufficient
but not necessary
condition for the
minimization of B is
M
s ,
M
s
,
8.56
Ž
.
11
11
12
12
Ž
.
Ž
.
where M
s 1rn XX, M
s 1rn XZ are the so-called design moments.
11
12
Thus a sufficient condition for the minimization of B is the equality of the
design moments, M
and M
, to the corresponding region moments, 
11
12
11
and  .
12
The minimization of JsVqB is not possible without the specification of
Ž
.
r	 . Box and Draper
1959, 1963
showed that unless V is considerably
larger than B, the optimal design that minimizes J has characteristics similar
to those of a design that minimizes just B.
Examples of designs that minimize V alone or B alone can be found in
Ž
.
Ž
.
Box and Draper
1987, Chapter 13 , Khuri and Cornell 1996, Chapter 6 ,
Ž
.
and Myers 1976, Chapter 9 .
8.5. ALPHABETIC OPTIMALITY OF DESIGNS
Ž
.
Let us again consider model 8.47 , which we now assume to be correct, that
Ž .
Ž .
is, the true mean response,  x , is equal to f x . In this case, the matrix
XX plays an important role in the determination of an optimal design, since
Ž
.y1
the elements of XX
are proportional to the variances and covariances of
w
Ž
.x
the least-squares estimators of the model’s parameters see formula 8.39 .
The mathematical theory of optimal designs, which was developed by
Ž
.
Kiefer 1958, 1959, 1960, 1961, 1962a, b , is concerned with the choice of de-
Ž
.y1
signs that minimize certain functions of the elements of XX
. The kernel
of Kiefer’s approach is based on the concept of design measure, which
represents a generalization of the traditional design concept. So far, each of
the designs that we have considered for fitting a response surface model has
Ž
.
consisted of a set of n points in a k-dimensional space kG1 . Suppose that
Ž
.
x , x , . . . , x
are distinct points of an n-point design
mFn with the lth
1
2
m
Ž
.
Ž
.
Ž
ls1, 2, . . . , m
point being replicated n
G1 times
that is, n
repeated
l
l
.
observations are taken at this point . The design can therefore be regarded as
a collection of points in a region of interest R with the lth point being
Ž
.
m
assigned the weight n rn ls1, 2, . . . , m , where nsÝ
n . Kiefer general-
l
ls1
l
ized this setup using the so-called continuous design measure, which is

ALPHABETIC OPTIMALITY OF DESIGNS
363
Ž .
basically a probability measure  x defined on R and satisfies the conditions
 x G0
for all xgR
and
d x s1.
Ž .
Ž .
H
R
In particular, the measure induced by a traditional design D with n points is
called a discrete design measure and is denoted by  . It should be noted that
n
while a discrete design measure is realizable in practice, the same is not true
of a general continuous design measure. For this reason, the former design is
called exact and the latter design is called approximate.
By definition, the moment matrix of a design measure  is a symmetric
Ž
.
w
Ž
.x
matrix of the form M  s m

, where
i j
m

s
f x f x d x .
8.57
Ž
.
Ž .
Ž .
Ž .
Ž
.
H
i j
i
j
R
Ž .
Ž .
Ž
.
Here, f x is the ith element of f x in formula 8.47 , is1, 2, . . . , p. For a
i
Ž
.
discrete design measure  , the i, j th element of the moment matrix is
n
m
1
m

s
n f x
f x
,
8.58
Ž
.
Ž
.
Ž
.
Ž
.
Ý
i j
n
l
i
l
j
l
n ls1
where m is the number of distinct design points and n is the number of
l
Ž
.
replications at the lth point
ls1, 2, . . . , m . In this special case, the matrix
Ž
.
Ž
.
M 
reduces to the usual moment matrix 1rn XX, where X is the same
Ž
.
matrix as in formula 8.36 .
For a general design measure , the standardized prediction variance,
Ž
.
denoted by d x,  , is defined as
y1
d x, 
sf x
M 
f x ,
8.59
Ž
.
Ž .
Ž
.
Ž .
Ž
.
Ž
.
where M 
is assumed to be nonsingular. In particular, for a discrete design
Ž
.
measure
 ,
the
prediction
variance
in
formula
8.40
is
equal
to
n
Ž
2
. Ž
.
	 rn d x,  .
n
Let H denote the class of all design measures defined on the region R.
A prominent design criterion that has received a great deal of attention is
Ž
.
that of D-optimality, in which the determinant of M 
is maximized. Thus a
design measure 
is D-optimal if
d
det M 
s sup det M 
.
8.60
Ž
.
Ž
.
Ž
.
d
gH
The rationale behind this criterion has to do with the minimization of the
ˆ
generalized variance of the least-squares estimator  of the parameter vector
ˆ
. By definition, the generalized variance of  is the same as the determi-
ˆ
nant of the variancecovariance matrix of . This is based on the fact that

OPTIMIZATION IN STATISTICS
364
Ž
.
under the normality assumption, the content volume of a fixed-level confi-
w
Ž
.xy1r2
dence region on  is proportional to det XX
. The review articles by
Ž
.
Ž
.
St. John and Draper
1975 , Ash and Hedayat
1978 , and Atkinson
Ž
.
1982, 1988 contain many references on D-optimality.
Another design criterion that is closely related to D-optimality is G-
optimality, which is concerned with the prediction variance. By definition, a
design measure 
is G-optimal if it minimizes over H the maximum
g
standardized prediction variance over the region R, that is,
sup d x, 
s inf
sup d x, 
.
8.61
Ž
.
Ž
.
Ž
.
½
5
g
gH
xgR
xgR
Ž
.
Kiefer and Wolfowitz 1960 showed that D-optimality and G-optimality, as
Ž
.
Ž
.
defined by formulas 8.60 and 8.61 , are equivalent. Furthermore, a design
Ž
.
measure  * is G-optimal or D-optimal if and only if
sup d x,  * sp,
8.62
Ž
.
Ž
.
xgR
Ž
.
where p is the number of parameters in the model. Formula 8.62 can be
conveniently used to determine if a given design measure is D-optimal, since
Ž
.
in general sup
d x,  Gp for any design measure gH. If equality can
x g R
be achieved by a design measure, then it must be G-optimal, and hence
D-optimal.
EXAMPLE 8.5.1.
Consider fitting a second-order model in one input
w
x
Ž
.
variable x over the region Rs y1, 1 . In this case, model 8.47 takes the
Ž .
2
Ž .
Ž
2.
form y x s q xq
x q, that is, f x s 1, x, x
. Suppose that the
0
1
11
design measure used is defined as
1 ,
xsy1, 0, 1,
3

x s
8.63
Ž .
Ž
.
½ 0
otherwise.
Thus  is a discrete design measure that assigns one-third of the experimen-
tal runs to each of the points y1, 0, and 1. This design measure is
D-optimal. To verify this claim, we first need to determine the values of the
1
Ž
.
Ž
.
elements of the moment matrix M  . Using formula 8.58 with n rns
l
3
2
2
for ls1, 2, 3, we find that m
s1, m
s0, m
s , m
s , m
s0, and
11
12
13
22
23
3
3
2
m
s . Hence,
33
3
2
1
0
3
2
0
0
M 
s
,
Ž
.
3
2
2
0
3
3
3
0
y3
3
y1
0
0
M

s
.
Ž
.
2
9
y3
0
2

ALPHABETIC OPTIMALITY OF DESIGNS
365
9
9
2
4
Ž
.
Ž
.
By applying formula 8.59 we find that d x,  s3y x q x ,y1FxF1.
2
2
Ž
.
w
x
Ž
.
We note that
d x,  F3 for all
x in
y1, 1
with
d 0,  s3. Thus
Ž
.
sup
d x,  s3. Since 3 is the number of parameters in the model, then
xg R
Ž
.
by condition 8.62 we conclude that the design measure defined by formula
Ž
.
8.63 is D-optimal.
In addition to the D- and G-optimality criteria, other variance-related
design criteria have also been investigated. These include A- and E-optimal-
ity. By definition, a design measure is A-optimal if it maximizes the trace of
Ž
.
M  . This is equivalent to minimizing the sum of the variances of the
least-squares estimators of the fitted model’s parameters. In E-optimality,
Ž
.
the smallest eigenvalue of M 
is maximized. The rationale behind this
criterion is based on the fact that
f x f x
Ž . Ž .
d x, 
F
,
Ž
.
min
Ž
.
as can be seen from formula 8.59 , where 
is the smallest eigenvalue of
min
Ž
.
Ž
.
M  . Hence, d x, 
can be reduced by maximizing 
.
min
The efficiency of a design measure gH with respect to a D-optimal
design is defined as
1rp
det M 
Ž
.
D-efficiencys
,
½
5
sup
det M 
Ž
.
 g H
where p is the number of parameters in the model. Similarly, the G-
efficiency of  is defined as
p
G-efficiencys
.
sup
d x, 
Ž
.
x g R
w
x
Both D- and G-efficiency values fall within the interval 0, 1 . The closer
these values are to 1, the more efficient their corresponding designs are.
Ž
.
Ž
Lucas 1976 compared several second-order designs such as central compos-
.
ite and BoxBehnken designs
on the basis of their D- and G-efficiency
values.
Ž
.
The equivalence theorem of Kiefer and Wolfowitz 1960 can be applied
to construct a D-optimal design using a sequential procedure. This proce-
Ž
.
dure is described in Wynn 1970, 1972 and goes as follows: Let D
denote an
n0
initial response surface design with n
points, x , x , . . . , x
, for which the
0
1
2
n0
matrix XX is nonsingular. A point x
is found in the region R such that
n q1
0
d x
, 
s sup d x, 
,
Ž
.
Ž
.
n q1
n
n
0
0
0
xgR

OPTIMIZATION IN STATISTICS
366
where 
is the discrete design measure that represents D . By augmenting
n
n
0
0
D
with x
we obtain the design D
. Then, another point x
is
n
n q1
n q1
n q2
0
0
0
0
chosen such that
d x
, 
s sup d x, 
,
Ž
.
Ž
.
n q2
n q1
n q1
0
0
0
xgR
where 
is the discrete design measure that represents D
. The point
n q1
n q1
0
0
x
is then added to D
to obtain the design D
. By continuing this
n q2
n q1
n q2
0
0
0
process
we
obtain
a
sequence
of
discrete
design
measures,
namely,
Ž
.
 , 
, 
, . . . . Wynn 1970 showed that this sequence converges to the
n
n q1
n q2
0
0
0
D-optimal design  , that is,
d
det M 
™det M 
Ž
.
Ž
.
n qn
d
0
Ž
.
as n™. An example is given in Wynn 1970, Section 5 to illustrate this
sequential procedure.
The four design criteria, A-, D-, E-, and G-optimality, are referred to as
alphabetic optimality. More detailed information about these criteria can be
Ž
.
Ž
.
Ž
.
found in Atkinson 1982, 1988 , Fedorov 1972 , Pazman 1986 , and Silvey
Ž
.
1980 . Recall that to perform an actual experiment, one must use a discrete
design. It is possible to find a discrete design measure 
that approximates
n
an optimal design measure. The approximation is good whenever n is large
Ž
.
with respect to p the number of parameters in the model .
Ž
.
Note that the equivalence theorem of Kiefer and Wolfowitz 1960 applies
to general design measures and not necessarily to discrete design measures,
that is, D- and G-optimality criteria are not equivalent for the class of
discrete design measures. Optimal n-point discrete designs, however, can still
be found on the basis of maximizing the determinant of XX, for example. In
this case, finding an optimal n-point design requires a search involving nk
variables, where k is the number of input variables. Several algorithms have
been introduced for this purpose. For example, the DETMAX algorithm by
Ž
.
Ž
.
Mitchell
1974
is used to maximize det XX . A review of algorithms for
constructing optimal discrete designs can be found in Cook and Nachtsheim
Ž
. Ž
.
1980
see also Johnson and Nachtsheim, 1983 .
One important criticism of the alphabetic optimality approach is that it is
set within a rigid framework governed by a set of assumptions. For example,
a specific model for the response function must be assumed as the ‘‘true’’
model. Optimal design measures can be quite sensitive to this assumption.
Ž
.
Box
1982
presented a critique to this approach. He argued that in a
response surface situation, it may not be realistic to assume that a model
Ž
.
such as 8.47 represents the true response function exactly. Some protection
against bias in the model should therefore be considered when choosing a
Ž
.
response surface design. On the other hand, Kiefer 1975 criticized certain
aspects of the preoccupation with bias, pointing out examples in which the
variance criterion is compromised for the sake of the bias criterion. It follows

DESIGNS FOR NONLINEAR MODELS
367
Ž
that design selection should be guided by more than one single criterion see
.
Kiefer, 1975, page 286; Box, 1982, Section 7 . A reasonable approach is to
Ž
select compromise designs that are sufficiently good
but not necessarily
.
optimal from the viewpoint of several criteria that are important to the user.
8.6. DESIGNS FOR NONLINEAR MODELS
The models we have considered so far in the area of response surface
methodology were linear in the parameters; hence the term linear models.
There are, however, many experimental situations in which linear models do
not adequately represent the true mean response. For example, the growth
of an organism is more appropriately depicted by a nonlinear model. By
definition, a nonlinear model is one of the form
y x sh x,  q ,
8.64
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
where xs x , x , . . . , x
 is a vector of k input variables, s  ,  , . . . ,  
1
2
k
1
2
p
Ž
.
is a vector of p unknown parameters,  is a random error, and h x,  is a
known function, nonlinear in at least one element of . An example of a
nonlinear model is
 x
1
h x,  s
.
Ž
.
 qx
2
Ž
.
Here, s  , 
 and 
is a nonlinear parameter. This particular model is
1
2
2
known as the MichaelisMenten model for enzyme kinetics. It relates the
initial velocity of an enzymatic reaction to the substrate concentration x.
In contrast to linear models, nonlinear models have not received a great
deal of attention in response surface methodology, especially in the design
area. The main design criterion for nonlinear models is the D-optimality
criterion, which actually applies to a linearized form of the nonlinear model.
More specifically, this criterion depends on the assumption that in some
Ž
.
neighborhood of a specified value 
of , the function h x, 
is approxi-
0
Ž
.
mately linear in . In this case, a first-order Taylor’s expansion of h x, 
Ž
.
yields the following approximation of h x,  :
p
"h x, 
Ž
.
0
h x,  fh x, 
q
 y
.
Ž
.
Ž
.
Ž
.
Ý
0
i
i0
"i
is1
Thus if  is close enough to  , then we have approximately the linear model
0
p
"h x, 
Ž
.
0
z x s

q ,
8.65
Ž .
Ž
.
Ý
i
"i
is1
Ž .
Ž .
Ž
.
Ž
where z x sy x yh x,  , and 
is the ith element of 	sy
is
0
i
0
.
1, 2, . . . , p .

OPTIMIZATION IN STATISTICS
368
Ž
.
For a given design consisting of n experimental runs, model 8.65 can be
written in vector form as
zsH 
	q ,
8.66
Ž
.
Ž
.
0
Ž
.
Ž
.
Ž
.
where H 
is an np matrix whose u, i th element is "h x ,  r" with
0
u
0
i
x
being the vector of design settings for the k input variables at the uth
u
Ž
.
experimental run
is1, 2, . . . , p; us1, 2, . . . , n . Using the linearized form
Ž
.
given by model
8.66 , a design is chosen to maximize the determinant
w
Ž
. Ž
.x
Ž
det H  H 
. This is known as the BoxLucas criterion
see Box and
0
0
.
Lucas, 1959 .
It can be easily seen that a nonlinear design obtained on the basis of the
BoxLucas criterion depends on the value of  . This is an undesirable
0
characteristic of nonlinear models, since a design is supposed to be used for
estimating the unknown parameter vector . By contrast, designs for linear
models are not dependent on the fitted model’s parameters. Several proce-
dures have been proposed for dealing with the problem of design depen-
dence on the parameters of a nonlinear model. These procedures are
Ž
.
mentioned in the review article by Myers, Khuri, and Carter 1989 . See also
Ž
.
Khuri and Cornell 1996, Section 10.5 .
EXAMPLE 8.6.1.
Let us again consider the MichaelisMenten model
Ž
.
mentioned earlier. The partial derivatives of h x,  with respect to 
and
1

are
2
"h x, 
x
Ž
.
s
,
"
 qx
1
2
"h x, 
y x
Ž
.
1
s
.
2
"
 qx
Ž
.
2
2
Suppose that it is desired to find a two-point design that consists of the
settings x and x
using the BoxLucas criterion. In this case,
1
2
"h x , 
"h x , 
Ž
.
Ž
.
1
0
1
0
"
"
1
2
H 
s
Ž
.
0
"h x , 
"h x , 
Ž
.
Ž
.
2
0
2
0
"
"
1
2
x
y
x
1
10
1
2

qx

qx
Ž
.
20
1
20
1
s
,
x
y
x
2
10
2
2

qx

qx
Ž
.
20
2
20
2

DESIGNS FOR NONLINEAR MODELS
369
Ž
.
where 
and 
are the elements of  . In this example, H 
is a square
10
20
0
0
matrix. Hence,
2
det H 
H 
s det H 

4
Ž
.
Ž
.
Ž
.
0
0
0
2
2
2
2

x x
x yx
Ž
.
10
1
2
2
1
s
.
8.67
Ž
.
4
4

qx

qx
Ž
. Ž
.
20
1
20
2
To determine the maximum of this determinant, let us first equate its partial
derivatives with respect to x
and x
to zero. It can be verified that the
1
2
Ž
.
solution of the resulting equations that is, the stationary point falls outside
Ž
the region of feasible values for x
and
x
both
x
and
x
must be
1
2
1
2
.
nonnegative . Let us therefore restrict our search for the maximum within
Ž
.
4
the region Rs
x , x
0Fx Fx
, 0Fx Fx
, where x
is the maxi-
1
2
1
max
2
max
max
mum allowable substrate concentration. Since the partial derivatives of the
Ž
.
determinant in formula 8.67 do not vanish in R, then its maximum must be
attained on the boundary of R. On x s0, or x s0, the value of the
1
2
determinant is zero. If x sx
, then
1
max
2
2
2
2

x
x
x yx
Ž
.
10
max
2
2
max
det H 
H 
s
.
Ž
.
Ž
.
0
0
4
4

qx

qx
Ž
. Ž
.
20
max
20
2
It can be verified that this function of x
has a maximum at the point
2
Ž
.
x s
x
r 2
qx
with a value given by
2
20
max
20
max
 2 x6
10
max
max
det H 
H 
s
.
8.68

4
Ž
.
Ž
.
Ž
.
0
0
6
2
x sx
16

qx
1
max
Ž
.
20
20
max
Similarly, if x sx
, then
2
max
2
2
2
2

x
x
x
yx
Ž
.
10
max
1
max
1
det H 
H 
s
,
Ž
.
Ž
.
0
0
4
4

qx

qx
Ž
. Ž
.
20
1
20
max
Ž
.
which attains the same maximum value as in formula
8.68
at the point
Ž
.
x s 
x
r 2
q x
.
We
conclude
that
the
maximum
of
1
20
max
20
max
w
Ž
. Ž
.x
det H  H 
over the region R is achieved when x sx
and x s
0
0
1
max
2
Ž
.
Ž
.

x
r 2
qx
, or when x s
x
r 2
qx
and x sx
.
20
max
20
max
1
20
max
20
max
2
max
We can clearly see in this example the dependence of the design settings
on  , but not on  . This is attributed to the fact that 
appears linearly in
2
1
1
the model, but 
does not. In this case, the model is said to be partially
2
nonlinear. Its D-optimal design depends only on those parameters that do
not appear linearly. More details concerning partially nonlinear models can
Ž
.
be found in Khuri and Cornell 1996, Section 10.5.3 .

OPTIMIZATION IN STATISTICS
370
8.7. MULTIRESPONSE OPTIMIZATION
By definition, a multiresponse experiment is one in which a number of
responses can be measured for each setting of a group of input variables. For
example, in a skim milk extrusion process, the responses,
y spercent
1
residual lactose and y spercent ash, are known to depend on the input
2
variables, x spH level, x stemperature, x sconcentration, and x stime
1
2
3
4
Ž
.
see Fichtali, Van De Voort, and Khuri, 1990 .
As in single-response experiments, one of the objectives of a multire-
sponse experiment is the determination of conditions on the input variables
that optimize the predicted responses. The definition of an optimum in a
multiresponse situation, however, is more complex than in the single-
response case. The reason for this is that when two or more response
variables are considered simultaneously, the meaning of an optimum be-
comes unclear, since there is no unique way to order the values of a
Ž
.
multiresponse function. To overcome this difficulty, Khuri and Conlon 1981
introduced a multiresponse optimization technique called the generalized
distance approach. The following is an outline of this approach:
Let r be the number of responses, and n be the number of experimental
runs for all the responses. Suppose that these responses can be represented
by the linear models
y sX q ,
is1, 2, . . . , r,
i
i
i
where y is a vector of observations on the ith response, X is a known matrix
i
of order np and rank p,  is a vector of p unknown parameters, and  is
i
i
Ž
.
a random error vector associated with the ith response
is1, 2, . . . , r . It is
w
x
assumed that the rows of the error matrix  :  :  : 
are statistically
1
2
r
independent
with
each
having
a
zero
mean
vector
and
a
common
variancecovariance matrix . Note that the matrix X is assumed to be the
same for all the responses.
Let x , x , . . . , x
be input variables that influence the r responses. The
1
2
k
Ž
.
predicted response value at a point xs x , x , . . . , x
 in a region R for the
1
2
k
ˆ
ˆ
y1
Ž .
Ž .
Ž
.
ith response is given by y x sf x  , where  s XX
Xy is the least-
ˆi
i
i
i
Ž
.
Ž .
squares estimator of 
is1, 2, . . . , r . Here, f x is of the same form as a
i
row of X, except that it is evaluated at the point x. It follows that
y1
Var y x
s	 f x
XX
f x ,
is1, 2, . . . , r,
Ž .
Ž . Ž
.
Ž .
ˆi
ii
y1
Cov y x , y x
s	 f x
XX
f x ,
ijs1, 2, . . . , r,
Ž .
Ž .
Ž . Ž
.
Ž .
ˆ
ˆ
i
j
i j
Ž
.
where 	
is the
i, j th element of . The variancecovariance matrix of
i j
Ž .
w
Ž .
Ž .
Ž .x
y x s y x , y x , . . . , y x  is then of the form
ˆ
ˆ
ˆ
ˆ
1
2
r
y1
Var y x
sf x
XX
f x .
Ž .
Ž . Ž
.
Ž .
ˆ

MULTIRESPONSE OPTIMIZATION
371
ˆ
Since  is in general unknown, an unbiased estimator, , of  can be used
instead, where
1
y1
ˆs
Y I yX XX
X Y,
Ž
.
n
nyp
ˆ
w
x
and Ys y : y :  : y . The matrix  is nonsingular provided that Y is of
1
2
r
w Ž .x
rank rFnyp. An estimate of Var y x
is then given by
ˆ
$
y1
ˆ
Var y x
sf x
XX
f x .
8.69
Ž .
Ž . Ž
.
Ž .
Ž
.
ˆ
Ž .
Let  denote the optimum value of y x optimized individually over the
ˆ
i
i
Ž
.
Ž
.
region R
is1, 2, . . . , r . Let s  ,  , . . . ,  . These individual optima
1
2
r
do not in general occur at the same location in R. To achieve a compromise
w Ž .
x
optimum, we need to find x that minimizes  y x ,  , where  is some
ˆ
Ž .
metric that measures the distance of y x from . One possible choice for 
ˆ
is the metric
1r2
$
y1
 y x ,  s
y x y  Var y x
y x y
,
Ž .
Ž .
Ž .
Ž .
ˆ
ˆ
ˆ
ˆ

4
Ž
.
which, by formula 8.69 , can be written as
1r2
y1
ˆ
y x y 
y x y
Ž .
Ž .
ˆ
ˆ
 y x ,  s
.
8.70
Ž .
Ž
.
ˆ
y1
f x
XX
f x
Ž . Ž
.
Ž .
Ž .
We note that s0 if and only if y x s, that is, when all the responses
ˆ
attain their individual optima at the same point; otherwise, 0. Such a
Ž
.
point if it exists
is called a point of ideal optimum. In general, an ideal
optimum rarely exists.
In order to have conditions that are as close as possible to an ideal
optimum, we need to minimize  over the region R. Let us suppose that the
minimum occurs at the point x gR. Then, at x the experimental conditions
0
0
can be described as being near optimal for each of the r response functions.
We therefore refer to x
as a point of compromise optimum.
0
Ž
.
Note that the elements of  in formula 8.70 are random variables since
Ž .
Ž .
Ž .
they are the individual optima of
y x , y x , . . . , y x . If the variation
ˆ
ˆ
ˆ
1
2
r
associated with  is large, then the metric  may not accurately measure the
Ž .
deviation of y x from the true ideal optimum. In this case, some account
ˆ
should be taken of the randomness of  in the development of the metric.
Ž
.
To do so, let s  ,  , . . . ,  , where 
is the optimum value of the true
1
2
r
i
Ž
mean of the ith response optimized individually over the region R
is
.
1, 2, . . . , r . Let D
be a confidence region for . For a fixed xgR and

whenever gD , we obviously have

 y x ,  F max  y x ,  .
8.71
Ž .
Ž .
Ž
.
ˆ
ˆ
gD

OPTIMIZATION IN STATISTICS
372
w Ž .
x
The right-hand side of this inequality serves as an upper bound on  y x ,  ,
ˆ
Ž .
which represents the distance of y x from the true ideal optimum. It follows
ˆ
that
min  y x ,  F min
max  y x , 
.
8.72
Ž .
Ž .
Ž
.
ˆ
ˆ
½
5
xgR
xgR
gD
The right-hand side of this inequality provides a conservative measure of
distance between the compromise and ideal optima.
The confidence region D can be determined in a variety of ways. Khuri

Ž
.
and Conlon 1981 considered a rectangular confidence region of the form
 F F
,
is1, 2, . . . , r,
1i
i
2 i
where
 s yg

MS1r2 t
,
Ž
.
1i
i
i
i
i
 r2, nyp
8.73
Ž
.
1r2

s qg

MS
t
,
Ž
.
2 i
i
i
i
i
 r2, nyp
where MS is the error mean square for the ith response,  is the point at
i
i
Ž .
Ž
.
which y x attains the individual optimum  , t
is the upper r2 
ˆi
i
 r2, nyp
100th percentile of the t-distribution with nyp degrees of freedom, and
Ž
.
g 
is given by
i
i
1r2
y1
g

s f 
XX
f 
,
is1, 2, . . . , r.
Ž
.
Ž
. Ž
.
Ž
.
i
i
i
i
Ž
.
Khuri and Conlon 1981 showed that such a rectangular confidence region
has approximately a confidence coefficient of at least 1y*, where *s1
Ž
.r
y 1y .
It should be noted that the evaluation of the right-hand side of inequality
Ž
.
w Ž .
x
8.72 requires that  y x ,  be maximized first with respect to  over D
ˆ

for a given xgR. The maximum value thus obtained, being a function of x, is
then minimized over the region R. A computer program for the implementa-
Ž
.
tion of this minmax procedure is described in Conlon and Khuri
1992 .
A complete electronic copy of the code, along with examples, can be
downloaded from the Internet at ftp:rrftp.stat.ufl.edurpubrmr.tar.Z.
Numerical examples that illustrate the application of the generalized
distance approach for multiresponse optimization can be found in Khuri and
Ž
.
Ž
.
Conlon 1981 and Khuri and Cornell 1996, Chapter 7 .
8.8. MAXIMUM LIKELIHOOD ESTIMATION
AND THE EM ALGORITHM
Ž
.
We recall from Section 7.11.2 that the maximum likelihood ML estimates of
a set of parameters,  ,  , . . . ,  , for a given distribution maximize the
1
2
p

MAXIMUM LIKELIHOOD ESTIMATION AND THE EM ALGORITHM
373
likelihood function of a sample, X , X , . . . , X , of size n from the distribu-
1
2
nˆ ˆ
ˆ
tion. The ML estimates of the  ’s denoted by  ,  , . . . ,  , can be found by
i
1
2
p
Ž
solving the likelihood equations
the likelihood function must be differen-
.
tiable and unimodal
ˆ
" log L x, 
Ž
.
s0,
is1, 2, . . . , p,
8.74
Ž
.
"i
ˆ
ˆ ˆ
ˆ
Ž
.
Ž
.
Ž
.
Ž
.
where s  ,  , . . . ,  , xs x , x , . . . , x
, and
L x,  sf x, 
with
1
2
p
1
2
n
Ž
.
Ž
.
f x, 
being the density function
or probability mass function
of Xs
Ž
.
Ž
.
n
Ž
.
X , X , . . . , X . Note that f x, 
can be written as Ł
g x ,  , where
1
2
n
is1
i
Ž
.
Ž
.
g x,  is the density function or probability mass function associated with
the distribution.
Ž
.
Equations
8.74
may not have a closed-form solution. For example,
consider the so-called truncated Poisson distribution whose probability mass
Ž
.
function is of the form see Everitt, 1987, page 29
ey x
g x,  s
,
xs1, 2, . . . .
8.75
Ž
.
Ž
.
y
1ye
x!
Ž
.
In this case,
n
log L x,  slog
g x , 
Ž
.
Ž
.
Ł
i
is1
n
n
y
synq log 
x y
log x !yn log 1ye
.
Ž
.
Ž
.
Ý
Ý
i
i
is1
is1
Hence,
n
y
"L* x, 
1
ne
Ž
.
synq
x y
,
8.76
Ž
.
Ý
i
y
"

1ye
is1
Ž
.
Ž
.
where L* x,  slog L x, 
is the log-likelihood function. The likelihood
Ž
.
equation, which results from equating the right-hand side of formula 8.76 to
zero, has no closed-form solution for .
Ž
.
In general, if equations 8.74 do not have a closed-form solution, then, as
Ž
.
was seen in Section 8.1, iterative methods can be applied to maximize L x, 
w
Ž
.x
Ž
or L* x,  . Using, for example, the NewtonRaphson method see Section
ˆ
ˆ
.
8.1.2 , if 
is an initial estimate of  and 
is the estimate at the ith
0
i
Ž
.
iteration, then by applying formula 8.7 we have
ˆ
ˆ
y1
ˆ
ˆ

s yH
x,  
L* x, 
,
is0, 1,2, . . . ,
Ž
.
Ž
.
iq1
i
L*
i
i

OPTIMIZATION IN STATISTICS
374
Ž
.
Ž
.
where H
x, 
and 
L* x, 
are, respectively, the Hessian matrix and
L*
gradient vector of the log-likelihood function. Several iterations can be made
until a certain convergence criterion is satisfied. A modification of this
procedure is the so-called Fisher’s method of scoring, where H
is replaced
L*
by its expected value, that is,
y1
ˆ
ˆ
ˆ
ˆ

s y E H
x, 

L* x, 
,
is0, 1, 2, . . . .
8.77
Ž
.
Ž
.
Ž
.
½
5
iq1
i
L*
i
i
Here, the expected value is taken with respect to the given distribution.
Ž
.
EXAMPLE 8.8.1.
Everitt, 1987, pages 3031 . Consider the truncated
Ž
.
Poisson distribution described in formula 8.75 . In this case, since we only
Ž
.
Ž
.
have one parameter , the gradient takes the form 
L* x,  s"L* x,  r",
Ž
.
which is given by formula 8.76 . Hence, the Hessian matrix is
" 2L* x, 
Ž
.
H
x,  s
Ž
.
L*
2
"
n
y
1
ne
sy
x q
.
Ý
i
2
2
y

1ye
Ž
.
is1
Furthermore, if X denotes the truncated Poisson random variable, then

y
x
xe

E X s
Ž
.
Ý
y
1ye
x!
Ž
.
xs1

s
.
y
1ye
Thus
2
" L* x, 
Ž
.
E H
x, 
sE
Ž
.
L*
2
"
1
n
ney
sy
q
2
y
2
y

1ye
1ye
Ž
.
ney 1q yn
Ž
.
s
.
2
y
 1ye
Ž
.
Suppose now we have the sample 1, 2, 3, 4, 5, 6 from this distribution. Let
ˆ s1.5118 be an initial estimate of . Several iterations are made by
0
Ž
.
applying formula 8.77 , and the results are shown in Table 8.6. The final

MAXIMUM LIKELIHOOD ESTIMATION AND THE EM ALGORITHM
375
Table 8.6. Fisher’s Method of Scoring for the Truncated Poisson Distribution
2
"L*
" L*
Iteration

L*
2
"
"
1
y685.5137
y1176.7632
1.5118
y1545.5549
2
y62.0889
y1696.2834
0.9293
y1303.3340
3
y0.2822
y1750.5906
0.8927
y1302.1790
4
0.0012
y1750.8389
0.8925
y1302.1792
Ž
.
Source: Everitt 1987, page 31 . Reproduced with permission of Chapman and Hall, London.
estimate of  is 0.8925, which is considered to be the maximum likelihood
estimate of  for the given sample. The convergence criterion used here is
ˆ
ˆ



y 0.001.
iq1
i
8.8.1. The EM Algorithm
The EM algorithm is a general iterative procedure for maximum likelihood
estimation in incomplete data problems. This encompasses situations involv-
ing missing data, or when the actual data are viewed as forming a subset of a
larger system of quantities.
Ž
.
The term EM was introduced by Dempster, Laird, and Rubin 1977 . The
reason for this terminology is that each iteration in this algorithm consists of
Ž
.
two steps called the expectation step
E-step
and the maximization step
Ž
.
M-step . In the E-step, the conditional expectations of the missing data are
found given the observed data and the current estimates of the parameters.
These expected values are then substituted for the missing data and used to
complete the data. In the M-step, maximum likelihood estimation of the
parameters is performed in the usual manner using the completed data.
More generally, missing sufficient statistics can be estimated rather than the
individual missing data. The estimated parameters are then used to reesti-
Ž
.
mate the missing data or missing sufficient statistics , which in turn lead to
new parameter estimates. This defines an iterative procedure, which can be
carried out until convergence is achieved.
More details concerning the theory of the EM algorithm can be found in
Ž
.
Ž
Dempster, Laird, and Rubin 1977 , and in Little and Rubin 1987, Chapter
.
7 . The following two examples, given in the latter reference, illustrate the
application of this algorithm:
Ž
.
EXAMPLE 8.8.2.
Little and Rubin, 1987, pages 130131 . Consider a
sample of size n from a normal distribution with a mean  and a variance
	 2.
Suppose
that
x , x , . . . , x
are
observed
data
and
that
1
2
m
Ž
.
X
, X
, . . . , X
are missing data. Let x
s x , x , . . . , x
. For ism
mq1
mq2
n
obs
1
2
m
Ž
2.
q1, mq2, . . . , n, the expected value of X given x
and s , 	
 is .
i
obs
Now, from Example 7.11.3, the log-likelihood function for the complete data

OPTIMIZATION IN STATISTICS
376
set is
n
n
n
1
2
2
2
L* x,  sy
log 2	
y
x y2
x qn
,
8.78
Ž
.
Ž
.
Ž
.
Ý
Ý
i
i
2 ž
/
2
2	
is1
is1
Ž
.
n
2
n
where xs x , x , . . . , x
. We note that Ý
X
and Ý
X are sufficient
1
2
n
is1
i
is1
i
statistics. Therefore, to apply the E-step of the algorithm, we only have to
find the conditional expectations of these statistics given x
and the current
obs
estimate of . We thus have
n
m
ˆ

E
X  , x
s
x q nym  ,
js0, 1, 2, . . . ,
8.79
Ž
.
Ž
.
ˆ
Ý
Ý
i
j
obs
i
j
ž
/
is1
is1
n
m
2
2
2
2
ˆ

E
X
 , x
s
x q nym
 q	
,
js0, 1, 2, . . . ,
8.80
Ž
.
Ž
.
ˆ
ˆ
Ý
Ý
ž
/
i
j
obs
i
j
j
ž
/
is1
is1
ˆ
2
ˆ
Ž
.
where  s  , 	
 is the estimate of  at the jth iteration with  being an
ˆ
ˆ
j
j
j
0
initial estimate.
From Section 7.11 we recall that the maximum likelihood estimates of 
2
Ž
.
n
Ž
.
n
2
and 	
based on the complete data set are 1rn Ý
x and 1rn Ý
x y
is1
i
is1
i
wŽ
.
n
x2
1rn Ý
x
. Thus in the M-step, these same expressions are used, except
is1
i
Ž
.
that the current expectations of the sufficient statistics in formulas 8.79 and
Ž
.
8.80 are substituted for the missing data portion of the sufficient statistics.
2
Ž
.
In other words, the estimates of  and 	
at the
jq1 th iteration are given
by
m
1

s
x q nym 
,
js0, 1,2, . . . ,
8.81
Ž
.
Ž
.
ˆ
ˆ
Ý
jq1
i
j
n
is1
m
1
2
2
2
2
2
	
s
x q nym
 q	
y
,
js0, 1, 2, . . . .
8.82
Ž
.
Ž
.
ˆ
ˆ
ˆ
ˆ
Ý
ž
/
jq1
i
j
j
jq1
n
is1
Ž
.
Ž
.
By setting  s
s and 	 s	
s	 in equations 8.81 and 8.82 ,
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
j
jq1
j
jq1
we find that the iterations converge to
m
1
s
x ,
ˆ
Ý
i
m is1
m
1
2
2
2
	 s
x y ,
ˆ
ˆ
Ý
i
m is1
which are the maximum likelihood estimates of  and 	 2 from x
.
obs

MAXIMUM LIKELIHOOD ESTIMATION AND THE EM ALGORITHM
377
The EM algorithm is unnecessary in this example, since the maximum
likelihood estimates of  and 	 2 can be obtained explicitly.
Ž
.
EXAMPLE 8.8.3.
Little and Rubin, 1987, pages 131132 . This example
Ž
.
was originally given in Dempster, Laird, and Rubin
1977 . It involves a
1
1
1
1
1
Ž
.
Ž
.
multinomial xs x , x , x , x
 with cell probabilities
y , , ,
,
1
2
3
4
2
2
4
4
2
Ž
.
where 0FF1. Suppose that the observed data consist of x
s 38, 34, 125 
obs
such that
x s38, x s34, x qx s125. The likelihood function for the
1
2
3
4
complete data is
x qx qx qx
!
Ž
.
x
x
x
x
1
2
3
4
1
2
3
4
1
1
1
1
1
L x,  s
y 


.
Ž
.
Ž
. Ž
. Ž
. Ž .
2
2
4
4
2
x !x !x !x !
1
2
3
4
The log-likelihood function is of the form
x qx qx qx
!
Ž
.
1
2
3
4
1
1
L* x,  slog
qx log
y 
Ž
.
Ž
.
1
2
2
x !x !x !x !
1
2
3
4
1
1
1
qx log
 qx log
 qx log
.
Ž
.
Ž
.
Ž .
2
3
4
4
4
2
Ž
.
By differentiating L* x, 
with respect to  and equating the derivative to
zero we obtain
x
x
x
1
2
3
y
q
q
s0.
1y


Hence, the maximum likelihood estimate of  for the complete data set is
x qx
2
3
ˆs
.
8.83
Ž
.
x qx qx
1
2
3
Let us now find the conditional expectations of X , X , X , X
given the
1
2
3
4
observed data and the current estimate of :
ˆ

E X  , x
s38,
Ž
.
1
i
obs
ˆ

E X
 , x
s34,
Ž
.
2
i
obs
1 ˆ
125

Ž
.
i
4
ˆ

E X
 , x
s
,
Ž
.
3
i
obs
1
1 ˆ
q i
2
4
1
125Ž .
2
ˆ

E X  , x
s
.
Ž
.
4
i
obs
1
1 ˆ
q i
2
4

OPTIMIZATION IN STATISTICS
378
Table 8.7. The EM Algorithm for Example 8.8.3
ˆ
Iteration

0
0.500000000
1
0.608247423
2
0.624321051
3
0.626488879
4
0.626777323
5
0.626815632
6
0.626820719
7
0.626821395
8
0.626821484
Ž
.
Source: Little and Rubin 1987, page 132 . Reproduced
with permission of John Wiley & Sons, Inc.
Ž
.
Thus at the iq1 st iteration we have
1
1
1
ˆ
ˆ
34q 125
 r
q 
Ž
.Ž
. Ž
.
i
i
4
2
4
ˆ
s
,
8.84
Ž
.
iq1
1
1
1
ˆ
ˆ
38q34q 125
 r
q 
Ž
.Ž
. Ž
.
i
i
4
2
4
Ž
.
as can be seen from applying formula 8.83 using the conditional expectation
Ž
.
of X
instead of x . Formula
8.84
can be used iteratively to obtain the
3
3
maximum likelihood estimate of  on the basis of the observed data. Using
1
ˆ
an initial estimate  s , the results of this iterative procedure are given in
0
2
ˆ
ˆ
ˆ
Ž
.
Table 8.7. Note that if we set 
s s in formula 8.84 we obtain the
iq1
i
quadratic equation,
ˆ2
ˆ
197 y15y68s0
ˆ
whose only positive root is s0.626821498, which is very close to the value
obtained in the last iteration in Table 8.7.
8.9. MINIMUM NORM QUADRATIC UNBAISED ESTIMATION
OF VARIANCE COMPONENTS
Consider the linear model
c
ysXq
U  ,
8.85
Ž
.
Ý
i
i
is1
where y is a vector of n observations;  is a vector of fixed effects;
 ,  , . . . , 
are vectors of random effects; X, U , U , . . . , U
are known
1
2
c
1
2
c
matrices of constants with  s, the vector of random errors; and U sI .
c
c
n
We assume that the  ’s are uncorrelated with zero mean vectors and
i
variancecovariance matrices 	 2I
, where m is the number of columns of
i
m
i
i

MINIMUM NORM QUADRATIC UNBIASED ESTIMATION OF VARIANCE
379
Ž
.
2
2
2
U
is1, 2, . . . , c . The variances 	 , 	 , . . . , 	
are referred to as variance
i
1
2
c
Ž
.
components. Model 8.85 can be written as
ysXqU,
8.86
Ž
.
w
x
Ž



 .
Ž
.
where Us U : U :  : U , s  ,  , . . . ,  . From model 8.86 we have
1
2
c
1
2
c
E y sX ,
Ž .
c
8.87
Ž
.
2
Var y s
	 V ,
Ž .
Ý
i
i
is1
with V sU U
.
i
i
i
Let us consider the estimation of a linear function of the variance
components, namely, Ýc
a 	 2, where the a ’s are known constants, by a
is1
i
i
i
quadratic estimator of the form yAy. Here, A is a symmetric matrix to be
determined so that yAy satisfies certain criteria, which are the following:
1. Translation In®ariance. If instead of  we consider sy , then
0
Ž
.
from model 8.86 we have
yyX sXqU.
0
c
2
Ž
.
Ž
.
In this case, Ý
a 	
is estimated by
yyX
A yyX
. The
is1
i
i
0
0
estimator yAy is said to be translation invariant if
yAys yyX
A yyX
.
Ž
.
Ž
.
0
0
In order for this to be true we must have
AXs0.
8.88
Ž
.
Ž
.
c
2
Ž
2. Unbiasedness. E yAy sÝ
a 	 . Using a result in Searle 1971, The-
is1
i
i
.
orem 1, page 55 , the expected value of the quadratic form yAy is given
by
E yAy sXAXqtr A Var y
,
8.89
Ž
.
Ž .
Ž
.
Ž .
Ž
. Ž
.
Ž
.
since E y sX. From formulas 8.87 , 8.88 , and 8.89 we then have
c
2
E yAy s
	
tr AV .
8.90
Ž
.
Ž
.
Ž
.
Ý
i
i
is1
By comparison with Ýc
a 	 2, the condition for unbiasedness is
is1
i
i
a str AV ,
is1, 2, . . . , c.
8.91
Ž
.
Ž
.
i
i

OPTIMIZATION IN STATISTICS
380
Ž
.
3. Minimum Norm. If  ,  , . . . ,  in model 8.85 were observable, then
1
2
c
a natural unbaised estimator of Ýc
a 	 2 would be Ýc
a 
 rm ,
is1
i
i
is1
i
i
i
i
Ž

.
Ž
2.
2
since E   str I
	
sm 	 , is1, 2, . . . , c. This estimator can be
i
i
m
i
i
i
i
written as , where  is the block-diagonal matrix
a
a
a
1
2
c
sDiag
I
,
I
, . . . ,
I
.
m
m
m
1
2
c
ž
/
m
m
m
1
2
c
The difference between this estimator and yAy is
yAyys UAUy ,
Ž
.
since AXs0. This difference can be made small by minimizing the
	
	
Euclidean norm
UAUy
.
2
The quadratic estimator yAy is said to be a minimum norm quadratic
Ž
.
c
2
unbiased estimator MINQUE of Ý
a 	
if the matrix A is deter-
is1
i
i
	
	
mined so that
UAUy
attains a minimum subject to the condi-
2
Ž
.
Ž
.
tions given in formulas 8.88 and 8.91 . Such an estimator was intro-
Ž
.
duced by Rao 1971, 1972 .
	
	
Ž
.
The minimization of
UAUy
is equivalent to that of tr AVAV ,
2
where VsÝc
V. The reason for this is the following:
is1
i
2
	
	
UAUy
str
UAUy
UAUy
Ž
. Ž
.
2
str UAUUAU y2 tr UAU qtr 2 .
8.92
Ž
.
Ž
.
Ž
.
Ž
.
Now,
tr UAU str AUU
Ž
.
Ž
.
c
ai

str A
U
I
U
Ý
i
m
i
i
ž
/
mi
is1
c
ai

str
AU U
Ý
i
i
ž
/
mi
is1
c
ai
str
AV
Ý
i
ž
/
mi
is1
c
ai
s
tr AV
Ž
.
Ý
i
mi
is1
c
2
ai
s
,
by 8.91
Ž
.
Ý mi
is1
str 2 .
Ž
.

MINIMUM NORM QUADRATIC UNBIASED ESTIMATION OF VARIANCE
381
Ž
.
Formula 8.92 can then be written as
	
	 2
2
UAUy
str UAUUAU ytr 
Ž
.
Ž
.
2
str AVAV ytr 2 ,
Ž
.
Ž
.
since VsÝc
V sÝc
U U
sUU. The trace of 2 does not involve A;
is1
i
is1
i
i
hence the problem of MINQUE reduces to finding A that minimizes
Ž
.
Ž
.
Ž
.
Ž
.
tr AVAV subject to conditions
8.88
and
8.91 . Rao
1971
showed
that the solution to this optimization problem is of the form
c
As
 RV R,
8.93
Ž
.
Ý
i
i
is1
where
y
y1
y1
y1
y1
RsV
yV
X XV
X
XV
Ž
.
Ž
y1
.y
y1
with XV
X
being a generalized inverse of XV
X, and the  ’s
i
are obtained from solving the equations
c
 tr RV RV sa ,
js1, 2, . . . , c,
Ž
.
Ý
i
i
j
j
is1
which can be expressed as
Ssa,
8.94
Ž
.
Ž
.
Ž
.
where
s  ,  , . . . ,  , S is the cc matrix
s
with
s s
1
2
c
i j
i j
Ž
.
Ž
.
c
2
tr RV RV , and as a , a , . . . , a . The MINQUE of Ý
a 	
can
i
j
1
2
c
is1
i
i
then be written as
c
c
y
 RV R ys
 yRV Ry
Ý
Ý
i
i
i
i
ž
/
is1
is1
sq,
Ž
.
Ž
.
where qs q , q , . . . , q  with q syRV Ry
is1, 2, . . . , c . But, from
1
2
c
i
i
Ž
.
y
y
formula
8.94 , saS , where S
is a generalized inverse of S.
y
Ž
2
2
2.
Hence, qsaS qsa, where s 	 , 	 , . . . , 	
 is a solution
ˆ
ˆ
ˆ
ˆ
ˆ
1
2
c
of the equation
Ssq.
8.95
Ž
.
ˆ
This equation has a unique solution if and only if the individual
Ž
variance components are unbiasedly estimable
see Rao, 1972, page

OPTIMIZATION IN STATISTICS
382
.
2
114 . Thus the MINQUEs of the 	 ’s are obtained from solving
i
Ž
.
equation 8.95 .
Ž
.
If the random effects in model 8.85 are assumed to be normally
distributed, then the MINQUEs of the variance components reduce to
the
so-called
minimum
variance
quadratic
unbiased
estimators
Ž
.
MIVQUEs . An example that shows how to compute these estimators
in the case of a random one-way classification model is given in
Ž
.
Ž
Swallow and Searle
1978 . See also Milliken and Johnson
1984,
.
Chapter 19 .
´
8.10. SCHEFFE’S CONFIDENCE INTERVALS
Consider the linear model
ysXq ,
8.96
Ž
.
where y is a vector of n observations, X is a known matrix of order np and
Ž
.
rand r Fp ,  is a vector of unknown parameters, and  is a random error
vector. It is assumed that  has the normal distribution with a mean 0 and a
variancecovariance matrix 	 2I . Let sa be an estimable linear func-
n
tion of the elements of . By this we mean that there exists a linear function
Ž
.
ty of y such that E ty s, where t is some constant vector. A necessary
and sufficient condition for  to be estimable is that a belongs to the row
Ž
space of X, that is, a is a linear combination of the rows of X
see, for
.
example, Searle, 1971, page 181 . Since the rank of X is r, the row space of X,
Ž .
denoted by
 X , is an
r-dimensional subspace of the
p-dimensional
p
Ž .
Euclidean space R . Thus a estimable if and only if ag X .
Suppose that a is an arbitrary vector in a q-dimensional subspace LL of
ˆ
y
Ž .
Ž
.
 X , where qFr. Then asa XX
Xy is the best linear unbiased
estimator of a, and its variance is given by
y
2
ˆ
Var a s	 a XX
a,
Ž
.
Ž
.
Ž
.y
Ž
where XX
is a generalized inverse of XX see, for example, Searle, 1971,
ˆ
y
.
Ž
.
pages 181182 . Both a and a XX
a are invariant to the choice of
Ž
.y
Ž
.
XX
, since a is estimable see, for example, Searle, 1971, page 181 . In
Ž
.y
Ž
.y1
particular, if rsp, then XX is of full rank and XX
s XX
.
Ž
.
Theorem 8.10.1.
Simultaneous 1y 100% confidence intervals on a
Ž .
for all agLL, where LL is a q-dimensional subspace of  X , are of the form
1r2
y
1r2
ˆ
a q MS F
a XX
a
,
8.97
Ž
.
Ž
.
Ž
.
E
 , q, nyr

SCHEFFE’S CONFIDENCE INTERVALS
´
383
where F
is the upper 100th percentile of the F-distribution with q
, q, nyr
and nyr degrees of freedom, and MS
is the error mean square given by
E
1
y
MS s
y I yX XX
X y.
8.98
Ž
.
Ž
.
E
n
nyr
In Theorem 8.10.1, the word ‘‘simultaneous’’ means that with probability
1y, the values of a for all agLL satisfy the double inequality
1r2
y
1r2
ˆ
ay q MS F
a XX
a
Ž
.
Ž
.
E
 , q, nyr
1r2
y
1r2
ˆ
FaFaq q MS F
a XX
a
.
8.99
Ž
.
Ž
.
Ž
.
E
 , q, nyr
Ž
.
A proof of this theorem is given in Scheffe 1959, Section 3.5 . Another proof
´
is presented here using the method of Lagrange multipliers. This proof is
based on the following lemma:

q
4
Lemma 8.10.1.
Let C be the set xgR
xAxF1 , where A is a positive


Ž
y1 .1r2
definite matrix of order qq. Then xgC if and only if
lx F lA
l
for all lgRq.
Proof. Suppose that xgC. Since A is positive definite, the boundary of C
is an ellipsoid in a q-dimensional space. For any lgRq, let e be a unit vector
in its direction. The projection of x on an axis in the direction of l is given by
ex. Consider optimizing ex with respect to x over the set C. The minimum
and maximum values of ex are obviously determined by the end points of the
projection of C on the l-axis. This is equivalent to optimizing ex subject to
the constraint xAxs1, since the projection of C on the l-axis is the same as
the projection of its boundary, the ellipsoid xAxs1. This constrained
optimization problem can be solved by using the method of Lagrange
multipliers.
Ž
.
Let Gsexq xAxy1 , where  is a Lagrange multiplier. By differenti-
ating G with respect to x , x , . . . , x , where x
is the ith element of x
1
2
q
i
Ž
.
is1, 2, . . . , q , and equating the derivatives to zero, we obtain the equation
Ž
.
y1
eq2Axs0, whose solution is xsy 1r2 A
e. If we substitute this value
of x into the equation xAxs1 and then solve for , we obtain the two
1
1
y1
1r2
y1
1r2
Ž
.
Ž
.
solutions  sy
eA
e
,  s
eA
e
. But, exsy2, since xAx
1
2
2
2
s1. It follows that the minimum and maximum values of ex under the
Ž
y1 .1r2
Ž
y1 .1r2
constraint xAxs1 are y eA
e
and eA
e
, respectively. Hence,
1r2
y1


ex F eA
e
.
8.100
Ž
.
Ž
.
	 	
	 	
Since ls l
e, where
l
is the Euclidean norm of l, multiplying the two
2
2

OPTIMIZATION IN STATISTICS
384
Ž
.
	 	
sides of inequality 8.100 by l
yields
2
1r2
y1


lx F lA
l
.
8.101
Ž
.
Ž
.
Ž
.
q
Vice versa, if inequality
8.101
is true for all lgR , then by choosing
lsxA we obtain
1r2
y1


xAx F xAA
Ax
,
Ž
.
which is equivalent to xAxF1, that is, xgC.

Proof of Theorem 8.10.1. Let L be a qp matrix of rank q whose rows
Ž .
form a basis for the q-dimensional subspace LL of  X . Since y in model
2
ˆ
y
Ž
.
Ž
.
Ž
.
8.96
is distributed as N X, 	 I
, LsL XX
Xy is distributed as
n
w
2 Ž
.y
x
N L, 	 L XX
L . Thus the random variable
y1
y
ˆ
ˆ
L y
 L XX
L
L y
Ž
.
Ž
.
Ž
.
Fs
q MSE
Ž
has the F-distribution with q and nyr degrees of freedom see, for example,
.
Searle, 1971, page 190 . It follows that
P FFF
s1y.
8.102
Ž
.
Ž
.
 , q, nyr
ˆ
Ž
.
Ž
.
By applying Lemma 8.10.1 to formula
8.102
with xsL y
and As
w Ž
.y
xy1 Ž
.
L XX
L
r q MS F
we obtain the equivalent probability state-
E
, q, nyr
ment
1r2
y
1r2
q
ˆ
P
lL y
F q MS F
lL XX
Ll
#lgR
s1y.
Ž
.
Ž
.
Ž
.
½
5
E
 , q, nyr
Let aslL. We then have
1r2
y
1r2
ˆ
P
a y
F q MS F
a XX
a
#agLL s1y.
Ž
.
Ž
.
Ž
.
½
5
E
 , q, nyr
Ž
.
We conclude that the values of a satisfy the double inequality 8.99 for all
Ž
.
ag LL with probability 1y. Simultaneous 1y 100% confidence inter-
Ž
.
vals on a are therefore given by formula 8.97 . We refer to these intervals
as Scheffe’s confidence intervals.
´
Theorem 8.10.1 can be used to obtain simultaneous confidence intervals
on all contrasts among the elements of . By definition, the linear function
a is a contrast among the elements of  if Ý p
a s0, where a is the ith
is1
i
i
Ž
.
element of a
is1, 2, . . . , p . If a is in the row space of X, then it must
Ž .
belong to a q-dimensional subspace of  X , where qsry1. Hence, simul-

SCHEFFE’S CONFIDENCE INTERVALS
´
385
Ž
.
taneous
1y 100% confidence intervals on all such contrasts can be
Ž
.
obtained from formula 8.97 by replacing q with ry1.

8.10.1. The Relation of Scheffe’s Confidence Intervals to the F-Test
´
Ž
.
There is a relationship between the confidence intervals 8.97 and the F-test
used to test the hypothesis H : Ls0 versus H : L0, where L is the
0
a
Ž .
matrix whose rows form a basis for the q-dimensional subspace LL of  X .
Ž
.
The test statistic for testing H
is given by see Searle, 1971, Section 5.5
0
y1
y
ˆ
ˆ
L L XX
L
L
Ž
.
Fs
,
q MSE
which under H
has the F-distribution with q and nyr degrees of freedom.
0
The hypothesis H
can be rejected at the -level of significance if F
0
F
. In this case, by Lemma 8.10.1, there exits at least one lgRq such
, q, nyr
that
1r2
y
1r2
ˆ


lL  q MS F
lL XX
Ll
.
8.103
Ž
.
Ž
.
Ž
.
E
 , q, nyr
It follows that the F-test rejects H
if and only if there exists a linear
0
ˆ
q
combination a, where aslL for some lgR , for which the confidence
ˆ
Ž
.
interval in formula 8.97 does not contain the value zero. In this case, a is
said to be significantly different from zero.
Ž
.
q
It is easy to see that inequality 8.103 holds for some lgR
if and only if
ˆ 2


lL
sup
q MS F
,
y
E
 , q, nyr
q lL XX
Ll
Ž
.
lgR
or equivalently,
lG l
1
sup
q MS F
,
8.104
Ž
.
E
 , q, nyr
q lG l
2
lgR
where
ˆˆ
G sLL,
8.105
Ž
.
1
y
G sL XX
L.
8.106
Ž
.
Ž
.
2
However, by Theorem 2.3.17,
lG l
1
y1
sup
se
G
G
Ž
.
max
2
1
q lG l
2
lgR
y1
y
ˆ
ˆ
sL L XX
L
L,
8.107
Ž
.
Ž
.

OPTIMIZATION IN STATISTICS
386
Ž
y1
.
y1
where e
G
G
is the largest eigenvalue of G
G . The second equality
max
2
1
2
1
y
y1 ˆˆ
Ž
.
w Ž
.
x
in 8.107 is true because the nonzero eigenvalues of L XX
L
LL
ˆ
y
y1 ˆ
w Ž
.
x
are the same as those of L L XX
L
L by Theorem 2.3.9. Note that
the latter expression is the numerator sum of squares of the F-test statistic
for H .
0
y1
Ž
y1
.
The eigenvector of G
G
corresponding to e
G
G
is of special
2
1
max
2
1
interest. Let l* be such an eigenvector. Then
l*G l*
1
y1
se
G
G
.
8.108
Ž
.
Ž
.
max
2
1
l*G l*
2
This follows from the fact that l* satisfies the equation
G ye
G
l*s0,
Ž
.
1
max
2
Ž
y1
.
where e
is an abbreviation for e
G
G
. It is easy to see that l* can be
max
max
2
1
y1 ˆ
chosen to be the vector G
L, since
2
y1
y1 ˆ
y1 ˆˆ
y1 ˆ
G
G
G
L sG
LL G
L
Ž
.
Ž
.
2
1
2
2
2
ˆ
y1 ˆ
y1 ˆ
s LG
L G
L
ž
/
2
2
y1 ˆ
se
G
L.
max
2
y1 ˆ
y1
This shows that G
L is an eigenvector of G
G
for the eigenvalue e
.
2
2
1
max
Ž
.
Ž
.
From inequality 8.104 and formula 8.108 we conclude that if the F-test
rejects H at the -level, then
0
1r2
y
1r2
ˆ


l*L  q MS F
l*L XX
Ll*
.
Ž
.
Ž
.
E
 , q, nyr
ˆ
This means that the linear combination a*, where a*sl*L, is signifi-
ˆ
cantly different from zero. Let us express a* as
q

ˆ
l*Ls
l  ,
8.109
Ž
.
ˆ
Ý
i
i
is1

ˆ
where l
and 
are the ith elements of l* and sL, respectively
ˆ
ˆ
i
i
Ž
.
w
is1, 2, . . . , q . If we divide 
by its estimated standard error 
which is
ˆ
ˆ
i
i
equal to the square root of the ith diagonal element of the variancecovari-
ˆ
2
y
2
Ž
.
ance matrix of L, namely, 	 L XX
L with 	
replaced by the error
Ž
.x
Ž
.
mean square MS
in formula 8.98 , then formula 8.109 can be written as
E
q

ˆ
l*Ls
l   ,
8.110
Ž
.
ˆˆ
Ý
i
i
i
is1
 
where  s r , is1, 2, . . . , q. Consequently, large values of l
 identify
ˆ
ˆ ˆ
ˆ
i
i
i
i
i
those elements of  that are influential contributors to the significance of
ˆ

SCHEFFE’S CONFIDENCE INTERVALS
´
387
the F-test concerning H . Note that the elements of sL form a set of
0
linearly independent estimable linear functions of .
We conclude from the previous arguments that the eigenvector l*, which
corresponds to the largest eigenvalue of Gy1G , can be conveniently used to
2
1
identify an estimable linear function of  that is significantly different from
zero whenever the F-test rejects H .
0
Ž
.
Ž
It should be noted that if model 8.96 is a response surface model in this
.
case, the matrix X in the model is of full column rank, that is, rsp whose
input variables, x , x , . . . , x , have different units of measurement, then
1
2
k
these variables must be made scale free. This is accomplished as follows: If
x
denotes the uth measurement on x , then we may consider the transfor-
ui
i
mation
x yx
ui
i
z s
,
is1, 2, . . . , k; us1, 2, . . . , n,
ui
si
n
n
2 1r2
Ž
.
w
Ž
. x
where x s 1rn Ý
x , s s Ý
x yx
, and n is the total number
i
us1
ui
i
us1
ui
i
of observations. One advantage of this scaling convention, besides making the
input variables scale free, is that it can greatly improve the conditioning of
Ž
the matrix X with regard to multicollinearity see, for example, Belsley, Kuh,
.
and Welsch, 1980, pages 183185 .
EXAMPLE 8.10.1.
Let us consider the one-way classification model
y sq q ,
is1, 2, . . . , m; js1, 2, . . . , n ,
8.111
Ž
.
i j
i
i j
i
where  and 
are unknown parameters with the latter representing the
i
effect of the ith level of a certain factor at m levels; n observations are
i
obtained at the ith level. The  ’s are random errors assumed to be
i j
independent and normally distributed with zero means and a common
variance 	 2.
Ž
.
Ž
.
Model 8.111 can be represented in vector form as model 8.96 . Here,
Ž
.
y s y , y
, . . . , y
, y , y
, . . . , y
, . . . , y
, y
, . . . , y
,
 s
11
12
1n
21
22
2 n
m1
m2
m n
1
2
m
Ž
.
Ž
.
w
x
,  ,  , . . . , 
, and X is of order n mq1 of the form Xs 1 : T ,
1
2
m
n
where 1
is a vector of ones of order
n1,
nsÝm n , and Ts
n
is1
i
Ž
.
Diag 1 , 1
, . . . , 1
. The rank of X is rsm. For such a model, the
n
n
n
1
2
m
hypothesis of interest is
H :  s s  s ,
0
1
2
m
which can be expressed as H : Ls0, where L is a matrix of order
0

OPTIMIZATION IN STATISTICS
388
Ž
.
Ž
.
my1  mq1 and rank my1 of the form
0
1
y1
0

0
0
1
0
y1

0
.
.
.
.
.
Ls
.
.
.
.
.
.
.
.
.
.
.
0
1
0
0

y1
This hypothesis states that the factor under consideration has no effect on
the response. Note that each row of L is a linear combination of the rows of
X. For example, the ith row of Lwhose elements are equal to zero except
Ž
.
for the second and the
iq2 th elements, which are equal to 1 and y1,
respectivelyis the difference between rows 1 and  q1 of X, where
i
 sÝi
n , is1,2, . . . , my1. Thus the rows of L form a basis for a
i
js1
j
Ž .
q-dimensional subspace LL of  X , the row space of X, where qsmy1.
Let  sq . Then 
is the mean of the ith level of the factor
i
i
i
Ž
.
m
m
is1, 2, . . . , m . Consider the contrast sÝ
c  , that is, Ý
c s0. We
is1
i
i
is1
i
Ž
.
can write sa, where as 0, c , c , . . . , c
belongs to a q-dimensional
1
2
m
subspace of Rmq1. This subspace is the same as LL, since each row of L is of
Ž
.
m
the form 0, c , c , . . . , c
with Ý
c s0. Vice versa, if sa is such that
1
2
m
is1
i
Ž
.
m
as 0, c , c , . . . , c
with Ý
c s0, then a can be expressed as
1
2
m
is1
i
as yc ,yc , . . . , yc
L,
Ž
.
2
3
m
since c syÝm
c . Hence, ag LL. It follows that LL is a subspace associ-
1
is2
i
ated with all contrasts among the means  ,  , . . . , 
of the m levels of the
1
2
m
factor.
Ž
.
Simultaneous
1y 100% confidence intervals on all contrasts of the
m
Ž
.
form sÝ
c  can be obtained by applying formula 8.97 . Here, qs
is1
i
i
my1, rsm, and a generalized inverse of XX is of the form
y
0
0
XX
s
,
Ž
.
0
D
Ž
y1
y1
y1.
where DsDiag n
, n
, . . . , n
and 0 is a zero vector of order m1.
1
2
m
Hence,
y
ˆ
as 0, c , c , . . . , c
XX
Xy
Ž
. Ž
.
1
2
m
m
s
c y ,
Ý
i
i.
is1

SCHEFFE’S CONFIDENCE INTERVALS
´
389
ni
Ž
.
where y s 1rn Ý
y , is1, 2, . . . , m. Furthermore,
i.
i
js1
i j
m
2
ci
y
a XX
as
.
Ž
.
Ý ni
is1
Ž
.
By making the substitution in formula 8.97 we obtain
1r2
2
m
m ci
1r2
c y 
my1 MS F
.
8.112
Ž
.
Ž
.
Ý
Ý
i
i.
E
 , my1 , nym ž
/
ni
is1
is1
Now, if the F-test rejects H
at the -level, then there exists a contrast
0

m
ˆ
Ý
c y sa*, which is significantly different from zero, that is, the inter-
is1
i
i.
Ž
.
 Ž
.
val 8.112 for c sc
is1, 2, . . . , m does not contain the value zero. Here,
i
i
y1 ˆ
y1
a*sl*L, where l*sG
L is an eigenvector of G
G
corresponding to
2
2
1
Ž
y1
.
e
G
G
. We have that
mam
2
1
y1
1
y1
y
y1
G
s L XX
L
s
J
q
,
Ž
.
2
my1
ž
/
n1
Ž
.
Ž
.
where J
is a matrix of ones of order
my1  my1 , and
s
my1
Ž
y1
y1
y1.
Diag n
, n
, . . . , n
. By applying the ShermanMorrisonWoodbury for-
2
3
m
Ž
.
mula see Exercise 2.15 , we obtain
y1

y1
G
sn
n q1
1
Ž
.
2
1
1
my1
my1
y11
1
y1
my1
my1
y1
s
y

y1
n q1

1
1
my1
my1
n2
n
1
3.
w
x
sDiag n , n , . . . , n
y
n , n , . . . , n
.
Ž
.
2
2
m
2
3
m
.
n
.
nm
Also,
y yy
1.
2.
y yy
1.
3.
y
ˆ
.
sLsL XX
Xys
.
Ž
.
ˆ
..
y yy
1.
m.
y1 ˆ
It can be verified that the ith element of l*sG
L is given by
2
m
niq1
l sn
y yy
y
n
y yy
,
is1, 2, . . . , my1.
8.113
Ž
.
Ž
.
Ž
.
Ý
i
iq1
1.
iq1 .
j
1.
j.
n
js2

OPTIMIZATION IN STATISTICS
390
Ž
.
The estimated standard error,  , of the ith element of  is1, 2, . . . , my1
ˆ
ˆ
i
Ž
.y
is the square root of the
ith diagonal element of
MS L XX
Ls
E
wŽ
.
x
1rn J
q MS , that is,
1
my1
E
1r2
1
1
 s
q
MS
,
is1, 2, . . . , my1.
ˆi
E
ž
/
n
n
1
iq1
Ž
.
 
Thus by formula
8.110 , large values of
l

identify those elements of
ˆ
i
i
ˆ
sL that are influential contributors to the significance of the F-test. In
ˆ
Ž
.
particular, if the data set used to analyze model 8.111 is balanced, that is,
n snrm for is1, 2, . . . , m, then
i
1r2
2n





l
 s y
yy
MS
,
is1, 2, . . . , my1,
ˆ
i
i
iq1 .
. .
E
ž
/
m
m
Ž
.
where y s 1rm Ý
y .
. .
is1
i.
 ˆ
Alternatively, the contrast a  can be expressed as
y
ˆ
a*sl*L XX
Xy
Ž
.
my1




s 0,
l ,yl ,yl , . . . , yl
0, y
, y
, . . . , y

Ž
.
Ý
i
1
2
my1
1.
2.
m.
ž
/
is1
m

s
c y ,
Ý
i
i.
is1

Ž
.
where l
is given in formula 8.113 and
i
my1
°
l ,
is1,
Ý
j
 ~
c s
8.114
Ž
.
js1
i

¢yl
,
is2, 3, . . . , m.
iy1
1r2
Ž
.
Since the estimated standard error of y
is
MS rn
, is1, 2, . . . , m, by
i.
E
i
dividing y
by this value we obtain
i.
1r2
m
1

ˆ
a*s
MS
c w ,
Ý
E
i
i
ž
/
ni
is1
1r2
Ž
.
Ž
.
where w sy r MS rn
is a scaled value of y
is1, 2, . . . , m . Hence,
i
i.
E
i
i.

1r2
Ž
.


large values of
MS rn
c
identify those y ’s that contribute signifi-
E
i
i
i.
cantly to the rejection of H . In particular, for a balanced data set,
0
n
l s
y yy
,
is1, 2, . . . , my1.
Ž
.
i
. .
iq1 .
m

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
391
Ž
.
Thus from formula 8.114 we get
1r2
1r2
MS
n
E





c
s
MS
y yy
,
is1, 2, . . . , m.
i
E
i.
. .
ž
/
ž
/
n
m
i


We conclude that large values of y yy
are responsible for the rejection of
i.
. .
H
by the F-test. This is consistent with the fact that the numerator sum of
0
m
2
Ž
.
squares of the F-test statistic for H
is proportional to Ý
y yy
when
0
is1
i.
. .
the data set is balanced.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Adby, P. R., and M. A. H. Dempster 1974 . Introduction of Optimization Methods.
Ž
Chapman and Hall, London. This book is an introduction to nonlinear methods
of optimization. It covers basic optimization techniques such as steepest descent
.
and the NewtonRaphson method.
Ž
.
Ash, A., and A. Hedayat
1978 . ‘‘An introduction to design optimality with an
overview of the literature.’’ Comm. Statist. Theory Methods, 7, 12951325.
Ž
.
Atkinson, A. C. 1982 . ‘‘Developments in the design of experiments.’’ Internat. Statist.
Re®., 50, 161177.
Ž
.
Atkinson, A. C.
1988 . ‘‘Recent developments in the methods of optimum and
related experimental designs.’’ Internat. Statist. Re®., 56, 99115.
Ž
.
Bates, D. M., and D. G. Watts 1988 . Nonlinear Regression Analysis and its Applica-
Ž
tions. Wiley, New York.
Estimation of parameters in a nonlinear model is
addressed in Chaps. 2 and 3. Design aspects for nonlinear models are briefly
.
discussed in Section 3.14.
Ž
.
Bayne, C. K., and I. B. Rubin 1986 . Practical Experimental Designs and Optimization
Ž
Methods for Chemists. VCH Publishers, Deerfield Beach, Florida.
Steepest
ascent and the simplex method are discussed in Chap. 5. A bibliography of
optimization and response surface methods, as actually applied in 17 major fields
.
of chemistry, is provided in Chap. 7.
Ž
.
Belsley, D. A., E. Kuh, and R. E. Welsch 1980 . Regression Diagnostics. Wiley, New
Ž
York. Chap. 3 is devoted to the diagnosis of multicollinearity among the columns
of the matrix in a regression model. Multicollinearity renders the model’s
least-squares parameter estimates less precise and less useful than would other-
.
wise be the case.
Ž
.
Biles, W. E., and J. J. Swain
1980 . Optimization and Industrial Experimentation.
Ž
Wiley-Interscience, New York. Chaps. 4 and 5 discuss optimization techniques
.
that are directly applicable in response surface methodology.
Ž
.
Bohachevsky, I. O., M. E. Johnson, and M. L. Stein 1986 . ‘‘Generalized simulated
annealing for function optimization.’’ Technometrics, 28, 209217.
Ž
.
Box, G. E. P. 1982 . ‘‘Choice of response surface design and alphabetic optimality.’’
Utilitas Math., 21B, 1155.
Ž
.
Box, G. E. P., and D. W. Behnken 1960 . ‘‘Some new three level designs for the study
of quantitative variables.’’ Technometrics, 2, 455475.

OPTIMIZATION IN STATISTICS
392
Ž
.
Box, G. E. P., and N. R. Draper
1959 . ‘‘A basis for the selection of a response
surface design.’’ J. Amer. Statist. Assoc., 55, 622654.
Ž
.
Box, G. E. P., and N. R. Draper
1963 . ‘‘The choice of a second order rotatable
design.’’ Biometrika, 50, 335352.
Ž
.
Box, G. E. P., and N. R. Draper
1965 . ‘‘The Bayesian estimation of common
parameters from several responses.’’ Biometrika, 52, 355365.
Ž
.
Box, G. E. P., and N. R. Draper
1987 . Empirical Model-Building and Response
Ž
Surfaces. Wiley, New York. Chap. 9 introduces the exploration of maxima with
second-order models; the alphabetic optimality approach is critically considered
.
in Chap. 14. Many examples are given throughout the book.
Ž
.
Box, G. E. P., and H. L. Lucas 1959 . ‘‘Design of experiments in nonlinear situations.’’
Biometrika, 46, 7790.
Ž
.
Box, G. E. P., and K. B. Wilson 1951 . ‘‘On the experimental attainment of optimum
conditions.’’ J. Roy. Statist. Soc. Ser. B, 13, 145.
Ž
.
Bunday, B. D.
1984 . Basic Optimization Methods. Edward Arnold Ltd., Victoria,
Ž
Australia.
Chaps. 3 and 4 discuss basic optimization techniques such as the
.
NelderMead simplex method and the DavidonFletcherPowell method.
Ž
.
Conlon, M. 1991 . ‘‘The controlled random search procedure for function optimiza-
Ž
tion.’’ Personal communication.
This is a FORTRAN file for implementing
.
Price’s controlled random search procedure.
Ž
.
Conlon, M., and A. I. Khuri
1992 . ‘‘Multiple response optimization.’’ Technical
Report, Department of Statistics, University of Florida, Gainesville, Florida.
Ž
.
Cook, R. D., and C. J. Nachtsheim 1980 . ‘‘A comparison of algorithms for construct-
ing exact D-optimal designs.’’ Technometrics, 22, 315324.
Ž
.
Dempster, A. P., N. M. Laird, and D. B. Rubin 1977 . ‘‘Maximum likelihood from
incomplete data via the EM algorithm.’’ J. Roy. Statist. Soc. Ser. B, 39, 138.
Ž
.
Draper, N. R. 1963 . ‘‘Ridge analysis of response surfaces.’’Technometrics, 5, 469479.
Ž
.
Everitt, B. S.
1987 . Introduction to Optimization Methods and Their Application in
Ž
Statistics. Chapman and Hall, London. This book gives a brief introduction to
optimization methods and their use in several areas of statistics. These include
maximum likelihood estimation, nonlinear regression estimation, and applied
.
multivariate analysis.
Ž
.
Fedorov, V. V. 1972 . Theory of Optimal Experiments. Academic Press, New York.
ŽThis book is a translation of a monograph in Russian. It presents the mathemati-
.
cal apparatus of experimental design for a regression model.
Ž
.
Fichtali, J., F. R. Van De Voort, and A. I. Khuri 1990 . ‘‘Multiresponse optimization
of acid casein production.’’ J. Food Process Eng., 12, 247258.
Ž
.
Ž
Fletcher, R. 1987 . Practical Methods of Optimization, 2nd ed. Wiley, New York. This
book gives a detailed study of several unconstrained and constrained optimiza-
.
tion techniques.
Ž
.
Fletcher, R., and M. J. D. Powell 1963 . ‘‘A rapidly convergent descent method for
minimization.’’ Comput. J., 6, 163168.
Ž
.
Hartley, H. O., and J. N. K. Rao 1967 . ‘‘Maximum likelihood estimation for the
mixed analysis of variance model.’’ Biometrika, 54, 93108.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
393
Ž
.
Hoerl, A. E. 1959 . ‘‘Optimum solution of many variables equations.’’ Chem. Eng.
Prog., 55, 6978.
Ž
.
Huber, P. J. 1973 . ‘‘Robust regression: Asymptotics, conjectures and Monte Carlo.’’
Ann. Statist., 1, 799821.
Ž
.
Ž
Huber, P. J.
1981 . Robust Statistics. Wiley, New York.
This book gives a solid
foundation in robustness in statistics. Chap. 3 introduces and discusses M-estima-
.
tion; Chap. 7 addresses M-estimation for a regression model.
Ž
.
Johnson, M. E., and C. J. Nachtsheim 1983 . ‘‘Some guidelines for constructing exact
D-optimal designs on convex design spaces.’’ Technometrics, 25, 271277.
Ž
.
Jones, E. R., and T. J. Mitchell 1978 . ‘‘Design criteria for detecting model inade-
quacy.’’ Biometrika, 65, 541551.
Ž
.
Karson, M. J., A. R. Manson, and R. J. Hader 1969 . ‘‘Minimum bias estimation and
experimental design for response surfaces.’’ Technometrics, 11, 461475.
Ž
.
Khuri, A. I., and M. Conlon 1981 . ‘‘Simultaneous optimization of multiple responses
represented by polynomial regression functions.’’ Technometric, 23, 363375.
Ž
.
Khuri, A. I., and J. A. Cornell 1996 . Response Surfaces, 2nd ed. Marcel Dekker, New
Ž
York. Optimization techniques in response surface methodology are discussed in
.
Chap. 5.
Ž
.
Khuri, A. I., and R. H. Myers 1979 . ‘‘Modified ridge analysis.’’ Technometrics, 21,
467473.
Ž
.
Khuri, A. I., and H. Sahai
1985 . ‘‘Variance components analysis: A selective
literature survey.’’ Internat. Statist. Re®., 53, 279300.
Ž
.
Kiefer, J. 1958 . ‘‘On the nonrandomized optimality and the randomized nonoptimal-
ity of symmetrical designs.’’ Ann. Math. Statist., 29, 675699.
Ž
.
Ž
.
Kiefer, J. 1959 . ‘‘Optimum experimental designs’’ with discussion . J. Roy. Statist.
Soc. Ser. B, 21, 272319.
Ž
.
Kiefer, J. 1960 . ‘‘Optimum experimental designs V, with applications to systematic
and rotatable designs.’’ In Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Probability, Vol. 1. University of California Press,
Berkeley, pp. 381405.
Ž
.
Kiefer, J. 1961 . ‘‘Optimum designs in regression problems II.’’ Ann. Math. Statist.,
32, 298325.
Ž
.
Kiefer, J. 1962a . ‘‘Two more criteria equivalent to D-optimality of designs.’’ Ann.
Math. Statist., 33, 792796.
Ž
.
Kiefer, J. 1962b . ‘‘An extremum result.’’ Canad. J. Math., 14, 597601.
Ž
.
Kiefer, J.
1975 . ‘‘Optimal design: Variation in structure and performance under
change of criterion.’’ Biometrika, 62, 277288.
Ž
.
Kiefer, J., and J. Wolfowitz 1960 . ‘‘The equivalence of two extremum problems.’’
Canad. J. Math., 12, 363366.
Ž
.
Kirkpatrick, S., C. D. Gelatt, and M. P. Vechhi 1983 . ‘‘Optimization by simulated
annealing.’’ Science, 220, 671680.
Ž
.
Little, R. J. A., and D. B. Rubin 1987 . Statistical Analysis with Missing Data. Wiley,
Ž
New York. The theory of the EM algorithm is introduced in Chap. 7. The book
presents a systematic approach to the analysis of data with missing values, where
inferences are based on likelihoods derived from formal statistical models for the
.
data.

OPTIMIZATION IN STATISTICS
394
Ž
.
Lucas, J. M.
1976 . ‘‘Which response surface design is best.’’ Technometrics, 18,
411417.
Ž
.
Miller, R. G., Jr. 1981 . Simultaneous Statistical Inference, 2nd ed. Springer-Verlag,
Ž
.
New York. Scheffe’s simultaneous confidence intervals are derived in Chap. 2.
´
Ž
.
Milliken, G. A., and D. E. Johnson 1984 . Analysis of Messy Data. Lifetime Learning
Ž
Publications, Belmont, California.
This book presents several techniques and
.
methods for analyzing unbalanced data.
Ž
.
Mitchell, T. J. 1974 . ‘‘An algorithm for the construction of D-optimal experimental
designs.’’ Technometrics, 16, 203210.
Ž
.
Myers, R. H.
1976 . Response Surface Methodology. Author, Blacksburg, Virginia.
ŽChap. 5 discusses the determination of optimum operating conditions in re-
sponse surface methodology; designs for fitting first-order and second-order
models are discussed in Chaps. 6 and 7, respectively; Chap. 9 presents the
.
J-criterion for choosing a response surface design.
Ž
.
Myers, R. H.
1990 . Classical and Modern Regression with Applications, 2nd ed.,
Ž
PWS-Kent, Boston. Chap. 3 discusses the effects and hazards of multicollinear-
ity in a regression model. Methods for detecting and combating multicollinearity
.
are given in Chap. 8.
Ž
.
Myers, R. H., and W. H. Carter, Jr. 1973 . ‘‘Response surface techniques for dual
response systems.’’ Technometrics, 15, 301317.
Ž
.
Myers, R. H., and A. I. Khuri 1979 . ‘‘A new procedure for steepest ascent.’’ Comm.
Statist. Theory Methods, 8, 13591376.
Ž
.
Myers, R. H., A. I. Khuri, and W. H. Carter, Jr. 1989 . ‘‘Response surface methodol-
ogy: 19661988.’’ Technometrics, 31, 137157.
Ž
.
Nelder, J. A., and R. Mead 1965 . ‘‘A simplex method for function minimization.’’
Comput. J., 7, 308313.
Ž
.
Nelson, L. S.
1973 . ‘‘A sequential simplex procedure for non-linear least-squares
estimation and other function minimization problems.’’ In 27th Annual Technical
Conference Transaction, American Society for Quality Control, pp. 107117.
Ž
.
Olsson, D. M., and L. S. Nelson 1975 . ‘‘The NelderMead simplex procedure for
function minimization.’’ Technometrics, 17, 4551.
Ž
.
Pazman, A.
1986 .
Foundations of Optimum Experimental Design. D. Reidel,
Dordrecht, Holland.
Ž
.
Plackett, R. L., and J. P. Burman
1946 . ‘‘The design of optimum multifactorial
experiments.’’ Biometrika, 33, 305325.
Ž
.
Price, W. L. 1977 . ‘‘A controlled random search procedure for global optimization.’’
Comput. J., 20, 367370.
Ž
.
Rao, C. R.
1970 . ‘‘Estimation of heteroscedastic variances in linear models.’’
J. Amer. Statist. Assoc., 65, 161172.
Ž
.
Rao, C. R. 1971 . ‘‘Estimation of variance and covariance componentsMINQUE
theory.’’ J. Multi®ariate Anal., 1, 257275.
Ž
.
Rao, C. R.
1972 . ‘‘Estimation of variance and covariance components in linear
models.’’ J. Amer. Statist. Assoc., 67, 112115.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Reading, Massachusetts.

EXERCISES
395
Ž
.
Rustagi, J. S., ed. 1979 . Optimizing Methods in Statistics. Academic Press, New York.
Ž
.
Ž
Scheffe, H.
1959 . The Analysis of Variance. Wiley, New York.
This classic book
´
.
presents the basic theory of analysis of variance, mainly in the balanced case.
Ž
.
Ž
Searle, S. R. 1971 . Linear Models. Wiley, New York. This book describes general
procedures of estimation and hypothesis testing for linear models. Estimable
.
linear functions for models that are not of full rank are discussed in Chap. 5.
Ž
.
Ž
Seber, G. A. F. 1984 . Multi®ariate Obser®ations, Wiley, New York. This book gives a
comprehensive survey of the subject of multivariate analysis and provides many
.
useful references.
Ž
.
Silvey, S. D. 1980 . Optimal Designs. Chapman and Hall, London.
Ž
.
Spendley, W., G. R. Hext, and F. R. Himsworth 1962 . ‘‘Sequential application of
simplex designs in optimization and evolutionary operation.’’ Technometrics, 4,
441461.
Ž
.
St. John, R. C., and N. R. Draper
1975 . ‘‘D-Optimality for regression designs:
A review.’’ Technometrics, 17, 1523.
Ž
.
Swallow, W. H., and S. R. Searle
1978 . ‘‘Minimum variance quadratic unbiased
Ž
.
estimation MIVQUE of variance components.’’ Technometrics, 20, 265272.
Ž
.
Watson, G. S. 1964 . ‘‘A note on maximum likelihood.’’ Sankhya Ser. A, 26, 303304.
Ž
.
Wynn, H. P. 1970 . ‘‘The sequential generation of D-optimum experimental designs.’’
Ann. Math. Statist., 41, 16551664.
Ž
.
Wynn, H. P. 1972 . ‘‘Results in the theory and construction of D-optimum experi-
mental designs.’’ J. Roy. Statist. Soc. Ser. B, 34, 133147.
Ž
.
Zanakis, S. H., and J. S. Rustagi, eds. 1982 . Optimization in Statistics. North-Holland,
Ž
Amsterdam, Holland. This is Volume 19 in Studies in the Management Sciences.
It contains 21 articles that address applications of optimization in three areas of
statistics, namely, regression and correlation; multivariate data analysis and
.
design of experiments; and statistical estimation, reliability, and quality control.
EXERCISES
8.1. Consider the function
f x , x
s8 x 2y4 x x q5x 2.
Ž
.
1
2
1
1
2
2
Ž
.
Ž
.
Minimize f x , x
using the method of steepest descent with x s 5, 2 
1
2
0
as an initial point.
8.2. Conduct a simulated steepest ascent exercise as follows: Use the
function
 x , x
s47.9q3x yx q4 x 2q4 x x q3x 2
Ž
.
1
2
1
2
1
1
2
2
as the true mean response, which depends on two input variables x1
and x . Generate response values by using the model
2
y x s x q ,
Ž .
Ž .

OPTIMIZATION IN STATISTICS
396
where  has the normal distribution with mean 0 and variance 2.25,
Ž
.
and xs x , x
. Fit a first-order model in x
and x
in a neighbor-
1
2
1
2
hood of the origin using a 22 factorial design along with the corre-
sponding simulated response values. Make sure that replications are
taken at the origin in order to test for lack of fit of the fitted model.
Determine the path of steepest ascent, then proceed along it using
simulated response values. Conduct additional experiments as de-
scribed in Section 8.3.1.
8.3. Two types of fertilizers were applied to experimental plots to assess
their effects on the yield of a certain variety of potato. The design
settings used in the experiment along with the corresponding yield
values are given in the following table:
Original Settings
Coded Settings
Yield y
Ž
.
Fertilizer 1
Fertilizer 2
x
x
lbrplot
1
2
50.0
15.0
y1
y1
24.30
120.0
15.0
1
y1
35.82
50.0
25.0
y1
1
40.50
120.0
25.0
1
1
50.94
1r2
35.5
20.0
y2
0
30.60
1r2
134.5
20.0
2
0
42.90
1r2
85.0
12.9
0
y2
22.50
1r2
85.0
27.1
0
2
50.40
85.0
20.0
0
0
45.69
( )
a
Fit a second-order model in the coded variables
F y85
F y20
1
2
x s
,
x s
1
2
35
5
to the yield data, where F
and F
are the original settings of
1
2
fertilizers 1 and 2, respectively, used in the experiment.
( )
b
Apply the method of ridge analysis to determine the settings of the
Ž
two fertilizers that are needed to maximize the predicted yield in
the space of the coded input variables, the region R is the interior
and boundary of a circle centered at the origin with a radius equal
1r2 .
to 2
.
8.4. Suppose that  and 
are two values of the Lagrange multiplier 
1
2
used in the method of ridge analysis. Let y and y be the correspond-
ˆ
ˆ
1
2
ing values of y on the two spheres xxsr 2 and xxsr 2, respectively.
ˆ
1
2
Show that if r sr
and   , then y y .
ˆ
ˆ
1
2
1
2
1
2

EXERCISES
397
8.5. Consider again Exercise 8.4. Let x
and x
be the stationary points
1
2
corresponding to  and  , respectively. Consider also the matrix
1
2
ˆ
M x
s2 By I ,
is1, 2.
Ž
.
Ž
.
i
i
Ž
.
Ž
.
Show that if r sr , M x
is positive definite, and M x
is indefinite,
1
2
1
2
then y y .
ˆ
ˆ
1
2
8.6. Consider once more the method of ridge analysis. Let x be a stationary
point that corresponds to the radius r.
( )
a
Show that
2
2
2
2
k
k
k
" r
" x
" x
" x
i
i
i
3
2
2
r
s2r
q r
y
x
,
Ý
Ý
Ý
i
2
ž
/
ž
/ ž
/
"
"
"
"
is1
is1
is1
Ž
.
where x is the ith element of x is1, 2, . . . , k .
i
( )
Ž .
b
Make use of part a to show that
" 2r
0
if r0.
2
"
Ž .
8.7. Suppose that the ‘‘true’’ mean response  x is represented by a model
of order d
in k input variables x , x , . . . , x
of the form
2
1
2
k
 x sf x qg x ,
Ž .
Ž .
Ž .
Ž
.
Ž
.
where xs x , x , . . . , x
. The fitted model is of order d
d
of the
1
2
k
1
2
form
ˆ
y x sf x  ,
Ž .
Ž .
ˆ
ˆ
where  is an estimator of , not necessarily obtained by the method
ˆ
Ž
.
of least squares. Let sE  .
( )
Ž .
a
Give an expression for B, the average squared bias of y x , in terms
ˆ
of , , and .
( )
b
Show that B achieves its minimum value if and only if  is of the
Ž
.
w
y1
x
form sC, where s ,   and Cs I: 

. The matri-
11
12
Ž
.
ces 
and 
are the region moments used in formula 8.54 .
11
12
( )
Ž .
c
Deduce from part
b that B achieves its minimum value if and
Ž
only if C is an estimable linear function see Searle, 1971, Section
.
5.4 .

OPTIMIZATION IN STATISTICS
398
( )
Ž .
d
Use part c to show that B achieves its minimum value if and only
w
x
if there exists a matrix L such that CsL X: Z , where X and Z are
Ž .
Ž .
matrices consisting of the values taken by f x and g x , respec-
tively, at n experimental runs.
( )
Ž .
e
Deduce from part d that B achieves its minimum for any design
w
x
for which the row space of X: Z contains the rows of C.
ˆ
ˆ
( )f
Show that if  is the least-squares estimator of , that is, s
Ž
.y1
XX
Xy, where y is the vector of response values at the n
Ž .
experimental runs, then the design property stated in part e holds
for any design that satisfies the conditions described in equations
Ž
.
8.56 .
w Note: This problem is based on an article by Karson, Manson, and
Ž
.
Hader 1969 , who introduced the so-called minimum bias estima-
x
tion to minimize the average squared bias B.
Ž .
3
8.8. Consider again Exercise 8.7. Suppose that f x s qÝ
 x is a
0
is1
i
i
first-order model in three input variables fitted to a data set obtained
by using the design
yg
yg
yg
g
g
yg
Ds
,
g
yg
g
yg
g
g
where g is a scale factor. The region of interest is a sphere of radius 1.
Suppose that the ‘‘true’’ model is of the form
3
 x s q
 x q
x x q
x x q
x x .
Ž .
Ý
0
i
i
12
1
2
13
1
3
23
2
3
is1
( )
a
Can g be chosen so that D satisfies the conditions described in
Ž
.
equations 8.56 ?
( )
b
Can g be chosen so that D satisfies the minimum bias property
Ž .
described in part e of Exercise 8.7?
8.9. Consider the function
h , D s,
Ž
.
Ž
.
where  is a vector of unknown parameters as in model 8.48 ,  is the
Ž
.

matrix in formula 8.53 , namely sA Ay
AyA q , and
11
12
12
22
D is the design matrix.

EXERCISES
399
( )
Ž
.
a
Show that for a given D, the maximum of h , D over the region
 
24
2
Ž
.
Ž
.
s  Fr
is equal to r e
 , where e
 is the largest
max
max
eigenvalue of .
( )
Ž .
b
Deduce from part a a design criterion for choosing D.
8.10. Consider fitting the model
y x sf x q ,
Ž .
Ž .
where  is a random error with a zero mean and a variance 	 2.
Suppose that the ‘‘true’’ mean response is given by
 x sf x qg x .
Ž .
Ž .
Ž .
Ž .
Let X and Z be the same matrices defined in part d of Exercise 8.7.
Ž
.
Consider the function  , D sS, where
y1
SsZ IyX XX
X Z,
Ž
.
Ž
.
2
and D is the design matrix. The quantity  , D r	
is the noncentral-
ity parameter associated with the lack of fit F-test for the fitted model
Ž
.
2
see Khuri and Cornell, 1996, Section 2.6 . Large values of r	
Ž
.
increase the power of the lack of fit test. By formula
8.54 , the
minimum value of B is given by
n
B
s
T,
min
2
	
where Ts
y
 y1 . The fitted model is considered to be
22
12
11
12
inadequate if there exists some constant 0 such that TG.
Show that
inf Ss e
Ty1S ,
Ž
.
min
g
Ž
y1 .
y1
where e
T
S
is the smallest eigenvalue of T
S and  is the
min
 
4
region  TG .
w Note: On the basis of this problem, we can define a new design
Ž
y1 .
criterion, that which maximizes e
T
S with respect to D. A design
min
Ž
chosen according to this criterion is called $ -optimal see Jones and
1
. x
Mitchell, 1978 .
8.11. A second-order model of the form
y x s q x q x q
x 2q
x 2q
x x q
Ž .
0
1
1
2
2
11
1
22
2
12
1
2

OPTIMIZATION IN STATISTICS
400
is fitted using a rotatable central composite design D, which consists of
a factorial 22 portion, an axial portion with an axial parameter s21r2,
and n
center-point replications. The settings of the 22 factorial
0
portion are 1. The region of interest R consists of the interior and
boundary of a circle of radius 21r2 centered at the origin.
( )
a
Express V, the average variance of the predicted response given by
Ž
.
formula 8.52 , as a function of n .
0
( )
b
Can n be chosen so that it minimizes V ?
0
8.12. Suppose that we have r response functions represented by the models
y sX q ,
is1, 2, . . . , r,
i
i
i
where X is a known matrix of order np and rank p. The random
error vectors have the same variancecovariance structure as in Section
Ž .
w
x
w
x
8.7. Let FsE Y sXB, where Ys y :y :  :y
and Bs  :  :  : .
1
2
r
1
2
r
Ž
. Ž
.
Show that the determinant of
YyF  YyF
attains a minimum
ˆ
ˆ
value when BsB, where B is obtained by replacing each  in B with
i
ˆ
y1
Ž
.
Ž
.
 s XX
Xy
is1, 2, . . . , r .
i
i
w
Ž
. Ž
.
Note: The minimization of the determinant of
YyF  YyF
with
respect to B represents a general multiresponse estimation criterion
Ž
known as the BoxDraper determinant criterion see Box and Draper,
. x
1965 .
8.13. Let A be a pp matrix with nonnegative eigenvalues. Show that
det A Fexp tr AyI
.
Ž .
Ž
.
p
w
Ž
.
Note: This inequality is proved in an article by Watson 1964 . It is
Ž
.
based on the simple inequality aFexp ay1 , which can be easily
x
proved for any real number a.
8.14. Let x , x , . . . , x
be a sample of n independently distributed random
1
2
n
Ž
.
vectors from a p-variate normal distribution N , V . The correspond-
ing likelihood function is
n
1
1
y1
Ls
exp y
x y V
x y
.
Ž
.
Ž
.
Ý
i
i
nr2
n pr2
2
2
det V
Ž
.
Ž .
is1
It is known that the maximum likelihood estimate of  is x, where
n
Ž
.
Ž
.
xs 1rn Ý
x
see, for example, Seber, 1984, pages 5961 . Let S be
is1
i
the matrix
n
1
Ss
x yx
x yx .
Ž
. Ž
.
Ý
i
i
n is1

EXERCISES
401
Show that S is the maximum likelihood estimate of V by proving that
n
1
1
y1
exp y
x yx V
x yx
Ž
.
Ž
.
Ý
i
i
nr2
2
det V
Ž .
is1
n
1
1
y1
F
exp y
x yx S
x yx
,
Ž
.
Ž
.
Ý
i
i
nr2
2
det S
Ž .
is1
or equivalently,
n
n
nr2
y1
y1
det SV
exp y
tr SV
Fexp y
tr I
.
Ž
.
Ž
.
Ž
.
p
2
2
w
x
Hint: Use the inequality given in Exercise 8.13.
8.15. Consider the random one-way classification model
y sq q ,
is1, 2, . . . , a; js1, 2, . . . , n ,
i j
i
i j
i
Ž
2.
where the  ’s and  ’s are independently distributed as N 0, 	
and
i
i j

Ž
2.
Ž
.
N 0, 	
. Determine the matrix S and the vector q in equation 8.95

that can be used to obtain the MINQUEs of 	 2 and 	 2.


8.16. Consider the linear model
ysXq ,
where X is a known matrix of order np and rank p, and  is
normally distributed with a zero mean vector and a variancecovari-
2
Ž .
ance matrix 	 I . Let y x denote the predicted response at a point x
ˆ
n
in a region of interest R.
Ž
.
Use Scheffe’s confidence intervals given by formula 8.97 to obtain
´
simultaneous confidence intervals on the mean response values at the
Ž
.
points x , x , . . . , x
mFp in R. What is the joint confidence coeffi-
1
2
m
cient for these intervals?
8.17. Consider the fixed-effects two-way classification model
y
sq q q 
q
,
Ž
. i j
i jk
i
j
i jk
is1, 2, . . . , a; js1, 2, . . . , b; ks1, 2, . . . , m,

OPTIMIZATION IN STATISTICS
402
Ž
.
where 
and  are unknown parameters,

is the interaction
i
j
i j
effect, and 
is a random error that has the normal distribution with
i jk
a zero mean and a variance 	 2.
( )
a
Use Scheffe’s confidence intervals to obtain simultaneous confi-
´
Ž
.
dence intervals on all contrasts among the  ’s, where  sE y
i
i
i. .
b
m
Ž
.
and y s 1rbm Ý
Ý
y
.
i. .
js1
ks1
i jk
( )
b
Identify those y ’s that are influential contributors to the sig-
i. .
nificance of the F-test concerning the hypothesis H :  s s
0
1
2
 s .
a

C H A P T E R
9
Approximation of Functions
The class of polynomials is undoubtedly the simplest class of functions. In
this chapter we shall discuss how to use polynomials to approximate continu-
Ž
.
ous functions. Piecewise polynomial functions splines will also be discussed.
Attention will be primarily confined to real-valued functions of a single
variable x.
9.1. WEIERSTRASS APPROXIMATION
Ž .
We may recall from Section 4.3 that if a function f x has derivatives of all
orders in some neighborhood of the origin, then it can be represented by a
power series of the form Ý
a x n. If  is the radius of convergence of this
ns0
n
 
Ž
series, then the series converges uniformly for
x Fr, where r
see
.
Theorem 5.4.4 . It follows that for a given 0 we can take sufficiently many
Ž .
n
k
terms of this power series and obtain a polynomial p
x sÝ
a x
of
n
ks0
k
 Ž .
Ž .
 
degree n for which
f x yp
x
 for x Fr. But a function that is not
n
differentiable of all orders does not have a power series representation.
w
x
However, if the function is continuous on the closed interval
a, b , then it
can be approximated uniformly by a polynomial. This is guaranteed by the
following theorem:
Ž
.
w
x
Theorem 9.1.1 Weierstrass Approximation Theorem .
Let f: a, b ™R
Ž .
be a continuous function. Then, for any 0, there exists a polynomial p x
such that
w
x
f x yp x

for all xg a, b .
Ž .
Ž .
w
x
Proof. Without loss of generality we can consider a, b to be the interval
w
x
0, 1 . This can always be achieved by making a change of variable of the form
xya
ts
.
bya
403

APPROXIMATION OF FUNCTIONS
404
As x varies from a to b, t varies from 0 to 1. Thus, if necessary, we consider
that such a linear transformation has been made and that t has been
renamed as x.
Ž .
For each n, let b
x be defined as a polynomial of degree n of the form
n
n
k
n
nyk
k
b
x s
x
1yx
f
,
9.1
Ž .
Ž
.
Ž
.
Ý
n
ž /
ž /
k
n
ks0
where
n!
n s
.
ž /
k
k! nyk !
Ž
.
We have that
n
n
nyk
k
x
1yx
s1,
9.2
Ž
.
Ž
.
Ý ž /
k
ks0
n
k
n
nyk
k
x
1yx
sx,
9.3
Ž
.
Ž
.
Ý ž /
k
n
ks0
n
2
k
1
x
n
nyk
k
2
x
1yx
s 1y
x q
.
9.4
Ž
.
Ž
.
Ý
2 ž /
ž
/
k
n
n
n
ks0
These identities can be shown as follows: Let Y
be a binomial random
n
Ž
.
variable B n, x . Thus Y
represents the number of successes in a sequence
n
of n independent Bernoulli trials with x the probability of success on a single
Ž
.
Ž
.
Ž
. Ž
trial. Hence, E Y
snx and Var Y
snx 1yx
see, for example, Harris,
n
n
.
1966, page 104; see also Exercise 5.30 . It follows that
n
n
nyk
k
x
1yx
sP 0FY Fn s1.
9.5
Ž
.
Ž
.
Ž
.
Ý
n
ž /
k
ks0
Furthermore,
n
n
nyk
k
k
x
1yx
sE Y
snx,
9.6
Ž
.
Ž
.
Ž
.
Ý
n
ž /
k
ks0
n
2
n
nyk
2
k
2
k
x
1yx
sE Y
sVar Y
q E Y
Ž
.
Ž
.
Ž
.
Ž
.
Ý
n
n
n
ž /
k
ks0
1
x
2
2
2
2
snx 1yx qn x sn
1y
x q
.
9.7
Ž
.
Ž
.
ž
/
n
n
Ž
. Ž
.
Ž
. Ž
.
Identities 9.2  9.4 follow directly from 9.5  9.7 .

WEIERSTRASS APPROXIMATION
405
Ž .
Ž .
Let us now consider the difference f x yb
x , which with the help of
n
Ž
.
identity 9.2 can be written as
n
k
n
nyk
k
f x yb
x s
f x yf
x
1yx
.
9.8
Ž .
Ž .
Ž .
Ž
.
Ž
.
Ý
n
ž /
ž /
k
n
ks0
Ž .
w
x
Since f x
is continuous on 0, 1 , then it must be bounded and uniformly
Ž
.
continuous there see Theorems 3.4.5 and 3.4.6 . Hence, for the given 0,
there exist numbers  and m such that

f x
yf x
F
if
x yx

Ž
.
Ž
.
1
2
1
2
2
and
w
x
f x
m
for all xg 0, 1 .
Ž .
Ž
.
From formula 9.8 we then have
n
k
n
nyk
k
f x yb
x
F
f x yf
x
1yx
.
Ž .
Ž .
Ž .
Ž
.
Ý
n
ž /
ž /
k
n
ks0


 Ž .
Ž
.
 Ž .
If
xykrn , then
f x yf krn
r2; otherwise, we have
f x y
Ž
.
Ž
. Ž
.
f krn
2m for 0FxF1. Consequently, by using identities 9.2  9.4 we
obtain

n
nyk
k
f x yb
x
F
x
1yx
Ž .
Ž .
Ž
.
Ý
n
ž /
k
2


xykrn 
n
nyk
k
q
2m
x
1yx
Ž
.
Ý
ž /
k


xykrn G
2

krnyx
Ž
.
n
nyk
k
F
q2m
x
1yx
Ž
.
Ý
2 ž /
k
2
krnyx
Ž
.


xykrn G
2

2m
k
n
nyk
k
F
q
yx
x
1yx
Ž
.
Ý
2
ž /
ž
/
k
2
n



xykrn G
n
2

2m
k
2kx
n
nyk
2
k
F
q
y
qx
x
1yx
Ž
.
Ý
2
2
ž /
ž
/
k
2
n

n
ks0

2m
1
x
2
2
2
F
q
1y
x q
y2 x qx
.
2 ž
/
2
n
n


APPROXIMATION OF FUNCTIONS
406
Hence,

2m x 1yx
Ž
.
f x yb
x
F
q
Ž .
Ž .
n
2
2
n


m
F
q
,
9.9
Ž
.
2
2
2n
1
Ž
.
since x 1yx F
for 0FxF1. By choosing n large enough that
4
m


,
2
2
2n
we conclude that
f x yb
x

Ž .
Ž .
n
w
x
Ž .
Ž .
for all xg 0, 1 . The proof of the theorem follows by taking p x sb
x .
n

Ž .
w
x
Ž .
Definition 9.1.1.
Let f x
be defined on
0, 1 . The polynomial b
x
n
Ž
.
defined by formula 9.1 is called the Bernstein polynomial of degree n for
Ž .
f x .


Ž .4
By the proof of Theorem 9.1.1 we conclude that the sequence
b
x
n
ns1
Ž .
w
x
of Bernstein polynomials converges uniformly to f x on 0, 1 . These polyno-
mials are useful in that they not only prove the existence of an approximating
Ž .
polynomial for f x , but also provide a simple explicit representation for it.
Ž .
Another advantage of Bernstein polynomials is that if f x
is continuously
w
x
Ž .
differentiable on 0, 1 , then the derivative of b
x converges also uniformly
n
Ž .
to f x . A more general statement is given by the next theorem, whose proof
Ž
.
can be found in Davis 1975, Theorem 6.3.2, page 113 .
Ž .
w
x
Theorem 9.1.2.
Let f x
be p times differentiable on 0, 1 . If the pth
derivative is continuous there, then
d pb
x
d pf x
Ž .
Ž .
n
lim
s
p
p
dx
dx
n™
w
x
uniformly on 0, 1 .

Ž .4
Obviously, the knowledge that the sequence
b
x
converges uni-
n
ns1
Ž .
w
x
formly to f x on 0, 1 is not complete without knowing something about the
rate of convergence. For this purpose we need to define the so-called
Ž .
w
x
modulus of continuity of f x on a, b .

WEIERSTRASS APPROXIMATION
407
Ž .
w
x
Definition 9.1.2.
If f x
is continuous on a, b , then, for any 0, the
Ž .
w
x
modulus of continuity of f x on a, b is

  s
sup
f x
yf x
,
Ž
.
Ž
.
Ž
.
1
2


x yx
F
1
2
w
x
where x and x
are points in a, b .

1
2
On the basis of Definition 9.1.2 we have the following properties concern-
ing the modulus of continuity:
Ž
.
Ž
.
Lemma 9.1.1.
If 0 F , then 
 
F
 
.
1
2
1
2
Ž .
w
x
Lemma 9.1.2.
For a function f x to be uniformly continuous on a, b it
Ž .
is necessary and sufficient that lim

  s0.
 ™0
The proofs of Lemmas 9.1.1 and 9.1.2 are left to the reader.
Ž
.
Ž
.
Ž .
Lemma 9.1.3.
For any 0, 
  F q1 
  .
Proof. Suppose that 0 is given. We can find an integer n such that
Ž
.
wŽ
. x
nFnq1. By Lemma 9.1.1, 
  F
 nq1  . Let x
and x
be two
1
2
w
x


Ž
.
points in a, b such that x x
and x yx
F nq1 . Let us also divide
1
2
1
2
w
x
Ž
. Ž
.
the interval
x , x
into nq1 equal parts, each of length
x yx r nq1 ,
1
2
2
1
by means of the partition points
x yx
Ž
.
2
1
y sx qi
,
is0, 1, . . . , nq1.
i
1
nq1
Then
f x
yf x
s f x
yf x
Ž
.
Ž
.
Ž
.
Ž
.
1
2
2
1
n
s
f y
yf y
Ž
.
Ž
.
Ý
iq1
i
is0
n
F
f y
yf y
Ž
.
Ž
.
Ý
iq1
i
is0
F nq1 
  ,
Ž
.
Ž
.


w
Ž
.x

since y
yy s 1r nq1
x yx
F for is0, 1, . . . , n. It follows that
iq1
i
2
1

nq1  s
sup
f x
yf x
F nq1 
  .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
1
2


Ž
.
x yx
F nq1 
1
2

APPROXIMATION OF FUNCTIONS
408
Consequently,

  F
nq1  F nq1 
 
Ž
.
Ž
.
Ž
.
Ž
.
F q1 
  .

Ž
.
Ž
.
Ž .
w
x
Ž .
Theorem 9.1.3.
Let f x
be continuous on 0, 1 , and let b
x
be the
n
Ž
.
Bernstein polynomial defined by formula 9.1 . Then
3
1
f x yb
x
F

Ž .
Ž .
n
ž
/
'
2
n
w
x
Ž .
Ž .
w
x
for all xg 0, 1 , where 
 
is the modulus of continuity of f x on 0, 1 .
Ž
.
Ž
.
Proof. Using formula 9.1 and identity 9.2 , we have that
n
k
n
nyk
k
f x yb
x
s
f x yf
x
1yx
Ž .
Ž .
Ž .
Ž
.
Ý
n
ž /
ž /
k
n
ks0
n
k
n
nyk
k
F
f x yf
x
1yx
Ž .
Ž
.
Ý
ž /
ž /
k
n
ks0
n
k
n
nyk
k
F

xy
x
1yx
.
Ž
.
Ý
ž /
ž
/
k
n
ks0
Now, by applying Lemma 9.1.3 we can write
k
k
1r2
y1r2

xy
s
 n
xy
n
ž
/
ž
/
n
n
k
1r2
y1r2
F 1qn
xy

 n
.
Ž
.
ž
/
n
Thus
n
k
n
nyk
1r2
y1r2
k
f x yb
x
F
1qn
xy

 n
x
1yx
Ž .
Ž .
Ž
.
Ž
.
Ý
n
ž /
ž
/
k
n
ks0
n
k
n
nyk
y1r2
1r2
k
F
 n
1qn
xy
x
1yx
.
Ž
.
Ž
.
Ý
ž /
k
n
ks0

WEIERSTRASS APPROXIMATION
409
Ž
.
But, by the CauchySchwarz inequality
see part 1 of Theorem 2.1.2 , we
have
n
k
n
nyk
k
xy
x
1yx
Ž
.
Ý
ž /
k
n
ks0
1r2
1r2
n
k
n
n
nyk
nyk
k
k
s
xy
x
1yx
x
1yx
Ž
.
Ž
.
Ý
ž /
ž /
k
k
n
ks0
1r2
1r2
2
n
n
k
n
n
nyk
nyk
k
k
F
xy
x
1yx
x
1yx
Ž
.
Ž
.
Ý
Ý
ž /
ž /
ž
/
k
k
n
ks0
ks0
1r2
2
n
k
n
nyk
k
s
xy
x
1yx
,
by identity 9.2
Ž
.
Ž
.
Ý
ž /
ž
/
k
n
ks0
1r2
1
x
2
2
2
s x y2 x q 1y
x q
,
by identities
9.3 and 9.4
Ž
.
Ž
.
ž
/
n
n
1r2
x 1yx
Ž
.
s
n
1r2
1
1
F
,
since x 1yx F .
Ž
.
4
4n
It follows that
1r2
1
y1r2
1r2
f x yb
x
F
 n
1qn
,
Ž .
Ž .
Ž
.
n
ž /
4n
that is,
3
y1r2
f x yb
x
F 
 n
Ž .
Ž .
Ž
.
n
2
w
x
for all xg 0, 1 .

We note that Theorem 9.1.3 can be used to prove Theorem 9.1.1 as
Ž .
w
x
Ž .
follows: If f x is continuous on 0, 1 , then f x is uniformly continuous on
w
x
Ž
y1r2 .
0, 1 . Hence, by Lemma 9.1.2, 
 n
™0 as n™.
Ž .
Ž
.
Corollary 9.1.1.
If f x is a Lipschitz continuous function Lip K, 
on
w
x
0, 1 , then
3
y r2
f x yb
x
F Kn
9.10
Ž .
Ž .
Ž
.
n
2
w
x
for all xg 0, 1 .

APPROXIMATION OF FUNCTIONS
410
Proof. By Definition 3.4.6,



f x
yf x
FK x yx
Ž
.
Ž
.
1
2
1
2
w
x
for all x , x
in 0, 1 . Thus
1
2

  FK .
Ž
.
By Theorem 9.1.3 we then have
3
y r2
f x yb
x
F Kn
Ž .
Ž .
n
2
w
x
for all xg 0, 1 .

Ž
.
Ž .
w
x
Theorem 9.1.4 Voronovsky’s Theorem .
If f x is bounded on 0, 1 and
w
x
has a second-order derivative at a point x
in 0, 1 , then
0
1
lim n b
x
yf x
s x
1yx
f
x
.
Ž
.
Ž
.
Ž
.
Ž
.
n
0
0
0
0
0
2
n™
Ž
.
Proof. See Davis 1975, Theorem 6.3.6, page 117 .

We note from Corollary 9.1.1 and Voronovsky’s theorem that the conver-
Ž .
gence of Bernstein polynomials can be very slow. For example, if f x
w
x
satisfies the conditions of Voronovsky’s theorem, then at every point xg 0, 1
Ž .
Ž .
Ž .
where f x 0, b
x converges to f x just like crn, where c is a constant.
n
1
'
Ž .
Ž
.
EXAMPLE 9.1.1.
We recall from Section 3.4.2 that f x s x is Lip 1, 2
for xG0. Then, by Corollary 9.1.1,
3
'x yb
x
F
,
Ž .
n
1r4
2n
for 0FxF1, where
1r2
n
k
n
nyk
k
b
x s
x
1yx
.
Ž .
Ž
.
Ý
n
ž /
ž /
k
n
ks0
9.2. APPROXIMATION BY POLYNOMIAL INTERPOLATION
Ž .
Ž .
One possible method to approximate a function f x with a polynomial p x
Ž .
Ž .
is to select such a polynomial so that both f x
and p x
have the same
Ž .
values at a certain number of points in the domain of f x . This procedure is
Ž .
Ž .
called interpolation. The rationale behind it is that if f x agrees with p x
at some known points, then the two functions should be close to one another
at intermediate points.
Let us first consider the following result given by the next theorem.

APPROXIMATION BY POLYNOMIAL INTERPOLATION
411
Theorem 9.2.1.
Let a , a , . . . , a be nq1 distinct points in R, the set of
0
1
n
real numbers. Let b , b , . . . , b be any given set of nq1 real numbers. Then,
0
1
n
Ž .
Ž
.
there exists a unique polynomial p x
of degree Fn such that p a sb ,
i
i
is0, 1, . . . , n.
Ž .
Proof. Since p x is a polynomial of degree Fn, it can be represented as
Ž .
n
j
p x sÝ
c x . We must then have
js0
j
n
j
c a sb ,
is0, 1, . . . , n.
Ý
j
i
i
js0
These equations can be written in vector form as
2
n
1
a
a

a
c
b
0
0
0
0
0
2
n
1
a
a

a
c
b
1
1
1
1
1
s
.
9.11
.
.
.
.
.
.
Ž
.
.
.
.
.
.
.
.
.
.
.
.
.
2
n
1
a
a

a
c
b
n
n
n
n
n
Ž
.
Ž
.
The determinant of the
nq1  nq1 matrix on the left side of equation
Ž
.
n
Ž
.
9.11 is known as Vandermonde’s determinant and is equal to Ł
a ya .
i j
i
j
Ž
The proof of this last assertion can be found in, for example, Graybill 1983,
.
Theorem 8.12.2, page 266 . Since the a ’s are distinct, this determinant is
i
different from zero. It follows that this matrix is nonsingular. Hence, equa-
Ž
.
tion 9.11 provides a unique solution for c , c , . . . , c .

0
1
n
Ž .
Corollary 9.2.1.
The polynomial p x
in Theorem 9.2.1 can be repre-
sented as
n
p x s
b l
x ,
9.12
Ž .
Ž .
Ž
.
Ý
i i
is0
where
n
xyaj
l
x s
,
is0, 1, . . . , n.
9.13
Ž .
Ž
.
Ł
i
a ya
js0
i
j
ji
Ž .
Ž
.
Proof. We have that l
x
is a polynomial of degree n
is0, 1, . . . , n .
i
Ž
.
Ž
.
Ž
.
Furthermore, l a s0 if ij, and l a s1 is0, 1, . . . , n . It follows that
i
j
i
i
n
Ž .
Ý
b l
x
is
a
polynomial of degree
Fn
and
assumes
the
values
is0
i i
b , b , . . . , b
at a , a , . . . , a , respectively. This polynomial is unique by
0
1
n
0
1
n
Theorem 9.2.1.

Ž
.
Definition 9.2.1.
The polynomial defined by formula
9.12
is called a
Lagrange interpolating polynomial. The points a , a , . . . , a
are called points
0
1
n

APPROXIMATION OF FUNCTIONS
412
Ž
.
Ž .
Ž
.
of interpolation
or nodes , and l
x
in formula
9.13
is called the ith
i
Lagrange polynomial associated with the a ’s.

i
Ž
.
The values b , b , . . . , b
in formula
9.12
are frequently the values of
0
1
n
Ž .
Ž .
some function f x at the points a , a , . . . , a . Thus f x and the polynomial
0
1
n
Ž .
Ž
.
p x in formula 9.12 attain the same values at these points. The polynomial
Ž .
p x , which can be written as
n
p x s
f a
l
x ,
9.14
Ž .
Ž
. Ž .
Ž
.
Ý
i
i
is0
Ž .
w
x
provides therefore an approximation for f x over a , a .
0
n
Ž .
1r2
EXAMPLE 9.2.1.
Consider the function f x sx
. Let a s60, a s70,
0
1
a s85, a s105 be interpolation points. Then
2
3
p x s7.7460l
x q8.3666l
x q9.2195l
x q10.2470l
x ,
Ž .
Ž .
Ž .
Ž .
Ž .
0
1
2
3
where
xy70
xy85
xy105
Ž
. Ž
. Ž
.
l
x s
,
Ž .
0
60y70
60y85
60y105
Ž
. Ž
. Ž
.
xy60
xy85
xy105
Ž
. Ž
. Ž
.
l
x s
,
Ž .
1
70y60
70y85
70y105
Ž
. Ž
. Ž
.
xy60
xy70
xy105
Ž
. Ž
. Ž
.
l
x s
,
Ž .
2
85y60
85y70
85y105
Ž
. Ž
. Ž
.
xy60
xy70
xy85
Ž
. Ž
. Ž
.
l
x s
.
Ž .
3
105y60
105y70
105y85
Ž
. Ž
. Ž
.
( )
1rrrrr2
Table 9.1. Approximation of f x sx
by the Lagrange Interpolating
( )
Polynomial p x
Ž .
Ž .
x
f x
p x
60
7.74597
7.74597
64
8.00000
7.99978
68
8.24621
8.24611
70
8.36660
8.36660
74
8.60233
8.60251
78
8.83176
8.83201
82
9.05539
9.05555
85
9.21954
9.21954
90
9.48683
9.48646
94
9.69536
9.69472
98
9.89949
9.89875
102
10.09950
10.09899
105
10.24695
10.24695

APPROXIMATION BY POLYNOMIAL INTERPOLATION
413
Ž .
Ž .
w
x
Using p x as an approximation of f x over the interval 60, 105 , tabulated
Ž .
Ž .
values of f x and p x were obtained at several points inside this interval.
The results are given in Table 9.1.
9.2.1. The Accuracy of Lagrange Interpolation
Let us now address the question of evaluating the accuracy of Lagrange
interpolation. The answer to this question is given in the next theorem.
Ž .
Theorem 9.2.2.
Suppose that f x
has n continuous derivatives on the
w
x
Ž
.
Ž
.
interval a, b , and its
nq1 st derivative exists on a, b . Let asa a 
0
1
w
x
Ž .
 a sb be nq1 points in
a, b . If p x
is the Lagrange interpolating
n
Ž
.
Ž
.
polynomial defined by formula 9.14 , then there exists a point cg a, b such
w
x
Ž
.
that for any xg a, b , xa
is0, 1, . . . , n ,
i
1
Žnq1.
f x yp x s
f
c g
x ,
9.15
Ž .
Ž .
Ž .
Ž .
Ž
.
nq1
nq1 !
Ž
.
where
n
g
x s
xya
.
Ž .
Ž
.
Ł
nq1
i
is0
Ž .
Proof. Define the function h t as
g
tŽ .
nq1
h t sf t yp t y f x yp x
.
Ž .
Ž .
Ž .
Ž .
Ž .
g
x
Ž .
nq1
Ž .
Ž
.
If tsx, then h x s0. For tsa
is0, 1, . . . , n ,
i
g
a
Ž
.
nq1
i
h a
sf a
yp a
y f x yp x
s0.
Ž
.
Ž
.
Ž
.
Ž .
Ž .
i
i
i
g
x
Ž .
nq1
Ž .
w
x
Ž
.
The function h t
has n continuous derivatives on
a, b , and its
nq1 st
Ž
.
Ž .
derivative exists on
a, b . Furthermore, h t
vanishes at x and at all nq1
w
x
interpolation points, that is, it has at least nq2 different zeros in a, b . By
Ž
.
Ž .
Rolle’s theorem Theorem 4.2.1 , h t
vanishes at least once between any
Ž .
Ž
.
two zeros of h t and thus has at least nq1 different zeros in a, b . Also by
Ž .
Ž
.
Rolle’s theorem, h t has at least n different zeros in
a, b . By continuing
Žnq1.Ž .
Ž
.
this argument, we see that h
t has at least one zero in a, b , say at the
point c. But,
f x yp x
Ž .
Ž .
Žnq1.
Žnq1.
Žnq1.
Žnq1.
h
t sf
t yp
t y
g
t
Ž .
Ž .
Ž .
Ž .
nq1
g
x
Ž .
nq1
f x yp x
Ž .
Ž .
Žnq1.
sf
t y
nq1 !,
Ž .
Ž
.
g
x
Ž .
nq1

APPROXIMATION OF FUNCTIONS
414
Ž .
Ž .
since p t is a polynomial of degree Fn and g
t is a polynomial of the
nq1
form t nq1 q t nq t ny1 q  q
for suitable constants  ,  , . . . , 
.
1
2
nq1
1
2
nq1
We thus have
f x yp x
Ž .
Ž .
Žnq1.
f
c y
nq1 !s0,
Ž .
Ž
.
g
x
Ž .
nq1
Ž
.
from which we can conclude formula 9.15 .

Žnq1.Ž .
w
x
Corollary 9.2.2.
Suppose that
f
x
is continuous on
a, b . Let

Žnq1.Ž .

Ž .

ssup
f
x
, 
ssup
g
x
. Then
nq1
aF xF b
nq1
aF xF b
nq1


nq1
nq1
sup
f x yp x
F
.
Ž .
Ž .
nq1 !
Ž
.
aFxFb
Ž
.
Ž .
Proof. This follows directly from formula 9.15 and the fact that f x y
Ž .
p x s0 for xsa , a , . . . , a .

0
1
n

Ž .

n
Ž
.
We note that 
, being the supremum of
g
x
s Ł
xya
nq1
nq1
is0
i
w
x
over a, b , is a function of the location of the a ’s. From Corollary 9.2.2 we
i
can then write
 a , a , . . . , a
Ž
.
0
1
n
Žnq1.
sup
f x yp x
F
sup
f
x
,
9.16
Ž .
Ž .
Ž .
Ž
.
nq1 !
Ž
.
aFxFb
aFxFb
Ž
.

Ž .
where  a , a , . . . , a
ssup
g
x
. This inequality provides us with
0
1
n
aF xF b
nq1
Ž .
Ž .
an upper bound on the error of approximating f x
with p x
over the
interpolation region. We refer to this error as interpolation error. The upper
bound clearly shows that the interpolation error depends on the location of
the interpolation points.
Corollary 9.2.3.
If, in Corollary 9.2.2, ns2, and if a ya sa ya s,
1
0
2
1
then
'3
3
sup
f x yp x
F
  .
Ž .
Ž .
3
27
aFxFb
Ž .
Ž
.Ž
.Ž
.
Proof. Consider g
x s xya
xya
xya , which can be written as
3
0
1
2
Ž .
Ž
2
2.
g
x sz z y
, where zsxya . This function is symmetric with respect
3
1

Ž .
to xsa . It is easy to see that
g
x
attains an absolute maximum over
1
3
'
a FxFa , or equivalently, yFzF, when zsr 3 . Hence,
0
2
2
2
 s
sup
g
x
s
max
z z y
Ž .
Ž
.
3
3
yFzF
a FxFa
0
2
2


2
2
3
s
y
s
 .
ž
/
'
'
3
3
3 3

APPROXIMATION BY POLYNOMIAL INTERPOLATION
415
By applying Corollary 9.2.2 we obtain
'3
3
sup
f x yp x
F
  .
Ž .
Ž .
3
27
a FxFa
0
2
We have previously noted that the interpolation error depends on the choice
of the interpolation points. This leads us to the following important question:
How can the interpolation points be chosen so as to minimize the interpola-
Ž
.
tion error? The answer to this question lies in inequality 9.16 . One reason-
able criterion for the choice of interpolation points is the minimization of
Ž
.
 a , a , . . . , a
with respect to a , a , . . . , a . It turns out that the optimal
0
1
n
0
1
n
locations of a , a , . . . , a are given by the zeros of the Chebyshev polynomial
0
1
n
Ž
.
Ž
.
of the first kind of degree nq1 see Section 10.4.1 .

Definition 9.2.2.
The Chebyshev polynomial of degree n is defined by
T
x scos n Arccos x
Ž .
Ž
.
n
2
n
n
n
ny2
2
ny4
2
sx q
x
x y1 q
x
x y1
q  ,
Ž
.
Ž
.
ž /
ž /
2
4
ns0, 1, . . . . 9.17
Ž
.
Ž .
Obviously, by the definition of T
x , y1FxF1. One of the properties of
n
Ž .
T
x is that it has simple zeros at the following n points:
n
2iy1
Ž
.
 scos
 ,
is1, 2, . . . , n.

i
2n
Ž
.
The proof of this property is given in Davis 1975, pages 6162 .
w
x
We can consider Chebyshev polynomials defined on the interval a, b by
making the transformation
aqb
bya
xs
q
t,
2
2
which transforms the interval y1FtF1 into the interval aFxFb. In this
case, the zeros of the Chebyshev polynomial of degree n over the interval
w
x
a, b are given by
aqb
bya
2iy1
z s
q
cos
 ,
is1, 2, . . . , n.
i
ž
/
2
2
2n
We refer to the z ’s as Chebyshev points. These points can be obtained
i
w
x
geometrically by subdividing the semicircle over the interval
a, b
into n

APPROXIMATION OF FUNCTIONS
416
Ž
equal arcs and then projecting the midpoint of each arc onto the interval see
.
De Boor, 1978, page 26 .
Chebyshev points have a very interesting property that pertains to the
Ž
.
Ž
.
minimization of  a , a , . . . , a
in inequality
9.16 . This property is de-
0
1
n
Ž
scribed in Theorem 9.2.3, whose proof can be found in Davis 1975, Section
.
Ž
.
3.3 ; see also De Boor 1978, page 30 .
Theorem 9.2.3.
The function
n
 a , a , . . . , a
s sup

xya
,
Ž
.
Ž
.
0
1
n
i
is0
aFxFb
w
x
where the a ’s belong to the interval a, b , achieves its minimum at the zeros
i
of the Chebyshev polynomial of degree nq1, that is, at
aqb
bya
2iq1
z s
q
cos
 ,
is0, 1, . . . , n,
9.18
Ž
.
i
ž
/
2
2
2nq2
and
nq1
2 bya
Ž
.
min
 a , a , . . . , a
s
.
Ž
.
0
1
n
nq1
4
a , a , . . . , a
0
1
n
Ž
.
From Theorem 9.2.3 and inequality 9.16 we conclude that the choice of
Ž
.
the Chebyshev points given in formula
9.18
is optimal in the sense of
reducing the interpolation error. In other words, among all sets of interpo-
lation points of size nq1 each, Chebyshev points produce a Lagrange
Ž .
w
x
polynomial approximation for f x
over the interval
a, b with a minimum
Ž
.
upper bound on the error of approximation. Using inequality
9.16 , we
obtain the following interesting result:
nq1
2
bya
Žnq1.
sup
f x yp x
F
sup
f
x
.
9.19
Ž .
Ž .
Ž .
Ž
.
ž
/
nq1 !
4
Ž
.
aFxFb
aFxFb
The use of Chebyshev points in the construction of Lagrange interpolating
Ž .
Ž .
w
x
polynomial p x for the function f x over a, b produces an approximation
which, for all practical purposes, differs very little from the best possible
Ž .
approximation of f x by a polynomial of the same degree. This was shown
Ž
.
Ž .
by Powell
1967 . More explicitly, let
p* x
be the best approximating
Ž .
Ž .
w
x
polynomial of f x of the same degree as p x over a, b . Then, obviously,
sup
f x yp* x
F sup
f x yp x
.
Ž .
Ž .
Ž .
Ž .
aFxFb
aFxFb
Ž
.
De Boor 1978, page 31 pointed out that for nF20,
sup
f x yp x
F4 sup
f x yp* x
.
Ž .
Ž .
Ž .
Ž .
aFxFb
aFxFb

APPROXIMATION BY POLYNOMIAL INTERPOLATION
417
This indicates that the error of interpolation which results from the use of
Lagrange polynomials in combination with Chebyshev points does not exceed
the minimum approximation error by more than a factor of 4 for nF20. This
is a very useful result, since the derivation of the best approximating
polynomial can be tedious and complicated, whereas a polynomial approxi-
mation obtained by Lagrange interpolation that uses Chebyshev points as
interpolation points is simple and straightforward.
9.2.2. A Combination of Interpolation and Approximation
In Section 9.1 we learned how to approximate a continuous function f:
w
x
a, b ™R with a polynomial by applying the Weierstrass theorem. In this
w
x
section we have seen how to interpolate values of f on
a, b
by using
Lagrange polynomials. We now show that these two processes can be
combined. More specifically, suppose that we are given nq1 distinct points
w
x
in a, b , which we denote by a , a , . . . , a
with a sa and a sb. Let 0
0
1
n
0
n
Ž .
 Ž .
Ž .
be given. We need to find a polynomial q x such that f x yq x
 for
w
x
Ž
.
Ž
.
all x in a, b , and f a sq a , is0, 1, . . . , n.
i
i
Ž .
By Theorem 9.1.1 there exists a polynomial p x such that
w
x
f x yp x

for all xg a, b ,
Ž .
Ž .
Ž
.
where r 1qM , and M is a nonnegative number to be described later.
Ž .
Furthermore, by Theorem 9.2.1 there exists a unique polynomial u x
such
that
u a
sf a
yp a
,
is0, 1, . . . , n.
Ž
.
Ž
.
Ž
.
i
i
i
This polynomial is given by
n
u x s
f a
yp a
l
x ,
9.20
Ž .
Ž
.
Ž
.
Ž .
Ž
.
Ý
i
i
i
is0
Ž .
Ž
.
where l
x is the ith Lagrange polynomial defined in formula 9.13 . Using
i
Ž
.
formula 9.20 we obtain
n
max
u x
F
f a
yp a
max
l
x
Ž .
Ž
.
Ž
.
Ž .
Ý
i
i
i
aFxFb
aFxFb
is0
FM,
n
 Ž .
where MsÝ
max
l
x
, which is some finite nonnegative number.
is0
aF xF b
i
w
x
Ž .
Note that M depends only on a, b and a , a , . . . , a . Now, define q x as
0
1
n
Ž .
Ž .
Ž .
q x sp x qu x . Then
q a
sp a
qu a
sf a
,
is0, 1, . . . , n.
Ž
.
Ž
.
Ž
.
Ž
.
i
i
i
i

APPROXIMATION OF FUNCTIONS
418
Furthermore,
f x yq x
F f x yp x
q u x
Ž .
Ž .
Ž .
Ž .
Ž .
qM
w
x

for all xg a, b .
9.3. APPROXIMATION BY SPLINE FUNCTIONS
Ž .
Ž .
Approximation of a continuous function f x with a single polynomial p x
Ž .
may not be quite adequate in situations in which f x
represents a real
physical relationship. The behavior of such a function in one region may be
unrelated to its behavior in another region. This type of behavior may not be
satisfactorily matched by any polynomial. This is attributed to the fact that
the behavior of a polynomial everywhere is governed by its behavior in any
small region. In such situations, it would be more appropriate to partition the
Ž .
domain of f x into several intervals and then use a different approximating
polynomial, usually of low degree, in each subinterval. These polynomial
segments can be joined in a smooth way, which leads to what is called a
piecewise polynomial function.
By definition, a spline function is a piecewise polynomial of degree n. The
Ž
.
various polynomial segments all of degree n are joined together at points
called knots in such a way that the entire spline function is continuous and its
first ny1 derivatives are also continuous. Spline functions were first intro-
Ž
.
duced by Schoenberg 1946 .
9.3.1. Properties of Spline Functions
w
x
Let a, b can be interval, and let as     
sb be parti-
0
1
m
mq1
w
x
Ž .
tion points in
a, b . A spline function s x
of degree n with knots at the
points  ,  , . . . , 
has the following properties:
1
2
m
Ž .
i. s x
is a polynomial of degree not exceeding n on each subinterval
w
x

,  , 1FiFmq1.
iy1
i
Ž .
w
x
ii. s x has continuous derivatives up to order ny1 on a, b .
In particular, if ns1, then the spline function is called a linear spline and
can be represented as
m


s x s
a xy ,
Ž .
Ý
i
i
is1
where a , a , . . . , a
are fixed numbers. We note that between any two knots,
1
2
m


xy , is1, 2, . . . , m, represents a straight-line segment. Thus the graph of
i
Ž .
s x is made up of straight-line segments joined at the knots.

APPROXIMATION BY SPLINE FUNCTIONS
419
We can obtain a linear spline that resembles Lagrange interpolation: Let
 ,  , . . . , 
be given real numbers. For 1FiFm, consider the functions
0
1
mq1
xy 1
°
,
 FxF ,
0
1
~  y
l
x s
0
1
Ž .
0
¢0,
 FxF
,
1
mq1
°
w
x
0,
x 
, 
,
iy1
iq1
xyiy1 ,

FxF ,
iy1
i
~  y
l
x s
Ž .
i
iy1
i

yx
iq1
,
 FxF
,
i
iq1
¢
y
iq1
i
0,
 FxF ,
°
0
m
xy
~
m
l
x s
Ž .
mq1
,
 FxF
.
m
mq1
¢
y
mq1
m
Then the linear spline
mq1
s x s
 l
x
9.21
Ž .
Ž .
Ž
.
Ý
i i
is0
Ž .
has the property that s  s , 0FiFmq1. It can be shown that the linear
i
i
spline having this property is unique.
Another special case is the cubic spline for ns3. This is a widely used
Ž .
Ž .
spline function in many applications. It can be represented as s x ss x s
i
a qb xqc x 2qd x 3, 
FxF , is1, 2, . . . , mq1, such that for is
i
i
i
i
iy1
i
1, 2, . . . , m,
s 
ss

,
Ž
.
Ž
.
i
i
iq1
i
s
 
ss

,
Ž
.
Ž
.
i
i
iq1
i
s 
ss

.
Ž
.
Ž
.
i
i
iq1
i
In general, a spline of degree n with knots at  ,  , . . . , 
is represented
1
2
m
as
m
n
s x s
e
xy
qp x ,
9.22
Ž .
Ž
.
Ž .
Ž
.
Ý
i
i
q
is1

APPROXIMATION OF FUNCTIONS
420
Ž .
where e , e , . . . , e
are constants, p x is a polynomial of degree n, and
1
2
m
n
xy
,
xG ,
Ž
.
n
i
i
xy
s
Ž
.
i
q ½ 0,
xF .
i
For an illustration, consider the cubic spline
a qb xqc x 2qd x 3,
aFxF ,
1
1
1
1
s x s
Ž .
2
3
½ a qb xqc x qd x ,
FxFb.
2
2
2
2
Ž .
Here, s x along with its first and second derivatives must be continuous at
xs. Therefore, we must have
a qb qc  2qd  3sa qb qc  2qd  3,
9.23
Ž
.
1
1
1
1
2
2
2
2
b q2c q3d  2sb q2c q3d  2,
9.24
Ž
.
1
1
1
2
2
2
2c q6d s2c q6d  .
9.25
Ž
.
1
1
2
2
Ž
.
Equation 9.25 can be written as
c yc s3 d yd
 .
9.26
Ž
.
Ž
.
1
2
2
1
Ž
.
Ž
.
From equations 9.24 and 9.26 we get
b yb q3 d yd
 2s0.
9.27
Ž
.
Ž
.
1
2
2
1
Ž
.
Ž
.
Ž
.
Using now equations 9.26 and 9.27 in equation 9.23 , we obtain a ya q
1
2
Ž
.
3
Ž
.
3
Ž
.
3
3 d yd  q3 d yd  q d yd  s0, or equivalently,
1
2
2
1
1
2
a ya q d yd
 3s0.
9.28
Ž
.
Ž
.
1
2
1
2
We conclude that
1
1
1
d yd s
a ya
s
b yb
s
c yc
.
9.29
Ž
.
Ž
.
Ž
.
Ž
.
2
1
1
2
2
1
1
2
3
2
3

3
Ž .
Ž
.
Let us now express s x in the form given by equation 9.22 , that is,
3
2
3
s x se
xy
q q xq x q x .
Ž .
Ž
. q
1
0
1
2
3
In this case,
 sa ,
 sb ,
 sc ,
 sd ,
0
1
1
1
2
1
3
1

APPROXIMATION BY SPLINE FUNCTIONS
421
and
ye  3q sa ,
9.30
Ž
.
1
0
2
3e  2q sb ,
9.31
Ž
.
1
1
2
y3e q sc ,
9.32
Ž
.
1
2
2
e q sd .
9.33
Ž
.
1
3
2
Ž
.
Ž
. Ž
.
In light of equation
9.29 , equations
9.30  9.33
have a common solu-
Ž
.Ž
.
Ž
2.Ž
.
tion for
e
given by
e sd yd s 1r3
c yc
s 1r3
b yb
s
1
1
2
1
1
2
2
1
Ž
3.Ž
.
1r
a ya .
1
2
9.3.2. Error Bounds for Spline Approximation
w
x
Let as     
sb be a partition of a, b . We recall that
0
1
m
mq1
Ž .
Ž
.
Ž .
the linear spline s x given by formula 9.21 has the property that s  s ,
i
i
0FiFmq1, where  ,  , . . . , 
are any given real numbers. In particu-
0
1
mq1
Ž .
w
x
lar, if  is the value at 
of a function f x defined on the interval
a, b ,
i
i
Ž .
Ž .
w
x
then s x
provides a spline approximation of f x
over
a, b which agrees
Ž .
Ž .
with f x at  ,  , . . . , 
. If f x has continuous derivatives up to order 2
0
1
mq1
w
x
over a, b , then an upper bound on the error of approximation is given by
Ž
.
see De Boor, 1978, page 40
2
1
Ž2.
max
f x ys x
F
max  
max
f
x
,
Ž .
Ž .
Ž .
ž
/
i
8
aFxFb
i
aFxFb
where   s
y , is0, 1, . . . , m. This error bound can be made small by
i
iq1
i
reducing the value of max   .
i
i
A more efficient and smoother spline approximation than the one pro-
vided by the linear spline is the commonly used cubic spline approximation.
w
x
We recall that a cubic spline defined on a, b is a piecewise cubic polynomial
Ž .
w
x
that is twice continuously differentiable. Let f x be defined on a, b . There
Ž .
exists a unique cubic spline s x
that satisfies the following interpolatory
constraints:
s 
sf 
,
is0, 1, . . . , mq1,
Ž
.
Ž
.
i
i
s 
sf 
,
Ž
.
Ž
.
0
0
s 
sf 
,
9.34
Ž
.
Ž
.
Ž
.
mq1
mq1
Ž
.
see Prenter, 1975, Section 4.2 .
Ž .
w
x
If f x has continuous derivatives up to order 4 on a, b , then information
on the error of approximation, which results from using a cubic spline, can be
Ž
.
obtained from the following theorem, whose proof is given in Hall 1968 :

APPROXIMATION OF FUNCTIONS
422
Theorem 9.3.1.
Let as     
sb be a partition of
0
1
m
mq1
w
x
Ž .
Ž .
a, b . Let s x
be a cubic spline associated with f x
and satisfies the
Ž
.
Ž .
constraints described in 9.34 . If f x has continuous derivatives up to order
w
x
4 on a, b , then
4
5
Ž4.
max
f x ys x
F
max  
max
f
x
,
Ž .
Ž .
Ž .
ž
/
i
384
aFxFb
i
aFxFb
where   s
y , is0, 1, . . . , m.
i
iq1
i
Another advantage of cubic spline approximation is the fact that it can be
Ž .
used to approximate the first-order and second-order derivatives of f x .
Ž
.
Ž .
Hall and Meyer 1976 proved that if f x satisfies the conditions of Theorem
9.3.1, then
3
1
Ž4.
max
f x ys x
F
max  
max
f
x
,
Ž .
Ž .
Ž .
ž
/
i
24
aFxFb
i
aFxFb
2
3
Ž4.
max
f
x ys
x
F
max  
max
f
x
.
Ž .
Ž .
Ž .
ž
/
i
8
aFxFb
i
aFxFb
 Ž .
Ž .

Ž .
Ž .
Furthermore, the bounds concerning
f x ys x
and
f x ys x
are
best possible.
9.4. APPLICATIONS IN STATISTICS
There is a wide variety of applications of polynomial approximation in
statistics. In this section, we discuss the use of Lagrange interpolation in
optimal design theory and the role of spline approximation in regression
Ž
.
analysis. Other applications will be seen later in Chapter 10 Section 10.9 .
9.4.1. Approximate Linearization of Nonlinear Models
by Lagrange Interpolation
We recall from Section 8.6 that a nonlinear model is one of the form
y x sh x,  q ,
9.35
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
where xs x , x , . . . , x
 is a vector of k input variables, s  ,  , . . . ,  
1
2
k
1
2
p
Ž
.
is a vector of p unknown parameters,  is a random error, and h x,  is a
known function which is nonlinear in at least one element of .
Ž
.
We also recall that the choice of design for model 9.35 , on the basis of
the BoxLucas criterion, depends on the values of the elements of  that
appear nonlinearly in the model. To overcome this undesirable design
dependence problem, one possible approach is to construct an approximation

APPLICATIONS IN STATISTICS
423
Ž
.
to the mean response function h x,  with a Lagrange interpolating polyno-
mial. This approximation can then be utilized to obtain a design for parame-
ter estimation which does not depend on the parameter vector . We shall
Ž
.
restrict our consideration of model
9.35
to the case of a single input
variable x.
w
x
Let us suppose that the region of interest, R, is the interval a, b , and that
 belongs to a parameter space . We assume that:
Ž
.
a. h x,  has continuous partial derivatives up to order rq1 with respect
w
x
to x over
a, b for all g, where r is such that rq1Gp with p
Ž
.
being the number of parameters in model 9.35 , and is large enough so
that
rq1
rq1
2
bya
"
h x, 
Ž
.
sup

9.36
Ž
.
rq1
ž
/
rq1 !
4
" x
Ž
.
aFxFb
for all g, where  is a small positive constant chosen appropriately
Ž
.
so that the Lagrange interpolation of h x,  achieves a certain accu-
racy.
Ž
.
b. h x,  has continuous first-order partial derivatives with respect to the
elements of .
c. For any set of distinct points, x , x , . . . , x , such that aFx x  
0
1
r
0
1
Ž .
Ž
.
x Fb, where r is the integer defined in a , the p rq1 matrix
r
U  s 
h x ,  : 
h x ,  :  :
h x , 
Ž .
Ž
.
Ž
.
Ž
.
0
1
r
Ž
.
is of rank p, where 
h x , 
is the vector of partial derivatives of
i
Ž
.
Ž
.
h x ,  with respect to the elements of  is0, 1, . . . , r .
i
Let us now consider the points z , z , . . . , z , where z is the ith Cheby-
0
1
r
i
Ž
.
Ž
.
shev point defined by formula 9.18 . Let p
x,  denote the corresponding
r
Ž
.
w
x
Lagrange interpolating polynomial for h x,  over a, b , which utilizes the
Ž
.
z ’s as interpolation points. Then, by formula 9.14 we have
i
r
p
x,  s
h z ,  l
x ,
9.37
Ž
.
Ž
. Ž .
Ž
.
Ý
r
i
i
is0
Ž .
where l
x is a polynomial of degree r which can be obtained from formula
i
Ž
.
Ž
.
Ž
.
9.13 by substituting z for a
is0, 1, . . . , r . By inequality 9.19 , an upper
i
i
Ž
.
Ž
.
bound on the error of approximating h x,  with p
x,  is given by
r
rq1
rq1
2
bya
"
h x, 
Ž
.
sup
h x,  yp
x, 
F
sup
.
Ž
.
Ž
.
r
rq1
ž
/
rq1 !
4
" x
Ž
.
aFxFb
aFxFb

APPROXIMATION OF FUNCTIONS
424
Ž
.
However, by inequality 9.36 , this upper bound is less than . We then have
sup
h x,  yp
x, 

9.38
Ž
.
Ž
.
Ž
.
r
aFxFb
for all g. This provides the desired accuracy of approximation.
On the basis of the above arguments, an approximate representation of
Ž
.
model 9.35 is given by
y x sp
x,  q.
9.39
Ž .
Ž
.
Ž
.
r
Ž
.
Ž
.
Model 9.39 will now be utilized in place of h x,  to construct an optimal
design for estimating .
Let us now apply the BoxLucas criterion described in Section 8.6 to
Ž
.
Ž .
approximate the mean response in model 9.39 . In this case, the matrix H 
w
Ž
.x
Ž
.
Ž
.
see model 8.66
is an np matrix whose u, i th element is " p
x ,  r" ,
r
u
i
Ž
.
where x
is the design setting at the uth experimental run
us1, 2, . . . , n
u
Ž
.
and n is the number of experimental runs. From formula 9.37 we than have
r
" p
x , 
"h z , 
Ž
.
Ž
.
r
u
j
s
l
x
,
is1, 2, . . . , p.
Ž
.
Ý
j
u
"
"
i
i
js0
These equations can be written as

p
x ,  sU   x
,
Ž
.
Ž .
Ž
.
r
u
u
Ž
.
w Ž
.
Ž
.
Ž
.x
Ž .
Ž
.
where  x
s l
x
, l
x
, . . . , l
x
 and U  is the p rq1 matrix
u
0
u
1
u
r
u
U  s 
h z ,  : 
h z ,  :  :
h z , 
.
Ž .
Ž
.
Ž
.
Ž
.
0
1
r
Ž .
Ž .
Ž .
By assumption c , U 
is of rank p. The matrix H 
is therefore of the
form
H  sU  ,
Ž .
Ž .
where
s  x
:  x
:  : x
.
Ž
.
Ž
.
Ž
.
1
2
n
Thus
H  H  sU  U  .
9.40
Ž .
Ž .
Ž .
Ž .
Ž
.
Ž
.
If nGrq1 and at least rq1 of the design points that is, x , x , . . . , x
are
1
2
n
distinct, then  is a nonsingular matrix. To show this, it is sufficient to
prove that  is of full column rank rq1. If not, then there must exist
constants  ,  , . . . ,  , not all equal to zero, such that
0
1
r
r
 l
x
s0,
us1, 2, . . . , n.
Ž
.
Ý
i i
u
is0

APPLICATIONS IN STATISTICS
425
r
Ž .
This indicates that the rth degree polynomial Ý
 l
x
has n roots,
is0
i i
namely, x , x , . . . , x . This is not possible, because nGrq1 and at least
1
2
n
Ž
.
Ž
rq1 of the x ’s us1, 2, . . . , n are distinct a polynomial of degree r has at
u
.
most r distinct roots . This contradiction implies that  is of full column
rank and  is therefore nonsingular.
Applying the BoxLucas design criterion to the approximating model
Ž
.
w
Ž . Ž .x
9.37 amounts to finding the design settings that maximize det H  H  .
Ž
.
From formula 9.40 we have
det H  H 
sdet U  U 
.
9.41
Ž .
Ž .
Ž .
Ž .
Ž
.
n
Ž
.
Ž
.
We note that the matrix sÝ
 x
 x
depends only on the design
us1
u
u
Ž
.
Ž
.
settings. Let 
x , x , . . . , x
and 
x , x , . . . , x
denote, respectively,
min
1
2
n
max
1
2
n
the smallest and the largest eigenvalue of . These eigenvalues are
positive, since  is positive definite by the fact that  is nonsingular, as
Ž
.
was shown earlier. From formula 9.41 we conclude that
p
det U  U 

x , x , . . . , x
Fdet H  H 
Ž .
Ž .
Ž
.
Ž .
Ž .
min
1
2
n
p
Fdet U  U 

x , x , . . . , x
.
Ž .
Ž .
Ž
.
max
1
2
n
9.42
Ž
.
This
double
inequality
follows
from
the
fact
that
the
matrices
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.

x , x , . . . , x
U  U 
y H  H 
and
H  H 
y
m ax
1
2
n
Ž
. Ž .
Ž .

x , x , . . . , x
U  U 
are positive semidefinite. An application of
min
1
2
n
Ž .
Ž
.
Theorem 2.3.19 1 to these matrices results in the double inequality
9.42
Ž
.
Ž .
Ž .
Ž .
Ž .
why? . Note that the determinant of U  U  is not zero, since U  U  ,
Ž .
which is of order pp, is of rank p by assumption c .
Ž
.
Now, from the double inequality
9.42
we deduce that there exists a
number , 0FF1, such that
p
p
det H  H 
s 
x , x , . . . , x
q 1y 
x , x , . . . , x
Ž .
Ž .
Ž
.
Ž
.
Ž
.
min
1
2
n
max
1
2
n
det U  U 
.
Ž .
Ž .
If  is integrated out, we obtain
1
1
p
p
det H  H 
ds

x , x , . . . , x
q
x , x , . . . , x
Ž .
Ž .
Ž
.
Ž
.
H
min
1
2
n
max
1
2
n
2
0
det U  U 
.
Ž .
Ž .
Consequently, to construct an optimal design we can consider finding
x , x , . . . , x
that maximize the function
1
2
n
1
p
p

x , x , . . . , x
s

x , x , . . . , x
q
x , x , . . . , x
.
9.43
Ž
.
Ž
.
Ž
.
Ž
.
1
2
n
min
1
2
n
max
1
2
n
2

APPROXIMATION OF FUNCTIONS
426
This is a modified version of the BoxLucas criterion. Its advantage is that
the optimal design is free of . We therefore call such a design a parameter-
Ž
.
free design. The maximization of  x , x , . . . , x
can be conveniently car-
1
2
n
Ž
.
ried out by using a FORTRAN program written by Conlon 1991 , which is
Ž
.
based on Price’s 1977 controlled random search procedure.
EXAMPLE 9.4.1.
Let us consider the nonlinear model used by Box and
Ž
.
Lucas 1959 of a consecutive first-order chemical reaction in which a raw
material A reacts to form a product B, which in turn decomposes to form
substance C. After time x has elapsed, the mean yield of the intermediate
product B is given by
1
y
x
y x
2
1
h x,  s
e
ye
,
Ž
.
Ž
.
 y
1
2
where 
and 
are the rate constants for the reactions A™B and B™C,
1
2
respectively.
w
x
Suppose that the region of interest R is the interval
0, 10 . Let the
parameter space  be such that 0F F1, 0F F1. It can be verified that
1
2
" rq1h x, 

Ž
.
1
rq1
rq1
y
x
rq1
y x
2
1
s y1

e
y
e
.
Ž
.
Ž
.
2
1
rq1
 y
" x
1
2
Ž
.
rq1
y x
Let us consider the function 
 x,  s
e
. By the mean value theo-
Ž
.
rem Theorem 4.2.2 ,
" w x, 
Ž
.
rq1
y
x
rq1
y x
2
1

e
y
e
s  y
,
Ž
.
2
1
2
1
"
Ž
.
Ž
.
where " w x,  r" is the partial derivative of w x, 
with respect to 
evaluated at , and where  is between 
and  . Thus
1
2
rq1
y
x
rq1
y x
r
y x
rq1
y x
2
1

e
y
e
s  y
rq1 e
y
xe
.
Ž
. Ž
.
2
1
2
1
Hence,
rq1
"
h x, 
Ž
.
r
y x

sup
F 
sup
e
rq1yx
1
rq1
" x
0FxF10
0FxF10


F
sup
rq1yx .
0FxF10

APPLICATIONS IN STATISTICS
427
However,
rq1yx
if rq1Gx,


rq1yx s½ yry1qx
if rq1x.
Since 0FxF10, then


sup
rq1yx Fmax rq1, 9yr .
Ž
.
0FxF10
We then have
rq1
"
h x, 
Ž
.
sup
Fmax rq1, 9yr .
Ž
.
rq1
" x
0FxF10
Ž
.
By inequality 9.36 , the integer r is determined such that
rq1
2
10
max rq1, 9yr .
9.44
Ž
.
Ž
.
ž /
rq1 !
4
Ž
.
If we choose s0.053, for example, then it can be verified that the smallest
Ž
.
positive integer that satisfies inequality 9.44 is rs9. The Chebyshev points
Ž
.
in formula 9.18 that correspond to this value of r are given in Table 9.2. On
choosing n, the number of design points, to be equal to rq1s10, where all
Ž
.
ten design points are distinct, the matrix
 in formula
9.40
will be
Ž
.
nonsingular of order 1010. Using Conlon’s 1991 FORTRAN program for
Ž
.
the maximization of the function  in formula 9.43 with ps2, it can be
shown that the maximum value of  is 17.457. The corresponding optimal
values of x , x , . . . , x
are given in Table 9.2.
1
2
10
Table 9.2. Chebyshev Points and Optimal Design
Points for Example 9.4.1
Chebyshev Points
Optimal Design Points
9.938
9.989
9.455
9.984
8.536
9.983
7.270
9.966
5.782
9.542
4.218
7.044
2.730
6.078
1.464
4.038
0.545
1.381
0.062
0.692

APPROXIMATION OF FUNCTIONS
428
9.4.2. Splines in Statistics
There is a broad variety of work on splines in statistics. Spline functions are
quite suited in practical applications involving data that arise from the
physical world rather than the mathematical world. It is therefore only
natural that splines have many useful applications in statistics. Some of these
applications will be discussed in this section.
9.4.2.1. The Use of Cubic Splines in Regression
Let us consider fitting the model
ysg x q ,
9.45
Ž .
Ž
.
Ž .
where g x is the mean response at x and  is a random error. Suppose that
the domain of x is divided into a set of mq1 intervals by the points
Ž
.
     
such that on the ith interval
is1, 2, . . . , mq1 ,
0
1
m
mq1
Ž .
g x is represented by the cubic spline
s
x sa qb xqc x 2qd x 3,

FxF .
9.46
Ž .
Ž
.
i
i
i
i
i
iy1
i
Ž
As was seen earlier in Section 9.3.1, the parameters
a , b , c , d
is
i
i
i
i
.
1, 2, . . . , mq1 are subject to the following continuity restrictions:
a qb  qc  2qd  3sa
qb
 qc
 2qd
 3,
9.47
Ž
.
i
i
i
i
i
i
i
iq1
iq1
i
iq1
i
iq1
i
Ž .
Ž .
that is, s  ss
 , is1, 2, . . . , m;
i
i
iq1
i
b q2c  q3d  2sb
q2c
 q3d
 2,
9.48
Ž
.
i
i
i
i
i
iq1
iq1
i
iq1
i

Ž .

Ž .
that is, s  ss
 , is1, 2, . . . , m; and
i
i
iq1
i
2c q6d  s2c
q6d
 ,
9.49
Ž
.
i
i
i
iq1
iq1
i
Ž .

Ž .
that is, s  ss
 , is1, 2, . . . , m. The number of unknown parameters
i
i
iq1
i
Ž
.
Ž
.
in model
9.45
is therefore equal to 4 mq1 . The continuity restrictions
Ž
. Ž
.
9.47  9.49
reduce the dimensionality of the parameter space to mq4.
However, only mq2 parameters can be estimated. This is because the spline
method does not estimate the parameters of the s ’s directly, but estimates
i
Ž
.
Ž .
the ordinates of the s ’s at the points  ,  , . . . , 
, that is, s 
and s  ,
i
0
1
mq1
1
0
i
i
is1, 2, . . . , mq1. Two additional restrictions are therefore needed. These
Ž
are chosen to be of the form see Poirier, 1973, page 516; Buse and Lim,
.
1977, page 64 :
s 
s s 
,
Ž
.
Ž
.
1
0
0
1
1
or
2c q6d  s
2c q6d 
,
9.50
Ž
.
Ž
.
1
1
0
0
1
1 1

APPLICATIONS IN STATISTICS
429
and
s

s
s

,
Ž
.
Ž
.
mq1
mq1
mq1
mq1
m
or
2c
q6d

s
2c
q6d

,
9.51
Ž
.
Ž
.
mq1
mq1 mq1
mq1
mq1
mq1 m
where 
and 
are known.
0
mq1
Let y , y , . . . , y
be n observations on the response y, where nmq2,
1
2
n
w
x
such that n observations are taken in the ith interval 
,  , is1, 2, . . . , m
i
iy1
i
q1. Thus nsÝmq1 n . If y , y , . . . , y
are the observations in the ith
is1
i
i1
i2
ini
Ž
.
Ž
.
interval is1, 2, . . . , mq1 , then from model 9.45 we have
y sg x
q ,
is1, 2, . . . , mq1; js1, 2, . . . , n ,
9.52
Ž
.
Ž
.
i j
i j
i j
i
where x
is the setting of x for which ysy , and the  ’s are distributed
i j
i j
i j
independently with means equal to zero and a common variance 	 2. The
Ž
.
estimation of the parameters of model 9.45 is then reduced to a restricted
Ž
. Ž
.
least-squares problem with formulas 9.47  9.51 representing linear restric-
Ž
.
w
tions on the 4 mq1
parameters of the model see, for example, Searle
Ž
.
1971, Section 3.6 , for a discussion concerning least-squares estimation
x
under linear restrictions on the fitted model’s parameters . Using matrix
Ž
.
Ž
. Ž
.
notation, model
9.52
and the linear restrictions
9.47  9.51
can be ex-
pressed as
ysXq ,
9.53
Ž
.
Cs,
9.54
Ž
.
Ž 


.
Ž
.
where ys y : y :  : y
 with y s y , y , . . . , y
, is1, 2, . . . , mq1, X
1
2
mq1
i
i1
i2
ini
Ž
.
w Ž
.x
sDiga X , X , . . . , X
is a block-diagonal matrix of order n 4 mq1
1
2
mq1
with X
being a matrix of order n 4 whose jth row is of the form
i
i
Ž
2
3 .
Ž



.
1, x , x , x
, js1, 2, . . . , n ; is1, 2, . . . , mq1; s  :  :  : 

i j
i j
i j
i
1
2
mq1
Ž
.
Ž



.
with  s a , b , c , d , is1, 2, . . . , mq1; and s  :  :  : 
,
i
i
i
i
i
1
2
mq1
where 
is the vector of random errors associated with the observations in
j
w




 x
the ith interval, is1, 2, . . . , mq1. Furthermore, Cs C : C : C : C ,
0
1
2
3
where, for ls0, 1, 2,


ye
e
0

0
0
1l
1l


0
ye
e

0
0
2 l
2 l
.
.
.
.
.
C s
l
.
.
.
.
.
.
.
.
.
.


0
0
0

ye
e
ml
ml

APPROXIMATION OF FUNCTIONS
430
w Ž
.x

Ž
2
3.

is a matrix of order m 4 mq1
such that e s 1,  ,  , 
, e s
i0
i
i
i
i1
Ž
2.

Ž
.
0, 1, 2 , 3
, e s 0, 0, 2, 6 , is1, 2, . . . , m, and
i
i
i2
i
0
0
2  y1
6   y

0
0
0
0
Ž
.
Ž
.
0
0
1
0
C s
3
0
0
0
0

0
0
2 
y1
6 
 y
Ž
.
Ž
.
mq1
mq1 m
mq1
w Ž
.x
Ž




 .
is a 2 4 mq1
matrix. Finally, s  :  :  :  s0, where the
0
1
2
3
partitioning of  into  ,  ,  , and 
conforms to that of C. Conse-
0
1
2
3
Ž
.
Ž
.
quently, and on the basis of formula 103 in Searle
1971, page 113 , the
Ž
.
least-squares estimator of  for model 9.53 under the restriction described
Ž
.
by formula 9.54 is given by
y1
y1
y1
ˆ
ˆ
ˆ
 sy XX
C C XX
C
Cy
Ž
.
Ž
.
Ž
.
r
y1
y1
y1
ˆ
ˆ
sy XX
C C XX
C
C,
Ž
.
Ž
.
ˆ
y1
Ž
.
where s XX
Xy is the ordinary least-squares estimator of .
Ž
.
This estimation procedure, which was developed by Buse and Lim 1977 ,
demonstrates that the fitting of a cubic spline regression model can be
reduced to a restricted least-squares problem. Buse and Lim presented a
numerical example based on Indianapolis 500 race data over the period
Ž
.
19111971 to illustrate the implementation of their procedure.
Other papers of interest in the area of regression splines include those of
Ž
.
Ž
.
Poirier 1973 and Gallant and Fuller 1973 . The paper by Poirier discusses
the basic theory of cubic regression splines from an economic point of view.
In the paper by Gallant and Fuller, the knots are treated as unknown
parameters rather than being fixed. Thus in their procedure, the knots must
be estimated, which causes the estimation process to become nonlinear.
9.4.2.2. Designs for Fitting Spline Models
A number of papers have addressed the problem of finding a design to
Ž
.
Ž .
estimate the parameters of model
9.45 , where g x
is represented by a
spline function. We shall make a brief reference to some of these papers.
Ž
.
Ž .
Agarwal and Studden
1978
considered a representation of g x
over
Ž .
Ž
.
0FxF1 by a linear spline s x , which has the form given by 9.21 . Here,
Ž .
g x is assumed to be continuous. If we recall, the  coefficients in formula
i
Ž
.
9.21 are the values of s at  ,  , . . . , 
.
0
1
mq1
w
x
Let x , x , . . . , x be r design points in 0, 1 . Let y denote the average of
1
2
r
i
Ž
.
Ž
.
n observations taken at x
is1, 2, . . . , r . The vector s  ,  , . . . , 

i
i
0
1
mq1
can therefore be estimated by
ˆsAy,
9.55
Ž
.
Ž
.
Ž
.
where ys y , y , . . . , y  and A is an mq2 r matrix. Hence, an estimate
1
2
r
Ž .
of g x is given by
ˆ
g x sl x sl x Ay,
9.56
Ž .
Ž .
Ž .
Ž
.
ˆ
Ž .
w Ž .
Ž .
Ž .x
where l x s l
x , l
x , . . . , l
x .
0
1
mq1

APPLICATIONS IN STATISTICS
431
ˆ
Ž .
w Ž
.
Ž
.
Ž
.x
w Ž .x
Now, E  sAg , where g s g x , g x
, . . . , g x
. Thus E g x s
ˆ
r
r
1
2
r
Ž .
Ž .
l x Ag , and the variance of g x is
ˆ
r
2
ˆ
Var g x
sE l x yl x Ag
Ž .
Ž .
Ž .
ˆ
r
	 2
y1
s
l x AD
Al x ,
Ž .
Ž .
n
where
D
is
an
r  r
diagonal
matrix
with
diagonal
elements
Ž .
n rn, n rn, . . . , n rn. The mean squared error of g x
is the variance plus
ˆ
1
2
r
Ž .
the squared bias of g x . It follows that the integrated mean squared error
ˆ
Ž
.
Ž . Ž
.
IMSE of g x
see Section 8.4.3 is
ˆ
n
n
1
1
2
Js
Var g x
dxq
Bias
g x
dx
Ž .
Ž .
ˆ
ˆ
H
H
2
2
	
	
0
0
sVqB,
1
1
y1
Ž
.
where 
s H dx
s , and
0
2
2
2
Bias
g x
s g x yl x Ag
.
Ž .
Ž .
Ž .
ˆ
r
Thus
1
n
1
1
2
y1
Js
l x AD
Al x
dxq
g x yl x Ag
dx
Ž .
Ž .
Ž .
Ž .
H
H
r
2
2
2	
0
0
1
n
1
2
y1
s
tr AD
AM q
g x yl x Ag
dx,
Ž
.
Ž .
Ž .
H
r
2
2
2	
0
1 Ž . Ž .
where MsH l x l x dx.
0
Ž
.
Ž .
Agarwal and Studden 1978 proposed to minimize J with respect to i
Ž
. Ž .
the design that is, x , x , . . . , x
as well as n , n , . . . , n , ii the matrix A,
1
2
r
1
2
r
Ž
.
and iii the knots  ,  , . . . ,  , assuming that g is known.
1
2
m
Ž
.
Ž
.
Park 1978 adopted the D-optimality criterion see Section 8.5 for the
Ž .
choice of design when g x is represented by a spline of the form given by
Ž
.
formula 9.22 with only one intermediate knot.
Ž
.
Draper, Guttman, and Lipow 1977 extended the design criterion based
Ž
.
on the minimization of the average squared bias B
see Section 8.4.3
to
situations involving spline models. In particular, they considered fitting
first-order or second-order models when the true mean response is of the
second order or the third order, respectively.
9.4.2.3. Other Applications of Splines in Statistics
Spline functions have many other useful applications in both theoretical and
applied statistical research. For example, splines are used in nonparametric

APPROXIMATION OF FUNCTIONS
432
regression and data smoothing, nonparametric density estimation, and time
series analysis. They are also utilized in the analysis of response curves in
Ž
.
agriculture and economics. The review articles by Wegman and Wright 1983
Ž
.
and Ramsay 1988 contain many references on the various uses of splines in
Ž
.
statistics
see also the article by Smith, 1979 . An overview of the role of
Ž
.
splines in regression analysis is given in Eubank 1984 .
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Agarwal, G. G., and W. J. Studden 1978 , ‘‘Asymptotic design and estimation using
linear splines.’’ Comm. Statist. Simulation Comput., 7, 309319.
Ž
.
Box, G. E. P., and H. L. Lucas 1959 . ‘‘Design of experiments in nonlinear situations.’’
Biometrika, 46, 7790.
Ž
.
Buse, A., and L. Lim
1977 . ‘‘Cubic splines as a special case of restricted least
squares.’’ J. Amer. Statist. Assoc., 72, 6468.
Ž
.
Cheney, E. W. 1982 . Introduction to Approximation Theory, 2nd ed. Chelsea, New
Ž
York. The Weierstrass approximation theorem and Lagrange interpolation are
.
covered in Chap. 3; least-squares approximation is discussed in Chap. 4.
Ž
.
Conlon, M. 1991 . ‘‘The controlled random search procedure for function optimiza-
Ž
tion.’’ Personal communication.
This is a FORTRAN file for implementing
.
Price’s controlled random search procedure.
Ž
.
Cornish, E. A., and R. A. Fisher 1937 . ‘‘Moments and cumulants in the specification
of distribution.’’ Re®. Internat. Statist. Inst., 5, 307320.
Ž
.
Cramer, H.
1946 . Mathematical Methods of Statistics. Princeton University Press,
´
Ž
Princeton. This classic book provides the mathematical foundation of statistics.
.
Chap. 17 is a good source for approximation of density functions.
Ž
.
Ž
Davis, P. J. 1975 . Interpolation and Approximation. Dover, New York. Chaps. 2, 3, 6,
8, and 10 are relevant to the material on Lagrange interpolation, least-squares
.
approximation, and orthogonal polynomials.
Ž
.
Ž
De Boor, C. 1978 . A Practical Guide to Splines. Springer-Verlag, New York. Chaps.
1 and 2 provide a good coverage of Lagrange interpolation, particularly with
regard to the use of Chebyshev points. Chap. 4 discusses cubic spline approxima-
.
tion.
Ž
.
Draper, N. R., I. Guttman, and P. Lipow 1977 . ‘‘All-bias designs for spline functions
joined at the axes.’’ J. Amer. Statist. Assoc., 72, 424429.
Ž
.
Eubank, R. L. 1984 . ‘‘Approximate regression models and splines.’’ Comm. Statist.
Theory Methods, 13, 433484.
Ž
.
Gallant, A. R., and W. A. Fuller 1973 . ‘‘Fitting segmented polynomial regression
models whose join points have to be estimated.’’ J. Amer. Statist. Assoc., 68,
144147.
Ž
.
Graybill, F. A.
1983 . Matrices with Applications in Statistics, 2nd ed. Wadsworth,
Belmont, California.
Ž
.
Hall, C. A. 1968 . ‘‘On error bounds for spline interpolation.’’ J. Approx. Theory, 1,
209218.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
433
Ž
.
Hall, C. A., and W. W. Meyer
1976 . ‘‘Optimal error bounds for cubic spline
interpolation.’’ J. Approx. Theory, 16, 105122.
Ž
.
Harris, B. 1966 . Theory of Probability. Addison-Wesley, Reading, Massachusetts.
Ž
.
Johnson, N. L., and S. Kotz 1970 . Continuous Uni®ariate Distributions1. Houghton
Ž
Mifflin,
Boston.
Chap.
12
contains
a
good
discussion
concerning
the
.
CornishFisher expansion of percentage points.
Ž
.
Kendall, M. G., and A. Stuart 1977 . The Ad®anced Theory of Statistics, Vol. 1, 4th ed.
Ž
Macmillan, New York. This classic book provides a good source for learning
.
about the GramCharlier series of type A and the CornishFisher expansion.
Ž
.
Lancaster, P., and K. Salkauskas 1986 . Cur®e and Surface Fitting. Academic Press,
Ž
London. This book covers the foundations and major features of several basic
.
methods for curve and surface fitting that are currently in use.
Ž
.
Park, S. H. 1978 . ‘‘Experimental designs for fitting segmented polynomial regression
models.’’ Technometrics, 20, 151154.
Ž
.
Poirier, D. J.
1973 . ‘‘Piecewise regression using cubic splines.’’ J. Amer. Statist.
Assoc., 68, 515524.
Ž
.
Powell, M. J. D.
1967 . ‘‘On the maximum errors of polynomial approximation
defined by interpolation and by least squares criteria.’’ Comput. J., 9, 404407.
Ž
.
Ž
Prenter, P. M. 1975 . Splines and Variational Methods. Wiley, New York. Lagrange
interpolation is covered in Chap. 2; cubic splines are discussed in Chap. 4. An
interesting feature of this book is its coverage of polynomial approximation of a
.
function of several variables.
Ž
.
Price, W. L. 1977 . ‘‘A controlled random search procedure for global optimization.’’
Comput. J., 20, 367370.
Ž
.
Ramsay, J. O.
1988 . ‘‘Monotone regression splines in action.’’ Statist. Sci., 3,
425461.
Ž
.
Rice, J. R. 1969 . The Approximation of Functions, Vol. 2. Addison-Wesley, Reading,
Ž
.
Massachusetts. Approximation by spline functions is presented in Chap. 10.
Ž
.
Rivlin, T. J. 1969 . An Introduction to the Approximation of Functions. Dover, New
Ž
York.
This book provides an introduction to some of the most significant
methods of approximation of functions by polynomials. Spline approximation is
.
also discussed.
Ž
.
Schoenberg, I. J. 1946 . ‘‘Contributions to the problem of approximation of equidis-
tant data by analytic functions.’’ Quart. Appl. Math., 4, Part A, 4599; Part B,
112141.
Ž
.
Searle, S. R. 1971 . Linear Models. Wiley, New York.
Ž
.
Smith, P. L. 1979 . ‘‘Splines as a useful and convenient statistical tool.’’ Amer. Statist.,
33, 5762.
Ž
.
Szidarovszky, F., and S. Yakowitz
1978 . Principles and Procedures of Numerical
Ž
Analysis. Plenum Press, New York.
Chap. 2 provides a brief introduction to
.
approximation and interpolation of functions.
Ž
.
Wegman, E. J., and I. W. Wright
1983 . ‘‘Splines in statistics.’’ J. Amer. Statist.
Assoc., 78, 351365.
Ž
.
Wold, S. 1974 . ‘‘Spline functions in data analysis.’’ Technometrics, 16, 111.

APPROXIMATION OF FUNCTIONS
434
EXERCISES
In Mathematics
Ž .
w
x
9.1. Let f x
be a function with a continuous derivative on 0, 1 , and let
Ž .
b
x
be the nth degree Bernstein approximating polynomial of f.
n
Then, for some constant c and for all n,
c
sup
f x yb
x
F
.
Ž .
Ž .
n
1r2
n
0FxF1
9.2. Prove Lemma 9.1.1.
9.3. Prove Lemma 9.1.2.
w
x
9.4. Show that for every interval ya, a there is a sequence of polynomials
Ž .
Ž .
Ž .
 
w
x
p
x such that p 0 s0 and lim
p
x s x uniformly on ya, a .
n
n
n™
n
Ž .
w
x
9.5. Suppose that f x is continuous on 0, 1 and that
1
n
f x x dxs0,
ns0, 1, 2, . . . .
Ž .
H
0
Ž .
w
x
Show that f x s0 on 0, 1 .
w
1 Ž .
Ž .
Ž .
x
Hint: H f x p
x dxs0, where p
x is any polynomial of degree n.
0
n
n
Ž .
9.6. Suppose that the function f x
has nq1 continuous derivatives on
w
x
w
x
a, b . Let asa a   a sb be nq1 points in a, b . Then
0
1
n

hnq1
nq1
sup
f x yp x
F
,
Ž .
Ž .
4 nq1
Ž
.
aFxFb
Ž .
Ž
.
where p x
is the Lagrange polynomial defined by formula
9.14 ,

Žnq1.Ž .
Ž
.

ssup
f
x
, and hsmax a
ya , is0, 1, . . . , ny1.
nq1
aF xF b
iq1
i
w

n
Ž
.
Ž
nq1
. x
Hint: Show that Ł
xya
Fn! h
r4 .
is0
i
Ž .
9.7. Apply Lagrange interpolation to approximate the function f x slog x
w
x
over the interval 3.50, 3.80 using a s3.50, a s3.60, a s3.70, and
0
1
2
a s3.80 as interpolation points. Compute an upper bound on the
3
error of approximation.
w
x
9.8. Let as     sb be a partition of
a, b . Suppose that
0
1
n
Ž .
w
x
f x
has continuous derivatives up to order 2 over
a, b . Consider a

EXERCISES
435
Ž .
cubic spline s x that satisfies
s 
sf 
,
is0, 1, . . . , n,
Ž
.
Ž
.
i
i
s a sf a ,
Ž .
Ž .
s b sf b .
Ž .
Ž .
Show that
b
b
2
2
f
x
dxG
s
x
dx.
Ž .
Ž .
H
H
a
a
Ž .
9.9. Determine the cubic spline approximation of the function
f x s
Ž
.
w
x
cos 2 x over the interval 0,  using five evenly spaced knots. Give an
upper bound on the error approximation.
In Statistics
9.10. Consider the nonlinear model
y x sh x,  q ,
Ž .
Ž
.
where
h x,  s
1y ey 3 x ,
Ž
.
Ž
.
1
2
such that 0F F50, 0F F1, 0F F1. Obtain a Lagrange inter-
1
2
3
polating polynomial that approximates the mean response function
Ž
.
w
x
h x,  over the region 0, 8 with an error not exceeding s0.05.
9.11. Consider the nonlinear model
ysq 0.49y exp y xy8
q ,
Ž
.
Ž
.
where  is a random error with a zero mean and a variance 	 2.
w
x
Suppose that the region of interest is the interval 10, 40 , and that the
parameter space  is such that 0.36FF0.41, 0.06FF0.16. Let
Ž .
s x be the cubic spline that approximates the mean response, that is,
Ž
.
Ž
.
w
Ž
.x
w
x
 x, ,  sq 0.49y exp y xy8 , over
10, 40 . Determine
the number of knots needed so that
max
 x,  ,  ys x
0.001
Ž
.
Ž .
10FxF40
Ž
.
for all ,  g.

APPROXIMATION OF FUNCTIONS
436
9.12. Consider fitting the spline model
2
ys q xq
xy
q
Ž
. q
0
1
2
w
x
over the interval y1, 1 , where  is a known constant, y1FF1. A
three-point design consisting of x , x , x
with y1Fx Fx x F
1
2
3
1
2
3
1 is used to fit the model. Using matrix notation, the model is written as
ysXq.
where X is the matrix
1
x
0
1
2
1
x
x y
Ž
.
Xs
,
2
2
2
1
x
x y
Ž
.
3
3
Ž
.
and s  ,  , 
. Determine x , x , x
so that the design is D-
0
1
2
1
2
3
optimal, that is, it maximizes the determinant of XX.
w
Ž
. x
Note: See Park 1978 .

C H A P T E R
1 0
Orthogonal Polynomials
The subject of orthogonal polynomials can be traced back to the work of the
Ž
.
French mathematician Adrien-Marie Legendre
17521833
on planetary
motion. These polynomials have important applications in physics, quantum
mechanics, mathematical statistics, and other areas in mathematics.
This chapter provides an exposition of the properties of orthogonal
polynomials. Emphasis will be placed on Legendre, Chebyshev, Jacobi,
Laguerre, and Hermite polynomials. In addition, applications of these poly-
nomials in statistics will be discussed in Section 10.9.
10.1. INTRODUCTION
Ž .
Ž .
w
x
Ž .
Suppose that f x and g x are two continuous functions on a, b . Let w x
w
x
be a positive function that is Riemann integrable on a, b . The dot product
Ž .
Ž .
Ž .
Ž
.
of f x and g x with respect to w x , which is denoted by
fg
, is defined

as
b
fg
s
f x g x w x
dx.
Ž
.
Ž . Ž .
Ž .
H

a
Ž .
Ž .
	
	
The norm of f x
with respect to w x , denoted by
f
, is defined as

	
	
w
b
2Ž . Ž .
x1r2
Ž .
Ž .
f
s H f
x w x dx
. The functions
f x
and g x
are said to be

a
w
Ž .x
Ž
.
orthogonal with respect to w x
if
fg
s0. Furthermore, a sequence


Ž .4
w
x
f
x
of continuous functions defined on a, b are said to be orthogonal
n
ns0
Ž .
Ž
.
	
	
with respect to w x if
f f
s0 for mn. If, in addition,
f
s1 for

m
n 
n
Ž .
all n, then the functions f
x , ns0, 1, 2, . . . , are called orthonormal. In
n

Ž .4
Ž
.
particular, if Ss p
x
is a sequence of polynomials such that
p p
n
ns0
n
m 
s0 for all mn, then S forms a sequence of polynomials orthogonal with
Ž .
respect to w x .
A sequence of orthogonal polynomials can be constructed on the basis of
the following theorem:
437

ORTHOGONAL POLYNOMIALS
438

Ž .4
Theorem 10.1.1.
The polynomials
p
x
which are defined accord-
n
ns0
ing to the following recurrence relation are orthogonal:
p
x s1,
Ž .
0
xp p
x1
Ž
.
Ž
. 
0
0

p
x sxy
sxy
,
Ž .
1
2
2
p
1

0

10.1
Ž
.
...
p
x s xya
p
x yb p
x ,
ns2, 3, . . . ,
Ž .
Ž
.
Ž .
Ž .
n
n
ny1
n
ny2
where
xp
p
Ž
.
ny1
ny1

a s
10.2
Ž
.
n
2
pny1

xp
p
Ž
.
ny1
ny2

b s
10.3
Ž
.
n
2
pny2

Ž
.
Proof. We show by mathematical induction on n that
p p
s0 for
n
i 
in. For ns1,
x1
Ž
.
b

p p
s
xy
w x
dx
Ž
.
Ž .
H
1
0

2
a
1

2
1

s x1
y x1
Ž
.
Ž
.


2
1

s0.
Ž
.
Now, suppose that the assertion is true for ny1 nG2 . To show that it is
true for n. We have that
b
p p
s
xya
p
x yb p
x
p
x w x
dx
Ž
.
Ž
.
Ž .
Ž .
Ž .
Ž .
H
n
i
n
ny1
n
ny2
i

a
s xp
p
ya
p
p
yb
p
p
.
Ž
.
Ž
.
Ž
.
ny1
i
n
ny1
i
n
ny2
i



Thus, for isny1,
2
p p
s xp
p
ya
p
yb
p
p
Ž
.
Ž
.
Ž
.
n
i
ny1
ny1
n
ny1
n
ny2
ny1




s0,

INTRODUCTION
439
Ž
.
Ž
.
Ž
by the definition of a
in
10.2
and the fact that
p
p
s p

n
ny2
ny1 
ny1
.
p
s0. Similarly, for isny2,
ny2 
p p
s xp
p
ya
p
p
yb
p
p
Ž
.
Ž
.
Ž
.
Ž
.
n
i
ny1
ny2
n
ny1
ny2
n
ny2
ny2




2
s xp
p
yb
p
Ž
.
ny1
ny2
n
ny2


s0,
by 10.3 .
Ž
.
Finally, for iny2, we have
p p
s xp
p
ya
p
p
yb
p
p
Ž
.
Ž
.
Ž
.
Ž
.
n
i
ny1
i
n
ny1
i
n
ny2
i




b
s
xp
x p
x w x
dx.
10.4
Ž .
Ž .
Ž .
Ž
.
H
ny1
i
a
But, from the recurrence relation,
p
x s xya
p
x yb
p
x ,
Ž .
Ž
.
Ž .
Ž .
iq1
iq1
i
iq1
iy1
that is,
xp
x sp
x qa
p
x qb
p
x .
Ž .
Ž .
Ž .
Ž .
i
iq1
iq1
i
iq1
iy1
It follows that
bxp
x p
x w x
dx
Ž .
Ž .
Ž .
H
ny1
i
a
b
s
p
x
p
x qa
p
x qb
p
x
w x
dx
Ž .
Ž .
Ž .
Ž .
Ž .
H
ny1
iq1
iq1
i
iq1
iy1
a
s
p
p
qa
p
p
qb
p
p
Ž
.
Ž
.
Ž
.
ny1
iq1
iq1
ny1
i
iq1
ny1
iy1



s0.
Ž
. Ž
.
Hence, by 10.4 ,
p p
s0.

n
i 
Ž
.
Ž .
It is easy to see from the recurrence relation 10.1 that p
x is of degree
n
n, and the coefficient of x n is equal to one. Furthermore, we have the
following corollaries:
Corollary 10.1.1.
An arbitrary polynomial of degree Fn is uniquely
Ž .
Ž .
Ž .
expressible as a linear combination of p
x , p
x , . . . , p
x .
0
1
n
ny1
Ž .
n
Ž
.
Corollary 10.1.2.
The coefficient of x
in p
x is yÝ
a
nG1 .
n
is1
i
ny1
Ž . Ž
.
Proof. If d
denotes the coefficient of x
in p
x
nG2 , then by
n
n
comparing the coefficients of x ny1 on both sides of the recurrence relation

ORTHOGONAL POLYNOMIALS
440
Ž
.
10.1 , we obtain
d sd
ya ,
ns2, 3, . . . .
10.5
Ž
.
n
ny1
n
Ž
.
The result follows from 10.5 and by noting that d sya

1
1
Another property of orthogonal polynomials is given by the following
theorem:

Ž .4
Theorem 10.1.2.
If
p
x
is a sequence of orthogonal polynomials
n
ns0
Ž .
w
x
Ž . Ž
.
with respect to w x
on
a, b , then the zeros of p
x
nG1 are all real,
n
w
x
distinct, and located in the interior of a, b .
Ž
.
b
Ž . Ž .
Proof. Since
p p
s0 for nG1, then H p
x w x dxs0. This indi-
n
0 
a
n
Ž .
Ž
. w
Ž .
cates that p
x must change sign at least once in
a, b
recall that w x is
n
x
Ž .
positive . Suppose that p
x changes sign between a and b at just k points,
n
Ž .
Ž
.Ž
.
Ž
.
denoted
by
x , x , . . . , x . Let
g x s xyx
xyx
 xyx
. Then,
1
2
k
1
2
k
Ž . Ž .
Ž
.
p
x g x is a polynomial with no zeros of odd multiplicity in
a, b . Hence,
n
b
Ž . Ž . Ž .
Ž
.
H p
x g x w x dx0, that is,
p g
0. If kn, then we have a contra-
a
n
n

Ž . w Ž .
diction by the fact that p is orthogonal to g x
g x , being a polynomial of
n
Ž .
Ž .
Ž .
degree k, can be expressed as a linear combination of p
x , p
x , . . . , p
x
0
1
k
x
Ž .
by Corollary 10.1.1 . Consequently, ksn, and p
x has n distinct zeros in
n
w
x
the interior of a, b .

Particular orthogonal polynomials can be derived depending on the choice
w
x
Ž .
of the interval
a, b , and the weight function w x . For example, the
well-known orthogonal polynomials listed below are obtained by the follow-
w
x
Ž .
ing selections of a, b and w x :
Ž .
Orthogonal Polynomial
a
b
w x
Legendre
y1
1
1


Ž
. Ž
.
Jacobi
y1
1
1yx
1qx
, , y1
2 y1r2
Ž
.
Chebyshev of the first kind
y1
1
1yx
2 1r2
Ž
.
Chebyshev of the second kind
y1
1
1yx
2
yx r2
Hermite
y

e
yx

Laguerre
0

e
x ,
y1
These polynomials are called classical orthogonal polynomials. We shall
study their properties and methods of derivation.
10.2. LEGENDRE POLYNOMIALS
These polynomials are derived by applying the so-called Rodrigues formula
n
n
2
1
d
x y1
Ž
.
p
x s
,
ns0, 1, 2, . . . .
Ž .
n
n
n
2 n!
dx

LEGENDRE POLYNOMIALS
441
Thus, for ns0, 1, 2, 3, 4, for example, we have
p
x s1,
Ž .
0
p
x sx,
Ž .
1
3
1
2
p
x s x y ,
Ž .
2
2
2
5
3
3
p
x s x y x,
Ž .
3
2
2
35
30
3
4
2
p
x s
x y
x q .
Ž .
4
8
8
8
Ž .
From the Rodrigues formula it follows that p
x is a polynomial of degree n
n
2n
2n
n
n
n
Ž .
and the coefficient of x
is
r2 . We can multiply p
x by 2 r
to
n
ž /
ž /
n
n
n
Ž
.
make the coefficient of x
equal to one ns1, 2, . . . .
Another definition of Legendre polynomials is obtained by means of the
generating function,
1
g x, r s
,
Ž
.
1r2
2
1y2rxqr
Ž
.
by expanding it as a power series in r for sufficiently small values of r. The
n
Ž .
coefficient of r
in this expansion is p
x , ns0, 1, . . . , that is,
n

n
g x, r s
p
x r .
Ž
.
Ž .
Ý
n
ns0
Ž
.y1r2
To demonstrate this, let us consider expanding 1yz
in a neighborhood
of zero, where zs2 xryr 2:
1
1
3
5
2
3
s1q zq z q
z q  ,
z 1,
2
8
16
1r2
1yz
Ž
.
2
3
1
3
5
2
2
2
s1q
2 xryr
q
2 xryr
q
2 xryr
q 
Ž
.
Ž
.
Ž
.
2
8
16
3
1
5
3
2
2
3
3
s1qxrq
x y
r q
x y x r q  .
Ž
.
Ž
.
2
2
2
2
2
3
Ž .
Ž .
We note that the coefficients of 1, r, r , and r
are the same as p
x , p
x ,
0
1
Ž .
Ž .
p
x , p
x , as was seen earlier. In general, it is easy to see that the
2
3
n
Ž . Ž
.
coefficient of r
is p
x
ns0, 1, 2, . . . .
n
Ž
.
By differentiating g x, r with respect to r, it can be seen that
" g x, r
Ž
.
2
1y2rxqr
y xyr g x, r s0.
Ž
.
Ž
. Ž
.
" r

ORTHOGONAL POLYNOMIALS
442
Ž
.

Ž .
n
By substituting g x, r sÝ
p
x r
in this equation, we obtain
ns0
n


2
ny1
n
1y2rxqr
np
x r
y xyr
p
x r s0.
Ž
.
Ž .
Ž
.
Ž .
Ý
Ý
n
n
ns1
ns0
The coefficient of r n must be zero for each n and for all values of x
Ž
.
ns1, 2, . . . . We thus have the following identity:
nq1 p
x y 2nq1 xp
x qnp
x s0,
ns1, 2, . . .
10.6
Ž
.
Ž .
Ž
.
Ž .
Ž .
Ž
.
nq1
n
ny1
This is a recurrence relation that connects any three successive Legendre
3
1
5
3
2
3
Ž .
Ž .
polynomials. For example, for p
x s x y , p
x s x y x, we find
2
3
2
2
2
2
Ž
.
from 10.6 that
1
p
x s
7xp
x y3p
x
Ž .
Ž .
Ž .
4
3
2
4
35
30
3
4
2
s
x y
x q .
8
8
8
10.2.1. Expansion of a Function Using Legendre Polynomials
Ž .
w
x
1
Ž .
Ž .
Suppose that f x is a function defined on y1, 1 such that H
f x p
x dx
y1
n
exists for ns0, 1, 2, . . . . Consider the series expansion

f x s
a p
x .
10.7
Ž .
Ž .
Ž
.
Ý
i
i
is0
Ž
.
Ž .
Multiplying both sides of 10.7 by p
x and then integrating from y1 to 1,
n
we obtain, by the orthogonality of Legendre polynomials,
y1
1
1
2
a s
p
x
dx
f x p
x
dx,
ns0, 1, . . . .
Ž .
Ž .
Ž .
H
H
n
n
n
y1
y1
Ž
.
It can be shown that see Jackson, 1941, page 52
2
1
2
p
x
dxs
,
ns0, 1, 2, . . . .
Ž .
H
n
2nq1
y1
Ž .
Ž
.
Hence, the coefficient of p
x in 10.7 is given by
n
2nq1
1
a s
f x p
x
dx.
10.8
Ž .
Ž .
Ž
.
H
n
n
2
y1

JACOBI POLYNOMIALS
443
Ž .
n
Ž .
Ž
.
If s
x denotes the partial sum Ý
a p x of the series in 10.7 , then
n
is0
i
i
nq1
f tŽ .
1
s
x s
p
t p
x yp
t p
x
dt,
Ž .
Ž .
Ž .
Ž .
Ž .
H
n
nq1
n
n
nq1
2
tyx
y1
ns0, 1, 2, . . . .
10.9
Ž
.
Ž
.
Ž .
This is known as Christoffel’s identity see Jackson, 1941, page 55 . If f x is
w
x
Ž
.
continuous on y1, 1 and has a derivative at xsx , then lim
s
x
s
0
n™
n
0
Ž
.
Ž
.
Ž
. Ž
f x
, and hence the series in 10.7 converges at x
to the value f x
see
0
0
0
.
Jackson, 1941, pages 6465 .
10.3. JACOBI POLYNOMIALS
Jacobi polynomials, named after the German mathematician Karl Gustav
Ž
.
w
x
Jacobi 18041851 , are orthogonal on
y1, 1
with respect to the weight
Ž .
Ž
.Ž
. 
function w x s 1yx
1qx
, y1, y1. The restrictions on 
Ž .
w
x
and  are needed to guarantee integrability of w x over the interval y1, 1 .
Ž,  .Ž .
These polynomials, which we denote by p
x , can be derived by applying
n
the Rodrigues formula:
qn
n
qn
n
d
1yx
1qx
y1
Ž
.
Ž
.
Ž
.
y
y
Ž ,  .
p
x s
1yx
1qx
,
Ž .
Ž
.
Ž
.
n
n
n
2 n!
dx
ns0, 1, 2, . . . .
10.10
Ž
.
This formula reduces to the one for Legendre polynomials when ss0.
Thus, Legendre polynomials represent a special class of Jacobi polynomials.
Ž
.
Applying the so-called Leibniz formula
see Exercise 4.2 in Chapter 4
Ž .
concerning the nth derivative of a product of two functions, namely, f
x s
n
Ž
.qn
Ž .
Ž
. qn
Ž
.
1yx
and g
x s 1qx
in 10.10 , we obtain
n
qn
qn
n
n
d
1yx
1qx
Ž
.
Ž
.
n
Ži.
Žnyi .
s
f
x g
x ,
ns0, 1, . . . ,
Ž .
Ž .
Ý
n
n
n
ž /
i
dx
is0
10.11
Ž
.
Ži.Ž .
Ž
.qnyi
where for is0, 1, . . . , n, f
x
is a constant multiple of
1yx
s
n
Ž
.Ž
.nyi
Žnyi .Ž .
Ž
. qi
1yx
1yx
, and
g
x
is a constant multiple of
1qx
s
n
Ž
. Ž
.i
Ž
.
Ž
.Ž
. 
1qx
1qx . Thus, the nth derivative in 10.11 has 1yx
1qx
as a
Ž
.
Ž,  .Ž .
factor. Using formula 10.10 , it can be shown that p
x is a polynomial
n
Ž
n.
of degree n with the leading coefficient that is, the coefficient of x
equal
Ž
n
. Ž
.
Ž
.
to 1r2 n! ! 2nqqq1 r! nqqq1 .

ORTHOGONAL POLYNOMIALS
444
10.4. CHEBYSHEV POLYNOMIALS
These polynomials were named after the Russian mathematician Pafnuty
Ž
.
Lvovich Chebyshev
18211894 . In this section, two kinds of Chebyshev
polynomials will be studied, called, Chebyshev polynomials of the first kind
and of the second kind.
10.4.1. Chebyshev Polynomials of the First Kind
Ž .
These polynomials are denoted by T
x and defined as
n
T
x scos n Arccos x ,
ns0, 1, . . . ,
10.12
Ž .
Ž
.
Ž
.
n
Ž .
where 0FArccos xF. Note that T
x can be expressed as
n
2
n
n
n
ny2
2
ny4
2
T
x sx q
x
x y1 q
x
x y1
q  ,
ns0, 1, . . . ,
Ž .
Ž
.
Ž
.
n
ž /
ž /
2
4
10.13
Ž
.
Ž
.
where y1FxF1. Historically, the polynomials defined by
10.13
were
originally called Chebyshev polynomials without any qualifying expression.
Ž
.
Using 10.13 , it is easy to obtain the first few of these polynomials:
T
x s1,
Ž .
0
T
x sx,
Ž .
1
T
x s2 x 2y1,
Ž .
2
T
x s4 x 3y3x,
Ž .
3
T
x s8 x 4y8 x 2q1,
Ž .
4
T
x s16 x 5y20 x 3q5x,
Ž .
5
...
Ž .
The following are some properties of T
x :
n
Ž .
1. y1FT
x F1 for y1FxF1.
n
Ž
.
Ž
.n
Ž .
2. T yx s y1
T
x .
n
n
Ž .
3. T
x has simple zeros at the following n points:
n
2iy1 
Ž
.
 scos
,
is1, 2, . . . , n.
i
2n

CHEBYSHEV POLYNOMIALS
445
We may recall that these zeros, also referred to as Chebyshev points,
were instrumental in minimizing the error of Lagrange interpolation in
Ž
.
Chapter 9 see Theorem 9.2.3 .
Ž .
Ž .
Ž
2.y1r2
4. The weight function for T
x is w x s 1yx
. To show this, we
n
have that for two nonnegative integers, m, n,

cos m cos n ds0,
mn,
10.14
Ž
.
H
0
and

r2,
n0,
2
cos n ds
.
10.15
Ž
.
H
½  ,
ns0
0
Ž
.
Ž
.
Making the change of variables xscos  in
10.14
and
10.15 , we
obtain
T
x T
x
Ž .
Ž .
1
m
n
dxs0,
mn,
H
1r2
2
y1
1yx
Ž
.
T 2 x
Ž .
1
r2,
n0,
n
dxs
H
1r2
½
2
 ,
ns0.
y1 1yx
Ž
.

Ž .4
This shows that T
x
forms a sequence of orthogonal polynomials
n
ns0
w
x
Ž .
Ž
2.y1r2
on y1, 1 with respect to w x s 1yx
.
5. We have
T
x s2 xT
x yT
x ,
ns1, 2, . . . .
10.16
Ž .
Ž .
Ž .
Ž
.
nq1
n
ny1
To show this recurrence relation, we use the following trigonometric
identities:
cos
nq1  scos n cos ysin n sin  ,
Ž
.
cos
ny1  scos n cos qsin n sin .
Ž
.
Adding these identities, we obtain
cos
nq1  s2 cos n cos ycos
ny1  .
10.17
Ž
.
Ž
.
Ž
.
Ž .
Ž
.
Ž
.
If we set xscos  and cos nsT
x
in
10.17 , we obtain
10.16 .
n
Ž .
Ž .
Recall that T
x s1 and T
x sx.
0
1
10.4.2. Chebyshev Polynomials of the Second Kind
These polynomials are defined in terms of Chebyshev polynomials of the first
Ž .
kind as follows: Differentiating T
x scos n with respect to xscos , we
n

ORTHOGONAL POLYNOMIALS
446
obtain,
dT
x
d
Ž .
n
syn sin n
dx
dx
sin n
sn
.
sin 
Ž .
Let U
x be defined as
n
1
dT
x
Ž .
nq1
U
x s
Ž .
n
nq1
dx
sin
nq1 
Ž
.
s
,
ns0, 1, . . .
10.18
Ž
.
sin 
This polynomial, which is of degree n, is called a Chebyshev polynomial of
the second kind. Note that
sin n cos qcos n sin 
U
x s
Ž .
n
sin 
sxU
x qT
x ,
ns1, 2, . . . ,
10.19
Ž .
Ž .
Ž
.
ny1
n
Ž .
Ž
.
Ž .
where U
x s1. Formula 10.19 provides a recurrence relation for U
x .
0
n
Ž .
Another recurrence relation that is free of T
x
can be obtained from the
n
following identity:
sin
nq1  s2 sin n cos ysin
ny1  .
Ž
.
Ž
.
Hence,
U
x s2 xU
x yU
x ,
ns2, 3, . . . .
10.20
Ž .
Ž .
Ž .
Ž
.
n
ny1
ny2
Ž .
Ž .
Ž
.
Using the fact that U
x s1, U x s2 x, formula
10.20
can be used to
0
1
Ž .
derive expressions for U
x , ns2, 3, . . . . It is easy to see that the leading
n
n
Ž .
n
coefficient of x
in U
x is 2 .
n

Ž .4
We now show that U
x
forms a sequence of orthogonal polynomials
n
ns0
Ž .
Ž
2.1r2
w
x
with respect to the weight function, w x s 1yx
, over y1, 1 . From
the formula

sin
mq1  sin
nq1 
ds0,
mn,
Ž
.
Ž
.
H
0

HERMITE POLYNOMIALS
447
we get, after making the change of variables xscos ,
1
1r2
2
U
x U
x
1yx
dxs0,
mn.
Ž .
Ž . Ž
.
H
m
n
y1
Ž .
Ž
2.1r2
This shows that w x s 1yx
is a weight function for the sequence

Ž .4
1
2Ž .Ž
2.1r2
U
x
. Note that H
U
x 1yx
dxsr2.
n
ns0
y1
n
10.5. HERMITE POLYNOMIALS

Ž .4
Hermite polynomials, denoted by
H
x
, were named after the French
n
ns0
Ž
.
mathematician Charles Hermite 18221901 . They are defined by the Ro-
drigues formula,
dn eyx 2 r2
Ž
.
2
n
x r2
H
x s y1
e
,
ns0, 1, 2, . . . .
10.21
Ž .
Ž
.
Ž
.
n
n
dx
Ž
.
From 10.21 , we have
dn eyx 2 r2
Ž
.
2
n
yx r2
s y1
e
H
x .
10.22
Ž
.
Ž .
Ž
.
n
n
dx
Ž
.
By differentiating the two sides in 10.22 , we obtain
2
nq1
yx r2
d
e
dH
x
Ž .
Ž
.
2
2
n
n
yx r2
yx r2
s y1
yxe
H
x qe
.
10.23
Ž
.
Ž .
Ž
.
n
nq1
dx
dx
Ž
.
But, from 10.21 ,
dnq1 eyx 2 r2
Ž
.
2
nq1
yx r2
s y1
e
H
x .
10.24
Ž
.
Ž .
Ž
.
nq1
nq1
dx
Ž
.
Ž
.
From 10.23 and 10.24 we then have
dH
x
Ž .
n
H
x sxH
x y
,
ns0, 1, 2, . . . ,
Ž .
Ž .
nq1
n
dx

Ž .4
which defines a recurrence relation for the sequence
H
x
. Since
n
ns0
Ž .
Ž .
H
x s1, it follows by induction, using this relation, that
H
x
is a
0
n
polynomial of degree n. Its leading coefficient is equal to one.

ORTHOGONAL POLYNOMIALS
448
Ž .
yx 2 r2
Note that if w x se
, then
x 2
t 2
w xyt
sexp y
qtxy
Ž
.
ž
/
2
2
t 2
sw x exp txy
.
Ž . ž
/
2
Ž
.
Applying Taylor’s expansion to w xyt , we obtain
n
n

y1
d
w x
Ž
.
Ž .
n
w xyt s
t
Ž
.
Ý
n
n!
dx
ns0

nt
s
H
x w x .
Ž .
Ž .
Ý
n
n!
ns0
Ž .
n
Ž
Consequently, H
x is the coefficient of t rn! in the expansion of exp txy
n
2
.
t r2 . It follows that
nw2x
nw4x
nw6x
n
ny2
ny4
ny6
H
x sx y
x
q
x
y
x
q  ,
Ž .
n
2
3
2.1!
2 2!
2 3!
wr x
Ž
.Ž
.
Ž
.
where n
sn ny1 ny2  nyrq1 . This particular representation of
Ž .
Ž
.
H
x is given in Kendall and Stuart 1977, page 167 . For example, the first
n
seven Hermite polynomials are
H
x s1,
Ž .
0
H
x sx,
Ž .
1
H
x sx 2y1,
Ž .
2
H
x sx 3y3x,
Ž .
3
H
x sx 4y6 x 2q3,
Ž .
4
H
x sx 5y10 x 3q15x,
Ž .
5
H
x sx6y15x 4q45x 2y15,
Ž .
6
...
Ž .
Another recurrence relation that does not use the derivative of H
x
is
n
given by
H
x sxH
x ynH
x ,
ns1, 2, . . . ,
10.25
Ž .
Ž .
Ž .
Ž
.
nq1
n
ny1

HERMITE POLYNOMIALS
449
Ž .
Ž .
Ž
.
Ž
.
with H
x s1 and H
x sx. To show this, we use 10.21 in 10.25 :
0
1
dnq1 eyx 2 r2
Ž
.
2
nq1
x r2
y1
e
Ž
.
nq1
dx
dn eyx 2 r2
dny1 eyx 2 r2
Ž
.
Ž
.
2
2
n
ny1
x r2
x r2
sx y1
e
yn y1
e
Ž
.
Ž
.
n
ny1
dx
dx
or equivalently,
dnq1 eyx 2 r2
dn eyx 2 r2
dny1 eyx 2 r2
Ž
.
Ž
.
Ž
.
y
sx
qn
.
n
nq1
ny1
dx
dx
dx
This is true given the fact that
d eyx 2 r2
Ž
.
2
yx r2
syxe
.
dx
Hence,
dnq1 eyx 2 r2
dn xeyx 2 r2
Ž
.
Ž
.
y
s
n
nq1
dx
dx
dny1 eyx 2 r2
dn eyx 2 r2
Ž
.
Ž
.
sn
qx
,
10.26
Ž
.
n
ny1
dx
dx
Ž
.
which results from applying Leibniz’s formula to the right-hand side of 10.26 .

Ž .4
We now show that H
x
forms a sequence of orthogonal polynomials
n
ns0
Ž .
yx 2 r2
Ž
.
with respect to the weight function w x se
over
y,  . For this
purpose, let m, n be nonnegative integers, and let c be defined as

2
yx r2
cs
e
H
x H
x
dx.
10.27
Ž .
Ž .
Ž
.
H
m
n
y
Ž
.
Ž
.
Then, from 10.21 and 10.27 , we have

n
yx 2 r2
d
e
Ž
.
n
cs y1
H
x
dx.
Ž
.
Ž .
H
m
n
dx
y
Integrating by parts gives

2
2
ny1
yx r2
ny1
yx r2

d
e
dH
x
d
e
Ž .
Ž
.
Ž
.
m
n
cs y1
H
x
y
dx
Ž
.
Ž .
H
m
ny1
ny1
½
5
dx
dx
dx
y
y

ny1
yx 2 r2
dH
x
d
e
Ž .
Ž
.
m
nq1
s y1
dx.
10.28
Ž
.
Ž
.
H
ny1
dx
dx
y

ORTHOGONAL POLYNOMIALS
450
Ž
.
Ž .
ny1Ž
yx 2 r2.
ny1
Formula 10.28 is true because H
x d
e
rdx
, which is a poly-
m
nomial multiplied by eyx 2 r2, has a limit equal to zero as
x™. By
repeating the process of integration by parts my1 more times, we obtain for
nm
2
m
nym
yx r2
 d
H
x
d
e
Ž .
Ž
.
m
mqn
cs y1
dx.
10.29
Ž
.
Ž
.
H
m
nym
dx
dx
y
Ž .
Note that since H
x is a polynomial of degree m with a leading coefficient
m
mw
Ž .x
m
equal to one, d
H
x rdx
is a constant equal to m!. Furthermore, since
m
nm, then

nym
yx 2 r2
d
e
Ž
.
dxs0.
H
nym
dx
y
It follows that cs0. We can also arrive at the same conclusion if nm.
Hence,

2
yx r2
e
H
x H
x
dxs0,
mn.
Ž .
Ž .
H
m
n
y

Ž .4
This shows that
H
x
is a sequence of orthogonal polynomials with
n
ns0
Ž .
yx 2 r2
Ž
.
respect to w x se
over y,  .
Ž
.
Note that if msn in 10.29 , then
n
 d
H
x
Ž .
2
n
yx r2
cs
e
dx
H
n
dx
y

2
yx r2
sn!
e
dx
H
y

2
yx r2
s2n!
e
dx
H
0
'
sn! 2
Ž
.
By comparison with 10.27 , we conclude that

2
yx r2
2
'
e
H
x
dxsn! 2 .
10.30
Ž .
Ž
.
H
n
y
Hermite polynomials can be used to provide the following series expansion
Ž .
of a function f x :

f x s
c H
x ,
10.31
Ž .
Ž .
Ž
.
Ý
n
n
ns0

LEGUERRE POLYNOMIALS
451
where

1
2
yx r2
c s
e
f x H
x
dx,
ns0, 1, . . . .
10.32
Ž .
Ž .
Ž
.
H
n
n
'
n! 2
y
Ž
.
Ž
.
Formula
10.32
follows
from multiplying
both
sides
of
10.31
with
yx 2 r2
Ž .
Ž
.
Ž
.
e
H
x , integrating over y,  , and noting formula 10.30 and the
n

Ž .4
orthogonality of the sequence
H
x
.
n
ns0
10.6. LAGUERRE POLYNOMIALS
Laguerre polynomials were named after the French mathematician Edmond
Ž
.
Ž .Ž .
Laguerre 18341886 . They are denoted by L
x and are defined over the
n
Ž
.
interval 0,  , ns0, 1, 2, . . . , where y1.
The development of these polynomials is based on an application of
Leibniz formula to finding the nth derivative of the function

x sx qneyx.
Ž .
n
Ž .Ž .
More specifically, for y1, L
x
is defined by a Rodrigues-type for-
n
mula, namely,
dn x nq eyx
Ž
.
n
Ž .
x
y
L
x s y1
e x
,
ns0, 1,2, . . . .
Ž .
Ž
.
n
n
dx
Ž .
Ž .Ž .
We shall henceforth use L
x instead of L
x .
n
n
Ž .
From this definition, we conclude that L
x is a polynomial of degree n
n
with a leading coefficient equal to one. It can also be shown that Laguerre
Ž .
yx

polynomials are orthogonal with respect to the weight function w x se
x
Ž
.
over 0,  , that is,
 yx

e
x L
x L
x
dxs0,
mn
Ž .
Ž .
H
m
n
0
Ž
.
see Jackson, 1941, page 185 . Furthermore, if msn, then

2
yx

e
x
L
x
dxsn!! qnq1 ,
ns0, 1, . . . .
Ž .
Ž
.
H
n
0
Ž .
A function
f x
can be expressed as an infinite series of Laguerre
polynomials of the form

f x s
c L
x ,
Ž .
Ž .
Ý
n
n
ns0

ORTHOGONAL POLYNOMIALS
452
where

1
yx

c s
e
x L
x f x
dx,
ns0, 1, 2, . . . .
Ž . Ž .
H
n
n
n!! qnq1
Ž
.
0
Ž .
A recurrence relation for L
x is developed as follows: From the definition
n
Ž .
of L
x , we have
n
dn x nq eyx
Ž
.
n
 yx
y1
x e
L
x s
,
ns0, 1, 2, . . . .
10.33
Ž
.
Ž .
Ž
.
n
n
dx
Ž
.
Replacing n by nq1 in 10.33 gives
dnq1 x nqq1 eyx
Ž
.
nq1
 yx
y1
x e
L
x s
.
10.34
Ž
.
Ž .
Ž
.
nq1
nq1
dx
Now,
x nqq1 eyx sx x nq eyx .
10.35
Ž
.
Ž
.
Ž
.
Applying the Leibniz formula for the
nq1 st derivative of the product on
Ž
.
the right-hand side of 10.35 and noting that the nth derivative of x is zero
for ns2, 3, 4, . . . , we obtain
dnq1 x nqq1 eyx
dnq1 x nq eyx
dn x nq eyx
Ž
.
Ž
.
Ž
.
sx
q nq1
Ž
.
n
nq1
nq1
dx
dx
dx
n
nq
yx
n
nq
yx
d
d
x
e
d
x
e
Ž
.
Ž
.
sx
q nq1
,
10.36
Ž
.
Ž
.
n
n
dx
dx
dx
Ž
.
Ž
.
Ž
.
Using 10.33 and 10.34 in 10.36 gives
nq1
 yx
y1
x e
L
x
Ž
.
Ž .
nq1
d
n
n
 yx
 yx
sx
y1
x e
L
x
q y1
nq1 x e
L
x
Ž
.
Ž .
Ž
. Ž
.
Ž .
n
n
dx
dL
x
Ž .
n
n
 yx
s y1
x e
qnq1yx L
x qx
.
10.37
Ž
.
Ž
.
Ž .
Ž
.
n
dx
Ž
.
Ž
.nq1
x
y
Multiplying the two sides of 10.37 by y1
e x
, we obtain
dL
x
Ž .
n
L
x s xyyny1 L
x yx
,
ns0, 1, 2, . . . .
Ž .
Ž
.
Ž .
nq1
n
dx
Ž .
Ž .
This recurrence relation gives L
x in terms of L
x and its derivative.
nq1
n
Ž .
Note that L
x s1.
0

LEAST-SQUARES APPROXIMATION WITH ORTHOGONAL POLYNOMIALS
453
Another recurrence relation that does not require using the derivative of
Ž .
Ž
.
L
x is given by see Jackson, 1941, page 186
n
L
x s xyy2ny1 L
x yn qn L
x ,
ns1, 2, . . . .
Ž .
Ž
.
Ž .
Ž
.
Ž .
nq1
n
ny1
10.7. LEAST-SQUARES APPROXIMATION WITH ORTHOGONAL
POLYNOMIALS
In this section, we consider an approximation problem concerning a continu-

Ž .4n
ous function. Suppose that we have a set of polynomials,
p x
, orthogo-
i
is0
Ž .
w
x
Ž .
nal with respect to a weight function w x over the interval
a, b . Let f x
w
x
Ž .
be a continuous function on a, b . We wish to approximate f x by the sum
n
Ž .
Ý
c p x , where c , c , . . . , c are constants to be determined by minimiz-
is0
i
i
0
1
n
ing the function
2
n
b
 c , c , . . . , c
s
c p
x yf x
w x
dx,
Ž
.
Ž .
Ž .
Ž .
Ý
H
0
1
n
i
i
a
is0
	
n
	
that is,  is the square of the norm, Ý
c p yf
. If we differentiate 

is0
i
i
with respect to c , c , . . . , c
and equate the partial derivatives to zero, we
0
1
n
obtain
n
"
b
s2
c p
x yf x
p
x w x
dxs0,
is0, 1, . . . , n.
Ž .
Ž .
Ž .
Ž .
Ý
H
j
j
i
"c
a
i
js0
Hence,
n
b
b
p
x p
x w x
dx c s
f x p
x w x
dx,
is0, 1, . . . , n.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ý H
H
i
j
j
i
a
a
js0
10.38
Ž
.
Ž
.
Equations 10.38 can be written in vector form as
Scsu,
10.39
Ž
.
Ž
.
Ž
.
b Ž .
Ž . Ž .
where cs c , c , . . . , c
, us u , u , . . . , u
with u sH f x p x w x dx,
0
1
n
0
1
n
i
a
i
Ž
.
Ž
.
Ž
.
and S is an nq1  nq1 matrix whose i, j th element, s , is given by
i j
b
s s
p
x p
x w x
dx,
i, js0, 1, . . . , n.
Ž .
Ž .
Ž .
H
i j
i
j
a
Ž .
Ž .
Ž .
Since p
x , p
x , . . . , p
x
are orthogonal, then S must be a diagonal
0
1
n
matrix with diagonal elements given by
b
2
2
s s
p
x w x
dxs p
,
is0, 1, . . . , n.
Ž .
Ž .
H
ii
i
i

a

ORTHOGONAL POLYNOMIALS
454
Ž
.
y1
From equation 10.39 we get the solution csS
u. The ith element of c is
therefore of the form
ui
c s
i
sii
fp
Ž
.
i

s
,
is0, 1, . . . , n.
2
pi

For such a value of c,  has an absolute minimum, since S is positive definite.
It follows that the linear combination
n

p
x s
c p
x
Ž .
Ž .
Ý
n
i
i
is0
n
fp
Ž
.
i

s
p
x
10.40
Ž .
Ž
.
Ý
i
2
p
is0
i

Ž .
minimizes . We refer to p
x as the least-squares polynomial approxima-
n
Ž .
Ž .
Ž .
Ž .
tion of f x with respect to p
x , p
x , . . . , p
x .
0
1
n

Ž .4
Ž .
If
p
x
is a sequence of orthogonal polynomials, then p
x
in
n
ns0
n
Ž
.

wŽ
.
10.40
represents
a partial sum of the infinite series Ý
fp
r
ns0
n 
	
	 2 x
Ž .
Ž .
p
p
x . This series may fail to converge point by point to f x . It

n
n
Ž .
	 	
converges, however, to f x
in the norm

. This is shown in the next

theorem.
w
x
Theorem 10.7.1.
If f : a, b ™R is continuous, then
b
2

f x yp
x
w x
dx™0
Ž .
Ž .
Ž .
H
n
a
Ž .
Ž
.
as n™, where p
x is defined by formula 10.40 .
n
Ž
.
Proof. By the Weierstrass theorem Theorem 9.1.1 , there exists a polyno-
Ž .
Ž .
w
x
mial b
x of degree n that converges uniformly to f x on a, b , that is,
n
sup
f x yb
x
™0
as n™.
Ž .
Ž .
n
aFxFb
Hence,
b
2
f x yb
x
w x
dx™0
Ž .
Ž .
Ž .
H
n
a
as n™, since
b
b
2
2
f x yb
x
w x
dxF sup
f x yb
x
w x
dx.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
H
n
n
a
a
aFxFb

ORTHOGONAL POLYNOMIALS DEFINED ON A FINITE SET
455
Furthermore,
b
b
2
2

f x yp
x
w x
dxF
f x yb
x
w x
dx,
10.41
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
Ž
.
H
H
n
n
a
a
Ž .
since, by definition, p
x
is the least-squares polynomial approximation of
n
Ž .
Ž
.
	
 	
f x . From inequality 10.41 we conclude that
fyp
™0 as n™.


n
10.8. ORTHOGONAL POLYNOMIALS DEFINED ON A FINITE SET
Ž .
Ž .
Ž .
In this section, we consider polynomials, p
x , p
x , . . . , p
x , defined on a
0
1
n

4
finite set Ds x , x , . . . , x
such that aFx Fb, is0, 1, . . . , n. These poly-
0
2
n
i
Ž .
nomials are orthogonal with respect to a weight function w
x over D if
n

w
x
p
x
p
x
s0,
m ;
m, s0, 1, . . . , n.
Ž
.
Ž
.
Ž
.
Ý
i
m
i

i
is0
Such polynomials are said to be orthogonal of the discrete type. For example,
 Ž
.4n
the set of discrete Chebyshev polynomials,
t
j, n
, which are defined
i
is0
Ž .
over the set of integers js0, 1, . . . , n, are orthogonal with respect to w
j s
Ž
1, js0, 1, 2, . . . , n, and are given by the following formula see Abramowitz
.
and Stegun, 1964, page 791 :
i
j! nyk !
Ž
.
i
iqk
k
t
j, n s
y1
,
Ž
.
Ž
.
Ý
i
ž /ž
/
k
k
jyk !n!
Ž
.
ks0
is0, 1, . . . , n;
js0, 1, . . . , n.
10.42
Ž
.
For example, for is0, 1, 2, we have
t
j, n s1,
js0, 1, 2, . . . , n,
Ž
.
0
2
t
j, n s1y
j,
js0, 1, . . . , n,
Ž
.
1
n
6
j jy1
Ž
.
t
j, n s1y
jy
Ž
.
2
n
ny1
6 j
jy1
s1y
1y
ž
/
n
ny1
6 j
nyj
s1y
,
js0, 1, 2, . . . , n.
ž
/
n
ny1

ORTHOGONAL POLYNOMIALS
456
A recurrence relation for the discrete Chebyshev polynomials is of the form
iq1
nyi t
j, n s
2iq1
ny2 j t
j, n yi nqiq1 t
j, n ,
Ž
. Ž
.
Ž
.
Ž
. Ž
. Ž
.
Ž
.
Ž
.
iq1
i
iy1
is1, 2, . . . , n.
10.43
Ž
.
10.9. APPLICATIONS IN STATISTICS
Orthogonal polynomials play an important role in approximating distribution
functions of certain random variables. In this section, we consider only
univariate distributions.
10.9.1. Applications of Hermite Polynomials
Hermite polynomials provide a convenient tool for approximating density
functions and quantiles of distributions using convergent series. They are
associated with the normal distribution, and it is therefore not surprising that
they come up in various investigations in statistics and probability theory.
Here are some examples.
10.9.1.1. Approximation of Density Functions and Quantiles of Distributions
Ž .
Let  x
denote the density function of the standard normal distribution,
that is,
1
2
yx r2
 x s
e
,
yx.
10.44
Ž .
Ž
.
'2

Ž .4
We recall from Section 10.5 that the sequence
H
x
of Hermite
n
ns0
Ž .
yx 2 r2
polynomials is orthogonal with respect to w x se
, and that
nw2x
nw4x
nw6x
n
ny2
ny4
ny6
H
x sx y
x
q
x
y
x
q  ,
10.45
Ž .
Ž
.
n
2
3
21!
2 2!
2 3!
wrx
Ž
.Ž
.
Ž
.
Ž .
where n
sn ny1 ny2  nyrq1 . Suppose now that g x
is a den-
Ž .
sity function for some continuous distribution. We can represent g x
as a
series of the form

g x s
b H
x  x ,
10.46
Ž .
Ž .
Ž .
Ž
.
Ý
n
n
ns0
Ž
.
where, as in formula 10.32 ,

1
b s
g x H
x
dx.
10.47
Ž .
Ž .
Ž
.
H
n
n
n!
y

APPLICATIONS IN STATISTICS
457
Ž .
Ž
.
Ž
.
By substituting H
x , as given by formula
10.45 , in formula
10.47 , we
n
obtain an expression for b
in terms of the central moments,  ,  , . . . ,
n
0
1
Ž .
 , . . . , of the distribution whose density function is g x . These moments
n
are defined as

n
 s
xy
g x
dx,
ns0, 1, 2, . . . ,
Ž
.
Ž .
H
n
y
where  is the mean of the distribution. Note that  s1,  s0, and
0
1
 s	 2, the variance of the distribution. In particular, if s0, then
2
b s1,
0
b s0,
1
1
b s
 y1 ,
Ž
.
2
2
2
1
b s  ,
3
3
6
1
b s
 y6 q3 ,
Ž
.
4
4
2
24
1
b s
 y10
,
Ž
.
5
5
3
120
1
b s
 y15 q45 y15 ,
Ž
.
6
6
4
2
720
...
Ž .
Ž
.
The expression for g x in formula 10.46 can then be written as
1
1
g x s x
1q
 y1 H
x q  H
x
Ž .
Ž .
Ž
.
Ž .
Ž .
2
2
3
3
2
6
1
q
 y6 q3 H
x q  .
10.48
Ž
.
Ž .
Ž
.
4
2
4
24
This expression is known as the GramCharlier series of type A.
Ž .
Thus the GramCharlier series provides an expansion of g x in terms of
its central moments, the standard normal density, and Hermite polynomials.
Ž
.
Ž
.
Ž .
Using formulas 10.21 and 10.46 , we note that g x can be expressed as a
Ž .
series of derivatives of  x of the form
n

c
d
n
g x s
 x ,
10.49
Ž .
Ž .
Ž
.
Ý
ž /
n!
dx
ns0
where

n
c s y1
g x H
x
dx,
ns0, 1, . . . .
Ž
.
Ž .
Ž .
H
n
n
y

ORTHOGONAL POLYNOMIALS
458
Ž
.
Cramer 1946, page 223 gave conditions for the convergence of the series
´
Ž
.
Ž .
on the right-hand side of formula 10.49 , namely, if g x is continuous and
Ž
.

Ž .
Ž
2
.
of bounded variation on y,  , and if the integral H
g x exp x r4 dx is
y
Ž
.
convergent, then the series in formula 10.49 will converge for every x to
Ž .
g x .
We can utilize GramCharlier series to find the upper -quantile, x , of

Ž .
the distribution with the density function g x . This point is defined as
x g x
dxs1y.
Ž .
H
y
Ž
.
From 10.46 we have that

g x s x q
b H
x  x .
Ž .
Ž .
Ž .
Ž .
Ý
n
n
ns2
Then

x
x
x



g x
dxs
 x
dxq
b
H
x  x
dx.
10.50
Ž .
Ž .
Ž .
Ž .
Ž
.
Ý
H
H
H
n
n
y
y
y
ns2
However,
x H
x  x
dxsyH
x
 x
.
Ž .
Ž .
Ž
.
Ž
.
H
n
ny1


y
Ž
.
To prove this equality we note that by formula 10.21
n
x
x
d


n
H
x  x
dxs y1
 x
dx
Ž .
Ž .
Ž
.
Ž .
H
H
n
ž /
dx
y
y
ny1
d
n
s y1
 x
,
Ž
.
Ž
.

ž /
dx
Ž
.ny1 Ž
.
Ž
.
Ž .
where
drdx
 x
denotes the value of the
ny1 st derivative of  x

Ž
.
at x . By applying formula 10.21 again we obtain

x
n
ny1
H
x  x
dxs y1
y1
H
x
 x
Ž .
Ž .
Ž
. Ž
.
Ž
.
Ž
.
H
n
ny1


y
syH
x
 x
.
Ž
.
Ž
.
ny1


Ž
.
By making the substitution in formula 10.50 , we get

x
x


g x
dxs
 x
dxy
b H
x
 x
.
10.51
Ž .
Ž .
Ž
.
Ž
.
Ž
.
Ý
H
H
n
ny1


y
y
ns2

APPLICATIONS IN STATISTICS
459
Now, suppose that
z
is the upper -quantile of the standard normal

distribution. Then
x
z


g x
dxs1ys
 x
dx.
Ž .
Ž .
H
H
y
y
Ž
.
Using the expansion 10.51 , we obtain

x
z


 x
dxy
b H
x
 x
s
 x
dx.
10.52
Ž .
Ž
.
Ž
.
Ž .
Ž
.
Ý
H
H
n
ny1


y
y
ns2
Ž
.
If we expand the right-hand side of equation 10.52 using Taylor’s series in a
neighborhood of x , we get

j
jy1

z
x
z yx
d
Ž
.




 x
dxs
 x
dxq
 x
Ž .
Ž .
Ž
.
Ý
H
H

ž /
j!
dx
y
y
js1
j

x
z yx
Ž
.



jy1
s
 x
dxq
y1
H
x
 x
,
Ž .
Ž
.
Ž
.
Ž
.
Ý
H
jy1


j!
y
js1
using formula 10.21
Ž
.
j

x
x yz
Ž
.



s
 x
dxy
H
x
 x
.
10.53
Ž .
Ž
.
Ž
.
Ž
.
Ý
H
jy1


j!
y
js1
Ž
.
Ž
.
From formulas 10.52 and 10.53 we conclude that
j


x yz
Ž
.


b H
x
 x
s
H
x
 x
.
Ž
.
Ž
.
Ž
.
Ž
.
Ý
Ý
n
ny1


jy1


j!
ns2
js1
Ž
.
By dividing both sides by  x
we obtain

j


x yz
Ž
.


b H
x
s
H
x
.
10.54
Ž
.
Ž
.
Ž
.
Ý
Ý
n
ny1

jy1

j!
ns2
js1
This provides a relationship between x , the -quantile of the distribution

Ž .
with the density function g x , and z , the corresponding quantile for the

standard normal. Since the b ’s are functions of the moments associated with
n
Ž .
Ž
.
g x , then it is possible to use 10.54 to express x
in terms of z
and the


Ž .
Ž
.
moments of g x . This was carried out by Cornish and Fisher 1937 . They
Ž
provided an expansion for x
in terms of z
and the cumulants
instead


.
Ž . Ž
of the moments associated with g x . See Section 5.6.2 for a definition of
cumulants. Note that there is a one-to-one correspondence between mo-
.
ments
and
cumulants.
Such
an
expansion
became
known
as
the

ORTHOGONAL POLYNOMIALS
460
Ž
.
CornishFisher expansion. It is reported in Johnson and Kotz 1970, page 34
Ž
.
Ž
.
see Exercise 10.11 . See also Kendall and Stuart 1977, pages 175178 .
10.9.1.2. Approximation of a Normal Integral
A convergent series representing the integral
x
1
2
yt r2

x s
e
dt
Ž .
H
'2
o
Ž
.
was derived by Kerridge and Cook 1976 . Their method is based on the fact
that
2 nq1

x
xr2
x
Ž
.
Ž2 n.
f t
dts2
f
10.55
Ž .
Ž
.
Ý
H
ž /
2nq1 !
2
Ž
.
0
ns0
Ž .
for any function f t
with a suitably convergent Taylor’s expansion in a
neighborhood of xr2, namely,
n

1
x
x
Žn.
f t s
ty
f
.
10.56
Ž .
Ž
.
Ý
ž
/
ž /
n!
2
2
ns0
Ž
.
Ž
.
Formula 10.55 results from integrating 10.56 with respect to t from 0 to x
Ž .
yt 2 r2
and noting that the even terms vanish. Taking f t se
, we obtain
2
2 nq1
2 n
yt r2

x
xr2
d
e
Ž
.
Ž
.
2
yt r2
e
dts2
.
10.57
Ž
.
Ý
H
2 n
2nq1 !
dt
Ž
.
0
ns0
tsxr2
w
Ž
.x
Using the Rodrigues formula for Hermite polynomials formula 10.21 , we
get
dn eyx 2 r2
Ž
.
2
n
yx r2
s y1
e
H
x ,
ns0, 1, . . . .
Ž
.
Ž .
n
n
dx
Ž
.
By making the substitution in 10.57 , we find
2 nq1

x
xr2
x
Ž
.
2
2
yt r2
yx r2
e
dts2
e
H
Ý
H
2 nž /
2nq1 !
2
Ž
.
0
ns0
2 nq1

xr2
x
Ž
.
2
yx r8
s2e
H
.
10.58
Ž
.
Ý
2 nž /
2nq1 !
2
Ž
.
ns0
Ž .
n
Ž .
Ž
.
This expression can be simplified by letting %
x sx H
x rn! in 10.58 ,
n
n
which gives

x
%
xr2
Ž
.
2
2
2 n
yt r2
yx r8
e
dtsxe
.
Ý
H
2nq1
0
ns0

APPLICATIONS IN STATISTICS
461
Hence,

1
%
xr2
Ž
.
2
2 n
yx r8

x s
xe
.
10.59
Ž .
Ž
.
Ý
'
2nq1
2
ns0
Ž
.
Ž .
Note that on the basis of formula 10.25 , the recurrence relation for %
x is
n
given by
x 2 % y%
Ž
.
n
ny1
%
s
,
ns1, 2, . . . .
nq1
nq1
Ž .
The %
x ’s are easier to handle numerically than the Hermite polynomials,
n
Ž
.
as they remain relatively small, even for large n. Kerridge and Cook 1976
Ž
.
report that the series in 10.59 is accurate over a wide range of x. Divgi
Ž
.
1979 , however, states that the convergence of the series becomes slower as
x increases.
10.9.1.3. Estimation of Unknown Densities
Let X , X , . . . , X
represent a sequence of independent random variables
1
2
n
Ž .
with a common, but unknown, density function f x
assumed to be square
Ž
.
integrable. From 10.31 we have the representation

f x s
c H
x ,
Ž .
Ž .
Ý
j
j
js0
or equivalently,

f x s
a h
x ,
10.60
Ž .
Ž .
Ž
.
Ý
j
j
js0
Ž .
where h
x
is the so-called normalized Hermite polynomial of degree j,
j
namely,
1
2
yx r4
h
x s
e
H
x ,
js0, 1, . . . ,
Ž .
Ž .
j
j
1r2
'2 j!
Ž
.

Ž .
Ž .

2Ž .
Ž
.
and a sH
f x h
x dx, since H
h
x dxs1 by virtue of 10.30 .
j
y
j
y
j
Ž
.
Ž .
Schwartz 1967 considered an estimate of f x of the form
Ž .
q n
ˆf
x s
a h
x ,
Ž .
Ž .
ˆ
Ý
n
jn
j
js0
where
n
1
a s
h
X
,
Ž
.
ˆ
Ý
jn
j
k
n ks1

ORTHOGONAL POLYNOMIALS
462
Ž .
Ž .
Ž .
and q n is a suitably chosen integer dependent on n such that q n so n
Ž
.
as n™. Under these conditions, Schwartz 1967, Theorem 1 showed that
ˆŽ .
Ž .
f
x is a consistent estimator of f x in the mean integrated squared error
n
sense, that is,
2
ˆ
lim E
f
x yf x
dxs0.
Ž .
Ž .
H
n
n™
ˆ
Ž .
Ž .
Under additional conditions on f x , f
x
is also consistent in the mean
n
squared error sense, that is,
2
ˆ
lim E f x yf
x
s0
Ž .
Ž .
n
n™
uniformly in x.
10.9.2. Applications of Jacobi and Laguerre Polynomials
Ž
.
Dasgupta 1968 presented an approximation to the distribution function of
1Ž
.
Xs
rq1 , where r is the sample correlation coefficient, in terms of a beta
2
density and Jacobi polynomials. Similar methods were used by Durbin and
Ž
.
Watson 1951 in deriving an approximation of the distribution of a statistic
used for testing serial correlation in least-squares regression.
Quadratic forms in random variables, which can often be regarded as
having joint multivariate normal distributions, play an important role in
analysis of variance and in estimation of variance components for a random
or a mixed model. Approximation of the distributions of such quadratic forms
Ž
can be carried out using Laguerre polynomials see, for example, Gurland,
.
Ž
.
1953, and Johnson and Kotz, 1968 . Tiku 1964a developed Laguerre series
expansions of the distribution functions of the nonnormal variance ratios
used for testing the homogeneity of treatment means in the case of one-way
classification for analysis of variance with nonidentical group-to-group error
Ž
.
distributions that are not assumed to be normal. Tiku
1964b
also used
Laguerre polynomials to obtain an approximation to the first negative mo-
Ž
y1.
ment of a Poisson random variable, that is, the value of E X
, where X
has the Poisson distribution.
Ž
.
More recently, Schone and Schmid 2000 made use of Laguerre polyno-
¨
mials to develop a series representation of the joint density and the joint
distribution of a quadratic form and a linear form in normal variables. Such a
representation can be used to calculate, for example, the joint density and
the joint distribution function of the sample mean and sample variance. Note
that for autocorrelated variables, the sample mean and sample variance are,
in general, not independent.
10.9.3. Calculation of Hypergeometric Probabilities Using Discrete
Chebyshev Polynomials
The hypergeometric distribution is a discrete distribution, somewhat related
to the binomial distribution. Suppose, for example, we have a lot of M items,

APPLICATIONS IN STATISTICS
463
r of which are defective and Myr of which are nondefective. Suppose that
Ž
.
we choose at random m items without replacement from the lot
mFM .
Let X be the number of defectives found. Then, the probability that Xsk
is given by
r
Myr
ž /ž
/
k
myk
P Xsk s
,
10.61
Ž
.
Ž
.
Mž /
m
Ž
.
Ž
.
where max 0, myMqr FkFmin m, r . A random variable with the proba-
Ž
.
bility mass function 10.61 is said to have a hypergeometric distribution. We
Ž
.
denote such a probability function by h k; m, r, M .
Ž
. Ž
There are tables for computing the probability value in
10.61
see,for
.
example, the tables given by Lieberman and Owen, 1961 . There are also
several algorithms for computing this probability. Recently, Alvo and Cabilio
Ž
.
2000
proposed to represent the hypergeometric distribution in terms of
discrete Chebyshev polynomials, as was seen in Section 10.8. The following is
 Ž
.4m
a summary of this work: Consider the sequence
t
k, m
of discrete
n
ns0
w
Chebyshev polynomials defined over the set of integers ks0, 1, 2, . . . , m see
Ž
.x
formula 10.42 , which is given by
n
k! myi !
Ž
.
n
nqi
i
t
k, m s
y1
,
Ž
.
Ž
.
Ý
n
ž /ž
/
i
i
kyi !m!
Ž
.
is0
ns0, 1, . . . , m,
ks0, 1, . . . , m.
10.62
Ž
.
Ž
.
Let X have the hypergeometric distribution as in 10.61 . Then according to
Ž
.
Theorem 1 in Alvo and Cabilio 2000 ,
m
t
k, m h k; m, r, M st
r, M
10.63
Ž
. Ž
.
Ž
.
Ž
.
Ý
n
n
ks0
w
Ž
.
for
all
n s 0, 1, . . . , m
and
r s 0, 1, . . . , M.
Let
t s t 0, m ,
n
n
Ž
.
Ž
.x
Ž
.
t 1, m , . . . , t
m, m
, ns0, 1, . . . , m, be the base vectors in an
mq1 -
n
n
dimensional Euclidean space determined from the Chebyshev polynomials.
Ž .
Let g k
be any function defined over the set of integers, ks0, 1, . . . , m.
Ž .
Then g k can be expressed as
m
g k s
g t
k, m ,
ks0, 1, . . . , m,
10.64
Ž .
Ž
.
Ž
.
Ý
n n
ns0
	
	 2
w Ž .
Ž .
Ž
.x
where g sgt r t
, and gs g 0 , g 1 , . . . , g m
. Now, using the result
n
n
n

ORTHOGONAL POLYNOMIALS
464
Ž
.
Ž
.
in 10.63 , the expected value of g X
is given by
m
E g X
s
g k h k; m, r, M
Ž
.
Ž . Ž
.
Ý
ks0
m
m
s
g t
k, m h k; m, r, M
Ž
. Ž
.
Ý Ý
n n
ks0 ns0
m
m
s
g
t
k, m h k; m, r, M
Ž
. Ž
.
Ý
Ý
n
n
ns0
ks0
m
s
g t
r, M .
10.65
Ž
.
Ž
.
Ý
n n
ns0
Ž
.
This shows that the expected value of g X
can be computed from knowl-
edge of the coefficients g
and the discrete Chebyshev polynomials up to
n
order m, evaluated at r and M.
Ž .
In particular, if g x is an indicator function taking the value one at xsk
and the value zero elsewhere, then
E g X
sP Xsk
Ž
.
Ž
.
sh k; m, r, M .
Ž
.
Ž
.
Applying the result in 10.65 , we then obtain
m
t
k, m
Ž
.
n
h k; m, r, M s
t
r, M .
10.66
Ž
.
Ž
.
Ž
.
Ý
n
2
t
ns0
n
Ž
.
Because of the recurrence relation 10.43 for discrete Chebyshev polynomi-
Ž
.
als, calculating the hypergeometric probability using
10.66
can be done
simply on a computer.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Abramowitz, M., and I. A. Stegun 1964 . Handbook of Mathematical Functions with
Ž
Formulas, Graphs, and Mathematical Tables. Wiley, New York.
This useful
volume was prepared by the National Bureau of Standards. It was edited by
.
Milton Abramowitz and Irene A. Stegun.
Ž
.
Alvo, M., and P. Cabilio 2000 . ‘‘Calculation of hypergeometric probabilities using
Chebyshev polynomials.’’ Amer. Statist., 54, 141144.
Ž
.
Cheney, E. W. 1982 . Introduction to Approximation Theory, 2nd ed. Chelsea, New
Ž
.
York. Least-squares polynomial approximation is discussed in Chap. 4.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
465
Ž
.
Chihara, T. S. 1978 . An Introduction to Orthogonal Polynomials. Gordon and Breach,
Ž
New York. This text deals with the general theory of orthogonal polynomials,
including recurrence relations, and some particular systems of orthogonal polyno-
.
mials.
Ž
.
Cornish, E. A., and R. A. Fisher 1937 . ‘‘Moments and cumulants in the specification
of distributions.’’ Re®. Internat. Statist. Inst., 5, 307320.
Ž
.
Cramer, H.
1946 . Mathematical Methods of Statistics. Princeton University Press,
´
Ž
Princeton. This classic book provides the mathematical foundation of statistics.
.
Chap. 17 is a good source for approximation of density functions.
Ž
.
Dasgupta, P.
1968 . ‘‘An approximation to the distribution of sample correlation
coefficient, when the population is non-normal.’’ Sankhya, Ser. B., 30, 425428.
Ž
.
Ž
Davis, P. J. 1975 . Interpolation and Approximation. Dover, New York. Chaps. 8 and
.
10 discuss least-squares approximation and orthogonal polynomials.
Ž
.
Divgi, D. R.
1979 . ‘‘Calculation of univariate and bivariate normal probability
functions.’’ Ann. Statist., 7, 903910.
Ž
.
Durbin, J., and G. S. Watson 1951 . ‘‘Testing for serial correlation in least-squares
regression II.’’ Biometrika, 38, 159178.
Ž
.
Ž
Freud, G. 1971 . Orthogonal Polynomials. Pergamon Press, Oxford. This book deals
with fundamental properties of orthogonal polynomials, including Legendre,
Chebyshev, and Jacobi polynomials. Convergence theory of series of orthogonal
.
polynomials is discussed in Chap. 4.
Ž
.
Gurland, J. 1953 . ‘‘Distributions of quadratic forms and ratios of quadratic forms.’’
Ann. Math. Statist., 24, 416427.
Ž
.
Jackson, D. 1941 . Fourier Series and Orthogonal Polynomials. Mathematical Associa-
Ž
tion of America. This classic monograph provides a good coverage of orthogonal
polynomials, including Legendre, Jacobi, Hermite, and Laguerre polynomials.
.
The presentation is informative and easy to follow.
Ž
.
Johnson, N. L., and S. Kotz
1968 . ‘‘Tables of distributions of positive definite
quadratic forms in central normal variables.’’ Sankhya, Ser. B, 30, 303314.
Ž
.
Johnson, N. L., and S. Kotz 1970 . Continuous Uni®ariate Distributions1. Houghton
Ž
Mifflin,
Boston.
Chap.
12
contains
a
good
discussion
concerning
the
.
CornishFisher expansion of quantiles.
Ž
.
Kendall, M. G., and A. Stuart 1977 . The Ad®anced Theory of Statistics, Vol. 1, 4th ed.
Ž
Macmillan, New York. This classic book provides a good source for learning
.
about the GramCharlier series of Type A and the CornishFisher expansion.
Ž
.
Kerridge, D. F., and G. W. Cook 1976 . ‘‘Yet another series for the normal integral.’’
Biometrika, 63, 401403.
Ž
.
Lieberman, G. J., and Owen, D. B. 1961 . Tables of the Hypergeometric Probability
Distribution. Stanford University Press, Palo Alto, California.
Ž
.
Ralston, A., and P. Rabinowitz
1978 .
A First Course in Numerical Analysis.
Ž
McGraw-Hill, New York. Chap. 7 discusses Chebyshev polynomials of the first
.
kind .
Ž
.
Ž
Rivlin, T. 1990 . Chebyshe® Polynomials, 2nd ed. Wiley, New York. This book gives a
.
survey of the most important properties of Chebyshev polynomials.
Ž
.
Schone, A., and W. Schmid 2000 . ‘‘On the joint distribution of a quadratic and a
¨
linear form in normal variables.’’ J. Mult. Analysis, 72, 163182.

ORTHOGONAL POLYNOMIALS
466
Ž
.
Schwartz, S. C. 1967 . ‘‘Estimation of probability density by an orthogonal series,’’
Ann. Math. Statist., 38, 12611265.
Ž
.
Subrahmaniam, K.
1966 . ‘‘Some contributions to the theory of non-normalityI
Ž
.
univariate case .’’ Sankhya, Ser. A, 28, 389406.
Ž
.
Szego, G.
1975 . Orthogonal Polynomials, 4th ed. Amer. Math. Soc., Providence,
¨
Ž
Rhode Island.
This much-referenced book provides a thorough coverage of
.
orthogonal polynomials.
Ž
.
Tiku, M. L. 1964a . ‘‘Approximating the general non-normal variance ratio sampling
distributions.’’ Biometrika, 51, 8395.
Ž
.
Tiku, M. L.
1964b . ‘‘A note on the negative moments of a truncated Poisson
variate.’’ J. Amer. Statist. Assoc., 59, 12201224.
Ž
.
Viskov, O. V. 1992 . ‘‘Some remarks on Hermite polynomials.’’ Theory of Probability
and its Applications, 36, 633637.
EXERCISES
In Mathematics
'

10.1. Show that the sequence
1r 2 , cos x, sin x, cos 2 x, sin 2 x, . . . , cos nx,
4
Ž .
w
x
sin nx, . . .
is orthonormal with respect to w x s1r over y,  .

Ž .4
10.2. Let
p
x
be a sequence of Legendre polynomials
n
ns0
( )
a
Use the Rodrigues formula to show that
( )
1
m
Ž .
i
H
x p
x dxs0 for ms0, 1, . . . , ny1,
y1
n
2 nq1
y1
2n
1
n
( )
Ž .
ii
H
x p
x dxs
,
ns0, 1, 2, . . .
y1
n
ž /
n
2nq1
( )
Ž .
1
Ž .
Ž .
Ž .
b
Deduce from
a
that H
p
x 
x dxs0, where 
x
y1
n
ny1
ny1
denotes an arbitrary polynomial of degree at most equal to ny1.
( )
Ž .
Ž .
1
2Ž .
Ž
4
c
Make use of a and b to show that H
p
x dxs2r 2nq1 ,
y1
n
ns0, 1, . . . .

Ž .4
10.3. Let T
x
be a sequence of Chebyshev polynomials of the first
n
ns0
wŽ
.
x
kind. Let  scos 2iy1 r2n , is1, 2, . . . , n.
i
( )
Ž .
Ž
.
a
Verify that  ,  , . . . , 
are zeros of T
x , that is, T  s0,
1
2
n
n
n
i
is1, 2, . . . , n.
( )
Ž .
b
Show that  ,  , . . . , 
are simple zeros of T
x .
1
2
n
n
w

Ž
.
x
Hint: show that T
 0 for is1, 2, . . . , n.
n
i

Ž .4
10.4. Let H
x
be a sequence of Hermite polynomials. Show that
n
ns0
dH
x
Ž .
n
( )
Ž .
a
snH
x ,
ny1
dx
d2H
x
dH
x
Ž .
Ž .
n
n
( )
Ž .
b
yx
qnH
x s0.
n
2
dx
dx

EXERCISES
467

Ž .4

Ž .4
10.5. Let T
x
and U
x
be sequences of Chebyshev polynomials
n
ns0
n
ns0
of the first and second kinds, respectively,
( )

Ž .
a
Show that U
x
Fnq1 for y1FxF1.
n
w
Ž
.
Hint: Use the representation 10.18 and mathematical induction
x
on n.
( )

Ž .

2
b
Show that
dT
x rdx Fn
for all y1FxF1, with equality
n
Ž
.
holding only if xs1 nG2 .
( )
c
Show that
x
T
t
U
x
Ž .
Ž .
n
ny1
2
'
dtsy
1yx
H
2
'
n
y1
1yt
for y1FxF1 and n0.
Ž .
10.6. Show that the Laguerre polynomial L
x
of degree n satisfies the
n
differential equation
d2L
x
dL
x
Ž .
Ž .
n
n
x
q q1yx
qnL
x s0.
Ž
.
Ž .
n
2
dx
dx
10.7. Consider the function
1
yx trŽ1yt .
H x, t s
e
,
y1.
Ž
.
q1
1yt
Ž
.
Ž
.
n
Expand H x, t as a power series in t and let the coefficient of t
be
Ž
.n
Ž .
denoted by y1
g
x rn! so that
n
n

y1
Ž
.
n
H x, t s
g
x t .
Ž
.
Ž .
Ý
n
n!
ns0
Ž .
Ž .
Ž .
Show that g
x
is identical to L
x
for all n, where L
x
is the
n
n
n
Laguerre polynomial of degree n.
Ž .
10.8. Find the least-squares polynomial approximation of the function f x
x
w
x
se
over the interval
y1, 1
by using a Legendre polynomial of
degree not exceeding 4.
Ž .
10.9. A function f x
defined on y1FxF1 can be represented using an
infinite series of Chebyshev polynomials of the first kind, namely,

c0
f x s
q
c T
x ,
Ž .
Ž .
Ý
n
n
2
ns1

ORTHOGONAL POLYNOMIALS
468
where
2
f x T
x
Ž .
Ž .
1
n
c s
dx,
ns0, 1, . . . .
H
n
2
'

y1
1yx
Ž .
This series converges uniformly whenever f x
is continuous and of
w
x
Ž .
x
bounded variation on
y1, 1 . Approximate the function f x se
using the first five terms of the above series.
In Statistics
10.10. Suppose that from a certain distribution with a mean equal to zero we
have knowledge of the following central moments:  s1.0,  s
2
3
y0.91,  s4.86,  sy12.57,  s53.22. Obtain an approximation
4
5
6
for the density function of the distribution using GramCharlier series
of type A.
10.11. The CornishFisher expansion for x , the upper -quantile of a

certain distribution, standardized so that its mean and variance are
Ž
equal to zero and one, respectively, is of the following form
see
.
Johnson and Kotz, 1970, page 34 :
1
1
2
3
x sz q
z y1  q
z y3z

Ž
.
Ž
.



3


4
6
24
1
1
3
2
4
2
y
2 z y5z
 q
z y6z q3 
Ž
.
Ž
.


3


5
36
120
1
1
4
2
4
2
3
y
z y5z q2   q
12 z y53z q17 
Ž
.
Ž
.


3
4


3
24
324
1
5
3
q
z y10z q15z

Ž
.



6
720
1
5
3
y
2 z y17z q21z
 
Ž
.



3
5
180
1
5
3
2
y
3z y24z q29z

Ž
.



4
384
1
5
3
2
q
14z y103z q107z
 
Ž
.



3
4
288
1
5
3
4
y
252 z y1688 z q1511z
 q  ,
Ž
.



3
7776
where z
is the upper -quantile of the standard normal distribution,

Ž
.
and  is the the rth cumulant of the distribution rs3, 4, . . . . Apply
r
this expansion to finding the upper 0.05-quantile of the central chi-
squared distribution with ns5 degrees of freedom.
w Note: The mean and variance of a central chi-squared distribution
with n degrees of freedom are n and 2n, respectively. Its rth cumu-

EXERCISES
469
lant, denoted by 
, is
r

sn ry1 ! 2 ry1 ,
rs1, 2, . . . .
Ž
.
r
Hence, the rth cumulant,  , of the standardized chi-squared distribu-
r
Ž
.yr r2

 Ž
. x
tion is  s 2n

rs2, 3, . . . .
r
r
10.12. The normal integral H x eyt 2 r2 dt can be calculated from the series
0
x
3
5
7
x
x
x
2
yt r2
e
dtsxy
q
y
q  .
H
2
3
231!
2 52!
2 73!
0
( )
1 yt 2 r2
a
Use this series to obtain an approximate value for H e
dt.
0
( )
Ž .
Ž
.
b
Redo part a using the series given by formula 10.59 , that is,

x
%
xr2
Ž
.
2
2
2 n
yt r2
yx r8
e
dtsxe
.
Ý
H
2nq1
0
ns0
( )
Ž .
Ž .
c
Compare the results from a and b with regard to the number of
terms in each series needed to achieve an answer correct to five
decimal places.
Ž
.
10.13. Show that the expansion given by formula
10.46
is equivalent to
Ž .
representing the density function g x as a series of the form

n
c
d  x
Ž .
n
g x s
,
Ž .
Ý
n
n!
dx
ns0
Ž .
where  x is the standard normal density function, and the c ’s are
n
constant coefficients .
10.14. Consider the random variable
n
2
Ws
X ,
Ý
i
is1
where X , X , . . . , X are independent random variables from a distri-
1
2
n
bution with the density function

d3 x

d4 x
2 d6 x
Ž .
Ž .
Ž .
3
4
3
f x s x y
q
q
,
Ž .
Ž .
3
4
6
6
24
72
dx
dx
dx

ORTHOGONAL POLYNOMIALS
470
Ž .
where  x is the standard normal density function and the quantities

and 
are, respectively, the standard measures of skewness and
3
4
kurtosis for the distribution. Obtain the moment generating function
of W, and compare it with the moment generating function of a
Ž
chi-squared distribution with n degrees of freedom.
See Example
.
6.9.8 in Section 6.9.3.
w
x
Hint: Use Hermite polynomials.
10.15. A lot of Ms10 articles contains rs3 defectives and 7 good articles.
Suppose that a sample of ms4 articles is drawn from the lot without
replacement. Let X denote the number of defective articles in the
Ž
.
3
Ž
.
sample. Find the expected value of g X sX
using formula 10.65 .

C H A P T E R
1 1
Fourier Series
Fourier series were first formalized by the French mathematician Jean-
Ž
.
Baptiste Joseph Fourier
17681830
as a result of his work on solving a
particular partial differential equation known as the heat conduction equa-
tion. However, the actual introduction of the so-called Fourier theory was
motivated by a problem in musical acoustics concerning vibrating strings.
Ž
.
Daniel Bernoulli
17001782
is credited as being the first to model the
motion of a vibrating string as a series of trigonometric functions in 1748,
twenty years before the birth of Fourier. The actual development of Fourier
theory took place in 1807 upon Fourier’s return from Egypt, where he was a
participant in the Egyptian campaign of 1798 under Napoleon Bonaparte.
11.1. INTRODUCTION
A series of the form

a0
w
x
q
a cos nxqb sin nx
11.1
Ž
.
Ý
n
n
2
ns1
Ž .
is called a trigonometric series. Let f x be a function defined and Riemann
w
x
integrable on the interval y,  . By definition, the Fourier series associ-
Ž .
Ž
.
ated with f x is a trigonometric series of the form 11.1 , where a
and b
n
n
are given by

1
a s
f x cos nxdx,
ns0, 1, 2, . . . ,
11.2
Ž .
Ž
.
H
n

y

1
b s
f x sin nxdx,
ns1, 2, . . . .
11.3
Ž .
Ž
.
H
n

y
471

FOURIER SERIES
472
In this case, we write

a0
w
x
f x 
q
a cos nxqb sin nx .
11.4
Ž .
Ž
.
Ý
n
n
2
ns1
Ž .
The numbers a
and b
are called the Fourier coefficients of f x . The
n
n
symbol  is used here instead of equality because at this stage, nothing is
Ž
.
w
x
known about the convergence of the series in 11.4 for all x in y,  .
Ž .
Even if the series converges, it may not converge to f x .
We can also consider the following reverse approach: if the trigonometric
Ž
.
Ž .
w
x
series in 11.4 is uniformly convergent to f x on y,  , that is,

a0
w
x
f x s
q
a cos nxqb sin nx ,
11.5
Ž .
Ž
.
Ý
n
n
2
ns1
Ž
.
Ž
.
then a
and b
are given by formulas
11.2
and
11.3 . In this case, the
n
n
Ž
.
derivation of a
and b
is obtained by multiplying both sides of 11.5 by
n
n
w
x
cos nx and sin nx, respectively, followed by integration over y,  . More
Ž
.
Ž
.
specifically, to show formula 11.2 , we multiply both sides of 11.5 by cos nx.
For n0, we then have

a0
w
x
f x cos nxs
cos nxq
a cos kx cos nxqb sin kx cos nx dx.
11.6
Ž .
Ž
.
Ý
k
k
2
ks1
Since the series on the right-hand side converges uniformly, it can be
Ž
integrated term by term this can be easily proved by applying Theorem 6.6.1
.
to the sequence whose nth term is the nth partial sum of the series . We then
have


a0
f x cos nxdxs
cos nxdx
Ž .
H
H
2
y
y



q
a cos kx cos nxdxq
b sin kx cos nxdx .
Ý H
H
k
k
y
y
ks1
11.7
Ž
.
But

cos nxdxs0,
ns1, 2, . . . ,
11.8
Ž
.
H
y

sin kx cos nx dxs0,
11.9
Ž
.
H
y

0,
kn,
cos kx cos nxdxs
11.10
Ž
.
H
½  ,
ksnG1.
y

INTRODUCTION
473
Ž
. Ž
.
Ž
.
Ž
.
Ž
.
From 11.7  11.10 we conclude 11.2 . Note that formulas 11.9 and 11.10
can be shown to be true by recalling the following trigonometric identities:
1
sin kx cos nxs
sin
kqn x qsin
kyn x
,

4
Ž
.
Ž
.
2
1
cos kx cos nxs
cos
kqn x qcos
kyn x
.

4
Ž
.
Ž
.
2
Ž
.
For ns0 we obtain from 11.7 ,

f x
dxsa  .
Ž .
H
0
y
Ž
.
Formula 11.3 for b
can be proved similarly. We can therefore state the
n
following conclusion: a uniformly convergent trigonometric series is the
Fourier series of its sum.
Ž
.
Note. If the series in
11.1
converges or diverges at a point x , then it
0
Ž
.
converges or diverges at x q2n
ns1, 2, . . . due to the periodic nature of
0
Ž
.
the sine and cosine functions. Thus, if the series 11.1 represents a function
Ž .
w
x
f x
on
y,  , then the series also represents the so-called periodic
Ž .
extension of f x
for all values of x. Geometrically speaking, the periodic
Ž .
Ž .
w
x
extension of f x
is obtained by shifting the graph of f x
on y, 
by
Ž .
2, 4, . . . to the right and to the left. For example, for y3xFy, f x
Ž
.
Ž .
Ž
.
is defined by f xq2 , and for xF3, f x
is defined by f xy2 ,
Ž .
etc. This defines f x for all x as a periodic function with period 2.
Ž .
w
x
EXAMPLE 11.1.1.
Let f x be defined on y, 
by the formula
0,
yFx0,
°
x
~
f x s
Ž .
,
0FxF .
¢
Ž
.
Then, from 11.2 we have

1
a s
f x cos nxdx
Ž .
H
n

y

1
s
x cos nxdx
H
2

0


1
x sin nx
1
s
y
sin nxdx
H
2
n
n

0
0
1
n
s
y1
y1 .
Ž
.
2
2
 n

FOURIER SERIES
474
Thus, for ns1, 2, . . . ,
0,
n even,
a s
2
2
n ½ y2r  n
,
n odd.
Ž
.
Ž
.
Also, from 11.3 , we get

1
b s
f x sin nxdx
Ž .
H
n

y

1
s
x sin nxdx
H
2

0


1
x cos nx
1
s
y
q
cos nxdx
H
2
n
n

0
0

1

sin nx
n
s
y
y1
q
Ž
.
2
2
n

n
0
nq1
y1
Ž
.
s
.
 n
For ns0, a
is given by
0

1
a s
f x
dx
Ž .
H
0

y

1
s
x dx
H
2

0
1
s .
2
Ž .
The Fourier series of f x is then of the form
n


1
2
1
1
y1
Ž
.
f x 
y
cos
2ny1 x y
sin nx.
Ž .
Ž
.
Ý
Ý
2
2
4

n

2ny1
Ž
.
ns1
ns1
Ž .
2Ž
.
EXAMPLE 11.1.2.
Let f x sx
yFxF . Then

1
2
a s
x dx
H
0

y
2 2
s
,
3

CONVERGENCE OF FOURIER SERIES
475

1
2
a s
x cos nxdx
H
n

y

2

x sin nx
2
s
y
x sin nxdx
H
 n
 n
y
y

2
sy
x sin nxdx
H
 n
y


2 x cos nx
2
s
y
cos nxdx
H
2
2
 n
 n
y
y
4 cos n
s
2
n
n
4 y1
Ž
.
s
,
ns1, 2, . . . ,
2
n

1
2
b s
x sin nxdx
H
n

y
s0,
2
Ž .
since x sin nx is an odd function. Thus, the Fourier expansion of f x is
 2
cos 2 x
cos 3x
2
x 
y4 cos xy
q
y  .
2
2
ž
/
3
2
3
11.2. CONVERGENCE OF FOURIER SERIES
In this section, we consider the conditions under which the Fourier series of
Ž .
Ž .
Ž .
f x converges to f x . We shall assume that f x is Riemann integrable on
w
x
2Ž .
w
x
y,  . Hence, f
x is also Riemann integrable on y, 
by Corollary
Ž .
6.4.1. This condition will be satisfied if, for example, f x
is continuous on
w
x
Ž
y,  , or if it has a finite number of discontinuities of the first kind see
.
Definition 3.4.2 in this interval.
In order to study the convergence of Fourier series, the following lemmas
are needed:
Ž .
w
x
Lemma 11.2.1.
If f x is Riemann integrable on y,  , then

lim
f x cos nxdxs0,
11.11
Ž .
Ž
.
H
n™
y

lim
f x sin nx dxs0.
11.12
Ž .
Ž
.
H
n™
y

FOURIER SERIES
476
Ž .
Proof. Let s
x
denote the following partial sum of Fourier series of
n
Ž .
f x :
n
a0
w
x
s
x s
q
a cos kxqb sin kx ,
11.13
Ž .
Ž
.
Ý
n
k
k
2
ks1
Ž
.
Ž
.
Ž
.
where a
ks0, 1, . . . , n
and b
ks1, 2, . . . , n
are given by
11.2
and
k
k
Ž
.
11.3 , respectively. Then


a0
f x s
x
dxs
f x
dx
Ž .
Ž .
Ž .
H
H
n
2
y
y
n


q
a
f x cos kxdxqb
f x sin kxdx
Ž .
Ž .
Ý
H
H
k
k
y
y
ks1
2
n
 a0
2
2
s
q
a qb
.
Ž
.
Ý
k
k
2
ks1
It can also be verified that
2
n

 a0
2
2
2
s
x
dxs
q
a qb
.
Ž .
Ž
.
Ý
H
n
k
k
2
y
ks1
Consequently,




2
2
2
f x ys
x
dxs
f
x
dxy2
f x s
x
dxq
s
x
dx
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
H
H
H
n
n
n
y
y
y
y
2
n

 a0
2
2
2
s
f
x dxy
q
a qb
.
Ž .
Ž
.
Ý
H
k
k
2
y
ks1
It follows that
2
n

a
1
0
2
2
2
q
a qb
F
f
x
dx.
11.14
Ž .
Ž
.
Ž
.
Ý
H
k
k
2

y
ks1
Ž
.
Since the right-hand side of 11.14 exists and is independent of n, the series

Ž
2
2.
Ý
a qb
must be convergent. This follows from applying Theorem 5.1.2
ks1
k
k
and the fact that the sequence
n

2
2
s s
a qb
Ž
.
Ý
n
k
k
ks1

Ž
2
2.
is bounded and monotone increasing. But, the convergence of Ý
a qb
ks1
k
k
Ž
2
2.
Ž
.
implies that lim
a qb
s0
see Result 5.2.1 in Chapter 5 . Hence,
k ™
k
k
lim
a s0, lim
b s0.

k ™
k
k ™
k

CONVERGENCE OF FOURIER SERIES
477
Ž .
w
x
Corollary 11.2.1.
If  x is Riemann integrable on y,  ,then

1
lim
 x sin
nq
x dxs0.
11.15
Ž .
Ž
.
Ž
.
H
2
n™
y
Proof. We have that
x
x
1
 x sin
nq
x s  x cos
sin nxq  x sin
cos nx.
Ž .
Ž .
Ž .
Ž
.
2
2
2
Ž .
Ž .
Ž
.
Ž .
Ž .
Ž
.
Ž .
Ž .
Let 
x s x sin xr2 , 
x s x cos xr2 . Both 
x and 
x are
1
2
1
2
Riemann integrable by Corollary 6.4.2. By applying Lemma 11.2.1 to both
Ž .
Ž .

x and 
x , we obtain
1
2

lim

x cos nxdxs0,
11.16
Ž .
Ž
.
H
1
n™
y

lim

x sin nxdxs0.
11.17
Ž .
Ž
.
H
2
n™
y
Ž
.
Ž
.
Ž
.
Formula 11.15 follows from the addition of 11.16 and 11.17 .

Ž .
w
x
Corollary 11.2.2.
If  x is Riemann integrable on y,  , then
0
1
lim
 x sin
nq
x dxs0,
Ž .
Ž
.
H
2
n™
y

1
lim
 x sin
nq
x dxs0.
Ž .
Ž
.
H
2
n™
0
Ž .
Ž .
Proof. Define the functions h
x and h
x as
1
2
0,
0FxF ,
h
x s
Ž .
1
½  x ,
yFx0,
Ž .
 x ,
0FxF ,
Ž .
h
x s
Ž .
2
½
0,
yFx0.
Ž .
Ž .
w
x
Both h
x
and h
x
are Riemann integrable on
y,  . Hence, by
1
2
Corollary 11.2.1,

0
1
1
lim
 x sin
nq
x dxs lim
h
x sin
nq
x dx
Ž .
Ž .
Ž
.
Ž
.
H
H
1
2
2
n™
n™
y
y
s0


1
1
lim
 x sin
nq
x dxs lim
h
x sin
nq
x dx
Ž .
Ž .
Ž
.
Ž
.
H
H
2
2
2
n™
n™
0
y
s0.


FOURIER SERIES
478
Lemma 11.2.2.
1
n
1
sin
nq
u
Ž
.
2
q
cos kus
.
11.18
Ž
.
Ý
2
2 sin ur2
Ž
.
ks1
Ž .
Proof. Let G u be defined as
n
n
1
G
u s
q
cos ku.
Ž .
Ý
n
2
ks1
Ž
.
Multiplying both sides by 2 sin ur2 and using the identity
u
1
1
2 sin
cos kussin
kq
u ysin
ky
u ,
Ž
.
Ž
.
2
2
2
we obtain
n
u
u
1
1
2 sin
G
u ssin
q
sin
kq
u ysin
ky
u
Ž .

4
Ž
.
Ž
.
Ý
n
2
2
2
2
ks1
1
ssin
nq
u .
Ž
.
2
Ž
.
Hence, if sin ur2 0, then
1
sin
nq
u
Ž
.
2
G
u s
.

Ž .
n
2sin ur2
Ž
.
Ž .
w
x
Theorem 11.2.1.
Let f x be Riemann integrable on y,  , and let it
Ž .
be extended periodically outside this interval. Suppose that at a point x, f x
satisfies the following two conditions:
Ž
y.
Ž
q.
Ž
y.
Ž
q.
i. Both f x
and f x
exist, where f x
and f x
are the left-sided
Ž .
and right-sided limits of f x , and
1
y
q
f x s
f x
qf x
.
11.19
Ž .
Ž
.
Ž
.
Ž
.
2
ii. Both one-sided derivatives,
xqh yf xq
Ž
.
Ž
.

q
f
x
s lim
,
Ž
.
q
h
h™0
f xqh yf xy
Ž
.
Ž
.

y
f
x
s lim
,
Ž
.
y
h
h™0
exist.

CONVERGENCE OF FOURIER SERIES
479
Ž .
Ž .
Then the Fourier series of f x converges to f x at x, that is,
f x
if x is a point of continuity,
Ž .
s
x ™
Ž .
1
n
q
y
½ w
x
f x
qf x
if x is a point of discontinuity of the first kind.
Ž
.
Ž
.
2
Ž .
Before proving this theorem, it should be noted that if f x is continuous
Ž
.
at x, then condition 11.19 is satisfied. If, however, x is a point of disconti-
Ž .
Ž .
nuity of f x
of the first kind, then f x
is defined to be equal to the
Ž
.
Ž .
right-hand side of 11.19 . Such a definition of f x does not affect the values
Ž
.
Ž
.
of the Fourier coefficients, a and b , in 11.2 and 11.3 . Hence, the Fourier
n
n
Ž .
series of f x remains unchanged.
Ž
. Ž
.
Ž
.
Proof of Theorem 11.2.1. From 11.2 , 11.3 , and 11.13 , we have
n


1
1
s
x s
f t
dtq
f t cos kt dt cos kx
Ž .
Ž .
Ž .
Ý
H
H
n
ž
/
2

y
y
ks1

1
q
f t sin kt dt sin kx
Ž .
Hž
/

y
n

1
1
s
f t
q
cos kt cos kxqsin kt sin kx
dt
Ž .
Ž
.
Ý
H
2

y
ks1
n

1
1
s
f t
q
cos k tyx
dt.
Ž .
Ž
.
Ý
H
2

y
ks1
Ž .
Using Lemma 11.2.2, s
x can be written as
n
1

1
sin
nq
tyx
Ž
.
Ž
.
2
s
x s
f t
dt.
11.20
Ž .
Ž .
Ž
.
H
n
½
5

2 sin
tyx r2
Ž
.
y
Ž
.
If we make the change of variable tyxsu in 11.20 , we obtain
1
yx
1
sin
nq
u
Ž
.
2
s
x s
f xqu
du.
Ž .
Ž
.
H
n

2 sin ur2
Ž
.
yyx
1
Ž
.
wŽ
. x w
Ž
.x
Since both
f xqu
and sin nq
u r 2sin ur2
have period 2
with
2
respect to u, the integral from yyx to yx has the same value as the
one from y to . Thus,
1

1
sin
nq
u
Ž
.
2
s
x s
f xqu
du.
11.21
Ž .
Ž
.
Ž
.
H
n

2sin ur2
Ž
.
y

FOURIER SERIES
480
We now need to show that for each x,
1
y
q
lim s
x s
f x
qf x
.
Ž .
Ž
.
Ž
.
n
2
n™
Ž
.
Formula 11.21 can be written as
1
1
sin
nq
u
Ž
.
0
2
s
x s
f xqu
du
Ž .
Ž
.
H
n

2 sin ur2
Ž
.
y
1

1
sin
nq
u
Ž
.
2
q
f xqu
du
Ž
.
H

2 sin ur2
Ž
.
0
1
1
sin
nq
u
Ž
.
0
2
y
s
f xqu yf x
du
Ž
.
Ž
.
H

2 sin ur2
Ž
.
y
1
1
sin
nq
u
Ž
.
0
2
y
qf x
du
Ž
. H

2 sin ur2
Ž
.
y
1

1
sin
nq
u
Ž
.
2
q
q
f xqu yf x
du
Ž
.
Ž
.
H

2 sin ur2
Ž
.
0
1

1
sin
nq
u
Ž
.
2
q
qf x
du.
11.22
Ž
.
Ž
.
H

2 sin ur2
Ž
.
0
Ž
.
The first integral in 11.22 can be expressed as
1
1
sin
nq
u
Ž
.
0
2
y
f xqu yf x
du
Ž
.
Ž
.
H

2 sin ur2
Ž
.
y
y
1
f xqu yf x
u
Ž
.
Ž
.
0
1
s
sin
nq
u du.
Ž
.
H
2

u
2 sin ur2
Ž
.
y
We note that the function
f xqu yf xy
u
Ž
.
Ž
.

u
2 sin ur2
Ž
.
w
x
is Riemann integrable on y, 0 , and at us0 it has a discontinuity of the
first kind, since
f xqu yf xy
Ž
.
Ž
.

y
lim
sf
x
,
and
Ž
.
y
u
u™0
u
lim
s1,
y 2 sin ur2
Ž
.
u™0

CONVERGENCE OF FOURIER SERIES
481
that is, both limits are finite. Consequently, by applying Corollary 11.2.2 to
w Ž
.
Ž
y.x
4
w
Ž
.x
the function
f xqu yf x
ru . ur 2 sin ur2 , we get
1
1
sin
nq
u
Ž
.
0
2
y
lim
f xqu yf x
dus0.
11.23
Ž
.
Ž
.
Ž
.
H

2 sin ur2
n™
Ž
.
y
Ž
.
We can similarly show that the third integral in 11.22 has a limit equal to
zero as n™, that is,
1

1
sin
nq
u
Ž
.
2
q
lim
f xqu yf x
dus0.
11.24
Ž
.
Ž
.
Ž
.
H

2 sin
ur2
n™
Ž
.
0
Furthermore, from Lemma 11.2.2, we have
1
n
sin
nq
u
1
Ž
.
0
0
2
dus
q
cos ku du
Ý
H
H
2 sin ur2
2
Ž
.
y
y
ks1

s
,
11.25
Ž
.
2
1
n


sin
nq
u
1
Ž
.
2
dus
q
cos ku du
Ý
H
H
2 sin ur2
2
Ž
.
0
0
ks1

s
.
11.26
Ž
.
2
Ž
. Ž
.
From 11.22  11.26 , we conclude that
1
y
q
lim s
x s
f x
qf x
.

Ž .
Ž
.
Ž
.
n
2
n™
Ž .
Definition 11.2.1.
A function f x is said to be piecewise continuous on
w
x
w
x
a, b if it is continuous on a, b except for a finite number of discontinuities
w
x
Ž
q.
Ž
y.
of the first kind in a, b , and, in addition, both f a
and f b
exist.

Ž .
w
x
Corollary 11.2.3.
Suppose that f x is piecewise continuous on y,  ,
and that it can be extended periodically outside this interval. In addition, if,
w
x

Ž
q.

Ž
y.

Ž
q.
at each interior point of y,  , f
x
and f
x
exist and f y
and

Ž
y.
Ž .
f 
exist, then at a point x, the Fourier series of f x
converges to
1
y
q
w Ž
.
Ž
.x
f x
qf x
.
2
Proof. This follows directly from applying Theorem 11.2.1 and the fact
w
x
that a piecewise continuous function on a, b is Riemann integrable there.


FOURIER SERIES
482
Ž .
Ž
.
EXAMPLE 11.2.1.
Let f x sx
yFxF . The periodic extension of
Ž . Ž
w
x.
f x
outside the interval y, 
is defined everywhere. In this case,

1
a s
x cos nxdxs0,
ns0, 1, 2, . . . ,
H
n

y

1
b s
x sin nxdx
H
n

y
nq1
2 y1
Ž
.
s
,
ns1, 2, . . . .
n
Ž .
Hence, the Fourier series of f x is
nq1

2 y1
Ž
.
x
sin nx.
Ý
n
ns1
Ž
.
This series converges to x at each x in y,  . At xsy,  we have
Ž .
discontinuities of the first kind for the periodic extension of f x . Hence, at
xs, the Fourier series converges to
1
1
y
q
f 
qf 
s
q y
Ž
.
Ž
.
Ž
.
2
2
s0.
Similarly, at xsy, the series converges to
1
1
y
q
f y
qf y
s
q y
Ž
.
Ž
.
Ž
.
2
2
s0.
For other values of x, the series converges to the value of the periodic
Ž .
extension of f x .
Ž .
2
EXAMPLE 11.2.2.
Consider the function f x sx
defined in Example
11.1.2. Its Fourier series is
n
2


y1
Ž
.
2
x 
q4
cos nx.
Ý
2
3
n
ns1
Ž .
The periodic extension of f x is continuous everywhere. We can therefore
write
n
2


y1
Ž
.
2
x s
q4
cos nx.
Ý
2
3
n
ns1

DIFFERENTIATION AND INTEGRATION OF FOURIER SERIES
483
In particular, for xs, we have
2 n
2


y1
Ž
.
2
 s
q4
,
Ý
2
3
n
ns1
2


1
s
.
Ý
2
6
n
ns1
11.3. DIFFERENTIATION AND INTEGRATION
OF FOURIER SERIES
Ž .
In Section 11.2 conditions were given under which a function f x defined on
w
x
y, 
is represented as a Fourier series. In this section, we discuss the
conditions under which the series can be differentiated or integrated term by
term.

w
x
Theorem 11.3.1.
Let a r2qÝ
a cos nxqb sin nx
be the Fourier
0
ns1
n
n
Ž .
Ž .
w
x
Ž
.
Ž
.

Ž .
series of f x . If f x is continuous on y,  , f y sf  , and f
x is
w
x
piecewise continuous on y,  , then
Ž .

Ž .
a. at each point where f
x
exists, f
x
can be represented by the
Ž .
derivative of the Fourier series of f x , where differentiation is done
term by term, that is,


w
x
f
x s
nb cos nxyna sin nx ;
Ž .
Ý
n
n
ns1
Ž .
Ž .
b. the Fourier series of f x
converges uniformly and absolutely to f x
w
x
on y,  .
Proof.
Ž .
Ž .
a. The Fourier series of f x converges to f x by Corollary 11.2.3. Thus,

a0
w
x
f x s
q
a cos nxqb sin nx .
Ž .
Ý
n
n
2
ns1
Ž .
Ž
.
Ž
.
The periodic extension of f x
is continuous, since f  sf y .

Ž .
Ž .
Furthermore, the derivative, f
x , of f x
satisfies the conditions of

Ž .

Ž .
Corollary 11.2.3. Hence, the Fourier series of f
x converges to f
x ,
that is,

0

w
x
f
x s
q
 cos nxq sin nx ,
11.27
Ž .
Ž
.
Ý
n
n
2
ns1

FOURIER SERIES
484
where

1

 s
f
x
dx
Ž .
H
0

y
1
s
f  yf y
Ž
.
Ž
.

s0,

1

 s
f
x cos nxdx
Ž .
H
n

y

1
n

s
f x cos nx
q
f x sin nx dx
Ž .
Ž .
H
y


y
n
y1
Ž
.
s
f  yf y
qnb
Ž
.
Ž
.
n
n
snb ,
n

1

 s
f
x sin nxdx
Ž .
H
n

y

1
n

s
f x sin nx
y
f x cos nxdx
Ž .
Ž .
H
y


y
syna .
n
Ž
.
By substituting  ,  , and 
in 11.27 , we obtain
0
n
n


w
x
f
x s
nb cos nxyna sin nx .
Ž .
Ý
n
n
ns1

Ž .
Ž
.
b. Consider the Fourier series of f
x in 11.27 , where  s0,  snb ,
0
n
n
Ž
.
 syna . Then, using inequality 11.14 , we obtain
n
n


1
2

2
2
 q
F
f
x
dx.
11.28
Ž .
Ž
.
Ž
.
Ý
H
n
n

y
ns1
Ž
.

Ž
2
2.
Inequality 11.28 indicates that Ý
 q
is a convergent series.
ns1
n
n
Ž .
n
w
x
Now,
let
s
x s a r2 q Ý
a cos kx q b sin kx .
Then,
for
n
0
ks1
k
k
nGmq1,
n
w
x
s
x ys
x
s
a cos kxqb sin kx
Ž .
Ž .
Ý
n
m
k
k
ksmq1
n
F
a cos kxqb sin kx .
Ý
k
k
ksmq1

DIFFERENTIATION AND INTEGRATION OF FOURIER SERIES
485
Note that
1r2
2
2
a cos kxqb sin kx F a qb
.
11.29
Ž
.
Ž
.
k
k
k
k
Ž
.
Inequality 11.29 follows from the fact that a cos kxqb sin kx is the
k
k
Ž
.
Ž
.
dot product, u  v, of the vectors us a , b
, vs cos kx, sin kx , and
k
k


	 	 	 	
Ž
.
u  v F u
v
see Theorem 2.1.2 . Hence,
2
2
n
1r2
2
2
s
x ys
x
F
a qb
Ž .
Ž .
Ž
.
Ý
n
m
k
k
ksmq1
n
1
1r2
2
2
s
 q
.
11.30
Ž
.
Ž
.
Ý
k
k
k
ksmq1
But, by the CauchySchwarz inequality,
1r2
1r2
n
n
n
1
1
1r2
2
2
2
2
 q
F
 q
,
Ž
.
Ž
.
Ý
Ý
Ý
k
k
k
k
2
k
k
ksmq1
ksmq1
ksmq1
Ž
.
and by 11.28 ,
n

1
2

2
2
 q
F
f
x
dx.
Ž .
Ž
.
Ý
H
k
k

y
ksmq1
In addition, Ý
1rk 2 is a convergent series. Hence, by the Cauchy
ks1
criterion, for a given 0, there exists a positive integer N such that
Ýn
1rk 2 2 if nmN. Hence,
ksmq1
n
s
x ys
x
F
a cos kxqb sin kx
Ž .
Ž .
Ý
n
m
k
k
ksmq1
Fc ,
if mnN,
11.31
Ž
.
Ž
. 
w

Ž .x2
41r2
Ž
.
where
cs 1r H
f
x
dx
. The double inequality
11.31
y
Ž .
shows that the Fourier series of f x
converges absolutely and uni-
Ž .
w
x
formly to f x on y, 
by the Cauchy criterion.

Ž
.

Ž
2
2.1r2
Note that from 11.30 we can also conclude that Ý
a qb
satisfies
ks1
k
k
the Cauchy criterion. This series is therefore convergent. Furthermore, it is
easy to see that


1r2
2
2
a
q b
F
2 a qb
.
Ž
.
Ž
.
Ý
Ý
k
k
k
k
ks1
ks1

FOURIER SERIES
486

Ž


 .
This indicates that the series Ý
a
q b
is convergent by the compari-
ks1
k
k
son test.
Note that, in general, we should not expect that a term-by-term differenti-
Ž .

Ž .
ation of a Fourier series of f x will result in a Fourier series of f
x . For
Ž .
Ž
example, for the function f x sx, yFxF, the Fourier series is see
.
Example 11.2.1
nq1

2 y1
Ž
.
x
sin nx.
Ý
n
ns1

Ž
.nq1
Differentiating this series term by term, we obtain Ý
2 y1
cos nx.
ns1

Ž .
This, however, is not the Fourier series of f
x s1, since the Fourier series

Ž .
Ž
.
Ž
.
of f
x s1 is just 1. Note that in this case, f  f y , which violates
one of the conditions in Theorem 11.3.1.
Ž .
w
x
Theorem 11.3.2.
If f x is piecewise continuous on y,  and has the
Fourier series

a0
w
x
f x 
q
a cos nxqb sin nx ,
11.32
Ž .
Ž
.
Ý
n
n
2
ns1
then a term-by-term integration of this series gives the Fourier series of
x
Ž .
w
x
H
f t dt for xg y,  , that is,
y

x
a
qx
a
b
Ž
.
0
n
n
f t
dts
q
sin nxy
cos nxycos n
,
Ž .
Ž
.
Ý
H
2
n
n
y
ns1
yFxF .
x
Ž .
Furthermore, the integrated series converges uniformly to H
f t dt.
y
Ž .
Proof. Define the function g x as
x
a0
g x s
f t
dty
x.
11.33
Ž .
Ž .
Ž
.
H
2
y
Ž .
w
x
If f x
is piecewise continuous on y,  , then it is Riemann integrable
Ž .
w
x
there, and by Theorem 6.4.7, g x
is continuous on y,  . Furthermore,
Ž .
Ž .
by Theorem 6.4.8, at each point where f x is continuous, g x is differen-
tiable and
a0

g
x sf x y
.
11.34
Ž .
Ž .
Ž
.
2

DIFFERENTIATION AND INTEGRATION OF FOURIER SERIES
487

Ž .
w
x
This implies that g
x
is piecewise continuous on y,  . In addition,
Ž
.
Ž
.
Ž
.
g y sg  . To show this, we have from 11.33
y
a0
g y s
f t
dtq

Ž
.
Ž .
H
2
y
a0
s
 .
2

a0
g  s
f t
dty

Ž
.
Ž .
H
2
y
a0
sa y

0
2
a0
s
 ,
2
Ž .
by the definition of a . Thus, the function g x
satisfies the conditions of
0
Ž .
Theorem 11.3.1. It follows that the Fourier series of g x
converges uni-
Ž .
w
x
formly to g x on y,  . We therefore have

A0
w
x
g x s
q
A cos nxqB sin nx .
11.35
Ž .
Ž
.
Ý
n
n
2
ns1
Ž .
Moreover, by part a of Theorem 11.3.1, we have


w
x
g
x s
nB cos nxynA sin nx .
11.36
Ž .
Ž
.
Ý
n
n
ns1
Ž
. Ž
.
Ž
.
Then, from 11.32 , 11.34 , and 11.36 , we obtain
a snB ,
ns1, 2, . . . ,
n
n
b synA ,
ns1, 2, . . . .
n
n
Ž
.
Substituting in 11.35 , we get

A
b
a
0
n
n
g x s
q
y
cos nxq
sin nx .
Ž .
Ý
2
n
n
ns1
Ž
.
From 11.33 we then have

x
a x
A
b
a
0
0
n
n
f t
dts
q
q
y
cos nxq
sin nx .
11.37
Ž .
Ž
.
Ý
H
2
2
n
n
y
ns1

FOURIER SERIES
488
Ž
.
To find the value of A , we set xsy in 11.37 , which gives
0

a 
A
b
0
0
n
0sy
q
q
y
cos n .
Ý ž
/
2
2
n
ns1
Hence,

A
a 
b
0
0
n
s
q
cos n .
Ý
2
2
n
ns1
Ž
.
Substituting A r2 in 11.37 , we finally obtain
0

x
a
qx
a
b
Ž
.
0
n
n
f t
dts
q
sin nxy
cos nxycos n
.

Ž .
Ž
.
Ý
H
2
n
n
y
ns1
11.4. THE FOURIER INTEGRAL
We have so far considered Fourier series corresponding to a function defined
w
x
on the interval y,  . As was seen earlier in this chapter, if a function is
w
x
w
x
initially defined on y,  , we can extend its definition outside y,  by
Ž
.
Ž
.
considering its periodic extension. For example, if f y sf  , then we
Ž .
Ž
.
Ž
.
Ž .
can define f x everywhere in y,  by requiring that f xq2 sf x for
w
x
all x. The choice of the interval y, 
was made mainly for convenience.
Ž .
More generally, we can now consider a function f x defined on the interval
w
x
yc, c . For such a function, the corresponding Fourier series is given by

a
n x
n x
0 q
a cos
qb sin
,
11.38
Ž
.
Ý
n
n
ž
/
ž
/
2
c
c
ns1
where
c
1
n x
a s
f x cos
dx,
ns0, 1, 2, . . . ,
11.39
Ž .
Ž
.
H
n
ž
/
c
c
yc
c
1
n x
b s
f x sin
dx,
ns1, 2, . . . .
11.40
Ž .
Ž
.
H
n
ž
/
c
c
yc
Ž .
Now, a question arises as to what to do when we have a function f x that
Ž
.
is already defined everywhere on y,  , but is not periodic. We shall show
that, under certain conditions, such a function can be represented by an
infinite integral rather than by an infinite series. This integral is called a
Fourier integral. We now show the development of such an integral.
Ž
. Ž
.
Substituting the expressions for a
and b
given by 11.39 , 11.40 into
n
n
Ž
.
11.38 , we obtain the Fourier series

c
c
1
1
n
f t
dtq
f t cos
tyx
dt.
11.41
Ž .
Ž .
Ž
.
Ž
.
Ý
H
H
2c
c
c
yc
yc
ns1

THE FOURIER INTEGRAL
489
Ž .
w
x
If c is finite and f x satisfies the conditions of Corollary 11.2.3 on yc, c ,
1
y
q
Ž
.
w Ž
.
Ž
.x
then the Fourier series 11.41 converges to
f x
qf x
. However, this
2
Ž .
w
x
series representation of f x
is not valid outside the interval yc, c unless
Ž .
f x is periodic with the period 2c.
In order to provide a representation that is valid for all values of x when
Ž .
Ž
.
f x
is not periodic, we need to consider extending the series in 11.41 by
Ž .
letting c go to infinity, assuming that f x
is absolutely integrable over the
whole real line. We now show how this can be done:
Ž
.

Ž .
As c™, the first term in 11.41 goes to zero provided that H
f t dt
y
Ž
.
exists. To investigate the limit of the series in
11.41
as c™, we set
 s rc,  s 2rc, . . . ,  s nrc, . . . ,   s 
y  s rc,
n s
1
2
n
n
nq1
n
1, 2, . . . . We can then write


c
c
1
n
1
f t cos
tyx
dts
 
f t cos 
tyx
dt.
Ž .
Ž
.
Ž .
Ž
.
Ý
Ý
H
H
n
n
c
c

yc
yc
ns1
ns1
11.42
Ž
.
Ž
.
When c is large,  
is small, and the right-hand side of 11.42 will be an
n
approximation of the integral


1
f t cos  tyx
dt
d.
11.43
Ž .
Ž
.
Ž
.
H H½
5

0
y
Ž .
Ž
.
This is the Fourier integral of f x . Note that 11.43 can be written as

a  cos xqb  sin x d,
11.44
Ž .
Ž .
Ž
.
H
0
where

1
a  s
f t cos t dt,
Ž .
Ž .
H

y

1
b  s
f t sin t dt.
Ž .
Ž .
H

y
Ž
.
The expression in 11.44 resembles a Fourier series where the sum has been
replaced by an integral and the parameter  is used in place of the integer n.
Ž .
Ž .
Moreover, a  and b  act like Fourier coefficients.
Ž
.
We now show that the Fourier integral in 11.43 provides a representa-
Ž .
Ž .
tion for f x provided that f x satisfies the conditions of the next theorem.
Ž .
Theorem 11.4.1.
Let f x be piecewise continuous on every finite inter-
w
x

 Ž .
Ž
.
val
a, b . If H
f x
dx exists, then at every point x yx where
y

FOURIER SERIES
490


1
q
y
y
Ž
.
Ž
.
Ž .
w Ž
.
f
x
and f
x
exist, the Fourier integral of f x converges to
f x
q
2
Ž
q.x
f x
, that is,


1
1
y
q
f t cos  tyx
dt ds
f x
qf x
.
Ž .
Ž
.
Ž
.
Ž
.
H H
2
½
5

0
y
The proof of this theorem depends on the following lemmas:
Ž .
w
x
Lemma 11.4.1.
If f x is piecewise continuous on a, b , then
b
lim
f x sin nxdxs0,
11.45
Ž .
Ž
.
H
n™
a
b
lim
f x cos nx dxs0.
11.46
Ž .
Ž
.
H
n™
a
w
x
Proof. Let the interval
a, b be divided into a finite number of subinter-
Ž .
vals on each of which f x is continuous. Let any one of these subintervals be
w
x
Ž
.
denoted by
p, q . To prove formula 11.45 we only need to show that
q
lim
f x sin nxdxs0.
11.47
Ž .
Ž
.
H
n™
p
w
x
For this purpose, we divide the interval
p, q into k equal subintervals using
the partition points x sp, x , x , . . . , x sq. We can then write the integral
0
1
2
k
Ž
.
in 11.47 as
ky1
xiq1 f x sin nx dx,
Ž .
Ý H
xi
is0
or equivalently as
ky1
x
x
iq1
iq1
f x
sin nxdxq
f x yf x
sin nxdx .
Ž
.
Ž .
Ž
.
Ý
H
H
i
i
½
5
x
x
i
i
is0
It follows that
ky1
q
cos nx ycos nx
i
iq1
f x sin nxdx F
f x
Ž .
Ž
.
Ý
H
i
n
p
is0
ky1
xiq1
q
f x yf x
dx.
Ž .
Ž
.
Ý H
i
xi
is0

THE FOURIER INTEGRAL
491
 Ž .
w
x
Let M denote the maximum value of f x
on
p, q . Then
ky1
q
x
2 Mk
iq1
f x sin nxdx F
q
f x yf x
dx.
11.48
Ž .
Ž .
Ž
.
Ž
.
Ý
H
H
i
n
p
xi
is0
Ž .
w
x
Furthermore, since f x
is continuous on
p, q , it is uniformly continuous
w
Ž .
there if necessary, f x
can be made continuous at p, q by simply using
Ž
q.
Ž
y.
Ž .
x
f p
and f q
as the values of f x
at p, q, respectively . Hence, for a
given 0, there exists a 0 such that

f x
yf x

11.49
Ž
.
Ž
.
Ž
.
1
2
2 qyp
Ž
.


w
x
if
x yx
, where x
and x
are points in
p, q . If k is chosen large
1
2
1
2




enough so that
x
yx , and hence
xyx  if x FxFx
, then
iq1
i
i
i
iq1
Ž
.
from 11.48 we obtain
ky1
q
x
2 Mk

iq1
f x sin nxdx F
q
dx,
Ž .
Ý
H
H
n
2 qyp
Ž
.
p
xi
is0
or
q
2 Mk

f x sin nxdx F
q
,
Ž .
H
n
2
p
since
ky1
ky1
xiq1 dxs
x
yx
Ž
.
Ý
Ý
H
iq1
i
xi
is0
is0
sqyp.
Choosing n large enough so that 2 Mkrnr2, we finally get
q
f x sin nxdx .
11.50
Ž .
Ž
.
H
p
Ž
.
Ž
.
Ž
.
Formula 11.45 follows from 11.50 , since 0 is arbitrary. Formula 11.46
can be proved in a similar fashion.

Ž .
w
x

Ž q.
Lemma 11.4.2.
If
f x
is piecewise continuous on
0, b
and
f 0
exists, then
sin nx

b
q
lim
f x
dxs
f 0
.
Ž .
Ž
.
H
x
2
n™
0

FOURIER SERIES
492
Proof. We have that
sin nx
sin nx
b
b
q
f x
dxsf 0
dx
Ž .
Ž
.
H
H
x
x
0
0
f x yf 0q
Ž .
Ž
.
b
q
sin nxdx.
11.51
Ž
.
H
x
0
But
sin nx
sin x
b
bn
lim
dxs lim
dx
H
H
x
x
n™
n™
0
0
 sin x

s
dxs
H
x
2
0
Ž
.
Ž
.w Ž .
Ž q.x
see Gillespie, 1959, page 89 . Furthermore, the function 1rx
f x yf 0
w
x
Ž .
is piecewise continuous on 0, b , since f x is, and
f x yf 0q
Ž .
Ž
.

q
lim
sf
0
,
Ž
.
q
x
x™0
which exists. Hence, by Lemma 11.4.1,
f x yf 0q
Ž .
Ž
.
b
lim
sin nxdxs0.
H
x
n™
0
Ž
.
From 11.51 we then have
sin nx

b
q
lim
f x
dxs
f 0
.

Ž .
Ž
.
H
x
2
n™
0
Ž .
w
x

Ž
y.
Lemma 11.4.3.
If f x
is piecewise continuous on
a, b , and f
x
,
0

Ž
q.
f
x
exist at x , ax b, then
0
0
0
sin n xyx

Ž
.
b
0
y
q
lim
f x
dxs
f x
qf x
.
Ž .
Ž
.
Ž
.
H
0
0
xyx
2
n™
a
0
Proof. We have that
x
sin n xyx
sin n xyx
Ž
.
Ž
.
b
0
0
0
f x
dxs
f x
dx
Ž .
Ž .
H
H
xyx
xyx
a
a
0
0
sin n xyx
Ž
.
b
0
q
f x
dx
Ž .
H
xyx
x
0
0
x ya
sin nx
0
s
f x yx
dx
Ž
.
H
0
x
0
sin nx
byx 0
q
f x qx
dx.
Ž
.
H
0
x
0

THE FOURIER INTEGRAL
493
Lemma 11.4.2 applies to each of the above integrals, since the right-hand
Ž
.
Ž
.

Ž
y.

Ž
q.
derivatives of f x yx
and f x qx
at xs0 are yf
x
and f
x
,
0
0
0
0
respectively, and both derivatives exist. Furthermore,
lim f x yx sf xy
Ž
.
Ž
.
0
0
q
x™0
and
lim f x qx sf xq .
Ž
.
Ž
.
0
0
q
x™0
It follows that
sin n xyx

Ž
.
b
0
y
q
lim
f x
dxs
f x
qf x
.

Ž .
Ž
.
Ž
.
H
0
0
xyx
2
n™
a
0
Ž .
Proof of Theorem 11.4.1. The function f x
satisfies the conditions of
w
x
Lemma 11.4.3 on the interval a, b . Hence, at any point x, axb, where

Ž
y.

Ž
q.
f
x
and f
x
exist,
0
0
sin  tyx

Ž
.
b
y
q
lim
f t
dts
f x
qf x
.
11.52
Ž .
Ž
.
Ž
.
Ž
.
H
tyx
2
™
a
Let us now partition the integral

sin  tyx
Ž
.
Is
f t
dt
Ž .
H
tyx
y
as
a
sin  tyx
sin  tyx
Ž
.
Ž
.
b
Is
f t
dtq
f t
dt
Ž .
Ž .
H
H
tyx
tyx
y
a

sin  tyx
Ž
.
q
f t
dt.
11.53
Ž .
Ž
.
H
tyx
b
Ž
.
From the first integral in 11.53 we have
a
a
sin  tyx
f t
Ž
.
Ž .
f t
dt F
dt.
Ž .
H
H
tyx
tyx
y
y


Since tFa and ax, then tyx Gxya. Hence,
a
a
f t
1
Ž .
dtF
f t
dt.
11.54
Ž .
Ž
.
H
H
tyx
xya
y
y

FOURIER SERIES
494
Ž
.

 Ž .
The integral on the right-hand side of
11.54
exists because H
f t
dt
y
Ž
.
does. Similarly, from the third integral in 11.53 we have, if xb,


sin  tyx
f t
Ž
.
Ž .
f t
dt F
dt
Ž .
H
H
tyx
tyx
b
b

1
F
f t
dt
Ž .
H
byx
b

1
F
f t
dt.
Ž .
H
byx
y
Ž
.
Hence, the first and third integrals in 11.53 are convergent. It follows that
for any 0, there exists a positive number N such that if ayN and
bN, then these integrals will each be less than r3 in absolute value.
Ž
.
Furthermore, by 11.52 , the absolute value of the difference between the
Ž
.
Ž
.w Ž
y.
Ž
q.x
second integral in 11.53 and the value r2
f x
qf x
can be made
less then r3, if  is chosen large enough. Consequently, the absolute value
Ž
.w Ž
y.
Ž
q.x
of the difference between the value of the integral I and r2
f x
qf x
will be less than , if  is chosen large enough. Thus,

sin  tyx

Ž
.
y
q
lim
f t
dts
f x
qf x
.
11.55
Ž .
Ž
.
Ž
.
Ž
.
H
tyx
2
™
y
w Ž
.x Ž
.
Ž
.
The expression sin  tyx r tyx in 11.55 can be written as
sin  tyx
Ž
.

s
cos  tyx
d.
Ž
.
H
tyx
0
Ž
.
Formula 11.55 can then be expressed as

1

1
y
q
f x
qf x
s
lim
f t
dt
cos  tyx
d
Ž
.
Ž
.
Ž .
Ž
.
H
H
2
 ™
y
0

1

s
lim
d
f t cos  tyx
dt.
11.56
Ž .
Ž
.
Ž
.
H
H
 ™
0
y
Ž
.
The change of the order of integration in
11.56
is valid because the
Ž
.
 Ž .
integrand in
11.56
does not exceed
f t
in absolute value, so that the

Ž .
w
Ž
.x
Ž
integral H
f t cos  tyx
dt converges uniformly for all 
see Carslaw,
y
.
Ž
.
1930, page 199; Pinkus and Zafrany, 1997, page 187 . From 11.56 we finally
obtain


1
1
y
q
f x
qf x
s
f t cos  tyx
dt
d.

Ž
.
Ž
.
Ž .
Ž
.
H H
2
½
5

0
y

APPROXIMATION OF FUNCTIONS BY TRIGONOMETRIC POLYNOMIALS
495
11.5. APPROXIMATION OF FUNCTIONS BY TRIGONOMETRIC
POLYNOMIALS
By a trigonometric polynomial of the nth order it is meant an expression of
the form
n
0
w
x
t
x s
q
 cos kxq sin kx .
11.57
Ž .
Ž
.
Ý
n
k
k
2
ks1
A theorem of Weierstrass states that any continuous function of period 2
can be uniformly approximated by a trigonometric polynomial of some order
Ž
.
see, for example, Tolstov, 1962, Chapter 5 . Thus, for a given 0, there
Ž
.
exists a trigonometric polynomial of the form 11.57 such that
f x yt
x

Ž .
Ž .
n
Ž .
for all values of x. In case the Fourier series for f x is uniformly convergent,
Ž .
Ž .
then t
x
can be chosen to be equal to s
x , the nth partial sum of the
n
n
Ž .
Fourier series. However, it should be noted that t
x is not merely a partial
n
Ž .
sum of the Fourier series for f x , since a continuous function may have a
Ž
.
Ž .
divergent Fourier series see Jackson,1941, page 26 . We now show that s
x
n
has a certain optimal property among all trigonometric polynomials of the
Ž .
same order. To demonstrate this fact, let f x
be Riemann integrable on
w
x
Ž .
y,  , and let s
x be the partial sum of order n of its Fourier series, that
n
Ž .
n
w
x
Ž .
Ž .
Ž .
is, s
x sa r2qÝ
a cos kxqb sin kx . Let r
x sf x ys
x . Then,
n
0
ks1
k
k
n
n
Ž
.
from 11.2 ,


f x cos kxdxs
s
x cos kxdx
Ž .
Ž .
H
H
n
y
y
s a ,
ks0, 1, . . . , n.
k
Hence,

r
x cos kxdxs0
for kFn.
11.58
Ž .
Ž
.
H
n
y
We can similarly show that

r
x sin kxdxs0
for kFn.
11.59
Ž .
Ž
.
H
n
y

FOURIER SERIES
496
Ž .
Ž .
Ž .
Ž .
Ž
.
Now, let u
x st
x ys
x , where t
x is given by 11.57 . Then
n
n
n
n


2
2
f x yt
x
dxs
r
x yu
x
dx
Ž .
Ž .
Ž .
Ž .
H
H
n
n
n
y
y



2
2
s
r
x
dxy2
r
x u
x
dxq
u
x
dx
Ž .
Ž .
Ž .
Ž .
H
H
H
n
n
n
n
y
y
y


2
2
s
f x ys
x
dxq
u
x
dx,
11.60
Ž .
Ž .
Ž .
Ž
.
H
H
n
n
y
y
Ž
.
Ž
.
since, by 11.58 and 11.59 ,

r
x u
x
dxs0.
Ž .
Ž .
H
n
n
y
Ž
.
From 11.60 it follows that


2
2
f x yt
x
dxG
f x ys
x
dx.
11.61
Ž .
Ž .
Ž .
Ž .
Ž
.
H
H
n
n
y
y

w Ž .
This shows that for all trigonometric polynomials of order n, H
f x y
y
Ž .x2
Ž .
Ž .
t
x
dx is minimized when t
x ss
x .
n
n
n
11.5.1. Parseval’s Theorem
Ž
.
Ž .
Suppose that we have the Fourier series 11.5 for the function f x , which is
Ž .
assumed to be continuous of period 2. Let s
x be the nth partial sum of
n
the series. We recall from the proof of Lemma 11.2.1 that
2
n


 a0
2
2
2
2
f x ys
x
dxs
f
x
dxy
q
a qb
.
11.62
Ž .
Ž .
Ž .
Ž
.
Ž
.
Ý
H
H
n
k
k
2
y
y
ks1
We also recall that for a given 0, there exists a trigonometric polynomial
Ž .
t
x of order n such that
n
f x yt
x
.
Ž .
Ž .
n
Hence,

2
2
f x yt
x
dx2 .
Ž .
Ž .
H
n
y

THE FOURIER TRANSFORM
497
Ž
.
Applying 11.61 , we obtain


2
2
f x ys
x
dxF
f x yt
x
dx
Ž .
Ž .
Ž .
Ž .
H
H
n
n
y
y
2 2.
11.63
Ž
.
Ž
.
Ž
.
Since 0 is arbitrary, we may conclude from 11.62 and 11.63 that the
Ž
.
limit of the right-hand side of 11.62 is zero as n™, that is,
2


1
a0
2
2
2
f
x
dxs
q
a qb
.
Ž .
Ž
.
Ý
H
k
k

2
y
ks1
This result is known as Parse®al’s theorem after Marc Antoine Parseval
Ž
.
17551836 .
11.6. THE FOURIER TRANSFORM
In the previous sections we discussed Fourier series for functions defined on
Ž
a finite interval
or periodic functions defined on R, the set of all real
.
numbers . In this section, we study a particular transformation of functions
defined on R which are not periodic.
Ž .
Ž
.
Ž .
Let f x be defined on Rs y,  . The Fourier transform of f x is a
function defined on R as

1
yi w x
F w s
f x e
dx,
11.64
Ž
.
Ž .
Ž
.
H
2
y
'
where i the complex number
y1 , and
eyi w xscos wxyi sin wx.
A proper understanding of such a transformation requires some knowledge
of complex analysis, which is beyond the scope of this book. However, due to
the importance and prevalence of the use of this transformation in various
fields of science and engineering, some coverage of its properties is neces-
sary. For this reason, we merely state some basic results and properties
concerning this transformation. For more details, the reader is referred to
Ž
standard books on Fourier series, for example, Pinkus and Zafrany 1997,
.
Ž
.
Ž
Chapter 3 , Kufner and Kadlec
1971, Chapter 8 , and Weaver
1989,
.
Chapter 6 .
Ž .
Theorem 11.6.1.
If f x
is absolutely integrable on R, then its Fourier
Ž
.
transform F w exists.

FOURIER SERIES
498
Ž .
Theorem 11.6.2.
If f x
is piecewise continuous and absolutely inte-
Ž
.
grable on R, then its Fourier transform F w has the following properties:
Ž
.
a. F w is a continuous function on R.
Ž
.
b. lim
F w s0.
w ™
Ž .
Note that f x is piecewise continuous on R if it is piecewise continuous
w
x
on each finite interval a, b .
Ž .
y x
EXAMPLE 11.6.1.
Let f x se
. This function is absolutely integrable
on R, since


y x
yx
e
dxs2
e
dx
H
H
y
0
s2.
Its Fourier transform is given by

1
y x
yi w x
F w s
e
e
dx
Ž
.
H
2
y

1
y x
s
e
cos wxyi sin wx
dx
Ž
.
H
2
y

1
y x
s
e
cos wx dx
H
2
y

1
yx
s
e
cos wx dx.
H

0
Integrating by parts twice, it can be shown that
1
F w s
.
Ž
.
2
 1qw
Ž
.
EXAMPLE 11.6.2.
Consider the function
1
x Fa,
f x s
Ž . ½ 0
otherwise,
where a is a finite positive number. This function is absolutely integrable on
R, since

a
f x
dxs
dx
Ž .
H
H
y
ya
s2a.

THE FOURIER TRANSFORM
499
Its Fourier transform is given by
a
1
yi w x
F w s
e
dx
Ž
.
H
2
ya
1
iw a
yi w a
s
e
ye
Ž
.
2 iw
sin wa
s
.
 w
The next theorem gives the condition that makes it possible to express the
Ž .
function f x
in terms of its Fourier transform using the so-called in®erse
Fourier transform.
Ž .
Theorem 11.6.3.
Let f x
be piecewise continuous and absolutely inte-

Ž
y.

Ž
q.
grable on R. Then for every point xgR where f
x
and f
x
exist, we
have

1
y
q
iw x
f x
qf x
s
F w e
dw.
Ž
.
Ž
.
Ž
.
H
2
y
Ž .
In particular, if f x is continuous on R, then

iw x
f x s
F w e
dw.
11.65
Ž .
Ž
.
Ž
.
H
y
By applying Theorem 11.6.3 to the function in Example 11.6.1, we obtain

iw x
e
y x
e
s
dw
H
2
 1qw
Ž
.
y

1
s
cos wxqi sin wx
dw
Ž
.
H
2
 1qw
Ž
.
y

cos wx
s
dw
H
2
 1qw
Ž
.
y

2
cos wx
s
dw.
H
2

1qw
0
11.6.1. Fourier Transform of a Convolution
Ž .
Ž .
Let f x and g x be absolutely integrable functions on R. By definition, the
function

h x s
f xyy g y dy
11.66
Ž .
Ž
. Ž .
Ž
.
H
y
Ž .
Ž .
Ž
.Ž .
is called the convolution of f x and g x and is denoted by
f  g
x .

FOURIER SERIES
500
Ž .
Ž .
Theorem 11.6.4.
Let f x
and g x
be absolutely integrable on R. Let
Ž
.
Ž
.
F w
and G w
be their respective Fourier transforms. Then, the Fourier
Ž
.Ž .
Ž
.
Ž
.
transform of the convolution
f  g
x is given by 2 F w G w .
11.7. APPLICATIONS IN STATISTICS
Fourier series have been used in a wide variety of areas in statistics, such as
time series, stochastic processes, approximation of probability distribution
functions, and the modeling of a periodic response variable, to name just a
few. In addition, the methods and results of Fourier analysis have been
Ž
effectively utilized in the analytic theory of probability
see, for example,
.
Kawata, 1972 .
11.7.1. Applications in Time Series
A time series is a collection of observations made sequentially in time.
Examples of time series can be found in a variety of fields ranging from
economics to engineering. Many types of time series occur in the physical
sciences, particularly in meteorology, such as the study of rainfall on succes-
sive days, as well as in marine science and geophysics.
The stimulus for the use of Fourier methods in time series analysis is the
recognition that when observing data over time, some aspects of an observed
physical phenomenon tend to exhibit cycles or periodicities. Therefore, when
considering a model to represent such data, it is natural to use models that
contain sines and cosines, that is, trigonometric models, to describe the
behavior. Let y , y , . . . , y denote a time series consisting of n observations
1
2
n
obtained over time. These observations can be represented by the trigono-
metric polynomial model
m
a0
w
x
y s
q
a cos 
 tqb sin 
 t ,
ts1, 2, . . . , n,
Ý
t
k
k
k
k
2
ks1
where
2 k

 s
,
ks0, 1, 2, . . . , m,
k
n
n
2
a s
y cos 
 t,
ks0, 1, . . . , m,
Ý
k
t
k
n ts1
n
2
b s
y sin
 t,
ks1, 2, . . . , m.
Ý
k
t
k
n ts1

APPLICATIONS IN STATISTICS
501
The values 
 , 
 , . . . , 
are called harmonic frequencies. This model
1
2
m
provides a decomposition of the time series into a set of cycles based on the
harmonic frequencies. Here, n is assumed to be odd and equal to 2mq1, so
that the harmonic frequencies lie in the range 0 to . The expressions for ak
Ž
.
Ž
.
ks0, 1, . . . , m and b
ks1, 2, . . . , m were obtained by treating the model
k
as a linear regression model with 2mq1 parameters and then fitting it to the
2mq1 observations by the method of least squares. See, for example, Fuller
Ž
.
1976, Chapter 7 .
The quantity
n
2
2
I

s
a qb
,
ks1, 2, . . . , m,
11.67
Ž
.
Ž
.
Ž
.
n
k
k
k
2
represents the sum of squares associated with the frequency 
 . For ks
k
Ž
.
1, 2, . . . , m, the quantities in 11.67 define the so-called periodogram.
If y , y , . . . , y are independently distributed as normal variates with zero
1
2
n
means and variances 	 2, then the a ’s and b ’s, being linear combinations of
k
k
the y ’s, will be normally distributed. They are also independent, since the
t
w
Ž
2.xŽ
2
2.
sine and cosine functions are orthogonal. It follows that nr 2	
a qb
,
k
k
for ks1, 2, . . . , m, are distributed as independent chi-squared variates with
two degrees of freedom each. The periodogram can be used to search for
cycles or periodicities in the data.
Much of time series data analysis is based on the Fourier transform and its
efficient computation. For more details concerning Fourier analysis of time
Ž
.
series, the reader is referred to Bloomfield 1976 and Otnes and Enochson
Ž
.
1978 .
11.7.2. Representation of Probability Distributions
One of the interesting applications of Fourier series in statistics is in
providing a representation that can be used to evaluate the distribution
Ž
.
function of a random variable with a finite range. Woods and Posten 1977
introduced two such representations by combining the concepts of Fourier
Ž
.
series and Chebyshev polynomials of the first kind see Section 10.4.1 . These
representations are given by the following two theorems:
Theorem 11.7.1.
Let X be a random variable with a cumulative distribu-
Ž .
w
x
Ž .
tion function F x
defined on 0, 1 . Then, F x
can be represented as a
Fourier series of the form
0,
x0,
°

~1yryÝ
b sin n ,
0FxF1,
F x s
Ž .
ns1
n
¢1,
x1,

FOURIER SERIES
502
Ž
.
w
Ž
.x
w
Ž
.x
w
Ž
.x
where sArccos 2 xy1 , b s 2r n
E T
X , and E T
X
is the
n
n
n
expected value of the random variable

T
X scos n Arccos 2 Xy1
,
0FXF1.
11.68
Ž
.
Ž
.
Ž
.
n
Ž .
Note that T
x is basically a Chebyshev polynomial of the first kind and
n
w
x
of the nth degree defined on 0, 1 .
Ž
.
Proof. See Theorem 1 in Woods and Posten 1977 .

The second representation theorem is similar to Theorem 11.7.1, except
w
x
that X is now assumed to be a random variable over y1, 1 .
Theorem 11.7.2.
Let X be a random variable with a cumulative distribu-
Ž .
w
x
tion function F x defined on y1, 1 . Then
0,
xy1,
°

~1yryÝ
b sin n ,
y1FxF1,
F x s
Ž .
ns1
n
¢1,
x1,
w
Ž
.x
w
Ž
.x
w
Ž
.x
where sArccos x, b s 2r n
E T
X , E T
X
is the expected value
n
n
n
of the random variable
w
x
T
X scos n Arccos X ,
y1FXF1,
Ž
.
n
Ž .
w
and T
x is Chebyshev polynomial of the first kind and the nth degree see
n
Ž
.x
formula 10.12 .
Ž
.
Proof. See Theorem 2 in Woods and Posten 1977 .

Ž .
To evaluate the Fourier series representation of F x , we must first
compute
the
coefficients
b .
For
example,
in
Theorem
11.7.2,
b s
n
n
w
Ž
.x
w
Ž
.x
Ž .
2r n
E T
X . Since the Chebyshev polynomial T
x can be written in
n
n
the form
n
k
T
x s

x ,
ns1, 2, . . . ,
Ž .
Ý
n
nk
ks0
the computation of b is equivalent to evaluating
n
n
2

b s

 ,
ns1, 2, . . . ,
Ý
n
nk
k
n ks0

Ž
k.
where  sE X
is the kth noncentral moment of X. The coefficients 
k
nk
Ž
.
can be obtained by using the recurrence relation 10.16 , that is,
T
x s2 xT
x yT
x ,
ns1, 2, . . . ,
Ž .
Ž .
Ž .
nq1
n
ny1

APPLICATIONS IN STATISTICS
503
Ž .
Ž .
with T
x s1, T
x sx. This allows us to evaluate the 
’s recursively. The
0
1
nk
series


F x s1y
y
b sin n
Ž .
Ý
n

ns1
is then truncated at nsN. Thus
N

F x f1y
y
b sin k.
Ž .
Ý
k

ks1
Several values of N can be tried to determine the sensitivity of the approxi-
mation. We note that this series expansion provides an approximation of
Ž .
F x
in terms of the noncentral moments of X. Good estimates of these
moments should therefore be available.
It is also possible to extend the applications of Theorems 11.7.1 and 11.7.2
to a random variable X with an infinite range provided that there exists a
w
x
transformation which transforms X to a random variable Y over 0, 1 , or
w
x
over y1, 1 , such that the moments of Y are known from the moments of X.
Ž
.
In another application, Fettis 1976 developed a Fourier series expansion
Ž .
for Pearson Type IV distributions. These are density functions, f x , that
satisfy the differential equation
df x
y xqa
Ž .
Ž
.
s
f x ,
Ž .
2
dx
c qc xqc x
0
1
2
where a, c , c , and c
are constants determined from the central moments
0
1
2
 ,  ,  , and  , which can be estimated from the raw data. The data are
1
2
3
4
standardized so that  s0,  s1. This results in the following expressions
1
2
for a, c , c , and c :
0
1
2
2y1
c s
,
0
2 q1
Ž
.
c sa
1

y1
Ž
.
3
s
,
q1
1
c s
,
2
2 q1
Ž
.

FOURIER SERIES
504
where
3  y2 y1
Ž
.
4
3
s
.
2
2 y3 y6
4
3
Ž
.
Fettis 1976 provided additional details that explain how to approximate the
cumulative distribution function,
x
F x s
f t
dt,
Ž .
Ž .
H
y
using Fourier series.
11.7.3. Regression Modeling
In regression analysis and response surface methodology, it is quite common
to use polynomial models to approximate the mean  of a response variable.
There are, however, situations in which polynomial models are not adequate
representatives of the mean response, as when  is known to be a periodic
function. In this case, it is more appropriate to use an approximating function
which is itself periodic.
Ž
.
Kupper 1972 proposed using partial sums of Fourier series as possible
models to approximate the mean response. Consider the following trigono-
metric polynomial of order d,
d
w
x
s q
 cos nq sin n ,
11.69
Ž
.
Ý
0
n
n
us1
where 0FF2 represents either a variable taking values on the real line
between 0 and 2, or the angle associated with the polar coordinates of a
Ž
.
point on the unit circle. Let us u , u
, where u scos , u ssin . Then,
1
2
1
2
Ž
.
when ds2, the model in 11.69 can be written as
s q u q u q u2y u2q2 u u ,
11.70
Ž
.
0
1
1
1
2
2
1
2
2
2
1
2
since sin 2s2 sin  cos s2u u , and cos 2scos2 ysin2 su2yu2.
1
2
1
2
One of the objectives of response surface methodology is the determina-
tion of optimum settings of the model’s control variables that result in a
Ž
.
maximum or minimum predicted response. The predicted response y at a
ˆ
Ž
.
point provides an estimate of  in
11.69
and is obtained by replacing
ˆ
Ž
.
 ,  , 
in 11.69 by their least-squares estimates  ,  , and  , respec-
ˆ
ˆ
0
n
n
0
n
n
tively, ns1, 2, . . . , d. For example, if ds2, we have
2
ˆ
ys q
 cos nq sin n ,
11.71
Ž
.
ˆ
ˆ
ˆ
Ý
0
n
n
ns1

APPLICATIONS IN STATISTICS
505
Ž
.
which can be expressed using 11.70 as

ˆ

ˆ
ys qu bqu Bu,
ˆ
ˆ0
ˆ
ˆ

Ž
.
where bs  , 
and
ˆ1
1
ˆ


ˆ2
2
ˆBs ˆ
yˆ
2
2
with u
us1. The method of Lagrange multipliers can then be used to
determine the stationary points of y subject to the constraint u
us1. Details
ˆ
Ž
.
of this procedure are given in Kupper 1972, Section 3 . In a follow-up paper,
Ž
.
Kupper 1973 presented some results on the construction of optimal designs
Ž
.
for model 11.69 .
Ž
.
Ž
.
More recently, Anderson-Cook 2000 used model 11.71 in experimental
situations involving cylindrical data. For such data, it is of interest to model
the relationship between two correlated components, one a standard linear
measurement y, and the other an angular measure . Examples of such data
Ž
.
arise, for example, in biology
plant or animal migration patterns , and
Ž
.
geology direction and magnitude of magnetic fields . The fitting of model
Ž
.
11.71
is done by using the method of ordinary least squares with the
assumption that
y is normally distributed and has a constant variance.
Anderson-Cook used an example, originally presented in Mardia and Sutton
Ž
.
Ž
1978 , of a cylindrical data set in which y is temperature
measured in
.
Ž
.
degrees Fahrenheit and  is wind direction measured in radians . Based on
this example, the fitted model is
ys41.33y2.43 cos y2.60 sin q3.05 cos 2q2.98 sin 2.
ˆ
ˆ
ˆ
The corresponding standard errors of  ,  ,  ,  , 
are 1.1896, 1.6608,
ˆ
ˆ
ˆ
0
1
1
2
2
1.7057, 1.4029, 1.7172, respectively. Both 
and 
are significant parame-
0
2
ters at the 5% level, and 
is significant at the 10% level.
2
11.7.4. The Characteristic Function
Ž .
We have seen that the moment generating function  t
for a random
Ž
variable X is used to obtain the moments of X
see Section 5.6.2 and
.
Ž .
Example 6.9.8 . It may be recalled, however, that  t may not be defined for
Ž .
all values of t. To generate all the moments of X, it is sufficient for  t to
Ž
.
be defined in a neighborhood of ts0 see Section 5.6.2 . Some well-known
distributions do not have moment generating functions, such as the Cauchy
Ž
.
distribution see Example 6.9.1 .
Another function that generates the moments of a random variable in a
Ž .
manner similar to  t , but is defined for all values of t and for all random

FOURIER SERIES
506
variables, is the characteristic function. By definition, the characteristic func-
Ž .
tion of a random variable X, denoted by  t , is
c
w
it X x

t sE e
Ž .
c

it x
s
e
dF x ,
11.72
Ž .
Ž
.
H
y
Ž .
where F x is the cumulative distribution function of X, and i is the complex
'
number
y1 . If X is discrete and has the values c , c , . . . , c , . . . , then
1
2
n
Ž
.
11.72 takes the form

itcj

t s
p c
e
,
11.73
Ž .
Ž
.
Ž
.
Ý
c
j
js1
Ž
.
w
x
where p c sP Xsc , js1, 2, . . . . If X is continuous with the density
j
j
Ž .
function f x , then

it x

t s
e
f x
dx.
11.74
Ž .
Ž .
Ž
.
H
c
y
Ž .
The function  t is complex-valued in general, but is defined for all values
c
it x

Ž .

Ž .
of t, since e
scos txqi sin tx, and both H
cos tx dF x and H
sin tx dF x
y
y
exist by the fact that


cos tx dF x F
dF x s1,
Ž .
Ž .
H
H
y
y


sin tx dF x F
dF x s1.
Ž .
Ž .
H
H
y
y
The characteristic function and the moment generating function, when the
latter exists, are related according to the formula

t s it .
Ž .
Ž .
c
Furthermore, it can be shown that if X has finite moments, then they can be
Ž .
obtained by repeatedly differentiating  t and evaluating the derivatives at
c
zero, that is,
n
1 d 
tŽ .
c
n
E X
s
,
ns1, 2, . . . .
Ž
.
n
n
i
dt
ts0
Ž .
Although  t
generates moments, it is mainly used as a tool to derive
c
Ž
.
distributions. For example, from 11.74 we note that when X is continuous,
the characteristic function is a Fourier-type transformation of the density

APPLICATIONS IN STATISTICS
507
Ž .
Ž
.
Ž .
function f x . This follows from 11.64 , the Fourier transform of f x , which
Ž
. 
Ž . yi t x
Ž .
is given by
1r2 H
f x e
dx. If we denote this transform by G t
,
y
Ž .
Ž .
then the relationship between  t and G t is given by
c

t s2 G yt .
Ž .
Ž
.
c
Ž .
By Theorem 11.6.3, if f x
is continuous and absolutely integrable on R,
Ž .
Ž .
Ž
.
then f x can be derived from  t by using formula 11.65 , which can be
c
written as

it x
f x s
G t e
dt
Ž .
Ž .
H
y

1
it x
s

yt e
dt
Ž
.
H
c
2
y

1
yi t x
s

t e
dt.
11.75
Ž .
Ž
.
H
c
2
y
This is known as the in®ersion formula for characteristic functions. Thus
the distribution of
X can be uniquely determined by its characteristic
function. There is therefore a one-to-one correspondence between distribu-
tion functions and their corresponding characteristic functions. This provides
a useful tool for deriving distributions of random variables that cannot be
easily calculated, but whose characteristic functions are straightforward.
Ž
.
Waller, Turnbull, and Hardin
1995
reviewed and discussed several algo-
rithms for inverting characteristic functions, and gave several examples from
Ž
.
various areas in statistics. Waller
1995
demonstrated that characteristic
functions provide information beyond what is given by moment generating
functions. He pointed out that moment generating functions may be of more
mathematical than numerical use in characterizing distributions. He used an
example to illustrate that numerical techniques using characteristic functions
can differentiate between two distributions, even though their moment gen-
Ž
.
erating functions are very similar see also McCullagh, 1994 .
Ž
.
Luceno 1997 provided further and more general arguments to show that
˜
characteristic functions are superior to moment generating and probability
Ž
.
generating functions see Section 5.6.2 in their numerical behavior.
One of the principal uses of characteristic functions is in deriving limiting
Ž
distributions. This is based on the following theorem
see, for example,
.
Pfeiffer, 1990, page 426 :

Ž .4
Theorem 11.7.3.
Consider the sequence
F
x
of cumulative distri-
n
ns1

Ž .4
bution functions. Let 
t
be the corresponding sequence of character-
nc
ns1
istic functions.
Ž .
Ž .
a. If F
x
converges to a distribution function F x
at every point of
n
Ž .
Ž .
Ž .
Ž .
continuity for F x , then 
t converges to  t for all t, where  t
nc
c
c
Ž .
is the characteristic function for F x .

FOURIER SERIES
508
Ž .
Ž .
Ž .
b. If 
t
converges to  t
for all t and  t
is continuous at ts0,
nc
c
c
Ž .
Ž .
then  t is the characteristic function for a distribution function F x
c
Ž .
Ž .
Ž .
such that F
x converges to F x at each point of continuity of F x .
n
It should be noted that in Theorem 11.7.3 the condition that the limiting
Ž .
function  t
is continuous at ts0 is essential for the validity of the
c
theorem. The following example shows that if this condition is violated, then
the theorem is no longer true:
Consider the cumulative distribution function
0,
xFyn,
°
xqn
~
,
ynxn,
F
x s
Ž .
n
2n
¢1,
xGn.
The corresponding characteristic function is
n
1
it x

t s
e
dx
Ž .
H
nc
2n
yn
sin nt
s
.
nt
Ž .
Ž .
As n™, 
t converges for every t to  t defined by
nc
c
1,
ts0,

t s
Ž .
c
½ 0,
t0.
1
Ž .
Ž .
Thus,  t is not continuous for ts0. We note, however, that F
x ™
for
c
n
2
Ž .
every fixed x. Hence, the limit of F
x
is not a cumulative distribution
n
function.
EXAMPLE 11.7.1.
Consider the distribution defined by the density func-
Ž .
yx
tion f x se
for x0. Its characteristic function is given by
 it x yx

t s
e
e
dx
Ž . H
c
0
 yx Ž1yi t.
s
e
dx
H
0
1
s
.
1yit
EXAMPLE 11.7.2.
Consider the Cauchy density function
1
f x s
,
yx,
Ž .
2
 1qx
Ž
.

APPLICATIONS IN STATISTICS
509
given in Example 6.9.1. The characteristic function is

it x
1
e

t s
dx
Ž .
H
c
2

1qx
y


1
cos tx
i
sin tx
s
dxq
dx
H
H
2
2


1qx
1qx
y
y

1
cos tx
s
dx
H
2

1qx
y
sey  t  .
Note that this function is not differentiable at ts0. We may recall from
Example 6.9.1 that all moments of the Cauchy distribution do not exist.
EXAMPLE 11.7.3.
In Example 6.9.8 we saw that the moment generating
Ž
.
function for the gamma distribution G , 
with the density function
x y1eyx r 
f x s
,
0,
0,
0x,
Ž .

!  
Ž
.
Ž .
Ž
.y
Ž .
Ž
.y
is  t s 1yt
. Hence, its characteristic function is  t s 1yit
.
c
EXAMPLE 11.7.4.
The characteristic function of the standard normal dis-
tribution with the density function
1
2
yx r2
f x s
e
,
yx,
Ž .
'2
is

1
2
yx r2
it x

t s
e
e
dx
Ž .
H
c
'2
y

1
1
2
Ž
.
y
x y2 it x
2
s
e
dx
H
'2
y
yt 2 r2

e
1
2
Ž
.
y
xyi t
2
s
e
dx
H
'2
y
seyt 2 r2.

FOURIER SERIES
510
Ž .
Vice versa, the density function can be retrieved from  t by using the
c
Ž
.
inversion formula 11.75 :

1
2
yt r2
yi t x
f x s
e
e
dt
Ž .
H
2
y

1
1
2
Ž
.
y
t q2 it x
2
s
e
dt
H
2
y

1
1
2
2
2
w
Ž
.
Ž
. x
y
t q2 t i x q i x
Ži x. r2
2
s
e
e
dt
H
2
y
yx 2 r2

e
1
1
2
Ž
.
y
tqi x
2
s
e
dt
H
'
'
2
2
y
yx 2 r2

e
1
2
yu r2
s
e
du
H
'
'
2
2
y
eyx 2 r2
s
.
'2
11.7.4.1. Some Properties of Characteristic Functions
Ž
.
The book by Lukacs
1970
provides a detailed study of characteristic
functions and their properties. Proofs of the following theorems can be found
in Chapter 2 of that book.
Theorem 11.7.4.
Every characteristic function is uniformly continuous on
the whole real line.
Ž .
Ž .
Ž .
Theorem 11.7.5.
Suppose that 
t , 
t , . . . , 
t are characteristic
1c
2 c
nc
functions. Let a , a , . . . , a
be nonnegative numbers such that Ýn
a s1.
1
2
n
is1
i
n
Ž .
Then Ý
a 
t is also a characteristic function.
is1
i
ic
Theorem 11.7.6.
The characteristic function of the convolution of two
distribution functions is the product of their characteristic functions.
Theorem 11.7.7.
The product of two characteristic functions is a charac-
teristic function.
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Anderson-Cook, C. M. 2000 . ‘‘A second order model for cylindrical data.’’ J. Statist.
Comput. Simul., 66, 5165.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
511
Ž
.
Bloomfield, P. 1976 . Fourier Analysis of Time Series: An Introduction. Wiley, New
Ž
York. This is an introductory text on Fourier methods written at an applied level
.
for users of time series.
Ž
.
Carslaw, H. S. 1930 . Introduction to the Theory of Fourier Series and Integrals, 3rd ed.
Dover, New York.
Ž
.
Churchill, R. V.
1963 .
Fourier Series and Boundary Value Problems, 2nd ed.
Ž
McGraw-Hill, New York. This text provides an introductory treatment of Fourier
series and their applications to boundary value problems in partial differential
equations of engineering and physics. Fourier integral representations and ex-
.
pansions in series of Bessel functions and Legendre polynomials are also treated.
Ž
.
Davis, H. F. 1963 . Fourier Series and Orthogonal Functions. Allyn & Bacon, Boston.
Ž
.
Fettis, H. E. 1976 . ‘‘Fourier series expansions for Pearson Type IV distributions and
probabilities.’’ SIAM J. Applied Math., 31, 511518.
Ž
.
Fuller, W. A. 1976 . Introduction to Statistical Time Series. Wiley, New York.
Ž
.
Gillespie, R. P. 1959 . Integration. Oliver and Boyd, London.
Ž
.
Jackson, D. 1941 . Fourier Series and Orthogonal Polynomials. Mathematical Associa-
tion of America, Washington.
Ž
.
Kawata, T. 1972 . Fourier Analysis in Probability Theory. Academic Press, New York.
ŽThis text presents useful results from the theories of Fourier series, Fourier
transforms, Laplace transforms, and other related topics that are pertinent to the
.
study of probability theory.
Ž
.
Kufner, A., and J. Kadlec
1971 . Fourier Series. Iliffe BooksThe Butterworth
Ž
.
Group, London. This is an English translation edited by G. A. Toombs.
Ž
.
Kupper, L. L.
1972 . ‘‘Fourier series and spherical harmonics regression.’’ Appl.
Statist., 21, 121130.
Ž
.
Kupper, L. L. 1973 . ‘‘Minimax designs for Fourier series and spherical harmonics
regressions: A characterization of rotatable arrangements.’’ J. Roy. Statist. Soc.,
Ser. B, 35, 493500.
Ž
.
Luceno, A. 1997 . ‘‘Further evidence supporting the numerical usefulness of charac-
˜
teristic functions.’’ Amer. Statist., 51, 233234.
Ž
.
Ž
Lukacs, E.
1970 . Characteristic Functions, 2nd ed. Hafner, New York.
This is a
.
classic book covering many interesting details concerning characteristic functions.
Ž
.
Mardia, K. V., and T. W. Sutton
1978 . ‘‘Model for cylindrical variables with
applications.’’ J. Roy. Statist. Soc., Ser. B, 40, 229233.
Ž
.
McCullagh, P. 1994 . ‘‘Does the moment-generating function characterize a distribu-
tion?’’ Amer. Statist., 48, 208.
Ž
.
Otnes, R. K., and L. Enochson 1978 . Applied Time Series Analysis. Wiley, New York.
Ž
.
Pfeiffer, P. E. 1990 . Probability for Applications. Springer-Verlag, New York.
Ž
.
Pinkus, A., and S. Zafrany 1997 . Fourier Series and Integral Transforms. Cambridge
University Press, Cambridge, England.
Ž
.
Ž
Tolstov, G. P. 1962 . Fourier Series. Dover, New York. Translated from the Russian
.
by Richard A. Silverman.
Ž
.
Waller, L. A. 1995 . ‘‘Does the characteristic function numerically distinguish distri-
butions?’’ Amer. Statist., 49, 150152.

FOURIER SERIES
512
Ž
.
Waller, L. A., B. W. Turnbull, and J. M. Hardin
1995 . ‘‘Obtaining distribution
functions by numerical inversion of characteristic functions with applications.’’
Amer. Statist., 49, 346350.
Ž
.
Wea®er, H. J. 1989 . Theory of Discrete and Continuous Fourier Analysis. Wiley, New
York.
Ž
.
Woods, J. D., and H. O. Posten 1977 . ‘‘The use of Fourier series in the evaluation of
probability
distribution
functions.’’
Commun.
Statist.Simul.
Comput.,
6,
201219.
EXERCISES
In Mathematics
11.1. Expand the following functions using Fourier series:
( )
Ž .
 
a
f x s x ,
yFxF.
( )
Ž .


b
f x s sin x .
( )
Ž .
2
c
f x sxqx ,
yFxF.
11.2. Show that

2
1

s
.
Ý
2
8
2ny1
Ž
.
ns1
w
2 x
Hint: Use the Fourier series for x .
11.3. Let a
and b
be the Fourier coefficients for a continuous function
n
n
Ž .
w
x
Ž
.
Ž
.

Ž .
f x defined on y, 
such that f y sf  , and f
x is piece-
w
x
wise continuous on y,  . Show that
( )
Ž
.
a
lim
na
s0,
n™
n
( )
Ž
.
b
lim
nb
s0.
n™
n
Ž .
w
x
Ž
.
Ž
.

Ž .
11.4. If f x is continuous on y,  , f y sf  , and f
x is piece-
w
x
wise continuous on y,  , then show that
c
f x ys
x
F
,
Ž .
Ž .
n
'n
where
n
a0
w
x
s
x s
q
a cos kxqb sin kx ,
Ž .
Ý
n
k
k
2
ks1
and

1

2
2
c s
f
x
dx.
Ž .
H

y

EXERCISES
513
Ž .
w
x
11.5. Suppose that f x
is piecewise continuous on y, 
and has the
Ž
.
Fourier series given in 11.32 .
( )

Ž
.n
a
Show that Ý
y1
b rn is a convergent series.
ns1
n
( )

b
Show that Ý
b rn is convergent.
ns1
n
w
x
Hint: Use Theorem 11.3.2.

Ž
.
11.6. Show that the trigonometric series, Ý
sin nx rlog n, is not a Fourier
ns2
series of any integrable function.
w
Ž .
Hint: If it were a Fourier series of a function f x , then b s1rlog n
n
would be the Fourier coefficient of an odd function. Apply now part
Ž .
b of Exercise 11.5 and show that this assumption leads to a contra-
x
diction.
Ž .
11.7. Consider the Fourier series of f x sx given in Example 11.2.1.
( )
a
Show that
nq1
2
2

x

y1
cos nx
Ž
.
s
y
for yx .
Ý
2
4
12
n
ns1
w
x
Ž .
x
Hint: Consider the Fourier series of H
f t dt.
0
( )
b
Deduce that
nq1
2

y1

Ž
.
s
.
Ý
2
12
n
ns1
11.8. Make use of the result in Exercise 11.7 to find the sum of the series

wŽ
.nq1
x
3
Ý
y1
sin nx rn .
ns1
Ž .
yx 2
11.9. Show that the Fourier transform of f x se
, yx, is given
by
1
2
yw
r4
F w s
e
.
Ž
.
'
2 

1
w
Ž
.
Ž
.
x
Hint: Show that F w q wF w s0.
2
11.10. Prove Theorem 11.6.4 using Fubini’s theorem.
11.11. Use the Fourier transform to solve the integral equation

2
yx r2
f xyy f y dyse
Ž
. Ž .
H
y
Ž .
for the function f x .

FOURIER SERIES
514
Ž .
Ž
.
Ž
.
11.12. Consider the function f x sx, yx, with f y sf  s0,
Ž .
Ž
.
Ž .
and f x 2-periodic defined on y,  . The Fourier series of f x
is
nq1

2 y1
sin nx
Ž
.
.
Ý
n
ns1
Ž .
Let s
x be the nth partial sum of this series, that is,
n
kq1
n
2 y1
Ž
.
s
x s
sin kx.
Ž .
Ý
n
k
ks1
Let x syrn.
n
( )
a
Show that
n
2 sin krn
Ž
.
s
x
s
.
Ž
.
Ý
n
n
k
ks1
( )
b
Show that
 sin x
lim s
x
s2
dx
Ž
.
H
n
n
x
n™
0
f1.18 .
Note: As n™, x ™y. Hence, for n sufficiently large,
n
s
x
yf x
f1.18ys0.18 .
Ž
.
Ž
.
n
n
n
w
Ž .x
Thus, near xs
a point of discontinuity for f x , the partial
sums of the Fourier series exceed the value of this function by
approximately the amount 0.18s0.565. This illustrates the so-
called Gibbs phenomenon according to which the Fourier series of
Ž .
Ž .
f x
‘‘overshoots’’ the value of f x
in a small neighborhood to
Ž .
the left of the point of discontinuity of f x . It can also be shown
that in a small neighborhood to the right of xsy, the Fourier
Ž .
Ž .
series of f x ‘‘undershoots’’ the value of f x .
In Statistics
11.13. In the following table, two observations of the resistance in ohms are
recorded at each of six equally spaced locations on the perimeter of a

EXERCISES
515
Ž
.
new type of solid circular coil see Kupper, 1972, Table 1 :
Ž
.
Ž
.
 radians
Resistance ohms
0
13.62, 14.40
r3
10.552, 10.602
2r3
2.196, 3.696

6.39, 7.25
4r3
8.854, 10.684
5r3
5.408, 8.488
( )
a
Use the method of least squares to estimate the parameters in the
following trigonometric polynomial of order 2:
2
w
x
s q
 cos nq sin n ,
Ý
0
n
n
ns1
where 0FF2, and  denotes the average resistance at loca-
tion .
( )
Ž .
b
Use the prediction equation obtained in part a to determine the
points of minimum and maximum resistance on the perimeter of
the circular coil.
11.14. Consider the following circular data set in which  is wind direction
Ž
.
and y is temperature see AndersonCook, 2000, Table 1 .
Ž
.
Ž
.
Ž
.
Ž
.
 radians
y &F
 radians
y &F
4.36
52
4.54
38
3.67
41
2.62
40
4.36
41
2.97
49
1.57
31
4.01
48
3.67
53
4.19
37
3.67
47
5.59
37
6.11
43
5.59
33
5.93
43
3.32
47
0.52
41
3.67
51
3.67
46
1.22
42
3.67
48
4.54
53
3.32
52
4.19
46
4.89
43
3.49
51
3.14
46
4.71
39
Fit a secondyorder trigonometric polynomial to this data set, and
verify that the prediction equation is given by
ys41.33y2.43 cos y2.60 sin q3.05 cos 2q2.98 sin 2.
ˆ

FOURIER SERIES
516

4
11.15. Let
Y
be a sequence of independent, indentically distributed
n ns1
random variables with mean  and variance 	 2. Let
Y y
n

s s
,
n
'
	r n
n
Ž
.
where Y s 1rn Ý
Y .
n
is1
i
( )

a
Find the characteristic function of s .
n
( )
Ž .
b
Use Theorem 11.7.3 and part a to show that the limiting distri-
bution of s as n™ is the standard normal distribution.
n
Ž .
Note: Part b represents the statement of the well-known central
limit theorem, which asserts that for large n, the arithmetic mean
Y
of a sample of independent, identically distributed random
n
variables is approximately normally distributed with mean  and
'
standard deviation 	r n .

C H A P T E R
1 2
Approximation of Integrals
Integration plays an important role in many fields of science and engineering.
For applications, numerical values of integrals are often required. However,
in many cases, the evaluation of integrals, or quadrature, by elementary
functions may not be feasible. Hence, approximating the value of an integral
in a reliable fashion is a problem of utmost importance. Numerical quadra-
ture is in fact one of the oldest branches of mathematics: the determination,
approximately or exactly, of the areas of regions bounded by lines or curves, a
Ž
.
subject which was studied by the ancient Babylonians see Haber, 1970 . The
word ‘‘quadrature’’ indicates the process of measuring an area inside a curve
by finding a square having the same area. Probably no other problem has
exercised a greater or a longer attraction than that of constructing a square
equal in area to a given circle. Thousands of people have worked on this
problem, including the ancient Egyptians as far back as 1800 B.C.
In this chapter, we provide an exposition of methods for approximating
integrals, including those that are multidimensional.
12.1. THE TRAPEZOIDAL METHOD
This is the simplest method of approximating an integral of the form
b Ž .
H f x dx, which represents the area bounded by the curve of the function
a
Ž .
ysf x and the two lines xsa, xsb. The method is based on approximat-
ing the curve by a series of straight line segments. As a result, the area is
approximated with a series of trapezoids. For this purpose, the interval from
a
to
b
is divided
into
n
equal parts
by the
partition
points
as
x , x , x , . . . , x sb. For the ith trapezoid, which lies between x
and x ,
0
1
2
n
iy1
i
Ž
.Ž
.
its width is hs 1rn bya and its area is given by
h
A s
f x
qf x
,
is1, 2, . . . , n.
12.1
Ž
.
Ž
.
Ž
.
i
iy1
i
2
517

APPROXIMATION OF INTEGRALS
518
The sum, S , of A , A , . . . , A
provides an approximation to the integral
n
1
2
n
b Ž .
H f x dx.
a
n
S s
A
Ý
n
i
is1
h
s
f x
qf x
q f x
qf x
q  q f x
qf x

4
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
0
1
1
2
ny1
n
2
ny1
h
s
f x
qf x
q2
f x
.
12.2
Ž
.
Ž
.
Ž
.
Ž
.
Ý
0
n
i
2
is1
12.1.1. Accuracy of the Approximation
The accuracy in the trapezoidal method depends on the number n of
trapezoids we take. The next theorem provides information concerning the
error or approximation.
Ž .
Theorem 12.1.1.
Suppose that f x
has a continuous second derivative

w
x
w
x
on a, b , and
f
x
FM
for all x in a, b . Then
Ž .
2
3
bya
M
Ž
.
b
2
f x dxyS
F
,
Ž .
H
n
2
12n
a
Ž
.
where S
is given by formula 12.2 .
n
Proof. Consider the partition points asx , x , x , . . . , x sb such that
0
1
2
n
Ž
.Ž
.
Ž .
hsx yx
s 1rn bya , is1, 2, . . . , n. The integral of f x from x
to
i
iy1
iy1
x is
i
xi
I s
f x
dx.
12.3
Ž .
Ž
.
H
i
xiy1
Ž .
w
x
Now, in the trapezoidal method, f x is approximated in the interval x
, x
iy1
i
by the right-hand side of the straight-line equation,
1
p
x sf x
q
f x
yf x
xyx
Ž .
Ž
.
Ž
.
Ž
. Ž
.
i
iy1
i
iy1
iy1
h
x yx
xyx
i
iy1
s
f x
q
f x
Ž
.
Ž
.
iy1
i
h
h
xyx
xyx
i
iy1
s
f x
q
f x
,
is1, 2, . . . , n.
Ž
.
Ž
.
iy1
i
x
yx
x yx
iy1
i
i
iy1
Ž .
Ž
Note that p x
is a linear Lagrange interpolating polynomial
of degree
i
.
w
Ž
.x
ns1 with x
and x as its points of interpolation see formula 9.14 .
iy1
i

THE TRAPEZOIDAL METHOD
519
Using Theorem 9.2.2, the error of interpolation resulting from approximating
Ž .
Ž .
w
x
f x with p x over
x
, x
is given by
i
iy1
i
1

f x yp
x s
f

xyx
xyx
,
is1, 2, . . . , n,
12.4
Ž .
Ž .
Ž
. Ž
. Ž
.
Ž
.
i
i
iy1
i
2!
Ž
.
Ž
.
where x
 x . Formula
12.4
results from applying formula
9.15 .
iy1
i
i
Ž
.
Hence, the error of approximating I with A in 12.1 is
i
i
xi
I yA s
f x yp
x
dx
Ž .
Ž .
H
i
i
i
xiy1
x
1
i

s
f

xyx
xyx
dx
Ž
.
Ž
. Ž
.
H
i
iy1
i
2!
xiy1
1

1
1
3
3
2
2
s
f

x yx
y
x
qx
x yx
qx
x
x yx
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
i
i
iy1
iy1
i
i
iy1
iy1
i
i
iy1
3
2
2!
h
2

1
1
2
2
s
f

x qx
x qx
y
x
qx
qx
x
Ž
.
Ž
.
Ž
.
i
i
iy1
i
iy1
iy1
i
iy1
i
3
2
2!
h

1
2
2
s
f

2 x
x yx
yx
Ž
.
Ž
.
i
iy1
i
iy1
i
6
2!
h3

sy
f

,
is1, 2, . . . , n.
12.5
Ž
.
Ž
.
i
12
b Ž .
The total error of approximating H f x dx with S
is then given by
a
n
3
n
h
b

f x
dxyS sy
f

.
Ž .
Ž
.
Ý
H
n
i
12
a
is1
It follows that
3
nh M
b
2
f x
dxyS
F
Ž .
H
n
12
a
3
bya
M
Ž
.
2
s
.

12.6
Ž
.
2
12n
b Ž .
An alternative procedure to approximating the integral H f x dx by a sum
a
x i
Ž .
of trapezoids is to approximate H
f x dx by a trapezoid bounded from
x iy1
Ž .
above by the tangent to the curve of ysf x at the point x
qhr2, which
iy1
w
x
is the midpoint of the interval
x
, x . In this case, the area of the ith
iy1
i
trapezoid is
h

A shf
x
q
,
is1, 2, . . . , n.
i
iy1
ž
/
2

APPROXIMATION OF INTEGRALS
520
Hence,
n
h
bf x
dxfh
f
x
q
,
12.7
Ž .
Ž
.
Ý
H
iy1
ž
/
2
a
is1
Ž .
w
x
and f x is approximated in the interval
x
, x
by
iy1
i
h
h
h


p
x sf
x
q
q
xyx
y
f
x
q
.
Ž .
i
iy1
iy1
iy1
ž
/ ž
/ ž
/
2
2
2
Ž
.
Ž .
By applying Taylor’s theorem Theorem 4.3.1 to f x in a neighborhood of
x
qhr2, we obtain
iy1
h
h
h

f x sf
x
q
q
xyx
y
f
x
q
Ž .
iy1
iy1
iy1
ž
/ ž
/ ž
/
2
2
2
2
1
h

q
xyx
y
f

Ž
.
iy1
i
ž
/
2!
2
2
1
h


sp
x q
xyx
y
f
 ,
Ž .
Ž
.
i
iy1
i
ž
/
2!
2
where 
lies between
x
qhr2 and
x. The error of approximating
i
iy1
x i
Ž .

H
f x dx with A
is then given by
x
i
iy1
2
x
x
1
h
i
i


f x yp
x
dxs
xyx
y
f

dx
Ž .
Ž .
Ž
.
H
H
i
iy1
i
ž
/
2!
2
x
x
iy1
iy1
2
x
M
h
i
2
F
xyx
y
dx
H
iy1
ž
/
2!
2
xiy1
h3M2
s
.
24
Consequently, the absolute value of the total error in this case has an upper
bound of the form
3
n
x
nh M
i
2

f x yp
x
dx F
Ž .
Ž .
Ý H
i
24
xiy1
is1
3
bya
M
Ž
.
2
s
.
12.8
Ž
.
2
24n

SIMPSON’S METHOD
521
Ž
.
Ž
.
We note that the upper bound in 12.8 is half as large as the one in 12.6 .
This alternative procedure is therefore slightly more precise than the original
Ž
2.
one. Both procedures produce an approximating error that is O 1rn . It
should be noted that this error does not include the roundoff errors in the
computation of the areas of the approximating trapezoids.
EXAMPLE 12.1.1.
Consider approximating the integral H 2dxrx, which has
1
w
x
an exact value equal to log2f0.693147. Let us divide the interval 1, 2 into
1
ns10 subintervals of length hs
. Hence, x s1, x s1.1, . . . , x
s2.0.
0
1
10
10
Ž
.
Using the first trapezoidal method formula 12.2 , we obtain
9
dx
1
1
1
2
f
1q
q2 Ý
H
x
20
2
x
1
i
is1
s0.69377.
w
Ž
.x
Using now the second trapezoidal method formula 12.7 , we get
10
dx
1
1
2
f
Ý
H
x
10
x
q0.05
1
iy1
is1
s0.69284.
12.2. SIMPSON’S METHOD
b Ž .
Let us again consider the integral H f x dx. Let asx x   x

a
0
1
2 ny1
x
sb be a sequence of equally spaced points that partition the interval
2 n
w
x
a, b such that x yx
sh, is1, 2, . . . , 2n. Simpson’s method is based on
i
iy1
Ž .
w
x
approximating the graph of the function f x over the interval
x
, x
by
iy1
iq1
Ž .
a parabola which agrees with f x at the points x
, x , and x
. Thus, over
iy1
i
iq1
w
x
Ž .
x
, x
, f x
is approximated with a Lagrange interpolating polynomial
iy1
iq1
w
Ž
.x
of degree 2 of the form see formula 9.14
xyx
xyx
Ž
. Ž
.
i
iq1
q
x sf x
Ž .
Ž
.
i
iy1
x
yx
x
yx
Ž
. Ž
.
iy1
i
iy1
iq1
xyx
xyx
xyx
xyx
Ž
. Ž
.
Ž
. Ž
.
iy1
iq1
iy1
i
qf x
qf x
Ž
.
Ž
.
i
iq1
x yx
x yx
x
yx
x
yx
Ž
. Ž
.
Ž
. Ž
.
i
iy1
i
iq1
iq1
iy1
iq1
i
xyx
xyx
xyx
xyx
Ž
. Ž
.
Ž
. Ž
.
i
iq1
iy1
iq1
sf x
yf x
Ž
.
Ž
.
iy1
i
2
2
2h
h
xyx
xyx
Ž
. Ž
.
iy1
i
qf x
.
Ž
.
iq1
2
2h

APPROXIMATION OF INTEGRALS
522
It follows that
x
x
iq1
iq1
f x
dxf
q
x
dx
Ž .
Ž .
H
H
i
x
x
iy1
iy1
x
f x
Ž
.
iq1
iy1
s
xyx
xyx
dx
Ž
. Ž
.
H
i
iq1
2
2h
xiy1
x
f x
Ž
.
iq1
i
y
xyx
xyx
dx
Ž
. Ž
.
H
iy1
iq1
2
h
xiy1
x
f x
Ž
.
iq1
iq1
q
xyx
xyx
dx
Ž
. Ž
.
H
iy1
i
2
2h
xiy1
f x
2h3
f x
y4h3
f x
2h3
Ž
.
Ž
.
Ž
.
iy1
i
iq1
s
y
q
2
2
2
ž
/
ž
/
ž
/
3
3
3
2h
h
2h
h
s
f x
q4 f x
qf x
,
is1,3, . . . , 2ny1.
Ž
.
Ž
.
Ž
.
iy1
i
iq1
3
12.9
Ž
.
Ž
.
By adding up all the approximations in
12.9
for is1, 3, . . . , 2ny1, we
obtain
n
ny1
h
bf x
dxf
f x
q4
f x
q2
f x
qf x
.
12.10
Ž .
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ý
Ý
H
0
2 iy1
2 i
2 n
3
a
is1
is1
As before, in the case of the trapezoidal method, the accuracy of the
Ž
.
Ž
.
approximation in 12.10 can be figured out by using formula 9.15 . Courant
Ž
.
and John 1965, page 487 , however, stated that the error of approximation
can be improved by one order of magnitude by using a cubic interpolating
Ž .
polynomial which agrees with f x at x
, x , x
, and whose derivative at
iy1
i
iq1

Ž
.
Ž .
x is equal to f
x . Such a polynomial gives a better approximation to f x
i
i
w
x
over
x
, x
than the quadratic one, and still provides the same approxi-
iy1
iq1
Ž
.
Ž .
mation formula 12.l0 for the integral. If q x
is chosen as such, then the
i
Ž .
Ž .
error of interpolation resulting from approximating f x
with q x
over
i
w
x
Ž .
Ž .
Ž
.
Ž4.Ž
.Ž
.Ž
.2Ž
x
, x
is given by f x yq x s 1r4! f

xyx
xyx
xy
iy1
iq1
i
i
iy1
i
.
Ž4.Ž .
x
, where x
 x
, provided that f
x
exists and is continuous
iq1
iy1
i
iq1
w
x
Ž
.
on a, b . This is equivalent to using formula 9.15 with ns3 and with two
of the interpolation points coincident at x . We then have
i
x
x
M
iq1
iq1
4
2
f x yq
x
dx F
xyx
xyx
xyx
dx,
Ž .
Ž .
Ž
. Ž
. Ž
.
H
H
i
iy1
i
iq1
4!
x
x
iy1
iy1
is1,3, . . . , 2ny1,
12.11
Ž
.

NEWTONCOTES METHOD
523

Ž4.Ž .
where M
is an upper bound on
f
x
for aFxFb. By computing the
4
Ž
.
integral in 12.11 we obtain
5
x
M h
iq1
4
f x yq
x
dx F
,
is1, 3, . . . , 2ny1.
Ž .
Ž .
H
i
90
xiy1
Ž
.
Consequently, the total error of approximation in 12.10 is less than or equal
to
5
5
nM h
M
bya
Ž
.
4
4
s
,
12.12
Ž
.
4
90
2880n
Ž
.
since hs bya r2n. Thus the error of approximation with Simpson’s method
Ž
4.
w
x
is O 1rn , where n is half the number of subintervals into which
a, b is
divided. Hence, Simpson’s method yields a much more accurate approxima-
tion than the trapezoidal method.
As an example, let us apply Simpson’s method to the calculation of the
w
x
integral in Example 12.1.1 using the same division of 1, 2 into 10 subinter-
1
Ž
.
vals, each of length hs
. By applying formula 12.10 we obtain
10
dx
0.10
1
1
1
1
1
2
f
1q4
q
q
q
q
H
ž
/
x
3
1.1
1.3
1.5
1.7
1.9
1
1
1
1
1
1
q2
q
q
q
q
ž
/
1.2
1.4
1.6
1.8
2
s0.69315.
12.3. NEWTON–COTES METHODS
The trapezoidal and Simpson’s methods are two special cases of a general
series of approximate integration methods of the so-called NewtonCotes
type. In the trapezoidal method, straight line segments were used to approxi-
Ž .
mate the graph of the function f x between a and b. In Simpson’s method,
the approximation was carried out using a series of parabolas. We can refine
this approximation even further by considering a series of cubic curves,
quartic curves, and so on. For cubic approximations, four equally spaced
w
x Ž
points are used to subdivide each subinterval of a, b
instead of two points
.
for the trapezoidal method and three points for Simpson’s method , whereas
five points are needed for quartic approximation, and so on. All such
approximations are of the NewtonCotes type.

APPROXIMATION OF INTEGRALS
524
12.4. GAUSSIAN QUADRATURE
All NewtonCotes methods require the use of equally spaced points, as was
seen in the cases of the trapezoidal method and Simpson’s method. If this
requirement is waived, then it is possible to select the points in a manner that
reduces the approximation error.
w
x
Let x x x   x
be nq1 distinct points in a, b . Consider the
0
1
2
n
approximation
n
bf x
dxf

 f x
,
12.13
Ž .
Ž
.
Ž
.
Ý
H
i
i
a
is0
where the coefficients, 
 , 
 , . . . , 
 , are to be determined along with the
0
1
n
Ž
.
points x , x , . . . , x . The total number of unknown quantities in 12.13 is
0
1
n
2nq2. Hence, 2nq2 conditions must be specified. According to the so-called
Gaussian integration rule, the 
 ’s and x ’s are chosen such that the approxi-
i
i
Ž
.
mation in 12.13 will be exact for all polynomials of degrees not exceeding
Ž
.
2nq1. This is equivalent to requiring that the approximation
12.13
be
Ž .
j
exact for f x sx , js0, 1, 2, . . . , 2nq1, that is,
n
b
j
j
x dxs

 x ,
js0, 1, 2, . . . , 2nq1.
12.14
Ž
.
Ý
H
i
i
a
is0
This process produces 2nq2 equations to be solved for the 
 ’s and x ’s. In
i
i
particular, if the limits of integration are asy1, bs1, then it can be shown
Ž
.
see Phillips and Taylor, 1973, page 140 that the x -values will be the nq1
i
Ž .
Ž
.
zeros of the Legendre polynomial p
x of degree nq1 see Section 10.2 .
nq1
Ž
.
The 
 -values can be easily found by solving the system of equations 12.14 ,
i
which is linear in the 
 ’s.
i
1
2
'
Ž .
Ž
.
For example, for ns1, the zeros of p
x s
3x y1 are x sy1r 3 ,
2
0
2
'
Ž
.
x s1r 3 . Applying 12.14 , we obtain
1
1 dxs
 q
´

 q
 s2,
H
0
1
0
1
y1
1
1 x dxs
 x q
 x
´
y
 q
s0,
Ž
.
H
0
0
1
1
0
1
'3
y1
1
1
2
2
2
2
x dxs
 x q
 x
´

 q
s ,
Ž
.
H
0
0
1
1
0
1
3
3
y1
1
1
3
3
3
x dxs
 x q
 x
´
y
 q
s0.
Ž
.
H
0
0
1
1
0
1
'
3 3
y1
We note that the last two equations are identical to the first two. Solving the

GAUSSIAN QUADRATURE
525
latter for 
and 
 , we get 
 s
 s1. Hence, we have the approximation
0
1
0
1
1
1
1 f x
dxff y
qf
,
Ž .
H
ž
/ ž
/
'
'
3
3
y1
Ž .
which is exact if f x is a polynomial of degree not exceeding 2nq1s3.
If the limits of integration are not equal to y1, 1, we can easily convert the
b Ž .
integral H f x dx to one with the limits y1, 1 by making the change of
a
variable
2 xy aqb
Ž
.
zs
.
bya
b Ž .
wŽ
This
converts
the
general
integral
H f x dx
to
the
integral
b y
a
.
x
1
Ž .
a r2 H
g z dz, where
y1
bya zqbqa
Ž
.
g z sf
.
Ž .
2
We therefore have the approximation
n
bya
bf x
dxf

 g z
,
12.15
Ž .
Ž
.
Ž
.
Ý
H
i
i
2
a
is0
Ž .
where the z ’s are the zeros of the Legendre polynomial p
z .
i
nq1
Ž
.
It can be shown that see Davis and Rabinowitz, 1975, page 75 that when
Ž
.
asy1, bs1, the error of approximation in 12.13 is given by
4
2 nq3
n
bya
nq1 !
Ž
.
Ž
.
b
Ž2 nq2.
f x
dxy

 f x
s
f
 ,
ab,
Ž .
Ž
.
Ž
.
Ý
H
i
i
3
a
2nq3
2nq2 !
Ž
. Ž
.
is0
12.16
Ž
.
Ž2 nq2.Ž .
w
x
provided that f
x is continuous on a, b . This error decreases rapidly
as n increases. Thus this Gaussian quadrature provides a very good approxi-
Ž
.
mation with a formula of the type given in 12.13 .
Ž
.
There are several extensions of the approximation in
12.13 . These
extensions are of the form
n
b x f x
dxf

 f x
,
12.17
Ž . Ž .
Ž
.
Ž
.
Ý
H
i
i
a
is0

APPROXIMATION OF INTEGRALS
526
Ž .
where  x is a particular positive weight function. As before, the coefficients
w
x

 , 
 , . . . , 
and the points
x , x , . . . , x , which belong to
a, b , are
0
1
n
0
1
n
Ž
.
chosen so that 12.17 is exact for all polynomials of degrees not exceeding
Ž .
2nq1. The choice of the x ’s depends on the form of  x . It can be shown
i
that the values of x are the zeros of a polynomial of degree nq1 belonging
i
w
x
to the sequence of polynomials that are orthogonal on a, b with respect to
Ž . Ž
 x
see Davis and Rabinowitz, 1975, page 74; Phillips and Taylor, 1973,
.
Ž .
Ž
.Ž
. 
page 142 . For example, if asy1, bs1,  x s 1yx
1qx
, y1,
Ž,  .Ž . Ž
y1, then the x ’s are the zeros of the Jacobi polynomial p
x
see
i
nq1
.
Ž .
Ž
2.y1r2
Section 10.3 . Also, if asy1, bs1,  x s 1yx
, then the x ’s are
i
Ž . Ž
the zeros of the Chebyshev polynomial of the first kind, T
x
see Section
nq1
.
Ž
.
10.4 , and so on. For those two cases, formula
12.17
is called the
GaussJacobi quadrature formula and the GaussChebyshe® quadrature for-
Ž .
Ž
.
mula, respectively. The choice  x s1 results in the original formula 12.13 ,
which is now referred to as the GaussLegendre quadrature formula.
1
Ž
.
EXAMPLE 12.4.1.
Consider the integral H dxr 1qx , which has the exact
0
Ž
.
value log2s0.69314718. Applying formula 12.15 , we get
n
dx

1
i
1
f Ý
H
2
1
1qx
1q
z q1
Ž
.
0
i
2
is0
n

i
s
,
y1Fz F1,
12.18
Ž
.
Ý
i
3qzi
is0
Ž .
where the z ’s are the zeros of the Legendre polynomial p
z , zs2 xy1.
i
nq1
1
2
'
Ž .
Ž
.
Let ns1; then
p
z s
3z y1
with zeros equal to z sy1r 3 ,
2
0
2
'
Ž
.
z s1r 3 . We have seen earlier that 
 s1, 
 s1; hence, from 12.18 , we
1
0
1
obtain
1
dx

1
i
f Ý
H 1qx
3qz
0
i
is0
'
'
3
3
s
q
'
'
3 3 y1
3 3 q1
s0.692307691.
1
3
Ž
.
Ž .
Ž
. Ž
Let us now use ns2 in 12.18 . Then p
z s
5z y3z
see Section
3
2
3
3
1r2
1r2
.
Ž .
Ž .
10.2 . Its zeros are z sy
, z s0, z s
. To find the 
 ’s, we
0
1
2
i
5
5
Ž
.
apply formula
12.14
using asy1, bs1, and
z in place of
x. For

GAUSSIAN QUADRATURE
527
js0, 1, 2, 3, 4, 5, we have
1 dzs
 q
 q
 ,
H
0
1
2
y1
1 z dzs
 z q
 z q
 z ,
H
0
0
1
1
2
2
y1
1
2
2
2
2
z dzs
 z q
 z q
 z ,
H
0
0
1
1
2
2
y1
1
3
3
3
3
z dzs
 z q
 z q
 z ,
H
0
0
1
1
2
2
y1
1
4
4
4
4
z dzs
 z q
 z q
 z ,
H
0
0
1
1
2
2
y1
1
5
5
5
5
z dzs
 z q
 z q
 z .
H
0
0
1
1
2
2
y1
These equations can be written as

 q
 q
 s2,
0
1
2
1r2
3
y
 q
s0,
Ž
.
Ž .
0
2
5
3
2

 q
s ,
Ž
.
0
2
5
3
3r2
3
y
 q
s0,
Ž
.
Ž .
0
2
5
9
2

 q
s ,
Ž
.
0
2
25
5
1r2
9
3
y
 q
s0.
Ž
.
Ž .
0
2
25
5
The above six equations can be reduced to only three that are linearly
independent, namely,

 q
 q
 s2,
0
1
2
y
 q
 s0,
0
2
10

 q
 s
,
0
2
9
5
8
5
the solution of which is 
 s , 
 s , 
 s . Substituting the 
 ’s and z ’s
0
1
2
i
i
9
9
9

APPROXIMATION OF INTEGRALS
528
Ž
.
in 12.18 , we obtain
dx



1
0
1
2
f
q
q
H 1qx
3qz
3qz
3qz
0
0
1
2
5
8
5
9
9
9
s
q
q
1r2
1r2
3
3
3
3y
3q
Ž .
Ž .
5
5
s0.693121685.
For higher values of n, the zeros of Legendre polynomials and the
Ž
corresponding values of 
 can be found, for example, in Shoup 1984, Table
i
.
Ž
.
7.5 and Krylov 1962, Appendix A .
12.5. APPROXIMATION OVER AN INFINITE INTERVAL
 Ž .
Consider an integral of the form H f x dx, which is improper of the first
a
Ž
.
b Ž .
kind see Section 6.5 . It can be approximated by using the integral H f x dx
a
 Ž .
for a sufficiently large value of b, provided, of course, that H f x dx is
a
convergent. The methods discussed earlier in Sections 12.112.4 can then be
b Ž .
applied to H f x dx.
a
 Ž . Ž .
For improper integrals of the first kind, of the form H  x f x dx,
0

Ž . Ž .
H
 x f x dx, we have the following Gaussian approximations:
y
n

 x f x
dxf

 f x
,
12.19
Ž . Ž .
Ž
.
Ž
.
Ý
H
i
i
0
is0
n

 x f x
dxf

 f x
,
12.20
Ž . Ž .
Ž
.
Ž
.
Ý
H
i
i
y
is0
Ž
.
Ž
.
where, as before, the x ’s and 
 ’s are chosen so that 12.19 and 12.20 are
i
i
exact for all polynomials of degrees not exceeding 2nq1. For the weight
Ž .
Ž
.
Ž .
yx
function  x
in
12.19 , the choice  x se
gives the GaussLaguerre
quadrature, for which the x ’s are the zeros of the Laguerre polynomial
i
Ž .
Ž
.
L
x of degree nq1 and s0 see Section 10.6 . The associated error of
nq1
Ž
.
approximation is given by see Davis and Rabinowitz, 1975, page 173
2
n

nq1 !
Ž
.
yx
Ž2 nq2.
e
f x
dxy

 f x
s
f
 ,
0.
Ž .
Ž
.
Ž
.
Ý
H
i
i
2nq2 !
Ž
.
0
is0
Ž .
yx 2
Ž
.
Choosing  x se
in 12.20 gives the GaussHermite quadrature, and the
Ž .
corresponding
x ’s are the zeros of the Hermite polynomial H
x
of
i
nq1
Ž
.
degree nq1 see Section 10.5 . The associated error of approximation is of
Ž
.
the form see Davis and Rabinowitz, 1975, page 174
n
'

nq1 ! 
Ž
.
2
yx
Ž2 nq2.
e
f x
dxy

 f x
s
f
 ,
y.
Ž .
Ž
.
Ž
.
Ý
H
i
i
nq1
2
2nq2 !
Ž
.
y
is0

APPROXIMATION OVER AN INFINITE INTERVAL
529
We can also use the Gauss-Laguerre and the GaussHermite quadrature
 Ž .
formulas
to approximate
convergent
integrals
of the
form
H f x dx,
0

Ž .
H
f x dx:
y

 yx
x
f x
dxs
e
e f x
dx
Ž .
Ž .
H
H
0
0
n
x i
f

 e f x
,
Ž
.
Ý
i
i
is0


2
2
yx
x
f x
dxs
e
e f x
dx
Ž .
Ž .
H
H
y
y
n
2
x i
f

 e f x
.
Ž
.
Ý
i
i
is0
EXAMPLE 12.5.1.
Consider the integral

yx
e
x
Is
dx
H
y2 x
1ye
0
n
f

 f x
,
12.21
Ž
.
Ž
.
Ý
i
i
is0
Ž .
Ž
y2 x.y1
where
f x sx 1ye
and the
x ’s are the zeros of the Laguerre
i
Ž .
Ž .
polynomial L
x . To find expressions for L
x , ns0, 1, 2, . . . , we can use
nq1
n
Ž
.
the recurrence relation 10.37 with s0, which can be written as
dL
x
Ž .
n
L
x s xyny1 L
x yx
,
ns0, 1, 2, . . . .
12.22
Ž .
Ž
.
Ž .
Ž
.
nq1
n
dx
Ž .
Ž
.
Recall that L
x s1. Choosing ns1 in 12.21 , we get
0
If
 f x
q
 f x
.
12.23
Ž
.
Ž
.
Ž
.
0
0
1
1
Ž
.
From 12.22 we have
L
x s xy1 L
x
Ž .
Ž
.
Ž .
1
0
sxy1,
d xy1
Ž
.
L
x s xy2 L
x yx
Ž .
Ž
.
Ž .
2
1
dx
s xy2
xy1 yx
Ž
. Ž
.
sx 2y4 xq2.
'
'
Ž .
The zeros of L
x
are x s2y 2 , x s2q 2 . To find 
and 
 ,
2
0
1
0
1
Ž
.
formula 12.19 must be exact for all polynomials of degrees not exceeding

APPROXIMATION OF INTEGRALS
530
2nq1s3. This is equivalent to requiring that
 yx
e
dxs
 q
 ,
H
0
1
0
 yx
e
x dxs
 x q
 x ,
H
0
0
1
1
0
 yx
2
2
2
e
x dxs
 x q
 x ,
H
0
0
1
1
0
 yx
3
3
3
e
x dxs
 x q
 x .
H
0
0
1
1
0
Only two equations are linearly independent, the solution of which is 
 s
0
Ž
.
0.853553, 
 s0.146447. From 12.23 we then have
1

 x

 x
0
0
1
1
If
q
y2 x
y2 x
0
1
1ye
1ye
s1.225054.
Ž
.
Let us now calculate
12.21
using ns2, 3, 4. The zeros of Laguerre
Ž .
Ž .
Ž .
polynomials L
x , L
x , L
x , and the values of 
 for each n are shown
3
4
5
i
Ž
in Table 12.1. These values are given in Ralston and Rabinowitz 1978, page
.
Ž
.
106 and also in Krylov 1962, Appendix C . The corresponding approximate
values of I are given in Table 12.1. It can be shown that the exact value of I
is  2r8f1.2337.
(
)
Table 12.1. Zeros of Laguerre Polynomials
x , Values of 
 ,
i
i
a
and the Corresponding Approximate Values of I
n
x

I
i
i
1
0.585786
0.853553
3.414214
0.146447
1.225054
2
0.415775
0.711093
2.294280
0.278518
6.289945
0.010389
1.234538
3
0.322548
0.603154
1.745761
0.357419
4.536620
0.038888
9.395071
0.000539
1.234309
4
0.263560
0.521756
1.413403
0.398667
3.596426
0.075942
7.085810
0.003612
12.640801
0.000023
1.233793
a
Ž
.
See 12.21 .

THE METHOD OF LAPLACE
531
12.6. THE METHOD OF LAPLACE
This method is used to approximate integrals of the form
b
hŽ x.
I  s
' x e
dx,
12.24
Ž .
Ž .
Ž
.
H
a
Ž .
w
x
where  is a large positive constant, ' x is continuous on a, b , and the first
Ž .
w
x
and second derivatives of h x are continuous on a, b . The limits a and b
may be finite or infinite. This integral was used by Laplace in his original
Ž
.
development of the central limit theorem see Section 4.5.1 . More specifi-
cally, if X , X , . . . , X , . . . is a sequence of independent and identically
1
2
n
distributed random variables with a common density function, then the
density function of the sum S sÝn
X
can be represented in the form
n
is1
i
Ž
. Ž
.
12.24
see Wong, 1989, Chapter 2 .
Ž .
w
x
Suppose that h x
has a single maximum in the interval
a, b at xst,

Ž .
Ž .
hŽ x.
aFtb, where h t s0 and h t 0. Hence, e
is maximized at t for
any 0. Suppose further that ehŽ x. becomes very strongly peaked at xst
w
x
and decreases rapidly away from xst on a, b as ™. In this case, the
Ž .
Ž .
hŽ x.
major portion of I  comes from integrating the function ' x e
over a
small neighborhood around xst. Under these conditions, it can be shown
that if atb, and as ™,
1r2
y2
hŽt.
I  ' t e
,
12.25
Ž .
Ž .
Ž
.

h
tŽ .
Ž
.
Ž
.
where  denotes asymptotic equality see Section 3.3 . Formula 12.25 is
known as Laplace’s approximation.
Ž
.
Ž .
A heuristic derivation of 12.25 can be arrived at by replacing ' x and
Ž .
h x
by the leading terms in their Taylor’s series expansions around xst.
The integration limits are then extended to y and , that is,

b
b
2

hŽ x.
' x e
dxf
' t exp h t q
xyt
h
t
dx
Ž .
Ž .
Ž .
Ž
.
Ž .
H
H
2
a
a


2

f
' t exp h t q
xyt
h
t
dx
Ž .
Ž .
Ž
.
Ž .
H
2
y


2

hŽt.
s' t e
exp
xyt
h
t
dx
12.26
Ž .
Ž
.
Ž .
Ž
.
H
2
y
1r2
y2
hŽt.
s' t e
.
12.27
Ž .
Ž
.

h
tŽ .
Ž
.
Ž
.
 yx 2
Formula 12.27 follows from 12.26 by making use of the fact that H e
dx
0
1
1
'
Ž .
Ž .
Ž
.
s !
s  r2, where !  is the gamma function see Example 6.9.6 , or
2
2
by simply evaluating the integral of a normal density function.

APPROXIMATION OF INTEGRALS
532
If tsa, then it can be shown that as ™,
1r2
y
hŽa.
I  ' a e
.
12.28
Ž .
Ž .
Ž
.

2h
a
Ž .
Ž
.
Ž
.
Ž
Rigorous proofs of
12.27
and
12.28
can be found in Wong
1989,
.
Ž
.
Ž
.
Chapter 2 , Copson 1965, Chapter 5 , Fulks 1978, Chapter 18 , and Lange
Ž
.
1999, Chapter 4 .
EXAMPLE 12.6.1.
Consider the gamma function
 yx
n
! nq1 s
e
x dx,
ny1.
12.29
Ž
.
Ž
.
H
0
Ž
.
Let us find an approximation for ! nq1 when n is large an positive, but
Ž
.
not necessarily an integer. Let xsnz; then 12.29 can be written as
 yn z
! nq1 sn
e
exp nlog nz
dz
Ž
.
Ž
.
H
0
 yn z
w
x
sn
e
exp n log nqn log z dz
H
0

nq1
sn
exp n yzqlog z
dz.
12.30
Ž
.
Ž
.
H
0
Ž .
Ž .
Let h z syzqlog z. Then h z
has a unique maximum at zs1 with

Ž .
Ž .
Ž
.
Ž
.
h 1 s0 and h 1 sy1. Applying formula 12.27 to 12.30 , we obtain
1r2
y2
nq1
yn
! nq1 n
e
Ž
.
n y1
Ž
.
yn
n'
se
n
2 n ,
12.31
Ž
.
Ž
.
as n™. Formula 12.31 is known as Stirling’s formula.
EXAMPLE 12.6.2.
Consider the integral,

1
I
 s
exp  cos x cos nxdx,
Ž .
Ž
.
H
n

0
Ž
.
Ž .
as ™. This integral looks like
12.24
with h x scos x, which has a
w
x
Ž .
Ž
.
single maximum at xs0 in 0,  . Since h 0 sy1, then by applying 12.28

MULTIPLE INTEGRALS
533
we obtain, as ™,
1r2
1
y

I
 
e
Ž .
n

2 y1
Ž
.
e
s
.
'2
EXAMPLE 12.6.3.
Consider the integral

w
x
I  s
exp  sin x dx
Ž . H
0
Ž .
as ™. Here, h x ssin x, which has a single maximum at xsr2 in
w
x
Ž
.
Ž
.
0,  , and h r2 sy1. From 12.27 , we get
1r2
y2

I  e
Ž .
 y1
Ž
.
2

se( 
as ™.
12.7. MULTIPLE INTEGRALS
We recall that integration of a multivariable function was discussed in
Section 7.9. In the present section, we consider approximate integration
formulas for an n-tuple Riemann integral over a region D in an n-dimen-
sional Euclidean space Rn.
Ž
.
For example, let us consider the double integral IsHH f x , x
dx dx ,
D
1
2
1
2
where D;R2 is the region
Ds
x , x
aFx Fb, 
x
Fx F x
.

4
Ž
.
Ž
.
Ž
.
1
2
1
1
2
1
Then
b
Ž
.
 x1
Is
f x , x
dx
dx
Ž
.
H H
1
2
2
1
Ž
.
a
 x1
b
s
g x
dx ,
12.32
Ž
.
Ž
.
H
1
1
a
where
Ž
.
 x1
g x
s
f x , x
dx .
12.33
Ž
.
Ž
.
Ž
.
H
1
1
2
2
Ž
.
 x1

APPROXIMATION OF INTEGRALS
534
Ž
.
Let us now apply a Gaussian integration rule to
12.32
using the points
x sz , z , . . . , z
with the matching coefficients 
 , 
 , . . . , 
 ,
1
0
1
m
0
1
m
m
bg x
dx f

 g z
.
Ž
.
Ž
.
Ý
H
1
1
i
i
a
is0
Thus
m
Ž
.
 zi
If

f z , x
dx .
12.34
Ž
.
Ž
.
Ý H
i
i
2
2
Ž
.
 zi
is0
Ž
.
For the ith of the mq1 integrals in
12.34
we can apply a Gaussian
integration rule using the points
y , y , . . . , y
with the corresponding
i0
i1
in
coefficients ® , ® , . . . , ® . We then have
i0
i1
in
n
Ž
.
 zi f z , x
dx f
® f z , y
.
Ž
.
Ž
.
Ý
H
i
2
2
i j
i
i j
Ž
.
 zi
js0
Hence,
m
n
Is

 ® f z , y
.
Ž
.
Ý Ý
i i j
i
i j
is0 js0
This procedure can obviously be generalized to higher-order multiple inte-
Ž
.
grals. More details can be found in Stroud 1971 and Davis and Rabinowitz
Ž
.
1975, Chapter 5 .
The method of Laplace in Section 12.6 can be extended to an n-dimen-
sional integral of the form
I  s
' x ehŽx. dx,
Ž .
Ž .
H
D
Ž
.
which is a multidimensional version of the integral in 12.24 . Here, D is a
region in Rn, which may be bounded or unbounded,  is a large positive
Ž
.
constant, and xs x , x , . . . , x
. As before, it is assumed that:
1
2
n
Ž
.
a. ' x is continuous in D.
Ž .
b. h x has continuous first-order and second-order partial derivatives with
respect to x , x , . . . , x
in D.
1
2
n
Ž .
c. h x has a single maximum in D at xst.
Ž .
If t is an interior point of D, then it is also a stationary point of h x , that

is, " hr" x
s0, is1, 2, . . . , n, since t is a point of maximum and the
i xst

THE MONTE CARLO METHOD
535
Ž .
partial derivatives of h x exist. Furthermore, the Hessian matrix,
" 2h x
Ž .
H
t s
,
Ž .
h
ž
/
" x " x
i
j
xst
Ž .
is negative definite. Then, for large , I  is approximately equal to
nr2
2
y1r2
hŽt.
I  
' t
ydet H
t
e
.

4
Ž .
Ž .
Ž .
h
ž /

Ž
.
A proof of this approximation can be found in Wong 1989, Section 9.5 . We
Ž
.
note that this expression is a generalization of Laplace’s formula 12.25 to an
n-tuple Riemann integral.
If t happens to be on the boundary of D and still satisfies the conditions

Ž .
that "hr" x
s0 for is1, 2, . . . , n, and H
t is negative definite, then it
xst
i
h
can be shown that for large ,
nr2
1
2
y1r2
hŽt.
I  
' t
ydet H
t
e
,

4
Ž .
Ž .
Ž .
h
ž /
2

Ž . Ž
which is one-half of the previous approximation for I 
see Wong,1989,
.
page 498 .
12.8. THE MONTE CARLO METHOD
A new approach to approximate integration arose in the 1940s as part of the
Ž
.
Monte Carlo method of S. Ulam and J. von Neumann Haber, 1970 . The
basic idea of the Monte Carlo method for integrals is described as follows:
suppose that we need to compute the integral
b
Is
f x
dx.
12.35
Ž .
Ž
.
H
a
We consider I as the expected value of a certain stochastic process. An
estimate of I can be obtained by random sampling from this process, and the
estimate is then used as an approximation to I. For example, let X be a
Ž
.
continuous random variable that has the uniform distribution U a, b
over
w
x
Ž
.
the interval a, b . The expected value of f X
is
1
b
E f X
s
f x
dx
Ž
.
Ž .
H
bya
a
I
s
.
bya

APPROXIMATION OF INTEGRALS
536
Ž
.
w Ž
.x
Let x , x , . . . , x
be a random sample from U a, b . An estimate of E f X
1
2
n
n
ˆ
Ž
.
Ž
.
is given by 1rn Ý
f x . Hence, an approximate value of I, denoted by I ,
is1
i
n
can be obtained as
n
bya
ˆI s
f x
.
12.36
Ž
.
Ž
.
Ý
n
i
n
is1
ˆ
ˆ
The justification for using I
as an approximation to I is that I
is a
n
n
consistent estimator of I, that is, for a given 0,
ˆ
lim P
I yI G s0.
Ž
.
n
n™
Ž
.
n
Ž
.
w Ž
.x
This is true because 1rn Ý
f x
converges in probability to E f X
as
is1
i
Ž
.
n™, according to the law of large numbers see Sections 3.7 and 5.6.3 . In
ˆ
other words, the probability that I
will be different from I can be made
n
arbitrarily close to zero if n is chosen large enough. In fact, we even have the
ˆ
stronger result that I converges strongly, or almost surely, to I, as n™, by
n
Ž
.
the strong law of large numbers see Section 5.6.3 .
ˆ
ˆ
The closeness of I
to I depends on the variance of I , which is equal to
n
n
n
1
2
ˆ
Var I
s bya
Var
f x
Ž
.
Ž
.
Ž
.
Ý
n
i
n is1
2
2
bya
	
Ž
.
f
s
,
12.37
Ž
.
n
2
Ž
.
where 	
is the variance of the random variable f X , that is,
f
2
	 sVar f X
Ž
.
f
2
2
sE f
X
y E f X

4
Ž
.
Ž
.
2
1
I
b
2
s
f
x
dxy
.
12.38
Ž .
Ž
.
H
ž
/
bya
bya
a
Ž
.
By the central limit theorem
see Section 4.5.1 , if n is large enough,
ˆ
then I
is approximately normally distributed with mean I and variance
n
Ž
.Ž
.2
2
1rn bya 	 . Thus,
f
ˆI yI
n
d
6
Z,
'
bya 	 r n
Ž
.
f
d
6
where Z has the standard normal distribution, and the symbol
denotes

THE MONTE CARLO METHOD
537
Ž
.
convergence in distribution
see Section 4.5.1 . It follows that for a given
0,


1
2
yx r2
ˆ
P
I yI F
bya 	
f
e
dx.
12.39
Ž
.
Ž
.
H
n
f
'
'
n
2
y
Ž
.
The right-hand side of 12.39 is the probability that a standard normal
distribution attains values between y and . Let us denote this probability
Ž
.
by 1y. Then sz
, which is the upper
r2 100th percentile of the
r2
ˆ
standard normal distribution. If we denote the error of approximation, I yI,
n
Ž
.
by E , then formula 12.39 indicates that for large n,
n
1
E
F
bya 	 z
12.40
Ž
.
Ž
.
n
f
 r2
'n
with an approximate probability equal to 1y, which is called the confi-
Ž
.
dence coefficient. Thus, for a fixed , the error bound in 12.40 is propor-
'
tional to 	 and is inversely proportional to
n . For example, if 1ys0.90,
f
then z
s1.645, and
r2
1.645
E
F
bya 	
Ž
.
n
f
'n
with an approximate confidence coefficient equal to 0.90. Also, if 1ys0.95,
then z
s1.96, and
r2
1.96
E
F
bya 	
Ž
.
n
f
'n
with an approximate confidence coefficient equal to 0.95.
Ž
.
2
In order to compute the error bound in
12.40 , an estimate of 	
is
f
Ž
.
needed. Using 12.38 and the random sample x , x , . . . , x , an estimate of
1
2
n
	 2 is given by
f
2
n
n
1
1
2
2
	 s
f
x
y
f x
.
12.41
Ž
.
Ž
.
Ž
.
ˆ
Ý
Ý
f n
i
i
n
n
is1
is1
12.8.1. Variance Reduction
In order to increase the accuracy of the Monte Carlo approximation of I, the
Ž
.
error bound in 12.40 should be reduced for a fixed value of . We can
achieve this by increasing n. Alternatively, we can reduce 	 by considering a
f
distribution other than the uniform distribution. This can be accomplished by
using the so-called method of importance sampling, a description of which
follows.

APPROXIMATION OF INTEGRALS
538
Ž .
w
x
Let g x
be a density function that is positive over the interval
a, b .
Ž .
b Ž .
Ž
.
Thus, g x 0, aFxFb, and H g x dxs1. The integral in 12.35 can be
a
written as
f x
Ž .
b
Is
g x
dx.
12.42
Ž .
Ž
.
H g x
Ž .
a
Ž
.
Ž
.
In this case, I is the expected value of f X rg X , where X is a continuous
Ž .
random variable with the density function g x . Using now a random sample,
x , x , . . . , x , from this distribution, an estimate of I can be obtained as
1
2
n
n
1
f x
Ž
.
i
ˆI s
.
12.43
Ž
.
Ý
n
n
g x
Ž
.
i
is1
ˆ
The variance of I
is then given by
n
	 2
f g
ˆ
Var I
s
,
Ž
.
n
n
where
f X
Ž
.
2
	 sVar
f g
g X
Ž
.
2
2
f X
f X
Ž
.
Ž
.
sE
y
E½
5
½
5
g X
g X
Ž
.
Ž
.
f 2 x
Ž .
b
2
s
g x
dxyI .
12.44
Ž .
Ž
.
H
2
g
x
Ž .
a
As before, the error bound can be derived on the basis of the central limit
ˆ
theorem, using the fact that I
is approximately normally distributed with
n
Ž
.
2
Ž
.
mean I and variance 1rn 	
for large n. Hence, as in 12.40 ,
f g
1

E
F
	
z
n
f g
 r2
'n

ˆ
with an approximate probability equal to 1y, where E sI yI. The
n
n
Ž .
density g x should therefore be chosen so that an error bound smaller than

THE MONTE CARLO METHOD
539
Table 12.2. Approximate Values of IsH 2x2 dx
1
(
)
Using Formula 12.36
ˆ
n
In
50
2.3669
100
2.5087
150
2.2227
200
2.3067
250
2.3718
300
2.3115
350
2.3366
Ž
.
Ž .
the one in 12.40 can be achieved. For example, if f x 0, and if
f x
Ž .
g x s
,
12.45
Ž .
Ž
.
bf x
dx
Ž .
H
a
2
Ž
.
then 	 s0 as can be seen from formula 12.44 .
f g
b Ž .
Unfortunately, since the exact value of H f x dx is the one we seek to
a
Ž
.
Ž .
compute, formula
12.45
cannot be used. However, by choosing g x
to
Ž . w
Ž .
x
behave approximately as f x
assuming f x
is positive , we should expect
a reduction in the variance. Note that the generation of random variables
Ž .
from the g x distribution is more involved than just using a random sample
Ž
.
from the uniform distribution U a, b .
7
2
2
EXAMPLE 12.8.1.
Consider the integral
IsH x dxs f2.3333. Sup-
1
3
Ž
.
pose that we use a sample of 50 points from the uniform distribution U 1, 2 .
ˆ
Ž
.
Applying formula
12.36 , we obtain I s2.3669. Repeating this process
50
several times using higher values of n, we obtain the values in Table 12.2.
EXAMPLE 12.8.2.
Let us now apply the method of importance sampling
1
x
Ž .
to the integral IsH e dxf1.7183. Consider the density function g x s
0
2Ž
.
w
x
1qx over the interval 0, 1 . Using the method described in Section 3.7, a
3
random sample, x , x , . . . , x , can be generated from this distribution as
1
2
n
Ž .
Ž .
w
x
follows: the cumulative distribution function for g x is ysG x sP XFx ,
that is,
x
ysG x s
g t
dt
Ž .
Ž .
H
0
x
2
s
1qt
dt
Ž
.
H
3
0
2
x 2
s
xq
,
0FxF1.
ž
/
3
2

APPROXIMATION OF INTEGRALS
540
Table 12.3. Approximate Values of IsH1e x dx
0
ˆ
ˆ
w
Ž
.x
w
Ž
.x
n
I
Formula 12.46
I
Formula 12.36
n
n
50
1.7176
1.7156
100
1.7137
1.7025
150
1.7063
1.6854
200
1.7297
1.7516
250
1.7026
1.6713
300
1.7189
1.7201
350
1.7093
1.6908
400
1.7188
1.7192
Ž .
w
x
The only solution of ysG x in 0, 1 is
1r2
xsy1q 1q3y
,
0FyF1.
Ž
.
Ž .
Hence, the inverse function of G x is
1r2
y1
G
y sy1q 1q3y
,
0FyF1.
Ž .
Ž
.
If
y , y , . . . , y
form a random sample of n values from the uniform
1
2
n
Ž
.
y1Ž
.
y1Ž
.
y1Ž
.
distribution U 0, 1 , then x sG
y , x sG
y
, . . . , x sG
y
will
1
1
2
2
n
n
Ž .
form a sample from the distribution with the density function g x . Formula
Ž
.
12.43 can then be applied to approximate the value of I using the estimate
n
x i
3
e
ˆI s
.
12.46
Ž
.
Ý
n
2n
1qxi
is1
ˆ
Table 12.3 gives I
for several values of n. For the sake of comparison,
n
ˆ
x
Ž
. w
Ž .
x
values of I
from formula
12.36
with f x se , as0, bs1 were also
n
Ž
.
computed using a sample from the uniform distribution U 0, 1 . The results
ˆ
are shown in Table 12.3. We note that the values of I
are more stable and
n
ˆ
closer to the true value of I than those of I .
n
12.8.2. Integrals in Higher Dimensions
The Monte Carlo method can be extended to multidimensional integrals.
Ž .
Consider computing the integral IsH f x dx, where D is a bounded region
D
n
Ž
.
in R , the n-dimensional Euclidean space, and xs x , x , . . . , x
. As be-
1
2
n
fore, we consider I as the expected value of a stochastic process having a
certain distribution over D. For example, we may take X to be a continuous
random vector uniformly distributed over D. By this we mean that the
Ž
.
Ž
.
probability of X being in D is 1r® D , where ® D denotes the volume of D,

APPLICATIONS IN STATISTICS
541
and the probability of X being outside D is zero. Hence, the expected value
Ž .
of f X is
1
E f X
s
f x dx
Ž .
Ž .
H
® D
Ž
.
D
I
s
.
® D
Ž
.
Ž .
2
The variance of f X , denoted by 	 , is
f
2
2
2
	 sE f
X
y E f X

4
Ž .
Ž .
f
2
1
I
2
s
f
x dxy
.
Ž .
H
® D
® D
Ž
.
Ž
.
D
Let us now take a sample of N independent observations on X, namely,
w Ž .x
Ž
.
N
Ž
.
X , X , . . . , X . Then a consistent estimator of E f X
is 1rN Ý
f X ,
1
2
N
is1
i
and hence, an estimate of I is given by
N
® D
Ž
.
ˆI s
f X
,
Ž
.
Ý
N
i
N
is1
ˆ
which can be used to approximate I. The variance of I
is
N
	 2
f
2
ˆ
Var I
s
®
D .
12.47
Ž
.
Ž
.
Ž
.
N
N
ˆ
If N is large enough, then by the central limit theorem, I
is approximately
N
Ž
.
normally distributed with mean I and variance as in 12.47 . It follows that
® D
Ž
.
E
F
	 z
N
f
 r2
'N
ˆ
with an approximate probability equal to 1y, where E sIyI
is the
N
N
Ž
.
error of approximation. This formula is analogous to formula 12.40 .
The method of importance sampling can also be applied here to reduce
the error of approximation. The application of this method is similar to the
case of a single-variable integral as seen earlier.
12.9. APPLICATIONS IN STATISTICS
Approximation of integrals is a problem of substantial concern for statisti-
cians. The statistical literature in this area has grown significantly in the last
20 years, particularly in connection with integrals that arise in Bayesian

APPROXIMATION OF INTEGRALS
542
Ž
.
statistics. Evans and Swartz
1995
presented a survey of the major tech-
niques and approaches available for the numerical approximation of integrals
Ž
.
in statistics. The proceedings edited by Flournoy and Tsutakawa
1991
includes several interesting articles on statistical multiple integration, includ-
ing a detailed description of available software to compute multidimensional
Ž
.
integrals see the article by Kahaner, 1991, page 9 .
12.9.1. The Gauss-Hermite Quadrature
The GaussHermite quadrature mentioned earlier in Section 12.5 is often
used for numerical integration in statistics because of its relation to Gaussian
Ž
.
normal
densities. We recall that this quadrature is defined in terms of

yx 2 Ž .
Ž
.
integrals of the form H
e
f x dx. Using formula 12.20 , we have approxi-
y
mately
n

2
yx
e
f x
dxf

 f x
,
Ž .
Ž
.
Ý
H
i
i
y
is0
Ž .
where the x ’s are the zeros of the Hermite polynomial H
x
of degree
i
nq1
nq1, and the 
 ’s are suitably corresponding weights. Tables of x and 
i
i
i
Ž
.
values are given by Abramowitz and Stegun 1972, page 924 and by Krylov
Ž
.
1962, Appendix B .
Ž
.
Liu and Pierce 1994 applied the GaussHermite quadrature to integrals

Ž .
of the form H
g t dt, which can be expressed in the form
y


g t
dts
f t  t, , 	
dt,
Ž .
Ž .
Ž
.
H
H
y
y
Ž
.
where  t, , 	
is the normal density
1
1
2
 t, , 	
s
exp y
ty
,
Ž
.
Ž
.
2
2
'
2	
2	
Ž .
Ž .
Ž
.
and f t sg t r t, , 	 . Thus,


1
1
2
g t
dts
f t exp y
ty
dt
Ž .
Ž .
Ž
.
H
H
2
2
'
2	
y
y
2	

1
2
yx
'
s
f q 2 	 x e
dx
Ž
.
H
'
y
n
1
'
f

 f q 2 	 x
,
12.48
Ž
.
Ž
.
Ý
i
i
'
is0

APPLICATIONS IN STATISTICS
543
Ž .
where the x ’s are the zeros of the Hermite polynomial H
x
of degree
i
nq1
nq1. We may recall that the x ’s and 
 ’s are chosen so that this approxima-
i
i
tion will be exact for all polynomials of degrees not exceeding 2nq1. For
Ž
.
Ž
.
this reason, Liu and Pierce 1994 recommend choosing  and 	 in 12.48
Ž .
so that f t
is well approximated by a low-order polynomial in the region
'
Ž
.
where the values of q 2 	 x are taken. More specifically, the
nq1 th-
i
Ž
.
order GaussHermite quadrature in
12.48
will be highly effective if the
Ž .
Ž
2.
ratio of g t to the normal density  t, , 	
can be well approximated by a
Ž .
polynomial of degree not exceeding 2nq1 in the region where g t
is
Ž .
substantial. This arises frequently, for example, when g t
is a likelihood
w
Ž .
x
function if g t 0 , or the product of a likelihood function and a normal
Ž
.
density, as was pointed out by Liu and Pierce
1994 , who gave several
Ž
.
examples to demonstrate the usefulness of the approximation in 12.48 .
12.9.2. Minimum Mean Squared Error Quadrature
Ž
Correlated observations may arise in some experimental work Piegorsch and
.
Bailer, 1993 . Consider, for example, the model
y sf t
q
,
Ž
.
qr
q
qr
where y
represents the observed value from experimental unit r at time t
qr
q
Ž
.
Ž
.
qs0, 1, . . . , m; rs1, 2, . . . , n , f t
is the underlying response function, and
q
Ž
.
Ž
.

is a random error term. It is assumed that E 
s0, Cov 
, 
s	
,
qr
qr
pr
qr
pq
Ž
.
Ž
.
and Cov 
, 
s0
rs for all p, q. The area under the response curve
pr
qs
over the interval t tt
is
0
m
tm
As
f t
dt.
12.49
Ž .
Ž
.
H
t0
This is an important measure for a variety of experimental situations,
including the assessment of chemical bioavailability in drug disposition stud-
Ž
.
ies Gibaldi and Perrier, 1982, Chapters 2, 7 and other clinical settings. If the
Ž .
Ž
.
functional form of f  is unknown, the integral in 12.49 is estimated by
numerical methods. This is accomplished using a quadrature approximation
Ž
.
of the integral in
12.49 . By definition, a quadrature estimator of this
integral is
m
ˆ
ˆ
As
 f ,
12.50
Ž
.
Ý
q
q
qs0
ˆ
ˆ ˆ
Ž
.
Ž
.
where f
is some unbiased estimator of f sf t
with Cov f , f
s	 rn,
q
q
q
p
q
pq
and the  ’s form a set of quadrature coefficients.
q

APPROXIMATION OF INTEGRALS
544
ˆ
The expected value of A,
m
ˆ
E A s
 f ,
Ž
.
Ý
q
q
qs0
may not necessarily be equal to A due to the quadrature approximation
ˆ
employed in calculating A. The bias in estimating A is
ˆ
biassE A yA,
Ž
.
ˆ
Ž
.
and the mean squared error MSE of A is given by
2
ˆ
ˆ
ˆ
MSE A sVar Aq E A yA
Ž
.
Ž
.
2
m
m
m
	pq
s
 
q
 f yA
,
Ý Ý
Ý
p
q
q
q
ž
/
n
ps0 qs0
qs0
which can be written as
1
2


ˆ
MSE A s
 Vq f yA
,
Ž
.
Ž
.
n
Ž
.
Ž
.

Ž
.
where Vs 	
, fs f , f , . . . , f
, and  s  ,  , . . . , 
. Hence,
pq
0
1
m
0
1
m
1



2
ˆ
MSE A s
Vqf f
y2 Af qA .
12.51
Ž
.
Ž
.
n
ˆ
Ž
.
Ž
.
Let us now seek an optimum value of  that minimizes MSE A in 12.51 .
ˆ
Ž
.
For this purpose, we equate the gradient of MSE A
with respect to ,
ˆ
Ž
.
namely 
 MSE A , to zero. We obtain

1

ˆ

 MSE A s2
Vqf f
yAf s0.
12.52
Ž
.
Ž
.

ž
/
n
In order for this equation to have a solution, the vector Af must belong to
Ž
.

the column space of the matrix 1rn Vqf f . Note that this matrix is positive
semidefinite. If its inverse exists, then it will be positive definite, and the only

Ž
.
solution,  , to 12.52 , namely,
y1
1


 sA
Vqf f
f,
12.53
Ž
.
ž
/
n
ˆ

Ž
. Ž
.
Ž
.
yields a unique minimum of MSE A
see Section 7.7 . Using 
in 12.50 ,

APPLICATIONS IN STATISTICS
545
we obtain the following estimate of A:
ˆ
ˆ

A sf 
y1
1


ˆ
sAf
Vqf f
f.
12.54
Ž
.
n
Ž
. Ž
.
Using the ShermanMorrison formula
see Exercise 2.14 ,
12.54
can be
written as


y1
y1
ˆ
nf V
f f V
f


y1
ˆ
ˆ
A snA f V
fy
,
12.55
Ž
.

y1
1qnf V
f
ˆ
ˆ ˆ
ˆ

ˆ
Ž
.
where fs f , f , . . . , f
. We refer to A
as an MSE-optimal estimator of
0
1
m
ˆ
A. Replacing f with its unbiased estimator f, and
A with some initial
ˆ
estimate, say A sf  , where 
is an initial value of , yields the
0
0
0
approximate MSE-optimal estimator
c

ˆ
A
s
A ,
12.56
Ž
.
0
1qc
where c is the quadratic form,
ˆ
y1ˆ
csnf V
f.
ˆ
˜
˜
ˆ
w
Ž
.x
This estimator has the form f , where s cr 1qc  . Since cG0, A
0
provides a shrinkage of the initial estimate in A toward zero. This creates a
0
biased estimator of A with a smaller variance, which results in an overall
reduction in MSE. A similar approach is used in ridge regression to estimate
the parameters of a linear model when the columns of the corresponding
Ž
.
matrix X are multicollinear see Section 5.6.6 .
We note that this procedure requires knowledge of V. In many applica-
Ž
.
tions, it may not be possible to specify V. Piegorsch and Bailer 1993 used an
estimate of V when V was assumed to have certain particular patterns, such
2
2wŽ
.
x
as Vs	 I or Vs	
1y IqJ , where I and J are, respectively, the
identity matrix and the square matrix of ones, and 	 2 and  are unknown
constants. For example, under the equal variance assumption Vs	 2I, the
Ž
.
ratio cr 1qc is equal to
y1
2
c
	
s 1q
.

ž
/
1qc
ˆˆ
nf f
ˆ
ˆ
ˆ
w Ž
. x
If f is normally distributed, fN f, 1rn V as is the case when f is a vector

n
ˆ
Ž
.
Ž
.
of means, fs y , y , . . . , y
with
y s 1rn Ý
y then an unbiased
0
1
m
q
rs1
qr

APPROXIMATION OF INTEGRALS
546
estimate of 	 2 is given by
m
n
1
2
2s s
y yy
.
Ž
.
Ý Ý
qr
q
mq1
ny1
Ž
. Ž
. qs0 rs1
2
2
Ž
.
Substituting s
in place of 	
in 12.56 , we obtain the area estimator
y1
2s

ˆ
A
sA
1q
.
0

ž
/
ˆˆ
nf f
Ž
.
A similar quadrature approximation of A in 12.49 was considered earlier
Ž
.
by Katz and D’Argenio 1983 using a trapezoidal approximation to A. The
quadrature points were selected so that they minimize the expectation of the
square of the difference between the exact integral and the quadrature
approximation. This approach was applied to simulated pharmacokinetic
problems.
12.9.3. Moments of a Ratio of Quadratic Forms
Consider the ratio of quadratic forms,
y
Ay
Qs
,
12.57
Ž
.

y By
where A and B are symmetric matrices, B is positive definite, and y is an
n1 random vector. Ratios such as Q are frequently encountered in
statistics and econometrics. In general, their exact distributions are mathe-
matically intractable, especially when the quadratic forms are not indepen-
dent. For this reason, the derivation of the moments of such ratios, for the
purpose of approximating their distributions, is of interest. Sutradhar and
Ž
.
Bartlett 1989 obtained approximate expressions for the first four moments
of the ratio Q for a normally distributed y. The moments were utilized to
approximate the distribution of Q. This approximation was then applied to
calculate the percentile points of a modified F-test statistic for testing
treatment
effects
in a one-way model under
correlated
observations.
Ž
.
Morin 1992
derived exact, but complicated, expressions for the first four
moments of Q for a normally distributed y. The moments are expressed in
terms of confluent hypergeometric functions of many variables.
If y is not normally distributed, then no tractable formulas exist for the
Ž
.
moments of Q in 12.57 . Hence, manageable and computable approxima-
Ž
.
tions for these moments would be helpful. Lieberman
1994
used the
method of Laplace to provide general approximations for the moments of Q
without making the normality assumption on y. Lieberman showed that if
Ž 
.
wŽ 
.kx
E y By and E y Ay
exist for kG1, then the Laplace approximation of

APPLICATIONS IN STATISTICS
547
Ž
k.
E Q
, the kth noncentral moment of Q, is given by
k

E
y Ay
Ž
.
k
E
Q
s
.
12.58
Ž
.
Ž
.
L
k

E y By
Ž
.
Ž
.
In particular, if yN ,  , then the Laplace approximation for the mean
and second noncentral moment of Q are written explicitly as
tr A q
A
Ž
.
E
Q s
Ž
.

L
tr B q B
Ž
.
2

E
y Ay
Ž
.
2
E
Q
s
Ž
.
L
2

E y By
Ž
.
2


Var y Ay q E y Ay
Ž
.
Ž
.
s
2

E y By
Ž
.
2
2


2tr
A
q4AAq tr A qA
Ž
.
Ž
.
s
2

tr B q B
Ž
.
Žsee Searle, 1971, Section 2.5 for expressions for the mean and variance of a
.
quadratic form in normal variables .
EXAMPLE 12.9.1.
Consider the linear model, ysXq, where X is
np of rank p,  is a vector of unknown parameters, and  is a random
error vector. Under the null hypothesis of no serial correlation in the
Ž
2 .
elements of , we have N 0, 	 I . The corresponding Durbin-Watson
Ž
.
1950, 1951 test statistic is given by

PA P
1
ds
,

 P
(

)1

where PsIX X X
X , and A
is the matrix
1
1
y1
0
0

0
0
y1
2
y1
0

0
0
0
y1
2
y1

0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A s
.
1
.
.
.
.
.
.
.
0
0

y1
2
y1
0
0
0

0
y1
2
y1
0
0

0
0
y1
1

APPROXIMATION OF INTEGRALS
548
Ž
.
Ž .
Then, by 12.58 , the Laplace approximation of E d is
E 
PA P
Ž
.
1
E
d s
.
12.59
Ž .
Ž
.

L
E  P
Ž
.
Ž
.
Durbin and Watson 1951 showed that d is distributed independently of its
own denominator, so that the moments of the ratio d are the ratios of the
corresponding moments of the numerator and denominator, that is,
k

E
 PA P
Ž
.
1
k
E d
s
.
12.60
Ž
.
Ž
.
k

E
 P
Ž
.
Ž
.
Ž
.
From 12.59
and
12.60
we note that the Laplace approximation for the
Ž .
Ž
.
mean, E d , is exact. For kG2, Lieberman 1994 showed that
E
dk
1
Ž
.
L
s1qO
.
k
ž /
n
E d
Ž
.
Thus, the relative error of approximating higher-order moments of d is
Ž
.
O 1rn , regardless of the matrix X.
12.9.4. Laplace’s Approximation in Bayesian Statistics
Ž
.
Suppose
Kass,
Tierney,
and
Kadane,
1991
that
a data
vector ys
Ž
.
Ž 
.
y , y , . . . , y
has a distribution with the density function p y  , where 
1
2
n
Ž .
is an unknown parameter. Let L 
denote the corresponding likelihood
Ž 
.
function, which is proportional to p y  . In Bayesian statistics, a prior
Ž .
density,   , is assumed on , and inferences are based on the posterior
Ž  .
Ž . Ž .
density q  y , which is proportional to L    , where the proportionality
Ž  .
constant is determined by requiring that q  y integrate to one. For a given
Ž .
real-valued function g  , its posterior expectation is given by
g  L   
d
Ž .
Ž .
Ž .
H
E g 
y s
.
12.61
Ž .
Ž
.
L   
d
Ž .
Ž .
H
Ž
.
Ž
.
Tierney, Kass, and Kadane
1989
expressed the integrands in
12.61
as
follows:
g  L    sb

exp ynh

,
Ž .
Ž .
Ž .
Ž .
Ž .
N
N
L    sb

exp ynh

,
Ž .
Ž .
Ž .
Ž .
D
D

APPLICATIONS IN STATISTICS
549
Ž .
Ž .
where b

and b

are smooth functions that do not depend on n and
N
D
Ž .
Ž .
h

and h

are constant-order functions of n, as n™. Formula
N
D
Ž
.
12.61 can then be written as
b

exp ynh

d
Ž .
Ž .
H N
N
E g 
y s
.
12.62
Ž .
Ž
.
b

exp ynh

d
Ž .
Ž .
H D
D
Ž
.
Ž
.
Applying Laplace’s approximation in 12.25 to the integrals in 12.62 , we
obtain, if n is large,
1r2

ˆ
ˆ
ˆ
b

exp ynh

h

Ž
.
Ž
.
Ž
.
N
N
N
N
D
D
E g 
y f
,
12.63
Ž .
Ž
.

ˆ
ˆ
ˆ
h

b

exp ynh

Ž
.
Ž
.
Ž
.
N
N
D
D
D
D
ˆ
ˆ
Ž .
where 
and 
are the locations of the single maxima of yh

and
N
D
N
Ž .
Ž .
Ž .
yh
 ,
respectively.
In
particular,
if
we
choose
h
 s h
 s
D
N
D
Ž
.
w
Ž . Ž .x
Ž .
Ž .
Ž .
Ž
.
y 1rn log L    , b
 sg 
and b
 s1, then 12.63 reduces to
N
D
ˆ
E g 
y fg  ,
12.64
Ž .
Ž
.
Ž .
ˆ
Ž
.
w
Ž . Ž .x
where  is the point at which 1rn log L   
attains its maximum.
Ž
.
w
x
Formula 12.64 provides a first-order approximation of E g 
y . This
Ž .
ˆ
approximation is often called the modal approximation because  is the mode
of the posterior density. A more accurate second-order approximation of
w
x
Ž
.
E g 
y was given by Kass, Tierney, and Kadane 1991 .
Ž .
12.9.5. Other Methods of Approximating Integrals in Statistics
There are several major techniques and approaches available for the numeri-
cal approximation of integrals in statistics that are beyond the scope of this
book. These techniques, which include the saddlepoint approximation and
Marko® chain Monte Carlo, have received a great deal of attention in the
statistical literature in recent years.
The saddlepoint method is designed to approximate integrals of the
Laplace type in which both the integrand and contour of integration are
allowed to be complex valued. It is a powerful tool for obtaining accurate
expressions for densities and distribution functions. A good introduction to
Ž
the basic principles underlying this method was given by De Bruijn 1961,
.
Ž
.
Chapter 5 . Daniels 1954 is credited with having introduced it in statistics in
the context of approximating the density of a sample mean of independent
and identically distributed random variables.
Ž
.
Markov chain Monte Carlo MCMC is a general method for the simula-
tion of stochastic processes having probability densities known up to a

APPROXIMATION OF INTEGRALS
550
constant of proportionality. It generally deals with high-dimensional statisti-
cal problems, and has come into prominence in statistical applications during
the past several years. Although MCMC has potential applications in several
areas of statistics, most attention to date has been focused on Bayesian
applications.
Ž
.
For a review of these techniques, see, for example, Geyer 1992 , Evans
Ž
.
Ž
.
Ž
.
and Swartz 1995 , Goutis and Casella 1999 , and Strawderman 2000 .
FURTHER READING AND ANNOTATED BIBLIOGRAPHY
Ž
.
Abramowitz, M., and I. A. Stegun, eds. 1972 . Handbook of Mathematical Functions
Ž
with Formulas, Graphs, and Mathematical Tables. Wiley, New York. This volume
is an excellent source for a wide variety of numerical tables of mathematical
functions. Chap. 25 gives zeros of Legendre, Hermite, and Laguerre polynomials
.
along with their corresponding weight factors.
Ž
.
Copson, E. T. 1965 . Asymptotic Expansions. Cambridge University Press, London.
Ž
.
The method of Laplace is discussed in Chap. 5.
Ž
.
Courant, R., and F. John
1965 . Introduction to Calculus and Analysis, Volume 1.
Ž
Wiley, New York.
The trapezoidal and Simpson’s methods are discussed in
.
Chap. 6.
Ž
.
Daniels, H. 1954 . ‘‘Saddlepoint approximation in statistics.’’ Ann.Math. Statist., 25,
631650.
Ž
.
Davis, P. J., and P. Rabinowitz 1975 . Methods of Numerical Integration. Academic
Ž
Press, New York.
This book presents several useful numerical integration
methods, including approximate integrations over finite or infinite intervals as
.
well as integration in two or more dimensions.
Ž
.
De Bruijn, N. G.
1961 . Asymptotic Methods in Analysis, 2nd ed. North-Holland,
Ž
Amsterdam. Chap. 4 covers the method of Laplace, and the saddlepoint method
.
is the topic of Chap. 5.
Ž
.
Durbin, J., and G. S. Watson 1950 . ‘‘Testing for serial correlation in least squares
regression, I.’’ Biometrika, 37, 409428.
Ž
.
Durbin, J., and G. S. Watson 1951 . ‘‘Testing for serial correlation in least squares
regression, II.’’ Biometrika, 38, 159178.
Ž
.
Evans, M., and T. Swartz 1995 . ‘‘Methods for approximating integrals in statistics
with special emphasis on Bayesian integration problems.’’ Statist. Sci., 10, 254272.
Ž
.
Flournoy, N., and R. K. Tsutakawa, eds.
1991 . Statistical Multiple Integration,
Contemporary Mathematics 115. Amer. Math. Soc., Providence, Rhode Island.
ŽThis volume contains the proceedings of an AMSIMSSIAM joint summer
research conference on statistical multiple integration, which was held at Hum-
.
boldt University, Arcata, California, June 1723, 1989.
Ž
.
Ž
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York. Section 18.3 of this
.
book contains proofs associated with the method of Laplace.
Ž
.
Geyer, C. J. 1992 . ‘‘Practical Markov chain Monte Carlo.’’ Statist. Sci., 7, 473511.

FURTHER READING AND ANNOTATED BIBLIOGRAPHY
551
Ž
.
Ghazal, G. A.
1994 . ‘‘Moments of the ratio of two dependent quadratic forms.’’
Statist. Prob. Letters, 20, 313319.
Ž
.
Gibaldi, M., and D. Perrier 1982 . Pharmacokinetics, 2nd ed. Dekker, New York.
Ž
.
Goutis, C., and G. Casella 1999 . ‘‘Explaining the saddlepoint approximation.’’ Amer.
Statist., 53, 216224.
Ž
.
Haber, S.
1970 . ‘‘Numerical evaluation of multiple integrals.’’ SIAM Re®., 12,
481526.
Ž
.
Kahaner, D. K. 1991 . ‘‘A survey of existing multidimensional quadrature routines.’’
In Statistical Multiple Integration, Contemporary Mathematics 115, N. Flournoy
and R. K. Tsutakawa, eds. Amer. Math. Soc., Providence, pp. 922.
Ž
.
Kass, R. E., L. Tierney, and J. B. Kadane
1991 . ‘‘Laplace’s method in Bayesian
analysis.’’ In Statistical Multiple Integration, Contemporary Mathematics 115,
N. Flournoy and R. K. Tsutakawa, eds. Amer. Math. Soc., Providence, pp. 8999.
Ž
.
Katz, D., and D. Z. D’Argenio 1983 . ‘‘Experimental design for estimating integrals
by numerical quadrature, with applications to pharmacokinetic studies.’’ Biomet-
rics, 39, 621628.
Ž
.
Ž
Krylov, V. I. 1962 . Approximate Calculation of Integrals. Macmillan, New York. This
book considers only the problem of approximate integration of functions of a
.
single variable. It was translated from the Russian by A. H. Stroud.
Ž
.
Ž
Lange, K. 1999 . Numerical Analysis for Statisticians. Springer, New York. This book
contains a wide variety of topics on numerical analysis of potential interest to
statisticians, including recent topics such as bootstrap calculations and the
.
Markov chain Monte Carlo method.
Ž
.
Lieberman, O.
1994 . ‘‘A Laplace approximation to the moments of a ratio of
quadratic forms.’’ Biometrika, 81, 681690.
Ž
.
Liu, Q., and D. A. Pierce 1994 . ‘‘A note on GaussHermite quadrature.’’ Biometrika,
81, 624629.
Ž
.
Morin, D. 1992 . ‘‘Exact moments of ratios of quadratic forms.’’ Metron, 50, 5978.
Ž
.
Morland, T.
1998 . ‘‘Approximations to the normal distribution function.’’ Math.
Gazette, 82, 431437.
Ž
.
Nonweiler, T. R. F. 1984 . Computational Mathematics. Ellis Horwood, Chichester,
Ž
.
England. Numerical quadrature is covered in Chap. 5.
Ž
.
Phillips, C., and B. Cornelius
1986 . Computational Numerical Methods. Ellis Hor-
Ž
.
wood, Chichester, England. Numerical integration is the subject of Chap. 6. .
Ž
.
Phillips, G. M., and P. J. Taylor 1973 . Theory and Applications of Numerical Analysis.
Ž
.
Academic Press, New York. Gaussian quadrature is covered in Chap. 6. .
Ž
.
Piegorsch, W. W., and A. J. Bailer 1993 . ‘‘Minimum mean-square error quadrature.’’
J. Statist. Comput. Simul, 46, 217234.
Ž
.
Ralston, A., and P. Rabinowitz
1978 .
A First Course in Numerical Analysis.
Ž
.
McGraw-Hill, New York. Gaussian quadrature is covered in Chap. 4.
Ž
.
Reid, W. H., and S. J. Skates 1986 . ‘‘On the asymptotic approximation of integrals.’’
SIAM J. Appl. Math., 46, 351358.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Reading, Massachusetts.
Ž
.
Searle, S. R. 1971 . Linear Models. Wiley, New York.

APPROXIMATION OF INTEGRALS
552
Ž
.
Shoup, T. E. 1984 . Applied Numerical Methods for the Microcomputer. Prentice-Hall,
Englewood Cliffs, New Jersey.
Ž
.
Stark, P. A. 1970 . Introduction to Numerical Methods. Macmillan, London.
Ž
.
Strawderman, R. L. 2000 . ‘‘Higher-order asymptotic approximation: Laplace, sad-
dlepoint, and related methods.’’ J. Amer. Statist. Assoc., 95, 13581364.
Ž
.
Stroud, A. H.
1971 . Approximate Calculation of Multiple Integrals. Prentice-Hall,
Englewood Cliffs, New Jersey.
Ž
.
Sutradhar, B. C., and R. F. Bartlett 1989 . ‘‘An approximation to the distribution of
the ratio of two general quadratic forms with application to time series valued
designs.’’ Comm. Statist. Theory Methods, 18, 15631588.
Ž
.
Tierney, L., R. E. Kass, and J. B. Kadane
1989 . ‘‘Fully exponential Laplace
approximations to expectations and variances of nonpositive functions.’’ J. Amer.
Statist. Assoc., 84, 710716.
Ž
.
Wong, R. 1989 . Asymptotic Approximations of Integrals. Academic Press, New York.
ŽThis is a useful reference book on the method of Laplace and Mellin transform
.
techniques for multiple integrals. All results are accompanied by error bounds.
EXERCISES
In Mathematics
12.1. Consider the integral
n
I s
log x dx.
H
n
1
It is easy to show that
I sn log nynq1.
n
( )
a
Approximate I
by using the trapezoidal method and the partition
n
points x s1, x s2, . . . , x sn, and verify that
0
1
n
1
I flog n! y log n.
Ž
.
n
2
( )
Ž .
nq1r2
yn
b
Deduce from a that n! and n
e
are of the same order of
magnitude, which is essentially what is stated in Stirling’s formula
Ž
.
see Example 12.6.1
1
Ž
.
12.2. Obtain an approximation of the integral H dxr 1qx
by Simpson’s
0
method for the following values of n: 2, 4, 8, 16. Show that when ns8,
the error of approximation is less than or equal to 0.000002.

EXERCISES
553
12.3. Use GaussLegendre quadrature with ns2 to approximate the value
of the integral H r2 sin  d. Give an upper bound on the error of
0
Ž
.
approximation using formula 12.16 .
( )
12.4.
a
Show that

1
2
2
yx
ym
e
dx
e
,
m0.
H
m
m
w
yx 2
ym x
x
Hint: Use the inequality e
e
for xm.
( )
Ž .
b
Find a value of m so that the upper bound in a is smaller than
10y4.
( )
Ž .
 yx 2
c
Use part
b
to find an approximation for H e
dx correct to
0
three decimal places.
 Ž
x
yx
.y1
12.5. Obtain an approximate value for the integral H x e qe
y1
dx
0
w
using the GaussLaguerre quadrature.
Hint: Use the tables in Ap-
Ž
.
pendix C of the book by Krylov 1962, page 347 giving the zeros of
x
Laguerre polynomials and the corresponding values of 
 .i
12.6. Consider the indefinite integral
x
dt
I x s
sArctan x.
Ž . H
2
1qt
0
( )
Ž .
a
Make an appropriate change of variables to show that I x can be
written as
du
1
I x s2 x
Ž .
H
2
2
y1 4qx
uq1
Ž
.
( )
b
Use a five-point GaussLegendre quadrature to provide an ap-
Ž .
proximation for I x .
12.7. Investigate the asymptotic behavior of the integral
1

I  s
cos x
dx
Ž .
Ž
.
H
0
as ™.
( )
12.8.
a
Use the Gauss-Laguerre quadrature to approximate the integral
Hey6 x sin x dx using ns1, 2, 3, 4, and compare the results with
0
the true value of the integral.

APPROXIMATION OF INTEGRALS
554
( )
b
Use the Gauss-Hermite quadrature to approximate the integral

2
Ž
.
H
x exp y3x
dx using ns1, 2, 3, 4, and compare the results
y
with the true value of the integral.
12.9. Give an approximation to the double integral
1
2 1r2
Ž
.
1r2
1yx 1
2
2
1yx yx
dx dx
Ž
.
H H
1
2
1
2
0
0
Ž
.
Ž
.
by applying the GaussLegendre rule to formulas 12.32 and 12.33 .
1

2 yn
1r2
Ž
.
Ž
.
12.10. Show that H 1qx
dx is asymptotically equal to
rn
as
0
2
n™.
In Statistics
12.11. Consider a sample, X , X , . . . , X , of independent and indentically
1
2
n
distributed random variables from the standard normal distribution.
Ž
.
Suppose that n is odd. The sample median is the
mq1 th order
Ž
.
statistic X
, where ms ny1 r2. It is known that X
has
Žmq1.
Žmq1.
the density function
m
ny1
m
f x sn

x
1y x
 x ,
Ž .
Ž .
Ž .
Ž .
ž
/
m
where
1
x 2
 x s
exp y
Ž .
ž
/
'
2
2
is the standard normal density function, and
x
 x s
 t
dt
Ž .
Ž .
H
y
Ž
.
see Roussas, 1973, page 194 . Since the mean of X
is zero, the
Žmq1.
variance of X
is given by
Žmq1.

m
ny1
2
m
Var X
sn
x 
x
1y x
 x
dx.
Ž .
Ž .
Ž .
H
Žmq1.
ž
/
m
y
Obtain an approximation for this variance using the GaussHermite
quadrature for ns11 and varying numbers of quadrature points.

EXERCISES
555
Ž
.
12.12.
Morland, 1998. Consider the density function
1
2
yx r2
 x s
e
Ž .
'2
for the standard normal distribution. Show that if xG0, then the
cumulative distribution function,
x
1
2
yt r2
 x s
e
dt,
Ž .
H
'2
y
can be represented as the sum of the series
2
4
6
1
x
x
x
x
 x s
q
1y
q
y
Ž .
'
2
6
40
336
2
8
2 n
x
x
n
q
y  q y1
q  .
Ž
.
n
3456
2nq1 2 n!
Ž
.
w Note: By truncating this series, we can obtain a polynomial approxi-
Ž .
mation of  x . For example, we have the following approximation of
order 11:
2
4
6
8
10
1
x
x
x
x
x
x
x
 x f
q
1y
q
y
q
y
.
Ž .
'
2
6
40
336
3456
42240
2
12.13. Use the result in Exercise 12.12 to show that there exist constants
a, b,c, d, such that
1
x
1qax2qbx4
 x f
q
.
Ž .
2
4
'
2
1qcx qdx
2
12.14. Apply the method of importance sampling to approximate the value of
the integral H 2e x 2 dx using a sample of size ns150 from the distribu-
0
1
Ž .
Ž
.
tion whose density function is g x s
1qx , 0FxF2. Compare
4
your answer with the one you would get from applying formula
Ž
.
12.36 .

APPROXIMATION OF INTEGRALS
556
12.15. Consider the density function
nq1
1Ž
.
y
nq1
2
!
2
ž
/
x
2
f x s
1q
,
yx,
Ž .
n ž
/
n
'n !ž /
2
Ž
.
for a t-distribution with n degrees of freedom see Exercise 6.28 . Let
Ž .
x
Ž .
F x sH
f t dt be its cumulative distribution function. Show that
y
for large n,
x
1
2
yt r2
F x f
e
dt,
Ž .
H
'2
y
which is the cumulative distribution function for the standard normal.
w
x
Hint: Apply Stirling’s formula.

A P P E N D I X
Solutions to Selected Exercises
CHAPTER 1
1.3. AjC;B implies A;B. Hence, AsAlB. Thus, AlB;C implies
A;C. It follows that AlCs.
( )
1.4.
a
xgAB implies that xgA but B, or xgB but A; thus
x g A j B y A l B.
Vice
versa,
if
x g A j B y A l B,
then
xgAB.
( )
Ž
.
c
xgAl BD
implies that xgA and xgBD, so that either
Ž
xgAlB but AlD, or xgAlD but AlB, so that xg A
.
Ž
.
Ž
.
Ž
.
lB  AlD . Vice versa, if xg AlB  AlD , then either
xgAlB but AlD, or xgAlD but AlB, so that either
x is in A and B but D, or x is in A and D, but B; thus
Ž
.
xgAl BD .
1.5. It is obvious that  is reflexive, symmetric, and transitive. Hence, it is
Ž
.
an equivalence relation. If
m , n
is an element in
A, then its
0
0
Ž
.
equivalence class is the set of all pairs
m, n in A such that mrm s
0
nrn , that is, mrnsm rn .
0
0
0
Ž
.
Ž
.
1.6. The equivalence class of 1, 2 consists of all pairs m, n in A such that
mynsy1.
1.7. The first elements in all four pairs are distinct.
( )
Ž
n
.
Ž .
n
1.8.
a
If
ygf 
A , then
ysf x , where xg
A . Hence, if
is1
i
is1
i
Ž .
Ž
.
Ž .
n
Ž
.
xgA and f x gf A
for some i, then f x g
f A ; thus
i
i
is1
i
Ž
n
.
n
Ž
.
f 
A ;
f A . Vice versa, it is easy to show that
is1
i
is1
i
n
Ž
.
Ž
n
.

f A ;f 
A .
is1
i
is1
i
( )
Ž
n
.
Ž .
b
If
ygf 
A , then
ysf x , where
xgA
for all i; then
is1
i
i
Ž .
Ž
.
Ž .
n
Ž
.
f x gf A
for all i; then f x g
f A . Equality holds if f is
i
is1
i
one-to-one.
557

SOLUTIONS TO SELECTED EXERCISES
558
q
Ž .
2
1.11. Define f : J ™A such that f n s2n q1. Then f is one-to-one and
onto, so A is countable.
'
'
'
'
1.13. aq b scq d ´aycs d y b . If asc, then bsd. If ac, then
'
'
'
'
'
Ž
. Ž
d y b is a nonzero rational number and
d q b s dyb r
d
'
'
'
'
'
.
y b . It follows that both
d y b and
d q b are rational num-
'
'
bers, and therefore
b and
d must be rational numbers.
Ž
.
1.14. Let gsinf A . Then gFx for all x in A, so ygGyx, hence, yg is
an upper bound of yA and is the least upper bound: if yg
 is any
other upper bound of yA, then yg
Gyx, so xGg
, hence, g
 is a


Ž
.
lower bound of A, so g Fg, that is, yg Gyg, so ygssup yA .
1.15. Suppose that bA. Since b is the least upper bound of A, it must be a
limit point of A: for any 0, there exists an element agA such that
aby. Furthermore, ab, since bA. Hence, b is a limit point of
A. But A is closed; hence, by Theorem 1.6.4, bgA.
1.17. Suppose that G is a basis for FF, and let pgB. Then Bs U , where


U
belongs to G. Hence, there is at least one U such that pgU ;B.



Vice versa, if for each BgFF and each pgB there is a UgG such
that pgU;B, then G must be a basis for FF: for each pgB we can

4
find a set U gG such that pgU ;B; then Bs U
pgB , so G is
p
p
p
a basis.
1.18. Let p be a limit point of AjB. Then p is a limit point of A or of B.
In either case, pgAjB. Hence, by Theorem 1.6.4, AjB is a closed
set.

4

4
1.19. Let C
be an open covering of B. Then B and C
form an open


covering of A. Since A is compact, a finite subcollection of the latter
covering covers A. Furthermore, since B does not cover B, the mem-

4
bers of this finite covering that contain B are all in C
. Hence, B is

compact.
Ž
.
1.20. No. Let
A, FF
be a topological space such that A consists of two
points a and b and FF consists of A and the empty set . Then A is
compact, and the point a is a compact subset; but it is not closed, since
the complement of a, namely b, is not a member of FF, and is therefore
not open.
( )
Ž
.
yn

1.21.
a
Let wgA. Then X w Fxxq3
for all n, so wg
A .
ns1
n

Ž
.
yn
Vice versa, if wg
A , then X w xq3
for all n. To
ns1
n
Ž
.
Ž
.
Ž
.
yn
show that X w Fx: if X w x, then X w xq3
for some n,
Ž
.
a contradiction. Therefore, X w Fx, and wgA.

CHAPTER 1
559
( )
Ž
.
Ž
.
yn
b
Let wgB. Then X w x, so X w xy3
for some n, hence,


Ž
.
yn
wg
B . Vice versa, if wg
B , then X w Fxy3
for
ns1
n
ns1
n
Ž
.
Ž
.
Ž
.
yn
some n, so X w x: if X w Gx, then X w xy3
for all n, a
contradiction. Therefore, wgB.
( )
Ž
.
1.22.
a
P XG2 Fr2, since s.
( )
b
P XG2 s1yp X2
Ž
.
Ž
.
s1yp 0 yp 1
Ž .
Ž .
s1yey q1 .
Ž
.
To show that

y
1ye
q1 
.
Ž
.
2
Ž .
yŽ
.
Let   sr2qe
q1 y1. Then
Ž .
i.  0 s0, and

1
y
Ž .
ii.   s
ye
0 for all .
2
Ž .
Ž .
Ž .
From i and ii we conclude that   0 for all 0.
( )
1.23.
a
2
2
p
Xy Gc sP
Xy
Gc
Ž
.
Ž
.
	 2
F
,
2
c
Ž
.2
2
by Markov’s inequality and the fact that E Xy
s	 .
( )
Ž .
b
Use csk	 in a .
( )
c
P
Xy k	
s1yP
Xy Gk	
Ž
.
Ž
.
1
G1y
.
2
k
1
1
2
1
2
Ž
.
Ž
  .
Ž
  .
1.24. sE X sH
x 1y x
dxs0, 	 sH
x
1y x
dxs .
y1
y1
6
( )
a
1
1
2
2
Ž

.
i. P
X G
F	 r s .
2
4
3
1
1
1
3
2
Ž

.
Ž

.
ii. P
X 
sP
X G
F	 r s .
3
3
9
2
( )
b
1
1
1
P
X G
sP XG
qP XFy
Ž
.
Ž
.
Ž
.
2
2
2
1
2
s
 .
4
3

SOLUTIONS TO SELECTED EXERCISES
560
( )
1.25.
a
True, since X
Gx if and only if X Gx for all i.
Ž1.
i
( )
b
True, since X
Fx if and only if X Fx for all i.
Žn.
i
( )
c
P X
Fx s1yP X
x
Ž
.
Ž
.
Ž1.
Ž1.
n
s1y 1yF x
.
Ž .
( )
Ž
.
w
Ž .xn
d
P X
Fx s F x
.
Žn.
Ž
.
Ž
.
Ž
.
1.26. P 2FX
F3 sP X
F3 yP X
F2 . Hence,
Ž1.
Ž1.
Ž1.
5
5
P 2FX
F3 s1y 1yF 3
y1q 1yF 2
Ž .
Ž .
Ž
.
Ž1.
5
5
s 1yF 2
y 1yF 3
.
Ž .
Ž .
Ž .
x
y2 t
y2 x
But F x sH 2e
dts1ye
. Hence,
0
5
5
y4
y6
P 2FX
F3 s e
y e
Ž
.
Ž
.
Ž
.
Ž1.
sey20 yey30 .
CHAPTER 2
w
x
2.1. If mn, then the rank of the nm matrix Us u : u :  : u
is less
1
2
m
than or equal to n. Hence, the number of linearly independent columns
of U is less than or equal to n, so the m columns of U must be linearly
dependent.
2.2. If u , u , . . . , u
and v are linearly dependent, then v must belong to W,
1
2
n
a contradiction.
2.5. Suppose that u , u , . . . , u
are linearly independent in U, and that
1
2
n
n
Ž
.
Ž
n
.
Ý
 T u s0 for some constants  ,  , . . . ,  . Then T Ý
 u
is1
i
i
1
2
n
is1
i
i
s0.
If
T
is
one-to-one,
then
Ýn
 u s0,
hence,
 s0
for
is1
i
i
i
Ž
.
all i, since the u ’s are linearly independent. It follows that T u ,
i
1
Ž
.
Ž
.
T u
, . . . , T u
must also be linearly independent. Vice versa, let
2
n
Ž .
ugU such that T u s0. Let e , e , . . . , e
be a basis for U. Then
1
2
n
n
Ž .
n
Ž .
usÝ
 e for some constants  ,  , . . . ,  . T u s0 ´ Ý
 T e
is1
i
i
1
2
n
is1
i
i
Ž
.
Ž
.
Ž
.
s0 ´  s0 for all i, since T e , T e
, . . . , T e
are linearly inde-
i
1
2
n
pendent. It follows that us0 and T is one-to-one.
Ž
.
Ž 
 .
2
2.7. If As a
, then tr AA sÝ Ý a . Hence, As0 if and only if
i j
i
j
i j
Ž 
 .
tr AA s0.

CHAPTER 2
561
2.8. It is sufficient to show that Avs0 if v
Avs0. If v
Avs0, then A1r2 vs0,
w
1r2
and hence Avs0. Note: A
is defined as follows: since A is symmet-
ric, it can be written as AsP P
 by Theorem 2.3.10, where  is a
diagonal matrix whose diagonal elements are the eigenvalues of A, and
P is an orthogonal matrix. Furthermore, since A is positive semidefinite,
its eigenvalues are greater than or equal to zero by Theorem 2.3.13.
The matrix A1r2 is defined as P1r2 P
, where the diagonal elements of
1r2
are the square roots of the corresponding diagonal elements
x
of .
2.9. By Theorem 2.3.15, there exists an orthogonal matrix P and diagonal
matrices  , 
such that AsP P
, BsP P
. The diagonal ele-
1
2
1
2
ments of 
and 
are nonnegative. Hence,
1
2
ABsP  P
1
2
is positive semidefinite, since the diagonal elements of  
are
1
2
nonnegative.

Ž
.
Ž
.
Ž .
2.10. Let CsAB. Then C sy BA. Since tr AB str BA , we have tr C s
Ž

.
Ž .
Ž .
tr y C sytr C and thus tr C s0.
2.11. Let BsAyA
. Then



tr B B str
A yA
AyA
Ž
.
Ž
. Ž
.



str
A y A A ytr
A yA A
Ž
.
Ž
.


str
A yA A ytr A AyA
Ž
.
Ž
.


str
A yA A ytr
AyA A
Ž
.
Ž
.

s2 tr
A yA A s0,
Ž
.
since A
yA is skew-symmetric. Hence, by Exercise 2.7, Bs0 and thus
AsA
.
2.12. This follows easily from Theorem 2.3.9.
2.13. By Theorem 2.3.10, we can write
AyI sP yI
P
,
Ž
.
n
n
where P is an orthogonal matrix and  is diagonal with diagonal
elements equal to the eigenvalues of A. The diagonal elements of
yI
are the eigenvalues of AyI . Now, k diagonal elements of
n
n
yI
are equal to zero, and the remaining nyk elements must
n
be different from zero. Hence, AyI
has rank nyk.
n

SOLUTIONS TO SELECTED EXERCISES
562
Ž
.2
2
2
2.17. If ABsBAsB, then
AyB
sA yAByBAqB sAy2BqBs
Ž
.2
AyB. Vice versa, if AyB is idempotent, then AyBs AyB
s
A2yAByBAqB2sAyAByBAqB. Thus,
ABqBAs2B,
so ABqABAs2AB and thus ABsABA. We also have
ABAqBAs2BA,
hence,
BAsABA.
It follows that ABsBA. We finally conclude that
BsABsBA.
2.18. If A is an nn orthogonal matrix with determinant 1, then its eigen-
values are of the form e i1, e i 2, . . . , e iq, 1, where 1 is of multiplic-
ity ny2q and none of the real numbers 
is a multiple of 2
j
Ž
. Ž
js1, 2, . . . , q
those that are odd multiples of  give an even number
.
of eigenvalues equal to y1 .
2.19. Using Theorem 2.3.10, it is easy to show that
e
A I FAFe
A I ,
Ž .
Ž .
min
n
max
n
Ž .
where the inequality on the right means that e
A I yA is positive
max
n
Ž .
semidefinite, and the one on the left means that Aye
A I
is
min
n
positive semidefinite. It follows that
e
A L
LFL
ALFe
A L
L
Ž .
Ž .
min
max
Ž .
Hence, by Theorem 2.3.19 1 ,
e
A tr L
L Ftr L
AL Fe
A tr L
L .
Ž .
Ž
.
Ž
.
Ž .
Ž
.
min
max
( )
Ž .

2.20.
a
We have that AGe
A I
by Theorem 2.3.10. Hence, LALG
min
n
Ž .

Ž

.
Ž .
Ž

 .
e
A L L, and therefore, e
LAL Ge
A e
L L by Theorem
min
min
min
min
Ž .

2.3.18 and the fact that e
A G0 and L L is nonnegative definite.
min
( )
Ž .
b
This is similar to a .
2.21. We have that
e
B AFA1r2 BA1r2 Fe
B A,
Ž .
Ž .
min
max
since A is nonnegative definite. Hence,
e
B tr A Ftr A1r2 BA1r2 Fe
B tr A .
Ž .
Ž .
Ž
.
Ž .
Ž .
min
max

CHAPTER 2
563
The result follows by noting that
tr AB str A1r2 BA1r2 .
Ž
.
Ž
.
( )
Ž
.
2.23.
a
AsI y 1rn J , where J
is the matrix of ones of order nn.
n
n
n
A2sA, since J 2snJ . The rank of A is the same as its trace, which
n
n
is equal to ny1.
( ) Ž
.
2
2
Ž
2. 
2
Ž
2. Ž
2
.
b
ny1 s r	 s 1r	
y Ay
, since 1r	
A 	 I
is idem-
ny1
n
potent of rank ny1, and the noncentrality parameter is zero.
( )
c
1

Cov y, Ay sCov
1 y, Ay
Ž
.
n
ž
/
n
1

2
s
1
	 I
A
Ž
.
n
n
n
	 2

s
1 A
n
n
s0
.
Since y is normally distributed, both y and Ay, being linear trans-
formations of y, are also normally distributed. Hence, they must be
independent, since they are uncorrelated. We can similarly show
1r2
that y and A
y are uncorrelated, and hence independent, by the
fact that 1
 As0
 and thus 1
 A1 s0, which means 1
 A1r2 s0
. It
n
n
n
n


1r2
1r2
follows that y and y A
A
ysy Ay are independent.
( )
2.24.
a
The model is written as
ysXgq ,
w
a
x
where Xs 1
:
1
,
denotes the direct sum of matrices,
[
[
N
n
is1
i
Ž
.
a
gs ,  ,  , . . . , 
, and NsÝ
n . Now, q is estimable,
1
2
a
is1
i
i
since it can be written as a
 g, where a
 is a row of X, is1, 2, . . . , a.
i
i
Ž 

 .


It follows that  y s a ya
g is estimable, since the vector
i
i
i
i
a
 ya

 belongs to the row space of X.
i
i
( )
b
Suppose that  is estimable; then it can be written as
a
s

q
,
Ž
.
Ý
i
i
is1
where  ,  , . . . , 
are constants, since q , q , . . . , q
1
2
a
1
2
a
form a basic set of estimable linear functions. We must therefore
have Ýa  s1, and  s0 for all i, a contradiction. Therefore,  is
isl
i
i
nonestimable.

SOLUTIONS TO SELECTED EXERCISES
564
( )
Ž

.y

Ž

.y

Ž

.y

Ž .
2.25.
a
X X X
X X X X
X sX X X
X by Theorem 2.3.3 2 .
( )
Ž

 .





b
E lllll y s ´lllll Xs . Hence,  slllll X. Now,
y
y





2
ˆ
Var   s X X
X X X X
 	
Ž
.
Ž
.
Ž
.
y
y





2
s lllll X X X
X X X X
X lllll	
Ž
.
Ž
.
y



2
s lllll X X X
X lllll	 ,
by Theorem 2.3.3 2
Ž
.
Ž .
F lllll

lllll	 2,
Ž

.y

since I yX X X
X
is positive semidefinite. The result follows
n
Ž

 .

2
from the last inequality, since Var lllll y slllll lllll	 ,

y1


ˆ
y1



ˆ


Ž
.
2.26.  sP qkI
P X y. But
sP
P X y, so
 P sP X y.
p
Hence,
y1


ˆ
 sP qkI
 P 
Ž
.
p

ˆ
sPDP .
2.27.
2

2
y1
y1
sup  s sup  C
AC

Ž
.
1
2
x, y
 , 
2

y1
y1
s sup
sup  C
AC

Ž
.
1
2
½
5


2



y1
y1
s sup
sup b 
b s C
AC
Ž
.
Ž
.
1
2
½
5


s sup
sup 
bb

Ž
.
½
5


s sup e
bb
,
by applying 2.9

4
Ž
.
Ž
.
max

s sup e
b
b

4
Ž
.
max

s sup 
Cy1ACy2A
Cy1 

4
1
2
1

se
Cy1ACy2A
Cy1 ,
by 2.9
Ž
.
Ž
.
max
1
2
1
se
Cy2ACy2A
Ž
.
max
1
2
se
By1ABy1A
 .
Ž
.
max
1
2

CHAPTER 3
565
CHAPTER 3
( )
3.1.
a
x 5y1
2
3
4
lim
s lim 1qxqx qx qx
Ž
.
xy1
x™1
x™1
s5.
( )
Ž
.
b
xsin 1rx
F x ´ lim
x sin 1rx s0.
Ž
.
x™0
( )
c
The limit does not exist, since the function is equal to 1 except
Ž
.
when
sin 1rx s0,
that
is,
when
xs1r, 1r2, . . . , 
1rn, . . . . For such values, the function takes the form 0r0, and is
therefore undefined. To have a limit at xs0, the function must be
defined at all points in a deleted neighborhood of xs0.
1
( )
Ž .
Ž .
y
q
d
lim
f x s
and lim
f x s1. Therefore, the function
x™0
x™0
2
Ž .
f x does not have a limit as x™0.
( )
Ž
3.
2
Ž
3.
2
3.2.
a
To
show
that
tan x
rx ™0
as
x ™0:
tan x
rx s
3
3
2
3
3
3
2
Ž
.Ž
.
1rcos x
sin x
rx . But
sin x
F x
. Hence,
tan x
rx
F
Ž
.
 

3
x r cos x
™0 as x™0.
'
( )
b
xr x ™0 as x™0.
( )
Ž .
Ž .
c
O 1
is bounded as x™. Therefore, O 1 rx™0 as x™, so
Ž .
Ž .
O 1 so x as x™.
( )
d
1
1
2
f x g x s xqo x
qO
Ž . Ž .
Ž
.
2
ž /
x
x
1
1
1
1
2
2
s
qx O
q
o x
qo x
O
.
Ž
.
Ž
.
2
ž /
ž /
x
x
x
x
Now, by definition,
xO 1rx
is bounded by a positive constant as
Ž
.
Ž
2.
2
Ž
2.
2
x™0, and o x
rx ™0 as x™0. Hence, o x
rx
is bounded,
Ž .
Ž
2.
Ž
.
w Ž
2.
2x
Ž
.
that is, O 1 . Furthermore, o x
O 1rx sx o x
rx
xO 1rx ,
which goes to zero as x™0, is also bounded. It follows that
1
f x g x s
qO 1 .
Ž . Ž .
Ž .
x
( )
Ž .
Ž .
Ž .
3.3.
a
f 0 s0 and lim
f x s0. Hence, f x is continuous at xs0. It
x™0
is also continuous at all other values of x.
( )
Ž .
w
x
Ž .
Ž .
y
b
The function f x is defined on 1, 2 , but lim
f x s, so f x
x™2
is not continuous at xs2.

SOLUTIONS TO SELECTED EXERCISES
566
( )
Ž .
c
If n is odd, then f x is continuous everywhere, except at xs0. If
Ž .
Ž
n is even, f x will be continuous only when x0. m and n must
be expressed in their lowest terms so that the only common divisor
.
is 1 .
( )
Ž .
d
f x is continuous for x1.
3.6.
y3,
x0,
f x s
Ž . ½ 0,
xs0.
Ž .
Thus f x is continuous everywhere except at xs0.
Ž .
Ž .
Ž .
y
q
3.7. lim
f x s2 and lim
f x s0. Therefore, f x cannot be made
x™0
x™0
continuous at xs0.
Ž
.
Ž .
Ž .
Ž .
3.8. Letting asbs0 in f aqb sf a qf b , we conclude that f 0 s0.
Now, for any x , x gR,
1
2
f x
sf x
qf x yx
.
Ž
.
Ž
.
Ž
.
1
2
1
2
Let zsx yx . Then
1
2
f x
yf x
s f z
Ž
.
Ž
.
Ž .
1
2
s f z yf 0
.
Ž .
Ž .
Ž .
Since f x is continuous at xs0, for a given 0 there exists a 0
such that
f z yf 0
 if
z . Hence,
f x
yf x
 for
Ž .
Ž .
Ž
.
Ž
.
1
2
Ž .
all x , x
such that
x yx
. Thus f x
is uniformly continuous
1
2
1
2
everywhere in R.
Ž .
Ž .
Ž .
Ž .
y
q
3.9. lim
f x slim
f x sf 1 s1, so f x
is continuous at xs1.
x™1
x™1
w
x
It is also continuous everywhere else on
0, 2 , which is closed and
bounded. Therefore, it is uniformly continuous by Theorem 3.4.6.
3.10.
x yx
x qx
2
1
2
1
cos x ycos x
s 2sin
sin
1
2
ž
/ ž
/
2
2
x yx
2
1
F2 sinž
/
2
F x yx
for all x , x in R.
1
2
1
2
Ž
.
Thus, for a given 0, there exists a 0 namely, 
such that
cos x ycos x
 whenever
x yx
.
1
2
1
2

CHAPTER 3
567
Ž .
w
x
3.13. f x s0 if x is a rational number in a, b . If x is an irrational number,

4
Ž
then it is a limit of a sequence
y
of rational numbers
any
n ns1
.
neighborhood of x contains infinitely many rationals . Hence,
f x sf
lim y
s lim f y
s0,
Ž .
Ž
.
ž
/
n
n
n™
n™
Ž .
Ž .
w
x
since f x is a continuous function. Thus f x s0 for every x in a, b .
Ž .
3.14. f x can be written as
3y2 x,
xFy1,
f x s
Ž .
5,
y1x1,
½ 3q2 x,
xG1.
Ž .
Hence, f x has a unique inverse for xFy1 and for xG1.
3.15. The inverse function is
y,
yF1,
y1
xsf
y s
Ž .
1½
yq1 ,
y1.
Ž
.
2
y1
( )
Ž .
Ž .
'
3.16.
a
The inverse of f x is f
y s2y
yr2 .
y1
( )
Ž .
Ž .
'
b
The inverse of f x is f
y s2q
yr2 .
3.17. By Theorem 3.6.1,
n
n
f a
yf b
FK
a yb
Ž
.
Ž
.
Ý
Ý
i
i
i
i
is1
is1
K.
Choosing  such that rK, we get
n
f a
yf b
 ,
Ž
.
Ž
.
Ý
i
i
is1
for any given 0.
3.18. This inequality can be proved by using mathematical induction: it is
obviously true for ns1. Suppose that it is true for nsm. To show that
it is true for nsmq1. For nsm, we have
Ým
a x
Ým
a f x
Ž
.
is1
i
i
is1
i
i
f
F
,
ž
/
A
A
m
m

SOLUTIONS TO SELECTED EXERCISES
568
where A sÝm a . Let
m
is1
i
Ým
a x
is1
i
i
b s
.
m
Am
Then
Ýmq1 a x
A b qa
x
is1
i
i
m
m
mq1
mq1
f
sfž
/
ž
/
A
A
mq1
mq1
A
a
m
mq1
F
f b
q
f x
.
Ž
.
Ž
.
m
mq1
A
A
mq1
mq1
Ž
.
Ž
.
m
Ž
.
But f b
F 1rA
Ý
a f x . Hence,
m
m
is1
i
i
Ýmq1 a x
Ým
a f x
qa
f x
Ž
.
Ž
.
is1
i
i
is1
i
i
mq1
mq1
f
F
ž
/
A
A
mq1
mq1
Ýmq1 a f x
Ž
.
is1
i
i
s
.
Amq1

4
3.19. Let a be a limit point of S. There exists a sequence a
in S such
n ns1
Ž
.
that lim
a sa
if S is finite, then S is closed already . Hence,
n™
n
Ž .
Ž
.
Ž
.
f a sf lim
a
slim
f a
s0. It follows that agS, and S is
n™
n
n™
n
therefore a closed set.
Ž .
w Ž .x
3.20. Let g x sexp f x . We have that
f x q 1y x
Ff x
q 1y f x
Ž
.
Ž
.
Ž
. Ž
.
1
2
1
2
for all x , x
in D, 0FF1. Hence,
1
2
g x q 1y x
Fexp f x
q 1y f x
Ž
.
Ž
.
Ž
. Ž
.
1
2
1
2
Fg x
q 1y g x
,
Ž
.
Ž
. Ž
.
1
2
x
Ž .
since the function e
is convex. Hence, g x is convex on D.
2
( )
Ž
.
Ž
.
3.22.
a
We have that E
X
G E X
. Let XN , 	
. Then
Ž
.

1
1
2
E X s
exp y
xy
x dx.
Ž
.
Ž
.
H
2
2
'
2	
y
2	

CHAPTER 3
569
Therefore,

1
1
2
E X
F
exp y
xy
x
dx
Ž
.
Ž
.
H
2
2
'
2	
y
2	
sE
X
.
Ž
.
( )
Ž
yX .
y
Ž
.
yx
b
We have that E e
Ge
, where sE X , since e
is a
convex function. The density function of the exponential distribu-
tion with mean  is
1
yx r 
g x s
e
,
0x.
Ž .

Hence,

1
yX
yx r  yx
E e
s
e
e
dx
Ž
.
H

0
1
1
1
s
s
.
 1q1r
q1
But
1
1

2
n
e s1qq
 q  q
 q 
2!
n!
G1q.
It follows that
1
y
e
F q1
and thus
E eyX
Gey.
Ž
.
2
1r2
Ž
.
Ž
.
3.23. P
X
G sP
X
G
™0 as n™, since
X
converges in
n
n
n
probability to zero.
3.24.
2
2
	 s E
Xy
Ž
.
2
sE
Xy
2
GE
Xy
,
hence,
	GE
Xy
.

SOLUTIONS TO SELECTED EXERCISES
570
CHAPTER 4
4.1.
f h yf yh
f h yf 0
f 0 yf yh
Ž .
Ž
.
Ž .
Ž .
Ž .
Ž
.
lim
s lim
q lim
2h
2h
2h
h™0
h™0
h™0
f 
 0
1
f yh yf 0
Ž .
Ž
.
Ž .
s
q
lim
2
2
yh
h™0
Ž
.
f 
 0
f 
 0
Ž .
Ž .

s
q
sf
0 .
Ž .
2
2
Ž .
 
The converse is not true: let f x s x . Then
f h yf yh
Ž .
Ž
.
s0,
2h

Ž .
and its limit is zero. But f 0 does not exist.
Ž
.
4.3. For a given 0, there exists a 0 such that for any xgN
x
,

0
xx ,
0
f x yf x
Ž .
Ž
.
0

yf
x
.
Ž
.
0
xyx0
Hence,
f x yf x
Ž .
Ž
.
0

 f
x
q
Ž
.
0
xyx0
and thus
f x yf x
A xyx
,
Ž .
Ž
.
0
0

where As f
x
q.
Ž
.
0
Ž .
Ž
.
Ž .

Ž
.
4.4. g x sf xq1 yf x sf
 , where  is between x and xq1. As

Ž
.
Ž .
x™, we have ™ and f
 ™0, hence, g x ™0 as x™.

Ž .
Ž
3
. Ž
.
Ž
2
q
y
4.5. We have that f 1 slim
x y2 xq1 r xy1 slim
ax y
x™1
x™1
. Ž
.
Ž .
bxq1q1 r xy1 , and thus 1s2ayb. Furthermore, since f x
is
continuous at xs1, we must have aybq1sy1. It follows that
as3, bs5, and

3x 2y2,
xG1,
f
x s
Ž . ½ 6 xy5,
x1,
which is continuous everywhere.

CHAPTER 4
571
4.6. Let x0.
( )
a
Taylor’s theorem gives
2
2h
Ž
.


f xq2h sf x q2hf
x q
f
 ,
Ž
.
Ž .
Ž .
Ž
.
2!
where xxq2h, and h0. Hence,
1


f
x s
f xq2h yf x
yhf
 ,
Ž .
Ž
.
Ž .
Ž
.
2h

so that
f
x
Fm rhqhm .
Ž .
0
2

( )
b
Since m is the least upper bound of
f
x
,
Ž .
1
m0
m F
qhm .
1
2
h
Equivalently,
m h2ym hqm G0
2
1
0
This inequality is valid for all h0 provided that the discriminant
 sm2y4m m
1
0
2
is less than or equal to zero, that is, m2F4m m .
1
0
2

Ž .
4.7. No, unless f
x is continuous at x . For example, the function
0
xq1,
x0,
f x s
Ž .
0,
xs0,
xy1,
x0,

Ž .
does not have a derivative at xs0, but lim
f
x s1.
x™0
4.8.
n
y ya qÝ
y ya yÝ
y yy
j
i j
i
is1
i
j

D
y
s lim
Ž
.
j
ayy
a™y j
j
y ya qÝ
y ya yÝ
y yy
j
i j
i
i j
i
j
s lim
,
ayy
a™y j
j
Ž
.
which does not exist, since lim
y ya r ayy
does not exist.
a™y
j
j
j

SOLUTIONS TO SELECTED EXERCISES
572
4.9.

d
f x
x f
x yf x
Ž .
Ž .
Ž .
s
.
2
dx
x
x

Ž .
Ž .
x f
x yf x ™0 as x™0. Hence, by l’Hospital’s rule,



d
f x
f
x qx f
x yf
x
Ž .
Ž .
Ž .
Ž .
lim
s lim
dx
x
2 x
x™0
x™0
1

s
f
0 .
Ž .
2
4.10. Let x , x gR. Then,
1
2

f x
yf x
s
x yx
f

,
Ž
.
Ž
.
Ž
.
Ž
.
1
2
1
2

Ž .
here 
is between
x
and
x . Since f
x
is bounded for all
x,
1
2

f

M for some positive constant M. Hence, for a given 0,
Ž
.
there exists a 0, where M, such that
f x
yf x
 if
Ž
.
Ž
.
1
2
Ž .
x yx
. Thus f x is uniformly continuous on R.
1
2



1
Ž .
Ž .
4.11. f
x s1qcg
x , and
cg
x
cM. Choose c such that cM . In
Ž .
2
this case,

1
cg
x
 ,
Ž .
2
so
1

1
y cg
x  ,
Ž .
2
2
and,
1

3
f
x  .
Ž .
2
2

Ž .
Ž .
Hence, f
x is positive and f x is therefore strictly monotone increas-
Ž .
ing, thus f x is one-to-one.

Ž .
Ž
.
4.12. It is sufficient to show that g
x G0 on 0,  :
x f 
 x yf x
Ž .
Ž .

g
x s
,
x0.
Ž .
2
x
By the mean value theorem,
f x sf 0 qx f 
 c ,
0cx
Ž .
Ž .
Ž .
sx f 
 c .
Ž .

CHAPTER 4
573
Hence,
x f 
 x yx f 
 c
f 
 x yf 
 c
Ž .
Ž .
Ž .
Ž .

g
x s
s
,
x0.
Ž .
2
x
x

Ž .
Since f
x is monotone increasing, we have for all x0,
f 
 x Gf 
 c
and thus
g
 x G0
Ž .
Ž .
Ž .
Ž
. x
4.14. Let ys 1q1rx
. Then,
1
log 1q1rx
Ž
.
log ysx log 1q
s
.
ž
/
x
1rx
Applying l’Hospital’s rule, we get
y1
1
1
y
1q
2 ž
/
x
x
lim log ys lim
s1.
1
x™
x™
y
2
x
Therefore, lim
yse.
x™
( )
Ž .
Ž .
Ž
. x
q
4.15.
a
lim
f x s1, where f x s sin x
.
x™0
( )
Ž .
Ž .
y1r x
q
b
lim
g x s0, where g x se
rx.
x™0
Ž .
w
Ž .x1r x
Ž .
w
4.16. Let f x s 1qaxqo x
, and let ysaxqo x . Then ysx aq
Ž .x
w
Ž .x1r x
Ž
.ar yŽ
.oŽ1.r y
o 1 , and 1qaxqo x
s 1qy
1qy
. Now, as x™0, we
Ž
.ar y
a
Ž
.oŽ1.r y
have y™0, 1qy
™e , and 1qy
™1, since
o 1
Ž .
log 1qy ™0
as y™0.
Ž
.
y
Ž .
a
It follows that f x ™e .

Ž .

Ž .
Ž
.
4.17. No, because both f
x and g
x vanish at xs0.541 in 0, 1 .
Ž .
Ž .
Ž
.
4.18. Let g x sf x y xya . Then
g
 x sf 
 x y ,
Ž .
Ž .
g
 a sf 
 a y0,
Ž .
Ž .
g
 b sf 
 b y0.
Ž .
Ž .

SOLUTIONS TO SELECTED EXERCISES
574
Ž .
w
x
The function g x is continuous on a, b . Therefore, it must achieve its
w
x
absolute minimum at some point  in a, b . This point cannot be a or

Ž .

Ž .

Ž
.
b, since g a 0 and g b 0. Hence, ab, and g
 s0, so

Ž
.
f
 s.
Ž .
Ž .
Ž
.
4.19. Define g x sf x y xya , where
n

s
 f
x
.
Ž
.
Ý
i
i
is1
There exist c , c
such that
1
2
max f 
 x
sf 
 c
,
ac b,
Ž
.
Ž
.
i
2
2
i
min f 
 x
sf 
 c
,
ac b.
Ž
.
Ž
.
i
1
1
i

Ž
.

Ž
.

Ž
.
If f
x
sf
x
s  sf
x
, then the result is obviously true. Let us
1
2
n
therefore assume that these n derivatives are not all equal. In this case,
f 
 c
f 
 c
.
Ž
.
Ž
.
1
2
Apply now the result in Exercise 4.18 to conclude that there exists a

Ž .
point c between c and c
such that f
c s.
1
2
4.20. We have that
n
n

f y
yf x
s
f
c
y yx
,
Ž
.
Ž
.
Ž
. Ž
.
Ý
Ý
i
i
i
i
i
is1
is1
Ž
.
where c
is between x
and y
is1, 2, . . . , n . Using Exercise 4.19,
i
i
i
Ž
.
there exists a point c in a, b such that
Ýn
y yx
f 
 c
Ž
.
Ž
.
is1
i
i
i

sf
c .
Ž .
n
Ý
y yx
Ž
.
is1
i
i
Hence,
n
n

f y
yf x
sf
c
y yx
.
Ž
.
Ž
.
Ž .
Ž
.
Ý
Ý
i
i
i
i
is1
is1

ny1
n
Ž
.
Ž
.
4.21. log 1qx sÝ
y1
x rn,
x 1.
ns1
1
Ž .
4.23. f x has an absolute minimum at xs .
3

CHAPTER 4
575
Ž .
2
w
x
4.24. The function f x is bounded if x qaxqb0 for all x in y1, 1 . Let
 sa2y4b. If  0, then x 2qaxqb0 for all x. The denominator
has an absolute minimum at xsyar2. Thus, if y1Fyar2F1, then
Ž .
Ž .
f x
will have an absolute maximum at xsyar2. Otherwise, f x
attains its absolute maximum at xsy1 or xs1. If  s0, then
1
f x s
.
Ž .
2
a
xq
ž
/
2
w
x
In this case, the point yar2 must fall outside y1, 1 , and the absolute
Ž .
maximum of f x is attained at xsy1 or xs1. Finally, if  0, then
1
f x s
,
Ž .
xyx
xyx
Ž
. Ž
.
1
2
1
1
'
'
Ž
.
Ž
.
where x s
yay  , x s
yaq  . Both x
and x
must fall
1
2
1
2
2
2
1
w
x
Ž
.
outside y1, 1 . In this case, the point xsyar2 is equal to
x qx
,
1
2
2
w
x
Ž .
which falls outside y1, 1 . Thus f x attains its absolute maximum at
xsy1 or xs1.
Ž .
y1w
Ž
.x
4.25. Let H y
denote the cumulative distribution function of G
F Y
.
Then
y1
H y sP G
F Y
Fy
Ž .
Ž
.

4
sP F Y FG y
Ž
.
Ž .
y1
sP YFF
G y
Ž .

4
y1
sF F
G y
Ž .

4
s G y .
Ž .
Ž
.
Ž
.
yw 2
4.26. Let g w be the density function of W. Then g w s2we
, wG0.
( )
Ž
.
4.27.
a
Let g w be the density function of W. Then
1
1
2
1r3
g w s
exp y
w
y1
,
w0.
Ž
.
Ž
.
2r3'
0.08
3w
0.08
( )
Ž
.
Ž
.
b
Exact mean is E W s1.12. Exact variance is Var W s0.42.
( )
Ž
.
Ž
.
c
E w f1, Var w f0.36.

SOLUTIONS TO SELECTED EXERCISES
576
Ž .
4.28. Let G y be the cumulative distribution function of Y. Then
G y sP YFy
Ž .
Ž
.
sP Z 2Fy
Ž
.
1r2
sP
Z Fy
Ž
.
sP yy1r2 FZFy1r2
Ž
.
s2F y1r2 y1,
Ž
.
Ž .
where F 
is the cumulative distribution function of Z. Thus the
yy r2
Ž .
'
density function of Y is g y s1r
2 y e
, y0. This represents
the density function of a chi-squared distribution with one degree of
freedom.
F xqh yF x
Ž
.
Ž .
( )
4.29.
a
failure rates
.
1yF x
Ž .
( )
b
dF x rdx
Ž .
Hazard rates 1yF x
Ž .
1
1
yx r	
s
e
yx r	 ž
/
	
e
1
s
.
	
( )
c
If
dF x rdx
Ž .
sc,
1yF x
Ž .
then
ylog 1yF x
scxqc ,
Ž .
1
hence,
1yF x sc eyc x.
Ž .
2
Ž .
Since F 0 s0, c s1. Therefore,
2
F x s1yeyc x.
Ž .

CHAPTER 5
577
4.30.
r
nyr
n ny1  nyrq1
t
t
Ž
.
Ž
.
P Y sr s
1y
Ž
.
n
ž / ž
/
r!
n
n
r
yr
n
n ny1  nyrq1
t
t
t
Ž
.
Ž
. Ž
.
s
1y
1y
.
r
ž
/ ž
/
n
r!
n
n
As n™, the first r factors on the right tend to 1, the next factor
is fixed, the next tends to 1, and the last factor tends to ey t.
Hence,
r
y t
e
t
Ž
.
lim P Y sr s
.
Ž
.
n
r!
n™
CHAPTER 5
( )

4
5.1.
a
The sequence b
is monotone increasing, since
n ns1

4

4
max a , a , . . . , a
Fmax a ,a , . . . , a
.
1
2
n
1
2
nq1
It is also bounded. Therefore, it is convergent by Theorem 5.1.2. Its
limit is sup
a , since a Fb Fsup
a
for nG1.
nG1
n
n
n
nG1
n
( )
b
Let d slog a ylog c, is1, 2, . . . . Then
i
i
n
n
1
1
log c s
log a slog cq
d .
Ý
Ý
n
i
i
n
n
is1
is1
Ž
.
n
To show that 1rn Ý
d ™0 as n™: We have that d ™0 as
is1
i
i
i™. Therefore, for a given 0, there exists a positive integer
N such that
d r2 if iN . Thus, for nN ,
1
i
1
1
N
n
1
1
1

d F
d q
nyN
Ž
.
Ý
Ý
i
i
1
n
n
2n
is1
is1
N1
1


d q
.
Ý
i
n
2
is1
Furthermore, there exists a positive integer N such that
2
N1
1

d 
,
Ý
i
n
2
is1
n
Ž
.
if nN . Hence, if nmax N , N ,
1rn Ý
d , which
Ž
.
2
1
2
is1
i
Ž
.
n
implies that 1rn Ý
d ™0, that is, log c ™log c, and c ™c as
is1
i
n
n
n™.

SOLUTIONS TO SELECTED EXERCISES
578

4

4
5.2. Let a and b be the limits of
a
and
b
, respectively. These
n ns1
n ns1
limits exist because the two sequences are of the Cauchy type. To show


that d ™d, where ds ayb :
n
d yd s
a yb y ayb
n
n
n
F
a yb
y ayb
Ž
.
Ž
.
n
n
F a ya q b yb .
n
n
It is now obvious that d ™d as a ™a and b ™b.
n
n
n
5.3. Suppose that a ™c. Then, for a given 0, there exists a positive
n
integer N such that a yc  if nN. Let b sa
be the nth term
n
n
k n
of a subsequence. Since k Gn, we have
b yc  if nN. Hence,
n
n
b ™c. Vice versa, if every subsequence converges to c, then a ™c,
n
n

4
since a
is a subsequence of itself.
n ns1

4
5.4. If E is not bounded, then there exists a subsequence b
such that
n ns1
b ™, where b sa , k k   k   . This is a contradic-
n
n
k
1
2
n
n

4
tion, since a
is bounded.
n ns1
( )
5.5.
a
For a given 0, there exists a positive integer N such that
a yc  if nN. Thus, there is a positive constant M such
n
that
a yc M for all n. Now,
n
n
n
Ý
 a
1
is1
i
i yc s

a yc
Ž
.
Ý
i
i
n
n
Ý

Ý

is1
i
is1
i
is1
N
n
1
1
s

a yc
q

a yc
Ž
.
Ž
.
Ý
Ý
i
i
i
i
n
n
Ý

Ý

is1
i
is1
i
is1
isNq1
N
n
1
1
F
 a yc q
 a yc
Ý
Ý
i
i
i
i
n
n
Ý

Ý

is1
i
is1
i
is1
isNq1
N
n
M

F
 q

Ý
Ý
i
i
n
n
Ý

Ý

is1
i
is1
i
is1
isNq1
ÝN

is1
i
M
q.
n
Ý

is1
i
Hence, as n™,
n
Ý
 a
is1
i
i yc ™0.
n
Ý

is1
i

CHAPTER 5
579
( )
Ž
.n

4
Ž
.
n
b
Let a s y1 . Then, a
does not converge. But 1rn Ý
a
n
n ns1
is1
i
is equal to zero if n is even, and is equal to y1rn if n is odd.
Ž
.
n
Hence, 1rn Ý
a goes to zero as n™.
is1
i
5.6. For a given 0, there exists a positive integer N such that
anq1 yb 
an
if nN. Since b1, we can choose  so that bq1. Then
anq1 bq1,
nN.
an
Hence, for nGNq1,
a
a
bq ,
Ž
.
Nq2
Nq1
2
a
a
bq a
bq
,
Ž
.
Ž
.
Nq3
Nq2
Nq1
...
aNq1
n
nyNy1
a a
bq
s
bq
.
Ž
.
Ž
.
n
Nq1
Nq1
bq
Ž
.
Ž
. Nq1
n
Letting csa
r bq
, rsbq, we get a cr , where 0
Nq1
n
r1, for nGNq1.
5.7. We first note that for each n, a 0, which can be proved by induction.
n
Let us consider two cases.
Ž .
1.
b1. In this case, we can show that the sequence is i bounded
Ž .
from above, and ii monotone increasing:
'
'
i. The sequence is bounded from above by
b , that is, a  b :
n
a2 b is true for ns1 because a s1b. Suppose now that
n
1
a2 b; to show that a2
b:
n
nq1
2
2
2
a
3bqa
Ž
.
n
n
2
bya
sby
nq1
2
2
3a qb
Ž
.
n
3
2
bya
Ž
.
n
s
0.
2
2
3a qb
Ž
.
n

SOLUTIONS TO SELECTED EXERCISES
580
2
'
Thus a
b, and the sequence is bounded from above by
b .
nq1
Ž
ii. The sequence is monotone increasing: we have that
3bq
2. Ž
2
.
2
a r 3a qb 1, since a b, as was seen earlier. Hence,
n
n
n
a
a
for all n.
nq1
n
Ž .
By Corollary 5.1.1 1 , the sequence must be convergent. Let c be
its limit. We then have the equation
c 3bqc2
Ž
.
cs
,
2
3c qb
Ž
which results from taking the limit of both sides of a
sa 3bq
nq1
n
2. Ž
2
.
a r 3a qb
and noting that lim
a
slim
a sc. The
n
n
n™
nq1
n™
n
'
only solution to the above equation is cs b .
2.
b1. In this case, we can similarly show that the sequence is
bounded from below and is monotone decreasing. Therefore, by
'
Ž .
Corollary 5.1.1 2 it must be convergent. Its limit is equal to
b .
5.8. The sequence is bounded from above by 3, that is, a 3 for all n: This
n
is true for ns1. If it is true for n, then
1r2
1r2
a
s 2qa
 2q3
3.
Ž
.
Ž
.
nq1
n
By induction, a 3 for all n. Furthermore, the sequence is monotone
n
'
increasing:a Fa , since a s1, a s 3 . If a Fa
, then
1
2
1
2
n
nq1
1r2
1r2
a
s 2qa
G 2qa
sa
.
Ž
.
Ž
.
nq2
nq1
n
nq1
Ž .
By induction, a Fa
for all n. By Corollary 5.1.1 1 the sequence
n
nq1
must be convergent. Let c be its limit, which can be obtained by solving
the equation
1r2
cs 2qc
.
Ž
.
The only solution is cs2.
Ž
.
Ž
.
5.10. Let
m s 2n.
Then
a y a s 1r n q 1 q 1r n q 2 q  q1r2n.
m
n
Hence,
n
1
a ya 
s
.
m
n
2n
2
1
Therefore,
a ya
cannot be made less than
no matter how large
m
n
2
m and n are, if ms2n. This violates the condition for a sequence to
be Cauchy.

CHAPTER 5
581
5.11. For mn, we have that
a ya
s
a ya
q a
ya
Ž
.
Ž
.
m
n
m
my1
my1
my2
q  q a
ya
q a
ya
Ž
.
Ž
.
nq2
nq1
nq1
n
br my1 qbr my2 q  qbr n
sbr n 1qrqr 2q  qr myny1
Ž
.
br n 1yr myn
br n
Ž
.
s

.
1yr
1yr
n Ž
.
For a given 0, we choose n large enough such that br r 1yr ,
which implies that
a ya , that is, the sequence satisfies the
m
n
Cauchy criterion; hence, it is convergent.

4
5.12. If s
is bounded from above, then it must be convergent, since it is
n ns1
monotone increasing and thus Ý
a converges. Vice versa, if Ý
a
ns1
n
ns1
n

4
is convergent, then
s
is a convergent sequence; hence, it is
n ns1
bounded, by Theorem 5.1.1.
5.13. Let s
be the nth partial sum of the series. Then
n
n
1
1
1
s s
y
Ý
n
ž
/
3
3iy1
3iq2
is1
1
1
1
1
1
1
1
s
y
q
y
q  q
y
ž
/
3
2
5
5
8
3ny1
3nq2
1
1
1
s
y
ž
/
3
2
3nq2
1
™
as n™.
6
5.14. For n2, the binomial theorem gives
n
1
1
1
1
n
n
1q
s1q
q
q  q
n
2
ž /
ž /
ž
/
1
2
n
n
n
n
1
1
1
1
2
1
s2q
1y
q
1y
1y
q  q
n
ž
/
ž
/ž
/
2!
n
3!
n
n
n
n
1
2q Ý i!
is2
n
1
2q Ý
iy1
2
is2
3
n.

SOLUTIONS TO SELECTED EXERCISES
582
Hence,
1
1r n
1q
n
,
ž
/
n
1
p
1r n
n
y1

.
Ž
.
p
n

Ž
1r n
. p
Therefore, the series Ý
n
y1
is divergent by the comparison
ns1
test, since Ý
1rn p is divergent for pF1.
ns1
( )
5.15.
a
Suppose that a M for all n, where M is a positive constant.
n
Then
a
a
n
n

.
1qa
1qM
n

Ž
.
The series Ý
a r 1qa
is divergent by the comparison test,
ns1
n
n
since Ý
a
is divergent.
ns1
n
( )
b
We have that
a
1
n
s1y
.
1qa
1qa
n
n
 4
If a
is not bounded, then
ns1
an
lim a s,
hence,
lim
s10.
n
1qa
n™
n™
n

Ž
.
Therefore, Ý
a r 1qa
is divergent.
ns1
n
n

4
5.16. The sequence
s
is monotone increasing. Hence, for ns2, 3, . . . ,
n ns1
a
a
s ys
1
1
n
n
n
ny1
F
s
s
y
,
2
s s
s s
s
s
s
n
ny1
n
ny1
ny1
n
n
n
n
a
a
a
i
1
i
s
q
Ý
Ý
2
2
2
s
s
s
i
1
i
is1
is2
a
1
1
1
1
1
F
q
y
q  q
y
2 ž
/
ž
/
s
s
s
s
s
1
2
ny1
n
1
a
1
1
a
1
1
1
s
q
y
™
q
,
2
2
s
s
s
s
s
1
n
1
1
1
since s ™ by the divergence of Ý
a . It follows that Ý
a rs2 is
n
ns1
n
ns1
n
n
a convergent series.

CHAPTER 5
583
5.17. Let A denote the sum of the series Ý
a . Then r sAys
, where
ns1
n
n
ny1
n

4
s sÝ
a . The sequence r
is monotone decreasing. Hence,
n
is1
i
n ns2
n
a
a qa
q  qa
r yr
i
m
mq1
ny1
m
n

s
Ý r
r
r
i
m
m
ism
rn
s1y
.
rm
Since r ™0, we have 1yr rr ™1 as n™. Therefore, for 01,
n
n
m
there exists a positive integer k such that
a
a
a
m
mq1
mqk
q
q  q
.
r
r
r
m
mq1
mqk
This implies that the series Ý
a rr
does not satisfy the Cauchy
ns1
n
n
criterion. Hence, it is divergent.
Ž
.
Ž
.
Ž
.
Ž
.1r n
5.18.
1rn log 1rn sy log n rn™0 as n™. Hence, 1rn
™1. Simi-
Ž
.
Ž
2.
Ž
.
Ž
2.1r n
larly, 1rn log 1rn
sy 2 logn rn™0, which implies 1rn
™1.
( )
1r n
1r n

5.19.
a
a
sn
y1™01 as n™. Therefore, Ý
a is convergent
n
ns1
n
by the root test.
( )
w
Ž
.x
n2
w
Ž
.x
2
b
a  log 1qn rlog e
s log 1qn rn . Since
n



log 1qx
1
dx
Ž
.
dxsy
log 1qx
q
Ž
.
H
H
2
x
x xq1
x
Ž
.
1
1
1

x
slog2qlogž
/
xq1
1
s2 log2,

w
Ž
.x
2
the series Ý
log 1qn rn
is convergent by the integral test.
ns1
Hence, Ý
a converges by the comparison test.
ns1
n
( )
Ž
.Ž
. Ž
.2
Ž
.
c
a ra
s 2nq2 2nq3 r 2nq1
´ lim
n
a ra
y1
n
nq1
n™
n
nq1
3

s 1. The series Ý
a
is convergent by Raabe’s test.
ns1
n
2
( )
d
'
'
'
a s
nq2 n y n
n
'
nq2 n yn
s
™1
as n™.
'
'
'nq2 n q n
Therefore, Ý
a
is divergent.
ns1
n

SOLUTIONS TO SELECTED EXERCISES
584
( ) 
 1r n
e
a
s4rn™01. The series is absolutely covergent by the
n
root test.
( )
Ž
.n
Ž
.
Ž
.
f
a s y1
sin rn . For large
n, sin rn rn. Therefore,
n
Ý
a
is conditionally convergent.
ns1
n
( )
5.20.
a
Applying the ratio test, we have the condition
anq1
2
 x
lim
1,
a
n™
n
which is equivalent to
  2
x
'
 
1,
that is,
x  3 .
3
'
 
The series is divergent if
x s 3 . Hence, it is uniformly conver-
'
w
x
gent on yr, r where r 3 .
( )
b
Using the ratio test, we get
anq1
x lim
1,
a
n™
n
where a s10 nrn. This is equivalent to
n
10 x 1.
1
 
The series is divergent if x s
. Hence, it is uniformly convergent
10
1
w
x
on yr, r where r
.
10
( )
w
x
c
The series is uniformly convergent on yr, r where r1.
( )
d
The series is uniformly convergent everywhere by Weierstrass’s
M-test, since
cos nx
1
F
for all x,
2
2
n n q1
n n q1
Ž
.
Ž
.
Ž
2
.
and the series whose nth term is 1rn n q1 is convergent.
5.21. The series Ý
a
is convergent by Theorem 5.2.14. Let s denote its
ns1
n
sum:
1
1
1
1
1
1
ss1y q y
y
y
y
y 
Ž
.
Ž
.
2
3
4
5
6
7
1
1
5
10
1y q s s
.
2
3
6
12

CHAPTER 5
585
Now,

1
1
1
1
1
b s 1q y
q
q y
q 
Ž
.
Ž
.
Ý
n
3
2
5
7
4
ns1
1
1
1
q
q
y
q  .
ž
/
4ny3
4ny1
2n
Let s
denote the sum of the first 3n terms of Ý
b . Then,
3n
ns1
n
s
 ss
qu ,
3n
2 n
n
where s
is the sum of the first 2n terms of Ý
a , and
2 n
ns1
n
1
1
1
u s
q
q  q
,
ns1, 2, . . .
n
2nq1
2nq3
4ny1

4
The sequence
u
is monotone increasing and is bounded from
n ns1
1
above by
, since
2
n
u 
,
ns1, 2, . . .
n
2nq1
Ž
.
 
 4
the number of terms that make up u
is equal to n . Thus s
is
n
3n ns1
convergent, which implies convergence of Ý
b . Let t denote the
ns1
n
sum of this series. Note that
1
1
1
1
1
11
t 1q y
q
q y
s
,
Ž
.
Ž
.
3
2
5
7
4
12
Ž
.
Ž
.
since 1r 4ny3 q1r 4ny1 y1r2n0 for all n.
5.22. Let c denote the nth term of Cauchy’s product. Then
n
n
c s
a a
Ý
n
k
nyk
ks0
n
1
n
s y1
.
Ž
. Ý
1r2
nykq1
kq1
Ž
. Ž
.
ks0

SOLUTIONS TO SELECTED EXERCISES
586
Since nykq1Fnq1 and kq1Fnq1,
n
1
c
G
s1.
Ý
n
nq1
ks0
Hence, c
does not go to zero as n™. Therefore, Ý
c
is diver-
n
ns0
n
gent.
Ž .
Ž .
5.23. f
x ™f x , where
n
0,
xs0,
f x s
Ž . ½ 1rx,
x0.
w
.
Ž .
The convergence is not uniform on 0,  , since f
x is continuous on
n
w
.
Ž .
0,  for all n, but f x is discontinuous at xs0.
( )
x
1q

1q
5.24.
a
1rn F 1rn
.
Since
Ý
1rn
is
a
convergent
series,
ns1

x
w
.
Ý
1rn
is uniformly convergent on 1q, 
by Weierstrass’s
ns1
M-test.
( )
Ž
x.
Ž
.
x

Ž
.
x
b
drdx 1rn
sy log n rn . The series Ý
log n rn
is uni-
ns1
w
.
formly convergent on 1q,  :
log n
1
log n
F
,
xGq1.
x
1q r2
 r2
n
n
n
Ž
.
r2
Since log n rn
™0 as n™, there exists a positive integer N
such that
log n
1
if nN.
r2
n
Therefore,
log n
1

x
1q r2
n
n
if nN and xGq1. But, Ý
1rn1q r2 is convergent. Hence,
ns1

Ž
.
x
w
.
Ý
log n rn
is uniformly convergent on
1q, 
by Weier-
ns1
Ž .

x
strass’s M-test. If  x
denotes the sum of the series Ý
1rn ,
ns1
then

log n


x sy
,
xGq1.
Ž .
Ý
x
n
ns1

CHAPTER 5
587
5.25. We have that

1
nqky1
n
x s
,
ks1,2, . . . ,
Ý
k
ž
/
n
1yx
Ž
.
ns0
Ž
.
if y1x1 see Example 5.4.4 . Differentiating this series term by
term, we get

k
nqky1
ny1
nx
s
.
Ý
kq1
ž
/
n
1yx
Ž
.
ns1
It follows that

r 1yp
Ž
.
n
nqry1
r
r
n
p
1yp
sp
Ž
.
Ý
rq1
ž
/
r
p
ns0
r 1yp
Ž
.
s
.
p
Taking now second derivatives, we obtain

k kq1
Ž
.
nqky1
ny2
n ny1 x
s
.
Ž
.
Ý
kq2
ž
/
n
1yx
Ž
.
ns2
From this we conclude that

kx 1qkx
Ž
.
nqky1
2
n
n
x s
.
Ý
kq2
ž
/
n
1yx
Ž
.
ns1
Hence,
r

p
1yp r 1qr 1yp
Ž
.
Ž
.
n
nqry1
2
r
n
p
1yp
s
Ž
.
Ý
rq2
ž
/
n
p
ns0
r 1yp
1qryrp
Ž
. Ž
.
s
.
2
p
5.26.

nqry1
nt
r
n
 t s
e
p q
Ž .
Ý ž
/
n
ns0

n
nqry1
r
t
sp
qe
Ž
.
Ý ž
/
n
ns0
pr
t
s
,
qe 1.
t
1yqe

SOLUTIONS TO SELECTED EXERCISES
588
Ž
x
The series converges uniformly on
y, s
where sylog q. Yes,
Ž
.
Ž .
formula 5.63 can be applied, since there exists a neighborhood N 0

contained inside the interval of convergence by the fact that ylog q0.

Ž

.
n
5.28. It is sufficient to show that the series Ý
 rn! 
is absolutely
ns1
n
Ž
.
convergent for some 0 see Theorem 5.6.1 . Using the root test,
1rn



n
s lim sup
1rn
n™
n!
Ž
.
1rn

n
s e lim sup
1r2 n
1q1r2 n
n™
2
n
Ž
.
1rn

n
s e lim sup
.
n
n™
Let
1rn

n
ms lim sup
.
n
n™

Ž

.
n
If m0 and finite, then the series Ý
 rn! 
is absolutely
ns1
n
Ž
.
convergent if 1, that is, if 1r em .
( )
5.30.
a
n
n
nyk
k
E X
s
k
p
1yp
Ž
.
Ž
.
Ý
n
ž /
k
ks0
n
n
nyk
ky1
sp
k
p
1yp
Ž
.
Ý ž /
k
ks1
ny1
n
nyky1
k
sp
kq1
p
1yp
Ž
.
Ž
.
Ý
ž
/
kq1
ks0
ny1
n!
nyky1
k
sp
p
1yp
Ž
.
Ý k! nyky1 !
Ž
.
ks0
ny1
snp pq1yp
Ž
.
snp.
We can similarly show that
E X 2 snp 1yp qn2p2.
Ž
.
Ž
.
n

CHAPTER 5
589
Hence,
Var X
sE X 2 yn2p2
Ž
.
Ž
.
n
n
Var X
snp 1yp .
Ž
.
Ž
.
n
2
2
( )
Ž
.
Ž
b
We have that P
Y
Gr	 F1rr . Let r	s, where 	 sp 1y
n
.
p rn. Then
p 1yp
Ž
.
P
Y
G F
.
Ž
.
n
2
n
( )
Ž
.
c
As n™, P
Y
G ™0, which implies that Y
converges in
n
n
probability to zero.
( )
5.31.
a
n
t

t s p e qq
,
q s1yp
Ž .
Ž
.
n
n
n
n
n
n
t
s 1qp
e y1
.
Ž
.
n
Let np sr . Then
n
n
n
rn
t

t s 1q
e y1
Ž .
Ž
.
n
n
t
™exp  e y1
Ž
.
Ž .
as n™. The limit of 
t is the moment generating function of a
n
Poisson distribution with mean . Thus S
has a limiting Poisson
n
distribution with mean .
5.32. We have that
2
2
3
Qs I yH qkH yk H qk H y 
Ž
.
n
1
2
3
4
s I yH
qk 2H y2k 3H q  .
Ž
.
n
1
3
4
Thus
y
Qysy
 I yH
yqk 2 y
H yy2k 3y
H yq 
Ž
.
n
1
3
4
sSS qk 2S y2k 3S q 
E
3
4

iy1
sSS q
iy2
yk
S .
Ž
. Ž
.
Ý
E
i
is3

SOLUTIONS TO SELECTED EXERCISES
590
CHAPTER 6
Ž .
Ž
.
6.1. If f x is Riemann integrable, then inequality 6.1 is satisfied; hence,
Ž
.
equality
6.6
is true, as was shown to be the case in the proof of
Ž
.
Theorem 6.2.1. Vice versa, if equality
6.6
is satisfied, then by the
Ž
.
Ž
.
double inequality
6.2 , S P, f
must have a limit as  ™0, which
p
Ž .
w
x
implies that f x is Riemann integrable on a, b .
1
Ž .
w
x
Ž .
6.2. Consider the function f x
on 0, 1 such that f x s0 if 0Fx ,
2
Ž .
Ž .
n
i
Ž
. Ž
.
f x s2
if
xs1,
and
f x sÝ
1r2
if
nq1 r nq2 Fx
is0
Ž
. Ž
.
nq2 r nq3 for ns0, 1, 2, . . . . This function has a countable num-
1
2
3
w
x
ber of discontinuities at
, , , . . . , but is Riemann integrable on 0, 1 ,
2
3
4
Ž
.
since it is monotone increasing see Theorem 6.3.2 .
Ž .
6.3. Suppose that f x
has one discontinuity of the first kind at xsc,
Ž .
Ž .
y
q
acb. Let lim
f x sL , lim
f x sL , L L . In any
x™c
1
x™c
2
1
2
w
x
partition P of a, b , the point c appears in at most two subintervals.
Ž .
Ž .
The contribution to US
f yLS
f
from these intervals is less than
P
P
Ž
.
Ž .
2 Mym  , where M, m are the supremum and infimum of f x on
p
w
x
a, b , and this can be made as small as we please. Furthermore,
Ý M  x yÝ m  x for the remaining subintervals can also be made
i
i
i
i
i
i
Ž .
Ž .
as small as we please. Hence, US
f yLS
f
can be made smaller
P
P
Ž .
than  for any given 0. Therefore, f x is Riemann integrable on
w
x
Ž .
a, b . A similar argument can be used if f x
has a finite number of
w
x
discontinuities of the first kind in a, b .
w
x

Ž
.
6.4. Consider the following partition of
0, 1 : Ps 0, 1r2n, 1r 2ny1 ,
4
. . . , 1r3, 1r2, 1 . The number of partition points is 2nq1 including 0
and 1. In this case,
2n
1
 f
s
cos  ny0
Ý
i
2n
is1
1

1
q
cos
2ny1
y
cos  n
Ž
.
2ny1
2
2n
1
1

q
cos  ny1
y
cos
2ny1
q 
Ž
.
Ž
.
2ny2
2ny1
2
1
1
3

1
q
cos y
cos
q cos
y
cos 
ž /
ž /
2
3
2
2
2
1
1
1
s
q
q
q  q1.
n
ny1
ny2

CHAPTER 6
591
2 n

Ž .
As n™, Ý
 f ™, since Ý
1rn is divergent. Hence, f x
is
is1
i
ns1
w
x
not of bounded variation on 0, 1 .
( )
6.5.
a
For a given 0, there exists a constant M0 such that

f
x
Ž .
yL 

g
x
Ž .

Ž .
if xM. Since g
x 0,



f
x yLg
x
 g
x .
Ž .
Ž .
Ž .
Hence, if  and 
are chosen larger than M, then
1
2


2
2




f
x yLg
x
dx F
f
x yLg
x
dx
Ž .
Ž .
Ž .
Ž .
H
H


1
1
2 

g
x
dx.
Ž .
H
1
( )
Ž .
b
From a we get
f 
yLg 
yf 
qLg 
 g 
yg 
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
2
1
1
2
1
Ž
. w
Divide both sides by g 
which is positive for large  , since
2
2
Ž .
x
g x ™ as x™ , we obtain
f 
f 
g 
g 
Ž
.
Ž
.
Ž
.
Ž
.
2
1
1
1
yLy
qL

1y
g 
g 
g 
g 
Ž
.
Ž
.
Ž
.
Ž
.
2
2
2
2
.
Hence,
f 
f 
g 
Ž
.
Ž
.
Ž
.
2
1
1
yL q
q L
.
g 
g 
g 
Ž
.
Ž
.
Ž
.
2
2
2
( )
c
For sufficiently large  , the second and third terms on the
2
right-hand side of the above inequality can each be made smaller
than ; hence,
f 
Ž
.
2
yL 3.
g 
Ž
.
2
6.6. We have that
m g x Ff x g x FM g x ,
Ž .
Ž . Ž .
Ž .

SOLUTIONS TO SELECTED EXERCISES
592
Ž .
w
x
where m and M are the infimum and supremum of f x
on
a, b ,
w
x
Ž
.
respectively. Let  and  be points in
a, b
such that msf  ,
Ž .
Msf  . We conclude that
bf x g x
dx
Ž . Ž .
H
a
f 
F
Ff  .
Ž
.
Ž .
bg x
dx
Ž .
H
a
Ž
.
By the intermediate-value theorem
Theorem 3.4.4 , there exists a
constant c, between  and , such that
bf x g x
dx
Ž . Ž .
H
a
sf c .
Ž .
bg x
dx
Ž .
H
a
Note that c can be equal to  or  in case equality is attained at the
lower end or the upper end of the above double inequality.
6.9. Integration by parts gives
b
b
f x
dg x sf b g b yf a g a q
g x
d yf x
.
Ž .
Ž .
Ž . Ž .
Ž . Ž .
Ž .
Ž .
H
H
a
a
Ž .
Ž . Ž .
Since g x
is bounded, f b g b ™0 as b™. Let us now establish
b Ž . w
Ž .x
convergence of H g x d yf x
as b™: let M0 be such that
a
 Ž .
g x
FM for all xGa. In addition,
bM d yf x
sMf a yMf b
Ž .
Ž .
Ž .
H
a
™Mf a
as b™.
Ž .

w
Ž .x
Ž .
Hence, H M d yf x
is a convergent integral. Since yf x is monotone
a
w
.
increasing on a,  , then


g x
d yf x
F
M d yf x
.
Ž .
Ž .
Ž .
H
H
a
a

Ž . w
Ž .x
This implies absolute convergence of H g x d yf x . It follows that
a
 Ž .
Ž .
the integral H f x dg x is convergent.
a

CHAPTER 6
593
6.10. Let n be a positive integer. Then
n

sin x
sin x
sin x
2
dxs
dxy
dx
H
H
H
x
x
x
0
0

n
sin x
ny1
q  q y1
dx
Ž
.
H
x
Ž
.
ny1 

1
1
1
s
sin x
q
q  q
dx
H
x
xq
xq ny1 
Ž
.
0

1
1
1

q
q  q
sin x dx
H
ž
/

2
n
0
2
1
1
s
1q
q  q
™
as n™.
ž
/

2
n

'
( )
Ž
. Ž
.
6.11.
a
This is convergent by the integral test, since H
log x r x x dxs
1
4Heyx x dxs4 by a proper change of variable.
0
( ) Ž
. Ž
3
.
2

2
b
nq4 r 2n q1 1r2n . But Ý
1rn
is convergent by the
ns1
integral test, since H dxrx2s1. Hence, this series is convergent by
1
the comparison test.

'
'
'
( )
Ž
.
c
1r
nq1 y1 1r n . By the integral test, Ý
1r n is diver-
ns1



'
'
'
Ž
.
gent, since H dxr x s2 x
s. Hence, Ý
1r
nq1 y1 is
1
ns1
1
divergent.
( )
my1Ž
.ny1
my1
1
my1
6.13.
a
Near xs0 we have x
1yx
x
, and H
x
dxs1rm.
0
my1Ž
.ny1
Ž
.ny1
Also,
near
x s 1
we
have
x
1 y x
 1 y x
,
and
1Ž
.ny1
Ž
.
H 1yx
dxs1rn. Hence, B m, n converges if m0, n0.
0 '
( )
b
Let
x ssin . Then

1
ny1
2
my1
2 my1
2 ny1
x
1yx
dxs2
sin
cos
 d.
Ž
.
H
H
0
0
( )
Ž
.
c
Let xs1r 1qy . Then

ny1
y
1
ny1
my1
x
1yx
dxs
dy.
Ž
.
H
H
mqn
1qy
Ž
.
0
0
Letting zs1yx, we get
1
0
ny1
my1
my1
ny1
x
1yx
dxsy
1yz
z
dz
Ž
.
Ž
.
H
H
0
1
1
my1
ny1
s
x
1yx
dx.
Ž
.
H
0

SOLUTIONS TO SELECTED EXERCISES
594
Ž
.
Ž
.
Hence, B m, n sB n, m . It follows that

my1
x
B m, n s
dx.
Ž
. H
mqn
1qx
Ž
.
0
( )
d

ny1
x
B m, n s
dx
Ž
. H
mqn
1qx
Ž
.
0
ny1

ny1
x
x
1
s
dxq
dx.
H
H
mqn
mqn
1qx
1qx
Ž
.
Ž
.
0
1
Let ys1rx in the second integral. We get

ny1
my1
x
y
1
dxs
dy
H
H
mqn
mqn
1qx
1qy
Ž
.
Ž
.
1
0
Therefore,
x ny1
x my1
1
1
B m, n s
dxq
dx
Ž
. H
H
mqn
mqn
1qx
1qx
Ž
.
Ž
.
0
0
x ny1 qx my1
1
s
dx.
H
mqn
1qx
Ž
.
0
( )
6.14.
a


dx
dx
dx
1
s
q
.
H
H
H
3
3
3
'
'
'
0
0
1
1qx
1qx
1qx
The first integral exists because the integrand is continuous. The
second integral is convergent because


dx
dx

s2.
H
H
3r2
3
'
x
1
1
1qx

3
'
Hence, H dxr 1qx
is convergent.
0
( )
Ž
3.1r3
Ž
.
b
Divergent, since for large x we have 1r 1qx
1r 1qx , and

Ž
.
H dxr 1qx s.
0
( )
Ž .
Ž
3.1r3
Ž .
Ž
.1r3
c
Convergent, since if f x s1r 1yx
and g x s1r 1yx
,
then
1r3
f x
1
Ž .
lim
s
.
ž /
y g x
3
Ž .
x™1
3
1
1
Ž .
Ž .
But H g x dxs . Hence, H f x dx is convergent.
0
0
2

CHAPTER 6
595
( )
d
Partition the integral as the sum of

dx
dx
1
and
.
H
H
'
'
x 1qx
x 1q2 x
Ž
.
Ž
.
0
1
The
first
integral
is
convergent,
since
near
xs0 we
have
1
'
'
'
w
Ž
.x
1r
x 1qx 1r x , and H dxr x s2. The second integral is
0
3r2
'
w
Ž
.x
also convergent, since as x™ we have 1r
x 1q2 x 1r2 x
,
and H dxrx3r2 s2.
1
6.15. The proof of this result is similar to the proof of Corollary 6.4.2.
6.16. It is is easy to show that
ny1
byx
Ž
.

Žn.
h
x sy
f
x .
Ž .
Ž .
n
ny1 !
Ž
.
Hence,
b 
h
a sh
b y
h
x
dx
Ž .
Ž .
Ž .
H
n
n
n
a
1
b
ny1
Žn.
s
byx
f
x
dx,
Ž
.
Ž .
H
ny1 !
Ž
.
a
Ž .
since h
b s0. It follows that
n
ny1
bya
Ž
.

Žny1.
f b sf a q bya
f
a q  q
f
a
Ž .
Ž .
Ž
.
Ž .
Ž .
ny1 !
Ž
.
1
b
ny1
Žn.
q
byx
f
x
dx.
Ž
.
Ž .
H
ny1 !
Ž
.
a
Ž .
x Ž .
Ž .

Ž .
6.17. Let G x sH g t dt. Then, G x is uniformly continuous and G x s
a
Ž .
g x by Theorem 6.4.8. Thus
b
b
b

f x g x
dxsf x G x
y
G x f
x
dx
Ž . Ž .
Ž .
Ž .
Ž .
Ž .
H
H
a
a
a
b

sf b G b y
G x f
x
dx.
Ž .
Ž .
Ž .
Ž .
H
a
Ž .
Now, since G x is continuous, then by Corollary 3.4.1,
G 
FG x FG 
Ž
.
Ž .
Ž .

SOLUTIONS TO SELECTED EXERCISES
596
w
x
w
x
Ž .
for all x in a, b , where ,  are points in a, b at which G x achieves
w
x
its infimum and supremum in
a, b , respectively. Furthermore, since
Ž .
Ž
.

Ž .
f x
is monotone
say monotone increasing
and f
x
exists, then

Ž .
f
x G0 by Theorem 4.2.3. It follows that
b
b
b



G 
f
x
dxF
G x f
x
dxFG 
f
x
dx.
Ž
.
Ž .
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
This implies that
b
b


G x f
x
dxs
f
x
dx,
Ž .
Ž .
Ž .
H
H
a
a
Ž
.
Ž .
where G  FFG  . By Theorem 3.4.4, there exists a point c
Ž .
between  and  such that sG c . Hence,
b
b


G x f
x
dxsG c
f
x
dx
Ž .
Ž .
Ž .
Ž .
H
H
a
a
c
s f b yf a
g x
dx.
Ž .
Ž .
Ž .
H
a
Consequently,
b
b

f x g x
dxsf b G b y
G x f
x
dx
Ž . Ž .
Ž .
Ž .
Ž .
Ž .
H
H
a
a
c
b
sf b
g x
dxy f b yf a
g x
dx
Ž .
Ž .
Ž .
Ž .
Ž .
H
H
a
a
c
b
sf a
g x
dxqf b
g x
dx
Ž .
Ž .
Ž .
Ž .
H
H
a
c
6.18. This follows directly from applying the result in Exercise 6.17 and
Ž .
Ž .
letting f x s1rx, g x ssin x.
c
sin x
1
1
b
b
dxs
sin x dxq
sin x dx
H
H
H
x
a
b
a
a
c
1
1
s
cos aycos c q
cos cycos b .
Ž
.
Ž
.
a
b
Therefore,
sin x
1
1
b
dx F
cos aycos c q
cos cycos b
H
x
a
a
a
4
F
.
a

CHAPTER 6
597
6.21. Let 0 be given. Choose 0 such that
 g b yg a
.
Ž .
Ž .
Ž .
w
x Ž
.
Since f x is uniformly continuous on a, b
see Theorem 3.4.6 , there
exists a 0 such that
f x yf z

Ž .
Ž .
w
x
if
xyz  for all x, z in a, b . Let us now choose a partition P of
w
x
a, b whose norm  
is smaller than . Then,
p
n
US
f, g yLS
f, g s
M ym
 g
Ž
.
Ž
.
Ž
.
Ý
P
P
i
i
i
is1
n
F
 g
Ý
i
is1
s g b yg a
Ž .
Ž .
.
Ž .
w
x
It follows that f x is RiemannStieltjes integrable on a, b .
Ž .
6.22. Since g x
is continuous, for any positive integer n we can choose a
partition P such that
g b yg a
Ž .
Ž .
 g s
,
is1, 2, . . . , n.
i
n
Ž .
Ž
.
Ž
.
Also, since f x is monotone increasing, then M sf x , m sf x
,
i
i
i
iy1
is1, 2, . . . , n. Hence,
n
g b yg a
Ž .
Ž .
US
f, g yLS
f, g s
f x
yf x
Ž
.
Ž
.
Ž
.
Ž
.
Ý
P
P
i
iy1
n
is1
g b yg a
Ž .
Ž .
s
f b yf a
.
Ž .
Ž .
n
This can be made less than , for any given 0, if n is chosen large
Ž .
enough. It follows that f x
is RiemannStieltjes integrable with re-
Ž .
spect to g x .
Ž .
k Ž
2.
Ž .
ky2
Ž .
Ž .
6.23. Let f x sx r 1qx
, g x sx
. Then lim
f x rg x s1. The
x™


ky2

ky2
ky1
integral H
x
dx is divergent, since H
x
dxsx
r ky1
s
Ž
.
0
0
0
if k1. If ks1, then H x ky2 dxsH dxrxs. By Theorem 6.5.3,
0
0
 Ž .
H f x dx must be divergent if kG1. A similar procedure can be used
0
0
Ž .
to show that H
f x dx is divergent.
y

SOLUTIONS TO SELECTED EXERCISES
598
Ž
.
Ž
.
Ž
2.
6.24. Since E X s0, then Var X sE X
, which is given by

2
x
x e
2
E X
s
dx.
Ž
. H
2
x
y 1qe
Ž
.
x Ž
x.
Let use r 1qe
. The integral becomes
2
u
1
2
E X
s
log
du
Ž
. H
ž
/
1yu
0
1
u
s log
u log uq 1yu log 1yu
Ž
.
Ž
.
½
5
ž
/
1yu
0
u log uq 1yu log 1yu
Ž
.
Ž
.
1
y
du
H
u 1yu
Ž
.
0
log u
log 1yu
Ž
.
1
1
sy
duy
du
H
H
1yu
u
0
0
log u
1
sy2
du.
H 1yu
0
But
logu
1
1
2
n
dus
1ququ q  qu q  log u du,
Ž
.
H
H
1yu
0
0
and
1
1
1
1
1
n
nq1
n
u log u dus
u
log u y
u du
H
H
nq1
nq1
0
0
0
1
sy
.
2
nq1
Ž
.
Hence,

2
log u
1

1
dusy
sy
,
Ý
H
2
1yu
6
0
nq1
Ž
.
ns0
Ž
2.
2
and E X
s r3.
Ž .
Ž .

Ž .

6.26. Let g x sf x rx , h x s1rx . We have that
g x
Ž .
lim
sf 0 .
Ž .
q h x
Ž .
x™0

CHAPTER 6
599

Ž .
1y Ž
.
Since H h x dxs
r 1y  for any 0, by Theorem 6.5.7,
0
 Ž .

H f x rx
dx exists. Furthermore,
0


f x
1
1
Ž .
dxF
f x
dxF
.
Ž .
H
H



x




Hence,
 f x
Ž .
y
E X
s
dx
Ž
. H

x
0
exists.
Ž .
Ž .
1q
Ž .
6.27. Let g x sf x rx
, h x s1rx. Then
g x
Ž .
lim
sk.
q h x
Ž .
x™0
 Ž .
 Ž .
Since H h x dx is divergent for any 0, then so is H g x dx by
0
0
Theorem 6.5.7, and hence


yŽ1q .
w
x
E X
s
g x
dxq
g x
dxs.
Ž .
Ž .
H
H
0

Ž
.
6.28. Applying formula 6.84 , the density function of W is given by
nq1
Ž
.
y nq1 r2
2!
2
ž
/
w
2
g w s
1q
,
wG0.
Ž
.
n ž
/
n
'n !ž /
2
( )
6.29.
a
From Theorem 6.9.2, we have
1
P
Xy Gu	
F
.
Ž
.
2
u
Letting u	sr, we get
	 2
P
Xy Gr F
.
Ž
.
2r
( )
Ž .
b
This follows from a and the fact that
	 2
E X
s,
Var X
s
.
Ž
.
Ž
.
n
n
n

SOLUTIONS TO SELECTED EXERCISES
600
( )
c
2
	
P
X y G F
.
Ž
.
n
2
n
Ž
.
Hence, as n™, P
X y G ™0.
n
( )
6.30.
a
Using the hint, for any u and ®0 we have
u2
q2u® q®2
G0.
ky1
k
kq1
Since 
G0 for kG1, we must therefore have
ky1
®2 2y®2

F0,
kG1,
k
ky1
kq1
that is,
 2F

,
ks1, 2, . . . , ny1.
k
ky1
kq1
( )
2
1r2
b
 s1. For ks1,  F . Hence,  F
. Let us now use
0
1
2
1
2
mathematical induction to show that  1r nF 1rŽ nq1. for all n for
n
nq1
which 
and 
exist. The statement is true for ns1. Suppose
n
nq1
that
 1rŽ ny1. F 1r n.
ny1
n
To show that  1r nF 1rŽ nq1.: We have that
n
nq1
 2F

n
ny1
nq1
F Žny1. r n
.
n
nq1
Thus,
 Žnq1. r nF
.
n
nq1
Hence,
 1r nF 1rŽ nq1. .
n
nq1
CHAPTER 7
( )
Ž
.
7.1.
a
Along x s0, we have f 0, x
s0 for all x , and the limit is zero
1
2
2
as x™0. Any line through the origin can be represented by the
equation x st x . Along this line, we have
1
2
t x
t
x
2
2
f t x , x
s
exp y
Ž
.
2
2
2
2
ž
/
x
x
2
2
t
t
s
exp y
,
x 0.
2
ž
/
x
x
2
2
Ž
.
Ž
.
Using l’Hospital’s rule Theorem 4.2.6 , f t x , x
™0 as x ™0.
2
2
2

CHAPTER 7
601
( )
2
b
Along the parabola x sx , we have
1
2
x 2
x 2
2
2
f x , x
s
exp y
Ž
.
1
2
2
2
ž
/
x
x
2
2
sey1,
x 0,
2
Ž
.
which does not go to zero as x ™0. By Definition 7.2.1, f x , x
2
1
2
does not have a limit as x™0.
( )
7.5.
a
This function has no limit as x™0, as was shown in Example 7.2.2.
Hence, it is not continuous at the origin.
( )
b
" f x , x
1
0 x
Ž
.
1
2
1
s
lim
y0
s0,
2
½
5
" x
 x
 x ™0
 x
q0
Ž
.
1
1
1
xs0
1
" f x , x
1
0 x
Ž
.
1
2
2
s
lim
y0
s0.
2
½
5
" x
 x
 x ™0
0q  x
Ž
.
2
2
2
xs0
2
Ž
.
7.6. Applying formula 7.12 , we get
k
df
" f u
Ž .
ny1
s
x
snt
f x ,
Ž .
Ý
i
dt
" ui
is1
where u is the ith element of ustx, is1, 2, . . . , k. But, on one hand,
i
k
" f u
" f u
" u
Ž .
Ž .
j
s Ý
" x
" u
" x
i
j
i
js1
" f u
Ž .
st
,
" ui
and on the other hand,
" f u
" f x
Ž .
Ž .
n
st
.
" x
" x
i
i
Hence,
" f u
" f x
Ž .
Ž .
ny1
st
.
" u
" x
i
i
It follows that
k
" f u
Ž .
ny1
x
snt
f x
Ž .
Ý
i
" ui
is1

SOLUTIONS TO SELECTED EXERCISES
602
can be written as
k
" f x
Ž .
ny1
ny1
t
x
snt
f x ,
Ž .
Ý
i
" xi
is1
that is,
k
" f x
Ž .
x
snf x .
Ž .
Ý
i
" xi
is1
( )
Ž
.
7.7.
a
f x , x
is not continuous at the origin, since along x s0 and
1
2
1
Ž
.
x 0, f x , x
s0, which has a limit equal to zero as x ™0. But,
2
1
2
2
along the parabola x sx 2,
2
1
x 4
1
1
f x , x
s
s
if x 0.
Ž
.
1
2
1
4
4
2
x qx
1
1
1
2
Ž
.
Hence, lim
f x , x
s 0.
x ™0
1
1
2
1
( )
b
The directional derivative in the direction of the unit vector vs
Ž
.
® , ®
at the origin is given by
1
2
" f x
" f x
Ž .
Ž .
®
q®
.
1
2
" x
" x
1
2
xs0
xs0
This derivative is equal to zero, since
2
" f x
1
 x
0
Ž .
Ž
.
1
s
lim
y0 s0,
4
" x
 x
 x ™0
 x
q0
Ž
.
1
1
1
xs0
1
" f x
1
0 x
Ž .
2
s
lim
y0 s0.
2
" x
 x
 x ™0
0q  x
Ž
.
2
2
2
xs0
2
7.8. The directional derivative of f at a point x on C in the direction of v is
k
Ž .
Ý
® " f x r" x . But v is given by
is1 i
i
dx
dx
vs dt
dt
1r2
2
k
dx
dg
tŽ .
i
s
Ý½
5
dt
dt
is1
dx
dg
s
,
dt
dt

CHAPTER 7
603
Ž .
where g is the vector whose ith element is g t . Hence,
i
k
k
" f x
1
dg
t
" f x
Ž .
Ž .
Ž .
i
®
s
.
Ý
Ý
i
" x
dgrdt
dt
" x
i
i
is1
is1
Furthermore,
ds
dg
s
.
dt
dt
It follows that
k
k
" f x
dt
dg
t
" f x
Ž .
Ž .
Ž .
i
®
s
Ý
Ý
i
" x
ds
dt
" x
i
i
is1
is1
k
dx " f x
Ž .
i
s Ý ds
" xi
is1
df x
Ž .
s
.
ds
( )
7.9.
a
"
"
f x , x
ff 0, 0 q
x
qx
f 0, 0
Ž
.
Ž
.
Ž
.
1
2
1
2
ž
/
" x
" x
1
2
1
" 2
" 2
" 2
2
2
q
x
q2 x x
qx
f 0, 0
Ž
.
1
1
2
2
2
2
ž
/
2!
" x " x
" x
" x
1
2
1
2
s1qx x .
1
2
( )
b
3
"
f x , x , x
ff 0, 0, 0 q
x
f 0, 0, 0
Ž
.
Ž
.
Ž
.
Ý
1
2
3
i
ž
/
" xi
is1
2
3
1
"
q
x
f 0, 0, 0
Ž
.
Ý
i
ž
/
2!
" xi
is1
1
2
2
ssin 1 qx cos 1 q
x
cos 1 ysin 1
q2 x cos 1
.
Ž .
Ž .
Ž .
Ž .
Ž .

4
1
1
2
2!
( )
c
"
"
f x , x
ff 0, 0 q
x
qx
f 0, 0
Ž
.
Ž
.
Ž
.
1
2
1
2
ž
/
" x
" x
1
2
1
" 2
" 2
" 2
2
2
q
x
q2 x x
qx
f 0, 0
Ž
.
1
1
2
2
2
2
ž
/
2!
" x " x
" x
" x
1
2
1
2
s1,
since all first-order and second-order partial derivatives vanish at
xs0.

SOLUTIONS TO SELECTED EXERCISES
604
( )
Ž
.
7.10.
a
If u sf x , x
and " fr" x 0 at
x , then by the implicit
1
1
2
1
0
Ž
.
function theorem Theorem 7.6.2 , there is neighborhood of x
in
0
Ž
.
which the equation u sf x , x
can be solved uniquely for x
in
1
1
2
1
Ž
.
terms of x
and u , that is, x sh u , x
. Thus,
2
1
1
1
2
f h u , x
, x
u .
Ž
.
1
2
2
1
Hence, by differentiating this identity with respect to x , we get
2
" f
"h
" f
q
s0,
" x
" x
" x
1
2
2
which gives
"h
" f
" f
sy
" x
" x
" x
2
2
1
in a neighborhood of x .
0
( )
Ž .
Ž
.
b
On the basis of part a , we can consider g x , x
as a function of
1
2
Ž
.
x
and u, since x sh u , x
in a neighborhood of x . In such a
2
1
1
2
0
neighborhood, the partial derivative of g with respect to x
is
2
" g
"h
" g
"
f, g
" f
Ž
.
q
s
,
" x
" x
" x
" x , x
" x
Ž
.
1
2
2
1
2
1
Ž
.
Ž
.
w
which is equal to zero because " f, g r" x , x
s0. Recall that
1
2
Ž
.Ž
.x
"hr" x sy " fr" x
"hr" x
.
2
2
1
( )
Ž .
w Ž
.
x
c
From part b , g h u , x
, x
is independent of x
in a neighbor-
1
2
2
2
Ž
.
w Ž
.
x
hood of x . We can then write  u
sg h u , x
, x . Since
0
1
1
2
2
Ž
.
Ž
.
x sh u , x
is equivalent to u sf x , x
in a neighborhood of
1
1
2
1
1
2
w Ž
.x
Ž
.
x ,  f x , x
sg x , x
in this neighborhood.
0
1
2
1
2
( )
Ž
.
Ž
.
w Ž
.x
d
Let G f, g sg x , x
y f x , x
. Then, in a neighborhood of
1
2
1
2
Ž
.
x , G f, g s0. Hence,
0
"G " f
"G " g
q
0,
" f " x
" g " x
1
1
"G " f
"G " g
q
0.
" f " x
" g " x
2
2
In order for these two identities to be satisfied by values of
Ž
.
Ž
.
"Gr" f, "Gr" g not all zero, it is necessary that " f, g r" x , x
1
2
be equal to zero in this neighborhood of x .
0

CHAPTER 7
605
Ž
.
Ž
.
7.11. u x , x , x
su   ,   , 
. Hence,
1
2
3
1
3
2
3
3
" u
" u " x
" u " x
" u " x
1
2
3
s
q
q
"
" x
"
" x
"
" x
"
3
1
3
2
3
3
3
" u
" u
" u
s
q
q
,
1
2
" x
" x
" x
1
2
3
so that
" u
" u
" u
" u

s 
q 
q
3
1
3
2
3
3
"
" x
" x
" x
3
1
2
3
" u
" u
" u
sx
qx
qx
snu.
1
2
3
" x
" x
" x
1
2
3
Integrating this partial differential equation with respect to  , we get
3
log usnlog  q  , 
,
Ž
.
3
1
2
Ž
.
where   , 
is a function of  , . Hence,
1
2
1
2
n
us exp   , 
Ž
.
3
1
2
sx nF  , 
Ž
.
3
1
2
x
x
1
2
n
sx F
,
,
3 ž
/
x
x
3
3
Ž
.
w
Ž
.x
where F  , 
sexp   , 
.
1
2
1
2
( )
2
2
2
7.13.
a
The Jacobian determinant is 27x x x , which is zero in any subset
1
2
3
of R3 that contains points on any of the coordinate planes.
( )
b
The unique inverse function is given by
x su1r3,
x su1r3,
x su1r3.
1
1
2
2
3
3
Ž
.
Ž
.
7.14. Since " g , g
r" x , x
0, we can solve for x , x
uniquely in terms
1
2
1
2
1
2
of y and y
by Theorem 7.6.2. Differentiating g
and g
with respect
1
2
1
2
to y , we obtain
1
" g
" x
" g
" x
" g
1
1
1
2
1
q
q
s0,
" x
" y
" x
" y
" y
1
1
2
1
1
" g
" x
" g
" x
" g
2
1
2
2
2
q
q
s0,
" x
" y
" x
" y
" y
1
1
2
1
1
Solving for " x r" y and " x r" y
yields the desired result.
1
1
2
1

SOLUTIONS TO SELECTED EXERCISES
606
Ž
.
Ž
.
7.15. This is similar to Exercise 7.14. Since " f, g r" x , x
0, we can
1
2
solve for x , x
in terms of x , and we get
1
2
3
dx
"
f, g
"
f, g
Ž
.
Ž
.
1 sy
,
dx
" x , x
" x , x
Ž
.
Ž
.
3
3
2
1
2
dx
"
f, g
"
f, g
Ž
.
Ž
.
2 sy
.
dx
" x , x
" x , x
Ž
.
Ž
.
3
1
3
1
2
But
"
f, g
"
f, g
"
f, g
"
f, g
Ž
.
Ž
.
Ž
.
Ž
.
sy
,
sy
.
" x , x
" x , x
" x , x
" x , x
Ž
.
Ž
.
Ž
.
Ž
.
3
2
2
3
1
3
3
1
Hence,
dx
dx
dx
1
3
2
s
s
.
"
f, g
" x , x
"
f, g
" x , x
"
f, g
" x , x
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
Ž
.
2
3
1
2
3
1
1
1
( ) Ž
.
7.16.
a
y , y
is a point of local minimum.
3
3
( )
b
" f
s4 x yx q1s0,
1
2
" x1
" f
syx q2 x y1s0.
1
2
" x2
Ž
.
Ž
The solution of these two equations is x s1r 1y8 , x s 4y
1
2
. Ž
.
1 r 8y1 . The Hessian matrix of f is
4
y1
As
.
y1
2
Ž .
Here, f
s4, and det A s8y1.
11
1
i. Yes, if  .
8
ii. No, it is not possible.
1
iii. Yes, if  , 0.
8
( )
Ž
. Ž
.
c
The stationary points are
y2, y2 ,
4, 4 . The first is a saddle
point, and the second is a point of local minimum.
'
'
' '
( )
Ž
. Ž
.
Ž
.
d
The stationary points are
2 , y 2 , y 2 , 2 , and 0, 0 . The
Ž
.
2
first two are points of local minimum. At 0, 0 , f
f
yf
s0. In
11
22
12

CHAPTER 7
607

this case, hAh has the same sign as f
Ž0, 0.sy4 for all values of
11
Ž
.
hs h , h
, except when
1
2
f12
h qh
s0
1
2 f11
or h yh s0, in which case h
Ahs0. For such values of h,
1
2
Ž 
.3 Ž
.
Ž 
.4 Ž
.
Ž
4
4.
4
h 
f 0, 0 s0, but
h 
f 0, 0 s24 h qh
s48h , which is
1
2
1
Ž
.
nonnegative. Hence, the point 0, 0 is a saddlepoint.
7.19. The Lagrange equations are
2q8 x q12 x s0,
Ž
.
1
2
12 x q 4q2 x s0,
Ž
.
1
2
4 x 2qx 2s25.
1
2
We must have
4q1
q2 y36s0.
Ž
. Ž
.
The solutions are  sy4.25,  s2. For  sy4.25, we have the
1
2
1
points
i. x s1.5, x s4.0,
1
2
ii. x sy1.5, x sy4.0.
1
2
For  s2, we have the points
2
iii. x s2, x sy3,
1
2
iv. x sy2, x s3.
1
2
The matrix B has the value
1
2q8
12
8 x1
12
4q2
2 x
B s
.
2
1
8 x
2 x
0
1
2
The determinant of B is  .
1
1
Ž .
At i ,  s5000; the point is a local maximum.
1
Ž .
At ii ,  s5000; the point is a local maximum.
1
Ž
.
Ž .
At iii and iv ,  sy5000; the points are local minima.
1

SOLUTIONS TO SELECTED EXERCISES
608
2
2
2
Ž
2
2
2
2.
7.21. Let Fsx x x q x qx qx yc
. Then
1
2
3
1
2
3
"F
2
2
s2 x
x x q2x s0,
1
2
3
1
" x1
"F
2
2
s2 x x x q2x s0,
1
2
3
2
" x2
"F
2
2
s2 x x x q2x s0,
1
2
3
3
" x3
x 2qx 2qx 2sc2.
1
2
3
Since x , x , x
cannot be equal to zero, we must have
1
2
3
x 2 x 2qs0,
2
3
x 2 x 2qs0,
1
3
x 2 x 2qs0.
1
2
These equations imply that x 2sx 2sx 2sc2r3 and syc4r9. For
1
2
3
these values, it can be verified that  0,  0, where  
and  
1
2
1
2
are the determinants of B and B , and
1
2
2
2
2
2
2 x x q2
4 x x x
4 x x x
2 x
2
3
1
2
3
1
2
3
1
2
2
2
2
4 x x x
2 x x q2
4 x x x
2 x
1
2
3
1
3
1
2
3
2
B s
,
1
2
2
2
2
4 x x x
4 x x x
2 x x q2
2 x
1
2
3
1
2
3
1
2
3
2 x
2 x
2 x
0
1
2
3
2
2
2
2 x x q2
4 x x x
2 x
1
3
1
2
3
2
2
2
2
B s
.
4 x x x
2 x x q2
2 x
2
1
2
3
1
2
3
2 x
2 x
0
2
3
Since ns3 is odd, the function x 2 x 2 x 2 must attain a maximum value
1
2
3
Ž
2
.3
given by c r3 . It follows that for all values of x , x , x ,
1
2
3
3
2
c
2
2
2
x x x F
,
1
2
3 ž /
3
that is,
x 2qx 2qx 2
1r3
1
2
3
2
2
2
x x x
F
.
Ž
.
1
2
3
3

CHAPTER 7
609
7.24. The domain of integration, D, is the region bounded from above by the
parabola x s4 x yx 2 and from below by the parabola x sx 2. The
2
1
1
2
1
Ž
.
Ž
.
two parabolas intersect at 0, 0 and 2, 4 . It is then easy to see that
2
2
4
x
4 x yx
' 2
1
1f x , x
dx
dx s
f x , x
dx
dx .
Ž
.
Ž
.
H H
H H
1
2
2
1
1
2
1
2
2
0
x
0
2y
4yx
'
1
2
1
1yx
'
2
( )
w
Ž
.
x
7.25.
a
IsH
H
f x , x
dx
dx .
0
1
2
1
2
1yx 2
" f x , x
Ž
.
2
1
2
1yx
2
1
( )
Ž
.
Ž
.
b
dgrdx sH
dx y2 x f x , 1yx
qf x , 1yx .
1
1yx
2
1
1
1
1
1
1
" x1
7.26. Let usx 2rx , ®sx 2rx . Then u®sx x , and
2
1
1
2
1
2
" x , x
Ž
.
2
2
1
2
x x dx dx s
u®
du d®.
HH
H H
1
2
1
2
" u, ®
Ž
.
D
1
1
But
y1
" x , x
" u, ®
1
Ž
.
Ž
.
1
2
s
s
.
" u, ®
" x , x
3
Ž
.
Ž
.
1
2
Hence,
3
x x dx dx s
.
HH
1
2
1
2
4
D
'3
2
Ž .
Ž
.
7.28. Let I a sH
dxr aqx
. Then
0
2
d I a
dx
Ž .
'3
s2
.
H
2
3
2
da
0
aqx
Ž
.
On the other hand,
1
3
I a s
Arctan
Ž .
(
'
a
a
2
'
d I a
3
3
3
1
Ž .
y5r2
s
a
Arctan
q
(
2
2
4
a
4
da
a
aq3
Ž
.
'3
2aq3
Ž
.
q
.
2
2
2
a
aq3
Ž
.

SOLUTIONS TO SELECTED EXERCISES
610
Thus
'
dx
3
3
3
1
'3
y5r2
s
a
Arctan
q
(
H
3
2
2
8
a
8
a
aq3
Ž
.
0
aqx
Ž
.
'3
2aq3
Ž
.
q
.
2
2
4
a
aq3
Ž
.
Putting as1, we obtain
'
dx
3
7 3
'3
'
s
Arctan 3 q
.
H
3
2
8
64
0
1qx
Ž
.
( )
7.29.
a
The marginal density functions of X
and X
are
1
2
1
f
x
s
x qx
dx
Ž
.
Ž
.
H
1
1
1
2
2
0
1
sx q ,
0x 1,
1
1
2
1
f
x
s
x qx
dx
Ž
.
Ž
.
H
2
2
1
2
1
0
1
sx q ,
0x 1,
2
2
2
f x , x
f
x
f
x
.
Ž
.
Ž
.
Ž
.
1
2
1
1
2
2
The random variables X
and X
are therefore not independent.
1
2
( )
b
1
1
E X X
s
x x
x qx
dx dx
Ž
.
Ž
.
H H
1
2
1
2
1
2
1
2
0
0
1
s .
3
7.30. The marginal densities of X
and X
are
1
2
1qx ,
y1x 0,
°
1
1
~
f
x
s
Ž
.
1yx ,
0x 1,
1
1
1
1
¢0,
otherwise,
f
x
s2 x ,
0x 1.
Ž
.
2
2
2
2

CHAPTER 7
611
Ž
.
Ž
.
Ž
.
Note that f x , x
f
x
f
x
. Hence, X and X
are not indepen-
1
2
1
1
2
2
1
2
dent. But
x
1
2
E X X
s
x x dx
dx s0,
Ž
. H H
1
2
1
2
1
2
0
yx 2
0
1
E X
s
x
1qx
dx q
x
1yx
dx s0,
Ž
.
Ž
.
Ž
.
H
H
1
1
1
1
1
1
1
y1
0
1
2
2
E X
s
2 x dx s .
Ž
. H
2
2
2
3
0
Hence,
E X X
sE X
E X
s0.
Ž
.
Ž
.
Ž
.
1
2
1
2
( )
7.31.
a
The joint density function of Y and Y
is
1
2
1
y1
y1
qy1
yy 2
g y , y
s
y
1yy
y
e
.
Ž
.
Ž
.
1
2
1
1
2
!  ! 
Ž
.
Ž
.
( )
b
The marginal densities of Y and Y are, respectively,
1
2
! q
Ž
.
y1
y1
g
y
s
y
1yy
,
0y 1,
Ž
.
Ž
.
1
1
1
1
1
!  ! 
Ž
.
Ž
.
B  , 
Ž
.
qy1
yy 2
g
y
s
y
e
,
0y .
Ž
.
2
2
2
2
!  ! 
Ž
.
Ž
.
( )
Ž
.
Ž
. Ž
.
Ž
.
c
Since B ,  s!  !  r! q ,
g y , y
sg
y
g
y
,
Ž
.
Ž
.
Ž
.
1
2
1
1
2
2
and Y and Y are therefore independent.
1
2
7.32. Let UsX , WsX X . Then, X sU, X sWrU. The joint density
1
1
2
1
2
function of U and W is
2
10w
'
g u, w s
,
wu w ,
0w1.
Ž
.
2
u

SOLUTIONS TO SELECTED EXERCISES
612
The marginal density function of W is
du
w
'
2
g
w s10w
Ž
.
H
1
2
u
w
1
1
2
s10w
y
,
0w1.
ž
/
'
w
w
7.34. The inverse of this transformation is
X sY ,
1
1
X sY yY ,
2
2
1
...
X sY yY
,
n
n
ny1
and the absolute value of the Jacobian determinant of this transforma-
tion is 1. Therefore, the joint density function of the Y ’s is
i
g y , y , . . . , y
seyy 1eyŽ y2yy 1.  eyŽ ynyy ny1 .
Ž
.
1
2
n
seyy n,
0y y   y .
1
2
n
The marginal density function of Y
is
n
y
y
y
n
ny1
2
yy n
g
y
se

dy dy  dy
Ž
.
H H
H
n
n
1
2
ny1
0
0
0
y ny1
n
yy n
s
e
,
0y .
n
ny1 !
Ž
.
Ž
.
7.37. Let s  ,  , 
. The least-squares estimate of  is given by
0
1
2
y1


ˆs X X
X y,
Ž
.
Ž
.
where ys y , y , . . . , y
, and X is a matrix of order n3 whose first
1
2
n
column is the column of ones, and whose second and third columns are
given by the values of x , x 2, is1, 2, . . . , n.
i
i
Ž
.
7.38. Maximizing L x, p is equivalent to maximizing its natural logarithm.
Let us therefore consider maximizing log L subject to Ýk
p s1.
is1
i
Using the method of Lagrange multipliers, let
k
Fslog Lq
p y1 .
Ý
i
ž
/
is1

CHAPTER 8
613
Differentiating
F with respect to
p , p , . . . , p , and equating the
1
2
k
derivatives to zero, we obtain, x rp qs0 for is1, 2, . . . , k. Combin-
i
i
ing these equations with the constraint Ýk
p s1, we obtain the
is1
i
maximum likelihood estimates
xi
p s
,
is1 ,2, . . . , k.
ˆi
n
CHAPTER 8
Ž
.
Ž
.
8.1. Let x s x , x
, h s h , h
, is0, 1, 2, . . . . Then
i
i1
i2
i
i1
i2
2
2
f x qth
s8 x qth
y4 x qth
x qth
q5 x qth
Ž
.
Ž
.
Ž
. Ž
.
Ž
.
i
i
i1
i1
i1
i1
i2
i2
i2
i2
sa t 2qb tqc ,
i
i
i
where
a s8h2 y4h h q5h2 ,
i
i1
i1
i2
i2
b s16 x h y4 x h qx h
q10 x h ,
Ž
.
i
i1
i1
i1
i2
i2
i1
i2
i2
c s8 x 2 y4 x x q5x 2 .
i
i1
i1
i2
i2
Ž
.
The value of t that minimizes f x qth
in the direction of h is given
i
i
i
Ž
.
by the solution of " fr" ts0, namely, t syb r 2a . Hence,
i
i
i
bi
x
sx y
h ,
is0, 1, 2, . . . ,
iq1
i
i
2ai
where

f x
Ž
.
i
h sy
,
is0, 1, 2, . . . ,
i

f x
Ž
.
i
2
and


f x
s 16 x y4 x , y4 x q10 x
,
is0, 1, 2, . . . .
Ž
.
Ž
.
i
i1
i2
i1
i2

SOLUTIONS TO SELECTED EXERCISES
614
The results of the iterative minimization procedure are given in the
following table:
Ž
.
Iteration i
x
h
t
a
f x
i
i
i
i
i


Ž
.
Ž
.
0
5, 2
y1, 0
4.5
8
180


Ž
.
Ž
.
1
0.5, 2
0, y1
1.8
5
18


Ž
.
Ž
.
2
0.5, 0.2
y1, 0
0.45
8
1.8


Ž
.
Ž
.
3
0.05, 0.2
0, y1
0.18
5
0.18


Ž
.
Ž
.
4
0.05, 0.02
y1, 0
0.045
8
0.018


Ž
.
Ž
.
5
0.005, 0.02
0, y1
0.018
5
0.0018


Ž
.
Ž
.
6
0.005, 0.002
y1, 0
0.0045
8
0.00018
Ž
.
Note that a 0, which confirms that t syb r 2a
does indeed
i
i
i
i
Ž
.
minimize f x qth
in the direction of h . It is obvious that if we were
i
i
i
to continue with this iterative procedure, the point x would converge
i
to 0.
( )
8.3.
a
y x s45.690q4.919x q8.847x
Ž .
ˆ
1
2
y0.270 x x y4.148 x 2y4.298 x 2.
1
2
1
2
ˆ
( )
b
The matrix B is
y4.148
y0.135
ˆBs
.
y0.135
y4.298
ˆ
Its eigenvalues are  sy8.136,  sy8.754. This makes B a
1
2
negative definite matrix. The results of applying the method of
ridge analysis inside the region R are given in the following table:

x
x
r
yˆ
1
2
31.4126
0.0687
0.1236
0.1414
47.0340
13.5236
0.1373
0.2473
0.2829
48.2030
7.5549
0.2059
0.3709
0.4242
49.1969
4.5694
0.2745
0.4946
0.5657
50.0158
2.7797
0.3430
0.6184
0.7072
50.6595
1.5873
0.4114
0.7421
0.8485
51.1282
0.7359
0.4797
0.8659
0.9899
51.4218
0.0967
0.5480
0.9898
1.1314
51.5404
y0.4009
0.6163
1.1137
1.2729
51.4839
y0.7982
0.6844
1.2376
1.4142
51.2523
Ž
.
Note that the stationary point, x s 0.5601, 1.0117 , is a point of
0
ˆ
absolute maximum since the eigenvalues of B are negative. This
point falls inside R. The corresponding maximum value of y is
ˆ
51.5431.

CHAPTER 8
615
8.4. We know that
1
ˆ
ˆ
By I
x sy ,
Ž
.
1
k
1
2
1
ˆ
ˆ
By I
x sy ,
Ž
.
2
k
2
2
where x
and x
correspond to  and  , respectively, and are such
1
2
1
2
that x
 x sx
 x sr 2, with r 2 being the common value of r 2 and r 2.
1
1
2
2
1
2
The corresponding values of y are
ˆ
ˆ

 ˆ

 ˆ
y s qx qx Bx ,
ˆ1
0
1
1
1
ˆ

 ˆ

 ˆ
y s qx qx Bx .
ˆ2
0
2
2
2
We then have


1


2
ˆ
ˆ
ˆ
x Bx yx Bx q
x yx
s  y
r ,
Ž
.
Ž
.
1
1
2
2
1
2
1
2
2
1


2
ˆ
y yy s
x yx
q  y
r .
Ž
.
Ž
.
ˆ
ˆ
1
2
1
2
1
2
2
Furthermore, from the equations defining x and x , we have
1
2

1


 ˆ
 y
x x s
x yx
.
Ž
.
Ž
.
2
1
1
2
1
2
2
Hence,
y yy s  y
r 2yx
 x
.
Ž
.
ˆ
ˆ
Ž
.
1
2
1
2
1
2
2
	
	 	
	

But r s x
x
x x , since x
and x
are not parallel vectors.
2
2
1
2
1
2
1
2
We conclude that y y
whenever   .
ˆ
ˆ
1
2
1
2
Ž
.
8.5.
If M x
is positive definite, then 
must be smaller than all the
1
1
ˆ Ž
eigenvalues of B this is based on applying the spectral decomposition
ˆ
.
Ž
.
theorem to B and using Theorem 2.3.12 . Similarly, if M x
is indefi-
2ˆ
nite, then  must be larger than the smallest eigenvalue of B and also
2
smaller than its largest eigenvalue. It follows that   . Hence, by
1
2
Exercise 8.4, y y .
ˆ
ˆ
1
2
( )
8.6.
a
We know that
1
ˆ
ˆ
ByI
xsy .
Ž
.
k
2
Differentiating with respect to  gives
" x
ˆByI
sx,
Ž
.
k
"

SOLUTIONS TO SELECTED EXERCISES
616
and since x
xsr 2,
" x
" r

x
sr
.
"
"
A second differentiation with respect to  yields
" 2x
" x
ˆByI
s2
,
Ž
.
k
2
"
"

2
2
2
" x
" x " x
" r
" r

x
q
sr
q
.
2
2 ž /
" "
"
"
"
If we premultiply the second equation by " 2x
r"2 and the fourth
equation by " x
r", subtract, and transpose, we obtain
" 2x
" x
 " x

x
y2
s0.
2
" "
"
Substituting this in the fifth equation, we get

2
2
" r
" x " x
" r
r
s3
y
.
2
ž /
" "
"
"
Now, since
" r
"
1r2

s
x x
Ž
.
"
"
" xr"

sx
,
1r2

x x
Ž
.
we conclude that

2
2
" r
" x " x
" x

3
2
r
s3r
y x
.
2
ž
/
" "
"
"
( )
Ž .
b
The expression in a can be written as


2
2
" r
" x " x
" x " x
" x

3
2
2
r
s2r
q r
y x
.
2
ž
/
" "
" "
"
"
The first part on the right-hand side is nonnegative and is zero only
when rs0 or when " xr"s0. The second part is nonnegative by

CHAPTER 8
617
the fact that
2
2
" x
" x

2
x
Fr
ž
/
"
"
2
" x
 " x
2
sr
.
" "
Equality occurs only when xs0, that is, rs0, or when " xr"s0.
But when " xr"s0, we have xs0 if  is different from all the
ˆ
2
2
eigenvalues of B, and thus rs0. It follows that " rr" 0 except
when rs0, where it takes the value zero.
( )
8.7.
a
n
2
Bs
E y x
y x
dx

4
Ž .
Ž .
ˆ
H
2
	
R
n



s
y 
y y2 y  q   ,

4
Ž
.
Ž
.
Ž
.
11
12
22
2
	
where  ,  , 
are the region moments defined in Section
11
12
22
8.4.3.
( )
b
To minimize B we differentiate it with respect to , equate the
derivative to zero, and solve for . We get
2
y y2 s0,
Ž
.
11
12
sqy1 
11
12
sC.
This solution minimizes B, since 
is positive definite.
11
( )
c
B is minimized if and only if sC. This is equivalent to stating
ˆ
Ž
.
that C is estimable, since E  s.
ˆ
( )
d
Writing  as a linear function of the vector y of observations of
ˆ
the form sLy, we obtain
ˆ
E  sL E y
Ž .
Ž
.
sL XqZ
Ž
.
w
x
sL X : Z .
ˆ
Ž
.
w
x
But sE  sC. We conclude that Cs L X : Z .
( )
Ž .
e
It is obvious from part d that the rows of C are spanned by the
w
x
rows of X : Z .

SOLUTIONS TO SELECTED EXERCISES
618
( )
Ž

.y1

f
The matrix L defined by Ls X X
X satisfies the equation
w
x
L X : Z sC,
since
y1


w
x
w
x
L X : Z s X X
X X : Z
Ž
.
y1


s I : X X
X Z
Ž
.
y1
s I : M
M
11
12
y1
s I : 

11
12
sC.
8.8. If the region R is a sphere of radius 1, then 3g 2F1. Now,
1
0
0
0
1
0
0
0
5
 s
,
1
11
0
0
0
5
1
0
0
0
5
0
0
0
0
0
0
 s
.
12
0
0
0
0
0
0
w
x
Hence, Cs I : O
. Furthermore,
4
43
w
x
Xs 1 : D ,
4
2
2
2
g
g
g
2
2
2
g
yg
yg
Zs
.
2
2
2
yg
g
yg
2
2
2
yg
yg
g
Hence,
1

M
s X X
11
4
1
0
0
0
2
0
g
0
0
s
,
2
0
0
g
0
2
0
0
0
g

CHAPTER 8
619
1

M
s X Z
12
4
0
0
0
3
0
0
yg
s
.
3
0
yg
0
3
yg
0
0
( )
a
1
2
M
s
´
g s
11
11
5
M
s
´
gs0
12
12
Thus, it is not possible to choose g so that D satisfies the condi-
Ž
.
tions in 8.56 .
( )
b
Suppose that there exists a matrix L of order 44 such that
w
x
CsL X : Z .
Then
I sLX,
4
0sLZ.
The second equation implies that L is of rank 1, while the first
equation implies that the rank of L is greater than or equal to the
rank of I , which is equal to 4. Therefore, it is not possible to find a
4
matrix such as L. Hence, g cannot be chosen so that D satisfies the
Ž .
minimum bias property described in part e of Exercise 8.7.
( )

8.9.
a
Since  is symmetric, it can be written as sP P , where  is a
diagonal matrix of eigenvalues of  and the columns of P are the
corresponding orthogonal eigenvectors of , each of length equal
Ž
.
to 1 see Theorem 2.3.10 . It is easy to see that over the region ,
h , D F
e
 Fr 2e
 ,
Ž
.
Ž
.
Ž
.
max
max
Ž
.

by the fact that e
 IyP P is positive semidefinite. Without
max
loss of generality, we consider that the diagonal elements of  are
written in descending order. The upper bound in the above in-
Ž
.
equality is attained by h , D
for srP , where P
is the first
1
1
Ž
.
column of P, which corresponds to e
 .
max
( )
Ž
.
b
The design D can be chosen so that e
 is minimized over the
max
region R.
8.10. This is similar to Exercise 8.9. Write T as P P
, where  is a diagonal
matrix of eigenvalues of T, and P is an orthogonal matrix of corre-
sponding eigenvectors. Then 
Tsu
u, where us1r2 P
, and

SOLUTIONS TO SELECTED EXERCISES
620
Ž
.


y1r2

y1r2
 , D s Ssu 
P SP
u. Hence, over the region ,

SGe
y1r2 P
SPy1r2 .
Ž
.
min
But, if S is positive definite, then by Theorem 2.3.9,
e
y1r2 P
SPy1r2 se
Py1P
 S
Ž
.
Ž
.
min
min
se
Ty1S .
Ž
.
min
( )
Ž
.
Ž
8.11.
a
Using formula 8.52 , it can be shown that see Khuri and Cornell,
.
1996, page 229
1
4
Vs

kq2 y2k  y
Ž
.
4
2
2
ž
/


kq2 y k
Ž
.
2
4
2
2

kq1 y
ky1
Ž
.
Ž
.
4
2
q2k
,

kq4
Ž
.
4
Ž
.
where k is the number of input variables that is, ks2 ,
n
1
2
 s
x ,
is1, 2
Ý
2
ui
n us1
1
k
2
s
2 q2
Ž
.
n
8
s
,
n
n
1
2
2
 s
x
x
,
ij
Ý
4
ui
u j
n us1
2 k
s n
4
s
,
n
and ns2 kq2kqn s8qn
is the total number of observations.
0
0
Here, x
denotes the design setting of variable x , is1, 2; us
ui
i
1, 2, . . . , n.
( )
b
The quantity V, being a function of n , can be minimized with
0
respect to n .
0

CHAPTER 8
621
8.12. We have that


ˆ
ˆ
ˆ
ˆ
YyXB
YyXB s Yy XBqXByXB
YyXBqXByXB
Ž
. Ž
.
Ž
. Ž
.


ˆ
ˆ
ˆ
ˆ
s YyXB
YyXB q XByXB
XByXB ,
Ž
. Ž
.
Ž
. Ž
.
since




ˆ
ˆ
ˆ
ˆ
YyXB
XByXB s X YyX XB
ByB s0.
Ž
. Ž
.
Ž
. Ž
.
ˆ

ˆ
Ž
. Ž
.
Furthermore, since XByXB
XByXB is positive semidefinite, then
by Theorem 2.3.19,


ˆ
ˆ
e
YyXB
YyXB
Ge
YyXB
YyXB
,
is1, 2, . . . , r,
Ž
. Ž
.
Ž
. Ž
.
i
i
Ž .
Ž
where e 
denotes the ith eigenvalue of a square matrix. If
Yy
i

ˆ 
ˆ
. Ž
.
Ž
. Ž
.
XB
YyXB and YyXB
YyXB are nonsingular, then by multiply-
ing the eigenvalues on both sides of the inequality, we obtain


ˆ
ˆ
det
YyXB
YyXB
Gdet
YyXB
YyXB
.
Ž
. Ž
.
Ž
. Ž
.
ˆ
Equality holds when BsB.
8.13. For any b, 1qbFeb. Let as1qb. Then aFeay1. Now, let  denote
i
the ith eigenvalue of A. Then  G0, and by the previous inequality,
i
p
p
 Fexp
 yp .
Ł
Ý
i
i
ž
/
is1
is1
Hence,
det A Fexp tr AyI
.
Ž .
Ž
.
p
8.14. The likelihood function is proportional to
n
ynr2
y1
det V
exp y
tr SV
.
Ž .
Ž
.
2
Now, by Exercise 8.13,
y1
y1
det SV
Fexp tr SV
yI
,
Ž
.
Ž
.
p

SOLUTIONS TO SELECTED EXERCISES
622
Ž
y1.
Ž
y1r2
y1r2 .
Ž
y1.
Ž
y1r2
y1r2 .
since det SV
sdet V
SV
, tr SV
str V
SV
, and
Vy1r2 SVy1r2 is positive semidefinite. Hence,
n
n
nr2
y1
y1
det SV
exp y
tr SV
Fexp y
tr I
.
Ž
.
Ž
.
Ž
.
p
2
2
This results in the following inequality:
n
n
ynr2
ynr2
y1
det V
exp y
tr SV
F det S
exp y
tr I
,
Ž .
Ž
.
Ž .
Ž
.
p
2
2
which is the desired result.

ˆ
ˆ

y1


Ž .
Ž .
Ž
.
Ž .
Ž
.
8.16. y x sf x , where s X X
X y, and f x
is as in model
8.47 .
ˆ
Ž
.

Ž .
Simultaneous 1y 100% confidence intervals on f x  for all x in
R are of the form
1r2
1r2
y1



ˆ
f
x  p MS F
f
x
X X
f x
.
Ž .
Ž . Ž
.
Ž .
Ž
.
E
 , p, nyp
For the points x , x , . . . , x , the joint confidence coefficient is at least
1
2
m
1y.
CHAPTER 9
Ž .
w
x
9.1. If f x
has a continuous derivative on 0, 1 , then by Theorems 3.4.5
and 4.2.2 we can find a positive constant A such that
f x
yf x
FA x yx
Ž
.
Ž
.
1
2
1
2
w
x
for all x , x
in 0, 1 . Thus, by Definition 9.1.2,
1
2

  FA.
Ž
.
Using now Theorem 9.1.3, we obtain
3
A
f x yb
x
F
Ž .
Ž .
n
1r2
2 n
w
x
for all x in 0, 1 . Hence,
c
sup
f x yb
x
F
,
Ž .
Ž .
n
1r2
n
0FxF1
3
where cs A.
2

CHAPTER 9
623
9.2. We have that
f x
yf x
F
sup
f z
yf z
Ž
.
Ž
.
Ž
.
Ž
.
1
2
1
2
z yz
F
1
2
2
for all
x yx
F , and hence for all
x yx
F , since  F . It
1
2
2
1
2
1
1
2
follows that
sup
f x
yf x
F
sup
f z
yf z
,
Ž
.
Ž
.
Ž
.
Ž
.
1
2
1
2
x yx
F
z yz
F
1
2
1
1
2
2
Ž
.
Ž
.
that is, 
 
F
 
.
1
2
Ž .
w
x
9.3. Suppose that f x is uniformly continuous on a, b . Then, for a given
Ž .
0, there exists a positive  
such that
f x
yf x
 for all
Ž
.
Ž
.
1
2
w
x
Ž .
x , x
in a, b for which
x yx
. This implies that 
  F and
1
2
1
2
Ž .
Ž .
hence 
  ™0 as ™0. Vice versa, if 
  ™0 as ™0, then for a
Ž .
given 0, there exists  0 such that 
   if  . This
1
1
implies that
f x
yf x

if x yx
F ,
Ž
.
Ž
.
1
2
1
2
1
Ž .
w
x
and f x must therefore be uniformly continuous on a, b .
9.4. By Theorem 9.1.1, there exists a sequence of polynomials, namely the

Ž .4
Ž .
 
Bernstein polynomials
b
x
, that converges to f x s x
uni-
n
ns1
w
x
Ž .
Ž .
Ž .
Ž .
Ž .
formly on ya, a . Let p
x sb
x yb 0 . Then p 0 s0, and p
x
n
n
n
n
n
 
w
x
Ž .
converges uniformly to x on ya, a , since b 0 ™0 as n™.
n
1 Ž .
Ž .
9.5. The stated condition implies that H f x p
x dxs0 for any polynomial
0
n
Ž .
Ž .
p
x of degree n. In particular, if we choose p
x to be a Bernstein
n
n
Ž .
Ž .
w
x
polynomial for f x , then it will converge uniformly to f x on 0, 1 . By
Theorem 6.6.1,
1
1
2
f x
dxs lim
f x p
x
dx
Ž .
Ž .
Ž .
H
H
n
n™
0
0
s0.
Ž .
w
x
Since f x
is continuous, it must be zero everywhere on 0, 1 . If not,
w
x
then by Theorem 3.4.3, there exists a neighborhood of a point in 0, 1
w
Ž .
x
Ž .
1w Ž .x2
at which f x 0 on which f x 0. This causes H
f x
dx to be
0
positive, a contradiction.

SOLUTIONS TO SELECTED EXERCISES
624
Ž
.
9.6. Using formula 9.15 , we have
n
1
Žnq1.
f x yp x s
f
c
xya
.
Ž .
Ž .
Ž .
Ž
.
Ł
i
nq1 !
Ž
.
is0
n
nq1
Ž
. Ž
.
But Ł
xya
Fn! h
r4
see Prenter, 1975, page 37 . Hence,
Ž
.
is0
i

hnq1
nq1
sup
f x yp x
F
.
Ž .
Ž .
4 nq1
Ž
.
aFxFb
Ž
.
9.7. Using formula 9.14 with a , a , a , and a , we obtain
0
1
2
3
p x sll
x log a qll
x log a qll
x log a qll
x log a ,
Ž .
Ž .
Ž .
Ž .
Ž .
0
1
2
3
0
1
2
3
where
xya
xya
xya
1
2
3
ll
x s
,
Ž .
0
ž
/ž
/ž
/
a ya
a ya
a ya
0
1
0
2
0
3
xya
xya
xya
0
2
3
ll
x s
,
Ž .
1
ž
/ž
/ž
/
a ya
a ya
a ya
1
0
1
2
1
3
xya
xya
xya
0
1
3
ll
x s
,
Ž .
2
ž
/ž
/ž
/
a ya
a ya
a ya
2
0
2
1
2
3
xya
xya
xya
0
1
2
ll
x s
.
Ž .
3
ž
/ž
/ž
/
a ya
a ya
a ya
3
0
3
1
3
2
Ž .
Ž .
Values of f x slog x and the corresponding values of p x at several
w
x
points inside the interval 3.50, 3.80 are given below:
Ž .
Ž .
x
f x
p x
3.50
1.25276297
1.25276297
3.52
1.25846099
1.25846087
3.56
1.26976054
1.26976043
3.60
1.28093385
1.28093385
3.62
1.28647403
1.28647407
3.66
1.29746315
1.29746322
3.70
1.30833282
1.30833282
3.72
1.31372367
1.31372361
3.77
1.327075
1.32707487
3.80
1.33500107
1.33500107

CHAPTER 9
625
Using the result of Exercise 9.6, an upper bound on the error of
approximation is given by
 h4
4
sup
f x yp x
F
,
Ž .
Ž .
16
3.5FxF3 .8
Ž
.
where hsmax
a
ya s0.10, and
i
iq1
i
Ž4.
 s
sup
f
x
Ž .
4
3.5FxF3 .8
y6
s
sup
4
x
3.5FxF3 .8
6
s
.
4
3.5
Ž
.
Hence, the desired upper bound is
4
4
 h
6
0.10
4
s ž
/
16
16
3.5
s2.510y7.
9.8. We have that
b
b
b
2
2
2




f
x ys
x
dxs
f
x
dxy
s
x
dx
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
b 


y2
s
x
f
x ys
x
dx.
Ž .
Ž .
Ž .
H
a
But integration by parts yields
b 


s
x
f
x ys
x
dx
Ž .
Ž .
Ž .
H
a
b
b






ss
x
f
x ys
x
y
s
x
f
x yx
x
dx.
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
a
a

Ž .

Ž .
The first term on the right-hand side is zero, since f
x ss x
at
Ž .
xsa, b; and the second term is also zero, by the fact that s
x
is a
Ž .
Ž
.
constant, say s
x s , over  ,
. Hence,
i
i
iq1
ny1

b
iq1





s
x
f
x ys
x
dxs

f
x ys
x
dxs0.
Ž .
Ž .
Ž .
Ž .
Ž .
Ý
H
H
i
a
i
is0

SOLUTIONS TO SELECTED EXERCISES
626
It follows that
b
b
b
2
2
2




f
x ys
x
dxs
f
x
dxy
s
x
dx,
Ž .
Ž .
Ž .
Ž .
H
H
H
a
a
a
which implies the desired result.
9.10.
" rq1h x, 
Ž
.
r
rq1
y x
3
s y1
  
e
.
Ž
.
1
2
3
rq1
" x
Hence,
rq1
"
h x, 
Ž
.
sup
F50.
rq1
" x
0FxF8
Ž
.
Using inequality 9.36 , the integer r is determined such that
rq1
2
8
50 0.05.
Ž
.
ž /
rq1 !
4
Ž
.
The smallest integer that satisfies this inequality is rs10. The Cheby-
shev points corresponding to this value of r are z , z , . . . , z , where by
0
1
10
Ž
.
formula 9.18 ,
2iq1
z s4q4 cos
 ,
is0, 1, . . . , 10.
i
ž
/
22
Ž
.
Using formula 9.37 , the Lagrange interpolating polynomial that ap-
Ž
.
w
x
proximates h x,  over 0, 8 is given by
10
y z
3
i
p
x,  s
1y e
ll
x ,
Ž
.
Ž .
Ý
10
1
2
i
is0
Ž .
where ll
x is a polynomial of degree 10 which can be obtained from
i
Ž
.
Ž
.
9.13 by substituting z for a
is0, 1, . . . , 10 .
i
i
9.11. We have that
" 4 x,  , 
Ž
.
4
8 y x
s 0.49y  e
e
.
Ž
.
4
" x
Hence,
4
"  x,  , 
Ž
.
4 y2 
max
s 0.49y  e
.
Ž
.
4
" x
10FxF40
Ž
.
4 y2 
It can be verified that the function f  s e
is strictly monotone
Ž
.
increasing for 0.06FF0.16. Therefore, s0.16 maximizes
f 

CHAPTER 10
627
over this interval. Hence,
4
"  x,  , 
Ž
.
4
y0 .32
max
F 0.49y
0.16
e
Ž
. Ž
.
4
" x
10FxF40
4
y0 .32
F0.13 0.16
e
Ž
.
s0.0000619,
since 0.36FF0.41. Using Theorem 9.3.1, we have
5
4
max
 x,  ,  ys x
F
 
0.0000619 .
Ž
.
Ž .
Ž
.
384
10FxF40
Here, we considered equally spaced partition points with   s . Let
i
us now choose  such that
5
4
 
0.0000619 0.001.
Ž
.
384
This is satisfied if  5.93. Choosing  s5, the number of knots
needed is
40y10
ms
y1
 
s5.
9.12.
2
2
2

det X X s
x y
x yx
y x yx
x y
.
Ž
.
Ž
. Ž
.
Ž
. Ž
.
3
2
1
3
1
2
1
2
Ž
.
The determinant is maximized when x sy1, x s
1q
, x s1.
1
2
3
4
CHAPTER 10
10.1. We have that


1
1
cos nx cos mxdxs
cos nqm xqcos nym x dx
Ž
.
Ž
.
H
H

2
y
y
0,
nm,
s½ 1,
nsmG1,


1
1
cos nx sin mxdxs
sin nqm xysin nym x dx
Ž
.
Ž
.
H
H

2
y
y
s0
for all m, n,


1
1
sin nx sin mxdxs
cos nym xycos nqm x dx
Ž
.
Ž
.
H
H

2
y
y
0,
nm,
s½ 1,
nsmG1.

SOLUTIONS TO SELECTED EXERCISES
628
( )
10.2.
a
( )
w
m
Ž .
i
Integrating n times by parts
x
is differentiated and p
x is
n
x
integrated , we obtain
1
m
x p
x
dxs0
for ms0, 1, . . . , ny1.
Ž .
H
n
y1
( )
ii
Integrating n times by parts results in
1
n
1
1
n
2
x p
x
dxs
1yx
dx.
Ž .
Ž
.
H
H
n
n
2
y1
y1
Letting xscos , we obtain

n
1
2
2 nq1
1yx
dxs
sin
 d
Ž
.
H
H
y1
0
2 nq1
y1
2
2n
s
,
nG0.
ž /
n
2nq1
( )
Ž .
b
This is obvious, since a is true for ms0, 1, . . . , ny1.
( )
c
1
1
1
2n
2
n
p
x
dxs
x q
x
p
x
dx,
Ž .
Ž .
Ž .
H
H
n
ny1
n
n ž /
n
2
y1
y1
Ž .
where 
x denotes a polynomial of degree ny1. Hence,
ny1
Ž .
Ž .
using the results in a and b , we obtain
nq1
y1
1
2
1
2n
2n
2
p
x
dxs
Ž .
H
n
n ž /
ž /
n
n
2
2nq1
y1
2
s
,
nG0.
2nq1
( )
10.3.
a
2iy1 
Ž
.
T

scos n Arccos cos
Ž
.
n
i
½
5
2n
2iy1 
Ž
.
scos
s0
2
for is1, 2, . . . , n.

2
'
( )
Ž .
Ž
.
Ž
.
b
T
x s nr 1yx
sin n Arccos x . Hence,
n
n
2iy1 
Ž
.

T

s
sin
0
Ž
.
n
i
2
2
'1yi
for is1, 2, . . . , n.

CHAPTER 10
629
( )
10.4.
a
dH
x
dn eyx 2 r2
dnq1 eyx 2 r2
Ž .
Ž
.
Ž
.
2
2
n
n
n
x r2
x r2
s y1
xe
q y1
e
.
Ž
.
Ž
.
n
nq1
dx
dx
dx
Ž
.
Ž
.
Using formulas 10.21 and 10.24 , we have
dH
x
Ž .
2
2
n
n
n
x r2
yx r2
s y1
xe
y1
e
H
x
Ž
.
Ž
.
Ž .
n
dx
n
2
2
nq1
x r2
yx r2
q y1
e
y1
e
H
x
Ž
.
Ž
.
Ž .
nq1
sxH
x yH
x
Ž .
Ž .
n
nq1
snH
x ,
by formula 10.25 .
Ž .
Ž
.
ny1
( )
Ž
.
Ž
.
b
From 10.23 and 10.24 , we have
dH
x
Ž .
n
H
x sxH
x y
.
Ž .
Ž .
nq1
n
dx
Hence,
dH
x
dH
x
d2H
x
Ž .
Ž .
Ž .
nq1
n
n
sx
qH
x y
Ž .
n
2
dx
dx
dx
d2H
x
dH
x
dH
x
Ž .
Ž .
Ž .
n
n
nq1
sx
qH
x y
Ž .
n
2
dx
dx
dx
dH
x
Ž .
n
sx
qH
x y nq1 H
x ,
by a
Ž .
Ž
.
Ž .
Ž .
n
n
dx
dH
x
Ž .
n
sx
ynH
x ,
Ž .
n
dx
which gives the desired result.
( )
Ž
.
10.5.
a
Using formula 10.18 , we can show that
sin
nq1 
Ž
.
Fnq1
sin 
by mathematical induction. Obviously, the inequality is true for
ns1, since
sin 2
2 sin  cos 
s
F2.
sin
sin 

SOLUTIONS TO SELECTED EXERCISES
630
Suppose now that the inequality is true for nsm. To show that it
Ž
.
is true for nsmq1 mG1 :
sin
mq2 
sin
mq1  cos qcos
mq1  sin 
Ž
.
Ž
.
Ž
.
s
sin 
sin 
Fmq1q1smq2.
Therefore, the inequality is true for all n.
( )
b
From Section 10.4.2 we have that
dT
x
sin n
Ž .
n
sn
dx
sin 
Hence,
dT
x
sin n
Ž .
n
sn
dx
sin 
Fn2,
since
sin nrsin  Fn, which can be proved by induction as in
Ž .
a . Note that as x™1, that is, as ™0 or ™,
dT
x
Ž .
n
2
™n .
dx
( )
c
Making the change of variable tscos , we get
x
T
tŽ .
Arccos x
n
dtsy
cos n d
H
H
2
'
y1

1yt
Arccos x
1
sy
sin n
n

1
sy
sin n Arccos x
Ž
.
n
1
sy
sin n ,
where xscos 
n
1
sy
sin  U
x ,
by 10.18
Ž .
Ž
.
ny1
n
1
2
'
sy
1yx U
x .
Ž .
ny1
n

CHAPTER 10
631
Ž .
Ž .
10.7. The first two Laguerre polynomials are L
x s1 and L
x sxy
0
1
y1, as can be seen from applying the Rodrigues formula in Section
Ž
.
10.6. Now, differentiating H x, t with respect to t, we obtain
"H x, t
q1
x
Ž
.
s
y
H x, t ,
Ž
.
2
" t
1yt
1yt
Ž
.
or equivalently,
"H x, t
Ž
.
2
1yt
q
xyy1 q q1 t H x, t s0.
Ž
.
Ž
.
Ž
.
Ž
.
" t
Ž .
Ž
.
Ž .
Hence,
g
x s H x, 0 s 1,
and
g
x sy"H x, t r" t ts0 s xy
Ž
.
0
1
Ž .
Ž .
Ž .
Ž .
y1. Thus, g
x sL
x
and g
x sL
x . Furthermore, if the
0
0
1
1
representation
n

y1
Ž
.
n
H x, t s
g
x t
Ž
.
Ž .
Ý
n
n!
ns0
is substituted in the above equation, and if the coefficient of t n in the
resulting series is equated to zero, we obtain
g
x q 2nyxqq1 g
x q n2qn g
x s0.
Ž .
Ž
.
Ž .
Ž
.
Ž .
nq1
n
ny1
This is the same relation connecting the Laguerre polynomials
Ž .
Ž .
Ž .
L
x , L
x , and L
x , which is given at the end of Section
nq1
n
ny1
Ž .
Ž .
10.6. Since we have already established that g
x sL
x for ns0, 1,
n
n
we conclude that the same relation holds for all values of n.
Ž
.
10.8. Using formula 10.40 , we have
3x 2
1

p
x sc qc xqc
y
Ž .
4
0
1
2ž
/
2
2
5x 3
3x
35x 4
30 x 2
3
qc
y
qc
y
q
,
3
4
ž
/
ž
/
2
2
8
8
8
where
1
1
x
c s
e dx,
H
0
2
y1
3
1
x
c s
xe dx,
H
1
2
y1
5
3x 2
1
1
x
c s
y
e dx,
H
2
ž
/
2
2
2
y1

SOLUTIONS TO SELECTED EXERCISES
632
7
5x 3
3x
1
x
c s
y
e dx,
H
3
ž
/
2
2
2
y1
9
35x 4
30 x 2
3
1
x
c s
y
q
e dx.
H
4
ž
/
2
8
8
8
y1
In computing c , c , c , c , c
we have made use of the fact that
0
1
2
3
4
2
1
2
p
x
dxs
,
ns0, 1, 2, . . . ,
Ž .
H
n
2nq1
y1
Ž .
Ž
where p
x
is the Legendre polynomial of degree n
see Section
n
.
10.2.1 .
10.9.
c0
f x f
qc T
x qc T
x qc T
x qc T
x
Ž .
Ž .
Ž .
Ž .
Ž .
1
1
2
2
3
3
4
4
2
s1.266066q1.130318T
x q0.271495T
x
Ž .
Ž .
1
2
q0.044337T
x q0.005474T
x
Ž .
Ž .
3
4
s1.000044q0.99731 xq0.4992 x 2q0.177344 x 3q0.043792 x 4
w Note: For more computational details, see Example 7.2 in Ralston
Ž
. x
and Rabinowitz 1978 .
Ž
.
10.10. Use formula 10.48 with the given values of the central moments.
10.11. The first six cumulants of the standardized chi-squared distribution
with five degrees of freedom are  s0,  s1,  s1.264911064,
1
2
3
 s2.40,  s6.0715731,  s19.2. We also have that z
s1.645.
4
5
6
0.05
Applying the Cornish-Fisher approximation for x
, we obtain the
0.05
Ž
 2
.
 2
value x
f1.921. Thus, P 
1.921 f0.05, where 
denotes
0.05
5
5
the standardized chi-squared variate with five degrees of freedom. If
2
Ž

denotes the nonstandardized chi-squared counterpart
with five
5
 2
2
'
.
degrees of freedom , then  s 10 
q5. Hence, the correspond-
5
5
2
'
Ž
.
ing approximate value of the upper 0.05 quantile of 
is
10 1.921
5
q5s11.0747. The actual table value is 11.07.
( )
10.12.
a
1
1
1
1
2
yt r2
e
dtf1y
q
y
H
2
3
231!
2 52!
2 73!
0
1
q
s0.85564649.
4
2 94!

CHAPTER 10
633
( )
b
x
x
1
x
2
2
yt r2
yx r8
e
dtfxe
%
q
%
H
0
2
ž /
ž /
2
3
2
0
1
x
1
x
q
%
q
%
,
4
6
ž /
ž /
5
2
7
2
where
x
1
x
%
s
H
0
0
ž /
ž /
2
1!
2
s1,
2
x
1
x
x
%
s
H
2
2
ž /
ž /
ž /
2
2!
2
2
2
2
1
x
x
s
y1 ,
ž / ž /
2!
2
2
4
4
2
x
1
x
x
x
%
s
y6
q3 ,
4ž /
ž / ž /
ž /
2
4!
2
2
2
6
6
4
2
x
1
x
x
x
x
%
s
y15
q45
y15 .
6ž /
ž / ž /
ž /
ž /
2
6!
2
2
2
2
Hence,
1
2
yt r2
e
dtf0.85562427.
H
0
Ž
.
10.13. Using formula 10.21 , we have that

n
yx 2 r2
d
e
Ž
.
2
n
x r2
g x s
b
y1
e
 x
Ž .
Ž
.
Ž .
Ý
n
n
dx
ns0

n
yx 2 r2
1
d
e
Ž
.
n
s
y1
b
,
by 10.44
Ž
.
Ž
.
Ý
n
n
'
dx
2
ns0

n
c
d  x
Ž .
n
s
,
Ý
n
n!
dx
ns0
Ž
.n
where c s y1
n!b , ns0, 1, . . .
n
n

SOLUTIONS TO SELECTED EXERCISES
634
10.14. The moment generating function of any one of the X 2’s is
i

2
t x
 t s
e
f x
dx
Ž .
Ž .
H
y
3
4
2
6


d  x

d  x

d  x
Ž .
Ž .
Ž .
2
3
4
3
t x
s
e
 x y
q
q
dx.
Ž .
H
3
4
6
6
24
72
dx
dx
dx
y
Ž
.
By formula 10.21 ,
dn x
Ž .
n
s y1
 x H
x ,
ns0, 1, 2, . . .
Ž
.
Ž .
Ž .
n
n
dx
Hence,



2
3
4
t x
 t s
e
 x q
 x H
x q
 x H
x
Ž .
Ž .
Ž .
Ž .
Ž .
Ž .
H
3
4
6
24
y
2
3
q
 x H
x
dx
Ž .
Ž .
6
72
2



2
4
3
t x
s2
e
 x q
 x H
x q
 x H
x
dx,
Ž .
Ž .
Ž .
Ž .
Ž .
H
4
6
24
72
0
Ž .
Ž .
Ž .
since H
x
is an odd function, and H
x
and H
x
are even
3
4
6
functions. It is known that

2
yx r2
2 ny1
ny1
e
x
dxs2
! n ,
Ž .
H
0
Ž .
where ! n is the gamma function


2
yx
ny1
yx
2 ny1
! n s
e
x
dxs2
e
x
dx,
Ž . H
H
0
0
n0. It is easy to show that


1
2
2
t x
m
yŽ x r2.Ž1y2 t.
m
e
 x x
dxs
e
x
dx
Ž .
H
H
'2
0
0
m
2
1
2
mq1
Ž
.
y
mq1
2
s
1y2t
!
,
Ž
.
ž
/
'
2
2 

CHAPTER 11
635
where m is an even integer. Hence,

1
2
y1r2
1
t x
2
e
 x
dxs
1y2t
!
Ž .
Ž
.
Ž .
H
2
'
0
y1r2
s 1y2t
,
Ž
.


2
2
t x
t x
4
2
2
e
 x H
x
dxs2
e
 x
x y6 x q3 dx
Ž .
Ž .
Ž . Ž
.
H
H
4
0
0
4
y5r2
5
s
1y2t
!
Ž
.
Ž .
2
'
12
y3r2
y1r2
3
y
1y2t
!
q3 1y2t
,
Ž
.
Ž
.
Ž .
2
'


2
2
t x
t x
6
4
2
2
e
 x H
x
dxs2
e
 x
x y15x q45x y15 dx
Ž .
Ž .
Ž . Ž
.
H
H
6
0
0
8
60
y7r2
y5r2
7
5
s
1y2t
!
y
1y2t
!
Ž
.
Ž
.
Ž .
Ž .
2
2
'
'


90
y3r2
y1r2
3
q
1y2t
!
y15 1y2t
.
Ž
.
Ž
.
Ž .
2
'
Ž .
The last three integrals can be substituted in the formula for  t .The
w
Ž .xn
moment generating function of W is  t
. On the other hand, the
moment generating function of a chi-squared distribution with n
Ž
.yn r2 w
degrees of freedom is 1y2t
see Example 6.9.8 concerning the
Ž
.
moment generating function of a gamma distribution G ,  , of
which the chi-squared distribution is a special case with snr,
x
s2 .
CHAPTER 11
( )
11.1.
a

1
a s
x dx
H
0

y

2
s
x dxs ,
H

0

1
a s
x cos nxdx
H
n

y

2
s
xcos nxdx
H

0

SOLUTIONS TO SELECTED EXERCISES
636

2
sy
sin nxdx
H
n
0
2
n
s
y1
y1 ,
Ž
.
2
 n

1
b s
x sin nxdx
H
n

y
s0,

4
cos 3x
cos 5x
x s
y
cos xq
q
q  .
2
2
ž
/
2

3
5
( )
b

1
a s
sin x dx
H
0

y

2
4
s
sin x dxs
,
H


0

1
a s
sin x cos nxdx
H
n

y

2
s
sin x cos nxdx
H

0

1
s
sin nq1 xysin ny1 x dx
Ž
.
Ž
.
H

0
n
y1
q1
Ž
.
sy2
,
n1,
2
 n y1
Ž
.
a s0,
1

1
b s
sin x sin nxdx
H
n

y
s0,
2
4
cos 2 x
cos 4 x
cos 6 x
sin x s
y
q
q
q  .
ž
/


3
15
35

CHAPTER 11
637
( )
c

1
2
a s
xqx
dx
Ž
.
H
0

y
2 2
s
,
3

1
2
a s
xqx
cos nxdx
Ž
.
H
n

y

2
2
s
x cos nxdx
H

0
4
4
n
s
cos ns y1
,
Ž
.
2
2
n
n

1
2
b s
xqx
sin nxdx
Ž
.
H
n

y

2
s
x sin nxdx
H

0
2
ny1
s y1
,
Ž
.
n
 2
1
2
xqx s
q4 ycos xq
sin x
ž
/
3
2
1
1
y4 y
cos 2 xq
sin 2 x q 
2
ž
/
4
2
1wŽ
for yx. When xs, the sum of the series is
yq
2
2.
Ž
2.x
2
2
2
Ž
2
2

q q
s , that is,  s r3q4 1q1r2 q1r3
.
q  .
11.2. From Example 11.2.2, we have
n
2


y1
Ž
.
2
x s
q4
cos nx.
Ý
2
3
n
ns1
Putting xs, we get
2


1
s
.
Ý
2
6
n
ns1

SOLUTIONS TO SELECTED EXERCISES
638
Using xs0, we obtain
n
2


y1
Ž
.
0s
q4
,
Ý
2
3
n
ns1
or equivalently,
nq1
2


y1
Ž
.
s
.
Ý
2
12
n
ns1
Adding the two series corresponding to  2r6 and  2r12, we get
2

3
1
s2
,
Ý
2
12
2ny1
Ž
.
ns1
that is,
2


1
s
.
Ý
2
8
2ny1
Ž
.
ns1
11.3. By Theorem 11.3.1, we have


f
x s
nb cos nxyna sin nx .
Ž .
Ž
.
Ý
n
n
ns1
Ž
.

Ž
2
2
2
2.
Furthermore, inequality 11.28 indicates that Ý
n b qn a
is a
ns1
n
n
Ž
convergent series. It follows that nb ™0 and na ™0 as n™ see
n
n
.
Result 5.2.1 in Section 5.2 .
11.4. For mn, we have
m
s
x ys
x
s
a cos kxqb sin kx
Ž .
Ž .
Ž
.
Ý
m
n
k
k
ksnq1
m
1r2
2
2
F
a qb
Ž
.
Ý
k
k
ksnq1
m
1
1r2
2
2
s
 q
see 11.27
Ž
.
Ž
.
Ý
k
k
k
ksnq1
1r2
1r2
m
m
1
2
2
F
 q
.
Ž
.
Ý
Ý
k
k
2
ž
/
k
ksnq1
ksnq1

CHAPTER 11
639
Ž
.
But, by inequality 11.28 ,
m

1
2

2
2
 q
F
f
x
dx
Ž .
Ž
.
Ý
H
k
k

y
ksnq1
and
m

1
1
F
Ý
Ý
2
2
k
k
ksnq1
ksnq1
 dx
1
F
s
.
H
2
n
x
n
Thus,
c
s
x ys
x
F
,
Ž .
Ž .
m
n
'n
where

1
2

2
c s
f
x
dx.
Ž .
H

y
Ž .
Ž .
Ž .
w
x
By Theorem 11.3.1 b , s
x ™f x
on y, 
as m™. Thus, by
m
letting m™, we get
c
f x ys
x
F
.
Ž .
Ž .
n
'n
( )
11.5.
a
From the proof of Theorem 11.3.2, we have

b
A
a 
n
0
0
cos ns
y
.
Ý
n
2
2
ns1

Ž
.n
This implies that the series Ý
y1
b rn is convergent.
ns1
n
( )
b
From the proof of Theorem 11.3.2, we have

x
a
qx
a
b
Ž
.
0
n
n
f t
dts
q
sin nxy
cos nxycos n
.
Ž .
Ž
.
Ý
H
2
n
n
y
ns1
Putting xs0, we get

a 
b
0
0
n
n
f t
dts
y
1y y1
.
Ž .
Ž
.
Ý
H
2
n
y
ns1

Ž
.w
Ž
.nx
This implies convergence of the series Ý
b rn 1y y1
.
ns1
n

Ž
.n
Ž .
Since Ý
y1
b rn is convergent by part
a , then so is
ns1
n
Ý
b rn.
ns1
n

SOLUTIONS TO SELECTED EXERCISES
640
Ž .

11.6. Using the hint and part
b
of Exercise 11.5, the series Ý
b rn
ns2
n
would
be convergent,
where
b s1rlog n. However, the
series
n

Ž
.
Ž
Ý
1r n log n is divergent by Maclaurin’s integral test see Theo-
ns2
.
rem 6.5.4 .
( )
11.7.
a
From Example 11.2.1, we have
nq1

x
y1
Ž
.
s
sin nx
Ý
2
n
ns1
for yx. Hence,
2
x
x
t
s
dt
H
4
2
0
nq1
nq1


y1
y1
Ž
.
Ž
.
sy
cos nxq
.
Ý
Ý
2
2
n
n
ns1
ns1
Note that
nq1
2


y1
1
x
Ž
.
s
dx
Ý
H
2
2
4
n
y
ns1
 2
s
.
12
Hence,
nq1
2
2

x

y1
Ž
.
s
y
cos nx,
yx .
Ý
2
4
12
n
ns1
( )
Ž .
b
This follows directly from part a .
11.8. Using the result in Exercise 11.7, we obtain
nq1
2
2

x
x

t
y1
Ž
.
y
dts
cos nt dt
Ý
H
H
2
ž
/
12
4
n
0
0
ns1
nq1

y1
Ž
.
s
sin nx,
yx .
Ý
3
n
ns1
Hence,
nq1
2
3

y1

x
Ž
.
sin nxs
xy
,
yx .
Ý
3
12
12
n
ns1

CHAPTER 11
641
11.9.

1
2
yx
yi w x
F w s
e
e
dx,
Ž
.
H
2
y

i
2

yx
yi w x
F
w s
y2 xe
e
dx
Ž
.
Ž
.
H
4
y
Žexchanging the order of integration and differentation is permissible
.
here by an extension of Theorem 7.10.1 . Integrating by parts, we
obtain


i
2
2

yx
yi w x
yx
yi w x
y
F
w s
e
e
y
e
yiwe
dx
Ž
.
Ž
.
H
4
y

yi
2
yx
yi w x
s
e
yiw e
dx
Ž
.
H
4
y

w
1
2
yx
yi w x
sy
e
e
dx
H
2 2
y
w
sy
F w .
Ž
.
2
The general solution of this differential equation is
F w sAeyw 2 r4 ,
Ž
.
where A is a constant. Putting ws0, we obtain

1
2
yx
AsF 0 s
e
dx
Ž .
H
2
y
'
1
s
s
.
'
2
2 
2
yw
r4
'
Ž
.
Ž
.
Hence, F w s 1r2  e
.
Ž
.
Ž
.Ž .
11.10. Let H w be the Fourier transform of
f  g
x . Then


1
yi w x
H w s
f xyy g y dy e
dx
Ž
.
Ž
. Ž .
H
H
2
y
y


1
yi w x
s
g y
f xyy e
dx dy
Ž .
Ž
.
H
H
2
y
y


1
yi wŽ xyy .
yi w y
s
f xyy e
dx g y e
dy
Ž
.
Ž .
H
H
2
y
y

yi w y
sF w
g y e
dy
Ž
.
Ž .
H
y
s2 F w G w .
Ž
.
Ž
.

SOLUTIONS TO SELECTED EXERCISES
642
11.11. Apply the results in Exercises 11.9 and 11.10 to find the Fourier
Ž .
transform of f x ; then apply the inverse Fourier transform theorem
Ž
.
Ž .
Theorem 11.6.3 to find f x .
( )
11.12.
a
kq1
n
2 y1

Ž
.
s
x
s
sin k y
Ž
.
Ý
n
n
ž
/
k
n
ks1
kq1
n
2 y1
k
Ž
.
kq1
s
y1
sin
Ž
.
Ý
ž /
k
n
ks1
n
2 sin krn
Ž
.
s
.
Ý
k
ks1
( )
b
n
sin krn

Ž
.
s
x
s2
.
Ž
.
Ý
n
n
krn
n
ks1
w
x
wŽ
By
dividing
the
interval
0, 
into
n
subintervals
k y
.
x
1 rn, krn , 1FkFn, each of length rn, it is easy to see that
Ž
.
Ž
.
Ž .
s
x
is a Riemann sum
S P, g
for the function
g x s
n
n
Ž
.
Ž .
2 sin x rx. Here, g x is evaluated at the right-hand end point of
Ž
.
each subinterval of the partition P see Section 6.1 . Thus,
 sin x
lim s
x
s2
dx.
Ž
.
H
n
n
x
n™
0
( )
11.13.
a
ys8.512q3.198 cos y0.922 sin q1.903 cos 2q3.017 sin 2.
ˆ
( )
b
Estimates of the locations of minimum and maximum resistance
w
are s0.7944 and s0.1153, respectively.
Note: For more
Ž
.
x
details, see Kupper 1972 , Section 5.
( )
11.15.
a
n
1

s s
Y yn
Ý
n
j
ž
/
'
	 n
js1
n
1
s
U ,
Ý
j
'
	 n
js1

CHAPTER 11
643
Ž
.
Ž
.
where U sY y, js1, 2, . . . , n. Note that E U s0 and Var U
j
j
j
j
s	 2. The characteristic function of s is
n

t sE eit s
n
Ž .
Ž
.
n
n
itÝ
U r	 n
'
js1
j
sE e
Ž
.
n
it U r	 n
'
j
s
E e
.
Ž
.
Ł
js1
Now,
t 2
it U
2
j
E e
sE 1qit U y
U q 
Ž
.
j
j
ž
/
2
t 2	 2
2
s1y
qo t
,
Ž
.
2
2
2
t
t
it U r	 n
'
j
E e
s1y
qo
.
Ž
.
ž /
2n
n
Hence,
n
2
2
t
t

t s 1y
qo
.
Ž .
n
ž /
2n
n
Let
t 2
t 2

 sy
qo
n
ž /
2n
n
2t
1
s
y
qo 1
,
Ž .
n
2
Ž .
y1r2qo 1
2t r
 n

t s
1q
Ž .
Ž
.
n
n
Ž .
y1r2
o 1
2
2
t r
t r
n
n
s
1q
1q
.
Ž
.
Ž
.
n
n
As n™, 
 ™0 and
n
y1r2
2
2
t r
yt r2
n
1q
™e
,
Ž
.
n
Ž .
o 1
2t r
 n
1q
™1.
Ž
.
n

SOLUTIONS TO SELECTED EXERCISES
644
Thus,

t ™eyt 2 r2
as n™.
Ž .
n
( )
yt 2 r2
b
e
is the characteristic function of the standard normal distri-
bution Z. Hence, by Theorem 11.7.3, as n™, s™Z.
n
CHAPTER 12
( )
12.1.
a
I fS
n
n
ny1
h
s
log1qlog nq2
log x
Ý
i
ž
/
2
is1
ny1
ny1
s
log nq2
log i
Ý
ž
/
2 ny1
Ž
.
is2
1
s log nqlog 2qlog3q  qlog ny1
Ž
.
2
1
s logn qlog n! ylogn
Ž
.
2
1
slog n! y log n.
Ž
.
2
1
( )
Ž
.
b
n log nynq1flog n! y log n. Hence,
2
1
log n! f nq
log nynq1
Ž
.
Ž
.
2
1
n!fexp
nq
log nynq1
Ž
.
2
se eynnnq1r2 seynq1 nnq1r2 .
Ž
.
12.2. Applying formula 12.10 , we obtain the following approximate values
1
Ž
.
for the integral H dxr 1qx :
0
n
Approximation
2
0.69325395
4
0.69315450
8
0.69314759
16
0.69314708
The exact value of the integral is log2s0.69314718. Using formula
Ž
.
12.12 , the error of approximation is less than or equal to
5
M
bya
M
Ž
.
4
4
s
,
4
4
2880n
2880 8
Ž .

CHAPTER 12
645
Ž4.
Ž .
where M is an upper bound on
f
x
for 0FxF1. Here, f x s
Ž .
4
Ž
.
1r 1qx , and
24
Ž4.
f
x s
.
Ž .
5
1qx
Ž
.
Hence, M s24, and
4
M4
f0.000002.
4
2880 8
Ž .
12.3. Let
2yr2

zs
,
0FF
,
r2
2

s
1qz ,
y1FzF1.
Ž
.
4
Then


r2
1
sin  ds
sin
1qz
dz
Ž
.
H
H
4
4
0
y1
2


f

 sin
1qz
.
Ž
.
Ý
i
i
4
4
is0
3
Using the information in Example 12.4.1, we have z sy
, z s0,
'
0
1
5
3
5
8
5
z s
; 
 s , 
 s , 
 s . Hence,
'
2
0
1
2
5
9
9
9

5

3
8

r2sin  df
sin
1y
q
sin
(
H
½
ž
/
4
9
4
5
9
4
0
5

3
q
sin
1q(
5
ž
/
9
4
5
s1.0000081.
1
wŽ
.Ž
.x
The error of approximation associated with H
sin r4 1qz
dz is
y1
Ž
.
Ž .
obtained by applying formula
12.16
with asy1, bs1, f z s
wŽ
.Ž
.x
Ž6.Ž
.
Ž
.6
wŽ
.Ž
.x
sin r4 1qz ,
ns2. Thus,
f
 sy r4 sin r4 1q
,
y11, and the error is therefore less than
4
7
6
2
3!

Ž
.
.
3 ž /
4
7 6!
Ž
.

SOLUTIONS TO SELECTED EXERCISES
646
Consequently, the error associated with H r2sin  d is less than
0
4
4
7
6
7
w
x
w
x
 2
3!


3!
s
3
3
ž /
ž /
4
4
2
w
x
w
x
7 6!
7 6!
s0.0000117.
( )
12.4.
a
The inequality is obvious from the hint.
( )
b
Let ms3. Then,
1
1
2
ym
y9
e
s
e
m
3
s0.0000411.
( )
w
x
c
Apply Simpson’s method on 0, 3 with h0.2. Here,
f Ž4. x s 12y48 x 2q16 x 4 eyx 2,
Ž .
Ž
.
w
x
with a maximum of 12 at xs0 over 0, 3 . Hence, M s12. Using
4
Ž
.
Ž
.
the fact that hs bya r2n, we get ns bya r2h3r0.4s7.5.
Ž
.
Formula 12.12 , with ns8, gives
5
5
nM h
8 12
0.2
Ž
. Ž
.
4

90
90
s0.00034.
Hence, the total error of approximation for H eyx 2 dx is less than
0
0.0000411q0.00034s0.00038. This makes the approximation
correct to three decimal places.
12.5.


yx
x
x e
dxs
dx,
H
H
x
yx
y2 x
yx
e qe
y1
1qe
ye
0
0
n
xi
f

,
Ý
i
y2 x
yx
i
i
1qe
ye
is0
Ž .
where the x ’s are the zeros of the Laguerre polynomial L
x .
i
nq1
Using the entries in Table 12.1 for ns1, we get

x
0.58579
dxf0.85355
H
x
yx
y2Ž0.58579.
y0 .58579
e qe
y1
1qe
ye
0
3.41421
q0.14645
s1.18.
y2Ž3.41421.
y3 .41421
1qe
ye

CHAPTER 12
647
( )
Ž
.
12.6.
a
Let us 2tyx rx. Then
1
x
1
I x s
du
Ž .
H
2
1
2
2
y1 1q x
uq1
Ž
.
4
1
1
s2 x
du.
H
2
2
y1 4qx
uq1
Ž
.
( )
Ž .
b
Applying a GaussLegendre approximation of I x with ns4, we
obtain
4

i
I x f2 x
,
Ž .
Ý
2
2
4qx
u q1
Ž
.
is0
i
where u , u , u , u , and u
are the five zeros of the Legendre
0
1
2
3
4
Ž .
polynomial P u
of degree 5. Values of these zeros and the
5
corresponding 
 ’s are given below:
i
u s0.90617985,

 s0.23692689,
0
0
u s0.53846931,

 s0.47862867,
1
1
u s0,

 s0.56888889,
2
2
u sy0.53846931,

 s0.47862867,
3
3
u sy0.90617985,

 s0.23692689,
4
4
Ž
.
see Table 7.5 in Shoup, 1984, page 202 .
1Ž
.
1
 log cos x
12.7. H cos x
dxsH e
dx. Using the method of Laplace, the func-
0
0
Ž .
w
x
tion h x slogcos x has a single maximum in 0, 1 at xs0. Formula
Ž
.
12.28 gives
1r2
y
1  log cos x
e
dx
H

2h
0
Ž .
0
as ™, where
1

h
0 sy
Ž .
2
cos x
xs0
sy1.
Hence,

1  log cos x
e
dx
.
H
(2
0

SOLUTIONS TO SELECTED EXERCISES
648
12.9.

2 1r2
1
Ž
.
1r2
1yx 1
2
2
1yx yx
dx dx s
Ž
.
H H
1
2
1
2
6
0
0
f0.52359878.
Applying the 16-point GaussLegendre rule to both formulas gives
the approximate value 0.52362038.
w
Ž
. x
Note: For more details, see Stroud 1971, Section 1.6 .
12.10.


yn
2
2
yn log Ž1qx .
1qx
dxs
e
dx
Ž
.
H
H
0
0
Ž .
Apply the method of Laplace with ' x s1 and
h x sylog 1qx 2 .
Ž .
Ž
.
w
.
This function has a single maximum in 0,  at xs0. Using formula
Ž
.
12.28 , we get
1r2

y
yn
2
1qx
dx
,
Ž
.
H

2n h
0
Ž .
0
Ž .
where h 0 sy2. Hence,


1

yn
2
1qx
dx
s
.
Ž
.
H
(
(
4n
2
n
0
12.11.
Number of
Approximate Value
Quadrature Points
of Variance
2
0.1175
4
0.2380
8
0.2341
16
0.1612
32
0.1379
64
0.1372
w
Ž
. x
Source: Example 16.6.1 in Lange 1999 .
12.12. If xG0, then
x
1
1
0
2
2
yt r2
yt r2
 x s
e
dtq
e
dt
Ž .
H
H
'
'
2
2
y
0
x
1
1
2
yt r2
s
q
e
dt.
H
'
2
2
0

CHAPTER 12
649
Ž
.
Using Maclaurin’s series see Section 4.3 , we obtain
2
2
2
x
1
1
1
t
1
t
 x s
q
1q
y
q
y
Ž .
H
ž
/
ž
/
'
2
1!
2
2!
2
2
0
n
3
2
2
1
t
1
t
q
y
q  q
y
q 
dt
ž
/
ž
/
3!
2
n!
2
3
5
7
1
1
t
t
t
s
q
ty
q
y
2
3
'
2
321!
52 2!
72 3!
2
x
2 nq1
t
n
q  q y1
q 
Ž
.
n
2nq1 2 n!
Ž
.
0
2
4
6
1
x
x
x
x
s
q
1y
q
y
2
3
'
2
321!
52 2!
72 3!
2
2 n
x
n
q  q y1
q  .
Ž
.
n
2nq1 2 n!
Ž
.
12.13. From Exercise 12.12 we need to find values of a, b, c, d such that
x 2
x 4
x6
x8
2
4
2
4
1qax qbx f 1qcx qdx
1y
q
y
q
.
Ž
.ž
/
6
40
336
3456
Equating the coefficients of x 2, x 4, x6, x8 on both sides yields four
equations in a, b, c, d. Solving these equations, we get
17
739
95
55
as
,
bs
,
cs
,
ds
.
468
196560
468
4368
w
Ž
. x
Note: For more details, see Morland 1998 .
12.14.
x 1
ysG x s
1qt
dt
Ž .
Ž
.
H 4
0
1
x 2
s
xq
,
0FxF2.
ž
/
4
2
Ž .
w
x
Ž
.1r2
The only solution of ysG x in 0, 2 is xsy1q 1q8 y
, 0Fy

SOLUTIONS TO SELECTED EXERCISES
650
Ž .
F1. A random sample of size 150 from the g x distribution is given
by
1r2
x sy1q 1q8 y
,
is1, 2, . . . , 150,
Ž
.
i
i
Ž
.
where the y ’s form a random sample from the U 0, 1 distribution.
i
Ž
.
Applying formula 12.43 , we get
150
x 2
i
4
e
ˆI
s
Ý
150
150
1qxi
is1
s16.6572.
Ž
.
Using now formula 12.36 , we obtain
150
2
2
ui
ˆI
s
e
Ý
150
150 is1
s17.8878,
Ž
.
where the u ’s form a random sample from the U 0, 2 distribution.
i
12.15. As n™,
Ž
.
ynr2
y1r2
y nq1 r2
2
2
2
x
x
x
1q
s 1q
1q
n
n
n
eyx 2 r2.
Ž
.
Furthermore, by applying Stirling’s formula formula 12.31 , we obtain
Ž
.
1r2
ny1 r2
nq1
ny1
ny1
1Ž
.
y
ny1
2
!
e
2
,
ž
/
ž
/
2
2
2
Ž
.
1r2
ny2 r2
n
ny2
ny2
1Ž
.
y
ny2
2
!
e
2
.
ž /
ž
/
2
2
2

CHAPTER 12
651
Hence,
nq1
!
nr2
1r2
ž
/
1
1
ny1
ny2
ny1
2

1r2
n
'
ny2
ny2
e
ny1
n
'n !
2
ž /
(
2
2
nr2
1
1
1
1y
ny2
Ž
.
n

1r2
nr2
2
'
e
ny1
n
1y
Ž
.
n
2(
2
y1r2
1
1
e
n

(
1r2
y1
'
2
e
e
n
1
s
.
'2
Hence, for large n,
x
1
2
yt r2
F x f
e
dt.
Ž .
H
'2
y

General Bibliography
Ž
.
Abramowitz, M., and I. A. Stegun 1964 . Handbook of Mathematical Functions with
Formulas, Graphs, and Mathematical Tables. Wiley, New York.
Ž
.
Abramowitz, M., and I. A. Stegun, eds. 1972 . Handbook of Mathematical Functions
with Formulas, Graphs, and Mathematical Tables. Wiley, New York.
Ž
.
Adby, P. R., and M. A. H. Dempster 1974 . Introduction to Optimization Methods.
Chapman and Hall, London.
Ž
.
Agarwal, G. G., and W. J. Studden 1978 . ‘‘Asymptotic design and estimation using
linear splines.’’ Comm. Statist. Simul. Comput., 7, 309319.
Ž
.
Alvo, M., and P. Cabilio 2000 . ‘‘Calculation of hypergeometric probabilities using
Chebyshev polynomials.’’ Amer. Statist., 54, 141144.
Ž
.
Anderson, T. W., and S. D. Gupta 1963 . ‘‘Some inequalities on characteristic roots
of matrices.’’ Biometrika, 50, 522524.
Ž
.
Anderson, T. W., I. Olkin, and L. G. Underhill
1987 . ‘‘Generation of random
orthogonal matrices.’’ SIAM J. Sci. Statist. Comput., 8, 625629.
Ž
.
Anderson-Cook, C. M. 2000 . ‘‘A second order model for cylindrical data.’’ J. Statist.
Comput. Simul., 66, 5156.
Ž
.
Apostol, T. M.
1964 .
Mathematical Analysis. Addison-Wesley, Reading, Mas-
sachusetts.
Ž
.
Ash, A., and A. Hedayat
1978 . ‘‘An introduction to design optimality with an
overview of the literature.’’ Comm. Statist. Theory Methods, 7, 12951325.
Ž
.
Atkinson, A. C. 1982 . ‘‘Developments in the design of experiments.’’ Internat. Statist.
Re®., 50. 161177.
Ž
.
Atkinson, A. C.
1988 . ‘‘Recent developments in the methods of optimum and
related experimental designs.’’ Internat. Statist. Re®., 56, 99115.
Ž
.
Basilevsky, A. 1983 . Applied Matrix Algebra in the Statistical Sciences. North-Holland,
New York.
Ž
.
Bates, D. M., and D. G. Watts 1988 . Nonlinear Regression Analysis and its Applica-
tions. Wiley, New York.
Ž
.
Bayne, C. K., and I. B. Rubin 1986 . Practical Experimental Designs and Optimization
Methods for Chemists. VCH Publishers, Deerfield Beach, Florida.
Ž
.
Bellman, R. 1970 . Introduction to Matrix Analysis, 2nd ed. McGraw-Hill, New York.
652

GENERAL BIBLIOGRAPHY
653
Ž
.
Belsley, D. A., E. Kuh, and R. E. Welsch 1980 . Regression Diagnostics. Wiley, New
York.
Ž
.
Bickel, P. J., and K. A. Doksum
1977 . Mathematical Statistics. Holden-Day, San
Francisco.
Ž
.
Biles, W. E., and J. J. Swain
1980 . Optimization and Industrial Experimentation.
Wiley-Interscience, New York.
Ž
.
Bloomfield, P. 1976 . Fourier Analysis of Times Series: An Introduction. Wiley, New
York.
Ž
.
Blyth, C. R. 1990 . ‘‘Minimizing the sum of absolute deviations.’’ Amer. Statist., 44,
329.
Ž
.
Bohachevsky, I. O., M. E. Johnson, and M. L. Stein 1986 . ‘‘Generalized simulated
annealing for function optimization.’’ Technometrics, 28, 209217.
Ž
.
Box, G. E. P. 1982 . ‘‘Choice of response surface design and alphabetic optimality.’’
Utilitas Math., 21B, 1155.
Ž
.
Box, G. E. P., and D. W. Behnken 1960 . ‘‘Some new three level designs for the study
of quantitative variables.’’ Technometrics, 2, 455475.
Ž
.
Box, G. E. P., and D. R. Cox 1964 . ‘‘An analysis of transformations.’’ J. Roy. Statist.
Soc. Ser. B, 26, 211243.
Ž
.
Box, G. E. P., and N. R. Draper
1959 . ‘‘A basis for the selection of a response
surface design.’’ J. Amer. Statist. Assoc., 54, 622654.
Ž
.
Box, G. E. P., and N. R. Draper
1963 . ‘‘The choice of a second order rotatable
design.’’ Biometrika, 50, 335352.
Ž
.
Box, G. E. P., and N. R. Draper
1965 . ‘‘The Bayesian estimation of common
parameters from several responses.’’ Biometrika, 52, 355365.
Ž
.
Box, G. E. P., and N. R. Draper
1987 . Empirical Model-Building and Response
Surfaces. Wiley, New York.
Ž
.
Box, G. E. P., and H. L. Lucas 1959 . ‘‘Design of experiments in nonlinear situations.’’
Biometrika, 46, 7790.
Ž
.
Box, G. E. P., and K. B. Wilson 1951 . ‘‘On the experimental attainment of optimum
conditions.’’ J. Roy. Statist. Soc. Ser. B, 13, 145.
Ž
.
Box, G. E. P., W. G. Hunter, and J. S. Hunter
1978 . Statistics for Experimenters.
Wiley, New York.
Ž
.
Boyer, C. B. 1968 . A History of Mathematics. Wiley, New York.
Ž
.
Ž
Bronshtein, I. N., and K. A. Semendyayev 1985 . Handbook of Mathematics. English
.
translation edited by K. A. Hirsch. Van Nostrand Reinhold, New York.
Ž
.
Brownlee, K. A. 1965 . Statistical Theory and Methodology, 2nd ed. Wiley, New York.
Ž
.
Buck, R. C. 1956 . Ad®anced Calculus. McGraw-Hill, New York.
Ž
.
Bunday, B. D.
1984 . Basic Optimization Methods. Edward Arnold, Victoria, Aus-
tralia.
Ž
.
Buse, A., and L. Lim
1977 . ‘‘Cubic splines as a special case of restricted least
squares.’’ J. Amer. Statist. Assoc., 72, 6468.
Ž
.
Bush, K. A., and I. Olkin 1959 . ‘‘Extrema of quadratic forms with applications to
statistics.’’ Biometrika, 46, 483486.

GENERAL BIBLIOGRAPHY
654
Ž
.
Carslaw, H. S. 1930 . Introduction to the Theory of Fourier Series and Integrals, 3rd ed.
Dover, New York.
Ž
.
Chatterjee, S., and B. Price 1977 . Regression Analysis by Example. Wiley, New York.
Ž
.
Cheney, E. W. 1982 . Introduction to Approximation Theory, 2nd ed. Chelsea, New
York.
Ž
.
Chihara, T. S. 1978 . An Introduction to Orthogonal Polynomials. Gordon and Breach,
New York.
Ž
.
Churchill, R. V.
1963 .
Fourier Series and Boundary Value Problems, 2nd ed.
McGraw-Hill, New York.
Ž
.
Cochran, W. G. 1963 . Sampling Techniques, 2nd ed. Wiley, New York.
Ž
.
Conlon, M. 1991 . ‘‘The controlled random search procedure for function optimiza-
tion.’’ Personal communication.
Ž
.
Conlon, M., and A. I. Khuri
1992 . ‘‘Multiple response optimization.’’ Technical
Report, Department of Statistics, University of Florida, Gainesville, Florida.
Ž
.
Cook, R. D., and C. J. Nachtsheim 1980 . ‘‘A comparison of algorithms for construct-
ing exact D-optimal designs.’’ Technometrics, 22, 315324.
Ž
.
Cooke, W. P. 1988 . ‘‘L’Hopital’s rule in a Poisson derivation.’’ Amer. Math. Monthly,
ˆ
95, 253254.
Ž
.
Copson, E. T. 1965 . Asymptotic Expansions. Cambridge University Press, London.
Ž
.
Cornish, E. A., and R. A. Fisher 1937 . ‘‘Moments and cumulants in the specification
of distributions.’’ Re®. Internat. Statist. Inst., 5, 307320.
Ž
.
Corwin, L. J., and R. H. Szczarba 1982 . Multi®ariable Calculus. Marcel Dekker, New
York.
Ž
.
Courant, R., and F. John 1965 . Introduction to Calculus and Analysis, Vol. 1. Wiley,
New York.
Ž
.
Cramer, H.
1946 . Mathematical Methods of Statistics. Princeton University Press,
´
Princeton.
Ž
.
Daniels, H. 1954 . ‘‘Saddlepoint approximation in statistics.’’ Ann. Math. Statist., 25,
631650.
Ž
.
Dasgupta, P.
1968 . ‘‘An approximation to the distribution of sample correlation
coefficient, when the population is non-normal.’’ Sankhya, Ser. B., 30, 425428.
Ž
.
Davis, H. F. 1963 . Fourier Series and Orthogonal Functions. Allyn&Bacon, Boston.
Ž
.
Davis, P. J. 1975 . Interpolation and Approximation. Dover, New York
Ž
.
Davis, P. J., and P. Rabinowitz 1975 . Methods of Numerical Integration. Academic
Press, New York.
Ž
.
De Boor, C. 1978 . A Practical Guide to Splines. Springer-Verlag, New York.
Ž
.
DeBruijn, N. G.
1961 . Asymptotic Methods in Analysis, 2nd ed. North-Holland,
Amesterdam.
Ž
.
DeCani, J. S., and R. A. Stine 1986 . ‘‘A note on deriving the information matrix for
a logistic distribution.’’ Amer. Statist., 40, 220222.
Ž
.
Dempster, A. P., N. M. Laird, and D. B. Rubin 1977 . ‘‘Maximum likelihood from
incomplete data via the EM algorithm.’’ J. Roy. Statist. Soc. Ser. B, 39, 138.
Ž
.
Divgi, D. R.
1979 . ‘‘Calculation of univariate and bivariate normal probability
functions.’’ Ann. Statist., 7, 903910.

GENERAL BIBLIOGRAPHY
655
Ž
.
Draper, N. R. 1963 . ‘‘Ridge analysis of response surfaces.’’ Technometrics, 5, 469479.
Ž
.
Draper, N. R., and A. M. Herzberg
1987 . ‘‘A ridge-regression sidelight.’’ Amer.
Statist., 41, 282283.
Ž
.
Draper, N. R., and H. Smith 1981 . Applied Regression Analysis, 2nd ed. Wiley, New
York.
Ž
.
Draper, N. R., I. Guttman, and P. Lipow 1977 . ‘‘All-bias designs for spline functions
joined at the axes.’’ J. Amer. Statist. Assoc., 72, 424429.
Ž
.
Dugundji, J. 1966 . Topology. Allyn and Bacon, Boston.
Ž
.
Durbin, J., and G. S. Watson 1950 . ‘‘Testing for serial correlation in least squares
regression, I.’’ Biometrika, 37, 409428.
Ž
.
Durbin, J., and G. S. Watson 1951 . ‘‘Testing for serial correlation in least squares
regression, II.’’ Biometrika, 38, 159178.
Ž
.
Eggermont, P. P. B.
1988 . ‘‘Noncentral difference quotients and the derivative.’’
Amer. Math. Monthly, 95, 551553.
Ž
.
Eubank, R. L. 1984 . ‘‘Approximate regression models and splines.’’ Comm. Statist.
Theory Methods, 13, 433484.
Ž
.
Evans, M., and T. Swartz 1995 . ‘‘Methods for approximating integrals in statistics
with special emphasis on Bayesian integration problems.’’ Statist. Sci., 10, 254272.
Ž
.
Everitt, B. S.
1987 . Introduction to Optimization Methods and Their Application in
Statistics. Chapman and Hall, London.
Ž
.
Eves, H. 1976 . An Introduction to the History of Mathematics, 4th ed. Holt, Rinehart
and Winston, New York.
Ž
.
Fedorov, V. V. 1972 . Theory of Optimal Experiments. Academic Press, New York.
Ž
.
Feller, W. 1968 . An Introduction to Probability Theory and Its Applications, Vol. I, 3rd
ed. Wiley, New York.
Ž
.
Fettis, H. E. 1976 . ‘‘Fourier series expansions for Pearson Type IV distributions and
probabilities.’’ SIAM J. Applied Math., 31, 511518.
Ž
.
Fichtali, J., F. R. Van De Voort, and A. I. Khuri 1990 . ‘‘Multiresponse optimization
of acid casein production.’’ J. Food Process Eng., 12, 247258.
Ž
.
Fisher, R. A., and E. Cornish 1960 . ‘‘The percentile points of distribution having
known cumulants.’’ Technometrics, 2, 209225.
Ž
.
Fisher, R. A., A. S. Corbet, and C. B. Williams 1943 . ‘‘The relation between the
number of species and the number of individuals in a random sample of an
animal population.’’ J. Anim. Ecology, 12, 4258.
Ž
.
Fisz, M.
1963 . Probability Theory and Mathematical Statistics, 3rd ed. Wiley, New
York.
Ž
.
Fletcher, R. 1987 . Practical Methods of Optimization, 2nd ed. Wiley, New York.
Ž
.
Fletcher, R., and M. J. D. Powell 1963 . ‘‘A rapidly convergent descent method for
minimization.’’ Comput. J., 6, 163168.
Ž
.
Flournoy, N., and R. K. Tsutakawa, eds.
1991 . Statistical Multiple Integration,
Contemporary Mathematics 115. Amer. Math. Soc., Providence, Rhode Island.
Ž
.
Freud, G. 1971 . Orthogonal Polynomials. Pergamon Press, Oxford.
Ž
.
Fulks, W. 1978 . Ad®anced Calculus, 3rd ed. Wiley, New York.
Ž
.
Fuller, W. A. 1976 . Introduction to Statistical Time Series. Wiley, New York.

GENERAL BIBLIOGRAPHY
656
Ž
.
Gallant, A. R., and W. A. Fuller 1973 . ‘‘Fitting segmented polynomial regression
models whose join points have to be estimated.’’ J. Amer. Statist. Assoc., 68,
144147.
Ž
.
Gantmacher, F. R. 1959 . The Theory of Matrices, Vols. I and II. Chelsea, New York.
Ž
.
Georgiev, A. A.
1984 . ‘‘Kernel estimates of functions and their derivatives with
applications.’’ Statist. Prob. Lett., 2, 4550.
Ž
.
Geyer, C. J. 1992 . ‘‘Practical Markov chain Monte Carlo.’’ Statist. Sci., 7, 473511.
Ž
.
Ghazal, G. A.
1994 . ‘‘Moments of the ratio of two dependent quadratic forms.’’
Statist. Prob. Lett., 20, 313319.
Ž
.
Gibaldi, M., and D. Perrier 1982 . Pharmacokinetics, 2nd ed. Dekker, New York.
Ž
.
Gillespie, R. P. 1954 . Partial Differentiation. Oliver and Boyd, Edinburgh.
Ž
.
Gillespie, R. P. 1959 . Integration. Oliver and Boyd, London.
Ž
.
Golub, G. H., and C. F. Van Loan
1983 . Matrix Computations. Johns Hopkins
University Press, Baltimore.
Ž
.
Good, I. J. 1969 . ‘‘Some applications of the singular decomposition of a matrix.’’
Technometrics, 11, 823831.
Ž
.
Goutis, C., and G. Casella 1999 . ‘‘Explaining the saddlepoint approximation.’’ Amer.
Statist., 53, 216224.
Ž
.
Graybill, F. A. 1961 . An Introduction to Linear Statistical Models, Vol. I. McGraw-Hill,
New York.
Ž
.
Graybill, F. A. 1976 . Theory and Application of the Linear Model. Duxbury, North
Scituate, Massachusetts.
Ž
.
Graybill, F. A.
1983 . Matrices with Applications in Statistics, 2nd ed. Wadsworth,
Belmont, California.
Ž
.
Gurland, J. 1953 . ‘‘Distributions of quadratic forms and ratios of quadratic forms.’’
Ann. Math. Statist., 24, 416427.
Ž
.
Haber, S.
1970 . ‘‘Numerical evaluation of multiple integrals.’’ SIAM Re®., 12,
481526.
Ž
.
Hall, C. A. 1968 . ‘‘On error bounds for spline interpolation.’’ J. Approx. Theory, 1,
209218.
Ž
.
Hall, C. A., and W. W. Meyer
1976 . ‘‘Optimal error bounds for cubic spline
interpolation.’’ J. Approx. Theory, 16, 105122.
Ž
.
Hardy, G. H. 1955 . A Course of Pure Mathematics, 10th ed. The University Press,
Cambridge, England.
Ž
.
Hardy, G. H., J. E. Littlewood, and G. Polya 1952 . Inequalities, 2nd ed. Cambridge
´
University Press, Cambridge, England.
Ž
.
Harris, B. 1966 . Theory of Probability. Addison-Wesley, Reading, Massachusetts.
Ž
.
Hartig, D.
1991 . ‘‘L’Hopital’s rule via integration.’’ Amer. Math. Monthly, 98,
ˆ
156157.
Ž
.
Hartley, H. O., and J. N. K. Rao 1967 . ‘‘Maximum likelihood estimation for the
mixed analysis of variance model.’’ Biometrika, 54, 93108.
Ž
.
Healy, M. J. R. 1986 . Matrices for Statistics. Clarendon Press, Oxford, England.

GENERAL BIBLIOGRAPHY
657
Ž
.
Heiberger, R.M., P. F. Velleman, and M. A. Ypelaar 1983 . ‘‘Generating test data
with independently controllable features for multivariate general linear forms.’’
J. Amer. Statist. Assoc., 78, 585595.
Ž
.
Henderson, H. V., and S. R. Searle
1981 . ‘‘The vec-permutation matrix, the vec
operator and Kronecker products: A review.’’ Linear and Multilinear Algebra, 9,
271288.
Ž
.
Henderson, H. V., and F. Pukelsheim, and S. R. Searle 1983 . ‘‘On the history of the
Kronecker product.’’ Linear and Multilinear Algebra, 14, 113120.
Ž
.
Henle, J. M., and E. M. Kleinberg
1979 . Infinitesimal Calculus. The MIT Press,
Cambridge, Massachusetts.
Ž
.
Hillier, F. S., and G. J. Lieberman
1967 . Introduction to Operations Research.
Holden-Day, San Francisco.
Ž
.
Hirschman, I. I., Jr. 1962 . Infinite Series. Holt, Rinehart and Winston, New York.
Ž
.
Hoerl, A. E. 1959 . ‘‘Optimum solution of many variables equations.’’ Chem. Eng.
Prog., 55, 6978.
Ž
.
Hoerl, A. E., and R. W. Kennard 1970a . ‘‘Ridge regression: Biased estimation for
non-orthogonal problems.’’ Technometrics, 12, 5567.
Ž
.
Hoerl, A. E., and R. W. Kennard
1970b . ‘‘Ridge regression: Applications to
non-orthogonal problems.’’ Technometrics, 12, 6982; correction, 12, 723.
Ž
.
Hogg, R. V., and A. T. Craig 1965 . Introduction to Mathematical Statistics, 2nd ed.
Macmillan, New York.
Ž
.
Huber, P. J. 1973 . ‘‘Robust regression: Asymptotics, conjectures and Monte Carlo.’’
Ann. Statist. 1, 799821.
Ž
.
Huber, P. J. 1981 . Robust Statistics. Wiley, New York.
Ž
.
Hyslop, J. M. 1954 . Infinite Series, 5th ed. Oliver and Boyd, Edinburgh, England.
Ž
.
Jackson, D. 1941 . Fourier Series and Orthogonal Polynomials. Mathematical Associa-
tion of America, Washington.
Ž
.
James, A. T. 1954 . ‘‘Normal multivariate analysis and the orthogonal group.’’ Ann.
Math. Statist., 25, 4075.
Ž
.
James, A. T., and R. A. J. Conyers 1985 . ‘‘Estimation of a derivative by a difference
quotient: Its application to hepatocyte lactate metabolism.’’ Biometrics, 41,
467476.
Ž
.
Johnson, M. E., and C. J. Nachtsheim 1983 . ‘‘Some guidelines for constructing exact
D-optimal designs on convex design spaces.’’ Technometrics, 25, 271277.
Ž
.
Johnson, N. L., and S. Kotz
1968 . ‘‘Tables of distributions of positive definite
quadratic forms in central normal variables.’’ Sankhya, Ser. B, 30, 303314.
Ž
.
Johnson, N. L., and S. Kotz 1969 . Discrete Distributions. Houghton Mifflin, Boston.
Ž
.
Johnson, N. L., and S. Kotz 1970a . Continuous Uni®ariate Distributions1. Houghton
Mifflin, Boston.
Ž
.
Johnson, N. L., and S. Kotz 1970b . Continuous Uni®ariate Distributions2. Houghton
Mifflin, Boston.
Ž
.
Johnson, P. E. 1972 . A History of Set Theory. Prindle, Weber, and Schmidt, Boston.
Ž
.
Jones, E. R., and T. J. Mitchell 1978 . ‘‘Design criteria for detecting model inade-
quacy.’’ Biometrika, 65, 541551.

GENERAL BIBLIOGRAPHY
658
Ž
.
Judge, G. G., W. E. Griffiths, R. C. Hill, and T. C. Lee
1980 . The Theory and
Practice of Econometrics. Wiley, New York.
Kahaner, D. K. 1991. ‘‘A survey of existing multidimensional quadrature routines.’’ In
Statistical Multiple Integration, Contemporary Mathematics 115, N. Flournoy and
R. K. Tsutakawa, eds. Amer. Math. Soc., Providence, pp. 922.
Ž
.
Kaplan, W.
1991 .
Ad®anced Calculus, 4th ed. Addison-Wesley, Redwood City,
California.
Ž
.
Kaplan, W., and D. J. Lewis 1971 . Calculus and Linear Algebra, Vol. II. Wiley, New
York.
Ž
.
Karson, M. J., A. R. Manson, and R. J. Hader 1969 . ‘‘Minimum bias estimation and
experimental design for response surfaces.’’ Technometrics, 11, 461475.
Ž
.
Kass, R. E., L. Tierney, and J. B. Kadane
1991 . ‘‘Laplace’s method in Bayesian
analysis.’’ In Statistical Multiple Integration, Contemporary Mathematics 115,
N. Flournoy and R. K. Tsutakawa, eds. Amer. Math. Soc., Providence, pp. 8999.
Ž
.
Katz, D., and D. Z. D’Argenio 1983 . ‘‘Experimental design for estimating integrals
by numerical quadrature, with applications to pharmacokinetic studies.’’ Biomet-
rics, 39, 621628.
Ž
.
Kawata, T. 1972 . Fourier Analysis in Probability Theory. Academic Press, New York.
Ž
.
Kendall, M., and A. Stuart 1977 . The Ad®anced Theory of Statistics, Vol. 1, 4th ed.
Macmillian, New York.
Ž
.
Kerridge, D. F., and G. W. Cook 1976 . ‘‘Yet another series for the normal integral.’’
Biometrika, 63, 401403.
Ž
.
Khuri, A. I. 1982 . ‘‘Direct products: A powerful tool for the analysis of balanced
data.’’ Comm. Statist. Theory Methods, 11, 29032920.
Ž
.
Khuri, A. I. 1984 . ‘‘Interval estimation of fixed effects and of functions of variance
components in balanced mixed models.’’ Sankhya, Ser. B, 46, 1028.
Ž
.
Khuri, A. I., and G. Casella
2002 . ‘‘The existence of the first negative moment
revisited.’’ Amer. Statist., 56, 4447.
Ž
.
Khuri, A. I., and M. Conlon 1981 . ‘‘Simultaneous optimization of multiple responses
represented by polynomial regression functions.’’ Technometrics, 23, 363375.
Ž
.
Khuri, A. I., and J. A. Cornell 1996 . Response Surfaces, 2nd ed.
Marcel Dekker,
New York.
Ž
.
Khuri, A. I., and I. J. Good 1989 . ‘‘The parameterization of orthogonal matrices: A
review mainly for statisticians.’’ South African Statist. J., 23, 231250.
Ž
.
Khuri, A. I., and R. H. Myers 1979 . ‘‘Modified ridge analysis.’’ Technometrics, 21,
467473.
Ž
.
Khuri, A. I., and R. H. Myers 1981 . ‘‘Design related robustness of tests in regression
models.’’ Comm. Statist. Theory Methods, 10, 223235.
Ž
.
Khuri, A. I., and H. Sahai
1985 . ‘‘Variance components analysis: A selective
literature survey.’’ Internat. Statist. Re®, 53, 279300.
Ž
.
Kiefer, J. 1958 . ‘‘On the nonrandomized optimality and the randomized nonoptimal-
ity of symmetrical designs.’’ Ann. Math. Statist., 29, 675699.
Ž
.
Ž
.
Kiefer, J. 1959 . ‘‘Optimum experimental designs’’ with discussion . J. Roy. Statist.
Soc., Ser. B, 21, 272319.

GENERAL BIBLIOGRAPHY
659
Ž
.
Kiefer, J. 1960 . ‘‘Optimum experimental designs V, with applications to systematic
and rotatable designs.’’ In Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Probability, Vol. 1. University of California Press,
Berkeley, pp. 381405.
Ž
.
Kiefer, J. 1961 . ‘‘Optimum designs in regression problems II.’’ Ann. Math. Statist.,
32, 298325.
Ž
.
Kiefer, J. 1962a . ‘‘Two more criteria equivalent to D-optimality of designs.’’ Ann.
Math. Statist., 33, 792796.
Ž
.
Kiefer, J. 1962b . ‘‘An extremum result.’’ Canad. J. Math., 14, 597601.
Ž
.
Kiefer, J.
1975 . ‘‘Optimal design: Variation in structure and performance under
change of criterion.’’ Biometrika, 62, 277288.
Ž
.
Kiefer, J., and J. Wolfowitz 1960 . ‘‘The equivalence of two extremum problems.’’
Canad. J. Math., 12, 363366.
Ž
.
Kirkpatrick, S., C. D. Gelatt, and M. P. Vechhi 1983 . ‘‘Optimization by simulated
annealing,’’ Science, 220, 671680.
Ž
.
Knopp, K. 1951 . Theory and Application of Infinite Series. Blackie and Son, London.
Ž
.
Kosambi, D. D. 1949 . ‘‘Characteristic properties of series distributions.’’ Proc. Nat.
Inst. Sci. India, 15, 109113.
Ž
.
Krylov, V. I. 1962 . Approximate Calculation of Integrals. Macmillan, New York.
Ž
.
Kufner, A., and J. Kadlec
1971 . Fourier Series. Iliffe BooksThe Butterworth
Group, London.
Ž
.
Kupper, L. L.
1972 . ‘‘Fourier series and spherical harmonics regression.’’ Appl.
Statist., 21, 121130.
Ž
.
Kupper, L. L. 1973 . ‘‘Minimax designs for Fourier series and spherical harmonics
regressions: A characterization of rotatable arrangements.’’ J. Roy. Statist. Soc.,
Ser. B, 35, 493500.
Ž
.
Lancaster, P. 1969 . Theory of Matrices. Academic Press, New York.
Ž
.
Lancaster, P., and K. Salkauskas 1986 . Cur®e and Surface Fitting. Academic Press,
London.
Ž
.
Lange, K. 1999 . Numerical Analysis for Statisticians. Springer, New York.
Ž
.
Lehmann, E. L. 1983 . Theory of Point Estimation. Wiley, New York.
Ž
.
Lieberman, G. J., and Owen, D. B. 1961 . Tables of the Hypergeometric Probability
Distribution. Stanford University Press, Palo Alto, California.
Ž
.
Lieberman, O.
1994 . ‘‘A Laplace approximation to the moments of a ratio of
quadratic forms.’’ Biometrika, 81, 681690.
Ž
.
Lindgren, B. W. 1976 . Statistical Theory, 3rd ed. Macmillan, New York.
Ž
.
Little, R. J. A., and D. B. Rubin 1987 . Statistical Analysis with Missing Data. Wiley,
New York.
Ž
.
Liu, Q., and D. A. Pierce 1994 . ‘‘A note on GaussHermite quadrature.’’ Biometrika,
81, 624629.
Ž
.
Lloyd, E. 1980 . Handbook of Applicable Mathematics, Vol. II. Wiley, New York.
Ž
.
Lowerre, J. M. 1982 . ‘‘An introduction to modern matrix methods and statistics.’’
Amer. Statist., 36, 113115.

GENERAL BIBLIOGRAPHY
660
Ž
.
Lowerre, J. M.
1983 . ‘‘An integral of the bivariate normal and an application.’’
Amer. Statist., 37, 235236.
Ž
.
Lucas, J. M.
1976 . ‘‘Which response surface design is best.’’ Technometrics, 18,
411417.
Ž
.
Luceno, A. 1997 . ‘‘Further evidence supporting the numerical usefulness of charac-
˜
teristic functions.’’ Amer. Statist., 51, 233234.
Ž
.
Lukacs, E. 1970 . Characteristic Functions, 2nd ed. Hafner, New York.
Ž
.
Magnus, J. R., and H. Neudecker 1988 . Matrix Differential Calculus with Applications
in Statistics and Econometrics. Wiley, New York.
Ž
.
Mandel, J. 1982 . ‘‘Use of the singular-value decomposition in regression analysis.’’
Amer. Statist., 36, 1524.
Ž
.
Marcus, M., and H. Minc 1988 . Introduction to Linear Algebra. Dover, New York.
Ž
.
Mardia, K. V., and T. W. Sutton
1978 . ‘‘Model for cylindrical variables with
applications.’’ J. Roy. Statist. Soc., Ser. B, 40, 229233.
Ž
.
Marsaglia, G., and G. P. H. Styan 1974 . ‘‘Equalities and inequalities for ranks of
matrices.’’ Linear and Multilinear Algebra, 2, 269292.
Ž
.
May, W. G. 1970 . Linear Algebra. Scott, Foresman and Company, Glenview, Illinois.
Ž
.
McCullagh, P. 1994 . ‘‘Does the moment-generating function characterize a distribu-
tion?’’ Amer. Statist., 48, 208.
Ž
.
Menon, V. V., B. Prasad, and R. S. Singh 1984 . ‘‘Non-parametric recursive estimates
of a probability density function and its derivatives.’’ J. Statist. Plann. Inference, 9,
7382.
Ž
.
Miller, R. G., Jr.
1981 . Simultaneous Statistical Inference, 2nd ed. Springer, New
York.
Ž
.
Milliken, G. A., and D. E. Johnson 1984 . Analysis of Messy Data. Lifetime Learning
Publications, Belmont, California.
Ž
.
Mitchell, T. J. 1974 . ‘‘An algorithm for the construction of D-optimal experimental
designs.’’ Technometrics, 16, 203210.
Ž
.
Montgomery, D. C., and E. A. Peck 1982 . Introduction to Linear Regression Analysis.
Wiley, New York.
Ž
.
Moran, P. A. P.
1968 . An Introduction to Probability Theory. Clarendon Press,
Oxford.
Ž
.
Morin, D. 1992 . ‘‘Exact moments of ratios of quadratic forms.’’ Metron, 50, 5978.
Ž
.
Morland, T.
1998 . ‘‘Approximations to the normal distribution function.’’ Math.
Gazette, 82, 431437.
Ž
.
Morrison, D. F. 1967 . Multi®ariate Statistical Methods. McGraw-Hill, New York.
Ž
.
Muirhead, R. J. 1982 . Aspects of Multi®ariate Statistical Theory. Wiley, New York.
Ž
.
Myers, R. H. 1976 . Response Surface Methodology. Author, Blacksburg, Virginia.
Ž
.
Myers, R. H.
1990 . Classical and Modern Regression with Applications, 2nd ed.
PWS-Kent, Boston.
Ž
.
Myers, R. H., and W. H. Carter, Jr. 1973 . ‘‘Response surface techniques for dual
response systems.’’ Technometrics, 15, 301317.
Ž
.
Myers, R. H., and A. I. Khuri 1979 . ‘‘A new procedure for steepest ascent.’’ Comm.
Statist. Theory Methods, 8, 13591376.

GENERAL BIBLIOGRAPHY
661
Ž
.
Myers, R. H., A. I. Khuri, and W. H. Carter, Jr. 1989 . ‘‘Response surface methodol-
ogy: 19661988.’’ Technometrics, 31, 137157.
Ž
.
Nelder, J. A., and R. Mead 1965 . ‘‘A simplex method for function minimization.’’
Comput. J., 7, 308313.
Ž
.
Nelson, L. S.
1973 . ‘‘A sequential simplex procedure for non-linear least-squares
estimation and other function minimization problems.’’ In 27th Annual Technical
Conference Transactions, American Society for Quality Control, pp. 107117.
Ž
.
Newcomb, R. W. 1960 . ‘‘On the simultaneous diagonalization of two semidefinite
matrices.’’ Quart. Appl. Math., 19, 144146.
Ž
.
Nonweiler, T. R. F. 1984 . Computational Mathematics. Ellis Horwood, Chichester,
England.
Ž
.
Nurcombe, J. R, 1979 . ‘‘A sequence of convergence tests.’’ Amer. Math. Monthly, 86,
679681.
Ž
.
Ofir, C., and A. I. Khuri 1986 . ‘‘Multicollinearity in marketing models: Diagnostics
and remedial measures.’’ Internat. J. Res. Market., 3, 181205.
Ž
.
Olkin, I. 1990 . ‘‘Interface between statistics and linear algebra.’’ In Matrix Theory
and Applications, Vol. 40, C. R. Johnson, ed., American Mathematical Society,
Providence, pp. 233256.
Ž
.
Olsson, D. M., and L. S. Nelson 1975 . ‘‘The NelderMead simplex procedure for
function minimization.’’ Technometrics, 17, 4551.
Ž
.
Otnes, R. K., and L. Enochson 1978 . Applied Time Series Analysis. Wiley, New York.
Ž
.
Park, S. H. 1978 . ‘‘Experimental designs for fitting segmented polynomial regression
models.’’ Technometrics, 20, 151154.
Ž
.
Parzen, E. 1962 . Stochastic Processes. Holden-Day, San Francisco.
Ž
.
Pazman, A. 1986 . Foundations of Optimum Experimental Design. Reidel, Dordrecht,
Holland.
Ž
.
Perez-Abreu, V. 1991 . ‘‘Poisson approximation to power series distributions.’’ Amer.
´
Statist., 45, 4245.
Ž
.
Pfeiffer, P. E. 1990 . Probability for Applications. Springer, New York.
Ž
.
Phillips, C., and B. Cornelius
1986 . Computational Numerical Methods. Ellis
Horwood, Chichester, England.
Ž
.
Phillips, G. M., and P. J. Taylor 1973 . Theory and Applications of Numerical Analysis.
Academic Press, New York.
Ž
.
Piegorsch, W. W., and A. J. Bailer 1993 . ‘‘Minimum mean-square error quadrature.’’
J. Statist. Comput. Simul., 46, 217234.
Ž
.
Piegorsch, W. W., and G. Casella
1985 . ‘‘The existence of the first negative
moment.’’ Amer. Statist., 39, 6062.
Ž
.
Pinkus, A., and S. Zafrany 1997 . Fourier Series and Integral Transforms. Cambridge
University Press, Cambridge, England.
Ž
.
Plackett, R. L., and J. P. Burman
1946 . ‘‘The design of optimum multifactorial
experiments.’’ Biometrika, 33, 305325.
Ž
.
Poirier, D. J.
1973 . ‘‘Piecewise regression using cubic splines.’’ J. Amer. Statist.
Assoc., 68, 515524.
Ž
.
Powell, M. J. D.
1967 . ‘‘On the maximum errors of polynomial approximations
defined by interpolation and by least squares criteria.’’ Comput. J., 9, 404407.

GENERAL BIBLIOGRAPHY
662
Ž
.
Prenter, P. M. 1975 . Splines and Variational Methods. Wiley, New York.
Ž
.
Price, G. B.
1947 . ‘‘Some identities in the theory of determinants.’’ Amer. Math.
Monthly, 54, 7590.
Ž
.
Price, W. L. 1977 . ‘‘A controlled random search procedure for global optimization.’’
Comput. J., 20, 367370.
Ž
.
Pye, W. C., and P. G. Webster
1989 . ‘‘A note on Raabe’s test extended.’’ Math.
Comput. Ed., 23, 125128.
Ž
.
Ralston, A., and P. Rabinowitz
1978 .
A First Course in Numerical Analysis.
McGraw-Hill, New York.
Ž
.
Ramsay, J. O.
1988 . ‘‘Monotone regression splines in action.’’ Statist. Sci., 3,
425461.
Ž
.
Randles, R. H., and D. A. Wolfe 1979 . Introduction to the Theory of Nonparametric
Statistics. Wiley, New York.
Ž
.
Rao, C. R.
1970 . ‘‘Estimation of heteroscedastic variances in linear models.’’
J. Amer. Statist. Assoc., 65, 161172.
Ž
.
Rao, C. R. 1971 . ‘‘Estimation of variance and covariance componentsMINQUE
theory.’’ J. Multi®ariate Anal., 1, 257275.
Ž
.
Rao, C. R.
1972 . ‘‘Estimation of variance and covariance components in linear
models.’’ J. Amer. Statist. Assoc., 67, 112115.
Ž
.
Rao, C. R. 1973 . Linear Statistical Inference and its Applications, 2nd ed. Wiley, New
York.
Ž
.
Reid, W. H., and S. J. Skates 1986 . ‘‘On the asymptotic approximation of integrals.’’
SIAM J. Appl. Math, 46, 351358.
Ž
.
Rice, J. R. 1969 . The Approximation of Functions, Vol. 2. Addison-Wesley, Reading,
Massachusetts.
Ž
.
Rivlin, T. J. 1969 . An Introduction to the Approximation of Functions. Dover, New
York.
Ž
.
Rivlin, T. J. 1990 . Chebyshe® Polynomials, 2nd ed. Wiley, New York.
Ž
.
Roberts, A. W., and D. E. Varberg 1973 . Con®ex Functions. Academic Press, New
York.
Ž
.
Rogers, G. S. 1984 . ‘‘Kronecker products in ANOVAA first step.’’ Amer. Statist.,
38, 197202.
Ž
.
Roussas, G. G.
1973 . A First Course in Mathematical Statistics. Addison-Wesley,
Reading, Massachusetts.
Ž
.
Rudin, W.
1964 . Principles of Mathematical Analysis, 2nd ed. McGraw-Hill, New
York.
Ž
.
Rustagi, J. S., ed. 1979 . Optimizing Methods in Statistics. Academic Press, New York.
Ž
.
Sagan, H. 1974 . Ad®anced Calculus. Houghton Mifflin, Boston.
Ž
.
Satterthwaite, F. E.
1946 . ‘‘An approximate distribution of estimates of variance
components.’’ Biometrics Bull., 2, 110114.
Ž
.
Scheffe, H. 1959 . The Analysis of Variance. Wiley, New York.
´
Ž
.
Schoenberg, I. J. 1946 . ‘‘Contributions to the problem of approximation of equidis-
tant data by analytic functions.’’ Quart. Appl. Math., 4, 4599; 112141.

GENERAL BIBLIOGRAPHY
663
Ž
.
Schone, A., and W. Schmid 2000 . ‘‘On the joint distribution of a quadratic and a
¨
linear form in normal variables.’’ J. Mult. Anal., 72, 163182.
Ž
.
Schwartz, S. C. 1967 . ‘‘Estimation of probability density by an orthogonal series’’,
Ann. Math. Statist., 38, 12611265.
Ž
.
Searle, S. R. 1971 . Linear Models. Wiley, New York.
Ž
.
Searle, S. R. 1982 . Matrix Algebra Useful for Statistics. Wiley, New York.
Ž
.
Seber, G. A. F. 1984 . Multi®ariate Obser®ations. Wiley, New York.
Ž
.
Shoup, T. E. 1984 . Applied Numerical Methods for the Microcomputer. Prentice-Hall,
Englewood Cliffs, New Jersey.
Ž
.
Silvey, S. D. 1980 . Optimal Designs. Chapman and Hall, London.
Ž
.
Smith, D. E. 1958 . History of Mathematics, Vol. I. Dover, New York.
Ž
.
Smith, P. L. 1979 . ‘‘Splines as a useful and convenient statistical tool.’’ Amer. Statist.,
33, 5762.
Ž
.
Spendley, W., G. R. Hext, and F. R. Himsworth 1962 . ‘‘Sequential application of
simplex designs in optimization and evolutionary operation.’’ Technometrics, 4,
441461.
Ž
.
St. John, R. C., and N. R. Draper
1975 . ‘‘D-optimality for regression designs: A
review.’’ Technometrics, 17, 1523.
Ž
.
Stark, P. A. 1970 . Introduction to Numerical Methods. Macmillan, London.
Ž
.
Stoll, R. R. 1963 . Set Theory and Logic. W. H. Freeman, San Francisco.
Ž
.
Strawderman, R. L. 2000 . ‘‘Higher-order asymptotic approximation: Laplace, sad-
dlepoint, and related methods.’’ J. Amer. Statist. Assoc., 95, 13581364.
Ž
.
Stroud, A. H.
1971 . Approximate Calculation of Multiple Integrals. Prentice-Hall,
Englewood Cliffs, New Jersey.
Ž
.
Subrahmaniam, K.
1966 . ‘‘Some contributions to the theory of non-normalityI
Ž
.
univariate case .’’ Sankhya, Ser. A, 28, 389406.
Ž
.
Sutradhar, B. C., and R. F. Bartlett 1989 . ‘‘An approximation to the distribution of
the ratio of two general quadratic forms with application to time series valued
designs.’’ Comm. Statist. Theory Methods, 18, 15631588.
Ž
.
Swallow, W. H., and S. R. Searle
1978 . ‘‘Minimum variance quadratic unbiased
Ž
.
estimation MIVQUE of variance components.’’ Technometrics, 20, 265272.
Ž
.
Szego, G.
1975 . Orthogonal Polynomials, 4th ed. Amer. Math. Soc., Providence,
¨
Rhode Island.
Ž
.
Szidarovszky, F., and S. Yakowitz
1978 . Principles and Procedures of Numerical
Analysis. Plenum Press, New York.
Ž
.
Taylor, A. E., and W. R. Mann 1972 . Ad®anced Calculus, 2nd ed. Wiley, New York.
Ž
.
Thibaudeau, Y., and G. P. H. Styan 1985 . ‘‘Bounds for Chakrabarti’s measure of
imbalance in experimental design.’’ In
Proceedings of the First International
Tampere Seminar on Linear Statistical Models and Their Applications, T. Pukkila
and S. Puntanen, eds. University of Tampere, Tampere, Finland, pp. 323347.
Ž
.
Thiele, T. N.
1903 . Theory of Obser®ations. Layton, London. Reprinted in Ann.
Ž
.
Math. Statist. 1931 , 2, 165307.

GENERAL BIBLIOGRAPHY
664
Ž
.
Tierney, L., R. E. Kass, and J. B. Kadane
1989 . ‘‘Fully exponential Laplace
approximations to expectations and variances of nonpositive functions.’’ J. Amer.
Statist. Assoc., 84, 710716.
Ž
.
Tiku, M. L. 1964a . ‘‘Approximating the general non-normal variance ratio sampling
distributions.’’ Biometrika, 51, 8395.
Ž
.
Tiku, M. L.
1964b . ‘‘A note on the negative moments of a truncated Poisson
variate.’’ J. Amer. Statist. Assoc., 59, 12201224.
Ž
.
Ž
Tolstov, G. P. 1962 . Fourier Series. Dover, New York. Translated from the Russian
.
by Richard A. Silverman.
Ž
.
Tucker, H. G., 1962 . Probability and Mathematical Statistics. Academic Press, New
York.
Ž
.
Vilenkin, N. Y. 1968 . Stories about Sets. Academic Press, New York.
Ž
.
Viskov, O. V. 1992 . ‘‘Some remarks on Hermite polynomials.’’ Theory Prob. Appl.,
36, 633637.
Ž
.
Waller, L. A. 1995 . ‘‘Does the characteristic function numerically distinguish distri-
butions?’’ Amer. Statist., 49, 150152.
Ž
.
Waller, L. A., B. W. Turnbull, and J. M. Hardin
1995 . ‘‘Obtaining distribution
functions by numerical inversion of characteristic functions with applications.’’
Amer. Statist., 49, 346350.
Ž
.
Watson, G. S.
1964 . ‘‘A note on maximum likelihood.’’ Sankhya, Ser.
A, 26,
303304.
Ž
.
Weaver, H. J. 1989 . Theory of Discrete and Continuous Fourier Analysis. Wiley, New
York.
Ž
.
Wegman, E. J., and I. W. Wright
1983 . ‘‘Splines in statistics.’’ J. Amer. Statist.
Assoc., 78, 351365.
Ž
.
Wen, L. 2001 . ‘‘A counterexample for the two-dimensional density function.’’ Amer.
Math. Monthly, 108, 367368.
Wetherill, G. B., P. Duncombe, M. Kenward, J. Kollerstrom, S. R. Paul, and B. J.
¨
¨
Ž
.
Vowden
1986 .
Regression Analysis with Applications. Chapman and Hall,
London.
Ž
.
Wilks, S. S. 1962 . Mathematical Statistics. Wiley, New York.
Ž
.
Withers, C. S.
1984 . ‘‘Asymptotic expansions for distributions and quantiles with
power series cumulants.’’ J. Roy. Statist. Soc., Ser. B, 46, 389396.
Ž
.
Wold, S. 1974 . ‘‘Spline functions in data analysis.’’ Technometrics, 16, 111.
Ž
.
Wolkowicz, H., and G. P. H. Styan
1980 . ‘‘Bounds for eigenvalues using traces.’’
Linear Algebra Appl., 29, 471506.
Ž
.
Wong, R. 1989 . Asymptotic Approximations of Integrals. Academic Press, New York.
Ž
.
Woods, J. D., and H. O. Posten 1977 . ‘‘The use of Fourier series in the evaluation of
probability distribution functions.’’ Comm. Statist.Simul. Comput., 6, 201219.
Ž
.
Wynn, H. P. 1970 . ‘‘The sequential generation of D-optimum experimental designs.’’
Ann. Math. Statist., 41, 16551664.
Ž
.
Wynn, H. P. 1972 . ‘‘Results in the theory and construction of D-optimum experi-
mental designs.’’ J. Roy. Statist. Soc., Ser. B, 34, 133147.
Ž
.
Zanakis, S. H., and J. S. Rustagi, eds. 1982 . Optimization in Statistics. North-Holland,
Amsterdam.
Ž
.
Zaring, W. M. 1967 . An Introduction to Analysis. Macmillian, New York.

Index
Absolute error loss, 86
Absolutely continuous
function, 91
random variable, 116, 239
Addition of matrices, 29
Adjoint, 34
Adjugate, 34
Admissible estimator, 86
Approximation of
density functions, 456
functions, 403, 495
integrals, 517, 533
normal integral, 460, 469
quantiles of distributions, 456, 458, 468
Arcsin x, 6, 78
Asymptotically equal, 66, 147
Asymptotic distributions, 120
Average squared bias, 360, 431
Average variance, 360, 400
Balanced mixed model, 43
BehrensFisher test, 324
Bernoulli sequence, 14
Bernstein polynomial, 406, 410
Best linear unbiased estimator, 312
Beta function, 256
Bias, 359, 360
criterion, 359, 366
in the model, 359, 366
Biased estimator, 196
Binomial distribution, 194
Binomial random variable, 14, 203
BolzanoWeierstrass theorem, 11
Borel field, 13
Boundary, 262, 297
point, 262
Bounded sequence, 132, 262
Bounded set, 9, 13, 72, 262
Bounded variation, function of, 210, 212
Bounds on eigenvalues, 41
Box and Cox transformation, 120
BoxDraper determinant criterion, 400
BoxLucas criterion, 368, 422, 424425
Calibration, 242
Canonical correlation coefficient, 56
Cartesian product, 3, 21
Cauchy criterion, 137, 221
Cauchy distribution, 240
CauchySchwarz inequality, 25, 229
Cauchy sequence, 138, 222
Cauchy’s condensation test, 153
Cauchy’s mean value theorem, 103, 105, 129
Cauchy’s product, 163
Cauchy’s test, 149
Cell, 294
Center-point replications, 400
Central limit theorem, 120, 516
Chain rule, 97, 116
Change of variables, 219, 299
Characteristic equation, 37
Characteristic function, 505
inversion formula for, 507
Characteristic root, 36
Characteristic vector, 37
Chebyshev polynomials, 415, 444
discrete, 462
of the first kind, 444
of the second kind, 445
zeros of, 415, 444
Chebyshev’s inequality, 19, 184, 251, 259
Chi-squared random variable, 246
Christoffel’s identity, 443
Closed set, 1113, 72, 262
Coded levels, 347
Coefficient of variation, 242
665

INDEX
666
Cofactor, 31, 357
Compact set, 13
Comparison test, 144
Complement of a set, 2
relative, 2
Composite function, 6, 97, 265, 276
Concave function, 79
Condition number, 47
Confidence intervals, simultaneous, 382, 384,
388
Consistent estimator, 83
Constrained maximization, 329
Constrained optimization, 288
Constraints, equality, 288
Continuous distribution, 14, 82
Continuous function, 66, 210, 264
absolutely, 91
left, 68
piecewise, 481, 486, 489, 498
right, 68, 83
Continuous random variable, 14, 82, 116, 239
Contrast, 384
Controlled random search procedure, 336, 426
Convergence
almost surely, 192
in distribution, 120
in probability, 83, 191
in quadratic mean, 191
Convex function, 79, 84, 98
Convex set, 79
Convolution, 499
CornishFisher expansion, 460, 468
Correlation coefficient, 311
Countable set, 6
Covering of a set, 12
open, 12
Cubic spline regression model, 430
Cumulant generating function, 189
Cumulants, 189, 459
Cumulative distribution function, 14, 82, 304
joint, 14
Curvature, in response surface, 342
Cylindrical data, 505
d’Alembert’s test, 148
DavidonFletcherPowell method, 331
Deleted neighborhood, 57, 262
Density function, 14, 116, 239, 304
approximation of, 456
bivariate, 305
marginal, 304
Derivative, 93
directional, 273
partial, 267
total, 270
Design
A-optimal, 365
approximate, 363
BoxBehnken, 359
center, 344, 356
central composite, 347, 353, 358, 400
dependence of, 368369, 422
D-optimal, 363, 365, 369
efficiency of, 365
E-optimal, 365
exact, 363
first-order, 340, 356, 358
G-optimal, 364
$ -optimal, 399
1
matrix, 356
measure, 362363, 365
moments, 362
nonlinear model, 367
optimal, 362, 425
orthogonal, 358
parameter-free, 426
PlackettBurman, 358
points, 353, 360
response surface, 339, 355, 359
rotatable, 350, 356
second-order, 344, 358
3k factorial, 358
2 k factorial, 340, 358
Design measure
continuous, 362
discrete, 363, 366
Determinant, 31
Hessian, 276
Jacobian, 268
Vandermonde’s, 411
DETMAX algorithm, 366
Diagonalization of a matrix, 38
Differentiable function, 94, 113
Differential operator, 277
Differentiation under the integral sign, 301
Directional derivative, 273
Direct product, 30, 44
Direct search methods, 332
Direct sum
of matrices, 30
of vector subspaces, 25
Discontinuity, 67
first kind, 67
second kind, 67
Discrete distribution, 14
Discrete random variable, 14, 182

INDEX
667
Disjoint sets, 2
Distribution
binomial, 194
Cauchy, 240
chi-squared, 245
continuous, 14, 82
discrete, 14
exponential, 20, 91, 131
F, 383384
gamma, 251
hypergeometric, 463
logarithmic series, 193
logistic, 241
marginal, 304
multinomial, 325
negative binomial, 182, 195
normal, 120, 306, 311, 400
Pearson Type IV, 503
Poisson, 119, 183, 204
power series, 193
t, 259
truncated Poisson, 373
uniform, 83, 92, 116, 250
Domain of a function, 5
Dot product, 23
Eigenvalues, 3637
Eigenvectors, 37
EM algorithm, 372, 375
Empty set, 1
Equal in distribution, 15
Equality of matrices, 28
Equivalence class, 4
Equivalence theorem, 365
Equivalent sets, 5
Error loss
absolute, 86
squared, 86
Estimable linear function, 45, 382, 387
Estimation of unknown densities, 461
Estimator
admissible, 86
best linear unbiased, 312
biased, 196
consistent, 83
least absolute value, 327
M, 327
minimax, 86
minimum norm quadratic unbiased, 380
minimum variance quadratic unbiased, 382
quadratic, 379
ridge, 56, 196
Euclidean norm, 24, 42, 179, 261
Euclidean space, 21
Euler’s theorem, 317
Event, 13
Exchangeable random variables, 15
Exponential distribution, 20, 91, 131
Extrema
absolute, 113
local, 113
Factorial moment, 191
Failure rate, 130
F distribution, 383384
Fisher’s method of scoring, 374
Fixed effects, 44
Fourier
coefficients, 472
integral, 488
transform, 497, 507
Fourier series, 471
convergence of, 475
differentiation of, 483
integration of, 483
Fubini’s theorem, 297
Function, 5
absolutely continuous, 91
bounded, 65, 72
bounded variation, 210, 212
composite, 6, 97, 265, 276
concave, 79
continuous, 67, 210, 264
convex, 79, 84, 98
differentiable, 94, 113
homogeneous, 317
implicit, 273
inverse, 6, 76, 102
left continuous, 68
limit of, 57
Lipschitz continuous, 75, 409
loss, 86
lower semicontinuous, 90
monotone, 76, 101, 116, 210
multivariable, 261
one-to-one, 5
onto, 5
periodic, 473, 489, 504
Riemann integrable, 206, 210, 215
RiemannStieltjes integrable, 234
right continuous, 68, 83
risk, 86
uniformly continuous, 68, 74, 82, 265,
407
upper semicontinuous, 90
Functionally dependent, 318

INDEX
668
Function of a random variable, 116
Fundamental theorem of calculus, 219
Gamma distribution, 251
Gamma function, 246, 532
approximation for, 532
Gaussian quadrature, 524
GaussMarkov theorem, 55
Gauss’s test, 156
Generalized distance approach, 370
Generalized inverse, 36
Generalized simulated annealing method, 338
Generalized variance, 363
Geometric mean, 84, 230
Geometric series, 142
Gibbs phenomenon, 514
Gradient, 275
methods, 329
GramCharlier series, 457
GramSchmidt orthonormalization, 24
Greatest-integer function, 238
Greatest lower bound, 9, 72
Harmonic
frequencies, 501
mean, 85
series, 145
Hazard rate, 131
HeineBorel theorem, 13
Hermite polynomials, 447
applications of, 456, 542
normalized, 461
Hessian
determinant, 276
matrix, 275, 285, 374
Heteroscedasticity, 118
Holder’s inequality, 230231
¨
Homogeneous function, 317
Euler’s theorem for, 317
Homoscedasticity, 118
Hotelling’s T 2 statistic, 49
Householder matrix, 38
Hypergeometric
distribution, 463
probability, 462
series, 157
Idempotent matrix, 38
Ill conditioning, 46
Image of a set, 5
Implicit function, 273
Implicit function theorem, 282
Importance sampling, 537
Improper integral, 220
absolutely convergent, 222, 226
conditionally convergent, 222, 226
of the first kind, 220, 528
of the second kind, 221, 225
Indefinite integral, 217
Indeterminate form, 107
Inequality
CauchySchwarz, 25, 229
Chebyshev’s, 19, 184, 251
Holder’s, 230, 231
¨
Jensen’s, 84, 233
Markov’s, 19, 185
Minkowski’s, 232
Infimum, 9, 73
Inner product, 23
Input variables, 339, 351
Integral
improper, 220
indefinite, 217
Riemann, 206, 293
RiemannStieltjes, 234
Integral test, 153, 224
Interaction, 43
Interior point, 262
Intermediate-value theorem, 71, 217
Interpolation, 410
error, 414, 416, 423
Lagrange, 411, 419, 422
points, 411412, 414, 423
Intersection, 2
Interval of convergence, 174, 190
Inverse function, 6, 76, 102
Inverse function theorem, 280, 305
Inverse of a matrix, 34
Inverse regression, 242
Irrational number, 9
Jacobian
determinant, 268
matrix, 268
Jacobi polynomials, 443
applications of, 462
Jensen’s inequality, 84, 233
Jordan content, 298
Jordan measurable, 298
Kernel of a linear transformation, 26
Khinchine’s theorem, 192
Knots, 418, 431
Kolmogorov’s theorem, 192
Kronecker product, 30
Kummer’s test, 154

INDEX
669
Lack of fit, 342, 399
Lagrange interpolating polynomial, 411, 416,
423, 518, 521
Lagrange interpolation, 413, 419, 422
accuracy of, 413
Lagrange multipliers, 288, 312, 329, 341, 344,
352, 383
Laguerre polynomials, 451
applications of, 462
Laplace
approximation, 531, 546, 548
method of, 531, 534, 546
Law of large numbers, 84
Bernoulli’s, 204
strong, 192
weak, 192
Least-squares estimate, 46, 344
Least-squares polynomial approximation,
453455
Least upper bound, 9, 73
Legendre polynomials, 440, 442
Leibniz’s formula, 127, 443, 449, 452
Level of significance, 15
L’Hospital’s rule, 103, 120, 254
Likelihood
equations, 308, 373
function, 308, 373
Limit, 57
left sided, 59
lower, 136
of a multivariable function, 262
one sided, 58
right sided, 59
of a sequence, 133
subsequential, 136
two sided, 58
upper, 136
Limit point, 11, 262
Linearly dependent, 22
Linearly independent, 23
Linear model, 43, 46, 121, 195, 355, 367,
382
balanced, 43
first order, 340
second order, 343
Linear span, 23
Linear transformation, 25
kernel of, 26
one-to-one, 27
Lipschitz condition, 75
Lipschitz continuous function, 75, 409
Logarithmic series distribution, 193
Logistic distribution, 241
Loss function, 86
Lower limit, 136
Lower semicontinuous function, 90
Lower sum, 206, 294
Maclaurin’s integral test, 153
Maclaurin’s series, 111
Mapping, 5
Markov chain Monte Carlo, 549
Markov’s inequality, 19, 185
Matrix, 28
diagonal, 28
full column rank, 34
full rank, 34
full row rank, 34
Hessian, 275, 285, 374
Householder, 38
idempotent, 38
identity, 28
inverse of, 34
Jacobian, 268
moment, 363
orthogonal, 38, 351, 357
partitioned, 30
rank of, 33
singular, 32, 37
skew symmetric, 29
square, 28
symmetric, 29
trace of, 29
transpose of, 29
Maximum
absolute, 113, 283, 346
local, 113, 283
relative, 113
Maximum likelihood estimate, 308, 372
Mean, 117, 239
Mean response, 343, 359, 367, 423
maximum of, 343
Mean squared error, 360, 431
integrated, 360, 431
Mean value theorem, 99100, 271
for integrals, 217
Median, 124
Mertens’s theorem, 163
Method of least squares, 121
Minimax estimator, 86
Minimization of prediction variance, 356
Minimum
absolute, 113, 283, 336, 346
local, 113, 283
relative, 113
Minimum bias estimation, 398

INDEX
670
Minimum norm quadratic unbiased estimation,
327, 378
Minkowski’s inequality, 232
Minor, 31
leading principal, 32, 40, 285
principal, 32, 292
Modal approximation, 549
Modulus of continuity, 407
Moment generating function, 186, 250, 506
Moment matrix, 363
Moments, 182, 505
central, 182, 240, 457
design, 362
factorial, 191
first negative, 242
noncentral, 182, 240
region, 361
Monotone function, 76, 101, 210
Monotone sequence, 133
Monte Carlo method, 535, 540
error bound for, 537
variance reduction, 537
Monte Carlo simulation, 83
Multicollinearity, 46, 196, 353, 387
Multimodal function, 336, 338
Multinomial distribution, 325
Multiresponse
experiment, 370
function, 370
optimization, 370
Multivariable function, 261
composite, 265, 276
continuous, 264
inverse of, 280
limit of, 262
optima of, 283
partial derivative of, 267
Riemann integral of, 293
Taylor’s theorem for, 277
uniformly continuous, 265
Negative binomial, 182, 195
Neighborhood, 10, 262
deleted, 57, 262
NewtonCotes methods, 523
NewtonRaphson method, 331, 373
Noncentral chi-squared, 44
Noncentrality parameter, 44
Nonlinear model, 367
approximate linearization of, 422
design for, 367
Nonlinear parameter, 367
Norm, 179
Euclidean, 24, 42, 179, 261
minimum, 380
of a partition, 205
spectral, 179
Normal distribution, 120, 306
bivariate, 311
p-variate, 400
Normal integral, 460, 469
approximation of, 460, 469
Normal random variable, 245
Null space, 26
One-to-one correspondence, 5
One-to-one function, 5
One-way classification model, 387
o notation, 66, 117
O notation, 65, 156
Open set, 10, 262
Optimality of design
A, 365
D, 364, 367, 431
E, 365
G, 364
Optimization techniques, 339
Optimum
absolute, 113
compromise, 371
conditions, 370
ideal, 371
local, 113, 284
Ordered n-tuple, 4
Ordered pair, 3
Orthogonal complement, 25
Orthogonal design, 358
Orthogonal functions, 437
Orthogonal matrix, 38, 351, 357
parameterization, 49
Orthogonal polynomials, 437, 453
Chebyshev, 444
of the first kind, 444
of the second kind, 445
zeros of, 444
on a finite set, 455
Hermite, 447
applications of, 456
Jacobi, 443
Laguerre, 451
least-squares approximation with, 453
Legendre, 440, 442
sequence of, 437
zeros of, 440
Orthogonal vectors, 24
Orthonormal basis, 24

INDEX
671
Parameterization of orthogonal matrices, 49
Parseval’s theorem, 496
Partial derivative, 267
Partially nonlinear model, 369
Partial sum, 140
Partition, 205, 294
of a set, 5
Periodic
extension, 482
function, 473, 489, 504
Periodogram, 501
Piecewise continuous function, 481, 486, 489,
498
Poisson approximation, 194
Poisson distribution, 119, 183, 204
truncated, 373
Poisson process, 122, 131
Poisson random variable, 14, 124, 183
Polynomial
approximation, 403
Bernstein, 406, 410
Chebyshev, 444
Hermite, 447, 456, 542
interpolation, 410
Jacobi, 443, 462
Lagrange interpolating, 411, 416, 423
Laguerre, 451, 462
Legendre, 440, 442
piecewise, 418
trigonometric, 495, 500, 504
Power series, 174
distribution, 194
Prediction equation, 348
Prediction variance, 347, 350, 352, 356
minimization of, 356
standardized, 363364
Principal components, 328
Probability of an event, 13
Probability generating function, 190
continuity theorem for, 192
Probability space, 13
Product of matrices, 29
Projection of a vector, 25
Proper subset, 1
Quadratic form, 39
extrema of, 48
nonnegative definite, 40
positive definite, 40
positive semidefinite, 40
Quadrature, 524
GaussChebyshev, 526
GaussHermite, 528, 542
GaussJacobi, 526
GaussLaguerre, 528
GaussLegendre, 526
Raabe’s test, 155
Radius of convergence, 174
Random effects, 44
Random variable, 13
absolutely continuous, 116, 239
Bernoulli, 194
binomial, 14, 203
chi-squared, 246
continuous, 14, 82, 116, 239
discrete, 14, 182
function of, 116
normal, 245
Poisson, 14, 124, 183
Random vector, 304
transformations of, 305
Range of a function, 5
Rank of a matrix, 33
Rational number, 8
Ratio test, 148
Rayleigh’s quotient, 41
infimum of, 41
supremum of, 41
Rearrangement of series, 159
Refinement, 206, 214, 294
Region of convergence, 174
Relation, 4
congruence, 5
equivalence, 4
reflexive, 4
symmetric, 4
transitive, 4
Remainder, 110, 257, 279
Response
constrained, 352
maximum, 340
minimum, 340
optimum, 350
predicted, 341, 344, 355, 504
primary, 352
surface, 340
Response surface design, 339, 355, 359
Response surface methodology, 339, 355, 367,
504
Response variable, 121
Restricted least squares, 429
Ridge analysis, 343, 349, 352
modified, 350, 354
standard, 354
Ridge estimator, 56, 196

INDEX
672
Ridge plots, 346
Ridge regression, 195
Riemann integral, 206, 293
double, 295
iterated, 295
n-tuple, 295
RiemannStieltjes integral, 234
Risk function, 86
Rodrigues formula, 440, 443, 447, 451
Rolle’s theorem, 99, 110
Root test, 149
Rotatability, 347
Saddle point, 114, 284, 344, 349
Saddlepoint approximation, 549
Sample, 14
Sample space, 13
Sample variance, 14
Satterthwaite’s approximation, 324
Scalar, 21
product, 23
Scheffe’s confidence intervals, 382, 384
´
Schur’s theorem, 42
Sequence, 132, 262
bounded, 132, 262
Cauchy, 138, 222
convergent, 133, 262
divergent, 133
of functions, 165, 227
limit of, 133
limit infimum of, 136, 148, 151
limit supremum of, 136, 148, 151
of matrices, 178
monotone, 133
of orthogonal polynomials, 437
subsequential limit of, 136
uniformly convergent, 166, 169, 228
Series, 140
absolutely convergent, 143, 158, 174
alternating, 158
conditionally convergent, 158
convergent, 141
divergent, 141
Fourier, 471
of functions, 165
geometric, 142
GramCharlier, 457
harmonic, 145
hypergeometric, 157
Maclaurin’s, 111
of matrices, 178, 180
multiplication of, 162
of positive terms, 144
power, 174
rearrangement of, 159
sum of, 140
Taylor’s, 111, 121
trigonometric, 471
uniformly convergent, 166, 169
Ž .
Set s , 1
bounded, 9, 72, 262
closed, 11, 72, 262
compact, 13
complement of, 2
connected, 12
convex, 79
countable, 6
covering of, 12
disconnected, 12
empty, 1
finite, 6
image of, 5
open, 10, 262
partition of, 5
subcovering of, 13
uncountable, 7
universal, 2
ShermanMorrison formula, 54
ShermanMorrisonWoodbury formula, 54,
389
Simplex method, 332
Simpson’s method, 521
Simulated annealing, 338
Simultaneous diagonalization, 40
Singularity of a function, 225
Singular-value decomposition, 39, 45
Singular values of a matrix, 46
Size of test, 15
Spectral decomposition theorem, 38, 181, 351
Spherical polar coordinates, 322
Spline functions, 418, 428
approximation, 418, 421
cubic, 419, 428
designs for, 430
error of approximation by, 421
linear, 418
properties of, 418
Squared error loss, 86
Stationary point, 284, 345
Stationary value, 113
Statistic, 14
first-order, 20
Hotelling’s T 2, 49
nth-order, 20
Steepest ascent
method of, 330, 340
path of, 396
Steepest descent

INDEX
673
method of, 329
path of, 343
Step function, 237
Stieltjes moment problem, 185
Stirling’s formula, 532
Stratified sampling, 313
Stratum, 313
Submatrix, 29
leading principal, 29
principal, 29
Subsequence, 135, 262
Subsequential limit, 136
Subset, 1
Supremum, 9, 73
Symmetric difference, 17
Taylor’s formula, 110
Taylor’s series, 111, 121
remainder of, 110, 257, 279
Taylor’s theorem, 108, 114, 277
t distribution, 259
Time series, 500
Topological space, 10
Topology, 10
basis for, 10
Total derivative, 270
Transformation
Box and Cox, 120
of continuous random variables, 246
of random vectors, 305
variance stabilizing, 118
Translation invariance, 379
Trapezoidal method, 517
Triangle inequality, 25
Trigonometric
Ž .
polynomial s , 495, 500, 504
series, 471
Two-way crossed classification model, 43, 401
Uncountable set, 7
Uniform convergence
of sequences, 166, 169, 228
of series, 167, 169
Uniform distribution, 83, 92, 116, 250
Uniformly continuous function, 68, 74, 82, 265,
407
Union, 2
Universal set, 2
Upper limit, 136
Upper semicontinuous function, 90
Upper sum, 206, 294
Variance, 117, 239
Variance components, 44, 378
Variance criterion, 359, 366
Variance stabilizing transformation, 118
Vector, 21
column, 28
row, 28
zero, 21
Vector space, 21
basis for, 23
dimension of, 23
Vector subspace, 22
Voronovsky’s theorem, 410
Weierstrass approximation theorem, 403
Weierstrass’s M-test, 167, 172

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Peter Bloomfield, Noel A. C. Cressie, 
Nicholas I. Fisher, Iain M. Johnstone, J. B. Kadane, Louise M. Ryan, 
David W. Scott, Adrian F. M. Smith, Jozef L. Teugels
Editors Emeriti: Vic Barnett, J. Stuart Hunter, David G. Kendall
A complete list of the titles in this series appears at the end of this volume.

WILEY SERIES IN PROBABILITY AND STATISTICS
ESTABLISHED BY WALTER A. SHEWHART AND SAMUEL S. WILKS
Editors: David J. Balding, Peter Bloomfield, Noel A. C. Cressie,
Nicholas I. Fisher, Iain M. Johnstone, J. B. Kadane, Louise M. Ryan,
David W. Scott, Adrian F. M. Smith, Jozef L. Teugels
Editors Emeriti: Vic Barnett, J. Stuart Hunter, David G. Kendall
The Wiley Series in Probability and Statistics is well established and authoritative. It covers
many topics of current research interest in both pure and applied statistics and probability
theory. Written by leading statisticians and institutions, the titles span both state-of-the-art
developments in the field and classical methods.
Reflecting the wide range of current research in statistics, the series encompasses applied,
methodological and theoretical statistics, ranging from applications and new techniques
made possible by advances in computerized practice to rigorous treatment of theoretical
approaches.
This series provides essential and invaluable reading for all statisticians, whether in aca-
demia, industry, government, or research.
ABRAHAM and LEDOLTER · Statistical Methods for Forecasting
AGRESTI · Analysis of Ordinal Categorical Data
AGRESTI · An Introduction to Categorical Data Analysis
AGRESTI · Categorical Data Analysis, Second Edition
ANDEL · Mathematics of Chance
ANDERSON · An Introduction to Multivariate Statistical Analysis, Second Edition
*ANDERSON · The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG ·
Statistical Methods for Comparative Studies
ANDERSON and LOYNES · The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) · Advances in Biometry
ARNOLD, BALAKRISHNAN, and NAGARAJA · Records
*ARTHANARI and DODGE · Mathematical Programming in Statistics
*BAILEY · The Elements of Stochastic Processes with Applications to the Natural
Sciences
BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications
BARNETT · Comparative Statistical Inference, Third Edition
BARNETT and LEWIS · Outliers in Statistical Data, Third Edition
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical Inference
BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and
Applications
BASU and RIGDON · Statistical Methods for the Reliability of Repairable Systems
BATES and WATTS · Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER, and GOLDSMAN · Design and Analysis of Experiments for
Statistical Selection, Screening, and Multiple Comparisons
BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in Regression
BELSLEY, KUH, and WELSCH · Regression Diagnostics: Identifying Influential
Data and Sources of Collinearity
BENDAT and PIERSOL · Random Data: Analysis and Measurement Procedures, 
Third Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.

BERRY, CHALONER, and GEWEKE · Bayesian Analysis in Statistics and
Econometrics: Essays in Honor of Arnold Zellner
BERNARDO and SMITH · Bayesian Theory
BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and JOHNSON · Statistical Concepts and Methods
BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications
BILLINGSLEY · Convergence of Probability Measures, Second Edition
BILLINGSLEY · Probability and Measure, Third Edition
BIRKES and DODGE · Alternative Methods of Regression
BLISCHKE AND MURTHY · Reliability: Modeling, Prediction, and Optimization
BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN · Structural Equations with Latent Variables
BOROVKOV · Ergodicity and Stability of Stochastic Processes
BOULEAU · Numerical Methods for Stochastic Processes
BOX · Bayesian Inference in Statistical Analysis
BOX · R. A. Fisher, the Life of a Scientist
BOX and DRAPER · Empirical Model-Building and Response Surfaces
*BOX and DRAPER · Evolutionary Operation: A Statistical Method for Process
Improvement
BOX, HUNTER, and HUNTER · Statistics for Experimenters: An Introduction to
Design, Data Analysis, and Model Building
BOX and LUCEÑO · Statistical Control by Monitoring and Feedback Adjustment
BRANDIMARTE · Numerical Methods in Finance: A MATLAB-Based Introduction
BROWN and HOLLANDER · Statistics: A Biomedical Introduction
BRUNNER, DOMHOF, and LANGER · Nonparametric Analysis of Longitudinal Data in 
Factorial Experiments
BUCKLEW · Large Deviation Techniques in Decision, Simulation, and Estimation
CAIROLI  and DALANG · Sequential Stochastic Optimization
CHAN · Time Series: Applications to Finance
CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression
CHATTERJEE and PRICE · Regression Analysis by Example, Third Edition
CHERNICK · Bootstrap Methods: A Practitioner’s Guide
CHILÈS and DELFINER · Geostatistics: Modeling Spatial Uncertainty
CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and Methodologies
CLARKE and DISNEY · Probability and Random Processes: A First Course with
Applications, Second Edition
*COCHRAN and COX · Experimental Designs, Second Edition
CONGDON · Bayesian Statistical Modelling
CONOVER · Practical Nonparametric Statistics, Second Edition
COOK · Regression Graphics
COOK and WEISBERG · Applied Regression Including Computing and Graphics
COOK and WEISBERG · An Introduction to Regression Graphics
CORNELL · Experiments with Mixtures, Designs, Models, and the Analysis of Mixture
Data, Third Edition
COVER and THOMAS · Elements of Information Theory
COX · A Handbook of Introductory Statistical Methods
*COX · Planning of Experiments
CRESSIE · Statistics for Spatial Data, Revised Edition
CSÖRGO´´ and HORVÁTH · Limit Theorems in Change Point Analysis
DANIEL · Applications of Statistics to Industrial Experimentation
DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences, Sixth Edition
*DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data, 
Second Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.

DAVID · Order Statistics, Second Edition
*DEGROOT, FIENBERG, and KADANE · Statistics and the Law
DEL CASTILLO · Statistical Process Adjustment for Quality Control
DETTE and STUDDEN · The Theory of Canonical Moments with Applications in
Statistics, Probability, and Analysis
DEY and MUKERJEE · Fractional Factorial Plans
DILLON and GOLDSTEIN · Multivariate Analysis: Methods and Applications
DODGE · Alternative Methods of Regression
*DODGE and ROMIG · Sampling Inspection Tables, Second Edition
*DOOB · Stochastic Processes
DOWDY and WEARDEN · Statistics for Research, Second Edition
DRAPER and SMITH · Applied Regression Analysis, Third Edition
DRYDEN and MARDIA · Statistical Shape Analysis
DUDEWICZ and MISHRA · Modern Mathematical Statistics
DUNN and CLARK · Applied Statistics: Analysis of Variance and Regression, Second
Edition
DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences, 
Third Edition
DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large Deviations
*ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis
ETHIER and KURTZ · Markov Processes: Characterization and Convergence
EVANS, HASTINGS, and PEACOCK · Statistical Distributions, Third Edition
FELLER · An Introduction to Probability Theory and Its Applications, Volume I,
Third Edition, Revised; Volume II, Second Edition
FISHER and VAN BELLE · Biostatistics: A Methodology for the Health Sciences
*FLEISS · The Design and Analysis of Clinical Experiments
FLEISS · Statistical Methods for Rates and Proportions, Second Edition
FLEMING and HARRINGTON · Counting Processes and Survival Analysis
FULLER · Introduction to Statistical Time Series, Second Edition
FULLER · Measurement Error Models
GALLANT · Nonlinear Statistical Models
GHOSH, MUKHOPADHYAY, and SEN · Sequential Estimation
GIFI · Nonlinear Multivariate Analysis
GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems
GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate Observations,
Second Edition
GOLDSTEIN and LEWIS · Assessment: Problems, Development, and Statistical Issues
GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing
GROSS and HARRIS · Fundamentals of Queueing Theory, Third Edition
*HAHN · Statistical Models in Engineering
HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners
HALD · A History of Probability and Statistics and their Applications Before 1750
HALD · A History of Mathematical Statistics from 1750 to 1930
HAMPEL · Robust Statistics: The Approach Based on Influence Functions
HANNAN and DEISTLER · The Statistical Theory of Linear Systems
HEIBERGER · Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA · Design and Inference in Finite Population Sampling
HELLER · MACSYMA for Statisticians
HINKELMAN and KEMPTHORNE: · Design and Analysis of Experiments, Volume 1:
Introduction to Experimental Design
HOAGLIN, MOSTELLER, and TUKEY · Exploratory Approach to Analysis
of Variance
HOAGLIN, MOSTELLER, and TUKEY · Exploring Data Tables, Trends and Shapes
*Now available in a lower priced paperback edition in the Wiley Classics Library.

*HOAGLIN, MOSTELLER, and TUKEY · Understanding Robust and Exploratory
Data Analysis
HOCHBERG and TAMHANE · Multiple Comparison Procedures
HOCKING · Methods and Applications of Linear Models: Regression and the Analysis
of Variables
HOEL · Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN · Loss Distributions
HOLLANDER and WOLFE · Nonparametric Statistical Methods, Second Edition
HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition
HOSMER and LEMESHOW · Applied Survival Analysis: Regression Modeling of
Time to Event Data
HØYLAND and RAUSAND · System Reliability Theory: Models and Statistical Methods
HUBER · Robust Statistics
HUBERTY · Applied Discriminant Analysis
HUNT and KENNEDY · Financial Derivatives in Theory and Practice
HUSKOVA, BERAN, and DUPAC · Collected Works of Jaroslav Hajek—
with Commentary
IMAN and CONOVER · A Modern Approach to Statistics
JACKSON · A User’s Guide to Principle Components
JOHN · Statistical Methods in Engineering and Quality Assurance
JOHNSON · Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of Statistics: A
Volume in Honor of Samuel Kotz
JUDGE, GRIFFITHS, HILL, LÜTKEPOHL, and LEE · The Theory and Practice of
Econometrics, Second Edition
JOHNSON and KOTZ · Distributions in Statistics
JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences: From the
Seventeenth Century to the Present
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions,
Volume 1, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions,
Volume 2, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Discrete Multivariate Distributions
JOHNSON, KOTZ, and KEMP · Univariate Discrete Distributions, Second Edition
JURECKOVÁ and SEN · Robust Statistical Procedures: Aymptotics and Interrelations
JUREK and MASON · Operator-Limit Distributions in Probability Theory
KADANE · Bayesian Methods and Ethics in a Clinical Trial Design
KADANE AND SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti Evidence
KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time Data, Second
Edition
KASS and VOS · Geometrical Foundations of Asymptotic Inference
KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to Cluster
Analysis
KEDEM and FOKIANOS · Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE, and LE · Shape and Shape Theory
KHURI · Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW, and SINHA · Statistical Tests for Mixed Linear Models
KLUGMAN, PANJER, and WILLMOT · Loss Models: From Data to Decisions
KLUGMAN, PANJER, and WILLMOT · Solutions Manual to Accompany Loss Models: 
From Data to Decisions
KOTZ, BALAKRISHNAN, and JOHNSON · Continuous Multivariate Distributions,
Volume 1, Second Edition
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes 1 to 9
with Index
*Now available in a lower priced paperback edition in the Wiley Classics Library.

KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Supplement
Volume
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update
Volume 1
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update
Volume 2
KOVALENKO, KUZNETZOV, and PEGG · Mathematical Theory of Reliability of
Time-Dependent Systems with Practical Applications
LACHIN · Biostatistical Methods: The Assessment of Relative Risks
LAD · Operational Subjective Statistical Methods: A Mathematical, Philosophical, and
Historical Introduction
LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition
LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST, and GREENHOUSE ·
Case Studies in Biometry
LARSON · Introduction to Probability Theory and Statistical Inference, Third Edition
LAWLESS · Statistical Models and Methods for Lifetime Data
LAWSON · Statistical Methods in Spatial Epidemiology
LE · Applied Categorical Data Analysis
LE · Applied Survival Analysis
LEE and WANG · Statistical Methods for Survival Data Analysis, Third Edition
LEPAGE and BILLARD · Exploring the Limits of Bootstrap
LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health Statistics
LIAO · Statistical Group Comparison
LINDVALL · Lectures on the Coupling Method
LINHART and ZUCCHINI · Model Selection
LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition
LLOYD · The Statistical Analysis of Categorical Data
MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications in
Statistics and Econometrics, Revised Edition
MALLER and ZHOU · Survival Analysis with Long Term Survivors
MALLOWS · Design, Data, and Analysis by Some Friends of Cuthbert Daniel
MANN, SCHAFER, and SINGPURWALLA · Methods for Statistical Analysis of
Reliability and Life Data
MANTON, WOODBURY, and TOLLEY · Statistical Applications Using Fuzzy Sets
MARDIA and JUPP · Directional Statistics
MASON, GUNST, and HESS · Statistical Design and Analysis of Experiments with
Applications to Engineering and Science
McCULLOCH and SEARLE · Generalized, Linear, and Mixed Models
McFADDEN · Management of Data in Clinical Trials
McLACHLAN · Discriminant Analysis and Statistical Pattern Recognition
McLACHLAN and KRISHNAN · The EM Algorithm and Extensions
McLACHLAN and PEEL · Finite Mixture Models
McNEIL · Epidemiological Research Methods
MEEKER and ESCOBAR · Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of Independent
Random Vectors: Heavy Tails in Theory and Practice
*MILLER · Survival Analysis, Second Edition
MONTGOMERY, PECK, and VINING · Introduction to Linear Regression Analysis,
Third Edition
MORGENTHALER and TUKEY · Configural Polysampling: A Route to Practical
Robustness
MUIRHEAD · Aspects of Multivariate Statistical Theory
MURRAY · X-STAT 2.0 Statistical Experimentation, Design Data Analysis, and 
Nonlinear Optimization
*Now available in a lower priced paperback edition in the Wiley Classics Library.

MYERS and MONTGOMERY · Response Surface Methodology: Process and Product
Optimization Using Designed Experiments, Second Edition
MYERS, MONTGOMERY, and VINING · Generalized Linear Models. With
Applications in Engineering and the Sciences
NELSON · Accelerated Testing, Statistical Models, Test Plans, and Data Analyses
NELSON · Applied Life Data Analysis
NEWMAN · Biostatistical Methods in Epidemiology
OCHI · Applied Probability and Stochastic Processes in Engineering and Physical
Sciences
OKABE, BOOTS, SUGIHARA, and CHIU · Spatial Tesselations: Concepts and
Applications of Voronoi Diagrams, Second Edition
OLIVER and SMITH · Influence Diagrams, Belief Nets and Decision Analysis
PANKRATZ · Forecasting with Dynamic Regression Models
PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and Cases
*PARZEN · Modern Probability Theory and Its Applications
PEÑA, TIAO, and TSAY · A Course in Time Series Analysis
PIANTADOSI · Clinical Trials: A Methodologic Perspective
PORT · Theoretical Probability for Applications
POURAHMADI · Foundations of Time Series Analysis and Prediction Theory
PRESS · Bayesian Statistics: Principles, Models, and Applications
PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach
PUKELSHEIM · Optimal Experimental Design
PURI, VILAPLANA, and WERTZ · New Perspectives in Theoretical and Applied
Statistics
PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic Programming
*RAO · Linear Statistical Inference and Its Applications, Second Edition
RENCHER · Linear Models in Statistics
RENCHER · Methods of Multivariate Analysis, Second Edition
RENCHER · Multivariate Statistical Inference with Applications
RIPLEY · Spatial Statistics
RIPLEY · Stochastic Simulation
ROBINSON · Practical Strategies for Experimenting
ROHATGI and SALEH · An Introduction to Probability and Statistics, Second Edition
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS · Stochastic Processes for Insurance
and Finance
ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and Practice
ROSS · Introduction to Probability and Statistics for Engineers and Scientists
ROUSSEEUW and LEROY · Robust Regression and Outlier Detection
RUBIN · Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN · Simulation and the Monte Carlo Method
RUBINSTEIN and MELAMED · Modern Simulation and Modeling
RYAN · Modern Regression Methods
RYAN · Statistical Methods for Quality Improvement, Second Edition
SALTELLI, CHAN, and SCOTT (editors) · Sensitivity Analysis
*SCHEFFE · The Analysis of Variance
SCHIMEK · Smoothing and Regression: Approaches, Computation, and Application
SCHOTT · Matrix Analysis for Statistics
SCHUSS · Theory and Applications of Stochastic Differential Equations
SCOTT · Multivariate Density Estimation: Theory, Practice, and Visualization
*SEARLE · Linear Models
SEARLE · Linear Models for Unbalanced Data
SEARLE · Matrix Algebra Useful for Statistics
SEARLE, CASELLA, and McCULLOCH · Variance Components
SEARLE and WILLETT · Matrix Algebra for Applied Economics
SEBER · Linear Regression Analysis
*Now available in a lower priced paperback edition in the Wiley Classics Library.

SEBER · Multivariate Observations
SEBER and WILD · Nonlinear Regression
SENNOTT · Stochastic Dynamic Programming and the Control of Queueing Systems
*SERFLING · Approximation Theorems of Mathematical Statistics
SHAFER and VOVK · Probability and Finance: It’s Only a Game!
SMALL and MCLEISH · Hilbert Space Methods in Probability and Statistical Inference
SRIVASTAVA · Methods of Multivariate Statistics
STAPLETON · Linear Statistical Models
STAUDTE and SHEATHER · Robust Estimation and Testing
STOYAN, KENDALL, and MECKE · Stochastic Geometry and Its Applications, Second
Edition
STOYAN and STOYAN · Fractals, Random Shapes and Point Fields: Methods of
Geometrical Statistics
STYAN · The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON, and SONG · Methods for Meta-Analysis in
Medical Research
TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON · Empirical Model Building
THOMPSON · Sampling, Second Edition
THOMPSON · Simulation: A Modeler’s Approach
THOMPSON and SEBER · Adaptive Sampling
THOMPSON, WILLIAMS, and FINDLAY · Models for Investors in Real World Markets
TIAO, BISGAARD, HILL, PEÑA, and STIGLER (editors) · Box on Quality and
Discovery: with Design, Control, and Robustness
TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical Computing
and Dynamic Graphics
TSAY · Analysis of Financial Time Series
UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II:
Categorical and Directional Data
VAN BELLE · Statistical Rules of Thumb
VIDAKOVIC · Statistical Modeling by Wavelets
WEISBERG · Applied Linear Regression, Second Edition
WELSH · Aspects of Statistical Inference
WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and
Methods for p-Value Adjustment
WHITTAKER · Graphical Models in Applied Multivariate Statistics
WINKER · Optimization Heuristics in Economics: Applications of Threshold Accepting
WONNACOTT and WONNACOTT · Econometrics, Second Edition
WOODING · Planning Pharmaceutical Clinical Trials: Basic Statistical Principles
WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical Data,
Second Edition
WU and HAMADA · Experiments: Planning, Analysis, and Parameter Design
Optimization
YANG · The Construction Theory of Denumerable Markov Processes
*ZELLNER · An Introduction to Bayesian Inference in Econometrics
ZHOU, OBUCHOWSKI, and MCCLISH · Statistical Methods in Diagnostic Medicine
*Now available in a lower priced paperback edition in the Wiley Classics Library.


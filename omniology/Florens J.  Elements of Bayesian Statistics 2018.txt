
ELEMENTS OF 
BAYESIAN 
STATISTICS

PURE AND APPLIED MATHEMATICS
A Program o f Monographs, Textbooks, and Lecture Notes
EXECUTIVE EDITORS
Earl J. Taft 
Zuhair Nashed
Rutgers University 
University of Delaware
New Brunswick, New Jersey 
Newark, Delaware
CHAIRMEN OF THE EDITORIAL BOARD
S. Kobayashi 
Edwin Hewitt
University of California, Berkeley 
University of Washington
Berkeley, California 
Seattle, Washington
EDITORIAL BOARD
M. S. Baouendi 
Purdue University
Jack K. Hale 
Brown University
Marvin Marcus 
niversity of California, Santa Barbara
W. S. Massey 
Yale University
Leopoldo Nachbin 
Centro Brasileiro de Pesquisas Fisicas 
and University of Rochester
Anil Nerode 
Cornell University
Donald Passman
University of Wisconsin-Madison
Fred S. Roberts 
Rutgers University
Gian-Carlo Rota 
Massachusetts Institute of 
Technology
David Russell
University of Wisconsin-Madison
Jane Cronin Scanlon 
Rutgers University
Walter Schempp 
Universitat Siegen
Mark Teply 
University of Wisconsin-Milwaukee

MONOGRAPHS AND TEXTBOOKS IN 
PURE AND APPLIED MATHEMATICS
1. 
K. Yano, Integral Formulas in Riemannian Geometry (\910)(out of print,
2. 
S. Kobayashi, Hyperbolic Manifolds and Holomorphic Mappings (1970) 
(out of print)
3. 
V. S. Vladimirov, Equations of Mathematical Physics (A. Jeffrey, editor; 
A. Littlewood, translator) (1970) (out of print)
4. 
B. N. Pshenichnyi, Necessary Conditions for an Extremum (L. Neustadt, 
translation editor; K. Makowski, translator) (1971)
5. 
L. Narici, E. Beckenstein, and G. Bachman, Functional Analysis and 
Valuation Theory (1971)
6. S. S. Passman, Infinite Group Rings (1971)
7. L. Dornhoff Group Representation Theory (in two parts). Part A: 
Ordinary Representation Theory. Part B: Modular Representation Theor 
(1971,1972)
8. W. Boothby and G. L. Weiss (eds.), Symmetric Spaces: Short Courses 
Presented at Washington University (1972)
9. Y. Matsushima, Differentiable Manifolds (E. T. Kobayashi, translator) 
(1972)
10. 
L. E. Ward, Jr., Topology: An Outline for a First Course (1972) (out of 
print)
11. A. Babakhanian, Cohomological Methods in Group Theory (1972)
12. R. Gilmer, Multiplicative Ideal Theory (1972)
13. J. Yeh, Stochastic Processes and the Wiener Integral (1973) (out of print)
14. J. Barros-Neto, Introduction to the Theory of Distributions (1973)
(out of print)
15. R. Larsen, Functional Analysis: An Introduction (1973) (out of print)
16. K. Yano and S. Ishihara, Tangent and Cotangent Bundles: Differential 
Geometry (1973) (out of print)
17. C. Procesi, Rings with Polynomial Identities (1973)
18. R. Hermann, Geometry, Physics, and Systems (1973)
19. N. R. Wallach, Harmonic Analysis on Homogeneous Spaces (1973)
(out of print)
20. J. Dieudonne, Introduction to the Theory of Formal Groups (1973)
21. /. Vaisman, Cohomology and Differential Forms (1973)
22. B. - Y. Chen, Geometry of Submanifolds (1973)
23. 
M. Marcus, Finite Dimensional Multilinear Algebra (in two parts) (1973, 
1975)
24. 
R. Larsen, Banach Algebras: An Introduction (1973)
25. 
R. O. Kujala and A. L. Vitter (eds.), Value Distribution Theory: Part A;
Part B: Deficit and Bezout Estimates by Wilhelm Stoll (1973)
26. 
K. B. Stolarsky, Algebraic Numbers and Diophantine Approximation (197
27. A. R. Magid, The Separable Galois Theory of Commutative Rings (1974)
28. 
B. R. McDonald, Finite Rings with Identity (1974)
29. 
J. Satake, Linear Algebra (S. Koh, T. A. Akiba, and S. lhara, translators)
(1975)

30. J. S. Golan, Localization of Noncommutative Rings (1975)
31. 
G. Klambauer, Mathematical Analysis (1975)
32. 
M. K. Agoston, Algebraic Topology: A First Course (1976)
33. 
K. R. Goodearl, Ring Theory: Nonsingular Rings and Modules (1976)
34. 
L. E. Mansfield, Linear Algebra with Geometric Applications: Selected 
Topics (1976)
35. 
N. J. Pullman, Matrix Theory and Its Applications (1976)
36. 
B. R. McDonald, Geometric Algebra Over Local Rings (1976)
37. 
C. W. Groetsch, Generalized Inverses of Linear Operators: Representation 
and Approximation (1977)
38. J. E. Kuczkowski andJ. L. Gersting, Abstract Algebra: A First Look (1977)
39. 
C. O. Christenson and W. L. Voxman, Aspects of Topology (1977)
40. 
M. Nagata, Field Theory (1977)
41. 
R. L. Long, Algebraic Number Theory (1977)
42. 
W. F. Pfeffer, Integrals and Measures (1977)
43. 
R. L. Wheeden and A. Zygmund, Measure and Integral: An Introduction to 
Real Analysis (1977)
44. J. H. Curtiss, Introduction to Functions of a Complex Variable (1978)
45. 
K. Hrbacek and T. Jech, Introduction to Set Theory (1978)
46. 
W. S. Massey, Homology and Cohomology Theory (1978)
47. 
M. Marcus, Introduction to Modern Algebra (1978)
48. 
E. C. Young, Vector and Tensor Analysis (1978)
49. 
S. B. Nadler, Jr., Hyperspaces of Sets (1978)
50. 
S. K. Segal, Topics in Group Rings (1978)
51. 
A. C. M. van Rooij, Non-Archimedean Functional Analysis (1978)
52. 
L. Corwin and R. Szczarba, Calculus in Vector Spaces (1979)
53. 
C. Sadosky, Interpolation of Operators and Singular Integrals: An 
Introduction to Harmonic Analysis (1979)
54. J. Cronin, Differential Equations: Introduction and Quantitative Theory 
(1980)
55. 
C. W. Groetsch, Elements of Applicable Functional Analysis (1980)
56. 
I. Vaisman, Foundations of Three-Dimensional Euclidean Geometry (1980)
57. 
H. I. Freedman, Deterministic Mathematical Models in Population Ecology 
(1980)
58. 
S. B. Chae, Lebesgue Integration (1980)
59. 
C. S. Rees, S. M. Shah, and C. V. Stanojevic, Theory and Applications of 
Fourier Analysis (1981)
60. 
L. Nachbin, Introduction to Functional Analysis: Banach Spaces and 
Differential Calculus (R. M. Aron, translator) (1981)
61. 
G. Orzech and M. Orzech, Plane Algebraic Curves: An Introduction
Via Valuations (1981)
62. 
R. Johnsonbaugh and W. E. Pfaffenberger, Foundations of Mathematical 
Analysis (1981)
63. 
W. L. Voxman and R. H. Goetschel, Advanced Calculus: An Introduction 
to Modern Analysis (1981)
64. 
L. J. Corwin and R. H. Szcarba, Multivariable Calculus (1982)
65. 
V. I. Istratescu, Introduction to Linear Operator Theory (1981)
66. 
R. D. Jarvinen, Finite and Infinite Dimensional Linear Spaces: A 
Comparative Study in Algebraic and Analytic Settings (1981)

67. 
J. K. Beem and P. E. Ehrlich, Global Lorentzian Geometry (1981)
68. 
D. L. Armacost, The Structure of Locally Compact Abelian Groups (1981)
69. 
J. W. Brewer and M. K. Smith, eds., Emmy Noether: A Tribute to Her Life 
and Work (1981)
70. 
K. H. Kim, Boolean Matrix Theory and Applications (1982)
71. 
T. W. Wieting, The Mathematical Theory of Chromatic Plane Ornaments 
(1982)
72. 
D. B. Gauld, Differential Topology: An Introduction (1982)
73. 
R. L. Faber, Foundations of Euclidean and Non-Euclidean Geometry (1983)
74. 
M. Carmeli, Statistical Theory and Random Matrices (1983)
75. 
J. H. Carruth, J. A. Hildebrant, and R. J. Koch, The Theory of 
Topological Semigroups (1983)
76. 
R. L. Faber, Differential Geometry and Relativity Theory: An 
Introduction (1983)
77. 
S. Barnett, Polynomials and Linear Control Systems (1983)
78. 
G. Karpilovsky, Commutative Group Algebras (1983)
79. 
F. Van Oystaeyen and A. Verschoren, Relative Invariants of Rings: The 
Commutative Theory (1983)
80. 
I. Vaisman, A First Course in Differential Geometry (1984)
81. 
G. W. Swan, Applications of Optimal Control Theory in Biomedicine (1984)
82. 
T. Petrie and J. D. Randall, Transformation Groups on Manifolds (1984)
83. 
K. Goebel and S. Reich, Uniform Convexity, Hyperbolic Geometry, and 
Nonexpansive Mappings (1984)
84. 
T. Albu and C. Nastasescu, Relative Finiteness in Module Theory (1984)
85. 
K. Hrbacek and T. Jech, Introduction to Set Theory, Second Edition, 
Revised and Expanded (1984)
86. 
F. Van Oystaeyen and A. Verschoren, Relative Invariants of Rings: The 
Noncommutative Theory (1984)
87. 
B. R. McDonald, Linear Algebra Over Commutative Rings (1984)
88. 
M. Namba, Geometry of Projective Algebraic Curves (1984)
89. 
G. F. Webb, Theory of Nonlinear Age-Dependent Population 
Dynamics (1985)
90. 
M. R. Bremner, R. V. Moody, and J. Patera, Tables of Dominant Weight 
Multiplicities for Representations of Simple Lie Algebras (1985)
91. 
A. E. Fekete, Real Linear Algebra (1985)
92. 
S. B. Chae, Holomorphy and Calculus in Normed Spaces (1985)
93. 
A. J. Jerri, Introduction to Integral Equations with Applications (1985)
94. G. Karpilovsky, Projective Representations of Finite Groups (1985)
95. L. Narici and E. Beckenstein, Topological Vector Spaces (1985)
96. J. Weeks, The Shape of Space: How to Visualize Surfaces and Three- 
Dimensional Manifolds (1985)
97. 
P. R. Gribik and K. O. Kortanek, Extremal Methods of Operations Research 
(1985)
98. J.-A. Chao and W. A. Woyczynski, eds., Probability Theory and Harmonic 
Analysis (1986)
99. G. D. Crown, M. H. Fenrick, and R. J. Valenza, Abstract Algebra (1986)
100. J. H. Carruth, J. A. Hildebrant, and R. J. Koch, The Theory of
Topological Semigroups, Volume 2 (1986)

101. R. S. Doran and V. A. Belfi, Characterizations of C*-Algebras: The 
Gelfand-Naimark Theorems (1986)
102. M. W. Jeter, Mathematical Programming: An Introduction to 
Optimization (1986)
103. M. Altman, A Unified Theory of Nonlinear Operator and Evolution EquaÂ­
tions with Applications: A New Approach to Nonlinear Partial Differential 
Equations (1986)
104. A. Verschoren, Relative Invariants of Sheaves (1987)
105. R. A. Usmani, Applied Linear Algebra (1987)
106. P. Blass and J. Lang, Zariski Surfaces and Differential Equations in 
Characteristic p> 0(1987)
107. J. A. Reneke, R. E. Fennell, and R. B. Minton. Structured Hereditary 
Systems (1987)
108. H. Busemann and B. B. Phadke, Spaces with Distinguished Geodesics 
(1987)
109. R. Harte, Invertibility and Singularity for Bounded Linear 
Operators (1988).
110. G. S. Ladde, V. Lakshmikantham, and B. G. Zhang, Oscillation 
Theory of Differential Equations with Deviating Arguments (1987)
111. L. Dudkin, I. Rabinovich, and I. Vakhutinsky, Iterative Aggregation 
Theory: Mathematical Methods of Coordinating Detailed and Aggregate 
Problems in Large Control Systems (1987)
112. T. Okubo, Differential Geometry (1987)
113. D. L. Stancl and M. L. Stancl, Real Analysis with Point-Set Topology 
(1987)
114. T. C. Gard, Introduction to Stochastic Differential Equations (1988)
115. S. S. Abhyankar, Enumerative Combinatorics of Young Tableaux (1988)
116. H. Strade and R. Farnsteiner, Modular Lie Algebras and Their 
Representations (1988)
117. J. A. Huckaba, Commutative Rings with Zero Divisors (1988)
118. W. D. Wallis, Combinatorial Designs (1988)
119. W. Wiqsfaw, Topological Fields (1988)
120. G. Karpilovsky, Field Theory: Classical Foundations and Multiplicative 
Groups(1988)
121. S. Caenepeel and F. Van Oystaeyen, Brauer Groups and the Cohomology of 
Graded Rings (1989)
122. W. Kozlowski, Modular Function Spaces (1988)
123. E. Lowen-Colebunders, Function Classes of Cauchy Continuous Maps (1989)
124. M. Pavel, Fundamentals of Pattern Recognition (1989)
125. V. Lakshmikantham, S. Leela, and A. A. Martynyuk, Stability Analysis of 
Nonlinear Systems (1989)
126. R. Sivaramakrishnan, The Classical Theory of Arithmetic Functions (1989)
127. N. A. Watson, Parabolic Equations on an Infinite Strip (1989)
128. K. J. Hastings, Introduction to the Mathematics of Operations Research
(1989)
129. B. Fine, Algebraic Theory of the Bianchi Groups (1989)
130. D. N. Dikranjan, I. R. Prodanov, and L. N. Stoyanov, Topological Groups: 
Characters, Dualities, and Minimal Group Topologies (1989)

131. J. C. Morgan II, Point Set Theory (1990)
132. P. Biler and A. Witkowski, Problems in Mathematical Analysis (1990)
133. H. J. Sussmann, Nonlinear Controllability and Optimal Control
(1990)
134. J. -P. Florens, M. Mouchart, and J. M. Rolin, Elements of Bayesian 
Statistics (1990)
Other Volumes in Preparation


ELEMENTS OF 
BAYESIAN 
STATISTICS
JEAN PIERRE FLORENS
University o f Social Sciences o f Toulouse 
Toulouse, France
MICHEL MOUCHART 
JEAN-MARIE ROLIN
Catholic University o f Louvain 
Louvain  - la-Neuve 
Belgium
MARCEL DEKKER, INC.
New York and Basel

Library of Congress Cataloging-in-Publication Data
Florens, J. P.
Elements of Bayesian statistics / Jean-Pierre Florens, Michel 
Mouchart, Jean-Marie Rolin.
p. 
cm. â (Monographs and textbooks in pure and applied 
mathematics ; 134)
Includes bibliographical references.
ISBN 0-8247-8123-6 (alk. paper)
1. Bayesian statistical decision theory. I. Mouchart, Michel. 
II. Rolin, J. III. Title. IV. Series.
This book is printed on acid-free paper.
Copyright Â©  1990 by Marcel Dekker, Inc. All Rights Reserved.
Neither this book nor any part may be reproduced or transmitted in any 
form or by any means, electronic or mechanical, including photocopying, 
microfilming, and recording, or by any information storage and retrieval 
system, without permission in writing from the publisher.
Marcel Dekker, Inc.
270 Madison Avenue, New York, New York 10016
QA279.5.F56 1990 
519.5â42~dc20
89-77562
CIP
Current printing (last digit):
10 9
8
7
6
5
4
3
2
1
PRINTED IN THE UNITED STATES OF AMERICA

To Nicole, Nicole, 
and Marie-Jeanne


Preface
The M athem atical S tru ctu re of a Bayesian E xperim ent
Statistical theory is usually based on a mathematical structure defined 
by a family of (sampling) probabilities on a measurable space (viz. the 
sample space). Convenience or decision-theoretic considerations typically 
lead to introduce a âparameter spaceâ whose elements index the family 
of sampling probabilities. Endowing the parameter space with a cr-field 
which makes the sampling probabilities measurable is not restrictive (as 
long as a particular structure, such as separability, is not imposed on that cr- 
field). The approach to statistical methods underlying such a mathematical 
structure will be called âclassicalâ (or âsampling theoryâ) and we shall use 
the following notation : (5 ,5 ) and (A, A) denote the sample space and the 
parameter space respectively and P a( X ) stands for the sampling probability 
of X  G S  corresponding to the parameter a E A.
As far as no other restriction than making the sampling probabilities 
measurable is imposed on the cr-field A, the structure just described acÂ­
V

vi
Preface
commodates for so-called parametric as well as for nonparametric or semi- 
parametric methods; more specifically the elements of A may be taken as 
probability measures on the sample space as well as finite dimensional charÂ­
acteristics of a (proper) subset of all probability measures on the sample 
space.
In Bayesian methods, the preceding structure is enriched by a probabilÂ­
ity pi â to be called âa prioriâ â on the parameter space. This induces a 
unique probability II on the product space (A x S', A  0  S) which defines a 
Bayesian experiment. Thus the prior probability fi appears as the marginal 
probability of II on (A, *4) while the sampling probabilities P a constitute 
(regular, by construction) versions of the conditional probabilities of II (on 
(S , S )) given A. Under reasonably unrestrictive conditions, the product 
probability II admits a dual decomposition into a marginal probability P 
on(5, <S), to be called a predictive probability, and regular versions of the 
conditional probabilities given S  to be momentarily denoted fis and to be 
called âposterior probabilitiesâ. Thus the structure of a Bayesian ExperiÂ­
ment may be described as follows:
S â {A x S ) A Â®  S, II =  /i(g)Pa = P(g) /i5).
The object of this monograph is a systematic study of such a structure.
The originality of the Bayesian approach does therefore not he in the 
mathematical structure of the model (in particular, the structure of product 
space is not even essential) but rather in the interpretation of its constituÂ­
tive elements. Thus the cr-fields A  and S  formalize the unobservable (or 
unknown) events and the observable (or known) events respectively. For 
^-m easurable random variables a, and 5-measurable random variables s, 
marginal and conditional expectations will be a center of interest. In parÂ­
ticular, comparing E(a) and E(ci \ S ) will be useful to describe the learning 
process, while comparing E(s) and E(s | *4) will help to characterize the 
sampling process. A Bayesian experiment will therefore be analyzed from 
three points of view: one is based on the joint probability II, another one is 
based on the decomposition of that joint probability into a marginal probÂ­
ability on (A, A) (the prior probability) and the conditional probabilities 
given A  (the sampling probabilities) and a last one is based on the decomÂ­
position of II into a marginal probability on (5, S) (the predictive probabilÂ­
ity) and the conditional probabilities given S  (the posterior probabilities).

Preface
vii
From a strictly formal point of view, the properties of a Bayesian experiment 
may therefore be analyzed as properties of a decomposition (or desintegra- 
tion) of a probability measure into a marginal (or trace) probability and a 
conditional component with respect to a given sub-<j-field.
On Bayesian and Decision T heoretic A pproach
The main stream of thought in this monograph is Bayesian but not 
decision-theoretic oriented, as far as we never consider explicitly a comÂ­
pletely specified decision problem, i.e., a structure with a decision space 
and a loss function.
More specifically, although the first objective of this monograph is a 
systematic exposition of the Bayesian model, we also endeavour to get a 
deeper understanding of the role of the prior probability by examining, for 
instance, how far a given property would be robust to a given modification 
of the prior probability or by comparing concepts and results in a Bayesian 
and in a sampling-theory approach; in particular we give several theorems 
concerning the equivalence of the two approaches.
The decision-theoretic flavour is introduced by means of cr-fields on the 
parameter space representing âparameters of interestâ . Typically, such a 
cr-field would be the smallest one making the loss function measurable (for 
every decision) and would naturally be the first object of inference. Heuris- 
tically we retain, from a decision problem, the characterization of âwhat it 
depends onâ without taking care of the form of such dependence.
P rerequisite Knowledge and Intended A udience
We find it tempting to mimic the introduction of Bourbakiâs treatise 
by stating âwe take statistical analysis at its starting point; thus, reading 
this monograph does not require, in principle, any previous knowledge ... 
in statisticsâ . As a m atter of fact, three domains of knowledge may be 
required to ensure an easy access to this monograph. Firstly, we system atÂ­
ically rely on cr-fields and measurability arguments. This requires a knowlÂ­
edge of the basis of abstract probability theory such as exposited in Chung

viii
Preface
(1968) or Metivier (1972). We also refer to specific results in Neveu (1964), 
in Breiman (1968) or in Dellacherie and Meyer (1975, 1980) when we try to 
avoid repeating a more systematic exposition of the results we use. More 
precisely, the strictly Bayesian theory in its general formulation does not 
require more than a good understanding of conditional expectation but forÂ­
mulation in terms of densities or comparison between sampling theory and 
Bayesian theory are mathematicallly more demanding because such topics 
require the introduction of more structure. In order to make this monoÂ­
graph reasonably self-contained, we have briefly recalled, in the preliminary 
Chapter 0, some elements of probability theory as they are necessary for 
the reading of the first six chapters. The last three chapters use some less 
elementary results that are recalled when they come into use. Secondly, this 
monograph does not systematically treat usual Bayesian models such as norÂ­
mal or binomial models under natural conjugate prior distributions. Those 
models are introduced as examples only and their presentation will generally 
be rather sketchy. Our objective is basically to handle general properties of 
the Bayesian statistical structure; for a more detailed treatm ent of Bayesian 
m odels the reader is referred to classical textbooks as Berger (1980), Box 
and Tiao (1973), De Groot (1970), Good (1950, 1965), Learner (1978), 
Lindley (1975), Raiffa and Schlaifer (1961) or Zellner (1971). Neither does 
this monograph explicitly treat foundational aspects of Bayesian methods, 
but our choice of topics and techniques has clearly been guided by episte- 
mological considerations handled in De Finetti (1974), or Savage (1954); for 
a recent appraisal of the state of the art, see Shafer (1986). Let us menÂ­
tion that although those references on Bayesian models and on foundations 
of Bayesian methods are not a prerequisite for this monograph, they nevÂ­
ertheless provide an im portant guideline for the motivation underlying the 
topics treated. Thirdly some familiarity with sampling-theory concepts such 
as sufficiency, ancillarity or estimability would definitely benefit our reader; 
for these topics, Barra (1971), Cox and Hinkley (1974), Kendall and Stuart 
(1952, 1961 and 1966), or Zacks (1971) are most useful references insofar 
as they provide motivation and point of comparison for the concepts we 
develop. Let us nevertheless mention that our text has been made reasonÂ­
ably self-sufficient by remembering, when appropriate, the corresponding 
sampling-theory definitions and results.
These flexible prerequisites make about two-thirds of this monograph

Preface
ix
suitable as a reference text for a graduate course dealing with the matheÂ­
matical foundations of Bayesian methods. This suggestion is also based on 
the fact that the present version has benefited from earlier presentations 
at various places, in particular at the Statistical Department of Carnegie 
Mellon University (U.S.A.), and to graduate students of the Universities of 
Aix-Marseille II (France), Granada (Spain), Louvain-la-Neuve (Belgium), 
Paris VI, Louis Pasteur at Strasbourg and Toulouse (France).
C ontent
The main theme of this monograph is the theory of reduction of a 
Bayesian Experiment considered as a (unique) probability measure on a 
product space (parameter space x sample space). The main question to be 
considered may be phrased as follows: given that a Bayesian Experiment 
may be âreducedâ by marginalization or by conditioning on the parameter 
space and/or on the sample space, how far does such a reduction lose no 
ârelevantâ information? Clearly such questions are to be handled in terms 
of ancillarity and of sufficiency but the Bayesian approach allows for a 
symmetric treatm ent on the parameter and on the sample space. Also those 
questions will require a formalization of the ideas âlearning by observingâ 
and âwithout losing informationâ . So we also consider questions such as: 
what, in the parameter space, receives or does not receive information from 
the sample? W hat, in the sample space, provides or does not provide, 
information? More than a specific field in statistical methodology the theÂ­
ory of reduction actually develops a general approach to face the statistical 
analysis of empirical data.
That providing tools helpful for the specification of a statistical model 
is at the center of our concern may be appreciated from the following conÂ­
sideration: We systematically draw our results from hypotheses about some 
structural properties on distribution rather than from specific distributional 
assumptions. Thus our results concern classes of models. Our suggestion 
is actually to start the specification of a model by structural hypotheses 
(such as prior independence, admissibility of a conditioning, an invariance 
property), before eventually considering, in a parametric framework, specific 
distributional hypotheses.

X
Preface
Let us now briefly sketch how the material of this book has been alloÂ­
cated to ten chapters. A preliminary Chapter 0 introduces some notation 
and definitions and, more specifically, recalls the main probabilistic tools to 
be used in the sequel. Chapter 1 introduces the basic structure which will 
be systematically investigated, viz., the structure of a Bayesian experiment. 
The basic concepts of different reduced experiments are introduced and alÂ­
ternative interpretations are discussed. Chapter 2 presents the basic theory 
of sufficiency and ancillarity in an unreduced Bayesian experiment, i.e., in 
an experiment where a âcompleteâ observation is available and where the 
parameter of interest is identical to the complete param eter of the model. 
These two basic topics are again treated, for the case of a reduced exÂ­
periment, when introduced are the concepts of mutual sufficiency, mutual 
exogeneity and (Bayesian) cut in a framework of joint reduction, in ChapÂ­
ter 3. Chapter 4 analyzes minimal sufficiency along with related topics; 
there, identification and exact estimability are treated in that framework; 
these concepts will also be essential in asymptotic theory. The relationÂ­
ships between sufficiency and ancillarity have retained a wide attention in 
the sampling theory literature and constitute the topic of Chapter 5. These 
first five chapters refer to what may be called âone-shot analysisâ . The next 
two chapters handle the sequential nature of empirical observations. This 
leads to enrich the basic structure by a filtration (an increasing sequence of 
cr-fields) on the sample space. This new structure will allow us, in Chapter 6, 
to analyze problems of sequential reduction and, in Chapter 7, asymptotic 
questions. Chapter 6 centers on the sequential (observationwise) decompoÂ­
sition of the learning process and its reductions but also with a particular 
attention on models where conditioning on the sample space calls for a 
different, eventually sequential treatm ent of the conditioning cr-field, introÂ­
ducing the concepts of transitivity and of non-causality. Two main themes 
are treated in Chapter 7: on one side, the relationship between (admissible) 
reductions in finite sample and reductions in the asymptotic model; on the 
other side, the concept of (Bayesian) exact estimability as a formalization 
of the idea of consistency of Bayesian methods. 
Chapters 8 and 9 conÂ­
clude this monograph by considering invariant experiments, first in general 
(Chapter 8), next in the analysis of stochastic processes (Chapter 9). In 
Chapter 8 we conclude a general approach to invariance. Thus the basic 
structure of a Bayesian experiment is enriched by the introduction of a famÂ­

Preface
xi
ily of operators acting on the product space (parameters and observations), 
eventually introducing experiments that are invariant for a monoid or for a 
group of transformations. For such experiments, we obtain useful reductions 
and analyze some asymptotic properties. Finally, Chapter 9 may be viewed 
as an application of both Chapters 7 and 8. More specifically, the general 
approch to invariance is applied to the particular case of stationary and of 
exchangeable processes; in that context, asymptotic properties of posterior 
expectations in Markov, AR, MA and ARM A processes are derived.
M ethod of P resentation
We have systematically resorted to the use of conditional independence 
as a unifying principle for the presentation of this monograph. This concept 
is widely used in probability theory, in particular for the analysis of Markov 
processes. In statistics, Hall, Wijsman and Ghosh (1965) relied heavily on 
this concept for their presentation and mention, in their introduction, earÂ­
lier unpublished work by Ch. Stein; see also LeCam (1964). In the late 
sixties, a strong group of French statisticians at the âInstitut Statistique 
de FUniversite de Parisâ (ISUP) made a systematic use of this concept 
(M. Littaye, M. Oheix, J.L. Petit, B. Van Cutsem, Fr. M artin, J.P. Raoult, 
G. Romier, etc...) in a series of papers, partly assembled in a special isÂ­
sue of the Annales de Plnstitut Henri Poincare (1969) (see also Fr. Martin, 
J.L. Petit, M. Littaye (1973)). In 1973, T. Speed, in a private communicaÂ­
tion, drew our attention to these contributions and commented on a long 
bibliography on sufficiency that he was collecting with D. Basu (Basu and 
Speed (1974)). Later on, A.P. Dawid (1978, 1979) also pointed out the 
interest of this concept for Bayesian analysis.
As a general rule, the probabilistic tools are progressively introduced 
as they are used. Thus almost all chapters contain a nonstatistical secÂ­
tion introducing topics of probability theory; those sections present results 
either with proofs or with references to textbooks; in general our âstatisÂ­
ticalâ results will be presented, often without proofs, as simple corollaries 
of probabilistic results. This method of presentation has the advantage of 
alleviating the statistical theory from too much âtechnicalâ digression. 
This will be emphasized by calling âPropositionsâ those results that do not

xii
Preface
require explicit proofs for being simple corollaries of previous âTheoremsâ .
However, it cannot be inferred that this monograph is a simple appliÂ­
cation of probability theory. Indeed statistical motivations lead to develop 
concepts and definitions that are apparently of minor interest in probability 
theory. This implies that when we abstract statistical concepts related to 
statistics or parameters as arbitrary sub-cr-fields on a probability space, we 
obtain new probabilistic definitions and results.
The basic theory is couched in terms of cr-fields; at this level no density 
is used. However each chapter provides sections where the theory is reÂ­
peated in terms of densities for dominated experiments. Similarly, although 
this monograph basically presents the Bayesian theory, each chapter also 
contains sections where the relationships with the analogue sampling theÂ­
ory concepts are commented. Although we hope to have shown, in this 
monograph, that the basic Bayesian theory may be presented in relatively 
simple terms, the reader should nevertheless be warned that the sections 
with densities (i.e., dominated experiments) and the sections on the classiÂ­
cal (or sampling-theory) approach are more mathematically demanding.
O rigin
Our interest in the mathematical foundations of Bayesian experiments 
dates back to the early seventies when, taking advantage of a one-year 
visit of the first author to CORE, we were puzzled by some intricacies 
of a general version of the linear model (Florens, Mouchart and Richard 
(1974, 1976, 1979)). This motivated our interest for a better understanding 
of the mathematics underlying the Bayesian methods (Mouchart (1976)). 
This book reports an effort to systematize more than 10 years of work on 
that field. Many results have been previously reported in published papers 
and in unpublished Discussion Papers (particularly from CORE). In the 
introductory section of each chapter we recall our previous work relative to 
the topic to be handled along with some perspective on the literature. It 
should nevertheless be mentioned that most chapters contain large portions 
of hitherto unpublished material. Apart from new results, materials have 
been included that should make this text open to a wider audience than the 
typical journal reader.

Preface
xiii
A cknow ledgem ent
When concluding a work the span of which has been so many years, it is 
fitting to think gratefully of so many individuals who have made this work 
possible, even if their number makes an exhaustive list unmanageable.
Jacques Dreze has introduced one of us to the beauty of Bayesian thinkÂ­
ing and communicated to all of us his most uncommon enthusiasm for reÂ­
search, rigor and sheer curiosity for the unknown and the misunderstood. 
Deep is our debt to Jacques as a teacher, as a colleague and as a kingpin 
of CORE (Center of Operations Research and Econometrics). Jacques Vo- 
ranger guided the first steps of one of us in statistics (that could only be 
Bayesian) and in decision theory. Last but not least, this book would simply 
not have come to light if Jean-Pierre Raoult had not been the doctorate proÂ­
motor of one of us. His careful reading and his advice were a decisive incenÂ­
tive to proceed toward the publication of our first results. He also launched 
the idea of the âRencontre Franco-Beige de Statisticiensâ , so fruitful a foÂ­
rum for discussion. W ithin the doctoral program of Jean-Pierre Florens, 
Claude Dellacherie had a decisive influence through his guidance into a 
more analytical approach to probability theory.
The development of the works underlying this book has benefited, at 
every stage, from an exceptional scientific environment. In a sense, this 
book is a product of CORE; we particularly appreciated its most stimulating 
atmosphere materialized, in particular, by many insightful discussions with 
Jean-Frangois Mertens. We also thank the Department of M athematics, at 
Louvain-la-Neuve, where Jose Paris, head of the Unit for Probability Theory 
and Statistical Analysis, showed his continuing interest for our work and also 
provided appreciated research facilities. In France, GREQE ( Groupe de 
Recherches en Economie Quantitative et Econometrie de Marseille, France) 
and, later on, GREMAQ (Groupe de Recherche en Economie M athematique 
et Quantitative de Toulouse, France) completed in a most decisive way 
these favourable environments and we want to thank all the colleagues and 
administrative members of those centers.
Many stays in foreign institutions and participation in scientific meetÂ­
ings provided opportunities for exposing our ideas and receiving stimulating 
comments. From too long a list for a complete edition, we want to explicitly

xiv
Preface
thank some individuals with whom we had particularly stimulating discusÂ­
sions and who were instrumental for inviting (at least) one of us to visit 
their department and eventually to share the stimulus of their environment: 
Ph. Nanopoulos (University of Strasbourg), J. Kadane and M. H. de Groot 
(Carnegie Mellon), P. Diaconis (Stanford), K. P. S. Bashkara Rao (InÂ­
dian Statistical Institute, Calcutta), A. Zellner (University of Chicago), 
E. Moreno (University of Granada) along with the participants of the VaÂ­
lencia Meetings of Bayesian Statistics, the International Study Year on 
Bayesian Statistics (at the University of Warwick) and the Franco-Belgian 
Meetings of Statisticians.
Many ideas presented in this book have been elaborated jointly through 
research works with several colleagues. In particular, Jean-FranQois Richard 
has been very closely associated to much of the research reported here; 
also, several results mentioned in this book have been first worked out 
in papers co-authored with David Hendry, Renzo Orsi or Leopold Simar. 
Daniela Cocchi, Anne Feyssolle, Charles Lai Tong, Hugo Roche, Solange 
Scotto, Velayoudoun Marimoutou and Michele Ruggiero have been demandÂ­
ing Ph.D. students whose questions and requirements of clarification had an 
im portant catalytic and progress-provoking role.
At the stage of final presentation of our work, Joseph Hakizamunguâs 
work has been decisive in the preparation of the index. Vicky Barham 
had the desperate job of improving our linguistic talent. 
The late 
Elisabeth Pecquereau was an example of cheerful cooperation to transÂ­
form unreadable cryptics into elegantly typed mathematics and M ariette 
Huysentruyt has been in charge of the final presentation of the manuscript 
into a camera ready proof. Any one knowing the authors recognizes that the 
merits of Elisabeth and Mariette cannot be scaled on any bounded instruÂ­
ment. Sheila Verkaeren was brilliant in coping with the very difficult task 
of synchronizing the typing work within a permanently overloaded adminÂ­
istrative staff. Finally we have always found a totally cooperative attitude 
at Marcel Dekker.
Jean-Pierre Florens 
Michel Mouchart 
Jean-Marie Rolin

Contents
Preface 
v
Notation 
xxv
0. Basic Tools and Notation from Probability Theory 
1
0.1. Introduction 
1
0.2. Measurable Spaces 
2
0.2.1. cr-Fields 
2
0.2.2. Measurable Functions 
5
0.2.3. Product of Measurable Spaces 
9
0.2.4. Monotone Class Theorems 
11
0.3. Probability Spaces 
13
XV

xvi
Contents
0.3.1. Measures and Integrals 
13
0.3.2. Probabilities. Expectations. Null Sets 
15
0.3.3. Transition and Product Probability 
17
0.3.4. Conditional Expectation 
18
0.3.5. Densities 
22
1. Bayesian Experiments 
25
1.1. Introduction 
25
1.2. The Basic Concepts of Bayesian Experiments 
26
1.2.1. General Definitions 
26
1.2.2. Dominated Experiments 
28
1.2.3. Three Remarks on Regular and Dominated
Experiments 
31
1.2.4. A Remark Regarding the Interpretation of Bayesian
Experiments 
32
1.2.5. A Remark on Sampling Theory and Bayesian Methods 
33
1.2.6. A Remark Regarding So-called âImproperâ Prior
Distributions 
34
1.2.7. Families of Bayesian Experiments 
35
1.3. Some Examples of Bayesian Experiments 
36
1.4. Reduction of Bayesian Experiments 
46
1.4.1. Introduction 
_ 
46
1.4.2. Marginal Experiments 
47
1.4.3. Conditional Experiment 
51
1.4.4. Complementary Reductions 
55
1.4.5. Dominance in Reduced Experiments 
59

Contents
xvii
2. Admissible Reductions: Sufficiency and Ancillarity 
65
2.1. Introduction 
65
2.2. Conditional Independence 
66
2.2.1. Notation 
66
2.2.2. Definition of Conditional Independence 
67
2.2.3. Null Sets and Completion 
69
2.2.4. Basic Properties of Conditional Independence 
71
2.2.5. Conditional Independence and Densities 
73
2.2.6. Conditional Independence as Point Properties 
76
2.3. Admissible Reductions of an Unreduced Experiment 
77
2.3.1. Introduction 
77
2.3.2. Admissible Reductions on the Sample Space 
78
2.3.3. Admissible Reductions on the Parameter Space 
79
2.3.4. Some Comments on the Definitions 
79
2.3.5. Elementary Properties of Sufficiency and Ancillarity 
83
2.3.6. Sufficiency and Ancillarity in a Dominated Experiment 
84
2.3.7. Sampling Theory and Bayesian Methods 
87
2.3.8. A First Result on the Relations between Sufficiency
and Ancillarity 
91
3. Admissible Reductions in Reduced Experiments 
95
3.1. Introduction 
95
3.2. Admissible Reduction in Marginal Experiments 
99
3.2.1. Introduction 
99
3.2.2. Basic Concepts 
99
3.2.3. Sufficiency and Ancillarity in Unreduced and in
Marginal Experiments 
100

xviii
Contents
3.2.4. A Remark on âPartialâ Sufficiency 
102
3.3. Admissible Reductions in Conditional Experiments 
105
3.3.1. Introduction 
105
3.3.2. Reductions in the Sample Space 
106
3.3.3. Reductions in the Parameter Space 
107
3.3.4. Elementary Properties 
108
3.3.5. Relationships between Sufficiency and Ancillarity 
109
3.3.6. Sufficiency and Ancillarity in a Dominated Reduced
Experiment 
111
3.4. Jointly Admissible Reductions 
111
3.4.1. Mutual Sufficiency 
112
3.4.2. Mutual Exogeneity 
116
3.4.3. Bayesian Cut 
118
3.4.4. Joint Reductions in a Dominated Experiment 
120
3.4.5. Joint Reductions in a Conditional Experiment 
122
3.4.6. Some Examples 
125
3.5. Comparison of Experiments 
128
3.5.1. Comparison on the Sample Space: Sufficiency 
128
3.5.2. Comparison on the Parameter Space: Encompassing 
134
4. Optimal Reductions: M aximal Ancillarity and
Minimal Sufficiency 
139
4.1. Introduction 
139
4.2. Maximal Ancillarity 
140
4.3. Projections of <r-Fields 
143
4.3.1. Introduction 
143
4.3.2. Definition and Elementary Properties 
143

4.3.3. Projections and Conditional Independence 
145
4.4. Minimal Sufficiency 
153
4.4.1. Minimal Sufficiency in Unreduced and in
Marginal Experiments 
153
4.4.2. Elementary Properties of Minimal Sufficiency 
154
4.4.3. Minimal Sufficiency in a Dominated Experiment 
155
4.4.4. Sampling Theory and Bayesian Methods 
156
4.4.5. Minimal Sufficiency in Conditional Experiment 
157
4.4.6. Optimal Mutual Sufficiency 
158
4.5. Identification Among cr-Fields 
160
4.6. Identification in Bayesian Experiments 
165
4.6.1. Identification in a Reduced Experiment 
165
4.6.2. Sampling Theory and Bayesian Methods 
171
4.7. Exact and Totally Informative Experiments 
176
4.8. Punctual Exact Estimability 
184
5. Optimal Reductions: Further Results 
187
5.1. Introduction 
187
5.2. Measurable Separability 
189
5.3. Measurable Separability in Bayesian Experiments 
197
5.3.1. Measurably Separated Bayesian Experiment 
197
5.3.2. Basu Second Theorem 
200
5.3.3. Sampling Theory and Bayesian Methods 
202
5.4. Strong Identification of cr-Fields 
203
5.4.1. Definition and General Properties 
204
5.4.2. Strong Identification and Conditional
Independence 
208
Contents 
xix

XX
Contents
5.4.3. Minimal Splitting 
214
5.5. Completeness in Bayesian Experiments 
222
5.5.1. Completeness and Sufficiency 
222
5.5.2. Completeness and Ancillarity 
226
5.5.3. Successive Reductions of a Bayesian
Experiment 
230
5.5.4. Sampling Theory and Bayesian Methods 
235
5.5.5. Identifiability of Mixtures 
238
6. Sequential Experiments 
241
6.1. Introduction 
241
6.2. Sequences of Conditional Independences 
242
6.3. Sequential Experiments 
247
6.3.1. Definition of Sequential Experiments 
247
6.3.2. Admissible Reductions in Sequential
Experiments 
249
6.4. Transitivity 
253
6.4.1. Basic Theory 
253
6.4.2. Markovian Property and Transitivity 
258
6.5. Relations Among Admissible Reductions 
267
6.5.1. Admissible Reductions on the Parameter Space 
267
6.5.2. Admissible Reductions on the Sample Space 
267
6.5.3. Admissible Reductions in Joint Reductions 
269
6.6. The Role of Transitivity: Further Results 
276
6.6.1. Weakening of Transitivity Conditions 
276
6.6.2. Necessity of Transitivity Conditions 
280

7. A sym ptotic Experiments 
283
7.1. Introduction 
283
7.2. Limit of Sequences of Conditional Independences 
285
7.3. Asymptotically Admissible Reductions 
292
7.3.1. Asymptotic Properties of Sequential
Experiments 
292
7.3.2. Asymptotic Sufficiency 
293
7.3.3. Asymptotic Admissibility of Joint Reductions 
297
7.3.4. Asymptotically Admissible Reductions in
Conditional Experiments 
301
7.4. Asymptotic Exact Estimability 
306
7.4.1. Exact Estimability and Bayesian Consistency 
306
7.4.2. Sampling Theory and Bayesian Methods 
307
7.5. Estimability of Discrete <j-Fields 
313
7.6. Mutual Conditional Independence
and Conditional 0-1 Laws 
318
7.6.1. Mutual Conditional Independence 
319
7.6.2. Sifted Sequences of <r-Fields 
324
7.7. Tail-Sufficient and Independent
Bayesian Experiments 
329
7.7.1. Bayesian Tail-Sufficiency 
329
7.7.2. Bayesian Independence 
333
7.7.3. Independent Tail-Sufficient Bayesian
Experiments 
335
7.8. An Example 
339
7.8.1. Global and Sequential Analysis 
340
7.8.2. Asymptotic Analysis 
342
7.8.3. The Case /? = oo 
345
Contents 
xxi

xxii
Contents
7.8.4. The Case /3 < oo 
345
8. Invariant Experiments 
349
8.1. Introduction 
349
8.2. Invariance, Ergodicity and Mixing 
352
8.2.1. Invariant Sets and Functions 
352
8.2.2. Invariance as Point Properties 
355
8.2.3. Invariance and Conditional Invariance of
a-Fields 
359
8.2.4. Ergodicity and Mixing 
372
8.2.5. Existence of Invariant Measure 
373
8.2.6. Randomization of the Set of Transformations 
376
8.3. Invariant Experiments 
381
8.3.1. Construction and Definition of
an Invariant Bayesian Experiment 
381
8.3.2. Invariance and Reduction 
390
8.3.3. Invariance and Exact Estimability 
400
9. Invariance in Stochastic Processes 
403
9.1. Introduction 
403
9.2. Bayesian Stochastic Processes and Representations 
404
9.2.1. Introduction 
404
9.2.2. Representation of Experiments 
405
9.2.3. Bayesian Stochastic Processes 
408
9.2.4. Shift and Permutations 
410
9.3. Standard Bayesian Stochastic Processes 
415
9.3.1. Stationary Processes 
415

Contents
xxiii
9.3.2. Exchangeable and i.i.d. Processes 
420
9.3.3. Moving Average Processes 
429
9.3.4. Markovian Stationary Processes 
430
9.3.5. Autoregressive Moving Average Processes 
437
9.3.6. An Example 
440
9.4. Conditional Stochastic Processes 
448
9.4.1. Introduction 
448
9.4.2. Shift in Conditional Stochastic Processes 
451
9.4.3. Conditional Shift-Invariance 
452
Bibliography 
463
Author Index 
489
Subject Index
493


Notation
M : 
cr-field, 2
(M, M ): 
Measurable space, 2
M i C M : 
sub-cr-field, 2
M o = {<!>, M}: 
Trivial cr-field, 2
A ( l M :  
Trace of M  on A, 3
C\tÂ£TMt: 
Intersection of <r-fields, 3
cr(C): 
cr-field generated by C, 3
UterM t'- 
Union of ^-fields, 3
VteT Mt=o-(UteTM t)- 
Wedge of cr-fields, 3
B : 
Borel cr-field of JK, 4
x' ~ x, 4 
M
1 a ' 
Indicator function, 4 
A : : 
Atom of x in M , 4 
M â M / M :  
Quotient set, 5
M: 
Quotient cr-field, 5
XXV

xxvi
Notation
â¢ 
â¢
(M ,M): 
Quotient measurable space, 5
f ~ l (Af) = 
<r-field generated by /, 5
/  : (M, M )  ââº (AT, A/"): 
Measurable function, 5
m : ( M , M )  â âº (JR.B): 
Random variable, Borel function, 6
[M\, [A4]+, [M\oo'- 
Set of random variables, of positive random variables, 
and of bounded random variables respectively, 6
7Tf : XteT Mt 
Mt : 
Coordinate map, 10
(%)tÂ£T M t: 
Product cr-field of the M t, 10
(M t , M t ), 10
M i  <S> M 2 , 10
Bd = B(Md): 
Borel <r-field of lRd, 11
fi : M  
IR^ : 
Measure (positive), 13
(M, A4,p): 
Measure space, 13
I (ral^) = f A m dfi = f A m(x) fi(dx): 
Integral, 14
v <C fi: 
Absolutely continuous, 14
H Â±  v: 
Mutually singular , 14
du/dfz: 
Radon-Nikodym derivative, 15
P: 
Probability, 15
(M ,M, P): 
Probability space, 15
E{m) â f  mdP: 
Expectation (mathematical), 15
M o : 
Completed trivial cr-field, 15, 69
AT: 
Completed sub-cr-field, 16, 69
p
J7 : 
Completion of M  by the P-null sets, 16
||m||p: p-semi-norms, 16
[M\p, 16

Notation
xxvii
m â m ' a.s.P: 
Almost sure equality, 17
Â£ P, 17
P*(Â®) : ( M , M )  ---------<(N,Af): 
Transition, 17
Q Â® P X, 18
A/Vn: 
Conditional expectation of m with respect to A", 18
P(m ) = M o m : 
Expectation, 19 
P^(A): 
Conditional probability, 21
Ptf(A) = P(A): 
Trace of P on AT, Marginal probability, 21 
Pj$*(A): 
Conditional trace probability, 21
(5, S): 
Sample space, 26
Â£ = {(S,Â«S),Pa : a Â£ A}: 
Statistical experiment, 26
(A, *4): 
Parameter space, 26
P a\ 
Sampling probabilities, 26
/i = 11^: 
Restriction of II to A ) 26
P = II5 : 
Restriction of II to S } 26
A V  S: 
a-field generated by (A x 5) U (A x 5), 27
Â£ = (A x 5, A V <S, II): 
Bayesian experiment, 27
Am: 
Sampling expectation, 27
S m : 
Posterior expectation, 27
Â£ = (Ax 5, AVÂ«S, II = ii<g)PA = P(g)^5 ): 
Regular Bayesian experiment,
27
\i: 
Prior probability, 27
P: 
Predictive probability, 27
fis : 
Posterior probabilities, 27
P A : 
Sampling probabilities, 27
P*: 
Privileged dominating probability, 28

xxviii
Notation
g(a,s) â
Measurable family of experiments, 36 
BÂ£Â£(a,o) Bayesian exponential experiment associated to a and <r, 41 
I  ={</>, A x S ] ,  46 
Â£m : Marginal experiment, 47
Conditional experiment, 47 
p a =  n Â£ , fis = n * , p# = n Â£, ^  = n Â£ , 47
= (4  x S, A V T , IIavt): 
Bayesian experiment marginal on T, 48
= (A x 5, 8 V *5, II5V5 )' 
Bayesian experiment marginal on P, 49 
Â£r  =  (4  x S, .4 V Â«S, IIr ): 
Bayesian experiment conditional on T, 51
S B = (A x 5, *4 V S, IIe): 
Bayesian experiment conditional on B , 52
General reduced Bayesian experiment, 54 
Afm: 
conditional expectation of m given M  with respect to \x (g) P, 59
60
<7# ,62
.Mi JL A^2 I M 3 (equivalent to M i  X M 2 | M 3 >P): 
M \  and M 2 are
independent conditionally on M 3 , 67
M \  X  M 2 (equivalent to .Mi X A ^2 | Mq)\ 
(Marginal) Independence 
of A4i and M 2 , 67
T as: 
Conditional expectation of s given T  with respect to P a, 87
T+s: 
Conditional expectation of s given T  with respect to a privileged
dominating probability P*, 89
Â£b v s; n Bvs> 102
M 2M 1 : 
Projection of M i  on M 2 , 143
gS = dPa/dP+, 156
Si â cr{g* : a Â£ A}: 
Sampling minimal sufficient statistic, 156
Ai: 
Sampling minimal sufficient parameter, 156

Notation
xxix
M i < M 2 | M 3(equivalent to M i < M 2 | Ad3;P): 
M i is identified by
M 2 conditionally on M 3, 161
M i < Ad2( equivalent to M i < M 2 | Ado), 161
a '- a ,  171
s
A / s , A / o , 171 
a' ~ a, 175
t
A /i, 175
Afmax') 
P'max') 178
Sa(E) = 1 je7(Â«)* 
Point mass or Dirac distribution, 184
M i  || M 2 | M 3 (equivalent to M i  || M 2 | Ad3;P ): 
M i  and M 2 are 
measurably separated conditionally on Ad3, 190
M i  || M 2 (equivalent to M i  || M 2 | Ado), 190
Adi <Cp M 2 | Ad3 (equivalent to M i  <CP Ad2 | Ad3;P): 
M i  is strongly
p-identified by M 2 conditionally on Ad3, 204
Adi <CP M 2 (equivalent to M i  
M 2 | Ado), 204
Adi <C M 2 | Ad3 (equivalent to Adi <Coo Ad2 | Ad3): 
Adi is strongly 
identified by M 2 conditionally on M 3 , 204
Np(T): Null space of the linear operator T, 205
R q (T ): 
Range of the linear operator T, 206
^00 = v  Tn' 243
n>0
J~n |  Poo: 
Increasing sequence of (7-fields or filtration, 243
*7/ = VP<â<? -7n 
0 < p < g,243
= (Â£n)r = Hn>0 Vm>n 
Tail-<r-field, 286 
Â£n J: 
Â£n is non-increasing, 286
AT*, 
AT**, 302 
Â£**, 305

XXX
Notation
J L i e i K  I M i ,  319 
^  = V,â¬j^..319 
F]i[ = F i-{i) = V i e /  
319
M ^ = { i  E M  : <p_1(A) = A}: 
^-invariant cr-field, 352
^-invariant cr-field, 353
= {^(^) : <P â¬ $} V i E M ,  355
M  = M /~ , 358 
$
<1, A  358
$ I M i (equivalent to $ I M i; P): 
$  invariates .Mi , 359
M ^ = {A â¬ M  : <p~l {A) = A 
a.s.}: 
Almost surely-^-invariant cr-field,
361
Almost surely ^-invariant cr-field, 362
<Â£ I M i | M 2 (equivalent to $ I M i | M 2;P): 
$ invariates M i condiÂ­
tionally on M 2, 364
$L, 
374
M *  = ( 4 v 5 ) $ n M , 390 
M J = (.4vS)inM ,390
*4$: 
Maximal invariant parameter, 391 
A$: 
Almost surely maximal invariant parameter, 391 
S$>: 
Maximal invariant statistic, 391
<S$: 
Almost surely maximal invariant statistic, 391 
MA (q): 
Moving average processes of order <7, 404
ARMA (p, q): 
Autoregressive moving average processes, 404
Â£ = (A x S', A  V 5, II): 
Representation experiment, 407
x = (#n, 7i G .SV): 
Stochastic process, 408
= cr(a?n), 409

Notation
xxxi
x = (xn,n â¬ IN): 
Coordinate process, 409
a, 409
Xn = <r(xn), 409
r(u)n = un+1: 
Shift operator, 410 
Ur- 
cr-field of shift-invariant sets, 411 
Xp: 
Shift-invariant events of the stochastic process, 411
Xp: 
Shift invariant events of the coordinate process, 411
<r(u)n = ua(n) 
V n < k: 
Finite permutation operator, 413
E*, 
E, 413
Ue: 
(T-field of symmetric sets, 414
Xs: 
Symmetric events of the stochastic process, 414
X^. 
Symmetric events of the coordinate process, 414


ELEMENTS OF 
BAYESIAN 
STATISTICS


0
Basic Tools and Notation 
from Probability Theory
0.1 Introduction
This monograph uses three kinds of probabilistic tools:
(i) The basic results of probability theory, up to the properties of the 
conditional expectation with respect to a cr-field; they are essentially asÂ­
sumed to be known by the reader but some are briefly recalled in this 
preliminary chapter.
(ii) Some more original expositions of several probabilistic concepts parÂ­
ticularly powerful in statistics (such as conditional independence, measurÂ­
able separability, weak and strong identification among cr-fields, projection 
of cr-fields). These concepts are presented in separate sections of the chapÂ­
ter in which they are first used. Those probabilistic sections are identified 
by a slightly different notation; they refer to an abstract probability space 
(M ,M , P )  whereas in the statistical sections M  is the Cartesian product
A x  S.
(iii) Some more advanced but standard results of probability theory 
(such as martingale theory, invariance, ergodicity) are used in the last three 
chapters. These results are not reviewed in this chapter; they are recalled, 
and where useful reexpressed using our notation, as they become necessary.
1

2
0. Basic Tools and Notation from Probability Theory
This preliminary chapter thus lists the main concepts used (at least 
in the six first chapters), recalls the usual definitions, and introduces the 
notation employed throughout this book. Our terminology follows, in most 
cases, Dellacherie and Meyer (1975). In actual fact, relatively few concepts 
are necessary to follow our exposition; they essentially relate to the theory 
of the conditional expectation, and to the alternative characterizations of 
measurability. In this domain we apply some results beyond the range of 
elementary textbooks; these results are contained in this chapter. However, 
the basic properties of the concepts used here are not recalled, and the 
reader is referred to the textbook she(he) prefers.
The books on probability theory principally used are Metivier (1972), 
Neveu (1970), and Dellacherie and Meyer (1975 and 1980); Billingsley (1979), 
Chung (1968) and Breiman (1968) are also convenient references for most 
of the topics covered.
0.2 Measurable Spaces
0.2.1 
^-Fields
We first recall the notion of a cr-field of subsets of a set.
0.2.1 D efinition. Let M  be a set and M  be a family of subsets of M. Ad 
is a a-field if:
(i) 
<f) E Ad,
(ii) 
A e M = > A c e M ,
(iii) Ai E Ad 
V i E I 
I countable => Ui^iAi E Ad. 
â 
Using the algebraic properties of set theory, it follows from this definition 
that Ad is closed for every countable sequence of set operations (union, 
intersection, difference, symmetric difference). In particular,M E Ad, and 
if Ai G Ad 
V i G I and I is countable, then ntG/ Ai G Ad.
A measurable space is a pair (M, Ad) where M  is a set and Ad is a 
cr-field of subsets of M. The elements of Ad are called measurable sets or 
events. A sub-a-field Adi of Ad is a cr-field of subsets of M  belonging to Ad. 
The smallest (7-field defined on Mis Ado = {^,M } and is called the trivial 
a-field. It is a sub-(7-field of any cr-field of subsets of M. The largest cr-field

0.2. Measurable Spaces
3
defined on M  -is the family of all subsets of M . For any A C M, the family 
of subsets of A y A D M  = { A n B  : B Â£ M } ,  is a cr-field on A but not on 
M  and is called the trace of M  on A.
0.2.2 Proposition. If T  is an arbitrary index set and if, V t Â£ T, M t is a 
cr-field of subsets of M, then Ht^rMt is a cr-field and is the largest cr-field 
contained in each M t - 
â 
0.2.3 D efinition. If C is any collection of subsets of M, the intersection 
of all the cr-fields of M  containing C (which is a nonempty family) is, by 
Proposition 0.2.2, the smallest <T-field containing C and is called the a-field 
generated by C and denoted by a(C). 
â 
As an example, the cr-field generated by a nontrivial subset of M  â i.e., 
A C  M  with (j) ^  A ^  M  â is given by: <r({A}) = {<^, M, A ) A c}. Definition 
0.2.3 permits the smallest cr-field containing every element of an arbitrary 
collection of cr-fields to be defined. In general, if T  is an arbitrary index set, 
Ut^rMt is not a cr-field. This fact motivates the following definition.
0.2.4 D efinition. If T  is an arbitrary index set and if, V t Â£ T, M t  is a 
cr-field of subsets of M, then a(UtÂ£TMt) is the smallest cr-field containing 
each Mt- It is denoted by 
an(^ 
ca^e(l the wedge of the a-fields
M t- 
â 
0.2.5 Proposition. Let
'P â 
: S C T7, card(5) < oo, 
A% Â£ M t 
V t Â£ Â«?},
1.e., the collection of all finite intersections of elements of Ut ^ M t -  Then
= <r{V). 
m
The following two definitions introduce often suitable properties of a 
cr-field.
0.2.6 D efinition. A cr-field M  of subsets of M  is separable if there exists 
a countable collection of subsets of M  generating M , i.e., 3 C C M  where 
C is countable such that M  = oâ(C). 
â 

4
0. Basic Tools and Notation from Probability Theory
A useful example of separable cr-field is provided by the Borel cr-field of 
a Polish space. If M  is a topological space, the cr-field generated by its open 
sets is called its Borel a-field. It is also the cr-field generated by the closed 
sets. If M  is second countable, i.e., if there exists a countable base for the 
open sets, then the Borel cr-field is separable since it is the cr-field generated 
by this countable base. A Polish space is a topological space for which there 
exists a distance which is compatible with its topology and which makes it 
a complete separable metric space. Recall that a metric space is separable 
if there exists a dense countable subset. Therefore the Borel cr-field of a 
Polish space is generated by the open balls with rational radius and center 
belonging to a dense countable subset. In particular, the Borel cr-field of the 
real line M , denoted by B, is generated by the collection of open intervals 
with rationals endpoints.
A oâ-field M  on M  generates the following equivalence relation:
(0.2.1) 
x' ~ x 
l A(x') = l A(x) 
V A G M ,
J*A
where 1A is the indicator function of the set A, i.e., l^(a?) = 1 if x G A and 
l A(x) = 0 if x Cf A; we shall also use l{xeA} instead of l A(x). The atoms 
of M  are the equivalence classes defined by the equivalence relation (0.2.1). 
We denote by A^f1 the atom of x in M  , i.e.,
(0.2.2) 
A ?  
= 
{ x ' e  M  : l A(x') = l A(x) 
V A â¬ M},
= 
n{A G M  : x G A}.
Note that for any A G M., x G A implies AÂ£* C A. So every measurable 
set is a union of atoms of M .  In general, the atoms of a a-field are not 
measurable sets. Note, however, that {A C M  : 1A(xf) â l^(a:)} is a 
cr-field for any x , x ' G M\ therefore if M. = a(C), we have:
(0.2.3) 
A ?  = p | {*' â¬ M  : M * ')  =  M * )} ,
cec
and thus if Ai  is separable, the atoms are measurable.
0.2.7 D efinition. A cr-field M  of subsets of M  is separating if all its atoms 
are singletons, i.e.,
A ^  = {x} 
V x G M .  
â 

0.2. Measurable Spaces
5
Note that if the singletons are measurable, i.e., {#} E Ad V x E M, 
then Ad is separating. On the other hand, Ad is separating if and only if 
V a:' /  x, 3 A E Ad such that x E A and x' Â£ A. Hence if Ad is separating, 
the singletons are not necessarily measurable. But if Ad is both separable 
and separating the singletons are measurable.
â¢ 
â¢
To any (M, Ad), we can associate the quotient measurable space (M, Ad) 
where M  is the quotient set for equivalence relation (0.2.1) i.e., the set of 
atoms of M:
(0.2.4) 
M = M / M  = { A Â¥  : x E M },
and Ad , the quotient cr-field, is the image of Ad by the canonical application 
which associates its atom to any element of M, i.e.,
(0.2.5) 
Ad= {A : A E Ad},
where
(0.2.6) 
A = {A ** : x e ^ } .
Note that Ad is always separating and if Ad is separable, Ad is separable.
0.2.2 
Measurable Functions
A. Measurability in General
Let M  be a set, (AT,A/*) be a measurable space and /  be a function 
defined on M  with values in N. The family
(0.2.7) 
/ " 1(V) = { r 1(5) : S e V }
is a cr-field on M  called the or-field generated by f  and often denoted by 
*(/)â¢
0.2.8 D efinition. Let (M,Ad) and (N}Af) be two measurable spaces and 
/  : M  ââº N; then /  is measurable if 
C Ad, i.e., if / - 1(M) is a
sub-<T-field of Ad. We shall often condense this property by writing /  : 
(M, Ad) â> (N,Af). 
m

6
0. Basic Tools and Notation from Probability Theory
Thus f ~ 1(Af) is the smallest <r-field on M  which makes f  : M  
( NyAf) 
measurable. On the other hand the largest (r-field on N  which makes /  : 
(M, Ad) ââº N  measurable is given by:
For example, a constant function is measurable for any Ad and Af and 
generates the trivial cr-field Ado . Reciprocally, if Af is separating, a function 
generating the trivial cr-field Ado is constant. Note that measurability is 
preserved under the product of composition.
0.2.9 P roposition. Let (M, Ad), (A, Af), (S, S) be three measurable spaces. 
If /  : M  ââº N  and g : N â* S are both measurable; then g o f  : M  
S 
is measurable. 
â 
A useful criteria for measurability is provided by the following proposiÂ­
tion.
0.2.10 Proposition. Let M , N  be two sets, C be a collection of subsets of 
N  and /  : M  ââº AT, then 0"[/_ 1(C)] = / _ 1[cr(^)]- 
â 
A measurable function, m, defined on (M, Ad) with values in {IR,B) is 
called a random variable or a Borel function. The set of random variables 
defined on (M, Ad) is denoted by [Ad]. The set of positive (respectively 
bounded) random variables is denoted by [Ad]+ (respectively [Ad]oo)- Note 
that, by the above proposition, a real valued function m on (M, Ad) will 
be a random variable as soon as m-1 (] â oo,a]) Â£ Ad V a Â£ Q i.e., for all 
rational numbers.
A random variable is called simple if it takes only a finite number of 
distinct values. For such a random variable, there exists a finite measurable 
partition of M, i.e., {Ai : i Â£ /} C A4 -and a finite number of distinct 
real values {ct- : i Â£ 1} such that:
(0.2.8)
f ( M )  = {B C N  : f ~ \ B )  â¬ M ) .
(0.2.9)
For m <E [Ai]+ , let 
(0.2.10)

0.2. Measurable Spaces
7
Then m n is a sequence of simple random variables increasing to m 
pointwise and <r(mn) C <r(m) V nG  Wo- This is the key fact to prove the 
following theorem due to Doob. It is a very powerful tool for characterÂ­
izing the measurability of a random variable with respect to a sub-cr-field 
generated by a measurable function.
0.2 .11 Theorem . Let M  be a set, (A, A/*) be a measurable space and /  : 
M ââº A. Then the following properties are equivalent:
(i) 
m â¬ [ / - 1(A0 ] 
(resp. [/- 1(A/â)]+ , 
resp. [/"H-Af)]*)),
(ii) 
3 n G [Af] (resp. [A/"]+,resp. [A/]oo) such that m = no f. 
â 
For a proof see, e.g., Metivier (1972) Appendix 1 or Neveu (1970) PropoÂ­
sition II.2.5 or Dellacherie and Meyer (1975) Theorem 1.18. As remarked in 
this last reference this theorem may be extended by considering measurable 
functions with values in a Polish space instead of random variables.
B. M easurability through Atom s: Blackwell T heorem
Another way of proving the measurability of a function relies on the 
atoms of cr-fields. Indeed, if (M, A4) and (A, Af) are two measurable spaces 
and /  is a function defined on M  with values in A, the definition 0.2.8 of 
the measurability of /  clearly implies that:
(0.2.11) 
At* C A r ' W  
V i Â£ M ,
i.e., any atom of / â 1 (Ar) is a union of atoms of .VI. But
(0.2.12) 
Af-'W  = f ~1
Therefore if Af is separating,
(0-2.13) 
A ?  c  r ' M x ) } } ,
or, equivalently, /  is constant on the atoms of M . The Blackwell Theorem 
(in the terminology of Dellacherie and Meyer (1975)) states that under some 
technical assumptions, this is a sufficient condition for the measurability 
of /. On this topic, see also Bhaskara Rao and Rao (1981).
Am \  â 

8
0. Basic Tools and Notation from Probability Theory
0.2.12 D efinition. A cr-field M  of subsets of M  is a Blackwell cr-field if 
M  is separable and if V m Â£ [M] and V A Â£ M> m(A) is an analytic set 
of JR. A Souslin cr-field is a separating Blackwell cr-field. 
â 
An analytic set of M is the projection on 1R of a Borel set of JR2. Remark 
that any Borel set is an analytic set. Remark also that a separable sub- 
cr-field of a Blackwell cr-field is a Blackwell cr-field. Note that if M  is a 
Blackwell cr-field then M  is a Souslin cr-field. The concept of a Souslin cr-field 
is slightly more restrictive than the concept of a separable and separating 
cr-field; this is shown by the following two theorems.
0.2.13 Theorem . A cr-field M  of subsets of M  is separable and separating 
if and only if there exist B C M and 
: (M, M ) ââº (Â£, BOB) bijective and 
bimeasurable. 
â 
0.2.14 Theorem . A cr-field M  of subsets of M  is a Souslin cr-field if and 
only if there exist R, an analytic subset of JR, and <p : (M, M )  â+ (B, BOB) 
bijective and bim easurable. 
â 
For the proofs of these theorems see, e.g., Dellacherie and Meyer (1975) 
Theorem 1.11. and Theorem III.25. More regularity than a Souslin cr-field 
is sometimes required to obtain deeper results. This leads to the notion of 
a Standard Borel cr-field.
0.2.15 D efinition. A cr-field M  of subsets of M  is a Standard Borel or 
Lusin cr-field if and only if there exist B Â£ B (a Borel set) and a mapping 
<p : (M, M ) ââº (B , B O B )  bijective and bimeasurable.
Therefore any standard Borel cr-field is a Souslin cr-field and any Souslin 
cr-field is both separable and separating. Note also that the Borel cr-field of 
a Polish space is a Standard Borel cr-field (see Dellacherie and Meyer (1975)
III.17, III.20 and III.21). We now state the Blackwell Theorem.
0.2.16 Theorem . Let M  be a Blackwell cr-field of subsets of M, M i -be 
a sub-cr-field of M  and M 2 be a separable sub-cr-field of M . Then the 
following properties are equivalent:

0.2. Measurable Spaces 
9
(i) 
M 1 C M 2 ,
(ii) 
A ^ 2 C 
V x G M, i.e., any atom of Adi is a union of atoms
of .M2- 
â 
For a proof see, e.g., Dellacherie and Meyer (1975) Theorem III.26. This 
theorem entails the following important corollary.
0.2.17 Corollary. Let Ad be a Blackwell cr-field of subsets of M , Adi be 
a separable sub-cr-field of Ad, (N,Af) be a measurable space and 
/  : (M, Ad) ââº (N,Af). Then the following properties are equivalent:
(i) 
f ~ 1(Af) C Adi, i.e., /  is M i -measurable,
(ii) 
x' ~ x => f i x 1) ~ f(x).
M i  
Af
In particular if Af is separating, the following properties are equivalent:
(i) 
f ~ 1(Af) C M i ,  i.e., /  is Adi-measurable,
is constant on the atoms of M i .
Therefore if m E [M], the following properties are equivalent:
(i) 
m G [Mi],
(ii) 
m ( A ^ )  = {m(z)}, i.e., m is constant on the atoms of Adi. 
â 
For random variables, the property m 6 [Adi] always implies that m is 
constant on the atoms of M i .  However, the reciprocal condition requires 
that m G [Ad], where Ad is a Blackwell cr-field and A4i is a separable sub- 
cr-field of Ad.
0.2.3 
Product of Measurable Spaces
Let us consider {(Mt, M %) : t G T} an arbitrary family of measurable 
spaces. Let M  = 
Af<, i.e.,
(0.2.14) 
x G M  
<=> 
x = (xt : t G T) 
xt G Mt 
V t G T ,

10
0. Basic Tools and Notation from Probability Theory
and V t E T, 7r* : M  â* Mt the coordinate map, i.e.,
(0.2.15)
7Tt(x) = Xt 
v t e t .
Then the product a-field of the A lt, denoted by Â® tâ¬T Alt, is the cr-field 
of subsets of Mdefined by:
In particular, if Mt = M and Alt = Af V 1 G T, we denote the product of 
the measurable spaces {(Mt,Adt) â¢ t E T} by (MT, A1T) and M T may be 
viewed as the set of all functions defined on T  with values in M.
The product a-field Â® teT M t is also generated as follows: a cylinder 
set A is defined as a set of the form:
where B tj E 
V 1 < j  < k. By Proposition 0.2.5, the cr-field 
Alt 
as defined in (0.2.16) is equal to the cr-field generated by the class of all 
cylinder sets.
The following proposition addresses the measurability of functions with 
values in a product cr-field.
0.2.18 Proposition. Let (A, A ) be a measurable space, T  be an arbitrary 
index set, and {(Mt, Alt)? t E T} be a family of measurable spaces. Then 
/, defined on (A, A ) with values in (X <â¬t  A ft,Â® f6T Alt), is measurable if 
and only if 7Tt o /  : N  ^  Mt is measurable V t E T. 
â 
Let us consider the particular case of two measurable spaces (Mi, Afi) 
and (M2,A42). On M\ x M2, M \ and A l2 induce the following cr-fields, 
called a-fields of cylinder sets.
(0.2.16)
A 
= {x E M : Trtj(x) E Btj 
V 1 < j  < &}, 
= p| { x G M  : *fi â¬
(0.2.17)
(0.2.18)
xpC M i) 
= 
{ A x M 2 - . A z M i } ,  
x J x(M a) 
=  
{Mi x B : B e M 2}.
Then, by definition,
(0.2.19) 
Af i Â®Ad 2 = v f l (Mi) V 7T2 x(Ad2),

0.2. Measurable Spaces
11
and by Proposition 0.2.5, it is the case that
(0.2.20) 
M ! Â® M 2 = * [ { A x B : A e M u  B e M 2}\,
i.e., M \  Â® M 2 is also the cr-field on M\ x M 2 generated by the rectangles.
Warning. 
Throughout this book, we will identify M i and w*1 (Mi) for 
i â 1, 2, and therefore M \ 0  M 2 will be identified t o M \ W M 2. 
â 
This construction is trivially extended to a product of a finite number 
of measurable spaces. The most familiar example is provided by lRd. With 
the usual Euclidian metric, lRd is a Polish space. Using Proposition 0.2.5, 
the Borel (j-field on lRd, B( Md), may be shown to be equal to Bd > i.e., the 
product cr-field of the Borel (T-fields on M. A measurable function defined 
on ( M , M )  with values in (Md,Bd), m = (mi, m2,..., m^)', is called a 
random vector. By Proposition 0.2.18, this is equivalent to the property 
that mi G [M] V I < i < d.
0.2.4 
Monotone Class Theorems
In this section we present theorems useful for proving that a property 
satisfied by a class of subsets is also satisfied by the cr-field generated by 
this class. These theorems are usually called Monotone Class Theorems. 
Before reviewing these theorems, some new definitions are required.
0.2.19 D efinitions. A family C of subsets of M  is called
(i) 
an algebra or a field if
1. (j) G C,
2. A G C 
=> A c eC,
3. A u A 2 e c  => 
Ai U A 2 G C;
(ii) 
a monotone class if, An G C V n G l ,
1Â« An C A n+1 V n G fN 
=> UnewAn G C,
2. An+1 C An V n G l  
=> fl newAn G C\
(iii) a 7v-system if
A iy A 2 G C 
=> 
Ai D A 2 G C;

12
0. Basic Tools and Notation from Probability Theory
(iv) a d-system if
1 . M  e c ,
2. A\,A2 G C 
A\ C A 2 
=>â  
A 2 â A\ E C)
3. An e c  V n e I , i â C  ^ n +1 V n e w  => 
Unâ¬W An e c .  
m
Hence a field is closed for a finite sequence of set operations, a monotone 
class is closed for monotone sequences of subsets and a 7r-system is closed 
for finite intersections. It is important to note that the class of all cylinder 
sets, defined in Section 0.2.3, is a 7r-system. The following theorem will be 
useful for subsequent analysis.
0.2.20 Theorem.
(i) 
If a monotone class of subsets of M  contains a field of subsets of M  
it also contains the cr-field generated by this field.
(ii) 
If a d-system of subsets of M  contains a 7r-system of subsets of M  it 
also contains the <r-field generated by this 7r-system. 
â 
For a proof of (i), the reader is referred to: Dellacherie and Meyer (1975) 
Theorem 1.19 and Chung (1968) Theorem 2.1.2 and Neveu (1970) PropoÂ­
sitions 1.4.1 and 1.4.2; and for a proof of (ii), see: Neveu (1970) Exercise
1.4.5, Breiman (1968) Proposition 2.23, and Blumenthal and Getoor (1968) 
Chapter 0 Theorem 2.2.
We now provide a functional form of the Monotone Class Theorem, 
which is used throughout this monograph.
0.2.21 Theorem. Let M  be a set and C a 7r-system of subsets of M. Let 
Â£ be a vector space of real-valued functions on M. If
(i) 
I A E C V A E C,
(ii) 
1m E Â£,
(iii) f n G C 
V n E l ,  
0 < f n < fn+1 
V n G I ,  /  = supn f n bounded
=> f e e .
Then [o^C)]*, C C. 
â 
For a proof see, e.g., Blumenthal and Getoor (1968) Chapter 0, Theorem
2.3. The main argument is based on the fact that {A C M  : 1a G Â£} is a

0.3. 
Probability Spaces
13
d-system since A\ C A 2 implies 1 a2-Ai = 1a2~ 1 ai and A n |  U
= A 
implies 1 An t 1 a- This theorem is used mainly in connection with PropoÂ­
sition 0.2.5, which states that VteT M t = ^(P) where V is a 7r-system.
0.3 
Probability Spaces
0.3.1 
Measures and Integrals
We first recall the definition of a measure.
0.3.1 D efinition. Let ( M , M )  be a measurable space and p : M  â* M+ . 
Then,
(i) 
p is a (positive) measure on (M>M) if p is countably additive, i.e.,
Ai E M  
Vi E I, I countable, and A{ fl Ay = <j> 
V i ^  i' imply that
p(\JiÂ£iAi) = YliÂ£l P{Ai)i
(ii) the measure p is finite if p(M) < 00,
(iii) the measure p is a-finite if
3 i â E  M , n E I ,  M = UnGzv^n such that //(An) <00 V n E l .  â 
Note that the countable additivity of p implies that //(<^>) = 0. The triple 
( M , M , p ) is then called a measure space. The integral is constructed on 
this measure space (see, for instance, Metivier (1972) Chap. II-4, Billingsley 
(1979) Chap. 3, Sections 15 et 16).
0.3.2 Theorem . Let (M, M^p)  be a cr-finite measure space. There exists 
a unique mapping I 
: [M]+ ââº M+ satisfying:
(i) 
I (cirai 4- c2ra2) = c\ I (mi) + c2 I (m2) V mi E [Al]+, c* E 1K+ 
^ =  1, 2,
(ii) 
m n E [Af]+ 
mn |  m 
=> 
I (mn) |  I (m),
(iii) 
I (1A) = /i(t4) 
V A e  M .
This mapping, called the integral, is also written as I (m) = f  m dp. u

14
0. Basic Tools and Notation from Probability Theory
The proof is based on (0.2.10). Indeed under (i) (ii) and (iii), 
(0.3.1) 
I (m) = sup I (mn), 
and
-f n fi[{m > n}].
(0.3.2) I K ) =
l<k<n 2n â 1 
}
Note also the following alternative notation:
(0.3.3) 
I (ttiI a ) â /  m d(i â j m(x) fi(dx).
Ja 
Ja
A further useful theorem is the Lebesgue Decomposition Theorem.
0.3.3 Theorem . Let (M ,M ) be a measurable space and ft and v be two 
cr-finite measures on (M, Af). Then there exist N  E M. with fi(N) = 0 and 
m E [A4]+ such that:
(i) 
v(A) = v{A fl N) -f- f A mdfi V A E M .
Moreover, if N' E M  with //(N') = 0 and m; E [M]+ also satisfy (i), then,
(ii) u(NAN') = 0,
(iii) fi[{m' ^  m}] = 0. 
â 
For a proof see, e.g., Chow and Teicher (1978) Section 6.5. and Neveu 
(1970) Proposition IV-1-3.
0.3.4 D efinitions. Let /i and v be two (r-finite measures on the measurable 
space
(i) 
v is absolutely continuous with respect to //, and is denoted by v <C AL
if A E M  and fi{A) = 0 imply v(A) = 0.
(ii) fi and v are mutually singular, and is denoted by fi _L z/, if there exists
N  Â£ M  such that fi(N) = Oand v ( Nc) = 0. 
â 
With these definitions, Theorem 0.3.3. may be expressed as:
0.3.5 Corollary. Let fi and v be two cr-finite measures on the same meaÂ­
surable space 
Then there exist two unique cr-finite measures Ai
and A2 such that

0.3.
Probability Spaces
15
(i)
v â Ai + A2,
(ii)
Ai L p ,
(iii)
A2 <  p.
â 
The Radon-Nikodym Theorem is a corollary of Theorem 0.3.3:
0.3.6 Corollary. Let fi and is be two cr-finite measures on the measurable 
space (Af, Af). Then the following properties are equivalent:
(i) 
v <  /L
(ii) 
3 mG [Af]+ essentially unique such thati/(A) = j A m dfi V AEAf. 
u
Essential uniqueness refers to property (iii) of Theorem 0.3.3. A comÂ­
monly used notation for m is dvfdn and it is called a Radon-Nikodym derivaÂ­
tive of v with respect to /i.
0.3.2 
Probabilities. Expectations. Null Sets
Let us first recall the definitions of probability and expectation.
0.3.7 D efinition. Let (Af, Af) be a measurable space. A 'probability is a 
measure P such that P{M) = 1. The triple (Af, A f, P) is then called a probÂ­
ability space and the integral associated to P is called the (mathematical) 
expectation and is denoted indifferently as E(m) = f  mdP 
m G [Af]+. 
â 
The properties of probability and expectation may be found in Metivier 
(1972), Chung (1968) and Neveu (1970).
In the probability space (Af,Af,P), we denote by Afo the sub-cr-field 
of M  defined by
(0.3.4) 
M o = { A Â£ M  : P { A f  = P(A)}.
Thus, Afo is generated by the family of sets of probability zero, called null 
sets, and is called the completed trivial a-field. We also denote, for Af a

16
0. Basic Tools and Notation from Probability Theory
sub-cr-field of M ,
(0.3.5) 
77 = A f VMo .
This is the smallest cr-field containing Af and the (measurable) null sets and 
it is usually called the completed sub-cr-field 77. Remark that this definition 
of (measurable) completion should be distinguished from the Lebesgue comÂ­
pletion by all the subsets of the (measurable) null sets (see Neveu (1970) 1-4). 
Thus, in our definition, the completion M  of M  is equal to M.  If we want
n
to make explicit the role of the probability P  we write 77 
the compleÂ­
tion of Af by the P-null sets. In some cases it will be useful to increase a 
sub-cr-field M i  of M  with the null sets of a second sub-cr-field M 2, i.e., 
M i  V ( M 2 fl M o). When M i  is a sub-cr-field of M 2 , the following relation 
obtains:
(0.3.6) 
M i  V ( M 2 H ATo) = M i  fl M 2 
V M 1 C M 2 C M .
A measurable function defined on a probability space permits the transÂ­
fer of probability from one measurable space to another.
0.3.8 Proposition. Let ( M , M , P )  be a probability space, (N, Af) be a 
measurable space and /  : ( M , M )  â* (N,Af).  If Q =  P  o f ~ l then Q 
is a probability on (AT,Af) called the image under f  of the probability P. 
Moreover for all n E [A/]+, we have equivalently:
(i) 
E q {ti) = EP (n o /),
(ii) 
f N n d { P o f ~ l ) =  JM { n o f )  dP.  
m
For a random variable m E [M\, the p-semi-norms for p E (0,oo] are 
defined by
(0.3.7) 
||m ||p = {Â£[| m  f ]}1^  
p Â£ (0,00),
(0.3.8) 
Ilm l|oo = inf{c : P(| m |> c) = 0},
and the corresponding vector spaces usually denoted by Â£P(M ,M , P )  or 
more simply by Cp will hereafter be denoted by [M]p , i.e.,
(0.3.9)
[M]p = { m e  [M] : ||m||p < 00}.

0.3. 
Probability Spaces
17
The advantage of this notation is that it stresses the measurability propÂ­
erty: if M  is a sub-cr-field of M ,  [Af]p represents the linear subspace of [,M\ p 
of A-measurable random variables. Note that the same notation [M]oo is 
used for bounded and for almost surely bounded random variables. Recall 
that
(0.3.10) 
[M]P2C[M]Pl 
V P l < p 2.
The expectation may be extended as a linear functional on [M]i by 
setting
(0.3.11) 
E(m) = E(m+) -  i?(m"),
where m+ = max(ra, 0) and m~ = (âm)+.
Two random variables, m ,m ' G [M], are said to be almost surely equal 
if P[{m ^  m'}] = 0. This property will be denoted as m = m' a.s. or 
m = mf a.s.P if we want to make explicit the role of the probability P. 
This property is in fact an equivalence relation. Recall that the quotient 
space under this equivalence relation on CP) denoted as
(0.3.12) 
Lp = Cp/
is a Banach space V p G [1, oo], and that L2 is a Hilbert space. If ^ 
^ = 1
and p G [l,oo), then the dual space of 
denoted as L*, is isomorphic to
Lq.
0.3.3 
Transition and Product Probability
Transition is a primordial concept in Bayesian statistics.
0.3.9 Definition. Let (M, M
)  and ( N , A f )  be two measurable spaces. A 
transition (or transition probability, or Markov kernel) from (M, M ) to 
is a mapping P #(Â«) : M  x M  â> [0,1] such that
(i) 
P ' ( A ) e [ M
]  VAGA',
(ii) 
P x(â¢) is a probability on (A, A ) V x G M.
We denote such a transition by P #(Â«) : (M, M )
< ( A A ) .

18
0. Basic Tools and Notation from Probability Theory
A transition from (M ,M ) to (N,Af) and a probability on (M,A4) 
jointly define a probability on the product space:
0.3.10 Theorem . Let us consider a probability space (M,AA,Q) and a
transition P #(Â«) : (M, M )  -------- < (N,Af). Then there exists a unique
probability denoted by Q 0  Px on (M x N, M  0  Af) such that
0Q Â® P x) ( A x B ) =  f  P x(B)Q(dx) 
V A e M  
V B e A f .
Ja
Moreover if h E [M 0  Af]+ we have successively that
I n  h (x > y)px(dv)  G [X]+
I m x N h d(Q Â® P*) = f M Q(dx) f N h(x, y) P x{dy).
For a proof see Neveu (1970) III.2. In particular, if P X = P 
V xGM,  
the transition reduces to a probability on (A, Af) and the last property of 
Theorem 0.3.10 reduces to Fubini Theorem.
0.3.4 
Conditional Expectation
In this section, we review the most important properties of the condiÂ­
tional expectation. The conditional expectation will be the most important 
tool used in this monograph.
0.3.11 D efinition. Let ( M, M,  P) be a probability space and M  be a sub-
cr-field of M.  For any m E [Af]+ , we define a conditional expectation of m
with respect to Af, denoted by A/Vrc, as any n E 
such that
E[mr] = E[nr] 
V r E [Ar]+ . 
â 
Remark that n is essentially unique, in the sense that if n and n' satisfy 
Definition 0.3.11 then n = n' a.s.P; and if n satisfies Definition 0.3.11, 
n' â n a.s.P and n1 E [Af], then n' also satisfies Definition 0.3.11.
Warning. 
Afm might be interpreted as an equivalence class for almost 
sure equality. We will not adopt this interpretation here. Since Afm is only 
almost surely defined, any equality involving conditional expectations are 
then almost sure equalities but, in the following, we lighten the notation

0.3. 
Probability Spaces
19
by deleting the âa.s.â for those equalities where the reference probability is 
not ambiguous. Such equalities will thus be verified almost surely by any 
version of these conditional expectations. 
m
The existence of a conditional expectation may be based on the Radon- 
Nikodym Theorem as in Neveu (1970) IV.3. Indeed, if we define, for m E 
[Â«M]oo> A(R) = E[iti1b \ and p(B) = P (B ), then A and p are finite measures 
on (M,M) and A 
p- It may also relies on the Riesz Theorem about 
linear functionals on Hilbert spaces as, e.g., in Dellacherie and Meyer (1975)
II.3. This approach to conditional expectation motivates the notation M m  
introduced by Hunt (1957), rather than the usual notation E{m | M). We 
have adopted Huntâs notation because of its compactness and because, in 
the Hilbert case, M m  actually represents the orthogonal projection of the 
random variable m E [.M ]% onto the subspace [M]^ â¢ In Chapter 4 this 
notation is extended to the concept of projection of a cr-field onto another 
cr-field.
If M  â M o = 
i.e., the trivial cr-field, M om  is a constant and
is equal to E(m). Therefore, for the sake of coherence, we use the notation 
M o m  rather than E(m). Thus, using our notation, the definition of the 
conditional expectation of m with respect to M  may be rewritten as
(0.3.13) 
Mr ne[M]+,
(0.3.14) 
Mo( m â¢ r) = Mo(Mm â¢ r) 
VrE[A/ ]+ .
The computation of conditional expectations relies on the following 
proposition, the proof of which is a direct application of the Monotone 
Class Theorem.
0.3.12 Proposition. Let (M ,A f,P) be a probability space, M  be a sub- 
cr-field of M  and C be a 7r-system of subsets of M  such that M  = cr(C). If 
m E [Af]* and n E [Ar]i' satisfy:
(i) 
M om  â M on,
(ii) 
M 0( m l A) = M 0(nl A) 
VAE C,
then,

20
0. Basic Tools and Notation from Probability Theory
(iii) n = Afm 
a.s.P. 
â 
We now list the basic properties of conditional expectations. For a 
proof see, e.g., Metivier (1972) IV.2. or Neveu (1970) IV.3. The first set 
of properties are the same as those of the expectation except that they are 
only almost surely satisfied.
0.3.13 Proposition. Let 
be a probability space and Af be a
sub-cr-field of AA. Then:
(i) 
Af{cm) â cAf{m) 
V m Â£ [M]+ 
V c Â£ iK+ ,
(ii) 
Af(rrti + m2) = Afm\ + Afm 2 
V mÂ» Â£ [M]+ 
i = 1, 2,
(iii) rrii Â£ [M]+ 
i = 1,2 and mi < m2 imply 0 < Afm 1 < Afm2,
(iv) m n Â£ [M]+ 
V n Â£ iV and mn f m imply jVm = supn Afmn . 
m
In contrast, the following set of properties are particular to conditional 
expectations.
0.3.14 P roposition. Let (M ,A 4,P) be a probability space.
(i) 
If Af is a sub-cr-field of M , then: V m Â£ [A4]+ , 
V n Â£  [A/]+ ,
Af {mn) = n â¢ Â«Vm.
In particular, .V(n) = n and so W*(1m ) = 1-
(ii) 
If Afi C M  
i = 1,2 and Wi C A 2 then,
A/2 (A/i m) = Af\{Af2m) â Af\m V m Â£ [M]+.
In particular, Mo{Afm) = M om  
V Af C M , 
V m Â£ [A"(]+ . 
â 
Concerning the completion of sub-cr-fields by null sets, it is easily verified 
that:
(0.3.15) 
Afm = Afm 
a.s.P
although the equivalence class corresponding to Tfm is larger than that of 
Afm.

0.3. 
Probability Spaces
21
As for the expectation, the conditional expectation may be defined on 
[Ad]i by setting:
(0.3.16) 
Afm â A^ra+ â Afm~ .
Under this extension, the conditional expectation with respect to Af 
becomes a linear operator from [Ad]i into [Af]i. Most of the properties of 
the conditional expectations (Proposition 0.3.13 (i) (ii) (iii) and Proposition
0.3.14 (i) and (ii)) remain true provided that they make sense, i.e., +00 â 00 
must be avoided. The Jensen inequality is useful in this context.
0.3.15 P roposition. Let (M, A4,P) be a probability space and Af be a 
sub-<r-field of Ad. If m Â£ [Ad]i and <p : M ââº JR is a convex function such 
that <p(m) Â£ [Ad] 1, then <p(Afm) < Af[(p(m)]. 
â 
The conditional probability is defined through the conditional expectaÂ­
tion.
0.3.16 D efinition. Let (M, Ad, P) be a probability space and Af be a sub- 
cr-field of Ad, the conditional probability of A Â£ A4 with respect to Af, 
denoted by P ^(A ), is defined by P ^{A ) â A/*(l>i)- 
â 
We shall also use the notation Pjsf to denote the restriction of P to a 
sub-cr-field Af of Ad, i.e.,
Prt(A) = P{A) 
V A Â£ Af.
Ptf is also called the trace of P on Af or the marginal probability on Af. 
Combining these two notations we obtain for any Ad,-, 
i = 1,2, sub-cr- 
fields of Ad:
P f t ( A )  = P M â(A) 
V A â¬ M \ .
Note that Ad2 need not be a sub-cr-field of Adi, but both Adi and Ad2 are 
sub-cr-fields of Ad.
Recall that, in general, a conditional probability need not be a transiÂ­
tion. Indeed, the countable additivity property
(0.3.17) 
P*(l)nAn) = Y ,  P*(An),
n

22
0. Basic Tools and Notation from Probability Theory
for An E Af, 
n E W, 
An fl A n> = <j> V n /  n', is only almost sure. 
Therefore, by the uncountability of all these properties, the null sets where 
it fails may pile down to give a nonnull set.
0.3.17 D efinition. For Af i and M 2 , sub-cr-fields of M , one says that there 
exists a regular version of the conditional probability on Af 1 given M 2 , if 
there exists a transition P #(Â») : (M ,A f2) ---------< (M, A fi) such that
p%;(A) = p *(A) 
V A e M i .
P 9(*) is also called a desintegration of P on Afi given Af2- Jirina 
Theorem identifies a condition which, if satisfied, implies the existence of a 
regular conditional probability.
0.3.18 T heorem . Let (M ,A f,P) be a probability space and Afi be a 
Standard Borel sub-cr-field of M , then for any Af3 C Afi and M 2 C M , 
there exists a regular conditional probability on Af3 given Af2- 
â 
For a proof and more general results see, e.g., Blackwell and Dubbins 
(1975), Hoffman and J0rgensen (1971), Dellacherie and Meyer (1975) III.3, 
Neveu (1970), Corollary of Proposition V.4.4, or Parthasarathy (1977), SecÂ­
tion 46.
0.3.5 
Densities
Let (M, M , P )  be a probability space. If /i is a cr-finite measure on 
(M, Af) such that P <C //, then by the theorem of Radon-Nikodym, we 
know this is equivalent to stating that there exists /  E [Af]* such that
(0.3.18) 
P (A )=  f f  dp, 
V A E A f.
Ja
In the special case of a probability space such an /  is called a density, 
instead of a Radon-Nikodym derivative.
Consider now a transition P*(Â») : (M, A f) 
< (N,Jf) such that
(0.3.19)
Px <C 
V x E M.

0.3. 
Probability Spaces
23
Thus each P x admits a density f x Â£ \AT^. In a Bayesian framework, it 
may be suitable to have versions of these densities such that, considered as 
defined on M  x N, it is bimeasurable, i.e., /*(â¢) G [M Â® Af]. The followÂ­
ing theorem gives conditions implying the existence of such a bimeasurable 
selection of those densities.
0.3.19 Theorem . Let (N,Af) and (M ,M ) be measurable spaces with Af
separable. Let P *(â¢) : (M ,A 4 )---------< (AT, Af) be a transition and Q a
probability on (N, Af) such that
(i) 
P x <  Q 
V x e M .
Then there exists a function /  Â£ [A4 0  
such that
(ii) 
P * ( A )  =  f A
f ( x , y )  
Q ( d y )  
V A  G V .
For a proof see, e.g., Dellacherie and Meyer (1980) V.5.


1
Bayesian Experiments
1.1 Introduction
In this chapter, we introduce the formal structure to be analyzed in this 
monograph. This structure relies on the basic concept of a âBayesian experiÂ­
mentâ . Whereas in sampling theory the basic concept is that of a âstatistical 
experimentâ, i.e., a family of probability measures on a given sample space, 
the Bayesian experiment is characterized by a unique probability measure 
on the product of the parameter space and the sample space. This strucÂ­
ture is first introduced in its full generality, i.e., without imposing regularity 
conditions. Conditions for the existence of a regular conditional probability 
(such as the posterior probability) or for the existence of a measure domiÂ­
nating every sampling probability are discussed subsequently. Examples are 
given to show that, in order to be dealt within a single framework, standard 
situations do indeed require a fully general formulation of the Bayesian exÂ­
periment. The last section of this chapter handles the reduction of Bayesian 
experiments, i.e., the problem of reducing a probability measure on a prodÂ­
uct space and, which permits the introduction of the notions of marginal 
and of conditional experiments; we insist both on the symmetry between 
parameters and observations in the reduction process and on the care to be 
taken for those concepts of parameters and of observations in the treatment 
of a conditional experiment.
Unlike most other chapters, this one does not include a section on some
25

26
1. Bayesian Experiments
topic in probability theory as it does not make use of probabilistic techniques 
beyond those reviewed in the preliminary Chapter 0. It should nonetheless 
be mentioned that in the sections on dominated experiments the treatment 
of densities is more detailed than in most statistical textbooks; it also someÂ­
what idiosyncratic.
Sections 1.2 and 1.4 are based on Florens (1974) and Florens and Mouchart 
(1977), (1986a), but subsections 1.4.4 and 1.4.5 derive from later work. SecÂ­
tion 1.3 is based on Florens and Mouchart (1986b).
1.2 
The Basic Concepts of Bayesian Experiments
1.2.1 General Definitions
We start from a siaiistical experiment Â£ defined as:
where (S', S) is a measurable space (the sample space) and {Pa : a Â£ A} is a 
family of probability measures on the sample space indexed by a parameter 
a belonging to a parameter space A. The probability measures P a are called 
the sampling probabilities. A probability measure II on the product space 
A x S may be constructed by endowing the parameter space with a probÂ­
ability measure p on (A, A) where the <r-field A  makes P a(X) measurable 
for any X  Â£ S and by extending to A  0  S  (in a unique way) the function II 
defined on A  x S  as follows:
Â£ = { ( S , S ) , P a : a e A }
The measure constructed from (1.2.2) is denoted as:
(1.2.2)
(1.2.3)
n = h Â® p a .
We shall also denote by P, the marginal measure on (5, S):
(1.2.4)
p{x) = n(A  x x) x e s.
Note that by the construction (1.2.2), P a becomes a regular version of the 
restriction to S  of the conditional probability II given A  (thus we wrote, in
(1.2.3) P* instead of P a), and that p becomes the restriction of II to A, or

1.2. The Basic Concepts of Bayesian Experiments
27
the marginal measure on {A^A). (On the constructions (1.2.2) and (1.2.3) 
see Theorem 0.3.10.)
R em ark. Strictly speaking, we identify p, a probability measure on (A, A), 
with the restriction of II to the cr-field of cylinders .4 x 5 . More generally, we 
shall systematically identify the sub-cr-fields B C A  (respectively, T  C S) 
with the sub-cr-fields of the corresponding cylinders B x S (respectively, 
A x T); similarly, we identify the product <r-fields A  Â® S  with A  V S, the 
<7-field generated by (*4 x S) U (A x S). The central feature of the Bayesian 
model is that it views a statistical experiment as a (unique) probability 
measure on the product space A x 5; more formally we define:
1.2.1 Definition. A Bayesian experiment is defined by the following probÂ­
ability space:
(1.2.5) 
f  = ( A x S ,^ V 5 ,n )
where (A, A) is called the parameter space and (S>S) is called the sample 
space. The restriction of II on (A, A) is called the prior probability and 
the restriction of II on (S,S) is called the predictive probability. For any 
integrable random variable m defined on A x S', a conditional expectation 
given A , denoted .4m, is called a sampling expectation and a conditional exÂ­
pectation given S , denoted Â«9ra, is called a posterior expectation .A. statistic 
is either a <S-measurable function defined on A x  S with value in some meaÂ­
surable space or, more simply, a sub-<r-field of S. Similarly, a subparameter, 
or more simply a parameter, is a sub-(7-field of A. 
â 
Besides the decomposition of II as in (1.2.3), rendering this concept 
operational often requires the existence of the converse decomposition of II 
into a marginal probability P on (5, Â«S), and a regular conditional probability 
given <S, represented by a transition which is denoted ps \ this observation 
motivates the following definition:
1.2.2 Definition. A regular Bayesian experiment is defined as the followÂ­
ing (desintegrable) Bayesian experiment:
(1.2.6) 
Â£ = ( A x  S, A V S ,  U = f i Â® P A = P Â® n s )
where p and P are respectively the prior and the predictive probabilities, and

28
1. Bayesian Experiments
the transitions PA : (A, A) ---------< (5,5) and ps : (5,5) ---------< (A, A)
are called the sampling and the posterior probabilities, respectively. 
â 
1.2.2 Dominated Experiments
The statistical literature often uses arguments in terms of density, and 
the objective of this section is to introduce those density-based arguments 
relevant to Bayesian experiments. In a sampling theory framework, a domiÂ­
nated statistical experiment is a statistical experiment such that there exists 
a dominating cr-finite measure A, i.e., A is such that Va Â£ A : P a 
A. In 
such a case, the theorem of Halmos and Savage (1949) says that when a staÂ­
tistical experiment is dominated by some measure A, it is also dominated by 
a âprivilegedâ dominating probability; more precisely if P a <C A 
Va G 4, 
there exists Ao C A countable such that ? fl <  ?* <  A 
Va Â£ A for 
any P* of the form P* = YlaeA a (a)Pa where a(a) > 0  Va Â£ Ao and
E a6^oa (a) = 1
In a Bayesian framework, an interesting domination property arises 
when II <  /i Â® P, i.e., II is dominated by the independent product of its 
marginals (on A and on 5). This occurs, in particular, when the sampling 
probabilities are dominated, (i.e., P a <  A 
Va Â£ A) and when there exists a 
bimeasurable Radon-Nikodym density, i.e., there exists an AV5-measurable 
function /(a ,s) such that: 
dPa
ââ  = /(a, s) 
a.e.A Va Â£ A. 
a\
In this situation II is also dominated by p 0  A, since by Fubini Theorem:
T l ( E x X ) =  [  
f(a,s)p(da)\(ds).
J E x X
Furthermore, there also exists an A V 5-measurable function g*(a,s) that, 
for each a Â£ A is a density of P a with respect to P* and such that /(a, s) = 
#*(a, s)f*(s) a.e.^ 0  A where /*(s) = ]CaeA0 a (a) /( a>5)- As the Bayesian 
analogue of the Halmos-Savage Theorem says that II <C p 0) A implies II <C 
p 0  P, in a Bayesian experiment, the predictive probability P  plays a role 
similar to that of a privileged dominating probability, even when such a 
P* does not exist. (An example of this use of P  will be shown up in the 
theory of sufficiency in Section 2.3.7). This assertion is established in the 
next theorem.

1.2. The Basic Concepts of Bayesian Experiments
29
where
1.2.3 
Theorem . If II <C /iÂ®A where A is a cr-finite measure on (S, <S), then 
II <  j i 0 P.
Proof. Let h(a, s) be a positive A  V Â«S-measurable function such that:
(*) 
/ 
h(a,s)p(da)P(ds) = 0
Ja xS
We have to show that this implies:
(**) 
/ 
ft(a,s) /(a, s)//(da)A(ds) = 0
J A x S
Remembering that
P(X) = I L ( A x X ) =  [  
/(a, s)p(da)X(ds)
J A x X
we obtain, using the Fubini theorem:
P ( X ) =  [  k(s)X(ds)
J x
k(s) = / /(a,s)/i(da).
Ja
Therefore (*) may be written as:
I 
/i(a,s) k(s) p(da)X(ds) = 0.
Ja xS
Hence there exists So G S such that A (So) = 0 and
k(s) j h(a, s)p(da) = 0 
Vs G S q.
Ja
Therefore /i(a,s) = 0 
a.s.p 
Vs G S q fl {k > 0}. As well, from the 
definition of fc(s), we also have /(a ,s) = 0 
a.s.// Vs G Sq H {A: = 0}.
Therefore h(a,s)f(a,s) = 0 
a.s.// Vs G Sq. Hence, by Fubini Theorem, 
(*) implies (**) because:
I axs h(aâ s) f ( aâ s)ft(da)A(ds) = f s<> A(ds) f A h(a, s)f(a, s)n(da) = 0. â 
Thus, a Radon-Nikodym derivative of n  with respect to p 0  P may be 
taken as:
= 1 
when k(s) = 0,
(127)

30
1. Bayesian Experiments
where k(s) is defined as in Theorem 1.2.3. Note that any such Radon- 
Nikodym derivative also has the property that:
f A g(a, s)p(da) = 1 
a.s.P
f s g{a) s)P{ds) = 1 
a.s.p
These observations motivate the following definition:
1.2.4 
D efinition. A Bayesian experiment is dominated if the associated 
probability II is dominated by p Â® P. We shall denote g(a,s) a version of 
dli/d(p (Â£) -P). 
â 
Again, by Fubini Theorem, if a Bayesian experiment is dominated, it is 
also regular; indeed (1.2.2) may be written as:
(1.2.8) 
n (E x X ) 
= f dP f gdp
Jx 
Je
= 
j dp j gdP 
E e A , X  e S.
J e  
J x
Therefore, a regular version of the conditional probabilities of II given A  
and given S  may be specified by setting:
(1.2.9) 
PA{X) 
= 
/  gdP 
X â¬ S
Jx
(1.2.10) 
ns (E) 
= 
[  gdn 
E e A
Je
If p is represented by a density h(a) w.r.t. some measure po on (A, A) 
â i.e., dp/dpo = h a,.s.po â ps may also be specified in such a way that it 
is a.s. dominated by po; indeed its density /i(a|s) follows from (1.2.7) and
(1.2.10) and is nothing else than the Bayes Theorem expressed in terms of 
densities:
(1.2.11) 
dX - =
=
â
m n & a . . . . . .
dpo 
f A h(a)f(a, s)p0(da)
The relationship between dominance in sampling theory â i.e., P a <  A 
V a Â£ A â and Bayesian dominance â i.e., II <C p<8 >P â may be interpreted 
as follows. Suppose we build a Bayesian experiment, as in (1.2.2), from a 
given specification of P a and p. If II is dominated by p 0  P this does

1.2. The Basic Concepts of Bayesian Experiments
31
not imply that P a is dominated; but there does exist a family P a, e.g., 
P a = f  gdP as in (1.2.9), such that II = p(&Pa and P a < P  Va Â£ A. Note 
that for such a family of P a,P a( X ) = P a(X) a.s. p for any X  Â£ 5, but 
the exceptional set may depend on X . Conversely, if P a is dominated and 
even if there exists a bimeasurable version /(a ,s) of the Radon-Nikodym 
derivative, this does not imply that P a is dominated by P, but that P a may 
be replaced by an almost equivalent family P a as above.
Note finally that taking advantage of the symmetry (w.r.t. A and S ) of 
the Bayesian experiment, dominance of II by p Â® P is also obtained when 
the posterior probabilities ps are dominated in such a way that there exist 
bimeasurable Radon-Nikodym derivatives, in which case the above analysis 
may be symmetrically replicated.
R em ark. To calculate the Radon-Nikodym derivative of II with respect 
to p 0  P, a probability, rather than with respect to the product of refÂ­
erence measures (such as the Lebesgue or some counting measure) is as 
natural, in a Bayesian framework, as expressing, in hypothesis testing, a 
likelihood ratio as a Radon-Nikodym derivative of a (sampling) probability 
with respect to another (sampling) probability. Indeed, in sampling theory, 
it is known that likelihood ratios lead to the neglect of irrelevant factors in 
the likelihood function. Similarly, in a Bayesian framework, if one defines 
/*(a,s) = dU/d(po Â® A) where po (respectively, A) is considered as a ânatÂ­
uralâ reference measure on (A, A) (resp (5,5)) dominating p (respectively, 
P), one obtains
(1.2.12) 
/*(Â«, s) = h(a)k(s)g(a, s) 
a,.s.po<g>\
where h(a) = dp/dpo and k(s) = d P /d \ are two scale factors providing no 
information with regards to the stochastic association within A x 5.
1.2.3 
Three Remarks on Regular and Dominated Experiments
(i) In general, the existence of a regular conditional probability is a problem 
concerning the regularity of a measurable space. Thus, for instance, if both 
(A, A) and (5,5) are Polish spaces, any probability on (A x 5, A V5) defines 
a regular Bayesian experiment (see Theorem 0.3.18).
(ii) When the sampling probabilities P a are dominated by a cr-finite meaÂ­
sure A, Doob Theorem implies that if 5  is separable there exists an A  V 5-

32
1. Bayesian Experiments
measurable function /(a ,s) such that: Va E A , f(a,s) = dPa/dX a.e. A 
(see Theorem 0.3.19). As seen above, this is sufficient to define g(a, s) as in
(1.2.7) for any probability measure p on (A,*4), so the thus-defined Bayesian 
experiment is dominated and, consequently, regular.
(iii) 
The motivation for defining general Bayesian experiments that are 
neither dominated nor (even) regular is not purely esthetic. Firstly, it is 
worthwhile showing that basic concepts such as sufficiency or identification 
may be defined without relying on so-called âtechnicalâ (or regularity) conÂ­
ditions. Secondly, certain questions both in asymptotic theory and in non- 
parametric methods naturally involve nondominated sampling probabilities, 
and it is therefore useful to adopt a unique framework capable of handling 
these questions. This will be illustrated by some examples in Section 1.3.
1.2.4 A Remark Regarding the Interpretation of Bayesian ExperiÂ­
ments
As stated above, a usual interpretation of a Bayesian experiment conÂ­
sists of considering (A, A) as a space of unknown parameters and p as 
representing a typically subjective prior information: This is motivated, 
for instance, by a decision-theoretic argument after Savageâs Foundations of 
Statistics (Savage (1954)). The structure of a unique probability measure on 
a product space allows one other interpretations. Basically, the distinction 
between the spaces A and S may be based on an informational criterion; 
in particular A may involve not only unknown (and unobservable) parameÂ­
ters but also latent (or unobservable) variables and/or future observations 
of observable variables. In such a case, the probability p on (A, A) may 
be interpreted partly as purely subjective and partly as representation of 
some physical process; this flexibility in the interpretation does not affect 
the mathematical structure thus far developed.
An example of this flexibility may be sketched in the framework of preÂ­
diction problems. Let (Z, Z ) be a measurable space of future observations. 
In the general case, the probability II on A x S induced by a Bayesian exÂ­
periment may be extended to a probability II* on A x 5  x Z by introducing
a sampling transition QAwS : (A x 5, A  V S )  
< (Z , Z) in such a way
that II* = (p 0  P A) 0  Q ^wS. One may consider R , the restriction of II* on 
S V Z . It should be noted that in the probability space (5 x Z, S V Z , R), the

1.2. The Basic Concepts of Bayesian Experiments
33
subjective probability p, on (A>A) has been utilized essentially as a helping 
device to assign a probability on S  V Z\ this is consistent with de Finettiâs 
approach to coherent prediction (de Finetti (1937)). But note also that the 
probability space (S x 2 ,5  V Â£,72) has the same mathematical structure 
as a Bayesian experiment where (Â£, Â£ ) serves as a parameter space. This 
means that the analysis of Bayesian experiments presented below may be 
reinterpreted in a purely predictive framework; in this case, the marginal 
probability on (Z, Z) is a mixture of subjective prior probability on (A, A) 
and of sampling probability transitions (P A and QAvS).
Another example of flexible interpretation is found in the field of hiÂ­
erarchical models, particularly in superpopulation models with finite popÂ­
ulations. In such applications, a vector of parameters b = (6; : i E Af) 
characterizes the elements of a finite population (AT), and a model, with 
parameters c, characterizes the way the bfs have been generated. Interest 
lies in some functions of b and/or of c. The model generating 6, given c, 
may receive either a sampling interpretation or a purely subjective interpreÂ­
tation, and the 6*âs may or may not be observable. The distinction between 
âparameterâ and âobservationâ may therefore seem artificial in such conÂ­
texts; nevertheless, the basic structure of a unique probability on a product 
space remains the basic mathematical object of interest (see, e.g., Cassel, 
Sarndal and Wretman (1977), Cocchi and Mouchart (1986, 1989); see also 
Scott and Smith (1971, 1973), Smith (1983), Sugden (1979, 1985), and Sug- 
den and Smith (1984)). Note that these kinds of models (where the bfs may 
be in very large number) provides one of many motivations to âreduceâ a 
Bayesian experiment into a lower-dimensional one. This will be the object 
of Section 1.4.
1.2.5 
A Remark on Sampling Theory and Bayesian Methods
As indicated above, in a sampling theory framework, a statistical experÂ­
iment, such as (1.2.1), merely specifies a family of sampling probabilities. 
Therefore, both in estimation and in prediction, the only possible object of 
interest is the study of the sampling properties of some function defined on 
the sample space: there is, strictly speaking, no way of escaping conditioning 
on the parameter space. It should nevertheless be pointed out that there is 
some flexibility in the hierarchical model when latent variables or incidental

34
1. Bayesian Experiments
parameters can be integrated out conditionally one some hyper parameters 
(or superpopulation parameters).
In contrast, in a Bayesian experiment, the unique probability measure 
on A x S (or on (A x S x Z)) allows for a great diversity of interests. It is 
still fruitful to ask questions about sampling properties such as: What is the 
sampling distribution of the posterior expectation ? and does it converge 
(in some suitable sense) to the âtrueâ parameter ? These questions are, in 
most Bayesian works, considered of rather minor interest. What is usually 
considered as âgenuinely Bayesianâ is to take full advantage of the comÂ­
plete probabilistic structure to analyze either the predictive distributions 
(marginal distribution on S or on Z given S) by integrating out unknown 
parameters (and, possibly, unobservable variables) or the posterior distribuÂ­
tion on A  given tS, where conditioning is done on already observed variables 
only.
In other words, the basic distinction between sampling theory and BayesÂ­
ian methods is the following: Sampling theory develops properties conÂ­
ditionally on unobserved parameters whereas Bayesian methods condition 
only on already observed variables and integrate out irrelevant unobserved 
parameters or variables.
1.2.6 A Remark Regarding So-called âImproperâ Prior DistribuÂ­
tions
In this monograph we do not introduce prior measures on (A, *4) that 
are not probability measures. It should, however, be mentioned that if the 
prior measure is cr-finite (but not finite), a cr-finite measure on the product 
space A x S may be defined as in (1.1.3). However, the marginal measure 
P on (S,S) may fail to be cr-finite, and consequently the measure transition 
ps may fail to exist due to the non applicability of the Radon-Nikodym 
Theorem (see Corollary 0.3.6 or, for more details, see Mouchart (1976)). 
If P is cr-finite, ps may be defined as a probability transition, but several 
theorems presented below are no longer true; in particular, one has to take 
care of so-called âmarginalization paradoxesâ (see, e.g., Dawid, Stone and 
Zidek (1973)).
We also do not address the problem of specifying a Bayesian experiÂ­
ment, but we consider only a given experiment and analyze its reductions.

1.2. The Basic Concepts of Bayesian Experiments
35
Thus, the problem of specifying a prior distribution for a given sampling 
process does not receive systematic analysis. In particular, the question of 
defining and specifying so-called ânoninformativeâ prior distributions will be 
dealt with only as far as this question is related to the analysis of invariant 
Bayesian experiments, which is studied in Chapter 8. Readers interested in 
more detailed analysis of âimproperâ or ânoninformativeâ prior distribution 
may consult, inter a/w, Bernardo (1979), Hartigan (1964), Jeffreys (1961), 
Villegas (1971, 1972, 1977 a and b) or Zellner (1971).
1.2.7 Families of Bayesian Experiments
As stated above, a Bayesian experiment is defined with reference to a 
unique prior probability p on (A, A). Thus we do not deal systematically 
with families of prior probabilities. In particular, we do not develop an 
analysis in terms of the family of probabilities IP  on A x  S, indexed by the 
set of all probabilities on (A, *4). We shall nevertheless take two steps in that 
direction. The first step is the following proposition, which is motivated by 
the fact that in the subsequent analysis, the null sets of the prior probability 
play an important role. This proposition establishes a dominance property 
between two Bayesian experiments constructed from the same sampling 
process.
1.2.5 P roposition. Let II* = pi 0  PA = Pi 0  p f  
i = 1,2. Then:
P i< l* 2 => 
(Â«') 
Pi <  P2
(U) 
iii <  n 2
(iii) 
f g l  G [A]+
The proof is obvious, after noticing that for any X  E S,Pi(X) is weighted
average of nonnegative P A(X) with pi as weighting function. Note that the
converse implication (in particular, of (i)) is false. This proposition may 
clearly be restated by considering two probabilities II* (i = 1 ,2 ) with comÂ­
mon posterior probabilities ps and different predictive probabilities such 
that Pi <C TV This would clearly imply d lli/d ll2 is S-measurable.
The second step in the direction of a family of prior probabilities is 
taken in the next definition, although the concept of a measurable family 
of experiments has much wider use.

36
1. Bayesian Experiments
1 .2.6 D efinition. Consider a measurable space (M ,M ). One may define 
a measurable family of experiments Â£M on (A x S ,A  V S) by giving transiÂ­
tion probabilities IiM , pM , P AvM, P M and pSwM (which are respectively 
defined on M  x (A V S), M  x A, (A x M) x S , M  x S and (S x M) x A) 
such that
nM =ixM Â® p AwM = p M Â® 
â 
Recall that transition probabilities play a fundamental role in the study of 
conditional probabilities. Similarly, the concept of a measurable family of 
experiments will supply the natural ground for the analysis of conditional 
experiments in Section 1.4. One situation (see Lindley and Smith (1972)) 
in which this concept is useful is found when (M, M )  represents a (posÂ­
sibly parametric) family of prior probabilities on (A, A) for one then has: 
p A v M _  p A . th.is kind of structure is investigated in particular detail in 
the next chapter.
1.3 
Some Examples of Bayesian Experiments
In this section, some well-known Bayesian experiments are discussed to 
illustrate the diversity of situations in which the Bayesian statistician must 
act. We want to show, by way of these examples, that in spite of strikingly 
different levels of mathematical difficulty, there is an identical structure unÂ­
derlying all these situations, viz., a unique probability measure on a product 
space (A x 5 , A  VÂ«S) such that the description of such a structure does not 
require technicalities (such as topological requirements) other than a deÂ­
scription of the sets which are going to be considered as âmeasurableâ , and 
a description of the probability measure II on this product space through 
its dual decomposition p 0  P A and P Â® /i5 .
The first example has a double objective: we want to provide an example 
where it is natural to think in terms of probability measures rather than 
in terms of densities, and we want to show the flexibility possible in the 
interpretation of the basic spaces A and S. In particular, the basic structure 
may also be used in a purely predictive setup where A represents future (or 
unknown) observations, and S represents actual observations.
The second example also has a double motivation. This is a situation

1.3. Some Examples of Bayesian Experiments
37
where reasoning in terms of densities is most natural, and where the symÂ­
metry between the parameters and the observations is very striking.
The last two examples are situations where reasoning in terms of denÂ­
sities are definitely unsuitable; in Example 3, this is because the sampling 
probabilities are not dominated, and in Example 4 this is because the paÂ­
rameter space is âtoo bigâ .
These last two examples are slightly more technical in nature, and are 
merely sketched out in this section. Example 3 is treated more thoroughly 
in Chapters 7 and 9, whereas Example 4 will be revisited in Chapter 8.
Exam ple 1. (A normal experiment)
Suppose first that the statistical experiment specifies that the observation s 
is a p-dimensional vector generated according to a normal distribution with 
unknown expectation a and known covariance matrix V. We write:
The Bayesian experiment is therefore represented as a 2p-dimensional norÂ­
mal vector:
The Bayesian statistician is typically interested either in predicting s before 
observing it; this will be done by means of the marginal distribution of s:
or in revising his/her opinions on a after observing s; this will be done by 
means of the conditional distribution of a given s, which is also p-variate 
normal with moments:
( s \a ) ~ N p(a,V).
Let the prior distribution on a also be normal:
a ~  Np(mo,Vo).
s ~  Np(m0, Vo +  V)
E(a i s) 
= 
w r 1 + ^ ~ 1r 1o /o~1â¢o + v '-y )
V (a|Â« ) 
= 
[ V ^  + V - 1]-1.

38
1. Bayesian Experiments
A first extension of this Bayesian experiment is to consider a prediction 
problem. Let z be a âfutureâ observation generated as s and independently 
of s:
( z \ a , * ) ~ N p(ayV).
We then have a 3p-variate model:
( a ^
(  m 0 ^
(
s
~  Nzp
m 0
>
z )
A  mo
\
V0 
F0 
F0 
F0 + F
Vo 
\  
Vo
In a purely predictive approach, one could consider that the main object 
of interest is the pair (s,z), and that the parameter a has been introduced 
only to facilitate the probability assignment for (s,z); here the object of 
interest becomes a Bayesian predictive experiment:
N-2p
Vo + V 
Vo 
Vo 
V0 + V
T he prior predictive distribution of z is evidently the sam e as the predictive 
distribution of s in the original Bayesian experiment: z ~  N(mo, Vo + V)). 
The prediction of z after observing s will be realized through the distribution 
of (z | s) which is also normal with moments:
E ( z \ s )  
= JE(a|s) = [F0- 1 + F - 1] - 1[F0- 1m0 +  F - 1s]
V(z\s) 
= Vo + V â Vo (Vo + F)-1Fo = F + (F0-1 + V-1)-1.
It is useful to notice that the transformation, in distribution, of z ââº (z | s) 
in the Bayesian predictive experiment is formally the same as the transforÂ­
mation a ââº (a | s) in the original experiment.
Another extension of this original Bayesian experiment would be to 
decompose the probability on a with respect to another parameter c such
that (s | a, c) ~  (s | a). Suppose, for instance:
(a | c)
N k(c0 iC0)
Np(bo + B qc, A q)
where Co (respectively, Ao) is a k x k (respectively, p xp) covariance matrix, 
cq (respectively, bo) is a k x 1 (respectively, p x  1) vector and B q is a p x k

1.3. Some Examples of Bayesian Experiments
39
matrix. In order to obtain the same marginal distribution on a as in the 
original experiment, we assume:
mo 
= 
60 + B 0C0 
Vq 
=  
A q -f B qC qB q.
This kind of structure may arise in hierarchical models, in which case c is 
called a âhyperparameterâ and its role may be to facilitate the specification 
of the prior probability on (A, A).
In all of these situations there is a basic model represented by:
/ c \
c 
a
\  s )
N 2p+k
1 CO \
mo
_ \ m0 /
Co 
C0B '0 C0B '0
B 0Co 
Vo 
Vo
B0Co 
F0 
Vo + V
In some contexts, attention may be concentrated on the probability on 
(a,s), as already treated in the original Bayesian experiment, but in other 
contexts attention is instead focused on the probability on (c, s):
N k+p
Co 
CoB'o 
B0C0 
Vo + V
This is again the same structure as that of the original Bayesian experiment.
Let finally consider an i.i.d. sampling of a normal experiment with 
unknown variance. Here, a = (m , V ) G A = 1RP x Cp where Cp is the 
cone of the p x p positive definite matrices and s = ( s ,...,s n) G S = 
Mnp (depending on the context we shall interpret s as either an np-vector 
or an n x p-matrix). Here A  and S are the usual Borel cr-fields of their 
corresponding Euclidian spaces. The sampling probabilities are described
by
(Si \ a ) ~ i . N ( m , V )
and for the prior specification we shall consider an Inverted-Wishart distriÂ­
bution for V, i.e., V ~  IWp{vo} Vo). For the (prior) conditional distribution 
of (m | V) we again consider an normal distribution with a variance proporÂ­
tional to V, i.e., (m | V) ~  N(mo, tIq1 V). In terms of density (with respect 
to the Lebesgue measure):

40
1. Bayesian Experiments
p(a) 
= 
pi(V) x p 2 (m\V)
=  
[{2"<*/2ir(p-i>P/4 n ? = i r  (("o +  1 -  *)/2) } _1
x |Vo|1'0^21V")ât*'0 +P+1)/2exp { -I trV -'V o }} 
x [(27r)-p/2|no ^l y \~ ll 2 exp - | n 0(m -  mo)'V~l {m â m0)] .
with v0 > p, V0 G Cpi m0 G 
JRP et no > 0.
Note that the prior marginal distribution of m is p-variate Student:
m ~  Sp(mo,ng 1 Vo, vQ âP +  1) with density
.. /'r7,\ 
_  
p/2 r((!/0 + l)/2) 
. 
! 
,_1/2
-  
^ 
r ((â0 _ p + 1) / 2)lnÂ° vÂ°l
x [l +  n0(m â m o/V g^m  â mo)] ^ 0+1^ 2 .
It may be helpful, for the interpretation of this prior specification, to recall 
its moments:
if 1/q > 
p: 
E(m\V) â m0, V(m 
| V) = Uq V
if 1/q > 
p : 
E(m) = mo
if Z^o > 
p + 1 :
if i/0 > 
p + 1 : 
V^m) =
1
Vo -  p -  1 Vo
n 0 V 0.
i/o -  P -  1
Let us now describe the dual decomposition of the probability II on 
A x S. It may be checked (see, e.g., De Groot (1970), Chap. 9 or Press 
(1972), Section 6.2) that the predictive distribution of s, considered as an 
nxp-matrix, is matrix-Student: s ~  S nxV n^o, Vo, In+^o 1 Vn> vo), where 
in is an n-vector with all elements equal to 1, and with density
p(s) = I< | Vo +  (s -  inm'oy [In -  (n0 + n)_ 1iâi{,] (s -  iâm/0)|_('/Â°+n)/2.
where:
K  
=
Vnp/2  JJ r ((^b + 1 -  j) / 2) / r  ((vo + n + 1 -  j) / 2)
3 = 1  
'
x 
\v0 r
/2 In ~ (no +  n) -1
p/2
Furthermore, the posterior distribution of a = (m, V) has the same 
structure as its prior: (V | s) is again Inverted-Wishart (V^ | ^) ~  IWP(i/*, V*)

1.3. Some Examples of Bayesian Experiments
41
and (m | V, s) is again normal (m | V, s) ~  Np(m*,n+ l V) where the paÂ­
rameters of the posterior distributions are computed as follows:
z/* 
= 
i/ 0 -f n
V, = Vo + (s â iâm'oy  [ln -  
(no + n)_1iâ4] (s -  inm'0)
mÂ» 
= 
(no + n)~1(nomo + ns) 
where 
s' = â i'ns
n
n* 
= 
no + n.
The posterior marginal distribution of (m | s) and the posterior moÂ­
ments may be computed in exactly the same manner as a priori, replacing
formally the subindices o by *. This prior specification â called an Inverted
Wishart-Normal Distribution â has different, and interesting properties. 
Firstly, it is âclosedâ under normal sampling in the sense that the transÂ­
formation âprior-to-posteriorâ is only a matter of revising (a finite number 
of) parameters while keeping a same analytical form of the densities, this is 
a property of being ânatural-conjugateâ to the normal sampling (for more 
detail, see, e.g., Raiffa and Schlaifer (1961)). 
â 
Example 2. ( A  canonical Bayesian exponential experiment )
In this example, densities are the natural way to represent probabilities, 
and the symmetry between the parameters and the observations is at an 
extreme. Let A = S = 1RP and A  = S  = Bp. Also let a and a be two <r-finite 
measures, on (A,A) and (5,<S) respectively, such that f  exp (a's)d(a<g>cr) = 1 
where a's is the inner product. (Thus, a more general set-up would be 
to take A and S in duality for a given scalar product). Note that the 
main requirement on a and a is to make the function exp(a's) integrable: 
normalization to 1 is a matter of notational convenience only. The canonical 
Bayesian exponential experiment associated to a and cr is defined as:
BÂ£Â£(a,cr) = ^A x S, A  V 
dll/d(a 0  a) = ea 3 1 ,
i.e., II is characterized by
n  ( E x X ) =  [  
easd(a 0  a) 
E e A  
S e S .
J E x X
In other words, ea s is the density of II with respect to the measure a 0  a 
(which would make A  and S  independent in probability under a suitable

42
1. Bayesian Experiments
normalization of a 0  cr, as soon as a 0  a is a finite measure). Let us now
describe the dual decomposition of II in terms of densities. We first denote
the logarithm of the Laplace transform of a and a as:
L(a) = In J ea$dcr 
M(s) = In J easda.
Note that L (respectively, M) is well defined because f  ea s da > 0 
Va. We 
may now describe the prior and the predictive probability as:
E  -  eL(a) 
â  -  eM(>)
da 
da 
â
This exponential structure is dominated (and regular), and the sampling and 
the posterior probabilities both admit a representation in terms of densities:
â pa's-L(a) 
dp __ a' s-M (s)
da 
da
We may also write the density of II with respect to p <g) P rather than with 
respect to a 0  a:
dH _ 
aâ s-M{ s) -L( a)
d(p <g> P )
In many cases, the measures a and a are themselves defined through denÂ­
sities with respect to ânaturalâ measures ao and a0 (such as the Lebesgue 
measure or the counting measure). More specifically let:
=  e/(a) 
i f !  =
da o 
dao
The prior and predictive densities now become:
_ e/(a)+Â£(a) 
E L  = e9(â)+M(>).
dao 
dao
Also the usual form of the exponential family (for the sampling distribution) 
is obtained as:
dPa 
dao
_  e a's-L(a)+g(s)'
Similarly,
E
.  
_ ea'*-Af(Â»)+/(a)
dao
d{ao Â® (T0)

1.3. Some Examples of Bayesian Experiments
43
Suppose now that the differentiation and integration may be commuted in
/
d 
f d
â  exp(a's â M(s))da and / â exp(a's â L(a))dcr (for sufficient condi- 
ds 
J da
tions see, e.g., Monfort (1982), Barra (1971)), then we obtain:
E(s | a) 
= 
4-L(a) 
E(a \ s) = ^-M(s)
da 
ds
= 3 3 7 ^  
1/(0 1 s) =
As an example consider the case of normal sampling with known (unit) 
variance along with a normal prior distribution (with expectation equal to 
mo and variance equal to 6g 1 ). In such a case, one can easily check that 
s ~  N(m 0, 6q 1 + 1) and (a | s) ~  N[(b0 + l)_1(s + b0m 0) , (b0 + l)â 1]- 
Here both c*o and <7o are the Lebesgue measure (denoted as A).
More specifically, when
d- Â£  
= 
(2, ) - / â exp- ! ( * - Â« ) â
^  
= 
(2x)~1/26Â£/2e x p - y ( a - r a 0)2
the joint density becomes
f 
1 2 
1 2/i
+ 
a60m0 -  ttoijio -  ln(2.)  + t  lnfco >
and the reference measure may be taken as:
1 \  
= 
e ^ )  = e x p ( - i s 2 - i l n 2x
^  
-  
c/(Â«) 
d\ 
~
= 
exp ( - ^ a 2(6o +  1) -  ^b0m l + ab0m 0 -  ^ln(27r) + ^ ln &0 
or, with ip(z) denoting the value of a standardized normal density at z: 
g(s) 
= 
In <p(s)
f(a) 
â 
lny>[(60 +  l) = (a -  6o(6o + l)_1wo)]
-hmo6o(*o + I)-1 - ln[&o(&o + I)-1]

44
1. Bayesian Experiments
or with e0 = b0 (b0 + 1) 1 G (0,1),
f(a) = lny>[(l -  c0)" i(a  -  e0m 0)] -  i[e 0mo â  lneo].
Thus we obtain
L(a) = l n ^ - / ( a )  = l a 2.
It can be checked that:
E(s | a) = -j^-L(a) = a; 
U(s | a) = ^ 
= 1.
da 
da
Similarly:
M (s) 
=  
In ^ . ~ g ( s )
= 
\  ln(6o 1 +  !) + ^ (6o1 +  
1 + 2m0s -  m2],
and it can be checked that:
E(a | s) 
=  
^2W(s) =  i ( 6o 1 + l)- 1[2s6o 1 +  2m0]
_  
s +  6pmo 
&o H- 1
n Â« i Â» )  
= 
Â£ " ( * )  =  j j j r -
Until now the presentation has treated the parameter and the sample 
spaces symmetrically. Let us now consider the sampling, process as given, 
as also the measure a (on (S', <S)) and the function L(a). One may look 
for a prior specification which would ânaturally conjugateâ with the given 
sampling process. Diaconis and Ylvisaker (1979) have proposed the followÂ­
ing analysis. Suppose that <7, and therefore L(a), are such that for some 
(no, mo), and for some reference measure <*o, the integral f  enÂ°(a'mÂ°~L(a))dao 
converges; denote that positive quantity as [&o(no> mo)]-1 . Then the prior 
specification
= fc0(no,mo)e"Â°(a'mÂ° -i(a))
dao
has the property that the posterior density has the form

1.3. Some Examples of Bayesian Experiments
45
where n* = no + 1 and m* = (no + 1) 1(nomo + s). This choice for the 
prior specification corresponds to
dot
f(a ) = In - â = n 0 mf0a -  (n0 + 1 )L(a) + In k0 (n0, m0)
da o
M(s) = â lnAr0(n+,m*) + In Ar0(n0,m 0).
The extension of this experiment to i.i.d. sampling is presented in ChapÂ­
ter 2. 
â 
Exam ple 3. (An asymptotic experiment)
Consider the simple univariate normal model, with known variance, from 
an asymptotic point of view. More specifically let (sj \ a) ~  i.JV(a, 1) j  E IN 
along with a ~  N (0,1). Thus A = M and S â iR^, i.e., s = (si, S2 ...) and 
A  V S is the usual Borel <r-field (generated by the cylinders); p is the stanÂ­
dardized normal probability distribution on JR while P A is an independent 
Gaussian process (with easily found mean and covariance function) for each 
a E A. Thus II = p Â® P A is built without difficulty. Similarly the predictive 
process P (on 2R^) is an exchangeable Gaussian (the structure of exchangeÂ­
ability is investigated more systematically in Chapter 9) process (obtained 
as the projective limit of the finite dimensional symmetric distributions of 
(s i...s â ) with E(sj) = 0,V(8j) = 2,Cov(sJ-,Sfc) = 1 (for j  ^  k)). The 
strong law of large numbers implies that the posterior transition ps may be 
characterized as follows: ps = St a.s. where Sx is the degenerate probability 
giving mass 1 to the point x and t = limsup sn where sn is the sample mean,
i.e., sn = n~ 1 J2 i<j<nsji 
Â°fher words, the posterior distribution gives 
mass 1 to the value of a corresponding to the limiting value of the sample 
mean. We would like to draw attention to the fact that this Bayesian exÂ­
periment is genuinely simple: the probabilities and transitions p , P A yPyps 
and II are easily described even though the sampling probabilities are not 
dominated (once A is not countable). This is an example of a situation 
in which reasoning in terms of density is unsuitable despite the essential 
simplicity of the model.
Exam ple 4. (^4 non parametric experiment)
For simplicity, let (5,5) be the real line along with its Borel sets and 
A = {a : S  ââº [0,1]} =  [0, l]5 . For A, we take the product cr-field, (i.e.,

46
1. Bayesian Experiments
the cr-field generated by all the cylinders based on a Borel set of [0,1] for a 
finite number of coordinates, see Section 0.2.3). For the prior probability, 
Ferguson (1973, 1974) (see also Cifarelli, Muliere and Scarsini (1981), 
Mouchart and Simar (1984b), and Rolin (1983)) has shown that a Dirichlet 
process offers a workable alternative. This process may be built as the 
limit of the projective system defined as follows: for every (T i...T m), 
a measurable finite partition of S', the random vector (a(X i)... a(Tm)) 
is distributed as an m-dimensional Dirichlet distribution with parameters 
(ao(Ti)... ao(Tm)) where ao is a finite (i.e., turns out that, with a Dirichlet 
prior, a is (almost surely) a probability measure on (S', S). For the sampling 
transitions we simply consider a unit size random sample: (s | a) ~  a. Note 
that the product cr-field, A  on A, makes the functions a(T) measurable 
for every T  Â£ S, and therefore the product probability II = p 0  P A is a 
well-defined probability on (A x S ,A  V S).
In the dual decomposition II = P 0 //5 , we have P = Pq, where Po 
is defined by a0 = uqPq, and the posterior transition is again a Dirichlet 
process with parameters ao + 6S, where 6S is the probability measure giving 
mass 1 to the point {s}. 
â 
1.4 
Reduction of Bayesian Experiments
1.4.1 Introduction
We first adapt to Bayesian experiments some of the notation presented 
in Chapter 0.
Let M  C A  V S be a sub-cr-field of A  V S. We write m Â£ [M] to denote 
that m is a real-valued function defined on A x S and is A4-measurable. If it 
is also non-negative, we write m Â£ [A4]+ ; if \m\p is integrable, we write m Â£ 
[M\p, and if m is bounded we write m Â£ [A4]oo. For any A  V 5 -measurable 
integrable or positive function n, its conditional expectation given M. is 
written M n. The trivial cr-field {0,A x 5} is denoted Z; therefore, the 
mathematical expectation of n is denoted In.
One goal of statistical theory is to reduce a given problem to its essenÂ­
tial elements, so as to both simplify and reduce the burden of specification 
and the volume of the computations involved in the treatment of statistical

1.4. Reduction of Bayesian Experiments
47
data. In this section we essentially consider the problem of reducing a given 
Bayesian experiment Â£. There are two natural ways of reducing the probaÂ­
bility II characterizing Â£: marginalizing it, (i.e., considering a restriction of 
II), or conditioning it, (i.e., considering a desintegration of II), in both cases 
with respect to a given sub-cr-field Ai C A  V S. Thus, 11^ ,  the restriction 
of II to Ai defines a marginal experiment Â£m , and UM , the conditional 
probability II given A i , defines a conditional experiment Â£M . Under the 
usual regularity conditions (ensuring the existence of a regular version of 
the conditional probability), Â£M represents a measurable family of experiÂ­
ments (in the sense of Definition 1.2.6, the specification Ai C A  VÂ«S was not 
necessary). Note that when II is not reduced, i.e., is a probability on *4VtS, 
we drop the lower index (and write II rather than II^vs) when there is no 
danger of confusion. Remember that II ^ is identified with p, and II5 with 
P, i.e., when p (resp P) is a probability on A  (respectively, S ) we also drop 
the lower index; furthermore 11^ (respectively, 11^, 11^, IIg) is denoted as 
PA (respectively, ps ,P ^ , p ^ ) .
For reduction by marginalization, the most useful cases are obtained 
when Ai has a product structure: Ai = B\/ S (with Z?C*4)or.M  = .4V T  
(with T  C S). For expository purposes, these reductions are analyzed for, 
and motivated on, both the sample and parameter spaces.
Finally, recall that a sub-cr-field T  C S is typically the <r-field generated 
by some statistic, say Â£, defined on the sample space; a sub-<r-field B C A  
is typically the cr-field generated by some function, say 6, defined on the 
parameter space (see Bahadur (1955b)).
1.4.2 Marginal Experiments
A. M arginalization on th e Sam ple Space
Suppose we start by specifying a Bayesian experiment on 4 x 5  where 
S is the cr-field generated by an unreduced observation s. Then the statisÂ­
tician may want to restrict her/his attention to a statistic t E [<S], either 
because only t is actually observable, or because s would be too expensive 
to observe competely, or because the statistician thinks, rightly or wrongly, 
that âinformation would not be lostâ by observing t only. These reflections 
motivate the following definition.

48
1. Bayesian Experiments
1.4.1 D efinition. The Bayesian experiment marginal on T ,Â£ a vT, is deÂ­
fined as follows:
where UavT is the restriction of II on A  V T. If 8 avT is regular we also 
have:
Exam ple. Consider again a simple univariate normal model with known 
variance: s = ( s i,...,s n) and (sj | a) ~  z.A (a,l). Suppose now that 
only the sign of 
is observable. Here S = IRn, and S is its Borel cr-field. 
The actual observation may be represented by a statistic t = {ti,. â¢. ,tn) 
such as t( = l(o,oo)(5*)> but the sub-cr-field T  of <S, associated to 
is clearly 
invariant under any recoding oft] this justifies our consideration of marginal 
probabilities on a sub-cr-field rather than n  transformed under a measurable 
mapping. Note also that the (t{ | a) are independently distributed as a 
Bernoulli variable with parameter 1 â $ (âa) (where <Â£(â¢) is the cumulative 
distribution function of a standardized normal variable) whereas the prior 
probability on (A, A) is the same in the unreduced experiment 8  as in the 
marginal experiment 8 avT- 
â 
B. M arginalization on the P aram eter Space
In a statistical analysis attention is often focused on some functions 
of the parameter. For instance, in a decision-theoretic approach, the loss- 
function may merely depend on some functions of the parameters. In such 
circumstances #, a sub-cr-field of A, is said to represent the decision paÂ­
rameters (or the parameters of interest) if it is the smallest sub-cr-field of A  
which makes the loss-function measurable for every decision. Consequently, 
it may seem natural to restrict the Bayesian experiment to the cr-field B 
on A only: this leads to the elimination, by integration, of the so-called 
ânuisanceâ parameters (see also Basu (1977) and Dawid (1980)). Another 
reason for undertaking the same type of reduction may be that the statisÂ­
tician thinks, rightly or wrongly, that a function 6, generating the cr-field 
B , is âsufficientâ to describe the sampling process, and that the observation 
will give information about b only.
(1.4.1)
Â£ a v t = {A x S, A v T ,  IU v r)
(1.4.2)
IIa v t =  (iÂ®  P r - Pt Â® V?

1.4. Reduction of Bayesian Experiments
49
1.4.2 Definition. The Bayesian experiment marginal on B , Â£bvs, is deÂ­
fined as follows:
(1.4.3) 
Â£bvs = (A x S', B y  S, Ilgvs)
where Il^ys is the restriction of II on BVS. If Â£bvs is regular we also have:
(1.4.4) 
Ilgvs = Pb Â® P B = P<S>Pb - 
â 
Note that P B is sometimes called the marginalized sampling probability 
and may be computed, in the regular case, as follows (see also Raiffa and 
Schlaifer (1961), Section 2.1.1):
(1.4.5) 
P B(X) = [  P A(X)dpB 
X e S .
Ja
Indeed, for any t E [S]+, Proposition 0.3.14(ii) implies that Bt = B(At).
Example. Consider a simple univariate normal model with unknown variÂ­
ance: s = ( $ ! ,...,sn) and (s( \ a) ~  i.N(m,v) where a = (m,v). SupÂ­
pose first that m is the only parameter of interest. 
Thus we want to 
reduce the original model on (m, v,s) into a marginal model on (m, s). 
If, for example, (v | m) is distributed as an Inverted-Wishart variable â 
(r> | m) ~  IW(vo,vo(m)) â, the marginal sampling probability is Student 
â (s | m) ~  Sn(i/Q,minivo(m)In) â thus (s | m) is a random vector of 
exchangeable, uncorrelated, but not independent variables and the prior 
distribution in the marginal model on (m^s) is evidently the same as the 
marginal prior distribution on m in the unreduced model on (m, v, s). SupÂ­
pose now that v is the only parameter of interest, and that (m | i>) is a 
normal distribution: (m | t;) ~  N(mo 1riQ1v). Then the marginal sampling 
probability becomes (s | v) ~  Nn(moin,v[In + Uq 1in^n])j i*e*> an exchangeÂ­
able (equicorrelated) normal vector. Again, the prior distribution in the 
marginal model on (v,s) is the same as the marginal prior distribution on 
v in the unreduced model on (m, v, s). 
u
C. Joint M arginalization
There are many circumstances, in particular in sequential analysis, 
where marginal reductions are operated simultaneously on the sample space 
and on the parameter space. So, considering two sub-cr-fields B C A and 
T  C <S, we may define:
(1.4.6)
Â£bvt = (A x 5, B y  T, II^vt)

50
1. Bayesian Experiments
and, in the regular case, we have:
(1.4.7) 
II bvt = Pb Â® P t = Pt Â®P b -
In the marginal experiment SbvT, a statistic is either a sub-cr-field of T  or a 
T-measurable function defined on 5, a subparameter is either a sub-cr-field 
of B or a ^-measurable function defined on A. Note that any marginal exÂ­
periment has exactly the same mathematical structure as a general Bayesian 
experiment.
Exam ple. As an example of joint marginalization, suppose that in ExamÂ­
ple 1 of Section 1.3 the loss-function depends on a linear combination of 
the expectations only; i.e., consider the subparameter b = Y li 0iiai â a'a' 
It then seems reasonable to reduce the observation s to the same linear 
combination, viz., t = ^  ajS,' = a's; defining the sub-cr-field: B = a(b) 
and T  = cr(t), the marginal experiment Â£bvt is characterized by the joint 
probability:
N 2
/  a'mo\ /  a'Voa 
Xa'moJ \  
a'Vo or
a'Voa 
a'(Vo + V)a
In this reduction, the sampling probabilities are represented by a normal 
distribution with parameters:
E(i \b) = b
V(t | b) = a'Va
and the posterior distribution is also normal with parameters:
E(b 11) 
= 
(1 -  p)afm 0 + pi 
V(b\t) 
=
1 
1 
+
- l
where
P =
a'Voa 
afVa
a'Voa 
a'(V0 + V ) a â¢
In the next chapter, we shall see that this joint marginalization of the 
original experiment is indeed âânaturalâ in the following sense. If b = a'a is 
the only parameter of interest it will be shown, in a sense to be made precise, 
that the marginal experiment SbvT âdoes not lose relevant informationâ.

1.4. Reduction of Bayesian Experiments
51
1.4.3 
Conditional Experiment
Recall that conditional probabilities are defined a.s. only; and according 
to the convention introduced in Chapter 0, we omit the a.s. proviso in all 
equalities involving a conditional probability.
A. Conditioning on the Sample Space
The concept of conditional experiment may be motivated by the followÂ­
ing example.
Exam ple. Let us consider a regression model (y \ X, a) ~  Nn(Xb,vI) 
where y is an (n x l)-vector, X  is an (n x &)-matrix, b is an (k x 1)- 
vector of regression coefficient, v is a positive real number. This model may 
be embedded in a Bayesian experiment with observation s = (y,X) and 
parameter a = (6, 
c), where c are parameters characterizing the marginal
sampling process generating X. The nature of a regression model is to 
operate an analysis âconditionallyâ on the statistic X\ this is an analysis 
where the sampling process generating X  is not specified explicitly. Other 
examples will also be encountered in sequential analysis (Chap. 6.). 
â 
1.4.3 D efinition. The Bayesian experiment conditional on T, 
is deÂ­
fined as:
(1.4.8) 
Â£r  = (AxS,  AVS, nr )
where 11^ is a conditional probability of II given T. The conditional experÂ­
iment Sr  is said to be regular if there exists a regular version of 
such 
that there exist regular versions of ps and P AvT, in this case we then have:
(1.4.9) 
n r  = pr  Â® P AyT = P r  Â® p s 
â 
Equalities (1.4.9) are based on the following identities:
(1.4.10) n
[e x ( i n y ) ]  
= / n
t ( e  x  x)dP r
J y
= 
[  dPr /  ps (E)dPr
J y  
J x
= f dPr  f PAs/T(X)dpr  
Jy 
Je
E â¬ A, X  Â£ S, Y  e T.

52
1. Bayesian Experiments
Equalities (1.4.10) come from the fact that for any a E [A]+ and s E [S]+ , 
one has, using Proposition 0.3.14(ii), T(as) = T(s â¢ Sa) = T[a â¢ (A  V T)s].
Even though the last two equalities are true in general, the existence of 
a regular version of Ur  implies the existence of a regular version of pT and 
P r  and justifies the integrations in (1.4.10). A given version of IIr  gives a 
T-measurable family of probabilities on A  V S  to which one may associate a 
measurable family of regular experiments once the conditional probabilities 
P A w r  and ps admit a regular version. Note also that P A v T  is a sampling 
probability conditional on T.
B. Conditioning on the Param eter Space
The Bayesian treatment of exact restrictions on the parameter space 
leads naturally to the concept of a conditional experiment S B. Classically, 
an exact restriction is a surface in the parameter space. Imagine, for examÂ­
ple, that s is distributed as a bivariate normal: (s | a) ~  A2(m, V) where 
a = (m, V). Suppose now that m = (m i,m 2) has a continuous probability 
distribution and that one wishes to corporate the information âmi and m2 
have the same valueâ . It is well known, by the Borel-Kolmogorov paradox, 
â see Kolmogorov (1950) â that the distribution of mi conditionally on 
mi â m 2 = 0 or on m im j1 = 1 is not the same. This may be viewed as 
two different disintegrations of p or of II: the cr-field generated by mi â m2 
is clearly different from that generated by m im j1 and represents a differÂ­
ent structure of information, (i.e., a different set of âpossible messagesâ). 
A restriction on the parameter space should therefore be handled through 
a sub-<T-field of the parameter space rather than through a subset of the 
parameter space (see Mouchart and Orsi (1976, 1986), and Mouchart and 
Roche (1987)). This approach indeed leads to a coherent treatment of reÂ­
strictions defined by a surface of zero prior probability, and motivates the 
following definition:
1.4.4 D efinition. The Bayesian experiment conditional on B , is defined 
as:
(1.4.11) 
S B = (A x 5, A VS, HB)
where HB is a conditional probability of II given B. The conditional experÂ­
iment Â£ B is said to be regular if there exists a regular version of ILB such

1.4. Reduction of Bayesian Experiments
53
that there exists a regular version of p Bs/S\ in this case we have:
(1.4.12) 
ne =  HB Â® P A =  P B Â®fiBwS. 
m
As in (1.4.9), the above equalities are based on the following identities:
(1.4.13) 
n[(Â£TlF) x T] 
= 
j  Ub(E x T)d(iB
J F
= 
[  dfiB [  PA(T)d(xB
J f  
J e
= 
[  due [  nBwS(E)dPB
J f  
J t
E e A ,  F e B , T e S .
Again, (1.4.13) follows from the fact that, by Proposition 0.3.14(h), for any 
a e [A]+ and s e [<S]+, B(as) = B(a â¢ ,4s) = B[s â¢ (B V S)a]. The integrals 
in (1.4.13) are justified by the fact that the existence of a regular version 
of IL3  implies the existence of a regular version of pB and P B. Note also 
that 
has been assumed to be a transition of sampling probability. The 
next proposition makes the role of p& more precise for the construction of 
the conditional experiment Â£ B: it shows that Â£B is robust with respect to 
changes of ps into an equivalent p'B.
1.4.5 
P roposition. Let us consider Â£ and Â£', two Bayesian experiments 
on (A x S, AW S) with probabilities II and II' such that
(i) 
n -  n'
(ii) f P w .
ThenÂ£B = Â£,B. 
m
C. 
Com bined R eductions
Joint conditioning on the sample space and on the parameter space may 
also be defined as follows:
(1.4.14) 
S BvT = (A x S, A  V S, nBvr)
where, in the regular case, the conditional probability IlBvT may be decom- 
posed as:
(1.4.15) 
nBvT = /JBVT <g> PAwT = P BvT <g> /zBv5.

54
1. Bayesian Experiments
Subsequently, we shall also encounter various combinations of reduction by 
marginalization and reduction by conditioning. In general, for AA C A V S ,  
defined in the regular case, using:
(1.4.16) 
n ^ r  = &  Â® p ^ M = p y Â®  f i ^ T
D. Parameters and Observations in Conditional Experiments
The analysis of conditional experiments requires that some care be taken 
with regards to the concepts of a parameter and an observation. Let AA be 
any sub-cr-field of A  V*S. In SM , a statistic is basically a function defined on 
the available information, i.e., on the observation and on the conditioning 
variables. This leads to the following definitions:
1.4.6 Definition. A sub-cr-field Af is an SM -statistic if Af C AA V S and 
AA fl (AA V S) is called the trivial 
-statistic. 
â 
1.4.7 Definition. An SM -statistic, AT, which is also an ^-statistic, (i.e., 
Af C S) is called a uniform Â£M -statistic. 
m
1.4.8 Definition. An SM -statistic, Af, is a strong SM -statistic if AA C Af 
(and therefore AA C Af C AA V <S). 
â 
Note that a statistic Af C AA V S  generates a family of cr-fields on the 
sample space as follows. For any V G A/-, 
a Â£ A, define a section of V at 
a as follows: Va = {s E S | (a, s) E V}. Clearly, Va E S (see, e.g., Neveu 
(1974), Chap. III-2) and Afa = {Va | V E Af} is a sub-cr- field of S. In the 
uniform case, this family collapses into a unique cr-field T  C S.
Exam ple. Let us illustrate these concepts by the following example. Let 
(Si | a) ~  i.N(m, v) with a = (m, i>) and consider SM where AA â cr(m) E A  
is the cr-field generated by m ; let Af\ = cr(Xw(5i â m)2) anc^ consider 
also Af2 = 
~ m) 2 ,m) and N z = cr(Ei *>'. E i Â«<)â¢ 
= i . 2. 3)
are each Â£ M -statistics, but only A 3 is a uniform Â£M -statistic. In Â£M , (i.e., 
once m is known), Af\ and A/2 give the very same information: A/2 may

1.4. Reduction of Bayesian Experiments
55
be considered as a redefinition of Jf\ so as to incorporate M \ M2 is thus a 
strong SM -statistic. For each value of m, 
(or A/2) generates a family of 
different (7-fields on S  while A/3 generates only one such (7-field on S. 
â 
Similarly, in Â£M relevant functions of the parameters may depend on 
both A  and M . For example, the regression model y = Xf3 + e may be 
viewed as an experiment conditionalized on X  and the function X/3 may 
be considered as a parameter of interest; note also that in heteroscedastic 
models, the sampling variance of e may, for example, be a function of X/3 
or of X.
1.4.9 Definition. A cr-field C is an ZM -parameter iff C C A  V M  and 
(A V M )  n M  is called the trivial f-^-parameter. 
â 
1.4.10 Definition. An Â£^-param eter, Â£, which is also an ^-parameter,
(i.e., C C A) is called a uniform Â£M -parameter. 
u
1.4.11 Definition. An Â£M -parameter, Â£, is a strong Â£M -parameter 
iff
M  C C (and therefore M  C C C A V M ). 
â 
Remarks.
(i) Clearly a statistic (respectively, parameter) in Â£ is a uniform Â£M- statisÂ­
tic (respectively, Â£M -parameter) for any M .  Note also that if M  is trivial, 
â i.e., M  = X = {<Â£, A x S} â then Â£x = Â£, and any Â£J -statistic is both 
strong and uniform.
(ii) If M  C A  (respectively, A^f C Â«S), an f^-param eter (respectively, 
8 M -statistic) is necessarily uniform.
1.4.4 
Complementary Reductions
An important theme in the Bayesian literature is the analysis of the proÂ­
cess of âlearning by observingâ, viz., the transformation âprior-to-posteriorâ: 
\x ââº ps . The concept of complementary reductions corresponds to a deÂ­
composition of this learning process either on the sample space or on the 
parameter space.

56
1. Bayesian Experiments
A. Decom position on the Sample Space
Let T  be a sub-cr-field of S. The transformation of p into ps may be 
decomposed into two steps. The first step is to obtain pr  by revising p in 
the light of partial sample information T ; this is done in 
The second
step is to obtain ps by revising pr  in the light of the complete sample 
information; this is done in S'^s/S. This decomposition is illustrated in the 
following diagram:
Â£.AvT
P-----------------âº p 1 
 
âº p*
Â£avs
B. Decom position on the Parameter Space
Let B be a sub-<r-field of A. Let us decompose p into p& 0  pB and ps 
into Pq 0  pBs/S. In Â£gvs, Pb is revised by the result of a sampling process, 
characterized by P B, and in S B, p B is revised by the unreduced sampling 
process, characterized by PA . This may be illustrated as follows:

1.4. Reduction of Bayesian Experiments
57
These decompositions motivate the following definition:
1.4.12 Definition. For any sub-cr-field B C A  the reductions Â£bvs and 
Â£Avs are called complementary, as are the reductions Â£avT and Â£^ys fÂ°r 
any sub-cr-field T  C S. 
â 
Example. 
Consider the Bayesian experiment Â£ defined as follows: Let 
A = S = M 2 and A  = S = B 2 (the Borel cr-field of M2) along with II be 
defined through: (s \ a) ~  N 2(a,V\) and a ~  iV2(ao,Vo) where Vo and V\ 
are known positive definite symmetric 2 x 2 matrices. Consider B = ^(a'a) 
where a is a vector of given constants. Thus a pair of complementary 
reductions is Â£bvs and Â£awS- 
Â£bvs, the prior probability is given by 
a 1 a ~  N (a ,aoJ a'Voa) and the marginalized sampling probabilities are
(s | a'a) ~  N 2 [a0 + Vr0a(a'V r0a ) - 1a /(a -  a0), 
Vi + Vâ],
where Va = V (a | a'a) = Vo â Vo a(a' Vo a) ~1 a ' Vo. In this experiment, the 
prior probability is transformed into
(a'a | s) ~  N[a'a0 + a'V0 (V0 + 
-  a0), 
a '(^0_1 + V f1)-1^ .
In the complementary reduction, Â£%vSi the prior distribution (a \ a'a) ~ 
N2(aa , Va), where aa = E(a | a'a) = a0-f-Voa(a/VoQf)_1o /(aâao), is revised 
into
(a | a'a, s) ~  N 2 [aa + Va(Vi +  
-  aâ), Va -  Va (Vi + Va)~lVa].
Similarly, consider T  = cr(/?'s) where /? is a vector of given constants. In 
Â£avT the prior distribution a ~  iV2(ao, Vo) is revised into (a | /S's) ~  
N 2 (a/?, Vp), where
ap 
= 
E(a\(3's) = a0 + Vof3\/3'(V0 + V1)l3]-1f3 '( s - a 0)
Vp 
= 
F (a |/? ,S)^o-^o/?[/?'(Fo +  71)/?]-1/?,F0.
In the complementary reduction 
the prior distribution is the posterior 
distribution of Â£avT' (cl | /3fs) ~  AT2(aÂ£, V^), and is revised into
(a|/?'s,s) ~  (a | s) ~  A T jK V o-^V f^-^V o'^o +  Vf^J.CVo-' + V f'J " 1]. -

58
1. Bayesian Experiments
C. Complementary Reductions in Conditional Experim ents
The above definitions correspond to decompositions of an unreduced exÂ­
periment Â£a v s - Identical decompositions may be operated on marginal exÂ­
periments Â£avT j Â£bvs or Sbvt since they are all characterized by a unique 
probability on a product space. Some care must be taken when defining 
complementary reductions of a conditional experiment, since conditional 
experiments are characterized, in the regular case, by a probability transiÂ­
tion rather than a unique probability.
Let us consider arbitrary cr-fields B C A, T  C S  and M  C A  V S. 
From the discussion on parameters and statistics in conditional experiments 
(Section 1.4.3.D), one may identify the conditional experiments 
and
Â£(bvM)v(m v T)' 
Â°Iher words, in conditional experiments, the product 
structure B V T  is considered unaffected if the parameters involve observable 
variables included in the conditioning cr-field, or if the statistics involve 
parameters included in the conditioning cr-field.
The decomposition of 
on the sample space may be operated through 
a statistic of this conditional experiment, Af C M V T .  This decomposition 
is illustrated in the following diagram:
Similarly, the decomposition of 
on the parameter space may be operÂ­
ated through a parameter of the conditional experiment, C C B V AA. This

1.4. Reduction of Bayesian Experiments 
decomposition is illustrated in the following diagram:
59
As before, for any C C B V M ,  
Scvt an(^ 
are called compleÂ­
mentary reductions on the parameter space of 
, and for any 
M  C M  V T, 
Â£**â and S g fr *  are called complementary reductions on 
the sample space of E ^ ^ .
Let us note that when A4 is trivial, i.e., AA â X, the two decompoÂ­
sitions illustrated in the above diagrams coincide with the decompositions 
presented in Sections 1.4.4A and 1.4.4. B.
1.4.5 
Dominance in Reduced Experiments
If E is dominated in the sense of Definition 1.2.4, i.e., II is dominated by 
p, 0  P, we shall show that for every M  and M  sub-cr-fields of A  V S, 
II jtf 
is dominated in a particular sense by (/1 0  P)j$ .
A. N otation
We first recall some notation introduced in Chapter 0. As before, 
for any integrable random variable m defined on A x 5, we denote by M m  
the conditional expectation of m given M  with respect to II. Furthermore, 
we denote by Mm the conditional expectation of m given M  with respect to 
Thus, the symbol 
refers to conditional expectations with respect

60
1. Bayesian Experiments
to p 0  P. In particular the expectation of m with respect to p <g> P will be 
denoted by Xm.
Remember that all equalities for conditional expectations are almost 
sure equalities. More precisely, it may be noted that it is almost sure with 
respect to the trace of the probability on the largest cr-field involved in the 
equality. When the underlying probability is not ambiguous, we skip this 
âa.sp ro v iso , since conditional expectations are only defined almost surely. 
Such an omission should thus not endanger the correct understanding of the 
formulae and theorems. In this section, the proviso a.s.II will be omitted as 
before, whereas the proviso a.s.p 0  P will not be omitted.
B. Dominance in Marginal Experiments
B .l. Marginal Densities
It is readily seen that, for any sub-cr-field Af of A  VÂ«S, Il^f is dominated 
by (p 0  P)u^ If g denotes a version of dll/d(p 0  P) and g^f a version of 
dUjs/d(p 0  P)jsf, we have the following identity:
(1.4.17) 
g t f - N g  
a.s. /i 0 P.
This comes from the fact that, for any n Â£ [yV]+, by definition of gtf, 
Xn = X(n â¢ g^f) and, also, I n  = X(n â¢ g) = X(n â¢ Afg). These relationships 
lead to the identity (1.4.17).
In particular, since for any a Â£ [A]+ (respectively, s Â£ [Â«S]+), Xa â Xa 
(respectively, Xs â Zs), this implies gA â 1 a.s. pÂ® P (respectively, 
= 1 
a.s. p 0  P). From (1.4.17), we also obtain that for any B sub-cr-field of A, 
gg = 1 a.s. p 0  P, and for any T sub-cr-field of Â«S, g? â 1 a.s. p 0  P.
B.2. Dominance in Marginal Experiments
If E is dominated, and if B is a sub-cr-field of A  and T  a sub-cr-field of 5, 
the marginal experiment E&vt is dominated in the sense of Definition 1.2.4,
i.e., it is dominated by the product of its prior and predictive probabilities; 
indeed, one has clearly, (p 0  P) bvT = Pb 0  Pr and, therefore, gsvT is a 
version of dUg^T/ d(ps 0 /V ). Thus, the same argument as in Section 1.2.2 
leads to the following result:

1.4. Reduction of Bayesian Experiments
61
1.4.13 
Proposition. If Â£ is dominated (in the sense of Definition 1.2.4), 
then, for any sub-cr-fields B C A  and TC<S, the marginal experiment SbvT 
is regular, i.e., the conditional probabilities PÂ® and 
are regular and may 
be written as:
(1.4.18) 
P * ( X ) =  I  9bvT dP 
X & T
Jx
(1.4.19) 
t*J}(E)= f 9bvT dfi 
E e B  
â¢
J E
B.3. M arginal D ensities in th e R egular Case
Clearly with respect to p Â® P, A  and S are independent cr-fields. (The 
concept of independence among cr-fields is treated more systematically in 
Section 2.2). If B is a sub-<r-field of A  and T  a sub- cr-field of S y we can see 
that for any a E [A]* and s G [5]+
(1.4.20) 
BVT{as) = B a - T s  = B a T s
since II and p Â® P have the same marginals.
From this equation, we obtain that if p, (respectively, P) admits a 
regular conditional probability given B (respectively, T), one has:
(1.4.21) 
(txÂ®P)m T  = nB Â® P T .
From this, we obtain that gsvr may be computed as follows:
(1.4.22) 
9bvT = f  
g(a,s)fxB(da)PT(ds).
JaxS
In particular,
gsvs 
= 
/  g(a,s)fiB(da)
Ja
9AWT 
=  
J  9(ai s)PT (ds)
Exam ple. We consider a probit type model for which we analyze only a 
sample of size one: (s | a) ~  N(ay 1) with the prior probability a ~  N (0,1). 
Let A be the Lebesgue measure on IR. We first notice that 
dH 
/ 
x i 
r 
1 , 
.Â«},

62
1. Bayesian Experiments
thus, it may also be checked that:
dn
= 2* exp[-i(a2 - 2as + ^s2)]
d(/iÂ® P ) 
2 
2"
because s ~  A^(0,2). If t = l[oj0o[(5)> and T  = 
Ft giyes a probability 
equal to |  at ] â oo,0[ and at [0,oo[ and P r  is characterized by:
dPr
-ii-t
dX = [2f(s | 0, 2) l [0oo[(s)]<[2/(s  | 0 , 2 ) 1 ^
01(8 )] 
where /  is the density with respect to A of the JV(0,2) distribution. 
dUAvT 
_ 0/Tv/ â\i-t
d(fi Â® Pr)
and this density may be computed equivalently by the conditional expecÂ­
tation of dU/d(fi Â® P) given A M  T , or by integrating dH[/d(fi Â® P) with 
respect to P T 
u
C. 
Dominance in Conditional Experiments
C .l. Conditional Densities
We now consider M  and M ,  two sub-(7-fields of AM S. For any n E [A/"]+ 
and any m E [A4]+, the expectation of the product n â¢ m may be written as:
(1.4.23) 
2 (n â¢ m) = Z[n â¢ m â¢ flWvx] = 
â¢ >f(n â¢ ^
vx)]-
But it is also true that
(1.4.24) 
2(n â¢ m) = I[m â¢ A4n] = X[m â¢ A4n â¢
hence, for any m E [A4]+ ,
(1.4.25) 
Z[m â¢ Al(n â¢ g^fyM)] = 
â¢ A4n â¢ ^.m],
and this implies that, for any n E [A/]+ ,
(1.4.26) 
M {n â¢ gjsfvM) = M n  â¢ gM 
a.s.//Â® P.
Now 
= 0] = ZtfiWv.M l{0x=o}] = i[9M !{^=o}] = 0. So gM is 
positive a.s.II and {qm = 0} C 
â 0} a.s./i Â® P. Noticing that
<7.m Â£ [-M]+ > and ^  we define
(1.4.27) 
gj$ 
= 
if 
> o
9m
= 
1 
if gM =  0

1.4. Reduction of Bayesian Experiments
63
we obtain the following identity: for any n E [Af]+
(1.4.28) 
M n  = M[n â¢ gff] 
a.s.II.
This tells us that IT^ is in some sense dominated by (fi Â® P ) $  since a 
conditional expectation with respect to II may be replaced by a conditional 
expectation with respect to p 0  P, using the âdensityâ g $ .
It is important to note that gff G[J\f V M ]+ , and that we may rewrite
(1.4.27) as:
(1.4.29) 
9MvM = 9 m - 9 $  
a.s.A* Â® P-
C.2. Dominance in Conditional Experiments
If Â£ is dominated and M  is a sub-<r-field of A  VÂ«S, it is not clear whether 
the conditional experiment Â£M will be dominated in the sense of Definition 
1.2.4; indeed, if there exists a regular version of (/i 0  P)M , then 11^  will 
also admit a regular version, and thus also pM and PM ; but, in general, 
one would not have (/i 0  P)M = pM 0  PM .
In the special case where M  = Z?VT, with B C A  and T  C S  such that 
there exist regular versions of pB and P T, then (// 0  p )BvT and UBs/T will 
also admit regular versions and, as seen earlier, (// 0  P )Bvr = ^ B Â® p r . In 
this situation //evT (respectively, p 5 v r) is regular and dominated by pB 
(respectively, P r ); g ^ T (respectively, gjsVT) is a version of dpBvT jd p B 
(respectively, dPBs/T/d P r ). Indeed if E E A,
r i 4  8m ^ B v r ^ )  
= n B v r ( ^ x 5 )  
= 5 v T [ ff5 v T i B xS ]
{ 
} 
= SbÂ»8 9T TdÂ»BdPT 
= f E 9%vV
,
where the last equality comes from the fact that gBvT G [A V T]+ . SimiÂ­
larly, if X  G S,
(1.4.31) 
P BvT( X ) =  f gBs/TdPT .
Jx
Hence, pBwT 0  p BwT is regular and dominated by pB 0  P r , and 
g j^r  -^5 Vr is a version of d ( / v r 0 P Bvr) /c ! ( / 0  P r ). Now, if we define
8vT
(1.4.32) ^
9 A 
* $1 
^V T  * 0/3VS
on {gAvT > 0}fl{^ v 5 > 0} and 1 elsewhere, we obtain the following result.

64
1. Bayesian Experiments
1.4.14 
Theorem . If Â£ is dominated, then for any sub-cr-field B C A  and 
T  C S  such that there exist regular versions of pB and Pr , the conditional 
experiment Â£BvT is dominated and regular, i.e., the conditional probabilities 
P Awr and pBvS are regular and may be written as
(1.4.33) 
P AwT{X) 
= /  hBwTdPBwT
Jx
= 
[  9A v s d P T  
X Z S
J x
(1.4.34) 
fiBwS(E) = f  hB*Td ^ T
Je
= 
[  d v ! ^ B 
E e A
JE
Proof. If m Â£ [A V S ]+ ,
B V T [ m  â  hBwT â  gBwT â  fffvT] 
= 
B V T [ m  â¢ g ^ /s â¢ l {,Â«vr>0}n{, | v r>0}]
= 
B ~ T [ m . g By r ] = ( B V T ) ( m ) .
Since, as seen before, {g > 0} C {flUvr > 0} fl {<7bvs > 0} a.s.^z 0  P. 
â 
As a final remark, note that, in particular:
(1.4.35) 
5 â =  ,%
*, =  - 2 -
9 b v s
(1.4.36) 
hT = 
J  = -
9A v T

2
Admissible Reductions: 
Sufficiency and Ancillarity
2.1 Introduction
In this chapter we formalize the idea that a given reduction of a Bayesian 
experiment âdoes not lose useful informationâ. This concept leads to the 
definition of admissible reductions. Finally, we characterize the admissibility 
of reductions effectuated either by marginalization or by conditioning.
Given that the object of Bayesian inference is the transformation of 
âprior to posteriorâ probabilities, a Bayesian experiment is said to be âtoÂ­
tally non-informative if the transformation prior to posterior is trivial, i.e., 
if the posterior probabilities are almost all equal to the prior probability. It 
will therefore be said that a reduced experiment Â£i âloses no informationâ 
with respect to the unreduced experiment S if the complementary reduction 
Â£ 2 (in the sense of Section 1.4.4) is totally non-informative. Indeed in this 
case the revision of the prior probability is entirely determined in the reducÂ­
tion Si. Therefore, in such a situation, S\ is called an admissible reduction 
of S. As we shall see later on, S may be either an unreduced experiment 
Savs or an already reduced experiment, such as 
where B represents
the parameter of interest. In this chapter, we only consider reduction of an 
unreduced experiment Sa ws-
65

66
2. Admissible Reductions: Sufficiency and Ancillarity
A parameter (or a statistic) is said to be sufficient if the correspondÂ­
ing marginal reduction is admissible. A parameter (or a statistic) is said 
to be ancillary if the corresponding conditional reduction is admissible. In 
other words, sufficiency and ancillarity appear as concepts characterizing 
the admissibility of reductions by marginalization and by conditioning reÂ­
spectively.
In a Bayesian framework, those concepts are naturally rendered operÂ­
ational by using the tool of stochastic independence both in marginal and 
in conditional terms. For this reason, we first review in the next section 
the main properties of conditional independence; that section is based on 
Mouchart and Rolin (1984b) but Sections 2.2.5 and 2.2.6 are essentially 
new. The same material has also been partly presented in Van Putten and 
Van Schuppen (1985); for a table of correspondence of results, see Mouchart 
and Rolin (1985); on the same topic, see also Dawid (1979a, 1979b, 1980b) 
and Pitman and Speed (1973). Section 2.3 is based on Florens and Mouchart 
(1977, 1986a), but Sections 2.3.6 and 2.3.7 are essentially new.
2.2 
Conditional Independence
2.2.1 Notation
This section handles a topic in probability theory. For this reason, we 
recall the general notation introduced in Chapter 0, viz., (M ,A f,P) is an 
abstract probability space; thus M  need not be equal to A x S. Sub-cr- 
fields of Af will be denoted by Af,-; in particular Afo is always the trivial 
cr-field {<j),M}. Also, m Â£ [Af,-]+ indicates that m is a positive real-valued 
Af*-measurable function defined on M  and Af,-ra denotes the conditional 
expectation of m with respect to P, given Af*, i.e., Af*m = E(m | Af*).
The concepts and equalities introduced below are, in general, stated in 
terms of positive real-valued functions and are extended to integrable (realÂ­
valued) functions using standard arguments. Finally, let us also recall that 
equalities involving conditional expectations should be read as almost sure 
equalities.

2.2. 
Conditional Independence
67
2.2.2 Definition of Conditional Independence
The following theorem defines and completely characterizes the concept 
of conditional independence:
2.2.1 Theorem . The following conditions are equivalent:
(i) 
Af 3(m im 2) â Af3rai â¢ Af3ra2 
V m,- E [.M i]+, i = 1,2;
(ii) 
(Ad2 V Af3)mi = Af3rai 
V m iE [A fi]+ ;
(iii) Af 2(Af 3m i3) = Af 2m i3 
V m i3 E [Afi V Af3]*.
Proof. The equivalence of (i) and (ii) are standard (see, e.g., Dellacherie 
and Meyer (1975), Chap. 2). The equivalence of (i) and (iii) is less well 
known, and follows from the following argument
A fo (^ i^ 2^ 3) = Afo[m2 â¢ Af 3(m ira3)] 
V rat- G [AfÂ»]+ 
(i = 1,2,3)
if and only if
M o(mim2m3) = Af o[Af3mi â¢ Af 3m2 â¢ m3] 
V mt- G [Af*]+ 
(i = 1, 2,3)
since
Af0[m2 â¢ Ad3(m im 3)] = Afo[Af3m2 â¢ m im 3] = Afo[Af3mi â¢ Af3m2 â¢ m3\.
The proof is then completed by making use of the monotone class Theorem
0.2.21, to extend these equalities from the products mi â¢ m3 to all functions
mi3 E [Afi V Af3]. 
â 
2.2.2 D efinition. Under any one of the conditions of Theorem 2.2.1, we 
say that Afi and Af2 are independent conditionally on Af3 and we write 
Afi i  Af 2 | Af3. If we want to make explicit the role of the probability P 
in this concept, we write A f iX A f 2 |A f3;P . 
â 
R em arks.
1) 
Clearly when Af3 = Afo, Definition 2.2.2. corresponds to the usual 
(or âmarginalâ) independence of cr-fields, and we write Afi X Af2 rather

68
2. Admissible Reductions: Sufficiency and Ancillarity
than M \  I M
2 | -Mo- Note that conditions (i) and (ii) generalize, for 
the case of non-trivial M 3 , the familiar concept of stochastic independence 
(among cr-fields) whereas condition (iii) is genuinely a conditional concept 
in the sense that it boils down to condition (ii) for trivial M 3 .
2) It is clear from (i) that the concept of conditional independence is 
symmetric in M i  and M i- This allows us to add conditions (ii bis) and (iii 
bis) obtained by interchanging M i  and M i- In the sequel, we omit such 
obvious duplications.
3) Condition (iii) of Theorem 2.2.1 cannot be weakened into
A!2( ^ 3mi) = M itni 
V mi E [Mi]*
(as done in Martin, Petit and Littaye (1973)); this is shown by the folÂ­
lowing counterexample. Take three measurable sets Ai E M , each with 
positive probability, pairwise independent but not jointly independent, and 
define M i â cr(At). For any m E [Afi]+ , pairwise independence implies 
Af2(Af3m) â M 2m whereas, in general, Af3(77117722) = Afsmi â¢ M 31712 for 
mi E [Mi]*, and m2 E [Af2]+ is not implied by pairwise independence.
As A^3 C M 1 is equivalent to M 1 V Af3 = M 1, Theorem 2.2.1 trivially 
implies the following corollary.
2.2.3 Corollary. If Af3 C Afi, then the following properties are equivaÂ­
lent:
(i) 
M i  X  Af2 | Af3,
(ii) 
A fim 2 = Af3m2 
V m2 E [M 2]*,
(iii) Af2(Af3mi) = Af2mi 
V mi E [Afi]+. 
â 
The next corollary provides elementary properties of conditional indeÂ­
pendence. These properties follow in a straightforward way from elementary 
properties of conditional expectations.
2.2.4 Corollary.
(i) 
Afi C M 3 => M i  X M 2 I Af3 
V M 2,
(ii) 
Afi X  Af2 | Af3 
and 
Af5 C Afi => Af5 X Af2 | Af3-

2.2. 
Conditional Independence
69
2.2.3 Null Sets and Completion
In such topics as minimal sufficiency or asymptotic theory, null sets 
may play a crucial role, and the literature is plagued with errors due to 
the overlooking or misuse of null sets (see, e.g.,Basu (1955) and (1958), and 
Dawid (1979b)). In this section we introduce the measurable completion of 
(7-fields. This tool will usually allow a suitable treatment of null sets; more 
particularly, it provides a characterization of conditional independence in 
terms of a measurability property.
As in Section 0.3.2, the completed trivial a-field Mo is defined by:
(2.2.1) 
JTq = {A e M  : P(A ) 2 = P(A)}
and for M i, a sub-cr-field of M , we define the completed a-field M i as:
(2.2.2) 
M i  = M i V
Let us remark that we complete sub-cr-fields by measurable sets only 
and not by subsets of measurable null sets as is usually done (in Lebesgue 
completion). In this way, we avoid the danger of losing the separability of 
a cr-field.
Let us recall two basic properties of the conditional expectation:
(2.2.3) 
M im  
â 
M i m  
V m E [M]+
(2.2.4) 
m E [Mi]+ 
O  
m E [M]+ 
and 
m = M im .
The main properties used in operating with completion of cr-fields are 
summarized in the following lemma.
2.2.5 Lem m a.
(i) 
M i  V M 2 = M i  V M.2 = M i  V M 2 = M i  V M 2;
(ii) 
M i  fl M 2 c J 4 i n J ^  = M i n M 2 = M i O M ^ ;
(iii) 
M
i  C M 2 = >  M
i  fl M 2 = M
i  V (Mo H .M2). 
â 

70
2. Admissible Reductions: Sufficiency and Ancillarity
Properties (i) and (ii) are fairly standard. Property (iii) will be used 
repeatedly. More insight into this property may be obtained by first noticing 
that M 2 flM o represents the <7-field of the trivial sets of A42. Therefore, 
when M i  is included in 
the completion of M i  by the null sets oi M 2 
is equal to the sets of M 2 that are almost surely equal to sets of M i .
We are now in a position of characterizing conditional independence as 
a measurability property.
2.2.6 Theorem . The following properties are equivalent:
(i) 
M \  X  M 2 | M 3 .
(ii) 
m2 G [-M2]+ ^  ( X iV M 3)m2 â¬ [A i3 +.
Proof. Using the Condition (ii) in Theorem 2.2.1 and the basic property
(2.2.4), the proof follows from the following identity which is an application 
of Proposition 0.3.14(ii): V m2 E [
2]+,
( Mi  V A43)m2 = M 3 H M 1 V M 3 )1712] = A43m2. 
â 
Using Lemma 2.2.5, conditional independence may be extended to comÂ­
pleted (j-fields as follows.
2.2.7 Corollary. The following properties are equivalent:
(i) 
M i  X M 2 | M 3 ,
(ii) 
M i  X  M 2 | A43,
(iii) m ~ iÂ±
m ; \ m 3. 
m
Remember that for marginal independence the completed trivial cr-field 
is the largest cr-field that is, independent of itself. The next corollary extends 
this property to conditional independence.

2.2. 
Conditional Independence
71
2.2.8 Corollary. The following properties are equivalent:
(i) 
M i  C
(ii) 
M \  Â±  M i  | M 3 ,
(iii) mi E [Mi]+ => M i i M ^ m i )  = mi.
Proof. The equivalence of (i) and (ii) follows directly from Theorem 2.2.6. 
That (i) => (iii) is clear. That (iii) => (i) relies on the following identity for 
any mi E [Mi] oo *
M o{(m iâM a m i )2} = .M0{m i(m i-.M 3m i)} = M o{m i[m i-M i(M 3 m i)]} 
which is equal to zero under (iii). 
â 
Another facet of Corollary 2.2.8 is revealed by considering a conditional 
version of the well-known property that any event common to two indepenÂ­
dent cr-fields is trivial. More specifically, we have:
2.2.9 Corollary. If M i  JL M 2 | A^3 , then M i  (IM 2 C M 3 . 
â 
We shall see that this corollary, elementary from a probabilistic point 
of view, plays a crucial role in asymptotic theory.
2.2.4 
Basic Properties of Conditional Independence
The main tool supplied by conditional independence is indicated in the 
next theorem:
2.2.10 Theorem . The following properties are equivalent.
(i) 
M i  Â± M 2 \ M 3 and M i  Â± M
4 \ M 2 V M 3,
(ii) 
M i  Â± { M 2 V M 4 ) \ M 3 ,
(iii) M i  X M 4 I M 3 and M i  Â±  M 2 | M 4 V M 3 .
Proof. By the symmetry of .M2 and M 4) it suffices to prove the equivaÂ­
lence of (i) and (ii).

72
2. Admissible Reductions: Sufficiency and Ancillarity
(ii) => (i): the proof follows from a simple property of conditional expectaÂ­
tions: if m E [Ad]+ is such that M i m  = M ^ m  and if M i  C M 2 , then for 
any M 3 such that M i  C M 3 C M 2 , we have A ^m  = Adim. Hence, if (ii) 
is true, then V mi E [A4i]+ , (A ^ V A ^V A ^m i = Afsmi = (A ^ V A ^ m i. 
The last equality is due to the fact that M 3 C M 2 V M 3 C M 2 VAd4 V M 3 ; 
these two equalities are clearly equivalent to (i).
(i) => (ii): V mi G [A4i]+ : ( M 2 VA44 VM s)m i = (Ad2VAd3)mi = Adsmi 
is implied by (i). Therefore { ( M 2 VA44)
= M 3mi which is (ii). â 
Next corollary shows that the property M i  1. M 2 \ M 3 remains true 
if Adi (or Ad2) is enlarged in the direction of Ad3, or if Ad3 is enlarged in 
the direction of Adi and/or of Ad2.
2.2.11 Corollary. If Adi X Ad2 | M 3, Ads C M 1 V M 3 , Ad4 C Ad2 V Ad3, 
then
(i) 
(Adi V Ads) X (Ad2 V Ad4) | Ad3,
(Ii) 
Adi X Ad2 | Ada V Ad4 V Ads.
Proof. Indeed, Ad4 C Ad2 V Ad3 implies, by Corollary 2.2.4(i), that 
Adi X Ad4 | Ad2VAd3; along with Adi X Ad2 | Ad3 this gives, by Theorem 
2.2.10, the result for Ads = Ado- The proof is completed by repeating the 
same argument for an arbitrary Ads C Adi V Ad3. 
â 
Theorem 2.2.10 shows which pairs of conditions of conditional indepenÂ­
dence (viz., (i) or (iii)) do produce (and are actually equivalent to) the 
âlargerâ condition (ii). This is indeed a crucial property, and is used reÂ­
peatedly subsequently. In this chapter, as well as in Chapters 3 and 4, this 
theorem will have a central role in the development of the theory of admissiÂ­
ble reductions. When studying optimal reductions, we shall show that pairs 
of conditions other than (i) or (iii) may also imply, but not be equivalent 
to, the âlargerâ condition (ii), provided some supplementary conditions are 
granted: these new conditions are presented in Chapter 5.
If we wish to decrease the a-fields in Adi X A^2 I A43, the elementary 
property (ii) of Corollary 2.2.4 allows us to replace Adi or At 2 by any of 
their respective sub-a-fields; the next theorem characterizes a method for 
decreasing the conditioning cr-field.

2.2. 
Conditional Independence
73
2.2.12 Theorem .
If 
M 3 v  M 4 c  M u
M i  X Af2 | M s  
and 
M i  X Af2 | Af4,
then
(i) 
Afi X Af2 | M 3 fl Af4,
(ii) 
Afi X Af2 | Af3 fl Af4-
Proof, (i) is an immediate consequence of Theorem 2.2.6. (ii) then follows 
from Lemma 2.2.5 (ii) and Corollary 2.2.7. 
â 
Note that (i) and (ii) are clearly equivalent. Also, if Af3 V Af4 C Afi,
then any one of (i) or (ii) implies the other two conditions of this theorem.
Furthermore, taking Afi = Af3 V Af4 leads to the following corollary:
2.2.13 Corollary. For any sub-cr-fields Af ,â¢ (i = 2,3,4), the following propÂ­
erties are equivalent:
(i) 
M 2 X M s  | Af4 and Af2 X Af4 | Af3;
(ii) 
Af 2 X (Af3 V Af4) | Af3 H Af 4. 
â 
2.2.5 
Conditional Independence and Densities
We first characterize conditional independence in terms of densities. 
The basic idea is to start with a probability for which conditional indepenÂ­
dence is easily verified and to then characterize, in terms of the correspondÂ­
ing densities, conditional independence with respect to another probability 
dominated by the first one.
2.2.14 Theorem . Let P* be a probability on (M,A/f) such that P* <C P, 
and let g = dP'/dP. If Afi X  Af2 | M 3 ;P , then the following properties 
are equivalent:
(i) 
Afi X Af2 | Af3;P ',

74
2. Admissible Reductions: Sufficiency and Ancillarity
(**) 
^ i V X 2 â 
' 9
on 
iffMs > 0},
(iii) 
on 
{9M 3v m 3 > 0},
(iv) gMiVMiVMa =  SMiVMa ' 9M3VM3/9M3 
on 
{?A<3 > 0}.
Proof. As indicated in Section 1.4.5.C., one has, in general,
n____________ n 
nM 3 
M2V M$
9M. 1 vA42vM.3 
9m $ * 9m 2 * 9m. 1
It is then clear that (ii), 
(iii) and (iv) are equivalent. Now, by Formula
(1.4.28), if Af'm denotes a conditional expectation of m, given jV, with
respect to P ', one then has, in general, for any rat- E [A4,-]+ (i = 1,2):
= M 3(mi -m 2 
aâs^ '.
.M3(mi) 
= M z(m i â  g ^ \ )  
a.s.P',
=  M 3(m2 SfÂ£!*) 
a.s.P'.
By conditional independence with respect to P, and using Corollary 2.2.11, 
we have, a.s.P:
A43(mi â¢ 9m 0  â - ^ 3(012 â¢ <$*) =  Maim! â¢ m2 â¢ 
â¢ g%*).
Therefore,
M f3(mi â¢ m2) = 
â¢ M ,3rri2 
a.s.P'
is equivalent to:
Mz{mi â¢ m 2 â¢ 9m i VM2) = ^ ( m i  â¢ â¢2 â¢ 
â¢ gÂ£{Â®) 
a.s.P'.
This is equivalent to:
â 
_A^3 
_ 
3 _A^3 
_ _ r>
9
3 * 
va^2 â ^^3 â 
* 9m 2 
a.s.P,
and by definition of conditional densities, this is clearly equivalent to
9miVM2 ~  9mi ' 9mI 
a.s.P'. 
â 
This theorem shows that conditional independence may be characterized 
in terms of densities as well as in terms of conditional expectations, as we

2.2. 
Conditional Independence
75
have done until now. This is indeed in the spirit of Subsection 1.4.5., where 
we write marginal densities as conditional expectations, and conditional 
densities as ratios of marginal densities.
We prefer, however, to continue in terms of conditional expectations 
because, firstly, a dominated measure for which conditional independence 
is easily verified and densities easily computed is not always available and, 
secondly, because we feel that null sets are more easily taken into account 
in terms of conditional expectations. Indeed, in the case of densities, one 
must first choose a version of g on M \  V M 2 VM 3 for this density is defined 
a.s.P only; then the marginal densities, gMn are specified a.s.Pmh and the 
conditional densities, f f f y â are defined 
For this reason, both
the statement and the proof of this seemingly simple theorem are rather 
involved in terms of null sets, sometimes with respect to P, and sometimes 
with respect to P '. For example, Property (iii) in the theorem has to be 
verified a.s.P on the set {gM2vM 3 > 0} but is typically false on the set 
{ffM2V M 3 = 0} n { g M 3 > 0}.
2.2.15 Corollary. Let g be defined as in Theorem 2.2.14. If
(i) 
M i Â± M 2 |M 3;P ,
(ii) 
g e [M 2 v M 3]+ ,
then
(iii) M i X M 2 | M 3;P'.
Proof. By the definition of the marginal density (1.4.17) and by the meaÂ­
surability of <7, we easily obtain:
9 m 1s/m2v m 3 = 9 m 2v m 3 â 9  
a.s.P.
On the other hand by conditional independence with respect to P,
-  (M i V M 3)g = M 3g = gM3 
a-s.P.
The first relation implies gffivM = 9m 3 a-s-JF/ an(i 
second implies 
9m \ = 1 a.s.P'. This clearly implies gffivM, =  9m \ ' 9m \ &a -P'- 
â 
This corollary shows that conditional independence may be checked 
by a measurability of the density g. Generally, that is easily verified. The

76
2. Admissible Reductions: Sufficiency and Ancillarity
condition g E [M 2 V M 3J+ is equivalent to gJ^
/Mz = 1, which means that 
P and Pf do not differ on their conditional probabilities given M 2 V M 3 . In 
other words conditional independence with respect to P is not altered if P 
is modified in such a way that only its restriction on M 2 V M 3 is affected, 
and if the modified restriction Pm 2v m 3 ls dominated by Pm 2v m 3-
2.2.6 
Conditional Independence as Point Properties
Theorem 2.2.6 characterizes conditional independence as a measurabilÂ­
ity property. We now consider the problem of verifying such a measurability 
condition. The easiest case arises when the conditioning cr-field M 3 is genÂ­
erated by a measurable function /  : (M, M )  ââº (N,J\f), where 
is
an arbitrary measurable space, i.e., M 3 â / â 1(Ar). In such a situation, a 
simple corollary of Theorem 0.2.11 gives the following result.
2.2.16 Proposition. If M 3 = / _ 1(A0, then the following two properties 
are equivalent:
(i) 
A ll X A l2 | A l3;
(ii) 
V m2 E [Al2]+ , there exists n E [A/*]+ such that n o f  is a version of 
(All V M 3)^ 12- 
â 
Without this assumption on AI3, we have to introduce more technical 
conditions so as to be able to use Blackwell Theorem (see Theorem 0.2.16). 
Let us recall that the atoms of a cr-field AI3 â see (0.2.2) â are the equivÂ­
alence classes of the equivalence relation between points of M  defined as
x ~ x' ^  l x (x) = lx(x') 
V X E M 3,
.M3
i.e., two points of M  are equivalent for AI3 if they are not separated by 
an A l3-measurable set. Blackwell Theorem provides a characterization of 
conditional independence through the constancy, on the atoms of the conÂ­
ditioning cr-field, of the conditional expectations.
2.2.17 Proposition. If M \  is a Blackwell cr-field and if A^3 C M \  is a 
separable cr-field, then the two following properties are equivalent:
(i) 
M i  Â±  M 2 \ M s,

2.3. Admissible Reductions of an Unreduced Experiment 
77
(ii) 
V m G [A^2]+ there exists a version of M \ m  that is, constant on the 
atoms of M 3 . 
â 
Note that the condition of the separability of the conditioning cr-field is 
crucial for the validity of Proposition 2.2.17. Indeed, as a counterexample, 
let M  be the real line, M \  its Borel cr-field, M 2 any cr-field on (1R), and 
let AA3 be the (non separable) tr-field generated by the singletons. In this 
case, Afi is a Blackwell cr-field, M 3 a (strict) sub-cr-field of M i ,  and any 
function on M  is constant on the atoms of Af3, even when the property 
M i  X M 2 | M 3 is not verified.
2.3 
Admissible Reductions of an Unreduced Experiment
2.3.1 
Introduction
We are now in a position to render operational, in terms of stochasÂ­
tic independence, the concepts of ancillarity, and of sufficiency, i.e., the 
property of a cr-field that makes admissible the corresponding reduction by 
conditioning or by marginalization. In this section, we consider admissible 
reductions of an unreduced experiment Â£ = Sa v s - In the next chapter we 
shall consider admissible reductions of already reduced experiments either 
by marginalization (such as SawT or Â£&vs) in Section 3.2. or by condiÂ­
tioning (such as Â£a I s ) in Section 3.3. It should be noted that Sections 2.3 
and 3.2 are introduced for expository purposes only: Section 3.3. presents 
a general theory which encompasses both Sections 2.3 and 3.2.
The first step consists of rendering precise the concept of a totally non- 
informative experiment and of an admissible reduction. It should now be 
clear that the property âthe posterior probability is almost surely equal to 
the prior probabilityâ is a concept of stochastic independence between obÂ­
servation and parameter. This remark motivates the following definition for 
the case of a general Bayesian experiment Â£ ^ x  which includes, as particular 
cases, M  = 1 = {<j>, A x 5} 
or 
B â A  
or 
T  â S.

78
2. Admissible Reductions: Sufficiency and Ancillarity
2.3.1 D efinition. The Bayesian experiment Â£q^x 1S totally non-informative 
if and only if any one of the following equivalent conditions holds:
(i) 
B Â± T \ M )
(ii) 
^ ( E )  = ^
r (E) V E e B;
(iii) P P ( Y )  = P$yM (Y) 
V Y e T .
A reduction of a given Bayesian experiment is an admissible reduction if 
the complementary reduction, in the sense of Section 1.4.4, is totally non- 
informative. 
â 
Similarly to Definition 2.3.1 we define the concepts of ancillarity and sufÂ­
ficiency by means of three equivalent conditions. The first one, expressed in 
terms of stochastic independence, leads to the explicit identification of those 
reductions which are totally non-informative and those which are admissiÂ­
ble. The second equivalent condition is expressed in terms of the property of 
distributions on the parameter space; this provides a characteristic property 
of the âprior-to-posteriorâ transformation. The third equivalent condition 
is expressed in terms of a characterization of the sampling process, and 
is useful for comparisons with the corresponding sampling theory concepts 
and for rendering the role of the prior probability more precise. The fact 
that these three conditions are equivalent is, using Monotone Class TheÂ­
orems (see Section 0.2.4), a trivial consequence of the equivalent forms of 
conditional independence as in Theorem 2.2.1.
2.3.2 
Admissible Reductions on the Sample Space
Let us consider a statistic T, i.e., a sub-cr-field of S.
2.3.2 D efinition. T  is Â£ -sufficient if and only if Â£r  is totally non-informative,
i.e., if and only if one of the following equivalent conditions holds
(i) 
A X S  | T,
(ii) 
Â»S (E) = Â»T (E), 
v e  e a,
(iii) pA vT ^x) -  p t {x), 
v x e s .

2.3. Admissible Reductions of an Unreduced Experiment
79
2.3.3 D efinition. T  is Â£-ancillary if and only if Â£UvT is totally non- 
informative, i.e., if and only if one of the following equivalent conditions 
holds
(i) 
A !  T,
(ii) 
Â»t (E) = Â»(E), 
V E â¬ A,
(iii) P ?(X ) = PT (X), 
V X g T .  
-
2.3.3 
Admissible Reductions on the Parameter Space
Consider a parameter B, i.e., a sub-cr-field of A.
2.3.4 D efinition. B is S-sufficient if and only if S B is totally non-informative,
i.e., if and only if one of the following equivalent conditions holds:
(i) 
A JL 5 | B,
(ii) 
fiBvS(E) = nB(E), V E e A ,
(iii) P A(X) = P B(X), 
' i X e S .  
â¢
2.3.5 D efinition. B is S-ancillary if and only if Sbvs is totally non-informative,
i.e., if and only if one of the following equivalent conditions holds
(i) 
5 1 5 ,
(ii) 
p%(E) = pB(E), 
v ^ g s ,
(iii) P B(X) = P ( X ), 
V I G 5 .  
â 
2.3.4 
Some Comments on the Definitions
(i) 
The Bayesian framework allows us to provide a set of homogeneous 
definitions that reveal, in particular, the completely parallel treatment of 
sufficiency in the sample space and sufficiency in the parameter space. 
Barankin (1960), along with Picci (1977), take advantage of this formal

80
2. Admissible Reductions: Sufficiency and Ancillarity
analogy between the two concepts of sufficiency. Similarly, we have introÂ­
duced a definition of ancillarity on the parameter space that is completely 
parallel to the definition of ancillarity on the sample space.
(ii) All of the Conditions (ii) and (iii) of the definitions above are equivaÂ­
lent to measurability conditions (Theorem 2.2.6), and may be checked using 
Propositions 2.2.16 and 2.2.17. In particular, the sufficiency of T  C S  may 
be characterized as follows: in the first case, if T is generated by a statistic 
/  defined on S with values in some measurable space (N,Af), T  is sufficient 
if and only if V a E [Â«4]+ , Sa depends only on /; in the second case, if 
S  is a Blackwell cr-field and if T  is separable, T  is sufficient if and only if 
V a E [A]+ , Sa is constant on the atoms of T.
(iii) With respect to the concept of sufficiency on the sample space (reÂ­
spectively, on the parameter space), Condition (ii) says that the conditional 
reduction Â£ j vs (respectively, Â£Â®vS) is totally non-informative (i.e., âprior 
= posteriorâ a.s.) and, accordingly, that the marginal reduction Â£avT (reÂ­
spectively, Sjsvs) is an admissible reduction of Â£Uvs- On the sample space, 
this also means that in the process of âlearning by observingâ (i.e., in the 
transformation fi ââº 
it is âsufficientâ to observe T  instead of Â«S, when T  
is sufficient, in the sense that the inference will be the same (i.e., same prior 
and same posterior) in the reduced experiment Â£a vT as in the unreduced 
experiment Â£a v s - Note that Condition (iii) also says that the reason for 
which Â£%wS is totally non-informative is precisely that its sampling transiÂ­
tion âdoes not depend on A â: this reasoning will be deepened in Section
2.3.7 when comparing this Bayesian concept and the corresponding one in 
sampling theory. On the parameter space the sufficiency of B means that B 
is âsufficientâ to describe the sampling process (Condition (iii)) and, conseÂ­
quently, that the observation S  brings information on B only in the sense 
that, conditionally on 5, observation brings no information (Condition (ii)).
(iv) For the concept of ancillarity on the sample space (respectively, 
on the parameter space), Conditions (ii) says that the marginal reduction 
Â£a v t (respectively, Â£b vs) is totally non-informative, and accordingly, the 
conditional reduction Â£ j vs (respectively, Â£%vS) is an admissible reduction 
of Â£avs â¢ On the sample space, this means that in the process of âlearning- 
by-observingâ , the random character of T  is âancillaryâ , i.e., of no actual 
relevance, and may in fact be neglected in the sense of specifying the samÂ­

2.3. Admissible Reductions of an Unreduced Experiment
81
pling process âas ifâ T  were not random; this is indeed characteristic of 
Condition (iii) also shows that SavT is totally non-informative beÂ­
cause its sampling transition âdoes not depend on Aâ . On the parameter 
space the ancillarity of B means that the sampling process brings no inÂ­
formation on B (Condition (ii)). Note that the Condition (iii) is genuinely 
Bayesian, as it depends crucially on the conditional prior distribution pB, 
which is the only part of p to be revised by the observation S.
Let us now illustrate these concepts by some examples. We first note 
that the Bayesian concepts of sufficiency and ancillarity on the sample space 
are, up to //-null sets, the same as the corresponding concepts in sampling 
theory. In Section 2.3.7, we shall make this assertion more precise; neverÂ­
theless, in broad terms it can be said that any example of sufficient or of 
ancillary statistics, in a sampling theory framework, may also be used as 
an example in a Bayesian framework. Here we shall discuss some examples 
concerning the role of the //-null sets. (Discrete examples are given in Rolin 
(1985)).
Exam ple 1 . Consider a univariate normal experiment: s = (Â«i, â¢â¢â¢,$Â«) 
with (Si | a) ~  i.N(m, i>), where a = (m,v). In a sampling theory frameÂ­
work, t = (<i,t2) with ti = 
s* and f2 = Y lish  1S known to be sufÂ­
ficient whereas t\ is sufficient when the variance v is known, i.e., when 
(si | a) ~  
Vo), where vo is a fixed number. In a Bayesian framework,
the case of known variance is equivalently treated either by considering 
the sampling process (s* | a) ~  i.iV(m, vq) with some prior distribution on 
A = JR, or by considering the sampling process (st* | a) ~  i.N(m, i>) with a 
prior distribution on A = 1R x 1R+ such that p({v = vo}) = 1; in both cases 
t\ may be shown to be sufficient. 
â 
Exam ple 2. The role of the prior information may also be illustrated 
within the Example 1 of Section 1.3, viz., a p-dimensional normal experiÂ­
ment with known variance: (s | a) ~  Np(a,V) and a ~  Np(mo,Vo) where 
V and Vo are p x p positive semidefinite symmetric matrices of known elÂ­
ements. Let T  = (r(Ts) where T  is an (r x p)-matrix of known elements. 
Here t = Ts is a sufficient statistic if and only if cov(a, s \ i) = 0, i.e., if and 
only if l/0 -  V0T[T(Vq + Vr)T,]â 1T(Vro + V) = 0. Such a condition clearly 
links the prior information (Vo) and the sampling process (F). Let us conÂ­

82
2. Admissible Reductions: Sufficiency and Ancillarity
sider, in particular, the i.i.d. case; now the prior information is such that 
all components of a are almost surely equal, i.e., mo = fhip and Vo = voipip 
(thus Vo is of rank 1 and p is here the sample size). Furthermore, the samÂ­
pling process is such that V = vlp (where v is a known positive number). 
It is now easy to check that the sum of the observations t = ips is sufficient, 
indeed:
cov(a, s | i'ps) 
=  
voipip -  v0ipi'piP[ip(voiPip + vIp)ip]~1ip(voipi,p + vlp) 
=  
0
Note that the sufficiency of i'ps does not require that mo = fhip.
Let us now turn to the conditions for the ancillarity of T  = cr(Ts). Here 
t â Ts is ancillary if and only if cov(a,tf) = 0, i.e., if and only if VoT' = 0. In 
this (very) particular case, such a condition depends only on the structure 
of the prior information. For instance, in the i.i.d. case (i.e., Vo = voipip), t 
is ancillary when T  = /p â p*p*p (t â Ts is now the vector of the deviations 
of the observations from the empirical average). Note that any function of 
Ts would also be ancillary.
Let us now consider B = c(Ba), where B is an r x p-matrix of known 
elements. In this situation b = Ba is a sufficient parameter if and only 
if cov(a,s | b) = 0 which is equivalent to V(a | 6) = 0 because, in such 
a situation, E(s | a) = a. In other words, b is sufficient if a is a.s. a 
function of b. This condition is trivially satisfied for any regular matrix B , 
but otherwise depends on the structure of the prior information (viz., Vo). 
For any B , one has V(a\b) =  Vo â VoB '( B V qB ') + B V q (where A+ denotes 
the Moore-Penrose inverse of A) (see Marsaglia (1964)). In particular, if Vo 
has the form Vo = voipip this condition is satisfied for any 1 x p matrix B 
such that Bip ^  0. On the other hand, b = Ba is an ancillary parameter 
if and only if cov(6,s) = 0, which is equivalent to cov(a,6) = 0, (because, 
again, E(s | a) = a), i.e., that b is a.s. constant or BVo = 0. In particular, 
if Vo has the form Vo = voipifp, this condition is satisfied for any matrix B 
such that Bip = 0. 
â 

2.3. Admissible Reductions of an Unreduced Experiment
83
2.3.5 
Elementary Properties of Sufficiency and Ancillarity
Using Corollaries 2.2.1 l(ii) and 2.2.4(ii) we obtain the following monoÂ­
tonicity properties when considering two statistics T  and Z/, or two paramÂ­
eters B and C.
2.3.6 Proposition.
(i) 
If T  is ^-sufficient and T  C U then U is ^-sufficient,
(ii) 
If T  is ^-ancillary and U C T  then U is ^-ancillary,
(iii) If B is f-sufficient and B C C then C is â¬ -sufficient,
(iv) If B is ^-ancillary and C C B then C is ^-ancillary. 
â 
These properties imply that one will typically look for minimal sufficient 
statistics or parameters, and maximal ancillary statistics or parameters. 
This problem is treated in Chapter 4.
The role of the prior probability may be understood by asking which 
modifications of the prior probability guard intact a given property of suffiÂ­
ciency or of ancillarity. This aspect of robustness with respect to the prior 
specification is the object of the next proposition, the proof of which is a 
simple application of Corollary 2.2.15.
2.3.7 P roposition. Let us consider Â£ and Â£', two Bayesian experiments 
on (A x 5, A  V S) defined by their probabilities II and II' . If
(i) 
n' <  n,
(ii) 
dE'/dlL e [A]+.
Then
(iii) If T  C S  is an ^-sufficient statistic, it is also Â£'-sufficient;
(iv) If T  C S  is an ^-ancillary statistic, it is also ^'-ancillary;
(v) 
If B C A  is an ^-sufficient parameter, it is also ^'-sufficient;

84
2. Admissible Reductions: Sufficiency and Ancillarity
(vi) If B C A  is an ^-ancillary parameter, and if dU'/dU Â£ [#]+ ,
then it is also ^'-ancillary. 
â 
In other words, for a given sampling process, equivalent prior probabilÂ­
ities give the same sufficient cr-fields both on the sample space and on the 
parameter space and the same ancillary cr-fields on the sample space. The 
situation is quite different for ancillarity on the parameter space since this 
last property is retained when the prior probability is modified only on its 
restriction to the ancillary cr-field.
2.3.6 
Sufficiency and Ancillarity in a Dominated Experiment
We now express ancillarity and sufficiency, for the case of a dominated 
Bayesian experiment, as properties of the densities.
2.3.8 Proposition. Let Â£ = (A x S ,A  V S, II) be a dominated Bayesian 
experiment and g = dll/d(fi 0  P). Then
(i) 
T  is an ^-sufficient statistic if and only if <7^ 
â 1;
(ii) 
T  is an ^-ancillary statistic if and only if gAvT â 1;
(iii) B is an ^-sufficient parameter if and only if g ^ s = 1;
(iv) B is an ^-ancillary parameter if and only if g&vs = 1- 
â 
The proof of this proposition follows in a straightforward way from 
Theorem 2.2.14 after noting that A  and S  are evidently independent for 
the probability / i 0 P. Thus any condition of the type A  JL S | T; /i <g> P 
or B X S\ fi 0  P is trivially true . Let us note that these characterizations 
in terms of densities may take various forms. For example, T  will be an 
^-sufficient statistic if and only if g^ = g^ or, equivalently, g^vs ~  9 A 01 
equivalently gAvs = 9 AvT- However, the simplest property which must be 
verified is a measurability property of the density, such as the one given in 
Corollary 2.2.15. This leads to the following proposition.
2.3.9 Proposition. Let Â£ = (A x 5, A  V <S, n) be a dominated Bayesian 
experiment, and let g = dli/d^fi 0  P). Then

2.3. Admissible Reductions of an Unreduced Experiment
85
(i) 
T  is an ^-sufficient statistic if and only if g E [A V
(ii) B is an ^-sufficient parameter if and only if g Â£ [B V 5^^]. 
â 
Here A  V ' J ^ P denotes A V  T  completed by the p 0  P-null sets.
Proposition 2.3.8 may be viewed as a Bayesian analogue of the Neymanâs 
factorization theorem in sampling theory â see, e.g., Barra (1971), Chap. 2, 
Theorem 1. Consider indeed, as was done in the remark at the end of 
Section 1. 2. 2, the Radon-Nikodym derivative /*(a,s) of n  with respect 
to the product of two reference measures po on (A, A) and A on (5 ,S). 
Furthermore, when T is generated by a function t : (S,S) ââº {U^U) â i.e., 
T  = 
â then there exists, by Theorem 0.2.11, g : A x  U -+ 
such
that <?(a,s) = g(a,t(s)) a.s.(/i0 P), in which case (1.2.12) takes the familiar 
form of Neymanâs factorization:
(2.3.1) 
/*(a, s) = h(a) k(s) g(a, t(s)) 
a.s.(/i0 0  A)
This is illustrated in the following example.
Example. Consider a simple random sample of size n in the canonical 
Bayesian exponential experiment (as in Example 2 of Section 1.3). Thus 
Â« = ($ !,..., sn) and (s,* | a) are i.i.d. with density
dP?
= exp[a's,- -  L(a) + </(s,)]>
where <r0 is the reference measure on the sample space of one observation.
Let A be the reference measure on the sample space for n observations (i.e.,
A = crj), the data density of s = ($ i,â¢â¢ â¢, sn) is
d p a 
 ^ 
^
â
 = exp [o' ^ 2  S i -  nL(a) + ^  g(si)],
i 
i
and the predictive density of s is written as: 
dP 
_  
d f P an(da)
dX 
dX
= 
e x p { ^  #(si)} /  exp[a' 
Si â nL(a) + L(a)\ a(da)
i 
i
= 
ex p { ^ tf(si) +  M â (^ S i)} .

86
2. Admissible Reductions: Sufficiency and Ancillarity
We recall that, as defined in Section 1.3 (Example 2), L(a) = In dfi/da and 
M n is defined as:
(and, therefore M\(s) = M(s) = In f  exp(a's)da as defined in Section 1.3).
Let n n be the probability measure on the cr-field generated by (a, s) = 
(a, $ i,..., sn). 
The sufficiency of t = Â£ #-s* may be expressed using a 
Bayesian version of Neymanâs factorization theorem on the density of IIn 
with respect to the reference measure ao 0  A:
d(ao " 
= /*(Â«> s) = exp{a'< -  (n -  1 )L(a) + /(a) +  ^
 g(s{)}
(thus, in terms of the notation in (2.3.1), ln/i(a) = /(a) â (n â l)L(a), 
In k(s) = Â£,â¢ g(s{) and Ing(a,t) = a't). Alternatively, it may be interpreted 
using the property (i) of Proposition (2.3.8):
Let us note that the marginal experiment Sa vT > characterized by the image 
probability denoted by n a *, is also a canonical Bayesian exponential experÂ­
iment provided the measure on (A , .4) is allowed to depend on the sample 
size. More specifically, recall from Example 2, Section 1.3, that
Mn(s) = In / expfa's â (n â l)L(a)] a(da)
dUn/d(a 0 0  A)
d(fi 0  P)/d(a 0 0  A)
exp {a't -  (n -  1 )L{a) + /(a) + Â£ .  g(sj)} 
exp {/(a) + L(a) + 
gfa) + M n(t)}
exp {a't â Mn(t) â nL(a)}.
â ^  = exp[a'st- â L(a)\
and therefore:
Therefore, if we define
â exp â (n â l)L(a),

2.3. Admissible Reductions of an Unreduced Experiment
87
we obtain:
â - = exp[a'V^ Si] 
d(an Â® (rn) 
"
which implies
d(an Â® crt)
where c* is the image measure of crn by t = 
s*, i.e., <7t = o'*â , the n-th
convolution of cr. Thus Â£avT = BÂ£Â£(ani o*n).
2.3.7 
Sampling Theory and Bayesian Methods
In this section, we compare the sampling theory concepts of sufficiency 
and ancillarity in the sample space with the corresponding Bayesian conÂ­
cepts. Similar comparisons could be made in the parameter space using the 
duality properties. Here, Bayesian sufficiency and Bayesian ancillarity refer 
to Definitions 2.3.2. to 2.3.5.
Let us consider a statistical experiment Â£ = {(5, <S), P a, a E A} and A, a 
cr-field on A, which makes P a(X) measurable for any X  E S. For s E [<S]+ 
and T a sub-cr-field of S, we denote by T as the conditional expectation, 
given T, of s with respect to the sampling probability P a. In particular, 
l as denotes the expectation of s with respect to P a.
It is often desirable for T as to be bimeasurable. However, this is not 
in general implied by the fact that Pa(X) is measurable for any X  â¬ S. 
This therefore requires a supplementary assumption which is described in 
the following definition.
2.3.10 D efinition. We shall say that a statistic T  C S  is regular if, for 
each s E [Â£]+ , T as E [AV T] (more precisely, if there exists an A V  T- 
measurable function which is a version of T as for each a Â£ A). 
â 
Note that this condition of regularity is weaker than the condition of 
separability. Indeed, for any s E [S] J, define a family of measures on (5, S) 
as follows: dQa>s/ d P a = s. Clearly, T as = dQ^s j d P f  and, by Theorem
0.3.19, the separability of T  ensures the existence of a bimeasurable version 
of this Radon-Nikodym derivative. But the condition of the regularity of T  
is more general, in the sense that, if A is finite, then any sub-<r-field of S  is 
regular.

88
2. Admissible Reductions: Sufficiency and Ancillarity
A. Sufficiency
Let // be a probability on ( A ^ 4), and let S = (A x S, A  V Â«S,II) be the 
corresponding Bayesian experiment as defined in Section 1.2.1. It is then 
clear that if T  C S  is regular, T as can be chosen as a version of (.4 V T)s 
for any s E [<S]+.
In sampling theory a sufficient statistic is a sub-cr-field of S such that, 
given this cr-field, there exists a common version of the sampling expectaÂ­
tions, of any iS-measurable function. More precisely, following Halmos and 
Savage (1949) 
see also Bahadur (1955a) and Barra (1971) â we have:
2.3.11 D efinition. A sub-cr-field T  C S  is a sampling sufficient statistic if 
for any s E [<S]+ , there exists t E [T]+ such that V a E A , T as â t a.s.Pa. 
m
Note that, in general, this definition does not possess the monotonicity 
property of the corresponding Bayesian definition, in other words, T  suffiÂ­
cient and U D T  do not imply U sufficient (see Burkholder (1961)). This 
clearly shows that the two definitions are not equivalent without further 
assumptions. However, we have the following result:
2.3.12 T heorem . Let T  C S  be a sampling sufficient statistic. Then, for 
any prior probability //, T  is also Bayesian sufficient.
Proof. If s E [S]+ and t E [T]+ are such that T as = t a.s.Pa for any 
a E A, it follows that for any prior probability //, (A V T)s â t a.s.II, and 
so (.4 v T) s E[T]+. -
To obtain the converse implication, we need to place some assumptions 
on the prior probablity /i.
2.3.13 D efinition. We will say that the prior probability \i is regular for 
the family {Pa,a E A} if s E [S]^ and l as = 0 
a.s.// imply Xas = 0
V a E  A. 
â 
Two special cases of such a property are the following:
(i) 
if A is countable, a prior probablity // will be regular if it gives 
positive mass to each point of A;

2.3. Admissible Reductions of an Unreduced Experiment
89
(ii) 
if A is a topological space, and if {Pa,a Â£ A }, the sampling 
probabilities are such that for any s Â£ [*S]ooj %as is continuous on A , then 
a prior probability p will be regular if it gives positive probability to each 
open (measurable) subset of A.
This leads to the following result:
2.3.14 Theorem . Let /ib e a  regular prior probablity on (A, A). If 
T  C S  is a Bayesian sufficient statistic, then T  is a sampling sufficient 
statistic.
Proof. If T  is Bayesian sufficient, then V s Â£ [Â«S] + , {A V T)s = T s a.s.II. 
Hence, V 6 Â£ [*4]+ , V i Â£ [T]+, we obtain successively:
!(&â¢*â¢Â«) 
= I(6-< -T Â«),
It follows that, V * Â£ [S]+, V t Â£ [T]+
Xa(t â¢ s) = J a(t â¢ Ts) 
a.s./i.
Hence P a(f â¢ s) = T a(t â¢ Ts) V a & A since p is regular. This implies that,
V a Â£ A and V 5 Â£ [S]+ , T as = Ts a.s.Pa. 
â 
Suppose now that the statistical experiment is classically dominated, as 
in Section 1.2.2. By the factorization theorem, sufficiency becomes a meaÂ­
surability property and thus recovers the property of monotonicity exactly 
as in a Bayesian experiment where, even in the undominated case, suffiÂ­
ciency is a measurability property. Moreover in this situation, the definition 
of sufficiency is restated in terms of a âprivilegedâ dominating probability 
P* (see Section 1.2.2). A statistic T  C S  is then sufficient if and only if
V s Â£ [<S]+ , T as = T*s a.s.Pa V a Â£ A, where %s is the conditional 
expectation, given T, of s with respect to P* (see Halmos and Savage (1949 
â proof of Theorem 1)). Let us compare this equivalent definition with the 
Bayesian definition: V s Â£ [<S]+ , (A V T)s = T s  a.s.n; recall that if T  is 
regular, (iV T ) s  = T as a.s. n. We see that in a Bayesian experiment, the 
predictive probability plays a role similar to that of a privileged dominating 
measure even if the Bayesian experiment is not dominated.

90
2. Admissible Reductions: Sufficiency and Ancillarity
R em ark. It should be clear that the existence of a prior probablity /i reguÂ­
lar for the family {Pa : a G A} implies that for each a G A, P a is dominated 
by the predictive probability P (indeed, in Definition 2.3.12, let s be the 
indicator function of any P-null set); the statistical experiment is therefore 
classically dominated. Furthermore, the predictive probability is equivalent 
to any privileged dominating probability P* since they are both weighted 
averages of elements of {Pa : a G A}.
A further step would be to particularize these concepts to a family 
of experiments associated with a family of prior probabilities and identiÂ­
cal sampling process. In this approach, Martin, Petit and Littaye (1973) 
have shown that, in the dominated case, classical sufficiency (on the samÂ­
ple space) is equivalent to Bayesian sufficiency (on the sample space) for 
the family of all prior probabilities, and in the undominated case, classical 
pairwise sufficiency (on the sample space), (see e.g., Halmos and Savage 
(1949)) is equivalent to Bayesian sufficiency (on the sample space) for the 
family of purely atomic prior probabilities; further results may be found 
in Ramamoorthy (1980a, 1980b) and Roy and Ramamoorthy (1979). Note 
that Bayesian sufficiency may also be seen as a classical pairwise sufficiency; 
this fact is shown in the next theorem.
2.3.15 Theorem . If T  C S  is a Bayesian sufficient statistic, then A  V Tis 
sufficient in the classical experiment (A x S, .4 VÂ«S, {II, jx0 P}); the converse 
is also true if II is dominated by /x 0  P.
Proof. Vs G [Â«S]+, T s = T s a.s .P and hence a.s .(//0 P). But ( A V T ) s  = 
Ts 
a.s.(// 0  P). If T is sufficient, (.4 V T)s â T s a.s.II and, consequently, 
Ts is a common version of (A V T)s and (AVT)s. However, if t is a common 
version of (*4 V T)s and of (.4 V T)s, then t = T s = T s a.s.(/i 0  P), hence 
a.s.n by domination, and so (A V T)s = Ts 
a.s.n. 
â 
B. A ncillarity
We now compare ancillarity in sampling theory and in a Bayesian frameÂ­
work. Recall the sampling theory definition of an ancillary statistic.
2.3.16 D efinition. A sub-<r-field T  C S  is a sampling ancillary statistic 
if for any t G [T]+ , there exists a constant k in M+ such that Xat = k 
V a G A. 
m

2.3. Admissible Reductions of an Unreduced Experiment
91
Let p be a probability measure on (A, A) and Â£ = (A x S', A  V <S, II) the 
corresponding Bayesian experiment. Using Monotone Class Theorems, it is 
clear that V sG  [S]+, I as Â£ [.4]+ and so Xas may be chosen as a version of 
As for any s G [5]+ i.e., As =-Xas a.s.II. This gives the following results.
2.3.17 Theorem . If T  C S  is a sampling ancillary statistic, then for any 
prior probablity p , T  is also Bayesian ancillary. 
â 
2.3.18 Theorem . Let p be a regular prior probablity on (A, A). If
T C S  is a Bayesian ancillary statistic, then T  is a sampling ancillary 
statistic.
Proof. If T  is Bayesian ancillary, for any t Â£ [T]J>, At = Xt a.s.II and so 
Xat = Xt a.s.II and this is equivalent to Xa(t â Xt) = 0 a.s./i. Since p is 
regular this implies Xat = Xt V a Â£ A. 
â 
Similarly to Proposition 2.3.14, the Bayesian conception of ancillarity 
may be seen as classical pairwise ancillarity in the sense that T  C S  is a 
ancillary statistic in the Bayesian sense if and only if A  V T  is ancillary in 
the classical experiment (A x S ,A  V S, {n, p (Â£> P}).
2.3.8 
A First Result on the Relations between Sufficiency and AnÂ­
cillarity
The definitions of sufficiency and ancillarity expressed in terms of condiÂ­
tional independence allows the basic properties of conditional independence 
(Section 2.2.4) to be used, giving rise to the following result:
2.3.19 Theorem . In a Bayesian experiment Â£ = (A x S , *4VÂ«S, II)
(i) 
Any statistic U predictively independent of a sufficient statistic T  is 
ancillary. Moreover U and T  are sampling independent.
(ii) 
Any parameter C a priori independent of a sufficient parameter B is 
ancillary. Moreover C and B are a posteriori independent.

92
2. Admissible Reductions: Sufficiency and Ancillarity
Proof. By duality it suffices to prove (i). Now, by assumption, U JL T  and 
A JL S  | T  (by sufficiency of T). By Corollary 2.2.4(ii), A JL U | T. By 
Theorem 2.2.10, U J L T  and A X U | T  is equivalent to U JL (A V T) and 
is also equivalent to U X A and U JL T  \ A. 
â 
In sampling theory, this kind of results has received a considerable atÂ­
tention (see, e.g., Barra (1971), Basu (1955, 1958), Koehn and Thomas 
(1975), Soler (1970)). A sampling theory analogue of Theorem 2.3.18 may 
be stated as follows:
2.3.20 T heorem . In a dominated experiment, any statistic U independent 
of a sufficient statistic, with respect to a privileged dominating probablity, 
is ancillary and independent of this sufficient statistic with respect to each 
sampling probability.
Proof. Recall that T*s denotes the conditional expectation of s, given 
T, with respect to privileged dominating probability P*. By sufficiency, 
V sG  [S]+ , T as = %s a.s .Pa V a E A and by independence, V u Â£ [U]+ , 
%u = 2*u a.s.P*. Hence for any u Â£ [U]+, T au = Z*u a.s.Pa. V a E A. 
Thus V u e  [Z/]+, V t E [T]+, Xa{ut) = Za(Z*u â¢ t) = Z*u â¢ 2 at. This 
implies Z*u = 2 au, thus U is ancillary and l a(ut) = 2 au â¢ Zat, i.e., U JL T  
with respect to P a V a Â£ A. 
u
Note that Basu Theorems consider the following different problems. Let 
T  be a sufficient statistic (*4 JL S  \ T). Is a statistic U, independent of T  
in the sampling process (U JLT | *4), ancillary (U JL A) 7 Is an ancillary 
statistic U (U JL A) independent of T  in the sampling process (U X  T  | A) ?
The answer is known to be negative without further requirements as 
illustrated in the next example. The Bayesian versions of these problems 
are treated in Chapter 5.
Exam ple. Let S = {$i, $2} and A = {ai, 0 2 , 03}- Suppose that n  is defined 
by Table 1 and B is the cr-field generated by the function b(ai) = b(a2) = b1 
and b(as) = b2 where 61 ^  b2. That B is ancillary is easily seen from 
Table 2. but clearly B is not independent of A which is sufficient and is 
not independent of any other sufficient parameter. Therefore the converse 
of Theorem 2.3.19 is not true.

2.3. Admissible Reductions of an Unreduced Experiment
93
Table 1 . 
Table 2.
s
p(Si 1 bi)
bi
h
Si
1
3
1
3
S2
2
3
2
3
H(bj)
3
4
1
4
S
p(Si | Ctj)
a i
02
0>3
2
1
1
S1
5
4
3
3
3
2
S2
5
4
3
K ai)
5
12
4
12
3
12
The next proposition is genuinely Bayesian insofar as it has no sampling 
theory counterpart.
2.3.21 Theorem . In a Bayesian experiment E = (A x S, A V <S, II)
(i) 
Any statistic U independent of a sufficient parameter B is ancillary,
(ii) 
Any parameter C independent of a sufficient statistic T  is ancillary.
Proof. By duality, it suffices to prove (i). By assumptions we have: U X B 
and A X  S | B. This implies, by Corollary 2.2.4(ii), A A. U | B. Now, by 
Theorem 2.2.10, U JL B and A X U | B is equivalent to U A. A. 
â 


3
Admissible Reductions in 
Reduced Experiments
3.1 
Introduction
An implicit conclusion of Chapter 1 is that any statistical model actuÂ­
ally used in practice should be viewed as a reduction of a larger model, this 
reduction being obtained by (i) marginalization on actually available and 
seemingly relevant observations, (ii) marginalization on parameters deemed 
to be of some interest, and (iii) conditioning on functions of both parameters 
and observations. In particular, one undertakes conditioning given âreaÂ­
sonableâ assumptions about parameter values and about variables which 
should be regarded as âexogenousâ, i.e., as if the marginal processes of data 
generation were not worthy of complete specification. Thus any model in 
actual use may be viewed as a model of the form 
r , a reduction of an 
unreduced model Zavs (with B C A, T C S and M C A V S); in other 
words, instead of specifying a probability Uavs on a âhugeâ space A V <S, 
the statistician typically specifies a âsmallerâ probability measure 
thereby economizing on the specification of the two associated submodels, 
namely, IIm and 1 1 ^ ^ v r . The motivation for such a reduction is that the 
unreduced model Eavs would typically be unmanageable and, further, that 
the more detailed specification of Zm and Â°f ZBs/Mwr would not only not be 
worth the marginal cost, but would also probably aggravate the problem of 
specification errors. Thus the reduction Z$$r  is viewed not as an admissible
95

96
3. Admissible Reductions in Reduced Experiments
reduction of Â£avs but, rather, as an operational starting point; any loss of 
information related to this reduction being accepted ab initio.
In view of this state of affairs, two types of questions naturally emerge. 
How can 
be simplified further without additional loss of information? 
And when is a pair of experiments a jointly admissible reduction of Â£ 4vs? 
The response to these questions is the object of this chapter.
The first question concerns the admissible reduction of 
having
accepted a loss of information in the reduction of Â£avs to Â£#^7-, one seeks 
to further simplify Â£
without additional loss of information. Technically, 
this is the question of ancillarity and sufficiency in reduced experiments, and 
is studied in Sections 3.2 and 3.3. For expository purposes, we have chosen 
to examine firstly the admissibility of the reduction Â£bvt where the original 
experiment is reduced by marginalization only, and thereafter the reduction 
&BvT where the original experiment is reduced both by marginalization and 
by conditioning. In addition to providing a smoother development of the 
theory than a single exposition with regards to Â£5^7- only, the presentation 
regarding Â£bvT will also provide results that are easily put to use (both in 
applied work and in the theory to be exposed in later chapters).
The second kind of question is rather different in nature. In Section
1.4.4. we decomposed the learning process (i.e., the transformation p ââº ps ) 
either on the sample space or on the parameter space. Let us now consider 
a joint decomposition of the learning process, as illustrated below:
VA
^V5
) 
nS\
} 
Va
II
II
VB
Cbvt
V>B
Â£t
CBVS
Vb
0
0
A
CAVT
A ^
cBVT
CAVS
..BVS
Va

3.1. Introduction
97
We may now ask whether the pair of experiments (Sbvt , Â£avs ) constiÂ­
tutes a jointly admissible reduction of Â£av$ , (i-e., whether both 
and
SawT are totally noninformative), or whether the pair Â£awT ^ bvs constiÂ­
tute a jointly admissible reduction of Â£avs (i-c., whether both Â£bvt and 
Â£aws are totally noninformative). These questions are the object of Section
3.4. Note, in particular, that in the first case one would carry out reducÂ­
tions by marginalization (or conditioning) on both the parameter and the 
sample space whereas in the second case on would carry out reductions by 
marginalization on the one space and conditioning on the other space.
It should be stressed why the questions treated in Section 3.4 are differÂ­
ent in nature from the questions treated in Chapter 2, and also from those 
treated in Sections 3.2 and 3.3. Indeed, the properties of sufficiency and anÂ­
cillarity in a given experiment (unreduced or already reduced) are relevant 
mostly at the stage of inference or prediction once the data is collected, and 
the experiment is specified; at that level, a property of sufficiency and/or 
of ancillarity is a property of a given probability measure (on A  V <S), and 
is useful mainly for simplifying the calculations required by the inference 
and/or the prediction. By contrast, a property of joint admissibility, as 
discussed in Section 3.4, is useful mainly at the stage of model specification. 
Indeed, in practice, the specification of a Bayesian experiment, typically of 
the form Â£&$?, generally involves the following sequence of steps. A large 
experiment Â£avs is first considered as a general framework of reference, in 
the sense that the probability space (.4 x Â«S, A  V Â«S, II) is not specified in 
detail but is only briefly sketched. Subsequently, an answer is sought to 
the question: Which aspects of Â£a vs should be now specified in detail beÂ­
fore considering the stage of inference and/or of prediction? The conditions 
for jointly admissible reductions characterize the structural properties of II 
(namely, conditions of stochastic independence) before its detailed analytÂ­
ical form is specified making it possible to decide which parts of II should 
be further specified as they are associated with totally noninformative exÂ­
periments. Furthermore this section will also lead to conditions allowing for 
specifying separately part of (or, all) the probability measures involved in 
the two experiments constituting the pair of jointly admissible reductions of 
Â£a v s â¢ Finally, the structure of the loss function will often imply that only 
one of the two jointly admissible reductions actually involves the parameter 
of interest. As a result of adopting the appropriate specification strategy,

98
3. Admissible Reductions in Reduced Experiments
the experiment actually used at the stage of inference and/or of prediction 
will not only be much simpler than the reference experiment Â£avs (if this 
were completely specified) but will also be robust against a large class of 
specification errors.
Section 3.5 is, in a sense, complementary to Section 3.4, for we progresÂ­
sively shift emphasis from building ah experiment towards comparing two 
given experiments with a view to deciding whether or not one is redundant 
with respect to the other. We shall see that the inherent symmetry (beÂ­
tween parameters and observations) of a Bayesian experiment will provide 
a natural environment for a Bayesian presentation not only of sufficiency 
in the sense of Blackwell (1951, 1953) and LeCam (1966, 1986), but also 
of model choice, model specification or hypothesis testing. Note, however, 
that a detailed treatment of these topics would also require a treatment 
of approximate admissibility, a topic not treated in this monograph (inÂ­
terested readers are referred, in particular, to Csiszar (1967a), Torgersen 
(1972, 1976, 1981), and Florens (1983). In other words, Section 3.5 merely 
sketches a general framework which handles, from a Bayesian point of view, 
a large class of problems in statistical methodology, which a rapidly growing 
literature suggest are of fundamental import.
Florens and Mouchart (1977) has been the starting point for Sections 
3.2, 3.3, and 3.4 but the sections on dominated experiments (3.3.6 and 3.4.4) 
were worked out later. The Section 3.2.4 on partial sufficiency is based on 
Mouchart and Rolin (1984a). This presentation, along with its examples, 
also relies on Florens and Mouchart (1985a) and (1986a). Most of Section
3.5 sketches work under progress, the first steps of which have been reÂ­
ported in Florens, Mouchart and Scotto (1983), Florens and Scotto (1984), 
and Florens and Mouchart (1985c). Steps toward a Bayesian approach to 
hypothesis testing and encompassing are not reported in this chapter, but 
may be found in Florens and Mouchart (1988) and Florens (1988) respecÂ­
tively. The motivation underlying those works is found in the appeal for a 
better understanding of exogeneity in econometric modelling expressed in 
Florens, Mouchart and Richard (1974), and which led to a systematic invesÂ­
tigation of the linear model with error in the variables (Florens, Mouchart 
and Richard (1979)), which is the source of the examples of this chapter.

3.2. Admissible Reduction in Margined Experiments
99
Let us conclude this introduction by remarking that this chapter does 
not contain a strictly probabilistic section. Thus, at the technical level, this 
chapter makes use of exactly the same tools as the first two chapters. FurÂ­
thermore, a discussion of the relationship between Bayesian and sampling 
theory is confined to some remarks, the general discussion in Section 2.3.7 
continuing to serve as the general framework of reference.
3.2 Admissible Reduction in Marginal Experiments
3.2.1 
Introduction
In this section, we shall see that, because a marginal experiment has exÂ­
actly the same structure as an unreduced experiment â viz., a unique probÂ­
ability II on a product space â the concepts of ancillarity and of sufficiency 
are essentially the same. We shall therefore present the basic definitions 
with only very few comments. Subsequently, we analyze some relationships 
between admissible reductions in unreduced and in marginal experiments, 
and pay particular attention to the problem of sufficiency with respect to a 
sub-parameter for the class of all prior probabilities.
3.2.2 
Basic Concepts
We consider two sub-(7-fields B C A and T  C S. The marginal experÂ­
iment Sbvt was defined in Formula (1.4.6). Now, the Definitions 2.3.2 to
2.3.5 may be used in order to define sufficiency or ancillarity, in the exÂ­
periment SbvT, of any sub-<r-fields C C B or U C T. For instance, the 
condition:
(3.2.1) 
B Â± T \ U
is, as in Definition 2.3.2, equivalent to each one of the following two:
(3.2.2) 
nr {E) = n u {E) 
V E e B
(3.2.3) 
P BvW(X) = P U{X) 
V X e T
and means that U is ^gvT-sufficient or, equivalently, that Â£"[(v7- is totally 
noninformative. Similar comments can be made with respect to conditions 
such as B JL T  | C, i.e., Â£Â£vr-sufficency of C, B X 7/, i.e., Â£gvr - ancillarity

100
3. Admissible Reductions in Reduced Experiments
of U, or C X T , i.e., ^Bvr-ancillarity of C. It should be stressed that this 
analysis remains consistent with B = A  and/or T  = S.
Apart from this formal identity of concepts in Â£avs and Â£bvT > it should 
be noted that as the condition C JLT is symmetric in C and T, this not 
only means that Â£cvr is totally noninformative, but also means (equivaÂ­
lently) that C is f^vr-ancillary, or that T is Â£cvs-ancillary. This is a first 
example of a condition leading to an admissibility property in two (reduced) 
experiments. This topic is pursued more systematically in Section 3.4. The 
concern of the next definition is precisely to stress the reciprocal character 
of the ancillarity of C with respect to T, and of T with respect to C.
3.2.1 D efinition. The parameter C(C -4) and the statistic T(C S) are 
mutually ancillary if and only if they are independent, i.e., C X T . 
â 
In general, the conditions for admissible reductions of experiments marÂ­
ginalized on a sub-cr-field C of the parameter space depend crucially on the 
(conditional) prior probability \ f . The following example illustrates this 
point.
Exam ple. Let s = (* i,..., xn) with x, = (y,-,*,)' and
Suppose (Xi | a) ~  LA^O, a) and a is a priori distributed as an Inverse- 
Wishart distribution. If we define c = a n a ^1 and t = ( z i,...,z n)> then 
C = o(c) and T  = o(t) are mutually ancillary; indeed, a basic property of 
this prior distribution is c X 022; this implies, by Theorem 2.2.10, that c JL t 
because c JL t \ 0,22 (indeed (t | a) ~  An(0,c^Tn))- Note, however, that 
such a mutual ancillarity would no longer be true if the prior distribution 
on a did not render c and a.22 independent. 
â 
3.2.3 
Sufficiency and Ancillarity in Unreduced and in Marginal 
Experiments
As already mentioned in Section 2.3.8, the relationship between ancilÂ­
larity and sufficiency has drawn a good deal of attention in the statistical

3.2. Admissible Reduction in Margined Experiments
101
literature. The results on the unreduced experiment 
discussed in SecÂ­
tion 2.3.8, also hold within any of the marginal experiments SavT, 
or 
SbwT] they are not repeated here, and are left as an exercise for the reader. 
Rather, the object of this section is to examine the relationships between 
sufficiency (or ancillarity) in a marginal and in an unreduced experiment. 
These relationships we now report are fairly trivial consequences of the 
monotonocity properties of conditional independence (as given in Section 
2.2) and are presented not merely for their own interest, but also as a first 
step toward the analysis of jointly admissible reductions in Section 3.4.
By Corollary 2.2.4, one has the following implication:
(3.2.4) 
A  Â± S \ T  =Â» B JL U \ T  
V W c S  V B c A
Therefore, if T  is ^-sufficient then T  is ^ v^-sufficient for any B C A  
and any U such that T  C M C S . Similarly, as
(3.2.5) 
A Â± T  => B Â± T  
V B e  A
one has: 
if T  is ^-ancillary, then B and T  are mutually ancillary for any
B C *4, and T  is Â£gVÂ£/-anciliary for any B C A  and U such that T  C U C S.
Also, as:
(3.2.6) 
A  Â± S  \B => C Â± T  \B V C c A  V T c  S
one has: 
if B is ^-sufficient, then B is Â£cvr-sufficient for any B C C C A
and T  C S. Finally, as
(3.2.7) 
B Â± S  => B JL T 
V T c
one has: if B is ^-ancillary, then B and T  are mutually ancillary for any 
T C S, and B is Â£cvr-ancillary for any C such that B C C C A  and any 
T C S .
Clearly, the converse of these implications is false. In particular, to be 
sufficient in a marginal experiment, a sub-(7-field has to make a smaller class 
of functions measurable than in a complete experiment. A sub-<r-field may 
therefore be sufficient in a marginal experiment but not in the associated 
unreduced experiment, as illustrated by the following example.
Exam ple. (See Florens, Mouchart and Richard (1974)). Let s = (Â« i,..., sn) 
with (si | a) ~  i.N(nii,v) where a = (m i,..., m,-,..., mn, v). For the

102
3. Admissible Reductions in Reduced Experiments
prior distribution, define m â (m i,..., m*,..., m n) and suppose m X v, 
mi ~  i.N(0,1) and v has an arbitrary distribution (continuous on 172+). 
If we consider B = <r(i;) and T  = (]C1<t*<n 
^hen ^  *s Â£/3vÂ«s-sufficient 
(because (Â«,- | v) ~  i.N (0, v -f 1) but is clearly not ^-sufficient. 
â 
3.2.4 
A Remark on âPartialâ Sufficiency
Sufficient statistics for the marginal experiment Â£ b v s  are relevant when 
B is the only parameter of interest (i.e., B makes the loss-function measurÂ­
able for any decisions) and the âotherâ parameters are nuisance parameters 
(on nuisance parameter from a Bayesian point of view, see, e.g., Dawid 
(1980a), or for a sampling approach, see Godambe (1976, 1980)). This 
suggests that it is appropriate to speak of âpartialâ sufficiency in such cirÂ­
cumstances. Furthermore, a recurrent theme in the literature has been to 
examine the relationship between classical sufficiency and âBayes sufficiency 
for all prior distributionâ (see Section 2.3.7). These two considerations lead 
Kolmogorov (1942) to suggest the following definition of partial sufficiency.
3.2.2 D efinition. A statistic T  C S is K-sufficient for B C A  if, for 
every prior probability p on (A, A) , T  is Â£Â£wS-sufficient where Â£Â£wS is 
defined as follows: {A x 5, BVS, n ^ v5) and n ^v5 is the trace on B V S  of 
n ^v s = P Â® P A> When B = A, we simply say that T  is AT-sufficient. 
â 
In Section 2.3.7, we investigated the relationship between Bayesian sufÂ­
ficiency for a particular prior probability, /^-sufficiency (i.e., Bayesian suffiÂ­
ciency for any prior probability) and sampling sufficiency. Now, we consider 
the relationship between A'-sufficiency (for the unreduced parameter *4) and 
AT-sufficiency for a subparameter B C A. In this respect, Hajek (1965) â 
see also Basu (1977) â asserted this definition to be void in view of the 
following assertion:
A ssertion. 
(Hajek (1965)). If B is nontrivial (i.e., B 
â {<j),Ax 5}) 
and T is /^-sufficient for B then T is Ar-sufficient.
Martin, Petit and Littaye (1973) gave a counterexample to Hajekâs asÂ­
sertion and proved that if the sampling probabilities are all equivalent (i.e., 
have the same null sets) then Hajekâs Theorem is correct; the main arguÂ­
ment is that in this case, I\ -sufficiency is equivalent to pairwise sufficiency

3.2. Admissible Reduction in Marginal Experiments
103
(Martin, Petit and Littaye (1973 - Propr. 1-7); see also Speed (1976)). 
Consider, however, the following finite example:
Example.
Sampling Probabiliiies p(s{ \ aj)
ai
<*2
Â«3
U4
Si
.15
.10
.20
.25
S2
a
.30
.10
0
S3
.45
.30
.60
.75
s4
a
l
0
.30
.10
0
where 0 < a < .40
Consider the cr-fields B = <r(b) and T  = <r(t) defined by the following 
functions:
(3.2.8) 
K aj) = 
h  
i  =  1,2,3
= 
62 
j = 4
(3.2.9) 
t(si) = 
tx 
i â 1,3
= 
t 2 
i = 2,4
It may be checked that T is A'-sufficient for B for any value of a (even for 
a = 0) while T  is A-sufficient if and only if a = .2. 
â 
This example not only illustrates the nop-necessity of the condition of 
equivalent sampling probabilities, but also shows that the failure in the proof 
of Hajekâs Assertion is due to the non-respect of the following condition:

104
3. Admissible Reductions in Reduced Experiments
(H) 
p(sj | a,j) = p(si> | dj) = 0 implies: 
p(s{ | cij/) = X(si,Sii)p(sii | ajt) for any dj>.
In other words, condition (H ) says that in the matrix of the sampling probÂ­
abilities p(si\cij), any two zeros in the same column should always be assoÂ­
ciated to proportional rows. In view of the following theorem this condition 
turns out to also be sufficient.
3.2.3 Theorem . In the discrete case, Condition (H ) is necessary and sufÂ­
ficient to ensure that any statistic /^-sufficient for B (nontrivial) is also 
K -sufficient.
Proof, (i) Condition (H ) is necessary.
If 
p{si | dj0) = p(si/ | dj0) = 0, consider the following statistic t:
(3.2.10) 
tf(s) 
= 
t\ 
if s = Si or st/
= 
t% 
otherwise.
cr(t) is K -sufficient for a nontrivial subparameter defined by:
6(a) 
= 
6i 
if 
a = dj0 
= 
62 
otherwise.
Indeed, pfi(b\ | Si) = p^(6i | s,*/) = 0 for any prior probability p\ but if
T is also ZC-sufficient the two rows associated with s* and s*/, should be 
proportional.
(ii) Condition (H ) is sufficient.
We show that under (i/), for any non X-sufficient statistic T  and any 
nontrivial parameter B, there exists a prior probability such that T is not 
Bayesian sufficient for that B and that prior probability. Indeed, if T = a(t) 
is not /f-sufficient, there exist (s*,Sj/,aj,a; /) such that t(si) = t(sif) and 
p(s{ | dj)p(si> | djt) ^  p(si | dji)p(sii | dj). If b(dj) 
b(djt) then T  is 
not sufficient for B = a(b) once p(dj) = p{dj>) = 
If b(dj) = b(dj/), 
there exists am such that 6(am) ^  b(dj), because B is not trivial. Note that 
(Si, 
, dj, dj 1) are such that
[p(*i | aj) + p(siÂ» | aj)]\p(si | dji) +p(si> | djt)\ > 0 
and 
p(si | aj)/p(sv | a}) Â± p(s< | ay)/p{sv \ ay)
Then T  = 
(3.2.11)

3.3. Admissible Reductions in Conditional Experiments
105
on 
(where a/0 is defined as oo for a > 0). Furthermore, under (H ), 
either p(s{ \ am) -f p(si> | am) = 0 and
P (s i I 
I a y )  =  P ( s i I 
)?(*Â»â¢' I a j )
or p(si | am) +p(s,/ | am) > 0 but then the ratio p(s* | am)/p(sii \ am) 
is well defined and cannot be equal to both p(s{ \ aj)/p(sii | aj) 
and p(si | a>ji)/p{sit | ajt) and, therefore, T  is not sufficient for B 
(either for p(aj) â p(am) = \  or for p(aj>) = p(am) = 5). 
â 
In terms of conditional independence, the problem of equivalence beÂ­
tween /^-sufficiency for a nonconstant subparameter and K -sufficiency may 
be viewed as follows. If B and S  are independent conditionally on T  (i.e., 
B X S  | T  for any prior probability), under what conditions does this imply 
that A D  B and S  are independent conditionally on T  (i.e., A  X S | T  for 
any prior probability)? Clearly, in the general case, the crucial point is âfor 
any prior probabilityâ because, by Theorem 2.2.10, one such condition, for 
a particular prior probability, would be A  JL S  | B V T.
Condition (H), a condition on the sampling probabilities, provides the 
result for any nontrivial subparameter B. An alternative approach would 
be to look either for a property linking a statistic T  and a subparameter 
B or for a property, stronger than non-triviality, which should be possessed 
by a subparameter 5, in order for Hajekâs Assertion to be true. Recently, 
Cano, Hernandez and Moreno (1988) takled the case of an uncountable 
space (5, S ) when the sampling probabilities are dominated and the cr-field 
A  on the parameter space is separating. In such a context, they modified 
our condition (H ) into a similar necessary and sufficient condition for a 
given statistic T  and a given nontrivial parameter B.
3.3 Admissible Reductions in Conditional Experiments
3.3.1 
Introduction
We now extend the concepts of sufficiency and of ancillarity to an exÂ­
periment 
already reduced, by both conditioning and marginalization,
where B C A, T  C S  and M  C A  V S. This is the more general setting. 
Indeed, the definitions provided in this section generalize those provided

106
3. Admissible Reductions in Reduced Experiments
previously by setting M  = X, and also provide definitions for an experiÂ­
ment reduced by conditioning only, by setting B = A  and T  = S.
As mentioned in the introduction to this chapter, the general experiment 
is the natural set-up for almost any statistical analysis, and from a 
technical point of view, once the extension of the concepts of parameters and 
of statistics to conditional models (given in Section 1.4.3.D) is recalled, the 
formulation of the concepts of ancillarity and of sufficiency at this level of 
generality involves an essentially trivial extension of th conceptual apparatus 
elaborated for the unreduced experiment Â£a v s - Accordingly, we do not 
give extensive comments of these concepts so as to avoid largely trivial 
duplication of the discussions in Chapter 2. Instead, we shall limit ourselves 
to presenting some simple examples at the end of Section 3.3.2, after the 
general definitions; it should nevertheless be stressed that all forthcoming 
chapters make extensive use of the concepts developed in this section, and 
it may be viewed as a rich source of illustrations of the usefulness of these 
concepts (see also, e.g., Kalbfleisch (1975), and Sprott (1975)).
3.3.2 
Reductions in the Sample Space
Recall that in an experiment such as Â£&$t (see Section 1.4.3.D), a statisÂ­
tic Af is a, sub-<7-field of M  VT. Following the general principle that a reducÂ­
tion is admissible if the complementary reduction is totally noninformative, 
i.e., the prior probability is a.s. equal to the posterior probability, we obtain, 
using the diagrams of Subsection 1.4.4.C, the following definitions:
3.3.1 D efinition. Af is an Â£ ^ r -sufficient statistic if and only if:
(i) 
Af C M V  T]
(ii) 
B X T\Ai V Af. 
m
3.3.2 D efinition. Af is an Â£ ^ r - ancillarity statistic if and only if:
(i) 
A f c M V T ;
(ii) 
B Â± A f \ M .

3.3. Admissible Reductions in Conditional Experiments
107
Recall also that a conditional independence property may be written in 
various ways. For instance, Af is a ^ ^ r -suflicient statistic if and only if
(3.3.1) 
/ # vV vr(Â£) = ^
Vjvr(E ) V E e B
or, equivalently, if and only if
(3.3.2) 
P ^ ,My*r{X) -  P ^ M{X) 
V l e T .
3.3.3 
Reductions in the Parameter Space
Recall that in an experiment Â£#Jr  a parameter Â£  is a sub-cr-field of 
B\/ M . As before we obtain the following definitions:
3.3.3 D efinition. Â£  is an Â£ ^ 7- sufficient parameter if and only if:
(i) 
J C C B V M
(ii) 
B Â± T \ Â£ y M .  
m
3.3.4 D efinition. C  is an Â£ ^ 7â ancillary parameter if and only if:
(i) 
C C B V M ;
(ii) 
Â£ Â± T \ M .  
â 
Let us illustrate these concepts by two examples.
E xam ple 1 . (Sufficiency in Regression Models)
Let s'i = (j/,-, z<) e M1+p i = 1 ,..., n, s' = (Â« ;,..., s'n), y' -  {yu ..., yn), 
z' = (z j,..., zJJ, and Z â (z i,..., zn)', an n x p matrix. Now suppose that 
(z{ | a) ~  i.7V(m, Â£), and (yt- | Z,a) ~  i.N{(3*Zi, cr2), where a = (m, Â£,/?, <r2) 
and, in particular, f3 E 1RF. The regression model of (y | z) may 
be defined
as Z&t/T where M  = <r(Z), T  = <t(t/), and, in particular, 5  = a(/3,a2). 
A
natural sufficient statistic is Af = <r(Z'y, y'y) where, clearly, Af C M  y  T . 
Although #  = o(/3j(t2) is often a natural parameter, it is sometimes more 
suitable to use Â£ = (j(Z/?,cr2), which is obviously included in B V Ad, as a 
sufficient parameter. 
â 

108
3. Admissible Reductions in Reduced Experiments
Exam ple 2. (Sufficiency in Censored Data)
Let s\ =  (yi,Zi) Â£]R 2 i = l ,. . . , n ,  
s'â). Let
H a ) ~ i J V 3 [ ( g ) , J 2
where A  = fl'(ra), and y, is observed only if it is smaller than z%. Thus 
we define x = ( x i,...,x n); with x,* = min(y,*,Zi). The experiment to be 
analysed will often have the form Â£ ^ w h e r e  M  = v(z\, ..., zn), B = A, 
and T  = <t(xi,... , xn). For this reduced experiment, a sufficient statistic 
may be built as follows: define w = (wi ,.. .,  wn)' with Wi = 1 if xt- < Z{, and 
Wi = 0 if x* = Zi, i.e., w, = 1 {Xi<Zi} \ one may check that U  = a(w,wfx) is 
an Â£g^T-sufficient statistic. Note again that M  C M  V T. 
â 
3.3.4 
Elementary Properties
In the reduced experiment 
the monotonocity properties of suffiÂ­
cient and ancillary cr-fields of an unreduced experiment (Proposition 2.3.6) 
are preserved. This fact is made explicit in next proposition, the proof of 
which is a simple application of Corollaries 2.2.11(ii) and 2.2.4(ii).
3.3.5 Proposition. In a reduced experiment
(i) 
An Â£ ^ r -statistic (respectively parameter) including an ^ ^ r -sufficient 
statistic (respectively parameter) is also Â£g^r -sufficient.
(ii) 
An Â£j3^r -statistic (respectively parameter) included in an ancillary 
Â£ j^T-statistic (respectively parameter) and is also Â£g^r -ancillary. â 
The property of robustness of a sufficient or ancillary cr-field for a suitÂ­
able modification of the prior probability is also preserved. This fact is 
made explicit in next proposition, the proof of which is based on Corollary 
2.2.15.
3.3.6 P roposition. Let us consider S and Â£', two Bayesian experiments 
on {A x 5, A  VS) defined by their probabilities II and II'. Let B C A, T  C S 
and M  C A  V S. If

3.3. Admissible Reductions in Conditional Experiments 
109
(i) 
n' <  n
(ii) 
â¬ [Â£VA<J + .
Then:
(iii) An Â£ j Â£ r -sufficient statistic (respectively parameter) is also 
fg^r-sufficient;
(iv) An Â£g^r -ancillary statistic is also 
ancillary;
(v) 
If C  is an Â£ j^r -ancillary parameter and if furthermore
E [C V M \ + , then C is also an ^ ^ r -ancillary parameter. 
â 
Note that if assumption (ii) is replaced by (dU'/dli) E [M VT] + , 
conclusion (iii) still holds, but conclusion (iv) becomes: An Â£ ^ r -ancillary 
parameter is also ^g^-ancillary and conclusion (v) becomes: If Af is an 
5Â£Jr -ancillary statistic, and if furthermore (dll'/dll) G [A4 V Af\ + , then 
Af is also an Â£ ^ r -ancillary statistic. It may be useful to recall that, as 
in Proposition 2.3.7, the condition (dll'/dll) E [B V M \+ (respectively, 
(d n '/d n ) G [M V T j+) means that II' differs from II only in its marginal 
part of B V M  (respectively, M V T ) .
3.3.5 
Relationships between Sufficiency and Ancillarity
Note that a conditional independence property such as B JL T  \ A4 VAf, 
where B C A, T  C 5, M  C A V  S, Af C M  \/ T  may receive several 
interpretations. Indeed, it means that Af is an Â£ j^7'-sufficient statistic, and 
also that T  is a uniform i^ ^ -a n c illa ry  statistic and that B is a uniform 
^j^j^-ancillary parameter. Another interesting conditional independence 
property is A  JL S  | B V T, where B C A  and T  C S. It means both that B 
is a uniform Â£r -sufficient parameter and that T  is a uniform 5 B-sufficient 
statistic, and consequently that Â£ BwT is totally noninformative.
An interesting property of the stability of sufficiency in conditional exÂ­
periments may be derived from Corollary 2.2.11, viz., any 5-sufficient statisÂ­
tic (respectively, parameter) is also a uniform 5 ^ ^ -sufficient statistic (reÂ­
spectively, parameter) for any B C A  and T  C S. Furthermore, Theorem
2.2.10 entails the following result:

110
3. Admissible Reductions in Reduced Experiments
3.3.7 Proposition.
(i) 
T  C S is ^-sufficient (respectively, ^-ancillary) if and only if there exÂ­
ists B C A  such that T  is ^ V5-sufficient (respectively, Â£#v5-ancillary), 
and Â£ s -sufficient (respectively, Â£ e-ancillary).
(ii) B C A  is ^-sufficient (respectively, ^-ancillary) if and only if there
exists T C S  such that B is Â£ 4Vr-sufficient (respectively, Â£avT~ 
ancillary), and S r  sufficient (respectively, Â£r -ancillary). 
â 
Using the same theorem, Theorems 2.3.19 and 2.3.20 may be extended 
to the general reduced experiment Â£$*t .
3.3.8 Theorem. In Â£ ^
j ,
(i) 
any statistic (respectively, parameter) independent, given M , of an
Â£g^r -sufficient statistic (respectively, parameter) is Â£ ^ r -ancillary 
and is moreover independent of this sufficient statistic (respectively, 
parameter) given B V  M  (respectively, given M  V T);
(ii) any statistic (respectively, parameter) independent, given A4, of an
Â£ j^ r -sufficient parameter (respectively, statistic) is Â£ j^r -ancillary. â 
Proof. We prove (i) for a statistic only. In that case, the theorem may be 
restated as follows. Let Mi C M  V T, i = 1,2. If B X T  | M  V M2 , and 
Mi JL M2 | M , then B JL Mi \ M , and Mi JL M2 | B V M . It suffices to 
note that, since Mi C M  V T, B X T  | M  V M2 implies 
| M  V M2
(Corollary 2.2.11 (i) and Corollary 2.2.4 (ii)). By Theorem 2.2.10, when 
Mi JL M2 | M , this last relation is equivalent to Mi JL (BV M2 ) | M ,  and 
this is equivalent to the conclusion. The proof of (ii) follows exactly the 
same approach. 
â 
Note that, when Mi and M2 are uniform statistics (namely, Mi C T, 
i = 1, 2) it is enough to require M2 to be Â£^/j^flVj^2-sufficient.

3.4. 
Jointly Admissible Reductions
111
3.3.6 
Sufficiency and Ancillarity in a Dominated Reduced ExperiÂ­
ment
It is sometimes useful to deduce a conditional independence property 
from arguments based on the factorization of the density. 
In this secÂ­
tion, considerable use is made of Theorem 2.2.14. Since by construction, 
A  1  <S; /i Â® P, we can obtain sufficiency and ancillarity with respect to 
/i (g) P only if the conditioning (7-field M  is of the form C V U where C C A  
and U C S (Corollary 2.2.11). The next theorem describes the factorization 
properties of the density g^/T Â°f ^he experiment 
> that are implications 
of the existence of sufficient and ancillary statistics and parameters.
3.3.9 Proposition. Consider the experiment 
where B C A, T  C S,
M. = CVU with C C A  and U C S. Then
(i) 
M  is an f ^ r -sufficient statistic if and only if
nM  
_  nM  
âA4VAf.
9 b v t â 
QbV'Nâ * 
9r 
i
(ii) 
M  is an 
ancillary statistic if and only if 
s Â£ r = s Â£ '- 9f f s F M',,r;
(iii) C is an ^ ^ T-sufficient parameter if and only if
nM  
â nM  
. nc v M .
9& v T  â  9 c v t  
9 b j
(iv) C is an fj^^-ancillary parameter if and only if
nM  
â nM  . nM  . â C V M v T _
9 b v T  â  9c 
9t  
9 b  
â¢ 
â 
Although sometimes useful, these decompositions of the densities are 
nenetheless complicated and, in particular, verification requires integrations 
on some parameters and/or statistics. Additional assumptions may simplify 
this process. That is the subject of the next section.
3.4 Jointly Admissible Reductions
In Sections 3.2 and 3.3 we analyzed successive reductions of experiments. 
In this section, we analyze joint reductions of experiments, i.e., reductions 
on both sample and parameter spaces. In contrast with the analysis above,

112
3. Admissible Reductions in Reduced Experiments
both reductions are introduced simultaneously; this approach is motivated 
by the search both for computational simplicity and for robustness of the 
specifications.
When the jointly admissible reductions are obtained by marginalization 
on both the sample space and the parameter space, this is referred to as 
mutual sufficiency; when they are obtained by marginalization on one space 
and conditioning on the other space, this is referred to as mutual exogeneity. 
The concept of a Bayesian cut combines both possibilities.
As the concept of an unreduced experiment is actually relative to a given 
experiment, the definitions provided in Section 3.4.5 are for the most genÂ­
eral form of a reduced experiment S ^ r . For expository reasons, however, 
we introduce the concepts and provide both proofs and comments for the 
simpler case where M  = X, B = 
an d T  = S. The proofs for the general 
case are straightforward extensions of this simple case. The more general 
set-up is necessary, however, when dealing, for example, with sequential 
problems as in Chapter 6. In Section 3.4.6, some examples will reveal the 
relationships between the three basic concepts of mutual sufficiency, muÂ­
tual exogeneity, and the Bayesian cut; while in Sections 3.4.1 to 3.4.3, each 
example illustrates only one of these concepts.
3.4.1 
Mutual Sufficiency
If B C A  represents the only parameter of interest, we have seen that, 
without losing relevant information, the experiment Â£ may be reduced to 
EbvT if T  is an ^ V5-sufficient statistic. The transformation p& ââº 
will, 
in general, require the integration of PA with respect to pB so as to obtain 
P B. The next theorem states when such an integration may be avoided. 
However, when B C A  represents the only parameter of interest, it often 
happens that one may find a statistic T  such that the sampling distribution 
of T  depends on B only. Unless a further condition is introduced, this does 
not imply, in general, that statistical inference about B (thus, its posterior 
distribution) depends on T only. The next definition formalizes this double 
requirement.
3.4.1 Theorem . Let Â£ = {A x S ,A  V*S, n) be a Bayesian experiment. For 
B C A and T  C Â«S, the following properties are equivalent:

3.4.
Jointly Admissible Reductions
113
(i)
B Â± S \ T  and
A Â± T \ B
(ii)
Ta = S(Ba)
V a â¬ M]+
(iii)
Bs = A{Ts)
V 8 â¬ [S]+ .
Proof. By symmetry it suffices to prove the equivalence of (i) and (ii). 
By Theorem 2.2.1. (iii), A  JLT \ B implies Ta = T(Ba), and by TheoÂ­
rem 2.2.1(ii) B JL S  \ T  implies T(Ba) = S(Ba). Hence (i) implies (ii). 
Furthermore, applying (ii) for a Â£ [B]+ C [.4]+ yields B X S  | T; hence 
Va Â£ [v4]+ , S(Ba) = T(Ba), and applying again (ii) yields Ta = T(Ba), 
i.e., by Theorem 2.2.1(iii), A  JLT \ B. 
u
3.4.2 D efinition. Under any one of the conditions of Theorem 3.4.1., we
define B and T  to be mutually sufficient. 
u
Exam ple. Consider x = (y, z)f and (x \ a) ~  ^[(sin^jC os^)', 
with
a = (^>, 77) Â£ A = (â7r + 7r] x IRj. Let the prior distribution be an indepenÂ­
dent product of a gamma distribution for rj and a uniform distribution for 
(p. It is rather easily checked that the posterior distribution of 77 depends 
on r 2 = y2 -f z 2 only (i.e., 77 JL x \ r 2), while the sampling distribution of 
r 2 (being (1 /^JxK 7?)) depends on 77 only (i.e., a JL r 2 | 77); therefore 77 and 
r 2 are mutually sufficient. It is shown, in Chapter 8, that this example 
crucially depends on the fact that the prior distribution (on <p) displays a 
suitable invariance property on a compact parameter space. 
â 
R em ark. The concept of mutual sufficiency in the form of condition (i) 
in Theorem 3.4.1. is used by Martin, Petit and Littaye (1973), Definition
III-l, for a family of prior distributions. 
â 
The property of mutual sufficiency means that the pair of reductions 
Â£BvT and Â£bvt is jointly admissible in the sense that the pair Â£&vs and 
Â£ % r  are both totally noninformative. It is therefore natural to analyze 
some implications of mutual sufficiency for both Â£jsvT and Â£ Bs/r.

114
3. Admissible Reductions in Reduced Experiments
We first consider the marginal reduction Â£ b vT - The reduction by joint 
marginalization on both the parameters and the observations has been abunÂ­
dantly studied in the statistical literature, in particular after Barnardâs 
(1963) work on sufficiency. A key issue is the elimination, in a sampling 
theory framework, of nuisance parameters; âin the absence of any informaÂ­
tion on these parametersâ , the condition B JL S \ T  may be viewed as a 
Bayesian condition, (on the relationship between the prior distribution and 
the sampling process). This leads to the easy elimination of the nuisance 
parameter when the sampling distribution of T  depends on B only. Whereas 
the construction of a statistic T, mutually sufficient with a given parameter 
B y may be difficult in general, it is shown, in Chapter 8, that invariance arÂ­
guments arise quite naturally in such an undertaking; such arguments have 
been used, namely, in fiducial theory (see, e.g., Fraser (1968)), to justify 
more or less heuristic inferences on B based on T  only. From an analytical 
point of view, recall that in the regular case â see (1.4.7) â one has, in 
general:
(3.4.1) 
IIbvt = Pb Â® Pr â Pt Â® Pb 
where
(3.4.2) 
^ r P O =  /  Pt ( X W B 
V I S T .
JA
Now, if only the first conditional independence relation of condition (i) 
of Theorem 3.4.1. â viz., T  is Â£#v$-sufficient â were true, P B would 
require an integration as in (3.4.2). But the second conditional independence 
relation of condition (i) of Theorem 3.4.1. says that
(3.4.3) 
P#(X) = P t ( X )  
V X â¬ T.
The integration in (3.4.2) may thus be avoided in the course of the transforÂ­
mation pb â5â Pb â¢ Clearly, for a given sampling transition PB , the relation 
B JL S \ T  depends on p B only, whereas the relation A JL T  \ B does not 
depend on the prior probability (see 3.4.3). However, these two relations do 
not imply any restriction on n ^ v r, and are therefore properties of UBs/T 
only. In many instances there will be a B and T mutually sufficient for a 
family M  of prior distributions. If this is the case then only pb should be 
specified for the analysis of the transformation p b  ââº p%- Furthermore, by 
Corollary 2.2.15, if pb is modified into /ig, mutual sufficiency will not be 
affected provided p'B <C Pb-

3.4. 
Jointly Admissible Reductions
115
Mutual sufficiency also has useful implications on Â£ BwT. Suppose, for 
instance, that B is introduced to operate with exact restrictions, on the 
parameter space. Then, the condition A  JLT \ B means that T  is ancillary 
in Â£ B, and Â£ B may therefore be admissibly reduced to Â£ BvT. From an 
analytical point of view, recall that in the regular case, one has, in general,
(3.4.4) 
nBvr = /<BvT <g> PAwT = P BvT <g> /iBvB.
Now, A  JL T  \ B implies that:
(3.4.5) 
Â» BwT(E) = 
V E e A ,
i.e., the transformation 
â> fiBvS may be obtained from P AwT. If, furÂ­
thermore, B i  S  | T, i.e., B is ^-ancillary, then one has
(3.4.6) 
P BvTp 0  = Pr {X) 
V X 6 S, 
and in such a case (3.4.4) is now written as
(3.4.7) 
n Bvr = nB Â® PAwT = PT Â® fiBvS.
The concept of mutual sufficiency is weaker than the requirement that 
both B and T  be ^-sufficient. Indeed, as a corollary of Theorem 2.2.10, one 
has the following link between these concepts:
3.4.3 Proposition. For B C A  and T  C S, the following conditions are 
equivalent:
(i) 
A Â± S \ B  
and A Â± S \ T ]
(ii) 
B Â± S \ T  
and A  J L T  \ B 
and 
A Â± S \ B V T .  
â¢ 
Note that if B and T  were both ^-sufficient, one would have
(3.4.8) 
nBvr =  nB Â® P T = PT Â® n B.
This is precisely the meaning of Proposition 3.4.3: if B and T  are both 
5-sufficient, this is equivalent to B and T  be mutually sufficient, and to 
the conditional experiment Â£ Bs/T being totally noninformative, namely, 
> 1 1 5  | B y  T. Thus the 5^ ws -sufficiency of both B and T  would make 
Â£bvT an admissible reduction of Â£avs j whereas the mutual sufficiency of 
B and T  make Â£bvT, and Â£BwT a jointly admissible pair of reductions of 
Â£.Av5-

116
3. Admissible Reductions in Reduced Experiments
3.4.2 Mutual Exogeneity
In the previous section, mutual sufficiency leads to the consideration of 
joint reduction by marginalization (or conditioning) on both spaces A and 
S. The concept of mutual exogeneity, to be defined, leads to the consideraÂ­
tion of pairs of joint reductions involving marginalization on one space and 
conditioning on the other.
3.4.4 Definition. In a Bayesian experiment Â£ = {A x 5, A  VÂ«S, II), C C A, 
and T C S, are mutually exogenous if:
(i) 
C
A
T
;
(ii) 
A Â± S \ C V T .  
m
In other words, C and T are mutually exogenous if C and T  are muÂ­
tually ancillary (condition (i)), and C is an Â£r -sufficient parameter, or, 
equivalently T  is an Â£c-sufficient statistic (condition (ii))-
Example. Consider s = (t/, z)' and a = (6, c)'. Suppose that
m â )  
~  * [ ( : ! ; ) , ( ;  \ t
c 
~  
N{0,1), 
(6 | c) ~  N(c, 1).
Then z and c are mutually ancillary since (z | c) ~  7V(0,2) and, since 
(y | 6, c, z) ~  N(z -f 2c, 1), c is a sufficient parameter in Â£r , where T  is the 
cr-field generated by z. 
m
Thus, the concept of mutual exogeneity means that the pair of reducÂ­
tions Â£ % r  and Sjys ls jÂ°inHy admissible in the sense that the pairs Â£cvr 
and Â£Cwr are both totally noninformative. It is therefore natural to analyze 
(some) implications of mutual exogeneity on 
and on
First consider Sjvs- R^aU that, in the regular case (1.4.9), one has, in 
general:
(3.4.9) 
n lws = tilÂ®  PCvr = PTÂ®l4-
If C and T  are mutually exogenous, condition (i) is equivalent to:
(3.4.10) 
#Â£(Â£?) =/*(Â£?)

3.4. 
Jointly Admissible Reductions
117
and condition (ii) is equivalent to:
(3.4.11) 
p AwT( x )  = p cvT( x )  
v x  e s.
Thus, we obtain:
(3.4.12) 
n j v5 = ncÂ® PAwT = P T Â® 14,
i.e., the transformation fic -+ He ls obtained from PAyT without integratÂ­
ing this latter with respect to fiCwTâ¢ Note also that, in view of (3.4.11), 
condition (ii) does not depend on the prior probability. Therefore, fic is 
used merely to check condition (i). In other words, if only C is of interest, 
and if C and T  are mutually exogenous, the process generating T  becomes 
irrelevant, and the data-generating process may be directly conditioned on 
T  (without integration with respect to fic ). This is precisely the meaning of 
the so-called âexogenous variablesâ used in econometrics models (see, e.g., 
Koopmans (1950), or Malinvaud (1978)). This Bayesian analogue insists 
that the exogeneity concept is relative to a parameter of interest.
Now consider Â£Avt - In the regular case â see (1.4.12) â one has, in 
general,
(3.4.13) 
IicAvT = nc <g> P? = Pr Â® /
v r â¢
If C and T  are mutually exogenous, condition (i) is equivalent to:
(3.4.14) 
PÂ£(X) = Pr (X) 
V I G T
and condition (ii) is equivalent to:
(3.4.15) 
/
v5(Â£) = /*CVT (E)
Finally, under mutual exogeneity we obtain:
(3.4.16) 
UcAvT = hC 0  P /  = Pr Â® /
v5,
i.e., the transformation \ f  ââº /iCv5 is obtained from PÂ£ only (and not 
from P's). In other words, if C is used to represent exact restrictions on the 
parameter space, and if C and T  are mutually exogenous, the observations on 
the sample space may be admissibly marginalized on T, and the prediction 
on T  is not affected by the use of this exact restriction.
Note, finally, that for a given C, by Corollary 2.2.15, mutual exogeneity 
(as mutual sufficiency) is robust with respect to a modification of ptc into 
H'c if n'c C u e -

118
3. Admissible Reductions in Reduced Experiments
3.4.3 
Bayesian Cut
The notion of a Bayesian cut combines the concepts of mutual suffiÂ­
ciency and of mutual exogeneity in such a way as to provide a complete 
decomposition of a Bayesian experiment. It is also the Bayesian analogue 
of the concept of a cut as introduced by Barndorff-Nielsen (1973, 1978).
3.4.5 D efinition. Let E = (^4 x S', *4 VS, II) be a Bayesian experiment; let 
also B C A, C C -4, and T C S be sub-cr-fields. Then (B, C, T) operates a 
Bayesian cut on E if:
(i) 
B JLC\
(ii) 
A J L T \ B \
(iii) A J L S \ C V T .  
m
The basic idea of a Bayesian cut is that the sub-<j-fields of A  which 
make P? and P / v r measurable are a priori independent; in other words, 
conditions (ii) and (iii) may actually be viewed as definitions of the subpaÂ­
rameters B and C, thus leaving condition (i) as the proper characterization 
of a Bayesian cut. Unless otherwise indicated, any references to a cut in the 
sequel, should be interpreted as a Bayesian cut. The main properties of a 
cut are summarized in the following theorem:
3.4.6 Theorem . If (B,C,T) operates a cut on E, then:
(i) 
B y  C is ^-sufficient, i.e., A  JL S \ B V C;
(ii) 
B and T  are mutually sufficient, i.e., A  JL T  \ B and B JL S  \T;
(iii) C and T  are mutually exogenous, i.e., C JL T  and A  X S  | C V T;
(iv) B and C are a posteriori independent, i.e., B JL C \ T  and B JL C\ S.
Proof. In Definition 3.4.5., (ii) implies A  JL T  | BM C and (iii) implies 
A  JL S \ B V  CV T  (Corollary 2.2.11 (ii)). These two conditional indepenÂ­
dence relations are equivalent to A  X S  | B V C. This proves (i). Now 
A  JL T  \B  implies C JLT \B, and, along with B 1 C , this is equivalent to 
C 1  (T V B) which implies C JLT, and B JL C \ T. This proves (iii) and

3.4. 
Jointly Admissible Reductions
119
the first part of (iv). This also implies, by Theorem 2.2.10, that B X C | T. 
Since A  X S  | C V T implies B X S  | C VT, applying Theorem 2.2.10 again 
gives B X (S V C) | T . This is equivalent to B JL S  \ T  and B JL C\ S ,  and 
this proves (ii) and the second part of (iv). 
â 
In consequence, if B represents the parameter of interest, T  is Sbws- 
sufficient and the transformation fi& âÂ» fig does not require integration of 
the sampling probabilities P* with respect to fiB (or, equivalently, with 
respect to fic because B JL C). Likewise, if C represents the parameter 
of interest, T  is Â£cvs-anciliary and the transformation fic â* He does no^ 
require integration of the sampling probabilities p ^ vT with respect to fiCvT. 
In any case, inferences on B and C are completely separated in the sense that 
B and C are independent both a priori and a posteriori, and the revision 
of the prior probabilities is based on two different aspects of the sampling 
process â those characterized by P B and by Pgwr respectively.
The structure of a cut is not affected by a modification of the prior 
probability fi into // if fi' -C //, and if this modification preserves the inÂ­
dependence of B and C. This offers an easy characterization of the class of 
prior distributions compatible with a cut.
Given the concepts and results developed thus far, we can now respond 
to the following question: How far can we reduce a sampling process by conÂ­
ditioning on a statistic T  without losing information about the parameters 
of interest C? If all the parameters are of interest (i.e., C = *4), conditioning 
on T  will be admissible if T  is ancillary, in the sense of Chapter 2; this condiÂ­
tion is identical in a sampling theory framework and in a Bayesian approach, 
up to the //-null sets. If, however, the parameters of interest define a strict 
sub-cr-field of A , i.e., C C A  and C ^  A, the condition of mutual ancillarity 
between C and T  is the minimal condition which ensures the admissibility of 
conditioning on T  for inference on C. This condition, however, depends cruÂ­
cially on the specific form of /ic , the prior distribution conditional on C, and 
has no true equivalent for sampling theory. It is, in particular, difficult to 
describe a modification of fic that preserves a property of mutual ancillarity. 
Robustness with respect to the specification of f f  and ease of computation 
are improved if one furthermore requires C to be an Â£T-sufficient parameter. 
This is precisely the condition of mutual exogeneity but once again there is 
no fully equivalent concept in sampling theory. With the stronger condition

120
3. Admissible Reductions in Reduced Experiments
of a Bayesian cut, it is straightforward to characterize the modifications 
of /i that preserve the condition of a Bayesian cut: one needs merely to 
preserve the prior independence between C and #, where B is a parameter 
sufficient to describe the sampling process generating T ; this requirement is 
very close to the condition of being variation-free (i.e., factorization of the 
parameter space) used in the sampling theory concept of a cut.
Finally, there exists some kind of converse to Theorem 3.4.6 which links 
the three types of joint reductions when A  takes the form B y  C. This 
gives the following proposition, the proof of which is a direct application of 
Theorem 2.2.10:
3.4.7 P roposition. Let A  = B V C and T  C S.
(i) 
If C and T  are mutually exogenous and if B X  C | T, then (#, C,T) 
operates a cut on Â£;
(ii) 
If B and T  are mutually sufficient and B and C are independent both a
priori and a posteriori â i.e., B JL C and B JL C \ S  â then (B , C,T)
operates a cut on S. 
â 
3.4.4 
Joint Reductions in a Dominated Experiment
Consider a dominated Bayesian experiment E = (A x 5, A  V <S, II) and
dU
(3.4.17) 
gAyS = d(p 0  P) *
In this situation mutual sufficiency, mutual exogeneity, and the Bayesian 
cut may be characterized by a factorization of density.
3.4.8 Theorem . In a dominated Bayesian experiment Â£, B C A, and 
T  C S are mutually sufficient if and only if:
(3.4.18) 
gB = gÂ£ 
a.s./i Â® P.
Proof. In view of Theorem 2.2.14, B JL S \ T  and A  JL T  | B are equivalent 
to fifg = g% and gÂ£ = g$ or, equivalently, gBvS =  9 b v t and gAvT = 9 bmtâ 

3.4. 
Jointly Admissible Reductions
121
This implies gsvs = 9 AvT> which is equivalent to (3.4.18). Now, if (3.4.18) 
holds, it suffices to show that gevT = flUvr to complete the proof. But
since A  JL Â£ ;//Â® P , we have (A V T ) X (B\/S) | BVT; //Â® P  and so, using
2.2.1 
(ii), we obtain
9 b v t = (B V T)(gt3vS ) = (A V T)(g&vS) = (*4 V T)(flUvr) = 
â 
3.4.9 Theorem . In a dominated Bayesian experiment Â£ , C C A  , and 
T  C S  are mutually exogenous if and only if:
(3.4.19) 
9 c - 9 s yT 
a.s.// 0  P.
Proof. By Theorem 2.2.14, C 1 T  and .4 X S  \ C VT are equivalent to 
gcvT = 1, and flUvs = 0-4vT â¢ 9cvs/gcvTâ¢ This implies flUvs = flUvr -0cvs 
which is clearly equivalent to (3.4.19). To finish the proof, it suffices to show 
that (3.4.19) implies gcvT = 1- Now, since (*4VT) X  (CvÂ£) | C V T;//Â® P, 
using 2.2.1 (i), we obtain:
9c v t  
= 
(CV T)(<Mvs) = (C V T ) ^ v r  * 0cvs]
= 
(CV T)(<juvt ) â¢ (C V T)(^cv5)
= 
9cvT â¢ <7cvr = 9cVT â¢
Since X{gc\/r) = 1, it follows that </cvr is the indicator function of a set of 
probability one, i.e., gcvT = 1 a.s.// 0  P. 
â 
3.4.10 Theorem . Let Â£ be a dominated Bayesian experiment and let
B C A, C C A y and T  C S. If B X C, (P, C,T) operates a cut if and only 
if:
(3.4.20) 
= g% â  9s^ T 
a.s./i <g> P.
Proof. By Theorem 2.2.14, A  X  T  | B and A  X  S | C V T  are 
equivalent to gAwr = 9 bvT, and 
= flUvr * 9 cvs/gcvTâ¢ This implies
&4vs = 
' 9cvs/ 9cvT which is equivalent to (3.4.20). To finish the

122
3. Admissible Reductions in Reduced Experiments
proof, it suffices to show that (3.4.20) implies g A v r  = 9 b v T- N o w , since 
A  X  (C V S) | C V T; p 0  P, we obtain, by Theorem 2.2.1 (ii):
9AvT 
= 
(A V T )( g As,s ) = ( A V T )
9bvT * 9 c vs
9cvT
9BVT
9cvT
9bvT
9CvT
(A V T)(gcvs)
( n ~ < r \ ( â  
\ 
9 Bv T  ' 9Cv T
(C V i)(gCvs) = -----------------= 9 b v t â¢
gcvr
3.4.5 
Joint Reductions in a Conditional Experiment
If the Bayesian experiment S = (A x 
VÂ«S, II) has been reduced both 
by conditioning and by marginalization into Â£ $ ,r  where B C A , T  C S  and 
M  C A  V Â»S, we may generalize the concepts of mutual sufficiency, mutual 
exogeneity and the Bayesian cut as done below. Note, however, that this 
section is rather sketchy: the extension of the main concepts and results 
from Eavs to 
is formally straightforward, and so the extension of the 
comments and motivations is consequently rather trivial, and indeed superÂ­
fluous. It should nevertheless be emphasized that, because most statistical 
models used in empirical work are conditional, the framework developed 
in this section is actually the most relevant in practice and, in particular, 
underlies the whole of Chapter 6.
3.4.11 D efinition. In the experiment Â£ $ r , a parameter /C C B V M  and 
a statistic Af C M  V T  are mutually sufficient if:
(i) 
J C J L T \ M V A f
(ii) 
B Â± A f \ / C v M .
3.4.12 D efinition. In the experiment Â£ $ r , a parameter Â£ C B V M  and 
a statistic Af C Af V T  are mutually exogenous if:
(i) 
C Â± A f \ M
(ii) 
B Â± T \ C V A A V A f .
3.4.13 D efinition. In the experiment Â£ ^ 7-, the parameters K, C B V AA 
and C C B V A4, and the statistic Af C AA V T  operate a Bayesian cut if:

3.4.
Jointly Admissible Reductions
123
(i) 
1C JL Â£  | M
(ii) 
B X Af 11C V M
(iii) 
B Â± T \ C V M V A T .  
m
The theorems linking these concepts are easily extended to the condiÂ­
tional case.
3.4.14 P roposition. In the experiment 
a parameter 1C C B V M
and a statistic Af C M  V T  are ^ ^ r -sufficient if and only if they are
mutually sufficient and B JL T  | /CV .M V AT. 
â 
3.4.15 P roposition. In the experiment 
if /C C BV M ,  Â£  C B y  AA, 
and Af C M \ /  T  operate a cut, then:
(i) 
JC V Â£  is Â£ j^ r -sufficient, i.e., B JL T  \ JCy Â£ y  M;
(ii) 
JC and T  are mutually sufficient;
(iii) Â£ and T  are mutually exogenous;
(iv) JC and Â£  are a posteriori independent, i.e., JC JL Â£  | M  V T. 
â 
3.4.16 P roposition. Let B y M  = JCyÂ£yM where JC C B y M , Â£  C B y M , 
and Af C M  V T.
(i) 
If Â£ and A/" are mutually exogenous in Â£$$? and if JC X Â£  | M  V Af, 
then {JC,Â£,Af) operates a cut on
(ii) 
If JC and Af are mutually sufficient in 
and if JC and Â£ are inÂ­
dependent, given M ,  both a priori and a posteriori, i.e., JC JL Â£ | M  
and JC JL Â£ \ M y  T, then (JC, Â£,Af) operates a cut on 
â 

124
3. Admissible Reductions in Reduced Experiments
The definitions receive the same interpretations as in Sections 3.4.1 to 
3.4.3. Suppose K and Af are mutually sufficient. If K is the parameter of 
interest, the inference on /C, i.e., the transformation p ^  ââº p ^ r  only will 
depend upon Pjfi/M . If 1C represents an exact restriction, the inference on 
B , i.e., the transformation p )q /M â+ Pq^M vT will depend upon p ^ Ms/^f 
only.
Suppose C and Af are mutually exogenous. If C represents the paramÂ­
eter of interest, the inference on Â£, i.e., the transformation p ^  â* /iÂ£*v r 
may depend upon 
only. If C represents an exact restriction, the
inference on B, i.e., the transformation p ^ M â> p ^ Mwr will depend upon 
PffiM only.
Further, if (/C,Â£,A/*) operates a cut on Â£Â£$r> the inference on K, and 
C will be completely separated in the sense that 1C and C are independent 
both a priori and a posteriori. The inferences on 1C and Â£, i.e., the transÂ­
formations 
â+ /iÂ£*vT and p ^  â> /iÂ£*v r , will be carried out using two 
disjoint aspects of the sampling process â those characterized by PffiM 
and by p 8vxv.A/^ respectively.
Finally, if Â£ â (A x S, A  V Â«S, II) is a dominated Bayesian experiment, 
Theorems 3.4.8, 3.4.9. and 3.4.10. may be extended to the case of a general 
reduced experiment ^ ^ 7- where AA = CMU with C C A  and U C S. Indeed, 
factorizations of the densities impose conditional independence relations on 
p (g> P. In fact, B X T  | 
p 0  P can be deduced from A  X S\ p <g> P 
only if AA has the form CMU. With this caveat, the extensions of Theorems 
3.4.8, 3.4.9 and 3.4.10 are rather straightforward, and are summarized as 
follows:
3.4.17 P roposition. Let Â£ = (^4 x S, *4VÂ«S, II) be a dominated Bayesian 
experiment, and let g = dU/d(p 0  P). Let BMC C A, T  M U C S and 
M  = CM U. Let K and C be Â£ ^ r -parameters and let Af be an Â£ ^ 7â 
statistic. Then
(i) 
1C and Af are mutually sufficient if and only if
nM
/ o 4 o i \  
A fvT  __ 9fC 
B V M .
(oA.21) 
gK 
-  â  >gM ,
9 AT
(ii) 
C and Af are mutually exogenous if and only if

3.4. 
Jointly Admissible Reductions
125
nM
to * oo\ 
~MvT _ 
sic 
âBvMVj4.
(4.4.22) 
gc 
-  
â 
 9t  
>
St
(iii) If K JL C | M , (C,K,,N) operates a cut if and only if
(3.4.23)
3.4.6 
Some Examples
In this section we illustrate the different types of joint reductions and 
some of the links between them.
Exam ple 1. (The finite case). Let us consider bivariate parameters and 
observations: a â (&,c) and s = (t,u). Suppose now that each coordinate 
may assume two values only: b Â£ {61, 62} ^  Â£ {ci>c2}>^ â¬ {^15^2} and u Â£ 
{^1,1*2}. The Bayesian model characterized by the joint probability 7r(a,s) 
is suitably defined by the following 15 numbers (assumed to be different 
from zero):
/i(ci) = p 0
p (h  | Ci) =  m 
* =  1,2
P(h \ c i , b j ) = Pij 
i,j = 1,2
p(Â«i | ti,Cj,bk) = qijk 
i,j, k =  1,2.
Mutual ancillarity between c and t is equivalent to p(t\ \ c\) = p(t\ | C2),
i.e.,
(.Rl) 
m P n  + (1 -  Hi)pi2 = P2P21 +  (1 -  P2)P2 2â 
For mutual exogeneity, the supplementary condition (a 1  s | c, t) is equivÂ­
alent to the four equalities:
(J?2) 
qiji = qij 2  
i,j = 1,2.
For a cut, it is necessary to verify condition (R2) plus the following three: 
(R3) 
pi = p 2 
(i.e., 
b JL c)
(R4) 
pij = p2j 
j  = 1,2 
(i.e., 
a Â± t \ b ) .

126
3. Admissible Reductions in Reduced Experiments
Even under prior independence (R3), it should be clear that mutual exoÂ­
geneity (Rl + R2) does not imply a cut (R2 -f R3 + R4) but it may be 
checked that the two conditions of posterior independence (b JL c\t),  along 
with mutual ancillarity, are equivalent to (R3+R4): posterior independence 
(b JL c\ t )  plus mutual exogeneity do indeed imply a cut. 
â 
E xam ple 2. Contrary to Example 1, we now exhibit a case where mutual 
exogeneity is equivalent to a cut. Let the prior distribution be: 
a â (6,c) ~  N (0,E), and the sampling process generate s = (t,u) is as 
follows: (s | a) ~  N(a 1V)) where E and V are known 2 x 2  symmetric 
positive definite matrices. Clearly, 7r(a,s) is given by:
Here cov (6,c) = 0 (i.e., prior independence) is equivalent to cov(c, t) = 0 
(mutual ancillarity of c and t)\ note that it is also equivalent to the mutual 
ancillarity of b and u. Since b is clearly sufficient in the marginal experiment 
Ea,t (indeed (t | a) ~  N(b, un) implies a X i | 6) we conclude that, in this 
case, the mutual exogeneity of c and t (in this case , cov (c, t) = 0 and 
cov(6, u | c,t) = 0) does imply that there is a cut. Note also that, after 
some manipulations, one may verify that cov(6, u \ c,t) = 0 is equivalent, in 
this example, to the independence of u and t in the sampling process (i.e., 
cov (u,t | a) = 0). 
â 
In the next two examples we construct regression models from a larger 
model involving also the data generating processs of the regressors. 
In 
Example 3, the cut does not impose any restriction on the parameter space 
but merely involves a simultaneous decomposition of the parameters and 
of the sampling process. In Example 4, the cut does place restrictions on 
the parameter space. This example also leads to the analysis of so-called 
incomplete simultaneous equation models.
E xam ple 3. Let s = (a?i,x2, ..., xn) where x\ = (y-, z-) E IKm, y% Â£ JR*,
Zi E lRk (k -f Â£ = m). Let E be a m x m symmetric positive definite matrix 
and a = (<j{j : 1 < i < j  < m). We suppose that (a:,- | a) ~  i.N(0, E). If we 
set &i = Ey^Ej/, 62 = Eyy âEy^Ej/E^y and c = Ezz, and if we suppose that

3.4. 
Jointly Admissible Reductions
127
a priori E ~  I.W.{vq, S q), then 6 = (iq, 62) and c are independent. Moreover 
if we set y â (2/1, 7/2, â¢ â¢ â¢, 2/n)' and z = (21, 2r2, ..., zn)', then (6,c, z) operates 
a cut since (2:* | a) ~  z.iV(0,c) and (y* | a,z) ~  i.N(b\Zi, 62). Note that the 
structure of the cut is preserved if the prior distribution is modified while 
maintaining the prior independence between b and c. Suppose now that b 
alone is of interest; the conditional model (y | a, z) would be the only model 
of interest even if the marginal model generating z was modified in such a 
way that its new parameters, say c*, were independent of b. In this example, 
the cut places no restriction on the parameter E. 
â 
Exam ple 4. In the previous example, the aqâs are i.i.d. conditionally on 
a. Let us now consider the linear model (x{ | a) ~  LJV(Â£j,E) where E is 
a m x m symmetric positive definitive matrix, & G iKm, 1 < i < n, and 
a = (E,0,& : 1 < i < n). Suppose that A0& = 0 V 1 < i < n where 
A$ is a p x m matrix with rank p which is identified by 6 (i.e., known if 0  
is known). The &âs are generally called incidental parameters. As in the 
previous example, X{ is partitioned into yi and Z{, as is E. We also partition 
Â£ = 
â¢â¢-jfny into (Â£y,6 )> an(i M  into (B$,Ce). We now define
rji = E(yi | a,Zi) G 
and rj = (rji, 772,..., r/n)'. Even if we decompose 
the parameter a = (E,0,Â£) into b â (62, 0, 77) where &2 = Eyy â Ey* E j/E ^ , 
c = ('EZZ1Â£ Z ) 1 (6, c, z) operates a cut only under an (exogeneity) assumption 
placed on the parameter a, viz., B#bi -f C$ = 0 where 61 = Ey2E j/ or, 
equivalently, if (conditionally on a), z% and AqX{ are independent. Under 
this additional assumption, the conditional model is written as
(Vi I a,Zi) ~  i.N(r)i,b2)
Bern + CeZi = 0 a.s.
If B$ is square (p = ^), this model is equivalent to a simultaneous equations 
model Beyi+CeZi = ut- and (ut* | a) ~  i.JV(0,&2). In this case, the incidental 
parameters rji are eliminated by the identity rji = âB^CeZi] otherwise 
(i.e., p < m), this model corresponds to a so-called âincomplete modelâ. 
Note that the incompleteness of the model involves the presence of (n â p) 
incidental parameters (for more details, see Florens, Mouchart and Richard 
(1979), Griliches (1974), Lindley and El-Sayad (1968) Neyman and Scott 
(1948, 1951)). 
i

128
3. Admissible Reductions in Reduced Experiments
3.5 
Comparison of Experiments
Consider two Bayesian experiments:
Si = ( A x S , B i V T i , U i) 
i =  l , 2.
Note that assuming a common product space A x S is not restrictive once 
we allow for different <7-fields B% and 
For instance, if the sample spaces 
of those two experiments has ânothing in commonâ , one could formally 
embed their respective sample spaces into a common space S such as S = 
T\ x T2, and accordingly embed their natural (r-fields of observations (i.e., 
of observable events) into that common S.
If however, we want to âcompareâ Â£\ and 
fhey should share in 
common either the parameters (i.e., B\ = B2), or the observations (i.e., 
Ti = T2). These two cases correspond to different problems. Thus, when 
B\ = B2 = A, we handle a comparison on the sample space and have in 
mind comparisons of experiments in the sense of Blackwell (1951), LeCam 
(1966) (see also Sacksteder (1967) and Heyer (1982)), i.e., the choice between 
two experimental designs in order to infer on a given parameter and, when 
T\ = T2 = tS, we handle comparisons on the parameter space and have in 
mind choices of models to âexplainâ a given observation (and also predict 
future observations to be generated by the same âunknownâ model). In 
both cases we are interested in the question of whether experiment Â£ 2 is, 
in some sense, redundant with respect to experiment Â£\. In order to stress 
the difference of contexts, we shall say that Â£\ is âsufficientâ for Â£ 2 when 
B\ = # 2, and that Â£\ âencompassesâ Â£ 2 when 7i = T2.
The basic idea for comparing Â£\ and Â£ 2 is to look for an embedding 
of Â£ 1 into an extended experiment Â£ in such a way that its characteristic 
cr-field (7i when comparing on the sample space or B\ when comparing on 
the parameter space) is sufficient in the extended experiment Â£, and that 
Â£ 2 is partially embedded into Â£. This will lead to three levels of comparison 
according to the degree of embedding of Â£ 2 into Â£ and to be qualified as 
weak, coherent, and strong.
3.5.1 Comparison on the Sample Space: Sufficiency
In Section 3.2.3, we interpreted a condition of the type A  X  T2 | T\ 
when Ti C T2 C S  as âTi is Â£ 4VT2-sufficientâ . Noting that, in general, this

3.5. Comparison of Experiments
129
condition is equivalent to A  X (71 V Tf) | 71, we may interpret A  X  T2 | 71 
as â71 is Â£>ivTiVT2-sufficient1â even if 71 is not included in T2 .
Let us now reverse that analysis and consider two experiments:
(3.5.1) 
Si = ( A x S ,  A V T i , n * )  
i =  1,2.
Here, the important feature is that both experiments share a common paÂ­
rameter space (A,.4); thus, in the regular case, n* has the form:
(3.5.2) 
n i =tfAÂ® pr*-
We want to make precise the idea that E\ is sufficient for Â£ 2 in the sense 
that the information on A  contained in Â£ 2 is already contained in Â£ \.
3.5.1 D efinition. Â£\ is weakly (respectively, coherently, strongly) suffiÂ­
cient for Â£ 2 if there exists a probability n  on A  V 71 V 7^ such that:
(i) 
= n1.
(ii) 
-4 -1L 72 | 71; n.
(iii-w) (weakly): V t 2 E [T2]+ 
3 b  Â£  [A]+ spch that A 2t 2 = b and A t 2 = b. 
(iii-c) (coherently): weakly along with Pr2 = P}2-
(iii-s) (strongly): ILavT^ = n 2. 
â 
Here *42f2 denotes the conditional expectation of t2 given A  computed 
from the probability n 2; hence ^42f2 is a class of random variables defined up 
to ^-equivalence (where p 2 is the restriction of n 2 to *4), while A t 2 being 
constructed from n  is defined up to a p 1-equivalence (because of condition
(i)). Finally, condition (iii-w) means that there exists a common version, 
in Â£ and in Â£2, of the conditional expectation of any 72-measurable (and 
positive) function given A.
Thus the Bayesian experiment Â£\ is sufficient for Â£ 2 if it can be embedÂ­
ded (condition (i)) in an extended experiment Â£ on A  V71V72 in such a way 
that its observations 71 are ^-sufficient (condition (ii)), and that Â£ 2 also is 
(more or less) embedded in Â£ (conditions (iii)). In a sense, conditions (i) and

130
3. Admissible Reductions in Reduced Experiments
(ii) are somewhat technical in nature (i.e., apart from pathological cases, 
they can always be met) and the very idea of sufficiency among experiments 
lies in the juxtaposition of condition (ii) and one of the conditions (iii).
More specifically, we know, from Theorem 2.2.1-(iii), that condition (ii) 
above implies that A t 2 = A[Tit2] = A 1 [Tit2] V *2 Â£ p 2]+ - Thus in the 
regular case, and when S factorizes into S = Ti x T2 in such a way that 
both Ti and T2 are cylinders <r-fields based respectively on T\ and T2, the 
embedding experiment Â£ satisfying conditions (i) and (ii) will typically be 
build from a transition defined on T\ x T2 and denoted Q
as follows:
(3.5.3) 
n =
and the problem of sufficiency becomes the question of whether there exists 
a transition Q^  satisfying one of the conditions (iii). Note also that when 
II has the structure (3.5.3), the sampling probabilities of Â£ on T2 may be 
represented in the form:
(3.5.4) 
P * ( X 2) = j  Q%(X2) 
dP%* 
X 2 6  T2.
Let us now consider the three levels (conditions (iii-w,c, and s)) for suffiÂ­
ciency. Weak sufficiency â condition (iii-w) â means that the embedding 
probability II should be such that there exists a common version A t 2 and 
A 2t 2 (for any %2 â¬ pi]"*-) and therefore, in the regular case (3.5.3), a comÂ­
mon version of 
â as defined in (3.5.4) and of P ^ A â¢ In particular, when 
the two prior probabilities p \  and p \  are equivalent, all the versions of the 
conditional expectations A t 2 and A 2t 2 are both a.s.^^i and a.s./x^i equivaÂ­
lent under weak sufficiency, and therefore, in the regular case, the sampling 
probabilities of Â£ and Â£ 2 coincide on 7^:
(3.5.5) P%*(X2) = J Q%(X2) 
d P \ f  
a.s./4 , 
(i =  1,2) 
V I 2 â¬ T 2.
This interpretation of weak sufficiency shows that it can also be viewed in 
general as a sampling sufficiency in the same spirit as Theorem 2.3.15. More 
specifically we have:
3.5.2 Theorem . Under conditions (i) and (ii) of Definition 3.5.1, condition 
(iii-w) is equivalent to the sufficiency of A  in the classical experiment
M x 5 M v r 2l{ n ^ vra, n 2}).

3.5. Comparison of Experiments
131
Proof. The classical sufficiency of A  means that for any m E [A V T2]+, 
there exists a common version of A m  and of A 2m. By a monotone class 
argument, this is equivalent to require the existence of a common version 
for any m of the form m = a â¢ 1i with a E [-4]+ and ii E p 2]+ but this is 
equivalent to condition (iii-w) because A(a â¢ tf) = a Ati, and A 2(a â¢ *2) =
a A
2t 2 - 
â 
Finally weak sufficiency may also be viewed, under a very weak regularÂ­
ity condition â condition (o) in next theorem â as a Bayesian sufficiency 
in an extended Bayesian experiment.
3.5.3 Theorem . If there exists a probability II*2 on .4 VTi V72 such that:
(o) 
n & T3 = n 2
and if Z\ is weakly sufficient for S2, then there exists a probability II* on 
J  V A  V Ti V Ti where J  is the (7-field of all subsets of J  = {1,2} identified, 
as usual, with J  x A x  5, such that:
(Â« ) n*(j) > 0 j = 1, 2.
(Â« i) n & r . = if  
i  = i>2.
(*ih) a  j l t 2 1 T un*1.
(*iv) J  Â± T 2 \ A ; U \
Conversely, the existence of such a probability II* implies that Si is weakly 
sufficient for Si.
Proof. We first prove that, under (o), the weak sufficiency of Si for Si 
implies the existence of a measure II* satisfying the four conditions (*): For 
(*i), take any number a such that 0 < a < 1 and define: II*(1) = a and 
H*(2) = 1 â a. Next, for j  = 1, take II*1 = II where II is the probability 
involved in Definition 3.5.1: this guarantees (*ii) for j  â 1 and (*iii); and 
for j  â 2 take the II*2 implied in condition (o): this guarantees (*ii) for 
j = 2. Finally (*iv) is a direct implication of Theorem 3.5.2 because the

132
3. Admissible Reductions in Reduced Experiments
finite character of J entails both Theorems 2.3.12 and 2.3.14. It remains to 
remark that Theorem 3.5.2 also renders trivial the fact that the existence 
of a probability II* on J  V A  V 7} V T2 satisfying the four conditions (*) 
implies the weak sufficiency of Â£\ for Â£2. 
â 
In this theorem, condition (o) is a (very weak) regularity condition to 
be discussed below in a âtechnical digressionâ, the random element 
j  G J = {1, 2} should be viewed as a parameter labelling the two experiÂ­
ments Â£i and II* (j) as an (implicit) prior probability on these experiments. 
Now Theorem 3.5.3 says that weak sufficiency among experiments may also 
be viewed as the Â£*-sufficiency of the parameters common to the two exÂ­
periments (i.e., ,4) when we only observe the data available from the second 
experiment (i.e., T2), where Â£* is a Bayesian experiment extending to the 
labels of the given experiments Â£\ and Â£2, the extended experiment underÂ­
lying Definition 3.5.1.
Let us now turn to the coherent sufficiency. Among the extensions of 
II1 on A  V Ti into a probability II on A  V 71 V 72, it may seem desirable 
to constraint II to have the same predictive probability on 
as II2: this 
is condition (iii-c). It is particularly desirable, in a subjective approach to 
the concept of probability, when the Bayesian experiments Â£ x and Â£2 come 
from a same person (rather than from two âexpertsâ) willing to be coherent 
in his probability assignment. Note, however, that the condition Pj -2 = P^2, 
along with conditions (i) and (ii) does not imply condition (iii-w).
Clearly strong sufficiency (condition iii-s) implies that both experiments 
Â£ l and Â£2 share a common prior probability p = p 1 = p 2 on A  and that 
Â£ l can be extended into an experiment Â£ in A  V T\ V 
that has the same 
sampling probabilities and the same predictive probability on T2 as Â£ 2; in 
other words Â£ l and Â£2 may be viewed as two marginal reductions of an 
experiment Â£ for which T\ is sufficient. Clearly, this implies coherent and 
weak sufficiency of Â£ l for Â£2.
Technical Digression. 
The above discussion involves the difficulty of 
extending a probability on a given cr-field into a larger o'-field. More specifÂ­
ically, let (M, AA) be a measurable space, and M i  (i = 1, 2) be two sub- 
cr-fields of M .  A first question is: Under which condition a probability 
on ( M , M  1) can be extended to a probability on {M^ M\  V M 2) ? This

3.5. Comparison of Experiments
133
is clearly not always possible: for instance, a uniform distribution on the 
Borel sets of the unit interval cannot be extended to a probability on all 
the subsets of the unit interval; but we have mentioned in (3.5.3) that if M  
does factorize in such a way that the cr-fields Mi  (i = 1, 2) are cylinders 
based on the factors, this will always be possible by Theorem 0.3.10.
Now, strong sufficiency raises a rtiore cofriplex question: given two probÂ­
abilities Pi 
on 
( M, Mi )  
( i = 1,2 ), does it exist a probability 
P 
on (M, M \  V M 2) admitting Pi as marginals (i.e., such that Pj^i = Pi, 
i = 1,2)? This problem has been addressed in Strassen (1965), and Diaconis 
and Zabell(1982) have shown that a necessary condition is:
(3.5.6) 
V Xi E M i (i =1,2) : X 1 n X 2 = <j> => P i(X1) + P 2( X2) <1.
Note that this condition is trivially met when Mi  are cylinders cr-fields 
based on factors of M  because in such a case X\  fl X 2 â <j> implies that 
X\  = <t> or X 2 = </>. 
â 
Let us compare Definition 3.5.1 with previous works of Blackwell and 
LeCam. Blackwell (1951)âs original definition may be translated as follows:
D efinition A. Consider two statistical experiments:
Si = {{Tu Ti),Pi'a : a e A )  
i = l , 2 .
8 \ is sufficient for 8 2 if and only if there exists a transition probability 
A : (T i,7 i)---------< (T2,7^) such that
(3.5.7) 
P 2âa( X ) =  f  A \ X )  
P l'a{dt) 
V I G T 2.
JT\
â 
This definition may be generalized (see LeCam (1966)) as follows: 
Definition B. Consider two statistical experiments:
Â£i = {(S,Ti),Pi'a : aeA} 
2 = 1,2.
8 1 is sufficient for 8 2 if and only if there exists a family, {Pa : a Â£ A} of 
probabilities on Ti VT2 such that Pt,a are the restrictions of P a on 75,2 = 1,2 
and 7i is sufficient for the experiment 8  = {5, {T\ V 7^), P a : a E A}. 
m

134
3. Admissible Reductions in Reduced Experiments
Repeating the arguments in Section 2.3.7, and taking a cr-field A  on A 
that makes P a(X) ,4-measurable for any X  E T\ V72, we have the following 
relationships:
(i) If S\ is sufficient for Â£ 2 in the sense of Definition B, then for any prior 
probabilities pi and P2 on (A, ,4), Si is weakly sufficient for Â£ 2 in the sense 
of Definition 3.5.1. Moreover, if 
^1 is strongly sufficient for Â£2.
(ii) If Si is strongly sufficient for Â£ 2 in the sense of Definition 3.5.1, if the 
involved probability II admits a regular conditional probability on Ti V T2 
given A , (see Theorem 0.3.18) and if p is regular, then Si is sufficient for 
Â£ 2 in the sense of Definition B.
Let us illustrate these concepts by an example.
Exam ple. Let us consider A â IRq, S = JR2, (ti | a) ~  N(0, a) and 
(t2 | a) ~  iV(0,a + 1). To show that for any prior probability on a, Si is 
sufficient for Â£ 2 in the sense of Definition B, one may remark that the family 
of probabilities
clearly extends the two given families of (ti | a) and (t2 | a), that its 
conditional probability of (t2 | ti,a) does not depend on a (namely,
(t2 | ti,a) ~  N(ti,l)) and, therefore, the conditions of Definition B are
Note, finally, that Definition 3.5.1 is easily adapted to deal with the 
presence of nuisance parameters: it suffices to consider a sub-cr-field B C A  
instead of A. Similar problems as in Section 3.2.4 may be handled, but, in 
general, a comparison of experiments with nuisance parameter in a sampling 
theory framework raises conceptual difficulties (see, e.g., Goel and De Groot
3.5.2 Comparison on the Parameter Space: Encompassing
verified.
(1979)).
A dual analysis of Section 3.5.1 can be made for the comparison of two 
Bayesian experiments characterized by a same sample space but different

3.5. Comparison of Experiments
135
parameter spaces. Consider specifically:
(3.5.8) 
Â£i = ( A x S ,  B i V S , IF)
where, in the regular case, IF has the form
(3.5.9) 
IT' = / 4  <g> p y 3 i.
3.5.4 D efinition. Â£\ encompasses weakly (respectively, coherently, strongly) 
Â£2 if there exists a probability II on B\ V B 2 V S  such that
(i) 
U Blv s = n1;
(ii) 
B2 Â± S \ B u U;
(iii-w) (weakly) V b2 G [#2]+ 3 t G [Â»S]+ such that Sb2 = t and Â«S262 = f; 
(iii-c) (co/ierenf/y) weakly along with: p s 2 = / i|a;
(iii-s) (^ron^f/y) II^2v5 = II2. 
â 
Under the same notation as in Definition 3.5.1, condition (iii-w) means 
that for any (positive) B2-measurable function (of the parameter), the classes 
of their posterior expectations admit a common version under II and under 
II2, remembering that Sb2 is defined up to a Pj-a.s.-equivalence while S 2b2 
is defined up to a Pj-a.s.-equivalence.
We shall not repeat dual versions of Theorems 3.5.2 and 3.5.3, but, 
rather concentrate the attention of reinterpreting the analysis of Section
3.5.1 in the regular case (i.e., all conditional probabilities admitting a regular 
version) with a parameter space factorizable into A = B\X  B 2 'm such a way 
that the B f s are cylinders cr-fields based on the factors Pj. In this context, 
the embedding experiment Â£ satisfying conditions (i) and (ii) will typically 
be built from a transition defined on B\ x B2) and denoted 
as follows:
(3.5.10) 
n = pj Â® p1*  Â®
while the experiments Si are also representables as:
(3.5.11)

136
3. Admissible Reductions in Reduced Experiments
Note that the posterior probabilities in the extended experiment have the 
form:
(3.5.12) 
f i a(E,) = J  vjfciEh) 
d ^ f  
V Â£ 2 e S 2,
since by (ii) in Definition 3.5.4, we have Sb2 = S(Bib2) = S l {Bib2)
V b2 â¬ [B2]+.
The problem of encompassing is the question of whether there exists a 
transition 
satisfying one of the conditions (iii). Essentially, (weak) enÂ­
compassing is the existence of a common version of p 2^  and p,g2: this is the 
Bayesian formulation of the idea that any inference possible in Â£2 (i.e., 
)
may be reconstructed from Â£\ without retrieving the sample. This idea may 
be viewed as a statistical approach to the principle that, in the scientific 
world, a new theory {Â£{) should, at least, be able to explain the phenomena 
explained by an older theory (Â£2)- This concern has been repeatedly raised 
in D. Hendryâs econometric work (Hendry and Anderson (1977) and DavidÂ­
son, Hendry, Sbra and Yeo (1978) and formulated in Hendry and Richard 
(1982, 1983, 1987). The Bayesian formulation adopted here follows that of 
Florens, Mouchart and Scotto (1983), and Florens and Mouchart (1985).
The transition vÂ®2 is the key to reinterpret the inferences of the exÂ­
periment Â£2 from the experiment Â£\ and should therefore be viewed as 
a Bayesian analogue to the pseudo-true value which is the limit (a.s. or 
in probability) or the expectation of the estimator of a parameter of one 
model under another model. This idea has a long history in statistics (Cox 
(1961,1962), Berk (1966, 1970), Huber (1967), Akaike (1974)); and in econoÂ­
metrics Pesaran (1974), Hausman (1978), Sawa (1978), Hausman and Taylor 
(1981), White (1982), Gourieroux, Monfort and Trognon (1983,1984), Mi- 
zon and Richard (1986)).indexMizon and Richard Note that the classical 
pseudo-true value is a function of the parameter (of the âtrueâ model) while 
its Bayesian analogue is a transition (from the parameter of the encompassÂ­
ing experiment to that of the encompassed experiment).
Coherent encompassing means that in the regular case as in (3.5.9), one 
requires:
(3.5.13) 
l*la(E2) = J  v* ( E 2) 
d /4  
V E 2 e B 2,
i.e., a coherence of the prior distribution on B2 in the experiment Â£2 and 
in the extension of Â£\ into Â£\ this is also Condition (3.5.12) transported a

3.5. Comparison of Experiments
137
priori. In the general case, as in Definition 3.5.4, it follows from (i) that 
PBxnb2 = PBinb2 while conditions (iii-c) imply pb^ b2 = Pb^ b^  therefore 
coherent encompassing implies that p 1 and p2 must coincide on 
The
restrictiveness of this supplementary condition depends, among others, on 
the interpretation of the parameters; when Â£\ and Â£ 2 refer to two different 
sampling schemes, the crucial question is whether one considers the meanÂ­
ing, and eventually the prior probability, of the parameters, independent or 
not of the model.
Strong encompassing implies not only weak and coherent encompassÂ­
ing but also a same predictive probability on both experiments Â£\ and Â£2. 
This restriction would typically be unacceptable in most asymptotic i.i.d. 
models because a common predictive probability on JRÂ°Â° would imply a 
unique decomposition into a prior probability and an i.i.d. sampling (see, 
e.g., Hewitt and Savage (1955), Chow and Teicher (1978), or Dellacherie 
and Meyer (1980, Chap. 5)). Even in small samples, this requirement is 
in opposition with the idea of comparing two models on the basis of their 
predictive abilities, a clearly natural idea. In particular, in the light of TheÂ­
orem 3.5.3, transposed to the comparison on the parameter space, it may 
be shown (Florens and Mouchart (1985)) that a common predictive probaÂ­
bility implies that the model labels are not identifiable and, that the model 
labels are exactly estimable only if the predictive probabilities are mutually 
singular; this last aspect will be deepened in Chapter 6. Thus it should be 
clear that for the encompassing among experiments, the strong concept (i.e., 
condition (iii-s)) is definitely less palatable than for the sufficiency among 
experiments.
Let us conclude this brief presentation by mentioning that the concept 
of encompassing offers one possible avenue for a Bayesian theory of hyÂ­
pothesis testing, once is accepted the idea that the null and the alternative 
hypothesis may be associated to two experiments, being the two hypotheses 
nested or not. In this approach, testing is viewed as a problem of approxÂ­
imate encompassing in the sense of looking for a transition 
satisfying 
âas well as possibleâ the conditions of Definition 3.5.4. The idea is that 
if experiment Â£\ (exactly) encompasses experiment Â£2, the first one will 
be preferred to the second one. When this is not the case, one may look 
for a transition 
minimizing, in 
, either a distance between posterior 
expectations of a parameter of interest, or a divergence between posterior

138
5. Admissible Reductions in Reduced Experiments
distributions when these are evaluated in Â£ 2 and in Â£. This is a Bayesian 
analogue of proposals made by Mizon and Richard (1986) in a sampling theÂ­
ory framework; preliminary results of this Bayesian approach may be found 
in Florens and Mouchart (1988). Details and applications of encompassing 
analysis are given in Florens and Richard (1989), and Florens, Hendry and 
Richard (1989).

4
Optimal Reductions: Maximal Ancillarity 
and Minimal Sufficiency
4.1 
Introduction
In this chapter we seek to identify the âoptimalâ reduction of a given 
experiment. Clearly, such an optimal reduction should be derived from 
monotonicity properties of sufficiency and ancillarity.
As discussed in Section 2.3.5, ancillarity is preserved for any sub-cr-field 
of an ancillary one. One is therefore interested in identifying a maximal 
ancillary cr-field. Unfortunately, stochastic independence is not preserved 
by the wedge operation (V), and, consequently, neither is ancillarity. In 
Section 4.2. we make precise the concept of maximal ancillarity in both 
marginal and conditional experiments and we prove its existence using Zorn 
Lemma. Maximal ancillary cr-fields are not unique; but sufficient conditions 
to ensure that an ancillary cr-field is maximal are provided in Chapter 5.
In Section 2.3.5 it was also mentioned that, in a Bayesian experiment, 
any cr-field containing a sufficient (7-field is itself sufficient. Interest is thereÂ­
fore focused on the identification of a minimal sufficient cr-field. Theorem
2.2.12 showed that the intersection of two sufficient completed cr-field is itself 
sufficient. The minimal sufficient (7-field may therefore be defined through 
the intersection of all completed sufficient (7-fields. Consequently, minimal 
sufficient (7-fields are essentially unique.
139

140
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
In the next sections we take a more constructive approach: we construct 
the minimal sufficient cr-field using the concept of the projection among cr- 
fields. We first define and collect the main properties of the projection 
among ^-fields in Section 4.3; we then use this operation in Section 4.4 to 
construct the minimal sufficient cr-field. Next we introduce, in Section 4.5, a 
new probabilistic tool, namely, the concept of identification among cr-fields. 
We then specialize on the parameter space the analysis of Section 4.4 and 
consider, in Section 4.6, the general problem of identification as a problem 
of minimal sufficiency on the parameter space.
We conclude this chapter by describing in Section 4.7 an ideal (or limit) 
experiment, called âtotally informativeâ where observations give a perfect 
information on parameters. This will provide a Bayesian concept of exact 
estimability, that is introduced, in a punctual form, in Section 4.8.
Section 4.2 on maximal ancillarity essentially gives a set of definitions in 
the framework of this monograph but most substantial results on this topic 
are relegated to Chapter 5 because they require several results of this chapter 
along with new probabilistic tools developed in Chapter 5. Section 4.3 on 
the projections of cr-fields presents and extends the results contained in 
Mouchart and Rolin (1984b). The first steps toward the analysis of minimal 
sufficiency in Section 4.4 were set out in Florens (1974) and elaborated in 
Florens and Mouchart (1977). The work on the identification among cr- 
fields, started in Mouchart and Rolin (1984b), has been applied in Section
4.5 to the identification of parameters in Bayesian statistics after a series 
of papers â Florens (1974), Florens and Mouchart (1977, 1980, 1986a); 
its relationship with sampling theory was presented in Florens, Mouchart 
and Rolin (1985). Finally, the section on totally informative experiments 
elaborates on ideas first sketched in Florens and Rolin (1984).
4.2 
Maximal Ancillarity
As mentioned in the introduction, ancillarity is not preserved by wedge 
operation. Here is a standard example of such a situation
Exam ple. Let s = (x\, X2 , ..., xn) where X{ â (yi, Zi) and let a E] â 1,1[. 
Suppose that (#*â¢ | a) ~  LA^O, Â£ a) where Â£ a = (* 
i). Then each of
V â (Vi > â¢ â¢ â¢ > Vn) or z = (zi, ..., zn) are separately ancillary whatever is the

4.2. Maximal Ancillarity
141
prior distribution on a. But clearly a(y) V a(z) = <j(s) is not ancillary. (For 
a discussion of this example in the sampling theory framework see, e.g., Cox 
(1971) and Cox and Hinkley (1974), Example 2.30). 
â 
Therefore, one cannot speak of a unique maximal ancillary <r-field. We 
may however prove the existence of extremal, for the inclusion, ancillary 
cr-fields.
4.2.1 Theorem . The family of ancillary parameters (respectively, statisÂ­
tics) in an unreduced Bayesian experiment Â£ = (A x S, A  V <S, II), i.e., 
{B C A  : B X  <S}, (respectively, {T C 5 :.4  1 T } )  admits maximal eleÂ­
ments.
Proof. This family is nonempty since it contains the trivial sub-cr-field of 
A, i.e., 2 fl A, since trivially 2 X S. If T  is an arbitrary ordered set and 
{Bt : t G T} is a chain of ancillary parameters, i.e., t < i' =>- B% C 
it admits a maximal element Bt â < t { | J I n d e e d ,  V E Â£
S I e = 21 e - By an application of the monotone class theorem, S I#  = X Ie 
V E Â£ Bt , i.e., Bt is ancillary. By Zorn Lemma, this proves the existence 
of maximal elements in the ancillary parameters. 
â 
It should be noticed that any maximal ancillary parameter (respectively, 
statistics) contains the trivial parameter 2 fl A  (respectively, the trivial 
statistics Xfl<S). Indeed, by Lemma 2.2.5(iii), if B is an ancillary parameter, 
then BW (2 C\ A) = B PI A  is still an ancillary parameter. We will, however, 
restrict our attention to ancillary cr-fields which the following definitions 
identify as essentially maximal.
4.2.2 D efinition. A parameter B is Â£-maximal ancillary if
(i) 
B is ^-ancillary (i.e., B JL S);
(ii) 
if C is an ^-ancillary parameter and C D B then C C B fl A. 
â 

142 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4 .2.3 Definition. A statistic T  is Â£-maximal ancillary if
(i) 
T  is ^-ancillary (i.e., A X T );
(ii) 
if U is an ^-ancillary statistic and U D T  then U C T  fl S. 
â 
A ccording to these definitions, B (respectively, T  is an ^-m axim al anÂ­
cillary param eter (respectively, statistic) if and only if B fl A (respectively, 
T  fl S) is a m axim al element in the sense of Theorem  4.2.1.
These definitions extend n aturally to conditional experim ents as follows.
4 .2.4 Definition. A param eter C  is Â£ ^ r -maximal ancillary if and only if
(i) 
C is an Â£ g^ r -ancillary param eter (i.e., C C B V M  and C X T  | M );
(ii) 
If K  is an 5^ ^ -a n c illa r y  param eter and K  D Â£
then JC C C V  M n ( B V  M ) .  
â¢
4 .2.5 Definition. A statistic Af is Â£Â£$T-maximal ancillary if and only if
(i) 
Af is an 5^ r -ancillary statistic (i.e., Af C  AAV T  and B X  Af | A4);
(ii) 
If 1Z  is an ^ ^ T -ancillary statistic and 1 1  D  Af
then 71 C M  V Af H ( M  V T ). 
â 
N ote th at C is an f ^ r -m axim al ancillary param eter if and only if 
Â£V  AA fl (B V AA) is a m axim al element in the fam ily of f ^ r -ancillary 
param eters. The existence o f these m axim al elem ents is assured by a conÂ­
ditional version of Theorem  4.2.1.
A s m axim al ancillary param eters (or statistics) are not unique, the pracÂ­
tical problem  is to determ ine whether a given ancillary param eter (or sta tisÂ­
tic) is actually m axim al. Such a characterization o f m axim ality requires
tools developed in C hapter 5.

4.3. Projections of a-Fields
143
4.3 Projections of cr-Fields
4.3.1 
Introduction
A s in Section 2.2, we now handle a topic o f general probability theory 
and we adopt the sam e notation: 
(M , A f,P )  is an ab stract probability 
space, M f s are sub-cr-fields of A f , A fo =  
is the trivial cr-field and
Mo is the com pleted trivial cr-field.
T h e m otivation to introduce the projection of cr-fields is to construct the 
âsm allestâ sub-cr-field o f a  cr-field M 2  conditionally on which M 2  becom es 
independent of another given cr-field A fi.
4.3.2 
Definition and Elementary Properties
T he projection of a cr-field A fi on a cr-field AI2 is the sm allest sub-cr-field 
of A f2 with respect to which conditional expectation s of M i-m easurable 
functions given A f2 are m easurable. M ore precisely:
4 .3.1 D e fin itio n . For any two sub-cr-fields A fi, A f2 o f A f we define the 
projection of M i  on M2 ,  which we denote by A f2 A fi, as follows:
(4.3.1) 
A f2A fi =  cr{A f2m i : 
Â£ [A fi]+ }. 
-
R e m a r k s
(i) T h is definition has to be interpreted as the cr-field generated by evÂ­
ery version of the conditional expectation of every positive A fi-m easu rab le 
function.
(ii) T h is operation is crucially dependent on the probability P and, in p arÂ­
ticular, on the P-null sets.
4 .3.2 P r o p o s itio n . Elementary Properties of Projections of a-Fields
(i) 
A f 0 n A f 2 c  A ( i n A f 2 C A f 2A f i  C A f 2;
(ii) 
A f2 C M \  => A f2A f 1 =  M 2;

144
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
(iii) 
Mi  C M2 
M 2 M 1 = M\  n M2',
(iv) 
M3 C Mi  => M2M3 C M 2 M 1;
(v) 
A^4 C A^2 => M 4Mi  C A^4(A42A<i);
(vi) 
M2M1 = M2M1;
(vii) 
M 2M
1 = M 2M
1]
(viii) M 2M 1 Pi Â«M2 = M 2M 1. 
â  
R e m a r k s
(i) Note th at, due to Property (i), M 2 M 1  contains all the null sets o f A42,
i.e., M o  f l A 42. T h is is reflected in Property (viii), if we recall (L em m a 
2.2.5(iii)) th at M2M1 D M 2  =  (M2M1) V (Mo D M2 ) -  P roperty (ii) says 
th at if we project a <r-field on one of its sub-cr-fields we obtain exactly this 
sub-cr-field. On the other hand, by Property (iii), the projection on a cr-field 
of one of its sub-cr-fields am ounts to the com pletion of this sub-cr-field by 
the null sets of the greater (r-field.
(ii) It is im portant to rem ark th at projections of cr-fields is a nonassociative 
operation and henceforth requires the use of parentheses.
(iii) P roperties (i) through (iv) are analogous to properties of projections 
on linear spaces (up to null sets). T h is ju stifies the term  of âprojection â for 
this operation am ong <7-fields. N ote, however, th at in case of linear spaces 
Property (v) would be an equality; nonetheless, in the following exam ple of 
projections am ong cr-fields, only the strict inclusion is verified.
E x a m p le . Let s =  ( x} y)', a =  (6, c, V) and suppose th at
(s I a)
(6 |c,V ) 
~  
#(0,1).
If S  â cr(s) and A  =  cr(a), then >15 =  A.  N evertheless,

4.3. Projections of a-Fields
145
Therefore if B â a(c , V) and C =  a ( v n , v i2 +  c, V22 +  c2), then BS =  C C B 
and C 7^ B. T h is shows th at BS =  C is strictly included in # (.4 5 ) =  #*4 =  B. 
T h e last equality is ju stified by the elem entary Property 4.3.2(ii). 
â 
4.3.3 
Projections and Conditional Independence
We now respond to the question posed in Section 4.2.1 and show th at 
the projection of A fi on A f2 is the sm allest sub-cr-field o f M 2  conditionally 
on which A f 1 and M 2 are independent.
4 .3.3 Theorem. For any sub-cr-fields M
i  and A f2,
(i) 
M x X M 2 I M 2M i;
(ii) 
M 4  C M 2  and M \  X  M 2  | A f4 => M 2 M 1  C A^4.
P r o o f. For any cr-field M 4 C M 2 , Theorem  2.2.6 im plies th at:
A fi X  A f2 | A f4 if and only if A f2 ^  E [Af4]+ , V m  G [A fi]+ .
T h is im plies M 2M \  C M 4 and the proof is com pleted by noticing th at, 
trivially,
A f2m  e  [A f2A f 1]+ 
V m e [ A 4i]+ . 
-
T h is theorem  shows th at A f2A fi is the intersection of all sub-cr-fields 
of A f2 containing the null sets of A f2 and conditionally on which A fi and 
A f2 are independent. M 2M 1 m ay then be w ritten as follows
(4.3.2) 
A f 2 A f 1 =  n (A T n A f2)
ATâ¬^2.i
where T2.1 =  { M  C A f2 : A fi X  A42 | AT}. It m ay be noticed th at M ac- 
K ean ( 1963) defined the projection of cr-fields through the right-hand side 
of (4.3.2), using Lebesgue com pletion instead of m easurable com pletion as 
in this m onograph.
Note also th at, in the language of system  theory, the projection A f iA f 2 
(respectively, A f2A fi) gives ihe minimal internal splitting a-field of M i

146
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
and M 2 which is included \n M \  (respectively, M 2) (see, e.g., Lindquist 
and Picci (1979) and Lindquist, Picci and Ruckebusch ( 1979)).
We now investigate further the relation between conditional indepenÂ­
dence and projection. We first provide an alternative ch aracterization of 
conditional independence in term s of projections; this characterization is a 
straightforw ard application of Theorem  2.2.6.
4 .3.4 P r o p o s itio n . The following properties are equivalent:
(i) 
M i  X  M 2 | M 3 ;
(ii) 
( M i V M 3) M 2 C Ms;
(iii) ( M 2 V M s ) M i  C Ms-
In particu lar M \  JL M 2 if and only if M 2M 1 C M s  or if and only if 
M 1M 2 C Mq. 
â 
T h us, Proposition 4.3.4. characterizes conditional independence by inÂ­
clusions of projections. T h e following C orollary 4.3.5 relies on Theorem
2.2.10 
to obtain several equivalent decom positions of a given conditional 
independence property between cr-fields into conditional independences inÂ­
volving projections either on or of the conditioning cr-field. T h is corollary 
is repeatedly used in deducing new conditional independences from  a given 
one. In particular, in Corollary 4.3.6, a given conditional independence is 
shown to provide a refinement o f the elem entary Properties 4.3.2. in the 
form  of specific inclusions or equalities am ong projections of cr-fields.
4 .3.5 C o ro lla ry . T he following properties are equivalent:
(i) 
M i  X  M 2  | M s ;
(ii) 
(M i V M s) JL M 2 | M s M 2 \
(iii) 
1. M i  X M 2M 3 | M 3 ;
2. (M i V M s) 1  M 2 | M 2M 3 ;

4.3. Projections of cr-Fields
147
(iv) 
1. M i  X M 2  | M 3 M 1  V M 3 M 2 ]
2. ( M i  V M 2 )  X  M 3  | M 3 M 1  V M 3 M 2 5
(v) 
1. M ! M 3 x  M 2 M 3  | M 3;
2. M i M 3 X  M 2 I M 2 M 3 ]
3. M i  Â± M 2M 3 \ M i M 3]
4. M i  X  M 2  | M 1 M 3  V M 2 M 3 ;
5. (A^i V .M2) X A^3 I M 1 M 3  V M 2 M 3 .
P r o o f. A ll equivalences stated  in the proof come from  Theorem  2.2.10 and, 
unless otherwise stated , asserted but unproved conditional independences 
come from  Theorem  4.3.3.
(a) (i) is equivalent to (ii) since M 3  X  M 2  \ M 3 M 2  is alw ays true.
(b ) (i) is equivalent to (iii)l along with M i  X  M 2  | M 3 V M 2 M 3 . B u t 
this is equivalent to (iii) 2 since M 3  X . M 2  I M 2 M 3  is alw ays true. Hence,
(i) 
is equivalent to (iii).
(c ) (ii) is equivalent to the following two conditional independences:
(1) 
( M i  V M 3 M 1 )  X M 2 I M 3 M 2
(2) 
M 3  X  M 2  | M i  V M 3 M 1  V M 3M 2-
B u t (1) is equivalent to (iv )l since, by C orollary 2.2.11,
M 3  X  A^2 I M 3 M 2  
=> 
M 3 M 1  X  M 2  | M 3 M 2 '
On the other hand, (2) is equivalent to (iv)2, since by C orollary 2.2.11,
A^3 X  M i  | M 3 M 1  
=> 
M 3  X  M i  | M 3 M 1  V M 3M2- 
Hence (ii) is equivalent to (iv).
(d ) (iii)l is'equivalent to (v )l along with M i  X  M 2 M 3  \ M 3  V M i M 3- 
B u t this is equivalent to M i  X  ( M 3 W M 2 M 3 )  \ M 1 M 3  since it always true

148
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
that M i  X M s  | M 1M 3 . This last conditional independence is equivalent 
to (v)3 along with
(3) 
M i  X M s  | M i M s  V M 2M 3 .
On the other hand (iii)2 is equivalent to (v)2 along with
(4) 
(M i V M s) X M 2 | M i M s  V M 2M 3 
and this is equivalent to (v)4 along with
(5) 
M s  X M 2 | M i M s  V M. 2M 3 V M i.
It can be checked that (3) and (5) are equivalent to (v)5. Therefore (iii) is 
equivalent to (v). 
â 
Note that, from Proposition 4.3.4, Corollary 4.3.5. might also be preÂ­
sented as equivalences between sets of inclusions among various projections. 
In this spirit, the next corollary refines some of the most relevant inclusions 
of projections of cr-fields implied by Corollary 4.3.5, which are viewed as 
implications of conditional independence.
4.3.6 Corollary. If M i  X  M 2 | .M3, then
(i) 
M s ( M i  V M 2) â M s M  1 V M 3M 2 ]
(ii) 
(M i V M 2)M s  C M i M s  V M 2M 3 ]
(iii) M s M 2 C (M i V M s ) M 2 C M 3 M 2 ]
(iv) M 2 (M i V M 3 ) = M 2M 3 ]
(v) 
M 2M 1 â M 2 ( M iM s ) C M 2 ( M 3M i )  C M 2M 3 .
Proof.
(i) 
Clearly M 3M 1 V M 3 M 2 C M s ( M i  V ^
2)- Combining elementary 
Property 4.3.2 (viii), and Proposition 4.3.4, we successively obtain:
M 3 (M i V M 2) C M 3 M 1 V M 3M 2 
&  
M s ( M i  V M 2) C M 3M 1 V M 3M 2
(M-i V Ad.2 ) X M-s | M.3 M .1 V M>sM.2 *

4.3. Projections of cr-Fields
149
Therefore (i) is equivalent to (iv)2 in Corollary 4.3.5.
(ii) By Proposition 4.3.4, (ii) is equivalent to
(Afi V Af 2) JL M3  | M 1 M 3  V M 2 M 3  
which is (v)5 in Corollary 4.3.5.
(iii) Theorem 2.2.1 (ii) defining conditional independence implies (iii). Note
that by Proposition 4.3.4, (iii) is actually equivalent to M i  JL M2 | M3,
i.e., (i) in Corollary 4.3.5.
(iv) Clearly, M 2 M 3  C M 2 (M i V M 3). Combining elementary Property
4.3.2 (viii), and Proposition 4.3.4, we successively obtain:
M 2 ( M i  V M 3) C M 2 M 3  
&  
M 2{M\ V M3) C M 2M 3
<=> 
( M i  V -M3) JL M2 | M 2 M3 .
Therefore (iv) is equivalent to (iii)2 in Corollary 4.3.5.
(v) Clearly Af 2(Af iAf 3) C M 2M 1 and M 2 ( M s M i) C M 2M 3 ; by CorolÂ­
lary 4.3.5(h) and (iii)2 (with permutation of M i  and .M2), we obtain: 
M i  X M2 | M 3 M 1  and M i  X M2 I M 1 M 3 .  By (iv), we already know 
that:
Afi X M.2 I M 3 M 1  
Af2(Af 1 V .M3.M1) = A^2(-MsA^i).
Therefore we conclude that M 2M 1 C M 2 (M sM i). Moreover,
M i  X  M 2 | M 1 M 3  
M 2( M  1 V M 1 M 3 )  = Af2(Â«MiAJ3),
i.e., M 2M 1 =  Af2(Af iA f3).
It is interesting to remark that (v) is therefore equivalent to 
(vbis) 1. 
Afi X  Af2 | Af2(AfiAf3);
2. 
Af 1 X Af2 | Af2(Af3Af 1). 
â 
It should be noted that in Corollary 4.3.6, Property (iii) actually implies 
that (Af 1 V M s ) M 2 C Af3, i.e., Condition (ii) of Proposition 4.3.4 and is 
therefore equivalent to (and not only implied by) Af 1 X  Af2 | Af3.

150
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
Like Corollary 4.3.6, the next theorem exhibits equalities among projecÂ­
tions implied by a double conditional independence: this gives procedures 
to simplify the computation of the minimal conditioning cr-field; furtherÂ­
more, due to its symmetric form, this theorem imposes, as a corollary, some 
structural properties on projections.
4.3.7 Theorem. If
(i) 
M i  Â±  M 2 \ M 3 and M 3 C M 1 ,
(ii) 
M i  X M 2 | M 4 and M \  C M 2 , 
then
(iii) M 3M 4 C M 1M 2 C MzMa- 
More precisely,
(iv) M 3 M 4 = ( M i M 2) H M 3 = M 3 ( M i M 2 )',
(v) 
M 1M 2 = M 3M 4 H M i  â M i ( M 3M 4)-
Proof. Since (i) and (ii) imply M i  X M 4 | M 3 , we obtain, by Corollary 
4.3.6(iii), that: M 3M 4 C M 1M 4 C M 3M 4 . But from (ii) and Corollary 
4.3.6(iv), one has M \ M \  â M 1M 2 â¢ This proves (iii). To prove (iv) and
(v), it suffices to remark that (iii) implies M 3 M 4 â M 1M 2 and to use 
4.3.2(iii) and (viii). 
â 
This theorem may be phrased as follows: the only difference between 
M 1M 2 and M 3M 4 is a matter of null sets of M \ .  Note also that, by 
symmetry, (iii) may also be expressed in terms of M 2M 1 and M 4M 3 .
The next corollary is obtained by choosing a M 3 ( = M 1M 2) and a 
M 4 ( = M 2M 1) so as to make conditions (i) and (ii) in Theorem 4.3.7 
trivial. Consequently, this corollary provides structural properties of the 
projections.

4.3. Projections of <r-Fields
151
4.3.8 Corollary. For any sub-c-fields M i  and M 2  of M
M 1 M 2  =  { M i M 2) M2  =  
=  (.M 1 M 2 X M 2 M 1 ). 
â¢
This corollary, along with the elementary Properties 4.3.2, imply that, 
regardless of how big they are, expressions involving only projection operaÂ­
tions of two sub-cr-fields M i  and M 2 may always be simplified into either 
M 2M 1 0 1  M \ M 2 - We now turn to a further structural property of the proÂ­
jection of cr-fields, namely, that if M 3  C M i  and m E [A4i]+ , ( M2  V M 3 ) m  
will depend only on countably many conditional expectations 
with
rrii E [A4i]+ , and on A^-measurable functions.
4.3.9 Theorem . If M 3  C M i  then:
(i) 
( M2 V M f ) M  1 C ( M 2 M 1 )  V M3]
(ii) 
M 1 ( M2  V M 3 )  =  ( M 1 M 2 )  V M3 .
Proof. Clearly, M 3  C M i  implies that M 2  JL M 3  | M i .  Therefore by 
Corollary 4.3.6(i) and (ii), we successively obtain
M i ( M2 V M.3) 
â 
M 1 M 2  V M 1 M 3  
( M2  V M 3 ) M i  
C 
M 2 M 1 V M 3 M 1 .
Since M 3  C M i ,  M 3 M 1  = A<3, by elementary Property 4.3.2(h), and 
M 1M 3 = M 3 D AJi = M 3 V (A^i fl M f )  by elementary Property 4.3.2(iii) 
and Lemma 2.2.5(iii). The proof is completed by noticing that, by elemenÂ­
tary Property 4.3.2(i), M i  fl Mo C M i M 2 - 
â 
It is sometimes interesting to identify a minimal conditioning cr-field 
conditionally on which two cr-fields M i  and M 2 are independent in a reÂ­
strained class of such cr-fields. We may require, for instance, that it contains 
a third cr-field M3 .  A solution to this problem is given in the next propoÂ­
sition.
4.3.10 Proposition. For any sub-cr-fields M 1, .M2, and .M3:
(i) 
M i  X  .M2 | ( M2  V Ms ) ( M.  1 V M3)]

152
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
(ii) 
M 3 C M 4 C M 2 V M 3 and M i X M 2 | M 4 
=> {M2 V M 3) { M 1 V M3) C M 4-
Moreover
(M 2 V M 3)(M i V M 3) = {{M 2 V M 3)M i} V M 3. 
â 
Another interesting feature of projections of cr-fields is that they simplify 
the search for conditional independence, since it suffices to check the desired 
conditional independence on smaller cr-fields. More precisely:
4.3.11 Theorem . Let M i  and M2 be sub-<r-fields of M .  Then for any 
M 3 C M i, M 4 C M 2, M s  C M i :
(i) 
M s  X  M 2 M 1  | M 4 V M s  <=> M s  X M2 | M 4 V Ms]
(ii) 
M3 X  M 1 M 2  | M 4 V M 5 
M 3 X M 2 I M 4 V Ms- 
Moreover
(iii) M i X  M 2 I M 4 V M s  O  M 1 M 2  X M 2M i | M 4 V Ms-
Proof. Since M i X  M 2 | M iM 2 and M i X M 2 | M 2M i, always hold 
by Theorem 4.3.3, we obtain from Corollary 2.2.11(ii), that
M 3 X M 2 I M 1 M 2  V M 4 V M s  
M s  X M 2 I M 2M 1 V M 4 V M 5.
So by Theorem 2.2.10,
M s  X  M 2M i I M 4 V M s  
<=> 
M 3 X (M 2 V M 2M i) | M 4 V M 5,
and this is equivalent to M s  X M 2 | M 4 V M 5. Similarly
M 3 X M iM 2 I M 4 V M 5 
<=> 
M 3 X  (M 2 V M iM 2) I M 4 V M 5,
and this implies M 3 X  M 2 | M 4 V M 5 . To complete the proof it suffices 
to remark that (iii) is obtained by symmetrizing (i). 
â 

4.4. 
Minimal Sufficiency 
153
4.4 
Minimal Sufficiency
In this section, we use projections among cr-fields to construct a minimal 
sufficient statistic and a minimal sufficient parameter.
4.4.1 
M inim al Sufficiency in U nreduced and in M arginal E xperÂ­
im ents
We now apply the results of Section 4.3 to the ufireduced Bayesian 
experiment â¬ = Sa v s â¢ Minimality with respect to set-inclusion requires 
some care; this fact inspires the following definition:
4.4.1 D efinition. A paramete? B C A  is E-minimal sufficient if and only 
if:
(i) 
B is ^-sufficient (i.e., A  X S  \ B)\
(ii) 
C C A  and ^-sufficient 
=> 
B C C C\A. 
â 
Remember that B fl A  is the smallest sub-cr-field of A  containing B and 
all the null sets of A  while B is, in general, not a sub-<r-field of A . Next, from 
Definition 4.4.1, two sub-cr-fields of A  can be ^-minimal sufficient paramÂ­
eters even when one is strictly contained in the other one. This motivates 
the following definition.
4.4.2 D efinition. A parameter B C A, is A-compleie if B fl A  = B, i.e., if 
B contains all the null sets of A. 
â 
It is clear that two *4-complete ^-minimal sufficient parameters coincide. 
Hence the elementary Property 4.3.2(i) and Theorem 4.3.3 guarantee the 
existence of a unique .4-complete ^-minimal sufficient parameter.
4.4.3 P roposition. In the Bayesian experiment Z = {A x S', *4 V Â«S, n} 
the parameter A S  is the unique w4-complete ^-minimal sufficient 
parameter. Therefore a parameter B C A  is ^-minimal sufficient if and only 
if B D A  = w4Â»S. 
â 
By duality we obtain the same concepts on the sample space.

154 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.4.4 D efinition. A statistic T  C S  is Z-minimal sufficient if:
(i) 
T  is Â£ -sufficient (i.e., A  JL S  | T);
(ii) 
U C S  and Â£-sufficient T  C U  OS. 
u
4.4.5 D efinition. A statistic T  C S  is S-complete if T  fl Â«S = T, i.e., if T
contains all the null sets of S. 
u
4.4.6 P roposition. In the Bayesian experiment Â£ = {A x S', *4 V Â»S, 11} 
the statistic S A  is the unique 5 -complete ^-minimal sufficient statistic. 
Therefore a statistic T  C S  is Â£-minimal sufficient if and only if 
T n S  = SA. 
â¢
An identical analysis of minimal sufficiency in the marginal experiment 
Â£bs/T can be done by substituting B to A  and T  to S. So, in Â£b v t, B T  is the 
B-complete Â£bwT-^inimal sufficient parameter and T B  is the T-complete 
Â£evT-minim al sufficient statistic.
4.4.2 
Elementary Properties of Minimal Sufficiency
The search of the Â£-minimal sufficient parameter and statistic may be 
simplified if we already know a sufficient parameter B and a sufficient statisÂ­
tic T  using Theorem 4.3.7.
4.4.7 P roposition. If B is an ^-sufficient parameter and T  is an ^-sufficient 
statistic, then:
(i) 
B T  C A S  C W ]
(ii) 
T B  C S A  C TB. 
m
If we modify the prior distribution p in p! in such a way that pf <C p^ 
we have an inclusion among minimal sufficient sub-cr-fields.
4.4.8 T heorem . Let Â£ and Â£' be two Bayesian experiments on 
(A x S ,A  V S) defined by their probabilities II and II'. If

4.4. 
Minimal Sufficiency 
155
(i) 
ir <  n,
cHI' 
,
(â â ) 
j n Â« n r .
then the 5/-minimal sufficient parameter (resp., statistic) is included in the 
5-minimal sufficient parameter (resp., statistic) completed by the null sets 
with respect to II' of A (resp., of 5).
Proof. Let us denote by X* the null sets of A V 5  with respect to II' and 
by (AS)*/ the projection of 5 on A with respect to II'. Under (ii), by 
Corollary 2.2.15, A JL S  \ (>15)*;II implies A JL S \ (AS)*;II'. Therefore, 
(AS)*/ C (-45)* V (A n 1*) by Theorem 4.3.3(ii). By symmetry, 
(5.4)*/ C(5.4)* v ( 5 n f ' ) .
4.4.3 
Minimal Sufficiency in a Dominated Experiment
In a dominated Bayesian experiment Â£ = (A x 5, A V 5, II) with 
II Â«  /i 0  P  the search for the minimal sufficient parameter and the minimal 
sufficient statistic may be considerably simplified by looking at measurabilÂ­
ity properties of the density gAvs = dU/(dfi 0  P).
4.4.9 Theorem . In a dominated Bayesian experiment 
Â£ = 
x S ,4 V 5 ,II} , a parameter B C A  and a statistic T  C 5  are 
5-sufficient if and only if gAvs Â£ B\/ X ^ P .
Proof. If gAvs G 5  V T, then gAvs G B V 5  and gAvs G A V T  and, by 
Proposition 2.3.9, this is equivalent to saying that B is 5-sufficient and T  
is 5-sufficient. However, by Proposition 2.3.8(i) and (iii), B and T  are 
5-sufficient if and only if gAvs = flUvr and gAvs = g&vs- Now, since 
.4 X 5; //0 P  implies (*4VT) X (flV5) | flVT; //0 P  we have, successively,
gsvT 
= 
(fl V T)(gAvs) = (A V T)(gAvr) = (fl V 5)(<juvr )
= (fl v S)(gsvs) = g&vs = gAvs- 
Hence, gAvs = g&vT a.s.// 0  P. 
â 
This result provides the following characterization of the 5-minimal sufÂ­
ficient parameter and of the 5-minimal sufficient statistic.

156
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.4.10 Proposition. Let Â£ = {A x 5, A  V5, II} be a dominated Bayesian 
experiment. Then:
(i) 
9 As/S e A S  V S A 11Â®*3;
(ii) 
If gAvs G B V T ^ ,  
where B c A  
and T  C S;
then A S  C B C \A  
and 
S.4 C T  fl S. 
â¢
4.4.4 
Sampling Theory and Bayesian Methods
Let us consider a statistical experiment Â£ = {(5,S)]Pa 
a E ^4} and 
A  a cr-field on A which makes P a( X ) measurable for any X  E S. We use 
the notation introduced in Section 2.3.7. In sampling theory, a minimal 
sufficient statistic is known to exist only in the dominated case, whereas it 
always exists in a Bayesian experiment (see, e.g., Barra (1971), Chap. II,
Section 5; see also Pitcher (1957, 1965) . Indeed, if 
is a version of
dPa jdPi, where P* is a privileged dominating probability, it is known that
 p
T  is a sufficient statistic if and only if g% â¬ T  * V a E  A. Therefore if
 P
S\ = <t{gÂ£ : a E A}, Si is minimal sufficient in the sense that Si C T 
V T  sufficient.
4.4.11 T heorem . If p is a regular probability on (A, A) in the sense of 
Section 2.3.7, then the Â«S-complete (with respect to II) ^-minimal sufficient 
statistic is equal to Si completed by the null sets of S  with respect to P*, 
i.e., S A  = S i* .
Proof. By Theorem 2.3.12, S A  C 
HÂ«S. Now, if p is regular this implies 
that the predictive probability P  is equivalent to any privileged dominating 
probability P*(P ~  P*) and so 
fl S = c>f = Â«sf*. However, by TheoÂ­
rem 2.3.14, Si C S A P*. But, as before, S A P* = S A ^  fl S = SA, and so 
Sf* = SA. 
m
In a sampling theory framework, minimal sufficiency on the parameter 
space is introduced as follows (see, e.g. Barankin (1961)). For any X  E S, 
let A x  be the cr-field on A generated by the mapping a â âº P a(X) and 
let Ai = Vxe s  A x  â¢ Then A i is the minimal sufficient parameter, i.e., the

4.4. 
Minimal Sufficiency
157
smallest sub-<r-field of A  which makes the sampling probabilities P a(X) 
measurable V X  E S. We then have the following result.
4.4.12 Theorem . For any prior probability /i on (A, A) the ^-complete 
(with respect to II) ^-m inim al sufficient parameter is equal to A\ comÂ­
pleted by the null sets of A  with respect to /i, i.e., A S  = A i .
Proof. With the notation of Section 2.3.7, As = Xas a.s.II V s G [S]+- 
Hence Ai C A S U fl A  = AS. However, As E A f  C\A Vs E <S+ , i.e.,
AS cAâ¢nA = Ai-
4.4.5 
Minimal Sufficiency in Conditional Experiment
Minimal sufficiency in a conditional experiment Â£ ^ r  requires some 
care because, in general, the intersection of two Â£ j^r -sufficient statistics 
need not be ^ ^ r -sufficient; indeed, although Mi C Af V T  
(i = 1,2) and 
B JLT \ Mi V Mi do imply, by Theorem 2.2.12, that
B Â±  T  | (Ml v M i) n (M  V jV2),
it should be stressed that (M  VMi) fl (Mi V M2) does not have the form 
Mi V (M\ n M 2)- This is precisely why the attention is limited to strong 
Â£ j^T-statistics Mi, i.e., Ml C Mi C Ml V T.
We first note that Theorem 4.3.9(ii) implies that for any Ml:
(4.4.1) 
(M  V T)(B V M )  = [(Ml V T)B\ V Ml;
in other words, the projection of BVMi on Mi VT is a strong Â£ j^ r -statistic. 
Now Proposition 4.3.10 provides, through the next proposition, the natural 
construction of minimal sufficient (r-fields in conditional experiments.
4.4.13 Proposition. In the conditional experiment
(i) 
(Ml VT)(BVMi) is the sufficient statistic which is minimal among the 
(Mi V T)-complete strong Â£g^r -statistics;
(ii) 
(B V Ml)(Mi V T) is the sufficient parameter which is minimal among 
the (B V wM)-complete strong Â£ j^r -parameters. 
â 

158 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.4.6 
Optimal Mutual Sufficiency
If we wish to identify the âoptimalâ joint reduction of a given experiÂ­
ment, only mutual sufficiency will be considered, due to the non-uniqueness 
and the nonoperational character of maximal ancillarity; indeed, the strucÂ­
ture of mutual exogeneity and of Bayesian cut both involve ancillarity conÂ­
ditions.
We first note that if (B , T ) is mutually sufficient we may, fixing 5, 
optimize T  or, fixing T, optimize B.
4.4.14 T heorem . If (B,T) are mutually sufficient, then
(i) 
(B,SB) are mutually sufficient and SB C T  fl S;
(ii) 
(.4T,T) are mutually sufficient and *4T C B D A.
Proof. The mutual sufficiency of (#,T ) is equivalent to 
A  JL T  | B
and B JL S  \ T. Clearly, B X S  \ SB. Now, B JL S  \ T  is equivalent to 
SB  C T, and then A  JL T  \ B implies A  X SB | B , i.e., (B,SB) are 
mutually sufficient. Similarly, (*4T,T) are mutually sufficient. 
â 
Simultaneous optimization on B and T is not relevant without constraint
since the solution is trivial, i.e., X fl A  and X fl Â«S.
Now if B represents a parameter of interest, it is not always possible to 
exhibit a statistic T  such that B and T  are mutually sufficient; in particular, 
B and S are not mutually sufficient unless B is sufficient. It is therefore 
meaningful to look for a pair of cr-fields B\ and T  such that B C B\ and 
B\ and T are mutually sufficient. Clearly, A  and S  would satisfy these 
conditions but one would like to exhibit a pair of smallest ones. This is the 
goal of the next theorem:
4.4.15 Theorem . Let B be a sub-cr-field of A  in a Bayesian experiment S. 
Then there exists a couple of a-fields B\ C A  and T\ C S  such that:
(i) 
B C Bi ;

4.4. 
Minimal Sufficiency 
159
(ii) 
B\ and 71 are mutually sufficient;
(iii) C and T  mutually sufficient and B C C imply B\ C C fl A  and 
Ti C T  n <S.
The pair (S i,71) will be called B-opiimal mutually sufficient 
Proof. Let us consider:
$ = {{cm  \ c  = c d A d  auwbm
 = u n S D  sc}.
Note that 
is not empty because (-4,5) G 4> and that each pair (CM) i*1
<3> are mutually sufficient by Theorem 4.3.3. Thus we define:
$ 4  = {C | 3 U : {CM) â¬ 
Bi = fl{C | C G $*}
*5 = {W | 3 C : (C,Z/) G $} 
Ti = n{ZY | U G $s}.
Clearly, Si = Si fl*4, B C Si and 71 = T i OS. We only have to show that 
(S i,71) GÂ®. Indeed,
V C G ^  
3 U 
A T x C A U c C .
Hence ATi C B \. Similarly SSi C 71. 
â 
Note that the proof does not give a construction of Si and 71. However, 
according to Theorem 4.4.14, Si and 71 must obey some relations which 
may be helpful in searching for them.
4.4.16 Corollary. If (S i,7i) are S-optimal mutually sufficient, then:
(i) 
T i= S S i;
(ii) 
Si = S V A T i. 
â 
In applications, it often occurs that the determination 
of Si and 71
is facilitated due to the following argument: If S is the parameter of 
interest, then 71 = SB is the f^-minimal sufficient statistic. 
Now 
Si=^4Ti=^l(SS) is the Â£ri-mmimal sufficient parameter. If it can be proved 
that S C Si and Si X S  | 71, then (Si, 71) are the S-optimal mutually 
sufficient <r-fields. Indeed, if {CM) is mutually sufficient with S C C, then

160
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
C X S  | U implies B JL S  \U, and therefore, T\ = SB C U fl S. By the same 
token, A  Jl U \C implies A  X 7i | C, and therefore, B\ â AT\ C C fl A.
Note that B\ is equal to B if and only if there exists T  C S  such that 
(5, T) is mutually sufficient. But this is not generally true, as illustrated 
by the following example.
Exam ple. Consider again the example provided in Section 3.4.1, i.e., x = 
(y, z)' and
(x | a) ~  ^[(sinyjjcosyj)',?;-1^]
with
a = (<P, n) e A  = (â7r, 7r] x 27# .
Let <p JL r], where r) follows a gamma distribution and <p follows a uniform 
distribution. Recall that rj JL x \ r2 and a JL r 2 \ r] since (r2 | a) ~  âxK 7?)* 
Now, if B = <t{t7}, and S = <r{x}, 71 = SB = <r{r2}, and Bi = ATX = 5. 
Therefore, since B\ X S  | 71, by the remark following Corollary 4.4.16 it 
follows that (77, r 2) is 77-optimal mutually sufficient. 
â 
4.5 
Identification among (j-Fields
In a Bayesian experiment, the sample information bears on a sufficient 
parameter only: conditionally on a sufficient parameter, the Bayesian exÂ­
periment is totally non-informative. This raises the following question: why 
should one introduce a cr-field larger than the minimal sufficient parameter, 
i.e., the smallest cr-field that makes the sampling probabilities measurable? 
In other words, why should A  be different from AS? In nonexperimen- 
tal fields (such as econometrics â see, e.g., Dreze (1974), Fisher (1966), 
or Hannan (1971)) such models are often introduced. There, it is stanÂ­
dard procedure to introduce redundant parametrization as an early stage of 
model building or as a support for relevant prior information or because the 
parameter of interest (making e.g., the loss function measurable) is larger 
than the minimal sufficient parameter. In experimental fields, it may be the 
case that the experimental design will not provide information on all the 
parameters of a theoretically relevant model.
A model for which the parametrization is redundant (i.e., A S  is strictly 
contained in A) will be called non-identified. Otherwise, i.e., if A S  = A,

4.5. 
Identification among a-Fields
161
the model will be said identified. The object of the next section is to study 
the relations existing between A  and AS. We first present the abstract 
probability theory underlying the statistical analysis of identification that 
is presented in Section 4.6.
As in Sections 2.2 and 4.3, we handle a topic of general probability 
theory and so adopt the same notation; in particular, (M, A f,P ) is an 
abstract probability space.
4.5.1 D efinition. Afi is identified by M 2 conditionally on M 3 or, equivÂ­
alently, M 2 identifies M i  conditionally on M 3 if
(Afi V Af3)(Af2 V A ^) = M i  V M 3 .
In this case we write Afi < Af 2 | Af3. If M 3 = Mq we say M i  is identified 
by Ad25 and write M i  < M 2 - If we want to make explicit the role of the 
probability P, we write M 1 < Af2 I Afs; P and Af 1 < Af2 5 P . 
â 
Note that, by Definition 4.5.1, M i  < M 2 | Af3 is equivalent to 
(Afi V M 3 )  < { M2  V Afs). Consequently, theorems stated conditionally 
on A^3 need be proved for Af3 = Afo only.
4.5.2 Proposition. Elementary Properties of Identification
(i) 
Afi < Af2 | Afs 
4=^ M i  < M 2 | A^3 
Afi < Af2 | Af3
Afi < Af2 | Af3;
(ii) 
Afi C Af2 V Af3 => Afi < Af2 | Afs;
(iii) Af2 C Afi V Af3 and Afi < Af2 | Af3 => Afi V Af3 = Af2 V Afs;
(iv) Af2 C Af4 V Af3 and Afi < Af2 | Af3 => Afi < Af4 | Af3;
(v) 
(Afi V Af4) < Af2 | Af3 => Afi < Af2 | Af3 V Af4
=> Afi < (Af2 V Af4) | Af3-
Proof. Only elementary Property (v) requires a proof. Take Af3 = Afo- 
Clearly, by elementary Property (iv), Afi V Af4 < Af2 implies that:
Afi V Af4 < Af2 V Af4, 
i.e., 
Afi < Af2 | Af4.

162
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
Now, if M \  V Af4 < Af 2 V Af4, then
Afi C Afi VAf4 = (Afi V Af4)(A<2 V Af4) C Af4 V A fi(A f2 V Af4)
by Theorem 4.3.9(i). Next, M i  X (Af2 V Af4) | A fi(A f2 V Af4) implies, 
by Corollary 2.2.11 and Corollary 2.2.7, that
Afi X {Af4 V A fi(A f2 V Af4)} | A fi(A f2 V Af 4).
Therefore, Afi X Afi | Af i(A f2 V Af4) and by Corollary 2.2.8, this is 
equivalent to Afi C A fi(A f2 V Af4) and, therefore, Afi = A fi(A f2 V Af4) 
by elementary Property 4.3.2(viii), i.e., Af i < Af2 V Af4. 
â 
Note that elementary Property (iv) amounts to saying that the idenÂ­
tifying cr-field can be increased. However, the identified cr-field cannot be 
decreased: i.e., Afs C Afi V Af3 and Afi < Af2 | Af3 does not imply, in 
general, that M s  < M 2 \ M 3 . Indeed, consider the following example. Let 
{Ai : i = 1,2,3,4} be a measurable partition of M  such that P(A;) = 
take Afi = cr{Ai U A3, A2, A4}, Af2 = g {Ai U A2, A3 U A4}, Af3 = Afo 
and Af 5 = cr{A\ UA3, A2UA4}. Clearly, Af 1 Af 2 = Afi but M 5M 2 = Afo-
The next two theorems and their corollaries examine the relationships 
which link identification and conditional identification under some condiÂ­
tional independence conditions. In essence, the first theorem states the 
conditions under which identification implies conditional identification. The 
second theorem states conditions under which the identifying cr-field may be 
reduced and, as a corollary, it states when conditional identification implies 
identification.
4.5.3 Theorem . Let Af; (i = 1,2, 3,4) be sub-cr-fields of Af. If
(i) 
M 4 X  Af2 | Afi VAf3,
(ii) 
Afi < Af2 | Af3,
then
(iii) Afi < Af2 | Af3 V Af4.

4.5. 
Identification among a-Fields
163
Note that (i) is trivially satisfied if 
C M i  V M 3.
Proof. Take M 3 = M o . Now, by Theorem 4.3.9 (ii),
(M i V M a ){M 2 V M 4) = AT4 V (M i V
Under (i), by Corollary 4.3.6(iii), (A4i V M 4 ) M 2 = M 1M 2 and, by (ii), 
M 1M 2 = M i . Therefore
(M i V M a ) ( M 2 V AI4) = A4i V .M4,
and the result follows from elementary Property 4.3.2(viii). 
â 
4.5.4 Theorem . Let M i (i = 1, 2, 3,4) be sub-<7-fields of M . If
(i) 
M 4 i  M i  | M 2 V M 3 ,
(ii) 
M 1 < ( M 2 V M 4) \ M 3j
then
(iii) M i  < M 2 | -M3.
Note that (i) is trivially satisfied if .M4 C M 2 V M 3.
Proof. Take M 3 â 
M
o â¢ 
Under (i), .M i(.M2 V Ad 4) = 
M
1M 2 by 
Corollary 4.3.6(iv). But, under (ii), M \ ( M 2 V .M4) = M i  and, therefore, 
M 1 M 2  = M i ,  i.e., M i  < M 2. 
â 
4.5.5 Corollary. Let M i (i = 1, 2, 3,4) be sub-cr-fields of M . If
(i) 
M 4 JL-Mi | M 2 V M 3.
(ii) 
M i  < M 2 | M 3 V M 4,

then
(iii) M i < M 2 | M 3 -
Note that (i) is trivially satisfied if M 4 C M 2 V M 3.
Proof. Indeed, by elementary Property 4.5.2(v), M i  < M 2 \ M 3 V M 4 
implies M 1 < (M 2 V M 4) | .M3, and the result follows from Theorem
4.5.4. 
â 
Combining Theorems 4.5.3 and 4.5.4 together with Corollary 4.5.5 enÂ­
tails the following corollary.
4.5.6 Corollary. Let M i(i = 1, 2,3,4) be sub-cr-fields of M .  If
(0) 
M 4 JL (M i V .M2) | M 3
the following identification properties are equivalent:
(1) 
M i < M 2 | .M 3;
(ii) 
M i  < (A^2 V M 4 ) | .M3;
(iii) M i  < M 2 | .M3 V M 4 - 
â 
Restricting the conditioning cr-field in Theorem 4.5.4 to be the minimal 
one we obtain, as a corollary:
4.5.7 Corollary. Let M s  C M i , M e  C A^2- Then, for any M 4 C .M2,
(i) 
M 4 <  M i | Ms V Me ^  M 4 <  M 1M 2 | Ms V Me\
(ii) M \ <  M i | Ms V Me => M 4 < M 2M 1 | Ms V Me-
164 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency

4.6. 
Identification in Bayesian Experiments
165
Proof. By Corollary 2.2.11 and Theorem 4.3.3, we successively have:
M \  JL M .2 | M 1M 2 V M.e V M e
M \  X M 2 | M 2M x V M e V M.Â§.
Therefore MÂ± < M\  | M e  V M e  implies M a < M 1 M 2  | M e  V M e  
by Theorem 4.5.4, and the reverse implication is trivial. On the other
hand, by elementary Property 4.5.2(iv), M a < M i  \ M e  V M e  implies
M a < M i  V M 2 M 1  | M e  V M e  and, by Theorem 4.5.4, this implies 
M a < M 2 M 1  | M e  V M q. 
â 
4.6 
Identification in Bayesian Experiments
4.6.1 
Identification in a Reduced Experiment
Let us consider a general experiment 
where B C A, T  C S  and
M  C A V S. We have seen, in Proposition 4.4.13, that the minimal sufficient 
(B V.M)-complete strong fj^^-param eter is given by (B VM ) ( M  VT). In 
line with the introduction to Section 4.5, we are lead to provide the following 
definition of identification.
4.6.1 D efinition. The experiment S ^ r  is identified if B < T  \ M , i.e., if 
(B V M ) ( M  V T) = B V M . In particular, the unreduced experiment Â£ is 
identified if A  < <S, i.e., if A S  = A. 
â 
More generally, we may introduce the concept of identification for an 
^fivr"Parameter) i,e * j a sub-cr- field C C B V M .
4.6.2 D efinition. C is an Â£ ^ r -identified parameter if the experiment
is identified, i.e., C < T  | M  or ( C V M ) ( M V T )  = C V M . In particular, B 
is an Â£ -identified parameter if Â£&vs ls identified, i.e., B < S  or BS = B. 
â 
Note that identification appears as a problem of minimal sufficiency on 
the parameter space rather than on the sample space. This formal analÂ­
ogy has already been remarked among others by Barankin (1961), Kadane 
(1974) and Picci (1977).

166
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
Note also that the concept of identification makes this property depenÂ­
dent on the prior probability through its null sets (see the remarks following
4.3.1 and 4.3.2); the following example illustrates this fact.
Exam ple. Let A = 1R, A  be the Borel sets of JR. 
Let us assume 
(xi | a) ~  z.7V(|a|, 1) and s = (a?i, X2 , ..., xn)'. If p is a prior probability 
then, clearly, A S  = Ai where Ai = cr{|a|} = {E Â£ A  : â E â E}. If p is 
equivalent to the Lebesgue measure, the experiment is not identified but it 
will be identified if p is equivalent to the Lebesgue measure on 1R+ and if 
p(lR~) = 0. 
â 
From the elementary Properties 4.5.2(iv), we obtain the following result.
4.6.3 Proposition. The experiment S j^ r  is identified if and only if there 
exists an Â£ j^r -statistic Af C M  V T  such that Â£$tj{ is identified. 
â 
From the remark following 4.5.2, note that even if 
is identified, a 
subsequent marginalization on a statistic AT, 
or on a parameter Â£,
need not be identified. This is obvious for 
This is even more
striking for 
but recall that the sampling probabilities of an experiÂ­
ment marginalized on a parameter are obtained by averaging the sampling 
probabilities; this integration may evidently lead to the loss of identification. 
This problem has received a good deal of attention in particular applicaÂ­
tions in which some additional assumptions guarantee the identification of 
marginal models (see, e.g., Elbers and Ridders (1982) for such results in 
proportional hazard models). In particular, a sub-parameter of an identiÂ­
fied parameter is not necessarily identified. We have already provided one 
counterexample (following 4.5.2); here is another:
Exam ple. Let s = (a?i,..., zn), Xi = (xi*,^*)', a = (ao, a\ , ..., an, E) where 
E is a positive definite 2 x 2  symmetric matrix. Suppose
and let (ao, E) have a prior distribution equivalent to the Lebesgue measure. 
This model is clearly identified. If we marginalize this model on (a0, E), we

4.6. 
Identification in Bayesian Experiments
167
obtain
(xi | a0,E) ~  i.N2
(T il -f 1 
cr 12 +  Cto
cr12 H" a0 
^22 + a0
This marginal model is not identified.
It is interesting to remark that the identification of C in 
depends
crucially on 
If (a* | ao>]C) were nÂ°f normal in the above examÂ­
ple, identification would not be lost in the marginal experiment (see, e.g., 
Reierspl (1950)).
We now turn to the relationship between sufficiency and identification.
On the parameter space, a sufficient parameter is not necessarily idenÂ­
tified; but it is identified if and only if this parameter is almost surely equal 
to the minimal sufficient parameter.
4.6.4 Theorem . An Â£[3^ -sufficient parameter, Â£, is identified if and only 
if C V M  is almost surely equal to the minimal complete strong S ^r - 
sufficient parameter, i.e.,
Z V M  n (B V M ) = (B V M){M  V T).
Proof. C is Â£^Jr -sufficient is equivalent to {B yM ) X {M VT) | A4VÂ£. By 
Corollary 4.3.6(ii), (Â£ V M ) { M  V T) = {B V M ) { M  V T). If Â£ is identified, 
(Â£ V M ) { M  V T) = C V M .  Therefore, by elementary Property 4.3.2(viii), 
we obtain the result. However, if C V M  = {B V M ) ( M  V T), C is clearly 
sufficient and, by the elementary Property 4.3.2(vii) and Corollary 4.3.8, 
C V M  is identified as is C by elementary Property 4.5.2(i). 
â 
As we have seen, an Â£3^ r -parameter C contained in {B V M){M. V T) 
is not necessarily identified. Conversely, an Â£ ^ 7-identified parameter C is 
not necessarily contained in {B V M ) ( M  V T).
Exam ple. 
Let S = {^1,^2} and A = {ai,Â« 2jÂ«3}- Take
p(Â«i | ai) = 
i ,
p(Â«i I a2) -  
p(s1 | a3) 
-  i .

168
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
Then if //(a,-) > 0,2 = 1,2,3, we obtain
A S  = Bi = a({ai}, 
{a2,a 3}).
If we take B2 =  ^({a 1,02}, {<*3})) then B2S  = B2 and, clearly,
B2 <tBi = AS. 
m
On the sample space a sufficient statistic permits the verification of 
identification.
4.6.5 T heorem . In 
a parameter C is identified if and only if C is
identified by a sufficient statistic J\f.
Proof. B I T  | M V Af, since Af is sufficient, and this is equivalent to 
( B y M ) X ( M V T ) I MyAf. This implies that ( C y M ) X (Af VT) | My Af , 
and the result follows from Theorem 4.5.4. 
â 
As a simple corollary to Theorem 4.5.3 and Corollary 4.5.5, we obtain 
the next proposition which renders explicit the relationship between identiÂ­
fication of an experiment and identification in a further conditioning of this 
experiment.
4.6.6 Proposition.
(i) 
If S $ T is identified,
then Â£Â§^7 * is identified for any Â£ ^ r -parameter Â£;
(ii) 
If Â£jÂ£r/^ is identified for some Â£ ^ r -statistic Af,
then Â£ $ r  is identified. 
â 
When an experiment is not identified, a basic objective of the theory 
of identification is to look for a parameter such that the experiment condiÂ­
tioned on this parameter is identified. When this parameter represents exÂ­
act restrictions, they are called âidentifying restrictionsâ (see, e.g., Kadane 
(1974)). Hence the following definition:

4.6. 
Identification in Bayesian Experiments
169
4.6.7 D efinition. An Â£j^7'-parameter C is identifying for 
if S&fr* 
is identified. 
â 
Exam ple. Let s = ( z i,... , #â) and a = (ai, 02) . Suppose that the prior 
probability p is equivalent to the Lebesgue measure on M2 , and that 
(x{ | a) ~  i.N(a\ -f a2,l). This experiment is clearly not identified. The 
cr-field B generated by the linear function b = Ai<zi + A2G&2 (Aj known, 
j = 1, 2) is identifying unless Ai = A2. Indeed let C be the cr-field generated 
by ai +<22. Clearly, A S  = Cf)A and B\lC = A. Thus, by Theorem 4.3.9(ii), 
A ( S M B ) = B M A S  = A. 
â¢
Let us now look at the robustness of identification for a proper modificaÂ­
tion of the prior probability. Consider S and Â£', two Bayesian experiments 
on (A x S', AM S) defined by their probabilities II and II'. If II' <C II and 
dU'/dll E [A]+, and if Â£' is identified, we cannot conclude that Â£ is idenÂ­
tified because Theorem 4.4.8 only implies that A  is equal to A S  completed 
by the null sets of A  with respect to II'; this equality still involves II' since 
the class of null sets with respect to II' is larger than the class of null sets 
with respect to II in general. This is not so if the two prior probabilities are 
equivalent.
4.6.8 P roposition. Let Â£ and Â£' be two Bayesian experiments defined 
on (A x S ,A  V S) by their probabilities II and II'. Let B C A, T  C S, 
M C  AM S. If
(i) 
ir ~  n,
(ii) 
^
 â¬ [F v a TJ+ ,
then 
is identified if and only if Â£ ^ 7- is identified. 
â 
Even though the problem of identification is usually studied with referÂ­
ence to the parameter space, we can take advantage of the complete duality 
of the parameter space and the sample space of a Bayesian experiment to 
define the concept of an identified statistic:

170
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.6.9 D efinition. J f is an 
identified statistic if J f  < B \ M , i.e.,
( M V J f ) ( B V M )  = M V  Jf. 
m
This definition may be useful when searching for the minimal sufficient 
statistic, as illustrated by the following proposition which is the dual of 
Theorem 4.6.4.
4.6.10 P roposition. An Â£j^r -sufficient statistic J f is identified if and 
only if M  V J f is almost surely equal to the minimal complete strong S ^ r - 
sufficient statistic, i.e.,
M  W Jf H (M  VT) = ( M V T ) ( B V M ) .  
â¢
We conclude this section by studying the relationship between joint 
reductions and identification. There is not much to say in the case of mutual 
sufficiency and mutual exogeneity. Indeed, even if the unreduced experiment 
is identified, the definitions of mutual sufficiency and of mutual exogeneity 
do not ensure any minimal character to the parameter of interest.
The situation is quite different for the cut. Let S â {A x S', A  V 5,11} 
be an unreduced Bayesian experiment, and suppose that B C A, C C A  and 
T  C S operate a cut. If the unreduced model is identified, i.e., A  = ByC and 
(.ByC)S = ByC, the next theorem will show that SgvT is also identified, i.e., 
B T  = B , but, in general, Sjvs need nÂ°f t>e identified, i.e., [C VT)S ^  C VT 
because the minimal 
-sufficient parameter is not necessarily uniform. 
Consider, for instance, the following example:
Exam ple. Let s = (a?i, z2, â¢ â¢ â¢ > xn) with Xi â (xu,X 2i)f. 
Let also 
a = (ai,a2>. . . , a n,<Tii,(712, 022) with a{ = (6f,c,-)/. 
Suppose that 
(xi | a) ~  i.JV2(at',E) and that the prior probability on a is equivalent to 
the Lebesgue measure. Clearly, A S  = A  (the 2 n -f 3 components of a are 
identified). Now, (xi; | a) ~  z.iV(6i,crn) and (x2i | u,Â£i) ~  i.N[mi,d\ 
where 
= ct- -f (0â12/ ^ 11)(Â®ii -  h) and d = cr22 â (cr\2/ a n). Define 
T  = <T{xn ,...,xiâ}, B = <r{bi,...,bâ,<rn }, C = <r{ci -  (<ri2/cr11)61, 
-  (<Ti2/o"22)&n, ^}i and 
Af â a { m i , . . . , m n, d}. 
Clearly,
A  â B y  C and (B, C,T) operates a cut if B X C, and we also have BT = B , 
but (C V T )S  â J f V T  which is strictly included in C V T, and J f is not a 
uniform parameter. 
â 

4.6. 
Identification in Bayesian Experiments
171
4.6.11 Theorem . Let E = (A x S', A  V S, II) be an identified Bayesian 
experiment. Suppose B C A, C C A, and T  C S  operate a cut, and 
A  â BVC. Then EbwT is identified.
Proof. Since Â£ is identified, and A  = B \ t C, Z? VC < S. By the elementary 
Property 4.5.2 (v), this implies that B < C\/ S. Now, since B JL S | C V T, 
by Theorem 4.5.4, this implies B < C V T. But B 1  C and C 1  T  | B 
is equivalent to C 1  (B V T); this consequently implies that B 1  C | T. 
Therefore, by another application of Theorem 4.5.4, B < C V T  implies 
B < T .  
â 
4.6.2 
Sampling Theory and Bayesian Methods
A. Identification in U nreduced E xperim ents
In a sampling theory framework, one starts from a statistical experiÂ­
ment:
Â£ â {(5,S ) ,P a : a Â£ A}.
A widely accepted definition of identification is the following (see, e.g., 
Fisher (1966), Rothenberg (1971), LeCam and Schwartz (1960), Bunke and 
Bunke (1974) or Deistler and Seifert (1978)):
4.6.12 D efinition. The statistical experiment is s-identified (s for âsamÂ­
plingâ) if the mapping a â> P a is injective (i.e., if: a ^  af => P a ^  P a'). â 
R em ark. An equivalent definition may be stated as follows. One first introÂ­
duces an equivalence relation on S : a ~ a', read as a and a! are observation- 
ally equivalent, and defined as: a ~ a1 *<=> P a = P a . Denote the quotient 
space for this equivalence as A/s. The statistical experiment is then identiÂ­
fied if A/s =  A/o where A/o is the trivial partition: A/o =  {{a } | a E A}.
In a decision theory context, the parameter of interest is naturally inÂ­
troduced as a cr-field that makes both the loss function and the family of 
mappings a â âº P a(X), X  E S measurable. This leads to this other concept 
of identification (see, e.g., Neveu (1970), example in Section III.2).
4.6.13 D efinition. Let A  be a cr-field on A representing the parameter of 
interest. Let A\ be the smallest sub-cr-field of A  that makes the mappings

172 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
a â âº P a(X) , 1  G S , measurable (see Theorem 4.4.12). The statistical 
experiment Â£ is c-identified (c for âclassicalâ) if A  = A\. 
â 
The third definition has often been suggested as a Bayesian counterÂ­
part to Definition 4.6.12 (see, e.g., Schonfeld (1975), Deistler and Seifert 
(1978)). 
This concept requires that the statistical experiment Â£ is enÂ­
dowed with a prior probability p on (A, A) where A  makes the mappings 
a â âº P a(X )iX  E 5, measurable.
4.6.14 D efinition. The statistical experiment Â£ is a.s.s-identified (a.s.s. 
for âalmost sure samplingâ) if 3 Ao E A  such that p ( A q) = 0 and the statisÂ­
tical experiment Â£ 0 = {(5,5), P a : a E A â A 0} is s-identified (equivalently, 
if the mapping a â âº P a is injective on A â Ao). 
â 
For the sake of discussion, we recall the definition of the previous section:
4.6.15 D efinition. The Bayesian experiment Â£ = {A x 5, A  V 5,11} is 
b-identified (b for Bayesian) if A  = AS. 
u
We now examine the relationships between these four concepts of idenÂ­
tification. These concepts rely on different levels of specification: the first 
three concepts are based on a family of sampling probabilities without (DefÂ­
inition 4.6.12) or with (Definitions 4.6.13 and 4.6.14) a measurable structure 
on the parameter space, and without (Definitions 4.6.12 and 4.6.13) or with 
(Definition 4.6.14) a prior probability on that measurable structure. Note 
also that the fourth concept does not formally impose any regularity condiÂ­
tion on the Bayesian experiment.
Let us consider the relationships between the two ânon-Bayesianâ conÂ­
cepts. This involves the concept of a separating cr-field (Definition 0.2.7), 
i.e., a cr-field whose atoms are singletons or, in other words, A / A  â A/o.
4.6.16 Theorem , c-i dent ifi cat ion implies s-identification if and only if A  
is separating.
Proof. Recall that for any (7-field B C A, the B-measurable functions are 
constant on the 5-atoms. In particular, if B D  A i, P a(X) is constant on the

4.6. 
Identification in Bayesian Experiments
173
B-atoms; in other words, the elements of the 5-atoms are observationally
equivalent. We now prove that if B = Ai, the .4i-atoms are exactly the
equivalence classes of A/s. Indeed, if we denote by A x , as in Section 4.4.4,
the (7-field generated by the mapping a ââº P a( X ), we have, by Proposition
0.2.5, P a = P a> (i.e., a ~ a ') <=>> a ~  a1 V X  G S  O  a ~  a' , because 
s 
Ax 
A i
A\ = V xes Ax- ^  *s f^erefÂ°re clear that : A  = A\ => A /s = A/o if and 
only if A / A  = A/o. 
â 
4.6.17 Theorem . If A  is a Blackwell cr-field (see Definition 0.2.12) and S 
is separable then ^-identification implies c-identification:
A /s = A/o => A  = A i .
Proof.
(i) We first prove that if S  is separable, then A\ is also separable. Indeed, 
consider a countable 7r-system of sets {Xn} generating S. Then A x n is 
separable as it is the inverse image of Borel sets in [0,1] by the mapping 
a â> P a(Xn). Since {X  C S : a ââº P a(X) is Borel measurable} is a d- 
system, by Theorem 0.2.20(ii), it contains Â«S, and therefore A\ = \/n A x n > 
therefore A\ is separable.
(ii) s-identification implies that A \ is separating because a ~ a f 
a ~  a*
s 
A i
(see proof of Theorem 4.6.16). Therefore A  is also separating, as A \ C A.
(iii) As A\ is a separable sub-<7-field of a separating <7-field A , the result 
follows from Blackwellâs Theorem (see Theorem 0.2.16).
It should be noticed that the separability of S  is merely used to establish 
the separability of A \ without describing Ai explicitly. Recall that a Souslin 
a-field is a separating Blackwell (7-field (Definition 0.2.12). As the proof of 
Theorem 4.6.17 shows, in part (ii), that A  is separating, we may synthesize 
Theorems 4.6.16 and 4.6.17 as follows:
4.6.18 Theorem . If A  is a Souslin cr-field and S  is separable, s-idenfication 
and c-identification are equivalent. 
â 
We next consider the relationship between the two âBayesianâ concepts.

174
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.6.19 Theorem . If A is both separable and separating, 6-identification 
implies a.s.s.-identification.
Proof.
(i) .4 = .4 ^ because, by Theorem 4.4.12, A S  = AÂ±. 
Let us consider 
{En}, a countable algebra generating A. Then V n 3 Fn Â£ A\ such that 
m(EnAFn) = 0. We define A 0 = Un(EnAFn). Clearly p(A0) = 0.
(ii) A  separable and separating => V a, a' Â£ A, a /  a' 3n such that a Â£ En
and a' Â£ E%- Otherwise lEn(a) = 1 En(af) V n. This implies, by (0.2.3),
a ~ a ', i.e., a = a' since A  is separating. Recall that the separability of A  
A
implies that the ^4-atoms are measurable sets (see Section 0.2).
(iii) Now if a Â£ A â A q and a! ~  a (i.e., a' ~ a), and af ^  a, then a Â£ En CiFn
s 
Ai
and a1 Â£ E* D Fn, which implies that a1 Â£ Aq- Therefore A â Ao contains 
only one point of each ^ti-atoms, i.e., the mapping a â âº P a is injective on 
A â Ao. 
â 
4.6.20 Theorem . If A is a Blackwell <r-field and S is separable, then a.s.s.- 
identification implies 6-identification.
Proof. Let us consider the trace of A  on A â Ao\
A~ = { E n ( A - A 0) \ E Â£ A }  = A n ( A - A 0)
which is a Blackwell tr-field on A â Ao , and which makes the mappings 
a â âº P a(X)j X  Â£ Â«S, measurable on A â A q. Let Ai be the smallest cr-field 
on A â Ao such that the mapping a â âº P a( X ), X  Â£ 5, are measurable on 
A â Ao] then, as before, Ai is separable and by Theorem 4.6.17, A~ = A~[. 
Moreover, remark that
A t = { E n ( A - A 0) \ E e A i } .
Therefore A  and A\ have the same trace on a set of measure 1; therefore A  
and A\ are almost surely equal, i.e., A  = A i. 
â 
Similarly to the ânon-Bayesianâ concepts, one may summarize TheoÂ­
rems 4.6.19 and 4.6.20 as follows:

4.6. 
Identification in Bayesian Experiments
175
4.6.21 Theorem . If A  is a Souslin cr-field and S  is separable, a.s.s-idenÂ­
tification and 6-identification are equivalent. 
â 
Let us now consider the links between âBayesianâ and ânon-Bayesianâ 
concepts. Clearly:
(i) s-identification implies a.s.s.-identification for any p;
(ii) c-identification implies 6-identification for any p.
Converse results may be sought for either a specific p or for a family of 
p. Consider the following example:
Exam ple. Let (s | a) ~  iV(a, 1) if a2 /  1 and (s | a) ~  N{ 1,1) if a2 = 1. 
This model is not identified in any non-Bayesian terms because (â1) ~(+l). 
If the prior is such that p({â1}) â¢ /i({+l}) = 0, then this model is a.s.s- 
identified with A q = {â1} or A q = {+1} and, therefore, by Theorem 4.6.21, 
is 6-identified for any prior such that //({âl}) â¢ //({+!}) = 0. 
â 
This example shows that no âgentleâ condition on a specific p will enÂ­
sure that a.s.s.-identification or 6-identification implies s-identification or 
c-identification.
B. Identification in M arginal E xperim ents
Identification theory is a domain in which Bayesian and classical methÂ­
ods treat nuisance parameters differently. Consider a loss function Â£(a,d) 
where d is a decision. Parameters of interest may be defined either using 
the equivalence relationship:
a~a' &  V d:Â£(a,d) = Â£(<*',<!),
or using the smallest sub-<r-field B of A  that makes Â£(a,d) measurable for 
all d:
B = \/cr{Â£(;d)}.
d
The same argument as in the proof of Theorem 4.6.16 shows that these two 
approaches are equivalent in the sense that
A/Â£ = A/B.

176
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
We may now generalize the ânon-Bayesianâ concepts of identification by 
considering only the parameters of interest. Essentially, we are formalizing 
the intuitive idea that an âidentified subparameter should be a function of 
any sufficient parameterâ (see, e.g., Fisher (1966), Malinvaud (1978)). This 
entails the following definitions:
4.6.22 D efinition. A statistical experiment is:
(i) 
Â£-s-identified if A/s < A /t (equivalently if a ~  a! => a ~  a')
3 
t
(ii) 
Â£-c-identified if B C A\. 
â  
Then one gets the following results:
(a) 
By Theorem 4.6.16, ^-c-identification implies ^-s-identification for any 
loss function Â£\
(b) 
by Theorem 4.6.17, if A  is a Blackwell cr-field and S is separable, then 
^-identification and ^-c-identification are equivalent.
The classical condition B C Ai implies, in a Bayesian experiment deÂ­
fined through any p on {A, A) that
b c a [ = a s .
This property should not however be taken as an alternative Bayesian conÂ­
cept of partial identification. Indeed, as mentioned in Section 4.4, if B is the 
only parameter of interest, the relevant experiment is Ssws &nd the identiÂ­
fication condition in this experiment is B â BS. This is so because when B 
is the only parameter of interest, inference bears only on the transformation 
pB ââº pB. The examples of Section 4.6 have shown that, in general, there 
is non-implication between the two conditions B C A S  and B = BS.
4.7 
Exact and Totally Informative Experiments
This section is mainly motivated by Bayesian asymptotic theory but as 
a preliminary, we consider a somewhat broader question: to what extent

4.7. 
Exact and Totally Irformative Experiments
177
does an observation give âperfectâ information about a parameter? The 
natural answer to this question is: it does once the parameter is known 
perfectly after (almost) any observation of the sample. Consider the general 
reduced experiment Â£&$t - A parameter C C B V M  is known perfectly if 
C C M  V T. Note that C C M  V T  is equivalent to Â£ X  C | M  V T by 
Corollary 2.2.8. However, B X Â£  | M  V T  is implied by C C M y  T  and, 
by Corollary 2.2.11, implies Â£  JL C \ M V  T.
By symmetry this justifies the following definitions:
4.7.1 D efinition. In the general reduced Bayesian experiment Â£ ^ 7-,
(i) 
A parameter Â£  C B V M  is exactly estimable if Â£ C M  V T  or, 
equivalently, Â£ X B | M  V T.
(ii) 
A statistic A/" C Af V T  is exactly estimating if M  C 5  V M  or, 
equivalently, Af JL T  | B V A4. 
â 
For expository reasons, we provide comments with respect to the unreÂ­
duced experiment Â£ = Â£avs only-
In Â£a v s , a parameter C C A  is exactly estimable if C C S. Thus, any 
sub-<r-field of *4nÂ«S is exactly estimable. It should be stressed that in C C S, 
the completion of S  is made w.r.t. n, i.e., S is generated by S  and all the 
n-null sets of the product space A  V S. Exact estimability means that, for 
any c E [Â£]+, the posterior expectation Sc is equal to c a.s. n; equivalently 
if c is the indicator function of an event C E C, one has: S i c  = 1 c a.s.n. 
This is also: S ic  is equal to 0 or 1 a.s. P. In other words, C C S formalizes 
the idea that C is âknown perfectlyâ after the observation of the sample.
Similarly, a statistic U C S  is exactly estimating if U C A. Thus any 
sub-<r-field o l A D S  is exactly estimating. By the same argument as above, 
for any event U E U, its sampling probability *41j7 is equal to 0 or 1 a.s.//. 
Heuristically, any u E [U] is a.s. constant in the sampling.
As shown in the following theorem, the exactly estimable parameters 
are in complete duality with the exactly estimating statistics.
4.7.2 Theorem . In a general reduced Bayesian experiment Â£ ^ r , a paÂ­
rameter Â£  C B V M  is exactly estimable if and only if there exists an

178
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
exactly estimating statistic Jf C JA V T  such that C = Jf.
Proof.
(i) It suffices to pose J f â Cn(JAWT). Clearly, Af is is an ^ ^ r -statistic, i.e., 
J f C M y T; furthermore, J f is exactly estimating because J f C C C B V JA 
and J f  = C because J f  = C D M  V T (Lemma 2.2.5(h)). Then J f  = Â£ since 
Z C M  V T.
(ii) Reciprocally, if Â£ = A* C JAM U then C C B V M is exactly estimatÂ­
ing. 
â 
In the asymptotic case a parameter 6 Â£ [>t] will be exactly estimable 
if there exists a strongly consistent sequence of estimators of b , i.e., if 
3 in Â£ [S], n Â£ JN, such that tn â+ b a.s.II. In sampling theory, it is 
known that a necessary condition for the existence of such a sequence is the 
identifiability of the parameter.
The Bayesian analogue of this result, given in the next proposition, is a 
simple consequence of the elementary Property 4.5.2(h)
4.7.3 Proposition. In
(i) 
Any exactly estimable parameter, C C (B V M )  fl (M  V T), is identiÂ­
fied, i.e., C < T | M .
(ii) 
Any exactly estimating statistic, J f C (B V M ) n ( M V T ) , is identified, 
i.e., J f < B | JA. 
â¢
Clearly, in the general reduced experiment
Z m ax =  ( S V A f ) n ( A 7 v T )
is the unique maximal exactly estimable parameter and
Afmax = (B V M ) fl ( M V T)
is the unique maximal exactly estimating statistic. Furthermore, by Lemma 
2.2.5(h), we have:
Anax = v max =  (B V M )  n (M  V T).

4.7. 
Exact and Totally Informative Experiments
179
The elementary Property 4.3.2(i) provides two sets of inclusions;these 
are primordial to understanding the possible informational content of any 
Bayesian experiment.
4.7.4 P roposition. In the general reduced experiment
(i) 
The trivial parameter, i.e., (B V M ) fl AI, is exactly estimable and 
any exactly estimable parameter is included in the minimal sufficient 
strong parameter, i.e.,
( BV  M ) n M C ( B y  M ) H ( M V T) c ( B V  M ) ( M V T )  C B V M ]
(ii) the trivial statistic, i.e., M  fl (M  V T) is exactly estimating and any 
exactly estimating statistic is included in the minimal sufficient strong 
statistic, i.e.,
M ( 1 ( M V T ) C ( B V  M ) H ( M V T ) C ( M V T ) ( B V M )  C M V T .  â¢
Recall that equalities in the last inclusions in the above two relations 
amount to 
identifications, i.e., B < T  | M. and T  < B \ M . Note that 
there is no relationship between those two properties. Equality for the first 
inclusions means that only trivial parameters (respectively, trivial statistics) 
are exactly estimable (respectively, exactly estimating). Note that these two 
properties are in fact equivalent since, by Lemma 2.2.5(ii), each of these is 
equivalent to (B V M ) fl ( M V T) = M , i.e., Zmax = 
max = M . This 
situation is analyzed more deeply in Chapter 5.
The next theorem shows that equalities for the middle inclusions are also 
equivalent, i.e., equality between the maximal exactly estimable parameter 
and the minimal sufficient strong parameter is equivalent to equality beÂ­
tween the maximal exactly estimating statistic and the minimal sufficient 
strong statistic.
4.7.5 Theorem . In the general reduced Bayesian experiment 
the 
following conditions are equivalent:
(i) 
(B V M )  fl (M  V T) =  (SV M ) ( M  V T);
(ii) 
(B V M )  H (M  V T) = (M  V T)(B V M ).

180 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
Proof. By Theorem 4.3.3(i) and Corollary 2.2.11,
( B y  M ) ( M VT) C ( B y  M ) D ( M VT)
implies
( B y  M ) X ( M V T ) I ( B y  M ) fl ( M VT).
By Corollary 2.2.7 and Lemma 2.2.5(h), this is equivalent to
( B y M ) x ( M v T ) | ( B y M ) n ( M y r ) .
But, by Theorem 4.3.3(h) and by Lemma 2.2.5(h),
( B y M ) X (Af VT) I ( B y M ) fl ( M VT)
implies
( BvM ) ( M vT ) c (b V m ) fl ( M V T )
and, therefore,
( B y  M ) ( M VT) C ( B y M ) fl ( M V T).
Therefore (i) is equivalent to
(# v Af) x ( M  y T ) \ ( B y  M ) n ( M v t )
and is equivalent to (ii) by symmetry. 
â 
Let us comment on Propositions 4.7.3, 4.7.4 and Theorem 4.7.5 in the 
unreduced experiment Â£4 v sâ¢ It was mentioned, in Section 4.6.1, that for a 
parameter C C A  of Â£ 4vs there is, in general, no relationship between 
C C AS and C = CS. We notice that, if C is exactly estimable, i.e., 
C C A  fl Â«S, then, from Proposition 4.7.3, C = CS and, from Proposition
4.7.4, C C AS. Therefore, if C is exactly estimable these two properties are 
simultaneously satisfied. Note also that, in view of Theorem 4.4.12, C C A S  
is a.s. equivalent to the sampling definition of an identified subparameter, 
while C = CS is a genuinely Bayesian property since it requires integration 
on A  conditionally on C in order to be verified. Note also that if the minÂ­
imal sufficient parameter A S  is exactly estimable, i.e., A S  = A  fl S, the 
Bayesian equivalent definition of an identified subparameter, i.e., C C A S , 
implies that this parameter is exactly estimable and therefore is identified,

4.7. 
Exact and Totally Informative Experiments
181
i.e., C c A C i S  and C =  CS. But it has not been shown that if the minimal 
sufficient parameter is exactly estimable, i.e., A S  = A  fl S, then any idenÂ­
tified parameter, i.e., C C A  and C = CS, would be exactly estimable, i.e., 
C C A  fl S, unless, of course, the Bayesian experiment Saws is identified, 
i.e., A  = AS.
Proposition 4.7.4. states that the best one can hope for is to estimate 
exactly the functions of the minimal sufficient parameter, i.e., AS. This, 
along with Theorem 4.7.5, leads to the following definition:
4.7.6 D efinition. The general reduced Bayesian experiment 
is exact 
if any one of the following equivalent conditions holds:
(i) 
(B V M ) { M  V T) C M  VT;
(ii) 
(M  V T)(B V M )  C WSt M ,
i.e., the minimal sufficient strong parameter (respectively, statistic) is exÂ­
actly estimable (respectively, estimating). 
â 
The following proposition, whose proof is straightforward, completely 
characterizes the structure of an exact Bayesian experiment.
4.7.7 P roposition. If the general reduced Bayesian experiment 
- is 
exact, then
(B V M ) ( M  V T) = (M  V T)(B V M )  = (B V M )  n (M  V T). 
â 
In general, the minimal sufficient parameter is not exactly estimable in 
a finite sample experiment. But this may be true asymptotically. But in 
such a situation, relations such as (i) and (ii) in Definition 4.7.6 may be 
difficult to verify, for they require computation of the minimal sufficient 
strong parameter (or statistic). We thus need a more operationally useful 
criterion to establish whether an experiment is exact. To such an end, the 
most useful result, inspired by Theorem 4.7.2, is provided by the following 
theorem:

182 
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.7.8 Theorem . The general reduced Bayesian experiment S ^ r  is exact 
if and only if there exists either an exactly estimating sufficient statistic, 
i.e.,
(i) 
3 J\f C M  V T  such that
1. 
B Â± T  \ M  VAT,
2. 
A c  B V M ;
or, there exists an exactly estimable sufficient paramater, i.e.,
(ii) 
3 C C B V M such that
1. 
B Â± T \ M V Â£ ,
2. 
C C M V T.
Moreover, for any A  satisfying (i), we have
(iii) 
M V A  = ( B V M ) n ( M VT), 
and, for any Â£ satifying (ii), we have
(iv) 
CV M  = (BV M )  fl (M  V T).
Proof. By symmetry it suffices to prove (i). By Proposition 4.7.7 and 
Theorem 4.7.5, the necessity is clear by taking A  = (B V Af) fl (Af V T). 
On the other hand, by Proposition 4.3.10 or by Proposition 4.4.13, (i) 1. 
implies (A< V T)(B V A<) C AI V A  and (i) 2. implies Af V A  C B V  AI. 
Therefore (A4 V T )(# V AI) C #  V AI, i.e., S ^ r  is exact. More precisely, 
(AI V T )(# V AI) C AI V A  C (# V AI) fl (AI V T) and this shows (iii), by 
Proposition 4.7.7. 
â 
Let us provide comment on Theorem 4.7.8. in the unreduced experiment 
Â£ 4vs- In order to show that Â£.4vs is exact, i.e., A S  C Â£, it suffices to exhibit 
a sufficient statistic, i.e., T C S  such that A  JL S  | T, that is exactly

4.7. 
Exact and Totally Informative Experiments
183
estimating, i.e., T  C A. Let us note that any such statistic is almost 
surely unique since it follows from (iii), that T  = A  fl S  or, equivalently 
T  fl S = A  fl <S, i.e., T  completed by the null sets of S  is equal to the 
maximal exactly estimating statistic, which is also the minimal sufficient 
statistic S A  since when Â£ 4vs is exact, S A  = A  fl S  and A S  = A  fl S, 
(since A S  = S A  = A  fl S by Proposition 4.7.7). This also implies that 
A  n r  = A n s  = AS.
This section has presented the fundamental theorems of Bayesian asympÂ­
totic theory (insofar as a.s. convergence is concerned). In Chapters 7, 8 
and 9 we see how particular structures such as independent sampling and 
invariance can be used to make the criteria of exact estimability operational. 
Combining identification and exactness of a Bayesian experiment leads to 
the concept of a totally informative Bayesian experiment.
4.7.9 D efinition. The reduced Bayesian experiment 
is totally inforÂ­
mative if it is identified and exact, or equivalently, if B C M  V T. 
â 
The equivalence stated in the definition comes from Proposition 4.7.4(i). 
Indeed, identification and exactness amount to saying that the last two 
inclusions are actually equalities, and is therefore equivalent to
B V M  = (B V M )  fl (M  V T),
which is equivalent to B C M  V T.
Furthermore, if 
is totally non-informative, i.e., B I T  | A4,
this is equivalent, by Theorem 4.3.4 and Corollary 2.2.11, to saying that 
(B V M )(M  VT) = ( B \/M )r \M , i.e., the minimal sufficient complete strong 
parameter is equal to the trivial parameter. Therefore, 
totally non-
informative means that the first two inclusions in Proposition 4.7.4(i) are 
actually equalities. In other words, Â£Â£$? is totally non-informative if and 
only if S ^ r  is exact and if the maximal exactly estimable parameter is the 
trivial parameter. Finally, note that 
will be both totally informative 
and totally non-informative if all the inclusions in Proposition 4.7.4(i) are 
actually equalities; this is equivalent to saying that B C A4, i.e., the full 
parameter is trivial.

184
4. Optimal Reductions: Maximal Ancillarity and Minimal Sufficiency
4.8 
Punctual Exact Estimability
In asymptotic theory, Bayesian consistency has generally been presented 
as the convergence of the posterior distributions to a point mass or a Dirac 
distribution (see, e.g., LeCam (1986), Berk (1966, 1970), Hartigan (1983)). 
In this context, punctual exact estimability of a Bayesian experiment Â£ = 
( A x  S, A V S ,  II) may be presented as follows: There exists a measurable 
function a : (S , S ) ââº (A, A) such that <$a(s) is a version of the posterior 
probability /i5, i.e.,
In our conventions, this may be expressed as follows:
4.8.1 D efinition. In the unreduced Bayesian experiment 
Â£ = (A x 5, A  V tS, II), the parameter A  is punctually exactly estimable if 
there exists a measurable function /  : (A x S', <S) ââº (A x 5, .4) such that
This definition is in fact essentially equivalent to (4.8.1). Indeed, it suffices 
to define f(a,s) = (a(s),s).
We now compare Definition 4.8.1 with our Definition 4.7.1, i.e., A  C S.
4.8.2 Theorem . In the unreduced Bayesian experiment Â£a v s , if A  is puncÂ­
tually exactly estimable then A  is exactly estimable, i.e., A  C S.
Proof. From (4.8.2), it follows that (S 1 e ) 2 = (S lg). Therefore,
(4.8.1) 
ps(E) = 6a(s)(E) = l E[a(s)\ 
V E G A, 
V s e S.
(4.8.2) 
S l E = l E o f = l f - 1(E) 
a.s.n V E 6 A .
S  1e â I# 
a.s.II 
V E G A )
i.e., A  C S.
From the proof of Theorem 4.8.2, we see that (4.8.2) implies
(4.8.3)
1 e â !/-!(Â£;) 
a.s.n 
V E Â£ A;
fi = p Â° r 1,

4.8. Punctual Exact Estimability
185
i.e. the prior probability is the image under /  of the predictive probability 
(see Proposition 0.3.8). The converse of Theorem 4.8.2 requires that a 
regularity condition be satisfied.
4.8.3 Theorem . In the unreduced Bayesian experiment Â£a v s , if A  is a 
Standard Borel cr-field and if A  is exactly estimable, i.e., A  C S, then A  is 
punctually exactly estimable.
Proof. By theorem 0.3.18, let fis be a regular version of the posterior probÂ­
abilities, i.e., S  1e = PS(E) 
a.s.II 
V E G A. Since A  C S, ns(E) = 1 e 
a.s.II V E G A. Now, let A t = ^ â 1((âoo,f]) where <p is the bijection proÂ­
vided by Definition 0.2.15. Since /i5 is a probability measure on A, V s G S 
is increasing and right continuous in t. Therefore, if we define:
t(s) = inf{< : fis(At) = 1},
then, clearly, t G [Â«S], since
{s E S : t(s) < 
= {s G S : fP^At) = 1}.
Define o(s) = ^ â ^ (s)] and f(a ,s) = (a(s),s); by the monotone class 
Theorem 0.2.20(ii), we conclude that /  satisfies (4.8.2), since V t G 1R, 
V s G S, ns(At) = l At[a(s)]. 
â 


5
Optimal Reductions: Further Results
5.1 
Introduction
The goal of this chapter is to study the relationship between sufficiency 
and ancillarity in greater detail, paying particular attention to minimal 
sufficiency and maximal ancillarity. Along these lines, a first result was 
presented in Chapter 2 (Theorem 2.3.19) for an unreduced experiment, and 
in Chapter 3 (Theorem 3.3.8) for the general reduced experiment.
In sampling theory, other results have been established, most of which 
are due to Basu. Theorem Two in Basu (1955) states that any statistic 
which, in the sampling process, is independent of a sufficient statistic is 
ancillary provided a further condition links the parameter and the sufficient 
statistic. In the Bayesian framework the problem addressed by Basu TheoÂ­
rem may be expressed as: under what condition on A  and T  does A  JL S  | T 
and U JL T  \ A  imply U X A? This condition on A  and T  has been called 
the measurable separability of A  and T ; it essentially implies that A  and 
T  have no non-trivial event in common.
Theorem One in Basu (1955) states that any ancillary statistic is indeÂ­
pendent of a sufficient statistic, in the sampling process, provided a further 
condition, which links the parameter and the sufficient statistic is satisÂ­
fied, namely, the sufficient statistic must be boundedly complete. In the 
Bayesian framework this becomes: under what conditions on A  and T  does 
A  JL S \ T  and U JL A, imply that U JL T  \ A I  This condition has been
187

188
5. 
Optimal Reductions; Further Results
called the strong identifiability of T  by A. This name has been retained beÂ­
cause it is a stronger condition than the identifiability of T  by A  as defined 
in Chapter 4 (Definition 4.5.1).
In this chapter, we generalize these two notions (measurable separabilÂ­
ity â Section 5.2 â and strong identifiability â Section 5.4 â) so as to 
provide similar results in reduced experiments. These two properties are 
of some independent interest. We therefore present, in these sections, a 
general theory of these properties and of their relationship with both condiÂ­
tional independence and the projection of (7-fields. We subsequently discuss 
their application in statistics (in Sections 5.3 and 5.5), and the relationship 
between these concepts and the analogous sampling theory concepts.
In view of the principle of conditioning, the choice of a suitable ancillary 
statistic is generally considered as important in parametric inference, and 
in this context, it matters to know whether or not a given ancillary statisÂ­
tic is maximal, i.e., whether or not it is a non-injective function of some 
other ancillary statistic. Recent references on this topic include Barnard 
and Sprott (1971), Basu (1959, 1964,1975), Becker and Gordon (1983), Cox 
(1971), Cox and Hinkley (1974, Chap. 2), Fraser (1973). As mentioned 
in Chapter 4 (Section 4.2), although one may give a definition and prove 
the existence (using Zornâs Lemma) of maximal ancillary cr-fields (i.e., paÂ­
rameters or observations)), we know of no way to construct such maximal 
ancillary cr-field. However, the tools developed in this chapter provide two 
alternative criteria of maximality. These criteria may be illustrated by the 
following simple example (see Basu (1959), Example f). Consider a simÂ­
ple observation on a normal variate with known mean; more specifically 
(x | a) ~  N  (0, a); consider also the ancillary statistic y = l{;r>o}. We prove 
that, in order to assert that y is essentially maximal ancillary, any one of 
the following two arguments is valid: (i) x 2 is sufficient and complete and 
(x2 ,y) is equivalent to x\ (ii) conditionally on y , x is complete. Argument
(i) actually leads to a Bayesian version and an extension to the conditional 
experiment (such as regression-type models) of Theorem 7 in Basu (1959) 
providing sufficient conditions for the maximality of an ancillary statistic. 
Argument (ii) replaces the requirement that there exists a complete suffiÂ­
cient statistic by a requirement that a property of conditional completeness 
be verified; we shall see that this argument is actually both more general 
and more useful in practice than argument (i). Indeed, argument (ii) may be

5.2. Measurable Separability
189
used when the minimal sufficient statistic is not complete whereas when the 
minimal sufficient statistic is complete, in view of the independence between 
a complete sufficient statistic and any ancillary statistic, conditioning on an 
ancillary statistic becomes irrelevant. Furthermore, argument (ii) may be 
used to resolve the apparent contradiction between the principle of suffiÂ­
ciency and the principle of conditioning. Indeed, once a sample has been 
reduced to a sufficient statistic, further conditioning on any larger (i.e., not 
included) ancillary statistic will be irrelevant: the best reduction is obtained 
through the distribution of a minimal sufficient statistic conditionally on an 
included ancillary statistic which makes the sufficient statistic conditionally 
complete; argument (ii) ensures that such an ancillary statistic is maximal 
within that sufficient statistic.
Sections 5.2 and 5.4 treat measurable separability and strong identiÂ­
fication of cr-fields, the two probabilistic tools of this chapter; they make 
use of and extend Mouchart and Rolin (1984b) and (1986). The statistical 
Sections 5.3 and 5.4 are based on Mouchart and Rolin (1984b) and (1984c); 
however, Sections 5.3.3 and 5.5.4 on sampling theory and Bayesian methods 
are the fruit of more recent work.
5.2 
Measurable Separability
We now handle a topic in general probability theory and adopt, as beÂ­
fore, our usual notation; in particular, (M, A4,P) is an abstract probability 
space.
The question raised by Basu Second Theorem in the Introduction 5.1 
is actually equivalent to the following problem: if M i l M 2 | M 3 and 
M i  i  M 3 | .M2, under what supplementary condition is M i  X ( M 2 V M 3 )? 
In this section we treat a somewhat more general problem by considering a 
conditional version of this question. This leads to the concept of measurable 
separability among cr-fields. This concept is weaker than (i.e., is implied by) 
independence; like independence, it has both a conditional and a marginal 
version. In its marginal version, it says that M i  and M 2 are measurably 
separated if the only events in common are trivial and, in its conditional 
version, it corresponds to the property that events common to two cr-fields 
are in the conditioning cr-field.

190
5. 
Optimal Reductions: Further Results
5.2.1 Theorem . The following conditions are equivalent:
(i) 
(Afi VM3)n (Af2VAf3) = Af3;
(ii) (M i V Af3) fl ( M 2 V Af3) C Af3;
(iii) m G [ M i V M 3^  and ( M 2 V M 3 ) m â m imply M ^ m  â m.
Proof. Clearly (i) implies (ii) since for instance
(M \ V A43)n ( M 2 V M 3) C (M i V .M3)D ( M 2 V .M3). However, since, by 
Lemma 2.2.5 (ii), (M \ V M$)C\ ( M 2 V .M3) = (M i V .M3) fl ( M 2 V M 3 ),
(ii) implies that (M i V M 3) D ( M 2 V M 3 ) C M 3. Therefore (ii) implies
(i) 
since, clearly, M 3 C (M i V .M3) fl ( M 2 V .M3). And (iii) is equivalent 
to (ii) by (2.2.4). 
â 
5.2.2 D efinition. Under any one of the conditions of Theorem 5.2.1,
M
i  and .M2 are measurably separated conditionally on M 3 and we write 
M i || M 2 | .M3. If we want to make explicit the role of the probability P in 
this concept, we also write M 1 || M 2 | .M3; P. When M 3 = M
o  we simply 
say that M
i  and M 2 are measurably separated and write M
i  || M
2- 
â 
It is clear, from condition (i) in Theorem 5.2.1, that the concept of meaÂ­
surable separability is symmetric in M i  and M 2 ] thus, one could formally 
add conditions (ii bis) and (iii bis), which would be obtained by interchangÂ­
ing M i  and M 2 -
Let us also remark that events in (.Mi V .M3) fl (.M2 V .M3) may be 
characterized in several ways. Obviously,
A Â£ (M i V M 3 ) fl ( M 2 V M 3 )
(5.2.2)
A E M 1 V M 2 
and 
3 5  Â£ .M2 V .M3 
such that 
1 a = 1 b a.s.
By (2.2.4), this is also equivalent to
(5.2.3) 
A E .Mi V M 3 
and 
( M 2 V M 3 )1A = 1a

5.2. Measurable Separability
191
This clearly implies that
(5.2.4) A â¬ M i  V M z and { ( M 2 V M z) 1A } 2 =  (M 2 V M z) 1a,
i.e., the expectation of the indicator function of A conditionally on M 2 V M 3 
is a {0,l}-valued function, i.e., is itself an indicator function. But this is 
equivalent to
(5.2.5) A â¬ M i  V M 3 and (M 2 V M z) [{1a -  ( M 2 V M z) 1 a }5 = 0,
i.e., the variance of the indicator function of A conditionally on M 2 V M 3 
is equal to zero. This evidently implies (5.2.3). Therefore Properties (5.2.2) 
through (5.2.5) are equivalent characterizations of events in 
{Mi V M 3 ) fl ( M 2 V Mz).
Therefore, if M i  || M 2 | M ^ ) any set A satisfying one of the properties
(5.2.2) through (5.2.5) also satisfies:
(5.2.6) 
3 C G M 3 
such that 
1 a = 1 c a.s.
or, equivalently,
(5.2.7) 
M 3 1a = 1a
or, equivalently,
(5.2.8) 
( M z1a ) 2 = M z 1a
or, equivalently,
(5.2.9) 
M z [{1a - M z 1a} 2] = 0 .
To gain more insight into the basic idea of measurable separability, 
consider z,-, i = 1,2,3, real random variables and M i  = <r{xi}. Using 
Theorem 0.2.11, in this case M i  || M 2 | M 3 may also be expressed as 
follows: If their exist two (bounded) Borel measurable functions on IK2, /  
and g such that f  (x 1,2:3) = <7 (Â£25^3) a-s-> then there exists a (bounded) 
Borel measurable function on IK, h, such that f  {x 1, 2:3) = ^ (^ 3) a.s. In 
particular, M i  || M 2 is equivalent to the following statement: If there exist 
two Borel measurable functions on M , /  and g, such that f  (x 1) = g (X2) 
a.s., then there exists a constant c E JR such that f  (x 1) = c a.s. Such

192
5. 
Optimal Reductions: Further Results
a condition is not satisfied if the support of x\ and x 2 has the shape of 
Figure 1, where the events {#1 6 A} and {x2 â¬ B} are the same a.s. for 
the joint probability. Thus, the condition of measurable separability seeks 
to preclude such pathologies and, as will be shown in examples later on in 
this chapter, validates simple reasoning in terms of densities which would, 
otherwise, give invalid results.
Figure 1
It should be clear from Theorem 5.2.1 and from these remarks that meaÂ­
surable separability depends on the probability P through its null set only. 
More specifically we have the following property of robustness with respect 
to P:
5.2.3 P roposition. Let P and P f be two probability measures on (M ,M ). 
If P  ~  P ', then M i || M 2 I M 3 ; P <=> M i || M 2 \ M 3; P'. 
â 
We now collect a series of general properties of measurable separability. 
They do not require proof since they are simple consequences of the definiÂ­
tion and of Lemma 2.2.5. They are organized according to their similarity 
to the corresponding properties in the case of conditional independence.

5.2.4 Proposition. Elementary properties of measurable separability:
(i) 
M i  || M 2 | M 3 <=>* M i  || M 2 | M 3 <=> M i  || M 2 | M 3 ]
(ii) 
M i  C M 3 = >  M i  || M 2 | M 3 
V M 2]
(iii) M i  || M 2 \ M 3 ^
 (M i V M s) || ( M 2 V M s) \ Ms]
(iv) M i  || M 2 | M 3 and M 4 C M 2 V M 3 => M i  || Af4 | Ms- 
â 
The next theorem is similar to Theorem 2.2.10 insofar as it makes a 
characteristic property of measurable separability explicit.
5.2.5 Theorem . If
(i) 
M i  || M 2 | M 3,
(ii) 
M i  || M a I M 2 V M 3 ,
then
(iii) M i  || ( M 2 V M 4) | .Ms-
Proof. Indeed,
M i  V M s  
n 
M 2 v  M 4 v  M s
â M i  V M 3 n ^ M 1 V M 2 V M 3 n M 4 V M 2 V Adsjâ
= 
M 1 V M 3 n M 2 V M 3 
by (ii)
= 
M s  
by (i). 
â 
The next proposition is similar to the first part of Corollary 2.2.11.
5.2.6 P roposition. If Afi || A<2 | M s  then for any .M4 C M 2 V M s  and 
M$ C Afi V Afs,
(Afi V Af5) || (Af2 V Af4) I Ms-
5.2. Measurable Separability 
193

194
5. 
Optimal Reductions: Further Results
In view of the following theorem, measurable separability may be viewed 
as a necessary condition of conditional independence (this explains the noÂ­
tation || and X ).
5.2.7 Theorem . If M i  X M 2 \ M 3 then M \  || M 2 | M 3 .
Proof. M i  X M 2 | M 3 implies, by Corollary 2.2.11,
(A4i V M.3 ) X (A^2 V M.3 ) | M.3 .
An application of Corollary 2.2.9 finishes the proof. 
â 
It should be noticed that measurable separability preserves most, but 
not all, of the properties of independence. For instance, when there is conÂ­
ditional independence, the condition analogous to 5.2.5(iii) is equivalent to 
the condition analogous to 5.2.5(i) and (ii); this was Theorem 2.2.10. When 
there is measurable separability, Condition 5.2.5(iii) clearly implies 5.2.5(i) 
but not 5.2.5(ii). Similarly, Proposition 5.2.6 is the analogue for measurÂ­
able separability of Corollary 2.2.1 l(i) for conditional independence but the 
analogous statement for measurable separability of Corollary 2.2.11(ii) is
not true in general. More specifically, M i  || A^2 and M 4 
C M 2 do not
imply M i  || M 2 | MÂ±.
Here is an example of these two lacks of implications.
Exam ple. Let Af,*, (2 = 1,2, 3,4) be a finite partition of 
M with
P (M i) >  0 Vi. Take
M i  = a {Mi U M3} , 
M 2 â o' {Mi U M2, M3} ,
M .3 = A4o> 
M .4 â o {Mi U M2} .
Clearly, M 4 C M 2 , M i  || M 2 , but M 2 C M i V M 4 . Thus, M i  || M 2 | M 4 
is not verified. 
â 
The next result shows how a conditional independence property may 
be used to increase cr-fields without sacrificing a measurable separability 
property.

5.2. Measurable Separability
195
5.2.8 Theorem . Let
(i) 
Mi  X M 2 | M 3 V M 4
(ii) 
M 5 C M 1 V M 3 V M 4
(iii) .M5 H.M4 I.M3
then
M 5 || ( M 2 V M 4 ) | m 3.
Proof. Clearly, (i) and (ii) imply M 5 X  M 2 | M 3 V M 4 . By Theorem 
5.2.7, M 5 || M 2 | M 3 V M 4 and, along with (iii), gives the result by 
Theorem 5.2.5. 
â 
The next corollary applies Theorem 5.2.8 to minimal conditioning cr- 
fields and is analogous to Theorem 4.3.11 for conditional independence.
5.2.9 Corollary. Let M 3 V M 5 C M i and M 4 C M 2- Then
(i) 
M 3 || M 2 M 1 | M 4 V M s <=> M 3 || M 2 | M 4 V M 5;
(ii) 
M 3 || M 1M 2 | M 4 V M 5 
-M3 || M 2 | M 4 V M 5.
Moreover
(iii) M i || M 2 IM 4 V M 5 <=> M 1M 2 || M 2M 1 | M 4 V M 5.
Proof, (i) and (ii) are consequences of Theorem 5.2.8, if we notice that 
M.\ X .M2 | M.2M.\ V M >4 V .M5 and M.\ X M .2 \ M.\M .2 V M .4 V M.%.
(iii) is obtained by symmetrizing (i). 
â 
The last theorem in this section responds to the question raised at the 
beginning of this section and may be viewed as a variant of Theorem 2.2.10.

196
5. 
Optimal Reductions: Further Results
5.2.10 Theorem . If
(i) 
M i  X  A f2 | A f 4 V A f 3;
(ii) 
M i  X M ^  | A f 2 V M 3\
(iii) M 2 || A f4 | M 3,
then
(iv) M i  X  { M 2 V A f 4) \ M 3.
Proof, (i) = >  M i  X (M 2 V Af3) | M 4 V Af3 by Corollary 2.2.11(i); simÂ­
ilarly (ii) => M i  X (Af4 V Afs) | Af2 V Af3. By Corollary 2.2.13, these 
properties imply Afi X (Af2 V Af3 V Af4) | Af2 V Af3 fl Af3 V Af4.
By (iii), Afi X (Af2 V M 3 V Af4) | Af3, and this is equivalent to (iv). 
â 
Proposition 5.2.3 and Theorem 5.2.7 entail the following corollary which 
is very useful for verifying a measurable separability property as it shows 
that there is (conditional) measurable separability once the probability is 
equivalent (i.e, same null sets) to a probability that makes Afi and Af2 
(conditionally) independent.
5.2.11 Corollary. If 3 P' probability on (M, Af) such that
(i) 
P ~  P';
(ii) 
A^i JL 
I M 3\ P',
then for any M s  C Af 1 V Af3, and for any Af4 C Af2 V Af3,
(iii) A fi || A f2 
| A f3 V M 4 V A f5; P.
Proof. M i  A- M 2 \ M 3\ P' 
implies, by Corollary 2.2.1 1, that
A fi X  A f2 | A f3 V A f4 V A f5;P '.
Therefore, by Theorem 5.2.7,

5.3. 
Measurable Separability in Bayesian Experiments
197
M i || Mi 2 | Ml3 VM a V M $ \ P', 
and by Proposition 5.2.3, under (i), this implies 
M \ || Mi 2 | M 3 V Ml a V M$\P.
5.3 
Measurable Separability in Bayesian Experiments
As mentioned in the introduction to this chapter, measurable separabilÂ­
ity is both necessary and sufficient to ensure that any statistic independent 
of a sufficient statistic is ancillary. In fact, this condition is a very weak 
regularity condition which ensure, heuristically, that the parameter and the 
statistic are properly distinguished. Such a property is particulary desirable 
in finite sample as it precludes undesirable pathologies and justifies usual 
manipulations of sampling densities. In contrast, such a property is undesirÂ­
able in asymptotic theory where instead one hopes for total informativity; 
this latter property appears to be antithetic to measurable separability.
We first introduce the concept of a measurably separated Bayesian exÂ­
periment (Section 5.3.1) and then analyse the role of measurable separability 
in Basu Second Theorem both in a purely Bayesian form (Section 5.3.2) and 
in sampling theory (Section 5.3.3).
5.3.1 
Measurably Separated Bayesian Experiment
We first define the notion of a measurably separated experiment.
5.3.1 D efinition. The Bayesian experiment 
is measurably separated 
if B || T  | M , i.e., if any one of the following three equivalent conditions 
holds:
(i) 
( B v M ) f l  ( M V T )  = M;
(ii) 
(B V M )  n ( M V T) = (B V M )  HM;
(iii) 
(B V M ) fl ( M V T) = M  fl ( M V T ) .

198
5. 
Optimal Reductions: Further Results
In particular, the unreduced Bayesian experiment Â£ is measurably separated 
if A || S, i.e., if any one of the following three equivalent conditions holds:
(i) 
A H S = T;
(ii) A fl S = A fl J;
(iii) A n s  = i n s .  
m
This definition may be interpreted as follows: in a measurably sepaÂ­
rated Bayesian experiment 
the maximal exactly estimable parameÂ­
ter (B V M ) D (M VT) is equal to the trivial parameter (B V M ) fl M ,  
i.e., the conditioning (7-field, M , completed by the null sets of the full paÂ­
rameter B V M , or, equivalently, the maximal exactly estimating statistic 
(B V M )  fl (M  V T) is equal to the trivial statistic M  fl (M  V T), i.e., the 
conditioning (7-field, Af, completed by the null sets of the full observation 
M V T .
Recall the primordial sets of inclusions for the Bayesian experiment 
Â£ j^r , given in Proposition 4.7.4, i.e.,
(5.3.1) 
(B V M) n M  C ( BV  M) fl (Af VT)
C (6 V M) (M V T) C (B V M ) ,
(5.3.2) 
M fl ( M V T) C ( B V X ) n ( A 4 V T )
C ( M V T ) ( B V M ) C M VT.
The measurable separability property of the Bayesian experiment Â£ ^ r  may 
be seen as a property completely opposite to that of the total informativity 
of this experiment. Indeed, Â£$$T is measurably separated if any of the first 
inclusions in (5.3.1) and (5.3.2) is an equality, and if the maximal exactly 
estimable parameter is trivial. In contrast, Â£&/? 
totally informative if the 
maximal exactly estimable parameter is the full parameter (see Definition 
4.7.9). Recall that 
is exact if any of the second inclusions in (5.3.1) 
and (5.3.2) is an equality. Thus the general Bayesian experiment 
is
both measurably separated and exact if and only if
(5.3.3) 
(B V M ) H M = (B V M )  ( M V T)

5.3. 
Measurable Separability in Bayesian Experiments
199
or, equivalently, if and only if:
(5.3.4) 
M  n (Ad V T) = (Ad V 7 ) (B V Ad).
But, by Proposition 4.3.10, Corollary 2.2.7 and elementary Properties 
4.3.2(i), 5.3.3 or 5.3.4 is also equivalent to
(5.3.5) 
B Â± T \ M ,
i.e., the Bayesian experiment 
is totally non informative. Thus, in
such a situation, the trivial parameter, i.e., (B V Ad) fl Ad, is both the minÂ­
imal sufficient parameter and exactly estimable parameter (this is (5.3.3)); 
and nothing is learned about any other parameter (this is (5.3.5)). TypÂ­
ically, a finite sample experiment would be measurably separated whereas 
an asymptotic experiment would be exact.
Verification of the measurable separability of the Bayesian experiment 
S'emt may be simplified by using Corollary 5.2.9. This leads to the following 
proposition.
5.3.2 Proposition. A Bayesian experiment Â£ ^ 7* is measurably separated 
if and only if one of the following three equivalent conditions holds.
(i) 
There exists an Â£ j^r -sufficient statistic Af measurably separated from 
the parameter i.e., B || A7 | Ad;
(ii) 
The full observation T  is measurably separated from an Â£ ^ r -sufficient 
parameter Â£, i.e., C || 7  | Ad;
(iii) The minimal Â£ j^r -sufficient strong statistic and the minimal Â£ ^ 7-  
sufficient strong parameter are measurabily separated conditionally 
on Ad i.e., ( B V M ) ( M  V T) || (Ad VT) (BV Ad) | Ad. 
1
In the dominated case, measurable separability is generally readily verÂ­
ified as a consequence of the fact that A  X S\ p <S> P is always true; then 
Corollary 2.2.11 and a simple application of Proposition 5.2.11 give the 
following result.

200
5. 
Optimal Reductions: Further Results
5.3.3 P roposition. In a dominated Bayesian experiment 
Â£ = (A x 5, A  V Â«S, II) , if II ~  p Â®  P then V B C A, C C A, T c 5 ,  
Â« C 5 ,  
is measurably separated. 
â 
We conclude this section by discussing the definition of a measurably 
separated experiment and some of the connections between this definition 
and certain other previously defined characterizations of a Bayesian experÂ­
iment. For expository reasons, we restrain our attention to the unreduced 
experiment Â£ = Â£a v s -
By definition, Â£ is measurably separated if the maximal exactly esÂ­
timable parameter *4fl5 is the trivial sub-cr-field of A  or equivalently if the 
maximal exactly estimating statistic A  fl S is the trivial sub-cr-field of Â«S. 
As seen in Section 4.7, or by Formula (5.2.4), a set E in A  H S is entirely 
characterized by the fact that E E A  and [ps (E ) ] 2 = ps (E ), since this 
is equivalent to ps (E ) = 1e a.s.II. Similarly, a set X  in A  fl S  is charÂ­
acterized by the fact that X  E S and [PA (A )]2 = P A (X ), since this is 
equivalent to P A (X) = l x  a.s.II. It follows that the experiment Â£ will be 
m easurably separated if the following im plication holds:
E G A  
[fis (E)}2 = /  (E) => [Â»(E)f =p(E) 
or, equivalently, 
x e s  
[pa { X)}2 = p a (x)  = >  [ P p Q ]2 =  P ( x ) ,
i.e., Â£ is measurably separated if it does not exist a set X  E S (respecÂ­
tively, E E A) with 0 < P( X)  < 1 (respectively, 0 < p(E) < 1) such that 
the sampling probabilities (respectively, the posterior probabilities) are inÂ­
dicator functions. Such a set has been called a splitting set by Koehn and 
Thomas (1975). Note also that these properties are non-symmetric but 
they are equivalent to a symmetric one, i.e., A  fl S = X. Breiman, LeCam 
and Schwartz (1964) obtained the same result through a somewhat more 
involved argument (see also Skibinski (1960), who also treated families of 
probabilities).
5.3.2 
Basu Second Theorem
In this section, we provide a generalized Bayesian version of Basu second 
Theorem; this states that any statistic independent of a sufficient statistic

5.3. 
Measurable Separability in Bayesian Experiments
201
in the sampling process is ancillary.
5.3.4 Theorem . In a measurably separated Bayesian experiment Â£ ^ 7',
(i) 
Every Â£j^Jr -statistic 1Z sampling independent of an Â£ ^ r -sufficient 
statistic Af is ancillary. Moreover 1Z is predictively independent of Af.
(ii) 
Every 
-parameter 1C a posteriori independent of an 
-sufficient 
parameter C is ancillary. Moreover 1C is a priori independent of C.
Proof. By duality it suffices to prove the theorem for a statistic. By hyÂ­
pothesis 1Z JL Af | B y  AA. The sufficiency of Af, i.e., B JL T  \ AAW Af implies 
B X 1Z | AA V Af] the measurable separability of Â£ ^ 7', i.e., B || T  | AA, imÂ­
plies ^  || 
| AA. By Theorem 5.2.10, we obtain 7Z X (BV Af) \ AA, and
this implies 1Z X B \ AA, i.e., 1Z is ancillary and 1Z X Af \ AA, i.e., 1Z and 
Af are independent for the predictive probability. 
â 
The condition of measurable separability of the experiment Â£5^7- is not 
merely sufficient but also necessary.
5.3.5 Theorem . If, in a Bayesian experiment S j^r , every Â£5^7--statistic 
sampling independent of an Â£g^r -sufficient statistic is ancillary, then the 
experiment is measurably separated.
Proof. Afinax = {B V A4)n(A4 V T), the maximal exactly estimating Â£5^7^- 
statistic, is clearly independent in the sampling process of any Â£ j^r -sufficient 
statistic (in particular of T), since Afm3,x C B V AA. It is therefore ancillary,
i.e., Afmax JL B \ AA. But this is equivalent to AfmdLX JL B y  AA \ AA, and so 
Afinax X Amax | Ad. By Corollary 2.2.8 this is equivalent to A/*max C AA, or 
equivalently, Afm3iX = Adfl(Ad VT), i.e., the experiment Â£g^r  is measurably 
separated. 
â 
Note that this proof uses the hypothesis of Theorem 5.3.5 for Afmax 
only. This theorem cannot, however, be improved since once measurable 
separability has been verified from the ancillarity of Afm3X, Theorem 5.3.4 
implies the ancillarity of any other Â£^Jr -statistic sampling independent of 
a sufficient statistic.

202
5. 
Optimal Reductions: Further Results
We may point out that in the proof of Theorem 5.3.4, we use only the 
measurable separability of the parameter and the sufficient statistic, i.e., 
B || M  | M . But when Af is sufficient Proposition 5.3.2 (i) shows that 
this assumption is actually equivalent to B || T  | A4, i.e., the measurable 
separability of S ^ r .
5.3.3 
Sampling Theory and Bayesian Methods
Let us consider a statistical experiment Â£ = {(5, S ) , P a, a â¬ A} and A  
a cr-field on A which makes P a (X) measurable for any X  E S. We use the 
same notation as in Section 2.3.7.
In 1955 Basu stated that any statistic U C S independent of a suffiÂ­
cient statistic T  C 5, i.e., U X T ; P a, Va E A, is ancillary. Basu (1958) 
subsequently qualified his statement by giving a sufficient condition for the 
theorem to be true, i.e., T does not contain a splitting set.
5.3.6 Definition. A set X  Â£  S  is a splitting set if there exists a set E E A  
non-trivial (i.e., E ^  <j> and E ^  A) such that Pa (X) = ljg (a). 
â 
We then deduce the following characterization.
5.3.7 Proposition. A statistic T  C S  does not contain a splitting set if 
and only if the following implication holds:
X E T ,  [Pa (X)]2 = P a (X), Va <E A, imply
either P a (X) = 1 Va G A 
or 
P a (X) = 0 
V a â¬ A. 
â¢
More recently Khoen and Thomas (1975) proved that this condition is 
also necessary, i.e., any statistic U C S  independent of a sufficient statistic 
T C S  is ancillary if and only if T  does not contain a splitting set. For 
the sake of completeness, we reproduce the proof: Let u E [7/]+ . Since T  
is sufficient, there exists t E [T]+ such that T au = t a.s.Pa, V o E A  
But U X  T ; P a, V a E A, implies T au = Xau a.s.Pa, V a E A, and so 
t = Xau a.s.Pa, Va E A. Fix ao E A and define X  = {t = XaÂ°u}. Clearly, 
[Pa ( X ) ] 2 = P a (X) 
Va Â£ A. Now, since P aÂ° (X) = 1, if T  does not 
contain a splitting set P a (X) = 1, V a E A. Thus T au = XaÂ°u a.s.Pa,

5.4. Strong Identification of cr-Fields
203
Va Â£ A, and this implies l au = TaÂ°tz, Va Â£ j4, i.e., W is ancillary. For 
necessity it suffices to note that, if X is a splitting set and U is the cr-field 
generated by X , then U is not ancillary whereas U JL S ; P a, Va Â£ A. 
â 
Let us now compare the sampling theory condition, i.e., the non-existence 
of a splitting set in T, with the Bayesian condition, i.e., the measurable sepÂ­
arability of A  and T  . Recall that for a prior probability fi , VX Â£ <S, 
P A (X ) = P a (X) a.s./i. In the previous section we saw that A  and T  are 
measurably separated if and only if the following implication holds:
X  6  T, 
[PA ( X ) ] 2 = PA (X) = >  [P(X)]2 =  P  (X ).
To make precise the role of the prior probability fi, this condition may 
be rephrased as follows:
5.3.8 Proposition. A  and T  are measurably separated (A || T) if and only 
if the following implication holds:
X â¬ T, [PÂ° (X)]2 = Pa (X) 
a.s.// imply
either P a (X) = 1 
a.s./i 
, or 
P a (X) = 0 
a.s./i. 
â 
In view of Propositions 5.3.7 and 5.3.8, it is clear that if // is a prior 
probability such that the predictive probability dominates all the sampling 
probabilities, then A  \\ T  implies the non-existence of a splitting set in 
T. This will be true, in particular, if fi is regular (Definition 2.3.13). If, 
moreover, fi possesses the property that [Pa (X )]2 = P a (X) a.s./i implies 
[Pa (X)]2 = P a (X) , Va Â£ A, then the non-existence of a splitting set in 
T  is equivalent to the measurable separability of A  and T.
5.4 
Strong Identification of a-Fields
In this section, we treat a topic in general probability theory and adopt 
the usual notation. In particular, (M ,A t,P ) is an abstract probability 
space.
The question raised by Basu First Theorem in the introduction is actuÂ­
ally equivalent to the following problem: IfA ^ iX A ^ 2 |-^ 3 and M \  JL M  2,

204
5. 
Optimal Reductions: Further Results
under w hat condition on .M3 and M 2  is it true th at M i  X  (M 2  V .M 3)? 
T h is condition is called the strong identifiability of .M3 by Af2- We will 
generalize this condition so as to handle a conditional version of this probÂ­
lem . The identifiability of M3 by .M 2, as defined in C hapter 4 (D efinition 
4.5.1), is weaker than (since im plied by) the strong identification of M 3  by 
.M 2â¢ We use Section 4.5 as a guide in collecting general properties of the 
strong identification.
5.4.1 
Definition and General Properties
For reasons explained below, we introduce the concept of strong identiÂ­
fication in Lp-spaces.
5.4.1 D e fin itio n . M\  is strongly p-identified by M2 conditionally on M3 
or, equivalently, M 2 strongly p-identifies M\ conditionally on M3 if the 
following im plication holds:
m G [M\ V M 3}p 
and 
(M2 V A f 3) ra =  0 = >  m  =  0 
a.s.
In this case we write M i  <CP M 2  | M 3 .  If M 3  =  M o  we say that M i  is 
strongly p-identified by M 2  and we write M i  <Cp M 2 â¢ If P â 00, we sim ply 
say th at M i  is strongly identified by M 2  conditionally on M 3  and write 
M i  <C A f 2 | M3 .  If we want to m ake explicit the role of the probability P, 
we write A fi <CP A f2 | A f3; P . 
â 
We first m ake som e general rem arks about the above definition.
(i) In order to m ake the role of A f3 more explicit, note th at the im plication 
in the definition is actually equivalent to the following:
m Â£  [A fi V A f3]p and (A f2 V A f3) m G A f3 = >  m G A f3-
(ii) N ote also that A fi <CP A f2 | M3 is equivalent to
(A f 1 V A f 3) <CP (A f 2 V A f 3 ).
C onsequently properties stated  conditionally on M3 need be proved for 
A f 3 =  A f 0 only.
(iii) Remark that A fi <CP A f2 | M3 implies A fi <CP' A f2 I A I3 
Vp < p' < 00 since in that case [A fi V A f3]p/ C [A fi V A f3]p .

5.4. Strong Identification of a-Fields
205
W ith few exceptions, the m ost useful theorem s require only the w eakest 
version of strong identification, i.e., p =  oo. Therefore even if the general 
properties of strong identification are true for any p Â£  [1, oo], in the sequel 
they are stated  and proved only for p =  oo.
We now collect som e elem entary properties of strong identification. 
They do not require proof as they are sim ple consequences of the definiÂ­
tion.
5.4.2 Proposition. Elementary properties of strong identification
(i) 
M \  <C M 2 | M 3 <=> M i < M
2 \ M s 4=> M \  <C M 2 | M 3
M i  <C M 2 | M 3 ]
(ii) 
M i  C M 2  V M 3  = >  M i  <  M 2  | M3;
(iii) Mi 2 C M i  V M 3  
and 
M i  <C M 2  | M 3  
= >  M i  V M 3 =  M 2  V M 3;
(iv ) M 2  C Mi4 V Mi3 
and 
M i  <C M 2  | M 3  = >  M i  <C M a | M3;
(v ) 
M $  C M i  V M 3  
and 
M i  <C M 2  | M 3  ==> M s  
M 2  | M3-  
â 
Note th at Property (v) is, in general, not true for identification (see the 
comment following 4.5.2).
The concept of strong identification m ay be interpreted in term s of the 
geom etry of B anach spaces. Indeed, for m Â£ [M \, let m  be the equivalence 
class of m (in [M]) with respect to P -alm ost sure equality. For M i ,  a sub-cr- 
field of M ,  denote by Lp 
the linear space | m  : m  Â£ [ M i ]p |  which is a
closed linear subspace of the B anach space LP( M )  (see, e.g., Neveu ( 1964) 
Section II.6 and IV .3). 
For any p Â£ [l,o o ], the conditional expectation  
given M i  m ay be viewed as a continuous linear operator defined on Lp( M )  
onto Lp 
it is in fact an orthogonal projector (i.e., sym m etric and
idem potent) also denoted as M i .  Let us denote by Np (T ) the null space 
in LP( M )  of a continuous linear operator T , i.e., the closed linear subspace 
defined by | m  Â£ Lp( M )  : T  m â 0 j .  W ith this notation Definition 5.4.1 
m ay be rew ritten (for 1 <  p <  00) as
(5.4.1) 
Np [m 2 V M 3 0 M 1  V M 3] =  Np \ M i  V M 3] .

206
5. 
Optimal Reductions: Further Results
Indeed, (5.4.1) m eans th at Af 2 V M 3  M i  V M 3  m  
= 0  im plies
( M i  V M 3 )  m â 0, since the other inclusion is trivial. 
T h is is clearly 
equivalent to Definition 5.4.1. T h us (5.4.1) m eans th at strong identification 
m ay be viewed as the injectivity of the M 2  V A f 3-conditional expectation  
operator, restricted to the p-integrable functions o f [ Mi  V -M3]. 
In the 
reflexive case, i.e., p E (l,o o ), this is equivalent to the surjectivity of the 
adjoint operator. T he next theorem  m akes use of th at feature in order to 
characterize strong identification by an approxim ation property of M i  V A f 3- 
m easurable functions by a sequence of M i  V M 3-conditional expectation  of 
A f 2 V M 3-m easurable functions.
5.4.3 Theorem. For any p E (1,00), 1 +  1 = 1, A fi 
A f2 | A f3 if and 
only if for each m E [A fi V M 3 ] q there exists a sequence
{ m k : 1 <  k <  00} C [Af2 V Af3]?
such th at A fo [| m â (A fi V A fs) rnk |9] â âº 0 
as 
Jc â âº 00.
In particular, M i ^ M
2 | Af3 if and only if, for each m  E [Afi V Afs]2,
there exists a sequence { m k : 1 < k <  00} C [Af 2 V Afs]2 such that 
Afo 
â (Afi V Af3)m*}2] â âº 0 
as 
Jc â âº 0.
Proof. R ecall th at Afi <Cp A f2 | A f3 is equivalent to
Np [aTTv~ M l o M T V M 3] = Np [jWi v x 3]
where the two sides of the expression are closed linear su bspaces o f the 
B anach space LP( M) .  Recall th at in a norm ed linear space the null space 
o f a linear operator is the perpendicular space to the range of the adjoint 
operator defined on the dual space L q( M) .  Then, if we denote by R q (T) 
the range of the linear operator T  
: 
L q( M )  
â âº 
Lq( M) ,  
i.e.,
R q(T) =  | t  m  : m E Z ^ (A f)|, then A fi <CP A f2 | A f3 is equivalent to
(5.4.2) 
R q \m
x V M s o M 2 V A 43] A =  R t \ M i  V A 43] ^ .
T h is im plies that
(5.4.3) 
R t \ M i  V M 3 0 M 2 V M 3
= R q M 1 V M 3
Â±Â±

5.4. Strong Identification of a-Fields
207
If 1 < p < oo, then LP(M) is a reflexive Banach space: for such a space, 
if L is a closed linear subspace of its dual Lq( M ), then L1-1- = L. Recall 
also that in a normed linear space, the range of a linear operator is not 
necessarily closed, but the range of an idempotent operator (T 2 = T) is a 
closed linear space (since it is the null space of I âT). Hence, for 1 < p < oo,
(5.4.2) is equivalent to
{
(5.4.4) 
R q |A4i V 
j â R q Afi V Af3 o Af2 V Af3J
where the upper bar on the right hand side represents the closure of the 
linear space in the Banach space, and this implies Afi 
Af2 | Af3 when 
considering the perpendicular spaces.
Thus, (5.4.4) means that for any n E L q(M) there exists a sequence 
nk: 1 < Jc < 00 j  C Lq(M) such that Afi V Af3 |Af 2 V Af3 
converges
to Afi V Af3 n in L q(M) as Jc â âº 00, or, for any m E Lq (m  1 V Af3^
there exists a sequence ^m*: 1 < Jc < 001 C Lq ^Af2 V Af3^ such that
Afi V Af3 mjb converges to m in Lq(Af) as Jc â âº 00, this is clearly equivÂ­
alent to saying that for any m E [AfiVAfs]^ there exists a sequence 
{mjb : 1 < Jc < 00} C [Af2 V M f \ q such that (Afi V Afs) m % converge to 
m in [Af ]q. 
â 
The next theorem provides a sufficient condition for strong identification 
in the form of a representability condition of Afi-measurable functions by 
means of Af i-conditional expectation of Af 2-measurable functions.
5.4.4 Theorem . Let p E [l,oo]. 
If for any mi E [Afi]p there exists 
m2 E [M2]p such that mi = A fim 2, then Afi 
Af2 where 1/p + l/q  = 1.
Proof. Let m E [Af i]^ such that Af2?n = 0. Then, for any mi E [A fi]p 
representable as m i =  A fi m2 for m2 E [A f 2]^>
Afo (mmi) = Afo (m â¢ A fim 2) = Afo (mm2) = Afo (Af2m â¢ m2) = 0,
and this implies m = 0 a.s. 
â 
When p E (l,oo), this theorem is actually a particular application of 
Theorem 5.4.3, letting Af3 = Afo and m k = m2 
V Jc E W. Remark

208
5. 
Optimal Reductions: Further Results
th at M \  <Cq M 2  as soon as the above representation (m i =  A fi m2) holds 
m erely for a linear lattice in [M\]p generating [Mi]p .
5.4.2 Strong Identification and Conditional Independence
We first exam ine the relations existing between strong identification 
and conditional strong identification. In essence, the first theorem  states 
supplem entary hypotheses under which the conditioning cr-field in a condiÂ­
tional strong identification property m ay be increased; the second theorem  
states supplem entary hypotheses under which the strongly identifying cr- 
field m ay be reduced, and as a corollary, it reveals how a conditional strong 
identification im plies a strong identification.
5.4.5 T h e o r e m . Let M i  ( i =  1, 2, 3, 4) be sub-cr-fields of A f . If
(i) 
M 4 JL M 2 I M 3 V M i
(ii) 
M i  <  M 2 I M 3 ,
then
(iii) M i  
M 2 | M 3 V M 4.
N ote th at (i) is trivially satisfied if M \  C M i  V M 3 .
P r o o f. Take M 3  =  Mo .  Let m E [ Mi  V A f ^ ^  be such th at
{ M 2 V M 4) m â 0. Then, by Theorem  2.2.1 (iii), for any n E
(i) 
im plies th at M 2  [ Mi  (m n)] =  M 2  {mn). B ut
M 2  (m n) =  M 2 [n {M2 V M 4) m] =  0 and so, by (ii), M i  {mn) =  0.
Therefore for any n E [ M ^ ^  and r E [A41 ]^ , A4o (m n r) =  0 and, by
Theorem  0.2.21, this shows th at m =  0 a.s. 
â 
5.4.6 T h e o r e m . Let M i  ( i =  1, 2, 3, 4) be sub-<r-fields of M .  If
(i) 
M 4 X  A fi | A f3 V A f2

5.4. Strong Identification of a-Fields
209
(ii) 
M \  <  ( M2  V M a) | M$ ,  
then
(iii) M \  <  M 2  | M s .
N ote th at (i) is trivially satisfied if M a C Ad2 V Ad3.
P r o o f. Take M s  â Ado- Let m  Â£  [A d i]^ be such th at A^2m  =  0- By 
Theorem  2.2.1 (ii), (i) im plies that (A d 2V A d 4)m  =  A ^ m . 
Therefore 
(A d2 V Ad4) m =  0 and, by (ii), this im plies m â 0 a.s. 
â 
Elem entary Property 5.4.2(v) shows th at Adi <C Ad2 | Ad3 V M a  im Â­
plies Adi <C (A d2 V Ad4) | Ad3. Th is leads to the following straightforw ard 
corollary:
5.4.7 Corollary. Let M i ( i  = 1,2, 3,4) be sub-cr-fields of Ad. If
(i) 
Ad4 X  Adi | Ad3 V A d 2
(ii) 
Adi <  A d2 | Ad3 V A d4,
then
(iii) A d i < A d 2 |A d 3.
N ote th at (i) is trivially satisfied ifA d 4 C A d 2 V A d 3 . 
â 
R em ark th at Corollary 5.4.7 is rather trivial w hereas the analogous 
result for identification, Corollary 4.5.5, w as a consequence of elem entary 
P roperty 4.5.2(v), the proof of which was rather involved. C om bining T h eÂ­
orem s 5.4.5, 5.4.6 and C orollary 5.4.7, entails the following corollary:

210
5. 
Optimal Reductions: Further Results
5.4.8 Corollary. Let Af* (i = 1, 2, 3, 4) be sub-cr-fields of M .  If
(0) 
M 4 JL (Afi V A f 2) | A f 3
the following strong identification properties are equivalent:
(1) M i < M 2 \ M 3;
(ii) 
M i  <  (Af 2 
V M 4 )  | M 3\
(iii) 
M i  <  M 2 I M 3  V A ^4- 
â 
R estricting the conditional cr-field in Theorem  5.4.6 to be the m inim al 
one we obtain, as a corollary:
5.4.9 Corollary. Let M 3  C M i , M 6  C M 2. Then, for any M 4  C M 2,
(i) 
M 4  <  M i  | M 3  V M q <^=> M 4  
A f i M 2 \ M 3  V A fe;
(ii) 
A ^ 4  <  A fi | M 3  V A f 6 A f 4 <  M 2M i 
\ M 3  V A f 6 â 
Proof. B y Corollary 2.2.11 and Theorem  4.3.3, we successively have:
M i  X  M 4  | A fiA f 2 V A f5 V A f6 
M i  X  M 4  | M 2M i  V M 3  V A f 6-
Therefore M \  <C A fi | M 3  V A f6 im plies A f4 <C A f 1 A f2 | A fs V A f6, by 
Theorem  5.4.6, and the reverse im plication is trivial. However, by elem enÂ­
tary Property 5.4.2(iv), A f4 <C A fi | A fs V A f6 im plies
A f 4 <  (Afi V A f 2A f 1 ) | A f 5 V A f 6 
and, by Theorem 5.4.6, this implies A f 4 
M 2M
1 | Afs V A f 6 - 
â 
T he next theorem  shows that a conditional independence im plies the 
transitivity of strong identification.
5.4.10 Theorem . Let Af* (i = 1,2, 3,4) be sub-cr-fields of Af. If

5.4. Strong Identification of v-Fields
211
(i) 
Mi  Â± M 2 \ M 3 V M 4,
(ii) 
M 1 < M 4 \ M 3,
(iii) M 4 < M 2 \ M 3)
then
(iv) M\  <  M 2 | M 3.
Proof. Take M 3 = M o . By Theorem 2.2.1 (iii), for any m Â£ [Afi]^, 
M 2m = M 2 { M 4 m). If M 2m â 0, by condition (iii), M 4m = 0 and, by 
condition (ii), m =  0 a.s. 
â 
We now investigate the connection between identification and strong 
identification. The next theorem exhibits an incompatibility between indeÂ­
pendence and strong identification.
5.4.11 Theorem . Let Mi { i = 1, 2, 3,4) be sub-<r-fields of M.  If
(i) 
Mi  X M 2 | M 3
(ii) 
Mi  <  M 2 | M 3,
then
(iii) Mi  C M 3.
Proof. Let 
m Â£ [A^i]^. Under (i), we have ( M 2 V M 3)m  = M 3m 
by 2.2.1(ii). Therefore ( M 2 V M 3) {m â M 3 m} = 0. Clearly, we also 
have m -  M 3m Â£ [Afi V M s]^- So by (ii), m â M 3m = 0 a.s., i.e.,
m Â£
Ms
This theorem entails the fact that strong identification implies identifiÂ­
cation.

212
5. 
Optimal Reductions: Further Results
5.4.12 Theorem . Let Af; ( i = 1,2,3,4) be sub-cr-fields of M . If 
M i  <  M 2 | M 3 then M \  < M 2 \ M 3 .
Proof. Take M 3 = M o . Clearly, Af 1 X  Af2 I Af iA f2- 
By Theorem 
5.4.5, with M 4 = Af iAf 2, M \  <C M 2 implies that M i  <  M 2 \ Af 1 Af2- 
Therefore by Theorem 5.4.11, Afi C Af iA f2, and this is equivalent to 
M i  â M 1M 2 , i.e., M i  < Af 2. 
â 
We now provide the condition under which M i  X (M 2 V Af3) when 
M i  JL M 2 | M 3 and M i  JL M 2. This theorem may be viewed as a comÂ­
plement of Theorem 2.2.10.
5.4.13 Theorem . Let Mi ( i = 1, 2,3,4) be sub-cr-fields of M.  If
(i) 
M i  JL M 2 I M 3 ,
(ii) 
M i  JL M 2 | M 3 V Af4,
(iii) M 4 C M 2  |A43,
then
(iv) Afi X (Af2 V Ma) I Af3.
Proof. Since (ii) is equivalent to Afi X (Af2 V M 3 ) | M 3 V Af4, by 
Theorem 2.2.1 (iii), (Af 2 V M 3 ) [(Af3 V Af4) m] = (Af 2 V Af3) m for any 
m E [Adi]^. But, by (i) and 2.2.1 (ii), (A<2 V Af 3)771 = Afsra. Therefore
(Af 2 V M 3 ) [{M3 V Ma) m ~ Af3m] = 0.
By Condition (iii) this implies (Af 3 V Af 4) m = Af 3^1, 
namely,
Afi X Ma I A<3 and this, along with Condition (ii), is equivalent to (iv), 
by Theorem 2.2.10. 
â 
We end this section by showing that strong identification has useful imÂ­
plications for the theory of projection of cr-fields under a conditional indeÂ­
pendence relation. Indeed, if Afi X Af2 | M 3 , then by Corollary 4.3.6(v)

5.4. Strong Identification of cr-Fields
213
we have
M 2M 1 â M 2 { M iM 3) C M 2 { M 3M i )  C M 2M 3 .
Assumptions of strong p-identification with p E (1,00) will give equaliÂ­
ties instead of inclusions in the above relation.
5.4.14 Theorem . Let p E (l,oo) and M i {i = 1,2,3) be sub-<r-fields of 
M  such that:
(0) 
M i  Â±  M 2 I M 3 ,
(1) 
if M 3 M 1 <^p M i  
then M 2M 1 = M 2 { M 3 M 1) C M 2M 3 ]
(ii) if M 3< Â£ p M
i  
then 
M 2M 1 =  M 2 { M 3M 1) =  M 2M 3 .
Proof. Let m E [M ^M f]^. If M 3M 1 
Afi for some p E (l,oo) then, 
as a consequence of Theorem 5.4.3, there exists a sequence
{mk : 1 < k < 00} C [M\]q 
with 
l/p  + l/q  = 1
such that {M3M1) rrik ââº m a.s., and in [M]q as k â*â¢ 00. Since
M i  Â± M
3 \ M 3M i ,
we have, by Theorem 2.2.1(ii), that { M 3Mi)rrik = M 3 m k V Jc. And so 
M sm k ââº m in [Af]? as 
ââº 00. Now, for any Af2, Af2(Af3^fc) ââº Af2^  
in [Af]? as 
â> 00 since the conditional expectation is a continuous linear 
operator from [M]q to [M]q. Then there exists a subsequence
{rriki : 1 < Â£ < 00} C [Mi]q
such that A<2 (Af3mjfc/ ) ââº Af2^n 
a.s. 
Now, by Theorem 2.2.1(iii), if 
Afi JL M 2 | Af3, A<2(Af3mjbJ = M 2m kl. Hence M 2m kl â> Af2^n a.s.
as ^ ââº 00. 
This implies that M 2 m E Af 2 Afi. 
This is true for any
m E [AfsAfi]^ and so Af2(AfsAfi) C Af2Afi, and this is an equality, 
by 4.3.6(v). To prove (ii) it suffices to notice that, by the same argument, 
M 2 m E Af2Af 1 for any m E [M 3]mi and so M 2M 3 C M 2M 1 which, 
along with M 2M 1 C Af 2 (Af 3 Afi) C Af 2 Af 3, gives the result. 
â 

214
5. 
Optimal Reductions: Further Results
If we trivialize the conditional independence relation by letting 
M2 C M3 , then we see that strong p-identification of M3M1 by Mi  
is a supplementary condition that provides a result similar to the three- 
perpendicular theorem for projections of <r-fields.
5.4.15 Corollary. Let p E (l,oo) and M i  ( i = 1,2,3) be sub-<r-fields of
M .  If
(i) 
M 2 C M 3
(ii) 
M3M1 < p M i,
then
(iii) M2 {M3M\ )  = M2M1. 
m
We conclude this section by treating the problem of minimal splitting, 
i.e., the problem of characterizing the minimal cr-fields conditionally on 
which two given cr-fields are independent. We show that this concept is in 
fact intermediate between identification and strong identification.
5.4.3 
Minimal Splitting
We have seen, in Theorem 4.3.3, that the projection <r-field M 1 M 2  
is the smallest sub-cr-field of M i ,  which makes M i  and M2 conditionally 
independent. It would be interesting to characterize and to construct all 
sub-cr-fields of M  which are minimal in the class of the cr-fields that make 
M i  and M2 conditionally independent. This section treats a topic in probÂ­
ability theory which will not be used directly in the statistical sections. 
This subsection may therefore be skipped without inconvenience for what 
follows; it has nevertheless been included in this monograph not only for its 
intrinsic interest, but also because this problem has received a good deal of 
attention in system theory under the heading of âthe cr-algebraic realization 
problemâ (see, e.g., Lindquist, Picci and Ruckebusch (1979), Lindquist and 
Picci (1979), or Picci (1976)). The next definition uses the same terminology 
as in system theory and explains the title of this section:

5.4. Strong Identification of cr-Fields
215
5.4.16 D efinition. Let M i ( i = 1,2,3) be three sub-c-fields of M .
(i) 
M3 splits (Mi, M2) if Mi  X M2 | M3;
(ii) M 3 minimally splits ( Mi , M2) if
(a) M3 splits ( M1,M2)
(b) M 4 C M3 splits ( Mi , M2) => M3 = Ma-
(iii) M3 minimally and internally splits (M i, M3) if
(a) M3 minimally splits (Mi, M2) and
(b) M 3 C M 1 V M 2 -
Thus, in this parlance, Theorem 4.3.3 says that M1M2 splits minimally
and internally ( Mi , .M2), and is the unique sub-cr-field to enjoy this propÂ­
erty in the class of the sub-cr-fields of Mi . In this subsection we wish to 
investigate the class of all <r-fields that minimally split (Mi, M2) without 
requiring them to be sub-<r-fields of Mi  or of M2- We first characterize that 
class by means of necessary and of sufficient conditions. The next theorem 
shows that identification is a necessary condition for minimal splitting.
5.4.17 Theorem . If M 3 minimally splits (M i, M 2 ) then M 3 is identified 
both by M i  and by M 2 -
Proof. Corollary 4.3.5 (ii) shows that if M3 minimally splits (M i, M2), 
then M3 
= 
M3M2 therefore, by elementary Property 4.3.2(viii), 
M3 = M3M2 and the equality M3 â M3M1 is obtained by symmetry. â 
Next we show that identification by M i  and strong identification by 
M2 (or vice versa) is a sufficient condition for minimal splitting.
5.4.18 Theorem . If
(i) 
M 3 splits (Mi, M2) (i.e, Mi  X  M2 \ M3),
(ii) 
M3 is identified by M i  (i.e. M3 < M i), and
(iii) Ml3 is strongly identified by M2 (i*e. M3 
M2) 
then M3 minimally splits (Mi,M2)-

216
5. 
Optimal Reductions: Further Results
Proof. Suppose that Af4 C M 3 also splits (M  1, ^ 2); we show that 
assumptions (i) to (iii) imply M3 C M a â¢ 
Indeed, by Theorem 5.4.5, 
Ma C M3 and (iii) M3 <  M 2 imply M3 <  M 2 | M 4; therefore, by 
Theorem 5.4.13, Af 1 X (.M2 V .M3) | Af4 which implies Afi X M3 \ Ma; 
hence, by Theorem 4.3.3 and because Ma C M3 , we obtain .M3.M1 C 
Ma- Since M3 is identified by Afi (i.e., Af3 < Afi), we conclude that 
Af3 C Af4. 
â 
We know, from Theorem 5.4.12, that identification of cr-fields is weaker 
than strong identification. It may be conjectured that identification by one 
factor and strong identification by the other one are likely to be the weakÂ­
est sufficient conditions for minimal splitting; such a view is justified by 
the following arguments: (i) in the linear theory (i.e., cr-fields replaced by 
linear subspaces of the Hilbert space of square-integrable zero expectation 
random variables and independence replaced by uncorrelatedness), the corÂ­
responding concept of identification and strong identification do coincide 
(see e.g., Lindquist, Picci and Ruckebusch (1979), or Mouchart and Rolin 
(1986), Appendix B) and recall that in the Gaussian case uncorrelatedness 
is equivalent to independence; (ii) if we restrict Af3 to be a sub-cr-field of 
Af 2j then by Theorem 4.3.3, Af3 minimally splits (Afi,Af2) if and only if 
Af3 = Af 2 Afi. However, when Af3 C .M2, Af3 is trivially identified and 
strongly identified by Af2 (elementary Properties 4.3.2(ii) and 5.4.2(h)); 
therefore Af3 C Af2 minimally splits (Afi,Af2) if and only if Af3 splits 
(A fi,.M2) and is identified by Afi. Furthermore, we shall see that minimal 
splitting may in fact be seen as a property intermediate between identificaÂ­
tion and strong identification.
As a step toward the construction of such cr-fields, let us now turn to the 
representations of minimal splitting cr-fields. We have seen that any minimal 
splitting cr-field Af3 is by necessity splitting (Afi,Af2) and identified by Afi 
(also by .M2). These two properties are equivalent to the representability 
of Ms  as a projection of Afi on a cr-field containing M2, and is the focus 
of the next theorem.
5.4.19 T heorem . The following two conditions:
(i) 
Af3 splits (Afi,Af2) (i.e. Afi X Af2 | Af3) and

5.4. Strong Identification of <r-Fields
217
(ii) M 3 is identified by M i (i.e. M 3M 1 = .M3) 
are equivalent to:
(iii) 3 M 4 D M 2 such that M3 = A f ^ i .
If furthermore M3 C -Mi V M2 (i.e., internal splitting), and MÂ± satisfies
(iii) and
(iv) Ma C M i V M 2 ,
then M 4 is almost surely unique, i.e., M \  = M 2 V M 3.
Proof. To prove that (i) and (ii) imply (iii), it suffices to choose
(v) 
Ma â M2 V M3]
(iii) is obtained by remarking that { M 2 V M 3) -Mi = M 3M 1, by (i) and 
by Corollary 4.3.6(iii), and that M 3M 1 = -M3 by (ii). To prove that (iii) 
implies (i) and (ii), we first recall that M i 1  M \  \ M \ M \  for any M 4 , 
by Theorem 4.3.3; thus (iii) implies M \  X -M 4 | M 3 and this implies (i) 
because M 4 D M 2. Next note that by elementary Property 4.3.2(vii) and 
Corollary 4.3.8, (iii) successively implies
M 3M 1 = (M 4M 1) M i = M 4M 1 = M l;
this gives (ii) by taking the intersection with M 3 on both sides of the last 
equality. It remains to show that in the case of internal splitting M 4 is 
essentially unique if we also require (iv). Theorem 4.3.9 (ii) and elementary 
Property 4.3.2(ii) successively imply
M 2 V (M 4M 1) = M 4 (M i V M 2) â M 4.
The first equality holds because M 2 C M 4 and the second equality because 
M 4 C M i V M 2, and therefore, in view of (iii), M 4 = M 2 V M 3. 
â 
An important corollary of this theorem is to show that minimal splitting 
is a condition intermediate between identification and strong identification,

218
5. 
Optimal Reductions: Further Results
once it is recognized that any minimal splitting <r-field is representable as 
the projection of one factor. More specifically, we have:
5.4.20 Corollary. For any Ma D M 2 â¢ M 4 M 1 <C M 2 => M 4M 1 miniÂ­
mally splits ( M i , M 2) => M 4M 1 < M 2 - 
â 
Thus we have seen, in Theorem 5.4.17, that any minimal splitting cr- 
field is identified by each of its splitted (r-fields and, in Theorem 5.4.19, that 
this implies its double representability as a projection of one of these factors 
on a cr-field containing the other factor. The next theorem establishes the 
duality between these two representations with respect to M \  and M 2 , 
and shows that those representations may be taken as mutually symmetric 
and are in bijection, furthermore in the case of internal splitting, they are 
essentially unique and are in bijection.
5.4.21 Theorem . If M z  minimally splits ( M i,M 2) then
(i) 
3M4 D M2 and Ms D Mi  such that M3 â M4M1 = M5M2;
(ii) (a) for given M a, Ms may be taken as Ms â Mi  V M4M1,
(b) for given M
s ,  M 4 may be taken as M 4 = M 2 V M
s M 2,
(iii) if, furthermore M 3 C M
i M
M 2 (internal splitting), then the represenÂ­
tation in (i) with M 2 C M 4 C M i  \!M 2 and M i  C M s  C M i  VM 2 
is essentialy unique, namely: M 4 = M 2 V M 3 and M s  â M \  V M 3 .
Proof, (i) is clear from Theorems 5.4.17 and 5.4.19. For (ii), suppose that 
M 4 satisfies (i); then, from Theorem 5.4.17, we have that
(M4M1) M 2 = M4M1
and, by Corollary 4.3.5(ii)
(M4M1) M2 = ( M i V M 4M i ) M 2
because, from Theorem 4.3.3, we have M i  JL M 4 | M 4M 1 which, 
along with M 2 C M 4 , implies M i  JL M 2 | M 4M 1 . Therefore taking 
M s  â M i  V M 4M 1 ensures M 5M 2 = M 4M 1 . The very same argument

5.4. Strong Identification of a-Fields
219
applies for M a taken as M a = 
M s M i  with M s  satisfying (i). Finally,
(iii) is a direct consequence of Theorem 5.4.19 (iii). 
â 
Let us now consider the construction of minimal splitting a*-fields. From 
Theorem 5.4.18 and Corollary 5.4.20, any cr-field Ma such that M a D M 2 
and M a M \  <C M 2 may be used because those properties are sufficient to 
imply that M a M \  minimally splits (M i , M 2)â¢ As the strong identification 
of M aM \  (by M 2) may be difficult to check, it may be useful to first check 
a weaker (but more easily verified) condition as a screening test before a 
more complete examination; this is the goal of the following theorem:
5.4.22 Theorem . If
(i) 
Ma D M 2
(ii) 
M aM i < M 2, 
then
(iii) M i  X  Ma \ M 1M 2 - 
If, furthermore,
(iv ) Ma C M i V  M 2 (internal splitting) 
then
(v ) 
M a C M 2  V M \ M 2 -
Proof. We first prove an auxiliary result:
5.4.23 Lem m a. 
If
(v i) M i  X M 2  | M 3 ,
(v ii) M 3 < M 2 ,
(v iii) Ms C Mi  V M3
(ix) M i  X M 2 I M s, 
then
(x ) 
M i  X ( M2 V M 3 )  | M s -

220
5. 
Optimal Reductions: Further Results
P ro o f of th e lem m a: Remark that under (viii), (vi) is equivalent to:
(xi) (Mi V Ms) JL M2 | M3
and, by Theorem 2.2.10, (xi) is equivalent to the following two conditions:
(xii) M s  JL M 2 | M 3 and 
(xili) M i  X M2 | M3 V Ms-
By Theorem 5.4.5, (vii) along with (xii) implies that
(xiv) M3 <  M2 | Ms-
Finally, by Theorem 5.4.13, (ix) along with (xiii) and (xiv) implies (x).
P ro o f of th e theorem : 
Take, in the lemma, M3 = M4M1 and
M s  = M1M2- Clearly, (vi), (vii), (viii) and (ix) are satisfied; thus (x) 
is also satisfied and becomes:
(xv) M i  JL (M2 VM4M1) | M1M2-
In order to show that (iii) and (xv) are equivalent (under (i) and (ii)), we 
note that, by Theorem 2.2.10, (iii) is equivalent to (xv) along with
(xvi) Mi  X M4 | M4M1 V M2 V M1M2-
Clearly (xvi) always holds because of Corollary 2.2.11 and Theorem 4.3.3. 
Finally, in the internal case, (v) is shown by first noticing that, by Theorem
2.2.10, 
(iii) is also equivalent to M \  A. M 2 | M 1M 2 (which is always true, 
by Theorem 4.3.3) along with
(xvii) Mi  X M4 | M1M2 V M 2;
but by Corollary 2.2.11, (xvii) is also equivalent to:

5.4. Strong Identification of a-Fields
221
(xviii) (M1VM2) JL M 4 | M1M2 V M2-
Now, under (iv), (xviii) implies (v) by Corollary 2.2.8. 
â 
Looking at the linear theory as elaborated in Lindquist and Picci (1979, 
1982) or Lindquist, Picci and Ruckebush (1979), it is natural to ask whether 
any minimal splitting (7-field is representable as M 3 = M 4M i  with 
M 2 C M 4 C M 2 V M i M 2- From Theorem 5.4.18 and Corollary 5.4.20 we 
know that conditions (i) and (ii) are sufficient to imply the minimal splitting 
of (M i,M 2) by M 4M !  and, from Theorem 5.4.22, we know that, in the 
internal case, M 4 C M 2 V M \ M 2 - It is therefore natural to ask whether
(v) might be either necessary or sufficient for minimal splitting. In view 
of the following two arguments, there is little hope that (v) is a sufficient 
condition. Firstly, we know of no case where conditional independence, 
along with some other non trivial condition, implies strong identification. 
Secondly, the fact that (i) and (ii) imply (v) is a corollary of the auxiliary 
lemma for the special case M s = M i  M 2 C M i  whereas the fact that (i) 
and (ii) imply minimal splitting may also be viewed as a corollary of the 
same auxiliary lemma but when M s  C M 3 â M 4 M i  (in this respect, it 
is indeed interesting to compare the proofs of Theorem 5.4.18 and of the 
auxiliary lemma).
To conclude this section, let us suggest the following algorithmic conÂ­
struction of a minimal splitting cr-field. Let M i  and M 2 be two given 
cr-algebras, then:
(i) 
construct the projection M i M 2 \
(ii) 
pick M 4 such that M2 C M 4 and
(a) (general) Mi  X M 4 | M1M2,
(b) (internal) M 4 C M2 V M1M2',
(iii) construct the projection M 4M i\
(iv) check whether M 4M i  < M 2 -
if no, M 4M i  is not minimal splitting; go back to (ii); 
if yes, proceed to (v);

222
5. 
Optimal Reductions: Further Results
(v) 
check whether M 4M 1 <C Ad 2:
if yes, M 4M 1 is minimal splitting; stop, 
if no : proceed to (vi);
(vi) check whether Ad4At 1 <C Adi:
if yes, Ad4Adi is minimal splitting; stop, 
if no, inconclusive; go back to (ii).
Note that this constructive procedure should end up by displaying a 
minimal splitting cr-algebra but does not pretend to build any minimal splitÂ­
ting cr-field.
5.5 Completeness in Bayesian Experiments
In this section we investigate the most important implications of strong 
identification in the general reduced experiment S ^ T . In the statistical 
literature the concept of strong identification among cr-fields is generally 
called a condition of completeness (see, e.g., Lehmann and Scheffe (1950), 
1955)); this motivates the following definition.
5.5.1 D efinition. In the Bayesian experiment S\
a
 statistic J\f is p- 
complete if Af <â¬>PB | Ad and a parameter C is p-complete if C<^PT  | Ad. 
In particular, in the unreduced experiment Â£, a statistic T  is p-complete if 
T  <CP A  and a parameter B is p-complete if B 
S. When p â 00, we say 
complete instead of 00-complete. 
â 
Note that the âbounded completenessâ condition of Lehmann and Scheffe 
(1955) corresponds to what we call complete (or oo-complete).
We next examine some links between completeness and sufficiency or 
minimal sufficiency. Then we show how completeness provides sufficient 
conditions for maximal ancillarity and how an apparent paradox in the 
theory of conditional inference can be resolved. In the last two sections whe 
analyze some relationships between sampling theory and Bayesian methods.
5.5.1 
Completeness and Sufficiency
As an application of Theorem 5.4.6, the following theorem shows that 
the completeness of a statistic does not require the identification of the 
experiment.

5.5. Completeness in Bayesian Experiments
223
5.5.2 T heorem . In the Bayesian experiment 
a statistic (respectively, 
parameter) is complete if and only if it is strongly identified conditionally 
on Ad by a sufficient parameter (respectively, statistic).
Proof. 
By duality, it suffices to prove the result for a statistic. Let 
Af C Ad V T  be a complete statistic, i.e., Af <C B \ Ad, and let Â£ C B V Ad 
be a sufficient parameter, i.e., B X  T  | Â£ V Ad. We have to show that 
Af 
#  | Ad if and only if Af <C Â£ | Ad. Note that the sufficiency of 
Â£  implies that B JL Af | Â£ V Ad. Since Z? V Â£ V Ad = Z? V Ad we have, 
by Theorem 5.4.6, that AT <C B | Ad, i.e., AT 
(BVÂ£) | Ad, implies 
Af <C Â£ | Ad. Now, by elementary Property 5.4.2(iv), Af 
Â£ | Ad implies 
Af <  #  | Ad. 
â 
Theorem 5.4.12 provides relationships between the concepts of comÂ­
pleteness and of identification.
5.5.3 P rop osition . In the Bayesian experiment S ^ T
(i) 
any complete statistic is identified, i.e.,
Af C Ad V T, Af < B \ M
=> Af < B \ M  or, equivalently, (Ad V Af) (5 V Ad) = Ad V Af;
(ii) 
a complete parameter is identified, i.e.,
Â£ C # V A d , Â£ < T | A d
= >  Â£ < T  | Ad or, equivalently, (Â£ V Ad) (Ad V T) = Â£V  Ad. 
â 
For expository reasons, we provide comment on Proposition 5.5.3 in 
the unreduced experiment Â£ for the parameter case only. Recall that if 
the experiment is identified, i.e., A  < S or equivalently A S  = A, then a 
parameter B C A  is not necessarily identified, i.e., BS = B need not be 
true. However, in view of Proposition 5.5.3, under the stronger property 
that A  is complete, i.e., A  <C S, then, by elementary Property 5.4.2(v), B 
is complete, i.e., B 
S, and is therefore identified, i.e., BS = B.
Let us now examine the links between the concepts of a complete statisÂ­
tic or parameter and the concepts of a sufficient statistic or parameter. 
Recall that in Theorem 4.6.4 and Proposition 4.6.10 we proved that a suffiÂ­
cient parameter (respectively, statistic) is identified if and only if it is almost

224
5. 
Optimal Reductions: Further Results
surely equal to the minimal sufficient parameter (respectively, statistic). In 
view of Proposition 5.5.3, this entails the following result: a complete suffiÂ­
cient parameter (respectively, statistic) is almost surely equal to the minimal 
sufficient parameter (respectively, statistic). It is interesting to remark that 
usually in the statistical literature, the concept of identification is introduced 
usually in the parameter space, whereas the stronger concept of completeÂ­
ness is introduced only in the sample space. This is due to the fact that to 
show that B C A  is identified, i.e., BS = #, requires the evaluation of samÂ­
pling expectations, whereas to show that B C A  is complete, i.e., 6 E [B]^ 
and Sb = 0 imply 6 = 0 a.s., requires the evaluation of posterior expectaÂ­
tions. Similarly, to show that T  C S is identified, i.e. T B  = T, requires the 
evaluation of posterior expectations, whereas to show that T  C S  is comÂ­
plete, i.e., t E PIqo and At = 0 imply t = 0 a.s., requires the evaluation of 
sampling expectations. Therefore, the classical result that a complete suffiÂ­
cient statistic is minimal sufficient requires a proof because, in the sampling 
theory approach, the concept of the identification of a statistic is missing. 
Consequently, and for the sake of comparison, we prove this result directly.
5.5.4 Theorem . In the Bayesian experiment
(i) 
any complete sufficient statistic Af (i.e., B X T  | AA V Af 
and Af <C B | Ai )  is minimal sufficient in the sense that
AA V A f n ( A i  VT) = ( M  V T ) ( B V A A ) ;
(ii) 
any complete sufficient parameter C (i.e., B X T  | Â£  V A4 and 
C 
T | M )  is minimal sufficient in the sense that
Z V a 7  fl (5 V Ai )  = (6 V AA) (AA V T ) .
P roo f. By duality it suffices to prove the result for a statistic. Now 
B JL T  \ AA V Af clearly implies B X 1Z | AA V Af where 
7Z = (AA V T) (B V AA) C A A V T .  Hence, by Theorem 5.4.5,
Af <C B | AA implies Af 
B \ 71 since AA C 71. However,
( B y  AA) X (AA V T) | 11 implies B JL Af \ 11. Therefore, by Theorem 
5.4.11, Af C n .  Hence, J i y J f  n ( M W T )  C K n ( M y T )  =  71. By 
Proposition 4.3.10,1Z C AA V Af, and this gives the result. 
â 

5.5. Completeness in Bayesian Experiments
225
The next theorem exhibits another relationship between sufficiency and 
conditional strong identification. It may be paraphrased as follows: if the 
full sample is strongly identified by the full parameter conditionally on a 
statistic, then the sample is equivalent to this statistic along with any sufÂ­
ficient statistic.
5.5.5 T h eorem .
(i) 
If Af is an 5 ^ r -sufficient statistic and It is an ^ ^ r -statistic such 
that T  is 
j^-complete, then AAV T  = AAV Af Vlt;
(ii) 
If C is an Â£ j^r -sufficient parameter and K, is an 
parameter such 
that B is ^y^-com plete, then B V  AA = /C V CV AA.
Proof. By duality it suffices to prove (i) only. By Corollary 2.2.11, if Af 
is sufficient, i.e., B X T  | AA V Af , then B JL T  | Af V Af V It. 
Since 
Af C AA VT, by Theorem 5.4.5, T  <C B | AAVlt, implies T  <C B | JAVJfVlt. 
Therefore, by Theorem 5.4.11, T  C AAV Af V It and this is equivalent to 
AA V T  - A A  V A f V l t  
â 
Let us now consider other useful implications of strong p-identification 
in statistical experiments. For sake of brevity we discuss some of these 
implications in the unreduced experiment S only.
It is very often the case that in an identified experiment Â£, the paramÂ­
eter A  is strongly p-identified by the observation for some p E (l,oo). By 
definition, this means b E [^4]^ and Sb = 0 imply b = 0 a.s. or, equivalently, 
there does not exist a non-trivial p-integrable parameter with 0 posterior 
expectation for almost all realizations of the sample. By Theorem 5.4.3, this 
is equivalent to saying that for any c E [A]q (i.e., g-integrable parameter) 
there exists a sequence of statistics {tk : 1 < k < oo} C [S]q such that their 
sampling expectations Atk converge a.s. p and in [.4]^ to c. This gives an 
interesting representation of the parameters.
In the sample space, if T is a statistic, T  <Cp A  means that t E [T]p and 
At = 0 implies t = 0 a.s. or, equivalently, that there does not exist a nonÂ­
trivial p-integrable statistic with 0 sampling expectation for almost all values 
of the parameter. Again, this is equivalent to saying that any g-integrable

226
5. 
Optimal Reductions: Further Results
statistic u e T  may be represented as an almost sure limit of posterior 
expectations of a sequence {&* : 1 < k < oo} of g-integrable parameters.
We now consider a useful implication of Corollary 5.4.15.
5.5.6 Theorem . In the unreduced Bayesian experiment Â£,
(i) 
If T is a p-complete sufficient statistic for some p E (l,oo), then for
any U C S, U T is the minimal sufficient statistic in Sa v u]
(ii) If B is a p-complete sufficient parameter for some p E (1, oo), then, for
any C C A, CB is the minimal sufficient parameter in Â£cvs-
Proof. Since T  is a complete sufficient statistic, it is almost surely equal to 
the minimal sufficient statistic, i.e., TfltS = S A  so S A  
A  by elementary 
Property 5.4.2(i). By Corollary 5.4.15, for any U C S, U (Â£-4) = UA. It 
now suffices to recall that U (Â«S*4) = U 
z=zU (t ^J = UT, by elementary
Property 4.3.2(vi). The same argument may be used to prove the parameter 
case. 
â 
Note that the computation oiU T  only involves the predictive probabilÂ­
ity and the computation of CB, the prior probability.
In the next section we examine relations with ancillarity and maximal 
ancillarity.
5.5.2 Completeness and Ancillarity
A first result linking completeness and ancillarity is the fact that a 
complete statistic does not contain a non-trivial ancillary statistic.
5.5.7 Theorem . In the Bayesian experiment Â£ ^ 7-, any ancillary statisÂ­
tic (respectively, parameter) included in a complete statistic (respectively, 
parameter) is trivial, i.e., is included in M  fl (M  V T) (respectively, in 
( B V M ) n M ) .
Proof. By duality it suffices to prove the result for a statistic. Now if 
Af C M  V T is included in a complete statistic, Af is also complete by

5.5. Completeness in Bayesian Experiments
227
elementary Property 5.4.2(v), i.e., Af <C B \ AA. If, furthermore, Af is 
ancillary, i.e., B JL Af \ AA, then by Theorem 5.4.11, Af C AA. 
â 
By Theorems 5.5.4 and 5.5.7, in order to show that the minimal sufficient 
statistic is not complete it suffices to show that it contains a non-trivial 
ancillary statistic. Here is a example of such a situation.
E xam ple 1. Let a E A â Mq and s = (x}y)f Â£ S =  Mq x Mq such that 
x JL y \ a, (x \ a) ~  exp (a) and (y | a) ~  exp (a-1). If p is equivalent to 
the Lebesgue measure on IRq , by Theorem 4.4.11, S = cr{s} is minimal 
sufficient. Now, if U â o {xy}, then clearly U C S and U is ancillary, i.e., 
U JL A  and, therefore, S  is not complete. 
â 
A straightforward application of Theorem 5.4.5 entails the following 
result concerning completeness and ancillarity.
5.5.8 P roposition .
(i) 
If Af is an 
complete statistic and 71 is an 5 ^ ^ -an c illa ry  statisÂ­
tic, then Af is an Â£j^^-com plete statistic;
(ii) 
if C is an Â£{^Jr -complete parameter and JC is an 
-ancillary paÂ­
rameter, then J\f is an ^^^-co m p lete parameter. 
â 
Let us recall that the Basu First Theorem states that a complete sufÂ­
ficient statistic (which is therefore minimal sufficient and does not contain 
any non trivial ancillary statistic) is independent of any ancillary statistic 
in the sampling process. We now extend this result in a general reduced 
Bayesian experiment.
5.5.9 T heorem . In the Bayesian experiment Â£^,71
(i) 
any ancillary statistic is sampling and predictively independent of a 
complete sufficient statistic.
(ii) 
any ancillary parameter is a priori and a posteriori independent of a 
complete sufficient parameter.

228
5. 
Optimal Reductions: Further Results
Proof. By duality, it suffices to prove the result for a statistic only. The 
statement (i) may be written as follows. Let Af C Af V T, such that 
Af <C B | Af and B X T  | Af V Af; for any TZ C Af V T  such that 
B X TI | A f, we have 
X Af \ B V Af and TZ X  Af | AA. Now, clearly,
B 1  72. | Af V A/*. This, along with B X  TZ | M  and Af <C B \ M  imply, 
by Theorem 5.4.13, that TZ X (B V Af) | Af, and the result follows from 
Theorem 2.2.10. 
â 
Here is an example of a non trivial application of Theorem 5.5.9: 
Exam ple 2. Let
such that (Xi \ a) ~  i.T (6, c), i.e., the sampling densities of X{ with respect to 
the Lebesgue measure on Mq are given by p(x{ | a) = (bc/T(c))xci~le~hXi.
is a 1-complete sufficient statistic in Â£, i.e., A  X  S  \ T  and T<Ci^4. HowÂ­
ever, if U â cr(u), B = <r(b) and C = <r(c), since pc is equivalent to the 
Lebesgue measure on iR j, U is a 1-complete sufficient statistic in 
i.e., B JL S | CM U, and U <Ci B | C. Now, since b is a scale parameter, 
if we define W = a(w) where w = vu"1, then W X  B | C, i.e., W is 
^ v 5-anciHary. Therefore, by Theorem 5.5.9, W X (B MU) \ C which imÂ­
plies W X U | A. Note that this result is non-trivial since it is not an 
easy matter to characterize the joint sampling distribution of the statistic 
t = (u, w)'. 
â 
Since ancillarity permits an admissible reduction by conditioning, knowÂ­
ing whether or not a given ancillary statistic is maximal is important. Such 
a criteria is given by the third theorem in Basu (1959), which may be paraÂ­
phrased as follows: if the full sample is equivalent to a given ancillary statisÂ­
tic along with a complete sufficient statistic, then that ancillary statistic is
a 
= 
(6, c)f G A = Mq x IRq 
and 
s 
= 
(xi,X 2 i . . . , x n)' Â£ S = (IRq) *1

5.5. Completeness in Bayesian Experiments
229
maximal ancillary. But whereas this result is a useful characterization of 
maximal ancillarity, it is useless from the point of view of reduction by conÂ­
ditioning. Indeed, in the unreduced experiment Â£, if T  C S  is complete 
sufficient, and if U C S  is ancillary, then by Theorem 5.5.9, U X (A V T) 
and, therefore, Â£avT and S^ vt are essentially the same, and so conditioning 
is irrelevant. From this standpoint, a result which is both of more practical 
interest and stronger than Basu Third Theorem is provided by conditional 
strong identification. This may be paraphrased as follows: an ancillary 
statistic, conditionally on which the full sample is complete, is maximal 
ancillary.
5.5.10 Theorem .
(i) 
If 7t  is an Â£i^Jr -ancillary statistic such that T  is fj^T^-complete,
then 7t  is an Â£ ^ r -maximal ancillary statistic;
ii) 
if K, is an Â£ j^r -ancillary parameter such that B is Â£Â§^j^-complete,
then 1C is an Â£ ^ r -maximal ancillary parameter.
Proof. By duality, it suffices to prove (i). Following definition 4.2.5, let 
V D 7t be an ^ ^ r -ancillary statistic, i.e., B X V \ M .  By Corollary 2.2.11, 
this implies B JLV \ M V l t .  Now, T  Â£^j^-com plete, i.e., T  <C B | M V 7Â£, 
implies V <C B \ M  V 1Z by elementary Property 5.4.2(v). Therefore, by 
Theorem 5A.ll, V C M  V 71, which means that It is maximal ancillary. â 
Note that Theorem 5.5.10 does not rely on the existence of a complete 
sufficient statistic. We now show that Basu Third Theorem is a corollary 
of Theorem 5.5.10.
5.5.11 Corollary. In the Bayesian experiment Â£ ^ r
(i) 
If AT is a complete sufficient statistic and if It is an ancillary statistic 
such that A4VT = M. V M  V It, then It is maximal ancillary;
(ii) if Â£ is a complete sufficient parameter and if K is an ancillary paramÂ­
eter such that B y  M  = /C V Â£ V M ,  then K is maximal ancillary.

230
5. 
Optimal Reductions: Further Results
Proof. By Theorem 5.5.9, Af complete sufficient and 1Z ancillary imply 
(B V Af) X 1Z | M . Therefore by Theorem 5.4.8 or by Proposition 5.5.8, 
Af complete, i.e., Af C  B \ M , is equivalent to Af <C B | M  V TZ. But 
this is equivalent to T  <C B | AA V 7Â£, since M  V T = M  V Af VTZ, i.e., 
T  is i^ ^ -c o m p le te  and it suffices to apply Theorem 5.5.10 to obtain the 
result. 
â 
Let us remark upon the following: under the condition of Theorem
5.5.10, i.e., if T  is i^ ^ -c o m p le te , then by Theorem 5.5.5, any 
sufficient statistic Af is such that AA\I F â AAM Af V 7Â£, and so Af is 
complete, i.e., Af <C B \ Ai V TZ. Therefore, when there exists a 
complete sufficient statistic, Theorem 5.5.10 and Corollary 5.5.11 are actuÂ­
ally equivalent; thus Theorem 5.5.10 only generalizes Corollary 5.5.11 when 
the minimal sufficient statistic is not complete; this often arises with an exÂ­
ponential family with (exact) restrictions on the parameters. But as noted 
before, this is the only case where further reduction by conditioning is relÂ­
evant and, in such a situation, only Theorem 5.5.10 may be used to prove 
that an ancillary statistic is maximal.
As will be shown in later examples, if the sufficient condition for maxÂ­
imal ancillarity given in Theorem 5.5.10 does not seem to be minimal, it 
nevertheless clarifies an apparent contradiction between the principle of sufÂ­
ficiency and the principle of ancillarity in successive reductions of experiÂ­
ments; this is the object of next section.
5.5.3 
Successive Reductions of a Bayesian Experiment
Let us consider the Bayesian experiment 
We may first marginalÂ­
ize Â£$/? on the minimal sufficient statistic, i.e.,
and next condition 
on a maximal ancillary statistic 7Z, i.e., on a 1Z
such that
(5.5.1)
TV* = (M  V T) (Â£ V M )
(5.5.2)
B Â± T Z \ M  
U c M V A f
to end up with 
as a basis for inference.

5.5. Completeness in Bayesian Experiments
231
However, we may first condition on a maximal ancillary statistic V, i.e., 
on a V such that
(5.5.3) 
B Â± V \ M  
V C  M V T
and afterwards marginalize Â£&Â§jÂ£ on the minimal sufficient statistic, i.e.,
(5.5.4) 
O = (M  V T) (B V M  V V)
to end up with Â£'j^/SQ> as a basis for inference.
Now clearly, by Theorem 4.3.9 (ii), one has
(5.5.5) 
0 = W A f )
and so Â£ ^ q > is equivalent to 
in the sense that their probabilities
are the same. Therefore, these two successive admissible reductions end up 
with different final reductions, viz., 
with 72 C AA V Af and Â£ $ $
with V C M W  T. Restricting attention to a statistic V larger than 72, i.e.,
(5.5.6) 
72 C M  V7>,
the above paradox may be paraphrased as follows: as soon as an experiÂ­
ment Â£gMj' has been reduced by marginalization on the minimal sufficient 
statistic Af and afterwards reduced by conditioning on a maximal ancillary 
statistic 72 included in this minimal sufficient statistic, is there any interest 
in reducing it further by conditioning on a larger maximal ancillary statistic 
V in the initial experiment? The answer is negative if the probabilities are 
the same, i.e.,
(5.5.7) 
(BVAf) Â± V  \ M  V72 
for all statistics V satisfying
(5.5.8) 
B JLV \ AAW1Z 
V C  M V T .
Indeed, when 72 is Â£jÂ£/tf-ancillary, i.e., B X 72 | M , 
any V such that 
72 C M  V V will be Â£$,^-ancillary, i.e., B 1  V \ M , if and only if V 
is Â£j^^-ancillary, i.e., B JLV \ M  V 72 by Theorem 2.2.10. Amazingly 
enough, the sufficient condition provided by Theorem 5.5.10 fo 72 to be 
maximal ancillary in 
ensures that for any V such that B JLV \ M  V72 
and 72 C M  V V we also have (B V Af) JLV | M  V 72. Thus in such a 
situation, conditioning on a larger ancillary statistic will be irrelevant. This 
result is stated formally in the following theorem.

232
5. 
Optimal Reductions: Further Results
5.5.12 Theorem .
(i) 
Let X  be an Â£g^T-sufficient statistic and TZ be an Â£j^Jr -ancillary 
statistic such that X  is ^^^-co m p lete. Then for any Â£jg^r -ancillary 
statistic V such that 71 C M  W  we also have (B V X )  X V | M  V7Â£.
(ii) 
Let C be an Â£ j^ r -sufficient parameter and JC be an Â£ j^ r -ancillary paÂ­
rameter such that C is 
-complete. Then for any 
an ciliary
parameter Q such that JC C M V  Q we also have
Q X ( Â£ V T )  \ JCV M.
Proof. It suffices to prove (i) by duality. By Corollary 2.2.11, X  
sufficient, i.e., B X T | M  V X  implies that B J L T \ M V X V T Z .  Therefore 
X  is an Â£3^ ^ -complete sufficient statistic. By Theorem 2.2.10, if 71 is 
Â£Â£JT-ancillary, i.e., B JL 71 | M ,  any V C M  V T such that TZ C M
W
, 
will be Â£g^r -ancillary, i.e., B X V | M , if and only if B X V | M  V TZ, i.e., 
if and only if V is Â£j^^-ancillary. Therefore, (B V X )  X  V | M  V TZ by 
Theorem 5.5.9. 
â 
We conclude this section by some examples showing the uses and the 
limitations of the results hereby obtained.
The first example is characterized by the fact that both Theorem 5.5.10 
and Corollary 5.5.11 may be applied to prove the maximal character of the 
ancillary statistic.
Exam ple 3. Let s be a real valued random variable such that (s \ a) ~  
N  (0, a) and a E Mq = (0, oo). If the prior probability measure on 
Bq ) 
is such that each open subset has a positive probability (in this case Bayesian 
sufficiency is equivalent to sampling sufficiency, by Theorems 2.3.12 and 
2.3.14), then clearly s2 is minimal sufficient. It is also complete. Further, 
t = 1{5>o} is clearly ancillary. Remarking that (t,s2) is equivalent to s, we 
may apply Corollary 5.5.11 to deduce that t is maximal ancillary. However, 
without proving that s 2 is both complete and sufficient, it may be seen that 
for any bounded Borel function /  on M, if T  = o(t) and A  = cr (a) then

5.5. Completeness in Bayesian Experiments
233
+2(1 â t) J  
(27ra)1/2exp 
f(x)dx.
So, by continuity in a and unicity of the Laplace transform, it may be seen 
that s is a complete statistic conditionally on t. Therefore Theorem 5.5.10
In the next three examples, Corollary 5.5.11 is not applicable because 
the minimal sufficient statistic is not complete. However Theorem 5.5.10 
insures the maximal character of the ancillary statistic. This shows that 
Theorem 5.5.10 is of wider application than Corollary 5.5.11.
Exam ple 4. Let a Â£ A = (0,1) and p be a probability measure on A such 
that each open subset of A has positive probability. Let N  = {N\, N2, N3} 
have a multinomial distribution; more precisely:
{N i,N 2} is clearly minimal sufficient but not complete since Ni + N 2 is 
ancillary. Indeed, (N\ + N 2 | a) ~  Bi (n ,l/2 ), i.e., the binomial distribuÂ­
tion. Ni + N 2 is nevertheless maximal ancillary since (N\ | Ni + AT2, a) ~  
Bi (Ah + A^2, a) implies that conditionally on N\ + A^2, N\ is complete. 
â 
Exam ple 5. Let (6, c) = a Â£ A = (0,1) x (0,1) and p 0  v the prior probÂ­
ability on A where p and v are as in Example 4 (note that b JL c). Let 
N  = (N ijN 2 ,N 3 jN4) be such that
(N | a) ~  M N 4 (n , {6/2, (1 -  6)/2, c/2, (1 -  c )/2 Â»  .
Clearly, (N i,N 2 ,N3) is minimal sufficient but not complete since
(Ni + N 2 \ a ) ~ B i  (n, l/2 )
and N\ + N 2 is therefore ancillary. Nevertheless, N\ + N 2 is maximal anÂ­
cillary. Indeed Ni JL N 3 \ Ni + N2,a and
(Ni \Ni + N 2 , a ) ~  Bi (Nx + N 2 )b)
applies to deduce the maximality of t.
whereas
(N3 | N x + N 2 ,a) -  Bi (n -  N x -  N 2 ,c) .

234
5. 
Optimal Reductions: Further Results
Therefore,
Ni Â± c \ N x + N2,b 
and 
N 3 Â± b \ N i  + N 2 yc.
However,
N\ <  6 | Ni + N 2 
and 
N 3 <  c \ N\ + N 2. 
By repeated application of Theorem 2.2.10 we have that
(Nu b) X  (N3 ,c) | tfx + Wa;
this implies 6 X N 3 | N\ + N2, N\ and, therefore, N\ <  b | Ni + N 2j N 3 by
Theorem 5.4.5. It also implies that b JL c \ Ni + N 2, N3; 
so
N 3 <  c | Ni -f i\T2, 6, by Theorem 5.4.5, since (Ni,b) X (N3 ,c) | Ni + N 2 
implies that N\ JL c \ N\ + N2, N 3 yb- By Theorem 5.4.10, we conclude that 
(iVi,-ZV3) <C (6, c) | N\ -f N2. This means that, conditionally on Ni -f N 2,
(N i,N 2 ,N 3) is complete. 
â 
E xam ple 6. Let A = 1R, p a probability on A such that each open subset 
of A has positive probability. Let s = {si : 1 < i < n}, Si = (y*, z,-); and
Let y = {\)i : 1 < i < n} and z = {zi : 1 < i < n). If u â X^i<i<n Z1 an^ 
v = 5Z1<f'<n ziVii t-hen i uiv} 1S minimal sufficient since the logarithm of the 
likelihood is proportional to av â (l/2 )(l + a2)u. Now {u,u} is not complete 
since z is clearly ancillary and so is u. But u is maximal ancillary in {u^v} 
because conditionally on u, {u,t>} is complete. Indeed, it is obvious that, 
iKtKnVi I z >a an<^ (2/* I z ' a) ~  N  (azi,l)- Hence (u | z,a) ~  N  (au,u), and 
so (t> | ix, a) ~  N  (au,u). Therefore, conditionally on u, v is complete. 
â 
We end with an example showing that the condition of Theorem 5.5.10 
may not be necessary in the sense that the property of completeness is not 
satisfied by an apparently maximal ancillary statistic.
E xam ple 7. Let A = M, p a probability on A such that each open subset 
of A has positive probability. Let s = {s* : 1 < i < n} where (s{ | a) ~  
i.U (a,a-f 1), i.e., is the uniform distribution on the interval (a, a -f 1).

5.5. Completeness in Bayesian Experiments
235
If t = mini<;<n si and u = maxi<i<n Â«,â¢ then {tf,u} is, clearly, minimal 
sufficient but not complete, since u â i is ancillary. Even though u â t 
seems to be maximal ancillary in {t,u}, Theorem 5.5.10 cannot be used 
to prove it for, conditionally on u â t, {t,u} is not complete. Indeed, as 
(t | u â t, a) ~  U (a, a + 1  â (u â t)), it may be seen that if T  = a {u â t} 
and A  â cr {a}, then
{ A y r )
sm
27rt 
\
1 - { u - i ) J
| u - t ,
= 0 
V a &1R.
5.5.4 
Sampling Theory and Bayesian Methods
Let us consider a statistical experiment S = {(S,Â«S) \P a 
a G A} and 
A, a cr-field on A which makes the mapping a â âº P a (X) measurable for 
any X  E  S. We again adopt the notation introduced in Section 2.3.7.
In sampling theory the definition of completeness is as follows (see, e.g., 
Lehmann and Scheffe (1955)):
5.5.13 D efinition. A statistic T  C S  is p-complete (1 < p < oo) if the 
following implication holds:
t G [T]p , 
l at = 0 V a G A ==> t = 0 
a.s. 
Pa 
V a G A.
In case of doubt we say sampling-p-complete. 
â 
It is easy to see that this definition shares the same elementary propÂ­
erties as the Bayesian definition. Namely, any statistic U included in a 
p-complete statistic T  is also p-complete. A p-complete statistic is also 
p'-complete for V p < p' < oo. Any ancillary statistic U contained in 
a p-complete statistic T is trivial in the following sense: U C M  where 
M  = {X G S : P a (X) = 1 
V a G A 
or 
P a (X) = 0 V a G A}. Indeed, 
for any u G [7/]p there exists c G JR such that Xau â c, 
V a G A, since 
Z7 is ancillary; this implies J a (u â c) = 0, 
V a G A, i.e., u = c a.s. 
P a, 
V a G A, since Z7 is p-complete. Thus u G [Af].
The concept of completeness has three important applications in samÂ­
pling theory. Firstly, in the theory of unbiased estimation the 2-completeness

236
5. 
Optimal Reductions: Further Results
of a statistic T  C S  implies that if there exist unbiased estimators of a paÂ­
rameter b G [A] based on this statistic, then they are essentially unique, i.e., 
U Â£ P I 2 (2 = 
Xati = 6(a) V a G A (i = 1,2) imply 
-  t2 G [AT]. 
This leads to the theorem of Lehmann-Scheffe about the existence of a 
unique uniformly minimum variance unbiased estimator.
The second application of completeness is Basu First Theorem which 
states that any ancillary statistic is independent of a sufficient complete 
statistic. For the sake of comparison, we reproduce the proof of this theorem. 
Let T  be sufficient and complete and let U be ancillary. Then, for any 
u G 
there exists c G IR and t G [T]^ such that Xau = c V a G A, and 
T au â t a.s.Pa V a G A. NowTa(f â c) = XaT au â c = Xau âc = 0 V a G A  
Thus t = c a.s.Pa V a G A by completeness of T. Hence, T au = Xau a.s.Pa 
V u G 
and V a G A; this is equivalent to U X  T; P a V a G A. This is 
the sampling theory version of Theorem 5.5.9.
The third important application of completeness occurs in the domiÂ­
nated case only and is the sampling theory version of Theorem 5.5.4. It says 
that any sufficient complete statistic is almost surely equal to the minimal 
sufficient statistic Si introduced in Section 4.4.4. For the sake of comparison 
we give a proof of this theorem.
First note that in the dominated case,
M  = I* = 
G S  : P* (.X ) 2 = P* (X )} where P* is any privileged domi-
 
P *
nating probability and so T 
= T VAf. Note also that T  C S  is p-complete 
if and only if T  V Af is p-complete. Now let T  be sufficient and complete 
and let <Si be the minimal sufficient statistic. If t G [T]^, then S^t = S*t
a.s .Pa V a G A since $1 is sufficient. Trivially, Xa [t â 
= 0 V a G i
and so Xa [t â S^t] = 0 V a G A. Since Si c T V A f  (by minimal sufficiency) 
and T  V Af is complete, we have that t = 
a.s. P a V a G i ,  i.e., a.s. P*. 
Therefore, T  C <Si V Af and so T  V Af = Si V AT. 
â 
Theorem 5.5.10 suggests another application of completeness in samÂ­
pling theory for the characterization of maximal ancillarity. But it requires 
the unusual concept of conditional completeness.
5.5.14 D efinition. A statistic T C S  is p-complete (1 < p < 00) condiÂ­
tionally on a statistic U C S if the following implication holds: t G [TV U\p,

5.5. Completeness in Bayesian Experiments
237
Uat â 0 a.s.Pa V a E A 
=> t â 0 a.s.Pa Va E A. 
â 
Since the sampling theory version of Theorem 5.5.10 is not very well 
known, we state it as a theorem.
5.5.15 Theorem . In the statistical experiment E = {(S', S) ; P a a E A }, 
if U is an ancillary statistic such that S  is complete conditionally on Z/, then 
U is maximal ancillary.
Proof. Let V D U and suppose that V is ancillary. We first show that U is 
sufficient for V. Indeed, if a0 E A, u E [7/]^, v E [V]^, then
XaÂ° (u v ) = X ao (u UaÂ°v) .
But, by the ancillarity of V,
Za (iw) = XaÂ° (uv) 
V a E A 
and
2 a (u 7/aÂ°v) = Xao (u Uaov) 
V a E A.
Therefore, V a E A,
J a ( u v ) = I a (ti Z/aÂ°t;) 
i.e., 
Uav = Z/aÂ°i; 
a.s.Pa 
V a E
Therefore,
Ua ( v - U aov) = 0 
a.s.Pa 
V a E i ,
and since 5  is complete conditionally on Z/, v = UaÂ°v a.s.Pa 
V a E i ,  i.e. 
v E [Z /V ^a This means that U V W = V V 
or, equivalently, that Z7 is 
maximal ancillary. 
â 
Note that the minimal sufficiency of a complete sufficient statistic reÂ­
quires the domination assumption in sampling theory whereas the Bayesian 
analogue, Theorem 5.5.4, does not require this assumption. Therefore samÂ­
pling theory concept of completeness and the Bayesian concept of completeÂ­
ness may only be compared with respect to the dominated case. Recall that 
if p is a prior probability on (A, A) then, for any i E [<5]+, At = Zat a.s. p. 
Thus the Bayesian definition of p-completeness of a statistic in the regular 
case may be written as follows (in the sense of Definition 1.2.2):

238
5. 
Optimal Reductions: Further Results
5.5.16 D efinition. In the regular case, a statistic T  C S  is Bayesian p- 
complete with respect to the prior probability p if the following implication 
holds:
t G [T]p 
Xat = 0 
a.s.// = >  t â 0 
a.s.P. 
â 
The concept of a regular prior probability as defined in Section 2.3.7 
(Definition 2.3.13) gives the following result:
5.5.17 Theorem .
i) 
If p is a regular prior probability, then any T  C S Bayesian complete 
with respect to p is sampling complete;
ii) 
if T  C S is sampling complete, then it is Bayesian complete with reÂ­
spect to p for any regular prior probability p.
Proof.
(i) Remember that if p is a regular prior probability for the family 
{P a : a G A}, then this family is dominated and the predictive probability 
is equivalent to any privileged dominating probability. So if T  is Bayesian 
complete then 1 at = 0, V a G A, with t G [T]^, implies that t = 0 a.s. P 
which then is equivalent to t = 0 a.s. P a V a G A.
(ii) If T  is sampling complete and p regular, Xat = 0 a.s.// and t G 
imply Xat = 0 V a G A and so t = 0 a.s.Pa 
V a G A, which in turn implies 
t â 0 a.s.P. 
â 
The rest of this chapter will be devoted to the comparison of completeÂ­
ness on the parameter space with another concept found in the literature, 
namely, identifiability of mixtures.
5.5.5 
Identifiability of Mixtures
Let us consider a statistical experiment 8 =  {(S', *S) : P a 
a G A} 
and A  a cr-field on A which makes the mapping a â âº P a (X) measurable 
for any X  G S. The identifiability of mixtures as defined by Teicher (1960,

5.5. Completeness in Bayesian Experiments
239
1961, 1967) may be written as follows (see also Barndorff-Nielsen (1978), 
Chandra (1977) or Dawid (1979 a)):
5.5.18 D efinition. In the statistical experiment Â£, the mixtures are idenÂ­
tified if the following implication holds: for any p \,p 2 prior probabilities on
(A,A),
JA 
(Â«) Mi (da) =  f A l a (s) n2 (da) 
i s  â¬ [5]^ =>- Hi = /x2. 
â 
In our Bayesian framework, the concept of completeness may be exÂ­
tended to hold for any prior probability p:
5.5.19 D efinition. In the regular Bayesian experiment Â£a v s , *4 is uniÂ­
formly complete if A  <C S\ p 0  PA V /i, a prior probability on (A, A).
5.5.20 Theorem . In the statistical experiment Â£, the mixtures are identiÂ­
fied if and only if A  is uniformly complete.
Proof.
(i) Suppose that A  is uniformly complete. Let pi and p 2 be two probÂ­
abilities on (A,*4). If p = i pi -f \ p 2, then pi <C p 
i = 1,2 and if bi = 
dpi j  dp then 0 <  bi <  2 a.s.; this implies that we can consider that bi G MIqq 
for i = 1,2. Consider the Bayesian experiment Â£ = (A x 5, A  V iS, II) 
where II = p Â® P A . Now f A Xa(s)pi(da) = f AZ a(s)p2 (da) is equivalent to 
l[biAs] = X[b2As] 
Vs G [*5]^, i.e., X[bis] = X[b2s] 
Va G [S]^ and this 
is equivalent to Sb\ = Sb2 or S(b\ â b2) = 0. Since A  <C *5, in the Bayesian 
experiment Â£, then this implies b\ = b2 a.s., which is equivalent to pi = P2 -
(ii) Now suppose that the mixtures are identified in the experiment Â£. 
Let p be a prior probability on (A, A) and consider the Bayesian experiment 
Â£ â (A x S', A  V S, II) with II = p 0  P A . Let b G [A]^ such that 
= 0. 
Define p\ = /i and p 2 (E) = 
(1 -f a&) d/i where a -1 > sup^ |6|. Then
p 2 is a probability measure since 1 + ab > 0 and 26 = 0. 
Moreover, 
fA 
(s) dfii = 
1 â (s) dn2 i s  Â£ [5]^ since this condition is equivalent
to 2s = 2 [s (1 -f ab)] Vs G [S]^, and this is verified since Sb = 0 implies 
X (sb) â 0 Vs G [S]oq. Therefore by identifiability of the mixtures pi = p 2 
and this is equivalent to b = 0 a.s./i. Hence *4 <C <5; tt. 
â 


6
Sequential Experiments
6.1 
Introduction
In the next two chapters we study a richer structure than that invesÂ­
tigated until now. More specifically, the basic structure â viz., a unique 
probability on a product space A  V S  â is now endowed with an increasing 
sequence (Sn) of sub-<r-fields of S. Typically, Sn is generated by the sample 
information available up to time n. In other words, the increasing sequence 
(Â«Sn) â to be called a filtration â introduces time into the model. This 
has two consequences. Firstly, we shall compare the learning process up to 
time n and the sequence of learning processes associated with the transition 
from time n â 1 to time n; secondly, in the next chapter, we shall compare 
the learning process over a finite horizon with the learning process over an 
infinite horizon. Note that in Chapters 6 and 7, the index n will be considÂ­
ered as a fixed time. This material may be further developed by introducing 
random time: two natural orientations would be to consider either a stopÂ­
ping time (i.e., a random time associated to the filtration <Sn), or a random 
time independent of the filtration. These questions are not treated in this 
monograph although they are important to the correct handling of some 
problems with finite populations or with point processes.
As in previous chapters, the basic tool is the properties of conditional 
independence. In the next section, we initially provide some basic propÂ­
erties of sequences of conditional independence in the context of a genÂ­
241

242
6. Sequential Experiments
eral probability space. Section 6.3 describes the formal framework of a 
sequential Bayesian experiment and the different levels of analysis that apÂ­
pear to be relevant. These different levels of analysis lead, in general, to 
non-equivalent conditions for admissible reduction. Equivalence can neverÂ­
theless be achieved under some so-called transitivity condition, a concept 
introduced and analyzed in Section 6.4. Section 6.5 then presents the main 
theorems concerning the equivalence of admissible reductions at different 
levels. Finally, Section 6.6 provides a deeper discussion of the role of tranÂ­
sitivity; in particular, it envisages possible modifications of this condition 
and gives a proof of its necessity. Sections 6.4 and 6.6 depend heavily on 
the tools developed in Chapter 5.
The research which underlies this chapter has been motivated by an 
inquiry into the nature of exogeneity and non-causality in dynamic models 
beginning with Florens (1979), and successively systematized in Florens 
(1980), Florens and Mouchart (1980) and Florens, Mouchart and Rolin 
(1980). The relationships between different forms of non-causality have 
been presented in Florens and Mouchart (1984, 1985b) while the role of 
non-causality in Markovian models has been analyzed in Florens, Mouchart 
and Rolin (1987). The examples have, for the most part, been drawn from 
two expository papers by Florens and Mouchart (1985a, 1986b). This work 
has also furthered the study of specification and inference in linear dynamic 
models, as in Florens, Mouchart and Richard (1986, 1987); these last two 
papers are the source of several of the examples contained in this chapter. 
(See, for application, Fiori, Florens and Lai Tong (1982)). 
â 
6.2 
Sequences of Conditional Independences
As in Section 2.2, we handle a topic in abstract probability theory. For 
this reason, we adopt the same notation; in particular (M ,A 4,P) is an 
abstract probability space.
6.2.1 D efinition.
(i) 
A filtration ( fn )neIN 
is an increasing sequence of sub-cr-
fields of M , i.e., V n > 0 : T n C .Tvi+i C M . We denote

6.2. Sequences of Conditional Independences
243
Too = 
T n and write T n T Too â¢
n>0
(ii) 
A sequence of sub-cr-fields (J n ) n z l N  
adapted to a sequence of cr-fields 
(^n)n6W ^  V n > 0 : J n C ICn.
(iii) (J n) is (ICn)-recursive if (J n+1) is adapted to (Jn V JCn+i), i.e.,
V 71 > 0 
Jn+1 C Jn V /Cn-fi. 
â 
Moreover we introduce the following notation, for an arbitrary sequence 
of sub-cr-fields of A f:
(6.2.1) 
Jpq = \ J  Jn 
o < p < q .
p<n<q
In particular 
is a filtration in (M, A4) canonically associated to a
sequence (Jn) and JÂ£Â° = Vp>n 
*s a decreasing sequence of cr-fields.
Exam ple. Consider a sequence of random variables m n E [Af], n â¬ IN, and 
M.n â cr(mn) the cr-field generated by mn. Then (Af^)ne/?v Is a filtration 
in (M, Af) canonically associated to the sequence (mn)nâ¬jpy- Consider the 
sequence of the partial sums sn = Xa<i<n m* and 
= cr(^n)- Then 
0^n)neW *s no^ a filbrsttion but is (Afn)-recursive. 
â 
The essential objective of this section is to present two theorems; the 
second theorem, which deals with filtrations, plays a crucial role in sequenÂ­
tial analysis and in this respect is similar to Theorem 2.2.10 in the one-shot 
sampling context.
6.2.2 Theorem . Let (T n) be a filtration in (M, Af), let (J n) and (/Cn) be 
two sequences adapted to (Tn), and let Af; (i = 1,2) be two arbitrary 
sub-cr-fields of A f. If
(o) 
(Jn) is (/Cn)- recursive ,
then the following properties are equivalent:
(i)
(Â£â+i VA*i) Â± T n  \Jn VM2 
V n > 0

244
6. Sequential Experiments
(ii) 
(IC%+i V M i )  Â±  
|J n v Af2 
V n > 0.
Proof.
(ii) => (i) is trivial because /Cn+i C Â£Â£+i-
(i) 
=> (ii) Note that, from Definition 6.2.1. (iii), the hypothesis (o) is equivÂ­
alent to
(iii) 
J n + P C
j
n
V
K
n X l  
V p > l .
Note also that, by a monotone class argument, (ii) is equivalent to
(iv) 
(C t fv -M i) JLFn \ J n V M 2 
V p > 1, 
V n > 0.
We now prove (iv) by induction on p. Clearly, (iv) is true for p = 1 by hyÂ­
pothesis (i). Furthermore, hypothesis (i) implies, by Corollary 2.2.11, that
(v) 
(Â£â+p+i V M
i )  x  T n+P I J n+P V KZ%{ V J n V M i ,
because the fact that both (JCn) and (Jn) are adapted to the filtration (Tn) 
implies that 
C Tn+p and J n C 
and so (v) implies, by Corollary
2.2.4. and by (iii), that
(vi) 
(JCn+p+1 V My) Â±
T
n
\ J n V K % X l V M i .
The induction hypothesis (iv) and property (vi) implies, by Theorem
2.2.10, that (iv) is satisfied for p +  1.
Theorem 6.2.2. shows that recursivity is a sufficient condition to ensure 
that a sequential property of conditional independence (i) may be extended 
to an infinite horizon (ii). This theorem plays a crucial role in the proof of 
the next theorem, which is the cornerstone of this chapter.
6.2.3 
T heorem . Let (Tn) be a filtration in (M ,M ), (Qn) be a filtration 
in (M, Af) adapted to {Tn)^ and Aii (i = 1,2) be two arbitrary sub-cr-fields 
of Af.
Then the following seven conditions are equivalent:

1. 
(GooVMl) Â± ( T n V M 2) \ Fn-lVGn
2. 
(Â£ooV Ml )  X(jFâv M 2) \ T 0 VGn
3. 
(Gn+l v Ml )  JL (Tn v M 2) | To V Gn
4. (i) ( Gn+i y Mi ) X M 2 \Tn 
(ii) (gn+i V M , ) i f n | f 0vgâ 
5. (i) (GnVMi ) X M 2 \ T o 
(ii) (Gn+l v Ml )  Â± T n \ T o V G n V M 2 
6. (i) M l  Â± ( T n V M 2) \ T 0 VGn 
(ii) Gn+l Â±  (^n v M 2) I ^0 v Gn V M l  
7. (i) M l  X (Tn V M 2) I Tn-i V Â£â
(ii) 
G n +l X (T n  V M 2) I T o V Gn V M l
where all properties hold for any n > 0, and where Goo is defined as in 
Definition 6.2.1: Goo =  V n > o Gn, and with the convention, in 1 and 7(i), 
that T - i  = T 0.
Proof. 1 => 2 : We first prove that 1 implies:
8. 
( G o o V M ) X ( ^ â V - M 2) | ^ B_t VGâ 
1 < A; < n.
This is indeed true for k =  1. Suppose now that (8) is true for some k; 
condition 1 may also be written as:
9. 
(Geo V M i) X (Tn- k V M 2) | Tn-(k+l) VGn-k
and 9 implies, by Corollary 2.2.11 and because (Gn) is a filtration:
6.2. Sequences of Conditional Independences 
245

246
6. Sequential Experiments
10. 
(Goo V M \ )  X (Fn-k V M 2) I F n â(fc-fl) V Qn.
Condition 8 also implies, because (Fn) is a filtration:
11. 
(Â£00 V M l )  X (Fn V M 2) I ^n-(* +1) VSnV 
V M 2.
Clearly, 10 and 11 are equivalent, by Theorem 2.2.10, to:
12. 
(^oo V M i )  X (Fn V M 2) I ^n-(Jfc+i) V Â£n.
Therefore, 8 becomes also true for k = n, which is precisely 2.
2 
1 is trivial, by Corollary 2.2.11, because To C Fn~i C
2 
3 is a simple corollary of Theorem 6.2.2, where (Fn V Â«M2, Qn, Gn, F q)
replace (Fn, J n,ICn, M 2); indeed, (Gn) is trivially (C/n)-recursive.
3 
4 is a trivial application of Theorem 2.2.10; indeed that (Gn) is adapted
to the filtration (Fn) implies that F qV Fn V Gn = Fn-
3 <=> 5: clearly, 3 is equivalent to 5(ii) along with
13. 
(Gn + l V M i )  X  M 2 \F 0 VGn-
The equivalence between 5 (i) and 13 may be seen as an application
of the equivalence between 1 and 2 under the substitution: Gn is trivial,
Fn â>â  Gn V Fq, M i  and M 2 are permuted.
3 
6 is a trivial application of Theorem 2.2.10
1 
7: indeed, 1 is equivalent to 7 (i) along with
14. 
Goo X (Fn V M 2) \F n-i VGn VA4X.
The equivalence between 7 (ii) and 14 may be seen as an application of 
the equivalence between 1 and 3 under the substitution: M i  is trivial and 
F n 
* Fn V Ml l . 
â 
In the sequel, this theorem is used, in particular, to allow a sequenÂ­
tial reduction of a conditioning cr-field F n on its initial condition F q, or a

6.3. Sequential Experiments
247
sequential extension of a cr-field on a prediction space from an immediate 
prediction &n+i to an infinite horizon Qoq.
It should be noted that the equivalence between 6 and 7 may be rephrased 
as âunder 6 (ii), 6 (i) and 7 (i) are equivalentâ.
6.3 
Sequential Experiments
6.3.1 
Definition of Sequential Experiments
A sequential Bayesian experiment is a Bayesian experiment endowed 
with a filtration on the sample space. More specifically:
6.3.1 D efinition. A sequential Bayesian experiment is defined by:
E â (A x Sj *4 V c>, II, Sn { <S) 
where (Sn)n^pq is a filtration in (S', S) such that S  = \ J  Sn. 
â 
n>0
Typically, Sn represents the cr-field generated by the observations up to 
time n: Sn = cr(^o, 
, â¢ â¢ â¢, sn). S is then generated by the infinite sequence
(so, $1, â¢ â¢ â¢)â¢ Such a representation of Sn is unnecessary for the basic theory; 
in particular, in this chapter no assumption of stationarity is needed.
II is often constructed from a stochastic process (sn)neZ7V- It is perhaps 
useful to recall two classical approaches to assigning a probability measure 
to an infinite family of measurable functions sn. One approach is to define 
II, through Kolmogorovâs Theorem, as the projective limit of a projective 
system of finite-dimensional distributions. When the index set is 2A, it 
is sufficient to specify these finite dimensional distributions for the sets 
{0,1,..., n} for any n > 0 only. This approach naturally leads to a âglobalâ 
analysis of the sequence of experiments Â£>ivsn â¢ An alternative approach is to 
specify a sequence of transitions IIAvsn anc^ fÂ° define II using Ionescu Tulcea 
Theorem. This second approach naturally leads to a âsequentialâ analysis of 
the sequence of experiments 
. These two modes of construction have
different technical requirements that will not be made explicit here. Useful 
references to the literature on general stochastic processes are Blumenthal 
and Getoor (1968, Chap. 1), Dellacherie and Meyer (1975, Chap. 4), Neveu

248
6. Sequential Experiments
(1964, Chap. 3) and Metivier (1968). In general, we take the existence of 
the probability measure II on {A x S, A V  S) for granted, without taking 
care of its construction.
We systematically prefer using IN â {0,l,2,...}as the time index rather 
than Z  â {... â 2, â1,0, 1, 2,...}. The typical statistical problem conÂ­
sists of observing a stochastic process from a given instant, to be labelled 
an = 1â, and So is to be interpreted as the <x-field generated by the 
pre-sample information; in particular, if sn is indexed in JZ', one would 
define So = Vn<o<7(5Â«)- One motivation of doing so is to make the role 
of the initial condition explicit. In 2Z> one must specify a left-tail cr-field
S-oo = nnG2Â£ 
which plays essentially the same role as So. It should 
also be mentioned that deducing the properties of S-oo from those of the 
sequence (Â«Sn) often raises technical problems that do not arise with So and 
that do not appear particularly relevant in the statistical context; from time 
to time we shall sketch the nature of such problems.
A sequential Bayesian experiment can be analyzed from different view 
points:
(i) a âglobalâ analysis considers the sequence of experiments
Â£.4vsn = (A x S ,A  V Sn,IU v s J .
Under the usual regularity conditions
(6.3.1) 
IUvsâ = A* Â® Psâ = ps n Â® HSn â¢
The prior measure p is therefore common to every Â£ 4vs n and the compatiÂ­
bility conditions of a projective system require that PÂ£ (respectively, P$n) 
are the restrictions to Sn of P$ t (respectively, Psn,) as soon as n < n/. If 
Â£Avsn is dominated with density gAvsn then, for any n' < n, Â£>iv5n/ is 
also dominated with density gAvsn, = (A V Sni)gAvsn, i.e., the expectation 
of gAvsn, conditional on A  VÂ£^, with respect to p 0  P. Thus gAvsn is 
actually a martingale, a property to be used extensively in next chapter. 
Note that each experiment of the sequence Â£>iv5n has the same structure 
as the basic experiment Savs introduced in the first chapter.
(ii) An initial analysis considers the sequence of experiments
^Avsn ~ (^ x 
A vs n, n/v
5j.

6.3. Sequential Experiments
249
Under the usual regularity conditions,
(6.3.2) 
n % Sn = Â»s Â° Â® p Â£ v5Â° =  P |Â°  0  
.
This kind of analysis is motivated by the fact that the initial conditions,
formalized in So, typically represent presample information that 
deserves
different treatment. In this context, one of the relevant problems is to know 
to what extent is the conditioning on So admissible. Note that if So is 
trivial, then initial and global analyses coincide. The search for admissiÂ­
ble reductions of 
undertaken by setting, in the general reduced
experiment 
= S o, B = A  and T  â Sn.
(iii) A sequential analysis considers the sequence of experiments 
f
e
 =  ( A x 5 ,  A  V Sn, i f e j -  
Under the usual regularity conditions,
(6.3.3) 
u%-s\ =//-* 0 p/ ; 5- 1 = pÂ§:~i 0 vSn â 
A major concern of sequential analysis is to study the properties of the 
transformation pS n ~ 1 ââº pSn, i.e., the learning process associated with inÂ­
dividual observations when Sn is generated by a sequence of observations. 
The search for admissible reductions of 
*s undertaken by setting, in
the general reduced experiment 
M. â c>n-i, B â A  and T  = Sn.
6.3.2 
Admissible Reductions in Sequential Experiments
The theory developed in the first five chapters may be used to characÂ­
terize admissible reductions at each of these three levels of analysis. For the 
readerâs convenience we summarize, in the next table, the principal sorts of 
reductions undertaken at each level; note that (Tn) is a sequence of cr-fields 
adapted to (*Sn), and B and C are sub-cr-fields of A.
As mentioned earlier, when introducing the three levels of analysis in 
a sequential Bayesian experiment, the properties defining admissible reÂ­
ductions may be analyzed as straightforward applications of reductions in 
unreduced experiments (Chapter 2) at global level and reductions in condiÂ­
tional experiments (Section 3.3) at the initial and sequential levels. These 
properties are summarized in Table 6.1.

Table 6.1. Admissible reductions in sequential experim ents.
250 
6. Sequential Experiments
Level of analysis 
Corresponding 
experiment
global
Â£ A v S n
initial
C"S0
CAVSn
sequential
C&n-1
c A v S n
Admissible reductions
i) On the sample space: (Tn)
sufficient
A  JL  s n|Tn
A  JL  <Sâ|S0vTn
A JL  B n |5Â»-lV T â
ancillary
A J L r n
A  JL  Tn|So
A J L  T n lSn -j
ii) On the parameter space: B
sufficient
A JL  Sâ|B
A JL  <Sâ|BvS0
A JL  B â|B vB n_ ,
ancillary
B JL  5 n
B JL  S n|5o
B JL
iii) Joint reduction
mutual ancillarity
C and (Tâ)
C JL  Tn
c  J L r â | B 0
c  JL  T n|5 â _ 1
mutual sufficiency
A  J L  Tn\B
A J L  Tâ|BVB0
A  JL  Tn\BVSn-!
Band (Tn)
B JL  S n|Tâ
B JL  B n|S0vTâ
B JL  s â |B n_ iv r â
mutual exogeneity
c  JL  Tn
c  JL  râ|5 o
C JL  Tn\S n -l
C and (Tn)
A JL  Sâ|CvTâ A JL  Bâ|CvB0v r n A J L  Bâ|CVBâ_iVTâ
cut
B JL  C
B JL  C|50
B JL  C|S0
B, C and (Tn)
A JL  Tn|B
A  JL  Tâ|BV50
A JL  Tn|BVSâ_!
A JL  <Sâ|CvTâ A J L  SnjCVSoVTn A  JL  SJC V Sn-iV T n
R em ark. These properties are supposed to hold for any n  >  0 with the convention that
S n =  S q V n  <  0.

6.3. Sequential Experiments
251
The new problems concern the relationships between admissible reducÂ­
tions at different levels of analysis. In this chapter, we investigate the reÂ­
lationships between initial and sequential admissibility of reductions. The 
limit properties â i.e., passing from Sn to S = Vn>o 
â are analyzed in 
the next chapter which mainly involves global and sequential analysis. Links 
between global and initial levels of analysis are not analyzed systematically; 
this would essentially involve discussion of the interpretation of initial conÂ­
ditions along with some rather straightforward results of the following type: 
the independence between the parameters and the initial condition â i.e., 
(BV C) JL <So â ensures the equivalence of the first condition of a Bayesian 
cut for the three levels of analysis. We do not investigate the optimality 
of admissible reductions in the initial and global analysis (such as minimal 
sufficiency).
In this chapter, we essentially consider a sequence (Tn) adapted to a 
filtration (<5>n), and we investigate the properties of ancillarity and of suffiÂ­
ciency of this sequence from an initial and a sequential point of view. For 
this kind of problem it is natural to assume that (7^) is a filtration. In 
particular, if one starts with an arbitrary sequence (Z n) adapted to (iSn), 
the filtration (Tn) is canonically constructed as Tn = Vi<n 
an<^ 
re" 
suits of this chapter draw on the filtration (Tn) only. This explains why 
Theorems 6.2.2 and 6.2.3 are the basic tools of this chapter. Heuristically, 
the motivation of our results may be viewed as follows: in most applications 
Sn is generated by a vector-valued stochastic process (sn) and Tn is generÂ­
ated by a subvector of sn. This is at variance with Chapter 7 where Tn is 
typically generated by a statistic on (si,..., sn) so that the sequence (Tn) 
is adapted to (Sn), but is no longer increasing. Consequently, this chapter 
uses different tools.
Before studying the links between initial and sequential admissibility, 
two remarks are in order:
(i) 
The first condition for the sequential cut (i.e., the cut in Â£4^5^) has 
been given as
(6.3.4) 
B 1  C | So 
rather than
(6.3.5) 
B Â± C  |<Sn_i.
As these conditions are supposed to hold for any n > 0, (6.3.5) clearly

252
6. Sequential Experiments
implies (6.3.4); the next theorem shows that, in a cut, these conditions are 
actually equivalent.
6.3.2 T heorem . If (B,C,(Tn)) operates a sequential cut, B and C are a 
posteriori independent, i.e., B X C | Sn for all n.
Proof. We prove the theorem by induction on n. For n â 1, the theorem 
is a trivial application of Theorem 3.4.15. Now, if the theorem is true for 
n â 1, we may replace (6.3.4) by (6.3.5) and the conclusion is true again by 
the same theorem.
Had we introduced (6.3.5) instead of (6.3.4) in the definition of a sequenÂ­
tial cut, Theorem 6.3.2 would have been a direct consequence of Theorem 
3.4.15, but (6.3.5), for any n > 0, is precisely equivalent to the conclusion 
of Theorem 6.3.2. This explains our preference for condition (6.3.4) in the 
definition of a sequential cut.
(ii) 
The second remark concerns the initial condition used in the second 
column of the Table 6.1 (level of initial analysis). It may seem more natural 
to introduce To rather than So in conditions involving only n ^yj^, for 
example, ^ l l T n \ So, C JL Tn | So or A  X Tn \ B V So- There would be, 
in general, no implication between such a modified condition and the given 
one. It is nevertheless possible to use Theorem 2.2.10 in order to produce 
supplementary conditions ensuring the equivalence between the modified 
and the given condition. So, for instance:
f 
B Â± T n \C V T 0 
1 
\  S 0 ! T n \ B V C V T 0 J
( B Â± T n \ C V S 0 1
{ S 0 Â± T n \ CW% J
( B Â± T n \CWT 0 1 
\  B JL So | C V Tn J
f B Â± T n \ C V S 0 )
\  B Â± S 0 \ C V T 0 J '
We shall see that, in most cases, initial and sequential admissibility are 
equivalent only under a supplementary condition. This condition, to be 
called âtransitivityâ , is analyzed in the next section, which also includes,
(6.3.6) 
(B V S0) JLTâ \ C V T 0 
o
<$â 
(6.3.7) 
B Â±  (S0 VTâ) |CVT0

6.4. Transitivity
253
inter alia, conditions of the same type as the supplementary conditions just 
mentioned (viz., conditions of the type So l T n | B V To).
6.4 Transitivity
This section treats a topic in probability theory. Thus, (M, M , P) is an 
abstract probability space. In general, (A4n), (Afrl), (On) etc. are sequences 
of sub-<r-fields of At, and Mo is the trivial sub-cr-field of M.
6.4.1 
Basic Theory
A. In Sequences of cr-Fields
The concept of transitivity is at the core of the linkages between initial 
and sequential admissibility. This concept has been introduced in statistics 
by Bahadur (1954) and is used extensively, e.g., in Hall, Wijsman and Ghosh 
(1965). In addition to its usefulness in sequential analysis, this concept also 
has a long history in time series, particularly in the field of econometrics, 
where it is referred to as non-causality analysis (see, e.g., Granger (1969), 
Sims (1972) or Pierce and Haugh (1977) ). In this section we generalize this 
concept slightly, in the following way:
6.4.1 D efinition. (J\fn) is (M n)-iransiiive for (On) if the sequence (Afn) 
is adapted to (Mn VOn) and
(6.4.1) 
Afn+1 X On I MnVAfn, 
V 71 > 0.
It should be mentioned that in the Definition 6.4.1, the condition 
Afn C M n V O n is not restrictive because, by Corollary 2.2.11, (6.4.1) is 
equivalent to Afn+i -JL (Afn V On) \ M n V Afn, this implies that (Afn) is 
(Afn)-transitive for (On VA^).
The concept of transitivity will often be used in the following particÂ­
ular specification. (On) is a filtration representing the complete history 
of a stochastic process (for instance, On = Sn), (Afn) is also a filtration 
representing the complete history of a subprocess (for instance, Afn = Tn, 
an increasing sequence of uniform statistics) and M n is either constant, in 
which case we write M n â M  (for instance, M n â A  or So), or M n â Sn-i 
as in sequential analysis.

254
6. Sequential Experiments
The next two results give equivalent forms of transitivity in particular 
cases and will be interpreted in the framework of non-causality. The first 
one is a transposition of Theorem 6.2.2 with M \  and M 2 trivial and under 
the identification (Tn, J n ^ n )  ââº (M n V On, M n VMn,Mn)-
6.4.2 P roposition. If (M n V A/^) is (Mn)-recursive and (M n V On) is a 
filtration, then (Mn) is ( M n)~transitive for (On) if and only if
(6.4.2) 
N?+l J L O n \ M n V t f n ,  
V n > 0.
In Proposition 6.4.2, the condition of recursivity means indeed that 
Mn+i C M n V Mn 
Thus, if M n is constant, this condition
becomes trivial. However, if (Mn) is a filtration, namely, Mn T M, the condiÂ­
tion of recursivity becomes M n+1 
C 
M n V Mn+1, i.e., (M n) is 
(A/*n ^recursive which implies, by induction, that M n C Mo V Mn- ThereÂ­
fore, by Corollary 2.2.11, the (A4n)-transitivity of (Mn) for (On), (i.e., 
Mn+1 -L On | M n VMn) implies that Afn+1 -IL ( M n V O n) | Mo V M n VMi 
since Mo C M n V On (because, by hypothesis, (M n V On) is a filtration).
Thus Afn+i JL ( M n V On) | Mo V An, i.e., (A/ân) 
is (A4o)-transitive 
for
(M nV O n).
The second result is a transposition of condition 1, 2 and 3 of Theorem
6.2.3 with the substitution (Tn,Gn) ââº (On V M,J\fn) anJ M i  and M 2 
trivial.
6.4.3 P roposition. If (A/*n) is a filtration adapted to a filtration (On) with 
Moo â Vn>o^n, fhen the following conditions are equivalent:
(i) 
(Mn) is (Oo V A4)-transitive for (On)
(ii) 
A'oo Â± O n  \ M V O n - l  VMn 
V n >  0
(iii) Moo JLOn \ M v O 0 v M n 
V n > 0 
â 
Let us mention that condition (ii) or (iii) in Proposition 6.4.3 implies a 
weaker form of condition (i), namely, (i bis): (Afn) is (On- \  V A4)-transitive 
for (On)- This is an example of a property of transitivity given a variable 
conditioning cr-field, in fact being weaker than a property of transitivity

6.4. Transitivity
255
given a constant conditioning cr-field. Similarly, one may also look for a 
condition that would make a {Oo V Al)-transitivity equivalent to a {M)~ 
transitivity (i.e., a condition that allows one to drop Oq in condition (iii) of 
Proposition 6.4.3). That condition is, as an application of Theorem 6.2.3, 
one of the following three equivalent conditions:
(6.4.3) 
Mn J L O o \ M V  No 
V n >  0
(6.4.4) 
N n+i A . O o \ M v N n 
V n >  0
(6.4.5) 
Noo 1 0 o  | M W N 0.
The condition (6.4.3) was introduced in Section 6.3.2 â see (6.3.6) and
(6.3.7) â and may be interpreted as a self-predictivity requirement at the 
initial conditions.
B. In Stochastic Processes
{ M n) is typically used in the sequel, as a sequence of conditioning cr- 
fields for a sequence of experiments 
consequently, (M n) is adapted
to {A V5n), and (Wn) and (On) are Â£jfcsn-statistics. Thus, both sequences 
are adapted to { M n V <Sn). When M n â A or M n â A  V Â«So we handle 
properties of the sampling process up to null sets for the prior probability. 
More specifically, if II is transformed into II' such that II' <C II and 
dtt'/dll Â£ [A]+, then Corollary 2.2.15 ensures that a property of transitivity 
in a given experiment is preserved in the modified experiment. In most 
applications in the next section, M n has the form M n â A, M n = AVSo, 
Nin â So, Nin â A  V 
Or Nin ~~ Sn â 1*
The basic idea underlying the concept of transitivity may be viewed as 
sufficiency for prediction: The knowledge of M n VN n is sufficient to predict 
N n+i given the knowledge of M n V On- This interpretation may explain 
why econometricians have instead used the expression âOn does not cause 
N n (given M n)â while in system theory the usage is âN n causes On (given 
M n)â¢ A better expression would probably be âN n is selfpredictive with 
respect to On (given M n)ââ¢
It may also be noted that Theorem 6.2.2 may be interpreted as a theÂ­
orem about âgeneralized transitivityâ while Proposition 6.4.2 shows that 
in a rather general case, the immediate self-predictivity characterizing the 
transitivity is actually equivalent to a property of self-predictivity over an 
infinite horizon.

256
6. Sequential Experiments
A natural use for the above concepts may be found in the analysis of a 
stochastic vector-valued process xn = (yn>zn). The corresponding cr-fields 
will be denoted by script letters, e.g., Xn â <r(xn) and Xn â y n V Zn. The 
role of On will be played by the cr-field generated by the observations of the 
z-process from time 0 to n; it will be denoted as X q = 
V Z q . The role 
of Mn is played by Zq and the sequence M n is constant. Grangerâs (1969) 
concept of ây does not cause z (given A4)â may be written as:
(6.4.6) 
Zn+i X  
\ Z[ ?VM,  V n >  0,
i.e., â ( Z J ) is A4-transitive for ( X q )â because (6.4.6) is equivalent to:
(6.4.7) 
2 Â£ +1 Â±  X q \ Z q V M ,  V n > 0.
Since M n is constant, the recursivity condition of Proposition 6.4.2 is trivial. 
Therefore, by this proposition, (6.4.7) is equivalent to:
(6.4.8) 
X 
\ Z Â£ V M ,  V n > 0.
Simsâs (1972) concept of ây does not cause z (given A4)â may be written 
as:
(6.4.9) 
X y n | Z% V M ,  V n > 0
which is clearly implied by but not equivalent to Grangerâs concept. One 
can modify Simsâs concept into:
(6.4.10) 
Â£âÂ°Â°+1 J. 
I y ? - 1 V Z q V M ,  
V n >  0,
i.e., the prediction of yn on the basis of yfi"1, the history of y, of Z gÂ°, 
the complete trajectory of z, and of Mt, does not depend on the future 
values of the z-process, i.e., Z% +1. Proposition 6.4.3 shows that, because in
(6.4.10) y n may be replaced by 3^o j the modified Simsâs concept (6.4.10) is 
equivalent to:
(6.4.11) 
Z n+i X y 0n l y o V Z g  V M ,  
V n >  0,
i.e., (after replacing in (6.4.11) Zn+\ by Zq*1) ( Z q ) is (.M  V 3^o)-^ransitive 
for ( X q ). Note that A4-transitivity is stronger than ( M i  V ^o)-transitivity; 
therefore, Grangerâs concept implies the modified Simsâs concept but the 
modified Simsâs concept becomes equivalent to Grangerâs concept under a 
supplementary condition given in (6.4.3), viz.:
(6.4.12)
Z0n Â± y 0 | Z 0 V Mt, 
Vn > 0 ,

6.4. Transitivity
257
which is trivially satisfied when 
C M .
R em ark. It is interesting, in this context, to show how to modify the index 
set of the stochastic process from IN  to Z
. Formulae (6.4.6) to (6.4.12) 
are still well defined for n  Â£ Z  when â0â is replaced by 
ooâ because, 
in general, the sequence of ^-fields y Â£ is monotone decreasing in m. So 
we have yâ¢^ = V-oo<j<n^T and 
= fln^-oo* Proposition 6.4.2 is 
still valid and the conditions (6.4.6), (6.4.7) and (6.4.8) are still equivalent. 
However, Proposition 6.4.3 may no longer be used as such. The modified 
Simsâs concept is now rewritten as:
(6.4.13) 
ZnÂ°Â°+1 X y n I y ? - 1 V 22oo V M ,  
V n 6 l
The argument 1 => 2 in the proof of Theorem 6.2.3 may be repeated to 
prove that (6.4.13) is equivalent to:
(6.4.14) 
z ; + 1 l } ' n | 
p) ( C V Z ^ V A I ) ,  
V n e l
l<p<oo
Note, however, that the proof of (6.4.14) requires a generalization to seÂ­
quences of Theorem 2.2.12 (Theorem 7.2.7 in Chapter 7).
Therefore, if we want (6.4.13) to be equivalent to a condition analogous 
to (6.4.11), namely:
(6.4.15) 
Zn+1 X y ^  | y_oo V Z ^ V M ,  
V n E Z , 
we need the supplementary condition:
(6.4.16) 
P l ( ^ - -  v 2 -ooVA4) = y _ coV Z 2 TOVA4, 
V Â« e l
p >  1
This analysis leaves open the question of the actual importance of indexing 
the z-process in Z  rather than in JN, and of whether the ad hoc assumption
(6.4.16) on the left-tail is operational. More discussion of this issue may be 
found in Hosoya (1977) , Chamberlain (1982), and Florens and Mouchart 
(1982). We now illustrate the role of the conditioning cr-field M  in a propÂ­
erty of transitivity. Indeed, the next example exhibits a situation where 
A4-transitivity is verified for some choices of M  only.
Exam ple. Let Sn be generated by (ar0, a?i,...,  xn) where xn = (yn,zny . 
Suppose that
(xn+i I a, a?g) ~  N 2 [(azn, azn)', I\ 
and 
(a \ x0) ~  N ( 0,1).

258
6. Sequential Experiments
Consider M  = cr(a,aro) = A  V Xq] (Zq) is clearly (A V To)-transitive 
for (y j). Consider now M  = a(xq) = X q] (Zq ) is not To-transitive for 
)â¢ Indeed, ^ ( ^ n + i )  = 
â¢ XqCl and X q a does involve 
; for instance,
A'Ja =  (1 +  2z^)_120(yi + 21). 
â 
Let us now consider xn = (ynj^nj^n) and extend (6.4.6) and (6.4.8) as 
follows:
z n+1Â± y s  \ z s v w s v m
Z â¢+ 1 Â± y $  | Zq V Wq V M .
From Proposition 6.4.2, these two transitivity properties would be equivaÂ­
lent under the recursivity condition
Wâ+1 C 
V Z0n+1 V M ,
and under this condition the comment following Proposition 6.4.2 shows 
that the two transition properties are each equivalent to
Zn+1 X ( y 0n VW0n) | Z 0"VWoV. M
Z0Â°Â° X Oft V WS) | 2 0n V Wo VAX
6.4.2 Markovian Property and Transitivity
A. In Sequences of cr-Fields
Let us now return to the analysis of transitivity in the general frameÂ­
work. If we consider a sequence (An) of <7-fields such that (Aq1) is (A4n)- 
transitive for (On), i.e.,
(6.4.17) 
V 0n+1 Â±  On \ M n \ ? K ,  
or, equivalently,
(6.4.18) 
7Vn+i A. On | M n V J t f ,
the interpretation of this last formula as a sufficiency for prediction of An+i 
shows that as n increases, we must retain more and more information, i.e., 
Aq1. In view of this fact, it is obviously interesting to establish when it 
would be possible to retain merely the last k preceding elements of the 
sequence, with fixed k for each n. This property is clearly formalized as
(6.4.19)
K + l Â± O n \ M n V J W - k + 1.

6.4. Transitivity
259
This property actually means that (Arâ_jfc+1) is (Â«Mn)-transitive for (On). 
The rest of this section will be devoted to the comparison of the transitivity 
of (A/q1) and of (Mâ¢_k+1). These two concepts are naturally linked through 
Markovian conditions on the sequence (Mn), which we now define:
6.4.4 D efinition. (J\fn) is ( M n)-k-Markovian if
(6.4.20) 
Afn+ 1 Â±  MS I M n V K _ k+V
Note that if M n = M  for all n, an application of Theorem 6.2.2 shows 
that (6.4.20) is actually equivalent to
(6.4.21) 
â¬
+. l K
I  M v W _ k+1.
A straightforward application of Theorem 2.2.10 gives the following propoÂ­
sition:
6.4.5 Proposition. (Mq) is ( M n)-iransitive for (On) and (Mn) is (M n)- 
k-Markovian if and only if (Mâ¢__k+1) is (wMn)-transitive for (On) and (Mn) 
is (Mn V On)-A:-Markovian. 
â 
This proposition asserts that the two conditions (6.4.18) and (6.4.20) 
are equivalent to (6.4.19), along with
(6.4.22) 
Wâ+1 JL 
| M n V On V ^ . H 1.
An immediate corollary would be that under a transitivity condition, one 
Markovian property implies the other, i.e., under (6.4.19), (6.4.22) implies
(6.4.20) or under (6.4.18), (6.4.20) implies (6.4.22). 
Similarly, under a 
Markovian property, one transitivity condition implies the other, i.e., under
(6.4.20), (6.4.18) implies (6.4.19) or under (6.4.22), (6.4.19) implies (6.4.18).
Let us recall that, in general, a subprocess of a Markovian process is not 
necessarily Markovian. This problem may be analyzed by assuming that in 
Proposition (6.4.5), (On) is also a filtration. We obtain this by associating 
the filtration (Oq ) to the sequence (On) as follows. Clearly, if (Mn V On) is 
(A4n)-Â£-Markovian, i.e.,
(6.4.23) 
( K +i V On+1) JL ( K  V O^) \ M n V ( /Â£ _ fc+1 V 0 ^ _ t+1),

260
6. Sequential Experiments
then (Xn) is (M n V Oq)-fc-Markovian, i.e.,
(6.4.24) 
Mn + 1 JL K  I M n V 0%VMZ_k+1.
Proposition 6.4.5 says that (6.4.23) also implies that (Xn) is (Mn)-k- 
Markovian (i.e., (Oq) may be dropped from (6.4.24)) if (XÂ£_k+1) is ( M n)~ 
transitive for (Oq). Such a transitivity condition may be somewhat unsuitÂ­
able in this context, because (Xâ_k+1) is not a filtration but, as it is adapted 
to the filtration (Xq)> a more satisfactory condition would be that (A/o1) is 
(A4n)-transitive for ( O q ) . Using the concepts introduced in Chapter 5, this 
assumption will be sufficient if we add a measurable separability condition. 
More precisely, as a straightforward corollary of Theorem 5.2.10, we have 
the following proposition:
6.4.6 P roposition. Under On || Xq \ M n VA/^_j.+1, if (Xq) is ( M n)~ 
transitive for (On) and (Xn) is (Mn V 0n)-fc-Markovian, then (An) is (A4n)-
&-Markovian and (A/^_fc_j_1) is (A4n)-transitive for (^n)- 
â 
We now face the following questions: Are transitivity conditions really 
necessary to ensure that a subprocess of a Markovian process is Markovian? 
And further, is it true that the two transitivity conditions (6.4.18) and
(6.4.19) occur only in marginally and conditionally Markovian processes, 
i.e., processes satisfying both (6.4.20) and (6.4.22)? Using strong identiÂ­
fiability conditions, the answers to these questions are affirmative and, as 
corollaries of Theorem 5.4.13, follow from the next two propositions:
6.4.7 Proposition. If On <C Xq | M n VA/â. ^ ,  then if (Xn) is both 
(A4n)-&-Markovian and ( M n V On)~fc-Markovian, then this implies that 
both ( X q )  and (Arâ_jb+1) are (A4n)-transitive for (On). 
u
6.4.8 P roposition. If X q  <C On | M n V X Â£ _ k + 1 , then if both ( X q )  and 
.k+i) are (Mn)-transitive for (On), then this implies that (Xn) is both 
(Mn)- and (Mn V On)-k-Markovian. 
â 

6.4. Transitivity
261
B. In Stochastic Processes
Let us return to the analysis of a stochastic vector-valued process 
xn =  (yn,zn) as in Section 6.4.l.B and assume that (Xn) is an (M n)-k- 
Markovian process, i.e.,
In this subsection, the sequence of <r-fields (M n) may be used in rather 
different contexts. Firstly, when M n is constant (i.e., M n =  M )  it will 
typically either be trivial, as in a purely predictive set-up, or represent paÂ­
rameters and/or initial conditions. When (M n) is not constant, it may be 
a filtration, typically in the form M n = M V  Wq where wn is a stochastic 
process defined on the same basic space as xn, but ( M n) may fail to be a 
filtration, such as, for example, when ( M n) is generated by a sequence of 
statistics (in this case the sequence (M n) is typically adapted to a filtraÂ­
tion without being itself a filtration). This flexibility in the interpretation 
motivates leaving an âabstractâ sequence of cr-fields in the conditioning of 
the properties in the stochastic process context.
Now, (6.4.25) clearly implies that:
(6.4.26) 
Zn+i X Z0n | M n V ^ 0n VZÂ£_*+1, 
V n > 0,
i.e., (Z n) is ( M n V 
Markovian. By Proposition 6.4.5, if (Zâ¢_k+1) is
(Afn)-transitive for (y^ ), i.e.,
(6.4.25)
* n+1X * 0n | K V C w -  
V n > 0.
(6.4.27)
z n+1 Â± y z \ M n v z z _ k+1, 
V n > 0
then (Zn) is ( M n)-k-Markovian, i.e.,
(6.4.29)
Z n + i Â± y Z \ M n VZZ,  
V n > 0.
The transitivity condition (6.4.27) is rather unsatisfactory in the sense that 
it involves conditioning on Z % _ k + 1 and therefore, in a sequential specificaÂ­
tion of the process, requires integration on Z^~k. The transitivity condition

262
6. Sequential Experiments
(6.4.29) is a more satisfactory assumption. Proposition 6.4.6 shows that unÂ­
der the measurable separability condition
(6.4.30) 
y 0" || 2% \ M n VZ Z _ k + 1
if (Zq ) is (A4n)-transitive for (y$) â i-e-> (6.4.29) â and if (Z n) is 
(â A^tfi V)-&-Markovian â i.e., (6.4.26) â then (Z n) is (A4â)-fc-Markovian
â i.e., (6.4.28) â and, morever (Zâ_k+1) is also (A4n)-transitive for (yÂ£)
â i.e., (6.4.27). In order to verify condition (6.4.30) a very useful tool is 
provided by Corollary 5.2.11. Indeed, if there exists a probability II' equivÂ­
alent to II such that 
X Z q | A4n;II' V n > 0, then (6.4.30) is readily 
verified.
Proposition 6.4.7 shows that under the strong identification condition
(6.4.31) 
y% <  Z0" I M n V Z2 _ k + 1
if (Zn) is both ( M n)~ and ( M n V^o )-&-Markovian â i.e. conditions (6.4.28) 
and (6.4.26) â then (Zq ) and (Z^_k+1) are both (Â«Mn)-transitive for (yft). 
Proposition 6.4.8 shows that under the strong identification condition
(6.4.32) 
Z o < y o \ M n VZ Z _ h + 1
the two transitivity conditions â (6.4.27) and (6.4.29) â imply the two 
Markovian properties â (6.4.26 and (6.4.28).
Assumption (6.4.25) also implies that:
(6.4.26') 
Zn + 1 X 2 0" | Mâ V y^-k+i V ZH_ k + 1 
V n > 0,
i.e., (Zn) is ( M n V
+1)-fc-Markovian. Moreover, by Theorem 2.2.10 and 
Corollary 2.2.11, under Assumption (6.4.25), the two transitivity conditions
â i.e., (6.4.27) and (6.4.29) â are respectively equivalent to
(6.4.27') 
2 n+1 X  3Â£_t+1 | M n V Z ^
k + 1 
V n > 0
(6.4.29') 
Z n + 1 X  y " _ k + 1 \ M n V Z Â£  
V n > 0.
Therefore, substituting y â¢_k + 1 to yfi, Conditions (6.4.30) and (6.4.31) may 
be relaxed into respectively:
(6.4.30') 
yÂ£_jfc+1 || Z q | M n V Zâ¢_k+l

6.4. Transitivity
263
(6.4.31') 
W _ k+1 <  Z% | M n V Z"_k+1.
Therefore, if (Xn) is (Afn)-&-Markovian, Proposition 6.4.8 is irrelevant. 
Proposition 6.4.5 shows that if 
is (Afn)-transitive for (To )> then
(Zn) is (Afn)-&-Markovian and (Zq ) is (A4n)-transitive for (To). PropoÂ­
sition 6.4.6 shows that, under Condition (6.4.30'), (Zq ) is (A tn)-transitive 
for (To) implies that (Zn) is (A4n)-fc-Markovian and (Zâ¢_k+1) is (A4n)- 
transitive for (To )â¢ Proposition 6.4.7 shows that, under Condition (6.4.31'), 
(Zn) is (A4n)-&-Markovian implies that both (Zq ) and (2 â_fc+1) are (Afn)- 
transitive for (To )â¢
Note, however, that the strong identification Property (6.4.31') is not 
a necessary condition. There are in fact situations in which (Zn) is both 
( M n)- and (Afn VTo )-&-Markovian but where it does not verify transitivity 
properties. Therefore, (6.4.31') is not valid in such situations. Indeed,
(6.4.25) implies that
(6.4.33) 
Zn + 1 Â±  (yS V Z0") I M n V 3Â£_t+1 V 2 % _ k + l .
Therefore, under
(6.4.34) 
yÂ»_t+1 Â±  (yÂ»-â V zrk)
 I M n  V zz_k+1
we have, using Theorem 2.2.10,
(6.4.35) 
(Zn+1v y Z _ t+1) Â±  (
r
â v 2 J - â ) | M n VZZ_k+1.
Therefore, by Corollary 2.2.11, we also have
(6.4.36) 
(Zn+1 V y:_k+1) JL ( y r *  V 2Â£) I M n  V ZÂ£_k+1.
This implies that (Zn) is both ( M n)- and ( M n V To )-&-Markovian. HowÂ­
ever, (6.4.31') is violated since it follows, from (6.4.34), that for m Â£ [Tn]oo, 
m -  ( M n V ZZ_k+1) m â¬ [3Â£_4+1 V M n V Z"_k+ 
and
(MâV Z o ) { m - ( M n V Z"_k+1)m} = ( M n V Z o ) m - ( M â V Z " _ k+1)m = 0,
since (6.4.34) implies that y n JL Zq \ M n V Z â_k+1.
The same reasoning shows that, even if (Â«Tn) is ( M n)-k-Markovian, 
which implies that (Zn) is ( M n V To )-&-Markovian, it may well be true

264
6 . Sequential Experiments
that (Z n) is (Â«Mn)-^-Markovian with I > k. Indeed, by Corollary 2.2.11,
(6.4.25) 
implies that
This shows that with t > k , (Zn) is merely (A4n)-^-Markovian. The next 
four examples illustrate Propositions 6.4.5 to 6.4.8 as well as these last reÂ­
marks, and provide greater insight into the role of the technical assumptions.
Exam ple 1 . Let us consider the two-dimensional process xn = (yn^ nY 
defined by
(Z n) is transitive for (J ^ ) and Zn+\ JL Zq | Z n, i.e., (Zn) is 1-Markovian.
(6.4.37) 
Zn+i X  (y%-k V Z%~1) | M n V 3Â£_t+1 V Z^_t+l,
Therefore if, for I > k,
(6.4.38) 
M _ i+l X ( ^ - â V Z $ -') I M n V Z ^ _ t + 1
we have, by Theorem 2.2.10,
(6.4.39) 
(Z n + 1 V X U +1) X (y%-k V Zg~e) I M n V Z " _ e + 1
(xn + 1 I a?Â£) ~  N 2
All Vn + fl12 z n 
fl21 Vn + a22 Zn
Clearly, (Xn) is 1-Markovian. Now
(zn+1 | 2/0 > zo) ~  N\a2\yn + a22*n; 022]
and
If a21 = 0, then Zn+i JL (3^o v ^o ) I 
and therefore Zn+1 X 
| Z q,i.e.,
If Â«2i 7^ 0 and cr12 = (a n /a 2i)cr22, then
y n+1Â± ( y Z v z z ) \ z n+uz n
and, therefore,
yn+1 x y% I Z q+1.

6.4. Transitivity
265
/ 
<*11 
x 
. <*11 
<*11
(,<*12-----------<*22)Zn â \ HZn \ a i i  
âT~<r22
a2i 
a2i 
<*21
an
<*21
Hence
(Vn | Zo) ~  ^
and, therefore
(zâ+i I z%) ~  Af [a22zâ +  a2 1E(yn | z%)\<r22 +  a21V(j/â | zÂ£)] .
i.e.,
(zÂ»+i U o) ~
TV [(an +  a22)zâ +  (a2iai2 -  ana22)zâ_i; (1 -  aj1)<r22 +  a ^ n ]  .
This shows that in this case (Zn) is 2-Markovian if a n a 22 â <*21 <*12 ^  0. 
Conversely, if <*n<*22 -  <*2i<*i2 = 0, we obtain y n + 1 X  (y% V Z q ) | Z n+i â 
this is Condition (6.4.34) â and
(zâ+1 | Zq ) ~  N  [(an + a22)zâ; (1 -  a ^ ) ^  +  a ^ u ] ,
which shows that (Zn) is 1-Markovian. Note that (6.4.31) is not satisfied 
since Zq (yn â (an /  a2i)zn) = 0 and, clearly, (Zq ) is not transitive for (T â), 
i.e., Zn+1 1  3^o | ^  
nÂ°t satisfied since
(zn+1 I Xq) ~  N(a 2l yn + a2 2zn;<r22) and a21 ^ 0 . 
â 
Exam ple 2. Consider the two-dimensional process xn â (yn,zn)f defined 
by
yn 
\  
(  0 0
yn + azn ) â \ 0 
<72
(a;â+i | a;^) ~  N 2 
with the initial condition
Xq ~  N 2
a 2 
a 2
Clearly, (Xn) is 1-Markovian and Z n+1 1  ^
 | y n V Z n. this shows that 
(Zn) is (yn)-1-Markovian and that
(Zn+1 I Vo j zn) ~  N(yn + azn,<T2).
Thus (Zn) is not transitive for (3^o )â¢ However, due to the initial conditions, 
it is also true that Z n+1 X Xq \ Zq V Zn and that
(*n + l I *o) ~  N{zQ + azn,<T2).

266
6 . Sequential Experiments
Hence (Zn) is not 1-Markovian but (Zq ) is transitive for (To )â¢ Therefore 
the measurable separability condition (6.4.30), i.e, To 
|| Zq \ Zn, is not 
satisfied; this is evident since Zq V Zn C (To V Zn) fl Zq and Zq is not 
trivial. 
â 
In this example, the violation of the measurable separability assumption 
is due to the degeneracy of distributions. Nonetheless, in most standard 
situations, as shown by Corollary 5.2.11, this assumption is very mild and 
readily verified.
E xam p le 3. Consider the two-dimensional process xn = (yn^zny defined 
by
(Â®n+l I Â®o) ~ ^2
Clearly, Zn+i X X$ | Tn,Tn-i, Zn\ furthermore,
byn
yn -by n-i +azn
0
nâ 1
<r(yn -  byn- 1) X XÂ£ 
and 
cr(yn -  6t/â_i) X  Zn \ XÂ£
Therefore, <r(yn -b y n^ )  X (ZnV X ^ 1) and (zn + 1 | zÂ£) ~  Ni(azn,cr2+ t 2) 
by Theorem 2.2.10. This shows that (Zn) is both 1-Markovian and (To )~
1-Markovian. However, the two transitivity conditions are not satisfied. 
Therefore the strong identification property (6.4.31), i.e., 3? <  
I 
is not satisfied. Indeed, whereas yn â  byn- 1 Â£ [T?] an<^ l s  nÂ°f trivial, 
ZÂ£(yn - b y n-i) = 0. 
â 
E xam p le 4. Consider a two-dimensional process xn = (yn,zn)' such that, 
V n, Zq X  To an<^ i^n) is not Markovian; for instance,
(zn+1 I Zo) ~  Ni(azn + z0 ,a2).
Clearly, in this case (Zn) and ( Z q ) are both transitive for (To )> whereas 
( Z n ) is not (To1)-1-Markovian. Consequently the strong identification propÂ­
erty assumption (6.4.32), i.e., Zq <C To I Zn-k+i> is nÂ°t satisfied. This is 
clear since, in this context, this assumption is equivalent to Zq <C 
and, by elementary Property 5.4.2(iii), this implies Zq = Zâ¢_k+1. 
u

6.5. Relations Among Admissible Reductions
267
6.5 Relations Among Admissible Reductions
We now analyze the conditions under which initial and sequential adÂ­
missibility are equivalent. For the most part, we follow the table of Section
6.3.2. Throughout this section, (7^) is a filtration adapted to the filtration 
(Sn).
6.5.1 
Admissible Reductions on the Parameter Space
On the parameter space Theorem 6.2.3 ensures that initial and sequenÂ­
tial admissibility are equivalent without imposing further conditions.
If, in Theorem 6.2.3, we take M 2 and Qn to be trivial, then 7(ii) beÂ­
comes trivial and the equivalence between 7(i) and 3 provides the following 
result under the substitutions { M iyT n) â*â  (A,B VÂ«Sn), for sufficiency, and 
(M i.T n) ââº (Z?,Â«Sn), for ancillarity.
6.5.1 Proposition. A parameter B C A  is 
-sufficient (respectively, 
-ancillary) if and only if B is Â£jCv~sn-sufficient (respectively,
f^ - a n c illa ry ) . 
â 
6.5.2 
Admissible Reductions on the Sample Space
Initial and sequential admissibility of reductions on the sample space are 
shown to be equivalent only under a supplementary condition of transitivity.
For sufficiency on the sample space we consider experiments which are 
both conditioned on a parameter C C A  and marginalized on B. UnconÂ­
ditional sufficiency is straightforwardly obtained by choosing C so that it 
is trivial and B = A. Now choose M 2 in Theorem 6.2.3 so that it is trivÂ­
ial; the next proposition results from the equivalence between 6 and 7 under 
the substitution (Afi, Tn, Qn) ââº (B,Sn VC,7^). This implies, in particular, 
that Condition 3 becomes
(6.5.1) 
(BVTn+1) 1 5 â  \C W S 0 VTn.
6.5.2 Proposition. If (Tn) is (B V C V So)-transitive for (Sn), then (Tn) is 
-sufficient if and only if (7^) is 
-sufficient. 
â 

268
6 . Sequential Experiments
For ancillarity on the sample space, let C and (7^) be mutually ancillary 
conditionally on B\ complete ancillarity may be obtained by choosing C = A  
and B trivial. Now choose M \ in Theorem 6.2.3 so that it is trivial; the next 
result follows from the equivalence between 4 and 5 under the substitution 
(A42,77n,^ n) ââº (C,tSn VB,Tn) which involves, in particular, that Condition 
3 becomes:
(6.5.2) 
Tn+1 Â± ( C v S n) \ B V S 0 V T n.
6.5.3 Proposition. C and (7^) are Â£4^5Â°-mutually ancillary and (7^) is 
(Z?VCVc>o)-transitive for (Â«Sâ) if and only if C and (Tn) are 
-mutually
ancillary and (Tn) is (B V 5o)-transitive for (Â«Sn)* 
â 
Remark that we recover Proposition 6.5.1. by noting that the two tranÂ­
sitivity conditions of Proposition 6.5.3 are trivial when 7'n = Sn- The next 
examples provide insight into the necessity of the condition of transitivity 
in Proposition 6.5.3.
Exam ple 1. Let us consider the two-dimensional process xn = {yn^n )' 
defined by
0
a yn
1 
0 
0 
1 â a2
where a Â£ A = (â1,1). Suppose that (a | a?o) is uniformly distributed 
on 
A. 
Clearly, 
^ n+1 X (A V T0n), 
y n + 1 -L ^n+i I A  V * 0", 
and 
Zn + 1 X Xq | A  V yn. By Theorem 2.2.10 and Corollary 2.2.11, the two 
first conditional independences imply that 3^+1 X (Zn+1 VTq1) | A. ThereÂ­
fore, for n > 1,
and
E(zn+1 | a,Zo,y0) = a E(yn \ a,ZQ,y0 ) = a E(yn \ a) = 0,
V{zn+i \ a ,z ^ , y f}) 
= 
(1 -  a2) + V(a yn \ a,zg,y0)
= 
(1 -  a2) + a 2 V(yâ | a) -  1.
This means that, V n > 1,
(zn+1 I a, Zq , y0) ~  #(0,1) or, equivalently,

6.5. Relations Among Admissible Reductions
269
(zn | a, a?o) ~  i.AT(0, 1), V n >  2.
We conclude that A  and ZJ are mutually ancillary in ^ v A 'n* However, A
x nâ^
and Zq are not mutually ancillary in Sj&Xn. Indeed, Proposition 6.5.3 is 
not applicable because ( Z q ) is not (.4 V <To)-transitive for Xq . These two 
facts are evident since
(*n+i I o,*o) ~  N(ayn, l - a 7). 
m
Exam ple 2. Consider the two-dimensional process xn = (yn^ n Y  defined
by
(arn+i | a,a?o) ~  N2
where a = (b,c)f E A = JK2. Clearly, because (2n+i | a,a?o) ~  N(6 yn,l), 
X Â£ n+i | B V X0n. In other words, C and Z q are mutually ancillary in
cBv*o"
but not in 
because, e.g.,
(A V X 0)zn = b(A V Xo)yn - 1 = b{y0 + (n -  l)c}, V n > l .
Thus Proposition 6.5.3 is not applicable because ( Z q ) is not (B V Xo)- 
transitive for (^o1). Indeed (,4 V 
= b yn =  (BV A'o^Zn+i. 
â 
6.5.3 
Admissible Reductions in Joint Reductions
In this section the theorems provide transitivity conditions which guarÂ­
antee the equivalence of initial and sequential admissibility in the case of 
mutual sufficiency, mutual exogeneity and Bayesian cuts.
A. M utual Sufficiency
6.5.4 Theorem . B and ( Tn) are Â£%%Â£â¢-mutually sufficient and (Tn) 
is (,4 V So)-transitive for (5n) if and only if B and (Tn) are 
-mutually 
sufficient and (Tn) is (B V <So)-transitive for (Sn).
Proof. This theorem may be rephrased in terms of conditional indepenÂ­
dence as follows. Let:

270
6 . Sequential Experiments
(i) 
A  A-Tn \ B y  So]
(ii) 
B Â± S n \ S 0 VTn]
(iii) 'Tn+i JL Sn | A  V So V En ;
(iv) *4 JL 7^ |B V 5 n-i;
(v) 
B Â± S n |5 n_! VTn;
(vi) Tn+1 Â± S n | B V  So V Tn.
We need to prove that (i), (ii) and (iii) are equivalent to (iv), 
(v) and
(vi). Indeed, when C = A, Proposition 6.5.3 shows that (i) and (iii) are
equivalent to (iv) and (vi), and when C is trivial, Proposition 6.5.2 shows 
that under (vi), (ii) and (v) are equivalent.
It may be remarked, in view of Theorem 6.2.3, that the three conditions
of Theorem 6.5.4, i.e., B and (7^) are Â£jJvsn-mutually sufficient and (Tn) 
is (A V tSo)-transitive for (<Sn), are actually equivalent to the two following 
nonrelated conditional independence relations:
(6.5.3) 
Tn+1 Â±  (A V  Sn) \ B V  S 0 VTn
(6.5.4) 
B Â± S n | S 0 VTn.
Note also that the conditions of Theorem 6.5.4 imply (6.5.1) when C is 
trivial, i.e., (SV 7n+i) JL Sn \ So V Tn, and this in turn implies that (Tn) is 
<So-transitive for (Sn), i.e., Tn+i JL Sn | So V Tn.
B. 
M utual Exogeneity
Concerning mutual exogeneity, we have the following theorem:
6.5.5 Theorem . If (7^) is (^4 V Â£o)-transitive for (c>n), then C and (Tn)
Â£ 
S
are Sj&Sn-mutually exogenous if and only if C and (7^) are 
-mutually
exogenous and (Tn) is Â«So-transitive for (tSn).
Proof. This theorem may be rephrased in terms of conditional indepenÂ­
dence as follows. Let:

6.5. Relations Among Admissible Reductions
271
(i) 
C JL Tn | S0;
(ii) 
/ I S â  | C V S0 V Tn;
(iii) Tn + 1 i S n | A V S 0 VTn-,
(iv) C Â± T n |5 n_i;
(v) 
A Â± S n iC V S n ^ V T n ;
(vi) Tn + 1 Â± S n | S 0 VTn.
We need to prove that (i), (ii) and (iii) are equivalent to (iii), (iv), (v) 
and (vi). Indeed, when B = A, Proposition 6.5.2 shows that, under (iii),
(ii) 
is equivalent to (v) and is equivalent to (A V %i+i) JL Sn | C V So V Tn. 
This implies that
(vii) Tn+1 1  Sn | C V S o V T n.
And when B is trivial, Proposition 6.5.3 shows that (i) and (vii) are 
equivalent to (iv) and (vi). 
â 
With regards to mutual sufficiency, it may be shown, using Theorem 
6.2.3, that the three conditions of Theorem 6.5.5, i.e., C and (7^) are fj|v sn" 
mutually exogenous and (7^) is (*4 V c>o)-transitive for (Â«Sn), are equivalent 
to the two following nonrelated conditional independent relations:
Note that it follows from (6.5.5) that, under the conditions of Theorem
6.5.5, we also have that (Tn) is (C V 5o)-transitive for (<Sn).
C. 
Bayesian C uts
Combining these two theorems, we have the following theorem reÂ­
lating the initial and sequential cuts:
(6.5.5)
C4VTn+i) Â± S â  |C V 5 0 VTâ
(6.5.6)
C Â± T n | So.

272
6 . Sequential Experiments
6.5.6 Theorem . (S, C, (Tn)) operates a cut in Â£ % s n anc^ (^ 0  *s (*-4 V5o)- 
transitive for (<Sn) if and only if (#,C ,(Tn)) operates a cut in Â£ ^ s n and 
(Tn) is (B V Â«So)-transitive for (*Sn).
Proof. This theorem may be rephrased in terms of conditional indepenÂ­
dence, as follows. Let:
(i)
A  X % 1 B V So;
(ii)
A  X Sn \c V <So V Tâ;
(iii)
^n+l X ,Sn M V S 0 VTâ;
(iv)
A  X Tn \B
<
5Â°i
(v)
A  X Sn \c V<Sâ_i VTn;
(iv)
Tn+1 X ,Sn
>
>
Remark that (i), (ii) and (iii) are equivalent to (iv), (v) and (vi). Indeed, 
when C = A, Proposition 6.5.3 shows that (i) and (iii) are equivalent to (iv) 
and (vi); when B = A, Proposition 6.5.2 shows that under (iii), (ii) and (v) 
are equivalent. 
â 
Again, by Theorem 6.2.3, it may be shown that Conditions (i), (ii) 
and (iii) of the proof of Theorem 6.5.6 are equivalent to the two following 
non-related conditional independence conditions:
(6.5.7) 
%+i Â± ( A v S n) \ B V S 0 VTn,
(6.5.8) 
A Â± S n \ C V S 0 VTn.
Note also that the conditions of Theorem 6.5.6 imply (6.5.1) with B = A, 
i.e., (A V Tn+1) JL Sn | C V So V Tn , and this clearly implies that we also 
have (Tn) is (C V Â£o)-transitive for (Sn).
D. In itial and Sequential C uts in D om inated E xperim ents
It may be illuminating to look at what is really the matter of TheÂ­
orem 6.5.6 in the dominated case. We first need a definition.

6.5. Relations Among Admissible Reductions
273
6.5.7 
D efinition. 8  = (A  x S ,A V S , II, (Â£n)n>o) is a dominated sequential 
Bayesian experiment if for each n > 0, Â£Uvsn is a dominated Bayesian 
experiment. 
â 
According to Proposition 3.4.17, if B X C | <So, (#, C, (%)) operates a 
cut in ^ v 5n if an(^ only if
(6.5.9) 
g Â£ s Â° = g% sÂ° â¢ g ^ f 0^ " , 
V n > 1,
s
and (B, C, (Tn)) operates a cut in 
if and only if
/ . r i M  
AVSn-1 
BVSn-1 
CV<5n_iV Tn 
w 
^
(6.5.10) 
gSn 
= g Tn 
â  gSn 
, 
V n > 1.
But, by extending (1.4.29), we have
(6.5.11) 
g fn s Â° = 
H  g Â£ Si' \
1 <*<n
And so
//> r io\ 
â a v s 0 
TT â6v5*-1 
TT 
c v S i-iv T i
(6.5.12) 
gT. 
â¢ | |  95 .
1 <i<n 
l<i<n
Consequently g ^ So may be written as the product of a ^-measurable funcÂ­
tion and a C-measurable function. However, it should be realized that, 
unless certain additional conditions are satisfied, these two functions are 
not sampling densities. That condition (7^) is (B V <So)-transitive for (Sn) 
implies that
(6.5.13) 
n  
=  
* r Â°
l<t<n
and
(6.5.14) 
n  fe r5-
vrâ 
=  
j i r ,v r-.
l<i<n
E. 
Some Exam ples
We conclude this section with some examples of cuts in initial and seÂ­
quential experiments.
Exam ple 1. 
Let xn be a linear normal k-autoregressive process,i.e.,
(*â |al*J-1)~JV[P(L)*11;S]

274
6 . Sequential Experiments
where P(L) is a matrix whose elements are polynomials of degree k with
no constant terms in the lag operator. By convention, a = (P(T), Â£). If we 
partition xn into xn = (t/^,2^)' and P{L) and E into
then we obtain (zn \ a^xâ¢ *) ~  N[Pz (L)xn,T,zz]; further (yn | a,xÂ£ 
1 ,zn) 
is also normally distributed with
and if 6 X c | 
then (6, c, (zq )) certainly operates a cut in the sequential 
experiment but, in general, does not operate a cut in the initial experiment. 
Note that the condition of prior independence is satisfied when (P(L),E) 
are distributed a priori according to a ânatural conjugate distributionâ, i.e., 
matrix normal inverted Wishart, (see, e.g., Dreze and Richard (1983)).
However, if we assume Pzy(L) = 0, (zâ | a, Xq *) ~  N[Pzz(L)zn , Zzz\. 
In terms of conditional independence, this implies that
zn+1 JL (a,Xo)\b,x0,z%.
Therefore (zft) is both (&,Â£o) and (a, ^o)-transitive for (Xq), and (6,0,( 2$)) 
operates a cut both in the initial and in the sequential experiment. 
â 
E(yn | a, Xq 1,zn) 
=  
7rzn + Q(L)xn 
V{l/n | 
^0 
> zn) 
â 
^yy-z
where
Therefore, if one defines
b 
= 
(PZ(L), E â )
C =  
(7T, Q (T ), Yiyy.z)
Exam ple 2. 
Now consider a dynamic version of Example 4 of Section
3.4.6, i.e., (xn | a , ^ -1) ~  N[Â£n + P(L)(xn â Â£n),E] where P(L) is defined

6.5. Relations Among Admissible Reductions
275
as in Example 1, and 
a = (0,E,& ,1 < i < oo). Suppose that A$Â£i 
= 0,
V 1 < i < oo, where Aq is a known matrix function of 
6 . With the same
notation as in Example 3 and Example 4 of Section 3.4.6, we have:
{zn \ a , x l - 1) 
~  N[tZtn + Pz(L)(zn - t n ) , V zz]
(Vn \ aixo 
1 zn) 
~  ^Ky,n 4~ Py (P){xn ââ Â£n) 4~
ft{zn â Â£z,n ~ Pz{L){xn ~~ Â£n)}> ^yyz]
Boâ¬y,n + CeÂ£Zln 
= 0.
Without placing restrictions on the parameter a, there is no useful cut nor 
transitivity property. The restriction Pzy(L) = 0 implies that (zq) is (a, #o)- 
transitive for (a^). Assuming that B$ is a (square) invertible matrix, an 
obviously useful choice of restrictions for a cut in the sequential experiment 
is PZy(L) = 0,7r -f B q 1Cq = 0 and Pyz(L) = nPZz(L) -  Pyy(L)7r. Under 
these restrictions (6, c, (zÂ£)) will operate a cut both in the initial and in the 
sequential experiments with
b 
= (P**(L),E**,Â£2|i, 1 < i < oo)
and
C 
=  (Pyy (T), 7T, Hyy.z)
if the prior distribution is such that b i  c | Â£o- Indeed, with these restricÂ­
tions,
(zja .a tf-1) ~ Ar[Pâ(I)zB + (7 -P â ( I ) ) ^ in,Eâ]
and
(Vn I a ,z 0 
j zn) ~ N[Pyy(L)yn -f ( /  â Pyy(L))7rzn, Eyy.^].
Note that, in this case, Â£*%* captures all the incidental parameters; further, 
if we define un = yn -  wzn, then (un | a,xo_1,zn) ~  N[Pyy(L)un, Eyy.*],
i.e., un is an autoregressive process. 
â 
Exam ple 3. 
A second dynamic version of Example 4 of Section 3.4.6 is 
motivated by experiments where conditional expectations are unobservable 
functions of the process past. 
Using the same notation as in the preÂ­
ceding examples, consider the experiment (xn\a, ^g-1 ) ~  N (Â£n, E) with 
A*Â£n = P(L)xn. Clearly,
(Vn \ atxo 
i^n) 
~  N[Â£y>n +  ^(zn ~~ Â£2,71)1 ^-âyy.z\

276
6 . Sequential Experiments
and Be^y^n + CeÂ£Zin = P{L)xn. Using the same method as in Florens, 
Mouchart and Richard (1979, 1986, 1987) and in Engle, Hendry and Richard 
(1983) (see also Wu (1973) ), a sequential cut may be obtained through the
following restriction: Bgir -f C$ = 0. Indeed, under these conditions, the
conditional experiment may be written as (yn | a,Â£o_1, 2n) ~  ^{q n ^y y.z) 
and Bgrjn + C$zn = P{L)xn. Remark that (6,c, (zq )) will operate a cut in 
the sequential experiment, with
^ 
=  
{ ^ z z  j Â£z,ii 1 ^  2 
00)
and
c 
= 
(Be,Cg,P(L), 7]n : 1 < n < oo)
if the prior distribution is such that b JL c \ xq. Remark also that if Bg is 
(square) invertible the conditional experiment no longer has any incidental 
parameters, since (yn | cl}Xq~ 1 , zn) ~  N[irzn + B ^ 1 P(L)xn, 
In this
case we have a cut only in the sequential experiment. A cut in the initial 
experiment or, equivalently, a (6, a?o)-transitivity for (zq) generally depends 
on specific assumptions regarding the process generating (Â£*,n)- 
â 
6.6 
The Role of Transitivity: Further Results
Very often, particularly in discrete time series models, the specificaÂ­
tion of the model is sequential, i.e., defined by the sequence of experiments 
s
Â£jÂ£Jsn- Sequentially admissible reductions are therefore easily checked, but 
what remains of interest is to establish whether such reductions are also 
initially admissible. As seen in Section 6.5, this requires verification of 
transitivity conditions. In this section, we look successively at weakening 
these transitivity conditions, and at their necessity.
6.6.1 
Weakening of Transitivity Conditions
Even though transitivity conditions are theoretically relatively simple, 
verification may sometimes be difficult. In most cases, the (A V Â«So^transiÂ­
tivity is easy to verify since this depends on the sampling probabilities only. 
Indeed, this condition is equivalent to P ? ^ *  â Br^fiÂ°VTn' However, the 
(Z?VÂ«So)-transitivity may be difficult to verify since it requires integration on

6 .6 . The Role of Transitivity: Further Results
277
A  conditionally on 5, which may pose considerable difficulty. And further, 
modelling specifications frequently presuppose (A VÂ«So)-transitivity without 
making this explicit completely. It is therefore helpful to be aware of cerÂ­
tain weak additional assumptions which imply that other transitivities are 
verified.
In view of Proposition 6.5.2, this question is irrelevant in the compariÂ­
son of sufficiency in initial and sequential analysis. But, in view of PropoÂ­
sition 6.5.3, this problem arises when trying to deduce ancillarity in initial 
analysis from ancillarity in sequential analysis. The following theorem shows 
that under a measurable separability condition, the same transitivity condiÂ­
tion guarantees the equivalence between ancillarity in initial and sequential 
analysis.
6.6.1 Theorem . If C || Sn \ B V So V Tn and if (Tn) is ( BVCV i n Â­
transitive for (Sn) , then C and (Tn) are Â£ 4^ * â1-mutually ancillary 
if and only if they are Â£ 4^$Â°'-mutually ancillary. Moreover, in this case, 
(Tn) is also (B V Â£o)-transitive for (Â£n)-
Proof. Let
(i) 
C \ \ S n \ B V S 0 V Tn]
(ii) 
Tn+1 
\ B V C V S 0 VTn;
(iii) C X Tn+i | B V Sn]
(iv) C 1 7 n \ B V  So]
(v) 
Tn+1 Â± S n \ B V S 0 VTn.
We must prove that under (i) and (ii), (iii) is both equivalent to (iv) 
and implies (v). From Proposition 6.5.3 we know that (ii) and (iv) imply
(iii) and (v). By an application of Theorem 5.2.10, (i), (ii) and (iii) imply 
that Tn+1 1  (C V Sn) | B v S o V T n . Hence, C X Tn + 1 | B V S 0 V T n) 
and so (iv) is obtained by making use of the equivalence between 1 and 2 
in Theorem 6.2.3 with AI2 trivial, and under the substitution
(M u fn ,G n )-> (C ,B V S o V T n,B V S 0). 
â¢

278
6 . Sequential Experiments
We now apply this result to joint reductions. From the proof of Theorem
6.5.4 we obtain, as a corollary of Theorem 6.6.1, the following proposition:
6.6.2 Proposition. If A  || Sn | BVS$VTn and if {Tn) is (^4v5o)-transitive
s
for (<Sn), then B and (7^) are Â£ 4^  -mutually sufficient if and only if they 
are SjfvSn-mutually sufficient. Moreover, in this case, (Tn) is both (B VÂ«So)- 
and (<S0)-transitive for (Â£n)-
Following the same steps, we obtain, for mutual exogeneity, the following 
proposition:
6.6.3 Proposition. If C || Sn \ So VTn and if (Tn) is (.4 VÂ«So)-transitive for 
(Sn), then C and (7^) are 
-mutually exogenous if and only if they are
-mutually exogenous. Morevover, in this case, (Tn) is both [C V So)- 
and (Â£o)-transitive for (<Sn). 
â 
Finally, concerning cuts, we have the following theorem:
6.6.4 T heorem . If A  || Sn \ B V S 0 V Tn and if (Tn) is (.4 V S0)-transitive 
for (5n), then (#,C, (7^)) operates a cut in 
^  an(^ on^  ^  (#>Â£> O^n))
operates a cut in 
Moreover, in this case, (Tn) is both (B V Â£ 0)- and
(C V Â«So)-transitive for (5n)-
Proof. Let
(i)
A \ \ S n \ B v S 0 VTn;
(ii)
Tn+ 1 i- Sn \ A V S 0 VTn,
(iii)
A Â± T n \BVSn-l-,
(iv)
A  Â±  Sn 1 C V Sn - 1  v%i',
(v)
A Â± T â \BVSo-,
(vi) A Â± S n | C V Â«S0 V Tn.

6 .6 . The Role of Transitivity: Further Results
279
We must prove that under (i) and (ii), (iii) and (iv) are equivalent to
(v) 
and (vi). Proposition 6.5.2, with B = A, shows that under (ii), (iv) 
and (vi) are equivalent and imply that (7^) is (C V <So)-transitive for (Â«Sn). 
Theorem 6.6.1, with C = A, shows that under (i) and (ii), (iii) and (v) are 
equivalent and imply that (Tn) is (BV c>o)-transitive for (<Sn). 
â 
Note that all the measurable separability conditions appearing in TheÂ­
orem 6.6.1, Propositions 6.6.2 and 6.6.3 and in Theorem 6.6.4, are readily 
satisfied when for each n the probability IIn, that is the restriction of II on 
v4V5n, is equivalent to a probability measure 11^ such that A  X Sn | <So; 11^. 
This is a consequence of Corollary 5.2.11.
Theorem 6.6.4 is useful in ensuring that a cut in a sequential experiÂ­
ment is also a cut in the initial experiment, especially when the sequential 
experiment is specified in such a way that the conditional distributions of 
the process are not explicitly known. However, in some applications these 
distributions will be known (see Examples 1, 2 and 3 in Section 6.5). In such 
a context it is interesting to characterize a cut in 
and ^ v 5 n by propÂ­
erties of the prior distribution and of the conditional sampling distributions; 
such a characterization is provided by the following theorem.
6.6.5 Theorem . (B,C, (Tn)) operates a cut both in S^vSn anc* m ^Avsn 
if the following conditions hold:
1. 
B JL C \So;
2. 
Tn JL (*4 V Â«Sn_i) | B V  So V Tnâ\\ and
3. 
A  JL Sn | c V S n- i VTn.
Proof. Let
(i) 
A Â± T n \ B v S 0;
(ii) 
A Â± S n \ C V S 0 VTn]

280
6 . Sequential Experiments
(iii) A Â± T n |B V 5 â -i;
(iv) A Â± S n | C V Snâi V 'Tn;
(v) 
Tn+11 5 n M v 5 0 VTn;
(vi) Tn+1 1 5 n |Â£VÂ«S0 VTn.
We must prove that 2 and 3 imply (i), (ii), (iii) and (iv). By (6.5.2)
and Proposition 6.5.3 with C = A, 2 is equivalent to (i) and (v) and to (iii)
and (vi). Now, 3 is equal to (iv) and , under (v), by 
Proposition 6.5.2 with
B = A, (iv) is equivalent to (ii). 
â 
Conditions 2 and 3 are easily interpreted in the sampling distributions 
s
of Â£Anv-s\  Indeed, 2 means that the distribution of Tn, given A  V <Sn_i, 
depends on B  V S q V 7^_i only; 3 means that the distribution of iSn, given 
A  V Sn- 1 V Tn, depends on C V Sn_i V Tn only.
For applications it is interesting to rewrite Theorem 6.6.5 in terms of 
densities in a dominated sequential Bayesian experiment.
6.6.6 Corollary. Let Â£ = {A x 5, A  V S, II, (<Sn)n>o) be a dominated seÂ­
quential Bayesian experiment. Let B  C  A, C C  A  and let (7^) be a filtration 
adapted to (Sn). If
(*) 
9bvc = 9bÂ° * 9cÂ° j
(ii) 
V n > 0,
Ti-lV^n  
 
CVSn_iV Tn 
w 
\  n
(m ) 9sn 
= 9 sn 
V n > 0,
then (B,C> (%.)) operates a cut both in Â£^vsn and in 
â 
6.6.2 Necessity of Transitivity Conditions
As we have seen transitivity conditions allow the comparison of the 
initial and sequential admissibility of reductions in sequential Bayesian exÂ­
periments. It is natural to ask whether or not these conditions are relatively

6 .6 . The Role of Transitivity: Further Results
281
minimal in the sense that, under some other conditions on the experiment, 
the conditions of transitivity are really necessary. Along these lines, the 
main result is provided by the following theorem, which is to be compared 
to Proposition 6.5.3 and Theorem 6.6.1:
6.6.7 Theorem . If Sn 
C | B \/S q VTn and if C and (Tn) are both Â£^vsn~ 
and 
-mutually ancillary, then (Tn) is both (BVSo)- and (B VC V in Â­
transitive for (<Sn).
Proof. Let
(i) S n < C \ B
>
>
(ii) C Â± T n \ B V 50;
(iii)C Â±  Tn+1 | B V S n;
(iv)Tn+1 X  Sn 1 B v S 0 VTn;
(v)
Tn+1 X Sn | B V e V S0 V Tâ
We must prove that (i), (ii) and (iii) imply (iv) and (v). It follows from the 
proof of Theorem 6.2.3 (1 implies 2) that (ii) is equivalent to
(vi) C Â± T n + 1 \ B V S 0 VTn.
By an application of Theorem 5.4.13, (vi), (iii) and (i) imply that 
Tn+1Â±  (C V Sn) | B V So V Tn, which implies (iv) and (v). 
â 
This result is quite satisfactory but it must be recalled, in view of Section
5.5.2, that the assumption that Sn 
C | B V So V Tn is relatively strong in
this context. Indeed, along with C 1  Tn \ B \/S q, i.e., Tn is 
-ancillary,
by Theorem 5.5.10, Sn < C  | B\/So\/Tn implies that Tn is maximal ancillary
in f BwSQ 
m oCvSn.
Applying Theorem 6.6.7 to joint reductions entails the following propoÂ­
sitions:

282
6. Sequential Experiments
6.6.8 P roposition. If Sn <  A  | B V  SoV Tn and if B and (Tn) are both 
Â£^vsn- and 
-mutually sufficient, then (Tn) is (So)-, (BV So)- and
(A V <So)-transitive for (Â«Sn). 
â 
6.6.9 P roposition. If Sn <  C \ So V7^ and if C and (7^) are both S j^ Sn- 
and 
^-m utually exogenous, then (7^) is (<S0)- and (C V <So)-transitive 
for (Sn). 
â 
Remark that here we do not recover the necessity of (7^) being (*4VÂ«So)- 
transitive for (Â£â).
6.6.10 Proposition. If Sn <  A  \ B V So V Tn and if (#,Â£, (7^)) operates 
a cut both in Â£^vsn an(^ m 
then C^n) *s 
v ^o)-, (# V So)- and
(.4 V <So)-transitive for (Sn)- 
â 
We end this section with a proposition showing that under strong idenÂ­
tifiability conditions, two transitivity conditions imply mutual ancillarity in 
the initial experiment and in the sequential experiment. Using Theorem
5.4.13 again, the proof of next proposition parallels the proof of Theorem
6.6.7.
6.6.11 Proposition. If C <C Sn | B V So V Tn and if (Tn) is both (B V So)-
and (BVCV tS0)-transitive for (5n), then C and (7^) are both 
- and
s 
â¢ 
n
-mutually ancillary.
Let us remark that the assumption C 
Sn \ B V S q V Tn implies, 
by Theorem 5.5.4, that C V B V So V Tn is the minimal strong 
sufficient parameter. Note that this last property is frequently satisfied in 
applications.

7
Asymptotic Experiments
7.1 
Introduction
In this chapter, we continue the analysis of sequential Bayesian experiÂ­
ments begun in Chapter 6. Our present concern is the relationship between 
the experiment stopped at time n and the asymptotic experiment generÂ­
ating infinite sequences of observations. Of course, asymptotic theory is 
a vast field in statistical theory, and cannot be treated exhaustively in a 
single chapter. Thus, we concentrate the attention on a particular series of 
problems related to almost sure convergence. In particular, asymptotic norÂ­
mality of posterior distributions is not considered here (interested readers 
should refer to LeCam (1986), Chap. 12; see also Dickey (1976), Hartigan 
(1983), and Walker (1969).
As in previous chapters, probabilistic tools are presented separately in 
Sections 7.2 (on limits of sequences of conditional independences) and 7.6 
(on mutual conditional independences).
The first statistical topic compares admissibility conditions for global 
and asymptotic reduction. This problem was addressed in Chapter 6 with 
respect to filtrations (on the sample space). In contrast, in Section 7.3 
the hypothesis of filtration is avoided as far as possible; this permits the 
treatment of a sequence of statistics rather than an increasing sequence 
relative to the accumulation of observations of some variables.
283

284
7. Asymptotic Experiments
The second statistical topic, treated in Section 7.4, studies the exisÂ­
tence of a consistent estimator for real-valued functions of parameters. In 
the present framework this is precisely the asymptotic aspect of exact esÂ­
timability as presented in Section 4.7. We also examine, in some detail, 
the relationship between the Bayesian concept and various sampling-theory 
concepts of estimability. This second topic is illustrated, in Section 7.5, 
by the presentation of equivalent conditions for the exact estimability of 
discrete cr-fields (i.e., cr-fields generated by countable partitions).
Finally, Section 7.7 studies asymptotic properties in particular classes 
of experiments with special emphasis on conditional experiments; motivaÂ­
tion for this analysis is found, in particular, in the desirability of obtaining 
results relevant for the asymptotic properties of regression-type models. 
Heuristically, an almost sure convergence of posterior expectations is ofÂ­
ten based on the existence of a sub-cr-field (on the sample space) that is 
both asymptotically sufficient and almost surely included in the cr-field of 
the parameters. Asymptotic sufficiency is established using techniques deÂ­
scribed in Section 7.3; the property of being almost surely included relies 
on slight extensions of 0-1 laws, i.e., laws for independent variables which 
are extended to conditionally independent variables. The relevant 0-1 laws 
are presented in a pure probabilistic set-up in Section 7.6; their statistical 
applications are in Section 7.7.
In the literature on time series analysis â particularly in econometÂ­
rics â parameter estimability is generally obtained for stationary ergodic 
models. The results to be presented differ from the usual results in two 
respects. Firstly, the property of stationarity is weakened into a property 
of tail-sufficiency, which seems to be the weakest property which leads to 
suitable asymptotic behaviour. As this property may be difficult to verify 
directly we use, in Chapters 8 and 9, stationarity (in the sense of invariÂ­
ance for the shift) as a natural way of getting tail-sufficiency. Secondly, 
the property of ergodicity for the unreduced observations is weakened into 
a property of conditional ergodicity. In this chapter, ergodicity is in fact 
obtained through a condition of independence, to be weakened into condiÂ­
tional independence; because mixing properties require more structure, they 
are introduced in Chapter 8. In brief, a sequence of observations (yn,zn) is 
shown to exactly estimate the parameters characterizing the (y | z)-process 
under a condition of joint tail-sufficiency and of conditional independence.

7.2. 
Limit of Sequences of Conditional Independences
285
This chapter seeks to provide the basic tools for the study of the asympÂ­
totic properties of a Bayesian experiment; in consequence, only a minimum 
of structure is introduced and as the necessary conditions are presented in 
what seems to be the weakest possible form, they are not verified directly; 
indeed the object of Chapters 8 and 9 is to use invariance arguments â such 
as stationarity or exchangeability â as efficient tools for checking the necesÂ­
sary conditions. This explains why this chapter has relatively few examples, 
and instead emphasizes methods; Chapter 8, and particularly Chapter 9, 
may be viewed as presenting examples and applications of the methods disÂ­
cussed in this chapter. Nonetheless the final section of this chapter presents 
a simple example which illustrates most of the concepts presented in the 
preceding sections.
Florens and Rolin (1984) reported the first steps of the work underÂ­
lying this chapter; subsequent development of the asymptotic properties 
of conditional models and of finite parameter spaces is found in Florens, 
Mouchart and Rolin (1986) and Florens and Mouchart (1987c). However, 
this chapter considerably expands that material, and in particular incorpoÂ­
rates a systematic study of asymptotically admissible reductions and of the 
correspondence between sampling theory and Bayesian results.
7.2 
Limit of Sequences of Conditional Independences
The concern of this section is to describe those probabilistic tools which 
are most useful for the analysis of the properties of the limit of a sequence 
of conditional independence properties. We first recall the basic concepts of 
martingales and of tail cr-fields and then consider sequences of conditional 
independences, both with regards to filtrations and in general.
In this section, (M ,M , P )  is again an abstract probability space, M i  
are sub-cr-fields of M  (in particular Mq = {<]), M }),(F n) is a filtration in 
(M, M )  and (Qn) is a sequence of cr-fields adapted to (Fn) (see Definition 
6.2.1). Also remember that [Mi]p represents the class of p-integrable Mi- 
measurable (real-valued) random variables defined on M .

286
7. Asymptotic Experiments
7.2.1 D efinition. Consider a sequence (/n) of random variables. (/n) is a 
martingale adapted to a filtration T n (or (Pn)-martingale) if
(i) 
fn G [Tn] 1
(ii) 
T nfn+l = fn-
7.2.2 Theorem . Let T n |  Too and (/n) be an (Pn)-martingale.
1. 
If (fn) is bounded in C\, i.e., supn Mo(\fn\) < oo, then there exists
9 i Â£ [A4]i such that
(0) fn -*â¢ 9i a.s.P.
2. 
The following conditions are equivalent:
(1) 
3g2 G [M]\ such that f n ââº 02 in Li;
(ii) 
3<jf3 Â£ [A4]i such that fn =  P n#3 a.s.P, V n e JN;
(iii) (/n) is uniformly integrable,
i.e., lim supn X 0[|/n|l{|/n|>a}] = 0.
aâ*-oo U l J
Furthermore (iii) implies that (/n) is bounded in L\ and so if any one of
these conditions is satisfied this implies (o) and g\ 
= g2 
â Too9s a.s.P.
3. 
If (fn) is bounded in Lp with p > 1, i.e. supn A4o(|/n|p) < oo,
then 2.(iii) holds and therefore fn ââº g\ a.s.P and furthermore in Lp.
For proofs and more details on martingales see, e.g., Neveu (1964) IV-5 
(in particular Proposition IV-5-6), Neveu (1972), Chapter 4, or Dellacherie- 
Meyer (1980), Chapter V.
We now recall some facts on tail (or asymptotic) cr-fields.
7.2.3 D efinition. Let (Qn) be a sequence of sub-cr-fields of M. The tail- 
a-field Qt is defined as
Gt = Pi V Gm-
n>0m>n
We also write (Gn)r for Qt when such additional precision is required. 
When Qn is non-increasing (i.e., Qn+i C Gn 
V n Â£ JN or Qn j),

7.2. 
Limit of Sequences of Conditional Independences
287
Qt = fln>0 Gn whereas when Gn is non-decreasing â or equivalently a 
filtration â (i.e., Gn C Gn+i V n E JN or Gn t), Gt = Goo = Vn>o^n- 
When the sequence (C/n) is generated by a sequence of random variables 
<7n (i.e., Â£/n = cr(^n))j the tail cr-field Gt is the smallest cr-field that makes 
measurable the functions depending on the âlastâ coordinates gn but not 
on the first m ones for any finite m. Note also that Gt makes liminf gn and 
limsup #n measurable for any sequence (<7n) such that gn E [Gn]-
Note that Lemma 2.2.5(i) may be generalized into:
(7.2.1) 
\ /  5T =  V  &
n>0 
n>0
for arbitrary sequences (Gn) whereas f\Gn is, in general, different from f]Gn\ 
the following properties are nonetheless observed:
7.2.4 Proposition. (Elementary Properties of the Tail Operation)
(i) 
rin>oSÂ» CDn>o5TC^;
(ii) 
{Qn )t = Qt ',
(iii) If T n t ^oo and Qn t Qoo, then {Tn V Qn)^  = 
V 
;
(iv) If T n I 
, then T ff  = ( T n )00.
Proof. The first inclusion in (i) is trivial. For the second one, let us take 
B  E fl Gn) i-e., V n, 3 (B n ) such that B n E Gn and B  = B n a.s. Because 
limsup B n E G t ( by the definition of Gt), and B  = limsup B n a.s. we have 
B  E Gt- Note that if Gn is decreasing, the three cr-fields in (i) are equal; 
this proves (ii) and (iv). Finally, (iii) derives directly from the associativity 
of V-operation once it is remarked that T n V Gn is also increasing. 
â 
In the sequel we often rely on the following argument. Consider an 
integrable random variable defined on M  : m E [M] i and a filtration in 
M  : T n t Too. The sequence T n^n is a (^7n)-martingale which converges 
a.s. and in L\ to Toom \ this is an immediate application of Theorem 7.2.2. 
In particular, T nm ââº m once m E [^oo]!* The next theorem illustrates

288
7. Asymptotic Experiments
another use of Theorem 7.2.2 when applied on a sequence of projections of 
(7-fields.
7.2.5 T heorem . For any sub cr-field M i C M  and any filtration T n |  Too 
in M :
T o o M i  C ( T n M i ) T -
Proof. It is sufficient to show that, for any m G [A4i]i, TooTn is a.s. equal 
to a function measurable for the tail cr-field of the sequence (TnM i). Indeed:
V m G [M i]i : T nm ââº T ^ m  
a.s.P
by Theorem 7.2.2. As T nm G T nM i, lim suppnm G (TuM i )t - Therefore 
Toom is a.s. equal to a function in {TnM i)T- 
â 
Reversing the orientation of the time index yields the following reverse 
martingale theorem (for proof, see, e.g., Dellacherie and Meyer (1980), TheÂ­
orem V-33).
7.2.6 Theorem . Consider a decreasing sequence T n j T t in M . For any 
m G [M\i : T nm ââº T ttti both a.s. and in L\. In particular, T nm ââº m for
m Â£ [Tt ]i- 
â 
We now present some results which combine conditional independence 
and martingale. The first result is similar to Theorems 6.2.2 and 6.2.3 
where monotone class arguments are replaced by martingale arguments; 
this allows treatment of arbitrary sequences of conditioning (7-fields rather 
than filtrations.
7.2.7 Theorem . Consider three filtrations in M  : T n \ T 0Q) Qn |  Qoo and 
M n T Moo along with a sequence Wn adapted to (T n V M n), i.e.,
(0) 
H n C T n V M n  V TI Â£ IN.
If
(1) 
Tn Â±Qn | MnVKn  V n G l ,  
then
(ii) 
Too Â±Goo |(AfooV7fn)T.

7.2. 
Limit of Sequences of Conditional Independences
289
Proof. From (o) and (i) we have, for any g â¬ [Qkji, 0 < k < n ,
(iii) (Tâ V M n)g = (Tin V M n)g.
By Theorem 7.2.2:
(iv) (Tn V Mâ)g -*â¢ (Too V .Moo)<7 
a.s.
Taking the limsup on both sides of (iii) we have, from (iv):
(v) 
(T ^  V Moo)g = limsup (7iâ V Mâ)g 
a.s.
n
which may be written as
(vi) (Too V Moo)g â¬ [(Wâ V.Mâ)t ] â¢
As M n T Moo implies (7in V M u)t = (Un V M oo)t , (vi) implies:
(vii) (^oq V A^oo)^ =: (Wn V A4oo)t<7>
thus (ii) follows from the fact that (o) implies (7i n V M oo)t C ^oo V AIqo 
because of elementary Property 7.2.4. (iii) and from Monotone Class TheÂ­
orem 0.2.21. 
â 
Later on it will be useful to have a condition that allows one to replace 
(Moo V W â)t in (ii) of Theorem 7.2.7 by Moo V?7t; describing such a conÂ­
dition is the object of the next theorem; its corollary provides an alternative 
description of a tail-cr-field.
7.2.8 
Theorem . Let M i C M  and (Qn) be an arbitrary sequence in M . 
Denote, as in Chapter 6, Q* = 
\J  Qn (for 0 < r < s < oo). If
r<n<s
(i) 
M \ X (?oÂ° I Gt, 
then
(ii) 
( M i V Qu)t = M i  V Qt â¢

290
7. Asymptotic Experiments
Proof. We have, by definition, that:
(iii) { M ^ Q n)r = n â  Vm>â(M i VSm) = n â ( ^ i  V<?~);
therefore, as Afi V Qt C Afi V Qâ¢ V n, we have M \ V Gt C (Afi V Qn)T 
where the inclusion may be strict; the hypothesis (i) will be used to prove 
the reverse inclusion.
Indeed, (i) implies that
(iv) M i Â± g ? \ g ?  V n e w  
or, equivalently,
(v) 
( M 1 v g â¢) Â± G ? \ g ? V n e i N .
By (iii), this implies that
(vi) 
( M i  v g n ) T  Â± g ? \ g ? v n e i N .
By Theorem 7.2.7, (vi) implies that:
(vii) (Afi v gn)T Â± g â¢ \g T
and, therefore, by Corollary 2.2.11 as Afi C (Afi V Gn)T'-
(viii) (Afi V gn)T JL (Afi V t/oÂ°) I Afi V Gt .
But (iii) also implies that (Afi VGn)T C Afi VÂ£/qÂ°; therefore, by Corollary
2.2.8, (viii) implies that (Afi V Gn)T C Afi V Gt â¢ 
â 
Combining twice Theorems 7.2.7 and 7.2.8 we obtain the following corolÂ­
lary, which is useful for the subsequent analysis:
7.2.9 Corollary. Under the conditions of Theorem 7.2.7 (namely,
En T Too, Gn T Goo,Mn t Afoo and Tin c  Tn V Afn V Ti e IN), if:

7.2. 
Limit of Sequences of Conditional Independences
291
(i) 
T n Â±gn \ M n V H n  V n â¬ IN
(ii) 
M n Â± n s \ n n,
then
(iii) Too JL Goo I Moo V H t . 
â 
From Theorem 7.2.7 one may obtain the following dual result to TheoÂ­
rem 7.2.5, obtained by permuting projection and filtration.
7.2.10 Theorem . Let M i  C M  and T n t Too be a filtration in M .  Then 
â¢MiTn |  M iTooâ¢
Proof. As T n is a filtration, by elementary Property 4.3.2(iv), M i T n is 
also a filtration; therefore, ( M iT n)oo C M iT qo- By Theorem 4.3.3(i), 
M i X T n | M i T n V n; thus by Theorem 7.2.7, M i X Too | (M i T n)ooâ¢ 
By Theorem 4.3.3(h), 
MiToo is the smallest sub-cr-field of M i 
conditionally on which M i and Too are independent; this implies that 
MiToo C ( M iT n)oo- The equality is obtained once it is noted that both
sides contain all the null sets of M i- 
â 
The next theorem is useful for the asymptotic study of dominated exÂ­
periments.
7.2.11 Theorem . Let P and Q be two probabilities on ( M, M)  and let 
T n T Too be a filtration in M .  If
then
(ii) 
fn is a martingale and fn ââº foo a.s.Q;
(iii) f n ââº foo in Li w.r.t. Q if and only if P <C Q;
(iv) foo is a generalized Radom-Nikodym derivative, i.e., foo is characterÂ­
ized by: 3N  E T 00 such that Q(N) = 0 and
(i) 
P rn <  Q rn V n > 0 and if /â = d Prn/d  Q?n,

292
7. Asymptotic Experiments
Proof, (ii) is a simplified version of Proposition (III-2-7) in Neveu (1967);
(iii) follows straightforwardly from Theorem 7.2.2 (see also Neveu (1964, 
ex: IV-5-3)). 
â 
7.3 Asymptotically Admissible Reductions
In this section we analyze the asymptotic admissibility of reductions. 
We first present some asymptotic properties of sequential experiments (SecÂ­
tion 7.3.1) before considering the asymptotic admissibility of marginalizaÂ­
tion (Section 7.3.2) or of joint reductions (Section 7.3.3) in an unreduced 
experiment. We conclude this section by extending this analysis to condiÂ­
tional models.
7.3.1 
Asymptotic Properties of Sequential Experiments
Let us consider a sequential Bayesian experiment as in Definition 6.3.1:
S = {A x S, A  V S, n, Sn ] Soo).
In this chapter we implicitly assume, as in Chapter 6, that n Â£ JN but we 
accept that Soo C S is not necessarily equal to S (for instance, S may 
contain events due to randomization).
We first consider a limit property for posterior expectations. A direct 
application of Theorem 7.2.2 implies the following simple result:
7.3.1 Proposition. For any a Â£ [*4]i, Sna is a (Â«Sn)-martingale whose limit 
(a.s. and in Li) is equal to S^a . 
u
It should be stressed that Proposition 7.3.1. gives a genuinely Bayesian 
property in the sense that the martingale property of the Bayesian estimaÂ­
tor S na â for an arbitrary (but integrable) function a of the parameters 
â is a property of the joint probability II (more specifically, of the preÂ­
dictive probability P, i.e., the marginal of II on Soo); it differs from the 
usual asymptotic analysis of Bayesian estimators which is undertaken with 
respect to sampling probabilities. Connections between these approaches 
are considered in Section 7.4.

7.3. Asymptotically Admissible Reductions
293
Theorem 7.2.11 permits the characterization of the limit of the sequences 
of densities associated to dominated experiments. As the limiting sampling 
probabilities tend to be undominated in most interesting cases, we assume 
that the probability II is absolutely continuous with respect to the product 
of its margins in the finite horizon but not asymptotically.
7.3.2 P roposition. If
(i) 
n AwSn< fiÂ® P sâ , 
V n e l N  
then
(ii) 
gAvsn is a positive {A V Sn )-martingale with respect to p 0  P
which converges to goo a.s.// 0  P; the Lebesgue decomposition of II with 
respect to p 0  P  on A  V Â«S, is then given by
where N  Â£ A  V S  and (p 0  P)(N) = 0.
(iii) gAvsn ââº goo in Â£i if and only if II <  p 0  P  on A  V Soo ] 
in this case g ^  = gAvsÂ«, and gAvsn = (A V Â«Sn)Â£oo.
(iv) goo 
9 a.s./i 0  P  
if and only if 
II_L(// 0  P). 
â 
Note that (ii) is a simple consequence of (1.4.17) for J\f = A V Sn, if we 
recall that <Mv5n/ = (-4 V Â«Sn')0>ivsn V n' < n (see also in Section 6.3.1 the 
description of global analysis).
7.3.2 
Asymptotic Sufficiency
We now investigate some relationships between sufficiency in finite samÂ­
ples and asymptotic sufficiency.

294
7. Asymptotic Experiments
7.3.3 D efinitions.
(a) 
A sequence of statistics ('Tn) is a sufficient sequence if:
(i) (Tn) is (Sn)-adapted, i.e., Tn C S n 
V n G IN]
(ii) Tn is Sufficient in Â£Uv5nJ the experiment stopped at time n, i.e., 
A  1  5n | Tn V n e M.
(b) 
A statistic T is asymptotically sufficient if:
(i) T  c5oo;
(ii) 4 1 5 o o  | T. 
â 
Note that, in general (7^) is not a filtration.
7.3.4 Proposition.
(i) 
If (7^) is a sufficient sequence of statistics its tail-cr-field, 7r, is asympÂ­
totically sufficient (i.e. Tn C Sn and A  X Sn | Tn imply Tt C S qq and
A  A. Soo | Tt )\
(ii) 
If T  is an asymptotically sufficient cr-field, then (SnT ) is a sufficient 
sequence of cr-fields. 
â 
Note that (i) is a straightforward application of Theorem 7.2.7 while 
(ii) is a simple consequence of Corollaries 2.2.4 (ii) and 4.3.5 (iii). Also, (i) 
can be viewed as a Bayesian version of similar results in a sampling theory 
framework (see Dynkin (1978), Diaconis and Freedman (1978), Lauritzen 
(1980), Section IV.4); result (ii) provokes the following question: Is the tail 
cr-field of the sequence (SnT) equal to T? The answer is no, in general, but 
the following inclusion obtains:
7.3.5 P roposition. Any asymptotically sufficient a-field is included in the 
tail cr-field of the sufficient sequence formed by its projections on Â«Sn, comÂ­
pleted by the null sets of Soo, i-e., A  X S 00 | T  => T  C (SnT )T fl Soo â¢ 
â 
A. Sufficiency on th e Sam ple Space
Indeed, the inclusion T  C (S u T ) t  H S q0 is a direct consequence of 
Theorem 7.2.5, after noting that T  C Soo implies Soo'T = T flSoo; Example
9.3.6.D shows why the inclusion is not, in general, an equality.

7.3. Asymptotically Admissible Reductions
295
If we now start from a sufficient sequence (Tn) we know, by Proposition
7.3.4 (i), that its tail-cr-field Tt is asymptotically sufficient and by PropoÂ­
sition 7.3.4 (ii) that the sequence (SnTx) is a sufficient sequence. Example
9.3.6.D also shows that there is, in general, no adaptation between the two 
sequences (Tn) and (<Sn^r)- This is, however, the case if (Tn) is minimal 
sufficient.
B. M inim al Sufficiency on th e Sam ple Space
Recall, from Section 4.3, that (<Sn*4) is a sequence of minimal sufficient 
statistics and that Soo A  is a minimal asymptotically sufficient statistic. We 
now investigate the links between S oqA  and the sequence (Sn*4). From 
Theorems 7.2.5 and 7.3.4 we have successively,
In (7.3.1) the inclusion is generally strict; in other words, the minimal 
asymptotically sufficient statistic is generally smaller that the tail-cr-field 
of the sequence of minimal sufficient statistics (see Example 9.3.6.D). SimÂ­
ilarly, in (7.3.2), the inclusion is generally strict, i.e., if one projects the 
minimal asymptotically sufficient statistic tSoo*4 on the filtration (Sn) we 
get a sufficient sequence e>n(Â£oo^4) which is not necessarily minimal. HowÂ­
ever, this will be the case under a strong identification condition; the next 
theorem proves a slightly more general result:
7.3.6 Theorem . If T C Soo is asymptotically sufficient and two-complete, 
then the sufficient sequence (tSn^) is minimal, i.e., SnT  is Â£4vs n-minimal 
sufficient for all n.
Proof. In order to prove that <SnT  = SnA  we first note that, as T  <C2 A  
implies T  <C A, we have by Theorem 5.5.4 (i) that T = Soo A. Therefore, 
SooA<^2 *A by Proposition 5.4.2. By the elementary Property 4.3.2. (vi), 
we then obtain that SnT  = SnT  = tSn(Â£oov4). Finally, Theorem 5.4.14 
implies that <Sn(Â£oo.4) = SnA. 
â 
(7.3.1)
(7.3.2)
Soo A  
C 
(5â^4)t ; 
Sâ.4 
C 
<Sn(Soo.4);
SnA  
C 
<Sn(5co^) C 5â{(Â«5â^) t }.
Note that, in view of Theorem 5.4.14, the condition of 2-completeness

296
7. Asymptotic Experiments
may be replaced by âp-complete for some p such that 1 < p < ooâ. Example
9.3.6.D also shows that 2-completeness of an asymptotically sufficient cr- 
field does not imply, in general, that its projections on the filtration Sn are
2-complete. However, the issue of whether the tail-<r-field of a sufficient 
sequence of 2-complete cr-fields is the minimal asymptotically sufficient cr- 
field seems unresolved.
C. Sufficiency on th e P aram eter Space
Sufficiency on the parameter space in sequential experiments is a parÂ­
ticularly relevant topic in problems with incidental parameters such as 
xn ~  i.N(pnia2) (see Florens, Mouchart and Richard (1974, 1976, 1979, 
1986 and 1987); see also Neyman (1951), Neyman and Scott (1948, 1951), 
Reiers0l (1950), Solari (1969), Sprent (1966, 1970), Anderson (1976) and 
Deistler (1976, 1986)). In this context identification (i.e., minimal suffiÂ­
ciency) and estimability, along with their connections between the sequence 
of finite sample experiments and the asymptotic experiment, are not only 
of interest in their own right, but also represent the first step in the analysis 
of sequences of conditional models ( examined in Section 7.3.4).
Symmetrically to sufficiency on the sample space, we consider a sequence 
(Bn) of sub-cr-fields of A:
7.3.7 D efinitions.
(a) 
a sequence of parameters (Bn) is a sufficient sequence if
(i) 
Bn C A ,  
V n G l ;
(ii) Bn is sufficient in Â£Avsn, be., A  JL Sn \ Bn V n G l .
(b) 
a parameter is asymptotically sufficient if
(i) 
B C  A;
(ii) B is sufficient in the asymptotic experiment, i.e., A  JL Soo | B. â 
7.3.8 Proposition.
(i) If {Bn) is a sufficient sequence of parameters, then its tail-cr-field, Bt , is 
asymptotically sufficient (i.e., Bn C A  and A  X Sn \ Bn imply Bt C A  and 
A  X  S.* I Bt )

7.3. Asymptotically Admissible Reductions
297
(ii) If B is an asymptotically sufficient parameter, then B is Â£Avsn-sufficient 
for all n (i.e., B C A  and A  X <Soo I & imply that A  X Sn | B V n). 
â 
Note that (i) is a direct application of Theorem 7.2.7 whereas (ii) is a 
trivial application of Corollary 2.2.4 (ii). In Proposition 7.3.8, the sequence 
(Bn) is, in general, not a filtration. The sequence of minimal sufficient 
parameters is nevertheless a filtration. More specifically, we have from TheÂ­
orem 7.2.10:
7.3.9 Proposition. The sequence of minimal sufficient parameters is a 
filtration converging to the asymptotic minimal sufficient parameter (i.e., 
A S n |  ASoo). 
â 
As far as identification is concerned, it may be noted that if an exÂ­
periment Sa v s*. is identified (i.e., *4Sn = *4) for some finite n, then it is 
also identified for any n' > n and therefore asymptotically identified (i.e., 
AScc = .4); of course, the converse does not hold, but we have the following 
result:
7.3.10 Proposition. Any b E [ A S ^ i  is a limit, a.s. and in 
of a 
sequence bn E |y45n]i- In particular, if Â£avsq0 is identified, this property 
holds for any b E [A] i . 
â 
By Proposition 7.3.9, this follows straightforwardly from Theorem 7.2.2 
applied to the sequence bn = (*4<Sn)6. Note that if ASoo ^  A S, what is 
identified in Savs but not in Â£avs 00 cannot be approximated by a sequence 
bn E [.45n]i.
7.3.3 
A sym ptotic A dm issibility of Jo in t R eductions
A. M utual A ncillarity
Choosing { M n) and ('Hn) to be trivial in Theorem 7.2.7 gives a simple 
limit result for sequences of mutually ancillary <r-fields when their sequences 
are filtrations:

298
7. Asymptotic Experiments
7.3.11 Proposition. If (Cn) and (Tn) are two filtrations (in A  and in S) 
of mutually ancillary cr-fields, their limit is also mutually ancillary, i.e.,
Note that if either (Cn) or (7^) were not a filtration, the conclusion might 
fail to hold for the tail cr-fields; this is shown in the following example:
E xam ple. Let sn = (xn,yn)' and
let Sn = cr(si , ... ,sn), A  -  cr(a), T2n- i = v{xn), T2n = a(yn) and Cn = A. 
Clearly, the sequences (Cn) and (Tn) are mutually ancillary, however, (Tn) 
is not a filtration, (Cn) and (Vi<*<n 
are no^ mutually ancillary and, as 
will be shown in Chapter 9, Ct = A  and Tt are not mutually ancillary
B. M utual Exogeneity
With regards to mutual ancillarity, asymptotic properties are easily obÂ­
tained for filtrations:
7.3.12 Proposition. If two filtrations Cn |  Coo and Tn j 7 ^ form a pair 
of mutually exogenous sequences (i.e., Cn X Tn and 
| Cn V Tn),
then their increasing limits are also mutually exogenous (i.e., Coo JL 'Too 
and A  Â±  Soo | Coo V Too). 
â 
This proposition is a simple consequence of Proposition 7.3.11 and TheÂ­
orem 7.2.7, with T n â A, Qn = Â«Sn, M n â Cn M Tn and 7in trivial. If the 
sequence Tn is not a filtration, then a similar result holds, although someÂ­
what restricted as it requires (Cn) to be a constant sequence and strenghtens 
the condition of mutual ancillarity.
7.3.13 T heorem . Let C C A  and (7^) be an (Sn)-adapted sequence. If, 
in Â£.Avsn, C and Tn are mutually exogenous and C and Tq are mutually 
ancillary for any n, than C and 7 r are mutually exogenous and C and T0Â°Â° 
are mutually ancillary in ^vSoo â¢
Cn T Coo,Tn |  ^
 and C â l T n, V n G IN, imply Coo JL T^.
â 
since A  C 7^.

7.3. Asymptotically Admissible Reductions
299
Proof. After recalling that Tp C TqÂ° , one may rewrite the theorem as: If
(i) 
C X T0n V n e M
(ii) 
A Â± S n \ C V T n 
V n G l ,
then
(iii) C X TqÂ°
(iv) A Â± S o o  | CVTt .
That (i) implies (iii) is a simple consequence of Theorem 7.2.7; by this same 
theorem, (ii) implies that: A  X Soo I (C V Tu)t and by Theorem 7.2.8, (iii) 
implies that: CV Tt = (C V 7^)^. 
â 
C. M utual Sufficiency
For mutual sufficiency, the result analogous to Theorem 7.3.13 is the 
following:
7.3.14 
Theorem . Let (Bn) be a filtration of parameters, and let (Tn) be 
an arbitrary sequence of statistics. If Bn and Tn are mutually sufficient and 
Bn is 
-sufficient, then Boo and Tt are SavSoo-mutually sufficient and
Boo is SavTÂ£Â°-sufficient.
Proof. This theorem may be rewritten as: If
(i) 
Bn X Sn I Tn V n G m
(ii) 
A  x  T0" I Bn V n Â£ IN
then
(iii) Boo Â±  Soo \ Tt

300
7. Asymptotic Experiments
(iv) A  X TqÂ° | Boo,
since (iv) trivially implies that A  JL Tt \ Booâ¢ Now, by Theorem 7.2.7, (iii) 
is implied by (i) and (iv) is implied by (ii). 
â 
D. Cut
With regards to mutual exogeneity and mutual sufficiency, Theorem
7.2.7 trivially implies that, for filtrations on both the parameter and the 
sample spaces, cuts for finite n produce asymptotic cuts. More interesting 
are the following results in the same spirit as Theorems 7.3.13 and 7.3.14:
7.3.15 
T heorem . Let (Bn) be a filtration in A, C C A  and (Tn) an (Sn)- 
adapted sequence. If (BniC, Tn) operates a Bayesian cut in ^ v 5 n> an(^ ^  
Bn is Â£avT*-sufficient, then (5oo>C>Tr) operates an asymptotic cut, i.e., a 
cut in Â£avSoo > and Boo is Â£avTÂ£Â°-sufficient.
Proof. We first restate the theorem as: If
(i) 
Bn X C,
(ii) 
A  JLTq \ Bn
(iii) A Â± S n | CVTn,
then
(iv) Boo X C,
(v) 
A  X TqÂ° | Boo
(vi) *4XSoo | CVTT.
As before, (i) implies (iv) and (ii) implies (v). But, A  X Soo | (C V T u ) t  is 
implied 
by 
(iii); 
and 
(i) 
and 
(ii) 
imply, 
by 
Theorem 
2.2.10, 
C X (TÂ£ V Bn), which yields C X  TqÂ° . Under Theorem 7.2.8 this gives 
(C V Tu) t â (J V Tt< 
â 

7.3. Asymptotically Admissible Reductions
301
Note that when the sequence (7^) is not increasing, condition (ii bis) 
â A  1  T n  | Bn â is weaker than condition (ii) which is required in order 
to ensure a sequence of cuts to have a suitable asymptotic property, i.e., an 
asymptotic cut with the tail <7-field of (Tn). From Theorem 6.2.3, condition
(ii) may be rewritten in several equivalent forms. For instance, since (ii) is 
equivalent to (ii bis) and A  J L T â¢ \ BnVTn, then if (Bn,C, Tn) operates a cut 
the condition Bn is SavT*-sufficient is equivalent to Tn is S ^ Tn-sufficient.
7.3.4 
Asymptotically Admissible Reductions in Conditional ExperÂ­
iments
The asymptotic properties of regression-type models are based on the 
analysis of a sequence of conditional experiments such as 
, where both 
('Tn) and (M n) are filtrations. More precisely, Tn |  
is (Sn)-adapted and 
M n T Moo is (*4VÂ«Sn)-adapted (typically, (A4n) is generated by a sequence 
of exogenous variables). In the spirit of Definition 6.3.1, such a structure is 
called a sequential conditional Bayesian experiment.
In this section, we analyse merely the problems of sufficiency and anÂ­
cillarity. Joint reductions in conditional experiments do not raise problems 
specifically different from those encountered in the complete experiment.
A. A ncillarity
A simple consequence of Theorem 7.2.7 is the following proposition:
7.3.16 P roposition. A filtration 1Zn |  T^oo of ^Bwrn-ancillary statistics, 
i.e., 7Zn C M
n  V T n and B  X l t n I M
n , is asymptotically ancillary, i.e., 
B Â±
1Z0 0 \ M 0 0 .
B. Sufficiency on th e Sam ple Space
A straightforward reinterpretation of Proposition 7.3.4 entails the folÂ­
lowing proposition:
7.3.17 P roposition.
(i) 
If (An) is a sequence of Â£^3^-sufficient statistics, i.e., Afn C M n VTn 
and B  X T n | A4n VA/ân, then { M o o  V A /^t is a strong 
-sufficient

302
7. Asymptotic Experiments
statistics, i.e., Moo C {Moo 
and B X Too\{M  OO V Afn)r',
(ii) 
If Af is a 
-sufficient statistic, i.e., 3  i  Too \ Moo V Af, then
(AAn VTn)(AAoo VAf) is a sequence of strong 
-sufficient statistics,
i.e., M n C (M nVTn)(MooVAf) and B Â±  Tn | (A4nVTn)(MooVAf). -
With regards to minimal sufficiency on the sample space in conditional 
experiments, recall that
(7.3.4) 
Af*n = {M n V Tn)(B V M n)
and
(7.3.5) 
Af** = {Moo V Too){B V Moo)
are, respectively, the minimal complete strong sufficient 
- and 
-
statistics.
As in (7.3.1), (7.3.2) and (7.3.3) of Section 7.3.2.B, we have:
(7.3.6) 
Af** C Afi
(7.3.7) 
Af* C {M n V Tn)Af**,
and this implies, in turn, that
(7.3.8) 
AT* C {M n VTâ)ATt .
In contrast with unreduced experiments, equality in (7.3.7), i.e., the minimal 
sufficiency of the projection of Af** on {M n V Tn), is not assured by a 
strong identification condition only; it requires a supplementary transitivity 
hypothesis , as shown in the next theorem:
7.3.18 Theorem . If Af C Moo V Too is a 2-complete 
-sufficient
statistic, and if { M n) is 5-transitive for { M n V Tn), then the sequence 
{ M n V Tn)Af is a sequence of strong minimal Â£ ^ 7^-sufficient statistics.
Proof. By Definitions 5.5.1 and 6.4.1, and by Theorem 6.2.2, this theorem 
may be rephrased as: If
(i) 
B X Too \ M 0 0 VAf,

7.3. Asymptotically Admissible Reductions
303
(ii) 
^ < 2  B | Moo, and
(iii) Moo 1- % | B V M n,
then
(iv) ( M n VTn)M = M:.
Since Af <C2 B \ Moo implies Af 
B | Moo, and since A* V Afoo is 
a strong 
-statistic, by Theorem 5.5.4 (i), that Af V Moo = Af**.
Therefore, Af** <C2 B \ M 0o- This is equivalent to Af** <C2 (# V A4oo). 
Therefore, by Corollary 5.4.15,
( M n V Tn)Af** 
= 
(M nV T n){(M 00VT00)(5 v A 4 00)}
= 
(A4nVTn)(Â£VA4oo).
Now, since (iii) implies Moo JL {M n VTâ) | B V Afn, we conclude, from 
Corollary 4.3.6(iv), that (A4n\ZTn)(BVMoo) = (AfnVTn)(SvA4n) = A'*. â 
Recall that Proposition 6.4.3 gives alternative characterizations of the 
transitivity condition (iii).
C. 
Sufficiency on th e P aram eter Space
Under supplementary conditions on the conditioning filtration, we now 
extend Proposition 7.3.8 to conditional experiments.
7.3.19 
Theorem . If (Â£n) is a sequence of 
-sufficient parameters such 
that 
JL M n | Cn V n E IN, then Ct is asymptotically sufficient. MoreÂ­
over Â£gÂ° Â±  A4oo | Â£ t-
Proof. This may be rephrased as: If
(o) 
C n C B W M n ,
(i) 
B Â± T n \Cn V M n

304
7. Asymptotic Experiments
(ii) 
C % Â± M n \Cn, 
then
(iii) B X  Too | Â£ t  V Moo,
(iv) Â£gÂ°Â±A40O|Â£ T.
By Theorem 7.2.7, (i) implies B JL 7 ^ | (Â£n V M oo)t and (ii) implies 
Â£qÂ° i  M co | Â£ t â¢ It also implies (Cn V M oo)t = Â£ t  V .Moo, by Theorem
7.2.8. 
â 
7.3.20 
T heorem . C C B is a uniform i^ ^ -su ffic ie n t parameter and 
( M n) is Â£-transitive for (Tâ) if and only if Â£  is a uniform Â£ ^ 5^-sufficient 
parameter for each n and ( M n) is B-transitive for (Tn).
Proof. This may be rephrased as: Let
(0) 
Â£ C B.
Then
(1) 
B Â± T 00\CWMoo
(ii) 
Moo Â± T n \ Â£ V M n
are equivalent to
(iii) B I 7 â | Â£ V M n
(iv) Moo JLTâ \ B W M n.
Clearly (i) is equivalent, by Theorem 6.2.3, to:
(v) 
B 1-T n \ C V Moo-

7.3. Asymptotically Admissible Reductions
305
By Theorem 2.2.10, (v) and (ii) are equivalent to (BVMoo) -1L %i \ CV M n 
which is equivalent to (iii) and (iv). 
â 
For minimal sufficiency on the parameter space in conditional experiÂ­
ments, Proposition 7.3.9 may be extended; however some care is required in 
treating null sets, and a transitivity condition is also required. Recall that
(7.3.9) 
Cn 
= ( B V M n) ( M n VTn)
and
(7.3.10) 
Â£** 
= (BVMooXMooVToo)
are, respectively, the minimal complete strong sufficient parameter in 
and in 
.
7.3.21 Theorem . If {Mn) is S-transitive for (7^), then (Â£*) is a filtration 
such that Â£** =Z ^f l( BV Mo o )*
Proof. Since .Moo JL %i | B V M n implies that
(B V Moo) JL (Mn V Tn) | B V M n, we obtain, by Theorem 4.3.7 (iv), 
= {(B V Moo)(Mn V 7^)} fl (B V M n)- Now, clearly, Â£* is a filtration 
since (B V M n) and (B V Moo)(Mn V Tn) are filtrations. However, by 
Theorem 7.2.10, Â£** = [(B V Moo)(Mn V Tn)]oo. By Theorem 4.3.7 (v), 
(BVMoo)(MnVTn) = ( B V M n)(Mn VTâ)n(BVM oo) = Â£ ^ f\(B y  Moo)- 
And by Lemma 2.2.5 (iii), Â£* fl (B V .Moo) = Â£Â« V {(B V Moo) H 7} where, 
as usual, Z = {M G *4 V S  : n(M )2 = H(Af)}. Hence
Â£** = ^ v {(B V Moo) n 1} = n (B V M o o ),
by another application of Lemma 2.2.5 (iii). 
â 
As in the unreduced experiment, it is evident that if at some stage, the 
conditional experiment is identified (i.e., Â£* = BW M n), then the asympÂ­
totic conditional experiment is identified (i.e., Â£** = B V Moo)* However, 
by the martingale convergence theorem, using (2.2.3), any integrable Â£**- 
measurable function is the almost sure limit of the sequence of its projections 
on Â£* which, by definition, are Â£*-measurable.

306
7. Asymptotic Experiments
In this section we first examine the exact estimability from both a 
Bayesian and an asymptotic point of view. More specifically, in a Bayesian 
framework, consistency is exact estimability in an asymptotic experiment. 
We apply the results of Section 4.7 (on exact estimability) to the particular 
structure of an asymptotic experiment. For the general case, the first known 
results probably date back to Doob (1949); further results were obtained by 
Berk (1966, 1970) and LeCam (1986), Chap. 17. The finite (or countable) 
case is treated in the following section. Some discussion of the connection 
between Bayesian and sampling theory consistency, in the general case, conÂ­
cludes this section.
7.4.1 
Exact Estimability and Bayesian Consistency
We now examine concepts of exact estimability as introduced in Section
4.7, in the case of sequential Bayesian experiments, and relate this concept 
to the more familiar concept of consistency. For expository reasons, we proÂ­
vide definitions and results for the general sequence of conditional Bayesian 
experiments 
, but we provide comments on these definitions and reÂ­
sults for the sequence of unreduced experiment Â£a v su only. In this section 
we assume that M n |  Moo and Tn j 7 ^ .
The basic definition is based on the following theorem:
7.4.1 Theorem . Let C be an 
-parameter (i.e., C C B V Moo)\ the
following two conditions are then equivalent:
(i) 
T C .Moo V Too]
(ii) 
v e e [ j C \ u ( M n v T n)t - +e a.s.n.
Proof. By the martingale convergence theorem, (MnVTn)Â£ ââº {MooVToo)Â£ 
a.s.II, and it is clear that (.Moo V Too)Â£ â Â£ a.s.n, V Â£ E [Â£]i if and only if 
C C .Moo V Too- 
â 
7.4 
Asymptotic Exact Estimability
This motivates the next definition:

7.4. 
Asymptotic Exact Estimability
307
7.4.2 D efinition. An Â£5^ ^ -parameter C satisfying any one of the two 
equivalent conditions of Theorem 7.4.1, is said to be asymptotically exactly 
estimable in Â£g^r â¢ 
â 
For unreduced experiments Â£avsu this definition becomes: B C A  is 
asymptotically exactly estimable if Snb â* b a.s.II for all b G [B] 1. Recall 
that 5n6, the posterior expectation of 6, is the usual Bayesian estimator 
corresponding to a quadratic loss function. The property âSnb ââº b a.s.IIâ 
means that this sequence of estimators converges almost surely to the paÂ­
rameter b. This property thus represents the Bayesian concept of consisÂ­
tency. Thus a parameter B is asymptotically exactly estimable if every 
^-measurable integrable function admits the conditional expectations with 
respect to Sn as a strongly consistent sequence of estimators. The Bayesian 
aspect of this analysis turns on two particularities: firstly, the estimator 
is the posterior expectation; secondly, the almost sure convergence is with 
respect to the joint probability II. The links between these two particulariÂ­
ties and the usual sampling theory concepts of consistency are examined in 
Section 7.4.2.
Using the theorems of Section 4.7, the study of exact estimability is 
usually structured as follows: It is known that the maximal exactly estiÂ­
mated parameter, (B V .M oo) H (M o o  V Too), is contained in the minimal 
sufficient parameter (# V M o o )(M o o  VTqo) (Proposition 4.7.4(i)). The minÂ­
imal sufficient parameter will be exactly estimable if and only if there exists 
a statistic Af C  M o o VTqo sufficient and exactly estimating (Af C  B V M o o ) 
(Theorem 4.7.8). In view of Theorems 7.3.4 and 7.3.17, candidates for Af 
are tail cr-fields of sequences of statistics sufficient in finite experiments. The 
verification of the exactly estimating character of such tail-cr-fields will rely 
on 0-1 laws which are presented in the sequel. Furthermore, in Chapters 8 
and 9, sufficiency is also obtained using invariance arguments.
7.4.2 
Sampling Theory and Bayesian Methods
As in Section 2.3.7, let us consider a statistical experiment 
Â£ = {( S, S) , Pa a G A}, where a represents a point of A and P a is the 
corresponding sampling probability. A is a cr-field on A such that P a(X) 
is A-measurable, 
V X  G S, and {<Sn,n G JN} is a filtration in S with

308
7. Asymptotic Experiments
Soo = Vo<n<oo Sn- This is the usual framework for the analysis of asympÂ­
totic properties in sampling theory. In this context, a real parameter 6 
is an A-measurable real function of a, and 6(a) represents the values of 6 
at the point a. We first recall the sampling theory definitions of estimaÂ­
bility (see, for example, LeCam and Schwartz (1960), Breiman, LeCam 
and Schwartz (1964), Schwartz (1965), Skibinski (1967,1969), Martin and 
Vaguelsy (1969), Deistler and Seifert (1978).
7.4.3 D efinition. A real-valued parameter 
6 is strongly (respectively, 
weakly; respectively, p-) sampling estimable if there exists an (Sn)-adapted 
sequence of estimators {sn,n E IN} such that sn â*â  6(a) a.s.Pa (respecÂ­
tively, in probability with respect to P a, respectively, in Lp(P a)), V a E A. 
The sequence {snJn Â£ 
is then said to be strongly (respectively, weakly, 
respectively, p-) consistent for 6. 
â 
In a Bayesian framework, a probability p on (A, *4) is introduced; its 
presence provides two ways of blending Bayesian and sampling theory arÂ­
guments to relax Definition 7.4.3.
7.4.4 D efinition. A real-valued parameter 6 is p-strongly (respectively, /i- 
weakly; respectively, p-p-) sampling estimable if there exists E E A  with 
p(E) = 1 such that 6 is strongly (respectively, weakly, respectively, p-) 
sampling estimable on the restricted parameter set E. 
m
Let E = { i  x S ,*4 V <S, II = /i 0  P**}, the corresponding Bayesian exÂ­
periment as defined in Section 1.2.1. In this framework, genuinely Bayesian 
definitions of estimability of a real parameter 6 are provided by the following 
definitions:
7.4.5 D efinition. A real-valued parameter 
6 is strongly (respectively, 
weakly; respectively, p-) Bayesian estimable if there exists an (5n)-adapted 
sequence of estimators {sn,n E IN} such that sn ââº 6 a.s.II (respectively, in 
probability with respect to II; respectively, in Lp(II)). 
â 
We now examine the connections between the Definitions 7.4.3, 7.4.4 
and 7.4.5. Two series of implications are straightforward:

7.4. 
Asymptotic Exact Estimability
309
(i) Each kind of estimability in Definition 7.4.3 implies the correspondÂ­
ing estimability in Definition 7.4.4 for any prior probability p on (A ,.4);
(ii) In each of Definitions 7.4.3, 7.4.4 and 7.4.5, strong estimability imÂ­
plies weak estimability, and p-estimability implies weak estimability (with 
the same adapted sequence of estimators).
Another implication is easily obtained when the parameter space A is 
countable:
(iii) If A is countable and if p gives positive probability to each point of 
A, each ^/-sampling estimability in Definition 7.4.4 implies the corresponding 
sampling estimability in Definition 7.4.3.
If A is uncountable, such an implication will not be true in general. 
Indeed, the proof that a property which is true almost everywhere is in fact 
true everywhere requires some kind of continuity (see Definition 2.3.13); 
but in an asymptotic experiment, the sampling probabilities are generally 
mutually singular (i.e., P aÂ± P a', V a /  a').
Other series of implications are less obvious and so are described by 
the next two theorems. The first of these relates various kinds of Bayesian 
estimability in Definitions 7.4.5 between them and to the concept of asympÂ­
totic exact estimability (Definition 7.4.2):
7.4.6 Theorem .
(i) Any real-valued weakly Bayesian estimable parameter b is strongly BayesÂ­
ian estimable and A  fl 5oo-measurable (i.e., is measurable with respect to 
the maximal exactly estimable parameter);
(ii) Any real-valued parameter b Â£ [AH Soo]? ( 1 < P < oo) is strongly 
and p-Bayesian estimable. An adapted sequence of consistent estimators is 
given by the posterior expectations {Snb : n Â£ IN}.
Proof, (i) It suffices to note that if b is weakly Bayesian estimable and 
{sn : n Â£ IN} is an (5n)-adapted sequence of estimators such that sn â+ b in 
probability with respect to II, then there exists a subsequence {snk : k Â£ N} 
such that snk ââº b a.s. II (see Neveu (1964), Proposition II.4.3), and so b is 
strongly Bayesian estimable and since b = lim supsnfc a.s.II, b Â£ S qq.
(ii) If b E [AD Soojp, then b Â£ [A fl 5oo]x and, by Theorem 7.2.2,

310
7. Asymptotic Experiments
Snb ââº Soob a.s.II, and in la(II). Now Soob = b a.s.II since b E Sooâ¢ But 
b E [.A]p implies that Snb ââº b in Lp(n) by Theorem 7.2.2. 
â 
Note that the assumption of p-integrability of b in Theorem 7.4.6 (ii), 
i.e., b E [A]p depends on the prior probability p but not on the sampling 
probabilities. Theorem 7.4.6 is genuinely Bayesian in the following sense: In 
a sampling theory framework the existing subsequence, sUk, would depend 
on an unknown parameter and would therefore fail to be a sequence of estiÂ­
mators, whereas in a Bayesian framework this subsequence is based on the 
unique joint probability II and therefore does not depend on the parameter.
The second theorem examines some links between each kind of estimaÂ­
bility in 7.4.4 with the corresponding kind of estimability in 7.4.5:
7.4.7 Theorem .
(i) 
A real parameter b strongly Bayesian estimable is //-strongly sampling 
estimable;
(ii) 
A real parameter b //-weakly sampling estimable is weakly Bayesian 
estimable;
(iii) A bounded p-Bayesian estimable parameter b (i.e., b E [*4]oo) *s I*~P~ 
sampling estimable.
Proof, (i) Let 
= lim infsn and s+ = lim supsn. The event {sn â> b} 
may be written as {s_ = s+ â b}. This event clearly belongs to A  V S 
and, by hypothesis, we have that II[{s+ = 
= b}] = 1. This implies that
- b} = 1 a.s.//. By definition of II = p 0  PA, and by the fact 
that P a[s+ = 
= b(a)] is *4-measurable (see, for instance, Neveu (1964),
Proposition III-2-7), we obtain: ^41{5+=a_=6} = P a[s+ â s- â Ka)] a-s-/i- 
Hence there exists E e A  with p(E) = 1 such that sn âÂ»â  6(a) a.s.Pa 
V a E E.
(ii) 
By hypothesis, V a E E where E e A  and p(E) = 1, V e > 0, we 
have:
lim P a[| sn â 6(a) |> e] = 0.

7.4. 
Asymptotic Exact Estimability
311
Now, as in (i), P a[| sn â 6(a) |> e] is ^4-measurable and bounded by 1. Since
n[|sn -  6| > e] = f P a[\sn -  6(a)| > e]p(da),
Ja
an application of the Lebesgue dominated convergence theorem (see, for 
instance, Neveu (1964), II-3) entails the result, i.e.,
lim II[|sn â 6| > c] = 0.
nâ*-oo
(iii) 
By hypothesis, there exists an adapted sequence of estimators {sn : 
n G IN} such that sn ââº 6 in Lp(II). By Theorem 7.4.6(i), 6 G Soo and 
b G [-4]p, and so, by Theorem 7.4.6(ii), Snb ââº 6 a.s.II. But if b G [>t]oo5 
then there exists a G 
such that |6| < a, and so \Snb â b\p < (2a)p. By 
the dominated convergence theorem for conditional expectations (see, for inÂ­
stance, Neveu (1964), IV-3), limn_oo A(\Snbâb\p) = 0 a.s.II or, equivalently, 
a.s.//. Since, as in (i) of this theorem, l a(\Snb â b(a)\p) is A-measurable, 
then A(\Snb â b\p) = Xa(\Snb â b(a)\p) a.s.//. So, there exists E G A  with 
p(E) = 1 such that V a G E, \imn^ 0 0 Xa(\Snb â b(a)\p) = 0, i.e., Snb âÂ»â  6(a) 
in Lp(Pa) or, equivalently, 6 is //-p-sampling estimable. 
â 
We conclude this section with some discussion of these results.
R em ark 1. From the elem entary properties and from Theorem s 7.4.6(i) 
and 7.4.7(i) and (ii), //-strong sampling, //-weak sampling estimability, strong 
Bayesian estimability and weak Bayesian estimability are equivalent conÂ­
cepts, and so we will simply say Bayesian estimability. Moreover, from 
Theorems 7.4.6(ii) and 7.4.7(iii), if 6 is a bounded parameter, then Bayesian 
estimability, p-p-sampling estimability and p-Bayesian estimability are equivÂ­
alent concepts (see Table 1).
R em ark 2. Due to the relations of hereby defined estimability concepts 
with the concept of exact estimability of cr-fields (Theorem 7.4.6), the cr- 
field generated by a family of Bayesian estimable real-valued parameters 
is exactly estimable. Thus every integrable parameter c measurable for 
this cr-field will be Bayesian estimable. Note that, in sampling theory, if 
{bi : 1 < i < k} is a family of real-valued parameters that are strongly 
(respectively, weakly) sampling estimable, then every continuous function c 
of the 6t âs will be strongly (respectively, weakly) sampling estimable. But

Table 1. Estimability : Sampling Theory and Bayesian Concepts.
312 
7. Asymptotic Experiments
Def. 7.4.3.
Def. 7.4.4.
strongly 
/*-strongly 
<t=
sampling 
(4) 
sampling Th. 7.4.7(i)
(2)(
2)
weakly 
p-weakly 
==?âº
<===
ampling 
(4) 
sampling 
Th. 7.4.7(ii)
(3) 
(3)
sampling 
p-p-sampling 
<=
Th.7.4.7(iii) 
if be[A]oo
Def. 7.4.5.
strongly
Bayesian
(2)
Exact 
estimability
bâ¬AnSa
Th. 7.4.1
Th. 7.4.6(i)
weakly
Bayesian
(3)
p-Bayesian
if be[A]p
bÂ£ [^In^oojp
Th. 7.4.6(h)
(1) V /i, by definition.
(2) because almost sure convergence im plies convergence in probability.
(3) because convergence in Lp im plies convergence in probability.
(4) if A is countable and p gives positive probabilities to each aÂ£A.
in this framework it seems difficult to extend this result to every Borel 
measurable function of the b^s. The introduction of a prior probability p 
on the parameters permits a step to be taken in that direction. Indeed, if c 
is any real-valued parameter integrable with respect to p that is a (Borel) 
measurable function of the 6tâs, then c is //-strongly sampling estimable, 
i.e., there exists E Â£ A  with p{E) = 1 such that c is strongly sampling 
estimable on the restricted parameter set E. Moreover, if c is bounded 
then it is also p-sampling estimable on this restricted parameter set for

7.5. 
Estimability of Discrete a-Fields
313
any p E [l,oo[. The adapted sequence of estimators strongly and, for any 
p E [l,oo),p-consistent for c may be chosen to be {Snc : n E JN}.
R em ark 3. The above presentation of the relationships between the three 
kinds of estimability concepts is very similar to the presentation, in Section
4.6.2, of the relationships between different concepts of identification. The 
connections established in both analysis are also very similar: the âa.s.//â 
concepts and the Bayesian concepts are equivalent and are implied by the 
sampling concepts. Note that this last implication does not require a hyÂ­
pothesis on (A}A) for estimability but requires some regularity for identifiÂ­
cation (e.g., (A,*4) should be a Souslin space).
Moreover, it is known that estimability implies identification in each of 
the three approaches: for example, the sampling case is treated in LeCam 
and Schwartz (1960), the a.s./i sampling case in Deistler and Seifert (1978) 
and the Bayesian case is Corollary 4.7.3. Let us observe that, in view 
of the equivalences of âa.s./iâ and the Bayesian concepts, Corollary 4.7.3. 
establishes that /i-sampling estimability implies a.s.s.-identification.
7.5 
Estimability of Discrete <r-Fields
As an illustration of the above concepts We now consider the exact 
estimability of a cr-field V on the parameter space when it is generated by 
a measurable countable partition, i.e., V  â o{Ed,d â¬ D C JN} (where 
Ed 7^ <t>, Ed Â£ A, UdEd = A and d ^  d! => Ed fl Ed> = <j>)\ thus, Ed are the 
atoms of V. This application is motivated, in particular, by the problems 
of discriminant analysis or of model choice when V  is generated by the 
label of the sampling model (in such cases, the partition is typically finite) 
and the sampling probabilities are characterized by both a model label and 
a parameter that may be different for each model. In this situation, a 
fundamental issue is to establish whether, asymptotically, we âknow the 
true model for sureâ, i.e., whether V  is exactly estimable.
As V  is generated by a (at most) countable partition of A, any ImÂ­
measurable function may be represented as a function of the canonical variÂ­
able: d : A ââº D C JN defined by d(a) = d 
a E Ed and to be considered 
as a model label. Hence the following notation: V E E A, 
V x E <5,
a(d) = p(Ed)

314
7. Asymptotic Experiments
a s (d) = ps (Ed)
pd(E) = p(E  n Ed)/p{Ed) = p(E  I E d) 
n  d(E x X )  =  n [(p  n  Ed) x x \/n (E d x 5 )  =  n [ j ? x i | Â£ d x S ]  
pd(x ) = n ^ A  x x)  = n d(Ed x x)
Under the identification Ed C A 
d Â£ D we may write the marginal 
model Â£vvs as follows:
Â£ w s  = (D x 5, V V 
n  = a 0  P d = P  0  as )
where P d may also be viewed as a predictive probability within the (Ph-
model. This marginal model can be characterized by a finite (or countable 
parameter space); such a structure has been investigated, e.g., by De Groot 
(1970), Freedman (1963, 1965) or Freedman and Diaconis (1983).
The main consequence of the countable character of V is that we may 
suppose that a(d) > 0  V d Â£ D, and consequently the family (Pd)d^D 
(respectively, (n d)aâ¬i>) is equivalent to the corresponding marginal P (reÂ­
spectively, II), i.e., P d(X) = 0 V d 
-P(X) = 0). It will therefore be 
natural to consider, in particular, the Radon-Nikodym derivatives dPd jd P  
and dUd/dIL. The first result gives equivalent characterizations of the exact 
estimability of V  without reference to a filtration on S.
7.5.1 T heorem . The following properties are equivalent:
(i) 
V  C S (i.e., V  is exactly estimable);
(ii) 
3 a partition of 5, (Xd), such that P d{Xd>) = 1 {d=d*} (i-e., (Pd) are
mutually singular);
(iii) 3 a partition of 5, (X^), such that dUd/dIL = [l/a(d)]lAxx d a.s.II;
(iv) 3 a partition of 5, (Xd), such that as (d) = l x d a.s.P;
(v) 
3 a family in 5, (XJ), such that P d(X*d) = P(UX*d) = 1 and
P(X*d nX*d,) = 0 , d # d ' ;
(vi) 
(dPd/dP) â  (dPd'/d P ) = 0 a.s.P, 
V d^d'-,

7.5. 
Estimability of Discrete cr-Fields
315
(vii) as(d) =  0 a.s.Pd , 
V d ^  d!.
Proof. We shall frequently appeal to the following version of Lemma 2.2.5 (iii):
(7.5.1) M i  C M 2 <=> V Mi â¬ M i  
3 M 2 e M
2 : P(M iA M 2) = 0.
(i) o  (ii)
V  C S  &  
3 (X*d)dÂ£i> 
such that 
X d â¬ S  and
n[({d} x S)A(D x X3)] = 0 V d â¬ D 
(by 7.5.1)
O  3{X*d) 
such that a(d)Pd(S -  X*d) +  
<*(<?)Pd\x*d) = 0
d'Â±d
3(jr3) 
such that 
P d(S -  X*d) = P dâ{X*d) = 0, d! Â£ d
3(X*d) 
such that 
P d(X*d,) = l {d=td,y
If (X*j) is not a partition, define an associated partition as follows:
X i 
= 
X J U ( 5 - U ^ ) .
d
Xd 
=  * S - ( J  X # , 
V d #  l
d'<d
we also have Pd{Xd>) = l{d=d'}.
(ii) &  (iii)
3 a partition 
(Xd) such that Pd{Xd>) = l{d=d'}
3 a partition 
(Xf) such that IId(A x X d>) = l{d=d'}
<=> 
3 a partition 
(Xd) such that
(7.5.2) 
nd[M n(Ax X d)] = ILd(M) 
V M e A V S
nd[M n(Ax x d*)] = o 
v d Â± dâ.
Now, because II = 
we have
(7.5.3) 
n [M D(Ax X d)] = a(d)ir*(M ),i.e.,
d&d 
1 
.  
w ,
Tn 
= T (d jlAxX* 
v d â¬ D  
asJI

316
7. Asymptotic Experiments
(iv) =* (ii)
(iv)=> a(d) = P{Xd) because
â (<0 = f s 0 s (d) dP = f s l Xi 
dP = P ( X d)
=> P d{Xd>) = 1 {d=d'} because
pd( Y \ _  n[{d} X Xd>] 
1 
f  
S/J\ -I 
Jn
P ( X a , ) â
â
 W)J, l )
=  
m
 L
l x â l x ' iP -
(ii) =>â¢ (iv)
Recall that, in general, ots (d) = a{d){d,Pd /  dP). Therefore:
(ii) =âº V X e S
P d( x )  =  p d( x n x d) 
= 4a T a ( d ! ) p d' ( x n x d)
a { a )  z
'
= 4aP(xnxd) = X r f 1 XddP 
a(d) 
<x(d)Jx
dPd 1
Â°  i p  = S (d )i x â *-8 'p '
(ii) => (v) is trivial
(v) => (ii)
(v)=Â» P d(X*d,) = 0 d ^ d f .
Indeed, 
P d(X*d,) 
= P d(X* n X*,) 
because P d(X*d) =  1
= 0 
because P{X*d nX*d,) = 0,
and note that the associated partition X\  = X{ U {5 â \JdX d},
X d =  X*d -  \ J  X*dâ V d # l ,
d'<d
has the same properties.

7.5. 
Estimability of Discrete cr-Fields
317
(ii) => (vi)
dPd 
1 
^
 =>~dP = ^ (d )lXd **'P (SCe ^
dPd dPd' 
1 
1
^  I F
' I F  
^ d ) ^ d T ) l x ^
 = 0 because (Xd) is a partition'
(vi) => (ii)
Define X d = js Â£ 5 | 
> 0 j.
Thus we obtain successively:
dPd
P d(X*d) 
= 
1 
because â
 = 0  
on S -  X i
dP
P{X*d) > 
aid).
Therefore
dPd' 
dPd
(vi) => 
= 0 on X*d because 
> 0 o n I J ,
=âº P d(X*d.) = 0.
Thus (X*j) is an almost sure partition, to which one can associate, as above,
a partition with the desired property.
(vi) 
(vii)
dPd
(vii) o  a ^~c[p~ = ^ 
d ^  d*
â  dPd' = 0  
d Â£ d '
Jx dP
â 
f  dPd dPd' 
â 
, . â
o V X â¬ i  
Jx I p  ' I F  
=  
d * d
dPd 
dPd> 
n 
n 
, . â
^  dP 
d P ~  
*  
"
It is evident that the most important result in this theorem is the equivÂ­
alence between the exact estimability of a finite-cr-field on the parameter 
space and the mutual singularity of the associated sampling probabilities 
marginalized with respect to that finite cr-field. The other characterizations 
shed more light on equivalent forms of this mutual singularity. The next 
corollary states an implication of such a context ( the converse implication 
is obviously false).

318
7. Asymptotic Experiments
7.5.2 Corollary. If V  C S ,  the family (II4) is mutually singular. 
â 
Theorem 7.5.1 is particularly useful in the asymptotic experiment. This 
is the focus of next corollary.
7.5.3 Corollary. Let us consider a filtration on <S : S n |  5oo* The followÂ­
ing conditions are equivalent:
(i) 
V  C Soo',
(ii) 
3 a partition of S ,  (Xj) such that aSn(d) ââº 1*,, a.s.II;
(iii) 3 a partition of S, (Xd) such that aSn(d) â*â¢ lx d in Li(II);
(iv) aSn(d) -* \{d=d'} a-s.Pdâ. 
m
In other words, if we interpret d as a model label, then its exact esÂ­
timability is equivalent to a 0 â 1 law on its posterior probability. Since the 
first results established by Kakutani (1948), the probability theory literaÂ­
ture has developed a wealth of results in the form of conditions sufficient to 
ensure the mutual singularity of probabilities on filtration. Extensions to 
non i.i.d. sequences of Kakutaniâs original result are presented in a unified 
framework by Kabanov, Lipster and Shiryayev (1979-1980); for a shorter 
presentation, see Shiryayev (1981) or Memin (1985). It is worth mentioning 
that Theorem 7.5.1, in conjunction with that literature, has two different 
directions: either take two particular values {ai, 02} in the parameter space, 
consider the conditional model 
> an(^ no^e 
Theorem 7.5.1 holds
for arbitrary (strictly prositive) prior probability attached to those values, 
or consider d as a model label and integrate out any other parameters (see 
Zellner and Siow (1980)).
7.6 
Mutual Conditional Independence and 
Conditional 0-1 Laws
As announced in concluding Section 7.4.1, the verification of the exactly 
estimating character of tail-cr-fields relies on 0-1 laws. A celebrated 0-1 law 
is the Kolmogorov 0-1 law (see, e.g., Chow and Teicher (1978)), which is 
deduced from the mutual independence of a sequence of cr-fields. In the

7.6. Mutual Conditional Independence and Conditional 0-1 Laws
319
next section, we define mutual conditional independence of a sequence of 
cr-fields on a general probability space and study some properties of this 
concept. The following section considers the case where the conditioning is 
operated with respect to a sequence of cr-fields.
7.6.1 
Mutual Conditional Independence
In this section (M, M ,  P) is again an abstract probability space, M i  are 
sub-cr-fields of M ,  in particular Mo = 
We consider an arbitrary
family of sub-cr-fields of M , {Ti : i E 7} where I is an arbitrary index set.
7.6.1 D efinition. The family of sub-cr-fields of M ,  {Ti : i E 1} is a family 
of mutually conditionally independent cr-fields, where the conditioning is 
with respect to a sub-cr-field M i  of M  and we write X 
Ti \ M \  if, for 
every finite subset J  of 7 and for every /* E [Ti]+, V i E 7,
M  (n i6//j)  = UitJ Mi(fi).
Before stating the next theorem, which relates mutual conditional inÂ­
dependence to the wedge operation, we first introduce some notation. For 
any subset J  of 7 we will write
(7.6.1) 
T j = \ j T i .
ieJ
We will also write, when there is no ambiguity about 7,
(7.6.2) 
= 
V ^ . .
jei
3*i
7.6.2 Theorem . Let {Ti : i E 7} be an arbitrary family of sub-cr-fields of 
M } and let M \  be a sub-cr-field of M . If X tâ¬/ Ti \ M i ,  then V {Ij : j E J} 
partition of 7 where J is an arbitrary index set, we have:
. \ T j 3 | M i.
Proof. Let K  be a finite subset of J and Lj a finite subset of I j , V j  E K . 
Then L = (JjÂ£K Lj is a finite subset of 7. If /,â¢ E [Ti]+ V i E 7, we have

320
7. Asymptotic Experiments
successively:
M i
m ,  n
j e R
n n
jÂ£K iÂ£Lj
jÂ£K 
\i
i W n * ' )
jÂ£K 
\ieLj 
J
Define hj = YiieLj /*â¢ Clearly, hj Â£ [â¢77/J-]+ and
M i  
= n  M i  (hj).
jÂ£K 
j  
jÂ£K
Finally, using a monotone class argument, it is clear that the above relation
In what follows, we often consider a countable family of sub-cr-fields of 
M ,  {Tn,n Â£ IN}. In this situation a set of equivalent definitions of mutual 
conditional independence is particularly useful.
7.6.3 
Theorem . Let {T n \ n Â£ IN} be a sequence of sub-cr-fields of Ai and 
let M i be a sub-cr-field of M . The following conditions are equivalent:
is true for any hj Â£ 
j  G K.
â 
(i) 
X X n | M u  
nelN
(ii) 
X 
F k \ M i  
VneJV;
(iii) X0" X  
j | M i  
' I nZl N-
(iv) X â X X w | X i  
V n Â£ l ;
(v) 
Xn+i X  X0" | .Mi 
V n e l .
Proof. By definition, (i) and (ii) are equivalent because any finite subset of 
IN is included in an interval [0, n] for some finite n. Now, by Theorem 7.6.2,
(i) implies (iii) and (iv). Clearly, (iii) implies (v), and (iv) also implies (v).

7.6. Mutual Conditional Independence and Conditional 0-1 Laws
321
Now, (v) implies (ii) by a recurrence argument. Indeed, let fk E [T\fc]+, 
k = 1,2,..., n and let us suppose that the following relation is true for i\
Mi n ] = m i (n /*)x n M M ) .
\l<Jb<n 
/  
/ 
^+l<fc<n
Then this reduction is true for Â£ â 1, since by hypothesis,
Mx  (n a )  = m 1 ( n /*) -Mii f t ).
\ i <k<i 
J  
\ i < k < i - i  
J
Hence 
n h  
= n 
and this is (ii), by definition. 
â 
\l<K n 
J 
K k < n
Note that the equivalence between (i) and (ii) means that full mutual 
independence (i) is equivalent to (ii), i.e., the finite mutual independence at 
time n for any n. The equivalence between (i) and (v) says that full mutual 
independence (i) is equivalent to sequential independence at time n for any 
n, i.e., the (n+  l)f/i cr-field is independent of the first n ones at any stage n.
An important corollary of Theorem 7.6.3 is the following:
7.6.4 Corollary. Let Mi,  i = 1,2,3,4, be sub-cr-fields of M .  The following 
properties are equivalent:
(i) 
M i  X M 2 X M 3 | M 4;
(ii) 
M i  X (Af 2 V Af3) | Af4 and Af 2 X Af3 | Af4. 
â 
We now extend the Kolmogorov 0-1 law to mutual conditional indepenÂ­
dence:
7.6.5 Theorem . Let Afi be a sub-<r-field of Af and let {Tn, n Â£ IN} be a 
sequence of sub-cr-fields of A f. If
neJN
then

322
7. Asymptotic Experiments
(ii) 
{M 2 V T n)T C Afi 
V M 2 C Afi.
In particular,
(iii) !Ft C M \.
Proof. The proof is a slight extension of the proof given in Neveu (1964), 
Proposition IV-4-3. By Theorem 7.6.2, (i) implies Fq X  F%+x \ M i .  Since 
M2 C M \ and {M2 V Fn)T C M2 V ^^+i, this implies by Corollary 2.2.11, 
that (A4 2 V Fq ) X {M 2 V F u) t | M 1 and so by a monotone class argument 
(Theorem 6.2.3), {M 2 V F qÂ° )  X {M 2 V F u ) t  | Afi, which again implies 
that (Af2 V F u ) t  X { M 2 V F n)r | M i  and so (Af2 V F h )t  C A4i, by 
Corollary 2.2.8. 
â 
We conclude this section by extending some results of Section 4.3, reÂ­
lated to the properties of projections of conditionally independent cr-fields, 
to arbitrary families.
7.6.6 Theorem . Let Afi C .M and {Fi : i E 1} be an arbitrary family of 
subâcr-fields of A f. If:
(i) 
Â±  Ti | M ,
*ei
then, for any J C /, we have:
(ii) 
M iT j  =  \J M i T
ieJ
(iii) 
( \ /  ? i ) M i C \ !  TiMx.
ieJ 
ieJ
Proof.
(ii) 
Since for any K  finite subset of J and any fi E [Fi]+ 
i E K,
- ^ i(n ig jr/i) = Y l i z A Mi f i )  we have: M i(U ieK  fi) e [V i<z k m i ^ ] -  
By a monotone class argument: 
V /  E [F j ]+ : M i f  E [Vi^j MiFi]
and, therefore, M i F j  C [\fiej M iF i\. 
Furthermore, it is clear that 
Af iFi C M i F j  V i E J and, therefore, \fiÂ£j M iF{ C Af iFj.

7.6. Mutual Conditional Independence and Conditional 0-1 Laws
323
(iii) 
By Corollary 4.3.6 (ii) and by induction, we have that for any finite 
K  C J: T k M \  C \J ^ k ^ iM i or> equivalently, because
V  K M i  C ? K , 
T k J- M i  | \ /  W
i .
ieK 
iÂ£K
By Theorem 7.6.2 and by hypothesis, we have T k JL F j - k I M \ which 
implies, by Corollary 2.2.11, that T k 
I M i V ( \JieK T M i ) .
Therefore, by Theorem 2.2.10, T k JL (T j - k V M i)  \ \fieK F iM i. By 
another application of Corollary 2.2.11, T k JL (Tj - k VM i) | V;6j  ^ M i ) 
and this implies that T k JL M i  \ \Ji^j T iM i\ by a monotone class arÂ­
gument, this in turn implies that T j X M i \ \ J F i M i  and, therefore, 
T j M i  C Mi ^ j Ti Mi .  
â 
Two important applications of (ii) are to the cases where J  is a bounded 
interval (i.e., J = {0,1,...,n}) or where J = IN; this second situation is 
the focus of the next two corollaries (the proof of the second is trivial).
7.6.7 Corollary. Let M i  C M  and {Tn : n E I N }  be an independent seÂ­
quence of sub-cr-fields of M  conditionally on M i .  Then:
(i) 
MiT,Â§Â° = V
n elN
(ii) 
T t fl M i C ( M i T u)t -
Proof. As (i) is a trivial consequence of the above theorem, we 
only prove (ii). 
By Theorem 7.6.6, 
M iTâ¢ = V' m>n M i T m. So
M
i T
t  c \lm>nM iTm, V rt E I N , and this implies M
i T
t  C ( M i T n)T- 
By Theorem 7.6.5, T t  C M
i  and so, by 4.3.2 (iii), M
i T
t  â T t  H M
i .  
â 
7.6.8 Corollary. Under the conditions of Corollary 7.6.7, the sequence 
(TqM i ) is (Tn) recursive; more precisely:
FS+1M i C 
Fn+iMi C F qM i V T n+i-

324
7. Asymptotic Experiments
Suppose that the M \-condition ally independent sequence of sub-cr-fields 
of M,  {bin â¢ n Â£ TN}, is such that 7in = T n V Qn. In view of Theorem
7.6.3, 
this implies that X nelN^n I M \ and X  neIN&n I M \. ConseÂ­
quently, we may apply the theorems of Section 7.6.1 to (Tn), (Qn) and 
(Tin). In this section, we seek to obtain similar results concerning the seÂ­
quence (Tin) without making assumptions concerning the sequence (Gn) 
(such as X nelN^n I M i), but with assumptions on (Tn) conditionally 
on Â£7oÂ° (recall that our notation is such that QqÂ° = Gjn and, therefore,
G]n[ =  GlN-in}'
This requires some kind of transitivity condition, as is shown by the 
following theorem.
7.6.9 
Theorem . Let M i C M  and let {Tn : n E IN} and {Qn : n E IN} 
be two sequences of sub-cr-fields of M . Then:
7.6.2 
Sifted Sequences of <r-Fields
(i) 
X  
( GnVf n)  | M i
0<n<oo
if and only if
(ii) 
JL Qn | M i ,
0<n<oo
(iii) 
X  T n \MiVQ%>,
0<n<oo
(iv) ? n JLQ%>\Mi \t gn, 
V n e w .
Moreover under (iii), (iv) is equivalent to
(v) 
Ti  JL0S0 \ M i  VSi, 
V I C l N .
Proof. By Theorem 7.6.3, (i) is equivalent to:
(vi) (Qn V T n) X (Q]n[ V T ]n[) \ M i .
By successive applications of Theorem 2.2.10, (vi) is equivalent to:

7.6. Mutual Conditional Independence and Conditional 0-1 Laws
325
(vii) Qn X  Q]n[ I M l)
(viii) Qn X T]n[ | M l  V Q]n[]
(ix) Tn Â±J=in[\ M 1VQ^]
(x) 
Tn x  Q]n[ | M l  VQn-
Clearly, by Theorem 7.6.3, (vii) is equivalent to (ii) and (ix) is equivalent 
to (iii). Since (x) is of course equivalent to (iv), and, obviously (v) implies
(viii), the proof will be completed if we can prove that, under (iii), (iv) 
implies (v). As it is obviously true for any I with one element, we prove by 
recurrence that if (v) is true for 7, then it is true for I U {n} V n Â£ I, i.e.,
(Ti V T n) X Qâ¢ \ M \ V  Qi V Gm 
which, by Theorem 2.2.10, is equivalent to:
(xi) T t Â± G ? \ M 1 v G iV G n ,
(xii) T n Â± G ? \ M 1 V g I V G n V r J-
Now, clearly, (xi) is implied by (v). However, since T n X Ti \ M \  V Â£7oÂ°is 
implied by (iii), (iii) and (iv) imply, by Theorem 2.2.10, that
Tn Â± ( T I V G ? ) \ M 1 VGn,
and this implies (xii), by Corollary 2.2.11(ii). Hence (v) is true for any I 
finite subset of JN. If I is infinite, let (In) be a monotone sequence of finite 
subsets of JN such that In T I- Since
T In X f f  \ M ! V g Ini 
Vn,
an application of Theorem 7.2.7 under the substitution (Tn,Gn> M n,'Hn) â * 
( ^ , 0 8 Â° , -Ml V Gin,Mo) gives the result, i.e., T i X Â£gÂ° | M \  V Qi. 
â 
It is obvious that, under the conditions of Theorem 7.6.9, Theorem 7.6.5 
implies that
(7.6.3) 
(7 â V ?â)T C M .

326
7. Asymptotic Experiments
As stated at the beginning of this section, if one wishes to preserve most of 
the mutual conditional independences of the sequence (J-n) without makÂ­
ing any assumption with respect to the distribution of the sequence (Qn), 
then more structure is required; making these structural requirements more 
explicit is the goal of the next theorem:
7.6.10 Theorem . Let M \ be a sub-cr-field of M  and let (Tn) and (Qn) 
be two sequences of sub-cr-fields of M
. Then the following properties are 
equivalent:
i  
(i) 
Â±
T
n  |-Mi Vas0;
ntlN
(ii) ^ 1 ^ 1  MxMQn, 
VnGW.
II (iii) 
X  
T k
\ M i V Q S ,  
V n â¬ W;
0<fc<n
(iv) T k  X G o  | M
\  VC/*, 
V n,k G lN,k <  n.
III (v) T S  X G n + i  I M i V G S ,  
Vn G W
(vi) T n+1 x GS I M l  V
Gn+l, V n G l ;
(vii) Tn+i X TS | M i  V GS+\  
V n Â£ l .
7.6.11 D efinition. Under any one of Property I, II or III of Theorem 7.6.10, 
we say that (Tn) is 
-sifted by (^n)- 
â 
Before proving Theorem 7.6.10, let us make some remarks. Note that 
Property I consists of Conditions (iii) and (iv) of Theorem 7.6.9 and, thereÂ­
fore, may be understood as a weakening of the joint conditional indepenÂ­
dence as stated in (i) in Theorem 7.6.9. Property I is written in an asympÂ­
totic form (involving Â£7oÂ°), whereas Property II is in a global form (i.e., 
involving T q V Qq for any n) and Property III is in a sequential one (i.e., 
assumptions on the probability on T n+\ V (7n+i conditionally on Tft V Qq ); 
in particular, (v) is a transitivity condition (see Chapter 6), namely, (Qq) 
is M i -transitive for (Tq V Qq). In this regard, Theorems 7.6.3 and 7.6.10 
fit into the same framework.

7.6. Mutual Conditional Independence and Conditional 0-1 Laws
327
A weaker concept than Adi-sifted sequences is the following:
7.6.12 D efinition. The sequence (Qn) is Mi-allocated to the sequence 
(*â Â») ^
(7.6.4) 
?i 
\MiVGi  
V I C f f  
â 
Here the basic idea is that, conditionally on Adi, the dependence beÂ­
tween the sequence (Fn) and the sequence (Qn) is limited to matching inÂ­
dices. In view of Theorem 7.6.9, (7.6.4) is clearly implied by Property I in 
Theorem 7.6.10, and (7.6.4) trivially implies the transitivity condition (v). 
Recalling the proof of Theorem 7.6.9, one should also note that if CondiÂ­
tion (7.6.4) holds for finite subsets I only, then it also holds for any subset 
of IN.
P ro o f of T heorem  7.6.10
(i) o  (ii)
By Theorem 7.6.3, (i) is equivalent to Fn X Fqâ 1 | M i  V Â£oÂ°, V n E IN. 
So, by Theorem 2.2.10, (i) and (ii) are equivalent to
(viii) Fn X ( F r l V S0Â°Â°) I M i  V Gn 
V n â¬ IN.
Now, by Corollary 2.2.11 (ii), (viii) implies that
Tn 
\M i  VÂ£o, 
V n < < ,
and this is equivalent to (iii), by Theorem 7.6.3. Furthermore, by a monoÂ­
tone class argument, (ii) is equivalent to (iv). So (I) => (II). To show that 
(II) => (I), it remains to show that (iii) implies (i). But, by Theorem 7.6.3, 
(iii) is equivalent to Fk X F q â 1 | M i  V Qq, V k < n. By Theorem 7.2.7, 
with the transposition {Fn,Qn, M n,'Hn) 
{Fk,FQ~l , M \  V Gq, M q), we 
obtain Fk X F q~1 | M i  V GqÂ° , V Jb, which is equivalent to (i).
(I) ^  (III)
Note that, by Theorem 2.2.10, (vi) and (vii) are equivalent to
(ix) 
Fn+i X (FÂ£ V GJS) | M i  V gn+i 
V n E IN.

328
7. Asymptotic Experiments
Clearly (viii) (equivalent to I) implies (ix), and (7.6.4) (implied by I) implies 
(v). Furthermore, (v) and (ix) imply, by induction, that
(x) 
T n+1 JL(;T0"V S0n+* )|.M 1v g n+1 
Vn,fcâ¬W .
Indeed, (x) is clearly true for k = 0. Suppose now that (x) is true for a 
given k and remark that (x) for k -f 1 is equivalent to (x) along with
T n + 1 jl gn+k+i | M i  V 
V g%+k.
By Corollary 2.2.11 (ii), this is implied by (v). The proof is concluded by
noting that, by a monotone class argument, (x) is equivalent to (viii). 
â 
We now extend the Kolmogorov 0-1 law so as to characterize the tail 
cr-field of the pair (Tn V Qn) when (Tn) is .Mi-sifted by (Â£/n)-
7.6.13 Theorem . If (Tn) is M i-sifted by (Qn), then
(7.6.5) 
(T n y g n)T c ( M t v g j T .
If, moreover, M i X  QqÂ° | 
then
(7.6.6) 
(Fn V Qn)T C M \  VQt -
And if, moreover, Qt C M i, then
(7.6.7) 
{FnVQ nh C M 7.
Proof. By (7.6.4), we have
^nÂ° 
^oÂ° I M i  V Qâ¢, 
V n Â£ IN.
But, by Theorem 7.6.3, (i) of Theorem 7.6.10 is equivalent to 
Â± ? k \ M i V g ? ,  
V k < n.
So, by Theorem 2.2.10 and Corollary 2.2.11, we have
V 5â ) I ( / o V  
\ M i V g â¢,  
V k < n.

7.7. 
Tail-Sufficient and Independent Bayesian Experiments
329
So
(Tn V gn)T Â±  (T$ V C?0Â°Â°) | M iV g â¢, 
V k < n.
Since (.Mi V 
= (.Mi V gn)r, we have, by Theorem 7.2.7 (with
M n = M  1, n n = g?),
{Tn V gn)T -JL {T* V So00) I (A<1 V gn)T , 
v 
By a monotone class argument, this is equivalent to
{Tn v gn)T Â±  (^ 0Â°Â° v gs Â°) | (M i v gn)T.
But (T n y g n)T C (^ 0Â° VÂ£Â§Â°), and this is equivalent, by Corollary 2.2.8, to 
(^nVffn)? C (M i V 0n):r- Now if M i JL <7qÂ° I 
then, by Theorem 7.2.8,
(M l V gn)T = M l V Â£7t- 
â 
Note that in this theorem the result of Theorem 7.6.5, i.e., (7.6.3 ) 
or (7.6.7), is obtained under âmilderâ assumptions, i.e., M i X  g{jÂ° | Â£/t 
and g? C  M i rather than JL o<n<oo Gn I M i, even if this last assumption 
does not imply the other two. Note also that under (i) in Theorem 7.6.10, 
which, by Corollary 2.2.11, implies JL nelN ^nV Gn) \ M \  V^qÂ°, we obtain 
by Theorem 7.6.5,
(7.6.8) 
(Tn Vgn)T C  M i vgÂ§Â°.
7.7 
Tail-Sufficient and Independent Bayesian Experiments
Two types of assumptions will be studied in this section. We first deÂ­
fine Bayesian tail-sufficiency for non-conditional and for conditional experÂ­
iments. In such experiments, asymptotically sufficient statistics are easily 
found. We next consider independent and conditional independent experiÂ­
ments, and determine asymptotically exactly estimating statistics. Finally, 
putting together these two sets of assumptions, we will deduce exact esÂ­
timability properties.
7.7.1 
Bayesian Tail-Sufficiency
Underlying the concept of Bayesian tail-sufficiency is the property that, 
the posterior expectation of any parameter when all the observations are

330
7. Asymptotic Experiments
known, does not depend on the first n observations regardless of n; in effect 
this means that the time origin is arbitrary.
It is useful to introduce the notion of successive observations:
7.7.1 D efinition. A dynamic Bayesian experiment is a sequential Bayesian 
experiment
Â£ = { A x S , A V S , U , S n t^oo}
for which there exists a sequence of statistics {Xk : k E IN} (sub-cr-fields of 
S ) such that
(7.7.1) 
Sn = X? 
= 
\ /  Xit
0<k<n
and so
(7.7.2) 
500 = ^0Â°Â° 
= 
\ /  
Xk.
0<k<oo
In general, Xk will be interpreted as the information brought by the obserÂ­
vation at time Jc, as is natural.
In this framework, Bayesian tail-sufficiency may be formalized as:
7.7.2 Theorem . In a dynamic Bayesian experiment, the following condiÂ­
tions are equivalent:
(7.7.3) 
A Â± X g Â° \ X â¢,  
V nG W ;
(7.7.4) 
lim {X%a-X*a) = 0 
a.s. 
V a e [ ^ ] + , 
V n G IN;
kââºoo
(7.7.5) 
A  JL Xâ¢ | XT. 
.
7.7.3 D efinition. A tail-sufficient Bayesian experiment is a dynamic BayesÂ­
ian experiment satisfying any one of the conditions of Theorem 7.7.2. 
â 
Before proving Theorem 7.7.2, let us provide some comment with reÂ­
gards to Definition 7.7.3. Bayesian tail-sufficiency appears as a genuinely 
asymptotic concept and may be seen either as the sufficiency of the tail 
statistic (7.7.5), or as the property that the posterior expectations, given 
an infinite trajectory, do not depend on the beginning of the trajectory
(7.7.3), and (7.7.4) translates the very same idea in terms of approximating

7.7. 
Tail-Sufficient and Independent Bayesian Experiments 
331
the asymptotic posterior expectations in terms of truncated ones (up to the 
first k observations). Note that Bayesian tail-sufficiency, which is both an 
easy-to-interpret and results-providing condition, may nevertheless be diffiÂ­
cult to verify unless further structure is introduced; thus in Chapters 8 and 
9 we show that a property such as stationarity implies the tail-sufficiency 
property.
P ro o f of T heorem  7.7.2: The equivalence of (7.7.3) and (7.7.4) is a simple 
application of Theorem 7.2.2 (i.e., the martingale convergence theorem). 
By Theorem 7.2.7, (7.7.3) clearly implies (7.7.5). Since Xt C X ff C XÂ£Â°,
(7.7.5) also implies (7.7.3), by Corollary 2.2.11 (ii). 
â 
We now extend this concept to conditional Bayesian experiments.
7.7.4 D efinition. A dynamic conditional Bayesian experiment is defined 
by a conditional experiment, 
a^onS 
a sequence of conditional
experiments, Â£ ^ 7^, for which there exist two sequences of statistics (i.e., 
of sub-cr-fields of Moo V 7^o) (3^n) and (Zn) such that:
(7.7.6) 
rn = ys 
= 
V  yk
0<k<n
and
(7.7.7) 
M n = Z q 
= 
\ J  Z k
0 <k<n
and, therefore, Tn f Too = ^oÂ° an^ M n T Moo = ^oÂ°; such an experiment
Z oo
will usually be written as 
. 
â 
We also use the following notation:
(7.7.8) 
Xk = y k V Z k 
and 
XÂ£ = M n V Tn = 
V ZÂ£.
In this framework, conditional tail-sufficiency is introduced as follows:
ZÂ°O
7.7.5 Theorem . In a dynamic conditional Bayesian experiment Â£#v;y~â 
the following conditions are equivalent:
(7.7.9) 
B X y 0Â°Â° I Â£ 0Â°Â° V JC , 
V n G l ;
(7.7.10) 
lim ^oo [(2 0Â°Â° V y*)b -  (ZSÂ° V y*)6] = 0 
a.s.
Vbe[B]+, 
V n g l ;

332
7. Asymptotic Experiments
(7.7.11) 
b  Â±y%Â° \(z?vyn)T.
Proof. The proof is essentially the same as for Theorem 7.7.2. 
Note, 
however, that the implication of (7.7.9) by (7.7.11) relies on the fact that 
(2Â§Â°vyn)rVyâ =Z0~ v y â .
ZÂ°Â°
7.7.6 D efinition. A dynamic conditional Bayesian experiment Â£^yoo ls 
tail-sufficient if any one of the conditions in Theorem 7.7.5 is satisfied. 
â 
Note that Z qÂ° C (ZqÂ° V Wi)t> be., the asymptotic conditioning cr-field
is contained in the asymptotically sufficient statistic. From the viewpoint
of exact estimability, there is no hope, in such a case, of obtaining 0-1 type
properties of the asymptotically sufficient statistic. This could be achieved
if, in (7.7.11) one could replace (2â¢ wyn)r by (Zn V3^n)T- That would mean
that Sgvx 00 is a tail-sufficient Bayesian experiment, which suggests that tail-
zÂ°Â°
sufficiency in both the conditional experiment Â£B$yoo and in the marginal 
experiment S b s / z would be equivalent to tail sufficiency in Â£svxÂ°Â° â¢ But in 
fact this equivalence requires that a condition dealing with the elimination 
of irrelevant conditioning observations be satisfied (more precisely, this conÂ­
dition requires that the sequence (Z n) be allocated to (Wi) (see Definition 
7.6.12), i.e., V I  C W: 
X Zq 3 | Zj). Recall, from Section 7.6, that (Wi)
sifted by (Zn) implies that (Zn) is allocated to (Wi), which implies that Zq 
is transitive for Xq , i.e., Zq + 1 1  
| 2 J, V n G l .  We may now state
the following theorem.
7.7.7 Theorem .
(i) 
If (Zn) is allocated to (Xi)> and Ssvy^vzÂ°Â° is tail-sufficient, then
zÂ°Â°
Â£&mzâ¢ and Â£&yyÂ°Â° are tail-sufficient.
zÂ°Â°
(ii) 
If (Zn) is ^-allocated to (Wi), and both Â£bvz%Â° and Â£&yyoo are tail- 
sufficient, then Â£gv;yooV2:~ is tail-sufficient.
Proof. It suffices to prove that
(1) 
y ? Â± z ? \ z ?
(2) 
B Â± ( z % Â° v y ? )  |z ~ v y â Â° Â°

7.7. 
Tail-Sufficient and Independent Bayesian Experiments
333
are equivalent to
(3) 
1 2 J 3 | B V 2 ~ ,
(4) 
6
(5) 
| Z 0Â° Â° v y ~ .
By Theorem 2.2.10, (2) is equivalent to (5) and B X Z qÂ° \ Z ff 
This,
along with (1), are equivalent to (#V3^Â°) JL Zff \ Z ^ f. By Theorem 2.2.10, 
this last conditional independence relation is equivalent to (3) and (4). 
â 
Note the following robustness property of tail-sufficiency:
zÂ°Â° 
zÂ°Â°
7.7.8 Proposition. If Â£$vyÂ°o is tail-sufficient then, V C C B, Scvy00
tail-sufficient.
zÂ°Â°
Let us remark that the tail-sufficiency of the experiment Â£^yoo may 
implied by assumptions that have no relationship with the usual concept 
of stationarity, i.e., the invariance with respect to the shift operator. One 
rather trivial example is the following proposition:
â¢ â¢ 
zÂ°Â°
7.7.9 Proposition. If Â£ is an Â£#vyÂ°Â°-ancillary parameter (i.e., C C B\ZZqÂ°
and C X ToÂ° I ^oÂ°)> then ^cwyÂ°Â° 1S tail-sufficient. 
â 
7.7.2 
Bayesian Independence
We use the definitions and results of Section 7.6 to present a Bayesian 
analysis of independent experiments. We show that this independence hyÂ­
pothesis provides exactly estimating statistics in asymptotic experiments.
7.7.10 
D efinition. An independent Bayesian experiment Â£a vXÂ£Â° is a dyÂ­
namic Bayesian experiment such that
(7.7.12) 
X 
Xk | A. 
â 
V 
' 
0<A;<oo 
1
In an independent Bayesian experiment, the sequence X qA  of minimal sufÂ­
ficient statistics is (Tn)-re cursive, i.e., XfiA C Xn V Xq~ 1A] this important 
property is a simple consequence of Corollary 7.6.8. From Theorem 7.6.5, 
we obtain the following proposition:

334
7. Asymptotic Experiments
7.7.11 Proposition. In an independent Bayesian experiment, Xt is exÂ­
actly estimating, i.e., Xt C A. More generally, for any B C A }
(B V Xn)T C X  
â 
7.7.12 D efinition. A conditional independent Bayesian experiment is a
zÂ°Â°
dynamic conditional experiment 
Â«> such that (y n) is 5-sifted by (Zn), 
i.e.,
(7.7.13) 
Â±  
y n 
I 
BVZSÂ°
0<n<oo
(7.7.14) 
y n Â± Z 0Â°Â° 
\ 
B V Z n.
Note the crucial role of condition (7.7.14); indeed, were it dropped there 
would be no formal reason to consider the fixed <r-field ZÂ£Â° as a âparameterâ 
rather than an âobservationâ, and (7.7.13) alone would therefore be formally 
equivalent to (7.7.12). Thus (7.7.14) may be viewed as a characterization 
of the z-process. Note also that, under (7.7.13), the allocation assumption, 
i.e., (Zn) is ^-allocated to (3^n), is equivalent to (7.7.14), by Theorem 7.6.9. 
Theorem 7.6.13 entails the following proposition.
7.7.13 Proposition. In a conditional independent Bayesian experiment 
Xt is exactly estimating. More precisely:
XT C ( B V Z n)T C 
V 2 0Â°Â°. 
â 
From Theorem 7.6.13 we see that, under additional assumptions, PropoÂ­
sition 7.7.13 may be refined. Indeed, if B X Z qÂ° | Z t , i.e., 
Sbvzâ¢ is 
tail-sufficient, then
(7.7.15) 
XT C BM Zt .
Moreover, if Z t is exactly estimating, i.e., Z t C B) then so is Aât ,  i.e.,
(7.7.16) 
XT C B.
Finally, note that without the assumption of the ^-allocation of (Z n) to
zÂ°Â°
(^n), we obtain that X j is exactly estimating only in 
i.e.,
(7.7.17)
XT C S V 2 Â« .

7.7. 
Tail-Sufficient and Independent Bayesian Experiments
335
Finally, from Theorem 7.6.9, we obtain the following proposition:
7.7.14 P roposition. The experiment Â£Bvyoov z oÂ° is independent if and
zÂ°Â°
only if Â£bvz%Â° is independent and Â£^y^> is conditionally independent. 
â 
7.7.3 
Independent Tail-Sufficient Bayesian Experiments
Exact estimability results are now obtained by assuming both indepenÂ­
dence and tail-sufficiency.
7.7.15 D efinition. An independent tail-sufficient Bayesian experiment is 
a dynamic Bayesian experiment that is both independent and tail-sufficient. â 
We immediately obtain the following theorem:
7.7.16 Theorem . In an independent tail-sufficient Bayesian experiment 
Â£a v xÂ°Â° the minimal sufficient parameter is exactly estimable (i.e.,
AXqÂ° C Xjf). More precisely, X f  = A  n Xâ¢ = X ^ A  = A X ^ .
Proof. By Theorem 7.7.2, Xt is asymptotically sufficient. By ProposiÂ­
tion 7.7.11, Xt is also exactly estimating, and, finally, by Theorem 4.7.8, 
the minimal sufficient parameter A X qÂ° is the maximal exactly estimable 
parameter A  fl X qÂ° . 
â 
We now turn to conditional experiments. In order to extend Theorem
7.7.16 to the conditional case we must introduce a new definition:
7.7.17 D efinition. A conditional independent tail-sufficient Bayesian ex-
zÂ°Â°
periment is a dynamic conditional Bayesian experiment E^yoo that is both 
conditional independent and tail-sufficient. 
â 
7.7.18 Theorem . Let us consider a dynamic Bayesian experiment Â£bvXÂ£Â°, 
where X^ = 04 V 
for any k. If:
(i) 
Â£bvz%Â° is a tail-sufficient Bayesian experiment,

336
7. Asymptotic Experiments
zÂ°Â°
(ii) 
Â£&vyoo is a conditional independent tail-sufficient Bayesian experiÂ­
ment,
â¢ â¢ â¢ 
zÂ°Â°
(iii) Â£&vyoo is identified,
then B is exactly estimable. More precisely,
(7.7.18) 
* F = f lV 3 r .
Proof. 1) By Theorem 7.6.9 (v), (ii) implies that (Z n) is S-allocated to 
(Xi)- By Theorem 7.7.7 (ii), (i) and (ii) implies that Â£t3vxgÂ° is tail-sufficient, 
i.e., B X XqÂ° | X t ; and so, since Z t  C X t , X t is an Â£Â§^xÂ°Â°-sufficient 
statistic.
2) By Proposition 7.7.13, (ii) implies that Xt C (B\/ Z u)t . But, under
(i), by (7.7.15), XT C F v Z ? , i.e., XT is S ^ x  oo -exactly estimating.
3) Hence, by Theorem 4.7.8,
^ = ( B V  ZT)X0Â°Â° = * 0Â°Â°(B V ZT) = (B V ZT) fl
4) By Proposition 4.6.6 (ii), (# V Zq0) ^ 00 = B V ZqÂ°, i.e., the idenÂ­
tification of Â£gv*oo, implies (Z? V Zt )<ToÂ°Â° = #  V Zt , the identification of 
Â£bvXÂ£Â°' Therefore by 3), X t = Z? V Z t and, consequently, Z? C XqÂ° . 
â 
Let us point out that the three assumptions of Theorem 7.7.18 may be
interpreted as three steps of model building. The first assumption concerns
relationships between the parameter B and the sequence of statistics (Z n)
only. Note that this assumption is, in particular, implied by the ancillarity
of (Zn) in SBvx{ oo . Note also that this assumption of ancillarity justifies
the fact that, without losing information, we can consider the conditional 
zÂ°Â°
experiment Â£ ^yo o . However, the assumption of ancillarity is not necessary 
for the exact estimability of B .
The second assumption concerns the specification of the distribution of 
ygÂ° conditionally to the parameter B and the statistics ZqÂ° . We intended to 
express in terms of independence and tail-sufficiency the essential assumpÂ­
tions of the usual regression model. Our mode of presentation, however, is

7.7. 
Tail-Sufficient and Independent Bayesian Experiments 
337
couched in terms of cr-fields and does not require any particular functional 
form.
Finally, the third assumption is simply a coherence assumption between 
the specification of the conditional model and the parameter of interest.
Note also that, in Theorem 7.7.18, the assumptions are asymptotic. But, 
in light of Theorem 7.6.10, Bayesian conditional independence assumptions 
are equivalent to global or sequential assumptions. If the assumption that 
Â£bvz%Â° is tail-sufficient, is provided by an ancillarity assumption (B JL Â£qÂ°), 
it is also equivalent to a global assumption of ancillarity (B I  7 0n) and to 
sequential assumptions of ancillarity (2 n 1  B | 2 J " 1, 
V n E W, with 
Z q 1 = X) (see Chapter 6, Proposition 6.5.1). However, the conditional tail- 
sufficiency remains an asymptotic assumption. The introduction of the shift 
operator in Chapter 9 permits this property to be deduced from finite sample 
ones. From Theorem 7.3.21, and the comments which follow, the asymptotic 
property of identification may also be provided by an identification property 
of a finite conditional experiment.
To conclude this section, we consider the following problem. It is ofÂ­
ten the case in applications that one must establish whether the sufficient 
parameter C of a conditional experiment, obtained through a mutual exoÂ­
geneity condition or through a Bayesian cut condition, is exactly estimable. 
In this context, assumptions of conditional independence and of conditional 
tail-sufficiency are made with respect to sampling probabilities, i.e., in the 
experiment Â£avXÂ£Â° and not in the experiment Â£cvxÂ°Â°- The following theÂ­
orem addresses this problem in the case of mutual exogeneity; in light of 
Theorem 3.4.6, the Bayesian cut case receives the same answer.
7.7.19 Theorem . Let Â£avXÂ£Â° be a dynamic Bayesian experiment, where 
X\z â 34 V Zk for any h and a parameter C C A. If:
(i) 
C and Z q are Â£jlvx*-mutually exogeneous, 
V n E IN,
zÂ°Â°
(ii) 
is a conditional independent tail-sufficient Bayesian experiÂ­
ment,
â¢  â¢ â¢ 
â¢ 
â¢ 
â¢ 
z 00
(iii) C is included in the minimal strong Â£j&y*>-sufficient parameter,

(iv) A  and Z q are measurably separated conditionally on CVZk, V k < n, 
then C is exactly estimable.
zÂ°Â°
Proof. It suffices to show that ZqÂ° is Â£cvxgÂ°-ancillary and that Â£CvyÂ°Â° 
is an identified conditional independent tail-sufficient Bayesian experiment 
and to then apply Theorem 7.7.18. In other words, using Theorem 7.6.10, 
we must show that:
338 
7. Asymptotic Experiments
(1)
C JL 20",
V n e l ,
(2)
a x  y$ | C V Z q, 
V n Â£ IN,
(3)
yn+1 x y 0" \AvzÂ§Â°, V n e i ,
(4)
yk x  z% \AVZk, 
' i k , n Â£ l N ,  k < n
(5)
â 4xy0Â°Â° |2SÂ°vy~, V n e i ,
(6)
>
3,
u
-tco\ yoo 
^0 /^O Â»
(7)
A II zs | C V Z k, 
V k, n â¬ JN, Jc < n
imply
(8)
c X 20Â°Â°,
(9) 
y n + i  Â±
y
$
 \ CV 2 Â§Â°, 
V n G l ,
(10) y k Â±z% \ c v z k, 
V M e i ,  k < n ,
(11) CXToÂ°Â° |2Â§Â° V^nÂ°Â°, 
V n e l ,
(12) CVZg0 = (CVZgÂ°)XgÂ°.
Clearly, (1) is equivalent to (8) and (5) implies (11), since C C A. Now (2), 
by Theorem 7.2.7, implies
(13) AÂ±y%Â°  |c v z ? ,

7.8. An Example
339
which clearly implies
(14) A Â± y n + 1 | c v z 0Â°Â°.
By Theorem 2.2.10, (3) and (14) imply (9). Now, (2) implies
(15) A  JL yk \CV Z q
By Theorem 5.2.10, (15), (4) and (7) imply
(16 ) y k Â± ( A v z s ) \ c v z k,
which clearly implies (10). Finally, (A V Z qÂ°) X X qÂ° \ C V Z qÂ° follows from 
(13). So (A V Z qÂ°)XqÂ° = (CWZ^)XÂ§Â°. By (6), C C (A WZ^)XÂ§Â° and this 
implies that C V Z%Â° C (CV ZgÂ°)XÂ§Â° n (C V ZgÂ°) = (C V ZgÂ°) XÂ§Â° C C V 2 0Â°Â°. 
This gives (12). 
â 
Let us remark that the allocation assumption (4) and the measurable 
separability assumption (7) are used merely to obtain (16). So, with AsÂ­
sumption (16), the result is obtained without Assumption (iv). Note that 
Assumption (16) is a sampling property, i.e., the distribution of yk condiÂ­
tionally to A  V Z q depends on C V Zk only; this is easily verified.
Note also that, even if the result is an asymptotic property, the measurÂ­
able separability property is required only in finite sample (i.e., in 
in which it is generally a mild assumption. This is not at all the case in 
the asymptotic experiment because a typical situation is that in which the 
minimal sufficient parameter of the sequence (Z n) is exactly estimable, i.e., 
is included in Z qÂ° .
7.8 An Example
In this section we illustrate most of the concepts elaborated in this 
chapter by means of a simple example.
Let us consider s = {xn : n 6 JN} Â£ S = J R ^  and a Â£ A = JR. Suppose 
that (xn | a) ~  i.N(ana, a2) where a 2 Â£ JRq is a known real positive number

340
7. Asymptotic Experiments
and {an : n E IN} is a known sequence of non zero real numbers. Note that 
{an : n E N} may be, for instance, thought of as representing the shape of 
the trend of the dynamic process {zn : n E IN}. Suppose, furthermore, that 
a ~  N(ao, (or2/no)), where ao E IR and no E M q are known numbers.
7.8.1 
Global and Sequential Analysis
Let Xq = Vo<Jk<n 
where Xk = (?{xk) and A  = <r(a). Then, the usual 
computations on the normal distribution give:
(7.8.1)
where
(7.8.2)
0 <k<n
and
npap + f3ntn 
no + Pn
(7.8.3)
where
(7.8.4)
Note also that
Hence,
Let
(7.8.9)
Note that
(7.8.10)
Tn 
= 
<r(XÂ£a)

7.8. An Example
341
since tn and XqCl are a.s. in bijection. Clearly, by (7.8.1) and (7.8.3), Tn is 
x n-sufficient, i.e.,
(7.8.11) 
A Â±  XÂ£ | Tn
and, therefore,
(*7 Q 1 o \ 
r â \ 4
\  
AT { n o a o +  P n t n  
<r2 
\
(7.8.12) 
(a \tn) ~  N  [ 
â , â
T  
*
\  
no -f Pn 
no + Pn J
Using known results for the normal distribution, and noticing that the disÂ­
tributions of a and tn are regular probabilities, an application of Theorem 
5.5.17, shows that Tn is a 1-complete statistic and that A is a 1-complete 
parameter in Savx?, i-e->
(7.8.13) 
Tn 
< !  A
and
(7.8.14) 
A 
< i  Tn.
By Proposition 5.5.3 and Theorem 5.5.4, A and Tn are therefore identified 
and minimal sufficient, i.e.,
(7.8.15) 
AXon 
= ATn =A
(7.8.16) 
X0nA  
= % n X 0n
(7.8.17) 
Tn.4 = Tn.
Note also that, from Theorem 7.6.6,
(7.8.18) 
A X o"= \ f  A X k
0 < k < n
and AXk = A as soon as a* /  0, as supposed.
Furthermore, let us remark that
(7.8.19) 
Tn+i C 7 n V Xn+\
i.e., (7^) is (<Tn)-recursive. This is an example of Corollary 7.6.8, which
states that under mutual conditional independence
(7.8.20) 
XÂ£+1A  C X Â£ A V X n+1.
Note also that, from (7.8.19), we easily obtain that
(7.8.21) 
T0n = Xq 
V n e N
(7.8.22) 
Tâ¢ =  Tn VX?+l 
V n < m.

342
7. Asymptotic Experiments
7.8.2 
Asymptotic Analysis
According to Section 7.3.2, the above sequential analysis entails the 
following results. By Proposition 7.3.4 (i), we have:
(7.8.23)
A  
oÂ°Â° | Tt
or, equivalently, using (7.8.16) and 7.2.4 (ii):
(7.8.24)
XSÂ°A C Tt = (XÂ£A)t .
By Proposition 7.3.4 (ii) and (7.3.2)
(7.8.25)
XqA c 
A) C Xq{Tt )
and, according to Theorem 7.3.6., we have equalities rather than inclusions 
in (7.8.24) and (7.8.25) if we can show that Tt is 2-complete. Recall that 
this cannot be deduced from the fact that (7^) is a sequence of 2-complete 
sufficient statistics.
In order to characterize Tt note first that, from (7.8.22),
The second step in the analysis is to use the results concerning the 
mutual conditional independence of {Xn : n Â£ IN}, obtained in Section
Note first that, using 7.3.22, repeated application of Theorem 2.2.10 
and Corollary 2.2.11 establish that the following three sets of conditional 
independences are equivalent:
(7.8.26)
and, therefore,
(7.8.27)
TnÂ°Â° 
= 
T â V C +1 
V n e l
n^lN
7.6.1.
(7.8.28) 
* 0n X Xâ¢+1 | A  
and 
X0n X  A  \ Tn
(7.8.29) 
Tâ X 
i | A  
and 
* 0" X  (A V 
t ) | Tâ
(7.8.30) 
Tn X X & ! | A  
and 
X0n X  (A V TnÂ°Â°) | Tâ
According to Theorem 7.6.5 we have
(7.8.31)
Xt C A

7.8. An Example
343
and
(7.8.32) 
(A V Xn)T = A.
Since (7.8.23) implies that
(7.8.33) 
A  X T0Â°Â° | Tt ,
we also have, by Theorem 7.2.8, that
(7.8.34) 
(-4vT n)T = A  V Tt -
In order to obtain deeper results, we have to go further into the structure 
of the example. Since j3n â> /?, where
(7.8.35) 
/?=  
Y ,  a l
0<k<oo
and from the martingale convergence Theorem 7.2.2 and equation (7.8.3), 
we obtain the following result:
(7.8.36) 
tn -+t 
a.s. and in 
Lp 
V 1 < p < oo.
Moreover,
(7.8.37) 
t 
= XÂ§Â°a 
if 
0 = oo
â (l + 
X ?  a â ^-CLq 
if P < oo.
Let
(7.8.38) 
T  
= <r(t).
Then, clearly, from (7.8.37)
(7.8.39) 
T  
= <r{XgÂ°a}
and
(7.8.40) 
T  
C 7t .
Let us show that T  is also sufficient. More generally, (7.8.1) may be 
extended to
(7.8.41) 
( . , * , )  
~
where

344
7. Asymptotic Experiments
/<7 q AC)\ 
vrn 
â 
_  
n OaO +  
P n tn
'Si+l0 
- --- 
 
^------ â¢
ttO +  Pm 
Pn
Hence, if we define
(7.8.43) 
C  = 
a - X ^ a
(7.8.44) 
Â«â = 
a - X ^ a ,
then, clearly, if Vâ¢ = cr(vâ¢),
(7.8.45) 
V " Â±  XÂ«t ,
(7.8.46) 
^
*
( 0
, - ^
)
and
(7.8.47) 
vâ¢ 
ââº vn 
a.s. and in 
Li 
as m â* oo
by the martingale convergence Theorem 7.2.2. Now, for any w E [T^_|_1]00 
with k < m, and for any /  bounded continuous function on M,
(7.8.48)
Passing through the limit by the dominated convergence theorem, this imÂ­
plies that
(7.8.49) 
l[ f(v n)w\ = l[ f(v n)]l(w).
Therefore, by a monotone class argument, if Vn = cr(vn), then
(7.8.50) 
Vn Â± X â¢+1 
V n > â1.
By the same argument, but using the reverse martingale Theorem 7.2.6,
(7.8.51) 
vn â+a â XTa 
a.s. and in 
Li, 
and we also have that
(7.8.52) 
a ( a - X Ta) X XT .
Now, if we define
(7.8.53) 
V = <r{a-XÂ§Â°a}, 
by (7.8.39), we have that
(7.8.54) 
A  C V V T
and

7.8. An Example
345
(7.8.55) 
V JL^oÂ°Â°-
Applying Corollary 2.2.11 shows that
(7.8.56) 
( W  T) JL XÂ°Â° | T,
since T  C XqÂ° , and therefore
(7.8.57) 
A  AL *0Â°Â° | T.
7.8.3 
The Case /? = oo
Since a.s. convergence implies convergence in distribution, it follows 
from (7.8.46) that
(7.8.58) 
Xâ¢+1a 
= a 
a.s. 
V n > - 1 .
Therefore,
(7.8.59) 
a 
= t 
a.s.
and
(7.8.60) 
a 
â¬ [2?] .
Hence, clearly, Sa v x â¢ is tail-sufficient and a is exactly estimable. More 
precisely, from (7.8.59), (7.8.60) and (7.8.31) we obtain
(7.8.61) 
and, therefore,
(7.8.62)
7.8.4 
The case /? < oo
As before, from (7.8.5)
(7.8.63)
Therefore
we obtain that 
(t | a) ~  N  (a, y )
< sL

346
7. Asymptotic Experiments
and this implies
(7.8.65)
Note also that
and that
(7.8.67)
This shows that Il^ v r ~  (p<
8 >P)avT â¢ Since A  1  T; /i<g>P, from Proposition
5.2.3 and Theorem 5.2.7, we see that T  and A  are measurably separated,
which means that the maximal exactly estimable parameter is the trivial 
parameter.
On the other hand, an application of Theorem 5.5.17 shows that T  is 
1-complete, i.e.,
(7.8.71) 
T <x A
and therefore, by Theorem 7.3.6, we have
Now from (7.8.70) and the fact that Xt C A, we obtain that the tail <r-field 
is a trivial statistic, i.e.,
i.e.,
(7.8.68)
A  || T.
Therefore, by Corollary 5.2.9 (iii),
(7.8.69)
A  || XSÂ°, 
i.e.
(7.8.70)
.4 fl -To00 = I ,
(7.8.72)
(7.8.73)
(7.8.74)
X ^ A  
= 
T  n Tq00 
T A  
= 
T  
A X oTO 
= 
A T  = A
and
(7.8.75)
XqA  
= 
r n n Xq = x â¢t .
(7.8.76)
x T c i n x â¢.

7.8. An Example
347
Note that this can be shown directly. Indeed, from (7.8.42), we obtain
(n o *7*T\ 
voo _ 
_  
n0 a 0 + fit â Pntn
(7.8.77) 
+i 
-  - ~
; w
^ r
and, therefore,
(7.8.78) 
XTa = a0.
Hence from (7.8.52),
(7.8.79) 
A  Â±  XT .
Since Xt C A, we obtain, by Corollary 2.2.8,
(7.8.80) 
XT C l  nXÂ£Â°.
Clearly, Â£a v xÂ°Â° is not tail-sufficient.
In fact, in this situation, the observation of the entire sequence 
{arn â¢ ft G IN} is equivalent to the observation of the single statistic f, and 
this statistic is measurably separated from the parameter a.


8
Invariant Experiments
8.1 
Introduction
In this chapter, the basic structure S â (A x S, *4 V S, II) is endowed 
with a set of transformations, and we investigate the consequences of invariÂ­
ance properties with respect to these transformations. Several related issues 
motivate an interest in invariance in statistics; some of these concerns are 
common to both the sampling theory and the Bayesian approaches, whereas 
others stem essentially from Bayesian thinking.
One motivation is the search for âimpartialâ statistical procedures, as 
suggested, e.g., in Lehmann (1959), or Arnold (1981). Thus, in a given situÂ­
ation it may seem desirable to look for procedures which behave identically 
over those parameter values for which the statistical model is invariant. 
Furthermore, in a Bayesian framework it has been argued that an invariant 
prior specification is a natural representation of a lack of prior information.
Interest in invariance very much reaches beyond such considerations; 
indeed, invariance often provides the basic tool for the reduction of statisÂ­
tical experiments because (conditional) invariance leads naturally to (conÂ­
ditional) independence properties. For instance, in an i.i.d. sampling from 
a normal process s 2 = Â£($,- â s)2 is an invariant statistic for the translation
349

350
8. Invariant Experiments
group on the sample space and cr2 is a maximal invariant parameter for 
those transformations on the parameter space and is a sufficient parameter 
for s2; this is an example of the more general result that a maximal invariant 
parameter is sufficient for a maximal invariant statistic. For inference on or2 
the converse question becomes: is it true that marginalizing the observation 
on s2 would result in no loss of information for oâ2? In other words, is it 
true that a maximal invariant statistic is sufficient for a maximal invariant 
parameter? In a sampling theory framework the answer is, in general, âyesâ 
under the heuristic proviso âin the absence of information about the samÂ­
pling expectationâ. In a Bayesian framework, the answer is formally âyesâ 
under a condition of invariance in the prior specification. Parenthetically, 
this is an interesting example of when mutual sufficiency may hold without 
having necessarily a cut. Invariance arguments will also provide criteria 
for ancillarity and exogeneity, because trivial invariant cr-fields transform 
mutual sufficiency into an ancillarity property.
From an asymptotic point of view, invariance arguments are also used 
for characterizing exactly estimable parameters; thus for instance, an imporÂ­
tant result is that in ergodic stationary processes identified parameters are 
exactly estimable where stationarity means invariance for the shift, which 
leaves the parameters unaffected; this is again an example of a more general 
result which asserts that, for those Bayesian experiments in which transforÂ­
mations on the product space A x S leave the joint probability invariant, 
the invariant functions of the parameters are exactly estimable under an 
ergodicity condition. Note that a theory which can treat transformations 
acting on both the parameters and the observations constitutes a particuÂ­
larly attractive tool for the treatment of incidental parameters.
A common structure clearly emerges from these different motivations, 
the study of which is the concern of this chapter. The next section, similarly 
to other chapters, tackles the abstract probability theory underlying these 
statistical problems. This section contains a review of the basic concepts of 
invariant sets and functions, of invariant measures viewed as a relationship 
between a set of transformations and a (sub-)cr-field, of ergodicity, mixing, 
and of the existence of invariant measures.
Let us briefly draw attention to some of the innovative aspects of this 
section. 
As throughout this monograph, emphasis is placed on the a-

8.1. Introduction
351
algebraic approach which provides a natural extension of the usual concepts 
in terms of marginal and conditional invariance; this section also presents in 
a systematic manner, the relations between joint, marginal and conditional 
invariance and their connection with (conditional) independence. The point 
properties of invariance are carefully displayed in order to obtain a (rigorÂ­
ous) basis for an operational version of those abstract concepts. It is also 
shown that almost sure invariance does require particular attention insofar 
as the almost sure invariant <r-field is, in general, different from the comÂ­
pleted invariant cr-field. Finally, let us mention that most of the literature on 
invariance assumes that the set of transformations is endowed with a group 
structure, a natural example being the exchangeable processes (invariant 
under finite permutation) where, parenthetically, partial exchangeability 
is a natural example for conditional invariance. However, the statistical 
analysis of stationary processes (invariant under the shift operator) raises 
questions about the use of F o r  of Z  as the time index set: 2Z generates a 
group of transformations but often requires the introduction of non-testable 
hypothesis (on the initial conditions at the left tail) while IN generates only 
a monoid (i.e., a semigroup with identity). For this reason, the theory first 
displays the results available in terms of a monoid before endowing the set 
of transformations with a group structure. It should be mentioned that in 
Section 8.2.6 the set of transformations is also endowed with a probabilÂ­
ity measure; an interesting consequence of this addition is that it enables 
one to exhibit the equivalence of (conditional) invariance and (conditional) 
independence, up to some regularity conditions.
The last section introduces the idea of an invariant Bayesian experiment,
i.e., invariance on the product space A x S. The specification of invariant 
prior measure is not analyzed in detail; emphasis is given to sufficiency and 
exact estimability. This chapter deals with general models and examples; 
the next chapter applies this analysis to stochastic processes (time series) 
and exchangeable processes. This chapter draws upon and considerably exÂ­
tends Florens (1978, 1982) and (1986). In particular, results on the admisÂ­
sibility of reductions through invariance, characterization of a.s. invariant 
cr-fields, and randomization of transformations have been developed after 
the publication of these papers. The first two papers, however, develop the 
use of invariance arguments for the specification of prior measures in more 
detail than supplied here (see also Mouchart (1977)).

352
8. Invariant Experiments
8.2 
Invariance, Ergodicity, and Mixing
In this section ( M ,M ,P )  is, as usual, an abstract probability space, 
Mo = 
its trivial cr-field, and M i , i G IN', are sub-cr-fields of M .
8.2.1 
Invariant Sets and Functions
Let us consider a measurable transformation <p of Af, i.e., <p : M  ââº M  
such that <p~1(M) C M .  We shall say that a measurable set A G Af is 
<p-invariant if (p~1(A) = A and that a (real-valued) measurable function 
m G [Af] is p-invariant if m = m o <p. Note that these two definitions are 
coherent in the sense that <p~l (A) = A is equivalent to 1a = 1a Â° <p since 
1 ip-^A) = 1 AÂ°<P-
8.2.1 Theorem . The collection of ^-invariant measurable sets is a cr-field 
called the (p-invariant <r-field and is denoted by Af^, i.e.,
and the set of real-valued Af ^-measurable functions is equal to the set of 
^-invariant measurable functions, i.e.,
Proof. Mip is a (j-field because all set operations commute with inverse 
image operations. Recall that by definition, 1 a = 1 a 0 (P, VA G M<p. Thus, 
by Theorem 0.2.21, m G [Af<^>] => m G [Af] and m = m o  (p. However, 
if m G [Af] and m = m o <pt then for any Borel set J5, 
G Af
and 
= (m o p)~l(B) = m â 1(5), i.e., m ~ 1(B) G A f T h u s
m G [My]. 
â 
More generally, we will consider a class $ of measurable transformations 
of M . A measurable set A G Af will be said $ -invariant if it is ^-invariant 
V p> G $ and a (real-valued) measurable function m G [Af] is $ -invariant if 
it is ^-invariant V <p G 
In this situation we have the following straightÂ­
forward corollary:
(8.2.1)
M v = {A â¬ M  : 
= A}
(8.2.2)
m 6 [.M^] O  m â¬ [AI] 
and 
m = motp.

8.2. 
Invariance, Ergodicity, and Mixing
353
8.2.2 C orollary. The collection of ^-invariant measurable sets is a ^-field 
called the 4>-invariant a-field and is denoted by M $ , i.e.,
(8.2.3) 
M i  = f )  M v ,
and the set of real-valued A4$-measurable functions is equal to the set of 
^-invariant measurable functions, i.e.,
(8.2.4) 
m e [M$\ <&m e [M] 
and 
m = m o p ) 
V 
E  $. 
â 
We extend the definition of ^-invariance to not necessarily real-valued 
functions as:
8.2.3 D efinition. Let (N,Af) be a measurable space and h : M  ââº N  with
(i) 
h is $ -invariant, if h~l (fif) C
(ii) h is maximal ^-invariant, if h~l(N) = A4$. 
â 
Let us remark that, in view of (8.2.4), this definition of invariance is 
equivalent to the definition of invariance for real-valued functions. In fact, 
we have the same characterization as (8.2.4) under a minor assumption on 
(N ,X).
8.2.4 T h eorem . Let (N,J\f) be a measurable space and h : M  â> N  with 
h~l (N) C M .  Then:
(i) 
If h o <p = A, 
V p E  $, then h is ^-invariant;
(ii) If h is ^-invariant and fif is separating, then h o p  = h, 
V <Â£> E  $.
P ro of.
(i) 
Let B E  N . Then Aâ 1^ )  = (A o <p)~\B) = ^ [ h ^ B ) ]  
V p E  $. 
Hence, A"1^ )  E  
V 5 e ^ ,  i.e., 
C M $ .

354
8. Invariant Experiments
(ii) Since ft 1 (Af) C A4$, we know that, V B G Af, p *[ft 1(B)\ = h~ 1(B),
V <p G 
Therefore, V i G M ,  V 
G $, V 5  G.Af,
= 1b [A[Â¥>(*)]] = l h-i(B)(a;) = l B [A(a:)].
Hence, V i Â£ M, V >p â¬ $, /i[y?(x)] â¬ A ^ xy But, since .A/* is separating, 
) = Â« * Â»  (see Section 0.2.1). Therefore, (hop)(x) = ft(z), V ar G Af,
V 
G $. 
â 
As a corollary, we obtain the following characterization of a maximal 
^-invariant function:
8.2.5 
Corollary. Let (N,Af) be a measurable space with Af separating, 
and let ft : M  ââº N  with ft""1 (AT) C M .
(i) 
ft is ^-invariant, if and only if ft o 
= ft, 
V <Â£> G
(ii) 
ft is maximal-$-invariant if and only if:
1. 
h o cp â ft 
V 
G 
and
2. 
V m G [Af$J, 3 / m G [A/] such that m = / m o ft.
Proof. In view of Theorem 8.2.4, it suffices to show that Ad$ C h~ 1(Af) 
if and only if (ii.2) holds. This is obviously true, since, by Theorem 0.2.11,
m G [ft-1 (A/")] if and only if 3 f m G [Af] such that m â f m o ft. 
â 
Let us remark that the first part of Corollary 8.2.5 is the extension of
(8.2.4), in Corollary 8.2.2, to non-necessarily real-valued functions; it merely 
requires the minor assumption that Af is separating.
It is natural to endow <Â£ with a certain structure. Indeed, remark that 
if A is a ^-invariant measurable set, 
then A is also pn-invariant 
V n G I
(where <pn is the nth composition of 
<p with the 
convention that 
pÂ° = i is
the identity on Af). Therefore, AA<p C AA.^ V nG W , and thus
(8.2.5) 
$ = {pn : n G IN) =Â» M *  = M<p.
Further, if A is both ^-invariant and ^-invariant, then A is also 
(pi o <p2-invariant, i.e., AA(pi D M .<^2 C AAlfil0(p2. Therefore, concerning the

8.2. 
Invariance, Ergodicity, and Mixing
355
^-invariant cr-field, there is no loss of generality in supposing that $  is 
closed under composition of measurable transformations. Consequently, in 
what follows we assume that 
has at least the structure of a monoid (i.e., a 
semigroup with a unit; for this concept, see, e.g., Cohn (1974), Chapter 3).
Finally, it often happens that $ also has a group structure. Indeed, if <Â£ 
consists of bijective and bimeasurable transformations (i.e., such that ip~l is 
also a measurable transformation on M) and if A is a ^-invariant measurable 
set, then it is also a <Â£>-1-invariant measurable set. In this context, there is 
no loss of generality in assuming that 
is closed under inversion; 
is then 
a group of measurable transformations on M. Conversely, if $ is a group of 
measurable transformations on Af, $ consists of bijective and bimeasurable 
transformations on M .
Let us remark that the characterization of ^-invariance and maximal 
^-invariance given in Theorem 8.2.4 and Corollary 8.2.5 did not require 
to 
exhibit any particular structure. When $  is a group it is possible to obtain 
another characterization of these invariance properties as point properties. 
These point properties, presented below, are in fact the usual definitions of 
^-invariance and maximal $-invariance in elementary textbooks.
8.2.2 
Invariance as Point Properties
We first define a ^-maximal character of a function h in terms of point 
properties.
8.2.6 D efinition. Let (TV, M) be a measurable space and h: M  -+ N  with 
h~l (N) C M . We say that h is <F-adapted if
(8.2.6) 
h(x') = h(x) => 3 <p E $ such that xf = ip(x) 
or, equivalently, if
(8.2.7) 
/Â»_1[{% )}] 
V * â¬ M ,
where
(8.2.8)
= W(x) : <p â¬ $}.
â 

356
8. Invariant Experiments
The next two theorems link this definition with the inclusion of AA $ in 
h~l (Af). Let us first denote by 
and Ax 
^
 the atoms of x in M<s>
and in h~l (Af), i.e.,
(8.2.9) 
A ? *  
= {xâ : l A(x') = l A(x) 
V A E M $ }
and
(8.2.10) 
A * ' ' â¢ 
= {x1 : 1AÂ« )  = l A(x) \f A â¬ h -\A f)} .
By the definition of 
and noting that \ A[ip{x)\ â 
we have:
(8.2.11) 
C 
A ^ *  
V Â£ â¬ M.
Clearly,
(8.2.12) 
/Â»-1 [{% )}] 
C 
V Â£ G Af
and if Af is separating since by (0.2.12), Ax ^
 = /i-1 
we also
have:
(8.2.13) 
/i~ 1[{/l(Â£ )}] =  A y 1(Ar), 
V Â£ G M .
8.2.7 T heorem . If AA is a Blackwell <r-field, and if Af is both separating 
and separable, then h ^-adapted implies that M<& C h~ 1(Af).
Proof. By (8.2.7), (8.2.8), and (8.2.13) we see that
A hx~XW  C A " * ,  
Vx e M.
Note that h~1 (Af) is separable since Af is separable. Therefore, 
by
Blackwell Theorem 0.2.16, AA$ C h~ 1(Af). 
â 
Note that m = m o <p, V <Â£> E 
if and only if m is constant on
V xG M . So, by (8.2.4), m E [A4$] if and only if m E [AA] and m is constant
on $*, Vx E M, i.e., 
C ra-1 [{m(x)}], V x E M. Recall that m E [AA$] 
is constant on the atoms of 
i.e., ^4;^* C ra-1 [{m(x)}], V x E M. 
Therefore, in view of (8.2.11), without appealing to Blackwell Theorem, 
m E [AA$] if and only if m E [AA] and m is constant on the atoms of AA$.
The converse of Theorem 8.2.7 is obtained when <5> is a group.
First, note that, when 3> is group, 
is the equivalence class of x for 
the equivalence relation on M  defined as
(8.2.14) 
x' ~  x 
if 3 <p E $ such that x' = <p(x),

8.2. 
Invariance, Ergodicity, and Mixing
357
and 
is then called the orbit under <Â£ of x. (When $  is merely a monoid, 
the relation given by (8.2.14) is reflexive and transitive only.)
Second, note that when $ is a group,
<P~1 ($*) =  V ^ 6 $ , V i e M
(when $ is a monoid, we only have that 
C y>-1 ($ x), V ^ â¬ $ , V i â¬  M).
Therefore, when $ is a group we have:
(8.2.15) 
G M  => 
â¬ M *
and, in view of (8.2.9), we have that
(8.2.16)
We are now ready to state the converse of Theorem 8.2.7.
8.2.8 Theorem . If <F is a group and if the orbits under $ are measurable, 
i.e., 
G M , V x Â£ Af, then A4$ C A"1 (AT) implies that A is ^-adapted.
Proof. In view of Formulae (8.2.9) and (8.2.10), Af$ C A-1 (A/*) implies 
that A x ^
 C A ,Â£/l* . Therefore, by Equations (8.2.12) and (8.2.16), 
A-1 [{A(z)}] C  
i.e., (8.2.7) holds. 
â 
As a corollary of Theorems 8.2.7 and 8.2.8, we have:
8.2.9 Corollary. Let AI be a Blackwell cr-field, $  a group of measurable 
transformations of M  such that the orbits under 4> are measurable, M  a 
separable and separating cr-field on N, and h : M  â* N  with h~l (N) C AI. 
Then the following conditions are equivalent:
(i) 
A is a maximal $-in variant (i.e. Af$ = A_1(Ar));
(ii) 
a) A is ^-invariant (i.e., A = A o <p V <Â£> Â£ $);
b) A is ^-adapted (i.e. h(x/) = h(x) => 3 <p Â£ 4> : x* = <p(x));
(iii) <&x = A 1[{A(a:)}] V x Â£ M.

358
8. Invariant Experiments
Let us remark that (ii) is the usual textbook definition of a maximal 
^-invariant function.
The existence of a maximal ^-invariant function remains an open quesÂ­
tion. However, when considering the characterization given by (ii) there is 
a natural candidate for this function, namely, the canonical map from M  to 
the quotient set of M  under the equivalence relation induced by the group 
$. More precisely, let us define
(8.2.17) 
M  
= 
M / ~$
and
(8.2.18) 
: 
$(*) = $*.
As cr-field on M, it is natural to choose the largest cr-field with respect to 
which 3> is measurable, i.e.,
(8.2.19) 
M  = { C C M : $ ~ 1( C ) â¬ M } .
This clearly implies that
$ _ 1 (A < ) C  Ad.
According to Corollary 8.2.9, the maximal <Â£-in variant character of $ 
would normally require assumptions on (M, Ad), $ and (M, Ad). However, 
as shown in the next theorem, the canonical character of $ allows us to drop 
such hypotheses. This does not mean, however, that these assumptions are 
irrelevant; indeed, the constructions of M,Ad and 
are rather artificial, 
and are often difficult to characterize in applications.
8.2.10 Theorem . Let $ be a group. Then:
(i) 
$ is ^-invariant and ^-adapted, i.e., <l-1 [{$(Â£)}] = 3^, 
V x E M ,
(ii) 
<$ is maximal ^-invariant, i.e., Ad$ = $ _1(Ad).
Moreover
(iii) 
Ad = $(Ad$).

8.2. 
Invariance, Ergodicity, and Mixing
359
Proof.
(i) x' E ^>â 1[{l>(x)}] if and only if 
= <F(z) i.e. 
<3v = $ x and,
therefore, if and only if x* Â£ $ x.
(ii) From (i) $ o 
= $, 
V 
E 
and by Theorem 8.2.4(i), we obtain 
$ - l{M) C Af*. We next show that, V I  G Ad$, 
= X. If this is
true then, by definition of Af, $(X ) E Af and, therefore, Af$ C $ â 1(Af). 
Now, clearly, X  C ^ -1 [^(X)]. However, x E 
if and only if
3 x* E X  such that $(#) = <$(Â£'), i.e., 
= <Â£*/. Now, if x' E X  and
X E Af <j>, then, by (8.2.9), A%* C X and, by (8.2.11), 
C X. Therefore 
x E X , and
(iii) We saw in (ii) that <l(Af$) C A f. Clearly $  o 
is the identity on 
M. Therefore V C E A f, C = ^ ^ ( C ) ] ;  but $ - \ C )  E Af * and therefore, 
M  C $(Af<*>) 
â 
8.2.3 
Invariance and Conditional Invariance of a-Fields
It is usual to consider situations where P[(p~1 (A)] = P(A ), 
V A E A f, 
i.e., P o <p~l = P on At, and to say in this context that <p is a measure- 
preserving transformation of (M, A f,P) or that P is (p-invariant on Af. 
For later use it will be more convenient to define (^-invariance of a sub-cr- 
field of Af in the sense of the (^-invariance of the probability restricted to a 
sub-cr-field.
8.2.11 D efinition. Afi C Af is $-invariant or <F invariates Afi, denoted 
as $ I Af i , if
(8.2.20) 
Afo(rai op) â A fo^i 
V <p E $, 
Vmi E [Afi]+ .
If we want to stress the role of the probability P in this concept, we write
$ I A f i;P . 
â 
We first remark that, by Theorem 0.2.21, this definition is equivalent to 
the usual definition restricted to Afi, i.e., P o ^ " 1 = P on Adi. It is then 
sufficient to check the definition for mi equal to the indicator functions 1 a 
with A E Afi.

360
8. Invariant Experiments
The next two propositions state elementary but important properties of 
the ^-invariance of a cr-field, viz., its stability for the inclusion and the fact 
that, for any probability P on (M, Af), $  invariates the invariant cr-field 
Af $.
8.2.12 Proposition. If $ I Afi, then, V M 2 C A fi,$  I M 2 â¢ In particuÂ­
lar, $ I (.Mi n M o ). 
â 
While $ I M \  means P o <p~x = P on M 1, V (p E $, it should be noted 
that $ I (AfiflAfo) is in fact equivalent to P o ^ " 1 < P o n  .Mi, V <p on $, 
i.e., the restriction to M i  of P o p~l is absolutely continuous with respect 
to the restriction to M i of P; in particular, $ I Mo, being equivalent to 
P o v ? '1 < P ,  
means that transforming P  by <p does not create
new null sets.
8.2.13 P roposition. For any probability P  on (M, Af): $ I Af$. As a 
consequence $ I (Af $ fl Mo)- 
â 
For later use, it is interesting to consider sub-a-fields for which the 
measurability of the transformation (p is preserved.
8.2.14 D efinition.
(i) 
M \  C M  is (p-stable if
i.e., <p is measurable when considered as <p : (M ,A fi) ââº ( M ,M  1) or, 
alternatively.
(ii) 
M i  C M  is 
stable if Afi is ^-stable V <p E $. 
â 
The main consequence of invariance is provided by the ergodic theorem, 
which will now be stated in a slightly more general context. For a proof see, 
e.g., Breiman (1968), Proposition 6.21 and Corollary 6.25.
(8.2.21)
(8.2.22)
m i  G [ M i ]  =$> m i  o tp g  [ M i ] .

8.2. 
Invariance, Ergodicity, and Mixing
361
8.2.15 P roposition. If Afi is ^-stable and if $  I M i ,  then 
V mi E [Mi]i, VV E
As usual, in the presence of a probability P, it is interesting to enlarge 
the notion of invariant sets to almost sure invariant sets; this will be useful 
when linking invariance with conditional independence.
We say that a measurable set A E M  is almost surely <p-invariant if 
<p~l(A) = A a.s. or, equivalently, if P[A/\(p~l {A)\ = 0 and a measurable 
function m E [M] is almost surely (p-invariant if m = mop a.s. Analogously 
to Theorem 8.2.1, we have:
8.2.16 P roposition. The collection of almost surely ^-invariant measurÂ­
able sets is a cr-field called the almost surely-p-invariant cr-field and is deÂ­
noted by M ^ , i.e.,
and a measurable function m E [M] is AI*-measurable if and only if it is 
almost surely ^-invariant, i.e.,
The following theorem relates the almost sure invariant measurable sets 
to the completion by null sets of the invariant cr-field:
8.2.17 Theorem . If Afi is ^-stable and {p} I (Afi fl Afo) then
(8.2.23)
0<fc<n-l
a.s. and in L\ as n ââº oo.
(8.2.24) 
M*v> = { A e M : < p ~ 1 (A) = A 
a.s.},
(8.2.25) 
m 6 [M*^\ 
m â¬ [.M ] and m = motp 
a.s.
(8.2.26)
M i  n M% = M i  n M v n M i
or, equivalently,
(8.2.27) 
for m\ E [Afi], mi = mio^> a.s.
<=> 3 m[ E [Afi], with m[ = m[ o y>, such that mi = m[ a.s..

362
8. Invariant Experiments
In particular, if {<Â£>} I Ado, then
(8.2.28)
Proof. Clearly, M \  fl Ad^ C M \  fl A f*. However, Adi D Mo C Adi fl Ad*, 
since AEAdi, P(A) = 0 and {<p} I (AdiflAdo) imply P[p~1 (A)] = 0. Hence, 
A = <p-1 (A) a.s.P. Therefore, Adi fl Ad^flAdi C AdiflAd*. If A E Adi and 
A = </?-1(A) a.s., then A = <p~n(A) a.s. Indeed, if P[A A ^-n (A)] = 0, 
then P[(p~l [AA<p~n(A)]] = 0, i.e., p~l {A) = <Â£>â (n+1)(A) a.s. 
Hence, 
if A = ^>â 1(A) a.s. and A = <Â£>- n (A) a.s., then A = ^>â (n+1)(A) a.s. 
Now, let B = fln Um>â <Â£>âm(A). By the ^-stability of Adi, B E Adi 
and, clearly, <p~1(B) = B and A â B a.s. Hence, B E Adi fl Ad^ and
A e Adi n Ad^" n Adi. 
â 
The notion of almost sure invariance may be extended to a family 
of measurable transformations. We say that a measurable set A. E Ad is 
almost surely 
invariant if it is almost surely ^-invariant V <p E 
and 
a measurable function m E [Ad] is almost surely <2>-invariant if it is almost 
surely ^-invariant V <p E 
Analogously to Corollary 8.2.2, we have:
8.2.18 Corollary. The collection of almost surely-^- invariant measurable 
sets is a cr-field denoted M.% and is called the almost surely <$-invariant cr-
and a measurable function m E [Ad] is Ad ^-measurable if and only if it is 
almost surely <Â£-in variant, i.e.,
(8.2.30) 
m E [Ad$] <=> m E [Ad] and m = m o <p a.s. 
\/<p E 
â 
Remark that $ I (Adi fl Ado) is not only equivalent to P o ip~l <C P 
on Adi, 
V <p E 
but is also equivalent to Adi fl Ado C Adi fl A d|; 
indeed, consider any A E Adi such that P(A) = P (A )2; then A = p ~ x{A) 
a.s. is equivalent to saying that P(A) = P (^ - 1(A)). We can now extend 
Theorem 8.2.17 as follows:
8.2.19 Corollary. If Adi is ^-stable and <Â£ I Adi fl Ado, then
field, 
(8.2.29)
M% = f l  M%
(8.2.31)
M i  flM J = f~) M i D M p D M
<pâ¬$

8.2. 
Invariance, Ergodicity, and Mixing
363
or, equivalently,
(8.2.32) 
for m E  [M i],m  =  m  o (p a.s. 
V 
E
V <p E  $, 3 
E  [Mi], with on^ = 
o <Â£>, 
such that m = 
a.s.
In particular,
(8.2.33) 
n M i C  M i H M Â£.
Consequently, if $ I Mo, then
(8.2.34) 
M i C M % =  f |  A V  
â 
With some additional conditions on $, Corollary 8.2.19 may be strengthÂ­
ened:
8.2.20 Theorem . If
(i) 
<3> is a countable group, M i is ^-stable and $ I (M i flM o),
or if
(ii) 
$ = {<pn : n E IN}, M i is ^-stable and {<p} I (M i fl Mo), 
then
(8.2.35) 
M i H M  J = M i H M $ fl M i
or, equivalently,
(8.2.36) 
for mi E  [Mi], mi = mi o <p a.s. 
V ip E  $
^  ]  mj E  [-Ml] with m^ = m^ o (p V<Â£> E  $,
such that mi = m[ a.s.
In particular, if $ is a countable group and <Â£ I M o  or if $ = 
: n E  IN} 
and {<Â£>} I Mo, then
(8.2.37) 
M% = M l.

364
8. Invariant Experiments
Proof.
(i) Clearly Afi fl Ad$ fl M \ C Afi H M%. However, if A E Adi fl Af$, let 
B = U(p^^p~'1(A). Then, p>~l(B) = B, V (p E $, since $ is a group and, 
since $ is countable, B E M \ , i.e., 5  E Afi fl Ad$. But A = B a.s. Hence, 
M i fl M% C Afi fl Af$ fl Afi.
(ii) By (8.2.5), Af$ = A V  Clearly, by (8.2.29), M% C M * .  Now, if 
m E [Mi] and m o p n = m a.s., then by the ^-stability of Adi, m o p n E Adi. 
Since {p} I .Mi fl Ado, this implies m o p n + 1 = m o p  a.s. Therefore, 
m =  m o /  a.s. V n E W if m E Afi flAf* and so Adi flAf* = Adi flAd$. â 
The extension to the case of $ being uncountable requires that more 
structure be placed on $; this case is treated in Section 8.2.6.
Proposition 8.2.13 may be extended to almost sure invariance:
8.2.21 P roposition. For any probability P  on (M,Ad), $ I M%. ConseÂ­
quently, if $ I Ad0, then $ I Af $. 
â 
We now extend Definition 8.2.11 to the conditional case:
8.2.22 D efinition. Let M i  and M 2 be sub-cr-fields of Af. Adi is 3>- 
invariani conditionally on M2 or 3> invariaies M i  conditionally on M2 â 
denoted as $ I Afi | Af2 â if-
(i) 
* I (Af2 n M 0 ,
(ii) 
^ _1(Af 2)(^i o p) = (Af 2^ 1) o p  
V 
E $, V mi E [Afi]+ . 
â 
In the above definition, assumption (ii) is essential. Note that condition
(i) ensures that, if m2 and 
are two versions of Af 2^ 1, ^2 0 P an(i m 2 0  
are two versions of ^ _1(Af2)(^ i op). Condition (i) is therefore introduced 
in the definition for sake of coherence with the definition of conditional exÂ­
pectation since, according to our convention, (ii) is an almost sure equality.
It is worthwhile rewriting condition (ii) of Definition 8.2.22 for two parÂ­
ticular cases. Firstly, if $ is a group and Af 2 is ^-stable, then by bijectivity 
we in fact have that, v?~1(Af2) = Af2, and so condition (ii), i.e.,
(8.2.38) 
p ~~1 (Ad2)(n^i 0 (f) = (Ad2mi) o p

8.2. 
Invariance, Ergodicity, and Mixing
365
is equivalent to the simpler condition:
(8.2.39) 
A^2(m i 0 (f) â ( ^ 2 mi) 0(P V 
E 
V mi G [M i]+ .
Secondly, if M 2 C M |, 
= A a.s. 
V A G M 2, so that (i) is
trivially satisfied, <p~1( M 2 ) = M 2 â¢ Since (A ^ ^ i) 0 <P =  Mzmx a.s., using
(2.2.3), condition (ii) is therefore equivalent to:
(8.2.40) 
^
2(^1 0 V?) = M 2mi 
V y? G $, V m i E  [M i]+ .
By taking M 2 = Mo this shows, in particular, that Definition 8.2.22 is 
coherent with Definition 8.2.11, since $ I M i  | Mo is equivalent to $ I .Mi.
It is important to remark that, if M 2 is ^-stable, then (8.2.39) is actuÂ­
ally equivalent to (8.2.38) along with
(8.2.41) M 2(m! o <p) = ^ â 1(M 2)(m1 0 (f) 
V ^ E $ ,  V m i E  [M i]+ , 
and this is equivalent to
<p~1(M i) X M 2 | <p~1 ( M 2) 
Vy>G$.
It is worth noting that it will often be necessary to introduce this furÂ­
ther assumption in order to obtain interesting consequences of conditional 
invariance in more general situations than those where either $  is a group, 
or M 2 is included in M%.
Exam ple. Let x =  (Â») ~  JVafo.E), <pa(x) = ( ^ ) ,  $  = {<fa : a e M}, 
M i  = a(y) and M 2 â <^(^)* Let us write //v(y I â¢) for the density function 
(with respect to the Lebesgue measure) of a normal distribution on the y- 
space. We may show that $ I M i  | M 2, i.e., M 2(mi o <p) = (M 2mi) o (p) 
for any function mi defined on the y-space, provided <ryz = azz. Indeed,
M 2 {mio<pa) 
= J rn1(y + a)fN(y\Hy -  
+ z,cryy -  <Tzz)dy
M 2mi 
= J m 1(y)fN (y \(iy -  y.z + z, <ryy -  azz)dy
and, therefore,
( X 2mi) o >pa 
= J mi(y)fN(y \ fiy -  y.z + z +  a,(ryy -  <rzz)dy
= 
M 2 (m 1 oipa)

366
8. Invariant Experiments
where the last equality is obtained by a change of variable y ââº y â a and 
follows from the translation invariance of the Lebesgue measure. 
â 
Note that, as shown in the above example, $ I Afi | M 2 does not, in 
general, imply either $  I (Afi V M 2) nor $ I M \.
8.2.23 Theorem .
(i) 
If $  I M i  | M 2, then V Af3 C M i ,  $  I M 3 \ M 2;
(ii) $  I M i  | M 2 <=> 
$ I (Afi V M 2) | M 2;
(iii) 
If $ I Mo, then $ I M i  | M2  <=> $ I M i  | M 2 
$ I M i  | M2;
(iv) 
If $ I M i  | M2, then $ I (Afi V Af2) H Afo-
Proof.
(i) Follows from the definition.
(ii) Take mt- E [Af,]+, i â 1,2.
V?_1(Af 2)[(mim2) op] = 
(P~l { M 2 )[mi o p â¢ m2 o p]
= m2 o p â¢ ^>~1(Af2)(m1 0 (p)
(since m2 o <p E [^â 1(Af2)])
= m2 o ^ â¢ (Af 2?ni) o
= (m2 â¢ Af2^ 1) o <Â£> 
(by Definition 8.2.22 (ii))
= Af 2(m1m2) o <p,
for Af 2(m im 2) = m2Af 2^1 
implies Af 2(m im 2) o p  â (m2Ad2mi) o 
^ by
Definition 8.2.22(i), and the result follows by Theorem 0.2.21.
(iii) If m E [Afi], then there exists mi E [Afi] such that m â mi a.s. 
Therefore, Ad2m = Af 2mi a.s. and, since $ I Afo, m o p  â mi o p a.s. 
and (Af2m) o p  â (Af 2mi) o <Â£>. 
This shows that $ I Afi | A f2 imÂ­
plies that $ I Afi | M 2 when <Â£ I Afo- However, 
I Afo implies that 
p ~ l (Mo) C Afo- 
Therefore, p~ l { M 2 ) C <p~1( M 2) C <p~~1( M 2 )- 
By
(2.2.3), p~^(Af2)(mi op) â ^ _1(Af2)(mi op) and Af2mi = Af 2^1 a.s. 
Under $ I Afo this shows that $ I Afi | Af2 if and only if $ I Afi | Af2.

8.2. 
Invariance, Ergodicity, and Mixing
367
(iv) If X â¬ -Mi V M 2 and P(A) = 0, then
P[<p~l (A)] =  M 0[1a Â°  p ] =  A ^o[y,_ 1 (A < 2 )(l> i Â°  Â¥>)] =  - M o K A ^ I j i)  Â°  Â¥>] =  0 
by (ii) and Definition 8.2.22(i), since .M2 1a = 0 a.s. 
â 
The next theorem shows that conditional invariance generates some conÂ­
ditional independence properties. More precisely, if we recall that M 2M 1 
is the smallest sub-cr field of M 2 conditionally on which M 1 and .M2 are 
independent then, when $  I M \  \ M 2, this property still holds for cr-fields 
obtained under the inverse image of <p for any <p E $.
8.2.24 Theorem . If 4> I M i  | M 2, then, V <p G
(i) 
<p~1{ M 2)[(p~1(Mi)] =  
n
and this implies that
(ii) X 
<p~l { M 2 ) I 
<p~1 ( M 2 M i ) .
Proof. Since V <p G 
V mi 6 [Afi]+,
V?~1(Af2)(mi o y?) = ( M 2mi) o <p, (p-1( M 2 )[<P~1(Mi)] = <p~1( M 2M i). 
It now suffices to recall that, by definition of the projection of cr-fields, 
<P~1( M 2)[(P~l {Mi)\ fl <p~1{ M 2) = <P~1( M 2 )[<P~1(Mi)\. 
m
The main property of conditional invariance is provided by the following 
theorem which analyses the relationships between marginal, conditional and 
joint ^-invariance.
8.2.25 Theorem . Let 
i = 1,2,3 be sub-cr-fields of M . Then the folÂ­
lowing properties are equivalent:
(i) 
$ I ( M i  V M2) | M 3
(ii) 
$ I M i  | M 3  and $ I .M2 | M 3  V M i .

368
8. Invariant Experiments
Proof, (i) and (ii) entails that $ I (Afi V Af2 V ^
3) fl Mo by Theorem 
8.2.23(iv). If rrii E [Af*]+ ,i = 1,2, then:
P~ l (Mz)[{rnirri2) op]  
=  
p ~ l {M^)[mi o p  - m2 op]
= p ~ l {Mz)[m\ o p â¢ p~ 1(M i V A't3)(nr.2 0 <Â£>)]
=  ^â1 (A f3)[777.1 o p  â¢ (A fi V A f3)7772 o ^>]
if $  I Af2 | Af3 V Afi
= 
^ _ 1(Af3)[{^i â¢ (Afi V Af3)m2} o p]
= 
Af3[777i â¢ (Afi V Af3)7772] O p
if $ I (Afi V Afs) | AI3, which is implied by $  I Afi | Af3 
(Theorem 8.2.23(ii)),
= Af 3(77717772) o p.
Therefore, (ii) implies (i) by Theorem 0.2.21. Now, by Theorem 8.2.23(i)
and (ii), <Â£ I (Afi VAf2) | Af3 implies that $ I (Afi VAfs) | A<3. Therefore,
under (i),
P~^ (Afs) [7771 O P  â¢ p ~ 1( M l  V Af 3)(t772 o <Â£>)]
= 
P>~1{Mz)[mi o P  â¢ (Af 1 V Af3)7772 O p] 
V 777,- E [Af,] + , 2 = 1 , 2
or, equivalently, V 777,- C [Mi]+,i = 1,2,3,
Af o[777i o p  â¢ m3 o p  â¢ p ~ 1( M l  V Afs) (777 2 o p)]
= 
Afo[777i o p  â¢ 7773 o P  â¢ (Afi V Af3)7772 o p].
This shows that
^ _1(Af 1 V Afs)(7772 o p )  â (Afi V Af3)7772 o p )
i.e, $  I Af2 | Afi V Af3, and so (i) implies (ii). 
â 
The next theorem shows that conditional invariance provides condiÂ­
tional independence between a.s. invariant <r-fields. This is in fact the most 
useful consequence of the concept of invariance:
8.2.26 Theorem . Let Af,-, i â 1,2, be sub-cr-fields of Af. If
(i) 
$ I Afi | Af2,
(ii) 
Af2 is ^-stable,

8.2. 
Invariance, Ergodicity, and Mixing
369
(iii) A fiH A f| X A f2 \ ^
1{ M 2) 
V ^ G $ ,
then
(iv) M i  fl Af $ X  M 2 | M 2 n M %.
Proof. Let mi G [-Mi fl Af$]+ - Then mi o <p â mi a.s. and therefore, 
under (i), <p~1( M 2 )mi = 
o <p. Under (ii) and (iii), by Corollary
2.2.3(ii), M 2m\ = ^ -1 (Af 2) ^ 1- Therefore, Af 2^1 = Af 2*711 0 <Â£>, he., 
A<2^ i  C [Af 2 H Af %] and this is equivalent to (iv) by Theorem 2.2.6. 
â 
Let us remark that, under (i) and (ii), it follows from the proof that
(iii) and (iv) are actually equivalent.
However, as remarked after introducing the definition of conditional 
invariance, when $ is a group, then Af2 ^-stable is in fact equivalent to 
<P~1( M 2) = A<2, V (p G $. Therefore, in this situation, (iii) is readily 
satisfied. This gives the following corollary:
8.2.27 Corollary. If $  is a group, and if:
(i) 
$  I Afi | Af2
(ii) 
Af 2 is ^-stable,
then
(iii) MiCiM%  X Af2 I M*r\M%.  
â 
There is another important situation where condition (iii) of Theorem
8.2.26 
is readily satisfied. This is when Afi C Af$ and 4> I Af2- More 
precisely, we have the following theorem:
8.2.28 Theorem . If Afi C Af $ and Af2 is ^-stable, then the following 
statements are equivalent:

370
8. Invariant Experiments
(i) 
$ I Ad2 I M i
(ii) 
9  I (M i VAd2)
(iii) $  I Ad2 and $  I M 1 | M 2
(iv) $  I M 2 | Ad2 H A d| and M \  X M 2 | Ad2 fl A d|.
Proof. By Proposition 8.2.21 and Theorem 8.2.23(i), $  I M \ . Therefore, 
by Theorem 8.2.25, (i) is equivalent to (ii) and (ii) is equivalent to (iii). 
Clearly, (ii) implies $  I M 2 | M 2 H M% by Theorems 8.2.23(i) and 8.2.25. 
Now, if mi E [M\]00, then mi o <p = mi a.s. and therefore, under (iii), 
<p~1( M 2 )m 1 = Ad2mi o <p. But:
M o[{M 2 m 1 - ( M 2m 1)o<p}2] = M 0 [{M 2m 1 -(p~ 1( M 2)m1}2]
â Mo[mi-M2mi]-Mo[mi-(p~1(M2)mi]
â Mo[mi-M2mi]âMo[mi-(M2mi)o(p]
since Ad2 is ^-stable, i.e., <p- 1(Ad2) C Ad2. But under (ii), we have
Mo[mi â¢ (Ad2mi) o <p] 
= 
Mo[mi o <p â¢(Ad2mi) 0 p]
= 
Ad0[mi â¢ Ad2mi].
Therefore,
Ad 2 mi 
= 
(Ad2mi)o<p, 
i.e., 
M 2 m i ^ M
2 ^ M %
and, by Theorem 2.2.6, this is equivalent to Adi JL M 2 | Ad2 fl M%. Now, 
by Corollary 2.2.3(iii) and the ^-stability of Ad2, Adi JL M 2 | Ad2 fl M% 
implies that
Adi[(Ad2 fl Ad$)(m2 o 9?)] 
= 
Adi(m2 o^)
Adi[(Ad2 DwAd$)m2] 
= 
M\rri2 
V 
E $ V m2 E [Ad2]+ .
But, by (8.2.40), $  I Ad2 | Ad2 fl M% implies that 
(Ad2 fl Ad$)(m2 o y?) 
= 
( M 2 ^ M % ) m 2 .
Therefore, we obtain Adi(m2 op) = Adim2 V 
E $  and V m2 
G [Ad2]+
and, by (8.2.40), this is equivalent to (i), since Adi C Adj. 
â 
This theorem leads to the following result analogous to Theorem 8.2.26:

8.2. 
Invariance, Ergodicity, and Mixing
371
8.2.29 Theorem . Let A<i be a sub-cr-field of M . If
(i) 
9 I M i ,
then V M 2 C M i  
^-stable
(ii) 
M i H M l  Â± M 2 I M 2nM%.
Proof. By Theorem 8.2.23 (i), $  I M i  implies that
$ i{ A d 2 v ( M n A ^ ) } .
By Theorem 8.2.28, this implies (ii). 
â 
The next theorem gives a condition under which the conditional cr-field 
in the conditioning ^-invariance may be reduced.
8.2.30 Theorem . Let Mi> i = 1,2,3 be sub-cr-fields of M . Then the folÂ­
lowing statements are equivalent:
(i)
1.
M i  X  M 2 | M z
2.
$  I M. 1 | M. 2 V M z
(ii)
1.
<p~l{Mi) X  i p - \ M 2) | V - 'iM z )  
V y? â¬ $
2.
$  I M \  | M z  
___
3.
$  I ( M 2 V M z ) n Mq.
Proof. Clearly, (i) 2 implies (ii) 3 by definition. Under (i) 2, we have that
<p-\M 2 V M 3 )(m i o <p) = ( M 2 V M z ) m i  o ip, 
V mi G [ M i]+ .
But, under (i) 1, by Theorem 2.2.1(ii), ( M 2 V M s)m i = M $mi. Therefore, 
under (i),
{<P~l ( M 2) V (p~l (M ^)}(m i otp) = (M $ m i) o p.
This is clearly equivalent to the two following identities:
W ~ x(M2) V <p~1(M 3)}(m 1 o<p) 
=  
<p~1(M 3 )(m 1 o <p)
<p~1( M 3 ) ( m i  o <p) 
-  
(M 3 m i ) o p ,

372
8. Invariant Experiments
i.e., (ii) 1 and (ii) 2. However, (ii) 1 with (p = i is equivalent to (i) 1 and 
p ~1 (Ad2 V Ad3)(mi o p) = (A^3^i) Â° <Â£>> 
under (ii) 1 and (ii) 2. Under (i) 1 and (ii) 3,
(Ad2 V M z)m i o p  â M ^ m i o <Â£>.
Therefore,
V?_1(Ad2 V Ad3)(mi Â° <p) = (Ad2 V Ad3)mi o p 
and this, along with (ii) 3, is equivalent to (i) 2. 
â 
8.2.4 
Ergodicity and Mixing
We now extend the concept of ergodic transformations slightly.
8.2.31 D efinition. $  is ergodic on M i conditionally on M 2 if
(i) 
$ I (M i  V M 2 ) fl Mo
(ii) 
M i ^ M % c J T 2
If M 2 is trivial, we simply say that $ is ergodic on Adi- 
â 
Note that when M 2 is trivial the ergodicity of $ means that the 
^-invariant measurable sets have probability 0 or 1. The following lemma 
shows that when $  is ergodic on Adi conditionally on Ad 2 the a.s.^-invariant 
Adi-measurable functions are almost surely equal to a.s.$-invariant Ad2- 
measurable functions.
8.2.32 Lem m a. 
If 4> is ergodic on Adi conditionally on Ad2, then
M i D M %  C M 2 nM%.
Proof. If mi G [Adi PI Ad|], 
= mi o p a.s. V p G 
By definition
8.2.31(ii), there exists m2 G [Ad2] such that mi = m2 a.s. However, since

8.2. 
Invariance, Ergodicity, and Mixing
373
<3> I (.Mi V Ad2) fl .Mo, we have mi o ^ = m2 o ^ a.s. Hence, m2 = m2 o <p 
a.s. and mi = m2 a.s., i.e., mi E [Ad2 fl Ad$]. 
â 
A stronger but often more easily checked condition for ergodicity is that 
of mixing, which we now define precisely:
8.2.33 D efinition. $  is mixing on M i  conditionally on M 2 if
(i) 
$ I (Ali V M 2) fl Mo
(ii) 
V mi, m'j â¬ [Aii]oo
â  mi o tpn] ââº M .im \ â  M ^ m i 
a.s. n â* oo.
If Ad2 is trivial, we simply say that 4> is mixing on M i- 
â 
When Ad2 is trivial, the mixing property of $ may be equivalently stated 
on the indicator functions of Afi, as follows:
V A, B E Adi, V <p E 
P[A n <p~n(B)] -+ P{A)P(B) 
a.s. n ââº 00.
8.2.34 Theorem . If $ is mixing on Adi conditionally on M 2 , then <3> is 
ergodic on Adi conditionally on Ad2.
Proof. If mi E [Afi PlAdJ], then mi = mi o <pn a.s. V n E W. Therefore 
condition (ii) of mixing implies that
Ad2(m,1mi) = Af2(m i)Af2(mi) V m[ E [Adi]+ 
V mx E [Adi n AdÂ£]+.
This is equivalent to Adi -JL Afi fl A f| |A d 2, which by Corollary 2.2.8, 
implies that Adi fl Af $ C Ad2. 
â 
8.2.5 
Existence of Invariant Measure
In Section 8.2.3, we defined the ^-invariance of Afi (i.e. $ I Adi) for a 
given probability space (M, A f,P). One may also look for the existence of 
a probability measure P on a given measurable space (M, Ad), which would 
give $  I Adi. In general, if $ has only one element or, equivalently, has the

374
8. Invariant Experiments
form {<pn : n E W}, there may be a very large number of invariant probaÂ­
bility measures. But once we add new elements to $, the set of ^-invariant 
measures decreases and eventually will possibly be empty. Heuristic ally, 
the theory of Haar measures shows that even if $ becomes a âlarge enoughâ 
group, one may show both that there still exists such a ^-invariant measure 
and that it may be characterized.
Unfortunately, the theory of Haar measures requires several mathematÂ­
ical preliminaries which have not been necessary up to now. In this section 
we briefly sketch the main results, to be used in the sequel, and refer to 
Halmos (1950) Chapter X to XII, or Nachbin (1965) for a more systematic 
exposition.
When $  is a group, it is natural to associate to $  two groups of transÂ­
formations:
$ L  
=  
=  <P 0 <p' I 
â¬  $ }
= 
{ fR if')  = <P' Â° V I <P G $}â¢
These two groups clearly coincide if $  is a commutative group. 
In
order to define Haar measures in such a case, 
we need to assume that $
is a locally compact topological group endowed with its Borel cr-field T . 
Remember that a Radon measure on a topological space is a measure on 
the Borel sets which gives finite measure on the compact sets. In this case, 
we define:
8.2.35 D efinition. A left (respectively, right) Haar measure Q on (^ ,^ 7) 
is a nonidentically zero Radon measure such that 
I T  (respectively,
I T ) y i.e., such that V (p E 
V A E T , Q(po A) = Q(A) (respectively, 
Q(Aoip) -  Q(A)). 
â¢
The Haar Theorem is proved in Halmos (1950) (Section 58, Theorem B 
and Section 60, Theorem C) and in Nachbin (1965) (Theorems 1 and 4 in 
Chapter 2).
8.2.36 Proposition. For every locally compact topological group $,
(i) 
there exists a left (respectively, right) Haar measure which is unique 
up to a strictly positive factor of proportionality;

8.2. 
Invariance, Ergodicity, and Mixing
375
(ii) 
this invariant measure is finite if and only if $  is compact;
(iii) if $ is compact there is a unique probability which is both a right and 
left Haar measure. 
â 
The particular case of $  being compact is especially interesting. Indeed, 
if M i  C M  is such that the map ip(x) : ($ x M , T  <g) M i )  ââº (M, M i )  
is measurable, i.e., V A Â£ .Mi, {(<P)X) 
: ^(^) G A} G ^  Â® M i ,  then 
any probability measure P such that $  I Afi; P is uniquely and entirely 
determined by its trace on M i  flM%, i.e., Pm i HM%, and by its conditional 
probabilities to M i ,  i.e. P j^ 1. This may be seen as follows: if $ I M i; P, 
then for any mi Â£ [Mi]+ and m'i Â£ [Mi H M $]+,
j  mi[<p(x)]m[(x)P(dx) = [  mi(x)m[(x)P(dx).
Jm 
Jm
Therefore, by Fubini Theorem, if Q is the Haar probability on ($,F)
/ mi(x)m[(x)P(dx) = / rrT[{x)m,l {x)P{dx)
Jm 
J m
where
rar(z) = / rni[<p(x)]Q(d<p).
J#
Clearly, mj" Â£ [Mi fl M$] and, therefore, (.Mi fl M%)mi â mf, 
V mi Â£ [*Mi]+. Note that the computation of m~[(x) does not involve P.
Therefore, in the decomposition of P on M  as
P = PMlnM iÂ®PÂ£knM'*Â®P%1,
is unique and PmiHM% and Pj^
1 are arbitrary when <3> I M i; P.
In particular, if <3> is a finite group, then for any M i  C M  ^-stable 
and for any probability measure P on (M ,M ) such that <Â£ I M i \ P ,
{Mi H M%)mi -  mT, V mi Â£ [M i]+
where
and |$| is the cardinality of $.
' 1 ipe$

376
8. Invariant Experiments
8.2.6 
Randomization of the Set of Transformations
Let (M, Ad,P) be an abstract probability space and $  a monoid of 
measurable transformations on M. Let us take T  a cr-field on $  compatible 
with the product of composition on $ and the action of $ on Ad, i.e., 
o 
: ($ x 3>, T(&T) ââº (3>, T) defined as 
0(9?, p') = p o p 1 is measurable,
i.e., {(<p, p') : p Â° p' G B] G T  Â® T  VP G P
* 
: ($ x M , P  Â® Ad) ââº (M , Ad) defined as 
*(<Â£>, x) = ^>(x) 
is measurÂ­
able, i.e., {(<p, x) : <p(x) G ^4} G P  Â® Ad 
VA G Ad.
Let us note that, with these properties, if we define for each p G 3>,
= <p' o y>
: $ ââº $ 
: 
<Pl W )  = p o p '  
then 
and pl are measurable, i.e.,
C f  and 
v l \ F )  C T.
Similarly, if for each x G M  we define
x : $  
M : x(^>) = <Â£>(x)
then x is measurable, i.e., x- 1(Ad) C T .
It is often necessary to make another assumption concerning the action 
of ($,JF) on (M,Ad):
8.2.37 D efinition. The action of (^ ,^ r) on (M, Ad) is progressively meaÂ­
surable if V Adi C Ad ^-stable, the map:
* 
: ($ x M, P  Â® Adi) ââº (M, Adi) is measurable, i.e.,
{(<p, x) : <p(x) G ^4} G P  0  Adi 
V ^4 G Adi. 
â 
Note that this assumption is not too restrictive since we already have 
that, V mi G [Adi], the function mi[^(x)] : $ x M  ââº M considered as a 
function of p, i.e., mi ox, is ^-measurable, and considered as a function of x, 
i.e., mi o^>, is Ad i-measurable by the ^-stability of Adi. Thus this definition 
amounts to saying that a function measurable in each of its components is 
jointly measurable.
Now let Q be a probability measure on ($ ,^ ). By the presence of 
this structure we may hope, with some hypotheses on Q, to characterize

8.2. 
Invariance, Ergodicity, and Mixing
377
as in Theorem 8.2.20, the almost surely ^-invariant cr-field by relaxing the 
hypothesis of count ability of $. This will be the next theorem which slightly 
generalizes a similar theorem in Lehmann (1959) (Theorem 4, Chapter 6).
8.2.38 
T heorem . Let ( M ,M ,P )  a probability space and $  a set of meaÂ­
surable transformations on M .  If:
(i) 
$  is a group,
(ii) T  is a cr-field on 4> compatible with the structure such that the action
of $  on M  is progressively measurable,
(iii) Q is a probability on ($ ,P ) such that 
I F q\Q, i.e.,
Q Â° Vr 1 < Q ,
(iv) M i  is ^-stable and $ I M i  CiM q]P, i.e.,
P o (p~l < P  on M i  
E $,
then
(v) 
M i C\M% = M i  fl M$ fl M i,  i.e.,
for mi E [A4i], mi = mi o <p a.s.P V 
E $ if and only if
3 m'i E [Mi Pi -M$], i.e, 
V 9? E 3>, such that mx = mi
a.s.P.
Proof.
(i) By (8.2.33), we already know that M i  fl M$ fl M i  C M i  fl M$.
(ii) Let mi E [Mi fl A4|]oo- Let us define
rn1(x )=  /  m 1 [ip(x)]Q(d<p).
By (ii), (iv) and Fubini Theorem, mi E [Mi]. Let us successively define
N  
= 
{(<p,x):mi{p(x)}Â£m 1(x)},
N v 
=  
{ar : m i [ ^ ( i ) ]  /  m i ( x ) } ,
Mx 
=

378
8. Invariant Experiments
Then by (ii) and (iv) N  G T  Â® M \ ,  N^ G M i ,  and Mx G P . Since, by 
Fubini Theorem, P(N^  = 0 
V <p G
(Q Â® P ) ( N ) =  [  Q(Mx)P(dx) = [  P(N<p)Q(d<p) = 0
and Q{MX) is M \ -measurable. Therefore, if A = {x : Q(MX) > 0}, ^4 G A4i
and 
P(^4) = 0. 
Moreover, 
V x G A c, 
rrii[p(x)] = mi(#) a.s.Q and,
therefore, mi(x) = m\(x) , i.e., mi = m\ a.s .P. Let us similarly define
=  
{(?,*)'â  m ib(*)] #  â¢iW>.
7^, 
= 
{x : mi[v>(x)] ^  mi(x)},
M x 
= 
{y>: mi[v?(x)] 7^ m ^x)},
]4 
= 
{ x : Q ( M x) >  0}.
Now, if mi = m iljc , i.e. rn'^x) = mi(ar) for x G A and 0 for x G A, then 
clearly m[ G [M\] since, as before, A G M \ .  We now show that mi = m[ 
a.s.P and that m[o<p = m[ V <p G 
Since mio<p = mi a.s.P and mi = mi 
a.s.P, mi o p = mi a.s.P, i.e., P(A ^) = 0 V 
G 
As before, by Fubini 
Theorem, P{A) = 0 and, therefore, m[ = mi a.s.P and m[ = mi a.s.P. 
Now, if x G A*, then Q(MX) = 0 and by (iii), Q[<P~r (M x)\ = 0 V <p G 
but
<Pr1 (Mx) 
= 
W  â  mi[p'[ip{x)}} Â£  mj(x)}.
Therefore,
m i[(P,[(P(x)]] = 
mi(x) a.s.Q in 
<p*.
This implies that
nii[(p(x)] = 
mi(x) 
and
mi[^/[^>(:c)]] 
= 
a.s.Q in ip', i.e.,
Q(M<p(x)) = 
0.
Therefore, V z G 
V <p G
mi [<Â£>(#)] = 
mi(x) 
and <^(a?) G A .
However, by (i) if p(x) G 
then x = 
Therefore if x G A,
then ^>(z) G A V <Â£> G 
This shows that m[[p(x)] = m[(x) V x G M, 
V 
G 
â 

8.2. 
Invariance, Ergodicity, and Mixing
379
Taking A4i = M  entails the following corollary:
8.2.39 Corollary. If:
(i) 
$  is a group,
(ii) 
T  is a cr-field on $  compatible with the structure,
(iii) 
Q a probability on (^ ,^ ) such that <&r I To,Q
then
(v)
This probability structure on $, may be used to characterize ^-invariance 
as almost surely equivalent to independence on the product probability 
space. More precisely, under the assumption of compatibility of T  with 
the action of $  on A4, the map <p ââº M o(m o <p) = f M m[<p(x)]P(dx) is 
^-measurable,for any m Â£ [Af]. Under this weaker assumption alone, Q 
and P induces a unique probability measure R on ($ x M, T  <g> M )  defined 
by extending
where Xr denotes the expectation with respect to R.
However, the stronger assumption of the compatibility of P  with 
the action of $  on M  permits the use of Fubini Theorem, since m[(p(x)\ 
is T  0  Af-measurable. 
Therefore, under this stronger assumption, 
V r Â£ [TÂ® Af]+
(8.2.42)
More generally, V /  Â£ 
Â£ [A'f]+ ,
(8.2.43)

380
8. Invariant Experiments
Let us note that (8.2.44) may also be written as
(8.2.45) 
l R ( r ) = l Ro(roh),
where Ro = Q 0  P and h : ($ x M , T  0  M )  ââº ($ x M, T  0  A4) is defined 
as h(<p,x) = (y>,<Â£>(x)) and is measurable since V B G ^  and A â¬ M> 
h~l (B x A) = (5 x M) fl {(<p, x) : <p(x) G >1} â¬ ^  <S> A4.
If we identify, as we usually do, sub-cr-fields of T  (respectively, M )  with 
the corresponding sub-<r-fields on $  x M, and measurable functions on $ 
(respectively, M) with measurable functions on $ x M , then 
= T \ / M
and we may notice that f  o h = f  V /  G [^] and that m o h = m o <p 
V m G [A4]. With this identification, Definition (8.2.42) is in fact equivalent 
to saying that the conditional probability on T  with respect to R admits 
P o p ~l as a regular version. So, V m G [A4]+ ,
(8.2.46) 
= M o(m o <p) 
a.s.#.
This reinterpretation entails the following proposition:
8.2.40 Proposition. Let M \  be a sub-<r-field of M . Then
(i) 
P Â± M u R  
if and only if
(ii) 
Mo(mi o <p) = Mo(mi) a.s.Q 
V m i G  [Mi]+. 
m
If 4> is countable and Q gives positive probability on any <p G 
or if $ 
is a topological space such that V m  G [A4]oo, M o ( m  o ^) is a continuous 
function of ^  and Q gives positive probability to any open subset of <p, we 
have already seen that, in such situations, Q is regular for $  in the following 
sense:
8.2.41 D efinition. Q is regular for $  if m G [A4]oo and M o(m o <p) = 0 
a.s. Q implies M o(m o ^>) = 0 V 
G 
â 
Such a regularity property of Q entails the interpretation of conditional 
^-invariance as a conditional independence property, as is shown in the next 
theorem:
8.2.42 Theorem . If 4> I ( M 2 D M
q) \ P  and if Q is regular for 4>, then

8.3. 
Invariant Experiments
381
(i) 
$ \ M i  \ M 2\P
if and only if
(ii) 
T  X  M \  | M 2 \R.
Proof. Using condition (iii) of Theorem 2.2.1 on the equivalent characterÂ­
izations of conditional independence we have that:
M i  \ M r 9R
if and only if
(8.2.47) 
F ( m im 2) = Jr(77i2A42^i) 
V mi Â£ [Mi]+,i = 1,2.
By (8.2.46), this is equivalent to
(8.2.48) wM0[mi o cp â¢ m2 o (p] = Mo[m 2 o <p â¢ ( M 2m 1) 0 
a.s.Q.
By the regularity of Q for <Â£, (8.2.48) is then true V <p Â£ 4>, and is therefore 
equivalent to
(8.2.49)v?â 1(A/(2)(mi o <p) = (Â«M2mi) o p a.s .P, VyÂ»Â£ 
V mi Â£ [A4i]+ , 
which is condition (ii) of Definition 8.2.22 of conditional invariance. 
â 
Thus (conditional) invariance may be interpreted as a (conditional) inÂ­
dependence property between the transformations <pâ$ and the sets in Afi, 
once $ has been endowed with a regular probability Q on a suitable cr-field 
T . As this underlying probability is arbitrary (but regular), this equivalence 
shows that both properties are structurally similar and that in fact condiÂ­
tional invariance and conditional independence will share similar properties 
as, e.g., Theorem 2.2.10 and Theorem 8.2.25.
8.3 
Invariant Experiments
8.3.1 Construction and Definition of an Invariant Bayesian ExperiÂ­
ment
The structure of an invariant statistical experiment is usually presented 
as follows. In the statistical experiment S defined as
(8.3.1)
â¬ = {(S,S);Pa : a e A }

382
8. Invariant Experiments
there exists a set of measurable transformations G; this means that each 
g Â£ G is a function g : S ââº S such that ^â 1(*S) C S. The usual invariance 
condition is the following: V g Â£ G, Va Â£ A, 3 a unique a' Â£ A such that
(8.3.2) 
P 'fo - ^ X )]  = P a\ x )  
VX Â£ S .
In other words, the image under g of any sampling probability P a, i.e., 
P a o </-1 , is another sampling probability P a>. Thus, to each g Â£ G is 
associated a transformation g : A -+ A defined by g(a) = a'. Therefore, if 
G = {<7 : g Â£ G}, it is shown that if G is a monoid (resp. a group), G is a 
monoid (respectively, a group) and (8.3.2) may be rewritten as
(8.3.3) 
x a{tog) = l 1 {a)(t), ' i a e A , ^ g Â£ G , ' i t e [ S ] +.
Let us define <p : A x S ââº 4  x S by
(8.3.4) 
<p(a,s) = (g(a),g(s)).
Under the hypothesis that the cr-field A  on A is such that V g Â£ G , ' g : A - + A  
is a measurable transformation, i.e., ^"â1(w4) C A, then 3> = {<p : g Â£ G} is 
a set of measurable transformations on A x 5, i.e., ^>_1(*4 Â® *S) C ^4 0  S 
and $ is a monoid (respectively, a group) if G is a monoid (respectively, a 
group). Now, if p is a probability measure on (^4,^4), then recall that P a is 
by construction a regular version of the restriction to S of the conditional 
probability II on A.
So, if we identify, as before, sub-cr-fields of A  (respectively, S ) with sub- 
cr-fields of the corresponding cylinders on A x 5, and measurable functions 
on A (respectively, S ) with measurable functions on A x 5, then with this 
identification (8.3.3) may be rewritten in the Bayesian Experiment
(8.3.5) 
S 
= 
(A x S M V 5 , n)
as
(8.3.6) 
^4(to^) 
= 
(At)ocp 
V < e[S ]+ .
Note that, under (8.3.4),
(8.3.7) 
V*-1 M ) 
C 
A
and

8.3. 
Invariant Experiments
383
(8.3.8)
Now, if the prior probability /i on (A, A) is such that
(8.3.9)
g 1 
<  
/i V (/gG
or, equivalently, A G A, and 11(A) = 0 implies that II 
(A)] = 0, so <3> 
invariates the null sets of A. Therefore, under (8.3.2), (8.3.9) and mesurabil- 
ity conditions on the transformations in G and G, we see that, with (8.3.4), 
the invariance in sampling theory framework is translated in the Bayesian 
experiment by: 
is a set of measurable transformations on A x S such
that A  and S  are ^-stable, $  I S \ A  and, moreover, A  X  <p~1(S) | <p~1(A) 
V <p G 
More generally, we will define:
8.3.1 D efinition. Let Z = (A x S', A  V S, II) be an unreduced Bayesian 
experiment and $ a monoid of measurable transformations on A x  S. Then 
Z is 4>-invariant (respectively, sampling <$-invariant ) (respectively, 
posterior ^-invariant) if A  and S  are ^-stable and $ I (A V S) (respecÂ­
tively, <3> I <S | A) (respectively, $ I A  \ S). 
â 
These definitions may be extended quite naturally to general reduced 
Bayesian experiment Z ^ r  where B C A, T  C S  and M  C A  V S:
8.3.2 D efinition. Let $  be a monoid of measurable transformations 
on A x 5. Then Z$$r  is 
invariant (respectively, sampling 
invariant) 
(respectively, posterior $ -invariant) if M ,  B V M  and M V T  are ^-stable 
and $  I (B V T) | M  (respectively, $ I T  | B V M )  (respectively, 
Â§ I B \ M \ / T ) .  
â 
Theorem 8.2.25 allows us to link ^-invariance and conditional ^-invariÂ­
ance.

384
8. Invariant Experiments
8.3.3 P roposition. The following statements are equivalent:
(i) 
invariant;
(ii) 
is sampling $ -invariant and $ I B \ M  (i.e., prior $ -invariant);
(iii) 
is posterior $ -invariant and $ I T  | M  (i.e., predictively $-
invariant). 
â 
The next proposition shows that, when S is ^-invariant, any reduction 
on stable sub-cr-fields remains ^-invariant; the proof is a simple consequence 
of Theorems 8.2.23(i) and 8.2.25:
8.3.4 P roposition. If Â£ is ^-invariant, if B C A, T  C  S and M  C A  V S 
are such that A4, B V M  and A4VT are $-stable, then Â£$$^  is <3>-invariant. â 
In the unreduced Bayesian experiment Â£, the most usual use of the 
Proposition 8.3.3 runs as follows: the sampling model suggests a set $ of 
measurable transformations such that Â£ is sampling 4>-invariant. Let us supÂ­
pose that there exists a probability on (A, A) such that $ I A  and that this 
probability is chosen as the prior probability. Then Â£ will be 4>-in variant. 
From this it can be deduced that $ 1 5 , i.e. the predictive probability is inÂ­
variant under $ and $ I A  | 5, i.e. the posterior probabilities are invariant 
under $.
This procedure raises a major problem related to the choice of an invariÂ­
ant prior probability. If A  is almost surely invariant, i.e., if a is unchanged 
under <p, Vv? Â£ $, then this condition may sometimes be trivially satisfied 
for any prior probability. In terms of the introduction to this section, p 
is obtained by g) this means that the sampling probabilities are invariant 
under <7, i.e., "g is the identity on A } V# E G. Some examples, involving 
stationary and exchangeable processes, are presented in the next chapter. 
The opposite situation occurs when $ is sufficiently rich. 
In this case, 
the condition $ I A may characterize a unique invariant measure (up to a 
multiplicative function of p). To preserve the invariance structure and, in 
particular, to obtain an invariant estimate (i.e., a posterior invariance) it is 
natural to choose this measure as a prior one. Unfortunately in general this

8.3. 
Invariant Experiments
385
measure is not a probability, and some care is needed in using it; we omit 
the study of this situation in this book. As indicated in Chapter 1, recall 
that a prior measure may be used as soon as it induces a cr-finite predictive 
measure. In this case, the posterior probabilities are well defined. AddiÂ­
tional problems ermerge when decomposing a measure into marginals and 
conditionals, for the latter exercise makes sense only when the marginal is 
cr-finite. Consequently, extensions of the results obtained in this book to 
the case of an unbounded prior measure are rather delicate. The existence 
of a bounded invariant prior measure is generally asserted in compact paÂ­
rameter spaces, but this situation is not particularly standard. One can 
imagine intermediate situations in which the choice of the prior probability 
is free with respect to some of the parameters, typically on the invariant 
sub-cr-field, and is determined on the remaining parameters conditionally on 
this cr-field. The same difficulties arise due to the fact that this conditional 
prior is often unbounded.
We now turn to some examples.
Example 1.
Let s â (aji,a?2, . . . , a?n)j x i = ( V i ^ i Y  â¬ 
a â¬ [0,27t[= A with
angle a. 
Clearly, if ga{xi) = Raxi> then <7a(a) = a -f a. 
Therefore, 
<pa(a, s) = (a -f a , {RaX{ \ 1 < i < n}) where a + a is understood as modulo 
27r, i.e., in M/2 tt2Z. Let p be the uniform distribution on [0, 27t[, i.e., with a 
density with respect to Lebesgue measure equal to ^ l[o ,27r[(Â«)- Obviously, 
if 
= {(pa : a G [0, 27t[}, then $ is a group and $ I A  and $ I S | A. Hence, 
$ I 5, $ I A  | S, and $ I (A V S). Note that, by direct computation, the 
joint density is given by
7T (a,s) 
= 
(2 x )-ilt i e -5 s i<i<Â»{(yi-coso)2+ (^ -sina)2}
_ 
(271-)â ^  e-h(y/y+z'z+n)entcos(u- a)
where
Hence the predictive density is given by
p(s) = (27t) %IQ(nt)e h(y'y+z'z+n).

386
8. Invariant Experiments
Hence <3> I S  since R'aR a is the identity matrix. The posterior density is
fi(a | s) = (2tt)-5 /0(n<)-1e"âcos(a- u\
where Io(x) is the incomplete Bessel function (see Watson (1983), appendix A, 
and Abramowitz and Stegun (1984)). Therefore $ I A  | 5, since (a + a) â 
(w + a) = a â u. 
â 
Example 2.
Let s = ( $ ! ,...,sn), Si e m, a e IR with (s, | a) ~  i.N(a, 1). Let 
ga(si) 
= 
Si -f a, 
hence 
<7a (a) 
= 
a + a 
and 
therefore
<pa(a,s) 
= (a + a ,s i + 
+ Â»)â¢ 
Hence $ I S  
| 
A  with
<$ = {<pa : a G M}^ provided that the prior measure is equivalent to the 
Lebesgue measure on 1R; here, because of the obtained conditional invariÂ­
ance, the translation group appears as a ânaturalâ group of transformations 
operating on both the sample and the parameter space. But in this situaÂ­
tion, the Haar measure is the Lebesgue measure on JR. If this is chosen as 
a prior measure, then the joint density is given by
?r(a,s) = (27r)_ 7 e- 5 Ei<i<-.(si- a)2.
It is evident that this is invariant for 4>, i.e., $  I {A V S). The predictive 
measure is cr-finite and its density is given by
p(s) = (27r)_ V  n - i e - ^ 1^ ^ 5*-5)2,
where s = 
Clearly, p(s) is invariant under <Â£. The posterior
probabilities are given by
p(a | s) = (27r)â ^n^eâ ^ aâ 5)2,
which are obviously conditionally invariant under $. This situation illusÂ­
trates that Proposition 8.3.3 may hold even with improper prior measure.
Example 3. M ultivariate Regression
Let s = {xi,x2, 
a:* = (3/i, *,â¢)', yt e Mp, z,- G Mk, a = (B ',E )
where B is a k x p-matrix and S a  p x  p-symmetric positive definite matrix 
with
(yi \ a , z ^ ) ^ i . N p(B'zi,'L).

8.3. 
Invariant Experiments
387
Let gM,N(%i) = (Myi + Nzi,Zi)' where M  is a p x p-nonsingular matrix, 
and N  a, p x ^-matrix, and $  the set of such transformations. It follows 
that "gM N(a) = (M B' + JV, M Â£ Af'). Let the prior density with respect to
_ , L 
__ 
  P(P+1)
the Lebesgue measure on 
x Cp, where 
C M 
2 
corresponds to
the cone of symmetric positive definite p x p-matrices, be given by
Then,
$ I Pi | a V Z i.
The joint density is given by
I *i) =  |S |" i:Â±Â£^2Â±i(27r)-^ x e x p i - i t r p - ^ y - Z S H y - Z S ) ] } ,
where Y' = (2/1,..., yn) and 
= (zi, ..., zn). The predictive measure is 
cr-finite and its density with respect to the Lebesgue measure is given by
p(y? \z?) =  K  I y 'M ^ y r ^ lZ 'Z l- S
where 
= I â Z (Z IZ)~ 1Z I and K  is an appropriate constant. Now, since 
9m,n(Y) Z>) â (Y M 1 -f Z N \ Z )  and the Jacobian of this transformation is 
\M\n the predictive measure will be invariant if v â k. The posterior 
probability is characterized by
(B |Â£ ,Y ,Z ) 
-  
M N (B , (Z'Z ) " 1 <g> Â£), 
where B = (ZfZ)~l Z 'Y  and
(E | Y,Z) 
~  
I W ( Y fM z Y,n).
The density of the Inverted Wish art with respect to the trace of the Lebesgue 
measure on Cp is given by
li(X \Y ,Z ) 
= 
| 2 ^ ^ n r ( i ( n  +  i - ; ) )
Iy' M z y|^|E|â iÂ±^Â±i exp âltrE _1(y' Mz y).
The density of the matrix normal distribution of B with respect to the 
Lebesgue measure is given by
p(B I Y,,Y,Z) = [(2ir)p
k
\Z'Z\~p]~^ exp âI  
(B â B)'Z'Z(B â B).

388
8. Invariant Experiments
It can be verified that 4> I A  | S. (For more details on the computation of 
the posterior and the predictive probabilities, see Tiao and Zellner (1964)
Example 4. Pitm an Parametrization.
In the spirit of Pitman (1939), we can consider a statistical experiment 
defined as follows. Let (5,5, Q) be a probability space and A a group acting 
on S by the operation â : A x S â> S, i.e., if e is the identity of the group 
A, e*s = s Vs E S and ai*(<i2*s) = (aia2)*s V a i,a 2 G A, Vs E S. Let 
A  be a cr-field on A compatible with the structure of the group and with 
the action of the group on S', i.e., aia ^1 : A x A ââº A is measurable and 
a * s : A x S 
S is measurable. For a E A, let us define ga : S ââº S by 
ga(s) = a â s. Then ga is a measurable transformation on S, ge â h the 
identity on S, and gai o ga2 = gaia2 so that g~1 = ga- 1. For a e Awe define 
the sampling probabilities by P a = Q o g~l . By Fubini Theorem, V I  E<S, 
P a( X ) : A ââº R is measurable and, for any t E [Â«S] + , T a{t) = /5 f(a*s)Q(ds). 
For a E A let us define 
: A ââº A by </a (a) = aa and p a : A x  S 
A x  S 
by <Â£>Â«(a, s) = (</a (a), <7c*(s)) = (aa, a*s). Then, clearly, ~ge is the identity on 
A  9Cn Â°9a 2 = 
V a i ,a 2 E i  and so, V a E A, g' 1 = ga- 1. Similarly,
<pe is the identity on A x  S, p aiÂ°Pa2 = 
<*2 V 0:1,02 E A and 
= Pa-1'
If G = {#a : o E ^4}, G = {#a : o E 4} and $ = {^a : o E 4}, then G and 
$  are groups of transformations isomorphic to A, whereas the map o ââº ga 
from A to G is only a homomorphism, since gai = ga2 
o 1 = 02. Let 
/i be a probability measure on (A, A) and let II be the joint probability on 
(A x S ,A  V 5). Then, for any m E [A V Â«S]+ ,
and Ando and Kaufmann (1965).)
Therefore, for any t E [S]+ ,
A(t o p a) 
= 
1  t(aa~k a)Q(da) 
and

8.3. 
Invariant Experiments
389
Since V b Â£ [.4], 6 o <pa = b(aa) and Vt Â£ [5], t o (pa = t(a * 5). Obviously, 
.4 and S  are ^-stable. Now, 
I (A fl X) if and only if p is such that 
pog- 1 < / i V a G i o r , equivalently, if poJ~x ~  p Va Â£ A since G is a group. 
Therefore, for such a prior, $ I S | A. If there exists a prior probability p 
on {A, A) such that po~g~l = p Va Â£ A, then $  I A  Therefore $ I (.4VÂ«S), 
$ 1 5  and $ I A  | S. This may be checked directly since, Vm Â£ [.4 V <S]+ ,
Therefore, if //s is a regular version of the posterior probability then,
This example covers the cases where the parameter is either a location 
parameter or a scale parameter. Note that these results may be thought 
of as conditional on Q. If Q is also considered as a parameter then for 
any prior on Q, such as a Dirichlet process (see the next example), we will 
have that <Â£ I (Q, a, s) when defining 
a, s) = (Q, aa, c**s). Therefore
we will also have that $  I a | Â£, i.e., after the integration of the nuisance 
parameter Q. 
m
Example 5. Nonparametric Bayesian Experiment.
Let s = (a?i,..., xn) Â£ [0, l]n = S', a Â£ A, where A is the set of probaÂ­
bility measures on [0,1] with its Borel cr-field. We suppose that 7^
| a 
and P(xi Â£ B | a) = a(B). Let g : [0,1] ââº [0,1] strictly increasing continÂ­
uous with </(0) = 0 and #(1) = 1, and let G be the set of such functions. 
If g(s) = [g(x 1),... ,g(xn), then 7j(a) â a o g~l . Therefore, let us define 
(p(a,s) = (y(a), #(s)); if $ = {<p : g Â£ G}, then A  and S  are ^-stable 
and A{t o <p) = [At) o <p a.s. for any prior p on A. If we take as a prior 
a ~  Di(noao), i.e., the Dirichlet Process (see, for instance, Ferguson (1973, 
1974) or Rolin (1983)) where no Â£]0,oo[ and ao is a probability measure 
on [0, 1] â recall that, by definition, this means that for every measurÂ­
able partition ( B 1} R 2 , ..., B *,) of [0,1], the vector (a(F?i), a(f?2), â¢ â¢ â¢, a(7?fc))
J (m ).
V6 6 [.4]+, S(b o <pa) = (Sb) o <pa, i.e.,
f  b(aa')fiâ(da1) = f  6(a > "* * (da1)
Ja 
Ja

390
8. Invariant Experiments
is distributed as a fc-dimensional Dirichlet distribution with parameters 
(noao(5i), 71000(^ 2)
,
noao(^fc)) ââ¢ It has been shown that (a | s) ~  
Di(n*a+) where n* = n0 -f n and a* = (do/d*)d0 H- (n /n * ) F ( s), where 
F(s) is the empirical distribution of s , i.e., F(s)(B ) = ^Y2i<i<n 
In the class of Dirichlet priors, <Â£-in variant prior probabilities are charÂ­
acterized by ao = peo -f (1 â p)s 1 where p G [0,1], and ex is the Dirac 
measure at point x since, clearly, do 0 9 ~ 1 = o>o and if a ~  Di(nodo), then 
a o g~l ~  Di(nodo o g~l ). Hence $ I d. Moreover, if this is true, then 
$ I (d,s) and $ I s | a. This last conditional invariance can be verified 
directly, since d* o g~l â (do/d*)do + (n/ n*)F[9 (s)\â¢ The coherence of 
this example is troubled by the following problem: if a ~  Di(noao) with 
a0 = pÂ£0-f(lâp)ei, then a = 6Â£0-f(l â b)ei a.s. where b ~  B(n 0p, n0( l âp)), 
i.e. b has a beta distribution. Therefore, in the joint probability, this implies 
that almost surely Xi is equal to 0 or 1, which may contradict the observaÂ­
tions. More precisely, conditionally on d, x,- = 0 or 1 with probability b and 
1 â 6. This implies that a?,- = 0 or 1 with probability p, 1 â p. Another verÂ­
sion of the posterior distribution, i.e., conditionally on s, is therefore given 
by d = 6*Â£o -f (1 â 6*)Â£i a.s. where 6* ~  
n*( 1 â t)] and
do 
d / \ 
, v 
1 
^
t = â pH---- u(s) 
where u(s) = â 
> 
l / r =0).
d* 
d* 
n jrr*
1 <* <n
8.3.2 
Invariance and Reduction
As seen in Section 8.2.3, invariance properties imply conditional indeÂ­
pendence relations. These can be used in invariant Bayesian experiments 
to obtain admissible reductions. We first introduce the following notation. 
For M  C A  V S
(8.3.10) 
M $  = ( A V S ) * n M
is the ^-invariant sub-cr-field of M  and
(8.3.11) 
M l  = (,4 V*S)*nA4
is the almost surely ^-invariant sub-d-field of M .
In particular, we define:
8.3.5 D efinition. Let S = {A x 5, A  V <S, n} be an unreduced Bayesian 
experiment and <Â£ a monoid of measurable transformations on A x S such

8.3. 
Invariant Experiments
391
that A  is ^-stable and S  is ^-stable. Then
(8.3.12)
(8.3.13)
a ;
(8.3.14)
S 4,
(8.3.15)
s ;
is the maximal invariant parameter,
is the almost surely maximal invariant parameter,
is the maximal invariant statistic, and
is the almost surely maximal invariant statistic.
It is often useful to make the following assumptions on
8.3.6 D efinition. $ is regular for Z if
(8.3.16) 
$ 1 .4  H i 
and 
A$ = A<f>OA
(8.3.17) 
$ 1  I D S  
and S^^S^DS. 
â¢
This means that the almost surely maximal invariant parameter (respecÂ­
tively, statistic) is the maximal invariant parameter (respectively, statistic) 
completed by the null sets of A (respectively, S) i.e., the trivial parameter 
(respectively, statistic); we have already seen, in Sections 8.2.3 and 8.2.6, 
several situations in which $  is regular for Z.
We now turn to the main result of this section, the proof of which follows 
directly from Theorem 8.2.29:
8.3.7 Proposition. If Z is ^-invariant, the almost surely maximal invariÂ­
ant parameter and statistic are mutually sufficient, i.e.,
(8.3.18) 
A% 
JL 
5  
1
(8.3.19) 
A 
X  
Si\A%.
Moreover, if $ is regular for Z, the maximal invariant parameter and statistic
are mutually sifficient, i.e.,
(8.3.20) 
A* 
X 
S \S*
(8.3.21) 
A  
Â±  
S*\ A*. 
â¢
As a corollary of this proposition, let us consider the case where Z is only 
sampling $-in variant.

392
8. Invariant Experiments
8.3.8 Corollary. If $ is a group, and if Â£ is sampling ^-invariant, then the 
almost surely maximal ^-invariant parameter is sufficient in the experiment 
marginalized on both the parameter and the almost surely maximal $- 
invariant statistic, i.e.,
(8.3.22)
Since $ I S | A, this corollary is a direct consequence of Corollary 8.2.27.
Barnard (1963) defined the principle of ^-sufficiency based on Property
(8.3.22). This principle says that, since the distribution of 5J depends only 
on A%, then in the absence of prior information on A$, 5$ contains all 
the available information on A%. In the Bayesian framework this means 
that 5 | is sufficient for A%, i.e., A% X S  | 5 |. Therefore, in view of 
Theorem 8.3.7, the most natural assumption implying these properties is 
that the prior probability is such that $ I A; consequently, this invariance 
assumption seems to be the most appropriate way to represent absence of 
prior information.
This approach can be used, in particular, if B C A  represents a paramÂ­
eter of interest. Then one may look for a family G of transformations on A 
such that B = A$. One then looks for a family G of transformations on S 
such that $ 1 5  | A. With an invariant prior probability, i.e., $  I A , this 
ensures that $  I (A V S) and, therefore, B and 
are mutually sufficient; 
in this context, as shown in Section 3.4.1, inference on B enjoys both good 
robustness properties and is easily computed (see Section 3.4.1), even when 
no cut is available.
It is interesting to consider special cases of this theorem for particular 
structures of $. Recall that 
s) = (<7(a), g(s)). When g(a) = a 
Vg Â£ G 
( as, e.g., in stationary process with invariance with respect to the shift- 
operator), then A  C A$ and, therefore, A% = A<$> â A. In this case,
(8.3.19) becomes trivial and (8.3.18) says that 
is a sufficient statistic in 
Â£. The opposite situation is when A% = A  fl J, i.e., the trivial parameter. 
Then 8.3.18 becomes trivial and (8.3.19) says that 5$ is an ancillary statistic 
in Â£. Therefore, invariance may be seen as a tool for obtaining ancillary 
and sufficient statistics in a Bayesian experiment.
A Â± S l \ A % .

8.3. 
Invariant Experiments
393
8.3.9 Corollary.
(i) 
If Â£ is sampling ^-invariant and A% â A, then A  X S \ S$.
(ii) 
If $  is a group, Â£ is sampling ^-invariant and A% = A  0 2 , then 
A Â± S % .
(iii) If Â£ is ^-invariant and A% = A  fl J , then A  X S$. 
m
The proof of (i) follows directly from Theorem 8.2.28; (ii) is a consequence 
of Corollary 8.3.8, and (iii) of Proposition 8.3.7.
Theorem 8.3.7 and Corollary 8.3.8 are also useful in searching for pivotal 
functions, i.e., random variables functions of both the parameter and the 
observation whose distribution does not depend on the parameter.
8.3.10 D efinition. A sub-cr-field M  C A  V S  is called a pivotal a-field if 
M Â± A .  
m
The usual procedure for finding a pivotal cr-field is to look for a group 
of transformation G on A such that A$ is trivial. One then determines G, 
a group of transformations on S such that 3> I S | A. Finally, one need 
merely exhibit a random variable (i.e., function of both the parameter and 
the observation) that is <Â£-in variant. More precisely:
8.3.11 Corollary.
(i) 
If $  is a group and if Â£ is sampling ^-invariant, then
( A V S ) % Â± A \ A % .
Therefore, if A% = A  fl J, then (A V 5 ) | is a pivotal cr-field.
(ii) 
If Â£ is ^-invariant, then
( A V S ) % Â± A \ A % ,  
( A V S ) % Â± S \ S i .
Proof.
(i) $ I S | A  => $ I (A V S) | A, by Theorem 8.2.23(ii) and the result 
follows from Corollary 8.2.27.
(ii) follows directly from Theorem 8.2.29. 
â 

394
8. Invariant Experiments
In applications it is often the case that 
is trivial; it is rarely the 
case that S$ is trivial. However, if the experiment is reduced to a sufficient 
statistic, this may well be the case. This situation is of some interest in that, 
when a pivotal function is independent of the observation, the posterior 
distribution is the same as the joint and the sampling distributions.
We now turn to the problem of reducing an invariant experiment on 
a sufficient statistic. Similarly, a reduction may be made on a sufficient 
parameter. Recall that, by Theorem 8.2.25, $ I 5 | A  is equivalent to 
$ I T  | A  and that <Â£ I S  | A  V T, i.e., Â£ is sampling ^-invariant if and only 
if Â£a v t and 
are sampling ^-invariant.
The next results are Bayesian versions of the main results of Hall, Wij- 
man and Ghosh (1965) (Theorems 3.1 and 3.2) (see also Petit (1970)):
8.3.12 Theorem. Let $ be a group and let S  be sampling 3>-in variant. If 
7* C S  is a ^-stable 5-sufficient statistic, then 7$ is Â£,4vs*-sufficient and 
S$ and T  are sampling independent conditionally on 7$, i.e.,
A Â± S l \ T Â£  
and 
T  Â± S l \ A v T i .
Proof. Note that by Theorem 2.2.10, A  X S$ | 7$ and T  JL S$ | A  V 7$ 
is equivalent not only to (A V T) X 
| 7^ but also to A  X 
| T  and 
to T  JL 54 | 7$. Now, T  5-sufficient, i.e., A  1  S | T  clearly implies 
that A  X 
| 7 \ Since $ I S \ A  implies that <Â£ I S  | A  V T, and since 
A  JL S  | T, we also obtain, by Theorem 8.2.30, that $ I Â«S | 7 \ Therefore, 
by Corollary 8.2.27, S*$ A T  | TÂ£. 
m
The same result holds when the assumption that $ is a group is replaced 
by the assumption that $ I A.
8.3.13 Theorem. Let 5 be a ^-invariant experiment. 
If T  C  S is a 
^-stable 5-sufficient statistic then (A V T) JL 
| 7$.
Proof. The proof is the same as above, since <Â£ I (AVS) implies that $ I 5. 
Therefore 
JL T  | 7$ by Theorem 8.2.29. 
â 
The next result shows that, when 5 is sampling ^-invariant and

8.3. 
Invariant Experiments
395
A% = A y a ^-stable 5-sufficient statistic T  may be reduced to its almost 
surely 3>-in variant sub-cr-field.
8.3.14 Theorem. Let Â£  be sampling ^-invariant and A% = A. If T  is a 
^-stable 5-sufficient statistic, then 7J is also 5-sufficient. In particular, the 
minimal 5-sufficient statistic S A  is included in 5Â£. Therefore (5*4)! =
Proof. Since $ I S  | A  implies that $ I T  \ A, A  JL T  \TÂ£ by Corollary 
8.3.9(i). This, along with T  5-sufficient, i.e., A  X S  | T, is equivalent 
to A Â± S  \T Â£  by Theorem 2.2.10. By Corollary 8.3.9(i), A  JL S  | 5J. 
Therefore, by Theorem 4.3.3, 5.4 C 5 | D 5 = 5Â£ since X fl 5 C 5Â£. 
â 
In this kind of reduction,there is also a connection between pivotal cr- 
fields.
8.3.15 Corollary. Let $  be a group and let Â£ be a sampling ^-invariant 
experiment. Then (4 V 5 )$  X (4V T ) | (A VT)$ for any ^-stable statistic 
T  C 5.
Proof. $ I 5  | A  is, as before, equivalent to <Â£ I T  | A  and $ I 5  | A  V T. 
By Theorem 8.2.23(h) and (i), $ I (A V 5) | A  V T  and the result follows 
by Corollary 8.2.27.
We now extend the result of Corollary 8.3.11.
8.3.16 Corollary. Let Â£ be a ^-invariant experiment. 
If T  C 5  is a 
^-stable 5-sufficient statistic,
then
(i) 
( A V T ) % Â± A \ A %
(ii) 
( A V T ) % Â± S \ T Â£ .
Proof. Since $ I (A V5) implies $ I (.4 VT), by Corollary (8.3.1 l)(ii), we 
have (i) and (A V T)J X T  | 7J. Now, 4  X  5  | T  implies (4  VT) X  5 | T 
by Corollary 2.2.11. Therefore (4  V T)$ X  5  | T. But this, along with 
(4  V T)% X  T  | T |, is equivalent to (4  V T )J X 5 | 7J by Theorem 
2.2 .10. 
-

396
8. Invariant Experiments
Theorem 8.3.12 calls for two remarks. Firstly, if $ is regular for Â£, then 
tSJ and 7$ may be replaced by Â«S$; 7$ and this is precisely the results of 
Hall, Wijsman and Ghosh up to the null sets of A  for the prior probability. 
Secondly, the only place where the group structure of $ is used is in the 
application of Corollary 8.2.27. If $ is a monoid, the result will be obtained 
by Theorem 8.2.26 under the further assumption that S$ i  T | <p_ 1(T), 
and this is implied by <p_ 1(tS) JL T  | ^>_ 1(T), since 
C (p~1(S).
We conclude this section by looking for invariant cr-fields in the examples 
presented in Section 8.3.1.
Example 6. In this example, <F is regular for S because the uniform distriÂ­
bution on [0,27r[ is invariant under <F. *4$ = X and 
is somewhat difficult 
to describe. Note that x = (p, J)' is sufficient for a. If T  = cr(af), then 
A  JL S  | T. According to Corollary 8.3.8, for the invariant prior probability 
A  X 
i.e., S$ is ancillary. Now, 7$ = affix} = a{t2}, and therefore, by 
Theorem 8.3.12, A  JL 
| 7$ and S$ X T  | A  V 7$. These two relationÂ­
ships do not depend on the prior probability. 
â 
Example 7. In this example, $ is regular for 8 . 
A $  â J, by Corollary 
8.3.8. In fact, S$ = cr{st- â s : 1 < i < n} where s = L ^ 1<i<n s;, since 
u = (si â s, ..., sn â ?)' is maximal invariant and (u | a) ~  An(0, 1  â nee') 
where e =  (1 ,1 ,..., 1)'. Clearly, T  =  <7( 5} is sufficient for a, i.e., A  JL S  | T, 
and 7$ = 1 . Therefore, by Theorem 8.3.12, A  Â±  S$ and S$ X T  | A  for 
any prior probability, i.e., u JL a and u JL s | a. 
â 
Exam ple 8. In this example, let us choose Zi = e = (1 ,1 ,..., 1)' 
and set rj = B'e. 
Then, if a â (7,Â£)> 
(Vi | Â«) 
~  LNp(r},H),
9 M,v{yi) â Myi -f v where v â A'e, M  is a nonsingular p x p-matrix, 
and 
Â£) = (Mr) -f 
Let us further simplify this
example by taking M  = I. Then 
is the translation group on Mp. Let 
us define y 
= 
Â£Â£i<i<n!/i> 
u 
= 
(yi -  y , ..., yn -  y)' 
and
V = 
-  y){Vi -V ) ' -  nu'uâ¢ Now s * -  ai U} and ^
For any prior probability which is the product of the Lebesgue measure on

8.3. 
Invariant Experiments
397
Mp with any probability on Cp, by Proposition 8.3.7 and since <Â£ is reguÂ­
lar for Â£, *4$ X S  | 5$ and A  X 
| A$, i.e., E X y | u and p JL u | E. 
Now T  
= 
a{y,v} is the minimal sufficient statistic for (rj, E). 
7$ = <j(y) and, by Theorem 8.3.12, for any prior probability A  X 5$ | 7$ 
and 5$ X T  | *4V7$, i.e., (77, E) X u | v and u X  y | 77, E, v. Let us examine 
another special case of this example. Let us take E = cr2 1, M  = Q, 1/ = 0, 
where Q'Q = I. Now 0q(Â»7, <r2) = (Q77,cr2). Therefore *4$ = (77'77, cr2) 
for 77'77 = Â£'Â£ if and only if there exists Q orthogonal such that 77 = 
is difficult to describe. 
If to = t(v) = Â£Xa<i<n(& -VYiVi ~V)> 
then clearly T  = 
cr{y, w} is minimal sufficient for a proper prior 
probability and 7$ = <r{$y, w}. By Theorem 8.3.12, * 4 X 5 $  | 7$ and 
5$ X  T  | A  V 7$, i.e., 77 X 5$ | y'y,w and 5$ X 1/ 177,(72 ,y'y,w. With an 
invariant prior, *4$ X T  | 7$ and A  X 7$ | *4$, by Proposition 8.3.7, i.e., 
(rj'rj, cr2) JL y | y'y, w and tj Â±  (y'y, w) \ r)'r), a2. 
â¢
Exam ple 9. In this example, $ is a group isomorphic to A and p is a prior 
probability such that p o"g~l <C p V aG  A  Let us introduce the âinverseâ 
of a measure on a group A, by p~'1(B) = p(B~1). Then for any /  Â£ [A]+, 
f A f(a)p~ 1(da) = f A f ( a " l)p(da). Moreover, if ha : A â* A is defined as 
ha(a') = a 'a , then
Therefore, if po 7j â 1 ~  p Va Â£ A, then p~ 1 oh~ 1 ~  p " 1 Va Â£ A. Therefore, 
using Theorem 8.2.38, for any M  C A  V S such that, V M  Â£ M  the inverse 
image of M  under the map
belongs to A  Â® M ,  
= M<f> H M .  
One is thus interested only in 
^-invariant cr-fields. Observe that *4$ = X because 'ga{a) = aa and A 
is group. There is no interesting characterization of 5$ but, by CorolÂ­
lary 8.3.8, 5$ is ancillary, i.e., A  X 5$. Now if m Â£ [(A V 5)$], then
(a, a!, a) Â£ A x (A x S) ââº (aa', a â <r) Â£ A x S

398
8. Invariant Experiments
ra (a a ,a * s) = m(a,s) V a, a Â£ A and Vs E S. Taking a = a-1 shows 
that m (a,s) = m (e,a_1 *s). Now, u = a-1 
: A x S' â>â¢ 5 is maximal
invariant. Indeed, if aâ 1 
= a*"1 â cr, then a = (aa_1)^ s,; i.e., a = af *s
and a  = a'a, and therefore (a,<r) = <pa/(a,s). By Corollary 8.3.11, u X a, 
i.e., a is a pivotal function. By direct computation, V / positive Borel funcÂ­
tions on S , 2[/(u)] = J5 f(a)Q(dcr). Hence the joint distribution and the 
sampling distribution of u is given by Q. If // is such that $ I A  then, by 
Corollary 8.3.11, we also know that u 1  s | Â«S$. A most interesting situÂ­
ation is when there exists a ^-stable sufficient statistic with values in the 
group A, i.e., in the parameter space. More precisely, let t : A x 5 âÂ» A, 
T  = t~x{A) C S, such that t is sufficient, i.e., a JL s | t. We suppose that 
there exists /? : A ââº A, with B = /3~1(A) C A  such that t(a*s) = (3(a)t(s) 
Va G A, Vs G S. Since t o <pa = t(a *s), this ensures that T  is ^-stable. 
However, it is evident that P(oi\ot2) â /?(<*i)/?(<*2) and f3 is an endomorÂ­
phism on A. We first remark that B is sufficient for T  and then for S, since 
T  is sufficient. Indeed,
A ftt) ]  
= 
[  f[t{a*<r)]Q(dcr)
Js
= 
f  f[ft(a)T)]Q(dT)
JA
where Q = Q o r 1. Therefore A[f(t)] is B-measurable. If we suppose 
that S is identified: A  = B, and if the points of A are measurable, then 
!3 is bijective. By replacing t by /3~l o t, it is readily seen that we may 
suppose, without loss of generality, that t(a*s) = at(s). Therefore 7$ = 2, 
and if /[a a ,t(a  * 5)] = /[a,f(s)] Va E A, then f(a,t(s)) = /(e, a~1t(s)). 
This shows, by Corollary 8.2.5, that v = aTlt is maximal invariant, i.e., 
(*4VT)$ = u-1(,4). By Corollary 8.3.1 l(i), v X a; this may be seen directly 
as follows:
I[g(a)h(v)] 
= 
j 
g(a)h[a~1t(a * a)\p(doi)Q(dcr)
J axS
= 
I  9 ^)p(da) f  h(r)Q{dr).
Ja 
Ja
Hence a JL v and Q is the distribution of v. If the prior probability p is 
invariant then, by Corollary 8.3.15, v JL s and, therefore, v JL t. This is not 
readily seen since
l[f(t)h(v)] 
=  
I[f(av)h(v)]

8.3. 
Invariant Experiments 
399
= 
/  
f{ar)h{T)n{da)Q{dT)
JAxA 
= 
Ja K t){ Jj{aT)n{da)}Q{dr).
It will nonetheless be verified if we can show that
f (x ) P (dx),
where P is the predictive distribution of t. In fact, if p is a left-invariant 
probability on A this is also right-invariant and the above relation is then 
true with P = p. Indeed,
f{<x~l T(3]n{da) -  J r f ( a ~ 1)fi(da)
since p is left-invariant. Therefore, V/? E A,
/ 
= f Kdr) j f l ^ T ^ n i d a )
JA 
JA 
JA
= 
/  
[  f[<X~1T0 \tl(dT)
JA 
JA
= f Kda) [ f[T0lKdT)
JA 
JA
= 
f(rP)n[dT).
Hence,
/ f(jP)n{dr) = f f(r)fi(dr) = f f{ a ~ 1)n{da),
JA 
JA 
JA
i.e., p is right invariant and /iâ*1 = p.
It is now easy to compute the posterior distribution of the parameter. 
Indeed,
S\g(a)]=T\g(a)] 
= T ^ t T 1]]
=  /
JA
= 
/  <K<r)Q_1(Â£ir)
JA
-  
[  ff(a)(Q-1 oIT^ida).
JA
This shows that Q~l o ~gjl is the posterior distribution of a. 
â 

400
8. Invariant Experiments
R em ark. The above example illustrates an important point of view in 
statistics which is midway between sampling and Bayesian analysis. ForÂ­
mally, the model is Bayesian, since there is a prior probability on the paÂ­
rameters. However, this prior probability is in fact straightforwardly derived 
from the sampling process, due to an invariance property with respect to 
a group naturally acting on this process. This point of view bears several 
names in the literature. It is sometimes called âfiducialâ or ââstructuralâ 
(Fraser (1968, 1972)) or âinner inferenceâ (Villegas (1977(a), 1981)). In 
this approach, the most important technical problem is normally the unÂ­
boundedness of the invariant measure, and is the source of paradoxes (Stone 
(1976) and Dawid, Stone and Zidek (1973)). Unboundedness was avoided 
above either by considering only a sampling invariance property (where only 
null sets of the prior probability are taken into account) or by assuming the 
existence of an invariant prior probability. In this latter case the probability 
is both left- and right-invariant, which greatly simplified the proofs. The 
extension to the unbounded case requires further conditions , and these are 
not examined in this book. Let us remark however, that the main results 
presented in this chapter have been established in a more general setting 
than that of Pitmanâs experiment, as analyzed in the above example, by 
relaxing the group assumption, in particular. This approach satisfactorily 
relates invariance and the reduction of an experiment.
8.3.3 
Invariance and Exact Estimability
We now turn to the analysis of the role of ^-invariance in the study of 
exact estimability. Recall that the maximal exactly estimable parameter is 
A  fl<S, which may be difficult to characterize. It is therefore interesting to 
identify sub-<r-fields of A  D S. Ergodicity and ^-invariance give rise to the 
following kind of results:
8.3.17 Theorem . If S is ^-invariant and $ is ergodic on S  conditionally 
on A, then A $ S  is exactly estimable.
Proof. By Theorem 8.3.7, A$ JL S  | <S|, i.e., 
is an Â£a%vs- sufficient 
statistic. By Lemma 8.2.26, SJ C A% and hence the result follows by 
Theorem 4.7.8, with M  = 1 , T  â S ) B â A% and M  = 5$. 
â 

8.3. 
Invariant Experiments
401
Remark that it follows from the proof of Theorem 4.7.8 that, under the 
hypotheses of this theorem, we have
Note that A%S is not necessarily the maximal exactly estimable parameter 
in 
i-e., A  fl S. However, any exactly estimable subparameter B for
which S$ is sufficient is necessarily included in ,4$<S. Indeed, B X S | 
and B C S imply B C <5$. Hence B C S$ fl A. But S$ fl A  = A% fl S C\A 
= A % n S  = A%S, by (8.2.23) and (8.2.24), since A ^ n A  = A%.
The extension of Theorem 8.3.17 from the unreduced experiment Â£ to 
the general reduced experiment E ^ r  requires a further assumption:
8.3.18 Theorem . Let B C A, T C 5, Af C A  V<S. If Â£{Â£r is ^-invariant, 
(B V A f)| X Af V T | <p~1(M  V T) V (p E $, and $  is ergodic on M  V T 
conditionally on BV A f, then (B V Af)$(Af V T) is exactly estimable.
Proof. Since $ I (B V T) | Af then, $ I ( B V  Af) | Af V T by Theorem
8.2.25. This invariance and (# V Af)$ X (Af V T) | 
V T) V (p E $,
entail, by Theorem 8.2.26, that (B V M)% X Af V T  | (Af V T)%. The rest 
of the proof is identical to the proof of Theorem 8.3.15, setting A  = B V Af 
and S  = Af V T. 
â 
The further assumption, (# V A f)| X (Af VT) | ^ â^(A f VT) 
E 
is readily satisfied when $ is a group. By Theorems 8.2.25 and 8.2.29, this 
assumption is also redundant if $ I A f.
In the last chapter the general theory of invariant experiments, as exÂ­
panded in this chapter, will be applied to the analysis of stochastic processes; 
particular emphasis is placed on stationary and exchangeable processes.
(8.3.23)
A% s  =  A% n s
(8.3.24)
S l  = SA% = A % n S .


9
Invariance in Stochastic Processes
9.1 Introduction
In Chapter 8 we presented a general theory for invariant experiments, 
characterizing mutually sufficient cr-fields, exactly estimating and estimable 
cr-fields. In this chapter those results are used to analyze two classes of modÂ­
els which are particularly important for empirical work, namely, stationary 
and exchangeable processes. In contrast to Chapter 8, in this chapter the 
invariant transformations (p{a) s) = (g(a), g(s)) are such that g is always the 
identity on A.
We first consider experiments constructed from stationary or exchangeÂ­
able sampling processes, viewed as processes invariant with respect to shifts 
or to finite permutations. These sampling invariance properties lead to the 
invariance of the joint probability characterizing the Bayesian experiment, 
which is, in turn, equivalent to invariance properties of the predictive proÂ­
cess and of the posterior expectations. Thus stationarity or exchangeability 
involve the asymptotic sufficiency of the invariant <r-field; these important 
results follow directly from the general results obtained in Chapter 8.
The analysis is pursued by considering the properties of exact estimabilÂ­
ity for these experiments; clearly, such an investigation requires additional 
assumptions. The simplest case is the i.i.d. case, where identified paramÂ­
eters are exactly estimable. Furthermore, the i.i.d. case makes possible a 
complete characterization of exactly estimable exchangeable models.
403

404
9. Invariance in Stochastic Processes
For stationary processes, the case of Moving Average of order q processes 
(MA (g)) is straightforward because their g-dependent character implies 
that their tail-cr-field is exactly estimating. Markovian stationary processes 
are then studied and we present, in terms of measurable separability, a 
version of Doeblinâs condition which is sufficient for their exact estimability. 
We also give a general presentation of ARMA (p, q) models; their exact 
estimability is derived from the preceding analysis of Markov processes. 
Note that this presentation is in cr-algebraic terms, and does not rely on 
linear representations. Finally, the last section extends some of the results 
to conditional models.
This chapter follows a slightly different presentation schema. As usual, 
we start from a Bayesian Experiment Â£ = (A x S', A  V S, II) on which is 
defined a stochastic process (zn) with values in a measurable space (U,U). 
It is natural to introduce the canonical representation of this process, viz., 
a new Bayesian Experiment Â£ with the same sample parameter space A 
but with 
for the sample space; on this new sample space is defined 
the coordinate representation process on which the shift or the permutaÂ­
tion operators operate. But we do not wish to lose from view the original 
experiment Â£\ thus we shall systematically apply to Â£ the results derived 
more naturally on the representation experiment Â£. This double analysis 
in terms of both Â£ and Â£ may seem somewhat cumbersome but permits 
a change in the process, and therefore on the representation experiment, 
without affecting the original experiment. And in fact the analysis in terms 
of representation is particularly suited to making use of the results, obtained 
in Chapter 8, for stochastic processes, although it may be bypassed for the 
study of exact estimability.
The material developed in this chapter is based on (but considerably 
extends) Rolin (1986) for Section 9.3, and Florens, Mouchart and Rolin 
(1986) for Section 9.4.
9.2 
Bayesian Stochastic Processes and Representations
9.2.1 Introduction
Consider the following simple experiment: 
S = JRn, A = JR,
s = ($ i,..., sn) with (st- | a) ~  i.N(a, 1) along with a group of translations

9.2. Bayesian Stochastic Processes and Representations
405
on A x  S defined as <Â£>a (<b s) â (a -f a, Si + a , ..., sn -f a), ot G M. Clearly, 
the translated distribution is i.N(a + a, 1), so that we obtain $  I s \ a for 
$ = {<Â£>a : a G JR}. Here, because of the obtained conditional invariance, 
the group of translations appears as a ânaturalâ group of transformations 
operating on both the sample and the parameter spaces. One should nevÂ­
ertheless remark that, until now, and with the exception of Chapter 8, we 
have been reluctant to assume any algebraic or topological structure for 
(A x S',*4 V S): we wanted these spaces to be as âabstractâ as possible 
and merely required a measurable structure, i.e., a possibility of assigning 
probabilities to well-defined families of sets. It is therefore not natural to 
introduce groups of operators acting on such spaces; and in the above examÂ­
ple one may observe that the structure (s,- | a) ~  i.N(a, 1) implicitly refers 
to some representation of the observation â the translation group would 
appear much less natural for exps*-, for instance. Furthermore S{ is likely 
to be one of many possible random variables that may be defined from an 
actual experimental design. These considerations suggest that in actual staÂ­
tistical modelling one will introduce representations Â£ â (A x S', A  V5, n) of 
an abstract experiment Â£ = (A x 5, A  V <S, n) by means of transformations 
/  : A x S â> A x S which have the property that f(a,s) = (/i(a), / 2(s)), 
where each fk is measurable, and such that in Â£ one may exhibit, for inÂ­
stance, interesting invariance properties. We shall see subsequently that, for 
example, the shift operator is not naturally defined on an abstract sample 
space S although it can be naturally defined on processes defined on S', e.g., 
S = B p ,  f 2 (s) =  (xn)nâ¬w  and xn Â£ Bi.
Insofar as this section is meant to be rather general, the introduction of 
representations may appear as arbitrary. In fact, there may be interest in 
considering several representations of the same experiment, translating to 
the abstract experiment Â£ the results obtained for the different representaÂ­
tions Â£ (such as minimal sufficient cr-fields etc.).
9.2.2 Representation of Experiments
In this section we make the concept of representation precise and then 
present a basic result that allows one to translate results obtained with 
respect to a representation to the original experiment. We first state two 
theorems in the context of a general abstract probability space and then

406
9. Invariance in Stochastic Processes
apply them to a representation of a Bayesian experiment.
9.2.1 Theorem . Let (M, M , P) be a probability space, (M, Af) be a meaÂ­
surable space, and let /  : M  ââº M  be such that f ~ 1 (M ) C Af. 
If 
P â P o / _1, Af c  Af and Af = f ~ l (Af), then V m G [f~x(M)]+, 
3 m G [Af]+ such that
(i) 
m = fho f
(ii) 
Afm = (Afm) o /.
Proof. As before, Afo = 
and Afo = 
projections on Afo
and on Afo are mathematical expectations with respect to P and P reÂ­
spectively. The existence of m G [Af]+ satisfying (i) follows from Chapter 
0, Theorem 0.2.11. The proof of (ii) results from simple manipulations of 
conditional expectations and of image probabilities (see Proposition 0.3.8). 
Thus consider arbitrary n G [A/]+ and m G [Af]+ ; we then have that:
Af o[n o /  â¢ Af(m o /)] 
= 
Afo[n o /  â¢ m o /]
= 
Afo(nm) = Af 0[n(A'm)]
= 
Af0[H o /  â¢ (Afm) o /]
This shows that
Af(m of) 
= 
(Afm) of. 
â 
Theorem 9.2.1. permits properties of the representation experiment Â£ 
to be viewed as properties of the original experiment Â£. The next theorem 
illustrates as an example, the translation of a conditional independence 
property.
9.2.2 Theorem . Let (M ,A f,P) a probability space and (M ,A f,P ) a 
representation of this probability space through /  : M  â* M  as defined 
in Theorem 9.2.1. 
Let Af,* (i = 1,2,3) be sub-cr-fields of Af and let 
M i = f - 1 (Mi) (i = 1,2,3). Then:
(i) 
Afi X Af2 | M 3] P

9.2. Bayesian Stochastic Processes and Representations
407
if and only if
(ii) 
M i Â± M
2 \ M ^ P ,
Proof. Let ra* E [Mi]* 
(i = 1,2). Then, by Theorem 9.2.1,
M z(m i o f  â¢ m2 o /)  = A43(m im 2) o /  
a.s.P.
A43(mi o f )  â (Mzrhi) o /  
a.s.P.
A43(ra2 o /)  = (A43m2) o /  
a.s.P.
Therefore
A43(rai o /  â¢ m2 o /)  = A<3(^ i Â° /)  â¢ M 3 (fa 2 0 f) 
a.s .P. 
if and only if
.M3(m im 2) = vM3(mi) â¢ A43(ra2) 
a.s.P. 
â 
We now provide a precise definition of a representation experiment. This 
is a preliminary step before using the general results of Theorems 9.2.1. and 
9.2.2. on Bayesian experiments.
9.2.3 D efinition. Let Â£ = (A x S', .4 V <9, II) be a Bayesian experiment.
Then Â£ = (A x S', A  V <S, II) is said to be a representation of Â£ (through f )
if there exists a measurable function /  : A x S  ââº A x S  such that
(i) 
f  â (/1, /2) with fi : A  ââº A and /2 : 5 ââº S' both being measurable 
and, as usual, also considered as defined on A  x S.
(ii) 
n = n o f ~ l 
m
Note that, given this definition, any parameter (respectively, statistic) 
in Â£ is sent to a parameter (respectively, a statistic) in Â£ by the inverse 
image of f\ (respectively / 2).
Theorem 9.2.1. may be further refined in the context of a Bayesian 
experiments as is illustrated by the following proposition:
9.2.4 Proposition. Let Â£ = (A x S', A  V 5, II) 
be a representation
of Â£ = (A x 5, A  V S, II). For any B C A  and T  C 5, let us define

408
9. Invariance in Stochastic Processes
B = f i \ B )  and T  = / 2_1(f). Then:
(i) 
V b E [B]+ 
3 b Â£ [B]+ such that b = bo f x and Tb = (T 6) o f 2.
(ii) 
V t Â£ [T]+ 3 t Â£ [T]+ such that t = t o f 2 and Bt = (Bt ) o /*. 
â 
As an example of how to use both this result and Theorem 9.2.2., we 
draw attention to the following simple corollary.
9.2.5 Corollary. Let A' â f \ 1(A) and S' = 
Let T  C S and
T  = 
Then
i l S
 | f ; 9  ^  A1 Â± 5 '  |T; n,
i.e., any sufficient statistic in Â£ is translated, through / 2, into a sufficient 
statistic in ^U'vs7- 
â 
9.2.3 Bayesian Stochastic Processes
In the Bayesian experiment S = (A x S', *4 VÂ«S, II) a Bayesian stochastic 
process is a sequence of random variables {xn : n E JA} defined on (5 ,5 ) 
with values in some measurable space (Â£/,Â£/); this measurable space is called 
the state space of the process. We have already studied sequences of random 
variables in Chapter 6. However, at that stage, we did not require that 
each random variable xn has values in the same measurable space, which is 
essential when considering shift and permutation operators.
For the representation of a Bayesian stochastic process, we define:
(9.2.1) 
A = A, 
A  = A } 
/ i = i ,
i.e., fi is the identity map on A;
(9.2.2) 
S = U1N, 
S = UIN,
i.e., S  is the product cr-field of infinitely many copies of U\ and
(9.2.3) 
fi -  x -  (xn ,n e IN) = (x0 ,x i ,x 2, . â 

9.2. Bayesian Stochastic Processes and Representations
409
II is defined as in Definition 9.2.3, namely, II = II o /  1 and, in particular, 
pi â p and P = P o x~1.
We denote, as usual, the sub-cr-fields of S  generated by the stochastic 
process as follows:
(9.2.4) 
Xn = <r{xn} = x ~ \U )
Similarly, for any J subset of JN:
(9.2.5) 
X] =  \ f  Xn = <T{xn : n â¬ J}
nÂ£ J
In particular we denote
(9.2.6) 
*â"* =  V XP
n<p<m
(9.2.7) 
X
= V x m;
m>n
Xâ¢ is called the future after n of the process. Xâ¢ is the past up to time 
n of the process and is the canonical filtration associated to the stochastic 
process x. Note that
(9.2.8) 
XÂ£Â° = x~ 1 (UIN) = S'.
As in Chapter 7, the tail-<r-field of the process x is given by
(9-2.9) 
XT = f ) X â¢= D  \ /  Xm
n 
n m>n
It will on occasion be interesting to introduce the coordinate process (reÂ­
spectively, the parameter random variable) defined on the representation 
space (S,S) (respectively, the parameter space (A,A)) or, as usual, also 
considered as defined on (A x S, A  V <S), as follows:
(9.2.10) 
xn(a,u) = x n(u) 
= un
(9.2.11) 
a(a,u) = a(a) 
= a
V a 6  A, 
V u G 5 ,  
V n G l
To this coordinate process we may associate, using the same definitions, the 
sub-cr-fields of S. For instance:
(9.2.12)
Xn = x ~ 1(U),

410
9. Invariance in Stochastic Processes
and similarly for X j ,Xq , X f f ,X q* and so on. Note that for the coordinate 
process
(9.2.13) 
X I =  S = x-'L(li1N)
The notation for the coordinate process is motivated by the following idenÂ­
tity:
(9.2.14) 
xn o x = xn 
V n Â£ IN.
Note also that the coordinate process allows us to write the following idenÂ­
tity:
(9.2.15) 
l[b f o x ] = l [ b  fo x ] , 
V 6g[-4]+, 
V /  G [Z/W]+ .
9.2.4 Shift and Permutations
We now provide a standard definition of the shift operator and the 
finite permutation operators (see, for instance, Chung (1968), 8.1). These 
operators lead to the most typical invariances used in the statistical analysis 
of stochastic processes. For example, the shift operator is used in time-series 
analysis whereas the permutation operator is used in the analysis of survey 
data.
9.2.6 
D efinition. The shift operator, denoted by r  is the mapping from 
U ^  to 
defined by
(9.2.16) 
T(u)n = un+i V n Â£ IN'. 
â 
Note that r  is always a measurable transformation on ( U ^  ,U ^). ClearÂ­
ly r  is a surjective map but is not injective. Successive compositions of r 
give the iterates of r, i.e., r k : U ^
(9.2.17) 
r k(u)n = un+k V k , n â¬  IN.
If rÂ° is defined as the identity map on 
then the set of measurable 
transformations T = {rk : k Â£ IN} is a monoid but not a group. This fact is 
sometimes troublesome. One way to overcome this difficulty is to consider 
r as a map from 
to U^. Clearly, r is now a bijective measurable 
transformation on 
and would be quite natural if the Bayesian stochastic 
process were indexed by Z  instead of JN. Let us note that this is often done 
in stationary processes and in time series analysis. For the same reason as

9.2. Bayesian Stochastic Processes and Representations
411
in Chapter 6, this case will not be studied in detail in this chapter, which 
is why we do not introduce this representation formally.
The shift operator also acts by composition on the Bayesian stochastic 
process x. Namely,
(9.2.18) 
(rk o x)n -  xn+k 
V k , n Â£ N .
Thus r k o x is the kth times shifted stochastic process, i.e., starting at x &.
In general, it is difficult to define r  as a measurable transformation 
on the abstract measurable space (A x S', A  V S) while under our usual 
conventions r  may also be considered as a measurable transformation on the 
representation space ( A x  S, A  V S, II) and may afterwards be translated 
in the framework of the original experiment Â£ = ( A x  S, .4 VS, II) through 
(*. *)â¢
Let us note that, with these conventions on r  and the definition of the 
coordinate process, we have the following identities:
(9.2.19) 
xn+k = xn o r k = (rk o x)n 
V k,n G M
(9.2.20) 
a o r  = a.
This implies, in particular, that
(9.2.21) 
X n =  T - n ( X 0).
As in Chapter 8, an invariant sub-<r-field is associated to the shift operator. 
More precisely, we define the a-field of shift-invariant sets in U ^:
(9.2.22) 
Ur = { B e U IN : r - 1(J?) = B},
and, according to Theorem 8.2.1,
(9.2.23) 
/  â¬ [Ur ] 
/  â¬ [Um ] 
and f o r  = f.
The shift-invariant events of the stochastic process x are then defined as
(9.2.24) 
Aâr  =  a;-1(i/r)-
Similarly, we define the shift-invariant events of the coordinate process x as
(9.2.25) 
Xr = x - \ U T).

412
9. Invariance in Stochastic Processes
As in Chapter 8, we consider the T-invariant cr-field in Â£, (*4V5)r, and 
its traces either on sub-<7-fields of A  or on sub-cr-fields of S. The structure 
of these invariant cr-fields are given in the following lemma.
9.2.7 Lem m a. 
In the representation experiment Â£ = (A x S', A  V Â«S, II) 
of the experiment Â£ = (A x S, A  V 5, II), we have
(i) 
V B C  A  
Br = B
(ii) 
V T c S  
f r = Tr n f
and, in particular,
(iii) Sr = Xt -
Moreover, in the original experiment,
(iv) A'r = *_ 1(5r ).
Proof. To prove (i), it suffices to use (9.2.20), which shows that we have 
A  C (AW S)r- Now, by the definition of 7r, (ii) is a direct consequence 
of (iii). If fh E Ap, in = f  o x  with /  E [Z/r]. Therefore, by (9.2.19),
m o r  â f  ox o r â f  o r ox â f o x  â fh, and so m E [(-4 V Â«S)r fl Â«S] = [5r].
However, if m E [Â«Sr], then there exists /  E \U^] such that fh = f  ox  and 
m o t  â fh. This implies that f  ox = f  o r ox. Therefore, by the definition 
of the coordinate process, f o r  = /, i.e., /  E [Z/r], an<^ so m E [Ar]- Finally, 
(iv) is a direct consequence of (iii) and (9.2.14). 
â 
The shift operator may be used to represent the tail cr-field of the 
Bayesian process x , and of the coordinate process x. Indeed let us deÂ­
fine the cr-fields of sets in 
whose definition does not depend on the first 
n coordinates for any finite n, i.e.,
(9.2.26) 
UT = On r-"(W
or equivalently, by Theorem 0.2.11,

9.2. Bayesian Stochastic Processes and Representations
413
(9.2.27) 
f e [ U T] O  /  â¬ [WW] 
and 
V n Â£ l
3/n â¬ [Z/W] 
such that 
/  =  /â o r " .
Although Z/j is not really a tail cr-field, we nevertheless have:
(9.2.28) 
XT = s ^ Z / t )  
and
(9.2.29) 
A't = x~l {XT) = a r 1^ ) .
We now turn to the same construction for the group of finite permutations.
9.2.8 
D efinition. To a permutation of the first k integers we associate the 
finite permutation operator c from 
to 
defined by:
(9.2.30) 
cr(u)n 
= 
V n < k
= 
un V n >  k
We denote by E* the set of all finite permutation operators satisfying
(9.2.30); E = UkelN^k is the set of finite permutation operators.
Note that any c Â£ E is a bijective bimeasurable transformation on 
Hence E, as well as E* for any &, is a group of measurable 
transformations on {U ^\U ^ ).
As for the shift operator, E acts by composition on the process x. InÂ­
deed, if cr G Efc,
(9.2.31) 
((Tox)n 
= 
x9(n) 
V n < k
â 
xn 
V n >  k
Thus cr o x is the transformed stochastic process obtained by permuting, 
according to cr, the first k coordinates of the stochastic process x.
As for the shift operator, E may be considered as a group of measurable 
transformations on the representation space (A x S ,A V S ). Note that, with 
this convention,
(9.2.32) 
c o x  
= x o c
(9.2.33) 
a ocr 
=  a

As before, we define the a-field of symmetric sets in 
as
(9.2.34) 
WS =  {BGWW : <t_1(Â£) = B V a G Â£}, 
and note that, according to Theorem 8.2.1,
(9.2.35) 
/  â¬ [Z/E] &  f  G [UÂ®] 
and fo<r = f V <r â¬ S.
The symmetric events of the stochastic process x is then defined as
(9.2.36) 
Xs = ar_ 1(Z/s )
and, similarly, we defined the symmetric events of the coordinate process x 
as
(9.2.37) 
Xs = x -^U s).
Considering, as in Chapter 8, the E-invariant or-field in Â£, (A V 5)s and 
its traces on sub-<x-fields of A  or on sub-<7-fields of <S, we may reproduce the 
proof of Lemma 9.2.7 to obtain the following lemma:
9.2.9 Lem m a. 
In the representation experiment Â£ = (A x S, *4 V*S, II) 
of the experiment Â£ = (A x S, A  V S, II), then
(i) 
V Â£C *4
(ii) v f c s  
f E = * s n f ,
and, in particular,
(iii) Ss=A 's.
Moreover, in the original experiment,
(iv) 
X s  = a:-1(Â«Ss). 
â 
Now, it follows from (9.2.22), (9.2.26) and (9.2.34), that
(9.2.38) 
Uv CUt C U s C Um .
414 
9. Invariance in Stochastic Processes

9.3. Standard Bayesian Stochastic Processes
415
Therefore, for the coordinate process x ,
(9.2.39)
XT C Xt C 
C T0Â°Â°,
and the same inclusions hold for the Bayesian stochastic process Â£, i.e.,
Until now, all these asymptotic and invariant cr-fields have been analyzed 
without any use of the probability measure. Invariance properties of the 
probability measure will allow us to describe a.s. equivalent <r-fields and to 
refine the inclusion properties obtained above.
9.3 
Standard Bayesian Stochastic Processes
A natural application of invariant parameter experiments is the analÂ­
ysis of discrete time invariant stochastic processes such as stationary or 
exchangeable processes (see, e.g., Anderson (1971)). In what follows these 
processes are analyzed both from the point of view of invariance, in light of 
the general theory of Chapter 8, and from the point of view of exact estimaÂ­
bility, as developed in Chapter 7. This exact estimability will be obtained 
for the particular cases of i.i.d. and of Markovian stationary processes.
Both stationary and exchangeable processes are generally defined by 
properties of their sampling distributions. In the framework of a Bayesian 
experiment, this will be extended, in a natural way, to properties of the 
joint distribution; these experiments will consequently be characterized by 
properties of the predictive probability and of the posterior expectations. 
This is useful for finding asymptotically sufficient cr-fields and, therefore, 
conditions for exact estimability of the minimal sufficient parameter.
9.3.1 Stationary Processes
The structure of a stationary process is usually presented as follows. In 
the statistical experiment S defined as
(9.2.40)
XT C Xt C 
C T0Â°Â°.
(9.3.1)
Â£ = { ( S , S ) ; P a 
a â¬ A}

416
9. Invariance in Stochastic Processes
the stochastic process x = {xn : n Â£ IN} defined on (S, S) with values in 
(U,U) is stationary if
(9.3.2) 
l a [f(xi, x2, ..., xâ+i)] = I a[f(x0, x i , . . . ,  xn))
v Â« e l ,  
V a â¬ -A, 
V /e[Z/"+1]+ .
Using a monotone class argument and the definition of the shift operator in 
Section 9.2.4, (9.3.2) is actually equivalent to
(9.3.3) 
l a( f o r n o x ) =  l a( f  o x)
V n â¬ W ,  
V a e A ,  
V f e [ U IN}+.
In the Bayesian experiment
(9.3.4) 
Â£ = ( A x S )A v S , H )
it implies that, for any prior probability,
(9.3.5) 
^4[/(*i,iC2.---,*n+i)] = A[f (x 0 , x i , . . . , x n)]
V n Â£ l N ,  
V /e[Z/n+1]+,
and this is also equivalent to
(9.3.6) 
A ( f o r n o x ) = A ( f o x )  
V n g l ,  
V /e [Z /W]+.
Now, in the representation Â£ of S as defined in Section 9.2.3, (9.3.5) becomes
(9.3.7) 
^ (? o r)= ^ 4 (f) 
V n G l  V f Â£ [ Â£ 0n]+ ,
and (9.3.6) becomes
(9.3.8) 
A ( t o r n) = A(t) 
V n G l  
V f Â£ [ ^ 0Â°Â°]+ .
Now, recalling, by Lemma 9.2.7, that A r = A, one sees that (9.3.7) is 
actually equivalent, by Definition 8.2.22, to
(9.3.9) 
{r} I X Â£  \ A  V n Â£ IN,
and that (9.3.8), equivalent to (9.3.9), amounts to
(9.3.10) 
r  I i ^00 | A.
This leads to the following definition of Bayesian stationarity:

9.3. Standard Bayesian Stochastic Processes
417
9.3.1 D efinition. In the Bayesian experiment Â£ = (A x <S, A  V Â«S, II), the 
Bayesian stochastic process x = {xn : n Â£ JN} is M. - station ary, where 
A4 C .4 V 5, if
M( f o r o x )  = M( f o x )  
V /G [Z /^ ]+ . 
â 
This definition, specialized on a sub-cr-field B of *4, and reinterpreted 
in the representation Â£ of Â£ gives the following theorem, which is expressed 
using the notation of Section 9.2:
9.3.2 Theorem . In the Bayesian experiment Â£ = (A x 
V *5,11), let 
B C A  and x = {xn : n Â£ IN} be a Bayesian stochastic process. Then the 
following properties are equivalent:
(i) 
x is ^-stationary
(ii) r I S  | B
(iii) T I ( B V S )
(iv) V I S  z n d T I B \ S
(v) 
T
I
S
\
S
r
a . n d B  Â±
S
\
S
r .
Moreover, in this situation,
(vi) Sp = Sr C\S.
Proof. Clearly, in the representation Â£ of Â£, B (f o r o x) = B (f ox) is 
equivalent, by (9.2.15), to B (f o r  ox) = B (f o J); since r  o x = xo r, this is 
equivalent to t3(foxor) = B(fox). By Definition 8.2.22, this is equivalent 
to T I S | 5, because # r = #  by Lemma 9.2.7. 
Since 6  C (A V S)r 
by Theorem 8.2.28, (ii), (iii), (iv) are equivalent and are equivalent to 
T I S  | Sp and B JL S  \ Sp. By Theorem 8.2.20, Sp = SrH S and therefore, 
by Theorem 8.2.23 (iii) and Corollary 2.2.7, this is equivalent to (v). 
â 
Note that, by a monotone class argument, T I S  | B is actually equivÂ­
alent to T I X q | B V n Â£ IN. If the equivalences between (ii), (iii), and

418
9. Invariance in Stochastic Processes
(iv) remain true when S is replaced by X â , then (v) is no longer equivalent 
because Xq is not T-stable.
It is whorthwhile rewriting (iv) and (v) in terms of posterior expecÂ­
tations. Let us first note that T I S amounts to saying that, in S , the 
predictive probability is shift-invariant, whereas T I B | S  is equivalent to 
saying that
(9.3.11) 
r~k(S)b = (Sb) o r k 
V 6 6 [S]+ , 
V l e J V  
or, equivalently,
(9.3.12) 
Xj?b = (X$Â°b)ork 
V 6 e[tfj+ , 
V k â¬ IN.
Now, B X  S | Sp is equivalent to saying that 
= Xp is an Â£gv^-sufficient 
statistic or, equivalently,
(9.3.13) 
X0Â°Â°b = Xrb V &Â£[Â£]+.
Therefore, since Xp is shift-invariant,
(9.3.14) 
XÂ£Â°b = X ? b  = Xpb.
Then, according to Theorem 7.7.2, the dynamic Bayesian experiment 
Â£ â (A x S,*4 VÂ«S, II, Xq ) is tail-sufficient, i.e., Xt is also an 
sufficient 
statistic, but this is of course evident since by (9.2.39), Xp C X p .
Theorem 9.3.2 induces the following corollary on the original Bayesian 
experiment:
9.3.3 Corollary. In the Bayesian experiment Â£ = (A x 5,^4 V Â£>II), let 
x = {xn : n E IN} be a Bayesian stochastic process with state space (U,U) 
and let B C A. Then
(i) 
x is /^-stationary
if and only if
(ii) 
x is ^-stationary

9.3. Standard Bayesian Stochastic Processes
419
and
(iii) Xp is an Â£bvXÂ£Â°-sufficient statistic.
Moreover,
(iv)
Proof. By Lemma 9.2.7, (iv), 
Xr = x~x(Sr)â¢ So, by Theorem 9.2.2, 
B JL S | Sp; n  is equivalent to B X XqÂ° \ Xp; II . However, by (8.2.40),
T I S | Sr is equivalent to S p ( f o x o r k) = Sp(fox) V k E N, V /  E [U^]+. 
Since, by (9.2.19), x o r k = r k o x ) this is equivalent to Xp(f  o r k o x) = 
Xr ( / ox) V /  Â£ [Z7^]+ , V k E JN, i.e., x is Tp-stationary. 
â 
Note that (ii) is a property depending on the predictive probability only, 
whereas (iii) is a property of the posterior expectations.
By Theorem 4.7.8, the minimal sufficient parameter of a ^-stationary 
process will be exactly estimable if Xp is exactly estimating, i.e., Xp C B.
Note that in terms of Chapter 8, this may also be interpreted as T is 
ergodic on X qÂ° conditionally on B. As shown in Theorem 8.2.34, a sufficient 
condition for this ergodic property is the mixing condition given in DefiniÂ­
tion 8.2.33. For the special case of a ^-stationary process x, this mixing 
condition may be written as
(9.3.15) 
V f,g G 
B(f  o x â¢ g o rn o x) â âº B( f  o x)B(g o x)
a.s.n 
as n ââº oo
Note that under such a mixing condition we obtain from Theorem 4.7.8, or 
from (8.3.23) and (8.3.24),
(9.3.16) 
X f  = BXâ¢ = X ^ B  = 73 n X f ,
i.e., in Â£&vxÂ£Â° there exist almost sure equalities between the shift-invariant 
cr-field of the process x, the minimal sufficient parameter, the minimal sufÂ­
ficient statistic, and the maximal exactly estimating statistic and estimable 
parameter.

420
9. Invariance in Stochastic Processes
The following proposition follows directly from Theorems 8.2.23(i) and
8.2.25.
9.3.4 
P roposition. If a: is a ^-stationary Bayesian process then, for any 
C C B, x is a C-stationary Bayesian process. In particular, x is an 1- 
stationary Bayesian process, i.e., a predictively stationary Bayesian proÂ­
cess. 
â 
Note, however, that although T is ergodic on X qÂ° conditionally on 5, 
T is not necessarily ergodic on XqÂ° conditionally on C. We will see in the 
next sections that the condition of ergodicity becomes redundant once more 
structure, such as conditional independence properties, is imposed on the 
stochastic process since the mixing Condition (9.3.15) may be interpreted 
as an asymptotic conditional independence.
9.3.2 
Exchangeable and i.i.d. Processes
The structure of an exchangeable process is usually presented as follows. 
In the statistical experiment Â£, defined as
(9.3.17) 
Â£ = { ( S , S ) ] P a 
a e A },
the stochastic process x = {xn : n Â£ IN} defined on (S,S) with values in 
(U,U) is exchangeable if
(9.3.18) 
Xa [/[*â( o)Â»*<r(i). â¢â¢â¢,**(*)]] = Z a[/(*o,a:i, â¢â¢â¢,**)]
V k Â£ N ,  
V < r e E t+1, 
V a e . 4 ,  
V / â¬  [Uk+1]+.
Using the definitions of the finite permutation operators presented in 
Section 9.2, in the Bayesian experiment
(9.3.19) 
Â£ = { A x  S . A V S ,  11}
(9.3.18) implies that, for any prior probability,
(9.3.20) 
A ( f  o cr o x) = A ( f  o x) 
V / â¬ ^ ] +  
V a Â£ E.
In the representation Â£ of Â£ defined in Section 9.2.3, and using (9.2.15) and
(9.2.32), (9.3.20) becomes:
(9.3.21) 
,4(fo<7) = ,4(f) 
V t Â£ [ S ]+, 
V< j e Â£ .

9.3. Standard Bayesian Stochastic Processes
421
According to Definition (8.2.22), and since A s  = A  by Lemma 9.2.9,
(9.3.21) amounts to saying that
(9.3.22) 
E I S |  A.
Note also that, since E is a countable group, by Theorem 8.2.20,
(9.3.23) 
( AWS) 1 = ( A \ / S ) s
and the same relation holds for any E-stable sub-cr-field of A  V S.
This leads to the following definition of Bayesian exchangeability:
9.3.5 D efinition. In the Bayesian experiment Â£ = (A x S , A V S ,  II) the 
Bayesian stochastic process x = {xn : n E JN} is M-exchangeable, where 
M C  A M S ,  if
M ( f  o a o x) = M ( f  o x) 
V /E [Â£ /^ ]+ , 
V cr E E. 
â 
Taking conditional expectations entails the following proposition:
9.3.6 Proposition. If x = {xn : n E IN} is an Af-exchangeable Bayesian 
stochastic process, then V M  C M , x is AAexchangeable. In particular, x 
is X-exchangeable. 
â 
Since, conditionally on Af, (xo,... xn_i, xn) and (x i,... ,xn, xo) have 
the same distribution for any n E IN, this is true for (xo,..., xn_i) and 
(xi, ..., xn). This entails the following proposition:
9.3.7 Proposition. If x = {xn : n E IN} is a Af-exchangeable Bayesian 
stochastic process, then x is Af-stationary. 
â 
By duplicating the proofs of Theorem 9.3.2 and Corollary 9.3.3, we can 
reproduce Theorem 9.3.2 replacing T by E; we then obtain the following 
proposition:
9.3.8 P roposition. In the Bayesian experiment Â£ = (A x 5, *4 V S, II), let 
B C A, and x = {xn : n E IN} be a Bayesian stochastic process then:

422
9. Invariance in Stochastic Processes
(i) 
a? is ^-exchangeable 
if and only if
(ii) 
x is Aâs-exchangeable 
and
(iii) X% is an Ssvxg0-sufficient statistic.
Moreover,
(iv) 
x *  =  X
s n
x â¢
.
Note that, once again, (ii) is a property of the predictive probability 
only, whereas (iii) is a property of the posterior expectations. Indeed, (ii) 
is equivalent to
(9.3.24) 
Xs (fo<rox) = Xs ( f  o x) 
V <r G E, 
V /e [ Z /W]+ , 
whereas (iii) is equivalent to
(9.3.25) 
XoÂ°b â X^b 
Vbe[B]+.
This property reveals, in particular, that the posterior expectations are 
invariant under permutations of the components of the process x.
Characterizing exchangeable processes as the marginalization of i.i.d. 
processes is a well-established tradition in probability theory. The pioneerÂ­
ing paper of de Finetti (1937) for {0,1} valued processes was first generalized 
by Hewitt and Savage (1955) before subsequently being extended in several 
ways. (For a good exposition see Chow and Teicher (1978), 7.3. Th. 2 and 
Dellacherie and Meyer (1980), Ch. 5; see also Freedman (1962, 1963), PitÂ­
man (1978), Aldous and Pitman (1979), Diaconis and Freedman (1984), and 
Aldous (1985)). The goal of Theorem 9.3.11 is to present a general Bayesian 
version of these results; this theorem also provides another complete dual 
characterization of a ^-exchangeable Bayesian process. Before stating the 
theorem, some additional definitions are required.

9.3. Standard Bayesian Stochastic Processes
423
9.3.9 D efinition. In the Bayesian experiment Â£ â (A x S ,A  V *S,II), let 
M  C A  V S  and x = {xn : n Â£ IN} be a Bayesian stochastic process. Then
(i) 
x is M-identically distributed if
M  g(xt) = M  g(x0) 
V k e IN, 
V g G [Z/]+ ;
(ii) 
x is M-independent if
JL 
Xn | Af;
0<n<oo
(iii) x is M-i.i.d. if it is A4-independent and .M-identically distributed. â 
Let us remark that, in the representation Â£ of Â£, that x is 5-identically 
distributed is actually equivalent to
(9.3.26) 
r  I 
| B.
Indeed, since Br = 5, T I (BOX) by Proposition 8.2.13. Now,
B g{xk) = B g(x0) 
V * â¬ JN, V g â¬ [U]+ 
is equivalent, by (9.2.15), to
B(g o x k) = B(g o x 0) 
V k e IN, V g Â£ [Z/]+ , 
and since Xk = xo Â° Tk this is equivalent to
B(t oTk) = Bit) 
V k G IN, V f  G [i;0]+-
Note the following straightforward proposition.
9.3.10 Proposition. In the Bayesian experiment Â£ = {A x 5 , A  V 5, II) let 
M  C A  V 5 and let x = {#n : n Â£ 1ZV} be a Bayesian stochastic process. 
Then:
(i) 
if x is A4-stationary, then x is A4-identically distributed;
(ii) 
if x is .M-i.i.d., then x is Af-exchangeable.

424
9. Invariance in Stochastic Processes
Let us remark that, as a corollary, a Af-stationary and Af-independent 
process is a Af-i.i.d. process, and conversely. However, any marginalization 
on A  C Af of an Af-i.i.d. process is A-exchangeable, by Proposition 9.3.6, 
since it is Af-exchangeable.
The next theorem proves that any exchangeable process is a marginalÂ­
ization of an i.i.d. process.
9.3.11 Theorem . In the Bayesian experiment S = (A x 5,^4 V *S,n) let 
B C A  and let x = {xn : n E IN} be a Bayesian stochastic process. Then
(i) 
x is ^-exchangeable 
if and only if
(ii) 
x is Xp-i.i.d.
(iii) Xp is an Â£t3vxgÂ°-sufficient statistic.
Moreover if x is ^-exchangeable, then
(iv) Xp â Xp â X'p.
Proof, (i) => (ii) and (iii): Since ^-exchangeability implies #-stationarity, 
we already know, by Corollary 9.3.3, that (i) implies (iii) and that x is 
^-stationary. We also know, by Proposition 9.3.8(iii), that x is also X%- 
exchangeable.
Now, by the ergodic theorem (Proposition 8.2.15), V k E JN V g Â£ [Â£/]oo> 
as n ââº oo,
and
g(xk+i+\) 
Xpg(xk+i) 
a.s.n and in h .
0<Kn-l

9.3. Standard Bayesian Stochastic Processes
425
Therefore, by the conditional dominated convergence theorem (see, for inÂ­
stance, Neveu (1970), Section IV-3), V t E [Xq]oq, as n â> oo
-  E
0<Kn-l
Xyft 5 (**+*+i)] 
=
0<Kn â l
But, by A'^-exchangeability, V Â£ > 0,
X i;[t </(a!fc+f+ i)] =  X s\t ^(xjfc+i)].
Therefore, V k â¬ Sf, V g â¬ [Z/]oo, V t G [*0*]oo,
X s [< ff(xjfe+1)] = X^t â¢ Ap[if(xjfc+i)].
Taking t = 1 and conditional expectation with respect to <Tp shows that 
â¢*s[sf(*ifc+i)] = A'r[ff(x)t+i)] and that
Xk+1Â± X * \ X r 
and Xk+lJLX$\Xz.
By Theorem 7.6.3 and Proposition 9.3.10, x is Ap-i.i.d. and A's-i.i.d. Hence
(i) implies (ii). However, if /* E [Z/]oo, 0 < < n,
^E n /*(**)
0<Jfc<n
= 
n  'Vs[/*r(a?*)]
0<ib<Â»
=  
n
0<fc<n
0<k<n
Therefore by Theorem 0.2.21, V t E [<ToÂ°]oo -^e* = 
Since clearly,
-fr C -fj C -fs, this shows that Xs C <Tri this implies (iv).
(ii) 
and (iii) => (i): By (iii), B X  <T0Â°Â° | Ap, and (ii) is equivalent to 
Xq X X^+i | Xp V n E Wh y  Theorem 7.6.3. Therefore, by Theorem 7.6.4, 
B Â± X t51 X Xâ¢+1 | Xr and, by the same theorem, X$ X (B V Xâ¢+1) \ Xr . 
By Corollary 2.2.11, Xtf X XÂ£+x \ B V Xp and this is equivalent, by TheoÂ­
rem 7.6.3, to saying that x is (B V <Tp)-independent.

426
9. Invariance in Stochastic Processes
Now, by (ii) and (iii),
(B V Xp)[g(xk)] = Xp[g(xk)\ = Xp[g(xo\ = (B V Xp)[g(x0)].
Therefore (ii) and (iii) imply that x is (#VTr)-i.i.d. It is therefore (BVXr)- 
exchangeable (Proposition 9.3.10(h)) and hence ^-exchangeable (ProposiÂ­
tion 9.3.6). 
â 
The mixing condition given in (9.3.15) is in fact an asymptotic conÂ­
ditional independence. It thus seems quite natural to expect that, for a 
B-i.i.d. process, Xp would be exactly estimating and we indeed have the 
following theorem:
9.3.12 Theorem . In the Bayesian experiment S â (A x S, A  V S, II), let 
B C A  and x = {xn : n E IN} be a Bayesian stochastic process. Then
(i) 
x is B-i.i.d. 
if and only if
(ii) 
x is Xp-i.i.d. 
and
(iii) Xp is an Â£ &
-sufficient and exactly estimating statistic.
Moreover , if x is #-i.i.d.,
(iv) BXqÂ° = BXo
(v) 
Xp = X T = X E = BXo = X f B  = BDXâ¢.
Proof. Since (i) implies ^-exchangeability by Proposition 9.3.10(h), (i) imÂ­
plies (ii) and B X Xâ¢ | Xp by Theorem 9.3.11. Now, by Theorem 7.6.5, 
x ^-independent implies Xp C B which implies Xp C B , i.e., Xp is exÂ­
actly estimating. Now x ^-independent implies, by Corollary 7.6.7(i), that

9.3. Standard Bayesian Stochastic Processes
427
BXoÂ°Â° = Vo<n<oo 
x ^-identically distributed implies BXn = BXq. 
This shows (iv). Since Xp C B , Theorems 9.3.11(iv) and (9.3.16) imply (v). 
However, the proof of Theorem 9.3.11 shows that (ii) and B X  XqÂ° \ Xp 
imply that x is (5 V Tp)-i.i.d. It is therefore 5-i.i.d. under Xp C B. 
â 
This theorem shows that if x is B-i.i.d. the minimal sufficient parameter 
BXQÂ° is exactly estimable, i.e., 5ToÂ° C TqÂ° and, in particular, if B is 
identified, i.e., B = BXqÂ° , then B is exactly estimable. Note that, since 
BXQÂ° = 5 To, 5  will be identified by the process if and only if it is identified 
by the first observation.
It is interesting to remark that, by a slight conditional extension of 
Hewitt-Savage Theorem, x B-i.i.d. implies that Xp C B. This result has 
been obtained here without using the techniques of the proof of the Hewitt- 
Savage Theorem.
Let us remark that, comparing Corollary 9.3.3 with Theorem 9.3.11, the 
stronger condition of ^-exchangeability rather than 5 -stationarity merely 
implies a stronger property of the predictive probability, namely, Tp-i.i.d. 
instead of Tp-stationarity. Consequently this cannot greatly simplify the 
conditions under which Xp is exactly estimating. On the contrary, the next 
corollary show that, if x is not 5-i.i.d, Xp is not exactly estimating.
9.3.13 Corollary. Let x = {xn : n E IN} be a ^-exchangeable Bayesian 
stochastic process. Then Xp, is exactly estimating if and only if x is 5-i.i.d.
Proof. As shown in the proof of Theorem 9.3.11, x 5-exchangeable is 
equivalent to x (B V Tp)-i.i.d. Therefore Xp C B implies x 5-i.i.d. and, 
by Theorem 9.3.12, x B-i.i.d. implies Xp C B. 
â 
This corollary shows that a stochastic process which is 5-exchangeable 
but not 5-i.i.d., and for which the minimal sufficient parameter 5T 0Â°Â° is 
exactly estimable (c Tq0), has a minimal sufficient statistic, XqÂ°B = XpB, 
strictly smaller than Xp, i.e., XpB /  Xp. This means that in such a proÂ­
cess one must seek a sufficient and exactly estimating statistic inside Xp, 
since Xp is too large. Therefore, unlike 5-stationary processes, the exÂ­
act estimability of the minimal sufficient parameter of a 5 -exchangeable 
process cannot rely on the mixing Condition (9.3.15) unless it is 5-i.i.d.

428
9. Invariance in Stochastic Processes
Nonetheless, since any ^-exchangeable process is a marginalization of an
i.i.d. process, it is worthwhile to studying these processes in the hope of obÂ­
taining more tractable conditions for the exact estimability of the minimal 
sufficient parameter.
9.3.14 D efinition. In the Bayesian experiment S = (A x 5, A  V 5, II), a 
Bayesian stochastic process x = {xn : n E IN} with state space (U,U) is a 
5 -generalized i.i.d. process with B C A  with respect to M  C A  V S  if
(i) 
x is Af-i.i.d.
(ii) 
B X X qÂ° | A f. 
-
Note that, from Theorem 9.3.11, if x is 5-exchangeable, then it is a 5- 
generalized i.i.d. process with respect to Xr- However, reproducing the end 
of the proof of Theorem 9.3.11 shows that if a? is a ^-generalized i.i.d. process 
with respect to Af, then it is a (5 V Af)-i.i.d. process and, consequently, a 
5-exchangeable process by Propositions 9.3.6 and 9.3.10(ii).
The advantage of this definition derives from the fact that the experiÂ­
ment Â£bvxÂ£Â° is entirely characterized by the experiment Â£b v m v x 0- ThereÂ­
fore the condition for exact estimability of the minimal sufficient parameter 
relies merely on Â£b v m v x 0> where Af (in contrast with Xr) will generally be 
nonasymptotic. Indeed, we have in fact the following result:
9.3.15 Theorem . Let x = {xn : n E IN} be a 5-generalized i.i.d. process 
with respect to A f. The minimal sufficient parameter is exactly estimable 
if and only if
B ( M X 0) = (MXo)B = Bfl MX^.
Proof. By Proposition 4.7.7, the minimal sufficient parameter will be exÂ­
actly estimable in Â£bvXÂ£Â° if and only if XqÂ°B = BXgÂ°Â°. But Xr is suffiÂ­
cient in Â£bvXÂ£Â°* by Theorem 9.3.11, since x is 5-exchangeable. Therefore, 
by Theorem 4.3.7, X ^ B  = X^B and BXÂ§Â° = BXr . Now by Theorem 
9.3.12, x Af-i.i.d. implies that Xr = M X q. Therefore the minimal suffiÂ­
cient parameter will be exactly estimable, i.e., BXqÂ° C X qÂ° , if and only if 
( M X q)B = B (M X q). It follows from Section 4.7 that this implies that
( M X 0)B = B ( M X 0) = BD M X 0

9.3. Standard Bayesian Stochastic Processes
429
and, by Proposition 4.7.7, this relation is in fact equivalent to
B(MXo) C J4Xo,
and is also equivalent to
( M X 0)B C B. 
m
Let us remark that the condition of exact estimability of ^-generalized
i.i.d. processes with respect to M  does not imply that Xr is exactly estiÂ­
mating but in fact requires that XpB is exactly estimating.
Theorem 9.3.15 leads to the following important corollary:
9.3.16 Corollary. In any marginalization on the parameter space of an 
identified i.i.d. 
process the minimal sufficient parameter is exactly esÂ­
timable. In other words, if x = {xn : n G IN} is a B-i.i.d. process with 
BXo = B, then for any C C 5, CXqÂ° C XqÂ°.
Proof. Since C C 5, Condition (ii) of Definition 9.3.14, i.e., C X XqÂ° | B 
is trivially satisfied, and x is therefore a C-generalized i.i.d. process with 
respect to B. Since B X q =  B, C[BX0] = CB = C C B = BXo. Therefore 
C[BXo] C B X q and the condition of Theorem 9.3.15 is satisfied. 
â 
9.3.3 Moving Average Processes
In time series analysis moving average processes are defined by repreÂ­
senting a stationary process in terms of an i.i.d. process. More precisely, let 
x = {xn : n G IN} be a 5-i.i.d. process with state space (Md,Bd). The staÂ­
tionary process y = {yn : q < n  < oo} with state space (IRd)Bd) is defined 
in terms of x and matrix parameters (Ck ' 1 < k < g}, where Ck G [B\, by
(9.3.27) 
yn = xn + 
CjfcXn-It, 
V n > q .
l <k<q
The study of the exact estimability of such a process does not rely on the 
linear structure (9.3.27). This structure may be extended, for instance, to
(9.3.28)
Vn â / ( c? 
%n â 1) â¢ â¢ â¢ xn-q)-

430
9. Invariance in Stochastic Processes
Note that (9.3.28) implies that Tn C C V X^_q and> therefore,
yn+?+1 JL y ? | B, 
V n > },
i.e., y is a g-dependent process. Thus, in a Bayesian framework, a very 
general notion of moving average process may be considered:
9.3.17 D efinition. In the Bayesian experiment Â£ = (A x 5, *4 V 5,11) 
a Â£>-stationary process y = {yn : n E IN} with state space (U,U) is a 
moving-average process of order q if there exist C C B and a B-i.i.d. process 
x = {xn : n E IN} with state space (T, V) such that
T n C C V * n% , 
V n > g .
This leads to the following theorem:
9.3.18 Theorem . In a moving average of order q 5-stationary process y, 
the minimal sufficient parameter is exactly estimable. Moreover,
5^  = 
= bW  = 3^5  -  B n W  â¢
Proof. By 5 -stationarity (Corollary 9.3.3), we know that T r and T t are 
sufficient statistics and T r C T t- By Definition 9.3.17, it is easily seen 
that T t C f]n(C V TnÂ°Â°). 
Now, since C C 5  and 
X  X0n | 5, by
Theorem 7.6.5, (C V Xn)r C B. Hence T r and T t are exactly estimating 
and the result follows from Section 4.7. 
â 
9.3.4 
Markovian Stationary Processes
The structure of a Markovian process is usually presented as follows. In 
the statistical experiment 5, defined as
(9.3.29) 
Â£ = {(S,S);Pa a E A},
the stochastic process x = {xn : n E IN} defined on (5,5) with values in 
(U,U) is Markovian if:
(9.3.30) 
(* 0")Â« f ( x n+1) = (Xn)a f ( x n+i)
V n E l ,  
V a E i ,  
V /  E [U]+.

9.3. Standard Bayesian Stochastic Processes
431
Recall that, for s Â£ [<S]+ and T  C S ) T as denotes the expectation of s 
conditional on T  with respect to P a. In the Bayesian experiment
(9.3.31) 
8  = { A x S , A V S ,  11} 
this implies that, for any prior probability,
(9.3.32) 
X n + i Â± X t
Recall that such a relation was analyzed in Section 6.4.2. We are now led 
to the following definition:
9.3.19 D efinition. In the Bayesian experiment 8  = (A x S, A  VÂ«S,II), the 
Bayesian stochastic process x = {xn : n Â£ JN} is M-Markovian, where 
M  C A  V S, if:
(9.3.33) 
Xn+1 Â± X
q \ M V X n 
or, equivalently,
(9.3.34) 
X X + ^ X f l M V X n .
For the equivalence of (9.3.33) and (9.3.34) see Definition 6.4.4 or TheÂ­
orem 6.2.2.
It is well known that the stationarity of a Markovian process is equivaÂ­
lent to the stationarity of the transitions combined with an initial condition; 
this fact motivates the definition of the following concept:
9.3.20 D efinition. In the Bayesian experiment 8  = (A x S', *4VtS,II), the
Bayesian stochastic 
process x = {xn : n Â£ IN} is B-homogeneous) where
B C A, if V /  Â£ [U]+ 3h Â£ [B 
such that,
(9.3.35) (B V Xn)(f o xn+i) = h(a,xn) 
a.s.II 
V n Â£ JN. 
â 
In the representation 8  of 8 , defined in Section 9.2.3, and using the 
properties of the Shift operator as seen in Section 9.2.4, (9.3.35) becomes
(9.3.36) 
r
n(8 V Xq ) ( f o  r n ) =  {(Â£? V X 0) t }  o r n
v f e [ l i ] +, v Â« e i .

432
9. Invariance in Stochastic Processes
By Definition 8.2.22(h), this means that
(9.3.37) 
\ B V X 0 
as soon as
(9.3.38) 
T I (fiV A 'o)nl.
For a B-Markovian process, (9.3.36) may actually be extended to any 
t E [A'iÂ°Â°]+ , as shown in the following lemma:
9.3.21 Lem m a. 
Let x = {xn : n E IN} be a B-Markovian process. Then
(i) 
T I T i l B V ^ o  
if and only if
(ii) 
r i T 0Â° Â° | BV^ o .
Proof. By Theorem 0.2.21 it suffices to show that, V n E IN, T I Xtf \ BWX0. 
Now, by Theorem 8.2.23(ii), this is true for n = 1 and, by an induction 
argument, it suffices to show that if it is true for n, then it is true for n-f l .  
By Theorem 8.2.23(iv), T I (Tâ¢ V B) H i. Therefore, since r n+fc = r n o r fc, 
r I Xi I B V To implies that T I Â£n+1 | B V Xn. Indeed, V / E [U]+,
r~k(B V Â£ n)(f Â° Â£n+i 0 r k) = (B V Â£o)(f o x i ) o r n oTk 
V k e IN
and
(B V Xn)(f o xn+i) = ( B v f o K / o x i J o r â .
By Theorem (8.2.30),
* n+i+1 JL *T +* | B V * n+t 
(implied by the Â£?-Markovian property),
r  I X n + 1 | B V  Xn 
and 
n ( B v ^ 0â) n i
is equivalent to
X n + 1 Â±  XÂ£ \ B V  Xn 
and 
r  I Xn+x | BV XÂ£.

9.3. Standard Bayesian Stochastic Processes
433
But, by Theorem 8.2.25, this along with T I X q | B V X q is equivalent to
r i Xq+1 | By x0. 
m
By Theorem 8.2.25, we know that
(9.3.39) 
r  I T0Â°Â° | B,
is equivalent to T I XqÂ° | B V Xq along with
(9.3.40) 
T I X 0 \B,
which means that x is ^-identically distributed (see Definition 9.3.9(i)). 
Therefore a S-Markovian process is ^-stationary if and only if it is B- 
homogeneous and ^-identically distributed. More precisely, we have the 
following lemma:
9.3.22 Lem m a. 
A S-Markovian process x â {xn : n Â£ JN} is ^-stationary 
if and only if it is ^-homogeneous and
(9.3.41) 
B (f o Xl) = B (f o x 0) 
V /  â¬ [U]+.
Proof. Clearly, S-stationarity implies (9.3.41) and B-homogeneity. Now 
we show that (9.3.35) and (9.3.41) imply that x is ^-identically distributed. 
Indeed, by an induction argument,
B (f o *n+1) = B[(B V Xn){f o *n+1)] = B[ft(fl, xn)] 
= B[h(a, x0)] = B[(B V X0)(f o xi)] = B (f o xi),
i.e., if xn is ^-distributed as xq, Â£n+i is ^-distributed as x\. Nowjn 
the representation Â£ of Â£, since Br = B , by Proposition 8.2.13, T I (B H i )  
and therefore, by (8.2._40), T I Xq \ B. By Theorem 8.2.23(iv), this implies 
that 
T I (B V Tq) D X 
and, therefore, 
^-homogeneity is equivalent to 
T I Xi | B V Xq. By Lemma 9.3.21, this is equivalent to T I XqÂ° \ B V Xq 
and, therefore, T I XqÂ° \ B. 
u
The exactly estimating property of Xr for a ^-stationary process has 
been easily demonstrated under the additional structure of ^-independence. 
One way to relax this independence hypothesis is to suppose a ^-Markovian

434
9. Invariance in Stochastic Processes
property where independence is replaced by conditional independence. The 
following theorem represents a crucial step into this direction:
9.3.23 
Theorem . In the Bayesian experiment S = {A x S ,A  V <S,II), let 
B C A  and x = {xn : n Â£ IN} be a Bayesian stochastic process. Then
(0) 
a: is 
Markovian and 5 -stationary 
if and only if
(1) 
x is <Tp-Markovian and Ap-stationary,
(ii) 
Xr is an fgv;^00-sufficient statistic,
(iii) Xr C B y  X q.
Moreover, if x is B-Markovian and ^-stationary,
(iv) V /  6 [^r]+ 3h 6 
such that
f  o x = h(a, xn) a.s. II 
V n G l
which implies, in particular, that
(v) 
Xr C 
H 
B V X n.
0<n< oo
Proof, (o) => (i) (ii) (iii). By Corollary 9.3.3., we know that (o) implies
(ii) and that x is Ap-stationary. To prove that (o) implies (iv) and thus
(iii), note first that, by the Z?-Markov property for any /  E \Ur]oo
(BV Xn)(f o x) = (BV Xn)(f o t" o x) 
= (B V XÂ£) ( f  o r n ox) = (BW X f ) ( f  o x).
Therefore, by Theorem 7.2.2,
(B V Xn)(f o x) ââº /  o x 
a.s.II and in L\.
By Lemma 9.3.21, let h e [B 0 W]oo such that
(6 V Xn) ( f o x )  = h(a,xn) 
a.s.II 
V n G l .

9.3. Standard Bayesian Stochastic Processes
435
Hence, by ^-stationarity, V k G JN and V n > k,
B[\(B V Xn)(f o x) -  f  o x\] 
=  
B[\h(a,xn) ~  f ox\ ]
= 
B[\h(a,(Tn~k o x ) k) ~  f  oTn~k ox\[
=  
B[\h(a,xk) â fox\].
Therefore, by the conditional dominated convergence theorem,
B[\h(a, Xk) â /  o #|] = 0 VfcGW,
i.e., /  o x = h(a, Xk) a.s. n, V k G JN and this implies (v).
Now, since Xp C B V Xn, by the H-Markov property,
X n+ i  JL  X q \ B V  X p \ /  X n .
However, (ii) implies, by Corollary 2.2.11, that
B
Â± X
on \ X r V X n .
By Theorem 2.2.10, these two conditional independence relations are equivÂ­
alent to
( B V X n^ ) Â± X
0n l Xr V X n.
This clearly implies that
* n+1 Â± X Â£ \ X r V X n ,
i.e., x is Ap-Markovian.
(i) 
(ii) (iii) =>> (o). By Corollary 9.3.3., we already know 
that (i) and
(ii) imply that x is 5-stationary. Now, by (iii), for any /  G \Up]oo there
exists h G [B(&U] oo such that f o x  = h(a,x o) a.s. n. But by 5-stationarity
B[\h(a, xn) - f o  x\\ 
= 
B[\h[a, (r n o x)0] -  f  o r n o x\]
-  
B[\h(a,x0) -  f  o x\] = 0.
Therefore, V n E IN, f o x  â h(a,xn) a.s. n  and (iv) is satisfied.
By Proposition 7.6.4, since x Ap-Markovian is equivalent to
X Â£ Â± X â i \ X r V X n, 
V n E W ,

436
9. Invariance in Stochastic Processes
and since (ii) implies B X XqÂ° | Xp V Xn, we have 
XÂ£ Â± X ?+1 A. B \ X r V X n.
This implies in turn that
( 6 V ^ ) l C + 1 l ^ r V ^
and that
XÂ£ Â± X â¢1 \ B V X r V X n.
Since Xr C BV Xn, this is equivalent to Xq X X^_1 \ B V Xny i.e., x is 
Markovian. 
â 
In view of Theorem 9.3.23, it is very easy to find a sufficient condition 
for Xp to be exactly estimating in a ^-Markovian and ^-stationary process:
9.3.24 Proposition. Let x be a ZJ-Markovian and ^-stationary process. 
If ATq || ATi | B , then the minimal sufficient parameter is exactly estimable. 
Moreover,
3 r  = BXâ¢ = W B  = Bn XÂ§*. 
m
The condition Xq || Xi \ B is in fact slightly too strong. Indeed, it means 
that if there exist B , C &  BÂ®U such that 1 #(a, xo) = lc(a,xi) a.s. II, then 
3 E Â£ B such that lÂ£(a,a?o) = 1 js(a) a.s. II. But, by Theorem 9.3.23(iv), 
if A Â£ Xp and if B Â£ B Â® U is such that 1 a â 
(Â«, ^o) a.s. II, then
1a = 1b(a , ^i) a.s. II. Therefore Xp will be included in B if the following 
condition holds: if B Â£ B 0 U is such that 1 b(cl,xq) = l# (a ,Â£ i) a.s. II, 
then there exists E Â£ B such that 1b(ci,xo) = 1 e(^) a.s. II. Equivalently 
Xp C B under the following condition
(9.3.42) 
B Â£ 
(fl V * 0)lB (a,*i) = 1 B(a,x0)
=> {51B(a,a;o)}2 = B lB(a,iCo).
Under regularity assumptions such as the existence of a regular transiÂ­
tion function p(a, u, B) satisfying V a Â£ A, V B Â£ ZY,
(9.3.43)
( B V A âo)Ib (i i) =p(a,Xo,-B).

9.3. Standard Bayesian Stochastic Processes
437
Condition (9.3.42) is the Bayesian equivalent of the fact that V a  E  A ,  
the Markov process x is irreducible in the sense that there do not exist 
two nonempty closed disjoint sets where C  is a closed set if p ( a , u , C )  = 1, 
V u E  C  (this is Doeblinâs condition; see for instance Stout (1974), Section
3.6, or Breiman (1968), 7.3,). More details on Markov chains may be found, 
e.g., in Cohn (1965, 1974), and Orey (1971).
9.3.5 
Autoregressive Moving Average Processes
Autoregressive moving average processes are defined through a represenÂ­
tation of a stationary process in terms of an i.i.d. process. More precisely, 
let x = {xn : n E  IN} be a B-i.i.d. process with state space (JRd,Bd). The 
autoregressive moving average B-stationary process y = {yn : n E  IN} of 
order (p, q) with state space (lRd,Bd) is defined through x and matrix paÂ­
rameters { A k  : 1 < k < p )  and {Bt : 1 < Â£ < q} , where A k G [B] and 
Bt G [B], by
(9.3.44) 
yn = ~ T ,  Akyn- k +  ^
 Bt xn. i  + xn.
1<k<p 
i<*<Â«
Once again, this linear structure is irrelevant for the study of exact estimaÂ­
bility and we may consider the extended structure
(9.3.45) 
yn = /(c, yn â 1 > Vnâ2> â¢ â¢ â¢ J Vnâp j %n > 
â1 ? â¢ â¢ â¢ > %n â ?)â¢
Note that (9.3.45) implies that y n C C V 
V
This leads to a very general notion of autoregressive moving average 
process in a Bayesian experiment.
9.3.25 D efinition. In the Bayesian experiment Â£ = (A x 5, A  VB,II) a 
B-stationary process y = {yn : n E  IN} with state space (U,U) is an auÂ­
toregressive moving average process of order (p, q) if there exist C C B and 
a B-i.i.d. process x = {xn : n E IN} with state space (V, V) such that the 
process {(pn,^n) â¢ u E  IN} with state space (U x V,U 0  V) is B-stationary 
and such that
J ' n C C V y âIp V Xn-q, 
V n > r = max(p,q).
This leads to the following theorem.

438
9. Invariance in Stochastic Processes
9.3.26 
Theorem . In an autoregressive moving average S-stationary proÂ­
cess y of order (p, <j), the minimal sufficient parameter is exactly estimable 
if:
(9.3.46) 
y;zlp 
JL xzÂ°\B\tx;zl
(9.3.47) 
(y;zl V xrTz\ ) 
II 
(yl;~4 v x%z\) \ b ,
and, moreover,
(9.3.48) 
JF  = B W  = W~B =
Proof. Let us first show that under (9.3.46), V n > r xn is independent of 
y?Ip V ^r-q conditionally on B. Indeed, since a: is a 5-i.i.d. process,
(9.3.49) 
JL XrÂ°Â° | B,
and, by Theorem 2.2.10, (9.3.46) and (9.3.49) are equivalent to
(9.3.50) 
XrÂ°Â° JL ( XrrZ lq V y ^ p )  | B.
Since XÂ£Â° JL T'rnâ 1 | B, we also have, by Theorem 7.6.4,
(9.3.51) 
* nÂ°Â° X x r 1 JL (*;_-,1Vy;zl) IB,
and so
(9.3.52) 
X ?  X ( y TrZ lp V X?Iql VC) \ B
by Corollary 2.2.11, since C C B. But, by a recurrence argument, 
y n C CV y^Zp V XÂ£_t implies that
(9.3.53) 
^ - ' c C V y rrZp V X"Z~q .
Therefore,
(9.3.54) 
x nÂ°Â° x  (y?z} V X?y}) IB.
Now let us consider the process z = {zn : n > r â 1} defined by
(9.3.55) 
Zn â (yn, yn â i , . . . , 2/nâp-h 1> 
> %n â 1> â¢ â¢ â¢ 5 ^n â q+1)*

9.3. Standard Bayesian Stochastic Processes
439
This process is ^-stationary since {(yn, xn) : n E IN} is S-stationary. Clearly, 
in our usual notation,
(9.3.56) 
2 n 
= yâ% +i V ^ . J+1)
and so
(9.3.57) 
= y"Zp V X?Iq â 
However, since y â c C V y^Zp V XÂ£_q,
(9.3.58) 
= y ^ + 1 V XZ_ , + 1 v y n c c v y ^ i p  v
v
x n.
Therefore,
(9.3.59) 
Â£ n c C V Z n - x  V 4
But, by (9.3.54),
(9.3.60) 
Xn Â±  Z?Zi | H, 
and so, since C C B, by Corollary 2.2.11
(9.3.61) 
(C V Zn_! V Xn) JL Z ? ll | H V Zn_L 
Using (9.3.59), we obtain
(9.3.62) 
Â± Z ? I }  | HV2â_1.
The process z is therefore S-Markovian and ^-stationary. By Theorem
9.3.23 (v), Zp will be exactly estimating (i.e., Zp C B) if
(9.3.63) 
BV 2 r_i fl 6 V 4 - 1  = B,
and, in view of (9.3.57), this is precisely (9.3.47) . To complete the proof, 
it suffices to note that
(9.3.64) 
y p V X p C Z p
since, for instance, the process y may be seen as a function of the process 
z which commutes with the properly defined shift operators. Therefore, Tr 
is exactly estimating. 
â 

440
9. Invariance in Stochastic Processes
Before concluding this section, it is worthwhile commenting on condiÂ­
tions (9.3.46) and (9.3.47). Condition (9.3.47) is a very mild measurability 
assumption since, heuristically, it says that if / ( u , ^ r-i) = d{a,Z2r - 1) 
a.s. II, then / ( a , z r_i) = h (a) a.s. II. Condition (9.3.46) is in fact a very 
natural initial condition. Indeed, when the processes are indexed by Z, as 
is usually done in time series analysis, (9.3.46) is trivially satisfied under 
the commonly invoked assumption that
(9.3.65) 
C C V ^ .
9.3.6 
An Example
We now provide an example which illustrates many of the concepts 
expanded in Chapters 7 and 9.
Let A = IRq and A  = Bq = B fl Mq . Take a prior probability /i 
on (A, A) which admits a strictly positive density on A with respect to 
Lebesgue measure. Let m be a real random variable with M  â cr (m) such 
that
(9.3.66) 
M  1  
A
and
(9.3.67) 
m -  
N  (0,1).
Consider x = {xn : n Â£ IN} a real valued (A V A4)-i.i.d. stochastic proÂ­
cess normally distributed with expectation m and 
variance a, i.e.,
(9.3.68) 
Â±  
Xn \ A V M
0<n <oo
(9.3.69) 
(xn | a, m) ~  N  (m, a ) .
In this context, according to Sections 2.3.7 and 5.5.4, fi is a regular 
probability, and so Bayesian and sampling theory concepts of sufficiency and 
completion are equivalent. Remark also that, for the experiment Sa v m v x *, 
there exists IIo ~  II such that
(9.3.70) 
A 1  M  !  T0n; n 0 
Vn Â£ N.
This fact will prove useful in establishing measurable separability using 
Corollary 5.2.11.

9.3. Standard Bayesian Stochastic Processes
441
We now analyze the Bayesian experiments Â£(a v m )vxgÂ° > Â£m v XÂ£Â° > 
x â¢ >
and Â£a v x Â°Â° â¢
A. The Experiment Â£ ( a
v
m
) v X Â£ Â° *
If we define
(9.3.71) 
un 
= 
â  - â  
^  ^ 
= ^(Wn)
0<fc<n
(9.3.72) 
= 
- l y  
( * * - wÂ«)2 > 
Vn = <r(un)
0<ife<n
then, by standard results, i/â VVn is sufficient and 1-complete in 
ivA'" -
i.e.,
(9.3.73) 
(.4 V M )  Â±  XÂ£ | Un V Vn
(9.3.74) 
( W â W â ) < i ( ^ V A 4 ) ,  
and is therefore minimal sufficient, i.e.,
(9.3.75) 
XÂ£ ( A V M )  = (Un V Vn) H XÂ£.
Note also that 
is measurably separated and identified, i.e.,
(9.3.76) 
(A V M )  || Xq
(9.3.77) 
( A V M ) X g  = ( A v M ) X 0 = A V  M .
Indeed, (*4VA4)xo =  m 
and 
(,4V.M )xo =  m2 +  a. 
Therefore 
( A V M ) C ( A V M ) X 0.
Let us also recall that
(9.3.78) 
Un Â±. Vn \ A V  M
(9.3.79) 
(un | a, m) 
~  
IV (m , ^ y )
(9.3.80) 
(vâ | a, m) 
~
i.e., the chi-square distribution with n degrees of freedom. Therefore
(9.3.81) 
Vn 1  M  \A,

442
9. Invariance in Stochastic Processes
and by Theorem 2.2.10
(9.3.82) 
Vâ JL ( M VUn) \ A.
In Â£(a v m )v x~, since x is (A V A4)-i.i.d. and Â£(Av m )vx, oo is identified, by 
Theorem 9.3.12, Xp is sufficient and exactly estimating, i.e.,
(9.3.83) 
Th = * t = 
= A V M ,
and therefore
(9.3.84) 
X 0oo( A V M )  = X ^ n X 0Â°o.
Clearly, Xp is 1-complete, i.e.,
(9.3.85) 
Xr < i  ( A V M ) .
According to Theorem 7.3.6, XfiXp is Â£(AvM)vxg-minimal sufficient. 
This is clear since
(9.3.86) 
XÂ£Xr = XÂ£ ( A V M )  = (Un V Vâ) n X Â£ .
Note that, in this case, X q Xp is also 1-complete in Â£^a v m )v x *-
Now, by Proposition 7.3.4, (Un V Vn)T is also Â£(a v m )v x 00-sufficient 
and, by (7.3.1),
X g Â° { A V M )  C (Un V Vn)T,
and therefore
(9.3.87) 
A V M  C (Un W n)T.
Note that this inclusion can also be deduced from the strong law of large 
numbers, since un â âº m a.s.II and vn â âº a a.s. II. However, un and 
vn are invariant under any permutation of the (n + l)-first coordinates; 
consequently,
(9.3.88) 
(Un W n)T C T S,
and so
(9.3.89) 
(Un V Vn)T = A V M .
In this situation the tail <r-field of a sufficient sequence of 2-complete statisÂ­
tics is minimal asymptotically sufficient, i.e.,
(9.3.90) 
T0Â°Â° ( A V M )  = p | (J  X ^ ( A \ t M ) .
n m>n

9.3. Standard Bayesian Stochastic Processes
443
Note that, if
(9.3.91) 
yn = x n -  m,
then
(9.3.92) 
M  Â±  y ?  I A,
and y is an .4-i.i.d. process. Note also that SAvyâ¢ is identified, i.e.,
(9.3.93) 
A y ?  = A y 0 = A .
Indeed, Ay% = a. Therefore, by Theorem 9.3.12,
(9.3.94) 
y r = y T = y ^  = A  = y ? A .
Now, considering the definition of yn and vn, vn Â£ [y$] and vn is clearly 
invariant under permutations of the first (n -f l)-coordinates of the process 
y. Therefore Vt C 
and so Vt C A . Now, A  C Vt since vn â âº a 
a.s. II, and thus
(9.3.95) 
VV = A.
B. The Experiment E ^ v x Â®Â°*
In this experiment (with known variance) Un is sufficient and 1-complete 
m ^Mvxgy i-e->
(9.3.96) 
M  1. X q \ A y  Un
(9.3.97) 
U n ^ M  | A.
Un is therefore minimal sufficient, i.e.,
(9.3.98) 
(A V Xq) ( A V M )  = (A V Un) n(*4V X Â£ ) .
By Corollary 5.2.11, E^vx* is measurably separated, i.e.,
(9.3.99) 
M  || *on | A.
By Proposition 4.6.6, Sjtivxg *s a*so identified, i.e.,
(9.3.100) 
( A V M ) ( A V X S )  = A V M .
This is straightforward to verify since
(.4 V M ) ( A V  X^) = A  V {(>1V M )  Xg} = A  V M .

444
9. Invariance in Stochastic Processes
Now, recalling (9.3.81), Vn JL M  | A, Vn is Â£jtivx{ oo ancillary and, by
Basu First Theorem (5.5.9), we obtain
(9.3.101) 
Vn X  ( M V U n) \A,
which is (9.3.82), and a fundamental property of normal populations; conÂ­
sequently, this result may be viewed as a corollary of Basu First Theorem.
^m v x ( oo ) the process "((cz, 2?^) . ti G jCV) is (Â»4 V Â«A<4)âi.i.d. Since Â£
xÂ°Â° 
is identified, by Theorem (9.3.12) we know that
(9.3.102) 
( A V X n)r 
= 
( A V X n)T = ( A V  Xn)^ 
= 
A V M  = (A V  X ^ ) ( A V  M) .
By (9.3.83), Xt = A  V M .  Therefore
(9.3.103) 
(,A V X n)T = A  V XT = A V M .
Hence Xt is minimal sufficient and 1-complete in Â£jkivxgÂ°' By Theorem
7.3.6, 
(A VXÂ£)(A VX t ) is Â£jtivxn~minimal sufficient and is also 1-complete 
since, by (9.3.98),
(9.3.104) 
(A V  X ^ ) ( A V  XT) = A V U n.
By Proposition 7.3.4 and expression (7.3.1),
(9.3.105) 
{AV X ^ )  ( A y  M )  C ( A v U n)T .
However, un is invariant under any permutation of the (n + 1) first coordiÂ­
nates, so (A VUn)T C (A V Xn)^. Therefore, by (9.3.102),
(9.3.106) 
(A V Un)T = A V M .
In this experiment it is also the case that the tail (7-field of a 1-complete minÂ­
imal sufficient sequence is the 1-complete minimal asymptotically sufficient 
statistic, i.e.,
(9.3.107) 
( A V X Â£ Â° ) ( A \ / M )  = f) (J M v X$) (A V M) .
n k>n
Note that, from (9.3.106), we obtain that Ut C A  V M .  By the strong law of 
large numbers, un ââº m a.s. n, and (1 / n  4- i + 1) J2o<k<n x!+ k
m 2 -f a

9.3. Standard Bayesian Stochastic Processes
445
a.s.II V i G IN . But Xi+k = (* + k + 1)Ui+k â (i-f fc) u*+fc-i. Hence 
M  C Ut and A  C UfZ1 
Vi â¬ JV. Therefore
(9.3.108) 
Th = X vA ?,
and
(9.3.109) 
G4VWn)T = ,4'v Wt = -4VAf.
C. The Experiment Â£ % l x g Â° '
This experiment is very similar to the experiment Â£jtivxÂ°Â° â¢ 
whereas 
in Â£jttvxÂ°Â° 
minimal sufficient statistic Un was uniform, i.e., Un C X q , 
this is not the case in S f y Xn. Indeed, if we define
(9.3.110) 
tn = â
(x k - m f  ,Tn = cr(tn),
n 
0<k<n
then Tn is sufficient and 1-complete in Sfyxg *e*>
(9.3.111) 
A J L X Z  \ M  VTn,
(9.3.112) 
TnC iA  I M ,
but Tn is not uniform.
Let us recall that
(9.3.113) 
tn =  vn + (Â«Â« -  rn) 2 ,
and
(9.3.114) 
(tn | a,m) ~  ~^~rxl+i-
n + 1
This implies that
(9.3.115) 
M  JL Tn | A,
and shows that Tn is 1-complete. Just 
as in the above section, 
Â« is
measurably separated and identified, and Tn is minimal sufficient, i.e.,
(9.3.116) 
A  || Xq | M ,
(9.3.117) 
( A V M ) ( M V X Â£ )  = A V M ,
(9.3.118) 
( M  V T0") {A V M )  = ( M V Tn) n ( M V  T0n) ,

446
9. Invariance in Stochastic Processes
and in E fyXoo, Xt is minimal sufficient and exactly estimating (and thereÂ­
fore 1-complete), i.e.,
(9.3.119) 
(.M  V Xn)T = M  V Xt = A V  M =  ( M V  XgÂ°) (A V M ).
Now,
(9.3.120) 
(M  V X$) (M  V XT) = M  VTn 
and
(9.3.121) 
(M  VTn)T = J v T d .
But by (9.3.91) and (9.3.94), Tn is minimal sufficient and 1-complete in 
^Avy*- So
(9.3.122) 
7 t = 34,
and, therefore,
(9.3.123) 
(.M V T n)T = A4 VTt .
D. The Experiment
This experiment presents far more pathological features than the three 
preceding ones, and illustrates results obtained in Chapter 9. It also proÂ­
vides counterexamples to false conjonctures which might have been adÂ­
vanced in Chapter 7.
In Â£a vXÂ£, by standard computations,
(9.3.124) 
((zo,Â®i, â¢ â¢â¢ ,x n)' | a) ~  Nn+i (0,al+ ee')
where I is the identity matrix, and e = (1,1,..., l / .  The density function 
only depends on
(ar0, *i, â¢ â¢ â¢, *n) (ai +  ee')_1 (x0 , xi,. .. ,  xn)'
= f a o , x i , . . . , x n) ( I -  â ^ p [tel)(x0 , x l , . . . , x ny
-  sÂ± ifv 
l u2l 
,(n+1)2 .u2
-  a (Vn -t- Uâ) 
a(a+n + i) Un
-  2Â± Â± v 4. _J2Â±1_U2 
~  a 
n â a+n+1un>
where vn and un are given in (9.3.71) and (9.3.72).
If >Vn = a (ul) C Un then, according to Section 4.4.4, Vn V Wn is 
minimal sufficient, i.e.,
(9.3.125) 
A JL Xq | Vn VWn.

9.3. Standard Bayesian Stochastic Processes
447
&Avxn is measurably separated and identified, i.e.,
(9.3.126) 
A  || T0n
and
(9.3.127) 
A X on = A X 0 = A.
Indeed, A xl = 1 + a.
From (9.3.82), (9.3.79) and (9.3.80) we deduce that:
(9.3.128)
(9.3.129)
(9.3.130)
Therefore,
(9.3.131)
Note also that
Vn 
X  
Wn | A
(vn I a)
(Â«n | a)
N 
0;
Xn
a -|- n -{â  1 
n +  1
( Â« n l a )
a -f n 4* 1 o
â  -i 
Xi-
n -f 1
(9.3.132) 
(u l\a ,m )
n +  1
N
n + 1m, 1
01 
o
n -f 1
(n -f 1) m21
which is a non central chi-square distribution.
Consequently, if
(9.3.133) 
N  = a {m2} C M ,
then
(9.3.134) 
Wn JLM \AvAf.
From (9.3.129) and (9.3.131) we deduce that Vn V Wn is not complete,
since
(9.3.135) A  [vn -  n (u2n -  l)] =
a +  n +  1
n â n |  
 
 
1 J = 0.
n + 1
Now, in Savx, oo the process x is ^-exchangeable, according to Definition
9.3.5. 
By Theorem 9.3.11, Tp = Xt = Ts, z is Tp-i.i.d., and Tp is Â£a v x â¢- 
sufhcient. Now, by Corollary 9.3.13, since x is not .4-i.i.d., Tp is not exactly

448
9. Invariance in Stochastic Processes
estimating and we must search for a smaller sufficient and exactly estimating 
statistic.
Now, by Definition 9.3.14, x is an *4-generalized i.i.d. process with 
respect to AVM. and is also a marginalization on the parameter space since x 
is *4vA4-i.i.d. Recall that Â£(avM)vx%Â° and Â£avXÂ£Â° are identified. Therefore, 
by Corollary 9.3.16, A  is exactly estimable, i.e., A  C XÂ§Â°. More precisely, 
since Xr â A  V M , by Theorem 9.3.15, we have XÂ§Â°A = AXÂ§Â° = A. Now, 
by (9.3.95), Vt = A, and so Vt is sufficient and exactly estimable and 
therefore 1-complete. By Theorem 7.3.6,
Xq Vt = (VâVW â)nl0â ,
and this shows that the projection on Xq of a complete asymptotically 
sufficient statistic is not complete, although it is minimal sufficient.
Finally, by the strong law of large numbers in Â£(AvM)vxgÂ° Â» un ~ > m2 
a.s. II as n â> oo. It is then also true in Â£a vXÂ£Â°- Therefore, using (9.3.89), 
we have the following sequence of results:
(9.3.136) VV = A c A V 7 7 c (Vn V Wn)T c (V n V U n )T = A V M .
This shows that, in Sa v xâ¢,
(9.3.137) 
^ 4  ^  f l  U
n m>n
i.e., the minimal asymptotically sufficient statistic is strictly included in the 
tail of the sequence of the minimal sufficient statistics.
9.4 
Conditional Stochastic Processes
9.4.1 Introduction
A useful motivation for this section is the analysis of the regression 
model yn = f( z n ia) + en where, conditionally on z, the eâs are i.i.d. (with 
0 expectation). We shall say that the sampling process generating yn is 
conditionally independent and stationary, in a sense to be made precise.
It is well known that the existence of consistent estimators for a in 
such a (nonlinear) regression model requires more than assumptions on the

9.4. Conditional Stochastic Processes
449
conditional distributions generating (y | z)\ it also imposes restrictions on 
the behaviour of the z-process. In the linear case:
convergence of the least squares (or maximum likelihood) estimator of a 
is generally obtained by assuming the convergence of n~ 1Z /nZn (where 
Zn â [zti], 1 < t < n, 1 < i < k).
A preliminary question concerns the nature of zn. Sometimes, zn is 
considered as ânonrandomâ, as in the analysis of variance; in other cases, it 
may be more natural to view the znâs as having been randomly generated 
by some unspecified process. Thus, in the linear model, the convergence of 
n~ 1Z'nZn may be as a sequence of either nonrandom or random matrices, 
and it is easily shown that the convergence of the least squares estimator 
will be either in probability or almost sure according to the type of converÂ­
gence of n~l Z'nZn. When /  is not linear and zn is nonrandom, Malinvaud 
[1970] proposes an assumption on the asymptotic behaviour of the empirical 
distribution function. More recently, Burguete, Gallant, and Souza [1982] 
proposed for a random zn, an assumption on the convergence of the sequence 
n~l 
fÂ°r everY ^ dominated by a given integrable function.
Note that a trivial extension for the random regressor case would be to asÂ­
sume that zn and en are independent, both i.i.d., and that the distribution 
of zn is characterized by parameters a priori independent of both a and the 
parameter characterizing the distribution of en \ it is then easy to show that, 
in this context, (yn,zn) are jointly i.i.d. and the estimation of a (i.e., the 
maximum likelihood estimator or posterior expectation) is the same in the 
joint and the conditional models. Since the model is embedded in an i.i.d. 
framework, identification is a sufficient condition for exact estimability, and 
the assumption of convergence of n~ 1Z'nZn or of n-1Di<;<nv?(Â£*, Z{) is a 
consequence of the law of large numbers.
It should be noted that the assumptions that zn is i.i.d. and that there 
is a cut (prior independence of the parameters), are very constraining; this 
section seeks to obtain consistency of posterior expectations from weaker 
assumptions. The main results of this section may heuristically be viewed as 
follows: in an identified conditional i.i.d. process, a condition of stationarity 
on the conditioning process is a sufficient condition for the convergence of
l<i<k

450
9. Invariance in Stochastic Processes
posterior expectations.
In Chapter 7, Section 7.7.3, this problem was treated using an âarbiÂ­
trary time originâ , i.e., a tail-sufficiency condition; heuristically, this means 
that the posterior expectation of an arbitrary integrable function of the 
parameter given all the process {xn : n E IN} is not affected by deleting 
the first k observations, i.e., is equal to the posterior expectation given 
{xn : k < n < oo}. As shown in Theorem 7.7.2, this property is equivalent 
to the sufficiency of the tail (7-field. In this section we analyze this probÂ­
lem using the concept of the conditional invariance for the shift operator as 
studied in Chapter 8, since it was shown in Section 9.3 that shift invariance 
implies the tail-sufficiency property.
In Chapter 7, a first kind of result, namely, Theorems 7.7.18 and 7.7.19, 
was obtained, based on the weaker assumption of the tail-sufficiency propÂ­
erty; however, these results relied on asymptotic assumptions. The results 
of this section will be less general since based on the stronger assumption of 
shift invariance, but they make use of assumptions verifiable on finite sample 
size experiments. Note that for the marginal process generating zn, that is, 
the conditioning variable, we do not assume ergodicity but merely stationÂ­
arity. This feature leads to results for the conditional models different from 
those for the joint models. It may indeed be shown that if the joint proÂ­
cess generating xn = (yn ,zn) is stationary and ergodic, then the identified 
parameters are exactly estimable. In this section we show that if the joint 
process is stationary (which is implied by the stationarity of (yn | zn) and 
of (zn)), and if the conditional model generating (yn | zn) is ergodic, then 
its identified parameters are exactly estimable. In this section, the ergodÂ­
icity of the conditional model is obtained through an assumption of condiÂ­
tional (mutual) independence. It is natural to obtain conditional ergodicity 
through a weaker conditional mixing assumption. Such an approach is the 
natural framework for analyzing models such as yn = f ( y n- i, zn, a) -f en.
Another feature of our results concerns the treatment of nuisance paÂ­
rameters. In a Bayesian framework it is natural to integrate out nuisance 
parameters, and to then analyze the experiments marginalized on the space 
(parameters of interest x observations). In the main theorem, a general sitÂ­
uation is examined, whereas in its corollary it is assumed that the parameÂ­
ter of interest is a sufficient parameter for the conditional sampling process.

9.4. Conditional Stochastic Processes
451
Our results, however, do not depend on whether or not the marginal process 
provides information about the parameter of interest. This is so because we 
are concerned with consistency but not with asymptotic efficiency â our 
results implicitly assume a complete specification of the joint model. Thus a 
supplementary assumption of a cut or of mutual ancillarity is not necessary 
for obtaining exact estimability, although such an assumption would allow 
one to simplify the specification and, in general, obtain more robust results.
9.4.2 
Shift in Conditional Stochastic Processes
In the Bayesian experiment S = (A x 5, *4V5, II) the Bayesian stochastic 
process x = {xn : n Â£ JN} defined on (5,5) with values in the measurable 
space (U,U) is decomposed into
(9.4.1) 
xn = (yn,Zn),
where yn and zn are valued in (V, V) and (IT, W) respectively. Thus
(9.4.2) 
U = V x W  and U = V Â® W.
The sub-cr-fields generated by this process are
(9.4.3) 
= y-^V )
and
(9.4.4) 
Â£â = z ~ \W ) 
so that
(9.4.5) 
Xn = y n V Zn = x - \U ) .
Other related cr-fields are defined as in Section 9.2.3. As in that section, 
it is useful to introduce the coordinate process defined on the representation 
space S = (A x 5, A  V 5, II) by
(9.4.6) 
yn(u) = 
vâ
(9.4.7) 
zn(u) = 
wn
since un = (vn, wn).

452
9. Invariance in Stochastic Processes
To this coordinate process we associate the sub-cr-fields of *4VtS defined
by
(9.4.8)
I ?
II
and
(9.4.9)
= 
^ ( W )
so that
(9.4.10)
*n = y n VZn
-
 s-W
As in Section 9.2.4, if the shift operator acts by composition on the stochasÂ­
tic processes y and z as
(9.4.11) 
(rk o y)n 
= 
yn+k
(Tk o z)n 
= 
zn+k
it may be defined as acting on the representation space (A x S, A  V S) in 
such a way that
Vn Â° T k
â 
Vn-\-k
Zn O T
â 
z n + k
a o  r k
â 
a.
(9.4.12)
This permits the basic sub-cr-fields to be represented as:
(9.4.13) 
Â£n 
-  
T ~ n ( y 0 ) ,
Zn 
=  
T ~ n ( Z 0 ).
Clearly,
(9.4.14) 
Br 
= 
B 
V 6 C  X
Finally, let us recall that
(9.4.15) 
S â 
â¢
9.4.3 
Conditional Shift-Invariance
In the Bayesian experiment S = (A x 5, A  V <S, II), let B C A  be a 
subparameter and let x be a Bayesian stochastic process as defined in SecÂ­
tion 9.4.2. Consider the representation of the experiment S marginalized on 
B V X qÂ° , i.e.,
(9.4.16) 
= {,4 x Um ,B V

9.4. Conditional Stochastic Processes
453
and let us recall that Â£gvxÂ°Â° *s samP^nS shift-invariant if x is ^-stationary 
or, equivalently,
(9.4.17) 
r  I T0Â°Â° | B.
Since Br = B , this is in fact equivalent to saying, by Theorem 8.2.28, that 
^ bvxÂ°Â° 1S shift-iiivariant, i.e.,
(9.4.18) 
T I ( B v f 0Â°Â°).
As shown in Section 9.3.1, (9.4.17) is actually equivalent to
(9.4.19) 
r  I T0n | B 
V n G f ,
i.e., Zftyxoo is sampling shift-invariant if and only if Â£gvx n ls sampiing 
shift-invariant V n E IN; this means that an asymptotic property of shift- 
invariance is in fact equivalent to a finite horizon property of shift invariance. 
We now consider a less general version of Proposition 8.3.3, more suited to 
the present analysis.
9.4.1 Proposition.
(i) 
Â£&vx n 1S (sampling) shift-invariant if and only if Â£ ^ ^ n 
are (sampling) shift-in variant.
(ii) 
Â£gwxÂ°Â° *s (samPling) shift-invariant if and only if 
are (sampling) shift-invariant.
Let us recall the definitions of sampling shift-invariance conditionally 
on the z-process. Â£gÂ°~n *s sampling shift-invariant if
(9.4.20) 
r  I 5?o | B V  Z q ,
i.e., T I (B V Z q) fl J ,  and V l : ,n E l ,
(9.4.21) T~k( S V -2g)(Fo r k) 
= 
{(6 V Z%jt) o r k 
V t e  [5#]+ ,
and Si
and Si
or, equivalently, if
(9.4.22) (i?vÂ£]*+*) (Tot*) 
= 
{ ( Â§ V Z Z ) t } o Tk 
V Te [^o ] + -

454
9. Invariance in Stochastic Processes
Similarly, 
is sampling shift invariant if
Bvy q
(9.4.23) 
T i y S Â°  
i.e., r I ( b v Â£0Â°Â°) ni and V k Â£ IN,
(9.4.24) ( Â£ v Â£ Â£ Â° )  ( t o Tk) = { ( Â§ v Â£ $ Â° ) t } o r *  
V f â¬
^ '  +
Let us note that, contrary to the marginal experiment 
if
BV Xâ¢
?Zq
is shift invariant, i.e., V I ^  V }^0nJ | Z q , this is not equivalent to
^Bvyn samP^nÂ§ 
invariant, i.e., T I To I Â® V 
, since it also requires
^ z n 
.
.
.
.
.
 
^ 
^
k0 PriÂ°r-S^i^ invariant, i.e., T I B \ Zq , by Proposition 8.3.3 (ii).
This is, however, a consequence of Â£gv^n 
sampling shift invariant, i.e.,
T I Zq \ B. But it remains true that asymptotic sampling shift invariance 
is implied by a finite horizon sampling shift invariance in the experiment 
conditioned on the z-process. More precisely, we have the following theorem:
9.4.2 
T h eo rem .
(i) 
If T I (jS V ZqÂ°^ fl J, and if 
is sampling shift invariant Vn Â£ IN,
^.700
then 
is sampling shift invariant.
&vyoo 
r
(ii) 
If ( z n^ is B-allocated to 
and if 
^
 is sampling shift invari-
^ z n 
.
ant, then Â£^Â° ^  is sampling shift invariant Vn Â£ IN.
Â£vy0n 
r
P roof.
(i) 
We have to show that (9.4.22) implies (9.4.24).
For t Â£ |To] 
as n ââº 00, ^ V 2 fen+fcj (t o r k) ââº (jB V Z ^ j  (t o r k) 
and (jSVZQ^jt ââº ( j S V Z ^ t  a.s. 
(and in L\) by the martingale
convergence theorem, Theorem 7.7.2. 
Since 
T 
I 
(j3 V Z qÂ°^ H i ,

9.4. Conditional Stochastic Processes
455
^(j3 V Z q^J 
Â° r k ââº 
V Z q*^J 
o r k a.s. Therefore (9.2.24) holds
V t E To 
V t  Â£ W and, by Theorem 0.2.21, (9.2.24) holds V t Â£ T,
oo
'0
(ii) If (Zn'j is ^-allocated to 
and if t Â£ j^o ] 
>
( Â§VZ%Â°)  ( t o r k) = 
(tfV 
(fo r* )
^ B V Z ^ t  = 
( j SV Z^ t.
Therefore, since T I [b  V Z qÂ°^ DJ, (9.4.24) implies (9.4.22). Hence (9.4.23)
implies (9.4.20). 
â 
As recalled in the introduction, the ergodicity of 
is obtained
J 
Bvyâ¢
^ 2  00
through the usual assumption that 
is conditionally independent or
equivalently that the process (Tn) is #-sifted by (Zn), i.e.,
(9.4.25) 
Â±  
y n \ B V Z â¢
0<n<oo
along with
(9.4.26) 
y n JL Z%Â° | B V Zn Vn Â£ IN.
Recall that, by Theorem 7.6.10, this asymptotic assumption is equivÂ­
alent to a sequence of finite sample assumptions and that it implies that 
^Zn'j is ^-allocated to ^Tn) , i-c.,
(9.4.27) 
T/ Â±  ZÂ£Â° | B V  Zf VI C M.
^ 2  00
The assumption that 
is conditionally independent permits the
tswy q
^ZÂ°O
characterization of the sampling shift invariance of 
^  
to be simplified
0 
Bvy%Â°
as is shown in the following theorem:
âzÂ°Â° 
â¢ 
^z00
9.4.3 Theorem . If 
is conditionally independent, then Â£ ^Â°^ 
is
t3vy%Â° 
r  
Bvy%Â°
sampling shift invariant if and only if 
j- *s sampling shift invariant and
n ( b v z z Â°) ni.

456
9. Invariance in Stochastic Processes
Proof. By Theorem 9.4.2 (ii) and (9.4.27) the condition is necessary. By
Theorem 9.4.2 (i), the condition will also be sufficient if it implies that 
^ z n
^Â£vyn lS samP^nS 
invariant Vn E IN. Thus we have to prove that 
r  I Â£o I Â£ v  Z 0 and T I (#V  f 0Â°Â°) n i  implies r  I Stf | 6 v f j V n e J V .  
Since r  I 
2 â ) fl I  implies that T I 
V ZftJ fl I  by Proposition
8.2.12, it suffices to prove (9.4.22). Let f m  E [V]+ ,0 < m < n. Then for 
any k,n E IN:
( f i  V 
j  [r io < m < n  $m 0
â { p  V ZoÂ°^ [rio<m<n f m  Â° Vm+k
.0 <m<n
by (9.4.27)
by (9.4.25)
by (9.4.26)
Now, if r  I 3^o | B  V Z o , then
(^BVZm+kj [f m OJ/m + t] 
=  
( p  V Z 0)  [fm Â° So] Â° T m+k
=  
{ ( i ? v Â£ o )  [ f m Â° y o ] Â° T m } o r k
( p v  Z k +kSj |rio<m<n fm 0 Vm+k 
IIo<m<Â» {p V Zm) ifm Â° Vm\ Â° Tk 
{n0<m<â (pVZm'j [f mÂ°ym] } Â° r k
0 <m<n
{ ip  V z s )  [ n 0< m < â  f m  O Vm] }  o T k

9.4. Conditional Stochastic Processes
457
since T I ( Â§  V Â£ j )  Cl I . By Theorem 0.2.21, V t e  [5>0n] + ,
^ o r â ) = 
Â£?)<â } or*,
i.e., T I To I B V 
.
We can now state the main result of this section which, as indicated in 
the introduction, is analogous to Theorem 7.7.18. Although the assumptions 
used here are stronger than those underlying Theorem 7.7.18, they provide 
the same result: exact estimability of the parameter, but with the added 
feature that these assumptions are finite sample properties only. Since we 
are dealing with shift invariance, these assumptions provided more natural 
characterization of invariant cr-fields while Theorem 7.7.18 characterized tail 
cr-fields.
9.4.4 Theorem . Let Â£g^ ^  be a representation of a Bayesian experiment 
Â£a vXÂ£Â° marginalized on a subparameter B C A. Let us suppose that
(i) 
is sampling shift invariant Vn Â£ IN;
â^7 oo
(ii) 
is conditionally independent;
(iii) Â£%Â°^ is sampling shift invariant; 
v  
7 
Bvy0 
^ 
&  
â
(iv) Â£ ^Â°^ 
is identified;
then B is exactly estimable in Â£gvÂ£oo â¢ More precisely,
(v) 
Tp = B V Zp.
Proof.
1. 
As shown before, (i) is equivalent to T I Z qÂ° \ B. By Theorem 8.2.23
(iv), (i) implies
(vi) r i ( B v  z Â§ Â° ) n i.

458
9. Invariance in Stochastic Processes
However, by Theorems 8.2.28 and 8.2.29, since Br = B and Z qÂ° is T-stable, 
(i) implies {B V Â£oÂ°)f JL Z qÂ° \ ZÂ£- But under (vi), by Theorem 8.2.17,
(B W Z^Y r 
= 
( B V Z ^ ) r n ( B V Z â¢)
ZÂ£ 
= 
Zr n ZÂ£Â° .
Therefore (i) implies (B V Zâ¢)r JL Z qÂ° | Z r . By Corollary 2.2.11, since
B y  Zr C (BV ZÂ£Â°)r,
(i) implies
( H v f 0Â°Â°)r X (B v l 0Â°Â°) | B v f r .
By Corollary 2.2.8, this implies that (# V ^oÂ°)r C BV Zp. Therefore (i) 
implies
(vii) (Â£vioÂ°Â°)r = s v f r .
2. 
Now, by Theorem 9.4.3, and under (ii), (iii) and (iv) are equivalent to 
^SvyÂ°Â° 
samphng shift invariant, i.e., T I yâ¢ I Â® V ZqÂ°. Along with
(i), this is equivalent to
(viii) r i x0Â°Â° | b .
As in 1, (viii) implies
(ix) n ^ v ^ J n i ,  
and
(x) 
( B V X Â§ Â° ) r  =  B
V
X p.
However, by Theorem 8.2.28, (viii) is equivalent to T I X0Â°Â° | A'p and 
B X X qÂ° | Xp. But under (ix), by Theorem 8.2.17, Xp = XpflXg00. ThereÂ­
fore (viii) implies

9.4. Conditional Stochastic Processes
459
(xi) B X X
q Â°  | Xr,
i.e., Xr is an Â£ ^ Â£ 00-sufficient statistic.
3. 
Now (ii) implies (9.4.25), i.e., X o<n<oo Tn I B V Z gÂ° which, by CorolÂ­
lary 2.2.11, is equivalent to JL o<n<oo Xn \ B V Zq* . Hence, by Theorem
7.6.5, 
( S v x J )  
C Â£ v f 0Â°Â° and, therefore, (B V -f0â ) 
C Â£ v f Â§ Â° . But 
under (ix), by Definition 8.2.31, this means that T is ergodic on BV X q* conÂ­
ditionally on B V Z q*. Hence, by Lemma 8.2.32, (B V 
r C (j3 V 2 q Â° ^ .
But under (viii) and (ix), and since Zr C Xr, this is equivalent to
(xii) B V X r =  (Bv <Â£oÂ°) = ( Â£ v f g Â° ) r = f v f r .
Therefore Xr C BW Zr, i.e., Xr is exactly estimating in 
an<^ i*1
nr 00
B V X Â£ Â° '
4. 
Since Xr is sufficient and exactly estimating in this experiment 
is exact and, by Theorem 4.7.8 and Proposition 4.7.7,
(xiii) ^  = ( S  V f r) * 0Â°Â° = * o Â°Â° ( b  V f r )  = ( b  V f r) n x
f .
5. 
Finally, by Proposition 4.6.6 (ii), (iv) implies that Â£%r ^ 
is identified,
B y  X q
i.e., (B  V f r )  XÂ£Â° = B V f r . Therefore
(xiv) 
X r  =  B V Z r ,
and this shows that B C XÂ§Â°, i.e., B is exactly estimable. 
â 
Remark that in Theorem 9.4.4, assumption (iv) is the only asymptotic 
assumption. But recall that assumption (ii) implies the allocation Property
(9.4.27) which implies, in particular, that To X Z qÂ° | B V Zq , i.e., (Zq) is 
^-transitive for (To )â¢ Now, by Theorem 7.3.21, this implies that
(9.4.28) 
(Â£  V f 0Â°Â°) f 0Â°Â° = 
V  
(b -V f 0n) f 0n n ( Â£  V f 0Â°Â°) .
0<n<oo

460
9. Invariance in Stochastic Processes
â^ 00 
^ z n
Therefore Â£^Â° ^ 
is identified once Â£~Â°^ is identified for some n Â£ IN. 
Bvyâ¢ 
Bvyg
Let us compare Theorem 9.4.4 and Theorem 7.7.18. Firstly, assumption 
(i) of 9.4.4, i.e., T I ZqÂ° | B implies, by Theorems 8.2.28 and 8.2.17, that 
B X ZqÂ° I Z r . Since Z r C Z t , by Corollary 2.2.11, this implies that Â£ ^ Â£ 0 0  
is tail-sufficient; this is assumption (i) of Theorem 7.7.18. Secondly, (xi) in 
the proof of Theorem 9.4.4 implies that Â£gwÂ£oo 
tail-sufficient or, equivÂ­
alently, B X XÂ§Â° I X.%Â° V n Â£ IN by Theorem 7.7.2. But this is equivalent 
to B X 
V Z ^  | y
V ZÂ£Â° and, by Corollary 2.2.11, this implies that
B X ToÂ° I ^oÂ°Â° 
i.e., Â£g0~oo 
tail-sufficient. As the other assumptions
are the same, this shows that assumptions of Theorem 9.4.4 are stronger 
than the assumptions of Theorem 7.7.18 and therefore, as a by-product, 
Theorem 9.4.4 also entails that
(9.4.29) 
XT â BN ZT .
Hence Xt is both sufficient and exactly estimating in Â£g^Â£oo- Note that,
in this set-up, nothing shows that Zp = Z t or that AT = Xt ', this suggests 
that Theorems 7.7.18 and 9.4.4 may in fact be different in nature.
As in Section 7.7, we now present a version of Theorem 9.4.4, analogous 
to Theorem 7.7.19, that is based on properties of the sampling process before 
integrating out nuisance parameters.
9.4.5 Theorem . Let Â£ ^ 9 Â°Â° be a representation of Â£a v x Â°Â° aRd let B C A
AvAr 0 
0
be a subparameter such that:
(i) 
^rv/^oo is sampling shift invariant;
DV^o
(ii) 
Â£
^ 
is conditionally independent;
(iii) 
y is sampling shift invariant;
(iv) B is an Â£5Â° * -sufficient parameter;
â  
.â
00
(v) 
B is included in the minimal strong Â£^Â° ^ -sufficient parameter;

9.4. Conditional Stochastic Processes
461
then B is exactly estimable.
Proof. By Theorems 7.7.19 and 9.4.4, it suffices to prove that (iii) and (iv) 
imply
(vi) B is 
-sufficient V n E IN; 
v 
J 
A v y n
(vii) 
is sampling shift invariant.
v 
7 
Bvy0
'âzÂ°Â°
Indeed, under (vi), by Theorem 7.7.19, (ii) implies that Â£ - 0- oo is conditionÂ­
i n g 0
^zÂ°Â°
ally independent and (v) implies that Â£^0^oo is identified; the result then
i3vy 0
follows from Theorem 9.4.4. Thus, we have to prove that
(1) 
n ^ o M v f o ,
and
(2) 
A Â± y 0 \ B V Z o
imply
(3) r I 5?o | B V  Zo,
and
(4) 
A Â± y n \ B W Z n.
But this is an immediate consequence of Theorem 8.2.30 under the substiÂ­
tution 
V Z ^ j . 
u
Both Theorems 9.4.4 and 9.4.5 assume shift invariance of Â£gv^oo ancb 
therefore, its tail sufficiency. Thus 8 ^  ^  ls allowed to not enjoy such a
' 
O'
property provided that the prior distribution conditional on the parameter 
of interest is such that, after integrating out the nuisance parameter, 
is shift invariant, i.e., the process z will be B-stationary whithout necessarÂ­
ily being ,4-stationary. A simple example of such a situation is a process 
characterized, amongst other, by a priori i.i.d. incidental parameters. If 
shift invariance of Â£gvzoo is implied by the shift invariance of 
neiÂ­
ther shift invariance nor tail sufficiency of 
is a necessary condition
for shift invariance of 8$ ^
.
Z3V z% Â°

462
9. Invariance in Stochastic Processes
If B and Z qÂ° are mutually ancillary (i.e., B X  ZÂ§Â°), then by Theorem 
8.2.30 the shift invariance of Zgygoo ls equivalent to the shift invariance of
Â°f the predictive distribution of the process z. This supplementary
0 
^
^ 
âzÂ°Â°
condition of mutual ancillarity makes the reduction of Â£$â  ~ 
into 
^
BVXâ¢
^ isv zÂ°Â°
admissible but leaves open the possibility that the transition P^ 
0 under-
y?
lying the conditional experiment may not be sampling probabilities but a 
conditional probability obtained after integrating out the nuisance parameÂ­
ters of the sampling probabilities. In Theorem 9.4.5 the role of assumption
(iv), which under assumption (ii) implies the sufficiency of B in 
^
As/y o
is precisely to preclude such a possibility and, therefore, to let the other 
assumptions bear on the sampling probabilities. These assumptions of muÂ­
tual ancillarity and of sufficiency of B are implied, by Theorem 3.3.6 (iii), by 
an assumption of a cut in 
he., when the parameter characterizing
the distribution of the process z is a priori independent of the parameters 
characterizing the distribution of the process y conditional to the process z.
Thus thejnain difference between Theorems 9.4.4 and 9.4.5 lies in the
^BvZ00 
â¢ 
â¢ 
â¢ 
âZÂ°Â°
transition P^ 
0 
characterizing the conditional experiment Â£~Â° ^ : in
yoo 
.
.
.
.
 
. 
.
Theorem 9.4.4 the transition is obtained after integrating out the nuisance 
parameters whereas in Theorem 9.4.5 the transition is the sampling probaÂ­
bilities with B as a sufficient parameter.
As the exact estimability of B implies the exact estimability of any 
C C B, and as the shift invariance of 
implies the shift invariance of
BWZQ
^cvzÂ°Â° ^or any 
^  
(ky Theorem 8.2.23.(i), and since By = B implies
r  I (BV 
by Theorem 8.2.28), we may conclude that the main differÂ­
ence, for practical purposes, finally resides in whether or not the parameter 
of interest to be exactly estimated is, or is not, included in a parameter 
characterizing a conditional sampling process.

Bibliography
Abramowitz, M. and I.A. Stegun (1964), Handbook of Mathematical FuncÂ­
tions. National Bureau of Standards, New York: Dover Publications.
Akaike, H. (1974), A new look at the statistical model identification. 
IEEE Transactions on Automatic Control, 19, 716-723.
Aldous, D.J. (1985), Exchangeability and related topics. (Lecture Notes 
in Mathematics, 1117). New York: Springer-Verlag.
Aldous, D.J. and J.W. Pitman (1979), On the zero-one law for exchangeÂ­
able events. Annals of Probability, 7, 704-723.
Anderson, T.W. (1971), The Statistical Analysis of Time Series. New 
York: John Wiley.
Anderson, T.W. (1976), Estimation of linear functional relationships: apÂ­
proximate distributions and connexions with simultaneous equations 
in econometrics (with discussion). Journal of the Royal Statistical 
Society, Series B , 38, 1-36.
Anderson, T.W. (1984), Estimating linear statistical relationships. The 
Annals of Statistics, 12, 1-45.
Ando, A. and G.M. Kaufman (1965), Bayesian analysis of the independent 
multinormal process, neither mean nor precision known. Journal of 
the American Statistical Association, 60, 347-358.
Arnold, S. (1981), The Theory of Linear Models and Multivariate Analysis. 
New York: John Wiley.
463

464
Bibliography
Bahadur, R.R. (1954), Sufficiency and statistical decision functions. The 
Annals of Mathematical Statistics, 25, 423-462.
Bahadur, R.R. (1955a), A characterization of sufficiency. The Annals of 
Mathematical Statistics, 26(2), 286-293.
Bahadur, R.R. (1955b), Statistics and subfields. The Annals of MatheÂ­
matics, 26(3), 490-497.
Barankin, E.W. (1961), Sufficient parameters: solution of the minimal 
dimensionality problem. Annals of the Institute of Statistical MatheÂ­
matics, 12, 91-118.
Barnard, G.A. (1963), Some aspects of the fiducial argument. Journal of 
the Royal Statistical Society, Series B, 25, 111-114.
Barnard, G.A. and D.A. Sprott (1971), A note on Basuâs example of 
anomalous ancillary statistics. In: Foundations of Statistical InferÂ­
ence. A Symposium. See Godambe and Sprott (1971), 163-176.
Barndorff-Nielsen, O. (1973), On M-ancillarity. Biometrika, 60, 447-455.
Barndorff-Nielsen, O. (1978), Information and Exponential Families in StaÂ­
tistical Theory. New York: John Wiley.
Barra, J.R. (1971), Notions fondamentales de statistique mathematique. 
Paris: Dunod. (English translation: Mathematical Basis of Statistics. 
New York: Academic Press (1981)).
Basu, D. (1955), On statistics independent of a complete sufficient statistic. 
Sankhya, 15, 377-380.
Basu, D. (1958), On statistics independent of a sufficient statistics. 
Sankhya, 20, 223-226.
Basu, D. (1959), The family of ancillary statistics. Sankhya, 21, 247-256.
Basu, D. (1964), Recovery of ancillary information. Sankhya A, Series A, 
26, 3-16.
Basu, D. (1975), Statistical information and likelihood. Part I. Sankhya 
A, 37, 1-71.
Basu, D. (1977), On the elimination of nuisance parameters. Part I. JourÂ­
nal of the American Statistical Association, 72, 355-366.

Bibliography
465
Becker, N. and I. Gordon (1983), On Coxâs criterion for discriminating beÂ­
tween alternative ancillary statistics. International Statistical Review, 
51, 89-92.
Berger, J. (1980), Statistical Decision Theory: Foundations, Concepts and 
Methods. New York: Springer-Verlag.
Berk, R.H. (1966), Limiting behavior of posterior distributions when the 
model is incorrect. The Annals of Mathematical Statistics, 37, 51-58.
Berk, R.H. (1970), Consistency a posteriori. The Annals of Mathematical 
Statistics, 41, 894-906.
Bernardo, J.M. (1979), Reference posterior distributions for Bayesian inÂ­
ferences (with discussion). Journal of the Royal Statistical Society, 
Series B,41, 113-147.
Bernardo, J.M., De Groot, M.H., Lindley, D.V. and A.F.M. Smith (edÂ­
itors), (1980), Bayesian Statistics. 
Proceedings of the First InterÂ­
national Meeting on Bayesian Statistics, held in Valencia (Spain), 
May 28-June 2, 1979. Valencia: University Press.
Bernardo, J.M., De Groot, M.H., Lindley, D.V. and A.F.M. Smith (editors)
(1985), Bayesian Statistics 2. Proceedings of the Second Valencia 
International Meeting, September 6-10, 1983. Amsterdam: North- 
Holland.
Bernardo, J.M., De Groot, M.H., Lindley, D.V. and A.F.M. Smith (ediÂ­
tors) (1988), Bayesian Statistics 3. Proceedings of the Third Valencia 
International Meeting, 1987. Oxford: Clarendom Press.
Bhaskara Rao, K.P.S. and B.V.Rao (1981), Borel Spaces. (Dissertationes 
Mathematicae CXC). Warsaw: Polska Akademia Nauk, Instytut Ma- 
tematyczny.
Billingsley, P.(1979), Probability and Measures. New York: John Wiley.
Blackwell, D. (1951), Comparison of experiments. Proceedings of the SecÂ­
ond Berkeley Symposium on Mathematical Statistics and ProbabiliÂ­
ties. Berkeley: University of California Press, 93-102.
Basu, D. and T.P. Speed (1974), Bibliography of sufficiency. Unpublished.

466
Bibliography
Blackwell, D. (1953), Equivalent comparison of experiments. The Annals 
of Mathematical Statistics, 24, 265-272.
Blackwell, D. (1956), On a class of probability spaces. Proceedings of the 
Third Berkeley Symposium on Mathematical Statistics and ProbabilÂ­
ities, 2. Berkeley: University of California Press, 1-6.
Blackwell, D. and L.E. Dubins (1975), On existence and non-existence of 
proper, regular conditional distribution. The Annals of Probability, 
3, 741-752.
Blumenthal, R.M. and R.K. Getoor (1968), Markov Processes and PotenÂ­
tial Theory. New York: Academic Press.
Box, G.E.P. and G.C. Tiao (1973), Bayesian Inference in Statistical AnalÂ­
ysis. Reading, (Mass.): Addison-Welsey.
Breiman, L. (1968), Probability. London: Addison-Welsey.
Breiman, L., LeCam, L. and L. Schwartz (1964), Consistent estimates and 
zero-one sets. The Annals of Mathematical Statistics, 35, 157-161.
Bunke, H. and O. Bunke (1974), Identifiability and estimability. Mathe- 
matische Operationsforschung und Statistik, 5, 223-233.
Burguete, J.-F., A.R. Gallant and G. Souza (1982), On unification of the 
asymptotic theory of nonlinear econometric models. Econometric ReÂ­
view, 1, 151-190.
Burkholder, D.L. (1961), Sufficiency in the undominated case. The Annals 
of Mathematical Statistics, 32, 1191-1200.
Cano, J.A., Hernandez, A., and E. Moreno (1988), On Kolmogorov partial 
sufficiency. In: Bayesian Statistics 3. See J.M. Bernardo,
M.H. De Groot, D.V. Lindley, and A.F.M. Smith, (1988), 553-556.
Cassel, C.M., Sarndal, C.E. and J.II. Wretman (1977), Foundations of 
Inference in Survey Sampling. New York: John Wiley.
Chamberlain, G. (1982), On the general equivalence of Granger and Sims 
causality. Econometric a, 50, 569-581.
Chandra, S. (1977), On the mixtures of probablity distributions. ScandiÂ­
navian Journal of Statistics, 4, 105-112.

Bibliography
467
Chow, Y.S. and H. Teicher (1978), Probability Theory. Berlin: Springer 
Verlag.
Chung, K.L. (1968), A Course in Probability Theory. New York: Harcourt- 
Brace.
Cifarelli, D.M., Muliere, P. and M. Scarsini (1981), II Modello Lineare 
nellâApproccio Bayesiano non Parametrico. 
Quaderni dellâIstituto 
Matematico G. Castlenuovo, Roma.
Cocchi, D. and M. Mouchart (1986), Linear Bayes estimation in finite 
populations with a categorical auxiliary variable. CORE Discussion 
Paper 8615, Universite Catholique de Louvain, Louvain-la-Neuve, BelÂ­
gium.
Cocchi, D. and M. Mouchart (1989), Approximations of Bayesian solutions 
in finite population models. CORE Discussion Paper 8905, Universite 
Catholique de Louvain, Louvain-la-Neuve, Belgium.
Cohn, H. (1965), On a class of dependent random variables. Revue Rou- 
maine de Mathematiques Pures et Appliquees, 10, 1593-1606.
Cohn, H. (1974), On the tail events of a Markov chain. Zeitschrift fur 
Wahrscheinlichkeitstheorie und Verwandte Gebiete, 29, 65-72.
Cornet, B. and H. Tulkens (editors) (1989), Contributions to Operations 
Research and Economics. Proceedings of the CORE XXth AnniverÂ­
sary Symposium. Cambridge (Mass.): The MIT Press, forthcoming.
Cox, D.R. (1961), Tests of separate families of hypotheses. 
ProceedÂ­
ings of the Fourth Berkeley Symposium on Mathematical Statistics 
and Probability, 1. Berkeley: The University of California Press, 
105-123.
Cox, D.R. (1962), Further results on tests of separate families of hypotheÂ­
ses. Journal of the Royal Statistical Society, Series B, 24, 406-424.
Cox, D.R. (1971), The choice between alternative ancillary statistics. JourÂ­
nal of the Royal Statistical Society, Series B, 33, 251-255.
Cox, D.R. and D.V. Hinkley (1974), Theoretical Statistics. 
London: 
Chapman and Hall.

468
Bibliography
Csiszar, I. (1967a), Information type measures of differences of probability 
distributions and indirect observations. Studia Scientiarum Mathe- 
maticarum Hungria, 2, 299-318.
Csiszar, I. (1967b), On topological properties of /-divergences. Studia 
Scientiarum Mathematicarum Hungria, 2, 319-329.
Davidson, J.E.H., Hendry, D.F., Srba, F., and S. Yeo (1978), Econometric 
modelling of the aggregate time-series relationship between consumers 
expenditure and income in United Kingdom. Economic Journal, 88, 
661-692.
Dawid, A.P. (1979a), Conditional independence in statistical theory (with 
discussion). The Journal of the Royal Statistical Society, Series B, 
41(1), 1-31.
Dawid, A.P. (1979b), Some misleading arguments involving conditional 
independence. Journal of the Royal Statistical Society, Series B, 41, 
249-252.
Dawid, A.P. (1980a), A Bayesian look at nuisance parameters. In: Bayesian 
Statistics. See J.M. Bernardo, M.H. de Groot, D.V. Lindley, and 
A.F.M. Smith, (1980), 167-184.
Dawid, A.P. (1980b), Conditional independence for statistical operations. 
The Annals of Statistics, 8, 598-617.
Dawid, A.P., Stone, M., and J.V. Zidek (1973), Marginalization paradoxes 
in Bayesian and structural inference. Journal of The Royal Statistical 
Society, Series B, 35 189-233.
de Finetti, B. (1937), La prevision: ses lois logiques, ses sources subjectives. 
Annales de Plnstitut Henri Poincare, 7. (English translation: ForeÂ­
sight: its logical laws, its subjective sources. In Kyburg and Smokier 
(1964)).
de Finetti, B. (1974), Probability, Induction and Statistics. New York: 
John Wiley.
De Groot, M. (1970), Optimal Statistical Decisions. 
New York: 
McGraw Hill.

Bibliography
469
Deistler, M. (1976), The identifiability of linear econometric models with 
autocorrelated errors. International Economic Review, 17, 26-46.
Deistler, M. (1986), Identifiability and causality in linear dynamic errors- 
in-variables systems. 
In: Asymptotic Theory for Non i.i.d. 
ProÂ­
cesses. See J.-P. Florens, M. Mouchart, J.-P. Raoult, J.-M. Rolin, 
and L. Simar (1986), 145-154.
Deistler, M. and H.G. Seifert (1978), Identifiability and consistent estimaÂ­
bility in dynamic econometric models. Econometrica, 46, 969-980.
Dellacherie, C. and P.A. Meyer (1975), Probability et potentiel. Paris: 
Hermann.
Dellacherie, C. and P.A. Meyer (1980), Probability et potentiel (B), Theo- 
rie des martingales. Paris: Hermann.
Diaconis, P. and D. Freedman (1979), Sufficiency and exchangeability. UnÂ­
published.
Diaconis, P. and D. Freedman (1984) Partial exchangeability and suffiÂ­
ciency. In: Statistics: Applications and New Directions, edited by 
J. K. Ghosh and J. Roy. Calcutta: Indian Statistical Institute, 205- 
236.
Diaconis, P. and D. Freedman (1986), On the consistency of Bayes estiÂ­
mates ( with discussion ). The Annals of Statistics, 14, 1-67.
Diaconis, P. and D. Ylvisaker (1979), Conjugate priors for exponential 
families. The Annals of Statistics, 7(2), 269-281.
Diaconis, P. and S. Zabell (1982), Updating subjective probability. Journal 
of the American Statistical Association, 77, 822-830. (With a more 
extensive discussion in their Stanford technical report nÂ° 136, 1979.)
Dickey, J.M. (1976), Approximate posterior distributions. Journal of the 
American Statistical Association, 71, 680-689.
Doob, J.L. (1949), Applications of the theory of martingales. Colloques 
Internationaux du C.N.R.S.. Paris, C.N.R.S., 22-28.
Doob, J.L. (1953), Stochastic Processes. New York: John Wiley.

470
Bibliography
Dreze, J.H. (1974), Bayesian theory of identification in simultaneous equaÂ­
tions models. In: Studies of Bayesian Econometrics and Statistics, 
edited by S. Fienberg and A. Zellner. Amsterdam: North-Holland, 
159-174.
Dreze, J.H. and J.-F. Richard (1983), Bayesian analysis of simultaneÂ­
ous equation systems. 
In: Handbook of Econometrics, edited by 
Z. Griliches and M. Intriligator. 
Amsterdam: 
North-Holland,
517-598.
Dynkin, E.B. (1978), Sufficient statistics and extreme points. Annals of 
Probability, 5, 705-730.
Elbers, C. and G. Ridder (1982), True and spurious duration dependence: 
the identifiability of the proportional hazard model. Review of EcoÂ­
nomic Studies, 49, 403-410.
Engle, R.F., Hendry, D.F., and J.-F. Richard (1983), Exogeneity. Econo- 
metrica, 51(2), 277-304.
Ferguson, T.S. (1973), A Bayesian analysis of some nonparametric probÂ­
lems. Annals of Statistics, 1, 209-230.
Ferguson, T.S. (1974), Prior distribution on spaces of probability measures. 
Annals of Statistics, 2, 615-629.
Fiori, G., Florens, J.-P., and H.W. Lai Tong (1982), Analyse des innovaÂ­
tions dans un processus multivarie: application a des donnees fran- 
Qaises. Annales de PINSEE, 46, 3-24.
Fisher, F.M. (1966), 
The Identification Problem in Econometrics. 
New York: McGraw Hill.
Florens, J.-P. (1974), Contributions aux applications des statistiques baye- 
siennes a lâeconometrie. These de doctorat de troisieme cycle, Univer- 
site de Provence, Marseille, France.
Florens, J.-P? (1978), Mesures a priori et invariance dans une experience 
bayesienne. Publications de Plnstitut de Statistique de PUniversite de 
Paris, 23, 29-56.

Bibliography
471
Florens, J.-P. (1979), Exogeneite et non causalite dans un modele baye- 
sien. Communications aux Journees de VAssociation des Statisticiens 
Universitaires, Paris.
Florens, J.-P. (1980), Specification et reduction des experiences bayesiennes. 
Application au modele econometrique lineaire. 
These de doctor at 
dâEtat. Universite de Rouen, France.
Florens, J.-P. (1982), Experiences bayesiennes invariantes. Annales de 
llnstitut Henri Poincare, 18(1-2), 305-317.
Florens, J.-P. (1983), Approximate reductions of Bayesian experiments. 
In: Specifying Statistical Models. See J.-P. Florens, M. Mouchart, 
J.-P. Raoult, L. Simar, and A. F. M. Smith (1983), 85-92.
Florens, J.-P. (1986), Consistency and invariance in Bayesian experiment. 
Document de Travail 8601. GREQE, Marseille.
Florens, J.P. (1988), Exhaustivite parametrique et enveloppement de moÂ­
dules. In : Melanges economiques, Essais en lâhonneur d âEdmond 
Malinvaud. Paris: Economica.
Florens, J.P., Hendry, D., and J.-F. Richard (1989), Encompassing and 
specificity. Cahier 8904, GREMAQ, Universite des Sciences Sociales, 
Toulouse, France. (Also available as a discussion paper from Duke 
University).
Florens, J.-P. and M. Mouchart (1977), Reduction of Bayesian experiÂ­
ments. 
CORE Discussion Paper 7737, Universite Catholique de 
Louvain, Louvain-la-Neuve, Belgium (revised July 1979).
Florens, J.-P. and M. Mouchart (1980), Initial and sequential reduction 
of Bayesian experiments. CORE Discussion Paper 8015, Universite 
Catholique de Louvain, Louvain-la-Neuve, Belgium.
Florens, J.-P. and M. Mouchart (1982), A note on non causality. Econo- 
metrica, 50(3), 583-591.
Florens, J.-P. and M. Mouchart (1985a), Conditioning in dynamic models. 
Journal of Time Series Analysis, 53(1), 15-35.
Florens, J.-P. and M. Mouchart (1985b), A linear theory for non causality. 
Econometrica, 53(1), 157-175.

472
Bibliography
Florens, J.-P. and M. Mouchart (1985c), Model selection: some remarks 
from a Bayesian viewpoint. In: Model Choice. See J.-P. Florens, 
M. Mouchart, J.-P. Raoult and L. Simar (1985), 27-44.
Florens, J.-P. and M. Mouchart (1986a), Exhaustivite, ancillarite et idenÂ­
tification en statistique bayesienne. Annales d âEconomie et de Statis- 
tique, 4, 63-93.
Florens, J.-P. and M. Mouchart (1986b), Some examples of Bayesian exÂ­
periments. Statistica, 46(4), 439-448.
Florens, J.-P. and M. Mouchart (1989), Bayesian specification test. In: 
Contributions to Operations Research and Economics. See B. Cornet 
and H. Tulkens (editors) (1989).
Florens, J.-P., Mouchart, M., Raoult, J.-P., and L. Simar (1984), AlterÂ­
native Approaches to Time Series Analysis. Proceedings of the 3rd 
Franco-Belgian Meeting of Statisticians, held in Louvain-la-Neuve, 
November 25-26, 1982. Bruxelles: Publications des Facultes Univer- 
sitaires Saint-Louis.
Florens, J.-P., Mouchart, M., Raoult, J.-P., and L. Simar (1985), Model 
Choice. Proceedings of the 4th Franco-Belgian Meeting of StatistiÂ­
cians, held in Louvain-la-Neuve, November 24-25, 1983. Bruxelles: 
Publications des Facultes Universitaires Saint-Louis.
Florens, J.-P., Mouchart, M., Raoult, J.-P., Rolin, J.-M., and L. Simar
(1986), Asymptotic Theory for Non i.i.d. 
Processes. 
Proceedings 
of the 5th Franco-Belgian Meeting of Statisticians, held at C.I.R.M., 
Marseille, November 23-24, 1984. Bruxelles: Publications des Facultes 
Universitaires Saint-Louis.
Florens, J.-P., Mouchart, M., Raoult, J.-P., Simar, L., and A.F.M. Smith 
(1983), Specifying Statistical Models, from Parametric to Non-Para- 
metric Using Bayesian Approaches. Proceedings of the 2nd Franco- 
Belgian Meeting of Statisticians, held in Louvain-la-Neuve, October 
15-16, 1981. (Lecture Notes in Statistics, 16). New York: Springer- 
Verlag.

Bibliography
473
Florens, J.-P., Mouchart, M., and J.-F. Richard (1974), Bayesian inferÂ­
ence in error-in-variables models. Journal of Multivariate Analysis, 4, 
419-452.
Florens, J.-P., Mouchart, M., and J.-F. Richard (1976), Likelihood analysis 
of linear models. CORE Discussion Paper 7619, Universite Catholique 
de Louvain, Louvain-la-Neuve, Belgium.
Florens, J.-P., Mouchart, M., and J.-F. Richard (1979), Specification and 
inference in linear models. CORE Discussion Paper 7943, Universite 
Catholique de Louvain, Louvain-la-Neuve, Belgium.
Florens, J.-P., Mouchart, M., and J.-F. Richard (1986), Structural time 
series modelling: a Bayesian approach. Applied Mathematics and 
Computation, 20(3-4), 365-400.
Florens, J.-P., Mouchart, M., and J.-F. Richard (1987), Dynamic error-in- 
variables models and limited information analysis. Annales d âEcono- 
mie et de Statistique, 6/7, 289-310.
Florens, J.-P., Mouchart, M., and J.-M. Rolin (1980), Reductions dans les 
experiences bayesiennes sequentielles. Cahiers du Centre d âEtudes de 
Recherche Operationnelle, 22(3-4), 353-362.
Florens, J.-P., Mouchart, M. and J.-M. Rolin (1985), On two definitions 
of identification. Statistics, 16(2), 213-218.
Florens, J.-P., Mouchart, M., and J.-M. Rplin (1986), Exact estimability 
in conditional models. In: Asymptotic Theory for Non i.i.d. ProÂ­
cesses. See J.-P. Florens, M. Mouchart, J.-P. Raoult, J.-M. Rolin, and 
L. Simar (1986), 121-144.
Florens, J.-P., Mouchart, M., and J.-M. Rolin (1987), Non causality and 
marginal Markov processes. CORE Discussion Paper 8706, Universite 
Catholique de Louvain, Louvain-la-Neuve, Belgium.
Florens, J.-P., Mouchart, M., and S. Scotto (1983), Approximate suffiÂ­
ciency on the parameter space and model selection. In: 44th SesÂ­
sion of the International Statistical Institute: Contributed Papers, 2, 
763-766.

474
Bibliography
Florens, J.-P. and J.-F. Richard (1989), Encompassing in finite parameÂ­
ter space. Cahier 8905, GREMAQ, Universite des Sciences Sociales, 
Toulouse, France. (Also available as a discussion paper from Duke 
University).
Florens, J.-P. and J.-M. Rolin (1984), Asymptotic sufficiency and exact 
estimability in Bayesian experiments. In: Alternative Approaches to 
Time Series Analysis. See J.-P. Florens, M. Mouchart, J.-P. Raoult, 
and L. Simar (1984), 121-142.
Florens, J.P. and S. Scotto (1984), Information value and econometric 
modelling. D.P. 17, Southern European Economics Discussion Series, 
ASSET, I.E.P., Universidad del Pais Vasco, Bilbao (Spain).
Fraser, D.A.S., 1968, The Structure of Inference. New York: John Wiley.
Fraser, D.A.S., (1972), Bayes, likelihood, or structural. The Annals of 
Mathematical Statistics, 43, 777-790.
Fraser, D.A.S. (1973), The elusive ancillary. In: Multivariate Statistical 
Inference, edited by D.G. Kabe and R.P. Gupta. Amsterdam: North- 
Holland, 41-48.
Freedman, D.A. (1962), Invariants under mixing which generalize 
de Finettiâs theorem. The Annals of Mathematical Statistics, 33(3), 
916-923.
Freedman, D.A. (1963), Invariants under mixing which generalize 
de Finettiâs theorem. Continuous time parameter. The Annals of 
Mathematical Statistics, 34(4), 1194-1216.
Freedman, D.A. (1963), On the asymptotic behavior of Bayes estimates 
in the discrete case. 
The Annals of Mathematical Statistics, 34, 
1386-1403.
Freedman, D.A. (1965), On the asymptotic behavior of Bayes estimates 
in the discrete case II. The Annals of Mathematical Statistics, 36, 
454-456.
Freedman, D.A. and P. Diaconis (1983), On inconsistent Bayes estimates 
in the discrete case. The Annals of Statistics, 11, 1109-1118.

Bibliography
475
Godambe, V.P. (1976), Conditional likelihood and unconditional optimum 
estimating equations. Biometrika, 63(2), 277-284.
Godambe, V.P. (1980), On sufficiency and ancillarity in the presence of a 
nuisance parameter. Biometrika, 67(1), 155-162.
Godambe, V.P. and D.A. Sprott (1971), Foundations of Statistical InferÂ­
ence. A Symposium. Toronto: Holt, Reinehart, and Winston.
Goel, P.K. and M.H. De Groot (1979), Comparison of experiments and 
information measures. The Annals of Statistics, 7, 1066-1077.
Goldberger, A.S. (1964), Econometric Theory. New York: John Wiley.
Good, I.J. (1950), Probability and the Weighing of Evidence. London: 
Griffin.
Good, I.J. (1965) The Estimation of Probabilities. An Essay on Modern 
Bayesian Methods. Cambridge (Mass.): M.I.T. Press.
Gourieroux, C., Monfort, A., and A. Trognon (1983), Testing nested and 
non nested hypotheses. Journal of Econometrics, 21, 83-115.
Gourieroux, C., Monfort, A., and A. Trognon (1984), Pseudo-maximum 
likelihood methods: theory. Econometrica, 52, 681-700.
Granger, C.W.J. (1969), Investigating causal relations by econometric modÂ­
els and cross-spectral methods. Econometrica, 37, 424-438.
Griliches, Z. (1974), Errors in variables and other unobservables. EconoÂ­
metrica, 42(6), 971-988.
Hajek, J. (1965), On basic concepts of statistics. Proceedings of the Fifth 
Berkeley Symposium on Mathematical Statistics and Probabilities, 1, 
Berkeley: The University of California Press, 139-162.
Hall, W.J., Wijsman, R.A., and J.K. Ghosh (1965), The relationship beÂ­
tween sufficiency and invariance with applications in sequential analÂ­
ysis. The Annals of Mathematical Statistics, 36, 575-614.
Halmos, P.R. (1950), Measure Theory. New York: Van Nostrand Reinhold.
Halmos, P. and L.J. Savage (1949), Application of the R.N. theorem to the 
theory of sufficient statistics. The Annals of Mathematical Statistics, 
20, 225-241.

476
Bibliography
Hannan, E.J. (1971), The identification problem for multiple equation sysÂ­
tems with moving average errors. Econometrica 39, 751-767.
Hartigan, J.A. (1964), Invariant prior distributions. The Annals of MathÂ­
ematical Statistics, 35, 836-845.
Hartigan, J.A. (1983), Bayes Theory. New York: Springer-Verlag.
Haussman, J.A. (1978), Specification tests in econometrics. Econometrica, 
46, 1251-1271.
Haussman, J.A. and W. Taylor (1981), A generalized specification test. 
Economic Letters, 8, 239-245.
Hendry, D.F. and G.J. Anderson (1977), Testing dynamic specification 
in small simultaneous systems: an application to a model of building 
society behavior in the United Kingdom. In: Frontiers in Quantitative 
Economics, Vol. 3A, edited by M. D. Intriligator. Amsterdam: North- 
Holland, 361-383.
Hendry, D. and J.-F. Richard (1982), On the formulation of empirical 
models in dynamic econometrics. Journal of Econometrics, 20, 3-33.
Hendry, D. and J.F. Richard (1983), The econometric analysis of economic 
time series. International Statistical Review, 51, 111-163.
Hendry, D. and J.-F. Richard (1987), Recent developments in the theory 
of encompassing. CORE DP 8722, Universite Catholique de LouÂ­
vain, Louvain-la-Neuve, Belgium. In: Contributions to Operations 
Research and Economics. See B. Cornet and H. Tulkens, (editors) 
(1989).
Hewitt, E. and L.J. Savage (1955), Symmetric measures on Cartesian 
products. Transactions of the American Mathematical Society, 80, 
470-501.
Heyer, H. (1982), Theory of Statistical Experiments. New York: Springer- 
Verlag.
Hoffmann-j0rgensen, J. (1971), Existence of Conditional Probabilities. 
Mathematica Scandinavia, 28, 257-264.
Hosoya, Y. (1977), On the Granger condition for non-causality. EconoÂ­
metrica, 45, 1735-1736.

Bibliography
477
Huber, P.J. (1967), The behavior of maximum likelihood estimates under 
non-standard conditions. Proceedings of the Fifth Berkeley SympoÂ­
sium on Mathematical Statistics and Probability, 1, Berkeley: The 
University of California Press, 221-233.
Hunt, G.A. (1957), Markov processes and potentials I and II. Illinois JourÂ­
nal of Mathematics, 1, 44-93 and 316-369.
Jeffreys, H. (1961), Theory of Probability, Third edition. London: Oxford 
University Press.
Kabanov, Y., Lipster, A. Ch., and A.N. Shiryayev (1979-1980), Absolute 
continuity and singularity of locally absolute continuous probability 
distributions, I, II. Translated from Math. USSR Sbornisk, 35, 631â 
680 (1979), and 36, 31-58 (1980).
Kadane, J. (1974), The role of identification in Bayesian theory. In: Studies 
in Bayesian Econometrics and Statistics, edited by S. Fienberg and 
A. Zellner. Amsterdam: North-Holland.
Kagan, A., Linnik, Y.V., and C.R. Rao (1973), Characterization Problems 
in Mathematical Statistics. New York: John Wiley.
Kakutani, S. (1948), On equivalence of infinite product measures. Annals 
of Mathematics, 49, 214-224.
Kalbfleisch, J. D. (1975), Sufficiency and conditionality (with discussion). 
Biometrika, 62, 251-259.
Kendall, M.G. and A. Stuart (1952), The Advanced Theory of Statistics. 
Vol. 1 : Distribution Theory. London: Griffin.
Kendall, M.G. and A. Stuart (1961), The Advanced Theory of Statistics. 
Vol. 2 : Inference and Relationship. London: Griffin.
Kendall, M.G. and A. Stuart (1966), The Advanced Theory of Statistics. 
Vol. 3: Design and Analysis, and Tirrte Series. London: Griffin.
Koehn, U. and D.L. Thomas (1975), On statistics independent of a suffiÂ­
cient statistic: Basuâs lemma. The American Statistician, 39, 40-42.
Kolmogorov, A.N. (1942), Determination of the center of dispersion and 
degree of accuracy for a limited number of observations (in Russian). 
Izvestija Akademii Nauk SSSR, 6, 3-32.

478
Bibliography
Kolmogorov, A.N. (1950), Foundations of the Theory of Probability. New 
York: Chelsea. 
(Translated from: Grundbegriffe der Wahrschein- 
lichkeitsrechnung, (1933). Berlin: Springer-Verlag).
Koopmans, T.C. (1950), When is an equations system complete for statistiÂ­
cal purposes? In: Statistical Inference in Dynamic Economic Models, 
edited by T.C. Koopmans. New York: John Wiley, 393-490.
Koopmans, T.C. and O. Reiers0l (1950), The identification of structural 
characteristics. Annals of Mathematical Statistics, 21, 165-181.
Kyburg, H.E. and H.E. Smokier (editors), (1964), Studies in Subjective 
Probability. New York: John Wiley.
Lauritzen, S.L. (1982), Statistical Models as Extremal Families. Aalborg: 
Aalborg University Press.
Learner, E. (1978), Specification Searches: Ad Hoc Inference with Non 
Experimental Data. New York: John Wiley.
Learner, E.,(1983), Model choice and specification analysis. In: Handbook 
of Econometrics, edited by Z. Grilickes and M.D. Intriligator, Vol. 1. 
Amsterdam: North-Holi and.
LeCam, L., (1958) Les proprietes asymptotiques de solutions de Bayes. 
Publications de VInstitut de Statistique de PUniversite de Paris, 7, 
17-35.
LeCam, L. (1964), Sufficiency and approximate sufficiency. The Annals of 
Mathematical Statistics, 35, 1419-1455.
LeCam, L. (1966), Likelihood functions for large numbers of independent 
observations. In: Festschrift for J. Neyman, edited by F. N. David. 
New York: John Wiley. 167-187.
LeCam, L. (1969), Theorie asymptotique de la decision statistique. 
Montreal: Les Presses de PUniversite de Montreal.
LeCam, L. (1970), On the assumptions used to prove asymptotic normalÂ­
ity of maximum likelihood estimates. The Annals of Mathematical 
Statistics, 41(3), 802-828.
LeCam, L. (1986) Asymptotic Methods in Statistical Decision Theory. 
New York: Springer-Verlag.

Bibliography
479
LeCam, L. and L. Schwartz (1960), A necessary and sufficient condition 
for the existence of consistent estimates. The Annals of Mathematical 
Statistics, 31, 140-150.
Lehmann, E.L. (1959), Testing Statistical Hypothesis. New York: John 
Wiley.
Lehmann, E.L. and H. Scheffe (1950), Completeness, similar regions and 
unbiased tests, Part I. Sankhya, 10, 305-340.
Lehmann, E.L. and H. Scheffe (1955), Completeness, similar regions and 
unbiased tests. Part II. Sankhya, 15, 219-236.
Lindley, D.V. (1961), The use of prior probability distributions in statistiÂ­
cal inference and decisions. Proceedings of the Fourth Berkeley SymÂ­
posium on Probability and Statistics, 1. Berkeley: The University of 
California Press, 453-468.
Lindley, D.V. and G.M. El-Sayyad (1968), The Bayesian estimation of a 
linear functional relationship. Journal of the Royal Statistical Society, 
Series B, 30(1), 190-202.
Lindley, D.V. and A.F.M. Smith (1972), Bayes estimates for the linear 
model. 
Journal of the Royal Statistical Society, Series B, 34(1), 
1-41.
Lindquist, A. and G. Picci (1979), On stochastic realization problem. 
SIAM Journal on Control and Optimization, 17, 365-389.
Lindquist, A. and G. Picci (1982), On a condition for minimality of MarkoÂ­
vian splitting subspaces. Systems and Control letters, 1, 264-269.
Lindquist, A., Picci,G., and G. Ruckebusch (1979), On minimal splitÂ­
ting subspaces and Markovian representations. Mathematical Systems 
Theory, 12, 271-279.
Littaye-Petit, M., Piednoir, J., and B. Van Cutsem (1969), Exhaustivite. 
Annales de Plnstitut Henri Poincare, 5, 289-322.
Mac Kean, H.P. Jr. (1963), Brownian motion with a several-dimensional 
time. Theory of Probability and its Applications, 8, 335-354.
Malinvaud, E. (1978), Methodes statistiques de Peconometrie. Third ediÂ­
tion, Paris: Dunod.

480
Bibliography
Marsaglia, G. (1964), Conditional means and covariances of normal variÂ­
ables with singular covariance matrix. Journal of the American StaÂ­
tistical Association, 49, 1203-1204.
Martin, F., Petit, J.-L., and II. Littaye (1971), Comparaison des expeÂ­
riences. Annales de llnstitut Henri Poincare, 7, 145-179.
Martin, F., Petit, J.-L., and M. Littaye (1973), Independance condition- 
nelle dans le modele statistique bayesien. Annales de llnstitut Henri 
Poincare, 9, 19-40.
Martin, F. and D. Vaguelsy (1969), Proprietes asymptotiques du modele 
statistique. Annales de llnstitut Henri Poincare, 5(4), 355-384.
Memin, J. (1985), Contiguity, absolute continuity and tightness. In: Model 
choice. See J.-P. Florens, M. Mouchart, J.-P. Raoult, and L. Simar 
(1985), 11-25.
Metivier, M. (1968), Notions fondamentales de la theorie des probabilites. 
Second edition. Paris: Dunod.
Mizon, G.E. and J.-F. Richard (1986), The encompassing principle and 
its application to testing non-nested hypotheses. Econometrica, 54, 
657-678.
Monfort, A. (1982), Cours de Statistique mathematique. Paris: EconomÂ­
ica.
Mouchart, M. (1976), A note on Bayes theorem. 
Statistica, 36(2), 
349-357.
Mouchart, M. and R. Orsi (1976), Polynomial approximation of distributed 
lags and linear restrictions: a Bayesian approach. Empirical EcoÂ­
nomics, 1, 129-152.
Mouchart, M. and R. Orsi (1986), A note on price adjustment models in 
disequilibrium econometrics. Journal of Econometrics, 31, 209-217.
Mouchart, M. and H. Roche (1987), Bayesian analysis of load curves 
through spline functions. The Statistician, 36(2), 289-296.
Mouchart, M. and J.-M. Rolin (1984a), On ^-sufficiency. Statistica, 44(3), 
367-371.

Bibliography
481
Mouchart, M. and J.-M. Rolin (1984b), A note on conditional indepenÂ­
dence. Statistical 44(4), 557-584.
Mouchart, M. and J.-M. Rolin (1985), Letter to the Editor. Statistica, 
45(3), 427-430.
Mouchart, M. and J.-M. Rolin (1986), On the <r-algebraic realization probÂ­
lem. CORE Discussion Paper 8604, Universite Catholique de Louvain, 
Louvain-la-Neuve, Belgium.
Mouchart, M. and J.-M. Rolin (1989), On maximal ancillarity. StatisÂ­
tica ,49(1).
Mouchart, M. and L. Simar (1984), Bayesian predictions: non parametÂ­
ric methods and least squares approximations. In: Alternative ApÂ­
proaches to Time Series Analysis. See J.-P. Florens, M. Mouchart, 
J.-P. Raoult, and L. Simar (1984), 11-28.
Nachbin, L. (1965), The Haar Integral. New York: Van Nostrand Reinhold.
Neveu, J. (1964), Bases mathematiques du calcul des probabilites. Paris: 
Masson. (Second edition: 1970). English translation: Mathematical 
Foundations of the Calculus of Probability (1965). San Francisco: 
Holden-Day.
Neveu, J. (1972), Martingales en Temps Disctet. Paris: Masson.
Neyman, J. (1951), Existence of a consistent estimates of the directional 
parameter in a linear structural relation between two variables. AnÂ­
nals of Mathematical Statistics, 22, 497-512.
Neyman, J. and E. Scott (1948), Consistent estimates based on partially 
consistent observations. Econometrica, 16, 1-12.
Neyman, J. and E. Scott (1951), On certain methods of estimating the 
linear structural relationship. The Annals of Mathematical Statistics, 
22, 352-361. (Corrections: 23 (1952), 135.)
Orey, S. (1971), Limit Theorems for Markov Chain Probabilities. London: 
Van Nostrand.
Parthasarathy, K.R. (1977), Introduction to Probability and Measure. 
Delhi: The MacMillan Company of India Ltd.

482
Bibliography
Pesaran, M. H., (1974), On the general problem of model selection. Review 
of Economic Studies, 41, 153-171.
Petit, J.-L. (1970), Exhaustivite, ancillarite et invariance. Annales d e lâIns- 
titut Henri Poincare, 6(4), 327-334.
Picci, G. (1976), Stochastic realization of Gaussian processes. Proceedings 
of the IEEE, 64(1), 112-122.
Picci, G. (1977), Some connections between the theory of sufficient statisÂ­
tics and the identifiability problem. SIAM  Journal on Applied MathÂ­
ematics, 33, 383-398.
Pierce, D.A. and L.D. Haugh (1977), Causality in temporal systems: charÂ­
acterizations and a survey. Journal of Econometrics, 5, 265-293.
Pitcher, T.S. (1957), Sets of measures not admitting necessary and sufÂ­
ficient statistics or subfield. The Annals of Mathematical Statistics, 
28, 267-268.
Pitcher, T.S. (1965), A more general property than domination for sets of 
probability measures. Pacific Journal of Statistics, 15, 597-611.
Pitman, E.J.C. (1939), Location and scale parameters. Biometrika, 30, 
391-421.
Pitman, E.J.C. (1979), Some Basic Theory for Statistical Inference. 
London: Chapman and Hall.
Pitman, J.W. (1978), An extension of de Finettiâs theorem. Advances in 
Applied Probability, 10, 268-269.
Pitman, J.W. and T.S. Speed (1973), A note on random times. Stochastic 
Processes and Applications, 1, 369-374.
Press, S.J. (1972), Applied Multivariate Analysis. 
New York: 
Holt, 
Rinehart, and Winston.
Raiffa, H. and R. Schlaifer (1961), Applied Statistical Decision Theory. 
Boston: Division of Research, Harvard Business School.
Ramamoorthi, R.V. (1980a), Sufficiency, pairwise sufficiency and Bayes 
sufficiency in undominated experiments. Ph.D. Thesis submitted to 
the Indian Statistical Institute, Calcutta.

Bibliography
483
Ramamoorthi, R.V. (1980b), On pairwise sufficiency and sufficiency in 
standard Borel spaces. Sankhya, Series A, 47, 139-145.
Ramsey, F.P. (1926), Truth and probability. In: The Foundations of MathÂ­
ematics and Other Logical Essays, edited by R.B. Braithwaite (1950). 
New York: The Humanities Press. Also reprinted in Kyburg and 
Smokier (1964), 61-92.
Raoult, J.-P. (1975), Structures statistiques. Paris: Presses Universitaires 
de France.
Reiers0l, O. (1950), Identifiability of a linear relation between variables 
which are subject to error. Econometric a, 18, 375-389.
Rolin, J.-M. (1975), The inverse of a continuous additive functional. Pacific 
Journal of Mathematics, 2, 585-604.
Rolin, J.-M. (1983), Non parametric Bayesian statistics: a stochastic proÂ­
cess approach. In: Specifying Statistical Models, from Parametric 
to Non-Parametric Using Bayesian Approaches. See J.-P. Florens, 
M. Mouchart, J.-P. Raoult, and L. Simar (1983), 108-133.
Rolin, J.-M. (1985), Selection of variables in discriminant analysis. In: 
Model Choice. See J.-P. Florens, M. Mouchart, J.-P. Raoult, and 
L. Simar (1985), 103-120.
Rolin, J.-M. (1986), Asymptotic behaviour of posterior expectations. In: 
Asymptotic Theory for Non i.i.d. 
Processes. 
See J.-P. Florens, 
M. Mouchart, J.-P. Raoult and L. Simar (1986), 93-120.
Romier, G. (1969), Modele dâexperimentation statistique. Annales de ITns- 
titut Henri Poincare, 5, 275-288.
Rosenblatt, M.R. (1962), Random Processes. New York: Oxford UniverÂ­
sity Press.
Rothenberg, T.J. (1971), Identification in parametric models. EconometÂ­
rica, 39, 577-591.
Roy, K.V. and R.V. Ramamoorthi (1979), Relationship between Bayes, 
classical and decision-theoretic sufficiency. Sankhya, Series A, 41, 
48-58.

484
Bibliography
Sacksteder, R. (1967), A note on statistical equivalence. The Annals of 
Mathematical Statistics, 38, 784-794.
Savage, L.J. (1954), The Foundations of Statistics. New York: John Wiley.
Sawa, T. (1978), Information criteria for discriminating among alternative 
regression models. Econometrica, 46, 1273-1292.
Schonfeld, P. (1975), A survey of recent concepts of identification. CORE 
Discussion Paper 7515, Universite Catholique de Louvain, Louvain, 
Belgium.
Schwartz, L. (1960), Consistency of Bayes procedures. 
Unpublished 
Ph. D. Thesis, University of California, Berkeley.
Schwartz, L. (1965), On Bayes procedures. Zeitschrift fur Wahrschein- 
lichkeitstheorie und verwandte Gebiete, 4, 10-26.
Scott, A. and T.M.F. Smith (1971), Bayes estimates for subclasses in stratÂ­
ified sampling. Journal of the American Statistical Association, 66, 
834-836.
Scott, A. and T.M.F. Smith (1973), Survey design, symmetry, and posteÂ­
rior distributions. Journal of the Royal Statistical Society, Series B, 
35, 57-60.
Shafer, G. (1986), Savage revisited. Statistical Science, 1(4), 463-501.
Shiryayev, A.N. (1981), Martingales: recent developments, results and apÂ­
plications. International Statistical Review, 49(3), 199-234.
Sims, C.A. (1972), Money, income, and causality. American Economic 
Review, 62, 540-552.
Sims, C.A. (1980), Macroeconomics and reality. 
Econometrica, 48(1), 
1-48.
Skibinski, M. (1967), Adequate subfields and sufficiency. The Annals of 
Mathematical Statistics, 38, 155-161.
Skibinski, M. (1969), Some known results concerning zero-one sets. Annals 
of the Institute of Statistical Mathematics, 21, 541-545.
Smith, T.M.F. (1983), On the validity of inferences from non-random samÂ­
ples. Journal of the Royal Statistical Society, Series A, 146, 394-403.

Bibliography
485
Solari, M.E. (1969), The âmaximum likelihood solutionâ of the problem 
of estimating a linear functional relationship. Journal of the Royal 
Statistical Society, Series B, 31, 372-375.
Soler, J.-L. (1970), Notions de liberte en statistique mathematique. These 
de doctorat de 3eme cycle, Universite de Grenoble, France.
Speed, T.P. (1976), A note on pairwise sufficiency and completions. 
Sankhya, Series A, 38, 194-196.
Sprent, P. (1966), A generalized least squares approach to linear functional 
relationships. Journal of the Royal Statistical Society, Series B, 28, 
278-297.
Sprent, P. (1970), The saddle point of the likelihood surface for a linÂ­
ear functional relationship. Journal of the Royal Statistical Society, 
Series B, 32, 432-434.
Sprott, D.A. (1975), Marginal and conditional sufficiency. Biometrika, 62, 
599-605.
Stone, M. (1976), Strong inconsistency from uniform priors. Journal of the 
American Statistical Association, 71, 114-125.
Stout, W.F. (1974), Almost Sure Convergence. 
New York: Academic 
Press.
Strassen, V. (1965), The existence of probability measures with given marÂ­
ginals. The Annals of Mathematical Statistics, 36, 423-439.
Sugden, R.A. (1979), Inference on symmetric functions of exchangeable 
populations. Journal of the Royal Statistical Society, Series B, 41, 
269-273.
Sugden, R.A. (1985), A Bayesian view of ignorable designs in survey 
sampling inference. In: Bayesian Statistics 2. See J.M. Bernardo, 
M.H. de Groot, D.V. Lindley, and A.F.M. Smith (1985), 751-754.
Sugden, R.A. and T.M.F. Smith (1984), Ignorable and informative designs 
in survey sampling inference. Biometrika, 71, 495-506.
Teicher, H. (1960), On the mixture of distributions. The Annals of MathÂ­
ematical Statistics, 31, 55-73.

486
Bibliography
Teicher, H. (1961), Identifiability of mixtures. The Annals of Mathematical 
Statistics, 32, 244-248.
Teicher, H. (1967), Identifiability of mixtures of product measures. The 
Annals of Mathematical Statistics, 38, 1300-1302.
Tiao, G. and A. Zellner (1964), On the Bayesian estimation of multivariate 
regression. 
Journal of the Royal Statistical Society, Series B, 26, 
277-285.
Torgersen, E.N. (1972), Comparison of translation experiments. The AnÂ­
nals of Mathematical Statistics, 43, 1383-1399.
Torgersen, E.N. (1976), Comparison of statistical experiments. ScandinaÂ­
vian Journal of Statistics, 3, 186-208.
Torgersen, E.N. (1981), Measures of information based on comparison with 
total information and with total ignorance. The Annals of Statistics, 
9, 638-657.
Van Putten, C. and J.H. Van Schuppen (1985), Invariance properties of the 
conditional independence relation. The Annals of Probablity, 13(3), 
934-945.
Villegas, C. (1971), On Haar priors. In: Foundations of Statistical InÂ­
ference, edited by V.P. Godambe and D.A. Sproot. Toronto: Holt, 
Rinehart, and Winston.
Villegas, C. (1972), Bayesian inference in linear relations. Annals of MathÂ­
ematical Statistics, 43, 1767-1791.
Villegas, C. (1977a), Inner statistical inference. Journal of the American 
Statistical Association, 72, 453-458.
Villegas, C. (1977b), On the representation of ignorance. Journal of the 
American Statistical Association, 72, 653-654.
Villegas, C. (1981), Inner Statistical Inference II. Annals of Statistics, 9, 
768-776.
Walker, A.M. (1969), On the asymptotic behaviour of posterior distribuÂ­
tions. Journal of the Royal Statistical Society, Series B, 31(1), 80-88.
Watson, G.S. (1983), Statistics on Sphere. New York: John Wiley.

Bibliography
487
White, H. (1982), Maximum likelihood estimation of misspecified models. 
Econometrica, 50, 1-26.
Wu, D.L. (1973), Alternative tests of independence between stochastic 
regressors and disturbances. Econometrica, 41, 733-750.
Zacks, S. (1971), Theory of Statistical Inference. New York: John Wiley.
Zellner, A. (1971), An Introduction to Bayesian Inference in Econometrics. 
New York: John Wiley.
Zellner, A. and A. Siow (1980), Posterior odds ratios for selected reÂ­
gression hypothesis. 
In: Bayesian Statistics. 
See J.M. Bernardo, 
M.H. de Groot, D.V. Lindley, and A.F.M. Smith, (1980), 585-603.


Author Index
Abramowitz and Stegun, 386 
Akaike, 136 
Aldous, 422
Aldous and Pitman, 422 
Anderson, 296, 415 
Ando and Kaufmann , 388 
Arnold, 349
Bahadur, 47, 88, 253 
Barankin, 79, 156, 165 
Barnard, 114, 392 
Barnard and Sprott, 188 
Barndorff-Nielsen, 118, 239 
Barra, xiv, 43, 85, 88, 92, 156 
Basu, 48, 69, 92, 102, 187,
188, 202, 228 
Basu and Speed, xvii 
Becker and Gordon, 188 
Berger, xiv 
Berk, 136, 184, 306 
Bernardo, 35 
Bhaskara Rao and Rao, 7 
Billingsley, 2, 13 
Blackwell, 98, 128, 133 
Blackwell and Dubbins, 22 
Blumenthal and Getoor, 12, 247
Box and Tiao, xiv 
Breiman, xiv, 2, 12, 360, 437 
Breiman, LeCam and Schwartz, 200, 
308
Bunke and Bunke, 171 
Burguete, Gallant, and Souza, 449 
Burkholder, 88
Cano, Hernandez and Moreno, 105 
Cassel, Sarndal and Wretman, 33 
Chamberlain, 257 
Chandra, 239
Chow and Teicher, 14, 137, 318,
422
Chung, xiii, 2, 12, 15, 410
Cifarelli, Muliere and Scarsini, 46
Cocchi and Mouchart, 33
Cohn, 355, 437
Cox, 136, 141, 188
Cox and Hinkley, xiv, 141, 188
Csiszar, 98
Davidson, Hendry, Sbra and Yeo, 
136
Dawid, xvii, 48, 66, 69, 102, 239 
Dawid, Stone and Zidek, 34, 400
489

490
Author Index
De Finetti, xiv, 33, 422 
De Groot, xiv, 40, 314 
Deistler, 296
Deistler and Seifert, 171, 172, 308, 
313
Dellacherie and Meyer, xiv, 2, 7, 
8, 9, 12, 19, 22, 23, 67, 137, 
247, 286, 288, 422 
Diaconis and Freedman, 294, 422 
Diaconis and Ylvisaker, 44 
Diaconis and Zabell, 133 
Dickey, 283 
Doob, 7, 306 
Dreze, 160
Dreze and Richard, 274 
Dynkin, 294
Elbers and Ridders, 166 
Engle, Hendry and Richard, 276
Ferguson, 46, 389 
Fiori, Florens and Lai Tong, 242 
Fisher, 160, 171, 176 
Florens, 26, 98, 140, 242, 351 
Florens, Hendry and Richard, 138 
Florens and Mouchart, 26, 66, 98, 
136, 137, 138, 140, 242, 257, 
285
Florens, Mouchart and Richard, 
xviii,98, 101, 127, 242,276, 
296
Florens, Mouchart and Rolin,
140, 242, 285, 404 
Florens, Mouchart and Scotto, 98, 
136
Florens and Richard, 138
Florens and Rolin, 140, 285 
Florens and Scotto, 98 
Fraser, 114, 188, 400 
Freedman, 314, 422 
Friedman and Diaconis, 314
Godambe, 102
Goel and De Groot, 134
Good, xiv
Gourieroux, Monfort, and Trognon, 
136
Granger, 253, 256 
Griliches, 127
Hajek, 102
Hall, Wijman and Ghosh, xvii,
253, 394 
Halmos, 374
Halmos and Savage, 28, 88, 89, 90 
Hannan, 160 
Hartigan, 35, 184, 283 
Hausman, 136 
Hausman and Taylor, 136 
Hendry and Anderson, 136 
Hendry and Richard, 136 
Hewitt and Savage, 137, 422 
Heyer, 128
Hoffman and J0rgensen, 22 
Hosoya, 257 
Huber, 136 
Hunt, 19
Jeffreys, 35
Kabanov, Lipster and Shiryayev, 
318

Author Index
491
Kadane, 165, 168
Kakutani, 318
Kalbfleisch, 106
Kendall and Stuart, xiv
Khoen and Thomas, 92, 200, 202
Kolmogorov, 52, 102
Koopmans, 117
Lauritzen, 294
Learner, xiv
LeCam, xvii, 98, 128, 133, 184, 
283, 306 
LeCam and Schwartz, 171, 308, 
313
Lehmann and Scheffe, 222, 235 
Lehmann, 349, 377 
Lindley, xiv
Lindley and El-Sayad, 127 
Lindley and Smith, 36 
Lindquist and Picci, 146, 214, 221 
Lindquist, Picci and Ruckebusch, 
146, 214, 216, 221
MacKean, 145 
Malinvaud, 117, 176, 449 
Marsaglia, 82
Martin, Petit and Littaye, xvii, 
68, 90, 102, 103, 113 
Martin and Vaguelsy, 308 
Memin, 31 &
Metivier, xiv, 2, 7, 13, 15, 20, 248 
Mizon and Richard, 136, 138 
Monfort, 43
Mouchart, xviii, 34, 351 
Mouchart and Orsi, 52 
Mouchart and Roche, 52 
Mouchart and Rolin, 66, 98, 140,
189, 216 
Mouchart and Simar, 46
Nachbin, 374
Neveu, xiv, 2, 7, 12, 14, 15, 16, 
18, 19, 20, 22, 54, 171, 205, 
247, 286, 292, 309, 310, 311, 
322, 425 
Neyman, 296
Neyman and Scott, 127, 296
Oheix, xvii 
Orey, 437
Parthasarathy, 22 
Pesaran, 136 
Petit, 394 
Picci, 79, 165, 214 
Pierce and Haugh, 253 
Pitcher, 156 
Pitman, 388, 422 
Pitman and Speed, 66 
Press, 40
Raiffa and Schlaifer, xiv, 41, 49
Ramamoorthy, 90
Raoult, xvii
Reiers0l, 167, 296
Rolin, 46, 81, 389, 404
Romier, xvii
Rothenberg, 171
Roy and Ramamoorthy, 90
Sacksteder, 128 
Savage, xiv, 32 
Sawa, 136

492
Author Index
Schwartz, 308 
Schonfeld, 172 
Scott and Smith, 33 
Shafer, xiv 
Shiryayev, 318 
Sims, 253, 256 
Skibinski, 200, 308 
Smith, 33 
Solari, 296 
Soler, 92 
Speed, xvii, 103 
Sprent, 296 
Sprott, 106 
Stein, xvii 
Stone, 400 
Stout, 437 
Strassen, 133 
Sugden, 33
Teicher, 238
Tiao and Zellner, 388
Torgersen, 98
Van Cutsem, xvii 
Van Putten and Van Schuppen, 
66
Villegas, 35, 400
Walker, 283 
Watson, 386 
White, 136 
Wu, 276
Zacks, xiv 
Zellner, xiv, 35 
Zellner and Siow, 318
Sugden and Smith, 33

Subject Index
//-p-sampling estimable parameter, 
308
//-strongly sampling estimable 
parameter, 308 
//-weakly sampling estimable 
parameter, 308
7r-system , 12
cr-field, 2
cr-field generated by, 3, 5 
(7-field of cylinder sets, 11 
(7-field of shift-invariant sets, 411 
cr-field of sym m etric sets, 414 
cr-finite m easure, 14
^-invariant cr-field, 352 
^-invariant measurable 
function, 352 
<p-invariant measurable set, 352 
^-invariant probability, 359 
<p-stable cr-field, 360
<I> invariates M i conditionally 
on M 2 , 364 
^-adapted measurable function, 355
^-invariant cr-field, 353, 359 
^-invariant Bayesian 
experiment, 383 
^-invariant measurable 
function, 352, 353 
^-invariant measurable set, 352 
^-stable (7-field, 360
a.s.s-identified , 172 
6-identified ,172 
c-identified , 172 
d-system, 13
Â£-c-identified , 176 
^-s-identified , 176
p-Bayesian estimable 
parameter, 308 
p-complete parameter, 222 
p-complete statistic, 222, 235 
p-consistent, 308 
p-sampling estimable 
parameter, 308 
p-semi-norm, 17
493

494
Subject Index
s-identified , 171 
â4-complete parameter, 153
5-homogeneous Bayesian 
stochastic process, 431 
^-optimal mutually sufficient, 159 
^-generalized i.i.d. process, 428
^-ancillary statistic, 79, 83, 84 
^-ancillary parameter, 84 
^-maximal ancillary parameter, 141 
^-maximal ancillary statistic, 142 
^-minimal sufficient parameter, 153 
^-minimal sufficient statistic, 154 
^-parameter, 55 
^-statistic, 54 
^-sufficient parameter, 79 
^-sufficient statistic, 78 
SM -parameter, 55 
f-^-statistic, 54 
fg^-ancillary statistic, 106 
Â£ ^ r -ancillary parameter, 107 
^ e v r' sufficient- statistic, 106 
Â£g^r -sufficient parameter, 107
(/Câ)-recursive, 243
(Adn)-transitive, 253, 259 
(A/<n)-fc-Markovian, 259 
Ad-exchangeable Bayesian 
stochastic process, 421 
Ad-i.i.d. Bayesian stochastic 
process, 423 
Ad-identically distributed Bayesian 
stochastic process, 423
A'f-independent Bayesian 
stochastic process, 423 
Ad-Markovian Bayesian 
stochastic process, 431 
Ad-stationary Bayesian stochastic 
process, 417 
Adi-allocated, 327 
Adi-sifted, 326
tS-complete statistic, 154
A posteriori independence, 118,
119, 120, 123, 124, 201, 252 
A priori independence, 118, 119,
120, 123, 124, 201, 449, 462 
Absolutely continuous, 15 
Abstract probability space, 1 
Adapted sequence of sub-cr-fields,
243
Admissible reduction of a Bayesian 
experiment, 65, 78 
Algebra of sets, 12 
Almost sure equality, 18 
Almost surely ^-invariant cr-field, 
361
Almost surely ^-invariant 
measurable function, 361 
Almost surely ^-invariant 
measurable set, 361 
Almost surely ^-invariant 
cr-field, 362 
Almost surely ^-invariant 
measurable function, 362 
Almost surely ^-invariant 
measurable set, 362

Subject Index
495
Almost surely maximal invariant 
parameter,391 
Almost surely maximal invariant 
statistic, 391 
Analytic set, 8 
Ancillary parameter, 66 
Ancillary statistic, 66 
Asymptotic experiment, 45, 283 
Asymptotically exactly estimable 
parameter, 307 
Asymptotically sufficient 
parameter, 296 
Asymptotically sufficient 
statistic, 294 
Atoms of a <r-field, 4, 76 
Autoregressive moving average 
process, 437
Bayes theorem, 30 
Bayesian cut, 112, 118, 122 
Bayesian experiment, xii, 27, 335 
Bayesian methods, xii 
Bayesian models, xiv 
Bayesian p-complete statistic, 238 
Bayesian predictive experiment, 38 
Bayesian stochastic process,
408, 417, 421 
Blackwell cr-field, 8 
Blackwell theorem, 7, 8, 76 
Borel cr-field, 4 
Borel function, 6
Canonical Bayesian exponential 
experiment, 41 
Coherent encompassing, 135 
Coherent sufficiency, 129
Complementary reductions, 57 
Complete parameter, 222 
Complete statistic, 222 
Completed sub-<r-field, 17, 69 
Completed trivial cr-field, 16, 69 
Conditional Bayesian experiment,
47, 51, 52 
Conditional bimeasurable 
density, 23 
Conditional expectation , 19 
Conditional identification, 161,162 
Conditional independence, 67 
Conditional independent Bayesian 
experiment, 334 
Conditional independent tail-sufficient 
Bayesian experiment, 335 
Conditional measurable 
separability, 190 
Conditional probability, 22 
Conditional stochastic processes,
448
Conditional strong identification,
204
Coordinate map, 11 
Coordinate process, 409 
Countably additive measure, 14 
Cylinder set, 11
Density, 23 
Desintegration, 23 
Doeblinâs condition, 437 
Dominated Bayesian experiment,
30, 84
Dominated sequential Bayesian 
experiment, 273

496
Subject Index
Dominated statistical experiment, 
28
Dynamic Bayesian experiment, 330 
Dynamic conditional Bayesian 
experiment, 331
Ergodic on M \ conditionally on 
M 2, 372 
Ergodicity, 372 
Events, 2
Exact Bayesian experiment, 181 
Exactly estimable parameter, 177 
Exactly estimating statistic, 177 
Exchangeable Bayesian process, 
422
Expectation (mathematical), 16
Family of probability measures, 26 
Field of subsets, 12 
Filtration, 242 
Finite measure, 14 
Finite permutation operator, 413 
Fubini theorem, 19, 30
Global analysis, 248
Haar measure (Left, Right) ,374
Identification of mixtures, 239 
Identified cr-field, 161 
Identified experiment, 165 
Identified parameter, 165 
Identified statistic, 170 
Image under /  of P, 17 
Improper prior distributions, 34 
Independence of cr-fields, 67
Independent tail-sufficient Bayesian 
experiment, 335 
Indicator function, 4 
Initial analysis, 248 
Integral, 14
Jensen inequality, 22 
Jointly admissible reduction,
96, 97
K-sufficient statistic, 102
Lebesgue decomposition theorem, 
15
Lusin cr- field, 8
Marginal Bayesian experiment,
47, 48, 49 
Marginal probability, 22 
Marginalized sampling probability, 
49
Markov kernel, 18 
Markovian process, 432 
Markovian property, 258 
Martingale adapted to a filtration, 
286
Martingale convergence theorem, 
286, 454
Maximal ^-invariant function, 353 
Maximal ancillary parameter,
142, 229 
Maximal ancillary statistic,
142, 229
Maximal invariant parameter, 391
Independent Bayesian experiment,
333

Subject Index
497
Maximal invariant statistic, 391 
Measurable family of experiments, 
36
Measurable function, 5 
Measurable separability, 189 
Measurable sets, 2 
Measurable space, 2 
Measurably separated cr-fields, 190 
Measurably separated Bayesian 
experiment, 197 
Measure (positive, countably 
additive, finite, cr-finite) , 14 
Measure space, 14 
Measure-preserving transformation, 
359
Minimal internal splitting a-field, 
145, 215 
Minimal splitting, 215 
Minimal sufficient parameter, 153 
Minimal sufficient statistic, 153 
Mixing, 372, 373 
Monotone class, 12 
Monotone class theorem, 12 
Moving-aver age process, 430 
Mutual ancillarity, 100 
Mutual exogeneity, 112, 270 
Mutual sufficiency, 112 
Mutually conditionally independent 
cr-fields, 319 
Mutually exogenous, 116, 122 
Mutually singular, 15 
Mutually sufficient, 113, 122
Neymanâs factorization, 85 
Null sets, 16
Observationally equivalent, 171 
Orbit under $  of x , 357
Parameter random variable, 409 
Parameter space, 26, 27 
Pivotal cr-field , 393 
Polish space, 4
Posterior $-in variant , 383, 384 
Posterior expectation, 27 
Posterior probabilities, xii, 27 
Predictive probability, xii, 27 
Predictively ^-invariant, 384 
Prior ^-invariant, 384 
Prior probability, xii, 27 
Privileged dominating probability, 
28, 89 
Probability, 16 
Probability space, 16 
Product cr-field, 11 
Progressively measurable, 376 
Projection of a-fields, 143, 144 
Punctually exactly estimable 
parameter, 184
Quotient measurable space, 5
Radon-Nikodym derivative, 16 
Random variable, 6 
Random vector, 12 
Regular Bayesian experiment,
27, 48, 49, 51, 52 
Regular prior probability, 88 
Regular statistic, 87 
Regular version of the conditional 
probability, 23

498
Subject Index
Representation of a Bayesian 
stochastic process, 408 
Reverse martingale theorem, 288
Sample space, 26, 27 
Sampling ^-invariant , 383, 384 
Sampling ancillary statistic, 90 
Sampling expectation , 27 
Sampling-p-complete statistic, 235 
Sampling probabilities, xii, 26,
27 103
Sampling shift-in variant, 453 
Sampling sufficient statistic, 88 
Sampling theory (classical), xi 
Separable (7-field, 3, 4 
Separating (7-field, 4, 172 
Sequential analysis, 249 
Sequential Bayesian experiment, 
247, 292 
Sequential conditional Bayesian 
experiment, 301 
Shift-invariant events of the 
coordinate process, 411 
Shift-invariant events of the 
stochastic process, 411 
Shift operator, 410, 431 
Simple random variable, 6 
Souslin cr-field, 8, 173 
Splitting (j-field, 215 
Splitting set, 200, 202 
Standard Borel cr-field, 8 
State space, 408 
Stationary process, 436 
Statistic, 27
Statistical experiment, 26 
Strong encompassing, 135
Strong identifiability, 188, 204 
Strong sufficiency, 129 
Strong SM -parameter, 55 
Strong f^-statistic, 54 
Strongly Bayesian estimable 
parameter, 308 
Strongly consistent, 308 
Strongly p-identified (7-field, 204 
Strongly sampling estimable 
parameter, 308 
Sub-cr-field, 2 
Subparameter, 27 
Sufficient experiment, 133 
Sufficient parameter, 66, 460 
Sufficient sequence of parameters, 
296
Sufficient sequence of statistics, 294 
Sufficient statistic, 66 
Symmetric events of the 
coordinate process, 414 
Symmetric events of the 
stochastic process, 414
Tail-cr-field, 286
Tail-sufficient Bayesian experiment, 
330
Tail-sufficient dynamic conditional 
Bayesian experiment, 332 
Totally informative Bayesian 
experiment, 183 
Totally non-informative Bayesian 
experiment, 65, 78 
Trace of a cr-field, 3 
Trace of P on A7, 22 
Transition or Transition 
probability, 18

Subject Index
Trivial cr-field, 2 
Trivial SM -parameter, 55 
Trivial Â£M -statistic, 54
Uniform Â£M -parameter, 55 
Uniform Â£M -statistic, 54 
Uniformly complete parameter, 239
Weak encompassing, 135 
Weak sufficiency, 129 
Weakly Bayesian estimable 
parameter, 308 
Weakly consistent, 308 
Weakly sampling estimable 
parameter, 308 
Wedge of a-fields, 3


Leif Mejlbro
Random variables I
Probability Examples c-2
Download free books at

2 
Leif Mejlbro
Probability Examples c-2
Random variables I
Download free eBooks at bookboon.com

3 
Probability Examples c-2 – Random variables I
© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-516-5
Download free eBooks at bookboon.com

Random variables I
 
4 
Contents
 
Introduction  
3
1  
Some theoretical results  
4
2  
Simple introducing examples  
18
3  
Frequencies and distribution functions in 1 dimension  
20
4  
Frequencies and distributions functions, 2 dimensions  
33
5  
Functions of random variables, in general  
75
6  
Inequalities between two random variables  
81
7  
Functions Y = f(X) o f random variables  
93
8  
Functions of two random variables, f(X; Y )  
107
9  
Means and moments of higher order  
129
10  
Mean and variance in special cases  
143
 
Index  
157
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
5 
Introduction
Introduction
This is the second book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole Jørsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the ﬁrst one. We shall here deal with the basic stuﬀ, i.e. frequencies and distribution
functions in 1 and 2 dimensions, functions of random variables and inequalities between random
variables, as well as means and variances.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ﬁrst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
25th October 2009
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
6 
1. Some theoratical results
1
Some theoretical results
The abstract (and precise) deﬁnition of a random variable X is that X is a real function on Ω, where
the triple (Ω, F, P) is a probability ﬁeld, such that
{ω ∈Ω | X(ω) ≤x} ∈F
for every x ∈R.
This deﬁnition leads to the concept of a distribution function for the random variable X, which is the
function F : R →R, which is deﬁned by
F(x) = P{X ≤x}
(= P{ω ∈Ω | X(ω) ≤x}),
where the latter expression is the mathematically precise deﬁnition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 ≤F(x) ≤1
for every x ∈R.
The function F is weakly increasing, i.e. F(x) ≤F(y) for x ≤y.
limx→−∞F(x) = 0
and
limx→+∞F(x) = 1.
The function F is continuous from the right, i.e. limh→0+ F(x + h) = F(x)
for every x ∈R.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deﬁne a median of a random variable X with the distribution function F(x) as a real
number a = (X) ∈R, for which
P{X ≤a} ≥1
2
and
P{X ≥a} ≥1
2.
Expressed by means of the distribution function it follows that a ∈R is a median, if
F(a) ≥1
2
and
F(a−) = lim
h→0−F(x + h) ≤1
2.
In general we deﬁne a p-quantile, p ∈]0, 1[, of the random variable as a number ap ∈R, for which
P {X ≤ap} ≥p
and
P {X ≥ap} ≥1 −p,
which can also be expressed by
F (ap) ≥p
and
F (ap−) ≤p.
If the random variable X only has a ﬁnite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables I
 
7 
1. Some theoratical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
−∞
f(u) du,
x ∈R,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R →R is called a frequency of the random variable X.
Let again (Ω, F, P) be a given probability ﬁeld. Let us consider two random variables X and Y , which
are both deﬁned on Ω. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) ∈A}
for every Borel set A ⊆R2.
When the simultaneous distribution of (X, Y ) is known, we deﬁne the marginal distributions of X
and Y by
PX(B) = P{X ∈B} := P{(X, Y ) ∈B × R},
where B ⊆R is a Borel set,
PY (B) = P{Y ∈B} := P{(X, Y ) ∈R × B},
where B ⊆R is a Borel set.
Notice that we can always ﬁnd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ﬁnd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables I
 
8 
1. Some theoratical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deﬁned as the
function F : R2 →R, given by
F(x, y) := P{X ≤x ∧Y ≤y}.
We have
• If (x, y) ∈R2, then 0 ≤F(x, y) ≤1.
• If x ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisﬁes the condition limy→−∞F(x, y) = 0.
• If y ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisﬁes the condition limx→−∞F(x, y) = 0.
• When both x and y tend towards inﬁnity, then
lim
x, y→+∞F(x, y) = 1.
• If x1, x2, y1, y2 ∈R satisfy x1 ≤x2 and y1 ≤y2, then
F (x2, y2) −F (x1, y2) −F (x2, y1) + F (x1, y2) ≥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ﬁnd the distribution functions
of X and Y by the formulæ
FX(x) = F(x, +∞) =
lim
y→+∞F(x, y),
for x ∈R,
Fy(x) = F(+∞, y) =
lim
x→+∞F(x, y),
for y ∈R.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 →R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
−∞
 y
−∞
f(t, u) du

dt,
for (x, y) ∈R2.
In this case we can ﬁnd the function f(x, y) at the diﬀerentiability points of F(x, y) by the formula
f(x, y) = ∂2F(x, y)
∂x∂y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +∞
−∞
f(x, y) dy,
for x ∈R,
Download free eBooks at bookboon.com

Random variables I
 
9 
1. Some theoratical results
and
fY (y) =
 +∞
−∞
f(x, y) dx,
for y ∈R.
It was mentioned above that one far from always can ﬁnd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deﬁned on the same probability ﬁeld (Ω, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B ⊆R,
P{X ∈A ∧Y ∈B} = P{X ∈A} · P{Y ∈B},
which can also be put in the simpler form
F(x, y) = FX(x) · FY (y)
for every (x, y) ∈R2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi · gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) · fY (y)
almost everywhere.
The concept “almost everywhere” is rarely given a precise deﬁnition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ﬁnite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let ϕ : R →R and ψ : R →R be
given functions. Then ϕ(X) and ψ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables I
 
10 
1. Some theoratical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X ∈I} = 1.
2) Let τ : I →J be a bijective map of I onto an open interval J.
3) Furthermore, assume that τ is diﬀerentiable with a continuous derivative τ ′, which satisﬁes
τ ′(x) ̸= 0
for alle x ∈I.
Under the assumptions above Y := τ(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =
⎧
⎪
⎨
⎪
⎩
f
	
τ −1(y)

·

	
τ −1
′ (y)
 ,
for y ∈J,
0,
otherwise.
We note that if just one of the assumptions above is not fulﬁlled, then we shall instead ﬁnd the
distribution function G(y) of Y := τ(X) by the general formula
G(y) = P{τ(X) ∈] −∞, y]} = P

X ∈τ ◦−1(] −∞, y])

,
where τ ◦−1 = τ −1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisﬁed, then τ is necessarily monotone.
At a ﬁrst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deﬁne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞h(x, z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞h(x, x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞h

x , z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +∞
−∞h(zx , x) · |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables I
 
11 
1. Some theoratical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞f(x)g(z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞f(x)g(x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞f(x) g
 z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +∞
−∞f(zx)g(x) · |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) · FY (u)
for u ∈R,
and
FV (v) = 1 −(1 −FX(v)) · (1 −FY (v))
for v ∈R.
These formulæ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables I
 
12 
1. Some theoratical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) · fY (u) + fX(u) · FY (u),
for u ∈R,
and
fV (v) = (1 −FX(v)) · fY (v) + fX(v) · (1 −Fy(v)) ,
for v ∈R,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps ϕ = (ϕ1 , ϕ2) : R2 →R2, or subsets of R2.
We shall need the Jacobian of ϕ, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deﬁne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ˜D is an open domain in the
(y1 , y2) plane. Then let ϕ = (ϕ1 , ϕ2) be a bijective map of ˜D onto D with the inverse τ = ϕ−1, i.e.
the opposite of what one probably would expect:
ϕ = (ϕ1 , ϕ2) : ˜D →D,
with (x1 , x2) = ϕ (y1 , y2) .
The corresponding Jacobian is deﬁned by
Jϕ = ∂(x1 , x2)
∂(y1 , y2) =

∂ϕ1
∂y1
∂ϕ2
∂y1
∂ϕ1
∂y1
∂ϕ2
∂y2

,
where the independent variables (y1 , y2) are in the “denominators”. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D →R is an integrable
function, where D ⊆R2 is given as above, then for every (measurable) subset A ⊆D,

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
h (x1 , x2) ·

∂(x1 , x2)
∂(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we “delete the y-s”. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
(ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·
Jϕ (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables I
 
13 
1. Some theoratical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D ⊆R2 be an open domain, such that
P {(X1 , X2) ∈D} = 1.
Let τ : D →˜D be a bijective map of D onto another open domain ˜D, and let ϕ = (ϕ1 , ϕ2) =
τ −1, where we assume that ϕ1 and ϕ2 have continuous partial derivatives and that the corresponding
Jacobian is diﬀerent from 0 in all of ˜D.
Then the 2-dimensional random variable
(Y1 , Y2) = τ (X1 , X2) = (τ1 (X1 , X2) , τ2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
h (ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·

∂(x1 , x2)
∂(y1 , y2)
 ,
for (y1 , y2) ∈˜D,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deﬁne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi ∧Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ﬁxed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} · P {Y = yj} .
Analogously we deﬁne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X ≤x | Y = y} =
 x
−∞f(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deﬁned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that “0 times undeﬁned = 0”. Then we get the Law of total probability,
 +∞
−∞
f(x | y) · fY (y) dy =
 +∞
−∞
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables I
 
14 
1. Some theoratical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deﬁned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deﬁne the mean, or expectation
of X by
E{X} =
 +∞
−∞
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +∞[,
and the mean exists, then the mean is given by
E{X} =
 +∞
0
P{X ≥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = ϕ(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = ϕ(X) is given by
E{ϕ(X)} =

i
ϕ (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = ϕ(X) is
given by
E{ϕ(X)} =
 +∞
−∞
ϕ(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean μ. We add the following concepts, where k ∈N:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X −μ)k
.
The k-th absolute central moment,
E

|X −μ|k
.
The variance, i.e. the second central moment,
V {X} = E

(X −μ)2
,
Download free eBooks at bookboon.com

Random variables I
 
15 
1. Some theoratical results
provided that the deﬁning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = μ and variance V {X}. Then
E

(X −c)2
= V {X} + (μ −c)2
for every c ∈R,
V {X} = E

X2
−(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b ∈R,
V {aX + b} = a2V {X}
for every a, b ∈R.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diﬀers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (ˇCebyˇsev’s inequality). If the random variable X has the mean μ and the variance
σ2, then we have for every a > 0,
P{|X −μ| ≥a} ≤σ2
a2 .
If we here put a = kσ, we get the equivalent statement
P{μ −kσ < X < μ + kσ} ≥1 −1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables I
 
16 
1. Some theoratical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = ϕ(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

i, j
ϕ (xi , yj) · P {X = xi ∧Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

R2 ϕ(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and ϕ(x, y) = ϕ1(x) + ϕ2(y),
then
E {ϕ1(X) + ϕ2(Y )} = E {ϕ1(X)} + E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose ϕ(x, y) = ϕ1(x)·ϕ2(y), then also
E {ϕ1(X) · ϕ2(Y )} = E {ϕ1(X)} · E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular we get under the assumptions above
that
E{X · Y } = E{X} · E{Y },
and
E{(X −E{X}) · (Y −E{Y })} = 0.
These formulæ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ﬁnd a measure of how much they
“depend” on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = μX,
E{Y } = μY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
Download free eBooks at bookboon.com

Random variables I
 
17 
1. Some theoratical results
all exist. We deﬁne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X −μX) · (Y −μY )} .
We deﬁne the correlation between X and Y , denoted by ϱ(X, Y ), as
ϱ(X, Y ) := Cov(X, Y )
σX · σY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = μX,
E{Y } = μY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X · Y } −E{X} · E{Y },
|Cov(X, Y )| ≤σX · σy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
ϱ(X, Y ) = 0,
if X and Y are independent,
ϱ(X, X) = 1,
ϱ(X, −X) = −1,
|ϱ(X, Y )| ≤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b ∈R,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
ϱ(U, V ) = ϱ(aX + b, cY + d) = ϱ(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
j−1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deﬁned on the same probability
ﬁeld (Ω, F, P).
Download free eBooks at bookboon.com

Random variables I
 
18 
1. Some theoratical results
1) We say that Xn converges in probability towards a random variable X on the probability ﬁeld
(Ω, F, P), if
P {|Xn −X| ≥ε} →0
for n →+∞,
for every ﬁxed ε > 0.
2) We say that Xn converges in probability towards a constant c, if every ﬁxed ε > 0,
P {|Xn −c| ≥ε} →0
for n →+∞.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
n→+∞Fn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ﬁrst one resembles ˇCebyˇsev’s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deﬁned on (Ω, F, P), and assume that they all have the same mean and variance,
E {Xi} = μ
and
V {Xi} = σ2.
Then for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −μ
 ≥ε

→0
for n →+∞.
A slightly diﬀerent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deﬁned
on (Ω, F, P) where E {Xi} = μ, (notice that we do not assume the existence of the variance), then
for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −μ
 ≥ε

→0
for n →+∞.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Bray’s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a ≤Xn ≤b} = 1
for every n ∈N.
If ϕ is a continuous function on the interval [a, b], then
lim
n→+∞E {ϕ (Xn)} = E{ϕ(X)}.
In particular,
lim
n→+∞E {Xn}
and
lim
n→+∞V {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables I
 
19 
1. Some theoratical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables I
 
20 
2. Simple introduction examples
2
Simple introducing examples
Example 2.1 A motorist shall pass 4 traﬃc lights. We assume that at each of the traﬃc lights there
is the probability p that he must stop. There is furthermore such a long distance between the traﬃc
signals that there is no synchronization between them. Let X be the random variable, which indicates
the number of stops. Find the distribution of X. Sketch in the case p = 1
2 the corresponding diagram.
Let Y have the value k, if the ﬁrst stop is at signal number k, k = 1, 2, 3, 4. Is Y a random variable?
In this case the model is given by the binomial distribution X ∈B(4, p), thus
P{X = k} =
 4
k

pk(1 −p)4−k,
k = 0, 1, 2, 3, 4.
We deﬁne here a “success” as a stop (what we otherwise would not consider as a success).
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
1
2
3
4
Figure 1: The diagram, for p = 1
2.
We get in particular For p = 1
2,
P{X = k} =

4
k
 1
2
4
,
k = 0, 1, 2, 3, 4,
thus
p0 = p4 = 1
16,
p1 = p3 = 4
16 = 1
4,
p2 = 6
16 = 3
8.
If the ﬁrst stop is at signal number k, the car has not stopped earlier, so
P{Y = k} = (1 −p)k−1p,
k = 1, 2, 3, 4.
Download free eBooks at bookboon.com

Random variables I
 
21 
2. Simple introduction examples
Then
4

k=1
P{Y = k} =
4

k=1
p(1 −p)k−1 = 1 −(1 −p)4 < 1,
n˚ar p < 1.
Since the sum is not equal to 1, we conclude that Y is not a random variable.
The reason why Y is not a random variable, is that we have in the setup forgotten the possibility of
“no stops at all” of the probability (1 −p)4. If we add this value to Y for this event (where one e.g.
arbitrarily could let it correspond to the event Y = π2), then Y becomes a random variable. A more
reasonable deﬁnition would of course be Y = 5. However, there is nothing wrong in choosing Y = π2.
Example 2.2 A random variable X can have the possible values 1, 2, . . . of the probabilities
P{X = k} = A qk
k ,
k ∈N
(where q ∈]0 , 1[).
Find the constant A.
We put p = 1 −q. Since
∞

k=1
P{X = k} = 1,
it follows from the series of logarithm that
1 =
∞

k=1
P{X = k} = A
∞

k=1
qk
k = −A

−
∞

k=1
qk
k

= −A ln(1 −q) = A ln 1
p.
From 1
p > 1 follows that ln 1
p > 0, hence
A =
1
ln 1
p
=
1
| ln p|,
and thus
P{X = k} =
qk
k | ln(1 −q)|,
k ∈N.
Download free eBooks at bookboon.com

Random variables I
 
22 
3. Frequencies and distribution functions in 1 dimension
3
Frequencies and distribution functions in 1 dimension
Example 3.1 Check if the function
f(x) =
⎧
⎨
⎩
1
2 −kx,
x ∈[0, 6],
0
otherwise,
is a frequency for some k.
If f(x) is a frequency, then the following two conditions must be fulﬁlled:
1) f(x) ≥0 for every x ∈R,
2)
 ∞
−∞f(x) dx = 1.
By putting x = 6 into (1) we get 1
2 −6k ≥0, thus k ≤1
12.
A computation of (2) gives
1 =
 6
0
1
2 −kx

dx =
1
2 x −1
2 kx2
6
0
= 3 −18k,
hence
k = 3 −1
18
= 1
9 > 1
12.
The two requirements can never be satisﬁed simultaneously, so f(x) is not a frequency for any k ∈R.
Example 3.2 Find k, such that
f(x) =
⎧
⎨
⎩
kx2 	
1 −x3
,
x ∈[0, 1],
0,
otherwise,
is a frequency of a random variable, and sketch the function.
Find the median of X.
Obviously, f(x) ≥0. Then by an integration,
 1
0
x2 	
1 −x3
dx =
 1
0
	
x2 −x5
dx = 1
3 −1
6 = 1
6.
If we choose k = 6, then f(x) becomes a frequency, thus
f(x) =
⎧
⎨
⎩
6x2 	
1 −x3
= 6x2 −6x5
for x ∈[0, 1],
0
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
23 
3. Frequencies and distribution functions in 1 dimension
Since f(0) = f(1) = 0, and f ′(x) = 12x −30x4 = 0 for x =
3

2
5 ≈0, 74, we have a (global) maximum
18
5 ·
2
5
 2
3
≈1.95.
The function f is increasing in

0,
3

2
5

and decreasing in

3

2
5

.
The distribution function F(x) is in the interval [0, 1] given by
F(x) =
 x
0
f(t) dt =
 x
0

6t2 −6t5
dt = 2x3 −x6.
The median is given as the x ∈]0, 1[, for which F(x) = 1
2, thus
2x3 −x6 = 1
2,
or
x6 −2x3 + 1 = 1
2,
i.e.
	
x3 −1

2 = 1
2.
We get x3 = 1 ±
√
2
2 . However, since x ∈]0, 1[, we can only use the sign −, and the median is (here
uniquely determined by)
xmed =
3

1 −
√
2
2
≈0.66.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
24 
3. Frequencies and distribution functions in 1 dimension
It is possible to apply MAPLE, e-g. by:
> f:=x->6*x^2*(1-x^3);
f := x −→6x2 	
1 −x3
> plot(f,0..1,color=black);
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
> F:=int(f(x),x=0..t);
F := −t6 + 2t3
> fsolve(2*t^3-t^5 = 1/2,t=0..1);
.6641045243
> solve(2*t^3-t^6 = 1/2,t);
1
2(8 + 4
√
2)1/3, −1
4(8 + 4
√
2)1/3,
−1
4(8 + 4
√
2)1/3 −1
4I
√
3(8 + 4
√
2)1/3, 1
2(8 −4
√
2)1/3,
−1
4(8 −4
√
2)1/3 + 1
4I
√
3(8 −4
√
2)1/3, −1
4(8 −4
√
2)1/3 −1
4I
√
3(8 −4
√
2)1/3
> 1/2*(8-4*sqrt(2))^(1/3); # this is the median
1
2(8 −4
√
2)1/3
> F1:=x->2*x^3-x^6;
F1 := x −→2x3 −x6
Download free eBooks at bookboon.com

Random variables I
 
25 
3. Frequencies and distribution functions in 1 dimension
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
> plot(F1,0..1,color=black);
The former ﬁgure shows the graph of the frequency, and the latter ﬁgure shows the graph of the
distribution function. Notice the diﬀerence between using fsolve or solve.
With the exception of the sketches of the graphs we see that it is easy to perform the same computations
without using MAPLE. Furthermore, the MAPLE program is also less transparent then an explanation
in plain words.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables I
 
26 
3. Frequencies and distribution functions in 1 dimension
Example 3.3 A random variable X has the frequency
f(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
C(x −a),
a ≤x ≤a + b
2
,
C(b −x),
a + b
2
≤x ≤b,
0,
otherwise.
Find the constant C and the distribution function.
Compute
P

X ≤a + b
2

and
P
2a + b
3
≤X ≤a + 2b
3

.
This distribution is called the triangular distribution over ]a, b[.
b
a
Figure 2: The graph of the frequency f.
1) By considering the graph we immediately get
1 =
 ∞
−∞
f(x) dx = 1
2 · C b −a
2
· (b −a) = C
b −a
2
2
,
because the integral can be interpreted as the area of a triangle. Then
C =

2
b −a
2
.
2) When a ≤x ≤a + b
2
the distribution function is given by
F(x) =
 x
a
C(t −a) dt = C
(t −a)1
2
x
a
= 1
2

2
b −a
2
(x −a)2 = 2
x −a
b −a
2
.
Download free eBooks at bookboon.com

Random variables I
 
27 
3. Frequencies and distribution functions in 1 dimension
If instead a + b
2
≤x ≤b, then
F(x)
=
1
2 +
 x
a+b
2
C(b −t) dt = 1
2 +

2
b −a
2 
−(b −t)2
2
x
a+b
2
=
1
2 + 1
2

2
b −a
2 b −a
2
2
−(b −x)2

= 1
2 + 1
2 −2
b −x
b −a
2
=
1 −2
b −x
b −a
2
.
Summing up we get
F(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
for x ≤a,
2
x −a
b −a
2
,
for a < x ≤a + b
2
,
1 −2
b −x
b −a
2
,
for a + b
2
< x ≤b,
1,
for x > b.
0.2
0.4
0.6
0.8
1
–1
–0.5
0.5
1
x
Figure 3: The distribution function for a = −1 and b = 1.
3) By considering the graph (or by insertion of x a+b
2 ) we get
P

X ≤a + b
2

= F
a + b
2

= 1
2.
Another consideration of the graph gives
P
2a + b
3
≤X ≤a + 2b
3

= 1 −2 F
2a + b
3

= 1 −2 · 2 ·
⎧
⎪
⎨
⎪
⎩
b −a
3
b −a
⎫
⎪
⎬
⎪
⎭
2
= 1 −4
9 = 5
9.
Download free eBooks at bookboon.com

Random variables I
 
28 
3. Frequencies and distribution functions in 1 dimension
Example 3.4 Prove for some choice of the constant k that the function
f(x) = k · e−|x−2|,
x ∈R,
is the frequency of a random variable X.
Find the distribution function of X, and compute P{−1 ≤X ≤3} and P{X ≥0}.
Find the median of X.
Obviously, f(x) is continuous, and f(x) > 0, when k > 0. The remaining condition of a frequency is
that
 ∞
−∞f(x) dx = 1. Then by a computation,
 ∞
−∞
f(x) dx = k
 2
−∞
ex−2 dx +
 ∞
2
e−(x−2) dx

= 2k,
which is equal to 1 for k = 1
2.
0.1
0.2
0.3
0.4
0.5
–1
0
1
2
3
4
5
x
Figure 4: The graph of the frequency f. (NB Diﬀerent scales on the axes).
The random variable X has the frequency
f(x) = 1
2 e−|x−2|,
x ∈R.
Now
 x
−∞
f(t) dt =
 x
−∞
1
2 et−2 dt = 1
2 ex−2
for x ≤2,
and
 x
−∞
f(t) dt
=
 2
−∞
1
2 et−2 dt +
 x
2
e−(t−2) dt = 1
2 +
1
2 −1
2 e−(x−2)

=
1 −1
2 e−(x−2)
for x > 2,
Download free eBooks at bookboon.com

Random variables I
 
29 
3. Frequencies and distribution functions in 1 dimension
so the distribution function becomes
F(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
2 ex−2,
for x ≤2,
1 −1
2 e−(x−2),
for x > 2.
0.2
0.4
0.6
0.8
–1
0
1
2
3
4
5
x
Figure 5: The graph of the distribution function F.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables I
 
30 
3. Frequencies and distribution functions in 1 dimension
Finally,
P{−1 ≤X ≤3} = F(3) −F(−1) = 1 −1
2 e−1 −1
2 e−3 ≈0.79,
and
P{X ≥0} = 1 −F(0) = 1 −1
2 e−2 ≈0.93.
Since F(2) = 1
2, it is obvious that 2 is the median of X.
Example 3.5 Prove that the function
f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
k x exp

−1
2 x2

,
x ≥0,
0,
x < 0,
for some choice of the constant k, can be considered as the frequency of a random variable X.
Find the distribution function F of X.
Sketch the graph of the function f and of the function F.
Find the median of X.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.5
1
1.5
2
2.5
3
x
Figure 6: The graph of the frequency f.
1) If k > 0, then f(x) ≥0. The requirement for f(x) being a frequency is then reduced to
1 =
 ∞
−∞
f(x) dx = k
 ∞
0
x · exp

−1
2 x2

dx = k
 ∞
0
e−u du = k,
Download free eBooks at bookboon.com

Random variables I
 
31 
3. Frequencies and distribution functions in 1 dimension
where we have used the substitution u = 1
2 x2 with du = x dx. Consequently, k = 1, and
f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x exp

−1
2 x2

,
x ≥0,
0,
x < 0.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 7: The graph of the distribution functions F.
2) If x > 0, we use the substitution u = 1
2 t2 to obtain
F(x) =
 x
0
f(t) dt =
 x
0
t exp

−1
2 t2

dt =

1
2 x2
0
e−u du = 1 −exp

−1
2 x2

.
Hence, the distribution function of X is
F(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1 −exp

−1
2 x2

,
for x > 0,
0,
for x ≤0.
3) Consider the previous ﬁgures. We see that f(x) has a maximum for x =
1
√e, and a turning point
for F(x) at the same point.
4) The median is found from the equation
F(x) = 1 −exp

−1
2 x2

= 1
2,
i.e.
exp
1
2 x2

= 2,
thus 1
2 x2 = ln 2, and hence
(X) =
√
2 ln 2 ≈1.1774.
Download free eBooks at bookboon.com

Random variables I
 
32 
3. Frequencies and distribution functions in 1 dimension
Example 3.6 Prove that the function
f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
b
θ
x
θ
b−1
exp

−
x
θ
 b
,
x > 0,
0,
x ≤0,
where b and θ are positive constants, is the frequency of a random variable X, and ﬁnd the distribution
function of it.
Prove that P{X ≤θ} does not depend on b.
This distribution is called a Weibull distribution.
Clearly, f(x) ≥0.
We get for x > 0 by the substitution,
s = t
θ
followed by
u = sb =
 t
θ
b
,
that
F(x)
=
 x
0
b
θ
 t
θ
b−1
exp
!
−
 t
θ
b"
dt =

x
θ
0
b sb−1 exp
	
−sb
ds
=
 ( x
θ )
b
0
exp(−u) du = 1 −exp

−
x
θ
 b
→1
for x →∞.
We conclude that
F(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1 −exp

−
x
θ
 b
,
x > 0,
0,
x ≤0,
is the distribution function of a random variable X with f(x) as its frequency.
It follows by insertion that
P{X ≤θ} = F(θ) = 1 −exp
!
−
θ
θ
b"
= 1 −1
e
is independent of b.
Download free eBooks at bookboon.com

Random variables I
 
33 
3. Frequencies and distribution functions in 1 dimension
Example 3.7 A patient arrives to a doctor’s waiting room. The probability is p, where p ∈]0, 1[,
that he will be treated immediately; but if he does not, the probability that he must wait longer than
the time x is equal to e−ax, where a is some positive constant. Find the distribution function of the
random variable X, which indicates the waiting time.
1) If the patient is treated immediately, then the waiting time is X = 0, thus
P{X = 0} = p.
2) The probability that the patient must wait more than x, x ≥0, is
P{X > x} = P{pt. must wait} · P{waiting time > x | pt. must wait} = (1 −p)e−ax,
hence
P{X ≤x} = 1 −P{X > x} = 1 −(1 −p)e−ax.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
34 
3. Frequencies and distribution functions in 1 dimension
0
0.2
0.4
0.6
0.8
1
–1
–0.5
0.5
1
1.5
2
2.5
Figure 8: The graph of the distribution function F(x) when a = 1 and p = 1
2.
3) The distribution function F(x) = P{X ≤x} is here
F(x) =
⎧
⎨
⎩
0,
x < 0,
1 −(1 −p)e−x,
x ≥0.
The distribution of X is of mixed type, i.e. it is neither discrete nor continuous.
Download free eBooks at bookboon.com

Random variables I
 
35 
4. Frequencies and distributions functions, 2 dimensions
4
Frequencies and distributions functions, 2 dimensions
Example 4.1 Let X and Y be independent random variables with the frequencies
f(x) = x e−x,
x > 0,
g(y) = e−y,
y > 0,
(both frequencies are otherwise 0).
Find the frequency of X + Y .
Find the mean E{X}, E{Y } og E{X + Y }.
The frequency of X + Y is given by the convolution integral
k(z) =
 ∞
−∞
f(x) g(z −x) dx.
This expression is only > 0, when z > 0. We have furthermore the constraints z −x > 0 and x > 0,
so the convolution integral is reduced to
k(z) =
 z
0
f(x) g(z −x) dx =
 z
0
x e−x e−(z−x) dx = e−z
 z
0
x dx = 1
2 z2 e−z,
and k(z) = 0 for z ≤0.
The means are
E{X}
=
 ∞
0
x f(x) dx =
 ∞
0
x2 e−x = 2,
E{Y }
=
 ∞
0
y g(y) dy =
 ∞
0
y e−y dy = 1,
E{X + Y }
=
 ∞
0
z k(z) dz = 1
2
 ∞
0
z3 e−z dz = 3.
Remark 4.1 Here we are given that
 ∞
0
xne−x dx = n!
for n ∈N0.
This formula is easily proved by induction. When n = 0, it is trivial. In general we get by a partial
integration and the assumption above of the induction,
 ∞
0
xn+1e−x dx =
#
−xn2e−x$∞
0 + (n + 1)
 ∞
0
xne−x dx = (n + 1)n! = (n + 1)!,
and the claim follows. ♦
Download free eBooks at bookboon.com

Random variables I
 
36 
4. Frequencies and distributions functions, 2 dimensions
Example 4.2 Check if the function
F(x, y) =
⎧
⎨
⎩
1 −e−(x+y),
x ≥0, y ≥0,
0,
otherwise,
is a distribution function of a 2-dimensional random variable.
Since F ∈C∞(R+ × R+), we have
∂2F
∂x∂y = −e−(x+y) < 0
for (x, y) ∈R+ × R+,
so F cannot be a distribution function. In fact, if so then
f(x, y) = ∂2F
∂x∂y
(< 0)
should be a frequency, which is not possible, because frequencies are never negative.
Alternatively we prove that one of the necessary conditions is not fulﬁlled. Choose
x1 = y1 = 1
og
x2 = y2 = 1 + α,
α > 0.
Then
F(1 + α, 1 + α) −F(1, 1 + α) −F(1 + α, 1) + F(1, 1) = e−(1+α) −e−(2+2α) + e−(2+α) −e−2
= e−2 
2e−α −e−2α −1

= −e−2 	
1 −e−α
2 < 0,
and not ≥0, as it should be.
Example 4.3 Prove that the function
f(x, y) =
⎧
⎨
⎩
x e−x(y+1),
x > 0, y > 0,
0,
otherwise,
is a frequency of a 2-dimensional random variable (X, Y ).
Find the frequencies and the distribution functions of the random variables X and Y , and ﬁnd the
medians of these two distributions.
Check if the random variables X and Y are independent.
Clearly, f(x, y) ≥0 for every (x, y), and f is continuous, with the exception of the positive part of
the x-axis.
1) If x > 0 is kept ﬁxed, it follows by a vertical integration,
fX(x) = e−x
 ∞
y=0
x e−xy dy = e−x,
and fX(x) = 0 for x ≤0.
Download free eBooks at bookboon.com

Random variables I
 
37 
4. Frequencies and distributions functions, 2 dimensions
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.5
1
1.5
2
y
0.5
1
1.5
2
2.5
3
x
Figure 9: The graph of the frequency f(x, y).
2) If y > 0 is kept ﬁxed, we get by a horizontal integration, where we use the substitution z = x(y+1),
etc.,
fY (y) =
 ∞
x=0
x e−x(y+1) dy =
1
(y + 1)2 ,
and fY (y) = 0 for y ≤0.
3) It follows from
 ∞
0
e−x dx = 1,

possibly
 ∞
0
1
(y + 1)2 dy = 1

,
that f(x, y) is a frequency of a 2-dimensional random variable (X, Y ), and that X and Y have the
marginal frequencies fX(x) and fY (y), given in (1) and (2), resp..
Download free eBooks at bookboon.com

Random variables I
 
38 
4. Frequencies and distributions functions, 2 dimensions
4) The marginal distribution functions are
FX(x) =
 x
−∞
fX(t) dt =
⎧
⎨
⎩
1 −e−x,
for x > 0,
0,
for x ≤0,
and
FY (y) =
 y
−∞
fY (t) dt =
⎧
⎪
⎨
⎪
⎩
1 −
1
y + 1 =
y
y + 1,
for y > 0,
0,
for y ≤0.
5) Medians:
a) FX(x) = 1
2 = 1 −e−x for e−x = 1
2, hence x = (X) = ln 2.
b) Fy(y) = 1
2 = 1 −
1
y + 1 for
1
y + 1 = 1
2, thus y = (Y ) = 1.
6) Since
fX(x) · fY (y) =
e−x
(y + 1)2 ̸= x e−x(y+1) = f(x, y)
for x, y > 0,
X and Y are not independent.
Example 4.4 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
⎧
⎨
⎩
c x y,
0 < x < y < 1,
0,
otherwise.
Find the constant c. Find the frequencies and the distribution function of the random variables X and
Y . Check if the random variables X and Y are independent. Finally, ﬁnd the distribution function of
the 2-dimensional random variable (X, Y ).
1) If c > 0, then f(x, y) ≥0. It follows from
1 = c
 1
0
 y
0
xy dx

dy = c
 1
0
1
2 y3 dy = c
8,
that c = 8, hence the frequency is given by
f(x, y) =
⎧
⎨
⎩
8xy,
0 < x < y < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
39 
4. Frequencies and distributions functions, 2 dimensions
0
2
4
6
8
0.5
1
y
0.2
0.4
0.6
0.8
1
x
Figure 10: The graph of the frequency f(x, y) over 0 < x < y < 1.
0
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 11: The domain where f(x, y) > 0.
2) Clearly, fX(x) = 0 for x /∈]0, 1[. When x ∈]0, 1[ it follows by a vertical integration that
fX(x) =
 1
x
8xy dy = 8x
y2
2
1
x
= 4x −4x3,
hence the marginal frequency is
fX(x) =
⎧
⎨
⎩
4x −4x3,
x ∈]0, 1[,
0,
otherwise.
When x ∈]0, 1[, we get
FX(x) =
 x
0
fX(t) dt =
 x
0

4t −4t3
dt = 2x2 −x4,
Download free eBooks at bookboon.com

Random variables I
 
40 
4. Frequencies and distributions functions, 2 dimensions
thus the marginal distribution function is
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x ≤0,
2x2 −x4,
0 < x < 1,
1,
x ≥1.
Clearly, fY (y) = 0 for y /∈]0, 1[. If y ∈]0, 1[, we get by a horizontal integration that
fY (y) =
 y
0
8xy dx = 8y
x2
2
y
0
= 4y3,
and the marginal frequency is
fY (y) =
⎧
⎨
⎩
4y3,
y ∈]0, 1[,
0,
otherwise.
Then for y ∈]0, 1[,
FY (y) =
 y
0
fY (t) dt =
 y
0
4t3 dt = y4,
hence the marginal distribution function is
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
y4,
y ∈]0, 1[,
1,
y ≥1.
3) Since fX(x) · fY (y) ̸= f(x, y), we see that X and Y are not stochastically independent.
Remark 4.2 If in general the domain, in which the frequency f(x, y) > 0 is strictly positive, is not
a rectangle (possibly with inﬁnite sides, so e.g. R × R is in this sense considered as a degenerated
rectangle), then the random variables X and Y are never stochastic independent. ♦
4) If x, y ∈]0, 1[, then the distribution function is
F(x, y) =
 y
0
 x
0
f(t, u) dt

du,
so 0 ≤t ≤x ≤1, and 0 ≤u ≤y ≤1. Furthermore, f(t, u) = 8ty ̸= 0 for 0 < t < u < 1, and 0
otherwise, so 0 < t < min{x, u}, and thus
F(x, y) =
 y
0
 min{x,u}
0
8tu dt

du =
 y
0
4u · (min{x, u})2 du =
 y
0
4u · min

x2, u2
du.
If x ≥0, we get 0 < u < y ≤x ≤1, hence min

x2, u2
= u2, and thus
F(x, y) =
 y
0
4u · u2 du = y4.
Download free eBooks at bookboon.com

Random variables I
 
41 
4. Frequencies and distributions functions, 2 dimensions
If 0 ≤x ≤y, then we get instead
F(x, y)
=
 y
0
4u · min

x2, u2
du =
 x
0
4u3 du +
 y
x
4u x2 du
=
x4 + 2x2 	
y2 −x2
= 2x2y2 −x4.
Summing up the distribution function of the 2-dimensional random variable (X, Y ) is
F(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
2x2y2 −x4,
0 ≤x ≤y ≤1,
2x2 −x4,
0 ≤x ≤1 ≤y,
y4,
0 ≤y ≤x,
1,
1 ≤min{x, y},
0,
otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables I
 
42 
4. Frequencies and distributions functions, 2 dimensions
1
2*x^2-x^4
2*x^2*y^2-x^4
y^4
0
0
0
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
y
–0.4
–0.2
0.2
0.4
0.6
0.8
1
1.2
1.4
x
Figure 12: The distribution function F(x, y) of Example 4.4.
Example 4.5 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
⎧
⎨
⎩
ct2,
0 < x < y < 1,
0,
otherwise.
1) Find the constant c.
2) Find the frequencies and the distribution functions of the random variables X and Y .
3) Find the simultaneous distribution function of the 2-dimensional random variable (X, Y ).
1) If c > 0, then obviously f(x, y) ≥0. It follows from
1 = c
 1
0
 y
0
y2 dx

dy = c
 1
0
y3 dy = c
4,
that if c = 4, then
f(x, y) =
 4y2,
0 < x < y < 1,
0,
otherwise,
is the frequency of the 2-dimensional random variable (X, Y ).
2) By a vertical integration, x ∈]0, 1[ ﬁxed, we obtain the marginal frequency of X,
fX(x) =
 1
x
4y2 dy = 4
3
	
1 −x3
,
thus
fX(x) =
⎧
⎪
⎨
⎪
⎩
4
3
	
1 −x3
,
for x ∈]0, 1[,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
43 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 13: The graph of f(x, y).
y = x
x
y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 14: The domain 0 < x < y < 1.
When x ∈]0, 1[, the marginal distribution function of X is given by
FX(x) =
 x
0
4
3
	
1 −t3
dt = 4
3 x −1
3 x4,
Download free eBooks at bookboon.com

Random variables I
 
44 
4. Frequencies and distributions functions, 2 dimensions
hence
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
x ≤0,
1
3
	
4x −x4
,
0 < x < 1,
1,
x ≥1.
By a horizontal integration, y ∈]0, 1[ ﬁxed, we get the marginal frequency of Y ,
fY (y) =
 y
0
4y2 dx = 3y3,
hence
fY (y) =
⎧
⎨
⎩
4y3,
y ∈]0, 1[,
0,
otherwise.
It follows immediately by an integration that the marginal distribution function is given by
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
y4,
y ∈]0, 1[,
1,
y ≥1.
x.y
V
IV
III
II
I
Figure 15: The ﬁve sub-domains for the distribution function.
Download free eBooks at bookboon.com

Random variables I
 
45 
4. Frequencies and distributions functions, 2 dimensions
3) When the plane is divided into the ﬁve sub-domains I–V, it follows that
I F(x, y) = 1 for x ≥1 and y ≥1.
II F(x, y) = 0 for x ≤0 or y ≤0.
III F(x, y) = FX(x) = 1
3
	
4x −x4
for 0 < x < 1 and y ≥1.
IV F(x, y) = FY (y) = y4 for 0 < y ≤1 and x > y.
V Only here we need some computations. We keep (x, y) ﬁxed in domain V, thus 0 < x < y < 1, cf.
the ﬁgure. First take the inner integral [i.e. we ﬁrst integrate vertically] and then horizontally.
Then we get
F(x, y)
=
 x
0
 y
0
f(t, u) du

dt =
 x
0
 y
t
4u2 du

dt
=
 x
0
4
3 y3 −4
3 t3

dt = 4
3 xy3 −1
3 x4,
hence
F(x, y) = 1
3
	
4xy3 −x4
for 0 < x < y < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables I
 
46 
4. Frequencies and distributions functions, 2 dimensions
Example 4.6 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
⎧
⎨
⎩
cx,
0 < y < 2x < 2,
0,
otherwise.
1) Find the constant c.
2) Find the marginal frequencies and the distribution functions of the random variables X and Y .
3) (A hard question). Find the simultaneous distribution function F(x, y) of the 2-dimensional ran-
dom variable (X, Y ).
(The distribution function F(x, y) must be computed for every (x, y) ∈R2).
0
0.5
1
1.5
0.5
1
1.5
2
t
0.2
0.4
0.6
0.8
1
s
Figure 16: The graph of f(x, y), and its projection A, where f(x, y) > 0.
x
A
y = 2*x
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
Figure 17: The domain of integration A.
Download free eBooks at bookboon.com

Random variables I
 
47 
4. Frequencies and distributions functions, 2 dimensions
1) By means of a plane integral we get the condition (cf. the ﬁgure)
1 =

A
f(x, y) dx dy = c
 1
x=0
x
 2x
y=0
1 dy

dx = c
 1
0
2x2 dx = c · 2
3.
Therefore, if we choose c = 3
2, then f(x, y) ≥0 everywhere, and its integral is 1, so the frequency
is
f(x, y) =
⎧
⎪
⎨
⎪
⎩
3
2 x,
0 < y < 2x < 2,
0,
otherwise.
2) The marginal frequencies are found to be
fX(x) =
 2
y=0
x3
2 x dy = 3x2
for 0 < x < 1,
and
fY (y) =
 1
x= y
2
3
2 x dx =
3
4 x2
1
x= y
2
= 3
4 −3
16 y2,
for 0 < y < 2,
hence
fX(x) =
⎧
⎨
⎩
3x2,
0 < x < 1,
0,
otherwise,
fY (y) =
⎧
⎪
⎨
⎪
⎩
3
4 −3
16 y2,
0 < y < 2,
0,
otherwise.
We ﬁnd the remaining distribution functions by an integration:
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x < 0,
x3,
0 ≤x ≤1,
1,
x > 1,
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
y < 0,
3
4 y −1
16 y3,
0 ≤y ≤2,
1,
y > 2.
3) (The hard question). We divide R2 into the ﬁve domains I–V, cf. the ﬁgure.
Clearly,
F(x, y) = 0
in domain I,
F(x, y) = 1
in domain II.
In domain III (i.e. A) we get by ﬁrst integrating horizontally, u
2 ≤t ≤x, and then vertically,
0 ≤u ≤y,
F(x, y)
=
3
2
 y
u=0
 x
t= u
2
t dt

du = 3
4
 y
u=0

x2 −u2
4

du
=
3
4

x2u −1
12 u3
y
u=0
= 3
4 x2y −1
16 y3.
Download free eBooks at bookboon.com

Random variables I
 
48 
4. Frequencies and distributions functions, 2 dimensions
V
IV
III
II
I
x.y
y = 2*x
Figure 18: The ﬁve domains of the distribution function.
We get in domain IV,
F(x, y) = F(1, y) = FY (y) = 3
4 y −1
16 y3,
and in domain V,
F(x, y) = F(x, 2x) = 3
4 2x3 −1
16 · 8x4 (= FX(x)) = x3.
Summing up we obtain the distribution function
F(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
for x ≤0 or y ≤0,
1,
for x ≥1 and y ≥2,
3
4 x2y −1
16 y3,
for 0 ≤y ≤2x ≤2,
3
4 y −1
16 y3,
for x ≥1 and 0 ≤y ≤2,
x3,
for y ≥2x and 0 ≤x ≤1.
Download free eBooks at bookboon.com

Random variables I
 
49 
4. Frequencies and distributions functions, 2 dimensions
Example 4.7 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
⎧
⎨
⎩
cx,
0 ≤x ≤1, 0 ≤y ≤1 −x,
0,
otherwise.
1) Find the constant c.
2) Find the frequencies and the distribution functions of the random variables X and Y .
3) Find the simultaneous distribution function of (X, Y ).
0
1
2
3
4
5
6
0.2
0.4
0.6
0.8
1
t
0.2
0.4
0.6
0.8
1
s
Figure 19: The graph of the frequency f(x).
y = 1-x
x
y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 20: The domain of integration of the frequency f(x).
Download free eBooks at bookboon.com

Random variables I
 
50 
4. Frequencies and distributions functions, 2 dimensions
1) If c > 0, then f(x, y) ≥0. It follows from the condition
1 = c
 1
0
 1−x
0
x dy

dx = c
 1
0
	
x −x2
dx = c
1
2 −1
3

= c
6,
that if c = 6, then the frequency of (X, Y ) is given by
f(x, y) =
⎧
⎨
⎩
6x,
0 ≤x ≤1, 0 ≤y ≤1 −x,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
–0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 21: The graph of the frequency fX(x) of X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
51 
4. Frequencies and distributions functions, 2 dimensions
2) It follows by a vertical integration, x ∈]0, 1[ ﬁxed, that
fX(x) =
 1−x
0
6x dy = 6x −6x2,
so the frequency of X is
fX(x) =
⎧
⎨
⎩
6x −6x2,
x ∈]0, 1[,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
–0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 22: The distribution function FX(x) of X.
0
0.5
1
1.5
2
2.5
3
0.2 0.4 0.6 0.8
1
1.2
x
Figure 23: The graph of the frequency fY (y).
A horizontal integration, y ∈]0, 1[ ﬁxed, gives
fY (y) =
 1−y
0
6x dx = 3(1 −y)2,
Download free eBooks at bookboon.com

Random variables I
 
52 
4. Frequencies and distributions functions, 2 dimensions
hence the frequency of Y is
fY (y) =
⎧
⎨
⎩
3(1 −y)2,
0 < y < 1,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
–0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 24: The distribution function FY (y).
By an integration, for y ∈]0, 1[ ﬁxed,
FY (y) =
 y
0
3(1 −t)2 dt =
#
−(1 −t)3$y
0 = 1 −(1 −y)3 = 3y −3y2 + y3,
so the distribution function of Y is
FY (u) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
1 −(1 −y)3,
0 < y < 1,
1,
y ≥1.
3) If we divide the plane into the domains I–VI, it follows that
I F(x, y) = 1 for x ≥1 and y ≥1.
II F(x, y) = 0 for x ≤0 or y ≤0.
III F(x, y) = FX(x) = 3x2 −2x3 for 0 < x < 1 and y ≥1.
IV F(x, y) = FY (y) = 1 −(1 −y)3 for x ≥1 and 0 < y < 1.
Download free eBooks at bookboon.com

Random variables I
 
53 
4. Frequencies and distributions functions, 2 dimensions
VI
V
IV
III
II
I
Figure 25: The domains I–VI.
V
x
1-y
y
x.y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 26: The domain of integration in case V.
V Let 0 ≤x ≤1 and 1 −x ≤y ≤1. Then
F(x, y)
=
 1−y
0
 y
0
6u du

dt +
 x
1−y
 1−t
0
6t du

dt
=
 1−y
0
3y2 dt +
 x
1−y
	
6t −6t2
dt
=
3y2(1 −y) +
#
3t2 −2t3$x
1−y
=
3y2(1 −y) + 3x2 −2x3 −3(1 −y)2 + 2(1 −y)3
=
3x2 −2x3 + (1 −y)

3y −3y2 −3 + 3y + 2 −4y + 2y2
=
3x2 −2x3 + (1 −y)
	
−1 + 2y −y2
=
3x2 −2x3 −(1 −y)3.
Download free eBooks at bookboon.com

Random variables I
 
54 
4. Frequencies and distributions functions, 2 dimensions
VI Finally, if 0 ≤x ≤1 and 0 ≤y ≤1 −x, then
F(x, y) =
 x
0
 y
0
6t du

dt = 3x2y.
Summing up we get
F(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1,
for x ≥1 and y ≥1,
0,
for x ≤0 or y ≤0,
3x2 −2x3,
for 0 < x < 1 and y ≥1,
1 −(1 −y)3,
for x ≥1 and 0 < y < 1,
3x2 −2x3 −(1 −y)3,
for 0 ≤x ≤1 and 1 −x ≤y ≤1,
3x2y,
for 0 ≤x ≤1 and 0 ≤y ≤1 −x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables I
 
55 
4. Frequencies and distributions functions, 2 dimensions
Example 4.8 Let X1 and X2 be independent and identically distributed random variables of the
frequencies
f(x) =
⎧
⎪
⎨
⎪
⎩
1
2 x,
0 < x < 2,
0,
otherwise,
and let the 2-dimensional random variable
(Y1, Y2) = τ (X1, X2)
be given by
Y1 = X1X2,
Y2 = X1
X2
.
1. Compute the means E {X1} and E
 1
X1

.
2. Compute the means of Y1 and for Y2.
The vector function τ, given by
τ (x1, x2) =

x1x2, x1
x2

,
maps D = ]0, 2[ × ]0, 2[ bijectively onto
D′ =

(y1, y2) ∈R2 | 0 < y1 < 4y2, y1y2 < 4

.
3. Sketch D′, and ﬁnd the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
5. Are Y1 and Y2 independent?
1) It follows that
E {X1} = 1
2
 2
0
x2 dx = 1
2
x3
3
2
0
= 4
3,
and
E
 1
X1

= 1
2
 2
0
x
x dx = 1.
2) Since X1 and X2 are independent, we conclude from (1) that
E {Y1} = E {X1X2} = E {E1} E {X2} =
4
3
3
= 16
9 ,
and
E {Y2} = E
X1
X2

= E {X1} · E
 1
X2

= 4
3 · 1 = 4
3.
Download free eBooks at bookboon.com

Random variables I
 
56 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
1
2
3
4
Figure 27: The domain D′ lies between the y2-axis, the hyperbola y1y2 = 4 and the straight line
y1 = 4y2.
3) It follows from y1 = x1x2 and y2 = x1
x2
that
x1 = √y1y2
and
x2 =
y1
y2
.
Hence the Jacobian becomes
∂(x1, x2)
∂(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2
1
√y1y2
−1
2
1
y2
y1
y2

= −1
4
 1
y2
+ 1
y2

= −1
2y2
.
The simultaneous frequency of (X1, X2) is
g (x1, x2) f (x1) · f (x2) =
⎧
⎪
⎨
⎪
⎩
1
4 x1x2,
for (x1, x2) ∈]0, 2[ × ]0, 2[,
0
otherwise,
so the simultaneous frequency of (Y1, Y2) is
k (y1, y2) =
⎧
⎪
⎨
⎪
⎩
1
8
y1
y2
for (y1, y2) ∈D′,
0
otherwise.
4) The marginal frequency of Y1 for 0 < y1 < 4 is given by
kY1 (y1) = y1
8

4
y1
y1
4
1
y2
dy2 = y1
8 [ln y2]
4
y1
y1
4 = y1
4 ln
 4
y1

= y1
4 (ln 4 −ln y1) ,
and = 0 otherwise.
The marginal frequency of Y2 for y2 ∈]0, 1] is given by
kY2 (y2) =
1
8y2
 4y2
0
y1 dy1 =
1
16y2
· 16y2
2 = y2.
Download free eBooks at bookboon.com

Random variables I
 
57 
4. Frequencies and distributions functions, 2 dimensions
If instead y2 ∈]1, ∞[, then
kY2 (y2) =
1
8y2

4
y2
0
y1 dy1 = 1
y3
2
.
Summing up we get
kY1 (y1) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
y1
4 ln
 4
y1

,
0 < y1 < 4,
0,
otherwise,
and
kY2 (y2) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
y2,
for y2 ∈]0, 1],
1
y3
2
,
for y2 ∈]1, ∞[,
0,
for y2 ≤0.
5) Since D′ is not a rectangular domain, we conclude that Y1 and Y2 cannot be independent.
There is clearly a trap here, because we get for (y1, y2) ∈D′,
k (y1, y2) = 1
8 · y1 · 1
y2
,
in which y1 and y2 apparently are separated.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables I
 
58 
4. Frequencies and distributions functions, 2 dimensions
Example 4.9 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
⎧
⎪
⎨
⎪
⎩
1
2 (x1 + x2) exp (−(x1 + x2)) ,
x1 > 0, x2,
0,
otherwise.
1. Compute the marginal frequencies of X1 and X2.
Then introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1 −X2.
2. Prove that Y1 are Y2 non-correlated.
The vector function τ given by
τ (x1, x2) = (x1 + x2, x1 −x2)
maps R+ × R+ bijectively onto
D′ =

(y1, y2) ∈R2 | |y2| < y1

.
3. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Compute the marginal frequency of Y1, and ﬁnd the mean of Y1.
5. Find the marginal frequency of Y2.
1) If x1 > 0, then
hX1 (x1) = 1
2 e−x1
 ∞
0
	
x1e−x2 + x2e−x2
dx2 = 1
2 (x1 + 1) e−x1,
and hX1 (x1) = 0 for x1 ≤0.
Then by symmetry, X1 and X2 have the same distribution, hence
hX2 (x2) =
⎧
⎪
⎨
⎪
⎩
1
2 (x2 + 1) e−x2,
for x2 > 0,
0,
for x2 ≤0.
2) It follows from (1) that V {X1} = V {X2}, thus
Cov (Y1, Y2) = Cov (X1 + X2, X1 −X2) = V {X1} −V {X2} + Cov (X1, X2) = 0,
which shows that Y1 and Y2 are non-correlated.
Notice that they are not independent, cf. (3), because the domain is not a rectangle parallel to
the coordinate axes.
Download free eBooks at bookboon.com

Random variables I
 
59 
4. Frequencies and distributions functions, 2 dimensions
3) It follows from y1 = x1 + x2 and y2 = x1 −x2 that
x1 = 1
2 (y1 + y2)
and
x2 = 1
2 (y1 −y2) ,
so the Jacobian becomes
∂(x1, x2)
∂(y1, y2) =

1
2
1
2
1
2
−1
2

= −1
2.
Hence,
k (y1, y2) =
⎧
⎪
⎨
⎪
⎩
1
4 y1e−y1,
for |y2| < y1,
0,
otherwise.
4) The marginal frequency of Y1 is
fY1 (y1) = 1
4 y1e−y1
 y1
−y1
dy2 = 1
2 y2
1e−y1
for y1 > 0,
and fY1 (y1) = 0 otherwise.
Since Y1 ∈Γ(3, 1) is gamma distributed, we get
E {Y1} = 3 · 1 = 2,
which can also be found directly from
E {Y1} = 1
2
 ∞
0
y3
1e−y1dy1 = 3!
2 = 3.
5) If y2 ∈R, then
fY2 (y2) = 1
4
 ∞
|y2|
y1e−y1 dy1 = 1
4
#
−(y1 + 1) e−y1$∞
|y2| = 1
4 (|y2| + 1) e−|y2|.
Download free eBooks at bookboon.com

Random variables I
 
60 
4. Frequencies and distributions functions, 2 dimensions
Example 4.10 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
⎧
⎪
⎨
⎪
⎩
1
2 (x1 + 1) e−(x1+x2),
x1 > 0,
x2 > 0,
0,
otherwise.
1. Compute the marginal frequencies of X1 and X2.
2. Compute the means of X1 and X2.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 =
X2
X1 + X2
.
The vector function τ, given by
τ (x1, x2) =

x1 + x2,
x2
x1 + x2

,
maps R+ × R+ bijectively onto D′ = R+× ]0, 1[.
3. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Compute the marginal frequencies of Y1 and Y2, and check if Y1 and Y2 are independent.
5. Compute the means of Y1 and Y2.
1) Since h (x1, x2) has a nice factorization,
h (x1, x2) = fX1 (x1) · fX2 (x2) ,
where
fX1 (x1) =
⎧
⎪
⎨
⎪
⎩
1
2 (x1 + 1) e−x1,
x1 > 0,
0,
x1 ≤0,
fX2 (x2) =
⎧
⎨
⎩
e−x2,
x2 > 0,
0,
x2 ≤0,
and fX1 (x1) ≥0 and fX2 (x2) ≥0, where
 ∞
−∞
fX1 (x1) dx1 = 1,
 ∞
−∞
fX2 (x2) dx2 = 1,
we have
a) found the marginal frequencies,
b) and shown that X1 and X2 are stochastically independent.
Alternatively we compute for x1 > 0, resp. x2 > 0,
fX1 (x1) =
 ∞
0
h (x1, x2) dx2 = 1
2 (x1 + 1) e−x1
 ∞
0
e−x2 dx2 = 1
2 (x1 + 1) e−x1,
Download free eBooks at bookboon.com

Random variables I
 
61 
4. Frequencies and distributions functions, 2 dimensions
and
fX2 (x2) =
 ∞
0
h (x1, x2) dx1 = 1
2 e−x2
 ∞
0
(x1 + 1) e−x1 dx1 = e−x2.
Here we have applied that
 ∞
0
xne−x dx = n!
for n ∈N0.
2) The means are
E {X1} =
 ∞
0
x1fX1 (x1) dx1 = 1
2
 ∞
0
	
x2
1 + x1

e−x1 dx1 = 1
2 (2! + 1!) = 3
2,
and
E {X2} =
 ∞
0
x2fX2 (x2) dx2 =
 ∞
0
x2 e−x2 dx2 = 1! = 1.
3) The transform formula is memorized by
h (x1, x2) dx1 dx2 = k (y1, y2)

∂(x1, x2)
∂(y1, y2)
 dy1 dy2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
62 
4. Frequencies and distributions functions, 2 dimensions
This formula shows that the task is to ﬁnd x1 and x2 expressed by (y1, y2). It follows from
⎧
⎪
⎨
⎪
⎩
y1 = x1 + x2,
y2 =
x2
x1 + x2
,
that
⎧
⎨
⎩
y1 = x1 + x2,
y1y2 = x2,
hence
⎧
⎨
⎩
x1 = y1 (1 −y2) ,
x2 = y1y2.
Thus we get the weight function
∂(x1, x2)
∂(y1, y2) =

1 −y2
−y1
y2
y1
 = y1 −y1y2 + y1y2 = y1 > 0,
because D′ = R+× ]0, 1[ is given.
In this domain we get the simultaneous frequency of (Y1, Y2),
k (y1, y2)
=
h (y1 (1 −y2) , y1y2) · y1 = 1
2 (y1 −y1y2 + 1) · e−y1 · y1
=
1
2
	
y2
1 −y2
1y2 + y1

e−y1
for (y1, y2) ∈D′,
and k (y1, y2) = 0 otherwise.
4) The marginal frequencies of Y1 and Y2 are computed for y1 > 0, resp. y2 ∈]0, 1[. (Otherwise they
are 0.)
kY1 (y1)
=
 1
y2=0
k (y1, y2) dy2 = 1
2 e−y1
 1
0
	
y2
1 −y2
1y2 + y1

dy2
=
1
2 e−y1

y2
1 −1
2 y2
1 + y1

=
1
4 y2
1 + 1
2 y1

e−y1,
kY2 (y2)
=
 ∞
y1=0
k (y1, y2) dy1 = 1
2
 ∞
0
	
y2
1 −y2
1y2 + y1

e−y1 dy1
=
1
2 (2 −2y2 + 1) = 3
2 −y2.
Since k (y1, y2) ̸= kY1 (y1) · kY2 (y2), we see that Y1 and Y2 are not independent.
5) The means are
E {Y1} = E {X1 + X2} = E {X1} + E {X2} = 3
2 + 1 = 5
2.
Alternatively,
E {Y1} =
 ∞
0
y1kY1 (y1) dy1 =
 ∞
0
1
4 y3
1 + 1
2 y2
1

e−y1 dy1 = 6
4 + 2
2 = 5
2.
For Y2 we get
E {Y2} =
 1
0
y2kY2 (y2) dy2 =
 1
0
3
2 y2 −y2
2

dy2 = 3
4 −1
3 = 5
12.
Download free eBooks at bookboon.com

Random variables I
 
63 
4. Frequencies and distributions functions, 2 dimensions
Example 4.11 Let (X, Y ) be a 2-dimensional random variable of the frequency
h(x, y) =
⎧
⎨
⎩
e−y,
for 0 ≤x ≤y,
0,
otherwise.
1) Find the marginal frequencies of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Find the variances V {X} and V {Y } of the random variables X and Y .
4) Compute E{XY }, and then the correlation ϱ(X, Y ).
5) Find the frequency of Z = X + Y .
1) The marginal frequency of X is
fX(x) =
 ∞
y=x
e−y dy = e−x
for x ≥0,
and fX(x) = 0 for x < 0.
Analogously the marginal frequency of Y is given by
fY (y) =
 y
x=0
e−y dx = y e−y
for y ≥0,
and fY (y) = 0 for y ≤0.
Summing up we get
X ∈Γ(1, 1)
and
Y ∈Γ(2, 1).
2) Then
E{X} =
 ∞
0
x e−x dx =
#
−(x + 1) e−x$∞
0 = 1,
and
E{Y } =
 ∞
0
y · y e−y dy =
 ∞
0
y2e−y dy =
#	
−y2 −2y −2

e−y$∞
0 = 2.
3) We ﬁrst compute
E

X2
=
 ∞
0
x2e−x dx =
#	
−x2 −2x −2

e−x$∞
0 = 2
and
E

Y 2
=
 ∞
0
y2 · y e−y dy =
 ∞
0
y3e−y dy = 3! = 6.
This gives us the variances
V {X}
=
E

X2
−(E{X})2 = 2 −12 = 1,
V {Y }
=
E

Y 2
−(E{Y })2 = 6 −22 = 2.
Download free eBooks at bookboon.com

Random variables I
 
64 
4. Frequencies and distributions functions, 2 dimensions
4) By a reduction to a double integral we get
E{XY } =
 
R2 xy h(x, y) dx dy =
 ∞
y=0
 y
x=0
x dx

y e−y dy =
 ∞
0
1
2 y3e−y dy = 3,
or alternatively
E{XY }
=
 ∞
x=0
x
 ∞
y=x
y e−y dy

dx =
 ∞
x=0
x
#
−(y + 1)e−y$∞
x dy
=
 ∞
0
	
x2e−x + x e−x
dx = 2 + 1 = 3.
Then
Cov(X, Y ) = E{XY } −E{X} · E{Y } = 3 −2 · 1 = 1,
hence
ϱ(X, Y ) =
Cov(X, Y )
%
V {X} V {Y }
=
1
√
1 · 2 =
√
2
2 .
5) The random variable Z = X + Y has its values in ]0, ∞[. If z > 0, then the frequency is given by
fZ(z) =
 ∞
−∞
h(x, z −x) dx,
where the condition 0 ≤x ≤y = z −x is reformulated to
0 ≤x ≤z
2.
Hence, when z > 0,
fZ(z) =

z
2
0
h(x, z −x) dx =

z
2
0
e−(z−x) dx = e−z 
exp
z
2

−1
 
= exp

−z
2

−exp(−z),
so
fZ(z) =
⎧
⎪
⎨
⎪
⎩
exp

−z
2

−e(−z),
for z ≥0,
0,
for z < 0.
Alternatively we compute the distribution function of Z by the following double integral,
FZ(z)
=

z
2
x=0
 z−x
y=x
e−y dy

dx =

z
2
x=0

e−x −ex−z
dx
=
1 −exp

−z
2

−e−z 
exp
z
2

−1

= 1 + e−z −2 exp

−z
2

=

1 −exp

−z
2
 2
.
Hence,
fZ(z) = F ′
Z(z) = exp

−z
2

−e−z
for z ≥0,
Download free eBooks at bookboon.com

Random variables I
 
65 
4. Frequencies and distributions functions, 2 dimensions
and fZ(z) = 0 for z < 0.
Additional remark. Since
E{Z} =
 ∞
0

z exp

−z
2

−z e−z 
dz = 4 −1 = 3,
and
E

Z2
=
 ∞
0

z2 exp

−z
2

−z2e−z 
dz = 16 −2 = 14,
we get
V {Z} = 14 −32 = 5 = V {X + Y }.
This gives
Cov(X, Y ) = 1
2 (V {X + Y } −V {X} −V {Y }) = 1
2 {5 −1 −2} = 1.
♦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
66 
4. Frequencies and distributions functions, 2 dimensions
Example 4.12 A 2-dimensional random variable (X, Y ) has the simultaneous frequency
f(x, y) = 1
2 xy,
0 < y < x < 2
(and 0 otherwise).
1) Compute the marginal frequencies of X and Y .
2) Compute the marginal distribution functions of X and Y .
3) Find the means of X and Y .
4) Find the medians of X and Y .
1) The marginal frequencies:
a) For ﬁxed x ∈[0, 2] we integrate with respect to y ∈[0, x], which gives
fX(x) =
 x
y=0
1
2 xy dy = 1
4 x3,
0 < x < 2,
and fX(x) = 0 otherwise.
b) For ﬁxed y ∈[0, 2] we integrate with respect to x ∈[y, 2], which gives
fY (y) =
 2
x=y
1
2 xy dx = y −1
4 y3,
0 < y < 2,
and fY (y) = 0 otherwise.
2) We get the distribution functions by integrating the frequencies,
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
x ≤0,
1
16 x4,
0 < x < 2,
1,
x ≥2,
and
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
1
2 y2 −1
16 y4,
0 < y < 2,
1,
y ≥2.
3) Then we compute the means,
E{X}
=
 2
0
x fX(x) dx =
 2
0
1
4 x4 dx =
 1
20 x5
2
0
= 32
20 = 8
5,
E{Y }
=
 2
9
y fY (y) dy =
 2
0

y2 −1
4 y4

dy = 8
3 −8
5 = 16
15.
Download free eBooks at bookboon.com

Random variables I
 
67 
4. Frequencies and distributions functions, 2 dimensions
4) The median of X is found from the equation
FX(x) = 1
16 x4 = 1
2,
i.e. x4 = 8, hence (X) =
4√
8.
The median of Y is found from the equation
FY (y) = 1
2 y2 −1
16 y4 = 1
2,
i.e.
y4 −8y2 + 8 = 0.
Since y2 ≤22 = 4, we get y2 = 4 −
√
8, so
(Y ) =
&
4 −
√
8.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables I
 
68 
4. Frequencies and distributions functions, 2 dimensions
Example 4.13 A rectangle has the edge lengths X1 and X2, where X1 and X2 are independent
random variables, both of the frequency
f(x) =
⎧
⎨
⎩
3x2,
0 < x < 1,
0,
otherwise.
1. Find the mean E {X1}.
2. Find the mean of the circumference of the rectangle, E {2X1 + 2X2}, and the mean of the area of
the rectangle, E {X1X2}.
We introduce the random variables Y1 and Y2 by
Y1 = X1X2,
Y2 = X1
X2
.
The vector function τ, given by
τ (x1, x2) =

x1x2, x1
x2

,
maps D = ]0, 1[ × ]0, 1[ bijectively onto
D′ =

(y1, y2) ∈R2 | 0 < y1 < y2, y1y2 < 1

.
3. Sketch D′ and ﬁnd the simultaneous frequency k (y1, y2) for (Y1, Y2).
4. Compute the marginal frequencies of Y1 and Y2.
(This can be answerede with or without using (3)).
5. Are Y1 and Y2 stochastically independent?
6. Find the mean and the median of Y2, and give an intuitive explanation of that the median is smaller
than the mean.
1) The mean is
E {X1} = E {X2} =
 1
0
3x3 dx = 3
4.
2) Since X1 and X2 are independent, we get
E {2X1 + 2X2} = 4 E {X1} = 3,
and
E {X1X2} = E {X1} · E {X2} =
3
4
2
= 9
16.
Download free eBooks at bookboon.com

Random variables I
 
69 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
0.20.40.60.8 1 1.2
3) From
x1 = √y1y2
og
x2 =
y1
y2
,
we get the Jacobian
∂(x1, x2)
∂(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2
1
√y1y2
−1
2
1
y2
y1
y2

= −1
2y2
< 0,
and the simultaneous frequency for (y1, y2) ∈D′, is given by
k (y1, y2) = 3 (√y1y2)2 · 3
y1
y2
2
·
1
2y2
= 9 · y1y2 · y1
y2
·
1
2y2
= 9
2
y2
1
y2
,
and k (y1, y2) = 0 otherwise.
4) The marginal frequency of Y1 for y1 ∈]0, 1[, is given by
kY1 (y1) = 9
2 y2
1

1
y1
y1
dy2
y2
= 9
2 y2
1 [ln y2]
1
y1
y1 = 9y2
1 ln 1
y1
= −9y2
1 ln y1,
and 0 otherwise.
If y2 ∈]0, 1], then
kY2 (y2) =
9
2y2
 y2
0
y2
1 dy1 =
3
2y2
y3
2 = 3
2 y2
2.
If y2 ∈]1, ∞[, then
kY2 (y2) =
9
2y2

1
y2
0
y2
1 dy1 =
3
2y2
· 1
y3
2
= 3
2 · 1
y4
2
,
Download free eBooks at bookboon.com

Random variables I
 
70 
4. Frequencies and distributions functions, 2 dimensions
hence
kY2 (y2) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
3
2 y2
2
for y2 ∈]0, 1],
3
2
1
y4
2
for y2 ∈]1, ∞[,
0
otherwise.
5) Since D′ is not a rectangular domain, we conclude that Y1 and Y2 are not independent.
Alternatively we see that
k (y1, y2) ̸= kY1 (y1) · kY2 (y2) .
6) The mean of Y2 is
E {Y2} = 3
2
 1
0
y3
2 dy2 + 3
2
 ∞
1
dy2
y3
2
= 3
8 + 3
4 = 9
8.
It follows from
P {Y2 < 1} =
 1
0
3
2 y2
2 dy2 = 1
2,
that the median is
⟨Y2⟩= 1 < E {Y2} .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables I
 
71 
4. Frequencies and distributions functions, 2 dimensions
Example 4.14 Let X1 and X2 be independent random variables of the frequencies
fX1 (x1) =
⎧
⎨
⎩
x1e−x1,
x1 > 0,
0,
x1 ≤0,
and
fX2 (x2) =
⎧
⎨
⎩
9x2e−3x2,
x2 > 0,
0,
x2 ≤0.
1. Find the means E {X1} and E {X2}.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + 3X2,
Y2 =
X1
X1 + 3X2
.
The vector function τ, given by
τ (x1, x2) =

x1 + 3x2,
x1
x1 + 3x2

,
maps R+ × R+ bijectively onto R+× ]0, 1[.
2. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
3. Compute the marginal frequencies of Y1 and Y2.
4. Check if Y1 and Y2 are stochastically independent.
5. Compute the mean E {Y2}, and prove that the median of Y2 is equal to E {Y2}.
1) Since X1 ∈Γ(2, 1), we have E {X1} = 2 · 1 = 2.
Since X2 ∈Γ

2, 1
3

, we have E {X2} = 2 · 1
3 = 2
3.
2) It follows from
y1 = x1 + 3x2
and
y2 =
x1
x1 + 3x2
that
x1 = y1y2
and
x2 = 1
3 (y1 −x1) = 1
3 y1 (1 −y2) .
The Jacobian becomes
∂(x1, x2)
∂(y1, y2) =

y2
y1
1
3 (1 −y2)
−1
3 y1

= −1
3 y1 < 0.
Download free eBooks at bookboon.com

Random variables I
 
72 
4. Frequencies and distributions functions, 2 dimensions
The simultaneous frequency og (y1, y2) ∈R+× ]0, 1[ is given by
k (y1, y2)
=
y1y2 · e−y1y2 · 9 · 1
3 y1 (1 −y2) · e−y1(1−y2) · 1
3 y1
=
y3
1 · e−y1 · y2 (1 −y2) =
1
6 y3
1e−y1

· {6y2 (1 −y2)} ,
and k (y1, y2) = 0 otherwise.
3) It follows immediately of the splitting of (2) that
kY1 (y1) =
⎧
⎪
⎨
⎪
⎩
1
6 y3
1e−y1
for y1 > 0,
0
otherwise,
and
kY2 (y2) =
⎧
⎨
⎩
6y2 (1 −y2)
for 0 < y2 < 1,
0
otherwise,
hence Y1 ∈Γ(4, 1) is gamma distributed, and Y2 ∈Be(2, 2) is beta distributed.
4) It follows from
k (y1, y2) = kY1 (y1) · kY2 (y2) ,
that Y1 and Y2 are independent.
5) By symmetry, the median is ⟨Y2⟩= 1
2, and the mean is
E {Y2} =
2
2 + 2 = 1
2 = ⟨Y2⟩.
Alternatively,
E {Y2} = 6
 1
0
	
y2
2 −y3
2

dy2 = 6
1
3 −1
4

= 6
12 = 1
2.
Download free eBooks at bookboon.com

Random variables I
 
73 
4. Frequencies and distributions functions, 2 dimensions
Example 4.15 Let X1 and X2 be independent random variables of the frequencies
fX1 (x1) =
⎧
⎨
⎩
e−x1,
x1 > 0,
0,
x1 ≤0,
and
fX2 (x2) =
⎧
⎨
⎩
x2e−x2,
x2 > 0,
0,
x2 ≤0.
1. Find the means E {X1}, E {X2} and E
 1
X2

.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1
X2
.
The vector function τ, given by
τ (x1, x2) =

x1 + x2, x1
x2

,
maps R+ × R+ bijectively onto itself.
2. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
3. Find the marginal frequencies of Y1 and Y2.
(The question can be answered with or without use of the result of (2).)
4. Check if Y1 and Y2 are independent.
5. Find the mean E {Y2}.
6. Find the median of Y2.
1. Since X1 ∈Γ(1, 1) and X2 ∈Γ(2, 1), it follows immediately that
E {X1} = 1
and
E {X2} = 2.
Alternatively,
E {X1} =
 ∞
0
x1e−x1 dx1 = 1
and
E {X2} =
 ∞
0
x2
2e−x2 dx2 = 2! = 2.
Finally,
E
 1
X2

=
 ∞
0
1
x2
· x2 e−x2 dx2 =
 ∞
0
e−x2 dx2 = 1.
Download free eBooks at bookboon.com

Random variables I
 
74 
4. Frequencies and distributions functions, 2 dimensions
2. We solve the equations y1 = x1 + x2 and y2 = x1
x2
, i.e. x1 = x2y2, with respect to (x1, x2). By
insertion,
y1 = x2y2 + x2 = x2 (1 + y2) ,
i.e. x2 =
y1
1 + y2
,
hence
x1 = y1y2
1 + y2
= y1 −
y1
1 + y2
and
x2 =
y1
1 + y2
.
The solution is unique, because y2 ̸= −1. From (y1, y2) ∈R+×R+ follows that (x1, x2) ∈R+×R+,
and vice versa, so τ maps the domain R+ × R+ bijectively onto itself.
The Jacobian is
∂(x1, x2)
∂(y1, y2) =

y2
1 + y2
y2
(1 + y2)2
1
1 + y2
−
y1
(1 + y2)2

= −
1
(1 + y2)3 (y2y1 + y1) = −
y1
(1 + y2)2 < 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
75 
4. Frequencies and distributions functions, 2 dimensions
Now, X1 and X2 are independent, so the simultaneous frequency of (X1, X2) is given by
f (x1, x2) =
⎧
⎨
⎩
x2 e−(x1+x2),
for x1 > 0 and x2 > 0,
0,
otherwise.
Thus the simultaneous frequency of (Y1, Y2) is
k (y1, y2) =
y1
1 + y2
e−y1 ·
y1
(1 + y2)2 = 1
2 y2
1 e−y1 ·
2
(1 + y2)3
for y1 > 0 and y2 > 0,
and k (y1, y2) = 0 otherwise.
3. & 4. It follows from the splitting of (2) that
kY1 (y1) =
⎧
⎪
⎨
⎪
⎩
1
2 y2
1e−y1
for y1 > 0,
0,
otherwise,
and
kY2 (y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2
(1 + y2)3
for y2 > 0,
0
otherwise.
It follows that Y1 and Y2 are independent.
Notice that Y1 ∈Γ(3, 1), and also by some elaboration that 2Y2 ∈F(2, 4).
3. Alternatively it follows for y1 > 0 that
kY1 (y1) =
 y1
0
e−(y1−t) · t · e−t dt = e−y1
 y1
0
t dt = 1
2 y2
1e−y1,
and kY1 (y1) = 0 for y1 ≤0.
If y2 > 0, we get by a formula that
kY2 (y2) =
 ∞
0
e−y2t · t e−y · |t| dt =
 ∞
0
t2e−(1+y2)t dt =
2!
(1 + y2)3 =
2
(1 + y2)3 ,
and kY2 (y2) = 0 for y2 ≤0.
5. Since X1 and X2 are independent, the mean is
E {Y2} = E
X1
X2

= E {X1} · E
 1
X2

= 1 · 1 = 1.
Alternatively,
E {Y2}
=
 ∞
0
2y2
(1 + y2)3 dy2 = 2
 ∞
0

1
(1 + y2)2 −
1
(1 + y2)3

dy2
=
2

−
1
1 + y2
+ 1
2
1
(1 + y2)2
∞
0
= 2

1 −1
2

= 1.
Download free eBooks at bookboon.com

Random variables I
 
76 
4. Frequencies and distributions functions, 2 dimensions
Alternatively, 2Y2 ∈F(2, 4), so
E {2Y2} =
n2
n2 −2 = 4
2 = 2,
hence
E {Y2} = 1.
6. The distribution function of y2 > 0 is given by
FY2 (y2) =
 y2
0
2
(1 + t)3 dt =

−
1
(1 + t)2
y2
0
= 1 −
1
(1 + y2)2 ,
so the median is determined by
1
(1 + y2)2 = 1
2,
hence
⟨Y2⟩=
√
2 −1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
77 
5. Functions and random variables, in general
5
Functions of random variables, in general
Example 5.1 Let X1 and X2 be random variables, and let Y1 = ϕ1 (X1) and Y2 = ϕ2 (X2).
1) Assume that X1 and X2 are independent. Is it possible to conclude that Y1 and Y2 are independent?
2) Assume that X1 and X2 are dependent. Does it follow that Y1 and Y2 are dependent?
1) The answer is ‘yes’. Let ϕ−1(C) = {t | ϕ(t) ∈C} denote the inverse set map. Then
P {ϕ1 (X1) ∈A ∧ϕ2 (X2) ∈B}
= P

X1 ∈ϕ−1
1 (A) ∧X2 ∈ϕ−1
2 (B)

= P

X1 ∈ϕ−1
1 (A)

· P

X2 ∈ϕ−1
2 (B)

,
because X1, X2 are independent
= P {ϕ1 (X1) ∈A} · P {ϕ2 (X2) ∈B} ,
and we conclude that ϕ1 (X1) and ϕ2 (X2) are stochastically independent.
2) The answer is ‘no’! It suﬃces to give an example. Let
ϕ1 (X1) = c1
and
ϕ2 (X2) = c2
be constant maps. Then
P {ϕ1 (X1) ∈A ∧ϕ2 (X2) ∈B} =
⎧
⎨
⎩
1,
if c1 ∈A and c2 ∈B,
0,
otherwise,
⎫
⎬
⎭
= P {c1 ∈A} · P {c2 ∈B} = P {ϕ1 (X1) ∈A} · P {ϕ2 (X2) ∈B} ,
proving that ϕ1 (X1) and ϕ (X2) are independent, no matter if X1 and X2 are independent or not.
Download free eBooks at bookboon.com

Random variables I
 
78 
5. Functions and random variables, in general
Example 5.2 A discrete random variable (X, Y ) has its distribution given by the following table
Y \ X
1
2
3
1
1
12
1
6
1
12
2
1
6
1
4
1
12
3
1
12
1
12
0
Find the marginal distributions of X and Y .
Compute P{X · Y is even}.
Compute P{X ≥Y }.
Are X and Y independent?
1) All probabilities are ≥0, and their sum is 1, so the table describes a distribution.
Y \ X
1
2
3
fY
1
1
12
1
6
1
12
1
3
2
1
6
1
4
1
12
1
2
3
1
12
1
12
0
1
6
fX
1
3
1
2
1
6
1
We conclude from the table that the marginal distributions are
P{X = 1} = 1
3,
P{X = 2} = 1
2,
P{X = 3} = 1
6,
and
P{Y = 1} = 1
3,
P{Y = 2} = 1
2,
P{Y = 3} = 1
6.
2) By a counting of the table we get
P{X · Y is even}
=
P{X = 2} + P{X = 1 ∧Y = 2} + P{X = 3 ∧Y = 2}
=
1
2 + 1
6 + 1
2 = 3
4.
3) We get in the same way
P{X ≥Y }
=
P{Y = 1} + P{Y = 2 ∧X = 2}
+P{Y = 2 ∧X = 3} + P{Y = 3 ∧X = 3}
=
1
3 + 1
4 + 1
12 + 0 = 8
12 = 2
3.
4) The random variables X and Y are not independent. We have e.g.
P{X = 3 ∧Y = 3} = 0
and
P{X = 3} · P{Y = 3} = 1
36 ̸= 0.
Download free eBooks at bookboon.com

Random variables I
 
79 
5. Functions and random variables, in general
Example 5.3 In a plane we draw two parallel lines ℓ1 and ℓ2 of the distance a. A needle of length
2b, where b < a, is thrown such that it falls randomly between the two lines in the following sense:
2*b
X
Y
l_1
l_2
a
The midpoint of the needle has the distance X from ℓ1, where X is rectangularly distributed over ]0, a[,
and the needle forms an angle Y with the two parallel lines, where Y is rectangularly distributed over
]0, π[. Finally, X and Y are independent.
1) Find a condition – expressed by X, Y , b – which describes that the needle intersects the line ℓ1.
2) Prove that the probability that the needle intersects ℓ1 is 2b
a · 1
π .
Remark 5.1 This example is called Buﬀon’s needle problem, and it can be traced back to 1777, when
Georges-Louis Leclerc, count of Buﬀon, published a paper on the subject. If a needle is thrown at
random many times, then the fraction when the needle intersects ℓ1, is approximately equal to 2b
a · 1
π .
If this fraction is denoted by f, then we have the approximation π ≈2b
a · 1
f . Since then many people
have tried to ﬁnd π in this way. In 1850 the astronomer Wolfe threw a needle 5000 times with the
values a = 45 mm, b = 36 mm. He obtained intersection in 2532 of the cases, hence f = 25332
5000 . This
gives the approximation 3.160 of π, which is quite fair. In 1901 Lazzarini published a paper with a far
better approximation of π. He used a = 3 cm, b = 2, 5 cm, the needle was thrown 3408 times, and he
obtained intersection 1808 times. In this case we get the approximation π ≈5
3 · 3408
1808 = 3.1415919 . . . ,
which is astonishingly in agreement with π = 3.1415926 . . . . However, Lazzarini was extremely lucky
in getting his paper published. Some mathematicians have later pointed out the fairly strange number
3408 of throws, and they also noted that Lazzarini’s fraction can be reduced to 355
113, which long has
been known as one of the very best rational approximations of π. Therefore, mathematicians of today
are convinced that the paper was a swindle.
Download free eBooks at bookboon.com

Random variables I
 
80 
5. Functions and random variables, in general
1) It follows by the geometry that the needle intersects ℓ1, if X ≤b · sin Y .
2) We shall ﬁnd P{X ≤b · sin Y }. Since X is rectangularly distributed over ]0, a[, and Y is rectan-
gularly distributed over ]0, π[, we get
fX(x) =
⎧
⎪
⎨
⎪
⎩
1
a,
for x ∈]0, a[,
0,
otherwise,
fY (y) =
⎧
⎪
⎨
⎪
⎩
1
π ,
for y ∈]0, π[,
0,
otherwise.
Then
P{b sin Y > X} =
1
π a
 π
y=0
 b sin y
x=0
1 dx

dy = 1
π a
 π
y=0
b sin y dy = 2b
a · 1
π .
Alternatively,
P{X ≤b · sin Y } = P{X −b · sin Y ≤0},
so we can instead ﬁnd the distribution function of Z = X −b · sin Y .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables I
 
81 
5. Functions and random variables, in general
We shall, however, ﬁrst ﬁnd the distribution function G(y) of −b · sin Y . This is given by
G(y) = P{−b · sin Y ≤y} = P

sin Y ≥−y
b
 
.
If y ≥0, then G(y) = 1, and if y ≤−b, then G(y) = 0. Finally, if y ∈] −b, 0[, then
G(y)
=
P

Arcsin

−y
b

≤Y ≤π −Arcsin

−y
b
 
=
1
π

π −2 Arcsin

−y
b
 
= 1 + 2
π Arcsin
y
b

,
hence the frequency is
g(y) = G′(y) = 2
π ·
1

1 −
y
b
 2 · 1
b = 2
πb
1

1 −
y
b
 2 ,
y ∈] −b, 0[,
and = 0 otherwise.
Since X and Y , and hence also X and −b sin Y are independent, we conclude that Z = X −b sin Y
has the frequency
h(s) =
 ∞
−∞
fX(s −x) g(x) dx,
s ∈R.
Thus, if b < a,
P{X ≤b · sin Y }
=
P{Z ≤0} =
 0
−∞
h(s) ds =
 0
−∞
 ∞
−∞
fX(s −x) g(x) dx

ds
=
 ∞
−∞
g(x)
 0
−∞
fX(s −x) ds

dx =
 ∞
−∞
g(x)
 −x
−∞
fX(s) ds

dx
=
2
πb
 0
−b
1

1 −
x
b
 2
 −x
−∞
1
a χ[0,a](s) ds

dx
=
2
πab
 b
0
1

1 −
x
b
 2
 x
0
χ[0,a](s) ds

dx,
where χ[0,a](s) = 1, if s ∈[0, a], and = 0 otherwise. Now,
 a
0
χ[0,a](s) ds =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
a
for x ≥a,
x,
for x ∈[0, a],
0,
for x < 0,
so we get for b < a,
P{X ≤b · sin Y }
=
2
πab
 b
0
x

1 −
x
b
 2 dx = 2b2
πab
 1
0
y
%
1 −y2 dy
=
2b
πa
'
−
%
1 −y2
(1
0 = 2b
a · 1
π ,
Download free eBooks at bookboon.com

Random variables I
 
82 
5. Functions and random variables, in general
which is the searched result.
Remark 5.2 If the needle is thrown a great number of times, then the relative frequency f that it
intersects ℓ1 will approximately be 2b
a · 1
π , so we conclude that
π ∼2b
a · 1
f .
This formula has earlier been used in the attempt of experimentally to ﬁnd π, however, without great
success. The results have either been too poor, or one has cheated (like e.g. Lazzarini). ♦
Remark 5.3 One can also go through this example without the assumption that b < a; but in this
case the computations become really tough, because the curve x = b · sin y then intersects the curve
x = a. ♦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables I
 
83 
6. Inequalities between two random variables
6
Inequalities between two random variables
Example 6.1 Two persons A and B have the intension of meeting between 8 AM and 9 AM. Both A
and B arrive at the meeting place at a randomly chosen time between 8 AM and 9 AM. Furthermore,
they have agreed that none of them will wait in more than 10 minutes. Find the probability that they
meet.
If instead, A and B have agreed that A will wait 15 minutes for B, while B will wait 5 minutes for
A, what is then the probability that they meet?
Hint. The arrival times of A and B are rectangularly distributed.
Let X be the arrival time of A, and let Y be the arrival time of B. Then X and Y are indepen-
dent random variables, which are both rectangularly distributed over an interval of length 1 hour,
represented by the interval ]0, 1[.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 28: The domain where the simultaneous frequency is 1.
The simultaneous frequency is
f(x, y) =
⎧
⎨
⎩
1,
for (x, y) ∈]0, 1[ × ]0, 1[,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 29: The domain C is the diagonal strip.
Download free eBooks at bookboon.com

Random variables I
 
84 
6. Inequalities between two random variables
1) Since 10 minutes = 1
6 hour, the event corresponds to |X −Y | < 1
6. The probability is equal to
the area of C, hence
P

|X −Y | < 1
6

= 1 −2 · 1
2
5
6
2
= 11
36 = 0.306.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 30: The domain D is the translated diagonal strip.
2) The event corresponds to −1
12 < Y −X < 1
4. The probability is equal to the area of D on the
ﬁgure,
1 −1
2
3
4
2
−1
2
11
12
2
= 1 −101
144 = 43
144 = 0.299.
Alternatively, (1) is solved in the following way:
P{meeting}
= P{A arrives ﬁrst between 800 and 850, and B at most 10 min. later}
+P{B arrives ﬁrst between 800 and 850, and A at most 10 min. later}
+P{A and B both arrive between 850 and 900}
= 5
6 · 1
6 + 5
6 · 1
6 + 1
6 · 1
6 = 11
36.
Alternatively, we ﬁnd the frequency h(z) of Z = X −Y . Since f(x, y) = 1[0,1]2(x, y), we get
h(z) =
 ∞
−∞
f(x, x −z) dx =
 1
0
1[0,1]2(x, x −z) dx =
 1
0
1[0,1](x −z) dx,
The integrand is only ̸= 0, if x ∈]0, 1[ and x −z ∈]0, 1[, i.e. x ∈]z, z + 1[, thus for z ∈] −1, 1[, cf. the
ﬁgure.
(i) For z ∈] −1, 0[ f˚as h(z) =
 z+1
0
dx = z + 1.
(ii) For z ∈]0, 1[ f˚as h(z) =
 1
z dx = 1 −z.
(iii) If z /∈] −1, 0[, then h(z) = 0.
Download free eBooks at bookboon.com

Random variables I
 
85 
6. Inequalities between two random variables
–1
–0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 31: The domain of integration for h(z).
Then the task can be treated in the following way:
1)
P

|X −Y | < 1
6

= P

−1
6 < Z < 1
6

=

1
6
−1
6
h(z) dz
=
 0
−1
6
(z + 1) dz +

1
6
0
(1 −z) dz =
1
2 (z + 1)2
0
−1
6
−
1
2 (1 −z)2
 1
6
0
= 1
2

1 −
5
6
2
−1
2
5
6
2
−1

= 1 −
5
6
2
= 11
36.
2)
P

−1
12 < Z < 1
4

=
 0
−1
12
(z + 1) dz +

1
4
0
(1 −z) dz =
1
2 (z + 1)2
0
−1
12
−
1
2 (1 −z)2
 1
4
0
= 1
2

1 −
11
12
2
+ 1
2

1 −
3
4
2
= 1 −1
2
11
12
2
+
3
4
2
= 43
144.
Download free eBooks at bookboon.com

Random variables I
 
86 
6. Inequalities between two random variables
Example 6.2 Henry and John arrive independently of each other to a meeting point. Both Henry
and John arrive at randomly chosen times between 8 AM and 9 AM.
1) Find the probability that Henry arrives as the ﬁrst one.
2) Find the probability that John arrives more than 10 minutes after Henry.
3) Find the probability that the diﬀerence between their arrival times is at most 5 minutes.
Hint. The arrival times of Henry and John are rectangularly distributed.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 32: The domain where the simultaneous distribution function is 1.
Let the random variable X denote Henry’s arrival time, and let the random variable Y denote John’s
arrival time. Since X and Y are independent and rectangularly distributed over e.g. ]0, 1[, the simul-
taneous frequency is
f(x, y) =
⎧
⎨
⎩
1,
(x, y) ∈]0, 1[ × ]0, 1[,
0,
otherwise,
and

A
f(x, y)f(x, y) dx dy = area(A),
for A ⫅]0, 1[ × ]0, 1[.
1) By an area consideration we get
P{Henry arrives ﬁrst} = P{X < Y } = the area of the upper triangle = 1
2.
2) Since 10 minutes = 1
6 hour, we get in the same way
P{John arrives more that 10 min. after Henry}
= P

Y > X + 1
6

= area of the upper triangle = 1
2
5
6
2
= 25
72.
Download free eBooks at bookboon.com

Random variables I
 
87 
6. Inequalities between two random variables
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 33: The domain given by X < Y is the upper triangle.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 34: The domain given by Y > X + 1
6 is the upper triangle.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 35: The domain where the diﬀerence is at most 5 minutes is represented by the domain around
the diagonal.
Download free eBooks at bookboon.com

Random variables I
 
88 
6. Inequalities between two random variables
3) Since 5 minutes = 1
12 hour, we get the condition −1
12 < X −Y < 1
12, and the probability is again
obtained by an area consideration,
P{the diﬀerence is at most 5 min.} = P

|X −Y | < 1
12

= area of the diagonal strip = 1 −2 · 1
2
11
12
2
= 23
144.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables I
 
89 
6. Inequalities between two random variables
Example 6.3 Two persons A and B arrive at a meeting point between 7 AM and 8 AM. Their
arrivals are independent of each other, and they both have a tendency of arriving at the end of the
interval which for convenience is put equal to ]0, 1[. (We adjust the time at 7 AM).
The arrival time of A is denoted by X, and we assume that its frequency is
f(x) =

2x,
0 < x < 1,
0,
otherwise,
while the arrival time of B is denoted by Y , and is also assumed to have the frequency
g(y) =

2y,
0 < x < 1,
0,
otherwise,
A will at most wait 20 minutes for B, while B is a very impatient person who does not want to wait
at all.
Find the probability that the two persons meet.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 36: The domain of integration is the diagonal strip.
Since X and Y are independent, the frequency of the 2-dimensional random variable (X, Y ) is given
by
h(x, y) = f(x)g(y) =
⎧
⎨
⎩
4xy,
0 < x, y < 1,
0,
otherwise,
Since 20 minutes = 1
3 hour, the task is to ﬁnd P

X ≤Y ≤X + 1
3

, i.e. the integral of h(x, y) over
the diagonal strip on the ﬁgure.
The domain of integration is split at x = 2
3. Then by ﬁrst integrating vertically (the inner integral,
Download free eBooks at bookboon.com

Random variables I
 
90 
6. Inequalities between two random variables
so x is kept ﬁxed),
P

X ≤Y ≤x + 1
3

=

2
3
0
 x+ 1
3
x
4xy dy

dx +
 1
2
3
 1
x
4xy dy

dx
=

2
3
0
2x

x + 1
3
2
−x2

dx +
 1
2
3

2x −2x3
dx
=

2
3
0
4
3 x2 + 2
9 x

dx +

x2 −1
2 x4
1
2
3
=
4
9 x3 + 1
9 x2
 2
3
0
+

1 −1
2

−
4
9 −8
81

= 4
9 · 8
27 + 1
9 · 4
9 + 1
2 −36 −8
81
= 32
243 + 12
243 + 1
2 −28
81
= 44
243 + 1
2 −84
243 = 1
2 −40
243 = 243 −80
486
= 163
486 ≈0, 335.
Example 6.4 According to their schedules, 2 trains A and B shall arrive to a station at the same
time on each their line. Train A shall stay at the station for 5 minutes, and train B for 4 minutes.
However, the trains are very often delayed up to 20 minutes, so we assume that the arrival time of
train A is rectangularly distributed over [0, 20] (measured in minutes), and the arrival time of train B
is also rectangularly distributed over [0, 20]. The delay time is counted from the planned arrival time.
1) Find the probability that train A arrives before train B.
2) Find the probability that the two trains meet at the station.
3) Find the probability that train A arrives before train B and departs after train B.
Since the arrival time X of train A and the arrival time Y of train B are independent and both
rectangularly distributed over [0, 20], the simultaneous frequency of (X, Y ) is
f(x, y) =
⎧
⎪
⎨
⎪
⎩
1
400,
for 0 < x, y < 20,
0,
otherwise.
1) It follows by an area consideration of weight
1
400 that
P{X < Y } = 1
2.
2) If the two trains meet at the station, then X −4 < Y < X + 5, so (X, Y ) lies in the diagonal strip.
Then by an area consideration,
P{X −4 < Y < X + 5}
=
1
400

202 −1
2 · 152 −1
2 · 162

=
1
400

400 −225
2
−256
2

=
1 −481
800 = 319
800.
Download free eBooks at bookboon.com

Random variables I
 
91 
6. Inequalities between two random variables
0
5
10
15
20
5
10
15
20
Figure 37: The event X < Y is represented by the upper triangle.
0
5
10
15
20
5
10
15
20
Figure 38: The event X −4 < Y < X + 5 is represented by the diagonal strip.
0
5
10
15
20
5
10
15
20
Figure 39: The event X < Y < X + 1 is represented by the diagonal strip.
3) If A arrives before B, i.e. X < Y , and departs after B, i.e. X + 5 > Y + 4, then X < Y < X + 1.
The probability can again be found by an area consideration,
P{X < Y < X + 1}
=
1
2 −1
2 ·
19
20
2
= 1
2

12 −
19
20
2
=
1
2

1 + 19
20
 
1 −19
20

= 1
2 · 39
20
1
20 = 39
800.
Download free eBooks at bookboon.com

Random variables I
 
92 
6. Inequalities between two random variables
Example 6.5 Henry and Peter throw dices. Every minute (t = 1, 2, 3, . . . ) Henry throws a dice
(probability
1
6 of getting a six), and Peter also throws another dice every minute.
We deﬁne the
random variables X and Y by
X = k, if Henry obtains his ﬁrst six in throw number k,
Y = k, if Peter obtains his ﬁrst six in throw number k.
1. Find P{X = k}, k ∈N, and ﬁnd the mean E{X}.
2. Find for every k ∈N the probability P{X = k ∧Y = k}, and then ﬁnd P{X = Y }.
3, Compute the two probabilities P{X < Y } and P{Y < X}.
4. We deﬁne a random variable Z by
Z = k, if Henry obtains his second six in throw number k.
Find P{Z = k}, k = 2, 3, 4, . . . .
5. Find for k = 2, 3, 4, . . . ,
P{Z = k ∧Y > k},
and then ﬁnd the probability that Henry gets at least two sixes before Peter obtains his ﬁrst six.
1) Since X (and also Y ) is geometric distributed with p = 1
6, we get
P{X = k} = 1
6
5
6
k−1
,
k ∈N,
and
E{X} = 6.
2) Since X and Y are independent, we get for k ∈N that
P{X = k ∧Y = k} = P{X = k} · P{Y = k} =

1
6
5
6
k−12
= 1
36
25
36
k−1
.
Then by a summation,
P{X = Y } =
∞

k=1
P{X = k ∧Y = k} = 1
36
∞

k=1
25
36
k−1
= 1
36 ·
1
1 −25
36
= 1
11.
3) Clearly,
P{X < Y } + P{Y < X} + P{X = Y } = 1.
It follows from the symmetry that P{X < Y } = P{Y < X}, so
P{X < Y } = P{Y < X} = 1
2 (1 −P{X = Y }) = 1
2

1 −1
11

= 5
11.
Download free eBooks at bookboon.com

Random variables I
 
93 
6. Inequalities between two random variables
Alternatively,
P{X < Y }
=
∞

k=1
P{X = k ∧Y > k} =
∞

k=1
P{X = k} · P{Y > k}
=
∞

k=1
1
6
5
6
k−1
·
5
6
k
= 1
6 · 5
6
∞

k=1
25
36
k−1
= 5
36 · 36
11 = 5
11.
4) The random variable Z can be written Z = X1 + X2, where X1 and X2 are independent of the
same distribution as X. Then we get for k ≥2,
P{Z = k}
=
k−1

i=1
P {X1 = i} · P {Xk = k −i} =
k−1

i=1
1
6
5
6
i−1
· 1
6
5
6
k−i−1
=
1
6
k−1

i=1
5
6
k−2
= (k −1) · 1
36
5
6
k−2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables I
 
94 
6. Inequalities between two random variables
Alternatively, Z = k means that in the ﬁrst k −1 throws we have obtained precisely one six,
and that we in the k-th throw get another six, hence
P{Z = k} = (k −1) · 1
6
5
6
k−2
· 1
6 = (k −1) · 1
36
5
6
k−2
.
5) Here,
P{Z < Y }
=
∞

k=2
P{Z = k ∧Y > k} = 1
36
∞

k=2
(k −1)
25
36
k−2
· 25
36
=
1
36 · 25
36 ·
1
11
36
2 = 25
121.
Download free eBooks at bookboon.com

Random variables I
 
95 
7. Functions Y = f(X) of random variables
7
Functions Y = f(X) of random variables
Example 7.1 Let X be rectangularly distributed over ]0, a[, where a > 0.
Find the distribution
function and the frequency of the random variable Y = X2 + X.
The frequency of X is
f(x) =
⎧
⎪
⎨
⎪
⎩
1
a,
0 < x < a,
0,
otherwise.
The function y = τ(x) = x2 + x maps ]0, a[ increasingly onto
$
0, a + a2#
. The inverse map is
x = τ −1(y) = −1
2 +

1
4 + y,
where
	
τ −1
′ (y) =
1
2

1
4 + y
.
Then the frequency of Y is
g(y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1
a ·
1
2

1
4 + y
= 1
a ·
1
√1 + 4y ,
0 < y < a2 + a,
0,
otherwise,
and the distribution function G(y) =
 y
−∞g(u) du is
G(y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
1
a τ −1(y) = 1
a

1
4 + y −1
2

,
0 < y < a2 + a,
1,
y > a2 + a.
Alternatively, we get for y ∈
$
0, a2 + a
#
,
G(y) = P{Y ≤y} = P

X ≤−1
2 +

1
4 + y

= 1
a

1
4 + y −1
2

,
hence in the same interval,
g(y) = G′(y) = 1
a ·
1
2

1
4 + y
= 1
a ·
1
√1 + 4y .
Download free eBooks at bookboon.com

Random variables I
 
96 
7. Functions Y = f(X) of random variables
Example 7.2 A line segment of length 1 is randomly divided into two pieces of the lengths X and
1−X, where we assume that X is rectangularly distributed over the interval ]0, 1[. We form a rectangle
of edge lengths X and 1 −X. Find the probability that the area of this rectangle is bigger than 1
8.
The area of the rectangle is Y = X(1−X). We shall ﬁnd the probability that this expression is bigger
than 1
8.
Now,
x(1 −x) = −x2 + x > 1
8,
if and only if
1
2 −
√
2
4
< x < 1
2 +
√
2
4 .
Since X is rectangularly distributed, we get
P

1
2 −
√
2
4
< X < 1
2 +
√
2
4

=

1
2 +
√
2
4
1
2 −
√
2
4
1 dx =
√
2
2 .
Remark 7.1 It is possible in general to ﬁnd the distribution function of
Y = f(X) = X(1 −X).
If 0 < y < 1
4, then
P{Y ≤y}
=
P

X ≤1
2 −

1
4 −y

∪

X ≥1
2 +

1
2 −y

=
1 −2

1
4 −y = 1 −
%
1 −4y,
so
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
y ≤0,
1 −√1 −4y,
0 < y < 1
4,
1,
y ≥1
4,
and
fY (y) =
⎧
⎪
⎨
⎪
⎩
2y
√1 −4y ,
0 < y < 1
4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
97 
7. Functions Y = f(X) of random variables
Note that the probability of Y = X(1 −X) being bigger than y is
P{Y > y} = 1 −FY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1,
for y ≤0,
√1 −4y,
for 0 < y < 1
4,
0,
for y ≥1
4.
If y = 1
8 we get
P

Y > 1
8

=

1 −1
2 =
√
2
2 .
♦
Example 7.3 Let the random variable X be rectangularly distributed over the interval
(
−π
2 , π
2
'
. Find
the distribution functions and the frequencies of the random variables
Y = sin X,
Z = cos X,
U = tan X.
Since X is rectangularly distributed over
(
−π
2 , π
2
'
, the frequency is given by
fX(x) =
⎧
⎪
⎨
⎪
⎩
1
π
for x ∈
(
−π
2 , π
2
'
,
0
otherwise.
1) Y = sin X.
If |y| < 1, then
P{Y ≤y} = P{X ≤Arcsin y} = 1
π

Arcsin y + π
2
 
= 1
π Arcsin y + 1
2,
hence the distribution function of Y is
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
y ≤−1,
1
π Arcsin y + 1
2,
−1 < y < 1,
1,
y ≥1,
and the frequency is then obtained by a diﬀerentiation,
fY (y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
π
%
1 −y2 ,
−1 < y < 1,
0,
otherwise.
Alternatively, we ﬁrst ﬁnd the frequency fY (y) of Y . Since y = sin x maps
(
−π
2 , π
2
'
increasingly
onto ] −1, 1[, it has an inverse map,
x = τ −1(y) = Arcsin y
with
	
τ −1
′ (y) =
1
%
1 −y2 .
Download free eBooks at bookboon.com

Random variables I
 
98 
7. Functions Y = f(X) of random variables
0
0.2
0.4
0.6
0.8
1
–1.5
–1
–0.5
0.5
1
1.5
Figure 40: The graph of z = cos x for x ∈] −π
2 , π
2 [.
Since f
	
τ −1(y)

= 1
π , we get that
fY (y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
π
%
1 −y2 ,
−1 < y < 1,
0,
otherwise.
and if −1 < y < 1, we get the distribution function
FY (y) = P{Y ≤y} =
 y
−1
1
π
%
1 −y2 dy =
 1
π Arcsin y
y
−1
= 1
π Arcsin y + 1
2.
2) Z = cos X.
In this case, only z ∈]0, 1[ is of interest. It follows by the symmetry – cf. the ﬁgure – that
P{Z ≤z}
=
2 P

Arccos z ≤X < π
2
 
= 2 · 1
π
π
2 −Arccos z
 
=
1 −2
π Arccos z = 2
π Arcsin z.
Hence, the distribution function is
FZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
z ≤0,
2
π Arcsin z,
0 < z < 1,
1,
z ≥1,
from which we get the frequency by diﬀerentiation
fZ(z) =
⎧
⎪
⎨
⎪
⎩
2
π
√
1 −z2 ,
0 < z < 1,
0,
otherwise.
Since z = cos x is not monotonous in
(
−π
2 , π
2
'
, we cannot apply the usual argument.
Download free eBooks at bookboon.com

Random variables I
 
99 
7. Functions Y = f(X) of random variables
3) U = tan X.
If u ∈R, then
P{U ≤u} = P{X ≤Arctan u} = 1
π

Arctan u + π
2
 
= 1
π Arctan u + 1
2,
hence
FU(u) = 1
π Arctan u + 1
2
og
fU(u) = F ′
U(u) =
1
π (1 + u2),
u ∈R.
Alternatively, like in (1) it is possible to ﬁnd the frequency of U, because u = τ(x) = tan x
maps
(
−π
2 , π
2
'
increasingly onto R with the inverse map x = τ −1(u) = Arctan(u), where
	
τ −1
′ (u) =
1
1 + u2 .
Then apply the standard formula.
The distribution of U is a Cauchy distribution.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
100 
7. Functions Y = f(X) of random variables
Example 7.4 Assume that the random variable X is rectangularly distributed over the interval ]0, π[.
Find the distribution functions and the frequencies of the random variables
Y = 1
X ,
Z = cos X,
U = sin X.
When X is rectangularly distributed over ]0, π[, then
fX(x) =
⎧
⎪
⎨
⎪
⎩
1
π
for x ∈]0, π[,
0
otherwise,
and
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0
for x ≤0,
x
π
for x ∈]0, π[,
1
for x ≥π.
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
2.5
3
Figure 41: The graph of y = 1
x in the interval ]0, π[.
1) The image of ]0, π[ by the map y = 1
x is
 1
π , ∞

. If y > 1
π , then we get the distribution function
FY (y) = P{Y ≤y} = P
 1
X ≤y

= P

X ≥1
y

= 1 −P

X < 1
y

= 1 −1
πy ,
hence
FY (y) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1 −1
πy
for y > 1
π ,
0
for y ≤1
π ,
and
fY (y) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
πy2
for y > 1
π ,
0
for y ≤1
π .
2) The image of ]0, π[ by z = cos x is ] −1, 1[. If z ∈] −1, 1[, then we get the distribution function
FZ(z)
=
P{Z ≤z} = P{cos X ≤z} = P{X ≥Arccos z}
=
1 −P{X < Arccos z} = 1 −1
π Arccos z,
Download free eBooks at bookboon.com

Random variables I
 
101 
7. Functions Y = f(X) of random variables
–1
–0.5
0
0.5
1
0.5
1
1.5
2
2.5
3
Figure 42: The graph of z = cos x for x ∈]0, π[.
hence
FZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
z ≤−1,
1 −1
π Arccos z,
−1 < z < 1,
1,
z ≥1,
and
fZ(z) =
⎧
⎪
⎨
⎪
⎩
1
π
1
√
1 −z2 ,
−1 < z < 1,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 43: The graph of u = sin x for x ∈]0, π[.
Download free eBooks at bookboon.com

Random variables I
 
102 
7. Functions Y = f(X) of random variables
3) The image of ]0, π[ by u = sin x is ]0, 1[. If u ∈]0, 1[, then we get the distribution function
FU(u)
=
P{U ≤u} = P{sin X ≤u}
=
P{X ≤Arcsin u} + P{X ≥π −Arcsin u}
=
P{X ≤Arcsin u} + 1 −P{X < π −Arcsin u}
=
1 + 1
π Arcsin u −1
π {π −Arcsin u} = 2
π Arcsin u,
hence
FU(u) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
u ≤0,
2
π Arcsin u,
0 < u < 1,
1,
u ≥1,
and
fU(u) =
⎧
⎪
⎨
⎪
⎩
2
π
1
√
1 −u2 ,
0 < u < 1,
0,
otherwise.
Alternatively, we may apply the usual formula in (1) and (2), but not in (3), because sinus is not
a bijective onto the given interval.
If t(x) is a bijective transformation, and x = x(t) is the inverse, then we have in the form of diﬀerentials,
fX(x) dx = fX(x(t))

dx
dt
 dt = fT (t) dt.
Hence, we shall always ﬁnd the inverse map x = x(t).
1) If x ∈]0, π[, then y = 1
x ∈
 1
π , ∞

. The inverse map is given by x = 1
y , thus
dx
dy = −1
y2 < 0,
and we get for y > 1
π that
fY (y) = fX
1
y

·
−1
y2
 = 1
π · 1
y2 ,
hence by an integration for y > 1
π ,
FY (y) =
 y
1
π
1
π · 1
η2 dη = 1
π

−1
η
y
1
π
= 1 −1
πy .
100
Download free eBooks at bookboon.com

Random variables I
 
103 
7. Functions Y = f(X) of random variables
2) If x ∈]0, π[, then z = cos x ∈] −1, 1[ bijectively. The inverse is given by x = Arccos z, thus
dx
dz = −
1
√
1 −z2 < 0.
If z ∈] −1, 1[, then
fZ(z) = fX(Arccos z) ·
−
1
√
1 −z2
 =
1
π
√
1 −z2 ,
so if z ∈] −1, 1[, then we get by an integration that
FZ(z) =
 z
−1
1
π ·
1
%
1 −ζ2 dζ = 1
π [−Arccos ζ]z
−1 = 1
π {π −Arccos z} = 1 −1
π Arccos z.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables I
 
104 
7. Functions Y = f(X) of random variables
Example 7.5 Assume that the random variable X has the frequency
f(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
3
2π ,
0 < x < π
2 ,
1
2π ,
π
2 < x < π,
0,
otherwise.
Find the distribution function of X.
Then ﬁnd the distribution functions and the frequencies of each of the random variables
Y = 1
X ,
Z = sin X.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 44: The frequency f(x).
0
0.2
0.4
0.6
0.8
1
1
2
3
4
x
Download free eBooks at bookboon.com

Random variables I
 
105 
7. Functions Y = f(X) of random variables
1) If x ≤0, then F(x) = 0.
If 0 < x ≤π
2 , then F(x) = 3x
2π .
In particular, F
π
2

= 3
4, hence for π
2 < x < π,
F(x) = F
π
2

+
 x
π
2
1
2π dx = 3
4 + 1
2π

x −π
2
 
= 1
2 + x
2π .
If x ≥π, then F(x) = 1.
Summing up we get the distribution function
F(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
x ≤0,
3x
2π ,
0 < x ≤π
2 ,
1
2 + x
2π ,
π
2 < x < π,
1,
x ≥π.
2) The interval ]0, π[ is by y = 1
x mapped bijectively onto
 1
π , ∞

.
1st variant. If y > 1
π , then we get the distribution function
FY (y)
=
P{Y ≤y} = P
 1
x ≤y

= P

X ≥1
y

= 1 −P

X < 1
y

=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1 −
3
2πy
for y > 2
π ,
1
2 −
1
2πy ,
for 1
π < y ≤2
π ,
0,
for y ≤1
π .
The frequency is obtained by a diﬀerentiation,
fY (y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
3
2πy2 ,
for y > 2
π ,
1
2πy2 ,
for 1
π < y < 2
π ,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
106 
7. Functions Y = f(X) of random variables
2nd variant. Since x = 1
y and dx
dy = −1
y2 , it follows that
fY (y) = FX
1
y

· 1
y2 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
3
2πy2
for y ≥2
π ,
1
2πy2
for 1
π < y < 2
π ,
0
otherwise.
If y ≤1
π , then FY (y) = 0.
We get for 1
π ≤y ≤2
π that
FY (y) =
 y
1
π
dη
2πη2 =

−1
2πη
y
1
π
= 1
2 −
1
2πy .
If y > 2
π , then
FY (y)
=
FY
 2
π

+
 y
2
π
3
2πη2 dη = 1
2 −1
4 + 3
2π

−1
η
y
2
π
=
1
4 + 3
4 −
3
2πy = 1 −
3
2πy .
3) The function z = sin x is not bijective, so we cannot apply the usual theorem. Since the image of
]0, π[ by z = sin x is ]0, 1[, we get for z ∈]0, 1[ that
FZ(z)
=
P{Z ≤z} = P{sin X ≤z}
=
P{X ≤Arcsin z} + P{X ≥π −Arcsin z}
=
1 + P{X ≤Arcsin z} −P{X < π −Arcsin z}.
Since Arcsin z ∈
(
0, π
2
'
, we must have π−Arcsin z ∈
(π
2 , π
'
. Then it follows from FX(x) =
P{X ≤x}, found in (1) that
FZ(z) = 1 + 3
2π Arcsin z −
1
2 + 1
2π (π −Arcsin z)

= 2
π Arcsin z,
thus
FZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
for z ≤0,
2
π Arcsin z,
for 0 < z < 1,
1,
for z ≥1,
and hence
fZ(z) =
⎧
⎪
⎨
⎪
⎩
1
π ·
1
√
1 −z2 ,
for 0 < z < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
107 
7. Functions Y = f(X) of random variables
Example 7.6 The random variable X has the frequency
f(x) =
⎧
⎨
⎩
e−x,
x > 0,
0,
x ≤0.
Find the frequencies of the random variables
Y = sinh X,
Z = cosh X.
We ﬁrst note that sinh : R+ →R+ and cosh : R+ →]1, +∞[ are bijective and monotonous.
1) Y = sinh X. In this case, y = τ(x) = sinh x, x ∈R+, hence
x = τ −1(y) = Arsinh y = ln

y +
%
1 + y2

,
y > 0,
and
dx
dy =
	
τ −1(y)

′ =
1
%
1 + y2 ,
y > 0.
It follows from the usual theorem that the frequency is given for y > 0 by
g(y) = f
	
τ −1(y)

·

	
τ −1
′ (y)
 = e
ln
“
y+√
1+y2
”
·
1
%
1 + y2 =
1
y +
%
1 + y2 ·
1
%
1 + y2 ,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables I
 
108 
7. Functions Y = f(X) of random variables
thus
g(y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
y +
%
1 + y2 ·
1
%
1 + y2 ,
for y > 0,
0,
otherwise.
2) Z = cosh X. In this case, z = τ(x) = cosh x, x ∈R+, thus
x = τ −1(z) = Arcosh z = ln

z +
%
z2 −1

,
z > 1,
and
dx
dz =
	
τ −1(z)

′ =
1
√
z2 −1
.
z > 1.
Applying the theorem we get for z > 1 the frequency
h(z) = f
	
τ −1(z)

·

	
τ −1
′ (z)
 = e−ln(z+
√
z2−1) ·
1
√
z2 −1
=
1
z +
√
z2 −1
·
1
√
z2 −1
,
hence
h(z) =
⎧
⎪
⎨
⎪
⎩
1
z +
√
z2 −1
·
1
√
z2 −1
,
for z > 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
109 
8. Functions of two random variables, f(X.Y)
8
Functions of two random variables, f(X, Y )
Example 8.1 1) Let X and Y be independent random variables with their frequencies
fX(x) =
k
π (k2 + x2),
x ∈R,
fY (y) =
1
π (1 + y2),
t ∈R,
where k denotes some positive constant. Prove that X + Y has the frequency
g(x) =
k + 1
π {(k + 1)2 + x2},
x ∈R.
2) Let X1 and X2 be independent random variables of frequencies
f1(x1) =
a1
π (a2
1 + x2
1),
x1 ∈R,
f2(x2) =
a2
π (a2
2 + x2
2),
x2 ∈R,
where a1 and a2 denote positive constants. Find by using the result of (1) the frequency of X1+X2.
3) Let Y1 and Y2 be independent random variables of the frequencies
g1(y1) =
a1
π

a2
1 + (y1 −b1)2 ,
y1 ∈R,
g2(y2) =
a2
π

a2
2 + (y2 −b2)2 ,
y2 ∈R,
where a1, a2 ∈R+ and b1, b2 ∈R. Find by using the result of (2) the frequency of Y1 + Y2.
1) (The hard question). The frequency g(x) of X + Y is given by the convolution
g(x) = 1
π2
 ∞
−∞
k
k2 + t2 ·
1
1 + (t −x)2 dt.
A decomposition gives us the structure
(1)
k
k2 + t2 ·
1
1 + (t −x)2 = a + bt
k2 + t2 + c + d(t −x)
1 + (t −x)2 ,
which has the integral
a
k Arctan
 t
k

+ c Arctan(t −x) + b
2 ln
	
k2 + t2
+ d
2 ln

1 + (t −x)2
.
The integral is clearly convergent, so d = −b, and the logarithmic terms disappear by taking the
limit.
We conclude that
g(x) = 1
π
a
k + c
 
,
where a = a(k, x) and c = c(k, x) depend on both k and x.
Download free eBooks at bookboon.com

Random variables I
 
110 
8. Functions of two random variables, f(X.Y)
If we put d = −b into (1), then
k
k2 + t2 ·
1
1 + x2 −2xt + t2 = a + bt
k2 + t2 +
bx + c −bt
1 + x2 −2xt + t2 ,
hence
(2) k = (a + bt)
	
1 + x2 −2x · t + t2
+
	
k2 + t2
(bx + c −bt).
The constant term of this equation is
k =
	
1 + x2
a + k2x · b + k2c,
thus
	
1 + x2
 a
k + kx · b + k · c = 1.
Since we want to ﬁnd a
k + c, we rewrite this as
(3)
	
1 + x2
 a
k + c
 
+ kx · b +
	
k −1 −x2
c = 1.
The coeﬃcient of t i (2) gives the equation
−2xa +
	
1 + x2
b −k2b = 0,
which is rewritten as
(4) −2kx
a
k + c
 
+
	
1 + x2 −k2
b + 2kx · c = 0.
The coeﬃcient of t2 in (2) implies the equation
a −xb + c = 0,
which is rewritten in the following way (cf. the above)
(5) k
a
k + c
 
−x · b + (1 −k) · c = 0.
Summing up we obtain the following linear system of the three unknowns a
k + c, b and c,
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
	
1 + x2
 a
k + c
 
+
kx · b
=
	
k −1 −x2
c
=
1,
−2kx
a
k + c
 
+
	
1 + x2 −k2
b
+
2kx · c
=
0,
k
a
k + c
 
−
x · b
+
(1 −k)c
=
0,
Then by Cramer’s formula,
a
k + c =

1
kx
k −1 −x2
0
1 + x2 −k2
2kx
0
−x
1 −k


1 + x2
kx
k −1 −x2
−2kx
1 + x2 −k2
2kx
k
−x
1 −k

.
Download free eBooks at bookboon.com

Random variables I
 
111 
8. Functions of two random variables, f(X.Y)
First compute the numerator

1 + x2 −k2
2kx
−x
1 −k

=
(1 −k)
	
1 + x2
−(1 −k)k2 + 2kx2
=
1 + x2 −k −kx2 + 2kx2 −k2 + k3
=
x2(1 + k) + (1 −k)
	
1 −k2
=
(k + 1)

x2 + (k −1)2
.
When the third column is replaced by the sum of the ﬁrst and the third column we see that the
denominator is reduced to

1 + x2
kx
k
−2kx
1 + x2 −k2
0
k
−x
1

=

1 + x2 −k2
2kx
0
−2kx
1 + x2 −k2
0
k
−x
1

=

1 + x2 −k2
2kx
−2kx
1 + x2 −k2
 =
	
1 + x2 −k2
2 + 4k2x2
=
x4 + x2 	
2 −2k2 + 4k2
+ k4 −2k2 + 1
=
x2 
x2 + (k −1)2
+ x2 	
2k2 + 2 −k2 + 2k −1

+
	
k2 −1

2
=
x2 
x2 + (k −1)2
+ x2(k + 1)2 + (k + 1)2(k −1)2
=
x2 
x2 + (k −1)2
+ (k + 1)2 
x2 + (k −1)2
=

x2 + (k −1)2 
x2 + (k + 1)2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
112 
8. Functions of two random variables, f(X.Y)
Hence, if (x, y) ̸= (0, 1),
a
k + c =
(k + 1)

x2 + (k −1)2
{x2 + (k + 1)2} {x2 + (k −1)2} =
k + 1
x2 + (k + 1)2 ,
which is extended by continuity to (x, k) = (0, 1).
Thus, the frequency is given by
g(x) = 1
π
a
k + c
 
= 1
π ·
k + 1
x2 + (k + 1)2
as required.
2) The frequency of X1 + X2 is
f(x)
=
a1a2
π2
 ∞
−∞
1
a2
1 + t2 ·
1
a2
2 + (t −x)2 dt,
u = t
a1
,
=
a2
π2
 ∞
−∞
1
1 +
 t
a1
2 ·
1
a2
2 + a2
1
 t
a1
−x
a1
2 d
 t
a1

=
1
π2a1
 ∞
−∞
1
1 + u2 ·
a2
a1
a2
a1
2
+

u −x
a1
2 du
k = a2
a1
and (1),
=
1
πa1
·
a2
a1
+ 1
a2
a1
+ 1
2
+
 x
a1
2 = 1
π ·
a1 + a2
(a1 + a2)2 + x2 .
3) In this case we get the frequency
g(y)
=
a1a2
π2
 ∞
−∞
1
a2
1 + (t −b1)2 ·
1
a2
2 + (y −t −b2)2 dt
u = t −b1,
=
a1a2
π2
 ∞
−∞
1
a2
1 + u2 ·
1
a2
2 + (y −u −b1 −b2)2 du
=
a1a2
π2
 ∞
−∞
1
a2
1 + u2 ·
1
a2
2 + (u −{y −b1 −b2})2 du
=
1
π ·
a1 + a2
(a1 + a2)2 + (y −{b1 + b2})2 ,
where we have applied (2).
Download free eBooks at bookboon.com

Random variables I
 
113 
8. Functions of two random variables, f(X.Y)
Example 8.2 Let X and Y be independent Cauchy distributed random variable of the frequencies
fX(x) =
1
π (1 + x2),
x ∈R,
fY (y) =
1
π (1 + y2),
y ∈R.
Prove that the random variable Z = XY has the frequency
fZ(z) = 2
π2 · ln |z|
z2 −1,
z ∈R,
(suitably modiﬁed for z = −1, 0, 1).
Hint: One may apply that
	
z2 −1

·
1
1 + z2 ·
1
z2 + x2 =
1
1 + x2 −
1
z2 + x2 .
If z ̸= −1, 0, 1, then the frequency of Z = XY is given by
fZ(z)
=
 ∞
−∞
g(x) g
 z
x
 1
|x| dx = 1
π2
 ∞
−∞
1
1 + x2 ·
1
1 +
 z
x
2 · 1
|x| dx
=
2
π
 ∞
0
1
1 + x2 ·
1
z2 + x2 x dx
symmetry; then u = x2,
=
1
π2
 ∞
0
1
1 + u ·
1
z2 + u du
=
1
π2
 ∞
0

1
z2 −1 ·
1
1 + u −
1
z2 −1 ·
1
z2 + u

du
=
1
π2 ·
1
z2 −1
lim
A→∞

0

1
1 + u −
1
z2 + u

du
=
1
π2 ·
1
z2 −1
lim
A→∞

ln
 u + 1
u + z2

−ln
 1
z2

= 2
π2 · ln |z|
z2 −1.
Since the exceptional set {−1, 0, 1} is a null set, we can choose fZ(z) = 0, which is suﬃcient at these
points.
Note that
lim
z→0 fZ(z) = ∞,
and that it follows by l’Hospital’s rule that
lim
z→±1
2
π2 · ln |z|
z2 −1 = 2
π2
lim
z→±1
1
z
2z = 2
π2
lim
z→±1
1
2z2 = 1
π2 .
Download free eBooks at bookboon.com

Random variables I
 
114 
8. Functions of two random variables, f(X.Y)
Example 8.3 Let X and Y be independent random variables, both rectangularly distributed over the
interval ]0, 1[.
1) Find the frequency of the random variable XY .
2) Find the frequency of the random variable X
Y .
3) Find P{Y > 2X}.
The two independent random variables X and Y have the same frequency,
f(x) =
⎧
⎨
⎩
1,
x ∈]0, 1[,
0,
otherwise.
0
0.5
1
1.5
2
y
–0.4 –0.2
0.2
0.4
0.6
0.8
1
1.2
1.4
x
Figure 45: The graph of the frequency g(s) of XY .
1) Since the values of XY lie in ]0, 1[, the frequency is for s ∈]0, 1[,
g(s) =
 1
x=s
f(x) f
 s
x
 1
x dx =
 1
x=s
1
x dx = −ln s,
thus
g(s) =
⎧
⎨
⎩
−ln s,
0 < s < 1,
0,
otherwise.
2) Since the values of X
Y lie in ]0, ∞[, it follows by an application of a formula for s ∈]0, ∞[ that the
frequency is given by
h(s) =
 ∞
0
f(s x) f(x) x dx.
This expression is only ̸= 0, if sx ∈]0, 1[ and x ∈]0, 1[, hence if 0 < x < 1 and 0 < x < 1
s. Then
we must split the investigation:
Download free eBooks at bookboon.com

Random variables I
 
115 
8. Functions of two random variables, f(X.Y)
–0.2
0
0.2
0.4
0.6
0.8
1
y
1
2
3
x
Figure 46: The graph of the frequency h(s) of X
Y .
a) If 0 < s ≤1, then 1 ≤1
s, hence
h(s) =
 1
0
1 · 1 · x dx = 1
2.
b) If 1 < s < ∞, then instead
h(s) =

1
s
0
x dx =
1
2s2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables I
 
116 
8. Functions of two random variables, f(X.Y)
Summing up we get
h(s) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
2,
0 < s ≤1,
1
2s2 ,
1 < s < ∞,
0,
s ≤0.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 47: The line y = 2x deﬁnes the upper triangle A, where y > 2x.
3) 1st variant. It follows from the geometry that
P{Y > 2X} = area(A) = 1
2 · 1
2 · 1 = 1
4.
2nd variant. It follows from (2) that
P{Y > 2X} = P
X
Y < 1
2

= 1
2.
An alternative solution is the following:
1) Since XY has its values lying in ]0, 1[, it follows from the ﬁgure that if s ∈]0, 1[. then
P{XY ≤s} = areal(A) =
 1
0
1 dx +
 1
s
s
x dx = s −s ln s,
and the frequency is obtained by a diﬀerentiation,
g(s) =
⎧
⎨
⎩
−ln s,
0 < s < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
117 
8. Functions of two random variables, f(X.Y)
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
Figure 48: The curve xy = s deﬁnes the domain A.
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
Figure 49: The domain A lies above the line x
y = s, 0 < s < 1.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1.2
Figure 50: The domain B lies above the line x
y = s, s > 1.
Download free eBooks at bookboon.com

Random variables I
 
118 
8. Functions of two random variables, f(X.Y)
2) It follows that the values of X
Y lie in ]0, ∞[. If s ∈]0, 1[, then it follows from the ﬁrst ﬁgure that
P
X
Y ≤s

= area(A) = 1
2 s,
0 < s < 1.
If s > 1, then it follows from the second ﬁgure that
P
X
Y ≤s

= area(B) = 1 −1
2s,
s ≥1.
Finally, the frequency is obtained by a diﬀerentiation,
h(s) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
2,
0 < s < 1,
1
2s2 ,
1 ≤s,
0,
s ≤0.
Example 8.4 Assume that X and Y are independent random variable, both attaining the values 1,
2, 3, 4, 5, 6. Assume that
fi = P {X = i} ,
gi = P {Y = i} ,
i = 1, 2, 3, 4, 5, 6.
It is well-known that if fi = gi = 1
6, i = 1, 2, 3, 4, 5, 6, then the probabilities P{X + Y = k}, k = 2,
3, . . . , 12, are not all the same.
Prove that no matter how fi and gi are chosen, we can never obtain P{X + Y = k} = 1
11 for all
k = 2, 3, . . . , 12.
Assume that
P{X + Y = k} = 1
11
for all k = 2, 3, . . . , 12.
If k = 2, then
1
11 = f1g1,
hence f1 > 0 and g1 > 0.
If k = 12, then
1
11 = f6g6,
hencer f6 > 0 and g6 > 0.
If k = 7, then
1
11 = f6g1 + {f5g2 + f4g3 + f3g4 + f2g5} + f1g6.
By subtracting the equation for k = 2 from the equation for k = 7, it follows by a rearrangement that
(f1 −f6) g1 = {f5g2 + f4g3 + f3g4 + f2g5} + f1g6 > 0,
Download free eBooks at bookboon.com

Random variables I
 
119 
8. Functions of two random variables, f(X.Y)
because {· · · } ≥0 and f1g6 > 0. Since g1 > 0, we must have f1 > f6.
If we subtract the equation for k = 12 from the equation for k = 7, then by a rearrangement.
(f6 −f1) g6 = {f5g2 + f4g3 + f3g4 + f2g5} + f6g1 > 0
for similar reasons. We conclude that f6 > f1.
These two claims cannot be simultaneously fulﬁlled, so the assumption must be wrong.
It even follows from the proof above that P{X + Y = 2}, P{X + Y = 7} and P{X + Y = 12} can
never have the same value, thus we can get a stronger result.
Alternatively we assume that we can choose the fi and the gj in such a way that the probabilities
are equal, i.e.
P{X + Y = k} = 1
11,
k = 2, . . . , 12.
Then in particular,
P{X + Y = 2} = f1g1 = 1
11
and
P{X + Y = 12} = f6g6 = 1
11,
hence f1g1 = f6g6. This is reformulated in the following way
f1
f6
= g6
g1
= x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables I
 
120 
8. Functions of two random variables, f(X.Y)
Considering the case k = 7 we get
1
11 = P{X + Y = 7} ≥f1g6 + f6g1 = f1g1
g6
g1
+ f6
f1

= 1
11

x + 1
x

.
Since x + 1
x > 1 (actually x + 1
x ≥2, when x > 0), this is not possible, and we have obtained a
contradiction, and the claim follows.
Example 8.5 Let the 2-dimensional random variable (X1, X2) have its frequency h (x1, x2) given by
h (x1, x2) =
⎧
⎪
⎨
⎪
⎩
1
πr2 ,
x2
1 + x2
2 < r2,
0,
otherwise,
(a uniform distribution over the disc x2
1 + x2
2 < r2).
Let the random variables Y1 and Y2 be given by
X1 = Y1 cos Y2,
X2 = Y1 sin Y2,
0 ≤Y1 < r,
0 ≤Y2 < 2π.
Find the frequency of the 2-dimensional random variable (Y1, Y2), and ﬁnd the marginal frequencies.
Are Y1 and Y2 independent?
Remark 8.1 This clearly corresponds to the transformation between rectangular and polar coordi-
nates over a ﬁxed disc. ♦
It follows that
x = (x1, x2) = ϕ(y) = (y1 cos y2, y1 sin y2) ,
y1 ∈[0, r[,
y2 ∈[0, 2π[.
The corresponding Jacobian is

∂x1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2

=

cos y2
−y1 sin y2
sin y2
y1 cos y2

= y1 ≥0.
Hence,
k (y1, y2) =
⎧
⎪
⎨
⎪
⎩
y1
πr2 ,
(y1, y2) ∈[0, r] × [0, 2π[,
0,
otherwise.
The frequencies of the marginal distributions are then
fY1 (y1) =
⎧
⎪
⎨
⎪
⎩
 2π
0
y1
πr2 dy2 = 2π · y1
πr2 = 2y1
r2 ,
y1 ∈[0, r[,
0,
otherwise,
Download free eBooks at bookboon.com

Random variables I
 
121 
8. Functions of two random variables, f(X.Y)
and
fY2 (y2) =
⎧
⎪
⎨
⎪
⎩
 r
0
y1
πr2 dy1 = 1
2π ,
y2 ∈[0, 2π[,
0,
otherwise.
It follows immediately that
k (y1, y2) = fY1 (y1) fY2 (y2) ,
hence Y1 and Y2 are stochastically independent.
Example 8.6 Let the 2-dimensional random variable (X1, X2) have the frequency
h (x1, x2) =
⎧
⎨
⎩
x1 + x2,
for 0 < x1 < 1 og 0 < x2 < 1,
0,
otherwise,
and let (Y1, Y2) = τ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = X2.
1) Prove that τ maps ]0, 1[ × ]0, 1[ bijectively onto the domain
D′ =

(y1, y2) ∈R2 | 0 < y2 < 1, y2 < y1 < y2 + 1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 51: The domain D′.
Since τ (x1, x2) = (x1 + x2, x2), i.e.
y1 = x1 + x2,
y2 = x2,
Download free eBooks at bookboon.com

Random variables I
 
122 
8. Functions of two random variables, f(X.Y)
it follows that the inverse map τ −1 exists,
τ −1 (y1, y2) = (y1 −y2, y2) ,
thus
x1 = y1 −y2,
x2 = y2.
The conditions 0 < x1 < 1 and 0 < x2 < 1 can now be replaced by
0 < y1 −y2 < 1,
0 < y2 < 1,
hence
y2 < y1 < y2 + 1
and
0 < y2 < 1,
and we have proved that τ maps ]0, 1[ × ]0, 1[ bijectively onto
D′ =

(y1, y2) ∈R2 | 0 < y2 < 1, y2 < y1 < y2 + 1

.
The Jacobian is then given by
Jτ (y1, y2) = ∂(x1, x2)
∂(y1, y2) =

1
−1
0
1
 = 1.
Then (Y1, Y2) i D′ has the frequency
k (y1, y2) = h (y1 −y2, y2) · 1 = y1,
thus
k (y1, y2) =
⎧
⎨
⎩
y1,
for 0 < y2 < 1 and y2 < y1 < y2 + 1,
0,
otherwise.
–0.2
0
0.2
0.4
0.6
0.8
1
1.2
y
0.5
1
1.5
2
x
Figure 52: The graph of FY1 (y1).
Download free eBooks at bookboon.com

Random variables I
 
123 
8. Functions of two random variables, f(X.Y)
Marginal frequencies.
1) We get for Y1 by a vertical integration
a) If 0 < y1 ≤1, then
fY1 (y1) =
 y1
y2=0
k (y1, y2) dy2 =
 y1
y2=0
y1 dy2 = y2
1.
b) If 1 < y1 < 2, then
fY1 (y1) =
 1
y2=y1−1
k (y1, y2) dy2 =
 1
y2=y1−1
y1 dy2 = y1 (2 −y1) ,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
124 
8. Functions of two random variables, f(X.Y)
hence summing up,
fY1 (y1) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
y2
1,
0 < y1 ≤1,
y1 (2 −y1) = 1 −(y1 −1)2 ,
1 < y1 < 2,
0,
otherwise.
–0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
y
–0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 53: The graph of fY2 (y2).
2) For Y2 it follows by a horizontal integration for 0 < y2 < 1 that
fY2 (y2)
=
 y2+1
y1=y2
k (y1, y2) dy1 =
 y2+1
y1=y2
y1 dy1 =
1
2 y2
1
y2+1
y1=y2
=
1
2

(y2 + 1)2 −y2
2
 
= 1
2 (2y2 + 1) · 1 = y2 + 1
2,
thus summing up
fY2 (y2) =
⎧
⎪
⎨
⎪
⎩
y2 + 1
2,
0 < y2 < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
125 
8. Functions of two random variables, f(X.Y)
Example 8.7 Assume that the 2-dimensional random variable (X1, X2) has the frequency
h (x1, x2) =
⎧
⎨
⎩
2 exp (−(x1 + x2)) ,
(x1, x2) ∈D,
0,
otherwise.
where
D =

(x1, x2) ∈R2 | 0 < x2 < x1 < ∞

,
and let (Y1, Y2) = τ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = (X1 −X2)2 .
1) Prove that τ maps D bijectively onto the domain
D′ =

(y1, y2) ∈R2 | 0 < y1 < ∞, 0 < y2 < y2
1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
4) Are Y1 and Y2 independent random variables?
0
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 54: The domain D lies between the X1 axis and the line x2 = x1.
1) It follows from
y1 = x1 + x2
and
y2 = (x1 −x2)2
og
x1 −x2 > 0
that
x1 + x2 = y1
and
x1 −x2 = +√y2,
hence
x1 = 1
2 {y1 + √y2}
and
x2 = 1
2 {y1 −√y2} .
Download free eBooks at bookboon.com

Random variables I
 
126 
8. Functions of two random variables, f(X.Y)
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 55: The domain D′ lies between the Y1 axis and the parabola y2 = y2
1.
Since (x1, x2) is uniquely determined by (y1, y2), we conclude that τ is bijective. The boundary
curve x2 = x1 is mapped into y2 = 0. The boundary curve x2 = 0 is mapped into y1 = √y2, i.e.
y2 = y2
1, y1 ≥0. Since x2 > 0 in D, we must have y1 > √y2, thus 0 < y2 < y2
1, and we have proved
that τ maps D bijectively onto D′.
2) We next compute the Jacobian,
∂(x1, x2)
∂(y1, y2) =

∂x1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2

=

1
2
1
4
1
√y2
1
2
−1
4
1
√y2

= −1
4
1
√y2
< 0.
It follows from x1 + x2 = y1 that the frequency of (Y1, Y2) is given by
k (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2 e−y1 · 1
4
1
√y2
= 1
2
e−y1
√y2
for (y1, y2) ∈D′,
0,
for (y1, y2) /∈D′.
3) The marginal frequency of Y1 is obtained by a vertical integration,
fY1 (y1) = 1
2
 y2
1
0
e−y1
√y2
dy2 = e−y1 [√y2]y2
1
0 = y1 e−y1
for y1 > 0,
and fY1 (y1) = 0 for y ≤0.
The frequency of Y2 is obtained by a horizontal integration,
fY2 (y2) = 1
2
 ∞
√y2
e−y1
√y2
dy1 = 1
2
e−√y2
√y2
for y2 > 0,
and fY2 (y2) = 0 for y2 ≤0.
Download free eBooks at bookboon.com

Random variables I
 
127 
8. Functions of two random variables, f(X.Y)
4) Since D′ is not a rectangle, it follows immediately that Y1 and Y2 are not independent. It also
follows from
fY1 (y1) · fY2 (y2) ̸= k (y1, y2) .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables I
 
128 
8. Functions of two random variables, f(X.Y)
Example 8.8 Assume that X1 and X2 are independent identically distributed random variables of
the frequency
f(x) =
⎧
⎨
⎩
x e−x,
x > 0,
0,
otherwise.
1. Compute the means E {X1} and E
 1
X1

.
2. Compute the probability P {X2 > X1}.
Deﬁne the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1
X2
.
It will without proof be given that the vector function τ given by
τ (x1, x2) =

x1 + x2, x1
x2

maps R+ × R+ bijectively onto itself.
3. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
(This question can be answered both with and without an application of the answer of question 3.).
5. Check if Y1 and Y2 are independent.
6. Compute the mean E {Y2}.
7. Find, e.g. by an application of question 2., the median of Y2.
8. Give an intuitive explanation of why the median of Y2 is smaller than the mean of Y2.
1) The means are
E {X1} =
 ∞
0
x2e−x dx = 2
and
E
 1
X1

=
 ∞
0
x
x e−x dx = 1.
2) By the symmetry,
P {X2 > X1} = P {X1 > X2} = 1
2 (P {X2 > X1} + P {X1 > X2}) = 1
2.
Alternatively, the frequency of Y2 = X1
X2
is zero for y2 ≤0, and when y2 > 0, then
fY2 (y2)
=
 ∞
−∞
f (y2x) f(x) · |x| dx =
 ∞
0
y2x e−y2x · x e−x · x dx
=
y2
 ∞
0
x3e−(1+y2)x dx =
y2
(1 + y2)4
 ∞
0
t3e−t dt =
6y2
(1 + y2)4 ,
Download free eBooks at bookboon.com

Random variables I
 
129 
8. Functions of two random variables, f(X.Y)
hence
P {X2 > X1}
=
P
X1
X2
< 1

=
 1
0
6y2
(1 + y2)4 dy2 = 6
 1
0

1
(1 + y2)3 −
1
(1 + y2)4

dy2
=
6

−1
2
1
(1 + y2)2 + 1
3
1
(1 + y2)3
1
0
= −3
22 + 2
23 + 3
12 −2
13
=
1 −3
4 + 1
4 = 1
2.
3) It follows from y1 = x1 + x2 and y2 = x1
x2
that
x1 = y2x2
and
y1 = x1 + x2 = (y2 + 1) x2,
hence
x1 = y1y2
y2 + 1 = y1 −
y1
y2 + 1
and
x2 =
y1
y2 + 1.
The Jacobian is
∂(x1, x2)
∂(y1, y2) =

y2
y2 + 1
y1
(y2 + 1)2
1
y2 + 1
−
y1
(y2 + 1)2

= −
y1
(y2 + 1)3 (y2 + 1) = −
y1
(y2 + 1)2 < 0.
The simultaneous frequency of (X1, X2) is
g (x1, x2) =
⎧
⎨
⎩
x1x2 e−(x1+x2)
for x1 > 0 og x2 > 0,
0
otherwise,
hence the simultaneous frequency of (Y1, Y2) is 0 for y1 ≤0 or y2 ≤0, and
k (y1, y2)
=
y1y2
y2 + 1 ·
y1
y2 + 1 · e−y1 ·
y1
(y2 + 1)4
=
y3
1y2
(y2 + 1)4 e−y1,
for y1 > 0 and y2 > 0,
which also can be written
k (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
6 y3
1e−y1 ·
6y2
(y2 + 1)4
for y1 > 0 and y2 > 0,
0
otherwise.
4) It follows from the rewriting of 3.
(possibly by the second variant of 2.)
that the marginal
frequencies are
kY1 (y1) =
⎧
⎪
⎨
⎪
⎩
1
6 y3
1e−y1
for y1 > 0,
0
for y1 ≤0,
Download free eBooks at bookboon.com

Random variables I
 
130 
8. Functions of two random variables, f(X.Y)
kY2 (y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
6y2
(y2 + 1)4
for y2 > 0,
0
for y2 ≤0.
5) It is obvious that Y1 and Y2 are independent because
k (y1, y2) = kY1 (y1) · kY2 (y2) .
6) Since X1 and X2 are independent, the mean is
E {Y2} = E
X1
X2

= E {X1} · E
 1
X2

= 2 · 1 = 2.
7) By question 2,
1
2 = P {X2 > X1} = P
X1
X2
< 1

= P {Y2 < 1} ,
hence the median is ⟨Y2⟩= 1.
Alternatively, Y2 has the distribution function
KY2 (y2)
=
 y2
0
6t
(t + 1)4 dt =
 y2+1
1
6(u −1)
u4
du = 6
 y2+1
1
 1
u3 −1
u4

du
=
6

−1
2
1
u2 + 1
3
1
u3
y2+1
1
= 1 −
3
(y2 + 1)2 +
2
(y2 + 1)3 = 1 −3y2 + 1
(y2 + 1)3 .
If we put KY2 (y2) = 1
2, then
3y2 + 1
(y2 + 1)3 = 1
2,
dvs.
6y2 + 2 = (y2 + 1)3 ,
or
y3
2 + 3y2
2 −3y2 −1 = (y2 −1)
	
y2
2 + 4y2 + 1

= 0.
The only positive solution is y2 = 1, which is the median.
8) The mass of probability is divided into two equal parts by the median 1. However, the mass of
probability is scattered more to the right of 1 then to the left of 1. Thus, the mean must lie to the
right of 1.
Download free eBooks at bookboon.com

Random variables I
 
131 
9. Means and moments of higher order
9
Means and moments of higher order
Example 9.1 Let X be a random variable of values in N0. Prove that E{X} exists, if and only if
∞

k=0
P{X > k} < ∞,
and that in the aﬃrmative case,
E{X} =
∞

k=0
P{X > k}.
It is often easier to apply this formula by computation of means.
First note that a necessary and suﬃcient condition of the existence of E{X} and is equal to
E{X} =
∞

n=0
n P{X = n} =
∞

n=1
n P{X = n},
is that
∞

n=1
n P{X = n} is absolutely convergent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables I
 
132 
9. Means and moments of higher order
Since the terms are non-negative, this is equivalent with
∞

n=1
n P{X = n} is (just) convergent.
If )∞
k=0 P{X > k} is convergent, then by a (correct) interchanging of the order of summation (because
all terms are ≥0),
∞

k=0
P{X > k} =
∞

k=0
∞

n=k+1
P{X = n} =
∞

n=1
n−1

k=0
P{X = n} =
∞

n=1
n P{X = n} = E{X}.
If conversely E{X} exists, we just repeat the computations above in the reverse order.
Example 9.2 Two persons A and B play the following game:
They each throw two coins. The winner is he who gets most heads in a throw. The game is a draw,
if they obtain an equal number of heads.
1. What is the probability q that the game is a draw?
2. What is the probability pA that A wins?
If the game is a draw, the game is continued in the same way. One stops ﬁrst time one of the two
players wins.
3. What is the probability that A wins in game number k?
4. Find the mean of the number of games.
B \ A
TT
TH
HT
HH
TT
0
1
1
1
TH
-1
0
0
1
HT
-1
0
0
1
HH
-1
-1
-1
0
Table 1: If A wins, we write 1. If B wins, we write -1. In case of a draw we write 0.
1) We write 1 if A wins, and −1 if B wins. Finally, 0 means a draw. Tail is denoted by T, and head
by H. Since the 16 possibilities all have the same probability, we get by simply counting
q = P{a draw} = 6
16 = 3
8.
2) It follows from (1) and the symmetry of A and B that
P{A wins} = pA = pB = P{B wins} = 5
16.
Download free eBooks at bookboon.com

Random variables I
 
133 
9. Means and moments of higher order
3) If A wins in game number k, then the ﬁrst k −1 games must all have been draws, hence
P{A wins in the kth game} = qk−1 · pA =
3
8
k−1
· 5
16.
4) Let X denote the number of games. Then
P{X = k} = P{A wins in game number k} + P{B wins in game number k} = 5
8 ·
3
8
k−1
.
When |x| < 1, then
∞

k=1
k xk−1 = d
dx
 ∞

k=0
xk

= d
dx

1
1 −x

=
1
(1 −x)2 .
Using this result we get the mean
E{X} = 5
8
∞

k=1
k
3
8
k−1
= 5
8 ·
1

1 −3
8
2 = 8
5.
Example 9.3 A box contains N balls of the numbers from 1 to N. Select at random n balls with
replacement. Let Xn denote the random variable which indicates the largest selected number.
Find the distribution of Xn.
Find the mean E {Xn}, and prove for large N that this mean is approximately equal to
n
n + 1 N.
Let X denote one selection at random of n numbers. Since all numbers have the same probability,
the distribution function is given by
FX(k) = P{X ≤k} = k
N ,
k = 1, 2, . . . , N.
Thus we derive the distribution function of Xn,
FXn(k) = P

max
j=1,...,n Xj ≤k

= (P{X ≤k})n =
 k
N
n
,
k = 1, 2, . . . , N,
hence
pk = P {Xn = k} = P {Xn ≤k} −P {Xn ≤k −1} = kn −(k −1)n
N n
.
The mean is
E {Xn}
=
N

k=1
k pk =
1
N n
⎧
⎨
⎩
N

k=1
kn+1 −
N

k=1 (=2)
k(k −1)n
⎫
⎬
⎭=
1
N n
 N

k=1
kn+1 −
N−1

k=1
(k + 1)kn

=
1
N n

N n+1 −

k=1
N −1kn

= N

1 −1
N
N−1

k=1
 k
N
n
.
Download free eBooks at bookboon.com

Random variables I
 
134 
9. Means and moments of higher order
Then notice that
1
N
)N−1
k=1
 k
N
n
can be interpreted as an approximating sum of the integral
 1
0 xn dx =
1
n + 1, hence
1
N
N−1

k=1
 k
N
n
→
 1
0
xn dx =
1
n + 1
for N →∞.
Then
E {Xn} = N

1 −
1
n + 1 + 1
N ε
 1
N

≈
n
n + 1 N
for store N.
Example 9.4 Let X be a random variable of the distribution function F(x), the frequency f(x) and
the mean μ. Prove that
μ =
 ∞
0
{1 −F(x)}dx −
 0
−∞
F(x) dx.
It is given that
 ∞
−∞|x| f(x) dx < ∞, and that
(6) μ =
 ∞
−∞
x f(x) dx =
 0
−∞
x f(x) dx +
 ∞
0
x f(x) dx.
Let A > 0. Then
 0
−A
x f(x) dx = [x F(x)]0
−A −
 0
−A
F(x) dx = A F(−A) −
 0
−A
F(x) dx,
and
 A
0
x f(x) dx = [x{F(x) −1}]A
0 +
 A
0
{1 −F(x)} dx = −A{1 −F(A)} +
 A
0
(1 −F(x)) dx.
Since
0 ≤A F(−A) = A
 −A
−∞
f(x) dx ≤
 −A
−∞
|x| f(x) dx →0
for A →∞,
we conclude that A F(−A) →0 for A →∞. Since
 0
−∞x f(x) dx is absolutely convergent, it follows
by taking the limit A →∞that
 0
−∞
x f(x) dx = lim
A→∞

A F(−A) −
 0
−A
F(x) dx

= −
 0
−∞
F(x) dx.
Analogously,
0 ≤A{1 −F(A)} = A
 ∞
A
f(x) dx ≤
 ∞
A
|x| f(x) dx →0
for A →∞,
Download free eBooks at bookboon.com

Random variables I
 
135 
9. Means and moments of higher order
so when A →∞, we conclude in the same way that
 ∞
0
x f(x) dx = lim
A→∞

−A{1 −F(A)} +
 A
0
{1 −F(x)} dx

=
 ∞
0
{1 −F(x)} dx,
where the integrals are even absolutely convergent.
Finally, by insertion into (6) we get
μ =
 0
−∞
x f(x) dx +
 ∞
0
x f(x) dx =
 ∞
0
{1 −F(x)} dx −
 0
−∞
F(x) dx
as required.
Alternatively, a more streamlined, though also more sophisticated method is the following. We
see that
 ∞
0
{1 −F(x)} dx =
 ∞
x=0
 ∞
y=x
f(y) dy

dx =
 ∞
y=0
f(y)
 y
x=0
1 dx

dy =
 ∞
0
y f(y) dy,
and
−
 0
−∞
F(x) dx
=
−
 0
x=−∞
 x
y=−∞
f(y) dy

dx =
 0
y=−∞
f(y)
 0
x=y
(−1) dx

dy
=
 0
−∞
y f(y) dy.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
136 
9. Means and moments of higher order
By adding these two expressions, we formally obtain that
 ∞
0
{1 −F(x)} dx −
 0
−∞
F(x) dx =
 ∞
−∞
y f(y) dy = μ.
However, we have assumed that the mean exists, which implies that all the integrals above are abso-
lutely convergent, so the formal calculation is also real.
Example 9.5 Let X be an non-negative random variable of the distribution function F(x) and fre-
quency f(x). Prove that
(7) E

Xk
= k
 ∞
0
xk−1{1 −F(x)} dx,
k ∈N.
(If the kth moment does not exist, then both the right hand side and the left hand side of (7) are equal
to ∞.)
Find a similar formula, if the random variable X is non-positive.
Remark. One can prove that formula (7) holds for every non-negative random variable X.
1) Assume that X is non-negative and that E

Xk
exists, i.e.
0 ≤E

Xk
=
 ∞
0
xkf(x) dx < ∞.
Let A > 0. Then by a partial integration,
 A
0
xkf(x) dx
=
#
xk{F(x) −1}
$A
0 + k
 A
0
xk−1{1 −F(x)} dx
=
−Ak{1 −F(A)} + k
 A
0
xk−1{1 −F(x)} dx.
Now
 ∞
0
xkf(x) dx < ∞, so we get the estimate
0 ≤Ak{1 −F(A)} = Ak
 ∞
A
f(x) dx ≤
 ∞
A
xk f(x) dx →0
for A →∞.
Then by taking the limit A →∞,
E

Xk
=
 ∞
0
xkf(x) dx = k
 ∞
0
xk−1{1 −F(x)} dx.
It is trivial that
 A
0
xkf(x) dx ≤k
 A
0
xk−1{1 −F(x)} dx,
so we infer that if E

Xk
= ∞, then k
 ∞
0
xk−1{1 −F(x)} dx = ∞.
Download free eBooks at bookboon.com

Random variables I
 
137 
9. Means and moments of higher order
2) Then assume that X is non-positive and that E

Xk
exists, i.e.
0 ≤
E

Xk =
 0
−∞
|x|kf(x) dx < ∞.
If A > 0, then
 0
−A
xkf(x) dx =
#
xkF(x)
$0
−A −k
 0
−A
xk−1F(x) dx = −(−A)kF(−A) −k
 0
−A
xk−1F(x) dx.
Since
0 ≤AkF(−A) = Ak
 −A
−∞
f(x) dx ≤
 −A
−∞
|x|kf(x) dx →0
for A →∞,
it follows by taking the limit that
(8) E

Xk
=
 0
−∞
xkf(x) dx = −k
 0
−∞
xk−1F(x) dx.
Clearly,
 0
−A
xkf(x) dx,
(−A)kF(−A)
og
−k
 0
−A
xk−1F(x) dx
have all the same sign. Hence,

 0
−A
xkf(x) dx
 ≤
k
 0
−A
xk−1F(x) dx
 .
Therefore, if E

Xk
does not exist, then both integrals of (8) are divergent.
Alternatively and more streamlined (and also more sophisticated), because one at ﬁrst does not
care so much for the convergence of the integrals (this should of course be done at last), we have the
following proof:
When k ∈N, and X is non-negative of the distribution function F(x) and the frequency f(x), then
k
 ∞
0
xk−1{1 −F(x)} dx = k
 ∞
x=0
xk−1
 ∞
y=x
f(y) dy

dx
=
 ∞
y=0
f(y)
 y
x=0
k xk−1 dx

dy =
 ∞
0
ykf(y) dy = E

Xk
.
Then notice that if the kth moment does not exist, then all the integrals involved are divergent of the
value ∞. Since the integrand is non-negative, we can interchange the order of integration. Conversely,
if the kth moment exists, then all the involved integrals are convergent with a non-negative integrand,
hence uniformly convergent, and all the computations are legal.
Then let X ≤0 have the distribution function F(x) and the frequency f(x). Then for k ∈N,
−k
 0
−∞
xk−1F(x) dx = −k
 0
x=−∞
xk−1
 x
y=−∞
f(y) dy

dx
=
 0
y=−∞
f(y)
 0
x=y
	
−k xk−1
dx

dy =
 0
−∞
ykf(y) dy = E

Xk
.
Download free eBooks at bookboon.com

Random variables I
 
138 
9. Means and moments of higher order
Thus we get in this case the formula
E

Xk
= −k
 0
−∞
xk−1F(x) dx.
We still have to remark that in the case of divergence the left hand side and the right hand side are
either both −∞or both +∞. It the kth moment exists, then all integrals are absolutely convergent.
Example 9.6 Let X and Y be non-negative random variables of distribution functions FX and FY ,
means E{X} and E{Y } variances V {X} and V {Y }.
The random variable X is said to be stochastically larger than Y , if
FX(x) ≤FY (x)
for all x ∈R.
1) If so, prove that E{Y } ≤E{X}.
2) Can one also conclude that V {Y } ≤V {X}?
1) Since X and Y are non-negative, it follows from Example 9.4 or Example 9.5 that
E{Y } =
 ∞
0
{1 −FY (x)} dx ≤
 ∞
0
{1 −FX(x)} dx = E{X}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 56: Illustration of FX(x) ≤FY (x) in (2).
2) The answer is “no”! We construct a counterexample. Let a ∈
1
2, 1

, and let X be rectangularly
distributed over ]a, 1[, and let Y be rectangularly distributed over ]0, a[. Then clearly X and Y
are non-negative, and FX(x) ≤FY (y), cf. the ﬁgure, hence X is stochastically larger than Y .
This is in agreement with
E{Y } = a
2 < 1 + a
2
= E{X}.
Download free eBooks at bookboon.com

Random variables I
 
139 
9. Means and moments of higher order
For the variances, however, we get
V {Y } = a2
12 > (1 −a)2
12
= V {X},
because a > 1 −a > 0 for 1
2 < a < 1.
Example 9.7 Let X be a random variable satisfying
E{X} = E

X2
= 1.
Find the distribution function of X.
Since
V {X} = E

X2
−(E{X})2 = 1 −1 = 0,
it follows that X is a constant, and since E{X} = 1, we get X ≡1. The distribution function is then
FX(x) =
⎧
⎨
⎩
1
for x ≥1,
0
for x < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
140 
9. Means and moments of higher order
Example 9.8 Let X be a random variable, for which the kth moment exists for some k ∈N \ {1}.
Prove the following generalization of Chebyshev’s inequality:
For every a ∈R+,
P{|X −E{X}| ≥a} ≤1
ak E

|X −E{X}|k
.
When E

Xk
exists for some k ∈N\{1}, then E

Xj
exists for every j = 1, . . . , k. In fact, |x|j ≤1
for |x| ≤1, and |x|j ≤|x|k for |x| ≥1, so
 ∞
−∞
|x|jf(x) dx ≤

{|x|≤1}
1 · f(x) dx +

{|x|≥1}
|x|kf(x) dx < ∞.
In particular, E{X} exists.
Then
E

|X −E{X}|k
=
 ∞
−∞
|x−E{X}|kf(x) dx ≥

{|x−E{X}≥a}
akf(x) dx = akP{|X −E{X}| ≥a},
and the inequality follows by a division by ak.
Example 9.9 Let X be a random variable.
1) Let g : R →R be an even, non-negative function, which is increasing on [0, ∞[.Prove that
P{|X| ≥a} ≤E{g(X)}
g(a)
for every a ∈R+.
2) Let g : R →R be a non-negative, increasing function. Prove that
P{X ≥a} ≤E{g(X)}
g(a)
for every a ∈R.
We always assume that E{g(X)} exists.
In the main proof we assume that X is of continuous type, i.e.
E{g(X)} =
 ∞
−∞
g(x) f(x) dx.
The similar proofs when X is either of discrete type or of mixed type are obtained by simple modiﬁ-
cations of the main proof.
1) First by a splitting
P{|X| ≥a} = P{X ≥a} + P{X ≤−a} =
 ∞
a
f(x) dx +
 −a
−∞
f(x) dx.
According to the assumptions, g(x) ≥0 and
g(a) ≤g(x) for x ≥a
and
g(−a) = g(a) ≤g(x) for x ≤−a,
Download free eBooks at bookboon.com

Random variables I
 
141 
9. Means and moments of higher order
hence
g(a)P{|X| ≥a} = g(a)
 ∞
a
f(x) dx + g(−a)
 −a
−∞
f(x) dx
≤
 ∞
a
g(x) f(x) dx +
 −a
−∞
g(x) f(x) dx ≤
 ∞
−∞
g(x) f(x) dx = E{g(X)}.
If g(a) ̸= 0, the result is obtained by a division by g(a) > 0.
2) Similarly,
g(a)P{X ≥a} = g(a)
 ∞
a
f(x) dx ≤
 ∞
a
g(x) f(x) dx ≤
 ∞
−∞
g(x) f(x) dx = E{g(X)}.
If g(a) ̸= 0, the result follows by a division by g(a) > 0.
Example 9.10 A random variable X is assumed to have the mean μ and the variance σ2. Prove that
if a is a median of X, then
|a −μ| ≤
√
2 · σ.
Hint. Apply Chebyshev’s inequality.
When a is a median, then
F(a) ≥1
2
and
F(a−) ≤1
2.
One of the sets {x ≤a} and {x ≥a} must necessarily be contained in the set {|x −μ| ≥|a −μ|}, and
since P{X ≤a} ≥1
2 and P{X ≥a} ≥1
2, we get
1
2 ≤P{|X −μ| ≥|a −μ|} ≤
σ2
|a −μ|2 , for a ̸= μ.
Then by a rearrangement,
|a −μ| ≤
√
2 · σ.
If a = μ, there is of course nothing to prove.
Download free eBooks at bookboon.com

Random variables I
 
142 
9. Means and moments of higher order
Example 9.11 Let X be a random variable, for which all moments mk = E

Xk
exist. We deﬁne
for k ∈N the kth decreasing moment
m(k) = E{X(X −1)(X −2) · · · (X −k + 1)}.
1) Find for n = 1, 2, 3, the nth decreasing as a linear combination of the kth moments k ≤n.
2) Find for n = 1, 2, 3, the nth moment as a linear combination of the kth decreasing moments for
k ≤n.
A. If n = 1, then
m(1) = E{X} = m1.
B. If n = 2, then
m(2) = E{X(X −1)} = E

X2
−E{X} = m2 −m1,
and hence
m2 = m(2) + m(1).
C. If n = 3, then
m(3)
=
E{X(X −1)(X −2)} = E

X3
−3E

X2
+ 2E{X}
=
m3 −3m2 + 2m1,
and conversely,
m3 = m(3) + 3m2 −2m(1) = m(3) + 3m(2) + m(1).
Example 9.12 Let X be a random variable, for which all moments mk = E

Xk
exist. It is then
well-known that all the central moments
vk = E

(X −m1)k 
.
also exist.
1) Express for n = 2, 3, 4, the nth central moment by the kth moments for k ≤n.
2) Express for n = 2, 3, 4, the nth moment by the kth central moments for k ≤n and by m1.
A. If n = 2, then
v2 = E

(X −m1)2 
= E

X2 −2m1X + m2
1

= m2 −m2
1,
and thus
m2 = v2 + m2
1.
Download free eBooks at bookboon.com

Random variables I
 
143 
9. Means and moments of higher order
B. If n = 3, then
v3
=
E

(X −m1)3 
= E

X3 −3m1X2 + 3m1X2 + 3m2
1X −m3
1

=
m3 −3m1m2 + 2m3
1,
and thus
m3 = v3 + 3m1
	
v2 + m2
1

−2m3
1 = v3 + 3m1v2 + m3
1.
C. If n = 4, then
v4
=
E

(X −n1)4 
= E

X4 −4m1X3 + 6m2
1X2 −4m3
1X + m4
1

=
m4 −4m1m3 + 6m2
1m2 −3m4
1,
and thus
m4
=
v4 + 4m1

v3 + 3m1v2 + m3
1

−6m2
1

v2 + m2
1

+ 3m4
1
=
v4 + 4m1v3 + 6m2
1v2 + m4
1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables I
 
144 
9. Means and moments of higher order
Example 9.13 A real function ϕ deﬁned on an open interval I is said to be convex, if for all a ∈I
there exists a real number ca, such that
ϕ(x) −ϕ(a) ≥ca(x −a)
for every x ∈I
(if ϕ is diﬀerentiable at the point a, then ca = ϕ′(a)).
Let X be a random variable, which only has values in the open interval I, and let ϕ : I →R be convex.
Assuming that both E{X} and E{ϕ(X)} exist, prove Jensen’s inequality
E{ϕ(X)} ≥ϕ(E{X}).
1) Let us ﬁrst check where the assumption of convexity can be applied. Hence, we insert as a test
any a ∈I,
E{ϕ(X)} −ϕ(a) = E{ϕ(X) −ϕ(a)} ≥ca E{X −a} = ca(E{X} −a).
2) Since E{X} ∈I, we get by choosing a = E{X},
E{ϕ(X)} −ϕ(E{X}) ≥0,
i.e.
E{ϕ(X)} ≥ϕ(E{X}).
Download free eBooks at bookboon.com

Random variables I
 
145 
10. Mean and variance in special cases
10
Mean and variance in special cases
Example 10.1 A random variable X has the frequency
f(x) =
⎧
⎨
⎩
a e−ax,
x > 0,
0,
x ≤0,
where a is a positive constant.
Find the frequency of Y = X2, and ﬁnd the mean and variance of Y .
The distribution function G(y) of Y = X2 is 0 for y ≤0. If y > 0, then
G(y)
=
P{Y ≤y} = P

X2 ≤y

= P {−√y ≤X ≤√y} = P {X ≤√y}
=
 √y
0
a e−ax dx = 1 −e−a√y,
for y > 0.
Thus, the corresponding frequency is
g(y) =
⎧
⎪
⎨
⎪
⎩
G′(y) =
a
2√y exp
	
−a√y

,
for y > 0,
0,
for y ≤0.
Then the mean of Y is
E{Y } = E

X2
=
 ∞
0
x2 · a e−ax dx = 1
a2
 ∞
0
t2 e−t dt = 2
a2 .
The variance of Y is
V {Y }
=
E

Y 2
−(E{Y })2 = E

X4
−
 2
a2
2
=
 ∞
0
x4 a e−ax dx −4
a4
=
1
a4
 ∞
0
t4 e−t dt −4
a4 = 4!
a4 −4
a4 = 24 −4
a4
= 20
a4 .
Example 10.2 Let X be rectangularly distributed over ] −h, h[.
Compute for k ∈N the moments
E

Xk
and
E

|X|k
.
We ﬁrst notice that
E

|X|k
= 1
2h
 h
−h
|x|k dx = 1
h
 h
0
xk dx =
hk
k + 1.
Then by the symmetry,
E

X2k+1
= 0,
k ∈N0,
2k + 1 odd,
Download free eBooks at bookboon.com

Random variables I
 
146 
10. Mean and variance in special cases
and
E

X2k
= E

|X|2k
=
h2k
2k + 1.
Example 10.3 Draw from the point (a, b), where b > 0, a line of the angle θ with the line x = a,
where θ ∈
(
−π
2 , π
2
'
. The line intersects the x axis at a point of the abscissa X.
X
v
a.b
0
0.5
1
1.5
2
0.5
1
1.5
2
Assuming that Θ(= v) is rectangularly distributed over
(
−π
2 , π
2
'
, ﬁnd the frequency of X.
Check if X has a mean.
The distribution of X is called a Cauchy distribution.
The relationship between the random variables X and Θ is
tan Θ = X −a
b
,
i.e.
Θ = τ −1(X) = Arctan X −a
b
,
[and X = τ(Θ) = a + b tan Θ]. Since Θ has the frequency
f(θ) =
⎧
⎪
⎨
⎪
⎩
1
π ,
−π
2 < θ < π
2 ,
0,
otherwise,
and τ −1(x) ∈
(
−π
2 , π
2
'
for every x ∈R, we derive that the frequency of X is
g(x) = f
	
τ −1(x)

·

dθ
dx
 = 1
π ·
1
1 +
	 x−a
b

2 · 1
b =
b
π {b2 + (x −a)2},
x ∈R.
Download free eBooks at bookboon.com

Random variables I
 
147 
10. Mean and variance in special cases
It follows from

x g(x) dx = b
π

x −a + a
b2 + (x −a)2 dx = b
2π ln

b2 + (x −a)2
+ a
π Arctan
x −a
b

,
that
 A
0
x · g(x) dx →∞
for A →∞,
(improper integral),
so X has no mean.
Alternatively,
|x| · g(x) ∼k · 1
|x|
for large x,
hence
 ∞
−∞
|x| g(x) dx = ∞.
Example 10.4 A line segment of length 1 is divided randomly into two parts of lengths X and 1−X,
where we assume that X is rectangularly distributed over ]0, 1[.
Let Y denote the length of the smallest of the two line segments, and let Z denote the length of the
largest line segment.
1) Find the distribution of Y and the distribution of Z.
2) Find the mean and variance of Z.
1) The distribution function of X is
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
for x ≤0,
x,
for 0 < x < 1,
1,
for x ≥1.
Obviously, Y = min{X, 1 −X} takes its values in

0, 1
2

. If y ∈

0, 1
2

, then
P{X ≤y or 1 −X ≤y} = P{0 < X ≤y} + P{1 −y ≤X < 1} = 2y,
hence
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1
for y ≥1
2,
2y
for 0 < y < 1
2,
0
for y ≤0,
and
fY (y) =
⎧
⎨
⎩
2
for 0 < y < 1
2,
0
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
148 
10. Mean and variance in special cases
Analogously, Z = max{X, 1 −X} takes its values in
1
2, 1

. If z ∈
1
2, 1

, then
FZ(z) = P{X ≤z and 1 −X ≤z} = P{1 −z ≤X ≤z} = 2z −1,
hence
FZ(z) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1
for z ≥1,
2z −1
for 1
2 < z < 1,
0
for z ≤1
2,
and
fZ(z) =
⎧
⎨
⎩
2
for 1
2 < z < 1,
0
otherwise.
2) Since Y is rectangularly distributed over
$
0, 1
2
#
, we get
E{Y } = 1
4
and
V {Y } = 1
12 ·
1
2
2
= 1
48.
Now, Z is rectangularly distributed over
1
2, 1

, so
E{Z} = 3
4
and
V {Z} = 1
12 ·
1
2
2
= 1
48.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables I
 
149 
10. Mean and variance in special cases
Example 10.5 A point A is chosen randomly in the unit square ]0, 1[ × ]0, 1[. This means that if X
and Y denote the abscissa and the ordinate, resp., of the point A, then (X, Y ) has the simultaneous
frequency
f(x, y) =
⎧
⎨
⎩
1,
in ]0, 1[ × ]0, 1[,
0,
otherwise.
1) Find the probability of the event that the distance from A to a given edge of the square is ≤t.
2) Let U denote the distance from A to the closest edge of the square. Find the distribution function
and the frequency of U.
3) Find the mean and the variance of U.
Valgt side
A
t
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1) Obviously, the probability that the distance from A to e.g. ]0, 1[ on the x axis, is < t for 0 < t < 1,
hence, the corresponding random variable is rectangularly distributed over ]0, 1[.
2) If U denotes the distance from A to the closest edge, then U has its values in ]0, 1
2[. If u ∈]0, 1
2[,
then U ≤u, if and only if A lies in the unions of the domains between a dotted line and the closest
parallel edge, so by considering an area,
FU(u) = 1 −(1 −2u)2 = 4
	
u −u2
= 4y −4u2,
0 < u < 1
2.
We conclude that the frequency is
fU(u) = F ′
U(u) = 4 −8u,
0 < u < 1
2,
and fU(u) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables I
 
150 
10. Mean and variance in special cases
1–2u
1-u
u
1–2u
1-u
u
0
0.2
0.4
0.6
0.8
1
–0.2
0.2
0.4
0.6
0.8
1
3) The mean is
E{U} =

1
2
0
u(4 −8u) du =

1
2
0
	
4u −8u2
du =

2u2 −8
3 u3
 1
2
0
= 1
2 −1
3 = 1
6.
Then compute
E

U 2
=

1
2
0
u2(4 −8u) du =

1
2
0
	
4u2 −8u3
du =
4
3 u3 −2u4
 1
2
0
= 1
6 −1
8 = 1
24,
so the variance is
V {U} = E

U 2
−(E{U})2 = 1
24 −1
36 = 1
72.
Download free eBooks at bookboon.com

Random variables I
 
151 
10. Mean and variance in special cases
Example 10.6 The function f is for 0 < x < 1 given by
f(x) =
1
π
%
x(1 −x)
,
while the function is equal to 0 for any other value of x.
1) Prove that f(x) is the frequency of a random variable X.
2) Find the mean and the variance of the random variable X.
3) Find the frequency of the random variable Y =
√
X.
4) Find the mean of the random variable Y .
The distribution of X is called the Arcussinus distribution.
1) Obviously, f(x) ≥0 for every x ∈R. Then
 ∞
−∞
f(x) dx
=
1
π
 1
0
dx
%
x(1 −x)
= 1
π

−1
2
1
2
dy
&	
y + 1
2

 	 1
2 −y

 = 1
π

1
2
−1
2
dy
&
1
4 −y2
=
1
π

1
2
−1
2
2 dy
%
1 −(2y)2 = 1
π
 1
−1
dt
√
1 −t2 = 1
π [Arcsin t]1
−1 = 1,
thus f(x) is the frequency of a random variable.
2) Since f(x) = 0 outside a bounded interval, all moments exist. In particular,
E{X}
=
 1
0
x f(x) dx = 1
π
 1
0
x −1
2 + 1
2
√
x −x2 dx
=
−1
2π
 1
x=0
d
	
x2 −x

√
x −x2 + 1
2 ·
 1
0
f(x) dx = 0 + 1
2 = 1
2,
which can also be seen graphically, because the graph of f(x) is clearly symmetric with respect to
the line x = 1
2.
Furthermore,
E{X(X −1)} = 1
π
 1
0
x(x −1)
%
x(1 −x)
dx = −1
π
 1
0
%
x(1 −x) dx.
Since the graph of the integrand
%
x(1 −x) is a half circle of centre
	 1
2, 0

and radius r = 1
2, we
have
E{X(X −1)} = −1
π area

halvcirkel, radius 1
2

= −1
π · 1
2 · π
1
2
2
= −1
8,
hence
E

X2
= E{X(X −1)} + E{X} = −1
8 + 1
2 = 3
8,
and
V {X} = E

X2
−(E{X})2 = 3
8 −
1
2
2
= 3
8 −1
4 = 1
8.
Download free eBooks at bookboon.com

Random variables I
 
152 
10. Mean and variance in special cases
3) Since y = ψ(x) = √x is a bijective map ψ : ]0, 1[ →]0, 1[, with the inverse x = ϕ(x) = y2, where
dy
dx = 2y > 0, we conclude that the frequency of Y =
√
X is
g(y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
f(ϕ(y)) · ϕ′(y) =
2y
π
%
y2 (1 −y2)
= 2
π ·
1
%
1 −y2 ,
for y ∈]0, 1[,
0,
otherwise,
corresponding to the distribution function
G(y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1,
for y ≥1,
2
π Arcsin y,
for y ∈]0, 1[,
0,
for y ≤0.
4) The mean of Y is
E{Y } = 2
π
 1
0
y
%
1 −y2 dy = 2
π
'
−
%
1 −y2
(1
0 = 2
π .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
153 
10. Mean and variance in special cases
Example 10.7 1) Prove that the function
f(x) =
⎧
⎪
⎨
⎪
⎩
ab
a −b
	
e−bx −e−ax
,
for x ≥0,
0,
for x < 0,
where a and b denote positive constants, a ̸= b, can be considered as the frequency of a random
variable X.
2) Find the distribution function of the random variable X.
3) Find E{X} and V {X}, expressed by a and b .
4) Let a be a ﬁxed number. Prove for every ﬁxed x that
lim
b→a f(x) = g(x),
where
g(x) =
⎧
⎨
⎩
a2x e−ax,
for x ≥0,
0,
for x < 0.
5) Prove that the function g(x) can be considered as a frequency of a random variable Y .
6) Finally, prove that E{X} →E{Y } for b →a.
1) We may assume that a > b. Then f(x) ≥0 for x ∈R, and
 ∞
−∞
f(x) dx =
ab
a −b
 ∞
0

e−bx −e−ax
dx =
ab
a −b
1
b −1
a

=
ab
a −b · a −b
ab
= 1,
thus f(x) can be considered as a frequency.
2) The distribution function F(x) of X is 0 for x ≤0. If x > 0, then
F(x)
=
ab
a −b
 x
0

e−bt −e−at
dt =
ab
a −b

−1
b e−bt + 1
a e−at
x
0
=
1
a −b
#
−a e−bt + b e−at$x
0
=
1
a −b

−a e−bx + b e−ax + a −b

= 1 +
1
a −b
	
b e−ax −a e−bx
.
3) We get by a partial integration, or by means of the Γ integral,
E{X}
=
 ∞
0
x f(x) dx =
ab
a −b
 ∞
0
x

e−bx −e−ax
dx
=
ab
a −b
 1
b2
 ∞
0
t e−t dt −1
a2
 ∞
0
t e−t dt

=
ab
a −b
 1
b2 −1
a2

=
ab
a −b · a2 −b2
a2b2
= a + b
ab
= 1
a + 1
b ,
Download free eBooks at bookboon.com

Random variables I
 
154 
10. Mean and variance in special cases
and
E

X2
=
 ∞
0
x2f(x) dx =
ab
a −b
 ∞
0
x2 
e−bx −e−ax
dx
=
ab
a −b
 1
b3
 ∞
0
t2e−t dt −1
a3
 ∞
0
t2e−t dt

=
ab
a −b
 2
b3 −2
a3

= 2ab
a −b · a3 −b3
(ab)3
= 2 · a2 + ab + b2
(ab)2
=
2
 1
a2 + 1
ab + 1
b2

,
hence
V {X} = E

X2
−(E{X})2 = 2
 1
a2 + 1
ab + 1
b2

−
 1
a2 + 2
ab + 1
b2

= 1
a2 + 1
b2 .
4) Let x > 0 and a > 0 be ﬁxed. Then by e.g. l’Hospital’s rule,
lim
b→a
ab
a −b
	
e−bx −e−ax
= a2 lim
b→a
e−bx −e−ax
a −b
= a2 lim
b→a
−x e−bx
−1
= a2x e−ax.
For x ≤0 we get of course 0, thus
lim
b→a f(x) = g(x).
5) Obviously, g(x) ≥0. Since
 ∞
−∞
g(x) dx = a2
 ∞
0
x e−ax dx =
 ∞
0
t e−t dt = 1,
it follows that g(x) is the frequency of a random variable Y .
6) the mean of Y is
E{Y } =
 ∞
−∞
x g(x) dx = a2
 ∞
0
x · x e−ax dx = 1
a
 ∞
0
t2e−t dt = 2
a.
It follows from (3) that
lim
b→a E{X} = lim
b→a
1
a + 1
b

= 2
a = E{Y }.
Remark 10.1 It is possible to give a simper solution. In fact, if X1 and X2 are independent random
variables of the frequencies
fX1(x) =
 a e−ax,
x ≥0,
0,
x < 0,
and
fX2(x) =

b e−bx,
x ≥0,
0,
x < 0,
Download free eBooks at bookboon.com

Random variables I
 
155 
10. Mean and variance in special cases
then X = X1 + X2 has the frequency f(x), and
E{X} = E {X1} + E {X2} = 1
a + 1
b ,
V {X} = V {X1} + V {X2} = 1
a2 + 1
b2 .
♦
Example 10.8 Let X and Y be independent random variables with the distributions given by
P{X = k} = P{Y = k} = pqk,
k ∈N0,
where p > 0, q > 0 and p + q = 1.
1) Find the means E{X} and E{Y }.
2) Find the variances V {X} and V {Y }.
3) Find P{X + Y = k}, k ∈N0.
1) The means are
E{X} = E{Y } =
∞

k=0
k · P{X = k} =
∞

k=1
kpqk = pq

k=1
k · qk−1 = pq ·
1
(1 −q)2 = pq
p2 = q
p.
Here we have used that by a partial diﬀerentiation with respect to q ∈]0, 1[ we obtain the important
expressions
1
1 −q =
∞

k=0
qk
and
d
dq

1
1 −q

=
1
(1 −q)2 =
∞

k=1
k · qk−1.
2) The variance is found by a smart rearrangement,
V {X} = E

X2
−(E{X})2 = E{X(X −1)} + E{X} −(E{X})2,
where
E{X(X −1)}
=
∞

k=2
k(k −1) P{X = k} =
∞

k=2
k(k −1)pqk = pq2
∞

k=2
k(k −1)qk−2
=
pq2 d2
dq2
 ∞

k=0
qk

= pq2 d2
dq2

1
1 −q

= pq2 ·
2
(1 −q)3 = 2pq2
p3
= 2 · q2
p2 ,
i.e.
V {X} = V {Y } = 2 · q2
p2 + q
p −q2
p2 = q2
p2 + q · p
p2
= q(p + q)
p2
= q
p2 .
Notice that it is easier to compute E{X(X −1)} than
E

X2
=
∞

k=1
k2P{X = k} =
∞

k=1
k2pqk.
Download free eBooks at bookboon.com

Random variables I
 
156 
10. Mean and variance in special cases
3) Since X and Y are independent, we get for k ∈N0,
P{X + Y = k}
=
k

i=0
P{X = i ∧Y = k −i} =
k

i=0
P{X = i} · P{Y = k −i}
=
k

i=0
pq · pqk−i = p2qk
k

i=0
1 = (k + 1)p2qk,
k ∈N0.
Remark 10.2 The distribution of X is a reduced waiting time distribution. ♦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
157 
10. Mean and variance in special cases
Example 10.9 There are given two components in an instrument. The life times of the components,
T1 and T2, are independent random variables, both of the frequency
f(t) =

a e−at,
t > 0,
0,
t ≤0,
where a is a positive constant.
We introduce the random variables X1, X2 and Y2 by
X1 = min {T1, T2} ,
X2 = max {T1, T2} ,
Y2 = X2 −X1.
Here, X1 denotes the time until the ﬁrst one of the components fails to function, and X2 indicates the
time until the second component also fails, while Y2 is the time from the failure of the ﬁrst component
until the failure of the second component.
1. Find the frequency and the mean of X1.
2. Find the mean of Y2.
Let it be given without proof that (X1, X2) has the simultaneous frequency
h (x1, x2) =

2a2e−a(x1+x2),
0 < x1 < x2,
0,
otherwise.
4. Find the simultaneous frequency of the 2-dimensional random variable (X1, Y2).
5. Find the frequency of Y2.
6. Check if the random variables X1 and Y2 are independent.
1) We get for X1,
P {X1 > x1} = P {T1 > x1 ∧T2 > x1} = P {T1 > x1} · P {T2 > x2} = e−2ax1,
thus
P {X1 ≤x1} = 1 −e−2ax1,
x1 > 0,
and we see that X1 is exponentially distributed with the frequency
fX1 =

2a e−2ax1,
x1 > 0,
0,
x1 ≤0,
and mean
1
2a.
2) For X2 we get
P {X2 ≤x2}
=
P {T1 ≤x2 ∧T2 ≤x2} = P {T1 ≤x2} · P {T2 ≤x2}
=
	
1 −e−ax2
2 ,
x2 > 0,
so X2 has the frequency
fX2 (x2) = 2a e−ax2 	
1 −e−ax2
= 2a e−ax2 −2a e−2ax2
for x2 > 0,
Download free eBooks at bookboon.com

Random variables I
 
158 
10. Mean and variance in special cases
and
fX2 (x2) = 0
for x2 ≤0.
The mean is
E {X2}
=
 ∞
0
x2fX2 (x2) dx2 =
 ∞
0

2a x2e−ax2 −2a x2e−2ax2
dx2
=
2
a −1
2a = 3
2a.
Additional. It is easy to ﬁnd the mean of X2 from X1 + X2 = T1 + T2, i.e.
E {X2} = E {T1} + E {T2} −E {X1} = 1
a + 1
a −1
2a = 3
2a.
3) This is trivial, because
E {Y2} = E {X2} −E {X1} = 3
2a −1
2a = 1
a.
4) The simultaneous frequency k (y1, y2) of
(Y1, Y2) = (X1, X2 −X1)
can e.g. be found directly from a convenient formula with a = 1, b = 0, c = −1 and d = −1,
k (y1, y2)
=
h
dy1 −by2
ad −bc , −cy1 + ay2
ad −bc

·
1
|ad −bc|
=
h (y1, y1 + y2) = 2a2e−a(2y1+y2)
for y1 > 0 og y2 > 0,
and
k (y1, y2) = 0
otherwise.
This is also written
k (y1, y2) =
 2a e−2ay1 · a e−ay2,
for y1 > 0 and y2 > 0,
0,
otherwise.
5) (and 6.) It follows immediately from 4. that Y1 (= X1) and Y2 are independent, and that Y2 has
the frequency
kY2 (y2) =

a e−ay2,
y2 > 0,
0,
y2 ≤0.
Download free eBooks at bookboon.com

Random variables I
 
159 
Index
Index
2-dimensional random variable, 5
almost everywhere, 7
Arcussinus distribution, 149
binomial distribution, 18
Buﬀon’s needle problem, 77
Cauchy distribution, 97, 111, 144
causal distribution, 4
Chebyshev’s inequality, 13, 138, 139
conditional distribution, 11
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence in distribution, 16
convergence in probability, 16
convex function, 142
correlation, 15
covariance, 15
Cramer’s formula, 108
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4, 20, 33
expectation, 11
fraud in Probability, 77
frequency, 5, 6, 20, 33
function of random variable, 75
function of random variables, 93
function of two random variables, 107
gamma distribution, 61
geometric distribution, 90
Helly-Bray’s lemma, 16
independent random variables, 7
inequality between random variables, 81
Jacobian, 10, 54
Jensen’s inequality, 142
law of total probability, 11
MAPLE, 22
marginal distribution, 5
marginal frequency, 6, 37, 54, 121
mean, 11, 129
median, 4, 21
moment, 12, 140, 143
moment of higher order, 129
null-set, 7
polar coordinates, 118
probability ﬁeld, 4
quantile, 4
random variable, 4
rectangular coordinates, 118
rectangular distribution, 93, 95, 98, 112, 143,
144
simultaneous distribution, 5
simultaneous distribution function, 6
simultaneous frequency, 54
transformation theorem, 8
weak law of large numbers, 16
Download free eBooks at bookboon.com


Leif Mejlbro
Random variables I
Probability Examples c-2
Download free books at

2 
Leif Mejlbro
Probability Examples c-2
Random variables I
Download free eBooks at bookboon.com

3 
Probability Examples c-2 â€“ Random variables I
Â© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-516-5
Download free eBooks at bookboon.com

Random variables I
 
4 
Contents
 
Introduction  
3
1  
Some theoretical results  
4
2  
Simple introducing examples  
18
3  
Frequencies and distribution functions in 1 dimension  
20
4  
Frequencies and distributions functions, 2 dimensions  
33
5  
Functions of random variables, in general  
75
6  
Inequalities between two random variables  
81
7  
Functions Y = f(X) o f random variables  
93
8  
Functions of two random variables, f(X; Y )  
107
9  
Means and moments of higher order  
129
10  
Mean and variance in special cases  
143
 
Index  
157
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
5 
Introduction
Introduction
This is the second book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole JÃ¸rsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the ï¬rst one. We shall here deal with the basic stuï¬€, i.e. frequencies and distribution
functions in 1 and 2 dimensions, functions of random variables and inequalities between random
variables, as well as means and variances.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ï¬rst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
25th October 2009
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
6 
1. Some theoratical results
1
Some theoretical results
The abstract (and precise) deï¬nition of a random variable X is that X is a real function on Î©, where
the triple (Î©, F, P) is a probability ï¬eld, such that
{Ï‰ âˆˆÎ© | X(Ï‰) â‰¤x} âˆˆF
for every x âˆˆR.
This deï¬nition leads to the concept of a distribution function for the random variable X, which is the
function F : R â†’R, which is deï¬ned by
F(x) = P{X â‰¤x}
(= P{Ï‰ âˆˆÎ© | X(Ï‰) â‰¤x}),
where the latter expression is the mathematically precise deï¬nition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 â‰¤F(x) â‰¤1
for every x âˆˆR.
The function F is weakly increasing, i.e. F(x) â‰¤F(y) for x â‰¤y.
limxâ†’âˆ’âˆF(x) = 0
and
limxâ†’+âˆF(x) = 1.
The function F is continuous from the right, i.e. limhâ†’0+ F(x + h) = F(x)
for every x âˆˆR.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deï¬ne a median of a random variable X with the distribution function F(x) as a real
number a = (X) âˆˆR, for which
P{X â‰¤a} â‰¥1
2
and
P{X â‰¥a} â‰¥1
2.
Expressed by means of the distribution function it follows that a âˆˆR is a median, if
F(a) â‰¥1
2
and
F(aâˆ’) = lim
hâ†’0âˆ’F(x + h) â‰¤1
2.
In general we deï¬ne a p-quantile, p âˆˆ]0, 1[, of the random variable as a number ap âˆˆR, for which
P {X â‰¤ap} â‰¥p
and
P {X â‰¥ap} â‰¥1 âˆ’p,
which can also be expressed by
F (ap) â‰¥p
and
F (apâˆ’) â‰¤p.
If the random variable X only has a ï¬nite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables I
 
7 
1. Some theoratical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
âˆ’âˆ
f(u) du,
x âˆˆR,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R â†’R is called a frequency of the random variable X.
Let again (Î©, F, P) be a given probability ï¬eld. Let us consider two random variables X and Y , which
are both deï¬ned on Î©. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) âˆˆA}
for every Borel set A âŠ†R2.
When the simultaneous distribution of (X, Y ) is known, we deï¬ne the marginal distributions of X
and Y by
PX(B) = P{X âˆˆB} := P{(X, Y ) âˆˆB Ã— R},
where B âŠ†R is a Borel set,
PY (B) = P{Y âˆˆB} := P{(X, Y ) âˆˆR Ã— B},
where B âŠ†R is a Borel set.
Notice that we can always ï¬nd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ï¬nd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables I
 
8 
1. Some theoratical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deï¬ned as the
function F : R2 â†’R, given by
F(x, y) := P{X â‰¤x âˆ§Y â‰¤y}.
We have
â€¢ If (x, y) âˆˆR2, then 0 â‰¤F(x, y) â‰¤1.
â€¢ If x âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisï¬es the condition limyâ†’âˆ’âˆF(x, y) = 0.
â€¢ If y âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisï¬es the condition limxâ†’âˆ’âˆF(x, y) = 0.
â€¢ When both x and y tend towards inï¬nity, then
lim
x, yâ†’+âˆF(x, y) = 1.
â€¢ If x1, x2, y1, y2 âˆˆR satisfy x1 â‰¤x2 and y1 â‰¤y2, then
F (x2, y2) âˆ’F (x1, y2) âˆ’F (x2, y1) + F (x1, y2) â‰¥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ï¬nd the distribution functions
of X and Y by the formulÃ¦
FX(x) = F(x, +âˆ) =
lim
yâ†’+âˆF(x, y),
for x âˆˆR,
Fy(x) = F(+âˆ, y) =
lim
xâ†’+âˆF(x, y),
for y âˆˆR.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 â†’R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
âˆ’âˆ
 y
âˆ’âˆ
f(t, u) du

dt,
for (x, y) âˆˆR2.
In this case we can ï¬nd the function f(x, y) at the diï¬€erentiability points of F(x, y) by the formula
f(x, y) = âˆ‚2F(x, y)
âˆ‚xâˆ‚y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +âˆ
âˆ’âˆ
f(x, y) dy,
for x âˆˆR,
Download free eBooks at bookboon.com

Random variables I
 
9 
1. Some theoratical results
and
fY (y) =
 +âˆ
âˆ’âˆ
f(x, y) dx,
for y âˆˆR.
It was mentioned above that one far from always can ï¬nd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deï¬ned on the same probability ï¬eld (Î©, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B âŠ†R,
P{X âˆˆA âˆ§Y âˆˆB} = P{X âˆˆA} Â· P{Y âˆˆB},
which can also be put in the simpler form
F(x, y) = FX(x) Â· FY (y)
for every (x, y) âˆˆR2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi Â· gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) Â· fY (y)
almost everywhere.
The concept â€œalmost everywhereâ€ is rarely given a precise deï¬nition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ï¬nite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let Ï• : R â†’R and Ïˆ : R â†’R be
given functions. Then Ï•(X) and Ïˆ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables I
 
10 
1. Some theoratical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X âˆˆI} = 1.
2) Let Ï„ : I â†’J be a bijective map of I onto an open interval J.
3) Furthermore, assume that Ï„ is diï¬€erentiable with a continuous derivative Ï„ â€², which satisï¬es
Ï„ â€²(x) Ì¸= 0
for alle x âˆˆI.
Under the assumptions above Y := Ï„(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =
â§
âª
â¨
âª
â©
f
	
Ï„ âˆ’1(y)

Â·

	
Ï„ âˆ’1
â€² (y)
 ,
for y âˆˆJ,
0,
otherwise.
We note that if just one of the assumptions above is not fulï¬lled, then we shall instead ï¬nd the
distribution function G(y) of Y := Ï„(X) by the general formula
G(y) = P{Ï„(X) âˆˆ] âˆ’âˆ, y]} = P

X âˆˆÏ„ â—¦âˆ’1(] âˆ’âˆ, y])

,
where Ï„ â—¦âˆ’1 = Ï„ âˆ’1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisï¬ed, then Ï„ is necessarily monotone.
At a ï¬rst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deï¬ne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆh(x, z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆh(x, x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆh

x , z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +âˆ
âˆ’âˆh(zx , x) Â· |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables I
 
11 
1. Some theoratical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆf(x)g(z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆf(x)g(x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆf(x) g
 z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +âˆ
âˆ’âˆf(zx)g(x) Â· |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) Â· FY (u)
for u âˆˆR,
and
FV (v) = 1 âˆ’(1 âˆ’FX(v)) Â· (1 âˆ’FY (v))
for v âˆˆR.
These formulÃ¦ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables I
 
12 
1. Some theoratical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) Â· fY (u) + fX(u) Â· FY (u),
for u âˆˆR,
and
fV (v) = (1 âˆ’FX(v)) Â· fY (v) + fX(v) Â· (1 âˆ’Fy(v)) ,
for v âˆˆR,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps Ï• = (Ï•1 , Ï•2) : R2 â†’R2, or subsets of R2.
We shall need the Jacobian of Ï•, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deï¬ne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ËœD is an open domain in the
(y1 , y2) plane. Then let Ï• = (Ï•1 , Ï•2) be a bijective map of ËœD onto D with the inverse Ï„ = Ï•âˆ’1, i.e.
the opposite of what one probably would expect:
Ï• = (Ï•1 , Ï•2) : ËœD â†’D,
with (x1 , x2) = Ï• (y1 , y2) .
The corresponding Jacobian is deï¬ned by
JÏ• = âˆ‚(x1 , x2)
âˆ‚(y1 , y2) =

âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y1
âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y2

,
where the independent variables (y1 , y2) are in the â€œdenominatorsâ€. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D â†’R is an integrable
function, where D âŠ†R2 is given as above, then for every (measurable) subset A âŠ†D,

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
h (x1 , x2) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we â€œdelete the y-sâ€. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
(Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·
JÏ• (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables I
 
13 
1. Some theoratical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D âŠ†R2 be an open domain, such that
P {(X1 , X2) âˆˆD} = 1.
Let Ï„ : D â†’ËœD be a bijective map of D onto another open domain ËœD, and let Ï• = (Ï•1 , Ï•2) =
Ï„ âˆ’1, where we assume that Ï•1 and Ï•2 have continuous partial derivatives and that the corresponding
Jacobian is diï¬€erent from 0 in all of ËœD.
Then the 2-dimensional random variable
(Y1 , Y2) = Ï„ (X1 , X2) = (Ï„1 (X1 , X2) , Ï„2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =
â§
âª
âª
â¨
âª
âª
â©
h (Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 ,
for (y1 , y2) âˆˆËœD,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deï¬ne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi âˆ§Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ï¬xed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} Â· P {Y = yj} .
Analogously we deï¬ne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X â‰¤x | Y = y} =
 x
âˆ’âˆf(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deï¬ned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that â€œ0 times undeï¬ned = 0â€. Then we get the Law of total probability,
 +âˆ
âˆ’âˆ
f(x | y) Â· fY (y) dy =
 +âˆ
âˆ’âˆ
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables I
 
14 
1. Some theoratical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deï¬ned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deï¬ne the mean, or expectation
of X by
E{X} =
 +âˆ
âˆ’âˆ
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +âˆ[,
and the mean exists, then the mean is given by
E{X} =
 +âˆ
0
P{X â‰¥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = Ï•(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = Ï•(X) is given by
E{Ï•(X)} =

i
Ï• (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = Ï•(X) is
given by
E{Ï•(X)} =
 +âˆ
âˆ’âˆ
Ï•(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean Î¼. We add the following concepts, where k âˆˆN:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X âˆ’Î¼)k
.
The k-th absolute central moment,
E

|X âˆ’Î¼|k
.
The variance, i.e. the second central moment,
V {X} = E

(X âˆ’Î¼)2
,
Download free eBooks at bookboon.com

Random variables I
 
15 
1. Some theoratical results
provided that the deï¬ning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = Î¼ and variance V {X}. Then
E

(X âˆ’c)2
= V {X} + (Î¼ âˆ’c)2
for every c âˆˆR,
V {X} = E

X2
âˆ’(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b âˆˆR,
V {aX + b} = a2V {X}
for every a, b âˆˆR.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diï¬€ers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (Ë‡CebyË‡sevâ€™s inequality). If the random variable X has the mean Î¼ and the variance
Ïƒ2, then we have for every a > 0,
P{|X âˆ’Î¼| â‰¥a} â‰¤Ïƒ2
a2 .
If we here put a = kÏƒ, we get the equivalent statement
P{Î¼ âˆ’kÏƒ < X < Î¼ + kÏƒ} â‰¥1 âˆ’1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables I
 
16 
1. Some theoratical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = Ï•(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

i, j
Ï• (xi , yj) Â· P {X = xi âˆ§Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

R2 Ï•(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and Ï•(x, y) = Ï•1(x) + Ï•2(y),
then
E {Ï•1(X) + Ï•2(Y )} = E {Ï•1(X)} + E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose Ï•(x, y) = Ï•1(x)Â·Ï•2(y), then also
E {Ï•1(X) Â· Ï•2(Y )} = E {Ï•1(X)} Â· E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular we get under the assumptions above
that
E{X Â· Y } = E{X} Â· E{Y },
and
E{(X âˆ’E{X}) Â· (Y âˆ’E{Y })} = 0.
These formulÃ¦ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ï¬nd a measure of how much they
â€œdependâ€ on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = Î¼X,
E{Y } = Î¼Y ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
Download free eBooks at bookboon.com

Random variables I
 
17 
1. Some theoratical results
all exist. We deï¬ne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X âˆ’Î¼X) Â· (Y âˆ’Î¼Y )} .
We deï¬ne the correlation between X and Y , denoted by Ï±(X, Y ), as
Ï±(X, Y ) := Cov(X, Y )
ÏƒX Â· ÏƒY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = Î¼X,
E{Y } = Î¼Y ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X Â· Y } âˆ’E{X} Â· E{Y },
|Cov(X, Y )| â‰¤ÏƒX Â· Ïƒy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
Ï±(X, Y ) = 0,
if X and Y are independent,
Ï±(X, X) = 1,
Ï±(X, âˆ’X) = âˆ’1,
|Ï±(X, Y )| â‰¤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b âˆˆR,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
Ï±(U, V ) = Ï±(aX + b, cY + d) = Ï±(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
jâˆ’1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deï¬ned on the same probability
ï¬eld (Î©, F, P).
Download free eBooks at bookboon.com

Random variables I
 
18 
1. Some theoratical results
1) We say that Xn converges in probability towards a random variable X on the probability ï¬eld
(Î©, F, P), if
P {|Xn âˆ’X| â‰¥Îµ} â†’0
for n â†’+âˆ,
for every ï¬xed Îµ > 0.
2) We say that Xn converges in probability towards a constant c, if every ï¬xed Îµ > 0,
P {|Xn âˆ’c| â‰¥Îµ} â†’0
for n â†’+âˆ.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
nâ†’+âˆFn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ï¬rst one resembles Ë‡CebyË‡sevâ€™s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deï¬ned on (Î©, F, P), and assume that they all have the same mean and variance,
E {Xi} = Î¼
and
V {Xi} = Ïƒ2.
Then for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Î¼
 â‰¥Îµ

â†’0
for n â†’+âˆ.
A slightly diï¬€erent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deï¬ned
on (Î©, F, P) where E {Xi} = Î¼, (notice that we do not assume the existence of the variance), then
for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Î¼
 â‰¥Îµ

â†’0
for n â†’+âˆ.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Brayâ€™s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a â‰¤Xn â‰¤b} = 1
for every n âˆˆN.
If Ï• is a continuous function on the interval [a, b], then
lim
nâ†’+âˆE {Ï• (Xn)} = E{Ï•(X)}.
In particular,
lim
nâ†’+âˆE {Xn}
and
lim
nâ†’+âˆV {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables I
 
19 
1. Some theoratical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables I
 
20 
2. Simple introduction examples
2
Simple introducing examples
Example 2.1 A motorist shall pass 4 traï¬ƒc lights. We assume that at each of the traï¬ƒc lights there
is the probability p that he must stop. There is furthermore such a long distance between the traï¬ƒc
signals that there is no synchronization between them. Let X be the random variable, which indicates
the number of stops. Find the distribution of X. Sketch in the case p = 1
2 the corresponding diagram.
Let Y have the value k, if the ï¬rst stop is at signal number k, k = 1, 2, 3, 4. Is Y a random variable?
In this case the model is given by the binomial distribution X âˆˆB(4, p), thus
P{X = k} =
 4
k

pk(1 âˆ’p)4âˆ’k,
k = 0, 1, 2, 3, 4.
We deï¬ne here a â€œsuccessâ€ as a stop (what we otherwise would not consider as a success).
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
1
2
3
4
Figure 1: The diagram, for p = 1
2.
We get in particular For p = 1
2,
P{X = k} =

4
k
 1
2
4
,
k = 0, 1, 2, 3, 4,
thus
p0 = p4 = 1
16,
p1 = p3 = 4
16 = 1
4,
p2 = 6
16 = 3
8.
If the ï¬rst stop is at signal number k, the car has not stopped earlier, so
P{Y = k} = (1 âˆ’p)kâˆ’1p,
k = 1, 2, 3, 4.
Download free eBooks at bookboon.com

Random variables I
 
21 
2. Simple introduction examples
Then
4

k=1
P{Y = k} =
4

k=1
p(1 âˆ’p)kâˆ’1 = 1 âˆ’(1 âˆ’p)4 < 1,
nËšar p < 1.
Since the sum is not equal to 1, we conclude that Y is not a random variable.
The reason why Y is not a random variable, is that we have in the setup forgotten the possibility of
â€œno stops at allâ€ of the probability (1 âˆ’p)4. If we add this value to Y for this event (where one e.g.
arbitrarily could let it correspond to the event Y = Ï€2), then Y becomes a random variable. A more
reasonable deï¬nition would of course be Y = 5. However, there is nothing wrong in choosing Y = Ï€2.
Example 2.2 A random variable X can have the possible values 1, 2, . . . of the probabilities
P{X = k} = A qk
k ,
k âˆˆN
(where q âˆˆ]0 , 1[).
Find the constant A.
We put p = 1 âˆ’q. Since
âˆ

k=1
P{X = k} = 1,
it follows from the series of logarithm that
1 =
âˆ

k=1
P{X = k} = A
âˆ

k=1
qk
k = âˆ’A

âˆ’
âˆ

k=1
qk
k

= âˆ’A ln(1 âˆ’q) = A ln 1
p.
From 1
p > 1 follows that ln 1
p > 0, hence
A =
1
ln 1
p
=
1
| ln p|,
and thus
P{X = k} =
qk
k | ln(1 âˆ’q)|,
k âˆˆN.
Download free eBooks at bookboon.com

Random variables I
 
22 
3. Frequencies and distribution functions in 1 dimension
3
Frequencies and distribution functions in 1 dimension
Example 3.1 Check if the function
f(x) =
â§
â¨
â©
1
2 âˆ’kx,
x âˆˆ[0, 6],
0
otherwise,
is a frequency for some k.
If f(x) is a frequency, then the following two conditions must be fulï¬lled:
1) f(x) â‰¥0 for every x âˆˆR,
2)
 âˆ
âˆ’âˆf(x) dx = 1.
By putting x = 6 into (1) we get 1
2 âˆ’6k â‰¥0, thus k â‰¤1
12.
A computation of (2) gives
1 =
 6
0
1
2 âˆ’kx

dx =
1
2 x âˆ’1
2 kx2
6
0
= 3 âˆ’18k,
hence
k = 3 âˆ’1
18
= 1
9 > 1
12.
The two requirements can never be satisï¬ed simultaneously, so f(x) is not a frequency for any k âˆˆR.
Example 3.2 Find k, such that
f(x) =
â§
â¨
â©
kx2 	
1 âˆ’x3
,
x âˆˆ[0, 1],
0,
otherwise,
is a frequency of a random variable, and sketch the function.
Find the median of X.
Obviously, f(x) â‰¥0. Then by an integration,
 1
0
x2 	
1 âˆ’x3
dx =
 1
0
	
x2 âˆ’x5
dx = 1
3 âˆ’1
6 = 1
6.
If we choose k = 6, then f(x) becomes a frequency, thus
f(x) =
â§
â¨
â©
6x2 	
1 âˆ’x3
= 6x2 âˆ’6x5
for x âˆˆ[0, 1],
0
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
23 
3. Frequencies and distribution functions in 1 dimension
Since f(0) = f(1) = 0, and f â€²(x) = 12x âˆ’30x4 = 0 for x =
3

2
5 â‰ˆ0, 74, we have a (global) maximum
18
5 Â·
2
5
 2
3
â‰ˆ1.95.
The function f is increasing in

0,
3

2
5

and decreasing in

3

2
5

.
The distribution function F(x) is in the interval [0, 1] given by
F(x) =
 x
0
f(t) dt =
 x
0

6t2 âˆ’6t5
dt = 2x3 âˆ’x6.
The median is given as the x âˆˆ]0, 1[, for which F(x) = 1
2, thus
2x3 âˆ’x6 = 1
2,
or
x6 âˆ’2x3 + 1 = 1
2,
i.e.
	
x3 âˆ’1

2 = 1
2.
We get x3 = 1 Â±
âˆš
2
2 . However, since x âˆˆ]0, 1[, we can only use the sign âˆ’, and the median is (here
uniquely determined by)
xmed =
3

1 âˆ’
âˆš
2
2
â‰ˆ0.66.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
24 
3. Frequencies and distribution functions in 1 dimension
It is possible to apply MAPLE, e-g. by:
> f:=x->6*x^2*(1-x^3);
f := x âˆ’â†’6x2 	
1 âˆ’x3
> plot(f,0..1,color=black);
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
> F:=int(f(x),x=0..t);
F := âˆ’t6 + 2t3
> fsolve(2*t^3-t^5 = 1/2,t=0..1);
.6641045243
> solve(2*t^3-t^6 = 1/2,t);
1
2(8 + 4
âˆš
2)1/3, âˆ’1
4(8 + 4
âˆš
2)1/3,
âˆ’1
4(8 + 4
âˆš
2)1/3 âˆ’1
4I
âˆš
3(8 + 4
âˆš
2)1/3, 1
2(8 âˆ’4
âˆš
2)1/3,
âˆ’1
4(8 âˆ’4
âˆš
2)1/3 + 1
4I
âˆš
3(8 âˆ’4
âˆš
2)1/3, âˆ’1
4(8 âˆ’4
âˆš
2)1/3 âˆ’1
4I
âˆš
3(8 âˆ’4
âˆš
2)1/3
> 1/2*(8-4*sqrt(2))^(1/3); # this is the median
1
2(8 âˆ’4
âˆš
2)1/3
> F1:=x->2*x^3-x^6;
F1 := x âˆ’â†’2x3 âˆ’x6
Download free eBooks at bookboon.com

Random variables I
 
25 
3. Frequencies and distribution functions in 1 dimension
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
> plot(F1,0..1,color=black);
The former ï¬gure shows the graph of the frequency, and the latter ï¬gure shows the graph of the
distribution function. Notice the diï¬€erence between using fsolve or solve.
With the exception of the sketches of the graphs we see that it is easy to perform the same computations
without using MAPLE. Furthermore, the MAPLE program is also less transparent then an explanation
in plain words.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables I
 
26 
3. Frequencies and distribution functions in 1 dimension
Example 3.3 A random variable X has the frequency
f(x) =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
C(x âˆ’a),
a â‰¤x â‰¤a + b
2
,
C(b âˆ’x),
a + b
2
â‰¤x â‰¤b,
0,
otherwise.
Find the constant C and the distribution function.
Compute
P

X â‰¤a + b
2

and
P
2a + b
3
â‰¤X â‰¤a + 2b
3

.
This distribution is called the triangular distribution over ]a, b[.
b
a
Figure 2: The graph of the frequency f.
1) By considering the graph we immediately get
1 =
 âˆ
âˆ’âˆ
f(x) dx = 1
2 Â· C b âˆ’a
2
Â· (b âˆ’a) = C
b âˆ’a
2
2
,
because the integral can be interpreted as the area of a triangle. Then
C =

2
b âˆ’a
2
.
2) When a â‰¤x â‰¤a + b
2
the distribution function is given by
F(x) =
 x
a
C(t âˆ’a) dt = C
(t âˆ’a)1
2
x
a
= 1
2

2
b âˆ’a
2
(x âˆ’a)2 = 2
x âˆ’a
b âˆ’a
2
.
Download free eBooks at bookboon.com

Random variables I
 
27 
3. Frequencies and distribution functions in 1 dimension
If instead a + b
2
â‰¤x â‰¤b, then
F(x)
=
1
2 +
 x
a+b
2
C(b âˆ’t) dt = 1
2 +

2
b âˆ’a
2 
âˆ’(b âˆ’t)2
2
x
a+b
2
=
1
2 + 1
2

2
b âˆ’a
2 b âˆ’a
2
2
âˆ’(b âˆ’x)2

= 1
2 + 1
2 âˆ’2
b âˆ’x
b âˆ’a
2
=
1 âˆ’2
b âˆ’x
b âˆ’a
2
.
Summing up we get
F(x) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
0,
for x â‰¤a,
2
x âˆ’a
b âˆ’a
2
,
for a < x â‰¤a + b
2
,
1 âˆ’2
b âˆ’x
b âˆ’a
2
,
for a + b
2
< x â‰¤b,
1,
for x > b.
0.2
0.4
0.6
0.8
1
â€“1
â€“0.5
0.5
1
x
Figure 3: The distribution function for a = âˆ’1 and b = 1.
3) By considering the graph (or by insertion of x a+b
2 ) we get
P

X â‰¤a + b
2

= F
a + b
2

= 1
2.
Another consideration of the graph gives
P
2a + b
3
â‰¤X â‰¤a + 2b
3

= 1 âˆ’2 F
2a + b
3

= 1 âˆ’2 Â· 2 Â·
â§
âª
â¨
âª
â©
b âˆ’a
3
b âˆ’a
â«
âª
â¬
âª
â­
2
= 1 âˆ’4
9 = 5
9.
Download free eBooks at bookboon.com

Random variables I
 
28 
3. Frequencies and distribution functions in 1 dimension
Example 3.4 Prove for some choice of the constant k that the function
f(x) = k Â· eâˆ’|xâˆ’2|,
x âˆˆR,
is the frequency of a random variable X.
Find the distribution function of X, and compute P{âˆ’1 â‰¤X â‰¤3} and P{X â‰¥0}.
Find the median of X.
Obviously, f(x) is continuous, and f(x) > 0, when k > 0. The remaining condition of a frequency is
that
 âˆ
âˆ’âˆf(x) dx = 1. Then by a computation,
 âˆ
âˆ’âˆ
f(x) dx = k
 2
âˆ’âˆ
exâˆ’2 dx +
 âˆ
2
eâˆ’(xâˆ’2) dx

= 2k,
which is equal to 1 for k = 1
2.
0.1
0.2
0.3
0.4
0.5
â€“1
0
1
2
3
4
5
x
Figure 4: The graph of the frequency f. (NB Diï¬€erent scales on the axes).
The random variable X has the frequency
f(x) = 1
2 eâˆ’|xâˆ’2|,
x âˆˆR.
Now
 x
âˆ’âˆ
f(t) dt =
 x
âˆ’âˆ
1
2 etâˆ’2 dt = 1
2 exâˆ’2
for x â‰¤2,
and
 x
âˆ’âˆ
f(t) dt
=
 2
âˆ’âˆ
1
2 etâˆ’2 dt +
 x
2
eâˆ’(tâˆ’2) dt = 1
2 +
1
2 âˆ’1
2 eâˆ’(xâˆ’2)

=
1 âˆ’1
2 eâˆ’(xâˆ’2)
for x > 2,
Download free eBooks at bookboon.com

Random variables I
 
29 
3. Frequencies and distribution functions in 1 dimension
so the distribution function becomes
F(x) =
â§
âª
âª
â¨
âª
âª
â©
1
2 exâˆ’2,
for x â‰¤2,
1 âˆ’1
2 eâˆ’(xâˆ’2),
for x > 2.
0.2
0.4
0.6
0.8
â€“1
0
1
2
3
4
5
x
Figure 5: The graph of the distribution function F.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables I
 
30 
3. Frequencies and distribution functions in 1 dimension
Finally,
P{âˆ’1 â‰¤X â‰¤3} = F(3) âˆ’F(âˆ’1) = 1 âˆ’1
2 eâˆ’1 âˆ’1
2 eâˆ’3 â‰ˆ0.79,
and
P{X â‰¥0} = 1 âˆ’F(0) = 1 âˆ’1
2 eâˆ’2 â‰ˆ0.93.
Since F(2) = 1
2, it is obvious that 2 is the median of X.
Example 3.5 Prove that the function
f(x) =
â§
âª
âª
â¨
âª
âª
â©
k x exp

âˆ’1
2 x2

,
x â‰¥0,
0,
x < 0,
for some choice of the constant k, can be considered as the frequency of a random variable X.
Find the distribution function F of X.
Sketch the graph of the function f and of the function F.
Find the median of X.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.5
1
1.5
2
2.5
3
x
Figure 6: The graph of the frequency f.
1) If k > 0, then f(x) â‰¥0. The requirement for f(x) being a frequency is then reduced to
1 =
 âˆ
âˆ’âˆ
f(x) dx = k
 âˆ
0
x Â· exp

âˆ’1
2 x2

dx = k
 âˆ
0
eâˆ’u du = k,
Download free eBooks at bookboon.com

Random variables I
 
31 
3. Frequencies and distribution functions in 1 dimension
where we have used the substitution u = 1
2 x2 with du = x dx. Consequently, k = 1, and
f(x) =
â§
âª
âª
â¨
âª
âª
â©
x exp

âˆ’1
2 x2

,
x â‰¥0,
0,
x < 0.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 7: The graph of the distribution functions F.
2) If x > 0, we use the substitution u = 1
2 t2 to obtain
F(x) =
 x
0
f(t) dt =
 x
0
t exp

âˆ’1
2 t2

dt =

1
2 x2
0
eâˆ’u du = 1 âˆ’exp

âˆ’1
2 x2

.
Hence, the distribution function of X is
F(x) =
â§
âª
âª
â¨
âª
âª
â©
1 âˆ’exp

âˆ’1
2 x2

,
for x > 0,
0,
for x â‰¤0.
3) Consider the previous ï¬gures. We see that f(x) has a maximum for x =
1
âˆše, and a turning point
for F(x) at the same point.
4) The median is found from the equation
F(x) = 1 âˆ’exp

âˆ’1
2 x2

= 1
2,
i.e.
exp
1
2 x2

= 2,
thus 1
2 x2 = ln 2, and hence
(X) =
âˆš
2 ln 2 â‰ˆ1.1774.
Download free eBooks at bookboon.com

Random variables I
 
32 
3. Frequencies and distribution functions in 1 dimension
Example 3.6 Prove that the function
f(x) =
â§
âª
âª
â¨
âª
âª
â©
b
Î¸
x
Î¸
bâˆ’1
exp

âˆ’
x
Î¸
 b
,
x > 0,
0,
x â‰¤0,
where b and Î¸ are positive constants, is the frequency of a random variable X, and ï¬nd the distribution
function of it.
Prove that P{X â‰¤Î¸} does not depend on b.
This distribution is called a Weibull distribution.
Clearly, f(x) â‰¥0.
We get for x > 0 by the substitution,
s = t
Î¸
followed by
u = sb =
 t
Î¸
b
,
that
F(x)
=
 x
0
b
Î¸
 t
Î¸
bâˆ’1
exp
!
âˆ’
 t
Î¸
b"
dt =

x
Î¸
0
b sbâˆ’1 exp
	
âˆ’sb
ds
=
 ( x
Î¸ )
b
0
exp(âˆ’u) du = 1 âˆ’exp

âˆ’
x
Î¸
 b
â†’1
for x â†’âˆ.
We conclude that
F(x) =
â§
âª
âª
â¨
âª
âª
â©
1 âˆ’exp

âˆ’
x
Î¸
 b
,
x > 0,
0,
x â‰¤0,
is the distribution function of a random variable X with f(x) as its frequency.
It follows by insertion that
P{X â‰¤Î¸} = F(Î¸) = 1 âˆ’exp
!
âˆ’
Î¸
Î¸
b"
= 1 âˆ’1
e
is independent of b.
Download free eBooks at bookboon.com

Random variables I
 
33 
3. Frequencies and distribution functions in 1 dimension
Example 3.7 A patient arrives to a doctorâ€™s waiting room. The probability is p, where p âˆˆ]0, 1[,
that he will be treated immediately; but if he does not, the probability that he must wait longer than
the time x is equal to eâˆ’ax, where a is some positive constant. Find the distribution function of the
random variable X, which indicates the waiting time.
1) If the patient is treated immediately, then the waiting time is X = 0, thus
P{X = 0} = p.
2) The probability that the patient must wait more than x, x â‰¥0, is
P{X > x} = P{pt. must wait} Â· P{waiting time > x | pt. must wait} = (1 âˆ’p)eâˆ’ax,
hence
P{X â‰¤x} = 1 âˆ’P{X > x} = 1 âˆ’(1 âˆ’p)eâˆ’ax.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
34 
3. Frequencies and distribution functions in 1 dimension
0
0.2
0.4
0.6
0.8
1
â€“1
â€“0.5
0.5
1
1.5
2
2.5
Figure 8: The graph of the distribution function F(x) when a = 1 and p = 1
2.
3) The distribution function F(x) = P{X â‰¤x} is here
F(x) =
â§
â¨
â©
0,
x < 0,
1 âˆ’(1 âˆ’p)eâˆ’x,
x â‰¥0.
The distribution of X is of mixed type, i.e. it is neither discrete nor continuous.
Download free eBooks at bookboon.com

Random variables I
 
35 
4. Frequencies and distributions functions, 2 dimensions
4
Frequencies and distributions functions, 2 dimensions
Example 4.1 Let X and Y be independent random variables with the frequencies
f(x) = x eâˆ’x,
x > 0,
g(y) = eâˆ’y,
y > 0,
(both frequencies are otherwise 0).
Find the frequency of X + Y .
Find the mean E{X}, E{Y } og E{X + Y }.
The frequency of X + Y is given by the convolution integral
k(z) =
 âˆ
âˆ’âˆ
f(x) g(z âˆ’x) dx.
This expression is only > 0, when z > 0. We have furthermore the constraints z âˆ’x > 0 and x > 0,
so the convolution integral is reduced to
k(z) =
 z
0
f(x) g(z âˆ’x) dx =
 z
0
x eâˆ’x eâˆ’(zâˆ’x) dx = eâˆ’z
 z
0
x dx = 1
2 z2 eâˆ’z,
and k(z) = 0 for z â‰¤0.
The means are
E{X}
=
 âˆ
0
x f(x) dx =
 âˆ
0
x2 eâˆ’x = 2,
E{Y }
=
 âˆ
0
y g(y) dy =
 âˆ
0
y eâˆ’y dy = 1,
E{X + Y }
=
 âˆ
0
z k(z) dz = 1
2
 âˆ
0
z3 eâˆ’z dz = 3.
Remark 4.1 Here we are given that
 âˆ
0
xneâˆ’x dx = n!
for n âˆˆN0.
This formula is easily proved by induction. When n = 0, it is trivial. In general we get by a partial
integration and the assumption above of the induction,
 âˆ
0
xn+1eâˆ’x dx =
#
âˆ’xn2eâˆ’x$âˆ
0 + (n + 1)
 âˆ
0
xneâˆ’x dx = (n + 1)n! = (n + 1)!,
and the claim follows. â™¦
Download free eBooks at bookboon.com

Random variables I
 
36 
4. Frequencies and distributions functions, 2 dimensions
Example 4.2 Check if the function
F(x, y) =
â§
â¨
â©
1 âˆ’eâˆ’(x+y),
x â‰¥0, y â‰¥0,
0,
otherwise,
is a distribution function of a 2-dimensional random variable.
Since F âˆˆCâˆ(R+ Ã— R+), we have
âˆ‚2F
âˆ‚xâˆ‚y = âˆ’eâˆ’(x+y) < 0
for (x, y) âˆˆR+ Ã— R+,
so F cannot be a distribution function. In fact, if so then
f(x, y) = âˆ‚2F
âˆ‚xâˆ‚y
(< 0)
should be a frequency, which is not possible, because frequencies are never negative.
Alternatively we prove that one of the necessary conditions is not fulï¬lled. Choose
x1 = y1 = 1
og
x2 = y2 = 1 + Î±,
Î± > 0.
Then
F(1 + Î±, 1 + Î±) âˆ’F(1, 1 + Î±) âˆ’F(1 + Î±, 1) + F(1, 1) = eâˆ’(1+Î±) âˆ’eâˆ’(2+2Î±) + eâˆ’(2+Î±) âˆ’eâˆ’2
= eâˆ’2 
2eâˆ’Î± âˆ’eâˆ’2Î± âˆ’1

= âˆ’eâˆ’2 	
1 âˆ’eâˆ’Î±
2 < 0,
and not â‰¥0, as it should be.
Example 4.3 Prove that the function
f(x, y) =
â§
â¨
â©
x eâˆ’x(y+1),
x > 0, y > 0,
0,
otherwise,
is a frequency of a 2-dimensional random variable (X, Y ).
Find the frequencies and the distribution functions of the random variables X and Y , and ï¬nd the
medians of these two distributions.
Check if the random variables X and Y are independent.
Clearly, f(x, y) â‰¥0 for every (x, y), and f is continuous, with the exception of the positive part of
the x-axis.
1) If x > 0 is kept ï¬xed, it follows by a vertical integration,
fX(x) = eâˆ’x
 âˆ
y=0
x eâˆ’xy dy = eâˆ’x,
and fX(x) = 0 for x â‰¤0.
Download free eBooks at bookboon.com

Random variables I
 
37 
4. Frequencies and distributions functions, 2 dimensions
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.5
1
1.5
2
y
0.5
1
1.5
2
2.5
3
x
Figure 9: The graph of the frequency f(x, y).
2) If y > 0 is kept ï¬xed, we get by a horizontal integration, where we use the substitution z = x(y+1),
etc.,
fY (y) =
 âˆ
x=0
x eâˆ’x(y+1) dy =
1
(y + 1)2 ,
and fY (y) = 0 for y â‰¤0.
3) It follows from
 âˆ
0
eâˆ’x dx = 1,

possibly
 âˆ
0
1
(y + 1)2 dy = 1

,
that f(x, y) is a frequency of a 2-dimensional random variable (X, Y ), and that X and Y have the
marginal frequencies fX(x) and fY (y), given in (1) and (2), resp..
Download free eBooks at bookboon.com

Random variables I
 
38 
4. Frequencies and distributions functions, 2 dimensions
4) The marginal distribution functions are
FX(x) =
 x
âˆ’âˆ
fX(t) dt =
â§
â¨
â©
1 âˆ’eâˆ’x,
for x > 0,
0,
for x â‰¤0,
and
FY (y) =
 y
âˆ’âˆ
fY (t) dt =
â§
âª
â¨
âª
â©
1 âˆ’
1
y + 1 =
y
y + 1,
for y > 0,
0,
for y â‰¤0.
5) Medians:
a) FX(x) = 1
2 = 1 âˆ’eâˆ’x for eâˆ’x = 1
2, hence x = (X) = ln 2.
b) Fy(y) = 1
2 = 1 âˆ’
1
y + 1 for
1
y + 1 = 1
2, thus y = (Y ) = 1.
6) Since
fX(x) Â· fY (y) =
eâˆ’x
(y + 1)2 Ì¸= x eâˆ’x(y+1) = f(x, y)
for x, y > 0,
X and Y are not independent.
Example 4.4 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
â§
â¨
â©
c x y,
0 < x < y < 1,
0,
otherwise.
Find the constant c. Find the frequencies and the distribution function of the random variables X and
Y . Check if the random variables X and Y are independent. Finally, ï¬nd the distribution function of
the 2-dimensional random variable (X, Y ).
1) If c > 0, then f(x, y) â‰¥0. It follows from
1 = c
 1
0
 y
0
xy dx

dy = c
 1
0
1
2 y3 dy = c
8,
that c = 8, hence the frequency is given by
f(x, y) =
â§
â¨
â©
8xy,
0 < x < y < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
39 
4. Frequencies and distributions functions, 2 dimensions
0
2
4
6
8
0.5
1
y
0.2
0.4
0.6
0.8
1
x
Figure 10: The graph of the frequency f(x, y) over 0 < x < y < 1.
0
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 11: The domain where f(x, y) > 0.
2) Clearly, fX(x) = 0 for x /âˆˆ]0, 1[. When x âˆˆ]0, 1[ it follows by a vertical integration that
fX(x) =
 1
x
8xy dy = 8x
y2
2
1
x
= 4x âˆ’4x3,
hence the marginal frequency is
fX(x) =
â§
â¨
â©
4x âˆ’4x3,
x âˆˆ]0, 1[,
0,
otherwise.
When x âˆˆ]0, 1[, we get
FX(x) =
 x
0
fX(t) dt =
 x
0

4t âˆ’4t3
dt = 2x2 âˆ’x4,
Download free eBooks at bookboon.com

Random variables I
 
40 
4. Frequencies and distributions functions, 2 dimensions
thus the marginal distribution function is
FX(x) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
x â‰¤0,
2x2 âˆ’x4,
0 < x < 1,
1,
x â‰¥1.
Clearly, fY (y) = 0 for y /âˆˆ]0, 1[. If y âˆˆ]0, 1[, we get by a horizontal integration that
fY (y) =
 y
0
8xy dx = 8y
x2
2
y
0
= 4y3,
and the marginal frequency is
fY (y) =
â§
â¨
â©
4y3,
y âˆˆ]0, 1[,
0,
otherwise.
Then for y âˆˆ]0, 1[,
FY (y) =
 y
0
fY (t) dt =
 y
0
4t3 dt = y4,
hence the marginal distribution function is
FY (y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
y â‰¤0,
y4,
y âˆˆ]0, 1[,
1,
y â‰¥1.
3) Since fX(x) Â· fY (y) Ì¸= f(x, y), we see that X and Y are not stochastically independent.
Remark 4.2 If in general the domain, in which the frequency f(x, y) > 0 is strictly positive, is not
a rectangle (possibly with inï¬nite sides, so e.g. R Ã— R is in this sense considered as a degenerated
rectangle), then the random variables X and Y are never stochastic independent. â™¦
4) If x, y âˆˆ]0, 1[, then the distribution function is
F(x, y) =
 y
0
 x
0
f(t, u) dt

du,
so 0 â‰¤t â‰¤x â‰¤1, and 0 â‰¤u â‰¤y â‰¤1. Furthermore, f(t, u) = 8ty Ì¸= 0 for 0 < t < u < 1, and 0
otherwise, so 0 < t < min{x, u}, and thus
F(x, y) =
 y
0
 min{x,u}
0
8tu dt

du =
 y
0
4u Â· (min{x, u})2 du =
 y
0
4u Â· min

x2, u2
du.
If x â‰¥0, we get 0 < u < y â‰¤x â‰¤1, hence min

x2, u2
= u2, and thus
F(x, y) =
 y
0
4u Â· u2 du = y4.
Download free eBooks at bookboon.com

Random variables I
 
41 
4. Frequencies and distributions functions, 2 dimensions
If 0 â‰¤x â‰¤y, then we get instead
F(x, y)
=
 y
0
4u Â· min

x2, u2
du =
 x
0
4u3 du +
 y
x
4u x2 du
=
x4 + 2x2 	
y2 âˆ’x2
= 2x2y2 âˆ’x4.
Summing up the distribution function of the 2-dimensional random variable (X, Y ) is
F(x, y) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
2x2y2 âˆ’x4,
0 â‰¤x â‰¤y â‰¤1,
2x2 âˆ’x4,
0 â‰¤x â‰¤1 â‰¤y,
y4,
0 â‰¤y â‰¤x,
1,
1 â‰¤min{x, y},
0,
otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables I
 
42 
4. Frequencies and distributions functions, 2 dimensions
1
2*x^2-x^4
2*x^2*y^2-x^4
y^4
0
0
0
â€“0.4
â€“0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
y
â€“0.4
â€“0.2
0.2
0.4
0.6
0.8
1
1.2
1.4
x
Figure 12: The distribution function F(x, y) of Example 4.4.
Example 4.5 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
â§
â¨
â©
ct2,
0 < x < y < 1,
0,
otherwise.
1) Find the constant c.
2) Find the frequencies and the distribution functions of the random variables X and Y .
3) Find the simultaneous distribution function of the 2-dimensional random variable (X, Y ).
1) If c > 0, then obviously f(x, y) â‰¥0. It follows from
1 = c
 1
0
 y
0
y2 dx

dy = c
 1
0
y3 dy = c
4,
that if c = 4, then
f(x, y) =
 4y2,
0 < x < y < 1,
0,
otherwise,
is the frequency of the 2-dimensional random variable (X, Y ).
2) By a vertical integration, x âˆˆ]0, 1[ ï¬xed, we obtain the marginal frequency of X,
fX(x) =
 1
x
4y2 dy = 4
3
	
1 âˆ’x3
,
thus
fX(x) =
â§
âª
â¨
âª
â©
4
3
	
1 âˆ’x3
,
for x âˆˆ]0, 1[,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
43 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 13: The graph of f(x, y).
y = x
x
y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 14: The domain 0 < x < y < 1.
When x âˆˆ]0, 1[, the marginal distribution function of X is given by
FX(x) =
 x
0
4
3
	
1 âˆ’t3
dt = 4
3 x âˆ’1
3 x4,
Download free eBooks at bookboon.com

Random variables I
 
44 
4. Frequencies and distributions functions, 2 dimensions
hence
FX(x) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
x â‰¤0,
1
3
	
4x âˆ’x4
,
0 < x < 1,
1,
x â‰¥1.
By a horizontal integration, y âˆˆ]0, 1[ ï¬xed, we get the marginal frequency of Y ,
fY (y) =
 y
0
4y2 dx = 3y3,
hence
fY (y) =
â§
â¨
â©
4y3,
y âˆˆ]0, 1[,
0,
otherwise.
It follows immediately by an integration that the marginal distribution function is given by
FY (y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
y â‰¤0,
y4,
y âˆˆ]0, 1[,
1,
y â‰¥1.
x.y
V
IV
III
II
I
Figure 15: The ï¬ve sub-domains for the distribution function.
Download free eBooks at bookboon.com

Random variables I
 
45 
4. Frequencies and distributions functions, 2 dimensions
3) When the plane is divided into the ï¬ve sub-domains Iâ€“V, it follows that
I F(x, y) = 1 for x â‰¥1 and y â‰¥1.
II F(x, y) = 0 for x â‰¤0 or y â‰¤0.
III F(x, y) = FX(x) = 1
3
	
4x âˆ’x4
for 0 < x < 1 and y â‰¥1.
IV F(x, y) = FY (y) = y4 for 0 < y â‰¤1 and x > y.
V Only here we need some computations. We keep (x, y) ï¬xed in domain V, thus 0 < x < y < 1, cf.
the ï¬gure. First take the inner integral [i.e. we ï¬rst integrate vertically] and then horizontally.
Then we get
F(x, y)
=
 x
0
 y
0
f(t, u) du

dt =
 x
0
 y
t
4u2 du

dt
=
 x
0
4
3 y3 âˆ’4
3 t3

dt = 4
3 xy3 âˆ’1
3 x4,
hence
F(x, y) = 1
3
	
4xy3 âˆ’x4
for 0 < x < y < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables I
 
46 
4. Frequencies and distributions functions, 2 dimensions
Example 4.6 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
â§
â¨
â©
cx,
0 < y < 2x < 2,
0,
otherwise.
1) Find the constant c.
2) Find the marginal frequencies and the distribution functions of the random variables X and Y .
3) (A hard question). Find the simultaneous distribution function F(x, y) of the 2-dimensional ran-
dom variable (X, Y ).
(The distribution function F(x, y) must be computed for every (x, y) âˆˆR2).
0
0.5
1
1.5
0.5
1
1.5
2
t
0.2
0.4
0.6
0.8
1
s
Figure 16: The graph of f(x, y), and its projection A, where f(x, y) > 0.
x
A
y = 2*x
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
1
Figure 17: The domain of integration A.
Download free eBooks at bookboon.com

Random variables I
 
47 
4. Frequencies and distributions functions, 2 dimensions
1) By means of a plane integral we get the condition (cf. the ï¬gure)
1 =

A
f(x, y) dx dy = c
 1
x=0
x
 2x
y=0
1 dy

dx = c
 1
0
2x2 dx = c Â· 2
3.
Therefore, if we choose c = 3
2, then f(x, y) â‰¥0 everywhere, and its integral is 1, so the frequency
is
f(x, y) =
â§
âª
â¨
âª
â©
3
2 x,
0 < y < 2x < 2,
0,
otherwise.
2) The marginal frequencies are found to be
fX(x) =
 2
y=0
x3
2 x dy = 3x2
for 0 < x < 1,
and
fY (y) =
 1
x= y
2
3
2 x dx =
3
4 x2
1
x= y
2
= 3
4 âˆ’3
16 y2,
for 0 < y < 2,
hence
fX(x) =
â§
â¨
â©
3x2,
0 < x < 1,
0,
otherwise,
fY (y) =
â§
âª
â¨
âª
â©
3
4 âˆ’3
16 y2,
0 < y < 2,
0,
otherwise.
We ï¬nd the remaining distribution functions by an integration:
FX(x) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
x < 0,
x3,
0 â‰¤x â‰¤1,
1,
x > 1,
FY (y) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
y < 0,
3
4 y âˆ’1
16 y3,
0 â‰¤y â‰¤2,
1,
y > 2.
3) (The hard question). We divide R2 into the ï¬ve domains Iâ€“V, cf. the ï¬gure.
Clearly,
F(x, y) = 0
in domain I,
F(x, y) = 1
in domain II.
In domain III (i.e. A) we get by ï¬rst integrating horizontally, u
2 â‰¤t â‰¤x, and then vertically,
0 â‰¤u â‰¤y,
F(x, y)
=
3
2
 y
u=0
 x
t= u
2
t dt

du = 3
4
 y
u=0

x2 âˆ’u2
4

du
=
3
4

x2u âˆ’1
12 u3
y
u=0
= 3
4 x2y âˆ’1
16 y3.
Download free eBooks at bookboon.com

Random variables I
 
48 
4. Frequencies and distributions functions, 2 dimensions
V
IV
III
II
I
x.y
y = 2*x
Figure 18: The ï¬ve domains of the distribution function.
We get in domain IV,
F(x, y) = F(1, y) = FY (y) = 3
4 y âˆ’1
16 y3,
and in domain V,
F(x, y) = F(x, 2x) = 3
4 2x3 âˆ’1
16 Â· 8x4 (= FX(x)) = x3.
Summing up we obtain the distribution function
F(x, y) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
0,
for x â‰¤0 or y â‰¤0,
1,
for x â‰¥1 and y â‰¥2,
3
4 x2y âˆ’1
16 y3,
for 0 â‰¤y â‰¤2x â‰¤2,
3
4 y âˆ’1
16 y3,
for x â‰¥1 and 0 â‰¤y â‰¤2,
x3,
for y â‰¥2x and 0 â‰¤x â‰¤1.
Download free eBooks at bookboon.com

Random variables I
 
49 
4. Frequencies and distributions functions, 2 dimensions
Example 4.7 A 2-dimensional random variable (X, Y ) has the frequency
f(x, y) =
â§
â¨
â©
cx,
0 â‰¤x â‰¤1, 0 â‰¤y â‰¤1 âˆ’x,
0,
otherwise.
1) Find the constant c.
2) Find the frequencies and the distribution functions of the random variables X and Y .
3) Find the simultaneous distribution function of (X, Y ).
0
1
2
3
4
5
6
0.2
0.4
0.6
0.8
1
t
0.2
0.4
0.6
0.8
1
s
Figure 19: The graph of the frequency f(x).
y = 1-x
x
y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 20: The domain of integration of the frequency f(x).
Download free eBooks at bookboon.com

Random variables I
 
50 
4. Frequencies and distributions functions, 2 dimensions
1) If c > 0, then f(x, y) â‰¥0. It follows from the condition
1 = c
 1
0
 1âˆ’x
0
x dy

dx = c
 1
0
	
x âˆ’x2
dx = c
1
2 âˆ’1
3

= c
6,
that if c = 6, then the frequency of (X, Y ) is given by
f(x, y) =
â§
â¨
â©
6x,
0 â‰¤x â‰¤1, 0 â‰¤y â‰¤1 âˆ’x,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
â€“0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 21: The graph of the frequency fX(x) of X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
51 
4. Frequencies and distributions functions, 2 dimensions
2) It follows by a vertical integration, x âˆˆ]0, 1[ ï¬xed, that
fX(x) =
 1âˆ’x
0
6x dy = 6x âˆ’6x2,
so the frequency of X is
fX(x) =
â§
â¨
â©
6x âˆ’6x2,
x âˆˆ]0, 1[,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
â€“0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 22: The distribution function FX(x) of X.
0
0.5
1
1.5
2
2.5
3
0.2 0.4 0.6 0.8
1
1.2
x
Figure 23: The graph of the frequency fY (y).
A horizontal integration, y âˆˆ]0, 1[ ï¬xed, gives
fY (y) =
 1âˆ’y
0
6x dx = 3(1 âˆ’y)2,
Download free eBooks at bookboon.com

Random variables I
 
52 
4. Frequencies and distributions functions, 2 dimensions
hence the frequency of Y is
fY (y) =
â§
â¨
â©
3(1 âˆ’y)2,
0 < y < 1,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
â€“0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 24: The distribution function FY (y).
By an integration, for y âˆˆ]0, 1[ ï¬xed,
FY (y) =
 y
0
3(1 âˆ’t)2 dt =
#
âˆ’(1 âˆ’t)3$y
0 = 1 âˆ’(1 âˆ’y)3 = 3y âˆ’3y2 + y3,
so the distribution function of Y is
FY (u) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
y â‰¤0,
1 âˆ’(1 âˆ’y)3,
0 < y < 1,
1,
y â‰¥1.
3) If we divide the plane into the domains Iâ€“VI, it follows that
I F(x, y) = 1 for x â‰¥1 and y â‰¥1.
II F(x, y) = 0 for x â‰¤0 or y â‰¤0.
III F(x, y) = FX(x) = 3x2 âˆ’2x3 for 0 < x < 1 and y â‰¥1.
IV F(x, y) = FY (y) = 1 âˆ’(1 âˆ’y)3 for x â‰¥1 and 0 < y < 1.
Download free eBooks at bookboon.com

Random variables I
 
53 
4. Frequencies and distributions functions, 2 dimensions
VI
V
IV
III
II
I
Figure 25: The domains Iâ€“VI.
V
x
1-y
y
x.y
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 26: The domain of integration in case V.
V Let 0 â‰¤x â‰¤1 and 1 âˆ’x â‰¤y â‰¤1. Then
F(x, y)
=
 1âˆ’y
0
 y
0
6u du

dt +
 x
1âˆ’y
 1âˆ’t
0
6t du

dt
=
 1âˆ’y
0
3y2 dt +
 x
1âˆ’y
	
6t âˆ’6t2
dt
=
3y2(1 âˆ’y) +
#
3t2 âˆ’2t3$x
1âˆ’y
=
3y2(1 âˆ’y) + 3x2 âˆ’2x3 âˆ’3(1 âˆ’y)2 + 2(1 âˆ’y)3
=
3x2 âˆ’2x3 + (1 âˆ’y)

3y âˆ’3y2 âˆ’3 + 3y + 2 âˆ’4y + 2y2
=
3x2 âˆ’2x3 + (1 âˆ’y)
	
âˆ’1 + 2y âˆ’y2
=
3x2 âˆ’2x3 âˆ’(1 âˆ’y)3.
Download free eBooks at bookboon.com

Random variables I
 
54 
4. Frequencies and distributions functions, 2 dimensions
VI Finally, if 0 â‰¤x â‰¤1 and 0 â‰¤y â‰¤1 âˆ’x, then
F(x, y) =
 x
0
 y
0
6t du

dt = 3x2y.
Summing up we get
F(x, y) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
1,
for x â‰¥1 and y â‰¥1,
0,
for x â‰¤0 or y â‰¤0,
3x2 âˆ’2x3,
for 0 < x < 1 and y â‰¥1,
1 âˆ’(1 âˆ’y)3,
for x â‰¥1 and 0 < y < 1,
3x2 âˆ’2x3 âˆ’(1 âˆ’y)3,
for 0 â‰¤x â‰¤1 and 1 âˆ’x â‰¤y â‰¤1,
3x2y,
for 0 â‰¤x â‰¤1 and 0 â‰¤y â‰¤1 âˆ’x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables I
 
55 
4. Frequencies and distributions functions, 2 dimensions
Example 4.8 Let X1 and X2 be independent and identically distributed random variables of the
frequencies
f(x) =
â§
âª
â¨
âª
â©
1
2 x,
0 < x < 2,
0,
otherwise,
and let the 2-dimensional random variable
(Y1, Y2) = Ï„ (X1, X2)
be given by
Y1 = X1X2,
Y2 = X1
X2
.
1. Compute the means E {X1} and E
 1
X1

.
2. Compute the means of Y1 and for Y2.
The vector function Ï„, given by
Ï„ (x1, x2) =

x1x2, x1
x2

,
maps D = ]0, 2[ Ã— ]0, 2[ bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < 4y2, y1y2 < 4

.
3. Sketch Dâ€², and ï¬nd the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
5. Are Y1 and Y2 independent?
1) It follows that
E {X1} = 1
2
 2
0
x2 dx = 1
2
x3
3
2
0
= 4
3,
and
E
 1
X1

= 1
2
 2
0
x
x dx = 1.
2) Since X1 and X2 are independent, we conclude from (1) that
E {Y1} = E {X1X2} = E {E1} E {X2} =
4
3
3
= 16
9 ,
and
E {Y2} = E
X1
X2

= E {X1} Â· E
 1
X2

= 4
3 Â· 1 = 4
3.
Download free eBooks at bookboon.com

Random variables I
 
56 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
1
2
3
4
Figure 27: The domain Dâ€² lies between the y2-axis, the hyperbola y1y2 = 4 and the straight line
y1 = 4y2.
3) It follows from y1 = x1x2 and y2 = x1
x2
that
x1 = âˆšy1y2
and
x2 =
y1
y2
.
Hence the Jacobian becomes
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2
1
âˆšy1y2
âˆ’1
2
1
y2
y1
y2

= âˆ’1
4
 1
y2
+ 1
y2

= âˆ’1
2y2
.
The simultaneous frequency of (X1, X2) is
g (x1, x2) f (x1) Â· f (x2) =
â§
âª
â¨
âª
â©
1
4 x1x2,
for (x1, x2) âˆˆ]0, 2[ Ã— ]0, 2[,
0
otherwise,
so the simultaneous frequency of (Y1, Y2) is
k (y1, y2) =
â§
âª
â¨
âª
â©
1
8
y1
y2
for (y1, y2) âˆˆDâ€²,
0
otherwise.
4) The marginal frequency of Y1 for 0 < y1 < 4 is given by
kY1 (y1) = y1
8

4
y1
y1
4
1
y2
dy2 = y1
8 [ln y2]
4
y1
y1
4 = y1
4 ln
 4
y1

= y1
4 (ln 4 âˆ’ln y1) ,
and = 0 otherwise.
The marginal frequency of Y2 for y2 âˆˆ]0, 1] is given by
kY2 (y2) =
1
8y2
 4y2
0
y1 dy1 =
1
16y2
Â· 16y2
2 = y2.
Download free eBooks at bookboon.com

Random variables I
 
57 
4. Frequencies and distributions functions, 2 dimensions
If instead y2 âˆˆ]1, âˆ[, then
kY2 (y2) =
1
8y2

4
y2
0
y1 dy1 = 1
y3
2
.
Summing up we get
kY1 (y1) =
â§
âª
âª
â¨
âª
âª
â©
y1
4 ln
 4
y1

,
0 < y1 < 4,
0,
otherwise,
and
kY2 (y2) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
y2,
for y2 âˆˆ]0, 1],
1
y3
2
,
for y2 âˆˆ]1, âˆ[,
0,
for y2 â‰¤0.
5) Since Dâ€² is not a rectangular domain, we conclude that Y1 and Y2 cannot be independent.
There is clearly a trap here, because we get for (y1, y2) âˆˆDâ€²,
k (y1, y2) = 1
8 Â· y1 Â· 1
y2
,
in which y1 and y2 apparently are separated.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables I
 
58 
4. Frequencies and distributions functions, 2 dimensions
Example 4.9 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
â§
âª
â¨
âª
â©
1
2 (x1 + x2) exp (âˆ’(x1 + x2)) ,
x1 > 0, x2,
0,
otherwise.
1. Compute the marginal frequencies of X1 and X2.
Then introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1 âˆ’X2.
2. Prove that Y1 are Y2 non-correlated.
The vector function Ï„ given by
Ï„ (x1, x2) = (x1 + x2, x1 âˆ’x2)
maps R+ Ã— R+ bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | |y2| < y1

.
3. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Compute the marginal frequency of Y1, and ï¬nd the mean of Y1.
5. Find the marginal frequency of Y2.
1) If x1 > 0, then
hX1 (x1) = 1
2 eâˆ’x1
 âˆ
0
	
x1eâˆ’x2 + x2eâˆ’x2
dx2 = 1
2 (x1 + 1) eâˆ’x1,
and hX1 (x1) = 0 for x1 â‰¤0.
Then by symmetry, X1 and X2 have the same distribution, hence
hX2 (x2) =
â§
âª
â¨
âª
â©
1
2 (x2 + 1) eâˆ’x2,
for x2 > 0,
0,
for x2 â‰¤0.
2) It follows from (1) that V {X1} = V {X2}, thus
Cov (Y1, Y2) = Cov (X1 + X2, X1 âˆ’X2) = V {X1} âˆ’V {X2} + Cov (X1, X2) = 0,
which shows that Y1 and Y2 are non-correlated.
Notice that they are not independent, cf. (3), because the domain is not a rectangle parallel to
the coordinate axes.
Download free eBooks at bookboon.com

Random variables I
 
59 
4. Frequencies and distributions functions, 2 dimensions
3) It follows from y1 = x1 + x2 and y2 = x1 âˆ’x2 that
x1 = 1
2 (y1 + y2)
and
x2 = 1
2 (y1 âˆ’y2) ,
so the Jacobian becomes
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
1
2
1
2
âˆ’1
2

= âˆ’1
2.
Hence,
k (y1, y2) =
â§
âª
â¨
âª
â©
1
4 y1eâˆ’y1,
for |y2| < y1,
0,
otherwise.
4) The marginal frequency of Y1 is
fY1 (y1) = 1
4 y1eâˆ’y1
 y1
âˆ’y1
dy2 = 1
2 y2
1eâˆ’y1
for y1 > 0,
and fY1 (y1) = 0 otherwise.
Since Y1 âˆˆÎ“(3, 1) is gamma distributed, we get
E {Y1} = 3 Â· 1 = 2,
which can also be found directly from
E {Y1} = 1
2
 âˆ
0
y3
1eâˆ’y1dy1 = 3!
2 = 3.
5) If y2 âˆˆR, then
fY2 (y2) = 1
4
 âˆ
|y2|
y1eâˆ’y1 dy1 = 1
4
#
âˆ’(y1 + 1) eâˆ’y1$âˆ
|y2| = 1
4 (|y2| + 1) eâˆ’|y2|.
Download free eBooks at bookboon.com

Random variables I
 
60 
4. Frequencies and distributions functions, 2 dimensions
Example 4.10 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
â§
âª
â¨
âª
â©
1
2 (x1 + 1) eâˆ’(x1+x2),
x1 > 0,
x2 > 0,
0,
otherwise.
1. Compute the marginal frequencies of X1 and X2.
2. Compute the means of X1 and X2.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 =
X2
X1 + X2
.
The vector function Ï„, given by
Ï„ (x1, x2) =

x1 + x2,
x2
x1 + x2

,
maps R+ Ã— R+ bijectively onto Dâ€² = R+Ã— ]0, 1[.
3. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Compute the marginal frequencies of Y1 and Y2, and check if Y1 and Y2 are independent.
5. Compute the means of Y1 and Y2.
1) Since h (x1, x2) has a nice factorization,
h (x1, x2) = fX1 (x1) Â· fX2 (x2) ,
where
fX1 (x1) =
â§
âª
â¨
âª
â©
1
2 (x1 + 1) eâˆ’x1,
x1 > 0,
0,
x1 â‰¤0,
fX2 (x2) =
â§
â¨
â©
eâˆ’x2,
x2 > 0,
0,
x2 â‰¤0,
and fX1 (x1) â‰¥0 and fX2 (x2) â‰¥0, where
 âˆ
âˆ’âˆ
fX1 (x1) dx1 = 1,
 âˆ
âˆ’âˆ
fX2 (x2) dx2 = 1,
we have
a) found the marginal frequencies,
b) and shown that X1 and X2 are stochastically independent.
Alternatively we compute for x1 > 0, resp. x2 > 0,
fX1 (x1) =
 âˆ
0
h (x1, x2) dx2 = 1
2 (x1 + 1) eâˆ’x1
 âˆ
0
eâˆ’x2 dx2 = 1
2 (x1 + 1) eâˆ’x1,
Download free eBooks at bookboon.com

Random variables I
 
61 
4. Frequencies and distributions functions, 2 dimensions
and
fX2 (x2) =
 âˆ
0
h (x1, x2) dx1 = 1
2 eâˆ’x2
 âˆ
0
(x1 + 1) eâˆ’x1 dx1 = eâˆ’x2.
Here we have applied that
 âˆ
0
xneâˆ’x dx = n!
for n âˆˆN0.
2) The means are
E {X1} =
 âˆ
0
x1fX1 (x1) dx1 = 1
2
 âˆ
0
	
x2
1 + x1

eâˆ’x1 dx1 = 1
2 (2! + 1!) = 3
2,
and
E {X2} =
 âˆ
0
x2fX2 (x2) dx2 =
 âˆ
0
x2 eâˆ’x2 dx2 = 1! = 1.
3) The transform formula is memorized by
h (x1, x2) dx1 dx2 = k (y1, y2)

âˆ‚(x1, x2)
âˆ‚(y1, y2)
 dy1 dy2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
62 
4. Frequencies and distributions functions, 2 dimensions
This formula shows that the task is to ï¬nd x1 and x2 expressed by (y1, y2). It follows from
â§
âª
â¨
âª
â©
y1 = x1 + x2,
y2 =
x2
x1 + x2
,
that
â§
â¨
â©
y1 = x1 + x2,
y1y2 = x2,
hence
â§
â¨
â©
x1 = y1 (1 âˆ’y2) ,
x2 = y1y2.
Thus we get the weight function
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1 âˆ’y2
âˆ’y1
y2
y1
 = y1 âˆ’y1y2 + y1y2 = y1 > 0,
because Dâ€² = R+Ã— ]0, 1[ is given.
In this domain we get the simultaneous frequency of (Y1, Y2),
k (y1, y2)
=
h (y1 (1 âˆ’y2) , y1y2) Â· y1 = 1
2 (y1 âˆ’y1y2 + 1) Â· eâˆ’y1 Â· y1
=
1
2
	
y2
1 âˆ’y2
1y2 + y1

eâˆ’y1
for (y1, y2) âˆˆDâ€²,
and k (y1, y2) = 0 otherwise.
4) The marginal frequencies of Y1 and Y2 are computed for y1 > 0, resp. y2 âˆˆ]0, 1[. (Otherwise they
are 0.)
kY1 (y1)
=
 1
y2=0
k (y1, y2) dy2 = 1
2 eâˆ’y1
 1
0
	
y2
1 âˆ’y2
1y2 + y1

dy2
=
1
2 eâˆ’y1

y2
1 âˆ’1
2 y2
1 + y1

=
1
4 y2
1 + 1
2 y1

eâˆ’y1,
kY2 (y2)
=
 âˆ
y1=0
k (y1, y2) dy1 = 1
2
 âˆ
0
	
y2
1 âˆ’y2
1y2 + y1

eâˆ’y1 dy1
=
1
2 (2 âˆ’2y2 + 1) = 3
2 âˆ’y2.
Since k (y1, y2) Ì¸= kY1 (y1) Â· kY2 (y2), we see that Y1 and Y2 are not independent.
5) The means are
E {Y1} = E {X1 + X2} = E {X1} + E {X2} = 3
2 + 1 = 5
2.
Alternatively,
E {Y1} =
 âˆ
0
y1kY1 (y1) dy1 =
 âˆ
0
1
4 y3
1 + 1
2 y2
1

eâˆ’y1 dy1 = 6
4 + 2
2 = 5
2.
For Y2 we get
E {Y2} =
 1
0
y2kY2 (y2) dy2 =
 1
0
3
2 y2 âˆ’y2
2

dy2 = 3
4 âˆ’1
3 = 5
12.
Download free eBooks at bookboon.com

Random variables I
 
63 
4. Frequencies and distributions functions, 2 dimensions
Example 4.11 Let (X, Y ) be a 2-dimensional random variable of the frequency
h(x, y) =
â§
â¨
â©
eâˆ’y,
for 0 â‰¤x â‰¤y,
0,
otherwise.
1) Find the marginal frequencies of the random variables X and Y .
2) Find the means E{X} and E{Y } of the random variables X and Y .
3) Find the variances V {X} and V {Y } of the random variables X and Y .
4) Compute E{XY }, and then the correlation Ï±(X, Y ).
5) Find the frequency of Z = X + Y .
1) The marginal frequency of X is
fX(x) =
 âˆ
y=x
eâˆ’y dy = eâˆ’x
for x â‰¥0,
and fX(x) = 0 for x < 0.
Analogously the marginal frequency of Y is given by
fY (y) =
 y
x=0
eâˆ’y dx = y eâˆ’y
for y â‰¥0,
and fY (y) = 0 for y â‰¤0.
Summing up we get
X âˆˆÎ“(1, 1)
and
Y âˆˆÎ“(2, 1).
2) Then
E{X} =
 âˆ
0
x eâˆ’x dx =
#
âˆ’(x + 1) eâˆ’x$âˆ
0 = 1,
and
E{Y } =
 âˆ
0
y Â· y eâˆ’y dy =
 âˆ
0
y2eâˆ’y dy =
#	
âˆ’y2 âˆ’2y âˆ’2

eâˆ’y$âˆ
0 = 2.
3) We ï¬rst compute
E

X2
=
 âˆ
0
x2eâˆ’x dx =
#	
âˆ’x2 âˆ’2x âˆ’2

eâˆ’x$âˆ
0 = 2
and
E

Y 2
=
 âˆ
0
y2 Â· y eâˆ’y dy =
 âˆ
0
y3eâˆ’y dy = 3! = 6.
This gives us the variances
V {X}
=
E

X2
âˆ’(E{X})2 = 2 âˆ’12 = 1,
V {Y }
=
E

Y 2
âˆ’(E{Y })2 = 6 âˆ’22 = 2.
Download free eBooks at bookboon.com

Random variables I
 
64 
4. Frequencies and distributions functions, 2 dimensions
4) By a reduction to a double integral we get
E{XY } =
 
R2 xy h(x, y) dx dy =
 âˆ
y=0
 y
x=0
x dx

y eâˆ’y dy =
 âˆ
0
1
2 y3eâˆ’y dy = 3,
or alternatively
E{XY }
=
 âˆ
x=0
x
 âˆ
y=x
y eâˆ’y dy

dx =
 âˆ
x=0
x
#
âˆ’(y + 1)eâˆ’y$âˆ
x dy
=
 âˆ
0
	
x2eâˆ’x + x eâˆ’x
dx = 2 + 1 = 3.
Then
Cov(X, Y ) = E{XY } âˆ’E{X} Â· E{Y } = 3 âˆ’2 Â· 1 = 1,
hence
Ï±(X, Y ) =
Cov(X, Y )
%
V {X} V {Y }
=
1
âˆš
1 Â· 2 =
âˆš
2
2 .
5) The random variable Z = X + Y has its values in ]0, âˆ[. If z > 0, then the frequency is given by
fZ(z) =
 âˆ
âˆ’âˆ
h(x, z âˆ’x) dx,
where the condition 0 â‰¤x â‰¤y = z âˆ’x is reformulated to
0 â‰¤x â‰¤z
2.
Hence, when z > 0,
fZ(z) =

z
2
0
h(x, z âˆ’x) dx =

z
2
0
eâˆ’(zâˆ’x) dx = eâˆ’z 
exp
z
2

âˆ’1
 
= exp

âˆ’z
2

âˆ’exp(âˆ’z),
so
fZ(z) =
â§
âª
â¨
âª
â©
exp

âˆ’z
2

âˆ’e(âˆ’z),
for z â‰¥0,
0,
for z < 0.
Alternatively we compute the distribution function of Z by the following double integral,
FZ(z)
=

z
2
x=0
 zâˆ’x
y=x
eâˆ’y dy

dx =

z
2
x=0

eâˆ’x âˆ’exâˆ’z
dx
=
1 âˆ’exp

âˆ’z
2

âˆ’eâˆ’z 
exp
z
2

âˆ’1

= 1 + eâˆ’z âˆ’2 exp

âˆ’z
2

=

1 âˆ’exp

âˆ’z
2
 2
.
Hence,
fZ(z) = F â€²
Z(z) = exp

âˆ’z
2

âˆ’eâˆ’z
for z â‰¥0,
Download free eBooks at bookboon.com

Random variables I
 
65 
4. Frequencies and distributions functions, 2 dimensions
and fZ(z) = 0 for z < 0.
Additional remark. Since
E{Z} =
 âˆ
0

z exp

âˆ’z
2

âˆ’z eâˆ’z 
dz = 4 âˆ’1 = 3,
and
E

Z2
=
 âˆ
0

z2 exp

âˆ’z
2

âˆ’z2eâˆ’z 
dz = 16 âˆ’2 = 14,
we get
V {Z} = 14 âˆ’32 = 5 = V {X + Y }.
This gives
Cov(X, Y ) = 1
2 (V {X + Y } âˆ’V {X} âˆ’V {Y }) = 1
2 {5 âˆ’1 âˆ’2} = 1.
â™¦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
66 
4. Frequencies and distributions functions, 2 dimensions
Example 4.12 A 2-dimensional random variable (X, Y ) has the simultaneous frequency
f(x, y) = 1
2 xy,
0 < y < x < 2
(and 0 otherwise).
1) Compute the marginal frequencies of X and Y .
2) Compute the marginal distribution functions of X and Y .
3) Find the means of X and Y .
4) Find the medians of X and Y .
1) The marginal frequencies:
a) For ï¬xed x âˆˆ[0, 2] we integrate with respect to y âˆˆ[0, x], which gives
fX(x) =
 x
y=0
1
2 xy dy = 1
4 x3,
0 < x < 2,
and fX(x) = 0 otherwise.
b) For ï¬xed y âˆˆ[0, 2] we integrate with respect to x âˆˆ[y, 2], which gives
fY (y) =
 2
x=y
1
2 xy dx = y âˆ’1
4 y3,
0 < y < 2,
and fY (y) = 0 otherwise.
2) We get the distribution functions by integrating the frequencies,
FX(x) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
x â‰¤0,
1
16 x4,
0 < x < 2,
1,
x â‰¥2,
and
FY (y) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
y â‰¤0,
1
2 y2 âˆ’1
16 y4,
0 < y < 2,
1,
y â‰¥2.
3) Then we compute the means,
E{X}
=
 2
0
x fX(x) dx =
 2
0
1
4 x4 dx =
 1
20 x5
2
0
= 32
20 = 8
5,
E{Y }
=
 2
9
y fY (y) dy =
 2
0

y2 âˆ’1
4 y4

dy = 8
3 âˆ’8
5 = 16
15.
Download free eBooks at bookboon.com

Random variables I
 
67 
4. Frequencies and distributions functions, 2 dimensions
4) The median of X is found from the equation
FX(x) = 1
16 x4 = 1
2,
i.e. x4 = 8, hence (X) =
4âˆš
8.
The median of Y is found from the equation
FY (y) = 1
2 y2 âˆ’1
16 y4 = 1
2,
i.e.
y4 âˆ’8y2 + 8 = 0.
Since y2 â‰¤22 = 4, we get y2 = 4 âˆ’
âˆš
8, so
(Y ) =
&
4 âˆ’
âˆš
8.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables I
 
68 
4. Frequencies and distributions functions, 2 dimensions
Example 4.13 A rectangle has the edge lengths X1 and X2, where X1 and X2 are independent
random variables, both of the frequency
f(x) =
â§
â¨
â©
3x2,
0 < x < 1,
0,
otherwise.
1. Find the mean E {X1}.
2. Find the mean of the circumference of the rectangle, E {2X1 + 2X2}, and the mean of the area of
the rectangle, E {X1X2}.
We introduce the random variables Y1 and Y2 by
Y1 = X1X2,
Y2 = X1
X2
.
The vector function Ï„, given by
Ï„ (x1, x2) =

x1x2, x1
x2

,
maps D = ]0, 1[ Ã— ]0, 1[ bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < y2, y1y2 < 1

.
3. Sketch Dâ€² and ï¬nd the simultaneous frequency k (y1, y2) for (Y1, Y2).
4. Compute the marginal frequencies of Y1 and Y2.
(This can be answerede with or without using (3)).
5. Are Y1 and Y2 stochastically independent?
6. Find the mean and the median of Y2, and give an intuitive explanation of that the median is smaller
than the mean.
1) The mean is
E {X1} = E {X2} =
 1
0
3x3 dx = 3
4.
2) Since X1 and X2 are independent, we get
E {2X1 + 2X2} = 4 E {X1} = 3,
and
E {X1X2} = E {X1} Â· E {X2} =
3
4
2
= 9
16.
Download free eBooks at bookboon.com

Random variables I
 
69 
4. Frequencies and distributions functions, 2 dimensions
0
1
2
3
4
0.20.40.60.8 1 1.2
3) From
x1 = âˆšy1y2
og
x2 =
y1
y2
,
we get the Jacobian
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2
1
âˆšy1y2
âˆ’1
2
1
y2
y1
y2

= âˆ’1
2y2
< 0,
and the simultaneous frequency for (y1, y2) âˆˆDâ€², is given by
k (y1, y2) = 3 (âˆšy1y2)2 Â· 3
y1
y2
2
Â·
1
2y2
= 9 Â· y1y2 Â· y1
y2
Â·
1
2y2
= 9
2
y2
1
y2
,
and k (y1, y2) = 0 otherwise.
4) The marginal frequency of Y1 for y1 âˆˆ]0, 1[, is given by
kY1 (y1) = 9
2 y2
1

1
y1
y1
dy2
y2
= 9
2 y2
1 [ln y2]
1
y1
y1 = 9y2
1 ln 1
y1
= âˆ’9y2
1 ln y1,
and 0 otherwise.
If y2 âˆˆ]0, 1], then
kY2 (y2) =
9
2y2
 y2
0
y2
1 dy1 =
3
2y2
y3
2 = 3
2 y2
2.
If y2 âˆˆ]1, âˆ[, then
kY2 (y2) =
9
2y2

1
y2
0
y2
1 dy1 =
3
2y2
Â· 1
y3
2
= 3
2 Â· 1
y4
2
,
Download free eBooks at bookboon.com

Random variables I
 
70 
4. Frequencies and distributions functions, 2 dimensions
hence
kY2 (y2) =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
3
2 y2
2
for y2 âˆˆ]0, 1],
3
2
1
y4
2
for y2 âˆˆ]1, âˆ[,
0
otherwise.
5) Since Dâ€² is not a rectangular domain, we conclude that Y1 and Y2 are not independent.
Alternatively we see that
k (y1, y2) Ì¸= kY1 (y1) Â· kY2 (y2) .
6) The mean of Y2 is
E {Y2} = 3
2
 1
0
y3
2 dy2 + 3
2
 âˆ
1
dy2
y3
2
= 3
8 + 3
4 = 9
8.
It follows from
P {Y2 < 1} =
 1
0
3
2 y2
2 dy2 = 1
2,
that the median is
âŸ¨Y2âŸ©= 1 < E {Y2} .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables I
 
71 
4. Frequencies and distributions functions, 2 dimensions
Example 4.14 Let X1 and X2 be independent random variables of the frequencies
fX1 (x1) =
â§
â¨
â©
x1eâˆ’x1,
x1 > 0,
0,
x1 â‰¤0,
and
fX2 (x2) =
â§
â¨
â©
9x2eâˆ’3x2,
x2 > 0,
0,
x2 â‰¤0.
1. Find the means E {X1} and E {X2}.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + 3X2,
Y2 =
X1
X1 + 3X2
.
The vector function Ï„, given by
Ï„ (x1, x2) =

x1 + 3x2,
x1
x1 + 3x2

,
maps R+ Ã— R+ bijectively onto R+Ã— ]0, 1[.
2. Compute the simultaneous frequency k (y1, y2) of (Y1, Y2).
3. Compute the marginal frequencies of Y1 and Y2.
4. Check if Y1 and Y2 are stochastically independent.
5. Compute the mean E {Y2}, and prove that the median of Y2 is equal to E {Y2}.
1) Since X1 âˆˆÎ“(2, 1), we have E {X1} = 2 Â· 1 = 2.
Since X2 âˆˆÎ“

2, 1
3

, we have E {X2} = 2 Â· 1
3 = 2
3.
2) It follows from
y1 = x1 + 3x2
and
y2 =
x1
x1 + 3x2
that
x1 = y1y2
and
x2 = 1
3 (y1 âˆ’x1) = 1
3 y1 (1 âˆ’y2) .
The Jacobian becomes
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

y2
y1
1
3 (1 âˆ’y2)
âˆ’1
3 y1

= âˆ’1
3 y1 < 0.
Download free eBooks at bookboon.com

Random variables I
 
72 
4. Frequencies and distributions functions, 2 dimensions
The simultaneous frequency og (y1, y2) âˆˆR+Ã— ]0, 1[ is given by
k (y1, y2)
=
y1y2 Â· eâˆ’y1y2 Â· 9 Â· 1
3 y1 (1 âˆ’y2) Â· eâˆ’y1(1âˆ’y2) Â· 1
3 y1
=
y3
1 Â· eâˆ’y1 Â· y2 (1 âˆ’y2) =
1
6 y3
1eâˆ’y1

Â· {6y2 (1 âˆ’y2)} ,
and k (y1, y2) = 0 otherwise.
3) It follows immediately of the splitting of (2) that
kY1 (y1) =
â§
âª
â¨
âª
â©
1
6 y3
1eâˆ’y1
for y1 > 0,
0
otherwise,
and
kY2 (y2) =
â§
â¨
â©
6y2 (1 âˆ’y2)
for 0 < y2 < 1,
0
otherwise,
hence Y1 âˆˆÎ“(4, 1) is gamma distributed, and Y2 âˆˆBe(2, 2) is beta distributed.
4) It follows from
k (y1, y2) = kY1 (y1) Â· kY2 (y2) ,
that Y1 and Y2 are independent.
5) By symmetry, the median is âŸ¨Y2âŸ©= 1
2, and the mean is
E {Y2} =
2
2 + 2 = 1
2 = âŸ¨Y2âŸ©.
Alternatively,
E {Y2} = 6
 1
0
	
y2
2 âˆ’y3
2

dy2 = 6
1
3 âˆ’1
4

= 6
12 = 1
2.
Download free eBooks at bookboon.com

Random variables I
 
73 
4. Frequencies and distributions functions, 2 dimensions
Example 4.15 Let X1 and X2 be independent random variables of the frequencies
fX1 (x1) =
â§
â¨
â©
eâˆ’x1,
x1 > 0,
0,
x1 â‰¤0,
and
fX2 (x2) =
â§
â¨
â©
x2eâˆ’x2,
x2 > 0,
0,
x2 â‰¤0.
1. Find the means E {X1}, E {X2} and E
 1
X2

.
We introduce the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1
X2
.
The vector function Ï„, given by
Ï„ (x1, x2) =

x1 + x2, x1
x2

,
maps R+ Ã— R+ bijectively onto itself.
2. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
3. Find the marginal frequencies of Y1 and Y2.
(The question can be answered with or without use of the result of (2).)
4. Check if Y1 and Y2 are independent.
5. Find the mean E {Y2}.
6. Find the median of Y2.
1. Since X1 âˆˆÎ“(1, 1) and X2 âˆˆÎ“(2, 1), it follows immediately that
E {X1} = 1
and
E {X2} = 2.
Alternatively,
E {X1} =
 âˆ
0
x1eâˆ’x1 dx1 = 1
and
E {X2} =
 âˆ
0
x2
2eâˆ’x2 dx2 = 2! = 2.
Finally,
E
 1
X2

=
 âˆ
0
1
x2
Â· x2 eâˆ’x2 dx2 =
 âˆ
0
eâˆ’x2 dx2 = 1.
Download free eBooks at bookboon.com

Random variables I
 
74 
4. Frequencies and distributions functions, 2 dimensions
2. We solve the equations y1 = x1 + x2 and y2 = x1
x2
, i.e. x1 = x2y2, with respect to (x1, x2). By
insertion,
y1 = x2y2 + x2 = x2 (1 + y2) ,
i.e. x2 =
y1
1 + y2
,
hence
x1 = y1y2
1 + y2
= y1 âˆ’
y1
1 + y2
and
x2 =
y1
1 + y2
.
The solution is unique, because y2 Ì¸= âˆ’1. From (y1, y2) âˆˆR+Ã—R+ follows that (x1, x2) âˆˆR+Ã—R+,
and vice versa, so Ï„ maps the domain R+ Ã— R+ bijectively onto itself.
The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

y2
1 + y2
y2
(1 + y2)2
1
1 + y2
âˆ’
y1
(1 + y2)2

= âˆ’
1
(1 + y2)3 (y2y1 + y1) = âˆ’
y1
(1 + y2)2 < 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
75 
4. Frequencies and distributions functions, 2 dimensions
Now, X1 and X2 are independent, so the simultaneous frequency of (X1, X2) is given by
f (x1, x2) =
â§
â¨
â©
x2 eâˆ’(x1+x2),
for x1 > 0 and x2 > 0,
0,
otherwise.
Thus the simultaneous frequency of (Y1, Y2) is
k (y1, y2) =
y1
1 + y2
eâˆ’y1 Â·
y1
(1 + y2)2 = 1
2 y2
1 eâˆ’y1 Â·
2
(1 + y2)3
for y1 > 0 and y2 > 0,
and k (y1, y2) = 0 otherwise.
3. & 4. It follows from the splitting of (2) that
kY1 (y1) =
â§
âª
â¨
âª
â©
1
2 y2
1eâˆ’y1
for y1 > 0,
0,
otherwise,
and
kY2 (y2) =
â§
âª
âª
â¨
âª
âª
â©
2
(1 + y2)3
for y2 > 0,
0
otherwise.
It follows that Y1 and Y2 are independent.
Notice that Y1 âˆˆÎ“(3, 1), and also by some elaboration that 2Y2 âˆˆF(2, 4).
3. Alternatively it follows for y1 > 0 that
kY1 (y1) =
 y1
0
eâˆ’(y1âˆ’t) Â· t Â· eâˆ’t dt = eâˆ’y1
 y1
0
t dt = 1
2 y2
1eâˆ’y1,
and kY1 (y1) = 0 for y1 â‰¤0.
If y2 > 0, we get by a formula that
kY2 (y2) =
 âˆ
0
eâˆ’y2t Â· t eâˆ’y Â· |t| dt =
 âˆ
0
t2eâˆ’(1+y2)t dt =
2!
(1 + y2)3 =
2
(1 + y2)3 ,
and kY2 (y2) = 0 for y2 â‰¤0.
5. Since X1 and X2 are independent, the mean is
E {Y2} = E
X1
X2

= E {X1} Â· E
 1
X2

= 1 Â· 1 = 1.
Alternatively,
E {Y2}
=
 âˆ
0
2y2
(1 + y2)3 dy2 = 2
 âˆ
0

1
(1 + y2)2 âˆ’
1
(1 + y2)3

dy2
=
2

âˆ’
1
1 + y2
+ 1
2
1
(1 + y2)2
âˆ
0
= 2

1 âˆ’1
2

= 1.
Download free eBooks at bookboon.com

Random variables I
 
76 
4. Frequencies and distributions functions, 2 dimensions
Alternatively, 2Y2 âˆˆF(2, 4), so
E {2Y2} =
n2
n2 âˆ’2 = 4
2 = 2,
hence
E {Y2} = 1.
6. The distribution function of y2 > 0 is given by
FY2 (y2) =
 y2
0
2
(1 + t)3 dt =

âˆ’
1
(1 + t)2
y2
0
= 1 âˆ’
1
(1 + y2)2 ,
so the median is determined by
1
(1 + y2)2 = 1
2,
hence
âŸ¨Y2âŸ©=
âˆš
2 âˆ’1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
77 
5. Functions and random variables, in general
5
Functions of random variables, in general
Example 5.1 Let X1 and X2 be random variables, and let Y1 = Ï•1 (X1) and Y2 = Ï•2 (X2).
1) Assume that X1 and X2 are independent. Is it possible to conclude that Y1 and Y2 are independent?
2) Assume that X1 and X2 are dependent. Does it follow that Y1 and Y2 are dependent?
1) The answer is â€˜yesâ€™. Let Ï•âˆ’1(C) = {t | Ï•(t) âˆˆC} denote the inverse set map. Then
P {Ï•1 (X1) âˆˆA âˆ§Ï•2 (X2) âˆˆB}
= P

X1 âˆˆÏ•âˆ’1
1 (A) âˆ§X2 âˆˆÏ•âˆ’1
2 (B)

= P

X1 âˆˆÏ•âˆ’1
1 (A)

Â· P

X2 âˆˆÏ•âˆ’1
2 (B)

,
because X1, X2 are independent
= P {Ï•1 (X1) âˆˆA} Â· P {Ï•2 (X2) âˆˆB} ,
and we conclude that Ï•1 (X1) and Ï•2 (X2) are stochastically independent.
2) The answer is â€˜noâ€™! It suï¬ƒces to give an example. Let
Ï•1 (X1) = c1
and
Ï•2 (X2) = c2
be constant maps. Then
P {Ï•1 (X1) âˆˆA âˆ§Ï•2 (X2) âˆˆB} =
â§
â¨
â©
1,
if c1 âˆˆA and c2 âˆˆB,
0,
otherwise,
â«
â¬
â­
= P {c1 âˆˆA} Â· P {c2 âˆˆB} = P {Ï•1 (X1) âˆˆA} Â· P {Ï•2 (X2) âˆˆB} ,
proving that Ï•1 (X1) and Ï• (X2) are independent, no matter if X1 and X2 are independent or not.
Download free eBooks at bookboon.com

Random variables I
 
78 
5. Functions and random variables, in general
Example 5.2 A discrete random variable (X, Y ) has its distribution given by the following table
Y \ X
1
2
3
1
1
12
1
6
1
12
2
1
6
1
4
1
12
3
1
12
1
12
0
Find the marginal distributions of X and Y .
Compute P{X Â· Y is even}.
Compute P{X â‰¥Y }.
Are X and Y independent?
1) All probabilities are â‰¥0, and their sum is 1, so the table describes a distribution.
Y \ X
1
2
3
fY
1
1
12
1
6
1
12
1
3
2
1
6
1
4
1
12
1
2
3
1
12
1
12
0
1
6
fX
1
3
1
2
1
6
1
We conclude from the table that the marginal distributions are
P{X = 1} = 1
3,
P{X = 2} = 1
2,
P{X = 3} = 1
6,
and
P{Y = 1} = 1
3,
P{Y = 2} = 1
2,
P{Y = 3} = 1
6.
2) By a counting of the table we get
P{X Â· Y is even}
=
P{X = 2} + P{X = 1 âˆ§Y = 2} + P{X = 3 âˆ§Y = 2}
=
1
2 + 1
6 + 1
2 = 3
4.
3) We get in the same way
P{X â‰¥Y }
=
P{Y = 1} + P{Y = 2 âˆ§X = 2}
+P{Y = 2 âˆ§X = 3} + P{Y = 3 âˆ§X = 3}
=
1
3 + 1
4 + 1
12 + 0 = 8
12 = 2
3.
4) The random variables X and Y are not independent. We have e.g.
P{X = 3 âˆ§Y = 3} = 0
and
P{X = 3} Â· P{Y = 3} = 1
36 Ì¸= 0.
Download free eBooks at bookboon.com

Random variables I
 
79 
5. Functions and random variables, in general
Example 5.3 In a plane we draw two parallel lines â„“1 and â„“2 of the distance a. A needle of length
2b, where b < a, is thrown such that it falls randomly between the two lines in the following sense:
2*b
X
Y
l_1
l_2
a
The midpoint of the needle has the distance X from â„“1, where X is rectangularly distributed over ]0, a[,
and the needle forms an angle Y with the two parallel lines, where Y is rectangularly distributed over
]0, Ï€[. Finally, X and Y are independent.
1) Find a condition â€“ expressed by X, Y , b â€“ which describes that the needle intersects the line â„“1.
2) Prove that the probability that the needle intersects â„“1 is 2b
a Â· 1
Ï€ .
Remark 5.1 This example is called Buï¬€onâ€™s needle problem, and it can be traced back to 1777, when
Georges-Louis Leclerc, count of Buï¬€on, published a paper on the subject. If a needle is thrown at
random many times, then the fraction when the needle intersects â„“1, is approximately equal to 2b
a Â· 1
Ï€ .
If this fraction is denoted by f, then we have the approximation Ï€ â‰ˆ2b
a Â· 1
f . Since then many people
have tried to ï¬nd Ï€ in this way. In 1850 the astronomer Wolfe threw a needle 5000 times with the
values a = 45 mm, b = 36 mm. He obtained intersection in 2532 of the cases, hence f = 25332
5000 . This
gives the approximation 3.160 of Ï€, which is quite fair. In 1901 Lazzarini published a paper with a far
better approximation of Ï€. He used a = 3 cm, b = 2, 5 cm, the needle was thrown 3408 times, and he
obtained intersection 1808 times. In this case we get the approximation Ï€ â‰ˆ5
3 Â· 3408
1808 = 3.1415919 . . . ,
which is astonishingly in agreement with Ï€ = 3.1415926 . . . . However, Lazzarini was extremely lucky
in getting his paper published. Some mathematicians have later pointed out the fairly strange number
3408 of throws, and they also noted that Lazzariniâ€™s fraction can be reduced to 355
113, which long has
been known as one of the very best rational approximations of Ï€. Therefore, mathematicians of today
are convinced that the paper was a swindle.
Download free eBooks at bookboon.com

Random variables I
 
80 
5. Functions and random variables, in general
1) It follows by the geometry that the needle intersects â„“1, if X â‰¤b Â· sin Y .
2) We shall ï¬nd P{X â‰¤b Â· sin Y }. Since X is rectangularly distributed over ]0, a[, and Y is rectan-
gularly distributed over ]0, Ï€[, we get
fX(x) =
â§
âª
â¨
âª
â©
1
a,
for x âˆˆ]0, a[,
0,
otherwise,
fY (y) =
â§
âª
â¨
âª
â©
1
Ï€ ,
for y âˆˆ]0, Ï€[,
0,
otherwise.
Then
P{b sin Y > X} =
1
Ï€ a
 Ï€
y=0
 b sin y
x=0
1 dx

dy = 1
Ï€ a
 Ï€
y=0
b sin y dy = 2b
a Â· 1
Ï€ .
Alternatively,
P{X â‰¤b Â· sin Y } = P{X âˆ’b Â· sin Y â‰¤0},
so we can instead ï¬nd the distribution function of Z = X âˆ’b Â· sin Y .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables I
 
81 
5. Functions and random variables, in general
We shall, however, ï¬rst ï¬nd the distribution function G(y) of âˆ’b Â· sin Y . This is given by
G(y) = P{âˆ’b Â· sin Y â‰¤y} = P

sin Y â‰¥âˆ’y
b
 
.
If y â‰¥0, then G(y) = 1, and if y â‰¤âˆ’b, then G(y) = 0. Finally, if y âˆˆ] âˆ’b, 0[, then
G(y)
=
P

Arcsin

âˆ’y
b

â‰¤Y â‰¤Ï€ âˆ’Arcsin

âˆ’y
b
 
=
1
Ï€

Ï€ âˆ’2 Arcsin

âˆ’y
b
 
= 1 + 2
Ï€ Arcsin
y
b

,
hence the frequency is
g(y) = Gâ€²(y) = 2
Ï€ Â·
1

1 âˆ’
y
b
 2 Â· 1
b = 2
Ï€b
1

1 âˆ’
y
b
 2 ,
y âˆˆ] âˆ’b, 0[,
and = 0 otherwise.
Since X and Y , and hence also X and âˆ’b sin Y are independent, we conclude that Z = X âˆ’b sin Y
has the frequency
h(s) =
 âˆ
âˆ’âˆ
fX(s âˆ’x) g(x) dx,
s âˆˆR.
Thus, if b < a,
P{X â‰¤b Â· sin Y }
=
P{Z â‰¤0} =
 0
âˆ’âˆ
h(s) ds =
 0
âˆ’âˆ
 âˆ
âˆ’âˆ
fX(s âˆ’x) g(x) dx

ds
=
 âˆ
âˆ’âˆ
g(x)
 0
âˆ’âˆ
fX(s âˆ’x) ds

dx =
 âˆ
âˆ’âˆ
g(x)
 âˆ’x
âˆ’âˆ
fX(s) ds

dx
=
2
Ï€b
 0
âˆ’b
1

1 âˆ’
x
b
 2
 âˆ’x
âˆ’âˆ
1
a Ï‡[0,a](s) ds

dx
=
2
Ï€ab
 b
0
1

1 âˆ’
x
b
 2
 x
0
Ï‡[0,a](s) ds

dx,
where Ï‡[0,a](s) = 1, if s âˆˆ[0, a], and = 0 otherwise. Now,
 a
0
Ï‡[0,a](s) ds =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
a
for x â‰¥a,
x,
for x âˆˆ[0, a],
0,
for x < 0,
so we get for b < a,
P{X â‰¤b Â· sin Y }
=
2
Ï€ab
 b
0
x

1 âˆ’
x
b
 2 dx = 2b2
Ï€ab
 1
0
y
%
1 âˆ’y2 dy
=
2b
Ï€a
'
âˆ’
%
1 âˆ’y2
(1
0 = 2b
a Â· 1
Ï€ ,
Download free eBooks at bookboon.com

Random variables I
 
82 
5. Functions and random variables, in general
which is the searched result.
Remark 5.2 If the needle is thrown a great number of times, then the relative frequency f that it
intersects â„“1 will approximately be 2b
a Â· 1
Ï€ , so we conclude that
Ï€ âˆ¼2b
a Â· 1
f .
This formula has earlier been used in the attempt of experimentally to ï¬nd Ï€, however, without great
success. The results have either been too poor, or one has cheated (like e.g. Lazzarini). â™¦
Remark 5.3 One can also go through this example without the assumption that b < a; but in this
case the computations become really tough, because the curve x = b Â· sin y then intersects the curve
x = a. â™¦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables I
 
83 
6. Inequalities between two random variables
6
Inequalities between two random variables
Example 6.1 Two persons A and B have the intension of meeting between 8 AM and 9 AM. Both A
and B arrive at the meeting place at a randomly chosen time between 8 AM and 9 AM. Furthermore,
they have agreed that none of them will wait in more than 10 minutes. Find the probability that they
meet.
If instead, A and B have agreed that A will wait 15 minutes for B, while B will wait 5 minutes for
A, what is then the probability that they meet?
Hint. The arrival times of A and B are rectangularly distributed.
Let X be the arrival time of A, and let Y be the arrival time of B. Then X and Y are indepen-
dent random variables, which are both rectangularly distributed over an interval of length 1 hour,
represented by the interval ]0, 1[.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 28: The domain where the simultaneous frequency is 1.
The simultaneous frequency is
f(x, y) =
â§
â¨
â©
1,
for (x, y) âˆˆ]0, 1[ Ã— ]0, 1[,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 29: The domain C is the diagonal strip.
Download free eBooks at bookboon.com

Random variables I
 
84 
6. Inequalities between two random variables
1) Since 10 minutes = 1
6 hour, the event corresponds to |X âˆ’Y | < 1
6. The probability is equal to
the area of C, hence
P

|X âˆ’Y | < 1
6

= 1 âˆ’2 Â· 1
2
5
6
2
= 11
36 = 0.306.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 30: The domain D is the translated diagonal strip.
2) The event corresponds to âˆ’1
12 < Y âˆ’X < 1
4. The probability is equal to the area of D on the
ï¬gure,
1 âˆ’1
2
3
4
2
âˆ’1
2
11
12
2
= 1 âˆ’101
144 = 43
144 = 0.299.
Alternatively, (1) is solved in the following way:
P{meeting}
= P{A arrives ï¬rst between 800 and 850, and B at most 10 min. later}
+P{B arrives ï¬rst between 800 and 850, and A at most 10 min. later}
+P{A and B both arrive between 850 and 900}
= 5
6 Â· 1
6 + 5
6 Â· 1
6 + 1
6 Â· 1
6 = 11
36.
Alternatively, we ï¬nd the frequency h(z) of Z = X âˆ’Y . Since f(x, y) = 1[0,1]2(x, y), we get
h(z) =
 âˆ
âˆ’âˆ
f(x, x âˆ’z) dx =
 1
0
1[0,1]2(x, x âˆ’z) dx =
 1
0
1[0,1](x âˆ’z) dx,
The integrand is only Ì¸= 0, if x âˆˆ]0, 1[ and x âˆ’z âˆˆ]0, 1[, i.e. x âˆˆ]z, z + 1[, thus for z âˆˆ] âˆ’1, 1[, cf. the
ï¬gure.
(i) For z âˆˆ] âˆ’1, 0[ fËšas h(z) =
 z+1
0
dx = z + 1.
(ii) For z âˆˆ]0, 1[ fËšas h(z) =
 1
z dx = 1 âˆ’z.
(iii) If z /âˆˆ] âˆ’1, 0[, then h(z) = 0.
Download free eBooks at bookboon.com

Random variables I
 
85 
6. Inequalities between two random variables
â€“1
â€“0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 31: The domain of integration for h(z).
Then the task can be treated in the following way:
1)
P

|X âˆ’Y | < 1
6

= P

âˆ’1
6 < Z < 1
6

=

1
6
âˆ’1
6
h(z) dz
=
 0
âˆ’1
6
(z + 1) dz +

1
6
0
(1 âˆ’z) dz =
1
2 (z + 1)2
0
âˆ’1
6
âˆ’
1
2 (1 âˆ’z)2
 1
6
0
= 1
2

1 âˆ’
5
6
2
âˆ’1
2
5
6
2
âˆ’1

= 1 âˆ’
5
6
2
= 11
36.
2)
P

âˆ’1
12 < Z < 1
4

=
 0
âˆ’1
12
(z + 1) dz +

1
4
0
(1 âˆ’z) dz =
1
2 (z + 1)2
0
âˆ’1
12
âˆ’
1
2 (1 âˆ’z)2
 1
4
0
= 1
2

1 âˆ’
11
12
2
+ 1
2

1 âˆ’
3
4
2
= 1 âˆ’1
2
11
12
2
+
3
4
2
= 43
144.
Download free eBooks at bookboon.com

Random variables I
 
86 
6. Inequalities between two random variables
Example 6.2 Henry and John arrive independently of each other to a meeting point. Both Henry
and John arrive at randomly chosen times between 8 AM and 9 AM.
1) Find the probability that Henry arrives as the ï¬rst one.
2) Find the probability that John arrives more than 10 minutes after Henry.
3) Find the probability that the diï¬€erence between their arrival times is at most 5 minutes.
Hint. The arrival times of Henry and John are rectangularly distributed.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 32: The domain where the simultaneous distribution function is 1.
Let the random variable X denote Henryâ€™s arrival time, and let the random variable Y denote Johnâ€™s
arrival time. Since X and Y are independent and rectangularly distributed over e.g. ]0, 1[, the simul-
taneous frequency is
f(x, y) =
â§
â¨
â©
1,
(x, y) âˆˆ]0, 1[ Ã— ]0, 1[,
0,
otherwise,
and

A
f(x, y)f(x, y) dx dy = area(A),
for A â«…]0, 1[ Ã— ]0, 1[.
1) By an area consideration we get
P{Henry arrives ï¬rst} = P{X < Y } = the area of the upper triangle = 1
2.
2) Since 10 minutes = 1
6 hour, we get in the same way
P{John arrives more that 10 min. after Henry}
= P

Y > X + 1
6

= area of the upper triangle = 1
2
5
6
2
= 25
72.
Download free eBooks at bookboon.com

Random variables I
 
87 
6. Inequalities between two random variables
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 33: The domain given by X < Y is the upper triangle.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 34: The domain given by Y > X + 1
6 is the upper triangle.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 35: The domain where the diï¬€erence is at most 5 minutes is represented by the domain around
the diagonal.
Download free eBooks at bookboon.com

Random variables I
 
88 
6. Inequalities between two random variables
3) Since 5 minutes = 1
12 hour, we get the condition âˆ’1
12 < X âˆ’Y < 1
12, and the probability is again
obtained by an area consideration,
P{the diï¬€erence is at most 5 min.} = P

|X âˆ’Y | < 1
12

= area of the diagonal strip = 1 âˆ’2 Â· 1
2
11
12
2
= 23
144.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables I
 
89 
6. Inequalities between two random variables
Example 6.3 Two persons A and B arrive at a meeting point between 7 AM and 8 AM. Their
arrivals are independent of each other, and they both have a tendency of arriving at the end of the
interval which for convenience is put equal to ]0, 1[. (We adjust the time at 7 AM).
The arrival time of A is denoted by X, and we assume that its frequency is
f(x) =

2x,
0 < x < 1,
0,
otherwise,
while the arrival time of B is denoted by Y , and is also assumed to have the frequency
g(y) =

2y,
0 < x < 1,
0,
otherwise,
A will at most wait 20 minutes for B, while B is a very impatient person who does not want to wait
at all.
Find the probability that the two persons meet.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 36: The domain of integration is the diagonal strip.
Since X and Y are independent, the frequency of the 2-dimensional random variable (X, Y ) is given
by
h(x, y) = f(x)g(y) =
â§
â¨
â©
4xy,
0 < x, y < 1,
0,
otherwise,
Since 20 minutes = 1
3 hour, the task is to ï¬nd P

X â‰¤Y â‰¤X + 1
3

, i.e. the integral of h(x, y) over
the diagonal strip on the ï¬gure.
The domain of integration is split at x = 2
3. Then by ï¬rst integrating vertically (the inner integral,
Download free eBooks at bookboon.com

Random variables I
 
90 
6. Inequalities between two random variables
so x is kept ï¬xed),
P

X â‰¤Y â‰¤x + 1
3

=

2
3
0
 x+ 1
3
x
4xy dy

dx +
 1
2
3
 1
x
4xy dy

dx
=

2
3
0
2x

x + 1
3
2
âˆ’x2

dx +
 1
2
3

2x âˆ’2x3
dx
=

2
3
0
4
3 x2 + 2
9 x

dx +

x2 âˆ’1
2 x4
1
2
3
=
4
9 x3 + 1
9 x2
 2
3
0
+

1 âˆ’1
2

âˆ’
4
9 âˆ’8
81

= 4
9 Â· 8
27 + 1
9 Â· 4
9 + 1
2 âˆ’36 âˆ’8
81
= 32
243 + 12
243 + 1
2 âˆ’28
81
= 44
243 + 1
2 âˆ’84
243 = 1
2 âˆ’40
243 = 243 âˆ’80
486
= 163
486 â‰ˆ0, 335.
Example 6.4 According to their schedules, 2 trains A and B shall arrive to a station at the same
time on each their line. Train A shall stay at the station for 5 minutes, and train B for 4 minutes.
However, the trains are very often delayed up to 20 minutes, so we assume that the arrival time of
train A is rectangularly distributed over [0, 20] (measured in minutes), and the arrival time of train B
is also rectangularly distributed over [0, 20]. The delay time is counted from the planned arrival time.
1) Find the probability that train A arrives before train B.
2) Find the probability that the two trains meet at the station.
3) Find the probability that train A arrives before train B and departs after train B.
Since the arrival time X of train A and the arrival time Y of train B are independent and both
rectangularly distributed over [0, 20], the simultaneous frequency of (X, Y ) is
f(x, y) =
â§
âª
â¨
âª
â©
1
400,
for 0 < x, y < 20,
0,
otherwise.
1) It follows by an area consideration of weight
1
400 that
P{X < Y } = 1
2.
2) If the two trains meet at the station, then X âˆ’4 < Y < X + 5, so (X, Y ) lies in the diagonal strip.
Then by an area consideration,
P{X âˆ’4 < Y < X + 5}
=
1
400

202 âˆ’1
2 Â· 152 âˆ’1
2 Â· 162

=
1
400

400 âˆ’225
2
âˆ’256
2

=
1 âˆ’481
800 = 319
800.
Download free eBooks at bookboon.com

Random variables I
 
91 
6. Inequalities between two random variables
0
5
10
15
20
5
10
15
20
Figure 37: The event X < Y is represented by the upper triangle.
0
5
10
15
20
5
10
15
20
Figure 38: The event X âˆ’4 < Y < X + 5 is represented by the diagonal strip.
0
5
10
15
20
5
10
15
20
Figure 39: The event X < Y < X + 1 is represented by the diagonal strip.
3) If A arrives before B, i.e. X < Y , and departs after B, i.e. X + 5 > Y + 4, then X < Y < X + 1.
The probability can again be found by an area consideration,
P{X < Y < X + 1}
=
1
2 âˆ’1
2 Â·
19
20
2
= 1
2

12 âˆ’
19
20
2
=
1
2

1 + 19
20
 
1 âˆ’19
20

= 1
2 Â· 39
20
1
20 = 39
800.
Download free eBooks at bookboon.com

Random variables I
 
92 
6. Inequalities between two random variables
Example 6.5 Henry and Peter throw dices. Every minute (t = 1, 2, 3, . . . ) Henry throws a dice
(probability
1
6 of getting a six), and Peter also throws another dice every minute.
We deï¬ne the
random variables X and Y by
X = k, if Henry obtains his ï¬rst six in throw number k,
Y = k, if Peter obtains his ï¬rst six in throw number k.
1. Find P{X = k}, k âˆˆN, and ï¬nd the mean E{X}.
2. Find for every k âˆˆN the probability P{X = k âˆ§Y = k}, and then ï¬nd P{X = Y }.
3, Compute the two probabilities P{X < Y } and P{Y < X}.
4. We deï¬ne a random variable Z by
Z = k, if Henry obtains his second six in throw number k.
Find P{Z = k}, k = 2, 3, 4, . . . .
5. Find for k = 2, 3, 4, . . . ,
P{Z = k âˆ§Y > k},
and then ï¬nd the probability that Henry gets at least two sixes before Peter obtains his ï¬rst six.
1) Since X (and also Y ) is geometric distributed with p = 1
6, we get
P{X = k} = 1
6
5
6
kâˆ’1
,
k âˆˆN,
and
E{X} = 6.
2) Since X and Y are independent, we get for k âˆˆN that
P{X = k âˆ§Y = k} = P{X = k} Â· P{Y = k} =

1
6
5
6
kâˆ’12
= 1
36
25
36
kâˆ’1
.
Then by a summation,
P{X = Y } =
âˆ

k=1
P{X = k âˆ§Y = k} = 1
36
âˆ

k=1
25
36
kâˆ’1
= 1
36 Â·
1
1 âˆ’25
36
= 1
11.
3) Clearly,
P{X < Y } + P{Y < X} + P{X = Y } = 1.
It follows from the symmetry that P{X < Y } = P{Y < X}, so
P{X < Y } = P{Y < X} = 1
2 (1 âˆ’P{X = Y }) = 1
2

1 âˆ’1
11

= 5
11.
Download free eBooks at bookboon.com

Random variables I
 
93 
6. Inequalities between two random variables
Alternatively,
P{X < Y }
=
âˆ

k=1
P{X = k âˆ§Y > k} =
âˆ

k=1
P{X = k} Â· P{Y > k}
=
âˆ

k=1
1
6
5
6
kâˆ’1
Â·
5
6
k
= 1
6 Â· 5
6
âˆ

k=1
25
36
kâˆ’1
= 5
36 Â· 36
11 = 5
11.
4) The random variable Z can be written Z = X1 + X2, where X1 and X2 are independent of the
same distribution as X. Then we get for k â‰¥2,
P{Z = k}
=
kâˆ’1

i=1
P {X1 = i} Â· P {Xk = k âˆ’i} =
kâˆ’1

i=1
1
6
5
6
iâˆ’1
Â· 1
6
5
6
kâˆ’iâˆ’1
=
1
6
kâˆ’1

i=1
5
6
kâˆ’2
= (k âˆ’1) Â· 1
36
5
6
kâˆ’2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables I
 
94 
6. Inequalities between two random variables
Alternatively, Z = k means that in the ï¬rst k âˆ’1 throws we have obtained precisely one six,
and that we in the k-th throw get another six, hence
P{Z = k} = (k âˆ’1) Â· 1
6
5
6
kâˆ’2
Â· 1
6 = (k âˆ’1) Â· 1
36
5
6
kâˆ’2
.
5) Here,
P{Z < Y }
=
âˆ

k=2
P{Z = k âˆ§Y > k} = 1
36
âˆ

k=2
(k âˆ’1)
25
36
kâˆ’2
Â· 25
36
=
1
36 Â· 25
36 Â·
1
11
36
2 = 25
121.
Download free eBooks at bookboon.com

Random variables I
 
95 
7. Functions Y = f(X) of random variables
7
Functions Y = f(X) of random variables
Example 7.1 Let X be rectangularly distributed over ]0, a[, where a > 0.
Find the distribution
function and the frequency of the random variable Y = X2 + X.
The frequency of X is
f(x) =
â§
âª
â¨
âª
â©
1
a,
0 < x < a,
0,
otherwise.
The function y = Ï„(x) = x2 + x maps ]0, a[ increasingly onto
$
0, a + a2#
. The inverse map is
x = Ï„ âˆ’1(y) = âˆ’1
2 +

1
4 + y,
where
	
Ï„ âˆ’1
â€² (y) =
1
2

1
4 + y
.
Then the frequency of Y is
g(y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
1
a Â·
1
2

1
4 + y
= 1
a Â·
1
âˆš1 + 4y ,
0 < y < a2 + a,
0,
otherwise,
and the distribution function G(y) =
 y
âˆ’âˆg(u) du is
G(y) =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
0,
y â‰¤0,
1
a Ï„ âˆ’1(y) = 1
a

1
4 + y âˆ’1
2

,
0 < y < a2 + a,
1,
y > a2 + a.
Alternatively, we get for y âˆˆ
$
0, a2 + a
#
,
G(y) = P{Y â‰¤y} = P

X â‰¤âˆ’1
2 +

1
4 + y

= 1
a

1
4 + y âˆ’1
2

,
hence in the same interval,
g(y) = Gâ€²(y) = 1
a Â·
1
2

1
4 + y
= 1
a Â·
1
âˆš1 + 4y .
Download free eBooks at bookboon.com

Random variables I
 
96 
7. Functions Y = f(X) of random variables
Example 7.2 A line segment of length 1 is randomly divided into two pieces of the lengths X and
1âˆ’X, where we assume that X is rectangularly distributed over the interval ]0, 1[. We form a rectangle
of edge lengths X and 1 âˆ’X. Find the probability that the area of this rectangle is bigger than 1
8.
The area of the rectangle is Y = X(1âˆ’X). We shall ï¬nd the probability that this expression is bigger
than 1
8.
Now,
x(1 âˆ’x) = âˆ’x2 + x > 1
8,
if and only if
1
2 âˆ’
âˆš
2
4
< x < 1
2 +
âˆš
2
4 .
Since X is rectangularly distributed, we get
P

1
2 âˆ’
âˆš
2
4
< X < 1
2 +
âˆš
2
4

=

1
2 +
âˆš
2
4
1
2 âˆ’
âˆš
2
4
1 dx =
âˆš
2
2 .
Remark 7.1 It is possible in general to ï¬nd the distribution function of
Y = f(X) = X(1 âˆ’X).
If 0 < y < 1
4, then
P{Y â‰¤y}
=
P

X â‰¤1
2 âˆ’

1
4 âˆ’y

âˆª

X â‰¥1
2 +

1
2 âˆ’y

=
1 âˆ’2

1
4 âˆ’y = 1 âˆ’
%
1 âˆ’4y,
so
FY (y) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
0,
y â‰¤0,
1 âˆ’âˆš1 âˆ’4y,
0 < y < 1
4,
1,
y â‰¥1
4,
and
fY (y) =
â§
âª
â¨
âª
â©
2y
âˆš1 âˆ’4y ,
0 < y < 1
4,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
97 
7. Functions Y = f(X) of random variables
Note that the probability of Y = X(1 âˆ’X) being bigger than y is
P{Y > y} = 1 âˆ’FY (y) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
1,
for y â‰¤0,
âˆš1 âˆ’4y,
for 0 < y < 1
4,
0,
for y â‰¥1
4.
If y = 1
8 we get
P

Y > 1
8

=

1 âˆ’1
2 =
âˆš
2
2 .
â™¦
Example 7.3 Let the random variable X be rectangularly distributed over the interval
(
âˆ’Ï€
2 , Ï€
2
'
. Find
the distribution functions and the frequencies of the random variables
Y = sin X,
Z = cos X,
U = tan X.
Since X is rectangularly distributed over
(
âˆ’Ï€
2 , Ï€
2
'
, the frequency is given by
fX(x) =
â§
âª
â¨
âª
â©
1
Ï€
for x âˆˆ
(
âˆ’Ï€
2 , Ï€
2
'
,
0
otherwise.
1) Y = sin X.
If |y| < 1, then
P{Y â‰¤y} = P{X â‰¤Arcsin y} = 1
Ï€

Arcsin y + Ï€
2
 
= 1
Ï€ Arcsin y + 1
2,
hence the distribution function of Y is
FY (y) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
y â‰¤âˆ’1,
1
Ï€ Arcsin y + 1
2,
âˆ’1 < y < 1,
1,
y â‰¥1,
and the frequency is then obtained by a diï¬€erentiation,
fY (y) =
â§
âª
âª
â¨
âª
âª
â©
1
Ï€
%
1 âˆ’y2 ,
âˆ’1 < y < 1,
0,
otherwise.
Alternatively, we ï¬rst ï¬nd the frequency fY (y) of Y . Since y = sin x maps
(
âˆ’Ï€
2 , Ï€
2
'
increasingly
onto ] âˆ’1, 1[, it has an inverse map,
x = Ï„ âˆ’1(y) = Arcsin y
with
	
Ï„ âˆ’1
â€² (y) =
1
%
1 âˆ’y2 .
Download free eBooks at bookboon.com

Random variables I
 
98 
7. Functions Y = f(X) of random variables
0
0.2
0.4
0.6
0.8
1
â€“1.5
â€“1
â€“0.5
0.5
1
1.5
Figure 40: The graph of z = cos x for x âˆˆ] âˆ’Ï€
2 , Ï€
2 [.
Since f
	
Ï„ âˆ’1(y)

= 1
Ï€ , we get that
fY (y) =
â§
âª
âª
â¨
âª
âª
â©
1
Ï€
%
1 âˆ’y2 ,
âˆ’1 < y < 1,
0,
otherwise.
and if âˆ’1 < y < 1, we get the distribution function
FY (y) = P{Y â‰¤y} =
 y
âˆ’1
1
Ï€
%
1 âˆ’y2 dy =
 1
Ï€ Arcsin y
y
âˆ’1
= 1
Ï€ Arcsin y + 1
2.
2) Z = cos X.
In this case, only z âˆˆ]0, 1[ is of interest. It follows by the symmetry â€“ cf. the ï¬gure â€“ that
P{Z â‰¤z}
=
2 P

Arccos z â‰¤X < Ï€
2
 
= 2 Â· 1
Ï€
Ï€
2 âˆ’Arccos z
 
=
1 âˆ’2
Ï€ Arccos z = 2
Ï€ Arcsin z.
Hence, the distribution function is
FZ(z) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
z â‰¤0,
2
Ï€ Arcsin z,
0 < z < 1,
1,
z â‰¥1,
from which we get the frequency by diï¬€erentiation
fZ(z) =
â§
âª
â¨
âª
â©
2
Ï€
âˆš
1 âˆ’z2 ,
0 < z < 1,
0,
otherwise.
Since z = cos x is not monotonous in
(
âˆ’Ï€
2 , Ï€
2
'
, we cannot apply the usual argument.
Download free eBooks at bookboon.com

Random variables I
 
99 
7. Functions Y = f(X) of random variables
3) U = tan X.
If u âˆˆR, then
P{U â‰¤u} = P{X â‰¤Arctan u} = 1
Ï€

Arctan u + Ï€
2
 
= 1
Ï€ Arctan u + 1
2,
hence
FU(u) = 1
Ï€ Arctan u + 1
2
og
fU(u) = F â€²
U(u) =
1
Ï€ (1 + u2),
u âˆˆR.
Alternatively, like in (1) it is possible to ï¬nd the frequency of U, because u = Ï„(x) = tan x
maps
(
âˆ’Ï€
2 , Ï€
2
'
increasingly onto R with the inverse map x = Ï„ âˆ’1(u) = Arctan(u), where
	
Ï„ âˆ’1
â€² (u) =
1
1 + u2 .
Then apply the standard formula.
The distribution of U is a Cauchy distribution.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
100 
7. Functions Y = f(X) of random variables
Example 7.4 Assume that the random variable X is rectangularly distributed over the interval ]0, Ï€[.
Find the distribution functions and the frequencies of the random variables
Y = 1
X ,
Z = cos X,
U = sin X.
When X is rectangularly distributed over ]0, Ï€[, then
fX(x) =
â§
âª
â¨
âª
â©
1
Ï€
for x âˆˆ]0, Ï€[,
0
otherwise,
and
FX(x) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0
for x â‰¤0,
x
Ï€
for x âˆˆ]0, Ï€[,
1
for x â‰¥Ï€.
0
0.5
1
1.5
2
2.5
0.5
1
1.5
2
2.5
3
Figure 41: The graph of y = 1
x in the interval ]0, Ï€[.
1) The image of ]0, Ï€[ by the map y = 1
x is
 1
Ï€ , âˆ

. If y > 1
Ï€ , then we get the distribution function
FY (y) = P{Y â‰¤y} = P
 1
X â‰¤y

= P

X â‰¥1
y

= 1 âˆ’P

X < 1
y

= 1 âˆ’1
Ï€y ,
hence
FY (y) =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1 âˆ’1
Ï€y
for y > 1
Ï€ ,
0
for y â‰¤1
Ï€ ,
and
fY (y) =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1
Ï€y2
for y > 1
Ï€ ,
0
for y â‰¤1
Ï€ .
2) The image of ]0, Ï€[ by z = cos x is ] âˆ’1, 1[. If z âˆˆ] âˆ’1, 1[, then we get the distribution function
FZ(z)
=
P{Z â‰¤z} = P{cos X â‰¤z} = P{X â‰¥Arccos z}
=
1 âˆ’P{X < Arccos z} = 1 âˆ’1
Ï€ Arccos z,
Download free eBooks at bookboon.com

Random variables I
 
101 
7. Functions Y = f(X) of random variables
â€“1
â€“0.5
0
0.5
1
0.5
1
1.5
2
2.5
3
Figure 42: The graph of z = cos x for x âˆˆ]0, Ï€[.
hence
FZ(z) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
z â‰¤âˆ’1,
1 âˆ’1
Ï€ Arccos z,
âˆ’1 < z < 1,
1,
z â‰¥1,
and
fZ(z) =
â§
âª
â¨
âª
â©
1
Ï€
1
âˆš
1 âˆ’z2 ,
âˆ’1 < z < 1,
0,
otherwise.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 43: The graph of u = sin x for x âˆˆ]0, Ï€[.
Download free eBooks at bookboon.com

Random variables I
 
102 
7. Functions Y = f(X) of random variables
3) The image of ]0, Ï€[ by u = sin x is ]0, 1[. If u âˆˆ]0, 1[, then we get the distribution function
FU(u)
=
P{U â‰¤u} = P{sin X â‰¤u}
=
P{X â‰¤Arcsin u} + P{X â‰¥Ï€ âˆ’Arcsin u}
=
P{X â‰¤Arcsin u} + 1 âˆ’P{X < Ï€ âˆ’Arcsin u}
=
1 + 1
Ï€ Arcsin u âˆ’1
Ï€ {Ï€ âˆ’Arcsin u} = 2
Ï€ Arcsin u,
hence
FU(u) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
u â‰¤0,
2
Ï€ Arcsin u,
0 < u < 1,
1,
u â‰¥1,
and
fU(u) =
â§
âª
â¨
âª
â©
2
Ï€
1
âˆš
1 âˆ’u2 ,
0 < u < 1,
0,
otherwise.
Alternatively, we may apply the usual formula in (1) and (2), but not in (3), because sinus is not
a bijective onto the given interval.
If t(x) is a bijective transformation, and x = x(t) is the inverse, then we have in the form of diï¬€erentials,
fX(x) dx = fX(x(t))

dx
dt
 dt = fT (t) dt.
Hence, we shall always ï¬nd the inverse map x = x(t).
1) If x âˆˆ]0, Ï€[, then y = 1
x âˆˆ
 1
Ï€ , âˆ

. The inverse map is given by x = 1
y , thus
dx
dy = âˆ’1
y2 < 0,
and we get for y > 1
Ï€ that
fY (y) = fX
1
y

Â·
âˆ’1
y2
 = 1
Ï€ Â· 1
y2 ,
hence by an integration for y > 1
Ï€ ,
FY (y) =
 y
1
Ï€
1
Ï€ Â· 1
Î·2 dÎ· = 1
Ï€

âˆ’1
Î·
y
1
Ï€
= 1 âˆ’1
Ï€y .
100
Download free eBooks at bookboon.com

Random variables I
 
103 
7. Functions Y = f(X) of random variables
2) If x âˆˆ]0, Ï€[, then z = cos x âˆˆ] âˆ’1, 1[ bijectively. The inverse is given by x = Arccos z, thus
dx
dz = âˆ’
1
âˆš
1 âˆ’z2 < 0.
If z âˆˆ] âˆ’1, 1[, then
fZ(z) = fX(Arccos z) Â·
âˆ’
1
âˆš
1 âˆ’z2
 =
1
Ï€
âˆš
1 âˆ’z2 ,
so if z âˆˆ] âˆ’1, 1[, then we get by an integration that
FZ(z) =
 z
âˆ’1
1
Ï€ Â·
1
%
1 âˆ’Î¶2 dÎ¶ = 1
Ï€ [âˆ’Arccos Î¶]z
âˆ’1 = 1
Ï€ {Ï€ âˆ’Arccos z} = 1 âˆ’1
Ï€ Arccos z.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables I
 
104 
7. Functions Y = f(X) of random variables
Example 7.5 Assume that the random variable X has the frequency
f(x) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
3
2Ï€ ,
0 < x < Ï€
2 ,
1
2Ï€ ,
Ï€
2 < x < Ï€,
0,
otherwise.
Find the distribution function of X.
Then ï¬nd the distribution functions and the frequencies of each of the random variables
Y = 1
X ,
Z = sin X.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 44: The frequency f(x).
0
0.2
0.4
0.6
0.8
1
1
2
3
4
x
Download free eBooks at bookboon.com

Random variables I
 
105 
7. Functions Y = f(X) of random variables
1) If x â‰¤0, then F(x) = 0.
If 0 < x â‰¤Ï€
2 , then F(x) = 3x
2Ï€ .
In particular, F
Ï€
2

= 3
4, hence for Ï€
2 < x < Ï€,
F(x) = F
Ï€
2

+
 x
Ï€
2
1
2Ï€ dx = 3
4 + 1
2Ï€

x âˆ’Ï€
2
 
= 1
2 + x
2Ï€ .
If x â‰¥Ï€, then F(x) = 1.
Summing up we get the distribution function
F(x) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
0,
x â‰¤0,
3x
2Ï€ ,
0 < x â‰¤Ï€
2 ,
1
2 + x
2Ï€ ,
Ï€
2 < x < Ï€,
1,
x â‰¥Ï€.
2) The interval ]0, Ï€[ is by y = 1
x mapped bijectively onto
 1
Ï€ , âˆ

.
1st variant. If y > 1
Ï€ , then we get the distribution function
FY (y)
=
P{Y â‰¤y} = P
 1
x â‰¤y

= P

X â‰¥1
y

= 1 âˆ’P

X < 1
y

=
â§
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
â©
1 âˆ’
3
2Ï€y
for y > 2
Ï€ ,
1
2 âˆ’
1
2Ï€y ,
for 1
Ï€ < y â‰¤2
Ï€ ,
0,
for y â‰¤1
Ï€ .
The frequency is obtained by a diï¬€erentiation,
fY (y) =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
3
2Ï€y2 ,
for y > 2
Ï€ ,
1
2Ï€y2 ,
for 1
Ï€ < y < 2
Ï€ ,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
106 
7. Functions Y = f(X) of random variables
2nd variant. Since x = 1
y and dx
dy = âˆ’1
y2 , it follows that
fY (y) = FX
1
y

Â· 1
y2 =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
3
2Ï€y2
for y â‰¥2
Ï€ ,
1
2Ï€y2
for 1
Ï€ < y < 2
Ï€ ,
0
otherwise.
If y â‰¤1
Ï€ , then FY (y) = 0.
We get for 1
Ï€ â‰¤y â‰¤2
Ï€ that
FY (y) =
 y
1
Ï€
dÎ·
2Ï€Î·2 =

âˆ’1
2Ï€Î·
y
1
Ï€
= 1
2 âˆ’
1
2Ï€y .
If y > 2
Ï€ , then
FY (y)
=
FY
 2
Ï€

+
 y
2
Ï€
3
2Ï€Î·2 dÎ· = 1
2 âˆ’1
4 + 3
2Ï€

âˆ’1
Î·
y
2
Ï€
=
1
4 + 3
4 âˆ’
3
2Ï€y = 1 âˆ’
3
2Ï€y .
3) The function z = sin x is not bijective, so we cannot apply the usual theorem. Since the image of
]0, Ï€[ by z = sin x is ]0, 1[, we get for z âˆˆ]0, 1[ that
FZ(z)
=
P{Z â‰¤z} = P{sin X â‰¤z}
=
P{X â‰¤Arcsin z} + P{X â‰¥Ï€ âˆ’Arcsin z}
=
1 + P{X â‰¤Arcsin z} âˆ’P{X < Ï€ âˆ’Arcsin z}.
Since Arcsin z âˆˆ
(
0, Ï€
2
'
, we must have Ï€âˆ’Arcsin z âˆˆ
(Ï€
2 , Ï€
'
. Then it follows from FX(x) =
P{X â‰¤x}, found in (1) that
FZ(z) = 1 + 3
2Ï€ Arcsin z âˆ’
1
2 + 1
2Ï€ (Ï€ âˆ’Arcsin z)

= 2
Ï€ Arcsin z,
thus
FZ(z) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0,
for z â‰¤0,
2
Ï€ Arcsin z,
for 0 < z < 1,
1,
for z â‰¥1,
and hence
fZ(z) =
â§
âª
â¨
âª
â©
1
Ï€ Â·
1
âˆš
1 âˆ’z2 ,
for 0 < z < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
107 
7. Functions Y = f(X) of random variables
Example 7.6 The random variable X has the frequency
f(x) =
â§
â¨
â©
eâˆ’x,
x > 0,
0,
x â‰¤0.
Find the frequencies of the random variables
Y = sinh X,
Z = cosh X.
We ï¬rst note that sinh : R+ â†’R+ and cosh : R+ â†’]1, +âˆ[ are bijective and monotonous.
1) Y = sinh X. In this case, y = Ï„(x) = sinh x, x âˆˆR+, hence
x = Ï„ âˆ’1(y) = Arsinh y = ln

y +
%
1 + y2

,
y > 0,
and
dx
dy =
	
Ï„ âˆ’1(y)

â€² =
1
%
1 + y2 ,
y > 0.
It follows from the usual theorem that the frequency is given for y > 0 by
g(y) = f
	
Ï„ âˆ’1(y)

Â·

	
Ï„ âˆ’1
â€² (y)
 = e
ln
â€œ
y+âˆš
1+y2
â€
Â·
1
%
1 + y2 =
1
y +
%
1 + y2 Â·
1
%
1 + y2 ,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables I
 
108 
7. Functions Y = f(X) of random variables
thus
g(y) =
â§
âª
âª
â¨
âª
âª
â©
1
y +
%
1 + y2 Â·
1
%
1 + y2 ,
for y > 0,
0,
otherwise.
2) Z = cosh X. In this case, z = Ï„(x) = cosh x, x âˆˆR+, thus
x = Ï„ âˆ’1(z) = Arcosh z = ln

z +
%
z2 âˆ’1

,
z > 1,
and
dx
dz =
	
Ï„ âˆ’1(z)

â€² =
1
âˆš
z2 âˆ’1
.
z > 1.
Applying the theorem we get for z > 1 the frequency
h(z) = f
	
Ï„ âˆ’1(z)

Â·

	
Ï„ âˆ’1
â€² (z)
 = eâˆ’ln(z+
âˆš
z2âˆ’1) Â·
1
âˆš
z2 âˆ’1
=
1
z +
âˆš
z2 âˆ’1
Â·
1
âˆš
z2 âˆ’1
,
hence
h(z) =
â§
âª
â¨
âª
â©
1
z +
âˆš
z2 âˆ’1
Â·
1
âˆš
z2 âˆ’1
,
for z > 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
109 
8. Functions of two random variables, f(X.Y)
8
Functions of two random variables, f(X, Y )
Example 8.1 1) Let X and Y be independent random variables with their frequencies
fX(x) =
k
Ï€ (k2 + x2),
x âˆˆR,
fY (y) =
1
Ï€ (1 + y2),
t âˆˆR,
where k denotes some positive constant. Prove that X + Y has the frequency
g(x) =
k + 1
Ï€ {(k + 1)2 + x2},
x âˆˆR.
2) Let X1 and X2 be independent random variables of frequencies
f1(x1) =
a1
Ï€ (a2
1 + x2
1),
x1 âˆˆR,
f2(x2) =
a2
Ï€ (a2
2 + x2
2),
x2 âˆˆR,
where a1 and a2 denote positive constants. Find by using the result of (1) the frequency of X1+X2.
3) Let Y1 and Y2 be independent random variables of the frequencies
g1(y1) =
a1
Ï€

a2
1 + (y1 âˆ’b1)2 ,
y1 âˆˆR,
g2(y2) =
a2
Ï€

a2
2 + (y2 âˆ’b2)2 ,
y2 âˆˆR,
where a1, a2 âˆˆR+ and b1, b2 âˆˆR. Find by using the result of (2) the frequency of Y1 + Y2.
1) (The hard question). The frequency g(x) of X + Y is given by the convolution
g(x) = 1
Ï€2
 âˆ
âˆ’âˆ
k
k2 + t2 Â·
1
1 + (t âˆ’x)2 dt.
A decomposition gives us the structure
(1)
k
k2 + t2 Â·
1
1 + (t âˆ’x)2 = a + bt
k2 + t2 + c + d(t âˆ’x)
1 + (t âˆ’x)2 ,
which has the integral
a
k Arctan
 t
k

+ c Arctan(t âˆ’x) + b
2 ln
	
k2 + t2
+ d
2 ln

1 + (t âˆ’x)2
.
The integral is clearly convergent, so d = âˆ’b, and the logarithmic terms disappear by taking the
limit.
We conclude that
g(x) = 1
Ï€
a
k + c
 
,
where a = a(k, x) and c = c(k, x) depend on both k and x.
Download free eBooks at bookboon.com

Random variables I
 
110 
8. Functions of two random variables, f(X.Y)
If we put d = âˆ’b into (1), then
k
k2 + t2 Â·
1
1 + x2 âˆ’2xt + t2 = a + bt
k2 + t2 +
bx + c âˆ’bt
1 + x2 âˆ’2xt + t2 ,
hence
(2) k = (a + bt)
	
1 + x2 âˆ’2x Â· t + t2
+
	
k2 + t2
(bx + c âˆ’bt).
The constant term of this equation is
k =
	
1 + x2
a + k2x Â· b + k2c,
thus
	
1 + x2
 a
k + kx Â· b + k Â· c = 1.
Since we want to ï¬nd a
k + c, we rewrite this as
(3)
	
1 + x2
 a
k + c
 
+ kx Â· b +
	
k âˆ’1 âˆ’x2
c = 1.
The coeï¬ƒcient of t i (2) gives the equation
âˆ’2xa +
	
1 + x2
b âˆ’k2b = 0,
which is rewritten as
(4) âˆ’2kx
a
k + c
 
+
	
1 + x2 âˆ’k2
b + 2kx Â· c = 0.
The coeï¬ƒcient of t2 in (2) implies the equation
a âˆ’xb + c = 0,
which is rewritten in the following way (cf. the above)
(5) k
a
k + c
 
âˆ’x Â· b + (1 âˆ’k) Â· c = 0.
Summing up we obtain the following linear system of the three unknowns a
k + c, b and c,
â§
âª
âª
âª
â¨
âª
âª
âª
â©
	
1 + x2
 a
k + c
 
+
kx Â· b
=
	
k âˆ’1 âˆ’x2
c
=
1,
âˆ’2kx
a
k + c
 
+
	
1 + x2 âˆ’k2
b
+
2kx Â· c
=
0,
k
a
k + c
 
âˆ’
x Â· b
+
(1 âˆ’k)c
=
0,
Then by Cramerâ€™s formula,
a
k + c =

1
kx
k âˆ’1 âˆ’x2
0
1 + x2 âˆ’k2
2kx
0
âˆ’x
1 âˆ’k


1 + x2
kx
k âˆ’1 âˆ’x2
âˆ’2kx
1 + x2 âˆ’k2
2kx
k
âˆ’x
1 âˆ’k

.
Download free eBooks at bookboon.com

Random variables I
 
111 
8. Functions of two random variables, f(X.Y)
First compute the numerator

1 + x2 âˆ’k2
2kx
âˆ’x
1 âˆ’k

=
(1 âˆ’k)
	
1 + x2
âˆ’(1 âˆ’k)k2 + 2kx2
=
1 + x2 âˆ’k âˆ’kx2 + 2kx2 âˆ’k2 + k3
=
x2(1 + k) + (1 âˆ’k)
	
1 âˆ’k2
=
(k + 1)

x2 + (k âˆ’1)2
.
When the third column is replaced by the sum of the ï¬rst and the third column we see that the
denominator is reduced to

1 + x2
kx
k
âˆ’2kx
1 + x2 âˆ’k2
0
k
âˆ’x
1

=

1 + x2 âˆ’k2
2kx
0
âˆ’2kx
1 + x2 âˆ’k2
0
k
âˆ’x
1

=

1 + x2 âˆ’k2
2kx
âˆ’2kx
1 + x2 âˆ’k2
 =
	
1 + x2 âˆ’k2
2 + 4k2x2
=
x4 + x2 	
2 âˆ’2k2 + 4k2
+ k4 âˆ’2k2 + 1
=
x2 
x2 + (k âˆ’1)2
+ x2 	
2k2 + 2 âˆ’k2 + 2k âˆ’1

+
	
k2 âˆ’1

2
=
x2 
x2 + (k âˆ’1)2
+ x2(k + 1)2 + (k + 1)2(k âˆ’1)2
=
x2 
x2 + (k âˆ’1)2
+ (k + 1)2 
x2 + (k âˆ’1)2
=

x2 + (k âˆ’1)2 
x2 + (k + 1)2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
112 
8. Functions of two random variables, f(X.Y)
Hence, if (x, y) Ì¸= (0, 1),
a
k + c =
(k + 1)

x2 + (k âˆ’1)2
{x2 + (k + 1)2} {x2 + (k âˆ’1)2} =
k + 1
x2 + (k + 1)2 ,
which is extended by continuity to (x, k) = (0, 1).
Thus, the frequency is given by
g(x) = 1
Ï€
a
k + c
 
= 1
Ï€ Â·
k + 1
x2 + (k + 1)2
as required.
2) The frequency of X1 + X2 is
f(x)
=
a1a2
Ï€2
 âˆ
âˆ’âˆ
1
a2
1 + t2 Â·
1
a2
2 + (t âˆ’x)2 dt,
u = t
a1
,
=
a2
Ï€2
 âˆ
âˆ’âˆ
1
1 +
 t
a1
2 Â·
1
a2
2 + a2
1
 t
a1
âˆ’x
a1
2 d
 t
a1

=
1
Ï€2a1
 âˆ
âˆ’âˆ
1
1 + u2 Â·
a2
a1
a2
a1
2
+

u âˆ’x
a1
2 du
k = a2
a1
and (1),
=
1
Ï€a1
Â·
a2
a1
+ 1
a2
a1
+ 1
2
+
 x
a1
2 = 1
Ï€ Â·
a1 + a2
(a1 + a2)2 + x2 .
3) In this case we get the frequency
g(y)
=
a1a2
Ï€2
 âˆ
âˆ’âˆ
1
a2
1 + (t âˆ’b1)2 Â·
1
a2
2 + (y âˆ’t âˆ’b2)2 dt
u = t âˆ’b1,
=
a1a2
Ï€2
 âˆ
âˆ’âˆ
1
a2
1 + u2 Â·
1
a2
2 + (y âˆ’u âˆ’b1 âˆ’b2)2 du
=
a1a2
Ï€2
 âˆ
âˆ’âˆ
1
a2
1 + u2 Â·
1
a2
2 + (u âˆ’{y âˆ’b1 âˆ’b2})2 du
=
1
Ï€ Â·
a1 + a2
(a1 + a2)2 + (y âˆ’{b1 + b2})2 ,
where we have applied (2).
Download free eBooks at bookboon.com

Random variables I
 
113 
8. Functions of two random variables, f(X.Y)
Example 8.2 Let X and Y be independent Cauchy distributed random variable of the frequencies
fX(x) =
1
Ï€ (1 + x2),
x âˆˆR,
fY (y) =
1
Ï€ (1 + y2),
y âˆˆR.
Prove that the random variable Z = XY has the frequency
fZ(z) = 2
Ï€2 Â· ln |z|
z2 âˆ’1,
z âˆˆR,
(suitably modiï¬ed for z = âˆ’1, 0, 1).
Hint: One may apply that
	
z2 âˆ’1

Â·
1
1 + z2 Â·
1
z2 + x2 =
1
1 + x2 âˆ’
1
z2 + x2 .
If z Ì¸= âˆ’1, 0, 1, then the frequency of Z = XY is given by
fZ(z)
=
 âˆ
âˆ’âˆ
g(x) g
 z
x
 1
|x| dx = 1
Ï€2
 âˆ
âˆ’âˆ
1
1 + x2 Â·
1
1 +
 z
x
2 Â· 1
|x| dx
=
2
Ï€
 âˆ
0
1
1 + x2 Â·
1
z2 + x2 x dx
symmetry; then u = x2,
=
1
Ï€2
 âˆ
0
1
1 + u Â·
1
z2 + u du
=
1
Ï€2
 âˆ
0

1
z2 âˆ’1 Â·
1
1 + u âˆ’
1
z2 âˆ’1 Â·
1
z2 + u

du
=
1
Ï€2 Â·
1
z2 âˆ’1
lim
Aâ†’âˆ

0

1
1 + u âˆ’
1
z2 + u

du
=
1
Ï€2 Â·
1
z2 âˆ’1
lim
Aâ†’âˆ

ln
 u + 1
u + z2

âˆ’ln
 1
z2

= 2
Ï€2 Â· ln |z|
z2 âˆ’1.
Since the exceptional set {âˆ’1, 0, 1} is a null set, we can choose fZ(z) = 0, which is suï¬ƒcient at these
points.
Note that
lim
zâ†’0 fZ(z) = âˆ,
and that it follows by lâ€™Hospitalâ€™s rule that
lim
zâ†’Â±1
2
Ï€2 Â· ln |z|
z2 âˆ’1 = 2
Ï€2
lim
zâ†’Â±1
1
z
2z = 2
Ï€2
lim
zâ†’Â±1
1
2z2 = 1
Ï€2 .
Download free eBooks at bookboon.com

Random variables I
 
114 
8. Functions of two random variables, f(X.Y)
Example 8.3 Let X and Y be independent random variables, both rectangularly distributed over the
interval ]0, 1[.
1) Find the frequency of the random variable XY .
2) Find the frequency of the random variable X
Y .
3) Find P{Y > 2X}.
The two independent random variables X and Y have the same frequency,
f(x) =
â§
â¨
â©
1,
x âˆˆ]0, 1[,
0,
otherwise.
0
0.5
1
1.5
2
y
â€“0.4 â€“0.2
0.2
0.4
0.6
0.8
1
1.2
1.4
x
Figure 45: The graph of the frequency g(s) of XY .
1) Since the values of XY lie in ]0, 1[, the frequency is for s âˆˆ]0, 1[,
g(s) =
 1
x=s
f(x) f
 s
x
 1
x dx =
 1
x=s
1
x dx = âˆ’ln s,
thus
g(s) =
â§
â¨
â©
âˆ’ln s,
0 < s < 1,
0,
otherwise.
2) Since the values of X
Y lie in ]0, âˆ[, it follows by an application of a formula for s âˆˆ]0, âˆ[ that the
frequency is given by
h(s) =
 âˆ
0
f(s x) f(x) x dx.
This expression is only Ì¸= 0, if sx âˆˆ]0, 1[ and x âˆˆ]0, 1[, hence if 0 < x < 1 and 0 < x < 1
s. Then
we must split the investigation:
Download free eBooks at bookboon.com

Random variables I
 
115 
8. Functions of two random variables, f(X.Y)
â€“0.2
0
0.2
0.4
0.6
0.8
1
y
1
2
3
x
Figure 46: The graph of the frequency h(s) of X
Y .
a) If 0 < s â‰¤1, then 1 â‰¤1
s, hence
h(s) =
 1
0
1 Â· 1 Â· x dx = 1
2.
b) If 1 < s < âˆ, then instead
h(s) =

1
s
0
x dx =
1
2s2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables I
 
116 
8. Functions of two random variables, f(X.Y)
Summing up we get
h(s) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
1
2,
0 < s â‰¤1,
1
2s2 ,
1 < s < âˆ,
0,
s â‰¤0.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 47: The line y = 2x deï¬nes the upper triangle A, where y > 2x.
3) 1st variant. It follows from the geometry that
P{Y > 2X} = area(A) = 1
2 Â· 1
2 Â· 1 = 1
4.
2nd variant. It follows from (2) that
P{Y > 2X} = P
X
Y < 1
2

= 1
2.
An alternative solution is the following:
1) Since XY has its values lying in ]0, 1[, it follows from the ï¬gure that if s âˆˆ]0, 1[. then
P{XY â‰¤s} = areal(A) =
 1
0
1 dx +
 1
s
s
x dx = s âˆ’s ln s,
and the frequency is obtained by a diï¬€erentiation,
g(s) =
â§
â¨
â©
âˆ’ln s,
0 < s < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
117 
8. Functions of two random variables, f(X.Y)
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
Figure 48: The curve xy = s deï¬nes the domain A.
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
Figure 49: The domain A lies above the line x
y = s, 0 < s < 1.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1.2
Figure 50: The domain B lies above the line x
y = s, s > 1.
Download free eBooks at bookboon.com

Random variables I
 
118 
8. Functions of two random variables, f(X.Y)
2) It follows that the values of X
Y lie in ]0, âˆ[. If s âˆˆ]0, 1[, then it follows from the ï¬rst ï¬gure that
P
X
Y â‰¤s

= area(A) = 1
2 s,
0 < s < 1.
If s > 1, then it follows from the second ï¬gure that
P
X
Y â‰¤s

= area(B) = 1 âˆ’1
2s,
s â‰¥1.
Finally, the frequency is obtained by a diï¬€erentiation,
h(s) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
1
2,
0 < s < 1,
1
2s2 ,
1 â‰¤s,
0,
s â‰¤0.
Example 8.4 Assume that X and Y are independent random variable, both attaining the values 1,
2, 3, 4, 5, 6. Assume that
fi = P {X = i} ,
gi = P {Y = i} ,
i = 1, 2, 3, 4, 5, 6.
It is well-known that if fi = gi = 1
6, i = 1, 2, 3, 4, 5, 6, then the probabilities P{X + Y = k}, k = 2,
3, . . . , 12, are not all the same.
Prove that no matter how fi and gi are chosen, we can never obtain P{X + Y = k} = 1
11 for all
k = 2, 3, . . . , 12.
Assume that
P{X + Y = k} = 1
11
for all k = 2, 3, . . . , 12.
If k = 2, then
1
11 = f1g1,
hence f1 > 0 and g1 > 0.
If k = 12, then
1
11 = f6g6,
hencer f6 > 0 and g6 > 0.
If k = 7, then
1
11 = f6g1 + {f5g2 + f4g3 + f3g4 + f2g5} + f1g6.
By subtracting the equation for k = 2 from the equation for k = 7, it follows by a rearrangement that
(f1 âˆ’f6) g1 = {f5g2 + f4g3 + f3g4 + f2g5} + f1g6 > 0,
Download free eBooks at bookboon.com

Random variables I
 
119 
8. Functions of two random variables, f(X.Y)
because {Â· Â· Â· } â‰¥0 and f1g6 > 0. Since g1 > 0, we must have f1 > f6.
If we subtract the equation for k = 12 from the equation for k = 7, then by a rearrangement.
(f6 âˆ’f1) g6 = {f5g2 + f4g3 + f3g4 + f2g5} + f6g1 > 0
for similar reasons. We conclude that f6 > f1.
These two claims cannot be simultaneously fulï¬lled, so the assumption must be wrong.
It even follows from the proof above that P{X + Y = 2}, P{X + Y = 7} and P{X + Y = 12} can
never have the same value, thus we can get a stronger result.
Alternatively we assume that we can choose the fi and the gj in such a way that the probabilities
are equal, i.e.
P{X + Y = k} = 1
11,
k = 2, . . . , 12.
Then in particular,
P{X + Y = 2} = f1g1 = 1
11
and
P{X + Y = 12} = f6g6 = 1
11,
hence f1g1 = f6g6. This is reformulated in the following way
f1
f6
= g6
g1
= x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables I
 
120 
8. Functions of two random variables, f(X.Y)
Considering the case k = 7 we get
1
11 = P{X + Y = 7} â‰¥f1g6 + f6g1 = f1g1
g6
g1
+ f6
f1

= 1
11

x + 1
x

.
Since x + 1
x > 1 (actually x + 1
x â‰¥2, when x > 0), this is not possible, and we have obtained a
contradiction, and the claim follows.
Example 8.5 Let the 2-dimensional random variable (X1, X2) have its frequency h (x1, x2) given by
h (x1, x2) =
â§
âª
â¨
âª
â©
1
Ï€r2 ,
x2
1 + x2
2 < r2,
0,
otherwise,
(a uniform distribution over the disc x2
1 + x2
2 < r2).
Let the random variables Y1 and Y2 be given by
X1 = Y1 cos Y2,
X2 = Y1 sin Y2,
0 â‰¤Y1 < r,
0 â‰¤Y2 < 2Ï€.
Find the frequency of the 2-dimensional random variable (Y1, Y2), and ï¬nd the marginal frequencies.
Are Y1 and Y2 independent?
Remark 8.1 This clearly corresponds to the transformation between rectangular and polar coordi-
nates over a ï¬xed disc. â™¦
It follows that
x = (x1, x2) = Ï•(y) = (y1 cos y2, y1 sin y2) ,
y1 âˆˆ[0, r[,
y2 âˆˆ[0, 2Ï€[.
The corresponding Jacobian is

âˆ‚x1
âˆ‚y1
âˆ‚x1
âˆ‚y2
âˆ‚x2
âˆ‚y1
âˆ‚x2
âˆ‚y2

=

cos y2
âˆ’y1 sin y2
sin y2
y1 cos y2

= y1 â‰¥0.
Hence,
k (y1, y2) =
â§
âª
â¨
âª
â©
y1
Ï€r2 ,
(y1, y2) âˆˆ[0, r] Ã— [0, 2Ï€[,
0,
otherwise.
The frequencies of the marginal distributions are then
fY1 (y1) =
â§
âª
â¨
âª
â©
 2Ï€
0
y1
Ï€r2 dy2 = 2Ï€ Â· y1
Ï€r2 = 2y1
r2 ,
y1 âˆˆ[0, r[,
0,
otherwise,
Download free eBooks at bookboon.com

Random variables I
 
121 
8. Functions of two random variables, f(X.Y)
and
fY2 (y2) =
â§
âª
â¨
âª
â©
 r
0
y1
Ï€r2 dy1 = 1
2Ï€ ,
y2 âˆˆ[0, 2Ï€[,
0,
otherwise.
It follows immediately that
k (y1, y2) = fY1 (y1) fY2 (y2) ,
hence Y1 and Y2 are stochastically independent.
Example 8.6 Let the 2-dimensional random variable (X1, X2) have the frequency
h (x1, x2) =
â§
â¨
â©
x1 + x2,
for 0 < x1 < 1 og 0 < x2 < 1,
0,
otherwise,
and let (Y1, Y2) = Ï„ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = X2.
1) Prove that Ï„ maps ]0, 1[ Ã— ]0, 1[ bijectively onto the domain
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y2 < 1, y2 < y1 < y2 + 1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 51: The domain Dâ€².
Since Ï„ (x1, x2) = (x1 + x2, x2), i.e.
y1 = x1 + x2,
y2 = x2,
Download free eBooks at bookboon.com

Random variables I
 
122 
8. Functions of two random variables, f(X.Y)
it follows that the inverse map Ï„ âˆ’1 exists,
Ï„ âˆ’1 (y1, y2) = (y1 âˆ’y2, y2) ,
thus
x1 = y1 âˆ’y2,
x2 = y2.
The conditions 0 < x1 < 1 and 0 < x2 < 1 can now be replaced by
0 < y1 âˆ’y2 < 1,
0 < y2 < 1,
hence
y2 < y1 < y2 + 1
and
0 < y2 < 1,
and we have proved that Ï„ maps ]0, 1[ Ã— ]0, 1[ bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y2 < 1, y2 < y1 < y2 + 1

.
The Jacobian is then given by
JÏ„ (y1, y2) = âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
âˆ’1
0
1
 = 1.
Then (Y1, Y2) i Dâ€² has the frequency
k (y1, y2) = h (y1 âˆ’y2, y2) Â· 1 = y1,
thus
k (y1, y2) =
â§
â¨
â©
y1,
for 0 < y2 < 1 and y2 < y1 < y2 + 1,
0,
otherwise.
â€“0.2
0
0.2
0.4
0.6
0.8
1
1.2
y
0.5
1
1.5
2
x
Figure 52: The graph of FY1 (y1).
Download free eBooks at bookboon.com

Random variables I
 
123 
8. Functions of two random variables, f(X.Y)
Marginal frequencies.
1) We get for Y1 by a vertical integration
a) If 0 < y1 â‰¤1, then
fY1 (y1) =
 y1
y2=0
k (y1, y2) dy2 =
 y1
y2=0
y1 dy2 = y2
1.
b) If 1 < y1 < 2, then
fY1 (y1) =
 1
y2=y1âˆ’1
k (y1, y2) dy2 =
 1
y2=y1âˆ’1
y1 dy2 = y1 (2 âˆ’y1) ,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
124 
8. Functions of two random variables, f(X.Y)
hence summing up,
fY1 (y1) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
y2
1,
0 < y1 â‰¤1,
y1 (2 âˆ’y1) = 1 âˆ’(y1 âˆ’1)2 ,
1 < y1 < 2,
0,
otherwise.
â€“0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
y
â€“0.2
0.2
0.4
0.6
0.8
1
1.2
x
Figure 53: The graph of fY2 (y2).
2) For Y2 it follows by a horizontal integration for 0 < y2 < 1 that
fY2 (y2)
=
 y2+1
y1=y2
k (y1, y2) dy1 =
 y2+1
y1=y2
y1 dy1 =
1
2 y2
1
y2+1
y1=y2
=
1
2

(y2 + 1)2 âˆ’y2
2
 
= 1
2 (2y2 + 1) Â· 1 = y2 + 1
2,
thus summing up
fY2 (y2) =
â§
âª
â¨
âª
â©
y2 + 1
2,
0 < y2 < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
125 
8. Functions of two random variables, f(X.Y)
Example 8.7 Assume that the 2-dimensional random variable (X1, X2) has the frequency
h (x1, x2) =
â§
â¨
â©
2 exp (âˆ’(x1 + x2)) ,
(x1, x2) âˆˆD,
0,
otherwise.
where
D =

(x1, x2) âˆˆR2 | 0 < x2 < x1 < âˆ

,
and let (Y1, Y2) = Ï„ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = (X1 âˆ’X2)2 .
1) Prove that Ï„ maps D bijectively onto the domain
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < âˆ, 0 < y2 < y2
1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
4) Are Y1 and Y2 independent random variables?
0
0.2
0.4
0.6
0.8
1
y
0.2
0.4
0.6
0.8
1
x
Figure 54: The domain D lies between the X1 axis and the line x2 = x1.
1) It follows from
y1 = x1 + x2
and
y2 = (x1 âˆ’x2)2
og
x1 âˆ’x2 > 0
that
x1 + x2 = y1
and
x1 âˆ’x2 = +âˆšy2,
hence
x1 = 1
2 {y1 + âˆšy2}
and
x2 = 1
2 {y1 âˆ’âˆšy2} .
Download free eBooks at bookboon.com

Random variables I
 
126 
8. Functions of two random variables, f(X.Y)
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 55: The domain Dâ€² lies between the Y1 axis and the parabola y2 = y2
1.
Since (x1, x2) is uniquely determined by (y1, y2), we conclude that Ï„ is bijective. The boundary
curve x2 = x1 is mapped into y2 = 0. The boundary curve x2 = 0 is mapped into y1 = âˆšy2, i.e.
y2 = y2
1, y1 â‰¥0. Since x2 > 0 in D, we must have y1 > âˆšy2, thus 0 < y2 < y2
1, and we have proved
that Ï„ maps D bijectively onto Dâ€².
2) We next compute the Jacobian,
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

âˆ‚x1
âˆ‚y1
âˆ‚x1
âˆ‚y2
âˆ‚x2
âˆ‚y1
âˆ‚x2
âˆ‚y2

=

1
2
1
4
1
âˆšy2
1
2
âˆ’1
4
1
âˆšy2

= âˆ’1
4
1
âˆšy2
< 0.
It follows from x1 + x2 = y1 that the frequency of (Y1, Y2) is given by
k (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
2 eâˆ’y1 Â· 1
4
1
âˆšy2
= 1
2
eâˆ’y1
âˆšy2
for (y1, y2) âˆˆDâ€²,
0,
for (y1, y2) /âˆˆDâ€².
3) The marginal frequency of Y1 is obtained by a vertical integration,
fY1 (y1) = 1
2
 y2
1
0
eâˆ’y1
âˆšy2
dy2 = eâˆ’y1 [âˆšy2]y2
1
0 = y1 eâˆ’y1
for y1 > 0,
and fY1 (y1) = 0 for y â‰¤0.
The frequency of Y2 is obtained by a horizontal integration,
fY2 (y2) = 1
2
 âˆ
âˆšy2
eâˆ’y1
âˆšy2
dy1 = 1
2
eâˆ’âˆšy2
âˆšy2
for y2 > 0,
and fY2 (y2) = 0 for y2 â‰¤0.
Download free eBooks at bookboon.com

Random variables I
 
127 
8. Functions of two random variables, f(X.Y)
4) Since Dâ€² is not a rectangle, it follows immediately that Y1 and Y2 are not independent. It also
follows from
fY1 (y1) Â· fY2 (y2) Ì¸= k (y1, y2) .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables I
 
128 
8. Functions of two random variables, f(X.Y)
Example 8.8 Assume that X1 and X2 are independent identically distributed random variables of
the frequency
f(x) =
â§
â¨
â©
x eâˆ’x,
x > 0,
0,
otherwise.
1. Compute the means E {X1} and E
 1
X1

.
2. Compute the probability P {X2 > X1}.
Deï¬ne the random variables Y1 and Y2 by
Y1 = X1 + X2,
Y2 = X1
X2
.
It will without proof be given that the vector function Ï„ given by
Ï„ (x1, x2) =

x1 + x2, x1
x2

maps R+ Ã— R+ bijectively onto itself.
3. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
(This question can be answered both with and without an application of the answer of question 3.).
5. Check if Y1 and Y2 are independent.
6. Compute the mean E {Y2}.
7. Find, e.g. by an application of question 2., the median of Y2.
8. Give an intuitive explanation of why the median of Y2 is smaller than the mean of Y2.
1) The means are
E {X1} =
 âˆ
0
x2eâˆ’x dx = 2
and
E
 1
X1

=
 âˆ
0
x
x eâˆ’x dx = 1.
2) By the symmetry,
P {X2 > X1} = P {X1 > X2} = 1
2 (P {X2 > X1} + P {X1 > X2}) = 1
2.
Alternatively, the frequency of Y2 = X1
X2
is zero for y2 â‰¤0, and when y2 > 0, then
fY2 (y2)
=
 âˆ
âˆ’âˆ
f (y2x) f(x) Â· |x| dx =
 âˆ
0
y2x eâˆ’y2x Â· x eâˆ’x Â· x dx
=
y2
 âˆ
0
x3eâˆ’(1+y2)x dx =
y2
(1 + y2)4
 âˆ
0
t3eâˆ’t dt =
6y2
(1 + y2)4 ,
Download free eBooks at bookboon.com

Random variables I
 
129 
8. Functions of two random variables, f(X.Y)
hence
P {X2 > X1}
=
P
X1
X2
< 1

=
 1
0
6y2
(1 + y2)4 dy2 = 6
 1
0

1
(1 + y2)3 âˆ’
1
(1 + y2)4

dy2
=
6

âˆ’1
2
1
(1 + y2)2 + 1
3
1
(1 + y2)3
1
0
= âˆ’3
22 + 2
23 + 3
12 âˆ’2
13
=
1 âˆ’3
4 + 1
4 = 1
2.
3) It follows from y1 = x1 + x2 and y2 = x1
x2
that
x1 = y2x2
and
y1 = x1 + x2 = (y2 + 1) x2,
hence
x1 = y1y2
y2 + 1 = y1 âˆ’
y1
y2 + 1
and
x2 =
y1
y2 + 1.
The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

y2
y2 + 1
y1
(y2 + 1)2
1
y2 + 1
âˆ’
y1
(y2 + 1)2

= âˆ’
y1
(y2 + 1)3 (y2 + 1) = âˆ’
y1
(y2 + 1)2 < 0.
The simultaneous frequency of (X1, X2) is
g (x1, x2) =
â§
â¨
â©
x1x2 eâˆ’(x1+x2)
for x1 > 0 og x2 > 0,
0
otherwise,
hence the simultaneous frequency of (Y1, Y2) is 0 for y1 â‰¤0 or y2 â‰¤0, and
k (y1, y2)
=
y1y2
y2 + 1 Â·
y1
y2 + 1 Â· eâˆ’y1 Â·
y1
(y2 + 1)4
=
y3
1y2
(y2 + 1)4 eâˆ’y1,
for y1 > 0 and y2 > 0,
which also can be written
k (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
1
6 y3
1eâˆ’y1 Â·
6y2
(y2 + 1)4
for y1 > 0 and y2 > 0,
0
otherwise.
4) It follows from the rewriting of 3.
(possibly by the second variant of 2.)
that the marginal
frequencies are
kY1 (y1) =
â§
âª
â¨
âª
â©
1
6 y3
1eâˆ’y1
for y1 > 0,
0
for y1 â‰¤0,
Download free eBooks at bookboon.com

Random variables I
 
130 
8. Functions of two random variables, f(X.Y)
kY2 (y2) =
â§
âª
âª
â¨
âª
âª
â©
6y2
(y2 + 1)4
for y2 > 0,
0
for y2 â‰¤0.
5) It is obvious that Y1 and Y2 are independent because
k (y1, y2) = kY1 (y1) Â· kY2 (y2) .
6) Since X1 and X2 are independent, the mean is
E {Y2} = E
X1
X2

= E {X1} Â· E
 1
X2

= 2 Â· 1 = 2.
7) By question 2,
1
2 = P {X2 > X1} = P
X1
X2
< 1

= P {Y2 < 1} ,
hence the median is âŸ¨Y2âŸ©= 1.
Alternatively, Y2 has the distribution function
KY2 (y2)
=
 y2
0
6t
(t + 1)4 dt =
 y2+1
1
6(u âˆ’1)
u4
du = 6
 y2+1
1
 1
u3 âˆ’1
u4

du
=
6

âˆ’1
2
1
u2 + 1
3
1
u3
y2+1
1
= 1 âˆ’
3
(y2 + 1)2 +
2
(y2 + 1)3 = 1 âˆ’3y2 + 1
(y2 + 1)3 .
If we put KY2 (y2) = 1
2, then
3y2 + 1
(y2 + 1)3 = 1
2,
dvs.
6y2 + 2 = (y2 + 1)3 ,
or
y3
2 + 3y2
2 âˆ’3y2 âˆ’1 = (y2 âˆ’1)
	
y2
2 + 4y2 + 1

= 0.
The only positive solution is y2 = 1, which is the median.
8) The mass of probability is divided into two equal parts by the median 1. However, the mass of
probability is scattered more to the right of 1 then to the left of 1. Thus, the mean must lie to the
right of 1.
Download free eBooks at bookboon.com

Random variables I
 
131 
9. Means and moments of higher order
9
Means and moments of higher order
Example 9.1 Let X be a random variable of values in N0. Prove that E{X} exists, if and only if
âˆ

k=0
P{X > k} < âˆ,
and that in the aï¬ƒrmative case,
E{X} =
âˆ

k=0
P{X > k}.
It is often easier to apply this formula by computation of means.
First note that a necessary and suï¬ƒcient condition of the existence of E{X} and is equal to
E{X} =
âˆ

n=0
n P{X = n} =
âˆ

n=1
n P{X = n},
is that
âˆ

n=1
n P{X = n} is absolutely convergent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables I
 
132 
9. Means and moments of higher order
Since the terms are non-negative, this is equivalent with
âˆ

n=1
n P{X = n} is (just) convergent.
If )âˆ
k=0 P{X > k} is convergent, then by a (correct) interchanging of the order of summation (because
all terms are â‰¥0),
âˆ

k=0
P{X > k} =
âˆ

k=0
âˆ

n=k+1
P{X = n} =
âˆ

n=1
nâˆ’1

k=0
P{X = n} =
âˆ

n=1
n P{X = n} = E{X}.
If conversely E{X} exists, we just repeat the computations above in the reverse order.
Example 9.2 Two persons A and B play the following game:
They each throw two coins. The winner is he who gets most heads in a throw. The game is a draw,
if they obtain an equal number of heads.
1. What is the probability q that the game is a draw?
2. What is the probability pA that A wins?
If the game is a draw, the game is continued in the same way. One stops ï¬rst time one of the two
players wins.
3. What is the probability that A wins in game number k?
4. Find the mean of the number of games.
B \ A
TT
TH
HT
HH
TT
0
1
1
1
TH
-1
0
0
1
HT
-1
0
0
1
HH
-1
-1
-1
0
Table 1: If A wins, we write 1. If B wins, we write -1. In case of a draw we write 0.
1) We write 1 if A wins, and âˆ’1 if B wins. Finally, 0 means a draw. Tail is denoted by T, and head
by H. Since the 16 possibilities all have the same probability, we get by simply counting
q = P{a draw} = 6
16 = 3
8.
2) It follows from (1) and the symmetry of A and B that
P{A wins} = pA = pB = P{B wins} = 5
16.
Download free eBooks at bookboon.com

Random variables I
 
133 
9. Means and moments of higher order
3) If A wins in game number k, then the ï¬rst k âˆ’1 games must all have been draws, hence
P{A wins in the kth game} = qkâˆ’1 Â· pA =
3
8
kâˆ’1
Â· 5
16.
4) Let X denote the number of games. Then
P{X = k} = P{A wins in game number k} + P{B wins in game number k} = 5
8 Â·
3
8
kâˆ’1
.
When |x| < 1, then
âˆ

k=1
k xkâˆ’1 = d
dx
 âˆ

k=0
xk

= d
dx

1
1 âˆ’x

=
1
(1 âˆ’x)2 .
Using this result we get the mean
E{X} = 5
8
âˆ

k=1
k
3
8
kâˆ’1
= 5
8 Â·
1

1 âˆ’3
8
2 = 8
5.
Example 9.3 A box contains N balls of the numbers from 1 to N. Select at random n balls with
replacement. Let Xn denote the random variable which indicates the largest selected number.
Find the distribution of Xn.
Find the mean E {Xn}, and prove for large N that this mean is approximately equal to
n
n + 1 N.
Let X denote one selection at random of n numbers. Since all numbers have the same probability,
the distribution function is given by
FX(k) = P{X â‰¤k} = k
N ,
k = 1, 2, . . . , N.
Thus we derive the distribution function of Xn,
FXn(k) = P

max
j=1,...,n Xj â‰¤k

= (P{X â‰¤k})n =
 k
N
n
,
k = 1, 2, . . . , N,
hence
pk = P {Xn = k} = P {Xn â‰¤k} âˆ’P {Xn â‰¤k âˆ’1} = kn âˆ’(k âˆ’1)n
N n
.
The mean is
E {Xn}
=
N

k=1
k pk =
1
N n
â§
â¨
â©
N

k=1
kn+1 âˆ’
N

k=1 (=2)
k(k âˆ’1)n
â«
â¬
â­=
1
N n
 N

k=1
kn+1 âˆ’
Nâˆ’1

k=1
(k + 1)kn

=
1
N n

N n+1 âˆ’

k=1
N âˆ’1kn

= N

1 âˆ’1
N
Nâˆ’1

k=1
 k
N
n
.
Download free eBooks at bookboon.com

Random variables I
 
134 
9. Means and moments of higher order
Then notice that
1
N
)Nâˆ’1
k=1
 k
N
n
can be interpreted as an approximating sum of the integral
 1
0 xn dx =
1
n + 1, hence
1
N
Nâˆ’1

k=1
 k
N
n
â†’
 1
0
xn dx =
1
n + 1
for N â†’âˆ.
Then
E {Xn} = N

1 âˆ’
1
n + 1 + 1
N Îµ
 1
N

â‰ˆ
n
n + 1 N
for store N.
Example 9.4 Let X be a random variable of the distribution function F(x), the frequency f(x) and
the mean Î¼. Prove that
Î¼ =
 âˆ
0
{1 âˆ’F(x)}dx âˆ’
 0
âˆ’âˆ
F(x) dx.
It is given that
 âˆ
âˆ’âˆ|x| f(x) dx < âˆ, and that
(6) Î¼ =
 âˆ
âˆ’âˆ
x f(x) dx =
 0
âˆ’âˆ
x f(x) dx +
 âˆ
0
x f(x) dx.
Let A > 0. Then
 0
âˆ’A
x f(x) dx = [x F(x)]0
âˆ’A âˆ’
 0
âˆ’A
F(x) dx = A F(âˆ’A) âˆ’
 0
âˆ’A
F(x) dx,
and
 A
0
x f(x) dx = [x{F(x) âˆ’1}]A
0 +
 A
0
{1 âˆ’F(x)} dx = âˆ’A{1 âˆ’F(A)} +
 A
0
(1 âˆ’F(x)) dx.
Since
0 â‰¤A F(âˆ’A) = A
 âˆ’A
âˆ’âˆ
f(x) dx â‰¤
 âˆ’A
âˆ’âˆ
|x| f(x) dx â†’0
for A â†’âˆ,
we conclude that A F(âˆ’A) â†’0 for A â†’âˆ. Since
 0
âˆ’âˆx f(x) dx is absolutely convergent, it follows
by taking the limit A â†’âˆthat
 0
âˆ’âˆ
x f(x) dx = lim
Aâ†’âˆ

A F(âˆ’A) âˆ’
 0
âˆ’A
F(x) dx

= âˆ’
 0
âˆ’âˆ
F(x) dx.
Analogously,
0 â‰¤A{1 âˆ’F(A)} = A
 âˆ
A
f(x) dx â‰¤
 âˆ
A
|x| f(x) dx â†’0
for A â†’âˆ,
Download free eBooks at bookboon.com

Random variables I
 
135 
9. Means and moments of higher order
so when A â†’âˆ, we conclude in the same way that
 âˆ
0
x f(x) dx = lim
Aâ†’âˆ

âˆ’A{1 âˆ’F(A)} +
 A
0
{1 âˆ’F(x)} dx

=
 âˆ
0
{1 âˆ’F(x)} dx,
where the integrals are even absolutely convergent.
Finally, by insertion into (6) we get
Î¼ =
 0
âˆ’âˆ
x f(x) dx +
 âˆ
0
x f(x) dx =
 âˆ
0
{1 âˆ’F(x)} dx âˆ’
 0
âˆ’âˆ
F(x) dx
as required.
Alternatively, a more streamlined, though also more sophisticated method is the following. We
see that
 âˆ
0
{1 âˆ’F(x)} dx =
 âˆ
x=0
 âˆ
y=x
f(y) dy

dx =
 âˆ
y=0
f(y)
 y
x=0
1 dx

dy =
 âˆ
0
y f(y) dy,
and
âˆ’
 0
âˆ’âˆ
F(x) dx
=
âˆ’
 0
x=âˆ’âˆ
 x
y=âˆ’âˆ
f(y) dy

dx =
 0
y=âˆ’âˆ
f(y)
 0
x=y
(âˆ’1) dx

dy
=
 0
âˆ’âˆ
y f(y) dy.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
136 
9. Means and moments of higher order
By adding these two expressions, we formally obtain that
 âˆ
0
{1 âˆ’F(x)} dx âˆ’
 0
âˆ’âˆ
F(x) dx =
 âˆ
âˆ’âˆ
y f(y) dy = Î¼.
However, we have assumed that the mean exists, which implies that all the integrals above are abso-
lutely convergent, so the formal calculation is also real.
Example 9.5 Let X be an non-negative random variable of the distribution function F(x) and fre-
quency f(x). Prove that
(7) E

Xk
= k
 âˆ
0
xkâˆ’1{1 âˆ’F(x)} dx,
k âˆˆN.
(If the kth moment does not exist, then both the right hand side and the left hand side of (7) are equal
to âˆ.)
Find a similar formula, if the random variable X is non-positive.
Remark. One can prove that formula (7) holds for every non-negative random variable X.
1) Assume that X is non-negative and that E

Xk
exists, i.e.
0 â‰¤E

Xk
=
 âˆ
0
xkf(x) dx < âˆ.
Let A > 0. Then by a partial integration,
 A
0
xkf(x) dx
=
#
xk{F(x) âˆ’1}
$A
0 + k
 A
0
xkâˆ’1{1 âˆ’F(x)} dx
=
âˆ’Ak{1 âˆ’F(A)} + k
 A
0
xkâˆ’1{1 âˆ’F(x)} dx.
Now
 âˆ
0
xkf(x) dx < âˆ, so we get the estimate
0 â‰¤Ak{1 âˆ’F(A)} = Ak
 âˆ
A
f(x) dx â‰¤
 âˆ
A
xk f(x) dx â†’0
for A â†’âˆ.
Then by taking the limit A â†’âˆ,
E

Xk
=
 âˆ
0
xkf(x) dx = k
 âˆ
0
xkâˆ’1{1 âˆ’F(x)} dx.
It is trivial that
 A
0
xkf(x) dx â‰¤k
 A
0
xkâˆ’1{1 âˆ’F(x)} dx,
so we infer that if E

Xk
= âˆ, then k
 âˆ
0
xkâˆ’1{1 âˆ’F(x)} dx = âˆ.
Download free eBooks at bookboon.com

Random variables I
 
137 
9. Means and moments of higher order
2) Then assume that X is non-positive and that E

Xk
exists, i.e.
0 â‰¤
E

Xk =
 0
âˆ’âˆ
|x|kf(x) dx < âˆ.
If A > 0, then
 0
âˆ’A
xkf(x) dx =
#
xkF(x)
$0
âˆ’A âˆ’k
 0
âˆ’A
xkâˆ’1F(x) dx = âˆ’(âˆ’A)kF(âˆ’A) âˆ’k
 0
âˆ’A
xkâˆ’1F(x) dx.
Since
0 â‰¤AkF(âˆ’A) = Ak
 âˆ’A
âˆ’âˆ
f(x) dx â‰¤
 âˆ’A
âˆ’âˆ
|x|kf(x) dx â†’0
for A â†’âˆ,
it follows by taking the limit that
(8) E

Xk
=
 0
âˆ’âˆ
xkf(x) dx = âˆ’k
 0
âˆ’âˆ
xkâˆ’1F(x) dx.
Clearly,
 0
âˆ’A
xkf(x) dx,
(âˆ’A)kF(âˆ’A)
og
âˆ’k
 0
âˆ’A
xkâˆ’1F(x) dx
have all the same sign. Hence,

 0
âˆ’A
xkf(x) dx
 â‰¤
k
 0
âˆ’A
xkâˆ’1F(x) dx
 .
Therefore, if E

Xk
does not exist, then both integrals of (8) are divergent.
Alternatively and more streamlined (and also more sophisticated), because one at ï¬rst does not
care so much for the convergence of the integrals (this should of course be done at last), we have the
following proof:
When k âˆˆN, and X is non-negative of the distribution function F(x) and the frequency f(x), then
k
 âˆ
0
xkâˆ’1{1 âˆ’F(x)} dx = k
 âˆ
x=0
xkâˆ’1
 âˆ
y=x
f(y) dy

dx
=
 âˆ
y=0
f(y)
 y
x=0
k xkâˆ’1 dx

dy =
 âˆ
0
ykf(y) dy = E

Xk
.
Then notice that if the kth moment does not exist, then all the integrals involved are divergent of the
value âˆ. Since the integrand is non-negative, we can interchange the order of integration. Conversely,
if the kth moment exists, then all the involved integrals are convergent with a non-negative integrand,
hence uniformly convergent, and all the computations are legal.
Then let X â‰¤0 have the distribution function F(x) and the frequency f(x). Then for k âˆˆN,
âˆ’k
 0
âˆ’âˆ
xkâˆ’1F(x) dx = âˆ’k
 0
x=âˆ’âˆ
xkâˆ’1
 x
y=âˆ’âˆ
f(y) dy

dx
=
 0
y=âˆ’âˆ
f(y)
 0
x=y
	
âˆ’k xkâˆ’1
dx

dy =
 0
âˆ’âˆ
ykf(y) dy = E

Xk
.
Download free eBooks at bookboon.com

Random variables I
 
138 
9. Means and moments of higher order
Thus we get in this case the formula
E

Xk
= âˆ’k
 0
âˆ’âˆ
xkâˆ’1F(x) dx.
We still have to remark that in the case of divergence the left hand side and the right hand side are
either both âˆ’âˆor both +âˆ. It the kth moment exists, then all integrals are absolutely convergent.
Example 9.6 Let X and Y be non-negative random variables of distribution functions FX and FY ,
means E{X} and E{Y } variances V {X} and V {Y }.
The random variable X is said to be stochastically larger than Y , if
FX(x) â‰¤FY (x)
for all x âˆˆR.
1) If so, prove that E{Y } â‰¤E{X}.
2) Can one also conclude that V {Y } â‰¤V {X}?
1) Since X and Y are non-negative, it follows from Example 9.4 or Example 9.5 that
E{Y } =
 âˆ
0
{1 âˆ’FY (x)} dx â‰¤
 âˆ
0
{1 âˆ’FX(x)} dx = E{X}.
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1.2
1.4
Figure 56: Illustration of FX(x) â‰¤FY (x) in (2).
2) The answer is â€œnoâ€! We construct a counterexample. Let a âˆˆ
1
2, 1

, and let X be rectangularly
distributed over ]a, 1[, and let Y be rectangularly distributed over ]0, a[. Then clearly X and Y
are non-negative, and FX(x) â‰¤FY (y), cf. the ï¬gure, hence X is stochastically larger than Y .
This is in agreement with
E{Y } = a
2 < 1 + a
2
= E{X}.
Download free eBooks at bookboon.com

Random variables I
 
139 
9. Means and moments of higher order
For the variances, however, we get
V {Y } = a2
12 > (1 âˆ’a)2
12
= V {X},
because a > 1 âˆ’a > 0 for 1
2 < a < 1.
Example 9.7 Let X be a random variable satisfying
E{X} = E

X2
= 1.
Find the distribution function of X.
Since
V {X} = E

X2
âˆ’(E{X})2 = 1 âˆ’1 = 0,
it follows that X is a constant, and since E{X} = 1, we get X â‰¡1. The distribution function is then
FX(x) =
â§
â¨
â©
1
for x â‰¥1,
0
for x < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables I
 
140 
9. Means and moments of higher order
Example 9.8 Let X be a random variable, for which the kth moment exists for some k âˆˆN \ {1}.
Prove the following generalization of Chebyshevâ€™s inequality:
For every a âˆˆR+,
P{|X âˆ’E{X}| â‰¥a} â‰¤1
ak E

|X âˆ’E{X}|k
.
When E

Xk
exists for some k âˆˆN\{1}, then E

Xj
exists for every j = 1, . . . , k. In fact, |x|j â‰¤1
for |x| â‰¤1, and |x|j â‰¤|x|k for |x| â‰¥1, so
 âˆ
âˆ’âˆ
|x|jf(x) dx â‰¤

{|x|â‰¤1}
1 Â· f(x) dx +

{|x|â‰¥1}
|x|kf(x) dx < âˆ.
In particular, E{X} exists.
Then
E

|X âˆ’E{X}|k
=
 âˆ
âˆ’âˆ
|xâˆ’E{X}|kf(x) dx â‰¥

{|xâˆ’E{X}â‰¥a}
akf(x) dx = akP{|X âˆ’E{X}| â‰¥a},
and the inequality follows by a division by ak.
Example 9.9 Let X be a random variable.
1) Let g : R â†’R be an even, non-negative function, which is increasing on [0, âˆ[.Prove that
P{|X| â‰¥a} â‰¤E{g(X)}
g(a)
for every a âˆˆR+.
2) Let g : R â†’R be a non-negative, increasing function. Prove that
P{X â‰¥a} â‰¤E{g(X)}
g(a)
for every a âˆˆR.
We always assume that E{g(X)} exists.
In the main proof we assume that X is of continuous type, i.e.
E{g(X)} =
 âˆ
âˆ’âˆ
g(x) f(x) dx.
The similar proofs when X is either of discrete type or of mixed type are obtained by simple modiï¬-
cations of the main proof.
1) First by a splitting
P{|X| â‰¥a} = P{X â‰¥a} + P{X â‰¤âˆ’a} =
 âˆ
a
f(x) dx +
 âˆ’a
âˆ’âˆ
f(x) dx.
According to the assumptions, g(x) â‰¥0 and
g(a) â‰¤g(x) for x â‰¥a
and
g(âˆ’a) = g(a) â‰¤g(x) for x â‰¤âˆ’a,
Download free eBooks at bookboon.com

Random variables I
 
141 
9. Means and moments of higher order
hence
g(a)P{|X| â‰¥a} = g(a)
 âˆ
a
f(x) dx + g(âˆ’a)
 âˆ’a
âˆ’âˆ
f(x) dx
â‰¤
 âˆ
a
g(x) f(x) dx +
 âˆ’a
âˆ’âˆ
g(x) f(x) dx â‰¤
 âˆ
âˆ’âˆ
g(x) f(x) dx = E{g(X)}.
If g(a) Ì¸= 0, the result is obtained by a division by g(a) > 0.
2) Similarly,
g(a)P{X â‰¥a} = g(a)
 âˆ
a
f(x) dx â‰¤
 âˆ
a
g(x) f(x) dx â‰¤
 âˆ
âˆ’âˆ
g(x) f(x) dx = E{g(X)}.
If g(a) Ì¸= 0, the result follows by a division by g(a) > 0.
Example 9.10 A random variable X is assumed to have the mean Î¼ and the variance Ïƒ2. Prove that
if a is a median of X, then
|a âˆ’Î¼| â‰¤
âˆš
2 Â· Ïƒ.
Hint. Apply Chebyshevâ€™s inequality.
When a is a median, then
F(a) â‰¥1
2
and
F(aâˆ’) â‰¤1
2.
One of the sets {x â‰¤a} and {x â‰¥a} must necessarily be contained in the set {|x âˆ’Î¼| â‰¥|a âˆ’Î¼|}, and
since P{X â‰¤a} â‰¥1
2 and P{X â‰¥a} â‰¥1
2, we get
1
2 â‰¤P{|X âˆ’Î¼| â‰¥|a âˆ’Î¼|} â‰¤
Ïƒ2
|a âˆ’Î¼|2 , for a Ì¸= Î¼.
Then by a rearrangement,
|a âˆ’Î¼| â‰¤
âˆš
2 Â· Ïƒ.
If a = Î¼, there is of course nothing to prove.
Download free eBooks at bookboon.com

Random variables I
 
142 
9. Means and moments of higher order
Example 9.11 Let X be a random variable, for which all moments mk = E

Xk
exist. We deï¬ne
for k âˆˆN the kth decreasing moment
m(k) = E{X(X âˆ’1)(X âˆ’2) Â· Â· Â· (X âˆ’k + 1)}.
1) Find for n = 1, 2, 3, the nth decreasing as a linear combination of the kth moments k â‰¤n.
2) Find for n = 1, 2, 3, the nth moment as a linear combination of the kth decreasing moments for
k â‰¤n.
A. If n = 1, then
m(1) = E{X} = m1.
B. If n = 2, then
m(2) = E{X(X âˆ’1)} = E

X2
âˆ’E{X} = m2 âˆ’m1,
and hence
m2 = m(2) + m(1).
C. If n = 3, then
m(3)
=
E{X(X âˆ’1)(X âˆ’2)} = E

X3
âˆ’3E

X2
+ 2E{X}
=
m3 âˆ’3m2 + 2m1,
and conversely,
m3 = m(3) + 3m2 âˆ’2m(1) = m(3) + 3m(2) + m(1).
Example 9.12 Let X be a random variable, for which all moments mk = E

Xk
exist. It is then
well-known that all the central moments
vk = E

(X âˆ’m1)k 
.
also exist.
1) Express for n = 2, 3, 4, the nth central moment by the kth moments for k â‰¤n.
2) Express for n = 2, 3, 4, the nth moment by the kth central moments for k â‰¤n and by m1.
A. If n = 2, then
v2 = E

(X âˆ’m1)2 
= E

X2 âˆ’2m1X + m2
1

= m2 âˆ’m2
1,
and thus
m2 = v2 + m2
1.
Download free eBooks at bookboon.com

Random variables I
 
143 
9. Means and moments of higher order
B. If n = 3, then
v3
=
E

(X âˆ’m1)3 
= E

X3 âˆ’3m1X2 + 3m1X2 + 3m2
1X âˆ’m3
1

=
m3 âˆ’3m1m2 + 2m3
1,
and thus
m3 = v3 + 3m1
	
v2 + m2
1

âˆ’2m3
1 = v3 + 3m1v2 + m3
1.
C. If n = 4, then
v4
=
E

(X âˆ’n1)4 
= E

X4 âˆ’4m1X3 + 6m2
1X2 âˆ’4m3
1X + m4
1

=
m4 âˆ’4m1m3 + 6m2
1m2 âˆ’3m4
1,
and thus
m4
=
v4 + 4m1

v3 + 3m1v2 + m3
1

âˆ’6m2
1

v2 + m2
1

+ 3m4
1
=
v4 + 4m1v3 + 6m2
1v2 + m4
1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables I
 
144 
9. Means and moments of higher order
Example 9.13 A real function Ï• deï¬ned on an open interval I is said to be convex, if for all a âˆˆI
there exists a real number ca, such that
Ï•(x) âˆ’Ï•(a) â‰¥ca(x âˆ’a)
for every x âˆˆI
(if Ï• is diï¬€erentiable at the point a, then ca = Ï•â€²(a)).
Let X be a random variable, which only has values in the open interval I, and let Ï• : I â†’R be convex.
Assuming that both E{X} and E{Ï•(X)} exist, prove Jensenâ€™s inequality
E{Ï•(X)} â‰¥Ï•(E{X}).
1) Let us ï¬rst check where the assumption of convexity can be applied. Hence, we insert as a test
any a âˆˆI,
E{Ï•(X)} âˆ’Ï•(a) = E{Ï•(X) âˆ’Ï•(a)} â‰¥ca E{X âˆ’a} = ca(E{X} âˆ’a).
2) Since E{X} âˆˆI, we get by choosing a = E{X},
E{Ï•(X)} âˆ’Ï•(E{X}) â‰¥0,
i.e.
E{Ï•(X)} â‰¥Ï•(E{X}).
Download free eBooks at bookboon.com

Random variables I
 
145 
10. Mean and variance in special cases
10
Mean and variance in special cases
Example 10.1 A random variable X has the frequency
f(x) =
â§
â¨
â©
a eâˆ’ax,
x > 0,
0,
x â‰¤0,
where a is a positive constant.
Find the frequency of Y = X2, and ï¬nd the mean and variance of Y .
The distribution function G(y) of Y = X2 is 0 for y â‰¤0. If y > 0, then
G(y)
=
P{Y â‰¤y} = P

X2 â‰¤y

= P {âˆ’âˆšy â‰¤X â‰¤âˆšy} = P {X â‰¤âˆšy}
=
 âˆšy
0
a eâˆ’ax dx = 1 âˆ’eâˆ’aâˆšy,
for y > 0.
Thus, the corresponding frequency is
g(y) =
â§
âª
â¨
âª
â©
Gâ€²(y) =
a
2âˆšy exp
	
âˆ’aâˆšy

,
for y > 0,
0,
for y â‰¤0.
Then the mean of Y is
E{Y } = E

X2
=
 âˆ
0
x2 Â· a eâˆ’ax dx = 1
a2
 âˆ
0
t2 eâˆ’t dt = 2
a2 .
The variance of Y is
V {Y }
=
E

Y 2
âˆ’(E{Y })2 = E

X4
âˆ’
 2
a2
2
=
 âˆ
0
x4 a eâˆ’ax dx âˆ’4
a4
=
1
a4
 âˆ
0
t4 eâˆ’t dt âˆ’4
a4 = 4!
a4 âˆ’4
a4 = 24 âˆ’4
a4
= 20
a4 .
Example 10.2 Let X be rectangularly distributed over ] âˆ’h, h[.
Compute for k âˆˆN the moments
E

Xk
and
E

|X|k
.
We ï¬rst notice that
E

|X|k
= 1
2h
 h
âˆ’h
|x|k dx = 1
h
 h
0
xk dx =
hk
k + 1.
Then by the symmetry,
E

X2k+1
= 0,
k âˆˆN0,
2k + 1 odd,
Download free eBooks at bookboon.com

Random variables I
 
146 
10. Mean and variance in special cases
and
E

X2k
= E

|X|2k
=
h2k
2k + 1.
Example 10.3 Draw from the point (a, b), where b > 0, a line of the angle Î¸ with the line x = a,
where Î¸ âˆˆ
(
âˆ’Ï€
2 , Ï€
2
'
. The line intersects the x axis at a point of the abscissa X.
X
v
a.b
0
0.5
1
1.5
2
0.5
1
1.5
2
Assuming that Î˜(= v) is rectangularly distributed over
(
âˆ’Ï€
2 , Ï€
2
'
, ï¬nd the frequency of X.
Check if X has a mean.
The distribution of X is called a Cauchy distribution.
The relationship between the random variables X and Î˜ is
tan Î˜ = X âˆ’a
b
,
i.e.
Î˜ = Ï„ âˆ’1(X) = Arctan X âˆ’a
b
,
[and X = Ï„(Î˜) = a + b tan Î˜]. Since Î˜ has the frequency
f(Î¸) =
â§
âª
â¨
âª
â©
1
Ï€ ,
âˆ’Ï€
2 < Î¸ < Ï€
2 ,
0,
otherwise,
and Ï„ âˆ’1(x) âˆˆ
(
âˆ’Ï€
2 , Ï€
2
'
for every x âˆˆR, we derive that the frequency of X is
g(x) = f
	
Ï„ âˆ’1(x)

Â·

dÎ¸
dx
 = 1
Ï€ Â·
1
1 +
	 xâˆ’a
b

2 Â· 1
b =
b
Ï€ {b2 + (x âˆ’a)2},
x âˆˆR.
Download free eBooks at bookboon.com

Random variables I
 
147 
10. Mean and variance in special cases
It follows from

x g(x) dx = b
Ï€

x âˆ’a + a
b2 + (x âˆ’a)2 dx = b
2Ï€ ln

b2 + (x âˆ’a)2
+ a
Ï€ Arctan
x âˆ’a
b

,
that
 A
0
x Â· g(x) dx â†’âˆ
for A â†’âˆ,
(improper integral),
so X has no mean.
Alternatively,
|x| Â· g(x) âˆ¼k Â· 1
|x|
for large x,
hence
 âˆ
âˆ’âˆ
|x| g(x) dx = âˆ.
Example 10.4 A line segment of length 1 is divided randomly into two parts of lengths X and 1âˆ’X,
where we assume that X is rectangularly distributed over ]0, 1[.
Let Y denote the length of the smallest of the two line segments, and let Z denote the length of the
largest line segment.
1) Find the distribution of Y and the distribution of Z.
2) Find the mean and variance of Z.
1) The distribution function of X is
FX(x) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
for x â‰¤0,
x,
for 0 < x < 1,
1,
for x â‰¥1.
Obviously, Y = min{X, 1 âˆ’X} takes its values in

0, 1
2

. If y âˆˆ

0, 1
2

, then
P{X â‰¤y or 1 âˆ’X â‰¤y} = P{0 < X â‰¤y} + P{1 âˆ’y â‰¤X < 1} = 2y,
hence
FY (y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
1
for y â‰¥1
2,
2y
for 0 < y < 1
2,
0
for y â‰¤0,
and
fY (y) =
â§
â¨
â©
2
for 0 < y < 1
2,
0
otherwise.
Download free eBooks at bookboon.com

Random variables I
 
148 
10. Mean and variance in special cases
Analogously, Z = max{X, 1 âˆ’X} takes its values in
1
2, 1

. If z âˆˆ
1
2, 1

, then
FZ(z) = P{X â‰¤z and 1 âˆ’X â‰¤z} = P{1 âˆ’z â‰¤X â‰¤z} = 2z âˆ’1,
hence
FZ(z) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
1
for z â‰¥1,
2z âˆ’1
for 1
2 < z < 1,
0
for z â‰¤1
2,
and
fZ(z) =
â§
â¨
â©
2
for 1
2 < z < 1,
0
otherwise.
2) Since Y is rectangularly distributed over
$
0, 1
2
#
, we get
E{Y } = 1
4
and
V {Y } = 1
12 Â·
1
2
2
= 1
48.
Now, Z is rectangularly distributed over
1
2, 1

, so
E{Z} = 3
4
and
V {Z} = 1
12 Â·
1
2
2
= 1
48.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables I
 
149 
10. Mean and variance in special cases
Example 10.5 A point A is chosen randomly in the unit square ]0, 1[ Ã— ]0, 1[. This means that if X
and Y denote the abscissa and the ordinate, resp., of the point A, then (X, Y ) has the simultaneous
frequency
f(x, y) =
â§
â¨
â©
1,
in ]0, 1[ Ã— ]0, 1[,
0,
otherwise.
1) Find the probability of the event that the distance from A to a given edge of the square is â‰¤t.
2) Let U denote the distance from A to the closest edge of the square. Find the distribution function
and the frequency of U.
3) Find the mean and the variance of U.
Valgt side
A
t
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1) Obviously, the probability that the distance from A to e.g. ]0, 1[ on the x axis, is < t for 0 < t < 1,
hence, the corresponding random variable is rectangularly distributed over ]0, 1[.
2) If U denotes the distance from A to the closest edge, then U has its values in ]0, 1
2[. If u âˆˆ]0, 1
2[,
then U â‰¤u, if and only if A lies in the unions of the domains between a dotted line and the closest
parallel edge, so by considering an area,
FU(u) = 1 âˆ’(1 âˆ’2u)2 = 4
	
u âˆ’u2
= 4y âˆ’4u2,
0 < u < 1
2.
We conclude that the frequency is
fU(u) = F â€²
U(u) = 4 âˆ’8u,
0 < u < 1
2,
and fU(u) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables I
 
150 
10. Mean and variance in special cases
1â€“2u
1-u
u
1â€“2u
1-u
u
0
0.2
0.4
0.6
0.8
1
â€“0.2
0.2
0.4
0.6
0.8
1
3) The mean is
E{U} =

1
2
0
u(4 âˆ’8u) du =

1
2
0
	
4u âˆ’8u2
du =

2u2 âˆ’8
3 u3
 1
2
0
= 1
2 âˆ’1
3 = 1
6.
Then compute
E

U 2
=

1
2
0
u2(4 âˆ’8u) du =

1
2
0
	
4u2 âˆ’8u3
du =
4
3 u3 âˆ’2u4
 1
2
0
= 1
6 âˆ’1
8 = 1
24,
so the variance is
V {U} = E

U 2
âˆ’(E{U})2 = 1
24 âˆ’1
36 = 1
72.
Download free eBooks at bookboon.com

Random variables I
 
151 
10. Mean and variance in special cases
Example 10.6 The function f is for 0 < x < 1 given by
f(x) =
1
Ï€
%
x(1 âˆ’x)
,
while the function is equal to 0 for any other value of x.
1) Prove that f(x) is the frequency of a random variable X.
2) Find the mean and the variance of the random variable X.
3) Find the frequency of the random variable Y =
âˆš
X.
4) Find the mean of the random variable Y .
The distribution of X is called the Arcussinus distribution.
1) Obviously, f(x) â‰¥0 for every x âˆˆR. Then
 âˆ
âˆ’âˆ
f(x) dx
=
1
Ï€
 1
0
dx
%
x(1 âˆ’x)
= 1
Ï€

âˆ’1
2
1
2
dy
&	
y + 1
2

 	 1
2 âˆ’y

 = 1
Ï€

1
2
âˆ’1
2
dy
&
1
4 âˆ’y2
=
1
Ï€

1
2
âˆ’1
2
2 dy
%
1 âˆ’(2y)2 = 1
Ï€
 1
âˆ’1
dt
âˆš
1 âˆ’t2 = 1
Ï€ [Arcsin t]1
âˆ’1 = 1,
thus f(x) is the frequency of a random variable.
2) Since f(x) = 0 outside a bounded interval, all moments exist. In particular,
E{X}
=
 1
0
x f(x) dx = 1
Ï€
 1
0
x âˆ’1
2 + 1
2
âˆš
x âˆ’x2 dx
=
âˆ’1
2Ï€
 1
x=0
d
	
x2 âˆ’x

âˆš
x âˆ’x2 + 1
2 Â·
 1
0
f(x) dx = 0 + 1
2 = 1
2,
which can also be seen graphically, because the graph of f(x) is clearly symmetric with respect to
the line x = 1
2.
Furthermore,
E{X(X âˆ’1)} = 1
Ï€
 1
0
x(x âˆ’1)
%
x(1 âˆ’x)
dx = âˆ’1
Ï€
 1
0
%
x(1 âˆ’x) dx.
Since the graph of the integrand
%
x(1 âˆ’x) is a half circle of centre
	 1
2, 0

and radius r = 1
2, we
have
E{X(X âˆ’1)} = âˆ’1
Ï€ area

halvcirkel, radius 1
2

= âˆ’1
Ï€ Â· 1
2 Â· Ï€
1
2
2
= âˆ’1
8,
hence
E

X2
= E{X(X âˆ’1)} + E{X} = âˆ’1
8 + 1
2 = 3
8,
and
V {X} = E

X2
âˆ’(E{X})2 = 3
8 âˆ’
1
2
2
= 3
8 âˆ’1
4 = 1
8.
Download free eBooks at bookboon.com

Random variables I
 
152 
10. Mean and variance in special cases
3) Since y = Ïˆ(x) = âˆšx is a bijective map Ïˆ : ]0, 1[ â†’]0, 1[, with the inverse x = Ï•(x) = y2, where
dy
dx = 2y > 0, we conclude that the frequency of Y =
âˆš
X is
g(y) =
â§
âª
âª
â¨
âª
âª
â©
f(Ï•(y)) Â· Ï•â€²(y) =
2y
Ï€
%
y2 (1 âˆ’y2)
= 2
Ï€ Â·
1
%
1 âˆ’y2 ,
for y âˆˆ]0, 1[,
0,
otherwise,
corresponding to the distribution function
G(y) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
1,
for y â‰¥1,
2
Ï€ Arcsin y,
for y âˆˆ]0, 1[,
0,
for y â‰¤0.
4) The mean of Y is
E{Y } = 2
Ï€
 1
0
y
%
1 âˆ’y2 dy = 2
Ï€
'
âˆ’
%
1 âˆ’y2
(1
0 = 2
Ï€ .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables I
 
153 
10. Mean and variance in special cases
Example 10.7 1) Prove that the function
f(x) =
â§
âª
â¨
âª
â©
ab
a âˆ’b
	
eâˆ’bx âˆ’eâˆ’ax
,
for x â‰¥0,
0,
for x < 0,
where a and b denote positive constants, a Ì¸= b, can be considered as the frequency of a random
variable X.
2) Find the distribution function of the random variable X.
3) Find E{X} and V {X}, expressed by a and b .
4) Let a be a ï¬xed number. Prove for every ï¬xed x that
lim
bâ†’a f(x) = g(x),
where
g(x) =
â§
â¨
â©
a2x eâˆ’ax,
for x â‰¥0,
0,
for x < 0.
5) Prove that the function g(x) can be considered as a frequency of a random variable Y .
6) Finally, prove that E{X} â†’E{Y } for b â†’a.
1) We may assume that a > b. Then f(x) â‰¥0 for x âˆˆR, and
 âˆ
âˆ’âˆ
f(x) dx =
ab
a âˆ’b
 âˆ
0

eâˆ’bx âˆ’eâˆ’ax
dx =
ab
a âˆ’b
1
b âˆ’1
a

=
ab
a âˆ’b Â· a âˆ’b
ab
= 1,
thus f(x) can be considered as a frequency.
2) The distribution function F(x) of X is 0 for x â‰¤0. If x > 0, then
F(x)
=
ab
a âˆ’b
 x
0

eâˆ’bt âˆ’eâˆ’at
dt =
ab
a âˆ’b

âˆ’1
b eâˆ’bt + 1
a eâˆ’at
x
0
=
1
a âˆ’b
#
âˆ’a eâˆ’bt + b eâˆ’at$x
0
=
1
a âˆ’b

âˆ’a eâˆ’bx + b eâˆ’ax + a âˆ’b

= 1 +
1
a âˆ’b
	
b eâˆ’ax âˆ’a eâˆ’bx
.
3) We get by a partial integration, or by means of the Î“ integral,
E{X}
=
 âˆ
0
x f(x) dx =
ab
a âˆ’b
 âˆ
0
x

eâˆ’bx âˆ’eâˆ’ax
dx
=
ab
a âˆ’b
 1
b2
 âˆ
0
t eâˆ’t dt âˆ’1
a2
 âˆ
0
t eâˆ’t dt

=
ab
a âˆ’b
 1
b2 âˆ’1
a2

=
ab
a âˆ’b Â· a2 âˆ’b2
a2b2
= a + b
ab
= 1
a + 1
b ,
Download free eBooks at bookboon.com

Random variables I
 
154 
10. Mean and variance in special cases
and
E

X2
=
 âˆ
0
x2f(x) dx =
ab
a âˆ’b
 âˆ
0
x2 
eâˆ’bx âˆ’eâˆ’ax
dx
=
ab
a âˆ’b
 1
b3
 âˆ
0
t2eâˆ’t dt âˆ’1
a3
 âˆ
0
t2eâˆ’t dt

=
ab
a âˆ’b
 2
b3 âˆ’2
a3

= 2ab
a âˆ’b Â· a3 âˆ’b3
(ab)3
= 2 Â· a2 + ab + b2
(ab)2
=
2
 1
a2 + 1
ab + 1
b2

,
hence
V {X} = E

X2
âˆ’(E{X})2 = 2
 1
a2 + 1
ab + 1
b2

âˆ’
 1
a2 + 2
ab + 1
b2

= 1
a2 + 1
b2 .
4) Let x > 0 and a > 0 be ï¬xed. Then by e.g. lâ€™Hospitalâ€™s rule,
lim
bâ†’a
ab
a âˆ’b
	
eâˆ’bx âˆ’eâˆ’ax
= a2 lim
bâ†’a
eâˆ’bx âˆ’eâˆ’ax
a âˆ’b
= a2 lim
bâ†’a
âˆ’x eâˆ’bx
âˆ’1
= a2x eâˆ’ax.
For x â‰¤0 we get of course 0, thus
lim
bâ†’a f(x) = g(x).
5) Obviously, g(x) â‰¥0. Since
 âˆ
âˆ’âˆ
g(x) dx = a2
 âˆ
0
x eâˆ’ax dx =
 âˆ
0
t eâˆ’t dt = 1,
it follows that g(x) is the frequency of a random variable Y .
6) the mean of Y is
E{Y } =
 âˆ
âˆ’âˆ
x g(x) dx = a2
 âˆ
0
x Â· x eâˆ’ax dx = 1
a
 âˆ
0
t2eâˆ’t dt = 2
a.
It follows from (3) that
lim
bâ†’a E{X} = lim
bâ†’a
1
a + 1
b

= 2
a = E{Y }.
Remark 10.1 It is possible to give a simper solution. In fact, if X1 and X2 are independent random
variables of the frequencies
fX1(x) =
 a eâˆ’ax,
x â‰¥0,
0,
x < 0,
and
fX2(x) =

b eâˆ’bx,
x â‰¥0,
0,
x < 0,
Download free eBooks at bookboon.com

Random variables I
 
155 
10. Mean and variance in special cases
then X = X1 + X2 has the frequency f(x), and
E{X} = E {X1} + E {X2} = 1
a + 1
b ,
V {X} = V {X1} + V {X2} = 1
a2 + 1
b2 .
â™¦
Example 10.8 Let X and Y be independent random variables with the distributions given by
P{X = k} = P{Y = k} = pqk,
k âˆˆN0,
where p > 0, q > 0 and p + q = 1.
1) Find the means E{X} and E{Y }.
2) Find the variances V {X} and V {Y }.
3) Find P{X + Y = k}, k âˆˆN0.
1) The means are
E{X} = E{Y } =
âˆ

k=0
k Â· P{X = k} =
âˆ

k=1
kpqk = pq

k=1
k Â· qkâˆ’1 = pq Â·
1
(1 âˆ’q)2 = pq
p2 = q
p.
Here we have used that by a partial diï¬€erentiation with respect to q âˆˆ]0, 1[ we obtain the important
expressions
1
1 âˆ’q =
âˆ

k=0
qk
and
d
dq

1
1 âˆ’q

=
1
(1 âˆ’q)2 =
âˆ

k=1
k Â· qkâˆ’1.
2) The variance is found by a smart rearrangement,
V {X} = E

X2
âˆ’(E{X})2 = E{X(X âˆ’1)} + E{X} âˆ’(E{X})2,
where
E{X(X âˆ’1)}
=
âˆ

k=2
k(k âˆ’1) P{X = k} =
âˆ

k=2
k(k âˆ’1)pqk = pq2
âˆ

k=2
k(k âˆ’1)qkâˆ’2
=
pq2 d2
dq2
 âˆ

k=0
qk

= pq2 d2
dq2

1
1 âˆ’q

= pq2 Â·
2
(1 âˆ’q)3 = 2pq2
p3
= 2 Â· q2
p2 ,
i.e.
V {X} = V {Y } = 2 Â· q2
p2 + q
p âˆ’q2
p2 = q2
p2 + q Â· p
p2
= q(p + q)
p2
= q
p2 .
Notice that it is easier to compute E{X(X âˆ’1)} than
E

X2
=
âˆ

k=1
k2P{X = k} =
âˆ

k=1
k2pqk.
Download free eBooks at bookboon.com

Random variables I
 
156 
10. Mean and variance in special cases
3) Since X and Y are independent, we get for k âˆˆN0,
P{X + Y = k}
=
k

i=0
P{X = i âˆ§Y = k âˆ’i} =
k

i=0
P{X = i} Â· P{Y = k âˆ’i}
=
k

i=0
pq Â· pqkâˆ’i = p2qk
k

i=0
1 = (k + 1)p2qk,
k âˆˆN0.
Remark 10.2 The distribution of X is a reduced waiting time distribution. â™¦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables I
 
157 
10. Mean and variance in special cases
Example 10.9 There are given two components in an instrument. The life times of the components,
T1 and T2, are independent random variables, both of the frequency
f(t) =

a eâˆ’at,
t > 0,
0,
t â‰¤0,
where a is a positive constant.
We introduce the random variables X1, X2 and Y2 by
X1 = min {T1, T2} ,
X2 = max {T1, T2} ,
Y2 = X2 âˆ’X1.
Here, X1 denotes the time until the ï¬rst one of the components fails to function, and X2 indicates the
time until the second component also fails, while Y2 is the time from the failure of the ï¬rst component
until the failure of the second component.
1. Find the frequency and the mean of X1.
2. Find the mean of Y2.
Let it be given without proof that (X1, X2) has the simultaneous frequency
h (x1, x2) =

2a2eâˆ’a(x1+x2),
0 < x1 < x2,
0,
otherwise.
4. Find the simultaneous frequency of the 2-dimensional random variable (X1, Y2).
5. Find the frequency of Y2.
6. Check if the random variables X1 and Y2 are independent.
1) We get for X1,
P {X1 > x1} = P {T1 > x1 âˆ§T2 > x1} = P {T1 > x1} Â· P {T2 > x2} = eâˆ’2ax1,
thus
P {X1 â‰¤x1} = 1 âˆ’eâˆ’2ax1,
x1 > 0,
and we see that X1 is exponentially distributed with the frequency
fX1 =

2a eâˆ’2ax1,
x1 > 0,
0,
x1 â‰¤0,
and mean
1
2a.
2) For X2 we get
P {X2 â‰¤x2}
=
P {T1 â‰¤x2 âˆ§T2 â‰¤x2} = P {T1 â‰¤x2} Â· P {T2 â‰¤x2}
=
	
1 âˆ’eâˆ’ax2
2 ,
x2 > 0,
so X2 has the frequency
fX2 (x2) = 2a eâˆ’ax2 	
1 âˆ’eâˆ’ax2
= 2a eâˆ’ax2 âˆ’2a eâˆ’2ax2
for x2 > 0,
Download free eBooks at bookboon.com

Random variables I
 
158 
10. Mean and variance in special cases
and
fX2 (x2) = 0
for x2 â‰¤0.
The mean is
E {X2}
=
 âˆ
0
x2fX2 (x2) dx2 =
 âˆ
0

2a x2eâˆ’ax2 âˆ’2a x2eâˆ’2ax2
dx2
=
2
a âˆ’1
2a = 3
2a.
Additional. It is easy to ï¬nd the mean of X2 from X1 + X2 = T1 + T2, i.e.
E {X2} = E {T1} + E {T2} âˆ’E {X1} = 1
a + 1
a âˆ’1
2a = 3
2a.
3) This is trivial, because
E {Y2} = E {X2} âˆ’E {X1} = 3
2a âˆ’1
2a = 1
a.
4) The simultaneous frequency k (y1, y2) of
(Y1, Y2) = (X1, X2 âˆ’X1)
can e.g. be found directly from a convenient formula with a = 1, b = 0, c = âˆ’1 and d = âˆ’1,
k (y1, y2)
=
h
dy1 âˆ’by2
ad âˆ’bc , âˆ’cy1 + ay2
ad âˆ’bc

Â·
1
|ad âˆ’bc|
=
h (y1, y1 + y2) = 2a2eâˆ’a(2y1+y2)
for y1 > 0 og y2 > 0,
and
k (y1, y2) = 0
otherwise.
This is also written
k (y1, y2) =
 2a eâˆ’2ay1 Â· a eâˆ’ay2,
for y1 > 0 and y2 > 0,
0,
otherwise.
5) (and 6.) It follows immediately from 4. that Y1 (= X1) and Y2 are independent, and that Y2 has
the frequency
kY2 (y2) =

a eâˆ’ay2,
y2 > 0,
0,
y2 â‰¤0.
Download free eBooks at bookboon.com

Random variables I
 
159 
Index
Index
2-dimensional random variable, 5
almost everywhere, 7
Arcussinus distribution, 149
binomial distribution, 18
Buï¬€onâ€™s needle problem, 77
Cauchy distribution, 97, 111, 144
causal distribution, 4
Chebyshevâ€™s inequality, 13, 138, 139
conditional distribution, 11
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence in distribution, 16
convergence in probability, 16
convex function, 142
correlation, 15
covariance, 15
Cramerâ€™s formula, 108
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4, 20, 33
expectation, 11
fraud in Probability, 77
frequency, 5, 6, 20, 33
function of random variable, 75
function of random variables, 93
function of two random variables, 107
gamma distribution, 61
geometric distribution, 90
Helly-Brayâ€™s lemma, 16
independent random variables, 7
inequality between random variables, 81
Jacobian, 10, 54
Jensenâ€™s inequality, 142
law of total probability, 11
MAPLE, 22
marginal distribution, 5
marginal frequency, 6, 37, 54, 121
mean, 11, 129
median, 4, 21
moment, 12, 140, 143
moment of higher order, 129
null-set, 7
polar coordinates, 118
probability ï¬eld, 4
quantile, 4
random variable, 4
rectangular coordinates, 118
rectangular distribution, 93, 95, 98, 112, 143,
144
simultaneous distribution, 5
simultaneous distribution function, 6
simultaneous frequency, 54
transformation theorem, 8
weak law of large numbers, 16
Download free eBooks at bookboon.com


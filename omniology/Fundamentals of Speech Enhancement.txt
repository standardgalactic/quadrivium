123
SPRINGER BRIEFS IN 
ELECTRICAL AND COMPUTER ENGINEERING
Jacob Benesty
Fundamentals 
of Speech 
Enhancement

SpringerBriefs in Electrical and Computer
Engineering
Series editors
Woon-Seng Gan, Nanyang Technological University, Singapore, Singapore
C.-C. Jay Kuo, University of Southern California, Los Angeles, CA, USA
Thomas Fang Zheng, Tsinghua University, Beijing, China
Mauro Barni, University of Siena, Siena, Italy

SpringerBriefs present concise summaries of cutting-edge research and practical
applications across a wide spectrum of ﬁelds. Featuring compact volumes of 50 to
125 pages, the series covers a range of content from professional to academic.
Typical topics might include: timely report of state-of-the art analytical techniques,
a bridge between new research results, as published in journal articles, and a
contextual literature review, a snapshot of a hot or emerging topic, an in-depth case
study or clinical example and a presentation of core concepts that students must
understand in order to make independent contributions.
More information about this series at http://www.springer.com/series/10059

Jacob Benesty
Fundamentals of Speech
Enhancement
123

Jacob Benesty
INRS-EMT
University of Quebec
Montreal, QC
Canada
ISSN 2191-8112
ISSN 2191-8120
(electronic)
SpringerBriefs in Electrical and Computer Engineering
ISBN 978-3-319-74523-7
ISBN 978-3-319-74524-4
(eBook)
https://doi.org/10.1007/978-3-319-74524-4
Library of Congress Control Number: 2017963977
© The Author(s), under exclusive licence to Springer International Publishing AG, part of Springer
Nature 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by the registered company Springer International Publishing AG part
of Springer Nature
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Abstract
The content of this book is essentially theoretical. We present and develop
some very important concepts of speech enhancement in a simple but rigorous
way. Many ideas are new; not only they shed light on this old problem but also
give good hints on how to make things work better than some well-known con-
ventional approaches. With the proposed presentation, all aspects of speech
enhancement, from single channel, multichannel, beamforming, time domain,
frequency domain, time-frequency domain, to binaural, are uniﬁed in a clear
and ﬂexible framework. We start with an exhaustive discussion on the funda-
mental best (linear and nonlinear) estimators, from which we show how they
are connected to some important measures such as the coeﬃcient of determi-
nation, the correlation coeﬃcient, the conditional correlation coeﬃcient, and
the SNR. Then, in the subsequent chapters, we show how to exploit these
measures in order to derive all kinds of noise reduction algorithms that can
compromise in a very accurate and versatile way between noise reduction and
speech distortion.
v

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
General Formulation of the Speech Enhancement Problem . . .
1
1.2
Organization of the Work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Best Speech Enhancement Estimator in the Frequency
Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.1
Signal Model and Problem Formulation . . . . . . . . . . . . . . . . . . .
5
2.2
Laws of Total Expectation and Total Variance . . . . . . . . . . . . .
6
2.3
Best Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.4
Example with Gamma Distributions . . . . . . . . . . . . . . . . . . . . . .
11
2.4.1
Reformulation of the Problem and Approximation . . . .
11
2.4.2
Best Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.5
A Brief Study of the Best Quadratic Estimator . . . . . . . . . . . . .
15
2.6
Generalization to the Multichannel Case. . . . . . . . . . . . . . . . . . .
17
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3
Best Speech Enhancement Estimator in the Time Domain
23
3.1
Signal Model and Problem Formulation . . . . . . . . . . . . . . . . . . .
23
3.2
Best Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.3
Best Linear Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.4
Generalization to the Binaural Case . . . . . . . . . . . . . . . . . . . . . .
34
3.4.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.4.2
Best Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.4.3
Best Widely Linear Estimator. . . . . . . . . . . . . . . . . . . . . .
42
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
4
Speech Enhancement Via Correlation Coeﬃcients . . . . . . . .
45
4.1
Signal Model and Problem Formulation . . . . . . . . . . . . . . . . . . .
45
4.2
Linear Filtering and Correlation Coeﬃcients . . . . . . . . . . . . . . .
46
4.3
Optimal Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
vii

viii
Contents
4.3.1
SPCC Between Filter Output and Desired Signal . . . . .
49
4.3.2
SPCC Between Filter Output and Noise Signal . . . . . . .
55
4.3.3
SPCC Between Filter Output and Filtered Desired
Signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.3.4
Other Possibilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
5
On the Output SNR in Speech Enhancement and
Beamforming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
5.1
Signal Model and Problem Formulation . . . . . . . . . . . . . . . . . . .
65
5.2
Linear Filtering, Output and Fullmode Input SNRs . . . . . . . . .
66
5.3
Optimal Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
5.3.1
Rank-One Speech Covariance Matrix . . . . . . . . . . . . . . .
72
5.3.2
Rank-Deﬁcient Speech Covariance Matrix . . . . . . . . . . .
73
5.3.3
Full-Rank Speech Covariance Matrix . . . . . . . . . . . . . . . .
75
5.4
Application to Fixed and Superdirective Beamforming . . . . . .
77
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
6
Speech Enhancement from the Fullband Output SNR
Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
6.1
Signal Model and Problem Formulation . . . . . . . . . . . . . . . . . . .
83
6.2
Speech Enhancement with Gains . . . . . . . . . . . . . . . . . . . . . . . . .
84
6.3
Determination of the Optimal Gains . . . . . . . . . . . . . . . . . . . . . .
87
6.3.1
Maximization of the Fullband Output SNR . . . . . . . . . .
87
6.3.2
Minimization of the Fullband Output SNR . . . . . . . . . .
90
6.4
Taking the Interframe Correlation Into Account . . . . . . . . . . . .
92
6.5
Generalization to the Multichannel Case. . . . . . . . . . . . . . . . . . .
98
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

Chapter 1
Introduction
In this chapter, we brieﬂy explain what is speech enhancement and describe
its general formulation. Then, we present the organization of this study.
1.1 General Formulation of the Speech Enhancement
Problem
We are routinely surrounded by undesired signals, i.e., noise and interfer-
ences. In all applications that are related to speech, from sound recording,
cellular phones, hands-free communication, teleconferencing, hearing aids, to
human-machine interfaces, a speech signal of interest captured by micro-
phones is always contaminated by noise and interferences. Therefore, speech
enhancement algorithms are required in order to clean the noisy signals from
their disturbances. A solution to this problem was ﬁrst proposed and devel-
oped ﬁve decades ago by Schroeder at Bell Laboratories [1], [2]. Since then,
a lot of progress has been made and many approaches have been derived to
solve this fundamental problem with a single microphone, multiple micro-
phones, and in diﬀerent domains; see [3], [4], [5], [6] and references therein to
have a pretty good idea on how this topic has evolved.
The very general way to formulate the speech enhancement problem is
y = x + v,
(1.1)
where the three vectors y, x, and v, of the same length, are the observed
(or noisy), speech, and additive noise signals, respectively. All signals are
zero mean, and x and v are assumed to be independent. The disturbance
is due, obviously, to the signal vector v, which aﬀects both the quality and
intelligibility of the signal vector of interest x. Depending on the context
and how we want things to be processed, the desired signal can be the ﬁrst
element, x1, of x, a part of x, or the whole vector x. Then, the objective of
1
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_1

2
1 Introduction
speech enhancement is to estimate this desired signal from the observed signal
vector, y. For that, we need at least to estimate the second-order statistics
of y, i.e., its covariance matrix Φy.
With a single sensor and in the time domain, (1.1) is expressed as
y(t) =
[ y(t) y(t −1) · · · y(t −L + 1) ]T
= x(t) + v(t),
(1.2)
where t is the discrete-time index, the superscript T is the transpose operator,
and x(t) and v(t) are deﬁned similarly to y(t). The goal is then to estimate
x(t), the ﬁrst component of x(t), from the observed signal vector, y(t), which
contains L successive time samples picked up by the microphone.
Continuing with the single-channel case but in the time-frequency domain,
we can write (1.1), thanks to the short-time Fourier transform, as
Y (k, n) = X(k, n) + V (k, n),
(1.3)
where k and n are the frequency bin and the time frame, respectively. Again,
the objective is to estimate X(k, n) from Y (k, n).
In the multichannel scenario, i.e., with multiple (M) microphones, and in
the frequency domain, (1.1) becomes
y(f) = X(f)d(f) + v(f),
(1.4)
where y(f) is a vector of length M containing all the microphone signals at
the frequency index f and d is the known steering (or transfer function ratio)
vector whose ﬁrst element is 1. Then, the objective of multichannel speech
enhancement or beamforming is to estimate X(f) from y(f).
In this book, we discuss these diﬀerent (and more) aspects of speech en-
hancement in a uniﬁed way.
1.2 Organization of the Work
This work is organized into six chapters including this one. The best (linear
and nonlinear) estimators are great tools in statistical signal processing. In
Chapter 2, we show how they are applied to the frequency-domain speech
enhancement problem. We also write these best estimators as a function of
some important performance measures, which will be very useful in the rest
of this study. In Chapter 3, we continue our investigation of the best esti-
mators but in the time domain. We also deal with the best binaural speech
enhancement estimator. In Chapter 4, we focus on the linear case and show
how the most relevant noise reduction ﬁlters as well as new ones can be easily
derived from the correlation coeﬃcient. In Chapter 5, we discuss the impor-

References
3
tance of the output SNR and show how it can be used to ﬁnd fundamental
noise reduction ﬁlters and beamformers. Finally, in Chapter 6, we explain
why the fullmode output SNR is of great interest and from this measure we
derive a whole family of ﬁlters that can compromise very smoothly between
noise reduction and speech distortion.
References
1. M. R. Schroeder, “Apparatus for suppressing noise and distortion in communication
signals,” U.S. Patent No. 3,180,936, Filed 1 Dec. 1960, Issued 27 Apr. 1965.
2. M. R. Schroeder, “Processing of communication signals to reduce eﬀects of noise,”
U.S. Patent No. 3,403,224, Filed 28 May 1965, Issued 24 Sept. 1968.
3. J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, 2009.
4. J. Benesty and J. Chen, Optimal Time-Domain Noise Reduction Filters–A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, 2011.
5. J. Benesty, J. Chen, and E. Habets, Speech Enhancement in the STFT Domain.
Springer Briefs in Electrical and Computer Engineering, 2011.
6. J. Benesty and I. Cohen, Canonical Correlation Analysis in Speech Enhancement.
Springer Briefs in Electrical and Computer Engineering, 2018.

Chapter 2
Best Speech Enhancement Estimator
in the Frequency Domain
This chapter gives a fresh perspective on the best speech enhancement esti-
mator in the frequency domain. We consider the general nonlinear case. In
the ﬁrst part, we deal with the single-channel scenario, where an example is
studied with gamma distributions and a best quadratic estimator is derived.
Then, in the second part, we focus on the multichannel scenario. Along this
study, a great emphasis is put on some important performance measures
(such as the coeﬃcient of determination in the general nonlinear case and
the correlation coeﬃcient in the particular linear case) that can accurately
tell us how the best estimators behave.
2.1 Signal Model and Problem Formulation
In all this chapter, we drop the dependence on the frequency, f, to simplify
the notation. Therefore, for example, when we mention the random variable
A, we mean A(f).
Let X and V be two zero-mean, independent, and circular complex ran-
dom variables belonging to the same probability space. The signal model
considered in a large part of this chapter is [1], [2], [3]
Y = X + V,
(2.1)
where Y , X, and V are the observed, desired (speech), and noise signals,
respectively. Our objective is to estimate X in the best possible way in some
sense given Y . We can, equivalently, estimate V given Y ﬁrst and then sub-
tract this estimate from the observed signal to obtain the estimate of the
speech signal.
Assuming that all variances are ﬁnite, the variance of Y is
5
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_2

6
2 Best Estimator in the Frequency Domain
var (Y ) = E
(
|Y |2)
(2.2)
= var (X) + var (V ) ,
where E(·) is the mathematical expectation, and var (X) and var (V ) are the
variances of X and V , respectively. From (2.2), it is easy to see that the input
signal-to-noise ratio (SNR) is
iSNR = var (X)
var (V ) .
(2.3)
We recall that the SNR is one of the most meaningful measures in speech
enhancement.
2.2 Laws of Total Expectation and Total Variance
We start this section by giving two important properties: the law of total
expectation (or the law of iterated expectations) and the law of total variance
(or Eve’s law).
Let A and B be two circular complex random variables. The conditional
expectation of A given B is the random variable E (A |B ), whose randomness
is inherited from B. The law of total expectation says that A and E (A |B )
have the same mean, i.e., [4]
E (A) = E [E (A |B )] ,
(2.4)
where the outer expectation on the right-hand side of (2.4) is over the distri-
bution of B. In fact, we can easily show the more general result:
E [f(B)A] = E {E [f(B)A |B ]}
(2.5)
= E [f(B)E (A |B )] ,
where f(B) is any function of B.
The law of total variance states that [4]
var (A) = E [var (A |B )] + var [E (A |B )] ,
(2.6)
where the outer expectation and the outer variance on the right-hand side of
(2.6) are over the distribution of B. This property can be proved by using the
law of total expectation. Basically, (2.6) says that the conditional variance
on average is smaller than the variance, which actually makes perfect sense
since the uncertainty is reduced.
Since variances are always nonnegative, we deduce from (2.6) that

2.3 Best Estimator
7
0 ≤var [E (A |B )]
var (A)
≤1.
(2.7)
As a result, the classical magnitude squared coherence function can be gen-
eralized to
γA|B
2 = 1 −E [var (A |B )]
var (A)
(2.8)
= var [E (A |B )]
var (A)
.
This expression is often called the coeﬃcient of determination in the literature
of statistics [5]. If A and B are independent, then var (A |B ) = var (A). As a
consequence,
γA|B
2 = 0. Conversely, if
γA|B
2 = 0, then var [E (A |B )] = 0,
which implies that A and B are independent. At the other limiting case,
if A = B, then var (B |B ) = 0, which leads to
γA|B
2 = 1. In fact, for
A = f(B), we have E (A |B ) = E [f(B) |B ] = f(B); as a result,
γA|B
2 = 1.
This coeﬃcient of determination, which is a direct consequence of the law
of total variance and measures how close A is to E (A |B ), plays a key role
in the best estimator in general and in speech enhancement in particular.
It can be used as a powerful performance measure in all aspects of speech
enhancement as explained in great details in the rest.
2.3 Best Estimator
Returning to our signal model in (2.1), it is well known that the best estimator
of X in the minimum mean-squared error (MMSE) sense is the conditional
expectation of X given Y [6], i.e.,
E (X |Y ) = ZX(Y ).
(2.9)
Indeed, let fX(Y ) be any (linear or nonlinear) function of Y , we always have
E
[
|X −ZX(Y )|2]
= E
(
|EX|2)
≤E
[
|X −fX(Y )|2]
,
(2.10)
where EX = X −E (X |Y ) is the error signal between the desired signal and
its best estimator. By virtue of the law of total expectation, the two random
variables X and ZX(Y ) have the same mean, i.e.,
E [ZX(Y )] = E (X) = 0.
(2.11)
It can also be veriﬁed that the MMSE is

8
2 Best Estimator in the Frequency Domain
E
(
|EX|2)
= E [var (X |Y )]
(2.12)
= var (X) −var [ZX(Y )]
= var (X)
(
1 −
γX|Y
2)
,
which clearly depends on the coeﬃcient of determination.
In the same way, the best estimator of V in the MMSE sense is the con-
ditional expectation of V given Y , i.e.,
E (V |Y ) = ZV (Y )
(2.13)
and for any (linear or nonlinear) function of Y , fV (Y ), we always have
E
[
|V −ZV (Y )|2]
= E
(
|EV |2)
≤E
[
|V −fV (Y )|2]
,
(2.14)
where EV = V −E (V |Y ) is the error signal between the noise and its best
estimator. By virtue of the law of total expectation, the two random variables
V and ZV (Y ) have the same mean, i.e.,
E [ZV (Y )] = E (V ) = 0.
(2.15)
We also deduce that the MMSE is
E
(
|EV |2)
= var (V )
(
1 −
γV |Y
2)
.
(2.16)
By adding together the best estimator of X and the best estimator of V ,
we obtain the observed signal, i.e.,
Y = E (Y |Y )
= E (X |Y ) + E (V |Y ) .
(2.17)
The above property is very interesting. As expected, it shows that the esti-
mation errors of both estimators cancel out. In other words, the best esti-
mator of X can be found, equivalently, from the best estimator of V . In the
best estimator of X, E (X |Y ) gives the speech distortion perspective while
Y −E (V |Y ) gives the noise reduction perspective. From (2.17), we easily
see that EX = −EV and, as a result, E
(
|EX|2)
= E
(
|EV |2)
. Then, equating
(2.12) and (2.16), we obtain
iSNR +
γV |Y
2 = 1 + iSNR ×
γX|Y
2 .
(2.18)
From the previous expression, we have

2.3 Best Estimator
9
lim
iSNR→0
γV |Y
2 = 1,
(2.19)
lim
iSNR→∞
γX|Y
2 = 1.
(2.20)
In words, the best estimator is able to completely remove the noise when the
input SNR is close to 0 and fully recover the desired signal when the input
SNR approaches inﬁnity. However, (2.18) does not give us any information
about speech distortion in the ﬁrst case and noise reduction in the second one.
These statements make intuitively sense and conﬁrm what we have always
observed for the best estimator. We also see from (2.18) that there is not
such a thing such as distortionless (i.e.,
γX|Y
2 = 1) with the best estimator
in general, unless the noise is completely removed (i.e.,
γV |Y
2 = 1) at the
same time.
In the pathological scenario where X and V are independent and identi-
cally distributed (i.i.d.), we have
E (X |Y ) = E (V |Y ) = Y
2 .
(2.21)
As a result, single-channel speech enhancement is not feasible with the best
estimator. So when the distribution of the noise resembles the one of the
speech, we should not expect much noise reduction, not because of the prob-
lem of estimating statistics of nonstationary signals but because the distri-
butions of the speech and noise may be similar. In this diﬃcult scenario, the
only way to attenuate the level of the noise is to use more than one sensor
(see Section 2.6).
Best Linear Estimator
It is of great interest to study the best linear estimator because of its simple
form. The study of this very important particular case can also lead to better
insights into the best estimator in general.
It is well known that the best linear estimator of X in the MMSE sense is
E (X |Y ) = HX,WY,
(2.22)
where
HX,W = var (X)
var (Y )
(2.23)
=
iSNR
1 + iSNR
is the celebrated Wiener gain. In this case, the coeﬃcient of determination
and the MMSE are, respectively,

10
2 Best Estimator in the Frequency Domain
γX|Y
2 = HX,W ≤1
(2.24)
and
E
(
|X −HX,WY |2)
= var (X) (1 −HX,W) .
(2.25)
The coeﬃcient of determination,
γX|Y
2, is a good measure of the desired
signal distortion; a value close to 1 (resp. 0) means low (resp. large) distortion.
In the same manner, the best linear estimator of V in the MMSE sense is
E (V |Y ) = HV,WY,
(2.26)
where
HV,W = var (V )
var (Y )
(2.27)
=
1
1 + iSNR.
We deduce that the coeﬃcient of determination and the MMSE are, respec-
tively,
γV |Y
2 = HV,W ≤1
(2.28)
and
E
(
|V −HV,WY |2)
= var (V ) (1 −HV,W) .
(2.29)
The coeﬃcient of determination,
γV |Y
2, is a good measure of noise reduc-
tion; a value close to 1 (resp. 0) means large (resp. low) noise reduction.
It is clear that
Y = E (X |Y ) + E (V |Y )
(2.30)
or, equivalently,
1 = HX,W + HV,W.
(2.31)
We also have
1 =
γX|Y
2 +
γV |Y
2 ,
(2.32)
which is only true for the best linear estimator. This relation shows the
fundamental compromise between noise reduction and speech distortion, in
the single-channel case and with Gaussian signals, as
γX|Y
2 and
γV |Y
2 go
in opposite directions. For large noise reduction (resp. low speech distortion),

2.4 Example with Gamma Distributions
11
γV |Y
2 (resp.
γX|Y
2) is rather close to 1, so that
γX|Y
2 (resp.
γV |Y
2) is
close to 0 implying large speech distortion (resp. low noise reduction).
We can state that nonlinear noise reduction in the single-channel case is
extremely eﬃcient if the following condition holds
γX|Y
2 +
γV |Y
2 ≈2.
(2.33)
Under this condition, which can be fulﬁlled depending on the distributions of
X and V but certainly not when they are both Gaussians, we can have large
noise reduction and low speech distortion. It can be very instructive to ﬁnd
some distributions for which (2.33) is more or less fulﬁlled. This is certainly
possible since
γX|Y
2 =
γV |Y
2 = 1 is a solution of (2.18).
This study suggests that, in the general case, we can combine the speech
distortion and noise reduction measures into one convenient measure:
ϱSC =
γX|Y
2 +
γV |Y
2 ,
(2.34)
where 1 ≤ϱSC ≤2, with the subscript SC standing for single channel. For ϱSC
close to 2, we have the almost perfect estimator while for ϱSC close to 1, we
deal with the linear case and the well-known unavoidable compromise. There-
fore, the larger is ϱSC, the less the compromise between speech distortion and
noise reduction.
We conclude this part by saying that
γX|Y
2,
γV |Y
2, and ϱSC are accu-
rate, convenient, and most useful performance measures for the evaluation
of the single-channel speech enhancement problem with the best estimator.
The ﬁrst measure quantiﬁes distortion of the desired signal, the second one
evaluates noise reduction, and the last one tells us about the compromise.
2.4 Example with Gamma Distributions
The study of this section is somewhat an extension of the works presented in
[7], [8], [9], [10].
2.4.1 Reformulation of the Problem and
Approximation
We can also express (2.1) as
|Y |eȷθY = |X|eȷθX + |V |eȷθV ,
(2.35)

12
2 Best Estimator in the Frequency Domain
where ȷ is the imaginary unit, and θY , θX, and θV are the phases of Y , X,
and V , respectively. In the rest, we will use the approximation:
|Y | = eY ≈e
X + eV = |X| + |V |,
(2.36)
as it is very often the case in the single-channel speech enhancement problem
in the frequency domain. Therefore, we have eY ∈[0, ∞) and e
X, eV ∈[0, eY ].
As a result, when the estimator be
X is derived for the magnitude of the speech,
the estimator of X is
b
X = be
XeȷθY .
(2.37)
Then, our objective is to derive and evaluate the best estimator of e
X, from
an MMSE perspective, with gamma distributions.
2.4.2 Best Estimator
It is well known that the modulus of the desired speech signal can be well
modeled with the gamma distribution:
p e
X
(
e
X
)
=
λα
Γ (α)
e
Xα−1e−λ e
X, e
X ≥0,
(2.38)
where α > 0 is the shape parameter, λ > 0 the scale parameter, and Γ(·) the
gamma function. The gamma distributed random variable e
X is denoted
e
X ∼Γα,λ.
(2.39)
The mean of e
X can be easily calculated; it is given by
E
(
e
X
)
= α
λ.
(2.40)
The magnitude of the noise can also be modeled with the gamma distri-
bution but with a diﬀerent shape parameter, β > 0, i.e.,
peV
(
eV
)
=
λβ
Γ (β)
eV β−1e−λeV , eV ≥0,
(2.41)
which we denote eV ∼Γβ,λ. The mean of eV is then
E
(
eV
)
= β
λ.
(2.42)
It can be veriﬁed that

2.4 Example with Gamma Distributions
13
peY
(
eY
)
= p e
X+eV
(
eY
)
=
∫eY
0
p e
X
(
e
X
)
peV
(
eY −e
X
)
d e
X
=
λα+β
Γ (α + β)
eY α+β−1e−λeY ,
(2.43)
meaning that eY is also a gamma distributed random variable, i.e., eY ∼
Γα+β,λ.
The joint distribution of eY and e
X is
peY , e
X
(
eY , e
X
)
= peV , e
X
(
eY −e
X, e
X
)
= peV
(
eY −e
X
)
p e
X
(
e
X
)
,
(2.44)
where the last equation is the consequence of the fact that e
X and eV are
independent. Therefore, the conditional distribution of e
X given eY is
p e
X|eY
(
e
X
eY
)
=
peY , e
X
(
eY , e
X
)
peY
(
eY
)
=
peV
(
eY −e
X
)
p e
X
(
e
X
)
peY
(
eY
)
.
(2.45)
Substituting (2.38), (2.41), and (2.43) into (2.45), we easily ﬁnd that
p e
X|eY
(
e
X
eY
)
= Γ (α + β)
Γ (α) Γ (β) × 1
e
X
×
( e
X
eY
)α (
1 −
e
X
eY
)β−1
.
(2.46)
Now, we have everything to ﬁnd the best estimator in the MMSE sense:
be
X = E
(
e
X
eY
)
=
∫eY
0
e
Xp e
X|eY
(
e
X
eY
)
d e
X
= Γ (α + β)
Γ (α) Γ (β)
∫eY
0
( e
X
eY
)α (
1 −
e
X
eY
)β−1
d e
X.
(2.47)
Making the change of variables U = e
X/eY , we can write the previous expres-
sion as

14
2 Best Estimator in the Frequency Domain
E
(
e
X
eY
)
= eY Γ (α + β)
Γ (α) Γ (β)B (α + 1, β) ,
(2.48)
where
B (α, β) =
∫1
0
U α−1 (1 −U)β−1 dU
(2.49)
= Γ (α) Γ (β)
Γ (α + β)
is the beta function. Using the relationship:
Γ (α + 1) = αΓ (α) ,
(2.50)
the best estimator simpliﬁes to
E
(
e
X
eY
)
= eY
α
α + β
= eY
E
(
e
X
)
E
(
e
X
)
+ E
(
eV
).
(2.51)
Obviously, the estimator in (2.51) leads to the MMSE, assuming the approx-
imation in (2.36). We see that noise reduction is possible because the two
distributions of the speech and noise have diﬀerent shapes.
Finally, we deduce that our estimator is
b
XG = HGY,
(2.52)
where
HG =
E (|X|)
E (|X|) + E (|V |)
(2.53)
is a positive gain. It is of interest to compare this approach to the classical
Wiener gain technique:
b
XW = HWY,
(2.54)
where
HW =
E
(
|X|2)
E (|X|2) + E (|V |2).
(2.55)
From (2.52), we easily ﬁnd that the noise reduction factor and the speech
distortion index are, respectively,

2.5 A Brief Study of the Best Quadratic Estimator
15
ξnr (HG) =
1
H2
G
(2.56)
and
υsd (HG) = (1 −HG)2 .
(2.57)
2.5 A Brief Study of the Best Quadratic Estimator
From Section 2.3, we know that any random variable X can be decomposed
as
X = E (X |Y ) + EX,
(2.58)
where EX is a zero-mean random variable with E (EX |Y )
=
0 and
E [f(Y )EX] = 0, and f(Y ) being any function of Y . Obviously, the same
decomposition applies for the noise signal, V . In this section, we assume that
at least one of the two signals X and V is not Gaussian. Therefore, we can
generalize the linear model to the quadratic one:
E (X |Y ) = eH∗
X,1Y + eH∗
X,2Y |Y | = ehH
Xey,
(2.59)
E (V |Y ) = eH∗
V,1Y + eH∗
V,2Y |Y | = ehH
V ey,
(2.60)
where the superscript ∗and H are the complex-conjugate and conjugate-
transpose operators, ehX and ehV are two complex-valued ﬁlters of length 2,
and
ey =
[ Y Y |Y |]T .
For convenience, we also deﬁne the two vectors of length 2:
ex =
[ X X|X|]T ,
ev =
[ V V |V |]T .
The minimization of E
(
|EX|2)
and E
(
|EV |2)
leads to the best quadratic
estimators:
ehX,Q = cov−1 (ey) cov (ey, ex) i,
(2.61)
ehV,Q = cov−1 (ey) cov (ey, ev) i,
(2.62)
where cov (ey) = E
(eyeyH)
, cov (ey, ex) = E
(eyexH)
, cov (ey, ev) = E
(eyevH)
, and
i =
[ 1 0 ]T . It is clear that

16
2 Best Estimator in the Frequency Domain
ehH
X,Qey + ehH
V,Qey = Y.
(2.63)
We deduce that the MMSEs are
E
(X −ehH
X,Qey

2)
= var (X)
(
1 −
eγX|Y
2)
,
(2.64)
E
(V −ehH
V,Qey

2)
= var (V )
(
1 −
eγV |Y
2)
,
(2.65)
where
eγX|Y
2 = iT cov (ex, ey) cov−1 (ey) cov (ey, ex) i
var (X)
,
(2.66)
eγV |Y
2 = iT cov (ev, ey) cov−1 (ey) cov (ey, ev) i
var (V )
.
(2.67)
Property 2.1. We have
eγX|Y
2 ≥
γX|Y
2 ,
(2.68)
eγV |Y
2 ≥
γV |Y
2 ,
(2.69)
where
γX|Y
2 = var (X) /var (Y ) and
γV |Y
2 = var (V ) /var (Y ) are the
coeﬃcients of determination for the best linear estimators (see Section 2.3).
Proof. Let us deﬁne the normalized covariance matrix:
covn (ey, ex) = cov (ey, ex)
var (X) .
(2.70)
It is easy to verify that the ﬁrst element of the vector covn (ey, ex) i is 1. We
can express (2.66) as
eγX|Y
2 = var (X) iT covn (ex, ey) cov−1 (ey) covn (ey, ex) i.
(2.71)
Using the Cauchy-Schwarz inequality:
[
iT covn (ex, ey) cov−1 (ey) covn (ey, ex) i
] [
iT cov (ey) i
]
≥
iT covn (ey, ex) i
2 = 1
(2.72)
and substituting this result into (2.71), we ﬁnd the inequality in (2.68). The
inequality in (2.69) can be shown in the exact same way. Inequalities in (2.68)
and (2.69) are also consequences of the MMSE.
From Property 2.1, we can say that the best quadratic estimator reduces
more noise and distorts less the desired speech than the best linear estimator.
We also have

2.6 Generalization to the Multichannel Case
17
eϱSC =
eγX|Y
2 +
eγV |Y
2 ≥1,
(2.73)
which means that the best quadratic estimator better compromises between
noise reduction and speech distortion than the best linear estimator.
2.6 Generalization to the Multichannel Case
In the multichannel context, we assume that we have M sensors and, hence,
M observations. Therefore, the observation signal vector is given by [11]
y =
[ Y1 Y2 · · · YM
]T
= Xd + v,
(2.74)
where X is the zero-mean random desired signal, d is the known steering (or
transfer function ratio) vector whose ﬁrst element is 1, and v is the zero-mean
random noise vector. Assuming that X and v are independent, we deduce
that the covariance matrix of y is
cov (y) = E
(
yyH)
(2.75)
= cov (x) + cov (v)
= var (X) ddH + cov (v) ,
where var (X) is the variance of X, and cov (x) and cov (v) are the covariance
matrices of x and v, respectively.
Considering the random variable X and the random vector y from the
signal model in (2.74), the law of total variance is
var (X) = E [var (X |y)] + var [E (X |y)] .
(2.76)
As a consequence, the coeﬃcient of determination is
γX|y
2 = 1 −E [var (X |y)]
var (X)
(2.77)
= var [E (X |y)]
var (X)
.
This measure is close to 1 when there is little noise and may get smaller
when the noise increases. Let V1 be the ﬁrst component of v. The law of
total variance and the coeﬃcient of determination are, respectively,
var (V1) = E [var (V1 |y)] + var [E (V1 |y)]
(2.78)
and

18
2 Best Estimator in the Frequency Domain
γV1|y
2 = 1 −E [var (V1 |y)]
var (V1)
(2.79)
= var [E (V1 |y)]
var (V1)
,
where var (V1) is the variance of V1. We see that when the noise dominates,
γV1|y
2 is close to 1.
Similar to the single-channel case, the best estimator of X in the MMSE
sense is
E (X |y) = ZX(y)
(2.80)
and the MMSE is
E
[
|X −ZX(y)|2]
= E [var (X |y)]
(2.81)
= var (X) −var [ZX(y)]
= var (X)
(
1 −
γX|y
2)
.
Also, the best estimator of V1 in the MMSE sense is
E (V1 |y) = ZV1(y)
(2.82)
and the MMSE is
E
[
|V1 −ZV1(y)|2]
= E [var (X |y)]
(2.83)
= var (V1) −var [ZV1(y)]
= var (V1)
(
1 −
γV1|y
2)
.
Let Y1 = X + V1 be the ﬁrst component of y. We have
Y1 = E (Y1 |y)
(2.84)
= E (X |y) + E (V1 |y) .
This means that if we know E (X |y), we can deduce E (V1 |y), and vice
versa. Also, in the best estimator of X, E (X |y) gives the speech distortion
perspective while Y1 −E (V1 |y) gives the noise reduction perspective. We
have the fundamental relation:
iSNRSC +
γV1|y
2 = 1 + iSNRSC ×
γX|y
2 ,
(2.85)
where
iSNRSC = var (X)
var (V1)
(2.86)

2.6 Generalization to the Multichannel Case
19
is the input SNR at the ﬁrst sensor, which is equivalent to the single-channel
input SNR.
Now, let us focus on the best linear estimators for X and V1 in the MMSE
sense. For X, we have
E (X |y) = hH
X,Wy,
(2.87)
where
hX,W = var (X) cov−1 (y) d
(2.88)
is the multichannel Wiener ﬁlter. We ﬁnd that the coeﬃcient of determination
is
γX|y
2 = var (X) dHcov−1 (y) d
(2.89)
=
var (X) dHcov−1 (v) d
1 + var (X) dHcov−1 (v) d,
which is a good measure of speech distortion. By analogy to the single-channel
case, another interesting way to deﬁne the input SNR in the multichannel
case is the fullmode input SNR (see Chapter 5 for a detailed discussion on
this measure):
iSNRFM = tr
[
cov−1 (v) cov (x)
]
M
(2.90)
= var (X) dHcov−1 (v) d
M
,
where tr[·] is the trace of a square matrix. Therefore, we can express (2.89)
as
γX|y
2 =
M × iSNRFM
1 + M × iSNRFM
≤1,
(2.91)
which strongly depends on M. As the number of microphones increases, this
measure gets closer to 1, which of course makes sense since increasing M
improves the estimator. For V1, we have
E (V1 |y) = hH
V1,Wy,
(2.92)
where
hV1,W = cov−1 (y) cov (v) i
(2.93)
is the multichannel Wiener ﬁlter for the estimation of V1, with i being the ﬁrst
column of the M × M identity matrix IM. The coeﬃcient of determination
is then

20
2 Best Estimator in the Frequency Domain
γV1|y
2 = iT cov (v) cov−1 (y) cov (v) i
var (V1)
(2.94)
= 1 + M × iSNRFM −iSNRSC
1 + M × iSNRFM
≤1,
which is a good measure of noise reduction.
We now give an important property.
Property 2.2. Let
ϱMC =
γX|y
2 +
γV1|y
2
(2.95)
be the combined speech distortion and noise reduction measures in the mul-
tichannel case. With the best linear estimators, we always have
ϱMC ≥1.
(2.96)
Proof. Using (2.91) and (2.94), it is easy to see that
ϱMC = 1 + M × iSNRFM −iSNRSC
1 + M × iSNRFM
.
(2.97)
We need to show that the quantity M × iSNRFM −iSNRSC is positive, i.e.,
M × iSNRFM −iSNRSC = var (X)
[
dHcov−1 (v) d −
1
var (V1)
]
(2.98)
≥0.
From the Cauchy-Schwarz inequality, we have
iT d
2 = 1 ≤
[
iT cov (v) i
] [
dHcov−1 (v) d
]
,
(2.99)
implying that
dHcov−1 (v) d ≥
1
var (V1).
(2.100)
As a result, ϱMC ≥1.
Therefore, we always have 1 ≤ϱMC ≤2. This is the fundamental diﬀer-
ence with the single-channel case, where ϱSC = 1, showing the compromise
between noise reduction and speech distortion, while in the multichannel sce-
nario, we can limit this distortion and have more noise reduction by using
more microphones. In fact, it is easy to show that
γX|y
2 ≥
γX|Y
2 and
γV |y
2 ≥
γV |Y
2, meaning that the multichannel best linear estimator dis-
torts less the desired speech and reduces more noise than the single-channel
best linear estimator. One can also verify that

2.6 Generalization to the Multichannel Case
21
lim
M→∞ϱMC = 2.
(2.101)
It can be checked that
Y1 = E (X |y) + E (V1 |y)
(2.102)
or, equivalently,
i = hX,W + hV1,W.
(2.103)
It is interesting to observe that the coeﬃcients of determination can also be
expressed as
γX|y
2 = hH
X,Wd
(2.104)
and
γV1|y
2 =
hH
V1,Wcov (v) i
var (V1)
,
(2.105)
which are well-known measures of the desired signal distortion and noise
reduction in the multichannel case with linear estimators. Indeed, the
closer hH
X,Wd is to 1, the less distorted the speech signal, and the closer
hH
V1,Wcov (v) i is to var (V1), the more noise reduction.
From (2.85) and (2.95), we deduce for the best estimator (linear or not)
that
γX|y
2 = iSNRSC −1 + ϱMC
1 + iSNRSC
(2.106)
and
γV1|y
2 = ϱMC × iSNRSC + 1 −iSNRSC
1 + iSNRSC
.
(2.107)
The two previous expressions tell us the following. For a low input SNR,
γV1|y
2 is close to 1, meaning that there is a good amount of noise reduction;
however,
γX|y
2 depends mostly on ϱMC−1, meaning that distortion depends
on the number of microphones and the distributions of X and V1. For a
large input SNR,
γX|y
2 is close to 1, meaning that there is low distortion;
however,
γV1|y
2 is close to ϱMC −1, meaning that noise reduction depends
on the number of microphones and the distributions of X and V1. For a
large number of sensors, the eﬀect of the distributions of X and V on the
performance of the multichannel best estimator becomes negligible.

22
2 Best Estimator in the Frequency Domain
References
1. J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, 2009.
2. P. Loizou, Speech Enhancement: Theory and Practice. Boca Raton, FL: CRC Press,
2007.
3. J. Benesty, J. Chen, and E. Habets, Speech Enhancement in the STFT Domain.
Springer Briefs in Electrical and Computer Engineering, 2011.
4. N. A. Weiss, P. T. Holmes, and M. Hardy, A Course in Probability. Boston: Addison-
Wesley, 2005.
5. R. Steyer, “Conditional expectations: an introduction to the concept and its applica-
tions in empirical sciences,” Methodika, vol. 2, issue 1, pp. 53–78, 1988.
6. S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory. Upper
Saddle River, NJ: Prentice Hall PTR, 1993.
7. R. Martin, “Speech enhancement using MMSE short time spectral estimation with
gamma distributed speech priors,” in Proc. IEEE ICASSP, 2002, pp. I-253–I-256.
8. R. C. Hendriks, J. S. Erkelens, J. Jensen, and R. Heusdens, “Minimum mean-square
error amplitude estimators for speech enhancement under the generalized gamma dis-
tribution,” in Proc. IWAENC, 2006.
9. J. S. Erkelens, R. C. Hendriks, R. Heusdens, and J. Jensen, “Minimum mean-square
error estimation of discrete Fourier coeﬃcients with generalized gamma priors,” IEEE
Trans. Audio, Speech, Language Process., vol. 15, pp. 1741–1752, Aug. 2007.
10. B. Fodor and T. Fingscheidt, “MMSE speech enhancement under speech presence
uncertainty assuming (generalized) gamma speech priors throughout,” in Proc. IEEE
ICASSP, 2012, pp. 4033–4036.
11. J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin, Ger-
many: Springer-Verlag, 2008.

Chapter 3
Best Speech Enhancement Estimator
in the Time Domain
In this chapter, we study the best speech enhancement estimator in the time
domain. The ﬁrst part focuses on the single-channel scenario, where impor-
tant insights are given thanks to diﬀerent kinds of correlation coeﬃcients;
in the linear case, we obtain the well-known Wiener ﬁlter whose functioning
is explained within this general framework. The second part deals with the
best binaural speech enhancement estimator; the approach taken here is by
the reformulation of the binaural problem into a monaural one thanks to
complex random variables. As a consequence, the linear case results in the
widely linear Wiener ﬁlter.
3.1 Signal Model and Problem Formulation
In the ﬁrst part of this chapter, we are concerned with the speech enhance-
ment (or noise reduction) problem, in which the time-domain desired signal,
xt, with t being the discrete-time index, needs to be recovered from the noisy
observation [1], [2], [3], [4]:
yt = xt + vt,
(3.1)
where vt is the unwanted additive noise signal, which is assumed to be in-
dependent of xt. All signals are considered to be real, zero mean, stationary,
and broadband.
The signal model given in (3.1) can be put into a vector form by considering
the L most recent successive time samples, i.e.,
yt =
[ yt yt−1 · · · yt−L+1
]T
= xt + vt,
(3.2)
23
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_3

24
3 Best Estimator in the Time Domain
where yt is a vector of length L, and xt and vt are deﬁned in a similar way
to yt.
Since xt and vt are independent by assumption, the covariance matrix (of
size L × L) of the noisy signal can be written as
cov (yt) = E
(
ytyT
t
)
(3.3)
= cov (xt) + cov (vt) ,
where cov (xt) and cov (vt) are the covariance matrices of xt and vt, respec-
tively. From (3.3), we deduce that the input SNR is
iSNR = tr [cov (xt)]
tr [cov (vt)]
(3.4)
= var (xt)
var (vt) ,
where var (xt) = E
(
x2
t
)
and var (vt) = E
(
v2
t
)
are the variances of xt and vt,
respectively.
Another important measure in the context of speech enhancement is the
squared Pearson correlation coeﬃcient (SPCC) [1], [5]. It is easy to see that
the SPCC between xt and yt is
ρ2
xt,yt =
cov2 (xt, yt)
var (xt) var (yt)
=
E2 (xtyt)
E (x2
t) E (y2
t )
=
iSNR
1 + iSNR.
(3.5)
In the same way, the SPCC between vt and yt is
ρ2
vt,yt =
E2 (vtyt)
E (v2
t ) E (y2
t )
=
1
1 + iSNR.
(3.6)
As a result,
1 = ρ2
xt,yt + ρ2
vt,yt.
(3.7)
This shows how the important correlation coeﬃcients are naturally related
to the input SNR since they also equivalently tell us how the observed signal
is noisy.
As it can be guessed, in this single-channel noise reduction problem, our
desired signal is xt that we would like to estimate from the observation signal
vector, yt, in an optimal way thanks to the best estimator.

3.2 Best Estimator
25
3.2 Best Estimator
Considering the random variable xt and the random vector yt, and using
conditional expectations, we can decompose the variance of xt as
var (xt) = E [var (xt |yt )] + var [E (xt |yt )] ,
(3.8)
where the outer expectation and the outer variance on the right-hand side of
(3.8) are over the distribution of yt. This property is called the law of total
variance [6]. Since variances are always nonnegative, we deduce from (3.8)
that
0 ≤var [E (xt |yt )]
var (xt)
≤1.
(3.9)
As a result, the SPCC can be generalized to
ρ2
xt|yt = 1 −E [var (xt |yt )]
var (xt)
(3.10)
= var [E (xt |yt )]
var (xt)
.
This expression is often called the coeﬃcient of determination in the literature
of statistics [7]. If xt and yt are independent, then var (xt |yt ) = E
(
x2
t |yt
)
−
E2 (xt |yt ) = var (xt). As a consequence, ρ2
xt|yt = 0. Conversely, if ρ2
xt|yt = 0,
then var [E (xt |yt )] = 0, which implies that xt and yt are independent. At the
other limiting case, if xt = yt, then var (yt |yt ) = 0, which leads to ρ2
xt|yt = 1.
In fact, for xt = f(yt), we have E (xt |yt ) = E [f(yt) |yt ] = f(yt); as a result,
ρ2
xt|yt = 1. This coeﬃcient of determination, which is a direct consequence
of the law of total variance and measures how close xt is to E (xt |yt ), plays
a key role in the best estimator in general and in speech enhancement in
particular. It can be used as a powerful performance measure in all aspects
of speech enhancement as explained in great details in the rest.
In the same manner, we can decompose the variance of vt as
var (vt) = E [var (vt |yt )] + var [E (vt |yt )] ,
(3.11)
from which we deduce the coeﬃcient of determination:
ρ2
vt|yt = 1 −E [var (vt |yt )]
var (vt)
(3.12)
= var [E (vt |yt )]
var (vt)
.
It is well known that the best estimator of xt in the MMSE sense is the
conditional expectation of xt given yt [8], i.e.,

26
3 Best Estimator in the Time Domain
E (xt |yt ) = zxt(yt).
(3.13)
Indeed, let
ext = xt −E (xt |yt )
(3.14)
be the error signal between the desired signal and its best estimator, and let
fxt(yt) be any (linear or nonlinear) function of yt. We always have
E
(
e2
xt
)
= E
{
[xt −zxt(yt)]2}
≤E
{
[xt −fxt(yt)]2}
.
(3.15)
By virtue of the law of total expectation, the two random variables xt and
zxt(yt) have the same mean, i.e.,
E [zxt(yt)] = E (xt) = 0.
(3.16)
It can also be veriﬁed that the MMSE is
E
(
e2
xt
)
= E [var (xt |yt )]
(3.17)
= var (xt) −var [zxt(yt)]
= var (xt)
(
1 −ρ2
xt|yt
)
.
The MMSE clearly depends on the coeﬃcient of determination, ρ2
xt|yt, which
can be seen as a good measure of distortion of the desired signal, xt. The closer
is ρ2
xt|yt to 1, the less distorted the desired signal with the best estimator.
Then, we can deduce a distortion measure, which is close to the conventional
speech distortion index, i.e.,
υsd = E
(
e2
xt
)
var (xt) = 1 −ρ2
xt|yt.
(3.18)
We also notice in (3.17) the law of total variance since E
(
e2
xt |yt
)
=
var (xt |yt ) and E
[
E
(
e2
xt |yt
)]
= E
(
e2
xt
)
.
In the same way, the best estimator of vt in the MMSE sense is the con-
ditional expectation of vt given yt, i.e.,
E (vt |yt ) = zvt(yt)
(3.19)
and for any (linear or nonlinear) function of yt, fvt(yt), we always have
E
(
e2
vt
)
= E
{
[vt −zvt(yt)]2}
≤E
{
[vt −fvt(yt)]2}
,
(3.20)
where
evt = vt −E (vt |yt )
(3.21)

3.2 Best Estimator
27
is the error signal between the noise and its best estimator. By virtue of the
law of total expectation, the two random variables vt and zvt(yt) have the
same mean, i.e.,
E [zvt(yt)] = E (vt) = 0.
(3.22)
We also deduce that the MMSE is
E
(
e2
vt
)
= var (vt)
(
1 −ρ2
vt|yt
)
.
(3.23)
The coeﬃcient of determination, ρ2
vt|yt, is a good measure of noise reduction.
The closer is ρ2
vt|yt to 1, the more noise reduction with the best estimator.
In the pathological scenario where xt and vt are independent and identi-
cally distributed (i.i.d.), we have
E (xt |yt ) = E (vt |yt ) = yt
2 ,
(3.24)
and ρ2
xt|yt = ρ2
vt|yt = 1/2. As a result, single-channel speech enhancement in
the time domain is not feasible with the best estimator.
By adding together the best estimator of xt and the best estimator of vt,
we obtain the observed signal, i.e.,
yt = E (yt |yt )
= E (xt |yt ) + E (vt |yt ) .
(3.25)
The above property shows that the estimation errors of both estimators cancel
out. In other words, the best estimator of xt can be found, equivalently, from
the best estimator of vt. In the best estimator of xt, E (xt |yt ) gives the
speech distortion perspective while yt −E (vt |yt ) gives the noise reduction
perspective. From (3.25), we easily see that ext = −evt and, as a result,
E
(
e2
xt
)
= E
(
e2
vt
)
. Then, equating (3.17) and (3.23), we obtain
iSNR + ρ2
vt|yt = 1 + iSNR × ρ2
xt|yt.
(3.26)
From the previous expression, we have
lim
iSNR→0 ρ2
vt|yt = 1,
(3.27)
lim
iSNR→∞ρ2
xt|yt = 1.
(3.28)
In words, the best estimator is able to completely remove the noise when the
input SNR is close to 0 and fully recover the desired signal when the input
SNR approaches inﬁnity. However, (3.26) does not give us any information
about speech distortion in the ﬁrst case and noise reduction in the second
one. Using the fact that

28
3 Best Estimator in the Time Domain
iSNR = ρ2
xt,yt
ρ2vt,yt
= 1 −ρ2
vt,yt
1 −ρ2xt,yt
,
(3.29)
we can also express (3.26) as
ρ2
xt,yt
ρ2vt,yt
=
1 −ρ2
vt|yt
1 −ρ2
xt|yt
= iSNR.
(3.30)
Property 3.1. We have
ρ2
xt|yt ≥ρ2
xt,yt,
(3.31)
ρ2
vt|yt ≥ρ2
vt,yt.
(3.32)
As a consequence,
ρ2
xt|yt + ρ2
vt|yt ≥1.
(3.33)
Proof. Let us consider an estimate of the desired signal that is proportional
to the observation, i.e.,
zxt(yt) = α
√
var (xt)
√
var (yt)
yt,
(3.34)
where α ̸= 0 is an arbitrary real number. In this case the MSE is
E
(
e2
xt
)
= E
{
[xt −zxt(yt)]2}
(3.35)
= var (xt)
[(
1 + α2)
−2αρxt,yt
]
.
Since E
(
e2
xt
)
≥E
(
e2
xt
)
, we deduce that
ρ2
xt|yt ≥−α2 + 2αρxt,yt.
(3.36)
For the particular value of α = ρxt,yt in the previous expression, we ﬁnd that
ρ2
xt|yt ≥ρ2
xt,yt. We can use a very similar proof to show the inequality in
(3.32).
The above suggests that we can combine the speech distortion and noise
reduction measures into one convenient measure:
ϱ = ρ2
xt|yt + ρ2
vt|yt,
(3.37)
where 1 ≤ϱ ≤2. Fundamentally, ϱ measures the compromise between speech
distortion and noise reduction. For ϱ close to 2, we have the almost perfect
estimator with the best estimator in the sense that the noise is almost all
removed and speech distortion is almost nonexistent. For ϱ = 1, the observed
signal is fundamentally not aﬀected; this will happen only when speech and

3.2 Best Estimator
29
noise are i.i.d., so that ρ2
xt|yt = ρ2
vt|yt = 1/2. From (3.26) and (3.37), we
deduce for the best estimator that
ρ2
xt|yt = ϱ −1 + iSNR
1 + iSNR
(3.38)
= ρ2
vt,yt (ϱ −1) + ρ2
xt,yt
and
ρ2
vt|yt = (ϱ −1) iSNR + 1
1 + iSNR
(3.39)
= ρ2
xt,yt (ϱ −1) + ρ2
vt,yt.
The two previous expressions tell us the following. For a low input SNR,
ρ2
vt|yt is close to 1, meaning that there is a good amount of noise reduction;
however, ρ2
xt|yt depends mostly on ϱ −1, meaning that distortion depends
on the distributions of xt and vt. For a large input SNR, ρ2
xt|yt is close to 1,
meaning that there is low distortion; however, ρ2
vt|yt is close to ϱ−1, meaning
that noise reduction depends on the distributions of xt and vt.
While the SPCCs ρ2
xt,yt and ρ2
vt,yt give a very good indication on the state
of the noisy signal (since they are related to the input SNR), the coeﬃcients
of determination ρ2
xt|yt and ρ2
vt|yt, as well as ϱ give a very good indication on
the enhanced noisy signal with the best estimator since ρ2
xt|yt and ρ2
vt|yt are
good measures of speech distortion and noise reduction, respectively, and ϱ
is a good measure on the compromise between the two.
From Property 3.1, we can also deﬁne the gain in SNR of the best estima-
tor:
G = oSNR
iSNR
=
(
ρ2
xt|yt
1 −ρ2
vt|yt
)2
≥1,
(3.40)
where oSNR is the output SNR of the best estimator1. This deﬁnition of the
gain in SNR is justiﬁed by the facts that G is always greater than or equal to
1 and for i.i.d. speech and noise, G = 1. Using (3.30), we easily see that the
output SNR in (3.40) is
1 It is important to keep in mind that a rigorous deﬁnition of the output SNR of the
best estimator in general may not be possible since the output SNR is a second-order
measure while the best estimator depends on distributions. This is why the coeﬃcients of
determination may be the most natural and reliable measures in this context.

30
3 Best Estimator in the Time Domain
oSNR = iSNR ×
(
ρ2
xt|yt
1 −ρ2
vt|yt
)2
=
ρ4
xt|yt
(
1 −ρ2
vt|yt
) (
1 −ρ2
xt|yt
) ≥iSNR.
(3.41)
One can check from the previous expression that when speech is large as
compared to noise, the output SNR is also large, and when speech is small as
compared to noise, the output SNR is also small; this is consistent with the
deﬁnition of the output SNR. In this context, we deﬁne the speech reduction
factor and the noise reduction factor as, respectively,
ξsr =
1
ρ4
xt|yt
(3.42)
and
ξnr =
1
(
1 −ρ2
vt|yt
)2 .
(3.43)
As a consequence, we deduce the fundamental relationship for the best esti-
mator:
ξnr
ξsr
= oSNR
iSNR ,
(3.44)
which is well known in the linear case. This is an even more insightful way to
explain the compromise between noise reduction and speech distortion with
more intuitive measures. It is quite remarkable that these measures, which
should resemble the conventional ones with linear ﬁltering, are derived in such
a simple way for the whole class of best estimators (linear and nonlinear).
Another interesting measure is the conditional correlation coeﬃcient
(CCC). The CCC between xt and vt given yt is
ρxt,vt|yt =
cov (xt, vt|yt)
√
var (xt|yt) var (vt|yt)
,
(3.45)
where
cov (xt, vt|yt) = E {[xt −E (xt|yt)] [vt −E (vt|yt)] |yt}
(3.46)
= E [extevt|yt]
= −var (xt|yt)
= −var (vt|yt) .
Therefore, we deduce that

3.3 Best Linear Estimator
31
ρxt,vt|yt = −1.
(3.47)
While ρxt,vt = 0, the magnitude of the CCC, |ρxt,vt|yt|, is maximized; this is
due to the fact that the best estimators of xt and vt are conditionally fully
correlated. The minus sign in (3.47) comes from the fact that ext = −evt.
3.3 Best Linear Estimator
The best linear estimator is a very important and extremely useful particular
case of the best estimator in general. It is well known that the best linear
estimator of xt in the MMSE sense is
E (xt |yt ) = hT
xt,Wyt,
(3.48)
where
hxt,W = cov−1 (yt) cov (xt) i
(3.49)
is the classical single-channel Wiener ﬁlter in the time domain [1], with i
being the ﬁrst column of the L × L identity matrix IL. We deduce that the
square of the coeﬃcient of determination is
ρ4
xt|yt =
[iT cov (xt) cov−1 (yt) cov (xt) i
var (xt)
]2
(3.50)
=
hT
xt,Wcov (xt) iiT cov (xt) hxt,W
var2 (xt)
= ξ−1
sr ,
which is a good measure of speech distortion. This measure is very similar to
the inverse of the conventional speech reduction factor [1]:
ξ−1
sr (hxt,W) =
hT
xt,Wcov (xt) hxt,W
var (xt)
.
(3.51)
From the Cauchy-Schwarz inequality, i.e.,
[
hT
xt,Wcov (xt) i
]2 ≤hT
xt,Wcov (xt) hxt,W × var (xt) ,
(3.52)
it results that
ξsr ≥ξsr (hxt,W) .
(3.53)
The vector xt can be decomposed into two orthogonal components; one
proportional to the desired signal, xt, and the other to what we may consider
as an interference [4]:

32
3 Best Estimator in the Time Domain
xt = xtγxt + xt,i,
(3.54)
where
γxt = cov (xt) i
var (xt)
(3.55)
is the normalized correlation vector between between xt and xt,
xt,i = xt −xtγxt
(3.56)
is the interference signal vector, and
E (xt,ixt) = 0.
(3.57)
Obviously, we can express the square of the coeﬃcient of determination in
(3.50) as
ρ4
xt|yt =
(
hT
xt,Wγxt
)2 ,
(3.58)
which may give a better perspective on distortion since when hT
xt,Wγxt is
close to 1, the desired signal, xt, is well recovered. If cov (xt) is of rank 1, i.e.,
xt,i = 0, then cov (xt) = var (xt) γxtγT
xt and hxt,W = var (xt) cov−1 (yt) γxt.
As a consequence,
ξsr = ξsr (hxt,W) .
(3.59)
Also, the best linear estimator of vt in the MMSE sense is
E (vt |yt ) = hT
vt,Wyt,
(3.60)
where
hvt,W = cov−1 (yt) cov (vt) i
(3.61)
is the Wiener ﬁlter for the estimation of vt. Then, the coeﬃcient of determi-
nation is
ρ2
vt|yt = iT cov (vt) cov−1 (yt) cov (vt) i
var (vt)
(3.62)
=
hT
vt,Wcov (vt) i
var (vt)
,
which is a good measure of noise reduction. It is not hard to see that
i = hxt,W + hvt,W.
(3.63)
Therefore, (3.62) can be rewritten as

3.3 Best Linear Estimator
33
(
1 −ρ2
vt|yt
)2
=
hT
xt,Wcov (vt) iiT cov (vt) hxt,W
var2 (vt)
= ξ−1
nr .
(3.64)
The measure
(
1 −ρ2
vt|yt
)2
is very similar to the inverse of the conventional
noise reduction factor [1]:
ξ−1
nr (hxt,W) =
hT
xt,Wcov (vt) hxt,W
var (vt)
(3.65)
and one can verify that
ξnr ≥ξnr (hxt,W) .
(3.66)
Using our deﬁnition of the output SNR in (3.41) of the best estimator, we
have
oSNR =
hT
xt,Wcov (xt) iiT cov (xt) hxt,W/var (xt)
hT
xt,Wcov (vt) iiT cov (vt) hxt,W/var (vt) ,
(3.67)
which resembles the conventional output SNR:
oSNR (hxt,W) =
hT
xt,Wcov (xt) hxt,W
hT
xt,Wcov (vt) hxt,W
.
(3.68)
Now, if we compute the SPCC between xt and hT
xt,Wyt, and the the SPCC
between vt and hT
vt,Wyt, it is easy to verify that
ρ2
xt,hT
xt,Wyt = ρ2
xt|yt,
ρ2
vt,hT
vt,Wyt = ρ2
vt|yt.
Let us open a short parenthesis on the so-called partial correlation co-
eﬃcient (PCC), whose function is to evaluate the correlation between two
variables after eliminating the eﬀect of another variable on these two vari-
ables. The PCC between xt and vt with respect to yt, denoted ρxt,vt·yt, is
computed in two steps. In the ﬁrst step, we ﬁnd the two ﬁlters hxt and hvt
that minimize the error signals ext = xt −hT
xtyt and evt = vt −hT
vtyt, respec-
tively. We get the Wiener ﬁlters hxt,W and hvt,W. Substituting these ﬁlters
back into the errors, we obtain the two residuals ext,W and evt,W. Then, in
the second step we compute the correlation coeﬃcient between ext,W and
evt,W, i.e.,

34
3 Best Estimator in the Time Domain
ρxt,vt·yt =
E (ext,Wevt,W)
√
E
(
e2
xt,W
)
E
(
e2
vt,W
).
(3.69)
It is easy to check that (3.69) simpliﬁes to
ρxt,vt·yt = −1,
(3.70)
showing that CCC and PCC are strictly equivalent in the linear case.
3.4 Generalization to the Binaural Case
3.4.1 Problem Formulation
In binaural speech enhancement, we need to extract two “clean” signals from
the sensor array that will be delivered to the left and right ears of a human
subject.
Without loss of generality, we consider the signal model in which an array
consisting of 2M sensors capture a source (speech) signal convolved with
acoustic impulse responses in some noise ﬁeld. The signal received at the ith
sensor is then expressed as [9]
y′
t,i = g′
t,i ∗xt + v′
t,i
(3.71)
= x′
t,i + v′
t,i, i = 1, 2, . . . , 2M,
where g′
t,i is the acoustic impulse response from the unknown desired source,
xt, location to the ith sensor, ∗stands for linear convolution, and v′
t,i is the
additive noise at sensor i. We assume that the impulse responses are time
invariant and that the signals x′
t,i = g′
t,i∗xt and v′
t,i are mutually independent,
zero mean, real, broadband, and stationary.
Since we are interested in binaural estimation, it is more convenient to
work in the complex domain in order that the original (binaural) problem is
transformed into the conventional (monaural) noise reduction processing with
a sensor array [10]. In other words, instead of having two real-valued outputs,
we will have one complex-valued output. Indeed, from the 2M real-valued
microphone signals given in (3.71), we can artiﬁcially build M complex-valued
sensor signals as
yt,m = y′
t,m + ȷy′
t,M+m
(3.72)
= xt,m + vt,m, m = 1, 2, . . . , M,
where

3.4 Generalization to the Binaural Case
35
xt,m = x′
t,m + ȷx′
t,M+m, m = 1, 2, . . . , M
(3.73)
is the complex desired speech signal and
vt,m = v′
t,m + ȷv′
t,M+m, m = 1, 2, . . . , M
(3.74)
is the complex additive noise at the complex sensor m.
It is customary to work with blocks of L successive time samples, i.e.,
yt,m =
[ yt,m yt−1,m · · · yt−L+1,m
]T
= xt,m + vt,m, m = 1, 2, . . . , M,
(3.75)
where xt,m and vt,m are deﬁned in a similar way to yt,m. Concatenating all
the observations together, we get the vector of length ML:
yt =
[yT
t,1 yT
t,2 · · · yT
t,M
]T
= xt + vt,
(3.76)
where xt and vt are also concatenated vectors of xt,m and vt,m, respectively.
We deduce that the ML × ML covariance matrix of yt is
cov
(
yt
)
= E
(
ytyH
t
)
(3.77)
= cov (xt) + cov (vt) ,
where cov (xt) and cov (vt) are the covariance matrices of xt and vt, respec-
tively.
Obviously, from the model given in (3.72), we deal with complex random
variables (CRVs) and it can be veriﬁed that, in general, xt,m and vt,m are
highly noncircular CRVs [11]. Let a be a zero-mean CRV, a good measure
of the second-order circularity is the circularity quotient [12] deﬁned as the
ratio between the pseudo-variance and the variance of a, i.e.,
γa = E
(
a2)
E (|a|2).
(3.78)
This measure coincides with the coherence function between a and a∗. Since
xt,m and/or vt,m are noncircular CRVs, the vector y∗
t should also be included
as part of the observations [13], [14]. Therefore, we deﬁne the augmented
observation vector of length 2ML as
eyt =
[ yt
y∗
t
]
= ext + evt,
(3.79)

36
3 Best Estimator in the Time Domain
where ext and evt are deﬁned similarly to eyt. We deduce that the 2ML×2ML
covariance matrix of eyt is
cov
(
eyt
)
= cov (ext) + cov (evt) ,
(3.80)
where cov (ext) and cov (evt) are the covariance matrices of ext and evt, respec-
tively.
In the rest, we consider the ﬁrst complex sensor signal, i.e., yt,1, as the
reference. Therefore, our aim is to recover the complex desired speech signal,
xt,1, from the augmented complex observation vector, eyt, in the best possible
way. Using this reference, we deﬁne the input SNR as
iSNR = var (xt,1)
var (vt,1) ,
(3.81)
where var (xt,1) = E
(
|xt,1|2)
and var (vt,1) = E
(
|vt,1|2)
are the variances
of xt,1 and vt,1, respectively.
Another perspective in the context of binaural speech enhancement is from
the magnitude squared Pearson correlation coeﬃcient (MSPCC) [1], [5]. It is
easy to see that the MSPCC between xt,1 and yt,1 is
ρxt,1,yt,1
2 =
E
(
xt,1y∗
t,1
)2
E
(
|xt,1|2)
E
(
|yt,1|2)
(3.82)
=
iSNR
1 + iSNR.
In the same way, the MSPCC between vt,1 and yt,1 is
ρvt,1,yt,1
2 =
E
(
vt,1y∗
t,1
)2
E
(
|vt,1|2)
E
(
|yt,1|2)
(3.83)
=
1
1 + iSNR.
As a result,
1 =
ρxt,1,yt,1
2 +
ρvt,1,yt,1
2 .
(3.84)
This shows how the important correlations are related to the input SNR.

3.4 Generalization to the Binaural Case
37
3.4.2 Best Estimator
Considering the CRV xt,1 and the complex random vector eyt, and using
conditional expectations, we can express the law of total variance [6] with
respect to xt,1 as
var (xt,1) = E
[
var
(
xt,1
eyt
)]
+ var
[
E
(
xt,1
eyt
)]
,
(3.85)
where the outer expectation and the outer variance on the right-hand side of
(3.85) are over the distribution of eyt. Since variances are always nonnegative,
we deduce from (3.85) that
0 ≤
var
[
E
(
xt,1
eyt
)]
var (xt,1)
≤1.
(3.86)
Therefore, the MSPCC can be generalized to
ρxt,1|eyt

2
= 1 −
E
[
var
(
xt,1
eyt
)]
var (xt,1)
(3.87)
=
var
[
E
(
xt,1
eyt
)]
var (xt,1)
.
This expression is often called the coeﬃcient of determination in the liter-
ature of statistics [7]. If xt,1 and eyt are independent, then var
(
xt,1
eyt
)
=
var (xt,1). As a consequence,
ρxt,1|eyt

2
= 0. Conversely, if
ρxt,1|eyt

2
= 0,
then var
[
E
(
xt,1
eyt
)]
= 0, which implies that xt,1 and eyt are indepen-
dent. At the other limiting case, if xt,1 = yt,1, then var
(
yt,1
eyt
)
= 0,
which leads to
ρxt,1|eyt

2
= 1. In fact, for xt,1 = f(yt,1, y∗
t,1), we have
E
(
xt,1
eyt
)
= E
[
f(yt,1, y∗
t,1)
eyt
]
= f(yt,1, y∗
t,1); as a result,
ρxt,1|eyt

2
= 1.
This coeﬃcient, which is a direct consequence of the law of total variance
and measures how close xt,1 is to E
(
xt,1
eyt
)
, plays a key role in the best
estimator in general and in binaural speech enhancement in particular. It can
be used as a powerful performance measure in all aspects of binaural speech
enhancement as explained in the rest.
In the same manner, we can express the variance of vt,1 as
var (vt,1) = E
[
var
(
vt,1
eyt
)]
+ var
[
E
(
vt,1
eyt
)]
,
(3.88)
from which we deduce the coeﬃcient of determination:

38
3 Best Estimator in the Time Domain
ρvt,1|eyt

2
= 1 −
E
[
var
(
vt,1
eyt
)]
var (vt,1)
(3.89)
=
var
[
E
(
vt,1
eyt
)]
var (vt,1)
.
The best estimator of xt,1 in the MMSE sense is well known to be the
conditional expectation of xt,1 given eyt [8], i.e.,
E
(
xt,1
eyt
)
= zxt,1
(
eyt
)
.
(3.90)
Indeed, let fxt,1
(
eyt
)
be any (linear or nonlinear) function of eyt, we always
have
E
(ext,1
2)
= E
[xt,1 −zxt,1
(
eyt
)
2]
≤E
[xt,1 −fxt,1
(
eyt
)
2]
,
(3.91)
where ext,1 = xt,1 −E
(
xt,1
eyt
)
is the error signal between the desired
signal and its best estimator. By virtue of the law of total expectation, the
two random variables xt,1 and zxt,1
(
eyt
)
have the same mean, i.e.,
E
[
zxt,1
(
eyt
)]
= E (xt,1) = 0.
(3.92)
It can also be veriﬁed that the MMSE is
E
(ext,1
2)
= E
[
var
(
xt,1
eyt
)]
(3.93)
= var (xt,1) −var
[
zxt,1
(
eyt
)]
= var (xt,1)
(
1 −
ρxt,1|eyt

2)
.
The MMSE clearly depends on the coeﬃcient of determination,
ρxt,1|eyt

2
,
which can be seen as a good measure of distortion of the desired signal, xt,1.
The closer is
ρxt,1|eyt

2
to 1, the less distorted the desired signal with the best
binaural estimator.
In the same way, the best estimator of vt,1 in the MMSE sense is the
conditional expectation of vt,1 given eyt, i.e.,
E
(
vt,1
eyt
)
= zvt,1
(
eyt
)
(3.94)

3.4 Generalization to the Binaural Case
39
and for any (linear or nonlinear) function of eyt, fvt,1
(
eyt
)
, we always have
E
(evt,1
2)
= E
[vt,1 −zvt,1
(
eyt
)
2]
≤E
[vt,1 −fvt,1
(
eyt
)
2]
,
(3.95)
where evt,1 = vt,1 −E
(
vt,1
eyt
)
is the error signal between the noise and its
best estimator. By virtue of the law of total expectation, the two random
variables vt,1 and zvt,1
(
eyt
)
have the same mean, i.e.,
E
[
zvt,1
(
eyt
)]
= E (vt,1) = 0.
(3.96)
We also deduce that the MMSE is
E
(evt,1
2)
= var (vt,1)
(
1 −
ρvt,1|eyt

2)
.
(3.97)
The coeﬃcient of determination,
ρvt,1|eyt

2
, is a good measure of noise re-
duction. The closer is
ρvt,1|eyt

2
to 1, the more noise reduction with the best
binaural estimator.
Now, if we add together the best estimator of xt,1 and the best estimator
of vt,1, we obtain the observed signal, i.e.,
yt,1 = E
(
yt,1
eyt
)
= E
(
xt,1
eyt
)
+ E
(
vt,1
eyt
)
.
(3.98)
The above property shows that the estimation errors of both estimators cancel
out. In other words, the best estimator of xt,1 can be found, equivalently, from
the best estimator of vt,1. In the best estimator of xt,1, E
(
xt,1
eyt
)
gives
the speech distortion perspective while yt,1 −E
(
vt,1
eyt
)
gives the noise
reduction perspective. From (3.98), we easily see that ext,1 = −evt,1 and, as
a result, E
(ext,1
2)
= E
(evt,1
2)
. Then, equating (3.93) and (3.97), we
obtain
iSNR +
ρvt,1|eyt

2
= 1 + iSNR ×
ρxt,1|eyt

2
.
(3.99)
From the previous expression, we have

40
3 Best Estimator in the Time Domain
lim
iSNR→0
ρvt,1|eyt

2
= 1,
(3.100)
lim
iSNR→∞
ρxt,1|eyt

2
= 1.
(3.101)
In words, the best binaural estimator is able to completely remove the noise
when the input SNR is close to 0 and fully recover the desired signal when
the input SNR approaches inﬁnity. However, (3.99) does not give us any
information about speech distortion in the ﬁrst case and noise reduction in
the second one. Using the fact that
iSNR =
ρxt,1,yt,1
2
ρvt,1,yt,1|
2 ,
(3.102)
we can also express (3.99) as
1 −
ρxt,1,yt,1
2
1 −
ρvt,1,yt,1
2 =
1 −
ρxt,1|eyt

2
1 −
ρvt,1|eyt

2 .
(3.103)
Property 3.2. We have
ρxt,1|eyt

2
≥
ρxt,1,yt,1
2 ,
(3.104)
ρvt,1|eyt

2
≥
ρxt,1,yt,1
2 .
(3.105)
As a consequence,
ρxt,1|eyt

2
+
ρvt,1|eyt

2
≥1.
(3.106)
Proof. Let us consider an estimate of the desired signal that is proportional
to the observed signal at the reference sensor, i.e.,
zxt,1
(
eyt
)
= α
√
var (xt,1)
√
var (yt,1)
yt,1,
(3.107)
where α ̸= 0 is an arbitrary complex number. In this case the MSE is
E
(ext,1
2)
= E
[xt,1 −zxt,1
(
eyt
)
2]
(3.108)
= var (xt,1)
[(
1 + |α|2)
−(α + α∗) ρxt,1,yt,1
]
.
Since E
(ext,1
2)
≥E
(ext,1
2)
, we deduce that

3.4 Generalization to the Binaural Case
41
ρxt,1|eyt

2
≥−|α|2 + (α + α∗) ρxt,1,yt,1.
(3.109)
For the particular value of α = ρxt,1,yt,1 in the previous expression2, we ﬁnd
that
ρxt,1|eyt

2
≥
ρxt,1,yt,1
2. We can use a very similar proof to show the
inequality in (3.105).
Property 3.2 suggests that we can combine the speech distortion and noise
reduction measures into one convenient measure:
ϱ =
ρxt,1|eyt

2
+
ρvt,1|eyt

2
,
(3.110)
where 1 ≤ϱ ≤2. Fundamentally, ϱ measures the compromise between speech
distortion and noise reduction. For ϱ close to 2, we have the almost perfect
estimator with the best binaural estimator in the sense that the noise is
almost all removed and speech distortion is almost nonexistent. From (3.99)
and (3.110), we deduce for the best binaural estimator that
ρxt,1|eyt

2
= ϱ −1 + iSNR
1 + iSNR
(3.111)
=
ρvt,1,yt,1
2 (ϱ −1) +
ρxt,1,yt,1
2
and
ρvt,1|eyt

2
= (ϱ −1) iSNR + 1
1 + iSNR
(3.112)
=
ρxt,1,yt,1
2 (ϱ −1) +
ρvt,1,yt,1
2 .
The two previous expressions tell us the following. For a low input SNR,
ρvt,1|eyt

2
is close to 1, meaning that there is a good amount of noise reduction;
however,
ρxt,1|eyt

2
depends mostly on ϱ−1, meaning that distortion depends
on the distributions of xt,1 and vt,1. For a large input SNR,
ρxt,1|eyt

2
is close
to 1, meaning that there is low distortion; however,
ρvt,1|eyt

2
is close to ϱ−1,
meaning that noise reduction depends on the distributions of xt,1 and vt,1.
While the MSPCCs
ρxt,1,yt,1
2 and
ρvt,1,yt,1
2 give a very good indication
on the state of the noisy signal (since they are related to the input SNR),
the coeﬃcients of determination
ρxt,1|eyt

2
and
ρvt,1|eyt

2
, as well as ϱ give
a very good indication on the enhanced noisy complex signal with the best
binaural estimator since
ρxt,1|eyt

2
and
ρvt,1|eyt

2
are good measures of speech
2 One can check that ρxt,1,yt,1 is a real number, i.e., ρxt,1,yt,1 =
√
iSNR
1+iSNR .

42
3 Best Estimator in the Time Domain
distortion and noise reduction, respectively, and ϱ is a good measure on the
compromise between the two.
3.4.3 Best Widely Linear Estimator
From the recent literature [10], [13], [14], it is known that the best widely
linear estimator of xt,1 in the MMSE sense is
E
(
xt,1
eyt
)
= hH
xt,1,Weyt,
(3.113)
where
hxt,1,W = cov−1 (
eyt
)
cov (ext) i
(3.114)
is the widely linear Wiener ﬁlter in the time domain [10], with i being the
ﬁrst column of the 2ML × 2ML identity matrix I2ML. We deduce that the
coeﬃcient of determination is
ρxt,1|eyt

2
=
iT cov (ext) cov−1 (
eyt
)
cov (ext) i
var (xt,1)
(3.115)
=
hH
xt,1,Wcov (ext) i
var (xt,1)
,
which is a good measure of speech distortion. This measure is very much
related to the inverse of the conventional speech reduction factor [1]:
ξ−1
sr
(
hxt,1,W
)
=
hH
xt,1,Wcov (ext) hxt,1,W
var (xt,1)
.
(3.116)
Also, the best widely linear estimator of vt,1 in the MMSE sense is
E
(
vt,1
eyt
)
= hH
vt,1,Weyt,
(3.117)
where
hvt,1,W = cov−1 (
eyt
)
cov (evt) i
(3.118)
is the widely linear Wiener ﬁlter for the estimation of vt,1. Then, the coeﬃ-
cient of determination is

References
43
ρvt,1|eyt

2
=
iT cov (evt) cov−1 (
eyt
)
cov (evt) i
var (vt,1)
(3.119)
=
hH
vt,1,Wcov (evt) i
var (vt,1)
,
which is a good measure of noise reduction. It is easy to check that
i = hxt,1,W + hvt,1,W.
(3.120)
Therefore, (3.119) can be rewritten as
ρvt,1|eyt

2
= 1 −
hH
xt,1,Wcov (evt) i
var (vt,1)
.
(3.121)
The measure 1 −
ρvt,1|eyt

2
is very much related to the inverse of the conven-
tional noise reduction factor [1]:
ξ−1
nr
(
hxt,1,W
)
=
hH
xt,1,Wcov (evt) hxt,1,W
var (vt,1)
.
(3.122)
References
1. J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, 2009.
2. P. Vary and R. Martin, Digital Speech Transmission: Enhancement, Coding and Error
Concealment. Chichester, England: John Wiley & Sons Ltd, 2006.
3. P. Loizou, Speech Enhancement: Theory and Practice. Boca Raton, FL: CRC Press,
2007.
4. J. Benesty and J. Chen, Optimal Time-Domain Noise Reduction Filters–A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, 2011.
5. J. Benesty, J. Chen, and Y. Huang, “On the importance of the Pearson correlation
coeﬃcient in noise reduction,” IEEE Trans. Audio, Speech, Language Process., vol.
16, pp. 757–765, May 2008.
6. N. A. Weiss, P. T. Holmes, and M. Hardy, A Course in Probability. Boston: Addison-
Wesley, 2005.
7. R. Steyer, “Conditional expectations: an introduction to the concept and its applica-
tions in empirical sciences,” Methodika, vol. 2, issue 1, pp. 53–78, 1988.
8. S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory. Upper
Saddle River, NJ: Prentice Hall PTR, 1993.
9. J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin, Ger-
many: Springer-Verlag, 2008.
10. J. Benesty, J. Chen, and Y. Huang, “Binaural noise reduction in the time domain
with a stereo setup,” IEEE Trans. Audio, Speech, Language Process., vol. 19, pp.
2260–2272, Nov. 2011.
11. P. O. Amblard, M. Gaeta, and J. L. Lacoume, “Statistics for complex variables and
signals–Part I: variables,” Signal Process., vol. 53, pp. 1–13, 1996.

44
3 Best Estimator in the Time Domain
12. E. Ollila, “On the circularity of a complex random variable,” IEEE Signal Process.
Lett., vol. 15, pp. 841–844, 2008.
13. D. P. Mandic and S. L. Goh, Complex Valued Nonlinear Adaptive Filters: Noncircu-
larity, Widely Linear and Neural Models. Wiley, 2009.
14. B. Picinbono and P. Chevalier, “Widely linear estimation with complex data,” IEEE
Trans. Signal Process., vol. 43, pp. 2030–2033, Aug. 1995.

Chapter 4
Speech Enhancement Via Correlation
Coeﬃcients
In the previous two chapters, we showed the importance of diﬀerent kinds of
correlation coeﬃcients in the formulation and analysis of the best estimators
for speech enhancement. In this chapter, we focus on the linear case and
show how the most relevant noise reduction ﬁlters as well as new ones can be
easily derived from the Pearson correlation coeﬃcient. We work in the time
domain but the extension of these ideas to the more convenient short-time
Fourier transform domain is straightforward. Also, to simplify derivations
and make things as clear as possible, we only focus on the single-channel
case; generalization to the multichannel scenario is immediate.
4.1 Signal Model and Problem Formulation
The contribution in this chapter is an extension and a generalization of the
work presented in [1], [2], [3], [4].
We consider the single-channel noise reduction problem in the time domain
described in Chapter 3 (Section 3.1), i.e.,
y(t) = x(t) + v(t),
(4.1)
where y(t), x(t), and v(t) are the microphone, desired, and noise signals,
respectively1. In a vector form, (4.1) is
y(t) =
[ y(t) y(t −1) · · · y(t −L + 1) ]T
= x(t) + v(t).
(4.2)
Thus, the covariance matrix (of size L × L) of the noisy signal is
1 In this chapter, we slightly change the notation for convenience.
45
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_4

46
4 Speech Enhancement Via Correlation Coeﬃcients
Ry = E
[
y(t)yT (t)
]
= Rx + Rv,
(4.3)
where Rx = E
[
x(t)xT (t)
]
and Rv = E
[
v(t)vT (t)
]
are the covariance matri-
ces of x(t) and v(t), respectively. Then, our objective is to estimate x(t) from
the observations, in diﬀerent ways and diﬀerent levels of compromises, thanks
to the many forms of the squared Pearson correlation coeﬃcient (SPCC)
among all signals of interest.
We end this section by recalling the deﬁnition of the input SNR:
iSNR = σ2
x
σ2v
,
(4.4)
where σ2
x = E
[
x2(t)
]
and σ2
v = E
[
v2(t)
]
are the variances of x(t) and v(t),
respectively.
4.2 Linear Filtering and Correlation Coeﬃcients
In this chapter, we estimate the desired signal sample, x(t), or the noise
signal sample, v(t), by applying a real-valued ﬁlter, h, of length L, to the
observation signal vector, y(t), i.e.,
z(t) = hT y(t)
(4.5)
= xfd(t) + vfn(t),
where z(t) can be either the estimate of x(t) or v(t),
xfd(t) = hT x(t)
(4.6)
is the ﬁltered desired signal, and
vfn(t) = hT v(t)
(4.7)
is the ﬁltered noise signal. If z(t) is the estimate of v(t), then the estimate of
x(t) is
bx(t) = y(t) −z(t)
(4.8)
= y(t) −hT y(t)
= (i −h)T y(t),
where i is the ﬁrst column of the L × L identity matrix IL. In the rest, we
will also use the notation hx and hv. The ﬁrst ﬁlter, hx, corresponds to the
estimation of x(t) while the second ﬁlter, hv, corresponds to the estimation

4.2 Linear Filtering and Correlation Coeﬃcients
47
of v(t). Obviously, from (4.8), we have the relationship:
hx + hv = i,
(4.9)
which will extensively be used in all this chapter. Therefore, when v(t) is
estimated with hv, we can estimate x(t) with hx, thanks to the relation in
(4.9).
It is of great interest to know how much of x(t) [resp. xfd(t)] or v(t) [resp.
vfn(t)] is contained in the estimator z(t). The best second-order statistics
based measure to evaluate this is via the SPCC [1]. Next, we propose four
diﬀerent forms of the SPCC.
We deﬁne the SPCC between z(t) and x(t) as
ρ2
z,x (h) =
E2 [z(t)x(t)]
E [z2(t)] E [x2(t)]
(4.10)
= σ2
x
(
hT γx
)2
hT Ryh
= hT Rx1h
hT Ryh ,
where
γx = E [x(t)x(t)]
σ2x
(4.11)
is the normalized correlation vector between x(t) and x(t), and
Rx1 = σ2
xγxγT
x
(4.12)
is a rank-1 matrix. In fact, we know from Chapter 3 that we can decompose
Rx as
Rx = σ2
xγxγT
x + E
[
xi(t)xT
i (t)
]
= Rx1 + Rxi,
(4.13)
where Rx1 is deﬁned in (4.12) and Rxi is the covariance matrix of the so-
called interference signal, xi(t), with E [xi(t)x(t)] = 0.
In the same manner, we deﬁne the SPCC between z(t) and v(t) as

48
4 Speech Enhancement Via Correlation Coeﬃcients
ρ2
z,v (h) =
E2 [z(t)v(t)]
E [z2(t)] E [v2(t)]
(4.14)
= σ2
v
(
hT γv
)2
hT Ryh
= hT Rv1h
hT Ryh ,
where γv is the normalized correlation vector between v(t) and v(t), and
Rv1 = σ2
vγvγT
v
(4.15)
is a rank-1 matrix. We also have
Rv = σ2
vγvγT
v + E
[
vu(t)vT
u (t)
]
= Rv1 + Rvu,
(4.16)
where Rvu is the covariance matrix of vu(t) = v(t) −v(t)γv, and this latter
vector is uncorrelated with v(t).
The SPCC between z(t) and xfd(t) is also of great interest. It is given by
ρ2
z,xfd (h) = hT Rxh
hT Ryh.
(4.17)
Using the decomposition in (4.13), (4.17) can be expressed as
ρ2
z,xfd (h) = ρ2
z,x (h) + hT Rxih
hT Ryh
(4.18)
= ρ2
z,x (h) + ρ2
z,xfi (h)
≥ρ2
z,x (h) ,
where ρ2
z,xfi (h) is the SPCC between z(t) and the ﬁltered interference, i.e.,
xﬁ(t) = hT xi(t). Expression (4.18) tells us that z(t) and xfd(t) are more
correlated than z(t) and x(t) are.
Finally, the last SPCC of interest is the one between z(t) and vfn(t), i.e.,
ρ2
z,vfn (h) = hT Rvh
hT Ryh.
(4.19)
With the help of (4.16), we can decompose (4.19) as
ρ2
z,vfn (h) = ρ2
z,v (h) + hT Rvuh
hT Ryh
(4.20)
= ρ2
z,v (h) + ρ2
z,vfu (h)
≥ρ2
z,v (h) ,

4.3 Optimal Filters
49
where ρ2
z,vfu (h) is the SPCC between z(t) and the ﬁltered uncorrelated noise,
i.e., vfu(t) = hT vu(t). We can observe from (4.20) that z(t) and vfn(t) are
more correlated than z(t) and v(t) are.
It can easily be checked that
ρ2
z,xfd (h) + ρ2
z,vfn (h) = 1,
(4.21)
but
ρ2
z,x (h) + ρ2
z,v (h) = 1 −ρ2
z,xfi (h) −ρ2
z,vfu (h)
(4.22)
≤1.
We see that the four SPCCs deﬁned above depend explicitly on the ﬁlter,
h, and measure diﬀerent kinds of correlation. So it makes intuitively sense to
optimize them in order to get diﬀerent kinds of noise reduction ﬁlters.
4.3 Optimal Filters
4.3.1 SPCC Between Filter Output and Desired Signal
In this subsection, we consider the SPCC between z(t) and x(t). A maximal
(resp. minimal) value of the SPCC implies that z(t) could be the estimate of
x(t) [resp. v(t)].
4.3.1.1 Maximization of the SPCC
It is obvious that the maximization of (4.10) leads to the estimate of the
desired signal since, in this case, x(t) will be maximally correlated with its
estimate, z(t). In (4.10), we recognize the generalized Rayleigh quotient [5].
It is well known that this quotient is maximized with the eigenvector, a1,
corresponding to the maximum eigenvalue of the matrix R−1
y Rx1
2. Let us
denote λa1 this maximum eigenvalue. Since the rank of the mentioned matrix
is equal to 1, we have
a1 =
R−1
y γx
√
γTx R−1
y γx
,
(4.23)
λa1 = σ2
xγT
x R−1
y γx,
(4.24)
2 In the rest of this chapter, we will use some well-known properties of joined diagonalized
matrices in order to simplify some of the expressions of the derived noise reduction ﬁlters.

50
4 Speech Enhancement Via Correlation Coeﬃcients
and the maximum SPCC is
ρ2
z,x (a1) = λa1.
(4.25)
As a result, the optimal ﬁlter is proportional to a1, i.e.,
hx = αR−1
y γx,
(4.26)
where α ̸= 0 is an arbitrary real number, whose value is important in practice
when we deal with nonstationary signals such as speech3; its value is even
more important when hx is implemented in another domain such as the STFT
domain, where a frequency-dependent scaling does not aﬀect the subband
performance measures but greatly aﬀects the fullband ones. Hence, with hx
in (4.26), the estimate of x(t) is
bx(t) = hT
x y(t)
(4.27)
and the output SNR is given by
oSNR (hx) = hT
x Rxhx
hTx Rvhx
≥iSNR.
(4.28)
Now, we need to determine α. There are at least three ways to ﬁnd this
parameter. The ﬁrst one is from the mean-squared error (MSE) criterion
between x(t) and bx(t), i.e.,
J (α) = E
{[
x(t) −hT
x y(t)
]2}
= E
{[
x(t) −αγT
x R−1
y y(t)
]2}
.
(4.29)
The minimization of J (α) with respect to α leads to
α = σ2
x.
(4.30)
Substituting this value into (4.26), we get the conventional Wiener ﬁlter [2]:
hW = σ2
xR−1
y γx
= R−1
y Rx1i
= R−1
y Rxi
=
(
IL −R−1
y Rv
)
i.
(4.31)
3 Obviously, for stationary signals, the value of α is not relevant at all as long as it is
diﬀerent from zero.

4.3 Optimal Filters
51
Obviously, this ﬁlter maximizes the SPCC in (4.10) but it does not maximize
the output SNR. We will see later which kind of the SPCC whose maximiza-
tion is equivalent to maximizing the output SNR.
The second possibility is from the distortion-based MSE, i.e.,
Jd (α) = E
{[
x(t) −hT
x x(t)
]2}
= E
{[
x(t) −αγT
x R−1
y x(t)
]2}
.
(4.32)
By minimizing Jd (α) with respect to α, we obtain
α =
λa1
γTx R−1
y RxR−1
y γx
(4.33)
and substituting the previous result into (4.26) gives the minimum distortion
(MD) ﬁlter:
hMD =
λa1R−1
y γx
γTx R−1
y RxR−1
y γx
.
(4.34)
Clearly, as far as the output SNR is concerned, the two ﬁlters hW and hMD
are equivalent but when implemented in the STFT domain, they will give
much diﬀerent values of the fullband output SNR.
Finally, the last manner to ﬁnd α is by plugging hv = i −αR−1
y γx into
(4.10). We get
ρ2
z,x (α) =
(
i −αR−1
y γx
)T Rx1
(
i −αR−1
y γx
)
(
i −αR−1
y γx
)T Ry
(
i −αR−1
y γx
)
= σ2
x
(
1 −αγT
x R−1
y γx
)2
σ2y −2α + α2γTx R−1
y γx
.
(4.35)
Since hv is involved in the SPCC, we need to minimize this latter. Minimizing
the previous expression is equivalent to minimizing its numerator. Therefore,
we have
α =
1
γTx R−1
y γx
.
(4.36)
As a result, we deduce the so-called minimum variance distortionless response
(MVDR) ﬁlter [6]:
hMVDR =
R−1
y γx
γTx R−1
y γx
.
(4.37)

52
4 Speech Enhancement Via Correlation Coeﬃcients
Indeed, one can check that hT
MVDRγx = 1, which means that the desired
signal is recovered if xi(t) is considered as an interference. This ﬁlter works
very well in the STFT domain [7], [8], [9], [10].
4.3.1.2 Minimization of the SPCC
Another perspective is to ﬁnd the ﬁlter that minimizes (4.10). Therefore, the
ﬁlter output will be the estimate of v(t). The matrix R−1
y Rx1 has L −1
eigenvalues equal to 0, since its rank is equal to 1. Let a2, a3, . . . , aL be the
corresponding eigenvectors and let us consider the ﬁlter, which is a linear
combination of these eigenvectors:
hv =
L
∑
i=2
αiai
(4.38)
= A2α,
where
A2 =
[ a2 a3 · · · aL
]
(4.39)
is a matrix of size L × (L −1) and
α =
[α2 α3 · · · αL
]T ̸= 0
(4.40)
is a vector of length L−1. It is clear that hv in (4.38) minimizes (4.10), since
ρ2
zx (hv) = 0.
(4.41)
Therefore, the estimates of v(t) and x(t) are, respectively,
bv(t) = hT
v y(t)
(4.42)
and
bx(t) = y(t) −bv(t)
= hT
x y(t),
(4.43)
where
hx = i −hv
(4.44)
is the equivalent ﬁlter for the estimation of x(t).
There are at least two interesting ways to ﬁnd α. The ﬁrst one is from the
power of the residual noise, i.e.,

4.3 Optimal Filters
53
Jr (α) = hT
x Rvhx
= (i −hv)T Rv (i −hv)
= σ2
v −2αT AT
2 Rvi + αT AT
2 RvA2α
(4.45)
and the second one is from the MSE between x(t) and bx(t), i.e.,
J (α) = E
{[
x(t) −hT
x y(t)
]2}
= E
{[
x(t) −(i −hv)T y(t)
]2}
= E
{[
v(t) −αT AT
2 y(t)
]2}
.
(4.46)
The minimization of Jr (α) with respect to α gives
α =
(
AT
2 RvA2
)−1 AT
2 Rvi.
(4.47)
As a result, we obtain the minimum noise (MN) ﬁlter for the estimation of
x(t):
hMN =
[
IL −A2
(
AT
2 RvA2
)−1 AT
2 Rv
]
i.
(4.48)
While this ﬁlter may reduce quite a lot of noise, it may introduce an unac-
ceptable amount of distortion to the desired signal.
By minimizing the MSE, we ﬁnd that
α =
(
AT
2 RyA2
)−1 AT
2 Rvi
= AT
2 Rvi.
(4.49)
We deduce the MVDR ﬁlter for the estimation of x(t):
hMVDR = i −A2AT
2 Rvi
= i −
(
R−1
y
−a1aT
1
)
Rvi
= hW +
(
aT
1 Rvi
)
a1
= hW +
(
aT
1 Ryi −aT
1 Rx1i
)
a1
=
(
aT
1 Ryi
)
a1
=
R−1
y γx
γTx R−1
y γx
.
(4.50)
As far as the output SNR is concerned, the two ﬁlters hW and hMVDR are
equivalent. However, in the STFT domain, hW and hMVDR will behave diﬀer-
ently. Another insightful way to derive hMVDR is by substituting hx = i−A2α
into the SPCC in (4.10). We get

54
4 Speech Enhancement Via Correlation Coeﬃcients
ρ2
z,x (α) = (i −A2α)T Rx1 (i −A2α)
(i −A2α)T Ry (i −A2α)
=
σ2
x
σ2y −2αT AT
2 Ryi + αT α.
(4.51)
Maximizing the previous expression is equivalent to minimizing its denomi-
nator. We easily obtain
α = AT
2 Ryi
= AT
2 Rxi + AT
2 Rvi
= AT
2 Rx1i + AT
2 Rvi
= AT
2 Rvi,
(4.52)
which leads to hMVDR. It is clear from the above that
σ2
v ≥αT α.
(4.53)
Therefore, with hW or hMVDR, we can express the SPCC between z(t) and
x(t) as
ρ2
z,x (hW) = λa1
=
σ2
x
σ2y −αT α
=
iSNR
σ2
v −αT α
σ2v
+ iSNR
,
(4.54)
which shows a very interesting relationship between the eigenvalue of interest
and the input SNR. We always have
ρ2
z,x (hW) ≥ρ2
x,y =
iSNR
1 + iSNR,
(4.55)
where ρ2
x,y is the SPCC between x(t) and y(t). The previous expression tells
us that z(t) (with the Wiener ﬁlter) and x(t) are more correlated than y(t)
and x(t) are, which basically means that the SNR of z(t) is better than that
of y(t).

4.3 Optimal Filters
55
4.3.2 SPCC Between Filter Output and Noise Signal
In this subsection, we consider the SPCC between z(t) and v(t). A maximal
(resp. minimal) value of the SPCC implies that z(t) could be the estimate of
v(t) [resp. x(t)].
4.3.2.1 Maximization of the SPCC
The rank of the matrix R−1
y Rv1 is equal to 1, so its only non-null and positive
eigenvalue is
λb1 = σ2
vγT
v R−1
y γv,
(4.56)
whose corresponding eigenvector is
b1 =
R−1
y γv
√
γTv R−1
y γv
.
(4.57)
As a result, the ﬁlter that maximizes (4.14) is
hv = βR−1
y γv,
(4.58)
where β ̸= 0 is an arbitrary real number, and the maximum SPCC is
ρ2
z,v (b1) = λb1.
(4.59)
This ﬁlter output gives the estimate of v(t), i.e.,
bv(t) = hT
v y(t).
(4.60)
We deduce that the estimate of the desired signal is
bx(t) = y(t) −bv(t)
= hT
x y(t),
(4.61)
where
hx = i −hv
(4.62)
is the equivalent ﬁlter for the estimation of x(t).
One way to ﬁnd β is from the MSE between x(t) and bx(t) [or, equivalently,
v(t) and bv(t)], i.e.,

56
4 Speech Enhancement Via Correlation Coeﬃcients
J (β) = E
{[
v(t) −hT
v y(t)
]2}
= E
{[
v(t) −βγT
v R−1
y y(t)
]2}
.
(4.63)
Indeed, the optimization of the previous expression leads to
β = σ2
v.
(4.64)
Therefore, we have
hv = σ2
vR−1
y γv
(4.65)
and from (4.62),
hW =
(
IL −R−1
y Rv
)
i,
(4.66)
which is the classical Wiener ﬁlter.
The second way to ﬁnd β is from the power of the residual noise, i.e.,
Jr (β) =
(
i −βR−1
y γv
)T Rv
(
i −βR−1
y γv
)
(4.67)
= σ2
v −2βγT
v R−1
y Rvi + β2γT
v R−1
y RvR−1
y γv.
After minimizing Jr (β) and substituting the obtained value of β into (4.58),
we easily ﬁnd that the MN-type ﬁlter for the estimation of x(t) is
hMN,2 =
[
IL −
R−1
y Rv1
tr
(
R−1
y RvR−1
y Rv1
)R−1
y Rv
]
i.
(4.68)
Finally, the last way to ﬁnd β is by plugging hx = i−βR−1
y γv into (4.14).
We get
ρ2
z,v (β) =
(
i −βR−1
y γv
)T Rv1
(
i −βR−1
y γv
)
(
i −βR−1
y γv
)T Ry
(
i −βR−1
y γv
)
= σ2
v
(
1 −βγT
v R−1
y γv
)2
σ2y −2β + β2γTv R−1
y γv
.
(4.69)
Minimizing the previous expression is equivalent to minimizing its numerator.
This leads to
β =
1
γTv R−1
y γv
.
(4.70)
As a result, we ﬁnd the null constraint (NC) ﬁlter for the estimation of x(t):

4.3 Optimal Filters
57
hNC =
(
IL −R−1
y γvγT
v
γTv R−1
y γv
)
i.
(4.71)
Indeed, it can easily be veriﬁed that hT
NCγv = 0, which means that the
correlated noise is completely canceled.
4.3.2.2 Minimization of the SPCC
Let b2, b3, . . . , bL be the eigenvectors corresponding to the L −1 null eigen-
values of the matrix R−1
y Rv1. Let us form the ﬁlter:
hx =
L
∑
i=2
βibi
= B2β,
(4.72)
where βi, i = 2, 3, . . . , L are arbitrary real numbers with at least one of them
diﬀerent from 0,
B2 =
[ b2 b3 · · · bL
]
(4.73)
is a matrix of size L × (L −1), and
β =
[ β2 β3 · · · βL
]T ̸= 0
(4.74)
is a vector of length L −1. It can be veriﬁed that hx in (4.72) minimizes
(4.14), since
ρ2
z,v (hx) = 0.
(4.75)
Therefore, the ﬁlter output can be considered as the estimate of the desired
signal, i.e.,
bx(t) = hT
x y(t).
(4.76)
The MSE between x(t) and bx(t) is then
J (β) = E
{[
x(t) −hT
x y(t)
]2}
(4.77)
= σ2
x −2βT BT
2 Rxi + βT β
= σ2
x −2βT BT
2 Rxi + βT BT
2 RxB2β + βT BT
2 RvB2β
= Jd (β) + Jr (β) .
From (4.77), we observe that we have at least two obvious options to ﬁnd β.
The ﬁrst one is to minimize J (β). The second option is to minimize Jd (β).

58
4 Speech Enhancement Via Correlation Coeﬃcients
From the ﬁrst option, we obtain the NC ﬁlter:
hNC = B2BT
2 Rxi
=
(
R−1
y
−b1bT
1
)
(Ry −Rv) i
= i −R−1
y Rvi −b1bT
1 Ryi + b1bT
1 Rv1i
= hW +
(
bT
1 Ryi
)
(λb1b1 −b1)
= i −
R−1
y γv
γTv R−1
y γv
=
(
IL −R−1
y γvγT
v
γTv R−1
y γv
)
i.
(4.78)
The second option gives the MD-type ﬁlter:
hMD,2 = B2
(
BT
2 RxB2
)−1 BT
2 Rxi,
(4.79)
where it is assumed that the rank of Rx is at least equal to L −1.
Now, let us ﬁnd β from the SPCC. Substituting hv = i −B2β into the
SPCC in (4.14), we obtain
ρ2
z,v (β) = (i −B2β)T Rv1 (i −B2β)
(i −B2β)T Ry (i −B2β)
=
σ2
v
σ2y −2βT BT
2 Ryi + βT β
.
(4.80)
Maximizing the previous expression is equivalent to minimizing its denomi-
nator. We get
β = BT
2 Ryi
= BT
2 Rxi + BT
2 Rvi
= BT
2 Rxi + BT
2 Rv1i
= BT
2 Rxi,
(4.81)
which leads to hNC. It is clear from the above that
σ2
x ≥βT β.
(4.82)
Therefore, we can express the maximum value of the SPCC between z(t) and
v(t) as

4.3 Optimal Filters
59
ρ2
z,v (b1) = λb1
=
σ2
v
σ2y −βT β
=
1
1 + iSNR × σ2
x −βT β
σ2x
,
(4.83)
which shows a very interesting relationship between the eigenvalue of interest
and the input SNR. We always have
ρ2
z,v (b1) ≥ρ2
v,y =
1
1 + iSNR,
(4.84)
where ρ2
v,y is the SPCC between v(t) and y(t). The previous expression tells
us that z(t) (which is here the estimate of v(t) with a ﬁlter proportional
to b1) and x(t) are more correlated than y(t) and v(t) are, which basically
means that the SNR of y(t) −z(t) is better than that of y(t).
4.3.3 SPCC Between Filter Output and Filtered
Desired Signal
This subsection is concerned with the SPCC between z(t) and xfd(t). A max-
imal (resp. minimal) value of the SPCC implies that z(t) is the estimate of
x(t) [resp. v(t)].
4.3.3.1 Maximization of the SPCC
Let λt1 be the largest eigenvalue, with multiplicity P, of the matrix R−1
y Rx4.
We denote t1, t2, . . . , tP the corresponding eigenvectors. It is clear that the
ﬁlter:
hx =
P
∑
p=1
θptp,
(4.85)
where θp, p = 1, 2, . . . , P are arbitrary real numbers with at least one of them
diﬀerent from 0, maximizes (4.17), and the maximum SPCC5 is
ρ2
z,xfd (hx) = λt1.
(4.86)
4 In practice, we may consider the P largest eigenvalues of R−1
y Rx. In this case, they are
denoted λt1, λt2, . . . , λtP .
5 In case we take the P largest eigenvalues, we have ρ2
z,xfd (hx) = ∑P
p=1 λtp/P.

60
4 Speech Enhancement Via Correlation Coeﬃcients
We can rewrite (4.85) as
hx = Tθ,
(4.87)
where
T =
[ t1 t2 · · · tP
]
(4.88)
is a matrix of size L × P and
θ =
[ θ1 θ2 · · · θP
]T ̸= 0
(4.89)
is a vector of length P. It can be checked that the SPCC can be written as
ρ2
z,xfd (hx) =
oSNR (hx)
1 + oSNR (hx),
(4.90)
which means that hx in (4.85) or in (4.87) also maximizes the output SNR.
The estimate of x(t) is
bx(t) = hT
x y(t).
(4.91)
The MSE between x(t) and bx(t) is then
J (θ) = E
{[
x(t) −hT
x y(t)
]2}
(4.92)
= σ2
x −2θT TT Rxi + θT θ
= σ2
x −2θT TT Rxi + θT TT RxTθ + θT TT RvTθ
= Jd (θ) + Jr (θ) .
From (4.92), we observe that we have at least two obvious options to ﬁnd θ.
The ﬁrst one is to minimize J (θ). The second option is to minimize Jd (θ).
From the ﬁrst option, we obtain the Wiener-type ﬁlter:
hW,2 = TTT Rxi.
(4.93)
The second option gives the MD-type ﬁlter:
hMD,3 = T
(
TT RxT
)−1 TT Rxi.
(4.94)
In the assumed case where we have a maximum eigenvalue, λt1, with multi-
plicity P, we have TT RxT = λt1IP , where IP is the P × P identity matrix.
As a result,
hMD,3 = λt1hW,2.
(4.95)
Now, substituting hv = i −Tθ into (4.17), we get

4.3 Optimal Filters
61
ρ2
z,xfd (θ) = (i −Tθ)T Rx (i −Tθ)
(i −Tθ)T Ry (i −Tθ)
(4.96)
= σ2
x −2θT TT Rxi + θT TT RxTθ
σ2y −2θT TT Ryi + θT θ
.
It is clear that minimizing (4.96) is the same as minimizing its numerator.
This leads to θ =
(
TT RxT
)−1 TT Rxi and then to hMD,3. This is another
way to derive the MD-type ﬁlter given in (4.94).
4.3.3.2 Minimization of the SPCC
Let λtL be the smallest eigenvalue, with multiplicity Q, of the matrix
R−1
y Rx6. We denote tL−Q+1 = t′
1, tL−Q+2 = t′
2, . . . , tL = t′
Q the corre-
sponding eigenvectors. The ﬁlter:
hv =
Q
∑
q=1
θ′
qt′
q,
(4.97)
where θ′
q, q = 1, 2, . . . , Q are arbitrary real numbers with at least one of them
diﬀerent from 0, minimizes (4.17), and the minimum SPCC7 is
ρ2
z,xfd (hv) = λtL.
(4.98)
A more convenient way to write (4.97) is
hv = T′θ′,
(4.99)
where
T′ =
[t′
1 t′
2 · · · t′
Q
]
(4.100)
is a matrix of size L × Q and
θ′ =
[ θ′
1 θ′
2 · · · θ′
Q
]T ̸= 0
(4.101)
is a vector of length Q. Therefore, the estimates of v(t) and x(t) are, respec-
tively,
bv(t) = hT
v y(t)
(4.102)
and
6 In practice, we may consider the Q smallest eigenvalues of R−1
y Rx. In this case, they
are denoted λtL−Q+1, λtL−Q+2, . . . , λtL.
7 In case we take the Q smallest eigenvalues, we have ρ2
z,xfd (hv) = ∑Q
q=1 λtL−q+1/Q.

62
4 Speech Enhancement Via Correlation Coeﬃcients
bx(t) = y(t) −bv(t)
= hT
x y(t),
(4.103)
where
hx = i −hv
(4.104)
is the equivalent ﬁlter for the estimation of x(t).
There are at least two interesting ways to ﬁnd θ′. The ﬁrst one is from the
power of the residual noise, i.e.,
Jr
(
θ′)
= E
{[
v(t) −θ′T T′T v(t)
]2}
(4.105)
and the second one is from the MSE between x(t) and bx(t), i.e.,
J
(
θ′)
= E
{[
v(t) −θ′T T′T y(t)
]2}
.
(4.106)
The minimization of Jr
(
θ′)
with respect to θ′ gives
θ′ =
(
T′T RvT′)−1 T′T Rvi.
(4.107)
As a result,
hv = T′ (
T′T RvT′)−1 T′T Rvi
(4.108)
and the MN-type ﬁlter for the estimation of x(t) is
hMN,3 =
[
IL −T′ (
T′T RvT′)−1 T′T Rv
]
i.
(4.109)
By minimizing the MSE, we ﬁnd the Wiener-type ﬁlter for the estimation
of x(t):
hW,3 =
[
IL −T′ (
T′T RyT′)−1 T′T Rv
]
i
=
(
IL −T′T′T Rv
)
i.
(4.110)
Now, let us see what happens from the SPCC perspective. Plugging hx =
i −T′θ′ into (4.17), we get

References
63
ρ2
z,xfd
(
θ′)
=
(
i −T′θ′)T Rx
(
i −T′θ′)
(
i −T′θ′)T Ry
(
i −T′θ′)
(4.111)
= σ2
x −2θ′T T′T Rxi + θ′T T′T RxT′θ′
σ2y −2θ′T T′T Ryi + θ′T θ′
=
σ2
x −λtL
(
2θ′T T′T Rxi −θ′T θ′)
σ2y −
(
2θ′T T′T Ryi −θ′T θ′)
.
Maximizing the previous expression is equivalent to minimizing the quantity
2θ′T T′T Ryi −θ′T θ′. Therefore, we get another Wiener-type ﬁlter:
hW,4 =
(
IL −T′T′T Ry
)
i.
(4.112)
Because of the relation (4.21), the optimization of the SPCC between the
ﬁlter output and the ﬁltered noise signal, i.e., ρ2
zvfn (h), will lead to the same
optimal ﬁlters derived in this subsection.
4.3.4 Other Possibilities
Obviously, it is possible to derive other noise reduction ﬁlters by combining
some of the deﬁned SPCCs. Here, we brieﬂy discuss one valuable possibility.
In this approach, we combine the two SPCCs:
ρ2
z,vfn (h) + ρ2
z,xfi (h) = hT (Rv + Rxi) h
hT Ryh
(4.113)
= ρ2
z,vfn+xfi (h) ,
where xi(t) is considered as an uncorrelated interference vector. Clearly, the
ﬁlter that minimizes (4.113) will make z(t) the estimate of x(t). A maximal
value of ρ2
z,vfn+xfi (h) implies that z(t) will be the estimate of v(t)+xi(t) and,
as a result, the estimate of x(t) will be y(t) −z(t). It is easy to derive all
relevant ﬁlters by following the same steps as above.
References
1. J. Benesty, J. Chen, and Y. Huang, “On the importance of the Pearson correlation
coeﬃcient in noise reduction,” IEEE Trans. Audio, Speech, Language Process., vol.
16, pp. 757–765, May 2008.
2. J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, 2009.

64
4 Speech Enhancement Via Correlation Coeﬃcients
3. J. Yu, J. Benesty, G. Huang, and J. Chen, “Examples of optimal noise reduction ﬁlters
derived from the squared Pearson correlation coeﬃcient,” in Proc. IEEE ICASSP,
2014, pp. 1571–1575.
4. J. Yu, J. Benesty, G. Huang, and J. Chen, “Optimal single-channel noise reduction
ﬁltering matrices from the Pearson correlation coeﬃcient perspective,” in Proc. IEEE
ICASSP, 2015, pp. 201–205.
5. J. N. Franklin, Matrix Theory. Englewood Cliﬀs, NJ: Prentice-Hall, 1968.
6. J. Benesty, J. Chen, Y. Huang, and T. Gaensler, “Time-domain noise reduction based
on an orthogonal decomposition for desired signal extraction,” J. Acoust. Soc. Am.,
vol. 132, pp. 452–464, July 2012.
7. J. Benesty and Y. Huang, “A single-channel noise reduction MVDR ﬁlter,” in Proc.
IEEE ICASSP, 2011, pp. 273–276.
8. A. Schasse and R. Martin, “Estimation of subband speech correlations for noise re-
duction via MVDR processing,” IEEE Trans. Audio, Speech, Language Process., vol.
22, pp. 1355-1365, Sept. 2014.
9. D. Fischer and T. Gerkmann, “Single-microphone speech enhancement using MVDR
ﬁltering and Wiener post-ﬁltering,” in Proc. IEEE ICASSP, 2016, pp. 201–205.
10. D. Fischer, S. Doclo, E. A. P. Habets, and T. Gerkmann, “Combined single-microphone
Wiener and MVDR ﬁltering based on speech interframe correlations and speech pres-
ence probability,” in Proc. ITG Conf. Speech Communication, 2016, pp. 292–296.

Chapter 5
On the Output SNR in Speech
Enhancement and Beamforming
The output SNR is a well-known and accurate measure of the SNR after the
ﬁltering/beamforming operation; it is widely used to evaluate all kinds of
optimal ﬁlters/beamformers for speech enhancement. However, it has never
really been fully exploited for the derivation of other noise reduction ﬁlters
than the classical maximum SNR ﬁlter. In this chapter, we ﬁrst show how
the output SNR is related to the fullmode input SNR and, then, derive very
interesting ﬁlters/beamformers by alternating between two related ﬁlters in
the maximization and minimization of the output SNR.
5.1 Signal Model and Problem Formulation
We consider the signal model in which we have M observed signals in a vector
form [1]:
y =
[ Y1 Y2 · · · YM
]T
= x + v,
(5.1)
where y is the noisy (observed) signal vector, x is the speech signal vec-
tor, v is the noise signal vector, and vectors x and v are deﬁned similarly
to vector y. All signals are assumed to be random, complex, circular, zero
mean, and stationary. Furthermore, the vectors x and v are assumed to be
uncorrelated, i.e., E
(
xvH)
= 0. It can be veriﬁed that the signal model in
(5.1) encompasses all aspects of speech enhancement and beamforming, from
the single-channel to the multichannel scenario, in the time, frequency, and
time-frequency domains. In the particular case of beamforming and taking
the ﬁrst microphone as the reference, (5.1) is expressed as [1], [2]
y = X1d + v,
(5.2)
65
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_5

66
5 Output SNR in Speech Enhancement/Beamforming
where d is the (deterministic) steering vector of length M, whose ﬁrst entry
is equal to 1.
Then, with the ﬁrst element of y being the reference, which will always be
true here, our objective in the general case is to estimate X1, i.e., the ﬁrst
element of x, given y uniquely from the output SNR, which is a good measure
of the SNR after linear processing. We want to show that the output SNR
is also an excellent criterion from which optimal ﬁlters/beamformers can be
derived.
From (5.1), we deduce that the covariance matrix (of size M × M) of y is
Φy = E
(
yyH)
(5.3)
= Φx + Φv,
where Φx = E
(
xxH)
and Φv = E
(
vvH)
are the covariance matrices of x
and v, respectively. It will always be assumed that Φv has full rank. For the
covariance matrix Φx, we are interested in three cases that often appear in the
problem of speech enhancement. They are the following. Case 1: rank (Φx) =
1 [and corresponds to the signal model in (5.2)], which implies that Φx =
ϕX1ddH, where ϕX1 = E
(
|X1|2)
is the variance of X1. Case 2: rank (Φx) =
P, where 1 ≤P < M. Case 3: rank (Φx) = M, i.e., Φx has full rank. From
(5.3), we can deﬁne the input SNR as
iSNR = tr (Φx)
tr (Φv)
(5.4)
= ϕX1
ϕV1
,
where ϕV1 = E
(
|V1|2)
is the variance of V1, i.e., the ﬁrst component of v. In
(5.4), it is explicitly assumed that ϕX1 ≈tr (Φx) /M and ϕV1 ≈tr (Φv) /M,
which is almost always the case in practice. With this conventional deﬁnition
of the SNR, we conclude this section.
5.2 Linear Filtering, Output and Fullmode Input SNRs
In this chapter, we estimate the desired speech signal, X1, or the noise signal,
V1, by applying a complex-valued ﬁlter, h, of length M, to the noisy signal
vector, y, i.e.,
Z = hHy
(5.5)
= Xfd + Vfn,
where Z can be either the estimate of X1 or V1,

5.2 Linear Filtering, Output and Fullmode Input SNRs
67
Xfd = hHx
(5.6)
is the ﬁltered desired signal, and
Vfn = hHv
(5.7)
is the ﬁltered noise signal. If Z is the estimate of V1, then the estimate of X1
is
b
X1 = Y1 −Z
(5.8)
= Y1 −hHy
= (i −h)H y,
where i is the ﬁrst column of the M × M identity matrix IM. In the rest, we
will also use the notation hX and hV . The ﬁrst ﬁlter, hX, corresponds to the
estimation of X1 while the second ﬁlter, hV , corresponds to the estimation
of V1. Obviously, from (5.8), we have the relationship:
hX + hV = i.
(5.9)
Therefore, when V1 is estimated with hV , we can estimate X1 with hX,
thanks to the relation in (5.9). From (5.5), we see that the variance of Z is
ϕZ = E
(
|Z|2)
(5.10)
= ϕXfd + ϕVfn,
where
ϕXfd = hHΦxh,
(5.11)
ϕVfn = hHΦvh.
(5.12)
As a result, the output SNR can be deﬁned as
oSNR (h) = hHΦxh
hHΦvh.
(5.13)
When the output SNR is maximized (resp. minimized), we write oSNR (hX)
[resp. oSNR (hV )] since in this case, the ﬁlter hX (resp. hV ) corresponds to
the estimation of X1 (resp. V1). We will see that by alternating between the
two ﬁlters hX and hV in the optimization (i.e., maximization or minimiza-
tion) of the output SNR, we can derive very interesting ﬁlters/beamformers.
Given the structure of the output SNR, which is simply the generalized
Rayleigh quotient, joint diagonalization is going to be a very natural tool to
exploit here. The two Hermitian matrices Φx and Φv can be jointly diago-
nalized as follows [3]:

68
5 Output SNR in Speech Enhancement/Beamforming
AHΦxA = Λ,
(5.14)
AHΦvA = IM,
(5.15)
where
A =
[a1 a2 · · · aM
]
(5.16)
is a full-rank square matrix (of size M × M) and
Λ = diag (λ1, λ2, . . . , λM)
(5.17)
is a diagonal matrix whose main elements are real and nonnegative. The
eigenvalues of Φ−1
v Φx are ordered as λ1 ≥λ2 ≥· · · ≥λM ≥0. We also
denote by a1, a2, . . . , aM, the corresponding eigenvectors.
The procedure for jointly diagonalizing Φx and Φv consists of two steps
[4].
(i)
Calculate Λ and A′, the eigenvalue and (unnormalized) eigenvector ma-
trices, respectively, of Φ−1
v Φx, i.e.,
Φ−1
v ΦxA′ = A′Λ.
(5.18)
(ii)
Normalize the eigenvectors of Φ−1
v Φx such that (5.15) is satisﬁed. De-
noting by a′
m, m = 1, 2, . . . , M the (unnormalized) eigenvectors of Φ−1
v Φx,
then we need to ﬁnd the constants Cm’s such that am = Cma′
m satisfy
aH
mΦvam = 1. Hence,
Cm =
1
√
a′H
m Φva′m
, m = 1, 2, . . . , M.
(5.19)
Thus, we have
A = A′C,
(5.20)
where
C
is
a
diagonal
normalization
matrix
with
the
elements
{C1, C2, . . . , CM} on its main diagonal.
In the particular case of rank (Φx) = 1, we have
A =
[a1 A2
]
,
(5.21)
Λ = diag (λ1, 0, . . . , 0) ,
(5.22)
where
a1 =
Φ−1
v d
√
dHΦ−1
v d
(5.23)
and

5.2 Linear Filtering, Output and Fullmode Input SNRs
69
λ1 = ϕX1dHΦ−1
v d.
(5.24)
It is always possible to write h in a basis formed from the vectors am, m =
1, 2, . . . , M, i.e.,
h = Aα,
(5.25)
where the components, αm, m = 1, 2, . . . , M, of the vector α are the coordi-
nates of h in the new basis. As a consequence, the output SNR in (5.13) can
be rewritten, equivalently, as
oSNR (α) = αHΛα
αHα .
(5.26)
Another possible measure of the SNR, which can be close to the input
SNR, is the fullmode input SNR deﬁned as
iSNRFM = tr
(
Φ−1
v Φx
)
M
(5.27)
= tr
(
AAHΦx
)
M
= tr (Λ)
M
.
From the previous expression, we deﬁne the mth (m = 1, 2, . . . , M) spectral
mode input SNR:
iSNRm = λm.
(5.28)
The number of nonnull spectral modes is obviously equal to the rank of
Φx. So in the case of rank (Φx) = 1, the ﬁrst spectral mode input SNR is
equal to M times the fullmode input SNR, i.e., iSNR1 = M × iSNRFM and
iSNRi = 0, i = 2, 3, . . . , M. As a result, we can express the fullmode input
and output SNRs as, respectively,
iSNRFM =
∑M
m=1 iSNRm
M
(5.29)
and
oSNR (α) =
∑M
m=1 |αm|2 iSNRm
∑M
m=1 |αm|2
.
(5.30)
Since
tr
(
Φ−1
v Φx
)
≥tr (Φx)
tr (Φv),

70
5 Output SNR in Speech Enhancement/Beamforming
it follows that
iSNRFM ≥iSNR
M
,
(5.31)
or, equivalently,
M
∑
m=1
iSNRm ≥iSNR.
(5.32)
Property 5.1. Let
cond (Φv) = λ1 (Φv)
λM (Φv)
(5.33)
be the condition number of the matrix Φv, where λ1 (Φv) and λM (Φv) are,
respectively, the largest and smallest eigenvalues of Φv. We have
iSNR
cond (Φv) ≤iSNRFM ≤cond (Φv) × iSNR,
(5.34)
with iSNRFM = iSNR if and only if cond (Φv) = 1.
Proof. Since Φv is a positive deﬁnite matrix and Φx is a positive semideﬁnite
matrix, it can be shown that
tr (Φx)
λ1 (Φv) ≤tr
(
Φ−1
v Φx
)
≤tr (Φx)
λM (Φv).
(5.35)
But
tr
(
Φ−1
v Φx
)
M
≤
tr (Φv)
MλM (Φv)iSNR
≤Mλ1 (Φv)
MλM (Φv)iSNR
≤cond (Φv) × iSNR
(5.36)
and
tr
(
Φ−1
v Φx
)
M
≥
tr (Φv)
Mλ1 (Φv)iSNR
≥MλM (Φv)
Mλ1 (Φv) iSNR
≥
iSNR
cond (Φv).
(5.37)

5.2 Linear Filtering, Output and Fullmode Input SNRs
71
What does the fullmode input SNR mean? We can see that it has the
potential to be quite large and much larger than the conventional input SNR,
depending on the condition number of Φv. The most interesting and insightful
part of the fullmode input SNR is its decomposition into diﬀerent spectral
modes, which clearly shows the repartition of the SNR at diﬀerent spectral
bands. So when cond (Φv) is large, this means that the fullmode input SNR
is mostly governed by its largest modes. A great consequence of this is that it
tells us what amount of the output SNR we can expect with a linear ﬁlter since
this amount is always upper bounded by the maximum spectral mode input
SNR. In other words, the fullmode input SNR gives us great insights into the
potential of noise reduction while the conventional input SNR deﬁnition does
not lead to much interpretation except for its main purpose.
From the formulation of the output SNR in (5.30), which weights the
diﬀerent spectral modes of the fullmode input SNR, three obvious particular
cases of α appear naturally. The ﬁrst one is the equal-coordinate ﬁlter, i.e.,
α = α1 = α
[ 1 1 · · · 1 ]T , where α ̸= 0, which equally weights the diﬀerent
modes; therefore
oSNR (α1) = oSNR (1) = iSNRFM.
The second particular case is the maximum SNR ﬁlter, i.e., αmax
=
[ α1 0 · · · 0 ]T , where α1 ̸= 0, which gives the maximum value of the out-
put SNR, i.e.,
oSNR (αmax) = iSNR1 ≥oSNR (α) , ∀α ̸= 0,
or, equivalently,
oSNR (hmax) = λ1 ≥oSNR (h) , ∀h ̸= 0,
where hmax = Aαmax. Finally, the last one is the minimum SNR ﬁlter, i.e.,
αmin =
[0 · · · 0 αM
]T , where αM ̸= 0, which gives the minimum value of
the output SNR, i.e.,
oSNR (αmin) = iSNRM ≤oSNR (α) , ∀α ̸= 0,
or, equivalently,
oSNR (hmin) = λM ≤oSNR (h) , ∀h ̸= 0,
where hmin = Aαmin. Also, by playing on the values of the αm’s, we can
precisely manipulate the diﬀerent spectral modes of the fullmode input SNR
as we wish for speech enhancement. In other words, improving the SNR with
a linear ﬁlter is just a matter of adjusting the diﬀerent spectral mode input
SNRs, showing the importance of the fullmode input SNR deﬁnition. From
the above, we see that we always have

72
5 Output SNR in Speech Enhancement/Beamforming
iSNRM ≤iSNRFM ≤iSNR1,
iSNRM ≤oSNR (α) ≤iSNR1, ∀α ̸= 0.
Of course, for the estimation of the desired signal, X1, we must always ensure
that
oSNR (hX) > oSNR (i) = iSNR.
(5.38)
5.3 Optimal Filters
In this section, we develop a large class of optimal ﬁlters from the output
SNR depending on the rank of the speech covariance matrix.
5.3.1 Rank-One Speech Covariance Matrix
When the rank of Φx is equal to 1, it is clear that the ﬁlter that maximizes
the output SNR is proportional to a1 [see (5.23)], i.e.,
hX = αΦ−1
v d,
(5.39)
where α ̸= 0 is an arbitrary complex number.
Now, we need to determine α. This can be done by observing that while
hX maximizes the output SNR and gives the estimate of X1, the output SNR
with the ﬁlter hV = i −αΦ−1
v d can also be minimized in order to get the
estimate of V1. Substituting hV into (5.13), we get
oSNR (α) = ϕX1
(
i −αΦ−1
v d
)H ddH (
i −αΦ−1
v d
)
(
i −αΦ−1
v d
)H Φv
(
i −αΦ−1
v d
)
.
(5.40)
Minimizing the previous expression is equivalent to minimizing its numerator.
Therefore, we have
α =
1
dHΦ−1
v d.
(5.41)
Substituting α back into (5.40), we see that oSNR (α) = 0, proving that
the output SNR is indeed minimized. As a result, we deduce the celebrated
MVDR ﬁlter:
hMVDR =
Φ−1
v d
dHΦ−1
v d.
(5.42)

5.3 Optimal Filters
73
Let us turn our attention to the estimation of V1 in the ﬁrst step. It is
cleat that the ﬁlter:
hV = A2α2,
(5.43)
where α2 ̸= 0 is a vector of length M −1, minimizes the output SNR since
oSNR (hV ) = 0. To obtain the estimate of X1, we plug hX = i −A2α2 in
the deﬁnition of the output SNR, resulting in
oSNR (α2) = ϕX1 (i −A2α2)H ddH (i −A2α2)
(i −A2α2)H Φv (i −A2α2)
=
ϕX1
(i −A2α2)H Φv (i −A2α2)
.
(5.44)
The maximization of oSNR (α2) is equivalent to the minimization of its de-
nominator. We easily get
α2 = AH
2 Φvi
(5.45)
and the optimal ﬁlter for the estimation of X1 is
hX = i −A2AH
2 Φvi
(5.46)
= i −
(
Φ−1
v
−a1aH
1
)
Φvi
=
(
aH
1 Φvi
)
a1
=
Φ−1
v d
dHΦ−1
v d = hMVDR,
(5.47)
which is again the MVDR ﬁlter.
5.3.2 Rank-Deﬁcient Speech Covariance Matrix
In this subsection, we focus on the case where rank (Φx) = P with 1 ≤P <
M. We already know that the ﬁlter that maximizes the output SNR is
hX = α1a1,
(5.48)
where α1 ̸= 0 is an arbitrary complex number. To ﬁnd α1, we use the ﬁlter
hV = i −α1a1 in the output SNR, which leads to

74
5 Output SNR in Speech Enhancement/Beamforming
oSNR (α1) = (i −α1a1)H Φx (i −α1a1)
(i −α1a1)H Φv (i −α1a1)
(5.49)
=
ϕX1 −λ1
[
2ℜ
(
α1iT Φva1
)
−|α1|2]
ϕV1 −
[
2ℜ(α1iT Φva1) −|α1|2]
,
and whose minimization gives
α1 = aH
1 Φvi
= aH
1 Φxi
λ1
,
(5.50)
where ℜ(·) is the real part of a complex number. We deduce the maximum
SNR ﬁlter with minimum distortion (MD):
hmMD = a1aH
1 Φxi
λ1
(5.51)
= a1aH
1 Φvi.
Obviously, this ﬁlter is very much diﬀerent from the MVDR ﬁlter in (5.42)
since a1 does not have the form in (5.23), in general. In fact, the larger the
value of P, the more diﬀerent are the two ﬁlters. While hmMD gives the
maximum possible output SNR, speech distortion worsens as P increases.
However, for P = 1, hMVDR and hmMD are identical.
Now, let us derive the optimal ﬁlter when V1 is estimated ﬁrst. Deﬁne the
matrix of size M × (M −P):
AP +1 =
[aP +1 aP +2 · · · aM
]
.
(5.52)
One can verify that the ﬁlter:
hV = AP +1αP +1,
(5.53)
where αP +1 ̸= 0 is a vector of length M −P, minimizes the output SNR since
oSNR (hV ) = 0. To get the estimate of X1, we insert hX = i −AP +1αP +1
in the deﬁnition of the output SNR, resulting in
oSNR (αP +1) = (i −AP +1αP +1)H Φx (i −AP +1αP +1)
(i −AP +1αP +1)H Φv (i −AP +1αP +1)
=
ϕX1
(i −AP +1αP +1)H Φv (i −AP +1αP +1)
.
(5.54)
The maximization of the previous expression gives
αP +1 = AH
P +1Φvi.
(5.55)

5.3 Optimal Filters
75
As a result, we obtain the distortionless (DL) ﬁlter:
hDL = i −AP +1AH
P +1Φvi.
(5.56)
This ﬁlter is, indeed, distortionless since
hT
DLx = X1 −iT ΦvAP +1AH
P +1x
= X1,
(5.57)
where we used the fact that AH
P +1x = 0 [derived from (5.14)]. For P = 1,
one can check that hDL and hMVDR are identical. As P increases, the output
SNR of hDL decreases.
5.3.3 Full-Rank Speech Covariance Matrix
When Φx = M, we can also derive the maximum SNR ﬁlter with minimum
distortion, i.e., hmMD. However, this ﬁlter may lead to very large distortions
since it considers only the main direction of the desired signal as compared
to the noise, i.e., the maximum spectral mode of the fullmode input SNR. In
order to reduce distortion, we need to consider more than one spectral mode
but at the price of a lower output SNR. This is the classical compromise
between noise reduction and speech distortion that we clearly see from this
formulation, which can lead to much more accurate compromises than those
obtained from some conventional approaches.
Let us consider the Q (1 ≤Q ≤M) largest spectral modes of the fullmode
input SNR. For that, we deﬁne the matrix of size M × Q:
A1:Q =
[ a1 a2 · · · aQ
]
.
(5.58)
We choose ﬁlters of the form:
hX,Q = A1:Qα1:Q,
(5.59)
where α1:Q ̸= 0 is a vector of length Q. To ﬁnd α1:Q, we use the ﬁlter
hV,Q = i −A1:Qα1:Q in the output SNR, which leads to
oSNR (α1:Q) = (i −A1:Qα1:Q)H Φx (i −A1:Qα1:Q)
(i −A1:Qα1:Q)H Φv (i −A1:Qα1:Q)
(5.60)
=
ϕX1 −
[
2ℜ
(
iT ΦvA1:QΛ1:Qα1:Q
)
−αH
1:QΛ1:Qα1:Q
]
ϕV1 −
[
2ℜ(iT ΦvA1:Qα1:Q) −αH
1:Qα1:Q
]
,
where

76
5 Output SNR in Speech Enhancement/Beamforming
Λ1:Q = diag (λ1, λ2, . . . , λQ) .
(5.61)
The minimization of (5.60) gives
α1:Q = AH
1:QΦvi
= Λ−1
1:QAH
1:QΦxi.
(5.62)
We deduce the ﬁrst class of compromising ﬁlters:
hX,1,Q = A1:QΛ−1
1:QAH
1:QΦxi
(5.63)
= A1:QAH
1:QΦvi.
For Q = 1, we have the maximum SNR ﬁlter with MD, i.e., hX,1,Q = hmMD,
and for Q = M, we have the identity ﬁlter, i.e., hX,1,M = i. We should always
have
oSNR (hX,1,1) ≥oSNR (hX,1,2) ≥· · · ≥oSNR (hX,1,M) = iSNR.
(5.64)
A very interesting particular case of (5.59) is
hX,1,Q = αA1:Q11:Q,
(5.65)
where α ̸= 0 and 11:Q is a vector of length Q whose all elements are 1’s. the
parameter α is obtained as explained above. We get
α =
1T
1:QAH
1:QΦvi
Q
(5.66)
As a result, the ﬁlter in (5.65) is
hX,1,Q =
A1:Q11:Q1T
1:QAH
1:QΦvi
Q
.
(5.67)
What makes this ﬁlter so interesting is that its output SNR is
oSNR (hX,1,Q) =
1T
1:QAH
1:QΦxA1:Q11:Q
1T
1:QAH
1:QΦvA1:Q11:Q
=
∑Q
q=1 iSNRq
Q
.
(5.68)
Therefore,
oSNR (hX,1,1) ≥oSNR (hX,1,2) ≥· · · ≥oSNR (hX,1,M) = iSNRFM. (5.69)
However, this ﬁlter may distort more the speech signal than hX,1,Q.

5.4 Application to Fixed and Superdirective Beamforming
77
Now, let us consider the M −R (0 ≤R ≤M −1) smallest spectral modes
of the fullmode input SNR and deﬁne the ﬁlters:
hV,R = AR+1αR+1,
(5.70)
where
AR+1 =
[aR+1 aR+2 · · · aM
]
(5.71)
is a matrix of size M × (M −R) and αR+1 ̸= 0 is a vector of length M −R.
Substituting hX,R = i −AR+1αR+1 into the output SNR, we get
oSNR (αR+1) = (i −AR+1αR+1)H Φx (i −AR+1αR+1)
(i −AR+1αR+1)H Φv (i −AR+1αR+1)
(5.72)
= ϕX1 −
[
2ℜ
(
iT ΦvAR+1ΛR+1αR+1
)
−αH
R+1Λ1:QαR+1
]
ϕV1 −
[
2ℜ(iT ΦvAR+1αR+1) −αH
R+1αR+1
]
,
where
ΛR+1 = diag (λR+1, λR+1, . . . , λM) .
(5.73)
From the maximization of (5.72), we obtain
αR+1 = AH
R+1Φvi
= Λ−1
R+1AH
R+1Φxi.
(5.74)
We deduce the second class of compromising ﬁlters:
hX,2,R = i −AR+1Λ−1
R+1AH
R+1Φxi
(5.75)
= i −AR+1AH
R+1Φvi,
which is equivalent to the ﬁrst class.
5.4 Application to Fixed and Superdirective
Beamforming
We consider a plane wave, in the farﬁeld, that propagates in an anechoic
acoustic environment at the speed of sound, i.e., c = 340 m/s, and impinges on
a uniform linear array (ULA) consisting of M omnidirectional microphones,
where the distance between two successive sensors is equal to δ. The direction
of the source signal to the array is parameterized by the azimuth angle θ. In
this context, the steering vector (of length M) is given by

78
5 Output SNR in Speech Enhancement/Beamforming
dθ =
[
1 e−ȷ2πfτ0 cos θ · · · e−ȷ(M−1)2πfτ0 cos θ ]T ,
(5.76)
where ȷ = √−1 is the imaginary unit, f > 0 is the temporal frequency, and
τ0 = δ/c is the delay between two successive sensors at the angle θ = 0. Like
in superdirective beamforming [5], [6], we assume that the main lobe is at
the angle θ = 0 (endﬁre direction) and the desired signal propagates from
the same angle, so that the corresponding steering vector is d0. It will also
be assumed that δ is small.
From the gain in SNR, two important measures, which do not depend on
the statistics of the signals but on some noise models, are derived for ﬁxed
beamforming. They are the white noise gain (WNG):
W (h) =
hHd0
2
hHh
(5.77)
and the directivity factor (DF):
D (h) =
hHd0
2
hHΓdh ,
(5.78)
where the elements of Γd are given by
[Γd]ij = sin [2πf(j −i)τ0]
2πf(j −i)τ0
= sinc [2πf(j −i)τ0] .
(5.79)
The WNG is a measure of the sensitivity of the microphone array to some of
its imperfections, such as sensor noise, while the DF quantiﬁes how the same
array performs in the presence of reverberation.
From the maximization of the WNG, we ﬁnd the well-known delay-and-
sum (DS) beamformer [4]:
hDS = d0
M ,
(5.80)
with W (hDS) = M = Wmax. While the DS beamformer maximizes the WNG,
it never ampliﬁes the diﬀuse noise since D (hDS) ≥1. However, this DF is not
very large and the beampattern of the DS beamformer is very frequency de-
pendent. If we maximize the DF, we easily get the superdirective beamformer
[4], [5], [6]:
hS =
Γ−1
d d0
dH
0 Γ−1
d d0
,
(5.81)

5.4 Application to Fixed and Superdirective Beamforming
79
with D (hS) = dH
0 Γ−1
d d0 = Dmax. While the superdirective beamformer max-
imizes the DF (leading to supergains), its WNG may be smaller than 1, which
implies white noise ampliﬁcation, especially at low frequencies.
Now, let us develop things from our perspective. Let us start by deﬁning
the set S = {d0, i2, . . . , iM} containing M linearly independent vectors that
span the M-dimensional Euclidean space, where ii is the ith column of IM.
Thanks to the Gram-Schmidt orthonormalization process, we can easily gen-
erate from S another set So = {u1, u2, . . . , uM} whose orthonormal vectors
span the same space. It is cleat that
u1 =
d0
√
dH
0 d0
=
d0
√
M
(5.82)
and
uH
i d0 = 0, i = 2, 3, . . . , M.
(5.83)
From So, we can form the M × M unitary matrix:
U =
[ u1 u2 · · · uM
]
(5.84)
=
[ u1 U2
]
,
where UUH = UHU = IM.
The beamformer that minimizes both the WNG and the DF has the form:
hV = U2α2,
(5.85)
where α2 ̸= 0 is an arbitrary complex-valued vector of length M −1. Indeed,
one can check that W (hV ) = D (hV ) = 0. Therefore, with hV , we can have
the estimate of the diﬀuse-plus-white noise at the reference sensor. To have
the estimate of the desired signal, we use the beamformer:
hX = i −U2α2.
(5.86)
Substituting (5.86) into the deﬁnition of the WNG, we get
W (α2) = (i −U2α2)H d0dH
0 (i −U2α2)
(i −U2α2)H (i −U2α2)
(5.87)
=
1
1 −2ℜ
(
αH
2 UH
2 i
)
+ αH
2 α2
,
whose maximization leads to
α2 = UH
2 i.
(5.88)
As a consequence, the beamformer in (5.86) becomes

80
5 Output SNR in Speech Enhancement/Beamforming
hX = i −U2UH
2 i
= i −
(
IM −u1uH
1
)
i
=
(
uH
1 i
)
u1
= hDS,
(5.89)
which is another way to derive the DS beamformer.
Using again (5.86) but in the deﬁnition of the DF gives
D (α2) = (i −U2α2)H d0dH
0 (i −U2α2)
(i −U2α2)H Γd (i −U2α2)
(5.90)
=
1
1 −2ℜ
(
αH
2 UH
2 Γdi
)
+ αH
2 UH
2 ΓdU2α2
.
From the maximization of the previous expression, we get
α2 =
(
UH
2 ΓdU2
)−1 UH
2 Γdi.
(5.91)
As a result, the beamformer in (5.86) becomes
hX = i −U2
(
UH
2 ΓdU2
)−1 UH
2 Γdi
= Γ−1/2
d
[
IM −Γ1/2
d
U2
(
UH
2 ΓdU2
)−1 UH
2 Γ1/2
d
]
Γ1/2
d
i
= Γ−1/2
d
(
Γ−1/2
d
u1uH
1 Γ−1/2
d
uH
1 Γ−1
d u1
)
Γ1/2
d
i
=
Γ−1
d u1
√
M × uH
1 Γ−1
d u1
= hS,
(5.92)
which is another way to derive the superdirective beamformer, where we have
used the fact that
IM = Γ1/2
d
U2
(
UH
2 ΓdU2
)−1 UH
2 Γ1/2
d
+ Γ−1/2
d
u1uH
1 Γ−1/2
d
uH
1 Γ−1
d u1
.
(5.93)
Now, if we want to compromise between supergains and white noise am-
pliﬁcation, we propose to maximize the DF subject to a constraint on the
WNG, the same way it was done in [6]. This is equivalent to minimizing
1/D (α2) with a constraint on 1/W (α2), i.e., minimizing
1
D (α2) + ϵ
1
W (α2) = 1 −2ℜ
(
αH
2 UH
2 Γdi
)
+ αH
2 UH
2 ΓdU2α2
+ ϵ
[
1 −2ℜ
(
αH
2 UH
2 i
)
+ αH
2 α2
]
,
(5.94)

References
81
where ϵ ≥0 is a Lagrange multiplier. We easily ﬁnd that
α2 =
(
UH
2 Γd,ϵU2
)−1 UH
2 Γd,ϵi,
(5.95)
where
Γd,ϵ = Γd + ϵIM.
(5.96)
Therefore, the robust superdirective beamformer is
hR,ϵ = i −U2
(
UH
2 Γd,ϵU2
)−1 UH
2 Γd,ϵi.
(5.97)
It is clear that hR,0 = hS and hR,∞= hDS.
References
1. J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin, Ger-
many: Springer-Verlag, 2008.
2. B. D. Van Veen and K. M. Buckley, “Beamforming: a versatile approach to spatial
ﬁltering,” IEEE Acoust., Speech, Signal Process. Mag., vol. 5, pp. 4-24, Apr. 1988.
3. J. N. Franklin, Matrix Theory. Englewood Cliﬀs, NJ: Prentice-Hall, 1968.
4. J. Benesty, I. Cohen, and J. Chen, Fundamentals of Signal Enhancement and Array
Signal Processing. Singapore: Wiley-IEEE, 2018.
5. H. Cox, R. M. Zeskind, and T. Kooij, “Practical supergain,” IEEE Trans. Acoust.,
Speech, Signal Process., vol. ASSP-34, pp. 393–398, June 1986.
6. H. Cox, R. M. Zeskind, and M. M. Owen, “Robust adaptive beamforming,” IEEE
Trans. Acoust., Speech, Signal Process., vol. ASSP-35, pp. 1365–1376, Oct. 1987.

Chapter 6
Speech Enhancement from the
Fullband Output SNR Perspective
Most of the speech enhancement algorithms are implemented in the time-
frequency domain, i.e., the short-time Fourier transform (STFT) domain.
The two main advantages of the STFT are that the algorithms can be im-
plemented very eﬃciently and the diﬀerent frequency bins can apparently be
manipulated in a very ﬂexible way in order to better compromise between
noise reduction and speech distortion. Therefore, it is important to under-
stand how things work from the fullband output SNR perspective and how
gains/ﬁlters for noise reduction can be improved by fully exploiting all facets
of this fundamental measure. This is the objective of this chapter, where two
cases of the single-channel problem are discussed as well as the multichannel
scenario.
6.1 Signal Model and Problem Formulation
The work developed in this chapter is an important generalization and ex-
tension of some of the ideas presented in [1].
Let us take the single-channel speech enhancement problem in the time
domain of Section 4.1 (Chapter 4), i.e.,
y(t) = x(t) + v(t),
(6.1)
where y(t), x(t), and v(t) are the microphone, desired, and noise signals,
respectively. Using the short-time Fourier transform (STFT), (6.1) can be
rewritten in the time-frequency domain as [2]
Y (k, n) = X(k, n) + V (k, n),
(6.2)
where the zero-mean complex random variables Y (k, n), X(k, n), and V (k, n)
are the STFTs of y(t), x(t), and v(t), respectively, at the frequency bin k ∈
{0, 1, . . . , K −1} and the time frame n. In order to simplify the notation, we
83
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4_6

84
6 Fullband Output SNR Perspective
drop the dependence on the time frame; therefore, (6.2) for example is written
as Y (k) = X(k) + V (k). Since x(t) and v(t) are uncorrelated by assumption,
the variance of Y (k) is
ϕY (k) = E
[
|Y (k)|2]
(6.3)
= ϕX(k) + ϕV (k),
where ϕX(k) = E
[
|X(k)|2]
and ϕV (k) = E
[
|V (k)|2]
are the variances of
X(k) and V (k), respectively. From (6.3), we can deﬁne the subband input
SNR:
iSNR(k) = ϕX(k)
ϕV (k)
(6.4)
and the fullband input:
iSNR =
∑K−1
k=0 ϕX(k)
∑K−1
k=0 ϕV (k)
.
(6.5)
It can be seen that
min
k iSNR(k) ≤iSNR ≤max
k
iSNR(k).
(6.6)
In words, the fullband input SNR can never exceed the maximum subband
input SNR and can never go below the minimum subband input SNR.
Then, our objective is the estimation of the desired signal, X(k), from the
observed signal, Y (k), in the best possible (or ﬂexible) way from the fullband
output SNR that will be deﬁned in the next section.
6.2 Speech Enhancement with Gains
The simplest and most eﬀective way to perform speech enhancement in the
STFT domain is by applying a complex gain, H(k), to the observed signal,
Y (k), i.e.,
Z(k) = H(k)Y (k)
(6.7)
= Xfd(k) + Vfn(k),
where Z(k) is either the estimate of X(k) or V (k), Xfd(k) = H(k)X(k) is the
ﬁltered desired signal, and Vfn(k) = H(k)V (k) is the ﬁltered noise. If Z(k) is
the estimate of V (k), then the estimate of X(k) is b
X(k) = Y (k) −Z(k). The
variance of Z(k) is then

6.2 Speech Enhancement with Gains
85
ϕZ(k) = |H(k)|2 ϕY (k)
(6.8)
= ϕXfd(k) + ϕVfn(k),
where ϕXfd(k) = |H(k)|2 ϕX(k) and ϕVfn(k) = |H(k)|2 ϕV (k) are the vari-
ances of Xfd(k) and Vfn(k), respectively.
It is clear that the subband input and output SNRs are equal, i.e.,
oSNR [H(k)] = ϕXfd(k)
ϕVfn(k)
(6.9)
= iSNR(k).
However, the fullband output SNR is
oSNR [H(:)] =
∑K−1
k=0 ϕXfd(k)
∑K−1
k=0 ϕVfn(k)
.
(6.10)
Therefore, our aim is to ﬁnd the K subband gains, H(k), k = 0, 1, . . . , K −1,
in such a way that the fullband output SNR is greater than the fullband
input SNR, i.e., oSNR [H(:)] > iSNR.
For convenience, we propose to use the index ki, i = 0, 1, . . . , K −1 and
ki ∈{0, 1, . . . , K −1}, which allows us to order the K subband input SNRs
from the largest to the smallest, i.e.,
iSNR(k0) ≥iSNR(k1) ≥· · · ≥iSNR(kK−1).
(6.11)
We can also express the fullband output SNR as
oSNR (h) = hHDXh
hHDV h
(6.12)
=
∑K−1
i=0 |H(ki)|2 ϕX(ki)
∑K−1
i=0 |H(ki)|2 ϕV (ki)
,
where
h =
[ H(k0) H(k1) · · · H(kK−1)]T
(6.13)
is a ﬁlter of length K containing all the subband gains and
DX = diag [ϕX(k0), ϕX(k1), . . . , ϕX(kK−1)]
(6.14)
DV = diag [ϕV (k0), ϕV (k1), . . . , ϕV (kK−1)]
(6.15)
are two diagonal matrices. It is assumed that ϕV (ki) ̸= 0, ∀ki ∈{0, 1, . . . , K−
1}. Let
λ(ki) = iSNR(ki), i = 0, 1, . . . , K −1.
(6.16)

86
6 Fullband Output SNR Perspective
It is worth noticing that
D−1
V DX = diag [λ(k0), λ(k1), . . . , λ(kK−1)]
(6.17)
is also a diagonal matrix containing all the K subband input SNRs ordered
from the largest to the smallest.
Now, we give two important properties.
Property 6.1. Let λ(k0) ≥λ(k1) ≥· · · ≥λ(kK−1) ≥0. We have
∑K−1
i=0 |αi|2 λ(ki)
∑K−1
i=0 |αi|2
≤
∑K−2
i=0 |αi|2 λ(ki)
∑K−2
i=0 |αi|2
≤· · ·
· · · ≤
∑1
i=0 |αi|2 λ(ki)
∑1
i=0 |αi|2
≤λ(k0)
(6.18)
or, equivalently,
∑K−1
i=0 |αi|2 ϕX(ki)
∑K−1
i=0 |αi|2 ϕV (ki)
≤
∑K−2
i=0 |αi|2 ϕX(ki)
∑K−2
i=0 |αi|2 ϕV (ki)
≤· · ·
· · · ≤
∑1
i=0 |αi|2 ϕX(ki)
∑1
i=0 |αi|2 ϕV (ki)
≤ϕX(k0)
ϕV (k0) ,
(6.19)
where αi, i = 0, 1, . . . , K −1 are arbitrary complex numbers with at least
one of them diﬀerent from 0.
Proof. The previous inequalities can be easily shown by induction.
Property 6.2. Let λ(k0) ≥λ(k1) ≥· · · ≥λ(kK−1) ≥0. We have
λ(kK−1) ≤
∑1
i=0 |βK−1−i|2 λ(kK−1−i)
∑1
i=0 |βK−1−i|2
≤· · ·
· · · ≤
∑K−2
i=0 |βK−1−i|2 λ(kK−1−i)
∑K−2
i=0 |βK−1−i|2
≤
∑K−1
i=0 |βK−1−i|2 λ(kK−1−i)
∑K−1
i=0 |βK−1−i|2
(6.20)
or, equivalently,
ϕX(kK−1)
ϕV (kK−1) ≤
∑1
i=0 |βK−1−i|2 ϕX(kK−1−i)
∑1
i=0 |βK−1−i|2 ϕV (kK−1−i)
≤· · ·
· · · ≤
∑K−2
i=0 |βK−1−i|2 ϕX(kK−1−i)
∑K−2
i=0 |βK−1−i|2 ϕV (kK−1−i)
≤
∑K−1
i=0 |βK−1−i|2 ϕX(kK−1−i)
∑K−1
i=0 |βK−1−i|2 ϕV (kK−1−i)
,
(6.21)
where βK−1−i, i = 0, 1, . . . , K −1 are arbitrary complex numbers with at
least one of them diﬀerent from 0.

6.3 Determination of the Optimal Gains
87
Proof. The previous inequalities can be easily shown by induction.
It follows from the previous properties that1
iSNR(kK−1) ≤oSNR (h) ≤iSNR(k0), ∀h,
(6.22)
as well as the inequalities in (6.6). Clearly, both the fullband input and output
SNRs can never exceed the maximum subband input SNR.
6.3 Determination of the Optimal Gains
There are two approaches to ﬁnd the optimal gains from the fullband output
SNR in order to perform speech enhancement. The ﬁrst one considers the
largest subband input SNRs. In this case, we get the estimate of the desired
signal directly. The second method considers the smallest subband input
SNRs. As a result, we get the estimate of the noise signal from which we
easily deduce the estimate of the desired signal.
6.3.1 Maximization of the Fullband Output SNR
The ﬁlter, h, that maximizes the fullband output SNR given in (6.12) is sim-
ply the eigenvector corresponding to the maximum eigenvalue of the matrix
D−1
V DX. Since this matrix is diagonal, its maximum eigenvalue is its largest
diagonal element, i.e., λ(k0). As a consequence, the maximum SNR ﬁlter is
hmax = α(k0)i1,
(6.23)
where α(k0) ̸= 0 is an arbitrary complex number and i1 is the ﬁrst column
of the K × K identity matrix, IK. Equivalently, we can write (6.23) as
{Hmax(k0) = α(k0)
Hmax(ki) = 0, i = 1, 2, . . . , K −1 .
(6.24)
With (6.23), we get the maximum possible fullband output SNR, which is
oSNR (hmax) = λ(k0) = max
k
iSNR(k) ≥iSNR.
(6.25)
As a result,
oSNR (hmax) ≥oSNR (h) , ∀h.
(6.26)
1 This is also a consequence of the deﬁnition of the fullband output SNR in (6.12), whose
form is the generalized Rayleigh quotient.

88
6 Fullband Output SNR Perspective
We deduce that the estimate of the desired signal is
{
b
Xmax(k0) = Hmax(k0)Y (k0)
b
Xmax(ki) = 0, i = 1, 2, . . . , K −1 .
(6.27)
Now, we need to determine α(k0). There are at least two ways to ﬁnd this
parameter. The ﬁrst one is from the MSE between X(k0) and b
Xmax(k0), i.e.,
J [α(k0)] = E
[
|X(k0) −α(k0)Y (k0)|2]
.
(6.28)
The second possibility is to use the distortion-based MSE, i.e.,
Jd [α(k0)] = E
[
|X(k0) −α(k0)X(k0)|2]
.
(6.29)
The minimization of J [α(k0)] leads to the Wiener gain at the frequency
bin k0, i.e.,
αW(k0) =
iSNR(k0)
1 + iSNR(k0),
(6.30)
while the minimization of Jd [α(k0)] gives the unitary gain at the frequency
bin k0, i.e.,
αU(k0) = 1.
(6.31)
Even though this method maximizes the fullband output SNR, it is ex-
pected to introduce a huge amount of distortion to the desired signal, since
all its frequency bins are put to 0 except at k0. A much better approach when
we deal with broadband signals such as speech is to form the ﬁlter from a
linear combination of the eigenvectors corresponding to the P(≤K) largest
eigenvalues of D−1
V DX, i.e.,
hP =
P −1
∑
p=0
α(kp)ip+1,
(6.32)
where α(kp), p = 0, 1, . . . , P −1 are arbitrary complex numbers with at least
one of them diﬀerent from 0 and ip+1 is the (p + 1)th column of IK. We can
also express (6.32) as
{ HP (kp) = α(kp), p = 0, 1, . . . , P −1
HP (ki) = 0, i = P, P + 1, . . . , K −1 .
(6.33)
Hence, the estimate of the desired signal is

6.3 Determination of the Optimal Gains
89
{
b
XP (kp) = HP (kp)Y (kp), p = 0, 1, . . . , P −1
b
XP (ki) = 0, i = P, P + 1, . . . , K −1
.
(6.34)
To ﬁnd the α(kp)’s, we can either optimize J [α(kp)] or Jd [α(kp)]. The ﬁrst
one leads to the Wiener gains at the frequency bins kp, p = 0, 1, . . . , P −1,
i.e.,
αW(kp) =
iSNR(kp)
1 + iSNR(kp),
(6.35)
while the second one gives the unitary gains at the frequency bins kp, p =
0, 1, . . . , P −1, i.e.,
αU(kp) = 1.
(6.36)
The ﬁlters (of length K) corresponding to (6.35) and (6.36) are, respectively,
hP,W =
[ αW(k0) · · · αW(kP −1) 0 · · · 0 ]T
(6.37)
and
hP,U =
[1 · · · 1 0 · · · 0]T .
(6.38)
For P = K, hK,W corresponds to the classical Wiener approach [2] and
hK,U is the identity ﬁlter, which does not aﬀect the observations. Clearly,
hP,U corresponds to the ideal binary mask [3], since the subband observation
signals with the P largest subband input SNRs are not aﬀected while the
K −P others with the smallest subband input SNRs are put to 0. We should
always have
oSNR
(
hP,U
)
≤oSNR
(
hP,W
)
.
(6.39)
From Property 6.1, we deduce that
iSNR ≤oSNR
(
hK,W
)
≤oSNR
(
hK−1,W
)
≤· · · ≤oSNR
(
h1,W
)
= λ(k0)
(6.40)
and
iSNR = oSNR
(
hK,U
)
≤oSNR
(
hK−1,U
)
≤· · · ≤oSNR
(
h1,U
)
= λ(k0).
(6.41)

90
6 Fullband Output SNR Perspective
6.3.2 Minimization of the Fullband Output SNR
It is clear that the ﬁlter denoted hV that minimizes the fullband output SNR
given in (6.12) is the eigenvector corresponding to the minimum eigenvalue
of the matrix D−1
V DX, which is λ(kK−1). Therefore, the minimum SNR ﬁlter
is
hV = β(kK−1)iK,
(6.42)
where β(kK−1) ̸= 0 is an arbitrary complex number and iK is the Kth column
of IK. Equivalently, we can write (6.42) as
{ HV (ki) = 0, i = 0, 1, . . . , K −2
HV (kK−1) = β(kK−1)
.
(6.43)
With (6.42), we get the minimum possible fullband output SNR, which is
oSNR (hV ) = λ(kK−1) = min
k iSNR(k) ≤iSNR.
(6.44)
As a result,
oSNR (hV ) ≤oSNR (h) , ∀h.
(6.45)
We deduce that the estimates of the noise and desired signals are, respectively,
{
bV (ki) = 0, i = 0, 1, . . . , K −2
bV (kK−1) = HV (kK−1)Y (kK−1)
(6.46)
and
{
b
X(ki) = Y (ki), i = 0, 1, . . . , K −2
b
X(kK−1) = HX(kK−1)Y (kK−1)
,
(6.47)
where
HX(kK−1) = 1 −HV (kK−1)
(6.48)
is the equivalent gain for the estimation of X(kK−1).
The MSE between X(kK−1) and b
XβK−1(kK−1) is
J [β(kK−1)] = E
[
|V (kK−1) −β(kK−1)Y (kK−1)|2]
(6.49)
= |β(kK−1)|2 ϕX(kK−1) + |1 −β(kK−1)|2 ϕV (kK−1)
= Jd [β(kK−1)] + Jr [β(kK−1)] .

6.3 Determination of the Optimal Gains
91
From the previous expression, we see that there are at least two ways to ﬁnd
β(kK−1). The minimization of J [β(kK−1)] leads to
βW(kK−1) =
1
1 + iSNR(kK−1),
(6.50)
which is the Wiener gain at the frequency bin kK−1 for the estimation of
V (kK−1) or, equivalently,
αW(kK−1) = 1 −βW(kK−1)
(6.51)
=
iSNR(kK−1)
1 + iSNR(kK−1),
which is the Wiener gain at the frequency bin kK−1 for the estimation of
X(kK−1). The minimization of the power of the residual noise, Jr [β(kK−1)],
gives
βU(kK−1) = 1,
(6.52)
which is the unitary gain at the frequency bin kK−1 for the estimation of
V (kK−1) or, equivalently,
αN(kK−1) = 1 −βU(kK−1)
(6.53)
= 0,
which is the null gain at the frequency bin kK−1 for the estimation of
X(kK−1).
Obviously, the approach presented above is not meaningful for broadband
signals, since only one frequency bin is processed while all the others are not
aﬀected at all. This is far to be enough as far as noise reduction is concerned,
even though very little distortion is expected. A more practical approach is
to form the ﬁlter from a linear combination of the eigenvectors corresponding
to the Q(≤K) smallest eigenvalues of D−1
V DX, i.e.,
hV,Q =
Q−1
∑
q=0
β(kK−Q+q)iK−Q+q+1,
(6.54)
where β(kK−Q+q), q = 0, 1, . . . , Q −1 are arbitrary complex numbers with
at least one of them diﬀerent from 0 and iK−Q+q+1 is the (K −Q + q + 1)th
column of IK. Therefore, the equivalent ﬁlter for the estimation of the desired
signal at the diﬀerent frequency bins is
hX,Q = 1 −hV,Q,
(6.55)
where 1 is a vector of length K with all its elements equal to 1. We can also
express (6.55) as

92
6 Fullband Output SNR Perspective
{ HX,Q(ki) = 1, i = 0, 1, . . . , K −Q −1
HX,Q(kK−Q+q) = 1 −β(kK−Q+q), q = 0, 1, . . . , Q −1 .
(6.56)
Hence, the estimate of the desired signal is
{
b
X(ki) = Y (ki), i = 0, 1, . . . , K −Q −1
b
X(kK−Q+q) = HX,Q(kK−Q+q)Y (kK−Q+q), q = 0, 1, . . . , Q −1 .
(6.57)
Following the same steps as above, we deduce the two ﬁlters of interest:
hX,Q,W =
[1 · · · 1 αW(kK−Q) · · · αW(kK−1)]T
(6.58)
and
hX,Q,N =
[1 · · · 1 0 · · · 0]T .
(6.59)
For Q = K, hX,K,W = hK,W corresponds to the classical Wiener approach
and hX,K,N = 0 is the null ﬁlter, which completely cancels the observations.
The ﬁlter hX,Q,W can be seen as a combination of the ideal binary mask
and Wiener, where the observations with large subband input SNRs are not
aﬀected while the ones with small subband input SNRs are processed with
the Wiener gains. The ﬁlter hX,Q,N is, obviously, the ideal binary mask. We
should always have
oSNR
(
hX,Q,N
)
≥oSNR
(
hX,Q,W
)
.
(6.60)
We can also deduce that
oSNR
(
hX,K,W
)
≥oSNR
(
hX,K−1,W
)
≥· · · ≥oSNR
(
hX,1,W
)
≥iSNR
(6.61)
and
oSNR
(
hX,K,N
)
≥oSNR
(
hX,K−1,N
)
≥· · · ≥oSNR
(
hX,1,N
)
≥iSNR.
(6.62)
6.4 Taking the Interframe Correlation Into Account
It is well known that a speech signal at successive time frames in the STFT
domain is highly correlated. Therefore, if we wish to improve the performance
of noise reduction, we need to take this interframe correlation into account.
Let us consider the L ≥1 most recent time frames of Y (k). Then, we can
express (6.2) as

6.4 Taking the Interframe Correlation Into Account
93
y(k) =
[ Y (k, n) Y (k, n −1) · · · Y (k, n −L + 1) ]T
= x(k) + v(k),
(6.63)
where x(k) and v(k) are deﬁned similarly to y(k). The L × L covariance
matrix of y(k) is
Φy(k) = E
[
y(k)yH(k)
]
(6.64)
= Φx(k) + Φv(k),
where Φx(k) and Φv(k) are the covariance matrices of x(k) and v(k), re-
spectively.
The two Hermitian matrices Φx(k) and Φv(k) in (6.64) can be jointly
diagonalized as follows [4]:
AH(k)Φx(k)A(k) = Λ(k),
(6.65)
AH(k)Φv(k)A(k) = IL,
(6.66)
where A(k) is a full-rank square matrix (of size L × L), Λ(k) is a diagonal
matrix whose main elements are real and nonnegative, and IL is the L × L
identity matrix. Furthermore, Λ(k) and A(k) are the eigenvalue and eigen-
vector matrices, respectively, of Φ−1
v (k)Φx(k), i.e.,
Φ−1
v (k)Φx(k)A(k) = A(k)Λ(k).
(6.67)
The eigenvalues of Φ−1
v (k)Φx(k), denoted λl(k), l = 1, 2, . . . , L, are ordered
as λ1(k) ≥λ2(k) ≥· · · ≥λL(k) ≥0 and the corresponding eigenvectors
are denoted a1(k), a2(k), . . . , aL(k). Obviously, the noisy signal covariance
matrix can also be diagonalized as
AH(k)Φy(k)A(k) = Λ(k) + IL.
(6.68)
We will see a bit later that this joint diagonalization is going to be very
useful.
Since the interframe correlation is now taken into account, X(k) is esti-
mated by applying a complex-valued ﬁlter, h(k) of length L, to the observa-
tion signal vector, y(k), i.e.,
Z(k) = hH(k)y(k)
(6.69)
= Xfd(k) + Vrn(k),
where Z(k) is the estimate of X(k)2, Xfd(k) = hH(k)x(k) is the ﬁltered
desired signal, and Vrn(k) = hH(k)v(k) is the residual noise. Obviously, the
case L = 1 corresponds to the conventional single-channel noise reduction
2 In this section, we only focus on the estimation of X(k); the extension of this approach
to the estimation of V (k) is straightforward.

94
6 Fullband Output SNR Perspective
approach in the STFT domain with gains [2]. The variance of Z(k) is then
ϕZ(k) = hH(k)Φy(k)h(k)
(6.70)
= ϕXfd(k) + ϕVrn(k),
where ϕXfd(k) = hH(k)Φx(k)h(k) and ϕVrn(k) = hH(k)Φv(k)h(k) are the
variances of Xfd(k) and Vrn(k), respectively. We deduce from (6.70) that the
subband and fullband output SNRs are, respectively,
oSNR [h(k)] = ϕXfd(k)
ϕVrn(k)
(6.71)
= hH(k)Φx(k)h(k)
hH(k)Φv(k)h(k)
and
oSNR [h(:)] =
∑K−1
k=0 ϕXfd(k)
∑K−1
k=0 ϕVrn(k)
.
(6.72)
As we did in previous sections, we propose to use the index ki, i =
0, 1, . . . , K −1 and ki ∈{0, 1, . . . , K −1}, which allows us to order the K sub-
band eigenvalues λ1(k), k = 0, 1, . . . , K −1 from the largest to the smallest,
i.e.,
λ1(k0) ≥λ1(k1) ≥· · · ≥λ1(kK−1).
(6.73)
So, with this indexing, the subband ﬁlter is denoted as h(ki), which is as-
sumed in the rest to have the form:
h(ki) = ψ(ki)a1(ki),
(6.74)
where ψ(ki) is an arbitrary complex number and a1(ki) is the eigenvector cor-
responding to λ1(ki). For ψ(ki) ̸= 0, it is clear that h(ki) in (6.74) maximizes
the subband output SNR, since
oSNR [h(ki)] = λ1(ki).
(6.75)
As a result, with h(ki) in (6.74), (6.73) is equivalent to saying that
oSNR [h(k0)] ≥oSNR [h(k1)] ≥· · · ≥oSNR [h(kK−1)] .
(6.76)
Also, we have
λ1(ki) ≥iSNR(ki)
(6.77)
and

6.4 Taking the Interframe Correlation Into Account
95
iSNR ≤λ1(k0).
(6.78)
In the particular case of L = 1, we have λ1(ki) = λ(ki) = iSNR(ki), which
corresponds to the study of previous sections.
Let
h =
[
hT (k0) hT (k1) · · · hT (kK−1)
]T
(6.79)
be a long ﬁlter of length KL containing all the ordered subband ﬁlters. We
can express the fullband output SNR as
oSNR (h) = hHDΦxh
hHDΦvh
,
(6.80)
where
DΦx = diag [Φx(k0), Φx(k1), . . . , Φx(kK−1)]
(6.81)
DΦv = diag [Φv(k0), Φv(k1), . . . , Φv(kK−1)]
(6.82)
are block diagonal matrices. It is worth noticing that
D−1
ΦvDΦxDA = DADΛ,
(6.83)
where
D−1
Φv = diag
[
Φ−1
v (k0), Φ−1
v (k1), . . . , Φ−1
v (kK−1)
]
,
(6.84)
DA = diag [A(k0), A(k1), . . . , A(kK−1)] ,
(6.85)
DΛ = diag [Λ(k0), Λ(k1), . . . , Λ(kK−1)] .
(6.86)
Therefore, our objective is to ﬁnd h in such a way that oSNR (h) > iSNR.
Since
oSNR (h) =
∑K−1
i=0 |ψ(ki)|2 λ1(ki)
∑K−1
i=0 |ψ(ki)|2
,
(6.87)
we deduce that
λ1(kK−1) ≤oSNR (h) ≤λ1(k0).
(6.88)
The ﬁlter, h, that maximizes the fullband output SNR given in (6.80) is
simply the eigenvector corresponding to the maximum eigenvalue, λ1(k0), of
the matrix D−1
ΦvDΦx. As a consequence, the maximum SNR ﬁlter (of length
KL) is
hmax =
[
ψ(k0)aT
1 (k0) 0T ]T ,
(6.89)

96
6 Fullband Output SNR Perspective
where ψ(k0) ̸= 0 . Equivalently, we can write (6.89) as
{ hmax(k0) = ψ(k0)a1(k0)
hmax(ki) = 0, i = 1, 2, . . . , K −1 .
(6.90)
With (6.89), we get the maximum possible fullband output SNR, which is
oSNR (hmax) = λ1(k0) ≥iSNR
(6.91)
and
oSNR (hmax) ≥oSNR (h) , ∀h.
(6.92)
We deduce that the estimate of the desired signal is
{
b
Xmax(k0) = hH
max(k0)y(k0)
b
Xmax(ki) = 0, i = 1, 2, . . . , K −1 .
(6.93)
Now, the parameter ψ(k0) needs to be determined. There are at least two
ways to ﬁnd it. The ﬁrst one is from the MSE between X(k0) and b
Xmax(k0),
i.e.,
J [ψ(k0)] = E
[X(k0) −ψ∗(k0)aH
1 (k0)y(k0)
2]
.
(6.94)
The second possibility is to use the distortion-based MSE, i.e.,
Jd [ψ(k0)] = E
[X(k0) −ψ∗(k0)aH
1 (k0)x(k0)
2]
.
(6.95)
The minimization of J [ψ(k0)] leads to the maximum SNR ﬁlter with min-
imum MSE at the frequency bin k0, i.e.,
hmax,1(k0) = a1(k0)aH
1 (k0)Φx(k0)i1
1 + λ1(k0)
,
(6.96)
while the minimization of Jd [ψ(k0)] gives a minimum distortion ﬁlter at the
frequency bin k0, i.e.,
hmax,2(k0) = a1(k0)aH
1 (k0)Φx(k0)i1
λ1(k0)
,
(6.97)
where i1 is the ﬁrst column of IL.
Clearly, this method maximizes the fullband output SNR but it is expected
to introduce a huge amount of distortion to the desired signal, since all its
frequency bins are put to 0 except at k0. A much better approach when we
deal with broadband signals such as speech is to form the ﬁlter (of length
KL) from a concatenation of the eigenvectors corresponding to the P(≤K)

6.4 Taking the Interframe Correlation Into Account
97
largest eigenvalues from the set {λ1(ki), i = 0, 1, . . . , K −1}, i.e.,
hP =
[
ψ(k0)aT
1 (k0) · · · ψP −1(kP −1)aT
1 (kP −1) 0T ]T ,
(6.98)
where ψ(kp), p = 0, 1, . . . , P −1 are arbitrary complex numbers with at least
one of them diﬀerent from 0. We can also express (6.98) as
{hP (kp) = ψ(kp)a1(kp), p = 0, 1, . . . , P −1
hP (ki) = 0, i = P, P + 1, . . . , K −1
.
(6.99)
Hence, the estimate of the desired signal is
{
b
XP (kp) = hH
P (kp)y(kp), p = 0, 1, . . . , P −1
b
XP (ki) = 0, i = P, P + 1, . . . , K −1
.
(6.100)
To ﬁnd the ψ(kp)’s, we can either optimize J [ψ(kp)] or Jd [ψ(kp)]. The
ﬁrst one leads to ﬁlters with minimum MSE at the frequency bins kp, p =
0, 1, . . . , P −1, i.e.,
hP,1(kp) = a1(kp)aH
1 (kp)Φx(kp)i1
1 + λ1(kp)
,
(6.101)
while the second one gives the minimum distortion ﬁlters at the frequency
bins kp, p = 0, 1, . . . , P −1, i.e.,
hP,2(kp) = a1(kp)aH
1 (kp)Φx(kp)i1
λ1(kp)
.
(6.102)
The ﬁlters (of length KL) corresponding to (6.101) and (6.102) are, respec-
tively,
hP,1 =
[ hT
P,1(k0) · · · hT
P,1(kP −1) 0T ]T
(6.103)
and
hP,2 =
[hT
P,2(k0) · · · hT
P,2(kP −1) 0T ]T .
(6.104)
This approach can be seen as a generalization of the ideal binary mask [3],
since the subband observation signals of the microphone with the P largest
subband output SNRs are processed with ﬁlters with minimum MSE or min-
imum distortion, while the K −P others with the smallest subband output
SNRs are put to 0. We should always have
oSNR
(
hP,2
)
≤oSNR
(
hP,1
)
.
(6.105)
We can deduce that

98
6 Fullband Output SNR Perspective
iSNR ≤oSNR
(
hK,1
)
≤oSNR
(
hK−1,1
)
≤· · · ≤oSNR
(
h1,1
)
= λ1(k0)
(6.106)
and
iSNR ≤oSNR
(
hK,2
)
≤oSNR
(
hK−1,2
)
≤· · · ≤oSNR
(
h1,2
)
= λ1(k0).
(6.107)
6.5 Generalization to the Multichannel Case
We consider the conventional signal model in which a microphone array with
M sensors captures a convolved source signal in some noise ﬁeld. The received
signals, at the time index t, are expressed as [5], [6]
ym(t) = gm(t) ∗x(t) + vm(t)
(6.108)
= xm(t) + vm(t), m = 1, 2, . . . , M,
where gm(t) is the acoustic impulse response from the unknown speech source,
x(t), location to the mth microphone, ∗stands for linear convolution, and
vm(t) is the additive noise at microphone m. We assume that the signals
xm(t) = gm(t) ∗x(t) and vm(t) are uncorrelated, zero mean, stationary, real,
and broadband. By deﬁnition, the convolved speech signals, xm(t), m =
1, 2, . . . , M, are coherent across the array while the noise signals, vm(t), m =
1, 2, . . . , M, are typically only partially coherent across the array. Using the
STFT, (6.108) can be rewritten in the time-frequency domain as
Ym(k, n) = Gm(k)X(k, n) + Vm(k, n)
(6.109)
= Xm(k, n) + Vm(k, n), m = 1, 2, . . . , M,
where Ym(k, n), Gm(k), X(k, n), Vm(k, n), and Xm(k, n) are the STFTs of
ym(t), gm(t), x(t), vm(t), and xm(t), respectively, at the frequency bin k ∈
{0, 1, . . . , K −1} and the time frame n. Assuming that the ﬁrst sensor is the
reference and dropping the dependence on n, we can write the M STFT-
domain microphone signals in a vector notation as
y(k) =
[ Y1(k) Y2(k) · · · YM(k)]T
= d(k)X1(k) + v(k)
= x(k) + v(k),
(6.110)
where
d(k) =
[
1 G2(k)
G1(k) · · · GM(k)
G1(k)
]T
,
(6.111)

6.5 Generalization to the Multichannel Case
99
and x(k) and v(k) are deﬁned similarly to y(k). Since Xm(k) and Vm(k) are
uncorrelated by assumption, we deduce that the M × M covariance matrix
of y(k) is
Φy(k) = E
[
y(k)yH(k)
]
(6.112)
= ϕX1(k)d(k)dH(k) + Φv(k)
= Φx(k) + Φv(k),
where ϕX1(k) = E
[
|X1(k)|2]
is the variance of X1(k), and Φx(k) and Φv(k)
are the covariance matrices of x(k) and v(k), respectively. It results that the
subband and input SNRs are, respectively,
iSNR(k) = ϕX1(k)
ϕV1(k)
(6.113)
and
iSNR =
∑K−1
k=0 ϕX1(k)
∑K−1
k=0 ϕV1(k)
,
(6.114)
where ϕV1(k) = E
[
|V1(k)|2]
is the variance of V1(k), the additive noise at
the ﬁrst (reference) sensor. It is obvious that
min
k iSNR(k) ≤iSNR ≤max
k
iSNR(k).
(6.115)
As before, the two Hermitian matrices Φx(k) and Φv(k) can be jointly
diagonalized as follows [4]:
AH(k)Φx(k)A(k) = Λ(k),
(6.116)
AH(k)Φv(k)A(k) = IM,
(6.117)
where A(k) is a full-rank square matrix (of size M × M), Λ(k) is a diagonal
matrix whose main elements are real and nonnegative, and IM is the M ×M
identity matrix. Furthermore, Λ(k) and A(k) are the eigenvalue and eigenvec-
tor matrices, respectively, of Φ−1
v (k)Φx(k). Since the rank of Φx(k) is equal
to 1, the eigenvalues of Φ−1
v (k)Φx(k) are λ1(k) = ϕX1(k)dH(k)Φ−1
v (k)d(k)
and λ2(k) = λ3(k) = · · · = λM(k) = 0. In other words, the ﬁrst and last
M −1 eigenvalues of the matrix product Φ−1
v (k)Φx(k) are positive and ex-
actly zero, respectively. We also denote a1(k), a2(k), . . . , aM(k), the corre-
sponding eigenvectors, where the ﬁrst one can be expressed as
a1(k) =
Φ−1
v (k)d(k)
√
dH(k)Φ−1
v (k)d(k)
.
(6.118)

100
6 Fullband Output SNR Perspective
Conventional multichannel speech enhancement in the STFT domain is
performed by applying a complex-valued ﬁlter, h(k) of length M, to the
observation signal vector, y(k), i.e.,
Z(k) = hH(k)y(k)
(6.119)
= Xfd(k) + Vrn(k),
where
Z(k)
is
the
estimate
of
X1(k, n),
Xfd(k)
=
hH(k)x(k)
=
X1(k)hH(k)d(k) is the ﬁltered desired signal, and Vrn(k) = hH(k)v(k) is
the residual noise. The variance of Z(k) is then
ϕZ(k) = hH(k)Φy(k)h(k)
(6.120)
= ϕXfd(k) + ϕVrn(k),
where ϕXfd(k) = ϕX1(k)
hH(k)d(k)
2 and ϕVrn(k) = hH(k)Φv(k)h(k) are
the variances of Xfd(k) and Vrn(k), respectively. We deduce from (6.120)
that the subband and fullband output SNRs are, respectively,
oSNR [h(k)] = ϕXfd(k)
ϕVrn(k)
(6.121)
= ϕX1(k)
hH(k)d(k)
2
hH(k)Φv(k)h(k)
and
oSNR [h(:)] =
∑K−1
k=0 ϕXfd(k)
∑K−1
k=0 ϕVrn(k)
.
(6.122)
Again, we propose to use the index ki, i = 0, 1, . . . , K −1 and ki ∈
{0, 1, . . . , K−1} to order the K subband eigenvalues λ1(k), k = 0, 1, . . . , K−1
from the largest to the smallest, i.e.,
λ1(k0) ≥λ1(k1) ≥· · · ≥λ1(kK−1).
(6.123)
So, with this indexing, the subband ﬁlter is denoted as h(ki), which is as-
sumed in the rest of this section to have the form:
h(ki) = ψ(ki)a1(ki),
(6.124)
where ψ(ki) is an arbitrary complex number and a1(ki) is the eigenvector
corresponding to λ1(ki). For ψ(ki) ̸= 0, it is clear that h(ki) maximizes the
subband output SNR, since
oSNR [h(ki)] = λ1(ki)
(6.125)
= ϕX1(ki)
aH
1 (ki)d(ki)
2 .

6.5 Generalization to the Multichannel Case
101
As a result, (6.123) is equivalent to
oSNR [h(k0)] ≥oSNR [h(k1)] ≥· · · ≥oSNR [h(kK−1)] .
(6.126)
Also, we have
λ1(ki) ≥iSNR(ki)
(6.127)
and
iSNR ≤λ1(k0).
(6.128)
Let
h =
[ hT (k0) hT (k1) · · · hT (kK−1)]T
(6.129)
be a long ﬁlter of length KM containing all the ordered subband ﬁlters. We
can express the fullband output in (6.122) as
oSNR (h) = hHDΦxh
hHDΦvh
,
(6.130)
where
DΦx = diag [Φx(k0), Φx(k1), . . . , Φx(kK−1)]
(6.131)
DΦv = diag [Φv(k0), Φv(k1), . . . , Φv(kK−1)]
(6.132)
are block diagonal matrices. It is worth noticing that
D−1
ΦvDΦxDA = DADΛ,
(6.133)
where
D−1
Φv = diag
[
Φ−1
v (k0), Φ−1
v (k1), . . . , Φ−1
v (kK−1)
]
,
(6.134)
DA = diag [A(k0), A(k1), . . . , A(kK−1)] ,
(6.135)
DΛ = diag [Λ(k0), Λ(k1), . . . , Λ(kK−1)] .
(6.136)
Since
oSNR (h) =
∑K−1
i=0 |ψ(ki)|2 λ1(ki)
∑K−1
i=0 |ψ(ki)|2
,
(6.137)
we deduce that
λ1(kK−1) ≤oSNR (h) ≤λ1(k0).
(6.138)
Therefore, our objective is to ﬁnd h in such a way that oSNR (h) > iSNR.

102
6 Fullband Output SNR Perspective
The ﬁlter, h, that maximizes the fullband output SNR given in (6.130)
is the eigenvector corresponding to the maximum eigenvalue, λ1(k0), of the
matrix D−1
ΦvDΦx. As a consequence, the maximum SNR ﬁlter (of length KM)
is
hmax =
[ ψ(k0)aT
1 (k0) 0T ]T ,
(6.139)
where ψ(k0) ̸= 0 is an arbitrary complex number. Equivalently, we can write
(6.139) as
{ hmax(k0) = ψ(k0)a1(k0)
hmax(ki) = 0, i = 1, 2, . . . , K −1 .
(6.140)
With (6.139), we get the maximum possible fullband output SNR, which is
oSNR (hmax) = λ1(k0) ≥iSNR
(6.141)
and
oSNR (hmax) ≥oSNR (h) , ∀h.
(6.142)
We deduce that the estimate of the desired signal is
{
b
X1,max(k0) = hH
max(k0)y(k0)
b
X1,max(ki) = 0, i = 1, 2, . . . , K −1 .
(6.143)
Now, we need to ﬁnd ψ(k0). The ﬁrst possibility is from the MSE between
X1(k0) and b
X1,max(k0), i.e.,
J [ψ(k0)] = E
[X1(k0) −ψ∗(k0)aH
1 (k0)y(k0)
2]
.
(6.144)
The second possibility is to use the distortion-based MSE, i.e.,
Jd [ψ(k0)] = E
[X1(k0) −ψ∗(k0)aH
1 (k0)x(k0)
2]
.
(6.145)
From the minimization of J [ψ(k0)], we get the classical Wiener ﬁlter at
the frequency bin k0, i.e.,
hmax,W(k0) =
√
ϕX1(k0)λ1(k0)
1 + λ1(k0)
a1(k0)
(6.146)
=
ϕX1(k0)Φ−1
v (k0)d(k0)
1 + ϕX1(k0)dH(k0)Φ−1
v (k0)d(k0),
while the minimization of Jd [ψ(k0)] gives the well-known MVDR ﬁlter at the
frequency bin k0, i.e.,

6.5 Generalization to the Multichannel Case
103
hmax,D(k0) =
√
ϕX1(k0)λ1(k0)
λ1(k0)
a1(k0)
(6.147)
=
Φ−1
v (k0)d(k0)
dH(k0)Φ−1
v (k0)d(k0).
Even though this method maximizes the fullband output SNR, it is ex-
pected to introduce a large distortion to the desired signal, since all its fre-
quency bins are put to 0 except at k0. A more practical approach when we
deal with broadband signals such as speech is to form the ﬁlter (of length
KM) from a concatenation of the eigenvectors corresponding to the P(≤K)
largest eigenvalues from the set {λ1(ki), i = 0, 1, . . . , K −1}, i.e.,
hP =
[
ψ(k0)aT
1 (k0) · · · ψ(kP −1)aT
1 (kP −1) 0T ]T ,
(6.148)
where ψ(kp), p = 0, 1, . . . , P −1 are arbitrary complex numbers with at least
one of them diﬀerent from 0. We can also express (6.148) as
{hP (kp) = ψ(kp)a1(kp), p = 0, 1, . . . , P −1
hP (ki) = 0, i = P, P + 1, . . . , K −1
.
(6.149)
Hence, the estimate of the desired signal is
{
b
X1,P (kp) = hH
P (kp)y(kp), p = 0, 1, . . . , P −1
b
X1,P (ki) = 0, i = P, P + 1, . . . , K −1
.
(6.150)
To ﬁnd the ψ(kp)’s, we can either optimize J [ψ(kp)] or Jd [ψ(kp)]. The ﬁrst
one leads to the Wiener ﬁlters at the frequency bins kp, p = 0, 1, . . . , P −1,
i.e.,
hP,W(kp) =
ϕX1(kp)Φ−1
v (kp)d(kp)
1 + ϕX1(kp)dH(kp)Φ−1
v (kp)d(kp),
(6.151)
while the second one gives the MVDR ﬁlters at the frequency bins kp, p =
0, 1, . . . , P −1, i.e.,
hP,D(kp) =
Φ−1
v (kp)d(kp)
dH(kp)Φ−1
v (kp)d(kp).
(6.152)
The ﬁlters (of length KM) corresponding to (6.151) and (6.152) are, respec-
tively,
hP,W =
[hT
P,W(k0) · · · hT
P,W(kP −1) 0T ]T
(6.153)
and
hP,D =
[ hT
P,D(k0) · · · hT
P,D(kP −1) 0T ]T .
(6.154)

104
6 Fullband Output SNR Perspective
For P = K, hK,W and hK,D correspond to the classical multichannel Wiener
and MVDR approaches, respectively. The case hP,D can be seen as a gen-
eralization of the ideal binary mask [3] to the multichannel case, since the
subband observation signals of the reference microphone with the P largest
subband output SNRs are processed in such a way that the desired signals
are undistorted while the K −P others with the smallest subband output
SNRs are put to 0. We should always have
oSNR
(
hP,D
)
≤oSNR
(
hP,W
)
.
(6.155)
We also deduce that
iSNR ≤oSNR
(
hK,W
)
≤oSNR
(
hK−1,W
)
≤· · · ≤oSNR
(
h1,W
)
= λ1(k0)
(6.156)
and
iSNR ≤oSNR
(
hK,D
)
≤oSNR
(
hK−1,D
)
≤· · · ≤oSNR
(
h1,D
)
= λ1(k0).
(6.157)
References
1. Y. Zhao, J. Benesty, and J. Chen, “Single-channel noise reduction in the STFT domain
from the fullband output SNR perspective,” in Proc. EUSIPCO, 2016, pp. 1956–1959.
2. J. Benesty, J. Chen, and E. Habets, Speech Enhancement in the STFT Domain.
Springer Briefs in Electrical and Computer Engineering, 2011.
3. D. Wang, “On ideal binary mask as the computational goal of auditory scene analysis,”
in Speech Separation by Humans and Machines, Pierre Divenyi, Ed., pp. 181–197,
Kluwer, 2005.
4. J. N. Franklin, Matrix Theory. Englewood Cliﬀs, NJ: Prentice-Hall, 1968.
5. J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin, Ger-
many: Springer-Verlag, 2008.
6. M. Brandstein and D. B. Ward, Eds., Microphone Arrays: Signal Processing Tech-
niques and Applications. Berlin, Germany: Springer-Verlag, 2001.

Index
acoustic impulse response, 34
basis, 69
beamforming, 65, 77
best estimator
frequency domain, 5
frequency domain, multichannel, 17
frequency domain, single channel, 7
time domain, 23
time domain, binaural, 37
time domain, single channel, 25
best linear estimator
frequency domain, multichannel, 19
frequency domain, single channel, 9
time domain, binaural, 42
time domain, single channel, 31
best quadratic estimator
frequency domain, single channel, 15
best widely linear estimator, 42
beta function, 14
binaural speech enhancement, 34
circularity quotient, 35
coeﬃcient of determination
frequency domain, 7
frequency domain, multichannel, 17
frequency domain, single channel, 8
time domain, binaural, 37
time domain, single channel, 25
coherence function, 35
complex random variable, 35
compromising ﬁlter, 76, 77
condition number, 70
conditional correlation coeﬃcient (CCC),
30
conditional distribution, 13
conditional expectation, 6, 25
correlation coeﬃcient, 46
delay-and-sum (DS), 78
directivity factor (DF), 78
distortion-based MSE, 51
distortionless ﬁlter, 75
Eve’s law, 6
ﬁxed beamformer
DS, 78, 80
robust superdirective, 81
superdirective, 78, 80
fullmode input SNR, 69
frequency domain, multichannel, 19
gain in SNR
time domain, single channel, 29
gamma distribution, 12
generalized Rayleigh quotient, 49, 67, 87
Gram-Schmidt orthonormalization, 79
ideal binary mask, 89, 92, 97, 104
input SNR, 66
frequency domain, 6
fullband, multichannel, 99
fullband, single channel, 84
subband, multichannel, 99
subband, single channel, 84
time domain, 24, 46
time domain, binaural, 36
interframe correlation, 92
joint diagonalization, 67, 93, 99
Lagrange multiplier, 81
law of iterated expectations, 6
105
© The Author(s), under exclusive licence to Springer International Publishing AG,
part of Springer Nature 2018
J. Benesty, Fundamentals of Speech Enhancement, SpringerBriefs in Electrical
and Computer Engineering, https://doi.org/10.1007/978-3-319-74524-4

106
Index
law of total expectation, 6
law of total variance, 6
linear ﬁltering, 46, 66
magnitude squared coherence function, 7
magnitude squared Pearson correlation
coeﬃcient (MSPCC), 36
maximum SNR ﬁlter, 71, 73
STFT domain, multichannel, 102
STFT domain, single channel, 87, 95
maximum SNR ﬁlter with minimum
distortion, 74
mean-squared error (MSE), 50
minimum distortion ﬁlter
STFT domain, single channel, 96, 97
time domain, single channel, 51
minimum distortion-type ﬁlter
time domain, single channel, 58, 60
minimum mean-squared error (MMSE), 7
minimum MSE ﬁlter
STFT domain, single channel, 96, 97
minimum noise ﬁlter
time domain, single channel, 53
minimum noise-type ﬁlter
time domain, single channel, 56, 62
minimum SNR ﬁlter, 71
STFT domain, single channel, 90
minimum variance distortionless response
(MVDR), 51
MVDR ﬁlter, 72, 73
STFT domain, multichannel, 102, 103
time domain, single channel, 51, 53
noise reduction factor
frequency domain, single channel, 14
time domain, binaural, 43
time domain, single channel, 30, 33
noise reduction ﬁlter, 45
noncircular, 35
null constraint ﬁlter
time domain, single channel, 56, 58
null gain
STFT domain, 91
optimal ﬁlter, 49, 72
optimal gain, 87
output SNR, 67
fullband, multichannel, 100
fullband, single channel, 85, 94
subband, multichannel, 100
subband, single channel, 85, 94
time domain, single channel, 29, 33, 50
partial correlation coeﬃcient (PCC), 33
positive deﬁnite matrix, 70
positive semideﬁnite matrix, 70
short-time Fourier transform (STFT), 83
signal-to-noise ratio (SNR), 6
SPCC, 46
spectral mode input SNR, 69
speech distortion index
frequency domain, single channel, 14
time domain, single channel, 26
speech enhancement, 1, 5, 23, 45, 65, 83
speech reduction factor
time domain, binaural, 42
time domain, single channel, 30, 31
squared Pearson correlation coeﬃcient
(SPCC), 24
steering vector, 2, 17, 66
ULA, 77
superdirective beamforming, 77
supergain, 79
time-frequency domain, 83
uniform linear array (ULA), 77
unitary gain
STFT domain, 88, 89, 91
white noise ampliﬁcation, 79
white noise gain (WNG), 78
widely linear Wiener ﬁlter, 42
Wiener ﬁlter
frequency domain, multichannel, 19
STFT domain, multichannel, 102, 103
time domain, single channel, 31, 32, 50,
56
Wiener gain, 9
STFT domain, 88, 89, 91
Wiener-type ﬁlter
time domain, single channel, 60, 62, 63


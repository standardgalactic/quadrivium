Statistics for 
Economics
Shahdad Naghshpour
The Economics Collection
Philip J. Romero and Jeffrey A. Edwards, Editors
www.businessexpertpress.com

Statistics for Economics


Statistics for Economics
Shahdad Naghshpour

Statistics for Economics
Copyright © Business Expert Press, 2012.
All rights reserved. No part of this publication may be reproduced, 
stored in a retrieval system, or transmitted in any form or by any 
means—electronic, mechanical, photocopy, recording, or any other 
except for brief quotations, not to exceed 400 words, without the prior 
permission of the publisher.
First published in 2012 by
Business Expert Press, LLC
222 East 46th Street, New York, NY 10017
www.businessexpertpress.com
ISBN-13: 978-1-60649-403-5 (paperback)
ISBN-13: 978-1-60649-404-2 (e-book)
DOI 10.4128/9781606494042
Business Expert Press Economics collection
Collection ISSN: 2163-761X (print)
Collection ISSN: 2163-7628 (electronic)
Cover design by Jonathan Pennell
Interior design by Exeter Premedia Services Private Ltd., 
Chennai, India
First edition: 2012
10 9 8 7 6 5 4 3 2 1
Printed in the United States of America.

To Donna
SN


Abstract
Statistics is the branch of mathematics that deals with real-life problems. 
As such, it is an essential tool for economists. Unfortunately, the way 
the concept of statistics is introduced to students is not compatible with 
the way economists think and learn. Th e problem is worsened by the use 
of mathematical jargon and complex  derivations. However, as this book 
demonstrates, neither is necessary. Th is book is written in simple English 
with minimal use of symbols, mostly for the sake of brevity and to make 
reading literature more meaningful.
All the examples and exercises in this book are constructed within the 
fi eld of economics, thus eliminating the diffi  culty of learning statistics 
with examples from fi elds that have no relation to business, politics, or 
policy. Statistics is, in fact, not more diffi  cult than economics.  Anyone 
who can comprehend economics can understand and use  statistics 
 successfully within this fi eld.
In my opinion, the most important aspect of statistics is its ability 
to summarize the information embedded in numerous data into few 
 parameters and to capture the essence of data. Th e ability to capture the 
inherent core meaning of data from seemingly random and varying bits 
of information is unique to statistics. It seems that somehow statistics is 
able to fi nd order in chaos.
Th is book utilizes Microsoft Excel to obtain statistical results, as well 
as to perform additional necessary computations. Microsoft Excel is not 
the software of choice for performing sophisticated statistical analysis. 
However, it is widely available, and almost everyone has some degree of 
familiarity with it. Using Excel will eliminate the need for students and 
readers to buy and learn new software, the need that itself would prove to 
be another impediment to learning and using statistics.

Keywords
null and alternative hypotheses, standardization, normal distribution func-
tion, statistical inference, test statistics, t distribution  function, F  distribution 
function, parameter, mean, standard deviation,  interpretation and analysis, 
coeffi  cient of determination, degrees of freedom, sampling distribution of 
sample statistics, standard error, unbiased, consistent,  effi  cient, central limit 
theorem, margin of error, individual error, average error, mean squared 
error, analysis of variance (ANOVA)

Contents
Statistics Is the Science of Finding Order in Chaos ...................................xi
Introduction ........................................................................................xiii
Chapter 1 Descriptive Statistics ..........................................................1
Chapter 2  Numerical Descriptive Statistics 
for Quantitative Variables ................................................29
Chapter 3  Some Applications of Descriptive Statistics ......................61
Chapter 4  Distribution Functions ....................................................81
Chapter 5  Sampling Distribution of Sample Statistics ......................99
Chapter 6  Point and Interval Estimation ........................................119
Chapter 7  Statistical Inference with Test of Hypothesis ..................141
Chapter 8  An Introduction to Regression Analysis .........................163
Chapter 9  Conclusion ....................................................................175
Glossary .............................................................................................179
Notes..................................................................................................185
References ...........................................................................................187
Index .................................................................................................189


Statistics Is the Science of 
Finding Order in Chaos
I wrote this manuscript to share my aff ection for statistics and to show 
that comprehending statistics does not require mastery of mathematical 
jargon or complex formulations and derivation. I do not claim that upon 
learning the material in this book you will be considered a statistician or 
can start a career in statistics; however, I promise you will have a much 
better understanding of the subject and will be able to apply its methods 
in the areas to which they apply. I also hope you will gain the wisdom of 
knowing where the things you have learned will not work and realize that 
you have to learn new material to handle such cases.
Statistics is the science of life. It does not live outside of real life. 
Conclusions in statistics are probabilistic in nature as compared to deter-
ministic in most branches of mathematics. Every aspect of life benefi ts 
from statistics.
Learning statistics is like learning to play an instrument or learning a 
foreign language. Simply reading and comprehending the material is not 
suffi  cient; you also need to practice, and memorizing the  material is also 
important. It is not suffi  cient to know the material or where to fi nd it. 
Th e same is true about learning foreign languages. Unless you would like 
to walk around with a dictionary or a statistics book under your arm, you 
must know the material by heart.
I am indebted to my wife Donna who has helped me in more ways 
than imaginable. I do not think I can thank her enough. I would like to 
thank Michael Webb and Candice Bright for their relentless assistance 
in all aspects of this book. He has been my most reliable source and I 
could always count on him. I also want to thank my graduate assistants 
Issam Abu-Ghallous and Brian Carriere. Th ey have provided many hours 
of help with all aspects of the process. Without the help of Mike, Issam, 
and Brian, this book would not have been completed. I also would like 
to thank Madeline Gillette, Anthony Calandrillo, and Matt Orzechowski 
who read parts of the manuscript. 


Introduction
Economics is a very interesting subject. Th e scope of the economic 
domain is vast. Economics deals with market structure, consumer 
behavior, investment, growth, fi scal policy, monetary policy, the roles of 
the bank, and so forth. Th e list can go on for quite some time. It also 
predicts how economic agents behave in response to changes in economic 
and noneconomic factors such as price, income, political party, stability, 
and so on. Economic theory, however, is not specifi c. For example, the 
theory proves that when the price of a good increases, the quantity sup-
plied increases, provided all the other pertinent factors remain constant, 
which is also known as ceteris paribus. What the theory does not and 
cannot state is how much the quantity increases for a given increase in 
price. Th e answer to this question seems to be more interesting to most 
people than the fact that quantity will increase as a result of an increase 
in price. Th e truth is that the theory that explains the above relationship 
is important for economists. For the rest of the population, knowledge 
of that relationship is worthless if the magnitude is unknown. Assume 
for a 10% increase in price, the quantity increases by 1%. Th is has many 
diff erent consequences if the quantity increases by 10%, and totally dif-
ferent consequences if the quantity increases by 20%. Th e knowledge of 
the magnitude of change is as important, if not more important, than 
the knowledge of the direction of change. In other words, predictions are 
valuable when they are specifi c. 
Statistics is the science that can answer specifi c issues raised above. 
Th e science of statistics provides necessary theories that can provide 
the foundation for answering such specifi c questions. Statistics theory 
indicates the necessary conditions to set up the study and collect data. It 
provides the means to analyze and clarify the meaning of the fi ndings. 
It also provides the foundation to explain the meaning of the fi ndings 
using statistical inference. 
In order to make an economic decision, it is necessary to know 
the economic conditions. Th is is true for all economic agents, from 
the smallest to the largest. Th e smallest economic agent might be an 

xiv 
INTRODUCTION
individual with little earning and disposable income, while the largest can 
be a multinational corporation with thousands of employees, or govern-
ment. Briefl y, we will discuss some of the main needs and uses of statis-
tics in economics, and then present some uses of regression analysis in 
economics.
Th e fi rst step in making any economic decision is to gain knowledge 
of the state of the economy. Economic condition is always in a state of 
fl ux. Sometimes it seems that we are not very concerned with mundane 
economic basics. For example, we may not try to forecast what the price 
of a loaf of bread is or a pound of meat. We know the average prices for 
these items; we consume them on a regular basis and will continue doing 
so as long as nothing drastic happens. However, if you were to buy a 
new car you would most likely call around and check some showrooms 
to learn about available features and prices because we tend not to have 
up to date information on big-ticket items or goods and services that we 
do not purchase regularly. Th e process described previously is a kind of 
sampling, and the information that you obtain is called sample statistics, 
which are used to make an informed decision about the average price of 
an automobile. When the process is performed according to strict and 
formal statistical methods, it is called statistical inference. Th e specifi c 
sample statistics is called sample mean. Th e mean is one of numerous 
statistical measures at the disposal of modern economists. 
Another useful measure is the median. Th e median is a value that 
divides observations into two equal halves, one with values less than the 
median and the other with values more than median. Statistics explains 
when each measure should be used and what determines which one is 
the appropriate measure. Median is the appropriate measure when deal-
ing with home prices or income. Applications of statistical analysis in 
economics are vast and sometimes reach to other disciplines that need 
economics for assistance. For example, when we need to build a bridge 
to meet economic, social, and even cultural needs of a community, it 
is important to fi nd a reliable estimate of the necessary capacity of the 
bridge. Statistics indicates the appropriate measure to be used by teaching 
us whether we should use the median or the mode. It also provides insight 
on the role that variance plays in this problem. In addition to  identifying 
the appropriate tools for the task at hand, statistics also provides the 

 
INTRODUCTION 
xv
methods of obtaining suitable data and procedure for performing analysis 
to deliver the necessary inference. 
One cannot imagine an economic problem that does not depend 
on statistical analysis. Every year, the Government Printing Offi  ce com-
piles the Economic Report for the President. Th e majority of the statis-
tics in the report are fact-based information about diff erent aspects of 
 economics, however, many of the statistics are based on some statistical 
analysis, albeit descriptive statistics. Descriptive statistics provide  simple, 
yet powerful insight to economic agents and enable them to make more 
informed decisions.
Another component of statistical analysis is inferential statistics. 
Inferential statistics allows the economist and political leaders to test 
hypotheses about economic conditions. For example, in the presence of 
infl ation, the Federal Reserve Board of Governors may choose to reduce 
money supply to cool down the economy and slow down the pace of 
infl ation. Th e knowledge of how much to reduce the supply of money is 
not only based on economic theory, but also depends on proper estima-
tion of the fi nal outcome. 
Another widely used application of statistical analysis is in policy deci-
sion. We hear a lot about the erosion of the middle class or that the mid-
dle class pays a larger percentage of its income in taxes than do lower 
and upper classes. How do we know who the middle class is? A set dollar 
amount of income would be inadequate because of infl ation although, 
we must admit, even a single dollar amount must also be obtained using 
statistics. However, statistical analysis has a much more meaningful and 
more elegant solution. Th e concept of interquartile range identifi es the 
middle 50% of the population or income. Interquartile range was not 
designed to identify the middle 50%, and it is not explained in these 
terms; nevertheless, the combination of economics and statistics is used 
to identify the middle 50% for economics and policy decision purposes. 
Knowledge of statistics can also help identify and comprehend daily 
news events. Recently, a report indicated that the chance of accident for 
teenage drivers increases by 40% when there are passengers in the car 
who are under 21 years of age. Th is is a meaningless report. Few teenagers 
drive alone or have passengers over 21 years of age. Total miles driven by 
teenagers when there are passengers less than 21 years of age far exceeds 

xvi 
INTRODUCTION
any other types of teenage driving. Other things equal, the more you 
drive, the higher the probability of an accident. Th is example indicates 
that knowledge of statistics is helpful in understanding everyday events 
and in making sound analyses.
One of the most important aspects of statistics is the discovery of 
rules that allow the use of a sample to draw inferences about population 
parameters. Inferential statistics allows us to make decisions about the 
possibility of an outcome based on its probability, not dissimilar to what 
we do in real life anyway. Life experience is private and is based on an 
individual. A friend is usually late, and based on that, we estimate his 
approximate arrival time. In statistics the process is formal. We take ran-
dom samples, and based on statistical theories of sampling distribution 
and the probabilities of outcomes, we make inferences and predictions 
about the outcomes. In essence, statistics formalizes the human experi-
ence of estimation and makes predictions more formal and provides theo-
retical proofs for anticipated outcomes. 
Th is book focuses on a few introductory topics in statistics and 
provides examples from economics. It takes a diff erent orientation for 
covering the material than most other books. Chapters 1 and 2 cover 
descriptive statistics from tabular, graphical, and numeric points of view. 
A summary table of all the tools introduced in these chapters is provided 
in Chapter 1 to help you see the big picture of what belongs where. Th is 
grouping helps relate topics to each other. Chapter 3 provides some appli-
cations of these basic tools in diff erent areas of economics. Th e purpose of 
Chapter 3 is to demonstrate that even simple statistics, when used prop-
erly, can be very useful and benefi cial. Interestingly, some, if not most, of 
descriptive  statistics are either intuitive or commonly utilized in everyday 
life.  However, as the fi rst three chapters reveal, it is useful to demonstrate 
their power using examples from economics.
Chapter 4 introduces some commonly used distribution  functions.  
Th ese will most likely be new for you. Th ese distribution functions are 
used as yardsticks to measure diff erent statistics to determine if they 
behave as expected, or they should be considered unusual outcomes. 
Interestingly, when we sample, the resulting sample statistics such as 
 sample mean,  follow certain distribution functions. Th ese important 
properties are discussed in Chapter 5, titled Sampling Distribution of 

 
INTRODUCTION 
xvii
Sample Statistics. Chapter 6 formally discusses estimation. Point estima-
tion uses sample statistics directly, while confi dence interval provides a 
range that covers population parameter with a desired level of confi dence. 
Finally, Chapter 7 combines materials from Chapters 4 through 6 to per-
form statistical inference. Statistical inference is a probabilistic statement 
about the expected outcome of a study.
A volume like the present work is not suffi  cient to do justice to the 
subject. Every aspect of science is touched by statistics, to some extent. 
Th erefore, specialty books about applications of statistics in diff erent 
fi elds abound. 


CHAPTER 1
Descriptive Statistics
 Introduction
A simple fact of life is that most phenomena have a random compo-
nent. Human beings have a natural height that is diff erent than the 
natural height of a dog or a tree. However, human beings are not all 
of the same height. Th e usually small range is governed by random 
error. For example, the range of adult human height is roughly 52–75 
inches. Th is does not mean that 100% of all mankind are in this range. 
Th e small portions that are outside this range are considered outliers. 
Summarizing the height of human beings is very common in statistics. 
However, in science, it is helpful to provide the associated level of con-
fi dence in a statement. For example, it is important to state that a par-
ticular percentage, say 90%, of human beings have a height between 
58 and 72 inches. One might think that it is important, or may be 
even necessary, to provide a range that covers all cases. However, such 
a range may prove to be too wide to be of actual use. For example, one 
might be able to say with 100% certainty that the annual income in 
the United States is between $0 and $100,000,000,000. Although, the 
lower end is a certainty, the upper end need not be as defi nite. Granted 
that the chance of anyone making $100,000,000,000 in a year is very 
low, nevertheless, there is no  compelling reason against it. Th erefore, 
one has to provide the probability of someone making such a huge 
income. Since this chance is low, it would be more meaningful to state 
an income range for a meaningful majority, such as income range of 
95% of people. It is more useful to know that 99% of all people in 
the United States earned less than $380,354 per individual return in 
2008,1 which is the same as saying that the top 1% made at least that 
much per individual return in the same year.  According to the same 
source, the top 10% made more than $113,799 per  individual return.  

2 
STATISTICS FOR ECONOMICS
Th e actual percentage is not important and depends on the task at 
hand. For example, the government might want to help the middle 
class, which has been losing ground in economic terms, by granting 
them a tax break to lower their tax burden to a burden equivalent to 
that of the upper and lower classes. One way of determining the mid-
dle class income of a population is to fi nd 50% of the people whose 
incomes are in the middle. Alternatively, this means to identify the cut-
off  income level for the lower 25% of incomes, and the cutoff  income 
level for the upper 25% of incomes. Th e two cutoff  incomes mark 
the income range that contains 50% of the incomes.  Computations 
necessary to determine these and other useful values are the subject of 
descriptive statistics.
Descriptive statistics provides quick and representative information 
about a population or a sample. A typical man is 5′10″, the average tem-
perature on July 4 is 89°, Olympic runners fi nish the 100 meter dash in 
under 10 seconds, the most common shoe size for women is seven, and 
so forth. Th ese statistics are describing something of interest about the 
population and condense all the facts in a single parameter. D escriptive 
statistics is the science of summarizing and condensing information in 
few parameters.
Th ere are many ways of condensing information to create descrip-
tive statistics. Diff erent types of data require diff erent tools. Data can be 
qualitative or quantitative. Th ese naming conventions actually refer to 
the way variables are measured and not the inherent characteristic of a 
phenomenon.  In my opinion, these naming conventions are inaccurate. 
Variables are used for statistical analysis and are measured based on their 
characteristics. Sometimes, qualitative variables are called categorical 
variables. Th ere are numerous measurement scales. Th erefore, we focus 
on qualitative and quantitative variables.
In many cases, analyzing qualitative and quantitative variables 
requires diff erent tools, but in some cases the tools are similar, if not iden-
tical, for both. However, the interpretations of qualitative and quantita-
tive variables are usually diff erent. Note that a population is not defi ned 
as either qualitative or quantitative. Rather, it is the variable of interest 
in the  population that is either qualitative or quantitative. For example, 
the population can be defi ned as a person. If the age of the person is of 

 
DESCRIPTIVE STATISTICS 
3
 interest, then the variable is quantitative; but if the gender of the person is 
of interest, then the variable is qualitative. If the population is a fi rm and 
the variable is pollution (the fi rm pollutes or does not pollute), then it is 
a qualitative variable. However, if the amount of pollution is of interest, 
then it is a quantitative variable. 
Deﬁ nition 1.1 
Qualitative variables are non-numeric. Th ey represent a label for a cat-
egory of similar items. For example, the color of socks of students in a 
class is a qualitative data. 
Deﬁ nition 1.2
Quantitative variables are numerical and countable values. Th e distance 
each student has to travel to get to school is a quantitative data.
Measurement Scales
Variables must be measured in a meaningful way. Th e following is a brief 
description of diff erent types of measurement scales. Over time, diff er-
ent vocabularies and naming conventions have evolved in naming dif-
ferent measurement scales. It is not possible to decipher an appropriate 
measurement scale by observing the measurement. Instead, the method 
of measurement and the quantities that are measured must be examined 
to determine the extent of the meaning one can assign to the numeric val-
ues, and hence, identify the measurement scale. Most of the methods in 
this text require an interval or measurement scales with stronger  relational 
requirements.
Deﬁ nition 1.3 
Nominal or categorical data are the “count” of the number of times an 
event occurs. As an example for categorical data, countries might be 
grouped according to their policy toward trade and might be classifi ed 
as open or closed economies. Care must be taken to assure that each case 

4 
STATISTICS FOR ECONOMICS
belongs to only one group. An ID number is an example of nominal 
data. As the relative size does not matter for nominal data, the customary 
arithmetic computations and statistical methods do not apply to these 
numbers. 
Deﬁ nition 1.4 
When there are only two nominal types, the data is dichotomous. Dichot-
omous variables are also known as dummy variables in econometrics. 
When there is no particular order the dichotomous variable is called 
the discrete dichotomous variable. Gender is an example of a discrete 
dichotomous variable. When one can place an order on the type of 
data, as in the case of young and old, then the variable is a continuous 
 dichotomous variable.
Deﬁ nition 1.5
An ordinal scale indicates that data is ordered in some way. Although 
orders or ranks are represented by numerical values, such values are void 
of content and cannot be used for typical computations such as averages. 
Th e distances between ranks are meaningless. Th e income of the person 
who is ranked 20th in a group of ordered income is not twice the income 
of someone who is ranked 40th. In the ordinal scale only the comparisons 
“greater,” “equal,” or “less” are meaningful. Th is is a very important scale 
in economics, as in the case of utility and indiff erence curves. It is not 
necessary to measure the amount of utility one receives from diff erent 
goods and services; it is suffi  cient to rank the utilities. Th e customary 
arithmetic computations and statistical methods do not apply to ordinal 
numbers. 
Deﬁ nition 1.6
A Likert scale is a special kind of ordinal scale where the subjects provide 
the ranking of each variable. Customarily, the numbers of the choices 

 
DESCRIPTIVE STATISTICS 
5
for ranking are odd numbers to allow the center value to represent the 
“neutral” case. 
Deﬁ nition 1.7
In an interval scale, the relative distances between any two sequential 
values are the same. In the interval scale, size of the diff erence between 
measurements is also important. Each numerical scale is actually meas-
ured from “accepted zero.” Th is makes use of the type of scale irrelevant as 
in the case of Celsius and Fahrenheit scales for temperatures. Both scales 
have an arbitrary zero. Some arithmetic computations such as addition 
and subtraction are meaningful.
Deﬁ nition 1.8
Th e ratio scale provides meaningful use of the ratio of measurements in 
addition to interval size and order of scales. For example, the ratio of 
sales, gross domestic product (GDP), and output are expressed as ratio 
scale.
Th ere are numerous other measurement scales, but these have lit-
tle practical use in economics. A classical work on measurement is by 
S. S. Stevens.2
Types of Available Tools
Descriptive statistics provides summaries of information about a popu-
lation or sample, both of which will be defi ned shortly.  Th e amount 
of information available is vast and comprehending their intrinsic value 
is diffi  cult. Descriptive statistics provides some means of condensing 
 massive amounts of information in as few parameters as possible. 
Deﬁ nition 1.9
A parameter is a characteristic of a population that is of interest. Para-
meters are constant and usually unknown. 

6 
STATISTICS FOR ECONOMICS
Examples of parameters include population mean, population vari-
ance, and regression coeffi  cients. One of the main purposes of statistics 
is to obtain information from a sample that can be used to make infer-
ences about population parameters. Th e estimated value obtained from a 
 sample is called a statistic.
Table 1.1. Descriptive Statistics 
Qualitative 
Variables
Tabular 
Methods
Frequency
Relative Frequency
Graphical 
Methods
Bar Graphs
Pie Charts
Quantitative 
Variables
Tabular 
Methods
Frequency Distribution
Relative Frequency
Cumulative Distribution
Percentiles
Quartiles 
Hinges
Graphical 
Methods
Histograms
Ogive
Stem-and-Leaf
Dot Plot
Scatter Plot
Box Plot
Numerical 
Methods
Measures of Location
Mean
Ungrouped Data
Grouped Data
Trimmed Mean
Median
Mode
Measures of Dispersion
Range
Interquartile Range
Variance
Ungrouped Data
Grouped Data
Standard Deviation
Coefﬁ cient of Variation
Measures of 
 Association
Covariance
Correlation Coefﬁ cient

 
DESCRIPTIVE STATISTICS 
7
Table 1.1 summarizes the descriptive methods for quantitative and 
qualitative variables. Note that these are only the descriptive statistics and 
by no means all the methods at our disposal.
Deﬁ nition 1.10
When data are summarized or organized to provide a better and more 
compact picture of reality, then data are grouped. Th e grouping can be in 
the form of relative frequency or summarized in cross tabulation tables 
or into classes.
Descriptive Statistics for Qualitative Variables
Th e available descriptive statistics for qualitative variables can be divided 
into graphical and tabular methods. Each one consists of several cus-
tomarily used tools. In order to be able to graph the data, it must be tabu-
lated in some fashion; therefore, we will discuss the tabular methods fi rst. 
Tabular Methods for Qualitative Variables
Th e most common tabular methods for qualitative variables are frequency 
and relative frequency.
Frequency Distribution for Qualitative Variables
A frequency distribution shows the frequency of occurrence for 
 non- overlapping classes.
Example 1.1
In a small town a small company is responsible for refi lling soda 
 dispensers of 30 businesses. Th e type of business, the average number 
of cans of soda (in 100 cans), the gender of the business owner, and the 
race of business owner are presented in Table 1.2. Find the frequencies 
of the business types where soda dispensers are located. 
(Continued )

8 
STATISTICS FOR ECONOMICS
(Continued )
Table 1.2. Some Information About Soda Dispensers
Store type
Average
Gender
Race
Gas Station
3.8
Male
Black
Gas Station
3.5
Female
Black
Gas Station
2.6
Male
White
Mechanic Shop
2.1
Male
Black
Mechanic Shop
1.9
Male
White
Mechanic Shop
3.4
Female
White
Mechanic Shop
2.7
Male
White
Mechanic Shop
1.8
Female
Black
Mechanic Shop
3.7
Male
White
Mechanic Shop
4
Female
White
Mechanic Shop
1.9
Female
White
Mechanic Shop
2.6
Female
White
Drug Store
2.7
Female
White
Drug Store
2.4
Male
Black
Drug Store
3.6
Female
Black
Drug Store
3.2
Female
White
Drug Store
2.7
Male
White
Hardware Store
3.5
Male
Black
Hardware Store
1.8
Female
White
Hardware Store
3.4
Male
White
Hardware Store
2.8
Male
Black
Hardware Store
2.1
Female
White
Hardware Store
3.1
Female
White
Hardware Store
2.6
Male
White
Sporting Goods
1.7
Male
White
Sporting Goods
4
Female
White
Sporting Goods
3.2
Male
White
Sporting Goods
2.4
Female
Black
School
3.7
Female
White
Tire Shop
2.8
Female
White
(Continued )

 
DESCRIPTIVE STATISTICS 
9
Solution
A frequency distribution for the business type will clarify the  information.
Gas Stations
///
Mechanic Shop
////
////
Drug Store
////
Hardware Store
////
//
Sporting Goods
////
School
/
Tire Shop
/
Th is certainly is an improvement, but Table 1.3 makes it even 
clearer and more condensed. 
Table 1.3. Business Types, Frequencies, and 
Relative Frequencies of Locations 
Location
Frequency
Gas Stations
3
Mechanic Shop
9
Drug Store
5
Hardware Store
7
Sporting Goods
4
School
1
Tire Shop
1
Total
30
It is easier to determine the locations, how many times each is 
restocked, as well as fi nding the most frequent, and the least frequent 
locations. 
A table with 30 cells has been reduced to a two-column table with 
seven rows. If there were 20,000 locations and the types of business 
remained similar, the resulting table would not be any larger. While no 
one can really understand anything from a table with 20,000 entries, the 
resulting table would be very clear.  Th is signifi es the power of statistics to 
condense information in as few parameters as possible. Th e result can be 

10 
STATISTICS FOR ECONOMICS
graphed for more visual presentation. One possible graph is called a bar 
graph. Other graphs, such as pie charts, are also available. 
Th e example could have been about the types of industries in a 
state, the kinds of automobiles produced at a plant, the kinds of ser-
vices provided by a fi rm, or the kinds of goods sold in a store. Th e 
method of determining the frequencies would be the same in all such 
cases. 
Relative Frequency for Qualitative Data
Th e magnitude of the frequency changes for diff erent populations and 
samples. For better comparison, the relative frequency is used. Th e rela-
tive frequency shows the percentage of each class to the total population 
or sample. It is obtained by dividing the frequency for each class by the 
total in the population or the sample (see Table 1.4).
Th e sum of the relative frequency is always 1.0. Here, however, the 
sum is not exactly 1 due to roundoff  error.
Graphical Methods for Qualitative Variables
Th e two most commonly used graphical methods for qualitative variables 
are bar graphs and pie charts. Many other graph types have been intro-
duced with the advent of spreadsheet and more are available in specialty 
software. 
Table 1.4. Business Types, Frequencies, and Relative Frequencies 
of Locations 
Location
Frequency
Relative frequency
Gas Stations
3
0.1
Mechanic Shop
9
0.3
Drug Store
5
0.166
Hardware Store
7
0.233
Sporting Goods
4
0.133
School
1
0.033
Tire Shop
1
0.033
Total
30
0.9998

 
DESCRIPTIVE STATISTICS 
11
Bar Graph
A bar graph is a graphical representation of the frequency distribution, 
or relative frequency distribution, when dealing with qualitative data. Th e 
names of the qualitative variables are placed on the x-axis and the fre-
quency is depicted on the y-axis. A histogram and a bar graph are identi-
cal except for the fact that the bar graph is used for qualitative variables, 
while the histogram is used for quantitative variables.
Example 1.2
Th e following table represents the frequency of the business type 
among 30 locations of soda dispensers. Provide a bar graph of the 
business types where soda dispensers are located.
Business type
Frequency
Gas Stations
3
Mechanic Shops
9
Drug Stores
5
Hardware Stores
7
Sporting Goods
4
School
1
Tire Shop
1
Total
30
Solution
All examples involving graphs are solved using Microsoft Offi  ce 
Excel. For the graphs, the appropriate Excel commands are given 
in each section. All the other Excel commands are included in the 
Appendix. 
Bar graph in Excel is rotated 90° to the right. Th e sequence of 
commands to plot a bar graph in Excel is provided for your  reference. 
Open a new spreadsheet in Excel. In cell A1, type “Business Type” 
and then going downward list the business types as shown in the table 
above. In cell B1, type “Frequency” and then going downward list the
(Continued )

12 
STATISTICS FOR ECONOMICS
(Continued )
frequency in cells A1 through B8. Go to “Insert,” which is the second 
tab at the top left on the spreadsheet. Click on “Column” (which looks 
like a bar graph) and then click on the fi rst chart (top left). Excel will 
populate a chart similar to the one below. 
Figure 1.1. Bar graph of business types, frequencies, and relative 
frequencies of locations.
0
Gas stations
Mechanic shops
Drug stores
Hardware stores
Sporting goods
School
Tire shop
1
2
3
4
5
6
7
8
9
10
Frequency
Frequency
Th e above graph can represent the relative frequencies, too. Only the 
unit of measurement on the y-axis will diff er.  In Excel this graph is called 
a “column” graph. 
Creating a bar graph of the relative frequencies would not provide 
additional meaningful results. Th e graph will be identical to the above 
graph, except the scales on the vertical axis would be the relative frequen-
cies (percentages) and not the actual frequencies. However, the relative 
frequencies are already known so further benefi t is not gained. Often it 
is more meaningful to plot the relative frequencies instead of the actual 
frequencies because you can easily compare relative frequencies and they 
are similar to probabilities. 
Pie Chart
A pie chart is a graphical presentation of frequency distribution and 
 relative frequency. In this regard, the pie chart is similar to the bar graph 

 
DESCRIPTIVE STATISTICS 
13
because one cannot diff erentiate between the graphs of actual and relative 
frequencies, except for the scale. In some cases when a quantitative vari-
able has only few outcomes you can use the pie chart to provide visual 
eff ects. 
A circle is divided into wedges representing each of the categories in 
the table. If frequencies are charted, their magnitude is placed under their 
name. When the pie chart is based on the relative frequencies instead of 
the frequencies, the scale will be diff erent but not the size of the slices on 
the pie. 
Example 1.3
Provide a pie graph for the business types of locations in Table 1.3.
Solution
Open a new spreadsheet in Excel. In cell A1, type “Business Type,” list 
the business types as shown in the Table 1.3. In cell B1, type “ Frequency” 
 followed by the frequency for each business type as shown in the Table 1.3. 
Capture data in cells A1 through B8. Go to “Insert,” which is the second 
tab at the top left hand corner of the spreadsheet. Click on “Pie.” Several 
options become available. You can select whichever Pie shape you wish. 
Excel will populate a chart similar to the one below. 
Frequency
Gas stations
Mechanic shops
Drug stores
Hardware stores
Sporting goods
School
Tire shop
Figure 1.2. Pie charts of business types for soda dispensers— 
(Continued).
(Continued )

14 
STATISTICS FOR ECONOMICS
(Continued )
Figure 1.2. Pie charts of business types for soda dispensers.
Frequency
Tire shop
School
Sporting goods
Hardware stores
Drug stores
Mechanic shops
Gas stations
3% 3%
10%
30%
17%
23%
14%
Notice that due to space limitation the legend is placed on the side. 
Furthermore, the labels represent “Business Type” and have nothing to 
do with the pie colors the computer provides.
Descriptive Statistics for Quantitative Variables
As depicted in Table 1.1, there are more methods available to describe 
quantitative data. Some are very similar to the methods used in quantita-
tive methods, but their interpretations are usually broader.
Tabular Methods for Quantitative Variables
Th ere are three commonly used tabular descriptive statistics for quantita-
tive variables. Th ey are frequency distribution, relative frequency distribu-
tion, and cumulative distribution.
Frequency Distribution for Quantitative Variables
A frequency distribution shows the frequency of occurrence for 
 non- overlapping classes. Unlike the qualitative frequency distribution, 
there are no set and predefi ned classes or groups. Th e researcher will 
determine the size of each class and the number of classes.  Such data are 
called grou ped data.

 
DESCRIPTIVE STATISTICS 
15
Example 1.4
An anthropologist is studying a small community of gold miners in 
a remote area. Th e community consists of nine families. Th e family 
income is reported below in thousands of dollars.
66, 58, 71, 73, 64, 70, 66, 55, 75
Solution
Th e researcher would like to summarize these data using descriptive 
statistics. We deliberately chose a small set to demonstrate the point 
better without boring calculations. In real life, data will be much 
larger, and it would make more sense to condense the data using some 
 technique, say descriptive statistics. As only one value is repeated, 
it does not make sense to build a frequency distribution; no real 
 summary will emerge. If we divide the data into classes, however, we 
can build the frequency distribution. Th e range of data is from 55 to 75. 
If the researcher wishes to have 5 classes, the size of each class would be:
Maximum
Minimum
75
55
Number of Classes
Class W
5
idth
4
−
−
=
=
=
Table 1.5. Classes and Their Frequencies
Classes
Frequency
55–59
2
60–64
1
65–69
2
70–74
3
 ≥ 75
1
Th e number of classes is arbitrary and any reasonable number of classes 
and class widths will work. Avoid extremities and unbalanced classes. To 
avoid decimal places in classes, we added an extra class for values greater 
than or equal to 75. Other solutions would be as valid. Th e quantitative 
data can include decimal numbers; however, in this case, extra caution is 
needed to avoid overlapping in the classes. 

16 
STATISTICS FOR ECONOMICS
Th e histogram command in Excel provides the frequency as well as 
the cumulative frequency. If the option “Chart Percentage” is selected 
from the histogram dialog box, the histogram and the Ogive will be 
graphed too.
A list of nine values has been reduced to a two-column table with six 
rows as shown in Table 1.5. Again here, the size of population is delib-
erately small to allow students to see the details easily and to be able 
to duplicate the results. Th e procedure would be the same for the fam-
ily incomes of the United States with a population around 300,000,000 
people. Th is signifi es the power of statistics to condense information in 
as few parameters as possible. Th e result can be graphed for more visual 
presentation. One such graph is called a dot plot. Other graphs such as a 
histogram are also available.
Relative Frequency Distribution for Quantitative Variables
Th e relative frequency for quantitative variables is computed in the 
same way as those of qualitative variables. Th e frequency for each class is 
divided by the total number of members in the population or sample to 
obtain the relative frequency. 
Example 1.5
Table 1.6 provides the relative and cumulative frequencies for the fam-
ily incomes indicated in the Example 1.4.
Table 1.6. Relative and Cumulative Frequencies of Family 
 Incomes
Classes
Frequency
Relative frequency
Cumulative frequency
55–59
2
0.222222222
0.222222222
60–64
1
0.111111111
0.333333333
65–69
2
0.222222222
0.555555556
70–74
3
0.333333333
0.888888889
≥ 75
1
0.111111111
1

 
DESCRIPTIVE STATISTICS 
17
Cumulative Frequency Distribution for Quantitative Variables
In the case of quantitative variables, the classes or values of interest 
are sequential and have meaningful order, usually from smallest to the 
largest. Th is allows us to obtain cumulative frequencies. Cumulative 
frequencies consist of sums of frequencies up to the value or class of 
interest. Th e last value is always 1 since it represents 100% of observa-
tions (see Table 1.6).
Percentiles 
A percentile is the demarcation value below which the stated percentage 
of the population or sample lie. For example, 17% of a population or a 
sample lies below the 17th percentile. 
To obtain a percentile, sort the data and identify which value repre-
sents the stated percentile. Th e 17th percentile of a data containing 84 
members is the 15th member of the sorted group (0.17 × 84 = 14.28). 
As countable data cannot take a fractional value, the 15th member of 
the sorted data is the observation where 17% of the data are smaller 
than it. 
To obtain the percentile, after sorting the data calculate an index i:
 
100
p
i
n
=
 
(1.1)
where p is the desired percentile and n is either the population or the 
sample size. When the result is an integer, add 1 to it to get the position of 
the percentile. If the result is a decimal value, use the next higher integer 
to get the position of the percentile.
Example 1.6
A retail store has collected sales data, in thousands of dollars for 
18 weeks. Find the 18th and the 50th percentiles for weekly sales.
66, 58, 71, 73, 64, 70, 66, 55, 75, 65, 57, 71, 72, 63, 71, 65, 55, 71
(Continued)

18 
STATISTICS FOR ECONOMICS
(Continued)
Solution
Sort the combined data.
55, 55, 57, 58, 63, 64, 65, 65, 66, 66, 70, 71, 71, 71, 71, 72, 73, 75
Th e 18th percentile is obtained by:
 
80
18
14.4
100
i =
×
=
Since the result is a real number, it has decimal value. Th us, use 
the next higher integer, which is 15 in this example.  Th e number in 
the 15th position is the 18th percentile. Th at value is 71.
 Th e 50th percentile is:
 
50
18
9
100
i =
×
=
Since the index is an integer, use the next higher integer, namely 
the 10th observation, which is 66.  
Quartiles
Quartiles divide the population into four equal portions, each equal to 
25% of the population. Like the median and percentiles, the data must 
be sorted fi rst. Th e fi rst quartile, Q1, is the data point such that 25% of 
the data are below it. Th e second quartile, Q2, is the data point such that 
50% of the data are below it. Th e third quartile, Q3, is the data point such 
that 75% of the data are below it. 
Th e fi rst quartile is the same as the 25th percentile. Th e second quar-
tile is the same as the 50th percentile, as well as the median. Th e third 
quartile is the same as the 75th percentile. Th e quartiles are calculated 
the same way as the 25th, 50th, and 75th percentiles using the following 
indices.
Use i =
×
25
100
n for the fi rst quartile.

 
DESCRIPTIVE STATISTICS 
19
Use i =
×
50
100
n for the second quartile.
Use i = 
×
75
100
n for the third quartile.
If the result of the index is an integer, use the next higher integer to 
fi nd the location of the quartile. If the result of the index is a real value, a 
value with a decimal number, the next higher integer will determine the 
position of the quartile. 
Example 1.7
For the weekly sales data of the retail store in Example 1.5, fi nd the 
fi rst, second, and the third percentiles. Th e data are repeated for your 
convenience.  
66, 58, 71, 73, 64, 70, 66, 55, 75, 65, 57, 71, 72, 63, 71, 65, 55, 71
Solution
Th e 3 quartiles are calculated using the following indexes.
i = 
×
25
100
18 = 4.5 the fi rst quartile is in the 5th position.
i = 
×
50
100
18 = 9 the second quartile is in the 10th position.
i = 
×
75
100
18 = 13.5 the third quartile is in the 14th position.
Sort the combined data.
55, 55, 57, 58, 63, 64, 65, 65, 66, 66, 70, 71, 71, 71, 71, 72, 73, 75 
 
Q1 
Q2 
Q3
Deﬁ  nition 1.11
Th e fi rst quartile is the 25th percentile, the second quartile is the 50th 
percentile, and the third quartile is the 75th percentile.

20 
STATISTICS FOR ECONOMICS
Hinges
Th e hinges also divide the data into four equal portions. Th e hinges, 
however, use the defi nition of the median. For example, sort the data 
(as indicated below) and then fi nd the median. Find the median of the 
lower half and call it the fi rst hinge. Find the median of the second half 
and call it the upper hinge. 
Example 1.8
For the weekly sales data of the retail store in Example 1.5, fi nd the 
fi rst, second, and third percentiles. 
Solution
Sort the combined data.
55, 55, 57, 58, 63, 64, 65, 65, 66, 66, 70, 71, 71, 71, 71, 72, 73, 75
 
lower hinge 
Median 
upper hinge
Graphical Methods for Quantitative Variables
Th e numbers of available graphical methods for quantitative variables far 
exceed the number of graphical methods available for qualitative  variables. 
Here, we will address histograms, Ogive, stem-and-leaf,  dot-plot, scatter 
plot, and box plot. Box plot uses some of the concepts that are introduced 
in Chapter 2.
Histogram
A histogram is a graphical representation of the frequency distribution 
or relative frequency distribution when dealing with quantitative data. 
Th e boundaries of the classes are used for the demarcation of the vertical 
bars. A histogram and a bar graph are identical except for the quantitative 
values used in the histogram on the x-axis. 

 
DESCRIPTIVE STATISTICS 
21
Example 1.9
Th e following data represents the income of gold miners in a small 
community. Th e corresponding histogram follows (see Figure 1.3).
Classes
Frequency
55–59
2
60–64
1
65–69
2
70–74
3
≥75
1
Open a new spreadsheet in Excel. In cell A1, type “Bin” and enter 
the Bin Range from the list below. In cell B1, type “Frequency” followed 
by the frequencies. Th e data should be captured in cells A1 through B6. 
Go to “Insert,” which is the second tab at the top left hand corner of 
the spreadsheet. Click on “Column” (which looks like a bar graph) and 
then click on the fi rst chart (top left). Excel will populate a chart similar 
to the one below.
Th e output from Excel is presented below. Th e frequency is shown 
too.
Figure 1.3. Histogram and related setup in Excel—(Continued)
55
Bin Range
58
59
64
64
66
69
66
74
70
71
73
75
Bin
Frequency
59
2
64
1
69
2
74
3
More
1
(Continued )

22 
STATISTICS FOR ECONOMICS
Figure 1.3. Histogram and related setup in Excel.
Histogram
Bin
More
74
69
64
59
4
2
0
Frequency
Frequency
Th e above graph can represent the relative frequencies, too. Only the 
unit of measurement on the y-axis will diff er.
Th e above graph was created in Excel. Ironically, in Excel this graph is 
called a “column” graph. Bar graph in Excel is the same thing except for 
the rotation of 90° to the right. 
Ogive
Th e graph for the cumulative frequencies is called Ogive. In carpentry, 
there is a molding bit for shaping the edge of the wood called Roman 
Ogive. Th e graphs of the cumulative frequencies usually resemble the 
 fi nished edge of the Roman Ogive molding.
Figure 1.4. Ogive superimposed on histogram.
Histogram
Bin
Frequency
59
0
0.5
1
1.5
2
2.5
3
3.5
64
69
74
More
Frequency
Cumulative %
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
120.00%
(Continued)

 
DESCRIPTIVE STATISTICS 
23
Example 1.10
For the below nine communities of gold miners, fi nd the graph 
 frequencies and cumulative frequencies.
66, 58, 71, 73, 64, 70, 66, 55, 75
Solution
Th e frequency, relative frequency, and cumulative frequency for these 
data are given in Table 1.7.
Table 1.7. Frequency, Relative Frequency, and Cumulative 
 Frequency
Classes
Frequency
Relative 
 frequency
Cumulative 
frequency
55–59
2
0.222
0.222
60–64
1
0.111
0.333
65–69
2
0.222
0.555
70–75
4
0.444
0.999
In Excel the Ogive is obtained from the histogram dialog box by 
selecting the cumulative percentage option.
Th e Ogive gives the cumulative area under the relative frequency his-
togram. Th e derivative of the function that represents the Ogive will give 
the relative frequency histogram function (see Figure 1.4).
Stem-and-Leaf
Stem-and-leaf is another descriptive way of summarizing information 
and, hence, qualifi es as descriptive statistics. Tukey3 introduced the con-
cepts of the stem-and-leaf. Some authors, such as Anderson et al.,4 place 
stem-and-leaf under the exploratory data. 
In the stem-and-leaf, usually the last digit of a value is recorded 
as the leaf and the preceding digits on a number as the stem. 

24 
STATISTICS FOR ECONOMICS
A  vertical line for easy visualization separates the leaves and stems. To 
 create a  stem-and-leaf display, place the fi rst digit(s) of each data to 
the left of a vertical line. Place the last digit of each data to the right 
of the line. 
Example 1.11
Provide a stem-and-leaf graph for the gold miners’ data.
66, 58, 71, 73, 64, 70, 66, 55, 75
Solution
Th e frequency, relative frequency, and cumulative frequency for this 
data are given in Table 1.7.
Figure 1.5(a). Stem-and-leaf graph.
6  6  4  6
5  8  5
7  0  1  3  5
Note that the result resembles a rotated histogram. If the data 
for  each leaf is also sorted, a better summary is obtained, as in 
 Figure 1.5(b).
Figure 1.5(b). Sorted stem-and-leaf graph.
6  4  4  6
5  5  8
7  0 1  3  5
If the numbers are too large, the fi rst two or more digits could be 
placed on the left side. Th e idea is to select the digits in a manner that 
makes the summary useful.

 
DESCRIPTIVE STATISTICS 
25
Dot Plot
Th e dot plot is useful when only one set of data is under consideration. 
Th e actual data are placed on the x-axis. For each occurrence of the value 
a dot is placed above it. All the dots are at the same height, which has no 
signifi cant meaning other than refl ecting the occurrence of the observa-
tion. In the case of multiple occurrences additional dots are placed above 
the previous ones. Th e dots are placed at equal distances for visual as well 
as representation purpose. 
Example 1.12 
Provide a dot plot for the gold miners’ data.
66, 58, 71, 73, 64, 70, 66, 55, 75
Solution
See Figure 1.6.
Figure 1.6. Dot plot.
55
58
64
66
70
71
73
75
A dot plot resembles an exaggerated histogram.
Scatter Plot
An observant reader would notice that all the previous examples have been 
based on only one variable with numerous classifi cations and categories. 
In economics and many other branches of science, it is also benefi cial to 
present graphics of two or more variables. Below, we will use a scatter plot 
to show the relationship between two variables (see Figure 1.7). More 
variables can be combined into one graph. Usually, numerous variables 
are placed on the x-axis and only one variable is placed on the y-axis to 
show the relationship between the variable in the former group and the 

26 
STATISTICS FOR ECONOMICS
variable on the y-axis. In such cases, the variables on the x-axis are usually 
the ones that aff ect the variable on the y-axis. Sometimes they are called 
factors and response variables, respectively.
Example 1.13
Graph a scatter plot of annual income and consumption data for the 
United States for the years 1990 through 2010 which is depicted in 
Table 1.8.
Table 1.8. Annual Income (I) and Consumption (C) in the 
United States (1990–2010)
Year
I
C
1990
17,004
15,331
1991
17,532
15,699
1992
18,436
16,491
1993
18,909
17,226
1994
19,678
18,033
1995
20,470
18,708
1996
21,355
19,553
1997
22,255
20,408
1998
23,534
21,432
1999
24,356
22,707
2000
25,944
24,185
2001
26,805
25,054
2002
27,799
25,819
2003
28,805
26,833
2004
30,287
28,179
2005
31,318
29,719
2006
33,157
31,102
2007
34,512
32,356
2008
36,166
32,922
2009
35,088
32,087
2010
36,051
33,039
Sources: Bureau of Economic Analysis: National Income and Product Account Tables (Table 
2.3.5-Personal Consumption Expenditures by Major Type of Product). GDP and Personal 
Income (SA1-3 Personal Income Summary). 
(Continued)

 
DESCRIPTIVE STATISTICS 
27
Solution
To obtain a scatter plot, type the information from Table 1.8 in an 
Excel spreadsheet.  In cell A1 type “I” for Income followed by the 
income data from the second column in Table 1.8. In cell B1, type 
“C” for Consumption followed by consumption data from the third 
column in Table 1.8. Highlight cells A1 through B22. Go to “Insert,” 
which is the second tab on the top left hand corner of the spreadsheet. 
Click on “Scatter,” which will reveal several options. Select the option 
at the top left. Excel will populate a chart similar to the one shown in 
Figure 1.7.
Figure 1.7. Scatter plot of income and consumption for the United 
States, 1990–2010.
10,000
10,000
15,000
20,000
25,000
Income
Consumption
30,000
35,000
40,000
15,000
20,000
25,000
30,000
35,000
Th ese graphs present, by no means, the extent of possible ways to 
present data for either qualitative or quantitative variables. Many other 
imaginative ways can be used, some of which are available in popular 
software such as Microsoft Excel or dedicated software such as Stata. 
Box Plot
A box plot is a visual representation of several basic descriptive statistics 
in a concise manner. Th e descriptive statistics that are used in a box plot 
are explained in Chapter 2. Th e graph consists of one box per variable. 
Th e borders of the box represent the 25th percentile (lower hinge) and 

28 
STATISTICS FOR ECONOMICS
the 75th percentile (upper hinge), with a line in the box representing 
the 50th percentile or the median. Lines, called whiskers, extend from the 
edges of the box to the adjacent values, capped by an adjacent line.5 Th e 
values further away from the box extending past the adjacent lines in 
either direction are called outside values.
Example 1.14 
Use the data from Table 1.2 to obtain the box plot of income by the 
type of business and by gender. 
Solution
Th e graph of box plot (Figure 1.8) is created in Stata.
Figure 1.8. Box plot of income by location and by gender.
1.5
2
2.5
3
3.5
4
Average
Tire shop
Sporting goods
School
Mechanic shop
Hardware store
Gas station
Drug store
Female
Male

CHAPTER 2
Numerical Descriptive 
 Statistics for Quantitative 
Variables
Introduction
One of the purposes of descriptive statistics is to summarize the 
 information in the data for a variable into as few parameters as possible. 
Measures of central tendency provide concise meaningful summaries of 
the population. Measures of central tendency are addressed in the below 
section. However, measures of central tendency are often not enough to 
 provide the full picture. Th e addition of measures of dispersion  provides 
a more complete picture. Measures of dispersion are covered in next, 
 followed by measures of association. 
Measures of Central Tendency
Mean
Th e arithmetic mean, or simply the mean, is the most commonly used 
descriptive measure. Other names for the mean are average, mathemati-
cal expectation, and the  expected value. Th is section deals with raw or 
 ungrouped data.
Arithmetic Mean
Th e mean takes the concept of condensing information to the extreme. 
Th e mean, a single value, is the representative or typical value that 
 represents a population. Th e mean is also known as the average, and 

30 
STATISTICS FOR ECONOMICS
more formally as the expected value. Th e mean is the sum of all the 
 elements in the population divided by the number of the elements.
 
m
=
= ∑
1
N
i
i
X
N
 
(2.1)
where m, pronounced mu, is the symbol used to represent the mean; Σ, 
pronounced sigma, represents the sum of some random variables. When 
there is no ambiguity, we can simply write:
 
(
)
X
N
m
∑
=
 
(2.2)
Th e mean is a parameter and provides information about the central 
tendency of the population. Th e mean is the representative, or expected 
value, of the population. For example, when we say that the average 
income of a country is $45,000, we are stating that if a person is selected 
at random his income is expected to be around $45,000. Th e mean is the 
expected value or the typical representative of a population when there is 
no other information. 
Th e mean, though being the most widely used and most important 
parameter of the population, has its limits. It is susceptible to extreme 
values. Since all values of the population are used in calculation of the 
mean, a single very large or very small value can have a major impact on 
it. Th is is not quite as important in the case of the population as it is with 
samples.
Sample Mean 
Th e sample mean is the sum of the sample values divided by the sample 
size.
 
m
∑
=
=
(
)
ˆ
X
X
n
 
(2.3)
Note that we used both, mˆ, pronounced mu-hat, and X–, pronounced 
x-bar to represent the sample mean. Both are widely accepted. However,  
mˆ has several advantages over X–. Th e fi rst advantage is that it reduces the 

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
31
number of symbols that one has to learn in half. Th e population  parameter 
is m and its estimate m. Th e second advantage is that it takes the guesswork 
out of which statistics you are dealing with, as long as you remember the 
population parameter. Th e third advantage is that it  provides a reasonably 
simple rule to follow. Population parameters are represented by Greek 
letters, and sample statistics are represented by Greek letters with a hat 
on them. 
Deﬁ nition 2.1
Statistics is a numeric fact or summary obtained from a sample. It 
is always known, because it is calculated by the researcher and it is a 
 variable.  Statistics is also used to make inferences about the corresponding 
 population parameter. 
Th e sample mean is a statistic. It is used to estimate the population 
parameter m.
Example 2.1
An anthropologist is studying a small community of gold miners in 
a remote area. Th e community consists of nine families. Th e family 
income is reported below in thousands of dollars. Find the sample 
mean. Th is data is hypothetical, but plausible. We use this data to 
show computational detail.
66, 58, 71, 73, 64, 70, 66, 55, 75
Solution
A careful reader would remember that the same data and scenario was 
introduced in Example 1.4 but with a major diff erence. Th e data there 
was presented as population data. Th e idea of a small community with 
nine families is acceptable but a sample of nine is more plausible. We 
are using the same data for ease of computation and we limit the data 
size to avoid tedious computations. Nevertheless, we use actual data 
with more observations in diff erent parts of the texts as well. 
(Continued )

32 
STATISTICS FOR ECONOMICS
Th e sample mean is:
66
58
71
73
64
70
66
55
75
598
ˆ
66.444
9
9
X
m
+
+
+
+
+
+
+
+
=
=
=
=
Th e expected income (in thousands of dollars) of any family from this 
population is 66.444. Th is statistic is an estimate of the population 
parameter m.
We did not provide an example for the population mean because most 
populations are large and it would take a lot of space to display such a vast 
amount of data. In addition, we were concerned that you would not see the 
forest for the trees. Th e procedure is nevertheless the same. We can assume 
that the above data is actually the population and obtain the mean, which 
would be the same number. However, there is a major diff erence between 
the sample mean and population mean, as between any statistics and 
parameter. Th e former is a variable while the latter is a constant. In actual 
research we seldom, if ever, know population parameters, which necessi-
tate collecting samples and obtaining sample statistics to make inference 
about the unknown population parameters. It is possible to have a small 
population, for example, the population can consist of the two children in 
a household but usually they are of little use in economic studies. 
If one of few extreme members of the population appears in a sample, 
especially a small sample, the impact will be detrimental. Remember the 
sample mean, a statistics, is used to estimate the population mean, a param-
eter. If the sample mean is erroneous, the estimated population mean will 
be misleading. Irrespective of what values appear in the sample, the sample 
mean does provide an unbiased estimate of the population mean. 
Example 2.2
Th e stock prices for Wal-Mart and Microsoft for the period from 
March  12 to March 30, 2012 and April  2 to April 21, 2012 are 
 provided in Table 2.1. We will use these data in many of the examples 
in this book. 
(Continued )
(Continued )

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
33
Table 2.1. Closing Prices of Wal-Mart (WMT) and Microsoft 
(MSFT) for the period from March 12 to March 30 and from 
April 2 to April 21, 2012 
Date
WMT
MSFT 
Date
WMT
MSFT
12 Mar.
$60.68 
$32.04
2 Apr.
$61.36
$32.29
13 Mar.
$61.00 
$32.67
3 Apr.
$60.65
$31.94
14 Mar.
$61.08 
$32.77
4 Apr.
$60.26
$31.21
15 Mar.
$61.23 
$32.85
5 Apr.
$60.67
$31.52
16 Mar.
$60.84 
$32.60
9 Apr.
$60.13
$31.10
19 Mar.
$60.74 
$32.20
10 Apr.
$59.93
$30.47
20 Mar.
$60.60 
$31.99
11 Apr.
$59.80
$30.35
21 Mar.
$60.56 
$31.91
12 Apr.
$60.14
$30.98
22 Mar.
$60.65 
$32.00
13 Apr.
$59.77
$30.81
23 Mar.
$60.75 
$32.01
16 Apr.
$60.58
$31.08
26 Mar.
$61.20 
$32.59
17 Apr.
$61.87
$31.44
27 Mar.
$61.09 
$32.52
18 Apr.
$62.06
$31.14
28 Mar.
$61.19 
$32.19
19 Apr.
$61.75
$31.01
29 Mar.
$60.82 
$32.12
20 Apr.
$62.45
$32.42
30 Mar.
$61.20 
$32.26
21 Apr.
$59.54
$32.12
Find the (sample) average price for Wal-Mart for the period from 
April 2 to April 21, 2012.
Solution
(
)
910.96
ˆ
$60.73
15
X
X
n
m
∑
=
=
=
=
 
Example 2.3
Suppose the researcher in Example 2.1 collected another sample: Th e 
income of these new families is reported below. Th e researcher wants 
to calculate the sample mean of their income as well.
65, 57, 71, 72, 63, 71, 65, 55, 71
Solution
590
ˆ
65.5555
9
X
m =
=
=
 

34 
STATISTICS FOR ECONOMICS
Th e sample mean changed since it is a statistic, which is a  variable. Th is 
sample mean provides an estimate of the population mean m.  Usually, the 
parameter remains unknown and statistics provide estimates of it. Th e mean 
of combined population can be obtained from the separate  component 
population means. If the researcher considers the 18  observations (of 
Examples 2.1 and 2.3) as one sample, the sample mean would be:
1188
ˆ
66
18
X
m =
=
=
Th e same result can be obtained from previous information:
1
1
2
2
ˆ
ˆ
66.4444 and
65.5555.
X
X
m
m
=
=
=
=
Th e mean of the combined samples is:
66.4444
65.555
ˆ
66
2
X
m
+
=
=
=
In this case, the sample sizes are equal, so simple arithmetic average 
works fi ne. In the case of diff erent sample sizes, the weighted average is 
the appropriate tool.
Later, we will cover two other measures of central tendency called 
median and mode. Th e knowledge of the median or the mode of two 
samples or two populations do not render to such calculation. Th e median 
or the mode of combined populations or samples cannot be obtained 
from the component populations or samples.
Trimmed Mea n 
Trimmed mean is a modifi cation of the mean. Th e sample data is sorted 
and a given percentage, say 5%, of the top and the bottom of the data are 
discarded, and the regular mean is calculated for the remaining data. Th is 
trimmed mean will be less susceptible to the extrem e values.
Geometric Mean 
Th e geometric mean is calculated using the following formula.
 
1
2
GM
...
N
N
X X
X
=
 
(2.4)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
35
In logarithmic form
  
1
2
1
log(
)
log(
)
log(
)
1
Log(GM)
log(
)
N
N
i
i
X
X
X
X
N
N
=
+
+
+
=
=
∑

 (2.5)
Th e advantage of the logarithmic formula is that it avoids taking the 
root of the results. Th is was more important before the advent of powerful 
calculators. Th e logarithmic formula is a linear sum of its elements. After 
taking the logarithm of the values the mean is calculated as the arithmetic 
mean.
Th e geometric mean is useful when the values change in geometric pro-
gression instead of arithmetic progression, as is the case with growth rates. 
Example 2.4
Assume that a new company grew at 28% the fi rst year, 15% the 
 second year, and 13% the third year. What is the rate of the growth of 
company?
1st year
1.00
beginning of the operation
2nd year
1.28
28% increase over the beginning
3rd year
1.472
15% increase over the 2nd year
4th year
1.66336
13% increase over the 3rd year
Th e arithmetic mean is not able to explain the geometric growth. Th e 
geometric mean will give the average.
3
3
GM
1.28 1.472 1.66336
3.134036378
1.463416715
=
×
×
=
=
Th e company grew at the average rate of 1.4634 or 46.34% per year.
Raise both sides to the 3rd power.
3.134036378 = (1.463416715)3 = (1 + 0.463476715)3
Th is is the formula for compound interest. To generalize, let P0 be the 
initial investment, Pn the amount after n years, and r the interest rate 
or the rate of growth.
 
(Continued)

36 
STATISTICS FOR ECONOMICS
 
Pn = P0 (1 + r)n 
(2.6)
In the example the ending value is known to be Pn = 3.134036378, the 
beginning value is P0 = 1.0, and n = 3. Th e (average) rate of growth is
3.134036378 = 1.0 (1 + r)3
3
(1
r)
3.134036378
1.463416715
+
=
=
r = 1.463416715 – 1 = 0.463416715
Th erefore, the (average) rate of growth is 46.34%, as was derived from 
the geometric mean.
 
Th e geometric mean is also the proper mean when dealing with ratio 
of items. 
Example 2.5 
Th e ratio of the average income to the price of an average car is 4 in 
year 1 and 5 in year 2. What is the average ratio of income to the price 
of a car? 
Solution
Using the arithmetic mean of income to car price provides an  incorrect 
answer: 
(4 + 5)/2 = 4.5
Th e arithmetic mean of car price to income 
⎛
⎞
+
⎜
⎟
⎝
⎠
+
=
=
=
1
1
(0.25
0.2)
4
5
0.225
2
2
Th e reciprocal of 0.225 is (1/0.225) = 4.444 not 4.5. Th erefore, the 
arithmetic mean is not a suitable measure when dealing with ratios. 
 
(Continued)
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
37
Th e average of the ratio of income to car price should not diff er from 
the average of the ratio of the car price to income.
Th e geometric mean for the two ratios is:
Geometric mean of income to car price = 
4
5
20
×
=
Geometric mean of car price to income = 
0.25 0.2
0.05
×
=
Th e reciprocal of 0.05 is (
) +
1/
0.05
20. Since the  geometric 
mean of the income to the price of car is the same as the reciprocal of 
the geometric mean of the price of car to income, the geometric mean 
is the proper average measure. 
 
Harmonic Mean 
Th e harmo nic mean is calculated using the following formula.
 
1
2
1
HM
1
1
1
1
N
N
i
i
N
N
X
X
X
X
=
=
=
+
+
+
∑

 
(2.7)
Th e harmonic mean is, in fact, the reciprocal of the arithmetic mean of 
the reciprocal of the values.
Example 2.6
A salesman travels to another city to meet a client. To make sure that 
he does not miss the appointment, he drives at 90 miles per hour. After 
a successful meeting he returns more leisurely at 45 miles per hour. 
What was his average speed? 
Solution
Th e speed was not (90 + 45)/2 = 67.5 mile per hour. For simplicity, 
assume he traveled a distance of 90 miles (any other value will work 
as well). 
 
(Continued)

38 
STATISTICS FOR ECONOMICS
Time while going = 90
1
90 =
Time while returning = 90
2
45 =
Total travel time = 1 + 2 = 3
+
=
=
=
=
+
90
90
Average speed
60
60 miles per hour
1
2
Distance
Time
Th e harmonic mean will give the correct answer where the arithmetic 
mean failed.  
×
=
=
=
=
+
+
2
2
2
90
HM
60 miles per hour
1
1
1
2
3
90
45
90
 
Rule 2.1
When n = 2 the geometric mean is equal to the square root of the arith-
metic mean times the harmonic mean.
 
GM
(
)(
)
AM
HM
=
 
(2.8)
Mean for Data with Frequencies
Th e expanded presentation below will be revealing.
 
1
2
1
2
1
2
1
1
1
N
N
N
X
X
X
X
X
X
N
N
N
N
X
X
X
N
N
N
m
+
+
+
=
=
+
+
+
=
+
+
+



 
(2.9)
Th e mean is the sum of 1/N times of each observation. In other 
words, each observation gets a weight equal to 1/N of the total. Some-
times, each value should receive a diff erent weight, say, f1, f2, …, fN for 
each of X1, X2, …, XN. In that case the result is in terms of frequencies 
of each value. 
 
m = f1X1 + f2X2 + … + fN XN = Sf X 
(2.10)
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
39
Note that Sf  = 1 and, hence, was not written. In general, the formula for 
the frequencies would be:
 
1
1
2
2
1
2
N
N
N
f X
f X
f
X
f
f
f
m
+
+
+
=
+
+
+


 
(2.11)
Weighted Mean 
Th e weighted mean is similar to the mean using the frequencies, except 
that the sum of weights need not add up to one. 
 
1
1
2
2
1
2
N
N
N
w X
w X
w X
w
w
w
m
+
+
+
=
+
+
+


 
(2.12)
Example 2.7
Refer to Example 2.1 and Example 2.3 regarding the incomes of gold 
miners from two samples. 
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
List the data according to incomes and their frequencies in a table. Use 
the number of cases, which is the same as frequencies in this example, 
as weight and calculate the weighted average.
Solution
Th is example is exactly the same as Example 2.8, except in the way the 
number of times an income is named. Here, they are considered as weights 
for each observation while in Example 2.8 they are considered frequencies.
Recall that the sample mean for the 18 men is
1188
ˆ
66
18
m =
=
Th e mean obtained by using the frequency distribution shown in the 
following table should be the same.
 
(Continued)

40 
STATISTICS FOR ECONOMICS
Observation
Weights
55
 2
57
 1
58
 1
63
 1
64
 1
65
 2
66
 2
70
 1
71
 4
72
 1
73
 1
75
 1
Total
18
Adding up the observations and dividing by 12 will give an  incorrect 
answer, except for coincidence. To obtain the correct mean, each 
observation must be multiplied by the number of times it occurs.
Note that the sum for the population is over the population size N, 
while the sum for the sample is for the sample size n.
55
2
57 1
58 1
63 1
64 1
65
2
66
2
70 1
71 4
72 1
73 1
75 1
ˆ
18
m
×
+
× +
× +
× +
× +
×
+
×
+
× +
×
+
× +
× +
×
=
 
1188
ˆ
66
18
m =
=
 
Relation Between Arithmetic, Geometric, and Harmonic Mean 
 
HM ≤ GM ≤ m 
(2.13)
Th e equality sign holds only if all sample values are identical.
Mean of Grouped Data
Statistics is the science of summarizing information. Th is goal is attained 
at several stages. Th e frequency distribution is a tabular summary of data. 
Th e mean is another summary statistics that is much more powerful 
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
41
in representing the data. Often, the data are only available after being 
 summarized as frequency distribution. In those cases, it is desirable to be 
able to fi nd the mean and other valuable parameters.
Mean of Data Summarized as Frequencies
Th e formula for the population mean is:
 
1
1
N
i
i
i
N
i
i
f X
f
m
=
=
=
∑
∑
 
(2.14)
Th e formula for the sample mean is:
 
m
=
=
=
∑
∑
1
1
ˆ
n
i
i
i
n
i
i
f X
f
 
(2.15)
Example 2.8
Refer to Example 2.1 and Example 2.3 regarding the incomes of gold 
miners from two samples. 
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
List the data according to incomes and their frequencies in a table. 
Calculate the mean using the values and their frequencies.
Solution
Using the table from Example 2.7 and multiplying the observations 
with the frequencies, we have
55
2
57 1
58 1
63 1
64 1
65
2
66
2
70 1
71 4
72 1
73 1
75 1
ˆ
18
m
×
+
× +
× +
× +
× +
×
+
×
+
× +
×
+
× +
× +
×
=
 
1188
ˆ
66
18
m =
=
 

42 
STATISTICS FOR ECONOMICS
Th erefore, summarizing the data into a frequency distribution does 
not aff ect the mean. It does not aff ect the variance either. Th e mean of 
the grouped data, however, will most likely be diff erent than the actual 
mean. Th is is due partly to the fact that the class sizes are arbitrary, and 
mostly due to the formula.
Mean for Grouped Data
Th e formula for the mean of the population grouped data is:
 
m
∑
= ∑
fM
f
 
(2.16)
Th e formula for the mean of the sample grouped data is:
 
m
∑
= ∑
ˆ
,
fM
f
 
(2.17)
where M is the mid-point of each class. Once again the only diff erence 
between the population and sample formulas is in the number of  elements 
and where they originate. Th is is the same as the weighted mean where 
the weights are the frequencies. Each value is given a weight equal to its 
number of occurrences or frequency.
Example 2.9
Group the data from Example 2.1 into four groups, each representing 
a 5-year interval. Calculate the mean using the grouped data.
Solution
Classes
Frequency
M
55–59
4
57
60–64
2
62
65–69
4
67
70–75
8
72.5
 
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
43
4
57
2
62
4
67
8
72.5
ˆ
18
228
124
268
580
66.6667
18
m
×
+
×
+
×
+
×
=
+
+
+
=
=
Th e result has changed slightly. Other groupings will result in diff erent 
calculated means. 
Quar tiles
Quartiles divide data into quarters. Th e fi rst quartile is such a number 
that 24% of data are below it. Similarly, 50% of data are below the second 
quartile and 75% are below the third quartile. To obtain quartiles, sort 
the data and fi nd 1 quarter, 2 quarters, and 3 quarters demarcations. See 
Examples 2.10 and 2.13 for numerical solutions.
Median
Th e median is such a value greater than 50% of the population. To obtain 
the median, sort the data—the one in the middle is the median. If there 
are two numbers at the middle, their average is the median. Some texts 
use M to designate the median. Th e median is the same as the 50th per-
centile, as well as the second quartile.
Example 2.10
Refer to Example 2.1 and Example 2.3 regarding the incomes of the 
gold miners in two communities. Calculate the median for each com-
munity separately. Find the median for combined data.
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
 
(Continued)

44 
STATISTICS FOR ECONOMICS
Solution
To obtain the median of each group, sort the values fi rst. Th e medians 
for the following two data sets are in bold letters.
55, 58, 64, 66, 66, 70, 71, 73, 75
55, 57, 63, 65, 65, 71, 71, 71, 72
Th e median of combined population cannot be obtained from the 
separate component population medians. 
55, 55, 57, 58, 63, 64, 65, 65, 66, 66, 70, 71, 71, 71, 71, 72, 73, 75
Th e median is (66 + 66)/2 = 66.
Mode
Th e mode is another measure of central tendency. Th e mode is the most 
frequently occurring value of the population. A population, or a sample, 
may have more than one mode or no mode at all. If all members of the 
population occur as frequently, either no mode is present or every  element 
is a mode. If there are two modes, the distribution is called bimodal. 
When the incomes of men and women are measured, there will be two 
modes, one for men and another for women.
Example 2.11
For data in Example 2.1 and Example 2.3, obtain the mode for the 
incomes of the two communities. Find the mode for the combined 
data as well.
Solution
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
Th e mode for the fi rst community is 66, and for the second community 
is 71. From the frequency distribution table provided in  Example 2.7 
the mode for the combined data is 71.
 
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
45
Th e mode of combined population cannot be obtained from the sepa-
rate component population modes. When the data is grouped, the mode 
is the midpoint of the interval with the greatest frequency. In a bar graph 
or a histogram, the tallest bar represents the modal value. In this case the 
range 70–75 years is the mode. 
Empirical Relation Between Mean, Median, and Mode
 
Mean – Mode = 3 (Mean – Median) 
(2.18)
Measures of Dispersion
Range
Th e range is a measure of dispersion. It refl ects how far the data are 
 scattered. It is calculated by subtracting the minimum value from the 
maximum value.
 
R = Maximum – Minimum 
(2.19)
Example 2.12
For the data in Example 2.1 and Example 2.3, obtain the range for the 
combined data. 
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
Solution
R = 75 – 55 = 20
 
Interquartile Range
Th e interquartile range is a measure of dispersion that measures the 
 distance between the fi rst and the third quartiles. 
 
IQR = Q3 – Q1 
(2.20)

46 
STATISTICS FOR ECONOMICS
Example 2.13
For the data in Example 2.1 and Example 2.3, obtain the interquartile 
range for the combined data.
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
Solution
Combine and sort the data.
55, 55, 57, 58, 63, 64, 65, 65, 66, 66, 70, 71, 71, 71, 71, 72, 73, 75
 
Q1 
Q2 
Q3
IQR = 71 – 63 = 12
 
Th e IQR can be used to fi nd the “middle class” of a population or a 
sample. It gives the lower and upper limits of the middle 50%. 
Variance
Th e variance is a measure of dispersion. It is one of the more  important 
parameters of a population. Th e concept of variance is used in many 
aspects of statistics.
Th e variance is the average error, squared. Th e need for the variance 
arises from the need to determine and calculate the error, which is an 
important statistical measure. Th e population mean is the best representa-
tive of the population. It represents the typical value or the expected value 
of any member of the population. Seldom, however, every member of the 
population has the identical value. If they did there would not be any point 
in studying and analyzing them. Each member of the population will be off  
from the expected value by some magnitude. Th e deviation may be positive 
or negative. Understanding and analyzing the individual errors would be 
diffi  cult. Instead, the average error is used. As with other subjects in statis-
tics, the goal is to reduce the phenomena to as few parameters as possible. 
Th is section deals with the raw or ungrouped data. Th e total error 
and, hence, the average error is always equal to zero. As a  mathematical 

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
47
property S(X – m) = 0. To overcome this problem the deviations are 
squared to obtain:
S(X – m)2
Th is value cannot be zero unless every observation is identical. Since 
larger populations will have larger sum of squared deviation, their average 
is calculated to enable comparison of diff erent sized populations.
Population Variance
Th e varia nce is the sum of the squares of the deviations of values from their 
mean, divided by population size. Th erefore, the variance is the mean of 
the squared deviations, which in the case of a sample it is called the mean 
squared error (MSE) and, hence, is an average  measure. Th e variance is 
the entire variation in a population. It does not change.
 
m
s
∑
−
=
2
2
(
)
X
N
 
(2.21)
Th e variance is also called “sigma squared” to refl ect the fact that it is 
a squared measure. Th e variance refl ects (the square of) how much a data 
point can deviate from the expected value, that is, the mean for the data. 
Th e numerator, the sum of squares of deviations, is usually called sum of 
squared total (SST).
 Sample Variance
Th e sample variance is the sum of the squares of the deviations of  values 
from the sample mean divided by the degrees of freedom. Th e concept 
of degrees of freedom is discussed in  Chapter 3. Th e  sample variance 
represents the entire variation in a given  sample.  Sample variance does 
not change for a given sample. As sample  variance is a statistic, its value 
will change from one sample to the other.
  

 2
2
(
)
1
X
X
n
m
s
∑
−
=
−
 
(2.22)

48 
STATISTICS FOR ECONOMICS
Example 2.14
R  efer to the gold miners income data of Example 2.1 and Example 2.3:
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
Calculate the sample variances of the incomes for each community.
Solution
Th e sample variance for the fi rst community is calculated as follows:
Income
(X – ˆm)
(X – ˆm)2
66
–0.44444
0.197531
58
–8.44444
71.30864
71
4.555556
20.75309
73
6.555556
42.97531
64
–2.44444
5.975309
70
3.555556
12.64198
66
–0.44444
0.197531
55
–11.4444
130.9753
75
8.555556
73.19753


m
s
∑
−
=
=
=
−
2
2
(
)
358.2222
44.7778
1
8
X
X
n
Similarly, the sample variance for the second community is calculated 
and is equal to

s =
2
40.277778
Verify that the variance of the combined samples is neither the sum 
of the variances of the samples, nor the average of their variances. You 
can also easily verify that the sum of (X – mˆ) equals zero, which will be 
useful in later discussions.
 

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
49
Standard Deviation 
Th e variance, of the population or sample, is in the square of the unit of 
the measurement of the observation. To make them comparable to the 
actual observation their square roots is taken.
 
2
(
)
X
N
m
s
∑
−
=
 
(2.23)
Th e s is called the standard deviation. Its counterpart is called sample 
standard deviation and is denoted by sˆ, pronounced sigma hat. 
 
 2
(
)
ˆ
1
X
X
n
m
s
∑
−
=
−
 
(2.24)
Th e standard deviation represents the average error of a population 
or sample. Th e standard deviation is a measure of risk, too. It refl ects how 
much a data point can deviate from the expected value, that is, the mean 
of the data, by chance. Th e standard deviation is the statistical “yardstick” 
that allows comparison of dissimilar entities. To measure the length of a 
room, place a yardstick at one end of the room; mark the fl oor at the end 
of the yardstick, move the yardstick to the mark, and mark the fl oor at the 
end of yardstick again, until the entire length of the room is measured. 
In other words, you divide the length of the room by the length of the 
yardstick, and the result will be a value in terms of the yard. Th e divisor 
provides the unit of measurement. Hence, the unit of measurement of 
standardized values is standard deviation.
The Standard Deviation of the Sample Mean 
When the value under consideration is the sample mean, its distribution 
is explained by the sampling distribution of the sample mean, a topic 
which is covered in detail in Chapter 5. For the time being, we will simply 
provide the relationship without the background information:
 
2
2
ˆ
ˆ
Var( )
n
m
s
m
s
=
=
 
(2.25)

50 
STATISTICS FOR ECONOMICS
If the population variance is not known, replace it with the sample 
 variance:
 


2
ˆ
2
ˆ
ˆ
Var( )
n
m
m
s
m
s
=
=
 
(2.26)
where s2 is the population variance and 2
s  is the sample variance.
Deﬁ nition 2.2
Th e square root of the variance of the sample mean is called the standard 
deviation of the sample mean. It is also called the standard error. 
Error
In statistics, e rror is the amount each data point misses the expected 
value or the average. To avoid using error for two diff erent things, s, 
or  the  standard deviation, is called the error and the (X − m)2 / N is 
called the variance or MSE. Th e term “MSE” is usually used in situa-
tions where part of the variation in data can be explained by trend line, 
treatment, block eff ect, and so forth, and the remaining unexplained por-
tion is called MSE. Th e term “variance” is more commonly used for the 
 population  variance, when no portion of it could be explained by other 
factors. 
Th e expected value is the parameter that represents the  population. Th e 
actual observations deviate from their mean due to random error. Th e ran-
dom error cannot be explained. In statistics, it is called the error. Th e error is 
the portion of the total variation that cannot be explained. Th e error is not 
necessarily a fi xed amount. It is the amount not explained by the given 
tool. Change the tool and the error might change.
Some Algebraic Relation s for Variance
Two important relations are used in dealing with variances and are worth 
reviewing. 

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
51
1. Th e variance of a constant is zero.
 
Var (C) = 0 
(2.27)
2. Th e variance of a constant multiple of a variable is equal to the square 
of the constant times the variance.
 
Var (CX  ) = C2 Var (X  ) 
(2.28)
Computational Formula  (Short-Cut)
Th e defi nitional formula for variance may result in lots of rounding 
off , especially for a large population or sample, and cause an erroneous 
 diff erence. If the mean is a never ending real number, the deviation 
will be a never ending real number. When this deviation is squared 
and added up, the small amount can add up and give a great bias. Th e 
computational formula(e) delay dividing the values as long as possible 
and do not introduce rounding off  into the computation until the last 
stages.
Th e computational formula for the population variance is
 
s
∑
∑
−
=
2
2
2
(
)
X
X
N
N
 
(2.29)
Th e derivation of the computational formula is relatively simple. It 
is important to point out that the letters X, N, and so on are dummy 
 notation and are used to represent a variable. Sometimes other letters, 
such as Y and M, might be used instead. Th e concept is the same, only the 
notation is diff erent. Th erefore the variance can also be written as:
 
s
∑
∑
−
=
2
2
2
(
)
Y
Y
N
N
 
(2.30)
Another computational formula delays division another step.
 
s
∑
−∑
=
2
2
2
2
(
)
N
X
X
N
 
(2.31)

52 
STATISTICS FOR ECONOMICS
Th e derivation of this formula is no more diffi  cult than the previous one. 
Th e corresponding computational formulas for the sample variance are:
 

s
∑
∑
−
=
−
2
2
2
(
)
,
1
X
X
n
n
 
(2.32)
this is the more commonly used formula in most texts
or 
 

s
∑
−∑
=
−
2
2
2
(
)
(
1)
n
X
X
n n
 
(2.33)
Example 2.15
Use the family income of gold miners from Example 2.1 and 
 Example 2.3 to calculate the sample variance for the incomes of gold 
miners using the computational formula. 
Income
X2 
 66
 4356
 58
 3364
 71
 5041
 73
 5329
 64
 4096
 70
 4900
 66
 4356
 55
 3025
 75
 5625
598
40092
Solution

2
2
2
2
598
(
)
40092
40092
39733.7778
9
1
8
8
358.2222
44.7777778
8
X
X
n
n
s
∑
−
∑
−
−
=
=
=
−
=
=
 
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
53
Or 

s
∑
−∑
×
−
=
=
=
−
×
2
2
2
(
)
9
40092
357604
44.7777778
(
1)
9
8
n
X
X
n n
 
In this example the choice of the formula did not make any diff erence 
to the accuracy of the results.
Average of Several Variances
Sometimes it is important to average several variances. Suppose two or more 
samples are taken from the same population and estimated (sample) variances 
are obtained. In order to gain a better estimate of the population variance, all 
the variances should be averaged. If the sample sizes are the same, a simple 
average will provide the desired mean. If sample sizes are diff erent, however, 
the observations, which in this case are the variances, should be weighted. 
Th e logical weight is the sample size, but since we are dealing with variances 
and unbiased estimates of population variance come from sample variance, 
which uses the degrees of freedom as the divisor, the weights to obtain the 
weighted mean of several sample variances are the degrees of freedom associ-
ated with each sample variance. Th is weighted mean of variances is usually 
called “pooled” variance. Recall that the formula for weighted mean is:
 
1
1
2
2
1
2
N
N
N
w X
w X
w X
w
w
w
m
+
+
+
=
+
+
+


 
(2.34)
Since we are dealing with the variances, and the customary name for the 
weighted average of variances is called “pooled variance,” and since we use 
S 2 for sample variance we will use the commonly used symbol s2
Pooled. 
Th e weights (w1, w2, …, wn) are degrees of freedom, and the Xs are sample 
variances,  

s
s
s
…
1
2
2
2
2
,
,
n . Let n1, n2,… nn be sample sizes. Th e formula for 
the case of two sample (estimated) variances is:
 

(
) 
(
) 
(
) (
)
s
s
s
−
+
−
−
−
=
+
2
2
2
1
1
2
2
1
2
1
1
1
1
Pooled
n
n
n
n
 
(2.35)
Repeat the pattern to average three or more sample variances.

54 
STATISTICS FOR ECONOMICS
Example 2.16
Calculate the weighted variance for the variances of the Microsoft 
stock prices between March 12 and March 30, 2010, and April 2 and 
April 21, 2012. 
Solution
We already have the following results:

s =
2
1
0.10941  

s =
2
2
0.372182
(
) 
(
) 
(
)
(
)
2
2
1
2
2
1
2
1
2
1
1
2
15
1 0.10941
15
1 0.372182
15
15
2
1.5317
5.210554
2
0.24079
8
8
Pooled
n
n
n
n
s
s
s
−
+
−
=
+
−
−
+
−
=
+
−
+
=
=
 
In this and similar examples when the sample sizes are the same the 
use of weighted average and simple arithmetic average will be the same. 
Pooled variances are used in the test of hypothesis of equality of two 
means, as will be seen in Chapter 7.
Variance of Data with Frequency
Statistics is the science of summarizing information. Th is goal is achieved 
at several stages. Th e frequency distribution is a tabular summary of data. 
Often, the data are only available in frequency distribution format. In 
those cases it is desirable to be able to fi nd the variance and other valuable 
parameters. Th e formula for the population variance for the frequency 
distribution is:
 
m
s
−
= ∑
∑
2
2
(
)
f X
f
 
(2.36)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
55
Th e formula for the sample variance for the frequency distribution is:
 

m
s
∑
−
=
∑
−
2
2
(
)
1
f X
f
 
(2.37)
where f represents the frequency of each variable. Th e limits of S for 
 population is N, while that of sample is n.
Th e computational formula for the population is:
 
s
∑
∑
−
∑
∑
=
2
2
2
(
)
f X
f X
f
f
 
(2.38)
Th e computational formula for the sample is:
 

s
∑
∑
−
=
∑
∑
−
2
2
2
(
)
1
f X
f X
f
f
 
(2.39)
Example 2.17
Use the family income of gold miners from Example 2.1 and  Example2.3 
that to calculate the sample variance for the fi rst  community using the 
frequency table.
66, 58, 71, 73, 64, 70, 66, 55, 75
65, 57, 71, 72, 63, 71, 65, 55, 71
Solution
Remember from Examples 2.1 and 2.7 the sample variance for these 
18 men is equal to

 2
2
(
)
1
X
X
n
m
s
−
∑
−
=
 = 42.68382
Suppose the raw data is presented in the frequency distribution form.  
(Continued)

56 
STATISTICS FOR ECONOMICS
Observation
Frequency
55
 2
57
 1
58
 1
63
 1
64
 1
65
 2
66
 2
70
 1
71
 4
72
 1
73
 1
75
 1
Total
18
Note that there are 18 observations and not 12. Th e sample variance for 
the combined data is equal to 40.23529412. Th e calculation of the sam-
ple variance for the above data using the computational formula follows.
X
f
xf
X2
X2f
55
2
110
3025
6050
57
1
57
3249
3249
58
1
58
3364
3364
63
1
63
3969
3969
64
1
64
4096
4096
65
2
130
4225
8450
66
2
132
4356
8712
70
1
70
4900
4900
71
4
284
5041
20164
72
1
72
5184
5184
73
1
73
5329
5329
75
1
75
5625
5625
18
1188
79092

s
−
−
=
=
=
=
2
2
(1188)
79092
79092
78408
684
18
40.23529
17
17
17
 
(Continued)

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
57
Th e answer is the same as the one obtained using raw data. Th erefore, 
summarizing the data into frequency distribution does not aff ect the vari-
ance, which will be addressed shortly. It does not aff ect the mean of the 
data arranged in a frequency distribution form either. Th e variance of the 
grouped data, however, will most likely be diff erent than the actual vari-
ance. Th is is due partly to the fact that the class sizes are arbitrary, and 
mostly due to the formula.
Variance for Grouped Data
Th e formula for the variance of the grouped data for the population is:
 
2
2
2
(
)
f M
fM
f
f
s
∑
∑
−
∑
∑
=
 
(2.40)
where M is the midpoint of each class. Th e formula for the variance of the 
grouped data for the sample is:
 

s
∑
∑
−
=
∑
∑
−
2
2
2
(
)
1
fM
fM
f
f
 
(2.41)
where M is the midpoint of each class. Th e main diff erence between the two 
computational formulas is the denominator and the range of  summation. 
Th e sum for the population covers the members of the  population, N. 
Th e sum for the sample covers the members of the  sample, n.
Measures of Associations
Th e measures of association determine the association between two 
 variables or the degree of association between two variables.
Covariance
Population Covariance
Th e covariance is a measure of association between two variables. Let the 
variables be X and Y and their corresponding means be mX and mY . Th e 
covariance is defi ned as:

58 
STATISTICS FOR ECONOMICS
 
Cov( ,
(
)
)
)(
X
Y
X
X Y
Y
N
m
m
∑
−
−
=
 
(2.42)
Th e covariance is the sum of the cross product of the deviations of the 
values of X and Y from their means divided by the population size. 
 Sometimes it is written as sXY for the population covariance. Th is is not 
a standard deviation—it is the notation that refl ects that the covariance 
has a sum of the cross products term. Th is compares to the notation of 
s 2
X  for the variance of the population and sX for the standard deviation 
of the population. 
Th e defi nitional formula for covariance suff ers from the roundoff  
error and also can become very tedious if the means have long decimal 
places. Th e computational formula for covariance is:
 
s
∑
∑
∑
−
=
(
)(
)
XY
X
Y
XY
N
N
 
(2.43)
Th e derivation of the computational formula is not diffi  cult.
Sample Covariance
In the sample covariance the population means are not known and have 
to be replaced by the sample means. Consequently the covariance loses a 
degree of freedom. Th e theoretical formula for the sample mean is:
 



)
)
(
(
1
X
Y
XY
X
X
n
m
m
s
∑
−
−
=
−
 
(2.44)
Th e computational formula for the sample covariance is:
 

s
∑
∑
∑
−
=
−
(
)
1
)(
XY
X
Y
n
n
XY
 
(2.45)
Th e derivation of Equation 2.45 is identical to the derivation of 
 Equation 2.43. Th e covariance shows association between two variables. 
Th e magnitude of the covariance is a function of the degree of association 
as well as the units of measurement of the values. Th e size of covariance 

 NUMERICAL DESCRIPTIVE STATISTICS FOR QUANTITATIVE VARIABLES 
59
will change if the units of measurement are changed, for example from 
feet to inches or to yards. 
Correlation Coefﬁ cient
Th e correlation coeffi  cient r uses the measures of association and disper-
sion to provide a measure without a unit. Th e measure of association is 
the covariance and is placed on the numerator. Th e measures of disper-
sion are the standard deviation of the X and the standard deviation of the 
Y, which are placed in the denominator. All three are subject to change 
when the unit of measurement changes, but the correlation coeffi  cient is 
immune. 
 
s
r
s s
=
XY
X
Y
 
(2.46)
where sXY is the covariance, sX is the standard deviation of the X values, 
and sY is the standard deviation of the Y values. 
Th e sample correlation coeffi  cient r is written as
 

 
s
r
s s
=
ˆ
XY
X
Y
 
(2.47)
Substituting the formulae for the population covariance and the standard 
deviations will yield:
 
2
2
(
(
)(
)
(
)
)
X
X
Y
Y
X
Y
N
N
X
Y
N
m
m
m
r
m
=
∑
−
∑
−
−
∑
−
, 
(2.48)
Th e corresponding formula for the sample correlation coeffi  cient is:
 




r
m
m
m
m
=
∑
−
∑
−
∑
−
−
−
−
−
2
2
ˆ
(
(
)(
)
(
)
1
1
)
1
X
Y
Y
X
X
Y
X
n
n
Y
n
 
(2.49)

60 
STATISTICS FOR ECONOMICS
Th e corresponding computational form for the population and the 
 sample are given in Equations 2.50 and 2.51 respectively.
 
2
2
2
2
(
)
(
)
(
)(
)
X
Y
X
Y
N
N
X
Y
XY
N
r =
∑
∑
∑
−
−
∑
−
∑
∑
∑
 
(2.50)
 
=
∑
∑
∑
∑
−
−
∑
∑
−
−
∑
2
2
2
2
(
)
(
(
)(
)
1
)
X
Y
X
X
Y
n
n
Y
n
XY
n
 
(2.51)
Notice that the only diff erence between the population and the sample 
correlation coeffi  cient in the computation formulas is the use of N instead 
of n. Each is the size of its corresponding data. Th e diff erence is concep-
tual and refl ects their origin. Do not overlook the fact that r is a param-
eter and a constant, while r is the statistics and a variable. Th e sample 
correlation coeffi  cient r is used to estimate and draw inference about the 
population correlation coeffi  cient r.
 

CHAPTER 3
Some Applications of 
 Descriptive Statistics
Introduction
Th e descriptive statistics that were covered in Chapters 1 and 2  provide 
summary statistics and graphical methods to present data in a more 
 concise and meaningful way. Although those measures and methods are 
useful in their own right as demonstrated in the previous chapters, they 
are also used to further create more powerful statistical  measures: some of 
which are discussed in this chapter. Later, in Chapter 7, these measures 
are utilized to provide statistical inference, which is the  foundation for 
testing hypotheses in every branch of  science. 
Coefﬁ cient of Variation
Th e coeffi  cient of variation is the ratio of the standard deviation to the 
mean. In other words, the coeffi  cient of variation expresses the standard 
deviation (the average error) as a percentage of the average of the popu-
lation or sample. It is a relative measure of dispersion. It measures the 
standard deviation in terms of the mean. 
 
CV
s
m
=
 
(3.1)
Th e coeffi  cient of variation is independent of the units of measurement 
of the variables. If two populations have the same standard deviation, the 
one with the lower coeffi  cient of variation has less variation.

62 
STATISTICS FOR ECONOMICS
Example 3.1
Th e manager of the mortgage department in a local bank has  gathered 
the amounts for approved second mortgage loans for every 100th 
 customer. Th e amounts are in dollars.
5,672 
6,578 
9,700 
12,000 
9,000 
6,350
4,495 
6,900 
7,835 
8,750 
10,000 
12,000
6,500 
7,200 
8,000 
18,000 
19,000 
12,000
4,560 
1,500 
5,900 
5,450 
6,500 
1,800
1,900 
10,500
Calculate the coeffi  cient of variation.
Solution
4257.58
CV
0.532
8003.46
s
m
=
=
=
Coeffi  cient of variation represents the average error as a fraction of 
the expected value. Th e coeffi  cient of variation is useful in  comparing 
data with diff erent magnitudes. Assume that two stocks are rated 
 similarly where they have the same characteristics such as objectives 
and the amount and frequency of dividends. In order to compare the 
relative variability of the average price of the two stocks, we use 
the coeffi  cient of variation. Relative to the mean price, the stock with 
the lower coeffi  cient of variation indicates lower variation and hence 
lower risk.
Th e coeffi  cient of variation is also useful in the comparison of 
 unrelated data, especially when the unit of measurement is diff erent. For 
example, if the effi  ciency of a gas-powered lawn mower is compared to the 
reliability of an electric edger, then the machine with the lower coeffi  cient 
of variation is more reliable.
Th e most eff ective use of the coeffi  cient of variation is in the 
 comparison of two diff erent experiments by fi nding the ratios of 
their  respective coeffi  cients of variation, as is seen in the following 
example.

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
63
Example 3.2
Refer to the stock prices for Wal-Mart and Microsoft from  Example 2.2. 
Determine which one is riskier using the data for March 2012.
Table 3.1. Closing Prices of Wal-Mart (WMT) and Microsoft 
(MSFT) for the period from March 12 to March 30 and 
from April 2 to April 21, 2012 
Date
WMT
MSFT 
Date
WMT
MSFT
12 Mar.
$60.68 
$32.04
2 Apr.
$61.36
$32.29
13 Mar.
$61.00 
$32.67
3 Apr.
$60.65
$31.94
14 Mar.
$61.08 
$32.77
4 Apr.
$60.26
$31.21
15 Mar.
$61.23 
$32.85
5 Apr.
$60.67
$31.52
16 Mar.
$60.84 
$32.60
9 Apr.
$60.13
$31.10
19 Mar.
$60.74 
$32.20
10 Apr.
$59.93
$30.47
20 Mar.
$60.60 
$31.99
11 Apr.
$59.80
$30.35
21 Mar.
$60.56 
$31.91
12 Apr.
$60.14
$30.98
22 Mar.
$60.65 
$32.00
13 Apr.
$59.77
$30.81
23 Mar.
$60.75 
$32.01
16 Apr.
$60.58
$31.08
26 Mar.
$61.20 
$32.59
17 Apr.
$61.87
$31.44
27 Mar.
$61.09 
$32.52
18 Apr.
$62.06
$31.14
28 Mar.
$61.19 
$32.19
19 Apr.
$61.75
$31.01
29 Mar.
$60.82 
$32.12
20 Apr.
$62.45
$32.42
30 Mar.
$61.20 
$32.26
21 Apr.
$59.54
$32.12
Solution
Let’s refer to Wal-Mart stock as “1” and to the Microsoft stock as 
“2.” Recall that we calculated the means and standard deviations for 
these stocks. Th e following information is available for the two stocks. 
Which one is relatively less risky and thus better?

1
60.91
m =
  

1
0.2428
s =

2
32.31
m =
  

2
0.3191
s =
1
0.2428
CV
0.0040
60.91
=
=
(Continued )

64 
STATISTICS FOR ECONOMICS
2
0.3191
CV
0.0099
32.31
=
=
1
2
CV
0.0040
0.4
CV
0.0099
=
=
Th erefore, stock 1, Wal-Mart, is less risky than stock 2, Microsoft.
Z Score
Th e Z score is a useful and intuitive concept and, as it will become 
 evident, is used often in statistics. Th e Z score uses two of the more com-
mon parameters, the mean and the standard deviation. Th e problem of 
accurate and consistent measurement has been a diffi  cult subject through-
out history. Th e yardstick diff ered from one time to another and across 
diff erent locations and cultures. Diff erent countries and rulers tried to 
unify the unit of measurement. Th e closest unit to become universally 
accepted is the meter. Even the metric system is not commonly used in 
all quarters in spite of its ease and applicability. Th e metric system has 
its limitations too. One problem is the diff erence in scale. Th e following 
example demonstrates the problem.
County fairs have farming contests and they give prizes for the “best” 
in diff erent categories. For example, the farmer with the biggest produce 
receives a prize. But by nature, even the largest apple on record is not a 
match for any pumpkin. Would it be fair to compare the amount of a 
cow’s milk with that of a goat? In economic terms, how can we compare 
the output of the most productive small manufacturer to that of a larger 
one, which is fully automated?
In statistics, everything is measured in relative terms. It makes no sense 
to compare the weight of a peach to that of a pumpkin, but  comparing 
their relative weights makes perfect sense. Let’s have a peach that weighs 
8.4 ounces and a pumpkin that weighs 274.9 ounces. Although the pump-
kin is actually heavier, that might not be the case when other factors are 
considered. One factor is the average weight of peaches and pumpkins. 
A typical peach is about 6 ounces, while a typical pumpkin is 22 pounds, 
(Continued )

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
65
or 352 ounces. Th e peach in this example is somewhat heavier than an 
average peach while the pumpkin is actually lighter than an average pump-
kin. Th erefore, relatively speaking, the peach is heavier than the pumpkin. 
However, this is not enough. We also need to divide the distance from the 
mean for each produce by its typical or average error. Let’s assume that 
the standard deviation for peaches is 1.15 ounces. Th erefore, the amount 
that the peach exceeds its average as measured by its own yardstick is 
(8.4 − 6) / 1.15 = 2.086 standard deviation. Let’s assume that the standard 
deviation for the pumpkin is 2.57 pounds or 16 × 2.57 = 41.12 ounces. 
Th erefore, the pumpkin is (274.9 − 352) / 41.12 = −1.875 standard devia-
tions below its expected weight. Th erefore, the pumpkin is  actually sub-
par, while the peach is a prize winner. Th is is the essence of what is called a 
Z score and the procedure is known as standardization. 
Th e Z score is defi ned as follows:
 
Observed
Expected
Standard Deviation of the Observed
Z
−
=
 
(3.2)
 
X
Z
m
s
−
=
 
(3.3)
Th e expected value of an observation is its mean or (m), its standard 
deviation is (s). To obtain the Z score of an observed value, subtract its 
mean from its observed value and divide the result by the standard devia-
tion of the observation. 
Th e distance of an observation from its expected value is also called 
its error. Some of the diff erent aspects of error will be discussed later in 
this chapter. Th e Z score is a scaled error. Th e unit of measurement of 
Z score is the standard deviation of the population or its sample estimate. 
 Obtaining the Z score of an observation is also known as  standardizing 
the value. Th e standardization process can be applied to any data or 
observation. If the item under consideration is a single observation from 
the population, the result is called the Z score.
X
Z
m
s
−
=

66 
STATISTICS FOR ECONOMICS
Suppose two students are given the task of measuring the error of 
an observation. Th e fi rst student found that the observation is 41.6666 
feet. In fact, the decimal place is never-ending. Disliking decimal places 
and especially the never-ending ones, he measured the deviation of the 
data point from the mean in inches and was relieved to fi nd out that 
it has no decimal place. He presented his error of the observation as 
500 inches. Th e second student, using an electronic measuring “tape,” 
subtracted the observation from the mean and reported 0.007891414 
miles as the error. While the fi rst error seems large and the second 
seems small, both are the same. Note that 500 inches is 41.6666667 
feet or 0.007891414 miles. Z score provides a unique and comparable 
measure of error to avoid the confusion that may arise from changes 
in units of measurement. Every error is reported in the units of its 
own standard deviation. Since the Z score is reported in terms of the 
standard deviation, it allows comparison of unrelated data measured in 
diff erent units.
Z Score for a Sample Mean 
If the value under consideration is the sample mean, ˆm, the resulting 
Z score would be:
 
ˆ
ˆ
Z
m
m
m
s
−
=
 
(3.4)
where ˆm is the sample mean, m is its expected value, which is also the 
population mean, and the standard deviation of the sample is 
ˆ
.
n
m
s
s
=
 
Th e standard deviation of the sample mean is also known as the standard 
error. In order to calculate the Z score for a sample mean, it is necessary 
to know the population variance. Usually, we do not know the population 
variance either. When the population variance is unknown, we must use a 
t value instead. We will discuss t distribution in more detail in Chapter 4. 
For the time being, we will assume that we know the population variance. 
Th e most realistic assumption is to use the sample variance as if it were 
the population variance. 

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
67
Theorem 3.1 Chebyshev Theorem 
Th e proportion of observations falling within k standard deviations of the 
mean is at least
 
2
1
1
.
K
⎛
⎞
−
⎜
⎟
⎝
⎠ 
(3.5)
Th is is the same Z score concept. Th e theorem indicates that we need to 
fi nd the diff erence of a value from its mean, that is, (X − m). Since the 
theorem applies to all the values within k standard deviations, that is, ks 
on either side of the mean, the absolute value is desired. Th e theorem sets 
a minimum limit for the |X − m| < ks. Th erefore, the Chebyshev theorem 
states that:
 
(
)
2
1
|
|
1
P X
k
K
m
s
⎛
⎞
−
<
≥
−
⎜
⎟
⎝
⎠ 
(3.6)
But since s is a non-negative value, dividing both sides of the inequality 
|X − m| < ks by s will not change the sign. Th erefore,
2
1
1
X
P
k
K
m
s
−
⎛
⎞
⎛
⎞
<
≥
−
⎜
⎟
⎜
⎟
⎝
⎠
⎝
⎠
 
(
)
2
1
|
|
1
P
Z
k
K
⎛
⎞
<
≥
−
⎜
⎟
⎝
⎠
 
(3.7)
As is evident, the Z score is the core of the Chebyshev theorem.  Chebyshev 
was one of the major contributors to the Central Limit Th eorem, which 
will be discussed in Chapter 5.
Th e fi rst part of the equation {P(|Z| < k)} is the same as the confi dence 
interval of a range, which is covered in  Chapter 6. Th e concept of Z score 
is also used in statistical inference, which is  covered in Chapter 7.

68 
STATISTICS FOR ECONOMICS
Example 3.3
Determine what percentage of the Microsoft stock prices from 
March 12 to March 30, 2012, lay within the two standard deviations 
of their mean. Verify the correctness of the result by counting the 
prices that are within two standard deviations of the mean. Assume 
the population’s mean and variance are the same as the sample mean 
and variance, respectively. See Example 3.2 for data.
Solution
Th e sample mean and sample variance are given in Example 3.2 as:

2
32.31
m
m
=
=
  

2
0.3191
s
s
=
=
Note that we are using the notation for population mean and popula-
tion standard deviation to make sure that the theorem applies. Th e 
theorem requires the knowledge of population mean and standard 
deviation, and we are assuming them to be the values we obtained 
from the sample.
Insert the data in Equation 3.6: 
(
)
2
1
|
|
1
P
X
k
K
m
s
⎛
⎞
−
<
≥
−
⎜
⎟
⎝
⎠ 
P(|X
32.31|
2 (0.3191)
1
4
)
1
⎛
⎞
−
<
≥⎜⎝
−
⎟⎠
P(|X − 32.31| < 0.6382) ≥ 0.75
Th erefore, at least 75% of the 15 observations will be within two 
standard deviations of the mean. Next we calculate the range “within 
2 standard deviations.”
32.31 − 0.6382 = $31.67
32.31 + 0.6382 = $32.95
Th e easiest way to verify that the result is valid is to sort the data. You 
will notice that the lowest price is $31.91 and the highest price is 
$32.85. Th erefore, 100% of the data are within two standard deviations 
of the mean, which exceeds the predicted minimal percentage of 75%.

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
69
Standardization
In order to be able to compare diff erent objects with diff erent scales we 
need a tool that places everything in a unifi ed perspective. A value can be 
standardized by fi nding its distance from the population mean, that is, 
its individual error, and then scale the result or put it in perspective with 
respect to its standard deviation. 
 
Observed
Expected
Standard Deviation of the Observed
Z
−
=
 
(3.8)
Th e unit of measurement is in standard deviation.
If a data point is 72 and its expected value and standard deviation are 
70 and 4.3, respectively, then the estimated error is ∈ˆ = 72 − 70 = 2. Its 
standardized value is:
Standardized va
2
0.465
4.3
lue
=
=
Th e data point is said to be 0.465 standard deviations to the right of its 
mean. If an ordinary data point is selected and its standardized value is 
calculated, then that value is called a Z score.
 
X
Z
m
s
−
=
 
(3.9)
Th erefore, the two statements “the data point is 0.465 standard devia-
tions above its mean” and “the Z score for the data point is 0.465” mean 
the same. Had the Z score been negative, then the data point would have 
been on the left side of its mean. If a random variable is from a popula-
tion with known mean and variance, then its standardized value is called 
a Z score.
Example 3.4
Calculate the Z scores for closing prices of Microsoft stock from March 
12 to March 30. Assume that the population’s mean and variance are 
equal to the sample mean and variance, respectively. 
(Continued)

70 
STATISTICS FOR ECONOMICS
Solution
Use Equation 3.9 and the following values from Example 3.2:

2
32.31
m
m
=
=
  

2
0.3191
s
s
=
=
Note that we are using the notation for population mean and popula-
tion standard deviation to make sure that the theorem applies. Th e 
theorem requires the knowledge of population mean and standard 
deviation, and we are assuming that we know them to be the values we 
obtained from the sample.
Date
MSFT 
Z Score
12 Mar.
$32.04
−0.86
13 Mar.
$32.67
1.11
14 Mar.
$32.77
1.43
15 Mar.
$32.85
1.68
16 Mar.
$32.60
0.89
19 Mar.
$32.20
−0.36
20 Mar.
$31.99
−1.02
21 Mar.
$31.91
−1.27
22 Mar.
$32.00
−0.99
23 Mar.
$32.01
−0.95
26 Mar.
$32.59
0.86
27 Mar.
$32.52
0.64
28 Mar.
$32.19
−0.39
29 Mar.
$32.12
−0.61
30 Mar.
$32.26
−0.17
0.00
Th e sum of the Z scores is provided at the bottom, which is zero. Th is 
is a mathematical property of individual errors, which add up to zero. 
See Defi nition 3.5.
(Continued)

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
71
Correlation Coefﬁ cient Is the Average 
of the  Product of Z Scores
Correlation coeffi  cient was introduced in Chapter 2. It measures the 
degree of association between two variables.
(
)
m
m
m
m
s
s
s
r
s s
s s
⎛
⎞
−
−
∑
−
−
∑
×
⎜
⎟
⎝
⎠
=
=
=
= ∑
(
)
(
)(
)
Y
X
X
Y
X
Y
XY
X
Y
X
Y
X
Y
Y
X
Y
N
N
X
Z
N
Z
Th e above derivation depends on the defi nition of a parameter as a 
constant. Th is allows moving parameters, such as standard deviations, 
into a summation notation. Note that anything that is added and divided 
by the number of observations is an average number. In this case, the 
product of two Z scores (ZX ZY) is added and divided by N. Hence, cor-
relation coeffi  cient is the average of the product of two Z scores. 
Standard Error
When data is obtained from a sample, the standard deviation of the esti-
mated sample statistics is called a standard e rror. Th is concept will be 
addressed in detail when the sampling distribution of the sample mean 
is discussed in Chapter 5. As the distributional properties of sample 
standard deviation are diff erent than that of the population standard 
deviation, we had to assume that the standard deviation obtained from 
the samples in Examples 3.2 and 3.4 were known to be the same as the 
population standard deviation.
ˆ
ˆ
2
Standard Error
m
m
s
s
=
=
Usually, the standard deviation of the sample mean is also unknown 
and has to be estimated, which is represented with a hat. 

2
ˆ
ˆ
Sample Standard Error
m
m
s
s
=
=
Th erefore, the correct representation of the t statistics from a sample is:
 
ˆ
ˆ
t
m
m
m
s
−
=
 
(3.10)

72 
STATISTICS FOR ECONOMICS
Note that Equation 3.10 is called a t and not a Z as in Equation 3.4. Th e 
reason for the diff erence in names is due to the diff erence in the denomi-
nators of the two equations. Gosset1 showed that if the population vari-
ance is unknown and has to be estimated by the sample variance, then the 
resulting standardization does not have a normal distribution and does in 
fact have a student t distribution. 
Deﬁ nition 3.1
Th   e Degree of Freedom is the number of elements that can be chosen 
freely in a sample. Th e degree of freedom only applies to a sample. Th e 
population parameters are constant values and are estimated by sample 
statistics.
Example 3.5
Let us have a small population, say, sized fi ve. For example, consider a 
family with fi ve children of ages 3, 5, 7, 8, and 9. Let us take samples 
without replacement of size three from this population. Th ere will be 10 
diff erent possible samples. Th e population mean is 32/5 = 6.4, that is, 
the average age of the children is 6.4 years. Th e mean value governs the 
outcome of the average of the mean of the samples, as is demonstrated 
below. Th e 10 possible samples and their corresponding means are:
Sample 
Mean 
8, 3, 7
6
8, 3, 5
5.333
8, 3, 9
6.667
8, 7, 5
6.667
8, 7, 9
8
8, 5, 9
7.333
3, 7, 5
5
3, 7, 9
6.333
3, 5, 9
5.667
7. 5. 9
7
Total
64
(Continued)

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
73
Even though none of the sample means equals the population 
mean, the population mean has exerted its infl uence on the  sample 
means. Find the average of the 10 possible sample means. It is 
64 / 10 = 6.4. Th erefore, the expected value or the average of all the 
possible sample means is equal to the population mean. Even if we 
do not know the population mean, every population has a mean, 
and that mean will infl uence all the sample means. It is even pos-
sible to prove, mathematically, that the average of sample means is 
equal to the population mean. In mathematical statistics the average 
is formally defi ned as the expected value and is represented by an E, 
therefore, E ( ˆm) = m.
In the Example 3.5, this means that any 9 of these 10 possible samples 
can be chosen freely. After 9 samples and their means are obtained, the 
10th, or the last one, is forced to have a mean value such that the average 
of all the sample means equals the population mean. Let us assume that 
the fourth possible sample in the above list is the one that is not taken 
yet, and its mean has not been calculated. Th e mean of this sample has 
to be 64 − 57.222 = 6.778. Within this sample any two numbers can be 
selected at random, but the last one must be such a number that its aver-
age equals 6.778. Two of the three numbers 8, 7, and 5 can be randomly 
selected. Say 5 and 7 are selected. Th e last number must be 8, since this is 
the only number that will make the average of this sample equal to 6.778 
and the average of all the sample means equal to m = 6.4. In general, n − 1 
sample points can be selected at random, but the value of the remaining 
one will be determined automatically by the value of the population. One 
degree of freedom is lost for every parameter that is unknown and must 
be estimated by a statistics.
Computation of variance requires the knowledge of the population 
mean. 
2
2
(
)
X
N
m
s
∑
−
=

74 
STATISTICS FOR ECONOMICS
If the population mean m is not known, the value of s2 cannot be deter-
mined. If instead of m its estimate is obtained, then the sample mean, or 
ˆm is used. Th e sample variance then is:

2
2
ˆ
(
)
1
X
n
m
s
∑
=
−
−
and it will lose one degree of freedom. Th e result of the adjustment to 
the sample variance—that is, dividing by the degree of freedom, n − 1, 
instead of the sample size, n—is that the sample variance becomes an 
unbiased estimate of the population variance.2
Properties of Estimators
Sample statistics are used as estimators of the population parameters. 
Since sample statistics provide a single value, they are also called point 
estimates. It is desirable to be able to compare diff erent point estimates 
of the same parameters and provide useful properties. 
Let q, pronounced theta, be the population parameter of interest. Let 
its estimate be qˆ, pronounced theta hat. Like any other point estimate, 
qˆ is a sample statistic and a known variable. 
Deﬁ nition 3.2
If the expected value o f a point estimate equals the population parameter, 
then the estimate is unbiased. In symbols:
 
E(qˆ) = q 
(3.11)
It can be shown that the sample mean ( ˆm), variance (2
s ), and 
 proportion (ˆp) are all unbiased estimates of their corresponding popula-
tion parameters.
ˆ
E( )
m
m
=
 
2
2
E(
)
s
s
=
ˆ
E( )
p
p
=

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
75
Deﬁ nition 3.3 
Th e effi  ciency of a point estimator is said to be higher if it has a smaller 
variance. If qˆ
1 and qˆ
2 are two point estimates of q and Var (qˆ
1) < Var (qˆ
2) 
then qˆ
1 is more effi  cient than qˆ
2. For example, the sample mean is more 
effi  cient than the sample median in estimating the population mean.
Deﬁ nition 3.4 
A point estimate is a consistent estimator if its varian ce gets smaller as the 
sample size increases. Th e variance of the sample mean 

2
2
n
s
s =
will decrease as the sample size increases. As the sample size increases, 
the sample mean will have a smaller and smaller variance. And as it is an 
unbiased estimate of the population mean, it will get closer and closer to 
the population mean. 
Error
Statistics deals with random phenomena. For a set of values X1, X2, ..., Xn 
there is a representative or expected value (mean). Th e Greek letter m is 
used to represent the expected value. Th e diff erence of each value from 
the expected value, also called the deviation from the mean, represents an 
individual error. 
Deﬁ nition 3.5
An  individual error is the diff erence between the value of an observa-
tion and its expected value. Th e expected value or the mean is the best 
estimate or representative of a population and, hence, the sample. For 
example, the average starting salary for an economics major in 2011 
was $54,400. If a recent economics graduate is selected at random his 
or her expected income is $54,000 then the error associated with this 
 observation is $400. In other words, the observation missed the expected 

76 
STATISTICS FOR ECONOMICS
value by $400. Th e reason for calling the deviation an error is that we do 
not have any explanation for the deviation other than a random error. 
Th erefore, the error is what we cannot explain. Since observations vary 
at random, the errors vary at random as well. Furthermore, the portion 
that cannot be explained depends on the model or procedure used. Some-
times it is possible to explain part of variation of observations from their 
expected value by developing more sophisticated methods. Th e portions 
that can be explained by the new procedure are no longer “unexplained,” 
and, thus, not part of the error any more. Th e unexplainable portion is 
still called an error. Note that unless all the observations in a sample or 
population are identical, they will deviate from their expected value and, 
hence, have a random error. 
Since there are as many individual errors as there are observations, 
we need to summarize them into fewer variables. A popular and useful 
statistic is the average or the mean. However, the average of individual 
errors is always zero because the sum of all errors is zero. Recall that indi-
vidual errors are deviations from their expected values, some of which are 
negative and the others are positive. Th us, by defi nition, they cancel each 
other out and the sum of all deviations is always zero. Symmetric distribu-
tions have equal numbers of positive and negative individual errors, but 
this is not a necessary condition for their sum to add to zero. Th e sum 
of individual errors for non-symmetric distributions is also zero, in spite 
of the fact that the count of negative values is diff erent from the count of 
the positive values. Th is is due to the fact that the expected value or the 
mean is the same as the center of gravity of the data. Imagine data on a 
line where they are arranged from smallest to largest. Placing a pin at the 
point of the average will balance the line. 
Th ere are several ways to overcome individual errors canceling each 
other out. One way is to use the absolute value of the individual errors. 
Th e average of the absolute values of the individual errors is called mean 
absolute error (MAE). Th e mean absolute error is commonly used in 
time series analysis. One advantage of MAE is that it has the same unit of 
measurement as the actual observations.
Another way to prevent individual errors from canceling each other 
out is to square them before averaging them. We are already familiar with 
this concept, which is called a variance.

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
77
Deﬁ nition 3.6
Th e variance is the average of the sum of the squared individual errors.
One advantage of variance over the mean absolute error is that it 
squares errors, which gives more power to values that are further away 
from the expected value. Th is makes the variances disproportionately 
large as individual errors get larger. Th e variances use of the squared 
 values of the individual errors is its shortcoming as well because the 
unit of measurement for the variance is the square of the unit of 
 measurement of the observations. If the observations are about length 
in feet, then the variance will be in feet squared, which is the unit of 
measurement of an area and not length. Seldom, if ever, the squared 
values of economic phenomenon have any meaning. If the variable of 
interest is price, measured in dollars, then the unit of measurement 
of its variance is in dollars-squared, which has no economic  meaning. 
To  remedy this problem, it is necessary to take the square root of 
 variance.
Deﬁ nition 3.7
Th e standard deviation is the square root of the variance.
Deﬁ nition 3.8
Th e standard deviation is the average error.
How Close Is Close Enough?
If the sum of the residual is not zero, then c heck your formulas and com-
putations. If the defi nitional formulas are used or the results are rounded 
off  at early stages, the sum of individual errors may be diff erent than zero. 
Use of computational formulas and less round-off  will reduce or elimi-
nate this problem provided the sample size is suffi  ciently large. If you use 
fi ve decimal places, the fi nal result can be accurate to about four decimal 
places. If you have been using fi ve decimal places in your calculations and 
the sum of the residual is 0.00007, the property has not been violated. It 
is zero to four decimal places as expected.

78 
STATISTICS FOR ECONOMICS
Sum of Squares
Th e sum of squares of deviation of values from their expected value 
(mean) is a prominent component of statistics. As we saw previously, 
the square of these individual errors divided by the population size is 
called population variance. Th e concept is the same for a sample, except 
that the divisor is the degree of freedom. Earlier, it was explained that 
the portion of the phenomenon that cannot be explained is called an 
error, and that the variance is one way of representing it. When alterna-
tive models are used to explain part of the error, it is more meaning-
ful to focus on the numerator alone, at least at fi rst. Th e numerator, or 
variance, is also called total sum of squares (TSS). Once a set of data is 
collected then the TSS becomes fi xed and will not change. Th e TSS will 
only change if another sample from the same population was collected at 
random. Decomposition of TSS is very common in a branch of statistics 
called experimental design. In experimental design methodology TSS is 
decomposed into diff erent components based on the design. Th ese com-
ponents include treatment SS, block SS, main eff ect SS, and so forth. 
In all cases there is always a component that remains unexplained, and 
is referred to as residual or error SS. By defi nition, dividing this unex-
plained remainder by appropriate degrees of freedom would result in the 
variance of the experiment, customarily known as mean squared error 
(MSE) or mean squares error. As one would expect, the square root of 
MSE is called root MSE, which is the same as the standard error. Just 
as a reminder:
2
sum(observed
expected)
MSE
n
k
−
=
−
Note that the term in the parentheses is the individual error. Th e k in the 
denominator is the number of parameters in the model, and the entire 
denominator is the degrees of freedom.
Skewness
As you noticed in Chapter 2, several of the relationships between mean, 
mode, and median were described. Th e relationship between these 

 
SOME APPLICATIONS OF  DESCRIPTIVE STATISTICS 
79
parameters can also be used to determine if the data are symmetric and 
the extent of the pointedness of the data. Of course, the data cannot be 
fl at, pointed, symmetric, or skewed. Th e fact is that we are talking about 
the shapes of data when they are plotted. 
Deﬁ nition 3.9
Th e skewness refers to the extent that a graph of a distribution function 
deviates from symmetry. A distribution function that is not symmetric 
is either negatively skewed, as in Figure 3.1, or positively skewed, as in 
Figure 3.2.
Figure 3.1. Negatively skewed distribution.
Mean
Median Mode
Figure 3.2. Positively skewed distribution.
Mean
Median
Mode

80 
STATISTICS FOR ECONOMICS
Relation Between Mean, Median, and Mode
Th e mean, median, and mode of symmetrical distributions are identical. 
If the distribution is positively skewed, the order of magnitude is 
 
Mode < Median < Mean 
(3.12)
If the distribution is negatively skewed, the order of magnitude is
 
Mean < Median < Mode 
(3.13)
Th e importance of skewness is in its use to test if the data follows a normal 
distribution. Normal distribution function is discussed in Chapter 4.
Deﬁ nition 3.10
Th e Pearson coeffi  cient of skewness is defi ned as:
3(mean
median)
S
standard deviation
−
=
Th e range of skewness is −3 < S < 3, and for a symmetric distribution 
S = 0. Th e sign of the Pearson coeffi  cient of skewness determines if it is 
positively or negatively skewed. 
Deﬁ nition 3.11
Kurtosis is a measure of pointedness or fl atness of a symmetric distri-
bution. Th e exact defi nition of Kurtosis requires the knowledge of the 
moments, which is beyond the scope of this text. A positive Kurtosis 
indicates the distribution is more pointed than the normal distribution, 
and a negative value for Kurtosis indicates the distribution is fl atter than 
normal distribution. Kurtosis and skewness are commonly used to test 
whether a data set follows a normal distribution. 

CHAPTER 4
Distribution Functions
Deﬁ nitions and Some Concepts
Sometimes it is possible to represent random variables as a function in a 
way that the function can determine the probability of an outcome of the 
random variable. 
Deﬁ nition 4.1
Th e outcome of a random variable is determined by chance. Th e outcome 
of a random variable might be constant as in the case of observing a head 
when a coin is fl ipped, or it can be a variable value such as the length of time 
it takes for diff erent students to learn this chapter. Th e subject “statistics” 
is used to study the properties of random variables and how they behave. 
Deﬁ nition 4.2
Th e probability distribution determines the probability of the outcomes 
of a random variable. In its simplest form, the probability distribution 
consists of values and probabilities. Th e probability distribution for fl ip-
ping a coin is:
Head with probability 1/2
( )
Tail with probability 1/2
f x
⎧
= ⎨
⎩
Th ere are more formal ways to defi ne probability distribution that are 
beyond the scope of this text. A distribution function can be presented in the 
form of a function, a table, or a statement. Th e subject of probability distri-
bution is vast, but we will focus on the few items needed to continue our dis-
cussion. Th ere are two types of random variables, discrete and continuous.

82 
STATISTICS FOR ECONOMICS
Deﬁ nition 4.3
A discrete random variable consists of integers only.
Deﬁ nition 4.4
A continuous random variable can take any value over a range.
Deﬁ nition 4.5
Th e probability distribution for a discrete random variable is called a 
discrete probability distribution and is represented as f (x).
Deﬁ nition 4.6
Th e probability distribution for a continuous random variable is called 
a probability density function.
In this text we may use distribution function for both discrete and 
continuous random variable as a matter of convenience. 
Continuous Distribution Functions
Th ere are numerous distribution functions, both discrete and continuous. 
In this text, for the sake of space only, the distribution functions that are 
used to provide inference are discussed. 
Normal Distribution Function
Normal distribution functions are the most important and most widely 
used distribution function. Standardized normal distribution functions, 
called standard normal, can be applied in any area and situation, provided 
that one is assured of the normality and either knows, or can estimate, 
the mean and variance of the distribution. Standard normal values are 
the Z scores of values that have a normal distribution. Converting values 
from diff erent normal distributions with diff erent means and variances to 
standard normal allows us to compare them. Furthermore, any standard-
ized value can be compared with the theoretical standard normal table. 

 
DISTRIBUTION FUNCTIONS 
83
Since normal distribution is a continuous distribution function, and 
there are infi nitely many points on any continuous interval, the probability 
of any single point is always zero. For continuous distribution functions, 
such as normal distribution, the probability is calculated for an interval. 
Direct computation of such probabilities requires integral calculus. For 
simplicity, a table of standard normal values is used. Spreadsheets such 
as Microsoft Excel are capable of computing the standard normal values. 
A table of values for normal distribution is provided in the Appendix. 
Properties of Normal Distribution
A normal distribution is depicted in Figure 4.1. Th e normal distribution 
curve is unimodal and symmetric. Consequently, its mean, mode, and 
median are all the same and fall in the middle of the curve. You may notice 
that the tails of the normal curve do not touch the x-axis. Th is is another 
property of the normal distribution. Th e x-axis is actually an asymptote 
of the functions, which means the curve does not touch the axis even at 
infi nity. Th is implies that the tails are stretched to infi nity, but in practice, 
we do not stretch the tails that far. As you will soon see, the probability 
of the tail areas becomes very negligible not too far from the center, mak-
ing it unnecessary to be concerned with infi nity. A normal distribution 
has two parameters: its mean and variance. In other words, the mean and 
variance of a normal distribution determine its specifi c center and spread. 
Since distribution is commonly used and has so many applications, it has 
become known as normal distribution. Furthermore, the mean and vari-
ance of the normal distribution are represented by m and s 2, respectively.
Th e area under the normal curve is equal to 1, as is the area under 
any distribution density function. Customarily, the distance from the 
center of normal distribution is measured by its standard deviation. If two 
m + s
m
Figure 4.1. Normal distribution with mean = m and variance = s 2.

84 
STATISTICS FOR ECONOMICS
normal distribution functions have the same mean and variance then the 
area under the curve for two points that are the same distance from the 
center are equal. By converting any normal distribution to one with a mean 
of 0 and a standard deviation of 1, we are able to calculate the area under the 
curve between the center and any point for any normal distribution regard-
less of its mean and variance. Th e area under normal distribution represents 
the probability between any two points on the curve. We will soon see that 
the probability for a point to be within one standard deviation from the 
mean of a normal distribution function is about 68%. Since the normal 
distribution is symmetric, it is possible to calculate area from the center to a 
point on one side of the center, and the probability for a point of the same 
distance from the mean on the other side, which will be equal. 
Standardizing Values from a Normal Distribution
In Chapter 3 we discussed Z score, which can be used to compare dis-
similar things by converting them to the same basis, which is by obtain-
ing their deviations from their expected values and scaling them by their 
standard deviation. Th e process is also called standardization. Standard-
ized values are comparable. Th e same is true if we standardize the values 
of a normal distribution. Note that the mean of normal distribution is m 
and its variance is s 2. As a result of standardization of a normal distribu-
tion, one obtains a normal distribution for which the mean is 0 and the 
variance is 1. We show this as N(0, 1). Since the area under any normal 
distribution is equal to 1 regardless of the value of its mean or variance, 
the area under N(0, 1) is also equal to 1. Th is makes it possible to create 
a single table for N(0, 1) and generalize to any other normal distributions 
with diff erent means and variances. Th e graph for N(0, 1) is exactly the 
same as the graph in Figure 4.1. Th e one depicted below in Figure 4.2 has 
1
0
Figure 4.2. The area under the normal distribution between 0 and 1.

 
DISTRIBUTION FUNCTIONS 
85
the added feature that marks the point which is one standard deviation to 
the right of the center. 
Area Under a Normal Distribution with Mean Zero 
and Variance One
To calculate area under a normal distribution, we can use Table A1 
from the Appendix. Th is table is similar to most other normal tables 
except it is more accurate because it graduates at half of the custom-
ary steps of most normal tables. To read the value of 1.0, go to the left 
margin of the table and identify the row marked 1.0. Th en identify the 
column marked 0. Th e value at their intersection is 0.3413, which is 
the probability corresponding to the shaded area between 0, the mean, 
and point 1, which is one standard deviation away from the mean to 
the right. 
In the above section we stated that about 68%, or a little more than 
2/3 of all observations, are within one standard deviation of the center. 
Since the points on each side of the center are the mirror images of 
each other, the probability of being one standard deviation below the 
mean is the same as the probability of being one standard deviation 
above the mean, and hence equal to 0.3413. Th erefore, the probabil-
ity of being within one standard deviation of the mean is double the 
amount of 0.6826, or about 68%. Computations of other values are 
similar.
P (0 < Z < 1.49) = 0.4319
P (0 < Z < 2.63) = 0.4957
P (−1.52 < Z < 0) = 0.4357
For other variations, it is best to graph the area and perform simple 
algebra to obtain the results.
P (−1.52 < Z < 1.49) = P (−1.52 < Z < 0) + P (0 < Z < 1.49) 
        = 0.4357 + 0.4319 = 0.8676
See Figure 4.3 for clarifi cation.

86 
STATISTICS FOR ECONOMICS
See Figure 4.4 for fi nding the shaded area for the last example. Make 
sure you put the point further away from the center fi rst, because of rules 
of geometry, and this will guarantee that you would not obtain a negative 
probability, which does not make any sense. 
Obtaining Probability Values for Normal Distribution with Excel
Th e above examples can be obtained by using the following command 
in Excel.
=NORMDIST(1.49,0,1,1) − 0.5 = 0.43188
Th e formula consists of:
=NORMDIST(X, mean, standard deviation, True)
where “true” represents the “cumulative” value. In this case the entire area 
under the curve consists of area to the left of the mean, which is equal to 
Figure 4.3. Area under normal distribution between −1.52 and 1.49.
0.4357
0.4319
–1.52
1.49
0
Figure 4.4. Area under normal distribution between −1.69 and −1.58.
–1.69
–1.58
0
P (−1.52 < Z < −1.49) = P (−1.52 < Z < 0) + P (−1.49 < Z < 0) 
    = 0.4357 − 0.4319 = 0.0038

 
DISTRIBUTION FUNCTIONS 
87
half the entire area or 0.5. Th at is why we are subtracting 0.5 to obtain the 
area from the mean to point 1.49.
Alternatively, point to the arrow on the side of the “Σ AutoSum” 
on Excel’s command ribbon and choose “more function.” From the 
Figure 4.5. Pull-down window for Excel functions.
Figure 4.6. Excel window for normal distribution.

88 
STATISTICS FOR ECONOMICS
 drop-down menu choose “Statistical” in the window labeled “or select a 
category” and scroll down until you get to NORMDIST command.
Th en scroll down until you see NORMDIST. Upon selecting the 
option, a new window opens up as shown in Figure 4.6.
Place the appropriate values in the correct boxes and press OK. If 
you look at the resulting cell, you will see the formula shown previously, 
except for 0.5, which we added to convert the result to a value between 
zero and a Z value. When using Excel, write the problem as in the forms 
shown previously, mark the area under the curve, and then decide if you 
are subtracting 0.5, adding probabilities, or subtracting them based on 
the shaded area. 
Area Under a Normal Distribution with any Mean and Variance
Convert all normal distributions to the standard normal by standardizing 
all values.
1. Rewrite the question using probability notations.
2. Convert the values into Z scores. Do not forget the probability.
3. Draw a graph and shade the area under investigation.
4. Look up the probability or use Excel to obtain probability.
When fi nding the area between two Z values, if Z values are on two 
sides of zero, that is, one is positive and the other is negative, fi nd the area 
between 0 and each Z value and add the probabilities. If the Z values are 
on the same side of 0, that is, either both are negative or both are positive, 
fi nd the area between 0 and each Z value and subtract the smaller prob-
ability from the larger one. 
Continuous distribution functions such as normal distribution share 
two concepts: (1) length and (2) area. Th e Z score is a measure of length. 
It shows how far a point is from the center (mean) of standard normal in 
terms of standard deviations. In other words, Z score indicates the num-
ber of units a point deviates from the mean. Th e probability of a particu-
lar Z value from the mean is an area. Th e area between mean (zero) and a 
point is the value provided in the standard normal table.

 
DISTRIBUTION FUNCTIONS 
89
Example 4.1
Assume that the random variable X has a normal distribution with 
mean 16.9 and standard deviation of 3.01. Find
1. P (X < 22.51)
2. P (X > 11.3)
3. P (13.93 < X < 23.41)
4. P (11.43 < X < 15.61)
5. P (17.64 < X < 21.45)
Solution
Th e fi rst step is to standardize the values, both alphabetically and 
numerically.
1. 
22.51 16.9
(
22.51)
(0
1.86)
0.5
0.4689
(
1.86)
3
0.5
0.968
01
9
.
X
P Z
P X
P
P
Z
m
s
−
−
⎛
⎞
>
=
<
⎜
<
=
=
<
<
+
=
⎝
=
⎟⎠
+
Note that the area of interest is everything to the left of 1.86.
Alternatively, the Excel formula below will give the same answer:
 
=NORMDIST(22.51, 16.9, 3.01, 1) = 0.9689
2. 
(
11.3)
(
1.86)
( 1.86
0)
0.5
0.4689
0.5
0
11.3
16.9
.
3 1
9 89
.0
6
P X
P
P Z
P
Z
X
m
s
>
=
=
> −
−
−
⎛
⎞
>
⎜
⎟
⎝
⎠
=
−
<
<
+
=
+
=
Alternatively, the Excel formula below will give the same answer:
 
=NORMDIST(22.51, 16.9, 3.01, 1) = 0.9689
3. 
13.93
16.9
23.41 16.9
3.
(13.93
01
3.
23.41
0
)
1
P
X
X
P
m
s
−
−
−
⎛
⎞
<
<
=
<
⎜⎝
<
⎟⎠ 
= P (−0.99 < Z < 2.16) 
= P (−0.99 < Z < 0) + P (0 < z < 2.16) 
= 0.3389 + 0.4846 = 0.8235
(Continued)

90 
STATISTICS FOR ECONOMICS
4. 
11.43
16.9
15.61 16.9
3.
(11.43
01
3.
15.61
0
)
1
P
X
X
P
m
s
−
−
−
⎛
⎞
<
<
=
<
⎜⎝
<
⎟⎠ 
= P (−1.82 < Z < −0.43) 
= P (−1.82 < Z < 0) − P (0 < Z < −0.43) 
= 0.4656 − 0.1664 = 0.2992
5. 
17.64
16.9
21.45
16.9
3.
(17.64
01
3.
21.45
0
)
1
P
X
X
P
m
s
−
−
−
⎛
⎞
<
<
=
<
⎜⎝
<
⎟⎠ 
= P (0.25 < Z < 1.51) 
= P (0 < Z < 1.51) − P (0 < Z < 0.25) 
= 0.4345 − 0.0987 = 0.5332
(Continued)
Example 4.2
Let the random variable X have a normal distribution with mean 15 
and standard deviation of 3. 
1. What is the cut off  value for the top 1% of this population?
2. Find the interquartile range.
Solution
1. In this example, the probability of the outcome is given and the 
value that determines the desired probability is the objective. 
P (Z > z) = 0.01 
where the lowercase z represents a specifi c value. Search for the 
probability that corresponds to 0.5 − 0.01 = 0.49 in the body of 
the normal table. Th e Z score that corresponds to the probability 
of 0.49 is Z = 2.325. 
Next, determine the X value by reversing the computation of 
the Z score.
(Continued)

 
DISTRIBUTION FUNCTIONS 
91
(Continued)
1
2.32
5
5
3
X
X
Z
m
s
=
=
−
−
=
X = 21.975
Th erefore, 1% of the population has an X value greater than 
21.975. Note that 21.975 is the 99th percentile.
2. For the fi rst quartile:
P (Z < − z1) = 0.25
P (−z1 < Z < 0) = 0.5 − 0.25 = 0.25
Search the body of the table for 0.25. Th e closest number is 
0.2502 that corresponds to the Z score of z1 = −0.675. In the Z 
score  formula solve for the value of X.
15
0.675
3
X
X
Z
m
s
−
−
=
−
=
=
Th erefore X = 12.975 is the fi rst quartile.
For the third quartile:
P (Z < z3) = 0.75
P (0 < Z < z3) = 0.75 − 0.5 = 0.25
From the above z3 = +0.675 
15
0.675
3
X
X
Z
m
s
−
−
=
+
=
=
and hence the third quartile is:
X = 17.025
Th e middle 50% of the population lies between 12.975 and 
17.025.
(Continued)

92 
STATISTICS FOR ECONOMICS
Solution Using Excel
1. Note that in Excel the top 1% is entered as 0.99 for probability.
Point to the arrow on the side of the “Σ AutoSum” on Excel’s ribbon 
command and choose the “more function.” From the drop-down 
window choose “Statistical” in the window labeled “or select a 
category” and scroll down until you get to the NORMINV 
 command (see Figure 4.5). 
Figure 4.7. Excel window for inverse normal values.
Place the appropriate values in the correct boxes and press OK.
Th e result 21.97904 is displayed, which is slightly diff erent 
from the result from the table due to roundoff  error. You could 
have entered the following formula as well.
 = NORMINV(0.99,15,3) 
2. 
= NORMINV(0.25,15,3) = 12.97653
= NORMINV(0.75,15,3) = 17.02347
Again the results are slightly diff erent due to roundoff  error. 
Interestingly, both the above values and the values for the normal 
distribution table are obtained from Excel. 
(Continued)

 
DISTRIBUTION FUNCTIONS 
93
Nonconformity with Normal Distribution
Normality Versus Skewness
Normal distribution is the cornerstone of statistical analysis. Th e exact 
shape of the normal curve depends on the probability density function 
of the normal distribution. Any deviation from the normal  probability 
 density function results in skewness or Kurtosis. It is easy to detect 
skewness visually because skewed distributions are not symmetric. As 
a member of continuous symmetric distribution functions, the normal 
 distribution function has the property that its mean, mode, and median 
coincide. Th e relationships between these three measures were provided 
in Chapter 3 for positively and negatively skewed distributions. In the 
subsequent graph, the positive and negatively skewed functions are super-
imposed on the normal distribution for comparison.
Normality Versus Kurtosis
Kurtosis measures the degree of fl atness or pointedness of a symmetric 
curve as compared to a normal distribution. Th ere is a formal measure 
Figure 4.8. Comparison of negative skewness with normal distribution.
Mean Median Mode
Figure 4.9. Comparison of positive skewness with normal distribution.
Mean
Median
Mode

94 
STATISTICS FOR ECONOMICS
of Kurtosis but involves the concept of the moment of a function which 
is beyond the scope of this text. However, it is benefi cial to depict the 
graph of a fl atter curve, called Platykurtic, which corresponds to a Kurto-
sis measure with negative value. A more peaked curve, called Leptokurtic 
corresponds to a Kurtosis measure with a positive value. Figures 4.10 and 
4.11 represent comparisons to the normal curve.
Chi-Squared Distribution Function
Theorem 4.1
Let the random variable X have a normal distribution with mean and 
positive variance:
X ∼ N (m, s 2)
Th en, the random variable
2
2
(
)
X
V
m
s
−
=
Figure 4.11. Comparison of positive Kurtosis (Leptokurtic or pointed) 
with normal.
Figure 4.10. Comparison of negative Kurtosis (Platykurtic or ﬂ atter) 
with normal.

 
DISTRIBUTION FUNCTIONS 
95
will have a chi-squared (c2) distribution with one (1) degree of freedom.
V ∼ χ2 (1)
Note that Z
X
m s
=
−
 hence Z 2 is a χ2 (1). Th erefore, the normal 
 values can be squared to obtain probabilities for the chi-square distribu-
tion with 1 degree of freedom.
P (|Z| < 1.96) = 0.956
P (Z 2 < (1.96)2) = P (Z 2 = 3.842) = 0.95
Th is is identical to the chi-square value with one degree of freedom. 
Th erefore, in confi dence intervals and tests of hypotheses one can use 
either normal distribution or a chi-square distribution. Each one, how-
ever, would be benefi cial in diff erent settings.
Theorem 4.2
Let X1, X2, . . ., Xn be a random sample of size n from a distribution 
N (μ, s  2). Recall

2
2
ˆ
(
)
ˆ
and
1
x
x
n
n
m
m
s
∑
∑
−
=
=
−
Th erefore
 
i. 
2
ˆ and
m
s  are independent.
 ii. 
2
2
(
1)
n
s
s
−
is distributed as a chi-squared with (n − 1) degrees of 
freedom. 
2
2
2
(
1)
(
1)
n
n
s
c
s
−
−
∼
Th e chi-square distribution is a special case of a gamma distribution. 
In a gamma distribution with parameters α and θ, let α = r/2 and θ = 2 
where r is a positive integer. Th e resulting distribution function will be a 
chi-square distribution with r degrees of freedom. Th e mean will be r and 
the variance will be 2r. 

96 
STATISTICS FOR ECONOMICS
t Distribution Function
Theorem 4.3
Let X be a random variable that is N (0, 1), and let U be a random variable 
that is χ2(r). Assume Z and U are independent. Th en 
 
X
t
U
r
=
 
(4.1)
has a t distribution with r degrees of freedom. 
Theorem 4.4
Let X be a random variable that is N (m, s2). Use the customary hat- notation 
to represent sample mean and sample variance. Th en the  following rela-
tionship has a t distribution with (n − 1) degrees of  freedom.
 

(
1)
2
2
ˆ
(
)
(
1)
(
1)
n
n
t
t
n
n
m
m
s
s
s
−
−
=
−
−
∼
 
(4.2)
When population variance is not known, using normal distribution for 
inference is misleading. Th e problem is more acute when the sample size 
is small. 
F Distribution Function
Theore m 4.5
Let U and V  be independent chi-square variables with r1 and r2 degrees of 
freedom, respectively. Th en 
 
F =
1
2
U r
V r
 
(4.3)
has an F distribution with r1, r2 degrees of freedom.

 
DISTRIBUTION FUNCTIONS 
97
2
2
1
1,
U
r
r
r
r
V
F
∼
Th e relation between F and Z is that 
 
F = e2Z 
(4.4)
Th e F Statistics consists of the ratio of two variables, each with a 
chi-squared, c2,  distribution divid ed by their corresponding degrees of 
freedom.


CHAPTER 5
Sampling Distribution 
of Sample Statistics
Sampling
A sample is a subset of population that is collected in a variety of ways. Th e 
process of collecting samples is called sampling. As will become  evident 
in this chapter, random sampling is very important for establishing the 
necessary theories for statistical analysis. For example, if a fi rm has 500 
employees and 300 of them are men, then the probability of choosing a 
male worker at random is 300/500 = 0.60 or 60%. Sampling techniques 
are not limited to random sampling. Each sampling technique has its 
advantages and disadvantages. Th is text is not going to focus on various 
sampling techniques. After a brief discussion about sampling, the focus 
will be on the properties and advantages of random sampling.  Th eories 
that are necessary for performing statistical inferences and are related 
to sampling are discussed in this chapter. In Chapter 2, we explained 
 statistics and defi ned it as: 
Statistics is a numeric fact or summary obtained from a sample. It 
is always known, because it is calculated by the researcher, and it 
is a variable. Usually, statistics is used to make an inference about 
the corresponding population parameter. 
Th e only way to know the parameters of a population is to conduct 
a census. Conducting a census is expensive and, contrary to common 
belief, is not always accurate. Since a census takes time, it is possible that 
the fi ndings are already inaccurate by the time the massive information 
is obtained and verifi ed for mistakes and tabulated. On average, it takes 

100 
STATISTICS FOR ECONOMICS
about 2 years or more to release the census results in the United States. 
During this time, all variables of collected data change; for example, new 
babies are born, some people die, and others move. Sampling, on the 
other hand, can be performed in a shorter amount of time, while occa-
sionally the census information is sampled in order to release some of the 
results quicker. 
Sometimes, taking a census is not an option. Th is is not only due to 
the time and money involved, but also because the census itself might 
be destructive. For example, in order to fi nd out the average life of light 
bulbs, they must be turned on and left until they burn out. Barring 
 mistakes, this would provide the average life of the light bulbs, but then 
there will not be any light bulbs left. Similarly, determining if oranges 
were not destroyed by frost requires cutting them up. Other examples 
abound. Conversely, there are lots of other reasons where it is  unrealistic, 
if not impossible, to conduct a census to obtain information about a 
 population and its parameters.
Collecting sample data is neither inexpensive nor eff ortless. Sampling 
textbooks devote many pages of explanation on how to obtain random 
samples from a population. One simple, but not necessarily pragmatic or 
effi  cient way, is to assign ID numbers to all members of the population. 
Th en pull the desired number of sample points by drawing lots at random 
of the ID numbers. 
In this instance, our interest in sampling is very limited. We are inter-
ested in obtaining an estimate from a relatively small portion of a popula-
tion to obtain insight about its parameter. Th e knowledge of parameters 
allows meaningful analysis of the nature of the characteristics of interest 
in the population and is vital for making decisions about the population 
of the study.
As discussed earlier, summary values obtained from a sample are 
called statistics. Since statistics are variables, diff erent samples result in 
slightly diff erent outcomes and the values of statistics diff er, which is 
the consequence of being a random variable. It is possible to have many 
 samples and thus many sample statistics, for example, a mean. It turns 
out that the sample means have certain properties that are very useful. 
Th ese p roperties allow us to conduct statistical inference.

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
101
Deﬁ nition 5.1
Statistical inference is the method of using sample statistics to make 
conclusions about a population parameter. 
Statistical inference requires the population of interest to be defi ned 
clearly and exclusively. Furthermore, the sample must be random in 
order to allow every member of the population an equal chance to appear 
in the sample, and to be able to take advantage of statistical theories and 
methods. Later, we will cover the theories that demonstrate why random 
sampling provides suffi  cient justifi cation for making inferences about 
population parameters. 
Th e method of using information from a sample to make inference 
about a population is called inductive statistics. In inductive statistics, 
we observe specifi cs to make an inference about the general population. 
Th is chapter introduces the necessary theories, while the remaining chap-
ters provide specifi c methods for making inferences under diff erent situa-
tions. We also use deductive methods in statistics. In deductive methods, 
we start from the general and make assertions about the specifi c. 
Statistical inference makes probabilistic statements about the 
expected outcome. It is essential to realize that since random events 
occur  probabilistically, there is no “certain” or “defi nite” statement about 
the  outcome. Th erefore, it is essential to provide the probability of the 
 outcome  associated with the expected outcome. 
Sample Size
Before we discuss the role of randomness and the usefulness or eff ective-
ness of a sample, it is important to understand how other factors infl uence 
the eff ectiveness of the sample statistics by providing reliable inference 
about a population parameter. Even if a sample is chosen at random, two 
other factors attribute to the reliability of the sample statistics. Th ey are 
the variance of the population and the sample size. For a population with 
identical members, the necessary sample size is one. For example, if the 
output of a fi rm is always the same, say 500 units per day, then  choosing 
any single given day at random would be suffi  cient to determine the fi rm’s 
output. Note that in the previous example, there was no need to sample at 

102 
STATISTICS FOR ECONOMICS
random, although one can argue that any day that is chosen is a random 
day. However, if the output changes every day due to random factors 
such as sickness, mistakes during production, or breakdown of equip-
ment, then sample size must increase. It is important to understand the 
possible diff erence in output among the days of the week and the month, 
if applicable. For example, Mondays and Fridays might have lower out-
put. By Friday, workers might be tired and not as productive. Machinery 
might break or need to be cleaned more often towards the end of the 
week. On Mondays, workers might be sluggish and cannot perform up 
to their potential. Workers might be preoccupied towards the end of the 
month or early in the month when they are running out of money, or 
when their bills are due. Th ese are just some of the issues that might have 
to be considered when setting up sampling techniques in order to assure 
the randomness of the sampled units. Th erefore, there should be a direct 
relationship between the sample size and the variance of the population, 
and larger samples must be taken from populations with larger variance. 
Since statistical inference is probabilistic to obtain higher levels of confi -
dence, we should take larger samples. 
It can be shown that the required sample size for estimating a mean 
is given by:
 
2
2
2
2
Z
n
E
a s
=
 
(5.1)
where, 
2
2
Za
 is the square of the Z score for desired level of signifi cance,
  s2 is the variance of the population,
  E  2 is the square of tolerable level of error.
Example 5.1
Assume that the population’s standard deviation for the output level is 
29. Also, assume that we desire to limit our error to 5%, which makes the 
level of signifi cance 95%. Let’s allow the tolerable level of error to be 5. 
To obtain the sample size, we fi rst need to obtain the Z score. In 
Figure 5.1, the area in the middle is 95%. Th erefore, the area at the two 
tails is equal to 0.05. 
(Continued)

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
103
Deﬁ nition 5.2
Th e reliability of a sample mean (mˆ) is equal to the probability that the 
deviation of the sample mean from the population mean is within the 
tolerable level of error (E):
 
Reliability = P (−E £ mˆ − m £ E ) 
(5.2)
Example 5.2
A 95% reliability for the sample mean is given by the area between the 
two vertical lines in Figure 5.2.
Reliability = P (m − E (−E £ mˆ − m £ E) = 0.95
–1.96
0
0.025
0.025
95%
1.96
Figure 5.1. Graph of a normal distribution.
Furthermore, the area between zero and the right-hand cutoff  point is 
0.45, which according to the table corresponds with a Z score of 1.96. 
In Excel, you could use the following formula:
=NORMINV (0.975,0,1) 
Note that the answer from Excel is 1.959964, which is slightly off .
Th e necessary sample size is given by: 
n =
 
2
2
2
3.84
841
(1.96 )(29 )
25
5
×
=
 = 129.23
Since fractional samples are not possible, we must always use the 
next integer to assure the desired level of accuracy. Th erefore, the 
 minimum sample size should be 130.
(Continued)

104 
STATISTICS FOR ECONOMICS
And the tolerable level of error is equal to: 
E = 1.96 
n
s
Figure 5.2. The range for 95% reliability for sample mean.
m – e
m + e
m
95%
An astute student will notice that in order to estimate the sample size, 
it is necessary to know the variance. It is also imperative to understand 
that in order to calculate variance, one needs the mean, which apparently, 
is not available; otherwise we would not have to estimate it. Sometimes 
one might have enough evidence to believe that the variance for a popula-
tion has not changed, while its mean has shifted. For example, everybody 
in a country is heavier, but the spread of the weights is no diff erent than 
those in the past. In situations that the known variance is also believed to 
have changed, the only solution is to take a pre-sample to have a rough 
idea about the mean and variance of the population and then use sample 
estimates as a starting point to determine a more reliable sample size. 
Sampling Distribution of Statistics
As stated earlier, sample statistics are a random variable and change from 
sample to sample. Th is means that the actual observed statistics is only 
one outcome of all the possible outcomes. A sampling distribution of 
any statistics explains how the statistics diff er from one sample to another. 
Th e most commonly used statistics are sample mean and sample  variance. 
Th erefore, we will study their sampling distributions by approaching this 
subject in a systematic way. We begin with sampling distribution for one 
sample mean and distinguish between the cases when  population  variance 
is known and when it is unknown. Next, we introduce two  sample means 
(Continued)

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
105
and address the cases of known and unknown variances, and so on. 
 However, before embarking on this mission, it is necessary to discuss the 
Law of Large Numbers and the Central Limit Th eorem, which are the 
foundations of inferential statistics. 
Theorem 5.1 Law of Large Numbers
For a sequence of independent and identically distributed random vari-
ables, each with mean (m) and variance (s2), the probability that the dif-
ference between the sample mean and population mean is greater than 
an arbitrary small number will approach zero as the number of samples 
approaches infi nity. 
Th e theorem indicates that as the number of random samples increase, 
the average of their means approaches the population’s mean. Since sam-
ple means are statistics and random, their values change and none of them 
is necessarily equal to the population mean. Th e law of large numbers is 
essential for the Central Limit Th eorem.
Theorem 5.2 Central Limit Theorem
Let q be a population parameter. Let qˆ  be the estimated value of q that 
is obtained from a sample. If we repeatedly sample at random from this 
population, the variable qˆ  will have the following properties:
1. Th e distribution function of qˆ  can be approximated by normal 
 distribution
2. E (qˆ ) = q
3. 

2
2
n
q
s
s =
Property number 2, above, states that the expected value of the  sample 
statistics (qˆ ) will be equal to the population parameter (q). In other words, 
the average of all such sample statistics will equal the actual value of the 
population parameter. Note that as the sample size increases, the  sample 
variance of the estimate (
2
2
s ) decreases. Th erefore, as the  sample  size 
increases, the sample statistics (estimate) gets closer and closer to the 
 population parameter (q). 

106 
STATISTICS FOR ECONOMICS
Th e approximation improves when the sample size is large and 
the population variance is known. When the population variance is 
unknown and the sample size is small, then the sample statistics qˆ  will 
have the following properties: 
1. Th e distribution function of qˆ  can be approximated by t distribu-
tion with (n − 1) degrees of freedom
2. E(qˆ ) = q
3. 


2
2
n
q
s
s =
As the sample size increases, the distinction between normal 
 distribution and t distribution vanishes. 
Th e general population parameter q can be the mean (m), the 
 proportion (π), or the variance (s2). For each of the above three param-
eters, we may be dealing with one or two populations (comparison). 
In each case, the mean and variance of the estimator will be diff erent. 
Th ere will be 10 diff erent cases, which can be viewed in a summary 
table. 
Lemma 5.1
Let Y = X1 + X2 + … + X n, where the Xs are random variables with a fi nite 
mean (m) and a fi nite and known positive variance (s2). Th en,
Y
m
s
−
has a standard normal distribution, which indicates that it is a normal 
distribution with mean = 0 and variance = 1.
Sampling Distribution of One Sample Mean
Population Variance Is Known
Sample mean is a statistics. Assume we know the variance of a  population 
from which the sample mean is obtained. Let (mˆ) be the mean of a  random 

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
107
sample of size n from a distribution with a fi nite mean (m) and a fi nite and 
known positive variance (s2). Using the central limit  theorem, we know 
the following is true about the sample mean (mˆ): 
1. Th e distribution function for (mˆ) can be approximated by the  normal 
distribution (since the population variance is known)
2. E(mˆ) = m
3. 
2
2
ˆ
n
m
s
s =
Th erefore, we can use the normal table values for comparison 
of the standardized values of the sample mean. Th e knowledge of 
 population variance s2 is essential for completion of the third out-
come. Th e  distribution function of sample mean for samples of size 
30 will be close to normal distribution. For random variables from a 
population that is symmetric, unimodal, and of the continuous type, 
a sample of size 4 or 5 might result in a very close approximation to 
normal  distribution. If the population is approximately normal, then 
the  sample mean would have a normal distribution when sample size 
is as little as 2 or 3.
Example 5.3
Assume that the variance for daily production of a good is 2800 
pounds. Find the sampling distribution of the sample mean (mˆ) for 
a sample of size 67.
Solution
1. Th e sampling distribution of sample mean (mˆ) is normal
2. E(mˆ) = m
3. 
2
ˆ
2800
67
m
s =
 = 41.79
As we see, there is very little computation involved. Nevertheless, the 
theoretical application is tremendous. 

108 
STATISTICS FOR ECONOMICS
Example 5.4
Assume that the variance for daily production of a good is 2800 
pounds. What is the probability that in a sample of 67 randomly 
selected days the output is 15 pounds, or more, below average?
Solution
We are interested in a deviation from the population average.
2
ˆ
(
–
)
600
15
ˆ
[(
)
15]
2800
67
P
P
n
m
m
m
m
s
⎡
⎤
−
⎢
⎥
<
=
< −
=
⎢
⎥
⎢
⎥
⎣
⎦
 
15
15
6.47
41.79
Z
P
Z
P ⎡
⎤
⎡
⎤
<
<
⎢
⎥
⎢
⎥
⎣
⎣
⎦
−
−
=
⎦
=
 
= P [Z < 2.32] = 0.5 − P [Z > 2.32] 
 
= 0.5 − 0.4898 = 0.0102
Note that for this problem, we do not need to know the true 
 population mean since we are interested in knowing the probability of 
producing below the population mean. 
Population Variance Is Unknown
Let (mˆ) be the mean of a random sample of size n from a distribution with 
a fi nite mean and a fi nite and unknown positive variance (s2). According 
to the Central Limit Th eorem:
1. Th e distribution of (mˆ) can be approximated by a t distribution 
function
2. E(mˆ) = m
3. 
2
2
ˆ
n
m
s
s =
Th erefore, we can use the t table values, which are provided in th e Appen-
dix for comparison of the standardized values of the sample mean.

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
109
When the population size is small, or if the sample size to  population 
size n N is greater than 5%, you should use a correction factor with the 
variance. Th e correction factor is 
1
N
n N
−
−. For fi nite populations, 
the  variance of the sample mean becomes:
2
2
ˆ
1
N
n
n N
m
s
s
−
=
−
  known variance

2
2
ˆ
1
N
n
n N
m
s
s
−
=
−
  unknown variance
Summary
Distribution function for one sample mean
Distribution
Mean
Variance
Population Variance is known
Normal
m
2
n
s
Population Variance is unknown
t
m
2
n
s
Sampling Distribution of One Sample Proportion
Let (pˆ ) be a proportion from a random sample of size n from a distribu-
tion with a fi nite proportion (p) and a fi nite positive variance (s2). When 
both np ≥ 5 and n(1 − p) ≥ 5, then the following theorem is correct based 
on the Central Limit Th eorem:
1. Th e distribution of (pˆ ) can be approximated by a normal distribu-
tion function
2. E (pˆ ) = p
3. 
2ˆ
ˆ
ˆ
(1
)
n
p
p
p
s
−
=
Note that in order to obtain the variance of the sample proportion, we 
must estimate the population proportion using the sample proportion. 
Th us, 

2
2
ˆ
ˆ
p
p
s
s
=
.

110 
STATISTICS FOR ECONOMICS
Th erefore, we can use the normal table values for comparison of the 
standardized values of the sample mean. Note that the symbol pi (p) 
is used to represent the sample proportion and has nothing to do with 
p = 3.141593…
An added advantage of using sampling distribution of sample propor-
tion is that you can use normal approximation to estimate probability of 
outcome for binomial distribution function without direct computa-
tion or the use of binomial distribution tables. When both n p ≥ 5 and 
n(1 − p) ≥ 5 it is reasonable to approximate a binomial distribution using 
a normal distribution.
Sampling Distribution of Two Sample Means
Th e extension from the distribution function of a single sample mean to 
two or more means is simple and follows naturally. However, it is neces-
sary to introduce appropriate theories.
Theorem 5.3 The Expected Value of Sum of Random Variables
Let Y  = X1 + X2 + … + X n, where the Xs are random variables. Th e expected 
value of Y is equal to the sum of the expected values of Xs.
E(Y ) = E(X1) + E(X2) + … + E(Xn)
Th eorem 5.3 does not require that Xs be independent. Th is theorem 
allows us to sum two or more random variables.
Sampling Distribution of Difference of Two Means
When conducting inferences about two population parameters, there 
are two sample statistics, one from each population. Often, in order to 
 conduct an inference the relationship between the parameters, and hence 
the corresponding statistics, has to be modifi ed and written as either the 
diff erence of the parameters or the ratio of the parameters. Th is requires 
knowledge of the distribution function for the diff erence of two sample 
 statistics or the distribution function for the ratio of two sample statistics. 

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
111
In this section, the sampling distribution of the diff erence of two sample 
means is discussed. Choose a distribution function for the diff erence of 
two sample proportion or the distribution function for the ratio of two 
variances to access those distribution functions. Th is section will address 
the distribution function of the diff erence of two means.
The Two Sample Variances Are Known and Unequal
Let (
1
m ) and (
2
m ) be the means of two random samples of sizes n1 and n2 
from distributions with fi nite means m1 and m2 and fi nite positive known 
and unequal variances 
2
1
s  and 
2
2
s . Let n1 and n 2 be the respective sample 
sizes. According to the Central Limit Th eorem and Th eorem 5.3:
1. Th e distribution of (
1
m  − 
2
m ) is normal
2. E(
1
m  − 
2
m ) = m1 − m2
3. 


1
2
2
2
1
2
1
1
Var(
)
.
n
n
s
s
m
m
+
−
=
Note that the variances of the two samples are added together, while 
the means are subtracted. Th erefore, we can use the normal table values 
for comparison of the standardized values of the diff erences of sample 
means.
The Two Sample Variances Are Known and Equal
Let (
1
m ) and (
2
m ) be the means of two random samples of sizes n1 and n2 
from distributions with fi nite means m1 and m2 and fi nite positive known 
and equal variances 
2
1
s  and 
2
2
s . Let n1 and n 2 be the respective sample 
sizes. According to the Central Limit Th eorem and Th eorem 5.3:
1. Th e distribution of (
1
m  − 
2
m ) is normal
2. E(
1
m  − 
2
m ) = m1 − m2
3. 


1
1
2
2
2
1
1
Var(
)
n
n
m
m
s
+
⎛
⎞
−
=
⎜
⎟
⎝
⎠
Since 
2
1
s  = 
2
2
s  = s2.

112 
STATISTICS FOR ECONOMICS
The Two Sample Variances Are Unknown and Unequal
Let (
1
m ) and (
2
m ) be the means of two random samples of sizes n1 and n2 
from distributions with fi nite means m1 and m2 and fi nite positive unknown 
and unequal variances 
2
1s  and 
2
2
s . Let n1 and n 2 be the  respective sample 
sizes. According to the Central Limit Th eorem and Th eorem 5.3:
1. Th e distribution of (
1
m  − 
2
m ) is normal
2. E (
1
m  − 
2
m ) = m1 − m2
3. Var 



2
2
1
1
2
2
2
1
(
)
n
n
m
s
s
m
−
+
=
The Two Sample Variances Are Unknown and Equal
Let (
1
m ) and (
2
m ) be the means of two random samples of sizes n1 
and n2 from distributions with fi nite means m1 and m2 and fi nite posi-
tive unknown and equal variances 
2
1
s  and 
2
2
s . Let n1 and n 2 be the 
respective sample sizes. According to the Central Limit Th eorem and 
Th eorem 5.3:
1. Th e distribution of (
1
m  − 
2
m ) is normal
2. E (
1
m  − 
2
m ) = m1 − m2
3. Var 
1
m  − 
2
m  = 
2
1
2
1
1
Pooled
n
n
s
⎛
⎞
+
⎜⎝
=
⎟⎠
where 
(
) 
(
) 
2
2
1
2
2
1
2
1
2
1
1
2
Pooled
n
n
n
n
s
s
s
−
+
−
+
−
=
.
Summary of Sampling Distribution of Sample Means
Do not let these seemingly diff erent and possibly diffi  cult formulae 
 confuse you. Th ey are similar. Th e most common case is case 3, which 
is listed earlier. Th e fi rst three cases can use this formula without any 
 problem. Th e last case, earlier, takes advantage of the fact that there 
are two estimates of the unknown variance instead of one. Logic dic-
tates that it would be better to average the two estimates using their 
 respective sample sizes as weights. Table 5.1 provides a summary of 

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
113
 sample  statistics, its distribution functions, and its parameters for one 
and two sample means. 
Sampling Distribution of Difference of Two Proportions
When conducting inferences about two population parameters there 
are two sample statistics, one from each population. Often, in order to 
 conduct an inference, the relationship between the parameters, and hence 
the corresponding statistics, has to be modifi ed and written as either the 
diff erence of the parameters or the ratio of the parameters. Th is requires 
knowledge of the distribution function for the diff erence of two sample 
statistics or the distribution function of the ratio of two sample statistics. 
In this section, sampling distribution of the diff erence of two sample pro-
portions is discussed. Choose the distribution function for the diff erence 
of two means or the distribution function for the ratio of two variances 
to access distribution functions for diff erences of two means or the ratio 
of two variances. Th is section will address the distribution function of the 
diff erence of two proportions. 
Let 
1
p  and 
2
p  be the proportions of interest in two random samples 
of sizes n1 and n2 from distributions with fi nite proportions p1 and p2 
Table 5.1 Summary of Sampling Distribution for Sample Mean
Sample 
statistics
Population 
variance(s)
Distribution
Mean
Variance of 
 sample statistics
mˆ
Known
 Normal 
m
s2
n
mˆ
Unknown
t
m

s2
n


m
m
−
1
2
Known and 
Unequal
Normal
m1 − m2
s
s
+
2
2
1
2
1
1
n
n


m
m
−
1
2
Known and 
Equal
Normal
m1 − m2
2
1
2
1
1
n
n
s ⎛
⎞
+
⎜
⎟
⎝
⎠


m
m
−
1
2
Unknown and 
Unequal
t
m1 − m2


s
s
+
2
2
1
2
1
2
n
n


m
m
−
1
2
Unknown an 
Equal
t
m1 − m2

2
1
2
1
1
Pooled
n
n
s
⎛
⎞
+
⎜
⎟
⎝
⎠

114 
STATISTICS FOR ECONOMICS
and fi nite positive variances 
2
1
s  and 
2
2
s . According to the Central Limit 
Th eorem: 
1. Th e distribution of ( 

1
2
p
p
−
) is normal
2. E( 

1
2
p
p
−
) = p1 − p2
3. Var( 

1
2
p
p
−
) = 



1
1
2
2
1
2
(1
)
(1
)
n
n
p
p
p
p
−
−
+
Th erefore, we can use the normal table values for comparison of the 
standardized values of the sample mean.
In practice, p1 − p2 is not known and their estimates, 
1
p  and 
2
p  respec-
tively, are used in calculating the variance of ( 

1
2
p
p
−
). Th e distribution 
function, expected value, and standard deviation for one and two sample 
proportions are given in Table 5.2.
Sampling Distribution of Sample Variance
Theorem 5.4
Let the random variable X have normal distribution with mean m and 
variance s2, then random variable 
V = 
2
X
m
s
−
⎛
⎞
⎜
⎟
⎝
⎠
 = Z 2
has a chi-squared distribution with one (1) degree of freedom, which is 
shown as c2(1). Z is the same Z score as discussed in previous chapters, 
which consists of individual error divided by average error. 
Table 5.2. Summary of Sampling Distributions of Sample Proportions
Sample 
statistics
Population 
variance(s)
Distribution
Mean
Variance of  sample 
statistics
p
NA
Normal
p


p
p
−
1
(1
)
n


p
p
−
1
2
NA
Normal
p1 − p2




p
p
p
p
−
−
+
1
1
2
2
1
2
(1
)
(1
)
n
n

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
115
Chi-squared distributions are cumulative. Th erefore, when n chi-
squared distribution functions are added up, the result is another 
 chi-squared distribution with n (sum of n distribution functions each with 
one) degree of freedom. 
Theorem 5.5
Let random variables X1, X2, …, Xn have normal distribution each with 
mean m and variance s2 then the sum 
2
X
m
s
−
⎛
⎞
∑⎜
⎟
⎝
⎠ has a chi-squared dis-
tribution with n degrees of freedom. From this relation, we can build 
confi dence intervals for one and two variances, and test hypothesis for 
one and two variances. 
Sampling Distribution of Two Samples Variances
When conducting inferences about two population parameters, there 
are two sample statistics, one from each population. Often, in order 
to  conduct an inference, the relationship between the parameters, and 
hence the corresponding statistics, has to be modifi ed and written as 
either the diff erence of the parameters or the ratio of the parameters. 
Th is requires knowledge of the distribution function of the diff erence 
of two sample statistics or the distribution function of the ratio of two 
sample statistics. In this section, the sampling distribution of the ratio 
of two sample variances is discussed. Choose the distribution function 
for diff erence of two means or the distribution function for diff erence of 
two proportions to access distribution functions for diff erences of means 
or proportions. Th is section will address the distribution function of the 
ratio of two variances. 
Theorem 5.6
Let random variables X1, X2, …, Xm have normal distribution each with 
mean m1 and variance 
2
1
s  and the random variable Y1, Y2, …, Yn have 
 normal distribution each with mean m2 and variance 
2
2
s  then random 
variable

116 
STATISTICS FOR ECONOMICS
2
2
1
2
X
Y
F
m
m
s
s
−
−
⎛
⎞
⎛
⎞
= ∑
∑
⎜
⎟
⎜
⎟
⎝
⎠
⎝
⎠
has an F distribution with m and n degrees of freedom. 
Note that in Th eorem 5.6, we could have expressed the numerator 
and denominator in terms of the corresponding chi-square  distributions 
as stated in Th eorem 5.5, which in turn is built upon Th eorem 5.4. 
In practice, when populations are not known, they are substituted by 
their respective sample variances.
F = 
2
1
2
2
s
s
It is assumed that the variance labeled 
2
1
s  is greater than the variance 
labeled 
2
2
s . Tabulated values of F are greater than or equal to 1. 
Th e most common use of F distribution at this level is for the test of 
hypothesis of equality of two variances. Th is test provides a way of deter-
mining whether or not to pool variances when testing for the equality of 
two means with unknown population variances. First, test the equality 
of variances. If the hypothesis is rejected, then the variances are diff erent 
and are not pooled. If the hypothesis is not rejected, then the variances 
are the same and must be pooled to obtain a weighted average of the two 
estimated values 
Another use of F distribution is in testing three or more means. When 
testing a hypothesis that involves more than two means, we cannot use 
the t distribution. Test of hypothesis and the use of t and F distributions 
are discussed in Chapter 6. 
Efﬁ ciency Comparison Between Mean and Median
Let mˆ be the sample mean and Mˆ  be the sample median. Th e expected 
value of, both, sample mean and sample median is equal to  population 
mean. Th at is, both provide unbiased estimates of the population mean. 
However, as shown below, the sample mean is more effi  cient than the 
sample median in estimating the population mean. An estimator of a 

 
SAMPLING DISTRIBUTION OF SAMPLE STATISTICS 
117
parameter is said to be more effi  cient than another estimator if the former 
has smaller variance. According to Central Limit Th eorem, the variance 
of the sample mean (mˆ) is

2
2
n
s
s =
It can be shown that the variance of the median is
Variance (Sample Median) = ps2
2n
where π = 3.141593…
2
2
Variance (Sample Mean)
2
2
0.64
Variance (Sample Median)
3.14159
2
n
n
s
p
ps
=
=
=
=
Th erefore, mˆ is more effi  cient than median in estimating the popula-
tion mean. Th e variance of the sample median from a sample of size 100 
is about the same as the variance of the sample mean from a sample of 
size 64.
It is worthwhile to note the following discrepancy, which is caused by 
having a diff erent orientation or starting point. 
Var (Mˆ ) = 1.57 Var (mˆ)
Var (mˆ) = 0.6366 Var (Mˆ )
Th erefore, the variance of sample mean is only 64% of the variance of 
the sample median. In estimating population mean, if we take a sample 
of size 100 and use the sample mean as the estimator, we will get a certain 
variance and hence, an error. To obtain the same level of error using the 
sample median to estimate the population mean, we need a sample of 
157, which is 1.57 times more than 100. 
Recall that when extreme observations exist, sample median is 
 preferred to sample mean because it is not infl uenced by extreme  values. 

118 
STATISTICS FOR ECONOMICS
For example, in real estate it is of interest to know the price of a typical 
house. Th e industry reports the prices of homes listed, sold, or withdrawn 
from the market each month. Usually, only a small fraction of existing 
homes is listed, sold, or withdrawn from the market. Th is causes large 
fl uctuations in the average prices of these groups. Th e industry reports 
the median instead of the average price for the listings. Both sample 
mean and sample median are unbiased estimates of population mean, 
however, the latter is not infl uenced by the extreme high or low prices and 
thus is a  better short-run estimator. Since sample median is less effi  cient 
than  sample mean in estimating the population mean, larger samples are 
needed.

CHAPTER 6
Point and Interval 
 Estimation
Estimation Versus Inference
Th ere are two distinct ways of using statistics: Descriptive  statistics 
and inferential statistics. Descriptive statistics provide summary 
 statistics in the forms of tables, graphs, or computed values. We can 
use  descriptive  statistics to describe population data or sample data. 
 Inferential  statistics is used to draw conclusions about population 
parameters using  sample statistics. Obtaining sample statistics for infer-
ential  statistics is the same as obtaining them for descriptive statistics. 
Th e use of the sample  statistics determines whether it is descriptive or 
inferential. Th e statistics obtained from a sample for the purpose  of 
inference is called  estimation, to emphasize the fact that they are esti-
mates for their respective parameters. In the previous chapters, the 
portions of  descriptive statistics that dealt with samples are actually 
estimations. When we obtain sample mean, proportion, or variance we 
are  calculating estimates of the corresponding population parameters. 
As we have demonstrated earlier, estimation is important. When sample 
estimates are used to test population parameters and to indicate how far 
the estimates are from parameters, then we are in the domain of inferen-
tial statistics. Some statisticians believe that the primary objective of sta-
tistics is to make inferences about population parameters using sample 
statistics. Using sample statistics to make deductions about population 
parameters is called statistical inference. Statistical inference can be 
based on point estimation or confi dence interval, both of which will 
be covered shortly. Th ey are closely related, and in some cases, they are 
interchangeable. 

120 
STATISTICS FOR ECONOMICS
Point Estimation
A point estimate is the statistic obtained from a sample. Th e reason for 
the name is because the estimate consists of a single value.  Examples of 
point estimates include sample mean (mˆ), sample  proportion (pˆ),  sample 
 variance (2
s ), and sample median. Th ese statistics are used to estimate 
the population mean (m), population proportion (p), and population 
variance (s2), respectively. Sample median can be used to estimate, 
both, population median and population mean. In fact, any single-
valued estimate obtained from a sample is a point estimate. Good 
estimates are close to the corresponding population parameter. Proper 
sampling provides accurate estimates of the unknown population 
parameter. Descriptive statistics addresses the procedure to obtain and 
calculate point estimates from the sample. It also explains their proper-
ties and uses. As discussed in Chapter 3, a suitable estimate is unbiased, 
 consistent, and effi  cient.
Although point estimates are useful in providing descriptive 
 information about a population, their usefulness is limited because we 
 cannot determine how far they are from the targeted parameter. In order 
to provide levels of confi dence and a probability for margin of error 
one needs to know the distribution function of sample statistics. Once a 
sampling distribution of the sample statistic is known, the probability of 
observing a certain sample statistic can be calculated with the aid of the 
corresponding table. 
Example 6.1 
Calculate point estimates of mean, median, variance, standard 
 deviation, and coeffi  cient of variation for the stock prices of Wal-Mart 
and Microsoft from March 12 to March 30, 2012.
Solution
Th e solutions are found by using Microsoft Excel. Refer to the 
 Appendix on Excel.
(Continued)

 
POINT AND INTERVAL  ESTIMATION 
121
Date
WMT
MSFT 
12 Mar.
$60.68 
$32.04
13 Mar.
$61.00 
$32.67
14 Mar.
$61.08 
$32.77
15 Mar.
$61.23 
$32.85
16 Mar.
$60.84 
$32.60
19 Mar.
$60.74 
$32.20
20 Mar.
$60.60 
$31.99
21 Mar.
$60.56 
$31.91
22 Mar.
$60.65 
$32.00
23 Mar.
$60.75 
$32.01
26 Mar.
$61.20 
$32.59
27 Mar.
$61.09 
$32.52
28 Mar.
$61.19 
$32.19
29 Mar.
$60.82 
$32.12
30 Mar.
$61.20 
$32.26
Mean
$60.89
$32.32
Variance
0.056464
0.10941
Median
$60.83
$32.20
St. Dev
0.237622
0.33078
CV
$0.0039
$0.0102
Interval Estimation
Statistics deals with random phenomena. Nothing remains constant in 
life. Methods of production change. Processes are modifi ed. Machines 
get out of calibration. New techniques are applied. In all these cases, 
statistics is used to determine what remains constant and what changes. 
In estimation theory, sample statistics is used to estimate the population 
parameter. However, when one uses point estimation, it is not clear how 
close the estimate is to the parameter and there is no measure of confi -
dence. Th is does not mean that point estimates are useless or unreliable. 
With proper sampling the point estimates will be unbiased, consistent, 
and effi  cient. 

122 
STATISTICS FOR ECONOMICS
Interval estimation augments point estimates by providing a margin 
of error for the point estimate. Th e margin of error is a range, based 
on the degree of certainty, for the estimate of the population parameter, 
which is added and subtracted from a point estimate. Confi dence inter-
vals are based on the point estimate of the parameter and the distribution 
function of the point estimate. It is aff ected by the level of certainty, the 
variance of data, and the sample size.
Calculating Conﬁ dence Intervals
Interval estimation is a simple notion and is defi ned as:
 
Point estimate ± Margin of error 
(6.1)
In order to explain this concept we need to recall things from 
 Chapters 2, 3, and 4. We covered point estimates in Chapter 2, although 
we did not use the same terminology. Point estimates are sample statis-
tics and are used to estimate the corresponding population parameters. 
Mean, median,  variance, and proportion are some common examples of 
point estimates. 
Th e margin of error is not totally new either. It is based on the use 
of Z score, which we covered in Chapter 3. In Chapter 4 we introduced 
the normal distribution function and demonstrated the use of Z score 
and standardization in obtaining probabilities of random variables from 
normal distribution. Also in Chapter 4, we used a Z score to obtain the 
probability under normal distribution between two points. Previously we 
pointed out that due to symmetry of the standardized normal distribu-
tion, the probability of the area from zero, that is, the center, to a point on 
the right is equal to the probability of the area from zero to the negative 
value of that number (see Figure 6.1).
Note that in Figure 6.1, the mean is 0 and the standard deviation is 1. 
Also recall that the unit of measurement of the Z score is the standard 
deviation. Conventionally, we do not write 1 but if the standard devia-
tion was diff erent from 1, we must include that in the measure, as in 
Figure 6.1. Th erefore, a point on the normal distribution curve is Zs 
away from the mean. 

 
POINT AND INTERVAL  ESTIMATION 
123
When dealing with sample statistics, the properties of sampling distri-
bution apply. Th erefore, in the case of statistics, that is, estimates obtained 
from the sample, the Central Limit Th eorem states that the standard devi-
ation is given by 
.
n
s
 Replacing s with the this formula provides
 
Margin of Error =
2
Z
n
a
s  
(6.2)
It is very important to realize that the margin of error formula in 
Equation 6.2 depends on the knowledge of population variance. When 
the population variance is unknown and the sample variance has to be 
used, then the formula must be adjusted by replacing the population 
standard deviation with the sample standard deviation and the Z value 
must be replaced by the t value as in Equation 6.3:
 
2
Margin of Er
ˆ
ror
t
n
a
s
=
 
(6.3)
Example 6.2 
Calculate the margin of error for Microsoft stock price for the 
period March 12–March 30, 2012, using 83% level of confi dence. 
Assume the real population variance is equal to the sample variance.
Solution 
First obtain the Z value that corresponds to half of the 0.83 level of 
confi dence so we can use the normal table provided in the Appendix 
to this book.
0.83
0.415
2
(0
)
P
Z
X
⎛
⎞=
⎜
⎟
⎝
⎠
<
<
=
Figure 6.1. Margin of error on normal distribution.
m – e
m + e
0
95%
(Continued )

124 
STATISTICS FOR ECONOMICS
Searching in the body of the normal table, we fi nd the  probability 
0.415 corresponds roughly to Z = 1.375. Note that we are using the 
sample variance as if it was the actual population variance.
According to Example 2.16 the variance of the data is 0.102.
0.102
1.375
0.08239
0.1133
1
Margin of Error
1.375
5
×
=
=
=
However, since the population variance is unknown we must 
use the t value instead of Z value. Unfortunately, t values for 
alpha of (1 – 0.83)/2 = 0.085 are not readily available.  However, 
 Microsoft Excel provides the necessary number by using the 
 following  command:
= t.inv(0.085, 14) = 1.44669
Note that Excel will provide a negative sign in front of the t value 
because it refers to the left hand tail. But we do not need to worry, 
since the distribution is symmetric. 
Th erefore, Margin of Error = 1.44669 × 0.2316 = 0.33505
Th is is the correct value of margin of error because it is using the 
t value, as required when population variance is unknown.
Th e concept of margin of error applies to sample statistics but not to 
the population parameters. Population parameters are constant values and 
do not have margin of error. However, sample statistics, which are random 
variables and are estimating their respective population parameters, have a 
margin of error. As seen in Equation 6.2, margin of error is directly related 
to variance of the population and the level of confi dence, as indicated by 
the Z score, and inversely related to the square root of the sample mean. 
Recall from Chapter 5 that the sampling distribution of sample mean 
is aff ected by the knowledge of the population variance. If the popula-
tion variance is known, the distribution function of the sample mean 
is a normal distribution. However, if the population variance is not 
known the distribution function of the sample mean is a t distribution. 
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
125
Th erefore, when calculating the margin of error we must use either the 
normal table or the t table, depending on whether the population vari-
ance is known or unknown, respectively. 
In Chapter 5, we learned the sampling distribution function of one 
sample statistics such as mean, proportion, and variance. When calculat-
ing margin of error for these statistics we need to use the appropriate 
distribution function. We learned the sampling distribution of the diff er-
ences of two sample means and two sample proportions, both of which 
are a normal distribution. We also learned the sampling  distribution of 
two variances, which is an F distribution. Once the margin of error is 
calculated, obtaining confi dence interval is simple by using  Equation 6.1. 
Terminology
Th e probability between –Z and +Z from a standard normal, that is, a normal 
distribution with mean zero 0 and variance 1, is shown by (1 – a)%. Th is is 
because the area outside of the above range is equal to a. In Chapter 7, we 
will provide more explanation for the naming of these areas and go into more 
detail on the meaning of the term a. 
Customarily, normal distribution tables are calculated for half of the 
area, because of symmetry. Th us, the Z value, which corresponds to one 
half of a is shown as 
2.
Za
Interval Estimation for One Population Mean
Th e correct way of writing interval estimation of Equation 6.1 when 
 estimating the population mean is
 
2
ˆ
Z
n
a
s
m ±
 
(6.4)
Deﬁ nition 6.1
Based on Chapter 5, the (1 – a)% confi dence interval for the mean of 
one population ( m) when the population variance is known is given by 
Equation (6.4).

126 
STATISTICS FOR ECONOMICS
Note that we need to calculate the following two values to obtain a 
confi dence interval.
 
Lower Bond: 
2
ˆ
Z
n
a
s
m −
 
(6.5)
 
Upper Bond: 
2
ˆ
Z
n
a
s
m +
 
(6.6)
Sometimes we refer to the lower bond and upper bond as LB and UB, 
respectively. 
Rule 6.1 Inference with Conﬁ dence Interval
Th e confi dence interval covers the true population parameter with (1– a)% 
confi dence. 
Example 6.3
Provide an 83% confi dence interval for Microsoft stock price for the 
period of March 12 to March 30, 2012. Assume the real population 
variance is equal to the sample variance.
Solution 
From Examples 6.1 and 6.2 we have the necessary numbers.
LB = 32.32 − 0.1133 = 31.21
UB = 32.32 + 0.1133 = 32.43
An 83% confi dence interval for the mean of Microsoft stock price 
between March 12 to March 30, 2012, is given by the range $31.21 
to $32.43.
Note that we assumed the sample variance is equal to the  population 
variance in order to apply the example to this case. If the variance 
is unknown, which is true more often than it is not, we have to use 
Equation 6.7. Furthermore, when the value of population variance is not 
known, we must use a t distribution value rather than a Z  distribution 
value. 

 
POINT AND INTERVAL  ESTIMATION 
127
Deﬁ nition 6.2 
Based on Chapter 5, the (1 – a)% confi dence interval for the mean of one 
population ( m) when the population variance is unknown, is given by 
the following equation:
 

2
ˆ
t
n
a
s
m ±
 
(6.7)
In earlier days when t tables were only available for 1%, 5%, and 10% 
levels of signifi cance, most researchers would use a normal table instead 
of the t table when sample size was greater than 30. With wide availability 
of t values for any level of signifi cance, this practice is no longer necessary.
Example 6.4
Provide a 95% confi dence interval for Microsoft stock price for the 
period of March 12 to March 30, 2012.
Solution 
Since the population variance is unknown and the sample size is less 
than 30, we need to use the t distribution. Th e t value for 95% con-
fi dence interval is ±2.14479. To obtain this value look under 2.5% 
probability of type I error with 14 degrees of freedom in the t table or 
use the following Excel command.
= t.inv(.025,14) = −2.14479
Th e value reported by Excel is negative because it is designed to report 
the lower-end critical value.
Example 6.2 reported the appropriate standard deviation based on 
the variance calculated in Example 1.16.
LB = 32.32 – 2.14479 (0.8239) = 30.55291
UB = 32.32 + 2.14479 (0.8239) = 34.08709 
A 95% confi dence interval for the mean of Microsoft stock price is given 
by the range $30.55 to $34.09. Th is is the correct confi dence interval 
because it uses the t value since the population variance is unknown.

128 
STATISTICS FOR ECONOMICS
Since t values are somewhat larger, to account for the fact that the 
population variance is unknown and must be estimated, the correspond-
ing confi dence interval is wider than the one calculated using a Z table 
when we assume to know the population variance. 
Deﬁ nition 6.3 
Based on Chapter 5, the (1 – a)% confi dence interval for the  proportion 
of one population (p) is given by the following equation:
 
2
ˆ
ˆ
(1
)
ˆ
Z
n
a
p
p
p
−
±
 
(6.8)
Example 6.5
Calculate a 95% confi dence interval for the proportion of stock prices 
of Microsoft that are higher than $32.5. Use the sample data from 
April 2 to April 21, 2012, provided in Example 3.2.
Solution
Th e sample proportion of stocks over $32.5 is
p =
=
4
ˆ
0.4
15
Th e Z value corresponding to 95% confi dence is 1.96.
LB
0.4
1.96
 0.4
1.96
0.1265
0.4
0.2479
0.
0.4
0.6
1
1
5
152
⎛
⎞
×
⎜
⎟
⎝
=
−
=
−
×
=
−
=
⎠
UB
0.4
1.96
0.4
1.96
0.1265
0.4
0.2479
0
0.4
0.
.6479
6
15
=
+
=
+
⎛
⎞
×
⎜
⎟
⎝
⎠
×
=
+
=
Th e range 0.1521 to 0.6479 covers the true population proportion of 
Microsoft stock prices that are $32.5 or higher. 
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
129
Since the population variance is not known and the sample size 
is smaller than 30, we should have used the t distribution instead of 
normal distribution. In practice, the sample size is much larger when 
proportions are used. Th e results using the t values are given by:
LB
0.4
2.145
0.4
2.145
0.1265
0.4
0.2713
0
0.4
0.
.1287
6
15
=
−
=
−
⎛
⎞
×
⎜
⎟
⎝
⎠
×
=
−
=
UB
0.4
2.145
0.4
2.145
0.1265
0.4
0.2713
0.
0.4
0.6
15
6713
=
+
+
⎛
⎞
×
⎜
⎟
⎝
⎠
×
=
+
=
Th e range 0.1287 to 0.6713 covers the true population proportion of 
Microsoft stock prices that are $32.5 or higher. Note that the range 
became wider when we used the t distribution value. Th is is the conse-
quence of not knowing the population variance. 
Deﬁ nition 6.4
Based on Chapter 5, the (1 – a)% confi dence interval for the  diff erence 
of means of two populations ( m1 – m2) when the population variances 
are known and unequal is given by the following equation:
 


2
2
1
2
1
2
2
1
1
Z
n
n
a
s
s
m
m
−
±
+
 
(6.9)
Deﬁ nition 6.5
Based on Chapter 5, the (1 – a)% confi dence interval for the  diff erence 
of means of two populations ( m1–m2) when the population variances 
are known and equal is given by the following equation:
 


2
1
2
2
1
2
1
(
1
)
Z
n
n
a
m
m
s
−
±
+
⎛
⎞
⎜
⎟
⎝
⎠ 
(6.10)

130 
STATISTICS FOR ECONOMICS
Deﬁ nition 6.6
Based on Chapter 5, the (1– a)% confi dence interval for the diff erence of 
means of two populations ( m1–m2) when the population variances are 
unknown and unequal is given by the following equation:
 




2
2
1
2
1
2
2
1
2
(
)
t
n
n
a
s
s
m
m
−
±
+
 
(6.11)
Example 6.6
Obtain a 95% confi dence interval for the diff erence in Microsoft 
stock prices between March 12 and March 30, 2012, and April 2 and 
April 21, 2012. Th e data is provided in Example 3.2. 
Solution
Let’s mark the data in March with the subscript “1” and those for April 
with subscript “2.” Th e required formula is given in Equation 6.11. 
According to the formula we need the means and variances from both 
periods as well as the t value corresponding to 95% confi dence.
We have the following results from previous examples. Make sure 
to verify the accuracy of the output. Remember that rounding off  the 
numbers in early stages of calculation may produce discrepancies in 
fi nal results. Here, we are using the output from Excel, which is some-
what larger than the values obtained using the computational method 
of Equation 2.35. Th e short-cut Equation 2.35 produces the least 
amount of rounding off , and thus is more accurate. 

1
32.32
m =
  
2
1
0.10941
s =

2
31.27
m =
  
2
2
0.372182
s =
= t.inv(0.025,28) = −2.0484
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
131
Since we lose two degrees of freedom the correct degrees of  freedom 
for the t distribution is 15 + 15 − 2 = 28. Recall that we need to use 
both positive and negative t values.




a
s
s
m
m
=
−
=
−
−
=
−
=
−
×
=
−
−
+
+
=
+
2
2
1
2
1
2
2
1
2
LB
(
)
(32.32
31.27)
2.0484
1
0.10941
0.372182
15
15
0.007294
0.024812
.05
2.0484
1.05
2.0484
0.179183
1.05
0.36704
$0.68
t
n
n




a
s
s
m
m
=
−
=
−
+
=
+
=
+
×
=
+
−
+
+
=
+
2
2
1
2
1
2
2
1
2
UB
(
)
(32.32
31.27)
2.0484
1
0.10941
0.372182
15
15
0.007294
0.024812
.05
2.0484
1.05
2.0484
0.179183
1.05
0.36704
$1.42
t
n
n
Th e range $0.68 to $1.42 covers the diff erence of the means of the 
two periods of stock prices for Microsoft with 95% probability. 
Deﬁ nition 6.7
Based on Chapter 5, the (1 – a)% confi dence interval for the  diff erence of 
means of two populations ( m1 – m2) when the population  variances are 
unknown and equal is given by the following equation:
 



2
1
2
2
1
2
1
1
(
)
Pooled
t
n
n
a
m
m
s
−
±
+
⎛
⎞
⎜
⎟
⎝
⎠ 
(6.12)
(
) 
(
) 
2
2
1
2
2
1
2
1
2
1
1
.
2
where 
Pooled
n
n
n
n
s
s
s
−
+
−
+
−
=

132 
STATISTICS FOR ECONOMICS
Example 6.7
Obtain a 95% confi dence interval for the diff erence in Microsoft stock 
prices between March 12 and March 30, 2012, and April 2 and April 
21, 2012. Th e data is provided in Example 3.2. 
Solution
Since the data belongs to the same company and are so close in time 
period, it is reasonable to assume that there is one variance for the 
company’s stock prices and the two sample statistics are two estimates 
of the same population variance. Th erefore, it is necessary to fi nd their 
weighted averages and then use Equation 6.12. We already have the 
following results:

1
32.32
m =
  
2
1
0.10941
s =

2
31.27
m =
  
2
2
0.372182
s =
= t.inv(.025,15) = – 2.144787
(
) 
(
) 
2
2
1
2
2
1
2
1
2
1
1
2
Pooled
n
n
n
n
s
s
s
−
+
−
+
−
=
 
(
)
(
)
15
1 0.10941
15
1 0.372182
15
15
2
−
+
−
=
+
−
 
1.531785
5.210
0.240798
554
28
+
=
=
 



2
2
1
2
2
1
1
1
B
)
L
(
Pooled
t
n
n
a
m
m
s
⎛
⎞
=
−
±
+
⎜
⎟
⎝
⎠
 
⎛
=
−
⎜⎝
⎠
−
⎞
+
⎟
1
1
0.240798 
 
15
(32.32
31.27)
5
4
1
2.048
 
=
=
−
−
×
1.05
2.0484
1.05
2.0484
0
0.
.1
032106
79183
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
133
= 1.05 – 0.3685
= 0.68



2
1
2
2
1
2
1
1
UB
(
)
Pooled
t
n
n
a
m
m
s
⎛
⎞
=
−
±
+
⎜
⎟
⎝
⎠
⎛
⎞
=
−
+
+
⎜
⎟
⎝
⎠
1
1
(32.32
31.27)
2.0484 0.240798 15
15
=
+
=
+
×
1.05
2.0484 0.032106
1.05
2.144787
0.179183
= 1.05+ 0.3685
= 1.42
Th e range $0.68 to $1.42 covers the diff erence of the means of the 
two periods of stock prices for Microsoft with 95% probability. 
Th e reason these results are exactly the same as the result for the 
previous case, where we did not assume the equality of the variances, is 
that the two sample sizes are equal. When samples have diff erent sizes 
the results will be diff erent. 
Deﬁ nition 6.8
Based on Chapter 5, the (1 – a)% confi dence interval for the diff er-
ence of two population proportions ( p1 – p2) is given by the following 
equation:
 






1
1
2
2
1
2
2
1
2
(1
(
)
(1
)
)
Z
n
n
a
p
p
p
p
p
p
−
−
±
+
−
 
(6.13)
Example 6.8
Calculate a 95% confi dence interval for the diff erence of the propor-
tion of stock prices of Microsoft that are more than or equal to $32.00 
for the periods March 12–30, 2012, and April 2–21, 2012; the data is 
provided in Example 3.2.
(Continued )

134 
STATISTICS FOR ECONOMICS
Solution
Let’s mark the data in March with the subscript “1” and those for 
April with subscript “2.” Th e sample proportion of stocks more than 
or equal to $32.00 for each period is given by:

1
13
0.867
15
p =
=
  

2
3
0.2
15
p =
=
Th e Z value corresponding to 95% confi dence is 1.96. Insert these 
values in Equation 6.13 to obtain the results.
LB = (0.867 – 0.2) – 1.96 
0.867(1
0.867)
0.2(1
0.2)
15
15
−
−
+
= 0.667 – 1.96 × 0.1354
= 0.402
UB = (0.867 – 0.2) + 1.96 
0.867(1
0.867)
0.2(1
0.2)
15
15
−
−
+
= 0.667 + 1.96 × 0.1354
= 0.933
Th e range from 0.402 to 0.933 covers the diff erence of the ratios of 
stock prices for Microsoft that is greater than or equal to $32.00 in the 
two periods March 12–30 and April 2–21.
Deﬁ nition 6.9
Based on Chapter 5, the (1– a)% confi dence interval for one population 
variance (s2) is given by the following equation:
 
(
) 
(
) 
2
2
2
2
2
1
2
2
1
1
n
n
a
a
s
s
s
c
c −
−
−
≤
≤
 
(6.14)
Note that the chi-squared distribution is not symmetric, there-
fore we cannot use the ± signs to form the confi dence interval. Also, 
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
135
it is important to note that the term 
2
2
a
c
 refers to the right side of the 
 distribution and, hence, it is larger than 
2
1
2
a
c −
, which refers to the 
left side of the distribution. Dividing the same numerator by a larger 
number provides a smaller result, hence the lower bond; while dividing 
the same numerator by a smaller value gives a larger result, hence the 
upper bond.
Example 6.9
Find the 95% confi dence interval for the variance of stock prices 
for Microsoft. Use the sample data for the period April 2–21, 2012, 
 provided in Example 3.2.
Solution
Th e sample variance for April 2–21 and the chi-squared values for 
0.025 and 0.975 with 14 degrees of freedom are: 
2
0.372182
s =
14
0.025
26.119
c
=
14
0.925
5.629
c
=
Use Equation 6.14 to build the confi dence interval
(
) 
(
)
2
2
2
1
15
1 0.372182
LB
0.1995
26.119
n
a
s
c
−
−
=
=
=
(
) 
(
)
2
2
1
2
1
15
1 0.372182
UB
0.9257
5.629
n
a
s
c −
−
−
=
=
=
Th e range 0.1995 to 0.9257 covers the population variance of Micro-
soft stock prices with 95% confi dence.

136 
STATISTICS FOR ECONOMICS
Deﬁ nition 6.10
Based on Chapter 5, the (1– a)% confi dence interval for the ratio of two 
population variances 
2
2
1
2
s
s  is given by the following equation:
 




2
2
1
1
2
2
2
2
1
2
2
2
1
2
2
F
F
a
a
s
s
s
s
s
s
−
≤
≤
 
(6.15)
Note that since the F distribution is not symmetric, we cannot use the 
± signs to form the confi dence interval. Also, it is important to note that 
term 
2
 Fa  refers to the right side of the distribution and, hence, it is larger 
than 
1
2,
F
a
−
 which refers to the left side of the distribution. Dividing the 
same numerator by a larger number provides a smaller result, hence the 
lower bond, while dividing the same numerator by a smaller value gives a 
larger result, hence the upper bond.
Example 6.10
Find the confi dence interval for the ratio of the variances for the two 
periods March 12–30, 2012, and April 2–21, 2012, for Microsoft 
stock prices.
Solution
Let’s mark the data for March with the subscript “1” and those for 
April with subscript “2.” Th e variances and the F values for 0.025 and 
0.975 with 14 degrees of freedom are: 
2
1
s = 0.10941
2
2
s = 0.372182
14,14
0.025
F
= 2.891479
14,14
0.925
F
= 0.339061
(Continued )

 
POINT AND INTERVAL  ESTIMATION 
137
0.372182
6.591466
0.10941
LB
2.28
2.891479
2.891479
=
=
=
0.372182
6.591466
0.10941
UB
19.44035
0.339061
0.339061
=
=
=
Th e range 2.28 to 19.44035 covers the ratio of variances for the 
two periods for Microsoft stock prices with 95% confi dence. 
Since the confi dence interval does not cover the value “1,” it is 
unreasonable to believe that the variances of the two periods are the 
same. We will address this in more detail in Chapter 7, when we dis-
cuss the test of hypothesis. In light of this fi nding, we should use Equa-
tion 6.9 when testing equality of the means for the two periods, as in 
 Example 6.6.
Determining the Sample Size
In Chapter 4 we showed the necessary sample size for estimating the sin-
gle mean of a population. Th e sample size in that case was obtained by 
algebraic manipulation of the margin of error in Equation 6.4, which is 
repeated below for your reference.
 
2
ˆ
Z
n
a
s
m ±
 
(6.4)
Setting the margin of error equal to a desired margin of error, E, and solv-
ing for n results in the following formula:
 
(
)
2
2
2
2
Z
E
a
s
 
(6.16)

138 
STATISTICS FOR ECONOMICS
Example 6.11
What size sample is needed to be within $0.10 of the actual price if the 
variance is 0.372182 with 95% confi dence?
Solution
2
2
0.372182
1.96
142.98
0.1
n
×
=
=
Th erefore, the necessary sample size is 143. Note that the variance in 
this formula is the population variance. When the population variance 
is unknown, use the sample variance instead, but remember to use the 
t value instead of the Z value.
Similar algebraic manipulations are applied to obtain sample sizes for 
cases with unknown variances, involving, both, the one or two popula-
tion means, proportions, and variances. We will only show the results for 
one population proportions for reference. 
 
(
)
2
2
2
ˆ
ˆ
(1
) Z
n
E
a
p
p
−
=
 
(6.17)
Example 6.11
What size sample is needed to be within 5% of the population 
proportion with 95% confi dence when the sample proportion is 0.4? 
Solution
×
=
=
2
(0.4
0.6)1.96
372
0.05
.182
n
Th erefore, the necessary sample size is n = 369.

 
POINT AND INTERVAL  ESTIMATION 
139
Inference with Conﬁ dence Intervals
Th e primary objective of statistics is to make inferences about population 
parameters using sample statistics. Using sample statistics to make deduc-
tions about population parameters is called statistical inference. Statistical 
inference can be based on point estimation, confi dence interval, or test of 
hypothesis. Th ese are closely related and in some aspects they are inter-
changeable. Th e inference can be based on the estimation theory or deci-
sion theory. Test of hypothesis is a tool for decision theory. Th e estimation 
theory consists of point estimation and interval estimation. Th is section 
will deal with the confi dence interval.
Population parameters are unknown and constants. Sample sta-
tistics, which are random by nature, are used to provide estimates 
of population parameters. If sampling is random, then the sample 
statistics is a good estimate of the corresponding population param-
eter. A good sample statistics has desirable properties, as discussed 
in Chapter 3. Th ese statistics are called point estimates because they 
Table 6.1. Summary of Conﬁ dence Interval for One Population 
Parameter
Parameter
Statistics
Distribution
Variance
Conﬁ dence  interval
m
mˆ  
Normal
Known
2
ˆ
Z
n
a
s
m ±
Unknown
2
ˆ
ˆ
t
n
a
s
m ±
Unknown
Known
2
ˆ
ˆ
t
n
a
s
m ±
Unknown
2
ˆ
ˆ
t
n
a
s
m ±
p
p
Normal or 
Unknown
Always 
Known
2
ˆ
ˆ
(1
)
ˆ
Z
n
a
p
p
p
−
±
s2
2
s
Normal
Always 
Unknown
(
) 
(
) 
2
2
2
2
2
1
2
2
1
1
n
n
a
a
s
s
s
c
c −
−
−
≤
≤
 

140 
STATISTICS FOR ECONOMICS
Table 6.2. Conﬁ dence Interval for Two Samples
Parameter Statistic
Status of 
variances
Conﬁ dence interval
m1–m2


1
2
m
m
−
Known and 
Unequal


2
2
1
2
1
2
2
1
1
Z
n
n
a
s
s
m
m
−
±
+
Known and 
Equal


2
1
2
2
1
2
1
1
(
)
Z
n
n
a
m
m
s ⎛
⎞
−
±
+
⎜
⎟
⎝
⎠ 
Unknown and 
Unequal




2
2
1
2
1
2
2
1
2
(
)
 
  
t
n
n
a
s
s
m
m
−
±
+
Unknown and 
Equal



2
1
2
2
1
2
1
1
(
)
Pooled
t
n
n
a
m
m
s
⎛
⎞
−
±
+
⎜
⎟
⎝
⎠
p1 – p2


1
2
p
p
−
Always 
 Unknown






1
1
2
2
1
2
2
1
2
(1
)
(1
)
(
)
Z
n
n
a
p
p
p
p
p
p
−
−
−
±
+
2
1
2
2
s
s


2
1
2
2
s
s
Always 
 Unknown




2
2
1
1
2
2
2
1
2
2
2
2
1
2
2
F
F
a
a
s
s
s
s
s
s
−
≤
≤
provide a single value as the estimate of the population parameter. If 
the estimator is “good” then it should be close to the unknown true 
value of the population parameter. Th e single estimate does not indi-
cate proximity to the true parameter or probability of being close to 
the true parameter. Confi dence intervals give, both, an idea of actual 
value of population parameter and also a probability, or a level of con-
fi dence, that the interval includes the population parameter.

CHAPTER 7
Statistical Inference with 
Test of Hypothesis
Choose Evidence with High Probability
One of the methods of drawing inference about population parameters 
using sample statistics is by testing the hypothesis about the parameters. 
With proper sampling techniques, a point estimate provides the best esti-
mate of the population parameter. Interval estimation provides a desired 
level of probability of level of confi dence in the estimate. Test of hypo-
thesis is used to make assertions on whether a hypothesized parameter can 
be refuted based on evidence from a sample. 
Using sample statistics to make deductions about population param-
eters is called statistical inference. Statistical inference can be based on 
point estimation, confi dence interval, or test of hypothesis. Th ey are 
closely related and in some aspects they are interchangeable. Th is section 
will deal with test of hypothesis. Th e technical defi nition of a hypothesis 
is based on the distributional properties of random variables.
Th e purpose of test of hypothesis is to make a decision on the valid-
ity of the value of a parameter stated in the null hypothesis based on 
the observed sample statistics. Recall that parameters are constant and 
unknown while statistics are variable and known. You calculate and 
observe the statistics. If the observed statistics is reasonably close to the 
hypothesized value, then nothing unexpected has happened and the 
(minor) diff erence can be attributed to random error. If the observed 
statistics is too far away from the hypothesized value, then either the null 
hypothesis is true and something unusual with very low probability hap-
pened, or the null hypothesis is false and the sample consists of unusual 
observations. Following sampling procedures and obtaining a random 
sample reduces the chance of an unusual sample, which may still occur 

142 
STATISTICS FOR ECONOMICS
in rare occasions. Th erefore, proper sampling assures that unusual sample 
statistics have low probabilities. On the other hand, sample statistics rep-
resenting the more common possibilities would have a high probability 
of being selected.
Statistical inference consists of accepting outcomes with high 
probability and rejecting outcomes with low probability. If the 
sample statistics does not contradict the hypothesized parameter, then 
the hypothesized parameter should not be rejected. If the probability 
of observed statistics is low we reject the null hypothesis; otherwise 
we fail to reject the null hypothesis. In order to fi nd the probability 
of an occurrence for sample statistics we need to know its sampling 
 distribution.
Hypothesis
A hypothesis is formed to make a statement about a parameter. Although 
in English language terms such as “statement” and “claim” may be used 
interchangeably, in statistics, as will be explained shortly, we use the word 
“claim” only about the alternative hypothesis. 
Deﬁ nition 7.1
A statistical hypothesis is an assertion about the distribution of one or 
more random variables. When a hypothesis completely specifi es the dis-
tribution, it is called a simple statistical hypothesis; otherwise, it is called 
a composite statistical hypothesis.
Hypotheses are customarily expressed in terms of parameters of the 
corresponding distribution function. If the parameter is set equal to 
a specifi c value it is a simple hypothesis; otherwise it is a compos-
ite hypothesis. For example, the hypothesis that the average price of 
a  particular stock is $33.69 is written as m = 33.69. Th is is a sim-
ple hypothesis. However, the hypothesis that the average price of a 
 particular stock is less than $33.69 is written as m < 33.69. Th is is 
a composite hypothesis because it does not completely specify the 
distribution.

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
143
Deﬁ nition 7.2
When t he hypothesis gives an exact value for all unknown parameters 
of the assumed distribution function, it is called a simple hypothesis; 
otherwise the hypothesis is composite. 
In this text we deal with the simple hypothesis exclusively. 
Null Hypothesis
Th e null hypothesis refl ects the status quo. It is about how things have 
been or are currently. For example, the average life of a car is 7 years. 
Th e null hypothesis can be a statement about the nature of something, 
such as, “an average man is 5'10" tall.” Th e null hypothesis might be the 
deliberate setting of equipment, such as, “a soda-dispensing machine 
puts 12 ounces of liquid in a can.” A statistical hypothesis is not limited 
to the average only. We can have a hypothesis about any parameter of a 
distribution function, such as, 54% of adults are Democrats; the variance 
for weekly sales is 50. Th e symbol for a null hypothesis is H0, pronounced 
h-sub-zero. Th e following represent the previous examples in the cus-
tomary notation of the hypothesis. Note that the stated null hypotheses 
are simple hypotheses as we will only address tests of simple hypotheses.
Single Mean
H0: m = 7 
H0: m = 5'10"
H0: m = 12
Single Proportion
H0: p = 0.54
Single Variance
H0: s2 = 50
In summary, the null hypothesis is a fact of life, the way things 
have been, or a state of nature. Th is includes the setting of  machinery, or 
deliberate calibration of equipment. If the fact of life, the state of nature, 

144 
STATISTICS FOR ECONOMICS
the setting of machinery, the calibration of equipment has not changed, 
or there is no doubt about them, then the null hypothesis is not tested. 
When you purchase a can of soda that states it contains 12 ounces of 
drink you do not check to see if it does actually contain 12 ounces; you 
have no doubt about it, so you do not test it. Many such “null hypoth-
eses” are believed to be true and, hence, not tested. It is tempting to state 
that the manufacturer is making the “claim” that the can contains 12 
ounces; however, their statement is more of an assertion or a promise 
and not a claim. As we will see shortly, the alternative hypothesis is the 
claim of the researcher, which is also known as the research question. 
Th e null hypothesis for a simple hypothesis is always equal to a constant. 
Th e format is:
 
H0: A parameter = A constant 
(7.1)
In order to test a hypothesis, we either have to know the distribution 
function or the random variables. If the distribution function has more 
than one parameter, we need to know the other parameter(s); otherwise 
we will be dealing with a composite hypothesis.
In a hypothesis, testing the expected value of the outcome of an 
experiment is the hypothesized value. Th e hypothesized value refl ects the 
status quo and will prevail until rejected. In practice, the observed value is 
indeed a statistics obtained from a sample of reasonable size. Care must 
be taken. 
When testing a hypothesis about variance, note that the constant 
2
0
(
)
s
 should be a non-negative number. In practice, zero variance is not 
a reasonable choice and the value is usually positive. Th is parameter is 
diff erent than the mean or proportion, and it has a chi-squared distribu-
tion. Th erefore, its test statistics will be very diff erent from the others. 
Null  Hypothesis for Equality of Two Parameters
Th e test of hypothesis can be used to test the equality of parameters from 
two populations. Let q1 and q2 be two parameters from two populations. 
With no prior information the two parameters would be assumed to be 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
145
the same until proven otherwise. For example, the average productivity of 
a man and a woman would be the same until proven otherwise. Th e null 
hypothesis should not be written as:
 
H0: q1 = q2 
(7.2)
Th is hypothesis is setting one parameter equal to the other, which 
makes it a composite hypothesis. Using algebra the hypothesis can be 
modifi ed to convert it to a simple hypothesis. Th ere are two possi-
ble modifi cations. Equation (7.2) can be written in the following two 
forms. 
 
H0: q1 − q2 = 0, or 
(7.3)
 
q
q =
1
0
2
 
H :
 1 
(7.4)
Th e only thing that remains to be established is that the diff erence 
of two parameters or the ratio of two parameters is also a parameter, and 
that we have an appropriate distribution function to use as test statistics. 
We have accomplished these in Chapter 5, but will reinforce them in this 
chapter as well. 
Null Hypothesis of Two Means
Th e representations in Equations 7.3 and 7.4 are to test the equality of 
two parameters. Depending on the parameters, we may use one or the 
other representation based on availability of a distribution function. 
In  Chapter  5, we showed appropriate distribution functions for the 
 diff erence of two means of random variables, each with a normal distri-
bution. Th e  diff erence of two means from a normally distributed function 
is a para meter, and if the two are equal, their diff erence will be zero. Th is 
will allow the use of normal distribution for testing the following hypoth-
esis, which is identical to H0: m1 = m2.
 
H0: m1 − m2 = 0 
(7.5)

146 
STATISTICS FOR ECONOMICS
Null Hypothesis of Two Proportions
In Chapter 5, we showed that if a population has a normal distribution 
with proportion p1 and another population has a normal distribution with 
proportion p2, the diff erence of the two parameters will also have a normal 
distribution with the proportion p1 − p2. Th e diff erence of the two propor-
tions of two normal populations is a parameter, and if the two are equal, 
their diff erence will be zero. Th is will allow the use of normal distribution for 
testing the following hypothesis, which is identical to H0: p1 = p2.
 
H0: p1 − p2 = 0 
(7.6)
Null Hypothesis of Two Variances 
2
1
s
In Chapter 5, we showed that s2 has a chi-squared distribution. Th e 
ratio of s2
1 to s2
2, after each is divided by its degrees of freedom, has an 
F distribution (see Chapter 5). Th e F distribution will have degrees of 
freedom associated with the corresponding numerator and denominator 
chi-squared distributions. Th e ratio of two variances is a parameter with F 
distribution, and if they are equal then their ratio will be equal to 1. Th is 
will allow the use of F distribution for testing the following hypothesis, 
which is identical to 
s
s
=
2
2
1
2
0
H :
.
 
2
2
0
1
2
H :
1
s
s
=  
(7.7)
Alternative Hypothesis
Th e alternative hypothesi  s is the claim a researcher has against the null 
hypothesis. It is the research question or the main purpose of the research. 
Th e formation of the null and the alternative hypotheses are the main 
problem of the novice. Remember the following:
• Th e null hypothesis is always of this form: 
A parameter = A constant when we deal with simple hypothesis. 
All of the previous null hypotheses are simple hypothesis. 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
147
• Th e claim might be that the parameter is greater than (>), 
less than (<), or not equal to (≠) a constant, which refl ects the 
claim that the parameter has increased (>), decreased (<), or it 
simply has changed (≠).
• Null means something that nullifi es something else. 
 Th erefore, the null hypothesis is the value that nullifi es the 
alternative hypothesis. In all three cases the relationship that 
nullifi es the (>), (<), and (≠) is the (=) sign. 
Th e alternative hypothesis is designated by H1 and is pronounced 
h-sub-one or alternative hypothesis. Th e only time a hypothesis is formed 
and consequently tested is when there is a doubt about null hypothesis.
How to Determine the Alternative Hypothesis
Th e claim of the research, that is, the research question, determines the 
alternative hypothesis. Every alternative hypothesis is a claim that the 
null hypothesis has changed. When the claim is that the value of param-
eter in the null hypothesis has declined, then the appropriate sign is the 
“less than” sign (<). Focus on the meaning and not the wording. When 
the claim is that the value of the parameter in the null hypothesis has 
increased, then the appropriate sign is the “greater than” sign (>). Th ese 
two alternative hypotheses are known as one-tailed hypotheses. When 
the claim is not specifi c, or is indeterminate, then the appropriate sign 
is the “not equal” sign (≠). Th is is known as a two-tailed hypothesis. 
None of the three alternative cases include an equal sign (=), because the 
equal sign nullifi es all of the above signs. Furthermore, in order to draw 
inference at this introductory level, the null hypothesis must be a simple 
hypothesis, which takes the form H0: A parameter = A constant.
Alternative Hypothesis for a Single Mean
• A consumer advocacy group claims that car manufacturers are 
cutting corners to maintain profi tability and make inferior 
cars that do not last as long.
H0: m = 7
H1: m < 7

148 
STATISTICS FOR ECONOMICS
• Men are getting taller because of better nutrition and more 
exercise.
H0: m = 5'10"
H1: m > 5'10"
• Th e quality manager would like to know if the calibration of 
soda dispensing machine is still correct.
H0: m = 12
H1: m ≠ 12
Alternative Hypothesis for a Single Proportion
• A political science researcher believes that due to globalization 
of the economy and political turmoil around the world, the 
percentage of Democrats has declined.
H0: p = 0.54
H1: p < 0.54
Alternative Hypothesis for a Single Variance
• Increased promotional advertising has increased the variance 
of weekly sales.
H0: s2 = 50
H1: s2 > 50
Th e alternative hypothesis is the claim someone has against the status 
quo. If there is no claim, then there is no alternative hypothesis and, 
hence, no need for a test. Th e nature of the claim determines the sign of 
the alternative. In the alternative hypothesis the parameter under consid-
eration can be less than, greater than, or not equal to the constant stated 
in the null hypothesis. Th e sign of the alternative hypothesis depends on 
the claim and nothing else.
Test Statistics
In Chapter 5 we saw that when statistics is a sample mean (mˆ), a  sample 
proportion (pˆ), two sample means 

m
m
−
1
2
(
), or two sample  proportions 


p
p
−
1
2
(
), the Central Limit Th eorem asserts that each of these  sample 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
149
 statistics have a normal distribution. Sample variance 

s2
(
) has a 
 chi-squared distribution. Th e ratio of two sample variances 

(
)
2
2
1
2
s
s
 has 
an F distribution. Cases involving two statistics test their equality. Th e 
test statistics for single mean (m), two means (m1 – m2), single proportion 
(p), and two proportions (p1 – p2) is provided by
 
Observed – Expected
Test statistics
Standard deviation of observed
=
 
(7.8)
Th e statistics obtained from the sample provides the observed portion 
of the formula. Th e null hypothesis provides the expected value. Th e other 
name for standard deviation of the observed value is standard error. 
Th e Central Limit Th eorem provides the distribution function and the 
standard error. Th e sampling distribution of sample statistics covered in 
Chapter 5 provides a summary of parameters, statistics, and  sampling 
Table 7.1. Summary of Null and Alternative Hypothesis
Case
Null hypothesis
Alternative 
hypothesis
Comments
Single Mean
m = A constant
m > A constant
m < A constant
m ≠ A constant
The claim  determines 
the sign of the 
 alternative hypothesis.
Single 
 Proportion
p = A constant
p > A constant
p < A constant
p ≠ A constant
The claim  determines 
the sign of the 
 alternative hypothesis.
Single 
 Variance
s2 = A constant
s2 > A constant
s2 < A constant
s2 ≠ A constant
The claim  determines 
the sign of the 
 alternative hypothesis.
Two Means
m1 − m2 = A constant
m1 − m2 > A constant
m1 − m2 < A constant
m1 − m2 ≠ A constant
Use to test the equality 
of two means.
Two 
 Proportions
p1 − p2 = A constant
p1 − p2 > A constant
p1 − p2 < A constant
p1 − p2 ≠ A constant 
Use to test the equality 
of two proportions.
Two 
 Variances
A constant
s
s =
2
1
2
2
 
2
1
2
2
A constant
s
s >
Use to test the equality 
of variances. Usually, 
no other alternative is 
tested.

150 
STATISTICS FOR ECONOMICS
variances of one and two populations. Whether the correct statistics for 
this hypothesis is Z test or t test depends on whether the population vari-
ance is known. Use the Z test when the po pulation variance is known, 
or when it is unknown and sample size is large. Note that Z statistics can  
only be used to test hypothesis about one mean, one proportion, two 
means, or two proportions. In the case of two means or two proportions, 
the hypotheses must be modifi ed to resemble a simple hypothesis. Use 
t statistics when the population variance is unknown and sample size is 
small. Note that t statistics can only be used to test hypotheses about one 
mean, one proportion, two means, or two proportions.
Th e test statistics for a single variance (s2) is given by
 
(
) 
s
c
s
−
=
2
2 
2
0
1
n
 
(7.9)
Th e test statistics for equality of two variances (
)
2
2
1
2
s
s  is given by 
 


s
s
=
2
1
2
2
F
 
(7.10)
Th e actual tests are provided in Table 7.2, which is a summary of tools 
developed in Chapters 5, 6, and 7.
All the null hypotheses are set equal to a constant. In the case of 
equality of two means and two proportions, the constant is zero. In 
the case of equality of two variances, the constant is one. Subscript zero 
represents the hypothesized null value, which is a constant.
Statistical Inference
Everything in this chapter up to this point was in preparation for con-
ducting statistical inferences. Th ere are two approaches for testing a 
hypothesis. Th e fi rst one is the method of P value and the second is the 
method of critical region. Th e two approaches are similar, but fi rst we 
need to explain the concept of inferential statistics.
Any event that has a probability of occurrence will occur. Some 
events have higher probability of occurrence than others, so they will 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
151
Table 7.2. Test Statistics for Testing Hypotheses
Case
Null hypothesis
Variance
Test statistics
Single Mean
m
Known
0
ˆ
ˆ
Z
n
m
m
s
−
=
Unknown
0
ˆ
ˆ
t
n
m
m
s
−
=
Single 
 Proportion
p 
Unknown
0
0
0
(1
ˆ
)
Z
n
p
p
p
p
−
=
−
Single 
 Variance
s2 
Unknown
(
) 2
2
0
1
n
s
c
s
−
=
Two Means
m1 – m2
Known and 
Unequal


(
)
1
2
1
2
2
2
1
2
1
2
(
)
Z
n
n
m
m
m
m
s
s
−
−
−
=
+
Known and 
Equal


(
)
1
2
1
2
2
1
2
(
)
1
1
Z
n
n
m
m
m
m
s
−
−
−
=
+
Unknown 
and Unequal


(
) (
)


1
2
1
2
2
2
1
2
1
2
t
n
n
m
m
m
m
s
s
−
−
−
=
+
Unknown 
and Equal


(
)

1
2
1
2
2
1
2
(
)
1
1
Pooled
t
n
n
m
m
m
m
s
−
−
−
=
+
Two 
 Proportions
p1 – p2
Unknown


(
)
1
2
1
2
1
1
2
2
1
2
(
)
(1
)
(1
)
Z
n
n
p
p
p
p
p
p
p
p
−
−
−
=
−
−
+
Two Variances
2
1
2
2
s
s  
Unknown


2
1
2
2
F
s
s
=
occur more often. Th e essence of statistical inference is that events that 
have high probability of occurrence are assumed to occur while events 
with low probability of occurrence are assumed not to occur. To clar-
ify, take the probability of having an accident while going through an 

152 
STATISTICS FOR ECONOMICS
 inter section. Th e probability of having an accident crossing an intersec-
tion when the traffi  c light is green is much lower than the probability of 
having an accident crossing the intersection when the traffi  c light is red. 
Th e statistical inference in this case would be that the chance of having 
an accident while crossing an intersection when the light is green is neg-
ligible, so we should go through an intersection when the light is green. 
On the other hand, the probability of having an accident when the light 
is red is high so we should not go through a red light. Note that there is 
still a chance that you go through a green light at an intersection and have 
an accident. It is also possible to go through a red light without having an 
accident. Th ese possibilities have low probabilities, so we “assume” they 
will not occur. Th is example has a special twist to it. Note that for every 
car involved in an accident while crossing an intersection when the light 
was green, the other party to the accident must have gone through a red 
light. Th e null and alternative hypothesis can be expressed as
H0: Going through green light does NOT cause an accident;
H1: Going through green light does cause an accident.
Note that the null hypothesis here is not a simple hypothesis because it is 
not of the form:
A Parameter = A Constant
Th erefore, this hypothesis cannot be tested by Z or t statistics, at least in 
its present form. However, it is suffi  cient to explain the concept.
Types of Error
Th e process of testing a hypothesis is similar to convicting a criminal. Th e 
null hypothesis is a conjecture to the eff ect that everybody is assumed 
to be innocent unless proven otherwise. If there is any reason to doubt 
this innocence, a claim is made against the null hypothesis, which is 
called an alternative hypothesis. Th e type of crime is decided, as indicated 
by charges of misdemeanor, felony, and so forth, which is similar to test 
statistics. Within this domain the evidence is collected, which is the same 
as taking a sample. Finally, based on the evidence a judgment is made, 
either innocent or guilty. If the prosecutor fails to provide evidence that 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
153
the person is guilty, it does not mean that the person is innocent. Th e 
degree or the probability that the person was innocent (but was con-
victed) is the probability of type I error, or the P value.
We start by assuming the null hypothesis that the suspect is innocent. 
We calculate a test statistics using the null hypothesis. Th is is similar to 
presenting evidence in the legal system assuming the suspect is innocent. 
Th en the test statistics is compared to the norm, provided by the appro-
priate statistical table. Th e null hypothesis of innocence is rejected if the 
probability of being innocent is low in light of evidence. Otherwise, we 
fail to reject the null hypothesis. Suppose we had a case that the prob-
ability was low enough and we actually rejected the null hypothesis. Th e 
basis for rejecting the null was low probability, but the observed statistics 
was nevertheless possible. It is possible that the null hypothesis is true, a 
sample statistics with low probability was observed, and we erroneously 
rejected the null hypothesis. Th is kind of error is known as type I error. 
Deﬁ nition 7.3
Type I Error occurs when the null hypothesis is true but is rejected.
Deﬁ nition 7.4
 Type II Error occurs when the null hypothesis was false but was not 
rejected. 
It is not possible to commit type I error if the null hypothesis is not 
rejected. It is not possible to commit type II error if the null hypothesis is 
rejected. Th ere is a type III error, which will be discussed shortly. 
Deﬁ nition 7.5
Type III error is rejecting a null hypothesis in favor of an alternative 
hypothesis with the wrong sign.
Table 7.3. Summary of Types of Error in Inference
H0 Rejected
H0 Not rejected
H0 is True
Type I Error
No Error
H1 is True
No Error
Type II Error

154 
STATISTICS FOR ECONOMICS
 Statistical Inference with the Method of P value
Th e observed value of sample statistics, such as sample mean and sample 
proportion, can be converted to a standardized value such as Z or t. Under 
null hypothesis, observed statistics should be close to the corresponding 
population parameter. Th is means that the corresponding standardized value 
should be close to zero. Recall that the numerators of Z and t are the diff er-
ence between sample statistics and population parameter, which is also called 
individual error. Th e further the calculated statistics is from the param-
eter, the larger the value of the standardized sample statistics is. In other 
words, the test statistics become larger when the observed statistics is further 
from the hypothesized parameter. Th e area under the curve from the value of 
the test statistics refl ects the probability of observing that or a more extreme 
value if the null hypothesis is correct. Th is probability, refl ecting the area 
under the tail area of the normal or t distribution, refl ects the probability 
of observing a more extreme value than the observed statistics. Th e smaller 
this probability is, the less likely that the null hypothesis is correct. Recall 
that in statistics, as in real life, we assume that events that are not likely will 
not occur, which is the same as saying that events that occur are more likely. 
Th erefore, the statistics that is observed from a random sample must be more 
likely to occur than other events. Th e area under the tail area corresponding 
to the extreme values actually represents the likelihood of the event. 
Deﬁ nition 7.6
Th e value representing the probability of the area under the tail-end of 
the distribution is called the p value. Th is gives rise to the following rule 
for statistical inference.
Rule 7.1
Reject the null hypothesis when the p value is small enough.
Since it is possible that the unlikely event has occurred, the above rule 
will always be wrong when the sample statistics is the result of a rare sam-
ple outcome. Th us, in such cases, rejecting the null hypothesis will result 
in type I error. Fortunately, by defi nition, this erroneous conclusion will 

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
155
seldom happen. Th e exact probability of committing such a type I error is 
actually equal to the area under the tail-end of the distribution. 
Deﬁ nition 7.7
Th e P value is equal to the probability of type I error. It is also called the 
Observed Signifi cant Level (OSL).
Type I error can only occur if we reject the null hypothesis. Th is might 
lead to the decision to make type I error very small, but not rejecting the 
null hypothesis unless the p value, that is, the probability of type I error, 
is very small. Th e problem with this strategy is that it increases the prob-
ability of not rejecting the null hypothesis when it is false, which means 
the probability of type II increases. Th ere is a tradeoff  between type I 
and type II errors: decreasing one increases the other. Th e tradeoff  is not 
linear, which is the same as saying that they do not add up to one. Th e 
only possible way to reduce both type I and type II error is by increasing 
the sample size. 
Signifi cance level indicates the probability or likelihood that observed 
results could have happened by chance, given that the null hypothesis 
is true. If the null hypothesis is true, observed results should have high 
probability. Consequently, when p value is high, there is no reason to 
doubt that the null hypothesis is true. However, if the observed results 
happen to have a low probability, it casts doubt about the  validity of 
the null hypothesis because we expect high  probability events to occur. 
Since the outcome has occurred by virtue of being observed, they imply 
that the null hypothesis is not likely  to be true. In other words,  p 
value is the probability of seeing what you saw, which is refl ected in the 
other common name for p value, OSL.
 Statistical Inference with Method of Critical Region
An alternative approach to the decision rule of P value is to calculate a 
critical value and compare the test statistics to it. In order to obtain a 
critical value, decide on the level of type I error you are willing and able 
to commit, for example 2.5%. Look up that probability in the body of 
the table such as a table for normal distribution. Read the corresponding 

156 
STATISTICS FOR ECONOMICS
Z score from the margins. Th e Z score corresponding to the chosen 
level of type I error is the critical value.
Rule 7.2
Reject the null hypothesis when the test statistics is more extreme than 
the critical value. 
As long as the same level of type I error is selected, the two methods 
result in the same conclusion. Th e method of P value is preferred because 
it gives the exact probability of type I error, while in the method of criti-
cal region the probability of type I error is never exact. If the test statistics 
is more extreme than the critical value, the probability of type I error is 
less than the selected probability. Another advantage of P value is that it 
allows the researcher to make a more informed  decision.
Steps for Test of Hypothesis
1. Determine the scope of the test
2. State the null hypothesis
3. Determine the alternative hypothesis
4. Determine a suitable test statistics
5. Calculate the test statistics
6. Provide inference
Test of Hypothesis with Conﬁ dence Interval
We covered confi dence intervals in Chapter 6 when discussing estimation. 
Confi dence intervals can be used to test a two-tailed hypothesis. Proceed to 
calculate the confi dence interval based on the desired level of signifi cance, 
as shown in Chapter 6, and apply the following rule to draw inference.
Rule 7.3
Reject the null hypothesis when the confi dence interval does NOT cover 
the hypothesized value. Fail to reject when the confi dence interval does 
cover the hypothesized value.

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
157
Whether the confi dence interval is for one parameter or two does 
not matter. Rule 7.3 applies to all confi dence intervals regardless of the 
parameter. So it can be used for a two tailed test of hypothesis of one or 
two parameter confi dence interval for means, percentages, or variances. 
Th e approach is based on the critical region method.
Th e coverage of the test of hypothesis completes the set of tools 
needed  for making inferences. We now provide examples for tests of 
hypotheses for one mean, one proportion, and one variance. Th en we 
give examples for tests of hypotheses for the equality of two means, two 
proportions, and two variances.
Since we will be using the stock price data, we have reproduced the 
same data for your convenience.
Date
WMT
MSFT 
Date
WMT
MSFT
12 Mar.
$60.68 
$32.04
2 Apr.
$61.36
$32.29
13 Mar.
$61.00 
$32.67
3 Apr.
$60.65
$31.94
14 Mar.
$61.08 
$32.77
4 Apr.
$60.26
$31.21
15 Mar.
$61.23 
$32.85
5 Apr.
$60.67
$31.52
16 Mar.
$60.84 
$32.60
9 Apr.
$60.13
$31.10
19 Mar.
$60.74 
$32.20
10 Apr.
$59.93
$30.47
20 Mar.
$60.60 
$31.99
11 Apr.
$59.80
$30.35
21 Mar.
$60.56 
$31.91
12 Apr.
$60.14
$30.98
22 Mar.
$60.65 
$32.00
13 Apr.
$59.77
$30.81
23 Mar.
$60.75 
$32.01
16 Apr.
$60.58
$31.08
26 Mar.
$61.20 
$32.59
17 Apr.
$61.87
$31.44
27 Mar.
$61.09 
$32.52
18 Apr.
$62.06
$31.14
28 Mar.
$61.19 
$32.19
19 Apr.
$61.75
$31.01
29 Mar.
$60.82 
$32.12
20 Apr.
$62.45
$32.42
30 Mar.
$61.20 
$32.26
21 Apr.
$59.54
$32.12
Mean
$60.89
$32.32
$60.82
$31.27
Variance
0.056464
0.109413
0.826873
0.372182
St. Dev
$0.24
0.330777
0.909325
$0.61
Th e point estimates at the bottom of the table are calculated using 
Excel, some of which are off  by a small margin.

158 
STATISTICS FOR ECONOMICS
Example 7.1
An investor would purchase Microsoft stock if the average price 
exceeded $32.00. Using the data from March 12 to March 30, would 
he buy the stock?
Solution
Based on the statement in the problem the alternative hypothesis is:
H1: m > 32 ⇒ H0: m = 32
From previous examples we have the following statistics that are 
obtained from Excel.
ˆ
32.32
m =
  
2
0.1094
s
=
Since population variance is unknown we need to use
m
m
s
−
−
=
=
=
=
0
ˆ
32.32
32
0.32
ˆ
0.085401
0.1094
3.74
2
15
70 8
t
n
Using the following Excel command we obtain the exact P value.
= t.dist.rt(3.747028,14) = 0.001083
Th e probability of obtaining an average of $32.32, if the true 
population average is $32.00, is only 0.001083. Th is is a low prob-
ability. Th erefore, we reject the null hypothesis in favor of the alterna-
tive hypothesis. Alternatively, we could say that the probability of type 
I error, if we reject the null hypothesis, is only 0.001083 and hence, 
we reject the null hypothesis. If you copy the value of the “t” into 
the formula for the P value you will get the same number as shown 
above, that is 0.001083. However, if you type in the rounded number, 
which is “3.74,” you would get “0.001098.”  Th e fi rst number is more 
 accurate because it uses the precise answer for “t.”

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
159
Example 7.2
Test the claim that more than 50% of the stock prices for Microsoft close 
higher than $32.20. Use the sample from March 12 to March 30, 2012. 
Solution
Sorting the data makes it easier to obtain the portion of sample prices 
over $32.20.
8
ˆ
0.53
15
p =
=
Based on the statement in the problem the alternative hypothesis is:
H1: π > 0.50 ⇒ H0: π = 0.5
From Table 7.2 the correct formula is:
0
0
0
ˆ
0.53
0.50
0.03
0.23
0.13
(1
)
0.50
0.50
15
n
Z
p
p
p
p
=
−
−
=
=
=
−
×
Th e probability of the region more extreme than Z = 0.23 is given by
P (Z > 0.23) = 0.5 – P (0 < Z < 0.23) = 0.5 – 0.0910 = 0.4090
Since the probability of type I error, if we reject the null hypothesis, is 
too high, we fail to reject the null hypothesis. 
Example 7.3
Test the claim that the variance for the stock prices of Microsoft is 
greater than 0.9. Use the sample from March 12 to March 30, 2012.
Solution
Based on the statement in the problem the alternative hypothesis is:
H0: s2 > 0.90 ⇒ H1: s2 = 0.90
(Continued )

160 
STATISTICS FOR ECONOMICS
From Table 7.2 the appropriate formula is:
(
) 
(
)
2
2
0
1
15
1 (0.10941)
1.531785
1.702
0.90
0.90
n
s
c
s
−
−
=
=
=
=
Using the following Excel command we obtain the P value:
= chisq.dist.rt(1.702,14) = 0.99997
Since the probability of type I error is too high, we fail to reject the 
null hypothesis.
(Continued )
Example 7.4
Are the means for Microsoft stock prices for periods March 12–30 and 
April 2–21 the same?
Solution
Th e objective is to determine if m1= m2. Since this format is not of the 
form of a parameter equal to a constant, we rewrite the hypothesis as:
H0: m1− m2 = 0  H1: m1− m2 ≠ 0
Since no particular directional claim has been made, the test is a two-
tailed test. Th e following information is available:
 

1
32.32
m =
 

2
32.27
m =
2
1
0.10941
s
=
  
2
2
0.372182
s
=
Since we do not know whether the variances are equal, we will test their 
equality fi rst, that is, 
2
2
1
2.
s
s
=
 Since this is of the form of a parame-
ter equal to another parameter, it has to be modifi ed to  resemble a 
 para meter = a constant format. 
(Continued )

 
STATISTICAL INFERENCE WITH TEST OF HYPOTHESIS 
161
2
1
0
2
2
H :
1
s
s
=
  
2
1
1
2
2
H :
1
s
s
>
It is customary to express this alternative hypothesis in the “greater 
than one” format. To assure that the ratio of the sample variances is 
actually greater than one, always place the sample variance that is 
larger in the numerator.
From Table 7.2, the appropriate formula to test the equality of two 
variances is


s
s
=
=
=
2
1
2
2
0.372182
F
0.10941
3.4017
Th e P value for this statistic is obtained from the following Excel 
command:
= f.dist.rt(3.4016,14,14) = 0.0144
Since the probability of type I error is low enough, we reject the null 
hypothesis that the two variances are equal. Th erefore, from Table 7.2, 
the following test statistics is used for testing equality of the mean 
prices for the two periods. 


(
) (
)


(
)
m
m
m
m
s
s
−
−
−
−
−
=
=
=
+
+
=
=
1
2
1
2
2
2
1
2
1
2
32.32
31.27
(0)
1.05
0.10941
0.372182
0.0344
14
14
1.05
0.18547
5.66
2
1232
t
n
n
Using the following Excel command we obtain the exact P value.
= t.dist.rt(5.6612,28) = 0.000,002
Since the P value is low enough, we reject the null hypothesis that 
the average prices of Microsoft stock are the same for the periods of 
March 12–30 and April 2–21.


CHAPTER 8
An Introduction to 
 Regression Analysis
Until this point, with the exception of covariance and correlation 
 coeffi  cient, the focus has been on a single variable. However, there are 
few, if any, economic phenomena that can be analyzed solely using its 
own information. Even in the simplest economic issues such as quan-
tity demanded, there are at least two variables, a quantity and a price. 
 Furthermore, many economic aff airs are too complicated to be analyzed 
and explained fully by merely observing the matter by itself without any 
consideration to other economics, social, cultural, and political factors 
that usually aff ect most economic problems. For example, it does not 
suffi  ce to analyze data on income in order to determine or forecast future 
incomes. Such a study does not provide a reasonable estimate of the cur-
rent situation, let alone future forecasts. Let us explore, briefl y, what other 
factors might aff ect income. First, we have to decide whether the orien-
tation of the study is macro or microeconomics. At the microeconom-
ics level the focus on income can be personal income or family income. 
For an individual, income is zero for many years. During these years the 
individual is growing up and attending school. In other words, he or she 
is acquiring human capital, which will aff ect earned income when the 
individual secures a job. Other factors include one’s natural ability, talent, 
work ethic, exertion, years of experience, and seniority, to name a few. 
Th ere are some other factors that, at least in theory, should not have an 
eff ect on one’s income, but in reality they do, such as gender and race. At 
the macroeconomics level the determinants of income, which in this case 
should be referred to as the national income, are functions of the nation’s 
productivity, its resources, population size, education levels, economic 
cycles, seasonal cycles, and other factors. 

164 
STATISTICS FOR ECONOMICS
Th ere are many powerful tools in statistical analysis that permit incor-
porating the factors that determine a phenomenon such as income. One 
such tool is regression analysis. Th e regression methodology acknowl-
edges that most real life occurrences are subject to random error. Take, 
for example, the income of a person. It is easy to calculate the average 
income of the nation. Let us take a single person and compare his or her 
income to the average. Chances are that the income of the person is not 
going to be the same as the national average. One can argue that it would 
be unreasonable to expect the income of a person chosen at random to be 
the same as the average income of the nation because the person is likely 
to have a diff erent level of education, years of experience, talent, ambi-
tion, health, and so forth than the average person. All of these do aff ect 
income. Th e important thing is that even if we account for all reasonable 
sources of  diff erences between two people, there still will be a diff erence 
in income due to random factors beyond our control, recognition, or 
ability to measure them. Th e presence of diff erence between observa-
tions is nothing new, and we addressed that when we discussed the con-
cept of individual error and error in general throughout this text. In the 
case of comparing a person to the average, most of the diff erence can be 
attributed to the diff erences in the infl uencing factors such as education, 
experience, and talent. In regression analysis, we try to account for these 
sources of infl uence and “explain” part of the error. Recall that a basic 
defi nition of error in statistics is “whatever that cannot be explained.” So 
the deviation of one person’s income from the average income is called 
individual error. In statistics it makes more sense to talk about averages 
because of the existence of random error. Averaging things removes the 
random error, which by defi nition has an expected value or an average 
equal to zero. In Chapter 2, we introduced variance, which gives a meas-
ure of error. In the same chapter we discussed standard deviation, which 
is the average error. In regression analysis we explain the part of the error 
that is caused by diff erences in the factors that aff ect the phenomenon 
of interest, in this case the income. After accounting for the role of all 
the infl uencing factors that determine income, there is a random com-
ponent that remains unexplainable, which by defi nition becomes the 
new “error.”1 Briefl y, the regression analysis is designed to minimize the 
squares of errors of observations from a hypothetical line that provides a 

 
AN INTRODUCTION TO  REGRESSION ANALYSIS 
165
relationship between a dependent variable, such as income, and an inde-
pendent variable such as education. Note that errors are simply devia-
tions of observations from the expected value. In descriptive statistics the 
expected value is simply the mean. In regression analysis, the regression 
line is the expected value.
We can extend the simple regression analysis from the case of one vari-
able to as many variables as deemed necessary. As the number of explana-
tory variables increases, the regression model should be able to explain 
more of the deviations in the dependent variable, which in our example is 
the income. Th is statement is valid only if we have the correct determin-
ing factors, are also called independent variables and, all the appropriate 
independent variables are included without having irrelevant variables in 
the model. In the above example, the independent variables are education, 
years of experience, talent, ambition, and so forth. As mentioned earlier, 
there are factors that infl uence the dependent variable even though they 
should not, such as gender and race, and factors that infl uence income but 
cannot be controlled or anticipated by the model, such as a war, natural 
disaster, and so on. We include these variables in the models as control 
variables. Control variables such as war or natural disaster have good 
economic explanations. Th ey are the variables that are assumed constant 
under the ceteris paribus assumption of economic theory.
Th e simple regression model consists of one dependent variable and 
one independent variable plus an error term.
 
Income = b0 + b1 Education + e 
(8.1)
where, income is the dependent variable,
education is the independent variable,
b0 is the intercept,
b1 is the slope, and
e is the error term.
Th is chapter is a brief introduction to a vast topic.2 Th e independent 
variable is believed to be outside of the model and is not to be explained 
in any manner. Here we are not interested in why people obtain the level 
of education they do. Instead we are observing one’s educational level and 

166 
STATISTICS FOR ECONOMICS
use it to explain his or her income. Th e dependent variable is endogenous 
to the model, which means the model is used to estimate the value of 
the dependent variable based on the level of income. In other words, the 
model determines the income, given a specifi c value of education. Th e 
Greek letters b0 and b1 are the parameters of the model. Th ey are also 
called the intercept and the slope, respectively. Th e interpretation of b1 is 
that for every unit change in education, income will change by the mag-
nitude of b1 and in the direction of its sign. Th e intercept b0 provides an 
estimate of income when one has zero education. Finally, e the error term, 
accounts for everything else that aff ects income, other than education, 
plus the random error inherent in real phenomena.
Th is simplistic model explains income with one variable. Th e hypoth-
esized claim for the slope of the regression line, b1, is that it is expected to 
be positive. Th is claim is based on theory and common sense. One expects 
higher income with more education, since education is a kind of invest-
ment called human capital. An educated person is more knowledgeable 
and hence, more productive, and thus deserves higher income per unit 
of time than someone with less education, other things equal. Th ere are 
several advantages to using regression analysis. One of the more impor-
tant ones is the fact that not only can we measure the contribution of the 
determinants of a dependent variable such as income, but we can also 
test to see if the determinants are actually signifi cant. A typical inference 
about a regression model consists of two diff erent tests. Th e one that tests 
the overall signifi cance of the model is based on the F test. In this case, 
the amount of the variation in the dependent variable that is explained 
by the model is compared to the amount that still remains unexplained. 
Th e portion that is explained by the model is called mean squared regres-
sion (MSR). Recall that the sum of squares of the portions not explained, 
divided by appropriate degrees of freedom is the same as variance, which 
in the jargon of regression analysis is called mean squared error (MSE). In 
Chapter 4, we explained that the variance has a chi-squared distribution. 
Th e MSR also has a chi-squared distribution since it has similar distribu-
tional properties as a variance. Th eorem 4.5 from Chapter 4 states that the 
ratio of two  chi-squared  distribution functions follows an F distribution. 
Th erefore, we can use an F statistics to test the relative magnitude of the 
portion of the variation in the dependent  variable that is explained by the 

 
AN INTRODUCTION TO  REGRESSION ANALYSIS 
167
model to the portion that is not. Th e null and alternative hypotheses for 
the model are: 
H0: Model is not good  H1: Model is good
Th e testing procedure is the same as before. If the p value is small enough, 
reject the null hypothesis; otherwise, fail to reject it. Once the above null 
hypothesis is rejected, then the individual slopes are tested for signifi -
cance. Th e customary null hypothesis is that the slope of interest is zero. 
When a slope is zero it indicates that the corresponding variable does not 
have any explanatory power, and it does not contribute to the reduction 
of the variations in the error.
H0: bEducation = 0  H1: bEducation > 0
Th e appropriate test statistics for this hypothesis is a t statistics. 
Testing procedure is the same as usual. Reject the null hypothesis if the 
corresponding p value is low enough. All software designed to perform 
statistical analysis can perform regression analysis with a relatively easy set 
of commands or procedures, or both. In fact, many of the commercially 
available software are menu-driven, similar to a typical application soft-
ware. Most have reasonably good help features that will show the neces-
sary steps or commands. Even Microsoft Excel, a spreadsheet software, 
has a menu driven procedure to perform regression analysis, to provide 
test statistics for testing the model and slopes, and to provide estimates of 
slopes and the explanatory power of the model.3 
Example 8.1
Let us test the hypothesis that education increases income. As pointed 
out earlier, there are numerous measures of income, from per capita 
personal income to the national income; the former represented a 
microeconomics aspect of income, while the latter is a macroeconom-
ics perspective.4 Care must be taken to analyze the data in light of its 
nature and not to imply or indicate more than the meaning of the 
outcome. Th e data is presented in Table 8.1.

168 
STATISTICS FOR ECONOMICS
Table 8.1. Data on Education and Income from 1970 to 2010 for the 
United States
Year
Total
Elementary
High school
College
Income
Up to 4
5 to 8
<4
>=4
<4
>=4
1975
116,897
4,912
20,633
18,237
42,353
14,518
16,244
1.33E+09
1976
118,848
4,601
19,912
18,204
43,157
15,477
17,496
1.47E+09
1977
120,870
4,509
19,567
18,318
43,602
16,247
18,627
1.63E+09
1978
123,019
4,445
19,309
18,175
44,381
17,379
19,332
1.83E+09
1979
125,295
4,324
18,504
17,579
45,915
18,393
20,579
2.05E+09
1980
130,409
4,390
18,426
18,086
47,934
19,379
22,193
2.29E+09
1981
132,899
4,358
17,868
18,041
49,915
20,042
22,674
2.57E+09
1982
135,526
4,119
17,232
18,006
51,426
20,692
24,050
2.76E+09
1983
138,020
4,119
16,714
17,681
52,060
21,531
25,915
2.94E+09
1984
140,794
3,884
16,258
17,433
54,073
22,281
26,862
3.26E+09
1985
143,524
3,873
16,020
17,553
54,866
23,405
27,808
3.48E+09
1986
146,606
3,894
15,672
17,484
56,338
24,729
28,489
3.68E+09
1987
149,144
3,640
15,301
17,417
57,669
25,479
29,637
3.91E+09
1988
151,635
3,714
14,550
17,847
58,940
25,799
30,787
4.22E+09
1989
154,155
3,861
14,061
17,719
59,336
26,614
32,565
4.54E+09
1990
156,538
3,833
13,758
17,461
60,119
28,075
33,291
4.83E+09
1991
158,694
3,803
13,046
17,379
61,272
29,170
34,026
5.01E+09
1992
160,827
3,449
11,989
17,672
57,860
35,520
34,337
5.34E+09
1993
162,826
3,380
11,747
17,067
57,589
37,451
35,590
5.56E+09
1994
164,512
3,156
11,359
16,925
56,515
40,014
36,544
5.87E+09
1995
166,438
3,074
10,873
16,566
56,450
41,249
38,226
6.19E+09
1996
168,323
3,027
10,595
17,102
56,559
41,372
39,668
6.58E+09
1997
170,581
2,840
10,472
17,211
57,586
41,774
40,697
6.99E+09
1998
172,211
2,834
9,948
16,776
58,174
42,506
41,973
7.52E+09
1999
173,754
2,742
9,655
15,674
57,935
43,176
43,803
7.91E+09
2000
175,230
2,742
9,438
15,674
58,086
44,445
44,845
8.55E+09
2001
180,389
2,810
9,518
16,279
58,272
46,281
47,228
8.88E+09
2002
182,142
2,902
9,668
16,378
58,456
46,042
48,696
9.05E+09
2003
185,183
2,915
9,361
16,323
59,292
46,910
50,383
9.37E+09
2004
186,876
2,858
8,888
15,999
59,811
47,571
51,749
9.93E+09
2005
189,367
2,983
8,935
16,099
60,893
48,076
52,381
1.05E+10
2006
191,884
2,951
8,791
16,154
60,898
49,371
53,720
1.13E+10
2007
194,318
2,830
8,462
16,451
61,490
49,243
55,842
1.19E+10
2008
196,305
2,599
8,226
15,516
61,183
50,994
57,787
1.25E+10
2009
198,285
2,785
8,043
15,587
61,626
51,670
58,574
1.19E+10
2010
199,928
2,615
7,836
15,260
62,456
51,920
59,840
1.24E+10

 
AN INTRODUCTION TO  REGRESSION ANALYSIS 
169
First, we regress income on the total number of people in the 
United States with education, regardless of the level of education.5 See 
Table 8.2. Th e regression output is obtained from Excel. We will focus 
on the row named “Total,” which refers to the name we used for the 
independent variable representing education. Remember that the data 
for education are represented in thousands of people. Th e coeffi  cient 
for the independent variable is 127,095.6414. Th erefore, for every 
1,000 people who are educated, the income increases by $127,095. 
Th e other information in the output indicates that the model is a 
good model.
Next, let us regress the income on the number of people with four or 
more years of college (see Table 8.3). 
Th e coeffi  cient for the independent variable “4 or more years of col-
lege education” is $253,879. Th is indicates that for every 1,000  persons 
obtaining four or more years of college education, income increases by 
Table 8.2. Regression of Income on Total Education from 1970 to 2010 
in the United States
SUMMARY OUTPUT
Regression statistics
Multiple R
0.977959617
R Square
0.956405013
Adjusted R Square
0.955287192
Standard Error
779350434.8
Observations
41
ANOVA
df
SS
MS
F
Signiﬁ cance F
Regression
 1
5.19679E+20
5.2E+20
855.5983
3.81799E−28
Residual
39
2.36881E+19
6.07E+17
Total
40
5.43367E+20
 
 
 
Coefﬁ cients
Standard 
error
t stat
p-value
Intercept
−14108412049
680868741.5
−20.7212
1.18E−22
Total
127095.6414
4345.059321
29.25061
3.82E−28

170 
STATISTICS FOR ECONOMICS
$253,879. Th is fi gure is twice the $127,095 we obtained for overall 
increase in education attainment. Th erefore, as expected, more education 
results in higher income for the country. 
What do you think would happen if we regressed income on 
the number of people with less than 4 years of education? Th e aver-
age number of years of education in the United States is much higher 
than 4 years of education. A person with such a low educational level 
will cause a reduction in the expected income of the country. Let us 
see if the  regression analysis can demonstrate this point. Th e result of 
regressing income on up to 4 years of elementary education is shown 
in Table 8.4.
As anticipated, the coeffi  cient of the independent variable is nega-
tive, indicating that for every 1,000 people who are over 25 years of age 
the income declines by $3,751,795. Other statistics in the table indicate 
Table 8.3. Regression of Income on Four or More Years of College 
Education from 1970 to 2010 in the United States
SUMMARY OUTPUT
Regression statistics
Multiple R
0.9925175
R Square
0.98509099
Adjusted R Square
0.9847087
Standard Error
455762886
Observations
41
ANOVA
df
SS
MS
F
Signiﬁ cance F
Regression
 1
5.35266E+20
5.35E+20
2576.867
3.08304E−37
Residual
39
8.10107E+18
2.08E+17
Total
40
5.43367E+20
 
 
Coefﬁ cients
Standard 
error
t stat
p-value
Intercept
−3120662505
183892483.1
−16.97
1.33E−19
>=4
253879.32
5001.28153
50.76285
3.08E−37

 
AN INTRODUCTION TO  REGRESSION ANALYSIS 
171
that the model is a good model. It is important to caution the reader that 
these models are very simplistic, and there are more issues that have to be 
addressed. A comprehensive study of the impact on education is a huge 
undertaking, and this brief introduction is not suffi  cient to identify the 
true impact of education on income. One shortcoming of these examples 
is that they do not consider other factors that aff ect income, as discussed 
earlier. Th e simple regression analysis is easily extended to include all the 
variables that a researcher deems necessary. Th e main determining fac-
tor for including a variable in a regression model is the theory in the 
discipline in which the research is conducted. In our case, the governing 
theory belongs to the fi eld of economics. 
 
Income = b0 + b1 Education + b2 Experience 
 
(8.2)
 
    + b3 Race + b4 Gender + b5 Determination + e
Table 8.4. Regression of Income on up to 4 Years of Elementary 
 Education
SUMMARY OUTPUT
Regression statistics
Multiple R
0.901856924
R Square
0.813345911
Adjusted R Square
0.808559909
Standard Error
1612624517
Observations
41
ANOVA
df
SS
MS
F
Signiﬁ cance F
Regression
 1
4.41946E+20
4.41946E+20
169.9427
8.5373E−16
Residual
39
1.01422E+20
2.60056E+18
Total
40
5.43367E+20
 
 
Coefﬁ cients
Standard 
error
t stat
p-value
Intercept
19434322304
1099162811
17.68102242
3.2E−20
Up to 4
−3751794.554
287798.0556
−13.03620535
8.54E−16

172 
STATISTICS FOR ECONOMICS
Th e previous model is a typical model based on the variables we 
identifi ed earlier as important. Note that the list is not exhaustive. One 
reason is that social variables are somewhat correlated. Education is not 
really independent of race or gender. Although there is no justifi able 
reason for education to be infl uenced by race or gender, the reality in 
the United States is that it is. Similarly, there is no reason to include race 
and gender as control variables in the model explaining the determinants 
of income. However, the fact is that in the current US society these 
variables do aff ect the level of income, other things equal. Th e second 
reason for limiting the number of variables is that, usually few impor-
tant variables are suffi  cient to provide reasonable estimates or forecasts of 
the dependent variable. Generally, if two models are performing about 
the same, the one with fewer variables is preferred. One variable merits 
additional comments. Th e variable is “determination.” Th ere is no doubt 
that the amount of eff ort that a person puts into his or her job does 
aff ect the resulting income. Th us, the hypothesized value of b5 is posi-
tive. However, there is no acceptable way of measuring one’s resolve or 
how much eff ort one puts into his or her job. It is fairly easy to identify 
those that slack off  or those that exert themselves, but neither can be 
measured. More importantly, any arbitrary ranking or measurement of 
“determination” is inaccurate and incomplete in the sense that it cannot 
be compared because it is a not a cardinal measure. Frequently, we face 
this problem in economics. For example, there is no cardinal measure 
of utility, which is a very important economic concept. Th e discussion 
of how we deal with the inability to measure utility with cardinal meas-
ures is beyond the scope of this text. In the case of variables such as 
“determination” in regression model, we have two alternatives. Th e fi rst 
one is to accept that it is not a measurable phenomenon and not worry 
about including it in the model. Th e consequence is that the error term 
is enlarged, and there will be more of the variation in the dependent 
variable that cannot be explained as compared to the case if we could 
measure “determination” and use it in the model. Th is exclusion has seri-
ous consequences and is usually covered under misspecifi cation of the 
model. Th e second method is to use a proxy variable that could represent 
the desired variable, albeit, not precisely or accurately. Can you think of 
a good proxy for “determination”?

 
AN INTRODUCTION TO  REGRESSION ANALYSIS 
173
A typical way of writing a model with unknown number of variables 
is of the form:
 
Y = b0 + b1X1 + b2X2 + … + bK  XK + e 
(8.3)
Each beta represents the contribution of the corresponding factor to 
an explanation of the dependent variable, keeping all the other factors 
 constant.
Regression analysis is a powerful and useful tool used in many areas of 
science but has a special place in economics. As one might expect, there 
are many issues that pertain to economic reality that need not be applica-
ble to other areas of science. We have already seen two such cases. One is 
the fact that the independent variables are somewhat related to each other 
in economics. Th is is due to the fact that many, if not all economic  factors 
are subject to the economics and social realities of the same country. Th e 
same applies to the individual fi rms and people in a country. In many 
other fi elds, it is much easier to ensure that exogenous variables are inde-
pendent from each other, which is a requirement of regression analysis.6 
Th e second issue is the role of factors that are social in nature and refl ect 
the social/cultural structure of a country. For example, the fact is that race 
and gender infl uence one’s income. Th ere is no theoretical reason for such 
a role, except racism and chauvinism. Consequently, a special branch of 
science has been created called econometrics.


CHAPTER 9
Conclusion
Th is text is a brief introduction to statistics. Th e main focus has been on 
the application, comprehension, interpretation, and a sense of apprecia-
tion for statistics. Th e hope is that the reader has become interested in 
statistics and will pursue the topic further. In fact, the main reason for 
including the regression chapter, Chapter 8, was to show additional pos-
sibilities that go beyond a single-variable analysis. It demonstrates that we 
can explore the infl uence of one or more variables on a variable of interest. 
Within the subject of regression, one can explore theoretical and empiri-
cal aspects of cross section and time series data. Regression analysis has 
been augmented to utilize data that are qualitative in nature. Th e quali-
tative data can be used as dependent variables or independent variables. 
Many economic decisions can be represented as qualitative dependent 
variables, for example, the decision to buy a good or not to buy it, obtain 
a college degree, take a vacation (i.e. consume leisure), or to save, to name 
a few. Qualitative variables can also be independent variables, such as 
race, gender, political persuasion, and nationality.
Th e domain of statistics is vast and covers numerous specialized fi elds 
such as econometrics, biostatistics, sampling, and actuary to name a few. 
However, there is not a fi eld that does not utilize statistical analysis; from 
agriculture to zoology.
Th is text groups related topics and focuses on the interrelationship 
of diff erent topics. Th e best way to see this is to refer to Table 1.1 in 
 Chapter 1. Th e table divides descriptive statistics into two major categories 
of qualitative variables and quantitative variables. Th e scope of methods for 
 quantitative variables is much broader than those for the qualitative vari-
ables because the methods used for qualitative variables are also applicable 
for the quantitative variable, but the reverse is not true necessarily. Within 
each category the analytical methods are broken down to tabular methods 

176 
STATISTICS FOR ECONOMICS
and graphical methods. Recall that these are all descriptive methods and 
their purpose is to provide insight to the nature of data and to condense 
the massive amount of information into as few parameters as possible. 
Although graphical and tabular methods are very helpful in providing a 
visual description of data, the analytical power of statistics is more evident 
in the numerical methods that apply to quantitative variables. Within the 
last category, it is customary to distinguish among three diff erent classifi ca-
tions: measures of central tendency, measures of dispersion, and  measures 
of association. Each of these measures provides diff erent refi nements to 
analytical power and allows researchers to diff erentiate among diff erent 
types of data where certain aspects might be similar while the nature of 
data are very diff erent, for example, as in the case of two populations with 
the same means but diff erent variances. Th e knowledge about parameters 
provides an insight into the nature of data. Massive databases are like chaos 
of numbers. In spite of the fact that the human brain is extremely good 
at fi nding order in events when the order is not easy to detect, the data is 
too large, or the relationships are too complex, it needs statistics to com-
prehend what is going on. A good example to clarify the above point is 
the saying “to miss the forest for the trees.” Statistics provides a way of 
summarizing the evidence. Th e advantage of statistics is that it provides 
numerous descriptive and analytical tools that were not available prior to 
the discovery of statistics. Now it is possible to determine, with an appro-
priate level of probability, the outcome of a certain phenomenon or how to 
explain one or more variables using one or more other variables. 
In Chapter 3, we put the few descriptive tools that were introduced in 
Chapter 2 into use by showing the applications of Z score and coeffi  cient 
of determination. Th e chapter also provided additional tools to deepen the 
knowledge about life and to improve the analytical power of statistics. Th e 
concept of error is one of the major contributions of statistics to  science. 
Th is notion allows us to divide variations in a phenomenon, which is ever-
present in all real life situations, into two components: one that can be 
explained by statistical analysis and one that cannot be explained. One 
object of statistical analysis is to reduce the magnitude of the part that is 
unexplained. For example, the mean of a data explains part of the varia-
tion in it and leaves a part unexplained. In Chapter 8, we saw a glimpse of 
a regression analysis where part of the previously “ unexplainable” error is 

 
CONCLUSION 
177
explained by appropriate independent variables that are identifi ed by eco-
nomic theories or theories of other disciplines. Th is is just the beginning. 
Th ere are numerous modifi cations to the simple regression analysis that 
allows us to reduce the unexplained portion by use of theory, assumptions, 
facts, prior information, and mathematical manipulations.
One mathematical manipulation is the discovery of diff erent kinds 
of distribution functions. Th ese mathematical relationships have certain 
known properties that are used to conduct statistical inference. Th ey are 
also used to compare actual data to them and to apply the properties of 
these distribution functions to the actual data. Th e most important of 
such distribution functions is the normal distribution function. Although 
many natural events resemble the normal distribution function, many do 
not. Nevertheless, the use of theorems such as the Chebyshev’s  theorem 
and the Central Limit Th eorem allows us to use the properties of the 
normal distribution in dealing with some of the statistics obtained from 
real data that either have a complicated distribution function or do not 
even have a known distribution function. For example, the distribution 
function of the quantity demanded of a good is usually unknown. How-
ever, we can use the theories mentioned previously to address the average 
quantity demanded. Th e link between the above theorems and statistics is 
the main subject of sampling distribution of sample statistics. We devoted 
Chapter 5 to this topic exclusively. Th e next step after  identifying the 
 distributional properties of the sample statistics is to use them to make 
inferences about population parameters. Population parameters deter-
mine the population and the underlying laws that govern them. Th e 
knowledge of parameters is similar to the knowledge about the phenom-
enon of interest, but in a manageable way. 
Th e content of this text is a small portion of basic statistics. Th e next 
step for most economists is to learn regression analysis. Th e fi rst step would 
be to learn simple and multiple regression for cross section data followed 
by the use of the same techniques modifi ed to handle time series data. 
Almost all economic programs require at least one course in economet-
rics, which is the application of linear models such as regression  analysis 
to economic issues. More serious students that pursue  graduate work 
in economics are required to learn and sometimes prove the  applicable 
theorems used in econometrics; however, a purely  pragmatic approach 

178 
STATISTICS FOR ECONOMICS
of learning the methods is utilized by many programs. Th e next logical 
step is to combine the cross section and time series data, which since 1990 
has become a distinct area commonly known as panel data  analysis. In 
panel data analysis the problems that cause diffi  culty in the regressions 
using cross section or time series data are utilized to provide a better 
analysis. For example, the existence of correlation among units over time 
and the presence of correlations among independent variables are incor-
porated into the analysis rather than excluded or avoided. Probably the 
best example of this point is the analysis based on seemingly unrelated 
data. In this methodology, the fact that similar fi rms are subject to simi-
lar economic conditions and thus respond in similar manners in certain 
areas is the foundation of the methodology. Another recent development 
is spatial econometrics where the space related information is incorpo-
rated in the form of weight assigned to economic events. For example, it 
is reasonable to expect “neighboring” countries to act more similar than 
distant counties. Th ere are numerous ways of defi ning neighbors, such as 
distance, existence of border, and so forth.
Finally, the hope is that the present text has been able to answer 
some of the questions readers had and also to spark an interest in this 
 fascinating subject.

Glossary
Bar graph is a graphical representation of the frequency distribution or relative 
frequency distribution when dealing with qualitative data.
Binomial distribution function is a probability distribution representing a 
dichotomous binary variable.
Box plot is a visual representation of several basic descriptive statistics in a concise 
manner.
Categorical variable is another name for a qualitative variable.
Center of gravity of the data is the same as the expected value, or mean. 
Central Limit Th eorem states that in repeated random samples from a 
population, the sample mean will have a distribution function approximated 
by normal distribution, the expected value of the sample mean is equal to the 
true value of the population mean, and the variance of the sample mean is 
equal to population variance divided by the sample size. 
Ceteris paribus is Latin for “other things being equal.”
Chi-square represents the distribution function of a variance.
Claim is a testable hypothesis.
Coeffi  cient of variation is the ratio of the standard deviation to the mean.
Confi dence interval provides a probabilistic estimate of a population parameter 
with a desired level of confi dence. 
Consistent means the sample variance becomes smaller as sample size increases.
Continuous dichotomous variables exist when one can place an order on the 
type of data.
Continuous variable random is a variable that can assume any real value. It 
represents all the values over a range.
Correction factor with the variance is used when the sample size is small or the 
sample is more than 5% of the population.
Cross sectional analysis is a study of a snapshot of regions at a given time. 
Cumulative frequencies consist of sum of frequencies up to the value or class 
of interest.
Deductive statistics start from general information to make inferences about 
specifi cs.
Degree of freedom is the number of elements that can be chosen freely in a 
sample.
Dependent variable is the variable of interest that is explained by statistical 
analysis. Other names such as endogenous variable, Y-variable, response 
variable, or even output are often used as well. 

180 
GLOSSARY
Descriptive statistics provide a descriptive, instead of analytic view, of variables.
Dichotomous variables, also called dummy variables in econometrics, exist 
when there are only two nominal types of data. 
Discrete dichotomous variable is a dichotomous variable that can take integer 
values.
Discrete random variables consist of integers only.
Dot plot represents frequencies as stacked dots. It is useful when only one set of 
data is under consideration. 
Dummy variable is a qualitative variable used as an independent variable. 
Econometrics is the application of statistics to economics.
Effi  cient refers to the estimator with the smallest variance compared to the other 
estimator(s).
Error is the diff erence between an observed value and its expected value. Error is 
the portion of variation that cannot be explained. 
Errors in measurement refer to incorrectly measuring or recording the values of 
dependent or independent variables.
Expected value is the theoretical value of parameter. It is the same as the 
arithmetic mean.
Experimental design is a type of statistics where the experiment is controlled for 
diff erent variables to ensure desired levels of confi dence for the estimates of 
the variable. 
F statistics is used to test complex hypothesis. It consists of the ratios of two 
variance measures.
Frequency distribution shows the frequency of occurrence for non-overlapping 
classes.
Grouped data are summarized or organized to provide a better and more compact 
picture of reality. 
Harmonic mean is the average of rates. It is the reciprocal of the arithmetic mean 
of the reciprocal of the values.
Histogram is a graphical representation of the frequency distribution or relative 
frequency distribution when dealing with quantitative data.
Independent variable is a variable that is used to explain the response or 
dependent variable. Other names such as exogenous variable, X-variable, 
regressor, input, factor, or predictor variable are also used. 
Individual error is the diff erence between an observed value and its expected value.
Inductive statistics observes specifi cs to make inference about the general 
population.
Inferential statistics is the methodology that allows making decision based on 
the outcome of a statistics from a sample. 
An interval scale includes relative distances of any two sequential values, such as 
a Fahrenheit scale.

 
GLOSSARY 
181
Kurtosis is a measure of pointedness or fl atness of a symmetric distribution.
A Likert scale is a kind of ordinal scale, where the subjects provide the ranking 
of each variable.
Th e lower hinge is the 25th percentile of a box plot. 
Mean is the arithmetic average. It represents the center of gravity of data. 
Mean absolute error (MAE) is the average of the absolute values of individual errors. 
Mean squared error is the same as variance. 
Measurement scales are types of variables.
Measures of association determine the association between two variables or the 
degree of association between two variables. Th ey consist of covariance and 
correlation coeffi  cient.
Measures of central tendency provide concise meaningful summaries of central 
properties of a population.
Measures of dispersion refl ect how data are scattered. Th e most important 
dispersion measures are variance and standard deviation. 
Median is a value that divides observations into two equal groups. It is the 
midpoint among a group of numbers ranked in order. 
Mode is the most frequent value of a population. 
Nominal or categorical data are the “count” of the number of times an event 
occurs.
Normal distribution is a very common distribution function that refl ects many 
randomly occurring events in life.
Null hypothesis refl ects the status quo or how things have been or are currently. 
Observed signifi cant level is another name for the p value, which is the 
probability of seeing what you saw. 
Ogive is a graph for cumulative frequencies. 
Ordinal scale indicates that data is ordered in some way but the numbering has 
no value. 
P value represents the probability of type I error for inference about a coeffi  cient.
A parameter is a characteristic of a population that is of interest; it is constant 
and usually unknown.
A percentile is the demarcation value below which the stated percentage of the 
population or sample lie.
A pie chart is a graphical presentation of frequency distribution and relative 
frequency.
Point estimate is statistics that consists of a single value, such as mean or 
variance.
Probability is the likelihood that something will happen, expressed in the form 
of a ratio or a percentage. 
Probability distribution determines the probability of the outcomes of a 
random variable.

182 
GLOSSARY
Probability distribution for a continuous random variable is called a probability 
density function.
Probability distribution for a discrete random variable is called a discrete 
probability distribution and is represented as f (x).
Qualitative variables are non-numeric and represent a label for a category of 
similar items.
Quantitative variables are numerical and countable values. 
Quartiles divide the population into four equal portions, each equal to 25% of 
the population.
Random variables are selected in a random fashion and by chance. 
A ratio scale provides meaningful use of the ratio of measurements.
Real numbers consist of all rational and irrational numbers.
Relative frequency shows the percentage of each class to the total population or 
sample.
Relative variability is the comparison of variability using coeffi  cient of variation. 
Reliability of a sample mean (ˆm) is equal to the probability that the deviation of 
the sample mean, from the population mean, is within the tolerable level of 
error (E ).
Root mean squared error is the square root of the mean square error and is the 
same as the standard error. 
Sample standard deviation is the average error of the sample. Th is is the standard 
deviation obtained from a sample and is not the same as standard error.
Sample statistics are random values obtained from a sample. Th ey estimate the 
corresponding population parameters and are used to make inferences about 
them.
Sample variance is an estimate of the population variance. It is the sum of the 
squares of the deviations of values from the sample mean divided by the 
degrees of freedom. 
Sampling is a subset of population that is collected in a variety of ways.
Sampling distribution of any statistics explains how the statistics diff er from one 
sample to another. 
Scatter plot is a graph customarily used in presenting data from a regression 
analysis model. 
Simple hypothesis gives an exact value for the unknown parameter of the 
assumed distribution function.
Skewness refers to the extent that a distribution function deviates from symmetric 
distribution. 
Standard deviation is the square root of variance and represents the average error 
of a population or sample.
Standard error is the standard deviation of the estimated sample statistics. 
Standardization is the conversion of the value of an observation into its Z score. 

 
GLOSSARY 
183
A statistic is a numerical value calculated from a sample that is variable and 
known.
Statistical hypothesis is an assertion about distribution of one or more random 
variables. 
Statistical inference is the process of drawing conclusions based on evidence 
obtained from a sample. All statistical inferences are probabilistic.
Stem-and-leaf is a graphical way of summarizing information and is a type of a 
descriptive statistics. 
Stochastic means that a model is probabilistic in nature and would result in 
varying results refl ecting the random nature of the model. 
t distribution is a distribution function that is designed to handle statistics from 
small samples correctly.
A testable hypothesis is a claim about a relationship among two or more 
variables. 
Time series analysis is the analysis of time series data.
Tolerable level of error is the amount of error that the researcher is willing to 
accept. 
Tolerance level is a measure for detecting multicollinearity. It is the reciprocal of 
Variance Infl ation Factor (VIF). A tolerance value less than 0.1 is an indicative 
of the presence of multicollinearity.
Total sum of square (TSS) represents the total variation in the dependent 
variable.
Trimmed mean is a modifi cation of the mean, where outliers are discarded.
Type I error is rejecting the null hypothesis even though it is true. 
Type II error is failure to reject a false hypothesis. 
Type III error is rejecting a null hypothesis in favor of an alternative hypothesis 
with the wrong sign. 
Typical refers to the average.
Unbiased refers to an estimate whose expected value is equal to the corresponding 
population parameter.
Th e upper hinge is the 75th percentile of a box plot. 
Validity is the lack of measurement error.
Variance is the sum of the squares of the deviations of values from their mean, 
divided by population size. It is the average of the squared individual errors. 
Weighted mean is similar to the mean except the weights for observation are not 
equal and represent their contribution to the total. Calculation of GPA is an 
example of weighted mean.
Z score is a statistics based on mean and standard deviation. It is used to 
standardize unrelated variables for the purpose of comparing them. 


Notes
Chapter 1
1. Internal Revenue Service (2009).
2. Stevens (1946).
3. Tukey (1977).
4. Anderson et al. (2010).
5. Tukey (1977).
Chapter 3
1. Gosset (1908).
2. Gosset (1908).
Chapter 8
1. A more formal and detailed explanation of this process is available in 
Naghshpour (2012).
2. Interested readers should consult Naghshpour (2012) for more detailed 
discussion of the topic.
3. For more detail refer to Naghshpour (2012).
4. For the sake of this example, we use the national income that is obtained from 
http://www.bea.gov/histdata/Releases/Regional/2010/PI/state/preliminary
_March-23-2011/SA1-3.csv. Th e data on education, which are in 1,000s, 
are obtained from http://www.census.gov/hhes/socdemo/education/data/cps
/historical/index.html.
5. Th ere is not enough space to discuss and explain all the numbers that are 
provided in Table 8.2, interested readers should consult Naghshpour (2012).
6. Naghshpour (2012).


References
Anderson, D. R., Sweeney, D. J., & Williams, T. A. (2011). Statistics for business 
and economics. South-Western College Publisher.
Anderson, D. R., Sweeney, D. J., & Williams, T. A. (2010). Statistics for business 
and economics. South-Western College Publisher.
Bureau of Economic Analysis, GDP and Personal Income: SA1-3 Personal 
Income Summary.
Bureau of Economic Analysis, National Income and Product Account Tables: 
Table 2.3.5-Personal Consumption Expenditures by Major Type of Product.
Gosset, W. S. (1908). Probable error of a correlation coeffi  cient. Biometrika 
6, 302–310. 
Internal Revenue Service. (2009). SOI Tax Stats—Tax Stats at a Glance. 
Summary of Collections Before Refunds by Type of Return, FY 2009 [1]. 
http://www.irs.gov/taxstats/article/0,,id=102886,00.html
Naghshpour, S. (2012). Regression for economics. New York, NY: Business Expert 
Press.
Stevens, S. S. (1946). On the theory of scales of measurement. Science 7, 677–680. 
Tukey, J. W. (1977). Exploratory data analysis. Reading, MA: Addison-Wesley. 


A
Alternative hypothesis
determination, 147
formation, 146–147
for single mean, 147–148
for single proportion, 148
for single variance, 148
Arithmetic mean, 29–30
B
Bar graph, 11–12
Binomial distribution function, 110
Box plot, 27–28
C
Categorical variable, 3–4
Center of gravity, 76
Central limit theorem, 105–106
Ceteris paribus, 165
Chebyshev theorem, 67–68
Chi-squared distribution function, 
94–95
Coeffi  cient of variation, 61–64
Composite statistical hypothesis, 142
Computational formula, 51–53
Confi dence intervals, 122–125, 
139–140, 156–161
Continuous dichotomous variable, 4
Continuous random variable, 82
Control variables, 165
Correlation coeffi  cient, 59–60, 71
Covariance
population, 57–58
sample, 58–59
Critical value calculation, 155–156
Cumulative frequency 
distribution, 17
D
Degree of freedom, 72–74
Descriptive statistics
applications
coeffi  cient of variation, 61–64
correlation coeffi  cient, 59–60, 71
error, 75–77
Kurtosis, 80
properties of estimators, 74–75
relation between mean, median, 
and mode, 80
skewness, 78–79
standard error, 71–74
standardization, 69–70
sum of squares, 78
Z score, 64–68
measurement scales
categorical data, 3–4
dichotomous variables, 4
interval scale, 5
Likert scale, 4–5
nominal, 4
ordinal scale, 4
ratio scale, 5
numerical
measures of association, 57–60
measures of central tendency, 
29–45
measures of dispersion, 45–57
qualitative variables
defi nition, 3
graphical methods, 10–14
tabular methods, 7–10
quantitative variables
defi nition, 3
graphical methods, 20–28
tabular methods, 14–20
types of tools, 5–7
Dichotomous variables, 4
Discrete dichotomous variable, 4
Discrete random variable, 82
Dot plot, 25
Dummy variables. See Dichotomous 
variables
E
Econometrics, 173
Error, 50, 75–77
Index

190 
INDEX
Estimators, properties of, 74–75
Expected value. See Mean
F
F distribution function, 96–97
Frequency distribution, 17
qualitative variables, 7–10
quantitative variables, 14–17
G
Geometric mean, 34–37
Grouped data, 14
H
Harmonic mean, 37–38
Hinges, 20
Histogram, 20–22
Human capital, 166
I
Individual error, 75
Inductive statistics, 101
Interquartile range, 45–46
Interval estimation
defi nition, 121–122
one population mean, 125–137
Interval scale, 5
K
Kurtosis, 80
vs. normality, 93–94
L
Law of large numbers, 105
Likert scale, 4–5
M
MAE. See Mean absolute error
Margin of error, 122–125
Mean
of data summarized as frequencies, 
41–42
for data with frequencies, 38–39
defi nition, 29
for grouped data, 42–43
of grouped data, 40–42
Mean absolute error (MAE), 76
Mean squared error (MSE), 78, 166
Mean squared regression (MSR), 166
Measurement scales
categorical data, 3–4
dichotomous variables, 4
interval scale, 5
Likert scale, 4–5
ordinal scale, 4
ratio scale, 5
Measures of association
correlation coeffi  cient, 59–60, 71
population covariance, 57–58
sample covariance, 58–59
Measures of central tendency
arithmetic mean, 29–30
geometric mean, 34–37
harmonic mean, 37–38
mean, 29
mean for data with frequencies, 
38–39
mean for grouped data, 42–43
mean of data summarized as 
frequencies, 41–42
mean of grouped data, 40–42
median, 43–44
mode, 44–45
quartiles, 43
sample mean, 30–34
trimmed mean, 34
weighted mean, 39–40
Measures of dispersion
algebraic relations for variance, 
50–51
average of several variances, 53–54
computational formula, 51–53
error, 50
interquartile range, 45–46
population variance, 47
range, 45
sample variance, 47–48
standard deviation, 49–50
variance, 46–47
variance of data with frequency, 
54–57
variance of grouped data, 57
Median, 43–44
Mode, 44–45
MSE. See Mean squared error
MSR. See Mean squared regression

 
INDEX 
191
N
Nominal data, 3–4
Normal distribution functions
area under with any mean and 
variance, 93
area under with mean zero and 
variance one, 85–86
description, 82–83
normality vs. Kurtosis, 93–94
normality vs. skewness, 93
probability values with excel, 
86–88
properties, 83–84
standardizing values, 84–85
Null hypothesis
defi nition, 143–144
equality of two parameters, 
144–145
of two means, 145
of two proportions, 146
of two variances, 146
Numerical descriptive statistics
measures of association, 57–60
measures of central tendency, 
29–45
measures of dispersion, 45–57
O
Ogive, 22–23
Ordinal scale, 4
P
Panel data analysis, 178
Parameter, 5–6
Pearson coeffi  cient of skewness, 80
Percentile, 17–18
Pie chart, 12–14
Point estimates, 74–75
Point estimation, 120–121
Pooled variance, 53
Population covariance, 57–58
Population variance, 47
Probability density function, 82
Probability distribution
for continuous random variable, 82
defi nition, 81
for discrete random variable, 82
p value, 154–155
Q
Qualitative variables
defi nition, 3
graphical methods
bar graph, 11–12
pie chart, 12–14
tabular methods
frequency distribution, 7–10
relative frequency, 10
Quantitative variables
graphical methods
box plot, 27–28
dot plot, 25
histogram, 20–22
Ogive, 22–23
scatter plot, 25–27
stem-and-leaf, 23–24
tabular methods
cumulative frequency 
distribution, 17
frequency distribution, 14–16
hinges, 20
percentile, 17–18
quartiles, 18–19
relative frequency distribution, 16
Quartiles, 18–19, 43
R
Random variable, 81
Range, 45
Ratio scale, 5
Regression analysis, 163–173
Relative frequency, 10
Relative frequency distribution, 16
S
Sample covariance, 58–59
Sample mean, 30–34
Sample size, 101–104, 137–138
Sample variance, 47–48
Sampling distribution
central limit theorem, 105–106
diff erence of two proportions, 
113–114
law of large numbers, 105
mean vs. median effi  ciency, 
116–118
one sample mean, 106–109

192 
INDEX
one sample proportion, 109–110
one sample variance, 114–115
two sample means, 110–113
two sample proportion, 113–114
two sample variance, 115–116
Scatter plot, 25–27
Simple hypothesis, 142–143
Skewness, 78–79
vs. normality, 93
Spatial econometrics, 178
Standard deviation, 49–50
Standard error, 66, 71–74
Standardization, 69–70
Statistical hypothesis, 142
Statistical inference, 101, 119, 
150–152, 154–156
Stem-and-leaf, 23–24
Sum of squares, 78
T
t distribution function, 96
Test of hypothesis, 156–161
Test statistics, 148–150
Total sum of squares (TSS), 78
Trimmed mean, 34
TSS. See Total sum of squares
Type I error, 153
Type II error, 153
Type III error, 153
V
Variance
algebraic relations, 50–51
of data with frequency, 54–57
defi nition, 46–47
of grouped data, 57
pooled, 53
population, 47
sample, 47–48
W
Weighted mean, 39–40
Z
Z score, 64–68

OTHER TITLES IN OUR ECONOMICS  COLLECTION
Phil Romero, The University of Oregon and Jeffrey Edwards, North Carolina A&T 
State University, Collection Editors
• 
 Managerial Economics: Concepts and Principles by Donald N. Stengel
• 
 Working With Economic Indicators: Interpretation and Sources by Donald N. Stengel 
and Priscilla Chaffe-Stengel
• 
 Using Economic Data for Personal and Corporate Decision Making: What Have We 
Learned From California? by Phillip J. Romero
• 
 Applying the Logic of the Five Forces Model to Your Products and Services. How Strong 
is Your Firm’s Competitive Advantage? by Daniel R. Marburger
• 
 Innovative Pricing Strategies to Increase Proﬁ ts by Daniel R. Marburger
• 
 Regression for Economics by Shahdad Naghshpour
Announcing the Business Expert Press Digital Library
Concise E-books Business Students Need for Classroom 
and Research
This book can also be purchased in an e-book collection by your library as
• 
a one-time purchase,
• 
that is owned forever,
• 
allows for simultaneous readers,
• 
has no restrictions on printing, and
• 
can be downloaded as PDFs from within the library community.
Our digital library collections are a great solution to beat the rising cost of textbooks. 
e-books can be loaded into their course management systems or onto student’s e-book readers.
The Business Expert Press digital libraries are very affordable, with no obligation to buy in 
future years. For more information, please visit www.businessexpertpress.com/librarians. To set 
up a trial in the United States, please contact Adam Chesler at adam.chesler@businessexpertpress
.com for all other regions, contact Nicole Lee at nicole.lee@igroupnet.com.


Statistics for Economics
Shahdad Naghshpour
Statistics is the branch of mathematics that deals with real-life problems. 
As such, it is an essential tool for economists. Unfortunately, the way you 
and many other economists learn the concept of statistics is not compat-
ible with the way economists think and learn. The problem is worsened by 
the use of mathematical jargon and complex derivations.
Here’s a book that proves none of this is necessary. All the examples and 
exercises in this book are constructed within the field of economics, thus 
eliminating the difficulty of learning statistics with examples from fields 
that have no relation to business, politics, or policy. Statistics is, in fact, not 
more difficult than economics. Anyone who can comprehend economics 
can understand and use statistics successfully within this field, including 
you!
This book utilizes Microsoft Excel to obtain statistical results, as well as 
to perform additional necessary computations. Microsoft Excel is not the 
software of choice for performing sophisticated statistical analysis. Howev-
er, it is widely available, and almost everyone has some degree of familiar-
ity with it. Using Excel will eliminate the need for students and readers to 
buy and learn new software, the need that itself would prove to be another 
impediment to learning and using statistics.
Dr. Shahdad Naghshpour has over 65 publications in journals, such as 
Journal of Economics and Finance; Review of Regional Studies; Journal of Eco-
nomic Studies; Peace Economics, Peace Science, and Public Policy; International 
Journal of Trade and Global Markets; International Journal of Monetary Econom-
ics and Finance; Politics & Policy; and International Journal of Economic Policy 
in Emerging Economies among others. He is the coauthor of Revolutionary 
Iran and the United States: Low-intensity Conflict in the Persian Gulf. Dr. Nagh-
shpour received the Distinguished Professorship Award for the period 
2008–09 in the category of research. In 1999, Dr. Naghshpour received 
the Teaching Excellence Award of College of Business Administration. 
He has received grants and has been a consultant to numerous state 
agencies, such as the Southern Mississippi Planning and Development 
District and Gulf Regional Planning Commission.
ISBN: 978-1-60649-403-5
9 781606 494035
90000
www.businessexpertpress.com
The Economics Collection
Philip J. Romero and Jeffrey A. Edwards, Editors


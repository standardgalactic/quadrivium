COMPILERS AND
OPERATING SYSTEMS FOR
LOW POWER

Related Recent Title
System-Level Power Optimization for Wireless Multimedia Communication: Power Aware
Computing
Ramesh Karri and David Goodman (Eds .)
ISBN 1-4020-7204-X, 2002
http://wwvi.wkap.nllprod/b/1-4020-7204-X

COMPILERS AND
OPERATING SYSTEMS FOR
LOW POWER
Edited by
LUCA BENINI
University of Bologna
MAHMUTKANDEMIR
The Pennsylvania State University
J. RAMANUJAM
Louisiana State University
...."
Kluwer Academic Publishers
Boston/DordrechVLondon

Distributors for North, Central and South America:
Kluwer Academic Publishers
101 Philip Drive
Assinippi Park
Norwell, Massachusetts 02061 USA
Telephone (781) 871-6600
Fax (781) 681-9045
E-Mail: kluwer@wkap.com
Distributors for all other countries:
Kluwer Academic Publishers Group
Post Office Box 322
3300 AH Dordrecht, THE NETHERLANDS
Telephone 31 786576 000
Fax 31 786576474
E-Mail: services@wkap.nl
....
"
Electronic Services <http://www.wkap.nl>
Library of Congress Cataloging-in-Publication Data
Compilers andoperatingsystems for lowpowerI editedbyLucaBenini, Mahmut
Kandemir, J. Ramanujam.
p. em.
Includes bibliographical references and index.
ISBN 1-4020-7573-1 (alk. paper)
I. Operating systems (Computers) 2. Compilers (Computer programs) 1. Benini, Luca,
1967- II. Kandemir, Mahmut. Ill. Ramanujam, J.
QA76.76.063C6552003
005.4'3--dc22
2003054676
Copyright © 2003 by Kluwer Academic Publishers
AlI rights reserved . No part of this work may be reproduced, stored in a retrieval
system, or transmitted in any form or by any means, electronic, mechanical ,
photocopying, microfilming. recording, or otherwise, without written permission
from the Publisher, with the exception of any material supplied specifically for the
purpose of being entered and executed on a computer system, for exclusive use by
the purchaser of the work.
Permission forbooks published in Europe: permissions@wkap.nl
Permissions forbookspublished in the United Statesof America: permissions@wkap.com
Printed on acid-free paper.
Printed in United Kingdom by Biddles/lBT Global

Contents
List of Figures
xi
List of Tables
xv
Contributing Authors
xvii
Preface
xix
1
Low Power Operating System for Heterogeneous Wireless Communica-
tion System
Suet-PetLi, RoySulton, JanRabaey
1
Introduction
2
2
Event-driven versus General-purpose as
3
2.1
PicoRadio II Protocol Design
3
2.2
General-purpose Multi-tasking as
4
2.3
Event-driven as
8
2.4
Comparison Summary
9
3
Low Power Reactive as for Heterogeneous Architectures
12
3.1
Event-driven Global Scheduler and Power Management
12
3.2
TinyOS Limitations and Proposed Extensions
14
4
Conclusion and Future Work
15
References
16
2
A Modified Dual-Priority Scheduling Algorithm for Hard Real-TIme Sys-
17
terns to Improve Energy Savings
M. AngelsMoncusi, AlexArenas, JesusLabarta
1
Introduction
17
2
Dual-Priority Scheduling
19
3
Power-Low Modified Dual-Priority Scheduling
21
4
Experimental Results
28
5
~mmary
%
References
36
3
Toward the Placement of Power Management Points in Real-Time Applications
37

vi
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
NevineAbouGhazaleh, Daniel Mosse, Bruce Childers, Rami Melhem
1
Introduction
2
Model
3
Sources of Overhead
3.1
Computing the New Speed
3.2
Setting the New Speed
4
Speed Adjustment Schemes
4.1
Proportional Dynamic Power Management
4.2
Dynamic Greedy Power Management
4.3
Evaluation of Power Management Schemes
5
Optimal Number of PMPs
5.1
Evaluation of the Analytical Model
6
Conclusion
Appendix: Derivation of Formulas
References
37
39
40
40
40
41
41
42
43
44
45
48
48
51
4
Energy Characterization of Embedded Real-Time Operating Systems
AndreaAcquaviva, Luca Benini, Bruno Ricco
1
2
3
4
5
6
7
Introduction
Related Work
System Overview
3.1
The Hardware Platform
3.2
RTOS overview
Characterization Strategy
RTOS Characterization Results
5.1
Kernel Services
5.2
I/O Drivers
5.2.1
Burstiness Test
5.2.2
Clock Speed Test
5.2.3
Resource Contention Test
5.3
Application Example: RTOS vs Stand-alone
5.4
Cache Related Effects in Thread Switching
Summary of Findings
Conclusions
53
53
55
56
56
57
59
60
60
62
62
63
64
65
66
66
67
References
72
5
Dynamic Cluster Reconfiguration for Power and Performance
75
Eduardo Pinheiro, RicardoBianchini, Enrique V. Carrera, TaliverHeath
1
Motivation
77
2
Cluster Configuration and Load Distribution
78
2.1
Overview
78
2.2
Implementations
81
3
Methodology
83
4
Experimental Results
84
5
Related Work
89
6
Conclusions
91

Contents
References
vii
91
6
Energy Management of Virtual Memory on Diskless Devices
JerryHom, Ulrich Kremer
1
2
3
4
5
6
7
Introduction
Related Work
Problem Formulation
EELRM Prototype Compiler
4.1
Phase 1 • Analysis
4.2
Phase 2 - Code Generation
4.3
Performance Model
4.4
Example
4.5
Implementation Issues
Experiments
5.1
Benchmark Characteristics
5.2
Simulation Results
Future Work
Conclusion
References
95
96
97
98
100
100
101
102
102
103
105
106
107
110
111
111
7
Propagating Constants Past Software to Hardware Peripherals on Fixed-
115
Application Embedded Systems
Greg Stitt, Frank Vahid
1
Introduction
116
2
Example
119
3
Parameters in Cores
120
4
Propagating Constants from Software to Hardware
123
5
Experiments
125
5.1
8255A Programmable Peripheral Interface
126
5.2
8237A DMA Controller
127
5.3
PC 16550A UART
128
5.4
Free-DCT-L Core
128
5.5
Results
131
6
Future Work
133
7
Conclusions
134
References
134
8
Constructive liming Violation for Improving Energy Efficiency
137
Toshinori Sato, Itsujiro Arita
1
Introduction
137
2
Low Power via Fault-Tolerance
139
3
Evaluation Methodology
143
4
Simulation Results
143
5
Related Work
147
6
Conclusion and Future Work
151
References
151

viii
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
9
Power Modeling and Reduction of VLIW Processors
Weiping Liao, Lei He
1
2
3
4
5
Introduction
Cycle-Accurate VLIW Power Simulation
2.1
IMPACT Architecture Framework
2.2
Power Models
2.3
PowerImpact
Clock Ramping
3.1
Clock Ramping with Hardware Prescan (CRHP)
3.2
Clock Ramping with Compiler-based Prediction (CRCP)
3.2.1
Basic CRCP Algorithm
3.2.2
Reduction of Redundant Ramp-up Instructions
3.2.3
Control Flow
3.2.4
Load Instructions
Experimental Results
Conclusions and Discussion
References
155
155
156
156
157
158
159
160
162
162
164
165
165
165
169
170
10
Low-Power Design of Turbo Decoder with Exploration of Energy-Throughput
173
Trade-off
Amout Vandecappelle, BrunoBougard, K.C. Shashidhar; Francky Cauhoor
1
Introduction
173
2
Data Transfer and Storage Exploration Methodology
176
3
Global Data Flow and Loop Transformations
178
3.1
Removal of Interleaver Memory
178
3.2
Enabling Parallelism
179
4
Storage Cycle Budget Distribution
180
4.1
Memory Hierarchy Layer Assignment
181
4.2
Data Restructuring
182
4.3
Loop Transformations for Parallelization
183
4.3.1
Loop Merging
183
4.3.2
Loop Pipelining
184
4.3.3
Partial Loop Unrolling
184
4.3.4
Loop Transformation Results
185
4.4
Storage Bandwidth Optimization
185
5
Memory Organization
186
5.1
Memory Organization Exploration
186
5.2
Memory Organization Decision
188
6
Conclusions
190
References
190
11
Static Analysis of Parameterized Loop Nest'> for Energy Efficient Use of
193
Data Caches
Paolo D'Alberto, AlexandruNicolau, Alexander Yeidenbaum, RajeshGupta
1
Introduction
193
2
Energy and Line Size
195
3
Background
195
4
The Parameterized Loop Analysis
197

Contents
5
6
4.1
Reduction to Single Reference Interference
4.2
Interference and Reuse Trade-off
STAMINAImplementation Results
5.1
Swim from SPEC 2000
5.2
Self Interference
5.3
Tiling and Matrix Multiply
Summary and Future Work
References
ix
199
200
200
201
201
202
203
203
12
A Fresh Look at Low-Power Mobile Computing
209
Michael Franz
1
Introduction
209
2
Architecture
211
3
Handover and the Quantization of Computational Resources
212
3.1
Standardization of Execution Environment's Parameters
214
3.2
A Commercial Vision: Impact on Billing, Customer Loyalty
and Churn
215
4
Segmentation of Functionality: The XU-MS Split
215
4.1
Use of Field-Programmable Hardware in the Mobile Station
217
4.2
Special End-To-End Application Requirements
217
5
Status and Research Vision
218
References
219
Index
221

List of Figures
1.1
Model of computation for PicoRadio protocol stack
5
1.2
Implementing PicoRadio II with VCC
6
1.3
Code generation with general-purpose eCOS
7
1.4
PicoRadio II chip floorplan. Xtensa is the embedded mi-
croprocessor
7
1.5
Implementing PicoRadio II Protocol stacks in TinyOS.
Arrows show events/commands propagation in the system
9
1.6
Total cycle count comparison: General-purpose versus
event-driven as.Key at right identifies system components
10
1.7
Percentage breakdown comparison: General-purpose ver-
sus event-driven as. Key at right identifies system com-
ponents
11
1.8
Behavior diagram of the PicoRadio sensor node
13
1.9
Architectural diagram for PicoRadio sensor node
14
2.1
Pseudo code for Power Low Modified Dual-Priority (PLMDP)
Scheduling
22
2.2
Maximum extension time in three different situations
24
2.3
Execution time in LPFPS when all tasks use 100% WCET
25
2.4
Execution time in PLMDP when all tasks use 100% WCET
25
2.5
Execution time in LPFPS when all tasks use 50% WCET
27
2.6
Execution time in PLMDP when all tasks use 50% WCET
28
2.7
Comparison of both algorithms in the task set proposed
by Shin and Choi [4]
28
2.8
Comparison of both algorithms when the workload of
the system is 80%
30
2.9
System workload variation when all tasks consume the
100% ofWCET
31
2.10
System workload variation when all tasks consume the
50%ofWCET
31
2.11
System workload and harmonicity of the tasks periods variation
32

xii
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
2.12
Max imum task workload variation Non-harmonics periods
32
2.13
Tminffmax variation
33
2.14
Comparison of both algorithms in the avionics task set [9]
34
2.15
Comparison of both algorithms in the INS task set [10]
35
2.16
Comparison of both algorithms in the CNC task set [11]
35
3.1
Actual execution times of a task set using the Static, Pro-
portional and Dynamic Greedy schemes
42
3.2
Total energy consumption for different schemes versus
the number of PMPs
43
3.3
Total energy consumption for the Proportional scheme
versus the number of PMP
46
3.4
Total energy consumption for Dynamic Greedy scheme
versus the number of PMPs
46
4.1
The hardware platform: HP SmartBadgeIV
57
4.2
The software layer: eCos structure
58
4.3
Thread switch experiment: Energy consumption for dif-
ferent clock frequencies at the maximum switching frequency
62
4.4
Energy consumption of the audio driver for different clock
speeds at fixed data burstiness
64
5.1
Cluster evolution and resource demands for the WWW server
8S
5.2
Power consumption for the WWW server under static
and dynamic cluster configurations
86
5.3
Cluster evolution and resource demands for the WWW server
86
5.4
Cluster evolution and resource demands in the power-
aware as
88
5.5
Power consumption for the power-aware as under static
and dynamic cluster configurations
88
5.6
Cluster evolution and resource demands in the power-
aware as
89
6.1
Comparison of compiler vs. OS directed power mangement
99
6.2
Sample code
103
6.3
Partial view of tomcatv's page fault behav ior during execution
108
6.4
One iteration of tomcatv's primary, outermost loop
109
7.1
Core-based embedded system design
116
7.2
A simple example of propagating constants to hardware
(a) soft core, (b) synthesized core structure, (c) synthe-
sized core structure after propagating constants contJeg(O)=O
and contJeg(l)=I
121
7.3
The Intel 8255A parallel peripheral interface
122
7.4
Method for propagating constants to peripheral cores
124

List ofFigures
xiii
7.5
Block diagram of DCT core
129
8.1
ALU utilizing proposed technique
141
8.2
Clock signals
142
8.3
Energy consumption (Squash) 164.gzip
144
8.4
Energy consumption (Squash) 175.vpr
144
8.5
Energy consumption (Squash) 176.gcc
144
8.6
Energy consumption (Squash) 186.crafty
145
8.7
Energy consumption (Squash) 197.parser
145
8.8
Energy consumption (Squash) 252.eon
145
8.9
Energy consumption (Squash) 255.vortex
146
8.10
Energy consumption (Squash) 256.bzip2
146
8.11
Energy consumption (Reissue) 164.gzip
148
8.12
Energy consumption (Reissue) 175.vpr
148
8.13
Energy consumption (Reissue) 176.gcc
148
8.14
Energy consumption (Reissue) 186.crafty
149
8.15
Energy consumption (Reissue) 197.parser
149
8.16
Energy consumption (Reissue) 252.eon
149
8.17
Energy consumption (Reissue) 255.vortex
150
8.18
Energy consumption (Reissue) 256.bzip2
150
9.1
Flow diagram for IMPACT
156
9.2
Overall structure of PowerImpact
159
9.3
The relationship of states
161
9.4
Utilization rate for FPUs
161
9.5
Distribution of instruction numbers in bundles, with max-
imum bundle width =6
162
9.6
Insert ramp -up instructions
163
9.7
Insertion of ramp-up instructions beyond the current hyperb lock 164
9.8
Performance loss (in percentage as the Z-axis variable)
of CRHP and CRCP approaches for equake
166
9.9
Power reduction (in percentage as the Z-axis variable) of
CRHP and CRCP approaches for equake
166
9.10
Performance loss (in percentage as the Z-axis variable)
of CRHP and CRCP approaches for art
166
9.11
Power reduction (in percentage as the Z-axis variable) of
CRHP and CRCP approaches for art
167
9.12
Performance loss (in percentage) for T, =10 and Ta =16
167
9.13
Power reduction (in percentage) for T; =10 and To =16
168

xiv
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
9.14
Performance Loss (in percentage) before and after the
amendment for load instruction, for t; =10, Ta =16 and
t, =9
169
10.1
Turbo coding-decoding scheme
174
10.2
Energy-performance trade-off
177
10.3
Transformed data flow of turbo decoding scheme
178
10.4
Parallelization of the MAP algorithm
179
10.5
Turbo decoding data flow and timing
180
10.6
Dependencies between memory accesses of two loops
183
10.7
Dependencies after merging the two loops of Figure 10.6
184
10.8
Dependencies after pipelining the merged loop of Figure 10.7
185
10.9
Pareto curves for 7 workers, for two and for seven dual-
port memories per worker
187
11.1
Grid cells and band cells in a plane
198
11.2
Tiling of Matrix Multiply. 6 parameters: loop bounds
and A,B and C offsets
205
11.3
SWIM: calc 1() in C code
206
11.4
Matrix Multiply. Two parameters: loop bounds and A offset
206
11.5
Self interference and analysis results
207
12.1
System architecture
212

List of Tables
1.1
General comparison
9
1.2
Memory requirements comparison
10
2.1
Benchmark task set used by Shin and Choi [4]
24
2.2
Avionics benchmark task set [9]
33
2.3
INS benchmark task set [10]
33
2.4
CNC benchmark task set [11]
34
3.1
Theoretical versus Simulation choice of optimal number
of PMPs for the Proportional scheme
47
3.2
Theoretical versus Simulation choice of optimal number
of PMPs for the Dynamic Greedy scheme
47
4.1
Thread switch experiment: Energy variation due to dif-
ferent switching frequencies with a fixed clock frequency
(103.2Mhz)
62
4.2
Audio driver average power consumption due to differ-
ent level of data burstiness at a fixed clock frequency
63
4.3
Average power consumption of the wireless LAN driver
due to different level of data burstiness at a fixed clock
frequency
64
4.4
Variation of the energy consumed by the audio driver in
presence of device contention for different switch frequencies
65
4.5
Comparison between the energy consumed by two ver-
sion of the speech enhancer: as based and stand-alone
65
4.6
Testing parameters for the experiment related to Tables 4.7
thru 4.9
65
4.7
Energy consumption of thread management and sched-
uler functions at minimum and maximum clock frequencies
68
4.8
Energy consumption of thread communication and syn-
cronization functions at minimum and maximum clock
frequencies
69

xvi
COMPILERS AND OPERATINGSYSTEMS FOR LOW POWER
4.9
Energy consumption of time management functions at
minimum and maximum clock frequencies
71
4.10
Energy cost of thread switching in presence of cache-
related effects
72
6.1
Page faults for different memory sizes in terms of pages,
assuming that each array requires 4 pages of memory space
103
6.2
Dynamic page hit/miss prediction accuracy
105
6.3
Benchmark parameters
106
6.4
Relative energy consumption of benchmark programs
with EELRMenergy management.
Energy values are
percentages of as approach.
Active WaveLAN card
contributes 40% to overall energy budget
107
6.5
Relative performance of benchmark programs under as
or EELRM energy management. Reported values are per-
centages of 00 threshold -
card always awake
110
7.1
Comparison of cores before and after constant propagation
132
8.1
Processor configuration
143
8.2
Benchmark programs
146
9.1
Partitions in our power models
158
9.2
System configuration for experiments
163
10.1
Data structures, sizes and memory hierarchy layer as-
signment. N is the window size, M is the number of
workers , 2NM is the size of one frame which is itera-
tively decoded
181
10.2
Data structures, sizes and memory hierarchy layer as-
signment after data restructuring. 2N is the size of one
worker. Each of these data structures exists M times, i.e.
once for each worker
182
10.3
Effect of parallelizing loop transformations on maximally
achievable throughput and latency
186
10.4
Memories architecture with simulated access energy and
number of accesses per frame
189
11.1
Self interference example
201
11.2
Interference table, for the procedure in Figure 11.4
202
11.3
Interference table for the procedure ijk.matrix.multiplyA
in Figure 11.2
202
12.1
Different classes of execution units and applicable usage
scenarios
213

Contributing Authors
Nevine AbouGhazaleh
Andrea Acquaviva
Alex Arenas
Itsujiro Arita
Luca Benini
Ricardo Bianchini
Bruno Bougard
Francky Catthoor
Bruce Childers
Paolo D'Alberto
Michael Franz
Rajesh Gupta
Taliver Heath
Lei He
Jerry Hom
Ulrich Kremer
Jesus Labarta
Weiping Liao
Suet-Fei Li
Rami Melhem
M. Angels Moncusf
Daniel Mosse
Alexandru Nicolau
Eduardo Pinheiro
Jan Rabaey
Bruno Ricco
Toshinori Sato
K.C. Shashidhar
Greg Stitt
Roy Sutton
Enrique V.Carrera
Frank Vahid
Arnout Vandecappelle
Alexander Veidenbaum
University of Pittsburgh, USA
University of Bologna, Italy
Universitat Rovira i Virgili, Spain
Kyushu Institute of Technology, Japan
University of Bologna, Italy
Rutgers University, USA
IMEC, Belgium
IMEC, Belgium
University of Pittsburgh, USA
University of California-Irvine, USA
University of California-Irvine, USA
University of California-Irvine, USA
Rutgers University, USA
University of California-Los Angeles, USA
Rutgers University, USA
Rutgers University, USA
Universitat Politecnica de Catalunya, Spain
University of California-Los Angeles, USA
University of California-Berkeley, USA
University of Pittsburgh, USA
Universitat Rovira i Virgili, Spain
University of Pittsburgh, USA
University of California-Irvine, USA
Rutgers University, USA
University of California-Berkeley, USA
University of Bologna, Italy
Kyushu Institute of Technology, Japan
IMEC, Belgium
University of California-Riverside, USA
University of California-Berkeley, USA
Rutgers University, USA
University of California-Riverside, USA
lMEC, Belgium
University of California-Irvine, USA

Preface
In the last ten years, power dissipation has emerged as one of the most crit-
ical issues in the development of large-scale integrated circuits, and electronic
systems in general. Technology scaling is not the only cause for this trend: in
fact, we are moving toward a world of pervasive electronics, where our cars,
houses, and even our environment and our bodies will be linked in a finely-
knit network of communicating electronic devices capable of complex com-
putational tasks materializing a vision of "ambient intelligence," the ultimate
goal of embedded computing. Today, power consumption is probably the main
obstacle in the realization of this vision: current electronic systems still require
too much power to perform critical ambient intelligence tasks (e.g., voice pro-
cessing, vision, wireless communication). For this reason, power, or energy
(i.e., power-performance ratio) minimization is now aggressively targeted in
all the phases of electronic system design.
While early low-power (or energy-efficient) design focused on technology
and hardware optimization, it is now clear that software power optimization
is an equally critical goal.
Most of complex integrated systems are highly
programmable. In fact, the new millennium has seen the rapid diffusion of em-
bedded processor cores as the basic computational workhorse for large-scale
integrated systems on silicon, and today we are witnessing the rebirth of mul-
tiprocessor architectures, fully integrated on a single silicon substrate. It is
therefore obvious that the power consumption of integrated systems dominated
by core processors and memories is heavily dependent on the applications they
run and the middleware supporting them.
In general , we can view the software infrastructure as layered in applications
and run-time support middleware (often called "operating system"). Applica-
tions control the user-level functionality of the system, but they interface to
the SoC platform via hardware abstraction layers provided by the middleware.
Software energy minimization can be tackled with some hope ofsuccess only if
both application-level software and middleware are both optimized form maxi-
mum energy efficiency. The Compilers and Operating Systems for Low Power
(COLP) Workshop aims at creating a forum that brings together researchers
operating in both application-level energy optimization and low-power operat-

xx
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
ing systems. The main objective of this initiative is to create opportunities for
cross-fertilization between closely related areas that can greatly benefit from
a tighter interaction. Papers presented at COLP are work-in-progress and are
selected based on their potential for stimulating thoughts and creative discus-
sions.
This book is the result of a careful (and sometimes painful) process of se-
lection and refinementof the most significantcontributions to the 2001 edition
of COLP. The editors have first selected the papers based both on reviewer
evaluations and on feedback from the audience at the oral presentation. They
have then solicited an extended versionof the papers, in a format more suitable
for archival publications. The extended versions have then been reviewed by
the editors to ensure consistency. The results of this "distillation" process have
been collected in this book, which we hope will bring the reader a wealth of
fresh and valuable ideas for further research as well as technology transfer.
Organization
The book is divided into twelve chapters. The first six chapters focus on
low energy operating systems, or more in general, energy-aware middleware
services. The following five chapters are centered on compilationand code op-
timization. Finally, the last chapter takes a more general viewpoint on mobile
computing.
Chapter 1,entitled "Low Power Operating System for HeterogeneousWire-
less Communication Systems" is contributed by Suet-Fei Li, Roy Sutton, and
Jan Rabaey, from UC Berkeley. The chapter describes an ultra-low overhead
operating system for wireless microsensors and compares it with more tradi-
tional embedded operating systems.
Chapter 2, "Low Power Approach in a Modified Dual Priority Schedul-
ing for Hard Real-Time Systems" (by M. Angels Moncusf, A. Arenas, and
J. Labarta from Universitat Rovira i Virgili and Universitat Politecnica de
Catalunya) deal with task scheduling, one of the most classical problems in
real-time operating systems, and investigates a novel dual- priority algorithm
with high energy efficiency.
The thirdchapter,contributedby N. NevineAbouGazelah,D. Mosse, R. Mel-
hem, and B. Childers (from University of Pittsburgh) entitled "A Restricted
Model for the Optimal Placement of Power Management Points in Real Time
Applications" deals with an important issue at the boundary between applica-
tions and operating systems, namely the optimal insertion of systems calls that
dynamically change the supply voltage (and operating frequency) during the
execution of an application.
The fourth chapter, by A. Acquaviva, L. Benini and B. Ricco (Universitadi
Bologna), is entitled "Energy Characterizationof Embedded Real-Time Oper-

PREFACE
xxi
ating Systems." The chapter describes a methodology for characterizing the
energy cost of most primitives and function calls in embedded operating sys-
tems.
Chapter 5, by E. Pinheiro, R. Bianchini, E. Carrera and T. Heath (Rutgers
University), is entitled "Load Balancing and Unbalancing for Power and Per-
formance in cluster-Based Systems" and it deals with an important emerg-
ing topic, namely low-energy multiprocessors. The chapter gives a fresh look
at load balancing issues in cluster-based systems when energy constraints are
tight.
Chapter 6 closes the first group. It is entitled "Energy Management of Vir-
tual Memory on Diskless Devices" (by J. Hom and U. Kremer, Rutgers Uni-
versity) and it deals with virtual memory, one of the basic hardware abstraction
layers provided by standard operating systems.
The next chapter, entitled "Propagating Constants Past Software to Hard-
ware Peripherals in Fixed-Application Embedded Systems," contributed by
G. Stitt and F. Vahid, discusses how propagating application-level constant
to hardware improves both power and form factor, leading to up to 2-3 times
reductions in peripheral size.
In Chapter 8, entitled "Constructive Timing Violation for Improving En-
ergy Efficiency," T. Sato and I. Arita present a technique that relies on a fault-
tolerance mechanism and speculative execution to save power.
Their tech-
nique, called constructive timing violation, guarantees that the timing con-
straints for critical paths are not violated.
In the next chapter, entitled "Power Modeling and Reduction of VLIW Pro-
cessors," the authors W. Liao and L. He present an in-depth study of power
behavior of a VLIW architecture, and develop an infrastructure which can be
used for architecture-based as well as compiler studies.
Chapter 10, entitled "Low Power Design of Turbo Decoder Module with
Exploration of Power-Performance Tradeoffs," demonstrates how a system-
atic data transfer and storage exploration methodology helps characterize en-
ergy and performance behavior of Turbo Coding.
Vandecappelle, Bougard,
Shashidbar, and Catthoor also discuss the cycle budget-energy tradeoff.
In the next chapter, "Static Analysis of Parameterized Loop Nests for En-
ergy Efficient Use of Data Caches," P. D'Alberto, A. Nicolau, A. Veidenbaum,
and R. Gupta demonstrate that the compiler analysis of loop with regular ac-
cess patterns can reveal useful information for optimizing power.
Finally, in Chapter 12, entitled "A Fresh Look at Low-Power Mobile Com-
puting," M. Franz presents a technique that allows large portions of applica-
tions to be offtoaded to a base station for execution.
We believe that, with the proliferation of power-constrained devices, energy
optimizations will become even more important in the future. Consequently, it
is hard to imagine that architectural and circuit-level optimizations alone will

xxii
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
provide the required level of energy efficiency for demanding applications of
next generation computing. The research papers presented here do not only
demonstrate state-of-the-art, but they also prove that, to obtain the best en-
ergy/performance characteristics, compiler, system software, and architecture
must work together.
Acknowledgments
This book grew out of the Workshop on Compilers and Operating Systems,
2001 (COLP 01). We acknowledge the active contribution of the program
committee of COLP 01: Eduard Ayguade, R. Chandramouli, Bruce Childers,
Marco Comero, Rudi Eigenmann, Manish Gupta, Rajiv Gupta, Mary Jane Ir-
win, Uli Kremer, Rainer Leupers, Diana Marculescu, Enric Musoll , Anand
Sivasubramaniam, Mary Lou Soffa, Vamsi K. Srikantam, Chau-Wen Tseng,
Amout Vandecappelle, and N. Vijaykrishnan. In addition, we thank the fol-
lowing reviewers for their thoughtful reviews of the initial submissions to the
workshop: Bharadwaj Amrutur, Eui Young Chung, Anoop Iyer, Miguel Mi-
randa, Phillip Stanley-Marbell, Emil Talpes, Chun Wong, and and Peng Yang.
The feedback from the audience at the COLP 01 workshop is greatly appreci-
ated.
We sincerely thank Alex Greene and Melissa Sullivan, and the editorial team
at Kluwer for their invaluable help, enthusiasm and encouragement through-
out this project. We gratefully acknowledge the support of the U.S. National
Science Foundation through grants CCR-9457768, CCR-0073800, and CCR-
0093082 during this project.
LUCA BENINI, MAHMUT KANDEMIR,J. RAMANUJAM

Chapter 1
LOW POWER OPERATING SYSTEM FOR
HETEROGENEOUS WIRELESS
COMMUNICATION SYSTEM
Suet-Fei Li
Roy Sutton
Jan Rabaey
Department ofElectrical Engineering and Computer Science
University ofCalifornia at Berkeley
Abstract:
Operating systems in embedded wireless commun ication increasingly must
satisfy a tight set of constraints, such as power and real time performance, on
heterogeneous software and hardware architectures. In this domain, it is well
understood that traditional general-purpose operating systems are not efficient
or in many cases not sufficient. More efficient solutions are obtained with
OS's that are developed to exploit the reactive event-driven nature of the
domain and have built-in aggressive power management. As proof, we present
a comparison between two OS's that target this embedded domain : one that is
general-purpose multi-tasking and another that is event-driven.
Preliminary
results indicate that the event-driven as achieves an 8x improvement in
performance, 2x and 30x improvement in instruction and data memory
requirement,
and
a
12x
reduction
in
power
over
its
general-purpose
counterpart. To achieve further efficiency, we propose extensions to the event-
driven as paradigm to support power management at the system behavior,
system architecture, and architecture module level. The proposed novel hybrid
approach to system power management combines distributed power control
with global monitoring.
Key words:
Embedded operating systems, power management, ubiquitous computing, low
energy, heterogeneous architecture.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

2
COMPILERS AND OPERA TING SYSTEMSFOR LOW POWER
1.
Introduction
The implementation of small, mobile , low-cost, energy conscious devices
has
created
unique
challenges
for
today's
designers . The
drive
for
miniaturization and inexpensive fabrication calls for an unprecedented high
level of integration and system heterogeneity.
Limiting battery lifetimes
make energy efficiency a most critical design metric and the real time nature
of applications impose strict performance constraints.
To meet these conflicting and unforgiving constraints, we must rethink
traditional
operating
system
approaches
in
embedded
wireless
communication. General-purpose operating systems developed for broad
application are increasingly less suitable for these types of complex real
time, power-critical domain specific systems implemented on advanced
heterogeneous
architectures.
The
current
practice
of
independently
developing the as and the application, in particular the paradigm of blindly
treating
a task
as
a
random process,
is unlikely
to yield
efficient
implementation [1]. What is needed is an as that is more intimately coupled
to, aware of, and interactive with its managed applications. Specifically, we
need a "lean" but capable as that is developed to target the nature of these
reactive event-driven embedded systems. It should execute with minimal
overhead, be agile, and deploy aggressive power management schemes to
drive down overall system energy expenditure.
To illustrate these concepts, we construct our argument in two steps. To
demonstrate the benefit of a specialized as that closely matches the
application, we will first present a detailed comparison between two as
implementations of the same design -- a wireless protocol stack. The first is
eCOS [2], a popular embedded general-purpose multi-tasking OS and the
second is an event-driven as called TinyOS [3]. Preliminary results indicate
that the event-driven as achieves an 8x improvement in performance, 2x
and 30x improvement in instruction and data memory requirement, and a
l2x reduction in power over its general-purpose counterpart.
The results are certainly very positive, however, we believe that further
improvement can be obtained from proper extension of TinyOS. TinyOS
possess certain qualities that are very attractive for low power heterogeneous
systems. Its event-driven asynchronous characteristics can naturally support
the interactions and communications between modules of vastly different
behavior and processing speeds in a heterogeneous system. Its simplicity
incurs minimal overheads and it has some support for concurrency.
Nevertheless, TinyOS has its own limitations and is insufficient to fulfill
the ambitious role demanded by low power heterogeneous systems. First at
all, TinyOS primitives are microprocessor centric, while advanced system
architectures
consist
of
heterogeneous
modules
of
custom
logic,

Low Power Operating System for Wireless Communication
3
programmable logic, memories, DSPs, embedded processors, and other
optimized domain specific modules.
Furthermore, TinyOS only supports
rudimentary power management scheme. The logical next step is to extend
TinyOS
and
establish
it as
the
global
management
framework
that
incorporates the heterogeneous architecture modules in the system, as well
as devise sophisticated power management mechanisms.
The rest of the chapter is organized as follows. Section 2 presents a
detailed comparison between two as implementations of the same wireless
protocol design. Section 3 proposes a low power reactive operating system
for heterogeneous architectures and the associated global and local power
management strategies; and Section 4 concludes the chapter.
2.
Event-driven versus General-purpose OS
A close "match" between the application and the OS greatly improves
opportunity for an efficient final implementation. By match we mean to have
Models of Computation (MaC) [4] that are similar to that of the application.
MaC is a formal abstraction that defines the interaction of the basic blocks
in the system behavior. In particular, three important properties of the
specification: sequential behavior, concurrent behavior, and communication
have to be clearly defined .
In the following section, we will present a comparison between a
traditional general-purpose multi-tasking as and an event-driven as in
terms of MaC, generality, communication,
concurrency support,
and
memory and performance overhead. The implementation of a wireless
protocol design is used as the case study for both.
2.1
PicoRadio II Protocol Design
PicoRadio [4] is an ad hoc, sensor-based wireless network that comprises
hundreds of programmable and ultra-low power communicating nodes.
PicoRadio applications have the following characteristics: low-data rate,
ultra-low power budget, and mostly passive event-driven computation.
Reactivity is triggered by external events such as sensor data acquisition,
transceiver I/O, timer expiration, and other environmental occurrences. The
chosen MaC for the PicoRadio protocol stack is Concurrent Extended Finite
State Machines (CEFSM) [5]. CEFSM models a network of communicating
extended finite state machines (EFSM), which are finite state machines that
effectively express both control and the computation found in datapath
operations. Each layer in the protocol stack is modeled as an EFSM (Figure
I .I). The communication between EFSMs is asynchronous because the stack

4
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
layers work at differing rates: the lower layers typically run much faster than
the higher layers.
In this second version of the PicoRadio design (PicoRadio II), the
protocol stack has a simple User Interface (UI) layer, a transport layer, a
MAC layer, and an interface to the physical layer. Different layers in the
stack have vastly different processing granularities and speeds: Physical
layer processes at bit level and has to respond in microseconds, while VI
reacts to user requests in seconds and even minutes. Due to their different
behavior, activity and characteristics, the VI and transport layers are to be
implemented in software on the embedded processor while the MAC and
physical layers are implemented with the support of custom optimizes
hardware modules.
2.2
General-purpose Multi-tasking OS
The general-purpose multi-tasking OS was originally developed for the
PC platform and later adapted for general embedded systems. It is good for
supporting several
mostly
independent applications running
in virtual
concurrency.
Suspending
and
resuming
amongst
the
processes
when
appropriate provide support for multi-tasking and/or multi-threading. Inter-
task communication involves context switching which can become an
expensive overhead with increased switching frequency. This overhead is
tolerable for PC applications since the communication and hence switching
frequency
is typically low when compared to the computation block
granularity.
Moreover,
as
these
overheads
grow,
the
wasted
energy
expenditures are of relatively little concern for these virtually infinite energy
systems . As general-purpose OS's do not target low power applications, they
have no built-in energy management mechanisms and any employed are
wholly deferred to the application with its limited system scope.
It is apparent that the MOC of the general-purpose OS is quite different
from that of the protocol stack. The processes across the layered protocol
stack are not independent. They are coupled and activate and deactivated
with events from neighboring processes. In other words, the communication
frequency
is high amongst neighbors and high overheads are far less
tolerable. As we will see shortly, this MOC "mismatch" results in major
inefficiencies.
We have designed a chip (Figure 1.4) to implement the PicoRadio II
protocol stack. Our main design tool is Virtual Component Codesign (VCC)
from Cadence Design Systems [6]. The VCC flow covers the entire design
process from behavior specification to architecture exploration, all the way
down to final hardware/software implementation.

Low Power Operating System for Wireless Communication
5
c,·- - - - - -
....
(,1" .' _,,_.
.
1
. · v'ln.....
- c..,...
C_
. ' (,11-'
-<;1
.- I
, ...
· . (jr···~ ::J
~, -....
t''l.,'O
Protocol stack
i l.;_
~I-- i----'- .o:l
. 1
..· 0...· •
c ·
~ -
\/~
..
r{ '!';
..
.~ ...
.
.?
.' l . ....
r
.J
.'Q~"
1
c ·
Concurrent EFSMs
Figure 1.1. Model ofcomputation for PicoRadio protocol stack

6
COMPILERS AND OPERA TING SYSTEMSFORLOWPOWER
Figure 1.2. Implementing PicoRad io II with vee
Figure 1.2 shows the architecture exploration process. The behavior is
specified
hierarchically
and
then
mapped
onto
different
architecture
elements. In this
particular implementation, the
VI and all
the
leaf
components of the transport layer are mapped onto software/ processor.
The
software implementation
process
in
VCC
takes
a traditional
approach and assumes
a general
purpose multi-tasking as. The code
generation is accomplished by turning each concurrent system component in
the specification into a task. Communication wrapper functions are then
generated to connect the tasks and the as. Figure 1.3 shows the sketches of
the generated code (UI, transportJemote and transport_bs are all system
components). We have chosen the popular eCOS as our embedded as due to
its availability and efficiency.

Low Power Operating System / or Wireless Communication
cyg_thread_resume(task_ui_2_handle);
cyg_thread_resume(task_transport_l_transport_bs
_handle);
cyg_thread_resume(task_transport_Ctransport_re
mote_handle);
:.....}
Figure 1.3. Code generation with general-purpose eCOS
7
40
1/0
-T-
6
TAP
-r--
-
64 KB Instruction SRAM
0;cIii
.c
u
64 KB Data SRAM
'"es:
r-
Sonies
l<len sa
'"
\cUlro!
i
~
I/O
'--
Wring
etlanne)
71
I 38
PPI{
{
flash interfac e
3.6mm
PicoRadio II floorplan
0;
I
c
Iii
to-)
.c
36
u
Co
'" ~
~
~
audio data
1
Figure 1.4. PicoRadio II chip floorplan. Xtensa is the embedded microprocessor.
From the chip
layout, we notice that the software portion of the
architecture including the processor and its memory blocks occupies more

8
COMPILERS AND OPERATING SYSTEMSFOR LOW POWER
than 70% of the total area. This is especially inefficient considering that the
processor is greatly under-utilized (utilization < 7%). Reason being that the
software-implemented VI and transport layers run at much lower activity
and rate (user request and packet level processing) than the hardware-
implemented MAC and physical layers (bit level processing).
Careful analysis of the software code reveals that of the total 10K byte
instruction code size, about 50% is communication overhead. The massive
data memory size of 54K is a result of the communication overhead,
expensive scheduler overhead, memory management, and stack allocations.
2.3
Event-driven OS
TinyOS specifically targets event-driven communication systems. Its
MOC is CEFSM, which matches that of the protocol processing system.
This match drastically reduces the communication overhead as well as other
OS related costs. Because TinyOS is not designed to support a broad range
of general applications, it can cut down on expensive OS services such as
dynamic memory allocation, virtual memory, etc. In addition, unnecessary
performance-degrading polling
is eliminated and context switching is
minimized and very efficiently implemented.
In TinyOS, an application is written as a graph of components. For the
PicoRadio II example, the components would be the layers in the protocol
stack. Each component has command and event handlers that process
commands
and
events
from other components, tasks
that
provide a
mechanism for threaded description, and a static frame that stores internal
state and local variables.
The TinyOS system operation can be briefly described as following:
external events from the RF transceivers or sensors propagate from the
lowest layers up the component graph until handled by the higher layers. To
prevent event loss, the system must process incoming events faster than their
arrival rate. Threaded behavioral description is supported via tasks, which
are operations in the event or command handlers that require a "significant"
number of processor cycles. Tasks are pre-empted by the arrival of an
incoming event and are dispatched from a task queue. TinyOS uses a simple
FIFO task scheduler. Built-in power control is exercised by shutting down
the CPV when no tasks are present in the system after all event processing.

Low Power Operating System fo r Wireless Communication
9
sw
HW
Figure
1.5.
Implementing
PicoRadio
II
Protocol stacks
in
TinyOS.
Arrows
show
events/command s propagat ion in the system.
We have re-implemented the PicoRadio II protocol stack using TinyOS.
In the next section, we will present a comp arison between the general-
purpose OS (eCOS) and the event-driven OS (TinyOS) in three important
performance metrics: memory requirement, performance, and power.
Table 1.1. General comparison
General Purpose as
MaC
Multi-tasking
Generality
General
Communication
Large
Overhead
Communication
Infrequent
Frequency
Memory Requirement
Large
2.4
Comparison Summary
Event-driven as
Communicating
EFSMs
Target event driven
systems
Small
Frequent
Small
Table 1.1 summarizes the contrast between the two OS's as presented in
Sections 2.2 and 2.3. Table 1.2 shows the memo ry requirement comparison
between the two OS's. With the same processor selection (16 bit ARM7),
TinyOS needs
half the instruction memory and one-thirtieth the data
memory. Studies showed that the power consumption of SRAM scales

10
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
roughly as the square root of the capacity [7]. This implies that with TinyOS,
instruction memory power can be reduced by 1.6x, and data memory power
by 4.2x. Using a simpler processor such as 8-bit RISe could further reduce
memory size and power consumption.
Table 1.2. Memory requirements comparison
OS type
Processor
Application
Total
Data mem
instruction
Mem.
General
ARM7
10,096
22324
54988
Purpose
thumb
Event-
ARM7
5312
8000
2800
driven
thumb
Event-
8 bit RISe
2740
3176
709
driven
Total c d e count at I 1Hz
18000
_ 16000
:..
cg 14000 .
o transport_remote
~ 12000
• transport_bs
merge2
~ 10000
D merger1
t=j
8000
-
o data converter
(5
6000 .
. UI
f--o
OOS
4000
2000
o
I
Gen. OS
TinyOS
Figure 1.6. Total cycle count comparison: General-purpose versus event-driven OS. Key at
right identifies system components
Figure 1.6 presents the performance comparison in terms of the total
processor cycle count: 16365 vs. 2554. TinyOS shows a factor of eight
improvement, which translates directly to a factor of eight reduction in
processor power consumption. Figure 1.7 compares the os overhead (the
lowest portion of the bars) as a percentage of the total cycles. As an

Low Power Operating System for Wireless Communication
II
indication of its inefficiency, the general-purpose as has an as overhead of
86% while TinyOS has 10%.
Percentage breakdown at ] 1Hz
J
-
...,...-
o tran-;rt_rC~1~11
e
• transport_bs
mcrgc2
o merger I
I
o data converter
' -:---;~~~ ' Jl
UI
I
10 0
L
_
._---------
30% 1-....."....-1
20%
10%
0%
l--.J.-----'---..,----'----'-~
40%
100% ...---r-- - -, -.,.-..-- -,r---
- ,----,
90%
80%
70%
60%
50%
Gen. OS
TinyOS
Figure 1.7. Perc entage breakdown co mparison: General-purpose versus eve nt-d riven as. Key
at right identifies system components
Now let us calculate how much power is actually saved considering both
the processor and its memory blocks. With a 0.181-lm technology and a
supply voltage of 1.8V, an ARM7 consumes 0.25mW/MHz. For a memory
size of 64KB, read per access consumes OA07mW/MHz and write consumes
0.447mW/MHz. Assume that 10% of the instructions involve memory read
operations and 10% memory writes and apply memory size as well as
processor cycle count scaling, the power consumption for the two OS's are:
0.608mW/MHz and 0.053Mw/MHz. That is, TinyOS demonstrates a factor
of 12 improvement in power.
One may argue that eCOS is an overkill and could be optimized to yield
better performance and power.
ECOS is a reconfigurable OS, and the
authors choose the configuration options that yield the smallest and simplest
implementation. While further optimization could be applied, with a factor
of 12 win in power, TinyOS should remain far superior.

12
COMPILERS AND OPERATING SYSTEMSFOR LOW POWER
3.
Low Power Reactive OS for Heterogeneous
Architectures
Our driving research goal is to design an energy efficient as for domain
specific heterogeneous architectures. We believe that some basic TinyOS
concepts are very attractive and can be adopted to reach such a goal.
TinyOS's event-driven asynchronous characteristics can naturally support
the interactions and communications between modules of vastly different
behavior and processing speeds in a heterogeneous system. Its simplicity
reduces overheads and leads to more power efficient implementation. It also
provides some support for multiple flows of control that are typical of
wireless sensor applications.
However, TinyOS has its limitations and is insufficient for our research
goal. It has to be properly
extended
to the system level to include
management of not only computation on the embedded processor, but also
computation on the optimized architecture modules.
In the following
sections, we will elaborate on the roles of the "system level OS" in the
context
of PicoRadio
III,
a
next
generation
heterogeneous
wireless
communication system and discuss the necessary TinyOS extensions.
3.1
Event-driven Global Scheduler and Power
Management
In a complex heterogeneous system, the as acts like a hardware
abstraction layer [8] that manages a variety of system resources. For a power
critical application, simplicity should be the primary design philosophy. The
as should perform a dedicated set of indispensable duties and only these
duties.
There are two basic as duties: concurrency management at both
application and architecture levels and global power management.
Wireless sensor applications typically have multiple flows of control and
data. A sensor node can sense the environment, forwards packets and receive
commands all at the same time. The as needs to support concurrency in the
application
as
well
as
explore
and
utilize
the
concurrency
in
the
heterogeneous architecture. Since the as has the global "view" of the
system, it can also perform global power management to optimize the
system power consumption. Essentially, the as is a global scheduler with
power management.
In our vision of the system architecture, the as is refocused from the
microprocessor and becomes a separate unit that can be implemented in
software, hardware, reconfigurable logic, or some combination. Traditional
software centric control approaches have a central control unit that schedules
communication between the system modules.
We believe that a more

Low Power Operating System for Wireless Communication
13
efficient approach is to distribute control over the entire system. Global
monitoring is added to further improve the system performance via feedback
derived
from
observations with greater system scope.
In this
control
framework, communication can occur between certain modules without the
intervention of the as. When dependent reactive behaviors are mapped to
interconnected architecture modules, their communication can occur through
the established hardware event channels.
Power management should be applied
at all
levels of the
design
hierarchy: System level, architecture module level, circuit level and device
level [4]. By carefully incorporating local power management into the
individual architectural modules, we can push power management down the
design hierarchy. We propose a hybrid approach to power management,
which combines distributed power controls with global monitoring.
PicoRadio networks are sensor networks characterized by bursty and
mostly
aperiodic
traffic.
The
low-duty
cycle
makes
it essential
that
individual modules are powered down whenever not active. If not, leakage
current will dominate the power budget. Rather than assuming all the
modules in the systems are on and could be turned off to conserve energy,
we assume that they are "off' until powered-up by the arrival of events at
their interface. Internally, modules are awakened either by their neighbors or
by the as. This novel approach assumes the concept of a wake-up radio,
which only turns on when communications are truly needed [9].
Reactive
system OS
,,
"
.Ev nt:Sources
Figure 1.8. Behavior diagram of the PicoRadio sensor node

14
COMPILERS AND OPERA TING SYSTEMS FOR LOW POWER
Figure 1.9. Architectural diagram for PicoRadio sensor node
Figure 1.8 is the behavior diagram of the PicoRadio III sensor node. It
shows the different components in the system and the interactions between
them. Communication between components is purely reactive. Figure 1.9
shows the architectural diagram for the PicoRadio sensor node system. Each
module has an interface that is responsible for its own local power status and
control. When an event arrives from a neighbor, the interface logic will
decide which part of the module should be turned-on to process the event.
The interface could also have voltage-scaling capability built-in to further
control
the power
consumption of the module
by matching module
performance, and hence energy expenditure, with workload. The sleeping
mechanism can be implemented as some function of the module idle time
and wake-up overhead, etc.
On top of this distributed power control mechanism, global monitoring is
implemented to incorporate global information that local modules are not
able to "see".
For this, the system level as supports global power
management.
It
maintains
global
state
by
monitoring
the
module
interactions, and schedules periodic system maintenance accordingly. Based
on its knowledge of the entire system, it issues commands to a module's
interface to override local decisions when there are conflicts of interest. For
example, the network layer may wish to go to sleep since it has not received
any event for a certain amount of time, but the as senses some activity in
the RF layer and might find it advantageous to prevent the network layer
from going to sleep.
3.2
TinyOS Limitations and Proposed Extensions
Given our revised, broader prospective of the operating system, TinyOS is
limited
and
inadequate.
It
is
primarily
designed
for
uni-processor
architectures. All components except for the lowest layers of the application

Low Power Operating Systemfor Wireless Communication
15
are implemented in software. Low-level hardware components are required
to have a software wrapper to interact with the scheduler and the rest of the
system.
This software centric approach does not allow full exploration of
the
integrated,
heterogeneous
system
architecture.
Moreover,
TinyOS
assumes
off-the-shelf components
and
in essence
has
no
access
to
customized
power-efficient
blocks.
TinyOS's
rudimentary
power
management scheme also needs to be greatly improved.
We have proposed the following extensions to TinyOS to establish the
OS as the global system scheduler and power manager.
1. Replace
the simple
FIFO task scheduler in TinyOS
with a more
sophisticated scheduler, which supports voltage scaling of the modules to
which the tasks are assigned. This implies that each task should carry
some
real time
scheduling information.
Scheduling techniques for
variable voltage can be applied to minimize power consumption while
meeting the performance constraints [10] [II] [12].
2. Components can also be implemented in hardware . Moreover, hardware
components need not dispatch tasks. Tasks are introduced in TinyOS to
implement threads in the algorithm on uni-processor architectures. While
executing a task, the processor can be pre-empted to handle higher
priority incoming events. If components are implemented in hardware,
tasks are no longer needed and are removed for simplicity.
3. Tasks are dispatched to either software or hardware. This is to best utilize
the whole system resources since dedicated architectural modules could
be designed to support specific tasks. (In TinyOS , all tasks go into
software.)
4. Add event queues at the lowest layers. This can reduce the external event
losses and make the system more robust. The current TinyOS has no
queue implementation.
5. Add global power control mechanisms. The OS should collect runtime
profiles and statistics, perform periodic system maintenance operations
and maintain system level power state.
4.
Conclusion and Future Work
In this chapter, we have presented issues concerning the implementation
of a low power operating system for heterogeneous communication systems.
We argue that the OS should have a MOC that closely matches the
application, and showed the significant improvement of the event-driven
TinyOS over a popular general-purpose OS as a proof-of-concept. We have
also
discussed
the
necessary
extensions
to
TinyOS
for
supporting
heterogeneous
architectures,
and
proposed
a
novel
system
power

16
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
management framework. The next step is to build a simulation environment
to fine tune the concepts, and eventually implement the OS on the PicoRadio
III system.
References
[1]
K. Ramamritham and lA. Stankovic, "Scheduling Algorithms and Operating
Systems Support for Real-Time Systems", Proceedings ofthe IEEE, January
1994, pp. 55-67.
[2]
http://sources.redhat.com/ecos
[3]
David Culler et ai, The TinyOS group , Department ofEECS, UC Berkeley.
[4]
J. Rabaey et al., "PicoRadio Supports Ad Hoc Ultra-Low Power Wireless
Networking", IEEE Computer, Vol. 33, No.7, July 2000, pp. 42-48.
[5]
E.
Lee
and
A.
Sangiovanni-Vincentelli,
"A
Unified
Framework
for
Comparing Models of Computation", IEEE Trans. on Computer Aided
Design of Integrated Circuits and Systems, Vol. 17, N. 12, pp. 1217-1229,
December 1998.
[6]
http://www.cadence.com
[7]
R. Evans & P. Franzon, "Energy Consumption Modeling and Optimization
for SRAM's", Journal of Solid-State Circuits, Vol. 30, No.5, May 1995.
[8]
A. Ferrari and A. Sangiovanni- Vincentelli, "System Design: Traditional
Concepts and New Paradigms", Proceedings of the Int. Conf. on Computer
Design, Austin, Oct. 1999.
[9]
B.
Otis,
I.
Telliez
and
I.
Cambonie,
"PicoRadio
RF",
http://bwrc.eecs.berkeley.edu/Local/Research/PicoRadio/PHY, January 200 I.
[10]
Y Lin, C. Hwang & A. Wu, "Scheduling Techniques for Variable Voltage
Low Power Designs", ACM Transaction on Design Automation ofElectronic
Systems, Vol. 2, No.2, April 1997, pp 81-97.
(11]
J, Monteiro, S. Devadas, P. Ashar, A. Mauskar, "Scheduling techniques to
enable power management", 33rd Design Automation Conference, June 1996.
[12]
J. Brown, D. Chen, G.W. Greenwood, Xiaobo Hu; R. Taylor, "Scheduling for
power reduction in a real-time system", Proceedings 1997 International
Symposium on Low Power Electronics and Design.

Chapter 2
A MODIFIED DUAL-PRIORITY SCHEDULING
ALGORITHM FOR HARD REAL-TIME SYSTEMS
TO IMPROVE ENERGY SAVINGS
M. Angels Moncusi"
Alex Arenas*
Jesus Labarta**
"Dpt d'Enginyeria Informatica i Matematiques-Universitat Rovira ; Virgili
**Dpt d 'Arquitectura de Computadors-Universitat Politecnica de Catalunya
Abstract:
We present a modificat ion of the dual-priority scheduling algorithm for hard
real-time systems that takes advantage of its performance to efficiently
improve energy saving. The approach exploits the priority scheme to lengthen
the runtime of tasks by reducing the speed of the processor and the voltage
supply, thereby saving energy by spreading execution cycles up to the
maximal time constraints allowed. We show by simulation that our approach
improves the energy saving obtained with a pre-emptive fixed-priority
scheduling scheme.
Key words:
Energy-awareness, on-line scheduling, hard real-time systems, dual-priority
schedul ing, fixed-prior ity scheduling.
1.
Introduction
With the constraint of low power consumption, the design of portable
digital systems has a major drawback from the point of view of operability
and lifetime of the systems [1]. Significant effort has been made during the
last decade to address this problem, but the high levels of performance of
modern microprocessors and micro-controllers jointly with their increasing
functionality obtained via software still requires improvements in the power-
efficiency context.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

18
COMPILERS AND OPERA TING SYSTEMSFOR LOWPOWER
In the use of scheduling strategies to save energy, there exist two main
approaches to reduce power consumption of processors; these approaches
are reducing the speed of the processor and power-down. The first approach
consists of reducing the clock frequency along with the supply voltage
whenever the system does not require its maximum performance. The
second approach simply turns power off when there are no tasks to execute
in the ready queue, apart from the minimal amount of energy required by the
idle processor state (clock generation and timer circuits). Both approaches
are well suited for energy saving but they must be used carefully to ensure
reliable operation, especially in hard real-time systems (2, 3].
Recently, Shin and Choi [4] have proposed a power-efficient version of
fixed-priority scheduling for hard real-time systems that deals with the two
approaches mentioned earlier. The main idea of their approach is the use of
pre-emptive fixed-priority scheduling (Rate Monotonic Scheduling RMS [5]
or Deadline Monotonic Scheduling DMS) to organize the tasks with the pre-
emptive priority scheduler into a run queue that is used to exploit both
execution time variation and idle time intervals to save energy by reducing
speed and voltage or power down. The process ensures that all tasks meet
their deadlines. However, the strategy of Shin and Choi [4] can only reduce
the speed of the processor when there is only one task in the run queue, or
bring the processor to power-down mode when there is an idle interval;
otherwise the processor works at the maximum speed.
In this chapter we present an improvement of the strategy followed by
Shin and Choi [4] by using a modification of dual-priority scheduling, first
proposed by Davis and Wellings [6]. We harness the ability of the dual-
priority scheduling scheme to execute periodic tasks as late as possible in
order to save energy.
The dual-priority scheme was designed to execute aperiodic tasks
without deadlines as soon
as possible while
preserving the deadline
constraints of the periodic tasks . The algorithm is implemented as a three-
queue structure consisting of the upper run queue, the aperiodic run queue,
and the lower run queue. Whenever a periodic task is ready to be executed, it
enters the lower run queue. Eventually this task can be pre-empted by an
aperiodic task; finally, if the task cannot be delayed more because otherwise
its deadline could be compromised, then the task is promoted to the upper
run queue where its execution is prioritized.
This scenario is interesting even when no aperiodic tasks are present, as
is the case in our study ; in this particular case, the algorithm needs only two
queues. The energy-reduction is obtained mainly by means of speed and
voltage reduction and sometimes by using power-down . Our approach is to
run the tasks at the lowest speed that makes it possible that the active task
and the rest of tasks meet their timing constraints. This is done without
imposing the constraint of only one task in the run queue to save energy and

A Modified Dual-Priority Scheduling Algorithm
19
power-down the processor when there is an idle interval, as done by of Shin
and Choi [4].
This
approach
is
especially
interesting
because of the
quadratic
dependency of the power dissipation on the voltage supply in CMOS circuits
[1]. The power dissipation satisfies approximately the formula
where PI is the probability of switching in power transition, CL is the loading
capacitance, Vdd the voltage supply and !elk the clock frequency. This means
that it is always preferable from an energy point of view to execute at a
slower rate and at a lower voltage rather than quickly at higher voltage.
The basic idea of the modified algorithm we present is to organize the
run tasks in two levels of priorities. In the highest level, there are those
periodic tasks whose execution can no longer be delayed by tasks from the
lower priority level; otherwise these periodic tasks can miss their deadlines.
The second level is occupied by those periodic tasks whose execution time
can still be delayed without compromising their ability to meet their
deadlines. Each of the two
levels in tum
is hierarchically organized
according to some static priority assignment. To obtain additional savings in
power, another slight modification is introduced: the lower run queue is
sorted by the promotion times instead of fixed priorities. This approach is
simple enough to be implemented in most of the kernels. In contrast to Shin
and Choi [4], we only use an extra run queue and a promotion time for each
periodic task in the system. Thus, the amount of extra complexity introduced
by this new algorithm is minimal.
The chapter is organized as follows. In Section 2, we describe the basics
of dual-priority scheduling. Section 3 is devoted to the modification of the
algorithm to reduce energy consumption. In Section 4, we present the
experimental results and the comparison with Power Low Fixed Priority
scheduling, and in Section 5 we present some conclusions.
2.
Dual-Priority Scheduling
We assume that the framework of the hard real-time system we are going
to deal with is made up of periodic tasks I . These tasks -
numbered I ::; i ::; n
-
are specified by their periods, worst-case execution times and deadlines
rr, Cj and Dj respectively).
I. Note that the results are not exclusively for periodic tasks. We have considered only periodic tasks as a
matter of simplicity.

20
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
The system is organized as a set of concurrent tasks managed by a pre-
emptive priority-based scheduler whose details are described below . The
computation times for context switching and for the scheduler are assumed
to be negligible; this enables us to perform the analysis in a straightforward
manner without the danger of loss of generality. The extent to which these
assumptions are realistic is discussed in the analysis of the algorithm given
in [6], and it turns out to be practical if the context switching time is
subsumed by the worst case execution times of the different tasks.
The mechanics of the dual-priority scheduling algorithm is as follows.
Let us assume that the tasks have some initial priorities assigned according
to a fixed priority criterion in such a way that two different periodic tasks
never have the same priority. These initial priorities are altered by the
scheduler according to the following scheme. First, two levels of priorities
are organized: the highest level or the upper run queue (URQ) is for tasks
that can no longer be delayed by lower-priority tasks otherwise they could
miss their deadlines; the second level or the lower run queue (LRQ) is
occupied by those periodic tasks whose execution time can still be delayed
without compromising the meeting oftheir deadlines.
The scheduling algorithm is driven by the activation times of the tasks
and the promotion instants from the LRQ to the URQ, whenever one of the
following time signals appears:
a) The signal is the activation time (tajk) for the k" instance of the periodic
task i. In this case, for all tasks with activation times less or equal to the
current time t, the relative promotion time instant of task i (tPi) is pre-
computed as tp,= OJ- R, (R, corresponds to the worst case response time
of task i [8]). This value can be computed off-line and provides the
maximum time a task can be delayed so that it can still meet its deadline.
Those tasks with tPi=O are promoted to the URQ, and the rest are queued
in the LRQ. After that, we compute the absolute promotion time instant
for the k" activation of task i in the LRQ as tp., = taik + L, and a timer is
activated to this value.
b) The signal is a promotion time instant (tPik) for the kth instance of the
periodic task i. In this case, all tasks in the LRQ with tPik ~ tc (current
time) are moved to the URQ. Now, tp., corresponds to tPik = Oik - R,
where Oikis the absolute deadline for the kth activation of task i (tOj+ kTi+
OJ), where tOj is the arrival time of the first instance.
Finally, the next executing task is selected by picking the highest priority
task from the highest non-empty priority level (i.e., URQ or LRQ, in that
order). This task executes until its termination or pre-emption by a higher
priority task.
The on-line scheduling solution that dual-priority scheduling algorithm
provides is operative in the vast majority of kernels and computationally
efficient [6] [7]. This algorithm was conceived to schedule tasks with hard

A Modified Dual-Priority Scheduling Algorithm
21
deadlines in a hard real-time environment containing periodic and aperiodic
tasks. The goal of the dual-priority scheduling algorithm is to a give good
response time to aperiodic tasks delaying as much as possible the periodic
tasks without compromising their deadline. In this hard real-time scenario,
spare time is possible due to tasks not consuming all their worst case
execution time. The dual-priority algorithm can use this spare time to
execute aperiodic tasks sooner, giving them a better response time . Our goal
is to take advantage of this performance from the energy saving point of
view. The scheduling algorithm can be modified to extract the maximum
time extension allowed by the real-time system, and this lengthening of time
execution will be accompanied by a speed and voltage supply reduction, and
finally energy reduction, as we explain in the next section.
3.
Power-Low Modified Dual-Priority Scheduling
We have modified the dual-priority scheduling algorithm to help power
saving in a hard real-time system. The original dual-priority guarantees to
meet the periodic temporal constraints, then our modification only needs to
care about when and how to reduce energy by slowing speed and voltage
jointly (we are assuming a linear relation between speed and voltage supply
decreasing).
We have ordered the URQ by the static priority of the tasks and the LRQ
by their absolute promotion time Lik,. where Lik is the absolute promotion
time of the k" instance of task i. The decision to order the LRQ this way is
due to the fact that the task with the lowest Lik will promote earlier and then
it will execute earlier than the others. If the URQ is empty, the first task
from the LRQ will begin to execute as slowly as possible until its promotion
or a pre-emption by another task . Figure 2.1 shows the pseudo code for the
PLMDP (Power Low Modified Dual-Priority Scheduling). The algorithm
work as follows:
a) If both queues URQ and LRQ are empty, then the power-down mode
is activated until the arrival of the next task instance taik (Lines Ll-L4
of Figure 2.1).
b) If the queue URQ is empty but there are tasks in LRQ then the k"
activation of the task i with the highest priority in the LRQ (that is
ordered in terms of absolute promotion time tPik) is activated (Line L6
of Figure 2.1). Before the execution of this task, the algorithm needs
to fix the processor speed ratio according to the maximum spreading
in time that is allowed in executing this task i. The speed ratio is
calculated following the heuristics proposed by Shin and Choi [4] that
is based on the assumption that the delay is negligible. The safeness of

22
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
the system under these conditions is proved in Theorem 1 of the cited
work. In this case, we calculate tpik, the time the current active task
promotes, as taik plus the deadline OJ minus R, (worst case response
time of task i); see Line L7 ofFigure2.1. After that, we determine the
time we dispose before some other task will promote to the URQ to
calculate the maximum speed reduction allowed. Here we describe the
different scenarios we can find:
II minimum speed
II active task=Taski
- wake up delay)
Speed=---
ta, - tc
t p,
= t a,
+
Dj
-
Ri
if tak
< tPi and tpk
< tPi then
1
L9
Ll
if empty (URQ)
then
L2
if empty (LRQ) then
L3
Set timer to
(next taj
L4
Enter power-down mode
L5
else
L6
Active task = LRQ.head
L7
L8
L10
else
Lll
if t p,
< tPj and Pi
< Pj then
II jehp(i)
L12
min(tPj - tp., remainigrC, )
Speed = ---=--.-------
mm(tPj ' td.) - tc
L13
else
t p,
< tpm
< tPj and Pm
< Pi < Pj
L14
Speed =
min(tPj - tp;, remaining(C;)
mintrnaxftp,, tp, + remainig(C;), td.) - tc
L15
endif
L16
endif
L17
Execute active task
L18
endif
L19 else
L20
Active Task = URQ.head ;
L2l
if URQ.head .next
= NIL then
L22
Speed = min(tp~ - tC,remainigC;)
mm(tpk' td.) - tc
L23
else
L24
Speed = 1 .0
II maximum speed
L25
endif
L26
Execute active task
L27 endif
Figure 2.1. Pseudo code for Power Low Modified Dual-Priority (PLMDP) Scheduling

A Modified Dual-Priority Scheduling Algorithm
23
If there exists some task j that has not yet arrived, with a promotion
time (tpj) shorter than the promotion time of the active task (task i),
then, as soon as this task j arrives, it will pre-empt the active task .
Before the arrival time of this new task j, we have an interval time to
execute the active task with a speed reduction (see L8-L9 of Figure
2.1). In this case the speed of the processor should be the minimum
possible speed.
On the other hand, if the next promotion time happens to be the
promotion time of the active task, then there is no reason to restrict the
speed reduction until the following promotion time (tps) of any task k.
If the priority of task k in the URQ is higher than the priority of the
active task, then we can execute the active task i, until this time tpk.
reducing speed (Line LII of Figure 2.1). To assign the corresponding
speed in this case, we calculate the amount of work that the task should
execute (Tk) to not compromise the temporal constraints. I'k will be the
minimum of the
remaining execution time of C, and
the
time
difference between the promotion time of task k and the promotion
time of task i. We also calculate the time we have to execute this work
f k, i.e., the minimum of the promotion time of task k and the deadline
of task i, minus the current time . (Line LI2 of Figure 2.1).
If the priority of task k is lower than the priority of the active task (Line
L 13), we should look for the promotion time of a higher priority task of
active task, because task k will never pre-empt task i. See Figure 2.2
for a graphical explanation. r k is calculated as explained before, and to
calculate the time the task dispose to do the work r k we have to
calculate the
difference between the
minimum time between the
maximum between tp, and the tp, plus the remaining Ci, and the
deadline of task i, and the current time tc (Line L14 of Figure 2.1).
Again, see Figure 2.2 for a graphical explanation of the different
possibilities.
c) If the URQ has only one task to execute, then this is the active task
(Line L20 of Figure 2.1) and the processor speed is calculated as the
quotient of the minimum of the time of next promotion time from the
current time and the remaining C; and the total time available to
execute this task which is the minimum of the next promotion time and
the current task deadline, minus the current time (see L21-L22 of
Figure 2.1).
d) If there is more than one task in the URQ, the first task is executed at
the maximum speed allowed by the processor (see L24 of Figure 2.1).
In practice it is obvious that only certain discrete values of the frequency
of the clock and speed are available; in this case the selection is always a
frequency equal to or larger than the calculated one to ensure that time
constraints are met.

24
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
tp
Pi
tPi
i
\1/
\/
\ /
tPt
tPt
tp.,
\ /
~
I
tPil
I
tPil
!
I
I
II'
&
&
r tpm-Ie
T tPj+Cj-te
T
tpr te
-p
+P
tc
Graph a)
tc
Graph b)
te
Graph e)
Figure 2.2. Maximumextensiontime in three differentsituations
In this algorithm, the speed is calculated on the assumption that all tasks
consume their worst case execution time (WCET); but in practice, the tasks
execute only for part of their WCET. Nevertheless, it is impossible for the
scheduler to know a priori the fraction of WCET that will be used. This
implies that the speed calculated by the algorithm is the minimum speed that
can guarantee that the theoretical time constraints are met. The difference
between the theoretical time constraints and the time consumed in practice
could be normally used to reduce the speed of the next executing task.
Now we present an example to better illustrate the functioning of our
algorithm (PLMDP) in comparison with low power fixed-priority scheduling
(LPFPS). The benchmark presented in Table 2.1 is the same as the one used
in Shin and Choi [4]. In Figures 2.3 and 2.4, we represent the execution of
both algorithms (LPFPS and PLMDP) when all tasks consume 100% of their
WCET, and in Figures 2.5 and 2.6 we represent the execution of the
algorithms when all tasks consume the 50% of their WCET. In all these
figures, the vertical up arrows represent the arrival of the task to the system,
the vertical down arrows represent the promotion time of the task, and the
horizontal arrows stand for the time the task could lengthen its execution
time. Each box represent five time units (although our minimal calculation
unit corresponds to 1 time unit), and each line corresponds to task T I, T2
and T3 respectively. The shaded circles represent idle time in the system.
Note that there is no idle time in Figures 2.3 and 2.4, i.e., the tasks use all
available time.
Table 2.1. Benchmark task set used by Shin and Choi [4]
Task
T
0
WCET
R
D-R
P
Tl
50
50
10
10
40
1
T2
80
80
20
30
50
2
T3
100
100
40
80
20
3
In Figure 2.3, we have represented the behavior of LPFPS. The LPFPS
algorithm is driven by pre-emptive fixed-priority scheduling. This algorithm

A Modified Dual-Priority Scheduling Algorithm
25
consists of executing tasks as slow as possible while satisfying time
constraints. LPFPS reduces clock speed along with the voltage supply only
when there is a unique task ready to be executed; otherwise LPFPS does not
guarantee that the time constraints of the rest of the system tasks will be met.
It also powers down the processor when there are no ready tasks (see Shin
and Choi [4] for an extended explanation).
o
50
100
ISO
200
200
250
300
350
400
Figure 2.3. Execution time in LPFPS when all tasks use 100% WCET.
o
50
100
150
200
T2
"" *
T3 Lt:..JI.~_~
_____________t
200
250
300
350
400
Figure 2.4. Execution time in PLMDP when all tasks use 100% WCET.
Now, let focus our attention on Figure 2.4 which represents the behavior
of our algorithm, which is as follows. At time t=O, all the three tasks arrive
in the system and then they are placed at the LRQ sorted by their promotion
times (tPT3=20, tpTl=40, tpT2=50). The first task to be promoted according to

26
COMPILERS AND OPERA TING SYSTEMS FOR LOW POWER
our scheme will be T3, and it is activated. Its promotion time is at t=20, and
we can execute this task until t=40 (promotion time instant of T1) without
any problem. Executing T3 as late as possible implies that the execution
time of T3 should start at its promotion time (t=20) and it would be pre-
empted at t=40; this means that task T3 has 40 time units to execute 20 time
units, so we can then reduce the speed and the power supply. At t=40, Tl is
promoted and it pre-empts T3 because TI has a higher priority. TI is now
the active task, and has to execute at maximum speed because T3 is in the
URQ. TI executes for 10 units of time and finishes by its deadline, at time
t=50 . At that moment, T2 is promoted as it is the higher priority task in the
URQ; it executes at the maximum speed until it finishes, at time t=70. At
this time T3 is the unique task in the URQ so it can be executed until the
next promotion time, t=90. T3 can execute for 40 units of time at low speed,
but since 20 units of execution (in WCET) remain for T3, it has to execute at
the maximum speed to finish its WCET. The algorithm continues with this
behavior until time 200. At time t=200 we have, again, all the tasks in the
LRQ, but now the first promotion time corresponds to T2 (tpT2=2l0). T2
becomes the active task. In order to know how much time this task could
execute during its remaining time, we should look for the maximum value
between tpT2=210 plus the remaining time (20 units time) and tPT3=220; so
T2 continues until t=230, and T2 finishes. After that, T3 is the active task, it
is alone in the URQ but its remaining time is 40 units and t=250 is the
promotion time of TI, so T3 has to execute at maximum speed during 20
units of time. The algorithm continues with this behavior during all its
hyper-period. After that the tasks will repeat the same pattern.
In this way the algorithm uses the excess time to work at slower
processor speed and lower voltage. In Figures 2.5 and 2.6, we show the
execution of both algorithms in a different situation in which all tasks
consume 50% of their WCET.
In Figure 2.5 we have represented the behavior of the LPFPS (Shin and
Choi [4]), and in Figure 2.6 we represent the behavior of our algorithm.
Although the main behavior of the algorithm is identical to the behavior
described before, this new situation results
in more idle time for the
processor that should be used for energy saving. At time t=O, all three tasks
arrive in the system, but now, task T3 finishes at time 40 because it only
executes 50% of its WCET. Task Tl is now the active task in the URQ, it
executes 5 units time at maximum speed because its WCET is 10 and its
deadline is 50, but this task finishes at time 45 because it now executes only
5 units. After that, there is only task T2 in the LRQ that promotes at time 50
to the URQ. At time 50, task Tl will arrive; it enters the LRQ and promotes
to the URQ at time 90. Then we can reduce the clock speed expecting to
finish at its deadline at time 80, the minimum between the deadline of the
active task (t=80) and the promotion time of a higher priority task (t=90). As

A Modified Dual-Priority Scheduling Algorithm
27
task T2 executes only a half of its WCET, it finishes at time 63 and the
processor continues with task T1 that now can reduce speed again, expecting
to finish by its deadline that is in this case the minimum between the
promotion time of task T2 and the deadline task Tl. After that, task T1
executes at low speed and finishes by time 80. At time 80, task T2 arrives to
the LRQ and it stands alone until time 100 when task T1 and task T3 arrive.
The promotion time of task T3 occurs at time 120 while the promotion time
of task T2 occurs at time 130, so that in the LRQ task T3 will have the
highest priority because the promotion time is earlier. For that reason task
T2 should execute at the lowest possible speed only until task T3 arrives and
pre-empt task T2.
o
50
100
150
200
Figure2.5. Execution time in LPFPS when all tasks use 50% weET.
To summarize the comparison, in Figures 2.3 and 2.4, when tasks
consume all their WCET, neither LPFPS nor PLMDP have idle interval
times.
In this particular case there
is no big difference between the
performances of the two algorithms. The only difference is that the energy
saving occurs at different times but globally the total amount is the same. On
the other hand when the tasks consume only 50% ofWCET (Figures 2.5 and
2.6), PLMDP has only 20 units of idle time, while LPFPS has 167 free units
time. This effect translates in our algorithm into an energy saving of around
300% with respect to LPFPS .

28
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
In general, real-time systems behave in a mixed situation with a few tasks
consuming 100% of its WCET and the rest consuming fractions of their
WCET. In these cases, our algorithm shows improvement in energy saving
over that obtained with a fixed-priority scheduling algorithm.
200
III>
150
100
50
o
T2
T3 Y---z_
_ U~~t
T1
200
250
300
350
400
Figure2.6. Execution time in PLMDP when all tasks use 50% WCET.
c LPFPS
C PLMDP
1,0 -r--
-
, .-..---------
>.
*' s 0,8
; 'a 06
a: §
,
.!::!
CUI 0,4
OJ
E 8 0,2
oz
0,° j.LJ.-,-J=
,.-L'!;.J.,J.;:L.L.,..L-l;.J.,Jc::u.P..J.,-'''-L.L.,-I''L:.L,-L.;;.t.;;LJ
Consumed WCET
Figure 2.7. Comparison of both algorithms in the task set proposed by Shin and Choi [4]
4.
Experimental Results
In order to evaluate the capabilities of the PLMDP approach, we have
simulated several task sets (synthetic and real) and compared the total
energy results per hyper-period obtained with those of the Low-Power

A Modified Dual-Priority Scheduling Algorithm
29
Fixed-Priority Scheduling (LPFPS) proposed by Shin and Choi [4]. For
completeness, we plot the performance of both schemes in the example task
set explained before and represented by Table
2.1; the results of this
comparison are given
in Figure
2.7. In the experiment we vary
the
percentage of consumption of the worst case execution time (WCET) of
tasks to better analyze the performance in different situations.
In this example the average improvement calculated as the ratio between
the energy consumption of LPFPS and the energy consumption of PLMDP,
is 1.62, i.e., we save 38.27% of the average energy consumed by LPFPS .
Note that, even when the 100% of the WCET is consumed (see Figures 2.3
and 2.4), the total consumption energy is improved by our algorithm. This
difference in energy is due to the use of the idle time to reduce the processor
speed in different instants during the hyper-period.
To test our algorithm, we have also performed several experiments using
100 different synthetic task sets for each experiment. All tasks sets are
formed by 10 schedulable periodic tasks, and for each task, we vary from
10% to 100% of the WCET consumption. In all the experiments, we check
how harmonicity could affect the results, using harmonic task set and non-
harmonic task set, and we also check how workload could change the results
varying both the workload of the system, and the tasks workload. To
summarize we have made three groups of experiments:
a) Varying the load of the system between 50% and 90%: The maximum
task workload was fixed to 20%. The periods range from 100 to 1000
time units for the non-harmonic task sets and from 1024 to 131072 for
the harmonic task sets (Figures 2.8-2.11).
b) Varying the ratio between the maximum task period (Tmax) and the
minimum task period (Tmin) from 0.1 to 0.00001 : The periods range
from Tmin to Tmax. The workload of the system is fixed to 80 % and
the maximum task workload was fixed to 20%. (Figure 2.12).
c) Varying the maximum of task workload between 10% and 40%. The
workload of the system was fixed to 80% and the periods are range
from 100 to 1000 units of time for the non-harmonic task sets and
from 1024 to 131072 for the harmonic task sets. (Figure 2.13).
In Figure 2.8 we can see the influence of the usage of different
percentages of WCET in the efficiency of energy consumption. When all the
tasks consume 100% of their WCET, the improvement using our algorithm
is not representative, but as tasks consume lower percentages of WCET, our
algorithm improvement is very important. The normalized mean deviation of
the energy consumption for the LPFPS is 0.014, the maximum normalized
deviation being 0.018 and the minimum normalized deviation 0.004. This
implies that the accuracy of our results is within the 2.57% of error. And for

30
COMPILERS AND OPERATING SYSTEMS FOR LOWPO WER
the PLMDP, the normalized mean deviation of the energy consumption is
0.026, being the maximum normalized deviation 0.040 and the minimum
normalized deviation is 0.002. In that case the accuracy of the results is
within the 6.13%. The average improvement of our algorithm in this case is
1.25 times the energy efficiency obtained by using LPFPS.
1,0 .--
-
----,-
-
-
-
-
-
>-
0 LPFPS
2' e
0,8
0 PLMDP
QI 0
5i:a06
't:l
E
'
.~ iil 04
iii §
,
E u
0,2
s
0,0
<ft-
<ft-
<ft-
~
<ft-
~
~
<ft-
<ft-
<ft-
0
0
0
~
0
0
0
0
0
0
0
0
0
C\J
(")
"'"
lO
'"
,....
co
Ol
0
~
Consumed WCET
Figure 2.8. Comparison of both algorithms when the workload of the system is 80%.
In Figures 2.9 and 2.10, we can see the influence of the system workload
variation when all tasks consume its 100 % of WCET and its 50% of WCET
respectively. In the former, the average improvement of our algorithm is
1.02 times the energy efficiency obtained by LPFPS , while the improvement
increases as the percentage of WCET consumption decreases, being 1.42 in
the case of 50% of WCET consumption. In the extreme case of 10% of
WCET consumption the improvement achieves the ratio value of 34.98 . The
reason for this increment is that because our algorithm can adapt its behavior
to the real load of the system, thereby executing almost always at reduced
speed. The concl usion is that the system workload affects both algorithms
with the interval of differences in the energy consumption [0.01 - 0.02]
when tasks consume 100%, and [0.17 - 0.26] when tasks consumption is
50% of their WCET. These differences in energy consumption between both
algorithms are practically constant when varying the workload of the system.

A Modified Dual-Priority Scheduling Algorithm
31
0,0
1,0 ,.--
-
...,.-
IDLPFPS
0,8
IDPLMDP
>-2'c
Gl
0
Iii '[ 0,6
i
E
N
~
::
~
0,4
CIl
0
E 0
o
0,2
z
System Workload
Figure 2.9. System workload variation when all tasks consume the 100% ofWCET.
1,0 ,---,..--
-
-
-
-
-
- --
IDLPFPS
>-
2' e
0,8
IDPLMDP
Gl
0
Iii '[ 0,6
i
E
.t! ill 04
'iii S '
g 0
0,2
z
0,0
~
og
System Workload
Figure 2.10. System workload variation when all tasks consume the 50% of WCET
In Figure
2.11,
note
that
the
normalized
energy
consumption
is
represented on a logarithmic scale, so we can compare the improvement of
energy consumption of our algorithm (PLMDP) with that of LPFPS, when
all tasks consume 50% of the WeET, in two different situations: (i) when
the task periods are harmonic (LPFPS-h, PLMDP-h), and (ii) when the task
periods are non-harmonic. In the former case, the mean improvement
achieved by our algorithm is 1.42 and in the latter case it is 1.29. The
accuracy of our results are within a 6.15% in the case of LPFPS with
harmonic tasks, 18.5% in the case of PLMDP with harmonic tasks and for
the non-harmonic tasks is 5.17% and 9.85% in the case of LPFPS and
PLMDP respectively. In general we see that the differences in the accuracy
of the results due to the statistics is lower for LPFPS than for PLMDP
because while the LPFPS reduces speed when there is a unique task ready to
be executed, the behavior of PLMDP is more complex and is dependent on
the particular task set.

32
COMPILERS AND OPERATING SYSTEMS FORLOWPOWER
lEI LPFPS-h [J PLMDP-h [J PLFPS-nh EI PLMDP-nh I
1.0E+OO
1.0E+02 ..
1,OE+08
~
~ ~
1,OE+06
CIl Co
i
E
1,OE+04 "
N :l
:: l!!
"'
0
E u
oz
---_..._ - -_.._-_._ - - _ .._
....,
~
~
~
f5
R
f5
Consumed WCET
Figure 2.11. System workload and harmonicity of the tasks periods variation
[ [J LPFPS [J PLMDP I
task workload
Figure 2.12. Maximum task workload variation. Non-harmonics periods
We checked the performance of the system when the maximum task
workload varies (Figure 2.12). In this situation, we cannot see significant
differences in energy consumption independently if the task set is harmonic
or not. Finally, the performance is evaluated when the ratio of periods
enlarges. The results represented in Figure 2.13 show the variations in
energy consumption.
To conclude the present analysis, we have also collected some real time
applications: the Avionics task set [9], an Inertial Navigation System (INS)
[10], and a Computerized Numerical Control Machine (CNC) [11].
The two first sets represent critical mission applications and the last one
is an automatic control for specific machinery. The characteristics of the
tasks sets are given in Tables 2.2 to 2.4.

A Modified Dual-Priority Scheduling Algorithm
IElLPFPS CI PLM~
1,0 -,---- ----
-
-
>-
~ g 0,8
c
.~
Cla06
al E
'
~ iii 0,4
l'll
C
E 8 0,2
o
z
0,0 -t--L=;;.L...,.---"'"
t.s-oi
l ,E-02
1,E-03
1,E-04
1,E-05
Tmln/Tmax
Figure 2.J3. TminlTmax variation
Table 2.2. Avionics benchmark task set r9
Task
T
D
WCET
TI
100
100
5.1
T2
20000
20000
300
T3
2500
2500
200
T4
2500
2500
500
T5
4000
4000
100
T6
5000
5000
300
T7
5000
5000
500
T8
5900
5900
800
T9
8000
8000
900
TIO
8000
8000
200
Til
10000
10000
500
TI2
20000
20000
300
Tl3
20000
200000
100
TI4
20000
20000
100
Tl5
20000
20000
300
Tl6
100000
100000
100
Tl7
100000
100000
100
Table 2.3. INS benchmark task set [10]
Task
T
D
WCET
T1
250
250
118
T2
4000
4000
428
T3
62500
62500
1028
T4
100000
100000
2028
T5
100000
100000
10028
T6
125000
125000
2500
33

34
COMPILERS AND OPERATING SYSTEMS FORLOWPOWER
Table 2 4 CNC benchmark task set [11]
. .
Task
T
D
WCET
T1
2400
2400
35
T2
2400
2400
40
T3
4800
4800
180
T4
4800
4800
720
T5
2400
2400
165
T6
2400
2400
165
T7
9600
9600
570
T8
7800
7800
570
The results of energy consumption for each application are shown in
Figures 2.14 to 2.16. The average factor of improvement of our algorithm in
comparison with LPFPS is 1.18 times for the avionics data set, 1.21 times
for the INS task set and 2.09 times for the CNC data set.
IDLPFPS
ClPLMDP
1,0 ,-----,--
-
-
-
-
-
-
-
-
-,,,,""
>-
~ IS 0,8
c:.,
i e- 0,6
.~
~ 04
"iii §
,
E 0
02 ·
o
'
z
0,0 +""'----rJ"""'"-r""''''-r'''''''-r="'-r'''''''-r"'"'''"Y'''=,-LLa-,''''w.;
Consumed WCET
Figure 2.14. Comparison of both algorithms in the avionics task set [9]
If we pay attention to the specific behavior of the individual benchmarks
we
observe
that
the
relationship
between
periods
and
WCETs
are
responsible for the main differences between the two approaches; i.e., for
example in the avionics and INS task sets, there is a sub-set of tasks that
have a very large period compared with the respective WCET, this fact
implies that for a long time there is a unique task in the system, and then our
algorithm behaves very similar to LPFPS .
On the other hand, we observe also that both algorithms behave similarly
when the WCET is exhausted, in three of the four case studies. This is so
because in general, in this case, there is not any extra time to consume, and
then no more energy could be saved using only a scheduling strategy.
However, in the CNC task set there is a particular configuration of tasks that

A Modified Dual-Priority Scheduling Algorithm
35
have a very large ratio between the periods and WCET, but still it is possible
to take advantage of many short times with significant reduction of speed
even when the tasks are using the whole WCET, while LPFPS, in this same
situation, has usually a few large time intervals where the speed can be
reduced (see Figure 2.16). In the opposite situation, i.e., when the tasks
consume less than
10% of the WCET, it is difficult to perceive the
differences. Finally, when the utilization of the WCET is around half, the
difference between the performance ofLPFPS and PLMDP is more relevant.
>-
1,0
Il:J LPFPS
~ :5 0,8
CI PLMDP
lil a
0 6
i
§
,
.!:! [!! 0,4
'iii 0
~
u 0,2 .
z
0,0
+",,"--,-J""'=,w.~ua..r'""'a.,-""""-T..L:.LCY"u..,-
Consumed WCEr
Figure 2.15. Comparison of both algorithms in the INS task set [10]
>-
1,0
Il:J LPFPS
~ c
0,8 . CI PLMDP
c,g
i
~ 0,6
.!:! 1il 04
n; §
,
E u 02
o
'
z
0,0 -f-L"'--rll.L-r""""-r"""'Y"u..,-llL..I,-""'u,=-r'""u,
Consumed WCEr
Figure 2.16. Comparison of both algorithms in the CNC task set [I I]
All the experiments represent the results of the normalized average
energy obtained, varying the consumed worst execution time from 10% to
100%. We run the simulation over one hyper-period (that is, the least
common multiple of the periods of the tasks).

36
COMPILERS AND OPERA TING SYSTEMS FOR LOW POWER
5.
Summary
We have presented a modification of the dual-priority scheduling
algorithm to improve over fixed-priority scheduling, motivated by energy
savings while maintaining the low complexity of the algorithm. This
approach (PLMDP) has been shown to out-perform the mentioned LPFPS,
saving energy by an average factor that ranges from 1.17 up to 2.09
depending on the real-time application. The algorithm has the same time
complexity as LPFPS and can be easily implemented in most of the kernels.
References
[I]
A.P. Chandrakasan, S. Sheng and R. W. Brodersen, "Low-power CMOS digital
design," IEEE Journal ofSolid-State Circuits, vol. 27, pp. 473-484, April 1992.
[2]
D. Mosse, H. Aydin, B. Childers and R. Me/hem, "Compiler-assisted power-aware
scheduling for real-time applications," Proceedings ofthe Workshop on Compilers and
Operating Systems for Low Power (COLP 2000), Philadelphia, Pennsylvania, October
2000.
[3]
H. Aydin,
R. Melhem,
D. Mosse and P. Mejia-Alvarez, "Determining optimal
processor speeds for periodic real-time tasks with different power characteristics,"
Proceedings of the
13th Euromicro
Conference on Real-Time Systems,
Delft,
Netherlands, June 200 l.
[4)
Y. Shin and K. Choi, "Power conscious Fixed Priority scheduling in hard real-time
systems," Proceedings ofthe Design Automation Conference (DAC 99), New Orleans,
Louisiana, ACM 1-58113-7/99/06, 1999.
[5]
C. L. Liu and l.W. Layland, "Scheduling algorithms for multiprogramming in a hard
real-time environment," Journal ofthe Association for Computing Machinery (JACM),
vol. 20, pp. 46-61, 1973.
[6)
R. Davis and A. Wellings, "Dual-priority scheduling," Proceedings IEEE Real Time
Systems Symposium, pp. 100-109, 1995.
[7]
A. Burns and AJ. Wellings, "Dual-priority Assignment: A practical method for
increasing processor utilization," Proceedings of 5th Euromicro Workshop on Real-
Time Systems, IEEE Computer Society Press, pp. 48-55, 1993.
[8]
M. Joseph and P. Pandya, "Finding response times in a real-time system," British
Computer Society Computer Journal, 29(5): 390-395, Cambridge University Press,
1986.
[9]
C. Locke, D. Vogel and T. Mesler, "Building a predictable avionics platform in Ada: a
case study," Proceedings IEEE Real-Time Systems Symposium, December 1991.
[10]
A. Burns, K. Tindell and A. Wellings, "Effective analysis for engineering real-time
fixed priority schedulers," IEEE Transactions on Software Engineering, vol. 21, pp.
475-480, May 1995.
[11]
N. Kim, M. Ryu, S. Hong, M. Saksena, C. Choi and H. Shin, "Visual assessment of a
real-time system design : a case study on a CNC controller," Proceedings IEEE Real-
Time Systems Symposium, December 1996.

Chapter 3
TOWARD THE PLACEMENT
OF POWER MANAGEMENT POINTS
IN REAL-TIME APPLICATIONS*
Nevine AbouGhazaleh, Daniel Mosse, Bruce Childers, Rami Melhem
Department ofComputer Science
University ofPittsburgh
Pittsburgh. PA 15260
{nevine,mosse,childers,melhem} @cs.pitt.edu
Abstract
Dynamically changing CPU voltage and frequency has been shown to greatly
save the processor energy. These adjustments can be done at specific power
management points (PMPs), which are not without overheads . In this work we
study the effect of different overheads on both time and energy; these can be
seen as the overhead of computing the new speed, and then the overhead of
dynamically adjusting the speed. We propose a theoretical solution for choosing
the granularity of inserting PMPs in a program taking into consideration such
overheads. We validate our theoretical results and show that the accuracy of the
theoretical model is very close to the simulations we carry out.
Keywords:
Power management, Dynamic Voltage Scaling, real-time application.
1.
Introduction
In the last decade, there has been considerable research on low-power sys-
tem design. Ongoing research has greatly influenced the design of embedded
real-time systems due to the number of applications running on power-limited
systems that have tight temporal constraints. Recently, dynamic voltage scal-
ing (DVS), which involves dynamically adjusting CPU voltage and frequency,
has become a major research area. Reducing a processor's supply voltage
"This workhas beensupportedby the DefenseAdvancedResearchProjectsAgencyundercontract F33615-
OOC-1736.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

38
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
typically leads to considerable power savings, but also introduces delays in
response time and additional energy consumption overhead for speed adjust-
ments .
Typically, the reduction in energy savings is quadratic with the slowdown
while the delays increase only linearly.
The overhead of speed adjustment
changes depending on the architecture and how much the speed changes. Thus,
there is a need to optimize energy consumption while considering both the
savings achieved and the overhead of changing processor frequency and supply
voltage.
In Mosse et al. [8] we introduced what we now call Power Management
Points (PMPs), which are pieces of code that manage information about the
execution of program segments to make decisions about changing the CPU
speed. The desired speed is computed according to a speed setting algorithm
(for examples, see [8, 6, 11, 1)). Proposed PMPs in [8] can be inserted by the
compiler in a program or executed by the operating system at specific times
(e.g., context switch times). This work focuses on compiler-inserted PMPs .
Dynamic speed setting schemes used in periodic real-time systems take ad-
vantage of unused time to slow down the CPU speed of future tasks or task
segments. This can be done when the system load is light or when there is
slack time left from previous program segments. It is shown in [8] that statisti-
cal slack management produces savings in CPU energy consumption of up to
90% compared to no power management and up to 60% compared to a static
speed setting scheme.
Compiler insertion of PMPs is particularly useful for programs with fre-
quent procedure calls or loops with a relatively large number of iterations. To
select the granularity of the program segment that is assigned a single speed,
Hsu et al. [5] uses global program analyses to detect regions of sufficiently
large granularity, and then selects a single region with the highest predicted
benefit, where they place a speed setting instruction.
Our work presented here has two objectives: (1) modeling how to incorpo-
rate the effect of overhead in speed adjustment schemes, and (2) providing a
theoretical, yet practical, solution for deciding the optimal number of equally
spaced PMPs that achieve the minimum energy consumption. We compare
our results from the theoretical solution with simulated results of our previ-
ous speed setting schemes from [8]. The theoretical results show a decision
accuracy within five PMPs of the simulation results .
The rest of this chapter is organized as follows: we start by describing our
model in the next section. The effect of different overheads is considered in
Section 3 and shown for dynamic speed setting schemes in Section 4. Section
5 presents a theoretical solution for selecting the best number of PMPs and
compares these results with simulation results for each scheme. Concluding
remarks are in Section 6.

Toward the Placement of Power Manag ement Points in Real-Time Applications
39
2.
Model
Our techniques are targeted to embedded systems where applications exe-
cute for a specified allocated time, d, decided by a scheduler. This amount
d can be obtained through a variety of scheduling disciplines, such as EDF,
RMS, and CBS (based on task utilization), as well as fair-share and other re-
source allocation techniques that guarantee a certain CPU usage for each task
(called resource protection or resource isolation). We will refer to the quantity
d as the task deadline.
In this initial work, we consider the sequential form of program execution,
where a program can be divided into n segments of equal length, to determine
an optimal number of PMPs. Such a model is applicable to loops that have
large compile-time trip counts (i.e., number of iterations of the loop). In these
loops, power management points can be placed every so many loop iterations
to adjust processor speed. For simplicity, we will use "speed" to refer to the
more accurate "frequency/voltage" pair. We are currently extending our work
to more general programs.
In our proposed scheme, we insert a PMP before each program segment. A
segment is characterized by its worst case execution time, WCi , and average ex-
ecution time avgi' The actual execution time of segment i, aCi , is only known
at run time, but is limited by the WCi such that 0 ~ aCi ~ WCi . These times de-
scribe the execution behavior of segment i when the processor is running at its
maximum speed. The quantity 0' = avgi/wci is an indication of the expected
slack in the execution time of the segment.
Given the parameters above, we can compute the static slack in the system,
which is the amount of free time in the system with respect to the computa-
tional requirements of an application. The optimal static speed, S statie, for all
segments can be computed as L~=l 7
= load. It has been proven that this
speed is optimal, while meeting all deadlines in case of a static speed schedul-
ing [l]. Henceforth, we assume that all segments are slowed down to Sstatie,
making the CPU busy at all times (albeit at a reduced speed), if a Ci = WCi , Vi.
This is equivalent to having 100% load (or load =1).
Furthermore, extra slack is generated whenever a program segment finishes
its execution before the estimated worst-case time for this segment. This can
happen depending on input data that dynamically determines the actual trip
count for each loop; this is called reclaimed slack.
For CMOS technology, dynamic power consumption is directly proportional
to the frequency and to the square of the supply voltage :
P = aCS V 2 ,
(3.1)
where a is the activity factor in the processor, C is the effective switched ca-
pacitance, S is the operating frequency (speed), and V is the supply voltage.

40
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
(3.2)
In our work, we use a model similar to Transmeta's TM 5400 processor [12].
Our model has a 16-step frequency scale that ranges from 200MHz at 1.1V to
700 MHz at 1.65V. Each step is approximately 33MHz. In the next section, we
consider the different overheads of each frequency and voltage change.
3.
Sources of Overhead
When computing and changing CPU frequency and voltage, several sources
of overhead may be encountered. The principal sources of overhead are (1)
computing the new speed using a dynamic speed setting scheme, and (2) set-
ting the speed through a voltage transition in the processor's DC-DC regulator
(resulting in a processor frequency change) and the clock generator (PLL). We
denote changing both voltage and frequency by the term speedchange. Speed
changing takes time and consume energy. Knowing that E = Pt. the energy
overhead can be derived from Equation (3.1), where t is the time overhead
spent during adjusting the speed between tasks. Below is a detailed discussion
of how t is spent.
3.1
Computing the New Speed
For each adjustment scheme considered, the time overhead of computing
the value of the new task's speed, F, is approximately constant in terms of the
number of cycles needed for execution. This new speed computation includes
the overhead of calling library functions and performing the operations that
compute the new speed. Since this may be executed at different frequencies,
the time overhead, 0 1, equals to:
F
01(Si) = Si'
where Si is the CPU speed executing segment i (including the PMP code at the
end of segment i). From experiments with SimpleScalar 3.0 [3] (a micro archi-
tectural simulator), where we implemented speed setting and inserted PMPs in
applications like an MPEG decoder, we observed that the overhead of com-
puting the new speed varied between 280 and 320 cycles. In the experiments
below we fix the overhead of computing the new speed to 300 cycles .
3.2
Setting the New Speed
To change voltage, a DC-DC switching regulator is employed. This regu-
lator cannot make an instantaneous transition from one voltage to another [4].
This transition takes time and energy. When setting a new speed, the CPU
clock and the voltage fed to the CPU need to be changed, incurring a wide
range of delays. For example, the StrongArm SA-I 100 is capable of on-the-fly
clock frequency changes in the range of 59MHz to 206MHz where each speed

Toward the Placement of Power Management Points in Real-Time Applications
41
and voltage change incurs a latency of up to 150 usee (7], while the lpARM
processor (9] (a low-power implementation of the ARM8 architecture) takes
25 I-1s for a full swing from 10 MHz to 100 MHz. Another example is the
Transmeta TM5400, which is specifically designed for DVS (12]. Some sys-
tems can continue operation while speed and voltage change (9, 4], but the
frequency continues to vary during the transition period. Some systems stop
during steep changes. We take a conservative approach and assume that the
processor can not execute application code during this period.
Moreover, when looking at changing speed from the energy perspective,
the IpARM processor incurs at most 41-1J, which is equivalent to 712 full-load
cycles for the transition between 5 - 80 MHz(2]. In our simulation we assume
a constant number of overhead cycles, G, for each speed step transition. This
overhead is assumed to be 320 cycles for every 33MHz step (from (2], 712
cycles for 5-80 MHz transition ~ 320 cycles for a 33MHz transition). The
time overhead for speed changes, O2, depends on the speed that the CPU is
executing the PMP and can be computed as follows :
(3.3)
where d(Si, Sj) is a function that returns the number of speed steps needed to
make a transition between S, and Sj. In the Transmeta model, this function
returns how many multiples of 33MHz is the difference between S, and Sj .
The energy overhead is assumed to follow the same power function presented
in Section 2 multiplied by the time taken to accomplish the speed transition.
We study the impact of varying this overhead on the selection of the optimal
number of PMPs in Section 5. Next we show two different schemes for the
CPU dynamic speed adjustment.
4.
Speed Adjustment Schemes
We use two schemes from (8], namely the Proportional and the Greedy
schemes, as examples to demonstrate how to include the aforementioned over-
head in speed adjustments at each PMP. Deadlines are only violated in cases
where the processor needs to run at almost the maximum speed to meet the
application's deadline and there is not enough slack to accommodate the time
overhead for a single speed computation. We regard this case as insignificant
for the purpose of this study. Figure 3.1 shows an example of the actual exe-
cution of a task set using the Static, Proportional, and Greedy schemes.
4.1
Proportional Dynamic Power Management
In the proportional scheme, reclaimed slack is uniformly distributed to all
remaining segments proportional to their worst-case execution times. The time

d
: Sialic scheme
reclaimed slack
P2
P2
Time
T1
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
PI
PI
PO
t=~==ID===~===:I~§imiWl!liliiilL__l._P_ro_
P
O_r1iOnal Scheme
: Greedy scheme
42
s"Llti<:
PO
.""~
til
PO
Figure 3.1.
Actual executionof a task set usingthe Static,Proportional and Dynamic Greedy
schemes.
overheads for both computing and setting the new speed are subtracted from
the time remaining to the deadline. Our main concern here is to compute the
exact time left for the application to execute before the deadline. The proces-
sor's speed for segment i, Si, is computed as follows : the execution times for
the remaining tasks are stretched out based on the remaining time to the dead-
line (d - ti-1), while taking into consideration the overhead of switching the
speed of the current task (Ocur(Si-1 ,Si) = 0 1(Si-1) +02(Si-1, Si» and the
overhead of potentially needing to switch the speed of the next task to the static
optimal speed (Onext(Si,Sstatic) = OI(Sd + 02(Si, Sstatic», where Sstatic
is the optimal static speed [1]. The total overhead, Ototal, is:
With overhead consideration, this scheme computes a new speed as:
EJ=iWCj
s. = d
0
(S
S )Sstatic,
- ti-1 -
total
i-I ,
i
(3.5)
where ti-1 is time at the beginning of the i th PMP (the end of segment i-I).
Given that the voltage setting overhead is dependent on the new frequency,
we note that S, appears on both sides of the formula. It is solved iteratively
and we have observed that, on average, it converges in about two iterations.
Because the algorithm starts with the speed for the previous segment, there is
typically minimal change in speed, which leads to fast convergence.
4.2
Dynamic Greedy Power Management
The Dynamic Greedy scheme distributes the reclaimed slack only to the seg-
ment immediately after a PMP. The way to compute the speed of the next seg-
ment is similar to Equation (3.5): the time for the next task is spread over the
remaining time to the deadline, minus the overhead of computing and switch-

Toward the Placement ofPower Management Points in Real-Time Applications
43
ing speeds from Equation (3.4).
WCi
s, =
", n
S static'
d
t
L..o j= i+ l WC j
0
(8
8 )
-
i-I -
Sstatic
-
total
i-I ,
i
(3.6)
4.3
Evaluation of PowerManagement Schemes
We simulated our two power management schemes with and without over-
head for a hypothetical program divided into different number of equal length
segments. The actual execution times aCi of each segment are drawn randomly
from a normal distribution. Each data point presented is an average of 500
runs. We use the energy function from Section 2, and the energy consumption
shown in the graphs below is normalized to a case where no power manage-
ment is employed.
In Figure (3.2) we show the energy consumption based on the number of
PMPs for the Proportional and the Dynamic Greedy schemes.
The optimal
number of PMPs varies according to many factors, like 0' or the speed adjust-
ment scheme. We show results for 0' =0.6 and 0.8 and a variable number of
PMPs (from 5 to 30). From the figure, the optimal number of PMPs, based on
simulation, is between 10 and 20 for 0'=0.6 and between between 5 and 15 for
0' =0.8. Results for other values of 0' were consistent with these results.
100
Proportional/0 .8
98
Dynamic Greedy/0.8
c:
Proportionall0.6
0
96
Dynamic Greedy/0 .6
'';:::a.
E
94
---- ---
::l
(flc:
92
0o
>-
90
e>
Q)
~
~ .. ~ -.-
c:
88
w
:.!1
0
86
84
5
10
15
20
25
30
no. of PMPs
Figure 3.2.
Total energy consumption for different schemes versus the number of PMPs at Q
=0.6 and Q =0.8.
We noticed that for higher number of PMPs inserted in a program, the av-
erage number of step transitions needed at each PMP for the greedy scheme

44
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
exceeds those needed for the proportional scheme. This adds a greater energy
burden on the Greedy's total energy consumption more than the Proportional's
consumption. As a result, although in general, the greedy energy consumption
is less than the Proportional's at load =1, in Figure 3.2 there is an overlapping
of their energy curves for 0: = 0.8. There is a smaller difference in energy as
the number of PMPs increases for 0: = 0.6.
The bathtub-like curve shapes are due to two opposing forces. First, the
more management points, the better the power management and thus the lower
energy consumption. However, the amount of energy consumed increases with
an increasing number of PMPs, due to the overhead. The combination of these
two factors is illustrated in Figure 3.2.
We also see that Greedy has lower energy consumption than Proportional.
This is because Greedy is more aggressive at slowing down the CPU speed,
counting on future reclaiming for slowing down future tasks. We note that this
is the case when there is a minimum speed and the loads are moderate to high.
When the load is very low, or very high, all schemes perform approximately
the same, executing all tasks at maximum or minimum speed, respectively.
Further, when there is no minimum speed (as in [8]), Greedy will slow down
the first tasks to almost zero and consume almost all the slack; consequently,
later tasks will execute at high speeds, causing Greedy to consume more overall
energy than proportional.'
Next we present a theoretical solution for selecting the number of PMPs to
insert in a program.
5.
Optimal Number of PMPs
The minimum energy consumption for any of the presented schemes de-
pends on the number of placed PMPs due to the effect of the energy overhead
imposed by every added point. In this section, we develop a theoretical frame-
work for deciding on the optimal number of PMPs, given that: (1) the program
under consideration is divided into equally-sized segments, (2) each segment
has perfect execution behavior (i.e, aCi = avgi for all 1 ~ i ~ n), (3) there is
a constant time overhead, h ~ 0 1 + 02, for the insertion of any PMP, and (4)
the speed range is continuous.
The total energy consumed for n segments is computed based on Equa-
tion (3.1) as E = "£ Pt, where t is the time taken to run all n segments. Part
of each segment's energy is consumed in the actual execution of the segment,
avgi, while the other part is consumed in the overhead induced by changing
the speed. The total energy, En, is the summation of the segments' energies as
shown below.

Toward the Placement ofPower Management Points in Real-Time Applications
45
where the speed S, is proportional to voltage Vi, and 'Y is a multiplicative factor.
The energy overhead is reflected in this equation by the term hiSi, where it
represents the average time taken for each speed change at each PMP.
In our analytical solution, we compute the speed S, using the following for-
mulas. We use these speed values to evaluate the energy consumption of the
actual execution of the segment.
The formulas are derived from the corre-
sponding ones presented in Section 4 using our earlier assumptions.
Proportional
Ssisu«
S i
(3.8)
where n is the number of placed PMPs and 0' is the slack expectation; 0' = ~.
Dynamic Greedy
(3.9)
1 - (1 - O')i
SStatic =
s,
where 0' is defined as above.
The complete derivation of these formulas is shown in the appendix of this
chapter.
5.1
Evaluation of the Analytical Model
The optimal number of PMPs varies based on several parameters in the pro-
gram execution behavior, such as variability of the execution 0' or load 0', and
the amount of overhead for changing the speed. Figures 3.3 and 3.4 show the
effect of varying the number of overhead cycles, h, on the total energy con-
sumed using the analytic model. The results shown are for 0' =0.6, although
other values of 0' have similar behavior. The optimal number of PMPs in the
Proportional scheme lies in the range of 5-15, while for Dynamic Greedy it
is from 10 to 30. As predicted, this optimal number decreases with increased
overhead. However, this does not apply when 0' = 1, because as 0' reaches 1
the desired optimal speed reaches S static, with no CPU time to reclaim. Hence-
forth , we exclude the 0' = 1 case from our experiments.
We ran experiments to validate our theoretical model by comparing the
results with simulation results for the Proportional and the Dynamic Greedy
schemes.
Table 3.1 shows the number of PMPs determined by the theoretical model
and the simulation for the Proportional scheme. The table shows the results for
the same programs with different 0' and overhead values that might describe
different DVS processors. The overhead values are presented as a pair of the
theoretical overhead h and its corresponding simulated overhead F and G. For
example, (1000/ 300,320) means h is 1000 cycles, while F and G are equal
to 300 and 320 cycles, respectively. We use these figures for the overheads of

46
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
" -- ..._-----
66
c
64
o
''5. 62
E
5l 60
coo
58
>-
~
Ql 56
c
w 54
eft.
52
50
5
h=3000
h=2400
h=1800
h=1200
h= 600
10
..'
'- - '-'-'- - ---
15
no. of PMPs
.-.--
20
25
Figure3.3.
Total energy consumption for the Proportional scheme versus the number of PMPs,
for different overheads, where 0: = 0.6.
-.-.--.-.- - .-.-._.-._.
50
c
45
0
''5.
E
:J
III
C
40
0o
>-
Ol
....
Ql 35
cw
;,!!
0
30
h=3000
h=2400
h=1800
h=1200
\\ = 600
\ . \
.~.:~~~:\
\......"
\«~~>.>------------
"
....
--------------
.'.>:
.............._-
.
", -"
----
5
10
15
no, of PMPs
20
25
Figure 3.4.
Total energy consumption for Dynamic Greedy scheme versus the number of
PMPs, for different overheads, where 0: = 0.6.

Toward the Placement ofPower Management Points in Real-Time Applications
47
the two schemes (theoretical and simulated) because, from experiments with
SimpleScalar, we observed that, in the Proportional scheme, the average num-
ber of transitions for the whole programs is 2.2 (in the 300/1000 example,
300 + 2.2 * 320 ~ 1000). The table also shows variations of ±2 PMPs be-
tween the theoretical and the simulation results. There is a strong matching
in the o:'s middle range, which are the most typical values of 0:. The varia-
tions come from the assumption that the speed is continuous in the theoretical
method while it is discrete in the simulation . Moreover, as mentioned above,
the simulation in this chapter is limited by a minimum speed.
Table 3.1.
Theoretical (T) versus Simulation (S) choice of optimal number of PMPs for the
Proportional scheme.
(1000/ 300,320)
(2000/600,640)
(3000/ 900,960)
Q
T
S
T
S
T
S
0.2
10
12
7
7
6
7
0.4
12
12
9
9
7
6
0.6
12
12
9
9
7
6
0.4
12
12
9
9
7
6
0.8
II
9
8
6
7
5
During simulations, we noticed that, on average, Dynamic Greedy performs
three times more step transitions than the Proportional scheme. Thus the choice
of h = 3000 that corresponds to simulation overhead of F = 300 and G = 320.
Table 3.2 shows that the optimal number of PMPs varies dramatically with 0:.
For example, this variation at overhead (30001 300, 320) ranges from 9 to 29
PMPs, corresponding to o 's range 0.2-0.8. This higher number of PMPs is in
concert with the higher number of speed changes that are made by Greedy.
Table 3.2.
Theoretical (T) versus Simulation (S) choice of optimal number of PMPs for the
Dynamic Greedy scheme.
(3000/ 300,320)
(6000/600,640)
(9000/ 900,960)
Q
T
S
T
S
T
S
0.2
29
25
20
15
16
II
0.4
22
19
14
12
II
9
0.6
14
12
10
9
8
6
0.8
9
9
7
6
5
4
We observed that the theoretical results are closer to the simulated results
as the F and G overheads decrease.
The difference between the simulated
and theoretical results can be seen by comparing Figure 3.2 with Figures 3.3
and 3.4. The difference in results is because the analytical model does not take

48
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
overheads into account when computing the new speeds, only when computing
the energy. Further research is needed to obtain a more tight coupling between
the values of the theoretical and the simulated overheads.
We also saw that, although there is a difference between the number of
PMPs in the two schemes, the difference in energy between the schemes is
always less than 1%. This is because the energy consumption around the opti-
mal number of PMPs is relatively fiat (constant) and therefore a small mistake
in the number of PMPs does not affect the energy consumption significantly.
6.
Conclusion
For variable voltage systems, the overhead and selection of a speed setting
scheme must be carefully considered. There may be cases where the energy
consumption exerted by the overhead of selecting and setting a new speed over-
whelms any energy savings of a speed setting algorithm. This implies that
system energy can be jeopardized by employing such speed adjustments.
To minimize the overhead of speed adjustments, it is critical that for pro-
grams with a relatively small number of segments to know the optimal number
of adjustment points for choosing the best speed adjustment scheme. How-
ever, for programs with a large number of segments, it is sufficient to identify
the boundary of optimality, as the energy curve will become relatively flat-
ter beyond the optimal number of power management points . Clearly, "short"
and "long" programs depends on several factors, such as number of speeds of-
fered by the CPU, overhead of speed adjustments, and minimum and maximum
speeds.
We also saw that Greedy has smaller energy consumption than Proportional,
especially for workloads with higher variability in the actual execution time,
because Greedy is a more aggressive scheme that implicitly takes advantage of
future reclaimed time.
Appendix: Derivation of Formulas
In this Appendix we present the derivation of the formulas for the theoretical
solution.
Proportional Dynamic Scheme Formula
Lemma 1:
i-I
cPi = Ai(B +L cPd => cPi = AiBIIt:~(1 + AI)
1=1
(3.A.1)
Proof by induction:
Base case: at i = 1, it is trivial to see that the left hand side (LHS) of

(3.A.2)
=
Toward the Placement of Power Management Points in Real-Time Applications
49
Equation (3.A. l) is the same as the RHS:
LHS = AlB = RHS
Induction step: Let Equation (3.A.l) hold for all n < i. We prove that it
also holds for n = i. By substituting the RHS of Equation (3.A.l) for cPt. it is
sufficient to prove that:
i - I
cPi = Ai(B +L AI Brr~:\ (1+ Ak)) = Ai B rrl: ~(1 + AI)
1=1
Proof:
cPi
=
Ai(B + L l:~ AlBrri: \ (1+ Ak))
=
AiB(l + Ll:~ Alrr~:\ (1+ Ak))
=
Ai B(1+ A l + Li:~ Alrr~-==\ (1+ Ak ) )
AiB ((1+ Ad + L i: ~ A I (1 + AI )rr~-==~(1 + A k ) )
=
A i B(( 1 + A d + A2(l + Ad I:i:j A/rrt:}2(1+ A k ) )
=
A i B ((1 + Ad(l + A22:/:5A/ rr~:}2 ( 1 + A k ) ) )
=
=
A iB ((1 + Ad(1+ A2).....(1+ Ai-d )
=
A BIT1:~( 1 + AI)
=
RHS
End of Lemma 1.
We start from the formula for speed adjustment for the proportional scheme
presented in [8].
S
L~ i wq
S
i = " n
" n
(S
/S ) static,
LJI=l WCI - LJI=l aCI
static
I
where n is the number of segments. S, is the speed of segment i, and Sstatic
is the static speed. Let cPi = Sstatic/ Si. Recall that our assumption for the
theoretical model asserts that Vi, WCi = wc and Vi, aCi = auq, = avg. Now,
let Q = acf uic. Then,
(n-H 1)wc
S
'"
I
static
n wc- avgLJI=l (Sstatic/Sd
n-i+1
S
'"
I A.
static
n-oLJI=\ '1'1
n-oL;=l <PI
n-i+1
1
(
"
i - 1 ,J, )
n-i+1 n -
Q LJI=l 'PI
-
0
( -n
" i- I A. )
n-Hl a + LJI=I lf'1
By substituting in Lemma 1 with Ai =
n -=-i~l and B =
-on we get:
¢ =
n
rri - 1 [1 -
Q
J
t
n - i + 1
k=1
n - k + 1 .

50
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Greedy Dynamic Scheme Formula
Lemma 2:
(3.AJ)
Proof byinduction:
Base case: at i =
1, it is trivial to see that the left hand side (LHS) of
Equation (3.A.3) is the same as the RHS:
LHS= rPl=l=RHS
Induction step: Let Equation (3.A.3) hold for all n < i. We prove that it
also holds for n = i. By substituting the RHS of Equation (3.A.3) for cPt. it is
sufficient to prove that:
.
i- l (1 + C)I _ 1
(1 + C)i _ 1
rPi = z+ C L
= -'----,::'---
1=1
C
C
Proof:
A..
l
"
=
.+ C ",i-l (C+1 )I_l
'I'
Z
L.JI= l
C
=
i + 2:::1:U(C+ 1)1- 1)
=
i - (i - 1) + I::: ~ (C + 1)1
=
1+ 2:::::~ (C + 1)1
=
1 + (C + l)i-l - 1
" C
(C+ 1)' - 1
C
=
RHS
End of Lemma 2.
We start from the formul a for speed adjustment for the dynamic greedy
scheme presented in [8].
S"=
WCi
S
I
",n
",i - 1
S
"
",n
static ,
(3.AA)
L.JI= l WCI -
L.JI=l aC[~ -
L.JI=i+l WCI
where n is the number of segments, S, is the speed of segment i. and S static
is the static speed. Let rPi =
S static!Si . Recall that our assumption for the
theoretical model asserts that Vi,WCi = wc and Vi, aCi = a uq; = avg. Now,
let a = acf uic. Then,
,
1 wc
.
S static
n wc - avg 2:::1=1(S static/S I) - (n-I)Wc
.
~C 1
S static
Iwc - avg 2:::1=1(S staticl Sz)
rPi =
i r-- o: 2:::t:~ rPl

Toward the Placement ofPower Management Points in Real-Time Applications
51
By substituting in Lemma 2 with C =
- 0: we get:
(3.A.5)
Notes
I. We can call this the "rookie" effect: due to lack of experience. when faced with too muchperceived
slack, rookies willdo very little work in the beginning and speed tasks up later on.
References
[1] H. Aydin, R. Melhem, D. Mosse, and P. M. Alvarez. Dynamic and Ag-
gressive Power-Aware Scheduling Techniques for Hard Real-Time Sys-
tems. In Proceedings of the 2001 IEEE Real- Time Systems Symposium
(RTSS 'OI), December 2001.
[2] T. Burd, T. Pering, A. Stratakos, R. Brodersen. Dynamic Voltage Scaled
Microprocessor System. International Solid-State Circuits Conferen ce,
ISSCC 2000, pp.294-295, 2000 .
[3] D. Burger, and T. Austin . The SimpleScalar Tool Set, Ver 2.0 . University
of Wisconsin-Madison CS Technical report no. 1342, 1997.
[4] I. Hong, G. Qu, M. Potkonjak and M. Srivastava. Synthesis Techniques
for Low-Power Hard Real-Time Systems on Variable Voltage Processors.
In Proceedings of 19th IEEE Real-Tim e Systems Symposium (RTSS'98),
Madrid, Dec 1998.
[5] Chung-Hsing Hsu, Ulrich Kremer, and Michael Hsiac.
Compiler-
Directed Dynamic VoltagelFrequency Scheduling for Energy Reduction
in Microprocessors. International Symposium on Low Power Electronics
and Design, August 2001.
[6] C. M. Krishna and Y. H. Lee. Voltage Clock Scaling Adaptive Schedul-
ing Techniques for Low Power in Hard Real-Time Systems. In Proceed-
ings ofthe 6th IEEE Real-Time Technology and Applications Symposium
(RTAS 'OO), Washington D.C., May 2000.
[7] R. Min, T. Furrer, and A. Chandrakasan. Dynamic Voltage Scaling Tech-
niques for Distributed Microsensor Networks.
IEEE VLSI Workshop,
2000 .
[8] Daniel Mosse, Hakan
Aydin,
Bruce
Childers and Rami Melhem.
Compiler-Assisted Dynamic power-Aware Scheduling for Real-Time
Applications. Workshop on Compilers and Operating Systems fo r Low
Power, COLPOO. 2000.

52
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
[9] T. Pering, T. Burd, and R. Brodersen. Voltage Scheduling in the IpARM
Microprocessor System. International Symposium on Low Power Elec-
tronics and Design 2000, pp.96-101, 2000.
[10] J. Pouwelse, K. Langendoen and H. Sips. Dynamic Voltage Scaling on
a Low-Power Microprocessor. In Mobile Computing Conference (MOB/-
COM),2001.
[11] Y. Shin and K. Choi. Power Conscious Fixed Priority Scheduling for
Hard Real-Time Systems. In Design Automation Conference. DAC'99,
pp. 134-139.
[12] Transmeta
Corporation,
Crusoe
Processor
Specification,
http://www.transmeta.com

Chapter 4
ENERGY CHARACTERIZATION OF EMBEDDED
REAL-TIME OPERATING SYSTEMS
Andrea Acquaviva
Luca Benini
Bruno Ricco
University ofBologna. Italy
{andrea,lbenini,bruno}@unibo.it
Abstract
In this chapter we present a methodology to analyze the energy overhead due to
the presence of an embedded operating system in a wearable device. Our objec-
tive is to determine the key parameters affecting the energy consumption of the
RTOSallowingthe developmentof more efficientOS-based powermanagement
policies. Toachievethis target. we present a characterizationstrategy that stimu-
lates the RTOS both at the kernel and at the l/O driver levelby analyzing various
OS-related parameters. Our analysis focus in particular on the relationship be-
tween energy consumption and processor frequency characterizing the different
functionalities of an RTOS, suggesting a way to develop effective OS-aware
energy optimization policies based on variable voltage and frequency. Experi-
mental results are presented for eCos. an open-source embedded as ported and
installed on a prototype of wearable device, the HP Smartlsadgelv.!
1.
Introduction
Power consumption has always been a primary design constraint for most
wearable devices. At the same time, these systems feature an ever increasing
software and hardware complexity: a large number of heterogeneous applica-
tions must be supported while matching battery capacity. To handle complex-
ITheauthors wish tothank theVisual ComputingDepartmentof Hewlett-Packard Laboratories for support-
ingthis research.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

54
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
ity while ensuring modularity and fast time to market, designers are moving
toward processor-based systems-on-chip (SoC) architectures instead of dedi-
cated hardware like ASICs and DSPs. This class of architectures is well-suited
to be managed by an embedded operating system, which introduces a software
layer between applications and the underlaying hardware.
Clearly, in a power-constrained environment, a characterization of the en-
ergy impact of the RTOS is required, and the OS impact on power must be
taken into consideration while designing the system. Traditionally RTOSes
are characterized only for performance, even though they energy overhead can
strongly affect the effectiveness of power-aware design and power manage-
ment strategies.
Energy overhead is a direct consequence of operating system structure that
is not exactly tailored for a specific application. This is true even if recent em-
bedded OSes have an increasing degree of modularity and flexibility to address
issues of size, power, cost and performance. Modularity can be achieved by
allowing a fine grained configuration and extension of software components.
However, there is a minimum amount of functionalities that must be used by
the applications (namely intrinsic functionalities) . For example, in order to
access peripherals, applications must interface with the higher level of abstrac-
tion provided by device drivers instead of directly dealing with hardware. An-
other example is the thread management support which provides multi-tasking
environment.
Since operating system overhead affects in different ways the system en-
ergy consumption, it is important to apply a methodology suited to outline
those characteristics of RTOS that affect the energy profile of the system. OS
affects the energy profile of the system by two different mechanisms. One is
the overhead due to the execution of OS code (system overhead>, the other is
the way the OS provides requested functionalities to the applications (resource
management). Context switch time is an example of system overhead, while
I/O buffer management belongs to the second contribution, since it is respon-
sibility of the OS to provide the interaction between the applications and the
external environment.
As a case study, we considered an eCos open-source Real Time Operat-
ing System from Red Hat, running on a hand-held computer, the HP's Smart-
BadgeIV. As a result, in the following sections we will show a detailed energy
analysis of the OS impact on the power consumption of a wearable computer.
In particular, we will analyse key factors, like I/O data burstiness and thread
switch frequency, that influence the energy overhead of operating system ser-
vices and drivers.
Another important motivation for this work arises from recent developments
in variable-frequency, variable-voltage processors and the related power man-
agement problems (e.g. voltage scheduling, frequency setting). These tech-

Energy Characterization of Embedded RTOS
55
niques require modifications of basic RTOS schedulers to account for the pos-
sibility of adjusting the voltage and speed level of the processor at run-time
depending on the workload. As a consequence, clock throttling becomes a
tunable parameter affecting that we above called resource management. For
this reason , it is important to know how the power and performance of RTOS
services and drivers change as a function of the CPU clock frequency. As an
example, consider an audio driver sending samples toward the output channel.
Since the output rate is imposed by the sample frequency, changing the pro-
cessor speed changes the number of busy waiting intervals experienced by the
driver thread 2.
We performed energy characterization at different processor speeds by ex-
ploiting the frequency-setting capabilities of the StrongARM 1110, the proces-
sor core of the SmartBadge. In particular, we will show the energy characteris-
tics of a typical application running on wearable computer which is an adaptive
audio noise canceler (LMS filter). This application stresses both drivers and
CPU, since it is composed by three phases: input from the serial port, sam-
ples elaboration, output of processed samples toward the audio output. Since
the LMS filter runs as a single application, we are also allowed to perform a
comparison between the as supported and the stand-alone version.
The remainder of the chapter is organized as follows . We survey related
work in Section 2. An overview of the system , both hardware and software, is
provided in Section 3. In Section 4 we describe in detail how to characterize
a real-time operating system from the energy viewpoint, while in section 5
we provide experimental results for the characterization framework. Section 6
summarizes our findings and Section 7 concludes the chapter.
2.
Related Work
The problem of characterizing the energy profile of a real-time embedded
operating system arises from the increasing complexity of the software archi-
tecture of modem wearable embedded systems. Moreover, operating system
energy-behavior plays an important role for power optimization strategies. In
the past, indeed, some researchers investigated the possibility of a cooperation
between applications and as in order to achieve an energy efficient tuning of
the system resources [13][14][7].
Other authors have investigated various opportunities to improve the en-
ergy efficiency of an embedded operating system.
For example Vahdat et
al. [21] propose potential energy-improvements for each functionality, like
inter-process communication, memory allocation, CPU scheduling. Lebeck
2A we will explain later, in real-time systems device drivers are implemented as threads since they can be
preempted by other task in the system

56
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
et aI. [II] proposed a memory paging technique that aims at putting as many
memory components as possible in power-down mode. Lorch and Smith [12]
suggested heuristic techniques to put the processor in low-power states when
identifying idle conditions. Benini et aI. [5] designed a workload monitoring
tool for supporting effective dynamic power management policies.
In addition, a considerable amount of work has been done in the area of
energy efficient scheduling for real time operating systems [8][9][10][15][16]
[18][20][3]. In spite of this, the need of an energy characterization of an RTOS
is a relatively new concern. Indeed, researchers in the past have focused mainly
on the performance of RTOSes [19][17] . The first attempt to assess the en-
ergy overhead of an embedded OS is reported in [6]. This work analyses the
power profile of a commercial RTOS by running two applications and evalu-
ating the power consumed by the operating system calls. Power analysis are
carried out on a instruction-set simulator of the Fujitsu SPARClite processor
with instruction-level power models . In this work, the authors show by means
of two examples that power can be saved by optimizing how the RTOS services
are used by the applications.
3.
System Overview
In this section we describe the target system for our experiments. The hard-
ware is the HP SmartBadgeIV wearable computer, from Hewlett-Packard Lab-
oratories, while the OS is eCos, a real time embedded operating system from
Red Hat that we ported to the target platform.
3.1
The Hardware Platform
The energy analysis we present in this chapter is related to embedded sys-
tems and in particular to wearable devices.
These systems are often com-
posed as a System-on-Chip, containing a processor core, and external chips
like power supply regulators, sensors, audio and video CODEC. The Smart-
badgeIV, our case study, is represented in Figure 4.1.
It is equipped with
the StrongARM 1110 processor [2], which integrates in the same chip an
ARM core, memory management unit, data and instruction cache, interrupt
and DMA controller, and many I/O controllers like UART, audio and LCD. In
order to provide more 110capabilities, the board has a companion chip Stron-
gARM l l l l which communicates to the main processor by a dedicated off-
chip bus. The SA-III I provides two PCMCIA interfaces, an audio control1er
for the on-board Philips UDA341 audio chip, USB and PS/2 controllers. The
110devices we used for our experiments are a PCMCIA wireless LAN card and
the audio chip, both driven by the SA-Ill I. The system memory is organized
in three modules: 8MBytes of FLASH, 2MBytes of SRAM and 64MBytes of
SDRAM.

Energy Characterization ofEmbedded RTOS
57
Figure4.J.
The hardware platform : HP SmartBadgeIV
3.2
RTOS overview
The operating system that we analyze in our work is eCos, an open source,
real-time configurable operating system, targeted to deeply embedded applica-
tions. This OS is highly modular, and it allows easy adaptation and installation
on different kinds of embedded platforms while meeting memory space re-
quirements. Indeed it has a small memory footprint, in a range from 10 to
lOOKBytes (depending on the configuration). In addition, eCos is compatible
with Linux through the ELlIX software layer, that is a set of common system
calls. eCos can be considered a real time operating system because it can be
configured to provide structures able to managing alarms, timers and counters.
eCos system architecture is shown in Figure 4.2 where we evidenced the parts
we want to characterize. The overall structure consists of three components.
The first is the hardware abstraction layer (HAL), which defines architecture
and platform specific modules. The second component is the kernel, that is im-
plemented in an architecture-independent way and consists of a scheduler and
support for thread synchronization, exception handling, interrupt handling and
timers. An important role of the kernel is indeed to provide a basic mechanism
for supporting process synchronization. This is done in eCos with semaphores,
mutexes, condition variables and flags, which address also the problem of mu-
tual exclusion. However, semaphore are prone to priority inversion problems,
that is where a high priority thread is prevented from continuing by one of a
lower priority. As a consequence, in order to achieve predictability, the kernel
must provides special types of semaphores that support resource access pro-
tocollike Priority Inheritance, Priority Ceiling or Stack Resource Policy. The
current eCos release provides a relatively simple implementation of mutex pri-
ority inheritance. The kernel provides also for process communication. This is
done in eCos by message boxes.

58
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
Time management in eCos is performed by counters, clocks and alarms.
Counters are objects provided by the kernel that represent an abstraction of the
system clock . Alarms can be associated to counters, and alarms are identified
by the number of ticks until it triggers, the action performed on triggering and
the alarm repetition. Clocks are counters which are associated with a number
of ticks that represent time periods. The most important clock is the real-time
clock which is needed to implement time slicing for the preemptive scheduler.
The third component of eCos architecture is represented by device drivers .
The general mechanism for accessing a particular device is via a handle and
a name. Basic functions are provided to send and receive data from the de-
vice, as well as manipulate the state of the driver. eCos drivers are composed
by a high-level interface which is protocol dependent and a low-level architec-
ture/platfonn dependent structure.
Application Layer
'"
API
....
Cl.l>
'C
Q
Kernel
Cl.lo'>
Cl.l
Q
Hardware Abstraction Layer
Hardware
Figure 4.2.
The software layer: eCos structure
In order to port this RTOS on the SmartBadgeIV, we modified the HAL
structure to adapt it to the memory architecture and to support the processor
idle mode. In fact, the basic eCos version performs a busy waiting during idle
periods, which is very power expensive. On the contrary, in our implementa-
tion, whenever the processor reaches the idle thread it goes in idle state. While
in this state, the processor core does not consume switching power since its
clock is disabled.
Also the driver layer has been modified to interface with the on-board pe-
ripherals. At least we need a serial interface in the early phase of the porting
work in order to build a ROM monitor which acts as boot-loader and supports
remote debugging. Moreover, the basic driver layer has been enhanced by the
necessary wireless LAN PCMCIA and audio drivers .

Energy Characterization ofEmbedded RTOS
59
4.
Characterization Strategy
The characterization framework can be divided in three phases. In the first
we analyze kernel services, like thread management and synchronization. In
the second we evaluate the energy efficiency of the 1/0 drivers, while in the
last phase we compare the energy consumption obtained when running an ap-
plication under the RTOS with the stand-alone version.
An RTOS is useful in an embedded real-time device for several reasons. One
of these is that it creates a multi-threading environment and it provides time
management functions like alarms and counters. These features are available
through calls to kernel functions. In very complex systems with many real-time
applications running simultaneously, the energy overhead imposed by these
calls may become sizable.
We faced the problem of characterizing the energy cost of kernel services
by evaluating the energy spent by the system call as it is, independently from
the type of workload imposed by the application running. In order to carry out
this measurement. we evaluated the energy consumed by each single system
call.
In the first phase also we analyze how the energy cost of these calls is af-
fected by tunable parameters. Since usually the OS is managing multiple pro-
cesses we are interested in analyzing the overhead that arises when it switches
between threads as a function of the switching frequency. For this reason we
run two CPU-intensive threads (matrix multiplication), which maximize con-
tention for CPU cycles and do not give any opportunity for context switching
on 10-blocked processes. We compare the energy spent by running the threads
in a serialized way to that spent when the two threads alternate on the CPU.
This evaluation is made at different switching frequencies and at various clock
speeds. Since we want to isolate the energy overhead due to context switching.
we impose very small matrix dimensions, so that the cache contains both the
threads worksets. Indeed, if this is not true. there is an additional energy cost
due to cache-misses, which is an architecture-dependent effect.
Another important aspect of real-time operating system in an embedded
context is I/O support, allowing application designers to interface with periph-
erals at an high abstraction level disregarding hardware details. The drawback
is the complexity of the additional software that may lead to additional energy
costs. This motivates the second phase of our methodology, which consist in
setting up a number of benchmarks targeted to stimulate the device drivers
and to find out the main factors affecting the energy consumption. Since a
possible optimization framework may act on I/O buffer dimensions and pro-
cessor frequency. we measure the energy variations due to different levels of
data burstiness and clock speeds. In addition, we have examined the case of

60
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
the device contention. As a tunable parameter here we consider the frequency
switching between two competitors.
In the third phase of the characterization framework we run an application
that stresses the I/O drivers and we measure energy consumption. Then we
run a different version of the same application, built for running in a stand-
alone way, without any RTOS support. This experiment allow us to evaluate
the overall RTOS overhead. The results of our characterization are shown and
commented in next section.
5.
RTOS Characterization Results
Our experimental set-up consists of a hardware and a software component.
In order to perform a more detailed analysis, we measured both CPU and sys-
tem power. CPU power is observed by extracting the current to the StrongARM
1110 thanks to a special on board connector. System power is obtained by mea-
suring the current absorbed by the SmartBadge. This current flows through a
IIV conversion board that provides voltage values proportional to the absorbed
current to a data acquisition board (DAQ). The DAQ communicates to a PC
where a LABVIEW program controls the measurement framework. To obtain
energy consumption values we need to measure both the current and the execu-
tion time of the programs. For this reason we used an accurate software trigger.
Indeed the DAQ board allows an external signal to start and stop the measure-
ment. We provide this signal by driving a general-purpose input/output (GPIO)
pin of the StrongARM 1110, which can be programmed by writing a control
word on a memory-mapped CPU register. We verified on the DAQ specifi-
cation that the delay introduced by the DAQ board on the trigger signal with
respect to the analog inputs is 50ns, a value that is negligible in our context.
Only one instruction is needed to start and stop the measurement. The LAB-
VIEW software is responsible to combine power and time informations to give
energy values..
5.1
Kernel Services
The relative average switching overhead is shown in Table 4.1 for a fixed
clock frequency value (l03.2MHz).
The percentage values reported in this
table indicates how the energy cost increases as the switching frequency in-
creases. We measured the energy consumption needed to perform 2 millions
matrix multiplications where the matrix dimensions are 210 x 10. The reason
for the small matrix size, as explained in the previous section, is to minimize
the impact of cache conflicts between the two processes. The results show that
increasing the context switching frequency from zero (no switching) to 10KHz
does not affect the energy consumption in a significant way. Repeating the ex-
periment at different processor clock frequencies leads similar results, with the

Energy Characterization ofEmbedded RTOS
61
notable exception of the minimum available processor frequency (59MHz). At
this frequency the fastest context switching cannot be supported, and the sys-
tem malfunctions.
From this experiment we conclude that context switch is very efficient from
an energy viewpoint.
However, it must be considered that we choose the
benchmark in order to evaluate the overhead of the pure OS-related context
switch, disregarding the cache-related energy variations that arise if we in-
crease the matrix dimensions, as shown in Table 4.10 and explained later. It is
also important to remember that, even though context switching does not affect
much energy consumption at all processor clock frequencies, the total energy
needed to carry out the computation is strongly impacted by clock frequency.
In Figure 4.3 we reported the results of the experiment described above
for different clock frequency values, by maintaining the context switching fre-
quency fixed (maximum value). The shape of the plot shows that the energy
consumption decreases as the clock speed increases up to 20%. This is due
to the CPU-dominated nature of the workload (when I/O is dominant, the be-
havior is opposite [1]). This result can be easily explained by considering that
the energy is the product between the average power and the total execution
time. If we consider that the steady state current (and hence the power) profile
obtained when running this experiment is almost flat since the processor does
not access the external bus, the energy cost of thread switching is proportional
to the time spent besides the not-switching case. For this reason, when the
processor speed increases, the total execution time decreases, so the number of
switches decrease and the total time spent due to the kernel calls decrease even
if the time needed to perform a single switch decreases.
Besides context switching, we also performed an experiment in order to
evaluate the overhead of each single kernel call. The results are shown in Ta-
bles 4.7 thru 4.9. The testing parameters are shown in Table 4.6. We made
several calls for each kernel function, then in Tables 4.7 thru 4.9 we have re-
ported also the minimum and maximum energy values. In the table we reported
experimental results related to both the minimum and the maximum available
processor clock speed. In accordance to what we observed in the thread switch-
ing experiment, also in this case the energy cost is smaller at higher frequen-
cies with a few exceptions. In fact, the reduction in execution time due to the
increase of the processor speed doesn't affect the energy contribution propor-
tional to the frequency, but reduces the the cost of the static component. This
table can be used by application designers to estimate the cost of various as
calls in their code without resorting to detailed measurements.
Summarizing, the results obtained in this first phase indicate that for an
application characterized by a small data working set and by low peripheral
activity, it is convenient to work at a high speed.

62
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Table 4./ .
Thread switchexperiment: Energy variationdue to differentswitchingfrequencies
with a fixed clock frequency (l03 .2Mhz)
fSWT
Energy(mJ)
0
ref.
100Hz
+0.69%
2KHz
+0.74%
5KHz
+0.80%
10KHz
+1.22%
10000 r-----;:;l-r-
-
-
----r-----,-------,
9000
'".s
'"
l?
ill
w
8000
7000
'
~--'----
50
100
150
200
250
ClockFrequency(MHz)
Figure 4.3.
Thread switchexperiment: Energy consumption for different clock frequencies at
the maximum switchingfrequency
5.2
I/O Drivers
In this section we show results of measures performed on audio and PCM-
CIA wireless LAN driver. As explained in section 4, we evaluated the energy
characteristic of both drivers in a single and a multi-task environments. The
test set is composed by:
•
burstiness test. This test consist of analysing how the power depends
on the data burst size.
•
clock speed test. Here we derive the energy consumption of drivers as a
function of processor clock speed.
•
resource contention test. We tested the energy overhead which arises
when two thread attempt to use the same resource.
The details of these test are described in the following subsections.
5.2.1
Burstiness Test.
First, we evaluate the relative energy consump-
tion of the RTOS audio driver by sending a block of data toward the audio

Energy Characterization ofEmbedded RTOS
63
Table 4.2.
Audiodriveraveragepowerconsumptiondue todifferentlevelof data burstinessat
a fixedclock frequency
BurstSize(Kbytes)
AvgCPU Power(mW)
Avg System Power(mW)
4
101
910
40
101
903
400
99
897
channel for different level of burstiness and for a fixed clock frequency. Ta-
ble 4.2 shows the results of the experiment. When the burstiness is high, the
CPU accumulates a large burst of data (with respect to the device's output
buffer) before sending it to the device.
We note that starting from the smaller burst size, the energy consumption
weakly decreases. This is the result of two compensating: the first is the energy
overhead due to the higher number of calls to the driver primitives; the second,
which lightly overcomes the first, is the energy saved avoiding additional idle
cycles.
In effect, when the CPU sends data bursts that are large with respect to the
size of the device output buffer, the CPU experience idleness when the out-
put buffer is full. In such time intervals, it spends a non-negligible amount of
energy by polling a synchronization variable. On the contrary, when the bursti-
ness is comparable to the buffering capability of the device, idle intervals are
reduced. Because a small level of burstiness allows better system responsive-
ness, it is convenient to organize the data in little bursts, if possible. From this
results it comes out that we can do this without an additional energy cost.
In order to perform the same test for the waveIan driver, we set-up a c1ient-
server application. The server runs on a Linux-PC while the client runs on
the target board.
The client write data to the server using the read function
provided by the network stack which is a port of the OpenBSD stack for eCos.
The stack lies on the wavelan driver which interfaces directly to the PCMCIA
card. Results are shown in Table 4.3. It can be observed that a lot of power
is spent when the burst size is small. This is obvious since we pay the cost
of multiple calls of the read function. However, the energy variation is bigger
with respect to the audio driver case, since the cost of multiple read is larger
due to the ticker software layer involved (net protocol stack).
5.2.2
Clock Speed Test.
This test is performed by running the same
program built for the previous test but keeping fixed the burst size and chang-
ing the clock frequency. In order to run different instances of the same program
at different clock frequencies we used a system call change-frequency that we
added to our eCos implementation. This allows to safely change at run time the

64
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Table 4.3.
Average powerconsumption of the wireless LAN driverdue to different level of
databurstiness at a fixed clockfrequency
BurstSize(bytes)
Avg CPUPower(mW)
Avg SystemPower(mW)
10
279
1580
10000
126
1160
1000000
126
1150
processor clock frequency. Results are shown in Figure 4.4. In this case we ob-
serve a different behavior with respect to the CPU-bound application. Indeed,
the energy consumption increases significantly as the clock speed increases. In
effect, we notice a variation greater than 40% in the energy consumption from
the minimum to the maximum clock frequency value. The reason is that the
energy wasted by the CPU during idle intervals increases as the clock speed
increases, because idle intervals are longer. Experimental results indicate that
for an application characterized by an aggressive usage of external memory
and peripherals (e.g. data streaming) a lot of energy can be saved by setting
the processor speed as low as possible.
3500 r----~-----_-_---,
3000
~
i';; 2500
:;;
S
2000 L
1500
_----i-._
_
~
50
100
150
200
250
ClockFrequency (MHz)
Figure 4.4.
Energy consumption of the audio driver for different clock speeds at fixed data
burstiness
5.2.3
Resource Contention Test.
The last experiment we performed
for the audio drivers is the measure of device-contention costs . We set-up
two threads that alternatively access the audio driver with a certain switch fre-
quency. In Table 4.4 we show the results. The energy increases as the switching
frequency increases, and the energy variation is higher with respect to the case
of switching two CPU-bound threads .

Energy Characterization of Embedded RTOS
65
Table 4.4.
Variation of the energy consumed by the audio driver in presence of device con-
tention for different switch frequencies
fSW T
Energy(m1)
OHz
ref.
10Hz
+8.43 %
100Hz
+10.0%
1KHz
+10.1%
10KHz
+10.5%
Table4.5.
Comparison between the energy consumed by two version of the speech enhancer:
as based and stand-alone
E59MHz
E 132.7MHz
E 22I.2MHz
code S.(KB)
stdalone
7.6341
9.8341
12.4301
43.172
eCos
8.1181
13.20J
18.040J
55.540
Table 4.6.
Testing parameters for the experimen t related to Tables 4.7 thru 4.9
Testing parameters
Value
Clock samples
32
Thread
50
Thread switches
128
Mutexes
32
Semaphores
32
Scheduler operations
128
5.3
Application Example: RTOS vs Stand-alone
As final experiment, we run the same application with and without the RTOS
support. The application is an adaptive audio-noise canceler, which takes audio
samples from the input, perform LMS filtering and sends the filtered samples
toward the serial channel. In order to do the comparison, we built two versions
of the filter. One version exploits the serial and audio drivers provided by
eCos, the second accesses directly both the I/O channels. Since the application
must playa fixed amount of audio data on the output, the execut ion time is
the same for both versions and the energy measure is also a power measure.
In Table 4.5 we summarize the results of the comparison. Near to the energy
consumption values we show also the difference in code size. We note that the
energy overhead of the OS is not due to the increased size of the application,
but mainly to the driver's overhead.

66
COMPILERS AND OPERATINGSYSTEMS FOR LOW POWER
From these measurements we conclude that event though some kernel ser-
vices (e.g., context switching) are very efficiently implemented from an energy
viewpoint, the presence of an as (even a lightweight one, like eCos) can have
significant impact on the energy consumption of an embedded systems. Our
experiments show that most energy losses are due to device drivers and con-
tention management for I/O resources. These as routines are based on idle
waiting (for performance reasons) which is very energy-inefficient.
5.4
Cache Related Effects in Thread Switching
In order to evaluate the cache-related effects that manifest themselves when
two thread that are switching contend for the cache, we performed the same
matrix multiplication experiment described in the previous section, but with a
larger matrix size, 250x250, in such a way that the data cache does not hold the
whole thread work-set. The results presented in Table 4.10 show that in this
case the cost of the switching is much higher. Indeed the energy consumption
increases not only because the execution time of each thread is higher, but
also because there is an increase in the average power consumption due to the
memory external access related to cache-misses arising at each context switch.
Even though these effects are not caused by the presence of the as,they should
be considered when developing energy-efficient scheduling strategies.
6.
Summary of Findings
The analysis carried out in the previous section can be summarized in the
following points.
•
eCos manages context switching very efficiently, even at high switching
frequency (short time slices), thereby enabling fine-grain slicing with
reduced per-process latency.
•
Even though context switching is not critical from an energy viewpoint,
the programmer should be aware of the fact that system energy is not ad-
ditive with respect to the number of concurrent processes, because cache
locality disruption may cause significant power overhead. Therefore the
number of active processes should not be allowed to grow inordinately.
•
When computation is CPU-bound (i.e., there is limited interaction with
input-output channels and external memory), running at high clock fre-
quency is more energy-efficient than slowing down the clock. This is
because execution is faster, and the processor can be forced into a very
low-power idle state immediately after execution completion. Most eCos
system calls are CPU-bound, hence their energy cost decreases with fre-
quency.

Energy Characterization ofEmbedded RTOS
67
•
When computation is 10 and external memory bound, running at a low
clock frequency is more energy efficient than running at maximum speed.
This is because at high clock frequency the processor spends a higher
fraction of its time stalled for memory and 10 channel accesses, thereby
wasting a considerable amount of power. An alternative view is to ob-
serve that in this case the execution time does not decrease much when
the clock frequency is increased .
•
The above conclusions are valid when the voltage supply is not scaled
with the processor's clock frequency. We conjecture that variable volt-
age operation could make frequency downscaling always convenient, but
our conjecture needs to be validated on a hardware platform with full
variable-voltage support.
•
10 support in eCos (device drivers) does have a significant power over-
head. Stand-alone IO-bound applications consume significantly lower
energy than the same applications running under eCos. Therefore there
is significant room for improvement in the area of energy-efficient de-
vice drivers and 10 support. Our experiments show that buffering helps
reducing 10 energy consumption, but it increases memory usage.
•
Resource contention among different processes has a non-negligible en-
ergy cost. This cost is weakly influenced by the process switching fre-
quency. Therefore there is a significant opportunity in devising strategies
for contention reduction (e.g., scheduling, buffer management, etc.).
In summary, we can conclude that significant opportunities do exist for im-
proving the energy efficiently of embedded operating systems, and that appli-
cation developers should be aware of the energy cost of system calls and, more
generally, as services. These costs can be quantified with the characterization
methodology proposed in this work.
7.
Conclusions
In this chapter we presented a characterization methodology and a detailed
analysis of energy consumption for RTOSes. We presented extensive mea-
surements and a complete characterization for a case study, namely the eCos
operating systems, and the prototype wearable computer HP SmartBadgeIII.
Our work indicates that the knowledge of the energy behavior of the RTOS is
important for the effectiveness of power management policies based on volt-
age and frequency scaling and suggest how to improve them, by taking into
consideration the energy behavior of the different parts of an RTOS.

Table 4.7: Energy consumption of thread management and scheduler functions
at minimum and maximum clock frequencies
!eLK = 22I.2MHz
!eLK = 59MHz
Avg E(JL!)
Min
Max
Avg E(JL!)
Min
Max
Function
1674.43
1568.03
2245.26
1852.17
1732.96
2484.92
Create thread
152.94
148.59
343.40
169.35
164.11
413.63
Yield thread [all suspended]
130.60
111.49
250.65
144.30
129.94
273.49
Suspend [suspended] thread
121.47
111.49
213.54
134.21
129.94
229.05
Resume thread
164.98
148.59
352.70
182.91
170.95
393.12
Set priority
9.57
0.17
46.54
9.72
6.89
58.16
Get priority
283.96
259.92
862.93
313.07
283.75
981.01
Kill [suspended] thread
152.36
148.59
306.29
168.23
164.11
341.85
Yield [no other] thread
190.32
176.43
408 .34
209.19
194.88
413.63
Resume [suspended low prio] thread
127.12
111.49
213.54
140.61
129.94
246.15
Resume [runnable low prio] thread
173.66
157.86
417.65
191.45
174.37
468.31
Suspend [runnable] thread
152.49
148.59
324.86
168.38
164.11
352.10
Yield [only low prio) thread
124.83
111.49
167.13
138.42
129.94
188.04
Suspend [runnable -. not runnable]
277.60
250.65
816.55
306.39
276.91
922.91
Kill [runnable] thread
209.47
194.97
426.91
232.26
215.38
468.31
Destroy [deadI thread
402.12
371.24
760.88
444.23
417.04
909.24
Destroy [runnable] thread
793.50
714.50
1354.66
878.30
810.11
1480.03
Resume [high priority] thread
336.52
334.13
528.97
387.14
369.19
2279.84
Thread switch
6.15
0.17
18.70
7.23
6.89
27.40
Scheduler lock
107.11
102.18
111.49
118.32
116.27
119.68
Scheduler unlock [0 threads]
107.11
102.18
111.49
118.42
116.27
133.35
Scheduler unlock [I suspended]
109.81
102.18
130.02
121.41
119.68
153.87
Scheduler unlock [many suspended]
110.15
102.18
148.59
121.36
119.68
150.44
Scheduler unlock [many low prio]
0\
00
(jo
~
"""tlr::
~
:t.
~o
"""tl
~
::j
~
VJ
2;;
~
~
;j
~
r-
O
~
"""tlo
~
~

Table 4.8: Energy consumption of thread communication and syncronization
functions at minimum and maximum clock frequencies
!eLK = 221.2MHz
!eLK = 59MHz
Avg E(1Lf)
Min
Max
Avg E(JL1)
Min
Max
Function
35.53
27.97
130.02
38.71
34.23
140.18
Init mutex
163.95
148.59
334.13
181.52
167.54
376.03
Lock [unlocked] mutex
175.27
167.13
408.34
193.59
184.62
447.81
Unlock [locked] mutex
139.02
130.02
287.75
153.97
147.02
324.76
Trylock [unlocked] mutex
131.77
130.02
176.43
144.88
143.61
170.95
Trylock [locked] mutex
18.12
9.43
83.65
19.60
17.14
85.50
Destroy mutex
685.50
677.39
909.34
760.01
748.59
1083.54
Unlock/Lock mutex
54.37
46.54
157.86
59.97
54.74
177.78
Create mbox
15.80
9.43
46.54
18.53
13.73
58.16
Peek [empty] mbox
174.11
157.86
436.18
191.88
177.78
471.73
Put [first] mbox
3239.38
464.02
60 11.88
3552.70
478.57
6620.65
Peek [I msg] mbox
164.80
157.86
194.97
182.17
177.78
2 18.81
Put [second) mbox
4.20
0.17
9.43
4.10
3.47
13.73
Peek [2 msgs] mbox
173.22
157.86
36 1.97
189.64
181.21
358.94
Get [first] mbox
168.87
157.86
213.54
186.97
181.21
266.65
Get [second] mbox
167.44
157.86
334.13
184.41
177.78
389.70
Tryput [first] mbox
144.25
139.33
250.65
159.10
153.87
300.83
Peek item [non-empty] mbox
152.94
148.59
287.75
167.32
160.70
311.09
Tryget [non-empty] mbox
140.45
139.33
176.43
154.18
150.44
194.88
Peek item [empty] mbox
140.45
139.33
167.13
155.14
153.87
184.62
Tryget [empty] mbox
4.48
0.17
18.70
5.17
3.47
23.97
Waiting to get mbox
4.78
0.17
27.97
5.29
3.47
30.81
Waiting to put mbox
141.34
130.02
324.86
156.10
147.02
331.59
Delete mbox
419.66
408.34
668.13
463.50
451.23
745.17
Put/Get mbox
26.53
18.70
102.18
29.32
23.97
123.10
Init semaphore
~
;;;
~
'<
Q
l:l
i3
~
~...N'
~
~ .
..s;,
~
6-
~
~
~
:::>...
::tl
(j
Vl
0\
\0

120.45
111.49
148.59
134.10
129.94
167.54
Post [0] semaphore
122.77
120.76
157.86
135.70
133.35
177.78
Wait [1] semaphore
117.57
111.49
176.43
129.51
126.51
188.04
Trywait [0] semaphore
116.10
111.49
148.59
127.37
126.51
153.87
Trywait [I] semaphore
23.05
18.70
55.81
25.26
23.97
65.00
Peek semaphore
15.80
9.43
65.08
17.24
13.73
65.00
Destroy semaphore
465.46
454.75
668.13
514.14
505.91
724.66
PostIWait semaphore
-.Io
8
~
"1::l-
t-
~
V':l
:l:-§
o
"1::l
~:j
~
V':l
~
~
;j
:::0
t-
O
~
"1::lo
~

Table 4.9: Energy consumption of time management functions at minimum and
maximum clock frequencies
!eLK = 221. 2MHz
!eLK = 59MHz
Avg E(JU)
Min
Max
Avg E(JU)
Min
Max
Function
48.87
37.27
148.59
54.3 1
47.90
170.95
Create counter
14.05
0.17
46.54
15.64
6.89
51.33
Get counter value
8.55
0.17
37.27
9.13
6.89
37.64
Set counter value
142.78
139.33
157.86
158.14
157.28
184.62
Tick counter
14.63
9.43
55.8 1
15.97
13.73
65.00
Delete counter
68.26
65.08
130.02
76.21
71.8 3
143.61
Create alarm
183.07
167.13
287.75
202.88
188.04
321.35
Initialize alarm
6.80
0.17
37.27
7.10
3.47
37.64
Disable alarm
167.71
157.86
222.81
185.05
181.21
239.31
Enable alarm
16.68
9.43
65.08
18.43
13.73
78.67
Delete alarm
162.79
157.86
176.43
179.60
177.78
188.04
Tick counter [1 alarm]
960.06
955.71
992.82
1060.59
1059.62
1090.39
Tick counter [many alarms ]
207.42
204.24
232.08
228.95
225.64
246.15
Tick & fire counter [I alarm]
2459.80
2458.63
2495.74
2719.58
27 17.34
2748.09
Tick & fire counters [> I together]
1006.74
1002.09
1029.93
1111.96
1110.89
1145.07
Tick & fire counters I>I separately]
437.72
408.34
723.77
492.13
447.81
1117.73
Alarm latency [0 threads]
551.57
408.34
1540.19
580.44
468.31
1654.35
Alarm latency [2 threads]
1071.04
872.23
1799.94
1181.52
950.25
1500.54
Alarm latency [many threads]
1027.47
937.18
3562.64
11 63.44
1018.60
8958.54
Alarm -+ thread resume latency
~
<;;
~
'<:
9
:::.
i:$
~
~
...,
N'
:::.
~.
~
~
5-
~&
~
::tl
2i
Vl
-.J

72
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
Table4./0.
Energycost of thread switchingin presenceof cache-relatedeffects
fSWT
Energy(1)
OHz
14.662
100Hz
29.330
1KHz
30.108
2KHz
31.696
5KHz
23.190
10KHz
24.651
References
[1] A. Acquaviva, L. Benini, B. Ricco, "An Adaptive Algorithm for Low-
Power Streaming Multimedia Processing," Design, Automation and Test
in Europe Conference, pp. 273-279, March 2001.
[2] Advanced RISe Machines Ltd., Advanced RISe Machines Architectural
Reference Manual, Prentice Hall, New York, July 1996
[3] F. Bellosa, "Endurix: OS-Direct Throttling of Processor Activity for Dy-
namic Power Management," Technical Report TR-14-99-03, University of
Erlangen, June 1999.
[4] L. Benini, G. De Micheli, "System-Level Power Optimization: Tech-
niques and Tools,"
ACM, TODAES,
Vol. 5, No.2, pp. 115-192,
April 2000.
[5] L. Benini, A. Bogliolo, S. Cavallucci, B. Ricco, "Monitoring System
Activity for OS-Directed Dynamic Power Management," IEEE Interna-
tional Symposium on Low Power Electronic and Design, pp. 185-190,
Aug 1998.
[6] R. P. Dick, G. Lackshminarayana, A. Raghunathan, N. K. Jha, "Power
Analysis of Embebbed Operating Systems," Design and Automation Con-
ference, pp. 312-315, - 2000.
[7) M. Flinn, M. Satyanarayanan, "Energy-Aware Adaptation for Mobile Ap-
plication" ACM SOSP, pp. 48-63, December 1999.
[8] I. Hong, M. Potkonjak, M. B. Srivastava, "On-Line Scheduling of Hard
Real-Time Tasks on Variable Voltage Processors," ICCAD, pp. 653-656,
November 1998.
[9] C. M. Krishna, Y. H. Lee, "Voltage-Clock-Scaling Adaptive Schedul-
ing Techniques for Low-Power in Hard Real-Time Systems," RTAS,
May 2000.
[10] T. Ishihara, H. Yasuura, "Voltage Scheduling Problem for Dynamically
Variable Voltage Processor," ISLPED, pp. 197-202, August 1998.

Energy Characterizationof Embedded RTOS
73
[11] A. Lebeck, X. Fan, H. Zeng, C. Ellis, "Power Aware Page Allocation,"
ACM ASPLOS, pp. 105-116, June 2000.
[12] 1. Lorch, A. J. Smith, "Reducing Processor Power Consumption by Im-
proving Processor Time Management in a Single-User Operating Sys-
tem," MOBICOM, pp. 143-154, 1996.
[13] B. Noble, M. Satyanarayanan, D. Narayanan, 1. E. Tilton, J. Flinn,
K. R. Walker, "Agile Application-Aware Adaptation for Mobility," ACM
SOSP, pp. 276-287,1997.
[14] B. Noble, "System Support for Mobile, Adaptive Applications," IEEE
Personal Communications, pp. 44-49, February 2000.
[15] T. Okuma, T. Ishihara, H. Yasuura, "Real-Time Task Scheduling for a
Variable Voltage Processor," DAC, pp. 176-181, June 1998.
[16] Y. Shin, K. Choi, "Power Conscious Fixed Priority Scheduling for Hard
Real-Time Systems," DAC, pp. 134-139, June 1999.
[17] L. Thiele, S. Chakraborty, M. Naedele, "Real Time Calculus for Schedul-
ing Hard Real-Time Systems," IEEE International Symposium on Cir-
cuits and Systems, pp. 101-104, May 2000.
[18] I. Weiser, B. Welch, A. Demers, S. Shenker, "Scheduling for Reduced
CPU Energy," SOSDI, pp. 13-23, November 1994.
[19] K. Weiss, T.Steckstor, W. Rosenstiel, "Performance Analysis of an RTOS
by Emulation of an Embedded System," IEEE International Workshop on
Rapid System Prototyping, pp. 146-151, 1999.
[20] F. Yao, A. Demers, S. Shenker, "A Scheduling Model for Reduced CPU
Energy," Annual Foundation of Computer Science, pp. 374-382, Octo-
ber 1995.
[21] A. Vahdat, A. Lebeck, C. Ellis, "Every Joule is Precious: The Case for Re-
visiting Operating System Design for Energy Efficiency," ACM SIGOPS
European Workshop, 2000.

Chapter 5
DYNAMIC CLUSTER RECONFIGURATION
FOR POWER AND PERFORMANCE *
Eduardo Pinheiro, Ricardo Bianchini, Enrique V. Carrera, Taliver Heath
Department of Computer Science
Rutgers University
{edpin,ricardob,vinicio,taliver}@cs.rutgers.edu
Abstract
In this chapter we address power conservation for clusters of workstations or
pes. Our approach is to develop systems that dynamically turn cluster nodes on
- to be able to handle the load imposed on the system efficiently - and off - to
save power under lighter load. The key component of our systems is an algo-
rithm that makes cluster reconfiguration decisions by considering the total load
imposed on the system and the power and performance implications of chang-
ing the current configuration. The algorithm is implemented in two common
cluster-based systems : a network server and an operating system for clustered
cycle servers . Our experimental results are very favorable, showing that our
systems conserve both power and energy in comparison to traditional systems .
Keywords:
Load balancing , load concentration, power and energy conservation.
Introduction
Power and energy consumption have always been critical concerns for lap-
top and hand-held devices, as these devices generally run on batteries and are
not connected to the electrical power grid. Over the years, a large amount of
research has been devoted to low-power and low-energy design and conserva-
tion (e.g. [Halfhill, 2000; Weiser et al., 1994; Lebeck et al., 2000; Douglis and
Krishnan, 1995; Flinn and Satyanarayanan, 1999]).
In contrast with this line of research, in this chapter we focus on power
and energy conservation for clusters of workstations or pes, such as those that
support most Internet companies and a large number of research and teaching
'This research has been supported by NSF under grant # CCR·9986046.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

76
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
organizations. Our approach to conserving power and energy is to develop
systems that can leverage the widespread replication of resources in clusters.
In particular, we develop systems that can dynamically tum cluster nodes on -
to be able to handle the load imposed on the system efficiently - and off - to
save power under lighter load.
This research is inspired by previous work in cluster-wide load balancing
(e.g, [Barak and La'adan, 1998; Ghormley et al., 1998; Litzkow and Solomon,
1992; Pinheiro and Bianchini, 1999; Cisco, 2000; Bestavros et aI., 1998]).
When performing load balancing, the goal is to evenly spread the work over
the available cluster resources in such a way that idle nodes can be used and
performance can be promoted. The inverse of the load balancing operation
concentrates work in fewer nodes, idling other nodes that can be turned off.
This load concentration or unbalancing operation saves the power consumed
by the powered-down nodes, but can degrade the performance of the remaining
nodes and potentially increase their power consumption. Thus, load concen-
tration involves an interesting performance vs, power tradeoff.
Our systems exploit load concentration to conserve power. Their key com-
ponent is an algorithm that makes load balancing and concentration decisions
by considering both the total load imposed on the cluster and the power and
performance of different cluster configurations. In more detail, the algorithm
uses a control-theoretic approach to determine whether nodes should be added
to or removed from the cluster, and decides how the existing load should be
re-distributed in case of a configuration change.
To be able to understand
the implications of our algorithm, we implemented it in two popular types
of cluster-based systems: a network server and an operating system (OS) for
clustered cycle servers. The implementations were performed in two ways: (1)
at the application level for the network server; and (2) at the as level for the
cycle server. In a previous technical report [Pinheiro et aI., 2001a], we also
considered implementations that rely on application/OS interaction.
Even though we target power conservation primarily, our experimental re-
sults show that our secondary goal of saving energy is achieved as well. Our
results show that the modified network server can reduce the total power con-
sumption by as much as 71% and the energy consumption by 45% in compar-
ison to the original server running on a static cluster configuration. The mod-
ified as can reduce power consumption by as much as 88% for a synthetic
workload, while attempting to avoid any performance degradation, again in
comparison to the original system on a static cluster. The energy savings it ac-
crues in this case is 32%. When a 20% performance degradation is acceptable,
our system conserves 88% of the power and 40% of the energy consumed by
the static system.
The remainder of this chapter is organized as follows . The next section dis-
cusses our motivation. Section 2 describes our cluster configuration and load

Dynamic Cluster Reconfiguration for Power and Performance
77
distribution algorithm and its different implementations. Section 3 describes
our experimental set-up and the methodology used. Section 4 discusses our
experimental results. Section 5 discusses the related work. Finally, section 6
concludes the chapter.
1.
Motivation
Our motivation in pursuing this research is that large clusters consume sig-
nificant amounts of power and energy.
Power consumption is an important
concern for clusters as it directly influences their cooling requirements.
In
fact, a medium to large number of high-performance nodes racked closely to-
gether in the same room, as is usually the case with clusters, requires a sig-
nificant investment in cooling, both in terms of sophisticated racks and heavy-
duty air conditioning systems. Besides cooling under normal operation, power
consumption also influences the required investments in backup cooling and
backup power-generation equipment for clusters that can never be unavailable,
such as those of companies that provide services on the Internet. The recent
trend towards ultra-dense clusters [RLX Technologies, 2001] will only worsen
the cooling problem.
Taking a broader perspective, the power requirements of clusters have be-
come a major issue for several states, such as California and New York, at the
height of the economic boom in the United States. Even if these states make
a tremendous investment in new power plants in the next several years, power
conservation should still be an important goal in that most power-generation
technologies (such as nuclear and coal-based generation) have a negative im-
pact on the environment.
Energy consumption is also an important concern for clusters in that both the
computational and the air conditioning infrastructures consume energy. This
energy consumption is reflected in the electricity bill, which can be significant
for a large and/or dense cluster in a heavily air-conditioned room. Research
and teaching organizations, in particular, may find it difficult to cover high
energy costs.
The bottom line is that to conserve the power and energy consumed by
clusters eases deployment and installation, protects the environment, and can
potentially save a lot of money. In fact, even when it is not possible to reduce
the maximum power requirements of a cluster (i.e. it is not possible to cut down
the one-time cost of cooling and backup power-generation systems), reducing
the common-case power and energy consumption reduces the operational cost
of these systems and the electricity cost.

78
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
2.
Cluster Configurationand Load Distribution
2.1
Overview
Power vs. performance.
We consider the tradeoff between power and
two types of performance, namely throughput and execution time performance.
Throughput is the key issue for systems such as modern network servers, in
which the goal is to service as many requests as possible; the latency of each
request at the server is usually a small fraction of the overall latency of wide-
area client-server communication. Execution time is key for systems such as
cycle servers, as users may object to significant delays in the execution of their
jobs.
The cluster configuration and load distribution algorithm we propose de-
cides whether to add (turn on) or remove (turn off) nodes, according to the
expected performance and power implications of the decision. Decisions are
made dynamically for each cluster configuration and currently offered load.
For simplicity, the algorithm assumes that the cluster is comprised of homo-
geneous machines. Furthermore, the algorithm assumes that the removal of a
node does not cripple the file system. This is a valid assumption, since: (I) in
certain environments it is possible to replicate files at all nodes; and (2) when
this is not the case, the file servers can transparently be run on machines that
do not strictly belong to the cluster or that are not subject to the algorithm.
Addition/removal decision. To make node addition or removal decisions,
the algorithm requires the ability to predict the performance and the power con-
sumption of different cluster configurations. Exact power consumption predic-
tions are not straightforward. The problem is that it is difficult to predict the
power to be consumed by a node after it receives some arbitrary load. Con-
versely, it is difficult to predict the power to be consumed by a node after some
of its load is moved elsewhere.
Nevertheless, exact power consumption predictions are not really necessary
for the algorithm to achieve its main goal, namely to conserve power. The
reason for this is that each of our cluster nodes consumes approximately 70
Watts when idle and approximately 94 Watts when all resources, i.e. CPU,
caches, memory, network interface, and disk, are stretched to the maximum.
These measurements mean that: (a) there is a relatively small difference in
power consumption between an idle node and a fully utilized node; and (b) the
penalty for keeping a node powered on is high, even if it is idle. Thus, we find
in practice that turning a node off always saves power, even if its load has to
be moved to one or more other nodes. Thus, our algorithm always decreases
the number of nodes, provided that the expected performance of applications
is acceptable.
Performance predictions can also be difficult to make. We predict perfor-
mance by keeping track of the demand for (not the utilization of) resources on

Dynamic Cluster Reconfiguration fo r Power and Performance
79
all cluster nodes. With this information, our algorithm can estimate the per-
formance degradation that can be imposed on a node when new load is sent
to it. There is a caveat here, though. A degradation prediction is made based
on past resource demand history of the load to be moved on its current node,
so the prediction does not consider demand changes due to unexpected future
behavior. In particular, the initial settle-down period during which the caches
are warmed up with the new load is disregarded; we are more interested in
steady-state performance.
A throughput prediction can easily be made based on the resource demand
information. To see how this works, let us consider the throughput of a cluster-
based network server. Suppose a scenario with 3 cluster nodes, each of which
with demands for disk of 80%, 30%, and 20% of their nominal bandwidth.
By adding up all of these disk demands (and disregarding other resources to
simplify the example), we find that the server could run with no throughput
degradation on 2 nodes (130 < 200) and with a 30% throughput degradation
on 1 node (130 - 100 =30). Our algorithm should decide to remove one at
least; two nodes if a 30% degradation is acceptable.
Execution time predictions are much more complex, as they depend heavily
on the specific characteristics of the applications and on the amount and timing
of the demand imposed on the different resources. Therefore, we have to settle
for optimistic execution time predictions based on the demand for resources.
The predictions are optimistic because they assume that the use of resources is
fully pipelined and overlapped. To see how this works , let us consider the exe-
cution time performance of applications running on a cluster of cycle servers.
Suppose a scenario with 2 cluster nodes with demands for their CPUs of 80%
and 40%. Our optimistic prediction strategy says that these applications could
run with a 20% execution time degradation on 1 node (120 - 100 =20). Our
algorithm should decide to remove one of the nodes, if a 20% degradation is
acceptable. (In reality, 20% is a lower bound on the degradation.)
It is clear then that a key component of our algorithm is the demand for re-
sources at each point in time. However, the decisions made by the algorithm
must not be solely based on instantaneous demands to avoid reconfigurations
triggered by transient load variations. The algorithm should also take into ac-
count the past history of demands and the speed ofchange in demands. Control
theory provides a formal and well-understood approach to considering these
properties. Thus, we use a Proportional-Integral-Differential (PID) feedback
controller for each resource as the basis for our algorithm's decisions. The
controller with the largest output (in absolute value) is used to determine the
ideal cluster configuration at each point in time. The formula that describes the

80
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
output, o(t), of each controller is:
I
o(t) = kpe(t)+k;Le(t) +kd~e(t ).
o
Each controller calculates the current excess demand (with respect to the
current cluster configuration) for a resource, accumulates excess demand over
time, and computes the rate of change in excess demands. These are the pro-
portional, integral, and differential components of the controller, respectively.
Each of these components is weighted with a tunable constant, which should
reflect how much importance we want to give to each component. In our exper-
iments, we used kp= 0.7, k; =0.15, and kd=0.15. Furthermore, we saturate
the integral component at the resource capacity of a single node, i.e. 100% plus
the acceptable performance degradation. (These constants and saturation value
were chosen after some experimentation with our systems.) The output of the
controller is the sum of the weighted components. The controller is executed
every 10 seconds.
To guarantee stability, our algorithm computes excess demands with respect
to the arithmetic mean of the resource capacities of Nand N-1 nodes for a
configuration with N nodes. Moreover, the algorithm only triggers a reconfig-
uration if the absolute value of the controller's output is greater than half of the
resource capacity of a single node plus 10% of this value. To decide how many
nodes to add or remove , the algorithm divides the output of the controller mi-
nus half of the resource capacity of a node by the resource capacity of a node .
For instance, for a 5-node configuration and a 20% acceptable performance
degradation, excess demands would be computed with respect to 540% (the
average of 5 x 120 and 4 x 120). In this scenario, a controller output of -65%
means that the current configuration should not be altered (65 < 66). An out-
put of -300% should trigger the removal of two nodes <r(1- 3001- 60) /1201
== 2).
We refer to the acceptable degradation and the minimum time between re-
configurations as the degrad and elapse parameters of our algorithm. The
degrad parameter can be specified by the cluster administrator or by each
application (i.e. user). Ideally, the algorithm could also try to guarantee a max-
imum performance degradation. This is clearly not possible for execution time
performance, but is conceivable for throughput performance. However, even
in the case of throughput, such a strong guarantee cannot be made , given that
the load on the cluster may increase faster than the system can react to such
increase. Rather, we use our acceptable performance degradation parameter to
trigger actions that can reduce or eliminate any degradation.
Load (re-)distribution decision. After an addition or removal decision is
made, the load may have to be re-distributed. If the decision is to add one or
more nodes, the algorithm must determine what part of the current load should

Dynamic Cluster Reconfiguration for Powerand Performance
81
be sent to the added nodes. Obviously, the load to be migrated should come
from nodes undergoing excessive demand for resources.
If the decision is to remove one or more nodes, the algorithm must deter-
mine which nodes should be removed and, if necessary, where to send the load
currently assigned to the soon-to-be-removed nodes. Obviously, the algorithm
should give preference to lightly loaded victim nodes and destination nodes
that would not undergo excessive demand for resources after receiving the new
load.
The details of how to select victim nodes and of how to migrate load around
the cluster depend on the system for which the algorithm is implemented, so
we leave the description of these decisions for the next subsection.
2.2
Implementations
Our algorithm has been implemented with minor variations in two different
environments: (1) at the application level for a network server that runs alone
on a cluster; and (2) at the system level for a OS for clustered cycle servers.
In both implementations, the algorithm is run by a master node (node 0),
which is a regular node except that it receives periodic resource demand mes-
sages from all other nodes and it cannot be turned off. We chose centralized
implementations of the algorithm due to their simplicity and the fact that load
messages can be infrequent. For fault tolerance, a distributed implementation
would be best, but that is beyond the scope of this chapter.
Power-aware cluster-based network server. We modified PRESS [Car-
rera and Bianchini, 2001], a cluster-based, event-driven WWW server to im-
plement our algorithm completely at the application level. The server is based
on the observation that serving a request from any memory cache, even a re-
mote cache, is substantially more efficient than serving it from a disk, even a
local disk. Essentially, the server distributes HTTP requests across nodes based
on cache locality and load balancing considerations, so that files are unlikely
to be read from disk if there is a cached copy somewhere in the cluster. Since
the cacheable files are static, each node stores a copy of all files on its local
disk.
We implemented the cluster configuration and load distribution algorithm
in the server making all nodes periodically inform the master node about their
CPU, disk, and network interface demands. The CPU demand is computed by
reading information from the Iproc directory. whereas network and disk de-
mands are computed based on internal server information. To smooth out short
bursts of activity, each of these demands is exponentially amortized over time
using the following formula: a x old.demand + (1 - a) x current.demand.
For our experiments, a = 0.8 and the interval between demand computations

82
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
is 10 seconds. In case of the server, we are interested in throughput perfor-
mance.
With information from all nodes, the master runs the cluster configuration
and load distribution algorithm described in the previous section. If a removal
decision is made, the master determines the maximum demand for any resource
at each node and picks the node(s) with the lowest of the maximum demands
as the victim. For the WWW server, it is not necessary to migrate load from a
node to be excluded from the cluster. The load can be naturally redistributed
among the remaining nodes, by the server's own HTTP request distribution
algorithm and/or a load balancing front-end. Similarly, the addition of a new
node to the cluster does not require migrating any load from other nodes to it.
Note that at the application level it is impossible to determine the demand
for network interface (due to buffering in the kernel) and CPU (due to the fact
that the server is single-process) resources, so our server cannot deal with a
throughput degradation that is greater than 0%. In fact, in our experiments we
assume that the resource capacity of a single node is either 70% or 85% of its
actual capacity, i.e. we study degrad parameters of -30% and -15%. These
values provide some slack to compensate for the time it takes for a node to be
booted, approximately 100 seconds. We set the default value of the elapse pa-
rameter to 120 seconds. Given that the interval between demand computations
is 10 seconds, this setting for elapse allows the server two observations of the
state of a reconfigured cluster before another reconfiguration is permitted.
Power-aware OS for clusters. We modified Nomad [Pinheiro and Bian-
chini, 1999], a Linux-based single-system-image as for clusters of uni andlor
multiprocessor cycle servers. For the purposes of this chapter, the most impor-
tant characteristics of the as are that (a) it has a shared file system; (b) it starts
each application on the most lightly loaded node of the cluster at the moment;
and (c) it performs dynamic checkpointing and migration of whole applications
(with all its processes and state, including open file descriptors, static libraries,
data, stack, registers and the like) between nodes to balance load. Resource
demand is computed for each node in the as, by checking the resource queues
every second. Whenever the average CPU demand, the memory consumption,
or the 1/0 demand observed locally at a node remains higher than a threshold
for 5 seconds, the as considers the node to be undergoing excessive demand
and attempts to migrate some of its load out to a more lightly-loaded node with
respect to the heavily demanded resource.
To avoid excessive migration activity, the migration of an application can
only happen if a few conditions are verified. First, an application can only be
migrated if it has already executed at least as long as the estimated time to mi-
grate a process of its size. Second, a node that has just migrated an application
elsewhere will not migrate another one until a period of stabilization, currently
set to 30 seconds, has elapsed. Third, no incoming migration will be accepted

Dynamic Cluster Reconfiguration for Power and Performance
83
by a node that has been either the source or the destination of a migration dur-
ing the stabilization period. Finally, the OS was designed for clustered cycle
servers, i.e. time-shared execution of sequential applications on uniprocessor
nodes and of parallel applications on multiprocessor nodes, so applications that
do not conform to these restrictions cannot be migrated by the system.
Again, we implemented the cluster configuration and load distribution algo-
rithm in the as making all nodes periodically inform the master node about
their CPU, memory, and I/O demands. The CPU demand and the memory
consumption are computed by reading information from /proc, whereas I/O
demands are determined by instrumenting read and write system calls and get-
ting swap information from /proc. To smooth out short bursts of activity, the
demands are amortized using the same formula used by the WWW server. For
our experiments, a = 0.8 and the interval between demand computations is 10
seconds. In case of the OS implementation of our algorithm, we are interested
in execution time performance.
With information from all nodes, the master can run our algorithm. If a re-
moval decision is made, the master selects the nodes with the lowest demands
for each resource as candidate victims . Unlike the WWW server, in the OS
case the load of the victim must be migrated to other nodes , so the master se-
lects the two nodes with the lightest load with respect to each resource (CPU,
I/O, and memory) and selects the source/destination pair that would lead to
the lowest overall demand for resources. To simplify our prototype implemen-
tation, the destination node receives all applications that are running on the
victim node. Any load imbalances are later corrected by the as according to
its load balancing policy.
In the modified OS, a node addition is not effected if only one application
is responsible for the excessive demand. After a new node is turned on, the
OS will start migrating applications to it, so that the load will be balanced
again. Given that adding nodes takes a significant amount of time (about 100
seconds), it might take a while before the demand for resources becomes ac-
ceptable again, after a long-lasting surge of activity. We experiment with two
values for degrad: 0% and 20%. The elapse parameter is set to 150 sec-
onds. This setting allows the system time to reconfigure, migrate applications
to balance the load, and re-evaluate the resource demands before another re-
configuration is allowed.
3.
Methodology
To study the performance of our algorithm and systems, we performed ex-
periments with a cluster of 8 pes connected by a Fast Ethernet switch and a
Giganet switch . Each of the nodes contains an 800-MHz Pentium III proces-
sor, 512 MBytes of memory, two 7200 rpm disks (only one disk is used in our

84
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
experiments), and two network interfaces . Shutting a node down takes approx-
imately 45 seconds and bringing it back up takes approximately 100 seconds.
All machines are connected to a power strip that allows for remote control
of the outlets. Machines can be turned on and off by sending commands to
the IP address of the power strip. The total amount of power consumed by
the cluster nodes is then monitored by a multimeter connected to the power
strip. The multimeter collects instantaneous power measurements 3-4 times
per second and sends these measurement to another computer, which stores
them in a log for later use. We obtain the power consumed by different cluster
configurations by aligning the log and our systems' statistics.
Network server experiments. Besides the main cluster, we use another 14
Pentium-based machines to generate load for the modified WWW server. For
simplicity, we did not experiment with a front-end device that would hide the
powering down of cluster nodes from clients. Instead, clients poll all servers
every 10 seconds and can thus detect cluster reconfigurations and adapt their
behavior accordingly. The clients send requests to the available nodes of the
server in randomized fashion according to a trace of the accesses to the World
Cup '98 site from 12pm on 07/12 to 12pm on 07114. The trace includes the
day of the championship match. To shorten the duration of the experiments,
we accelerated the trace 20 times.
Distributed OS experiments. The synthetic workload used for our modi-
fied as experiments draws applications from a number of sources: all integer
applications from the SPEC2000 benchmark, the Berkeley MPEG movie en-
coder, and two I/O benchmarks, 10call and 10zone. IOcall is a benchmark to
measure as performance on I/O calls, especially file read system calls. 10zone
is a file system benchmarking tool [lOzone, 2000]; it generates and measures
the performance of a variety of file operations. Applications are arbitrarily as-
signed to nodes and are run in arbitrary groups. Because the cluster size varies
dynamically according to the resource demand imposed on it, we start with
only one machine powered on (the master), which is responsible for launch-
ing all applications in the workload. The offered demand conforms to a bell-
shaped curve. To shorten the length of the experiments, we generate significant
changes in offered demand in very little time.
4.
Experimental Results
Power-aware cluster-based network server. We describe two experiments
with our server. In the first experiment, the parameters for the cluster recon-
figuration algorithm are set to guarantee quick reaction to fluctuations in load,
so that we can tackle significant increases in load without performance degra-
dation. More specifically, the algorithm tries to keep 30% spare resources for
any cluster configuration. Figure 5.1 presents the evolution of the cluster con-

Dynamic Cluster Reconfigurationfor Power and Performance
85
5 ~
4 <;
~
3 z
CPUload -
Diskload •••••••
Nelload
..
Number01nodes ._.•.--~..-
7
."\
~:~:.....
i'c ,"',IJ\ ",.Io,
"
_ .:.....;,. ......."......,)...~/"..,.' • ••"••\ro"""........,,!" '.oJ
,.."I.... 'o' •
....••.:·'t....~.........A..:
oL---l_--'-_-'-_-'-_-'--_.L..-----''----L----'
o
1000
2000
3000
4000
5000
6000
7000
8000
Time(sec)
700
600
Figure 5.1.
Cluster evolution and resource demands for the WWW server. elapse = 120
seconds; degrad =-30%.
figuration and demands for each resource as a function of time in seconds for
this experiment. The demand for each resource is plotted as a percentage of
the nominal throughput of the same resource in one node.
The figure shows that for this particular workload the network interface is
the bottleneck resource throughout the whole execution of the experiment (144
minutes). We started the experiment with a two-node configuration. The traf-
fic directed to the server initially increases slowly, triggering the addition of
a node, before increasing substantially and triggering the addition of several
new nodes in quick sequence. The traffic then subsides, until another period of
high traffic occurs, which is followed by a substantial decline in traffic. Note
that throughout the experiment the system reacts quickly to increases in traffic,
because of the spare capacity it consistently retains. As a result of the spare
capacity, the performance of the server is not affected by the dynamic recon-
figuration of the cluster.
Figure 5.2 presents the power consumption of the whole cluster for two ver-
sions of the same experiment as a function of time. The lower curve (labeled
"Dynamic Configuration") represents the version in which we run the power-
aware server, i.e. the cluster configuration is dynamically adapted to respond
to variations in resource demand. The higher curve (labeled "Static Configu-
ration") represents a situation where we run the original server, i.e. the cluster
configuration is fixed at 7 nodes. As can be seen in the figure, our modified
WWW server can reduce power consumption significantly for most of the exe-
cution of our experiment. Power savings actually reach 71% when the resource
demands require only two nodes. Our energy savings are also significant. Cal-
culating the area below the two curves, we find that the power-aware server

~
~!~!
SIalicConfiguration-
Otnamic Conliguration .._.••.
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
1000
2000
aoco
4000
5000
6000
7000
6000
TIme(sec)
86
800
700
600
500
400
JOO
200
100
0
0
Figure 5.2.
Power consumption for the WWW server under static and dynamic cluster config-
urations. elapse =120 seconds; degrad =-30%.
700
800
5 •i
4 "5I
3 z
100
•
"""':·..f.!...-{...:·....·...·~"..\J\\ ·"''''''rv.,''',...../ / \.•)\ ........t\~I'•./v)''.........,......".~....."\/ .\,\.:-J
o'---"'---.J'---l_--'-_--'-_-L._-'-_---'-_--'-~
o
1000
2000
3000
4000
5000
6000
7000
8000
Time(sec)
Figure 5.3.
Cluster evolution and resource demands for the WWW server. elapse =120
seconds; degrad = -15%.
saves 38% in energy. Thus, the load on the cooling infrastructure over time is
also reduced by 38%.
Even though these are significant gains, we can do better. The reason is
that keeping spare capacity promotes performance at the cost of higher power
and energy consumption. If we can estimate how fast the offered traffic can
increase, we can reduce the spare capacity to the minimum required to avoid
excessively long request latencies. For our trace, this minimum is 15%. Thus,
Figure 5.3 presents the evolution of the cluster configuration when the system

Dynamic Cluster Reconfiguration for Power and Performance
87
attempts to retain this much spare capacity. Comparing Figures 5.1 and 5.3
we can see that during most of the experiment the system now requires fewer
active nodes to handle the offered load. In this case, the power and energy
gains that can be achieved in comparison to a static system with 7 nodes are
71% and 45%, respectively.
In general, it might not be possible to determine the maximum rate of work-
load change a priori. In these cases, mismatches between the rate of work-
load change and cluster reconfiguration delays can be alleviated by adjusting
the elapse and/or degrad parameters dynamically. We believe however that
in practice values of a few minutes for elapse and a few percent for degrad
should work just fine, since real network server workloads usually change more
slowly than in our experiments.
Power-aware OS for clusters. Figure 5.4 presents the evolution of the clus-
ter configuration and demands for each resource with elapse = 150 seconds
and degrad = 0%, as a function of time. The experiment lasted about 46 min-
utes. The CPU is always the bottleneck resource during the experiment. The
experiment starts with a single-node configuration. This node is responsible for
starting all the applications in the workload. As new applications are started,
the CPU demand increases and eventually triggers the addition of a new node .
When the new node is added by the master, the OS attempts to balance the
load by migrating some applications to the new node. As the number of appli-
cations started increases, they trigger the addition of other nodes, one at a time.
The OS is able to track the demand increases fairly well by increasing the size
of the cluster. At about half way through the experiment, the demand for CPU
becomes much higher than can be managed by an 8-node cluster. Right after
this peak in demand however, some applications start to finish and the demand
for resources drops quickly.
The master responds to this change in load by
excluding the now idle nodes, one at a time . Again, the system does a good job
of tracking the decrease in resource demand.
Figure 5.5 presents the power consumption of the whole cluster for two
versions of the same experiment as a function of time. As can be seen in the
figure, our power-aware OS can reduce power consumption significantly for
most of the execution time of the experiment. Power savings actually reach
88% when the resource demands require only a single node. Energy savings
are also significant. The area below the two curves indicates that the power-
aware OS saves 35% in energy for this workload.
It is interesting to note that the workload used in this experiment finishes
a little earlier on the static configuration (around 45 minutes) than on the dy-
namic one (around 46 minutes). If we compare the energy consumed by the
static configuration during the first 45 minutes of the experiment against that
of the dynamic configuration for the whole experiment, we find that our en-
ergy savings are only slightly smaller, 32%. (This comparison is not really fair

88
1000
800
I
600
g
"0
!!
~
400
COMPILERS AND OPERATING SYSTEMS FORLOWPOWER
CPUload -
Memload •••••••
VQload
.
Numberof nodes ._"_0.' _.
Figure5.4.
Cluster evolution and resource demands in the power-aware as. elapse = 150
seconds; degrad = 0%.
S1alic Conliguralion -
DynamicConfiguration ••••--.
~W1
~
~
~:L .I._Lj
iUWIW"
700
500
600
8OO....-----,---"T----,----,----.----,
3000
2500
2000
1500
Time(s)
1000
500
ol..-._--''--_--'-__--'-__-'-__--'-_--'
o
Figure 5.5.
Power consumption for the power-aware as under static and dynamic cluster con-
figurations . elapse = 150 seconds; degrad = 0%.
however, since real, i.e. static, cycle servers are never turned off). In any case,
it is clear that the load on the cooling infrastructure is reduced by at least 32%
under the dynamic system.
To investigate the tradeoff between performance and power, we also per-
formed experiments in which our intended performance degradation is 20%.
We kept elapse at 150 seconds. Figure 5.6 illustrates the evolution of the
cluster configuration in this case. As one would expect, allowing for some
performance degradation has the effect of slowing down the addition of new

Dynamic Cluster Reconfigurationfor Power and Performance
1200.----.----,~---,,-------r---r----, 10
CPU load-
Memload•••••••
VQload ..,...
Numberofnodes '.--."•.-.
1000
800
I
~ ~j-';\d\(~~}\):>',.
,.._ ..
o
500
, 000
,SOO
2000
2SOO
3000
Time(s)
89
Figure 5.6.
Cluster evolution and resource demands in the power-aware OS. elapse = 150
seconds; degrad =20%.
nodes and speeding up the removal of unnecessary nodes. As a result, the sys-
tem decides to jump directly from 6 to 8 nodes when ramping up and to jump
directly from 7 to 5 nodes when down-sizing. Overall, our system conserves
88% and 42% of the power and energy consumed by its static counterpart in
this experiment. If we consider that the workload finishes 2 minutes later on
the dynamic than on the static configuration, the energy gains are of 40%.
5.
Related Work
Most of the previous work on conservation has been focused on laptop com-
puters and embedded and hand-held devices. Research on these devices has
included optimizations for the processor (e.g. [Weiser et aI., 1994; Halfhill,
2000; Hsu et aI., 2000)), for the memory (e.g.
[Lebeck et aI., 2000; Vi-
jaykrishnan et aI., 2000; Delaluz et al., 2001)), for the disk (e.g. [Li et aI.,
1994; Douglis and Krishnan, 1995; Helmbold et al., 1996)), and for offload-
ing computation from them to non-battery -operated computers (e.g. [Rudenko
et aI., 1998; Kremer et aI., 2000)).
Some of this research can be used to optimize each node of a cluster inde-
pendently, so we can also benefit from it. However, our research is orthogonal
to these contributions in the sense that we focus on cluster-wide power and en-
ergy conservation, i.e. conservation that considers all of the cluster resources
and the load offered to the cluster as a whole.
We originally proposed the ideas and systems described here in [Pinheiro
et al., 2001a]. A shorter and revised version of that paper appears in [Pinheiro
et aI., 200lb]. This chapter extends our original work in several ways: (a) our

90
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
original cluster configuration and load distribution algorithm did not consider
the past behavior and speed of change of the offered workload when making
its decisions; (b) our original cluster reconfiguration decisions were limited to
adding or removing a single node at a time; and (c) our original evaluation of
the power-aware server assumed a synthetic workload. Extensions (a) and (b)
increased our ability to track and quickly adjust to variations in offered load,
whereas extension (c) allows us to demonstrate the usefulness of our approach
in realistic scenarios.
Two recent papers [Chase et al., 2001; Elnozahy et al., 2002] also deal with
power and energy research for clusters. Chase et ai. [Chase et al., 2001] tack-
led the general problem of resource allocation in hosting centers using market-
based policies. In terms of power and energy conservation, they evaluated
a resource allocation policy for a clustered WWW server that is similar to the
cluster configuration algorithm we study here. Elnozahy et al. [Elnozahy et al.,
2002] evaluated different combinations of cluster reconfiguration and dynamic
voltage scaling for clusters. They showed that the benefits of our technique can
be increased by coupling it with coordinated (cluster-wide) voltage scaling.
As mentioned earlier, load concentration is inspired by previous work in
cluster-wide load balancing (e.g. [Barak and La'adan, 1998; Ghormley et al.,
1998; Litzkow and Solomon, 1992; Douglis and Ousterhout, 1991; Pinheiro
and Bianchini, 1999; Cisco, 2000; Bestavros et al., 1998]). Some systems do
use some form of load concentration, but only as a remedial technique like in
systems that harvest idle workstations (e.g. [Barak and La'adan, 1998; Litzkow
and Solomon, 1992]) or as a management technique for manually excluding a
cluster node. We use load concentration as a first-class technique for conserv-
ing power and energy in clusters.
The technique that is closest in spirit to load concentration for power and en-
ergy is offloading computation from a battery-operated device to a remote non-
battery-operated computer (e.g. [Rudenko et al., 1998; Kremer et al., 2000]) .
However, load concentration as described here involves different challenges
and tradeoffs, mainly because the load on the cluster and the effect of applying
the technique must be determined before any action can be taken.
A few other projects deal with cluster reconfiguration (e.g. [Fox et al., 1997;
Appleby et al., 2001; Van Renesse et al., 1998; Goldszmidt and Hunt, 1999]).
Even though these projects do not consider power and energy issues, they lend
themselves nicely to the powering down of unused systems.
In terms of our algorithm, the most closely related work is that of Skadron
et al. [Skadron et al., 2002]. They proposed the use of control-theoretic tech-
niques for dynamic thermal management of microprocessors. We apply similar
techniques to cluster reconfiguration for power and performance.

Dynamic Cluster Reconfiguration for Power and Performance
6.
Conclusions
91
In this chapter we addressed power and energy conservation for clusters.
We proposed a control-theoretic cluster configuration and load distribution al-
gorithm and applied it under two different scenarios. Our experiments showed
that it is indeed possible to conserve significant power and energy in the con-
text of clusters. Based on our experimental results, we conclude that exploiting
periods of light load can provide tremendous gains for organizations and com-
panies that rely on large clusters of servers.
Acknowledgments
We would like to thank Carla Ellis, Brett Fleisch, and Liviu Iftode for com-
ments on the topic of this research. We also thank Uli Kremer, Mike Hsiao,
and the rest of the people in the Programming Languages reading group, who
helped us improve the quality of this chapter. Finally, we would like to thank
Uli Kremer for letting us use the power measurement infrastructure of the En-
ergy Efficiency and Low-Power (EEL) lab at Rutgers.
References
Appleby, K., Fakhouri, S., Fong, L., Goldszmidt, G., Kalantar, M., Krish-
nakumar, S., Pazel, D., Pershing, J., and Rochwerger, B. (2001) . Oceano -
SLA Based Management of a Computing Utility. In Proceedings ofthe 7th
IFIPIIEEE International Symposium on Integrated Network Management.
Barak, A. and La 'adan, O. (1998). The MOSIX Multicomputer Operating Sys-
tem for High Performance Cluster Computing. Journal of Future Genera-
tion Computer Systems, 13(4-5):361-372.
Bestavros, A., Crovella, M., Liu, J., and Martin, D. (1998). Distributed Packet
Rewriting and its Application to Scalable Server Architectures. In Proceed-
ings ofthe International Conferen ce on Network Protocols.
Carrera, E. v., and Bianchini, R. (2001) . Efficiency vs. portability in cluster
based network servers. In Proceedings of the 8th ACM SIGPLAN Sympo-
sium on Principles and Practice ofParallel Programming.
Chase, 1., Anderson, D., Thackar, P., Vahdat, A., and Boyle, R. (October 2001) .
Managing energy and server resources in hosting centers. In Proceedings of
the 18th Symposium on Operating Systems Principles.
Cisco (2000). Cisco LocalDirector. http://www.cisco.coml.
Delaluz, v., Kandemir, M., Vijaykrishnan, N., Sivasubramaniam, A., and Ir-
win, M. J. (January 2001) . DRAM energy management using software and
hardware directed power mode control. In Proceedings ofthe International
Symposium on High-Performance Computer Architecture.

92
COMPILERS AND OPERATING SYSTEMS FOR LOWPOWER
Douglis, F. and Krishnan, P. (1995). Adaptive disk spin-down policies for mo-
bile computers. Computing Systems, 8(4) :381-413.
Douglis, F. and Ousterhout, J. (1991). Transparent Process Migration: Design
and Alternatives and the Sprite Implementation. Software: Practice and Ex-
perience, 21(8):757-785.
Elnozahy, E. N., Kistler, M., and Rajamony, R. (2002). Energy-Efficient Server
Clusters. In Proceedings ofthe 2nd Workshop on Power-Aware Computing
Systems.
Flinn, J. and Satyanarayanan, M. (1999). Energy-aware adaptation for mobile
applications. In Proceedings of the 17th Symposium on Operating Systems
Principles, pages 48-63.
Fox, A., Gribble, S., Chawathe, Y, Brewer, E., and Gauthier, P. (1997). Cluster-
Based Scalable Network Services. In Proceedings ofthe International Sym-
posium on Operating Systems Principles, pages 78-91.
Ghormley, D., Petrou, D., Rodrigues, S., Vahdat, A., and Anderson, T. (1998).
GLUnix: a Global Layer Unix for a Network of Workstations. Software:
Practice and Experience.
Goldszmidt, G. and Hunt, G. (1999). Scaling Internet Services by Dynamic Al-
location of Connections. In Proceedings ofthe 6th IFlPIIEEE International
Symposium on Integrated Network Management, pages 171-184.
Halfhill, T. (February 2000). Transmeta breaks the x86 low-power barrier. In
Microprocessor Report.
Helmbold, D. P., Long, D. D. E., and Sherrod, B. (1996). A dynamic disk spin-
down technique for mobile computing. In Proceedings of the 2nd Interna-
tional Conference on Mobile Computing (MOBICOM96), pages 130-142.
Hsu, C.-H., Kremer, U., and Hsiao, M. (November 2000). Compiler-directed
dynamic frequency and voltage scaling. In Proceedings ofthe Workshop on
Power-Aware Computer Systems.
IOzone (2000). Iozone filesystem benchmark. http://www.iozone.org.
Kremer, D., Hicks, 1., and Regh , J. (October 2000). Compiler-directed remote
task execution for power management. In Proceedings of the Workshop on
Compilers and Operating Systemsfor Low Power.
Lebeck, A. R., Fan, X., Zeng, H., and Ellis, C. S. (2000). Power aware page
allocation. In Proceedings ofthe 9th International Conference on Architec-
tural Support for Programming Languages and Operating Systems (ASP-
LOS IX), pages 105-116.
Li, K , Kumpf, R., Horton, P., and Anderson, T. (1994). A quantitative analysis
of disk drive power management in portable computers. In Proceedings of
the 1994 Winter USENIX Conference, pages 279-291.
Litzkow, M. J. and Solomon, M. (1992). Supporting Checkpoint and Process
Migration Outside the UNIX Kernel. In Usenix Conference Proceedings,
pages 283-290, San Francisco, CA .

DynamicClusterReconfiguration for Power and Performance
93
Pinheiro, E. and Bianchini, R. (December 1999). Nomad: A scalable operating
system for clusters of uni and multiprocessors. In Proceedings of the Ist
IEEE International Workshop on Cluster Computing.
Pinheiro, E., Bianchini, R., Carrera, E. v., and Heath, T. (2001a). Load Balanc-
ing and Unbalancing for Power and Performance in Cluster-Based Systems.
Technical Report DCS-TR-440, Department of Computer Science, Rutgers
University.
Pinheiro, E., Bianchini, R., Carrera, E. v.. and Heath, T. (200 1b). Load Balanc-
ing and Unbalancing for Power and Performance in Cluster-Based Systems.
In Proceedings ofthe International Workshop on Compilers and Operating
Systemsfor Low Power.
RLX Technologies (2001). Serverblade. http://www.rlxtechnologies.comJ.
Rudenko, A., Reiher, P., Popek, G. J., and Kuenning, G. H. (1998). Saving
portable computer battery power through remote process execution. Mobile
Computing and Communications Review, 2(1):19-26.
Skadron, K., Stan, M., and Abdelzaher, T. (February 2002). Control-theoretic
techniques and thermal-rc modeling for accurate and localized dynamic
thermal management. In Proceedings of the International Symposium on
High-Performance Computer Architecture.
Van Renesse, R., Birman, K., Hayden, M., Vaysburd, A., and Karr, D. (1998).
Building adaptive systems using Ensemble. Software Practice and Experi-
ence, 28(9):963-979.
Vijaykrishnan, N., Kandemir, M., Irwin, M., Kim, H., and Ye, W. (2000).
Energy-driven integrated hardware-software optimizations using Simple-
Power. In Proceedings ofthe 27th Annual International Symposium on Com-
puter Architecture, pages 95-106.
Weiser, M., Welch, B., Demers, A., and Shenker, S. (1994). Scheduling for re-
duced cpu energy. In Proceedings ofthe Ist Symposium on Operating Sys-
tem Design and Implementation.

Chapter 6
ENERGY MANAGEMENT OF
VIRTUAL MEMORY ON DISKLESS DEVICES*
Jerry Hom
Ulrich Kremer
Department of Computer Science
Rutgers University
Piscataway. New Jersey
{jhom,uli}@cs.rutgers.edu
Abstract
In a pervasive computing environment, applications are able to run across differ-
ent platforms with significantly different resources. Such platforms range from
high-performance desktops to handheld PDAs. This chapter discusses a com-
piler approach to reduce the energy consumption of a disklessdevice where the
swap space is provided by a remotely mounted file system accessible via a wire-
less connection. Predicting swapping events at compile time allows effective
energy management of a PDA's wireless communication component such as a
802.11 or Bluetoothcard.
The compiler activates and de-activates the communication card based on
compile-time knowledge of the past and futurememory footprintof an applica-
tion. In contrast to as techniques, the compiler can better predict future pro-
gram behavior, and can change this behavior through program transformations
that enable additional optimizations.
A prototype compilation system EELRM has been implemented as part of
the SU1F2 compiler infrastructure. Preliminaryexperimentsbased on the Sirn-
pleScalarsimulation toolset and threenumerical programs indicate the potential
benefitsof the newtechnique.
"This work was partially supported by NSF CAREER award No. CCR-9985050.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

96
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
1.
Introduction
Many handheld devices and machines already have wireless communication
capabilities, allowing them to be part of a large and pervasive computing en-
vironment that supports sharing of resources across the network. Traditional
desk-top applications will become increasingly important for handhelds which
have developed from electronic address books and appointment schedulers to
portable workstations. For instance, the newest Compaq iPAQ H3600 hand-
held has 64MB of RAM, 16MB of flash memory, and a 206MHz low-power
StrongARM processor [7].
Such devices will run spread-sheets. voice and
image recognizers, and even computation intensive simulation programs, just
to mention a few. However, many mobile machines may not have secondary
storage such as a disk . Giving mobile machines the ability to support virtual
memory through a wireless connection can significantly increase their func-
tionality since the same programs can be executed on a desktop machine and
the handheld. This is particularly important for programs where the memory
needs vary significantly based on the provided input data. However, the op-
tion of swapping pages over the wireless connection comes with the price of
additional energy requirements due to the wireless networking card and com-
munication costs . In this chapter we discuss a compilation strategy that will
reduce the energy overhead of swapping over a wireless network through net-
work card hibernation.
Resource hibernation is an effective strategy to save power and energy of
system components and resources that are not needed during some parts of a
program execution. While not in use, these components and resources con-
sume energy which may be saved by transferring them into a hibernation or
sleep state during their idle periods.
System resources may implement dif-
ferent levels of hibernation , where each level has a specific tradeoff between
power saved vs. the time it takes to deactivate or reactivate the resource. Typ-
ically, the "deeper" the hibernation or sleep mode, the longer it will take to
make a transition to and from this hibernation state, but the less power will be
used by the resource during the hibernation period. Effective power and energy
management of a wireless connection is crucial for handheld devices that rely
on battery power since the communication component typically consumes a
substantial share of the overall energy and power budget. On Compaq's iPAQ
H3600 pocketf'C, communication via an Orinoco WaveLAN 802.llb wireless
card consumes more than 40% of the overall energy budget with an image
processing application [161.
The ACPI (Advanced Configuration and Power Interface [8]) standard spec-
ifies hibernation states for different system resources such as disks, wired and
wireless Ethernet controllers, processors, and displays. ACPI conforming sys-
tems are possible target systems for our compilation strategy. Most work in

Energy Management of Virtual Memo ry
97
resource management for power and energy savings purposes has concentrated
on operating system and hardware techniques [11, 5, 18, 19,22]. In this chap-
ter, we investigate the potential benefit of compiler directed resource manage-
ment for a system resource such as a wireless communication card. We will
also compare our approach with an as approach where deactivation is based
on a threshold strategy, and activation is done on demand. Our benefit study
is based on a set of three numerical, array based applications (shal, adi, and
tomcatv). All three applications represent regular problems, for which many
program characteristics can be derived at compile time. In the future, we will
consider irregular problems and pointer based programs. We believe that com-
putation intensive simulation codes wiII be part of the program mix for portable
workstations such as Compaq's iPAQ pocketPC.
In this chapter, we assume that only a single application is executing on
the handheld machine. In a multi-programming environment, the information
collected by our compiler can be used by the underlying operating system to
effectively schedule page requests across different active processes.
2.
Related Work
Various approaches to managing physical memory chips for energy consid-
erations have been investigated by the Microsystems Design Lab at Pennsyl-
vania State University. In particular, Delaluz et aI. show effective compiler
techniques to conserve energy from DRAM chips [10]. In contrast, our ap-
proach examines a higher abstraction level, namely virtual memory and the
mechanisms for updating it.
Traditionally, the next level after physical memory in the memory hierarchy
is disk storage. In the presence of virtual memory, the disk acts as a backing
store. On diskless devices, a network interconnection provides the same func-
tionality. Indeed the disk, whether local or remote, can be simply classified
as peripheral storage. More generally then, accessing peripheral storage is the
target resource we wish to manage. Disk power management for mobile com-
puters has already received much attention [18, 12, 24, 17]. Although typically
managed by the as using an idle time threshold mechanism, Li et al. found
the optimal time threshold should be 2 seconds [17]. Our experiments with
accessing remote peripheral storage agree in principle with this finding.
The idea of remote virtual memory, particularly distributed and/or shared,
has been an ongoing subject for over 15 years. Comer and Griffioen examine
the usefulness of a dedicated memory server [6]. They make the distinction
of separating the paging operation from the file backing store operation. Then
they can focus separately on designing efficient memory and file servers. An-
other approach views the sum total memory of a cluster as a single cache space

98
COMPILERS AND OPERATINGSYSTEMS FOR LOW POWER
[9]. Dahlin et al. suggest utilizing the memory of idle nodes. These approaches
improve performance by optimizing the use of extended virtual memory.
Recognizing the utility of remote resources, Schilit and Duchamp make the
case for thin clients [21]. They conclude the feasibility and desirability for thin
clients without a disk and smaller amounts of memory. While not necessarily
studying energy consumption impacts, their work establishes a reference point
in motivating low power designs of diskless devices. From a compiler point of
view, we attempt to optimize energy demands by managing resources such as
remote virtual memory paging over a wireless connection.
3.
Problem Formulation
For simplicity, we assume that a communication card only supports three
power mode states: active, idle, and sleep (hibernate). In the active mode,
the card is transmitting data. In idle mode, the card is not sending messages,
but listens to the wireless networking traffic. Finally, in the sleep or hiberna-
tion mode, the card has been shut down to save power. There is an overhead
for transitions between hibernation modes. We assume that the performance
penalties for shutting down and waking up the card are the same.
Figure 6.1 shows the power profile of a sample application without any
power management, with operating system guided , and with compiler-directed
power management. The simple OS based technique transfers the card into
sleep mode after a predefined (static) inactivity threshold. The wake-up opera-
tion is performed on demand, and as a result incurs a performance penalty.
This simple example illustrates the advantages of a compiler-directed ap-
proach vs. a threshold based OS approach. In the former approach, system
resources can make the transition into power saving states earlier, can be reac-
tivated just-in-time to avoid performance penalties, and enable additional op-
timization opportunities for idle periods which are shorter than the threshold
used by the OS based technique. It is important to note that there are more so-
phisticated OS based dynamic power management techniques than the simple
technique discussed here [18, 22]. However, the point we want to make is that
in many cases the compiler can predict future program behavior and resource
requirements more accurately than OS based techniques, allowing additional
opportunities for power and energy management optimizations.
The handheld device is connected to a network file system (NFS) via the
wireless connection. Each time a page fault occurs, the required page has to
be requested over the wireless link, and the program blocks until the page is
received. Each page fault event leads to a new working set, with the empty set
as the initial working set of an application.
Our compilation strategy tries to identify program parts of the program ex-
ecution where the working set is either

Energy Management of Virtual Me11l01Y
99
...
~o
Q.
no
power management
execution time
...
~o
Q.
as directed
power management
execution time
...
~o
Q.
compiler directed
power management
execution time
_
active
Q
idle
o
hibernating
o
shut down I wake up
Figure 6.1.
Comparison of compiler vs. as directed power mangement.
1 the same for the next x machine cycles, or
2 is about to change in y machine cycles.
This information is used to suspend the wireless card if x is larger than a prede-
termined benefit threshold, or resume the card in y cycles, where y is the time
needed to reactivate the card. Both entities will be determined by the compiler
using static performance prediction.
OS guided hibernation may use threshold techniques to shut down system
components such as a wireless card. Threshold techniques assume that if a
resource has not been used within the past threshold time units, it will not be
used in the future .

100
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
4.
EELRM Prototype Compiler
The EELRM 1 prototype compiler is based on the SUIF2 compilation infras-
tructure [23]. The compilation strategy consists of two main phases, with each
phase having multiple steps. During the first phase, program regions are iden-
tified for which the wireless connection needs to be activated or deactivated.
The data objects accessed in each region are summarized, and a forward data
flow problem approximates the data objects that will be in memory before en-
tering each region . If the set of data objects that will be referenced in a region
is a subset of the data objects currently in memory. the execution of the region
does not require the wireless connection to be active.
In the second phase, system calls are inserted that either activate or deac-
tivate the wireless PC card . Deactivation is done as soon as possible, and
activation is performed on-demand. Activation and deactivation operations are
assumed to be atomic, i.e., once the PC card is in the process of being shut-
down, a pending wake-up request has to wait until the shut-down has been
completed and vice versa. The second phase requires performance prediction
for efficiently placing activation requests. An activation request before a pro-
gram region should only be executed if the card is in a hibernation state. If
the card is active, no action needs to be taken.
This can be easily handled
through the activation routine itself, or through compiler generated guards for
each activation or deactivation request.
Performance prediction is needed to activate the PC card just in time. For in-
stance, if the overhead of activation is 106 cycles, the activation request should
be issued 106 cycles before the card needs to be active. In addition, perfor-
mance prediction is required to assess the benefit of deactivating the PC card.
Deactivating the card is not beneficial if the next activation request follows too
closely (i.e., before the card is shut-down, a request to reactivate it is already
pending).
4.1
Phase 1 - Analysis
This analysis phase consists of several subtasks.
Program regions are identified that will serve as the basis for our analy-
sis. The compiler will insert hibernate or activate instruction only before
such regions . The initial prototype system recognizes innermost loop
nests, called phases [14], and calls to runtime system functions (e.g.
printf) as program regions. REGIONS denotes the resulting set of re-
gions. The region control flow graph (RCFG) has REGIONS as its set of
I EELstands for EnergyEfficiency and Low-power. and RMstands for ResourceManagement. Information
about the EEL laboratorycan be found at htrp.z/www.cs.rutgers .eduz-vuli/eel .

Energy Management a/Virtual Memory
101
nodes, with edges representing the possible control flow between these
regions. The RCFG is similar to the phase control flow graph (PCFG)
introduced by Kennedy and Kremer [14].
2 Initially, data objects are scalar variables and arrays with their declared
sizes. For instance, subcomponents of arrays, such as single rows and
columns in the two-dimensional case, are not considered. For each re-
gion r E REGIONS, two sets of data objects d are determined:
(a) dE MUST -REF(r), if d is referenced during every execution of
region r;
(b) dE MAY-REF(r), if d may be referenced during an execution of
region r;
The MUST -REF sets are used to describe data objects that will be in
memory after the execution of the corresponding region, and MAY..REF
sets are the basis to predict future data object references that may require
swapping over the wireless connection.
3 The data flow problem IN....MEM(r) is solved. For each entry point of a
region r the set of data objects that are in memory is determined. Since
cache policies such as LRU keep track of the sequence of data references
within a finite window of past references, a notion of time or decay has
to be incorporated into the data flow formulation. Initially, we will solve
this problem by a simulation process.
4 Each region r is labeled as yes or 110 depending on whether the region
may require swapping over the wireless connection or not.
if MAY-REF(r) ~ IN....MEM(r)
then no, otherwise yes
4.2
Phase 2 - Code Generation
The compiler inserts calls to runtime routines activate and hibernate. The
effect of these routines are
.
{
system call "card.on"
activate ~
.
no action
{
system call "card.off"
hibernate ~
no action
if card is inactive
if card is active
if card is active
if card is inactive

102
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
The initial approach will place calls to activate and hibernate at region en-
try points. A limited set of reshaping transformations to enable additional op-
timizations will be considered. Performance prediction will be used to move
activate statements up the region control flow graph to program points that
allow the overhead of the activation to be overlapped with program execution.
Performance prediction will also be used to eliminate hibernate statements
that are considered unprofitable due to subsequent activate operations. If the
distance in terms of execution cycles between a hibernate and activate opera-
tion is too close, the benefit of shutting-down the card is lost. A backward-flow,
v-information data flow problem can be used to determine the length of the
minimal activate-free path for any region exit point. Hoisting of activate op-
erations, and elimination of hibernate operations may be done in a combined
analysis pass.
Our initial benefit analysis assumes that the compiler can perform a reshape
optimization called page fault clustering. Assuming that swapping operations
are atomic, i.e., cannot be overlapped, this transformation will not directly im-
pact the overall performance of the program. Page fault clustering is applied
if the memory footprint of a region (MAY-REF(region )) fits into memory.
Prefetch instructions are generated before such regions, allowing all poten-
tial page faults to occur before the execution of the region , leaving the region
free of page faults.
This transformation allows potential hibernation of the
communication card during the entire region execution.
4.3
Performance Model
For each region, the performance model has to report the number of cycles
needed to execute it. In our initial system, symbolic entities such as program
size and loop bounds are assumed to be known at compile time. We will use
a micro-benchmarking approach to determine basic computation and memory
access costs as well as the suspension and activation time of the wireless com-
munication card [3, 20, 15].
At a later point, we will consider parameterized (symbolic) performance
expressions. Our analysis and code generation strategy has to be modified in
order to allow the evaluation of these expressions at runtime, and based on the
results, will execute the guarded activate and hibernate operations.
4.4
Example
In the example program shown in Figure 6.2, we assume a memory size of 4,
8, and 12 pages, a write-allocate paging strategy, and a LRU page replacement
policy. The array size II is set such that each array occupies 4 pages . To sim-
plify the example, scalar variables are ignored, and arrays are assumed to be

Energy Management ofVirtual Memory
float A(n), B(n), C(n)
RI
do i = 1, n
AU) = ...
enddo
R2
do i = 1, n
BU) = ...
enddo
R3 do i = 1, n
CU) = ...
enddo
R4
do i = 1, n
BU) = . .. CU)
enddo
RS
do i = 1 , n
A(i) = ... B(i)
enddo
R6 pr i nt A
Figure 6.2.
Sample code
103
Table 6.1.
Page faults for different memory sizes in terms of pages, assuming that each array
requires 4 pages of memory space.
region
memory size
4
8
12
Rl
miss
miss
miss
R2
miss
miss
miss
R3
miss
miss
miss
R4
miss
no miss
no miss
R5
miss
miss
no miss
R6
miss
no miss
no miss
aligned at page boundaries. Table 6.1 lists the data space page faults expected
to occur for different memory sizes.
Whether a card should be shut down for a region that does not incur a page
fault will depend on the predicted execution times for the region . For example,
if it takes longer to shut down the card than executing regions R4 or R6, then
it is unprofitable to shut down the card for these two regions for the 8 page
memory case. However, for the 12 page memory, shutting down the card will
be profitable.
4.5
Implementation Issues
For our initial implementation, we started with a simple memory access
model to see how closely we approach actual behavior.
In simplifying the
memory access, we assume an entire array will be loaded (used) whenever
there is a reference to it.
By examining the array 's declared size and data

104
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
type, we calculate the number of required memory pages. However, there are
instances where only a single row/column is accessed, or the array is accessed
in a triangular pattern . In such cases, we will need more accurate tools to
analyze memory patterns . We plan to use a modified form of Data Access
Descriptors (DADs) [4,2].
Using DADs can aid our analysis in two ways. First, DADs describe an
iteration order in walking through the dimensions of an array. As pages are
swapped out after a given loop, we may reasonably estimate which pages of an
array remain in memory. For instance, one loop may iterate forward over an
array, while another loop may iterate backward over the same array. It can be
safe to assume the last x pages of the array are still in memory. Secondly, DADs
also help by more accurately indicating the accessed regions of an array. If
only a single row/column is needed, then the array's memory access summary
is given by the necessary page(s), and the overall loop memory block summary
will be more concise.
The current prototype implementation approximates LRU. Our LRU simu-
lation strategy does not consider virtual addresses, but instead uses data and
code access summary information. For each region, a single data structure
describes all data and system calls (printf) referenced in the region. In addi-
tion, the total number of pages needed to store all data and code in memory is
recorded.
A key component for approximating LRU is the notion of age. Along with
summarizing array accesses at a region level, we associate a relative age for
each region. Hence, all array accesses within a region have the same age and
will be replaced at the same time. This is easily represented in a queue, where
each element is the region summary information. In addition, we can remove
elements from anywhere in the queue. For example, if a referenced array is
found somewhere in memory, the containing region is removed and placed at
the end. If a region is larger than the total memory, the net effect is to clear the
contents in memory.
The current implementation computes MAYJ?EF(r) for each region r. In-
stead of computing separate MUST ..REF(r) sets, we set MUST ..REF(r) :=
MAY..REF(r), which is a simplification. The solution to IN-MEM(r) is ap-
proximated by applying the LRU simulation process to nodes in the RCFG,
starting with the entry node, and choosing the next node according to the rPOS-
TORDER numbering [1]. The initial value of IN-MEM(r) is 0. Ifa loop is en-
countered, its entire body is visited twice. The resulting values in IN-MEM(r)
represent the final solution for region r. This process is applied recursively
for nested loops. Our heuristic is motivated by the observation that the stable
state typically occurs after a loop has iterated at least twice. The heuristic may
lead to visiting sequences exponential in the loop nesting depths. However, the
maximal loop nesting depth in a program is typically a small constant. Our

Energy Management of Virtual Memory
Table6.2.
Dynamicpage hit/miss predictionaccuracy.
shal
adi
105
tomcatv
True Hit
True Miss
FalseHit
False Miss
17
9
1
2
62
1
o
o
304
304
2
100
current implementation always picks the most frequently executed branch of a
conditional statement as the only branch that is ever executed.
Although we have used and made several simplifying assumptions, our anal-
ysis is able to predict most page faults correctly. Table 6.2 shows the total num-
ber of correctly predicted hits and misses (True Hit/Miss) as well as incorrect
predictions (False HitlMiss). The page size is set at 4KB. The False Miss count
was significant only in the case of tom catv, The misprediction occurred for a
rather small region, resulting in no impact on the overall energy savings and
performance. Detailed energy and performance results are given in the next
section.
5.
Experiments
We modified the SimpleScalar simulator to keep track of page faults that oc-
cur during the execution of a program. In addition, the simulator logs the cycle
times where program regions such as loops are entered and exited. The simula-
tor allows the assessment of the amount of computation performed for a given
working set, and the resulting potential benefit of suspension and resumption
of the wireless card .
For three different programs, we evaluated the working sets for different
memory and program sizes. Given a particular overhead of the suspend and
resume operation (25,000 CPU cycles), we determined the performance impact
and energy savings of our optimization.
If working sets change frequently, the wireless card should never be sus-
pended. If the working sets are changed very infrequently, both as and com-
piler based approaches wilI lead to similar results.
Compiler techniques are
superior to as techniques in cases where a working set does not change for
a length of time that is comparable to the as based suspension threshold and
on-demand resumption times.
We assume a performance predictor tells us which regions take longer than
the time required for a suspend operation and then use on-demand resume. We
compared the potential energy savings of our compiler techniques vs. as
static inactivity thresholds strategies of varying lengths. Through ACPI, the

106
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Table 6.3.
Benchmark parameters.
Parameters
N
M
shal
32
32
adi
16
16
tomcatv
32
16
OS allows the user to tune threshold levels for various devices. Therefore, we
use thresholds relative to the suspend operation time (suspend overhead) .
From simulation traces, we have a notion of time (cycle counts) for each
benchmark. We also have a correlation of system power consumption by the
WaveLAN card given that earlier measurements show the WaveLAN to con-
sume 40% of total system power (iPAQ + WaveLAN). Hibernation mode re-
duces power consumption to 5%. Therefore, while in hibernation, we consider
total power demands to drop by 1/3, which is a conservative estimate. Trans-
lating this into energy comparisons is merely a summation or integral under
the curve of the power levels across execution time.
5.1
Benchmark Characteristics
In shal, there are few regions which access the same arrays consecutively
across loops. Conversely with adi, each loop uses all arrays; hence there is
one large region to suspend the card after the arrays have been loaded. Finally,
tomcatv reveals more interesting behavior where there are some opportunities
to suspend within a large loop containing several nested loops, yet the entire
loop does not fit into memory. Thus, power management strategies can be used
within each iteration of the outermost loop to save overall energy.
These three benchmarks use two dimensional arrays of size NxN. We chose
sizes of N along with the number of memory pages M that exhibited interesting
behavior. Each memory page is assumed to have 4KB. If M is too large, then
after initial array accesses there will be no more page faults. If M is too small,
arrays may not fit at all, resulting in many page faults. Adjusting N mainly
affects the simulation execution time, therefore we try to keep it small. The
parameters used in these benchmarks are as shown in Table 6.3.
We want to use OS inactivity thresholds relative to the suspend operation,
however we have measured both suspend/resume times to be about l30ms
each, under Linux 2.4 for the H3600 pocketPC, which amounts to about 25
million cycles. Interesting benchmark problem sizes then requires simulation
runs on the order of days. In order to reduce simulation times, our analysis
scales the suspend/resume overhead by a factor of 1000 before calculating po-
tential energy benefits. This allows us to use smaller program and memory
sizes, and therefore shorter simulation times.

Energy Management a/Virtual Memory
107
Table 6.4.
Relative energy consumption of benchmark programs with EELRM energy man-
agement. Energy values are percentages of OS approach. Active WaveLAN card contributes
40% to overall energy budget.
EELRM Energy Results
OS threshold
shal
adi
IOl1Icatv
tomcatv (PFC)
I x
101.0
99.3
126.5
95.3
lOx
100.1
92.6
116.3
87.6
20x
99.7
86.2
104.2
78.5
24x
99.4
30 x
99.7
80.6
98.6
74.3
35 x
99.7
78.1
96.7
72.9
54 x
99.7
69.1
96.7
72.8
00
99.7
71.3
96.7
72.8
5.2
Simulation Results
Table 6.4 shows the effectiveness of our compilation strategy over an operat-
ing system approach which is based on static inactivity thresholds for card sus-
pension. The reported figures assume a 25,000 CPU cycles suspension over-
head. Results for different as threshold values are listed, where each such
value is a multiple of the suspension overhead of the wireless communication
card. The
00 threshold represents the case where the communication card is
always on i.e., is never suspended. Boldface numbers indicate the points at
which longer thresholds have equivalent energy/performance characteristics as
the 00 threshold.
Comparing a range of thresholds reveals subtle results. The optimal static
threshold value varies across different programs, precluding the selection of a
universally optimal value. However, the results point towards smaller thresh-
olds as better. Furthermore, small changes to the threshold, in hopes of tuning
energy and performance, have negligible impact. A slight change may allow
the as to hibernate during an additional region, but possibly at the cost of
incurring a performance penalty at another region. Conversely, adjusting the
threshold to avoid a performance penalty may prohibit the as from hibernat-
ing in another region. An example of this behavior was observed in shal. From
lx - lOx, energy usage vacillated while performance improved marginally.
Overall, the results show that for shal, the as technique and our EEL RM
compiler perform roughly equivalently with little, if any, energy savings. In
cases where compiler and as/hardware techniques perform comparably, the
compiler technique can avoid the overhead in the OS/hardware, leading to
additional energy and performance savings. Our compiler does a better job
against larger thresholds in the adi case due to the fact that it is able to suspend

108
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
tomcatv
,-----
-
-
-
-----------
-
-
-
-
- - ------
--
-
5001
10001
15001
20001
Cycle (xl 000)
25001
30001
Figure 6.3.
Partial view of tomcatv's page fault behavior during execution.
the card earlier. Since there is one large potential region to suspend, the com-
piler's advantage grows linearly with respect to varying the threshold limit.
This results in energy savings of about 30% over the as based technique.
For tomcatv, our compiler does not perform well compared to short as
threshold values. This occurs because of computationally large loops which
contain page faults. Our compiler identifies these page faults and keeps the
card enabled. Therefore, we miss significant opportunities for hibernation.
As mentioned in Section 5.1, tomcatv consists of one primary loop with
several (8) nested loops. Figure 6.3 shows an overview of tomcatv's dynamic
page fault behavior during execution. The page fault behavior was derived
from simulation traces. Around cycle 2500, tomcatv's primary loop begins
executing. Each iteration takes around 2500 cycles, and therefore displays a
very regular memory access pattern. Figure 6.4 presents a zoomed view of one
iteration in the primary, outermost loop. For illustration, the iteration's cycles
and page faults have been normalized to range between cycles 0 and 2500.
Hence, keep in mind that Figure 6.4 represents a single iteration of the primary
loop rather than the very beginning of tomcatv itself. Note that overall cycle
times in both figures are scaled by 1000.
Besides showing when page faults occur, Figure 6.4 shows the cycles spent
in different phases, i.e., nested loops within the outermost loop. The ith loop
is designated on the y-axis.
Loops 3 and 5 are very short, lasting 3 and 5

Energy Management of Virtual Memory
tomcatv
109
-----
+------ --- - ---
..
-+
+-
-
------- -------------- - -- --
-- -- +
8
7
6
3
2
oa
500
1000
1500
Cycle (x1000)
Page Faults --
2000
Loop(i) Execut ion Length ---+---
2500
Figure 6.4.
One iteration of tomcatv's primary, outermost loop. Two sets of data are overlaid
here. Vertical bars indicate a page fault at that cycle. Horizontal lines span the execution region
of Loop(i). i E [1..8]. Loops (3) and (5) are tiny, appearing as single points on this graph.
(thousand) cycles, respectively. Typically, page faults occur near the beginning
of loops. which may be somewhat hard to see in the figure.
From the initial results of missing such hibernation opportunities, we exam-
ined a new transformation called page fault clustering as an enabling optimiza-
tion. By swapping in all necessary data before a region, the compiler can direct
the card to hibernate within the region. In the presence of page fault clustering
(tomcatv (PFC), our approach always does better than the as approach, with
energy savings of up to 27%. In all cases, the compiler based approach reduced
the energy cons umption of all benchmarks as compared to the case without any
power management.
Note that there is an implicit asymptotic limit of the energy savings attain-
able by power managing the wireless card (i.e., shutting down the card im-
mediately after program start and for the entire duration). For the case of a
WaveLAN on iPAQ, the energy savings limit is about 33%. Indeed, results
from adi show our technique reaches 28.7% savings. On the opposite extreme,
we cannot do much for shal, but neither can the as. In programs exhibiting
behavior similar to that of Figure 6.1, tomcatv reveals the potential for more
intelligent power management through those idle periods than the as.

110
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Table 6.5.
Relative performance of benchmark programs under as or EELRM energy man-
agement. Reported values are percentages of 00 threshold -
card always awake.
OS and EELRM Performance Results
OS threshold
shal
adi
tomcatv
Ix
101.3
101.7
105.4
lOx
100.3
100.2
103.0
20x
100.2
100.2
102.9
24 x
100.3
100.2
30x
100.0
100.2
101.0
35x
100.0
100.2
101.5
54x
100.0
103.2
100.0
00
100.0
100.0
100.0
EELRM
100.2
101.7
101.0/103.9 (PFC)
Although the reported results were obtained for small problem and main
memory sizes, we expect the results to scale well if both entities grow pro-
portionally (N 2 oc M). However, if the problem sizes grow faster than the main
memory sizes, enabling transformations such as page fault clustering and index
set splitting will become increasingly important for effective compiler-based
techniques.
Using a power management approach may lead to performance degradation
due to the on-demand resumption penalty of the wireless card. A summary of
the overall performance penalties is given in Table 6.5. The largest penalty we
observed for EEL RM was 3.9% relative to the program performance without
any power management. Overall, the performance penalties can be considered
insignificant.
6.
Future Work
Clearly, additional analysis and experiments plus more advanced techniques
are needed to further validate the effectiveness of our approach. Our current
implementation does not use a performance model to eliminate hibernate state-
ments or perform just-in-time card activation. We are in the process of inte-
grating page fault clustering as an enabling transformation into our compiler.
Related to page fault clustering, we can apply this analysis at larger granulari-
ties for local disk hibernation on portable computers.
We will consider index set splitting as an enabling transformation in cases
where the working set of a region is too large to fit into main memory. This
strategy, along with loop tiling, will also be investigated for caches with decay
capabilities[ 13] as another resource to apply our resource management tech-

Energy Management a/Virtual Memory
111
niques. Cache decay is a method for reducing leakage current by deactivating
cache lines.
We plan to extend our methods to consider explicit file I/O, irregular appli-
cations, and programs with pointer-based data structures. We will investigate
how much improvement over our current approach can be achieved by using
refined DAD-based implementations for MAY -REF, MUST-REF, and solv-
ing the INM EM data flow problem. We also plan to apply our techniques
to non-scientific applications such as voice recognition, image understanding,
and browsers.
7.
Conclusion
Compiler-directed energy management of a wireless communication card
can be an effective strategy as compared to an as based energy management
approach. Simulation results showed energy savings of up to 30% over the as.
For OS inactivity thresholds of lOx - 20x card suspension overhead, energy
savings improvements of up to 21.5% were observed, assuming that page fault
clustering was applied to enable energy optimizations. Not only do these re-
sults show potential energy benefits, but we also wish to emphasize that, even
under adverse conditions, our compiler does not perform significantly worse
than the as. That is, our analysis tries to ensure actual energy savings before
directing the wireless card to hibernate.
Although our intent is to show the benefits and feasibility of compiler tech-
niques, our results also provide an interesting guide for ACPI. The IOx - 20x
threshold listed above corresponds to an idle time range around 2 seconds,
which Li et al. has suggested for local disks [17].
The wireless card sus-
pend/resume operations require much less time than a disk . Therefore our
findings suggest even shorter thresholds can be used for wireless communica-
tion cards. In general, smaller thresholds yielded more energy gains with little
performance delay. This can be understood by noticing that
program execution time » resume overhead.
Our preliminary estimates in eliminating this performance delay by assuming
just-in-time activation by the compiler provide up to an additional 5% energy
savings.
References
[1] A. V. Aho, R. Sethi, and J. Ullman. Compilers: Principles, Techniques,
and Tools. Addison Wesley, Reading, MA, second edition, 1986.
[2] V. Balasundaram. A mechanism for keeping useful internal information
in parallel programming tools : The data access descriptor.
Journal of
Parallel and Distributed Computing, 9(2): 154-170, June 1990.

112
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
[3] V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. A static perfor-
mance estimator to guide data partitioning decisions. In ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming, pages
213-223, Williamsburg, VA, April 1991.
[4] V. Balasundaram and K. Kennedy. A technique for summarizing data ac-
cess and its use in parallelism enhancing transformations. In Proceedings
ofthe SIGPLAN '89 Conference on Programming Language Design and
Implementation, Portland, OR, June 1989.
[5] T. Burd and R. Brodersen. Processor design for portable systems. Journal
ofVLSI Signal Processing, 13(2-3):203-222, 1996.
[6] D. Comer and J. Griffioen. A new design for distributed systems: The
remote memory model.
In Proc. Summer 1990 USENIX Conf., pages
127-126, Anaheim, CA (USA) , 1990.
[7] Compaq
Corp.
iPAQ
H3600
pocketPC
handheld
Pc.
http://www.handhelds.org/Compaq.
[8] Intel Corp., Microsoft Corp., and Toshiba Corp.
ACPI Implementer's
Guide, February 1998.
[9] M. Dahlin, R. Wang, T. Anderson, and D. Patterson. Cooperative caching:
Using remote client memory to improve file system performance.
In
Proc. Symp. on Operating Systems Design and Implementation, pages
267-280, Monterey CA (USA), 1994.
[10] V. Delaluz, M. Kandemir, N. Vijaykrishnan, A. Sivasubramaniam, and
M. 1. Irwin . DRAM energy management using software and hardware
directed power mode control. In Proc. Symp. on High-Performance Com-
puter Architecture, Nuevo Leone, Mexico, 2001.
[11] S. Devadas and S. Malik. A survey of optimization techniques targeting
low power VLSI circuits. In Proceedings ofthe 32th Design Automation
Conference, 1995.
[12] F. Douglis and P. Krishnan. Adaptive disk spin-down policies for mobile
computers. Computing Systems, 8(4):381-413,1995.
[13] S. Kaxiras, Z. Hu, and M. Martonosi. Cache decay : Exploiting genera-
tional behavior to reduce cache leakage power. In Inti. Symp. on Com-
puter Architecture, Goteborg, Sweden, 2001.
[14] K. Kennedy and U. Kremer. Automatic data layout for distributed mem-
ory machines. ACM Transactions on Programming Languages and Sys-
tems (TOPLAS), 20(4):869-916, 1998.
[15] U. Kremer. Fortran RED - a retargetable environment for automatic data
layout. In Eleventh Workshop on Languages and Compilers for Parallel
Computing, Chapel Hill, NC, August 1998.

Energy Management of Virtual Memory
113
[16] U. Kremer, J. Hicks, and J. Rehg. A compilation framework for power
and energy management on mobile computers. In International Work-
shop on Languages and Compilers for Parallel Computing (LCPCO/),
August 2001.
[17] K. Li, R. Kumpf, P. Horton, and T. E. Anderson. A quantitative analy-
sis of disk drive power management in portable computers. In USENIX
Winter, pages 279-291 , 1994.
[18] J. Lorch and A. Smith.
Software strategies for portable computer en-
ergy management. IEEE Personal Communications Magazine, 5(3), June
1998.
[19] E. Macii, M. Pedram, and F. Somenzi. High-level power modeling, es-
timation, and optimization.
IEEE Trans. on Computer Aided Design,
17(11), November 1998.
[20] R. Saavedra-Barrera. CPU Performance Evaluation and Execution Time
Prediction Using Narrow Spectrum Benchmarking.
PhD thesis, u.c.
Berkeley, February 1992. UCB/CSD-92-684.
[21] B. Schilit and D. Duchamp. Adaptive remote paging for mobile comput-
ers. Technical Report CUCS-004-91, Columbia University, 1991.
[22] T. Simunic, L. Benini, P. Glynn, and G. De Micheli.
Dynamic power
management for portable systems.
In Proceedings of the Sixth Annual
International Conference on Mobile Computing and Networking (Mobi-
Com), Boston, MA, August 2000.
[23] Stanford University.
National Compiler Infrastructure (NCI) project,
1998.
Co-funded by NSFIDARPA. Overview available online at
http://www-suif.stanford.edu/suif/NCI/index.html.
[24] G. F. Welch. A survey of power management techniques in mobile com-
puting operating systems. Operating Systems Review, 29(4):47-56,1995.

Chapter
7
PROPAGATING CONSTANTS PAST
SOFTWARE TO HARDWARE PERIPHERALS
ON FIXED-APPLICATION EMBEDDED
SYSTEMS
Greg Stitt
Frank Vahid
Department ofComputer Science and Engineering
University ofCalifornia. Riverside
Abstract:
Many embedded systems include a microprocessor that executes a single
program for the Iif~time of the system. These programs often contain constants
used to initialize control registers in peripheral hardware components. Now
that peripherals are often purchased in intellectual property (core) form and
synthesized along with the microprocessor onto a single chip, new optimization
opportunities exist. We introduce one such optimization, which involves
propagating
the
initialization constants
past
the
microprocessor
to the
peripheral, such that synthesis can further propagate the constants inside the
peripheral core. While constant propagation in synthesis tools is commonly
done, this work illustrates the benefits of recognizing initialization constants
from the software as really being constants for hardware. We describe results
that demonstrate 2-3 times reductions in peripheral size, and 10-30% savings in
power, on several common peripheral examples .
Key words:
Cores, system-on-a-chip, embedded systems, synthesis, low power, constant
propagation, platforms, tuning, intellectual property.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

116
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
1.
Introduction
Embedded system designers are increasingly composing their designs
from pre-designed intellectual-property cores, integrating those cores into a
single chip model as shown in Figure 7.1, and then fabricating a chip [3]. A
core is a description of a system-level component, like a microprocessor,
memory, or peripheral component like a direct-memory access (DMA)
controller or universal asynchronous receiver/transmitters (UART) . Cores
come in three forms. A soft core is a synthesizable hardware description
language (HDL) model. A firm core is a structural HDL model. A hard core
is a technology-specific layout. Many commercial core libraries now exist,
e.g., [4], and core standards are evolving rapidly [9].
Figure 7.1. Core-based embedded system design.
A designer gains many advantages from building a system from standard
cores, such as a standard DMA controller or UART. Most importantly, the
designer gains improved time-to-market due to familiarity with the standard
core and compatibility with development tools. Such standard cores typically
come with parameters [8]. Some are pre-fabrication parameters, which are
set by a designer before synthesis, thus influencing the synthesis results.
Such parameters are typically achieved using generics or constants in a

Propagating Constants Past Software to Hardware Peripherals
117
hardware description language (HDL), but can also be achieved using
module generators, which generate unique HDL models depending on the
parameter selection. For example, a lPEG decompression core might by
synthesizable to have either 12 or 16-bit resolution. Synthesizing for 12-bit
resolution would yield a smaller core.
Other parameters, in contrast, are post-fabrication parameters, set only
after the core has been synthesized. Such parameters' settings are typically
stored in registers or non-volatile memory inside the core. They are more
commonly referred to as software configurable parameters. For example, a
DMA controller will have a base register to indicate the starting address in
memory from which the controller should move data, and a block size
register to indicate the number of words that should be moved. An arbiter
core might have a register whose setting determines whether arbitration uses
a fixed or rotating priority scheme.
We make the observation that an embedded system typically runs a single
program that never changes - the application is fixed. In fact, in many cases
that program cannot be changed, because it may be burned into ROM (using
mask-programmed
ROM)
that
appears
with
the
microprocessor
and
peripherals on a single chip to reduce chip cost, size and power (at the
expense ofless flexibility).
A typical embedded system will execute a boot program upon system
reset,
and this program will, among
other things, set these software
configurable parameters in the system's peripherals, as shown in Figure 7.1.
However, if the embedded system 's program never changes , then those
register values never change during the execution of the embedded system.
For example, a particular embedded system may use a DMA controller to
repeatedly send data directly from an array, of size 48 and starting from
memory location 100, to a display device.
The system's boot program may
set the DMA controller base register to 100, and the block size register to 48.
These values will never change for the life of the embedded system.
Previously,
when
systems
were
built
using
discrete
off-the-shelf
integrated circuits, such software configuration was necessary. However,
since
today's
systems are
being
built
with
cores,
we now
have
an
optimization opportunity that did not previously exist. Specifically, for an
embedded system whose program does not change, the values to which the
software
configurable
peripheral
parameters
are
being
set
are
really
constants. As compiler writers are well aware, constants provide excellent
optimization capability, through
the well-known compiler optimization
known
as
constant
propagation
and
constant
folding[ 1][10].
Such

118
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
propagation consists of replacing a variable holding a constant by the
constant itself. This replacement can result, for example, in branch conditions
that always evaluate to false, resulting in tum in dead code that can then be
eliminated. It can also enable compile-time evaluation of expressions.
Such
dead
code
resulting
from
constant propagation is especially
common
when
propagating
constants
into
subroutines
through
the
subroutine's parameters. While the subroutine may have been designed to
handle a variety of sets of parameters, a particular program may only call the
subroutine with certain constant values for those parameters, resulting in
much dead code in the subroutine.
We can think of a peripheral core as similar to a subroutine, in fact, as a
subroutine that has been implemented using additional hardware. The core
may have been designed to handle a variety of sets of software configurable
parameters. However, a particular program may only use the core with
certain constant values for those parameters, resulting in much dead code in
the core.
We therefore propose a deeper propagation of constants than
performed by compilers.
In particular, we propose to propagate those
constants beyond the microprocessor's program, to the microprocessor's
peripheral cores - essentially propagating those constants all the way to
peripheral hardware. Those constants would then be fed into the synthesis
tool being used to synthesize the cores. The synthesis tool could then perform
constant propagations and dead code elimination during synthesis, where the
code here refers to the core's HDL description. Most commercial synthesis
tools already include such compiler optimizations, but those optimizations
are only applied to the pre-fabrication parameter constants.
We will show
that much benefit would come from enabling the synthesis tool to recognize
the post-fabrication parameter values as constants also.
The end result of such propagation is that the synthesized core will be
optimized for the particular program that is using the core, something we
refer to as architecture tuning [8). By optimized, we mean that the core will
have fewer gates, and consume less power, than a standard version of the
same core. Reducing size is important since such reduction can increase chip
yield and reduce chip cost, and many embedded systems are extremely cost
sensitive, especially those being manufactured in high volumes. Reducing
power is important since many embedded systems operate on batteries or
draw power from very limited sources, and so power reduction is an
important design criterion.
In the following sections, we introduce the concept of propagating
constants past software to hardware peripheral cores. After an introductory

Propagating Constants Past Software to Hardware Peripherals
119
example, we will describe common core parameters that are candidates for
constant propagation, discuss methods for achieving such propagation, and
highlight experiments showing the size and power reductions possible. The
results
motivate future work on developing tools that introduce some
cooperation between the compilers and the synthesis tools being used in
developing a system-on-a-chip from cores.
2.
Example
As a simple illustration of propagating constants to hardware, let us
consider a trivially simple peripheral core that has two parallel ports. Each
port can be configured to be an input port or an output port.
A VHDL
description of part of the core is shown in Figure 7.2(a). The core description
declares a control register coni reg with two bits. The first bit makes port A
an output port when set to 0, and an input port when set to I. Likewise, the
second bit makes port B an input or output port. The VHDL description
begins with initialization of the control register during a reset. Next, it would
describe the synchronous monitoring of the bus for an address corresponding
to the control register, and the writing of the control register in this case -
this code is omitted from the figure. Next, the VHDL description describes
the control logic for the tri-state buffers that implement the port direction
functionality. Finally, other behavior ofthe core would be described.
Synthesis converts this soft core to hardware structure, shown in Figure
7.2(b). Note that logic is generated to handle the bus monitoring and the
control of the four required buffers.
Now, consider the situation where this core is used in an embedded
system and controlled by a microprocessor executing a fixed C program. We
might see the following assembly code embedded in the reset routine of the
C program:
OUT cant_reg,
#"00000010"
Assuming contJeg
is the
address of the
control
register
in
the
microprocessor's I/O address space, then this code would write the constant
"00000010" onto the peripheral bus, resulting in a 0 being written into
cantJeg(O) and a 1 into cantJ eg(1). The peripheral core would thus be
configured with port A as an output port, and port B as an input port. The rest
of the C program would then access these ports appropriately.

120
COMPILERS AND OPERATING SYSTEMSFORLOW POWER
Now, suppose we could somehow propagate the constant "10" into the
VHDL description of the core, before the core were synthesized, letting the
synthesis tool know that contJeg would be written by that constant and only
that constant. If we did this in a way that our synthesis tool could make use
of that information, then the synthesis tool would find much dead code in the
VHDL description. First, the control register would not be needed, since a
constant can be derived directly from power and ground in hardware.
Second, the logic to monitor the bus for the control register address and then
write the register would not be needed. Third, each buffer control signal if
statement would have one branch that was always true and the other always
false. Finally, the reset code of the core would not be needed. After all of this
dead code is eliminated, the synthesis tool would output the structure shown
in Figure 7.2(c). The resulting structure in this case requires less hardware,
and would also consume less power due in part to elimination of the bus
monitoring.
3.
Parameters in Cores
We examined a number of common peripheral cores, and found many
software configurable parameters that could be candidates for constant
propagation. Some
common
peripheral
cores
include
the Intel 8255A
(programmable peripheral interface), the 8237A (DMA controller), and the
M16550A (UART - Universal Asynchronous Receiver-Transmitter).
Figure 7.3 is the block diagram of the 8255A. The 8255A interfaces with
a microprocessor on one side, and provides three configurable ports on the
other side.
Its software configurable parameters include mode of operation,
number of ports in use, and direction of each port (input or output) . These
parameters are set by a microprocessor by writing an 8-bit control word into
a control register in the 8255A.
The
8237A
includes
even
more
software configurable
parameters,
including the number of channels, the type of priority scheme (fixed or
rotating) being used to arbitrate between channels, whether each channel
operates in single transfer mode or block transfer mode, the starting address
and block size for each channel, etc. There are thus several control registers
in the 8237A.
Likewise, the UART's parameters include the baud rate, parity type,
mode of communication, etc.

Propagating Constants Past Software to Hardware Peripherals
signal cont_reg :
if
(con t _ r eg ( 0)
= ' 0' ) then
UNSIGNED(l downto 0) ;
A_out
< =
' 1 ' ;
-- declarations for A,
B and
A_in
< = '0' ;
buffers omitted.
else
process (clk,
reset)
A_in
< =
' 1' ;
begin
A_out
< = '0' ;
if
(reset)
then
e nd if ;
cont_reg = "00" ;
if
(cont_reg(l)
='0 ')
then
A_out
<=
Ill;
A_in
<=
'0 ' ;
B_out
<=
'1' ;
B_out
< =
'II;
B_in
< =
' O' i
B_in
'O' i
end if;
else
if rising edge(clk)
then
B_in
< =
' 1 ' ;
Code to detect write reques t
B_out
< = '0';
from bus to cont_reg,
and
end if ;
to update cont_reg , omitted end if;
-- Other behavior omitted
end process ;
(a)
121
(b)
(c)
Figure
7.2. A simple example of propagating constants to hardware (a) soft core, (b)
synthesized core structure,
(C) synthesized core structure after propagating constants
conueg(O)=O and contregfI)«].

122
COMPILERS AND OPERATING SYSTEMSFORLOW POWER
8255A
To
microprocessor
CPU interface
and control
logic
Figure 7.3. The Intel 8255A parallel peripheral interface.
In general, peripheral cores tend to have several types of configurable
parameters, related to features such as:
-
Size of internal data or external data bus
-
Number of channels or ports
-
Modes of operation
-
I/O direction
-
Rate of data transfer
-
Resolution
Supporting numerous parameters is necessary in order for a peripheral to
be applicable in a variety of systems and thus to sell in large quantities.
While some parameters appear in a core as user-settable constants or
generics, others appear as software configurable control registers. Such
software configurability is used in peripherals for several reasons. One
reason is that, before the advent of cores, software configuration was the only
way to configure a peripheral integrated circuit (IC). A core may thus be
modeling a widely-used standard peripheral that was defined in the time of
K's, such as UART and DMA controller cores. A second reason is that, even
for cores representing new peripherals, the core designer does not know if the
peripheral will be controlled by a microprocessor whose application will not
change. If the core were used in a system whose application did change, then

Propagating Constants Past Software to Hardware Peripherals
123
constant or generic-based parameters would not be appropriate. Thus,
support of software configurable parameters is very common, but results in
extra hardware size as well as power consumption.
4.
Propagating Constants from Software to Hardware
We now describe a method for manually propagating constants across the
software/hardware boundary in a core-based synthesis methodology, and
discuss potential approaches for automating this method. The method is
summarized in Figure 7.4. For each core, the first step is to determine all of
the registers in the core that serve as control registers for the various
parameters listed in the previous section . Next, for each such control register,
we must look for all references to that register in the driving microprocessor
program. If the only access to that register is a write with a constant, and this
write occurs during the reset or boot routines, as is often the case in
embedded systems, then we have a candidate for constant propagation to
peripheral cores. We replace the register's declaration in the core by a
constant declaration. We delete any behavior that involves detecting and
carrying out a write to that register from the peripheral bus. We can then run
the synthesis tool on this modified core. A synthesis tool will then detect and
eliminate the dead code created by the constants we introduced in the model,
and thus result in a simpler synthesized structure . Most modem synthesis
tools
already
carry
out standard compiler optimizations like constant
propagation, constant folding, and dead code elimination.
We can also eliminate the behavior in the microprocessor's program
relating to writing the control register, but this is not always necessary. If we
do choose to leave it, then we must ensure that the lack of a response from
the core is acceptable. If a response is needed, like an acknowledgement,
then we leave such behavior in the core.
The above method has the advantage of being immediately applicable in
any existing core-based design process, without any modification to existing
tools. Of course, the constant propagation across the software/hardware
boundary must be performed manually in the above case. Thus, we describe
a potential approach to automating the method . A big help to such
automation is if a core design framework is being used. Such frameworks,
many of which are commercially available, manage
the retrieval
and
instantiation of cores (e.g., [2][5]). They typically already have support for

124
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
instantiating cores with specific values for constants or generics (a generic is
essentially a parameter whose value must be chosen before instantiation) and
for keeping track of all register address assignments in a system of cores.
Thus,
modifying
such
frameworks
to
handle
software
configurable
parameters can be seen as an extension of an existing method.
for each peripheral core P
for each control register C in P
for each write, W, to C in processor's program
if W consists of a single write, of a constant X,
in a reset or boot routine, then
replace C in P by a constant declaration set to X
delete behavior related to writing C in P
end if
end for
end for
end for
run synthesis as usual
Figure 7.4. Method for propagating constants to peripheral cores.
One approach to automation would be to extend the software compiler to
output a list of external I/O addresses that are assigned a single constant by
the program in a reset or boot routine, along with each address' associated
constant. This requires that the compiler be aware of the location of those
reset or boot routines. Next, each core must have its control registers known
to the core framework - this can be done by the framework developer, or the
framework user, without too much effort. Furthermore, the framework must
know where in the core to find the code that writes the register. Given this
setup, the framework can read the contents of the file output by the software
compiler,
and
for
each
address
the
framework
can
then
replace
the
corresponding register declaration by a constant declaration, and delete write
behavior from the core, before instantiating the core into the design. Then,

Propagating Constants Past Software to Hardware Peripherals
125
synthesis can be run on the instantiated core, and the constants will result in
dead code that can be eliminated.
A second approach is possible, and in fact even simpler than the above. In
particular, we observe that modern core-based frameworks actually generate
the reset or boot code themselves, including the code for initializing
peripherals [5]. In other words, suppose a user wishes to instantiate a DMA
controller into a system already having a microprocessor and memory . The
framework will query the user to ask for the values of software configurable
parameters, like transfer mode, base address and block size. The framework
then generates the necessary driver software on the microprocessor. The
second approach extends the above by having the framework also ask if the
software configurable parameter values will ever change, or if instead they
are in fact constants. If they are constants, then the framework can withhold
generation of the related driver software, and instead directly proceed to
instantiate the core with the corresponding register declaration replaced by a
constant, and with the register-write behavior deleted.
5.
Experiments
We performed several experiments to evaluate the size and power savings
possible by using our method of propagating constants to peripheral cores.
We modeled three popular peripherals as register-transfer level VHDL soft
cores: the 8255A programmable peripheral interface, the 8237A DMA
controller, and the 16550A UART. Each core model is nearly a fully-
functional model. The three soft-core models required 1045, 920 and 1063
lines of VHDL code, respectively. We also obtained a discrete cosine
transform (DCT) core (Free-DCT-L) from http://www.opencores.org, which
consisted of 910 lines of code. We manually modified these models to
eliminate dead code that would have resulted from constant propagation of
the software-configurable parameters described below. We synthesized the
cores twice, once before and once after dead code elimination, using the
Synopsys Design Compiler. Area and power were measured using Synopsys
analysis tools, with power measured while running a suite of test vectors for
each core. Because we wanted to see first-hand the impact of the constant
propagation on the size of the VHDL code, we performed the propagation of
the constants and the dead code elimination manually, so we could measure
the resulting lines of code.

126
COMPILERS AND OPERATING SYSTEMSFOR LOW POWER
5.1
8255A Programmable Peripheral Interface
The 8255A had only one configuration register used for selecting the
modes of various ports. We examined the impact of propagating constants
for three different configurations of this register. ModeO corresponded to a
configuration where port A of the device was used as an output port. Mode 1
corresponded to port A being used as an output port with handshaking I/O.
Mode2 corresponded to port A being used as a bi-directional port with
interrupt I/O. Each situation resulted in a reduction of the number of lines in
the model from 1045 to an average of only 415 lines.
Optimizations from constant propagation are shown in the following
code:
if( cont_reg(4)='1 '
) then
pao <= "ZZZZZZZZ";
paen <= '1';
elsif( cont_reg(4)='O'
) then
paen <= '0';
end if i
In this example, cont_reg represents the control register for the device.
By setting the fourth bit to 1, port A is configured as an input port. Note that
this is accomplished with tri-state buffers that set the output signal, pao, to a
disconnected state. Paen is used to enable port A for input. If we know that
a program will always require port A to be used for output, we can simply
replace the example with an assignment of zero to the paen signal. This will
reduce area by eliminating several tri-state buffers and the logic required to
implement the if statement.
Another example is shown below:
if( cont_reg(7)
= '1'
and
cont_reg(6 downto 5)
"00")
then
a_mode <= A_ 0 i
elsif( cont_reg(7)
= '1'
and
cont_reg(6 downto 5)=
"11"
) then
a_mode <= A_1 ;
elsif( cont_reg(7)
= '1'
and cont_reg(6) = '1'
) then
a_mode <= A_2 ;
end if;

Propagating Constants Past Software to Hardware Peripherals
127
In this example, the control register is being checked to determine the
appropriate mode for port A.
Therefore, if we set the control register to a
constant value, then a_mode is also set to a constant value.
We can then
propagate the a_mode constant into the following code:
case( a mode
) is
when A 0 =>
implements mode 0 for Port A
when A 1
=>
implements mode 1 for Port A
when A 2
=>
implements mode 2 for Port A
end case;
If a_mode is a constant, then two of the when statements will never be
executed and can therefore be eliminated.
Because these
statements
implement much of the functionality of the port, large area savings can be
achieved.
5.2
8237A DMA Controller
The 8237A had several configuration registers, including those that select
the arbitration mode, the number of active channels, and the transfer mode,
base address, and block size of each channel. We examined the situation of
using only a single channel, in single transfer mode. This reduced the model
from 920 to 435 lines.
The following code shows the channel being selected based on the
configuration register, command, and the input drequest:
if
( command (4)
=
'0 I
)
then
if
( drequest(O)
=
'1 '
)
then
channel
<=
0;
elsif
( drequest(l)
, l'
then
channel
<=
1 ;
elsif
( drequest(2)
, l'
then
channel
<= 2;
elsif
( drequest (3)
'1'
then
channel
<= 3;
end if;

128
COMPILERS AND OPERATING SYSTEMSFOR LOW POWER
If the program writes a constant to command and dreqeust, this implies
that only a single channel is being used.
The channel signal is used
frequently in the code, as shown below:
db
<= curr addr(channel) (15 downto 8) ;
a7 4
<= curr_addr(channel) (7 downto 4);
a3_0
<= curr_addr(channel) (3 downto 0);
dack(channel)
<= command (7) ;
This code can be further optimized by propagating the constant value of
channel. Curr_addr is implemented as 4 separate 16-bit registers, one for
each channel.
Therefore, if we are only using one channel, we can
completely remove the three other registers.
This optimization also applies
to other registers in the design, such as those that store the base address for
each channel.
In the entire system, fifteen different 16-bit registers can be
eliminated by this one constant.
5.3
PC16550A DART
The PC16550 also had several configuration registers, including those
that enable transmit and receive, select the interrupt mode, and select the
baud rate. We examined two situations, one where the device was configured
for transmit only at a specific baud rate, and the other where it was
configured for receive only at a specific baud rate. Each reduced the model's
lines of code from 1063 to roughly 625.
Configuring the UART for transmit only allows for the entire process
associated with receiving to be removed from the code. By making the baud
rate a constant value, all registers used to store the baud rate and all logic
required to read the baud rate from the input are removed.
In addition, a
custom counter can be implemented in order to generate the fixed baud rate.
5.4
Free-DCT-L Core
The OCT core had configuration registers for selecting between forward
and inverse OCT, and for selecting among 8, 9,10, or l2-bit resolution. The
basic structure of the OCT core is shown Figure 7.5. Note that the controller

Propagating Constants Past Software to Hardware Peripherals
129
and cyclic register components both have configuration signals as input,
making them obvious candidates for constant propagation.
Serial
Data
Input
Input Buffering
Cyclic Register
Configuration
64K Word
ROM
Multiply
Accumulator( 1)
Controller
Configuration
Input
Control signals for notation of
next input and output data words
Figure 7.5. Block diagram ofDCT core.
Multiply
Accumulator(2)
Parallel
Data Out
Portions of the VHDL code for the controller that deal with control
registers are shown below:
with dctselect select
rows
<= add_tmp2(S downto 3)
when '1',
add_tmp2(2 downto 0)
when others;
with dctselect select
columns
<= add_tmp2(2 downto 0)
when '1',
add_tmp2(5 downto 3)
when others;
The signal dctselect is used to select between inverse and forward DCT. By
converting dctselect into a constant with a value of zero, we can optimize the
code into the following assignments:
rows
columns
<= add_tmp2(2 downto 0);
<= add_tmp2(S downto 3);

"01" then
12 ;
"10" then
11;
9 ;
130
COMPILERS AND OPERA TING SYSTEMS FOR LOW POWER
This change eliminates two multiplexors (used to select between add_tmp2(5
downto 3) and add_tmp2(2 downto 0)) and a small amount of wires.
The controller also contains a finite state machine that depends on the
control registers.
The following code represents a state from the FSM that
reads the mode signal in order to select the resolution:
when 5
=>
if mode = "00" then
state
: = 6 ;
elsif mode
=
state
: =
elsif mode =
state
: =
else
state
:=
end if;
By converting mode to a constant, we can convert this code into a single
assignment to the state variable.
This eliminates the need for the 3
comparisons. In addition, several of the states can only be reached from the
assignments in this control statement.
Therefore, depending on the value of
the constant assigned to mode, we may be able to eliminate states from the
FSM.
This type of optimization might be very difficult for a normal
compiler to make because it would have to prove that a certain region of
code could not be reached .
However,
for a synthesis tool, constant
propagation can be followed by FSM state minimization.
It would not be
hard to detect states that were never reached because there would be no
transitions to these states. Therefore, a synthesis tool could easily remove
unneeded states resulting from constant propagation.
The cyclic register component also uses the same control register, mode,
which selects the resolution.
The code is shown below:
if rising_edge (ck)
then
if rst = '1'
then
internal
<=
(others
=>
'0');
- -
8 bits resolution mode
else mode
= "00" then

Propagating Constants Past Software to Hardware Peripherals
internal{63 downto 1)
<= internal{62 downto 0);
internal{O)
<= din_tmp;
9 bits resolution mode
elsif mode = "01" then
internal{71 downto 1)
<= internal{70 downto 0);
internal{O)
<= din_tmp;
10 bits resolution mode
elsif mode = "10" then
internal{79 downto 1)
<= internal{78 downto 0);
internal{O)
<= din_tmp;
12 bits resolution mode
else
internal{95 downto 1)
<= internal{94 downto 0) ;
internal{O)
<= din_tmp;
end if;
end if;
131
In this example, internal is likely to be synthesized as a 96-bit shift register,
where shifts only occur in specified ranges.
By assigning mode a constant
value of" 11", the code is simplified by requiring only one shifting range of
95 bits, resulting in much smaller hardware.
In our experiments, we tested the configuration of inverse DCT with 12-
bit resolution. This configuration reduced the size of the code from 910 lines
to 867 lines.
5.5
Results
Note that the parameters used in all 'the described examples were not
represented by constants or generics in the VHDL source. Rather, the cores
were designed to be synthesized to support software configuration of these
parameters, as is common.
The size and power data is summarized in Table 7.1. We see that size
after synthesis was reduced by an average of 58%, and power by an average
of 22% . The reason that power is not reduced as much as size is because
many of the gates eliminated through constant propagation were not used
during a core's execution even when present, so didn't consume much

132
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
power.
This can be seen in the UART example, by the removal of logic to
handle receiving. This logic might be a large part of overall area, but if it
isn't being used frequently, it is unlikely to consume much power.
We
believe that the power reductions that do occur result from less switching
activity due to simpler control and datapath switching logic.
For the DCT
core, power savings are actually greater than area savings. This results from
the removal of a small amount of frequently used logic that contributes
greatly to the switching activity of the system.
Table 7.1. Comparisonof cores before and after constant propagation.
Cores
8225A
rnode-O
8225A
mode-l
8225A
mode-2
8237A
single
transfer
PCI6550
Tx
PC 16550
Rx
DCT
Forward
8-bit
Average
savings
Gates,
Original
3069
3069
3069
7276
2503
2503
2295
Gates, with
% size
constant
savings
propagation
834
730/0
918
70%
953
69%
2344
68%
1169
53%
1188
53%
1872
18%
58%
Power,
original
(micro-
watts)
2772
2915
2952
2453
1461
1449
1391
Power, with
constant
propagation
(micro-
watts)
1902
2098
2124
2097
1249
1307
958
%
power
savings
31%
28%
28%
15%
15%
10%
31%
22%

Propagating Constants Past Software to Hardware Peripherals
133
These reductions come of course at the cost of not being able to
reprogram the configurable parameters of the core once the system has been
implemented.
Thus,
if modifying
the
microprocessor's
program
is
a
possibility,
then
propagating
constants
across
the
software/hardware
boundary should either not be done, or should be done only to the extent that
the designer is certain that particular constants won't change. However, as
mentioned earlier, many embedded systems have their programs fixed in
mask-programmed ROM, and thus the configurable parameters could never
have been modified anyway, meaning our approach would have no impact on
flexibility in those cases.
6.
Future Work
A core-based design flow often involves more than a microprocessor and
peripheral cores.
In many cases, co-processors or custom hardware may be
used in order to speed up frequent operations. A common example of this is
adding a floating-point co-processor core to a microprocessor core with only
an integer pipeline.
We have previously studied the effects of moving
frequently executed loops into custom hardware cores, where we are more
concerned with improving energy efficiency as opposed to reducing area.
In some cases, we can apply constant propagation optimizations to these
custom cores.
For example, consider a core that implements a frequently
executed function that takes several parameters as input and returns a value
based on the inputs. If we trace the values of the inputs and can determine
that one of the parameters has the same value a large percentage of the time,
we can create a custom hardware implementation of the function that treats
the parameter as a constant.
This allows us to perform all optimizations
associated with constant propagation, resulting in a smaller, faster, and more
energy efficient core. Of course, since we are implementing the function for
only one value of the inputs, we would either have to create an additional
core to implement the function for all other inputs, or simply execute the
function in software. The latter method would have the additional overhead
of checking the values of the function in software in order to determine
whether the function should be executed in the optimized hardware. We plan
to investigate the benefits of such approaches in the future.

134
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
7.
Conclusions
As core-based design methodologies grow in popularity, cores will be
heavily parameterized
to
increase applicability and
hence
sales.
Pre-
fabrication parameters, specified using HDL generics or constants, can result
in optimized hardware. However, post-fabrication parameters, known as
software configurable parameters, until
now
have
not
been
exploited
similarly. We introduced the idea of propagating constants beyond the
microprocessor software, to the peripheral hardware. We showed that such
propagation yielded reductions in size by 58%, and good power reductions of
between 10-30%, using several standard peripheral examples. This work is
part of the VCR Dalton project, which seeks to develop techniques for
parameterized core-based system-on-a-chip design [7]. This work motivates
the need for future work on system-on-a-chip frameworks whose compilers
are able to detect "constants" in the sense of software configurable register
values, and are able to coordinate between compilers and synthesis tools to
propagate those constants to hardware.
Acknowledgments
This work was supported by the National Science Foundation under grant
number CCR-9876006.
We would
like to thank Rilesh Patel for his
contribution to this work.
References
[I]
Aho, A.V., R. Sethi, J.D. Ullman . "Compilers: Principles Techniques, and Tools,"
Reading, Addison-Wesley Publishing Company, March 1998.
[2J
Escalade Corporation, http://www.escalade.com!.
[3]
Gupta, R., and Y. Zorian. Introducing Core-Based System Design. IEEE Design
& Test, Vol. 14, No.4, Oct-Dec 1997, pp. 15-25.
[4]
Inventra core library, Mentor Graphics, http.r/www .mentor.com/inventra/,
[5J
Platform Express . Mentor Graphics, http://www .mentor.com!soc/platform_exl.
[6]
Stitt, G., F. Vahid, T. Givargis, and R. Lysecky . A First-step Towards an
Architecture Tuning Methodology for Low Power. Compilers, Architectures, and
Synthesis for Embedded Systems (CASES'OO), November 2000, pp. 187-192.
[7J
The UCR Dalton project: http://www .cs.ucr.edu/-dalton.

Propagating Constants Past Software to Hardware Peripherals
135
[8)
Vahid, F., and T. Givargis. Platform Tuning for Embedded Systems Design. IEEE
Computer, Vol. 34, No.3, March 2001, pp. 112-114.
[9]
Virtual Socket Interface Association, Architecture Document, http://www .vsi.org,
1997.
[10]
Wegman, M., and F.K. Zadeck. Constant Propagation with Conditional Branches.
ACM Transactions on Programming Languages and Systems, Vol 18, No 2, April
1991, pp. 181-210.

Chapter 8
CONSTRUCTIVE TIMING VIOLATION
FOR IMPROVING ENERGY EFFICIENCY
Toshinori Sato
Department ofArtificial Intelligence
Kyushu lnstitute ofTechnology, Japan
tsato@aLkyutech.ac.jp
Itsujiro Arita
Department ofArtificial Intelligence
Kyushu Institute ofTechnology, Japan
arita@aLkyutech.ac.jp
Abstract
A novel technique for improving the energy efficiency of microprocessors is
disclosed. This new method relies on a fault-tolerance mechanism for timing
violations, based on a speculative execution technique. Since power reduces
quadratically with supply voltage, supply voltage reductions can result in sub-
stantialpowersavings. However, thesereductionsalsocause a longergatedelay,
and so the clock frequency mustbe reducedso that timingconstraintsof critical
paths are not violated. If any fault-tolerance mechanism is providedfor timing
faults, it is not necessary to maintain the constraints. From these observations,
we proposea fault-tolerancetechniquefor timing violations, that efficiently uti-
lizes the speculative executionmechanism and reducespowerconsumption. We
call the techniqueconstructive timing violation. The presentstudyevaluatedour
proposal regarding this technique using a cycle-by-cycle simulator and deter-
mined the technique's efficiency regarding energyconsumption.
Keywords:
Low powerdesign, energy efficiency, timing constraints, fault tolerance, specu-
lativeexecution.
1.
Introduction
The increasing popularity of portable and mobile computer platforms such
as laptop pes and smart cell phones is a driving force in investigatation of
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

138
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
high-performance and power-efficient microprocessors. For example, Java-2
MicroEdition (12ME) works on cell phones [11], allowing users to download
several applications such as 3D-animated games and play them on their cell
phones. Guides for travellers and flight ticket reservations are also available,
and mobile banking and trading are provided. Therefore, it is reqiured that
embedded processors have high performance, and their designers have begun
to include features that are traditionally found in general purpose processors
[19]. For example, modem embedded microprocessors support out-of-order
execution [12]. As the computing power of microprocessors for mobile devices
increases, however, their power consumption also increases. In addition, while
power is already a major design constraint in the area of mobile and embedded
computer platforms, it has also become a limiting issue in general-purpose
microprocessors.
The active power Pactive and gate delay tpd of a CMOS circuit are given by
(8.1)
(8.2)
Vdd
tpd oc
,
(Vdd - VriJ)U
where f is the clock frequency, Cload the load capacitance, Vdd the supply volt-
age, and Vrh the threshold voltage of the device. a is a factor dependent upon
the carrier velocity saturation and is approximately 1.3-1.5 in advanced MOS-
FETs [7]. Based on Eq.(8.1), it can easily be found that a power-supply reduc-
tion is the most effective way to lower power consumption. However, Eq.(8.2)
tells us that reductions in the supply voltage increase gate delay, resulting in
a slower clock frequency, and thus diminishing the computing performance of
the microprocessor.
It is possible to not degrade computing power by maintaining clock fre-
quency before and after reductions in supply voltage. This approach causes
timing violations, however, resulting in logic errors. If a fault-tolerance mech-
anism is provided for the violations, however, the logic errors can be avoided.
In other words, to improve energy efficiency we propose that timing constraints
not be met and that violations be tolerated [17]. We call this technique con-
structive timing violation (CTV) and have already applied it to boosting com-
puting power. In this chapter, we evaluate this new technique with regard to
the research area of low-power design.
This chapter does not discuss the practicality of CTV,focusing instead on its
potential for improving energy efficiency. The rest of this chapter is organized
as follows.
Section 2 explains how to reduce power consumption. Section
3 describes our evaluation methodology, and Section 4 discusses our results.
Section 5 surveys related work. Finally, Section 6 presents our conclusions.

Constructive Timing Violation for Improving Energy Efficiency
2.
Low Power via Fault-Tolerance
139
(8.3)
Our proposal is based on a kind of paraIlelism, that is a space redundancy.
Circuits consist of a main part and checker parts . The main part is responsible
for computing power -high throughput and low latency-, but it can suffer
timing violations. In contrast, the checker parts, which are free of timing vi-
olations, support the main part and revert the processor state to a safe point
where a timing violation is detected. The main and checker parts are equiva-
lent in design, but their distribution of clock frequencies is different. For the
main part, we provide a clock frequency that is higher than that decided by the
critical path, since it is expected that a typical circuit delay is shorter than the
critical delay and that timing faults rarely occur [9]. For example, it has been
reported that nearly 80% of paths have delays of half the critical time [21].
On the other hand, since the checker parts are used for detecting the timing
violations, they work at a frequency that meets the critical delay. This design
technique exploits the fact that the longest path for an individual operation of
a logic circuit is generally much shorter than the critical path of the circuit.
Furthermore, it utilizes the input signals, which determine the critical path, be-
ing limited to a few variations. Considering the characteristics of logic circuits
and their critical paths, the circuits could be designed as the longest path de-
termined by most of the operations, for a function is shorter than the expected
cycle time. In order to maintain throughput, the checker parts are duplicated
if necessary. Please note that the main part is essential for maintaining low la-
tency and that the checker parts only maintain throughput. If there are serious
dependencies between instructions, high throughput also cannot be maintained
without the main part.
The power reduction is achieved as follows . It is assumed that the main
part is violation-free when the Vdd voltage is supplied. Based on Eq.(8 .2), its
maximum clock frequency idd is as follows.
(\I.
V.) 1.3-1.5
I'
dd -
Ih
Jdd 0::
Vdd
For easy understanding, Eq.(8.3) is simplified as
(8.4)
without loss of generality: i.e., it is assumed that a is 2. On the surfice this
simplification is fairly impractical, especially when Vdd is small.
However,
it is already known that tpd is not sensitive to Vrh scaling if Vrh is scaled to-
gether with Vdd [5]. We expect that this joint scaling will be possible with fu-
ture technologies such as multithreshold-voltage CMOS [22] and the variable-
threshold-voltage scheme [10]. In addition, Vdd reduction in deep-submicron
CMOS is not as deterimental to speed as in long-channel CMOS [5]. To re-
duce power consumption, we would like to supply a voltage VL lower than Vdd .

140
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
This is possible by using on-chip DC/DC converters [16]. The clock frequency
should usually be reduced to fL, as determined by VL, but we keep it as fdd'
The checker parts are used to detect timing violations and thus they work at
a frequency fL with the supply voltage VL . That is, they are timing-violation
free. If the power reduction due to the lower supply voltage is larger than
the increase in power consumption caused by the amount of parallelism, the
proposed technique can efficiently decrease power consumption.
We will explain our proposal using an example. However, this proposal is
applicable to any combinational logics, including datapaths and control log-
ics. The only stipulation is that latches should meet timing constraints. In
other words, timing violations should not occur in latches when working at a
higher clock that is distributed to the main part. Figure 8.1 depicts an ALU
utilizing the proposed technique. While it is true that cases caches are critical
in many and determine clock frequency of a microprocessor, it has been re-
ported that adders and instruction-scheduling mechanisms become bottlenecks
limiting the increase in clock frequency [14, 15]. For example, Compaq's Al-
pha 21264 processor splits execution pipes into two clusters to meet timing
constraints, while it has a data cache operating at twice the frequency of the
processor clock [8]. Thus, we believe that ALU is suitable for explaining our
proposal because of its simplicity. The shaded box shows additional circuits
for the checker parts . It is assumed that the ALU currently executes at the
clock frequency /dd with supply voltage Vdd, and it is expected that the supply
voltage is reduced to VL , say VL = Yr. First, ALU is duplicated three times.
One ALU, which we call the main ALU, works at fdd with VL and the remain-
ing two ALUs, which we call checker ALUs, work at!L = !ft with VL. Thus,
the checker ALUs are free from timing violations and are used to verify oper-
ation of the main ALU . Please remember that the main ALU is essential for
maintaining low latency and that the checker ALUs only maintain throughput.
If there are serious dependencies between instructions, high throughput cannot
be maintained without the main ALU. Using this technique, the ALU's power
consumption is reduced as follows. The power in the main ALU is reduced
2
v.
2
J. C
v2
from fddqoadVdd to fddCload (~)
=
dd
I~ad a; The power in the two checker
ALUs is 2 * !ftqoad(Yr)2 = fddCt~ad V1 . Thus, total power consumption is re-
duced from fddqoadVJd to fddC''2adV1 . That is, it is reduced to ideally by half.
While this represents a rather ideal case , the power reduction is realistic and
attractive when we use typical values of 1.3-1 .5 for a. The aim of this estimate
is to demonstrate CTV potential.
It should be explained again that this approach is applicable not only to a
datapath such as ALU but also to any combinational logics. One of the ap-
plications of this approach for the control path is the logic that detects data
dependencies between instructions. This logic is on one of the critical paths in

Constructive Timing Violation for Improving Energy Efficien cy
opl-~~~
Figure8.1.
ALU utilizing proposed technique
141
in-order issue microprocessors. Assuming this logic is asserted and is critical
only when there are dependencies, the crv mechanism can relieve timing con-
straints so as not to cause timing violations only if there are no dependencies
between instructions.
Clock signals distributed to the ALUs are shown in Figure 8.2. Because the
clock signals of the checker ALUs are complementary, they work alternatively
to verify the main ALU. Figure 8.2 explains how two consecutive operations
start and are verified. The verification is based on comparing two outputs from
the main ALU and one of the corresponding checker ALUs.
If the outputs
do not match, a timing violation is detected. In such cases, a recovery action
should be initiated. Since the comparators should be violation-free, they will
work at a slower clock frequency Ii,
In order to revert the processor state
to a safe point where the error is detected, we propose utilization of a recov-
ery mechanism used in modem microprocessors for speculative execution. In
other words, an instruction's timing violation is regarded as a misspeculated
instruction. Thus, there is little hardware overhead in the recovery mechanism.

142
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
fdd/2
if #1
!:.:
vefli.:
-
l.-
I--
"'--1--
I--
,..--~
....-
....-
-L.-
-
-
L...-
l...-
I.--
L.-
-
-
L.-
L.-
sta t #1
fdd
fdd/2
start #2
verify #2
Figure 8.2.
Clock signals
We consider two mechanisms for the recovery action. One uses the exist-
ing speculation recovery mechanism for mispredicted branches, and the other
is based on the instruction-reissue mechanism for incorrect data speculation
[13, 18]. Its limited mechanism has already been included in modern micro-
processors [6]. Note that the correct value is provided by the checker ALUs,
and thus instruction retry is successful for recovery from timing faults. That
is, when a fault is detected, it is sufficient to re-execute instructions following
the fault instruction. In the case that the recovery mechanism for mispredicted
branches is utilized, the microprocessor flushes its pipeline when a timing fault
is detected and then restarts at the corresponding instruction. All instructions
following the fault instruction are squashed, and thus the penalty might be very
large.
We call this recovery mechanism instruction-squashing. In contrast,
when using instruction-reissue, only instructions dependent upon the fault in-
struction are selectively invalidated and re-executed. Hence. little performance
loss is expected. Explaining the process of instruction-reissue is beyond the
scope of this chapter and can be found in [18] .
Based on these considerations, it is possible to detect timing faults and to
tolerate violations.

Constructive Timing Violation for Improving Energy Efficiency
143
3.
Evaluation Methodology
We implemented a timing simulator using the SimpleScalar/Alpha tool set
(ver.3.0a) [2]. The baseline model is an out-of-order execution superscalar
processor based on the register update unit [20], and its configuration is sum-
marized in Table 8.1.
Table 8.1.
Processor configuration
Fetch Width
Branch Predictor
1nsn. Windows
Issue Width
Commit Width
Functional Units
Latency
(total/issue)
Register Files
Insn. Cache
Data Cache
L2 Cache
4 instructions
512-set, 4-way set-associative BTB, 2048-entry bimodal
predictor, updated in commit stage, 8-entry return address
stack, 3-cycle miss penalty
l6-entry instruction queue, 8-entry loadlstore queue
4 instructions
4 instructions
4 iALU's, I iMUL/DlV, 2 LdlSt's, 4 fALU's, I fMUL/DIV
iALU Ill, iMUL 3/1, iDiV 20/19, LdlSt 2/1,
fADD 2/1, fMUL 4/1, IDIV 12/12
32 32-bit fixed point registers, 32 32-bit floating point registers
16K direct-mapped, 32-byte blocks, 6-cycle miss penalty
16K 4-way set-associative, 32-byte blocks, 2-port, write-back,
non-blocking load, hit under miss, 6-cycle miss penalty
unified, 256K 4-way set-associative, 64-byte blocks, 48-cycle
miss penalty
The SPEC2000 CINT benchmark suite is used for this study. Table 8.2 lists
the benchmarks and the input sets. We use the object files provided by the
University of Michigan. They were compiled by DEC C V5.9-008 on Digi-
tal UNIX V4.0 (Rev.1229). For each program except for 252. eon, 1 billion
instructions are skipped before the actual simulation begins. Each program is
executed to completion or for 100 million instructions. We do not count nop
instructions.
4.
Simulation Results
In this section, we present preliminary results obtained with the approach
described in Section 2. Note that the technique should be applied to every
element that likely violates timing constraints. As explained above, the goal of
this chapter is not to prove that the CTV is practical, but to present its potential
with regard to improving energy-efficiency.
When we use one main part and two checker parts, the clock frequencies !L
should satisfy the following condition.
12fdd 5: fL < Is«.

144
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
+
+- ..
... •+-
+
+
...
... ..... ... + .... +-
... +
- --
- --
- --
0.5
c:
alpha=2.0 -
o
alpha=1.5 .... .
~
1.5
Ipha=1.3 -0- .
'"en
c:8
~
CI>c:
W
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Faull probability
Figure8.3.
Energy consumption (Squash) 164.gzip
.+ . ...... -+•• + ......... + ........
... .....
... + .....+-
... + ....... + ... . +-
0.5
c:
alpha=2.0 ...-
.Q
alpha=1.5 .... .
~
1.5
Ipha=1.3 -0-.
'"en
c:8
2>
2!w
olllo-o'-'G........la....=.....m=-=-JlL.::~=-=-JL::c..::IlI::..=...a=..=IIl:...=..JIL=...::Y!:..::...:w.=...=.!l!:..=..::!I!.::.~
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
Figure8.4.
Energy consumption (Squash) 175.vpr
+
+
+
-+- •
.. .....
... +
... .+..... ..
... ..~ .. + .........
...
0.5
alpha=2.0 -
c:
alpha=1.5 .... .
.g
1.5
Ipha=1.3 -0- .
a.E
'"
~8
2>
CI>c:
W
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
O......'-'""'''-=-'~__-=..II~''''''-.=....III-=-=~::...A=-=sl>:..:'-'II..::...::JOr:...:::...1ICc.=Jllt...::..:lIl::...=.JOI
o
Figure 8.5.
Energy consumption (Squash) 176.gcc

<:o'li
1.5
E"
III<:8,.,
!:?'
<l>
~
0.5
Constructive Timing Violationfor Improving Energy Efficiency
.. _.+ .. ...,_.+ ..
--too
..
-+_ .. + ...+_ ... + ... +-0 .. +-- .~ ....... + ..
O~.....~a....a""c.=JIL=...:IIt:..:..JIlL::....::III::...;::...a.::....:lw::..:::...JII.:::....=JII:....=....JIL:~~""-=-~
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
Figure 8.6.
Energy consumption (Squash) 186.crafty
145
s
''5.
1.5
E"
III<:8,.,
!:?'
~w
0.5
-+ .... + ....... .. + ...... ...
.. -+- .. + .. -+- .. + .. ....... + .......... -+.... +- •
O~""'-......la-s.""c.=JIL=...:IIt:..:..JIlL::....::III::...;::...a.::....:lw::..:::...JII.:::....=JII:....=....JIL:~~""-=-~
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
Figure 8.7.
Energy consumption (Squash) 197.parser
2,---,---.--.----r-,--,..--.----r---,-..,.---,.-...----r-,---,
<:o
.~
1.5
"
~8
E>
<l>
ill
0.5
.. + ... ~ .. + .... +- .. -+.... + ......... + .. 00+ -
..
.. _+_ .. + .. _+_ .. + ... +-0
O [l...o....=--lIl.-=~"-=-a...=~=-=-__=....:m=-=-__=...::lIt:...::...III..=..=III:...::..3I!..::..=.II!:...=...::!I!::.=_'I'
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Faull probabil ity
Figure 8.8.
Energy consumption (Squash) 252.eon

146
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
Table8.2.
Benchmark programs
program
164.gzip
175.vpr
176.gcc
186.crafty
197.parser
252.eon
255.vortex
256.bzip2
eo
~
1.5
"1!!8
~
ffi
0.5
inputset
input.compressed
net.in arch.in
cccp.i
crafty.in
test.in
chair
lendian.raw
input.random
.. + ....... .. + .... ~ ......... + .. .,.- .. + .. -+- ..
.. ~... + .. -+" .. + ........
olllo-oa..olll"'-"......'-.'IIli::....=.IIL.::...:lIIi::....=....,,:..::Ill~.a.:=-=t:...=..JU..::'-=Mr....::...JU..::c..::JI<....::...:!IC..=.!"
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
Figure8.9.
Energy consumption (Squash)255.vortex
3~-.--..---.--..---.--~--r--~--r--~--r-~--r-~.....,
§
2.5
Ipha=2.0-
''5.
alpha=1.5 -+- .
§
2 alpha=1.3 -0- '
'"
c8
1.5
>-
e'
Q)cW
0.5
O........-.ollI----.l..........cD=-'"-'I~:III::-.:..JIIl.=....:iIl:..:::...JII.=IIC..:~~O:""::'..w..:~<....::..=..:.:.:r'
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
%Fault probability
Figure8./0.
Energy consumption (Squash) 256.bzip2
Thus, we choose a clock frequency of ifdd for the checker parts, as it results
in the greatest power reduction efficiency. Please note that this is an ideal
case, and practically, the efficiency will be less than in this case. In order to
evaluate how timing violations affect processor performance, we have the vi-

Constructive Timing Violation for Improving Energy Efficiency
147
olations occur randomly. We vary the probability that timing violations will
occur between 0 and 30% of operations, and then measure energy consump-
tion. We call the probability the fault probability. Please note that in practice
there must be a correlation between the fault probability and [i. Determining
the optimal tradeoff point remains as a task for future study. Figures 8.3 -
8.10 present the energy consumption when instruction-squashing is used. It is
calculated by multiplying the active power by the number of execution cycles,
and is normalized based on the baseline model. Even when a equals 2.0, the
energy consumption is substantially reduced if the fault probability is less than
approximately 15%. The rising energy consumption is caused by increasing
the cycle time due to recovery from timing errors . Since instruction-squashing
involves a long miss penalty for the recovery, a high fault probability signifi-
cantly diminishes the power efficiency. This loss can be mitigated by a small
miss penalty in instruction-reissue. If we use typical value of 1.3-1.5 for a, we
can observe that the energy consumption is significantly reduced even when
the fault probability reaches 30%.
Figures 8.11 - 8.18 present energy consumption when instruction-reissue
is used. Only the case where a equals 2.0 is shown. As can be easily seen,
the energy reduction of 50% is maintained across almost all the fault frequen-
cies evaluated. While this technique requires the slightly high-cost instruction-
reissue mechanism, it will likely be implemented in future microprocessors.
Thus, it is applicable for mobile pes to utilize this fault-tolerance technique
for reducing energy consumption.
5.
Related Work
Kondo et al. [9] have proposed a Variable Latency Pipeline (VLP) structure
for integer ALUs. When properly using two kinds of circuits according to the
longest circuit path for each operation, the effective execution latency can be
almost one cycle. Our proposal is strongly influenced by the VLP, even though
Kondo et al. are not at all interested in fault-tolerance or power-efficiency. In
addition, our proposal is applicable not only to ALUs, but also to any combi-
national logics. Using the SPEC92 CINT benchmark suite, the usefulness of
the VLP can be evaluated on a platform of an in-order execution scalar pro-
cessor, but with dynamically scheduled superscalar processors. In this chapter,
we made our evaluation on the 4-way dynamically-scheduled superscalar pro-
cessor.
Chandrakasan et al. [3] have utilized a parallel architecture to reduce supply
voltage by maintaining throughput. With their method, two identical circuits
are used to make each unit work at half the original frequency while main-
taining the original throughput. Because the speed requirement for the circuit
is halved, the supply voltage can be decreased. In this case, the amount of

148
COMPILERS AND OPERATING SYSTEMSFOR LOW POWER
1.0
~--r-"-""""T--r----r-""T"""--'-"'T""--,....--.--r--.--..--""""---'
cg
~
:l
l:'!
!-_-.....------..-.....-------.....----~
80.5
e>
~w
0l---L---Jl--....L..----I_....L..----I_...l----l_..L.----I._..L.-.......L_.J.____L.--I
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.11.
Energy consumption (Reissue) 164.gzip
1.0 ...-----..--r----..---.--..............--,--...---.--...----r--.--,..........,..-,
c
.Q
~
:l
~0.5l-__----+--+----<-------e--------+-"""j
>-
e>
Q)c
W
O'---'---''---'-........_...L-........_~........_.J._.......L_.J.____L._.L__-L--'
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.12.
Energy consumption(Reissue) 175.vpr
1.0 ...--.--r-.....,..-..----r--r-""""T--r----r-.....---,....--.--..--.-.........
co'a
E
:l
l:'!
80.5 ~----------------~----
......--j
>-
e>
Q)c
W
OL.--l-_..L..----I._..L.----l_....L..-..I_..J....----Il--....L_L--L_..L..--l-----J
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.13.
Energyconsumption (Reissue) 176.gcc

Constructive Timing Violation for Improving Energy Efficiency
c
.Q
Q.
E
~
80.5L---.....------..-.....-------....----~
~
QlcUJ
01--...J.._'--...J...---J1..-...J...---J1..-...L---I_...L.---1_...L.--.L_...I---'---l
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.14.
Energy consumption (Reissue) 186.crafty
c:o
.~
"
Ul80.5 L--
-
~~
-
- ___:f
~
Qlc:
UJ
0L-......L._......--l_-'-_L-....J-_-'---'_....L-_.l---L_...L.----JL-..--1..---J
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.15.
Energy consumption (Reissue) 197.parser
c:
.S!
15.E
~
L__.....- __-_-..-....------------j
80.5
~
Qlc:
UJ
O'---'--"'-.....L.---JI....---'----JI....---L..--'_...J-.--1_...J-.-.L_...I---'---l
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure8.16.
Energy consumption (Reissue) 252.eon
149

150
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
co
'~
~c
L-+-_-_-+-~~-_---<
-
-_~
80.5
>-e'
Q)c
W
0l.-....L---L_...L...-L_.l.-.....l-.---l_-l---L_.1..-.........--J1.-.-1----L---J
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probability
Figure 8.17.
Energy consumption (Reissue) 255.vortex
co''5,
E
:>
~ L
----<---........--+----<---........-----<..---+--J
80.5
e>
Q)c
W
0l-...L..---1_...l.--L_.l.-.....l-.----ll.-....L.----L_.1..-.........--J1.-.-1----L---J
o
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
Fault probabil ity
Figure 8.18.
Energy consumption (Reissue) 256.bzip2
parallelism can be increased to further reduce total power consumption. Our
proposal also utilize parallelism. However, we maintain not only throughput
but also the latency of the applied element. Thus, our method is efficient even
for irregular applications with little data parallelism.
DIVA [1] is an example of a fault-tolerant microprocessor based on space
redundancy. A simple checker processor is used to dynamically verify com-
mitted instructions. Any hardware faults are corrected using a recovery mecha-
nism for incorrect branch predictions. Hence, DIVA is a hardware-based mech-
anism and is transparent. However, DIVA requires additional ports for register
files and caches so that the checker processor can share processor contexts,
thus increasing the design complexity and circuit delay of the main processor.
While reducing hardware complexity is investigated in [4], power-efficiency is
not considered.
Instruction-reissue is a technique for recovering the processor state when a
misspeculation occurs . It selectively invalidates instructions dependent upon

Constructive Timing Violation for Improving Energy Efficiency
151
the misspeculated instruction and then reissues them in the instruction window.
Lipasti et al. [13] introduced the instruction reissue concept. Instructions de-
pendent upon a predicted instruction are forced to be retained in the reservation
stations. When the predicted instruction produces an actual value, the predicted
value must be compared with the actual one.
If they match, the prediction
is correct and the dependent instructions release the reservation stations. If
the prediction fails, all dependent instructions are concurrently invalidated and
reissued. However, Lipasti et al. proposed only the concept of the instruction
reissue. They do not present any practical implementation of the scheme. If
the scheme were implemented, the processor cycle time would increase since
it is very difficult to find in parallel all dependent instructions using moder-
ate hardware cost. We proposed a practical implementation of the instruction
reissue in [18].
6.
Conclusion and Future Work
Inthis chapter, we have proposed a fault-tolerance technique to improve the
energy efficiency of microprocessors, calling the technique constructive timing
violation. Our preliminary evaluation shows that the proposed mechanism is
energy-efficient under the condition that timing errors occur infrequently.
One necessary future study will be the modeling relationship between clock
frequency and fault probability. This model is useful for understanding the
effectiveness of the proposed technique in the real world. Another future di-
rection is reducing hardware overhead when introducing fault-tolerance. One
possible solution is the sharing of circuits between main and checker parts by
pipelining the element using transparent latches, thus eliminating the duplica-
tion of each element and considerably reducing the hardware budget.
Acknowledgments
This work is supported in part by a research grant from Toshiba Corporation.
References
[1] T.M. Austin, "DIVA: a reliable substrate for deep submicron microar-
chitecture design," 32nd International Symposium on Microarchitecture,
1999.
[2] D. Burger and T.M. Austin, "The SimpleScalar tool set, version 2.0,"
ACM SIGARCH Computer Architecture News, vol.25, no.3, 1997.
[3] A.P. Chandrakasan and R.W. Brodersen, "Minimizing power consump-
tion in digital CMOS circuits," Proceedings of IEEE, vo1.83, no.4, 1995.
[4] S. Chatterjee, C. Weaver and T. Austin, "Efficient checker processor de-
sign," 33rd International Symposium on Microarchitecture, 2000.

152
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
[5] K. Chen and C. Hu, "Performance and Vdd scaling in deep submicrometer
CMOS," IEEE Journal of Solid-State Circuits, vo1.33, no.10, 1998.
[6] G. Hinton, D. Sager, M. Upton , D. Boggs, D. Carmean, A. Kyker and P.
Roussel, "The microarchitecture of the Pentium 4 processor," Intel Tech-
nical Journal, issue Q1, 2001.
[7] T. Hiramoto and M. Takamiya, "Low power and low voltage MOSFETs
with variable threshold voltage controlled by back-bias," IEICE Transac-
tions on Electronics, vol.E83-C, no.2, 2000.
[8] R.E. Kessler, E.J. McLellan and D.A. Webb, "The Alpha 21264 micro-
processor architecture," International Conference on Computer Design,
1998.
[9] Y. Kondo, N. Ikumi, K. Ueno, 1. Mori and M. Hirano, "An early-
completion-detecting ALU for a IGHz 64b datapath," International Solid
State Circuit Conference, 1997.
[10) T. Kuroda, T. Fujita, S. Mita, T. Nagamatsu, S. Yoshioka, F. Sano, M.
Norishima, M. Murota, M. Kato, M. Kinugasa, M. Kakumu and T. Saku-
rai, "A 0.9V, 150MHz, 1OmW, 4mm 2, 2-D discrete cosine transform core
processor with variable-threshold-voltage scheme," International Solid
State Circuit Conference, 1996.
[11] M. Levy, "Java to go: part I," Microprocessor Report , vol.l5, archive 2,
2001.
[12) M. Levy, "NEC processor goes out of order", Microprocessor Report,
vol.l5, archive 9, 2001.
[13) M.H. Lipasti, C.B. Wilkerson and J.P. Shen, "Value locality and load
value prediction," International Conference on Architectural Support for
Programming Languages and Operating Systems VII, 1996.
[14] T. Liu and S.-L. Lu, "Performance improvement with circuit-level spec-
ulation," 33rd International Symposium on Microarchitecture, 2000.
[15] S. Palacharla, N.P. Jouppi and J.E. Smith, "Complexity-effective super-
scalar processors," 24th International Symposium on Computer Architec-
ture, 1997.
[16] S. Sakiyama, J. Kajiwara, M. Kinoshita, K. Satomi , K. Ohtani and A.
Matsuzawa, "An on-chip high-efficiency and low-noise DCIDC converter
using divided switches with current control technique," International
Solid-State Circuits Conference, 1999.
[17] T. Sato and 1. Arita, "Give up meeting timing constraints, but tolerate
violations," 4th International Symposium on Low-Power and High-Speed
Chips, 2001.

Constructive Timing Violation/or Improving Energy Efficiency
153
[18] T. Sato, "Evaluating the impact of reissued instructions on data specula-
tive processor performance," Microprocessors and Microsystems, voJ.25,
issue 9-10, 2002.
[19] 1. Scott, L.H. Lee, A. Chin, 1. Arends and B. Moyer, "Designing the
MeCore™
M3 CPU architecture," International Conference on Com-
puter Design, 1999.
[20] O.S. Sohi, "Instruction issue logic for high-performance, interruptible,
multiple functional unit, pipelined computers," IEEE Transactions on
Computers, voJ.39, no.3, 1990.
[21] K Usami, M. Igarashi, F. Minami, T. Ishikawa, M. Kanazawa, M. Ichida
and K Nogami, "Automated low-power technique exploiting multiple
supply voltages applied to a media processor," IEEE Journal of Solid-
State Circuits, voU3, no.3, 1998.
[22] L. Wet, Z. Chen, M. Johnson and KRoy, "Design and optimization of
low voltage and high performance dual threshold CMOS circuits," Inter-
national Design Automation Conference, 1998.

Chapter 9
POWER MODELING AND REDUCTION OF VLIW
PROCESSORS*
Weiping Liao and Lei He
Electrical Engineering Department
University ofCaliforn ia. Los Angeles, CA 90095
Abstract
In this chapter, we first present a cycle-accurate power simulator based on the
IMPACT toolset. This simulator allows a designer to evaluate both VLlW com-
piler and micro-architecture innovations for power reduction. Using this sim-
ulator, we then develop and compare the following techniques with a bounded
performance loss of I% compared to the case without any dynamic throttling:
(i) clock ramping with hardware-based prescan (CRHP), and (ii) clock ramping
with compiler-based prediction (CRCP) . Experiments using SPEC2000 floating
point benchmarks show that the power consumed by floating point units can be
reduced by up to 3 I% and 37%, in CRHP and CRCP respectively.
1.
Introduction
Power is rapidly becoming one of the primary design constraints for mod-
ern processor design due to increased complexity and speed of the system.
Cycle-accurate microarchitecture-level power simulators such as Wattch [4],
SimplePower [17], and TEM2P 2EST [5], have been developed and used ex-
tensively to validate power-efficient microarchitecture innovations, including
clock gating [16], dynamically reconfiguring resources [10], etc. However, all
aforementioned work focuses on superscalar architecture.
In this chapter, we study power modeling and reduction for VLIW architec-
tures. Our contributions include:
•
We integrate the Cai-Lim power model [2, 7] into the IMPACT toolset
[3] and develop a cycle-accurate power simulator named PowerImpact.
'This research was partially supported by SRC grant 2000-HJ-782 and Intel. We used comp uters donated
by HP and SUN Microsystems. Address comments to Ihe@ee.ucla.edu.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

156
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
This simulator allows the designer to evaluate both VLIW compiler and
microarchitecture innovations for power reduction.
•
We develop and compare the following techniques with a bounded per-
formance loss of 1%, compared to the case without any dynamic throt-
tling: (i) clock ramping with hardware-based prescan (CRHP), and (ii)
clock ramping with compiler-based prediction (CRCP).
•
Experiments using PowerImpact and SPEC2000 floating-point bench-
marks show that the power consumed by floating-point units can be re-
duced by up to 31% for CRHP and 37% for CRCP, respectively.
The rest of the chapter is organized as follows . Section 2 describes the
power simulator for VLIW architectures. Sections 3 presents implementations
of CRHP and CRCP. Section 4 shows the experiment results. Section 5 con-
cludes the chapter and discusses ongoing work.
2.
Cycle-Accurate VLIW Power Simulation
Existing work [4, 17,5] considers superscalar architecture. There has been
limited work on architectural-level power simulator for Very Long Instruction
Word(VLIW) architectures. In this section, we first introduce the IMPACT
infrastructure for VLIW architectures[3] and the power models we used. We
then present the power simulation enhancement to IMPACT.
Simulation
results
IMPACT C
com p i l er
LSIM
simulator
Tra c e
f o r
Ls im
Figure9.1.
Flowdiagramfor IMPACT
2.1
IMPACT Architecture Framework
The IMPACT toolset (http://www.crhc.uiuc.edu/IMPACT/) contains the IM-
PACT EPIC architecture, compiler, and emulation and simulation tools (see
Figure 9.1). The IMPACT compiler compiles a C benchmark with both front-
end and back-end optimizations. The emulator translates the intermediate rep-
resentation to C code, which can be compiled by the host compiler, and the ex-
ecutable code is generated. Lsim , the cycle-accurate microarchitectural-level
performance simulator, takes the executable code directly, generates a trace

Power Modeling and Reduction of VLIW Processors
157
on-the-fty and consumes the trace for simulation. The executable code can
also be executed on the host machine to generate a trace as the input of Lsim.
In our experiments, we used the former mode, as shown in Figure 9.1. This
toolset has been successfully utilized to conduct the system-level architectural
experiment and new code optimization [3, 1).
2.2
PowerModels
The essential idea of microarchitecture power models is to partition a pro-
cessor into multiple modules. Given the value of active energy Pa and inactive
energy Pi per cycle for each module, the total energy for each module can be
calculated as
E
=
Pa X totai.aciiue.cucles + Pi x total.inaciiue.cucles
where the number of total active and inactive cycles are collected by a cycle-
accurate performance simulator. The whole system energy is the sum of the
total energy for each module. The difference between different power models
is the ways in which partition the processor and get the value of power for each
module. There are mainly two ways to get the power value. One is based on
empirical data and the other is based on formulae. Corresponding to the two
ways, there are two prevalent power models for microarchitecture-level power
simulation in the literature. One is the Cai-Lim power model [2] and the other
is Wattch [4]. They were both originally used for superscalar architectures.
The Cai-Lim power model [2] is an empirical model. It partitions the pro-
cessor into Functional Unit Blocks (FUBs). The Pa and Pi for each FUB can
be given directly. Alternatively, each FUB is characterized by areas (A(n))
of four circuit types - dynamic, static, PLA (programmable logic array), and
clock and memory - to implement this FUB, as well as active power densities
Pa(n) and inactive power densities Pi(n) for each type of circuit. Then, the
energy dissipation of a FUB is given by
E
= L An X (Pa(n) X actiue.cucles + pi(n) X inactiue.cuclesi
n
where Pa and Pi are active and inactive power densities. An is the area for each
circuit type. n iterates over the four circuit types. The energy dissipation com-
puted separately for each FUB is added up to get the total power dissipation.
The Wattch model [4], on the other hand, is a formula-based model.
It
classifies components of a processor into four catalogs: array structures, fully
associative content-addressable memories , combinational logic and wires, and
clocking. For components in each catalog, formulas are used to calculate active

158
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
and inactive power. The formulas are formed based on circuit simulation of
components' circuit designs.
2.3
Powerlmpact
Table 9.1.
Partitions in our power models.
FUBsin Empirical Model
FUB's name
corresponding hardware
FUB's name
corresponding hardware
npclog
next pc generation logic
decodepla
Instruction decoder
btblog
BTB logic
decodemisp
Misprediction handlinglogic
btbcac
BTBcache
fuint
Integerexecutionunit
rsbcac
ReturnStack Buffer
fufp
Floatingpoint execution unit
itlbcac
Instruction TLB
ul2log
UnifiedL2 cache logic
dtlbcac
Data TLB
ul2tag
Unified L2cache tag
ill log
Ll instruction cache logic
ul2cac
Unified L2cache array
il!tag
Ll instruction cache tag
reglog
Register File logic
ill cac
L1 instruction cache array
reg
Registers
dillog
Ll data cache logic
dl!tag
Ll data cache tag
dll cac
Ll data cache array
biu
Bus/iO buffer
decodestall
Decoder stall logic
pmhlog
Page miss handler
Components in Formula-based Model
Array Structures
L1 and L2 caches, BTB, Register file
Fully Associative CAM
TLBs
Combinational Logic
Functional Units
Clocking
Clock buffers, clock wires
We integrate both empirical and formula-based power models into the IM-
PACT toolset and name the resulting new toolset as PowerImpact I. Figure 9.2
illustrates the overall structure of PowerImpact. We develop an interface be-
tween Lsim and the power models. In the empirical model similar to Cai-Lim
model, we partition the VLIW architecture supported by Lsim into the twenty-
four total Functional Unit Blocks(FUBs) shown in Table 9.1, which are slightly
different from the microarchitecture structure in the original Lsim simulator. In
the formula-based model similar to the model in Wattch, the major components
include a branch predictor. register file, L1 and L2 cache, integer and floating-
point ALUs, TLB and clock, as shown in Table 9.1. Energy-per-cycle values
are calculated by formulas for these components. Overall, one can see that our
empirical model has a finer granularity than our formul a-based model. In the
PowerImpact toolset, users can choose any of two models in their convenience.
I Powerlmpact isavailable at htlp:lleda.ee.ucla.edulPowerlmpacV.

Power Modeling and Reduction of VLlW Processors
159
Powerlrnpact reads the user-specified power information and the system con-
figuration, and then the activities and corresponding power information are
collected in every clock cycle . The Powerlmpact toolset is able to simulate
the performance, average power, and step power (i.e., the power difference be-
tween two consecutive cycles) for every functional block or component and
the whole system for given benchmark programs.
Powe
parame
Power
Es t imation
Pe rf orma nc oa
Es t i mat ion
r
t e rs
.l Power Hade-I s
wi.t h
~-
~
s i mu l a t o r
int e rface
r e
f
pti on
PACT
ey e Le
e e cc u r a t e
Pe r f o r rr ar.c e
Si :r.~l a:c~
( L s i.mt
Ha rdwa
d e s c r i
f or
1M
Benchma r k
co mpi l ed by
I MPACT
Figure 9.2.
Overall structure of Powerlmpact
3.
Clock Ramping
Clock gating is effective to reduce the dynamic power consumption of func-
tional units. Most existing papers [14, 11, 6] assume that the dynamic throttling
can be achieved instantly. However, turning on/off a functional unit in a short
time (e.g. within one clock cycle) will lead to a large surge current. A large
surge current requires higher design and manufacturing costs for the power
supply, reduces the circuit reliability, and limits the voltage scaling for further
power reduction.
To reduce the surge current by these clock gating technologies, Tiwari et al
(13, 12] first proposed to extend the switch on/off time by inserting "waking
up" and "going to sleep" time between the on and off states.
In this case,
the clock gating takes a few cycles and can be called clock ramping, different
from the conventional clock gating approach in [14]. To avoid the performance
penalty introduced by the extra switching cycles, clock rampi ng with hardware
prescan (eRR?) is proposed in [15]. An extra set of fetch-and-decode logic2
is used to prescan the incoming instructions so that the clock gated functional
units can be ramped up in time for the upcoming instructions. A superscalar
architecture is assumed in [15].
In this chapter, we develop a new compiler optimization technology, which
automatically inserts ramp-up instructions (RUn based on hyperblock schedul-
ing to instruct the in-time ramping up of functional units . Therefore, no extra
2We can also use a larger instruction buffer to avoid the extra set of fetch and decode logic.
But the
performance in our experiment became much worse due to branches.

160
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
fetch and decode logic used in the hardware prescan is needed. We call the
new clock ramping technology clock ramping using compiler-based prediction
(CRC?). For comparison, we also implement an improved CRHP technique for
VLIW architectures. It uses a finer clock ramping granularity to achieve more
power reduction compared to [15].
In the following subsections, we first present the improved CRHP and then
the new CRCP. Because FPUs consume about 10% of the processor power, we
use FPUs to illustrate our ideas based on SPEC benchmark simulations.
3.1
Clock Ramping with HardwarePrescan (CRHP)
The conventional floating point unit (FPU) only has two states: inactive
state and active state (see Figure 9.3(a)). When there are floating-point instruc-
tions executed, the FPU is in the active state and consumes active power (Pa ).
On the other hand, FPUs have no activity in the inactive state and dissipate
leakage power (Pi), about 10% of the active power (Pa ), in present process
technology. When any floating-point instruction gets into the FPU, the FPU
will jump up from the inactive state to the active state in one clock cycle. This
approach may lead to a large surge current.
To reduce the surge current at the architecture level, we assume that the
power level does not change within a clock cycle, and define the step power
as the power difference between the previous and present cycles. Further, we
assume that the bigger the step power, the larger the surge current. Therefore,
the step power can be used as a figure of merit for the surge current. Then, we
can insert a few cycles between the fetch and execution stages and introduce
intermediate power consumption levels between the inactive and active states
to reduce the step power. Figure 9.3(b) illustrates the clock ramping technique
first proposed in [12, 13]. This approach may result in a big performance loss
however.
In comparison, Figure 9.3(c) shows our clock ramping with instruction pres-
can method. The coming instructions are prescanned before they are fed into
the instruction fetch (IF) stage, and the corresponding FPUs are ramped up
based on the result of prescanning.
For the microarchitecture in Lsim, there are two clock cycles from IF to
EXE stages. If we prescan a floating-point instruction N clock cycles before
it gets into the IF stage, we can have (N+2) clock cycles to gradually power
up the target FP unit to the active state, if there is no functional unit stall.
We call N the prescan time (Tp). Further, we define the time to ramp up a
functional unit as ramping time T«. T; of a functional unit is decided by the
design constraints on the surge current, and it is assumed to be independent of
the pipeline stall. When there is no pipeline stall, Tp + 2 = T; is required to

Power Model ing and Reduction of VLlW Processors
161
Active state (Pa)
Base Case
j-'l
J-iL
Inactlves~@1e_{~lt
.
.~_
_
(.)
Ramp ing with
pre-scan
~-'-Ir - -j-L
j
~ --I---:--uJ
InacU~~_~~~~r
I
I
-1
~
I
JI
j4 --- ~
~ - - - -- --~
(c)
~ - - -
--
•
Pre-seen time
Active waiting
Active waiting
time
lime
Ramp ing without IActivestate
r 1_.L-..
pre-scan
~--L i
.
Inactl~~ state
_
1 '---_
_
(b)
Active state
Figure 9.3.
The relationship of states.
ensure no performance loss and is assumed in [15]. This assumption will be
removed in this chapter for better performance and greater power reduction.
As in [15], we define the active waiting time (Ta ) as the time that an idle
FPU remains in the active state before its ramping down. It helps to exploit the
spatial and temporal locality of FP instructions.
Note that we apply clock ramping to each individual FPU. In the implemen-
tation presented in [15], all FPUs are treated as a whole floating-point block
and are ramped up and down simultaneously. Clearly, not all FPUs are used
at the same time. Figure 9.4 shows the run-time utilization rates of FPUs for
SPEC2000 FP benchmarks equake and art, with the hardware configuration of
6-issue width and total 4 FPUs. Clearly, only a small fraction of total FPUs
are required most of the time. It is easy to predict that our ramping of each
individual FPU can reduce more power compared to the ramping of the whole
FP block in [15].
&quake
srt
1(,%
':~
1(,%
'"'""'""'"
3<,%
,,%
. . - ':
----'"--- - - - -
o
I
2
3
4
0
1
2
3
..
The number 01FPU. beIng uaed . lmulteneously
The number 01 FPU. beIng uMd almultlitleoUllV
Figure9.4.
Utilization rate for FPUs. This figure shows the distribution of FPU usage in terms
of different numbers of FPUs used at the same time.

162
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
3.2
Clock Ramping with Compiler-based Prediction
(CRCP)
As an alternative to hardware-based prediction, the compiler can be used
to predict incoming FP instructions.
In our compiler-based clock ramping
method, the compiler decides when and how many FPUs are needed by the
incoming floating-point instruction. Such decisions can be coded into a special
type of instruction called a ramp-up instructions (RUf), which can be inserted
into the instruction sequence. When RUIs are fetched, the hardware will ramp
up as many FPUs as needed.
We call this method clock ramping based on
compiler prediction (CRC?). Note that the ramping down is still decided by
the hardware, same as for CRHP.
In VLIW architectures, instructions are grouped into bundles. An interesting
observation is that bundles are not full most of the time. Figure 9.5 shows the
utilization rates of bundles for SPEC2000 floating-point benchmark programs
equake and art. Clearly only a small faction of bundles are full. Therefore,
RUIs can be inserted into empty bundle slots. The basic CRCP algorithms and
a variety of improvement will be discussed below.
.cruake
....
art
Figure 9.5.
Distribution of instruction numbers in bundles. with maximum bundle width =6.
The numbers 1 to 6 indicate how many instructions are in one bundle.
3.2.1
Basic CRCP Algorithm.
We chose hyperblock [9] as the basic
structures in our CRCP algorithm. A hyperblock is a set of predicated basic
blocks in which control may only enter from the top, but may exit from one or
more locations. The motivation behind using hyperblocks is to group a number
of basic blocks from different control flow paths into a single manageable block
for compiler optimization and scheduling [9].
We first define two concepts for the ease of description: (l ) the latency
of a bundle as the maximum latency of the instructions in the bundle; (2) the
distance between two bundles A and B as the sum of the latencies of all bundles
between A and B, including the latency of bundle A.

Power Modeling and Reduction of VLlW Processors
163
We apply our CRCP algorithm as an extra back -end compiler optimization
after the compiler finishes performance-related optimization and scheduling.
Our algorithm searches each hyperblock for floating-point instructions (FPf).
During our search, once we find a bundle with FP instructions, called FPbun-
dles, we go upstream with distance D and reach the bundle called the target
bundle. If we succeed in inserting a RUI into the target bundle, the distance
D is called prediction time Tp . It is the counterpart of the prescan time Tp
in CRHP, so we use the same symbol to represent them. When there is no
pipeline stall, Tp + 2 = T; is required to prevent performance loss. Figure
9.6(a) illustrates how we choose the target bundle. In this figure, bundle B is
the FP bundle and bundle A is the target bundle. The distance between A and
B is Tp . In this case, the RUI contains only the number of FPUs needed by the
correspondent FP bundle.
p
Ta r g e t
Bu ndle
I
rt
FP bund l e
I
p
Ta r g e t
Bu ndle
1
i'
FP b u ndle
( a )
Ib)
Figure 9.6.
Insert ramp-up instructions
Table 9.2.
System configuration for experiments
Issue width
6
BTB size
1024 entries 2-way associative
Memory
page size 4096 bytes, latency 30 cycles
Memory bus bandwidth
8 bytes/cycle
Function al Unit
number
Latency
Integer Unit
4
1
FPU
4
2 for FP add and FP
multiply, 15 for FP division
Cache
number of sets
block size
associativity
Replace Policy
L2 Cache
4096
256
1
LRU
Ll Instruction Cache
1024
64
2
LRU
Ll Data Cache
512
64
4
LRU
It is possible that the target bundle is full, meaning there is no slot to insert
RUI into this bundle. In this case, we choose to continue going upstream until
we find a bundle with one empty slot to insert the RUI. However, in this case
Tp +2> Tr , which means the hardware will ramp up FPUs too early and cause

164
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
unnecessary power consumption. To avoid this, we record the distance Dp
(as shown in Figure 9.6) between the ideal location for the RUI and the first
feasible location for the RUI. The hardware will not ramp up FPUs right after
it fetches an RUI, but ramps up FPUs Dp cycles later.
Further, if we reach the head of the entrance point of a hyperblock, we
should consider each branch, except those off-trace branches, to this block and
continue searching upstream on each branch point. Figure 9.7 shows this case.
Clearly, it may introduce extra RUIs and increase power consumption. But
such RUIs are necessary to improve performance.
When a RUI is fetched, the hardware obtains the D p and the number ofFPUs
that are needed by the incoming FP bundle. After D p cycles, the hardware
checks the states of all FPUs, then ramps up as many FPUs as needed. For
example, if the incoming FP bundle has four FP instructions as indicated by
RUI and there are already two FPUs in the active state , then only two extra
FPUs will be ramped up. It is easy to see that in our CRCP approach, the
hardware is much simpler than that in CRHP. No extra set of fetch or decode
logic is needed.
After an FPU is used, it is kept in the active state for the length of the active
waiting time. This is the same as in CRHP. The rest of this subsection describes
improvements over the basic algorithm.
r amp - up
ramp-up
Instruction
Instructi on
<,
/
FP bundl e
Figure 9.7.
Insertion of ramp-up instructions beyond the current Hyperblock
3.2.2
Reduction of Redundant Ramp-up Instructions.
Inside each
block, if the distance between two FP bundles is smaller than the active waiting
time, and the latter FP bundle has no more FPIs than previous one, then we
can simply skip the latter FP bundle and do not need to insert an RUI for it.
Because the two bundles are in the same block, it is very possible (but not

Power Modeling and Reduction of VLlW Processors
165
definite because we chose hyperblock, not basic block) that the previous FP
bundle is executed before the latter one. So within the active waiting time, if
the latter one has fewer FP instructions, its requirement will be met for sure.
For this reason, we avoid inserting RUI for the latter FP bundle so that we can
eliminate redundant RUIs and save power.
3.2.3
Control Flow.
If we confront a procedure call instruction when
searching upstream, we find the return instructions of the procedure and con-
tinue searching upstream from the return instruction.
Also, when we move out of a block while searching upstream, we should
check if this is the head of a procedure. If so, we need to search the whole pro-
gram, find every procedure call to the current procedure, and continue search-
ing upstream from every procedure call instruction.
3.2.4
Load Instructions.
Load instructions have pre-defined latencies
in IMPACT. However, the actual run-time latencies for load instructions can be
much larger than the pre-defined value when cache misses happen . Because the
ramping of FPUs does not stall when the pipeline is stalled, if the load latency
becomes larger than the sum of the FPU ramp-up time and active waiting time,
the FPU will ramp down before the instruction arrives at the execute stage,
which may causes a large performance loss.
To reduce the performance loss, we apply the following simple amendment.
If we detect a data hazard due to a load instruction during the decode stage,
we simply pick one active FPU and keep it in the active state until the load
instruction finishes. Because an FP bundle is most likely to contain one FP
instruction (see Figure 9.4), keeping one FPU in the active state can prevent a
large performance loss with small power consumption overhead as shown by
experimental results in Section 4.
4.
Experimental Results
In this section, SPEC2000 FP benchmark programs equake and art are used
to study the performance and power impacts of various power reduction tech-
niques. We measure performance in IPC, and compare our performance and
power to those without any dynamic throttling. The system configuration used
in our experiment is summarized in Table 9.2.
Figures 9.8-9.11 show the performance loss and power reduction achieved
by the CRHP and CRCP approaches for the benchmark programs equake and
art, respectively. The two parameters in the figures are the active waiting time
Ta and prediction/prescan time Tp . We assume that the ramping time is T; =
10 in all experiments in this chapter.
According to these figures, the longer the active waiting time, the better the
performance. Further, one can easily see that Ta = 16 can satisfy the bounded

166
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
••
1(1
T.
ICRHP!
TP'
•
10
tI
ICRCP!
Figure 9.8.
Performance loss (in percentage as the Z-axis variable) of CRHP and CRCP ap-
proaches for equake.
T.
~
'"
: .;;y :-• . !.
~.
~
/.
~
~
""
. '. :
~
,
,
~ ,
.
,.
'.
:
,.
,
,
,
"
.. ,
.
.. .,
~
"
..
""..
[CRHPl
ICRCP!
Figure 9.9.
Power reduction (in percentage as the Z-axis variable) of CRHP and CRCP ap-
proaches for equake.
"
1 ~
T.
~.
, "
____ I
~
,.J1
'-;1
,
]
r-r
l ~
q
:IT'
I:
"
If.;
I . , .. ..
..
ICRHP)
ICRCPI
Figure 9.10.
Performance loss (in percentage as the Z-axis variable) of CRHP and CRCP
approaches for art.
performance loss of 1%. Therefore, we will assume Ta = 16 in the rest of this
chapter.
Moreover, there exists an optimal Tp for the given active waiting time. In
general, a Tp that is too small or too large is not beneficial for performance
because it does not ramp FPUs in time, contributing to the performance loss.
However, a large Tp degrades performance less than a small Tp does . This is

Power Modeling and Reduction of VLlW Processors
167
.. _." ...ofIC··,
-"'"
Il "
_.~•. ,.. -
..V
J
..:l
..~
~ .. ~
H
..<
.~
~
". ..
"
Ij
II..
.,
';' "
....
..
~
10 • . ,
.,
T•
. ..
..
. ..
..
T.
.. ..
T.
..
IC1iHP )
ICRCP)
Figure 9.//.
Power reduction (in percentage as the Z-axis variable) of CRHP and CRCP ap-
proaches for art,
due to the fact that the FPU is kept active for the active waiting time and there-
fore the performance loss by a ramping that is too early can be compensated.
I
0.9 •
0 8 ~
0.7 '---~---
0 6 I
05
-
0.4
0.3 ;....-----
0.2
o ~----
Tp
10
11
12
• CRCP
. CRHP
Figure 9.12.
Performance loss (in percentage) for T; == 10 and T« == 16
Figure 9.12 shows the performance for CRCP and CRHP when Ta = 16
for benchmark equake (art has a similar trend). Clearly, the performance of
CRCP is a convex curve with the single local optimal Tp = 9. However, the
performance of CRHP is not a convex curve, and has a few local optimal Tp
values. Therefore in the theoretic sense, an exhaustive enumeration of Tp is
needed to find the best Tp for CRHP while the best Tp for CRCP can be easily
found as a local optimal value without exhaustive enumeration.
Figure 9.13 shows the power reduction for CRCP and CRHP when Ta =
16 for benchmark equake (again, art has a similar trend) . When Tp < 10
cycles, CRCP consumes less power than CRHP. With respect to the best Tp =
6 for CRHP and the best Tp = 9 for CRCP, the energy consumed by FPUs
can be reduced by 31% and 37% for CRHP and CRCP, respectively, while

168
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
.5
.0
35
30
25
20
15
10
0
5
6
10
11
12
Tp
• CRCP
CRHP
Figure 9.13.
Power reduction (in percentage) for T ; =10 and Ta =16
the performance loss is negligible at 0.2% and 0.1% for CRHP and CRCP
respectively. It is worthwhile to point out that we do not consider the power
dissipation and cache misses for hardware prescan. So the actual power and
performance by CRHP will be worse than those in Figure 9.8 and 9.12.
However, when Tp > 10 cycles, CRCP consumes more power than CRHP.
The reason is that whenever the compiler moves out of a block when searching
upstream, RUls will be inserted in every block that could possible branch to
the current block. This increases the number of RUIs and leads to unnecessary
hardware ramping. As Tp increases, the chance for the latter situation to occur
also increases, which will eventually outweigh the benefit brought by reducing
redundant RUls at a certain point.
Therefore, CRCP might result in worse
power reduction than CRHP.
It is worthwhile to point out that we believe that, in practice, Tp should be
less than 10 cycles, especially when the number of stages between fetch and
execution becomes large, for example, there are five stages between the initial
fetch stage and execution stage in the Intel Itanium processor [8]. Given that Tp
is less than 10 cycles in practice and CRCP has a higher performance and uses
less energy, the compiler-based CRCP is recommended for VLIW processors.
We have considered our load amendment in Figures 9.8-9.11. To appreciate
the contribution of this amendment, we show in Figure 9.14 the performance
before and after our amendment for the CRCP approach. Benchmark art is
used as it has a relatively low cache hit rate for load instructions. Surprisingly,
this simple amendment can reduce the performance loss from over 6% to less
than 1%.

Power Modeling and Reduction ofVLlW Processors
8 ........,.---~---."....------_
169
6
5
3
6
Tp
_ r Before1
- Alter
\0
Figure 9.14.
Performance Loss (in percentage) before and after the amendment for load in-
struction, for T; =10, Ta =16 and Tp =9.
5.
Conclusions and Discussion
In this chapter we first presented PowerImpact, a cycle-accurate power sim-
ulator based on the IMPACT infrastructure for VLIW processors. We then
used PowerImpact to study the following power reduction techniques with
a bounded performance loss of 1%, compared to the cases without any dy-
namic throttling: (i) clock ramping with hardware-based prescan (CRHP), and
(ii) clock ramping with compiler-based prediction (CRCP). Experiments us-
ing SPEC2000 floating-point benchmarks show that the power consumed by
floating-point units can be reduced by up to 31% and 37% for the CRCP ap-
proach and CRHP approach, respectively.
A limitation of our work was that IMPACT is designed originally as a C
compiler. There are only a few SPEC FP benchmarks written in C, while most
SPEC FP benchmarks are written in Fortran. As far as we know the Fortran
front-end for IMPACT is under development and will be available soon. More
floating-point benchmarks will be tested then.
Our recent work considers the leakage power modeling and reduction. We
study leakage power reduction using power gating in the forms of Virtual
power/ground Rails Clamp (VRC) and Multi-threshold CMOS (MTCMOS).
We apply power gating to three circuit component types : memory-based units,
datapath components, and control logic. Using power and timing models de-
rived from detailed circuit designs and a microarchitecture-Ievel power simu-
lator, we further study the leakage power modeling and reduction at the system
level for modem high-performance VLIW processors. We show that the leak-
age power can be over 60% of the total power for such processors. Moreover,
we propose compiler-based scheduling of MTCMOS to reduce power up to

170
COMPILERS AND OPERATINGSYSTEMS FOR LOW POWER
81.7% for integer and floating-point units, and propose time-out scheduling
of VRC to reduce power up to 94.9% for L2 cache. Such power savings is
equivalent to more than 50% total power reduction for the VLIW processors
we study. Details about the recent progress and the Powerlmpact tool can be
found at http://eda.ee.ucla.edu/PowerImpact/.
Acknowledgments
The authors would like to thank Dr. George Cai at Intel and Mr. Joe Basile
at the University of Wisconsin-Madison for the useful discussions with them.
References
[IJ D.l. August, D.A. Connors, and et al S.A. Mahlke. Integrated predicated
and sepculative execution in the impact epic architecture. In Proceedings
ofthe 25th ISCA, July 1998.
[2] G. Cai and C.H. Lim. Architectural level power/performance optimiza-
tion and dynamic power estimation. In Cool Chips Tutorial colocated
with MICR032, November 1999.
[3] P.P. Chang, S.A. Mahlke, W.Y. Chen, N.J. Warter, and W.W. Hwu. Im-
pact: An architectural framework for multiple-instruction-issue proces-
sors. In Proceedings ofthe 18th 1SCA, May 1991.
[4J D. Brooks, V. Tiwari, and M. Martonosi.
Wattch: A framework for
architectural-level power analysis optimization. In [SCA, 2000.
[5J A. Dhodapkar, C.R. Lim, G. Cai, and W.R. Daasch. Tem2p2est: A ther-
mal enabled multi-model power/performance estimator. In Workshop on
Power-Aware Computer Systems, in conjuction with the Ninth Interna-
tional Conference on Architectural Support for Programming Languages
and Operating Systems, November 2000.
[6J E. Musoll.
Predicting the usefulness of a block result: a micro-
architectural technique for high-performance low-power processors. In
32nd Annual International Symposium on Microarchitecture, November
1999.
[7] S. Ghiasi and D. Grunwald. A comparison of two architectural power
models.
In Workshop on Power-Aware Computer Systems, in conjuc-
tion with the Ninth International Conference on Architectural Support
for Programming Languages and Operating Systems, November 2000.
[8J Intel Inc.
"Intel Itanium Processor", httpr/rwwwiruel.com/itanium/,
2001.
[9] S.A. Mahlke, D.C. Lin, W.Y. Chen, R.E. Hank, and R.A. Bringmann.
Effective compiler support for predicated execution using the hyperblock.
In Proc. ofMicro 25, pages 45-54,1992.

Power Modeling and Reduction of VLlW Processors
171
[10] R. Maro, Y. Bai, and R.I. Bahar. Dynamically reconfiguring processor
resources to reduce power consumption in high-performance processors.
In Workshop on Power-Aware Computer Systems, in conjuction with the
Ninth International Conference on Architectural Support for Program-
ming Languages and Operating Systems , November 2000.
[11] N. Vijaykrishnan, M. Kandemir, MJ. Irwin, and H.S. Kim.
Energy-
driven integrated hardware-software optimization using simplepower. In
ISCA,2000.
[12] M.D. Pant, P. Pant, D.S. Wills, and V. Tiwari. Inductive noise reduction
at the architectural level. In International Conference on VLSI Design,
pages 162-167,2000.
[13] M.D. Pant, P. Pant, D.S. Wills, and V. Tiwari. An architectural solution
for the inductive noise problem due to clock-gating. In Proc. Int. Symp.
on Low Power Electronics and Design, pages 255-257,1999.
[14] S. Manne, A. Klauser, and D. Grunwald. Pipeline gating: Speculation
control for energy reduction. In [SCA, 1998.
[15] Z. Tang, N. Chang, S. Lin, W. Xie, S. Nakagawa, and L. He.
Ramp
up/down floating point unit to reduce inductive noise. In Workshop on
Power-Aware Computer Systems, in conjuction with the Ninth Interna-
tional Conference on Architectural Support for Programming Languages
and Operating Systems, November 2000.
[16] V. Tiwari, D. Singh, S. Rajgopal, and G. Mehta . Reducing power in high-
performance microprocessors. In DAC, 1998.
[17] W. Ye, N. Vijaykrishnan, M. Kandemir, and M.J. Irwin. The design and
use of simplepower: a cycle-accurate energy estimation tool.
In DAC,
2000.

Chapter 10
LOW-POWER DESIGN OF
TURBO DECODER WITH EXPLORATION OF
ENERGY-THROUGHPUT TRADE-OFF
Amout Vandecappelle
Arnout.Vandecappelle@imec.be
Bruno Bougard
Bruno.Bougard@ imec.be
K.C. Shashidhar
Shashidhar.Kodamballi@imec.be
Francky Catthoor
Francky.Catthoor@imec.be
IMECvzw
Kapeldreef 75
3001 Leuven
Belgium
Abstract
Turbo coding has become an attractive scheme for design of current commu -
nication systems , providing near optimal bit error rates for data transmission
at low signal to noise ratios. However, it is as yet unsuitable for use in high
data rate mobile systems owing to the high energy consumption of the decoder
scheme. Due to the data dominated nature of the decoder, a memory organiza-
tion providing sufficient bandwidth is the main bottleneck for energy. We have
systematically optimized the memory organization's energy consumption using
our Data Transfer and Storage Exploration methodology. This chapter discusses
the exploration of the energy versus throughput trade-off for the turbo decoder
module . which was obtained using our storage bandwidth optimization tool.
1.
Introduction
Faithful replication of a transmitted signal at the receiver depends mainly
on the performance limitation of the coding scheme employed. Most known
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

174
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
.--
--i+u
decoder
channel
s
C2
------+--=~
encoder
r-----!--<.- - - - - -
I
Figure 10.1.
Turbo coding-decoding scheme
traditional coding schemes have fallen short of the theoretical bound laid down
by Shannon [16] on what best can be achieved. It is only fairly recently that a
new scheme, called turbo coding [2], has enabled reliable data transmission at
very low signal-to-noise ratios . Since it outperforms all previously known for-
ward error correction codes, the technique has naturally become an attractive
choice for design of current communication systems.
The main drawback of the turbo coding scheme, notwithstanding its near
optimal performance, is the large latency and energy consumption of the de-
coder. This is due to the large number of memory accesses of the highly data
dominated, iterative decoding scheme. Hence, memory organization is a very
critical requirement for an efficient implementation of the scheme. Data Trans-
fer and Storage Exploration (DTSE) [5] is a systematic methodology for opti-
mization of memory accesses in such a data dominated application.
Many flavors of the turbo coding scheme can be seen in recent literature.
We consider here the parallel concatenated convolution coding scheme [2], as
shown in Figure 10.1. The data bit stream I is encoded by two convolution
encoders C\ and C2. The input of C2 is first interleaved by the interleaver n.
The encoded signals are transmitted over the channel, where noise gets added
on to the signals. At the receiver, the signals are fed to the Soft-Input Soft-
Output (SISO) decoders D, and D2. A SISO produces extrinsic information
(e); this is calculated from the uncoded, systematic input (Ys), the coded input
(yp), as well as the previously calculated a priori information (i). i is simply
the (de)interleaved extrinsic information from the other SISO. The decoder is
essentially an iterative scheme, wherein the extrinsic output from one decoder
is fed to the other decoder as a priori information, and the process repeated
until the required degree of convergence is achieved.
We consider here as SISO algorithm the Maximum A Posteriori (MAP)
algorithm [1], which is the theoretically optimal algorithm to soft-decode the
component codes. We use the log-max variant of the MAP [14], which is

Design ofturbo decoder with exploration ofenergy-throughput trade-off
175
defined by the following equations:
ak(m')
=
maxlll(ak_l (m) +y (Sk' ,Sk~I ) )
(10.1)
Pk(m)
=
maxm (Pk+l(m') +Y(Sk' ,Sk~I ) )
(10.2)
LLR(dk) =
maxm ,ml (Y(Sk' ,Sk~l ,dk = 1)+ak(m)+Pk+l (m'))
maxm ,IIII(Y(Sk' ,Sk~l ,dk = 0) +ak(m) + Pk+1 (m')) (10.3)
where a is the forward state metric vector, Pis the backward state metric vec-
tor, y the branch metric vector; S,// represents the state m at time k, and dk the
transmitted bit. Branch metrics yare computed as a function of the systematic
(Ys), coded (yp) and apriori (i) information. LLR(dk) is the log-likelihood ratio
of the transmitted bit.
Issues relating to VLSI implementation of the turbo coding scheme have
been addressed in [13,17] and low complexity implementations to reduce en-
ergy consumption have been provided in [7, 8, 11], and many others. However,
memory organization and optimization issues are not sufficiently addressed in
literature. Memory access optimization, by applying early transformation steps
of the DTSE methodology, has been investigated and important gains thereof
have been shown in [12, 15]. However, the latter result is a single design
point. The global energy and performance trade-off possibilities with respect
to memory bandwidth, which are crucial for a designer to make a sound choice
of the memory architecture, are as yet not available. In this work, our storage
bandwidth optimization tool is demonstrated to provide such trade-off points
in an implementation of the turbo decoder module. This is compared with a
previous, manual design of the memory organization [9, 12].
In contrast to traditional design tools, which produce a single optimal so-
lution, this trade-off allows the designer to explore the design space. For the
turbo decoder, latency and throughput can be improved by sacrificing some
energy consumption, or conversely more energy can be saved by lowering the
performance. The designer can only efficiently decide on these trade-offs if
there is a way to visualize them. Therefore, we propose Pareto plots, which are
generated automatically, to estimate the energy cost of achieving a particular
throughput.
This chapter is organized as follows . Section 2 briefly introduces the DTSE
methodology and positions our work in it. Section 3 summarizes the platform-
independent steps previously applied. Section 4 discusses the transformations
oriented towards improving the storage bandwidth, and thus the throughput.
Section 5 discusses the details of the memory organization. Section 6 draws
conclusions.

176
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
2.
Data Transfer and Storage Exploration Methodology
In data dominated applications, typically found in the multi-media and tele-
communications domain, data storage and transfers are the most important fac-
tors in terms of energy consumption, area and system performance. OTSE is
a systematic, step-wise, system-level methodology to optimize data dominated
applications for memory accesses, and hence, energy consumption [5,6]. The
main goal of the methodology is to start from the specification of the appli-
cation (for example in the C language) and transform the code to achieve an
optimal execution order for data transfers, together with an optimal memory
architecture for data storage.
The OTSE methodology is explained at length along with case studies in
[5, 6], but to position our work, we briefly summarize it here. It essentially
consists of the following orthogonal steps that are sequentially applied:
Global Data Flow Transformations The set of data flow transformations ap-
plied in this step have the most crucial effect on the system exploration
decisions. Two main categories exist. The first one directly optimizes
the important DTSE cost factors by removing redundant accesses. The
second category serves as enabling transformations for the subsequent
steps by removing the data flow bottlenecks.
Global Loop Transformations The loop and control flow transformations in
this step aim at improving the data access locality for multi-dimensional
arrays and at removing the system-level buffers introduced due to mis-
matches in production and consumption ordering.
Data Reuse Decisions The goal of this step is to better exploit a hierarchical
memory organization by making use of available temporal locality in the
data accesses. The result is that frequently accessed data is available in
smaller and hence, less power consuming memories.
Storage Cycle Budget Distribution Application of this step results in distri-
bution of the available cycle budget over the iterative parts of the specifi-
cation in a globally balanced way, such that the required memory band-
width is reduced.
Memory Allocation and Assignment The goal of this step is to select mem-
ory modules from a memory library and to assign the data to the best
suited memory modules under the given cycle budget and other timing
constraints.
In-place Optimization In this step, optimal placement of data in the mem-
ories is determined such that the required memory size is minimal and
cache conflict misses are reduced.

Design ofturb o decoder with exploration of energy-throughput trade-off
177
Suboptimal
/
points
I
I
I
Critical
Path
Fully
I
Se uential :
Needed Bandwidth
Freedom
Figure 10.2.
Energy-performance trade-off
The first three steps of the methodology have been discussed in previous
work and are summarized in Section 3. This chapter completes the script with
the two memory organization steps : storage cycle budget distribution is dis-
cussed in Section 4, and memory allocation and assignment in Section 5. These
steps make it possible to identify a range of energy-performance trade-off pos-
sibilities. Step 6, as mentioned, optimizes the area consumption and the cache
misses, but these are not bottlenecks in our implementation.
'Figure 10.2 shows how the exploration points are represented. The vertical
axis shows the memory organization's energy consumption (or another rele-
vant cost measure), while the horizontal axis shows the performance (e.g. the
cycle budget, throughput, or latency). The cycle budget ranges from fully se-
quential, where all memory accesses are scheduled one after the other, to the
critical path schedule, where memory accesses are sequential only if there is
a dependency between them. A smaller cycle budget means more parallel ac-
cesses, which will lead to a more complex memory organization with a higher
energy consumption. Each possible memory architecture entails a certain en-
ergy consumption and a certain performance. The interesting trade-off points
(called Pareto-optimal points) are those indicated by the curve, which are not
dominated by any other point in both energy and performance. With such a
Pareto curve, the designer can ignore the points which are clearly sub-optimal
and concentrate on the interesting trade-off points. The purpose of the memory
organization tools is to find precisely these trade-off points, without exploring
all non-Pareto-optimal possibilities.

178
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Y,
~:::===~~_f»-{T-
I
__
e _~
u
/* first S1S0 */
for (i)
apriori[i) =
.. . (apriori[i) ...
/* second S1S0 */
for (i)
addr = interleaver[i]
apriori[addr) =
. . . (apriori[addr) .. .
Figure 10.3.
Transformeddata flow of turbo decoding scheme
3.
Global Data Flow and Loop Transformations
The first 3 steps of the OTSE methodology are essentially platform inde-
pendent source code transformation steps. These have previously been applied
on the turbo decoder [lS, 12]. This section gives an overview of the results of
these transformations, to facilitate the understanding of the later sections.
3.1
Removal of Interleaver Memory
The turbo coding paradigm as proposed by Berrou [2] is depicted in Fig-
ure 10.1. This paradigm considers two SISO decoders and two interleavers. If
the same code is used for both component encoders, which is the case for most
of the currently standardized turbo coding schemes (e.g. UMTS,OVB-RCS),
the turbo decoding can be done with only one SISO decoder. Moreover, the re-
quired storage and transfers for the interleaving and de-interleaving operations
can be reduced by not storing the extrinsic information e and subsequently in-
terleaving them, but directly writing the a priori information i in the correct
place.
In addition, the same memory can be used for the a priori input of the SISO
decoder and the extrinsic output, by storing the a priori/extrinsic information
in a linear way. During the first half-iteration, corresponding to 01 in the
paradigm, the SISO decoder reads the a priori information linearly in the stor-
age unit and writes the extrinsic information back in the same place, overwrit-
ing the value which is not needed anymore. During the second half-iteration
(02), the SISO decoder reads the a priori information in the storage units at
addresses generated by a look-up table storing the interleaving pattern, which
emulates the interleaving of the paradigm. The extrinsic information is written
back at the addresses of their corresponding a priori information, which em-
ulates the de-interleaving process of the paradigm. The optimized data flow
is depicted in Figure 10.3, together with a code example implementing this
interleaved addressing.

Design ofturbo decoder with exploration ofenergy-throughput trade- off
179
[
'a... "00'
,n"
a. standardOSW·MAP
b. one worker
~'
-,
~
". "
.:'\
.' ,
,
,,
c. parallelMAP
Figure lOA.
Parallelization of the MAP algorithm
3.2
Enabling Parallelism
With the overhead of the interleaving removed by the above dataflow trans-
formation, the double recursion (&. and ~ calculation) of the SISO module be-
comes the bottleneck of the design regarding latency as well as energy con-
sumption. Usually, this latency bottleneck is broken using the so-called over-
lapping sliding windows MAP algorithms. The frame to be decoded is subdi-
vided into windows of size N. The forward recursion (&. calculation) starts at
the beginning of the window as in the normal algorithm. However, the back-
ward recursion (~ calculation) already starts at the end of the window instead
of at the end of the frame. A training sequence then initializes the state metrics
(~ metrics) at the beginning of each window. A lower bound on the window
size N is imposed by the minimum training sequence length. This length has
to be at least 4 times the constraint length (K) of the code. Since we use a code
with K = 4, N has to be at least 16.
Figure lOA.a depicts the timing of the overlapping sliding windows scheme.
The latency of the decoder is the time between the first bit of input and the first
bit of output of the entire decoder. It is reduced by a factor roughly equal to the
number of windows. However, the throughput stays unchanged. To increase
the throughput, we proposed [15] to unroll the windows loop, i.e. calculating
several windows in parallel. To reduce the required number of training se-
quences, adjacent windows are combined by pairs in structures called workers
(Figure lOA.b). The forward recursion of one window is used as training se-
quence for the (delayed) forward recursion of the adjacent window. and vice
versa for the backward recursion. The workers can be combined in parallel as
depicted in Figure IOA.c. With such a structure, the decoding latency is no
longer proportional to the frame size, but rather to the worker size (2 x N).

180
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
MAP
Interleaving
MAP
De-!nlelleaving
MAP
Interleaving
MAP
h.tflt.... tlon3
hall lI.aUGn 1
hllf Ite"Uon 2
Input
tMJ"era\'--
.,..-----"
''--
.,..-----'
ltefltionO
l1erlUon1
Figure 10.5.
Turbodecodingdata flow and timing
The remaining training sequences can be avoided by exploiting the iterative
aspect of the turbo decoding paradigm. The idea is to initialize the state metrics
of each worker at the first recursion step with values deduced from the last
recursion step of adjacent workers at a previous half-iteration. This method is
called Next Iteration Initialization (NIl) and can only be used between half-
iterations of same parity due to the interleaving between two half-iterations.
Figure 10.5 illustrates graphically the turbo decoding timing and data flow with
NIL One cross corresponds to one worker. The x-axis represents the time, and
the y-axis represents the frame index.
4.
StorageCycleBudgetDistribution
Since in a data-dominated application like the Turbo decoder, the memory
organization is a bottleneck for both energy and performance, the memory ar-
chitecture is optimized before doing the detailed scheduling and synthesis. To
this end, the storage cycle budget distribution steps makes an exploration of
the memory bandwidth, i.e. the number of memories or memory ports acces-
sible in parallel. The goal is to find a minimum-cost memory organization for
different points with different performance, leading to a Pareto plot as shown
in Figure 10.2.
Several memory organization issues are explored in the storage cycle budget
distribution step, which are systematically approached in four sub-steps. First,

Design ofturbo decoder with exploration ofenergy-throughput trade-off
181
Table 10.J.
Data structures, sizes and memory hierarch y layer assignment. N is the window
size. M is the number of workers, 2NM is the size of one frame which is iteratively decoded.
value stored
layer
depth x width
Yls
systematic information for DI
2
2NMx4
Y2s
systematic information for Dz
2
2NM x4
YIp
coded information for DI
2
2NM x4
YZp
coded information for Dz
2
2NM x4
i
a priori/extrinsics
2
2NM x6
a
state metrics
2
7 x 2NMx7
~
state metrics
2
7 x 2NM x7
y
branch metrics
I
6 xM x6
a
state metrics (copy)
1
7 x Mx 7
~
state metrics (copy)
1
7 x M x 7
IT
interleaver
ROM
2NMx9
the background data storage is distributed over a memory hierarchy, so that
often-accessed data can be stored in small memories close to the data process-
ing units. This is called memory hierarchy layer assignment and is the topic
of Section 4.1. Second, data structures are organized to be efficiently accessed
in parallel when needed, as discussed in Section 4.2. Third, loop transfor-
mations (Section 4.3) create possibilities for parallelization across the scope
of loops. Fina lly, Section 4.4 introduces the storage bandwidth optimization
proper, which makes a detailed exploration of parallelism of memory accesses
within loop bodies.
4.1
Memory HierarchyLayer Assignment
This is the first sub-step in storage cycle budget distribution, which decides
for each data structure, the layer in the memory hierarchy it will be assigned to.
The subsequent memory allocation and assignment step determines the optimal
memory architecture for each layer. In effect, this sub-step fully determines the
static caching decisions.
To benefit from the available temporal locality in the data accesses, a mem-
ory hierarchy has been defined [12]. Table 10.1 summarizes the memory hier-
archy layer assignment decisions we made for the turbo decoder. The farthest
memory layer (layer 2) consists of background memories (SRAM) storing the
received systematic (Ys) and coded information (Yp), the a priori/extrinsic in-
formation (e) and the state metrics (Ct., ~) . The closest memory layer (layer
1) contains the branch metrics (y). The state metrics are also buffered in this
layer. Energy consumption is reduced because the bigger memories of layer 2
are only accessed once per recursion step to load or store layer 2 values while

182
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
Table 10.2.
Data structures, sizes and memory hierarchy layer assignment after data restruc-
turing. 2N is the size of one worker. Each of these data structures exists M times, i.e, once for
each worker.
value stored
layer
depth x width
Yls
systematic information for DI
2
2N x4
Y2s
systematic information for Dz
2
2N x4
Yi p
coded information for DI
2
2N x4
YZp
coded information for Dz
2
2N x4
j
a priori/extrinsics
2
2N x6
a~
state metrics
2
2N x98
y
branch metrics
1
2 x 18
up
state metrics (copy)
1
1x 98
n
interleaver
ROM
2N x9
the first layer, which is smaller and less energy consuming, is accessed more
intensively by the data paths.
4.2
Data Restructuring
Data restructuring refers to merging and/or splitting of data structures in
order to make more efficient parallel accesses to the data structures. Merging
of data structures is done when elements in two different data structures are
always read and written together. Splitting of a data structure is done if two
separate parts of the data structure are often accessed in parallel.
Theoretically, each worker has to be able to compute and store 2K - 1 state
metric values. With K = 4, this implies that 8 a and 8 ~ metrics have to be
stored (for each worker). To reduce the cost (energy and area) of the corre-
sponding functional units, special attention has been paid to the quantization of
the state metrics (exand ~). Since all metrics are invariant if a constant is added,
they can be normalized by subtracting the state-O metrics. By doing that, state-
ametrics are always equal to zero after normalization and therefore do not have
to be stored. Moreover, such a normalization reduces the dynamic range of the
metrics and, by consequence, the number of bits required to represent them.
Quantization of the state metrics to 7 bits only marginally «
O.ldB) reduces
the decoding performance compared to a floating point decoder [9]. Similarly,
branch metrics (y) as well as a priori/extrinsic information (e and i) are quan-
tized to 6 bits. Input log-likelihood ratios (systematic and coded inputs used to
compute the branch metrics) are quantized to 4 bits. These quantizations result
in the word widths indicated in Table 10.1.
To enable parallel accesses, all state metrics (7 x 7-bit a values and 7 x 7-bit
~ values) are stored in a very long word (98-bit). Similarly, three (out of six)
branch metrics are stored together in a 18-bit word. This merging allows the

Design ofturbo decoder with exploration of energy-throughput trade-off
183
Figure 10.6.
Dependencies between memory accesses of two loops
data to be used in parallel without need of a seven-port respectively three-port
memory.
In addition, splitting is applied on the metrics, extrinsic and input
data: the data for each window is stored in its own memory. Table 10.2 shows
the resulting sizes for the data structures. Previous investigation [12] has shown
that the introduction of this very distributed memory architecture together with
the quantization and normalization rules and the memory hierarchy described
above lead to an energy reduction by a factor 14 compared to a straightforward
implementation.
4.3
Loop Transformations for Parallelization
The storage bandwidth optimization tool parallelizes the memory accesses
within a loop body. When the available parallelism is limited, it can be im-
proved by looking across the scope of loops for parallelization opportunities.
This is achieved by transformations of the loops: loop merging, loop pipelining
and partial loop unrolling.
4.3.1
Loop Merging.
The turbo decoder originally consists of a se-
quence of four loops: D I , Il, D2 and n-'. Due to the dataflow transformations
(see Section 3.1), the interleaver loops (Il and n- I ) are merged with D2. Each
SISO module (D I and D2) still comprises a sequence of three loops: alpha
calculation, beta calculation and extrinsic calculation. To improve parallelism,
the alpha and beta calculation are merged together, as shown in Figures 10.6
and 10.7.
By merging the two loops, accesses from the two bodies can be

184
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
for
(0 • •N)
Figure 10.7.
Dependencies after merging the twoloopsof Figure 10.6
placed in parallel (since there are no dependencies between them). Since our
memories have maximum one read and one write port, the merged loop body
can be scheduled in four cycles, for a total of 4 x N cycles. The original loop
bodies required 3 cycles each, for a total of 6 x N cycles.
4.3.2
Loop Pipelining.
Due to dependencies between memory ac-
cesses the available parallelism in the merged loops is still limited.
For in-
stance, in Figure 10.7, one cycle of the loop body is reserved exclusively for
the write to extrinsic. To create more opportunities for parallelism, we pipeline
the loop. Figure 10.8 shows the result of this. The dependencies between two
accesses within one execution of the loop body have changed to dependencies
between two different iterations. For instance, alpha is read in one iteration and
the result of the extrinsic calculation is written to extrinsic in the next iteration.
This has been indicated by dashed arrows in Figure 10.8. The result of loop
pipelining is that the total execution time is then reduced to 2 x N + 2 cycles.
This transformation is applied to the inner loops of D( and Dz.
4.3.3
Partial Loop Unrolling.
Finally, partial unrolling (combined
with the data structure splitting mentioned in Section 4.2) allows different it-
erations of a loop to be executed in parallel. In the turbo decoder, the sliding
windows scheme was selected for exactly this purpose. Since there are no (or
limited) dependencies between two windows, it is possible to process them in
parallel on different processors (with separate memories). This is reflected in
the source code by partially unrolling the loop over the windows.
Even after unrolling, parallelizing the different workers is not possible due
to the interleaved addressing of the second SISO. Indeed, the pseudo-random
characteristic of the interleaving pattern used for the address mapping intro-
duces collisions in memo ry accesses. Typically, several workers would write
in the same memory at the same time.
Restrictions to the permutation pat-

Design ofturbo decoder with exploration of energy-throughput trade-off
185
Figure 10.8.
Dependencies after pipelining the merged loop of Figure 10.7
tern can be made in order to arrive at a collision free behavior. However, such
regularity creates the risk of loosing much of the code's quality. Therefore,
a method has been developed to generate collision-free interleavers in a sys-
tematic way, showing at the same time excellent coding gain [10]. The energy
consumption overhead due to the interleaving is mainly due to the address gen-
eration. In our implementation, the addresses are generated by look-up tables
stored in low-power ROMs.
4.3.4
Loop Transformation Results.
Table 10.3 summarizes the re-
sult of the loop transformations on the performance of the turbo decoder. Each
transformation significantly improves both the throughput and the latency. For
the loop pipelining transformation, it may be surprising that also latency is im-
proved. However, this behavior is to be expected since the inner loops were
pipelined; if, on the other hand, the decoder iterations loop, which lies outer-
most, is pipelined, there is no corresponding reduction of the latency.
4.4
Storage Bandwidth Optimization
The storage bandwidth optimization tool fully automatically minimizes the
required storage bandwidth for a given cycle budget by partially ordering the
application's memory accesses. The outputs are the conflict graphs , for each

186
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Table 10.3.
Effectof parallelizingloop transformations on maximallyachievable throughput
and latency
Loop transformation
None
Merging
Pipelined
Unrolled
Throughput [Mbitls]
6.4
9.7
15.6
115.8
Latency [ILS]
70.4
46.0
28.8
3.87
cycle budget, that contain information about which data structures are accessed
simultaneously and therefore have to be assigned to different single port mem-
ories or a multi-port memory.
5.
Memory Organization
The storage bandwidth optimization tool is used in conjunction with a tool
which determines the memory architecture and the assignment of data struc-
tures to memories. This tool provides accurate energy and area estimates for
each input memory architecture and for each cycle budget, from fully sequen-
tial to the critical path schedule. These estimates, in our case only for en-
ergy, plotted against the throughput (which is derived from available the cycle
budget) gives the Pareto curve, which the designer can use to decide on the
memory architecture.
In the results that follow, we have used the energy consumption values of
a memory module generator. The memory modules have one read port and
one write port which can be accessed in parallel. For each memory memory
used, we have evaluated the access energy by simulating with SPICE the netlist
extracted from the corresponding RAM layout in a typical .181l technology
operating in typical conditions (voltage 1.8V,junctions temperature nOe) and
assuming a constant wire load of .20pF.
5.1
Memory Organization Exploration
We first explore the possibilities for an architecture limited to two memo-
ries per worker, to avoid overly complex layout. At low bit rates, the optimal
memory organization is to store the a -
~ metrics in one memory (per worker),
which is 98 bits wide and contains 32 words and consumes 0.36 nJ per decoded
bit. All other data is stored in the other memory, which is 6 bits wide and con-
tains 160 words. Its energy consumption is 0.89 nJ per decoded bit, for a total
of 1.25 nJ per decoded bit.
This memory organization, however, limits the bit rate to 27.5 Mbit/s. To
increase it, one of the coded inputs must be stored separately from the other
arrays, so it can be accessed in parallel to other array accesses. It can still be

Design ofturbo decoder with exploration ofenergy-throughput trade-off
187
2.5
~
2.0
....
;Q...
1.5
r
...
Q,
....
ell......c
1.0
~
------
0.5
-
two memori es/worker
-- derated
-
seven memories/worker
-- derated
+
Design point
o
25
50
75
100
Throughput (Mbps)
125
Figure10.9.
Paretocurvesfor7 workers, for twoand forsevendual-portmemories per worker
stored in the same memory as the ex - r3 metrics. This memory then contains
64 words (still of 98 bits) and consumes 0.94nJ per decoded bit. The other
memory becomes somewhat smaller, 128 words of 6 bits, and only consumes
0.69nJ per decoded bit, bringing the total to 1.63 nJ per decoded bit. Thus,
a throughput of 31.2Mbit/s can be achieved. At even higher throughput, also
the extrinsic information must be stored separately from the other input arrays,
which results in a total memory energy consumption of 2.20nJ per decoded
bit. Figure 10.9 shows these results in a Pareto curve.
In addition to the variation due to the memory organization, the trade-offcan
be adapted by changing the supply voltage. Scaling it up from its nominal 1.8V
increases the throughput but also the energy consumption, and vice versa for
scaling down. Figure 10.9 shows this derating trade-off with a thin line. Note
that it is not possible to reliably decrease the voltage under 1.5V,therefore the
curve flattens when this voltage is reached. Likewise, the upper bound for the
voltage is 2.05 V.
The trade-off points to be considered by the designer depends on the per-
formance constraint that is being addressed. An increase in performance, as
shown, also increases energy consumption. However, the available range of
possibilities make a good trade-offfeasible . When area is at a premium, though
not in our context, a further energy-area trade-off becomes necessary.
The other extreme of a very distributed memory architecture is also shown
in Figure 10.9. With seven memories per worker, the additional memory band-

188
COMPILERS AND OPERATING SYSTEMSFOR LOWPOWER
width increases the maximal achievable throughput to 115.8Mbit/s. This mem-
ory organization also consumes less energy than the two memories per worker,
since each memory is much smaller. Note that the energy dissipated in inter-
connect is assumed constant here.
5.2
Memory Organization Decision
For the turbo decoder design we chose to maximize the throughput and to
lower the energy consumption by selecting the more distributed memory or-
ganization. An architecture has been defined and implemented previously [9].
This section highlights the major characteristics of the proposed architecture.
All state metrics are updated in parallel by a combinational data path imple-
menting the Equations 10.1 and 10.2. Output values are clamped if needed to
fit with the 7-bit representation. The extrinsic information is computed from
the state- and branch metrics according to Equation 10.3 by a 3-level pipelined
data path, they are also clamped if needed. Such a structure allows to do one
recursion step in one cycle. To set up and break down the processor pipeline,
an additional 4 cycles per MAP decoding (D 1 or D2) are needed. The critical
path, which is located in the state metrics calculation unit in our implementa-
tion in .18Jl CMOS technology, has been reduced to 5 ns. Consequently, the
total delay to do one MAP decoding (half-iteration) is (2 x N +4) x 5ns. With
N = 32 and a clock frequency of 200 MHz, the data path throughput of one
worker is 188.2Mbit/s.
The proposed architecture achieves constant decoding latency. The through-
put is determined by the number of workers M. We do not pipeline the iter-
ations over the SISO modules, so that the turbo-decoder throughput is given
by:
.
2xMxN
f[MHz]
throughput[Mblt/s] =
x
(10.4)
2 xl
2xN+4
where f is the system clock frequency (up to 200MHz with the considered
.18Jl technology), 2 x N +4 represents the MAP delay, I the number of fu11-
iterations and 2 x M x N is actually the frame size. The minimum decoding
latency is given by:
103
latency[ns] = 2 x I x (2 x N +4) x ---
f[MHz]
(10.5)
The energy consumption depends on the memory architecture and on the
number of accesses to each memory. The selected memory architecture is sum-
marized in the first two columns of Table 10.4. The number of accesses to each
memory is derived as follows . Each input log-likelihood ratio is written once
for each frame , and three ratios (one systematic and two coded) exist for each
of the 2 x M x N frame indices . In each MAP decoding, one systematic and

Design ofturbo decoder with exploration ofenergy-throughput trade -off
189
Table IDA.
Memories architecture with simulated accessenergy and numberof accesses per
frame
memory type
#occurrences
access energy
#reads
#Writes
[pJ/access]
2N x4
6 xM
12.70
16x/ xM xN
6 xM xN
2N x6
2 xM
18.75
8 x/ xM xN
4 x/ xM xN
2N x98
M
60.03
2 x/ xM xN
2 x/ xM xN
one coded input is read once for the forward and once for the backward recur-
sion, for a total of four reads per half-iteration, while 2 x I half iterations have
to be executed to decode one frame .. The extrinsic information for each frame
index is written once in each half-iteration, and read twice. The state metrics
are written once for each frame index in the first half of each half-iteration,
and read once in the second half. Table 10.4 summarizes these access rates
per frame. The third column shows the energy per access for each memory, as
derived from SPICE simulations. The average of read and write is taken here.
The memory energy (assuming N = 32) to decode one frame is then given by
the following equation.
energy[nJ] = I x M x 21.39+M x 2.44
(10.6)
We have implemented this architecture on silicon [9] with parameters N =
32 and M = 7. The corresponding frame size is equal to 2 x N x M = 448 bits. I
is programmable up to 6 or can be adapted to the required coding quality with
an early stop criterion. Assuming the average case with early stop criterion
(I = 3), by applying formula 10.6, the decoding energy per decoded bit for
this implementation is estimated at 1.04 nJlbit. Assuming a clock frequency of
200MHz, which is allowed by our critical path delay, the worst-case (I = 6)
performance with those parameters according to Equations 10.4 and 10.5 will
be a throughput of 109.8 Mbit/s and a latency of 4.08JlS.
For comparison with the memory architecture exploration, the throughput-
energy trade-off of the actual design has been plotted in Figure 10.9. Two
reasons exist why this point lies slightly off the curve. First, the throughput of
the actual design includes the 4 cycles per half-iteration (corresponding to 5%)
overhead needed for the data processing (setting up of MAP decoder pipeline),
while the exploration only takes into account the data transfers. Second, the
energy estimate for the exploration is based on a formula derived from fitting to
different simulated memory architectures, while energy estimate of the actual
design directly derives from SPICE simulations and thus is much more accu-
rate. Still, the actual design point is close enough to the exploration Pareto
curve to validate its usefulness in deciding on design parameters.

190
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
6.
Conclusions
Identifying energy-performance trade-offs is crucial for an optimal imple-
mentation of turbo decoder. Since the decoder is highly data dominated, the
maximum gain is in finding the energy optimal memory architecture which
meets the performance requirements. We have identified the available trade-
offs of an implementation of MAP decoding algorithm, using a systematic
methodology. Tool support made possible an almost impossible task of manu-
ally checking the entire range of trade-offs for different combinations of mem-
ory architectures. Code transformations have been crucial to allow the SBO
tool to provide an enhanced range of cycle budget distributions and hence, ex-
posing the otherwise hidden trade-offs.
The methodology explained makes the trade-off points in the turbo decoder
visible to the designer, very early in the design cycle, allowing better design
exploration.
This coupled with further optimization of memory accesses is
an important step towards the possibility of using turbo coding schemes in a
whole range of wireless communication applications.
References
[1] L. R. Bahl, J. Cocke, F. Jelinek, J. Raviv. Optimal Decoding of Linear
Codes for Minimising Symbol Error Rate. IEEE Transactions on Infor-
mation Theory, Vol. 20, pp. 284-287. 1974.
[2] C. Berrou, A. Glavieux. Near Optimum Error Correcting,
Coding
and Decoding: Turbo Codes. IEEE Transactions on Communications,
Vol. 44, No. 10, pp. 1261-1271. 1996.
[3] E. Brockmeyer, A. Vandecappelle, S. Wuytack, F. Catthoor. Low Power
Storage Cycle Budget Distribution ToolSupportfor Hierarchical Graphs.
13th International Symposium on System Synthesis, pp. 20-22, Madrid,
Spain. 2000.
[4] E. Brockmeyer, A. Vandecappelle, F. Catthoor. Systematic Cycle Bud-
get versus System Power Trade-off: A New Perspective on System Explo-
ration of Real-time Data-dominated Applications. International Sympo-
sium on Low Power Electronics and Design, pp. 137-142, Rapallo, Italy.
2000.
[5] F. Catthoor, S. Wuytack, E. de Greef, F. Balasa, L. Nachtergaele, A. Van ·
decappelle. Custom Memory Management Methodology -
Explora,., :i
ofMemory Organisationfor Embedded Multim edia System Design . ISBN
0-7923-8288-9, Kluwer Academic Publishers, Boston. 1998.
[6] F. Catthoor, K. Danckaert, C. Kulkarni, E. Brockmeyer, P-G. Kjelds-
berg, T. van Achteren, T. Omnes. Data Access and Storage Management

Design ofturbo decoder with exploration ofenergy-throughput trade -off
191
for Embedded Programmable Processors. Kluwer Academic Publishers,
Boston. 2002.
[7] H. Dawid, H. Meyr. Real-time Algorithms and VLSI Architectures for
Soft Output MAP Convolutional Decoding. 6th IEEE International Sym-
posium on Personal, Indoor and Mobile Radio Communications, Vol. 1,
pp. 193-197. 1995.
[8] F. Gilbert, A. Worm, N. Wehn. Low Power Implementation of a Turbo-
Decoder on Programmable Architectures. Asia South Pacific Design Au-
tomation Conference (ASP-DAC), pp. 400-403, Yokohama, Japan. 2001.
[9] A. Giulietti, B. Bougard, V. Derudder, S. Dupont, J. W. Weijers, L. Van
der Perre. A 80 Mb/s Low-power Scalable Turbo Codec Core. IEEE Cus-
tom Integrated Circuit Conference, pp. 389-392, Orlando, May 2002
[10] A. Giulietti, L. Van der Perre, M. Strum. Parallel turbo coding inter-
leavers: avoiding collisions in accesses to storage elements, lEE Elec-
tronics Letters, Vol. 38, No.5. February 2002.
(11] S. Hong, W. E. Stark et al. Design and Implementation of a Low Com-
plexity VLSI Turbo-Code Decoder Architecture for Low Energy Mobile
Wireless Communications. Journal of VLSI Signal Processing, Vol. 24,
No.1, pp. 43-57. 2000.
[12] F. Maessen, A. Giuletti, B. Bougard, L. Van der Perre, F. Catthoor, M.
Engels. Memory Power Reduction for the High-Speed Implementation
of Turbo Codes. IEEE Workshop on Signal Processing Systems (SIPS)
Design and Implementation, pp. 16-24, Antwerp, Belgium. September
2001.
[13] G. Masera, G. Piccinini, M. R. Roch, M. Zamboni. VLSI Architectures
for Turbo Codes. IEEE Transactions on VLSI Systems, Vol. 7. No.3,
pp. 369-379. 1999.
[14] P. Robertson, P. Hoeher. Optimal and sub-optimal Maximum a Posteriori
Algorithms Suitable for turbo decoding, IEEE International Conference
on Communications, pp. 1009-1013,1995
[15] C. Schurgers, F. Catthoor, M. Engels. Memory Optimization of MAP
Turbo Decoder Algorithms. IEEE Transactions on VLSI Systems, Vol. 9,
No.2, pp. 305-312. 2001.
[16] C. E. Shannon. A Mathematical Theory ofCommumication . Bell System
Technical Journal, Vol. 27, pp. 379-423, 623-656. 1948.
[17] Z. Wang, H. Suzuki, K. K. Parhi. VLSI Implementation Issues of Turbo
Decoder Design for Wireless Applications. IEEE Workshop on Signal
Processing Systems: Design and Implementation, Taipei. 1999.

Chapter 11
STATIC ANALYSIS OF PARAMETERIZED LOOP
NESTS FOR ENERGY EFFICIENT USE OF DATA
CACHES*
Paolo D'Alberto
Alexandru Nicolau
Alexander Veidenbaum
Rajesh Gupta
Informat ion and Computer Science
Centerfor Embedded Computer Systems
University of California at Irvine
{paolo,nicolau,alexv,rgupla}@ics.ucLedu
Abstract
Caches are an important part of architectural and compiler low-power strategies
by reducing memory accesses and energy per access. In this chapter, we ex-
amine efficient utilization of data caches for low power in an adaptive memory
hierarchy. We focus on the optimization of data reuse through the static analysis
of line size adaptivity. We present an approach that enables the quant ification
of data misses with respect to cache line size at compile-time. This analys is
is implemented in a software package STAMINA . Experimental results demon-
strate effectiveness and accuracy of the analytical results compared to alternative
simulation based methods.
Keywords:
Cache Analysis, cache interference, spatial reuse
1.
Introduction
In modern uniprocessor systems the memory hierarchy is an important con-
cern of performance, area and energy. It is also the component requiring most
of the die area in system s-an-chip and it is the principal power consumer, ac-
•Supported by AMRM DABT63-98-C-0045
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

194
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
counting for as much as 20-50% of the total chip power (Kamble 1997; Ghose
1995). In recent years, there has been a great effort on the engineering of sev-
erallevels of cache, to reduce the impact on performance/power of caches. The
focus of our work on memory hierarchy is adaptivity in cache subsystems. We
have built an architecture that enables static and dynamic adaptation of mem-
ory hierarchy: its configuration and policies (Veidenbaum 1999). In this chap-
ter, we focus on (compiler-driven) data cache line size adaptation (Van Vleet
1999; Anderson 2001; Veidenbaum 1999). In fact, the architecture is able to
change dynamically the line size (by hardware monitoring or application in-
struction) during the execution of the application. To exploit fully the potential
of this adaptation, we need a way to target it, that is, (statically) determine
the application cache behavior to trace adaptation for maximum performance
and/or minimum energy dissipation.
Related work on cache behavior analysis can be distinguished in profiling-
based and static approaches. Profiling has been used to determine the memory
behavior by direct measure. Varying some parameters of the architecture, the
direct measure quantifies the variation of the memory performance (Ji 2000).
Static cache analyzers are independent from the inputs and focus on the analy-
sis of perfect loop nests (Ghosh 1999; Vera2000). In (Ghosh 1999), the authors
propose to model the cache misses of memory references by equations, Cache
Miss Equations. Then every iteration (or a sampled version) in the loop nest is
checked whether it satisfies the equations or it does not. The approaches count
the solutions of the equations to achieve an estimation of the number of cache
misses.
There are two limitations in the current static approaches: 1) The loop nest
bounds must be known at compile time. This is not realistic because they are
often parameterized and it is not practical, because they can be very large. 2)
The analyzable loops are sensitive to tiling loop transformation. For example,
if tiling is performed on the three-loop-algorithm for matrix multiplication and
the tile sizes do not divide evenly the loop bounds, the inner loops bounds
cannot be represented by affine functions. The resulting nest is not analyzable.
To overcome these limitations, in this chapter we propose a static approach
to investigate perfect loop nests and determine the relation between line size
and number of misses on a per-nest-base. The analysis result is annotated in
the code and it can be used at run time to set the line size.
This chapter is organized as follows. In Section 2 we present two models for
energy dissipation and execution time in function of the miss ratio. In Section
3 we introduce notations about loop nests and cache equations. In Section 4
we introduce the theoretical frame work and our approach. Finally, in Section
5 we show the results of our analysis for three representative examples.

Static Analysis ofParameterized Loop Nests
195
2.
Energy and Line Size
In this section we explore the line size effect on energy dissipation, using a
theoretical model based on the cache models presented and used in (Kamble
1997; Su 1995; Givargis 1999; Wilton 1996). The energy dissipation per access
on a cache, C, with line size, L, is Eacc(L) = Etag + Eline + Estatus (Kamble
1997; Su 1995). Etag is the energy to determine row and line in the cache (for
adaptive fetch size, it is constant).
Eline is the energy to pre-charge, buffer
and deliver on the bus the line accessed.
Estatus is the energy to check the
cache line status (dirty, present). In direct mapped caches, the dominant term
is Eline = R,rLjL ibus1where R , is a constant for the i-th level of the memory
hierarchy and Libus is the physical bus width (Givargis 1999). The energy for
a hit on Ll is independent from the line size, because of subbanking (Ghose
1995; Su 1995). If A(L) is the data miss ratio in function of the line size and
IMaceI the memory accesses, the energy dissipated for a two level memory
hierarchy is:
E(L) = IMaccl[(I- A,(L))RI +A,(L )R2 r~1]
(ll.I)
L2b...
The access time formula is very similar to the energy dissipation formula. The
access time on Ll is constant (Ghose 1995) but for other levels it depends on
the line size. The access time is:
L
T(L) < IMaccl[(I-A(L))T, + A(L)Tzr-
l l
(11.2)
L2huof
This is an upper bound since some of the accesses may be pipelined and the
access time can be hidden.
For example, with 'A(L) = IlL and L> L2/m, the minimum is when L =
-
KI~~bUS with K, equal to either T; or Ri. This implies that in general the op-
timal line size to achieve minimum access time is different from the optimal
line size to achieve minimum energy dissipation (for the memory hierarchy
system). A very interesting case is when the line size is shorter than the bus
width (r LL 1= 1), because the line minimizing the miss ratio is optimal for
'bus
both performance and energy.
In the rest of this chapter we focus our analysis on line size optimization.
This optimization is a classical trade-off between reuse and conflicts. We re-
port on the analysis line model that enables the trade-off analysis, therefore
quantifying the relation between line size and data miss ratio (A(L)).
3.
Background
A perfect loop nest of depth k (Muchnick) determines a set (iteration space)
of integral points (iteration points), in JGl. The loop order specifies a strong
order between any two iteration points} = (jo ,. .. ,}k-l) and i =(io, . .. , ii.: I).
i precedesJ,and we indicate as i <] j, if it exists a 0 :s l :s k - I so that in = }n

196
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
for every n < I and it < kc. The inequality for two points i ~Jis verified if
either they coincide or i <JJ.
A geometrical order can be inferred too. i is
smaller than J, i < J, if in ::; in for every n but a I so that if < k Graphically, a
point i in the iteration space determines a unique bounded polytope ({II} ::; i}),
a point is smaller than another if a bounded polytope is contained in the other.
It is easy to see that if i <Jthen 1<JJ, but not vice versa. Informally, the first
coordinate io of an iteration point i is associated with the outer loop of the nest
and the last coordinate h-I is associated with the inner loop.
Formally, the iteration space is a bounded polytope (lattice). The iteration
space is the polytope {iiO ~i ~ N(0)l- in short Pj~N (ii)' where N(0) = Fo+GP
with F and G matrices of size k x k and p parameters vector.
Given a point i E Pj~N(ii) and a vector f, it is common to investigate the
bounded polytope pr(i) == {j E Pj~N(ii)Ii- r <JJ~ I}. It is an interval in the
iteration space. Indeed, pr(i) = Pj<Jl - Pj<Jl-i' is the difference of two parame-
terized polytopes containing the orIgin. As introduced in (Wolf 1991), a refer-
ence RB of a k-dimensional array B, i.e. B(io] ...[h- d, has temporal reuse if in
different iteration points the same memory location is accessed: Addr(RB(1)) =
Addr(RB(J)). A vector f summarizes the reuse such as for every iteration point
LAddr(RB(i)) = Addr(RB(f+f)) . We assume that the address of a reference
is a linearfunction: Addr(RB(f)) = Bi+ B_1. For every reuse vector, Bfis O.
Spatial reuse is attained when elements of the same line is accessed. Temporal
reuse is a particular case of spatial reuse .
If reference RB interferes with reference RA , the reuse of RA is prevented by
RB. The interference between memory references is investigated by the Cache
Miss Equation (CME) model (see (Ghosh 1999)).
Definition 11.1 Given two array references RA (interferer) and RB(interferee)
ofthe arrays A and B respectively, we define a subset ofpoints EQI.
EQI == {Ai+~_1 =~t+B :I +nC+I+Dp
(11.3)
with i E pr(t) and t E Pj~N (ii)
A and B are integer matrices of size 1 x k describing the index computation.
C is the cache size in bytes, III < L - 1 is the offset in the cache line and Lis
the cache line size. The free variable n i 0 describes the distance in number
ofcache size blocks between the references. p is a vector ofparameters. f is a
reuse vectorfor RB'
In a direct mapped cache, if the equation has solution, there is a miss. A k-way
cache needs k interferences before to have a miss. To estimate the number of
misses we need to count the interferences at least L bytes apart. We analyze
references with very short reuse vectors (McKinley 1996). The interferer has
a few chances to interfere with different memory locations. Therefore, the
problem is very often reduced into the existence of interference.

Static Analysis ofParameterized Loop Nests
197
4.
The Parameterized Loop Analysis
Let us focus on the determination of interferences and, indirectly, on a sub-
set of cold misses. I . When p is considered constant, Equation 11.3 can be
rewritten in.
(11.4)
We define the interference density, PE E [0,1], of an equation as the ratio of
iteration points where the interference equation is satisfied over all the iteration
points.
When the reuse vector is short (distance equal to 1) -and this is often the
case for code optimized to exploit spatial locality- the Equation 11.4 can be
simplified:
d-I
AB_I +L (ABktk) = nC+I with ABk = Ak - B,
k=O
(11.5)
Property 11.1 IfAB_I + It:,J(ABktk) = nC+ I has solution and ABk mod C
=°with°::; k ::; d - 1, then PE = 1.
Proof: lAB_l+~~J(ABktk)J=lA~1 + ~t=J~BktklJ=lA~1 +I%:'~I (~tk)J =lA~1 J
+ I%:'~ I nktk· If Equation 11.5 has solution, it is independent from t, every
iteration point is solution.
<)
By Property 11.1 we need to focus on the following equation.
(11.6)
We distinguish integer solutions and rational solutions. The existence of in-
teger solution assures the existence of rational solutions. We show a rational
solution space such that contains every integer solution. We then determine the
density in the rational space.
Given qthe smallest rational solution to Equation 11.6 a grid consists of the
solution points I](q) =
n~1 for any k gk = qk+ A~m Pkwith Pk natural number}.
k
A grid cell is the smallest polygon that has all vertices in the grid. Given an in-
teger I, a band is the set of rational points 'B(l) = {ilbl-L < 1+ If:;.J ABk'ilbk <
L} . Note that the origin always belongs to a band when -L < I < L. A band
cell is the polygon bounded by the two hyper-planes -L = I+ Lf~JABkt:J.bk
I Wedo not considercapacitymisses because they should be independentfrom the linesize.

198
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
and L = 1+'Lt:J ABk6.bkand the planes \/k =f j ,6.bk = - Land 6.bk= L for any
o~ j ~ d - 1. For every grid point we can determine a band ('B(l +AB~l))'
Every point in the band has the same solution value for n. In the band, we
can distinguish different band cells. The space determined by the grid and the
bands on the grid points is dense as formalized in the following property.
Property 11.2 For any integer solution z, there is a grid point in the band
passing through z.
Proof: By definition, AB~I +'Lt:JAB'kIZk = n.C + lz; we can represent Zk =
PZk A~k +YZk where YZk = Zk mod A~k' Therefore, AB~I +'Lt:J ABk (PzkA~k +
YZk) =AB~ 1+'Lt:J PZk +'Lt:J ABkYZk =nzC+l.. We have that
(11.7)
" " " 1
....J
ABm +I,d- I ABmyz
We can see that - (d- 1) :::; L
-I
teo
k
kJ :::; d - 1 because for every k we
have AB'k'YZk = Zk mod C. There are several points in the neighborhood of z
and in the grid that have the same solution in n, therefore in the band passing
through Z.
<:;
For any grid cell there is only one band splitting the cell in two, so that two
vertices are apart. The band is determined by two planes, and by construction
they have same inclination of the plane passing through the the grid points.
See Figure 11.1 for an example in a 2-dimensional space. Now we are ready
grid edt
1[ ·.·.·.·.-' .
' ~ :::: :
_.' ·1
" : :::'-:: :1~~;-~:_:::::~-~~-~:t--:-.~:':
0.
..;: ~-~-~~:~ ' i
~l
L·::.: ,:~.:_·
-,:: :::-·:t_-~;-;~~~~-:~~~-;]-_-..:.:.:::,
L .'.~: : ::-,,...
C/A B .
Figure 11.1.
Grid cells and band cells in a plane. In a 2-dimensional space the grid cell is a
rectangle and the band is a between two lines crossing the grid cell on only two grid points. Two
different bands are crossing the remaining two vertices.
to determine the solution density.

Static Analysis ofParameterized Loop Nests
Property 11.3 Every grid cell has Jet
m solution points.
n k=oABk
Property 11.4 Every band cell has at most nFPdm solution points.
k=o ABk
199
Property 11.5 Every grid cell intersects at most three bands and at most
2L 1(2~)d-1 band cells.
Proof: When d = 2, the solution space is a line. The line intersecting the cube
(rectangle) is a diagonal. The number of possible integer solutions are not more
than the number of integer coordinates in each dimension. Therefore, they are
min iE{O,1}(A~i ~) .
When d> 2, the solution space is a (d-l)-dimensional
plane. It intersects d vertices of the grid cell. We project the grid cell and the
solution on any plane t, =O. If the projections on the plane tj =0 is projection
with minimum integer solutions, the number of integer solutions must be at
most 1/2 of the minimum integer solutions of the projections multiplied by
l~J + 1. Therefore, the upper bound of integer solution isf(d) = i(~)f(d­
1) withf(2) = ~ . f(d) is 2L 1n1:i (~)·
0
Theorem 1 IfAB_I +I%:ci (ABktk) =nC+I has solution and ABk mod C # 0
with 0 :S k :S d - 1 and C ~ 4L, then PE :S 2L ~.
PE is the ratio of the number of solutions in a band intersecting a grid cell and
the points in a grid cell.
When the reuse vector has distance h, it is possible to write the Equation
11.4 as a system of h equations. Each equation differs for a constant term. We
approximate the density as P = min (l ,hpE).
The interference existence for an equation is a function XP(L) where P(L) is
a polyhedron determined by the interference equation and for which the line
size L is parameter. If P(L) has an integral solution, XP(L) = 1; if it has not
integral solution XP(L) = O.
X is a monotone increasing function. Indeed, if
Lo :S L(, then XP(Lo) :S XP(L,)· If there is interference for a line size Li, there is
interference for any larger line size.
Corollary 11.1 Let m be the number of coefficients in Equation 11.3 so that
(Ai - Bi) mod C # O. If m > 0, then the cache miss ratio is at most P =
XP(L) min( 1, 2!~1 ~). If m = 0, the cache miss ratio is at most XP(L)'
4.1
Reduction to Single Reference Interference
We now show how the general case can be reduced to the simplest case. The
simplest case is as follows: there is an iteration space with III iteration points;
there is a reference with onl y one interference equation EQI (one reuse vector
of size h and one interferer); the interference polyhedron is function of the line
size and it is denoted as P(Li )' The number of misses is III* P * XP(L)'

200
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
1) A reference RA has k interferers, and RA has just one reuse vector r. Every
interference equation has density solution, Pi, and the solution existence func-
tion, 'XPj(L) (0 ~ i ~ k). Since the interferences due to different interferers are
independent to each other, we can add their contribution: J1.(L) = r~OPi'Xpj (L)'
2 We identify the function J1.(L) as interference density per reference. The up-
per bound to the number of misses for a direct mapped cache is 111* J1.(L). If
we would model a m-way associative cache, we could consider as estimation
of the number of misses IIIlll;,; )J(this is an approximation, not an upper bound
unless Vi,Pi = 1).
2) RA (interferee) and RB (interferer). RA has multiple reuse {ri}O,m-1 so
that ro > rl > ... > rill-I. Therefore we have prj(i) ~ prj(i) with i < j, and
nf=opri(i) = pk(i). We consider only the shortest reuse, because if the reuse is
prevented, there is a miss. If it is not, there is no miss.
3) RA has k interferers and m reuse vectors. A set of equations Eo ... Ek-I
represent the interferences with different references.
For each equation we
consider only the shortest reuse vector.
For every reference we are able to quantify the interference. In the follow-
ing, we investigate the effect of interference on spatial reuse. Larger line size
increases interference (it decreases performance) but also spatial reuse.
4.2
Interference and Reuse Trade-off
In this section we consider the effect of the line size on cache reuse and the
trade-off with conflicts. Every reference reuse vector has a type of reuse, i.e.
spatial or temporal. If a reference has spatial reuse and no interference, the
reference has a miss every ~ access , where eis the line size in data elements.'
If Interference is present some of the reuse can be prevented. The the miss
density for spatial reuse is 11(L) = i +J1.(L) if J1.(L) < 1, 11(L) = J1.(L) otherwise.
It is always possible to label the references so that R; with 0 ~ i ~ n - 1 are
references with spatial reuse and R, with n ~ i ~ m - 1 with temporal reuse.
The density ofthe misses for the loop nest is E(L) = r7:d 11i(L) + r7~-,/ J1.i(L).
A(L) '" III *E(L) is the number of misses for which the line size has any
effect. It is an estimation of the effect of line size on cache performance.
5.
STAMINA Implementation Results
The reuse and interference analysis is implemented in the software pack-
age StaMInA (Static Modeling of Interference And reuse as a part of AMRM
compiler suite). It is built on top of SUIF 1.3 compiler adapting the code de-
2 Interferes are truly independent to each other if they are at least one cache line apart
3[n general it would be qwhere h is the access stride/reuse vector length with h < e

Static Analysis ofParameterized Loop Nests
201
veloped in (Ghosh 1999) and using polylib (Wilde 1993; Clauss and Loechner
1996; Clauss 1998; Clauss 1996). We consider three examples to explore three
important aspects of our analysis.
5.1
Swim from SPEC 2000
swim is a scientific application. It has a main loop with four function calls.
Each function has a loop nest for which the bounds are parameters introduced
at run time. For sake of exposition, we present the analysis for the main loop
nest of one procedure cold 0 (Figure 11.3 written in C language). We analyze
the interference for two different matrix sizes, the reference size 1335 x 1335
and the power of two 1024 x 1024. For the reference size, there is no interfer-
ence for any cache line. For power of two matrices there is always interference.
The execution of SWIM with reference input takes lhr on a sun ultra 5,
450MHz. Any full simulation takes at least 50 times more. Even the single
loop simulation is time consuming. Our analysis takes less than one minute
for each routine whether there is interference or there is no interference.
Due to the number of equations to verify, it is very difficult to verify by
hand the accuracy of the analysis. We simulate 10 of the 800 calls to the calc1
routine using cachesim5 from Shade (Cmelik 1994). The simulation results
confirm our analysis.
5.2
Self Interference
We now consider self interference. Self interference happens when two ref-
erences of the same array, or the same reference in different iterations, interfere
in cache. The example, Figure 11.5, is the composition of six loops with only
one memory reference in each. Each memory reference has a different spa-
tial reuse and it is very long. STAMINA recognizes that the interval between
Table l l,J.
Self interference example. Loop four and five have no interference dependent
from the line size, the output is set to zero.
Loop 0
Line
8
16
32
64
128
256
£cl(L)
0.50
0.25
1.00
1.00
1.00
1.00
Loop 1
£cl(L)
0.50
0.25
0.12
1.00
1.00
1.00
Loop 2
£cl(L)
0.50
0.25
0.12
0.06
1.00
1.00
Loop 3
£cl(L)
0.50
0.25
0.12
0.06
0.03
1.00
Loop 4
ECI(L)
0.00
0.00
0.00
0.00
0.00
0.00
Loop 5
ECI(L)
0.00
0.00
0.00
0.00
0.00
0.00
reuses is after one iteration of the outer loop. It computes the reuse distance
and, in the current implementation, it fixes the value of the interference density
at p = 1. It assumes there is a miss due to capacity (in general the distance is

202
COMPILERSAND OPERATING SYSTEMS FOR LOWPOWER
not a constant and it cannot be compared to the cache size). For this particular
case, it is a tight estimation. In general it is an over estimation. The existence
of interference plays the main role, it discriminates when there is interference
and when to count the interferences. In Table 11.1, we report the results of the
analysis.
5.3
Tiling and Matrix Multiply
We analyze two variations of the common ijk-matrix-multiply algorithm
(e.g. (Golub 1996)). In Figure 11.4 the size of matrix A is not a power of
two, but it is for Band C. The size of A has been chosen so that if there is
interference due the reference on A, it does not happen very often. The index
computation for A is parameterized (0 :S m :S 64 and 0 :S n :S 64). Accesses
on matrix C interfere with the accesses on B. Due to the upper bounds we
choose for the parameters, A does not interfere with any other matrix. Even if
it could, the interference density would be small. We are able to distinguish
two different contribution: at compile time, £er(L), and at run time, £rr(L). In
Table 11.2.
Interference table . for the procedure in Figure 11.4. Reference on A does not
interfere with C and B with a :'S n,m :'S 64. It would if we use lager parameters values. Note
that the optimal line size is 16B. With a physical line of 32B the line size is optimal for both
performance and energy.
Line
8
16
32
64
128
256
Eel (L)
2.00
1.00
2.00
2.00
2.00
2.00
Err(L)
0.00
0.00
0.00
0.00
0.00
0.00
Table 11.2 we can see that the suggested line size is 16B. This example has
been introduced to show a case where the optimal line reduces interference
and it is smaller than the common 32B line. Let us consider a more interesting
Table 11.3.
Interference table for the procedure ijk.matrix.multiplyA in Figure 11.2
Line
8
16
32
64
128
256
Eer{L)
0.00
0.00
0.00
0.00
0.00
0.00
Err{L)
2.00
2.00
2.00
2.00
2.01
2.03
example, where we analyze the tiled version of matrix multiplication Figure
11.2. We analyze only the loop nest in the procedure ijk.matrix.multiply.A, and
the result of the analysis is in Table 11.3. Every matrix interferes with every
matrix. The interference due to matrix A is negligible since is an invariant for
the inner loop. The interference between C and B can be at every iteration
point. There is no interference whenever 1m - »[mod C = L. This example is

Static Analysis ofParameterized Loop Nests
203
very peculiar because the line size is not set once for loop nest, it is determined
at run time.
In the example in Figure 11.4 the analysis takes no more than two min-
utes. For the example in Figure 11.2 it takes more than 8 hrs, on a Sun ultra 5
450MHz. The difference of the execution times is expected. Most of the time is
spent in the search for the existence of the integer solution. This is our perfor-
mance bottleneck and it will be subject of further investigations/optimizations.
6.
Summary and Future Work
We present a fast approach to statically determine the line size effect on the
cache behavior of scientific applications. We use the static cache model intro-
duced in (Ghosh 1999) and we present an approach to analyze parameterized
loop bounds and memory references. The approach is designed to investigate
the trade-off between spatial reuse and interferences of loop nests on direct
mapped cache. Experimental results demonstrate the accuracy and efficiency
of our approach. We plan to expand our implementation to consider multi-way
associative caches and to improve the performance of the existence test, by
applying the gcd-test as proposed in (Banerjee 1993).
Acknowledgments
The authors wish to thank Vincent Loechner, Somnath Ghosh and the mem-
bers of AMRM project for their help on Ehrhart polynomials, the existence
test, cache miss equation determination and our countless discussions. Fi-
nancial support for this research was provided by DARPAIITO under contract
DABT63-98-C-0045.
References
Anderson, E., Van Vleet, T., Brown, L., Baer, J., and Karlin, A.R. (2001). On
the performance potential of dynamic cache line sizes. Technical Report
UW-CSE-99-02-0I. Department of Computer Science University of Wash-
ington .
Banerjee, U. (1993). Loop Transformations for Restructuring Compilers The
Foundations. Kluwer Academic Publishers.
Clauss, Ph. (1998). Advances in parameterized linear diophantine equations
for precise program analysis, Technical Report ICPS RR 98-02. Image and
Scientific Parallel Computing of Strasbourg, France.
Clauss, Ph. (1996). Counting solutions to linear and non-linear constraints
through Ehrhart polynomials: Applications to analyze and transform scien-
tific programs. Proceeding 10th ACM Int. Con! on Supercomputing, ICS'96.

204
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
Clauss, Ph., and Loechner, V. (1996). Parametric analysis of polyhedral it-
eration spaces. IEEE Int. Con! on Application Specific Array Processors,
ASAP'96, Chicago, Illinois.
Cmelik, B., and Keppel, D. (1994). Shade: a fast instruction-set simulator for
execution profiling. Proceedings of the 1994 conference on Measurement
and modeling ofcomputer systems. Pages 128-137.
Ghosh, S.,Martonosi, M., and Malik, S. (1999) Cache miss equations: a com-
piler framework for analyzing and tuning memory behavior. ACM Transac-
tions on Programming Languages and Systems. Vol. 21, No.4, Pages 703-
746.
Ghose, K, and Kamble, M.B. (1999) Reducing power in superscalar processor
caches using subbanking, multiple line buffers and bit-line segmentation.
Proceedings 1999 International Symposium on Low Power Electronics and
Design. Pages 70-75.
Givargis, T.D., Henkel, J., and Vahid, F. (1999) Interface and cache power ex-
ploration for core-based embedded system design. Proceeding of the 1999
International Conference on Computer-aided Design. Pages 270-273.
Golub, G.H., and Van Loan, CF, (1996). Matrix computations. Johns Hopkins
Series in the Mathematical Sciences.
Ji, X., Nicolaescu, D., Veidembaum, A , Nicolau, A, and Gupta, R. (2000).
Compiler-directed cache assist adaptivity. ICS Techincal Report #00 17,
May 2000. University of California at Irvine Department of Information
and Computer Science.
Kamble, M.B. , and Ghose, K Analytical energy dissipation models for low-
power caches. Proceedings of the 1997 International Symposium on Low
Power Electronics and Design. Pages 143-148.
McKinley, KS., and Temam, O. (1996). A quantitative analysis of loop nest
locality. Proceedings ofthe 7th International Conference Architectural Sup-
port for Programming Languages and Operating Systems (ASPLOS VII
10/96 MA, USA). Pages 94-104.
Muchnick, S.S. Advanced compiler design implementation. Morgan Kaufman.
Wilton, SJ.E., and Jouppi, N.P. (1996) CACTI : an enhanced cache access and
cycle time model. IEEE Journal ofSolid-State Circuits. Vol. 31. No.5.
Su, C; and Despain, AM. (1995) . Cache design trade-offs for power and per-
formance optimization: a case study. Proceedings 1995 International Sym-
posium on Low Power Design. Pages 63-68 .
Van Vleet, P., Anderson, E., Brown, L., Baer, 1. and Karlin, A.R. (1999). Pur-
suing the performance potential of dynamic cache line sizes. Int. Conference
on Computer Design (ICDD'99).
Veidenbaum, A v., Tang, W., Gupta, R., Nicolau, A. and Ji, X. (1999). Adap-
tive cache line size to application behavior. In Proceedings ofInternational
Conference on Supercomputing. Pages 145-154.

Static Analysis ofParameterized Loop Nests
205
Vera, X., Llosa, 1., Gonzales, A. and Bermudo, N. (2000). A fast and accu-
rate approach to analyze cache memory behavior. Parallel Processing 6th
International Euro-Par Conference Munich , Germany. Pages 194-198
Wilde, D.K. (1993). A library for doing polyhedral operations. Publication in-
terne N785.
Wolf, M.E., and Lam, M.S. A data locality optimizing algorithm. Proc. of
the ACM SIGPLAN'91 Conference on Programming Languages Design and
Implem entation. Toronto, Ontario, Canada. Pages 30-44.
'define MAX 204 8
double A [ MAX] [MAX].
B[MAX] (MAX] .
C (MAX] [MAXI ;
void ij k matri x multi pl y 4 ( i nt
X, int y,
int z,
i nt m,
l nt
n, int p )
(
int i,),k ;
-
-
fo r( i =O; i<x;i++ )
f or (k-O; k<y ; kH )
fo r (j -O ; j<z; jHI
(
C [i] [j+m)
+- A[i) [k+n)
+ B [k ) /j +p] ;
vo i d matrix_multiply_new_tiling ()
{
lnt ii , jj ,kk :
f or (kk-O ; k<MAX/b; kkH)
for (ii-a; i<MAX/b; ii++)
fo r (jj -O ; j<MAX / b;j jH)
ljk matrix muLt. Lp Ly
4 (min (b ,MAX-i i*b),
min Ib,MAX-jj*b) ,
-
-
-
min (b , MAX-kk+b) .
(ii +MAX+jj)+b.
( i i " MAX+kk ) " b. (kk"MAX+jjll;
Figure 11.2.
Tiling of Matrix Multiply. 6 parameters: loop bounds and A,B and C offsets.
The first procedure describes the computation on a tile.

206
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
' de fi ne
Nl
1335
Itdef i ne N2
133 5
extern
do uble U[N11 IN2 J.
VIN 1!lN2J.
P [N1 1 [N2 J. UNE:W[ N1 1 [N21 ,
VNE:W [N1 1 IN2J.
PNEW[N1 ) [N2] ,
UOLD[Nl I [N2 ) ,
VOLDI N1 j ["2 1,
POLDIN 1} [N21 ,
CU IN1 !lN2J,
CV[Nl ] [N2J.
Z {N1 )IN2 J.
H[Nl ]I N2 1,
PS IIN1] IN2 ] ;
ext ern doubl e DO,
OX ,
DYi
vo Ld calcl ( int
H,
i n t
N)
{
int
i , j;
doub le
FSDX, FSDYi
for
(1= 0 ; i<M; i+ + )
for
(j =Ojj<N; j ++)
/I
ANO
1
2
3
CU(i +II [ i I
= DO' (P [ i 'I I I i I +PI i I [i] I 'U ( i <1 I I i I ;
/l C
#
1 2 3 0
12
/I
/I C
AN 4
5
2
6
CV l i l l i +l) = DO' ( P li l l i <1 ) ' P [i l Iii I' V li l li+ 1 1 ;
/l C
* 5 2
6
4
/ /c
AN 7
8
6
9
Z li+l 1 [i +l ] = IFS DX' (V[i+ l ) ti -r i-v t n l i +1 I I - FSDY' (U [ i +1 ) Ij+1)
I '
C
AN
3
2
1
10
5
«r
-U l i+ 1 ] [ j) l l/ I P [ il Ij) +P l i +1 1 [j ) ' Pl i ' 1 1 [ j+ 1 1' P[iJ [j+ l)l ;
#
8
6
9 3
2
1 10
5
7
RN 11
2
3
3
12
P (i l [jl +DO· (u [ i +l ) [j j- U [ i+1 ) [j ] +U { i J (j}*U (iJ U)
9
9
1 3
1)
-vt n ( j+ l )+ V(iJ ( j +l] ...v t i r t rr -vt i : l il l ,
3
12
9
13
2
11
/ /
/ /
H I i] [ j I =
1/
RN
Figure 113.
SWIM: calc lt) in C code, in the comment lines the reference number and the
order of the references are specified.
r- B[OJ [OJ
120000000
C[O] [OJ
*f
#define MAX 4000
#define MAXCOL 2048
double A(MAX] (MAX].
B[MAXCOL] [MAXCOLJ.
C[MAXCOLJ lMAXCOLJ;
void ijk_matrix_multip1y( int n,
int m)
(
int i,j,k:
for {i;O ;i<n ;i++)
for (k=O; k-cn , k++)
for{j=O;j <n ;j++)
C[i) (j +3J
+= Ati) lk-sm] * B[k] (j) ;
Figure 11.4.
Matrix Multiply. Two parameters: loop bounds and A offset. The parameters n
and m up to 64

Static Analysis of Parameterized Loop Nests
#define CACHE_SIZE 16384
int A[CACHE_SIZE 116J [ (CACHE_SIZE+16) /4J ;
int B[CACHE_SIZE 1 32J [ (CACHE_SIZE+32) /4] ;
int C[CACHE_SIZE 1 64) [ (CACHE_SIZE+64) /4] ;
int D[CACHE_SIZE 1 128 ) I (CACHE_SI ZE+128)/ 4 ) ;
int E[CACHE_SIZE 1 256) [ (CACHE_SIZE+2561/4] ;
int F[CACHE_SIZE 1 512) [ (CACHE_SIZE+5121/4) ;
int
main
( )
(
int i ,j ,k ,l ;
int step ;
1
= 0 ;
for
(j =O;
<:4 ;j++ 1 (
for
(k
0; k
<: CACHE_SIZE
1 16 k++1
A[k ] [ ] ++;
for
(j=0 ;
<:8 ;j++)
{
for
(k
0; k
<: CACHE_SIZE 1 32; k++)
Blk] [ J++;
for
(j =O; <16;j++)
{
for
(k
0 ; k
<: CACHE_SI ZE 1 64;
k++1
C[k] [ ) ++;
for
(j=O ; <32;j++ ) {
for
(k
0 ;
k
<: CACHE SIZE
1 128 ; k++ )
D[k][J++;
-
for
(j =O;
<:64; j ++)
(
for
(k
0 ; k < CACHE SIZE 1 256 ; k++ )
Elk] [ ]++ ;
-
for
Ij=O ; <128;j++)
{
for
(k
0 ; k < CACHE SIZE
1 512 ; k++ )
F[kJ [
)++ ;
-
return 0;
Figure11.5.
Self interference and analysis results
207

Chapter 12
A FRESH LOOK AT LOW-POWER MOBILE
COMPUTING
Michael Franz
UniversityofCalifornia. Irvine
Abstract:
We challenge the apparent consensus that next-generation mobile devices must
necessarily provide resource-intensive capabilities such as on-device Java
implement ations to support advanced applications.
Instead, we propose an
architecture that exploits the high "last mile" bandwidth in third generation
wireless networks to enable the largest part of such applications to run inside a
base station, effectively reducing the mobile device to a dumb terminal. We
discuss some implications of this architecture, with respect to hand-off,
confident iality while roaming in potentially hostile networks, and the need for
a server-transparent segmentation of applications into a computational and a
user interface component.
Key words:
Mobile devices, mobile code, in-network code generation and execution .
1.
Introduction
Latency and bandwidth continue to lag behind demand, and this situation
will continue even with the availability of third generation mobile networks.
In future wireless
networks, available bandwidth between
the mobile
terminal and the base station increases significantly, but this doesn't solve
the problems of bandwidth contention further upstream or those relating to
latency, especially when dealing with remote applications.
The traditional solution of overcoming these problems has been to
place more local intelligence at the user's site.
The most successful
approach to this so far has been that of downloadable "applets" or "mobile
agents" that travel from a server to the user's terminal device and conduct
most of their computations locally, unhindered by bandwidth or latency
constraints.
L. Benini et al. (Eds.), Compilers and Operating Systems for Low Power
© Kluwer Academic Publishers 2003

210
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
Unfortunately,
downloading
and
executing
mobile
code
requires
considerable resources, both computationally as well as storage wise, with
an immediate implication on power consumption. At first sight, this might
rule out mobile-code client
solutions targeted
at low-cost low-power
handheld devices such as next generation mobile voice/data terminals and
networked embedded processors in consumer products.
We propose a solution based on off-loading the resource-intensive parts
of the client program to the mobile base station I or to an appropriate
computational device in the user's personal sphere, such as a "home base
station" or a more capable notebook/PDA computer carried on the user's
body and connected via a technology such as Bluetooth.
In the most
extreme case, the user's mobile device then only needs the functionality of a
dumb terminal, making use of a high available bandwidth between the actual
execution unit (XU) and the mobile station (MS)-but the MS might also
perform
critical
functions
locally,
perhaps
utilizing
reprogrammable
hardware. The original two-level client-server architecture thereby becomes
a three-level MS-XU-server architecture',
A key characteristic of the proposed architecture is that a remote server
need not be aware of the fact that the client is not actually running on the
mobile station.
Further, the division of concerns between MS and XU can
be adjusted, potentially even at run-time, without notifying the server of this
fact. Finally, local state is kept at the XU (albeit possibly partial1y cached at
the MS), increasing overall robustness in the event of power failure of the
MS or intermittent communication breakdown.
The following sections describe the architecture in more detail and
point to areas that require more research . Among other facets, we discuss
the implications of handover and the resulting need to transfer state between
different XUs. Of particular interest to service providers might be that the
provision of computational resources at the XU constitutes a possible source
of revenue, and that control over such user state might tie users closer to
providers and thus reduce chum.
We discuss the issue of automated
segmentation of functionality, by which a controller decides which parts of
the client program should be downloaded to the MS and which parts should
run at the Xu.
In case of a MS that includes reconfigurable FPGA
I In a GSM system, the entity performing these resource-intensive tasks would most probably
be co-located with the MSe (or rather, be part of the Serving GPRS Support Node) while
in cdma2000, these functions would fit within the sse node.
2 This idea can be expanded even further to a plurality of devices in each category: in such an
{MS}-{XU}-{server}
architecture,
an
application
might
be
based
on k software
components distributed across I servers, communicat ing with m end-user devices that are
carried by one or several collaborating users, which in tum are supported by n execution
units (m not necessarily equal to n) . For reasons of simplicity, we don't pursue this idea
any further in the text.

A Fresh Look at Low-Power Mobile Computing
211
hardware, historical
usage profiles
might be used to select an initial
configuration of the FPGA grid, but this could be updated in real time (using
dynamic recompilation) under the remote control of an XU that constantly
models the execution of the reconfigurable hardware in the MS. Lastly, we
discuss the fact that certain security applications such as banking might
require dedicated functionality (such as encryption) to be downloaded to the
reconfigurable hardware in order to guarantee end-to-end functionality, and
how this might be modeled in our architecture.
2.
Architecture
A common approach to masking the latency and limited bandwidth of
client-server applications has been the addition of local processing power at
the client 's site ("fatter" clients). The spectacularly successful Java platform
is
the
current
champion
of this
paradigm.
Java's
inventor,
Sun
Microsystems Inc., has been trying to extend the reach of the Java platform
ever further down the performance spectrum to embedded consumer devices
such as PDAs, smart-phones, or even household appliances, by introducing a
core (picoJava) for dedicated processors that can execute Java bytecode
directly on the silicon.
Processors based on the pico.Iava core require far less external memory
than
alternate
solutions
based
on
general-purpose
processors
for
interpretation
(or
even
dynamic
translation)
of
Java
bytecode.
Unfortunately, however, pico.Iava 's implementations are likely to be quite
complex and consequently relatively expensive and power-hungry.
Hence,
cost and battery-life considerations reduce the appeal of increasingly "smart"
mobile terminals.
We propose an alternative architecture (Figure 12.1) in which the client
program is further subdivided between an execution unit (XU) and a mobile
station (MS).
For example, and this is our main target scenario, the XU
could be incorporated into a mobile service provider's infrastructure and the
MS could be a mobile terminal-but the XU might also be a television set-
top box (or network-connected games console) and the MS a remote control
unit. The key characteristic of our model is that there is a direct (i.e., very
low latency) connection between XU and MS, and that this connection has
sufficient bandwidth. Unlike the connection between the XU and the larger
Internet, we also assume that the XU-MS connection uses some hardware-
based link security (such as provided by 802.11b). Given these parameters,
the MS in the most extreme case could then act simply as a "dumb terminal".

212
COMPILERSAND OPERATINGSYSTEMS FOR LOW POWER
erver
Internet
lient a seen by
erver
low-high bandwidth
high latency
ba e station
et-top box
mobile phone
remote control
Figure 12./. System architecture
In our model, the remote server can be left unaware of the fact that the
client is actually composed of two distinct parts, XU and MS3•
To the
server, the XU-MS pair acts as an atomic entity that is physically located
wherever the MS happens to be (for location-aware services) and that has at
least the computational resources of the XU (the MS may contribute to the
total) .
Hence, in the Java case, a JVM class file would appear to be
downloaded to the MS, but most of it would actually be executed on the XU,
with the segmentation between the two either being hard-coded into the
libraries on the XU (simple case) or dynamically decided and adjusted using
just-in-time code generation (a much more challenging case).
3.
Handover and the Quantization of Computational
Resources
A cornerstone of our architecture is the provision of a low-latency
connection between the XU and the MS.
In the case of a MS that is
physically moving, this implies moving the state of the computation that is
running on the XU along with the MS to keep it "close" and the latency low.
While one might also envisage a solution in which the computation remains
stationary on the initial XU and all traffic is routed via that XU, perhaps
using a fixed-bandwidth reservation scheme, this would clearly be sub-
3 This doesn't preclude that the application couldn't be aware ofthis fact and make use of this
knowledge.

A Fresh Look at Low-Power Mobile Computing
213
optimal; only true mobility of the computations themselves will result in
fully scalable performance.
Hence, it becomes necessary to transfer state from one XU to another.
This brings with it the potential to have XUs in different organizational
domains seamlessly working together:
For example, while the user is
physically at a home or office location, execution services might be provided
more cheaply by a private server installed on those premises.
As a user
enters
a
locality
that
offers
such
a (cryptographically
authenticated)
execution environment, the whole active state is transferred away from the
service provider and is eventually restored there as the user leaves the
locality; all of this completely transparently to the user.
Table 12.1 shows
some of the possible scenarios.
Note that in such a transfer from service
provider to local context and back, the service provider can actually cache
the local state, so that only the changes need to be communicated as the MS
re-enters the domain of the service provider.
Table J2.J. Different classes of execution units and applicable usage scenarios
Execution Unit
Scenario
Network service provider additionally also supplies
computational infrastructure, probably for a fee. Handover
requires continuous transfer of computational state "staying
close to the mobile station" in order to preserve latency
network/provider-based
characteristics. Unlike voice cal1s, which are dropped when the
current provider's network is left, it would be desirable to
provide continuous handover of computational state even when
roaming across different service providers' networks.
user's mobile context
home/office base station
User carries an additional device such as a laptop or PDA that
can provide computational services to the MS and that is linked
to the MS using some body-area networking technology such as
Bluetooth. Upload/Download of state both from/to external
service provider and/or home base station possible under user
control; similar to "synching" today's PDAs.
As user enters home or office, active computational state is
transferred from network provider's computers to a local
execution unit installed on private premises, relieving the
network provider of the obligation to host the computation and
temporarily suspending billing for these services.
A third scenario depicted in Table 12.1 is that of a user carrying his or
her own mobile context that is sufficiently capable computationally to carry
out the functions required of an XU. This is not really all that far-fetched.
For example, the user may be carrying a PDA or laptop in close proximity
that is sitting idle while the user is making a flight reservation using his or
her mobile phone.
It might make sense off-loading the computation to the

214
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
local device while the user is, for example, sitting on a train for several
hours.
The problem here is that the device hosting the local context may
itself run out of battery power or else may be shut off at any time; this
suggests that the signaling of the upload/download actions would most
probably have to be done by the end-users themselves and that fault-
tolerance considerations need to be incorporated into the final design.
This brings us to the issue of the actual transfer of the local state, which
requires all of the following :
1. The receiving XU must have sufficient resources to be able to add this
computation to its existing workload.
2. The receiving XU must have the required libraries available locally to
support the ongoing computation. Note that an XU that is underused can
prefetch libraries used at "neighboring" MSCs in anticipation of future
handovers.
3. There must be sufficient network resources to enable the transfer of state
information between the two XUs within a useful time frame. But note
that the timing constraints for this sort of handover are much less
stringent than they are for voice connections, and that they need not even
occur simultaneously with an ongoing voice handover.
In fact, if one
allows for a slight temporary increase of latency, one may permit a
nomadic user to move a certain number of "hops" away from its current
XU and hand over the computation only when the number of hops
exceeds a certain threshold or the user becomes stationary.
This
approach may be particularly attractive when complemented by the
previously mentioned idea of a "user-carried" XU that might be used,
e.g., on a longer high-speed trainjoumey.
3.1
Standardization of Execution Environment's
Parameters
We believe that a general adoption of our architecture would most
probably
lead
to
a
standardization
of the
execution
environment's
parameters, not just in terms of the libraries available (as in Sun's JavaOS
project) but in terms of quantifying actual processing and memory resources
in a standardized way independent of an actual processor platform.
Such
standardization not only aids in allocating resources when transferring
computations between XUs, but might also be useful for billing purposes.
For example, a user might subscribe to a guaranteed minimum workspace of
25kObjects and a minimum throughput of 50kTicks per minute, with
specified penalties for going over those limits. A combination of paid-for-
resource allocation and dynamic resource control is also an effective defense
against denial of service attacks.

A Fresh Look at Low-Power Mobile Computing
215
In our initial approach to exploring these issues, we are using the Pi
calculus and the Pict programming language [PT97] for systematically
modeling the connectivity, communication and mobility (MS moving from
one XU to another) arising in our architecture.
3.2
A Commercial Vision: Impact on Billing, Customer
Loyalty and Churn
Most likely, the provision of computational services could be turned into
a significant source of revenue for service providers.
On one hand, off-
loading the computations into the network makes for extremely cheap
mobile devices that can be distributed without a subsidy-"disposable"
devices are entirely feasible. On the other hand, it would enable a greater
range of services to be provided in the first place that might otherwise
bypass mobile service providers completely.
Furthermore, the available
level of network-based computation might tum into a key service distinction
between competing providers.
Of note is also that many of the envisioned applications are of the
"always on" kind, i.e., the application programs never "quit". These could
be structured in such a way that they increase customer "stickiness".
For
example, the user might have two applications implementing an electronic
organizer and an MP3 jukebox, both of which are permanently running on
his MS, hosted by the actual service provider. Although these are in reality
downloadable applications like any other, they could be marketed as a basic
service (i.e., not count towards the allocated "clicks" and "object space") and
as far as the user is concerned might just as well be built into the MS itself.
As a consequence of this marketing strategy, the user will not choose a
competing service from an external source.
But now, all user-defined
preferences and settings are "owned" by the network service provider,
reducing the users' propensity of changing providers.
4.
Segmentation of Functionality: The XU-MS Split
Client applications are jointly executed by the XU and the MS, but
creating the illusion to the server that all of the processing happens at the
geographic location of the MS. This leads to the question how functionality
is divided between the XU and the MS.
In the simplest case, the MS would simply be a dumb terminal.
The
XU would run a "virtual terminal" that translates GUI commands coming
from the client application to remote-terminal instructions for the MS. Since
all interaction with physical I/O devices in Java is via well-established

216
COMPILERSAND OPERATING SYSTEMS FOR LOW POWER
libraries, this would be very simple to accomplish in the Java case. We also
note that others have been successful at building an architecture in which
MS Windows GUI commands are intercepted on a server and relayed to a
client residing within a web browser [Citrix] , enabling the remote execution
of local desktop software from anywhere on the web-this is essentially the
exact same architecture as our proposed "dumb terminal" solution.
A much more challenging objective would be to actually perform part
of the processing on the MS itself.
For the purpose of minimizing power
consumption on the
MS,
one
needs
to include
the power used
for
communication in the overall cost equation.
Using a "dumb terminal"
approach, the mobile station is likely to be receiving much more data than it
needs to send, but there may still be significant number of handshakes and
other data exchanges that might be reduced using local processing on the
MS.
Note that the XU will be able to model current and future power
consumption of the MS quite accurately as it gathers usage statistics on a
per-application or even per-user basis.
The task of partitioning the application between a resource-constrained
MS and a remote XU is not unlike the task of partitioning an embedded
system (between slow but flexible software, and fast but rigid hardware).
There are also some similarities with partitioning for parallel execution,
particularly considering the fact that some partitionings may require more
communication among the participants than others.
Hence, it is quite likely
that
ideas
from
the
embedded
systems
community
and
the
parallel/distributed systems community can be adapted to this problem
domain.
There has also been promising research into automatic distributed
application decomposition [HS99] that is likely to influence our work.
Further commonalities exist with communication synthesis for distributed
embedded systems [OB98] , in which the goal is to automate the error-prone
task
of
crafting
appropriate
communication
protocols
between
the
constituent parts of a partitioned system.
It is also important to note that the current version of JVM-code is
unlikely to survive as the dominant mobile-code interchange format in the
longer-range future. As we have demonstrated in previous research [FK97,
ADFOl, ADROl), there are alternative formats not based on virtual machines
that provide much better performance than JVM does, and more easily
verifiable security.
It is likely that such future formats would include
annotations that would guide in the partitioning of functionality between XU
and MS. We are currently working on mechanisms for integrating specific
annotations describing programmer-specified parallelism and compiler-
derived parallelism into a mobile code format in a manner that is useful to a
parallelizing compiler back-end at the code recipient's site.

A Fresh Look at Low-Power Mobile Computing
217
4.1
Use of Field-Programmable Hardware in the Mobile
Station
The deployment of field-programmable hardware in mobile stations then
becomes an interesting possibility.
Ideally, the same analysis process that
partitions functionality between the XU and the MS could be taken one step
further, creating custom hardware on-the-fly on the MS where this is most
beneficial.
Such downloaded dedicated routines might include encryption
protocols, vocoders for speech (input, output, and telephony), and visual-
object rendering (texture mapping etc.) capabilities for gaming applications.
The availability of an appropriate (non-virtual-machine) intermediate
representation makes this task considerably simpler.
For example, at LlCl
we have been experimenting with mobile-code representations based on the
condensed graph model [Mor96].
Condensed graphs express Availability-
Driven, Coercion-Driven and Control-Driven Computing in a single unified
model of computation and have transformations that allow you to "tum up or
down"
the
amount of processing
power
required.
Essentially,
this
transforms the partitioning problem from a speculative, data driven model
using a substantial amount resources to a lazy, demand driven model that
tries
to
minimize
resource
usage
by
applying
simple
topological
transformations.
The availability of such a model in the mobile-code representation
itself significantly reduces the burden on the XU (or appropriate controller)
for computing such a model, and makes it possible to even adjust these
bindings on-the-fly in reaction to changing user activities on the MS: based
on a model of what the MS is doing, the XU at regular intervals sends it an
updated FPGA configuration.
4.2
Special End-To-End Application Requirements
Lastly, we observe that there are applications that require special care in
partitioning.
For example, communications with a banking application are
encrypted, with the assumption being that (1) the client program is actually
(atomically) executed at a single terminal location and (2) that this terminal
location provides a trusted computing infrastructure, i.e., the data path
between the mobile client program and the user (through the OS, web
browser, etc.) is not corrupted by an interposed malicious agent.
Obviously, condition (I) is violated in our architecture, and indeed the
data path between the XU and the MS is potentially much longer and subject
to eavesdropping and corruption.
Worse still, when roaming in foreign
networks, any potential secret would be readily available to the foreign
entities running such networks simply by observing the XU.

218
COMPILERS AND OPERATING SYSTEMS FOR LOW POWER
A solution to this problem lies in designing an appropriate type system
into the mobile-code transportation scheme itself that can directly express a
notion of confidentiality and trust.
The XU is then under the obligation to
partition the mobile client program in such a manner that the trusted parts
reside on the MS.
This property can be verified using proof-carrying
authentication protocols [AF99] incorporating the code being downloaded
itself-after download, the MS generates a response sent back to the server
that vouches for the fact that the trusted subset of the computation is now
running on a trusted host.
5.
Status and Research Vision
We are currently embarked on a substantial project (funded by NSF and
DARPA)
to
design
a new
mobile-code
distribution
architecture
that
reconciles provable security with execution efficiency.
A first major result of this project has been our discovery of a class of
mobile code representations that can encode only programs that provably
cannot harm the target machine-these mobile code formats therefore
provide security "by construction" rather than "by verification" as currently
required by Java [ADFOl , ADROl].
Our
project also
addresses
some
of the
issues of functionality
segmentation, as well as the required protocols to perform mobile-code
verification, dynamic translation, and execution at perhaps physically
disjoint sites. However, it does not currently address the handover issues we
have brought up here, nor does it currently address field programmable
hardware.
We
are
currently
building
a
substantial
prototype
that
incorporates and extends the architecture shown in Figure 12.1.
Our vision is to elevate good power management into a system-wide
quality of mobile-code systems. We believe to have identified several topics
requiring further research and are already embarked on the quest for
solutions.
Acknowledgments
This work was inspired by an afternoon that the author spent at the TIK
institute of ETH Zurich talking to B. Plattner, L. Thiele, and their associates.
Thanks are due to N. Dalton,
V. Haldar and W.-K. Hong for their
constructive comments. The author would also like to thank the anonymous
referees for their constructive comments.

A Fresh Look at Low-Power Mobile Computing
219
This research effort is partiall y funded by the U.S. Department of
Defense, Critical Infrastructure Protection and High Confidence, Adaptable
Software (CIP/SW) Research Program of the University Research Initiative
administered by the Office of Naval Research under agreement NOOO14-01-
1-0854, and by the National Science Foundation, Program in Operating
Systems and Compilers, under grant CCR-9901689.
References
[ADFOI]
W. Amme, N. Dalton, P. H. Frohlich, V. Haldar, P. S. Housel, 1. v. Ronne, Ch.
H. Stork, S. Zhenochin, and M. Franz; "Project transPRO se: Reconciling Mobile-
Code Security With Execution Efficiency"; in The Second DARPA Information
Survivability Conference and Exhibition (DISCEX II). Anaheim. California;
IEEE Computer Society Press, [SBN 0-7695-1212-7, pp. 11.196-11.2[0; 200 1.
[ADROI]
W. Amme, N. Dalton, J. v. Ronne, and M. Franz; "SafeTSA: A Type Safe and
Referentially
Secure
Mobile-Code
Representation
Based
on Static
Single
Assignment Form"; in Proceedings of the ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI 2001), Snowb ird,
Utah, pp. [37-147 ; 200 1.
[AF99)
A. W. Appel and E. W. Felten; "Proof-Carrying Authentication"; in Proceedings
of
the 6th ACM Conference on Computer and Communications Security.
Singapore; 1999.
[Citrix)
Citrix,
Inc.;
Citrixtr)
Independent
Computing
Architecture
and
Citrix
Metaltramel"; http://www.citrix.com.
[FK97]
M. Franz and T. Kistler; "Slim Binaries"; Communications of the ACM, 40:12,
pp. 87-94; 1997.
[HS99]
G. C. Hunt and M. L. Scott; "The Coign Automatic Distributed Partitioning
System"; in Proceedings ofthe Third Symposium on Operating Systems Design
and Implementation (OSDI'99), New Orleans, LA, pp. [87-200; [999.
[Mor96]
1. Morrison; Condensed Graphs: Unifying Availability-Driven. Coercion-Driven
and
Control-Driven
Computing;
Technische
Universiteit
Eindhoven,
ISBN-90-386-0478-5; 1996.
[OB98]
R. B. Ortega and G. Borriello; "Communication Synthe sis for Distributed
Embedded
Systems;
in
Proceedings of the
International
Conference
on
Computer Aided Design; pp 437-444; [99 8
[PT97]
B. C. Pierce and D. N. Turner; "Pict: A Programming Language Based on the Pi-
Calculus", in G. Plotkin , C. Stirling, and M. Tofte (Eds.), Proof. Language and
Interaction: Essays in Honour ofRobin Milner, MIT Press; 2000.


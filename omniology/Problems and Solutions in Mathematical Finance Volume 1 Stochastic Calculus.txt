

Problems and Solutions
in Mathematical Finance

For other titles in the Wiley Finance Series
please see www.wiley.com/finance

Problems and Solutions
in Mathematical Finance
Volume 1: Stochastic Calculus
Eric Chin, Dian Nel and Sverrir Ã“lafsson

This edition first published 2014
Â© 2014 John Wiley & Sons, Ltd
Registered office
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial offices, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our website at www.wiley.com.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any
form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK
Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with
standard print versions of this book may not be included in e-books or in print-on-demand. If this book refers to
media such as a CD or DVD that is not included in the version you purchased, you may download this material at
http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their
respective owners. The publisher is not associated with any product or vendor mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing
this book, they make no representations or warranties with the respect to the accuracy or completeness of the
contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular
purpose. It is sold on the understanding that the publisher is not engaged in rendering professional services and
neither the publisher nor the author shall be liable for damages arising herefrom. If professional advice or other
expert assistance is required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Chin, Eric,
Problems and solutions in mathematical finance : stochastic calculus / Eric Chin, Dian Nel and Sverrir Ã“lafsson.
pages cm
Includes bibliographical references and index.
ISBN 978-1-119-96583-1 (cloth)
1. Finance â€“ Mathematical models. 2. Stochastic analysis. I. Nel, Dian, II. Ã“lafsson, Sverrir, III. Title.
HG106.C495 2014
332.01â€²51922 â€“ dc23
2013043864
A catalogue record for this book is available from the British Library.
ISBN 978-1-119-96583-1 (hardback) ISBN 978-1-119-96607-4 (ebk)
ISBN 978-1-119-96608-1 (ebk)
ISBN 978-1-118-84514-1 (ebk)
Cover design: Cylinder
Typeset in 10/12pt TimesLTStd by Laserwords Private Limited, Chennai, India
Printed in Great Britain by CPI Group (UK) Ltd, Croydon, CR0 4YY

â€œthe beginning of a task is the biggest stepâ€
Plato, The Republic


Contents
Preface
ix
Prologue
xi
About the Authors
xv
1
General Probability Theory
1
1.1 Introduction
1
1.2 Problems and Solutions
4
1.2.1 Probability Spaces
4
1.2.2 Discrete and Continuous Random Variables
11
1.2.3 Properties of Expectations
41
2
Wiener Process
51
2.1 Introduction
51
2.2 Problems and Solutions
55
2.2.1 Basic Properties
55
2.2.2 Markov Property
68
2.2.3 Martingale Property
71
2.2.4 First Passage Time
76
2.2.5 Reflection Principle
84
2.2.6 Quadratic Variation
89
3
Stochastic Differential Equations
95
3.1 Introduction
95
3.2 Problems and Solutions
102
3.2.1 ItÂ¯o Calculus
102
3.2.2 One-Dimensional Diffusion Process
123
3.2.3 Multi-Dimensional Diffusion Process
155
4
Change of Measure
185
4.1 Introduction
185
4.2 Problems and Solutions
192
4.2.1 Martingale Representation Theorem
192

viii
Contents
4.2.2 Girsanovâ€™s Theorem
194
4.2.3 Risk-Neutral Measure
221
5
Poisson Process
243
5.1 Introduction
243
5.2 Problems and Solutions
251
5.2.1 Properties of Poisson Process
251
5.2.2 Jump Diffusion Process
281
5.2.3 Girsanovâ€™s Theorem for Jump Processes
298
5.2.4 Risk-Neutral Measure for Jump Processes
322
Appendix A Mathematics Formulae
331
Appendix B Probability Theory Formulae
341
Appendix C Differential Equations Formulae
357
Bibliography
365
Notation
369
Index
373

Preface
Mathematical finance is based on highly rigorous and, on occasions, abstract mathematical
structures that need to be mastered by anyone who wants to be successful in this field, be it
working as a quant in a trading environment or as an academic researcher in the field. It may
appear strange, but it is true, that mathematical finance has turned into one of the most advanced
and sophisticated field in applied mathematics. This development has had considerable impact
on financial engineering with its extensive applications to the pricing of contingent claims
and synthetic cash flows as analysed both within financial institutions (investment banks) and
corporations. Successful understanding and application of financial engineering techniques to
highly relevant and practical situations requires the mastering of basic financial mathematics.
It is precisely for this purpose that this book series has been written.
In Volume I, the first of a four volume work, we develop briefly all the major mathematical
concepts and theorems required for modern mathematical finance. The text starts with prob-
ability theory and works across stochastic processes, with main focus on Wiener and Poisson
processes. It then moves to stochastic differential equations including change of measure and
martingale representation theorems. However, the main focus of the book remains practical.
After being introduced to the fundamental concepts the reader is invited to test his/her knowl-
edge on a whole range of different practical problems. Whereas most texts on mathematical
finance focus on an extensive development of the theoretical foundations with only occasional
concrete problems, our focus is a compact and self-contained presentation of the theoretical
foundations followed by extensive applications of the theory. We advocate a more balanced
approach enabling the reader to develop his/her understanding through a step-by-step collec-
tion of questions and answers. The necessary foundation to solve these problems is provided
in a compact form at the beginning of each chapter. In our view that is the most successful way
to master this very technical field.
No one can write a book on mathematical finance today, not to mention four volumes, without
being influenced, both in approach and presentation, by some excellent text books in the field.
The texts we have mostly drawn upon in our research and teaching are (in no particular order
of preference), Tomas BjÃ¶rk, Arbitrage Theory in Continuous Time; Steven Shreve, Stochastic
Calculus for Finance; Marek Musiela and Marek Rutkowski, Martingale Methods in Financial
Modelling and for the more practical aspects of derivatives John Hull, Options, Futures and

x
Preface
Other Derivatives. For the more mathematical treatment of stochastic calculus a very influen-
tial text is that of Bernt Ã˜ksendal, Stochastic Differential Equations. Other important texts are
listed in the bibliography.
Note to the student/reader. Please try hard to solve the problems on your own before you
look at the solutions!

Prologue
IN THE BEGINNING WAS THE MOTION. . .
The development of modern mathematical techniques for financial applications can be traced
back to Bachelierâ€™s work, Theory of Speculation, first published as his PhD Thesis in 1900.
At that time Bachelier was studying the highly irregular movements in stock prices on the
French stock market. He was aware of the earlier work of the Scottish botanist Robert Brown,
in the year 1827, on the irregular movements of plant pollen when suspended in a fluid. Bache-
lier worked out the first mathematical model for the irregular pollen movements reported by
Brown, with the intention to apply it to the analysis of irregular asset prices. This was a highly
original and revolutionary approach to phenomena in finance. Since the publication of Bache-
lierâ€™s PhD thesis, there has been a steady progress in the modelling of financial asset prices.
Few years later, in 1905, Albert Einstein formulated a more extensive theory of irregular molec-
ular processes, already then called Brownian motion. That work was continued and extended
in the 1920s by the mathematical physicist Norbert Wiener who developed a fully rigorous
framework for Brownian motion processes, now generally called Wiener processes.
Other major steps that paved the way for further development of mathematical
finance included the works by Kolmogorov on stochastic differential equations, Fama
on efficient-market hypothesis and Samuelson on randomly fluctuating forward prices.
Further important developments in mathematical finance were fuelled by the realisation of the
importance of ItÂ¯oâ€™s lemma in stochastic calculus and the Feynman-Kac formula, originally
drawn from particle physics, in linking stochastic processes to partial differential equations
of parabolic type. The Feynman-Kac formula provides an immensely important tool for
the solution of partial differential equations â€œextractedâ€ from stochastic processes via ItÂ¯oâ€™s
lemma. The real relevance of ItÂ¯oâ€™s lemma and Feynman-Kac formula in finance were only
realised after some further substantial developments had taken place.
The year 1973 saw the most important breakthrough in financial theory when Black and
Scholes and subsequently Merton derived a model that enabled the pricing of European call and
put options. Their work had immense practical implications and lead to an explosive increase
in the trading of derivative securities on some major stock and commodity exchanges. How-
ever, the philosophical foundation of that approach, which is based on the construction of
risk-neutral portfolios enables an elegant and practical way of pricing of derivative contracts,
has had a lasting and revolutionary impact on the whole of mathematical finance. The devel-
opment initiated by Black, Scholes and Merton was continued by various researchers, notably
Harrison, Kreps and Pliska in 1980s. These authors established the hugely important role of

xii
Prologue
martingales and arbitrage theory for the pricing of a large class of derivative securities or,
as they are generally called, contingent claims. Already in the Black, Scholes and Merton
model the risk-neutral measure had been informally introduced as a consequence of the con-
struction of risk-neutral portfolios. Harrison, Kreps and Pliska took this development further
and turned it into a powerful and the most general tool presently available for the pricing of
contingent claims.
Within the Harrison, Kreps and Pliska framework the change of numÃ©raire technique plays
a fundamental role. Essentially the price of any asset, including contingent claims, can be
expressed in terms of units of any other asset. The unit asset plays the role of a numÃ©raire. For
a given asset and a selected numÃ©raire we can construct a probability measure that turns the
asset price, in units of the numÃ©raire, into a martingale whose existence is equivalent to the
absence of an arbitrage opportunity. These results amount to the deepest and most fundamental
in modern financial theory and are therefore a core construct in mathematical finance.
In the wake of the recent financial crisis, which started in the second half of 2007, some
commentators and academics have voiced their opinion that financial mathematicians and their
techniques are to be blamed for what happened. The authors do not subscribe to this view. On
the contrary, they believe that to improve the robustness and the soundness of financial con-
tracts, an even better mathematical training for quants is required. This encompasses a better
comprehension of all tools in the quantâ€™s technical toolbox such as optimisation, probability,
statistics, stochastic calculus and partial differential equations, just to name a few.
Financial market innovation is here to stay and not going anywhere, instead tighter regu-
lations and validations will be the only way forward with deeper understanding of models.
Therefore, new developments and market instruments requires more scrutiny, testing and val-
idation. Any inadequacies and weaknesses of model assumptions identified during the vali-
dation process should be addressed with appropriate reserve methodologies to offset sudden
changes in the market direction. The reserve methodologies can be subdivided into model
(e.g., Black-Scholes or Dupire model), implementation (e.g., tree-based or Monte Carlo sim-
ulation technique to price the contingent claim), calibration (e.g., types of algorithms to solve
optimization problems, interpolation and extrapolation methods when constructing volatility
surface), market parameters (e.g., confidence interval of correlation marking between under-
lyings) and market risk (e.g., when market price of a stock is close to the optionâ€™s strike price
at expiry time). These are the empirical aspects of mathematical finance that need to be a core
part in the further development of financial engineering.
One should keep in mind that mathematical finance is not, and must never become, an eso-
teric subject to be left to ivory tower academics alone, but a powerful tool for the analysis of
real financial scenarios, as faced by corporations and financial institutions alike. Mathematical
finance needs to be practiced in the real world for it to have sustainable benefits. Practitioners
must realise that mathematical analysis needs to be built on a clear formulation of financial
realities, followed by solid quantitative modelling, and then stress testing the model. It is our
view that the recent turmoil in financial markets calls for more careful application of quanti-
tative techniques but not their abolishment. Intuition alone or behavioural models have their
role to play but do not suffice when dealing with concrete financial realities such as, risk quan-
tification and risk management, asset and liability management, pricing insurance contracts
or complex financial instruments. These tasks require better and more relevant education for
quants and risk managers.
Financial mathematics is still a young and fast developing discipline. On the other hand, mar-
kets present an extremely complex and distributed system where a huge number of interrelated

Prologue
xiii
financial instruments are priced and traded. Financial mathematics is very powerful in pricing
and managing a limited number of instruments bundled into a portfolio. However, modern
financial mathematics is still rather poor at capturing the extremely intricate contractual inter-
relationship that exists between large numbers of traded securities. In other words, it is only
to a very limited extent able to capture the complex dynamics of the whole markets, which is
driven by a large number of unpredictable processes which possess varying degrees of corre-
lation. The emergent behaviour of the market is to an extent driven by these varying degrees of
correlations. It is perhaps one of the major present day challenges for financial mathematics to
join forces with modern theory of complexity with the aim of being able to capture the macro-
scopic properties of the market, that emerge from the microscopic interrelations between a
large number of individual securities. That this goal has not been reached yet is no criticism
of financial mathematics. It only bears witness to its juvenile nature and the huge complexity
of its subject.
Solid training of financial mathematicians in a whole range of quantitative disciplines,
including probability theory and stochastic calculus, is an important milestone in the further
development of the field. In the process, it is important to realise that financial engineering
needs more than just mathematics. It also needs a judgement where the quant should
constantly be reminded that no two market situations or two market instruments are exactly
the same. Applying the same mathematical tools to different situations reminds us of the
fact that we are always dealing with an approximation, which reflects the fact that we are
modelling stochastic processes i.e. uncertainties. Students and practitioners should always
bear this in mind.


About the Authors
Eric Chin is a quantitative analyst at an investment bank in the City of London where he
is involved in providing guidance on price testing methodologies and their implementation,
formulating model calibration and model appropriateness on commodity and credit products.
Prior to joining the banking industry he worked as a senior researcher at British Telecom inves-
tigating radio spectrum trading and risk management within the telecommunications sector. He
holds an MSc in Applied Statistics and an MSc in Mathematical Finance both from University
of Oxford. He also holds a PhD in Mathematics from University of Dundee.
Dian Nel has more than 10 years of experience in the commodities sector. He currently works
in the City of London where he specialises in oil and gas markets. He holds a BEng in Elec-
trical and Electronic Engineering from Stellenbosch University and an MSc in Mathematical
Finance from Christ Church, Oxford University. He is a Chartered Engineer registered with
the Engineering Council UK.
Sverrir Ã“lafsson is Professor of Financial Mathematics at Reykjavik University; a Visiting
Professor at Queen Mary University, London and a director of Riskcon Ltd, a UK based risk
management consultancy. Previously he was a Chief Researcher at BT Research and held
academic positions at The Mathematical Departments of Kings College, London; UMIST
Manchester and The University of Southampton. Dr Ã“lafsson is the author of over 95 ref-
ereed academic papers and has been a key note speaker at numerous international conferences
and seminars. He is on the editorial board of three international journals. He has provided an
extensive consultancy on financial risk management and given numerous specialist seminars to
finance specialists. In the last five years his main teaching has been MSc courses on Risk Man-
agement, Fixed Income, and Mathematical Finance. He has an MSc and PhD in mathematical
physics from the Universities of TÃ¼bingen and Karlsruhe respectively.


1
General Probability Theory
Probability theory is a branch of mathematics that deals with mathematical models of trials
whose outcomes depend on chance. Within the context of mathematical finance, we will review
some basic concepts of probability theory that are needed to begin solving stochastic calculus
problems. The topics covered in this chapter are by no means exhaustive but are sufficient to
be utilised in the following chapters and in later volumes. However, in order to fully grasp the
concepts, an undergraduate level of mathematics and probability theory is generally required
from the reader (see Appendices A and B for a quick review of some basic mathematics and
probability theory). In addition, the reader is also advised to refer to the notation section (pages
369â€“372) on set theory, mathematical and probability symbols used in this book.
1.1
INTRODUCTION
We consider an experiment or a trial whose result (outcome) is not predictable with certainty.
The set of all possible outcomes of an experiment is called the sample space and we denote it
by Î©. Any subset A of the sample space is known as an event, where an event is a set consisting
of possible outcomes of the experiment.
The collection of events can be defined as a subcollection â„±of the set of all subsets of Î©
and we define any collection â„±of subsets of Î© as a field if it satisfies the following.
Definition 1.1 The sample space Î© is the set of all possible outcomes of an experiment
or random trial. A field is a collection (or family) â„±of subsets of Î© with the following
conditions:
(a) âˆ…âˆˆâ„±where âˆ…is the empty set;
(b) if A âˆˆâ„±then Ac âˆˆâ„±where Ac is the complement of A in Î©;
(c) if A1, A2, . . . , An âˆˆâ„±, n â‰¥2 then â‹ƒn
i=1 Ai âˆˆâ„±â€“ that is to say, â„±is closed under finite
unions.
It should be noted in the definition of a field that â„±is closed under finite unions (as well
as under finite intersections). As for the case of a collection of events closed under countable
unions (as well as under countable intersections), any collection of subsets of Î© with such
properties is called a ğœ-algebra.
Definition 1.2 If Î© is a given sample space, then a ğœ-algebra (or ğœ-field) â„±on Î© is a family
(or collection) â„±of subsets of Î© with the following properties:
(a) âˆ…âˆˆâ„±;
(b) if A âˆˆâ„±then Ac âˆˆâ„±where Ac is the complement of A in Î©;
(c) if A1, A2, . . . âˆˆâ„±then â‹ƒâˆ
i=1 Ai âˆˆâ„±â€“ that is to say, â„±is closed under countable unions.

2
1.1
INTRODUCTION
We next outline an approach to probability which is a branch of measure theory. The reason
for taking a measure-theoretic path is that it leads to a unified treatment of both discrete and
continuous random variables, as well as a general definition of conditional expectation.
Definition 1.3 The pair (Î©, â„±) is called a measurable space. A probability measure â„™on a
measurable space (Î©, â„±) is a function â„™âˆ¶â„±î‚¶â†’[0, 1] such that:
(a) â„™(âˆ…) = 0;
(b) â„™(Î©) = 1;
(c) if A1, A2, . . . âˆˆâ„±and {Ai}âˆ
i=1 is disjoint such that Ai âˆ©Aj = âˆ…, i â‰ j then â„™(â‹ƒâˆ
i=1 Ai) =
âˆ‘âˆ
i=1 â„™(Ai).
The triple (Î©, â„±, â„™) is called a probability space. It is called a complete probability space
if â„±also contains subsets B of Î© with â„™-outer measure zero, that is â„™âˆ—(B) = inf{â„™(A) âˆ¶A âˆˆ
â„±, B âŠ‚A} = 0.
By treating ğœ-algebras as a record of information, we have the following definition of a
filtration.
Definition 1.4 Let Î© be a non-empty sample space and let T be a fixed positive number, and
assume for each t âˆˆ[0, T] there is a ğœ-algebra â„±t. In addition, we assume that if s â‰¤t, then
every set in â„±s is also in â„±t. We call the collection of ğœ-algebras â„±t, 0 â‰¤t â‰¤T, a filtration.
Below we look into the definition of a real-valued random variable, which is a function that
maps a probability space (Î©, â„±, â„™) to a measurable space â„.
Definition 1.5 Let Î© be a non-empty sample space and let â„±be a ğœ-algebra of subsets of Î©.
A real-valued random variable X is a function X âˆ¶Î© î‚¶â†’â„such that {ğœ”âˆˆÎ© âˆ¶X(ğœ”) â‰¤x}
âˆˆâ„±for each x âˆˆâ„and we say X is â„±measurable.
In the study of stochastic processes, an adapted stochastic process is one that cannot â€œsee
into the futureâ€ and in mathematical finance we assume that asset prices and portfolio positions
taken at time t are all adapted to a filtration â„±t, which we regard as the flow of information
up to time t. Therefore, these values must be â„±t measurable (i.e., depend only on information
available to investors at time t). The following is the precise definition of an adapted stochastic
process.
Definition 1.6 Let Î© be a non-empty sample space with a filtration â„±t, t âˆˆ[0, T] and let Xt
be a collection of random variables indexed by t âˆˆ[0, T]. We therefore say that this collection
of random variables is an adapted stochastic process if, for each t, the random variable Xt is
â„±t measurable.
Finally, we consider the concept of conditional expectation, which is extremely important
in probability theory and also for its wide application in mathematical finance such as pricing
options and other derivative products. Conceptually, we consider a random variable X defined
on the probability space (Î©, â„±, â„™) and a sub-ğœ-algebra ğ’¢of â„±(i.e., sets in ğ’¢are also in â„±).
Here X can represent a quantity we want to estimate, say the price of a stock in the future, while

1.1
INTRODUCTION
3
ğ’¢contains limited information about X such as the stock price up to and including the current
time. Thus, ğ”¼(X|ğ’¢) constitutes the best estimation we can make about X given the limited
knowledge ğ’¢. The following is a formal definition of a conditional expectation.
Definition 1.7 (Conditional Expectation) Let (Î©, â„±, â„™) be a probability space and let ğ’¢be
a sub-ğœ-algebra of â„±(i.e., sets in ğ’¢are also in â„±). Let X be an integrable (i.e., ğ”¼(|X|) < âˆ)
and non-negative random variable. Then the conditional expectation of X given ğ’¢, denoted
ğ”¼(X|ğ’¢), is any random variable that satisfies:
(a) ğ”¼(X|ğ’¢) is ğ’¢measurable;
(b) for every set A âˆˆğ’¢, we have the partial averaging property
âˆ«A
ğ”¼(X|ğ’¢) dâ„™= âˆ«A
X dâ„™.
From the above definition, we can list the following properties of conditional expectation.
Here (Î©, â„±, â„™) is a probability space, ğ’¢is a sub-ğœ-algebra of â„±and X is an integrable random
variable.
â€¢ Conditional probability. If 1IA is an indicator random variable for an event A then
ğ”¼(1IA|ğ’¢) = â„™(A|ğ’¢).
â€¢ Linearity. If X1, X2, . . . , Xn are integrable random variables and c1, c2, . . . , cn are con-
stants then
ğ”¼(c1X1 + c2X2 + . . . + cnXn|ğ’¢) = c1ğ”¼(X1|ğ’¢) + c2ğ”¼(X2|ğ’¢) + . . . + cnğ”¼(Xn|ğ’¢).
â€¢ Positivity. If X â‰¥0 almost surely then ğ”¼(X|ğ’¢) â‰¥0 almost surely.
â€¢ Monotonicity. If X and Y are integrable random variables and X â‰¤Y almost surely then
ğ”¼(X|ğ’¢) â‰¤ğ”¼(Y|ğ’¢).
â€¢ Computing expectations by conditioning. ğ”¼[ğ”¼(X|ğ’¢)] = ğ”¼(X).
â€¢ Taking out what is known. If X and Y are integrable random variables and X is ğ’¢measur-
able then
ğ”¼(XY|ğ’¢) = X â‹…ğ”¼(Y|ğ’¢).
â€¢ Tower property. If â„‹is a sub-ğœ-algebra of ğ’¢then
ğ”¼[ğ”¼(X|ğ’¢)|â„‹] = ğ”¼(X|â„‹).
â€¢ Measurability. If X is ğ’¢measurable then ğ”¼(X|ğ’¢) = X.
â€¢ Independence. If X is independent of ğ’¢then ğ”¼(X|ğ’¢) = ğ”¼(X).
â€¢ Conditional Jensenâ€™s inequality. If ğœ‘âˆ¶â„î‚¶â†’â„is a convex function then
ğ”¼[ğœ‘(X)|ğ’¢] â‰¥ğœ‘[ğ”¼(X|ğ’¢)].

4
1.2.1
Probability Spaces
1.2
PROBLEMS AND SOLUTIONS
1.2.1
Probability Spaces
1. De Morganâ€™s Law. Let Ai, i âˆˆI where I is some, possibly uncountable, indexing set.
Show that
(a) (â‹ƒ
iâˆˆI Ai
)c = â‹‚
iâˆˆI Ac
i .
(b) (â‹‚
iâˆˆI Ai
)c = â‹ƒ
iâˆˆI Ac
i .
Solution:
(a) Let a âˆˆ(â‹ƒ
iâˆˆI Ai
)c which implies a âˆ‰â‹ƒ
iâˆˆI Ai, so that a âˆˆAc
i for all i âˆˆI. Therefore,
(
â‹ƒ
iâˆˆI
Ai
)c
âŠ†
â‹‚
iâˆˆI
Ac
i .
On the contrary, if we let a âˆˆâ‹‚
iâˆˆIAc
i then a âˆ‰Ai for all i âˆˆI or a âˆˆ(â‹ƒ
iâˆˆI Ai
)c and
hence
â‹‚
iâˆˆI
Ac
i âŠ†
(
â‹ƒ
iâˆˆI
Ai
)c
.
Therefore, (â‹ƒ
iâˆˆI Ai
)c = â‹‚
iâˆˆIAc
i .
(b) From (a), we can write
(
â‹ƒ
iâˆˆI
Ac
i
)c
=
â‹‚
iâˆˆI
(Ac
i
)c =
â‹‚
iâˆˆI
Ai.
Taking complements on both sides gives
(
â‹‚
iâˆˆI
Ai
)c
=
â‹ƒ
iâˆˆI
Ac
i .
â—½
2. Let â„±be a ğœ-algebra of subsets of the sample space Î©. Show that if A1, A2, . . . âˆˆâ„±then
â‹‚âˆ
i=1 Ai âˆˆâ„±.
Solution: Given that â„±is a ğœ-algebra then Ac
1, Ac
2, . . . âˆˆâ„±and â‹ƒâˆ
i=1 Ac
i âˆˆâ„±. Further-
more, the complement of â‹ƒâˆ
i=1 Ac
i is (â‹ƒâˆ
i=1 Ac
i
)c âˆˆâ„±.
Thus, from De Morganâ€™s law (see Problem 1.2.1.1, page 4) we have (â‹ƒâˆ
i=1 Ac
i
)c =
â‹‚âˆ
i=1
(Ac
i
)c = â‹‚âˆ
i=1 Ai âˆˆâ„±.
â—½
3. Show that if â„±is a ğœ-algebra of subsets of Î© then {âˆ…, Î©} âˆˆâ„±.
Solution: â„±is a ğœ-algebra of subsets of Î©, hence if A âˆˆâ„±then Ac âˆˆâ„±.
Since âˆ…âˆˆâ„±then âˆ…c = Î© âˆˆâ„±. Thus, {âˆ…, Î©} âˆˆâ„±.
â—½

1.2.1
Probability Spaces
5
4. Show that if A âŠ†Î© then â„±= {âˆ…, Î©, A, Ac} is a ğœ-algebra of subsets of Î©.
Solution: â„±= {âˆ…, Î©, A, Ac} is a ğœ-algebra of subsets of Î© since
(i) âˆ…âˆˆâ„±.
(ii) For âˆ…âˆˆâ„±then âˆ…c = Î© âˆˆâ„±. For Î© âˆˆâ„±then Î©c = âˆ…âˆˆâ„±. In addition, for A âˆˆâ„±
then Ac âˆˆâ„±. Finally, for Ac âˆˆâ„±then (Ac)c = A âˆˆâ„±.
(iii) âˆ…âˆªÎ© = Î© âˆˆâ„±,
âˆ…âˆªA = A âˆˆâ„±,
âˆ…âˆªAc = Ac âˆˆâ„±,
Î© âˆªA = Î© âˆˆâ„±,
Î© âˆª
Ac = Î© âˆˆâ„±, âˆ…âˆªÎ© âˆªA = Î© âˆˆâ„±, âˆ…âˆªÎ© âˆªAc = Î© âˆˆâ„±and Î© âˆªA âˆªAc = Î© âˆˆâ„±.
â—½
5. Let {â„±i}iâˆˆI, I â‰ âˆ…be a family of ğœ-algebras of subsets of the sample space Î©. Show that
â„±= â‹‚
iâˆˆI â„±i is also a ğœ-algebra of subsets of Î©.
Solution: â„±= â‹‚
iâˆˆI â„±i is a ğœ-algebra by taking note that
(a) Since âˆ…âˆˆâ„±i, i âˆˆI therefore âˆ…âˆˆâ„±as well.
(b) If A âˆˆâ„±i for all i âˆˆI then Ac âˆˆâ„±i, i âˆˆI. Therefore, A âˆˆâ„±and hence Ac âˆˆâ„±.
(c) If A1, A2, . . . âˆˆâ„±i for all i âˆˆI then â‹ƒâˆ
k=1 Ak âˆˆâ„±i, i âˆˆI and hence A1, A2, . . . âˆˆâ„±
and â‹ƒâˆ
k=1 Ak âˆˆâ„±.
From the results of (a)â€“(c) we have shown â„±= â‹‚
iâˆˆIâ„±i is a ğœ-algebra of Î©.
â—½
6. Let Î© = {ğ›¼, ğ›½, ğ›¾} and let
â„±1 = {âˆ…, Î©, {ğ›¼}, {ğ›½, ğ›¾}}
and
â„±2 = {âˆ…, Î©, {ğ›¼, ğ›½}, {ğ›¾}}.
Show that â„±1 and â„±2 are ğœ-algebras of subsets of Î©.
Is â„±= â„±1 âˆªâ„±2 also a ğœ-algebra of subsets of Î©?
Solution: Following the steps given in Problem 1.2.1.4 (page 5) we can easily show â„±1
and â„±2 are ğœ-algebras of subsets of Î©.
By setting â„±= â„±1 âˆªâ„±2 = {âˆ…, Î©, {ğ›¼}, {ğ›¾}, {ğ›¼, ğ›½}, {ğ›½, ğ›¾}}, and since {ğ›¼} âˆˆâ„±and {ğ›¾} âˆˆ
â„±, but {ğ›¼} âˆª{ğ›¾} = {ğ›¼, ğ›¾} âˆ‰â„±, then â„±= â„±1 âˆªâ„±2 is not a ğœ-algebra of subsets of Î©.
â—½
7. Let â„±be a ğœ-algebra of subsets of Î© and suppose â„™âˆ¶â„±î‚¶â†’[0, 1] so that â„™(Î©) = 1. Show
that â„™(âˆ…) = 0.
Solution: Given that âˆ…and Î© are mutually exclusive we therefore have
âˆ…âˆ©Î© = âˆ…and âˆ…âˆªÎ© = Î©.
Thus, we can express
â„™(âˆ…âˆªÎ©) = â„™(âˆ…) + â„™(Î©) âˆ’â„™(âˆ…âˆ©Î©) = 1.
Since â„™(Î©) = 1 and â„™(âˆ…âˆ©Î©) = 0 therefore â„™(âˆ…) = 0.
â—½

6
1.2.1
Probability Spaces
8. Let (Î©, â„±, â„™) be a probability space and let â„šâˆ¶â„±î‚¶â†’[0, 1] be defined by â„š(A) = â„™(A|B)
where B âˆˆâ„±such that â„™(B) > 0. Show that (Î©, â„±, â„š) is also a probability space.
Solution: To show that (Î©, â„±, â„š) is a probability space we note that
(a) â„š(âˆ…) = â„™(âˆ…|B) = â„™(âˆ…âˆ©B)
â„™(B)
= â„™(âˆ…)
â„™(B) = 0.
(b) â„š(Î©) = â„™(Î©|B) = â„™(Î© âˆ©B)
â„™(B)
= â„™(B)
â„™(B) = 1.
(c) Let A1, A2, . . . be disjoint members of â„±and hence we can imply A1 âˆ©B, A2 âˆ©B, . . .
are also disjoint members of â„±. Therefore,
â„š
( âˆ
â‹ƒ
i=1
Ai
)
= â„™
( âˆ
â‹ƒ
i=1
Ai
|||||
B
)
=
â„™(â‹ƒâˆ
i=1(Ai âˆ©B))
â„™(B)
=
âˆ
âˆ‘
i=1
â„™(Ai âˆ©B)
â„™(B)
=
âˆ
âˆ‘
i=1
â„š(Ai).
Based on the results of (a)â€“(c), we have shown that (Î©, â„±, â„š) is also a probability space.
â—½
9. Booleâ€™s Inequality. Suppose {Ai}iâˆˆI is a countable collection of events. Show that
â„™
(
â‹ƒ
iâˆˆI
Ai
)
â‰¤
âˆ‘
iâˆˆI
â„™(Ai
) .
Solution: Without loss of generality we assume that I = {1, 2, . . .} and define B1 = A1,
Bi = Ai\(A1 âˆªA2 âˆª. . . âˆªAiâˆ’1), i âˆˆ{2, 3, . . .} such that {B1, B2, . . .} are pairwise dis-
joint and
â‹ƒ
iâˆˆI
Ai =
â‹ƒ
iâˆˆI
Bi.
Because Bi âˆ©Bj = âˆ…, i â‰ j, i, j âˆˆI we have
â„™
(
â‹ƒ
iâˆˆI
Ai
)
= â„™
(
â‹ƒ
iâˆˆI
Bi
)
=
âˆ‘
iâˆˆI
â„™(Bi)
=
âˆ‘
iâˆˆI
â„™(Ai\(A1 âˆªA2 âˆª. . . âˆªAiâˆ’1))
=
âˆ‘
iâˆˆI
{â„™(Ai) âˆ’â„™(Ai âˆ©(A1 âˆªA2 âˆª. . . âˆªAiâˆ’1))}
â‰¤
âˆ‘
iâˆˆI
â„™(Ai).
â—½

1.2.1
Probability Spaces
7
10. Bonferroniâ€™s Inequality. Suppose {Ai}iâˆˆI is a countable collection of events. Show that
â„™
(
â‹‚
iâˆˆI
Ai
)
â‰¥1 âˆ’
âˆ‘
iâˆˆI
â„™(Ac
i
) .
Solution: From De Morganâ€™s law (see Problem 1.2.1.1, page 4) we can write
â„™
(
â‹‚
iâˆˆI
Ai
)
= â„™
((
â‹ƒ
iâˆˆI
Ac
i
)c)
= 1 âˆ’â„™
(
â‹ƒ
iâˆˆI
Ac
i
)
.
By applying Booleâ€™s inequality (see Problem 1.2.1.9, page 6) we will have
â„™
(
â‹‚
iâˆˆI
Ai
)
â‰¥1 âˆ’
âˆ‘
iâˆˆI
â„™(Ac
i
)
since â„™(â‹ƒ
iâˆˆIAc
i
) â‰¤âˆ‘
iâˆˆIâ„™(Ac
i
).
â—½
11. Bayesâ€™ Formula. Let A1, A2, . . . , An be a partition of Î©, where
nâ‹ƒ
i=1
Ai = Î©, Ai âˆ©Aj = âˆ…,
i â‰ j and each Ai, i, j = 1, 2, . . . , n has positive probability. Show that
â„™(Ai|B) =
â„™(B|Ai)â„™(Ai)
âˆ‘n
j=1 â„™(B|Aj)â„™(Aj).
Solution: From the definition of conditional probability, for i = 1, 2, . . . , n
â„™(Ai|B) = â„™(Ai âˆ©B)
â„™(B)
=
â„™(B|Ai)â„™(Ai)
â„™
(â‹ƒn
j=1(B âˆ©Aj)
) = â„™(B|Ai)â„™(Ai)
âˆ‘n
j=1 â„™(B âˆ©Aj) =
â„™(B|Ai)â„™(Ai)
âˆ‘n
j=1 â„™(B|Aj)â„™(Aj).
â—½
12. Principle of Inclusion and Exclusion for Probability. Let A1, A2, . . . , An, n â‰¥2 be a col-
lection of events. Show that
â„™(A1 âˆªA2) = â„™(A1) + â„™(A2) âˆ’â„™(A1 âˆ©A2).
From the above result show that
â„™(A1 âˆªA2 âˆªA3) = â„™(A1) + â„™(A2) + â„™(A3) âˆ’â„™(A1 âˆ©A2) âˆ’â„™(A1 âˆ©A3)
âˆ’â„™(A2 âˆ©A3) + â„™(A1 âˆ©A2 âˆ©A3).
Hence, using mathematical induction show that
â„™
( n
â‹ƒ
i=1
Ai
)
=
n
âˆ‘
i=1
â„™(Ai) âˆ’
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
â„™(Ai âˆ©Aj) +
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
â„™(Ai âˆ©Aj âˆ©Ak)

8
1.2.1
Probability Spaces
âˆ’. . . + (âˆ’1)n+1â„™(A1 âˆ©A2 âˆ©. . . âˆ©An).
Finally, deduce that
â„™
( n
â‹‚
i=1
Ai
)
=
n
âˆ‘
i=1
â„™(Ai) âˆ’
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
â„™(Ai âˆªAj) +
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
â„™(Ai âˆªAj âˆªAk)
âˆ’. . . + (âˆ’1)n+1â„™(A1 âˆªA2 âˆª. . . âˆªAn).
Solution: For n = 2, A1 âˆªA2 can be written as a union of two disjoint sets
A1 âˆªA2 = A1 âˆª(A2\ A1) = A1 âˆª(A2 âˆ©Ac
1).
Therefore,
â„™(A1 âˆªA2) = â„™(A1) + â„™(A2 âˆ©Ac
1)
and since â„™(A2) = â„™(A2 âˆ©A1) + â„™(A2 âˆ©Ac
1) we will have
â„™(A1 âˆªA2) = â„™(A1) + â„™(A2) âˆ’â„™(A1 âˆ©A2).
For n = 3, and using the above results, we can write
â„™(A1 âˆªA2 âˆªA3) = â„™(A1 âˆªA2) + â„™(A3) âˆ’â„™[(A1 âˆªA2) âˆ©A3]
= â„™(A1) + â„™(A2) + â„™(A3) âˆ’â„™(A1 âˆ©A2) âˆ’â„™[(A1 âˆªA2) âˆ©A3].
Since (A1 âˆªA2) âˆ©A3 = (A1 âˆ©A3) âˆª(A2 âˆ©A3) therefore
â„™[(A1 âˆªA2) âˆ©A3] = â„™[(A1 âˆ©A3) âˆª(A2 âˆ©A3)]
= â„™(A1 âˆ©A3) + â„™(A2 âˆ©A3) âˆ’â„™[(A1 âˆ©A3) âˆ©(A2 âˆ©A3)]
= â„™(A1 âˆ©A3) + â„™(A2 âˆ©A3) âˆ’â„™(A1 âˆ©A2 âˆ©A3).
Thus,
â„™(A1 âˆªA2 âˆªA3) = â„™(A1) + â„™(A2) + â„™(A3) âˆ’â„™(A1 âˆ©A2) âˆ’â„™(A1 âˆ©A3)
âˆ’â„™(A2 âˆ©A3) + â„™(A1 âˆ©A2 âˆ©A3).
Suppose the result is true for n = m, where m â‰¥2. For n = m + 1, we have
â„™
(( m
â‹ƒ
i=1
Ai
)
âˆªAm+1
)
= â„™
( m
â‹ƒ
i=1
Ai
)
+ â„™(Am+1
) âˆ’â„™
(( m
â‹ƒ
i=1
Ai
)
âˆ©Am+1
)
= â„™
( m
â‹ƒ
i=1
Ai
)
+ â„™(Am+1
) âˆ’â„™
( m
â‹ƒ
i=1
(Ai âˆ©Am+1
)
)
.

1.2.1
Probability Spaces
9
By expanding the terms we have
â„™
(m+1
â‹ƒ
i=1
Ai
)
=
m
âˆ‘
i=1
â„™(Ai) âˆ’
mâˆ’1
âˆ‘
i=1
m
âˆ‘
j=i+1
â„™(Ai âˆ©Aj) +
mâˆ’2
âˆ‘
i=1
mâˆ’1
âˆ‘
j=i+1
m
âˆ‘
k=j+1
â„™(Ai âˆ©Aj âˆ©Ak)
âˆ’. . . + (âˆ’1)m+1â„™(A1 âˆ©A2 âˆ©. . . âˆ©Am)
+â„™(Am+1
) âˆ’â„™((A1 âˆ©Am+1) âˆª(A2 âˆ©Am+1) . . . âˆª(Am âˆ©Am+1))
=
m+1
âˆ‘
i=1
â„™(Ai) âˆ’
mâˆ’1
âˆ‘
i=1
m
âˆ‘
j=i+1
â„™(Ai âˆ©Aj) +
mâˆ’2
âˆ‘
i=1
mâˆ’1
âˆ‘
j=i+1
m
âˆ‘
k=j+1
â„™(Ai âˆ©Aj âˆ©Ak)
âˆ’. . . + (âˆ’1)m+1â„™(A1 âˆ©A2 âˆ©. . . âˆ©Am)
âˆ’
m
âˆ‘
i=1
â„™(Ai âˆ©Am+1) +
mâˆ’1
âˆ‘
i=1
m
âˆ‘
j=i+1
â„™(Ai âˆ©Aj âˆ©Am+1)
+ . . . âˆ’(âˆ’1)m+1â„™(A1 âˆ©A2 âˆ©. . . âˆ©Am+1)
=
m+1
âˆ‘
i=1
â„™(Ai) âˆ’
m
âˆ‘
i=1
m+1
âˆ‘
j=i+1
â„™(Ai âˆ©Aj) +
mâˆ’1
âˆ‘
i=1
m
âˆ‘
j=i+1
m+1
âˆ‘
k=j+1
â„™(Ai âˆ©Aj âˆ©Ak)
âˆ’. . . + (âˆ’1)m+2â„™(A1 âˆ©A2 âˆ©. . . âˆ©Am+1).
Therefore, the result is also true for n = m + 1. Thus, from mathematical induction we
have shown for n â‰¥2,
â„™
( n
â‹ƒ
i=1
Ai
)
=
n
âˆ‘
i=1
â„™(Ai) âˆ’
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
â„™(Ai âˆ©Aj) +
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
â„™(Ai âˆ©Aj âˆ©Ak)
âˆ’. . . + (âˆ’1)n+1â„™(A1 âˆ©A2 âˆ©. . . âˆ©An).
From Problem 1.2.1.1 (page 4) we can write
â„™
( n
â‹‚
i=1
Ai
)
= â„™
(( n
â‹ƒ
i=1
Ac
i
)c)
= 1 âˆ’â„™
( n
â‹ƒ
i=1
Ac
i
)
.
Thus, we can expand
â„™
( n
â‹‚
i=1
Ai
)
= 1 âˆ’
n
âˆ‘
i=1
â„™(Ac
i ) +
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
â„™(Ac
i âˆ©Ac
j ) âˆ’
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
â„™(Ac
i âˆ©Ac
j âˆ©Ac
k)
+ . . . âˆ’(âˆ’1)n+1â„™(Ac
1 âˆ©Ac
2 âˆ©. . . âˆ©Ac
n)
= 1 âˆ’
n
âˆ‘
i=1
(1 âˆ’â„™(Ai)) +
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
(1 âˆ’â„™(Ai âˆªAj))
âˆ’
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
(1 âˆ’â„™(Ai âˆªAj âˆªAk))

10
1.2.1
Probability Spaces
+ . . . âˆ’(âˆ’1)n+1(1 âˆ’â„™(A1 âˆªA2 âˆª. . . âˆªAn))
=
n
âˆ‘
i=1
â„™(Ai) âˆ’
nâˆ’1
âˆ‘
i=1
n
âˆ‘
j=i+1
â„™(Ai âˆªAj) +
nâˆ’2
âˆ‘
i=1
nâˆ’1
âˆ‘
j=i+1
n
âˆ‘
k=j+1
â„™(Ai âˆªAj âˆªAk)
âˆ’. . . + (âˆ’1)n+1â„™(A1 âˆªA2 âˆª. . . âˆªAn).
â—½
13. Borelâ€“Cantelli Lemma. Let (Î©, â„±, â„™) be a probability space and let A1, A2, . . . be sets in
â„±. Show that
âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak âŠ†
âˆ
â‹ƒ
k=m
Ak
and hence prove that
â„™
( âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak
)
=
â§
âª
âª
â¨
âª
âªâ©
1
if Ai âˆ©Aj = âˆ…, i â‰ j and
âˆâˆ‘
k=1
â„™(Ak) = âˆ
0
if
âˆâˆ‘
k=1
â„™(Ak) < âˆ.
Solution: Let Bm =
âˆâ‹ƒ
k=m
Ak and since
âˆ
â‹‚
m=1
Bm âŠ†Bm therefore we have
âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak âŠ†
âˆ
â‹ƒ
k=m
Ak.
From Problem 1.2.1.9 (page 6) we can deduce that
â„™
( âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak
)
â‰¤â„™
( âˆ
â‹ƒ
k=m
Ak
)
â‰¤
âˆ
âˆ‘
k=m
â„™(Ak).
For the case
âˆâˆ‘
k=1
â„™(Ak) < âˆand given it is a convergent series, then lim
mâ†’âˆ
âˆâˆ‘
k=m
â„™(Ak) = 0
and hence
â„™
( âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak
)
= 0
if
âˆâˆ‘
k=1
â„™(Ak) < âˆ.
For the case Ai âˆ©Aj = âˆ…, i â‰ j and
âˆâˆ‘
k=1
â„™(Ak) = âˆ, since â„™
( âˆ
â‹ƒ
k=m
Ak
)
+ â„™
(( âˆâ‹ƒ
k=m
Ak
)c)
=
1 therefore from Problem 1.2.1.1 (page 4)
â„™
( âˆ
â‹ƒ
k=m
Ak
)
= 1 âˆ’â„™
(( âˆ
â‹ƒ
k=m
Ak
)c)
= 1 âˆ’â„™
( âˆ
â‹‚
k=m
Ac
k
)
.

1.2.2
Discrete and Continuous Random Variables
11
From independence and because
âˆâˆ‘
k=1
â„™(Ak) = âˆwe can express
â„™
( âˆ
â‹‚
k=m
Ac
k
)
=
âˆ
âˆ
k=m
â„™(Ac
k
) =
âˆ
âˆ
k=m
(1 âˆ’â„™(Ak
)) â‰¤
âˆ
âˆ
k=m
eâˆ’â„™(Ak) = eâˆ’âˆ‘âˆ
k=m â„™(Ak) = 0
for all m and hence
â„™
( âˆ
â‹ƒ
k=m
Ak
)
= 1 âˆ’â„™
( âˆ
â‹‚
k=m
Ac
k
)
= 1
for all m. Taking the limit m â†’âˆ,
â„™
( âˆ
â‹‚
m=1
âˆ
â‹ƒ
k=m
Ak
)
= lim
mâ†’âˆâ„™
( âˆ
â‹ƒ
k=m
Ak
)
= 1
for the case Ai âˆ©Aj = âˆ…, i â‰ j and
âˆâˆ‘
k=1
â„™(Ak) = âˆ.
â—½
1.2.2
Discrete and Continuous Random Variables
1. Bernoulli Distribution. Let X be a Bernoulli random variable, X âˆ¼Bernoulli(p), p âˆˆ[0, 1]
with probability mass function
â„™(X = 1) = p,
â„™(X = 0) = 1 âˆ’p.
Show that ğ”¼(X) = p and Var(X) = p(1 âˆ’p).
Solution: If X âˆ¼Bernoulli(p) then we can write
â„™(X = x) = px(1 âˆ’p)1âˆ’x,
x âˆˆ{0, 1}
and by definition
ğ”¼(X) =
1
âˆ‘
x=0
xâ„™(X = x) = 0 â‹…(1 âˆ’p) + 1 â‹…p = p
ğ”¼(X2) =
1
âˆ‘
x=0
x2â„™(X = x) = 0 â‹…(1 âˆ’p) + 1 â‹…p = p
and hence
ğ”¼(X) = p,
Var(X) = ğ”¼(X2) âˆ’ğ”¼(X)2 = p(1 âˆ’p).
â—½

12
1.2.2
Discrete and Continuous Random Variables
2. Binomial Distribution. Let {Xi}n
i=1 be a sequence of independent Bernoulli random vari-
ables each with probability mass function
â„™(Xi = 1) = p,
â„™(Xi = 0) = 1 âˆ’p,
p âˆˆ[0, 1]
and let
X =
n
âˆ‘
i=1
Xi.
Show that X follows a binomial distribution, X âˆ¼Binomial(n, p) with probability mass
function
â„™(X = k) =
(
n
k
)
pk(1 âˆ’p)nâˆ’k,
k = 0, 1, 2, . . . , n
such that ğ”¼(X) = np and Var(X) = np(1 âˆ’p).
Using the central limit theorem show that X is approximately normally distributed, X âˆ»
ğ’©(np, np(1 âˆ’p)) as n â†’âˆ.
Solution: The random variable X counts the number of Bernoulli variables X1, . . . , Xn
that are equal to 1, i.e., the number of successes in the n independent trials. Clearly X takes
values in the set N = {0, 1, 2, . . . , n}. To calculate the probability that X = k, where k âˆˆN
is the number of successes we let E be the event such that Xi1 = Xi2 = . . . = Xik = 1 and
Xj = 0 for all j âˆˆN\S where S = {i1, i2, . . . , ik}. Then, because the Bernoulli variables
are independent and identically distributed,
â„™(E) =
âˆ
jâˆˆS
â„™(Xj = 1)
âˆ
jâˆˆN\S
â„™(Xj = 0) = pk(1 âˆ’p)nâˆ’k.
However, as there are
(
n
k
)
combinations to select sets of indices i1, . . . , ik from N, which
are mutually exclusive events, so
â„™(X = k) =
(
n
k
)
pk(1 âˆ’p)nâˆ’k,
k = 0, 1, 2, . . . , n.
From the definition of the moment generating function of discrete random variables (see
Appendix B),
MX(t) = ğ”¼(etX) =
âˆ‘
x
etxâ„™(X = x)
where t âˆˆâ„and substituting â„™(X = x) =
(
n
x
)
px(1 âˆ’p)nâˆ’x we have
MX(t) =
n
âˆ‘
x=0
etx
(
n
x
)
px(1 âˆ’p)nâˆ’x =
n
âˆ‘
x=0
(
n
x
)
(pet)x(1 âˆ’p)nâˆ’x = (1 âˆ’p + pet)n.

1.2.2
Discrete and Continuous Random Variables
13
By differentiating MX(t) with respect to t twice we have
Mâ€²
X(t) = npet(1 âˆ’p + pet)nâˆ’1
Mâ€²â€²
X(t) = npet(1 âˆ’p + pet)nâˆ’1 + n(n âˆ’1)p2e2t(1 âˆ’p + pet)nâˆ’2
and hence
ğ”¼(X) = Mâ€²
X(0) = np
Var(X) = ğ”¼(X2) âˆ’ğ”¼(X)2 = Mâ€²â€²
X(0) âˆ’Mâ€²
X(0)2 = np(1 âˆ’p).
Given the sequence Xi âˆ¼Bernoulli(p), i = 1, 2, . . . , n are independent and identically dis-
tributed, each having expectation ğœ‡= p and variance ğœ2 = p(1 âˆ’p), then as n â†’âˆ, from
the central limit theorem
âˆ‘n
i=1 Xi âˆ’nğœ‡
ğœ
âˆš
n
D
âˆ’âˆ’â†’ğ’©(0, 1)
or
X âˆ’np
âˆš
np(1 âˆ’p)
D
âˆ’âˆ’â†’ğ’©(0, 1).
Thus, as n â†’âˆ, X âˆ»ğ’©(np, np(1 âˆ’p)).
â—½
3. Poisson Distribution. A discrete Poisson distribution, Poisson(ğœ†) with parameter ğœ†> 0
has the following probability mass function:
â„™(X = k) = ğœ†k
k! eâˆ’ğœ†,
k = 0, 1, 2, . . .
Show that ğ”¼(X) = ğœ†and Var(X) = ğœ†.
For a random variable following a binomial distribution, Binomial(n, p), 0 â‰¤p â‰¤1 show
that as n â†’âˆand with p = ğœ†âˆ•n, the binomial distribution tends to the Poisson distribution
with parameter ğœ†.
Solution: From the definition of the moment generating function
MX(t) = ğ”¼(etX) =
âˆ‘
x
etxâ„™(X = x)
where t âˆˆâ„and substituting â„™(X = x) = ğœ†x
x! eâˆ’ğœ†we have
MX(t) =
âˆ
âˆ‘
x=0
etx ğœ†x
x! eâˆ’ğœ†= eâˆ’ğœ†
âˆ
âˆ‘
x=0
(ğœ†et)x
x!
= eğœ†(etâˆ’1).
By differentiating MX(t) with respect to t twice
Mâ€²
X(t) = ğœ†eteğœ†(etâˆ’1),
Mâ€²â€²
X(t) = (ğœ†et + 1)ğœ†eteğœ†(etâˆ’1)

14
1.2.2
Discrete and Continuous Random Variables
we have
ğ”¼(X) = Mâ€²
X(0) = ğœ†
Var(X) = ğ”¼(X2) âˆ’ğ”¼(X)2 = M
â€²â€²
X(0) âˆ’M
â€²
X(0)2 = ğœ†.
If X âˆ¼Binomial(n, p) then we can write
â„™(X = k) =
n!
k!(n âˆ’k)!pk(1 âˆ’p)nâˆ’k
= n(n âˆ’1) . . . (n âˆ’k + 1)(n âˆ’k)!
k!(n âˆ’k)!
Ã— pk
(
1 âˆ’(n âˆ’k)p + (n âˆ’k)(n âˆ’k âˆ’1)
2!
p2 + . . .
)
= n(n âˆ’1) . . . (n âˆ’k + 1)
k!
pk
(
1 âˆ’(n âˆ’k)p + (n âˆ’k)(n âˆ’k âˆ’1)
2!
p2 + . . .
)
.
For the case when n â†’âˆso that n â‰«k we have
â„™(X = k) â‰ˆnk
k! pk
(
1 âˆ’np + (np)2
2!
+ . . .
)
= nk
k! pkeâˆ’np.
By setting p = ğœ†âˆ•n, we have â„™(X = k) â‰ˆğœ†k
k! eâˆ’ğœ†.
â—½
4. Exponential Distribution. Consider a continuous random variable X following an expo-
nential distribution, X âˆ¼Exp(ğœ†) with probability density function
fX(x) = ğœ†eâˆ’ğœ†x,
x â‰¥0
where the parameter ğœ†> 0. Show that ğ”¼(X) = 1
ğœ†and Var(X) = 1
ğœ†2 .
Prove that X âˆ¼Exp(ğœ†) has a memory less property given as
â„™(X > s + x|X > s) = â„™(X > x) = eâˆ’ğœ†x,
x, s â‰¥0.
For a sequence of Bernoulli trials drawn from a Bernoulli distribution, Bernoulli(p), 0 â‰¤
p â‰¤1 performed at time Î”t, 2Î”t, . . . where Î”t > 0 and if Y is the waiting time for the
first success, show that as Î”t â†’0 and p â†’0 such that pâˆ•Î”t approaches a constant ğœ†> 0,
then Y âˆ¼Exp(ğœ†).
Solution: For t < ğœ†, the moment generating function for a random variable X âˆ¼Exp(ğœ†) is
MX(t) = ğ”¼(etX) = âˆ«
âˆ
0
etuğœ†eâˆ’ğœ†udu = ğœ†âˆ«
âˆ
0
eâˆ’(ğœ†âˆ’t)udu =
ğœ†
ğœ†âˆ’t.

1.2.2
Discrete and Continuous Random Variables
15
Differentiation of MX(t) with respect to t twice yields
M
â€²
X(t) =
ğœ†
(ğœ†âˆ’t)2 ,
M
â€²â€²
X (t) =
2ğœ†
(ğœ†âˆ’t)3 .
Therefore,
ğ”¼(X) = M
â€²
X(0) = 1
ğœ†,
ğ”¼(X2) = M
â€²â€²
X(0) = 2
ğœ†2
and the variance of X is
Var(X) = ğ”¼(X2) âˆ’[ğ”¼(X)]2 = 1
ğœ†2 .
By definition
â„™(X > x) = âˆ«
âˆ
x
ğœ†eâˆ’ğœ†udu = eâˆ’ğœ†x
and
â„™(X > s + x|X > s) = â„™(X > s + x, X > s)
â„™(X > s)
= â„™(X > s + x)
â„™(X > s)
=
âˆ«âˆ
s+x ğœ†eâˆ’ğœ†udu
âˆ«âˆ
s
ğœ†eâˆ’ğœ†ğ‘£dğ‘£
= eâˆ’ğœ†x.
Thus,
â„™(X > s + x|X > s) = â„™(X > x).
If Y is the waiting time for the first success then for k = 1, 2, . . .
â„™(Y > kÎ”t) = (1 âˆ’p)k.
By setting y = kÎ”t, and in the limit Î”t â†’0 and assuming that p â†’0 so that pâˆ•Î”t â†’ğœ†,
for some positive constant ğœ†,
â„™(Y > y) = â„™
(
Y >
( x
Î”t
)
Î”t
)
â‰ˆ(1 âˆ’ğœ†Î”t)yâˆ•Î”t
= 1 âˆ’ğœ†y +
(
y
Î”t
) (
y
Î”t âˆ’1
)
2!
(ğœ†Î”t)2 + . . .
â‰ˆ1 âˆ’ğœ†y + (ğœ†y)2
2!
+ . . .
= eâˆ’ğœ†x.
In the limit Î”t â†’0 and p â†’0,
â„™(Y â‰¤y) = 1 âˆ’â„™(Y > y) â‰ˆ1 âˆ’eâˆ’ğœ†y
and the probability density function is therefore
fY(y) = d
dyâ„™(Y â‰¤y) â‰ˆğœ†eâˆ’ğœ†y, y â‰¥0.
â—½

16
1.2.2
Discrete and Continuous Random Variables
5. Gamma Distribution. Let U and V be continuous independent random variables and let
W = U + V. Show that the probability density function of W can be written as
fW(ğ‘¤) = âˆ«
âˆ
âˆ’âˆ
fV(ğ‘¤âˆ’u)fU(u) du = âˆ«
âˆ
âˆ’âˆ
fU(ğ‘¤âˆ’ğ‘£)fV(ğ‘£) dğ‘£
where fU(u) and fV(ğ‘£) are the density functions of U and V, respectively.
Let X1, X2, . . . , Xn âˆ¼Exp(ğœ†) be a sequence of independent and identically distributed ran-
dom variables, each following an exponential distribution with common parameter ğœ†> 0.
Prove that if
Y =
n
âˆ‘
i=1
Xi
then Y follows a gamma distribution, Y âˆ¼Gamma (n, ğœ†) with the following probability
density function:
fY(y) = (ğœ†y)nâˆ’1
(n âˆ’1)!ğœ†eâˆ’ğœ†y,
y â‰¥0.
Show also that ğ”¼(Y) = n
ğœ†and Var(Y) = n
ğœ†2 .
Solution: From the definition of the cumulative distribution function of W = U + V we
obtain
FW(ğ‘¤) = â„™(W â‰¤ğ‘¤) = â„™(U + V â‰¤ğ‘¤) = âˆ«âˆ«
u+ğ‘£â‰¤ğ‘¤
fUV(u, ğ‘£) dudğ‘£
where fUV(u, ğ‘£) is the joint probability density function of (U, V). Since U âŸ‚âŸ‚V therefore
fUV(u, ğ‘£) = fU(u)fV(ğ‘£) and hence
FW(ğ‘¤) = âˆ«âˆ«
u+ğ‘£â‰¤ğ‘¤
fUV(u, ğ‘£) dudğ‘£
= âˆ«âˆ«
u+ğ‘£â‰¤ğ‘¤
fU(u)fV(ğ‘£) dudğ‘£
= âˆ«
âˆ
âˆ’âˆ
{
âˆ«
ğ‘¤âˆ’u
âˆ’âˆ
fV(ğ‘£) dğ‘£
}
fU(u) du
= âˆ«
âˆ
âˆ’âˆ
FV(ğ‘¤âˆ’u)fU(u) du.
By differentiating FW(ğ‘¤) with respect to ğ‘¤, we have the probability density function
fW(ğ‘¤) given as
fW(ğ‘¤) = d
dğ‘¤âˆ«
âˆ
âˆ’âˆ
FV(ğ‘¤âˆ’u)fU(u) du = âˆ«
âˆ
âˆ’âˆ
fV(ğ‘¤âˆ’u)fU(u) du.

1.2.2
Discrete and Continuous Random Variables
17
Using the same steps we can also obtain
fW(ğ‘¤) = âˆ«
âˆ
âˆ’âˆ
fU(ğ‘¤âˆ’ğ‘£)fV(ğ‘£) dğ‘£.
To show that Y = âˆ‘n
i=1 Xi âˆ¼Gamma (n, ğœ†) where X1, X2, . . . , Xn âˆ¼Exp(ğœ†), we will prove
the result via mathematical induction.
For n = 1, we have Y = X1 âˆ¼Exp(ğœ†) and the gamma density fY(y) becomes
fY(y) = ğœ†eâˆ’ğœ†y,
y â‰¥0.
Therefore, the result is true for n = 1.
Let us assume that the result holds for n = k and we now wish to compute the density for
the case n = k + 1. Since X1, X2, . . . , Xk+1 are all mutually independent and identically
distributed, by setting U = âˆ‘k
i=1 Xi and V = Xk+1, and since U â‰¥0, V â‰¥0, the density of
Y = âˆ‘k
i=1 Xi + Xk+1 can be expressed as
fY(y) = âˆ«
y
0
fV(y âˆ’u)fU(u) du
= âˆ«
y
0
ğœ†eâˆ’ğœ†(yâˆ’u) â‹…(ğœ†u)kâˆ’1
(k âˆ’1)!ğœ†eâˆ’ğœ†u du
= ğœ†k+1eâˆ’ğœ†y
(k âˆ’1)! âˆ«
y
0
ukâˆ’1 du
= (ğœ†y)k
k! ğœ†eâˆ’ğœ†y
which shows the result is also true for n = k + 1. Thus, Y = âˆ‘n
i=1 Xi âˆ¼Gamma (n, ğœ†).
Given that X1, X2, . . . , Xn âˆ¼Exp(ğœ†) are independent and identically distributed with com-
mon mean 1
ğœ†and variance 1
ğœ†2 , therefore
ğ”¼(Y) = ğ”¼
( n
âˆ‘
i=1
Xi
)
=
n
âˆ‘
i=1
ğ”¼(Xi) = n
ğœ†
and
Var(Y) = Var
( n
âˆ‘
i=1
Xi
)
=
n
âˆ‘
i=1
Var(Xi) = n
ğœ†2 .
â—½
6. Normal Distribution Property I. Show that for constants a, L and U such that t > 0 and
L < U,
1
âˆš
2ğœ‹t âˆ«
U
L
e
ağ‘¤âˆ’1
2
(
ğ‘¤
âˆš
t
)2
dğ‘¤= e
1
2 a2t
[
Î¦
(
U âˆ’at
âˆš
t
)
âˆ’Î¦
(
L âˆ’at
âˆš
t
)]
where Î¦(â‹…) is the cumulative distribution function of a standard normal.

18
1.2.2
Discrete and Continuous Random Variables
Solution: Simplifying the integrand we have
1
âˆš
2ğœ‹t âˆ«
U
L
e
ağ‘¤âˆ’1
2
(
ğ‘¤
âˆš
t
)2
dğ‘¤=
1
âˆš
2ğœ‹t âˆ«
U
L
e
âˆ’1
2
(
ğ‘¤2âˆ’2ağ‘¤t
t
)
dğ‘¤
=
1
âˆš
2ğœ‹t âˆ«
U
L
e
âˆ’1
2
[(
ğ‘¤âˆ’at
âˆš
t
)2
âˆ’a2t
]
dğ‘¤
= e
1
2 a2t
âˆš
2ğœ‹t âˆ«
U
L
e
âˆ’1
2
(
ğ‘¤âˆ’at
âˆš
t
)2
dğ‘¤.
By setting x = ğ‘¤âˆ’at
âˆš
t
we can write
âˆ«
U
L
1
âˆš
2ğœ‹t
e
âˆ’1
2
(
ğ‘¤âˆ’at
âˆš
t
)2
dğ‘¤= âˆ«
Uâˆ’at
âˆš
t
Lâˆ’at
âˆš
t
1
âˆš
2ğœ‹
eâˆ’1
2 x2dx = Î¦
(
U âˆ’at
âˆš
t
)
âˆ’Î¦
(
L âˆ’at
âˆš
t
)
.
Thus,
1
âˆš
2ğœ‹t âˆ«
U
L
e
ağ‘¤âˆ’1
2
(
ğ‘¤
âˆš
t
)2
dğ‘¤= e
1
2 a2t
[
Î¦
(
U âˆ’at
âˆš
t
)
âˆ’Î¦
(
L âˆ’at
âˆš
t
)]
.
â—½
7. Normal Distribution Property II. Show that if X âˆ¼ğ’©(ğœ‡, ğœ2) then for ğ›¿âˆˆ{ âˆ’1, 1},
ğ”¼[max{ğ›¿(eX âˆ’K), 0}] = ğ›¿eğœ‡+ 1
2 ğœ2Î¦
(ğ›¿(ğœ‡+ ğœ2 âˆ’log K)
ğœ
)
âˆ’ğ›¿KÎ¦
(ğ›¿(ğœ‡âˆ’log K)
ğœ
)
where K > 0 and Î¦(â‹…) denotes the cumulative standard normal distribution function.
Solution: We first let ğ›¿= 1,
ğ”¼[max{eX âˆ’K, 0}] = âˆ«
âˆ
log K
(ex âˆ’K) fX(x) dx
= âˆ«
âˆ
log K
(ex âˆ’K)
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
dx
= âˆ«
âˆ
log K
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
+x dx âˆ’K âˆ«
âˆ
log K
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
dx.
By setting ğ‘¤= x âˆ’ğœ‡
ğœ
and z = ğ‘¤âˆ’ğœwe have
ğ”¼[max{eX âˆ’K, 0}] = âˆ«
âˆ
log Kâˆ’ğœ‡
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2+ğœğ‘¤+ğœ‡dğ‘¤âˆ’K âˆ«
âˆ
log Kâˆ’ğœ‡
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤

1.2.2
Discrete and Continuous Random Variables
19
= eğœ‡+ 1
2 ğœ2
âˆ«
âˆ
log Kâˆ’ğœ‡
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 (ğ‘¤âˆ’ğœ)2dğ‘¤âˆ’K âˆ«
âˆ
log Kâˆ’ğœ‡
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤
= eğœ‡+ 1
2 ğœ2
âˆ«
âˆ
log Kâˆ’ğœ‡âˆ’ğœ2
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz âˆ’K âˆ«
âˆ
log Kâˆ’ğœ‡
ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤
= eğœ‡+ 1
2 ğœ2Î¦
(ğœ‡+ ğœ2 âˆ’log K
ğœ
)
âˆ’KÎ¦
(ğœ‡âˆ’log K
ğœ
)
.
Using similar steps for the case ğ›¿= âˆ’1 we can also show that
ğ”¼[max{K âˆ’eX, 0}] = KÎ¦
(log K âˆ’ğœ‡
ğœ
)
âˆ’eğœ‡+ 1
2 ğœ2Î¦
(log K âˆ’(ğœ‡+ ğœ2)
ğœ
)
.
â—½
8. For x > 0 show that
(1
x âˆ’1
x3
)
1
âˆš
2ğœ‹
eâˆ’1
2 x2 â‰¤âˆ«
âˆ
x
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz â‰¤
1
x
âˆš
2ğœ‹
eâˆ’1
2 x2.
Solution: Solving âˆ«
âˆ
x
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz using integration by parts, we let u = 1
z , dğ‘£
dz =
z
âˆš
2ğœ‹
eâˆ’1
2 z2 so that du
dz = âˆ’1
z2 and ğ‘£= âˆ’
1
âˆš
2ğœ‹
eâˆ’1
2 z2. Therefore,
âˆ«
âˆ
x
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz = âˆ’
1
z
âˆš
2ğœ‹
eâˆ’1
2 z2||||||
âˆ
x
âˆ’âˆ«
âˆ
x
1
z2âˆš
2ğœ‹
eâˆ’1
2 z2 dz
=
1
x
âˆš
2ğœ‹
eâˆ’1
2 x2 âˆ’âˆ«
âˆ
x
1
z2âˆš
2ğœ‹
eâˆ’1
2 z2 dz
â‰¤
1
x
âˆš
2ğœ‹
eâˆ’1
2 x2
since âˆ«
âˆ
x
1
z2âˆš
2ğœ‹
eâˆ’1
2 z2 dz > 0.
To obtain the lower bound, we integrate âˆ«
âˆ
x
1
z2âˆš
2ğœ‹
eâˆ’1
2 z2 dz by parts where we let u =
1
z3 , dğ‘£
dz =
z
âˆš
2ğœ‹
eâˆ’1
2 z2 so that du
dz = âˆ’3
z4 and ğ‘£= âˆ’
1
âˆš
2ğœ‹
eâˆ’1
2 z2 and hence
âˆ«
âˆ
x
1
z2âˆš
2ğœ‹
eâˆ’1
2 z2 dz = âˆ’
1
z3âˆš
2ğœ‹
eâˆ’1
2 z2||||||
âˆ
x
âˆ’âˆ«
âˆ
x
3
z4âˆš
2ğœ‹
eâˆ’1
2 z2 dz
=
1
x3âˆš
2ğœ‹
eâˆ’1
2 x2 âˆ’âˆ«
âˆ
x
3
z4âˆš
2ğœ‹
eâˆ’1
2 z2 dz.

20
1.2.2
Discrete and Continuous Random Variables
Therefore,
âˆ«
âˆ
x
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz =
1
x
âˆš
2ğœ‹
eâˆ’1
2 z2 âˆ’
1
x3âˆš
2ğœ‹
eâˆ’1
2 x2 + âˆ«
âˆ
x
3
z4âˆš
2ğœ‹
eâˆ’1
2 z2 dz
â‰¥
(1
x âˆ’1
x3
)
1
âˆš
2ğœ‹
eâˆ’1
2 x2
since âˆ«
âˆ
x
3
z4âˆš
2ğœ‹
eâˆ’1
2 z2 dz > 0. By taking into account both the lower and upper bounds
we have
(1
x âˆ’1
x3
)
1
âˆš
2ğœ‹
eâˆ’1
2 x2 â‰¤âˆ«
âˆ
x
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz â‰¤
1
x
âˆš
2ğœ‹
eâˆ’1
2 x2.
â—½
9. Lognormal Distribution I. Let Z âˆ¼ğ’©(0, 1), show that the moment generating function of
a standard normal distribution is
ğ”¼(eğœƒZ) = e
1
2 ğœƒ2
for a constant ğœƒ.
Show that if X âˆ¼ğ’©(ğœ‡, ğœ2) then Y = eX follows a lognormal distribution, Y = eX âˆ¼
log-ğ’©(ğœ‡, ğœ2) with probability density function
fY(y) =
1
yğœ
âˆš
2ğœ‹
e
âˆ’1
2
( log yâˆ’ğœ‡
ğœ
)2
with mean ğ”¼(Y) = eğœ‡+ 1
2 ğœ2 and variance Var(Y) =
(
eğœ2 âˆ’1
)
e2ğœ‡+ğœ2.
Solution: By definition
ğ”¼(eğœƒZ) = âˆ«
âˆ
âˆ’âˆ
eğœƒz â‹…
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz
= âˆ«
âˆ
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 (zâˆ’ğœƒ)2+ 1
2 ğœƒ2 dz
= e
1
2 ğœƒ2
âˆ«
âˆ
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 (zâˆ’ğœƒ)2 dz
= e
1
2 ğœƒ2.
For y > 0, by definition
â„™(eX < y) = â„™(X < log y) = âˆ«
log y
âˆ’âˆ
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
dx

1.2.2
Discrete and Continuous Random Variables
21
and hence
fY(y) = d
dyâ„™(eX < y) =
1
yğœ
âˆš
2ğœ‹
e
âˆ’1
2
( log yâˆ’ğœ‡
ğœ
)2
.
Given that log Y âˆ¼ğ’©(ğœ‡, ğœ2) we can write log Y = ğœ‡+ ğœZ, Z âˆ¼ğ’©(0, 1) and hence
ğ”¼(Y) = ğ”¼(eğœ‡+ğœZ) = eğœ‡ğ”¼(eğœZ) = eğœ‡+ 1
2 ğœ2
since ğ”¼(eğœƒZ) = e
1
2 ğœƒ2 is the moment generating function of a standard normal distribution.
Taking second moments,
ğ”¼(Y2) = ğ”¼(e2ğœ‡+2ğœZ) = e2ğœ‡ğ”¼(e2ğœZ) = e2ğœ‡+2ğœ2.
Therefore,
Var(Y) = ğ”¼(Y2) âˆ’[ğ”¼(Y)]2 = e2ğœ‡+2ğœ2 âˆ’
(
eğœ‡+ 1
2 ğœ2)2
=
(
eğœ2 âˆ’1
)
e2ğœ‡+ğœ2.
â—½
10. Lognormal Distribution II. Let X âˆ¼log-ğ’©(ğœ‡, ğœ2), show that for n âˆˆâ„•
ğ”¼(Xn) = enğœ‡+ 1
2 n2ğœ2.
Solution: Given X âˆ¼log-ğ’©(ğœ‡, ğœ2) the density function is
fX(x) =
1
xğœ
âˆš
2ğœ‹
e
âˆ’1
2
( log xâˆ’ğœ‡
ğœ
)2
,
x > 0
and for n âˆˆâ„•,
ğ”¼(Xn) = âˆ«
âˆ
0
xnfX(x) dx
= âˆ«
âˆ
0
1
xğœ
âˆš
2ğœ‹
xne
âˆ’1
2
( log xâˆ’ğœ‡
ğœ
)2
dx.
By substituting z = log x âˆ’ğœ‡
ğœ
so that x = eğœz+ğœ‡and dz
dx = 1
xğœ,
ğ”¼(Xn) = âˆ«
âˆ
âˆ’âˆ
1
xğœ
âˆš
2ğœ‹
en(ğœz+ğœ‡)eâˆ’1
2 z2xğœdz
= âˆ«
âˆ
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 z2+n(ğœz+ğœ‡) dz
= enğœ‡+ 1
2 n2ğœ2
âˆ«
âˆ
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 (zâˆ’nğœ)2 dz

22
1.2.2
Discrete and Continuous Random Variables
= enğœ‡+ 1
2 n2ğœ2
since âˆ«
âˆ
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 (zâˆ’nğœ)2 dz = 1.
â—½
11. Folded Normal Distribution. Show that if X âˆ¼ğ’©(ğœ‡, ğœ2) then Y = |X| follows a folded
normal distribution, Y = |X| âˆ¼ğ’©f(ğœ‡, ğœ2) with probability density function
fY(y) =
âˆš
2
ğœ‹ğœ2 e
âˆ’1
2
(
y2+ğœ‡2
ğœ2
)
cosh
(ğœ‡y
ğœ2
)
with mean
ğ”¼(Y) = ğœ
âˆš
2
ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡
[
1 âˆ’2Î¦
(
âˆ’ğœ‡
ğœ
)]
and variance
Var (Y) = ğœ‡2 + ğœ2 âˆ’
{
ğœ
âˆš
2
ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡
[
1 âˆ’2Î¦
(
âˆ’ğœ‡
ğœ
)]}2
where Î¦(â‹…) is the cumulative distribution function of a standard normal.
Solution: For y > 0, by definition
â„™(|X| < y) = â„™(âˆ’y < X < y) = âˆ«
y
âˆ’y
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
dx
and hence
fY(y) = d
dyâ„™(|X| < y) =
1
ğœ
âˆš
2ğœ‹
[
e
âˆ’1
2
( y+ğœ‡
ğœ
)2
+ e
âˆ’1
2
( yâˆ’ğœ‡
ğœ
)2]
=
âˆš
2
ğœ‹ğœ2 e
âˆ’1
2
(
y2+ğœ‡2
ğœ2
)
cosh
(ğœ‡y
ğœ2
)
.
By definition
ğ”¼(Y) = âˆ«
âˆ
âˆ’âˆ
y fY(y) dy
= âˆ«
âˆ
0
y
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( y+ğœ‡
ğœ
)2
dy + âˆ«
âˆ
0
y
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( yâˆ’ğœ‡
ğœ
)2
dy.
By setting z = (y + ğœ‡)âˆ•ğœand ğ‘¤= (y âˆ’ğœ‡)âˆ•ğœwe have
ğ”¼(Y) = âˆ«
âˆ
ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
(ğœz âˆ’ğœ‡)eâˆ’1
2 z2dz + âˆ«
âˆ
âˆ’ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
(ğœğ‘¤+ ğœ‡)eâˆ’1
2 ğ‘¤2dğ‘¤

1.2.2
Discrete and Continuous Random Variables
23
=
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
âˆ’ğœ‡
[
1 âˆ’Î¦
(ğœ‡
ğœ
)]
+
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
âˆ’ğœ‡
[
1 âˆ’Î¦
(
âˆ’ğœ‡
ğœ
)]
=
2ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡
[
Î¦
(ğœ‡
ğœ
)
âˆ’Î¦
(
âˆ’ğœ‡
ğœ
)]
= ğœ
âˆš
2
ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡
[
1 âˆ’2Î¦
(
âˆ’ğœ‡
ğœ
)]
.
To evaluate ğ”¼(Y2) by definition,
ğ”¼(Y2) = âˆ«
âˆ
âˆ’âˆ
y2 fY(y) dy
= âˆ«
âˆ
0
y2
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( y+ğœ‡
ğœ
)2
dy + âˆ«
âˆ
0
y2
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( yâˆ’ğœ‡
ğœ
)2
dy.
By setting z = (y + ğœ‡)âˆ•ğœand ğ‘¤= (y âˆ’ğœ‡)âˆ•ğœwe have
ğ”¼(Y2) = âˆ«
âˆ
ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
(ğœz âˆ’ğœ‡)2eâˆ’1
2 z2dz + âˆ«
âˆ
âˆ’ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
(ğœğ‘¤+ ğœ‡)2eâˆ’1
2 ğ‘¤2dğ‘¤
=
ğœ2
âˆš
2ğœ‹âˆ«
âˆ
ğœ‡âˆ•ğœ
z2eâˆ’1
2 z2dz âˆ’2ğœ‡ğœ
âˆš
2ğœ‹âˆ«
âˆ
ğœ‡âˆ•ğœ
zeâˆ’1
2 z2dz + ğœ‡2
âˆ«
âˆ
ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 z2dz
+ ğœ2
âˆš
2ğœ‹âˆ«
âˆ
âˆ’ğœ‡âˆ•ğœ
ğ‘¤2eâˆ’1
2 ğ‘¤2dğ‘¤+ 2ğœ‡ğœ
âˆš
2ğœ‹âˆ«
âˆ
âˆ’ğœ‡âˆ•ğœ
ğ‘¤eâˆ’1
2 ğ‘¤2dğ‘¤
+ ğœ‡2
âˆ«
âˆ
âˆ’ğœ‡âˆ•ğœ
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤
=
ğœ2
âˆš
2ğœ‹
[(ğœ‡
ğœ
)
e
âˆ’1
2
( ğœ‡
ğœ
)2
+
âˆš
2ğœ‹
(
1 âˆ’Î¦
(ğœ‡
ğœ
))]
âˆ’2ğœ‡ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡2 [
1 âˆ’Î¦
(ğœ‡
ğœ
)]
+ ğœ2
âˆš
2ğœ‹
[(
âˆ’ğœ‡
ğœ
)
e
âˆ’1
2
( ğœ‡
ğœ
)2
+
âˆš
2ğœ‹
(
1 âˆ’Î¦
(
âˆ’ğœ‡
ğœ
))]
+ 2ğœ‡ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡2 [
1 âˆ’Î¦
(
âˆ’ğœ‡
ğœ
)]
= (ğœ‡2 + ğœ2) [
2 âˆ’Î¦
(ğœ‡
ğœ
)
âˆ’Î¦
(
âˆ’ğœ‡
ğœ
)]
= ğœ‡2 + ğœ2.

24
1.2.2
Discrete and Continuous Random Variables
Therefore,
Var(Y) = ğ”¼(Y2) âˆ’[ğ”¼(Y)]2 = ğœ‡2 + ğœ2 âˆ’
{
ğœ
âˆš
2
ğœ‹
e
âˆ’1
2
( ğœ‡
ğœ
)2
+ ğœ‡
[
1 âˆ’2Î¦
(
âˆ’ğœ‡
ğœ
)]}2
.
â—½
12. Chi-Square Distribution. Show that if X âˆ¼ğ’©(ğœ‡, ğœ2) then Y = X âˆ’ğœ‡
ğœ
âˆ¼ğ’©(0, 1).
Let Z1, Z2, . . . , Zn âˆ¼ğ’©(0, 1) be a sequence of independent and identically distributed ran-
dom variables each following a standard normal distribution. Using mathematical induc-
tion show that
Z = Z2
1 + Z2
2 + . . . + Z2
n âˆ¼ğœ’2(n)
where Z has a probability density function
fZ(z) =
1
2
n
2 Î“
(
n
2
)z
n
2 âˆ’1eâˆ’z
2 ,
z â‰¥0
such that
Î“
(n
2
)
= âˆ«
âˆ
0
eâˆ’xx
n
2 âˆ’1dx.
Finally, show that ğ”¼(Z) = n and Var(Z) = 2n.
Solution: By setting Y = X âˆ’ğœ‡
ğœ
then
â„™(Y â‰¤y) = â„™
(X âˆ’ğœ‡
ğœ
â‰¤y
)
= â„™(X â‰¤ğœ‡+ ğœy).
Differentiating with respect to y,
fY(y) = d
dyâ„™(Y â‰¤y)
= d
dy âˆ«
ğœ‡+ğœy
âˆ’âˆ
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
dx
=
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( ğœ‡+ğœyâˆ’ğœ‡
ğœ
)2
ğœ
=
1
âˆš
2ğœ‹
eâˆ’1
2 y2
which is a probability density function of ğ’©(0, 1).
For Z = Z2
1 and given Z â‰¥0, by definition
â„™(Z â‰¤z) = â„™(Z2
1 â‰¤z) = â„™
(
âˆ’
âˆš
z < Z1 <
âˆš
z
)
= 2 âˆ«
âˆš
z
0
1
âˆš
2ğœ‹
eâˆ’1
2 z2
1 dz1

1.2.2
Discrete and Continuous Random Variables
25
and hence
fZ(z) = d
dz
[
2 âˆ«
âˆš
z
0
1
âˆš
2ğœ‹
eâˆ’1
2 z2
1 dz1
]
=
1
âˆš
2ğœ‹
zâˆ’1
2 eâˆ’z
2 ,
z â‰¥0
which is the probability density function of ğœ’2(1).
For Z = Z2
1 + Z2
2 such that Z â‰¥0 we have
â„™(Z â‰¤z) = â„™(Z2
1 + Z2
2 â‰¤z) = âˆ«âˆ«
z2
1+z2
2â‰¤z
1
2ğœ‹eâˆ’1
2 (z2
1+z2
2) dz1dz2.
Changing to polar coordinates (r, ğœƒ) such that z1 = r cos ğœƒand z2 = r sin ğœƒwith the Jaco-
bian determinant
|J| =
||||
ğœ•(z1, z2)
ğœ•(r, ğœƒ)
||||
=
|||||||||
ğœ•z1
ğœ•r
ğœ•z1
ğœ•ğœƒ
ğœ•z2
ğœ•r
ğœ•z2
ğœ•ğœƒ
|||||||||
=
||||
cos ğœƒâˆ’r sin ğœƒ
sin ğœƒ
r cos ğœƒ
||||
= r
then
âˆ«âˆ«
z2
1+z2
2â‰¤z
1
2ğœ‹eâˆ’1
2 (z2
1+z2
2) dz1dz2 = âˆ«
2ğœ‹
ğœƒ=0 âˆ«
âˆš
z
r=0
1
2ğœ‹eâˆ’1
2 r2r drdğœƒ= 1 âˆ’eâˆ’1
2 z.
Thus,
fZ(z) = d
dz
â¡
â¢
â¢
â¢â£
âˆ«âˆ«
z2
1+z2
2â‰¤z
1
2ğœ‹eâˆ’1
2 (z2
1+z2
2) dz1dz2
â¤
â¥
â¥
â¥â¦
= 1
2 eâˆ’1
2 z,
z â‰¥0
which is the probability density function of ğœ’2(2).
Assume the result is true for n = k such that
U = Z2
1 + Z2
2 + . . . + Z2
k âˆ¼ğœ’2(k)
and knowing that
V = Z2
k+1 âˆ¼ğœ’2(1)
then, because U â‰¥0 and V â‰¥0 are independent, using the convolution formula the density
of Z = U + V = âˆ‘k+1
i=1 Z2
i can be written as
fZ(z) = âˆ«
z
0
fV(z âˆ’u)fU(u) du
= âˆ«
z
0
{
1
âˆš
2ğœ‹
(z âˆ’u)âˆ’1
2 eâˆ’1
2 (zâˆ’u)
}
â‹…
â§
âª
â¨
âªâ©
1
2
k
2 Î“
(
k
2
)u
1
2 kâˆ’1eâˆ’1
2 u
â«
âª
â¬
âªâ­
du

26
1.2.2
Discrete and Continuous Random Variables
=
eâˆ’1
2 z
âˆš
2ğœ‹2
k
2 Î“
(
k
2
) âˆ«
z
0
(z âˆ’u)âˆ’1
2 u
k
2 âˆ’1 du.
By setting ğ‘£= u
z we have
âˆ«
z
0
(z âˆ’u)âˆ’1
2 u
k
2 âˆ’1 du = âˆ«
1
0
(z âˆ’ğ‘£z)âˆ’1
2 (ğ‘£z)
k
2 âˆ’1z dğ‘£
= z
k+1
2 âˆ’1
âˆ«
1
0
(1 âˆ’ğ‘£)âˆ’1
2 ğ‘£
k
2 âˆ’1 dğ‘£
and because âˆ«
1
0
(1 âˆ’ğ‘£)âˆ’1
2 ğ‘£
k
2 âˆ’1 dğ‘£= B
(1
2, k
2
)
=
âˆš
ğœ‹Î“
(
k
2
)
Î“
(
k+1
2
)
(see Appendix A) there-
fore
fZ(z) =
1
2
k+1
2 Î“
(
k+1
2
)z
k+1
2 âˆ’1eâˆ’1
2 z,
z â‰¥0
which is the probability density function of ğœ’2(k + 1) and hence the result is also true for
n = k + 1. By mathematical induction we have shown
Z = Z2
1 + Z2
2 + . . . + Z2
n âˆ¼ğœ’2(n).
By computing the moment generation of Z,
MZ(t) = ğ”¼(etZ) = âˆ«
âˆ
0
etzfZ(z) dz =
1
2
n
2 Î“
(
n
2
) âˆ«
âˆ
0
z
n
2 âˆ’1eâˆ’1
2 (1âˆ’2t) dz
and by setting ğ‘¤= 1
2(1 âˆ’2t)z we have
MZ(t) =
1
2
n
2 Î“
(
n
2
)
(
2
1 âˆ’2t
) n
2
âˆ«
âˆ
0
ğ‘¤
n
2 âˆ’1eâˆ’ğ‘¤dğ‘¤= (1 âˆ’2t)âˆ’n
2 ,
t âˆˆ
(
âˆ’1
2, 1
2
)
.
Thus,
M
â€²
Z(t) = n(1 âˆ’2t)
âˆ’
( n
2 +1
)
,
M
â€²â€²
Z (t) = 2n
(n
2 + 1
)
(1 âˆ’2t)
âˆ’
( n
2 +2
)
such that
ğ”¼(Z) = M
â€²
Z(0) = n,
ğ”¼(Z2) = M
â€²â€²
Z (0) = 2n
(n
2 + 1
)
and
Var (Z) = ğ”¼(Z2) âˆ’[ğ”¼(Z)]2 = 2n.
â—½

1.2.2
Discrete and Continuous Random Variables
27
13. Marginal Distributions of Bivariate Normal Distribution. Let X and Y be jointly normally
distributed with means ğœ‡x, ğœ‡y, variances ğœ2
x, ğœ2
y and correlation coefficient ğœŒxy âˆˆ(âˆ’1, 1)
such that the joint density function is
fXY(x, y) =
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
.
Show that X âˆ¼ğ’©(ğœ‡x, ğœ2
x) and Y âˆ¼ğ’©(ğœ‡y, ğœ2
y).
Solution: By definition,
fX(x) = âˆ«
âˆ
âˆ’âˆ
fXY(x, y) dy
= âˆ«
âˆ
âˆ’âˆ
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy
= âˆ«
âˆ
âˆ’âˆ
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
Ã— e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[
(1âˆ’ğœŒ2
xy)
( xâˆ’ğœ‡x
ğœx
)2
+ğœŒ2
xy
( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy
=
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2
âˆ«
âˆ
âˆ’âˆ
g(x, y) dy
where
g(x, y) =
1
âˆš
1 âˆ’ğœŒ2
xyğœy
âˆš
2ğœ‹
e
âˆ’
1
2(1âˆ’ğœŒ2xy)ğœ2y
[
yâˆ’
(
ğœ‡y+ğœŒxyğœy
( xâˆ’ğœ‡x
ğœx
))]2
is
the
probability
density
function
for
ğ’©(ğœ‡y + ğœŒxyğœy
(x âˆ’ğœ‡x
) âˆ•ğœx, (1 âˆ’ğœŒ2
xy)ğœ2
y
).
Therefore,
âˆ«
âˆ
âˆ’âˆ
g(x, y) dy = 1
and hence
fX(x) =
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2
.
Thus, X âˆ¼ğ’©(ğœ‡x, ğœ2
x). Using the same steps we can also show that Y âˆ¼ğ’©(ğœ‡y, ğœ2
y).
â—½

28
1.2.2
Discrete and Continuous Random Variables
14. Covariance of Bivariate Normal Distribution. Let X and Y be jointly normally distributed
with means ğœ‡x, ğœ‡y, variances ğœ2
x, ğœ2
y and correlation coefficient ğœŒxy âˆˆ(âˆ’1, 1) such that the
joint density function is
fXY(x, y) =
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
.
Show that the covariance of X and Y, Cov(X, Y) = ğœŒxyğœxğœy and hence show that X and Y
are independent if and only if ğœŒxy = 0.
Solution: By definition, the covariance of X and Y is
Cov(X, Y) = ğ”¼[(X âˆ’ğœ‡x)(Y âˆ’ğœ‡y)]
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
(x âˆ’ğœ‡x)(y âˆ’ğœ‡y) fXY(x, y) dxdy
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy âˆ’ğœ‡y âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xfXY(x, y) dxdy
âˆ’ğœ‡x âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
yfXY(x, y) dxdy + ğœ‡xğœ‡y âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
fXY(x, y) dxdy
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy âˆ’ğœ‡y âˆ«
âˆ
âˆ’âˆ
x
[
âˆ«
âˆ
âˆ’âˆ
fXY(x, y) dy
]
dx
âˆ’ğœ‡x âˆ«
âˆ
âˆ’âˆ
y
[
âˆ«
âˆ
âˆ’âˆ
fXY(x, y) dx
]
dy + âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
ğœ‡xğœ‡yfXY(x, y) dxdy
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy âˆ’ğœ‡y âˆ«
âˆ
âˆ’âˆ
xfX(x) dx âˆ’ğœ‡x âˆ«
âˆ
âˆ’âˆ
yfY(y) dy
+ âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
ğœ‡xğœ‡yfXY(x, y) dxdy
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy âˆ’ğœ‡xğœ‡y âˆ’ğœ‡xğœ‡y + ğœ‡xğœ‡y
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy âˆ’ğœ‡xğœ‡y
where
fX(x) = âˆ«âˆ
âˆ’âˆfXY(x, y) dy
and
fY(y) = âˆ«âˆ
âˆ’âˆfXY(x, y) dx.
Using
the
result
of
Problem 1.2.2.13 (page 27) we can deduce that
âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dxdy = âˆ«
âˆ
âˆ’âˆ
x
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2 (
âˆ«
âˆ
âˆ’âˆ
yg(x, y) dy
)
dx
where
g(x, y) =
1
âˆš
1 âˆ’ğœŒ2
xyğœy
âˆš
2ğœ‹
e
âˆ’
1
2(1âˆ’ğœŒ2xy)ğœ2y
[
yâˆ’
(
ğœ‡y+ğœŒxyğœy
( xâˆ’ğœ‡x
ğœx
))]2

1.2.2
Discrete and Continuous Random Variables
29
is
the
probability
density
function
for
ğ’©(ğœ‡y + ğœŒxyğœy
(x âˆ’ğœ‡x
) âˆ•ğœx, (1 âˆ’ğœŒ2
xy)ğœ2
y
).
Therefore,
âˆ«
âˆ
âˆ’âˆ
yg(x, y) dy = ğœ‡y + ğœŒxyğœy
(x âˆ’ğœ‡x
ğœx
)
.
Thus,
Cov(X, Y) = âˆ«
âˆ
âˆ’âˆ
x
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2 [
ğœ‡y + ğœŒxyğœy
(x âˆ’ğœ‡x
ğœx
)]
dx âˆ’ğœ‡xğœ‡y
= ğœ‡xğœ‡y +
ğœŒxyğœy
ğœx
âˆ«
âˆ
âˆ’âˆ
x2
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2
dx âˆ’
ğœŒxyğœyğœ‡2
x
ğœx
âˆ’ğœ‡xğœ‡y
= ğœ‡xğœ‡y +
ğœŒxyğœy
ğœx
(ğœ2
x + ğœ‡2
x
) âˆ’
ğœŒxyğœyğœ‡2
x
ğœx
âˆ’ğœ‡xğœ‡y
= ğœŒxyğœxğœy
where
ğ”¼(X2) = âˆ«
âˆ
âˆ’âˆ
x2
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2
dx = ğœ2
x + ğœ‡2
x.
To show that X and Y are independent if and only if ğœŒxy = 0 we note that if X âŸ‚âŸ‚Y then
Cov(X, Y) = 0, which implies ğœŒxy = 0. On the contrary, if ğœŒxy = 0 then from the joint den-
sity of (X, Y) we can express it as
fXY(x, y) = fX(x) fY(y)
where fX(x) = âˆ«
âˆ
âˆ’âˆ
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡x
ğœx
)2
dx and fY(y) = âˆ«
âˆ
âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2
dy and so
X âŸ‚âŸ‚Y.
Thus, if the pair X and Y has a bivariate normal distribution with means ğœ‡x, ğœ‡y, variances
ğœ2
x, ğœ2
y and correlation ğœŒxy then X âŸ‚âŸ‚Y if and only if ğœŒxy = 0.
â—½
15. Minimum and Maximum of Two Correlated Normal Distributions. Let X and Y be jointly
normally distributed with means ğœ‡x, ğœ‡y, variances ğœ2
x, ğœ2
y and correlation coefficient ğœŒxy âˆˆ
(âˆ’1, 1) such that the joint density function is
fXY(x, y) =
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
.
Show that the distribution of U = min{X, Y} is
fU(u) = Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡y +
ğœŒxyğœy
ğœx (u âˆ’ğœ‡x)
ğœy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fX(u) + Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡x +
ğœŒxyğœx
ğœy (u âˆ’ğœ‡y)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fY(u).

30
1.2.2
Discrete and Continuous Random Variables
and deduce that the distribution of V = max{X, Y} is
fV(ğ‘£) = Î¦
â›
âœ
âœ
âœâ
ğ‘£âˆ’ğœ‡y âˆ’
ğœŒxyğœy
ğœx (ğ‘£âˆ’ğœ‡x)
ğœy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fX(ğ‘£) + Î¦
â›
âœ
âœ
âœâ
ğ‘£âˆ’ğœ‡x âˆ’
ğœŒxyğœx
ğœy (u âˆ’ğœ‡y)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fY(ğ‘£)
where fX(z) =
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( zâˆ’ğœ‡x
ğœx
)2
, fY(z) =
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
zâˆ’ğœ‡y
ğœy
)2
and Î¦(â‹…) denotes the
cumulative standard normal distribution function (cdf).
Solution: From Problems 1.2.2.13 (page 27) and 1.2.2.14 (page 28) we can show that
X âˆ¼ğ’©(ğœ‡x, ğœ2
x), Y âˆ¼ğ’©(ğœ‡y, ğœ2
y), Cov(X, Y) = ğœŒxyğœxğœy such that ğœŒxy âˆˆ(âˆ’1, 1).
For U = min{X, Y} then by definition the cumulative distribution function (cdf) of U is
â„™(U â‰¤u) = â„™(min{X, Y} â‰¤u)
= 1 âˆ’â„™(min{X, Y} > u)
= 1 âˆ’â„™(X > u, Y > u) .
To derive the probability density function (pdf) of U we have
fU(u) = d
duâ„™(U â‰¤u)
= âˆ’d
duâ„™(X > u, Y > u)
= âˆ’d
du âˆ«
âˆ
x=u âˆ«
âˆ
y=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
Ã— e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dydx
= g(u) + h(u)
where
g(u) = âˆ«
âˆ
y=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( uâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( uâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy
and
h(u) = âˆ«
âˆ
x=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
uâˆ’ğœ‡y
ğœy
)
+
(
uâˆ’ğœ‡y
ğœy
)2]
dx.

1.2.2
Discrete and Continuous Random Variables
31
By focusing on
g(u) = âˆ«
âˆ
y=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( uâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( uâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy
= âˆ«
âˆ
y=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
{[(
yâˆ’ğœ‡y
ğœy
)
âˆ’ğœŒxy
( uâˆ’ğœ‡x
ğœx
)]2
+
( uâˆ’ğœ‡x
ğœx
)2
(1âˆ’ğœŒ2
xy)
}
dy
=
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( uâˆ’ğœ‡x
ğœx
)2
âˆ«
âˆ
y=u
1
ğœy
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[(
yâˆ’ğœ‡y
ğœy
)
âˆ’ğœŒxy
( uâˆ’ğœ‡x
ğœx
)]2
dy
and letting ğ‘¤=
y âˆ’ğœ‡y
ğœy
âˆ’ğœŒxy
(u âˆ’ğœ‡x
ğœx
)
âˆš
1 âˆ’ğœŒ2
xy
we have
g(u) =
1
ğœx
âˆš
2ğœ‹
e
âˆ’1
2
( uâˆ’ğœ‡x
ğœx
)2
âˆ«
âˆ
ğ‘¤=
uâˆ’ğœ‡y
ğœy
âˆ’ğœŒxy
( uâˆ’ğœ‡x
ğœx
)
âˆš
1âˆ’ğœŒ2xy
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤
= Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡y +
ğœŒxyğœy
ğœx (u âˆ’ğœ‡x)
ğœy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fX(u).
In a similar vein we can also show
h(u) = âˆ«
âˆ
x=u
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
uâˆ’ğœ‡y
ğœy
)
+
(
uâˆ’ğœ‡y
ğœy
)2]
dx
=
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
uâˆ’ğœ‡y
ğœy
)2
âˆ«
âˆ
ğ‘¤=
uâˆ’ğœ‡x
ğœx
âˆ’ğœŒxy
( uâˆ’ğœ‡y
ğœy
)
âˆš
1âˆ’ğœŒ2xy
1
âˆš
2ğœ‹
eâˆ’1
2 ğ‘¤2dğ‘¤
= Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡x +
ğœŒxyğœx
ğœy (u âˆ’ğœ‡y)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fY(u).
Therefore,
fU(u) = Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡y +
ğœŒxyğœy
ğœx (u âˆ’ğœ‡x)
ğœy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fX(u) + Î¦
â›
âœ
âœ
âœâ
âˆ’u + ğœ‡x +
ğœŒxyğœx
ğœy (u âˆ’ğœ‡y)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fY(u).

32
1.2.2
Discrete and Continuous Random Variables
As for the case V = max{X, Y}, then by definition the cdf of V is
â„™(V â‰¤ğ‘£) = â„™(max{X, Y} â‰¤ğ‘£)
= â„™(X â‰¤ğ‘£, Y â‰¤ğ‘£) .
The pdf of V is
fV(ğ‘£) = d
dğ‘£â„™(V â‰¤ğ‘£)
= d
dğ‘£â„™(X â‰¤ğ‘£, Y â‰¤ğ‘£)
= d
dğ‘£âˆ«
x=ğ‘£
âˆ’âˆâˆ«
y=ğ‘£
âˆ’âˆ
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
Ã— e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy dx
= âˆ«
y=ğ‘£
âˆ’âˆ
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( ğ‘£âˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( ğ‘£âˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
dy
+ âˆ«
x=ğ‘£
âˆ’âˆ
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
ğ‘£âˆ’ğœ‡y
ğœy
)
+
(
ğ‘£âˆ’ğœ‡y
ğœy
)2]
dx.
Following the same steps as described above we can write
fV(ğ‘£) = Î¦
â›
âœ
âœ
âœâ
ğ‘£âˆ’ğœ‡y âˆ’
ğœŒxyğœy
ğœx (ğ‘£âˆ’ğœ‡x)
ğœy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fX(ğ‘£) + Î¦
â›
âœ
âœ
âœâ
ğ‘£âˆ’ğœ‡x âˆ’
ğœŒxyğœx
ğœy (ğ‘£âˆ’ğœ‡y)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
fY(ğ‘£).
â—½
16. Bivariate Standard Normal Distribution. Let X âˆ¼ğ’©(0, 1) and Y âˆ¼ğ’©(0, 1) be jointly nor-
mally distributed with correlation coefficient ğœŒxy âˆˆ(âˆ’1, 1) where the joint cumulative
distribution function is
ğš½(ğ›¼, ğ›½, ğœŒxy) = âˆ«
ğ›½
âˆ’âˆâˆ«
ğ›¼
âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’1
2
(
x2âˆ’2ğœŒxyxy+y2
1âˆ’ğœŒ2xy
)
dxdy.
By using the change of variables Y = ğœŒxyX +
âˆš
1 âˆ’ğœŒ2
xyZ, Z âˆ¼ğ’©(0, 1), X âŸ‚âŸ‚Z show that
ğš½(ğ›¼, ğ›½, ğœŒxy) = âˆ«
ğ›¼
âˆ’âˆ
fX(x)Î¦
â›
âœ
âœ
âœâ
ğ›½âˆ’ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dx

1.2.2
Discrete and Continuous Random Variables
33
where fX(x) =
1
âˆš
2ğœ‹
eâˆ’1
2 x2 and Î¦(â‹…) is the cumulative distribution function of a standard
normal.
Finally, deduce that ğš½(ğ›¼, ğ›½, ğœŒxy) + ğš½(ğ›¼, âˆ’ğ›½, âˆ’ğœŒxy) = Î¦(ğ›¼).
Solution: Let y = ğœŒxyx +
âˆš
1 âˆ’ğœŒ2
xyz. Differentiating y with respect to z we have dy
dz =
âˆš
1 âˆ’ğœŒ2
xy, and hence
ğš½(ğ›¼, ğ›½, ğœŒxy) = âˆ«
ğ›½
âˆ’âˆâˆ«
ğ›¼
âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’1
2
(
x2âˆ’2ğœŒxyxy+y2
1âˆ’ğœŒ2xy
)
dx dy
= âˆ«
ğ›½âˆ’ğœŒxyx
âˆš
1âˆ’ğœŒ2xy
âˆ’âˆ
âˆ«
ğ›¼
âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2
xy
Ã— e
âˆ’1
2
â›
âœ
âœâ
x2âˆ’2ğœŒxyx(ğœŒxyx+âˆš
1âˆ’ğœŒ2xyz)+(ğœŒxyx+âˆš
1âˆ’ğœŒ2xyz)2
1âˆ’ğœŒ2xy
â
âŸ
âŸâ 
âˆš
1 âˆ’ğœŒ2
xy dx dz
= âˆ«
ğ›½âˆ’ğœŒxyx
âˆš
1âˆ’ğœŒ2xy
âˆ’âˆ
âˆ«
ğ›¼
âˆ’âˆ
1
2ğœ‹eâˆ’1
2 (x2+z2)dx dz
= âˆ«
ğ›¼
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 x2 â¡
â¢
â¢â£âˆ«
ğ›½âˆ’ğœŒxyx
âˆš
1âˆ’ğœŒ2xy
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz
â¤
â¥
â¥â¦
dx
= âˆ«
ğ›¼
âˆ’âˆ
fX(x)Î¦
â›
âœ
âœ
âœâ
ğ›½âˆ’ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dx.
Finally,
ğš½(ğ›¼, ğ›½, ğœŒxy) + ğš½(ğ›¼, âˆ’ğ›½, âˆ’ğœŒxy) = âˆ«
ğ›¼
âˆ’âˆ
fX(x)Î¦
â›
âœ
âœ
âœâ
ğ›½âˆ’ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dx
+ âˆ«
ğ›¼
âˆ’âˆ
fX(x)Î¦
â›
âœ
âœ
âœâ
âˆ’ğ›½+ ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dx
= âˆ«
ğ›¼
âˆ’âˆ
fX(x)
â¡
â¢
â¢
â¢â£
Î¦
â›
âœ
âœ
âœâ
ğ›½âˆ’ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
+ Î¦
â›
âœ
âœ
âœâ
âˆ’ğ›½+ ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
â¤
â¥
â¥
â¥â¦
dx

34
1.2.2
Discrete and Continuous Random Variables
= âˆ«
ğ›¼
âˆ’âˆ
fX(x) dx
= Î¦(ğ›¼)
since Î¦
â›
âœ
âœ
âœâ
ğ›½âˆ’ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
+ Î¦
â›
âœ
âœ
âœâ
âˆ’ğ›½+ ğœŒxyx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
= 1.
N.B. Similarly we can also show that
ğš½(ğ›¼, ğ›½, ğœŒxy) = âˆ«
ğ›½
âˆ’âˆ
fY(y)Î¦
â›
âœ
âœ
âœâ
ğ›¼âˆ’ğœŒxyy
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dy
where fY(y) =
1
âˆš
2ğœ‹
eâˆ’1
2 y2 and ğš½(ğ›¼, ğ›½, ğœŒxy) + ğš½(âˆ’ğ›¼, ğ›½, âˆ’ğœŒxy) = Î¦(ğ›½).
â—½
17. Bivariate Normal Distribution Property. Let X and Y be jointly normally distributed with
means ğœ‡x, ğœ‡y, variances ğœ2
x, ğœ2
y and correlation coefficient ğœŒxy âˆˆ(âˆ’1, 1) such that the joint
density function is
fXY(x, y) =
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
.
Show that
ğ”¼[max{eX âˆ’eY, 0}] = eğœ‡x+ 1
2 ğœ2
x Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y + ğœx(ğœx âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
âˆ’eğœ‡y+ 1
2 ğœ2
y Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’ğœy(ğœy âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
.
Solution: By definition
ğ”¼[max{eX âˆ’eY, 0}] = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
(ex âˆ’ey) fXY(x, y) dxdy
= I1 âˆ’I2
where I1 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
exfXY(x, y) dxdy and I2 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
eyfXY(x, y) dxdy.

1.2.2
Discrete and Continuous Random Variables
35
For the case I1 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
exfXY(x, y) dxdy we have
I1 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
ex
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
= âˆ«
y=âˆ
y=âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2
Ã—
â¡
â¢
â¢
â¢â£
âˆ«
x=âˆ
x=y
ex
ğœx
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[
xâˆ’
(
ğœ‡x+ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
))]2
dx
â¤
â¥
â¥
â¥â¦
dy
= âˆ«
y=âˆ
y=âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2 [
âˆ«
x=âˆ
x=y
exg(x, y) dx
]
dy
where g(x, y) =
1
ğœx
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[
xâˆ’
(
ğœ‡x+ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
))]2
which is the probability
density function of ğ’©
[
ğœ‡x + ğœŒxyğœx
(y âˆ’ğœ‡y
ğœy
)
, (1 âˆ’ğœŒxy)2ğœ2
x
]
. Thus, from Problem 1.2.2.7
(page 18) we can deduce
âˆ«
x=âˆ
x=y
exg(x, y) dx = e
ğœ‡x+ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
+ 1
2 (1âˆ’ğœŒ2
xy)ğœ2
x
Ã— Î¦
â›
âœ
âœ
âœ
âœâ
ğœ‡x + ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
+ (1 âˆ’ğœŒxy)2ğœ2
x âˆ’y
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸ
âŸâ 
.
Thus, we can write
I1 = eğœ‡x+ 1
2 (1âˆ’ğœŒ2
xy)ğœ2
x
Ã— âˆ«
y=âˆ
y=âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
[(
yâˆ’ğœ‡y
ğœy
)2
âˆ’2ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)]
Ã— Î¦
â›
âœ
âœ
âœ
âœâ
ğœ‡x + ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
+ (1 âˆ’ğœŒxy)2ğœ2
x âˆ’y
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸ
âŸâ 
dy

36
1.2.2
Discrete and Continuous Random Variables
= eğœ‡x+ 1
2 ğœ2
x
âˆ«
y=âˆ
y=âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡yâˆ’ğœŒxyğœxğœy
ğœy
)2
Ã— Î¦
â›
âœ
âœ
âœ
âœâ
ğœ‡x + ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
+ (1 âˆ’ğœŒxy)2ğœ2
x âˆ’y
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸ
âŸâ 
dy.
Let u =
y âˆ’ğœ‡y âˆ’ğœŒxyğœxğœy
ğœy
then from the change of variables
I1 = eğœ‡x+ 1
2 ğœ2
x
âˆ«
u=âˆ
u=âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 u2Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y + ğœx(ğœx âˆ’ğœŒxyğœy) âˆ’u(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
du
= eğœ‡x+ 1
2 ğœ2
x
âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘£=
ğœ‡xâˆ’ğœ‡y+ğœx(ğœxâˆ’ğœŒxyğœy)
ğœx
âˆš
1âˆ’ğœŒ2xy
âˆ’
u(ğœyâˆ’ğœŒxyğœx)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘£=âˆ’âˆ
1
2ğœ‹eâˆ’1
2 (u2+ğ‘£2) dğ‘£du.
By setting ğ‘¤= ğ‘£+
u(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
and ğœ2 = ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y,
I1 = eğœ‡x+ 1
2 ğœ2
x
Ã— âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡y+ğœx(ğœxâˆ’ğœŒxyğœy)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘¤=âˆ’âˆ
1
2ğœ‹
Ã— e
âˆ’1
2
â¡
â¢
â¢â£
ğ‘¤2âˆ’2uğ‘¤
â›
âœ
âœâ
ğœyâˆ’ğœŒxyğœx
ğœx
âˆš
1âˆ’ğœŒ2xy
â
âŸ
âŸâ 
+
(
1+
(ğœyâˆ’ğœŒxyğœx)2
ğœ2x (1âˆ’ğœŒ2xy)
)
u2
â¤
â¥
â¥â¦dğ‘¤du
= eğœ‡x+ 1
2 ğœ2
x
Ã— âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡y+ğœx(ğœxâˆ’ğœŒxyğœy)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘¤=âˆ’âˆ
1
2ğœ‹
Ã— e
âˆ’1
2
(
ğœ2
ğœ2x (1âˆ’ğœŒ2xy)
)â¡
â¢
â¢
â¢
â¢â£
ğ‘¤2
(
ğœ2
ğœ2x (1âˆ’ğœŒ2xy)
) âˆ’2uğ‘¤
ğœx(ğœyâˆ’ğœŒxyğœx)âˆš
1âˆ’ğœŒ2xy
ğœ2
+u2
â¤
â¥
â¥
â¥
â¥â¦dğ‘¤du.

1.2.2
Discrete and Continuous Random Variables
37
Finally, by setting ğ‘¤=
ğ‘¤
â›
âœ
âœ
âœâ
ğœ
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
,
I1 = eğœ‡x+ 1
2 ğœ2
x
âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡y+ğœx(ğœxâˆ’ğœŒxyğœy)
ğœ
ğ‘¤=âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy) (ğ‘¤2âˆ’2ğœŒxyuğ‘¤+u2)
dğ‘¤du
where ğœŒxy =
ğœy âˆ’ğœŒxyğœx
ğœ
.
Therefore,
I1 = eğœ‡x+ 1
2 ğœ2
x ğš½
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y + ğœx(ğœx âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
, âˆ, ğœŒxy
â
âŸ
âŸ
âŸâ 
= eğœ‡x+ 1
2 ğœ2
x Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y + ğœx(ğœx âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
where ğš½(ğ›¼, ğ›½, ğœŒ) = âˆ«
ğ›½
âˆ’âˆâˆ«
ğ›¼
âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2 e
âˆ’1
2
(
x2âˆ’2ğœŒxy+y2
1âˆ’ğœŒ2
)
dxdy is the cumulative distri-
bution function of a standard bivariate normal.
Using similar steps for the case I2 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
eyfXY(x, y) dxdy we have
I2 = âˆ«
y=âˆ
y=âˆ’âˆâˆ«
x=âˆ
x=y
ey
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒxy
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
= âˆ«
y=âˆ
y=âˆ’âˆ
ey
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2
Ã—
â¡
â¢
â¢
â¢â£
âˆ«
x=âˆ
x=y
1
ğœx
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[
xâˆ’
(
ğœ‡x+ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
))]2
dx
â¤
â¥
â¥
â¥â¦
dy
= âˆ«
y=âˆ
y=âˆ’âˆ
ey
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2 [
âˆ«
x=âˆ
x=y
g(x, y) dx
]
dy

38
1.2.2
Discrete and Continuous Random Variables
where g(x, y) =
1
ğœx
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[
xâˆ’
(
ğœ‡x+ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
))]2
which is the probability
density function of ğ’©
[
ğœ‡x + ğœŒxyğœx
(y âˆ’ğœ‡y
ğœy
)
, (1 âˆ’ğœŒxy)2ğœ2
x
]
.
Thus,
I2 = âˆ«
y=âˆ
y=âˆ’âˆ
ey
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2
â¡
â¢
â¢
â¢
â¢
â¢â£
âˆ«
x=âˆ
x=y
1
ğœx
âˆš
2ğœ‹(1 âˆ’ğœŒ2
xy)
e
âˆ’1
2
â›
âœ
âœ
âœâ
xâˆ’ğœ‡xâˆ’ğœŒxyğœx
( yâˆ’ğœ‡y
ğœy
)
ğœx
âˆš
1âˆ’ğœŒ2xy
â
âŸ
âŸ
âŸâ 
2
dx
â¤
â¥
â¥
â¥
â¥
â¥â¦
dy
and by setting z =
x âˆ’ğœ‡x âˆ’ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
,
I2 = âˆ«
y=âˆ
y=âˆ’âˆ
ey
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
(
yâˆ’ğœ‡y
ğœy
)2 â¡
â¢
â¢
â¢â£
âˆ«
z=âˆ
z=
yâˆ’ğœ‡xâˆ’ğœŒxyğœx
( yâˆ’ğœ‡y
ğœy
)
ğœx
âˆš
1âˆ’ğœŒ2xy
1
âˆš
2ğœ‹
eâˆ’1
2 z2 dz
â¤
â¥
â¥
â¥â¦
dy
= âˆ«
y=âˆ
y=âˆ’âˆ
1
ğœy
âˆš
2ğœ‹
e
âˆ’1
2
[(
yâˆ’ğœ‡y
ğœy
)2
âˆ’2y
]
Î¦
â›
âœ
âœ
âœ
âœâ
âˆ’y + ğœ‡x + ğœŒxyğœx
(
yâˆ’ğœ‡y
ğœy
)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸ
âŸâ 
dy.
By setting z =
y âˆ’ğœ‡y
ğœy
therefore
I2 = eğœ‡y+ 1
2 ğœ2
y
âˆ«
z=âˆ
z=âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 (zâˆ’ğœy)2Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’z(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
dz
and substituting u = z âˆ’ğœy,
I2 = eğœ‡y+ 1
2 ğœ2
y
âˆ«
u=âˆ
u=âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 u2Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’(u + ğœy)(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
du
= eğœ‡y+ 1
2 ğœ2
y
âˆ«
u=âˆ
u=âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 u2Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’ğœy(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
âˆ’
u(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
du

1.2.2
Discrete and Continuous Random Variables
39
= eğœ‡y+ 1
2 ğœ2
y
âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘£=
ğœ‡xâˆ’ğœ‡yâˆ’ğœy(ğœyâˆ’ğœŒxyğœx)
ğœx
âˆš
1âˆ’ğœŒ2xy
âˆ’
u(ğœyâˆ’ğœŒxyğœx)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘£=âˆ’âˆ
1
2ğœ‹eâˆ’1
2 (u2+ğ‘£2) dğ‘£du.
Using the same steps as described before, we let ğ‘¤= ğ‘£+
u(ğœy âˆ’ğœŒxyğœx)
ğœx
âˆš
1 âˆ’ğœŒ2
xy
and ğœ2 = ğœ2
x âˆ’
2ğœŒxyğœxğœy + ğœ2
y so that
I2 = eğœ‡y+ 1
2 ğœ2
y
Ã— âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡yâˆ’ğœy(ğœyâˆ’ğœŒxyğœx)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘¤=âˆ’âˆ
1
2ğœ‹
Ã— e
âˆ’1
2
â¡
â¢
â¢â£
ğ‘¤2âˆ’2uğ‘¤
â›
âœ
âœâ
ğœyâˆ’ğœŒxyğœx
ğœx
âˆš
1âˆ’ğœŒ2xy
â
âŸ
âŸâ 
+
(
1+
(ğœyâˆ’ğœŒxyğœx)2
ğœ2x (1âˆ’ğœŒ2xy)
)
u2
â¤
â¥
â¥â¦dğ‘¤du
= eğœ‡y+ 1
2 ğœ2
y
âˆ«
u=âˆ
u=âˆ’âˆ
Ã— âˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡yâˆ’ğœy(ğœyâˆ’ğœŒxyğœx)
ğœx
âˆš
1âˆ’ğœŒ2xy
ğ‘¤=âˆ’âˆ
1
2ğœ‹
Ã— e
âˆ’1
2
(
ğœ2
ğœ2x (1âˆ’ğœŒ2xy)
)â¡
â¢
â¢
â¢
â¢â£
ğ‘¤2
(
ğœ2
ğœ2x (1âˆ’ğœŒ2xy)
) âˆ’2uğ‘¤
ğœx(ğœyâˆ’ğœŒxyğœx)âˆš
1âˆ’ğœŒ2xy
ğœ2
+u2
â¤
â¥
â¥
â¥
â¥â¦dğ‘¤du.
By setting ğ‘¤=
ğ‘¤
â›
âœ
âœ
âœâ
ğœ
ğœx
âˆš
1 âˆ’ğœŒ2
xy
â
âŸ
âŸ
âŸâ 
,
I2 = eğœ‡y+ 1
2 ğœ2
y
âˆ«
u=âˆ
u=âˆ’âˆâˆ«
ğ‘¤=
ğœ‡xâˆ’ğœ‡yâˆ’ğœy(ğœyâˆ’ğœŒxyğœx)
ğœ
ğ‘¤=âˆ’âˆ
1
2ğœ‹
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy) (ğ‘¤2âˆ’2ğœŒxyuğ‘¤+u2)
dğ‘¤du
where ğœŒxy =
ğœy âˆ’ğœŒxyğœx
ğœ
, thus
I2 = eğœ‡y+ 1
2 ğœ2
y ğš½
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’ğœy(ğœy âˆ’ğœŒxyğœx)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
, âˆ, ğœŒxy
â
âŸ
âŸ
âŸâ 
= eğœ‡y+ 1
2 ğœ2
y Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’ğœy(ğœy âˆ’ğœŒxyğœx)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
.

40
1.2.2
Discrete and Continuous Random Variables
By substituting I1 and I2 back into ğ”¼[max{eX âˆ’eY, 0}] we have
ğ”¼[max {eX âˆ’eY, 0}] = eğœ‡x+ 1
2 ğœ2
x Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y + ğœx(ğœx âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
âˆ’eğœ‡y+ 1
2 ğœ2
y Î¦
â›
âœ
âœ
âœâ
ğœ‡x âˆ’ğœ‡y âˆ’ğœy(ğœy âˆ’ğœŒxyğœy)
âˆš
ğœ2
x âˆ’2ğœŒxyğœxğœy + ğœ2
y
â
âŸ
âŸ
âŸâ 
.
â—½
18. Markovâ€™s Inequality. Let X be a non-negative random variable with mean ğœ‡. For ğ›¼> 0,
show that
â„™(X â‰¥ğ›¼) â‰¤ğœ‡
ğ›¼.
Solution: Since ğ›¼> 0, we can write
1I{Xâ‰¥ğ›¼} =
{
1
X â‰¥ğ›¼
0
otherwise
and since X â‰¥0, we can deduce that
1I{Xâ‰¥ğ›¼} â‰¤X
ğ›¼.
Taking expectations
ğ”¼(1I{Xâ‰¥ğ›¼}
) â‰¤ğ”¼(X)
ğ›¼
and
since
ğ”¼(1I{Xâ‰¥ğ›¼}
) = 1 â‹…â„™(X â‰¥ğ›¼) + 0 â‹…â„™(X â‰¤ğ›¼) = â„™(X â‰¥ğ›¼)
and
ğ”¼(X) = ğœ‡,
we have
â„™(X â‰¥ğ›¼) â‰¤ğœ‡
ğ›¼.
N.B. Alternatively, we can also show the result as
ğ”¼(X) = âˆ«
âˆ
0
(1 âˆ’Fx(u)) du â‰¥âˆ«
ğ›¼
0
(1 âˆ’Fx(u)) du â‰¥ğ›¼(1 âˆ’Fx(ğ›¼))
and hence it follows that â„™(X â‰¥ğ›¼) = 1 âˆ’Fx(ğ›¼) â‰¤ğ”¼(X)
ğ›¼
.
â—½
19. Chebyshevâ€™s Inequality. Let X be a random variable with mean ğœ‡and variance ğœ2. Then
for k > 0, show that
â„™(|X âˆ’ğœ‡| â‰¥k) â‰¤ğœ2
k2 .
Solution: Take note that |X âˆ’ğœ‡| â‰¥k if and only if (X âˆ’ğœ‡)2 â‰¥k2. Because (X âˆ’ğœ‡)2 â‰¥0,
and by applying Markovâ€™s inequality (see Problem 1.2.2.18, page 40) we have
â„™(
(X âˆ’ğœ‡)2 â‰¥k2) â‰¤
ğ”¼[
(X âˆ’ğœ‡)2]
k2
= ğœ2
k2

1.2.3
Properties of Expectations
41
and hence the above inequality is equivalent to
â„™(|X âˆ’ğœ‡| â‰¥k) â‰¤ğœ2
k2 .
â—½
1.2.3
Properties of Expectations
1. Show that if X is a random variable taking non-negative values then
ğ”¼(X) =
â§
âª
âª
â¨
âª
âªâ©
âˆ
âˆ‘
x=0
â„™(X > x)
if X is a discrete random variable
âˆ«
âˆ
0
â„™(X â‰¥x) dx
if X is a continuous random variable.
Solution: We first show the result when X takes non-negative integer values only. By
definition
ğ”¼(X) =
âˆ
âˆ‘
y=0
yâ„™(X = y)
=
âˆ
âˆ‘
y=0
y
âˆ‘
x=0
â„™(X = y)
=
âˆ
âˆ‘
x=0
âˆ
âˆ‘
y=x+1
â„™(X = y)
=
âˆ
âˆ‘
x=0
â„™(X > x).
For the case when X is a continuous random variable taking non-negative values we have
ğ”¼(X) = âˆ«
âˆ
0
yfX(y) dy
= âˆ«
âˆ
0
{
âˆ«
y
0
fX(y) dx
}
dy
= âˆ«
âˆ
0
{
âˆ«
âˆ
x
fX(y) dy
}
dx
= âˆ«
âˆ
0
â„™(X â‰¥x) dx.
â—½
2. HÃ¶lderâ€™s Inequality. Let ğ›¼, ğ›½â‰¥0 and for p, q > 1 such that 1
p + 1
q = 1 show that the fol-
lowing inequality:
ğ›¼ğ›½â‰¤ğ›¼p
p + ğ›½q
q

42
1.2.3
Properties of Expectations
holds. Finally, if X and Y are a pair of jointly continuous variables, show that
ğ”¼(|XY|) â‰¤{ğ”¼(|Xp|)}1âˆ•p{ğ”¼(|Yq|)}1âˆ•q.
Solution: The inequality certainly holds for ğ›¼= 0 and ğ›½= 0. Let ğ›¼= exâˆ•p and ğ›½= eyâˆ•q
where x, y âˆˆâ„. By substituting ğœ†= 1âˆ•p and 1 âˆ’ğœ†= 1âˆ•q,
eğœ†x+(1âˆ’ğœ†)y â‰¤ğœ†ex + (1 âˆ’ğœ†)ey
holds true since the exponential function is a convex function and hence
ğ›¼ğ›½â‰¤ğ›¼p
p + ğ›½q
q .
By setting
ğ›¼=
|X|
{ğ”¼(|Xp|)}1âˆ•p ,
ğ›½=
|Y|
{ğ”¼(|Yq|)}1âˆ•q
hence
|XY|
{ğ”¼(|Xp|)}1âˆ•p{ğ”¼(|Yq|)}1âˆ•q â‰¤
|X|p
pğ”¼(|Xp|) +
|Y|q
qğ”¼(|Yq|).
Taking expectations we obtain
ğ”¼(|XY|) â‰¤{ğ”¼(|Xp|)}1âˆ•p{ğ”¼(|Yq|)}1âˆ•q.
â—½
3. Minkowskiâ€™s Inequality. Let X and Y be a pair of jointly continuous variables, show that
if p â‰¥1 then
{ğ”¼(
|X + Y|p)}1âˆ•p â‰¤{ğ”¼(|Xp|)}1âˆ•p + {ğ”¼(|Yp|)}1âˆ•p.
Solution: Since ğ”¼(|X + Y|) â‰¤ğ”¼(|X|) + ğ”¼(|Y|) and using HÃ¶lderâ€™s inequality we can
write
ğ”¼(|X + Y|p) = ğ”¼(|X + Y||X + Y|pâˆ’1)
â‰¤ğ”¼(|X||X + Y|pâˆ’1) + ğ”¼(|Y||X + Y|pâˆ’1)
â‰¤{ğ”¼(|Xp|)}1âˆ•p{ğ”¼(|X + Y|(pâˆ’1)q)}1âˆ•q + {ğ”¼(|Yp|)}1âˆ•p{ğ”¼(|X + Y|(pâˆ’1)q)}1âˆ•q
= {ğ”¼(|Xp|)}1âˆ•p{ğ”¼(|X + Y|p)}1âˆ•q + {ğ”¼(|Yp|)}1âˆ•p{ğ”¼(|X + Y|p)}1âˆ•q
since 1
p + 1
q = 1.
Dividing the inequality by {ğ”¼(|X + Y|p)}1âˆ•q we get
{ğ”¼(
|X + Y|p)}1âˆ•p â‰¤{ğ”¼(|Xp|)}1âˆ•p + {ğ”¼(|Yp|)}1âˆ•p.
â—½

1.2.3
Properties of Expectations
43
4. Change of Measure. Let Î© be a probability space and let â„™and â„šbe two probability
measures on Î©. Let Z(ğœ”) be the Radonâ€“NikodÂ´ym derivative defined as
Z(ğœ”) = â„š(ğœ”)
â„™(ğœ”)
such that â„™(Z > 0) = 1. By denoting ğ”¼â„™and ğ”¼â„šas expectations under the measure â„™and
â„š, respectively, show that for any random variable X,
ğ”¼â„š(X) = ğ”¼â„™(XZ),
ğ”¼â„™(X) = ğ”¼â„š(X
Z
)
.
Solution: By definition
ğ”¼â„š(X) =
âˆ‘
ğœ”âˆˆÎ©
X(ğœ”)â„š(ğœ”) =
âˆ‘
ğœ”âˆˆÎ©
X(ğœ”)Z(ğœ”)â„™(ğœ”) = ğ”¼â„™(XZ).
Similarly
ğ”¼â„™(X) =
âˆ‘
ğœ”âˆˆÎ©
X(ğœ”)â„™(ğœ”) =
âˆ‘
ğœ”âˆˆÎ©
X(ğœ”)
Z(ğœ”)â„š(ğœ”) = ğ”¼â„š(X
Z
)
.
â—½
5. Conditional Probability. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra
of â„±(i.e., sets in ğ’¢are also in â„±). If 1IA is an indicator random variable for an event A
defined as
1IA(ğœ”) =
{
1
if ğœ”âˆˆA
0
otherwise
show that
ğ”¼(1IA|ğ’¢) = â„™(A|ğ’¢).
Solution: Since ğ”¼(1IA|ğ’¢) is ğ’¢measurable we need to show that the following partial
averaging property:
âˆ«B
ğ”¼(1IA|ğ’¢) dâ„™= âˆ«B
1IA dâ„™= âˆ«B
â„™(A|ğ’¢) dâ„™
is satisfied for B âˆˆğ’¢. Setting
1IB(ğœ”) =
{
1
if ğœ”âˆˆB
0
otherwise
and
1IAâˆ©B(ğœ”) =
{
1
if ğœ”âˆˆA âˆ©B
0
otherwise
and expanding âˆ«B
â„™(A|ğ’¢) dâ„™we have
âˆ«B
â„™(A|ğ’¢) dâ„™= â„™(A âˆ©B) = âˆ«Î©
1IAâˆ©B dâ„™= âˆ«Î©
1IA â‹…1IB dâ„™= âˆ«B
1IA dâ„™.

44
1.2.3
Properties of Expectations
Since ğ”¼(1IA|ğ’¢) is ğ’¢measurable we have
âˆ«B
ğ”¼(1IA|ğ’¢) dâ„™= âˆ«B
1IA dâ„™
and hence ğ”¼(1IA|ğ’¢) = â„™(A|ğ’¢).
â—½
6. Linearity. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e., sets
in ğ’¢are also in â„±). If X1, X2, . . . , Xn are integrable random variables and c1, c2, . . . , cn
are constants, show that
ğ”¼(c1X1 + c2X2 + . . . + cnXn|ğ’¢) = c1ğ”¼(X1|ğ’¢) + c2ğ”¼(X2|ğ’¢) + . . . + cnğ”¼(Xn|ğ’¢).
Solution: Given ğ”¼(c1X1 + c2X2 + . . . + cnXn|ğ’¢) is ğ’¢measurable, and for any A âˆˆğ’¢,
âˆ«A
ğ”¼(c1X1 + c2X2 + . . . + cnXn|ğ’¢) dâ„™= âˆ«A
(c1X1 + c2X2 + . . . + cnXn) dâ„™
= c1âˆ«A
X1 dâ„™+ c2âˆ«A
X2 dâ„™
+ . . . + cnâˆ«A
Xn dâ„™.
Since âˆ«A
Xi dâ„™= âˆ«A
ğ”¼(Xi|ğ’¢) dâ„™for i = 1, 2, . . . , n therefore ğ”¼(c1X1 + c2X2 + . . . +
cnXn|ğ’¢) = c1ğ”¼(X1|ğ’¢) + c2ğ”¼(X2|ğ’¢) + . . . + cnğ”¼(Xn|ğ’¢).
â—½
7. Positivity. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e., sets
in ğ’¢are also in â„±). If X is an integrable random variable such that X â‰¥0 almost surely
then show that
ğ”¼(X|ğ’¢) â‰¥0
almost surely.
Solution: Let A = {ğ‘¤âˆˆÎ© âˆ¶ğ”¼(X|ğ’¢) < 0} and since ğ”¼(X|ğ’¢) is ğ’¢measurable therefore
A âˆˆğ’¢. Thus, from the partial averaging property we have
âˆ«A
ğ”¼(X|ğ’¢) dâ„™= âˆ«A
X dâ„™.
Since X â‰¥0 almost surely therefore âˆ«A
X dâ„™â‰¥0 but âˆ«A
ğ”¼(X|ğ’¢) dâ„™< 0, which is a con-
tradiction. Thus, â„™(A) = 0, which implies ğ”¼(X|ğ’¢) â‰¥0 almost surely.
â—½
8. Monotonicity. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e.,
sets in ğ’¢are also in â„±). If X and Y are integrable random variables such that X â‰¤Y almost
surely then show that
ğ”¼(X|ğ’¢) â‰¤ğ”¼(Y|ğ’¢).

1.2.3
Properties of Expectations
45
Solution: Since ğ”¼(X âˆ’Y|ğ’¢) is ğ’¢measurable, for A âˆˆğ’¢we can write
âˆ«A
ğ”¼(X âˆ’Y|ğ’¢) dâ„™= âˆ«A
(X âˆ’Y) dâ„™
and since X â‰¤Y, from Problem 1.2.3.7 (page 44) we can deduce that
âˆ«A
ğ”¼(X âˆ’Y|ğ’¢) dâ„™â‰¤0
and hence
ğ”¼(X âˆ’Y|ğ’¢) â‰¤0.
Using the linearity of conditional expectation (see Problem 1.2.3.6, page 44)
ğ”¼(X âˆ’Y|ğ’¢) = ğ”¼(X|ğ’¢) âˆ’ğ”¼(Y|ğ’¢) â‰¤0
and therefore ğ”¼(X|ğ’¢) â‰¤ğ”¼(Y|ğ’¢).
â—½
9. Computing Expectations by Conditioning. Let (Î©, â„±, â„™) be a probability space and let ğ’¢
be a sub-ğœ-algebra of â„±(i.e., sets in ğ’¢are also in â„±). Show that
ğ”¼[ğ”¼(X|ğ’¢)] = ğ”¼(X).
Solution: From the partial averaging property we have, for A âˆˆğ’¢,
âˆ«A
ğ”¼(X|ğ’¢) dâ„™= âˆ«A
X dâ„™
or
ğ”¼[1IA â‹…ğ”¼(X|ğ’¢)] = ğ”¼(1IA â‹…X)
where
1IA(ğœ”) =
{
1
if ğœ”âˆˆA
0
otherwise
is a ğ’¢measurable random variable. By setting A = Î© we obtain ğ”¼[ğ”¼(X|ğ’¢)] = ğ”¼(X).
â—½
10. Taking Out What is Known. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a
sub-ğœ-algebra of â„±(i.e., sets in ğ’¢are also in â„±). If X and Y are integrable random
variables and if X is ğ’¢measurable show that
ğ”¼(XY|ğ’¢) = X â‹…ğ”¼(Y|ğ’¢).

46
1.2.3
Properties of Expectations
Solution: Since X and ğ”¼(Y|ğ’¢) are ğ’¢measurable therefore X â‹…ğ”¼(Y|ğ’¢) is also ğ’¢measur-
able and it satisfies the first property of conditional expectation. By calculating the partial
averaging of X â‹…ğ”¼(Y|ğ’¢) over a set A âˆˆğ’¢and by defining
1IA(ğœ”) =
{
1
if ğœ”âˆˆA
0
otherwise
such that 1IA is a ğ’¢measurable random variable we have
âˆ«A
X â‹…ğ”¼(Y|ğ’¢) dâ„™= ğ”¼[1IA â‹…Xğ”¼(Y|ğ’¢)]
= ğ”¼[1IA â‹…XY]
= âˆ«A
XY dâ„™.
Thus, X â‹…ğ”¼(Y|ğ’¢) satisfies the partial averaging property by setting âˆ«A
XY dâ„™=
âˆ«A
ğ”¼(XY|ğ’¢) dâ„™. Therefore, ğ”¼(XY|ğ’¢) = X â‹…ğ”¼(Y|ğ’¢).
â—½
11. Tower Property. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±
(i.e., sets in ğ’¢are also in â„±). If â„‹is a sub-ğœ-algebra of ğ’¢(i.e., sets in â„‹are also in ğ’¢)
and X is an integrable random variable, show that
ğ”¼[ğ”¼(X|ğ’¢)|â„‹] = ğ”¼(X|â„‹).
Solution: For an integrable random variable Y, by definition we know that ğ”¼(Y|â„‹) is
â„‹measurable, and hence by setting Y = ğ”¼(X|ğ’¢), and for A âˆˆâ„‹, the partial averaging
property of ğ”¼[ğ”¼(X|ğ’¢)|â„‹] is
âˆ«A
ğ”¼[ğ”¼(X|ğ’¢)|â„‹] dâ„™= âˆ«A
ğ”¼(X|ğ’¢) dâ„™.
Since A âˆˆâ„‹and â„‹is a sub-ğœ-algebra of ğ’¢, A âˆˆğ’¢. Therefore,
âˆ«A
ğ”¼(X|â„‹) dâ„™= âˆ«A
X dâ„™= âˆ«A
ğ”¼(X|ğ’¢) dâ„™.
This shows that ğ”¼(X|â„‹) satisfies the partial averaging property of ğ”¼[ğ”¼(X|ğ’¢)|â„‹], and
hence ğ”¼[ğ”¼(X|ğ’¢)|â„‹] = ğ”¼(X|â„‹).
â—½
12. Measurability. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e.,
sets in ğ’¢are also in â„±). If the random variable X is ğ’¢measurable then show that
ğ”¼(X|ğ’¢) = X.

1.2.3
Properties of Expectations
47
Solution: From the partial averaging property, for A âˆˆÎ©,
âˆ«A
ğ”¼(X|ğ’¢) dâ„™= âˆ«A
X dâ„™
and if X is ğ’¢measurable then it satisfies
ğ”¼(X|ğ’¢) = X.
â—½
13. Independence. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e.,
sets in ğ’¢are also in â„±). If X = 1IB such that
1IB(ğœ”) =
{
1
if ğœ”âˆˆB
0
otherwise
and 1IB is independent of ğ’¢show that
ğ”¼(X|ğ’¢) = ğ”¼(X).
Solution: Since ğ”¼(X) is non-random then ğ”¼(X) is ğ’¢measurable. Therefore, we now need
to check that the following partial averaging property:
âˆ«A
ğ”¼(X) dâ„™= âˆ«A
X dâ„™= âˆ«A
ğ”¼(X|ğ’¢) dâ„™
is satisfied for A âˆˆğ’¢.
Let X = 1IB such that
1IB(ğœ”) =
{
1
if ğœ”âˆˆB
0
otherwise
and the random variable 1IB is independent of ğ’¢. In addition, we also define
1IA(ğœ”) =
{
1
if ğœ”âˆˆA
0
otherwise
where 1IA is ğ’¢measurable. For all A âˆˆğ’¢we have
âˆ«A
X dâ„™= âˆ«A
1IB dâ„™= âˆ«A
â„™(B) dâ„™= â„™(A)â„™(B).
Furthermore, since the sets A and B are independent we can also write
âˆ«A
X dâ„™= âˆ«A
1IB dâ„™= âˆ«Î©
1IA1IB dâ„™= âˆ«Î©
1IAâˆ©B dâ„™= â„™(A âˆ©B)

48
1.2.3
Properties of Expectations
where
1IAâˆ©B(ğœ”) =
{
1
if ğœ”âˆˆA âˆ©B
0
otherwise
and hence
âˆ«A
X dâ„™= â„™(A âˆ©B) = â„™(A)â„™(B) = â„™(A)ğ”¼(X) = âˆ«A
ğ”¼(X) dâ„™.
Thus, we have ğ”¼(X|ğ’¢) = ğ”¼(X).
â—½
14. Conditional Jensenâ€™s Inequality. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a
sub-ğœ-algebra of â„±(i.e., sets in ğ’¢are also in â„±). If ğœ‘âˆ¶â„î‚¶â†’â„is a convex function and
X is an integrable random variable show that
ğ”¼[ğœ‘(X)|ğ’¢] â‰¥ğœ‘[ğ”¼(X|ğ’¢)].
Deduce that if X is independent of ğ’¢then the above inequality becomes
ğ”¼[ğœ‘(X)] â‰¥ğœ‘[ğ”¼(X)].
Solution: Given that ğœ‘is a convex function,
ğœ‘(x) â‰¥ğœ‘(y) + ğœ‘â€²(y)(y âˆ’x).
By setting x = X and y = ğ”¼(X|ğ’¢) we have
ğœ‘(X) â‰¥ğœ‘[ğ”¼(X|ğ’¢)] + ğœ‘â€²[ğ”¼(X|ğ’¢)][ğ”¼(X|ğ’¢) âˆ’X]
and taking conditional expectations,
ğ”¼[ğœ‘(X)|ğ’¢] â‰¥ğœ‘[ğ”¼(X|ğ’¢)].
If X is independent of ğ’¢then from Problem 1.2.3.13 (page 47) we can set y = ğ”¼(X|ğ’¢) =
ğ”¼(X). Using the same steps as described above we have
ğœ‘(X) â‰¥ğœ‘[ğ”¼(X)] + ğœ‘â€²[ğ”¼(X)][ğ”¼(X) âˆ’X]
and taking expectations we finally have
ğ”¼[ğœ‘(X)] â‰¥ğœ‘[ğ”¼(X)].
â—½

1.2.3
Properties of Expectations
49
15. Let (Î©, â„±, â„™) be a probability space and let ğ’¢be a sub-ğœ-algebra of â„±(i.e., sets in ğ’¢are
also in â„±). If X is an integrable random variable and ğ”¼(X2) < âˆshow that
ğ”¼[ğ”¼(X|ğ’¢)2] â‰¤ğ”¼(X2) .
Solution: From the conditional Jensenâ€™s inequality (see Problem 1.2.3.14, page 48) we
set ğœ‘(x) = x2 which is a convex function. By substituting x = ğ”¼(X|ğ’¢) we have
ğ”¼(X|ğ’¢)2 â‰¤ğ”¼(X2|ğ’¢) .
Taking expectations
ğ”¼[ğ”¼(X|ğ’¢)2] â‰¤ğ”¼[ğ”¼(X2|ğ’¢)]
and from the tower property (see Problem 1.2.3.11, page 46)
ğ”¼[ğ”¼(X2|ğ’¢)] = ğ”¼(X2) .
Thus, ğ”¼[ğ”¼(X|ğ’¢)2] â‰¤ğ”¼(X2).
â—½


2
Wiener Process
In mathematics, a Wiener process is a stochastic process sharing the same behaviour as Brow-
nian motion, which is a physical phenomenon of random movement of particles suspended in a
fluid. Generally, the terms â€œBrownian motionâ€ and â€œWiener processâ€ are the same, although the
former emphasises the physical aspects whilst the latter emphasises the mathematical aspects.
In quantitative analysis, by drawing on the mathematical properties of Wiener processes to
explain economic phenomena, financial information such as stock prices, commodity prices,
interest rates, foreign exchange rates, etc. are treated as random quantities and then mathe-
matical models are constructed to capture the randomness. Given these financial models are
stochastic and continuous in nature, the Wiener process is usually employed to express the
random component of the model. Before we discuss the models in depth, in this chapter we
first look at the definition and basic properties of a Wiener process.
2.1
INTRODUCTION
By definition, a random walk is a mathematical formalisation of a trajectory that consists of
taking successive random steps at every point in time. To construct a Wiener process in con-
tinuous time, we begin by setting up a symmetric random walk â€“ such as tossing a fair coin
infinitely many times where the probability of getting a head (H) or a tail (T) in each toss is
1
2. By defining the i-th toss as
Zi =
{
1
if toss is H
âˆ’1
if toss is T
and setting M0 = 0, the process
Mk =
k
âˆ‘
i=1
Zi,
k = 1, 2, . . .
is a symmetric random walk. In a continuous time setting, to approximate a Wiener process
for n âˆˆâ„¤+ we define the scaled symmetric random walk as
W(n)
t
=
1
âˆš
n
MâŒŠntâŒ‹=
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
Zi
such that in the limit of n â†’âˆwe can obtain the Wiener process where
lim
nâ†’âˆW(n)
t
= lim
nâ†’âˆ
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
Zi
D
âˆ’âˆ’â†’ğ’©(0, t).

52
2.1
INTRODUCTION
Given that a Wiener process is a limiting distribution of a scaled symmetric random walk,
the following is the definition of a standard Wiener process.
Definition 2.1 (Standard Wiener Process) Let (Î©, â„±, â„™) be a probability space. A stochas-
tic process {Wt âˆ¶t â‰¥0} is defined to be a standard Wiener process (or â„™-standard Wiener
process) if:
(a) W0 = 0 and has continuous sample paths;
(b) for each t > 0 and s > 0, Wt+s âˆ’Wt âˆ¼ğ’©(0, s) (stationary increment);
(c) for each t > 0 and s > 0, Wt+s âˆ’Wt âŸ‚âŸ‚Wt (independent increment).
A standard Wiener process is a standardised version of a Wiener process, which need not
begin at W0 = 0, and may have a non-zero drift term ğœ‡â‰ 0 and a variance term not necessarily
equal to t.
Definition 2.2 (Wiener Process) A process { Ì‚Wt âˆ¶t â‰¥0} is called a Wiener process if it can
be written as
Ì‚Wt = ğœˆ+ ğœ‡t + ğœWt
where ğœˆ, ğœ‡âˆˆâ„, ğœ> 0 and Wt is a standard Wiener process.
Almost all financial models have the Markov property and without exception the Wiener
process also has this important property, where it is used to relate stochastic calculus to partial
differential equations and ultimately to the pricing of options. The following is an important
result concerning the Markov property of a standard Wiener process.
Theorem 2.3 (Markov Property) Let (Î©, â„±, â„™) be a probability space. The standard Wiener
process {Wt âˆ¶t â‰¥0} is a Markov process such that the conditional distribution of Wt given
the filtration â„±s, s < t depends only on Ws.
Another generalisation from the Markov property is the strong Markov property, which is an
important result in establishing many other properties of Wiener processes such as martingales.
Clearly, the strong Markov property implies the Markov property but not vice versa.
Theorem 2.4 (Strong Markov Property) Let (Î©, â„±, â„™) be a probability space. If {Wt âˆ¶t â‰¥
0} is a standard Wiener process and given â„±t is the filtration up to time t, then for s > 0,
Wt+s âˆ’Wt âŸ‚âŸ‚â„±t.
Once we have established the Markov properties, we can use them to show that a Wiener
process is a martingale. Basically, a stochastic process is a martingale when its conditional
expected value of an observation at some future time t, given all the observations up to some
earlier time s, is equal to the observation at that earlier time s. In formal terms the definition
of this property is given as follows.

2.1
INTRODUCTION
53
Definition 2.5 (Martingale Property for Continuous Process) Let (Î©, â„±, â„™) be a probabil-
ity space. A stochastic process {Xt âˆ¶t â‰¥0} is a continuous-time martingale if:
(a) ğ”¼(Xt|â„±s) = Xs, for all 0 â‰¤s â‰¤t;
(b) ğ”¼(|Xt|) < âˆ;
(c) Xt is â„±t-adapted.
In addition to properties (b) and (c), the process is a submartingale if ğ”¼(Xt|â„±s) â‰¥Xs and a
supermartingale if ğ”¼(Xt|â„±s) â‰¤Xs for all 0 â‰¤s â‰¤t.
In contrast, we can also define the martingale property for a discrete process.
Definition 2.6 (Martingale Property for Discrete Process) A discrete process X = {Xn âˆ¶
n = 0, 1, 2, . . .} is a martingale relative to (Î©, â„±, â„™) if for all n:
(a) ğ”¼(Xn+1|â„±n) = Xn;
(b) ğ”¼(|Xn|) < âˆ;
(c) Xn is â„±n-adapted.
Together with properties (b) and (c), the process is a submartingale if ğ”¼(Xn+1|â„±n) â‰¥Xn and a
supermartingale if ğ”¼(Xn+1|â„±n) â‰¤Xn for all n.
A most important application of the martingale property of a stochastic process is in the area
of derivatives pricing where under the risk-neutral measure, to avoid any arbitrage opportu-
nities, all asset prices have the same expected rate of return that is the risk-free rate. As we
shall see in later chapters, by modelling an asset price with a Wiener process to represent the
random component, under the risk-neutral measure, the expected future price of the asset dis-
counted at a risk-free rate given its past information is a martingale. In addition, martingales
do have certain features even when they are stopped at random times, as given in the following
theorem.
Theorem 2.7 (Optional Stopping (Sampling)) Let (Î©, â„±, â„™) be a probability space. A ran-
dom time T âˆˆ[0, âˆ) is called a stopping time if {T â‰¤t} âˆˆâ„±t for all t â‰¥0. If T < âˆis a
stopping time and {Xt} is a martingale, then ğ”¼(XT) = ğ”¼(X0) if any of the following are true:
(a) ğ”¼(T) < âˆ;
(b) there exists a constant K such that ğ”¼(|Xt+Î”t âˆ’Xt|) â‰¤K, for Î”t > 0.
Take note that an important application of the optional stopping theorem is the first passage
time of a standard Wiener process hitting a level. Here one can utilise it to analyse American
options, where in this case the exercise time is a stopping time. In tandem with the stop-
ping time, the following reflection principle result allows us to find the joint distribution of
(max0â‰¤sâ‰¤tWs, Wt) and the distribution of max0â‰¤sâ‰¤tWs, which in turn can be used to price exotic
options such as barrier and lookback options.

54
2.1
INTRODUCTION
Theorem 2.8 (Reflection Principle) Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥
0} be a standard Wiener process. By setting T as a stopping time and defining
ÌƒWt =
{
Wt
if t â‰¤T
2WT âˆ’Wt
if t > T
then { ÌƒWt âˆ¶t â‰¥0} is also a standard Wiener process.
Finally, another useful property of the Wiener process is the quadratic variation,
where if {Wt âˆ¶t â‰¥0} is a standard Wiener process then by expressing dWt as the
infinitesimal increment of Wt and setting ti = itâˆ•n, i = 0, 1, 2, . . . , n âˆ’1, n > 0 such that
0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, the quadratic variation of Wt is defined as
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)2 = âˆ«
t
0
dW2
u = t
which has a finite value. In addition, the cross-variation of Wt and t, the quadratic variation of
t and the p-order variation, p â‰¥3 of Wt can be expressed as
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)(ti+1 âˆ’ti) = âˆ«
t
0
dWudu = 0,
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti)2 = âˆ«
t
0
du2 = 0
and
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)p = âˆ«
t
0
dWp
u = 0,
p â‰¥3.
Informally, we can therefore write
(dWt)2 = dt,
dWtdt = 0,
(dt)2 = 0,
(dWt)p = 0,
p â‰¥3
where dWt and dt are the infinitesimal increment of Wt and t, respectively. The significance
of the above results constitutes the key ingredients in ItÂ¯oâ€™s formula to find the differential of a
stochastic function and also in deriving the Blackâ€“Scholes equation to price options.
In essence, the many properties of the Wiener process made it a very suitable choice to
express the random component when modelling stock prices, interest rates, foreign currency
exchange rates, etc. One notable example is when pricing European-style options, where,
due to its inherent properties we can obtain closed-form solutions which would not be pos-
sible had it been modelled using other processes. In addition, we could easily extend the
one-dimensional Wiener process to a multi-dimensional Wiener process to model stock prices
that are correlated with each other as well as stock prices under stochastic volatility. How-
ever, owing to the normal distribution of the standard Wiener process it tends to provide stock
price returns that are symmetric and short-tailed. In practical situations, stock price returns are
skewed and have heavy tails and hence they do not follow a normal distribution. Nevertheless,

2.2.1
Basic Properties
55
even if there exist complex models to capture such attributes (for example, using a LÃ©vy
process), a Wiener process is still a vital component to model sources of uncertainty in finance.
2.2
PROBLEMS AND SOLUTIONS
2.2.1
Basic Properties
1. Let (Î©, â„±, â„™) be a probability space. We consider a symmetric random walk such that the
j-th step is defined as
Zj =
â§
âª
â¨
âªâ©
1
with probability 1
2
âˆ’1
with probability 1
2
where Zi âŸ‚âŸ‚Zj, i â‰ j. By setting 0 = k0 < k1 < k2 < . . . < kt, we let
Mki =
ki
âˆ‘
j=1
Zj,
i = 1, 2, . . . , t
where M0 = 0.
Show that the symmetric random walk has independent increments such that the random
variables
Mk1 âˆ’Mk0, Mk2 âˆ’Mk1, . . . , Mkt âˆ’Mktâˆ’1
are independent.
Finally, show that ğ”¼(Mki+1 âˆ’Mki) = 0 and Var(Mki+1 âˆ’Mki) = ki+1 âˆ’ki.
Solution: By definition
Mki âˆ’Mkiâˆ’1 =
ki
âˆ‘
j=kiâˆ’1+1
Zj = si
and for m < n, m, n = 1, 2, . . . , t,
â„™(Mkn âˆ’Mknâˆ’1 = sn|Mkm âˆ’Mkmâˆ’1 = sm)
=
â„™(Mkn âˆ’Mknâˆ’1 = sn, Mkm âˆ’Mkmâˆ’1 = sm)
â„™(Mkm âˆ’Mkmâˆ’1 = sm)
= â„™(sum of walks in [knâˆ’1 + 1, kn] âˆ©sum of walks in [kmâˆ’1 + 1, km])
â„™(sum of walks in [kmâˆ’1 + 1, km])
.
Because m < n and since Zi is independent of Zj, i â‰ j, there are no overlapping
events between the intervals [knâˆ’1 + 1, kn] and [kmâˆ’1 + 1, km] and hence for m < n,
m, n = 1, 2, . . . , t,
â„™(Mkn âˆ’Mknâˆ’1 = sn|Mkm âˆ’Mkmâˆ’1 = sm) = â„™(sum of walks in [knâˆ’1 + 1, kn])
= â„™(Mkn âˆ’Mknâˆ’1 = sn).

56
2.2.1
Basic Properties
Thus, we can deduce that Mk1 âˆ’Mk0, Mk2 âˆ’Mk1, . . . , Mkt âˆ’Mktâˆ’1 are independent.
Since ğ”¼(Zj) = 1 â‹…1
2 âˆ’1 â‹…1
2 = 0 and Var(Zj) = ğ”¼(Z2
j ) = 1 â‹…1
2 + 1 â‹…1
2 = 1, and because Zj,
j = 1, 2, . . . are independent, we have
ğ”¼(Mki âˆ’Mkiâˆ’1) =
ki
âˆ‘
j=kiâˆ’1+1
ğ”¼(Zj) = 0
and
Var(Mki âˆ’Mkiâˆ’1) =
ki
âˆ‘
j=kiâˆ’1+1
Var(Zj) =
ki
âˆ‘
j=kiâˆ’1+1
1 = ki âˆ’kiâˆ’1.
â—½
2. Let (Î©, â„±, â„™) be a probability space. For a symmetric random walk
Mk =
k
âˆ‘
i=1
Zi
with starting point at M0 = 0, â„™(Zi = 1) = â„™(Zi = âˆ’1) = 1
2, show that Mk is a martingale.
Solution: To show that Mk is a martingale we note that
(a) For j < k, j, k âˆˆâ„¤+, using the independent increment property and ğ”¼(Mk âˆ’Mj) = 0
as shown in Problem 2.2.1.1 (page 55),
ğ”¼(Mk|â„±j) = ğ”¼(Mk âˆ’Mj + Mj|â„±j)
= ğ”¼(Mk âˆ’Mj|â„±j) + ğ”¼(Mj|â„±j)
= ğ”¼(Mk âˆ’Mj) + Mj
= Mj.
(b) Given Mk = âˆ‘k
i=1 Zi it follows that
|Mk| =
||||||
k
âˆ‘
i=1
Zi
||||||
â‰¤
k
âˆ‘
i=1
|Zi| =
k
âˆ‘
i=1
1 = k < âˆ.
(c) Mk is clearly â„±k-adapted.
From the results of (a)â€“(c), we have shown that Mk is a martingale.
â—½
3. Donsker Theorem. Let (Î©, â„±, â„™) be a probability space. For a symmetric random walk
Mk =
k
âˆ‘
i=1
Zi

2.2.1
Basic Properties
57
where M0 = 0, â„™(Zi = 1) = â„™(Zi = âˆ’1) = 1
2 and by defining
W(n)
t
=
1
âˆš
n
MâŒŠntâŒ‹=
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
Zi
for a fixed time t, show that
lim
nâ†’âˆW(n)
t
= lim
nâ†’âˆ
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
Zi
D
âˆ’âˆ’â†’ğ’©(0, t).
Solution: Since for all i = 1, 2, . . . ,
ğ”¼(Zi) = 0
and
Var(Zi) = 1
and given Zi âŸ‚âŸ‚Zj, i â‰ j,
ğ”¼
(
W(n)
t
)
=
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
ğ”¼(Zi) = 0
and
Var
(
W(n)
t
)
= 1
n
âŒŠntâŒ‹
âˆ‘
i=1
Var(Zi) = 1
n
âŒŠntâŒ‹
âˆ‘
i=1
1 = âŒŠntâŒ‹
n .
For n â†’âˆwe can deduce that
lim
nâ†’âˆğ”¼
(
W(n)
t
)
= 0
and
lim
nâ†’âˆVar
(
W(n)
t
)
= lim
nâ†’âˆ
âŒŠntâŒ‹
n
= t.
Therefore, from the central limit theorem,
lim
nâ†’âˆW(n)
t
= lim
nâ†’âˆ
1
âˆš
n
âŒŠntâŒ‹
âˆ‘
i=1
Zi
D
âˆ’âˆ’â†’ğ’©(0, t).
â—½
4. Covariance of Two Standard Wiener Processes. Let (Î©, â„±, â„™) be a probability space and
let {Wt âˆ¶t â‰¥0} be a standard Wiener process. Show that
Cov(Ws, Wt) = min{s, t}
and deduce that the correlation coefficient of Ws and Wt is
ğœŒ=
âˆš
min{s, t}
max{s, t}.

58
2.2.1
Basic Properties
Solution: Since Wt âˆ¼ğ’©(0, t), Ws âˆ¼ğ’©(0, s) and by definition
Cov(Ws, Wt) = ğ”¼(WsWt) âˆ’ğ”¼(Ws)ğ”¼(Wt) = ğ”¼(WsWt).
Let s â‰¤t and because Ws âŸ‚âŸ‚Wt âˆ’Ws,
ğ”¼(WsWt) = ğ”¼(Ws(Wt âˆ’Ws) + W2
s
)
= ğ”¼(Ws(Wt âˆ’Ws)) + ğ”¼(W2
s
)
= ğ”¼(Ws)ğ”¼(Wt âˆ’Ws) + ğ”¼(W2
s
)
= s.
For s > t and because Wt âŸ‚âŸ‚Ws âˆ’Wt,
ğ”¼(WsWt) = ğ”¼(Wt(Ws âˆ’Wt) + W2
t
)
= ğ”¼(Wt)ğ”¼(Ws âˆ’Wt) + ğ”¼(W2
t
)
= t.
Therefore, Cov(Ws, Wt) = s âˆ§t = min{s, t}.
By definition, the correlation coefficient of Ws and Wt is defined as
ğœŒ=
Cov(Ws, Wt)
âˆš
Var(Ws)Var(Wt)
= min{s, t}
âˆš
st
.
For s â‰¤t,
ğœŒ=
s
âˆš
st
=
âˆš
s
t
whilst for s > t,
ğœŒ=
t
âˆš
st
=
âˆš
t
s.
Therefore, ğœŒ=
âˆš
min{s, t}
max{s, t}.
â—½
5. Joint Distribution of Standard Wiener Processes. Let (Î©, â„±, â„™) be a probability space and
let {Wtk âˆ¶tk â‰¥0}, k = 0, 1, 2, . . . , n be a standard Wiener process where 0 = t0 < t1 <
t2 < . . . < tn.
Find the moment generating function of the joint distribution (Wt1, Wt2, . . . , Wtn) and its
corresponding probability density function.

2.2.1
Basic Properties
59
Show that for t < T, the conditional distributions
Wt|WT = y âˆ¼ğ’©
(yt
T , t(T âˆ’t)
T
)
and
WT|Wt = x âˆ¼ğ’©(x, T âˆ’t).
Solution: By definition, the moment generating function for the joint distribution (Wt1,
Wt2, . . . , Wtn) is given as
MWt1,Wt2, ... ,Wtn
(
ğœƒ1, ğœƒ2, . . . , ğœƒn
)
= ğ”¼
(
eğœƒ1Wt1+ğœƒ2Wt2+ ... +ğœƒnWtn
)
,
ğœƒ1, ğœƒ2, . . . , ğœƒn âˆˆâ„.
Given that Wt1, Wt2 âˆ’Wt1, . . . , Wtn âˆ’Wtnâˆ’1 are independent and normally distributed, we
can write
ğœƒ1Wt1 + ğœƒ2Wt2 + . . . + ğœƒnWtn = (ğœƒ1 + ğœƒ2 + . . . + ğœƒn)Wt1 + (ğœƒ2 + . . . + ğœƒn)(Wt2 âˆ’Wt1)
+ . . . + ğœƒn(Wtn âˆ’Wtnâˆ’1)
and since for any ğœƒ, s < t, eğœƒ(Wtâˆ’Ws) âˆ¼log-ğ’©(0, ğœƒ2(t âˆ’s)) therefore
ğ”¼
(
eğœƒ1Wt1+ğœƒ2Wt2+ ... +ğœƒnWtn
)
= ğ”¼
[
e(ğœƒ1+ğœƒ2+ ... +ğœƒn)Wt1+(ğœƒ2+ ... +ğœƒn)
(
Wt2âˆ’Wt1
)
+ ... +ğœƒn
(
Wtnâˆ’Wtnâˆ’1
)]
= ğ”¼
[
e(ğœƒ1+ğœƒ2+ ... +ğœƒn)Wt1
]
ğ”¼
[
e(ğœƒ2+ ... +ğœƒn)(Wt2âˆ’Wt1)]
Ã— . . . Ã— ğ”¼
[
e
ğœƒn
(
Wtnâˆ’Wtnâˆ’1
)]
= e
1
2(ğœƒ1+ğœƒ2+ ... +ğœƒn)2t1+ 1
2(ğœƒ2+ ... +ğœƒn)2(t2âˆ’t1)+ ... + 1
2 ğœƒ2
n(tnâˆ’tnâˆ’1)
= e
1
2 ğœ½Tğšºğœ½
where ğœ½T = (ğœƒ1, ğœƒ2, . . . , ğœƒn) and ğšºis the covariance matrix for the Wiener process. From
Problem 2.2.1.4 (page 57) we can express
ğšº=
â¡
â¢
â¢
â¢
â¢â£
ğ”¼(W2
t1)
ğ”¼(Wt1Wt2) . . . ğ”¼(Wt1Wtn)
ğ”¼(Wt2Wt1) ğ”¼(W2
t2)
. . . ğ”¼(Wt2Wtn)
â‹®
â‹®
â‹±â‹®
ğ”¼(WtnWt1) ğ”¼(WtnWt2) . . . ğ”¼(W2
tn)
â¤
â¥
â¥
â¥
â¥â¦
=
â¡
â¢
â¢
â¢â£
t1 t1 . . . t1
t1 t2 . . . t2
â‹®â‹®â‹±â‹®
t1 t2 . . . tn
â¤
â¥
â¥
â¥â¦
.
Using elementary column operations we can easily show that the determinant
|ğšº| = âˆn
i=1(ti âˆ’tiâˆ’1) â‰ 0. Therefore, ğšºâˆ’1 exists and the probability density function for
the joint distribution (Wt1, Wt2, . . . , Wtn) is given as
fWt1,Wt2, ... ,Wtn (x) =
1
(2ğœ‹)
n
2 |ğšº|
1
2
eâˆ’1
2 xTğšºâˆ’1x
where xT = (x1, x2, . . . , xn).

60
2.2.1
Basic Properties
For the case of joint distribution of (Wt, WT), t < T we can deduce that
fWt,WT(x, y) =
1
(2ğœ‹)
âˆš
t(T âˆ’t)
exp
[
âˆ’1
2
(Tx2 âˆ’2txy + ty2
t(T âˆ’t)
)]
with conditional density function of Wt given WT = y
fWt|WT(x|y) =
fWt,WT(x, y)
fWT(y)
=
1
âˆš
2ğœ‹
(t(T âˆ’t)
T
)
exp
â¡
â¢
â¢
â¢â£
âˆ’1
2
(
x âˆ’yt
T
)2
t(T âˆ’t)
T
â¤
â¥
â¥
â¥â¦
and conditional density function of WT given Wt = x
fWT|Wt(y|x) =
fWt,WT(x, y)
fWt(x)
=
1
âˆš
2ğœ‹(T âˆ’t)
exp
[
âˆ’1
2
(y âˆ’x)2
T âˆ’t
]
.
Thus,
Wt|WT = y âˆ¼ğ’©
(yt
T , t(T âˆ’t)
T
)
and
WT|Wt = x âˆ¼ğ’©(x, T âˆ’t).
â—½
6. Reflection. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener
process. Show that under reflection, Bt = âˆ’Wt is also a standard Wiener process.
Solution:
(a) B0 = âˆ’W0 = 0, and given that Wt has continuous sample paths we can deduce that Bt
has continuous sample paths as well.
(b) For t > 0, s > 0
Bt+s âˆ’Bt = âˆ’Wt+s + Wt = âˆ’(Wt+s âˆ’Wt) âˆ¼ğ’©(0, s).
(c) Since Wt+s âˆ’Wt âŸ‚âŸ‚Wt therefore
ğ”¼[(Bt+s âˆ’Bt)Bt] = Cov(âˆ’Wt+s + Wt, âˆ’Wt) = âˆ’Cov(Wt+s âˆ’Wt, Wt) = 0.
Given Bt âˆ¼ğ’©(0, t) and Bt+s âˆ’Bt âˆ¼ğ’©(0, s) and the joint distribution of Bt and Bt+s âˆ’
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s âˆ’Bt, Bt) = 0
so Bt+s âˆ’Bt âŸ‚âŸ‚Bt.
From the results of (a)â€“(c) we have shown that Bt = âˆ’Wt is also a standard Wiener pro-
cess.
â—½

2.2.1
Basic Properties
61
7. Time Shifting. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. Show that under time shifting, Bt = Wt+u âˆ’Wu, u > 0 is also a standard
Wiener process.
Solution: By setting Ìƒt = t + u.
(a) B0 = Wu âˆ’Wu = 0, and given Wt has continuous sample paths therefore we can
deduce Bt has continuous sample paths as well.
(b) For t > 0, u > 0, s > 0
Bt+s âˆ’Bt = WÌƒt+s âˆ’WÌƒt+u âˆ¼ğ’©(0, s).
(c) Since Wt+s âˆ’Wt âŸ‚âŸ‚Wt therefore
ğ”¼[(Bt+s âˆ’Bt)Bt] = Cov(Wt+u+s âˆ’Wt+u, Wt+u) = Cov(WÌƒt +s âˆ’WÌƒt , WÌƒt ) = 0.
Given Bt âˆ¼ğ’©(0, t) and Bt+s âˆ’Bt âˆ¼ğ’©(0, s) and the joint distribution of Bt and Bt+s âˆ’
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s âˆ’Bt, Bt) = 0
so Bt+s âˆ’Bt âŸ‚âŸ‚Bt.
From the results of (a)â€“(c) we have shown that Bt = Wt+u âˆ’Wu is also a standard Wiener
process.
â—½
8. Normal Scaling. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. Show that under normal scaling, Bt = cW t
c2 , c â‰ 0 is also a standard
Wiener process.
Solution:
(a) B0 = cW0 = 0 and it is clear that Bt has continuous sample paths for t â‰¥0 and c â‰ 0.
(b) For t > 0, s > 0
Bt+s âˆ’Bt = c
(
W t+s
c2 âˆ’W t
c2
)
with mean
ğ”¼(Bt+s âˆ’Bt) = cğ”¼
(
W t+s
c2
)
âˆ’cğ”¼
(
W t
c2
)
= 0
and variance
Var(Bt+s âˆ’Bt) = Var
(
cW t+s
c2
)
+ Var
(
cW t
c2
)
âˆ’2Cov
(
cW t+s
c2 , cW t
c2
)
= t + s + t âˆ’2min{t + s, t}
= s.
Since both W t+s
c2 âˆ¼ğ’©
(
0, t + s
c2
)
and W t
c2 âˆ¼ğ’©
(
0, t
c2
)
, then Bt+s âˆ’Bt âˆ¼ğ’©(0, s).

62
2.2.1
Basic Properties
(c) Finally, to show that Bt+s âˆ’Bt âŸ‚âŸ‚Bt we note that since Wt has the independent incre-
ment property, so
ğ”¼[(Bt+s âˆ’Bt
) Bt
] = ğ”¼(Bt+sBt) âˆ’ğ”¼(B2
t
)
= Cov(Bt+s, Bt) âˆ’ğ”¼(B2
t
)
= min{t + s, t} âˆ’t
= 0.
Since Bt âˆ¼ğ’©(0, t) and Bt+s âˆ’Bt âˆ¼ğ’©(0, s) and the joint distribution of Bt and Bt+s âˆ’
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s âˆ’Bt, Bt) = 0
so Bt+s âˆ’Bt âŸ‚âŸ‚Bt.
From the results of (a)â€“(c) we have shown that Bt = cW t
c2 , c â‰ 0 is also a standard Wiener
process.
â—½
9. Time Inversion. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. Show that under time inversion,
Bt =
{
0
if t = 0
tW 1
t
if t â‰ 0
is also a standard Wiener process.
Solution:
(a) B0 = 0 and it is clear that Bt has continuous sample paths for t > 0. From continuity
at t = 0 we can deduce that tW 1
t â†’0 as t â†’0.
(b) Since Wt âˆ¼ğ’©(0, t) we can deduce tW 1
t âˆ¼ğ’©(0, t), t > 0. Therefore,
Bt+s âˆ’Bt = (t + s)W 1
t+s âˆ’tW 1
t
with mean
ğ”¼(Bt+s âˆ’Bt) = ğ”¼
(
(t + s)W 1
t+s
)
âˆ’ğ”¼
(
tW 1
t
)
= 0
and variance
Var(Bt+s âˆ’Bt) = Var
(
(t + s)W 1
t+s
)
+ Var
(
tW 1
t
)
âˆ’2Cov
(
(t + s)W 1
t+s , tW 1
t
)
= t + s + t âˆ’2min{t + s, s}
= s.
Since the sum of two normal distributions is also a normal distribution, so Bt+s âˆ’Bt âˆ¼
ğ’©(0, s).

2.2.1
Basic Properties
63
(c) To show that Bt+s âˆ’Bt âŸ‚âŸ‚Bt we note that since Wt has the independent increment
property, so
ğ”¼[(Bt+s âˆ’Bt)Bt] = ğ”¼(Bt+sBt) âˆ’ğ”¼(B2
t
)
= Cov(Bt+s, Bt) âˆ’ğ”¼(B2
t
)
= min{t + s, t} âˆ’t
= 0.
Since Bt âˆ¼ğ’©(0, t) and Bt+s âˆ’Bt âˆ¼ğ’©(0, s) and the joint distribution of Bt and Bt+s âˆ’
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s âˆ’Bt, Bt) = 0
so Bt+s âˆ’Bt âŸ‚âŸ‚Bt.
From the results of (a)â€“(c) we have shown that Bt =
{
0
if t = 0
tW 1
t if t â‰ 0 is also a standard
Wiener process.
â—½
10. Time Reversal. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. Show that under time reversal, Bt = W1 âˆ’W1âˆ’t is also a standard Wiener
process.
Solution:
(a) B0 = W1 âˆ’W1 = 0 and it is clear that Bt has continuous sample paths for t â‰¥0.
(b) Since Wt is a standard Wiener process, we have W1 âˆ¼ğ’©(0, 1) and W1âˆ’t âˆ¼ğ’©(0, 1 âˆ’t).
Therefore,
Bt+s âˆ’Bt = W1 âˆ’W1âˆ’(t+s) âˆ’W1 + W1âˆ’t = W1âˆ’t âˆ’W1âˆ’(t+s)
with mean
ğ”¼(Bt+s âˆ’Bt) = ğ”¼(W1âˆ’t) âˆ’ğ”¼(W1âˆ’(t+s)) = 0
and variance
Var(Bt+s âˆ’Bt) = Var(W1âˆ’t) + Var(W1âˆ’(t+s)) âˆ’2Cov(W1âˆ’t, W1âˆ’(t+s))
= 1 âˆ’t + 1 âˆ’(t + s) âˆ’2min{1 âˆ’t, 1 âˆ’(t + s)}
= s.
Since the sum of two normal distributions is also a normal distribution, so Bt+s âˆ’Bt âˆ¼
ğ’©(0, s).
(c) To show that Bt+s âˆ’Bt âŸ‚âŸ‚Bt we note that since Wt has the independent increment
property, so
ğ”¼[(Bt+s âˆ’Bt)Bt] = ğ”¼(Bt+sBt) âˆ’ğ”¼(B2
t
)
= Cov(Bt+s, Bt) âˆ’ğ”¼(B2
t
)

64
2.2.1
Basic Properties
= min{t + s, t} âˆ’t
= 0.
Given Bt âˆ¼ğ’©(0, t) and Bt+s âˆ’Bt âˆ¼ğ’©(0, s) and the joint distribution of Bt and Bt+s âˆ’
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s âˆ’Bt, Bt) = 0
so Bt+s âˆ’Bt âŸ‚âŸ‚Bt.
From the results of (a)â€“(c) we have shown that Bt = W1 âˆ’W1âˆ’t is also a standard Wiener
process.
â—½
11. Multi-Dimensional Standard Wiener Process. Let (Î©, â„±, â„™) be a probability space and let
{W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n be a sequence of independent standard Wiener processes.
Show that the vector Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)T
is an n-dimensional standard Wiener
process with the following properties:
(a) W0 = ğŸand Wt is a vector of continuous sample paths;
(b) for each t > 0 and s > 0, Wt+s âˆ’Wt âˆ¼ğ’©n(ğŸ, sI) where I is an n Ã— n identity matrix;
(c) for each t > 0 and s > 0, Wt+s âˆ’Wt âŸ‚âŸ‚Wt.
Solution:
(a) Since W(i)
0 = 0 for all i = 1, 2, . . . , n therefore W0 =
(
W(1)
0 , W(2)
0 , . . . , W(n)
0
)T
= ğŸ
where ğŸ= (0, 0, . . . , 0)T is an n-vector of zeroes. In addition, Wt is a vector of con-
tinuous sample paths due to the fact that W(i)
t , t â‰¥0 has continuous sample paths.
(b) For s, t > 0 and for i, j = 1, 2, . . . , n we have
ğ”¼(W(i)
t+s âˆ’W(i)
t
) = 0,
i = 1, 2, . . . , n
and
ğ”¼
[(
W(i)
t+s âˆ’W(i)
t
) (
W(j)
t+s âˆ’W(j)
t
) ]
=
{
s
i = j
0
i â‰ j.
Therefore, Wt+s âˆ’Wt âˆ¼ğ’©n(ğŸ, sI) where I is an n Ã— n identity matrix.
(c) For s, t > 0 and since W(i)
t+s âˆ’W(i)
t
âŸ‚âŸ‚W(i)
t , W(i)
t
âŸ‚âŸ‚W(j)
t , i â‰ j, i, j = 1, 2, . . . , n we can
deduce that W(i)
t+s âˆ’W(i)
t
âŸ‚âŸ‚W(j)
t . Therefore, Wt+s âˆ’Wt âŸ‚âŸ‚Wt.
From the results of (a)â€“(c) we have shown that Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)T
is an
n-dimensional standard Wiener process.
â—½
12. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that the pair of random variables
(
Wt, âˆ«t
0 Ws ds
)
has the following covariance
matrix
ğšº=
â¡
â¢
â¢â£
t
1
2t2
1
2t2
1
3t3
â¤
â¥
â¥â¦
with correlation coefficient
âˆš
3
2 .

2.2.1
Basic Properties
65
Solution: By definition, the covariance matrix for the pair
(
Wt, âˆ«t
0 Ws ds
)
is
ğšº=
â¡
â¢
â¢â£
Var(Wt)
Cov
(
Wt, âˆ«t
0 Ws ds
)
Cov
(
Wt, âˆ«t
0 Ws ds
)
Var
(
âˆ«t
0 Ws ds
)
â¤
â¥
â¥â¦
.
Given that Wt âˆ¼ğ’©(0, t) we know
Var(Wt) = t.
For the case Cov
(
Wt, âˆ«t
0 Ws ds
)
we can write
Cov
(
Wt, âˆ«
t
0
Ws ds
)
= ğ”¼
(
Wt âˆ«
t
0
Ws ds
)
âˆ’ğ”¼(Wt)ğ”¼
(
âˆ«
t
0
Ws ds
)
= ğ”¼
(
Wt âˆ«
t
0
Ws ds
)
= ğ”¼
(
âˆ«
t
0
WtWs ds
)
= âˆ«
t
0
ğ”¼(WtWs) ds
= âˆ«
t
0
ğ”¼[Ws(Wt âˆ’Ws) + W2
s
] ds.
From the independent increment property of a Wiener process we have
Cov
(
Wt, âˆ«
t
0
Ws ds
)
= âˆ«
t
0
ğ”¼(Ws)ğ”¼(Wt âˆ’Ws) ds + âˆ«
t
0
ğ”¼
(
W2
s
)
ds
= âˆ«
t
0
ğ”¼(W2
s
) ds
= âˆ«
t
0
s ds
= t2
2 .
Finally,
Var
(
âˆ«
t
0
Ws ds
)
= ğ”¼
[(
âˆ«
t
0
Ws ds
)2]
âˆ’
[
ğ”¼
(
âˆ«
t
0
Ws ds
)]2
= ğ”¼
[(
âˆ«
t
0
Ws ds
) (
âˆ«
t
0
Wu du
)]
âˆ’
[
âˆ«
t
0
ğ”¼(Ws)
]2

66
2.2.1
Basic Properties
= ğ”¼
[
âˆ«
s=t
s=0 âˆ«
u=t
u=0
WsWu duds
]
= âˆ«
s=t
s=0 âˆ«
u=t
u=0
ğ”¼(WsWu) duds.
From Problem 2.2.1.4 (page 57) we have
ğ”¼(WsWu
) = min{s, u}
and hence
Var
(
âˆ«
t
0
Ws ds
)
= âˆ«
s=t
s=0 âˆ«
u=t
u=0
min{s, u} duds
= âˆ«
s=t
s=0 âˆ«
u=s
u=0
u duds + âˆ«
s=t
s=0 âˆ«
u=t
u=s
s duds
= âˆ«
t
0
1
2s2 ds + âˆ«
t
0
s(t âˆ’s) ds
= 1
3t3.
Therefore,
ğšº=
â¡
â¢
â¢â£
t
1
2t2
1
2t2
1
3t3
â¤
â¥
â¥â¦
with correlation coefficient
ğœŒ=
Cov
(
Wt, âˆ«
t
0
Ws ds
)
âˆš
Var(Wt)Var
(
âˆ«
t
0
Ws ds
) =
t2âˆ•2
t2âˆ•
âˆš
3
=
âˆš
3
2 .
â—½
13. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that the covariance of âˆ«s
0 Wu du and âˆ«t
0 Wğ‘£dğ‘£, s, t > 0 is
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
= 1
3min{s3, t3} + 1
2|t âˆ’s|min{s2, t2}
with correlation coefficient
âˆš
min{s3, t3}
max{s3, t3} + 3
2|t âˆ’s|
âˆš
min{s, t}
max{s3, t3}.

2.2.1
Basic Properties
67
Solution: By definition
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
= ğ”¼
[(
âˆ«
s
0
Wu du
) (
âˆ«
t
0
Wğ‘£dğ‘£
)]
âˆ’ğ”¼
[
âˆ«
s
0
Wu du
]
ğ”¼
[
âˆ«
t
0
Wğ‘£dğ‘£
]
= âˆ«
s
0 âˆ«
t
0
ğ”¼(WuWğ‘£) dudğ‘£âˆ’
[
âˆ«
s
0
ğ”¼(Wu) du
] [
âˆ«
t
0
ğ”¼(Wğ‘£) dğ‘£
]
= âˆ«
s
0 âˆ«
t
0
min{u, ğ‘£} dudğ‘£
since ğ”¼(WuWğ‘£) = min{u, ğ‘£} and ğ”¼(Wu) = ğ”¼(Wğ‘£) = 0.
For the case s â‰¤t, using the results of Problem 2.2.1.12 (page 64),
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
= âˆ«
s
0 âˆ«
t
0
min{u, ğ‘£} dudğ‘£
= âˆ«
s
0 âˆ«
s
0
min{u, ğ‘£} dudğ‘£+ âˆ«
s
0 âˆ«
t
s
min{u, ğ‘£} dudğ‘£
= 1
3s3 + âˆ«
s
0 âˆ«
t
s
ğ‘£dudğ‘£
= 1
3s3 + âˆ«
s
0
ğ‘£(t âˆ’s) dğ‘£
= 1
3s3 + 1
2(t âˆ’s)s2.
Following the same steps as described above, for the case s > t we can also show
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
= 1
3t3 + 1
2(s âˆ’t)t2.
Thus,
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
= 1
3min{s3, t3} + 1
2|t âˆ’s|min{s2, t2}.
From the definition of the correlation coefficient between âˆ«s
0 Wu du and âˆ«t
0 Wğ‘£dğ‘£,
ğœŒ=
Cov
(
âˆ«
s
0
Wu du, âˆ«
t
0
Wğ‘£dğ‘£
)
âˆš
Var
(
âˆ«
s
0
Wu du
)
Var
(
âˆ«
t
0
Wğ‘£dğ‘£
)

68
2.2.2
Markov Property
=
1
3min{s3, t3} + 1
2|t âˆ’s|min{s2, t2}
1
3
âˆš
s3t3
= min{s3, t3}
âˆš
s3t3
+ 3
2
|t âˆ’s|min{s2, t2}
âˆš
s3t3
.
For s â‰¤t we have
ğœŒ=
âˆš
s3
t3 + 3
2(t âˆ’s)
âˆš
s
t3
and for s > t
ğœŒ=
âˆš
t3
s3 + 3
2(s âˆ’t)
âˆš
t
s3 .
Therefore, we can deduce
ğœŒ=
âˆš
min{s3, t3}
max{s3, t3} + 3
2|t âˆ’s|
âˆš
min{s, t}
max{s3, t3}.
â—½
2.2.2
Markov Property
1. The Markov Property of a Standard Wiener Process. Let (Î©, â„±, â„™) be a probability space
and let {Wt âˆ¶t â‰¥0} be a standard Wiener process with respect to the filtration â„±t, t â‰¥0.
Show that if f is a continuous function then there exists another continuous function g
such that
ğ”¼[f(Wt)|â„±u] = g(Wu)
for 0 â‰¤u â‰¤t.
Solution: For 0 â‰¤u â‰¤t we can write
ğ”¼[f(Wt)|â„±u
] = ğ”¼[f(Wt âˆ’Wu + Wu)|â„±u
] .
Since Wt âˆ’Wu âŸ‚âŸ‚â„±u and Wu is â„±u measurable, by setting Wu = x where x is a constant
value
ğ”¼[f(Wt âˆ’Wu + Wu)|â„±u] = ğ”¼[f(Wt âˆ’Wu + x)].
Because Wt âˆ’Wu âˆ¼ğ’©(0, t âˆ’u) we can write ğ”¼[f(Wt âˆ’Wu + x)] as
ğ”¼[f(Wt âˆ’Wu + x)] =
1
âˆš
2ğœ‹(t âˆ’u) âˆ«
âˆ
âˆ’âˆ
f(ğ‘¤+ x)eâˆ’
ğ‘¤2
2(tâˆ’u) dğ‘¤.

2.2.2
Markov Property
69
By setting ğœ= t âˆ’u and y = ğ‘¤+ x, we can rewrite ğ”¼[f(Wt âˆ’Wu + x)] = ğ”¼[f(Wt âˆ’Wu +
Wu)] as
ğ”¼[f(Wt âˆ’Wu + Wu)] =
1
âˆš
2ğœ‹ğœâˆ«
âˆ
âˆ’âˆ
f(y)eâˆ’(yâˆ’x)2
2ğœdy
= âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Wu, y)dy
where the transition density
p(ğœ, Wu, y) =
1
âˆš
2ğœ‹ğœ
eâˆ’(yâˆ’Wu)2
2ğœ
is the density of Y âˆ¼ğ’©(Wu, ğœ). Since the only information from the filtration â„±u is Wu,
therefore
ğ”¼[f(Wt)|â„±u] = g(Wu)
where
g(Wu) = âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Wu, y)dy.
â—½
2. The Markov Property of a Wiener Process. Let (Î©, â„±, â„™) be a probability space and let
{Wt âˆ¶t â‰¥0} be a standard Wiener process with respect to the filtration â„±t, t â‰¥0. By
considering the Wiener process
Ì‚Wt = a + bt + cWt,
a, b âˆˆâ„,
c > 0
show that if f is a continuous function then there exists another continuous function g such
that
ğ”¼[f( Ì‚Wt)|â„±u] = g( Ì‚Wu)
for 0 â‰¤u â‰¤t.
Solution: For 0 â‰¤u â‰¤t we can write
ğ”¼
[
f( Ì‚Wt)|||â„±u
]
= ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + Ì‚Wu)|||â„±u
]
.
Since Wt âˆ’Wu âŸ‚âŸ‚â„±u we can deduce that Ì‚Wt âˆ’Ì‚Wu âŸ‚âŸ‚â„±u. In addition, because Wu is â„±u
measurable, hence Ì‚Wu is also â„±u measurable. By setting Ì‚Wu = x where x is a constant
value,
ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + Ì‚Wu)|||â„±u
]
= ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + x)
]
.

70
2.2.2
Markov Property
Taking note that Ì‚Wt âˆ’Ì‚Wu âˆ¼ğ’©(b(t âˆ’u), c2(t âˆ’u)), we can write ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + x)
]
as
ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + x)
]
=
1
c
âˆš
2ğœ‹(t âˆ’u) âˆ«
âˆ
âˆ’âˆ
f(ğ‘¤+ x)e
âˆ’1
2
[
(ğ‘¤âˆ’b(tâˆ’u))2
c2(tâˆ’u)
]
dğ‘¤.
By setting ğœ= t âˆ’u and y = ğ‘¤+ x, we can rewrite ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + x)
]
= ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu +
Ì‚Wu)
]
as
ğ”¼
[
f( Ì‚Wt âˆ’Ì‚Wu + Ì‚Wu)
]
=
1
c
âˆš
2ğœ‹ğœâˆ«
âˆ
âˆ’âˆ
f(y)e
âˆ’1
2
[
(yâˆ’bğœâˆ’Ì‚Wu)2
c2ğœ
]
dy
= âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Ì‚Wu, y) dy
where the transition density
p(ğœ, Wu, y) =
1
âˆš
2ğœ‹ğœ
e
âˆ’1
2
[
(yâˆ’bğœâˆ’Ì‚Wu)2
c2ğœ
]
is the density of Y âˆ¼ğ’©(bğœ+ Ì‚Wu, c2ğœ). Since the only information from the filtration â„±u
is Wu, therefore
ğ”¼
[
f( Ì‚Wt)|||â„±u
]
= g( Ì‚Wu)
where
g( Ì‚Wu) = âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Ì‚Wu, y) dy.
â—½
3. The Markov Property of a Geometric Brownian Motion. Let (Î©, â„±, â„™) be a probability
space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process with respect to the filtration â„±t,
t â‰¥0. By considering the geometric Wiener process
St = S0e
(
ğœ‡âˆ’1
2 ğœ2)
t+ğœWt,
ğœ‡âˆˆâ„,
ğœ> 0
where S0 > 0 show that if f is a continuous function then there exists another continuous
function g such that
ğ”¼[f(St)|â„±u] = g(Su)
for 0 â‰¤u â‰¤t.
Solution: For 0 â‰¤u â‰¤t we can write
ğ”¼[f(St)|â„±u] = ğ”¼
[
f
( St
Su
â‹…Su
)|||||
â„±u
]

2.2.3
Martingale Property
71
where
log
( St
Su
)
âˆ¼ğ’©
((
ğœ‡âˆ’1
2ğœ2)
(t âˆ’u), ğœ2(t âˆ’u)
)
.
Since Stâˆ•Su âŸ‚âŸ‚â„±u and Su is â„±u measurable, by setting Su = x where x is a constant value
ğ”¼
[
f
( St
Su
â‹…Su
)|||||
â„±u
]
= ğ”¼
[
f
( St
Su
â‹…x
)]
.
By setting ğœˆ= ğœ‡âˆ’1
2ğœ2 so that Stâˆ•Su âˆ¼log-ğ’©(ğœˆ(t âˆ’u), ğœ2(t âˆ’u)), we can write
ğ”¼
[
f
( St
Su
â‹…x
)]
as
ğ”¼
[
f
( St
Su
â‹…x
)]
=
1
ğœğ‘¤
âˆš
2ğœ‹(t âˆ’u) âˆ«
âˆ
âˆ’âˆ
f(ğ‘¤â‹…x)
e
âˆ’1
2
[
(log ğ‘¤âˆ’ğœˆ(tâˆ’u))2
ğœ2(tâˆ’u)
]
dğ‘¤.
By setting ğœ= t âˆ’u and y = ğ‘¤â‹…x, we can rewrite ğ”¼
[
f
( St
Su
â‹…x
)]
= ğ”¼
[
f
( St
Su
â‹…Su
)]
as
ğ”¼
[
f
( St
Su
â‹…Su
)]
=
1
ğœy
âˆš
2ğœ‹ğœâˆ«
âˆ
âˆ’âˆ
f(y)e
âˆ’1
2
[
(log ( y
x)âˆ’ğœˆğœ)
2
ğœ2ğœ
]
dy
= âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Wu, y) dy
where the transition density
p(ğœ, Wu, y) =
1
ğœy
âˆš
2ğœ‹ğœ
e
âˆ’1
2
[
(log yâˆ’log Suâˆ’ğœˆğœ)2
ğœ2ğœ
]
is the density of Y âˆ¼log-ğ’©(log Su + ğœˆğœ, ğœ2ğœ). Since the only information from the filtra-
tion â„±u is Su, therefore
ğ”¼[f(St)|â„±u] = g(Su)
such that
g(Su) = âˆ«
âˆ
âˆ’âˆ
f(y)p(ğœ, Wu, y) dy.
â—½
2.2.3
Martingale Property
1. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that Wt is a martingale.
Solution: Given Wt âˆ¼ğ’©(0, t).

72
2.2.3
Martingale Property
(a) For s â‰¤t and since Wt âˆ’Ws âŸ‚âŸ‚â„±s, we have
ğ”¼(Wt|â„±s) = ğ”¼(Wt âˆ’Ws + Ws|â„±s) = ğ”¼(Wt âˆ’Ws|â„±s) + ğ”¼(Ws|â„±s) = Ws.
(b) Since Wt âˆ¼ğ’©(0, t), |Wt| follows a folded normal distribution such that |Wt| âˆ¼
ğ’©f(0, t). From Problem 1.2.2.11 (page 22), we can deduce ğ”¼(|Wt|) =
âˆš
2tâˆ•ğœ‹< âˆ.
In contrast, we can also utilise HÃ¶lderâ€™s inequality (see Problem 1.2.3.2, page 41) to
deduce that ğ”¼(|Wt|) â‰¤
âˆš
ğ”¼(W2
t ) =
âˆš
t < âˆ.
(c) Wt is clearly â„±t-adapted.
From the results of (a)â€“(c) we have shown that Wt is a martingale.
â—½
2. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that Xt = W2
t âˆ’t is a martingale.
Solution: Given Wt âˆ¼ğ’©(0, t)
(a) For s â‰¤t and since Wt âˆ’Ws âŸ‚âŸ‚â„±s, we have
ğ”¼(W2
t âˆ’t|â„±s
) = ğ”¼
[(Wt âˆ’Ws + Ws
)2||| â„±s
]
âˆ’t
= ğ”¼
[(Wt âˆ’Ws
)2||| â„±s
]
+ 2ğ”¼
[
Ws
(Wt âˆ’Ws
)||| â„±s
]
+ ğ”¼
(
W2
s ||| â„±s
)
âˆ’t
= t âˆ’s + 0 + W2
s âˆ’t
= W2
s âˆ’s.
(b) Since |Xt| = |W2
t âˆ’t| â‰¤W2
t + t we can therefore write
ğ”¼
(|||W2
t âˆ’t|||
)
â‰¤ğ”¼(W2
t + t) = ğ”¼(W2
t
) + t = 2t < âˆ.
(c) Since Xt = W2
t âˆ’t is a function of Wt, hence it is â„±t-adapted.
From the results of (a)â€“(c) we have shown that Xt = W2
t âˆ’t is a martingale.
â—½
3. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
For ğœ†âˆˆâ„show that Xt = eğœ†Wtâˆ’1
2 ğœ†2t is a martingale.
Solution: Given Wt âˆ¼ğ’©(0, t) we can write log Xt = ğœ†Wt âˆ’1
2ğœ†2t âˆ¼ğ’©
(
âˆ’1
2ğœ†2t, ğœ†2t
)
and
hence eXt âˆ¼log-ğ’©
(
âˆ’1
2ğœ†2t, ğœ†2t
)
.
(a) For s â‰¤t and since Wt âˆ’Ws âŸ‚âŸ‚â„±s, we have
ğ”¼
(
eğœ†Wtâˆ’1
2 ğœ†2t||||
â„±s
)
= eâˆ’1
2 ğœ†2tğ”¼
(
eğœ†Wt||| â„±s
)
= eâˆ’1
2 ğœ†2tğ”¼
[
eğœ†(Wtâˆ’Ws)+ğœ†Ws||| â„±s
]

2.2.3
Martingale Property
73
= eâˆ’1
2 ğœ†2tğ”¼
[
eğœ†(Wtâˆ’Ws)||| â„±s
]
ğ”¼
[
eğœ†Ws||| â„±s
]
= eâˆ’1
2 ğœ†2t â‹…e
1
2 ğœ†2(tâˆ’s) â‹…eğœ†Ws
= eğœ†Wsâˆ’1
2 ğœ†2s.
(b) By setting |Xt| = ||||
eğœ†Wtâˆ’1
2 ğœ†2t||||
= eğœ†Wtâˆ’1
2 ğœ†2t,
ğ”¼(|Xt|) = ğ”¼
(
eğœ†Wtâˆ’1
2 ğœ†2t)
= eâˆ’1
2 ğœ†2tğ”¼(eğœ†Wt) = eâˆ’1
2 ğœ†2t â‹…e
1
2 ğœ†2t = 1 < âˆ.
(c) Since Xt is a function of Wt, hence it is â„±t-adapted.
From the results of (a)â€“(c) we have shown that Xt = eğœ†Wtâˆ’1
2 ğœ†2t is a martingale.
â—½
4. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that Xt = W3
t âˆ’3tWt is a martingale.
Solution:
(a) For s â‰¤t and since Wt âˆ’Ws âŸ‚âŸ‚â„±s, we have
ğ”¼(Xt|| â„±s
) = ğ”¼
(
W3
t âˆ’3tWt||| â„±s
)
= ğ”¼
[
Wt
((Wt âˆ’Ws + Ws
)2 âˆ’3t
)||||
â„±s
]
= ğ”¼
[
Wt
(Wt âˆ’Ws
)2||| â„±s
]
+ 2Wsğ”¼
[
Wt
(Wt âˆ’Ws
)||| â„±s
]
+W2
s ğ”¼(Wt|| â„±s
) âˆ’3tğ”¼(Wt|| â„±s
)
= ğ”¼
[(Wt âˆ’Ws + Ws
) (Wt âˆ’Ws
)2||| â„±s
]
+2Wsğ”¼
[(Wt âˆ’Ws + Ws
) (Wt âˆ’Ws
)||| â„±s
]
+ W3
s âˆ’3tWs
= ğ”¼
[(Wt âˆ’Ws
)3||| â„±s
]
+ ğ”¼
[
Ws
(Wt âˆ’Ws
)2||| â„±s
]
+2Wsğ”¼
[(Wt âˆ’Ws
)2||| â„±s
]
+ 2Wsğ”¼
[
Ws
(Wt âˆ’Ws
)||| â„±s
]
+ W3
s âˆ’3tWs
= Ws(t âˆ’s) + 2Ws(t âˆ’s) + W3
s âˆ’3tWs
= W3
s âˆ’3sWs
where, from Problem 2.2.1.5 (page 58), we can deduce that the moment generating
function of Wt is MWt(ğœƒ) = e
1
2 ğœƒ2t2 and ğ”¼(W3
t ) = d3
dğœƒ3 MWt(ğœƒ)||||ğœƒ=0
= 0.

74
2.2.3
Martingale Property
(b) Since |Xt| = |W3
t âˆ’3tWt| â‰¤|W3
t | + 3t|Wt| and from HÃ¶lderâ€™s inequality (see Prob-
lem 1.2.3.2, page 41),
ğ”¼(|Xt|) â‰¤
âˆš
ğ”¼
(
||W2
t ||
2)
ğ”¼
(
||Wt||
2)
+ 3tğ”¼(|Wt|)
=
âˆš
ğ”¼
(
W4
t
)
ğ”¼
(
W2
t
)
+ 3tğ”¼
(
|Wt|
)
.
Since W2
t âˆ•t âˆ¼ğœ’(1) we have ğ”¼(W4
t ) = 3t and utilising HÃ¶lderâ€™s inequality again, we
have ğ”¼(|Wt|) â‰¤
âˆš
ğ”¼(W2
t
) =
âˆš
t < âˆ. Therefore, ğ”¼(|Xt|) â‰¤t
âˆš
2 + 3t
âˆš
t < âˆ.
(c) Since Xt is a function of Wt, hence it is â„±t-adapted.
From the results of (a)â€“(c) we have shown that Xt = W3
t âˆ’3tWt is a martingale.
â—½
5. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
For ğœ†âˆˆâ„show that the following hyperbolic processes:
Xt = eâˆ’1
2 ğœ†2t cosh(ğœ†Wt)
Yt = eâˆ’1
2 ğœ†2t sinh(ğœ†Wt)
are martingales.
Solution: By definition we can write
Xt = eâˆ’1
2 ğœ†2t cosh(ğœ†Wt) = 1
2
(
eğœ†Wtâˆ’1
2 ğœ†2t + eâˆ’ğœ†Wtâˆ’1
2 ğœ†2t)
.
Since for ğœ†âˆˆâ„, X(1)
t
= 1
2eğœ†Wtâˆ’1
2 ğœ†2t and X(2)
t
= 1
2eâˆ’ğœ†Wtâˆ’1
2 ğœ†2t are martingales we have the
following properties:
(a) For s â‰¤t, ğ”¼
(
X(1)
t
+ X(2)
t
||| â„±s
)
= X(1)
s
+ X(2)
s .
(b) Because ğ”¼
(|||X(1)
t
|||
)
< âˆand ğ”¼
(|||X(2)
t
|||
)
< âˆ, so ğ”¼
(|||X(1)
t
+ X(2)
t
|||
)
< âˆ.
(c) Since Xt is a function of Wt, hence it is â„±t-adapted.
From the results of (a)â€“(c) we have shown that Xt = eâˆ’1
2 ğœ†2t cosh(ğœ†Wt) is a martingale.
For Yt = eâˆ’1
2 ğœ†2t sinh(ğœ†Wt) we note that
Yt = eâˆ’1
2 ğœ†2t sinh(ğœ†Wt) = 1
2
(
eğœ†Wtâˆ’1
2 ğœ†2t âˆ’eâˆ’ğœ†Wtâˆ’1
2 ğœ†2t)
and similar steps can be applied to show that Yt is also a martingale.
â—½
6. Let (Î©, â„±, â„™) be a probability space. Show that the sample paths of a standard Wiener
process {Wt âˆ¶t â‰¥0} are continuous but not differentiable.

2.2.3
Martingale Property
75
Solution: The path Wt âˆ¼ğ’©(0, t) is continuous in probability if and only if, for every
ğ›¿> 0 and t â‰¥0,
lim
Î”tâ†’0â„™(|Wt+Î”t âˆ’Wt| â‰¥ğ›¿) = 0.
Given that Wt is a martingale and from Chebyshevâ€™s inequality (see Problem 1.2.2.19,
page 40),
â„™(|Wt+Î”t âˆ’Wt| â‰¥ğ›¿) = â„™(|Wt+Î”t âˆ’ğ”¼(Wt+Î”t|â„±t)| â‰¥ğ›¿)
â‰¤Var(Wt+Î”t|â„±t)
ğ›¿2
= Var(Wt+Î”t âˆ’Wt + Wt|â„±t)
ğ›¿2
= Var(Wt+Î”t âˆ’Wt)
ğ›¿2
+ Var(Wt|â„±t)
ğ›¿2
= Î”t
ğ›¿2
since Wt+Î”t âˆ’Wt âŸ‚âŸ‚â„±t and Wt is â„±t measurable and hence Var(Wt|â„±t) = 0.
Taking the limit Î”t â†’0, we have
â„™(|Wt+Î”t âˆ’Wt| â‰¥ğ›¿) â†’0
and therefore we conclude that the sample path is continuous. By setting
Î”Wt = Wt+Î”t âˆ’Wt = ğœ™
âˆš
Î”t
where ğœ™âˆ¼ğ’©(0, 1) and taking limit Î”t â†’0, we have
lim
Î”tâ†’0
Î”Wt
Î”t
= lim
Î”tâ†’0
ğœ™
âˆš
Î”t
= Â±âˆ
depending on the sign of ğœ™. Therefore, Wt is not differentiable.
â—½
7. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process
with respect to the filtration â„±t, t â‰¥0.
Show that if ğœ‘is a convex function and ğ”¼[|ğœ‘(Wt)|] < âˆfor all t â‰¥0, then ğœ‘(Wt) is a
submartingale.
Finally, deduce that |Wt|, W2
t , eğ›¼Wt, ğ›¼âˆˆâ„and W+
t = max{0, Wt} are non-negative sub-
martingales.
Solution: Let s < t and because Wt is a martingale we therefore have ğ”¼(Wt|â„±s) = Ws
and hence ğœ‘[ğ”¼(Wt|â„±s)] = ğœ‘(Ws). From the conditional Jensenâ€™s inequality (see Prob-
lem 1.2.3.14, page 48),
ğ”¼[ğœ‘(Wt)|â„±s] â‰¥ğœ‘[ğ”¼(Wt|â„±s)] = ğœ‘(Ws).
In addition, since ğ”¼[|ğœ‘(Wt)|] < âˆand ğœ‘(Wt) is clearly â„±t-adapted we can conclude that
ğœ‘(Wt) is a submartingale.

76
2.2.4
First Passage Time
By setting ğœ‘(x) = |x|, x âˆˆâ„and for ğœƒâˆˆ(0, 1) and x1, x2 âˆˆâ„such that x1 â‰ x2,
we have
|ğœƒx1 + (1 âˆ’ğœƒ)x2| â‰¤ğœƒ|x1| + (1 âˆ’ğœƒ)|x2|.
Therefore, |x| is a non-negative convex function. Because ğ”¼(|Wt|) < âˆfor all t â‰¥0, so
|Wt| is a non-negative submartingale.
On the contrary, by setting ğœ‘(x) = x2, x âˆˆâ„and since ğœ‘â€²â€²(x) = 2 â‰¥0 for all x , so x2 is a
non-negative convex function. Since ğ”¼(|W2
t |) < âˆfor all t â‰¥0, so W2
t is a non-negative
submartingale.
Using the same steps we define ğœ‘(x) = eğ›¼x where ğ›¼, x âˆˆâ„and since ğœ‘â€²â€²(x) = ğ›¼2eğ›¼x â‰¥0
for all x, so eğ›¼x is a non-negative convex function. Since ğ”¼(||eğ›¼Wt||
) < âˆfor all t â‰¥0 we
can conclude that eğ›¼Wt is a non-negative submartingale.
Finally, by setting ğœ‘(x) = max{0, x}, x âˆˆâ„and for ğœƒâˆˆ(0, 1) and x1, x2 âˆˆâ„such that
x1 â‰ x2, we have
max{0, ğœƒx1 + (1 âˆ’ğœƒ)x2} â‰¤max{0, ğœƒx1} + max{0, (1 âˆ’ğœƒ)x2}
= ğœƒ1max{0, x1} + (1 âˆ’ğœƒ1)max{0, x2}.
Therefore, x+ = max{0, x} is a non-negative convex function and since ğ”¼(|W+
t |) < âˆfor
all t â‰¥0, so W+
t = max{0, Wt} is a non-negative submartingale.
â—½
2.2.4
First Passage Time
1. Doobâ€™s Maximal Inequality. Let (Î©, â„±, â„™) be a probability space and let {Xt âˆ¶0 â‰¤t â‰¤T}
be a continuous non-negative submartingale with respect to the filtration â„±t, 0 â‰¤t â‰¤T.
Given ğœ†> 0 and ğœ= min{t âˆ¶Xt â‰¥ğœ†}, show that
ğ”¼(X0
) â‰¤ğ”¼
(
Xmin{ğœ,T}
)
â‰¤ğ”¼(XT
) .
By writing
Xmin{ğœ,T} = Xğœ1I{ğœâ‰¤T} + XT1I{ğœ>T}
show that
â„™
(
sup
0â‰¤tâ‰¤T
Xt â‰¥ğœ†
)
â‰¤
ğ”¼(XT
)
ğœ†
.
Deduce that if {Yt âˆ¶0 â‰¤t â‰¤T} is a continuous non-negative supermartingale with respect
to the filtration â„±t, 0 â‰¤t â‰¤T then
â„™
(
sup
0â‰¤tâ‰¤T
Yt â‰¥ğœ†
)
â‰¤
ğ”¼(Y0
)
ğœ†
.
Solution: For ğœ†> 0 we let ğœ= min{t âˆ¶Xt â‰¥ğœ†} so that 0 â‰¤min{ğœ, T} â‰¤T. Because Xt
is a non-negative submartingale we have
ğ”¼
(
XT|| â„±min{ğœ,T}
)
â‰¥Xmin{ğœ,T}

2.2.4
First Passage Time
77
or
ğ”¼
(
Xmin{ğœ,T}
)
â‰¤ğ”¼
[
ğ”¼
(
XT|| â„±min{ğœ,T}
)]
= ğ”¼(XT
) .
Using the same steps we can deduce
ğ”¼(X0
) â‰¤ğ”¼
(
Xmin{ğœ,T}
)
â‰¤ğ”¼(XT
) .
By definition
Xmin{ğœ,T} = Xğœ1I{ğœâ‰¤T} + XT1I{ğœ>T}
where
1I{ğœâ‰¤T} =
{
1
ğœâ‰¤T
0
ğœ> T ,
1I{ğœ>T} =
{
1
ğœ> T
0
ğœâ‰¤T.
Therefore,
ğ”¼
(
Xmin{ğœ,T}
)
= ğ”¼
(
Xğœ1I{ğœâ‰¤T}
)
+ ğ”¼
(
XT1I{ğœ>T}
)
â‰¥ğœ†â„™(ğœâ‰¤T) + ğ”¼
(
XT1I{ğœ>T}
)
.
Taking note that ğ”¼
(
Xmin{ğœ,T}
)
â‰¤ğ”¼(XT
), we can write
ğœ†â„™(ğœâ‰¤T) â‰¤ğ”¼
(
Xmin{ğœ,T}
)
âˆ’ğ”¼
(
XT1I{ğœ>T}
)
â‰¤ğ”¼(XT
) âˆ’ğ”¼
(
XT1I{ğœ>T}
)
â‰¤ğ”¼(XT
) .
Since
{ğœâ‰¤T} â‡â‡’
{
sup
0â‰¤tâ‰¤T
Xt â‰¥ğœ†
}
therefore
â„™
(
sup
0â‰¤tâ‰¤T
Xt â‰¥ğœ†
)
â‰¤ğ”¼(XT)
ğœ†
.
If {Yt}0â‰¤tâ‰¤T is a supermartingale then
ğ”¼(YT
) â‰¤ğ”¼
(
Ymin{ğœ,T}
)
â‰¤ğ”¼(Y0
)
where in this case ğœ= min{t âˆ¶Yt â‰¥ğœ†} and by analogy with the above steps, we have
â„™
(
sup
0â‰¤tâ‰¤T
Yt â‰¥ğœ†
)
â‰¤
ğ”¼(Y0
)
ğœ†
.
â—½
2. Doobâ€™s Lp Maximal Inequality. Let (Î©, â„±, â„™) be a probability space and let Z be a contin-
uous non-negative random variable where for m > 0, ğ”¼(Zm) < âˆ. Show that
ğ”¼(Zm) = m âˆ«
âˆ
0
ğ›¼mâˆ’1â„™(Z > ğ›¼) dğ›¼.

78
2.2.4
First Passage Time
Let {Xt âˆ¶0 â‰¤t â‰¤T} be a continuous non-negative submartingale with respect to the fil-
tration â„±t, 0 â‰¤t â‰¤T. Using the above result, for p > 1 and if ğ”¼(sup0â‰¤tâ‰¤TXp
t
) < âˆshow
that
ğ”¼
(
sup
0â‰¤tâ‰¤T
Xp
t
)
â‰¤
(
p
p âˆ’1
)p
ğ”¼(Xp
T
) .
Deduce that if {Yt}0â‰¤tâ‰¤T is a continuous non-negative supermartingale with respect to the
filtration â„±t, 0 â‰¤t â‰¤T then
ğ”¼
(
sup
0â‰¤tâ‰¤T
Yp
t
)
â‰¤
(
p
p âˆ’1
)p
ğ”¼(Yp
0
) .
Solution: By defining the indicator function
1I{Z>ğ›¼} =
{
1
Z > ğ›¼
0
Z â‰¤ğ›¼
we can prove
âˆ«
âˆ
0
mğ›¼mâˆ’1â„™(Z > ğ›¼) dğ›¼= âˆ«
âˆ
0
mğ›¼mâˆ’1ğ”¼(1I{Z>ğ›¼}) dğ›¼
= ğ”¼
[
âˆ«
âˆ
0
mğ›¼mâˆ’11I{Z>ğ›¼} dğ›¼
]
= ğ”¼
[
âˆ«
Z
0
mğ›¼mâˆ’1 dğ›¼
]
= ğ”¼(Zm).
Let ğœ= min{t âˆ¶Xt â‰¥ğœ†}, ğœ†> 0 so that 0 â‰¤min{ğœ, T} â‰¤T. Therefore, we can deduce
{ğœâ‰¤T} â‡â‡’
{
sup
0â‰¤tâ‰¤T
Xt â‰¥ğœ†
}
and from Problem 2.2.4.1 (page 76) we can show that
ğ”¼
(
sup
0â‰¤tâ‰¤T
Xp
t
)
= âˆ«
âˆ
0
pğœ†pâˆ’1â„™
(
sup
0â‰¤tâ‰¤T
Xt > ğœ†
)
dğœ†
â‰¤âˆ«
âˆ
0
pğœ†pâˆ’2ğ”¼
(
XT1I{sup0â‰¤tâ‰¤TXtâ‰¥ğœ†}
)
dğœ†
= p ğ”¼
(
XT âˆ«
âˆ
0
ğœ†pâˆ’21I{sup0â‰¤tâ‰¤TXtâ‰¥ğœ†} dğœ†
)
= p ğ”¼
(
XT âˆ«
sup0â‰¤tâ‰¤TXt
0
ğœ†pâˆ’2 dğœ†
)
=
p
p âˆ’1ğ”¼
(
XT â‹…
{
sup
0â‰¤tâ‰¤T
Xpâˆ’1
t
})
.

2.2.4
First Passage Time
79
Using HÃ¶lderâ€™s inequality and taking note that ğ”¼(sup0â‰¤tâ‰¤TXp
t
) < âˆ, we can write
ğ”¼
(
sup
0â‰¤tâ‰¤T
Xp
t
)
â‰¤
p
p âˆ’1ğ”¼(Xp
T
) 1
p ğ”¼
(
sup
0â‰¤tâ‰¤T
Xpâˆ’1
t
) pâˆ’1
p
or
ğ”¼
(
sup
0â‰¤tâ‰¤T
Xp
t
) 1
p
â‰¤
p
p âˆ’1ğ”¼(Xp
T
) 1
p
and hence
ğ”¼
(
sup
0â‰¤tâ‰¤T
Xp
t
)
â‰¤
(
p
p âˆ’1
)p
ğ”¼(Xp
T
) .
From Problem 2.2.4.1 (page 76), if {Yt}0â‰¤tâ‰¤T is a supermartingale then
ğ”¼(YT
) â‰¤ğ”¼
(
Ymin{ğœ,T}
)
â‰¤ğ”¼(Y0
)
where in this case ğœ= min{t âˆ¶Yt â‰¥ğœ†}. Following the same steps as discussed before, we
have
ğ”¼
(
sup
0â‰¤tâ‰¤T
Yp
t
)
â‰¤
(
p
p âˆ’1
)p
ğ”¼(Yp
0
) .
â—½
3. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Using Doobâ€™s maximal inequality show that for every T > 0 and ğœ†, ğœƒ> 0, we can express
â„™
(
sup
0â‰¤tâ‰¤T
eğœƒWt â‰¥eğœƒğœ†
)
â‰¤e
1
2 ğœƒ2Tâˆ’ğœƒğœ†
and hence prove that
â„™
(
sup
0â‰¤tâ‰¤T
Wt â‰¥ğœ†
)
â‰¤eâˆ’ğœ†2
2T .
Finally, deduce that
â„™
(
sup
0â‰¤tâ‰¤T
||Wt|| â‰¥ğœ†
)
â‰¤2eâˆ’ğœ†2
2T .
Solution: Since ğœ†> 0 and for ğœƒ> 0 we can write
â„™
(
sup
0â‰¤tâ‰¤T
Wt â‰¥ğœ†
)
= â„™
(
sup
0â‰¤tâ‰¤T
ğœƒWt â‰¥ğœƒğœ†
)
= â„™
(
sup
0â‰¤tâ‰¤T
eğœƒWt â‰¥eğœƒğœ†
)
.
From Problem 2.2.3.7 (page 75) we have shown that eğœƒWt is a submartingale and it is also
non-negative. Thus, from Doobâ€™s maximal inequality theorem,
â„™
(
sup
0â‰¤tâ‰¤T
eğœƒWt â‰¥eğœƒğœ†
)
â‰¤
ğ”¼(eğœƒWT)
eğœƒğœ†
= e
1
2 ğœƒ2Tâˆ’ğœƒğœ†.

80
2.2.4
First Passage Time
By minimising e
1
2 ğœƒ2Tâˆ’ğœƒğœ†we obtain ğœƒ= ğœ†âˆ•T and substituting it into the inequality
we have
â„™
(
sup
0â‰¤tâ‰¤T
Wt â‰¥ğœ†
)
â‰¤eâˆ’ğœ†2
2T .
Given that Wt âˆ¼ğ’©(0, t) we can deduce that â„™(|Wt| â‰¥ğœ†) = â„™(Wt â‰¥ğœ†) + â„™(Wt â‰¤âˆ’ğœ†) =
2â„™(Wt â‰¥ğœ†) and hence â„™
(
sup
0â‰¤tâ‰¤T
||Wt|| â‰¥ğœ†
)
â‰¤2eâˆ’ğœ†2
2T .
â—½
4. Let (Î©, â„±, â„™) be a probability space and let Wt be the standard Wiener process with respect
to the filtration â„±t, t â‰¥0.
By writing Xt = x0 + Wt where x0 âˆˆâ„show that {Xt âˆ¶t â‰¥0} is a continuous martingale.
Let Ta = inf{t â‰¥0 âˆ¶Xt = a} and Tb = inf{t â‰¥0 âˆ¶Xt = b} where a < b. Show, using the
optional stopping theorem, that
â„™(Ta < Tb) = b âˆ’x0
b âˆ’a .
Solution: Let Xt = x0 + Wt where Wt is a standard Wiener process. Because Wt is a mar-
tingale (see Problem 2.2.3.1, page 71) and because x0 is a constant value, following the
same steps we can easily prove that Xt is also a martingale.
By writing T = inf{t â‰¥0 âˆ¶Xt âˆ‰(a, b)} as the first exit time from the interval (a, b), then
from the optional stopping theorem
ğ”¼(XT|X0 = x0) = ğ”¼(x0 + WT|X0 = x0
) = x0
and by definition
ğ”¼(XT|X0 = x0
) = aâ„™(XT = a|X0 = x0) + bâ„™(XT = b|X0 = x0)
x0 = aâ„™(Ta < Tb
) + bâ„™(Ta â‰¥Tb
)
x0 = aâ„™(Ta < Tb
) + b [1 âˆ’â„™(Ta < Tb
)] .
Therefore,
â„™(Ta < Tb) = b âˆ’x0
b âˆ’a .
â—½
5. Let (Î©, â„±, â„™) be a probability space and let Wt be the standard Wiener process with respect
to the filtration â„±t, t â‰¥0.
By writing Xt = x + Wt where x âˆˆâ„show that Xt and (Xt âˆ’x)2 âˆ’t are continuous mar-
tingales.
Let T = inf{t â‰¥0 âˆ¶Xt âˆ‰(a, b)} be the first exit time from the interval (a, b), a < x < b
and assuming T < âˆalmost surely show, using the optional stopping theorem, that
â„™(XT = a|X0 = x) = b âˆ’x
b âˆ’a
and
â„™(XT = b|X0 = x) = x âˆ’a
b âˆ’a
with
ğ”¼(T) = (b âˆ’x)(x âˆ’a).

2.2.4
First Passage Time
81
Solution: Let Xt = x + Wt where Wt is a standard Wiener process. Because Wt and
W2
t âˆ’t are martingales (see Problems 2.2.3.1, page 71 and 2.2.3.2, page 72) and
because x0 is a constant value, following the same steps we can easily prove that Xt and
(Xt âˆ’x)2 âˆ’t are also martingales.
For x âˆˆ(a, b) and because Xt is a martingale, from the optional stopping theorem
ğ”¼(XT|X0 = x) = ğ”¼(x + WT|X0 = x) = x
and by definition
ğ”¼(XT|X0 = x) = aâ„™(XT = a|X0 = x) + bâ„™(XT = b|X0 = x)
x = aâ„™(XT = a|X0 = x) + b [1 âˆ’â„™(XT = a|X0 = x)] .
Therefore,
â„™(XT = a|X0 = x) = b âˆ’x
b âˆ’a
and
â„™(XT = b|X0 = x) = 1 âˆ’â„™(XT = a|X0 = x) = x âˆ’a
b âˆ’a.
Since Yt = (Xt âˆ’x)2 âˆ’t is a martingale, from the optional stopping theorem we have
ğ”¼(YT|X0 = x) = ğ”¼(Y0|X0 = x) = 0
or
ğ”¼
[(XT âˆ’x)2 âˆ’T||| X0 = x
]
= ğ”¼
[(XT âˆ’x)2||| X0 = x
]
âˆ’ğ”¼(T|X0 = x) = 0.
Therefore,
ğ”¼(T|X0 = x) = ğ”¼
[(XT âˆ’x)2||| X0 = x
]
= (a âˆ’x)2â„™(XT = a|X0 = x) + (b âˆ’x)2â„™(XT = b|X0 = x)
= (a âˆ’x)2 (b âˆ’x
b âˆ’a
)
+ (b âˆ’x)2 (x âˆ’a
b âˆ’a
)
= (b âˆ’x)(x âˆ’a).
â—½
6. Let (Î©, â„±, â„™) be a probability space and let {Xt âˆ¶t â‰¥0} be a continuous martingale on â„.
Show that if T = inf{t â‰¥0 âˆ¶Xt âˆ‰(a, b)} is the first exit time from the interval (a, b) and
assuming T < âˆalmost surely then for ğœƒ> 0,
Zt = (eğœƒb âˆ’eâˆ’ğœƒa)eğœƒXtâˆ’1
2 ğœƒ2t + (eâˆ’ğœƒa âˆ’eğœƒb)eâˆ’ğœƒXtâˆ’1
2 ğœƒ2t
is a martingale. Using the optional stopping theorem and if a < x < b, show that
ğ”¼
(
eâˆ’1
2 ğœƒ2T||||
X0 = x
)
=
cosh
[
ğœƒ
(
x âˆ’1
2(a + b)
)]
cosh
[
1
2ğœƒ(b âˆ’a)
]
,
ğœƒ> 0.

82
2.2.4
First Passage Time
Solution: Given that Xt is a martingale and both eğœƒb âˆ’eâˆ’ğœƒa and eâˆ’ğœƒa âˆ’eğœƒb are indepen-
dent of Xt, then using the results of Problem 2.2.3.3 (page 72) we can easily show that
{Zt âˆ¶t â‰¥0} is a martingale.
From the optional stopping theorem
ğ”¼(ZT|X0 = x) = ğ”¼(Z0|X0 = x)
and using the identity sinh(x) + sinh(y) = 2 sinh
(x + y
2
)
cosh
(x âˆ’y
2
)
we have
ğ”¼(Z0|X0 = x) = eğœƒ(bâˆ’x) âˆ’eâˆ’ğœƒ(bâˆ’x) + eâˆ’ğœƒ(a+x) âˆ’eğœƒ(aâˆ’x)
= 2 {sinh [ğœƒ(b âˆ’x)] + sinh [âˆ’ğœƒ(a âˆ’x)]}
= 4 sinh
[1
2ğœƒ(b âˆ’a)
]
cosh
[
ğœƒ
(
x âˆ’1
2(a + b)
)]
and
ğ”¼(ZT|X0 = x) = ğ”¼(ZT|XT = a, X0 = x)â„™(XT = a|X0 = x)
+ğ”¼(ZT|XT = b, X0 = x)â„™(XT = b|X0 = x)
= ğ”¼(ZT|XT = a, X0 = x)â„™(XT = a|X0 = x)
+ğ”¼(ZT|XT = b, X0 = x)(1 âˆ’â„™(XT = a|X0 = x))
= [ğ”¼(ZT|XT = a, X0 = x) âˆ’ğ”¼(ZT|XT = b, X0 = x)]â„™(XT = a|X0 = x)
+ğ”¼(ZT|XT = b, X0 = x)
= [eğœƒ(bâˆ’a) âˆ’eâˆ’ğœƒ(bâˆ’a)] ğ”¼
(
eâˆ’1
2 ğœƒ2T||||
X0 = x
)
= 2 sinh[ğœƒ(b âˆ’a)]ğ”¼
(
eâˆ’1
2 ğœƒ2T||||
X0 = x
)
since ğ”¼(ZT|XT = a, X0 = x) = ğ”¼(ZT|XT = b, X0 = x). Finally,
ğ”¼
(
eâˆ’1
2 ğœƒ2T||||
X0 = x
)
=
4 sinh
[
1
2ğœƒ(b âˆ’a)
]
cosh
[
ğœƒ
(
x âˆ’1
2(a + b)
)]
2 sinh[ğœƒ(b âˆ’a)]
=
cosh
[
ğœƒ
(
x âˆ’1
2(a + b)
)]
cosh
[
1
2ğœƒ(b âˆ’a)
]
.
â—½

2.2.4
First Passage Time
83
7. Laplace Transform of First Passage Time. Let (Î©, â„±, â„™) be a probability space and let
{Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that for ğœ†âˆˆâ„, Xt = eğœ†Wtâˆ’1
2 ğœ†2t is a martingale.
By setting Ta,b = inf{t â‰¥0 âˆ¶Wt = a + bt} as the first passage time of hitting the sloping
line a + bt, where a and b are constants, show that the Laplace transform of its distribution
is given by
ğ”¼(eâˆ’ğœƒTa,b) = eâˆ’a(b+
âˆš
b2+2ğœƒ),
ğœƒ> 0
and hence show that
ğ”¼(Ta,b) =
(a
b
)
eâˆ’2ab
and
â„™(Ta,b < âˆ) = eâˆ’2ab.
Solution: To show that Xt = eğœ†Wtâˆ’1
2 ğœ†2t is a martingale, see Problem 2.2.3.3 (page 72).
From the optional stopping theorem
ğ”¼(XTa,b) = ğ”¼(X0
) = 1
and because at t = Ta,b, WTa,b = a + bTa,b we have
ğ”¼
[
eğœ†(a+bTa,b)âˆ’1
2 ğœ†2Ta,b
]
= 1
or
ğ”¼
[
e
(
ğœ†bâˆ’1
2 ğœ†2)
Ta,b
]
= eâˆ’ğœ†a.
By setting ğœƒ= âˆ’
(
ğœ†b âˆ’1
2ğœ†2)
we have ğœ†= b Â±
âˆš
b2 + 2ğœƒ. Since ğœƒ> 0 we must have
ğ”¼(eâˆ’ğœƒTa,b) â‰¤1 and therefore ğœ†= b +
âˆš
b2 + 2ğœƒ. Thus,
ğ”¼(eâˆ’ğœƒTa,b) = eâˆ’a(b+
âˆš
b2+2ğœƒ).
To find ğ”¼(Ta,b) we differentiate ğ”¼(eâˆ’ğœƒTa,b) with respect to ğœƒ,
ğ”¼(Ta,beâˆ’ğœƒTa,b) =
a
âˆš
b2 + 2ğœƒ
eâˆ’a(b+
âˆš
b2+2ğœƒ)
and setting ğœƒ= 0,
ğ”¼(Ta,b
) =
(a
b
)
eâˆ’2ab.
Finally, to find â„™(Ta,b < âˆ) by definition
ğ”¼(eâˆ’ğœƒTa,b) = âˆ«
âˆ
0
eâˆ’ğœƒtfTa,b(t)dt

84
2.2.5
Reflection Principle
where fTa,b(t) is the probability density function of Ta,b. By setting ğœƒ= 0,
â„™(Ta,b < âˆ) = ğ”¼(eâˆ’ğœƒTa,b)|||ğœƒ=0 = eâˆ’2ab.
N.B. Take note that if b = 0 we will have ğ”¼(Ta,b
) = âˆand â„™(Ta,b < âˆ) = 1 (we say
Ta,b is finite almost surely).
â—½
2.2.5
Reflection Principle
1. Reflection Principle. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a stan-
dard Wiener process. By setting T as a stopping time and defining
ÌƒWt =
{
Wt
if t â‰¤T
2WT âˆ’Wt
if t > T
show that { ÌƒWt âˆ¶t â‰¥0} is also a standard Wiener process.
Solution: If T is finite then from the strong Markov property both the paths Yt = {Wt+T âˆ’
WT âˆ¶t â‰¥0} and âˆ’Yt = {âˆ’(Wt+T âˆ’WT) âˆ¶t â‰¥0} are standard Wiener processes and inde-
pendent of Xt = {Wt âˆ¶0 â‰¤t â‰¤T}, and hence both (Xt, Yt) and (Xt, âˆ’Yt) have the same
distribution. Given the two processes defined on [0, T] and [0, âˆ), respectively, we can
paste them together as follows:
ğœ™âˆ¶(X, Y) î‚¶â†’{Xt1I{tâ‰¤T} + (Ytâˆ’T + WT)1I{tâ‰¥T} âˆ¶t â‰¥0}.
Thus, the process arising from pasting Xt = {Wt âˆ¶0 â‰¤t â‰¤T} to Yt = {Wt+T âˆ’WT âˆ¶t â‰¥
0} has the same distribution, which is {Wt âˆ¶t â‰¥0}. In contrast, the process arising from
pasting Xt = {Wt âˆ¶0 â‰¤t â‰¤T} to âˆ’Yt = {âˆ’(Wt+T âˆ’WT) âˆ¶t â‰¥0} is { ÌƒWt âˆ¶t â‰¥0}. Thus,
{ ÌƒWt âˆ¶t â‰¥0} is also a standard Wiener process.
â—½
2. Reflection Equality. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. By defining Tm = inf{t â‰¥0 âˆ¶Wt = m}, m > 0 as the first passage time,
then for ğ‘¤â‰¤m, m > 0, show that
â„™(Tm â‰¤t, Wt â‰¤ğ‘¤) = â„™(Wt â‰¥2m âˆ’ğ‘¤).
Solution: From the reflection principle in Problem 2.2.5.1 (page 84), since WTm = m,
â„™(Tm â‰¤t, Wt â‰¤ğ‘¤) = â„™(Tm â‰¤t, 2WTm âˆ’Wt â‰¤ğ‘¤)
= â„™(Wt â‰¥2m âˆ’ğ‘¤).
â—½

2.2.5
Reflection Principle
85
3. First Passage Time Density Function. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶
t â‰¥0} be a standard Wiener process. By setting Tğ‘¤as a stopping time such that Tğ‘¤=
inf{t â‰¥0 âˆ¶Wt = ğ‘¤}, ğ‘¤â‰ 0 show, using the reflection principle, that
â„™(Tğ‘¤â‰¤t) = 1 + Î¦
(
âˆ’|ğ‘¤|
âˆš
t
)
âˆ’Î¦
(
|ğ‘¤|
âˆš
t
)
and the probability density function of Tğ‘¤is given as
fTğ‘¤(t) =
|ğ‘¤|
t
âˆš
2ğœ‹t
eâˆ’ğ‘¤2
2t .
Solution: We first consider the case when ğ‘¤> 0. By definition
â„™(Tğ‘¤â‰¤t) = â„™(Tğ‘¤â‰¤t, Wt â‰¤ğ‘¤) + â„™(Tğ‘¤â‰¤t, Wt â‰¥ğ‘¤).
For the case when Wt â‰¤ğ‘¤, by the reflection equality
â„™(Tğ‘¤â‰¤t, Wt â‰¤ğ‘¤) = â„™(Wt â‰¥ğ‘¤).
On the contrary, if Wt â‰¥ğ‘¤then we are guaranteed Tğ‘¤â‰¤t. Therefore,
â„™(Tğ‘¤â‰¤t, Wt â‰¥ğ‘¤) = â„™(Wt â‰¥ğ‘¤)
and hence
â„™(Tğ‘¤â‰¤t) = 2â„™(Wt â‰¥ğ‘¤) = âˆ«
âˆ
ğ‘¤
1
âˆš
2ğœ‹t
eâˆ’y2
2t dy.
By setting x = yâˆ•
âˆš
2t we have
â„™(Tğ‘¤â‰¤t) = 2 âˆ«
âˆ
ğ‘¤
âˆš
t
1
âˆš
2ğœ‹
eâˆ’x2
2 dx = 1 + Î¦
(
âˆ’ğ‘¤
âˆš
t
)
âˆ’Î¦
(
ğ‘¤
âˆš
t
)
.
For the case when ğ‘¤â‰¤0, using the reflection principle
â„™(Tğ‘¤â‰¤t) = â„™(Tğ‘¤â‰¤t, Wt â‰¤ğ‘¤) + â„™(Tğ‘¤â‰¤t, Wt â‰¥ğ‘¤)
= â„™(Tğ‘¤â‰¤t, âˆ’Wt â‰¥âˆ’ğ‘¤) + â„™(Tğ‘¤â‰¤t, âˆ’Wt â‰¤âˆ’ğ‘¤)
= â„™(Tğ‘¤â‰¤t, âˆ’Wt â‰¥âˆ’ğ‘¤) + â„™(Tğ‘¤â‰¤t, âˆ’Wt â‰¤âˆ’ğ‘¤)
= â„™(Tğ‘¤â‰¤t, ÌƒWt â‰¥âˆ’ğ‘¤) + â„™(Tğ‘¤â‰¤t, ÌƒWt â‰¤âˆ’ğ‘¤)
where ÌƒWt = âˆ’Wt is also a standard Wiener process. Therefore,
â„™(Tğ‘¤â‰¤t) = 2â„™( ÌƒWt â‰¥âˆ’ğ‘¤) = 1 + Î¦
(
ğ‘¤
âˆš
t
)
âˆ’Î¦
(
âˆ’ğ‘¤
âˆš
t
)

86
2.2.5
Reflection Principle
and hence
â„™(Tğ‘¤â‰¤t) = 1 + Î¦
(
âˆ’|ğ‘¤|
âˆš
t
)
âˆ’Î¦
(
|ğ‘¤|
âˆš
t
)
.
To find the density of Tğ‘¤we note that
fTğ‘¤(t) = d
dtâ„™(Tğ‘¤â‰¤t) = d
dt
[
1 + Î¦
(
âˆ’|ğ‘¤|
âˆš
t
)
âˆ’Î¦
(
|ğ‘¤|
âˆš
t
)]
=
|ğ‘¤|
t
âˆš
2ğœ‹t
eâˆ’ğ‘¤2
2t .
â—½
4. Let (Î©, â„±, â„™) be a probability space and let Mt = max
0â‰¤sâ‰¤t Ws be the maximum level reached
by a standard Wiener process {Wt âˆ¶t â‰¥0} in the time interval [0, t]. Then for a â‰¥0, x â‰¤a
and for all t â‰¥0, show using the reflection principle that
â„™(Mt â‰¤a, Wt â‰¤x) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âªâ©
Î¦
(
x
âˆš
t
)
âˆ’Î¦
(
x âˆ’2a
âˆš
t
)
a â‰¥0, x â‰¤a
Î¦
(
a
âˆš
t
)
âˆ’Î¦
(
âˆ’a
âˆš
t
)
a â‰¥0, x â‰¥a
0
a â‰¤0
and the joint probability density function of (Mt, Wt) is
fMt,Wt(a, x) =
â§
âª
â¨
âªâ©
2(2a âˆ’x)
t
âˆš
2ğœ‹t
e
âˆ’1
2
(
2aâˆ’x
âˆš
t
)2
a â‰¥0, x â‰¤a
0
otherwise.
Solution: For a â‰¥0, x â‰¤a, let Ta = inf{t â‰¥0 âˆ¶Wt = a} then
{Mt â‰¥a} â‡â‡’{Ta â‰¤t}.
Taking T = Ta in the reflection principle,
â„™(Mt â‰¥a, Wt â‰¤x) = â„™(Ta â‰¤t, Wt â‰¤x)
= â„™(Ta â‰¤t, Wt â‰¥2a âˆ’x)
= â„™(Wt â‰¥2a âˆ’x)
= 1 âˆ’Î¦
(
2a âˆ’x
âˆš
t
)
= Î¦
(
x âˆ’2a
âˆš
t
)
.

2.2.5
Reflection Principle
87
Because
â„™(Wt â‰¤x) = â„™(Mt â‰¤a, Wt â‰¤x) + â„™(Mt â‰¥a, Wt â‰¤x)
then for a â‰¥0, x â‰¤a,
â„™(Mt â‰¤a, Wt â‰¤x) = â„™(Wt â‰¤x) âˆ’â„™(Mt â‰¥a, Wt â‰¤x)
= Î¦
(
x
âˆš
t
)
âˆ’Î¦
(
x âˆ’2a
âˆš
t
)
.
For the case when a â‰¥0, x â‰¥a and because Mt â‰¥Wt, we have
â„™(Mt â‰¤a, Wt â‰¤x) = â„™(Mt â‰¤a, Wt â‰¤a) = â„™(Mt â‰¤a)
and by substituting x = a into Î¦
(
x
âˆš
t
)
âˆ’Î¦
(
x âˆ’2a
âˆš
t
)
we have
â„™(Mt â‰¤a, Wt â‰¤x) = Î¦
(
a
âˆš
t
)
âˆ’Î¦
(
âˆ’a
âˆš
t
)
.
Finally, for the case when a â‰¤0, and since Mt â‰¥W0 = 0, then â„™(Mt â‰¤a, Wt â‰¤x) = 0.
To obtain the joint probability density function of (Mt, Wt), by definition
fMt,Wt(a, x) =
ğœ•2
ğœ•ağœ•xâ„™(Mt â‰¤a, Wt â‰¤x)
and since â„™(Mt â‰¤a, Wt â‰¤x) is a function in both a and x only for the case when a â‰¥0,
x â‰¤a, then
fMt,Wt(a, x) =
ğœ•2
ğœ•ağœ•x
[
Î¦
(
x
âˆš
t
)
âˆ’Î¦
(
x âˆ’2a
âˆš
t
)]
= ğœ•
ğœ•a
â¡
â¢
â¢â£
1
âˆš
2ğœ‹t
e
âˆ’1
2
(
x
âˆš
t
)2
âˆ’
1
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’2a
âˆš
t
)2â¤
â¥
â¥â¦
= âˆ’2(x âˆ’2a)
t
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’2a
âˆš
t
)2
= 2(2a âˆ’x)
t
âˆš
2ğœ‹t
e
âˆ’1
2
(
2aâˆ’x
âˆš
t
)2
.
â—½

88
2.2.5
Reflection Principle
5. Let (Î©, â„±, â„™) be a probability space and let mt = min
0â‰¤sâ‰¤t Ws be the minimum level reached
by a standard Wiener process {Wt âˆ¶t â‰¥0} in the time interval [0, t]. Then for all t â‰¥0
show that
â„™(mt â‰¥b, Wt â‰¥x) =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
Î¦
(
âˆ’x
âˆš
t
)
âˆ’Î¦
(
2b âˆ’x
âˆš
t
)
b â‰¤0, b â‰¤x
Î¦
(
âˆ’b
âˆš
t
)
âˆ’Î¦
(
b
âˆš
t
)
b â‰¤0, b â‰¥x
0
b â‰¥0
and the joint probability density function of (mt, Wt) is
fmt,Wt(b, x) =
â§
âª
â¨
âªâ©
âˆ’2(2b âˆ’x)
t
âˆš
2ğœ‹t
e
âˆ’1
2
(
2bâˆ’x
âˆš
t
)2
b â‰¤0, b â‰¤x
0
otherwise.
Solution: Given mt = âˆ’max
0â‰¤sâ‰¤t{âˆ’Wt} we have for b â‰¤0, b â‰¤x,
â„™(mt â‰¥b, Wt â‰¥x) = â„™
(
âˆ’max
0â‰¤sâ‰¤t
{âˆ’Wt
} â‰¥b, Wt â‰¥x
)
= â„™
(
âˆ’max
0â‰¤sâ‰¤t
{âˆ’Wt
} â‰¥b, âˆ’Wt â‰¤âˆ’x
)
= â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¤âˆ’b, ÌƒWt â‰¤âˆ’x
)
,
where the last equality comes from the symmetric property of the standard Wiener process
such that ÌƒWt = âˆ’Wt âˆ¼ğ’©(0, t) is also a standard Wiener process.
Since
â„™( ÌƒWt â‰¤âˆ’x) = â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¤âˆ’b, ÌƒWt â‰¤âˆ’x
)
+ â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¥âˆ’b, ÌƒWt â‰¤âˆ’x
)
and from Problem 2.2.5.4 (page 86), we can write
â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¥âˆ’b, ÌƒWt â‰¤âˆ’x
)
= Î¦
(
2b âˆ’x
âˆš
t
)
so
â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¤âˆ’b, ÌƒWt â‰¤âˆ’x
)
= â„™( ÌƒWt â‰¤âˆ’x) âˆ’â„™
(
max
0â‰¤sâ‰¤t
ÌƒWt â‰¥âˆ’b, ÌƒWt â‰¤âˆ’x
)
= Î¦
(
âˆ’x
âˆš
t
)
âˆ’Î¦
(
2b âˆ’x
âˆš
t
)
.

2.2.6
Quadratic Variation
89
Consequently, if b â‰¤0, b â‰¤x we have
â„™(mt â‰¥b, Wt â‰¥x) = Î¦
(
âˆ’x
âˆš
t
)
âˆ’Î¦
(
2b âˆ’x
âˆš
t
)
.
For the case when b â‰¤0, b â‰¥x and since mt â‰¤Wt,
â„™(mt â‰¥b, Wt â‰¥x) = â„™(mt â‰¥b, Wt â‰¥b) = â„™(mt â‰¥b).
Setting x = b in Î¦
(
âˆ’x
âˆš
t
)
âˆ’Î¦
(
2b âˆ’x
âˆš
t
)
would therefore yield
â„™(mt â‰¥b, Wt â‰¥x) = Î¦
(
âˆ’b
âˆš
t
)
âˆ’Î¦
(
b
âˆš
t
)
.
Finally, for b â‰¥0, since mt â‰¤W0 = 0 so â„™(mt â‰¥b, Wt â‰¥x) = 0.
To obtain the joint probability density function by definition,
fmt,Wt(b, x) =
ğœ•2
ğœ•bğœ•xâ„™(mt â‰¤b, Wt â‰¤x)
=
ğœ•2
ğœ•bğœ•xâ„™(mt â‰¥b, Wt â‰¥x)
and since â„™(mt â‰¥b, Wt â‰¥x) is a function in both b and x only for the case when b â‰¤0,
b â‰¤x, so
fmt,Wt(b, x) =
ğœ•2
ğœ•bğœ•x
[
Î¦
(
âˆ’x
âˆš
t
)
âˆ’Î¦
(
2b âˆ’x
âˆš
t
)]
= ğœ•
ğœ•b
â¡
â¢
â¢â£
âˆ’
1
âˆš
2ğœ‹t
e
âˆ’1
2
(
x
âˆš
t
)2
+
1
âˆš
2ğœ‹t
e
âˆ’1
2
(
2bâˆ’x
âˆš
t
)2â¤
â¥
â¥â¦
= âˆ’2(2b âˆ’x)
t
âˆš
2ğœ‹t
e
âˆ’1
2
(
2bâˆ’x
âˆš
t
)2
.
â—½
2.2.6
Quadratic Variation
1. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that it has finite quadratic variation such that
âŸ¨W, WâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)2 = t
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Finally, deduce that dWt â‹…dWt = dt.

90
2.2.6
Quadratic Variation
Solution: Since the quadratic variation is a sum of random variables, we need to show
that its expected value is t and its variance converges to zero as n â†’âˆ.
Let Î”Wti = Wti+1 âˆ’Wti âˆ¼ğ’©(0, tâˆ•n) where ğ”¼(Î”W2
ti
) = tâˆ•n then, by taking expectations
we have
ğ”¼
(
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Î”W2
ti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
(
Î”W2
ti
)
= t.
Because Î”W2
tiâˆ•(tâˆ•n) âˆ¼ğœ’2(1) we have ğ”¼(Î”W4
ti) = 3(tâˆ•n)2. Therefore, by independence of
increments
Var
(
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Î”W2
ti
)
= ğ”¼
â¡
â¢
â¢â£
(
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Î”W2
ti âˆ’t
)2â¤
â¥
â¥â¦
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[(
Î”W2
ti âˆ’t
n
)2]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
3t2
n2 âˆ’2t2
n2 + t2
n2
)
= lim
nâ†’âˆ
2t2
n
= 0.
Since lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)2 = âˆ«t
0 dWs â‹…dWs = t and âˆ«t
0 ds = t, then by differentiating
both sides with respect to t we have dWt â‹…dWt = dt.
â—½
2. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that the following cross-variation between Wt and t, and the quadratic variation of
t, are
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (ti+1 âˆ’ti
) = 0
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti
)2 = 0
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Finally, deduce that dWt â‹…dt = 0 and dt â‹…dt = 0.
Solution: Since Wti+1 âˆ’Wti âˆ¼ğ’©(0, tâˆ•n) then taking expectation and variance we have
ğ”¼
[
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (ti+1 âˆ’ti
)
]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti
) ğ”¼
(
Wti+1 âˆ’Wti
)
= 0

2.2.6
Quadratic Variation
91
and
Var
[
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (ti+1 âˆ’ti
)
]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti
)2Var
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
( t
n
)2 t
n
= lim
nâ†’âˆ
t3
n
= 0.
Thus, lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
ti+1 âˆ’ti
)
= 0.
In addition,
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti)2 = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
( t
n
)2
= lim
nâ†’âˆ
t2
n = 0.
Finally, because
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (ti+1 âˆ’ti
) = âˆ«
t
0
dWs â‹…ds = 0
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti
)2 = âˆ«
t
0
ds â‹…ds = 0
then, by differentiating both sides with respect to t, we can deduce that dWt â‹…dt = 0 and
dt â‹…dt = 0.
â—½
3. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that it has unbounded first variation such that
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
|||Wti+1 âˆ’Wti
||| = âˆ
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Solution: Since
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
â‰¤
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| â‹…
nâˆ’1
âˆ‘
i=0
|||Wti+1 âˆ’Wti
|||, then
nâˆ’1
âˆ‘
i=0
|||Wti+1 âˆ’Wti
||| â‰¥
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
|||
.

92
2.2.6
Quadratic Variation
As Wt is continuous, lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| = 0 and lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti
)2 = t < âˆ,
we can conclude that lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
|||Wti+1 âˆ’Wti
||| = âˆ.
â—½
4. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Show that for p â‰¥3,
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)p
= 0
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Finally, deduce that (dWt)p = 0, p â‰¥3.
Solution: We prove this result via mathematical induction.
For p = 3, we can express
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)3||||||
â‰¤
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| â‹…
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2||||||
=
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| â‹…
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
.
Because Wt is continuous, we have
lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| = 0.
In addition, because lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
= t < âˆ(see Problem 2.2.6.1, page 89), so
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)3||||||
â‰¤0.
or
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)3
= 0.
Thus, the result is true for p = 3.
Assume the result is true for p = m, m > 3 so that lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti
)m = 0.
Then for p = m + 1,
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)m+1||||||
â‰¤
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| â‹…
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)m||||||

2.2.6
Quadratic Variation
93
and
because
Wt
is
continuous
such
that
lim
nâ†’âˆ
max
0â‰¤kâ‰¤n âˆ’1
(Wtk+1 âˆ’Wtk
) = 0
and
lim
nâ†’âˆ
n âˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti
)m = 0, so
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)m+1||||||
â‰¤0
or
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)m+1
= 0.
Thus, the result is also true for p = m + 1. From mathematical induction we have shown
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti
)p = 0, p â‰¥3.
Since for p â‰¥3, lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti
)p = âˆ«
t
0
(dWs
)p = 0, by differentiating both sides
with respect to t we have (dWt
)p = 0, p â‰¥3.
â—½


3
Stochastic Differential Equations
A stochastic differential equation (SDE) is a differential equation in which one or more of
the terms has a random component. Within the context of mathematical finance, SDEs are
frequently used to model diverse phenomena such as stock prices, interest rates or volatilities
to name but a few. Typically, SDEs have continuous paths with both random and non-random
components and to drive the random component of the model they usually incorporate a Wiener
process. To enrich the model further, other types of random fluctuations are also employed in
conjunction with the Wiener process, such as the Poisson process when modelling discontin-
uous jumps. In this chapter we will concentrate solely on SDEs having only a Wiener process,
whilst in Chapter 5 we will discuss SDEs incorporating both Poisson and Wiener processes.
3.1
INTRODUCTION
To begin with, a one-dimensional stochastic differential equation can be described as
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
where Wt is a standard Wiener process, ğœ‡(Xt, t) is defined as the drift and ğœ(Xt, t) the volatility.
Many financial models can be written in this form, such as the lognormal asset random walk,
common spot interest rate and stochastic volatility models.
From an initial state X0 = x0, we can write in integrated form
Xt = X0 + âˆ«
t
0
ğœ‡(Xs, s) ds + âˆ«
t
0
ğœ(Xs, s) dWs
with âˆ«
t
0
[|ğœ‡(Xs, s)| + ğœ(Xs, s)2] ds < âˆand the solution of the integral equation is called an
ItÂ¯o process or an ItÂ¯o diffusion.
In Chapter 2 we described the properties of a Wiener process and, given that it is
non-differentiable, there is a crucial difference between stochastic calculus and integral
calculus. We consider an integral with respect to a Wiener process and write
It = âˆ«
t
0
f(Ws, s) dWs
where the integrand f(Wt, t) is â„±t measurable (i.e., f(Wt, t) is a stochastic process adapted to
the filtration of a Wiener process) and is square-integrable
ğ”¼
(
âˆ«
t
0
f(Ws, s)2 ds
)
< âˆ
for all t â‰¥0.

96
3.1
INTRODUCTION
Let t > 0 be a positive constant and assume f(Wti, ti) is constant over the interval [ti, ti+1),
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t for n âˆˆâ„•. Here we call such a process
f an elementary process or a simple process, and for such a process the ItÂ¯o integral It can be
defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)(Wti+1 âˆ’Wti).
We now give a formal result of the ItÂ¯o integral as follows.
Theorem 3.1 Let {Wt âˆ¶t â‰¥0} be a standard Wiener process on the probability space
(Î©, â„±, â„™), and let â„±t, t â‰¥0 be the associated filtration. The ItÂ¯o integral defined by
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)(Wti+1 âˆ’Wti)
where f is a simple process and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•has
the following properties:
â€¢ The paths of It are continuous.
â€¢ For each t, It is â„±t measurable.
â€¢ If It = âˆ«t
0 f(Ws, s) dWs and Jt = âˆ«t
0 g(Ws, s) dWs where g is a simple process, then
It Â± Jt = âˆ«
t
0
[ f(Ws, s) Â± g(Ws, s)] dWs
and for a constant c
cIt = âˆ«
t
0
cf(Ws, s) dWs.
â€¢ It is a martingale.
â€¢ ğ”¼[I2
t ] = ğ”¼
(
âˆ«
t
0
f(Ws, s)2 ds
)
.
â€¢ The quadratic variation âŸ¨I, IâŸ©t = âˆ«
t
0
f(Ws, s)2 ds.
In mathematics, ItÂ¯oâ€™s formula (or lemma) is used in stochastic calculus to find the differential
of a function of a particular type of stochastic process. In essence, it is the stochastic calculus
counterpart of the chain rule in ordinary calculus via a Taylor series expansion. The formula is
widely employed in mathematical finance and its best-known application is in the derivation
of the Blackâ€“Scholes equation used to value options. The following is a formal result of ItÂ¯oâ€™s
formula.
Theorem 3.2 (One-Dimensional ItÂ¯oâ€™s Formula) Let {Wt âˆ¶t â‰¥0} be a standard Wiener
process on the probability space (Î©, â„±, â„™) and let â„±t, t â‰¥0 be the associated filtration. Con-
sider a stochastic process Xt satisfying the following SDE
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt

3.1
INTRODUCTION
97
or in integrated form,
Xt = X0 + âˆ«
t
0
ğœ‡(Xs, s) ds + âˆ«
t
0
ğœ(Xs, s) dWs
with âˆ«
t
0
[|ğœ‡(Xs, s)| + ğœ(Xs, s)2] ds < âˆ. Then for any twice differentiable function g(Xt, t),
the stochastic process Yt = g(Xt, t) satisfies
dYt = ğœ•g
ğœ•t (Xt, t) dt + ğœ•g
ğœ•Xt
(Xt, t) dXt + 1
2
ğœ•2g
ğœ•X2
t
(Xt, t)(dXt)2
=
[
ğœ•g
ğœ•t (Xt, t) + ğœ‡(Xt, t) ğœ•g
ğœ•Xt
(Xt, t) + 1
2ğœ(Xt, t)2 ğœ•2g
ğœ•X2
t
(Xt, t)
]
dt + ğœ(Xt, t) ğœ•g
ğœ•Xt
(Xt, t) dWt
where (dXt)2 is computed according to the rule
(dWt)2 = dt,
(dt)2 = dWt dt = dt dWt = 0.
In integrated form,
Yt = Y0 + âˆ«
t
0
ğœ•g
ğœ•t (Xs, s) ds + âˆ«
t
0
ğœ•g
ğœ•Xt
(Xs, s) dXs + 1
2 âˆ«
t
0
ğœ•2g
ğœ•X2
t
(Xs, s) dâŸ¨X, XâŸ©s
= Y0 + âˆ«
t
0
[
ğœ•g
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•g
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2g
ğœ•X2
t
(Xs, s)
]
ds
+ âˆ«
t
0
ğœ(Xs, s) ğœ•g
ğœ•Xt
(Xs, s) dWs
where âŸ¨X, XâŸ©t = âˆ«t
0 ğœ(Xs, s)2 ds.
The theory of SDEs is a framework for expressing the dynamical models that include both
the random and non-random components. Many solutions to SDEs are Markov processes,
where the future depends on the past only through the present. For this reason, the solutions
can be studied using backward and forward Kolmogorov equations, which turn out to be lin-
ear parabolic partial differential equations of diffusion type. But before we discuss them, the
following Feynmanâ€“Kac theorem describes an important link between stochastic differential
equations and partial differential equations.
Theorem 3.3 (Feynmanâ€“Kac Formula for One-Dimensional Diffusion Process) Let
{Wt âˆ¶t â‰¥0} be a standard Wiener process on the probability space (Î©, â„±, â„™) and let â„±t,
t â‰¥0 be the associated filtration. Let Xt be the solution of the following SDE:
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt

98
3.1
INTRODUCTION
and define r as a function of t. For t âˆˆ[0, T] where T > 0 and if V(Xt, t) satisfies the partial
differential equation (PDE)
ğœ•V
ğœ•t (Xt, t) + 1
2ğœ(Xt, t)2 ğœ•2V
ğœ•X2
t
(Xt, t) + ğœ‡(Xt, t) ğœ•V
ğœ•Xt
(Xt, t) âˆ’r(t)V(Xt, t) = 0
subject to the boundary condition V(XT, T) = Î¨(XT), then under the filtration â„±t the solution
of the PDE is given by
V(Xt, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(u)du Î¨(XT)||||
â„±t
]
.
The Feynmanâ€“Kac formula can be used to study the transition probability density function
of the one-dimensional stochastic differential equation
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt.
For t âˆˆ[0, T], T > 0 and conditioning Xt = x we can write
V(x, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(u)duÎ¨(XT)|||â„±t
]
= eâˆ’âˆ«T
t
r(u)du
âˆ«
âˆ
âˆ’âˆ
Î¨(y)p(x, t; y, T) dy
where the random variable XT has transition probability density p(x, t; y, T) in the y variable.
The transition probability density function p(x, t; y, T) satisfies two parabolic partial dif-
ferential equations, the forward Kolmogorov equation (or Fokkerâ€“Planck equation) and the
backward Kolmogorov equation. In the forward Kolmogorov function, it involves derivatives
with respect to the future state y and time T, whilst in the backward Kolmogorov function, it
involves derivatives with respect to the current state x and time t.
Theorem 3.4 (Forward Kolmogorov Equation for One-Dimensional Diffusion Pro-
cess) Let {Wt âˆ¶t â‰¥0} be the standard Wiener process on the probability space (Î©, â„±, â„™).
Consider the stochastic differential equation
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
and for t âˆˆ[0, T], T > 0 and by conditioning Xt = x, the random variable XT having a tran-
sition probability density p(x, t; y, T) in the y variable satisfies
ğœ•
ğœ•T p(x, t; y, T) = 1
2
ğœ•2
ğœ•y2 (ğœ(y, T)2p(x, t; y, T)) âˆ’ğœ•
ğœ•y(ğœ‡(y, T)p(x, t; y, T)).
Theorem 3.5 (Backward Kolmogorov Equation for One-Dimensional Diffusion Pro-
cess) Let {Wt âˆ¶t â‰¥0} be the standard Wiener process on the probability space (Î©, â„±, â„™).
Consider the stochastic differential equation
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt

3.1
INTRODUCTION
99
and for t âˆˆ[0, T], T > 0 and by conditioning Xt = x, the random variable XT having a tran-
sition probability density p(x, t; y, T) in the x variable satisfies
ğœ•
ğœ•tp(x, t; y, T) + 1
2ğœ(x, t)2 ğœ•2
ğœ•x2 p(x, t; y, T) + ğœ‡(x, t) ğœ•
ğœ•xp(x, t; y, T) = 0.
In contrast, a multi-dimensional diffusion process can be described by a set of stochastic
differential equations
dX(i)
t
= ğœ‡(i)(X(i)
t , t) dt +
n
âˆ‘
j=1
ğœ(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where Wt =
[
W(1)
t , W(2)
t , . . . , W(n)
t
]T
is the n-dimensional standard Wiener process such that
dW(i)
t dW(j)
t
= ğœŒijdt, ğœŒij âˆˆ(âˆ’1, 1) for i â‰ j, i, j = 1, 2, . . . , n, ğ(Xt, t) =
[
ğœ‡(1)(X(1)
t , t), ğœ‡(2)(X(2)
t ,
t), . . . , ğœ‡(m)(X(m)
t
, t)
]T
is the m-dimensional drift vector and
ğˆ(Xt, t) =
â¡
â¢
â¢
â¢
â¢â£
ğœ(1,1)(X(1)
t , t) ğœ(1,2)(X(1)
t , t) . . . ğœ(1,n)(X(1)
t , t)
ğœ(2,1)(X(2)
t , t) ğœ(2,2)(X(2)
t , t) . . . ğœ(2,n)(X(2)
t , t)
â‹®
â‹®
â‹±
â‹®
ğœ(m,1)(X(m)
t
, t) ğœ(m,2)(X(m)
t
, t) . . . ğœ(m,n)(X(m)
t
, t)
â¤
â¥
â¥
â¥
â¥â¦
is the m Ã— n volatility matrix. Given the initial state X(i)
0 , we can write in integrated form
X(i)
t
= X(i)
0 + âˆ«
t
0
ğœ‡(i)(X(i)
s , s) ds +
n
âˆ‘
j=1 âˆ«
t
0
ğœ(i,j)(X(i)
s , s) dW(j)
s
with âˆ«
t
0
[
|ğœ‡(i)(X(i)
s , s)| + |ğœ(i,p)(X(i)
s , s)ğœ(i,q)(X(i)
s , s)|
]
ds < âˆfor i = 1, 2, . . . , m and p, q =
1, 2, . . . , n.
Definition 3.6 (Multi-Dimensional ItÂ¯oâ€™s Formula) Let {W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n be a
sequence of standard Wiener processes on the probability space (Î©, â„±, â„™), and let â„±t, t â‰¥0
be the associated filtration. Consider a stochastic process X(i)
t
satisfying the following SDE
dX(i)
t
= ğœ‡(i)(X(i)
t , t) dt +
n
âˆ‘
j=1
ğœ(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
or in integrated form
X(i)
t
= X(i)
0 + âˆ«
t
0
ğœ‡(i)(X(i)
s , s) ds +
n
âˆ‘
j=1 âˆ«
t
0
ğœ(i,j)(X(i)
s , s) dW(j)
s
with âˆ«t
0 |ğœ‡(i)(X(i)
s , s)| + |ğœ(i,p)(X(i)
s , s)ğœ(i,q)(X(i)
s , s)| ds < âˆfor i = 1, 2, . . . , m and p, q =
1, 2, . . . , n. Then, for any twice differentiable function h(Xt, t), where Xt =
[
X(1)
t , X(2)
t , . . . ,

100
3.1
INTRODUCTION
X(m)
t
]T
, the stochastic process Zt = h(X(1)
t , X(2)
t , . . . , X(m)
t
, t) satisfies
dZt = ğœ•h
ğœ•t (Xt, t) dt +
m
âˆ‘
i=1
ğœ•h
ğœ•X(i)
t
(Xt, t) dX(i)
t + 1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
ğœ•2h
ğœ•X(i)
t ğœ•X(j)
t
(Xt, t) dX(i)
t dX(j)
t
=
[
ğœ•h
ğœ•t (Xt, t) +
m
âˆ‘
i=1
ğœ‡(i)(X(i)
t , t) ğœ•h
ğœ•X(i)
t
(Xt, t)
+1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
ğœŒij
( n
âˆ‘
k=1
n
âˆ‘
l=1
ğœ(i,k)(X(i)
t , t)ğœ(j,l)(X(j)
t , t)
)
ğœ•2h
ğœ•X(i)
t ğœ•X(j)
t
(Xt, t)
]
dt
+
m
âˆ‘
i=1
( n
âˆ‘
j=1
ğœ(i,j)(X(i)
t , t)
)
ğœ•h
ğœ•X(i)
t
(Xt, t) dW(i)
t
where dX(i)
t dX(j)
t
is computed according to the rule
dW(i)
t dW(j)
t
= ğœŒijdt,
(dt)2 = dW(i)
t dt = dt dW(i)
t
= 0
such that ğœŒij âˆˆ(âˆ’1, 1) and ğœŒii = 1. In integrated form
Zt = Z0 + âˆ«
t
0
ğœ•h
ğœ•t (Xs, s) ds + âˆ«
t
0
m
âˆ‘
i=1
ğœ•h
ğœ•X(i)
t
(Xs, s) dX(i)
s
+ âˆ«
t
0
1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
ğœ•2h
ğœ•X(i)
t ğœ•X(j)
t
(Xs, s) dâŸ¨X(i), X(j)âŸ©s
= Z0 + âˆ«
t
0
[
ğœ•h
ğœ•t (Xs, s) +
m
âˆ‘
i=1
ğœ‡(i)(X(i)
s , s) ğœ•h
ğœ•X(i)
t
(Xs, s)
+1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
ğœŒij
( n
âˆ‘
k=1
n
âˆ‘
l=1
ğœ(i,k)(X(i)
s , s)ğœ(j,l)(X(j)
s , s)
)
ğœ•2h
ğœ•X(i)
t ğœ•X(j)
t
(Xs, s)
]
ds
+ âˆ«
t
0
m
âˆ‘
i=1
( n
âˆ‘
j=1
ğœ(i,j)(X(i)
s , s)
)
ğœ•h
ğœ•X(i)
t
(Xs, s) dW(i)
s ,
where âŸ¨X(i), X(j)âŸ©t = âˆ«
t
0
ğœŒij
n
âˆ‘
k=1
n
âˆ‘
l=1
ğœ(i,k)(X(i)
s , s)ğœ(j,l)(X(j)
s , s) ds.
The Feynmanâ€“Kac theorem for a one-dimensional diffusion process also extends to a
multi-dimensional version.
Theorem 3.7 (Feynmanâ€“Kac Formula for Multi-Dimensional Diffusion Process) Let
{W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n be a sequence of standard Wiener processes on the probability

3.1
INTRODUCTION
101
space (Î©, â„±, â„™), and let â„±t, t â‰¥0 be the associated filtration. Let X(i)
t
be the solution of the
following SDE
dX(i)
t
= ğœ‡(i)(X(i)
t , t) dt +
n
âˆ‘
j=1
ğœ(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= ğœŒijdt, ğœŒij âˆˆ(âˆ’1, 1) for i â‰ j, ğœŒii = 1, i, j = 1, 2, . . . , n and define r to be
a function of t. By denoting Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, for t âˆˆ[0, T] where T > 0 and if
V(Xt, t) satisfies the PDE
ğœ•V
ğœ•t (Xt, t) + 1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
( n
âˆ‘
k=1
n
âˆ‘
l=1
ğœ(i,k)(X(i)
t , t)ğœ(j,l)(X(j)
t , t)
)
ğœ•2V
ğœ•X(i)
t ğœ•X(j)
t
(Xt, t)
+
m
âˆ‘
i=1
ğœ‡(i)(X(i)
t , t) ğœ•V
ğœ•X(i)
t
(Xt, t) âˆ’r(t)V(Xt, t) = 0
subject to the boundary condition V(XT, T) = Î¨(XT), then under the filtration â„±t, the solution
of the PDE is given by
V(Xt, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(u)du Î¨(XT)
||||
â„±t
]
.
Similarly, we have the Kolmogorov forward and backward equations for multi-dimensional
diffusion processes as well.
Theorem 3.8 (Forward Kolmogorov Equation for Multi-Dimensional Diffusion Pro-
cess) Let {W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n be a sequence of standard Wiener processes on the
probability space (Î©, â„±, â„™). Consider the stochastic differential equations
dX(i)
t
= ğœ‡(i)(X(i)
t , t) dt +
n
âˆ‘
j=1
ğœ(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= ğœŒijdt, ğœŒij âˆˆ(âˆ’1, 1) for i â‰ j, ğœŒii = 1, i, j = 1, 2, . . . , n. By denoting
Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, and for t âˆˆ[0, T], T > 0 and by conditioning Xt = x where
x =
[
x(1), x(2), . . . , x(m)]T
, the m-dimensional random variable XT having a transition
probability density p(x, t; y, T) in the m-dimensional variable y =
[
y(1), y(2), . . . , y(m)]T
satisfies
ğœ•
ğœ•T p(x, t; y, T) = 1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
ğœ•2
ğœ•y(i)ğœ•y(j)
[ n
âˆ‘
k=1
n
âˆ‘
l=1
(ğœŒklğœ(i,k)(y(i), T)ğœ(j,l)(y(j), T)) p(x, t; y, T)
]
âˆ’
m
âˆ‘
i=1
ğœ•
ğœ•y(i) (ğœ‡(i)(y(i), T)p(x, t; y, T)).

102
3.2.1
ItÂ¯o Calculus
Theorem 3.9 (Backward Kolmogorov Equation for Multi-Dimensional Diffusion Pro-
cess) Let {W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n be a sequence of independent standard Wiener pro-
cesses on the probability space (Î©, â„±, â„™). Consider the stochastic differential equations
dX(i)
t
= ğœ‡(i)(X(i)
t , t) dt +
n
âˆ‘
j=1
ğœ(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= ğœŒijdt, ğœŒij âˆˆ(âˆ’1, 1) for i â‰ j, ğœŒii = 1, i, j = 1, 2, . . . , n. By denoting
Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, and for t âˆˆ[0, T], T > 0 and by conditioning Xt = x where
x =
[
x(1), x(2), . . . , x(m)]T
, the m-dimensional random variable XT having a transition
probability density p(x, t; y, T) in the m-dimensional variable y =
[
y(1), y(2), . . . , y(m)]T
satisfies
ğœ•
ğœ•tp(x, t; y, T) + 1
2
m
âˆ‘
i=1
m
âˆ‘
j=1
( n
âˆ‘
k=1
n
âˆ‘
l=1
ğœŒklğœ(i,k)(x(i), t)ğœ(j,l)(x(j), t)
)
ğœ•2
ğœ•x(i)ğœ•x(j) p(x, t; y, T)
+
m
âˆ‘
i=1
ğœ‡(i)(x(i), t) ğœ•
ğœ•x(i) p(x, t; y, T) = 0.
3.2
PROBLEMS AND SOLUTIONS
3.2.1
ItÂ¯o Calculus
1. ItÂ¯o Integral. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener
process. Let the ItÂ¯o integral of Wt dWt be defined as the following limit
I(t) = âˆ«
t
0
Ws dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(
Wti+1 âˆ’Wti
)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t for n âˆˆâ„•.
Show that the quadratic variation of Wt is
âŸ¨W, WâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
= t
and hence
I(t) = 1
2
(W2
t âˆ’t) .
Finally, show that the ItÂ¯o integral is a martingale.
Solution: For the first part of the solution, see Problem 2.2.6.1 (page 89).
Given lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
= t and by expanding,
I(t) = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(
Wti+1 âˆ’Wti
)

3.2.1
ItÂ¯o Calculus
103
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
{
1
2
(
W2
ti+1 âˆ’W2
ti
)
âˆ’1
2
(
Wti+1 âˆ’Wti
)2}
= 1
2
[
lim
nâ†’âˆ
(
W2
tn âˆ’W2
0
)
âˆ’t
]
= 1
2
(W2
t âˆ’t) .
To show that I(t) is a martingale, see Problem 2.2.3.2 (page 72).
N.B. Without going through first principles, we can also show that âˆ«t
0 Ws dWs =
1
2
(W2
t âˆ’t) by using ItÂ¯oâ€™s formula on Xt = 1
2W2
t , where
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•Wt
dWt + 1
2
ğœ•2Xt
ğœ•W2
t
dW2
t + . . .
= WtdWt + 1
2dt.
Taking integrals,
âˆ«
t
0
dXs = âˆ«
t
0
Ws dWs + 1
2 âˆ«
t
0
ds
Xt âˆ’X0 = âˆ«
t
0
Ws dWs + 1
2t
and since W0 = 0, so âˆ«
t
0
Ws dWs = 1
2
(W2
t âˆ’t).
â—½
2. Stratonovich Integral. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a stan-
dard Wiener process. Let the Stratonovich integral of Wt âˆ˜dWt be defined by the following
limit
S(t) = âˆ«
t
0
Ws âˆ˜dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
(
Wti+1 + Wti
) (
Wti+1 âˆ’Wti
)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Show that S(t) = 1
2W2
t and show also that the Stratonovich integral is not a martingale.
Solution: Expanding,
S(t) = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
(
Wti+1 + Wti
) (
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
(
W2
ti+1 âˆ’W2
ti
)
= lim
nâ†’âˆ
1
2
(
W2
tn âˆ’W2
t0
)
= 1
2W2
t .

104
3.2.1
ItÂ¯o Calculus
Let S(u) = âˆ«
u
0
Ws âˆ˜dWs, u < t and under the filtration â„±u and because Wt âˆ’Wu âŸ‚âŸ‚â„±u,
we have
ğ”¼(S(t)|â„±u) = 1
2ğ”¼
(
W2
t ||| â„±u
)
= 1
2ğ”¼
[(Wt âˆ’Wu + Wu
)2||| â„±u
]
= 1
2ğ”¼
[(Wt âˆ’Wu
)2||| â„±u
]
+ ğ”¼
[
Wu
(Wt âˆ’Wu
)||| â„±u
]
+ 1
2ğ”¼
(
W2
u||| â„±u
)
= 1
2(t âˆ’u) + 1
2W2
u
â‰ 1
2W2
u.
Therefore, S(t) is not a martingale.
â—½
3. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Let the integral of Wt âˆ—dWt be defined by the following limit
J(t) = âˆ«
t
0
Ws âˆ—dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(
Wti+1 âˆ’Wti
)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Show that the quadratic variation of Wt is
âŸ¨W, WâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
= t
and hence
J(t) = 1
2
(W2
t + t) .
Finally, show also that the integral is not a martingale.
Solution: To show that lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
= t, see Problem 2.2.6.1 (page 89).
By expanding,
J(t) = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
{
1
2
(
W2
ti+1 âˆ’W2
ti
)
+ 1
2
(
Wti+1 âˆ’Wti
)2}
= 1
2
[
lim
nâ†’âˆ
(
W2
tn âˆ’W2
0
)
+ t
]
= 1
2
(W2
t + t) .

3.2.1
ItÂ¯o Calculus
105
To show that J(t) is not a martingale, we note from Problem 3.2.1.2 (page 103) that under
the filtration â„±u, u < t,
ğ”¼(J(t)|â„±u) = ğ”¼
(1
2
(
W2
t + t
)|||â„±u
)
= 1
2t + 1
2ğ”¼
(
W2
t |||â„±u
)
= 1
2t + 1
2(t âˆ’u) + 1
2W2
u
â‰ 1
2
(
W2
u + u
)
.
Therefore, J(t) is not a martingale.
â—½
4. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
We define the Stratonovich integral of f(Wt, t) âˆ˜dWt as the following limit
âˆ«
t
0
f(Ws, s) âˆ˜dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f
(Wti + Wti+1
2
, ti
) (
Wti+1 âˆ’Wti
)
and the ItÂ¯o integral of f(Wt, t) dWt as
âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)(Wti+1 âˆ’Wti)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Show that
âˆ«
t
0
f(Ws, s) âˆ˜dWs = 1
2 âˆ«
t
0
ğœ•f(Ws, s)
ğœ•Ws
ds + âˆ«
t
0
f(Ws, s) dWs.
Solution: To prove this result we consider the difference between the two stochastic inte-
grals and using the mean value theorem,
âˆ«
t
0
f(Ws, s) âˆ˜dWs âˆ’âˆ«
t
0
f(Ws, s) dWs
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f
(Wti + Wti+1
2
, ti
) (
Wti+1 âˆ’Wti
)
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
[
f
(Wti + Wti+1
2
, ti
)
âˆ’f(Wti, ti)
] (
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
[
f
(
Wti +
Wti+1 âˆ’Wti+1
2
, ti
)
âˆ’f(Wti, ti)
] (
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
[
1
2
ğœ•f(Wti, ti)
ğœ•Wti
(
Wti+1 âˆ’Wti
)2
+ 1
4
ğœ•2f(Wti, ti)
ğœ•W2
ti
(
Wti+1 âˆ’Wti
)3
+ . . .
]

106
3.2.1
ItÂ¯o Calculus
since lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)p = 0 for p â‰¥3 and hence, for a simple process g(Wt, t),
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
g(Wti, ti)
(
Wti+1 âˆ’Wti
)p||||||
â‰¤lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1|g(Wtk, tk)|
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)p||||||
= 0,
p â‰¥3
or
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
g(Wti, ti)
(
Wti+1 âˆ’Wti
)p
= 0,
p â‰¥3.
Therefore,
âˆ«
t
0
f(Ws, s) âˆ˜dWs âˆ’âˆ«
t
0
f(Ws, s) dWs
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
ğœ•f(Wti, ti)
ğœ•Wti
(
Wti+1 âˆ’Wti
)2
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
ğœ•f(Wti, ti)
ğœ•Wti
(
ti+1âˆ’ti
)
+ lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
ğœ•f(Wti, ti)
ğœ•Wti
[(
Wti+1âˆ’Wti
)2
âˆ’
(
ti+1 âˆ’ti
)]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
ğœ•f(Wti, ti)
ğœ•Wti
(
ti+1 âˆ’ti
)
= 1
2 âˆ«
t
0
ğœ•f (Ws, s)
ğœ•Ws
ds
since, from Problem 2.2.6.1 (page 89),
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
2
ğœ•f(Wti, ti)
ğœ•Wti
[(
Wti+1 âˆ’Wti
)2
âˆ’
(
ti+1 âˆ’ti
)]
â‰¤lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||||
1
2
ğœ•f(Wtk, tk)
ğœ•Wtk
|||||
[nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
âˆ’
nâˆ’1
âˆ‘
i=0
(
ti+1 âˆ’ti
)]
= 0.
Thus,
âˆ«
t
0
f(Ws, s) âˆ˜dWs = 1
2 âˆ«
t
0
ğœ•f(Ws, s)
ğœ•Ws
ds + âˆ«
t
0
f(Ws, s) dWs.
â—½

3.2.1
ItÂ¯o Calculus
107
5. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process
such that â„±t is the associated filtration. The ItÂ¯o integral with respect to the standard Wiener
process can be defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
where f is a simple process and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Show that the path of It is continuous and that it is also â„±t measurable for all t.
Solution: Given that Wti, ti = itâˆ•n, n âˆˆâ„•is both continuous and â„±ti measurable, then
for a simple process, f,
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
.
The path of the ItÂ¯o integral is also continuous and â„±t measurable for all t.
â—½
6. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process
such that â„±t is the associated filtration. The ItÂ¯o integrals with respect to the standard
Wiener process can be defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
and
Jt = âˆ«
t
0
g(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
g(Wti, ti)
(
Wti+1 âˆ’Wti
)
where f and g are simple processes and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t,
n âˆˆâ„•. Show that
It Â± Jt = âˆ«
t
0
[
f(Ws, s) Â± g(Ws, s)
]
dWs
and for constant c,
cIt = âˆ«
t
0
cf(Ws, s) dWs,
cJt = âˆ«
t
0
cg(Ws, s) dWs.
Solution: Using the sum rule in integration,
It Â± Jt = âˆ«
t
0
f(Ws, s) dWs Â± âˆ«
t
0
g(Ws, s) dWs
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
Â± lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
g(Wti, ti)
(
Wti+1 âˆ’Wti
)

108
3.2.1
ItÂ¯o Calculus
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
[
f(Wti, ti) Â± g(Wti, ti)
](
Wti+1 âˆ’Wti
)
= âˆ«
t
0
[
f(Ws, s) Â± g(Ws, s)
]
dWs.
For constant c,
cIt = c âˆ«
t
0
f(Ws, s) dWs
= c lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
c f(Wti, ti)
(
Wti+1 âˆ’Wti
)
= âˆ«
t
0
c f(Ws, s) dWs.
The same steps can be applied to show that cJt = âˆ«
t
0
cg(Ws, s) dWs.
â—½
7. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process
such that â„±t is the associated filtration. The stochastic ItÂ¯o integral with respect to the
standard Wiener process can be defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
where f is a simple function and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Using the properties of the standard Wiener process, show that It is a martingale.
Solution: Given that Wt is a martingale, we note the following:
(a) Under the filtration â„±u, u < t, by definition
âˆ«
t
0
f(Ws, s) dWs = âˆ«
u
0
f(Ws, s) dWs + âˆ«
t
u
f(Ws, s) dWs
= lim
nâ†’âˆ
mâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
+ lim
nâ†’âˆ
[nâˆ’1
âˆ‘
i=m
f(Wti, ti)
(
Wti+1 âˆ’Wti
)]
where
Iu = âˆ«
u
0
f(Ws, s) dWs = lim
nâ†’âˆ
mâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
,
m < n âˆ’1
and
ğ”¼(Iu|â„±u) = Iu.

3.2.1
ItÂ¯o Calculus
109
Finally, because {Wt âˆ¶t â‰¥0} is a martingale we have
ğ”¼(It|â„±u) = ğ”¼
[
lim
nâ†’âˆ
mâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)||||||
â„±u
]
+ğ”¼
[
lim
nâ†’âˆ
[nâˆ’1
âˆ‘
i=m
f(Wti, ti)
(
Wti+1 âˆ’Wti
)]||||||
â„±u
]
= Iu + lim
nâ†’âˆğ”¼
[ nâˆ’1
âˆ‘
i=m
f(Wti, ti)
(
Wti+1 âˆ’Wti
)||||||
â„±u
]
= Iu + lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=m
ğ”¼
[
f(Wti, ti)
(
Wti+1 âˆ’Wti
)|||â„±u
]
= Iu + lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=m
ğ”¼
[
ğ”¼
[
f(Wti, ti)
(
Wti+1 âˆ’Wti
)|||â„±ti
] |||â„±u
]
= Iu + lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=m
ğ”¼
[
f(Wti, ti)
(
Wti âˆ’Wti
)|||â„±u
]
= Iu.
(b) Assuming | f(Wt, t)| < âˆ, we have
|It| = lim
nâ†’âˆ
||||||
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)||||||
â‰¤lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
||||
f(Wti, ti)
(
Wti+1 âˆ’Wti
)||||
â‰¤lim
nâ†’âˆ
[
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
|||
nâˆ’1
âˆ‘
i=0
||| f(Wti, ti)|||
]
.
Because Wt is continuous we have lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| = 0, therefore we can
deduce that ğ”¼(|It|) < âˆ.
(c) Since It is a function of Wt, hence it is â„±t-adapted.
From the results of (a)â€“(c) we have shown that It is a martingale.
N.B. From the above result we can easily deduce that if {Mt}tâ‰¥0 is a martingale with
continuous sample paths and by defining the following stochastic ItÂ¯o integral:
It = âˆ«
t
0
f(Ms, s) dMs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Mti, ti)
(
Mti+1 âˆ’Mti
)
where f is a simple process and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•,
then It is a martingale.
â—½

110
3.2.1
ItÂ¯o Calculus
8. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
The stochastic ItÂ¯o integral with respect to a standard Wiener process can be defined as
âˆ«
t
0
s dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ti
(
Wti+1 âˆ’Wti
)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•. Prove that
âˆ«
t
0
s dWs = tWt âˆ’âˆ«
t
0
Ws ds.
Solution: By definition, W0 = 0 and we can expand
âˆ«
t
0
s dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ti
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
ti
(
Wti+1 âˆ’Wti
)
+ ti+1Wti+1 âˆ’ti+1Wti+1
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
ti+1Wti+1 âˆ’tiWti âˆ’(ti+1 âˆ’ti
) Wti+1
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
ti+1Wti+1 âˆ’tiWti
)
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(ti+1 âˆ’ti
)
= tWt âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(ti+1 âˆ’ti
) .
By definition,
âˆ«
t
0
Ws ds = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(ti+1 âˆ’ti
)
and to show
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(ti+1 âˆ’ti
) âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(ti+1 âˆ’ti
) = 0
we note from Problem 2.2.6.2 (page 90) that
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (ti+1 âˆ’ti
) = 0.
Therefore, we can deduce that
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti+1
(ti+1 âˆ’ti
) = âˆ«
t
0
Ws ds

3.2.1
ItÂ¯o Calculus
111
and hence
âˆ«
t
0
s dWs = tWt âˆ’âˆ«
t
0
Ws ds.
â—½
9. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
The stochastic ItÂ¯o integral with respect to a standard Wiener process can be defined as
âˆ«
t
0
W2
s dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
W2
ti
(
Wti+1 âˆ’Wti
)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•. Prove that
âˆ«
t
0
W2
s dWs = 1
3W3
t âˆ’âˆ«
t
0
Ws ds.
Solution: By definition,
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
W2
ti
(
Wti+1 âˆ’Wti
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
3
(
W3
ti+1 âˆ’W3
ti
)
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(
Wti+1 âˆ’Wti
)2
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
3
(
Wti+1 âˆ’Wti
)3
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
3
(
W3
ti+1 âˆ’W3
ti
)
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
(
ti+1 âˆ’ti
)
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
[(
Wti+1 âˆ’Wti
)2
âˆ’(ti+1 âˆ’ti
)]
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
1
3
(
Wti+1 âˆ’Wti
)3
= 1
3W3
t âˆ’âˆ«
t
0
Ws ds
since from Problems 2.2.6.1 (page 89) and 2.2.6.4 (page 92),
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
Wti
[(
Wti+1 âˆ’Wti
)2
âˆ’(ti+1 âˆ’ti)
]||||||
â‰¤lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1 |Wtk|
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)2
âˆ’
nâˆ’1
âˆ‘
i=0
(ti+1 âˆ’ti
)||||||
= lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1 |Wtk||t âˆ’t|
= 0

112
3.2.1
ItÂ¯o Calculus
and
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
)3
= 0.
Therefore, âˆ«
t
0
W2
s dWs = 1
3W3
t âˆ’âˆ«
t
0
Ws ds.
N.B. Instead of going through first principles, we can also show âˆ«
t
0
W2
s dWs = 1
3W3
t âˆ’
âˆ«
t
0
Ws ds by applying ItÂ¯oâ€™s formula on Xt = 1
3W3
t , where
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•Wt
dWt + 1
2
ğœ•2Xt
ğœ•W2
t
dW2
t + . . .
= W2
t dWt + Wtdt.
Taking integrals,
âˆ«
t
0
dXs = âˆ«
t
0
W2
s dWs + âˆ«
t
0
Ws ds
Xt âˆ’X0 = âˆ«
t
0
W2
s dWs + âˆ«
t
0
Ws ds
and since W0 = 0, therefore âˆ«
t
0
W2
s dWs = 1
3W3
t âˆ’âˆ«
t
0
Ws ds.
â—½
10. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
The stochastic ItÂ¯o integral with respect to a standard Wiener process can be defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
where f is a simple process and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Using the properties of a standard Wiener process, show that
ğ”¼
[
âˆ«
t
0
f(Ws, s) dWs
]
= 0
and deduce that if {Mt âˆ¶t â‰¥0} is a martingale then
ğ”¼
[
âˆ«
t
0
g(Ms, s) dMs
]
= 0
where g is a simple process.

3.2.1
ItÂ¯o Calculus
113
Solution: By definition,
ğ”¼
[
âˆ«
t
0
f(Ws, s) dWs
]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)
(
Wti+1 âˆ’Wti
)]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
ğ”¼
[
f(Wti, ti)
(
Wti+1 âˆ’Wti
) |||â„±ti
]]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)
(
Wti âˆ’Wti
)]
= 0
since {Wt âˆ¶t â‰¥0} is a martingale and therefore ğ”¼
(
Wti+1
|||â„±ti
)
= Wti.
Using the same steps as described above, if {Mt âˆ¶t â‰¥0} is a martingale then
ğ”¼
[
âˆ«
t
0
g(Ms, s) dMs
]
= 0
where g is a simple process.
â—½
11. ItÂ¯o Isometry. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process. The stochastic ItÂ¯o integral with respect to a standard Wiener process can
be defined as
It = âˆ«
t
0
f(Ws, s) dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)
where f is a simple process and ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•.
Using the properties of a standard Wiener process, show that
ğ”¼
[(
âˆ«
t
0
f(Ws, s) dWs
)2]
= ğ”¼
[
âˆ«
t
0
f(Ws, s)2ds
]
.
Solution: Using the property of independent increments of a standard Wiener process as
well as the martingale properties of Wt and W2
t âˆ’t, we note that
ğ”¼
[(
âˆ«
t
0
f(Ws, s) dWs
)2]
âˆ’ğ”¼
[
âˆ«
t
0
f(Ws, s)2ds
]
= ğ”¼
â¡
â¢
â¢â£
{
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)
(
Wti+1 âˆ’Wti
)}2â¤
â¥
â¥â¦
âˆ’ğ”¼
[
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
f(Wti, ti)2 (ti+1 âˆ’ti
)
]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)2(
Wti+1 âˆ’Wti
)2]
âˆ’lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)2 (ti+1 âˆ’ti
)]

114
3.2.1
ItÂ¯o Calculus
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)2
{(
Wti+1 âˆ’Wti
)2
âˆ’(ti+1 âˆ’ti
)}]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
ğ”¼
[
f(Wti, ti)2
{(
Wti+1 âˆ’Wti
)2
âˆ’(ti+1 âˆ’ti
)}|||||
â„±ti
]]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
ğ”¼
[
f(Wti, ti)2 {
W2
ti+1 âˆ’2Wti+1Wti + W2
ti âˆ’ti+1 + ti
}||||
â„±ti
]]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
{
ğ”¼
[
ğ”¼
[
f(Wti, ti)2 (
W2
ti+1 âˆ’ti+1
)||||
â„±ti
]
âˆ’2ğ”¼
[
f(Wti, ti)2Wti+1Wti
|||â„±ti
]
+ ğ”¼
[
f(Wti, ti)2 (
W2
ti + ti
)||||
â„±ti
]]}
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Wti, ti)2 (
W2
ti âˆ’ti âˆ’2W2
ti + W2
ti + ti
)]
= 0.
Therefore, ğ”¼
[(
âˆ«
t
0
f(Ws, s) dWs
)2]
= ğ”¼
[
âˆ«
t
0
f(Ws, s)2ds
]
.
â—½
12. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
The stochastic ItÂ¯o integral with respect to a standard Wiener process can be defined as
It = âˆ«
t
0
f(Ws, s) dWs
where f is a simple process. Show that the ItÂ¯o integral has quadratic variation process
âŸ¨I, IâŸ©t given by
âŸ¨I, IâŸ©t = âˆ«
t
0
f(Ws, s)2 ds.
Solution: By definition,
âŸ¨I, IâŸ©t = lim
mâ†’âˆ
mâˆ’1
âˆ‘
k=0
(
Itk+1 âˆ’Itk
)2
where tk = ktâˆ•m, 0 = t0 < t1 < t2 < . . . < tmâˆ’1 < tm = t, m âˆˆâ„•. We first concentrate on
one of the subintervals [tk, tk+1) on which f(Ws, s) = f(Wtk, tk) is a constant value for all
s âˆˆ[tk, t+1). Partitioning
tk = s0 < s1 < . . . < sn = tk+1,
we can write
Isi+1 âˆ’Isi = âˆ«
si+1
si
f(Wtk, tk) dWu = f(Wtk, tk)
(
Wsi+1 âˆ’Wsi
)
.

3.2.1
ItÂ¯o Calculus
115
Therefore,
(Itk+1 âˆ’Itk)2 = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Isi+1 âˆ’Isi
)2
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
[
f(Wtk, tk)
(
Wsi+1 âˆ’Wsi
)]2
= f(Wtk, tk)2 lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wsi+1 âˆ’Wsi
)2
= f(Wtk, tk)2 (tk+1 âˆ’tk
)
since limnâ†’âˆ
âˆ‘nâˆ’1
i=0
(
Wsi+1 âˆ’Wsi
)2
converges to the quadratic variation of a standard
Wiener process over [tk, tk+1) which is tk+1 âˆ’tk. Therefore,
(Itk+1 âˆ’Itk)2 = f(Wtk, tk)2 (tk+1 âˆ’tk
) = âˆ«
tk+1
tk
f(Ws, s)2 ds
where f(Ws, s) is a constant value for all s âˆˆ[tk, tk+1).
Finally, to obtain the quadratic variation of the ItÂ¯o integral It,
âŸ¨I, IâŸ©t = lim
mâ†’âˆ
mâˆ’1
âˆ‘
k=0
(
Itk+1 âˆ’Itk
)2
= lim
mâ†’âˆ
mâˆ’1
âˆ‘
k=0
f(Wtk, tk)2 (tk+1 âˆ’tk
) = âˆ«
t
0
f(Ws, s)2 ds.
N.B. In differential form we can write dâŸ¨I, IâŸ©t = dItdIt = f(Wt, t)2dt. By comparing the
results of the quadratic variation âŸ¨I, IâŸ©t and ğ”¼(I2
t ), we can see that the former is computed
path-by-path and hence the result is random. In contrast, the variance of the ItÂ¯o integral,
Var(It) = ğ”¼(I2
t ) = ğ”¼(âŸ¨I, IâŸ©t), is the mean value of all possible paths of the quadratic varia-
tion and hence is non-random.
â—½
13. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Using integration by parts, show that
âˆ«
t
0
Ws ds = âˆ«
t
0
(t âˆ’s) dWs
and prove that âˆ«
t
0
Ws ds âˆ¼ğ’©
(
0, t3
3
)
.
Is
Bt =
â§
âª
â¨
âªâ©
0
t = 0
âˆš
3
t
âˆ«
t
0
Ws ds
t > 0
a standard Wiener process?

116
3.2.1
ItÂ¯o Calculus
Solution: Using integration by parts,
âˆ«u
(dğ‘£
ds
)
ds = uğ‘£âˆ’âˆ«ğ‘£
(du
ds
)
ds.
We set u = Ws and dğ‘£âˆ•ds = 1. Therefore,
âˆ«
t
0
Ws ds = sWs|||
t
0 âˆ’âˆ«
t
0
s dWs
= tWt âˆ’âˆ«
t
0
s dWs
= âˆ«
t
0
(t âˆ’s) dWs.
Taking expectations,
ğ”¼
(
âˆ«
t
0
Ws ds
)
= ğ”¼
(
âˆ«
t
0
(t âˆ’s) dWs
)
= 0
ğ”¼
[(
âˆ«
t
0
Ws ds
)2]
= ğ”¼
(
âˆ«
t
0
(t âˆ’s)2 ds
)
= t3
3 .
To show that âˆ«
t
0
Ws ds follows a normal distribution, let
Mt = âˆ«
t
0
Ws ds = âˆ«
t
0
(t âˆ’s) dWs
and using the properties of the ItÂ¯o integral (see Problems 3.2.1.7, page 108 and 3.2.1.12,
page 114), we can deduce that
Mt is a martingale
and
âŸ¨M, MâŸ©t = âˆ«
t
0
(t âˆ’s)2 ds = t3
3
so that dMt â‹…dMt = t2dt.
By defining
Zt = e
ğœƒMtâˆ’1
2 ğœƒ2
(
t3
3
)
,
ğœƒâˆˆâ„
then expanding dZt using Taylorâ€™s theorem and applying ItÂ¯oâ€™s lemma, we have
dZt = ğœ•Zt
ğœ•t dt + ğœ•Zt
ğœ•Mt
dMt + 1
2
ğœ•2f
ğœ•M2
t
(dMt)2 + . . .
= âˆ’1
2ğœƒ2t2ZTdt + ğœƒZtdMt + 1
2ğœƒ2Zt(dMt)2 + . . .
= âˆ’1
2ğœƒ2t2ZTdt + ğœƒZtdMt + 1
2ğœƒ2t2Ztdt
= ğœƒZtdMt.

3.2.1
ItÂ¯o Calculus
117
Taking integrals, we can express
âˆ«
t
0
dZu = âˆ«
t
0
ğœƒZu dMu
Zt âˆ’Z0 = ğœƒâˆ«
t
0
Zu dMu
Zt = 1 + ğœƒâˆ«
t
0
Zu dMu.
Finally, by taking expectations and knowing that Mt is a martingale, we have
ğ”¼(Zt) = 1 + ğœƒğ”¼
(
âˆ«
t
0
Zu dMu
)
= 1
and hence
ğ”¼(eğœƒMt) = e
1
2 ğœƒ2
(
t3
3
)
which is the moment generating function of a normal distribution with mean zero and
variance t3
3 . Thus, âˆ«
t
0
Ws ds âˆ¼ğ’©
(
0, t3
3
)
.
Even though Bt âˆ¼ğ’©(0, t) for t > 0, Bt is not a standard Wiener process since for t, u > 0,
ğ”¼(Bt+u âˆ’Bt) = ğ”¼(Bt+u) âˆ’ğ”¼(Bt)
= ğ”¼
( âˆš
3
t + u âˆ«
t+u
0
Ws ds
)
âˆ’ğ”¼
(âˆš
3
t
âˆ«
t
0
Ws ds
)
= 0
and using the result of Problem 2.2.1.13 (page 66),
Var(Bt+u âˆ’Bt) = Var(Bt+u) + Var(Bt) âˆ’2Cov(Bt+u, Bt)
= Var
( âˆš
3
t + u âˆ«
t+u
0
Ws ds
)
+ Var
(âˆš
3
t
âˆ«
t
0
Ws ds
)
âˆ’2Cov
( âˆš
3
t + u âˆ«
t+u
0
Ws ds,
âˆš
3
t
âˆ«
t
0
Ws ds
)
= t + u + t âˆ’
6
t(t + u)
[1
3t3 + 1
2ut2]
= 2t + u âˆ’t(2t + 3u)
t + u
=
u2
t + u
â‰ u

118
3.2.1
ItÂ¯o Calculus
which shows that Bt does not have the stationary increment property. Therefore, Bt is not
a standard Wiener process.
â—½
14. Generalised ItÂ¯o Integral. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a
standard Wiener process. Given that f is a simple process, show
âˆ«
t
0
f(Ws, s) dWs = Wt f(Wt, t) âˆ’âˆ«
t
0
[
Ws
ğœ•f
ğœ•t (Ws, s) + ğœ•f
ğœ•Wt
(Ws, s) + 1
2Ws
ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
âˆ’âˆ«
t
0
Ws
ğœ•f
ğœ•Wt
(Ws, s) dWs
and
âˆ«
t
0
f(Ws, s) ds = tf(Wt, t) âˆ’âˆ«
t
0
s
[
ğœ•f
ğœ•t (Ws, s) + 1
2
ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
âˆ’âˆ«
t
0
s ğœ•f
ğœ•Wt
(Ws, s) dWs.
Solution: For the first result, using Taylorâ€™s theorem on d(Wt f(Wt, t)) and subsequently
applying ItÂ¯oâ€™s formula we have
d(Wt f(Wt, t)) = Wt
ğœ•f
ğœ•t (Wt, t) dt +
[
f(Wt, t) + Wt
ğœ•f
ğœ•Wt
(Wt, t)
]
dWt
+1
2
[
ğœ•f
ğœ•Wt
(Wt, t) + ğœ•f
ğœ•Wt
(Wt, t) + Wt
ğœ•2f
ğœ•W2
t
(Wt, t)
]
dW2
t + . . .
= Wt
ğœ•f
ğœ•t (Wt, t) dt +
[
f(Wt, t) + Wt
ğœ•f
ğœ•Wt
(Wt, t)
]
dWt
+1
2
[
2 ğœ•f
ğœ•Wt
(Wt, t) + Wt
ğœ•2f
ğœ•W2
t
(Wt, t)
]
dt
=
[
Wt
ğœ•f
ğœ•t (Wt, t) + ğœ•f
ğœ•Wt
(Wt, t) + 1
2Wt
ğœ•2f
ğœ•W2
t
(Wt, t)
]
dt
+
[
f(Wt, t) + Wt
ğœ•f
ğœ•Wt
(Wt, t)
]
dWt.
Taking integrals from 0 to t,
âˆ«
t
0
d(Wsf(Ws, s)) = âˆ«
t
0
[
Ws
ğœ•f
ğœ•t (Ws, s) + ğœ•f
ğœ•Wt
(Ws, s) + 1
2Ws
ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
+ âˆ«
t
0
[
f(Ws, s) + Ws
ğœ•f
ğœ•Wt
(Ws, s)
]
dWs

3.2.1
ItÂ¯o Calculus
119
and rearranging the terms, finally
âˆ«
t
0
f(Ws, s) dWs = Wt f(Wt, t) âˆ’âˆ«
t
0
[
Ws
ğœ•f
ğœ•t (Ws, s) + ğœ•f
ğœ•Wt
(Ws, s) + 1
2Ws
ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
âˆ’âˆ«
t
0
Ws
ğœ•f
ğœ•Wt
(Ws, s) dWs
since W0 = 0.
As for the second result, from Taylorâ€™s theorem and ItÂ¯oâ€™s formula
d(tf(Wt, t)) = f(Wt, t) dt + tğœ•f
ğœ•t (Wt, t) dt + t ğœ•f
ğœ•Wt
(Wt, t) dWt + 1
2t ğœ•2f
ğœ•W2
t
(Wt, t) dW2
t + . . .
= f(Wt, t) dt + tğœ•f
ğœ•t (Wt, t) dt + t ğœ•f
ğœ•Wt
(Wt, t) dWt + 1
2t ğœ•2f
ğœ•W2
t
(Wt, t) dt
=
[
f(Wt, t) + tğœ•f
ğœ•t (Wt, t) + 1
2t ğœ•2f
ğœ•W2
t
(Wt, t)
]
dt + t ğœ•f
ğœ•Wt
(Wt, t) dWt.
Taking integrals from 0 to t we have
âˆ«
t
0
d(sf(Ws, s)) = âˆ«
t
0
[
f(Ws, s) + sğœ•f
ğœ•t (Ws, s) + 1
2s ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
+ âˆ«
t
0
s ğœ•f
ğœ•Wt
(Ws, s) dWs
and hence
âˆ«
t
0
f(Ws, s) ds = tf(Wt, t) âˆ’âˆ«
t
0
s
[
ğœ•f
ğœ•t (Ws, s) + 1
2
ğœ•2f
ğœ•W2
t
(Ws, s)
]
ds
âˆ’âˆ«
t
0
s ğœ•f
ğœ•Wt
(Ws, s) dWs.
â—½
15. One-Dimensional LÃ©vy Characterisation Theorem. Let (Î©, â„±, â„™) be a probability space
and let {Mt âˆ¶t â‰¥0} be a martingale with respect to the filtration â„±t, t â‰¥0. By assuming
M0 = 0, Mt has continuous sample paths whose quadratic variation
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Mti+1 âˆ’Mti
)2
= t
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•show that Mt is a standard
Wiener process.

120
3.2.1
ItÂ¯o Calculus
Solution: We first need to show that Mt âˆ¼ğ’©(0, t) or, using the moment generating func-
tion approach, we need to show that ğ”¼(eğœƒMt) = e
1
2 ğœƒt2 for a constant ğœƒ.
Let f(Mt, t) = eğœƒMtâˆ’1
2 ğœƒ2t and since dMt â‹…dMt = dt and (dt)ğœˆ= 0, ğœˆâ‰¥2 from ItÂ¯oâ€™s formula,
df(Mt, t) = ğœ•f
ğœ•t dt + ğœ•f
ğœ•Mt
dMt + 1
2
ğœ•2f
ğœ•M2
t
(dMt)2 + . . .
=
(
ğœ•f
ğœ•t + 1
2
ğœ•2f
ğœ•M2
t
)
dt + ğœ•f
ğœ•Mt
dMt
=
(
âˆ’1
2ğœƒ2t + 1
2ğœƒ2t
)
f(Mt, t) dt + ğœƒf(Mt, t) dMt
= ğœƒf(Mt, t) dMt.
Taking integrals from 0 to t, and then taking expectations, we have
âˆ«
t
0
df(Ms, s) = ğœƒâˆ«
t
0
f(Ms, s) dMs
f(Mt, t) âˆ’f(M0, 0) = ğœƒâˆ«
t
0
f(Ms, s) dMs
ğ”¼(f(Mt, t)) = 1 + ğœƒğ”¼
[
âˆ«
t
0
f(Ms, s) dMs
]
.
By definition of the stochastic ItÂ¯o integral we can write
ğ”¼
[
âˆ«
t
0
f(Ms, s) dMs
]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Mti, ti)
(
Mti+1 âˆ’Mti
)]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
ğ”¼
[
f(Mti, ti)
(
Mti+1 âˆ’Mti
) |||â„±ti
]]
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğ”¼
[
f(Mti, ti)
(
Mti âˆ’Mti
)]
= 0
since {Mt}tâ‰¥0 is a martingale and hence
ğ”¼(f(Mt, t)) = 1
or
ğ”¼(eğœƒMt) = e
1
2 ğœƒ2t
which is the moment generating function for the normal distribution with mean zero and
variance t. Therefore, Mt âˆ¼ğ’©(0, t).

3.2.1
ItÂ¯o Calculus
121
Since Mt is a martingale, for s < t
ğ”¼(Mt|| â„±s
) = ğ”¼(Mt âˆ’Ms + Ms|| â„±s
)
= ğ”¼(Mt âˆ’Ms|| â„±s
) + ğ”¼(Ms|| â„±s
)
= ğ”¼(Mt âˆ’Ms|| â„±s
) + Ms
= Ms.
Therefore, ğ”¼(Mt âˆ’Ms|| â„±s
) = ğ”¼(Mt âˆ’Ms
) = 0 and hence Mt âˆ’Ms âŸ‚âŸ‚â„±s. So, we have
shown that Mt has the independent increment property.
Finally, to show that Mt has the stationary increment, for t > 0 and s > 0 we have
ğ”¼(Mt+s âˆ’Mt
) = ğ”¼(Mt+s
) âˆ’ğ”¼(Mt
) = 0
and using the independent increment property of Mt
Var (Mt+s âˆ’Mt
) = Var (Mt+s
) + Var (Mt
) âˆ’2Cov (Mt+s, Mt
)
= t + s + t âˆ’2 [ğ”¼(Mt+sMt
) âˆ’ğ”¼(Mt+s
) ğ”¼(Mt
)]
= 2t + s âˆ’2ğ”¼(Mt+sMt
)
= 2t + s âˆ’2ğ”¼
(
Mt
(
Mt+s âˆ’Mt
)
+ M2
t
)
= 2t + s âˆ’2ğ”¼(Mt
) ğ”¼(Mt+s âˆ’Mt
) âˆ’2ğ”¼(M2
t
)
= 2t + s âˆ’2t
= s.
Therefore, Mt+s âˆ’Mt âˆ¼ğ’©(0, s).
Because M0 = 0 and also Mt has continuous sample paths with independent and stationary
increments, so Mt is a standard Wiener process.
â—½
16. Multi-Dimensional LÃ©vy Characterisation Theorem. Let (Î©, â„±, â„™) be a probability space
and let {M(1)
t
âˆ¶t â‰¥0}, {M(2)
t
âˆ¶t â‰¥0}, . . . , {M(n)
t
âˆ¶t â‰¥0} be martingales with respect to
the filtration â„±t, t â‰¥0. By assuming M(i)
0 = 0, M(i)
t
has continuous sample paths whose
quadratic variation
lim
nâ†’âˆ
m
âˆ‘
k=0
(
M(i)
tk+1 âˆ’M(i)
tk
)2
= t
and cross-variation between M(i)
t
and M(j)
t , i â‰ j, i, j = 1, 2, . . . , n
lim
mâ†’âˆ
m
âˆ‘
k=0
(
M(i)
tk+1 âˆ’M(i)
tk
) (
M(j)
tk+1 âˆ’M(j)
tk
)
= 0
where tk = ktâˆ•m, 0 = t0 < t1 < t2 < . . . < tmâˆ’1 < tm = t, m âˆˆâ„•, show that M(1)
t ,
M(2)
t , . . . , M(n)
t
are independent standard Wiener processes.

122
3.2.1
ItÂ¯o Calculus
Solution: Following
Problem
3.2.1.15
(page
119),
we
can
easily
prove
that
M(1)
t , M(2)
t , . . . , M(n)
t
are standard Wiener processes. In order to show M(1)
t , M(2)
t , . . . , M(n)
t
are mutually independent, we need to show the joint moment generating function of
M(1)
t , M(2)
t , . . . ,
M(n)
t
is
ğ”¼
(
eğœƒ(1)M(1)
t
+ğœƒ(2)M(2)
t
+ ... ğœƒ(n)M(n)
t
)
= e
1
2 (ğœƒ(1))2t â‹…e
1
2 (ğœƒ(2))2t Â· Â· Â· e
1
2 (ğœƒ(n))2t
for constants ğœƒ(1), ğœƒ(2), . . . , ğœƒ(n).
Let f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = âˆn
i=1 eğœƒ(i)M(i)
t âˆ’1
2 (ğœƒ(i))2t and since dM(i)
t â‹…dM(i)
t
= dt,
dM(i)
t
â‹…dM(j)
t
= 0, i, j = 1, 2, . . . , n, i â‰ j and (dt)ğœˆ= 0, ğœˆâ‰¥2, from ItÂ¯oâ€™s formula
df(M(1)
t , M(2)
t , . . . , M(n)
t , t) = ğœ•f
ğœ•t dt +
n
âˆ‘
i=1
ğœ•f
ğœ•M(i)
t
dM(i)
t
+1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœ•2f
ğœ•M(i)
t ğœ•M(j)
t
dM(i)
t dM(j)
t
= ğœ•f
ğœ•t dt +
n
âˆ‘
i=1
ğœ•f
ğœ•M(i)
t
dM(i)
t
+ 1
2
n
âˆ‘
i=1
ğœ•2f
ğœ•(M(i)
t )2 (dM(i
t )2
=
(
ğœ•f
ğœ•t + 1
2
n
âˆ‘
i=1
ğœ•2f
ğœ•(M(i)
t )2
)
dt +
n
âˆ‘
i=1
ğœ•f
ğœ•M(i)
t
dM(i)
t
=
n
âˆ‘
i=1
ğœ•f
ğœ•M(i)
t
dM(i)
t
=
n
âˆ‘
i=1
ğœƒ(i)f(M(1)
t , M(2)
t , . . . , M(n)
t , t) dM(i)
t
since
ğœ•f
ğœ•t + 1
2
n
âˆ‘
i=1
ğœ•2f
ğœ•(M(i)
t )2 =
[
âˆ’1
2
n
âˆ‘
i=1
(ğœƒ(i))2 + 1
2
n
âˆ‘
i=1
(ğœƒ(i))2
]
f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = 0.
Integrating both sides from 0 to t and taking expectations, we have
âˆ«
t
0
df(M(1)
s , M(2)
s , . . . , M(n)
s , s) =
n
âˆ‘
i=1 âˆ«
t
0
ğœƒ(i)f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = f(M(1)
0 , M(2)
0 , . . . , M(n)
0 , 0)
+
n
âˆ‘
i=1 âˆ«
t
0
ğœƒ(i)f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
ğ”¼[ f(M(1)
t , M(2)
t , . . . , M(n)
t , t)] = 1 +
n
âˆ‘
i=1
ğœƒ(i)ğ”¼
[
âˆ«
t
0
f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
]
.

3.2.2
One-Dimensional Diffusion Process
123
Because M(i)
t
is a martingale, we can easily show (see Problem 3.2.1.15, page 119) that
ğ”¼
[
âˆ«
t
0
f
(
M(1)
s , M(2)
s , . . . , M(n)
s , t
)
dM(i)
s
]
= 0
for i = 1, 2, . . . , n and hence
ğ”¼
(
eğœƒ(1)M(1)
t
+ğœƒ(2)M(2)
t
+ ... ğœƒ(n)M(n)
t
)
= e
1
2 (ğœƒ(1))2t â‹…e
1
2 (ğœƒ(2))2t Â· Â· Â· e
1
2 (ğœƒ(n))2t
where the joint moment generating function of M(1)
t , M(2)
t , . . . , M(n)
t
is a product of
moment generating functions of M(1)
t , M(2)
t , . . . , M(n)
t . Therefore, M(1)
t , M(2)
t , . . . , M(n)
t
are independent standard Wiener processes.
â—½
3.2.2
One-Dimensional Diffusion Process
1. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Find the SDE for the random process Xt = Wn
t , n âˆˆâ„¤+.
Show that
ğ”¼(Wn
t
) = 1
2n(n âˆ’1) âˆ«
t
0
ğ”¼
(
W(nâˆ’2)
s
)
ds
and using mathematical induction prove that
ğ”¼(Wn
t
) =
â§
âª
â¨
âªâ©
n!t
n
2
2
n
2
(
n
2
)
!
n = 2, 4, 6, . . .
0
n = 1, 3, 5, . . .
Solution: By expanding dXt using Taylorâ€™s formula and applying ItÂ¯oâ€™s formula,
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•Wt
dWt + 1
2
ğœ•2Xt
ğœ•W2
t
dW2
t + . . .
dWn
t = nW(nâˆ’1)
t
dWt + 1
2n(n âˆ’1)W(nâˆ’2)
t
dt.
Taking integrals,
âˆ«
t
0
dWn
s = âˆ«
t
0
nW(nâˆ’1)
s
dWs + 1
2n(n âˆ’1) âˆ«
t
0
W(nâˆ’2)
s
ds
Wn
t = âˆ«
t
0
nW(nâˆ’1)
s
dWs + 1
2n(n âˆ’1) âˆ«
t
0
W(nâˆ’2)
s
ds.
Finally, by taking expectations,
ğ”¼(Wn
t
) = 1
2n(n âˆ’1) âˆ«
t
0
ğ”¼
(
W(nâˆ’2)
s
)
ds.

124
3.2.2
One-Dimensional Diffusion Process
To prove the final result, we will divide it into two sections, one for even numbers n = 2k,
k âˆˆâ„¤+ and another for odd numbers n = 2k + 1, k âˆˆâ„¤+. We note that for n = 2 we have
ğ”¼(W2
t
) = 2!t
2 = t
and because Wt âˆ¼ğ’©(0, t), the result is true for n = 2.
We assume that the result is true for n = 2k, k âˆˆâ„¤+. That is
ğ”¼(W2k
t
) = (2k)!tk
2kk! .
For n = 2(k + 1), k âˆˆâ„¤+ we have
ğ”¼
(
W2(k+1)
t
)
= 1
2(2k + 2)(2k + 1) âˆ«
t
0
ğ”¼(W2k
s
) ds
= 1
2(2k + 2)(2k + 1) âˆ«
t
0
(2k)!sk
2kk!
ds
= (2k + 2)!
2k+1k!
âˆ«
t
0
sk ds
= (2k + 2)!tk+1
2k+1(k + 1)!
=
(2(k + 1))!t
2(k+1)
2
2
2(k+1)
2
(2(k + 1)âˆ•2)!
.
Thus, the result is also true for n = 2(k + 1), k âˆˆâ„¤+.
For n = 1, we have
ğ”¼(Wt
) = 0
and because Wt âˆ¼ğ’©(0, t), the result is true for n = 1.
We assume the result is true for n = 2k + 1, k âˆˆâ„¤+ such that
ğ”¼(W2k+1
t
) = 0.
For n = 2(k + 1) + 1, k âˆˆâ„¤+
ğ”¼
(
W2(k+1)+1
t
)
= 1
2(2k + 3)(2k + 2) âˆ«
t
0
ğ”¼(W2k+1
s
) ds = 0
and hence the result is also true for n = 2(k + 1) + 1, k âˆˆâ„¤+. Therefore, by mathematical
induction
ğ”¼(Wn
t
) =
â§
âª
â¨
âªâ©
n!t
n
2
2
n
2
(
n
2
)
!
n = 2, 4, 6, . . .
0
n = 1, 3, 5, . . .
â—½

3.2.2
One-Dimensional Diffusion Process
125
2. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
For a constant ğœƒfind the SDE for the random process Xt = eğœƒWtâˆ’1
2 ğœƒ2t.
By writing the SDE in integral form calculate ğ”¼(eğœƒWt), the moment generating function
of a standard Wiener process.
Solution: Expanding dXt using Taylorâ€™s theorem and applying ItÂ¯oâ€™s formula,
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•Wt
dWt + 1
2
ğœ•2Xt
ğœ•W2
t
dW2
t + . . .
=
(
ğœ•Xt
ğœ•t + 1
2
ğœ•2Xt
ğœ•W2
t
)
dt + ğœ•Xt
ğœ•Wt
dWt
=
(
âˆ’1
2ğœƒ2Xt + 1
2ğœƒ2Xt
)
dt + ğœƒXtdWt
= ğœƒXtdWt.
Taking integrals, we have
âˆ«
t
0
dXs = âˆ«
t
0
ğœƒXs dWs
Xt âˆ’X0 = âˆ«
t
0
ğœƒXs dWs.
Since X0 = 1 and taking expectations,
ğ”¼(Xt
) âˆ’1 = ğ”¼
(
âˆ«
t
0
ğœƒXs dWs
)
= 0
so
ğ”¼(Xt
) = 1
or
ğ”¼
(
eğœƒWtâˆ’1
2 ğœƒ2t)
= 1.
Therefore, ğ”¼(eğœƒWt) = e
1
2 ğœƒ2t.
â—½
3. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Consider the process
Zt = eğœƒWt
where ğœƒis a constant parameter.
Using ItÂ¯oâ€™s formula, find an SDE for Zt.
By setting mt = ğ”¼(eğœƒWt) show that the integrated SDE can be expressed as
dmt
dt âˆ’1
2ğœƒ2mt = 0.
Given W0 = 0, solve the first-order ordinary differential equation to find mt.

126
3.2.2
One-Dimensional Diffusion Process
Solution: Using ItÂ¯oâ€™s formula we expand Zt as
dZt = ğœ•Zt
ğœ•Wt
dWt + 1
2
ğœ•2Zt
ğœ•W2
t
(dWt)2 + . . .
= ğœƒeğœƒWtdWt + 1
2ğœƒ2eğœƒWtdt
= ğœƒZtdWt + 1
2ğœƒ2Ztdt.
Taking integrals and then expectations,
âˆ«
t
0
dZs = âˆ«
t
0
ğœƒZs dWs + âˆ«
t
0
1
2ğœƒ2Zs ds
Zt âˆ’Z0 = âˆ«
t
0
ğœƒZs dWs + âˆ«
t
0
1
2ğœƒ2Zs ds
ğ”¼(Zt) âˆ’ğ”¼(Z0) = ğ”¼
(
âˆ«
t
0
ğœƒZs dWs
)
+ ğ”¼
(
âˆ«
t
0
1
2ğœƒ2Zs ds
)
ğ”¼(Zt) âˆ’1 = âˆ«
t
0
1
2ğœƒ2ğ”¼(Zs) ds
where Z0 = 1 and ğ”¼
(
âˆ«
t
0
ğœƒZs dWs
)
= 0. By differentiating the integral equation we have
dğ”¼(Zt)
dt
= d
dt âˆ«
t
0
1
2ğœƒ2ğ”¼(Zs) ds
dğ”¼(Zt)
dt
= 1
2ğœƒ2ğ”¼(Zt)
or
dmt
dt âˆ’1
2ğœƒ2mt = 0.
Setting the integrating factor as I = eâˆ’âˆ«1
2 ğœƒ2dt = eâˆ’1
2 ğœƒ2t and multiplying the differential
equation with I, we have
d
dt
(
mteâˆ’1
2 ğœƒ2t)
= 0
or
eâˆ’1
2 ğœƒ2tğ”¼(eğœƒWt) = C
where C is a constant. Since ğ”¼(eğœƒW0) = 1, so C = 1 and hence we finally obtain
ğ”¼
(
eğœƒWt)
= e
1
2 ğœƒ2t.
â—½
4. Let Mt = âˆ«
t
0
f(s) dWs and show that the SDE satisfied by
Xt = exp
{
ğœƒMt âˆ’1
2ğœƒ2
âˆ«
t
0
f(s)2 ds
}
is
dXt = ğœƒf(t)XtdWt

3.2.2
One-Dimensional Diffusion Process
127
and show also that Mt âˆ¼ğ’©
(
0, âˆ«
t
0
f(s)2 ds
)
.
Solution: From ItÂ¯oâ€™s formula,
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•Wt
dWt + 1
2
ğœ•2Xt
ğœ•W2
t
dW2
t + . . .
= ğœ•Xt
ğœ•t dt +
( ğœ•Xt
ğœ•Mt
â‹…ğœ•Mt
ğœ•Wt
)
dWt + 1
2
ğœ•
ğœ•Wt
( ğœ•Xt
ğœ•Mt
â‹…ğœ•Mt
ğœ•Wt
)
dt
=
[ğœ•Xt
ğœ•t + 1
2
ğœ•
ğœ•Wt
( ğœ•Xt
ğœ•Mt
â‹…ğœ•Mt
ğœ•Wt
)]
dt +
( ğœ•Xt
ğœ•Mt
â‹…ğœ•Mt
ğœ•Wt
)
dWt.
Since
ğœ•Xt
ğœ•Mt
= ğœƒexp
{
ğœƒMt âˆ’1
2 âˆ«
t
0
f(s)2 ds
}
= ğœƒXt,
ğœ•Mt
ğœ•Wt
= f(t)
ğœ•
ğœ•Wt
( ğœ•Xt
ğœ•Mt
â‹…ğœ•Mt
ğœ•Wt
)
= ğœƒ2f(t)2Xt,
ğœ•Xt
ğœ•t = âˆ’1
2ğœƒ2f(t)2Xt
so
dXt = ğœƒf(t)XtdWt.
Writing in integral form and then taking expectations,
âˆ«
t
0
dXs = âˆ«
t
0
ğœƒf(s)Xs dWs
Xt âˆ’X0 = âˆ«
t
0
ğœƒf(s)Xs dWs
ğ”¼(Xt
) âˆ’ğ”¼(X0
) = ğ”¼
(
âˆ«
t
0
ğœƒf(s)Xs dWs
)
= 0
ğ”¼(Xt
) = ğ”¼(X0
) = 1.
Therefore,
ğ”¼(eğœƒMt) = exp
(
1
2ğœƒ2
âˆ«
t
0
f(s)2 ds
)
which is the moment generating function for a normal distribution with mean zero and
variance âˆ«
t
0
f(s)2 ds. Hence Mt âˆ¼ğ’©
(
0, âˆ«
t
0
f(s)2 ds
)
.
â—½
5. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Suppose Xt follows the generalised SDE
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
where ğœ‡and ğœare functions of Xt and t. Show that Xt is a martingale if ğœ‡(Xt, t) = 0.

128
3.2.2
One-Dimensional Diffusion Process
Solution: It suffices to show that under the filtration â„±s where s < t, for Xt to be a mar-
tingale
ğ”¼(Xt|â„±s
) = Xs.
By taking integrals of the SDE,
âˆ«
t
s
dXğ‘£= âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£+ âˆ«
t
s
ğœ(Xğ‘£, ğ‘£) dWğ‘£
Xt âˆ’Xs = âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£+ âˆ«
t
s
ğœ(Xğ‘£, ğ‘£) dWğ‘£.
Taking expectations,
ğ”¼(Xt âˆ’Xs
) = ğ”¼
[
âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£
]
or
ğ”¼(Xt
) = ğ”¼(Xs
) + ğ”¼
[
âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£
]
.
Under the filtration â„±s, s < t
ğ”¼(Xt|â„±s
) = ğ”¼(Xs|â„±s
) + ğ”¼
[
âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£
|||||
â„±s
]
= Xs + ğ”¼
[
âˆ«
t
s
ğœ‡(Xğ‘£, t) dğ‘£
|||||
â„±s
]
.
Therefore, if ğœ‡(Xt, t) = 0 then Xt is a martingale.
â—½
6. Bachelier Model (Arithmetic Brownian Motion). Let (Î©, â„±, â„™) be a probability space and
let {Wt âˆ¶t â‰¥0} be a standard Wiener process. Suppose Xt follows the arithmetic Brown-
ian motion with SDE
dXt = ğœ‡dt + ğœdWt
where ğœ‡and ğœare constants. Taking integrals show that for t < T,
XT = Xt + ğœ‡(T âˆ’t) + ğœWTâˆ’t
where WTâˆ’t = WT âˆ’Wt âˆ¼ğ’©(0, T âˆ’t). Deduce that XT, given Xt = x, follows a normal
distribution with mean
ğ”¼(XT|Xt = x) = x + ğœ‡(T âˆ’t)
and variance
Var (XT|Xt = x) = ğœ2(T âˆ’t).

3.2.2
One-Dimensional Diffusion Process
129
Solution: Taking integrals of dXt = ğœ‡dt + ğœdWt we have
âˆ«
T
t
dXs = âˆ«
T
t
ğœ‡ds + âˆ«
T
t
ğœdWs
XT âˆ’Xt = ğœ‡(T âˆ’t) + ğœ(WT âˆ’Wt)
or
XT = Xt + ğœ‡(T âˆ’t) + ğœWTâˆ’t
where WTâˆ’t âˆ¼ğ’©(0, T âˆ’t). Since Xt, ğœ‡and ğœare deterministic components therefore XT,
given Xt = x follows a normal distribution with mean x + ğœ‡(T âˆ’t) and variance ğœ2(T âˆ’t).
â—½
7. Blackâ€“Scholes Model (Geometric Brownian Motion). Let (Î©, â„±, â„™) be a probability space
and let {Wt âˆ¶t â‰¥0} be a standard Wiener process. Suppose Xt follows the geometric
Brownian motion with SDE
dXt = ğœ‡Xtdt + ğœXtdWt
where ğœ‡and ğœare constants. By applying ItÂ¯oâ€™s formula to Yt = log Xt and taking integrals
show that for t < T,
XT = Xte
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
where WTâˆ’t âˆ¼ğ’©(0, T âˆ’t). Deduce that XT, given Xt = x follows a lognormal distribution
with mean
ğ”¼(XT|Xt = x) = xeğœ‡(Tâˆ’t)
and variance
Var (XT|Xt = x) = x2 (
eğœ2(Tâˆ’t) âˆ’1
)
e2ğœ‡(Tâˆ’t).
Solution: From Taylorâ€™s expansion and subsequently using ItÂ¯oâ€™s formula,
d(log Xt) = 1
Xt
dXt âˆ’
1
2X2
t
(dXt)2 + . . .
= ğœ‡dt + ğœdWt âˆ’
1
2X2
t
(ğœ2X2
t dt)
=
(
ğœ‡âˆ’1
2ğœ2)
dt + ğœdWt.
Taking integrals,
âˆ«
T
t
d(log Xu) = âˆ«
T
t
(
ğœ‡âˆ’1
2ğœ2)
du + âˆ«
T
t
ğœdWu
log XT âˆ’log Xt =
(
ğœ‡âˆ’1
2ğœ2)
(T âˆ’t) + ğœ(WT âˆ’Wt)
XT = Xte
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t

130
3.2.2
One-Dimensional Diffusion Process
where WT âˆ’Wt = WTâˆ’t âˆ¼ğ’©(0, T âˆ’t). Therefore,
XT âˆ¼log-ğ’©
[
log Xt +
(
ğœ‡âˆ’1
2ğœ2)
(T âˆ’t), ğœ2(T âˆ’t)
]
and from Problem 1.2.2.9 (page 20) the mean and variance of XT, given Xt = x are
ğ”¼(XT|Xt = x) = e
log x+
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+ 1
2 ğœ2(Tâˆ’t)
= xeğœ‡(Tâˆ’t)
and
Var (XT|Xt = x) =
(
eğœ2(Tâˆ’t) âˆ’1
)
e
2 log x+2
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+ğœ2(Tâˆ’t)
= x2 (
eğœ2(Tâˆ’t) âˆ’1
)
e2ğœ‡(Tâˆ’t)
respectively.
â—½
8. Generalised Geometric Brownian Motion. Let (Î©, â„±, â„™) be a probability space and let
{Wt âˆ¶t â‰¥0} be a standard Wiener process. Suppose Xt follows the generalised geometric
Brownian motion with SDE
dXt = ğœ‡tXtdt + ğœtXtdWt
where ğœ‡t and ğœt are time dependent. By applying ItÂ¯oâ€™s formula to Yt = log Xt and taking
integrals show that for t < T,
XT = Xt exp
{
âˆ«
T
t
(
ğœ‡s âˆ’1
2ğœ2
s
)
ds + âˆ«
T
t
ğœsdWs
}
.
Deduce that XT, given Xt = x follows a lognormal distribution with mean
ğ”¼(XT|Xt = x) = xeâˆ«T
t
ğœ‡sds
and variance
Var (XT|Xt = x) = x2 (
eâˆ«T
t
ğœ2
s ds âˆ’1
)
e2 âˆ«T
t
ğœ‡sds.
Solution: From Taylorâ€™s expansion and using ItÂ¯oâ€™s formula,
d(log Xt) = 1
Xt
dXt âˆ’
1
2X2
t
(dXt)2 + . . .
= ğœ‡tdt + ğœdWt âˆ’
1
2X2
t
(ğœ2
t X2
t dt)
=
(
ğœ‡t âˆ’1
2ğœ2
t
)
dt + ğœtdWt.

3.2.2
One-Dimensional Diffusion Process
131
Taking integrals,
âˆ«
T
t
d(log Xs) = âˆ«
T
t
(
ğœ‡s âˆ’1
2ğœ2
s
)
ds + âˆ«
T
t
ğœs dWs
log XT âˆ’log Xt = âˆ«
T
t
(
ğœ‡s âˆ’1
2ğœs
2)
ds + âˆ«
T
t
ğœs dWs
XT = Xt exp
{
âˆ«
T
t
(
ğœ‡s âˆ’1
2ğœ2
s
)
ds + âˆ«
T
t
ğœdWs
}
.
Thus,
XT âˆ¼log-ğ’©
[
log Xt + âˆ«
T
t
(
ğœ‡s âˆ’1
2ğœ2
s
)
ds, âˆ«
T
t
ğœ2
s ds
]
and from Problem 1.2.2.9 (page 20) we have mean
ğ”¼(XT|Xt = x) = e
log x+âˆ«T
t
(
ğœ‡sâˆ’1
2 ğœ2
s
)
ds+ 1
2 âˆ«T
t
ğœ2
s ds
= xeâˆ«T
t ğœ‡sds
and variance
Var (XT|Xt = x) =
(
eâˆ«T
t
ğœ2
s ds âˆ’1
)
e
2
(
log x+âˆ«T
t
(
ğœ‡sâˆ’1
2 ğœ2
s
)
ds
)
+âˆ«T
t
ğœ2
s ds
= x2 (
eâˆ«T
t
ğœ2
s ds âˆ’1
)
e2 âˆ«T
t
ğœ‡sds.
â—½
9. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Suppose Xt follows the geometric Brownian motion with SDE
dXt = ğœ‡Xtdt + ğœXtdWt
where ğœ‡and ğœare constants. Show that if Yt = Xn
t , for some constant n, then Yt follows
the geometric Brownian process of the form
dYt
Yt
= n
(
ğœ‡+ 1
2(n âˆ’1)ğœ2)
dt + nğœdWt.
Deduce that given Yt, t < T, YT follows a lognormal distribution with the form
YT = Yte
n
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+nğœWTâˆ’t
where WTâˆ’t âˆ¼ğ’©(0, T âˆ’t) with mean
ğ”¼(YT|Yt = y) = ye
n
(
ğœ‡+ 1
2 (nâˆ’1)ğœ2)
(Tâˆ’t)

132
3.2.2
One-Dimensional Diffusion Process
and variance
Var (YT|Yt = y) = y2(en2ğœ2(Tâˆ’t) âˆ’1)e
2n
(
ğœ‡+ 1
2 (nâˆ’1)ğœ2)
(Tâˆ’t).
Solution: From ItÂ¯oâ€™s formula,
dYt = nXnâˆ’1
t
dXt + 1
2n(n âˆ’1)Xnâˆ’2
t
dX2
t
= nXnâˆ’1
t
(ğœ‡Xtdt + ğœXtdWt) + 1
2n(n âˆ’1)ğœ2Xn
t dt
= n
(
ğœ‡+ 1
2(n âˆ’1)ğœ2)
Xn
t dt + nğœXn
t dWt.
By substituting Xn
t = Yt we have
dYt
Yt
= n
(
ğœ‡+ 1
2(n âˆ’1)ğœ2)
dt + nğœdWt.
Since Yt follows a geometric Brownian motion, by analogy with Problem 3.2.2.7 (page
129) and by setting ğœ‡â†n
(
ğœ‡+ 1
2(n âˆ’1)ğœ2)
and ğœâ†nğœ, we can easily show that
YT = Yte
n
(
ğœ‡âˆ’1
2 ğœ2)
(Tâˆ’t)+nğœWTâˆ’t
follows a lognormal distribution where WTâˆ’t âˆ¼ğ’©(0, T âˆ’t). In addition, we can also
deduce
ğ”¼(YT|Yt = y) = ye
n
(
ğœ‡+ 1
2 (nâˆ’1)ğœ2)
(Tâˆ’t)
and
Var (YT|Yt = y) = y2 (
en2ğœ2(Tâˆ’t) âˆ’1
)
e
2n
(
ğœ‡+ 1
2 (nâˆ’1)ğœ2)
(Tâˆ’t).
â—½
10. Ornsteinâ€“Uhlenbeck Process. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0}
be a standard Wiener process. Suppose Xt follows the Ornsteinâ€“Uhlenbeck process with
SDE
dXt = ğœ…(ğœƒâˆ’Xt) dt + ğœdWt
where ğœ…, ğœƒand ğœare constants. By applying ItÂ¯oâ€™s formula to Yt = eğœ…tXt and taking integrals
show that for t < T,
XT = Xteâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of XT, given Xt = x.
Deduce that XT follows a normal distribution.

3.2.2
One-Dimensional Diffusion Process
133
Solution: Expanding Yt = eğœ…tXt using Taylorâ€™s formula and applying ItÂ¯oâ€™s formula,
we have
d(eğœ…tXt) = ğœ…eğœ…tXtdt + eğœ…tdXt + 1
2ğœ…2eğœ…tXt(dt)2 + . . .
= ğœ…eğœ…tXtdt + eğœ…t(ğœ…(ğœƒâˆ’Xt) dt + ğœdWt)
= ğœ…ğœƒeğœ…tdt + ğœeğœ…tdWt.
Integrating the above expression,
âˆ«
T
t
d(eğœ…tXs) = âˆ«
T
t
ğœ…ğœƒeğœ…sds + âˆ«
T
t
ğœeğœ…sdWs
XT = Xteâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs.
Given the fact that
ğ”¼
(
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs
)
= 0
and
ğ”¼
[(
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs
)2]
= ğ”¼
(
âˆ«
T
t
ğœ2eâˆ’2ğœ…(Tâˆ’s)ds
)
= ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
the mean and variance of XT, given Xt = x are
ğ”¼(XT|Xt = x) = xeâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
and
Var (XT|Xt = x) = ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
respectively.
Since âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs can be written in the form
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğœeâˆ’ğœ…(Tâˆ’ti) (
Wti+1 âˆ’Wti
)
where ti = t + i(T âˆ’t)âˆ•n, t = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = T, n âˆˆâ„•then due to the
stationary increment of a standard Wiener process, we can see that each term of Wti+1 âˆ’
Wti âˆ¼ğ’©
(
0, T âˆ’t
n
)
is normal multiplied by a deterministic exponential term. Thus, the
product is normal and given that the sum of normal variables is normal we can deduce
XT âˆ¼ğ’©
(
xeâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] , ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)])
.
â—½

134
3.2.2
One-Dimensional Diffusion Process
11. Geometric Mean-Reverting Process. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶
t â‰¥0} be a standard Wiener process. Suppose Xt follows the geometric mean-reverting
process with SDE
dXt = ğœ…(ğœƒâˆ’log Xt)Xtdt + ğœXtdWt,
X0 > 0
where ğœ…, ğœƒand ğœare constants. By applying ItÂ¯oâ€™s formula to Yt = log Xt show that the
diffusion process can be reduced to an Ornsteinâ€“Uhlenbeck process of the form
dYt =
[
ğœ…(ğœƒâˆ’Yt) âˆ’1
2ğœ2]
dt + ğœdWt.
Show also that for t < T,
log XT = (log Xt)eâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) (1 âˆ’eâˆ’ğœ…(Tâˆ’t)) + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of XT, given Xt = x and deduce that XT follows a lognormal distribution.
Solution: By expanding Yt = log Xt using Taylorâ€™s formula and subsequently applying
ItÂ¯oâ€™s formula, we have
d(log Xt) = 1
Xt
dXt âˆ’
1
2X2
t
(dXt)2 + . . .
= ğœ…(ğœƒâˆ’log Xt) dt + ğœdWt âˆ’1
2ğœ2dt
=
(
ğœ…(ğœƒâˆ’log Xt
) âˆ’1
2ğœ2)
dt + ğœdWt
and hence
dYt =
(
ğœ…(ğœƒâˆ’Yt) âˆ’1
2ğœ2)
dt + ğœdWt.
Using the same steps in solving the Ornsteinâ€“Uhlenbeck process, we apply ItÂ¯oâ€™s formula
on Zt = eğœ…Yt such that
d(eğœ…tYt) = ğœ…eğœ…tYtdt + eğœ…tdYt + 1
2ğœ…2eğœ…tYt(dt)2 + . . .
= ğœ…eğœ…tYtdt + eğœ…t [(
ğœ…(ğœƒâˆ’Yt) âˆ’1
2ğœ2)
dt + ğœdWt
]
=
(
ğœ…ğœƒeğœ…t âˆ’1
2ğœ2eğœ…t)
dt + ğœeğœ…tdWt.
Taking integrals from t to T, we have
âˆ«
T
t
d(eğœ…sYs) = âˆ«
T
t
(
ğœ…ğœƒeğœ…s âˆ’1
2ğœ2eğœ…s)
ds + âˆ«
T
t
ğœeğœ…sdWs

3.2.2
One-Dimensional Diffusion Process
135
or
YT = Yteâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) (1 âˆ’eâˆ’ğœ…(Tâˆ’t)) + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs.
By analogy with Problem 3.2.2.10 (page 132), we can deduce that YT = log XT follows a
normal distribution and hence
log XT âˆ¼ğ’©
(
log Xt
(eâˆ’ğœ…(Tâˆ’t)) +
(
ğœƒâˆ’ğœ2
2ğœ…
) (1 âˆ’eâˆ’ğœ…(Tâˆ’t)) , ğœ2
2ğœ…
(1 âˆ’eâˆ’2ğœ…(Tâˆ’t)))
or
XT âˆ¼log âˆ’ğ’©
(
log Xt
(eâˆ’ğœ…(Tâˆ’t)) +
(
ğœƒâˆ’ğœ2
2ğœ…
) (1 âˆ’eâˆ’ğœ…(Tâˆ’t)) , ğœ2
2ğœ…
(1 âˆ’eâˆ’2ğœ…(Tâˆ’t)))
with mean
ğ”¼(XT|Xt = x) = exp
{
eâˆ’ğœ…(Tâˆ’t) log x +
(
ğœƒâˆ’ğœ2
2ğœ…
)
(1 âˆ’eâˆ’ğœ…(Tâˆ’t)) + ğœ2
4ğœ…(1 âˆ’eâˆ’2ğœ…(Tâˆ’t))
}
and variance
Var (XT|Xt = x) = exp
{
ğœ2
2ğœ…
(1 âˆ’eâˆ’2ğœ…(Tâˆ’t)) âˆ’1
}
Ã— exp
{
2eâˆ’ğœ…(Tâˆ’t) log x + 2
(
ğœƒâˆ’ğœ2
2ğœ…
) (1 âˆ’eâˆ’ğœ…(Tâˆ’t)) +
ğœ2
2ğœ…
(1 âˆ’eâˆ’2ğœ…(Tâˆ’t))}
.
â—½
12. Coxâ€“Ingersollâ€“Ross (CIR) Model. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥
0} be a standard Wiener process. Suppose Xt follows the CIR model with SDE
dXt = ğœ…(ğœƒâˆ’Xt
) dt + ğœ
âˆš
XtdWt,
X0 > 0
where ğœ…, ğœƒand ğœare constants. By applying ItÂ¯oâ€™s formula to Zt = eğœ…tXt and Z2
t = e2ğœ…tX2
t
and taking integrals show that for t < T,
XT = Xteâˆ’ğœ…(Tâˆ’t) + ğœƒ
[
1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+ âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)âˆš
Xs dWs
and
X2
T = X2
t eâˆ’2ğœ…(Tâˆ’t) + (2ğœ…ğœƒ+ ğœ2)
âˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s)Xs ds + 2ğœâˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s)X
3
2
s dWs.
Using the properties of stochastic integrals on the above two expressions, find the mean
and variance of XT, given Xt = x.

136
3.2.2
One-Dimensional Diffusion Process
Solution: Using Taylorâ€™s formula and applying ItÂ¯oâ€™s formula on Zt = eğœ…tXt, we have
d (eğœ…tXt
) = ğœ…eğœ…tXtdt + eğœ…tdXt + 1
2ğœ…2eğœ…tXt(dt)2 + . . .
= ğœ…eğœ…tXtdt + eğœ…t (
ğœ…(ğœƒâˆ’Xt
) dt + ğœ
âˆš
XtdWt
)
= ğœ…ğœƒeğœ…tdt + ğœeğœ…tâˆš
XtdWt.
Integrating the above expression,
âˆ«
T
t
d (eğœ…tXs
) = âˆ«
T
t
ğœ…ğœƒeğœ…s ds + âˆ«
T
t
ğœeğœ…sâˆš
Xs dWs
and therefore
XT = Xteâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)âˆš
Xs dWs.
For the case of Z2
t = e2ğœ…tX2
t , by the application of Taylorâ€™s expansion and ItÂ¯oâ€™s formula
we have
d (e2ğœ…tX2
t
) = 2ğœ…e2ğœ…tX2
t dt + 2e2ğœ…tXtdXt + 1
2
(4ğœ…2) e2ğœ…tX2
t (dt)2 + 1
2
(2e2ğœ…t) (dXt
)2 + . . .
= 2ğœ…e2ğœ…tX2
t dt + 2e2ğœ…tXt
(
ğœ…(ğœƒâˆ’Xt
) dt + ğœ
âˆš
XtdWt
)
+ e2ğœ…t (ğœ2Xtdt)
= e2ğœ…t (2ğœ…ğœƒ+ ğœ2) Xtdt + 2ğœe2ğœ…tX
3
2
t dWt.
By taking integrals,
âˆ«
T
t
d (e2ğœ…sX2
s
) = (2ğœ…ğœƒ+ ğœ2)
âˆ«
T
t
e2ğœ…sXs ds + 2ğœâˆ«
T
t
e2ğœ…sX
3
2
s dWs
and we eventually obtain the following expression:
X2
T = eâˆ’2ğœ…(Tâˆ’t)X2
t + (2ğœ…ğœƒ+ ğœ2)
âˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s)Xs ds + 2ğœâˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s)X
3
2
s dWs.
Given Xt = x, and by taking the expectation of the expression XT, we have
ğ”¼(XT|Xt = x) = xeâˆ’ğœ…(Tâˆ’t) + ğœƒ(1 âˆ’eâˆ’ğœ…(Tâˆ’t)) .
To find the variance, Var (XT|Xt = x) we first take the expectation of X2
T,
ğ”¼(X2
T|Xt = x) = x2eâˆ’2ğœ…(Tâˆ’t) + (2ğœ…ğœƒ+ ğœ2)
âˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s)ğ”¼(Xs|Xt = x) ds
= x2eâˆ’2ğœ…(Tâˆ’t)

3.2.2
One-Dimensional Diffusion Process
137
+ (2ğœ…ğœƒ+ ğœ2)
âˆ«
T
t
eâˆ’2ğœ…(Tâˆ’s) [xeâˆ’ğœ…(sâˆ’t) + ğœƒ(1 âˆ’eâˆ’ğœ…(sâˆ’t))] ds
= x2eâˆ’2ğœ…(Tâˆ’t) +
(
2ğœ…ğœƒ+ ğœ2
ğœ…
)
(x âˆ’ğœƒ) (eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’2ğœ…(Tâˆ’t))
+ğœƒ(2ğœ…ğœƒ+ ğœ2)
2ğœ…
(1 âˆ’eâˆ’2ğœ…(Tâˆ’t)) .
Therefore,
Var (XT|Xt = x) = ğ”¼(X2
T|Xt = x) âˆ’[ğ”¼(XT|Xt = x)]2
= xğœ2
ğœ…
(eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’2ğœ…(Tâˆ’t)) + ğœƒğœ2
2ğœ…
(1 âˆ’2eâˆ’ğœ…(Tâˆ’t) + eâˆ’2ğœ…(Tâˆ’t)) .
â—½
13. Brownian Bridge Process. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a
standard Wiener process. Suppose Xt follows the Brownian bridge process with SDE
dXt = y âˆ’Xt
1 âˆ’t dt + dWt,
X1 = y
where the diffusion is conditioned to be at y at time t = 1. By applying ItÂ¯oâ€™s formula to
Yt = (y âˆ’Xt)âˆ•(1 âˆ’t) and taking integrals show that under an initial condition X0 = x and
for 0 â‰¤t < 1,
Xt = yt + (1 âˆ’t)
(
x + âˆ«
t
0
1
1 âˆ’s dWs
)
.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of Xt, given X0 = x and show that Xt follows a normal distribution.
Solution: By expanding Yt = (y âˆ’Xt)âˆ•(1 âˆ’t) using Taylorâ€™s formula and subsequently
applying ItÂ¯oâ€™s formula, we have
dYt = ğœ•Yt
ğœ•t dt + ğœ•Yt
ğœ•Xt
dXt + 1
2
ğœ•2Yt
ğœ•t2 (dt)2 + 1
2
ğœ•2Yt
ğœ•X2
t
(dXt)2 + . . .
= y âˆ’Xt
(1 âˆ’t)2 dt âˆ’
[(
1
1 âˆ’t
) (y âˆ’Xt
1 âˆ’t dt + dWt
)]
= âˆ’
(
1
1 âˆ’t
)
dWt.
Taking integrals,
âˆ«
t
0
dYs = âˆ’âˆ«
t
0
1
1 âˆ’s dWs
Yt âˆ’Y0 = âˆ’âˆ«
t
0
1
1 âˆ’s dWs

138
3.2.2
One-Dimensional Diffusion Process
y âˆ’Xt
1 âˆ’t = y âˆ’x âˆ’âˆ«
t
0
1
1 âˆ’s dWs.
Therefore,
Xt = yt + (1 âˆ’t)
(
x + âˆ«
t
0
1
1 âˆ’s dWs
)
.
Using the properties of stochastic integrals,
ğ”¼
(
âˆ«
t
0
1
1 âˆ’s dWs
)
= 0
ğ”¼
[(
âˆ«
t
0
1
1 âˆ’s dWs
)2]
= ğ”¼
(
âˆ«
t
0
1
(1 âˆ’s)2 ds
)
=
1
1 âˆ’t
therefore the mean and variance of Xt are
ğ”¼
(
Xt|X0 = x
)
= yt + x(1 âˆ’t)
and
Var (Xt|X0 = x) =
1
1 âˆ’t
respectively.
Since âˆ«
t
0
1
1 âˆ’s dWs is in the form âˆ«
t
0
f(s) dWs, from Problem 3.2.2.4 (page 126) we can
easily prove that âˆ«
t
0
1
1 âˆ’s dWs follows a normal distribution,
âˆ«
t
0
1
1 âˆ’s dWs âˆ¼ğ’©
(
yt + x(1 âˆ’t),
1
1 âˆ’t
)
.
â—½
14. Forward Curve from an Asset Price Following a Geometric Brownian Motion. Let
(Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process.
Suppose the asset price St at time t follows a geometric Brownian motion
dSt
St
= ğœ‡dt + ğœdWt
where ğœ‡is the drift parameter and ğœis the volatility. We define the forward price F(t, T)
as an agreed-upon price set at time t to be paid or received at time T, t â‰¤T and is given
by the relationship
F(t, T) = ğ”¼(ST|| â„±t
) .
Show that the forward curve follows
dF(t, T)
F(t, T) = ğœdWt.

3.2.2
One-Dimensional Diffusion Process
139
Solution: Given that St follows a geometric Brownian motion then, following
Problem 3.2.2.7 (page 129), we can easily show
ğ”¼(ST|| â„±t
) = Steğœ‡(Tâˆ’t).
Because F(t, T) = ğ”¼(ST|| â„±t
) = Steğœ‡(Tâˆ’t), and by expanding F(t, T) using Taylorâ€™s theo-
rem and applying ItÂ¯oâ€™s lemma,
dF(t, T) = ğœ•F
ğœ•t dt + ğœ•F
ğœ•St
dSt + 1
2
ğœ•2F
ğœ•S2
t
(dSt
)2 + . . .
= âˆ’ğœ‡Steğœ‡(Tâˆ’t)dt + eğœ‡(Tâˆ’t)dSt.
Since dSt = ğœ‡Stdt + ğœdWt we have
dF(t, T) = âˆ’ğœ‡Steğœ‡(Tâˆ’t)dt + eğœ‡(Tâˆ’t) (ğœ‡Stdt + ğœStdWt
)
= ğœSteğœ‡(Tâˆ’t)dWt
= ğœF(t, T) dWt.
â—½
15. Forward Curve from an Asset Price Following a Geometric Mean-Reverting Process. Let
(Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard Wiener process. Sup-
pose the asset price St at time t follows a geometric mean-reverting process
dSt
St
= ğœ…(ğœƒâˆ’log St
) dt + ğœdWt
where ğœ…is the mean-reversion rate, ğœƒis the long-term mean and ğœis the volatility param-
eter. We define the forward price F(t, T) as an agreed-upon price set at time t to be paid
or received at time T, t â‰¤T and is given by the relationship
F(t, T) = ğ”¼(ST|| â„±t
) .
Show that the forward curve follows
dF(t, T)
F(t, T) = ğœeâˆ’ğœ…(Tâˆ’t)dWt.
Solution: Using the steps described in Problem 3.2.2.11 (page 134), the mean of ST given
St is
ğ”¼
(
ST|| â„±t
)
= exp
{
eâˆ’ğœ…(Tâˆ’t) log St +
(
ğœƒâˆ’ğœ2
2ğœ…
) (
1 âˆ’eâˆ’ğœ…(Tâˆ’t))
+ ğœ2
4ğœ…
(
1 âˆ’eâˆ’2ğœ…(Tâˆ’t))}
.
Because F(t, T) = ğ”¼(ST|| â„±t
) and expanding F(t, T) using Taylorâ€™s theorem, we have
dF(t, T) = ğœ•F
ğœ•t dt + ğœ•F
ğœ•St
dSt + 1
2
ğœ•2F
ğœ•S2
t
(dSt)2 + . . .

140
3.2.2
One-Dimensional Diffusion Process
=
[
ğœ…eâˆ’ğœ…(Tâˆ’t) log St âˆ’
(
ğœƒâˆ’ğœ2
2ğœ…
)
ğœ…eâˆ’ğœ…(Tâˆ’t) âˆ’ğœ2
2 eâˆ’2ğœ…(Tâˆ’t)
]
F(t, T) dt
+eâˆ’ğœ…(Tâˆ’t)
St
F(t, T) dSt + 1
2
[
âˆ’eâˆ’ğœ…(Tâˆ’t)
S2
t
+ eâˆ’2ğœ…(Tâˆ’t)
S2
t
]
F(t, T)(dSt)2 + . . .
By substituting dSt = ğœ…(ğœƒâˆ’log St
) Stdt + ğœStdWt and applying ItÂ¯oâ€™s lemma,
dF(t, T)
F(t, T) =
[
ğœ…eâˆ’ğœ…(Tâˆ’t) log St âˆ’
(
ğœƒâˆ’ğœ2
2ğœ…
)
ğœ…eâˆ’ğœ…(Tâˆ’t) âˆ’ğœ2
2 eâˆ’2ğœ…(Tâˆ’t)
]
dt
+eâˆ’ğœ…(Tâˆ’t) [ğœ…(ğœƒâˆ’log St
) dt + ğœdWt
] âˆ’ğœ2
2 eâˆ’ğœ…(Tâˆ’t)dt + ğœ2
2 eâˆ’2ğœ…(Tâˆ’t)dt
= ğœeâˆ’ğœ…(Tâˆ’t)dWt.
â—½
16. Forwardâ€“Spot Price Relationship I. Let {Wt âˆ¶t â‰¥0} be the standard Wiener process on
the probability space (Î©, â„±, â„™). We define the forward curve F(t, T) following the SDE
dF(t, T)
F(t, T) = ğœ(t, T) dWt
as an agreed-upon price of an asset with current spot price St to be paid or received at time
T, t â‰¤T where F(t, T) = ğ”¼(ST|| â„±t
) such that ST is the spot price at time T and ğœ(t, T) > 0
is a time-dependent volatility.
Show that the spot price has the following SDE
dSt
St
=
[ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du + âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu
]
dt + ğœ(t, t) dWt.
Solution: By expanding log F(t, T) using Taylorâ€™s theorem and then applying ItÂ¯oâ€™s
lemma,
d log F(t, T) =
1
F(t, T)dF(t, T) âˆ’
1
2F(t, T)2 dF(t, T)2 + . . .
= ğœ(t, T) dWt âˆ’1
2ğœ(t, T)2dt
and taking integrals,
âˆ«
t
0
d log F(u, T) = âˆ«
t
0
ğœ(u, T) dWu âˆ’1
2 âˆ«
t
0
ğœ(u, T)2 du
so we have
F(t, T) = F(0, T)eâˆ’1
2 âˆ«t
0 ğœ(u,T)2du+âˆ«t
0 ğœ(u,T)dWu.
By setting T = t, the spot price St = F(t, t) can be expressed as
St = F(0, t)eâˆ’1
2 âˆ«t
0 ğœ(u,t)2du+âˆ«t
0 ğœ(u,t)dWu.

3.2.2
One-Dimensional Diffusion Process
141
Expanding using Taylorâ€™s theorem,
dSt = ğœ•St
ğœ•t dt + ğœ•St
ğœ•Wt
dWt + 1
2
ğœ•2St
ğœ•t2 dt2 + 1
2
ğœ•2St
ğœ•W2
t
dW2
t + ğœ•2St
ğœ•tğœ•Wt
dt dWt + . . .
and applying ItÂ¯oâ€™s lemma again,
dSt =
(
ğœ•St
ğœ•t + 1
2
ğœ•2St
ğœ•W2
t
)
dt + ğœ•St
ğœ•Wt
dWt.
From the spot price equation we have
ğœ•St
ğœ•t = ğœ•F(0, t)
ğœ•t
F(0, t)âˆ’1St âˆ’1
2
[
ğœ(t, t)2 +âˆ«
t
0
2ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du âˆ’2âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu
]
St
=
[ğœ•log F(0, t)
ğœ•t
âˆ’1
2ğœ(t, t)2 âˆ’âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du + âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu
]
St,
ğœ•St
ğœ•Wt
=
ğœ•
ğœ•Wt
[
âˆ«
t
0
ğœ(u, t) dWu
]
St = ğœ(t, t)St
and
ğœ•2St
ğœ•W2
t
= ğœ(t, t)2St.
By substituting the values of ğœ•St
ğœ•t , ğœ•St
ğœ•Wt
and ğœ•2St
ğœ•W2
t
into dSt =
(
ğœ•St
ğœ•t + 1
2
ğœ•2St
ğœ•W2
t
)
dt +
ğœ•St
ğœ•Wt
dWt, we finally have
dSt
St
=
[ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du + âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu
]
dt + ğœ(t, t) dWt.
â—½
17. Clewlowâ€“Strickland 1-Factor Model. Let {Wt âˆ¶t â‰¥0} be the standard Wiener process on
the probability space (Î©, â„±, â„™). Suppose the forward curve F(t, T) follows the process
dF(t, T)
F(t, T) = ğœeâˆ’ğ›¼(Tâˆ’t)dWt
where t â‰¤T, ğ›¼is the mean-reversion parameter and ğœis the volatility. From the
forwardâ€“spot price relationship F(t, T) = ğ”¼(ST|| â„±t
) where ST is the spot price at time
T, show that
dSt
St
=
[ğœ•log F(0, t)
ğœ•t
+ ğ›¼(log F(0, t) âˆ’log St
) + ğœ2
4
(1 âˆ’eâˆ’2ğ›¼t)]
dt + ğœdWt

142
3.2.2
One-Dimensional Diffusion Process
and the forward curve at time t is given by
F(t, T) = F(0, T)
[
St
F(0, t)
]eâˆ’ğ›¼(Tâˆ’t)
eâˆ’ğœ2
4ğ›¼eâˆ’ğ›¼T(e2ğ›¼tâˆ’1)(eâˆ’ğ›¼tâˆ’eâˆ’ğ›¼T).
Finally, show that conditional on F(0, T), F(t, T) follows a lognormal distribution with
mean
ğ”¼[F(t, T)| F(0, T)] = F(0, T)
and variance
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
ğœ2
2ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T] âˆ’1
}
.
Solution: For the case when
dF(t, T)
F(t, T) = ğœ(t, T) dWt
with ğœ(t, T) a time-dependent volatility, from Problem 3.2.2.16 (page 140) the correspond-
ing spot price SDE is given by
dSt
St
=
[ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du + âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu
]
dt + ğœ(t, t) dWt.
Let ğœ(t, T) = ğœeâˆ’ğ›¼(Tâˆ’t) and taking partial differentiation with respect to T, we have
ğœ•ğœ(t, T)
ğœ•T
= âˆ’ğ›¼ğœeâˆ’ğ›¼(Tâˆ’t).
Thus,
âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du = âˆ’âˆ«
t
0
ğ›¼ğœ2eâˆ’2ğ›¼(tâˆ’u) du = âˆ’ğœ2
2
(1 âˆ’eâˆ’2ğ›¼t)
and
âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu = âˆ’âˆ«
t
0
ğ›¼ğœeâˆ’ğ›¼(tâˆ’u) dWu.
Using ItÂ¯oâ€™s lemma,
d log F(t, T) =
1
F(t, T)dF(t, T) âˆ’
1
2F(t, T)2 dF(t, T)2 + . . .
= ğœeâˆ’ğ›¼(Tâˆ’t)dWt âˆ’1
2ğœ2eâˆ’2ğ›¼(Tâˆ’t)dt

3.2.2
One-Dimensional Diffusion Process
143
and taking integrals,
log F(t, T) = log F(0, T) âˆ’1
2 âˆ«
t
0
ğœ2eâˆ’2ğ›¼(Tâˆ’u) du + âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu.
By setting T = t such that F(t, t) = St, we can write
log St = log F(0, t) âˆ’1
2 âˆ«
t
0
ğœ2eâˆ’2ğ›¼(tâˆ’u) du + âˆ«
t
0
ğœeâˆ’ğ›¼(tâˆ’u) dWu
therefore
âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu = ğ›¼(log F(0, t) âˆ’log St
) âˆ’1
2 âˆ«
t
0
ğ›¼ğœ2eâˆ’2ğ›¼(tâˆ’u) du
= ğ›¼(log F(0, t) âˆ’log St
) âˆ’ğœ2
4
(1 âˆ’eâˆ’2ğ›¼t) .
By substituting the values of âˆ«
t
0
ğœ(u, t)ğœ•ğœ(u, t)
ğœ•t
du and âˆ«
t
0
ğœ•ğœ(u, t)
ğœ•t
dWu into the spot
price SDE and taking note that ğœ(t, t) = ğœ, we eventually have
dSt
St
=
[ğœ•log F(0, t)
ğœ•t
+ ğ›¼(log F(0, t) âˆ’log St
) + ğœ2
4
(1 âˆ’eâˆ’2ğ›¼t)]
dt + ğœdWt.
Since
F(t, T) = F(0, T)eâˆ’1
2 âˆ«t
0 ğœ2eâˆ’2ğ›¼(Tâˆ’u)du+âˆ«t
0 ğœeâˆ’ğ›¼(Tâˆ’u)dWu
with
âˆ«
t
0
ğœ2eâˆ’2ğ›¼(Tâˆ’u) du = ğœ2
2ğ›¼eâˆ’2ğ›¼T (e2ğ›¼t âˆ’1)
and using the spot price equation
âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu = eâˆ’ğ›¼(Tâˆ’t)
âˆ«
t
0
ğœeâˆ’ğ›¼(tâˆ’u) dWu
= eâˆ’ğ›¼(Tâˆ’t)
{
log
[
St
F(0, t)
]
+ 1
2 âˆ«
t
0
ğœ2eâˆ’2ğ›¼(tâˆ’u) du
}
= eâˆ’ğ›¼(Tâˆ’t)
{
log
[
St
F(0, t)
]
+ ğœ2
4ğ›¼
(1 âˆ’eâˆ’2ğ›¼t)}
therefore
F(t, T) = F(0, T)
[
St
F(0, t)
]eâˆ’ğ›¼(Tâˆ’t)
eâˆ’ğœ2
4ğ›¼eâˆ’ğ›¼T(e2ğ›¼tâˆ’1)(eâˆ’ğ›¼tâˆ’eâˆ’ğ›¼T).
Finally, using
d log F(t, T) = ğœeâˆ’ğ›¼(Tâˆ’t)dWt âˆ’1
2ğœ2eâˆ’2ğ›¼(Tâˆ’t)dt

144
3.2.2
One-Dimensional Diffusion Process
and taking integrals we have
log F(t, T) = log F(0, T) + âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu âˆ’ğœ2
4ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T] .
From the property of the ItÂ¯o integral,
ğ”¼
[
âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu
]
= 0
and
ğ”¼
[(
âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu
)2]
= ğ”¼
[
âˆ«
t
0
ğœ2eâˆ’2ğ›¼(Tâˆ’u) du
]
= ğœ2
2ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T] .
Since âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu is in the form âˆ«
t
0
f(u) dWu, from Problem 3.2.2.4 (page 126)
we can easily show that âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu follows a normal distribution.
In addition, since we can also write the ItÂ¯o integral as
âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
ğœeâˆ’ğ›¼(Tâˆ’ti)(Wti+1 âˆ’Wti)
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•and due to the stationary
increment of a standard Wiener process, each term of Wti+1 âˆ’Wti âˆ¼ğ’©
(
0, t
n
)
is normally
distributed multiplied by a deterministic term and we can deduce that
âˆ«
t
0
ğœeâˆ’ğ›¼(Tâˆ’u) dWu âˆ¼ğ’©
(
0, ğœ2
2ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T])
.
Hence,
log F(t, T) âˆ¼ğ’©
(
log F(0, T) âˆ’ğœ2
4ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T] , ğœ2
2ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T])
which implies
ğ”¼[F(t, T)| F(0, T)] = F(0, T)
and
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
ğœ2
2ğ›¼
[eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T] âˆ’1
}
.
â—½

3.2.2
One-Dimensional Diffusion Process
145
18. Constant Elasticity of Variance Model. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶
t â‰¥0} be a standard Wiener process with respect to the filtration â„±t, t â‰¥0. Suppose St > 0
follows a constant elasticity of variance (CEV) model of the form
dSt = rStdt + ğœ(St, t)StdWt
where r is a constant and ğœ(St, t) is a local volatility function. By setting ğœ(St, t) = ğ›¼Sğ›½âˆ’1
t
with ğ›¼> 0 and 0 < ğ›½< 1, show using ItÂ¯oâ€™s formula that ğœ(St, t) satisfies
dğœ(St, t)
ğœ(St, t) = (ğ›½âˆ’1)
{[
r + 1
2(ğ›½âˆ’2)ğœ(St, t)2]
dt + ğœ(St, t) dWt
}
.
Finally, conditional on St show that for t < T,
ST = erT
[
eâˆ’r(1âˆ’ğ›½)tS1âˆ’ğ›½
t
+ ğ›¼âˆ«
T
t
eâˆ’r(1âˆ’ğ›½)u dWu
]
1
1âˆ’ğ›½
.
Solution: From ItÂ¯oâ€™s formula,
dğœ(St, t) = ğœ•ğœ(St, t)
ğœ•St
dSt + 1
2
ğœ•2ğœ(St, t)
ğœ•S2
t
(dSt)2 + . . .
= ğ›¼(ğ›½âˆ’1)Sğ›½âˆ’2
t
dSt + 1
2ğ›¼(ğ›½âˆ’1)(ğ›½âˆ’2)Sğ›½âˆ’3
t
(dSt)2 + . . .
= ğ›¼(ğ›½âˆ’1)Sğ›½âˆ’2
t
(rSt dt + ğ›¼Sğ›½
t dWt) + 1
2ğ›¼(ğ›½âˆ’1)(ğ›½âˆ’2)(ğ›¼2S2ğ›½
t dt)
= (ğ›½âˆ’1)
[(
rğœ(St, t) + 1
2(ğ›½âˆ’2)ğœ(St, t)3)
dt + ğœ(St, t)2dWt
]
.
Therefore,
dğœ(St, t)
ğœ(St, t) = (ğ›½âˆ’1)
{[
r + 1
2(ğ›½âˆ’2)ğœ(St, t)2]
dt + ğœ(St, t) dWt
}
.
To find the solution of the CEV model, let Xt = eâˆ’rtSt and by ItÂ¯oâ€™s formula
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•St
dSt + 1
2
ğœ•2Xt
ğœ•S2
t
(dSt)2 + . . .
= âˆ’reâˆ’rtStdt + eâˆ’rt (
rStdt + ğ›¼Sğ›½
t dWt
)
= ğ›¼eâˆ’rtSğ›½
t dWt
= ğ›¼eâˆ’r(1âˆ’ğ›½)tXğ›½
t dWt.
Taking integrals
âˆ«
T
t
dXu
Xğ›½
u
= ğ›¼âˆ«
T
t
eâˆ’r(1âˆ’ğ›½)u dWu

146
3.2.2
One-Dimensional Diffusion Process
we have
X1âˆ’ğ›½
T
= X1âˆ’ğ›½
t
+ ğ›¼âˆ«
T
t
eâˆ’r(1âˆ’ğ›½)u dWu
XT =
[
X1âˆ’ğ›½
t
+ ğ›¼âˆ«
T
t
eâˆ’r(1âˆ’ğ›½)u dWu
]
1
1âˆ’ğ›½
or
ST = erT
[
eâˆ’r(1âˆ’ğ›½)tS1âˆ’ğ›½
t
+ ğ›¼âˆ«
T
t
eâˆ’r(1âˆ’ğ›½)u dWu
]
1
1âˆ’ğ›½
.
â—½
19. Geometric Average. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0} be a standard
Wiener process with respect to the filtration â„±t, t â‰¥0. Suppose St satisfies the following
geometric Brownian motion model:
dSt
St
= ğœ‡dt + ğœdWt,
where ğœ‡and ğœare constant parameters. By considering a geometric average of St
Gt = e
1
t âˆ«t
0 Sudu,
G0 = S0
show that Gt satisfies the following SDE:
dGt
Gt
= 1
2
(
ğœ‡âˆ’1
6ğœ2)
dt + ğœ
âˆš
3
d ÌƒWt
where ÌƒWt =
âˆš
3
t
âˆ«
t
0
Wu du.
Under what condition is this SDE valid?
Solution: Using the steps described in Problem 3.2.2.7 (page 129) for u < t,
Su = S0e(ğœ‡âˆ’1
2 ğœ2)u+ğœWu
or
log Su = log S0 +
(
ğœ‡âˆ’1
2ğœ2)
u + ğœWu.
Taking the natural logarithm of Gt,
log Gt = 1
t âˆ«
t
0
log Su du
= 1
t âˆ«
t
0
[
log S0 +
(
ğœ‡âˆ’1
2ğœ2)
u + ğœWu
]
du

3.2.2
One-Dimensional Diffusion Process
147
= 1
t âˆ«
t
0
log S0 du + 1
t âˆ«
t
0
(
ğœ‡âˆ’1
2ğœ2)
u du + ğœ
t âˆ«
t
0
Wu du
= log S0 + 1
2
(
ğœ‡âˆ’1
2ğœ2)
t + ğœ
t âˆ«
t
0
Wu du.
From Problem 3.2.1.13 (page 115), using integration by parts we can write
âˆ«
t
0
Wu du = âˆ«
t
0
(t âˆ’u) dWu
and we can deduce that âˆ«
t
0
Wu du âˆ¼ğ’©
(
0, 1
3t3)
and hence
ğœ
t âˆ«
t
0
Wu du âˆ¼ğ’©
(
0, 1
3ğœ2t
)
or
ğœ
âˆš
3
ÌƒWt âˆ¼ğ’©
(
0, 1
3ğœ2t
)
where ÌƒWt =
âˆš
3
t
âˆ«
t
0
Wu du âˆ¼ğ’©(0, t).
Thus,
log Gt = log S0 + 1
2
(
ğœ‡âˆ’1
2ğœ2)
t + ğœ
âˆš
3
ÌƒWt
or
dGt
Gt
= 1
2
(
ğœ‡âˆ’1
6ğœ2)
dt + ğœ
âˆš
3
d ÌƒWt
with G0 = S0.
However, given that ÌƒWt does not have the stationary increment property (see Problem
3.2.1.13, page 115), this SDE is only valid if the geometric average starts at time t = 0.
â—½
20. Feynmanâ€“Kac Formula for One-Dimensional Diffusion Process. We consider the follow-
ing PDE problem:
ğœ•V
ğœ•t + 1
2ğœ(St, t)2 ğœ•2V
ğœ•S2
t
+ ğœ‡(St, t) ğœ•V
ğœ•St
âˆ’r(t)V(St, t) = 0
with boundary condition V(ST, T) = Î¨(ST) where ğœ‡, ğœare known functions of St and t,
r and Î¨ are functions of t and ST, respectively with t < T. Using ItÂ¯oâ€™s formula on the
process
Zu = eâˆ’âˆ«u
t r(ğ‘£)dğ‘£V(Su, u)
where St satisfies the generalised SDE
dSt = ğœ‡(St, t) dt + ğœ(St, t) dWt

148
3.2.2
One-Dimensional Diffusion Process
such that {Wt âˆ¶t â‰¥0} is a standard Wiener process on the probability space (Î©, â„±, â„™),
show that under the filtration â„±t the solution of the PDE is given by
V(St, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£Î¨(ST)||||
â„±t
]
.
Solution: Let g(u) = eâˆ’âˆ«u
t r(ğ‘£)dğ‘£and hence we can write
Zu = g(u)V(Su, u).
By applying Taylorâ€™s expansion and ItÂ¯oâ€™s formula on dZu we have
dZu = ğœ•Zu
ğœ•u du + ğœ•Zu
ğœ•Su
dSu + 1
2
ğœ•2Zu
ğœ•S2
u
(dSu)2 + . . .
=
(
g(u)ğœ•V
ğœ•u + V(Su, u)ğœ•g
ğœ•u
)
du +
(
g(u) ğœ•V
ğœ•Su
)
dSu + 1
2
(
g(u)ğœ•2V
ğœ•S2
u
)
(dSu)2
=
(
g(u)ğœ•V
ğœ•u âˆ’r(u)g(u)V(Su, u)
)
du +
(
g(u) ğœ•V
ğœ•Su
) (ğœ‡(Su, u) du + ğœ(Su, u)dWu
)
+1
2
(
g(u)ğœ•2V
ğœ•S2
u
)
(ğœ(Su, u)2du)
= g(u)
(
ğœ•V
ğœ•u + 1
2ğœ(Su, u)2 ğœ•2V
ğœ•S2
u
+ ğœ‡(Su, u) ğœ•V
ğœ•Su
âˆ’r(u)V(Su, u)
)
du
+g(u)ğœ(Su, u) ğœ•V
ğœ•Su
dWu
= g(u)ğœ(Su, u) ğœ•V
ğœ•Su
dWu
since
ğœ•V
ğœ•t + 1
2ğœ(Su, u)2 ğœ•2V
ğœ•S2
u
+ ğœ‡(Su, u) ğœ•V
ğœ•Su
âˆ’r(u)V(Su, u) = 0.
Taking integrals we have
âˆ«
T
t
dZu = âˆ«
T
t
g(u)ğœ(Su, u) ğœ•V
ğœ•Su
dWu
ZT âˆ’Zt = âˆ«
T
t
eâˆ’âˆ«u
t r(ğ‘£)dğ‘£ğœ(Su, u) ğœ•V
ğœ•Su
dWu.
By taking expectations and using the properties of the ItÂ¯o integral,
ğ”¼(ZT âˆ’Zt
) = 0 or ğ”¼(Zt
) = ğ”¼(ZT
)

3.2.2
One-Dimensional Diffusion Process
149
and hence under the filtration â„±t,
ğ”¼(Zt|â„±t
) = ğ”¼(ZT|â„±t)
ğ”¼
[
eâˆ’âˆ«t
t r(ğ‘£)dğ‘£V(St, t)||| â„±t
]
= ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£V(ST, T)||||
â„±t
]
V(St, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£Î¨(ST)
||||
â„±t
]
.
â—½
21. Backward Kolmogorov Equation for One-Dimensional Diffusion Process. Let {Wt âˆ¶t â‰¥
0} be a standard Wiener process on the probability space (Î©, â„±, â„™). For t âˆˆ[0, T], T > 0
consider the generalised stochastic differential equation
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
where ğœ‡(Xt, t) and ğœ(Xt, t) are functions dependent on Xt and t. By conditioning Xt = x
and XT = y, let p(x, t; y, T) be the transition probability density of XT at time T starting at
time t at point Xt. For any function Î¨(XT), from the Feynmanâ€“Kac formula the function
f(x, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(u)du Î¨(XT)
||||
â„±t
]
= eâˆ’âˆ«T
t r(u)du
âˆ«Î¨(y)p(x, t; y, T) dy
satisfies the partial differential equation
ğœ•f
ğœ•t + 1
2ğœ(x, t)2 ğœ•2f
ğœ•x2 + ğœ‡(x, t)ğœ•f
ğœ•x âˆ’r(t)f(x, t) = 0
where r is a time-dependent function.
Show that the transition probability density p(t, x; T, y) in the y variable satisfies
ğœ•
ğœ•tp(x, t; y, T) + 1
2ğœ(x, t)2 ğœ•2
ğœ•x2 p(x, t; y, T) + ğœ‡(x, t) ğœ•
ğœ•xp(x, t; y, T) = 0.
Solution: From the Feynmanâ€“Kac formula, for any function Î¨(XT), the function
f(x, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(u)du Î¨(XT)||||
â„±t
]
= eâˆ’âˆ«T
t r(u)du
âˆ«Î¨(y)p(x, t; y, T) dy
satisfies the following PDE:
ğœ•f
ğœ•t + 1
2ğœ(x, t)2 ğœ•2f
ğœ•x2 + ğœ‡(x, t)ğœ•f
ğœ•x âˆ’r(t)f(x, t) = 0.

150
3.2.2
One-Dimensional Diffusion Process
By differentiation, we have
ğœ•f
ğœ•t = r(t)eâˆ’âˆ«T
t
r(u)du
âˆ«Î¨(y)p(x, t; y, T) dy + eâˆ’âˆ«T
t
r(u)du
âˆ«Î¨(y) ğœ•
ğœ•tp(x, t; y, T) dy
= r(t)f(x, t) + eâˆ’âˆ«T
t
r(u)du
âˆ«Î¨(y) ğœ•
ğœ•tp(x, t; y, T) dy
ğœ•f
ğœ•x = eâˆ’âˆ«T
t
r(u)du
âˆ«Î¨(y) ğœ•
ğœ•xp(x, t; y, T) dy and
ğœ•2f
ğœ•x2 = eâˆ’âˆ«T
t
r(u)du
âˆ«Î¨(y) ğœ•2
ğœ•x2 p(x, t; y, T) dy.
Substituting the above equations into the PDE, we obtain
eâˆ’âˆ«T
t
r(u)du Ã—
âˆ«Î¨(y)
[
ğœ•
ğœ•tp(x, t; y, T) + 1
2ğœ(x, t)2 ğœ•2
ğœ•x2 p(x, t; y, T) + ğœ‡(x, t) ğœ•
ğœ•xp(x, t; y, T)
]
dy = 0.
Finally, irrespective of the choice of Î¨(y) and r(t), the transition probability density func-
tion p(x, t; y, T) satisfies
ğœ•
ğœ•tp(x, t; y, T) + 1
2ğœ(x, t)2 ğœ•2
ğœ•x2 p(x, t; y, T) + ğœ‡(x, t) ğœ•
ğœ•xp(x, t; y, T) = 0.
â—½
22. Forward Kolmogorov Equation for One-Dimensional Diffusion Process. Let {Wt âˆ¶t â‰¥0}
be a standard Wiener process on the probability space (Î©, â„±, â„™). For t âˆˆ[0, T], T > 0
consider the generalised stochastic differential equation
dXt = ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
where ğœ‡(Xt, t) and ğœ(Xt, t) are functions dependent on Xt and t. Using ItÂ¯oâ€™s formula on the
function f(Xt, t), show that
f(XT, T) = f(Xt, t) + âˆ«
T
t
(
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•X2
t
(Xs, s)
)
ds
+ âˆ«
T
t
ğœ(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) dWs
and taking the expectation conditional on Xt = x, show that in the limit T â†’t
âˆ«
T
t
ğ”¼
[
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•Xt
2 (Xs, s)
|||||
Xt = x
]
ds = 0.

3.2.2
One-Dimensional Diffusion Process
151
Let XT = y and define the transition probability density p(x, t; y, T) of XT at time T starting
at time t at point Xt. By writing the conditional expectation in terms of p(x, t; y, T) and
integrating by parts twice, show that
ğœ•
ğœ•T p(x, t; y, T) = 1
2
ğœ•2
ğœ•y2 (ğœ(y, T)2p(x, t; y, T)) âˆ’ğœ•
ğœ•y(ğœ‡(y, T)p(x, t; y, T)).
Solution: For a suitable function f(Xt, t) and using ItÂ¯oâ€™s formula,
df(Xt, t) = ğœ•f
ğœ•t (Xt, t) dt + ğœ•f
ğœ•Xt
(Xt, t) dXt + 1
2
ğœ•2f
ğœ•X2
t
(Xt, t)(dXt)2 + . . .
= ğœ•f
ğœ•t (Xt, t) dt + ğœ•f
ğœ•Xt
(Xt, t) (ğœ‡(Xt, t) dt + ğœ(Xt, t) dWt
) + 1
2ğœ(Xt, t)2 ğœ•2f
ğœ•X2
t
(Xt, t) dt
=
(
ğœ•f
ğœ•t (Xt, t) + ğœ‡(Xt, t) ğœ•f
ğœ•Xt
(Xt, t) + 1
2ğœ(Xt, t)2 ğœ•2f
ğœ•X2
t
(Xt, t)
)
dt
+ ğœ(Xt, t) ğœ•f
ğœ•Xt
(Xt, t) dWt.
Taking integrals,
âˆ«
T
t
df(Xs, s) = âˆ«
T
t
(
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•X2
t
(Xs, s)
)
ds
+ âˆ«
T
t
ğœ(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) dWs
f(XT, T) = f(Xt, t)
+ âˆ«
T
t
(
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•X2
t
(Xs, s)
)
ds
+ âˆ«
T
t
ğœ(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) dWs.
By taking conditional expectations given Xt = x we have
ğ”¼(f(XT, T)|| Xt = x) = ğ”¼(f(Xt, t)|| Xt = x)
+ âˆ«
T
t
ğ”¼
(
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•X2
t
(Xs, s)
|||||
Xt = x
)
ds
and since in the limit
lim
Tâ†’t ğ”¼(f(XT, T)|| Xt = x) = ğ”¼(f(Xt, t)|| Xt = x)

152
3.2.2
One-Dimensional Diffusion Process
so
âˆ«
T
t
ğ”¼
[
ğœ•f
ğœ•t (Xs, s) + ğœ‡(Xs, s) ğœ•f
ğœ•Xt
(Xs, s) + 1
2ğœ(Xs, s)2 ğœ•2f
ğœ•X2
t
(Xs, s)
|||||
Xt = x
]
ds = 0
or
âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
[ğœ•f
ğœ•s(y, s) + ğœ‡(y, s)ğœ•f
ğœ•y(y, s) + 1
2ğœ(y, s)2 ğœ•2f
ğœ•y2 (y, s)
]
p(x, t; y, s) dyds = 0
where p(x, t; y, s) is the transition probability density function of Xt in the y-variable.
Integrating by parts,
âˆ«
T
t âˆ«
âˆ
âˆ’âˆ
ğœ•f
ğœ•s(y, s)p(x, t; y, s) dyds
= âˆ«
âˆ
âˆ’âˆâˆ«
T
t
ğœ•f
ğœ•s(y, s)p(x, t; y, s) dsdy
=
âˆ«
âˆ
âˆ’âˆ
f(y, s)p(x, t; y, s)||||
T
t
dy
âˆ’âˆ«
âˆ
âˆ’âˆâˆ«
T
t
f(y, s) ğœ•
ğœ•sp(x, t; y, s) dsdy
=
âˆ’âˆ«
âˆ
âˆ’âˆâˆ«
T
t
f(y, s) ğœ•
ğœ•sp(x, t; y, s) dsdy
=
âˆ’âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
f(y, s) ğœ•
ğœ•sp(x, t; y, s) dyds
âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
ğœ‡(y, s)ğœ•f
ğœ•y(y, s)p(x, t; y, s) dyds
=
âˆ«
T
t
f(y, s)ğœ‡(y, s)p(x, t; y, s)
|||||
âˆ
âˆ’âˆ
ds
âˆ’âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
f(y, s) ğœ•
ğœ•y(ğœ‡(y, s)p(x, t; y, s)) dyds
= âˆ’âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
f(y, s) ğœ•
ğœ•y(ğœ‡(y, s)p(x, t; y, s)) dyds
âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
1
2 ğœ(y, s)2 ğœ•2f
ğœ•y2 (y, s)p(x, t; y, s) dyds
= âˆ«
T
t
1
2
ğœ•
ğœ•yf(y, s)ğœ(y, s)2p(x, t; y, s)
|||||
âˆ
âˆ’âˆ
ds
âˆ’âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
1
2
ğœ•
ğœ•yf(y, s) ğœ•
ğœ•y(ğœ(y, s)2p(x, t; y, s)) dyds
= âˆ’âˆ«
T
t âˆ«
âˆ
âˆ’âˆ
1
2
ğœ•
ğœ•yf(y, s) ğœ•
ğœ•y(ğœ(y, s)2p(x, t; y, s)) dyds
= âˆ’âˆ«
T
t
1
2f(y, s) ğœ•
ğœ•y
(ğœ(y, s)2p(x, t; y, s))|||||
âˆ
âˆ’âˆ
ds

3.2.2
One-Dimensional Diffusion Process
153
+ âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
1
2f(y, s) ğœ•2
ğœ•y2 (ğœ(y, s)2p(x, t; y, s)) dyds
= âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
1
2f(y, s) ğœ•2
ğœ•y2 (ğœ(y, s)2p(x, t; y, s)) dyds
and by substituting the above relationships into the integro partial differential equation,
we have
âˆ«
T
t
âˆ«
âˆ
âˆ’âˆ
f(y, s)Ã—
[
ğœ•
ğœ•sp(x, t; y, s) + ğœ•
ğœ•y (ğœ‡(y, s) p (x, t; y, s)) âˆ’1
2
ğœ•2
ğœ•y2
(ğœ(y, s)2p(x, t; y, s))]
dyds = 0.
Finally, by differentiating the above equation with respect to T, we obtain
âˆ«
âˆ
âˆ’âˆ
f(y, T)Ã—
[
ğœ•
ğœ•T p(x, t; y, T) + ğœ•
ğœ•y (ğœ‡(y, T)p(x, t; y, T)) âˆ’1
2
ğœ•2
ğœ•y2
(ğœ(y, T)2p(x, t; y, T))]
dy = 0
and irrespective of the choice of f, the transition probability density function p(x, t; y, T)
satisfies
ğœ•
ğœ•T p(x, t; y, T) = 1
2
ğœ•2
ğœ•y2
(ğœ(y, T)2p(x, t; y, T)) âˆ’ğœ•
ğœ•y (ğœ‡(y, T)p(x, t; y, T)) .
â—½
23. Backward Kolmogorov Equation for a One-Dimensional Random Walk. We consider a
one-dimensional symmetric random walk where at initial time t0, a particle starts at x0 and
is at position x at time t. At time t + ğ›¿t, the particle can either move to x + ğ›¿x or x âˆ’ğ›¿x each
with probability 1
2. Let p(x, t; x0, t0) denote the probability density of the particle position
x at time t starting at x0 at time t0.
By writing the backward equation in a discrete fashion and expanding it using Taylorâ€™s
series, show that for ğ›¿x =
âˆš
ğ›¿t and in the limit ğ›¿t â†’0
ğœ•p(x, t; x0, t0)
ğœ•t
= âˆ’1
2
ğœ•2p(x, t; x0, t0)
ğœ•x2
.
Solution: By denoting p(x, t; x0, t0) as the probability density function of the particle posi-
tion x at time t, hence the discrete model of the backward equation is
p(x, t; x0, t0) = 1
2p(x âˆ’ğ›¿x, t + ğ›¿t; x0, t0) + 1
2p(x + ğ›¿x, t + ğ›¿t; x0, t0).
Using Taylorâ€™s series, we have
p(x âˆ’ğ›¿x, t + ğ›¿t; x0, t0) = p(x, t + ğ›¿t; x0, t0) âˆ’ğœ•p(x, t + ğ›¿t; x0, t0)
ğœ•x
ğ›¿x
+ 1
2
ğœ•2p(x, t + ğ›¿t; x0, t0)
ğœ•x2
(âˆ’ğ›¿x)2 + O((ğ›¿x)3)

154
3.2.2
One-Dimensional Diffusion Process
and
p(x + ğ›¿x, t + ğ›¿t; x0, t0) = p(x, t + ğ›¿t; x0, t0) + ğœ•p(x, t + ğ›¿t; x0, t0)
ğœ•x
ğ›¿x
+ 1
2
ğœ•2p(x, t + ğ›¿t; x0, t0)
ğœ•x2
(ğ›¿x)2 + O((ğ›¿x)3).
Substituting these two equations into the backward equation,
p(x, t; x0, t0) = p(x, t + ğ›¿t; x0, t0) + 1
2
ğœ•2p(x, t + ğ›¿t; x0, t0)
ğœ•x2
(ğ›¿x)2 + O((ğ›¿x)3).
By setting ğ›¿x =
âˆš
ğ›¿t, dividing the equation by ğ›¿t and taking limits ğ›¿t â†’0
âˆ’lim
ğ›¿tâ†’0
[p(x, t + ğ›¿t; x0, t0) âˆ’p(x, t; x0, t0)
ğ›¿t
]
= lim
ğ›¿tâ†’0
[
1
2
ğœ•2p(x, t + ğ›¿t; x0, t0)
ğœ•x2
+ O(
âˆš
ğ›¿t)
]
we finally have
ğœ•p(x, t; x0, t0)
ğœ•t
= âˆ’1
2
ğœ•2p(x, t; x0, t0)
ğœ•x2
.
â—½
24. Forward Kolmogorov Equation for a One-Dimensional Random Walk. We consider a
one-dimensional symmetric random walk where at initial time t0, a particle starts at y0
and is at position y at terminal time T > 0. At time T âˆ’ğ›¿T, the particle can either move to
y + ğ›¿y or y âˆ’ğ›¿y each with probability 1
2. Let p(y, T; y0, t0) denote the probability density
of the position y at time T starting at y0 at time t0.
By writing the forward equation in a discrete fashion and expanding it using Taylorâ€™s
series, show that for ğ›¿y =
âˆš
ğ›¿T and in the limit ğ›¿T â†’0
ğœ•p(y, T; y0, t0)
ğœ•T
= 1
2
ğœ•2p(y, T; y0, t0)
ğœ•y2
.
Solution: By denoting p(y, T; y0, t0) as the probability density function of the particle
position y at time T, hence the discrete model of the forward equation is
p(y, T; y0, t0) = 1
2p(y âˆ’ğ›¿y, T âˆ’ğ›¿T; y0, t0) + 1
2p(y + ğ›¿y, T âˆ’ğ›¿T; y0, t0).
By expanding p(y âˆ’ğ›¿y, T âˆ’ğ›¿T; y0, t0) and p(y + ğ›¿y, T âˆ’ğ›¿T; y0, t0) using Taylorâ€™s series,
we have
p(y âˆ’ğ›¿y, T âˆ’ğ›¿T; y0, t0) = p(y, T âˆ’ğ›¿T; y0, t0) âˆ’ğœ•p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y
ğ›¿y
+ 1
2
ğœ•2p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y2
(âˆ’ğ›¿y)2 + O((ğ›¿y)3)

3.2.3
Multi-Dimensional Diffusion Process
155
and
p(y + ğ›¿y, T âˆ’ğ›¿T; y0, t0) = p(y, T âˆ’ğ›¿T; y0, t0) + ğœ•p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y
ğ›¿y
+ 1
2
ğœ•2p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y2
(ğ›¿y)2 + O((ğ›¿y)3).
Substituting the above two equations into the discrete forward equation,
p(y, T; y0, t0) = p(y, T âˆ’ğ›¿T; y0, t0) + 1
2
ğœ•2p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y2
(ğ›¿y)2 + O((ğ›¿y)3).
By setting ğ›¿y =
âˆš
ğ›¿T, dividing the equation by ğ›¿T and taking limits ğ›¿T â†’0
lim
ğ›¿Tâ†’0
[p(y, T; y0, t0) âˆ’p(y, T âˆ’ğ›¿T; y0, t0)
ğ›¿T
]
= lim
ğ›¿Tâ†’0
[
1
2
ğœ•2p(y, T âˆ’ğ›¿T; y0, t0)
ğœ•y2
+ O(
âˆš
ğ›¿T)
]
we finally have
ğœ•p(y, T; y0, t0)
ğœ•T
= 1
2
ğœ•2p(y, T; y0, t0)
ğœ•y2
.
â—½
3.2.3
Multi-Dimensional Diffusion Process
1. Let (Î©, â„±, â„™) be a probability space and consider n assets with prices S(i)
t , i = 1, 2, . . . , n
satisfying the SDEs
dS(i)
t
= ğœ‡(i)S(i)
t dt + ğœ(i)S(i)
t dW(i)
t
(
dW(i)
t
) (
dW(j)
t
)
= ğœŒ(ij)dt
where {W(i)
t
âˆ¶t â‰¥0}, i = 1, 2, . . . , n are standard Wiener processes, ğœŒ(ij) âˆˆ(âˆ’1, 1), i â‰ j
and ğœŒ(ii) = 1.
By considering the function f(S(1)
t , S(2)
t , . . . , S(n)
t ), show using ItÂ¯oâ€™s formula that
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
âˆ‘
i=1
ğœ‡(i)S(i)
t
ğœ•f
ğœ•S(i)
t
dt + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœŒ(ij)ğœ(i)ğœ(j)S(i)
t S(j)
t
ğœ•2f
ğœ•S(i)
t ğœ•S(j)
t
dt
+
n
âˆ‘
i=1
ğœ(i)S(i)
t
ğœ•f
ğœ•S(i)
t
dW(i)
t .
Solution: By expanding df(S(1)
t , S(2)
t , . . . , S(n)
t ) using Taylorâ€™s formula,
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
âˆ‘
i=1
ğœ•f
ğœ•S(i)
t
dS(i)
t + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœ•2f
ğœ•S(i)
t S(j)
t
dS(i)
t dS(j)
t + . . .

156
3.2.3
Multi-Dimensional Diffusion Process
=
n
âˆ‘
i=1
ğœ•f
ğœ•S(i)
t
(
ğœ‡(i)S(i)
t dt + ğœ(i)S(i)
t dW(i)
t
)
+1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœ•2f
ğœ•S(i)
t S(j)
t
(
ğœ‡(i)S(i)
t dt + ğœ(i)S(i)
t dW(i)
t
) (
ğœ‡(j)S(j)
t dt + ğœ(j)S(j)
t dW(j)
t
)
+ . . .
By setting (dt)2 = 0, (dW(i)
t )2 = dt, (dW(i)
t )(dW(j)
t ) = ğœŒ(ij)dt, dW(i)
t dt = 0, i, j = 1, 2, . . . , n
we have
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
âˆ‘
i=1
ğœ‡(i)S(i)
t
ğœ•f
ğœ•S(i)
t
dt + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœŒ(ij)ğœ(i)ğœ(j)S(i)
t S(j)
t
ğœ•2f
ğœ•S(i)
t ğœ•S(j)
t
dt
+
n
âˆ‘
i=1
ğœ(i)S(i)
t
ğœ•f
ğœ•S(i)
t
dW(i)
t .
â—½
2. Let (Î©, â„±, â„™) be a probability space. We consider two assets with prices S(1)
t , S(2)
t
satisfying
the SDEs
dS(1)
t
= ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
dS(2)
t
= ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
dW(1)
t dW(2)
t
= ğœŒdt
where ğœ‡(1), ğœ‡(2), ğœ(1), ğœ(2) are constants and {W(1)
t
âˆ¶t â‰¥0}, {W(2)
t
âˆ¶t â‰¥0} are standard
Wiener processes with correlation ğœŒ.
By letting Ut = S(1)
t âˆ•S(2)
t
show that the SDE satisfied by Ut is
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) âˆ’ğœ‡(2) + (ğœ(2))2 âˆ’ğœŒğœ(1)ğœ(2), ğœ=
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2) and
Vt =
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2) .
Finally, show that {Vt âˆ¶t â‰¥0} is a standard Wiener process.
Solution: By using Taylorâ€™s expansion and applying ItÂ¯oâ€™s formula,
dUt = d
(
S(1)
t âˆ•S(2)
t
)
= 1
S(2)
t
dS(1)
t
âˆ’
S(1)
t
(S(2)
t )2 dS(2)
t
+
S(1)
t
(S(2)
t )3 (dS(2)
t )2 âˆ’
1
(S(2)
t )2 dS(1)
t dS(2)
t
+ . . .

3.2.3
Multi-Dimensional Diffusion Process
157
= ğœ‡(1)Utdt + ğœ(1)UtdW(1)
t
âˆ’
(
ğœ‡(2)Utdt + ğœ(2)UtdW(2)
t
)
+(ğœ(2))2Utdt âˆ’ğœŒğœ(1)ğœ(2)Utdt
= (ğœ‡(1) âˆ’ğœ‡(2) + (ğœ(2))2 âˆ’ğœŒğœ(1)ğœ(2)) Utdt + Ut
(
ğœ(1)dW(1)
t
âˆ’ğœ(2)dW(2)
t
)
= (ğœ‡(1) âˆ’ğœ‡(2) + (ğœ(2))2 âˆ’ğœŒğœ(1)ğœ(2)) Utdt +
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)UtdVt.
Therefore,
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) âˆ’ğœ‡(2) + (ğœ(2))2 âˆ’ğœŒğœ(1)ğœ(2), ğœ=
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2) and
Vt =
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2) .
To show that {Vt âˆ¶t â‰¥0} is a standard Wiener process, we first show that Vt âˆ¼ğ’©(0, t).
Taking expectations,
ğ”¼(Vt) =
ğœ(1)ğ”¼(W(1)
t ) âˆ’ğœ(2)ğ”¼(W(2)
t )
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2) = 0
ğ”¼(V2
t ) = ğ”¼
[
(ğœ(1))2(W(1)
t )2 + (ğœ(2))2(W(2)
t )2 âˆ’2ğœ(1)ğœ(2)W(1)
t W(2)
t
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
]
= (ğœ(1))2t + (ğœ(2))2t âˆ’2ğœŒğœ(1)ğœ(2)t
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
= t.
Given that W(1)
t
âˆ¼ğ’©(0, t), W(2)
t
âˆ¼ğ’©(0, t) and a linear combination of normal variates is
also normal, therefore Vt âˆ¼ğ’©(0, t).
To show that {Vt âˆ¶t â‰¥0} is a standard Wiener process, we note the following:
(a) V0 = 0 and it is clear that Vt has continuous sample paths for t â‰¥0.
(b) For t > 0, s > 0 we have
W(1)
t+s âˆ’W(1)
t
âˆ¼ğ’©(0, s),
W(2)
t+s âˆ’W(2)
t
âˆ¼ğ’©(0, s)
Cov
(
W(1)
t+s âˆ’W(1)
t , W(2)
t+s âˆ’W(2)
t
)
= ğœŒs
and hence
Vt+s âˆ’Vt =
ğœ(1) (
W(1)
t+s âˆ’W(1)
t
)
âˆ’ğœ(2) (
W(2)
t+s âˆ’W(2)
t
)
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)

158
3.2.3
Multi-Dimensional Diffusion Process
with mean
ğ”¼(Vt+s âˆ’Vt
) =
ğœ(1)ğ”¼
(
W(1)
t+s âˆ’W(1)
t
)
âˆ’ğœ(2)ğ”¼
(
W(2)
t+s âˆ’W(2)
t
)
âˆš
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
= 0
and variance
Var (Vt+s âˆ’Vt
) =
â¡
â¢
â¢â£
(ğœ(1))2Var
(
W(1)
t+s âˆ’W(1)
t
)
+ (ğœ(2))2Var
(
W(2)
t+s âˆ’W(2)
t
)
âˆ’2ğœ(1)ğœ(2)Cov
(
W(1)
t+s âˆ’W(1)
t , W(2)
t+s âˆ’W(2)
t
)
â¤
â¥
â¥â¦
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
= s.
Therefore, Vt+s âˆ’Vt âˆ¼ğ’©(0, s).
(c) For t > 0, s > 0, to show that Vt+s âˆ’Vt âŸ‚âŸ‚Vt we note that
ğ”¼[(Vt+s âˆ’Vt
) Vt
] = ğ”¼(Vt+sVt
) âˆ’ğ”¼(V2
t
)
= ğ”¼
â¡
â¢
â¢
â¢â£
(
ğœ(1)W(1)
t+s âˆ’ğœ(2)W(2)
t+s
) (
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
)
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
â¤
â¥
â¥
â¥â¦
âˆ’ğ”¼(V2
t
)
=
â¡
â¢
â¢â£
(ğœ(1))2ğ”¼
(
W(1)
t W(1)
t+s
)
âˆ’ğœ(1)ğœ(2)ğ”¼
(
W(1)
t+sW(2)
t
)
âˆ’ğœ(1)ğœ(2)ğ”¼
(
W(1)
t W(2)
t+s
)
+ (ğœ(2))2ğ”¼(W(2)
t W(2)
t+s)
â¤
â¥
â¥â¦
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
âˆ’t
=
[
(ğœ(1))2min {t, t + s} âˆ’ğœŒğœ(1)ğœ(2)min{t, t + s}
âˆ’ğœŒğœ(1)ğœ(2)min{t, t + s} + (ğœ(2))2min{t, t + s}
]
(ğœ(1))2 + (ğœ(2))2 âˆ’2ğœŒğœ(1)ğœ(2)
âˆ’t
= t âˆ’t
= 0.
Since Vt âˆ¼ğ’©(0, t), Vt+s âˆ’Vt âˆ¼ğ’©(0, s) and the joint distribution of Vt and Vt+s âˆ’Vt
is a bivariate normal (see Problem 2.2.1.5, page 58), if Cov (Vt+s âˆ’Vt, Vt
) = 0 then
Vt+s âˆ’Vt âŸ‚âŸ‚Vt.
From the results of (a)â€“(c) we have shown that Vt is a standard Wiener process.
â—½
3. Let (Î©, â„±, â„™) be a probability space. We consider two assets with prices S(1)
t , S(2)
t
satisfying
the SDEs
dS(1)
t
= ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
dS(2)
t
= ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
dW(1)
t dW(2)
t
= ğœŒdt

3.2.3
Multi-Dimensional Diffusion Process
159
where ğœ‡(1), ğœ‡(2), ğœ(1), ğœ(2) are constants and {W(1)
t
âˆ¶t â‰¥0}, {W(2)
t
âˆ¶t â‰¥0} are standard
Wiener processes with correlation ğœŒ.
By letting Ut = S(1)
t S(2)
t , show that the SDE satisfied by Ut is
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) + ğœ‡(2) + ğœŒğœ(1)ğœ(2), ğœ=
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2) and
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2) .
Show that {Vt âˆ¶t â‰¥0} is a standard Wiener process.
Solution: From ItÂ¯oâ€™s formula,
dUt = d(S(1)
t S(2)
t )
= S(1)
t dS(2)
t
+ S(2)
t dS(1)
t
+ (dS(1)
t )(dS(2)
t ) + . . .
= S(1)
t
(
ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
)
+ S(2)
t
(
ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
)
+ğœŒğœ(1)ğœ(2)S(1)
t S(2)
t dt
= (ğœ‡(1) + ğœ‡(2) + ğœŒğœ(1)ğœ(2)) Utdt + Ut
(
ğœ(1)dW(1)
t
+ ğœ(2)dW(2)
t
)
= (ğœ‡(1) + ğœ‡(2) + ğœŒğœ(1)ğœ(2)) Utdt +
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2)UtdVt.
Therefore,
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) + ğœ‡(2) + ğœŒğœ(1)ğœ(2), ğœ=
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2) and
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2) .
Following the same procedure as described in Problem 3.2.3.2 (page 156), we can show
that
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + 2ğœŒğœ(1)ğœ(2) âˆ¼N(0, t)
and is also a standard Wiener process.
â—½
4. Let (Î©, â„±, â„™) be a probability space. We consider three assets with prices S(1)
t , S(2)
t
and S(3)
t
satisfying the SDEs
dS(1)
t
= ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
dS(2)
t
= ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t

160
3.2.3
Multi-Dimensional Diffusion Process
dS(3)
t
= ğœ‡(3)S(3)
t dt + ğœ(3)S(3)
t dW(3)
t
dW(i)
t dW(j)
t
= ğœŒijdt,
i â‰ j
and
i, j = 1, 2, 3
where ğœ‡(1), ğœ‡(2), ğœ‡(3), ğœ(1), ğœ(2), ğœ(3) are constants and {W(1)
t
âˆ¶t â‰¥0}, {W(2)
t
âˆ¶t â‰¥0},
{W(3)
t
âˆ¶t â‰¥0} are standard Wiener processes with correlations dW(i)
t dW(j)
t
= ğœŒijdt, i â‰ j
and (dW(i)
t )2 = dt for i, j = 1, 2, 3.
By letting Ut = (S(1)
t S(2)
t )âˆ•S(3)
t , show that the SDE satisfied by Ut is
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) + ğœ‡(2) âˆ’ğœ‡(3) + ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) âˆ’ğœŒ23ğœ(2)ğœ(3), ğœ2 = (ğœ(1))2 +
(ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3) and
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3) .
Show that {Vt âˆ¶t â‰¥0} is a standard Wiener process.
Solution: From ItÂ¯oâ€™s lemma,
dUt = ğœ•Ut
ğœ•S(1)
t
dS(1)
t
+ ğœ•Ut
ğœ•S(2)
t
dS(2)
t
+ ğœ•Ut
ğœ•S(3)
t
dS(3)
t
+1
2
ğœ•2Ut
ğœ•(S(1)
t )2 (dS(1)
t )2 + 1
2
ğœ•2Ut
ğœ•(S(2)
t )2 (dS(2)
t )2 + 1
2
ğœ•2Ut
ğœ•(S(3)
t )2 (dS(3)
t )2
+
ğœ•2Ut
ğœ•S(1)
t ğœ•S(2)
t
dS(1)
t dS(2)
t
+
ğœ•2Ut
ğœ•S(1)
t ğœ•S(3)
t
dS(1)
t dS(3)
t
+
ğœ•2Ut
ğœ•S(2)
t ğœ•S(3)
t
dS(2)
t dS(3)
t
+ . . .
= S(2)
t
S(3)
t
(
ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
)
+ S(1)
t
S(3)
t
(
ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
)
âˆ’S(1)
t S(2)
t
(S(3)
t )2
(
ğœ‡(3)S(3)
t dt + ğœ(3)S(3)
t dW(3)
t
)
+ S(1)
t S(2)
t
(S(3)
t )3
(
(ğœ(3)S(3)
t )2dt
)
+ 1
S(3)
t
(
ğœŒ12ğœ(1)ğœ(2)S(1)
t S(2)
t dt
)
âˆ’
S(2)
t
(S(3)
t )2
(
ğœŒ13ğœ(1)ğœ(3)S(1)
t S(3)
t dt
)
âˆ’S(1)
t
(S(3)
t )2
(
ğœŒ23ğœ(2)ğœ(3)S(2)
t S(3)
t dt
)
= (ğœ‡(1) + ğœ‡(2) âˆ’ğœ‡(3) + ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) âˆ’ğœŒ23ğœ(2)ğœ(3)) Utdt
+Ut
(
ğœ(1)dW(1)
t
+ ğœ(2)dW(2)
t
âˆ’ğœ(3)dW(3)
t
)
= (ğœ‡(1) + ğœ‡(2) âˆ’ğœ‡(3) + ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) âˆ’ğœŒ23ğœ(2)ğœ(3)) Utdt

3.2.3
Multi-Dimensional Diffusion Process
161
+
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3)UtdVt
which therefore becomes
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) + ğœ‡(2) âˆ’ğœ‡(3) + ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) âˆ’ğœŒ23ğœ(2)ğœ(3), ğœ2 = (ğœ(1))2 +
(ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3) and
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3) .
Following the steps described in Problem 3.2.3.2 (page 156), we can easily show that
Vt =
ğœ(1)W(1)
t
+ ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 + 2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) âˆ’2ğœŒ23ğœ(2)ğœ(3) âˆ¼ğ’©(0, 1)
and is also a standard Wiener process.
â—½
5. Let (Î©, â„±, â„™) be a probability space. We consider three assets with prices S(1)
t , S(2)
t
and S(3)
t
satisfying the SDEs
dS(1)
t
= ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
dS(2)
t
= ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
dS(3)
t
= ğœ‡(3)S(3)
t dt + ğœ(3)S(3)
t dW(3)
t
dW(i)
t dW(j)
t
= ğœŒijdt,
i â‰ j
and
i, j = 1, 2, 3
where ğœ‡(1), ğœ‡(2), ğœ‡(3), ğœ(1), ğœ(2), ğœ(3) are constants and {W(1)
t
âˆ¶t â‰¥0}, {W(2)
t
âˆ¶t â‰¥0},
{W(3)
t
âˆ¶t â‰¥0} are standard Wiener processes with correlations dW(i)
t dW(j)
t
= ğœŒijdt, i â‰ j
and (dW(i)
t )2 = dt for i, j = 1, 2, 3.
By letting Ut = S(1)
t âˆ•(S(2)
t S(3)
t ), show that the SDE satisfied by Ut is
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) âˆ’ğœ‡(2) âˆ’ğœ‡(3) + (ğœ(2))2 + (ğœ(3))2 âˆ’ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) + ğœŒ23ğœ(2)ğœ(3),
ğœ2 = (ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3) and
Vt =
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3) .
Show that {Vt âˆ¶t â‰¥0} is a standard Wiener process.

162
3.2.3
Multi-Dimensional Diffusion Process
Solution: Applying ItÂ¯oâ€™s lemma,
dUt = ğœ•Ut
ğœ•S(1)
t
dS(1)
t
+ ğœ•Ut
ğœ•S(2)
t
dS(2)
t
+ ğœ•Ut
ğœ•S(3)
t
dS(3)
t
+1
2
ğœ•2Ut
ğœ•(S(1)
t )2 (dS(1)
t )2 + 1
2
ğœ•2Ut
ğœ•(S(2)
t )2 (dS(2)
t )2 + 1
2
ğœ•2Ut
ğœ•(S(3)
t )2 (dS(3)
t )2
+
ğœ•2Ut
ğœ•S(1)
t ğœ•S(2)
t
dS(1)
t dS(2)
t
+
ğœ•2Ut
ğœ•S(1)
t ğœ•S(3)
t
dS(1)
t dS(3)
t
+
ğœ•2Ut
ğœ•S(2)
t ğœ•S(3)
t
dS(2)
t dS(3)
t
+ . . .
=
1
S(2)
t S(3)
t
(
ğœ‡(1)S(1)
t dt + ğœ(1)S(1)
t dW(1)
t
)
âˆ’
S(1)
t
(S(2)
t )2S(3)
t
(
ğœ‡(2)S(2)
t dt + ğœ(2)S(2)
t dW(2)
t
)
âˆ’
S(1)
t
S(2)
t (S(3)
t )2
(
ğœ‡(3)S(3)
t dt + ğœ(3)S(3)
t dW(3)
t
)
+
S(1)
t
(S(2)
t )3S(3)
t
(
(ğœ(2)S(2)
t )2dt
)
+
S(1)
t
S(2)
t (S(3)
t )3
(
(ğœ(3)S(3)
t )2dt
)
âˆ’
1
(S(2)
t )2S(3)
t
(
ğœŒ12ğœ(1)ğœ(2)S(1)
t S(2)
t dt
)
âˆ’
1
S(2)
t (S(3)
t )2
(
ğœŒ13ğœ(1)ğœ(3)S(1)
t S(3)
t dt
)
+
S(1)
t
(S(2)
t S(3)
t )2
(
ğœŒ23ğœ(2)ğœ(3)S(2)
t S(3)
t dt
)
= (ğœ‡(1) âˆ’ğœ‡(2) âˆ’ğœ‡(3) + (ğœ(2))2 + (ğœ(3))2 âˆ’ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) + ğœŒ23ğœ(2)ğœ(3)) Utdt
+Ut
(
ğœ(1)dW(1)
t
âˆ’ğœ(2)dW(2)
t
âˆ’ğœ(3)dW(3)
t
)
= (ğœ‡(1) âˆ’ğœ‡(2) âˆ’ğœ‡(3) + (ğœ(2))2 + (ğœ(3))2 âˆ’ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) + ğœŒ23ğœ(2)ğœ(3)) Utdt
+
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3)UtdVt.
We can therefore write
dUt = ğœ‡Utdt + ğœUtdVt
where ğœ‡= ğœ‡(1) âˆ’ğœ‡(2) âˆ’ğœ‡(3) + (ğœ(2))2 + (ğœ(3))2 âˆ’ğœŒ12ğœ(1)ğœ(2) âˆ’ğœŒ13ğœ(1)ğœ(3) + ğœŒ23ğœ(2)ğœ(3),
ğœ2 = (ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3) and
Vt =
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3) .
As described in Problem 3.2.3.2 (page 156) we can easily show that
Vt =
ğœ(1)W(1)
t
âˆ’ğœ(2)W(2)
t
âˆ’ğœ(3)W(3)
t
âˆš
(ğœ(1))2 + (ğœ(2))2 + (ğœ(3))2 âˆ’2ğœŒ12ğœ(1)ğœ(2) âˆ’2ğœŒ13ğœ(1)ğœ(3) + 2ğœŒ23ğœ(2)ğœ(3) âˆ¼ğ’©(0, t)
and is a standard Wiener process.
â—½

3.2.3
Multi-Dimensional Diffusion Process
163
6. Bessel Process. Let (ğ›€, â„±, â„™) be a probability space and let Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)
be an n-dimensional standard Wiener process, with W(i)
t , i = 1, 2, . . . , n being independent
one-dimensional standard Wiener processes. By setting
Xt =
âˆš
(W(1)
t )2 + (W(2)
t )2 + . . . + (W(n)
t )2
show that
Wt = âˆ«
t
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t
0
W(n)
s
Xs
dW(n)
s
is a standard Wiener process.
Show also that Xt satisfies the n-dimensional Bessel process with SDE
dXt =
(
n âˆ’1
2Xt
)
dt + dWt
where X0 = 0.
Solution: We first need to show that Wt âˆ¼ğ’©(0, t). Using the properties of the stochastic
ItÂ¯o integral and the independence property of W(i)
t , i = 1, 2, . . . , n,
ğ”¼(Wt) = ğ”¼
[
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t
0
W(n)
s
Xs
dW(n)
s
]
= ğ”¼
(
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
)
+ ğ”¼
(
âˆ«
t
0
W(2)
s
Xs
dW(2)
s
)
+ . . . + ğ”¼
(
âˆ«
t
0
W(n)
s
Xs
dW(n)
s
)
= 0
ğ”¼(W2
t ) = ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
)2
+
(
âˆ«
t
0
W(2)
s
Xs
dW(2)
s
)2
+ . . . +
(
âˆ«
t
0
W(n)
s
Xs
dW(n)
s
)2
+ 2
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
(
âˆ«
t
0
W(i)
s
Xs
dW(i)
s
) (
âˆ«
t
0
W(j)
s
Xs
dW(j)
s
)â¤
â¥
â¥
â¥â¦
= ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
)2â¤
â¥
â¥â¦
+ ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t
0
W(2)
s
Xs
dW(2)
s
)2â¤
â¥
â¥â¦
+ . . . + ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t
0
W(n)
s
Xs
dW(n)
s
)2â¤
â¥
â¥â¦

164
3.2.3
Multi-Dimensional Diffusion Process
= ğ”¼
â¡
â¢
â¢â£âˆ«
t
0
(
W(1)
s
Xs
)2
ds
â¤
â¥
â¥â¦
+ ğ”¼
â¡
â¢
â¢â£âˆ«
t
0
(
W(2)
s
Xs
)2
ds
â¤
â¥
â¥â¦
+ . . . + ğ”¼
â¡
â¢
â¢â£âˆ«
t
0
(
W(n)
s
Xs
)2
ds
â¤
â¥
â¥â¦
= ğ”¼
(
âˆ«
t
0
ds
)
= t.
Because Wt is a function of n independent normal variates, so Wt âˆ¼ğ’©(0, t). To show that
Wt is also a standard Wiener process, we note the following:
(a) W0 = lim
tâ†’0
[
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t
0
W(n)
s
Xs
dW(n)
s
]
= 0 and it is
clear that Wt has continuous sample paths for t â‰¥0.
(b) For t > 0, u > 0
Wt+u âˆ’Wt = âˆ«
t+u
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t+u
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t+u
0
W(n)
s
Xs
dW(n)
s
âˆ’âˆ«
t
0
W(1)
s
Xs
dW(1)
s
âˆ’âˆ«
t
0
W(2)
s
Xs
dW(2)
s
âˆ’. . . âˆ’âˆ«
t
0
W(n)
s
Xs
dW(n)
s
= âˆ«
t+u
t
W(1)
s
Xs
dW(1)
s
+ âˆ«
t+u
t
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t+u
t
W(n)
s
Xs
dW(n)
s .
Taking expectations,
ğ”¼(Wt+u âˆ’Wt
) = ğ”¼
(
âˆ«
t+u
t
W(1)
s
Xs
dW(1)
s
)
+ ğ”¼
(
âˆ«
t+u
t
W(2)
s
Xs
dW(2)
s
)
+ . . . + ğ”¼
(
âˆ«
t+u
t
W(n)
s
Xs
dW(n)
s
)
= 0
and due to the independence of W(i)
t , i = 1, 2, . . . , n,
ğ”¼
[(Wt+u âˆ’Wt
)2]
= ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t+u
t
W(1)
s
Xs
dW(1)
s
)2
+
(
âˆ«
t+u
t
W(2)
s
Xs
dW(2)
s
)2
+ . . . +
(
âˆ«
t
0
W(n)
s
Xs
dW(n)
s
)2
+ 2
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
(
âˆ«
t+u
t
W(i)
s
Xs
dW(i)
s
) (
âˆ«
t+u
t
W(j)
s
Xs
dW(j)
s
)â¤
â¥
â¥
â¥â¦

3.2.3
Multi-Dimensional Diffusion Process
165
= ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t+u
t
W(1)
s
Xs
dW(1)
s
)2â¤
â¥
â¥â¦
+ ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t+u
t
W(2)
s
Xs
dW(2)
s
)2â¤
â¥
â¥â¦
+ . . . + ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t+u
t
W(n)
s
Xs
dW(n)
s
)2â¤
â¥
â¥â¦
= ğ”¼
â¡
â¢
â¢â£âˆ«
t+u
t
(
W(1)
s
Xs
)2
ds
â¤
â¥
â¥â¦
+ ğ”¼
â¡
â¢
â¢â£âˆ«
t+u
t
(
W(2)
s
Xs
)2
ds
â¤
â¥
â¥â¦
+ . . . + ğ”¼
â¡
â¢
â¢â£âˆ«
t+u
t
(
W(n)
s
Xs
)2
ds
â¤
â¥
â¥â¦
= ğ”¼
[
âˆ«
t+u
t
ds
]
= u.
Thus, Wt+u âˆ’Wt âˆ¼ğ’©(0, u).
(c) To show that Wt+u âˆ’Wt âŸ‚âŸ‚Wt we note that from the independent increment property
of W(i)
t , i = 1, 2, . . . , n,
ğ”¼[(Wt+u âˆ’Wt
) Wt
]
= ğ”¼(Wt+uWt
) âˆ’ğ”¼(W2
t
)
= ğ”¼
[(
âˆ«
t+u
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t+u
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t+u
0
W(n)
s
Xs
dW(n)
s
)
Ã—
(
âˆ«
t
0
W(1)
s
Xs
dW(1)
s
+ âˆ«
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + âˆ«
t
0
W(n)
s
Xs
dW(n)
s
)]
âˆ’ğ”¼
(
W2
t
)
=
n
âˆ‘
i=1
ğ”¼
â¡
â¢
â¢â£
(
âˆ«
t
0
W(i)
s
Xs
dW(i)
s
)2â¤
â¥
â¥â¦
+
n
âˆ‘
i=1
ğ”¼
[(
âˆ«
t+u
t
W(i)
s
Xs
dW(i)
s
) (
âˆ«
t
0
W(i)
s
Xs
dW(i)
s
)]
+
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
ğ”¼
[(
âˆ«
t+u
t
W(i)
s
Xs
dW(i)
s
) (
âˆ«
t
0
W(j)
s
Xs
dW(j)
s
)]
âˆ’ğ”¼
(
W2
t
)
=
n
âˆ‘
i=1
ğ”¼
â¡
â¢
â¢â£âˆ«
t
0
(
W(i)
s
Xs
)2
ds
â¤
â¥
â¥â¦
âˆ’ğ”¼
(
W2
t
)
= t âˆ’t
= 0.
Since Wt âˆ¼ğ’©(0, t), Wt+s âˆ’Wt âˆ¼ğ’©(0, s) and the joint distribution of Wt and Wt+s âˆ’
Wt is a bivariate normal (see Problem 2.2.1.5, page 58), if Cov(Wt+s âˆ’Wt, Wt) = 0
then Wt+s âˆ’Wt âŸ‚âŸ‚Wt.

166
3.2.3
Multi-Dimensional Diffusion Process
Thus, from the results of (a)â€“(c) we have shown that Wt is a standard Wiener process.
From ItÂ¯oâ€™s formula and given dW(i)
t dW(j)
t
= 0, i â‰ j,
dXt =
n
âˆ‘
i=1
ğœ•Xt
ğœ•W(i)
t
dW(i)
t
+ 1
2
n
âˆ‘
i=1
ğœ•2Xt
ğœ•(W(i)
t )2 (dW(i)
t )2
=
n
âˆ‘
i=1
ğœ•Xt
ğœ•W(i)
t
dW(i)
t
+ 1
2
n
âˆ‘
i=1
ğœ•2Xt
ğœ•(W(i)
t )2 dt
=
n
âˆ‘
i=1
W(i)
t
Xt
dW(i)
t
+ 1
2
â¡
â¢
â¢
â¢â£
n
âˆ‘
i=1
Xt âˆ’Xâˆ’1
t
(
W(i)
t
)2
X2
t
â¤
â¥
â¥
â¥â¦
dt
= dWt + 1
2
â¡
â¢
â¢
â¢â£
n
Xt
âˆ’
âˆ‘n
i=1
(
W(i)
t
)2
X3
t
â¤
â¥
â¥
â¥â¦
= dWt + 1
2
(
n
Xt
âˆ’X2
t
X3
t
)
= dWt +
(
n âˆ’1
2Xt
)
dt.
Therefore,
Xt =
âˆš
(W(1)
t )2 + (W(2)
t )2 + . . . + (W(n)
t )2
satisfies the SDE
dXt =
(
n âˆ’1
2Xt
)
dt + dWt.
â—½
7. Forwardâ€“Spot Price Relationship II. Let (Î©, â„±, â„™) be a probability space. We consider
the forward price F(t, T) of an asset St satisfying the SDE
dF(t, T)
F(t, T) = ğœ1(t, T) dW(1)
t
+ ğœ2(t, T) dW(2)
t
where ğœ1(t, T) > 0, ğœ2(t, T) > 0 are time-dependent volatilities and {W(1)
t
âˆ¶t â‰¥0},
{W(2)
t
âˆ¶t â‰¥0} are standard Wiener processes with correlation ğœŒâˆˆ(âˆ’1, 1). Given the
relationship F(t, T) = Ster(Tâˆ’t) where r is the risk-free interest rate, show that
dSt
St
=
{ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
[
ğœ1(u, t)ğœ•ğœ1(u, t)
ğœ•t
+ ğœŒ
(
ğœ1(u, t)ğœ•ğœ2(u, t)
ğœ•t
+ ğœ2(u, t)ğœ•ğœ1(u, t)
ğœ•t
)
+ ğœ2(u, t)ğœ•ğœ2(u, t)
ğœ•t
]
du + âˆ«
t
0
ğœ•ğœ1(u, t)
ğœ•t
dW(1)
u
+ âˆ«
t
0
ğœ•ğœ2(u, t)
ğœ•t
dW(2)
u
}
dt + ğœ(t, t) dWt

3.2.3
Multi-Dimensional Diffusion Process
167
where
ğœ(t, t) =
âˆš
ğœ1(t, t)2 + 2ğœŒğœ1(t, t)ğœ2(t, t) + ğœ2(t, t)2
and
Wt =
ğœ1(t, t)W(1)
t
+ ğœ2(t, t)W(2)
t
âˆš
ğœ1(t, t)2 + 2ğœŒğœ1(t, t)ğœ2(t, t) + ğœ2(t, t)2
is a standard Wiener process.
Solution: Expanding log F(t, T) using Taylorâ€™s theorem and then applying ItÂ¯oâ€™s lemma,
we have
d log F(t, T) =
1
F(t, T)dF(t, T) âˆ’
1
2F(t, T)2 dF(t, T)2 + . . .
= ğœ1(t, T) dW(1)
t
+ ğœ2(t, T) dW(2)
t
âˆ’1
2
[ğœ1(t, T)2 + ğœ2(t, T)2 + 2ğœŒğœ1(t, T)ğœ2(t, T)] dt
and taking integrals,
log
( F(t, T)
F(0, T)
)
= âˆ«
t
0
ğœ1(u, T) dW(1)
u
+ âˆ«
t
0
ğœ2(u, T) dW(2)
u
âˆ’1
2 âˆ«
t
0
[ğœ1(u, T)2 + ğœ2(u, T)2 + 2ğœŒğœ1(u, T)ğœ2(u, T)] du
We finally have
F(t, T) = F(0, T)eâˆ’1
2 âˆ«t
0 [ğœ1(u,T)2+ğœ2(u,T)2+2ğœŒğœ1(u,T)ğœ2(u,T)]du+âˆ«t
0 ğœ1(u,T)dW(1)
u +âˆ«t
0 ğœ2(u,T)dW(2)
u .
By setting T = t and taking note that St = F(t, t), the spot price St has the expression
St = F(0, t)eâˆ’1
2 âˆ«t
0 [ğœ1(u,t)2+ğœ2(u,t)2+2ğœŒğœ1(u,t)ğœ2(u,t)]du+âˆ«t
0 ğœ1(u,t)dW(1)
u +âˆ«t
0 ğœ2(u,t)dW(2)
u .
To find the SDE for St, we apply ItÂ¯oâ€™s lemma
dSt = ğœ•St
ğœ•t dt +
ğœ•St
ğœ•W(1)
t
dW(1)
t
+
ğœ•St
ğœ•W(2)
t
dW(2)
t
+ 1
2
ğœ•2St
ğœ•(W(1)
t )2 (dW(1)
t )2 + 1
2
ğœ•2St
ğœ•(W(2)
t )2 (dW(2)
t )2 +
ğœ•2St
ğœ•W(1)
t ğœ•W(2)
t
dW(1)
t dW(2)
t
+ . . .
=
(
ğœ•St
ğœ•t + 1
2
ğœ•2St
ğœ•(W(1)
t )2 + 1
2
ğœ•2St
ğœ•(W(2)
t )2 + ğœŒ
ğœ•2St
ğœ•W(1)
t ğœ•W(2)
t
)
dt +
ğœ•St
ğœ•W(1)
t
dW(1)
t
+ ğœ•St
ğœ•W(2)
t
dW(2)
t .

168
3.2.3
Multi-Dimensional Diffusion Process
Taking partial differentiations of St, we have
ğœ•St
ğœ•t = ğœ•log F(0, t)
ğœ•t
âˆ’1
2
[ğœ1(t, t)2 + ğœ2(t, t)2 + 2ğœŒğœ1(t, t)ğœ2(t, t)]
âˆ’âˆ«
t
0
[
ğœ1(u, t)ğœ•ğœ1(u, t)
ğœ•t
+ ğœŒ
(
ğœ1(u, t)ğœ•ğœ2(u, t)
ğœ•t
+ ğœ2(u, t)ğœ•ğœ1(u, t)
ğœ•t
)
+ ğœ2(u, t)ğœ•ğœ2(u, t)
ğœ•t
]
du
+ âˆ«
t
0
ğœ•ğœ1(u, t)
ğœ•t
dW(1)
u
+ âˆ«
t
0
ğœ•ğœ2(u, t)
ğœ•t
dW(2)
u ,
ğœ•St
ğœ•W(1)
t
= ğœ1(t, t)St,
ğœ•St
ğœ•W(2)
t
= ğœ2(t, t)St,
ğœ•2St
ğœ•(W(1)
t )2 = ğœ1(t, t)2St,
ğœ•2St
ğœ•(W(2)
t )2 = ğœ2(t, t)2St,
ğœ•2St
ğœ•W(1)
t ğœ•W(2)
t
= ğœ1(t, t)ğœ2(t, t)St
and substituting them into the SDE we eventually have
dSt
St
=
{ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
[
ğœ1(u, t)ğœ•ğœ1(u, t)
ğœ•t
+ ğœŒ
(
ğœ1(u, t)ğœ•ğœ2(u, t)
ğœ•t
+ ğœ2(u, t)ğœ•ğœ1(u, t)
ğœ•t
)
+ ğœ2(u, t)ğœ•ğœ2(u, t)
ğœ•t
]
du + âˆ«
t
0
ğœ•ğœ1(u, t)
ğœ•t
dW(1)
u
+ âˆ«
t
0
ğœ•ğœ2(u, t)
ğœ•t
dW(2)
u
}
dt
+ ğœ1(t, t) dW(1)
t
+ ğœ2(t, t) dW(2)
t
=
{ğœ•log F(0, t)
ğœ•t
âˆ’âˆ«
t
0
[
ğœ1(u, t)ğœ•ğœ1(u, t)
ğœ•t
+ ğœŒ
(
ğœ1(u, t)ğœ•ğœ2(u, t)
ğœ•t
+ ğœ2(u, t)ğœ•ğœ1(u, t)
ğœ•t
)
+ ğœ2(u, t)ğœ•ğœ2(u, t)
ğœ•t
]
du + âˆ«
t
0
ğœ•ğœ1(u, t)
ğœ•t
dW(1)
u
+ âˆ«
t
0
ğœ•ğœ2(u, t)
ğœ•t
dW(2)
u
}
dt
+ ğœ(t, t) dWt
where
ğœ(t, t) =
âˆš
ğœ1(t, t)2 + 2ğœŒğœ1(t, t)ğœ2(t, t) + ğœ2(t, t)2
and from the steps discussed in Problem 3.2.3.2 (page 156) we can prove that
Wt =
ğœ1(t, t)W(1)
t
+ ğœ2(t, t)W(2)
t
âˆš
ğœ1(t, t)2 + 2ğœŒğœ1(t, t)ğœ2(t, t) + ğœ2(t, t)2 âˆ¼ğ’©(0, t)
and is also a standard Wiener process.
â—½

3.2.3
Multi-Dimensional Diffusion Process
169
8. Gabillon 2-Factor Model. Let {W(s)
t
âˆ¶t â‰¥0} and {W(l)
t
âˆ¶t â‰¥0} be standard Wiener pro-
cesses on the probability space (Î©, â„±, â„™) with correlation ğœŒâˆˆ(âˆ’1, 1). Suppose the for-
ward curve F(t, T) follows the process
dF(t, T)
F(t, T) = ğœseâˆ’ğ›¼(Tâˆ’t)dW(s)
t
+ ğœl(1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) dW(l)
t
where t < T, ğ›¼is the mean-reversion parameter and ğœs and ğœl are the short-term and
long-term volatilities, respectively.
By setting
dW(s)
t
= dW(1)
t
and dW(l)
t
= ğœŒdW(1)
t
+
âˆš
1 âˆ’ğœŒ2dW(2)
t
where W(1)
t
and W(2)
t
are standard Wiener processes, W(1)
t
âŸ‚âŸ‚W(2)
t , show that
dF(t, T)
F(t, T) = ğœ(t, T) dWt
where
ğœ(t, T) =
âˆš
ğœ2
l + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’t) + 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’t)
and
Wt =
(ğœseâˆ’ğ›¼(Tâˆ’t) + ğœŒğœl
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t))) W(1)
t
+
âˆš
1 âˆ’ğœŒ2 (1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) W(2)
t
âˆš
ğœ2
l + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’t) + 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’t)
is a standard Wiener process.
Finally, show that conditional on F(0, T), F(t, T) follows a lognormal distribution with
mean
ğ”¼[F(t, T)| F(0, T)] = F(0, T)
and variance
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
ğœ2
l t + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) (
eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T
2ğ›¼
)
+2 (ğœŒğœsğœl âˆ’ğœ2
l
) (
eâˆ’ğ›¼(Tâˆ’t) âˆ’eâˆ’ğ›¼T
ğ›¼
)
âˆ’1
}
.
Solution: By defining dW(s)
t
= dW(1)
t
and dW(l)
t
= ğœŒdW(1)
t
+
âˆš
1 âˆ’ğœŒ2dW(2)
t
such that
W(1)
t
âŸ‚âŸ‚W(2)
t , the SDE of F(t, T) can be expressed as
dF(t, T)
F(t, T) = ğœseâˆ’ğ›¼(Tâˆ’t)dW(1)
t
+ ğ›¼l
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) (
ğœŒdW(1)
t
+
âˆš
1 âˆ’ğœŒ2dW(2)
t
)
= (ğœseâˆ’ğ›¼(Tâˆ’t) + ğœŒğœl(1 âˆ’eâˆ’ğ›¼(Tâˆ’t))) dW(1)
t
+
âˆš
1 âˆ’ğœŒ2ğœl
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) dW(2)
t
= ğœ(t, T) dWt

170
3.2.3
Multi-Dimensional Diffusion Process
where, owing to the fact that dW(1)
t
â‹…dW(2)
t
= 0, we have
ğœ(t, T)2 = (ğœseâˆ’ğ›¼(Tâˆ’t) + ğœŒğœl
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t)))2 + (1 âˆ’ğœŒ2) ğœ2
l
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t))2
= ğœ2
s eâˆ’2ğ›¼(Tâˆ’t) + 2ğœŒğœsğœleâˆ’ğ›¼(Tâˆ’t) (1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) + ğœ2
l
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t))2
= ğœ2
l + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’t) + 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’t)
and using the steps described in Problem 3.2.3.2 (page 156), we can easily show that
Wt =
(ğœseâˆ’ğ›¼(Tâˆ’t) + ğœŒğœl
(1 âˆ’eâˆ’ğ›¼(Tâˆ’t))) W(1)
t
+
âˆš
1 âˆ’ğœŒ2 (1 âˆ’eâˆ’ğ›¼(Tâˆ’t)) W(2)
t
âˆš
ğœ2
l + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’t) + 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’t)
âˆ¼ğ’©(0, t)
is also a standard Wiener process.
By expanding d log F(t, T) using Taylorâ€™s theorem and applying ItÂ¯oâ€™s formula, we have
d log F(t, T) = dF(t, T)
F(t, T) âˆ’1
2
(dF(t, T)
F(t, T)
)2
+ . . .
= ğœ(t, T) dWt âˆ’1
2ğœ(t, T)2dt.
Taking integrals from 0 to t,
âˆ«
t
0
d log F(u, T) = âˆ«
t
0
ğœ(u, T) dWu âˆ’1
2 âˆ«
t
0
ğœ(u, T)2 du
log F(t, T) = log F(0, T) + âˆ«
t
0
ğœ(u, T) dWu âˆ’1
2 âˆ«
t
0
ğœ(u, T)2 du.
From the property of ItÂ¯oâ€™s integral, we have
ğ”¼
[
âˆ«
t
0
ğœ(u, T) dWu
]
= 0
and
ğ”¼
[(
âˆ«
t
0
ğœ(u, T) dWu
)2]
= ğ”¼
[
âˆ«
t
0
ğœ(u, T)2 du
]
= âˆ«
t
0
ğœ(u, T)2 du.
Since the ItÂ¯o integral âˆ«
t
0
ğœ(u, T) dWu is in the form âˆ«
t
0
f(u) dWu, from Problem 3.2.2.4
(page 126) we can easily prove that âˆ«
t
0
ğœ(u, T) dWu follows a normal distribution,
âˆ«
t
0
ğœ(u, T) dWu âˆ¼ğ’©
(
0, âˆ«
t
0
ğœ(u, T)2 du
)
and hence
log F(t, T) âˆ¼ğ’©
(
log F(0, T) âˆ’1
2 âˆ«
t
0
ğœ(u, T)2 du, âˆ«
t
0
ğœ(u, T)2 du
)
.

3.2.3
Multi-Dimensional Diffusion Process
171
Solving the integral,
âˆ«
t
0
ğœ(u, T)2 du = âˆ«
t
0
[ğœ2
l + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’u) + 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’u)] du
= ğœ2
l u + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) eâˆ’2ğ›¼(Tâˆ’u)
2ğ›¼
+ 2 (ğœŒğœsğœl âˆ’ğœ2
l
) eâˆ’ğ›¼(Tâˆ’u)
ğ›¼
||||
t
0
= ğœ2
l t + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) (
eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T
2ğ›¼
)
+ 2 (ğœŒğœsğœl âˆ’ğœ2
l
) (
eâˆ’ğ›¼(Tâˆ’t) âˆ’eâˆ’ğ›¼T
ğ›¼
)
.
Since F(t, T) conditional on F(0, T) follows a lognormal distribution, we have
ğ”¼[F(t, T)|F(0, T)] = F(0, T)
and
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
ğœ2
l t + (ğœ2
s âˆ’2ğœŒğœsğœl + ğœ2
l
) (
eâˆ’2ğ›¼(Tâˆ’t) âˆ’eâˆ’2ğ›¼T
2ğ›¼
)
+ 2 (ğœŒğœsğœl âˆ’ğœ2
l
) (
eâˆ’ğ›¼(Tâˆ’t) âˆ’eâˆ’ğ›¼T
ğ›¼
)
âˆ’1
}
.
â—½
9. Integrated Square-Root Process. Let (Î©, â„±, â„™) be a probability space and let {Wt âˆ¶t â‰¥0}
be a standard Wiener process. Suppose Xt follows the CIR model with SDE
dXt = ğœ…(ğœƒâˆ’Xt) dt + ğœ
âˆš
XtdWt,
X0 > 0
where ğœ…, ğœƒand ğœare constants. We consider the integral
Yt = âˆ«
t
t0
Xu du,
Xt0 > 0
as the integrated square-root process of Xt up to time t from initial time t0, t0 < t.
Show that for n â‰¥1, m â‰¥1, n, m âˆˆâ„•, the process Xn
t Ym
t satisfies the SDE
d(Xn
t Ym
t )
Xn
t Ym
t
=
[(
nğœ…(ğœƒâˆ’Xt
) + 1
2n(n âˆ’1)ğœ2)
Xâˆ’1
t
+ mXtYâˆ’1
t
]
dt + nğœX
âˆ’1
2
t
dWt
and hence show that ğ”¼(Xn
t Ym
t |Xt0) satisfies the following first-order ordinary differential
equation
d
dtğ”¼
(
Xn
t Ym
t || Xt0
)
= nğœ…ğœƒğ”¼
(
Xnâˆ’1
t
Ym
t ||| Xt0
)
âˆ’nğœ…ğ”¼
(
Xn
t Ym
t || Xt0
)
+1
2n(n âˆ’1)ğœ2ğ”¼
(
Xn
t Ym
t |||Xt0
)
+ mğ”¼
(
Xn+1
t
Ymâˆ’1
t
|||Xt0
)
.
Finally, find the first two moments of Yt, given Xt0.

172
3.2.3
Multi-Dimensional Diffusion Process
Solution: From Taylorâ€™s theorem and subsequently applying ItÂ¯oâ€™s formula, we can write
d(Xn
t ) = nXnâˆ’1
t
dXt + 1
2n(n âˆ’1)Xnâˆ’2
t
(dX2
t ) + . . .
= nXnâˆ’1
t
[ğœ…(ğœƒâˆ’Xt) dt + ğœ
âˆš
XtdWt] + 1
2n(n âˆ’1)Xnâˆ’2
t
(ğœ2Xtdt)
=
[
nğœ…(ğœƒâˆ’Xt
) Xnâˆ’1
t
+ 1
2n(n âˆ’1)ğœ2Xnâˆ’1
t
]
dt + nğœX
nâˆ’1
2
t
dWt
and
d(Ym
t ) = mYmâˆ’1
t
dYt + 1
2m(m âˆ’1)Ymâˆ’2
t
(dYt)2 + . . .
= mXtYmâˆ’1
t
dt.
Thus,
d(Xn
t Ym
t ) = Ym
t d(Xn
t ) + Xn
t d(Ym
t ) + d(Xn
t )d(Ym
t )
=
[(
nğœ…(ğœƒâˆ’Xt
) + 1
2n(n âˆ’1)ğœ2)
Xnâˆ’1
t
Ym
t
]
dt + nğœX
nâˆ’1
2
t
Ym
t dWt
+ mXn+1
t
Ymâˆ’1
t
dt
or
d(Xn
t Ym
t )
Xn
t Ym
t
=
[(
nğœ…(ğœƒâˆ’Xt
) + 1
2n(n âˆ’1)ğœ2)
Xâˆ’1
t
+ mXtYâˆ’1
t
]
dt + nğœX
âˆ’1
2
t
dWt.
Taking integrals,
âˆ«
t
t0
d(Xn
uYm
u ) = âˆ«
t
t0
[
nğœ…(ğœƒâˆ’Xu
) Xnâˆ’1
u
Ym
u + 1
2n(n âˆ’1)ğœ2Xnâˆ’1
u
Ym
u + mXn+1
u
Ymâˆ’1
u
]
du
+ âˆ«
t
t0
nğœX
nâˆ’1
2
u
Ym
u dWu
Xn
t Ym
t = Xn
t0Ym
t0 +âˆ«
t
t0
[
nğœ…(ğœƒâˆ’Xu
) Xnâˆ’1
u Ym
u + 1
2n(n âˆ’1)ğœ2Xnâˆ’1
u
Ym
u + mXn+1
u
Ymâˆ’1
u
]
du
+âˆ«
t
t0
nğœX
nâˆ’1
2
u
Ym
u dWu
and taking expectations given Xt0 and because Yt0 = âˆ«
t0
t0
Xudu = 0, we have
ğ”¼[Xn
t Ym
t |Xt0] = âˆ«
t
t0
ğ”¼[nğœ…(ğœƒâˆ’Xu)Xnâˆ’1
u
Ym
u |Xt0]du + âˆ«
t
t0
ğ”¼
[1
2n(n âˆ’1)ğœ2Xnâˆ’1
u
Ym
u
||||
Xt0
]
du

3.2.3
Multi-Dimensional Diffusion Process
173
+ âˆ«
t
t0
ğ”¼[mXn+1
u
Ymâˆ’1
u
||| Xt0
]
du + âˆ«
t
t0
ğ”¼
[
nğœX
nâˆ’1
2
u
Ym
u
||||
Xt0
]
dWu
= âˆ«
t
t0
ğ”¼
[
nğœ…(ğœƒâˆ’Xu
) Xnâˆ’1
u
Ym
u |||Xt0
]
du + âˆ«
t
t0
ğ”¼
[1
2n(n âˆ’1)ğœ2Xnâˆ’1
u
Ym
u |||Xt0
]
du
+ âˆ«
t
t0
ğ”¼
[
mXn+1
u
Ymâˆ’1
u
|||Xt0
]
du.
Differentiating with respect to t yields
d
dtğ”¼
[
Xn
t Ym
t |||Xt0
]
= nğœ…ğ”¼
[(ğœƒâˆ’Xt
) Xnâˆ’1
t
Ym
t |||Xt0
]
+ 1
2n(n âˆ’1)ğœ2ğ”¼
[
Xnâˆ’1
t
Ym
t |||Xt0
]
+ mğ”¼
[
Xn+1
t
Ymâˆ’1
t
|||Xt0
]
= nğœ…ğœƒğ”¼
[
Xnâˆ’1
t
Ym
t |||Xt0
]
âˆ’nğœ…ğ”¼
[
Xn
t Ym
t |||Xt0
]
+ 1
2n(n âˆ’1)ğœ2ğ”¼
[
Xnâˆ’1
t
Ym
t |||Xt0
]
+ mğ”¼
[
Xn+1
t
Ymâˆ’1
t
|||Xt0
]
.
To find the mean of Yt given Xt0, we let n = 0 and m = 1 so that
d
dtğ”¼
[
Yt|||Xt0
]
= ğ”¼
[
Xt|||Xt0
]
.
From Problem 3.2.2.12 (page 135), we have
ğ”¼
[
Xt|||Xt0
]
= Xt0eâˆ’ğœ…(tâˆ’t0) + ğœƒ(1 âˆ’eâˆ’ğœ…(tâˆ’t0))
and therefore,
ğ”¼
[
Yt|||Xt0
]
= âˆ«
t
t0
[
Xt0eâˆ’ğœ…(uâˆ’t0) + ğœƒ
(
1 âˆ’eâˆ’ğœ…(uâˆ’t0))]
du
= ğœƒ(t âˆ’t0) + 1
ğœ…
(
Xt0 âˆ’ğœƒ
) (1 âˆ’eâˆ’ğœ…(tâˆ’t0)) .
For the second moment of Yt given Xt0, we set n = 0 and m = 2 so that
d
dtğ”¼
[
Y2
t |||Xt0
]
= 2ğ”¼
[
XtYt|||Xt0
]
and to find ğ”¼[XtYt|Xt0], we set n = 1 and m = 1 so that
d
dtğ”¼
[
XtYt|||Xt0
]
= ğœ…ğœƒğ”¼
[
Yt|||Xt0
]
âˆ’ğœ…ğ”¼
[
XtYt|||Xt0
]
+ ğ”¼
[
X2
t |||Xt0
]
or
d
dtğ”¼
[
XtYt|||Xt0
]
+ ğœ…ğ”¼
[
XtYt|||Xt0
]
= ğœ…ğœƒğ”¼
[
Yt|||Xt0
]
+ ğ”¼
[
X2
t |||Xt0
]
.

174
3.2.3
Multi-Dimensional Diffusion Process
By setting the integrating factor I = eğœ…t, the solution of the first-order differential
equation is
d
dt
[
eğœ…tğ”¼
[
XtYt|||Xt0
]]
= ğœ…ğœƒğ”¼
[
Yt|||Xt0
]
+ ğ”¼
[
X2
t |||Xt0
]
or
ğ”¼
[
XtYt|||Xt0
]
= eâˆ’ğœ…t
âˆ«
t
t0
{
ğœ…ğœƒğ”¼
[
Yu|||Xt0
]
+ ğ”¼
[
X2
u|||Xt0
]}
du.
Since
ğ”¼
[
Yt|||Xt0
]
= ğœƒ(t âˆ’t0) + 1
ğœ…
(
Xt0 âˆ’ğœƒ
) (1 âˆ’eâˆ’ğœ…(tâˆ’t0))
and from Problem 3.2.2.12 (page 135),
ğ”¼
[
X2
t |||Xt0
]
= X2
t0eâˆ’2ğœ…(tâˆ’t0) +
(
2ğœ…ğœƒ+ ğœ2
ğœ…
)
(Xt0 âˆ’ğœƒ) (eâˆ’ğœ…(tâˆ’t0) âˆ’eâˆ’2ğœ…(tâˆ’t0))
+
ğœƒ(2ğœ…ğœƒ+ ğœ2)
2ğœ…
(1 âˆ’eâˆ’2ğœ…(tâˆ’t0))
we can easily show that
ğ”¼
[
XtYt|||Xt0
]
= 1
2ğœ…ğœƒ2(t âˆ’t0)2eâˆ’ğœ…t + ğœƒ(t âˆ’t0)
(
Xt0 + ğœ2
2ğœ…
)
eâˆ’ğœ…t
+
(
ğœ…ğœƒ+ ğœ2
ğœ…2
)
(Xt0 âˆ’ğœƒ)
(
1 âˆ’eâˆ’ğœ…(tâˆ’t0))
eâˆ’ğœ…t
+ 1
2ğœ…
[
X2
t0 âˆ’
(
2ğœ…ğœƒ+ ğœ2
ğœ…
) (
Xt0 âˆ’1
2ğœƒ
)] (1 âˆ’eâˆ’2ğœ…(tâˆ’t0)) eâˆ’ğœ…t.
Finally, by substituting the result of ğ”¼[XtYt|Xt0] into
ğ”¼
[
Y2
t |||Xt0
]
= 2 âˆ«
t
t0
ğ”¼
[
XuYu|||Xt0
]
and using integration by parts, we eventually arrive at
ğ”¼
[
Y2
t |||Xt0
]
= 1
ğœ…2
[
X2
t0 +
(
4ğœƒ+ ğœ2
ğœ…
)
Xt0 + ğœƒ
(
ğœƒâˆ’ğœ2
2ğœ…
)] (1 âˆ’eâˆ’ğœ…(tâˆ’t0)) eâˆ’ğœ…t0
âˆ’ğœƒ
ğœ…
[
2(Xt0 + 1) + ğœ2
ğœ…+ t âˆ’t0
]
(t âˆ’t0)eâˆ’ğœ…t
âˆ’
(
ğœ…ğœƒ+ ğœ2
ğœ…3
) (
Xt0 âˆ’ğœƒ
) (1 âˆ’eâˆ’2ğœ…(tâˆ’t0)) eâˆ’ğœ…t0
âˆ’1
3ğœ…2
[
X2
t0 âˆ’
(
2ğœ…ğœƒ+ ğœ2
ğœ…
) (
Xt0 âˆ’1
2ğœƒ
)] (1 âˆ’eâˆ’3ğœ…(tâˆ’t0)) eâˆ’ğœ…t0.
â—½

3.2.3
Multi-Dimensional Diffusion Process
175
10. Heston Model. Let (Î©, â„±, â„™) be a probability space and let {WS
t âˆ¶t â‰¥0}, {Wğœ
t âˆ¶t â‰¥0}
be two standard Wiener processes with correlation ğœŒâˆˆ(âˆ’1, 1). Suppose the asset price St
takes the form
dSt = ğœ‡Stdt + ğœtStdWS
t ,
S0 > 0
dğœ2
t = ğœ…(ğœƒâˆ’ğœ2
t ) dt + ğ›¼ğœtdWğœ
t ,
ğœ0 > 0
dWS
t dWğœ
t = ğœŒdt
where ğœ‡, ğœ…, ğœƒand ğ›¼are constants, and ğœt is the stochastic volatility of St. By defining
{Bt âˆ¶t â‰¥0} as a standard Wiener process where Bt âŸ‚âŸ‚Wğœ
t , show that we can write
WS
t = ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt.
Using the above relation show also that for 0 â‰¤t â‰¤T we can write
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)
= ğœ‡(T âˆ’t) âˆ’1
2(1 âˆ’ğœŒ2) âˆ«
T
t
ğœ2
u du +
âˆš
1 âˆ’ğœŒ2
âˆ«
T
t
ğœu dBu
where ğœ‰s = eğœŒâˆ«s
0 ğœudWğœ
u âˆ’1
2 ğœŒ2 âˆ«s
0 ğœ2
udu.
Prove that the relation of ğœ‰T and ğœ‰t can be expressed as
ğœ‰T = ğœ‰t + ğœŒâˆ«
T
t
ğœuğœ‰u dWğœ
u .
Conditional on â„±t and {ğœu âˆ¶t â‰¤u â‰¤T}, show that
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T} âˆ¼ğ’©
[(
ğœ‡âˆ’1
2ğœ2
RMS
)
(T âˆ’t), ğœ2
RMS(T âˆ’t)
]
where ğœRMS =
âˆš(1 âˆ’ğœŒ2
T âˆ’t
)
âˆ«
T
t
ğœ2
u du.
Solution: From WS
t = ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt we have
ğ”¼(WS
t ) = ğ”¼(ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt) = ğœŒğ”¼(Wğœ
t ) +
âˆš
1 âˆ’ğœŒ2ğ”¼(Bt) = 0
and
Var (WS
t ) = Var (ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt) = ğœŒ2Var (Wğœ
t ) + (1 âˆ’ğœŒ2)Var (Bt) = t.
Given both Wğœ
t âˆ¼ğ’©(0, t) and Bt âˆ¼ğ’©(0, t), therefore
ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt âˆ¼ğ’©(0, t).

176
3.2.3
Multi-Dimensional Diffusion Process
In addition, using ItÂ¯oâ€™s formula and taking note that Wğœ
t âŸ‚âŸ‚Bt,
dWS
t â‹…dWğœ
t = d(ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt) â‹…dWğœ
t
= (ğœŒdWğœ
t +
âˆš
1 âˆ’ğœŒ2dBt) â‹…dWğœ
t
= ğœŒ(dWğœ
t )2 +
âˆš
1 âˆ’ğœŒ2dBt â‹…dWğœ
t
= ğœŒdt.
Thus, we can write WS
t = ğœŒWğœ
t +
âˆš
1 âˆ’ğœŒ2Bt.
Writing the SDEs of St and ğœ2
t in terms of Wğœ
t and Bt,
dSt = ğœ‡Stdt + ğœtSt(ğœŒdWğœ
t +
âˆš
1 âˆ’ğœŒ2dBt)
dğœ2
t = ğœ…(ğœƒâˆ’ğœ2
t ) dt + ğ›¼ğœtdWğœ
t
and following ItÂ¯oâ€™s lemma,
d log St = dSt
St
âˆ’1
2
(dSt
St
)2
+ . . .
= ğœ‡dt + ğœtSt(ğœŒdWğœ
t +
âˆš
1 âˆ’ğœŒ2dBt) âˆ’1
2ğœ2
t (ğœŒ2dt + (1 âˆ’ğœŒ2) dt)
=
(
ğœ‡âˆ’1
2ğœ2
t
)
dt + ğœŒğœtdWğœ
t +
âˆš
1 âˆ’ğœŒ2ğœtdBt
and taking integrals,
âˆ«
T
t
d log Su = âˆ«
T
t
(
ğœ‡âˆ’1
2ğœ2
u
)
du + ğœŒâˆ«
T
t
ğœu dWğœ
u +
âˆš
1 âˆ’ğœŒ2
âˆ«
T
t
ğœu dBu
log ST = log St + ğœ‡(T âˆ’t) âˆ’1
2 âˆ«
T
t
ğœ2
u du + ğœŒâˆ«
T
t
ğœu dWğœ
u +
âˆš
1 âˆ’ğœŒ2
âˆ«
T
t
ğœu dBu.
By setting
ğœ‰t = eğœŒâˆ«t
0 ğœu dWğœ
u âˆ’1
2 ğœŒ2 âˆ«t
0 ğœ2
u du
and
ğœ‰T = eğœŒâˆ«T
0 ğœu dWğœ
u âˆ’1
2 ğœŒ2 âˆ«T
0 ğœ2
u du
we have
log ST = log St + ğœ‡(T âˆ’t) âˆ’1
2(1 âˆ’ğœŒ2) âˆ«
T
t
ğœ2
u du +
âˆš
1 âˆ’ğœŒ2
âˆ«
T
t
ğœu dBu + log ğœ‰T âˆ’log ğœ‰t
or
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)
= ğœ‡(T âˆ’t) âˆ’1
2(1 âˆ’ğœŒ2) âˆ«
T
t
ğœ2
u du +
âˆš
1 âˆ’ğœŒ2
âˆ«
T
t
ğœu dBu.

3.2.3
Multi-Dimensional Diffusion Process
177
To show the relation between ğœ‰T and ğœ‰t, from Taylorâ€™s theorem and then using ItÂ¯oâ€™s lemma,
dğœ‰t = ğœ•ğœ‰t
ğœ•t dt + ğœ•ğœ‰t
ğœ•Wğœ
t
dWğœ
t + 1
2
ğœ•2ğœ‰t
ğœ•t2 (dt)2 + 1
2
ğœ•2ğœ‰t
ğœ•(Wğœ
t )2 (dWğœ
t )2 + . . .
= âˆ’1
2ğœŒ2ğœ2
t ğœ‰tdt + ğœŒğœtğœ‰tdWğœ
t + 1
2ğœŒ2ğœ2
t ğœ‰tdt
= ğœŒğœtğœ‰tdWğœ
t .
Taking integrals,
âˆ«
T
t
dğœ‰u = âˆ«
T
t
ğœŒğœuğœ‰udWğœ
u
or
ğœ‰T = ğœ‰t + ğœŒâˆ«
T
t
ğœuğœ‰u dWğœ
u .
Finally, conditional on â„±t and {ğœu âˆ¶t â‰¤u â‰¤T},
ğ”¼
[
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]
= ğœ‡(T âˆ’t) âˆ’1
2(1 âˆ’ğœŒ2) âˆ«
T
t
ğœ2
u du
=
(
ğœ‡âˆ’1
2ğœ2
RMS
)
(T âˆ’t)
and using the properties of ItÂ¯oâ€™s integral,
Var
[
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]
= (1 âˆ’ğœŒ2)Var
[
âˆ«
T
t
ğœu dBu
|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]
= (1 âˆ’ğœŒ2)ğ”¼
[(
âˆ«
T
t
ğœu dBu
)2||||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]
âˆ’(1 âˆ’ğœŒ2)ğ”¼
[
âˆ«
T
t
ğœu dBu
|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]2
= (1 âˆ’ğœŒ2)ğ”¼
[
âˆ«
T
t
ğœ2
u du
|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T}
]
= (1 âˆ’ğœŒ2) âˆ«
T
t
ğœ2
u du
= ğœ2
RMS(T âˆ’t)

178
3.2.3
Multi-Dimensional Diffusion Process
where ğœ2
RMS =
(1 âˆ’ğœŒ2
T âˆ’t
)
âˆ«
T
t
ğœ2
u du.
Since âˆ«
T
t
ğœu dBu is normally distributed, therefore
log
(STâˆ•ğœ‰T
Stâˆ•ğœ‰t
)|||||
â„±t, {ğœu âˆ¶t â‰¤u â‰¤T} âˆ¼ğ’©
[(
ğœ‡âˆ’1
2ğœ2
RMS
)
(T âˆ’t), ğœ2
RMS(T âˆ’t)
]
.
N.B. Given that the stochastic volatility ğœt follows a CIR process, from the results of Prob-
lem 3.2.3.9 (page 171) we can easily find the mean and variance of ğœ2
RMS.
â—½
11. Feynmanâ€“Kac Formula for Multi-Dimensional Diffusion Process. Let (Î©, â„±, â„™) be a prob-
ability space and let St = (S(1)
t , S(2)
t , . . . , S(n)
t ). We consider the following PDE problem:
ğœ•V
ğœ•t (St, t) + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœŒijğœ(S(i)
t , t)ğœ(S(j)
t , t)
ğœ•2V
ğœ•S(i)
t ğœ•S(j)
t
(St, t)
+
n
âˆ‘
i=1
ğœ‡(S(i)
t , t) ğœ•V
ğœ•S(i)
t
(St, t) âˆ’r(t)V(St, t) = 0
with boundary condition V(ST, T) = Î¨(ST) where ğœ‡, ğœare known functions of S(i)
t
and t,
r and Î¨ are functions of t and ST, respectively where t < T. Using ItÂ¯oâ€™s formula on the
process,
Zu = eâˆ’âˆ«u
t r(ğ‘£)dğ‘£V(Su, u)
where S(i)
t satisfies the generalised SDE
dS(i)
t
= ğœ‡(S(i)
t , t) dt + ğœ(S(i)
t , t) dW(i)
t
such that {W(i)
t
âˆ¶t â‰¥0} is a standard Wiener process and dW(i)
t
â‹…dW(j)
t
= ğœŒij dt, ğœŒij âˆˆ
(âˆ’1, 1) for i â‰ j and ğœŒij = 1 for i = j where i, j = 1, 2, . . . , n, show that under the filtration
â„±t, the solution of the PDE is given by
V(St, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£Î¨(ST)
||||
â„±t
]
.
Solution: In analogy with Problem 3.2.2.20 (page 147), we let g(u) = eâˆ’âˆ«u
t r(ğ‘£)dğ‘£and set
Zu = g(u)V(Su, u).
By applying Taylorâ€™s expansion and ItÂ¯oâ€™s formula on dZu, we have
dZu = ğœ•Zu
ğœ•u du +
n
âˆ‘
i=1
ğœ•Zu
ğœ•S(i)
u
dS(i)
u + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœ•2Zu
ğœ•S(i)
u ğœ•S(j)
u
dS(i)
u dS(j)
u + . . .

3.2.3
Multi-Dimensional Diffusion Process
179
=
(
g(u)ğœ•V
ğœ•u + V(Su, u)ğœ•g
ğœ•u
)
du +
n
âˆ‘
i=1
(
g(u) ğœ•V
ğœ•S(i)
u
)
dS(i)
u
+1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
(
g(u)
ğœ•2V
ğœ•S(i)
u ğœ•S(j)
u
)
dS(i)
u dS(j)
u
=
(
g(u)ğœ•V
ğœ•u âˆ’r(u)g(u)V(Su, u)
)
du
+
n
âˆ‘
i=1
(
g(u) ğœ•V
ğœ•S(i)
u
)
(ğœ‡(S(i)
u , u) du + ğœ(S(i)
u , u) dW(i)
u )
+1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
(
g(u)
ğœ•2V
ğœ•S(i)
u ğœ•S(j)
u
) (
ğœŒijğœ(S(i)
u , u)ğœ(S(j)
u , u) dt
)
= g(u)
(
ğœ•V
ğœ•u (Su, u) + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœŒijğœ(S(i)
u , t)ğœ(S(j)
u , u)
ğœ•2V
ğœ•S(i)
u ğœ•S(j)
u
(Su, u)
+
n
âˆ‘
i=1
ğœ‡(S(i)
u , u) ğœ•V
ğœ•S(i)
u
(Su, u) âˆ’r(u)V(Su, u)
)
du
+g(u)
n
âˆ‘
i=1
ğœ(S(i)
u , u) ğœ•V
ğœ•S(i)
u
dW(i)
u
= g(u)
n
âˆ‘
i=1
ğœ(S(i)
u , u) ğœ•V
ğœ•S(i)
u
dW(i)
u
since
ğœ•V
ğœ•u (Su, u) + 1
2
n
âˆ‘
i=1
n
âˆ‘
j=1
ğœŒijğœ(S(i)
u , t)ğœ(S(j)
u , u)
ğœ•2V
ğœ•S(i)
u ğœ•S(j)
u
(Su, u)
+
n
âˆ‘
i=1
ğœ‡(S(i)
u , u) ğœ•V
ğœ•S(i)
u
(Su, u) âˆ’r(u)V(Su, u) = 0.
By integrating both sides of dZu we have
âˆ«
T
t
dZu =
n
âˆ‘
i=1
{
âˆ«
T
t
g(u)ğœ(S(i)
u , u) ğœ•V
ğœ•S(i)
u
dW(i)
u
}
ZT âˆ’Zt =
n
âˆ‘
i=1
{
âˆ«
T
t
eâˆ’âˆ«u
t r(ğ‘£)dğ‘£ğœ(S(i)
u , u) ğœ•V
ğœ•S(i)
u
dW(i)
u
}
.
Taking expectations and using the property of ItÂ¯o calculus,
ğ”¼(ZT âˆ’Zt
) = 0 or ğ”¼(Zt
) = ğ”¼(ZT
) .

180
3.2.3
Multi-Dimensional Diffusion Process
Therefore, under the filtration â„±t,
ğ”¼(Zt|â„±t
) = ğ”¼(ZT|â„±t
)
ğ”¼
[
eâˆ’âˆ«t
t r(ğ‘£)dğ‘£V(St, t)||| â„±t
]
= ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£V(ST, T)||||
â„±t
]
V(St, t) = ğ”¼
[
eâˆ’âˆ«T
t
r(ğ‘£)dğ‘£Î¨(ST)
||||
â„±t
]
.
â—½
12. Backward Kolmogorov Equation for a Two-Dimensional Random Walk. We consider a
two-dimensional symmetric random walk where at initial time t0, a particle starts at (x0, y0)
and is at position (x, y) at time t. At time t + ğ›¿t, the particle can either move to (x + ğ›¿x, y),
(x âˆ’ğ›¿x, y), (x, y + ğ›¿y) or (x, y âˆ’ğ›¿y) each with probability 1
4. Let p(x, y, t; x0, y0, t0) denote
the probability density of the particle position (x, y) at time t starting at (x0, y0) at time t0.
By writing the backward equation in a discrete fashion and expanding it using Taylorâ€™s
series, show that for ğ›¿x = ğ›¿y =
âˆš
ğ›¿t and in the limit ğ›¿t â†’0,
ğœ•p(x, y, t; x0, y0, t0)
ğœ•t
= âˆ’1
4
(ğœ•2p(x, y, t; x0, y0, t0)
ğœ•x2
+ ğœ•2p(x, y, t; x0, y0, t0)
ğœ•y2
)
.
Solution: By denoting p(x, y, t; x0, y0, t0) as the probability density function of the particle
position (x, y) at time t, the discrete model of the backward equation is
p(x, y, t; x0, y0, t0) = 1
4p(x âˆ’ğ›¿x, y, t + ğ›¿t; x0, y0, t0) + 1
4p(x + ğ›¿x, y, t + ğ›¿t; x0, y0, t0)
+1
4p(x, y âˆ’ğ›¿y, t + ğ›¿t; x0, y0, t0) + 1
4p(x, y + ğ›¿y, t + ğ›¿t; x0, y0, t0).
Expanding
p(x âˆ’ğ›¿x, y, t + ğ›¿t; x0, y0, t0),
p(x + ğ›¿x, y, t + ğ›¿t; x0, y0, t0),
p(x, y âˆ’ğ›¿y, t +
ğ›¿t; x0, y0, t0) and p(x, y + ğ›¿y, t + ğ›¿t; x0, y0, t0) using Taylorâ€™s series, we have
p(x âˆ’ğ›¿x, y, t + ğ›¿t; x0, y0, t0) = p(x, y, t + ğ›¿t; x0, y0, t0) âˆ’ğœ•p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x
ğ›¿x
+ 1
2
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x2
(âˆ’ğ›¿x)2 + O((ğ›¿x)3)
p(x + ğ›¿x, y, t + ğ›¿t; x0, y0, t0) = p(x, y, t + ğ›¿t; x0, y0, t0) + ğœ•p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x
ğ›¿x
+ 1
2
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x2
(ğ›¿x)2 + O((ğ›¿x)3)
p(x, y âˆ’ğ›¿y, t + ğ›¿t; x0, y0, t0) = p(x, y, t + ğ›¿t; x0, y0, t0) âˆ’ğœ•p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y
ğ›¿y
+ 1
2
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y2
(âˆ’ğ›¿y)2 + O((ğ›¿y)3)

3.2.3
Multi-Dimensional Diffusion Process
181
and
p(x, y + ğ›¿y, t + ğ›¿t; x0, y0, t0) = p(x, y, t + ğ›¿t; x0, y0, t0) + ğœ•p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y
ğ›¿y
+ 1
2
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y2
(ğ›¿y)2 + O((ğ›¿y)3).
By substituting the above equations into the discrete backward equation,
p(x, y, t; x0, y0, t0) = p(x, y, t + ğ›¿t; x0, y0, t0) + 1
4
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x2
(ğ›¿x)2
+1
4
ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y2
(ğ›¿y)2 + O((ğ›¿x)3) + O((ğ›¿y)3).
Setting ğ›¿x = ğ›¿y =
âˆš
ğ›¿t and dividing the equation by ğ›¿t and in the limit ğ›¿t â†’0
lim
ğ›¿tâ†’0
[p(x, y, t + ğ›¿t; x0, y0, t0) âˆ’p(x, y, t; x0, y0, t0)
ğ›¿t
]
= âˆ’lim
ğ›¿tâ†’0
1
4
(ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•x2
)
âˆ’lim
ğ›¿tâ†’0
1
4
(ğœ•2p(x, y, t + ğ›¿t; x0, y0, t0)
ğœ•y2
)
+ lim
ğ›¿tâ†’0 O(
âˆš
ğ›¿t)
we eventually arrive at
ğœ•p(x, y, t; x0, y0, t0)
ğœ•t
= âˆ’1
4
(ğœ•2p(x, y, t; x0, y0, t0)
ğœ•x2
+ ğœ•2p(x, y, t; x0, y0, t0)
ğœ•y2
)
.
â—½
13. Forward Kolmogorov Equation for a Two-Dimensional Random Walk. We consider a
two-dimensional symmetric random walk where at initial time t0, a particle starts at
(X0, Y0) and is at position (X, Y) at terminal time T > 0. At time T âˆ’ğ›¿T, the particle can
either move to (X + ğ›¿X, Y), (X âˆ’ğ›¿X, Y), (X, Y + ğ›¿Y) or (X, Y âˆ’ğ›¿Y) each with probability
1
4. Let p(X, Y, T; X0, Y0, t0) denote the probability density of the position (X, Y) at time T
starting at (X0, Y0) at time t0.
By writing the forward equation in a discrete fashion and expanding it using Taylorâ€™s
series, show that for ğ›¿X = ğ›¿Y =
âˆš
ğ›¿T and in the limit ğ›¿T â†’0
ğœ•p(X, Y, T; X0, Y0, t0)
ğœ•T
= 1
4
(ğœ•2p(X, Y, T; X0, Y0, t0)
ğœ•X2
+ ğœ•2p(X, Y, T; X0, Y0, t0)
ğœ•Y2
)
.

182
3.2.3
Multi-Dimensional Diffusion Process
Solution: By denoting p(X, Y, T; X0, Y0, t0) as the probability density function of the par-
ticle position (X, Y) at time T starting at (X0, Y0) at initial time t0, the discrete model of
the forward equation is
p(X, Y, T; X0, Y0, t0) = 1
4p(X âˆ’ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
+ 1
4p(X + ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
+ 1
4p(X, Y âˆ’ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0)
+ 1
4p(X, Y + ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0).
Expanding p(X âˆ’ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0), p(X + ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0), p(X, Y âˆ’
ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0) and p(X, Y + ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0) using Taylorâ€™s series, we have
p(X âˆ’ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0) = p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
âˆ’ğœ•p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X
ğ›¿X
+1
2
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X2
(âˆ’ğ›¿X)2 + O((ğ›¿X)3)
p(X + ğ›¿X, Y, T âˆ’ğ›¿T; X0, Y0, t0) = p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
+ ğœ•p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X
ğ›¿X
+1
2
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X2
(ğ›¿X)2 + O((ğ›¿X)3)
p(X, Y âˆ’ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0) = p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
âˆ’ğœ•p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y
ğ›¿Y
+1
2
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y2
(âˆ’ğ›¿Y)2 + O((ğ›¿Y)3)
and
p(X, Y + ğ›¿Y, T âˆ’ğ›¿T; X0, Y0, t0) = p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
+ ğœ•p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y
ğ›¿Y
+1
2
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y2
(ğ›¿Y)2 + O((ğ›¿Y)3).

3.2.3
Multi-Dimensional Diffusion Process
183
By substituting the above equations into the discrete forward equation,
p(X, Y, T; X0, Y0, t0) = p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0) + 1
4
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X2
(ğ›¿X)2
+ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y2
(ğ›¿Y)2 + O((ğ›¿X)3) + O((ğ›¿Y)3).
Setting ğ›¿X = ğ›¿Y =
âˆš
ğ›¿T, dividing the equation by ğ›¿T and in the limit ğ›¿T â†’0
lim
ğ›¿Tâ†’0
[p(X, Y, T; X0, Y0, t0) âˆ’p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğ›¿T
]
= lim
ğ›¿Tâ†’0
1
4
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•X2
+ lim
ğ›¿Tâ†’0
1
4
ğœ•2p(X, Y, T âˆ’ğ›¿T; X0, Y0, t0)
ğœ•Y2
+ lim
ğ›¿Tâ†’0 O(
âˆš
ğ›¿T)
we eventually arrive at
ğœ•p(X, Y, T; X0, Y0, t0)
ğœ•T
= 1
4
(ğœ•2p(X, Y, T; X0, Y0, t0)
ğœ•X2
+ ğœ•2p(X, Y, T; X0, Y0, t0)
ğœ•Y2
)
.
â—½


4
Change of Measure
In finance, derivative instruments such as options, swaps or futures can be used for both
hedging and speculation purposes. In a hedging scenario, traders can reduce their risk expo-
sure by buying and selling derivatives against fluctuations in the movement of underlying risky
asset prices such as stocks and commodities. Conversely, in a speculation scenario, traders can
also use derivatives to profit in the future direction of underlying prices. For example, if a trader
expects an asset price to rise in the future, then he/she can sell put options (i.e., the purchaser
of the put options pays an initial premium to the seller and has the right but not the obligation
to sell the shares back to the seller at an agreed price should the share price drop below it at the
option expiry date). Given the purchaser of the put option is unlikely to exercise the option,
the seller would be most likely to profit from the premium paid by the purchaser. From the
point of view of trading such contracts, we would like to price contingent claims (or payoffs
of derivative securities such as options) in such a way that there is no arbitrage opportunity
(or no risk-free profits). By doing so we will ensure that even though two traders may differ in
their estimate of the stock price direction, yet they will still agree on the price of the derivative
security. In order to accomplish this we can rely on Girsanovâ€™s theorem, which tells us how
a stochastic process can have a drift change (but not volatility) under a change of measure.
With the application of this important result to finance we can convert the underlying stock
prices under the physical measure (or real-world measure) into the risk-neutral measure (or
equivalent martingale measure) where all the current stock prices are equal to their expected
future prices discounted at the risk-free rate. This is in contrast to using the physical measure,
where the derivative security prices will vary greatly since the underlying assets will differ in
degrees of risk from each other.
4.1
INTRODUCTION
From the seminal work of Black, Scholes and Merton in using diffusion processes and martin-
gales to price contingent claims such as options on risky asset prices, the theory of mathemat-
ical finance is one of the most successful applications of probability theory. Fundamentally,
the Blackâ€“Scholes model is concerned with an economy consisting of two assets, a risky asset
(stock) whose price St, t â‰¥0 is a stochastic process and a risk-free asset (bond or money mar-
ket account) whose value Bt grows at a continuously compounded interest rate. Here we can
assume that St and Bt satisfy the following equations
dSt
St
= ğœ‡tdt + ğœtdWt, dBt
Bt
= rtdt
where ğœ‡t is the stock price growth rate, ğœt is the stock price volatility, rt is the risk-free rate and
Wt is a standard Wiener process on the probability space (Î©, â„±, â„™). In the financial market we
would like to price a contingent claim Î¨(ST) at time t â‰¤T in such a way that no risk-free profit
or arbitrage opportunity exists. But before we define the notion of arbitrage we need some

186
4.1
INTRODUCTION
financial terminologies. The following first two definitions refer to the concepts of trading
strategy and self-financing trading strategy, which form the basis of creating a martingale
framework to price a contingent claim.
Definition 4.1(a) (Trading Strategy) In a continuous time setting, at time t âˆˆ[0, T] we con-
sider an economy which consists of a non-dividend-paying risky asset St and a risk-free asset
Bt. A trading strategy (or portfolio) is a pair (ğœ™t, ğœ“t) of stochastic processes which are adapted
to the filtration â„±t, 0 â‰¤t â‰¤T holding ğœ™t shares of St and ğœ“t units invested in Bt. Therefore, at
time t the value of the portfolio, Î t is
Î t = ğœ™tSt + ğœ“tBt.
Definition 4.1(b) (Self-Financing Trading Strategy) At time t âˆˆ[0, T], the trading strategy
(ğœ™t, ğœ“t) of holding ğœ™t shares of non-dividend-paying risky asset St and ğœ“t units in risk-free asset
Bt having a portfolio value
Î t = ğœ™tSt + ğœ“tBt
is called self-financing (or self-financing portfolio) if
dÎ t = ğœ™tdSt + ğœ“tdBt
which implies the change in the portfolio value is due to changes in the market conditions and
not to either infusion or extraction of funds.
Note that the change in the portfolio value is only attributed to St and Bt rather than ğœ™t and
ğœ“t. However, given that we cannot deposit or withdraw cash in the portfolio there will be
restrictions imposed on the pair (ğœ™t, ğœ“t). In order to keep the trading strategy self-financing we
can see that if ğœ™t is increased then ğœ“t will be decreased, and vice versa. Thus, when we make
a choice of either ğœ™t or ğœ“t, the other can easily be found.
Definition 4.1(c) (Admissible Trading Strategy) At time t âˆˆ[0, T], the trading strategy
(ğœ™t, ğœ“t) of holding ğœ™t shares of non-dividend-paying risky asset St and ğœ“t units in risk-free
asset Bt having a portfolio value
Î t = ğœ™tSt + ğœ“tBt
is admissible if it is self-financing and if Î t â‰¥âˆ’ğ›¼almost surely for some ğ›¼> 0 and t âˆˆ[0, T].
From the above definition, the existence of a negative portfolio value which is bounded from
below almost surely shows that the investor cannot go too far into debt and thus have a finite
credit line. Once we have restricted ourselves to a class of admissible strategies, the absence
of arbitrage opportunities (â€œno free lunchâ€) can be ensured when pricing contingent claims.
But before we discuss its precise terminology we need to define first the concept of attainable
contingent claim.
Definition 4.1(d) (Attainable Contingent Claim) Consider a trading strategy (ğœ™t, ğœ“t) at
time t âˆˆ[0, T] of holding ğœ™t shares of non-dividend-paying risky asset St and ğœ“t units in
risk-free asset Bt having a portfolio value
Î t = ğœ™tSt + ğœ“tBt.

4.1
INTRODUCTION
187
The contingent claim Î¨(ST) is attainable if there exists an admissible strategy worth Î T =
Î¨(ST) at exercise time T.
Definition 4.1(e) (Arbitrage) An arbitrage opportunity is an admissible strategy if the fol-
lowing criteria are satisfied:
(i) Î 0 = 0
(no net investment initially)
(ii) â„™(Î T â‰¥0) = 1 (always win at time T)
(iii) â„™(Î T > 0) > 0 (making a positive return on investment at time T).
For risky assets which are traded in financial markets, the pricing of contingent claims based
on the underlying assets is determined based on the presence/absence of arbitrageable opportu-
nities as well as whether the market is complete/incomplete. The following definition discusses
the concept of a complete market.
Definition 4.1(f) (Complete Market) A market is said to be complete if every contingent
claim is attainable (i.e., can be replicated by a self-financing trading strategy). Otherwise, it
is incomplete.
In general, when we have NA number of traded assets (excluding risk-free assets) and NR
sources of risk, the financial â€œrule of thumbâ€ is as follows:
â€¢ If NA < NR then the market has no arbitrage and is incomplete.
â€¢ If NA = NR then the market has no arbitrage and is complete.
â€¢ If NA > NR then the market has arbitrage.
Within the framework of a Blackâ€“Scholes model, the following theorem states the existence
of a trading strategy pair (ğœ™t, ğœ“t) in a portfolio.
Theorem 4.2 (One-Dimensional Martingale Representation Theorem) Let {Wt âˆ¶0 â‰¤t â‰¤
T} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™) and let â„±t, 0 â‰¤t â‰¤T
be the filtration generated by Wt. If Mt, 0 â‰¤t â‰¤T is a martingale with respect to this filtration
then there exists an adapted process ğ›¾t, 0 â‰¤t â‰¤T such that
Mt = M0 + âˆ«
t
0
ğ›¾u dWu,
0 â‰¤t â‰¤T.
From the martingale representation theorem it follows that martingales can be represented
as ItÂ¯o integrals. However, the theorem only states that an adapted process ğ›¾t exists but does
not provide a method to find it explicitly. Owing to the complexity of the proof which involves
functional analysis, the details are omitted in this book.
To show the relationship between the martingale representation theorem and the trading
strategy pair (ğœ“t, ğœ™t), if we set B0 = 1 so that Bt = eâˆ«t
0 rudu then the discounted portfolio value
can be written as
ÌƒÎ t = Bâˆ’1
t Î t.

188
4.1
INTRODUCTION
Therefore, the trading strategy pair (ğœ™t, ğœ“t) is self-financing if and only if the discounted port-
folio value can be written as a stochastic integral
ÌƒÎ t = ÌƒÎ 0 + âˆ«
t
0
ğœ™ğ‘£d(Bâˆ’1
ğ‘£Sğ‘£).
By assuming that (ğœ™t, ğœ“t) is a self-financing trading strategy which replicates the contingent
claim Î¨(ST), it is hoped that the discounted risky asset value Bâˆ’1
t St will be a martingale (or
â„™-martingale), so that by taking expectations under the â„™measure
ğ”¼â„™[
ÌƒÎ T||| â„±t
]
= ğ”¼â„™[
ÌƒÎ 0||| â„±t
]
+ ğ”¼â„™
[
âˆ«
T
0
ğœ™ğ‘£d(Bâˆ’1
ğ‘£Sğ‘£)
|||||
â„±t
]
= ÌƒÎ 0 + âˆ«
t
0
ğœ™ğ‘£d(Bâˆ’1
ğ‘£Sğ‘£) = ÌƒÎ t
or
Î t = ğ”¼â„™
[
eâˆ’âˆ«T
t
rudu Î T
||||
â„±t
]
= ğ”¼â„™
[
eâˆ’âˆ«T
t
rudu Î¨(ST)
||||
â„±t
]
since Î T = Î¨(ST). Although Bâˆ’1
t St is not a â„™-martingale on the probability space (Î©, â„±, â„™),
there exists another equivalent martingale measure â„š(or risk-neutral measure) such that Bâˆ’1
t St
is a â„š-martingale on the probability space (Î©, â„±, â„š). Before we state the theorem we first
present a few intermediate results which will lead to the main result.
Definition 4.3 Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe
another probability measure on (Î©, â„±, â„š). Assume that for every A âˆˆâ„±satisfying â„™(A) = 0,
we also have â„š(A) = 0, then we say â„šis absolutely continuous with respect to â„™on â„±and we
write it as â„šâ‰ªâ„™.
Theorem 4.4 (Radonâ€“NikodÂ´ym Theorem) Let (Î©, â„±, â„™) be the probability space satisfying
the usual conditions. Let â„šbe another probability measure on (Î©, â„±, â„š). Under the assumption
that â„šâ‰ªâ„™, there exists a non-negative random variable Z such that
dâ„š
dâ„™= Z
and we call Z the Radonâ€“NikodÂ´ym derivative of â„šwith respect to â„™.
Take note that in finance we need a stronger statement â€“ that is, â„što be equivalent to â„™,
â„šâˆ¼â„™which is â„šâ‰ªâ„™and â„™â‰ªâ„š. By imposing the condition â„što be equivalent to â„™, if an
event cannot occur under the â„™measure then it also cannot occur under the â„šmeasure and
vice versa. By doing so, we can now state the following definition of the equivalent martingale
measure for asset prices which pay no dividends.
Definition 4.5 (Equivalent Martingale Measure) Let (Î©, â„±, â„™) be the probability space
satisfying the usual conditions and let â„šbe another probability measure on (Î©, â„±, â„š). The
probability measure â„šis said to be an equivalent martingale measure (or risk-neutral mea-
sure) if it satisfies:
â€¢ â„šis equivalent to â„™, â„šâˆ¼â„™;

4.1
INTRODUCTION
189
â€¢ the discounted price processes {Bâˆ’1
t S(i)
t }, i = 1, 2, . . . , m are martingales under â„š, that
is
ğ”¼â„š[
Bâˆ’1
u S(i)
u ||| â„±t
]
= Bâˆ’1
t S(i)
t
for all 0 â‰¤t â‰¤u â‰¤T.
Note that for the case of dividend-paying assets, the discounted values of the asset prices with
the dividends reinvested are â„š-martingales.
Once the equivalent martingale measure is defined, the next question is can we transform a
diffusion process into a martingale by changing the probability measure? The answer is yes
where the transformation can be established using Girsanovâ€™s theorem, which is instrumental
in risk-neutral pricing for derivatives.
Theorem
4.6 (One-Dimensional
Girsanov
Theorem) Let
{Wt âˆ¶0 â‰¤t â‰¤T}
be
a
â„™-standard Wiener process on the probability space (Î©, â„±, â„™) and let â„±t, 0 â‰¤t â‰¤T be
the associated Wiener process filtration. Suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T and
consider
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds.
If
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
then Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T. By changing the measure â„™to a measure â„š
such that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt,
then
ÌƒWt = Wt + âˆ«
t
0
ğœƒu du
is a â„š-standard Wiener process.
In probability theory, the importance of Girsanovâ€™s theorem cannot be understated as it pro-
vides a formal concept of how stochastic processes change under changes in measure. The
theorem is especially important in the theory of financial mathematics as it tells us how to
convert from the physical measure â„™to the risk-neutral measure â„š. In short, from the appli-
cation of this theorem we can change from a Wiener process with drift to a standard Wiener
process.
In contrast, the converse of Girsanovâ€™s theorem says that every equivalent measure is given
by a change in drift. Thus, by changing the measure it is equivalent to changing the drift and
hence in the Blackâ€“Scholes model there is only one equivalent risk-neutral measure. Other-
wise we would have multiple arbitrage-free derivative prices.
Corollary 4.7 If {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on the probability space
(Î©, â„±, â„™) and â„šis equivalent to â„™then there exists an adapted process ğœƒt, 0 â‰¤t â‰¤T such
that:

190
4.1
INTRODUCTION
(i) ÌƒWt = Wt + âˆ«t
0 ğœƒu du is a â„š-standard Wiener process,
(ii) the Radonâ€“NikodÂ´ym derivative of â„šwith respect to â„™is
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds
for 0 â‰¤t â‰¤T.
By comparing the martingale representation theorem with Girsanovâ€™s theorem we can see
that in the former the filtration generated by the standard Wiener process is more restrictive
than the assumption given in Girsanovâ€™s theorem. By including this extra restriction on the
filtration in Girsanovâ€™s theorem we have the following corollary.
Corollary 4.8 Let {Wt âˆ¶0 â‰¤t â‰¤T} be a â„™-standard Wiener process on the probability
space (Î©, â„±, â„™) and let â„±t, 0 â‰¤t â‰¤T be the filtration generated by Wt. By assuming the
one-dimensional Girsanov theorem holds and if ÌƒMt, 0 â‰¤t â‰¤T is a â„š-martingale, there exists
an adapted process {Ìƒğ›¾t âˆ¶0 â‰¤t â‰¤T} such that
ÌƒMt = ÌƒM0 + âˆ«
t
0
Ìƒğ›¾u d ÌƒWu,
0 â‰¤t â‰¤T
where ÌƒWt is a â„š-standard Wiener process.
By extending the one-dimensional Girsanov theorem to multiple risky assets where each of
the assets is a random component driven by an independent standard Wiener process, we state
the following multi-dimensional Girsanov theorem.
Theorem 4.9 (Multi-Dimensional Girsanov Theorem) Let Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be an n-dimensional â„™-standard Wiener process, with {W(i)
t }0â‰¤tâ‰¤T, i = 1, 2, . . . , n being an
independent one-dimensional â„™-standard Wiener process on the probability space (Î©, â„±, â„™)
and let â„±t, 0 â‰¤t â‰¤T be the associated Wiener process filtration. Suppose we have an
n-dimensional adapted process ğœ½t = (ğœƒ(1)
t , ğœƒ(2)
t , . . . , ğœƒ(n)
t )T, 0 â‰¤t â‰¤T and we consider
Zt = exp
{
âˆ’âˆ«
t
0
ğœ½u â‹…dWu âˆ’1
2 âˆ«
t
0
â€–â€–ğœ½uâ€–â€–
2
2 du
}
.
If
ğ”¼â„™
(
exp
{
1
2 âˆ«
T
0
â€–â€–ğœ½tâ€–â€–
2
2 dt
})
< âˆ
where
||ğœ½t||2 =
âˆš
(ğœƒ(1)
t )2 + (ğœƒ(2)
t )2 + . . . + (ğœƒ(n)
t )2
then Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T. By changing the measure â„™to a measure â„š
such that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt

4.1
INTRODUCTION
191
for all 0 â‰¤t â‰¤T then
Ìƒ
Wt = Wt + âˆ«
t
0
ğœ½u du
is an n-dimensional â„š-standard Wiener process where Ìƒ
Wt = ( ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t )T and
ÌƒW(i)
t
= W(i)
t
+ âˆ«
t
0
ğœƒ(i)
t du, i = 1, 2, . . . , n such that the component processes of Ìƒ
Wt are
independent under â„š.
By amalgamating the results of the multi-dimensional Girsanov theorem we also state the
following multi-dimensional martingale representation theorem.
Theorem 4.10 (Multi-Dimensional Martingale Representation Theorem) Let Wt =
(W(1)
t , W(2)
t , . . . , W(n)
t )T be an n-dimensional â„™-standard Wiener process, with {W(i)
t }0â‰¤tâ‰¤T,
i = 1, 2, . . . , n being an independent one-dimensional â„™-standard Wiener process on the
probability space (Î©, â„±, â„™) and let â„±t, 0 â‰¤t â‰¤T be the filtration generated by Wt. If Mt,
0 â‰¤t â‰¤T is a martingale with respect to this filtration, then there exists an n-dimensional
adapted process ğœ¸t = (ğ›¾(1)
t , ğ›¾(2)
t , . . . , ğ›¾(n)
t )T, 0 â‰¤t â‰¤T such that
Mt = M0 + âˆ«
t
0
ğœ¸u â‹…dWu,
0 â‰¤t â‰¤T.
By assuming the multi-dimensional Girsanov theorem holds and if ÌƒMt, 0 â‰¤t â‰¤T is a
â„š-martingale there exists an n-dimensional adapted process Ìƒğœ¸t = (Ìƒğ›¾(1)
t ,Ìƒğ›¾(2)
t , . . . ,Ìƒğ›¾(n)
t )T,
0 â‰¤t â‰¤T} such that
ÌƒMt = ÌƒM0 + âˆ«
t
0
Ìƒğœ¸u â‹…dÌƒ
Wu,
0 â‰¤t â‰¤T
where ÌƒWt =
(
ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t
)T
is an n-dimensional â„š-standard Wiener process.
In our earlier discussion we used the risk-free asset Bt to construct our trading strategy where,
under the risk-neutral measure â„šthe discounted risky asset price Bâˆ’1
t St is a â„š-martingale,
that is
ğ”¼â„š[
Bâˆ’1
T ST||| â„±t
]
= Bâˆ’1
t St.
In finance terminology the risk-free asset Bt which does the discounting is called the numÃ©raire.
Definition 4.11 A numÃ©raire is an asset with positive price process which pays no dividends.
Suppose there is another non-dividend-paying asset Nt with strictly positive price pro-
cess and under the risk-neutral measure â„š, the discounted price Bâˆ’1
t Nt is a â„š-martingale.
From Girsanovâ€™s theorem we can define a new probability measure â„šN given by the
Radonâ€“NikodÂ´ym derivative
dâ„šN
dâ„š
||||â„±t
= Nt
N0
/ Bt
B0
.
By changing the â„šmeasure to an equivalent â„šN measure, the discounted price Nâˆ’1
t St is a
â„šN-martingale such that
ğ”¼â„šN [
Nâˆ’1
T ST||| â„±t
]
= Nâˆ’1
t St.

192
4.2.1
Martingale Representation Theorem
Thus, we can deduce that
Btğ”¼â„š
[ ST
BT
||||
â„±t
]
= Ntğ”¼â„šN [ ST
NT
||||
â„±t
]
and by returning to the discussion of the self-financing trading strategy, for a contingent claim
Î¨(ST) we will also eventually obtain
Btğ”¼â„š
[ Î¨(ST)
BT
||||
â„±t
]
= Ntğ”¼â„šN [ Î¨(ST)
NT
||||
â„±t
]
.
The idea discussed above is known as the change of numÃ©raire technique and is often used
to price complicated derivatives such as convertible bonds, foreign currency and interest rate
derivatives.
4.2
PROBLEMS AND SOLUTIONS
4.2.1
Martingale Representation Theorem
1. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process and let X be a real-valued random vari-
able on the probability space (Î©, â„±, â„™) such that ğ”¼â„™(|X|2) < âˆ. For the process
Mt = ğ”¼â„™(X|â„±t
)
show that ğ”¼â„™(M2
t
) < âˆand that Mt is a â„™-martingale with respect to the filtration â„±t,
0 â‰¤t â‰¤T generated by Wt.
Solution: By definition,
ğ”¼â„™(M2
t
) = ğ”¼â„™[
ğ”¼â„™(X|â„±t
)2]
.
From Jensenâ€™s inequality (see Problem 1.2.3.14, page 48) we set ğœ‘(x) = x2, which is a
convex function. By substituting x = ğ”¼â„™(
X|â„±t
)
we have
ğ”¼â„™(X|â„±t
)2 â‰¤ğ”¼â„™(X2|â„±t
) .
Taking the expectation,
ğ”¼â„™[
ğ”¼â„™(X|â„±t
)2]
â‰¤ğ”¼â„™[ğ”¼â„™(X2|â„±t
)]
and from the tower property (see Problem 1.2.3.11, page 46)
ğ”¼â„™[ğ”¼â„™(X2|â„±t
)] = ğ”¼â„™(X2) .
Thus,
ğ”¼â„™[
ğ”¼â„™(X|â„±t
)2]
â‰¤ğ”¼â„™(X2)
and because ğ”¼â„™(|X|2) < âˆ, so ğ”¼â„™(M2
t
) = ğ”¼â„™[
ğ”¼â„™(X|â„±t
)2]
< âˆ.

4.2.1
Martingale Representation Theorem
193
To show that Mt is a â„™-martingale we note that:
(a) Under the filtration â„±s, 0 â‰¤s â‰¤t and using the tower property (see Problem 1.2.3.11,
page 46),
ğ”¼â„™(Mt|| â„±s
) = ğ”¼â„™[
ğ”¼â„™(X|â„±t
)||| â„±s
]
= ğ”¼â„™(X|â„±s
) = Ms.
(b) Since ğ”¼â„™(|X|2) < âˆwe can deduce, using HÃ¶lderâ€™s inequality, that
ğ”¼â„™(|Mt|) = ğ”¼â„™[|||ğ”¼â„™(X|â„±t
)|||
]
â‰¤ğ”¼â„™[ğ”¼â„™(
|X| |â„±t
)] = ğ”¼â„™(|X|) â‰¤
âˆš
ğ”¼â„™(|X|2) < âˆ.
(c) Mt is clearly â„±t-adapted for 0 â‰¤t â‰¤T.
From the results of (a)â€“(c) we have shown that Mt is a â„™-martingale with respect to â„±t,
0 â‰¤t â‰¤T.
â—½
2. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process and let X be a real-valued random vari-
able on the probability space (Î©, â„±, â„™) such that ğ”¼â„™(|X|2) < âˆ. Given that the process
Mt = ğ”¼â„™(X|â„±t
)
is a â„™-martingale with respect to the filtration â„±t, 0 â‰¤t â‰¤T generated by Wt, then from the
martingale representation theorem there exists an adapted process ğ›¾u, 0 â‰¤u â‰¤T such that
Mt = M0 + âˆ«
t
0
ğ›¾u dWu,
0 â‰¤t â‰¤T.
Show that
(a) if X = WT then ğ›¾t = 1,
(b) if X = W2
T then ğ›¾t = 2Wt,
(c) if X = W3
T then ğ›¾t = 3(W2
t + T âˆ’t),
(d) if X = eğœWT, ğœâˆˆâ„then ğ›¾t = ğœeğœWt+ 1
2 ğœ2(Tâˆ’t),
for 0 â‰¤t â‰¤T.
Solution: To find the adapted process ğ›¾t we note that from ItÂ¯oâ€™s formula,
dMt = ğœ•Mt
ğœ•t dt + ğœ•Mt
ğœ•Wt
dWt + 1
2
ğœ•2Mt
ğœ•W2
t
dW2
t + . . . = ğ›¾tdWt
for 0 â‰¤t â‰¤T.
(a) For Mt = ğ”¼â„™(WT|| â„±t
) and since Wt is a â„™-martingale (see Problem 2.2.3.1, page 71),
Mt = ğ”¼â„™(WT|| â„±t
) = Wt
and hence, for 0 â‰¤t â‰¤T,
ğ›¾t = ğœ•Mt
ğœ•Wt
= 1.

194
4.2.2
Girsanovâ€™s Theorem
(b) For Mt = ğ”¼â„™(
W2
T
||| â„±t
)
and since W2
t âˆ’t is a â„™-martingale (see Problem 2.2.3.2,
page 72),
ğ”¼â„™(
W2
T âˆ’T||| â„±t
)
= W2
t âˆ’t
or
Mt = ğ”¼â„™(
W2
T
||| â„±t
)
= W2
t + T âˆ’t.
Thus,
ğ›¾t = ğœ•Mt
ğœ•Wt
= 2Wt
for 0 â‰¤t â‰¤T.
(c) If Mt = ğ”¼â„™(
W3
T
||| â„±t
)
and since W3
t âˆ’3tWt is a â„™-martingale (see Problem 2.2.3.4,
page 73),
ğ”¼â„™(
W3
T âˆ’3TWT||| â„±t
)
= W3
t âˆ’3tWt
or
Mt = ğ”¼â„™(
W3
T
||| â„±t
)
= W3
t + 3(T âˆ’t)Wt.
Therefore,
ğ›¾t = ğœ•Mt
ğœ•Wt
= 3 (W2
t + T âˆ’t)
for 0 â‰¤t â‰¤T.
(d) If Mt = ğ”¼â„™(eğœWT|| â„±t
) and since eğœWtâˆ’1
2 ğœ2t is a â„™-martingale (see Problem 2.2.3.3,
page 72),
ğ”¼â„™
(
eğœWTâˆ’1
2 ğœ2T||||
â„±t
)
= eğœWtâˆ’1
2 ğœ2t
or
Mt = ğ”¼â„™(
eğœWT||| â„±t
)
= eğœWt+ 1
2 ğœ2(Tâˆ’t).
Thus,
ğ›¾t = ğœ•Mt
ğœ•Wt
= ğœeğœWt+ 1
2 ğœ2(Tâˆ’t)
for 0 â‰¤t â‰¤T.
â—½
4.2.2
Girsanovâ€™s Theorem
1. Novikovâ€™s Condition I. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the probabil-
ity space (Î©, â„±, â„™) and let ğœƒt be an adapted process, 0 â‰¤t â‰¤T. By considering
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds

4.2.2
Girsanovâ€™s Theorem
195
and if
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
then, without using ItÂ¯oâ€™s formula, show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
Solution: Using the properties of stochastic integrals (see Problems 3.2.1.10, page 112;
3.2.1.11, page 113; and 3.2.2.4, page 126), we can deduce
âˆ«
t
0
ğœƒs dWs âˆ¼ğ’©
(
0, âˆ«
t
0
ğœƒ2
s ds
)
.
In the same vein, for any u < t, the random variable
âˆ«
t
u
ğœƒs dWs âˆ¼ğ’©
(
0, âˆ«
t
u
ğœƒ2
s ds
)
and âˆ«u
0 ğœƒsdWs âŸ‚âŸ‚âˆ«t
u ğœƒsdWs. To show that Zt is a â„™-martingale, we note the following.
(a) Under the filtration â„±u,
ğ”¼â„™(Zt ||â„±u
) = ğ”¼â„™
(
eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds||||
â„±u
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds ğ”¼â„™(
eâˆ’âˆ«t
0 ğœƒsdWs||| â„±u
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds ğ”¼â„™(
eâˆ’âˆ«u
0 ğœƒsdWsâˆ’âˆ«t
u ğœƒsdWs||| â„±u
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds ğ”¼â„™(
eâˆ’âˆ«u
0 ğœƒsdWs||| â„±u
)
ğ”¼â„™(
eâˆ’âˆ«t
u ğœƒsdWs||| â„±u
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds â‹…eâˆ’âˆ«u
0 ğœƒsdWs â‹…e
1
2 âˆ«t
u ğœƒ2
s ds
= eâˆ’âˆ«u
0 ğœƒsdWs â‹…eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds â‹…eâˆ’1
2 âˆ«u
t ğœƒ2
s ds
= eâˆ’âˆ«u
0 ğœƒsdWsâˆ’1
2 âˆ«u
0 ğœƒ2
s ds.
Therefore, ğ”¼â„™(Zt ||â„±u
) = Zu.
(b) Taking the expectation of |Zt|,
ğ”¼â„™(|Zt|) = ğ”¼â„™
(||||
eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds||||
)
= ğ”¼â„™(
eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s dsğ”¼â„™(
eâˆ’âˆ«t
0 ğœƒsdWs
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
s ds â‹…e
1
2 âˆ«t
0 ğœƒ2
s ds
= 1 < âˆ
since, for X âˆ¼log-ğ’©(ğœ‡, ğœ2), ğ”¼â„™(X) = eğœ‡+ 1
2 ğœ2.
(c) Because Zt is a function of Wt, it is â„±t-adapted.

196
4.2.2
Girsanovâ€™s Theorem
From the results of (a)â€“(c) we have shown that {Zt âˆ¶0 â‰¤t â‰¤T} is a â„™-martingale and
because Zt > 0, {Zt}0â‰¤tâ‰¤T is a positive â„™-martingale.
â—½
2. Novikovâ€™s Condition II. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the proba-
bility space (Î©, â„±, â„™) and let ğœƒt be an adapted process, 0 â‰¤t â‰¤T. By considering
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds
and if
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
then, using ItÂ¯oâ€™s formula, show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
Solution: Let Zt = f(Xt) = eXt where Xt = âˆ’âˆ«t
0 ğœƒs dWs âˆ’1
2 âˆ«t
0 ğœƒ2
s ds and by applying
Taylorâ€™s expansion and subsequently ItÂ¯oâ€™s formula,
dZt = ğœ•f
ğœ•Xt
dXt + 1
2
ğœ•2f
ğœ•X2
t
(dXt)2 + . . .
= eXt
(
âˆ’ğœƒtdWt âˆ’1
2ğœƒ2
t dt
)
+ 1
2eXtğœƒ2
t dt
= âˆ’ğœƒtZtdWt.
Integrating both sides of the equation from u to t, where u < t,
âˆ«
t
u
dZs = âˆ’âˆ«
t
u
ğœƒsZs dWs
Zt = Zu âˆ’âˆ«
t
u
ğœƒsZs dWs.
Under the filtration â„±u and because âˆ«t
u ğœƒsZs dWs is independent of â„±u, we have
ğ”¼â„™(Zt|â„±u
) = Zu
where ğ”¼â„™
(
âˆ«
t
u
ğœƒsZs dWs
|||||
â„±u
)
= ğ”¼â„™
(
âˆ«
t
u
ğœƒsZs dWs
)
= 0. Using the same steps as
described in Problem 4.2.2.1 (page 194), we can also show that ğ”¼â„™(|Zt|) < âˆ. Since Zt
is â„±t-adapted and Zt > 0, we have shown that {Zt âˆ¶0 â‰¤t â‰¤T} is a positive â„™-martingale.
â—½
3. Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe another
probability measure on (Î©, â„±, â„š) such that â„šis absolutely continuous with respect to â„™
on â„±. Using the Radonâ€“NikodÂ´ym theorem show that if dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T,
then Zt is a positive â„™-martingale.

4.2.2
Girsanovâ€™s Theorem
197
Solution: By definition, the probability measures â„™and â„šare functions
â„™âˆ¶Î© î‚¶â†’[0, 1] and â„šâˆ¶Î© î‚¶â†’[0, 1].
Because â„šis absolutely continuous with respect to â„™on â„±then, from the
Radonâ€“NikodÂ´ym theorem,
dâ„š
dâ„™âˆ¶Î© î‚¶â†’(0, âˆ) and â„™
(dâ„š
dâ„™> 0
)
= 1.
Furthermore,
ğ”¼â„™
[
dâ„š
dâ„™
]
= âˆ«Î©
dâ„š
dâ„™â‹…dâ„™= âˆ«Î©
dâ„š= 1.
Given dâ„š
dâ„™
||||â„±t
= Zt we can therefore deduce that Zt > 0 for all 0 â‰¤t â‰¤T. To show that Zt
is a positive martingale we have the following:
(a) Under the filtration â„±t we define the Radonâ€“NikodÂ´ym derivative as
Zt = ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
.
For 0 â‰¤s â‰¤t â‰¤T, and using the tower property of conditional expectation (see Prob-
lem 1.2.3.11, page 46),
ğ”¼â„™(Zt|| â„±s
) = ğ”¼â„™
[
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)|||||
â„±s
]
= ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±s
)
= Zs.
(b) Since dâ„š
dâ„™> 0, so |Zt| =
|||||
(
dâ„š
dâ„™
||||â„±t
)|||||
= dâ„š
dâ„™
||||â„±t
. Therefore, for 0 â‰¤t â‰¤T and using
the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
ğ”¼â„™(|Zt|) = ğ”¼â„™
[
dâ„š
dâ„™
||||
â„±t
]
ğ”¼â„™[ğ”¼â„™(||Zt||
)] = ğ”¼â„™
[
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)]
we have
ğ”¼â„™(||Zt||
) = ğ”¼â„™(dâ„š
dâ„™
)
= 1 < âˆ.
(c) Zt is clearly â„±t-adapted for 0 â‰¤t â‰¤T.
From the results of (a)â€“(c) we have shown that Zt is a positive â„™-martingale.
â—½

198
4.2.2
Girsanovâ€™s Theorem
4. Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe another
probability measure on (Î©, â„±, â„š) such that â„šis absolutely continuous with respect to â„™
on â„±. Let dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, so that Zt is a positive â„™-martingale. By letting
{Xt âˆ¶0 â‰¤t â‰¤T} be an â„±t measurable random variable, show using the Radonâ€“NikodÂ´ym
theorem that
ğ”¼â„š(Xt
) = ğ”¼â„™
[
Xt
(dâ„š
dâ„™
||||â„±t
)]
and hence deduce that for A âˆˆâ„±t,
âˆ«A
Xt dâ„š= âˆ«A
Xt
(
dâ„š
dâ„™
||||â„±t
)
dâ„™.
Solution: From the definition of the Radonâ€“NikodÂ´ym derivative dâ„š
dâ„™= Z where Z is a
positive random variable, we can write
âˆ«Î©
Xt dâ„š= âˆ«Î©
XtZ dâ„™
ğ”¼â„š(Xt
) = ğ”¼â„™(XtZ) .
Using the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
ğ”¼â„š(
Xt
)
= ğ”¼â„™(
XtZ
)
= ğ”¼â„™[ğ”¼â„™(XtZ|â„±t
)]
= ğ”¼â„™[Xtğ”¼â„™(Z|â„±t
)] .
Under the filtration â„±t, we define the Radonâ€“NikodÂ´ym derivative as
Zt = ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
and therefore
ğ”¼â„š(Xt
) = ğ”¼â„™(XtZt
)
or
ğ”¼â„š(Xt
) = ğ”¼â„™
[
Xt
(dâ„š
dâ„™
||||â„±t
)]
.
By defining
1IA =
{
1
if A âˆˆâ„±t
0
otherwise
and if Xt is â„±t measurable, then for any A âˆˆâ„±t,
ğ”¼â„š(1IAXt
) = ğ”¼â„™(1IAXtZt
)

4.2.2
Girsanovâ€™s Theorem
199
which is equivalent to
âˆ«A
Xt dâ„š= âˆ«A
XtZt dâ„™
or
âˆ«A
Xt dâ„š= âˆ«A
Xt
(
dâ„š
dâ„™
||||â„±t
)
dâ„™.
â—½
5. Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe another
probability measure on (Î©, â„±, â„š) such that â„šis absolutely continuous with respect to â„™
on â„±. Let dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, so that Zt is a positive â„™-martingale. By letting
{Xt âˆ¶0 â‰¤t â‰¤T} be an â„±t measurable random variable, show using the Radonâ€“NikodÂ´ym
theorem that
ğ”¼â„™(Xt
) = ğ”¼â„š
[
Xt
(dâ„š
dâ„™
||||â„±t
)âˆ’1]
and hence deduce that for A âˆˆâ„±t,
âˆ«A
Xt dâ„™= âˆ«A
Xt
(
dâ„š
dâ„™
||||â„±t
)âˆ’1
dâ„š.
Solution: From the definition of the Radonâ€“NikodÂ´ym derivative dâ„š
dâ„™= Z where Z is a
positive random variable, we can write
âˆ«Î©
Xt dâ„™= âˆ«Î©
XtZâˆ’1dâ„š
ğ”¼â„™(Xt
) = ğ”¼â„š(XtZâˆ’1) .
Using the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
ğ”¼â„™(Xt
) = ğ”¼â„š(XtZâˆ’1)
= ğ”¼â„š[ğ”¼â„š(XtZâˆ’1|â„±t
)]
= ğ”¼â„š[Xtğ”¼â„š(Zâˆ’1|â„±t
)] .
Under the filtration â„±t, we define the Radonâ€“NikodÂ´ym derivative as
Zt = ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
and therefore
ğ”¼â„™(Xt
) = ğ”¼â„š(XtZâˆ’1
t
)
or
ğ”¼â„™(Xt
) = ğ”¼â„š
[
Xt
(dâ„š
dâ„™
||||â„±t
)âˆ’1]
.

200
4.2.2
Girsanovâ€™s Theorem
By defining
1IA =
{
1
if A âˆˆâ„±t
0
otherwise
and if Xt is â„±t measurable, then for any A âˆˆâ„±t,
ğ”¼â„™(1IAXt
) = ğ”¼â„š(1IAXtZâˆ’1
t
)
which is equivalent to
âˆ«A
Xt dâ„™= âˆ«A
XtZâˆ’1
t dâ„š
or
âˆ«A
Xt dâ„™= âˆ«A
Xt
(
dâ„š
dâ„™
||||â„±t
)âˆ’1
dâ„š.
â—½
6. Let {Wt âˆ¶0 â‰¤t â‰¤T} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™)
and let â„±t, 0 â‰¤t â‰¤T be the associated Wiener process filtration. By defining
dâ„š
dâ„™
||||â„±t
= Zt = eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
udu
where for all 0 â‰¤t â‰¤T the adapted process ğœƒt satisfies ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ, show that â„š
is a probability measure.
Solution: From Problem 4.2.2.4 (page 198) we can deduce that for A âˆˆâ„±t,
âˆ«A
dâ„š= âˆ«A
Zt dâ„™
or
â„š(A) = ğ”¼â„™(Zt1IA
)
where
1IA =
{
1
if A âˆˆâ„±t
0
otherwise.
In addition, from Problem 4.2.2.1 (page 194) we can deduce that the adapted process ğœƒt
follows âˆ«
t
0
ğœƒs dWs âˆ¼ğ’©
(
0, âˆ«
t
0
ğœƒ2
s ds
)
so that
ğ”¼â„™(Zt) = eâˆ’1
2 âˆ«t
0 ğœƒ2
uduğ”¼â„™(
eâˆ’âˆ«t
0 ğœƒudWu
)
= eâˆ’1
2 âˆ«t
0 ğœƒ2
udu â‹…e
1
2 âˆ«t
0 ğœƒ2
udu = 1.
Since â„™is a probability measure and based on the above results, â„šis also a probability
measure because

4.2.2
Girsanovâ€™s Theorem
201
(a) â„š(âˆ…) = ğ”¼â„™(Zt1Iâˆ…
) = ğ”¼â„™(Zt)â„™(âˆ…) = 0 since â„™(âˆ…) = 0.
(b) â„š(Î©) = ğ”¼â„™(Zt1IÎ©) = ğ”¼â„™(Zt)â„™(Î©) = 1 since â„™(Î©) = 1 and ğ”¼â„™(Zt) = 1.
(c) For A1, A2, . . . âˆˆâ„±t, Ai âˆ©Aj = âˆ…, i â‰ j, i, j âˆˆ{1, 2, . . .}
âˆ
â‹ƒ
i=1
â„š(Ai
) =
âˆ
â‹ƒ
i=1
ğ”¼â„™(
Zt1IAi
)
=
âˆ
â‹ƒ
i=1
ğ”¼â„™(Zt
) â„™(Ai
)
and since â‹ƒâˆ
i=1 â„™(Ai) = âˆ‘âˆ
i=1 â„™(Ai),
âˆ
â‹ƒ
i=1
â„š(Ai
) =
âˆ
âˆ‘
i=1
ğ”¼â„™(Zt
) â„™(Ai
) =
âˆ
âˆ‘
i=1
ğ”¼â„™(
Zt1IAi
)
=
âˆ
âˆ‘
i=1
â„š(Ai).
â—½
7. Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe another
probability measure on (Î©, â„±, â„š) such that â„šis absolutely continuous with respect to â„™
on â„±. Let dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, so that Zt is a positive â„™-martingale. By letting
{Xt âˆ¶0 â‰¤t â‰¤T} be an â„±t measurable random variable and using the partial averaging
property given as
âˆ«A
ğ”¼â„™(Y|ğ’¢) dâ„™= âˆ«A
Ydâ„™,
for all
A âˆˆğ’¢
where Y is a random variable on the probability space (Î©, â„±, â„™) and ğ’¢is a sub-ğœ-algebra
of â„±show that for 0 â‰¤s â‰¤t â‰¤T,
ğ”¼â„š(Xt|â„±s
) =
(
dâ„š
dâ„™
||||â„±s
)âˆ’1
ğ”¼â„™
[
Xt
(
dâ„š
dâ„™
||||â„±t
)|||||
â„±s
]
.
Solution: First, note that
(
dâ„š
dâ„™
||||â„±s
)âˆ’1
ğ”¼â„™
[
Xt
(
dâ„š
dâ„™
||||â„±t
)|||||
â„±s
]
= Zâˆ’1
s ğ”¼â„™(XtZt|â„±s
) is
â„±s-measurable. For any A âˆˆâ„±s, and using the results of Problem 4.2.2.4 (page 198) as
well as partial averaging, we have
âˆ«A
Zâˆ’1
s ğ”¼â„™(XtZt|â„±s
) dâ„š= âˆ«A
Zâˆ’1
s ğ”¼â„™(XtZt|â„±s
) â‹…Zs dâ„™
= âˆ«A
XtZt dâ„™
= âˆ«A
Xt dâ„š.
Using the partial averaging once again, we have
âˆ«A
Xt dâ„š= âˆ«A
ğ”¼â„š(Xt|â„±s
) dâ„š.

202
4.2.2
Girsanovâ€™s Theorem
Therefore,
âˆ«A
Zâˆ’1
s ğ”¼â„™(XtZt|â„±s
) dâ„š= âˆ«A
ğ”¼â„š(Xt|â„±s
) dâ„š
or
Zâˆ’1
s ğ”¼â„™(XtZt|â„±s
) = ğ”¼â„š(Xt|â„±s
) .
By substituting Zt = dâ„š
dâ„™
||||â„±t
we have
ğ”¼â„š(Xt|â„±s) =
(
dâ„š
dâ„™
||||â„±s
)âˆ’1
ğ”¼â„™
[
Xt
(
dâ„š
dâ„™
||||â„±t
)|||||
â„±s
]
.
â—½
8. Let (Î©, â„±, â„™) be the probability space satisfying the usual conditions. Let â„šbe another
probability measure on (Î©, â„±, â„š) such that â„šis absolutely continuous with respect to â„™
on â„±. Let dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, so that Zt is a positive â„™-martingale. By letting
{Xt âˆ¶0 â‰¤t â‰¤T} be an â„±t measurable random variable and using the partial averaging
property given as
âˆ«A
ğ”¼â„™(Y|ğ’¢) dâ„™= âˆ«A
Y dâ„™,
for all
A âˆˆğ’¢
where Y is a random variable on the probability space (Î©, â„±, â„™) and ğ’¢is a sub-ğœ-algebra
of â„±show that for 0 â‰¤s â‰¤t â‰¤T,
ğ”¼â„™(Xt|â„±s
) =
(
dâ„š
dâ„™
||||â„±s
)
ğ”¼â„š
[
Xt
(
dâ„š
dâ„™
||||â„±t
)âˆ’1|||||
â„±s
]
.
Solution: First, note that
(
dâ„š
dâ„™
||||â„±s
)
ğ”¼â„š
[
Xt
(
dâ„š
dâ„™
||||â„±t
)âˆ’1|||||
â„±s
]
= Zsğ”¼â„š(XtZâˆ’1
t |â„±s
) is
â„±s measurable. For any A âˆˆâ„±s, and using the results of Problem 4.2.2.5 (page 199) as
well as partial averaging, we have
âˆ«A
Zsğ”¼â„š(XtZâˆ’1
t |â„±s
) dâ„™= âˆ«A
Zsğ”¼â„š(XtZâˆ’1
t |â„±s
) â‹…Zâˆ’1
s dâ„š
= âˆ«A
XtZâˆ’1
t dâ„š
= âˆ«A
Xt dâ„™.
Using the partial averaging once again, we have
âˆ«A
Xt dâ„™= âˆ«A
ğ”¼â„™(Xt|â„±s
) dâ„™.

4.2.2
Girsanovâ€™s Theorem
203
Therefore,
âˆ«A
Zsğ”¼â„š(XtZâˆ’1
t |â„±s
) dâ„™= âˆ«A
ğ”¼â„™(Xt|â„±s
) dâ„™
or
Zsğ”¼â„š(XtZâˆ’1
t |â„±s
) = ğ”¼â„™(Xt|â„±s
) .
By substituting Zt = dâ„š
dâ„™
||||â„±t
we have
ğ”¼â„™(Xt|â„±s
) =
(
dâ„š
dâ„™
||||â„±s
)
ğ”¼â„š
[
Xt
(
dâ„š
dâ„™
||||â„±t
)âˆ’1|||||
â„±s
]
.
â—½
9. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™) and
suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T. We consider
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds
and if
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
By changing the measure â„™to a measure â„šsuch that dâ„š
dâ„™
||||â„±t
= Zt, show that
ÌƒWt = Wt + âˆ«
t
0
ğœƒudu
is a â„š-martingale.
Solution: The first part of the result is given in Problem 4.2.2.1 (page 194) or Problem
4.2.2.2 (page 196). To show that ÌƒWt is a â„š-martingale we note the following:
(a) Let 0 â‰¤s â‰¤t â‰¤T, under the filtration â„±s and using the result of Problem 4.2.2.7 (page
201), we have
ğ”¼â„š(
ÌƒWt||| â„±s
)
= 1
Zs
ğ”¼â„™(
ÌƒWtZt||| â„±s
)
.
From ItÂ¯oâ€™s formula and using the results of Problem 4.2.2.2 (page 196),
d
(
ÌƒWtZt
)
= ÌƒWtdZt + Ztd ÌƒWt + d ÌƒWtdZt
= ÌƒWt
(âˆ’ğœƒtZtdWt
) + Zt
(dWt + ğœƒtdt) + (dWt + ğœƒtdt) (âˆ’ğœƒtZtdWt
)
=
(
1 âˆ’ğœƒt ÌƒWt
)
ZtdWt.

204
4.2.2
Girsanovâ€™s Theorem
By integrating both sides of the equation from s to t, where s < t,
âˆ«
t
s
d
(
ÌƒWuZu
)
= âˆ«
t
s
(
1 âˆ’ğœƒu ÌƒWu
)
Zu dWu
ÌƒWtZt = ÌƒWsZs + âˆ«
t
s
(
1 âˆ’ğœƒu ÌƒWu
)
Zu dWu.
Under the filtration â„±s and because âˆ«
t
s
(
1 âˆ’ğœƒu ÌƒWu
)
Zu dWu âŸ‚âŸ‚â„±s,
ğ”¼â„™(
ÌƒWtZt||| â„±s
)
= ÌƒWsZs
where ğ”¼â„™
(
âˆ«
t
s
(
1 âˆ’ğœƒu ÌƒWu
)
Zu dWu
|||||
â„±s
)
= ğ”¼â„™
(
âˆ«
t
s
(
1 âˆ’ğœƒu ÌƒWu
)
Zu dWu
)
= 0.
Thus,
ğ”¼â„š(
ÌƒWt||| â„±s
)
= 1
Zs
ğ”¼â„™(
ÌƒWtZt||| â„±s
)
= 1
Zs
ÌƒWsZs = ÌƒWs.
(b) For 0 â‰¤t â‰¤T,
||| ÌƒWt||| =
|||||
Wt + âˆ«
t
0
ğœƒu du
|||||
â‰¤||Wt|| +
|||||âˆ«
t
0
ğœƒu du
|||||
.
Taking expectations under the â„šmeasure, using HÃ¶lderâ€™s inequality (see
Problem 1.2.3.2, page 41) and taking note that the positive random variable
Zt âˆ¼log-ğ’©
(
âˆ’1
2 âˆ«
t
0
ğœƒ2
s ds, âˆ«
t
0
ğœƒ2
s ds
)
under â„™, we have
ğ”¼â„š(||| ÌƒWt|||
)
â‰¤ğ”¼â„š(||Wt||
) +
|||||âˆ«
t
0
ğœƒu du
|||||
= ğ”¼â„™(||Wt|| Zt
) +
|||||âˆ«
t
0
ğœƒu du
|||||
= ğ”¼â„™(||WtZt||
) +
|||||âˆ«
t
0
ğœƒu du
|||||
â‰¤
âˆš
ğ”¼â„™(W2
t
) ğ”¼â„™(Z2
t
) +
|||||âˆ«
t
0
ğœƒu du
|||||
=
âˆš
teâˆ«t
0 ğœƒ2udu +
|||||âˆ«
t
0
ğœƒu du
|||||
< âˆ
since ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
udu)
< âˆ.

4.2.2
Girsanovâ€™s Theorem
205
(c) Because ÌƒWt is a function of Wt, it is â„±t-adapted.
From the results of (a)â€“(c), we have shown that ÌƒWt, 0 â‰¤t â‰¤T is a â„š-martingale.
â—½
10. One-Dimensional Girsanov Theorem. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process
on the probability space (Î©, â„±, â„™) and suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T. We
consider
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds
and if
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
By changing the measure â„™to a measure â„šsuch that dâ„š
dâ„™
||||â„±t
= Zt, show that
ÌƒWt = Wt + âˆ«
t
0
ğœƒu du
is a â„š-standard Wiener process.
Solution: The first part of the result is given in Problem 4.2.2.1 (page 194) or Problem
4.2.2.2 (page 196). As for the second part of the result, we can prove it using two methods.
Method 1. We first need to show that under the â„šmeasure, ÌƒWt âˆ¼ğ’©(0, t) and to do this we
rely on the moment generating function. By definition, for a constant ğœ“,
M ÌƒWt(ğœ“) = ğ”¼â„š(
eğœ“ÌƒWt
)
= ğ”¼â„™(
eğœ“ÌƒWtZt
)
= ğ”¼â„™(
eğœ“Wt+ğœ“âˆ«t
0 ğœƒs ds â‹…eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds)
= e
âˆ«t
0
(
ğœ“ğœƒsâˆ’1
2 ğœƒ2
s
)
ds ğ”¼â„™(
eğœ“Wtâˆ’âˆ«t
0 ğœƒsdWs
)
= e
âˆ«t
0
(
ğœ“ğœƒsâˆ’1
2 ğœƒ2
s
)
ds ğ”¼â„™(
eâˆ«t
0 ğœ“dWsâˆ’âˆ«t
0 ğœƒsdWs
)
= e
âˆ«t
0
(
ğœ“ğœƒsâˆ’1
2 ğœƒ2
s
)
ds ğ”¼â„™(
eâˆ«t
0(ğœ“âˆ’ğœƒs)dWs
)
.
Using the properties of the stochastic ItÂ¯o integral under the â„™-measure
âˆ«
t
0
(ğœ“âˆ’ğœƒs) dWs âˆ¼ğ’©
(
0, âˆ«
t
0
(ğœ“âˆ’ğœƒs
)2 ds
)
therefore
ğ”¼â„™(
eâˆ«t
0(ğœ“âˆ’ğœƒs)dWs
)
= e
1
2 âˆ«t
0 (ğœ“âˆ’ğœƒs)2 ds

206
4.2.2
Girsanovâ€™s Theorem
and hence
M ÌƒWt(ğœ“) = e
1
2 ğœ“2t
which is a moment generating function of a normal distribution with mean zero and vari-
ance t.
Thus, under the â„šmeasure, ÌƒWt âˆ¼ğ’©(0, t).
Finally, to show that ÌƒWt, 0 â‰¤t â‰¤T is a â„š-standard Wiener process we have the following:
(a) ÌƒW0 = 0 and ÌƒWt certainly has continuous sample paths for t > 0.
(b) For t > 0 and s > 0, ÌƒWt+s âˆ’ÌƒWt âˆ¼ğ’©(0, s) since
ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt
)
= ğ”¼â„š(
ÌƒWt+s
)
âˆ’ğ”¼â„š(
ÌƒWt
)
= 0
and from the results of Problem 2.2.1.4 (page 57),
Varâ„š(
ÌƒWt+s âˆ’ÌƒWt
)
= Varâ„š(
ÌƒWt+s
)
+ Varâ„š(
ÌƒWt
)
âˆ’2Covâ„š(
ÌƒWt+s, ÌƒWt
)
= t + s + t âˆ’2min{t + s, t}
= s.
(c) Because ÌƒWt is a â„š-martingale (see Problem 4.2.2.9, page 203), then for t > 0, s > 0
and under the filtration â„±t,
ğ”¼â„š(
ÌƒWt+s||| â„±t
)
= ÌƒWt.
Since we can write
ğ”¼â„š(
ÌƒWt+s|||â„±t
)
= ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt + ÌƒWt||| â„±t
)
= ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt||| â„±t
)
+ ğ”¼â„š(
ÌƒWt||| â„±t
)
= ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt||| â„±t
)
+ ÌƒWt
then
ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt||| â„±t
)
= ğ”¼â„š(
ÌƒWt+s âˆ’ÌƒWt
)
= 0
which implies ÌƒWt+s âˆ’ÌƒWt âŸ‚âŸ‚â„±t. Therefore, ÌƒWt+s âˆ’ÌƒWt âŸ‚âŸ‚ÌƒWt.
From the results of (a)â€“(c) we have shown ÌƒWt, 0 â‰¤t â‰¤T is a â„š-standard Wiener process.
Method 2. The process ÌƒWt, 0 â‰¤t â‰¤T is a â„š-martingale with the following properties:
(i) ÌƒW0 = 0 and has continuous paths for t > 0.
(ii) By setting ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tnâˆ’1 < tn = t, n âˆˆâ„•the quadratic vari-
ation of ÌƒWt is
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=1
(
ÌƒWti+1 âˆ’ÌƒWti
)2
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=1
(
Wti+1 âˆ’Wti + âˆ«
ti+1
0
ğœƒu du âˆ’âˆ«
ti
0
ğœƒu du
)2
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=1
(
Wti+1 âˆ’Wti
)2

4.2.2
Girsanovâ€™s Theorem
207
+ lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=1
2
(
Wti+1 âˆ’Wti
) (
âˆ«
ti+1
ti
ğœƒu du
)
+ lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=1
(
âˆ«
ti+1
ti
ğœƒu du
)2
= t
since the quadratic variation of Wt is t and lim
nâ†’âˆâˆ«
ti+1
ti
ğœƒu du = 0. Informally, we can
write d ÌƒWtd ÌƒWt = dt.
Based on properties (i) and (ii), and from the LÃ©vy characterisation theorem (see Problem
3.2.1.15, page 119), we can deduce that ÌƒWt, 0 â‰¤t â‰¤T is a â„š-standard Wiener process.
â—½
11. Multi-Dimensional
Novikov
Condition.
Let
Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be
an
n-dimensional â„™-standard Wiener process, with {W(i)
t
âˆ¶0 â‰¤t â‰¤T}, i = 1, 2, . . . , n
an independent one-dimensional â„™-standard Wiener process on the probability space
(Î©, â„±, â„™). Suppose we have an n-dimensional adapted process ğœ½t = (ğœƒ(1)
t , ğœƒ(2)
t , . . . , ğœƒ(n)
t )T,
0 â‰¤t â‰¤T. By considering
Zt = exp
{
âˆ’âˆ«
t
0
ğœ½u â‹…dWu âˆ’1
2 âˆ«
t
0
â€–â€–ğœ½uâ€–â€–
2
2 du
}
and if
ğ”¼â„™
(
exp
{
1
2 âˆ«
T
0
â€–â€–ğœ½tâ€–â€–
2
2 dt
})
< âˆ
where
||ğœ½t||2 =
âˆš
(ğœƒ(1)
t )2 + (ğœƒ(2)
t )2 + . . . + (ğœƒ(n)
t )2
show that Zt is a positive â„™-martingale.
Solution: To show that Zt is a positive â„™-martingale, we have the following.
(a) We can write Zt as
Zt = exp
{
âˆ’âˆ«
t
0
ğœ½u â‹…dWu âˆ’1
2 âˆ«
t
0
â€–â€–ğœ½uâ€–â€–
2
2 du
}
= exp
{
âˆ’âˆ«
t
0
n
âˆ‘
i=1
ğœƒ(i)
u dW(i)
u âˆ’1
2 âˆ«
t
0
n
âˆ‘
i=1
(ğœƒ(i)
u )2du
}
= exp
{ n
âˆ‘
i=1
(
âˆ’âˆ«
t
0
ğœƒ(i)
u dW(i)
u âˆ’1
2 âˆ«
t
0
(ğœƒ(i)
u )2du
)}

208
4.2.2
Girsanovâ€™s Theorem
=
n
âˆ
i=1
exp
{
âˆ’âˆ«
t
0
ğœƒ(i)
u dW(i)
u âˆ’1
2 âˆ«
t
0
(ğœƒ(i)
u )2du
}
=
n
âˆ
i=1
Z(i)
t
where Z(i)
t
= eâˆ’âˆ«t
0 ğœƒ(i)
u dW(i)
u âˆ’1
2 âˆ«t
0 (ğœƒ(i)
u )2du. From Problem 4.2.2.1 (page 194), we have
shown that Z(i)
t is a positive â„™-martingale and since {W(i)
t
âˆ¶0 â‰¤t â‰¤T}, i = 1, 2, . . . , n
is an independent one-dimensional â„™-standard Wiener process on the probability
space (Î©, â„±, â„™), so for 0 â‰¤s â‰¤t,
ğ”¼â„™(Zt|â„±s
) = ğ”¼â„™
(
n
âˆ
i=1
Z(i)
t
|||||
â„±s
)
=
n
âˆ
i=1
ğ”¼â„™(
Z(i)
t
||| â„±s
)
=
n
âˆ
i=1
Z(i)
s = Zs.
(b) Furthermore, because ğ”¼â„™(|Z(i)
t |) < âˆ, hence
ğ”¼â„™(||Zt||
) = ğ”¼â„™
(|||||
n
âˆ
i=1
Z(i)
t
|||||
)
â‰¤ğ”¼â„™
( n
âˆ
i=1
|||Z(i)
t
|||
)
â‰¤
n
âˆ
i=1
ğ”¼â„™(|||Z(i)
t
|||
)
< âˆ.
(c) Zt is clearly â„±t-adapted for 0 â‰¤t â‰¤T.
From the results of (a)â€“(c) and since Zt > 0, we have shown that Zt is a positive
â„™-martingale.
â—½
12. Multi-Dimensional
Girsanov
Theorem.
Let
Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be
an
n-dimensional â„™-standard Wiener process, with {W(i)
t
âˆ¶0 â‰¤t â‰¤T}, i = 1, 2, . . . , n
an independent one-dimensional â„™-standard Wiener process on the probability space
(Î©, â„±, â„™). Suppose we have an n-dimensional adapted process ğœ½t = (ğœƒ(1)
t , ğœƒ(2)
t , . . . , ğœƒ(n)
t )T,
0 â‰¤t â‰¤T. By considering
Zt = exp
{
âˆ’âˆ«
t
0
ğœ½u â‹…dWu âˆ’1
2 âˆ«
t
0
â€–â€–ğœ½uâ€–â€–
2
2 du
}
and if
ğ”¼â„™
(
exp
{
1
2 âˆ«
T
0
â€–â€–ğœ½tâ€–â€–
2
2 dt
})
< âˆ
where
||ğœ½t||2 =
âˆš
(ğœƒ(1)
t )2 + (ğœƒ(2)
t )2 + . . . + (ğœƒ(n)
t )2
show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
By changing the measure â„™to a measure â„šsuch that dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, show
that
Ìƒ
Wt = Wt + âˆ«
t
0
ğœ½u du

4.2.2
Girsanovâ€™s Theorem
209
is an n-dimensional â„š-standard Wiener process where Ìƒ
Wt = ( ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t )T and
ÌƒW(i)
t
= W(i)
t
+ âˆ«t
0 ğœƒ(i)
t du, i = 1, 2, . . . , n such that the component processes of Ìƒ
Wt are inde-
pendent under â„š.
Solution: The first part of the result is given in Problem 4.2.2.11 (page 207).
As for the second part of the result, we note that given Ìƒ
Wt = ( ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t )T, and
following the steps given in Problem 4.2.2.9 (page 203), we can easily show that for each
i = 1, 2, . . . , n,
ÌƒW(i)
t
= W(i)
t
+ âˆ«
t
0
ğœƒ(i)
u du
is a â„š-martingale. Because ÌƒW(i)
0 = 0, and has continuous sample paths with quadratic
variation equal to t (see Problem 4.2.2.10, page 205), then from the multi-dimensional
LÃ©vy characterisation theorem (see Problem 3.2.1.16, page 121) we can show that ÌƒW(i)
t ,
i = 1, 2, . . . , n is a â„š-standard Wiener process. Finally, since W(i)
t
âŸ‚âŸ‚W(j)
t , i â‰ j, i, j =
1, 2, . . . , n we have
d ÌƒW(i)
t
â‹…d ÌƒW(j)
t
=
(
dW(i)
t
+ ğœƒ(i)
t dt
) (
dW(j)
t
+ ğœƒ(j)
t dt
)
= dW(i)
t dW(j)
t
+ ğœƒ(j)
t dW(i)
t dt + ğœƒ(i)
t dW(j)
t dt + ğœƒ(i)
t ğœƒ(j)
t dt2
= 0.
Thus, from the multi-dimensional LÃ©vy characterisation theorem (see Problem 3.2.1.16,
page 121) we can deduce that Ìƒ
Wt, 0 â‰¤t â‰¤T is an n-dimensional â„š-standard Wiener pro-
cess such that each component process of Ìƒ
Wt is independent under â„š.
â—½
13. Convergence Issue of Equivalent Measures. Let Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T be an
n-dimensional â„™-standard Wiener process, with {W(i)
t
âˆ¶0 â‰¤t â‰¤T}, i = 1, 2, . . . , n
an independent one-dimensional â„™-standard Wiener process on the probability space
(Î©, â„±, â„™). Suppose we have an n-dimensional adapted process ğœ½t = (ğœƒ(1)
t , ğœƒ(2)
t , . . . , ğœƒ(n)
t )T,
0 â‰¤t â‰¤T. We consider
Zt = exp
{
âˆ’âˆ«
t
0
ğœ½u â‹…dWu âˆ’1
2 âˆ«
t
0
â€–â€–ğœ½uâ€–â€–
2
2 du
}
and if
ğ”¼â„™
(
exp
{
1
2 âˆ«
T
0
â€–â€–ğœ½tâ€–â€–
2
2 dt
})
< âˆ
where
||ğœ½t||2 =
âˆš
(ğœƒ(1)
t )2 + (ğœƒ(2)
t )2 + . . . + (ğœƒ(n)
t )2
show that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.

210
4.2.2
Girsanovâ€™s Theorem
By changing the measure â„™to a measure â„šsuch that dâ„š
dâ„™
||||â„±t
= Zt for all 0 â‰¤t â‰¤T, show
that
Ìƒ
Wt = Wt + âˆ«
t
0
ğœ½u du
is an n-dimensional â„š-standard Wiener process where Ìƒ
Wt = ( ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t )T and
ÌƒW(i)
t
= W(i)
t
+ âˆ«t
0 ğœƒ(i)
t du, i = 1, 2, . . . , n such that the component processes of Ìƒ
Wt are inde-
pendent under â„š. Let X(i) = âˆ«
t
0
ğœƒ(i)
u dW(i)
u , i = 1, 2, . . . , n be a sequence of random vari-
ables on the probability space (Î©, â„±, â„™) and assume ğœƒ(1)
t
= ğœƒ(2)
t
= . . . = ğœƒ(n)
t
= ğœƒt where
ğœƒt is an adapted process, 0 â‰¤t â‰¤T. For n â†’âˆshow that under the â„™measure
â„™
(
lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
X(i) = 0
)
= 1
and under the equivalent â„šmeasure
â„š
(
lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
X(i) = 0
)
= 0.
Discuss the implications on the equivalent measures â„™and â„š.
Solution: The first two results are given in Problem 4.2.2.12 (page 208).
As for the final result, because ğœƒ(1)
t
= ğœƒ(2)
t
= . . . = ğœƒ(n)
t
= ğœƒt then under the â„™measure we
can deduce that
X(i) = âˆ«
t
0
ğœƒ(i)
u dW(i)
t
âˆ¼ğ’©
(
0, âˆ«
t
0
ğœƒ2
u du
)
for i = 1, 2, . . . , n. Because {W(i)
t
âˆ¶0 â‰¤t â‰¤T}, i = 1, 2, . . . , n are independent and iden-
tically, distributed,
1
n
n
âˆ‘
i=1
X(i) = 1
n
n
âˆ‘
i=1
{
âˆ«
t
0
ğœƒ(i)
u dW(i)
t
}
âˆ¼ğ’©
(
0, 1
n âˆ«
t
0
ğœƒ2
u du
)
or
1
n
n
âˆ‘
i=1
X(i) = 1
n
n
âˆ‘
i=1
{
âˆ«
t
0
ğœƒ(i)
u dW(i)
t
}
= ğœ™
âˆš
1
n âˆ«
t
0
ğœƒ2
u du,
ğœ™âˆ¼ğ’©(0, 1).
Taking limits n â†’âˆ, we have
â„™
(
lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
X(i) = 0
)
= 1.

4.2.2
Girsanovâ€™s Theorem
211
On the contrary, under the measure â„šfor i = 1, 2, . . . , n,
ÌƒW(i)
t
= W(i)
t
+ âˆ«
t
0
ğœƒ(i)
u du
is a â„š-standard Wiener process and ÌƒW(1)
t , ÌƒW(2)
t , . . . , ÌƒW(n)
t
are independent and identically
distributed. Thus,
d ÌƒW(i)
t
= dW(i)
t
+ ğœƒ(i)
t dt
and in turn we can write
âˆ«
t
0
ğœƒ(i)
u dW(i)
u = âˆ«
t
0
ğœƒ(i)
u d ÌƒW(i)
u âˆ’âˆ«
t
0
(ğœƒ(i)
u )2 du.
Because ğœƒ(1)
t
= ğœƒ(2)
t
= . . . = ğœƒ(n)
t
= ğœƒt and under the â„šmeasure,
X(i) = âˆ«
t
0
ğœƒ(i)
u dW(i)
u âˆ¼ğ’©
(
âˆ’âˆ«
t
0
ğœƒ2
u du, âˆ«
t
0
ğœƒ2
u du
)
.
Since { ÌƒW(i)
t }0â‰¤tâ‰¤T, i = 1, 2, . . . , n are independent and identically distributed,
1
n
n
âˆ‘
i=1
X(i) = 1
n
n
âˆ‘
i=1
{
âˆ«
t
0
ğœƒ(i)
u dW(i)
t
}
âˆ¼ğ’©
(
âˆ’âˆ«
t
0
ğœƒ2
u du, 1
n âˆ«
t
0
ğœƒ2
u du
)
or
1
n
n
âˆ‘
i=1
X(i) = 1
n
n
âˆ‘
i=1
{
âˆ«
t
0
ğœƒ(i)
u dW(i)
t
}
= âˆ’âˆ«
t
0
ğœƒ2
u du + ğœ™
âˆš
1
n âˆ«
t
0
ğœƒ2
u du,
ğœ™âˆ¼ğ’©(0, 1).
By setting n â†’âˆ,
â„š
(
lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
X(i) = 0
)
= 0.
Therefore, in the limit n â†’âˆ, the measures â„™and â„šfail to be equivalent.
â—½
14. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™). By
considering a Wiener process with drift
Ì‚Wt = ğ›¼t + Wt
and by using Girsanovâ€™s theorem to find a measure â„šunder which Ì‚Wt is a standard Wiener
process, show that the joint density of
Ì‚Wt
and
Ì‚Mt = max
0â‰¤sâ‰¤t
Ì‚Ws

212
4.2.2
Girsanovâ€™s Theorem
under â„™is
f â„™
Ì‚Mt, Ì‚Wt(x, ğ‘¤) =
â§
âª
â¨
âªâ©
2(2xâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eğ›¼ğ‘¤âˆ’1
2 ğ›¼2tâˆ’1
2t (2xâˆ’ğ‘¤)2
x â‰¥0, ğ‘¤â‰¤x
0
otherwise.
Finally, deduce that the joint density of
Ì‚Wt
and
Ì‚mt = min
0â‰¤sâ‰¤t
Ì‚Ws
under â„™is
f â„™
Ì‚mt, Ì‚Wt(y, ğ‘¤) =
â§
âª
â¨
âªâ©
âˆ’2(2yâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eğ›¼ğ‘¤âˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘¤)2
y â‰¤0, y â‰¤ğ‘¤
0
otherwise.
Solution: By defining
Ì‚Wt = ğ›¼t + Wt
and
Ì‚Mt = max
0â‰¤sâ‰¤t
Ì‚Ws
using ItÂ¯oâ€™s formula we can write
d Ì‚Wt = d ÌƒWt
where ÌƒWt = Wt + âˆ«t
0 ğ›¼du. From Girsanovâ€™s theorem there exists an equivalent probability
measure â„šon the filtration â„±s, 0 â‰¤s â‰¤t defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğ›¼dWuâˆ’1
2 âˆ«s
0 ğ›¼2du
= eâˆ’ğ›¼Wsâˆ’1
2 ğ›¼2s
= eâˆ’ğ›¼( Ì‚Wsâˆ’ğ›¼s)âˆ’1
2 ğ›¼2s
= eâˆ’ğ›¼Ì‚Ws+ 1
2 ğ›¼2s
so that Ì‚Wt is a â„š-standard Wiener process.
From Problem 2.2.5.4 (page 86), under â„š, the joint probability distribution of ( Ì‚Mt, Ì‚Wt)
can be written as
f â„š
Ì‚Mt, Ì‚Wt
(x, ğ‘¤) =
â§
âª
â¨
âªâ©
2(2xâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eâˆ’1
2t (2xâˆ’ğ‘¤)2
x â‰¥0, ğ‘¤â‰¤x
0
otherwise.
In order to find the joint cumulative distribution function of ( Ì‚Mt, Ì‚Wt) under â„™, we use the
result given in Problem 4.2.2.5 (page 199)
â„™
(
Ì‚Mt â‰¤x, Ì‚Wt â‰¤ğ‘¤
)
= ğ”¼â„™(
1I{ Ì‚Mtâ‰¤x, Ì‚Wtâ‰¤ğ‘¤}
)
= ğ”¼â„š(
Zâˆ’1
t 1I{ Ì‚Mtâ‰¤x, Ì‚Wtâ‰¤ğ‘¤}
)

4.2.2
Girsanovâ€™s Theorem
213
= ğ”¼â„š(
eğ›¼Ì‚Wtâˆ’1
2 ğ›¼2t1I{ Ì‚Mtâ‰¤x, Ì‚Wtâ‰¤ğ‘¤}
)
= âˆ«
ğ‘¤
âˆ’âˆâˆ«
x
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tf â„š
Ì‚Mt, Ì‚Wt
(u, ğ‘£) dudğ‘£.
Since Ì‚W0 = 0, Ì‚Mt â‰¥0 therefore Ì‚Wt â‰¤Ì‚Mt. By definition, the joint density of ( Ì‚Mt, Ì‚Wt) under
â„™is
f â„™
Ì‚Mt, Ì‚Wt(x, ğ‘¤) =
ğœ•2
ğœ•xğœ•ğ‘¤â„™
(
Ì‚Mt â‰¤x, Ì‚Wt â‰¤ğ‘¤
)
which implies
f â„™
Ì‚Mt, Ì‚Wt(x, ğ‘¤) =
{
2(2xâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eğ›¼ğ‘¤âˆ’1
2 ğ›¼2tâˆ’1
2t (2xâˆ’ğ‘¤)2 y â‰¥0, ğ‘¤â‰¤x
0
otherwise.
For the case of the joint density of
Ì‚Wt = ğ›¼t + Wt
and
Ì‚mt = min
0â‰¤sâ‰¤t
Ì‚Ws
from Problem 2.2.5.5 (page 88), the joint probability distribution of (Ì‚mt, Ì‚Wt) under â„šis
f â„š
Ì‚mt, Ì‚Wt
(y, ğ‘¤) =
â§
âª
â¨
âªâ©
âˆ’2(2yâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eâˆ’1
2t (2yâˆ’ğ‘¤)2
y â‰¤0, y â‰¤ğ‘¤
0
otherwise.
Using the same techniques as discussed earlier, the joint cumulative distribution of (Ì‚mt, Ì‚Wt)
under â„™is
â„™
(
Ì‚mt â‰¤y, Ì‚Wt â‰¤ğ‘¤
)
= ğ”¼â„™(
1I{Ì‚mtâ‰¤y, Ì‚Wtâ‰¤ğ‘¤}
)
= ğ”¼â„š(
Zâˆ’1
t 1I{Ì‚mtâ‰¤y, Ì‚Wtâ‰¤ğ‘¤}
)
= ğ”¼â„š(
eğ›¼Ì‚Wtâˆ’1
2 ğ›¼2t1I{Ì‚mtâ‰¤x, Ì‚Wtâ‰¤ğ‘¤}
)
= âˆ«
ğ‘¤
âˆ’âˆâˆ«
y
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tf â„š
Ì‚mt, Ì‚Wt
(u, ğ‘£) dudğ‘£.
Therefore,
f â„™
Ì‚mt, Ì‚Wt(y, ğ‘¤) =
ğœ•2
ğœ•yğœ•ğ‘¤â„™
(
Ì‚mt â‰¤y, Ì‚Wt â‰¤ğ‘¤
)
=
â§
âª
â¨
âªâ©
âˆ’2(2yâˆ’ğ‘¤)
t
âˆš
2ğœ‹t eğ›¼ğ‘¤âˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘¤)2
y â‰¤0, y â‰¤ğ‘¤
0
otherwise.
â—½

214
4.2.2
Girsanovâ€™s Theorem
15. Running Maximum and Minimum of a Wiener Process. Let {Wt âˆ¶t â‰¥0} be a â„™-standard
Wiener process on the probability space (Î©, â„±, â„™) and let
Xt = ğœˆ+ ğœ‡t + ğœWt
be a Wiener process starting from ğœˆâˆˆâ„with drift ğœ‡âˆˆâ„and volatility ğœ> 0. Let the
running maximum of the process Xt up to time t be defined as
MX
t = max
0â‰¤sâ‰¤t Xs.
Using Girsanovâ€™s theorem to find a measure â„šunder which Xt is a standard Wiener pro-
cess, show that the cumulative distribution function of the running maximum is
â„™(MX
t â‰¤x) = Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
âˆ’e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
âˆ’x + ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
,
x â‰¥ğœˆ
where Î¦(â‹…) is the standard normal cumulative distribution function.
Finally, deduce that the cumulative distribution function of the running minimum
mX
t = min
0â‰¤sâ‰¤t Xs
is
â„™(mX
t â‰¤x) = Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
+ e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
x âˆ’ğœˆ+ ğœ‡t
ğœ
âˆš
t
)
,
x â‰¤ğœˆ.
Find the probability density functions for MX
t and mX
t .
Solution: Let Yt = Xt âˆ’ğœˆ
ğœ
such that
Yt = ğ›¼t + Wt
where ğ›¼= ğœ‡âˆ•ğœ. Following this, we can define
MY
t = max
0â‰¤sâ‰¤t Ys
to be the running maximum of Yt = Xt âˆ’ğœˆ
ğœ
. From Problem 4.2.2.14 (page 211) we can
write the joint density of MY
t and Yt as
f â„™
MY
t ,Yt(y, ğ‘¤y) =
â§
âª
â¨
âªâ©
2(2yâˆ’ğ‘¤y)
t
âˆš
2ğœ‹t eğ›¼ğ‘¤yâˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘¤y)2
y â‰¥0, ğ‘¤y â‰¤y
0
otherwise.
Given that the pair of random variables (MY
t , Yt) only take values from the set
{(y, ğ‘¤y) âˆ¶y â‰¥0, y â‰¥ğ‘¤y}

4.2.2
Girsanovâ€™s Theorem
215
Figure 4.1
Region of MY
t â‰¤y in shaded area.
then in order to get the cumulative distribution function of MY
t we compute over the shaded
region given in Figure 4.1
â„™(MY
t â‰¤y) = âˆ«
0
âˆ’âˆâˆ«
y
0
f â„™
MY
t ,Yt(u, ğ‘£) dudğ‘£+ âˆ«
y
0 âˆ«
y
ğ‘£
f â„™
MY
t ,Yt(u, ğ‘£) dudğ‘£
such that
âˆ«
0
âˆ’âˆâˆ«
y
0
f â„™
MY
t ,Yt(u, ğ‘£) dudğ‘£= âˆ«
0
âˆ’âˆâˆ«
y
0
2(2u âˆ’ğ‘£)
t
âˆš
2ğœ‹t
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t (2uâˆ’ğ‘£)2dudğ‘£
= âˆ’âˆ«
0
âˆ’âˆ
1
âˆš
2ğœ‹t
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t (2uâˆ’ğ‘£)2||||||
u=y
u=0
dğ‘£
= âˆ’
1
âˆš
2ğœ‹t âˆ«
0
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘£)2dğ‘£
+
1
âˆš
2ğœ‹t âˆ«
0
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t ğ‘£2dğ‘£
and using similar steps we have
âˆ«
y
0 âˆ«
y
ğ‘£
f â„™
MY
t ,Yt(u, ğ‘£) dudğ‘£= âˆ’
1
âˆš
2ğœ‹t âˆ«
y
0
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘£)2dğ‘£
+
1
âˆš
2ğœ‹t âˆ«
y
0
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t ğ‘£2dğ‘£.

216
4.2.2
Girsanovâ€™s Theorem
Therefore,
â„™(MY
t â‰¤y) = âˆ’
1
âˆš
2ğœ‹t âˆ«
y
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t (2yâˆ’ğ‘£)2dğ‘£+
1
âˆš
2ğœ‹t âˆ«
y
âˆ’âˆ
eğ›¼ğ‘£âˆ’1
2 ğ›¼2tâˆ’1
2t ğ‘£2dğ‘£
and by completing the squares we have
ğ›¼ğ‘£âˆ’1
2ğ›¼2t âˆ’1
2t(2y âˆ’ğ‘£)2 = âˆ’1
2t(ğ‘£âˆ’2y âˆ’ğ›¼t)2 + 2ğ›¼y
ğ›¼ğ‘£âˆ’1
2ğ›¼2t âˆ’1
2tğ‘£2 = âˆ’1
2t(ğ‘£âˆ’ğ›¼t)2.
Thus, for y â‰¥0,
â„™(MY
t â‰¤y) = âˆ’e2ğ›¼y
âˆš
2ğœ‹t âˆ«
y
âˆ’âˆ
eâˆ’1
2t (ğ‘£âˆ’2yâˆ’ğ›¼t)2dğ‘£+ âˆ«
y
âˆ’âˆ
1
âˆš
2ğœ‹t
eâˆ’1
2t (ğ‘£âˆ’ğ›¼t)2dğ‘£
= âˆ’e2ğ›¼y
âˆ«
âˆ’yâˆ’ğ›¼t
âˆš
t
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 z2dz + âˆ«
yâˆ’ğ›¼t
âˆš
t
âˆ’âˆ
1
âˆš
2ğœ‹
eâˆ’1
2 z2dz
= âˆ’e2ğ›¼yÎ¦
(
âˆ’y âˆ’ğ›¼t
âˆš
t
)
+ Î¦
(
y âˆ’ğ›¼t
âˆš
t
)
or
â„™(MY
t â‰¤y) = Î¦
(
y âˆ’ğ›¼t
âˆš
t
)
âˆ’e2ğ›¼yÎ¦
(
âˆ’y âˆ’ğ›¼t
âˆš
t
)
.
By substituting
MY
t = MX
t âˆ’ğœˆ
ğœ
,
y = x âˆ’ğœˆ
ğœ
,
and
ğ›¼= ğœ‡
ğœ
we eventually have
â„™(MX
t â‰¤x) = Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
âˆ’e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
âˆ’x + ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
,
x â‰¥ğœˆ.
Finally, we focus on the minimum value of Xt. Take note that Y0 = 0 and hence by defining
mY
t = min
0â‰¤sâ‰¤t Ys
we have mY
t â‰¤0. Therefore, for y â‰¤0 and following the symmetry of the standard Wiener
process,
â„™(mY
t â‰¤y) = â„™
(
min
0â‰¤sâ‰¤t Ys â‰¤y
)
= â„™
(
âˆ’max
0â‰¤sâ‰¤t
{âˆ’Ys
} â‰¤y
)

4.2.2
Girsanovâ€™s Theorem
217
= â„™
(
âˆ’max
0â‰¤sâ‰¤t
{âˆ’ğ›¼s âˆ’Ws
} â‰¤y
)
= â„™
(
âˆ’max
0â‰¤sâ‰¤t
{âˆ’ğ›¼s + Ws
} â‰¤y
)
= â„™
(
max
0â‰¤sâ‰¤t
ÌƒYs â‰¥âˆ’y
)
where ÌƒYt = âˆ’ğ›¼t + Wt. Thus,
â„™(mY
t â‰¤y) = 1 âˆ’â„™
(
max
0â‰¤sâ‰¤t
ÌƒYs â‰¤âˆ’y
)
= 1 âˆ’Î¦
(
âˆ’y + ğ›¼t
âˆš
t
)
+ e2ğ›¼yÎ¦
(
y + ğ›¼t
âˆš
t
)
= Î¦
(
y âˆ’ğ›¼t
âˆš
t
)
+ e2ğ›¼yÎ¦
(
y + ğ›¼t
âˆš
t
)
.
Substituting
mY
t = mX
t âˆ’ğœˆ
ğœ
,
y = x âˆ’ğœˆ
ğœ
and
ğ›¼= ğœ‡
ğœ
we have
â„™(mX
t â‰¤x) = Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
+ e
2ğœ‡(xâˆ’a)
ğœ2
Î¦
(
x âˆ’ğœˆ+ ğœ‡t
ğœ
âˆš
t
)
,
x â‰¤ğœˆ.
By denoting fMX
t (x) and fmX
t (x) as the probability density functions for MX
t and mX
t , respec-
tively, therefore
fMX
t (x) = d
dxâ„™(MX
t â‰¤x)
= d
dx
[
Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
âˆ’e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
âˆ’x + ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)]
=
1
ğœ
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)2
âˆ’2ğœ‡
ğœ2 e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
âˆ’x + ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
+
1
ğœ
âˆš
2ğœ‹t
e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
âˆ’1
2
(
âˆ’x+ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)2
=
2
ğœ
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)2
âˆ’2ğœ‡
ğœ2 e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
âˆ’x + ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
,
x â‰¥ğœˆ
and

218
4.2.2
Girsanovâ€™s Theorem
fmX
t (x) = d
dxâ„™(mX
t â‰¤x)
= d
dx
[
Î¦
(
x âˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)
+ e
2ğœ‡(xâˆ’a)
ğœ2
Î¦
(
x âˆ’ğœˆ+ ğœ‡t
ğœ
âˆš
t
)]
=
1
ğœ
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)2
+ 2ğœ‡
ğœ2 e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
x âˆ’ğœˆ+ ğœ‡t
ğœ
âˆš
t
)
+
1
ğœ
âˆš
2ğœ‹t
e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
âˆ’1
2
(
xâˆ’ğœˆ+ğœ‡t
ğœ
âˆš
t
)2
=
2
ğœ
âˆš
2ğœ‹t
e
âˆ’1
2
(
xâˆ’ğœˆâˆ’ğœ‡t
ğœ
âˆš
t
)2
+ 2ğœ‡
ğœ2 e
2ğœ‡(xâˆ’ğœˆ)
ğœ2
Î¦
(
x âˆ’ğœˆ+ ğœ‡t
ğœ
âˆš
t
)
,
x â‰¤ğœˆ.
â—½
16. First Passage Time Density of a Standard Wiener Process Hitting a Sloping Line. Let
{Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™). By set-
ting Tğ›¼+ğ›½t as a stopping time such that Tğ›¼+ğ›½t = inf {t â‰¥0 âˆ¶Wt = ğ›¼+ ğ›½t}, ğ›¼, ğ›½âˆˆâ„, ğ›¼â‰ 0
show that the probability density function of Tğ›¼+ğ›½t is given as
fTğ›¼+ğ›½t(t) =
|ğ›¼|
t
âˆš
2ğœ‹t
eâˆ’1
2t (ğ›¼+ğ›½t)2.
Solution: By defining ÌƒWt = Wt âˆ’âˆ«t
0 ğ›½du such that
Tğ›¼+ğ›½t = inf
{
t â‰¥0 âˆ¶ÌƒWt = ğ›¼
}
then, from Girsanovâ€™s theorem, there exists an equivalent probability measure â„šon the
filtration â„±s, 0 â‰¤s â‰¤t defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ«s
0 ğ›½dWuâˆ’1
2 âˆ«s
0 ğ›½2du
= eğ›½Wsâˆ’1
2 ğ›½2s
= eğ›½( ÌƒWs+ğ›½s)âˆ’1
2 ğ›½2s
= eğ›½ÌƒWs+ 1
2 ğ›½2s
so that ÌƒWt is a â„š-standard Wiener process. From Problem 2.2.5.3 (page 85), the probability
density function of Tğ›¼+ğ›½t = inf {t â‰¥0 âˆ¶ÌƒWt = ğ›¼} under â„šis therefore
ÌƒfTğ›¼+ğ›½t(t) =
|ğ›¼|
t
âˆš
2ğœ‹t
eâˆ’1
2t ğ›¼2.
To find the probability density of Tğ›¼+ğ›½t under â„™we note that
â„™(Tğ›¼+ğ›½t â‰¤t) = ğ”¼â„™(
1ITğ›¼+ğ›½t
)

4.2.2
Girsanovâ€™s Theorem
219
= ğ”¼â„š(
Zâˆ’1
t 1ITğ›¼+ğ›½t
)
= ğ”¼â„š(
eâˆ’ğ›½ÌƒWtâˆ’1
2 ğ›½2t1ITğ›¼+ğ›½t
)
= ğ”¼â„š(
eâˆ’ğ›¼ğ›½âˆ’1
2 ğ›½2t1ITğ›¼+ğ›½t
)
= âˆ«
t
0
eâˆ’ğ›¼ğ›½âˆ’1
2 ğ›½2u ÌƒfTğ›¼+ğ›½t(u) du
= âˆ«
t
0
eâˆ’ğ›¼ğ›½âˆ’1
2 ğ›½2u
|ğ›¼|
u
âˆš
2ğœ‹u
eâˆ’1
2u ğ›¼2 du
= âˆ«
t
0
|ğ›¼|
u
âˆš
2ğœ‹u
eâˆ’1
2u (ğ›¼+ğ›½u)2 du.
Since
fTğ›¼+ğ›½t(t) = d
dtâ„™(Tğ›¼+ğ›½t â‰¤t)
we therefore have
fTğ›¼+ğ›½t(t) =
|ğ›¼|
t
âˆš
2ğœ‹t
eâˆ’1
2t (ğ›¼+ğ›½t)2.
â—½
17. Let {Wt âˆ¶t â‰¥0} be a â„™-standard Wiener process on the probability space (Î©, â„±, â„™), and
let â„±t be the filtration generated by Wt. Suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T and
by considering
Zt = eâˆ’âˆ«t
0 ğœƒsdWsâˆ’1
2 âˆ«t
0 ğœƒ2
s ds
and if
ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
t dt)
< âˆ
then Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T. By changing the measure â„™to a measure
â„šsuch that dâ„š
dâ„™
||||â„±t
= Zt, from Girsanovâ€™s theorem
ÌƒWt = Wt + âˆ«
t
0
ğœƒudu,
0 â‰¤t â‰¤T
is a â„š-standard Wiener process. If ÌƒMt, 0 â‰¤t â‰¤T is a â„š-martingale, then show that Mt =
Zt ÌƒMt, 0 â‰¤t â‰¤T is a â„™-martingale.
Using the martingale representation theorem, show that there exists an adapted process
{Ìƒğ›¾t âˆ¶0 â‰¤t â‰¤T} such that
ÌƒMt = ÌƒM0 + âˆ«
t
0
Ìƒğ›¾u d ÌƒWu,
0 â‰¤t â‰¤T.

220
4.2.2
Girsanovâ€™s Theorem
Solution: To show that Mt = Zt ÌƒMt is a â„™-martingale, we note the following:
(a) From Problem 4.2.2.8 (page 202),
ğ”¼â„™(Mt|â„±s
) = Zsğ”¼â„š(
MtZâˆ’1
t ||| â„±s
)
= Zs ÌƒMs = Ms.
(b) From Problem 4.2.2.5 (page 199),
ğ”¼â„™(|Mt|) = ğ”¼â„š
(
|Mt| 1
Zt
)
= ğ”¼â„š(|||MtZâˆ’1
t |||
)
= ğ”¼â„š(||| ÌƒMt|||
)
< âˆ
since ÌƒMt is a â„š-martingale.
(c) Mt = Zt ÌƒMt is clearly â„±t-adapted.
From the results of (a)â€“(c) we have shown that Mt = Zt ÌƒMt is a â„™-martingale.
Using ItÂ¯oâ€™s formula on d ÌƒMt = d(MtZâˆ’1
t ) and since dZt = âˆ’ğœƒtZtdWt (see Problem 4.2.2.2,
page 196) and d ÌƒWt = dWt + ğœƒtdt, we have
d
(
MtZâˆ’1
t
)
= 1
Zt
dMt âˆ’Mt
Z2
t
dZt + Mt
Z3
t
(dZt)2 âˆ’1
Z2
t
dMtdZt
= 1
Zt
dMt âˆ’Mt
Z2
t
(âˆ’ğœƒtZtdWt
) + Mt
Z3
t
(ğœƒ2
t Z2
t dt) âˆ’1
Z2
t
dMt
(âˆ’ğœƒtZtdWt
)
= 1
Zt
dMt + ğœƒt
Zt
dMtdWt + Mtğœƒt
Zt
(dWt + ğœƒtdt)
= 1
Zt
dMt + ğœƒt
Zt
dMtdWt + Mtğœƒt
Zt
d ÌƒWt.
Since Mt is a â„™-martingale then, from the martingale representation theorem, there exists
an adapted process ğ›¾t, 0 â‰¤t â‰¤T such that
Mt = M0 + âˆ«
t
0
ğ›¾u dWu,
0 â‰¤t â‰¤T
or
dMt = ğ›¾tdWt
and hence
d ÌƒMt = ğ›¾t
Zt
dWt + ğ›¾tğœƒt
Zt
dt + Mtğœƒt
Zt
d ÌƒWt
= ğ›¾t
Zt
(d ÌƒWt âˆ’ğœƒtdt) + ğ›¾tğœƒt
Zt
dt + Mtğœƒt
Zt
d ÌƒWt
=
(ğ›¾t + Mtğœƒt
Zt
)
d ÌƒWt
= Ìƒğ›¾td ÌƒWt

4.2.3
Risk-Neutral Measure
221
where Ìƒğ›¾t = ğ›¾t + Mtğœƒt
Zt
. Integrating on both sides, we therefore have
ÌƒMt = ÌƒM0 + âˆ«
t
0
Ìƒğ›¾ud ÌƒWu,
0 â‰¤t â‰¤T.
â—½
4.2.3
Risk-Neutral Measure
1. Geometric Brownian Motion. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = ğœ‡tStdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price drift rate, ğœt is the stock price volatility
(which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on
the probability space (Î©, â„±, â„™).
From the following discounted stock price process
Xt = eâˆ’âˆ«t
0 ruduSt
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equiva-
lent risk-neutral measure â„š, Xt is a â„š-martingale and by applying the martingale
representation theorem show also that
Xt = X0 + âˆ«
t
0
ğœuXud ÌƒWu,
0 â‰¤t â‰¤T
where ÌƒWt = Wt + âˆ«
t
0
ğœ†udu is a â„š-standard Wiener process with ğœ†t = ğœ‡t âˆ’rt
ğœt
defined as
the market price of risk.
Finally, show that under the â„šmeasure the stock price follows
dSt = rtStdt + ğœtStd ÌƒWt.
Solution: By expanding dXt using Taylorâ€™s theorem and then applying ItÂ¯oâ€™s formula,
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•St
dSt + 1
2
ğœ•2Xt
ğœ•t2 dt2 + 1
2
ğœ•2Xt
ğœ•S2
t
dS2
t + . . .
= âˆ’rtXtdt + eâˆ’âˆ«t
0 rududSt
= (ğœ‡t âˆ’rt)Xtdt + ğœtXtdWt
= ğœtXt
[(ğœ‡t âˆ’rt
ğœt
)
dt + dWt
]
= ğœtXtd ÌƒWt

222
4.2.3
Risk-Neutral Measure
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du such that ğœ†t = ğœ‡t âˆ’rt
ğœt
. From Girsanovâ€™s theorem there exists
an equivalent martingale measure or risk-neutral measure on the filtration â„±s, 0 â‰¤s â‰¤t
defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğœ†uduâˆ’1
2 âˆ«s
0 ğœ†2
udWu
so that ÌƒWt is a â„š-standard Wiener process. Given that under the risk-neutral measure â„š,
the discounted stock price diffusion process
dXt = ğœtXtd ÌƒWt
has no dt term, then Xt is a â„š-martingale (see Problem 3.2.2.5, page 127). Thus, from the
martingale representation theorem, there exists an adapted process ğ›¾u, 0 â‰¤u â‰¤T such that
Xt = X0 + âˆ«
t
0
ğ›¾u d ÌƒWu,
0 â‰¤t â‰¤T
or
Xt = X0 + âˆ«
t
0
ğœuXu d ÌƒWu,
0 â‰¤t â‰¤T
such that under â„š, the process âˆ«
t
0
ğœuXu d ÌƒWu is a â„š-martingale.
Finally, by substituting dWt = d ÌƒWt + ğœ†tdt into dSt = ğœ‡tStdt + ğœtStdWt, the stock price dif-
fusion process under the risk-neutral measure becomes
dSt = rtStdt + ğœtStd ÌƒWt.
â—½
2. Arithmetic Brownian Motion. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = ğœ‡tdt + ğœtdWt
where rt is the risk-free rate, ğœ‡t is the stock price drift rate, ğœt is the stock price volatility
(which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on
the probability space (Î©, â„±, â„™).
From the following discounted stock price process
Xt = eâˆ’âˆ«t
0 ruduSt
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equiva-
lent risk-neutral measure â„š, Xt is a â„š-martingale and by applying the martingale
representation theorem show also that
Xt = X0 + âˆ«
t
0
ğœuBâˆ’1
u d ÌƒWu,
0 â‰¤t â‰¤T

4.2.3
Risk-Neutral Measure
223
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du is a â„š-standard Wiener process such that
ğœ†t = ğœ‡t âˆ’rtSt
ğœt
is defined as the market price of risk.
Finally, show that under the â„šmeasure the stock price follows
dSt = rtStdt + ğœtd ÌƒWt.
Solution: By expanding dXt using Taylorâ€™s theorem and then applying ItÂ¯oâ€™s formula,
dXt = ğœ•Xt
ğœ•t dt + ğœ•Xt
ğœ•St
dSt + 1
2
ğœ•2Xt
ğœ•t2 dt2 + 1
2
ğœ•2Xt
ğœ•S2
t
dS2
t + . . .
= âˆ’rtXtdt + eâˆ’âˆ«t
0 rududSt
= eâˆ’âˆ«t
0 rudu(ğœ‡t âˆ’rtSt)dt + ğœteâˆ’âˆ«t
0 rududWt
= ğœteâˆ’âˆ«t
0 rudu
[(ğœ‡t âˆ’rtSt
ğœt
)
dt + dWt
]
= ğœtBâˆ’1
t d ÌƒWt
where Bt = eâˆ«t
0 rudu, ÌƒWt = Wt + âˆ«
t
0
ğœ†u du such that ğœ†t = ğœ‡t âˆ’rtSt
ğœt
. From Girsanovâ€™s theo-
rem there exists an equivalent martingale measure or risk-neutral measure on the filtration
â„±s, 0 â‰¤s â‰¤t defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğœ†uduâˆ’1
2 âˆ«s
0 ğœ†2
udWu
so that ÌƒWt is a â„š-standard Wiener process. Given that under the risk-neutral measure â„š,
the discounted stock price diffusion process
dXt = ğœtBâˆ’1
t d ÌƒWt
has no dt term, then Xt is a â„š-martingale (see Problem 3.2.2.5, page 127). Thus, from the
martingale representation theorem, there exists an adapted process ğ›¾u, 0 â‰¤u â‰¤T such that
Xt = X0 + âˆ«
t
0
ğ›¾ud ÌƒWu,
0 â‰¤t â‰¤T
or
Xt = X0 + âˆ«
t
0
ğœuBâˆ’1
u d ÌƒWu,
0 â‰¤t â‰¤T
such that under â„š, the process âˆ«
t
0
ğœuBâˆ’1
u d ÌƒWu is a â„š-martingale.

224
4.2.3
Risk-Neutral Measure
Finally, by substituting dWt = d ÌƒWt + ğœ†tdt into dSt = ğœ‡tdt + ğœtdWt, the stock price diffu-
sion process under the risk-neutral measure becomes
dSt = rtStdt + ğœtd ÌƒWt.
â—½
3. Discounted Portfolio. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = ğœ‡tStdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price drift rate, ğœt is the stock price volatility
(which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on
the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of stock
and ğœ“t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = eâˆ’âˆ«t
0 ruduÎ t
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent
risk-neutral measure â„š, then the discounted portfolio Yt is a â„š-martingale.
Solution: At time t, the portfolio Î t is valued as
Î t = ğœ™tSt + ğœ“tBt
and the differential of the portfolio is
dÎ t = ğœ™tdSt + ğœ“trtBtdt
= ğœ™t
(ğœ‡tStdt + ğœtStdWt
) + rtğœ“tBtdt
= rtÎ tdt + ğœ™t
(ğœ‡t âˆ’rt
) Stdt + ğœ™tğœtStdWt
= rtÎ tdt + ğœ™tğœtSt
(ğœ†tdt + dWt
)
where ğœ†t = ğœ‡t âˆ’rt
ğœt
. By expanding dYt using Taylorâ€™s theorem and then applying ItÂ¯oâ€™s
formula,
dYt = ğœ•Yt
ğœ•t dt + ğœ•Yt
ğœ•Î t
dSt + 1
2
ğœ•2Yt
ğœ•t2 dt2 + 1
2
ğœ•2Yt
ğœ•Î 2
t
dÎ 2
t + . . .
= âˆ’rtYtdt + eâˆ’âˆ«t
0 rududÎ t
= âˆ’rteâˆ’âˆ«t
0 ruduÎ tdt + eâˆ’âˆ«t
0 rudu (rtÎ tdt + ğœ™tğœtSt(ğœ†tdt + dWt))
= ğœ™td
(
eâˆ’âˆ«t
0 ruduSt
)

4.2.3
Risk-Neutral Measure
225
where ğœ†t = ğœ‡t âˆ’rt
ğœt
and from Problem 4.2.3.1 (page 221),
d
(
eâˆ’âˆ«t
0 ruduSt
)
= ğœteâˆ’âˆ«t
0 ruduSt(ğœ†tdt + dWt))
= ğœteâˆ’âˆ«t
0 ruduStd ÌƒWt
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du.
From Girsanovâ€™s theorem, there exists an equivalent martingale measure or risk-neutral
measure on the filtration â„±s, 0 â‰¤s â‰¤t defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğœ†uduâˆ’1
2 âˆ«s
0 ğœ†2
udWu
so that ÌƒWt is a â„š-standard Wiener process. Given that under the risk-neutral measure â„š,
the discounted stock price diffusion process
dYt = ğœ™tğœteâˆ’âˆ«t
0 ruduStd ÌƒWt
has no dt term, then the discounted portfolio Yt is a â„š-martingale (see Problem 3.2.2.5,
page 127).
â—½
4. Self-Financing Trading Strategy. Consider an economy consisting of a risk-free asset and
a stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = ğœ‡tStdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price drift rate, ğœt is the stock price volatility
(which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on
the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of stock
and ğœ“t units being invested in a risk-free asset. Under the risk-neutral measure â„š, the
following discounted portfolio value
Yt = eâˆ’âˆ«t
0 rudu Î t
is a â„š-martingale. Using the martingale representation theorem, show that the portfolio
(ğœ™t, ğœ“t) trading strategy has the values
ğœ™t = ğ›¾tBt
ğœtSt
,
ğœ“t = ğœtÎ t âˆ’ğ›¾tBt
ğœtBt
,
0 â‰¤t â‰¤T
where ğ›¾t, 0 â‰¤t â‰¤T is an adapted process.
Solution: Since Yt is a â„š-martingale with respect to the filtration â„±t, 0 â‰¤t â‰¤T (see Prob-
lem 4.2.3.3, page 224) and
dYt = ğœ™tğœteâˆ’âˆ«t
0 ruduStd ÌƒWt

226
4.2.3
Risk-Neutral Measure
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du, ğœ†t = ğœ‡t âˆ’rt
ğœt
then, from the martingale representation theo-
rem, there exists an adapted process ğ›¾t, 0 â‰¤t â‰¤T such that
Yt = Y0 + âˆ«
t
0
ğ›¾ğ‘£d ÌƒWğ‘£,
0 â‰¤t â‰¤T.
Taking integrals of dYt = ğœ™tğœteâˆ’âˆ«t
0 ruduStd ÌƒWt,
âˆ«
t
0
dYğ‘£= âˆ«
t
0
ğœ™ğ‘£ğœğ‘£eâˆ’âˆ«ğ‘£
0 ruduSğ‘£d ÌƒWğ‘£
Yt = Y0 + âˆ«
t
0
ğœ™ğ‘£ğœğ‘£eâˆ’âˆ«ğ‘£
0 ruduSğ‘£d ÌƒWğ‘£.
Therefore, for 0 â‰¤t â‰¤T, we can set
ğ›¾t = ğœ™tğœteâˆ’âˆ«t
0 ruduSt
or
ğœ™t = ğ›¾teâˆ«t
0 rudu
ğœtSt
= ğ›¾tBt
ğœtSt
where Bt = eâˆ«t
0 rudu. Since ğœ“tBt = Î t âˆ’ğœ™tSt, therefore
ğœ“t = ğœtÎ t âˆ’ğ›¾tBt
ğœtBt
.
â—½
5. Self-Financing Portfolio. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = ğœ‡tStdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price drift rate, ğœt is the stock price volatility
(which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on
the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of stock
and ğœ“t units being invested in a risk-free asset. For each of the following choices of ğœ™t:
(a) ğœ™t = ğ›¼, where ğ›¼is a constant
(b) ğœ™t = Sn
t , n > 0
(c) ğœ™t = âˆ«t
0 Sn
u du, n > 0
find the corresponding ğœ“t so that the trading strategy at time t, (ğœ™t, ğœ“t) is self-financing.
Solution: By definition, the portfolio at time t, Î t = ğœ™tSt + ğœ“tBt is self-financing if dÎ t =
ğœ™tdSt + ğœ“tdBt. Taking note that Bt = eâˆ«t
0 rudu satisfies dBt = rtBtdt and from ItÂ¯oâ€™s lemma,
we have (dSt)2 = ğœ2
t S2
t dt and (dSt)ğœˆ= 0 for ğœˆâ‰¥3. Then, for each of the following cases:

4.2.3
Risk-Neutral Measure
227
(a) If ğœ™t = ğ›¼, we have Î t = ğ›¼St + ğœ“tBt and in differential form
dÎ t = ğ›¼dSt + ğœ“tdBt + Btdğœ“t
and in order for the portfolio to be self-financing (i.e., dÎ t = ğ›¼dSt + ğœ“tdBt) we have
Btdğœ“t = 0 or ğœ“t = ğ›½for some constant ğ›½.
(b) If ğœ™t = Sn
t , we have Î t = Sn+1
t
+ ğœ“tBt and in differential form
dÎ t = (n + 1)Sn
t dSt + 1
2n(n + 1)Snâˆ’1
t
(dSt)2 + ğœ“tdBt + Btdğœ“t
and for the portfolio to be self-financing (i.e., dÎ t = Sn
t dSt + ğœ“tdBt) we therefore set
nSn
t dSt + 1
2n(n + 1)Snâˆ’1
t
(dSt)2 + Btdğœ“t = 0
or
ğœ“t = âˆ’âˆ«
t
0
Sn
u
Bu
dSu âˆ’âˆ«
t
0
n(n + 1)ğœ2
uSn+1
u
Bu
du
= âˆ’âˆ«
t
0
eâˆ’âˆ«u
0 rğ‘£dğ‘£Sn
u dSu âˆ’âˆ«
t
0
eâˆ’âˆ«u
0 rğ‘£dğ‘£n(n + 1)ğœ2
uSn+1
u
du.
(c) If ğœ™t = âˆ«t
0 Sn
u du, we have Î t =
(
âˆ«t
0 Sn
u du
)
St + ğœ“tBt and in differential form
dÎ t = Sn+1
t
dt +
(
âˆ«
t
0
Sn
u du
)
dSt + ğœ“tdBt + Btdğœ“t
and for the portfolio to be self-financing (i.e., dÎ t =
(
âˆ«t
0 Su du
)
dSt + ğœ“tdBt) we
require
Sn+1
t
dt + Btdğœ“t = 0
or
ğœ“t = âˆ’âˆ«
t
0
Sn+1
u
Bu
du = âˆ’âˆ«
t
0
eâˆ’âˆ«u
0 rğ‘£dğ‘£Sn+1
u
du.
â—½
6. Stock Price with Continuous Dividend Yield (Geometric Brownian Motion). Consider an
economy consisting of a risk-free asset and a dividend-paying stock price (risky asset). At
time t, the risk-free asset Bt and the stock price St have the following diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
such that rt is the risk-free rate, ğœ‡t is the stock price drift rate, Dt is the continuous dividend
yield, ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T}
is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).

228
4.2.3
Risk-Neutral Measure
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of stock
and ğœ“t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = eâˆ’âˆ«t
0 rudu Î t
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent
risk-neutral measure â„š, then the discounted portfolio Yt is a â„š-martingale.
Show also that under the â„š-measure the stock price follows
dSt = (rt âˆ’Dt)Stdt + ğœtStd ÌƒWt
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du is a â„š-standard Wiener process such that
ğœ†t = ğœ‡t âˆ’rt
ğœt
is defined as the market price of risk.
Solution: At time t, the portfolio Î t is valued as
Î t = ğœ™tSt + ğœ“tBt
and since the trader will receive DtStdt for every stock held and because the trader holds
ğœ™t of the stock, then in differential form
dÎ t = ğœ™tdSt + ğœ™tDtStdt + ğœ“tdBt
= ğœ™t
[(ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
] + ğœ™tDtStdt + ğœ“trtBtdt
= rtÎ tdt + ğœ™t(ğœ‡t âˆ’rt)Stdt + ğœ™tğœtStdWt
= rtÎ tdt + ğœ™tğœtSt(ğœ†tdt + dWt)
where ğœ†t = ğœ‡t âˆ’rt
ğœt
. Following Problem 4.2.3.3 (page 224),
dYt = d
(
eâˆ’âˆ«t
0 rudu Î t
)
= ğœ™ğœteâˆ’âˆ«t
0 ruduStd ÌƒWt
such that
ÌƒWt = Wt + âˆ«
t
0
ğœ†u du.
By applying Girsanovâ€™s theorem to change the measure â„™to an equivalent risk-neutral
measure â„š, under which ÌƒWt is a â„š-standard Wiener process, the discounted portfolio Yt
is a â„š-martingale.
By substituting dWt = d ÌƒWt + ğœ†tdt into dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt, the stock price dif-
fusion process under the risk-neutral measure becomes
dSt = (rt âˆ’Dt)Stdt + ğœtStd ÌƒWt.
â—½

4.2.3
Risk-Neutral Measure
229
7. Stock Price with Continuous Dividend Yield (Arithmetic Brownian Motion). Consider an
economy consisting of a risk-free asset and a dividend-paying stock price (risky asset). At
time t, the risk-free asset Bt and the stock price St have the following diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)dt + ğœtdWt
such that rt is the risk-free rate, ğœ‡t is the stock price drift rate, Dt is the continuous dividend
yield, ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T}
is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of stock
and ğœ“t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = eâˆ’âˆ«t
0 rudu Î t
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent
risk-neutral measure â„š, then the discounted portfolio Yt is a â„š-martingale.
Show also that under the â„š-measure the stock price follows
dSt = (rt âˆ’Dt)Stdt + ğœtd ÌƒWt
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du is a â„š-standard Wiener process such that
ğœ†t = ğœ‡t âˆ’rtSt
ğœt
is defined as the market price of risk.
Solution: At time t, the portfolio Î t is valued as
Î t = ğœ™tSt + ğœ“tBt
and since the trader will receive Dtdt for every stock held, then in differential form
dÎ t = ğœ™t
(dSt + Dtdt) + ğœ“tdBt
= ğœ™t
[ğœ‡tdt + ğœtdWt
] + ğœ“trtBtdt
= rtÎ tdt + ğœ™t
(ğœ‡t âˆ’rtSt
) dt + ğœ™tğœtdWt
= rtÎ tdt + ğœ™tğœt
(ğœ†tdt + dWt
)
where ğœ†t = ğœ‡t âˆ’rtSt
ğœt
.
Following Problem 4.2.3.3 (page 224), the discounted portfolio becomes
dYt = d
(
eâˆ’âˆ«t
0 rudu Î t
)
= ğœ™tğœteâˆ’âˆ«t
0 rudud ÌƒWt
such that
ÌƒWt = Wt + âˆ«
t
0
ğœ†u du.

230
4.2.3
Risk-Neutral Measure
By applying Girsanovâ€™s theorem to change the measure â„™to an equivalent risk-neutral
measure â„š, under which ÌƒWt is a â„š-standard Wiener process, the discounted portfolio Yt
is a â„š-martingale.
By substituting dWt = d ÌƒWt + ğœ†tdt into dSt = (ğœ‡t âˆ’Dt)dt + ğœtdWt, the diffusion process
under the risk-neutral measure becomes
dSt = (rt âˆ’Dt)Stdt + ğœtd ÌƒWt.
â—½
8. Commodity Price with Cost of Carry. Consider an economy consisting of a risk-free asset
and a commodity price (risky asset). At time t, the risk-free asset Bt and the commodity
price St have the following diffusion processes
dBt = rtBtdt, dSt = ğœ‡tStdt + Ctdt + ğœtStdWt
such that rt is the risk-free rate, ğœ‡t is the commodity price drift rate, Ct > 0 is the cost
of carry for storage per unit time, ğœt is the commodity price volatility (which are all time
dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on the probability space
(Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t units of com-
modity and ğœ“t units being invested in a risk-free asset. From the following discounted
portfolio value
Yt = eâˆ’âˆ«t
0 rudu Î t
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent
risk-neutral measure â„š, then the discounted portfolio Yt is a â„š-martingale.
Show also that under the â„š-measure the commodity price follows
dSt = rtStdt + Ctdt + ğœtStd ÌƒWt
where ÌƒWt = Wt + âˆ«
t
0
ğœ†u du is a â„š-standard Wiener process such that
ğœ†t = ğœ‡t âˆ’rt
ğœt
is defined as the market price of risk.
Solution: At time t, the portfolio Î t is valued as
Î t = ğœ™tSt + ğœ“tBt
and since the trader will pay Ctdt for storage, and because the trader holds ğœ™t of the com-
modity, then in differential form
dÎ t = ğœ™tdSt âˆ’ğœ™tCtdt + ğœ“trtBtdt
= ğœ™t
[ğœ‡tStdt + Ctdt + ğœtStdWt
] âˆ’ğœ™tCtdt + ğœ“trtBtdt

4.2.3
Risk-Neutral Measure
231
= rtÎ tdt + ğœ™t
(ğœ‡t âˆ’rt
) Stdt + ğœ™tğœtStdWt
= rtÎ tdt + ğœ™tğœtSt
(ğœ†tdt + dWt
)
where ğœ†t = ğœ‡t âˆ’rt
ğœt
. Following Problem 4.2.3.3 (page 224),
dYt = d
(
eâˆ’âˆ«t
0 ruduÎ t
)
= ğœ™tğœteâˆ’âˆ«t
0 ruduStd ÌƒWt
such that
ÌƒWt = Wt + âˆ«
t
0
ğœ†u du.
By applying Girsanovâ€™s theorem to change the measure â„™to an equivalent risk-neutral
measure â„š, under which ÌƒWt is a â„š-standard Wiener process, the discounted portfolio Yt
is a â„š-martingale.
By substituting dWt = d ÌƒWt + ğœ†tdt into dSt = ğœ‡tStdt + Ctdt + ğœtStdWt, the commodity
price diffusion process under the risk-neutral measure becomes
dSt = rtStdt + Ctdt + ğœtStd ÌƒWt.
â—½
9. Pricing a Security Derivative. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
where rt is the risk-free rate, Dt is the continuous dividend yield, ğœ‡t is the stock price
growth rate, ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤
t â‰¤T} is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t holding ğœ™t shares of
stock and ğœ“t units being invested in a risk-free asset. Let Î¨(ST) represents the payoff of a
derivative security at time T, 0 â‰¤t â‰¤T such that Î¨(ST) is â„±T measurable. Assuming the
trader begins with an initial capital Î 0 and a trading strategy (ğœ™t, ğœ“t), 0 â‰¤t â‰¤T such that
the portfolio value at time T is
Î T = Î¨(ST) almost surely
show that under the risk-neutral measure â„š,
Î¨(St) = ğ”¼â„š
[
eâˆ’âˆ«T
t
rudu Î¨(ST)||||
â„±t
]
,
0 â‰¤t â‰¤T.
Solution: At time t, in order to calculate the price of a derivative security Î¨(St), 0 â‰¤t â‰¤T
with payoff at time T given as Î¨(ST), we note that because eâˆ’âˆ«t
0 rudu Î t is a â„š-martingale
and Î T = Î¨(ST) almost surely,
eâˆ’âˆ«t
0 rudu Î t = ğ”¼â„š
[
eâˆ’âˆ«T
0 rudu Î T
||||
â„±t
]
= ğ”¼â„š
[
eâˆ’âˆ«T
0 rudu Î¨(ST)
||||
â„±t
]
.

232
4.2.3
Risk-Neutral Measure
Since the derivative security Î¨ and the portfolio Î  have identical values at T, and by
assuming the trader begins with an initial capital Î 0, the value of the portfolio Î t = Î¨(St)
for 0 â‰¤t â‰¤T. Thus, we can write
Î¨(St) = ğ”¼â„š
[
eâˆ’âˆ«T
t
rudu Î¨(ST)
||||
â„±t
]
,
0 â‰¤t â‰¤T.
â—½
10. First Fundamental Theorem of Asset Pricing. Consider an economy consisting of a
risk-free asset and a stock price (risky asset). At time t, the risk-free asset Bt and the stock
price St have the following diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price growth rate, Dt is the continuous dividend
yield, ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T}
is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
At time t, 0 â‰¤t â‰¤T we consider a trader who has a self-financing portfolio valued at Î t
and we define an arbitrage strategy such that the following criteria are satisfied:
(i) Î 0 = 0
(ii) â„™(Î T â‰¥0) = 1
(iii) â„™(Î T > 0) > 0.
Prove that if the trading strategy has a risk-neutral probability measure â„š, then it does not
admit any arbitrage opportunities.
Solution: We prove this result via contradiction.
Suppose the trading strategy has a risk-neutral probability measure â„šand it admits arbi-
trage opportunities. Since the discounted portfolio eâˆ’âˆ«t
0 rudu Î t is a â„š-martingale, by let-
ting Î 0 = 0, and from the optional stopping theorem,
ğ”¼â„š(
eâˆ’âˆ«T
0 rudu Î T
)
= Î 0 = 0.
Since â„™(Î T â‰¥0) = 1 and because â„™is equivalent to â„š, therefore â„š(Î T â‰¥0) = 1. In addi-
tion, since Î T â‰¥0 therefore eâˆ’âˆ«T
0 ruduÎ T â‰¥0. By definition,
ğ”¼â„š(
eâˆ’âˆ«T
0 rudu Î T
)
= âˆ«
âˆ
0
â„š
(
eâˆ’âˆ«T
0 rudu Î T â‰¥x
)
dx = âˆ«
âˆ
0
â„š
(
Î T â‰¥xeâˆ«T
0 rudu)
dx = 0
which implies
â„š(Î T â‰¥0) = 0.
Because â„šis equivalent to â„™, therefore
â„™(Î T â‰¥0) = 0
which is a contradiction to the fact that there is an arbitrage opportunity.
â—½

4.2.3
Risk-Neutral Measure
233
11. Second Fundamental Theorem of Asset Pricing. Consider an economy consisting of a
risk-free asset and a stock price (risky asset). At time t, the risk-free asset Bt and the stock
price St have the following diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock price growth rate, Dt is the continuous dividend
yield, ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T}
is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
We consider a market model under the risk-neutral measure â„šwhere at each time t, 0 â‰¤
t â‰¤T, the portfolio Î t is constructed with ğœ“t number of stocks and ğœ™t units of non-risky
assets. Prove that if the market is complete (i.e., every derivative security can be hedged),
then the risk-neutral measure is unique.
Solution: Suppose the market model has two risk-neutral measures â„š1 and â„š2. Let A âˆˆ
â„±T, and we consider the payoff of a derivative security
Î¨(ST) =
{
eâˆ«T
0 rudu
A âˆˆâ„±T
0
A âˆ‰â„±T.
As the market model is complete, there is a portfolio value process Î t, 0 â‰¤t â‰¤T with the
same initial condition Î 0 that satisfies Î T = Î¨(ST). Since both â„š1 and â„š2 are risk-neutral
measures, the discounted portfolio eâˆ’âˆ«t
0 rudu Î t is a martingale under â„š1 and â„š2.
From the optional stopping theorem we can write
â„š1(A) = ğ”¼â„š1
[
eâˆ’âˆ«T
0 rudu Î¨(ST)
]
= ğ”¼â„š1
(
eâˆ’âˆ«T
0 rudu Î T
)
= Î 0
and
â„š2(A) = ğ”¼â„š2
[
eâˆ’âˆ«T
0 rudu Î¨(ST)
]
= ğ”¼â„š2
(
eâˆ’âˆ«T
0 rudu Î T
)
= Î 0.
Therefore,
â„š1 = â„š2
since A is an arbitrary set.
â—½
12. Change of NumÃ©raire. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = (ğœ‡t âˆ’Dt)Stdt + ğœtStdWt
where rt is the risk-free rate, ğœ‡t is the stock drift rate, Dt is the continuous dividend yield,
ğœt is the stock price volatility (which are all time dependent) and {Wt âˆ¶0 â‰¤t â‰¤T} is a
â„™-standard Wiener process on the probability space (Î©, â„±, â„™).

234
4.2.3
Risk-Neutral Measure
Let Î¨(ST) represent the payoff of a derivative security at time T, 0 â‰¤t â‰¤T such that Î¨(ST)
is â„±T measurable and under the risk-neutral measure â„š, let the numÃ©raire N(i)
t , i = 1, 2 be a
strictly positive price process for a non-dividend-paying asset with the following diffusion
process:
dN(i)
t
= rtN(i)
t dt + ğœˆ(i)
t N(i)
t d ÌƒW(i)
t
where ğœˆ(i)
t
is the volatility and ÌƒW(i)
t , 0 â‰¤t â‰¤T is a â„š-standard Wiener process. Show that
for 0 â‰¤t â‰¤T,
N(1)
t ğ”¼â„š(1)
(
Î¨(ST)
N(1)
T
||||||
â„±t
)
= N(2)
t ğ”¼â„š(2)
(
Î¨(ST)
N(2)
T
||||||
â„±t
)
and
dâ„š(1)
dâ„š(2)
||||â„±t
= N(1)
t
N(1)
0
/N(2)
t
N(2)
0
where N(i) is a numÃ©raire and â„š(i) is the measure under which the stock price discounted
by N(i) is a martingale.
Solution: By solving dBt = rtBtdt we have Bt = eâˆ«t
0 rudu. For i = 1, 2 and given
Bt = eâˆ«t
0 rudu, from Problem 4.2.3.1 (page 221) we can deduce that
d(Bâˆ’1
t N(i)
t ) = Bâˆ’1
t N(i)
t ğœˆ(i)
t d ÌƒW(i)
t
is a â„š-martingale and from ItÂ¯oâ€™s formula we have
d(log (Bâˆ’1
t Nt)) = d(Bâˆ’1
t N(i)
t )
Bâˆ’1
t N(i)
t
âˆ’1
2
â›
âœ
âœ
âœâ
d
(
Bâˆ’1
t N(i)
t
)
Bâˆ’1
t N(i)
t
â
âŸ
âŸ
âŸâ 
2
+ . . .
= ğœˆ(i)
t d ÌƒW(i)
t
âˆ’1
2(ğœˆ(i)
t )2dt.
Taking integrals,
log
(
Bâˆ’1
t Nt
Bâˆ’1
0 N0
)
= âˆ«
t
0
ğœˆ(i)
u d ÌƒW(i)
u âˆ’âˆ«
t
0
1
2(ğœˆ(i)
u )2 du
or
Bâˆ’1
t N(i)
t
= N(i)
0 eâˆ«t
0 ğœˆ(i)
u d ÌƒW(i)
u âˆ’1
2 âˆ«t
0 (ğœˆ(i)
u )2du
since B0 = 1. Using Girsanovâ€™s theorem to change from the â„šmeasure to an equivalent
â„š(i) measure, the Radonâ€“NikodÂ´ym derivative is
dâ„š(i)
dâ„š
||||â„±t
= eâˆ’âˆ«t
0 (âˆ’ğœˆu)d ÌƒW(i)
u âˆ’1
2 âˆ«t
0 (ğœˆ(i)
u )2du =
N(i)
t
N(i)
0 Bt
so that W
(i)
t
= ÌƒW(i)
t
âˆ’âˆ«
t
0
ğœˆ(i)
u du, 0 â‰¤t â‰¤T is a â„š(i)-standard Wiener process.

4.2.3
Risk-Neutral Measure
235
Under the risk-neutral measure â„š, the stock price follows
dSt = (rt âˆ’Dt)Stdt + ğœtStd ÌƒWt
where ÌƒWt = Wt + âˆ«
t
0
(ğœ‡u âˆ’ru
ğœu
)
du is a â„š-standard Wiener process. By changing the â„š
measure to an equivalent â„š(i) measure, the discounted stock price
{N(i)
t }âˆ’1St
is a â„š(i)-martingale. Thus, for a derivative payoff Î¨(ST),
ğ”¼â„š(Î¨(ST)|â„±t
) =
(
dâ„š(i)
dâ„š
||||â„±t
)
ğ”¼â„š(i) â¡
â¢
â¢â£
Î¨(ST)
(
dâ„š(i)
dâ„š
||||â„±T
)âˆ’1||||||
â„±t
â¤
â¥
â¥â¦
= N(i)
t
N(i)
0 Bt
ğ”¼â„š(i)
(
Î¨T
N(i)
0 BT
N(i)
T
||||||
â„±t
)
= BTN(i)
t
Bt
ğ”¼â„š(i)
(
Î¨(ST)
N(i)
T
||||||
â„±t
)
.
Thus,
N(1)
t ğ”¼â„š(1)
(
Î¨(ST)
N(1)
T
||||||
â„±t
)
= N(2)
t ğ”¼â„š(2)
(
Î¨(ST)
N(2)
T
||||||
â„±t
)
.
To find dâ„š(1)
dâ„š(2) on â„±t we note that
ğ”¼â„š(1)
(
N(1)
0 Î¨(St)
N(1)
t
||||||
â„±0
)
= ğ”¼â„š(2)
(
N(2)
0 Î¨(St)
N(2)
t
||||||
â„±0
)
.
By letting Xt, 0 â‰¤t â‰¤T be an â„±t measurable random variable, from Problem 4.2.2.4
(page 198)
ğ”¼â„š(1)(Xt) = ğ”¼â„š(2) [
Xt
(
dâ„š(1)
dâ„š(2)
||||â„±t
)]
.
We can therefore deduce that
N(1)
0 Î¨(St)
N(1)
t
(
dâ„š(1)
dâ„š(2)
||||â„±t
)
=
N(2)
0 Î¨(St)
N(2)
t
or
dâ„š(1)
dâ„š(2)
||||â„±t
= N(1)
t
N(1)
0
/N(2)
t
N(2)
0
.
N.B. An alternative method is to let dâ„š(1)
dâ„š(2) = dâ„š(1)
dâ„š
/
dâ„š(2)
dâ„š
and the result follows.
â—½

236
4.2.3
Risk-Neutral Measure
13. Blackâ€“Scholes Equation. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rBtdt, dSt = ğœ‡Stdt + ğœStdWt
where r is the risk-free rate, ğœ‡is the stock price drift rate, ğœis the stock price volatility
(which are all constants) and {Wt âˆ¶0 â‰¤t â‰¤T} is a â„™-standard Wiener process on the
probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t, given as
Î t = ğœ™tSt + ğœ“tBt
where he holds ğœ™t shares of stock and ğœ“t units being invested in a risk-free asset. Show
that the portfolio (ğœ™t, ğœ“t) is self-financing if and only if Î t satisfies the Blackâ€“Scholes
equation
ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
+ rSt
ğœ•Î t
ğœ•St
âˆ’rÎ t = 0.
Solution: By applying Taylorâ€™s theorem on dÎ t,
dÎ t = ğœ•Î t
ğœ•t dt + ğœ•Î t
ğœ•St
dSt + 1
2
ğœ•2Î t
ğœ•S2
t
dS2
t + . . .
and since dS2
t = (ğœ‡Stdt + ğœStdWt)2 = ğœ2S2
t dt such that (dt)ğœˆ= 0, ğœˆâ‰¥2, we have
dÎ t = ğœ•Î t
ğœ•t dt + ğœ•Î t
ğœ•St
dSt + 1
2ğœ2 ğœ•2Î t
ğœ•S2
t
dt
= ğœ•Î t
ğœ•St
dSt +
(
ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
)
dt.
In contrast, the portfolio (ğœ™t, ğœ“t) is self-financing if and only if
dÎ t = ğœ™tdSt + ğœ“tdBt = ğœ™tdSt + rBtğœ“tdt.
By equating both of the equations we have
rBtğœ“t = ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
and
ğœ™t = ğœ•Î t
ğœ•St
and substituting the above two equations into
Î t = ğœ™tSt + ğœ“tBt
we have
ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
+ rSt
ğœ•Î t
ğœ•St
âˆ’rÎ t = 0.

4.2.3
Risk-Neutral Measure
237
N.B. Take note that the Blackâ€“Scholes equation does not contain the growth parameter
ğœ‡, which means that the value of a self-financing portfolio is independent of how rapidly
or slowly a stock grows. In essence, the only parameter from the stochastic differential
equation dSt = ğœ‡Stdt + ğœStdWt that affects the value of the portfolio is the volatility.
â—½
14. Blackâ€“Scholes Equation with Stock Paying Continuous Dividend Yield. Consider an econ-
omy consisting of a risk-free asset and a stock price (risky asset). At time t, the risk-free
asset Bt and the stock price St have the following diffusion processes
dBt = rBtdt, dSt = (ğœ‡âˆ’D)Stdt + ğœStdWt
where r is the risk-free rate, ğœ‡is the stock price drift rate, D is the continuous dividend
yield, ğœis the stock price volatility (which are all constants) and {Wt âˆ¶0 â‰¤t â‰¤T} is a
â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
At time t, we consider a trader who has a portfolio valued at Î t, given as
Î t = ğœ™tSt + ğœ“tBt
where he holds ğœ™t shares of stock and ğœ“t units being invested in a risk-free asset. Show
that the portfolio (ğœ™t, ğœ“t) is self-financing if and only if Î t satisfies the Blackâ€“Scholes
equation with continuous dividend yield
ğœ•Î t
ğœ•t + 1
2ğœ2 ğœ•2Î t
ğœ•S2
t
+ (r âˆ’D)St
ğœ•Î t
ğœ•St
âˆ’rÎ t = 0.
Solution: By applying Taylorâ€™s theorem on dÎ t,
dÎ t = ğœ•Î t
ğœ•t dt + ğœ•Î t
ğœ•St
dSt + 1
2
ğœ•2Î t
ğœ•S2
t
dS2
t + . . .
and since dS2
t = ((ğœ‡âˆ’D)Stdt + ğœStdWt)2 = ğœ2S2
t dt such that (dt)ğœˆ= 0, ğœˆâ‰¥2, we have
dÎ t = ğœ•Î t
ğœ•t dt + ğœ•Î t
ğœ•St
dSt + 1
2ğœ2 ğœ•2Î t
ğœ•S2
t
dt
= ğœ•Î t
ğœ•St
dSt +
(
ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
)
dt.
Since the trader will receive DStdt for every stock held, the portfolio (ğœ™t, ğœ“t) is
self-financing if and only if
dÎ t = ğœ™tdSt + ğœ“tdBt + ğœ™tDStdt = ğœ™tdSt + (rBtğœ“t + ğœ™tDSt)dt.
By equating both of the equations we have
rBtğœ“t + ğœ™tDSt = ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
and
ğœ™t = ğœ•Î t
ğœ•St

238
4.2.3
Risk-Neutral Measure
and substituting the above two equations into
Î t = ğœ™tSt + ğœ“tBt
we have
ğœ•Î t
ğœ•t + 1
2ğœ2S2
t
ğœ•2Î t
ğœ•S2
t
+ (r âˆ’D)St
ğœ•Î t
ğœ•St
âˆ’rÎ t = 0.
â—½
15. Foreign Exchange Rate under Domestic Risk-Neutral Measure. We consider a foreign
exchange (FX) market which at time t consists of a foreign-to-domestic FX spot rate Xt, a
risk-free asset in domestic currency Bd
t and a risk-free asset in foreign currency Bf
t . Here,
XtBf
t denotes the foreign risk-free asset quoted in domestic currency. Assume that the
evolution of these values has the following diffusion processes
dXt = ğœ‡tXtdt + ğœtXtdWx
t , dBd
t = rd
t Bd
t dt and dBf
t = rf
t Bf
t
where ğœ‡t is the drift parameter, ğœt is the volatility parameter, rd
t is the domestic risk-free
rate and rf
t is the foreign risk-free rate (which are all time dependent) and {Wx
t âˆ¶0 â‰¤t â‰¤
T} is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
From the discounted foreign risk-free asset in domestic currency
ÌƒXt = XtBf
t
Bd
t
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent domestic
risk-neutral measure â„šd then ÌƒXt is a â„šd-martingale.
Finally, show that under the â„šd measure the FX rate follows
dXt = (rd
t âˆ’rf
t )Xt + ğœtXtd ÌƒWd
t
where ÌƒWd
t = Wx
t + âˆ«
t
0
ğœ†u du is a â„šd-standard Wiener process with ğœ†t = ğœ‡t + rf
t âˆ’rd
t
ğœt
.
Solution: By applying Taylorâ€™s theorem on dÌƒXt,
dÌƒXt = ğœ•ÌƒXt
ğœ•Xt
dXt + ğœ•ÌƒXt
ğœ•Bf
t
dBf
t + ğœ•ÌƒXt
ğœ•Bd
t
dBd
t + 1
2
ğœ•2ÌƒXt
ğœ•X2
t
(dXt)2 + 1
2
ğœ•ÌƒXt
ğœ•Bf
t
(dBf
t )2 + 1
2
ğœ•ÌƒXt
ğœ•Bd
t
(dBd
t )2
+ ğœ•2ÌƒXt
ğœ•Xtğœ•Bf
t
(dXt)(dBf
t ) +
ğœ•2ÌƒXt
ğœ•Xtğœ•Bd
t
(dXt)(dBd
t ) +
ğœ•2ÌƒXt
ğœ•Bf
t ğœ•Bd
t
(dBf
t)(dBd
t ) + . . .

4.2.3
Risk-Neutral Measure
239
and from ItÂ¯oâ€™s formula,
dÌƒXt = ğœ•ÌƒXt
ğœ•Xt
dXt + ğœ•ÌƒXt
ğœ•Bf
t
dBf
t + ğœ•ÌƒXt
ğœ•Bd
t
dBd
t + 1
2ğœ2
t X2
t
ğœ•2ÌƒXt
ğœ•X2
t
dt
=
(
Bf
t
Bd
t
)
(ğœ‡tXtdt + ğœtXtdWx
t ) +
(
Xt
Bd
t
)
Bf
trf
t dt âˆ’
(
XtBf
t
(Bd
t )2
)
rd
t Bd
t dt
=
(
ğœ‡t + rf
t âˆ’rd
t
)
ÌƒXtdt + ğœtÌƒXtdWx
t
= ğœtÌƒXt
[(
ğœ‡t + rf
t âˆ’rd
t
ğœt
)
dt + dWx
t
]
= ğœtÌƒXtd ÌƒWd
t
where ÌƒWd
t = Wx
t + âˆ«
t
0
ğœ†u du such that ğœ†t = ğœ‡t + rf
t âˆ’rd
t
ğœt
. From Girsanovâ€™s theorem there
exists an equivalent domestic risk-neutral measure â„šd on the filtration â„±s, 0 â‰¤s â‰¤t
defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğœ†uduâˆ’1
2 âˆ«s
0 ğœ†2
udWx
u
so that ÌƒWd
t is a â„šd-standard Wiener process.
Therefore, by substituting
dWx
t = d ÌƒWd
t âˆ’
(
ğœ‡t + rf
t âˆ’rd
t
ğœt
)
dt
into dXt = ğœ‡tXt dt + ğœtXtdWx
t , under the domestic risk-neutral measure â„šd, the FX spot
rate SDE is
dXt = (rd
t âˆ’rf
t )Xt + ğœtXtd ÌƒWd
t .
â—½
16. Foreign Exchange Rate under Foreign Risk-Neutral Measure. We consider a foreign
exchange (FX) market which at time t consists of a foreign-to-domestic FX spot rate Xt, a
risk-free asset in domestic currency Bd
t and a risk-free asset in foreign currency Bf
t . Here,
Bd
t âˆ•Xt denotes the domestic risk-free asset quoted in foreign currency. Assume that the
evolution of these values has the following diffusion processes
dXt = ğœ‡tXtdt + ğœtXtdWx
t , dBd
t = rd
t Bd
t dt and dBf
t = rf
t Bf
t
where ğœ‡t is the drift parameter, ğœt is the volatility parameter, rd
t is the domestic risk-free
rate and rf
t is the foreign risk-free rate (which are all time dependent) and {Wx
t âˆ¶0 â‰¤t â‰¤
T} is a â„™-standard Wiener process on the probability space (Î©, â„±, â„™).
From the discounted domestic risk-free asset in foreign currency
ÌƒXt = Bd
t
XtBf
t

240
4.2.3
Risk-Neutral Measure
show, using Girsanovâ€™s theorem, that by changing the measure â„™to an equivalent foreign
risk-neutral measure â„šf then ÌƒXt is a â„šf-martingale.
Finally, show that under the â„šf measure the FX rate follows
dXt = (rd
t âˆ’rf
t + ğœ2
t )Xt + ğœtXtd ÌƒWf
t
where ÌƒWf
t = Wx
t + âˆ«
t
0
ğœ†u du is a â„šf -standard Wiener process with ğœ†t = ğœ‡t + rf
t âˆ’rd
t âˆ’ğœ2
t
ğœt
.
Solution: By applying Taylorâ€™s theorem on dÌƒXt,
dÌƒXt = ğœ•ÌƒXt
ğœ•Xt
dXt + ğœ•ÌƒXt
ğœ•Bf
t
dBf
t + ğœ•ÌƒXt
ğœ•Bd
t
dBd
t + 1
2
ğœ•2ÌƒXt
ğœ•X2
t
(dXt)2 + 1
2
ğœ•ÌƒXt
ğœ•Bf
t
(dBf
t )2 + 1
2
ğœ•ÌƒXt
ğœ•Bd
t
(dBd
t )2
+ ğœ•2ÌƒXt
ğœ•Xtğœ•Bf
t
(dXt)(dBf
t ) +
ğœ•2ÌƒXt
ğœ•Xtğœ•Bd
t
(dXt)(dBd
t ) +
ğœ•2ÌƒXt
ğœ•Bf
t ğœ•Bd
t
(dBf
t)(dBd
t ) + . . .
and from ItÂ¯oâ€™s formula,
dÌƒXt = ğœ•ÌƒXt
ğœ•Xt
dXt + ğœ•ÌƒXt
ğœ•Bf
t
dBf
t + ğœ•ÌƒXt
ğœ•Bd
t
dBd
t + 1
2ğœ2
t X2
t
ğœ•2ÌƒXt
ğœ•X2
t
dt
= âˆ’
(
Bd
t
X2
t Bf
t
)
(ğœ‡tXtdt + ğœtXtdWx
t ) âˆ’
(
Bd
t
Xt(Bf
t )2
)
Bf
t rf
t dt +
(
1
XtBf
t
)
rd
t Bd
t dt
+
(
Bd
t
X3
t Bf
t
)
ğœ2
t X2
t dt
=
(
rd
t âˆ’rf
t + ğœ2
t âˆ’ğœ‡t
)
ÌƒXtdt âˆ’ğœtÌƒXtdWx
t
= âˆ’ğœtÌƒXt
[(
ğœ‡t + rf
t âˆ’rd
t âˆ’ğœ2
t
ğœt
)
dt + dWx
t
]
= âˆ’ğœtÌƒXtd ÌƒWf
t
where ÌƒWf
t = Wx
t + âˆ«
t
0
ğœ†u du such that ğœ†t = ğœ‡t + rf
t âˆ’rd
t âˆ’ğœ2
t
ğœt
. From Girsanovâ€™s theorem
there exists an equivalent foreign risk-neutral measure â„šf on the filtration â„±s, 0 â‰¤s â‰¤t
defined by the Radonâ€“NikodÂ´ym derivative
Zs = eâˆ’âˆ«s
0 ğœ†uduâˆ’1
2 âˆ«s
0 ğœ†2
udWx
u
so that ÌƒWf
t is a â„šf -standard Wiener process.

4.2.3
Risk-Neutral Measure
241
Therefore, by substituting
dWx
t = d ÌƒWf
t âˆ’
(
ğœ‡t + rf
t âˆ’rd
t âˆ’ğœ2
t
ğœt
)
dt
into dXt = ğœ‡tXt dt + ğœtXtdWx
t , under the foreign risk-neutral measure â„šf , the FX spot rate
SDE is
dXt = (rd
t âˆ’rf
t + ğœ2
t )Xt + ğœtXtd ÌƒWf
t .
â—½
17. Foreign-Denominated Stock Price under Domestic Risk-Neutral Measure. Let (Î©, â„±, â„™)
be a probability space and let {Ws
t âˆ¶0 â‰¤t â‰¤T} and {Wx
t âˆ¶0 â‰¤t â‰¤T} be â„™-standard
Wiener processes on the filtration â„±t, 0 â‰¤t â‰¤T. Let St and Xt denote the stock price
quoted in foreign currency and the foreign-to-domestic exchange rate, respectively, each
having the following SDEs
dSt = (ğœ‡s âˆ’Ds)Stdt + ğœsStdWs
t
dXt = ğœ‡xXtdt + ğœxXtdWx
t
dWs
t â‹…dWx
t = ğœŒdt,
ğœŒâˆˆ(âˆ’1, 1)
where ğœ‡s, Ds and ğœs are the stock price drift, continuous dividend yield and volatility,
respectively, whilst ğœ‡x and ğœx are the exchange rate drift and volatility, respectively. Here,
we assume Ws
t and Wx
t are correlated with correlation coefficient ğœŒâˆˆ(âˆ’1, 1). In addition,
let Bf
t and Bd
t be the risk-free assets in foreign and domestic currencies, respectively, having
the following differential equations
dBf
t = rf Bf
tdt and dBd
t = rdBd
t dt
where rf and rd are the foreign and domestic risk-free rates.
Show that for the stock price denominated in domestic currency, XtSt follows the diffusion
process
d(XtSt)
XtSt
= (ğœ‡s + ğœ‡x + ğœŒğœsğœs âˆ’Ds)dt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xdWxs
t
where Wxs
t
=
ğœsWs
t + ğœxWx
t
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
x
is a â„™-standard Wiener process.
Using Girsanovâ€™s theorem show that under the domestic risk-neutral measure â„šd, the stock
price denominated in domestic currency has the diffusion process
d(XtSt)
XtSt
= (rd âˆ’Ds)dt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xd ÌƒWxs
t
where ÌƒWxs
t
= Wxs
t +
â›
âœ
âœ
âœâ
ğœ‡s + ğœ‡x + ğœŒğœsğœs âˆ’rd
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
x
â
âŸ
âŸ
âŸâ 
t is a â„šd-standard Wiener process.

242
4.2.3
Risk-Neutral Measure
Solution: From Problem 3.2.3.3 (page 158) we can easily show that for XtSt, its diffusion
process is
d(XtSt)
XtSt
= (ğœ‡s + ğœ‡x + ğœŒğœsğœx âˆ’Ds)dt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xdWxs
t
where Wxs
t
=
ğœsWs
t + ğœxWx
t
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
x
is a â„™-standard Wiener process.
At time t, we let the portfolio Î t be valued as
Î t = ğœ™tUt + ğœ“tBd
t
where ğœ™t and ğœ“t are the units invested in Ut = XtSt and the risk-free asset Bd
t , respectively.
Taking note that the holder will receive DsUtdt for every stock held, then
dÎ t = ğœ™t(dUt + DsUtdt) + ğœ“trdBd
t dt
= ğœ™t
[
(ğœ‡s + ğœ‡x + ğœŒğœsğœx)Utdt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xUtdWxs
t
]
+ ğœ“trdBd
t dt
= rdÎ tdt + ğœ™t
[
(ğœ‡s + ğœ‡x + ğœŒğœsğœx âˆ’rd)Utdt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xUtdWxs
t
]
= rdÎ tdt + ğœ™t
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xUtd ÌƒWxs
t
where ÌƒWxs
t
= ğœ†t + Wxs
t such that ğœ†= ğœ‡s + ğœ‡x + ğœŒğœsğœx âˆ’rd
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
x
.
By applying Girsanovâ€™s theorem to change the measure â„™to an equivalent risk-neutral
measure â„šd, under which ÌƒWxs
t is a â„šd-standard Wiener process, then the discounted port-
folio eâˆ’rdtÎ t is a â„šd-martingale.
Finally, by substituting dWxs
t
= d ÌƒWxs
t âˆ’ğœ†dt into
d(XtSt)
XtSt
= (ğœ‡s + ğœ‡x + ğœŒğœsğœs âˆ’Ds)dt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xdWxs
t
the stock price diffusion process under the risk-neutral measure â„šd becomes
d(XtSt)
XtSt
= (rd âˆ’Ds)dt +
âˆš
ğœ2
s + 2ğœŒğœsğœx + ğœ2
xd ÌƒWxs
t .
â—½

5
Poisson Process
In mathematical finance the most important stochastic process is the Wiener process, which
is used to model continuous asset price paths. The next important stochastic process is the
Poisson process, used to model discontinuous random variables. Although time is continuous,
the variable is discontinuous where it can represent a â€œjumpâ€ in an asset price (e.g., electricity
prices or a credit risk event, such as describing default and rating migration scenarios). In
this chapter we will discuss the Poisson process and some generalisations of it, such as the
compound Poisson process and the Cox process (or doubly stochastic Poisson process) that
are widely used in credit risk theory as well as in modelling energy prices.
5.1
INTRODUCTION
In this section, before we provide the definition of a Poisson process, we first define what a
counting process is.
Definition 5.1 (Counting Process) Let (Î©, â„±, â„™) be a probability space. A stochastic process
{Nt âˆ¶t â‰¥0} where Nt denotes the number of events that have occurred in the interval (0, t] is
said to be a counting process if it has the following properties:
(a) Nt â‰¥0;
(b) Nt is an integer value;
(c) for s < t, Ns â‰¤Nt and Nt âˆ’Ns is the number of events occurring in (s, t].
Once we have defined a counting process we can now give a proper definition of a Poisson
process. Basically, there are three definitions of a Poisson process (or homogeneous Poisson
process) and they are all equivalent.
Definition 5.2(a) Let (Î©, â„±, â„™) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt âˆ¶t â‰¥0} with intensity ğœ†> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
â„™(Nt+h = i + j|Nt = i) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
j = 0
ğœ†h + o(h)
j = 1
o(h)
j > 1.

244
5.1
INTRODUCTION
Definition 5.2(b) Let (Î©, â„±, â„™) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt âˆ¶t â‰¥0} with intensity ğœ†> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) â„™(Nt = k) = (ğœ†t)keâˆ’ğœ†t
k!
, k = 0, 1, 2, . . .
Definition 5.2(c) Let (Î©, â„±, â„™) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt âˆ¶t â‰¥0} with intensity ğœ†> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) the inter-arrival times of events ğœ1, ğœ2, . . . form a sequence of independent and identically
distributed random variables where ğœi âˆ¼Exp(ğœ†), i = 1, 2, . . . ;
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
â„™(Nt+h = i + j|Nt = i) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
j = 0
ğœ†h + o(h)
j = 1
o(h)
j > 1.
Like a standard Wiener process, the Poisson process has independent and stationary incre-
ments and so it is also a Markov process.
Theorem 5.3 (Markov Property) Let (Î©, â„±, â„™) be a probability space. The Poisson process
{Nt âˆ¶t â‰¥0} is a Markov process such that the conditional distribution of Nt given the filtration
â„±s, s < t depends only on Ns.
In addition, the Poisson process also has a strong Markov property.
Theorem 5.4 (Strong Markov Property) Let (Î©, â„±, â„™) be a probability space. If {Nt âˆ¶t â‰¥
0} is a Poisson process and given â„±t is the filtration up to time t then for s > 0, Nt+s âˆ’Nt âŸ‚âŸ‚â„±t.
By modifying the intensity parameter we can construct further definitions of a Poisson pro-
cess and the following definitions (which are all equivalent) describe a non-homogeneous
Poisson process with intensity ğœ†t being a deterministic function of time.
Definition 5.5(a) Let (Î©, â„±, â„™) be a probability space. A Poisson process (or non-
homogeneous Poisson process) {Nt âˆ¶t â‰¥0} with intensity function ğœ†t âˆ¶â„+ î‚¶â†’â„+ is a
counting process with the following properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;

5.1
INTRODUCTION
245
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
â„™(Nt+h = i + j|Nt = i) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†th + o(h)
j = 0
ğœ†th + o(h)
j = 1
o(h)
j > 1.
Definition 5.5(b) Let (Î©, â„±, â„™) be a probability space. A Poisson process (or non-
homogeneous Poisson process) {Nt âˆ¶t â‰¥0} with intensity function ğœ†t âˆ¶â„+ î‚¶â†’â„+ is a
counting process with the following properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) â„™(Nt = k) = (Î›t)keâˆ’Î›t
k!
, k = 0, 1, 2, . . . where Î›t = âˆ«
t
0
ğœ†u du is known as the intensity
measure (hazard function or cumulative intensity).
It should be noted that if ğœ†t is itself a random process, then {Nt âˆ¶t â‰¥0} is called a doubly
stochastic Poisson process, conditional Poisson process or Cox process. Given that ğœ†t is a
random process, the resulting stochastic process {Î›t âˆ¶t â‰¥0} defined as
Î›t = âˆ«
t
0
ğœ†u du
is known as the hazard process. In credit risk modelling, due to the stochastic process of the
intensity, the Cox process can be used to model the random occurrence of a default event, or
even the number of contingent claims in actuarial models (claims that can be made only if one
or more specified events occurs).
Given that the Poisson process Nt with intensity ğœ†is an increasing counting process, therefore
it is not a martingale. However, in its compensated form, Nt âˆ’ğœ†t is a martingale.
Theorem 5.6 Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the prob-
ability space (Î©, â„±, â„™) with respect to the filtration â„±t. By defining the compensated Poisson
process Ì‚Nt as
Ì‚Nt = Nt âˆ’ğœ†t
then Ì‚Nt is a martingale.
In the following we define an important generalisation of the Poisson process known as a
compound Poisson process, where the jump size (or amplitude) can be modelled as a random
variable.
Definition 5.7 (Compound Poisson Process) Let {Nt âˆ¶t â‰¥0} be a Poisson process with
intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with respect to the filtration â„±t,
and let X1, X2, . . . be a sequence of independent and identically distributed random variables

246
5.1
INTRODUCTION
with common mean ğ”¼(Xi) = ğ”¼(X) = ğœ‡and variance Var (Xi) = Var (X) = ğœ2. Assume also that
X1, X2, . . . are independent of Nt. The compound Poisson process Mt is defined as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0.
From the definition we can deduce that although the jumps in Nt are always of one unit, the
jumps in Mt are of random size since Xt is a random variable. The only similarity between
the two processes is that the jumps in Nt and Mt occur at the same time. Furthermore, like the
Poisson process, the compound Poisson process Mt also has the independent and stationary
increments property.
Another important generalisation of the Poisson process is to add a drift and a standard
Wiener process term to generate a jump diffusion process of the form
dXt = ğœ‡(Xtâˆ’, t) dt + ğœ(Xtâˆ’, t)dWt + J(Xtâˆ’, t)dNt
where J(Xtâˆ’, t) is a random variable denoting the size of the jump and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
In the presence of jumps, Xt is a cÃ dlÃ g process, where it is continuous from the right
lim
uâ†‘t Xu = lim
uâ†’t+ Xu = Xt
which is also inclusive of any jumps at time t. To specify the value just before a jump we write
Xtâˆ’, which is the limit from the left:
lim
uâ†“t Xu = lim
uâ†’tâˆ’Xu = Xtâˆ’.
Let Xt be a cÃ dlÃ g process with jump times ğœ1 < ğœ2 < . . . The integral of a function ğœ“t with
respect to Xt is defined by the pathwise Lebesgueâ€“Stieltjes integral
âˆ«
t
0
ğœ“s dXs =
âˆ‘
0<sâ‰¤t
ğœ“sÎ”Xs =
âˆ‘
0<sâ‰¤t
ğœ“s(Xs âˆ’Xsâˆ’) =
Xt
âˆ‘
s=1
ğœ“s
where the size of the jump, Î”Xt is denoted by
Î”Xt = Xt âˆ’Xtâˆ’
and given there is no jump at time zero we therefore set Î”X0 = 0.
Theorem 5.8 (One-Dimensional ItÂ¯o Formula for Jump Diffusion Process) Consider a
stochastic process Xt satisfying the following SDE:
dXt = ğœ‡(Xtâˆ’, t) dt + ğœ(Xtâˆ’, t)dWt + J(Xtâˆ’, t)dNt
or in integrated form
Xt = X0 + âˆ«
t
0
ğœ‡(Xsâˆ’, s) ds + âˆ«
t
0
ğœ(Xsâˆ’, s) dWs + âˆ«
t
0
J(Xsâˆ’, s) dNs

5.1
INTRODUCTION
247
with âˆ«
t
0
{|ğœ‡(Xs, s)| + ğœ(Xs, s)2} ds < âˆ. Then, for a function f(Xt, t), the stochastic process
Yt = f(Xt, t) satisfies
dYt = ğœ•f
ğœ•t (Xtâˆ’, t) dt + ğœ•f
ğœ•Xt
(Xtâˆ’, t)dXt + 1
2!
ğœ•2f
ğœ•X2
t
(Xtâˆ’, t)(dXt)2 + 1
3!
ğœ•3f
ğœ•X3
t
(Xtâˆ’, t)(dXt)3 + . . .
=
[
ğœ•f
ğœ•t (Xtâˆ’, t) + ğœ‡(Xtâˆ’, t) ğœ•f
ğœ•Xt
(Xtâˆ’, t) + 1
2ğœ(Xtâˆ’, t)2 ğœ•2f
ğœ•X2
t
(Xtâˆ’, t)
]
dt
+ ğœ(Xtâˆ’, t) ğœ•f
ğœ•Xt
(Xtâˆ’, t)dWt + [ f(Xtâˆ’+ Jtâˆ’, t) âˆ’f(Xtâˆ’, t)]dNt
where Jtâˆ’= J(Xtâˆ’, t), (dXt)m, m = 1, 2, . . . are expressed according to the rule
( dt)2 = ( dt)3 = . . . = 0
(dWt)2 = dt,
(dWt)3 = (dWt)4 = . . . = 0
(dNt)2 = (dNt)3 = . . . = dNt
dWt dt = dNt dt = dNtdWt = 0.
In integrated form
Yt = Y0 + âˆ«
t
0
ğœ•f
ğœ•t (Xsâˆ’, s) ds + âˆ«
t
0
ğœ•f
ğœ•Xt
(Xsâˆ’, s) dXs
+ 1
2! âˆ«
t
0
ğœ•2f
ğœ•X2
t
(Xsâˆ’, s)(dXs)2 + 1
3! âˆ«
t
0
ğœ•3f
ğœ•X3
t
(Xsâˆ’, s)(dXs)3 + . . .
= Y0 + âˆ«
t
0
[
ğœ•f
ğœ•t (Xsâˆ’, s) + ğœ‡(Xsâˆ’, s) ğœ•f
ğœ•Xt
(Xsâˆ’, s) + 1
2ğœ(Xsâˆ’, s)2 ğœ•2f
ğœ•X2
t
(Xsâˆ’, s)
]
ds
+ âˆ«
t
0
ğœ(Xsâˆ’, s) ğœ•f
ğœ•Xt
(Xsâˆ’, s) dWs + âˆ«
t
0
[ f(Xsâˆ’+ Jsâˆ’, s) âˆ’f(Xsâˆ’, s)] dNs
where
âˆ«
t
0
[ f(Xsâˆ’+ Jsâˆ’, s) âˆ’f(Xsâˆ’, s)] dNs =
âˆ‘
0<sâ‰¤t
[ f(Xs, s) âˆ’f(Xsâˆ’, s)] Î”Ns
=
Nt
âˆ‘
s=1
[ f(Xs, s) âˆ’f(Xsâˆ’, s)].
The following is the multi-dimensional version of ItÂ¯oâ€™s formula for a jump diffusion process.
Theorem 5.9 (Multi-Dimensional ItÂ¯o Formula for Jump Diffusion Process) Consider the
stochastic processes X(i)
t , i = 1, 2, . . . , n each satisfying the following SDE:
dX(i)
t
= ğœ‡(X(i)
tâˆ’, t) dt + ğœ(X(i)
tâˆ’, t)dWt + J(X(i)
tâˆ’, t)dNt

248
5.1
INTRODUCTION
or in integrated form
X(i)
t
= X(i)
0 + âˆ«
t
0
ğœ‡(X(i)
sâˆ’, s) ds + âˆ«
t
0
ğœ(X(i)
sâˆ’, s) dWs + âˆ«
t
0
J(X(i)
sâˆ’, s) dNs
with âˆ«
t
0
{
|ğœ‡(X(i)
s , s)| + ğœ(X(i)
s , s)2}
ds < âˆ. Then for a function f(X(1)
t , X(2)
t , . . . , X(n)
t , t), the
stochastic process Yt = f(X(1)
t , X(2)
t , . . . , X(n)
t , t) satisfies
dYt = ğœ•f
ğœ•t (X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t) dt +
n
âˆ‘
i=1
ğœ•f
ğœ•X(i)
t
(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)dX(i)
t
+
n
âˆ‘
i=1
n
âˆ‘
j=1
1
2!
ğœ•2f
ğœ•X(i)
t ğœ•X(j)
t
(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)dX(i)
t dX(j)
t + . . .
=
[
ğœ•f
ğœ•t (X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t) +
n
âˆ‘
i=1
ğœ‡(X(i)
tâˆ’, t) ğœ•f
ğœ•Xt
(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)
+
n
âˆ‘
i=1
n
âˆ‘
j=1
1
2ğœ(X(i)
tâˆ’, t)ğœ(X(j)
tâˆ’, t)
ğœ•2f
ğœ•X(i)
t ğœ•X(j)
t
(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)
]
dt
+
n
âˆ‘
i=1
ğœ(X(i)
tâˆ’, t) ğœ•f
ğœ•X(i)
t
(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)dWt
+
[
f(X(1)
tâˆ’+ J(1)
tâˆ’, X(2)
tâˆ’+ J(2)
tâˆ’, . . . , X(n)
tâˆ’+ J(n)
tâˆ’, t) âˆ’f(X(1)
tâˆ’, X(2)
tâˆ’, . . . , X(n)
tâˆ’, t)
]
dNt
where J(i)
tâˆ’= J(X(i)
tâˆ’, t), (dXt)m, m = 1, 2, . . . are expressed according to the rule
( dt)2 = ( dt)3 = . . . = 0
(dWt)2 = dt,
(dWt)3 = (dWt)4 = . . . = 0
(dNt)2 = (dNt)3 = . . . = dNt
dWt dt = dNt dt = dNtdWt = 0.
In integrated form
Yt = Y0 + âˆ«
t
0
ğœ•f
ğœ•t (X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s) ds + âˆ«
t
0
[ n
âˆ‘
i=1
ğœ•f
ğœ•X(i)
t
(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s) dX(i)
s
]
+ âˆ«
t
0
[ n
âˆ‘
i=1
n
âˆ‘
j=1
1
2!
ğœ•2f
ğœ•X(i)
t ğœ•X(j)
t
(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s) dX(i)
s dX(j)
s
]
+ . . .
= Y0 + âˆ«
t
0
[
ğœ•f
ğœ•t (X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s) +
n
âˆ‘
i=1
ğœ‡(X(i)
sâˆ’, s) ğœ•f
ğœ•Xt
(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)

5.1
INTRODUCTION
249
+
n
âˆ‘
i=1
n
âˆ‘
j=1
1
2ğœ(X(i)
sâˆ’, s)ğœ(X(j)
sâˆ’, s)
ğœ•2f
ğœ•X(i)
t ğœ•X(j)
t
(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)
]
ds
+ âˆ«
t
0
[ n
âˆ‘
i=1
ğœ(X(i)
sâˆ’, s) ğœ•f
ğœ•X(i)
t
(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)dWs
]
+ âˆ«
t
0
[
f(X(1)
sâˆ’+ J(1)
sâˆ’, X(2)
sâˆ’+ J(2)
sâˆ’, . . . , X(n)
sâˆ’+ J(n)
sâˆ’, s) âˆ’f(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)
]
dNs
where
âˆ«
t
0
[
f(X(1)
sâˆ’+ J(1)
sâˆ’, X(2)
sâˆ’+ J(2)
sâˆ’, . . . , X(n)
sâˆ’+ J(n)
sâˆ’, s) âˆ’f(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)
]
dNs
=
âˆ‘
0<sâ‰¤t
[
f(X(1)
s , X(2)
s , . . . , X(n)
s s) âˆ’f(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)
]
Î”Ns
=
Nt
âˆ‘
s=1
[
f(X(1)
s , X(2)
s , . . . , X(n)
s s) âˆ’f(X(1)
sâˆ’, X(2)
sâˆ’, . . . , X(n)
sâˆ’, s)
]
.
By recalling that we can use Girsanovâ€™s theorem to change the measure so that a Wiener
process (standard Wiener process with drift) becomes a standard Wiener process, we can also
change the measure for both Poisson and compound Poisson processes. For a Poisson process,
the outcome of the change of measure affects the intensity whilst for a compound Poisson
process, the change of measure affects both the intensity and the distribution of the jump ampli-
tudes. In the following we provide general results for the change of measure for Poisson and
compound Poisson processes.
Theorem 5.10 (Girsanovâ€™s Theorem for Poisson Process) Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Pois-
son process with intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with respect to the
filtration â„±t, 0 â‰¤t â‰¤T. Let ğœ‚> 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt.
By changing the measure â„™to a measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then, under the â„šmeasure, Nt âˆ¼Poisson(ğœ‚t) with intensity ğœ‚> 0.
Theorem 5.11 (Girsanovâ€™s Theorem for Compound Poisson Process) Let {Nt âˆ¶0 â‰¤t â‰¤
T} be a Poisson process with intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with
respect to the filtration â„±t, 0 â‰¤t â‰¤T and let X1, X2, . . . be a sequence of independent and
identically distributed random variables where each Xi, i = 1, 2, . . . has a probability density
function f â„™(Xi) under the â„™measure. Let X1, X2, . . . also be independent of Nt and Wt. From
the definition of the compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0

250
5.1
INTRODUCTION
we let ğœ‚> 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt =
â§
âª
âª
â¨
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under the â„š
measure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let â„šbe abso-
lutely continuous with respect to â„™on â„±. By changing the measure â„™to measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then, under the â„šmeasure, Mt is a compound Poisson process with intensity ğœ‚> 0 and Xi,
i = 1, 2, . . . is a sequence of independent and identically distributed random variables with
probability mass (density) functions â„š(Xi) (f â„š(Xi)), i = 1, 2, . . .
By augmenting the Wiener process to the Poisson and compound Poisson processes we have
the following results.
Theorem 5.12 (Girsanovâ€™s Theorem for Poisson Process and Standard Wiener Pro-
cess) Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 and {Wt âˆ¶0 â‰¤t â‰¤T}
be a standard Wiener process defined on the probability space (Î©, â„±, â„™) with respect to the
filtration â„±t, t â‰¥0. Suppose ğœƒt, 0 â‰¤t â‰¤T is an adapted process and ğœ‚> 0. We consider the
Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
= e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du.
By changing the measure â„™to measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then, under the â„šmeasure, Nt is a Poisson process with intensity ğœ‚> 0, the process ÌƒWt =
Wt + âˆ«
t
0
ğœƒu du is a â„š-standard Wiener process and Nt âŸ‚âŸ‚ÌƒWt.
Theorem 5.13 (Girsanovâ€™s Theorem for Compound Poisson Process and Standard
Wiener Process) Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 and
{Wt âˆ¶0 â‰¤t â‰¤T} be a standard Wiener process defined on the probability space (Î©, â„±, â„™)

5.2.1
Properties of Poisson Process
251
with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose ğœƒt, 0 â‰¤t â‰¤T is an adapted process,
ğœ‚> 0, X1, X2, . . . is a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function â„™(Xi) > 0 (f â„™(Xi) > 0)
under the â„™measure and assume also that the sequence X1, X2, . . . is independent of Nt. We
consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
=
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under the
â„šmeasure and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
u du)
< âˆ. For the case of continuous random variables Xi, i =
1, 2, . . . we also let â„šbe absolutely continuous with respect to â„™on â„±. By changing the
measure â„™to measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then, under the â„šmeasure, the process Mt = âˆ‘Nt
i=1 Xi is a compound Poisson process with
intensity ğœ‚> 0 where Xi, i = 1, 2, . . . is a sequence of independent and identically distributed
random variables with probability mass (density) functions â„š(Xi) (f â„š(Xi)), i = 1, 2, . . . , the
process ÌƒWt = Wt + âˆ«
t
0
ğœƒu du is a â„šstandard Wiener process and Mt âŸ‚âŸ‚ÌƒWt.
5.2
PROBLEMS AND SOLUTIONS
5.2.1
Properties of Poisson Process
1. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0. Show that Cov (Ns, Nt
) = ğœ†min{s, t} and deduce that the correlation coefficient
of Ns and Nt is
ğœŒ=
âˆš
min{s, t}
max{s, t}.
Solution: Since Nt âˆ¼Poisson(ğœ†t) with ğ”¼(Nt
) = ğœ†t, ğ”¼(N2
t
) = ğœ†t + ğœ†2t2 and by definition
Cov (Ns, Nt
) = ğ”¼(NsNt
) âˆ’ğ”¼(Ns
) ğ”¼(Nt
) .

252
5.2.1
Properties of Poisson Process
For s â‰¤t, using the stationary and independent properties of a Poisson process,
ğ”¼(NsNt
) = ğ”¼(Ns
(Nt âˆ’Ns
) + N2
s
)
= ğ”¼(Ns
(Nt âˆ’Ns
)) + ğ”¼(N2
s
)
= ğ”¼(Ns
) ğ”¼(Nt âˆ’Ns
) + ğ”¼(N2
s
) ,
Ns âŸ‚âŸ‚Nt âˆ’Ns
= ğœ†s â‹…ğœ†(t âˆ’s) + ğœ†s + ğœ†2s2
= ğœ†2st + ğœ†s
which implies
Cov (Ns, Nt
) = ğœ†2st + ğœ†s âˆ’ğœ†2st = ğœ†s.
Similarly, for t â‰¤s,
ğ”¼(NsNt
) = ğ”¼(Nt
(Ns âˆ’Nt
) + N2
t
)
= ğ”¼(Nt
(Ns âˆ’Nt
)) + ğ”¼(N2
t
)
= ğ”¼(Nt
) ğ”¼(Ns âˆ’Nt
) + ğ”¼(N2
t
) ,
Nt âŸ‚âŸ‚Ns âˆ’Nt
= ğœ†t â‹…ğœ†(s âˆ’t) + ğœ†t + ğœ†2t2
= ğœ†2st + ğœ†t
and hence
Cov(Ns, Nt) = ğœ†2st + ğœ†t âˆ’ğœ†2st = ğœ†t.
Thus, Cov(Ns, Nt) = ğœ†min{s, t}.
By definition, the correlation coefficient of Ns and Nt is defined as
ğœŒ=
Cov (Ns, Nt
)
âˆš
Var (Ns
) Var (Nt
)
= ğœ†min{s, t}
ğœ†
âˆš
st
= min{s, t}
âˆš
st
.
For s â‰¤t,
ğœŒ=
s
âˆš
st
=
âˆš
s
t
whilst for s > t,
ğœŒ=
t
âˆš
st
=
âˆš
t
s.
Therefore, ğœŒ=
âˆš
min{s, t}
max{s, t}.
â—½

5.2.1
Properties of Poisson Process
253
2. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0 satisfying the following conditions for small h > 0 and k âˆˆâ„•:
(a) N0 = 0 and Nt is a non-decreasing counting process;
(b) â„™(Nt+h = i + j|Nt = i) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
j = 0
ğœ†h + o(h)
j = 1
o(h)
j > 1;
(c) Nt has independent stationary increments.
By setting pk(t) = â„™(Nt = k), show that
pâ€²
k(t) =
{
âˆ’ğœ†p0(t)
k = 0
ğœ†pkâˆ’1(t) âˆ’ğœ†pk(t)
k â‰ 0
with boundary condition
pk(0) =
{
1
k = 0
0
k â‰ 0.
By solving the differential-difference equation using mathematical induction or the
moment generating function, show that
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t,
k = 0, 1, 2, . . .
Solution: By definition, for k â‰ 0,
â„™(Nt+h = k) =
âˆ‘
j
â„™(Nt+h = k|Nt = j)â„™(Nt = j)
= â„™(Nt+h = k|Nt = k âˆ’1)â„™(Nt = k âˆ’1)
+ â„™(Nt+h = k|Nt = k)â„™(Nt = k) + o(h)
= (ğœ†h + o(h))â„™(Nt = k âˆ’1) + (1 âˆ’ğœ†h + o(h))â„™(Nt = k).
By setting pk(t + h) = â„™(Nt+h = k), pkâˆ’1(t) = â„™(Nt = k âˆ’1) and pk(t) = â„™(Nt = k), there-
fore for k â‰ 0,
pk(t + h) = ğœ†hpkâˆ’1(t) + (1 âˆ’ğœ†h)pk(t) + o(h)
lim
hâ†’0
pk(t + h) âˆ’pk(t)
h
= ğœ†pkâˆ’1(t) âˆ’ğœ†pk(t) + lim
hâ†’0
o(h)
h
and we will have
pâ€²
k(t) = ğœ†pkâˆ’1(t) âˆ’ğœ†pk(t),
k â‰ 0.
For the case when k = 0,
â„™(Nt+h = 0) = â„™(Nt+h = 0|Nt = 0)â„™(Nt = 0) + o(h)
= (1 âˆ’ğœ†h + o(h))â„™(Nt = 0) + o(h)
or
p0(t + h) = (1 âˆ’ğœ†h)p0(t) + o(h)

254
5.2.1
Properties of Poisson Process
where p0(t + h) = â„™(Nt+h = 0) and p0(t) = â„™(Nt = 0). By subtracting p0(t) from both
sides of the equation, dividing by h and letting h â†’0, we will have
pâ€²
0(t) = âˆ’ğœ†p0(t).
Knowing that â„™(N0 = 0) = 1, hence
pâ€²
k(t) =
{
âˆ’ğœ†p0(t)
k = 0
ğœ†pkâˆ’1(t) âˆ’ğœ†pk(t)
k â‰ 0
with boundary condition
pk(0) =
{
1
k = 0
0
k â‰ 0.
The following are two methods to solve the differential-difference equation.
Method I: Mathematical Induction. Let k = 0. To solve
pâ€²
0(t) + ğœ†p0(t) = 0
we let the integrating factor I = eğœ†t. By multiplying the integrating factor on both sides of
the equation and taking note that p0(0) = â„™(N0 = 0) = 1, we have
â„™(Nt = 0) = eâˆ’ğœ†t.
Therefore, the result is true for k = 0.
Assume the result is true for k = j, j = 0, 1, 2, . . . such that
â„™(Nt = j) = (ğœ†t)jeâˆ’ğœ†t
j!
.
For k = j + 1 the differential-difference equation is
pâ€²
j+1(t) + ğœ†pj+1(t) = ğœ†(ğœ†t)jeâˆ’ğœ†t
j!
.
Setting the integrating factor I = eğœ†t and multiplying it by both sides of the differential
equation,
d
dt(eğœ†tpj+1(t)) = ğœ†j+1tj
j!
or
eğœ†tpj+1(t) = (ğœ†t)j+1
(j + 1)! + C
where C is a constant. Knowing that pj+1(0) = â„™(N0 = j + 1) = 0 we have C = 0 and
hence
â„™(Nt = j + 1) = (ğœ†t)j+1eâˆ’ğœ†t
(j + 1)! .
Therefore, the result is also true for k = j + 1. Thus, using mathematical induction we
have shown
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t,
k = 0, 1, 2, . . .

5.2.1
Properties of Poisson Process
255
Method II: Moment Generating Function. Let
MNt(t, u) = ğ”¼(euNt) =
âˆ
âˆ‘
k=0
eukâ„™(Nt = k) =
âˆ
âˆ‘
k=0
eukpk(t)
be defined as the moment generating function of the Poisson process. Taking the first
partial derivative with respect to t,
ğœ•MNt(t, u)
ğœ•t
=
âˆ
âˆ‘
k=0
eukpâ€²
k(t)
= ğœ†
âˆ
âˆ‘
k=1
eukpkâˆ’1(t) âˆ’ğœ†
âˆ
âˆ‘
k=0
eukpk(t)
= ğœ†eu
âˆ
âˆ‘
k=1
eu(kâˆ’1)pkâˆ’1(t) âˆ’ğœ†
âˆ
âˆ‘
k=0
eukpk(t)
or
ğœ•MNt(t, u)
ğœ•t
= ğœ†(eu âˆ’1)MNt(t, u)
with boundary condition MN0(0, u) = 1.
By setting the integrating factor I = eâˆ’ğœ†(euâˆ’1)t and multiplying it by the first-order differ-
ential equation, we have
ğœ•
ğœ•t
(
MNt(t, u)eâˆ’ğœ†(euâˆ’1)t)
= 0
or
MNt(t, u)eâˆ’ğœ†(euâˆ’1)t = C
where C is a constant. Since MN0(0, u) = 1 we have C = 1 and hence
MNt(t, u) = eğœ†(euâˆ’1)t.
By definition, MNt(t, u) = âˆ‘âˆ
k=0 eukâ„™(Nt = k) and using Taylorâ€™s series expansion we have
MNt(t, u) = eğœ†(euâˆ’1)t = eâˆ’ğœ†t
âˆ
âˆ‘
k=0
(ğœ†teu)k
k!
=
âˆ
âˆ‘
k=0
euk
((ğœ†t)k
k! eâˆ’ğœ†t
)
and hence
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t,
k = 0, 1, 2, . . .
â—½
3. Pure Birth Process. Let (Î©, â„±, â„™) be a probability space. If {Nt âˆ¶t â‰¥0} is a Poisson
process with intensity ğœ†> 0 then for small h > 0 and k âˆˆâ„•, show that it satisfies the
following property:
â„™(Nt+h = k + j|Nt = k) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
j = 0
ğœ†h + o(h)
j = 1
o(h)
j > 1.

256
5.2.1
Properties of Poisson Process
Solution: We first consider j = 0 where
â„™(Nt+h = k|Nt = k) = â„™(Nt+h = k, Nt = k)
â„™(Nt = k)
= â„™(zero arrival between (t, t + h], Nt = k)
â„™(Nt = k)
.
Since the event {Nt = k} relates to arrival during the time interval [0, t] and the event
{zero arrival between (t, t + h]} relates to arrivals after time t, both of the events are inde-
pendent and because Nt has stationary increments,
â„™(Nt+h = k|Nt = k) = â„™(zero arrival between (t, t + h])â„™(Nt = k)
â„™(Nt = k)
= â„™(zero arrival between (t, t + h])
= â„™(Nt+h âˆ’Nt = 0)
= eâˆ’ğœ†h
= 1 âˆ’ğœ†h + (ğœ†h)2
2!
âˆ’(ğœ†h)3
3!
+ . . .
= 1 âˆ’ğœ†h + o(h).
Similarly,
â„™(Nt+h = k + 1|Nt = k) = â„™(1 arrival between (t, t + h])
= â„™(Nt+h âˆ’Nt = 1)
= ğœ†heâˆ’ğœ†h
= ğœ†h
(
1 âˆ’ğœ†h + (ğœ†h)2
2!
âˆ’(ğœ†h)3
3!
+ . . .
)
= ğœ†h + o(h)
and finally
â„™(Nt+h > k + 1|Nt = k) = 1 âˆ’â„™(Nt+h = k|Nt+h = k) âˆ’â„™(Nt+h = k + 1|Nt = k)
= 1 âˆ’(1 âˆ’ğœ†h + o(h)) âˆ’ğœ†h + o(h)
= o(h).
â—½
4. Arrival Time Distribution. Let the inter-arrival times of â€œjumpâ€ events ğœ1, ğœ2, . . . be a
sequence of independent and identically distributed random variables where each ğœi âˆ¼
Exp(ğœ†), ğœ†> 0, i = 1, 2, . . . has a probability density function fğœ(t) = ğœ†eâˆ’ğœ†t, t â‰¥0. By
defining the arrival time of the n-th jump event as
Tn =
n
âˆ‘
i=1
ğœi

5.2.1
Properties of Poisson Process
257
where ğœi = Ti âˆ’Tiâˆ’1, show that the arrival time Tn, n â‰¥1 follows a gamma distribution,
Tn âˆ¼Gamma(n, ğœ†) with probability density function given as
fTn(t) = (ğœ†t)nâˆ’1
(n âˆ’1)!ğœ†eâˆ’ğœ†t,
t â‰¥0.
Let Nt be the number of jumps that occur at or before time t. Explain why for k â‰¥1,
â„™(Nt â‰¥k) = â„™(Tk â‰¤t)
and for k = 0,
â„™(Nt = 0) = â„™(T1 > t).
Using the above results show that Nt is a Poisson process with intensity ğœ†having the
probability mass function
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t,
k = 0, 1, 2, . . .
Solution: The first part of the proof is analogous to Problem 1.2.2.5 (page 16) and we
shall omit it. As for the remaining part of the proof we note that for k â‰¥1, Nt â‰¥k if and
only if there are at least k jump events by time t, which implies that Tk (the time of the
k-th jump) is less than or equal to t. Thus,
â„™(Nt â‰¥k) = â„™(Tk â‰¤t),
k â‰¥1.
On the contrary, for k = 0, Nt = 0 if and only if there are zero jumps by time t, which
implies T1 (the time of the first jump) occurs after time t. Therefore,
â„™(Nt = 0) = â„™(T1 > t).
For k = 0,
â„™(Nt = 0) = â„™(T1 > t) = â„™(ğœ1 > t) = âˆ«
âˆ
t
ğœ†eâˆ’ğœ†s ds = eâˆ’ğœ†t.
For k â‰¥1,
â„™(Nt = k) = â„™(Nt â‰¥k) âˆ’â„™(Nt â‰¥k + 1).
By solving
â„™(Nt â‰¥k + 1) = â„™(Tk+1 â‰¤t) = âˆ«
t
0
(ğœ†s)k
k! ğœ†eâˆ’ğœ†s ds
and using integration by parts âˆ«
b
a
u(x) d
dxğ‘£(x) dx = u(x)ğ‘£(x)
||||
b
a
âˆ’âˆ«
b
a
ğ‘£(x) d
dxu(x) dx, we
set
u = (ğœ†s)k
k!
â‡’du
ds = ğœ†kskâˆ’1
(k âˆ’1)!

258
5.2.1
Properties of Poisson Process
dğ‘£
ds = ğœ†eâˆ’ğœ†s â‡’ğ‘£= âˆ’eâˆ’ğœ†s.
Thus,
â„™(Nt â‰¥k + 1) = âˆ’(ğœ†s)k
k! eâˆ’ğœ†s|||||
s=t
s=0
+ âˆ«
t
0
(ğœ†s)kâˆ’1
(k âˆ’1)!ğœ†eâˆ’ğœ†s ds = âˆ’(ğœ†t)k
k! eâˆ’ğœ†t + â„™(Nt â‰¥k).
Therefore, for k â‰¥1
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t.
Since 0! = 1, in general we can express the probability mass function of a Poisson process
with intensity ğœ†> 0 as
â„™(Nt = k) = (ğœ†t)k
k! eâˆ’ğœ†t,
k = 0, 1, 2, . . .
â—½
5. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0. If ğœ1, ğœ2, . . . is a sequence of inter-arrival times, show that the random variables
are mutually independent and each follows an exponential distribution with parameter ğœ†.
Solution: We prove this result via mathematical induction.
Since
â„™(ğœ1 > t) = â„™(Nt = 0) = eâˆ’ğœ†t
the density of ğœ1 is
fğœ1(t) = d
dtâ„™(ğœ1 < t) = d
dt(1 âˆ’eâˆ’ğœ†t) = ğœ†eâˆ’ğœ†t
and hence ğœ1 âˆ¼Exp(ğœ†).
By conditioning on ğœ1 and since ğœ1 < ğœ2,
â„™(ğœ2 > t|ğœ1 = t1) = â„™(zero arrival in (t1, t1 + t]|ğœ1 = t1).
Since the event {ğœ1 = t1} is only related to arrivals during the time interval [0, t1] and
the event {zero arrival in (t1, t1 + t]} relates to arrivals during the time interval (t1, t1 + t],
these two events are independent. Thus, we have
â„™(ğœ2 > t|ğœ1 = t1) = â„™(zero arrival in (t1, t1 + t]) = â„™(ğœ2 > t) = eâˆ’ğœ†t
with ğœ2 âŸ‚âŸ‚ğœ1 and ğœ2 âˆ¼Exp(ğœ†).
We assume that for n > 1, ğœn âŸ‚âŸ‚ğœi and each ğœi âˆ¼Exp(ğœ†), i = 1, 2, . . . , n âˆ’1 such that
â„™(ğœn > t|ğœ1 = t1, ğœ2 = t2, . . . , ğœnâˆ’1 = tnâˆ’1) = â„™(zero arrival in (Tnâˆ’1, Tnâˆ’1 + t])
= eâˆ’ğœ†t
where Tnâˆ’1 =
nâˆ’1
âˆ‘
i=1
ti.

5.2.1
Properties of Poisson Process
259
Conditional on ğœ1 = t1, ğœ2 = t2, . . . , ğœn = tn, such that ğœ1 < ğœ2 < . . . < ğœn < ğœn+1 we have
â„™(ğœn+1 > t|ğœ1 = t1, ğœ2 = t2, . . . , ğœn = tn)
= â„™(zero arrival in (Tn, Tn + t]|ğœ1 = t1, ğœ2 = t2, . . . , ğœn = tn)
where Tn =
nâˆ‘
i=1
ti. Since the events {zero arrival in (Tn, Tn + t]} and {ğœi = ti}, i = 1,
2, . . . , n are mutually independent, then
â„™(ğœn+1 > t|ğœ1 = t1, ğœ2 = t2, . . . , ğœn = tn) = â„™(zero arrival in (Tn, Tn + t])
= â„™(ğœn+1 > t)
= eâˆ’ğœ†t
which implies ğœn+1 âŸ‚âŸ‚ğœi, i = 1, 2, . . . , n and ğœn+1 âˆ¼Exp(ğœ†).
Thus, from mathematical induction, the claim is true for all n â‰¥1.
â—½
6. Stationary and Independent Increments. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶
t â‰¥0} be a Poisson process with intensity ğœ†> 0. Suppose 0 = t0 < t1 < . . . < tn, using
the property of the Poisson process
â„™(Nti+h = ğ‘£|Nti = ğ‘¤) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
ğ‘£= ğ‘¤
ğœ†h + o(h)
ğ‘£= ğ‘¤+ 1
o(h)
ğ‘£> ğ‘¤+ 1
show that the increments
Nt1 âˆ’Nt0, Nt2 âˆ’Nt1, . . . , Ntn âˆ’Ntnâˆ’1
are stationary and independent with the distribution of Nti âˆ’Ntiâˆ’1 the same as the distri-
bution of Ntiâˆ’tiâˆ’1.
Solution: To show that {Nt âˆ¶t â‰¥0} is stationary we note that for 0 â‰¤tiâˆ’1 < ti, m â‰¥0
â„™(Nti âˆ’Ntiâˆ’1 = m) =
âˆ‘
u
â„™(Nti = u + m, Ntiâˆ’1 = u)
=
âˆ‘
u
â„™(Nti = u + m|Ntiâˆ’1 = u)â„™(Ntiâˆ’1 = u)
=
âˆ‘
u
pu,u+m(tiâˆ’1, ti)â„™(Ntiâˆ’1 = u)
where pu,u+m(tiâˆ’1, ti) = â„™(Nti = u + m|Ntiâˆ’1 = u).
From the property of the Poisson process
â„™(Nti+h = ğ‘£|Nti = ğ‘¤) =
â§
âª
â¨
âªâ©
1 âˆ’ğœ†h + o(h)
ğ‘£= ğ‘¤
ğœ†h + o(h)
ğ‘£= ğ‘¤+ 1
o(h)
ğ‘£> ğ‘¤+ 1

260
5.2.1
Properties of Poisson Process
we can write
â„™(Nti+h = ğ‘£|Ntiâˆ’1 = u) = â„™(Nti+h = ğ‘£|Nti = ğ‘£âˆ’1)â„™(Nti = ğ‘£âˆ’1|Ntiâˆ’1 = u)
+ â„™(Nti+h = ğ‘£|Nti = ğ‘£)â„™(Nti = ğ‘£|Ntiâˆ’1 = u)
+
âˆ‘
ğ‘¤<ğ‘£âˆ’1
â„™(Nti+h = ğ‘£|Nti = ğ‘¤)â„™(Nti = ğ‘¤|Ntiâˆ’1 = u)
= (ğœ†h + o(h))â„™(Nti = ğ‘£âˆ’1|Ntiâˆ’1 = u)
+ (1 âˆ’ğœ†h + o(h))â„™(Nti = ğ‘£|Ntiâˆ’1 = u) + o(h)
or
pâ€²
0,0(tiâˆ’1, ti) = âˆ’ğœ†p0,0(tiâˆ’1, ti)
pâ€²
u,ğ‘£(tiâˆ’1, ti) = âˆ’ğœ†pu,ğ‘£(tiâˆ’1, ti) + ğœ†pu,ğ‘£âˆ’1(tiâˆ’1, ti),
ğ‘£= u + 1, u + 2, . . .
where
pâ€²
u,ğ‘£(tiâˆ’1, ti) = lim
hâ†’0
â„™(Nti+h = ğ‘£|Ntiâˆ’1 = u) âˆ’â„™(Nti = ğ‘£|Ntiâˆ’1 = u)
h
.
Using the same steps as discussed in Problem 5.2.1.2 (page 253), we can deduce that
pu,ğ‘£(tiâˆ’1, ti) = ğœ†ğ‘£âˆ’u(ti âˆ’tiâˆ’1)ğ‘£âˆ’ueâˆ’ğœ†(tiâˆ’tiâˆ’1)
(ğ‘£âˆ’u)!
,
ğ‘£= u + 1, u + 2, . . .
By substituting the above result into â„™(Nti âˆ’Ntiâˆ’1 = m),
â„™(Nti âˆ’Ntiâˆ’1 = m) =
âˆ‘
u
pu,u+m(tiâˆ’1, ti)â„™(Ntiâˆ’1 = u)
= ğœ†m(ti âˆ’tiâˆ’1)meâˆ’ğœ†(tiâˆ’tiâˆ’1)
m!
âˆ‘
u
â„™(Ntiâˆ’1 = u)
= ğœ†m(ti âˆ’tiâˆ’1)meâˆ’ğœ†(tiâˆ’tiâˆ’1)
m!
which is a probability mass function for the Poisson (ğœ†(ti âˆ’tiâˆ’1)) distribution. Thus, for
all i = 1, 2, . . . , n, Nti âˆ’Ntiâˆ’1 is a stationary process and has the same distribution as
Poisson(ğœ†(ti âˆ’tiâˆ’1)).
Finally, to show {Nt âˆ¶t â‰¥0} has independent increments we note for mi â‰¥0, mj â‰¥0,
i < j, i, j = 1, 2, . . . , n and from the stationary property of the Poisson process,
â„™(Ntj âˆ’Ntjâˆ’1 = mj|Nti âˆ’Ntiâˆ’1 = mi) =
â„™(Ntj âˆ’Ntjâˆ’1 = mj, Nti âˆ’Ntiâˆ’1 = mi)
â„™(Nti âˆ’Ntiâˆ’1 = mi)
=
â„™(Ntjâˆ’tjâˆ’1 = mj, Ntiâˆ’tiâˆ’1 = mi)
â„™(Ntiâˆ’tiâˆ’1 = mi)
=
â„™(arrivals in (tjâˆ’1, tj] âˆ©arrivals in (tiâˆ’1, ti])
â„™(arrivals in (tiâˆ’1, ti])
.

5.2.1
Properties of Poisson Process
261
Because i < j and since there are no overlapping events between the time intervals (tiâˆ’1, ti]
and (tjâˆ’1, tj], thus for i < j, i, j = 1, 2, . . . , n
â„™(Ntj âˆ’Ntjâˆ’1 = mj|Nti âˆ’Ntiâˆ’1 = mi) = â„™(arrivals in (tjâˆ’1, tj]) = â„™(Ntjâˆ’tjâˆ’1 = mj).
N.B. In general, by denoting â„±tkâˆ’1 as the ğœ-algebra of information and observing the Pois-
son process Nt for 0 â‰¤t â‰¤tkâˆ’1, we can deduce that Ntk âˆ’Ntkâˆ’1 is independent of â„±tkâˆ’1.
â—½
7. Superposition. Let N(1)
t , N(2)
t , . . . , N(n)
t
be n independent Poisson processes with intensities
ğœ†(1), ğœ†(2), . . . , ğœ†(n), respectively, where the sequence of processes is defined on the proba-
bility space (Î©, â„±, â„™) with respect to the filtration â„±t. Show that N(1)
t
+ N(2)
t
+ . . . + N(n)
t
is a Poisson process with intensity ğœ†(1) + ğœ†(2) + . . . + ğœ†(n).
Solution: For N(i)
t
âˆ¼Poisson (ğœ†(i)t), i = 1, 2, . . . , n the moment generating function for
a Poisson process is
MN(i)
t (t, u) = ğ”¼
(
euN(i)
t
)
= eâˆ’ğœ†(i)t
âˆ
âˆ‘
x=0
eux(ğœ†(i)t)x
x!
= eâˆ’ğœ†(i)t
âˆ
âˆ‘
x=0
(ğœ†(i)teu)x
x!
= eğœ†(i)t(euâˆ’1)
where u âˆˆâ„.
By setting Nt = N(1)
t
+ N(2)
t
+ . . . + N(n)
t
such that N(i)
t
âŸ‚âŸ‚N(j)
t , i â‰ j, i, j = 1, 2, . . . , n the
moment generating function for Nt is
MNt(t, u) = ğ”¼(euNt)
= ğ”¼
(
euN(1)
t
+uN(2)
t
+ ... +uN(n)
t
)
= ğ”¼
(
euN(1)
t
)
â‹…ğ”¼
(
euN(2)
t
)
Â· Â· Â· ğ”¼
(
euN(n)
t
)
= eğœ†(1)t(euâˆ’1) â‹…eğœ†(2)t(euâˆ’1) Â· Â· Â· eğœ†(n)t(euâˆ’1)
= e(ğœ†(1)+ğœ†(2)+ ... +ğœ†(n))t(euâˆ’1).
Therefore,
N(1)
t
+ N(2)
t
+ . . . + N(n)
t
âˆ¼Poisson ((ğœ†(1) + ğœ†(2) + . . . + ğœ†(n))t) .
â—½
8. Markov Property. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on
the probability space (Î©, â„±, â„™) with respect to the filtration â„±t. Show that if f is a contin-
uous function then there exists another continuous function g such that
ğ”¼
[
f(Nt)|||â„±u
]
= g(Nu)
for 0 â‰¤u â‰¤t.
Solution: For 0 â‰¤u â‰¤t we can write
ğ”¼
[
f(Nt)|||â„±u
]
= ğ”¼
[
f(Nt âˆ’Nu + Nu)|||â„±u
]
.

262
5.2.1
Properties of Poisson Process
Since Nt âˆ’Nu âŸ‚âŸ‚â„±u and Nu is â„±u measurable, by setting Nu = x where x is a constant
value,
ğ”¼
[
f(Nt âˆ’Nu + Nu)|||â„±u
]
= ğ”¼[ f(Nt âˆ’Nu + x)] .
Because Nt âˆ’Nu âˆ¼Poisson(ğœ†(t âˆ’u)), we can write ğ”¼[ f(Nt âˆ’Nu + x)] as
ğ”¼[ f(Nt âˆ’Nu + x)] =
âˆ
âˆ‘
k=0
f(k + x)eâˆ’ğœ†(tâˆ’u)(ğœ†(t âˆ’u))k
k!
.
By setting ğœ= t âˆ’u and y = k + x, we can rewrite ğ”¼[ f(Ntâˆ’Nu+x)]=ğ”¼[ f(Nt âˆ’Nu+Nu)]
as
ğ”¼[ f(Nt âˆ’Nu + Nu)] =
âˆ
âˆ‘
y=Nu
f(y)eâˆ’ğœ†ğœ(ğœ†ğœ)yâˆ’Nu
(y âˆ’Nu)!
=
âˆ
âˆ‘
y=0
f(y + Nu)eâˆ’ğœ†ğœ(ğœ†ğœ)y
y!
.
Since the only information from the filtration â„±u is Nu, then
ğ”¼
[
f(Nt)|||â„±u
]
= g(Nu)
where
g(Nu) =
âˆ
âˆ‘
y=0
f(y + Nu)eâˆ’ğœ†ğœ(ğœ†ğœ)y
y!
.
â—½
9. Compensated Poisson Process. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0
defined on the probability space (Î©, â„±, â„™) with respect to the filtration â„±t. By defining the
compensated Poisson process, Ì‚Nt as
Ì‚Nt = Nt âˆ’ğœ†t
show that Ì‚Nt is a martingale.
Solution: To show that Ì‚Nt is a martingale, for 0 â‰¤s < t we note the following.
(a) Since the increment Nt âˆ’Ns is independent of â„±s and has expected value ğœ†(t âˆ’s), we
can write
ğ”¼
(
Ì‚Nt||| â„±s
)
= ğ”¼
(
Ì‚Nt âˆ’Ì‚Ns + Ì‚Ns||| â„±s
)
= ğ”¼
(
Ì‚Nt âˆ’Ì‚Ns|||â„±s
)
+ ğ”¼
(
Ì‚Ns|||â„±s
)
= ğ”¼
(
Nt âˆ’Ns âˆ’ğœ†(t âˆ’s)|||â„±s
)
+ Ì‚Ns
= ğ”¼(Nt âˆ’Ns
) âˆ’ğœ†(t âˆ’s) + Ì‚Ns
= Ì‚Ns.

5.2.1
Properties of Poisson Process
263
(b) ğ”¼
(|||Ì‚Nt|||
)
= ğ”¼(||Nt âˆ’ğœ†t||
) â‰¤ğ”¼(Nt
) + ğœ†t < âˆsince Nt â‰¥0 and hence ğ”¼(||Nt||
) =
2ğœ†t < âˆ.
(c) The process Ì‚Nt = Nt âˆ’ğœ†t is clearly â„±t-adapted.
From (a)â€“(c) we have shown that Ì‚Nt = Nt âˆ’ğœ†t is a martingale.
â—½
10. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t. By defining the compensated Poisson process,
Ì‚Nt as
Ì‚Nt = Nt âˆ’ğœ†t
show that Ì‚N2
t âˆ’ğœ†t is a martingale.
Solution: To show that Ì‚N2
t âˆ’ğœ†t is a martingale, for 0 â‰¤s < t we note the following.
(a) Since the increment Nt âˆ’Ns âŸ‚âŸ‚â„±s and has expected value ğœ†(t âˆ’s), and because Nt âˆ’
Ns âŸ‚âŸ‚Ns, we can write
ğ”¼
(
Ì‚N2
t âˆ’ğœ†t|||â„±s
)
= ğ”¼
[
(Ì‚Nt âˆ’Ì‚Ns + Ì‚Ns)2||| â„±s
]
âˆ’ğœ†t
= ğ”¼
[
(Ì‚Nt âˆ’Ì‚Ns)2||| â„±s
]
+ 2ğ”¼
[
Ì‚Ns(Ì‚Nt âˆ’Ì‚Ns)||| â„±s
]
+ ğ”¼
(
Ì‚N2
s ||| â„±s
)
âˆ’ğœ†t
= ğ”¼
[((Nt âˆ’Ns) âˆ’ğœ†(t âˆ’s))2||| â„±s
]
+ 2ğ”¼(Ns âˆ’ğœ†s) ğ”¼[(Nt âˆ’Ns) âˆ’ğœ†(t âˆ’s)] + Ì‚N2
s âˆ’ğœ†t
= ğœ†(t âˆ’s) + 0 + Ì‚N2
s âˆ’ğœ†t
= Ì‚N2
s âˆ’ğœ†s.
(b) Since |||Ì‚N2
t âˆ’ğœ†t||| â‰¤(Nt âˆ’ğœ†t)2 + ğœ†t therefore
ğ”¼
(|||Ì‚N2
t âˆ’ğœ†t|||
)
â‰¤ğ”¼
[(Nt âˆ’ğœ†t)2]
+ ğœ†t = 2ğœ†t < âˆ
as ğ”¼[(Nt âˆ’ğœ†t)2] = Var (Nt
) = ğœ†t.
(c) The process Ì‚N2
t âˆ’ğœ†= (Nt âˆ’ğœ†t)2 âˆ’ğœ†t is clearly â„±t-adapted.
From (a)â€“(c) we have shown that Ì‚N2
t âˆ’ğœ†t is a martingale.
â—½
11. Exponential Martingale Process. Let Nt be a Poisson process with intensity ğœ†> 0 defined
on the probability space (Î©, â„±, â„™) with respect to the filtration â„±t. Show that for u âˆˆâ„,
Xt = euNtâˆ’ğœ†t(euâˆ’1)
is a martingale.
Solution: Given that Nt âˆ¼Poisson(ğœ†t) then for u âˆˆâ„,
ğ”¼(euNt) = eâˆ’ğœ†t
âˆ
âˆ‘
x=0
eux(ğœ†t)x
x!
= eâˆ’ğœ†t
âˆ
âˆ‘
x=0
(ğœ†teu)x
x!
= eğœ†t(euâˆ’1)

264
5.2.1
Properties of Poisson Process
and hence for 0 â‰¤s < t and because Nt âˆ’Ns âˆ¼Poisson(ğœ†(t âˆ’s)),
ğ”¼(eu(Ntâˆ’Ns)) = eğœ†(tâˆ’s)(euâˆ’1).
To show that Xt = euNtâˆ’ğœ†t(euâˆ’1) is a martingale, for 0 â‰¤s < t we have the following:
(a) Since Nt âˆ’Ns âŸ‚âŸ‚â„±s and has stationary increment, therefore eNtâˆ’Ns âŸ‚âŸ‚â„±s and eNtâˆ’Ns âŸ‚âŸ‚
eNs. Thus, we can write
ğ”¼(Xt|â„±s
) = ğ”¼
(
euNtâˆ’ğœ†t(euâˆ’1)||| â„±s
)
= eâˆ’ğœ†t(euâˆ’1)ğ”¼
(
euNt||| â„±s
)
= eâˆ’ğœ†t(euâˆ’1)ğ”¼
(
euNtâˆ’uNs+uNs||| â„±s
)
= eâˆ’ğœ†t(euâˆ’1)ğ”¼
(
euNs||| â„±s
)
ğ”¼
(
eu(Ntâˆ’Ns)||| â„±s
)
= eâˆ’ğœ†t(euâˆ’1) â‹…euNs â‹…eğœ†(tâˆ’s)(euâˆ’1)
= euNsâˆ’ğœ†s(euâˆ’1)
= Xs.
(b) Since |Xt| = |||euNtâˆ’ğœ†t(euâˆ’1)||| = euNtâˆ’ğœ†t(euâˆ’1), we can write
ğ”¼(|Xt|) = ğ”¼(euNtâˆ’ğœ†t(euâˆ’1)) = eâˆ’ğœ†t(euâˆ’1)ğ”¼(euNt) = eâˆ’ğœ†t(euâˆ’1) â‹…eğœ†t(euâˆ’1) = 1 < âˆ.
(c) The process Xt = euNtâˆ’ğœ†t(euâˆ’1) is clearly â„±t-adapted.
From (a)â€“(c) we have shown that Xt = euNtâˆ’ğœ†t(euâˆ’1) is a martingale.
â—½
12. Compound Poisson Process. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†>
0 defined on the probability space (Î©, â„±, â„™) with respect to the filtration â„±t, and let
X1, X2, . . . be a sequence of independent and identically distributed random variables with
common mean ğ”¼(Xi) = ğ”¼(X) = ğœ‡and variance Var (Xi) = Var (X) = ğœ2. Let X1, X2, . . . be
independent of Nt. By defining the compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
show that the moment generating function for Mt is
ğœ‘Mt(t, u) = eğœ†t(ğœ‘X(u)âˆ’1),
u âˆˆâ„
where ğœ‘X(u) = ğ”¼(euX).
Further, show that ğ”¼(Mt) = ğœ‡ğœ†t and Var (Mt) = (ğœ‡2 + ğœ2)ğœ†t.
Solution: By definition, for u âˆˆâ„
ğœ‘Mt(t, u) = ğ”¼(euMt)
= ğ”¼
(
eu âˆ‘Nt
i=1 Xi
)

5.2.1
Properties of Poisson Process
265
= ğ”¼
[
ğ”¼
(
eu âˆ‘Nt
i=1 Xi||||
Nt
)]
=
âˆ
âˆ‘
n=0
ğ”¼
(
eu âˆ‘n
i=1 Xi||| Nt = n
)
â„™(Nt = n).
Since X1, X2, . . . , Xn are independent and identically distributed, we can therefore write
ğœ‘Mt(t, u) =
âˆ
âˆ‘
n=0
ğ”¼(euX)nâ„™(Nt = n)
=
âˆ
âˆ‘
n=0
(ğœ‘X(u))n eâˆ’ğœ†t(ğœ†t)n
n!
= eâˆ’ğœ†t
âˆ
âˆ‘
n=0
(ğœ†tğœ‘X(t))n
n!
= eğœ†t(ğœ‘X(u)âˆ’1).
By taking first and second partial derivatives of ğœ‘Mt(t, u) with respect to u, we have
ğœ•ğœ‘Mt(t, u)
ğœ•u
= ğœ†tğœ‘â€²
X(u)eğœ†t(ğœ‘X(u)âˆ’1)
and
ğœ•2ğœ‘Mt(t, u)
ğœ•u2
= {ğœ†tğœ‘â€²â€²
X(u) + (ğœ†t)2(ğœ‘â€²
X(u))2} eğœ†t(ğœ‘X(u)âˆ’1).
Since ğœ‘X(0) = 1, therefore
ğ”¼(Mt) =
ğœ•ğœ‘Mt(t, 0)
ğœ•u
= ğœ†tğœ‘â€²
X(0) = ğœ†tğ”¼(X) = ğœ‡ğœ†t
and
ğ”¼(M2
t
) =
ğœ•2ğœ‘Mt(t, 0)
ğœ•u2
= ğœ†tğœ‘â€²â€²
X(0) + (ğœ†t)2(ğœ‘â€²
X(0))2 = ğœ†tğ”¼(X2) + (ğœ†t)2ğ”¼(X)2.
Thus, the variance of Y is
Var (Mt) = ğ”¼(M2
t
) âˆ’ğ”¼(Mt
)2 = ğœ†tğ”¼(X2) = ğœ†t [Var (X) + ğ”¼(X)2] = (ğœ‡2 + ğœ2)ğœ†t.
â—½
13. Martingale Properties of Compound Poisson Process. Let {Nt âˆ¶t â‰¥0} be a Poisson
process with intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with respect
to the filtration â„±t, and let X1, X2, . . . be a sequence of independent and identically
distributed random variables with common mean ğ”¼(Xi) = ğ”¼(X) = ğœ‡and variance
Var (Xi) = Var (X) = ğœ2. Let X1, X2, . . . be independent of Nt. By defining the compound
Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0

266
5.2.1
Properties of Poisson Process
and assuming ğ”¼(|X|) < âˆ, show that for 0 â‰¤s < t
ğ”¼(Mt|â„±s)
â§
âª
â¨
âªâ©
= Ms
if ğœ‡= 0
â‰¥Ms
if ğœ‡> 0
â‰¤Ms
if ğœ‡< 0.
Solution: To show that Mt can be a martingale, submartingale or supermartingale, for
0 â‰¤s < t we have the following.
(a) ğ”¼(Mt|â„±s) = ğ”¼(Mt âˆ’Ms + Ms|â„±s) = ğ”¼(Mt âˆ’Ms|â„±s) + ğ”¼(Ms|â„±s) = ğœ‡ğœ†(t âˆ’s) + Ms
since the increment Mt âˆ’Ms is independent of â„±s and has mean ğœ‡ğœ†(t âˆ’s).
(b) ğ”¼|Mt|) = ğ”¼
(|||
âˆ‘Nt
i=1 Xi|||
)
â‰¤ğ”¼
(âˆ‘Nt
i=1 ||Xi||
)
= ğœ†tğ”¼(|X|) < âˆsince ğ”¼(|X|) < âˆ.
(c) The process Mt = âˆ‘Nt
i=1 Xi is clearly â„±t-adapted.
We can therefore deduce that Mt is a martingale, submartingale or supermartingale by
setting ğœ‡= 0, ğœ‡> 0 or ğœ‡< 0, respectively.
â—½
14. Compensated Compound Poisson Process. Let {Nt âˆ¶t â‰¥0} be a Poisson process with
intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with respect to the filtration
â„±t. Let X1, X2, . . . be a sequence of independent and identically distributed random vari-
ables with common mean ğ”¼(Xi) = ğ”¼(X) = ğœ‡and variance Var (Xi) = Var (X) = ğœ2. Let
X1, X2, . . . be independent of Nt. By defining the compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
and assuming ğ”¼(|X|) < âˆ, show that the compensated compound Poisson process
Ì‚Mt = Mt âˆ’ğœ‡ğœ†t
is a martingale.
Solution: To show that Ì‚Mt is a martingale, for 0 â‰¤s < t we have the following.
(a) Given the increment Mt âˆ’Ms is independent of â„±s and has mean ğœ‡ğœ†(t âˆ’s),
ğ”¼
(
Ì‚Mt||| â„±s
)
= ğ”¼
(
Ì‚Mt âˆ’Ì‚Ms + Ì‚Ms||| â„±s
)
= ğ”¼
(
Mt âˆ’Ms âˆ’ğœ‡ğœ†(t âˆ’s) + Ms âˆ’ğœ‡ğœ†s|||â„±s
)
= ğ”¼
(
Mt âˆ’Ms|||â„±s
)
âˆ’ğœ‡ğœ†(t âˆ’s) + Ms âˆ’ğœ‡ğœ†s
= ğœ‡ğœ†(t âˆ’s) âˆ’ğœ‡ğœ†(t âˆ’s) + Ms âˆ’ğœ‡ğœ†s
= Ms âˆ’ğœ‡ğœ†s.
(b) ğ”¼
(||| Ì‚Mt|||
)
= ğ”¼
(|||
âˆ‘Nt
i=1 Xi âˆ’ğœ‡ğœ†t|||
)
â‰¤ğ”¼
(âˆ‘Nt
i=1 ||Xi||
)
+ ğœ‡ğœ†t = ğœ†tğ”¼(|X|) + ğœ‡ğœ†t < âˆ
since ğ”¼(|X|) < âˆ.

5.2.1
Properties of Poisson Process
267
(c) The process Ì‚Mt = âˆ‘Nt
i=1 Xi âˆ’ğœ‡ğœ†t is clearly â„±t-adapted.
From the results of (a)â€“(c) we have shown that Ì‚Mt = âˆ‘Nt
i=1 Xi âˆ’ğœ‡ğœ†t is a martingale.
â—½
15. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t. Let X1, X2, . . . be a sequence of independent
and identically distributed random variables with common mean ğ”¼(Xi
) = ğ”¼(X) = ğœ‡and
variance Var (Xi
) = Var (X) = ğœ2. Let X1, X2, . . . be independent of Nt. By defining the
compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
show that for 0 = t0 < t1 < t2 < . . . < tn the increments
Mt1 âˆ’Mt0, Mt2 âˆ’Mt1, . . . , Mtn âˆ’Mtnâˆ’1
are independent, stationary and the distribution of Mti âˆ’Mtiâˆ’1 is the same as the distribu-
tion of Mtiâˆ’tiâˆ’1.
Solution: We first show that the increment Mtk âˆ’Mtkâˆ’1, k = 1, 2, . . . , n is stationary and
has the same distribution as Mtkâˆ’tkâˆ’1.
Using the technique of moment generating function, for u âˆˆâ„
ğœ‘Mtkâˆ’Mtkâˆ’1 (u) = ğ”¼
(
eu(Mtkâˆ’Mtkâˆ’1))
= ğ”¼
(
e
u
(âˆ‘Ntk
i=1 Xiâˆ’âˆ‘Ntkâˆ’1
i=1
Xi
))
= ğ”¼
(
e
u âˆ‘Ntk
Ntkâˆ’1 +1 Xi
)
= ğœ‘Mtkâˆ’tkâˆ’1 (u).
Therefore, Mtk âˆ’Mtkâˆ’1, k = 1, 2, . . . , n is stationary and has the same distribution as
Mtkâˆ’tkâˆ’1.
Finally, to show that Mti âˆ’Mtiâˆ’1 is independent of Mtj âˆ’Mtjâˆ’1, i, j = 1, 2, . . . , n and i â‰ j,
using the joint moment generating function, for u, ğ‘£âˆˆâ„
ğœ‘(Mtiâˆ’Mtiâˆ’1)(Mtjâˆ’Mtjâˆ’1)(u, ğ‘£) = ğœ‘(Mtiâˆ’tiâˆ’1)(Mtjâˆ’tjâˆ’1)(u, ğ‘£)
= ğ”¼
(
e
u(Mtiâˆ’tiâˆ’1)+ğ‘£(Mtjâˆ’tjâˆ’1))
= ğ”¼
â›
âœ
âœâ
e
u
(âˆ‘Nti
k=Ntiâˆ’1 +1 Xk
)
+ğ‘£
(âˆ‘Ntj
k=Ntjâˆ’1 +1 Xk
)â
âŸ
âŸâ 
= ğ”¼
(
e
u
(âˆ‘Nti
k=Ntiâˆ’1 +1 Xk
))
ğ”¼
â›
âœ
âœâ
e
ğ‘£
(âˆ‘Ntj
k=Ntjâˆ’1 +1 Xk
)â
âŸ
âŸâ 

268
5.2.1
Properties of Poisson Process
= ğœ‘Mtiâˆ’tiâˆ’1(u)ğœ‘Mtjâˆ’tjâˆ’1(ğ‘£)
since X1, X2, . . . is a sequence of independent and identically distributed random variables
and also the intervals [Ntiâˆ’1 + 1, Nti] and [Ntjâˆ’1 + 1, Ntj] have no overlapping events.
Thus, Mti âˆ’Mtiâˆ’1 âŸ‚âŸ‚Mtj âˆ’Mtjâˆ’1, for all i, j = 1, 2, . . . , n, i â‰ j.
N.B. Since the compound Poisson process Mt has increments which are independent and
stationary, therefore from Problem 5.2.1.8 (page 261) we can deduce that Mt is Markov.
That is, if f is a continuous function then there exists another continuous function g
such that
ğ”¼[ f(Mt)|â„±u
] = g(Mu)
for 0 â‰¤u â‰¤t.
â—½
16. Decomposition of a Compound Poisson Process (Finite Jump Size). Let Nt be a Poisson
process with intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™) with respect to
the filtration â„±t. Let X1, X2, . . . be a sequence of independent and identically distributed
random variables, and let X1, X2, . . . be independent of Nt such that
â„™(X = xk) = p(xk)
and
K
âˆ‘
k=1
â„™(X = xk) = 1
where X
d= Xi, i = 1, 2, . . . and x1, x2, . . . , xK is a finite set of non-zero numbers. By defin-
ing the compound Poisson process as
Mt =
Nt
âˆ‘
i=1
Xi
show that Mt and Nt can be written as
Mt =
K
âˆ‘
k=1
xkN(k)
t
and
Nt =
K
âˆ‘
k=1
N(k)
t
where N(k)
t
âˆ¼Poisson (ğœ†tp(xk)), k = 1, 2, . . . , K is a sequence of independent and identi-
cal Poisson processes.
Solution: From Problem 5.2.1.12 (page 264), the moment generating function for Mt is
ğœ‘Mt(t, u) = eğœ†t(ğœ‘X(u)âˆ’1),
u âˆˆâ„
where ğœ‘X(u) = ğ”¼(euX). Given â„™(X = xk) = p(xk), k = 1, 2, . . . , K we have
ğœ‘X(u) = ğ”¼
(
euX)
=
K
âˆ‘
k=1
euxkâ„™(X = xk)
=
K
âˆ‘
k=1
euxkp(xk).

5.2.1
Properties of Poisson Process
269
Substituting ğœ‘X(u) into the moment generating function for Mt,
ğœ‘Mt(t, u) = e
ğœ†t
(âˆ‘K
k=1 euxk p(xk)âˆ’1
)
.
Because âˆ‘K
k=1 p(xk) = 1,
ğœ‘Mt(t, u) = e
ğœ†t
(âˆ‘K
k=1 euxk p(xk)âˆ’âˆ‘K
k=1 p(xk)
)
= eğœ†t âˆ‘K
k=1(euxk âˆ’1)p(xk)
=
K
âˆ
k=1
eğœ†tp(xk)(euxk âˆ’1)
=
K
âˆ
k=1
ğ”¼
(
euxkN(k)
t
)
which is a product of K independent moment generating functions of the random variable
xkN(k)
t
where N(k)
t
âˆ¼Poisson (ğœ†tp(xk)). Thus,
ğœ‘Mt(t, u) =
K
âˆ
k=1
ğ”¼
(
euxkN(k)
t
)
= ğ”¼
(
eu âˆ‘K
k=1 xkN(k)
t
)
which implies Mt =
Kâˆ‘
k=1
xkN(k)
t .
Given that N(k)
t
âˆ¼Poisson (ğœ†tp(xk)), k = 1, 2, . . . , K is a sequence of independent and
identically distributed Poisson processes,
Kâˆ‘
k=1
N(k)
t
âˆ¼Poisson
(
Kâˆ‘
k=1
ğœ†tp(xk)
)
or
Kâˆ‘
k=1
N(k)
t
âˆ¼
Poisson(ğœ†t) and hence Nt =
Kâˆ‘
k=1
N(k)
t .
â—½
17. Let {X1, X2, . . . ,} be a series of independent and identically distributed random variables
with moment generating function
MX(ğœ‰) = ğ”¼(eğœ‰X) ,
ğœ‰âˆˆâ„
where X
d= Xi, i = 1, 2, . . . and let {t1, t2, . . .} be its corresponding sequence of indepen-
dent and identically distributed random jump times of a Poisson process {Nt âˆ¶t â‰¥0} with
intensity parameter ğœ†> 0 where ti âˆ¼Exp(ğœ†), i = 1, 2, . . . Let X1, X2, . . . be independent
of Nt.
Show that for n â‰¥1, ğœ…> 0 the process {Yt âˆ¶t â‰¥0} with initial condition Y0 = 0 given as
Yt =
n
âˆ‘
i=1
eâˆ’ğœ…(tâˆ’ti)Xi
has moment generating function MY(ğœ‰, t) = ğ”¼(eğœ‰Yt) given by
MY(ğœ‰, t) =
n
âˆ
i=1
{
âˆ«
t
0
MXi (ğœ‰eâˆ’ğœ…s) ğœ†eâˆ’ğœ†(tâˆ’s) ds
}

270
5.2.1
Properties of Poisson Process
and
ğ”¼(Yt) = nğœ‡
(
ğœ†
ğœ†âˆ’ğœ…
) (eâˆ’ğœ…t âˆ’eâˆ’ğœ†t) (1 âˆ’eâˆ’ğœ†t)nâˆ’1
Var (Yt) = n(ğœ‡2 + ğœ2)
(
ğœ†
ğœ†âˆ’2ğœ…
) (eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t) (1 âˆ’eâˆ’ğœ†t)nâˆ’1
+ n(n âˆ’1)ğœ‡2(
ğœ†
ğœ†âˆ’2ğœ…
)2(eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t)2(1 âˆ’eâˆ’ğœ†t)nâˆ’2
âˆ’n2ğœ‡2(
ğœ†
ğœ†âˆ’ğœ…
)2(eâˆ’ğœ…t âˆ’eâˆ’ğœ†t)2(1 âˆ’eâˆ’ğœ†t)2(nâˆ’1)
where ğ”¼(Xi) = ğœ‡and Var (Xi) = ğœ2 for i = 1, 2, . . .
Solution: By definition,
MY(ğœ‰, t) = ğ”¼(eğœ‰Yt) = ğ”¼
(
eğœ‰âˆ‘n
i=1 eâˆ’ğœ…(tâˆ’ti)Xi
)
=
n
âˆ
i=1
ğ”¼
(
eğœ‰eâˆ’ğœ…(tâˆ’ti)Xi
)
=
n
âˆ
i=1
MXi(ğœ‰eâˆ’ğœ…(tâˆ’ti)).
Using the tower property,
MXi(ğœ‰eâˆ’ğœ…(tâˆ’ti)) = ğ”¼
(
eğœ‰eâˆ’ğœ…(tâˆ’ti)Xi
)
= ğ”¼
[
ğ”¼
(
eğœ‰eâˆ’ğœ…(tâˆ’ti)Xi||| ti = s
)]
= âˆ«
t
0
ğ”¼
(
eğœ‰eâˆ’ğœ…(tâˆ’s)Xi
)
ğœ†eâˆ’ğœ†s ds
= âˆ«
t
0
MXi(ğœ‰eâˆ’ğœ…(tâˆ’s))ğœ†eâˆ’ğœ†s ds
= âˆ«
t
0
MXi(ğœ‰eâˆ’ğœ…s)ğœ†eâˆ’ğœ†(tâˆ’s) ds.
Therefore,
MY(ğœ‰, t) =
n
âˆ
i=1
{
âˆ«
t
0
MXi (ğœ‰eâˆ’ğœ…s) ğœ†eâˆ’ğœ†(tâˆ’s) ds
}
.
By setting ğœ‰= ğœ‰eâˆ’ğœ…s and MXi(ğœ‰) = âˆ«
t
0
MXi(ğœ‰eâˆ’ğœ…s)ğœ†eâˆ’ğœ†(tâˆ’s) ds, and differentiating MY(ğœ‰, t)
with respect to ğœ‰, we have
ğœ•MY(ğœ‰, t)
ğœ•ğœ‰
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
ğœ•MXi(ğœ‰)
ğœ•ğœ‰
n
âˆ
j=1
jâ‰ i
MXj(ğœ‰)
â«
âª
â¬
âªâ­
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
[
âˆ«
t
0
ğœ•MXi(ğœ‰)
ğœ•ğœ‰
â‹…ğœ•ğœ‰
ğœ•ğœ‰â‹…ğœ†eâˆ’ğœ†(tâˆ’s) ds
]
n
âˆ
j=1
jâ‰ i
MXj(ğœ‰)
â«
âª
â¬
âªâ­

5.2.1
Properties of Poisson Process
271
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
[
âˆ«
t
0
ğœ•MXi(ğœ‰)
ğœ•ğœ‰
ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds
]
n
âˆ
j=1
jâ‰ i
[
âˆ«
t
0
MXi(ğœ‰)ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­
and
ğœ•2MY(ğœ‰, t)
ğœ•ğœ‰2
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
ğœ•2MXi(ğœ‰)
ğœ•ğœ‰2
n
âˆ
j=1
jâ‰ i
MXj(ğœ‰)
â«
âª
â¬
âªâ­
+
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
â§
âª
â¨
âªâ©
ğœ•MXi(ğœ‰)
ğœ•ğœ‰
ğœ•MXj(ğœ‰)
ğœ•ğœ‰
n
âˆ
k=1
kâ‰ i,j
MXk(ğœ‰)
â«
âª
â¬
âªâ­
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
[
âˆ«
t
0
ğœ•2MXi(ğœ‰)
ğœ•ğœ‰
2
ğœ†eâˆ’ğœ†te(ğœ†âˆ’2ğœ…)s ds
]
n
âˆ
j=1
jâ‰ i
[
âˆ«
t
0
MXj(ğœ‰)ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­
+
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
â§
âª
â¨
âªâ©
[
ğœ•MXi(ğœ‰)
ğœ•ğœ‰
ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds
] â¡
â¢
â¢â£
ğœ•MXj(ğœ‰)
ğœ•ğœ‰
ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds
â¤
â¥
â¥â¦
Ã—
n
âˆ
k=1
kâ‰ i,j
[
âˆ«
t
0
MXi(ğœ‰)ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­
.
By substituting ğœ‰= 0 and taking note that MX(0) = 1, ğœ•MX(0)
ğœ•ğœ‰
= ğ”¼(X) = ğœ‡and
ğœ•2MX(0)
ğœ•ğœ‰
2
= ğ”¼(X2) = ğœ‡2 + ğœ2, we therefore have
ğ”¼(Yt) = ğœ•MY(0, t)
ğœ•ğœ‰
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
[
âˆ«
t
0
ğœ‡ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds
]
n
âˆ
j=1
jâ‰ i
[
âˆ«
t
0
ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­
= nğœ‡
(
ğœ†
ğœ†âˆ’ğœ…
) (
eâˆ’ğœ…t âˆ’eâˆ’ğœ†t) (
1 âˆ’eâˆ’ğœ†t)nâˆ’1
and
ğ”¼(Y2
t ) = ğœ•2MY(0, t)
ğœ•ğœ‰2
=
n
âˆ‘
i=1
â§
âª
â¨
âªâ©
[
âˆ«
t
0
(ğœ‡2 + ğœ2) ğœ†eâˆ’ğœ†te(ğœ†âˆ’2ğœ…)s ds
]
n
âˆ
j=1
jâ‰ i
[
âˆ«
t
0
ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­

272
5.2.1
Properties of Poisson Process
+
n
âˆ‘
i=1
n
âˆ‘
j=1
iâ‰ j
â§
âª
â¨
âªâ©
[ğœ‡ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds] [ğœ‡ğœ†eâˆ’ğœ†te(ğœ†âˆ’ğœ…)s ds]
n
âˆ
k=1
kâ‰ i,j
[
âˆ«
t
0
ğœ†eâˆ’ğœ†(tâˆ’s) ds
]â«
âª
â¬
âªâ­
= n(ğœ‡2 + ğœ2)
(
ğœ†
ğœ†âˆ’2ğœ…
) (eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t) (1 âˆ’eâˆ’ğœ†t)nâˆ’1
+ n(n âˆ’1)ğœ‡2(
ğœ†
ğœ†âˆ’2ğœ…
)2(eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t)2(1 âˆ’eâˆ’ğœ†t)nâˆ’2.
Therefore,
Var (Yt
) = ğ”¼(Y2
t
) âˆ’ğ”¼(Yt
)2
= n(ğœ‡2 + ğœ2)
(
ğœ†
ğœ†âˆ’2ğœ…
) (eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t) (1 âˆ’eâˆ’ğœ†t)nâˆ’1
+ n(n âˆ’1)ğœ‡2(
ğœ†
ğœ†âˆ’2ğœ…
)2(eâˆ’2ğœ…t âˆ’eâˆ’ğœ†t)2(1 âˆ’eâˆ’ğœ†t)nâˆ’2
âˆ’n2ğœ‡2(
ğœ†
ğœ†âˆ’ğœ…
)2(eâˆ’ğœ…t âˆ’eâˆ’ğœ†t)2(1 âˆ’eâˆ’ğœ†t)2(nâˆ’1).
â—½
18. Let {X1, X2, . . . , } be a series of independent and identically distributed random variables
with moment generating function
MX(ğœ‰) = ğ”¼(eğœ‰X) ,
ğœ‰âˆˆâ„
where X
d= Xi, i = 1, 2, . . . and let {t1, t2, . . .} be its corresponding sequence of random
jump times of a Poisson process {Nt âˆ¶t â‰¥0} with intensity parameter ğœ†> 0 where ti âˆ¼
Exp(ğœ†), i = 1, 2, . . . In addition, let the sequence X1, X2, . . . be independent of Nt. Show
that for ğœ…> 0 the process {Yt âˆ¶t â‰¥0} with initial condition Y0 = 0 given as
Yt =
Nt
âˆ‘
i=1
eâˆ’ğœ…(tâˆ’ti)Xi
has moment generating function MY(ğœ‰, t) = ğ”¼(eğœ‰Yt) given by
MY(ğœ‰, t) = exp
{
ğœ†âˆ«
t
0
(MX(ğœ‰eâˆ’ğœ…s) âˆ’1) ds
}
and
ğ”¼(Yt
) = ğœ†ğœ‡
ğœ…
(1 âˆ’eâˆ’ğœ…t)
Var (Yt
) = ğœ†
2ğœ…
(ğœ‡2 + ğœ2) (1 âˆ’eâˆ’2ğœ…t)
where ğ”¼(Xi
) = ğœ‡and Var (Xi
) = ğœ2 for i = 1, 2, . . .

5.2.1
Properties of Poisson Process
273
Solution: Given the mutual independence of the random variables Xi and Xj, i â‰ j the
conditional expectation of the process Yt given the first jump time, t1 = s is
ğ”¼
(
eğœ‰Yt||| t1 = s
)
= ğ”¼
(
exp
{
ğœ‰
Nt
âˆ‘
i=1
eâˆ’ğœ…(tâˆ’ti)Xi
}||||||
t1 = s
)
= ğ”¼
(
exp {ğœ‰eâˆ’ğœ…(tâˆ’s)X1
}||| t1 = s
)
ğ”¼
(
exp
{
ğœ‰
Nt
âˆ‘
i=2
eâˆ’ğœ…(tâˆ’ti)Xi
}||||||
t1 = s
)
.
From the properties of the Poisson process, the sum conditioned on t1 = s has the same
unconditional distribution as the original sum, starting with the first jump i = 1 until time
t âˆ’s, and we therefore have
ğ”¼
(
eğœ‰Yt||| t1 = s
)
= ğ”¼(exp {ğœ‰eâˆ’ğœ…(tâˆ’s)Xt
}) MY(ğœ‰, t âˆ’s)
= MX(ğœ‰eâˆ’ğœ…(tâˆ’s))MY(ğœ‰, t âˆ’s).
Since the first jump time is exponentially distributed, t1 âˆ¼Exp(ğœ†) and from the tower
property
MY(ğœ‰, t) = ğ”¼
[
ğ”¼
(
eğœ‰Yt||| t1 = s
)]
= âˆ«
t
0
ğ”¼(eğœ‰Yt|t1 = s) ğœ†eâˆ’ğœ†s ds
= âˆ«
t
0
MX(ğœ‰eâˆ’ğœ…(tâˆ’s))MY(ğœ‰, t âˆ’s)ğœ†eâˆ’ğœ†s ds
= âˆ«
t
0
MX(ğœ‰eâˆ’ğœ…s)MY(ğœ‰, s)ğœ†eâˆ’ğœ†(tâˆ’s) ds.
Differentiating the integral with respect to t, we have
ğœ•MY(ğœ‰, t)
ğœ•t
= MX(ğœ‰eâˆ’ğœ…t)MY(ğœ‰, t)ğœ†âˆ’ğœ†âˆ«
t
0
MX(ğœ‰eâˆ’ğœ…s)MY(ğœ‰, s)ğœ†eâˆ’ğœ†(tâˆ’s) ds
= ğœ†(MX(ğœ‰eâˆ’ğœ…t) âˆ’1)MY(ğœ‰, t)
and solving the first-order differential equation,
MY(ğœ‰, t) = exp
(
ğœ†âˆ«
t
0
{MX (ğœ‰eâˆ’ğœ…s) âˆ’1} ds
)
.
Setting ğœ‰= ğœ‰eâˆ’ğœ…s and differentiating MY(ğœ‰, t) with respect to ğœ‰, we have
ğœ•MY(ğœ‰, t)
ğœ•ğœ‰
= ğœ•
ğœ•ğœ‰
(
ğœ†âˆ«
t
0
{
MX
(
ğœ‰
)
âˆ’1
}
ds
)
MY(ğœ‰, t)
= ğœ†
{
âˆ«
t
0
eâˆ’ğœ…s ğœ•MX(ğœ‰)
ğœ•ğœ‰
ds
}
MY(ğœ‰, t)

274
5.2.1
Properties of Poisson Process
and differentiating the above equation with respect to ğœ‰,
ğœ•2MY(ğœ‰, t)
ğœ•ğœ‰2
= ğœ†ğœ•
ğœ•ğœ‰
{
âˆ«
t
0
eâˆ’ğœ…s ğœ•MX(ğœ‰)
ğœ•ğœ‰
ds
}
MY(ğœ‰, t) + ğœ†
{
âˆ«
t
0
eâˆ’ğœ…s ğœ•MX(ğœ‰)
ğœ•ğœ‰
ds
}
ğœ•MY(ğœ‰, t)
ğœ•ğœ‰
= ğœ†
{
âˆ«
t
0
eâˆ’2ğœ…s ğœ•2MX(ğœ‰)
ğœ•ğœ‰
2
ds
}
MY(ğœ‰, t) + ğœ†
{
âˆ«
t
0
eâˆ’ğœ…s ğœ•MX(ğœ‰)
ğœ•ğœ‰
ds
}
ğœ•MY(ğœ‰, t)
ğœ•ğœ‰
.
By substituting ğœ‰= 0, and since MY(0, t) = 1, we therefore have
ğ”¼(Yt
) = ğœ•MY(0, t)
ğœ•ğœ‰
= ğœ†ğœ‡
ğœ…
(1 âˆ’eâˆ’ğœ…t)
ğ”¼(Y2
t
) = ğœ•2MY(0, t)
ğœ•ğœ‰2
= ğœ†
2ğœ…
(ğœ‡2 + ğœ2) (1 âˆ’eâˆ’2ğœ…t) + ğ”¼(Yt)2
and hence
Var (Yt
) = ğœ†
2ğœ…(ğœ‡2 + ğœ2)(1 âˆ’eâˆ’2ğœ…t)
where ğ”¼(Xi
) = ğœ‡and Var (Xi
) = ğœ2 for i = 1, 2, . . .
â—½
19. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t, and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
show that its quadratic variation is
âŸ¨M, MâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Mti+1 âˆ’Mti
)2
=
Nt
âˆ‘
i=1
X2
i
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and deduce that dMt â‹…dMt =
X2
t dNt where Xt is the jump size if N jumps at t.
Solution: From the definition of quadratic variation
âŸ¨M, MâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
k=0
(
Mtk+1 âˆ’Mtk
)2
where tk = tkâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•, we can set
âŸ¨M, MâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
k=0
â›
âœ
âœâ
Ntk+1
âˆ‘
i=1
Xi âˆ’
Ntk
âˆ‘
i=1
Xi
â
âŸ
âŸâ 
2
= lim
nâ†’âˆ
Ntn
âˆ‘
i=Nt0
X2
i =
Nt
âˆ‘
i=1
X2
i
since Nt0 = N0 = 0.

5.2.1
Properties of Poisson Process
275
From the definition
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Mti+1 âˆ’Mti
)2
= âˆ«
t
0
(dMs
)2 =
Nt
âˆ‘
i=1
X2
i
and since
Nt
âˆ‘
i=1
X2
i = âˆ«
t
0
X2
uâˆ’dNu
where Xtâˆ’is the jump size before a jump event at time t, then by differentiating both sides
with respect to t we finally have dMt â‹…dMt = X2
t dNt where Xt is the jump size if N jumps
at t.
â—½
20. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
show that the cross-variation between Mt and Nt is
âŸ¨M, NâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Mti+1 âˆ’Mti
) (
Nti+1 âˆ’Nti
)
=
Nt
âˆ‘
i=1
Xi
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and deduce that dMt â‹…dNt =
XtdNt.
Solution: From the definition of cross-variation, for tk = ktâˆ•n, 0 = t0 < t1 < t2 < . . . <
tn = t, n âˆˆâ„•and since we can also write Nt = âˆ‘Nt
i=1 1,
âŸ¨M, NâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
k=0
(
Mtk+1 âˆ’Mtk
) (
Ntk+1 âˆ’Ntk
)
= lim
nâ†’âˆ
nâˆ’1
âˆ‘
k=0
â›
âœ
âœâ
Ntk+1
âˆ‘
i=1
Xi âˆ’
Ntk
âˆ‘
i=1
Xi
â
âŸ
âŸâ 
â›
âœ
âœâ
Ntk+1
âˆ‘
i=1
1 âˆ’
Ntk
âˆ‘
i=1
1
â
âŸ
âŸâ 
= lim
nâ†’âˆ
Ntn
âˆ‘
i=Nt0
Xi â‹…1
=
Nt
âˆ‘
i=1
Xi
since Nt0 = N0 = 0.
Given
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Mti+1 âˆ’Mti
) (
Nti+1 âˆ’Nti
)
= âˆ«
t
0
dMs â‹…dNs =
Nt
âˆ‘
i=1
Xi

276
5.2.1
Properties of Poisson Process
and because we can define
Nt
âˆ‘
i=1
Xi = âˆ«
t
0
Xsâˆ’dNs
where Xtâˆ’is the jump size before a jump event at time t, then by differentiating the integrals
with respect to t we have dMt â‹…dNt = XtdNt where Xt is the jump size if N jumps at t.
â—½
21. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t. Show that its quadratic variation is
âŸ¨N, NâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Nti+1 âˆ’Nti
)2
= Nt
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and deduce that dNt â‹…dNt = dNt.
Solution: Since Nt is a counting process, we can define the compound Poisson process
Mt as
Mt =
Nt
âˆ‘
i=1
Xi
where Xi is a sequence of independent and identically distributed random variables which
are also independent of Nt. Using the results in Problem 5.2.1.19 (page 274),
âŸ¨M, MâŸ©t =
Nt
âˆ‘
i=1
X2
i .
By setting Xi = 1 for all i = 1, 2, . . . , Nt, then Mt = Nt which implies
âŸ¨N, NâŸ©t =
Nt
âˆ‘
i=1
1 = Nt.
By definition,
âŸ¨N, NâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Nti+1 âˆ’Nti
)2
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and since
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Nti+1 âˆ’Nti
)2
= âˆ«
t
0
(dNs
)2 = Nt
then, by differentiating both sides with respect to t, we have dNt â‹…dNt = dNt.
â—½

5.2.1
Properties of Poisson Process
277
22. Let {Wt âˆ¶t â‰¥0} be a standard Wiener process and {Nt âˆ¶t â‰¥0} be a Poisson process with
intensity ğœ†> 0 defined on the probability space (Î©, â„±, â„™). Show that the cross-variation
between Wt and Nt is
âŸ¨W, NâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Nti+1 âˆ’Nti
)
= 0
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and deduce that dWt â‹…dNt = 0.
Solution: Since
||||||
nâˆ’1
âˆ‘
i=0
(Wti+1 âˆ’Wti)(Nti+1 âˆ’Nti)
||||||
â‰¤
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
|||
||||||
nâˆ’1
âˆ‘
i=0
(Nti+1 âˆ’Nti)
||||||
and because Wt is continuous, we have
lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| = 0.
Therefore, we conclude that
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Nti+1 âˆ’Nti
)||||||
â‰¤0
and hence
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Nti+1 âˆ’Nti
)
= 0.
Finally, because
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Nti+1 âˆ’Nti
)
= âˆ«
t
0
dWs â‹…dNs = 0
then, by differentiating both sides with respect to t, we can deduce that dWt â‹…dNt = 0.
â—½
23. Let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0

278
5.2.1
Properties of Poisson Process
show that the cross-variation between Wt and Mt is
âŸ¨W, MâŸ©t = lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Mti+1 âˆ’Mti
)
= 0
where ti = itâˆ•n, 0 = t0 < t1 < t2 < . . . < tn = t, n âˆˆâ„•and deduce that dWt â‹…dMt = 0.
Solution: Since we can write
||||||
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Mti+1 âˆ’Mti
)||||||
â‰¤
max
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
|||
||||||
nâˆ’1
âˆ‘
i=0
(Mti+1 âˆ’Mti)
||||||
and because Wt is continuous, we have
lim
nâ†’âˆmax
0â‰¤kâ‰¤nâˆ’1
|||Wtk+1 âˆ’Wtk
||| = 0.
Thus, we conclude that
||||||
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Mti+1 âˆ’Mti
)||||||
â‰¤0
or
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Mti+1 âˆ’Mti
)
= 0.
Finally, because
lim
nâ†’âˆ
nâˆ’1
âˆ‘
i=0
(
Wti+1 âˆ’Wti
) (
Mti+1 âˆ’Mti
)
= âˆ«
t
0
dWs â‹…dMs = 0
then, by differentiating both sides with respect to t, we can deduce that dWt â‹…dMt = 0.
â—½
24. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a standard Wiener process relative to the same filtration
â„±t, t â‰¥0. By considering the process
Zt = eu1Nt+u2Wt
where u1, u2 âˆˆâ„, use ItÂ¯oâ€™s formula to find an SDE for Zt.
By setting mt = ğ”¼(Zt
) show that solution to the SDE can be expressed as
dmt
dt âˆ’
[(eu1 âˆ’1)ğœ†+ 1
2u2
2
]
mt = 0.
Solve the differential equation to find ğ”¼(Zt
) and deduce that Nt âŸ‚âŸ‚Wt.

5.2.1
Properties of Poisson Process
279
Solution: By expanding Zt using Taylorâ€™s theorem and taking note that (dWt)2 = dt,
(dNt)2 = (dNt)3 = Â· Â· Â· = dNt, dWtdNt = 0,
dZt = ğœ•Zt
ğœ•Nt
dNt + ğœ•Zt
dWt
dWt + 1
2
ğœ•2Zt
ğœ•N2
t
(dNt)2 + 1
2
ğœ•2Zt
ğœ•W2
t
(dWt)2
+
ğœ•2Zt
ğœ•Ntğœ•Wt
(dNtdWt) + 1
3!
ğœ•3Zt
ğœ•N3
t
(dNt)3 + . . .
=
(
u1 + 1
2u2
1 + 1
3!u3
1 + . . .
)
ZtdNt + u2ZtdWt + 1
2u2
2Zt dt
= (eu1 âˆ’1)ZtdNt + u2ZtdWt + 1
2u2
2Zt dt.
Taking integrals, we have
âˆ«
t
0
dZs = (eu1 âˆ’1) âˆ«
t
0
Zs dNs + u2 âˆ«
t
0
Zs dWs + 1
2u2
2 âˆ«
t
0
Zs ds
Zt âˆ’1 = (eu1 âˆ’1) âˆ«
t
0
Zs dÌ‚Ns + u2 âˆ«
t
0
Zs dWs +
[
ğœ†(eu1 âˆ’1) + 1
2u2
2
]
âˆ«
t
0
Zs ds
where Z0 = 1 and Ì‚Nt = Nt âˆ’ğœ†t. Taking expectations,
ğ”¼(Zt) = 1 +
[
ğœ†(eu1 âˆ’1) + 1
2u2
2
]
âˆ«
t
0
ğ”¼(Zs) ds
where since both Wt and Ì‚Nt are martingales we therefore have
ğ”¼
(
âˆ«
t
0
Zs dWs
)
= 0
and
ğ”¼
(
âˆ«
t
0
Zs dÌ‚Ns
)
= 0.
By differentiating the integral equation,
d
dtğ”¼(Zt) =
[
ğœ†(eu1 âˆ’1) + 1
2u2
2
]
ğ”¼(Zt)
or
dmt
dt âˆ’
[
ğœ†(eu1 âˆ’1) + 1
2u2
2
]
mt = 0
where mt = ğ”¼(Zt).
By setting the integrating factor to be I = eâˆ’âˆ«(ğœ†(eu1âˆ’1)+ 1
2 u2
2) dt = eâˆ’(ğœ†(eu1âˆ’1)+ 1
2 u2
2)t and mul-
tiplying the differential equation with I, we have
d
dt
(
mteâˆ’ğœ†t(eu1âˆ’1)âˆ’1
2 u2
2t)
= 0
or
eâˆ’ğœ†t(eu1âˆ’1)âˆ’1
2 u2
2tğ”¼(eu1Nt+u2Wt) = C
where C is a constant. Since ğ”¼(eu1N0+u2W0) = 1 therefore C = 1, and hence we finally
obtain
ğ”¼(eu1Nt+u2Wt) = eğœ†t(eu1âˆ’1)+ 1
2 u2
2t.

280
5.2.1
Properties of Poisson Process
Since the joint moment generating function of
ğ”¼(eu1Nt+u2Wt) = eğœ†t(eu1âˆ’1) â‹…e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Nt and Wt, respec-
tively, we can deduce that Nt and Wt are independent.
â—½
25. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a standard Wiener process relative to the same filtration
â„±t, t â‰¥0. Let X1, X2, . . . be a sequence of independent and identically distributed ran-
dom variables which are also independent of Nt and Wt. By defining a compound Poisson
process Mt as
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
and by considering the process
Zt = eu1Mt+u2Wt
where u1, u2 âˆˆâ„, use ItÂ¯oâ€™s formula to find an SDE for Zt.
By setting mt = ğ”¼(Zt
) show that solution to the SDE can be expressed as
dmt
dt âˆ’
[(eğœ‘X(u1) âˆ’1) ğœ†+ 1
2u2
2
]
mt = 0
where ğœ‘X(u1) = ğ”¼(eu1Xt) is the moment generating function of Xt, which is the jump size
if N jumps at time t.
Finally, solve the differential equation to find ğ”¼(Zt
) and deduce that Mt âŸ‚âŸ‚Wt.
Solution: By expanding Zt using Taylorâ€™s theorem and taking note that (dWt)2 = dt,
dMtdWt = 0, (dNt)k = dNt and (dMt)k = Xk
t dNt for k = 1, 2, . . . ,
dZt = ğœ•Zt
ğœ•Mt
dMt + ğœ•Zt
dWt
dWt + 1
2
ğœ•2Zt
ğœ•M2
t
(dMt)2 + 1
2
ğœ•2Zt
ğœ•W2
t
(dWt)2
+
ğœ•2Zt
ğœ•Mtğœ•Wt
(dMtdWt) + 1
3!
ğœ•3Zt
ğœ•M3
t
(dMt)3 + . . .
=
(
u1Xt + 1
2u2
1X2
t + 1
3!u3
1X3
t + . . .
)
ZtdNt + u2ZtdWt + 1
2u2
2Zt dt
= (eu1Xt âˆ’1) ZtdNt + u2ZtdWt + 1
2u2
2Zt dt.
Integrating, we have
âˆ«
t
0
dZs = âˆ«
t
0
(eu1Xt âˆ’1) Zs dNs + u2 âˆ«
t
0
Zs dWs + 1
2u2
2 âˆ«
t
0
Zs ds
Zt âˆ’1 = âˆ«
t
0
(eu1Xt âˆ’1) Zs dÌ‚Ns + u2 âˆ«
t
0
Zs dWs + âˆ«
t
0
[
ğœ†(eu1Xt âˆ’1) + 1
2u2
2
]
Zs ds

5.2.2
Jump Diffusion Process
281
where Z0 = 1 and Ì‚Nt = Nt âˆ’ğœ†t. Taking expectations and because the jump size variable
Xt is independent of Nt and Wt, we have
ğ”¼(Zt) = 1 +
[
ğœ†(ğ”¼(eu1Xt) âˆ’1) + 1
2u2
2
]
âˆ«
t
0
ğ”¼(Zs
) ds
since ğ”¼
(
âˆ«
t
0
Zs dWs
)
= 0
and
ğ”¼
(
âˆ«
t
0
Zs dÌ‚Ns
)
= 0.
By differentiating the integral equation,
d
dtğ”¼(Zt
) =
[
ğœ†(ğ”¼(eu1Xt) âˆ’1) + 1
2u2
2
]
ğ”¼(Zt
)
or
dmt
dt âˆ’
[
ğœ†(ğœ‘X(u1) âˆ’1) + 1
2u2
2
]
mt = 0
where mt = ğ”¼(Zt
) and ğœ‘X(u1) = ğ”¼(eu1Xt).
By setting the integrating factor to be I = eâˆ’âˆ«(ğœ†(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2) dt = eâˆ’(ğœ†(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2)t and
multiplying the differential equation with I, we have
d
dt
(
mteâˆ’ğœ†t(ğœ‘X(u1)âˆ’1)âˆ’1
2 u2
2t)
= 0
or
eâˆ’ğœ†t(ğœ‘X(u1)âˆ’1)âˆ’1
2 u2
2tğ”¼(eu1Mt+u2Wt) = C
where C is a constant. Since ğ”¼(eu1M0+u2W0) = 1 therefore C = 1, and hence we finally
obtain
ğ”¼(eu1Mt+u2Wt) = eğœ†t(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2t.
Since the joint moment generating function of
ğ”¼(eu1Mt+u2Wt) = eğœ†t(ğœ‘X(u1)âˆ’1) â‹…e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Mt and Wt, respec-
tively, we can deduce that Mt and Wt, are independent.
â—½
5.2.2
Jump Diffusion Process
1. Pure Jump Process. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson
process with intensity ğœ†> 0 relative to the filtration â„±t, t â‰¥0. Suppose St follows a pure
jump process
dSt
Stâˆ’= (Jt âˆ’1)dNt
where Jt is the jump size variable if N jumps at time t, Jt âŸ‚âŸ‚Nt and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Explain why the term in dNt is Jt âˆ’1 and not Jt.

282
5.2.2
Jump Diffusion Process
Show that the above differential equation can also be written as
dSt
Stâˆ’= dMt
where Mt = âˆ‘Nt
i=1(Ji âˆ’1) is a compound Poisson process such that Ji, i = 1, 2, . . . is a
sequence of independent and identically distributed random variables which are also inde-
pendent of Nt.
Solution: Let Stâˆ’be the value of St just before a jump and assume there occurs an instan-
taneous jump (i.e., dNt = 1) in which St changes from Stâˆ’to JtStâˆ’where Jt is the jump
size. Thus,
dSt = JtStâˆ’âˆ’Stâˆ’= (Jt âˆ’1)Stâˆ’
or
dSt
Stâˆ’= (Jt âˆ’1).
Therefore, we can write
dSt
Stâˆ’= (Jt âˆ’1)dNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Given the compound Poisson process
Mt =
Nt
âˆ‘
i=1
(Ji âˆ’1)
we let Mtâˆ’be the value of Mt just before a jump event. If N jumps at time t then
dMt = Mt âˆ’Mtâˆ’= Jt âˆ’1.
Thus, in general, we can write
dMt = (Jt âˆ’1)dNt
which implies the pure jump process can also be expressed as dSt
Stâˆ’= dMt.
â—½
2. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process with inten-
sity ğœ†> 0 relative to the filtration â„±t, t â‰¥0. Suppose St follows a pure jump process
dSt
Stâˆ’= (Jt âˆ’1)dNt

5.2.2
Jump Diffusion Process
283
where Jt is the jump size variable if N jumps at time t and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Assume Jt follows a lognormal distribution such that log Jt âˆ¼ğ’©(ğœ‡J, ğœ2
J) and Jt is also
independent of Nt. By applying ItÂ¯oâ€™s formula on log St and taking integrals show that for
t < T,
ST = St
NTâˆ’t
âˆ
i=1
Ji
provided Ji âˆˆ(0, 2], i = 1, 2, . . . is a sequence of independent and identically distributed
jump size random variables which are independent of Nt.
Given St and NTâˆ’t = n, show that ST follows a lognormal distribution with mean
ğ”¼(ST|| St, NTâˆ’t = n) = Sten(ğœ‡J+ 1
2 ğœ2
J )
and variance
Var (ST|| St, NTâˆ’t = n) = S2
t (enğœ2
J âˆ’1)en(2ğœ‡J+ğœ2
J ).
Finally, given only St, show that
ğ”¼(ST|| St
) = St exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
and
Var (ST|| St
) = S2
t
[
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.
Solution: By letting Stâˆ’denote the value of St before a jump event, and expanding
d(log St) using Taylorâ€™s theorem and taking note that dNt â‹…dNt = dNt,
d(log St) = dSt
Stâˆ’âˆ’1
2
(dSt
Stâˆ’
)2
+ 1
3
(dSt
Stâˆ’
)3
âˆ’1
4
(dSt
Stâˆ’
)4
+ . . .
=
{
(Jt âˆ’1) âˆ’1
2(Jt âˆ’1)2 + 1
3(Jt âˆ’1)3 âˆ’1
4(Jt âˆ’1)4 + . . .
}
dNt
= log JtdNt
provided âˆ’1 < Jt âˆ’1 â‰¤1 or 0 < Jt â‰¤2.
By taking integrals, we have
âˆ«
T
t
d(log Su) = âˆ«
T
t
log Ju dNu
log
(ST
St
)
=
NTâˆ’t
âˆ‘
i=1
log Ji

284
5.2.2
Jump Diffusion Process
or
ST = St
NTâˆ’t
âˆ
i=1
Ji
where Ji âˆˆ(0, 2] is the jump size occurring at time instant ti and NTâˆ’t = NT âˆ’Nt is the
total number of jumps in the time interval (t, T].
Since log Ji âˆ¼ğ’©(ğœ‡J, ğœ2
J), i = 1, 2, . . . , NTâˆ’t are independent and identically distributed,
and conditional on St and NTâˆ’t = n,
log ST = log St +
NTâˆ’t
âˆ‘
i=1
log Ji
follows a normal distribution. Therefore, the mean and variance of ST are
ğ”¼(ST ||St, NTâˆ’t = n) = Sten(ğœ‡J+ 1
2 ğœ2
J )
and
Var (ST ||St, NTâˆ’t = n) = S2
t
(
enğœ2
J âˆ’1
)
en(2ğœ‡J+ğœ2
J )
respectively.
Finally, conditional only on St, by definition
ğ”¼(ST|| St
) = ğ”¼
(
St
NTâˆ’t
âˆ
i=1
Ji
||||||
St
)
= Stğ”¼
(NTâˆ’t
âˆ
i=1
Ji
||||||
St
)
= Stğ”¼
(NTâˆ’t
âˆ
i=1
Ji
)
= Stğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log Ji
)
.
By applying the tower property and from Problem 5.2.1.12 (page 264), we have
ğ”¼(ST|| St
) = Stğ”¼
[
ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log Ji||||
NTâˆ’t
)]
= St exp {ğœ†(T âˆ’t) [ğ”¼(elog Jt) âˆ’1]}
= St exp {ğœ†(T âˆ’t) [ğ”¼(Jt
) âˆ’1]} .
Since ğ”¼(Jt) = eğœ‡J+ 1
2 ğœ2
J therefore
ğ”¼(ST|St) = St exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
.

5.2.2
Jump Diffusion Process
285
For the case of variance of ST conditional on St, by definition
Var(ST|St) = Var
(
St
NTâˆ’t
âˆ
i=1
Ji
||||||
St
)
= S2
t Var
(NTâˆ’t
âˆ
i=1
Ji
)
= S2
t Var
(
e
âˆ‘NTâˆ’t
i=1
log Ji
)
= S2
t
{
ğ”¼
(
e2 âˆ‘NTâˆ’t
i=1
log Ji
)
âˆ’ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log Ji
)2}
= S2
t
{
ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log J2
i
)
âˆ’ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log Ji
)2}
.
By applying the tower property and from Problem 5.2.1.12 (page 264), we have
Var(ST|St) = S2
t ğ”¼
[
ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log J2
i ||||
NTâˆ’t
)]
âˆ’S2
t
{
ğ”¼
[
ğ”¼
(
e
âˆ‘NTâˆ’t
i=1
log Ji||||
NTâˆ’t
)]}2
= S2
t exp {ğœ†(T âˆ’t)
[ğ”¼(J2
t
) âˆ’1]} âˆ’S2
t exp {2ğœ†(T âˆ’t)
[ğ”¼(Jt) âˆ’1]} .
Since ğ”¼(Jt) = eğœ‡J+ 1
2 ğœ2
J and ğ”¼(J2
t ) = e2(ğœ‡J+ğœ2
J ) therefore
Var(ST|St) = S2
t
[
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.
â—½
3. Mertonâ€™s Model. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson
process with intensity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a standard Wiener process relative to the
same filtration â„±t, t â‰¥0. Suppose St follows a jump diffusion process with the following
SDE
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœdWt + (Jt âˆ’1)dNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt
with constants ğœ‡, D and ğœbeing the drift, continuous dividend yield and volatility, respec-
tively, and Jt the jump variable such that log Jt âˆ¼ğ’©(ğœ‡J, ğœ2
J). Assume that Jt, Wt and Nt are
mutually independent. In addition, we also let Ji, i = 1, 2, . . . be a sequence of indepen-
dent and identically distributed jump size random variables which are also independent of
Nt and Wt.

286
5.2.2
Jump Diffusion Process
By applying ItÂ¯oâ€™s formula on log St and taking integrals show that for t < T,
ST = Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
provided Jt âˆˆ(0, 2] and WTâˆ’t âˆ¼ğ’©(0, T âˆ’t).
Given St and NTâˆ’t = n, show that ST follows a lognormal distribution with mean
ğ”¼(ST|| St, NTâˆ’t = n) = Ste(ğœ‡âˆ’D)(Tâˆ’t)+n(ğœ‡J+ 1
2 ğœ2
J )
and variance
Var (ST|| St, NTâˆ’t = n) = S2
t
(
eğœ2(Tâˆ’t)+nğœ2
J âˆ’1
)
e2(ğœ‡âˆ’D)(Tâˆ’t)+n(2ğœ‡J+ğœ2
J ).
Finally, conditional only on St, show that
ğ”¼(ST|| St
) = Ste(ğœ‡âˆ’D)(Tâˆ’t) exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
and
Var (ST|| St
) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t) (
eğœ2(Tâˆ’t) âˆ’1
) [
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.
Solution: By letting Stâˆ’denote the value of St before a jump event, and expanding
d(log St) using both Taylorâ€™s theorem and ItÂ¯oâ€™s lemma,
d(log St) = dSt
Stâˆ’âˆ’1
2
(dSt
Stâˆ’
)2
+ 1
3
(dSt
Stâˆ’
)3
âˆ’1
4
(dSt
Stâˆ’
)4
+ . . .
= (ğœ‡âˆ’D) dt + ğœdWt + (Jt âˆ’1)dNt âˆ’1
2(ğœ2 dt + (Jt âˆ’1)2dNt)
+1
3(Jt âˆ’1)3dNt âˆ’1
4(Jt âˆ’1)4dNt + . . .
=
(
ğœ‡âˆ’D âˆ’1
2ğœ2)
dt + ğœdWt
+
{
(Jt âˆ’1) âˆ’1
2(Jt âˆ’1)2 + 1
3(Jt âˆ’1)3 âˆ’1
4(Jt âˆ’1)4 + . . .
}
dNt
=
(
ğœ‡âˆ’D âˆ’1
2ğœ2)
dt + ğœdWt + log JtdNt
provided âˆ’1 < Jt âˆ’1 â‰¤1 or 0 < Jt â‰¤2.
By taking integrals, we have
âˆ«
T
t
d(log Su) = âˆ«
T
t
(
ğœ‡âˆ’D âˆ’1
2ğœ2)
du + âˆ«
T
t
ğœdWu + âˆ«
T
t
log Ju dNu

5.2.2
Jump Diffusion Process
287
log
(ST
St
)
=
(
ğœ‡âˆ’D âˆ’1
2ğœ2)
(T âˆ’t) + ğœWTâˆ’t +
NTâˆ’t
âˆ‘
i=1
log Ji
or
ST = Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
where Ji âˆˆ(0, 2] is the random jump size occurring at time ti and NTâˆ’t = NT âˆ’Nt is the
total number of jumps in the time interval (t, T].
Since WTâˆ’t âˆ¼ğ’©(0, T âˆ’t), log Ji âˆ¼ğ’©(ğœ‡J, ğœ2
J), i = 1, 2, . . . and by independence,
log ST|{St, NTâˆ’t = n} âˆ¼ğ’©
[
log St +
(
ğœ‡âˆ’D âˆ’1
2ğœ2)
(T âˆ’t) + nğœ‡J, ğœ2(T âˆ’t) + nğœ2
J
]
.
Therefore, conditional on St and NTâˆ’t = n, ST follows a lognormal distribution with mean
ğ”¼(ST ||St, NTâˆ’t = n) = Ste(ğœ‡âˆ’D)(Tâˆ’t)+n(ğœ‡J+ 1
2 ğœ2
J )
and variance
Var (ST ||St, NTâˆ’t = n) = S2
t
(
eğœ2(Tâˆ’t)+nğœ2
J âˆ’1
)
e2(ğœ‡âˆ’D)(Tâˆ’t)+n(2ğœ‡J+ğœ2
J ).
Finally, given only St and from the mutual independence of a Wiener process and a com-
pound Poisson process, we have
ğ”¼(ST|St) = ğ”¼
[
Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
||||||
St
]
= Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)ğ”¼(eğœWTâˆ’t)ğ”¼
(NTâˆ’t
âˆ
i=1
Ji
)
= Ste(ğœ‡âˆ’D)(Tâˆ’t)ğ”¼
(NTâˆ’t
âˆ
i=1
Ji
)
.
Since ğ”¼(eğœWTâˆ’t) = e
1
2 ğœ2(Tâˆ’t), from Problem 5.2.2.2 (page 282) we therefore have
ğ”¼(ST|St) = Ste(ğœ‡âˆ’D)(Tâˆ’t) exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
.
As for the variance of St conditional on St, we can write
Var(ST|St) = Var
[
Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
||||||
St
]
= S2
t e
2
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)Var (eğœWTâˆ’t) Var
(NTâˆ’t
âˆ
i=1
Ji
)
.

288
5.2.2
Jump Diffusion Process
Since Var(eğœWTâˆ’t) = (eğœ2(Tâˆ’t) âˆ’1)eğœ2(Tâˆ’t) and using the results from Problem 5.2.2.3
(page 285), we finally have
Var(ST|St) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t)(eğœ2(Tâˆ’t) âˆ’1)
[
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.
â—½
4. Ornsteinâ€“Uhlenbeck Process with Jumps. Let (Î©, â„±, â„™) be a probability space and
let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a
standard Wiener process relative to the same filtration â„±t, t â‰¥0. Suppose St follows an
Ornsteinâ€“Uhlenbeck process with jumps of the form
dSt = ğœ…(ğœƒâˆ’Stâˆ’) dt + ğœdWt + log JtdNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt
with ğœ…, ğœƒand ğœbeing constant parameters such that ğœ…is the mean-reversion rate, ğœƒis
the long-term mean and ğœis the volatility. The random variable Jt is the jump amplitude
such that log Jt âˆ¼ğ’©(ğœ‡J, ğœ2
J), and Wt, Nt and Jt are mutually independent. Suppose the
sequence of jump amplitudes Ji, i = 1, 2, . . . is independent and identically distributed,
and also independent of Nt and Wt.
Using ItÂ¯oâ€™s lemma on eğœ…tSt show that for t < T,
ST = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
and conditional on St and NTâˆ’t = n,
ğ”¼(ST|St, NTâˆ’t = n) = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+nğœ‡J
(
ğœ†
ğœ†âˆ’ğœ…
) [eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)] [1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1
and
Var(ST|St, NTâˆ’t = n) = ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
+n (ğœ‡2
J + ğœ2
J
) (
ğœ†
ğœ†âˆ’2ğœ…
) [eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)] [1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1
+n(n âˆ’1)ğœ‡2
J
(
ğœ†
ğœ†âˆ’2ğœ…
)2[eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’2
âˆ’n2ğœ‡2
J
(
ğœ†
ğœ†âˆ’ğœ…
)2[eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]2(nâˆ’1).

5.2.2
Jump Diffusion Process
289
Finally, conditional on St show also that
ğ”¼(ST|St
) = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ†ğœ‡J
ğœ…
[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
and
Var (ST|St
) = ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)] + ğœ†
2ğœ…
(ğœ‡2
J + ğœ2
J
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] .
What is the distribution of ST given St?
Solution: In order to solve the jump diffusion process we note that
d(eğœ…tSt) = ğœ…eğœ…tSt dt + eğœ…tdSt + ğœ…2eğœ…tSt( dt)2 + . . .
and using ItÂ¯oâ€™s lemma,
d(eğœ…tSt) = ğœ…ğœƒeğœ…t dt + ğœeğœ…tdWt + eğœ…t log JtdNt.
For t < T the solution is represented by
ST = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu + âˆ«
T
t
eâˆ’ğœ…(Tâˆ’u) log JudNu.
Since
âˆ«
T
t
eâˆ’ğœ…(Tâˆ’u) log JudNu =
NT
âˆ‘
i=Nt
eâˆ’ğœ…(Tâˆ’ti) log Ji =
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
where Ji is the random jump size occurring at time ti and NTâˆ’t = NT âˆ’Nt is the total
number of jumps in the time interval (t, T], therefore
ST = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji.
Given St, from the properties of ItÂ¯oâ€™s integral we have
ğ”¼
[
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu
]
= 0
and
ğ”¼
[(
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu
)2]
= ğ”¼
[
âˆ«
T
t
ğœ2eâˆ’2ğœ…(Tâˆ’u) du
]
= ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)] .
Thus, given St and NTâˆ’t = n, from Problem 5.2.1.17 (page 269) we can easily obtain
ğ”¼
[NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
||||||
St, NTâˆ’t = n
]
= nğœ‡J
(
ğœ†
ğœ†âˆ’ğœ…
)[eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)][1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1

290
5.2.2
Jump Diffusion Process
and
Var
[NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
||||||
St, NTâˆ’t = n
]
= n(ğœ‡2
J + ğœ2
J)
(
ğœ†
ğœ†âˆ’2ğœ…
) [eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)] [1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1
+ n(n âˆ’1)ğœ‡2
J
(
ğœ†
ğœ†âˆ’2ğœ…
)2
Ã— [eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’2 âˆ’n2ğœ‡2
J
(
ğœ†
ğœ†âˆ’ğœ…
)2
Ã— [eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]2(nâˆ’1).
Therefore,
ğ”¼(ST|St, NTâˆ’t = n) = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+nğœ‡J
(
ğœ†
ğœ†âˆ’ğœ…
) [eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)] [1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1
and
Var (ST|St, NTâˆ’t = n) = ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
+n(ğœ‡2
J + ğœ2
J)
(
ğœ†
ğœ†âˆ’2ğœ…
) [eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)] [1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’1
+n(n âˆ’1)ğœ‡2
J
(
ğœ†
ğœ†âˆ’2ğœ…
)2[eâˆ’2ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]nâˆ’2
âˆ’n2ğœ‡2
J
(
ğœ†
ğœ†âˆ’ğœ…
)2[eâˆ’ğœ…(Tâˆ’t) âˆ’eâˆ’ğœ†(Tâˆ’t)]2[1 âˆ’eâˆ’ğœ†(Tâˆ’t)]2(nâˆ’1).
Finally, conditional on St, from Problem 5.2.1.18 (page 272) we have
ğ”¼
[NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
]
= ğœ†ğœ‡J
ğœ…
[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
and
Var
[NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
]
= ğœ†
2ğœ…
(ğœ‡2
J + ğœ2
J
) [1 âˆ’eâˆ’2ğœ…(Tâˆ’t)] .
Therefore,
ğ”¼(ST|St
) = Steâˆ’ğœ…(Tâˆ’t) + ğœƒ[1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ†ğœ‡J
ğœ…
[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
and
Var (ST|St
) = ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)] + ğœ†
2ğœ…
(ğœ‡2
J + ğœ2
J
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] .
Without the jump component, ST given St is normally distributed (see Problem 3.2.2.10,
page 132). With the inclusion of a jump component, the term âˆ‘NTâˆ’t
i=1 eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji

5.2.2
Jump Diffusion Process
291
consists of summing over the product of random jump times in the exponent and jump
amplitudes where we do not have an explicit expression for the distribution. Thus, the
distribution of ST conditional on St is not known.
â—½
5. Geometric Mean-Reverting Jump Diffusion Model. Let (Î©, â„±, â„™) be a probability space
and let {Nt âˆ¶t â‰¥0} be a Poisson process with intensity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a
standard Wiener process relative to the same filtration â„±t, t â‰¥0. Suppose St follows a
geometric mean-reverting jump diffusion process of the form
dSt
Stâˆ’= ğœ…(ğœƒâˆ’log Stâˆ’) dt + ğœdWt + (Jt âˆ’1)dNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt
with ğœ…, ğœƒand ğœbeing constant parameters. Here, ğœ…is the mean-reversion rate, ğœƒis the
long-term mean and ğœis the volatility. The random variable Jt is the jump amplitude
such that log Jt âˆ¼ğ’©(ğœ‡J, ğœ2
J), and Wt, Nt and Jt are mutually independent. Suppose the
sequence of jump amplitudes Ji, i = 1, 2, . . . is independent and identically distributed,
and also independent of Nt and Wt.
By applying ItÂ¯oâ€™s formula on Xt = log St, show that
dXt =
(
ğœ…(ğœƒâˆ’Xtâˆ’) âˆ’1
2ğœ2)
dt + ğœdWt + log JtdNt.
Using ItÂ¯oâ€™s lemma on eğœ…tXt and taking integrals show that for t < T,
log ST = (log St)eâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+ âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji.
Conditional on St and NTâˆ’t = n, show that
ğ”¼(ST|| St, NTâˆ’t = n) = Seâˆ’ğœ…(Tâˆ’t)
t
exp
{(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ2
4ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
Ã—
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
and
Var (ST|| St, NTâˆ’t = n) = S2eâˆ’ğœ…(Tâˆ’t)
t
exp
{
2
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]}
Ã—
[
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
âˆ’1
]
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}

292
5.2.2
Jump Diffusion Process
Ã—
{[
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
âˆ’
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]2n}
.
Finally, given only St, show that
ğ”¼(ST|St) = St exp
{
eâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ2
4ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
+ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s ds âˆ’ğœ†(T âˆ’t)
}
and
Var(ST|St) = S2
t exp
{
2eâˆ’ğœ…(Tâˆ’t) + 2
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]}
Ã—
[
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
âˆ’1
]
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
eâˆ’ğœ†(Tâˆ’t)
Ã—
[
exp
{
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s (
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’ğœ…s âˆ’2
)
ds
}
âˆ’eâˆ’ğœ†(Tâˆ’t)
]
.
Solution: We first let Stâˆ’denote the value of St before a jump event and by expanding
d(log St) using Taylorâ€™s theorem and subsequently applying ItÂ¯oâ€™s lemma, we have
d(log St) = dSt
Stâˆ’âˆ’1
2
(dSt
Stâˆ’
)2
+ 1
3
(dSt
Stâˆ’
)3
âˆ’1
4
(dSt
Stâˆ’
)4
+ . . .
= ğœ…(ğœƒâˆ’log Stâˆ’) dt + ğœdWt + (Jt âˆ’1)dNt âˆ’1
2[ğœ2 dt + (Jt âˆ’1)2dNt]
+1
3(Jt âˆ’1)3dNt âˆ’1
4(Jt âˆ’1)4dNt + . . .
=
[
ğœ…(ğœƒâˆ’log Stâˆ’) âˆ’1
2ğœ2]
dt + ğœdWt + log JtdNt
provided the random jump amplitude âˆ’1 < Jt âˆ’1 â‰¤1 or 0 < Jt â‰¤2.
Setting Xt = log Stâˆ’we can redefine the above SDE into an Ornsteinâ€“Uhlenbeck process
with jumps
dXt = ğœ…(ğœƒâˆ’Xtâˆ’) dt + ğœdWt + log JtdNt
and applying Taylorâ€™s theorem and then Itoâ€™s lemma on d(eğœ…tXt), we have
d(eğœ…tXt) = ğœ…eğœ…tXtâˆ’dt + eğœ…tdXt + ğœ…2eğœ…tXtâˆ’( dt)2 + . . .
= ğœ…ğœƒeğœ…t dt + ğœeğœ…tdWt + eğœ…t log JtdNt.

5.2.2
Jump Diffusion Process
293
Taking integrals for t < T,
âˆ«
T
t
d(eğœ…sXs) = âˆ«
T
t
(
ğœ…ğœƒeğœ…s âˆ’1
2ğœ2eğœ…s)
ds + âˆ«
T
t
ğœeğœ…sdWs + âˆ«
T
t
eğœ…s log Js dNs
or
XT = Xteâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji.
Substituting XT = log ST and Xt = log St, we finally have
log ST = (log St)eâˆ’ğœ…(Tâˆ’t) +
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+ âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
or
ST = Seâˆ’ğœ…(Tâˆ’t)
t
exp
{(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
+ âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’s)dWs +
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}
.
To find the expectation and variance of ST given St, we note that from ItÂ¯oâ€™s integral
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu âˆ¼ğ’©
(
0, ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)])
and hence
ğ”¼
[
exp
{
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu
}]
= exp
{
ğœ2
4ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
and
Var
[
exp
{
âˆ«
T
t
ğœeâˆ’ğœ…(Tâˆ’u)dWu
}]
=
[
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
âˆ’1
]
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
.
Conditional on NTâˆ’t = n, from Problem 5.2.1.17 (page 269) and by setting ğœ‰= 1, X =
log J and due to the independence of log Ji, i = 1, 2, . . . , n, we can write
ğ”¼
[
exp
{ n
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
=
n
âˆ
i=1
{
âˆ«
T
t
Mlog Ji (eâˆ’ğœ…s) ğœ†eâˆ’ğœ†(Tâˆ’tâˆ’s) ds
}
=
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n

294
5.2.2
Jump Diffusion Process
and
ğ”¼
[
exp
{
2
n
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
=
n
âˆ
i=1
{
âˆ«
T
t
Mlog Ji (2eâˆ’ğœ…s) ğœ†eâˆ’ğœ†(Tâˆ’tâˆ’s) ds
}
=
[
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
.
Therefore,
Var
[
exp
{ n
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
= ğ”¼
[
exp
{
2
n
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
âˆ’ğ”¼
[
exp
{ n
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]2
=
[
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
âˆ’
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]2n
.
Conditional on St and NTâˆ’t = n and due to the independence of Wt and Nt, we can easily
show that
ğ”¼(ST|| St, NTâˆ’t = n) = Seâˆ’ğœ…(Tâˆ’t)
t
exp
{(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ2
4ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
Ã—
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
and
Var (ST|| St, NTâˆ’t = n) = S2eâˆ’ğœ…(Tâˆ’t)
t
exp
{
2
(
ğœƒâˆ’ğœ2
2ğœ…
)
[1 âˆ’eâˆ’ğœ…(Tâˆ’t)]
}
Ã—
[
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
âˆ’1
]
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
Ã—
{[
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]n
âˆ’
[
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…sâˆ’ğœ†(Tâˆ’tâˆ’s) ds
]2n}
.
Finally, by treating NTâˆ’t as a random variable, from Problem 5.2.1.18 (page 272) and by
setting ğœ‰= 1 and X = log J, we can express
ğ”¼
[
exp
{NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
= exp
{
ğœ†âˆ«
T
t
[Mlog J(eâˆ’ğœ…s) âˆ’1] ds
}
= exp
{
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s ds âˆ’ğœ†(T âˆ’t)
}

5.2.2
Jump Diffusion Process
295
and
ğ”¼
[
exp
{
2
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
= exp
{
ğœ†âˆ«
T
t
[Mlog J(2eâˆ’ğœ…s) âˆ’1] ds
}
= exp
{
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…s ds âˆ’ğœ†(T âˆ’t)
}
and subsequently
Var
[
exp
{NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
= ğ”¼
[
exp
{
2
NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]
âˆ’ğ”¼
[
exp
{NTâˆ’t
âˆ‘
i=1
eâˆ’ğœ…(Tâˆ’tâˆ’ti) log Ji
}]2
= exp
{
ğœ†âˆ«
T
t
e2ğœ‡Jeâˆ’ğœ…s+ğœ2
J eâˆ’2ğœ…s ds âˆ’ğœ†(T âˆ’t)
}
âˆ’exp
{
2ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s ds âˆ’2ğœ†(T âˆ’t)
}
= eâˆ’ğœ†(Tâˆ’t)
[
exp
{
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s (
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’ğœ…s âˆ’2
)
ds
}
âˆ’eâˆ’ğœ†(Tâˆ’t)
]
.
Conditional only on St and from the independence of Wt and Nt, we have
ğ”¼(ST|St) = Seâˆ’ğœ…(Tâˆ’t)
t
exp
{(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)] + ğœ2
4ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]
+ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s ds âˆ’ğœ†(T âˆ’t)
}
and
Var(ST|St) = S2eâˆ’ğœ…(Tâˆ’t)
t
exp
{
2
(
ğœƒâˆ’ğœ2
2ğœ…
) [1 âˆ’eâˆ’ğœ…(Tâˆ’t)]}
Ã—
[
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
âˆ’1
]
exp
{
ğœ2
2ğœ…
[1 âˆ’eâˆ’2ğœ…(Tâˆ’t)]}
eâˆ’ğœ†(Tâˆ’t)
Ã—
[
exp
{
ğœ†âˆ«
T
t
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’2ğœ…s (
eğœ‡Jeâˆ’ğœ…s+ 1
2 ğœ2
J eâˆ’ğœ…s âˆ’2
)
ds
}
âˆ’eâˆ’ğœ†(Tâˆ’t)
]
.
â—½
6. Kouâ€™s Model. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶t â‰¥0} be a Poisson process
with intensity ğœ†> 0 and {Wt âˆ¶t â‰¥0} be a standard Wiener process relative to the same

296
5.2.2
Jump Diffusion Process
filtration â„±t, t â‰¥0. Suppose St follows a jump diffusion process with the following SDE
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœdWt + (Jt âˆ’1)dNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt
with constants ğœ‡, D and ğœ> 0 being the drift, continuous dividend yield and volatility,
respectively. The jump variable is denoted by Jt, where Xt = log Jt follows an asymmetric
double exponential distribution with density function
fXt(x) =
{
pğ›¼eâˆ’ğ›¼x
x â‰¥0
(1 âˆ’p)ğ›½eğ›½x
x > 0
where 0 â‰¤p â‰¤1, ğ›¼> 1 and ğ›½> 0. Assume that Jt, Wt and Nt are mutually independent
and let Ji, i = 1, 2, . . . be a sequence of independent and identically distributed random
variables which are independent of Nt and Wt.
By applying ItÂ¯oâ€™s formula on log St and taking integrals show that for t < T,
ST = Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
provided the random variable Ji âˆˆ(0, 2], i = 1, 2, . . . , NTâˆ’t and WTâˆ’t âˆ¼ğ’©(0, T âˆ’t).
Given St and NTâˆ’t = n, show that the mean and variance of ST are
ğ”¼(ST|St, NTâˆ’t = n) = Ste(ğœ‡âˆ’D)(Tâˆ’t)
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]n
and
Var (ST|St, NTâˆ’t = n) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t) (
eğœ2(Tâˆ’t) âˆ’1
)
Ã—
{
p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2 âˆ’
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]2}n
.
Finally, conditional only on St, show that
ğ”¼(ST|St) = Ste(ğœ‡âˆ’D)(Tâˆ’t)
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]
exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
and
Var(ST|St) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t) (
eğœ2(Tâˆ’t) âˆ’1
) [
p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2
]
Ã—
[
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.

5.2.2
Jump Diffusion Process
297
Solution: Using the same steps as described in Problem 5.2.2.3 (page 285), for t < T we
can show the solution of the jump diffusion process is
ST = Ste
(
ğœ‡âˆ’Dâˆ’1
2 ğœ2)
(Tâˆ’t)+ğœWTâˆ’t
NTâˆ’t
âˆ
i=1
Ji
provided Ji âˆˆ(0, 2], i = 1, 2, . . . , NTâˆ’t.
To find the mean and variance of Jt we note that
ğ”¼(Jt
) = ğ”¼(eXt)
= âˆ«
âˆ
âˆ’âˆ
exfXt(x) dx
= âˆ«
0
âˆ’âˆ
(1 âˆ’p)ğ›½e(1+ğ›½)xdx + âˆ«
âˆ
0
pğ›¼e(1âˆ’ğ›¼)xdx
= p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
and
ğ”¼(J2
t
) = ğ”¼(e2Xt)
= âˆ«
âˆ
âˆ’âˆ
e2xfXt(x) dx
= âˆ«
0
âˆ’âˆ
(1 âˆ’p)ğ›½e(2+ğ›½)xdx + âˆ«
âˆ
0
pğ›¼e(2âˆ’ğ›¼)xdx
= p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2
so that
Var (Jt) = ğ”¼(J2
t ) âˆ’ğ”¼(Jt)2
= p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2 âˆ’
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]2
.
Since WTâˆ’t âˆ¼ğ’©(0, T âˆ’t) so that
ğ”¼(eğœWTâˆ’t) = e
1
2 ğœ2(Tâˆ’t)
and
Var (eğœWTâˆ’t) =
(
eğœ2(Tâˆ’t) âˆ’1
)
eğœ2(Tâˆ’t)
and conditional on St, NTâˆ’t = n and from independence, we have
ğ”¼(ST|St, NTâˆ’t = n) = Ste(ğœ‡âˆ’D)(Tâˆ’t)âˆ’1
2 ğœ2(Tâˆ’t)ğ”¼(eğœWTâˆ’t)
n
âˆ
i=1
ğ”¼(Ji)
= Ste(ğœ‡âˆ’D)(Tâˆ’t)
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]n

298
5.2.3
Girsanovâ€™s Theorem for Jump Processes
and
Var (ST|St, NTâˆ’t = n) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t)âˆ’ğœ2(Tâˆ’t)Var (eğœWTâˆ’t)
n
âˆ
i=1
Var (Ji)
= S2
t e2(ğœ‡âˆ’D)(Tâˆ’t) (
eğœ2(Tâˆ’t) âˆ’1
)
Ã—
{
p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2 âˆ’
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]2}n
.
Finally, conditional on St and from Problem 5.2.2.2 (page 282), we can show
ğ”¼(ST|St
) = Ste(ğœ‡âˆ’D)(Tâˆ’t)âˆ’1
2 ğœ2(Tâˆ’t)ğ”¼(eğœWTâˆ’t) ğ”¼
(NTâˆ’t
âˆ
i=1
Ji
)
= Ste(ğœ‡âˆ’D)(Tâˆ’t)
[
p
ğ›¼
ğ›¼âˆ’1 + (1 âˆ’p)
ğ›½
ğ›½+ 1
]
exp
{
ğœ†
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)
(T âˆ’t)
}
and
Var (ST|St
) = S2
t e2(ğœ‡âˆ’D)(Tâˆ’t)âˆ’ğœ2(Tâˆ’t)Var (eğœWTâˆ’t) Var
(NTâˆ’t
âˆ
i=1
Ji
)
= S2
t e2(ğœ‡âˆ’D)(Tâˆ’t) (
eğœ2(Tâˆ’t) âˆ’1
) [
p
ğ›¼
ğ›¼âˆ’2 + (1 âˆ’p)
ğ›½
ğ›½+ 2
]
Ã—
[
exp
{
ğœ†(T âˆ’t)
(
e2(ğœ‡J+ğœ2
J ) âˆ’1
)}
âˆ’exp
{
2ğœ†(T âˆ’t)
(
eğœ‡J+ 1
2 ğœ2
J âˆ’1
)}]
.
â—½
5.2.3
Girsanovâ€™s Theorem for Jump Processes
1. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Let ğœ‚> 0 and consider the
Radonâ€“NikodÂ´ym derivative process
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt.
Show that
dZt
Ztâˆ’= (ğœ†âˆ’ğœ‚) dt âˆ’
(ğœ†âˆ’ğœ‚
ğœ†
)
dNt
for 0 â‰¤t â‰¤T.
Solution: From Taylorâ€™s theorem,
dZt = ğœ•Zt
ğœ•t dt + ğœ•Zt
ğœ•Nt
dNt + ğœ•2Zt
ğœ•tğœ•Nt
(dNt dt) + 1
2!
ğœ•2Zt
ğœ•N2
t
(dNt)2 + 1
3!
ğœ•3Zt
ğœ•N3
t
(dNt)3 + . . .

5.2.3
Girsanovâ€™s Theorem for Jump Processes
299
Since dNt dt = 0, (dNt)2 = (dNt)3 = . . . = dNt, we have
dZt = ğœ•Zt
ğœ•t dt +
(
ğœ•Zt
ğœ•Nt
+ 1
2!
ğœ•2Zt
ğœ•N2
t
+ 1
3!
ğœ•3Zt
ğœ•N3
t
+ . . .
)
dNt.
From Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt we can express
ğœ•Zt
ğœ•t = (ğœ†âˆ’ğœ‚)e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt = (ğœ†âˆ’ğœ‚)Zt
ğœ•Zt
ğœ•Nt
= e(ğœ†âˆ’ğœ‚)t log
(ğœ‚
ğœ†
) (ğœ‚
ğœ†
)Nt = log
(ğœ‚
ğœ†
)
Zt
ğœ•2Zt
ğœ•N2
t
= log
(ğœ‚
ğœ†
) ğœ•Zt
ğœ•Nt
=
[
log
(ğœ‚
ğœ†
)]2
Zt.
In general, we can deduce that
ğœ•mZt
ğœ•Nm
t
=
[
log
(ğœ‚
ğœ†
)]m
Zt,
m = 1, 2, . . .
Therefore,
dZt = (ğœ†âˆ’ğœ‚)Zt dt +
{
log
(ğœ‚
ğœ†
)
+ 1
2!
[
log
(ğœ‚
ğœ†
)]2
+ 1
3!
[
log
(ğœ‚
ğœ†
)]3
+ . . .
}
ZtdNt
= (ğœ†âˆ’ğœ‚)Zt dt +
{
e
log
( ğœ‚
ğœ†
)
âˆ’1
}
ZtdNt
= (ğœ†âˆ’ğœ‚)Zt dt +
(ğœ‚âˆ’ğœ†
ğœ†
)
ZtdNt.
By letting Ztâˆ’denote the value of Zt before a jump event, we have
dZt
Ztâˆ’= (ğœ†âˆ’ğœ‚) dt âˆ’
(ğœ†âˆ’ğœ‚
ğœ†
)
dNt.
â—½
2. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Let ğœ‚> 0 and consider the
Radonâ€“NikodÂ´ym derivative process
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt.
Show that ğ”¼â„™(Zt
) = 1 and Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.

300
5.2.3
Girsanovâ€™s Theorem for Jump Processes
Solution: From Problem 5.2.3.1 (page 298), Zt satisfies
dZt
Ztâˆ’= (ğœ†âˆ’ğœ‚) dt âˆ’
(ğœ†âˆ’ğœ‚
ğœ†
)
dNt
=
(ğœ‚âˆ’ğœ†
ğœ†
)
(dNt âˆ’ğœ†dt)
=
(ğœ‚âˆ’ğœ†
ğœ†
)
d(Nt âˆ’ğœ†t)
=
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nt
where Ì‚Nt = Nt âˆ’ğœ†t. Since Ì‚Nt = Nt âˆ’ğœ†t is a â„™-martingale, therefore
(ğœ‚âˆ’ğœ†
ğœ†
)
Ì‚Nt is also
a â„™-martingale.
Taking integrals,
âˆ«
t
0
dZu = âˆ«
t
0
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
Zt = Z0 + âˆ«
t
0
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
and taking expectations,
ğ”¼â„™(Zt
) = ğ”¼â„™(Z0
) + ğ”¼â„™
[
âˆ«
t
0
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
]
= 1
since Z0 = 1 and due to Ì‚Nt being a â„™-martingale, ğ”¼â„™
[
âˆ«
t
0
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
]
= 0. Thus,
ğ”¼â„™(Zt
) = 1 for all 0 â‰¤t â‰¤T.
To show that Zt is a positive â„™-martingale, by taking integrals for s < t,
âˆ«
t
s
dZu = âˆ«
t
s
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
Zt âˆ’Zs = âˆ«
t
s
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu.
Taking expectations under the filtration â„±s, s < t,
ğ”¼â„™(Zt âˆ’Zs|â„±s
) = ğ”¼â„™
[
âˆ«
t
s
Zuâˆ’
(ğœ‚âˆ’ğœ†
ğœ†
)
dÌ‚Nu
|||||
â„±s
]
and because Ì‚Nt is a â„™-martingale, therefore
ğ”¼â„™(Zt âˆ’Zs|â„±s
) = 0
or
ğ”¼â„™(Zt|â„±s
) = Zs.

5.2.3
Girsanovâ€™s Theorem for Jump Processes
301
In addition, since Zt > 0 for all 0 â‰¤t â‰¤T, therefore |Zt| = Zt and hence ğ”¼â„™(|Zt|) =
ğ”¼â„™(Zt
) = 1 < âˆfor all 0 â‰¤t â‰¤T. Finally, because Zt is â„±t-adapted, we can conclude
that Zt is a positive â„™-martingale for all 0 â‰¤t â‰¤T.
â—½
3. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Let ğœ‚> 0 and consider the
Radonâ€“NikodÂ´ym derivative process
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt.
By changing the measure â„™to a measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
show that under the â„šmeasure, Nt âˆ¼Poisson(ğœ‚t) with intensity ğœ‚> 0 for 0 â‰¤t â‰¤T.
Solution: To solve this problem we need to show
ğ”¼â„š(euNt) = eğœ‚t(euâˆ’1)
which is the moment generating function of a Poisson process, Nt âˆ¼Poisson(ğœ‚t),
0 â‰¤t â‰¤T.
For u âˆˆâ„, using the moment generating function approach,
ğ”¼â„š(euNt) = ğ”¼â„™
(
euNt dâ„š
dâ„™
||||â„±t
)
= ğ”¼â„™
[
euNte(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt]
= e(ğœ†âˆ’ğœ‚)tğ”¼â„™[
e(u+log( ğœ‚
ğœ†))Nt
]
= e(ğœ†âˆ’ğœ‚)t exp
{
ğœ†t
(
e
u+log
( ğœ‚
ğœ†
)
âˆ’1
)}
since the moment generating function of Nt âˆ¼Poisson(ğœ†t) is
ğ”¼â„™(emNt) = eğœ†t(emâˆ’1)
for all m âˆˆâ„and hence
ğ”¼â„š(euNt) = eğœ‚t(euâˆ’1)
which is the moment generating function of a Poisson process with intensity ğœ‚. Therefore,
under the â„šmeasure, Nt âˆ¼Poisson(ğœ‚t) for 0 â‰¤t â‰¤T.
â—½

302
5.2.3
Girsanovâ€™s Theorem for Jump Processes
4. Let {Nt âˆ¶0 â‰¤t â‰¤0} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, t â‰¥0. Suppose ğœƒt is an adapted process,
0 â‰¤t â‰¤T and ğœ‚> 0. We consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
= e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du.
Show that ğ”¼â„™(Zt) = 1 and Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
Solution: From Problem 4.2.2.1 (page 194) and Problem 5.2.3.2 (page 299), we have
shown that ğ”¼â„™(
Z(1)
t
)
= 1, ğ”¼â„™(
Z(2)
t
)
= 1 and both Z(1)
t
and Z(2)
t
are â„™-martingales for
0 â‰¤t â‰¤T.
From ItÂ¯oâ€™s lemma,
d(Z(1)
t Z(2)
t ) = Z(1)
tâˆ’dZ(2)
t
+ Z(2)
t dZ(1)
t
+ dZ(1)
t dZ(2)
t
where Z(1)
tâˆ’is the value of Z(1)
t
before a jump event. Since dZ(1)
t dZ(2)
t
= 0 thus, by taking
integrals,
âˆ«
t
0
d(Z(1)
u Z(2)
u ) = âˆ«
t
0
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
0
Z(2)
u dZ(1)
u
Z(1)
t Z(2)
t
= Z(1)
0 Z(2)
0
+ âˆ«
t
0
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
0
Z(2)
u dZ(1)
u .
Taking expectations under the â„™measure,
ğ”¼â„™(
Z(1)
t Z(2)
t
)
= ğ”¼â„™(
Z(1)
0 Z(2)
0
)
= 1
since Z(1)
0
= 1, Z(2)
0
= 1 and both Z(1)
t
and Z(2)
t
are â„™-martingales.
To show that Zt = Z(1)
t
â‹…Z(2)
t
is a positive â„™-martingale, by taking integrals for s < t of
d(Z(1)
t Z(2)
t ) = Z(1)
tâˆ’dZ(2)
t
+ Z(2)
t dZ(1)
t
we have
Z(1)
t Z(2)
t
= Z(1)
s Z(2)
s
+ âˆ«
t
s
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
s
Z(2)
u dZ(1)
u
and taking expectations with respect to the filtration â„±s, s < t
ğ”¼â„™(
Z(1)
t Z(2)
t
|||â„±s
)
= Z(1)
s Z(2)
s
since ğ”¼â„™
(
âˆ«
t
s
Z(1)
uâˆ’dZ(2)
u
|||||
â„±s
)
= ğ”¼â„™
(
âˆ«
t
s
Z(2)
u dZ(1)
u
|||||
â„±s
)
= 0.

5.2.3
Girsanovâ€™s Theorem for Jump Processes
303
In addition, since Z(1)
t
> 0 and Z(2)
t
> 0 for all 0 â‰¤t â‰¤T, therefore |||Z(1)
t Z(2)
t
||| = Z(1)
t Z(2)
t
and
hence ğ”¼â„™(||Zt||
) = ğ”¼â„™(Zt) = 1 < âˆfor 0 â‰¤t â‰¤T. Finally, because Zt is also â„±t-adapted
we can conclude that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
â—½
5. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probabil-
ity space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose ğœƒt is an adapted
process, 0 â‰¤t â‰¤T and ğœ‚> 0. We consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
= e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du.
By changing the measure â„™to measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
show that, under the â„šmeasure, Nt is a Poisson process with intensity ğœ‚> 0, the process
ÌƒWt = Wt + âˆ«
t
0
ğœƒu du is a â„š-standard Wiener process and Nt âŸ‚âŸ‚ÌƒWt for 0 â‰¤t â‰¤T.
Solution: The first two results are given in Problem 5.2.3.3 (page 301) and Problem
4.2.2.10 (page 205).
To prove the final result we need to show that, under the â„šmeasure,
ğ”¼â„š(eu1Nt+u2 ÌƒWt) = eğœ‚t(eu1âˆ’1) â‹…e
1
2 u2
2t
which is a joint product of independent moment generating functions of Nt âˆ¼Poisson(ğœ‚t)
and ÌƒWt âˆ¼ğ’©(0, t).
From the definition
ğ”¼â„š(
eu1Nt+u2 ÌƒWt
)
= ğ”¼â„™(
eu1Nt+u2 ÌƒWtZt
)
= ğ”¼â„™(
eu1NtZ(1)
t
â‹…eu2 ÌƒWtZ(2)
t
)
we let
Zt = Z
(1)
t
â‹…Z
(2)
t
where
Z
(1)
t
= eu1NtZ(1)
t
and
Z
(2)
t
= eu2 ÌƒWtZ(2)
t .
Using ItÂ¯oâ€™s lemma, we have
dZt = d
(
Z
(1)
t Z
(2)
t
)
= Z
(1)
t dZ
(2)
t
+ Z
(2)
t dZ
(1)
t
+ dZ
(1)
t dZ
(2)
t
where Z
(1)
tâˆ’is the value of Z
(1)
t
before a jump event.

304
5.2.3
Girsanovâ€™s Theorem for Jump Processes
For dZ
(1)
t
we can expand using Taylorâ€™s theorem,
dZ
(1)
t
= ğœ•Z
(1)
t
ğœ•Nt
dNt + ğœ•Z
(1)
t
ğœ•Z(1)
t
dZ(1)
t
+ 1
2!
â¡
â¢
â¢â£
ğœ•2Z
(1)
t
ğœ•N2
t
(dNt)2 + 2 ğœ•2Z
(1)
t
ğœ•Ntğœ•Z(1)
t
(dNtdZ(1)
t ) + ğœ•2Z
(1)
t
ğœ•(Z(1)
t )2 (dZ(1)
t )2
â¤
â¥
â¥â¦
+ 1
3!
â¡
â¢
â¢â£
ğœ•3Z
(1)
t
ğœ•N3
t
(dNt)3 + 3 ğœ•3Z
(1)
t
ğœ•N2
t ğœ•Z(1)
t
(dNt)2(dZ(1)
t )
+3
ğœ•3Z
(1)
t
ğœ•Ntğœ•(Z(1)
t )2 (dNt)(dZ(1)
t )2 + ğœ•3Z
(1)
t
ğœ•(Z(1)
t )3 (dZ(1)
t )3
â¤
â¥
â¥â¦
+ . . .
Because dZ(1)
t
=
(ğœ‚âˆ’ğœ†
ğœ†
)
Z(1)
tâˆ’dÌ‚Nt, Z
(1)
t
= eu1NtZ(1)
t
and dÌ‚Nt = dNt âˆ’ğœ†dt, we can write
dZ
(1)
t
= u1Z
(1)
tâˆ’dNt + eu1NtdZ(1)
t
+ 1
2!
[
u2
1Z
(1)
tâˆ’dNt + 2
(ğœ‚âˆ’ğœ†
ğœ†
)
u1Z
(1)
tâˆ’dNtdÌ‚Nt
]
+ 1
3!
[
u3
1Z
(1)
tâˆ’dNt + 3
(ğœ‚âˆ’ğœ†
ğœ†
)
u2
1Z
(1)
tâˆ’dNtdÌ‚Nt
]
+ . . .
=
[
u1 + 1
2!u2
1 + 1
3!u3
1 + . . .
]
Z
(1)
tâˆ’dNt +
(ğœ‚âˆ’ğœ†
ğœ†
)
eu1NtZ(1)
t dÌ‚Nt
+
(ğœ‚âˆ’ğœ†
ğœ†
) [
u1 + 1
2!u2
1 + 1
3!u3
1 + . . .
]
Z
(1)
tâˆ’dNtdÌ‚Nt
= (eu1 âˆ’1) Z
(1)
tâˆ’dNt +
(ğœ‚âˆ’ğœ†
ğœ†
)
Z
(1)
tâˆ’dÌ‚Nt +
(ğœ‚âˆ’ğœ†
ğœ†
)
(eu1 âˆ’1) Z
(1)
tâˆ’dNtdÌ‚Nt
=
(ğœ‚
ğœ†
)
(eu1 âˆ’1)Z
(1)
tâˆ’dNt +
(ğœ‚âˆ’ğœ†
ğœ†
)
Z
(1)
tâˆ’dÌ‚Nt
since (dNt)2 = dNt and dNt dt = 0.
For the case of dZ
(2)
t , by applying Taylorâ€™s theorem
dZ
(2)
t
= ğœ•Z
(2)
t
ğœ•ÌƒWt
d ÌƒWt + ğœ•Z
(2)
t
ğœ•Z(2)
t
dZ(2)
t
+ 1
2!
â¡
â¢
â¢â£
ğœ•2Z
(2)
t
ğœ•( ÌƒWt)2 (d ÌƒWt)2 + 2 ğœ•2Z
(2)
t
ğœ•ÌƒWtğœ•Z(2)
t
(d ÌƒWt)(dZ(2)
t ) + ğœ•2Z
(2)
t
ğœ•(Z(2)
t )2 (dZ(2)
t )2
â¤
â¥
â¥â¦
+ . . .

5.2.3
Girsanovâ€™s Theorem for Jump Processes
305
and since d ÌƒWt = dWt + ğœƒt dt and dZ(2)
t
= âˆ’ğœƒtZ(2)
t dWt (see Problem 4.2.2.2, page 196), we
can write
dZ
(2)
t
= 1
2u2
2Z
(2)
t
dt + (u2 âˆ’ğœƒt)Z
(2)
t dWt.
Since dÌ‚Nt = dNt âˆ’ğœ†dt, dNtdWt = 0, dWt dt = 0, dNt dt = 0, (dWt)2 = dt and ( dt)2 = 0,
we have
Z
(1)
tâˆ’dZ
(2)
t
= 1
2u2
2Zt dt + (u2 âˆ’ğœƒt)ZtdWt,
Z
(2)
t dZ
(1)
t
=
(ğœ‚
ğœ†
)
(eu1 âˆ’1)ZtdNt +
(ğœ‚âˆ’ğœ†
ğœ†
)
ZtdÌ‚Nt
and
dZ
(1)
t dZ
(2)
t
= 0.
Thus, by letting dNt = dÌ‚Nt + ğœ†dt, we have
dZt = 1
2u2
2Zt dt + (u2 âˆ’ğœƒt)ZtdWt +
(ğœ‚
ğœ†
)
(eu1 âˆ’1)ZtdNt +
(ğœ‚âˆ’ğœ†
ğœ†
)
ZtdÌ‚Nt
=
[
ğœ‚(eu1 âˆ’1) + 1
2u2
2
]
Zt dt + (u2 âˆ’ğœƒt)ZtdWt +
(ğœ‚eu1 âˆ’ğœ†
ğœ†
)
ZtdÌ‚Nt.
Taking integrals, we have
Zt = 1 + âˆ«
t
0
[
ğœ‚(eu1 âˆ’1) + 1
2u2
2
]
Zs ds + âˆ«
t
0
(u2 âˆ’ğœƒs)Zs dWs
+ âˆ«
t
0
(ğœ‚eu1 âˆ’ğœ†
ğœ†
)
Zs dÌ‚Ns
where Z0 = 1, and because Ì‚Nt and Wt are â„™-martingales, thus by taking expectations under
the â„™measure we have
ğ”¼â„™(
Zt
)
= 1 + âˆ«
t
0
[
ğœ‚(eu1 âˆ’1) + 1
2u2
2
]
ğ”¼â„™(
Zs
)
ds.
By differentiating the above equation with respect to t,
d
dtğ”¼â„™(
Zt
)
=
[
ğœ‚(eu1 âˆ’1) + 1
2u2
2
]
ğ”¼â„™(
Zt
)
or
dmt
dt âˆ’
[
ğœ‚(eu1 âˆ’1) + 1
2u2
2
]
mt = 0
where mt = ğ”¼â„™(
Zt
)
.

306
5.2.3
Girsanovâ€™s Theorem for Jump Processes
By setting the integrating factor to be I = eâˆ’âˆ«(ğœ‚(eu1âˆ’1)+ 1
2 u2
2) dt = eâˆ’(ğœ‚(eu1âˆ’1)+ 1
2 u2
2)t and mul-
tiplying it by the first-order ordinary differential equation, we have
d
dt
(
mteâˆ’ğœ‚t(eu1âˆ’1)âˆ’1
2 u2
2t)
= 0
or
eâˆ’ğœ‚t(eu1âˆ’1)âˆ’1
2 u2
2tğ”¼â„™(Zt) = C
where C is a constant. Since ğ”¼â„™(
Z0
)
= ğ”¼â„™(
eu1N0Z(1)
0
â‹…eu2 ÌƒW0Z(2)
0
)
= 1, therefore C = 1.
Thus, we will have
ğ”¼â„š(
eu1Nt+u2 ÌƒWt
)
= ğ”¼â„™(
Zt
)
= eğœ‚t(eu1âˆ’1)+ 1
2 u2
2t.
Since the joint moment generating function of
ğ”¼â„š(
eu1Nt+u2 ÌƒWt
)
= eğœ‚t(eu1âˆ’1) â‹…e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Nt âˆ¼Poisson(ğœ‚t)
and ÌƒWt âˆ¼ğ’©(0, t), respectively, we can deduce that Nt and ÌƒWt are independent.
â—½
6. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, t â‰¥0. Let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function â„™(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and âˆ‘K
k=1 â„™(X = xk) = 1. Let X1, X2, . . .
also be independent of Nt. From the definition of a compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
show that Mt and Nt can be expressed as
Mt =
K
âˆ‘
k=1
xkN(k)
t ,
Nt =
K
âˆ‘
k=1
N(k)
t
where N(k)
t
âˆ¼Poisson(ğœ†tp(xk)) such that N(i)
t
âŸ‚âŸ‚N(j)
t , i â‰ j.
Let ğœ‚1, ğœ‚2, . . . , ğœ‚K > 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt =
K
âˆ
k=1
Z(k)
t
where Z(k)
t
= e(ğœ†kâˆ’ğœ‚k)t
( ğœ‚k
ğœ†k
)N(k)
t
such that ğœ†k = ğœ†p(xk), k = 1, 2, . . . , K.
Show that ğ”¼â„™(Zt) = 1 and Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
Solution: The first part of the result follows from Problem 5.2.1.16 (page 268).

5.2.3
Girsanovâ€™s Theorem for Jump Processes
307
From Problem 5.2.3.1 (page 298), for each k = 1, 2, . . . , K, Z(k)
t
satisfies
dZ(k)
t
Z(k)
tâˆ’
=
(ğœ‚k âˆ’ğœ†k
ğœ†k
)
dÌ‚N(k)
t
where Ì‚N(k)
t
= N(k)
t
âˆ’ğœ†kt, ğœ†k = ğœ†tp(xk), therefore we can easily show that Ì‚N(k)
t
= N(k)
t
âˆ’
ğœ†kt is a â„™-martingale. Thus,
(ğœ‚k âˆ’ğœ†k
ğœ†k
)
Ì‚N(k)
t
is also a â„™-martingale and subsequently,
ğ”¼â„™(
Z(k)
t
)
= 1 and Z(k)
t
is a â„™-martingale for 0 â‰¤t â‰¤T.
For the case
Zt =
K
âˆ
i=1
Z(k)
t
and because the Poisson processes N(i)
t
and N(j)
t , i â‰ j have no simultaneous jumps, we can
deduce that Z(i)
t
âŸ‚âŸ‚Z(j)
t , i â‰ j. Therefore,
ğ”¼â„™(Zt
) = ğ”¼â„™
( K
âˆ
k=1
Z(k)
t
)
=
K
âˆ
k=1
ğ”¼â„™(
Z(k)
t
)
=
K
âˆ
k=1
1 = 1
for 0 â‰¤t â‰¤T.
Finally, to show that Zt is a positive â„™-martingale, 0 â‰¤t â‰¤T we prove this result via math-
ematical induction. Let K = 1, then obviously Zt is a positive â„™-martingale. Assume that
Zt is a positive â„™-martingale for K = m, m â‰¥1. For K = m + 1 and because âˆm
k=1 Z(k)
t
and
Z(m+1)
t
have no simultaneous jumps, by ItÂ¯oâ€™s lemma
d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
)
= Z(m+1)
tâˆ’
d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
+
(
Z(1)
tâˆ’Z(2)
tâˆ’. . . Z(m)
tâˆ’
)
dZ(m+1)
t
+ d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
dZ(m+1)
t
where Z(k)
tâˆ’is the value of Z(k)
t
before a jump event.
Since N(i)
t
âŸ‚âŸ‚N(j)
t , i â‰ j therefore d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
dZ(m+1)
t
= 0, and by taking integrals
for s < t,
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
= Z(1)
s Z(2)
s
. . . Z(m)
s
Z(m+1)
s
+ âˆ«
t
s
Z(m+1)
uâˆ’
d
(
Z(1)
u Z(2)
u
. . . Z(m)
u
)
+ âˆ«
t
s
(
Z(1)
uâˆ’Z(2)
uâˆ’. . . Z(m)
uâˆ’
)
dZ(m+1)
u
.
Because
mâˆ
k=1
Z(k)
t
and Z(m+1)
t
are positive â„™-martingales and the integrands are
left-continuous, taking expectations with respect to the filtration â„±s, s < t
ğ”¼â„™(
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
||| â„±s
)
= Z(1)
s Z(2)
s
. . . Z(m)
s
Z(m+1)
s
.

308
5.2.3
Girsanovâ€™s Theorem for Jump Processes
In addition, since Z(i)
t
> 0, i = 1, 2, . . . , m + 1 for all t â‰¥0, therefore
||||||
m+1
âˆ
k=1
Z(k)
t
||||||
=
m+1
âˆ
k=1
Z(k)
t
and hence for t â‰¥0,
ğ”¼â„™
(||||||
m+1
âˆ
k=1
Z(k)
t
||||||
)
= ğ”¼â„™
(m+1
âˆ
k=1
Z(k)
t
)
= 1 < âˆ.
Finally, because
m+1
âˆ
k=1
Z(k)
t
is â„±t-adapted, the process
m+1
âˆ
k=1
Z(k)
t
is a positive â„™-martingale.
Thus, from mathematical induction, Zt =
Kâˆ
k=1
Z(k)
t
is a positive â„™-martingale for
0 â‰¤t â‰¤T.
â—½
7. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function â„™(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and âˆ‘K
k=1 â„™(X = xk) = 1. Let X1, X2, . . .
also be independent of Nt. From the definition of a compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
t â‰¥0
we can set Mt and Nt as
Mt =
K
âˆ‘
k=1
xkN(k)
t ,
Nt =
K
âˆ‘
k=1
N(k)
t
where N(k)
t
âˆ¼Poisson (ğœ†tp (xk
)) is the number of jumps in Mt of size xk up to and includ-
ing time t such that N(i)
t
âŸ‚âŸ‚N(j)
t , i â‰ j.
Let ğœ‚1, ğœ‚2, . . . , ğœ‚K > 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt =
K
âˆ
k=1
Z(k)
t
where Z(k)
t
= e(ğœ†kâˆ’ğœ‚k)t
( ğœ‚k
ğœ†k
)N(k)
t
such that ğœ†k = ğœ†p(xk), k = 1, 2, . . . , K. By changing the
measure â„™to a measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt

5.2.3
Girsanovâ€™s Theorem for Jump Processes
309
show that under the â„šmeasure, Mt is a compound Poisson process with intensity ğœ‚=
âˆ‘K
k=1 ğœ‚k > 0 and X1, X2, . . . is a sequence of independent and identically distributed ran-
dom variables with common probability mass function
â„š(X = xk) = q(xk) =
ğœ‚k
ğœ‚1 + ğœ‚2 + . . . + ğœ‚K
such that
Kâˆ‘
k=1
â„š(X = xk) = 1.
Solution: From the independence of N(1)
t , N(2)
t , . . . , N(K)
t
under â„™, for u âˆˆâ„and using
the moment generating function approach
ğ”¼â„š(euMt) = ğ”¼â„™
(
euMt dâ„š
dâ„™
||||â„±t
)
= ğ”¼â„™
â¡
â¢
â¢â£
eu âˆ‘K
k=1 xkN(k)
t
â‹…
K
âˆ
k=1
e(ğœ†kâˆ’ğœ‚k)
( ğœ‚k
ğœ†k
)N(k)
t â¤
â¥
â¥â¦
=
K
âˆ
k=1
e(ğœ†kâˆ’ğœ‚k)tğ”¼â„™
[
e
(
uxk+log( ğœ‚k
ğœ†k )
)
N(k)
t
]
.
From Problem 5.2.1.14 (page 266), for a constant m > 0, ğ”¼â„™(
emN(k)
t
)
= eğœ†tp(xk)(emâˆ’1)
therefore
ğ”¼â„š(euMt) =
K
âˆ
k=1
e(ğœ†kâˆ’ğœ‚k)t â‹…eğœ†tp(xk)(e
uxk+log( ğœ‚k
ğœ†k
)
âˆ’1)
=
K
âˆ
k=1
e(ğœ†kâˆ’ğœ‚k)t â‹…e
ğœ†kt
( ğœ‚t
k
ğœ†k euxk âˆ’1
)
=
K
âˆ
k=1
eğœ‚kt(euxk âˆ’1)
= e
ğœ‚t
(âˆ‘K
k=1 q(xk)euxk âˆ’1
)
which is the moment generating function for a compound Poisson process with intensity
ğœ‚> 0 and jump size distribution â„š(Xi = xk) = q(xk) = ğœ‚k
ğœ‚, i = 1, 2, . . .
â—½
8. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent
of Nt, and define the compound Poisson process as
Mt =
Nt
âˆ‘
i=1
Xi,
0 â‰¤t â‰¤T.

310
5.2.3
Girsanovâ€™s Theorem for Jump Processes
By changing the measure â„™to a measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
show that for ğœ‚> 0 the Radonâ€“NikodÂ´ym derivative process Zt can be written as
Zt =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where â„™(X) (f â„™(X)) and â„š(X) (f â„š(X)) are the probability mass (density) functions of â„™
and â„š, respectively. Note that for the case of continuous variables Xi, i = 1, 2, . . . , we
assume â„šis absolutely continuous with respect to â„™on â„±.
Solution: We first consider the discrete case where we let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function â„™(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and âˆ‘K
k=1 â„™(X = xk) = 1.
From Problem 5.2.3.7 (page 308), the compound Poisson process Mt and the Poisson
process Nt can be expressed as
Mt =
K
âˆ‘
k=1
xkN(k)
t ,
Nt =
K
âˆ‘
k=1
N(k)
t
where N(k)
t
âˆ¼Poisson (ğœ†tp (xk
)) such that N(i)
t
âŸ‚âŸ‚N(j)
t , i â‰ j. By setting ğœ‚1, ğœ‚2, . . . , ğœ‚K > 0,
we consider the Radonâ€“NikodÂ´ym derivative process
Zt =
K
âˆ
k=1
Z(k)
t
where Z(k)
t
= e(ğœ†kâˆ’ğœ‚k)t
( ğœ‚k
ğœ†k
)N(k)
t
and ğœ†k = ğœ†p(xk) for k = 1, 2, . . . , K. Thus, under the
â„šmeasure, Mt is a compound Poisson process with intensity ğœ‚= âˆ‘K
k=1 ğœ‚k > 0 and
X1, X2, . . . is a sequence of independent and identically distributed random variables
with common probability mass function
â„š(X = xk) = q(xk) =
ğœ‚k
ğœ‚1 + ğœ‚2 + . . . + ğœ‚K
.
Therefore,
Zt =
K
âˆ
k=1
Z(k)
t
=
K
âˆ
k=1
e(ğœ†kâˆ’ğœ‚k)
( ğœ‚k
ğœ†k
)N(k)
t

5.2.3
Girsanovâ€™s Theorem for Jump Processes
311
= e
âˆ‘K
k=1(ğœ†kâˆ’ğœ‚k)t
K
âˆ
k=1
( ğœ‚k
ğœ†k
)N(k)
t
= e(ğœ†âˆ’ğœ‚)t
K
âˆ
k=1
(ğœ‚q(xk)
ğœ†p(xk)
)N(k)
t
= e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi).
By analogy with the discrete case, we can deduce that when Xi, i = 1, 2, . . . are continuous
random variables the Radonâ€“NikodÂ´ym derivative process can also be written as
Zt = e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi),
0 â‰¤t â‰¤T
where f â„™(X) and f â„š(X) are the probability density functions of â„™and â„šmeasures, respec-
tively, and â„šis absolutely continuous with respect to â„™on â„±.
â—½
9. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T and let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent of
Nt. Under the â„™measure, each Xi, i = 1, 2, . . . has a probability mass (density) function
â„™(Xi) (f â„™(Xi)). From the definition of the compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
0 â‰¤t â‰¤T
we let ğœ‚> 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the â„šmeasure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let â„š
be absolutely continuous with respect to â„™on â„±. By changing the measure â„™to measure
â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
show that under the â„šmeasure for 0 â‰¤t â‰¤T, Mt is a compound Poisson process
with intensity ğœ‚> 0 and Xi, i = 1, 2, . . . is a sequence of independent and identically

312
5.2.3
Girsanovâ€™s Theorem for Jump Processes
distributed random variables with probability mass (density) functions â„š(Xi) (f â„š(Xi)),
i = 1, 2, . . .
Solution: To solve this problem we need to show
ğ”¼â„š(euMt) = eğœ‚t(ğœ‘â„š
X (u)âˆ’1),
u âˆˆâ„
which is the moment generating function of a compound Poisson process with intensity ğœ‚
such that
ğœ‘â„š
X (u) = ğ”¼â„š(euX) =
â§
âª
âª
â¨
âª
âªâ©
Nt
âˆ‘
i=1
euXiâ„š(Xi)
if X is discrete
âˆ«
âˆ
âˆ’âˆ
euxf â„š(x) dx
if X is continuous.
Without loss of generality we consider only the continuous case where, by definition,
ğ”¼â„š(euMt) = ğ”¼â„™
(
euMt dâ„š
dâ„™
||||â„±t
)
= ğ”¼â„™(euMtZt
)
= ğ”¼â„™
(
eu âˆ‘Nt
i=1 Xie(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
)
= e(ğœ†âˆ’ğœ‚)tğ”¼â„™
[
ğ”¼â„™
(
eu âˆ‘Nt
i=1 Xi
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
||||||
Nt
)]
= e(ğœ†âˆ’ğœ‚)t
âˆ
âˆ‘
n=0
ğ”¼â„™
(
eu âˆ‘Nt
i=1 Xi
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
)
â„™(Nt = n).
Since X1, X2, . . . are independent and identically distributed random variables and
because Nt âˆ¼Poisson(ğœ†t), we can express
ğ”¼â„š(euMt) = e(ğœ†âˆ’ğœ‚)t
âˆ
âˆ‘
n=0
ğ”¼â„™
( Nt
âˆ
i=1
(ğœ‚
ğœ†
)
euXi f â„š(Xi)
f â„™(Xi)
)
â„™(Nt = n)
= e(ğœ†âˆ’ğœ‚)t
âˆ
âˆ‘
n=0
[
ğ”¼â„™
(ğœ‚
ğœ†euX f â„š(X)
f â„™(X)
)]n eâˆ’ğœ†t(ğœ†t)n
n!
.
Because
ğ”¼â„™
(ğœ‚
ğœ†euX f â„š(X)
f â„™(X)
)
= âˆ«
âˆ
âˆ’âˆ
ğœ‚
ğœ†eux f â„š(x)
f â„™(x) â‹…f â„™(x) dx
= âˆ«
âˆ
âˆ’âˆ
ğœ‚
ğœ†euxf â„š(x) dx
= ğœ‚
ğœ†ğœ‘â„š
X (u)

5.2.3
Girsanovâ€™s Theorem for Jump Processes
313
where ğœ‘â„š
X (u) = âˆ«
âˆ
âˆ’âˆ
euxf â„š(x) dx, then
ğ”¼â„š(euMt) = e(ğœ†âˆ’ğœ‚)t
âˆ
âˆ‘
n=0
(ğœ‚
ğœ†ğœ‘â„š
X (u)
)n eâˆ’ğœ†t(ğœ†t)n
n!
= eâˆ’ğœ‚t
âˆ
âˆ‘
n=0
(ğœ‚tğœ‘â„š
X (u))n
n!
= eğœ‚t(ğœ‘â„š
X (u)âˆ’1).
Based on the moment generating function we can deduce that under the â„šmeasure, Mt
is a compound Poisson process with intensity ğœ‚> 0 and X1, X2, . . . are also indepen-
dent and identically distributed random variables with probability density function f â„š(Xi),
i = 1, 2, . . .
â—½
10. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 defined on the probability
space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T and let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent
of Nt. Under the â„™measure each Xi, i = 1, 2, . . . has a probability mass (density) function
â„™(Xi) (f â„™(Xi)). From the definition of the compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
0 â‰¤t â‰¤T
we let ğœ‚> 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the â„šmeasure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let
â„šbe absolutely continuous with respect to â„™on â„±.
Show that ğ”¼â„™(Zt
) = 1 and Zt is a positive â„™martingale for 0 â‰¤t â‰¤T.
Solution: Without loss of generality we consider only the case of continuous random
variables Xi, i = 1, 2, . . .
Let Zt = e(ğœ†âˆ’ğœ‚)tGt, where Gt =
Ntâˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi) is defined as a pure jump process, and let Ztâˆ’
denote the value of Zt before a jump event.

314
5.2.3
Girsanovâ€™s Theorem for Jump Processes
From ItÂ¯oâ€™s lemma,
dZt = ğœ•Zt
ğœ•t dt + ğœ•Zt
ğœ•Gt
dGt + 1
2
ğœ•2Zt
ğœ•G2
t
(dGt)2 + . . .
= (ğœ†âˆ’ğœ‚)e(ğœ†âˆ’ğœ‚)tGt dt + e(ğœ†âˆ’ğœ‚)tdGt
= (ğœ†âˆ’ğœ‚)Ztâˆ’+ e(ğœ†âˆ’ğœ‚)tdGt.
Let Gtâˆ’be the value Gt just before a jump event and assume there occurs an instantaneous
jump at time t. Therefore,
dGt = Gt âˆ’Gtâˆ’
= ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)Gtâˆ’âˆ’Gtâˆ’
=
(ğœ‚f â„š(Xt)
ğœ†f â„™(Xt) âˆ’1
)
Gtâˆ’
where ğœ‚f â„š(Xt)
ğœ†f â„™(Xt) is the size of the jump variable if N jumps at time t.
By defining a new compound Poisson process
Ht =
Nt
âˆ‘
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
we let Htâˆ’be the value of Ht just before a jump. At jump time t (which is also the jump
time of Mt and Gt), we have
dHt = Ht âˆ’Htâˆ’= ğœ‚f â„š(Xt)
ğœ†f â„™(Xt).
Therefore, at jump times of N we can write
dGt
Gtâˆ’= dHt âˆ’1
and in general we can write
dGt
Gtâˆ’= dHt âˆ’dNt
where
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt
and
dHt = ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)dNt.

5.2.3
Girsanovâ€™s Theorem for Jump Processes
315
Thus,
dZt = (ğœ†âˆ’ğœ‚)Ztâˆ’dt + e(ğœ†âˆ’ğœ‚)t(dHt âˆ’dNt)Gtâˆ’
= (ğœ†âˆ’ğœ‚)Ztâˆ’dt + Ztâˆ’dHt âˆ’Ztâˆ’dNt
or
dZt
Ztâˆ’= d(Ht âˆ’ğœ‚t) âˆ’d(Nt âˆ’ğœ†t).
Since Ì‚Nt = Nt âˆ’ğœ†t is a â„™-martingale we now need to show that Ì‚Ht = Ht âˆ’ğœ‚t is also a
â„™-martingale. By definition,
ğ”¼â„™
[ğœ‚f â„š(X)
ğœ†f â„™(X)
]
= ğœ‚
ğœ†âˆ«
âˆ
âˆ’âˆ
f â„š(x)
f â„™(x) â‹…f â„™(x) dx = ğœ‚
ğœ†
since âˆ«
âˆ
âˆ’âˆ
f â„š(x) dx = 1. From Problem 5.2.1.14 (page 266), we can therefore deduce that
Ì‚Ht = Ht âˆ’ğ”¼â„™
[ğœ‚f â„š(X)
ğœ†f â„™(X)
]
ğœ†t = Ht âˆ’ğœ‚t
is a â„™-martingale as well.
Substituting Ì‚Nt = Nt âˆ’ğœ†t and Ì‚Ht = Ht âˆ’ğœ‚t into the stochastic differential equation, we
have
dZt
Ztâˆ’= dÌ‚Ht âˆ’dÌ‚Nt
and taking integrals, for t â‰¥0
âˆ«
t
0
dZu = âˆ«
t
0
Zuâˆ’dÌ‚Hu âˆ’âˆ«
t
0
Zuâˆ’dÌ‚Nu
Zt = Z0 + âˆ«
t
0
Zuâˆ’dÌ‚Hu âˆ’âˆ«
t
0
Zuâˆ’dÌ‚Nu.
Taking expectations,
ğ”¼â„™(Zt
) = ğ”¼â„™(Z0
) + ğ”¼â„™
[
âˆ«
t
0
Zuâˆ’dÌ‚Hu
]
âˆ’ğ”¼â„™
[
âˆ«
t
0
Zuâˆ’dÌ‚Nu
]
= 1
since Z0 = 1, Ì‚Ht and Ì‚Nt are both â„™-martingales.
To show that Zt is a positive â„™-martingale, by taking integrals for s < t
âˆ«
t
s
dZu = âˆ«
t
s
Zuâˆ’dÌ‚Hu âˆ’âˆ«
t
s
Zuâˆ’dÌ‚Nu
Zt âˆ’Zs = âˆ«
t
s
Zuâˆ’dÌ‚Hu âˆ’âˆ«
t
s
Zuâˆ’dÌ‚Nu

316
5.2.3
Girsanovâ€™s Theorem for Jump Processes
and taking expectations under the filtration â„±s,
ğ”¼â„™[Zt âˆ’Zs|â„±s
] = ğ”¼â„™
[
âˆ«
t
s
Zuâˆ’dÌ‚Hu
|||||
â„±s
]
âˆ’ğ”¼â„™
[
âˆ«
t
s
Zuâˆ’dÌ‚Nu
|||||
â„±s
]
.
Because Ì‚Ht and Ì‚Nt are â„™-martingales, therefore
ğ”¼â„™[Zt|â„±s
] = Zs.
In addition, since Zt > 0 for 0 â‰¤t â‰¤T, then |Zt| = Zt and hence ğ”¼â„™(|Zt|) = ğ”¼â„™(Zt
) =
1 < âˆfor 0 â‰¤t â‰¤T. Because Zt is also â„±t-adapted, we can conclude that Zt is a positive
â„™-martingale for 0 â‰¤t â‰¤T.
â—½
11. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 and {Wt âˆ¶0 â‰¤t â‰¤T}
be a standard Wiener process defined on the probability space (Î©, â„±, â„™) with respect
to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T and ğœ‚> 0.
Let X1, X2, . . . be a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function â„™(Xi) (f â„™(Xi)) under
the â„™measure. Let X1, X2, . . . also be independent of Nt and Wt.
From the definition of the compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
0 â‰¤t â‰¤T
we consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
=
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the â„šmeasure and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
u du)
< âˆ. For the case of continuous random variables Xi,
i = 1, 2, . . . we also let â„šbe absolutely continuous with respect to â„™on â„±.
Show that ğ”¼â„™(Zt
) = 1 and Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.

5.2.3
Girsanovâ€™s Theorem for Jump Processes
317
Solution: From Problem 4.2.2.1 (page 194) and Problem 5.2.3.10 (page 313), we have
shown that ğ”¼â„™(
Z(1)
t
)
= 1, ğ”¼â„™(
Z(2)
t
)
= 1, Z(1)
t
and Z(2)
t
are both â„™-martingales. From ItÂ¯oâ€™s
lemma,
d(Z(1)
t Z(2)
t ) = Z(1)
tâˆ’dZ(2)
t
+ Z(2)
t d + dZ(1)
t dZ(2)
t
where Ztâˆ’is the value of Z(1)
t
before a jump event. Since dZ(1)
t dZ(2)
t
= 0, by taking
integrals,
âˆ«
t
0
d(Z(1)
u Z(2)
u ) = âˆ«
t
0
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
0
Z(2)
u dZ(1)
u
Z(1)
t Z(2)
t
= Z(1)
0 Z(2)
0
+ âˆ«
t
0
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
0
Z(2)
u dZ(1)
u .
Taking expectations under the â„™measure,
ğ”¼â„™(
Z(1)
t Z(2)
t
)
= ğ”¼â„™(
Z(1)
0 Z(2)
0
)
= 1
since Z(1)
0
= 1, Z(2)
0
= 1 and both Z(1)
t
and Z(2)
t
are â„™-martingales.
To show that Zt = Z(1)
t
â‹…Z(2)
t
is a positive â„™-martingale, by taking integrals for s < t of
d(Z(1)
t Z(2)
t ) = Z(1)
tâˆ’dZ(2)
t
+ Z(2)
t dZ(1)
t
we have
Z(1)
t Z(2)
t
= Z(1)
s Z(2)
s
+ âˆ«
t
s
Z(1)
uâˆ’dZ(2)
u
+ âˆ«
t
s
Z(2)
u dZ(1)
u
and taking expectations with respect to the filtration â„±s, s < t
ğ”¼â„™(
Z(1)
t Z(2)
t
||| â„±s
)
= Z(1)
s Z(2)
s
since ğ”¼â„™
(
âˆ«
t
s
Z(1)
uâˆ’dZ(2)
u
|||||
â„±s
)
= ğ”¼â„™
(
âˆ«
t
s
Z(2)
u dZ(1)
u
|||||
â„±s
)
= 0.
In addition, since Z(1)
t
> 0 and Z(2)
t
> 0 for all 0 â‰¤t â‰¤T, therefore |||Z(1)
t Z(2)
t
||| = Z(1)
t Z(2)
t
and
hence ğ”¼â„™(|Zt|) = ğ”¼â„™(Zt
) = 1 < âˆfor 0 â‰¤t â‰¤T. Finally, because Zt is also â„±t-adapted,
we can conclude that Zt is a positive â„™-martingale for 0 â‰¤t â‰¤T.
â—½
12. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity ğœ†> 0 and {Wt âˆ¶0 â‰¤t â‰¤T}
be a standard Wiener process defined on the probability space (Î©, â„±, â„™) with respect
to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose ğœƒt is an adapted process, 0 â‰¤t â‰¤T and ğœ‚> 0.
Let X1, X2, . . . be a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function â„™(Xi) (f â„™(Xi)) under
the â„™measure. Let X1, X2, . . . also be independent of Nt and Wt.
From the definition of the compound Poisson process
Mt =
Nt
âˆ‘
i=1
Xi,
0 â‰¤t â‰¤T

318
5.2.3
Girsanovâ€™s Theorem for Jump Processes
we consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
=
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒudWuâˆ’1
2 âˆ«t
0 ğœƒ2
u du
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the â„šmeasure and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2
u du)
< âˆ. For the case of continuous random variables Xi,
i = 1, 2, . . . we also let â„šbe absolutely continuous with respect to â„™on â„±.
By changing the measure â„™to measure â„šsuch that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
show that under the â„šmeasure, the process Mt =
Ntâˆ‘
i=1
Xi is a compound Poisson process
with intensity ğœ‚> 0 where Xi, i = 1, 2, . . . is a sequence of independent and identically
distributed random variables with probability mass (density) functions â„š(Xi) (f â„š(Xi)), i =
1, 2, . . . , the process ÌƒWt = Wt + âˆ«
t
0
ğœƒu du is a â„š-standard Wiener process and Mt âŸ‚âŸ‚ÌƒWt.
Solution: The first two results are given in Problem 5.2.3.9 (page 311) and Problem
4.2.2.10 (page 205).
To prove the final result we need to show that, under the â„šmeasure,
ğ”¼â„š(
eu1Mt+u2 ÌƒWt
)
= eğœ‚t(ğœ‘â„š
X (u1)âˆ’1) â‹…e
1
2 u2
2t
which is a joint product of independent moment generating functions of Mt and ÌƒWt âˆ¼
ğ’©(0, t).
By definition,
ğ”¼â„š(
eu1Mt+u2 ÌƒWt
)
= ğ”¼â„™(
eu1Mt+u2 ÌƒWtZt
)
= ğ”¼â„™(
eu1MtZ(1)
t
â‹…eu2 ÌƒWtZ(2)
t
)
and we let
Zt = Z
(1)
t
â‹…Z
(2)
t
where
Z
(1)
t
= eu1MtZ(1)
t
and
Z
(2)
t
= eu2 ÌƒWtZ(2)
t .

5.2.3
Girsanovâ€™s Theorem for Jump Processes
319
By setting Z
(1)
tâˆ’to denote the value of Z
(1)
t
before a jump event and using ItÂ¯oâ€™s lemma, we
have
dZt = d(Z
(1)
t Z
(2)
t ) = Z
(1)
tâˆ’dZ
(2)
t
+ Z
(2)
t dZ
(1)
t
+ dZ
(1)
t dZ
(2)
t .
For dZ
(1)
t
we can expand, using Taylorâ€™s theorem,
dZ
(1)
t
= ğœ•Z
(1)
t
ğœ•Mt
dMt + ğœ•Z
(1)
t
ğœ•Z(1)
t
dZ(1)
t
+ 1
2!
â¡
â¢
â¢â£
ğœ•2Z
(1)
t
ğœ•M2
t
(dMt)2 + 2 ğœ•2Z
(1)
t
ğœ•Mtğœ•Z(1)
t
(dMtdZ(1)
t ) + ğœ•2Z
(1)
t
ğœ•(Z(1)
t )2 (dZ(1)
t )2
â¤
â¥
â¥â¦
+ 1
3!
â¡
â¢
â¢â£
ğœ•3Z
(1)
t
ğœ•M3
t
(dMt)3 + 3
ğœ•3Z
(1)
t
ğœ•M2
t ğœ•Z(1)
t
(dMt)2(dZ(1)
t )
+3
ğœ•3Z
(1)
t
ğœ•Mtğœ•(Z(1)
t )2 (dMt)(dZ(1)
t )2 + ğœ•3Z
(1)
t
ğœ•(Z(1)
t )3 (dZ(1)
t )3
â¤
â¥
â¥â¦
+ . . .
Because dMt = XtdNt, dZ(1)
t
= Z(1)
tâˆ’(dÌ‚Ht âˆ’dÌ‚Nt) where
dÌ‚Ht =
â§
âª
âª
â¨
âª
âªâ©
ğœ‚â„š(Xt)
ğœ†â„™(Xt)dNt âˆ’ğœ‚dt
if Xt is discrete, â„™(Xt) > 0
ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)dNt âˆ’ğœ‚dt
if Xt is continuous
and dÌ‚Nt = Nt âˆ’ğœ†dt (see Problem 5.2.3.10, page 313), we can write
dZ
(1)
t
= (u1Xt)Z
(1)
tâˆ’dNt + Z
(1)
tâˆ’(dÌ‚Ht âˆ’dÌ‚Nt) + 1
2!
[
(u1Xt)2Z
(1)
tâˆ’dNt + 2u1XtZ
(1)
tâˆ’dNt(dÌ‚Ht âˆ’dÌ‚Nt)
]
+ 1
3!
[
(u1Xt)3Z
(1)
tâˆ’dNt + 3(u1Xt)2Z
(1)
tâˆ’dNt(dÌ‚Ht âˆ’dÌ‚Nt)
]
+ . . .
=
[
(u1Xt) + 1
2!(u1Xt)2 + 1
3!(u1Xt)3 + . . .
]
Z
(1)
tâˆ’dNt + Z
(1)
tâˆ’(dÌ‚Ht âˆ’dÌ‚Nt)
+
[
(u1Xt) + 1
2!(u1Xt)2 + 1
3!(u1Xt)3 + . . .
]
Z
(1)
tâˆ’dNt(dÌ‚Ht âˆ’dÌ‚Nt)
= (eu1Xt âˆ’1)Z
(1)
tâˆ’dNt + Z
(1)
tâˆ’(dÌ‚Ht âˆ’dÌ‚Nt) + (eu1Xt âˆ’1)Z
(1)
tâˆ’dNt(dÌ‚Ht âˆ’dÌ‚Nt).
Since
dÌ‚Ht âˆ’dÌ‚Nt =
â§
âª
âª
â¨
âª
âªâ©
[ğœ‚â„š(Xt)
ğœ†â„™(Xt) âˆ’1
]
dNt âˆ’(ğœ‚âˆ’ğœ†) dt
if Xt is discrete, â„™(Xt) > 0
[ğœ‚f â„š(Xt)
ğœ†f â„™(Xt) âˆ’1
]
dNt âˆ’(ğœ‚âˆ’ğœ†) dt
if Xt is continuous

320
5.2.3
Girsanovâ€™s Theorem for Jump Processes
and
dNt(dÌ‚Ht âˆ’dÌ‚Nt) =
â§
âª
âª
â¨
âª
âªâ©
[ğœ‚â„š(Xt)
ğœ†â„™(Xt) âˆ’1
]
dNt
if Xt is discrete, â„™(Xt) > 0
[ğœ‚f â„š(Xt)
ğœ†f â„™(Xt) âˆ’1
]
dNt
if Xt is continuous
therefore
dZ
(1)
t
=
â§
âª
âª
â¨
âª
âªâ©
ğœ‚â„š(Xt)
ğœ†â„™(Xt)
(eu1Xt âˆ’1) Z
(1)
tâˆ’dNt + Z
(1)
tâˆ’
(
dÌ‚Ht âˆ’dÌ‚Nt
)
if Xt is discrete, â„™(Xt) > 0
ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)
(eu1Xt âˆ’1) Z
(1)
tâˆ’dNt + Z
(1)
tâˆ’
(
dÌ‚Ht âˆ’dÌ‚Nt
)
if Xt is continuous.
For the case of dZ
(2)
t , by applying Taylorâ€™s theorem
dZ
(2)
t
= ğœ•Z
(2)
t
ğœ•ÌƒWt
d ÌƒWt + ğœ•Z
(2)
t
ğœ•Z(2)
t
dZ(2)
t
+ 1
2!
â¡
â¢
â¢â£
ğœ•2Z
(2)
t
ğœ•( ÌƒWt)2 (d ÌƒWt)2 + 2 ğœ•2Z
(2)
t
ğœ•ÌƒWtğœ•Z(2)
t
(d ÌƒWt)(dZ(2)
t ) + ğœ•2Z
(2)
t
ğœ•(Z(2)
t )2 (dZ(2)
t )2
â¤
â¥
â¥â¦
+ . . .
and since d ÌƒWt = dWt + ğœƒt dt and dZ(2)
t
= âˆ’ğœƒtZ(2)
t dWt (see Problem 4.2.2.2, page 196), we
can write
dZ
(2)
t
= 1
2u2
2Z
(2)
t
dt + (u2 âˆ’ğœƒt)Z
(2)
t dWt.
Since dNtdWt = 0 we have
Z
(1)
t dZ
(2)
t
= 1
2u2
2Zt dt + (u2 âˆ’ğœƒt)ZtdWt,
Z
(2)
t dZ
(1)
t
=
â§
âª
âª
â¨
âª
âªâ©
ğœ‚â„š(Xt)
ğœ†â„™(Xt)
(eu1Xt âˆ’1) ZtdNt + Zt
(
dÌ‚Ht âˆ’dÌ‚Nt
)
if Xt is discrete, â„™(Xt) > 0
ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)
(eu1Xt âˆ’1) ZtdNt + Zt
(
dÌ‚Ht âˆ’dÌ‚Nt
)
if Xt is continuous
and
dZ
(1)
t dZ
(2)
t
= 0.
Without loss of generality, we let Xt be a continuous random variable and by letting dNt =
dÌ‚Nt + ğœ†dt we have
dZt = 1
2u2
2Zt dt + (u2 âˆ’ğœƒt)ZtdWt + ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)
(eu1Xt âˆ’1) ZtdNt + Zt
(
dÌ‚Ht âˆ’dÌ‚Nt
)

5.2.3
Girsanovâ€™s Theorem for Jump Processes
321
=
[ğœ‚f â„š(Xt)
f â„™(Xt)
(eu1Xt âˆ’1) + 1
2u2
2
]
Zt dt + (u2 âˆ’ğœƒt)ZtdWt
+ ğœ‚f â„š(Xt)
ğœ†f â„™(Xt)
(eu1Xt âˆ’1) ZtdÌ‚Nt + Zt(dÌ‚Ht âˆ’dÌ‚Nt).
Taking integrals,
Zt = 1 + âˆ«
t
0
[ğœ‚f â„š(Xs)
f â„™(Xs)
(eu1Xs âˆ’1) + 1
2u2
2
]
Zs ds + âˆ«
t
0
(u2 âˆ’ğœƒs)ZsdWs
+ âˆ«
t
0
ğœ‚f â„š(Xs)
ğœ†f â„™(Xs)
(eu1Xs âˆ’1) ZsdÌ‚Ns + âˆ«
t
0
+Zs
(
dÌ‚Hs âˆ’dÌ‚Ns
)
where Z0 = 1 and because the jump size variable Xt is independent of Nt and Wt, and Ì‚Nt,
Ì‚Ht and Wt are â„™-martingales, by taking expectations under the â„™measure we have
ğ”¼â„™(
Zt
)
= 1 + âˆ«
t
0
{
ğ”¼â„™
[ğœ‚f â„š(Xs)
f â„™(Xs)
(eu1Xs âˆ’1)]
+ 1
2u2
2
}
ğ”¼â„™(
Zs
)
ds.
By differentiating the integrals with respect to t,
d
dtğ”¼â„™(
Zt
)
=
{
ğ”¼â„™
[ğœ‚f â„š(Xt)
f â„™(Xt)
(eu1Xt âˆ’1)]
+ 1
2u2
2
}
ğ”¼â„™(
Zt
)
=
{
ğœ‚[ğ”¼â„š(eu1Xt) âˆ’1] + 1
2u2
2
}
ğ”¼â„™(
Zt
)
or
dmt
dt âˆ’
[
ğœ‚(ğœ‘â„š
X (u1) âˆ’1) + 1
2u2
2
]
mt = 0
where mt = ğ”¼â„™(
Zt
)
and ğœ‘â„š
X (u1) = ğ”¼â„š(eu1Xt).
By setting the integrating factor to be I = eâˆ’âˆ«(ğœ‚(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2) dt = eâˆ’(ğœ‚(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2)t and
multiplying the differential equation with I, we have
d
dt
(
mteâˆ’ğœ‚t(ğœ‘X(u1)âˆ’1)âˆ’1
2 u2
2t)
= 0
or
eâˆ’ğœ‚t(ğœ‘X(u1)âˆ’1)âˆ’1
2 u2
2tğ”¼â„™(
Zt
)
= C
where C is a constant. Since ğ”¼â„™(
Z0
)
= ğ”¼â„™(
eu1M0Z(1)
0
â‹…eu2 ÌƒW0Z(2)
0
)
= 1, therefore C = 1.
Thus, we finally obtain
ğ”¼â„š(
eu1Mt+u2 ÌƒWt
)
= ğ”¼â„™(
Zt
)
= eğœ‚t(ğœ‘X(u1)âˆ’1)+ 1
2 u2
2t.
Since the joint moment generating function of
ğ”¼â„š(
eu1Mt+u2 ÌƒWt
)
= eğœ‚t(ğœ‘X(u1)âˆ’1) â‹…e
1
2 u2
2t

322
5.2.4
Risk-Neutral Measure for Jump Processes
can be expressed as a product of the moment generating functions for Mt (which is a com-
pound Poisson process with intensity ğœ‚) and ÌƒWt âˆ¼ğ’©(0, t), respectively, we can deduce
that Mt and ÌƒWt are independent.
N.B. The same conclusion can also be obtained for the case of treating Xt as a discrete
random variable.
â—½
5.2.4
Risk-Neutral Measure for Jump Processes
1. Simple Jump Process. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶0 â‰¤t â‰¤T} be a
Poisson process with intensity ğœ†> 0 relative to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose the
asset price St follows a simple jump process
dSt
Stâˆ’= (J âˆ’1)dNt
where J is a constant jump amplitude if N jumps at time t and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Let r be the risk-free interest rate.
By considering the Radonâ€“NikodÂ´ym derivative process, for ğœ‚> 0
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt,
0 â‰¤t â‰¤T
show that by changing the measure â„™to the risk-neutral measure â„š, the above stochastic
differential equation is
dSt
Stâˆ’= (J âˆ’1)dNt
where Nt âˆ¼Poisson(ğœ‚t), ğœ‚= r(J âˆ’1)âˆ’1 > 0 provided J > 1.
Is the market arbitrage free and complete under the â„šmeasure?
Solution: Let ğœ‚> 0 and from the Radonâ€“NikodÂ´ym derivative process
Zt = e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt.
By changing the measure â„™to the risk-neutral measure â„šon the filtration â„±s, 0 â‰¤s â‰¤t
then under â„š, Nt âˆ¼Poisson(ğœ‚t) and the discounted asset price eâˆ’rtSt is a â„š-martingale.
By setting Stâˆ’to denote the value of St before a jump event and by expanding d(eâˆ’rtSt), we
have
d(eâˆ’rtSt) = âˆ’reâˆ’rtSt dt + eâˆ’rtdSt
= âˆ’reâˆ’rtSt dt + eâˆ’rtSt(J âˆ’1)dNt
= eâˆ’rtSt(âˆ’r + ğœ‚(J âˆ’1)) dt + eâˆ’rtSt(J âˆ’1)(dNt âˆ’ğœ‚dt).

5.2.4
Risk-Neutral Measure for Jump Processes
323
Since the compensated Poisson process Nt âˆ’ğœ‚t is a â„š-martingale (see Problem 5.2.1.9,
page 262), then, in order for eâˆ’rtSt to be a â„š-martingale, we can set
ğœ‚= r(J âˆ’1)âˆ’1.
Since ğœ‚> 0 we require J > 1. Thus, under the risk-neutral measure â„š,
dSt
Stâˆ’= (J âˆ’1)dNt
where Nt âˆ¼Poisson(ğœ‚t).
The market is arbitrage free since we can construct a risk-neutral measure â„šon the filtration
â„±s, 0 â‰¤s â‰¤t. Since â„šis unique, the market is also complete.
â—½
2. Pure Jump Process. Let (Î©, â„±, â„™) be a probability space and let {Nt âˆ¶0 â‰¤t â‰¤T} be a
Poisson process with intensity ğœ†> 0 relative to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose St
follows a pure jump process
dSt
Stâˆ’= (Jt âˆ’1)dNt
where Jt is the jump variable with mean ğ”¼â„™(Jt) = J if N jumps at time t and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Let r be the risk-free interest rate. Assume that Jt is independent of Nt and consider the
Radonâ€“NikodÂ´ym derivative process
Zt =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where ğœ‚> 0, Xi = Ji âˆ’1, â„™(X) (f â„™(X)) and â„š(X) (f â„š(X)) are the probability mass (density)
functions of â„™and â„š, respectively.
Show, by changing the measure â„™to the risk-neutral measure â„š, the above SDE can be
written as
dSt
Stâˆ’= dMt
where Mt = âˆ‘Nt
i=1(Ji âˆ’1) is a compound Poisson process with Ji, i = 1, 2, . . . a sequence of
independent and identically distributed jump amplitude random variables having intensity
ğœ‚=
r
ğ”¼â„š(Jt) âˆ’1 > 0
provided ğ”¼â„š(Jt) > 1.

324
5.2.4
Risk-Neutral Measure for Jump Processes
Is the market arbitrage free and complete under the â„šmeasure?
By assuming that the jump amplitude remains unchanged with the change of measure, find
the corresponding SDE under the equivalent martingale risk-neutral measure, â„šM.
Is the market arbitrage free and complete under the â„šM measure?
Solution: From Problem 5.2.2.1 (page 281) we can express the pure jump process as
dSt
Stâˆ’= dMt
where Mt = âˆ‘Nt
i=1(Ji âˆ’1) is a compound Poisson process with intensity ğœ†> 0.
Under the risk-neutral measure â„šon the filtration â„±s, 0 â‰¤s â‰¤t we let ğœ‚> 0 and by con-
sidering the Radonâ€“NikodÂ´ym derivative process
Zt =
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
where Xi = Ji âˆ’1, â„™(X) (f â„™(X)) and â„š(X) (f â„š(X)) are the probability mass (density) func-
tions of â„™and, â„šrespectively, then by changing the measure â„™to the risk-neutral measure
â„šwe have that, under â„š, Mt = âˆ‘Nt
i=1(Ji âˆ’1) is a compound Poisson process with intensity
ğœ‚> 0 and the discounted asset price eâˆ’rtSt is a â„š-martingale.
By letting Stâˆ’= St to denote the value of St before a jump event and expanding d(eâˆ’rtSt),
we have
d(eâˆ’rtSt) = âˆ’reâˆ’rtSt dt + eâˆ’rtdSt
= âˆ’reâˆ’rtSt dt + eâˆ’rtStdMt
= eâˆ’rtSt
(âˆ’r + ğœ‚ğ”¼â„š(Jt âˆ’1)) dt + eâˆ’rtSt
(dMt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1) dt) .
Given that the compensated compound Poisson process Mt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1)t is a
â„š-martingale (see Problem 5.2.1.14, page 266), and in order for eâˆ’rtSt to be a â„š-martingale,
we can set
ğœ‚=
r
ğ”¼â„š(Jt) âˆ’1
provided ğ”¼â„š(Jt) > 1.
Thus, under the risk-neutral measure â„šwe have
dSt
Stâˆ’= (Jt âˆ’1)dNt
where Nt âˆ¼Poisson(ğœ‚t), ğœ‚= r(ğ”¼â„š(Jt âˆ’1))âˆ’1 > 0.
The market is arbitrage free since we can construct a risk-neutral measure â„šon the filtration
â„±s, 0 â‰¤s â‰¤t. However, the market model is incomplete as â„šis not unique given that we
can vary the jump distribution Jt.

5.2.4
Risk-Neutral Measure for Jump Processes
325
By assuming that the distribution of the jump amplitude Jt remains unchanged under the
change of measure, there exists an equivalent martingale risk-neutral measure â„šM âˆ¼â„š
such that eâˆ’rtSt is a â„šM-martingale. Thus, under the equivalent martingale risk-neutral
measure â„šM,
ğ”¼â„šM (Jt âˆ’1) = ğ”¼â„™(Jt âˆ’1) = J âˆ’1
and the corresponding SDE is
dSt
Stâˆ’= (Jt âˆ’1)dNt
where Nt âˆ¼Poisson(ğœ‚t), ğœ‚= r(J âˆ’1)âˆ’1 provided J > 1.
The market model is arbitrage free since we can construct an equivalent martingale
risk-neutral measure â„šM on the filtration â„±s, 0 â‰¤s â‰¤t. However, the market is incomplete
as â„šM is not unique given that we can still vary the jump size distribution Jt and hence J.
â—½
3. Simple Jump Diffusion Process. Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson process with intensity
ğœ†> 0 and {Wt âˆ¶0 â‰¤t â‰¤T} be a standard Wiener process defined on the probability space
(Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose the stock price St follows a
jump diffusion process
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœdWt + (J âˆ’1)dNt
where Wt âŸ‚âŸ‚Nt, J is a constant jump amplitude if N jumps at time t and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
The constant parameters ğœ‡, D and ğœare the drift, continuous dividend yield and volatil-
ity, respectively. In addition, let Bt be the risk-free asset having the following differential
equation:
dBt = rBt dt
where r is the risk-free interest rate.
By considering the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
= e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒdWuâˆ’1
2 âˆ«t
0 ğœƒ2 du
where ğœƒâˆˆâ„, ğœ‚> 0 and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2 du)
< âˆshow that, under the risk-neutral measure
â„š, the SDE can be written as
dSt
Stâˆ’= (r âˆ’D âˆ’ğœ‚(J âˆ’1)) dt + ğœd ÌƒWt + (J âˆ’1)dNt

326
5.2.4
Risk-Neutral Measure for Jump Processes
where
ÌƒWt = Wt +
(ğœ‡âˆ’r + ğœ‚(J âˆ’1)
ğœ
)
t
is
the
â„š-standard
Wiener
process
and
Nt âˆ¼Poisson(ğœ‚t).
Is the market arbitrage free and complete under the â„šmeasure?
By assuming the jump component is uncorrelated with the market (i.e., the jump intensity
remains unchanged with the change of measure), find the corresponding SDE under the
equivalent martingale risk-neutral measure â„šM.
Is the market arbitrage free and complete under the â„šM measure?
Solution: Let ğœƒâˆˆâ„and ğœ‚> 0 and consider the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
= e(ğœ†âˆ’ğœ‚)t(ğœ‚
ğœ†
)Nt
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒdWuâˆ’1
2 âˆ«t
0 ğœƒ2 du
where ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2 du)
< âˆ. By changing the measure â„™to the risk-neutral measure â„šon
the filtration â„±s, 0 â‰¤s â‰¤t such that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then from Problem 5.2.3.5 (page 303), under the â„šmeasure, the Poisson process Nt âˆ¼
Poisson(ğœ‚t) has intensity ğœ‚> 0, the process ÌƒWt = Wt + âˆ«
t
0
ğœƒdu is a â„š-standard Wiener
process and Nt âŸ‚âŸ‚ÌƒWt.
In the presence of dividends, the strategy of holding a single stock is no longer self-financing
as it pays out dividends at a rate DSt dt. By letting Stâˆ’denote the value of St before a jump
event, at time t we let the portfolio Î t be valued as
Î t = ğœ™tSt + ğœ“tBt
where ğœ™t and ğœ“t are the units invested in St and the risk-free asset Bt, respectively. Given
that the holder will receive DSt dt for every stock held, then
dÎ t = ğœ™t(dSt + DSt dt) + ğœ“trBt dt
= ğœ™t
[ğœ‡St dt + ğœStdWt + (J âˆ’1)StdNt
] + ğœ“trBt dt
= rÎ t dt + ğœ™tSt
[(ğœ‡âˆ’r) dt + ğœdWt + (J âˆ’1)dNt
] .
By substituting Wt = ÌƒWt âˆ’âˆ«
t
0
ğœƒdu and taking note that the compensated Poisson process
Nt âˆ’ğœ‚t is a â„š-martingale, we have
dÎ t = rÎ t dt + ğœ™tSt
[
(ğœ‡âˆ’r âˆ’ğœğœƒ+ ğœ‚(J âˆ’1)) dt + ğœd ÌƒWt + (J âˆ’1)
(dNt âˆ’ğœ‚dt)]
.

5.2.4
Risk-Neutral Measure for Jump Processes
327
Since both ÌƒWt and Nt âˆ’ğœ‚t are â„š-martingales and in order for the discounted portfolio eâˆ’rtÎ t
to be a â„šmartingale
d(eâˆ’rtÎ t) = âˆ’reâˆ’rtÎ t dt + eâˆ’rtdÎ t
= eâˆ’rtğœ™tSt
[
(ğœ‡âˆ’r âˆ’ğœğœƒ+ ğœ‚(J âˆ’1)) dt + ğœd ÌƒWt + (J âˆ’1)
(dNt âˆ’ğœ‚dt)]
we set
ğœƒ= ğœ‡âˆ’r + ğœ‚(J âˆ’1)
ğœ
.
Therefore, the jump diffusion process under â„šis
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœ
(
d ÌƒWt âˆ’ğœƒdt
)
+ (J âˆ’1)dNt
= (r âˆ’D âˆ’ğœ‚(J âˆ’1)) dt + ğœd ÌƒWt + (J âˆ’1)dNt
where
ÌƒWt = Wt âˆ’
(ğœ‡âˆ’r + ğœ‚(J âˆ’1)
ğœ
)
t is a â„š-standard Wiener process and Nt âˆ¼
Poisson(ğœ‚t).
The market is arbitrage free since we can construct a risk-neutral measure â„šon the filtration
â„±s, 0 â‰¤s â‰¤t. The market model is incomplete as â„šis not unique, since there are more
degrees of freedom in selecting J and ğœ‚.
Under the assumption that the jump component is uncorrelated with the market, there exists
an equivalent martingale risk-neutral measure â„šM âˆ¼â„šsuch that eâˆ’rtÎ t is a â„šM-martingale.
Therefore, under â„šM,
Nt âˆ¼Poisson(ğœ†t)
and the SDE becomes
dSt
Stâˆ’= (r âˆ’D âˆ’ğœ†(J âˆ’1)) dt + ğœd ÌƒWt + (J âˆ’1)dNt
where
ÌƒWt = Wt âˆ’
(ğœ‡âˆ’r + ğœ†(J âˆ’1)
ğœ
)
t
is
a
â„šM-standard
Wiener
process
and
Nt âˆ¼Poisson(ğœ†t).
The market is arbitrage free since we can construct an equivalent martingale risk-neutral
measure â„šM on the filtration â„±s, 0 â‰¤s â‰¤t. However, the market is not complete as â„šM is
not unique, since we can still vary the jump amplitude J.
â—½
4. General Jump Diffusion Process (Mertonâ€™s Model). Let {Nt âˆ¶0 â‰¤t â‰¤T} be a Poisson pro-
cess with intensity ğœ†> 0 and {Wt âˆ¶0 â‰¤t â‰¤T} be a standard Wiener process defined on the
probability space (Î©, â„±, â„™) with respect to the filtration â„±t, 0 â‰¤t â‰¤T. Suppose the asset
price St follows a jump diffusion process
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœdWt + (Jt âˆ’1)dNt

328
5.2.4
Risk-Neutral Measure for Jump Processes
where Jt is the jump variable with mean ğ”¼â„™(Jt) = J if N jumps at time t and
dNt =
{
1
with probability ğœ†dt
0
with probability 1 âˆ’ğœ†dt.
Assume that Wt, Nt and Jt are mutually independent. The constant parameters ğœ‡, D and ğœ
are the drift, continuous dividend yield and volatility, respectively. In addition, let Bt be the
risk-free asset having the following differential equation:
dBt = rBt dt
where r is the risk-free interest rate.
By considering the Radonâ€“NikodÂ´ym derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
=
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒdWuâˆ’1
2 âˆ«t
0 ğœƒ2 du
where ğœƒâˆˆâ„, ğœ‚> 0, â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi = Ji âˆ’1,
i = 1, 2, . . . under the â„šmeasure and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2 du)
< âˆshow that, under risk-neutral
measure â„š, the above SDE can be written as
dSt
Stâˆ’= (r âˆ’D âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1)) dt + ğœd ÌƒWt + (Jt âˆ’1)dNt
where ÌƒWt = Wt âˆ’
(ğœ‡âˆ’r + ğœ‚ğ”¼â„š(Jt âˆ’1)
ğœ
)
t is the â„š-standard Wiener process and Nt âˆ¼
Poisson(ğœ‚t), ğœ‚> 0.
Is the market model arbitrage free and complete under the â„šmeasure?
By assuming the jump component is uncorrelated with the market (i.e., the jump inten-
sity and jump size distribution remain unchanged with the change of measure), find the
corresponding SDE under the equivalent martingale risk-neutral measure â„šM.
Is the market model arbitrage free and complete under the â„šM measure?
Solution: From Problem 5.2.2.1 (page 281) we can express the jump diffusion process as
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœdWt + dMt

5.2.4
Risk-Neutral Measure for Jump Processes
329
where Mt = âˆ‘Nt
i=1(Ji âˆ’1) is a compound Poisson process with intensity ğœ†> 0 such that
the jump size (or amplitude) Ji, i = 1, 2, . . . is a sequence of independent and identically
distributed random variables.
Let ğœƒâˆˆâ„, ğœ‚> 0 and Xi = Ji âˆ’1, i = 1, 2, . . . be a sequence of independent and identically
distributed random variables where each Xi, i = 1, 2, . . . has a probability mass (density)
function â„™(Xi) > 0 (f â„™(Xi) > 0) under the â„™measure. We consider the Radonâ€“NikodÂ´ym
derivative process
Zt = Z(1)
t
â‹…Z(2)
t
such that
Z(1)
t
=
â§
âª
âª
âª
â¨
âª
âª
âªâ©
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚â„š(Xi)
ğœ†â„™(Xi)
if Xi is discrete, â„™(Xi) > 0
e(ğœ†âˆ’ğœ‚)t
Nt
âˆ
i=1
ğœ‚f â„š(Xi)
ğœ†f â„™(Xi)
if Xi is continuous
and
Z(2)
t
= eâˆ’âˆ«t
0 ğœƒdWuâˆ’1
2 âˆ«t
0 ğœƒ2 du
where â„š(Xi) (f â„š(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the â„šmeasure and ğ”¼â„™(
e
1
2 âˆ«T
0 ğœƒ2 du)
< âˆ. By changing the measure â„™to the risk-neutral
measure â„šon the filtration â„±s, 0 â‰¤s â‰¤t such that
ğ”¼â„™
(
dâ„š
dâ„™
||||
â„±t
)
= dâ„š
dâ„™
||||â„±t
= Zt
then from Problem 5.2.3.12 (page 317), under â„š, the compound Poisson process Mt =
âˆ‘Nt
i=1(Ji âˆ’1) has intensity ğœ‚> 0, the process ÌƒWt = Wt + âˆ«
t
0
ğœƒdu is a â„š-standard Wiener
process and Mt âŸ‚âŸ‚ÌƒWt.
In the presence of dividends, the simple strategy of holding a single risky asset is no longer
self-financing as the asset pays out dividends at a rate DSt dt. By letting Stâˆ’denote the value
of St before a jump event at time t, we let the portfolio Î t be valued as
Î t = ğœ™tSt + ğœ“tBt
where ğœ™t and ğœ“t are the units invested in St and the risk-free asset Bd
t , respectively. Given
that the holder will receive DSt dt for every risky asset held, then
dÎ t = ğœ™t(dSt + DSt dt) + ğœ“trBt dt
= ğœ™t
[ğœ‡St dt + ğœStdWt + dMt
] + ğœ“trBt dt
= rÎ t dt + ğœ™tSt
[(ğœ‡âˆ’r) dt + ğœdWt + dMt
] .

330
5.2.4
Risk-Neutral Measure for Jump Processes
By substituting Wt = ÌƒWt âˆ’âˆ«
t
0
ğœƒdu and taking note that the compensated compound Pois-
son process Mt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1)t is a â„š-martingale, we have
dÎ t = rÎ t dt + ğœ™tSt
[(ğœ‡âˆ’r âˆ’ğœğœƒ+ ğœ‚ğ”¼â„š(Jt âˆ’1)) dt + ğœd ÌƒWt + dMt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1) dt
]
.
Since both ÌƒWt and Mt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1) t are â„š-martingales and in order for the discounted
portfolio eâˆ’rtÎ t to be a â„š-martingale
d(eâˆ’rtÎ t) = âˆ’reâˆ’rtÎ t dt + eâˆ’rtdÎ t
= eâˆ’rtğœ™tSt[(ğœ‡âˆ’r âˆ’ğœğœƒ+ ğœ‚ğ”¼â„š(Jt âˆ’1)) dt + ğœd ÌƒWt + dMt âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1) dt]
we set
ğœƒ= ğœ‡âˆ’r + ğœ‚ğ”¼â„š(Jt âˆ’1)
ğœ
.
Therefore, the jump diffusion process under â„šis
dSt
Stâˆ’= (ğœ‡âˆ’D) dt + ğœ
(
d ÌƒWt âˆ’ğœƒdt
)
+ (Jt âˆ’1)dNt
= (r âˆ’D âˆ’ğœ‚ğ”¼â„š(Jt âˆ’1)) dt + ğœd ÌƒWt + (Jt âˆ’1)dNt
where ÌƒWt = Wt âˆ’
(ğœ‡âˆ’r + ğœ‚ğ”¼â„š(Jt âˆ’1)
ğœ
)
t and Nt âˆ¼Poisson(ğœ‚t).
The market is arbitrage free since we can construct a risk-neutral measure â„šon the filtration
â„±s, 0 â‰¤s â‰¤t. However, the market model is incomplete as â„šis not unique, given that there
are more degrees of freedom in selecting ğœ‚and Jt.
Under the assumption that the jump component is uncorrelated with the market, there exists
an equivalent martingale risk-neutral measure â„šM âˆ¼â„šsuch that eâˆ’rtÎ t is a â„šM-martingale
and hence
ğ”¼â„šM(Jt âˆ’1) = ğ”¼â„™(Jt âˆ’1) = J âˆ’1
and
Nt âˆ¼Poisson(ğœ†t).
Thus, under â„šM the jump diffusion process becomes
dSt
Stâˆ’= (r âˆ’D âˆ’ğœ†(J âˆ’1)) dt + ğœd ÌƒWt + (Jt âˆ’1)dNt
where ÌƒWt = Wt âˆ’
(
ğœ‡âˆ’r + ğœ†(J âˆ’1)
ğœ
)
t is a â„šM-standard Wiener process and Nt âˆ¼
Poisson(ğœ†t).
The market is arbitrage free since we can construct an equivalent martingale risk-neutral
measure â„šM on the filtration â„±s, 0 â‰¤s â‰¤t. However, the market is incomplete as â„šM is
not unique, since we can still vary the jump size distribution Jt.
â—½

Appendix A
Mathematics Formulae
Indices
xaxb = xa+b,
xa
xb = xaâˆ’b,
(xa)b = (xb)a = xab
xâˆ’a = 1
xa ,
(
x
y
)a
= xa
ya ,
x0 = 1.
Surds
x
1
a =
aâˆš
x,
aâˆšxy =
aâˆš
x aâˆšy,
aâˆš
xâˆ•y =
aâˆš
x
aâˆšy
(
aâˆš
x
)a
= x,
aâˆš
bâˆš
x =
abâˆš
x,
(
aâˆš
x
)b
= x
b
a .
Exponential and Natural Logarithm
exey = ex+y,
(ex)y = (ey)x = exy,
e0 = 1
log (xy) = log x + log y,
log
(
x
y
)
= log x âˆ’log y,
log xy = y log x
log ex = x,
elog x = x,
ea log x = xa.
Quadratic Equation
For constants a, b and c, the roots of a quadratic equation ax2 + bx + c = 0 are
x = âˆ’b Â±
âˆš
b2 âˆ’4ac
2a
.
Binomial Formula
(
n
k
)
=
n!
k!(n âˆ’k)!,
(
n
k
)
+
(
n
k + 1
)
=
(
n + 1
k + 1
)
(x + y)n =
n
âˆ‘
k=0
(
n
k
)
xnâˆ’kyk =
n
âˆ‘
k=0
n!
k!(n âˆ’k)!xnâˆ’kyk.

332
Appendix A
Series
Arithmetic: For initial term a and common difference d, the n-th term is
Tn = a + (n âˆ’1)d
and the sum of n terms is
Sn = 1
2n[2a + (n âˆ’1)d].
Geometric: For initial term a and common ratio r, the n-th term is
Tn = arnâˆ’1
the sum of n terms is
Sn = a(1 âˆ’rn)
1 âˆ’r
,
and the sum of infinite terms is
lim
nâ†’âˆSn =
a
1 âˆ’r,
|r| < 1.
Summation
For n âˆˆâ„¤+,
n
âˆ‘
k=1
k = 1
2n(n + 1),
n
âˆ‘
k=1
k2 = 1
6n(n + 1)(2n + 1),
n
âˆ‘
k=1
k3 =
[1
2n(n + 1)
]2
.
Let a1, a2, . . . be a sequence of numbers:
â€¢ If âˆ‘an < âˆâ‡’lim
nâ†’âˆan = 0.
â€¢ If lim
nâ†’âˆan â‰ 0 â‡’âˆ‘an = âˆ.
Trigonometric Functions
sin(âˆ’x) = âˆ’sin x,
cos(âˆ’x) = cos x,
tan x = sin x
cos x
csc x =
1
sin x,
sec x =
1
cos x,
cot x =
1
tan x
cos2x + sin2x = 1,
tan2x + 1 = sec2x,
cot2x + 1 = csc2x
sin(x Â± y) = sin x cos y Â± cos x sin y
cos(x Â± y) = cos x cos y âˆ“sin x sin y
tan(x Â± y) = tan x Â± tan y
1 âˆ“tan x tan y.

Appendix A
333
Hyperbolic Functions
sinh x = ex âˆ’eâˆ’x
2
,
cosh x = ex + eâˆ’x
2
,
tanh x = sinh x
cosh x = ex âˆ’eâˆ’x
ex + eâˆ’x
csch x =
1
sinh x,
sech x =
1
cosh x,
coth x =
1
tanh x
sinh(âˆ’x) = âˆ’sinh x,
cosh(âˆ’x) = cosh x,
tanh(âˆ’x) = âˆ’tanh x
cosh2x âˆ’sinh2x = 1,
coth2x âˆ’1 = csch2x,
1 âˆ’tanh2x = sech2x
sinh(x Â± y) = sinh x cosh y Â± cosh x sinh y
cosh(x Â± y) = cosh x cosh y Â± sinh x sinh y
tanh(x Â± y) = tanh x Â± tanh y
1 Â± tanh x tanh y.
Complex Numbers
Let ğ‘¤= u + iğ‘£and z = x + iy where u, ğ‘£, x, y âˆˆâ„, i =
âˆš
âˆ’1 and i2 = âˆ’1 then
ğ‘¤Â± z = (u Â± x) + (ğ‘£Â± y)i,
ğ‘¤z = (ux âˆ’ğ‘£y) + (ğ‘£x + uy)i,
ğ‘¤
z =
(ux + ğ‘£y
x2 + y2
)
+
(ğ‘£x âˆ’uy
x2 + y2
)
i
z = x âˆ’iy,
z = z,
ğ‘¤+ z = ğ‘¤+ z,
ğ‘¤z = ğ‘¤â‹…z,
(
ğ‘¤
z
)
= ğ‘¤
z .
De Moivreâ€™s Formula: Let z = x + iy where x, y âˆˆâ„and we can write
z = r(cos ğœƒ+ i sin ğœƒ),
r =
âˆš
x2 + y2,
ğœƒ= tanâˆ’1 (y
x
)
.
For n âˆˆâ„¤
[r(cos ğœƒ+ i sin ğœƒ)]n = rn[cos(nğœƒ) + i sin(nğœƒ)].
Eulerâ€™s Formula: For ğœƒâˆˆâ„
eiğœƒ= cos ğœƒ+ i sin ğœƒ.
Derivatives
If f(x) and g(x) are differentiable functions of x and a and b are constants
Sum Rule:
d
dx(af(x) + bg(x)) = af â€²(x) + bgâ€²(x)
Product/Chain Rule:
d
dx(f(x)g(x)) = f(x)gâ€²(x) + f â€²(x)g(x)

334
Appendix A
Quotient Rule:
d
dx
( f(x)
g(x)
)
= f â€²(x)g(x) âˆ’f(x)gâ€²(x)
g(x)2
,
g(x) â‰ 0
where d
dxf(x) = f â€²(x) and d
dxg(x) = gâ€²(x).
If f(z) is a differentiable function of z and z = z(x) is a differentiable function of x, then
d
dxf(z(x)) = f â€²(z(x))zâ€²(x).
If x = x(s), y = y(s) and F(s) = f(x(s), y(s)), then
d
dsF(s) = ğœ•f
ğœ•x â‹…ğœ•x
ğœ•s + ğœ•f
ğœ•y â‹…ğœ•y
ğœ•s.
If x = x(u, ğ‘£), y = y(u, ğ‘£) and F(u, ğ‘£) = f(x(u, ğ‘£), y(u, ğ‘£)), then
ğœ•F
ğœ•u = ğœ•f
ğœ•x â‹…ğœ•x
ğœ•u + ğœ•f
ğœ•y â‹…ğœ•y
ğœ•u,
ğœ•F
ğœ•ğ‘£= ğœ•f
ğœ•x â‹…ğœ•x
ğœ•ğ‘£+ ğœ•f
ğœ•y â‹…ğœ•y
ğœ•ğ‘£.
Standard Differentiations
If f(x) and g(x) are differentiable functions of x and a and b are constants
d
dxa = 0,
d
dx[f(x)]n = n[f(x)]nâˆ’1f â€²(x)
d
dxe f(x) = f â€²(x)e f(x),
d
dx log f(x) = f â€²(x)
f(x) ,
d
dxa f(x) = f â€²(x)a f(x) log a
d
dx sin(ax) = a cos x,
d
dx cos(ax) = âˆ’a sin(ax),
d
dx tan(ax) = asec2x
d
dx sinh(ax) = a cosh(ax),
d
dx cosh(ax) = a sinh(ax),
d
dx tanh(ax) = asech2(ax)
where f â€²(x) = d
dxf(x).
Taylor Series
If f(x) is an analytic function of x, then for small h
f(x0 + h) = f(x0) + f â€²(x0)h + 1
2!f â€²â€²(x0)h2 + 1
3!f â€²â€²â€²(x0)h3 + . . .

Appendix A
335
If f(x, y) is an analytic function of x and y, then for small Î”x, Î”y
f(x0 + Î”x, y0 + Î”y) = f(x0, y0) + ğœ•f(x0, y0)
ğœ•x
Î”x + ğœ•f(x0, y0)
ğœ•y
Î”y
+ 1
2!
[ğœ•2f(x0, y0)
ğœ•x2
(Î”x)2 + 2ğœ•2f(x0, y0)
ğœ•xğœ•y
Î”xÎ”y + ğœ•2f(x0, y0)
ğœ•y2
(Î”y)2
]
+ 1
3!
[ğœ•3f(x0, y0)
ğœ•x3
(Î”x)3 + 3ğœ•3f(x0, y0)
ğœ•x2ğœ•y
(Î”x)2Î”y
+3ğœ•3f(x0, y0)
ğœ•xğœ•y2
Î”x(Î”y)2 + ğœ•3f(x0, y0)
ğœ•y3
(Î”y)3
]
+ . . .
Maclaurin Series
Taylor series expansion of a function about x0 = 0
1
1 + x = 1 âˆ’x + x2 âˆ’x3 + . . . ,
|x| < 1
1
1 âˆ’x = 1 + x + x2 + x3 + . . . ,
|x| < 1
ex = 1 + x + 1
2!x2 + 1
3!x3 + . . . ,
for all x
eâˆ’x = 1 âˆ’x + 1
2!x2 âˆ’1
3!x3 + . . . ,
for all x
log(1 + x) = x âˆ’1
2x2 + 1
3x3 âˆ’1
4x4 + . . . ,
x âˆˆ(âˆ’1, 1]
log(1 âˆ’x) = âˆ’x âˆ’1
2x2 âˆ’1
3x3 âˆ’1
4x4 + . . . ,
|x| < 1
sin x = x âˆ’1
3!x3 + 1
5!x5 âˆ’1
7!x7 + . . . ,
for all x
cos x = 1 âˆ’1
2!x2 + 1
4!x4 âˆ’1
6!x6 + . . . ,
for all x
tan x = x + 1
3x3 + 2
15x5 + 17
315x7 + . . . ,
|x| < ğœ‹
2
sinh x = x + 1
3!x3 + 1
5!x5 + 1
7!x7 + . . . ,
for all x
cosh x = 1 + 1
2!x2 + 1
4!x4 + 1
6!x6 + . . . ,
for all x
tanh x = x âˆ’1
3x3 + 2
15x5 âˆ’17
315x7 + . . . ,
|x| < ğœ‹
2 .
Landau Symbols and Asymptotics
Let f(x) and g(x) be two functions defined on some subsets of real numbers, then as x â†’x0

336
Appendix A
â€¢ f(x) = O(g(x)) if there exists a constant K > 0 and ğ›¿> 0 such that |f(x)| â‰¤K|g(x)| for
|x âˆ’x0| < ğ›¿.
â€¢ f(x) = o(g(x)) if lim
xâ†’x0
f(x)
g(x) = 0.
â€¢ f(x) âˆ¼g(x) if lim
xâ†’x0
f(x)
g(x) = 1.
Lâ€™Hospital Rule
Let f and g be differentiable on a âˆˆâ„such that gâ€²(x) â‰ 0 in an interval around a, except
possibly at a itself. Suppose that
lim
xâ†’a f(x) = lim
xâ†’a g(x) = 0
or
lim
xâ†’a f(x) = lim
xâ†’a g(x) = Â±âˆ
then
lim
xâ†’a
f(x)
g(x) = lim
xâ†’a
f â€²(x)
gâ€²(x).
Indefinite Integrals
If F(x) is a differentiable function and f(x) is its derivative, then
âˆ«f(x) dx = F(x) + c
where F
â€²(x) = d
dxF(x) = f(x) and c is an arbitrary constant.
If f(x) is a continuous function then
d
dx âˆ«f(x) dx = f(x).
Standard Indefinite Integrals
If f(x) is a differentiable function of x and a and b are constants
âˆ«a dx = ax + c,
âˆ«(ax + b)n dx = (ax + b)n+1
a(n + 1)
+ c,
n â‰ âˆ’1
âˆ«
f â€²(x)
f(x) dx = log|f(x)| + c,
âˆ«e f(x) dx =
1
f â€²(x)e f(x) + c
âˆ«log(ax) dx = x log(ax) âˆ’ax + c,
âˆ«ax dx =
ax
log a + c
âˆ«sin(ax) dx = âˆ’1
a cos(ax) + c,
âˆ«cos(ax) dx = 1
a sin(ax) + c

Appendix A
337
âˆ«sinh(ax) dx = 1
a cosh(ax) + c,
âˆ«cosh(ax) dx = 1
a sinh(ax) + c
where c is an arbitrary constant.
Definite Integrals
If F(x) is a differentiable function and f(x) is its derivative and is continuous on a close interval
[a, b], then
âˆ«
b
a
f(x) dx = F(b) âˆ’F(a)
where F
â€²(x) = d
dxF(x) = f(x).
If f(x) and g(x) are integrable functions then
âˆ«
a
a
f(x) dx = 0,
âˆ«
b
a
f(x) dx = âˆ’âˆ«
a
b
f(x) dx
âˆ«
b
a
[ğ›¼f(x) + ğ›½g(x)] dx = ğ›¼âˆ«
b
a
f(x) dx + ğ›½âˆ«
b
a
g(x) dx,
ğ›¼, ğ›½are constants
âˆ«
b
a
f(x) dx = âˆ«
c
a
f(x) dx + âˆ«
b
c
f(x) dx,
c âˆˆ[a, b].
Derivatives of Definite Integrals
If f(t) is a continuous function of t and a(x) and b(x) are continuous functions of x
d
dx âˆ«
b(x)
a(x)
f(t) dt = f(b(x)) d
dxb(x) âˆ’f(a(x)) d
dxa(x)
d
dx âˆ«
b(x)
a(x)
dt = d
dxb(x) âˆ’d
dxa(x).
If g(x, t) is a differentiable function of two variables then
d
dx âˆ«
b(x)
a(x)
g(x, t) dt = g(x, b(x)) d
dxb(x) âˆ’g(x, a(x)) d
dxa(x) + âˆ«
b(x)
a(x)
ğœ•g(x, t)
ğœ•x
dt.
Integration by Parts
For definite integrals
âˆ«
b
a
u(x)ğ‘£â€²(x) dx = u(x)ğ‘£(x)
|||||
b
a
âˆ’âˆ«
b
a
ğ‘£(x)uâ€²(x) dx.
where uâ€²(x) = d
dxu(x) and ğ‘£â€²(x) = d
dxğ‘£(x).

338
Appendix A
Integration by Substitution
If f(x) is a continuous function of x and gâ€² is continuous on the closed interval [a, b] then
âˆ«
g(a)
g(b)
f(x) dx = âˆ«
b
a
f(g(u))gâ€²(u) du.
Gamma Function
The gamma function is defined as
Î“(z) = âˆ«
âˆ
0
tzâˆ’1eâˆ’t dt
such that
Î“(z + 1) = zÎ“(z),
Î“
(1
2
)
=
âˆš
ğœ‹,
Î“(n) = (n âˆ’1)! for n âˆˆâ„•.
Beta Function
The beta function is defined as
B(x, y) = âˆ«
1
0
txâˆ’1(1 âˆ’t)yâˆ’1 dt
for x > 0, y > 0
such that
B(x, y) = B(y, x),
B(x, y) = Î“(x)Î“(y)
Î“(x + y) .
In addition
âˆ«
u
0
txâˆ’1(u âˆ’t)yâˆ’1 dt = ux+yâˆ’1B(x, y).
Convex Function
A set Î© in a vector space over â„is called a convex set if for x, y âˆˆÎ©, x â‰ y and for any
ğœ†âˆˆ(0, 1),
ğœ†x + (1 âˆ’ğœ†)y âˆˆÎ©.
Let Î© be a convex set in a vector space over â„. A function f âˆ¶Î© î‚¶â†’â„is called a convex
function if for x, y âˆˆÎ©, x â‰ y and for any ğœ†âˆˆ(0, 1),
f(ğœ†x + (1 âˆ’ğœ†)y) â‰¤ğœ†f(x) + (1 âˆ’ğœ†)f(y).
If the inequality is strict then f is strictly convex.
If f is convex and differentiable on â„, then
f(x) â‰¥f(y) + f â€²(y)(x âˆ’y).
If f is a twice continuously differentiable function on â„, then f is convex if and only if f
â€²â€² â‰¥0.
If f
â€²â€² > 0 then f is strictly convex.

Appendix A
339
f is a (strictly) concave function if âˆ’f is a (strictly) convex function.
Dirac Delta Function
The Dirac delta function is defined as
ğ›¿(x) =
{
0 x â‰ 0
âˆx = 0
and for a continuous function f(x) and a constant a, we have
âˆ«
âˆ
âˆ’âˆ
ğ›¿(x) dx = 1,
âˆ«
âˆ
âˆ’âˆ
f(x)ğ›¿(x) dx = f(0),
âˆ«
âˆ
âˆ’âˆ
f(x)ğ›¿(x âˆ’a) dx = f(a).
Heaviside Step Function
The Heaviside step function, H(x) is defined as the integral of the Dirac delta function given as
H(x) = âˆ«
x
âˆ’âˆ
ğ›¿(s) ds =
{
0 x < 0
1 x > 0.
Fubiniâ€™s Theorem
Suppose f(x, y) is A Ã— B measurable and if âˆ«AÃ—B
|f(x, y)|d(x, y) < âˆthen
âˆ«AÃ—B
f(x, y) d(x, y) = âˆ«A
(
âˆ«B
f(x, y) dy
)
dx = âˆ«B
(
âˆ«A
f(x, y) dx
)
dy.


Appendix B
Probability Theory Formulae
Probability Concepts
Let A and B be events of the sample space Î© with probabilities â„™(A) âˆˆ[0, 1] and â„™(B) âˆˆ
[0, 1], then
Complement:
â„™(Ac) = 1 âˆ’â„™(A).
Conditional:
â„™(A|B) = â„™(A âˆ©B)
â„™(B)
.
Independence: The events A and B are independent if and only if
â„™(A âˆ©B) = â„™(A) â‹…â„™(B).
Mutually Exclusive: The events A and B are mutually exclusive if and only if
â„™(A âˆ©B) = 0.
Union:
â„™(A âˆªB) = â„™(A) + â„™(B) âˆ’â„™(A âˆ©B).
Intersection:
â„™(A âˆ©B) = â„™(A|B)â„™(B) = â„™(B|A)â„™(A).
Partition:
â„™(A) = â„™(A âˆ©B) + â„™(A âˆ©Bc) = â„™(A|B)â„™(B) + â„™(A|Bc)â„™(Bc).
Bayesâ€™ Rule
Let A and B be events of the sample space Î© with probabilities â„™(A) âˆˆ[0, 1] and â„™(B) âˆˆ
[0, 1], then
â„™(A|B) = â„™(B|A)â„™(A)
â„™(B)
.

342
Appendix B
Indicator Function
The indicator function 1IA of an event A of a sample space Î© is a function 1IA âˆ¶Î© î‚¶â†’â„
defined as
1IA(ğœ”) =
{
1 if ğœ”âˆˆA
0 if ğœ”âˆˆAc.
Properties: For events A and B of the sample space Î©
1IAc = 1 âˆ’1IA,
1IAâˆ©B = 1IA1IB,
1IAâˆªB = 1IA + 1IB âˆ’1IA1IB
ğ”¼(1IA) = â„™(A),
Var (1IA) = â„™(A)â„™(Ac),
Cov (1IA, 1IB) = â„™(A âˆ©B) âˆ’â„™(A)â„™(B).
Discrete Random Variables
Univariate Case
Let X be a discrete random variable whose possible values are x = x1, x2, . . . , and let â„™(X = x)
be the probability mass function.
Total Probability of All Possible Values:
âˆ
âˆ‘
k=1
â„™(X = xk) = 1.
Cumulative Distribution Function:
â„™(X â‰¤xn) =
n
âˆ‘
k=1
â„™(X = xk).
Expectation:
ğ”¼(X) = ğœ‡=
âˆ
âˆ‘
k=1
xkâ„™(X = xk).
Variance:
Var (X) = ğœ2
= ğ”¼[(X âˆ’ğœ‡)2]
=
âˆ
âˆ‘
k=1
(xk âˆ’ğœ‡)2â„™(X = xk)
=
âˆ
âˆ‘
k=1
x2
kâ„™(X = xk) âˆ’ğœ‡2
= ğ”¼(X2) âˆ’[ğ”¼(X)]2.
Moment Generating Function:
MX(t) = ğ”¼(etX) =
âˆ
âˆ‘
k=1
etxkâ„™(X = xk),
t âˆˆâ„.

Appendix B
343
Characteristic Function:
ğœ‘X(t) = ğ”¼(eitX) =
âˆ
âˆ‘
k=1
eitxkâ„™(X = xk),
i =
âˆš
âˆ’1 and t âˆˆâ„.
Bivariate Case
Let X and Y be discrete random variables whose possible values are x = x1, x2, . . . and y =
y1, y2, . . . , respectively, and let â„™(X = x, Y = y) be the joint probability mass function.
Total Probability of All Possible Values:
âˆ
âˆ‘
j=1
âˆ
âˆ‘
k=1
â„™(X = xj, Y = yk) = 1.
Joint Cumulative Distribution Function:
â„™(X â‰¤xn, Y â‰¤ym) =
n
âˆ‘
j=1
m
âˆ‘
k=1
â„™(X = xj, Y = yk).
Marginal Probability Mass Function:
â„™(X = x) =
âˆ
âˆ‘
k=1
â„™(X = x, Y = yk),
â„™(Y = y) =
âˆ
âˆ‘
j=1
â„™(X = xj, Y = y).
Conditional Probability Mass Function:
â„™(X = x|Y = y) = â„™(X = x, Y = y)
â„™(Y = y)
,
â„™(Y = y|X = x) = â„™(X = x, Y = y)
â„™(X = x)
.
Conditional Expectation:
ğ”¼(X|Y) = ğœ‡x|y =
n
âˆ‘
j=1
xjâ„™(X = xj|Y = y),
ğ”¼(Y|X) = ğœ‡y|x =
n
âˆ‘
k=1
ykâ„™(Y = yk|X = x).
Conditional Variance:
Var (X|Y) = ğœ2
x|y
= ğ”¼[(X âˆ’ğœ‡x|y)2|Y]
=
âˆ
âˆ‘
j=1
(xj âˆ’ğœ‡x|y)2â„™(X = xj|Y = y)
=
âˆ
âˆ‘
j=1
x2
j â„™(X = xj|Y = y) âˆ’ğœ‡2
x|y

344
Appendix B
Var (Y|X) = ğœ2
y|x
= ğ”¼[(Y âˆ’ğœ‡y|x)2|X]
=
âˆ
âˆ‘
k=1
(yk âˆ’ğœ‡y|x)2â„™(Y = yk|X = x)
=
âˆ
âˆ‘
k=1
y2
kâ„™(Y = yk|X = x) âˆ’ğœ‡2
y|x.
Covariance: For ğ”¼(X) = ğœ‡x and ğ”¼(Y) = ğœ‡y
Cov (X, Y) = ğ”¼[(X âˆ’ğœ‡x)(Y âˆ’ğœ‡y)]
=
âˆ
âˆ‘
j=1
âˆ
âˆ‘
k=1
(xj âˆ’ğœ‡x)(yk âˆ’ğœ‡y)â„™(X = xj, Y = yk)
=
âˆ
âˆ‘
j=1
âˆ
âˆ‘
k=1
xjykâ„™(X = xj, Y = yk) âˆ’ğœ‡xğœ‡y
= ğ”¼(XY) âˆ’ğ”¼(X)ğ”¼(Y).
Joint Moment Generating Function: For s, t âˆˆâ„
MXY(s, t) = ğ”¼(esX+tY) =
âˆ
âˆ‘
j=1
âˆ
âˆ‘
k=1
esxj+tykâ„™(X = xj, Y = yk).
Joint Characteristic Function: For i =
âˆš
âˆ’1 and s, t âˆˆâ„
ğœ‘XY(s, t) = ğ”¼(eisX+itY) =
âˆ
âˆ‘
j=1
âˆ
âˆ‘
k=1
eisxj+itykâ„™(X = xj, Y = yk).
Independence: X and Y are independent if and only if
â€¢ â„™(X = x, Y = y) = â„™(X = x)â„™(Y = y).
â€¢ MXY(s, t) = ğ”¼(esX+tY) = ğ”¼(esX)ğ”¼(etY) = MX(s)MY(t).
â€¢ ğœ‘XY(s, t) = ğ”¼(eisX+itY) = ğ”¼(eisX)ğ”¼(eitY) = ğœ‘X(s)ğœ‘Y(t).
Continuous Random Variables
Univariate Case
Let X be a continuous random variable whose values x âˆˆâ„and let fX(x) be the probability
density function.
Total Probability of All Possible Values:
âˆ«
âˆ
âˆ’âˆ
fX(x) dx = 1.

Appendix B
345
Evaluating Probability:
â„™(a â‰¤X â‰¤b) = âˆ«
b
a
fX(x) dx.
Cumulative Distribution Function:
FX(x) = â„™(X â‰¤x) = âˆ«
x
âˆ’âˆ
fX(x) dx.
Probability Density Function:
fX(x) = d
dxFX(x).
Expectation:
ğ”¼(X) = ğœ‡= âˆ«
âˆ
âˆ’âˆ
xfX(x) dx.
Variance:
Var (X) = ğœ2 = âˆ«
âˆ
âˆ’âˆ
(x âˆ’ğœ‡)2fX(x) dx = âˆ«
âˆ
âˆ’âˆ
x2fX(x) dx âˆ’ğœ‡2 = ğ”¼(X2) âˆ’[ğ”¼(X)]2.
Moment Generating Function:
MX(t) = ğ”¼(etX) = âˆ«
âˆ
âˆ’âˆ
etxfX(x) dx,
t âˆˆâ„.
Characteristic Function:
ğœ‘X(t) = ğ”¼(eitX) = âˆ«
âˆ
âˆ’âˆ
eitxfX(x) dx,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Probability Density Function of a Dependent Variable:
Let the random variable Y = g(X). If g is monotonic then the probability density function of
Y is
fY(y) = fX
(gâˆ’1(y)) ||||
d
dygâˆ’1(y)
||||
âˆ’1
where gâˆ’1 denotes the inverse function.
Bivariate Case
Let X and Y be two continuous random variables whose values x âˆˆâ„and y âˆˆâ„, and let
fXY(x, y) be the joint probability density function.
Total Probability of All Possible Values:
âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
fXY(x, y) dxdy = âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
fXY(x, y) dydx = 1.

346
Appendix B
Joint Cumulative Distribution Function:
FXY(x, y) = â„™(X â‰¤x, Y â‰¤y) = âˆ«
x
âˆ’âˆâˆ«
y
âˆ’âˆ
fXY(x, y) dydx = âˆ«
y
âˆ’âˆâˆ«
x
âˆ’âˆ
fXY(x, y) dxdy.
Evaluating Joint Probability:
â„™(xa â‰¤X â‰¤xb, ya â‰¤Y â‰¤yb) = âˆ«
xb
xa
âˆ«
yb
ya
fXY(x, y) dydx
= âˆ«
yb
ya
âˆ«
xb
xa
fXY(x, y) dxdy
= FXY(xb, yb) âˆ’FXY(xb, ya) âˆ’FXY(xa, yb) + FXY(xa, ya).
Joint Probability Density Function:
fXY(x, y) =
ğœ•2
ğœ•xğœ•yFXY(x, y) =
ğœ•2
ğœ•yğœ•xFXY(x, y).
Marginal Probability Density Function:
fX(x) = âˆ«
âˆ
âˆ’âˆ
fXY(x, y) dy,
fY(y) = âˆ«
âˆ
âˆ’âˆ
fXY(x, y) dx.
Conditional Probability Density Function:
fX|Y(x|y) = fXY(x, y)
fY(y) ,
fY|X(y|x) = fXY(x, y)
fX(x) .
Conditional Expectation:
ğ”¼(X|Y) = ğœ‡x|y = âˆ«
âˆ
âˆ’âˆ
xfX|Y(x|y) dx,
ğ”¼(Y|X) = ğœ‡y|x = âˆ«
âˆ
âˆ’âˆ
yfY|X(y|x) dy.
Conditional Variance:
Var (X|Y) = ğœ2
x|y
= ğ”¼[(X âˆ’ğœ‡x|y)2|Y]
= âˆ«
âˆ
âˆ’âˆ
(x âˆ’ğœ‡x|y)2fX|Y(x|y) dx
= âˆ«
âˆ
âˆ’âˆ
x2fX|Y(x|y) dx âˆ’ğœ‡2
x|y
Var (Y|X) = ğœ2
y|x
= ğ”¼[(Y âˆ’ğœ‡y|x)2|X]
= âˆ«
âˆ
âˆ’âˆ
(y âˆ’ğœ‡y|x)2fY|X(y|x) dy

Appendix B
347
= âˆ«
âˆ
âˆ’âˆ
y2fY|X(y|x) dy âˆ’ğœ‡2
y|x.
Covariance: For ğ”¼(X) = ğœ‡x and ğ”¼(Y) = ğœ‡y
Cov (X, Y) = ğ”¼[(X âˆ’ğœ‡x)(Y âˆ’ğœ‡y)]
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
(x âˆ’ğœ‡x)(y âˆ’ğœ‡y)fXY(x, y) dy dx
= âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
xyfXY(x, y) dy dx âˆ’ğœ‡xğœ‡y
= ğ”¼(XY) âˆ’ğ”¼(X)ğ”¼(Y).
Joint Moment Generating Function: For t, s âˆˆâ„
MXY(s, t) = ğ”¼(esX+tY) = âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
esx+tyfXY(x, y) dy dx.
Joint Characteristic Function: For i =
âˆš
âˆ’1 and t, s âˆˆâ„
ğœ‘XY(s, t) = ğ”¼(eisX+itY) = âˆ«
âˆ
âˆ’âˆâˆ«
âˆ
âˆ’âˆ
eisx+ityfXY(x, y) dy dx.
Independence: X and Y are independent if and only if
â€¢ fXY(x, y) = fX(x)fY(y).
â€¢ MXY(s, t) = ğ”¼(esX+tY) = ğ”¼(esX)ğ”¼(etY) = MX(s)MY(t).
â€¢ ğœ‘XY(s, t) = ğ”¼(eisX+itY) = ğ”¼(eisX)ğ”¼(eitY) = ğœ‘X(s)ğœ‘Y(t).
Joint Probability Density Function of Dependent Variables: Let the random variables U =
g(X, Y), V = h(X, Y). If u = g(x, y) and ğ‘£= h(x, y can be uniquely solved for x and y in terms
of u and ğ‘£with solutions given by, say, x = p(u, ğ‘£) and y = q(u, ğ‘£) and the functions g and h
have continuous partial derivatives at all points (x, y) such that the determinant
J(x, y) =
||||||||
ğœ•g
ğœ•x
ğœ•g
ğœ•y
ğœ•h
ğœ•x
ğœ•h
ğœ•y
||||||||
= ğœ•g
ğœ•x
ğœ•h
ğœ•y âˆ’ğœ•g
ğœ•y
ğœ•h
ğœ•x â‰ 0
then the joint probability density function of U and V is
fUV(u, ğ‘£) = fXY(x, y)|J(x, y)|âˆ’1
where x = p(u, ğ‘£) and y = q(u, ğ‘£).
Properties of Expectation and Variance
Let X and Y be two random variables and for constants a and b
ğ”¼(aX + b) = ağ”¼(X) + b,
Var (aX + b) = a2Var (X)
ğ”¼(aX + bY) = ağ”¼(X) + bğ”¼(Y),
Var (aX + bY) = a2Var (X) + b2Var (Y) + 2abCov (X, Y).

348
Appendix B
Properties of Moment Generating and Characteristic Functions
If a random variable X has moments up to k-th order where k is a non-negative integer, then
ğ”¼(Xk) = dk
dtk MX(t)||||t=0
= iâˆ’k dk
dtk ğœ‘X(t)||||t=0
where i =
âˆš
âˆ’1.
If the bivariate random variables X and Y have moments up to m + n = k where m, n and k are
non-negative integers, then
ğ”¼(XmYn) =
dk
dsmdtn MXY(s, t)||||s=0,t=0
= iâˆ’k
dk
dsmdtn ğœ‘XY(s, t)||||s=0,t=0
where i =
âˆš
âˆ’1.
Correlation Coefficient
Let X and Y be two random variables with means ğœ‡x and ğœ‡y and variances ğœ2
x and ğœ2
y. The
correlation coefficient ğœŒxy between X and Y is defined as
ğœŒxy =
Cov (X, Y)
âˆš
Var (X)Var (Y)
=
ğ”¼[(X âˆ’ğœ‡x)(Y âˆ’ğœ‡y)]
ğœxğœy
.
Important information:
â€¢ ğœŒxy measures only the linear dependency between X and Y.
â€¢ âˆ’1 â‰¤ğœŒxy â‰¤1.
â€¢ If ğœŒxy = 0 then X and Y are uncorrelated.
â€¢ If X and Y are independent then ğœŒxy = 0. However, the converse is not true.
â€¢ If X and Y are jointly normally distributed then X and Y are independent if and only if
ğœŒXY = 0.
Convolution
If X and Y are independent discrete random variables with probability mass functions â„™(X = x)
and â„™(Y = y), respectively, then the probability mass function for Z = X + Y is
â„™(Z = z) =
âˆ‘
x
â„™(X = x)â„™(Y = z âˆ’x) =
âˆ‘
y
â„™(X = z âˆ’y)â„™(Y = y).
If X and Y are independent continuous random variables with probability density functions
fX(x) and fY(y), respectively, then the probability density function for Z = X + Y is
fZ(z) = âˆ«
âˆ
âˆ’âˆ
fX(x)fY(z âˆ’x) dx = âˆ«
âˆ
âˆ’âˆ
fX(z âˆ’y)fY(y) dy.

Appendix B
349
Discrete Distributions
Bernoulli: A random variable X is said to follow a Bernoulli distribution, X âˆ¼Bernoulli(p)
where p âˆˆ[0, 1] is the probability of success and the probability mass function is given as
P(X = x) = px(1 âˆ’p)1âˆ’x,
x = 0, 1
where ğ”¼(X) = p and Var (X) = p âˆ’p2. The moment generating function is
MX(t) = 1 âˆ’p + pet,
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) = 1 âˆ’p + peit,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Geometric: A random variable X is said to follow a geometric distribution, X âˆ¼Geometric(p),
where p âˆˆ[0, 1] is the probability of success and the probability mass function is given as
â„™(X = x) = p(1 âˆ’p)xâˆ’1,
x = 1, 2, . . .
where ğ”¼(X) = 1
p and Var (X) = 1 âˆ’p
p2 . The moment generating function is
MX(t) =
p
1 âˆ’(1 âˆ’p)et ,
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) =
p
1 âˆ’(1 âˆ’p)eit ,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Binomial: A random variable X is said to follow a binomial distribution, X âˆ¼Binomial(n, p),
p âˆˆ[0, 1], where p âˆˆ[0, 1] is the probability of success and n âˆˆâ„•0 is the number of trials and
the probability mass function is given as
â„™(X = x) =
(
n
x
)
px(1 âˆ’p)nâˆ’x,
x = 0, 1, 2, . . . , n
where ğ”¼(X) = np and Var (X) = np(1 âˆ’p). The moment generating function is
MX(t) = (1 âˆ’p + pet)n,
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) = (1 âˆ’p + peit)n,
i =
âˆš
âˆ’1 and t âˆˆâ„.

350
Appendix B
Negative Binomial: A random variable X is said to follow a negative binomial distribution,
X âˆ¼NB(r, p) where p âˆˆ[0, 1] is the probability of success and r is the number of successes
accumulated and the probability mass function is given as
â„™(X = x) =
(
x âˆ’1
r âˆ’1
)
pr(1 âˆ’p)nâˆ’r,
x = r, r + 1, r + 2, . . .
where ğ”¼(X) = r
p and Var (X) = r(1 âˆ’p)
p2
. The moment generating function is
MX(t) =
( 1 âˆ’p
1 âˆ’pet
)r
,
t < âˆ’log p
and the corresponding characteristic function is
MX(t) =
( 1 âˆ’p
1 âˆ’peit
)r
,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Poisson: A random variable X is said to follow a Poisson distribution, X âˆ¼Poisson(ğœ†), ğœ†> 0
with probability mass function given as
â„™(X = x) = eâˆ’ğœ†ğœ†x
x!
,
x = 0, 1, 2, . . .
where ğ”¼(X) = ğœ†and Var (X) = ğœ†. The moment generating function is
MX(t) = eğœ†(etâˆ’1),
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) = eğœ†(eitâˆ’1),
i =
âˆš
âˆ’1 and t âˆˆâ„.
Continuous Distributions
Uniform: A random variable X is said to follow a uniform distribution, X âˆ¼ğ’°(a, b), a < b
with probability density function given as
fX(x) =
1
b âˆ’a,
a < x < b
where ğ”¼(X) = a + b
2
and Var (X) = (b âˆ’a)2
12
. The moment generating function is
MX(t) = etb âˆ’eta
t(b âˆ’a) ,
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) = etb âˆ’eita
it(b âˆ’a) ,
i =
âˆš
âˆ’1 and t âˆˆâ„.

Appendix B
351
Normal: A random variable X is said to follow a normal distribution, X âˆ¼ğ’©(ğœ‡, ğœ2), ğœ‡âˆˆâ„,
ğœ2 > 0 with probability density function given as
fX(x) =
1
ğœ
âˆš
2ğœ‹
e
âˆ’1
2
( xâˆ’ğœ‡
ğœ
)2
,
x âˆˆâ„
where ğ”¼(X) = ğœ‡and Var (X) = ğœ2. The moment generating function is
MX(t) = eğœ‡t+ 1
2 ğœ2t2,
t âˆˆâ„
and the corresponding characteristic function is
ğœ‘X(t) = eiğœ‡tâˆ’1
2 ğœ2t2,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Lognormal: A random variable X is said to follow a lognormal distribution, X âˆ¼log-ğ’©(ğœ‡, ğœ2),
ğœ‡âˆˆâ„, ğœ2 > 0 with probability density function given as
fX(x) =
1
xğœ
âˆš
2ğœ‹
e
âˆ’1
2
( log xâˆ’ğœ‡
ğœ
)2
,
x > 0
where ğ”¼(X) = eğœ‡+ 1
2 ğœ2 and Var (X) = (eğœ2 âˆ’1)e2ğœ‡+ğœ2. The moment generating function is
MX(t) =
âˆ
âˆ‘
n=0
tn
n!enğœ‡+ 1
2 n2ğœ2,
t â‰¤0
and the corresponding characteristic function is
ğœ‘X(t) =
âˆ
âˆ‘
n=0
(it)n
n! enğœ‡+ 1
2 n2ğœ2,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Exponential: A random variable X is said to follow an exponential distribution, X âˆ¼Exp(ğœ†),
ğœ†> 0 with probability density function
fX(x) = ğœ†eâˆ’ğœ†x,
x â‰¥0
where ğ”¼(X) = 1
ğœ†and Var (X) = 1
ğœ†2 . The moment generating function is
MX(t) =
ğœ†
ğœ†âˆ’t,
t < ğœ†
and the corresponding characteristic function is
ğœ‘X(t) =
ğœ†
ğœ†âˆ’it,
i =
âˆš
âˆ’1 and t âˆˆâ„.

352
Appendix B
Gamma: A random variable X is said to follow a gamma distribution, X âˆ¼Gamma(ğ›¼, ğœ†), ğ›¼, ğœ†>
0 with probability density function given as
fX(x) = ğœ†eâˆ’ğœ†x(ğœ†x)ğ›¼âˆ’1
Î“(ğ›¼)
,
x â‰¥0
such that
Î“(ğ›¼) = âˆ«
âˆ
0
eâˆ’xxğ›¼âˆ’1 dx
where ğ”¼(X) = ğ›¼
ğœ†and Var (X) = ğ›¼
ğœ†2 . The moment generating function is
MX(t) =
(
ğœ†
ğœ†âˆ’t
)ğ›¼
,
t < ğœ†
and the corresponding characteristic function is
ğœ‘X(t) =
(
ğœ†
ğœ†âˆ’it
)ğ›¼
,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Chi-Square: A random variable X is said to follow a chi-square distribution, X âˆ¼ğœ’2(ğœˆ), ğœˆâˆˆâ„•
with probability density function given as
fX(x) =
1
2
ğœˆ
2 Î“
(
ğœˆ
2
)x
ğœˆ
2 âˆ’1eâˆ’x
2 ,
x â‰¥0
such that
Î“
(ğœˆ
2
)
= âˆ«
âˆ
0
eâˆ’xx
ğœˆ
2 âˆ’1 dx
where ğ”¼(X) = ğœˆand Var (X) = 2ğœˆ. The moment generating function is
MX(t) = (1 âˆ’2t)âˆ’ğœˆ
2 ,
âˆ’1
2 < t < 1
2
and the corresponding characteristic function is
MX(t) = (1 âˆ’2it)âˆ’ğœˆ
2 ,
i =
âˆš
âˆ’1 and t âˆˆâ„.
Bivariate Normal: The random variables X and Y with means ğœ‡x, ğœ‡y, variances ğœ2
x, ğœ2
y,
and correlation coefficient ğœŒxy âˆˆ(âˆ’1, 1) is said to follow a joint normal distribution,
(X, Y) âˆ¼ğ’©2 (ğ, ğšº) where ğ=
[ğœ‡x
ğœ‡y
]
and ğšº=
[
Var(X)
Cov (X, Y)
Cov (X, Y)
Var(Y)
]
=
[
ğœ2
x
ğœŒxyğœxğœy
ğœŒxyğœxğœy
ğœ2
y
]
with joint probability density function given as
fXY(x, y) =
1
2ğœ‹ğœxğœy
âˆš
1 âˆ’ğœŒ2
xy
e
âˆ’
1
2(1âˆ’ğœŒ2xy)
[( xâˆ’ğœ‡x
ğœx
)2
âˆ’2ğœŒ
( xâˆ’ğœ‡x
ğœx
)(
yâˆ’ğœ‡y
ğœy
)
+
(
yâˆ’ğœ‡y
ğœy
)2]
,
x, y âˆˆâ„.

Appendix B
353
The moment generating function is
MXY(s, t) = eğœ‡xs+ğœ‡yt+ 1
2 (ğœ2
x s2+2ğœŒxyğœxğœyst+ğœ2
y t2),
s, t âˆˆâ„
and the corresponding characteristic function is
ğœ‘XY(s, t) = eiğœ‡xs+iğœ‡ytâˆ’1
2 (ğœ2
x s2+2ğœŒxyğœxğœyst+ğœ2
y t2),
i =
âˆš
âˆ’1 and s, t âˆˆâ„.
Multivariate Normal: The random vector X = (X1, X2, . . . , Xn) is said to follow a multivariate
normal distribution, X âˆ¼ğ’©n (ğ, ğšº) where
ğ=
â¡
â¢
â¢
â¢â£
ğ”¼(X1)
ğ”¼(X2)
â‹®
ğ”¼(Xn)
â¤
â¥
â¥
â¥â¦
and ğšº=
â¡
â¢
â¢
â¢â£
Var (X1
)
Cov(X1, X2) . . . Cov(X1, Xn)
Cov(X1, X2)
Var(X2)
. . . Cov(X2, Xn)
â‹®
â‹®
â‹±
â‹®
Cov(X1, Xn) Cov(X2, Xn) . . .
Var(Xn)
â¤
â¥
â¥
â¥â¦
with probability density function given as
fX(x1, x2, . . . , xn) =
1
(2ğœ‹)
n
2 |ğšº|
1
2
eâˆ’1
2 XTğšºâˆ’1X.
The moment generating function is
MX(t1, t2, . . . , tn) = eğTt+ 1
2 tTğšºt,
t = (t1, t2, . . . , tn)T
and the corresponding characteristic function is
ğœ‘X(t1, t2, . . . , tn) = eiğTtâˆ’1
2 tTğšºt,
t = (t1, t2, . . . , tn)T.
Integrable and Square Integrable Random Variables
Let X be a real-valued random variable
â€¢ If ğ”¼(|X|) < âˆthen X is an integrable random variable.
â€¢ If ğ”¼(X2) < âˆthen X is a square integrable random variable.
Convergence of Random Variables
Let X, X1, X2, . . . , Xn be a sequence of random variables. Then
(a) Xn
a.s.
âˆ’âˆ’âˆ’â†’X converges almost surely if
â„™
(
lim
nâ†’âˆXn = X
)
= 1.

354
Appendix B
(b) Xn
râˆ’âˆ’â†’X converges in the r-th mean, r â‰¥1, if ğ”¼(|Xr
n|) < âˆand
lim
nâ†’âˆğ”¼(|Xn âˆ’X|r) = 0.
(c) Xn
P
âˆ’âˆ’â†’X converges in probability, if for all ğœ€> 0,
lim
nâ†’âˆâ„™(|Xn âˆ’X| â‰¥ğœ€) = 0.
(d) Xn
D
âˆ’âˆ’â†’X converges in distribution, if for all x âˆˆâ„,
lim
nâ†’âˆâ„™(Xn â‰¤x) = â„™(X â‰¤x).
Relationship Between Modes of Convergence
For any r â‰¥1
{
Xn
a.s
âˆ’âˆ’âˆ’â†’X
Xn
râˆ’âˆ’â†’X
}
â‡’{Xn
P
âˆ’âˆ’â†’X} â‡’{Xn
D
âˆ’âˆ’â†’X}.
If r > s â‰¥1 then
{Xn
râˆ’âˆ’â†’X} â‡’{Xn
sâˆ’âˆ’â†’X}.
Dominated Convergence Theorem
If Xn
a.s.
âˆ’âˆ’âˆ’â†’X and for any n âˆˆâ„•we have |Xn| < Y for some Y such that ğ”¼(|Y|) < âˆ, then
ğ”¼(|Xn|) < âˆand
lim
nâ†’âˆğ”¼(Xn) = ğ”¼(X).
Monotone Convergence Theorem
If 0 â‰¤Xn â‰¤Xn+1 and Xn
a.s.
âˆ’âˆ’âˆ’â†’X for any n âˆˆâ„•then
lim
nâ†’âˆğ”¼(Xn) = ğ”¼(X).
The Weak Law of Large Numbers
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean ğœ‡âˆˆâ„. Then for any ğœ€> 0,
â„™
(
lim
nâ†’âˆ
||||
X1 + X2 + . . . + Xn
n
âˆ’ğœ‡
||||
â‰¥ğœ€
)
= 0.

Appendix B
355
The Strong Law of Large Numbers
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean ğœ‡âˆˆâ„. Then
â„™
(
lim
nâ†’âˆ
X1 + X2 + . . . + Xn
n
= ğœ‡
)
= 1.
The Central Limit Theorem
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean ğœ‡âˆˆâ„and variance ğœ2 > 0 and denote the sample mean as
X = X1 + X2 + . . . + Xn
n
where ğ”¼
(
X
)
= ğœ‡and Var
(
X
)
= ğœ2
n . By defining
Zn =
X âˆ’ğ”¼
(
X
)
âˆš
Var
(
X
) = X âˆ’ğœ‡
ğœâˆ•
âˆš
n
then for n â†’âˆ,
lim
nâ†’âˆZn = lim
nâ†’âˆ
X âˆ’ğœ‡
ğœâˆ•
âˆš
n
D
âˆ’âˆ’â†’ğ’©(0, 1).
That is, Zn follows a standard normal distribution asymptotically.


Appendix C
Differential Equations Formulae
Separable Equations
The form
dy
dx = f(x)g(y)
has a solution
âˆ«
1
g(y) dy = âˆ«f(x) dx.
If g(y) is a linear equation and if y1 and y2 are two solutions, then y3 = ay1 + by2 is also a
solution for constant a and b.
First-Order Ordinary Differential Equations
General Linear Equation: The general form of a first-order ordinary differential equation
dy
dx + f(x)y = g(x)
has a solution
y = I(x)âˆ’1
âˆ«I(u)g(u) du + C
where I(x) = eâˆ«f(x)dx is the integrating factor and C is a constant.
Bernoulli Differential Equation: For n â‰ 1, the Bernoulli differential equation has the form
dy
dx + P(x)y = Q(x)yn
which, by setting ğ‘¤=
1
ynâˆ’1 , can be transformed to a general linear ordinary differential
equation of the form
dğ‘¤
dx + (1 âˆ’n)P(x)ğ‘¤= (1 âˆ’n)Q(x)
with a particular solution
ğ‘¤= (1 âˆ’n)I(x)âˆ’1
âˆ«I(u)Q(u) du

358
Appendix C
where I(x) = e(1âˆ’n) âˆ«P(x)dx is the integrating factor. The solution to the Bernoulli differential
equation becomes
y =
{
(1 âˆ’n)I(x)âˆ’1
âˆ«I(u)Q(u) du
}âˆ’1
nâˆ’1
+ C
where C is a constant value.
Second-Order Ordinary Differential Equations
General Linear Equation: For a homogeneous equation,
ad2y
dx2 + bdy
dx + cy = 0.
By setting y = eux the differential equation has a general solution based on the characteristic
equation
au2 + bu + c = 0
such that m1 and m2 are the roots of the quadratic equation, and if
â€¢ m1, m2 âˆˆâ„, m1 â‰ m2 then y = Aem1x + Bem2x
â€¢ m1, m2 âˆˆâ„, m1 = m2 = m then y = emx(A + Bx)
â€¢ m1, m2 âˆˆâ„‚, m1 = ğ›¼+ iğ›½, m2 = ğ›¼âˆ’iğ›½then y = eğ›¼x[A cos(ğ›½x) + B sin(ğ›½x)]
where A, B are constants.
Cauchyâ€“Euler Equation: For a homogeneous equation,
ax2 d2y
dx2 + bxdy
dx + cy = 0.
By setting y = xu the Cauchyâ€“Euler equation has a general solution based on the characteristic
equation
au2 + (b âˆ’a)u + c = 0
such that m1 and m2 are the roots of the quadratic equation, and if
â€¢ m1, m2 âˆˆâ„, m1 â‰ m2 then y = Axm1 + Bxm2
â€¢ m1, m2 âˆˆâ„, m1 = m2 = m then y = xm(A + B log x)
â€¢ m1, m2 âˆˆâ„‚, m1 = ğ›¼+ iğ›½, m2 = ğ›¼âˆ’iğ›½then y = xğ›¼[A cos(ğ›½log x) + B sin(ğ›½log x)]
where A, B are constants.
Variation of Parameters: For a general non-homogeneous second-order differential equation,
a(x)d2y
dx2 + b(x)dy
dx + c(x) = f(x)

Appendix C
359
has the solution
y = yc + yp
where yc, the complementary function, satisfies the homogeneous equation
a(x)d2yc
dx2 + b(x)dyc
dx + c(x) = 0
and yp, the particular integral, satisfies
a(x)
d2yp
dx2 + b(x)
dyp
dx + c(x) = f(x).
Let yc = C1y(1)
c (x) + C2y(2)
c (x) where C1 and C2 are constants, then the particular solution to
the non-homogeneous second-order differential equation is
yp = âˆ’y(1)
c (x) âˆ«
y(2)
c (x)f(x)
a(x)W(y(1)
c (x), y(2)
c (x))
dx + y(2)
c (x) âˆ«
y(1)
c (x)f(x)
a(x)W(y(1)
c (x), y(2)
c (x))
dx
where W(y(1)
c (x), y(2)
c (x)) is the Wronskian defined as
W(y(1)
c (x), y(2)
c (x)) =
||||||||||
y(1)
c (x)
y(2)
c (x)
d
dxy(1)
c (x) d
dxy(2)
c (x)
||||||||||
= y(1)
c (x) d
dxy(2)
c (x) âˆ’y(2)
c (x) d
dxy(1)
c (x) â‰ 0.
Homogeneous Heat Equations
Initial Value Problem on an Infinite Interval: The diffusion equation of the form
ğœ•u
ğœ•t = ğ›¼ğœ•2u
ğœ•x2 ,
ğ›¼> 0,
âˆ’âˆ< x < âˆ,
t > 0
with initial condition u(x, 0) = f(x) has a solution
u(x, t) =
1
2
âˆš
ğœ‹ğ›¼t âˆ«
âˆ
âˆ’âˆ
f(z)eâˆ’(xâˆ’z)2
4ğ›¼t
dz.
Initial Value Problem on a Semi-Infinite Interval: The diffusion equation of the form
ğœ•u
ğœ•t = ğ›¼ğœ•2u
ğœ•x2 ,
ğ›¼> 0,
0 â‰¤x < âˆ,
t > 0

360
Appendix C
with
â€¢ initial condition u(x, 0) = f(x) and boundary condition u(0, t) = 0 has a solution
u(x, t) =
1
2
âˆš
ğœ‹ğ›¼t âˆ«
âˆ
0
f(z)
[
eâˆ’(xâˆ’z)2
4ğ›¼t
âˆ’eâˆ’(x+z)2
4ğ›¼t
]
dz
â€¢ initial condition u(x, 0) = f(x) and boundary condition ux(0, t) = 0 has a solution
u(x, t) =
1
2
âˆš
ğœ‹ğ›¼t âˆ«
âˆ
0
f(z)
[
eâˆ’(xâˆ’z)2
4ğ›¼t
+ eâˆ’(x+z)2
4ğ›¼t
]
dz
â€¢ initial condition u(x, 0) = 0 and boundary condition u(0, t) = g(t) has a solution
u(x, t) =
x
2
âˆš
ğœ‹ğ›¼âˆ«
t
0
1
âˆš
t âˆ’ğ‘¤
g(ğ‘¤)eâˆ’
x2
4ğ›¼(tâˆ’ğ‘¤)
dğ‘¤.
Stochastic Differential Equations
Suppose Xt, Yt and Zt are ItÂ¯o processes satisfying the following stochastic differential
equations:
dXt = ğœ‡(Xt, t)dt + ğœ(Xt, t)dWx
t
dYt = ğœ‡(Yt, t)dt + ğœ(Yt, t))dWy
t
dZt = ğœ‡(Zt, t)dt + ğœ(Zt, t)dWz
t
where Wx
t , Wy
t and Wz
t are standard Wiener processes.
Reciprocal:
d
(
1
Xt
)
(
1
Xt
) = âˆ’dXt
Xt
+
(dXt
Xt
)2
.
Product:
d(XtYt)
XtYt
= dXt
Xt
+ dYt
Yt
+ dXt
Xt
dYt
Yt
.
Quotient:
d
(Xt
Yt
)
(Xt
Yt
) = dXt
Xt
âˆ’dYt
Yt
âˆ’dXt
Xt
dYt
Yt
+
(dYt
Yt
)2
.
Product and Quotient I:
d
(XtYt
Zt
)
(XtYt
Zt
) = dXt
Xt
+ dYt
Yt
âˆ’dZt
Zt
+ dXt
Xt
dYt
Yt
âˆ’dXt
Xt
dZt
Zt
âˆ’dYt
Yt
dZt
Zt
+
(dZt
Zt
)2
.

Appendix C
361
Product and Quotient II:
d
( Xt
YtZt
)
( Xt
YtZt
) = dXt
Xt
âˆ’dYt
Yt
âˆ’dZt
Zt
âˆ’dXt
Xt
dYt
Yt
âˆ’dXt
Xt
dZt
Zt
+ dYt
Yt
dZt
Zt
.
Blackâ€“Scholes Model
Blackâ€“Scholes Equation (Continuous Dividend Yield): At time t, let the asset price St follows
a geometric Brownian motion
dSt
St
= (ğœ‡âˆ’D)dt + ğœdWt
where ğœ‡is the drift parameter, D is the continuous dividend yield, ğœis the volatility parameter
and Wt is a standard Wiener process. For a European-style derivative V(St, t) written on the
asset St, it satisfies the Blackâ€“Scholes equation with continuous dividend yield
ğœ•V
ğœ•t + 1
2ğœ2S2
t
ğœ•2V
ğœ•S2
t
+ (r âˆ’D)St
ğœ•V
ğœ•St
âˆ’rV(St, t) = 0
where r is the risk-free interest rate. The parameters ğœ‡, r, D and ğœcan be either constants,
deterministic functions or stochastic processes.
European Options: For a European option having the payoff
Î¨(ST) = max{ğ›¿(ST âˆ’K), 0}
where ğ›¿âˆˆ{ âˆ’1, 1}, K is the strike price and T is the option expiry time, and if r, D and ğœare
constants the European option price at time t < T is
V(St, t; K, T) = ğ›¿Steâˆ’D(Tâˆ’t)Î¦(ğ›¿d+) âˆ’ğ›¿Keâˆ’r(Tâˆ’t)Î¦(ğ›¿dâˆ’)
where dÂ± =
log(Stâˆ•K) + (r âˆ’D Â± 1
2ğœ2)(T âˆ’t)
ğœ
âˆš
T âˆ’t
and Î¦(â‹…) is the cumulative distribution function
of a standard normal.
Reflection Principle: If V(St, t) is a solution of the Blackâ€“Scholes equation then for a constant
B > 0, the function
U(St, t) =
(St
B
)2ğ›¼
V
(
B2
St
, t
)
,
ğ›¼= 1
2
(
1 âˆ’r âˆ’D
1
2ğœ2
)
also satisfies the Blackâ€“Scholes equation.

362
Appendix C
Black Model
Black Equation: At time t, let the asset price St follows a geometric Brownian motion
dSt
St
= (ğœ‡âˆ’D)dt + ğœdWt
where ğœ‡is the drift parameter, D is the continuous dividend yield, ğœis the volatility parameter
and Wt is a standard Wiener process. Consider the price of a futures contract maturing at time
T > t on the asset St as
F(t, T) = Ste(râˆ’D)(Tâˆ’t)
where r is the risk-free interest rate. For a European option on futures V(F(t, T), t) written on
a futures contract F(t, T), it satisfies the Black equation
ğœ•V
ğœ•t + 1
2ğœ2F(t, T)2 ğœ•2V
ğœ•F2 âˆ’rV(F(t, T), t) = 0.
The parameters ğœ‡, r, D and ğœcan be either constants, deterministic functions or stochastic
processes.
European Options on Futures: For a European option on futures having the payoff
Î¨(F(T, T)) = max{ğ›¿(F(T, T) âˆ’K), 0}
where ğ›¿âˆˆ{ âˆ’1, 1}, K is the strike price and T is the option expiry time, and if r and ğœare
constants the price of a European option on futures at time t < T is
V(F(t, T), t; K, T) = ğ›¿eâˆ’r(Tâˆ’t)[F(t, T)Î¦(ğ›¿d+) âˆ’KÎ¦(ğ›¿dâˆ’)]
where dÂ± =
log(F(t, T)âˆ•K) Â± 1
2ğœ2(T âˆ’t)
ğœ
âˆš
T âˆ’t
and Î¦(â‹…) is the cumulative distribution function of a
standard normal.
Reflection Principle: If V(F(t, T), t) is a solution of the Black equation then for a constant
B > 0, the function
U(F(t, T), t) = F(t, T)
B
V
(
B2
F(t, T), t
)
also satisfies the Black equation.
Garmanâ€“Kohlhagen Model
Garmanâ€“Kohlhagen Equation: At time t, let the foreign-to-domestic exchange rate Xt follows
a geometric Brownian motion
dXt
Xt
= ğœ‡dt + ğœdWt

Appendix C
363
where ğœ‡is the drift parameter, ğœis the volatility parameter and Wt is a standard Wiener
process. For a European-style derivative V(Xt, t) which depends on Xt, it satisfies the
Garmanâ€“Kohlhagen equation
ğœ•V
ğœ•t + 1
2ğœ2X2
t
ğœ•2V
ğœ•X2
t
+ (rd âˆ’rf )Xt
ğœ•V
ğœ•Xt
âˆ’rdV(Xt, t) = 0
where rd and rf are the domestic and foreign currency risk-free interest rates. The parameters
ğœ‡, rd, rf and ğœcan be either constants, deterministic functions or stochastic processes.
European Options: For a European option having the payoff
Î¨(XT) = max{ğ›¿(XT âˆ’K), 0}
where ğ›¿âˆˆ{ âˆ’1, 1}, K is the strike price and T is the option expiry time, and if rd, rf and ğœ
are constants the European option price (domestic currency in one unit of foreign currency) at
time t < T is
V(Xt, t; K, T) = ğ›¿Xteâˆ’rf (Tâˆ’t)Î¦(ğ›¿d+) âˆ’ğ›¿Keâˆ’rd(Tâˆ’t)Î¦(ğ›¿dâˆ’)
where dÂ± =
log(Xtâˆ•K) + (rd âˆ’rf Â± 1
2ğœ2)(T âˆ’t)
ğœ
âˆš
T âˆ’t
and Î¦(â‹…) is the cumulative distribution func-
tion of a standard normal.
Reflection Principle: If V(Xt, t) is a solution of the Garmanâ€“Kohlhagen equation then for a
constant B > 0 the function
U(Xt, t) =
(Xt
B
)2ğ›¼
V
(
B2
Xt
, t
)
,
ğ›¼= 1
2
(
1 âˆ’
rd âˆ’rf
1
2ğœ2
)
also satisfies the Garmanâ€“Kohlhagen equation.


Bibliography
Abramovitz, M. and Stegun, I.A. (1970). Handbook of Mathematical Functions: with Formulas, Graphs
and Mathematical Tables. Dover Publications, New York.
Bachelier, L. (1900). ThÃ©orie de la spÃ©culation. Annales Scientifiques de lâ€™Ã‰cole Normale SupÃ©rieure,
3(17), pp. 21â€“86.
Baz, J. and Chacko, G. (2004). Financial Derivatives: Pricing, Applications and Mathematics.
Cambridge University Press, Cambridge.
BjÃ¶rk, T. (2009). Arbitrage Theory in Continuous Time, 3rd edn. Oxford Finance Series, Oxford Univer-
sity Press, Oxford.
Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political
Economy, 81, pp. 637â€“654.
BrezeÂ´zniak, Z. and Zastawniak, T. (1999). Basic Stochastic Processes. Springer-Verlag, Berlin.
Brown, R. (1828). A brief account of microscopical observations made in the months of June, July and
August 1827 on the particles contained in the pollen of plants. The Miscellaneous Botanical Works
of Robert Brown: Volume 1, John J. Bennet (ed), R. Hardwicke, London.
CapiÂ´nski, M., Kopp, E. and Traple, J. (2012). Stochastic Calculus for Finance. Cambridge University
Press, Cambridge.
Clark, I.J. (2011). Foreign Exchange Option Pricing: A Practitionerâ€™s Guide. John Wiley and Sons Ltd,
Chichester, United Kingdom.
Clark, I.J. (2014). Commodity Option Pricing: A Practitionerâ€™s Guide. John Wiley and Sons Ltd, Chich-
ester, United Kingdom.
Clewlow, S. and Strickland, C. (1999). Valuing energy options in a one factor model fitted to for-
ward prices. Working Paper, School of Finance and Economics, University of Technology, Sydney,
Australia.
Cont, R. and Tankov, P. (2003). Financial Modelling with Jump Processes. Chapman and Hall/CRC,
London.
Cox, J.C. (1996). The constant elasticity of variance option pricing model. The Journal of Portfolio
Management, Special Issue, pp. 15â€“17.
Cox, J.C., Ingersoll, J.E. and Ross, S.A. (1985). A theory of the term structure of interest rates. Econo-
metrica, 53, pp. 385â€“407.
Dufresne, D. (2001). The integrated square-root process. Research Paper Number 90, Centre for Actuarial
Studies, Department of Economics, University of Melbourne, Australia.
Einstein, A. (1956). Investigations of the Theory of Brownian Movement. Republication of the original
1926 translation, Dover Publications Inc.
Fama, E. (1965). The behavior of stock market prices. Journal of Business, 38(1), pp. 34â€“105.
Gabillon, J. (1991). The term structures of oil futures prices. Working Paper, Oxford Institute for Energy
Studies, Oxford, UK.

366
Bibliography
Garman, M.B. and Kohlhagen, S.W. (1983). Foreign currency option values. Journal of International
Money and Finance, 2, pp. 231â€“237.
Geman, H. (2005). Commodities and Commodity Derivatives: Modelling and Pricing for Agriculturals,
Metals and Energy. John Wiley & Sons, Chichester.
Girsanov, I. (1960). On transforming a certain class of stochastic processes by absolutely continuous
substitution of measures. SIAM Theory of Probability and Applications, 5(3), pp. 285â€“301.
Gradshteyn, I.S. and Ryzhik, I.M. (1980). Table of Integrals, Series and Products, Jeffrey, A. (ed.).
Academic Press, New York.
Grimmett, G. and Strizaker, D. (2001). Probability and Random Processes, 3rd edn. Oxford University
Press, Oxford.
Harrison, J.M. and Kreps, D.M. (1979). Martingales and arbitrage in multiperiod securities markets.
Journal of Economic Theory, 20, pp. 381â€“408.
Harrison, J.M. and Pliska, S.R. (1981). Martingales and stochastic integrals in the theory of continuous
trading. Stochastic Processes and their Applications, 11, pp. 215â€“260.
Harrison, J.M. and Pliska, S.R. (1983). A stochastic calculus model of continuous trading: Complete
markets. Stochastic Processes and their Applications, 15, pp. 313â€“316.
Heston, S.L. (1993). A closed-form solution for options with stochastic volatility with applications to
bond and currency options. The Review of Financial Studies, 6(2), pp. 327â€“343.
Ho, S.Y. and Lee, S.B. (2004). The Oxford Guide to Financial Modeling: Applications for Capital-
Markets, Corporate Finance, Risk Management and Financial Institutions. Oxford University Press,
Oxford, United Kingdom.
Hsu, Y.L., Lin, T.I. and Lee, C.F. (2008). Constant elasticity of variance (CEV) option pricing model:
Integration and detail derivation. Mathematics and Computers in Simulation, 79(1), pp. 60â€“71.
Hull, J. (2011). Options, Futures, and Other Derivatives, 6th edn. Prentice Hall, Englewood Cliffs, NJ.
ItÂ¯o, K. (1951). On stochastic differential equations: Memoirs. American Mathematical Society, 4,
pp. 1â€“51.
Joshi, M. (2008). The Concepts and Practice of Mathematical Finance, 2nd edn. Cambridge University
Press, Cambridge.
Joshi, M. (2011). More Mathematical Finance. Pilot Whale Press, Melbourne.
Jowett, B. and Campbell, L. (1894). Platoâ€™s Republic, The Greek Text, Edited with Notes and Essays.
Clarendon Press, Oxford.
Kac, M. (1949). On distributions of certain Wiener functionals. Transactions of the American Mathe-
matical Society, 65(1), pp. 1â€“13.
Karatzas, I. and Shreve, S.E. (2004). Brownian Motion and Stochastic Calculus, 2nd edn.
Springer-Verlag, Berlin.
Kluge, T. (2006). Pricing Swing Options and Other Electricity Derivatives. DPhil Thesis, University of
Oxford.
Kolmogorov, A. (1931), Ãœber die analytischen Methoden in der Wahrscheinlichkeitsrechnung. Mathe-
matische Annalen, 104, pp. 415â€“458.
Kou, S.G. (2002). A jump-diffusion model for option pricing. Management Science, 48, pp. 1086â€“1101.
Kwok, Y.K. (2008). Mathematical Models of Financial Derivatives, 2nd edn. Springer-Verlag, Berlin.
Lucia, J.J. and Schwartz, E.S. (2002). Electricity prices and power derivatives: Evidence from the Nordic
Power Exchange. Review of Derivatives Research, 5, pp. 5â€“50.
Merton, R. (1973). The theory of rational option pricing. Bell Journal of Economics and Management
Science, 4, pp. 141â€“183.
Merton, R. (1976). Option pricing when underlying stock returns are discontinuous. Journal of Financial
Economics, 3, pp. 125â€“144.
Musiela, M. and Rutkowski, M. (2007). Martingale Methods in Financial Modelling, 2nd edn.
Springer-Verlag, Berlin.
Ã˜ksendal, B. (2003). Stochastic Differential Equations: An Introduction with Applications, 6th edn.
Springer-Verlag, Heidelberg.

Bibliography
367
Rice, J.A. (2007). Mathematical Statistics and Data Analysis, 3rd edn. Duxbury, Belmont, CA.
Ross, S. (2002). A First Course in Probability, 6th edn. Prentice Hall, Englewood Cliffs, NJ.
Samuelson, P.A. (1965). Rational theory of warrant pricing. Industrial Management Review, 6(2), pp.
13â€“31.
Shreve, S.E. (2005). Stochastic Calculus for Finance; Volume I: The Binomial Asset Pricing Models.
Springer-Verlag, New York.
Shreve, S.E. (2008). Stochastic Calculus for Finance; Volume II: Continuous-Time Models.
Springer-Verlag, New York.
Stulz, R.M. (1982). Options on the minimum or the maximum of two risky assets: Analysis and appli-
cations. Journal of Financial Economics, 10, pp. 161â€“185.
Wiener, N. (1923). Differential space. Journal of Mathematical Physics, 2, pp. 131â€“174.
Wilmott, P., Dewynne, J. and Howison, S. (1993). Option Pricing: Mathematical Models and Computa-
tion. Oxford Financial Press, Oxford.


Notation
SET NOTATION
âˆˆ
is an element of
âˆ‰
is not an element of
Î©
sample space
â„°
universal set
âˆ…
empty set
A
subset of Î©
Ac
complement of set A
|A|
cardinality of A
â„•
set of natural numbers, {1, 2, 3, . . .}
â„•0
set of natural numbers including zero, {0, 1, 2, . . .}
â„¤
set of integers, {0, Â±1, Â±2, Â±3, . . .}
â„¤+
set of positive integers, {1, 2, 3, . . .}
â„
set of real numbers
â„+
set of positive real numbers, {x âˆˆâ„âˆ¶x > 0}
â„‚
set of complex numbers
A Ã— B
cartesian product of sets A and B, A Ã— B = {(a, b) âˆ¶a âˆˆA, b âˆˆB}
a âˆ¼b
a is equivalent to b
âŠ†
subset
âŠ‚
proper subset
âˆ©
intersection
âˆª
union
\
difference
Î”
symmetric difference
sup
supremum or least upper bound
inf
infimum or greatest lower bound
[a, b]
the closed interval {x âˆˆâ„âˆ¶a â‰¤x â‰¤b}
[a, b)
the interval {x âˆˆâ„âˆ¶a â‰¤x < b}
(a, b]
the interval {x âˆˆâ„âˆ¶a < x â‰¤b}
(a, b)
the open interval {x âˆˆâ„âˆ¶a < x < b}
â„±, ğ’¢, â„‹
ğœ-algebra (or ğœ-fields)

370
Notation
MATHEMATICAL NOTATION
x+
max{x, 0}
xâˆ’
min{x, 0}
âŒŠxâŒ‹
largest integer not greater than and equal to x, max{m âˆˆâ„¤| m â‰¤x}
âŒˆxâŒ‰
smallest integer greater than and equal to x, min{n âˆˆâ„¤| n â‰¥x}
x âˆ¨y
max{x, y}
x âˆ§y
min{x, y}
i
âˆš
âˆ’1
âˆ
infinity
âˆƒ
there exists
âˆƒ!
there exists a unique
âˆ€
for all
â‰ˆ
approximately equal to
p =â‡’q
p implies q
p â‡= q
p is implied by q
p â‡â‡’q
p implies and is implied by q
f âˆ¶X î‚¶â†’Y
f is a function where every element of X has an image in Y
f(x)
the value of the function f at x
lim
xâ†’a f(x)
limit of f(x) as x tends to a
ğ›¿x, Î”x
increment of x
f âˆ’1(x)
the inverse function of the function f(x)
f
â€²(x), f
â€²â€²(x)
the first and second-order derivative of the function f(x)
dy
dx, d2y
dx2
first and second-order derivative of y with respect to x
âˆ«y dx, âˆ«b
a y dx
the indefinite and definite integral of y with respect to x
ğœ•f
ğœ•xi
, ğœ•2f
ğœ•x2
i
first and second-order partial derivative of f with respect to xi
where f is a function on (x1, x2, . . . , xn)
ğœ•2f
ğœ•xiğœ•xj
second-order partial derivative of f with respect to xi and xj
where f is a function on (x1, x2, . . . , xn)
logax
logarithm of x to the base a
log x
natural logarithm of x
nâˆ‘
i=1
ai
a1 + a2 + . . . + an
nâˆ
i=1
ai
a1 Ã— a2 Ã— . . . Ã— an
|a|
modulus of a
(
nâˆš
a
)m
a
m
n
n!
n factorial
(n
k
)
n!
k!(n âˆ’k)! for n, k âˆˆâ„¤+
ğ›¿(x)
Dirac delta function
H(x)
Heaviside step function

Notation
371
Î“(t)
gamma function
B(x, y)
beta function
a
a vector a
|a|
magnitude of a vector a
a â‹…b
scalar or dot product of vectors a and b
a Ã— b
vector or cross-product of vectors a and b
M
a matrix M
MT
transpose of a matrix M
Mâˆ’1
inverse of a square matrix M
|M|
determinant of a square matrix M
PROBABILITY NOTATION
A, B, C
events
1IA
indicator of the event A
â„™, â„š
probability measures
â„™(A)
probability of event A
â„™(A|B)
probability of event A conditional on event B
X, Y, Z
random variables
X, Y, Z
random vectors
â„™(X = x)
probability mass function of a discrete random variable X
fX(x)
probability density function of a continuous random variable X
FX(x), â„™(X â‰¤x)
cumulative distribution function of a random variable X
MX(t)
moment generating function of a random variable X
ğœ‘X(t)
characteristic function of a random variable X
P(X = x, Y = y)
joint probability mass function of discrete variables X and Y
fXY(x, y)
joint probability density function of continuous random
variables X and Y
FXY(x, y), â„™(X â‰¤x, Y â‰¤y)
joint cumulative distribution function of random variables X
and Y
MXY(s, t)
joint moment generating function of random variables X and Y
ğœ‘XY(s, t)
joint characteristic function of random variables X and Y
p(x, t; y, T)
transition probability density of y at time T starting at time t at
point x
âˆ¼
is distributed as
â‰
is not distributed as
âˆ»
is approximately distributed as
a.s
âˆ’âˆ’âˆ’â†’
converges almost surely
râˆ’âˆ’â†’
converges in the r-th mean
P
âˆ’âˆ’â†’
converges in probability
D
âˆ’âˆ’â†’
converges in distribution
X
d= Y
X and Y are identically distributed random variables

372
Notation
X âŸ‚âŸ‚Y
X and Y are independent random variables
X âˆ•âŸ‚âŸ‚Y
X and Y are not independent random variables
ğ”¼(X)
expectation of random variable X
ğ”¼â„š(X)
expectation of random variable X under the probability
measure â„š
ğ”¼[g(X)]
expectation of g(X)
ğ”¼(X|â„±)
conditional expectation of X
Var(X)
variance of random variable X
Var(X|â„±)
conditional variance of X
Cov(X, Y)
covariance of random variables X and Y
ğœŒxy
correlation between random variables X and Y
Bernoulli(p)
Bernoulli distribution with mean p and variance p(1 âˆ’p)
Geometric(p)
geometric distribution with mean pâˆ’1 and variance (1 âˆ’p)pâˆ’2
Binomial(n, p)
binomial distribution with mean np and variance np(1 âˆ’p)
BN(n, r)
negative binomial distribution with mean rpâˆ’1 and variance
r(1 âˆ’p)pâˆ’2
Poisson(ğœ†)
Poisson distribution with mean ğœ†and variance ğœ†
Exp(ğœ†)
exponential distribution with mean ğœ†âˆ’1 and variance ğœ†âˆ’2
Gamma(ğ›¼, ğœ†)
gamma distribution with mean ğ›¼ğœ†âˆ’1 and variance ğ›¼ğœ†âˆ’2
ğ’°(a, b)
uniform distribution with mean 1
2(a + b) and variance
1
12(b âˆ’a)2
ğ’©(ğœ‡, ğœ2)
normal distribution with mean ğœ‡and variance ğœ2
log-ğ’©(ğœ‡, ğœ2)
lognormal distribution with mean eğœ‡+ 1
2 ğœ2 and variance
(eğœ2 âˆ’1)e2ğœ‡+ğœ2
ğœ’2(k)
chi-square distribution with mean k and variance 2k
ğ’©n(ğ, ğšº)
multivariate normal distribution with n-dimensional mean
vector ğœ‡and n Ã— n covariance matrix ğšº
Î¦(â‹…), Î¦(x)
cumulative distribution function of a standard normal
ğš½(x, y, ğœŒxy)
cumulative distribution function of a standard bivariate normal
with correlation coefficient ğœŒxy
Wt
standard Wiener process, Wt âˆ¼ğ’©(0, t)
Nt
Poisson process, Nt âˆ¼Poisson(ğœ†t)

Index
adapted stochastic processes, 2, 303
admissible trading strategy, 186
American options, 53
appendices, 1, 331â€“63
arbitrage, 53, 185â€“7, 232, 322â€“30
arithmetic Brownian motion, 128â€“9, 222â€“3,
229â€“30
see also Bachelier model
stock price with continuous dividend
yield, 229â€“30
arithmetic series, formulae, 332
arrival time distribution, Poisson
process, 256â€“8
see also stock...
fundamental theories, 232â€“5
attainable contingent claim, 186â€“7
Bachelier model
see also arithmetic Brownian motion
definition and formulae, 128â€“9
backward Kolmogorov equation, 97, 98â€“102,
149â€“50, 153â€“4,
180â€“1
see also diffusion; parabolic...
definition, 97, 98â€“9, 149â€“50, 153â€“4, 180â€“1
multi-dimensional diffusion
process, 99â€“102, 155â€“83
one-dimensional diffusion process, 87â€“8,
123â€“55
one-dimensional random walk, 153â€“4
two-dimensional random walk, 180â€“1
Bayesâ€™ Formula, 7
Bayesâ€™ rule, 341
Bernoulli differential equation, 357â€“8
Bernoulli distribution, 11, 12, 13, 14â€“15, 349
Bessel process, 163â€“6
beta function, 338
binomial distribution, 12â€“14, 349â€“50
bivariate continuous random variables, 345â€“7
see also continuous...
bivariate discrete random variables, 343â€“4
see also discrete...
bivariate normal distribution, 27â€“9, 34â€“40,
60â€“2, 158, 165, 352â€“3
see also normal distribution
covariance, 28â€“9, 57â€“9, 158, 165
marginal distributions, 27
Black equation, 362
Black model, 362
Blackâ€“Scholes equation, 54, 96, 236â€“8, 361
see also partial differential equations
reflection principle, 54â€“5, 361
Blackâ€“Scholes model, 129â€“30, 185, 236â€“8,
361
see also geometric Brownian motion
Bonferroniâ€™s inequality, 7
Booleâ€™s inequality, 6, 7, 10
Borelâ€“Cantelli lemma, 10â€“11
Brownian bridge process, 137â€“8
Brownian motion, 51â€“93, 95â€“183, 185â€“92,
221â€“4, 227â€“8, 361, 362â€“3
see also arithmetic...; diffusion...;
geometric...; random walks; Wiener
processes
definitions and formulae, 51, 128â€“32,
138â€“9, 221â€“4, 227â€“30
cÃ dlÃ g process, 246
Cauchyâ€“Euler equation, 358â€“9
cdf see cumulative distribution function
central limit theorem, 12, 13, 57, 355
CEV see constant elasticity of variance model
change of measure, 43, 185â€“242, 249â€“51,
301â€“30
see also Girsanovâ€™s theorem
definitions, 185â€“92, 249â€“51
Chebyshevâ€™s inequality, 40â€“1, 75
chi-square distribution, 24â€“6, 352
CIR see Coxâ€“Ingersollâ€“Ross model
Clewlowâ€“Strickland 1-factor model, 141â€“4

374
Index
compensated Poisson process, 245, 262â€“3,
266â€“7, 323â€“6, 330
see also Poisson...
complement, probability concepts, 1, 4â€“5, 341
complete market, 187, 233, 322â€“30
compound Poisson process, 243, 245â€“6,
249â€“51, 264â€“88, 306â€“22, 323â€“30
see also Poisson...
decomposition, 268â€“9, 306â€“8
Girsanovâ€™s theorem, 249â€“51
martingales, 265â€“7, 323â€“5
concave function, 339
conditional, probability concepts, 341
conditional expectation, 2â€“3, 43â€“9, 52â€“5,
75â€“6, 150â€“3, 192â€“3, 197â€“200, 273â€“4,
284â€“5, 341, 343, 346
conditional Jensenâ€™s inequality, properties of
conditional expectation, 3, 48â€“9, 75â€“6,
192â€“4
conditional probability
density function formulae, 346
mass function formulae, 343
properties of conditional expectation, 3,
43â€“5
conditional variance, 343, 346
constant elasticity of variance model
(CEV), 145â€“6
contingent claims, 185â€“92, 245
see also derivatives
continuous distributions, 350â€“3
convergence of random variables, 10â€“11,
209â€“11, 353â€“4
convex function, 3, 42, 48â€“9, 75â€“6, 192â€“4,
338â€“9
convolution formulae, 25â€“6, 348
countable unions, 1â€“2
counting process, 243â€“330
see also Poisson...
definition and formulae, 243â€“4
covariance, 28â€“9, 57â€“60, 64â€“8, 117â€“18,
157â€“8, 165â€“6, 251â€“2, 344, 347
matrices, 59, 64â€“8
covariance of bivariate normal
distribution, 28â€“9, 57â€“60, 158, 165â€“6
covariance of two standard Wiener
processes, 57â€“60, 64â€“8
Cox process (doubly stochastic Poisson
process), 243, 245
see also Poisson process
Coxâ€“Ingersollâ€“Ross model (CIR), 135â€“7,
171â€“4, 178
cumulative distribution function (cdf), 16â€“20,
22â€“4, 30â€“2, 37â€“40, 214â€“18, 342, 343,
345, 361â€“3
cumulative intensity, 245
see also intensity...
De Moivreâ€™s formula, 333
De Morganâ€™s law, 4, 7, 10
decomposition of a compound Poisson
process, 268â€“9, 306â€“8
differential equations, 52, 90â€“3, 95â€“183,
224â€“42, 253â€“5, 305â€“30, 357â€“63
see also ordinary...; partial...; stochastic...
differential-difference equations, 253â€“5
Dirac delta function, 339
see also Heaviside step function
discounted portfolio value, 187â€“92, 224â€“7,
228â€“32, 242, 324â€“30
discrete distributions, 349â€“50
discrete-time martingales, 53
dominated convergence theorem, 354
Donsker theorem, 56â€“7
Doobâ€™s maximal inequality, 76â€“80
elementary process, 96
equivalent martingale measure, 185, 188â€“9,
209â€“11, 222â€“5, 230â€“1, 234â€“5, 238â€“42,
325â€“30
see also risk-neutral...
Eulerâ€™s formula, 333
events, definition, 1, 243â€“51
exclusion for probability, definition, 7â€“10
expectation, 2â€“3, 40â€“9, 52â€“5, 75â€“6, 90â€“1,
122â€“3, 125â€“6, 148â€“9, 157â€“8, 164â€“6,
192â€“4, 197â€“205, 270â€“2, 279â€“81,
305â€“30, 342, 343â€“7
exponential martingale process, 263â€“4
see also martingales
Feynmanâ€“Kac theorem, 97â€“102, 147â€“9,
178â€“80
see also diffusion
multi-dimensional diffusion process, 178â€“80
one-dimensional diffusion process, 147â€“9,
178
fields, definition, 1â€“2
filtration, 2â€“3, 52â€“5, 68â€“71, 75â€“82, 95â€“123,
128, 145â€“9, 178â€“80, 186â€“242, 244â€“51,
261â€“330
first fundamental theorem of asset pricing,
definition, 232
first passage time density function, 85â€“9,
218â€“19
first passage time of a standard Wiener
processes, 53, 76â€“89, 218â€“19
hitting a sloping line, 218â€“19
Laplace transform, 83â€“4
first-order ordinary differential
equations, 125â€“6, 171â€“74, 255, 265,
273â€“4, 306, 357â€“8
Fokkerâ€“Planck equation see forward
Kolmogorov equation

Index
375
folded normal distribution, 22â€“4, 72
see also normal distribution
foreign exchange (FX), 51, 54, 192, 238â€“42, 362
foreign-denominated stock price under
domestic risk-neutral measure, 241â€“2
risk-neutral measure, 238â€“42
forward curve from an asset price following a
geometric Brownian motion, 138â€“9
forward curve from an asset price following a
geometric mean-reverting
process, 139â€“40, 169â€“71
forward curves, 138â€“44
forward Kolmogorov equation, 97, 98â€“102,
150â€“3, 154â€“5, 181â€“3
see also diffusion; parabolic...
multi-dimensional diffusion process, 102
one-dimensional diffusion process, 150â€“3
one-dimensional random walk, 154â€“5
two-dimensional random walk, 180â€“3
Fubiniâ€™s theorem, 339
FX see foreign exchange
Gabillon 2-factor model, 169â€“71
gamma distribution, 16â€“17, 257, 352
Garmanâ€“Kohlhagen equation/model, 362â€“3
general probability theory, concepts, 1â€“49,
185â€“92, 341â€“55
generalised Brownian motion, 130â€“2
generalised ItÂ¯o integral, 118â€“19
geometric average, 146â€“7
geometric Brownian motion, 70â€“1, 129â€“32,
138â€“9, 146â€“7, 221â€“2, 227â€“8, 361, 362â€“3
see also Blackâ€“Scholes options pricing
model; Brownian motion
definition and formulae, 129â€“32, 138â€“9,
146â€“7, 221â€“2, 227â€“8
forward curve from an asset price, 138â€“9
Markov property, 70â€“1
stock price with continuous dividend
yield, 227â€“8
geometric distribution, 349
geometric mean-reverting process, 134â€“5,
139â€“40, 169â€“71, 291â€“5
definition and formulae, 134â€“5, 139â€“40,
169â€“71, 291â€“5
forward curve from an asset price, 139â€“40,
169â€“71
jump-diffusion process, 291â€“5
geometric series, formulae, 332
Girsanovâ€™s theorem, 185, 189â€“92, 194â€“242,
249â€“51, 298â€“322
see also real-world measure; risk-neutral
measure
corollaries, 189â€“90
definitions, 185, 189â€“92, 194â€“225, 249â€“51,
298â€“322
formulae, 189â€“92, 194â€“225, 249â€“51,
298â€“322
jump processes, 298â€“322
Poisson process, 249â€“51, 298â€“322
running maximum and minimum of a Wiener
process, 214â€“18
hazard function, 245
see also intensity...
hazard process, 245
heat equations, 359â€“60
Heaviside step function, 339
see also Dirac delta function
Heston stochastic volatility model, 175â€“8
HÃ¶lderâ€™s inequality, 41â€“2, 72â€“4, 79, 193,
204â€“5
homogeneous heat equations, 359â€“60
homogeneous Poisson process see Poisson
process
hyperbolic functions, 74, 333
inclusion and exclusion for probability,
definition, 7â€“10
incomplete markets, 187, 324â€“30
independence, properties, 3, 11, 47â€“8, 163â€“6,
210, 259â€“61
independent events, probability
concepts, 259â€“61, 341, 344, 347
integrable random variable, 3, 44, 45â€“9, 353
see also random variables
integral calculus, definition, 95â€“6
integrated square-root process, 171â€“4
integration by parts, 19â€“20, 115â€“18, 147, 151,
174, 257â€“8, 337
ItÂ¯o integral, 102â€“7, 108â€“116, 118â€“20, 144,
148â€“9, 163â€“6, 177â€“8, 187â€“92, 195â€“6,
205â€“7
ItÂ¯o isometry, 113â€“14
ItÂ¯o processes, 54, 95â€“123, 360â€“1
ItÂ¯oâ€™s formula see ItÂ¯oâ€™s lemma
ItÂ¯oâ€™s lemma, 96â€“7, 99â€“100, 102â€“27, 129â€“33,
134â€“7, 139â€“46, 147â€“53, 155â€“62, 166â€“8,
170â€“80, 196, 203â€“5, 212â€“13, 220â€“7,
234â€“41, 246â€“51, 278â€“80, 283â€“98,
307â€“8, 314â€“22
see also stochastic differential equations;
Taylor series
multi-dimensional ItÂ¯o formulae for
jump-diffusion process, 247â€“9
one-dimensional ItÂ¯o formulae for
jump-diffusion process, 246â€“7
Jensenâ€™s inequality, properties of conditional
expectation, 3, 48â€“9, 75â€“6, 192â€“4
joint characteristic function, 344, 347

376
Index
joint cumulative distribution function, 16â€“17,
32, 212â€“13, 343, 345
joint distribution of standard Wiener
processes, 58â€“60, 64, 158, 165â€“6
joint moment generating function, 122â€“3, 267,
280â€“1, 306, 318â€“19, 321â€“2, 344, 347
joint probability density function, 16â€“17,
27â€“34, 86â€“9, 211â€“18, 345â€“6, 352
joint probability mass function, definition, 343
jump diffusion process, 246â€“51, 281â€“98,
298â€“30
see also diffusion; Poisson...; Wiener...
concepts, 246â€“51, 281â€“98, 325â€“30
geometric mean-reverting process, 291â€“5
Mertonâ€™s model, 285â€“8, 327â€“30
multi-dimensional ItÂ¯o formulae, 247â€“9
one-dimensional ItÂ¯o formulae, 246â€“7
pure jump process, 281â€“5, 323â€“5, 327â€“30
simple jump-diffusion process, 325â€“7
jumps, 95, 243â€“330
see also Poisson process
Girsanovâ€™s theorem, 298â€“322
risk-neutral measure, 322â€“30
Kouâ€™s model, 295â€“8
Laplace transform of first passage time, 83â€“4
laws of large numbers, formulae, 354â€“5
Lebesgueâ€“Stieltjes integral, 246
LÃ©vy processes, 55, 119â€“23, 207, 209
Lâ€™Hospital rule, formulae, 336
linearity, properties of conditional
expectation, 3, 44, 45
lognormal distribution, 20â€“2, 129â€“32, 134â€“5,
142â€“4, 169â€“71, 283â€“8, 351
Maclaurin series, 335
marginal distributions of bivariate normal
distribution, 27
marginal probability density function, 27â€“9,
346
marginal probability mass function, 343
Markov property of a geometric Brownian
motion, 70â€“1
Markov property of Poisson process, 244â€“5,
261â€“2, 268â€“9
Markov property of Wiener processes, 52,
68â€“71, 97
definition and formulae, 51â€“2, 68â€“71
Markovâ€™s inequality, definition and
formulae, 40
martingale representation theorem, 187â€“8,
190, 192â€“4, 219â€“26
martingales, 52â€“3, 71â€“84, 96â€“123, 127â€“8,
185â€“242, 245, 262â€“7, 279â€“81, 299â€“330
see also equivalent...; stochastic processes;
Wiener processes
compound Poisson process, 265â€“8, 322â€“4
continuous processes property, 53
discrete processes property, 53
theorems, 53â€“5, 187â€“92
maximum of two correlated normal
distributions, definition, 29â€“32
mean value theorem, 105â€“6
mean-reversion, 134â€“5, 139â€“44, 169â€“71,
288â€“95
see also geometric mean-reverting process
measurability, properties of conditional
expectation, 3, 43â€“4, 45, 46, 199â€“200
measurable space, definition, 2
measure theory, definition, 2
measures, 2, 185â€“242, 249â€“51,
301â€“30
see also Girsanovâ€™s theorem; real-world...;
risk-neutral...
change of measure, 185â€“242, 249â€“51,
301â€“30
Mertonâ€™s model, definition and
formulae, 285â€“8, 327â€“30
minimum and maximum of two correlated
normal distributions, 29â€“32
Minkowskiâ€™s inequality, 42
monotone convergence theorem, 354
monotonicity, properties of conditional
expectation, 3, 44â€“45
multi-dimensional diffusion process
see also diffusion
backward Kolmogorov equation, 101â€“2
definition and formulae, 99â€“102,
155â€“83
Feynmanâ€“Kac theorem, 178â€“80
forward Kolmogorov equation, 101
problems and solutions, 155â€“83
multi-dimensional Girsanov theorem, 190â€“1,
208â€“9, 249â€“51
multi-dimensional ItÂ¯o formulae, 99â€“100
multi-dimensional ItÂ¯o formulae jump-diffusion
process, 247â€“9
multi-dimensional LÃ©vy characterisation
theorem, 121â€“3, 209
multi-dimensional martingale representation
theorem, 191â€“2
multi-dimensional Novikov condition,
207â€“8
multi-dimensional Wiener processes, 54, 64â€“8,
99â€“102, 163â€“6
multiplication, probability concepts, 341
multivariate normal distribution, 353
see also normal distribution
mutually exclusive events, probability
concepts, 12, 341

Index
377
negative binomial distribution, 350
normal distribution, 17â€“24, 29â€“32, 54, 62â€“3,
117â€“18, 128â€“9, 132â€“3, 135â€“8, 144,
170â€“1, 178, 206â€“7, 284â€“5, 348, 351,
361â€“3
see also bivariate...; folded...; log...;
multivariate...
minimum and maximum of two correlated
normal distributions, 29â€“32
Novikovâ€™s conditions, 194â€“6, 207â€“8
numÃ©raire, definition, 191â€“2
one-dimensional diffusion process, 97â€“9,
123â€“55, 178
see also diffusion
backward Kolmogorov equation, 149â€“50
definition and formulae, 97â€“9, 147â€“53
Feynmanâ€“Kac formulae, 147â€“9, 178
forward Kolmogorov equation, 150â€“53
one-dimensional Girsanov theorem, 189â€“90,
205â€“7
one-dimensional ItÂ¯o formulae for
jump-diffusion process, 246â€“7
one-dimensional LÃ©vy characterisation
theorem, 119â€“21, 207
one-dimensional martingale representation
theorem, 187â€“8, 190
one-dimensional random walk
see also random walks
backward Kolmogorov equation, 153â€“4
forward Kolmogorov equation, 154â€“5
optional stopping (sampling) theorem, 53,
80â€“4, 218â€“19, 232
ordinary differential equations, 125â€“6, 357â€“9
Ornsteinâ€“Uhlenbeck process,
definition, 132â€“3, 134â€“5, 288â€“91, 292â€“5
parabolic partial differential equations, 97
see also backward Kolmogorov...;
Blackâ€“Scholes...; diffusion...; forward
Kolmogorov...
partial averaging property, 3, 43, 45, 46â€“7,
201â€“3
partial differential equations (PDEs), 52,
97â€“102, 149â€“53, 178â€“80
see also backward Kolmogorov...;
Blackâ€“Scholes...; forward
Kolmogorov...; parabolic...
concepts, 97â€“102, 147â€“53, 168, 178â€“80
stochastic differential equations, 97â€“102,
168, 178â€“80
partition, probability concepts, 341
PDEs see partial differential equations
pdf see probability density function
physical measure see real-world measure
Poisson distribution, 13â€“14, 350
Poisson process, 95, 243â€“330
see also compensated...; compound...; Cox...;
jump...
Girsanovâ€™s theorem, 249â€“51, 298â€“322
Markov property, 244â€“51, 261â€“3, 268
positivity, properties of conditional
expectation, 3, 44
principle of inclusion and exclusion for
probability, definition, 7â€“10
probability density function (pdf), 14â€“17,
20â€“2, 24â€“30, 35â€“40, 58â€“60, 84â€“9, 98,
150â€“3, 180â€“3, 214â€“18, 249â€“51, 256â€“9,
344, 345, 348â€“53
probability mass function, 11â€“13, 250, 257â€“8,
306â€“30, 342, 348, 349
probability spaces, 2â€“3, 4â€“11, 43â€“9, 52â€“93,
187â€“242
definition, 2
probability theory, 1â€“49, 185, 189â€“92, 341â€“55
formulae, 341â€“55
properties of characteristic function, 348
properties of conditional expectation, 3, 41â€“9,
192â€“4, 197â€“202, 269â€“72, 284â€“5
properties of expectation, 3, 40â€“9, 75â€“6,
192â€“4, 197â€“202, 270â€“2, 284â€“5, 347
definition and formulae, 40â€“9, 347
problems and solutions, 40â€“9
properties of moment generating, 348
properties of normal distribution, 17â€“20,
34â€“40
properties of the Poisson process, problems and
solutions, 243â€“51, 251â€“81
properties of variance, 347
pure birth process, 255â€“6
pure jump process, definition and
formulae, 281â€“85, 323â€“5
quadratic variation property of Wiener
processes, 54, 89â€“93, 96â€“123, 206â€“7,
209, 274â€“7
Radonâ€“NikodÂ´ym derivative, 43, 188, 190,
191â€“2, 196â€“200, 212â€“13, 218â€“19,
222â€“5, 234â€“5, 239â€“42, 249â€“51, 298â€“330
random walks, 51â€“93, 95, 153â€“5, 180â€“3
see also Brownian motion; continuous-time
processes; symmetric...; Wiener
processes
definition, 51â€“5, 153â€“5, 180â€“3
real-world measure, 185, 189â€“242
see also Girsanovâ€™s theorem
definition, 185
reflection principle, 53â€“5, 84â€“9, 361â€“3
Black equation, 362
Blackâ€“Scholes equation, 54, 361
definition, 53â€“4, 60, 84â€“9

378
Index
reflection principle (continued)
Garmanâ€“Kohlhagen equation, 362â€“3
Wiener processes, 53â€“5, 60, 84â€“9
risk-neutral measure, 53, 185, 188â€“242,
322â€“30
see also equivalent martingale...; Girsanovâ€™s
theorem
definition, 185, 188â€“9, 221â€“42, 322â€“30
FX, 238â€“42
jump processes, 322â€“30
problems and solutions, 221â€“42,
322â€“30
running maximum and minimum of a Wiener
process, Girsanovâ€™s theorem, 214â€“18
sample space, definition, 1, 4, 341â€“2
scaled symmetric random walk, 51â€“5
see also random walks
SDEs see stochastic differential equations
second fundamental theorem of asset pricing,
definition, 233
second-order ordinary differential equations
formulae, 358â€“9
variation of parameters, 358â€“9
self-financing trading strategy, 186â€“8, 192,
225â€“7, 236â€“8, 326â€“30
sets, 1, 2â€“11
definition, 1
ğœ-sigma-algebra, 1â€“5, 43â€“9, 201â€“5, 261
simple jump process, 322â€“3
simple jump-diffusion process, 325â€“7
simple process see elementary process
skew, 54
speculation uses of derivatives, 185
square integrable random variable, 95â€“6, 353
standard Wiener processes, 51â€“93, 95â€“183,
185â€“242, 244â€“51, 277â€“81, 285â€“98,
303â€“30
covariance of two standard Wiener
processes, 57â€“60, 64â€“8, 117â€“18
definition, 52, 54, 189â€“90, 250â€“1, 325â€“6
joint distribution of standard Wiener
processes, 58â€“63, 64, 158â€“9, 165â€“6
stationary and independent increments, Poisson
process, 259â€“81
stochastic differential equations
(SDEs), 95â€“183, 237â€“42, 246â€“51,
315â€“30, 360â€“1
definition, 95â€“102, 246â€“9
integral calculus contrasts, 95â€“6
partial differential equations, 97â€“102, 168,
178â€“80
stochastic processes, 2, 51, 52, 185â€“242,
243â€“330
see also martingales; Poisson...; Wiener...
definitions, 2, 51, 52
stochastic volatility, 54, 95, 175â€“8
stopping times, Wiener processes, 53â€“5, 80â€“9,
218â€“19, 232
Stratonovich integral, 103â€“6
the strong law of large numbers, formulae, 355
strong Markov property, 52, 84â€“9
submartingales, 53, 75â€“80
supermartingales, 53, 76â€“9
symmetric random walk, 51â€“68, 88â€“9, 153â€“5,
180â€“3
see also random walks
time inversion, Wiener processes, 62â€“3
time reversal, Wiener processes, 63â€“4
time shifting, Wiener processes, 61
total probability of all possible values,
formulae, 342â€“5
tower property, conditional expectation, 3, 46,
49, 192â€“4, 197â€“9, 270â€“4, 284â€“5
trading strategy, 186â€“8, 191â€“2, 225â€“7, 236â€“8,
326â€“30
transition probability density function, 98â€“102,
149â€“53
see also backward Kolmogorov...; forward
Kolmogorov...
two-dimensional random walk
see also random walks
backward Kolmogorov equation, 180â€“1
forward Kolmogorov equation, 181â€“3
uniform distribution, 350
union, probability concepts, 341
univariate continuous random variables, 344â€“7
see also continuous...
univariate discrete random variables, 342â€“4
see also discrete...
variance, 13â€“49, 52â€“93, 115â€“83, 264â€“330,
342, 343, 345, 346, 347
see also covariance...
constant elasticity of variance model, 145â€“6
volatilities, 54, 95â€“183, 185â€“242, 285â€“330,
361â€“3
see also local...; stochastic...
the weak law of large numbers, formulae, 354
Wiener processes, 51â€“93, 95â€“183, 185â€“242,
242, 246â€“51, 277â€“81, 285â€“98, 303â€“30,
360â€“63
see also Brownian motion; diffusion...;
martingales; random walks
covariance of two standard Wiener
processes, 57â€“60, 64â€“8, 115â€“18

Index
379
first passage time, 53, 76â€“89, 218â€“19
joint distribution of standard Wiener
processes, 58â€“60, 64, 158, 165â€“6
Markov property, 52, 68â€“71, 97â€“102
multi-dimensional Wiener processes, 54,
64â€“8, 99â€“102, 163â€“6
quadratic variation property, 54, 89â€“93,
96â€“123, 206â€“7, 209, 274â€“5
reflection principle, 54â€“5, 60, 84â€“9
running maximum and minimum of a Wiener
process, 214â€“18

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wileyâ€™s ebook EULA.




Problems and Solutions
in Mathematical Finance

For other titles in the Wiley Finance Series
please see www.wiley.com/finance

Problems and Solutions
in Mathematical Finance
Volume 1: Stochastic Calculus
Eric Chin, Dian Nel and Sverrir Ólafsson

This edition first published 2014
© 2014 John Wiley & Sons, Ltd
Registered office
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial offices, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our website at www.wiley.com.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any
form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK
Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with
standard print versions of this book may not be included in e-books or in print-on-demand. If this book refers to
media such as a CD or DVD that is not included in the version you purchased, you may download this material at
http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their
respective owners. The publisher is not associated with any product or vendor mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing
this book, they make no representations or warranties with the respect to the accuracy or completeness of the
contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular
purpose. It is sold on the understanding that the publisher is not engaged in rendering professional services and
neither the publisher nor the author shall be liable for damages arising herefrom. If professional advice or other
expert assistance is required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Chin, Eric,
Problems and solutions in mathematical finance : stochastic calculus / Eric Chin, Dian Nel and Sverrir Ólafsson.
pages cm
Includes bibliographical references and index.
ISBN 978-1-119-96583-1 (cloth)
1. Finance – Mathematical models. 2. Stochastic analysis. I. Nel, Dian, II. Ólafsson, Sverrir, III. Title.
HG106.C495 2014
332.01′51922 – dc23
2013043864
A catalogue record for this book is available from the British Library.
ISBN 978-1-119-96583-1 (hardback) ISBN 978-1-119-96607-4 (ebk)
ISBN 978-1-119-96608-1 (ebk)
ISBN 978-1-118-84514-1 (ebk)
Cover design: Cylinder
Typeset in 10/12pt TimesLTStd by Laserwords Private Limited, Chennai, India
Printed in Great Britain by CPI Group (UK) Ltd, Croydon, CR0 4YY

“the beginning of a task is the biggest step”
Plato, The Republic


Contents
Preface
ix
Prologue
xi
About the Authors
xv
1
General Probability Theory
1
1.1 Introduction
1
1.2 Problems and Solutions
4
1.2.1 Probability Spaces
4
1.2.2 Discrete and Continuous Random Variables
11
1.2.3 Properties of Expectations
41
2
Wiener Process
51
2.1 Introduction
51
2.2 Problems and Solutions
55
2.2.1 Basic Properties
55
2.2.2 Markov Property
68
2.2.3 Martingale Property
71
2.2.4 First Passage Time
76
2.2.5 Reflection Principle
84
2.2.6 Quadratic Variation
89
3
Stochastic Differential Equations
95
3.1 Introduction
95
3.2 Problems and Solutions
102
3.2.1 It¯o Calculus
102
3.2.2 One-Dimensional Diffusion Process
123
3.2.3 Multi-Dimensional Diffusion Process
155
4
Change of Measure
185
4.1 Introduction
185
4.2 Problems and Solutions
192
4.2.1 Martingale Representation Theorem
192

viii
Contents
4.2.2 Girsanov’s Theorem
194
4.2.3 Risk-Neutral Measure
221
5
Poisson Process
243
5.1 Introduction
243
5.2 Problems and Solutions
251
5.2.1 Properties of Poisson Process
251
5.2.2 Jump Diffusion Process
281
5.2.3 Girsanov’s Theorem for Jump Processes
298
5.2.4 Risk-Neutral Measure for Jump Processes
322
Appendix A Mathematics Formulae
331
Appendix B Probability Theory Formulae
341
Appendix C Differential Equations Formulae
357
Bibliography
365
Notation
369
Index
373

Preface
Mathematical finance is based on highly rigorous and, on occasions, abstract mathematical
structures that need to be mastered by anyone who wants to be successful in this field, be it
working as a quant in a trading environment or as an academic researcher in the field. It may
appear strange, but it is true, that mathematical finance has turned into one of the most advanced
and sophisticated field in applied mathematics. This development has had considerable impact
on financial engineering with its extensive applications to the pricing of contingent claims
and synthetic cash flows as analysed both within financial institutions (investment banks) and
corporations. Successful understanding and application of financial engineering techniques to
highly relevant and practical situations requires the mastering of basic financial mathematics.
It is precisely for this purpose that this book series has been written.
In Volume I, the first of a four volume work, we develop briefly all the major mathematical
concepts and theorems required for modern mathematical finance. The text starts with prob-
ability theory and works across stochastic processes, with main focus on Wiener and Poisson
processes. It then moves to stochastic differential equations including change of measure and
martingale representation theorems. However, the main focus of the book remains practical.
After being introduced to the fundamental concepts the reader is invited to test his/her knowl-
edge on a whole range of different practical problems. Whereas most texts on mathematical
finance focus on an extensive development of the theoretical foundations with only occasional
concrete problems, our focus is a compact and self-contained presentation of the theoretical
foundations followed by extensive applications of the theory. We advocate a more balanced
approach enabling the reader to develop his/her understanding through a step-by-step collec-
tion of questions and answers. The necessary foundation to solve these problems is provided
in a compact form at the beginning of each chapter. In our view that is the most successful way
to master this very technical field.
No one can write a book on mathematical finance today, not to mention four volumes, without
being influenced, both in approach and presentation, by some excellent text books in the field.
The texts we have mostly drawn upon in our research and teaching are (in no particular order
of preference), Tomas Björk, Arbitrage Theory in Continuous Time; Steven Shreve, Stochastic
Calculus for Finance; Marek Musiela and Marek Rutkowski, Martingale Methods in Financial
Modelling and for the more practical aspects of derivatives John Hull, Options, Futures and

x
Preface
Other Derivatives. For the more mathematical treatment of stochastic calculus a very influen-
tial text is that of Bernt Øksendal, Stochastic Differential Equations. Other important texts are
listed in the bibliography.
Note to the student/reader. Please try hard to solve the problems on your own before you
look at the solutions!

Prologue
IN THE BEGINNING WAS THE MOTION. . .
The development of modern mathematical techniques for financial applications can be traced
back to Bachelier’s work, Theory of Speculation, first published as his PhD Thesis in 1900.
At that time Bachelier was studying the highly irregular movements in stock prices on the
French stock market. He was aware of the earlier work of the Scottish botanist Robert Brown,
in the year 1827, on the irregular movements of plant pollen when suspended in a fluid. Bache-
lier worked out the first mathematical model for the irregular pollen movements reported by
Brown, with the intention to apply it to the analysis of irregular asset prices. This was a highly
original and revolutionary approach to phenomena in finance. Since the publication of Bache-
lier’s PhD thesis, there has been a steady progress in the modelling of financial asset prices.
Few years later, in 1905, Albert Einstein formulated a more extensive theory of irregular molec-
ular processes, already then called Brownian motion. That work was continued and extended
in the 1920s by the mathematical physicist Norbert Wiener who developed a fully rigorous
framework for Brownian motion processes, now generally called Wiener processes.
Other major steps that paved the way for further development of mathematical
finance included the works by Kolmogorov on stochastic differential equations, Fama
on efficient-market hypothesis and Samuelson on randomly fluctuating forward prices.
Further important developments in mathematical finance were fuelled by the realisation of the
importance of It¯o’s lemma in stochastic calculus and the Feynman-Kac formula, originally
drawn from particle physics, in linking stochastic processes to partial differential equations
of parabolic type. The Feynman-Kac formula provides an immensely important tool for
the solution of partial differential equations “extracted” from stochastic processes via It¯o’s
lemma. The real relevance of It¯o’s lemma and Feynman-Kac formula in finance were only
realised after some further substantial developments had taken place.
The year 1973 saw the most important breakthrough in financial theory when Black and
Scholes and subsequently Merton derived a model that enabled the pricing of European call and
put options. Their work had immense practical implications and lead to an explosive increase
in the trading of derivative securities on some major stock and commodity exchanges. How-
ever, the philosophical foundation of that approach, which is based on the construction of
risk-neutral portfolios enables an elegant and practical way of pricing of derivative contracts,
has had a lasting and revolutionary impact on the whole of mathematical finance. The devel-
opment initiated by Black, Scholes and Merton was continued by various researchers, notably
Harrison, Kreps and Pliska in 1980s. These authors established the hugely important role of

xii
Prologue
martingales and arbitrage theory for the pricing of a large class of derivative securities or,
as they are generally called, contingent claims. Already in the Black, Scholes and Merton
model the risk-neutral measure had been informally introduced as a consequence of the con-
struction of risk-neutral portfolios. Harrison, Kreps and Pliska took this development further
and turned it into a powerful and the most general tool presently available for the pricing of
contingent claims.
Within the Harrison, Kreps and Pliska framework the change of numéraire technique plays
a fundamental role. Essentially the price of any asset, including contingent claims, can be
expressed in terms of units of any other asset. The unit asset plays the role of a numéraire. For
a given asset and a selected numéraire we can construct a probability measure that turns the
asset price, in units of the numéraire, into a martingale whose existence is equivalent to the
absence of an arbitrage opportunity. These results amount to the deepest and most fundamental
in modern financial theory and are therefore a core construct in mathematical finance.
In the wake of the recent financial crisis, which started in the second half of 2007, some
commentators and academics have voiced their opinion that financial mathematicians and their
techniques are to be blamed for what happened. The authors do not subscribe to this view. On
the contrary, they believe that to improve the robustness and the soundness of financial con-
tracts, an even better mathematical training for quants is required. This encompasses a better
comprehension of all tools in the quant’s technical toolbox such as optimisation, probability,
statistics, stochastic calculus and partial differential equations, just to name a few.
Financial market innovation is here to stay and not going anywhere, instead tighter regu-
lations and validations will be the only way forward with deeper understanding of models.
Therefore, new developments and market instruments requires more scrutiny, testing and val-
idation. Any inadequacies and weaknesses of model assumptions identified during the vali-
dation process should be addressed with appropriate reserve methodologies to offset sudden
changes in the market direction. The reserve methodologies can be subdivided into model
(e.g., Black-Scholes or Dupire model), implementation (e.g., tree-based or Monte Carlo sim-
ulation technique to price the contingent claim), calibration (e.g., types of algorithms to solve
optimization problems, interpolation and extrapolation methods when constructing volatility
surface), market parameters (e.g., confidence interval of correlation marking between under-
lyings) and market risk (e.g., when market price of a stock is close to the option’s strike price
at expiry time). These are the empirical aspects of mathematical finance that need to be a core
part in the further development of financial engineering.
One should keep in mind that mathematical finance is not, and must never become, an eso-
teric subject to be left to ivory tower academics alone, but a powerful tool for the analysis of
real financial scenarios, as faced by corporations and financial institutions alike. Mathematical
finance needs to be practiced in the real world for it to have sustainable benefits. Practitioners
must realise that mathematical analysis needs to be built on a clear formulation of financial
realities, followed by solid quantitative modelling, and then stress testing the model. It is our
view that the recent turmoil in financial markets calls for more careful application of quanti-
tative techniques but not their abolishment. Intuition alone or behavioural models have their
role to play but do not suffice when dealing with concrete financial realities such as, risk quan-
tification and risk management, asset and liability management, pricing insurance contracts
or complex financial instruments. These tasks require better and more relevant education for
quants and risk managers.
Financial mathematics is still a young and fast developing discipline. On the other hand, mar-
kets present an extremely complex and distributed system where a huge number of interrelated

Prologue
xiii
financial instruments are priced and traded. Financial mathematics is very powerful in pricing
and managing a limited number of instruments bundled into a portfolio. However, modern
financial mathematics is still rather poor at capturing the extremely intricate contractual inter-
relationship that exists between large numbers of traded securities. In other words, it is only
to a very limited extent able to capture the complex dynamics of the whole markets, which is
driven by a large number of unpredictable processes which possess varying degrees of corre-
lation. The emergent behaviour of the market is to an extent driven by these varying degrees of
correlations. It is perhaps one of the major present day challenges for financial mathematics to
join forces with modern theory of complexity with the aim of being able to capture the macro-
scopic properties of the market, that emerge from the microscopic interrelations between a
large number of individual securities. That this goal has not been reached yet is no criticism
of financial mathematics. It only bears witness to its juvenile nature and the huge complexity
of its subject.
Solid training of financial mathematicians in a whole range of quantitative disciplines,
including probability theory and stochastic calculus, is an important milestone in the further
development of the field. In the process, it is important to realise that financial engineering
needs more than just mathematics. It also needs a judgement where the quant should
constantly be reminded that no two market situations or two market instruments are exactly
the same. Applying the same mathematical tools to different situations reminds us of the
fact that we are always dealing with an approximation, which reflects the fact that we are
modelling stochastic processes i.e. uncertainties. Students and practitioners should always
bear this in mind.


About the Authors
Eric Chin is a quantitative analyst at an investment bank in the City of London where he
is involved in providing guidance on price testing methodologies and their implementation,
formulating model calibration and model appropriateness on commodity and credit products.
Prior to joining the banking industry he worked as a senior researcher at British Telecom inves-
tigating radio spectrum trading and risk management within the telecommunications sector. He
holds an MSc in Applied Statistics and an MSc in Mathematical Finance both from University
of Oxford. He also holds a PhD in Mathematics from University of Dundee.
Dian Nel has more than 10 years of experience in the commodities sector. He currently works
in the City of London where he specialises in oil and gas markets. He holds a BEng in Elec-
trical and Electronic Engineering from Stellenbosch University and an MSc in Mathematical
Finance from Christ Church, Oxford University. He is a Chartered Engineer registered with
the Engineering Council UK.
Sverrir Ólafsson is Professor of Financial Mathematics at Reykjavik University; a Visiting
Professor at Queen Mary University, London and a director of Riskcon Ltd, a UK based risk
management consultancy. Previously he was a Chief Researcher at BT Research and held
academic positions at The Mathematical Departments of Kings College, London; UMIST
Manchester and The University of Southampton. Dr Ólafsson is the author of over 95 ref-
ereed academic papers and has been a key note speaker at numerous international conferences
and seminars. He is on the editorial board of three international journals. He has provided an
extensive consultancy on financial risk management and given numerous specialist seminars to
finance specialists. In the last five years his main teaching has been MSc courses on Risk Man-
agement, Fixed Income, and Mathematical Finance. He has an MSc and PhD in mathematical
physics from the Universities of Tübingen and Karlsruhe respectively.


1
General Probability Theory
Probability theory is a branch of mathematics that deals with mathematical models of trials
whose outcomes depend on chance. Within the context of mathematical finance, we will review
some basic concepts of probability theory that are needed to begin solving stochastic calculus
problems. The topics covered in this chapter are by no means exhaustive but are sufficient to
be utilised in the following chapters and in later volumes. However, in order to fully grasp the
concepts, an undergraduate level of mathematics and probability theory is generally required
from the reader (see Appendices A and B for a quick review of some basic mathematics and
probability theory). In addition, the reader is also advised to refer to the notation section (pages
369–372) on set theory, mathematical and probability symbols used in this book.
1.1
INTRODUCTION
We consider an experiment or a trial whose result (outcome) is not predictable with certainty.
The set of all possible outcomes of an experiment is called the sample space and we denote it
by Ω. Any subset A of the sample space is known as an event, where an event is a set consisting
of possible outcomes of the experiment.
The collection of events can be defined as a subcollection ℱof the set of all subsets of Ω
and we define any collection ℱof subsets of Ω as a field if it satisfies the following.
Definition 1.1 The sample space Ω is the set of all possible outcomes of an experiment
or random trial. A field is a collection (or family) ℱof subsets of Ω with the following
conditions:
(a) ∅∈ℱwhere ∅is the empty set;
(b) if A ∈ℱthen Ac ∈ℱwhere Ac is the complement of A in Ω;
(c) if A1, A2, . . . , An ∈ℱ, n ≥2 then ⋃n
i=1 Ai ∈ℱ– that is to say, ℱis closed under finite
unions.
It should be noted in the definition of a field that ℱis closed under finite unions (as well
as under finite intersections). As for the case of a collection of events closed under countable
unions (as well as under countable intersections), any collection of subsets of Ω with such
properties is called a 𝜎-algebra.
Definition 1.2 If Ω is a given sample space, then a 𝜎-algebra (or 𝜎-field) ℱon Ω is a family
(or collection) ℱof subsets of Ω with the following properties:
(a) ∅∈ℱ;
(b) if A ∈ℱthen Ac ∈ℱwhere Ac is the complement of A in Ω;
(c) if A1, A2, . . . ∈ℱthen ⋃∞
i=1 Ai ∈ℱ– that is to say, ℱis closed under countable unions.

2
1.1
INTRODUCTION
We next outline an approach to probability which is a branch of measure theory. The reason
for taking a measure-theoretic path is that it leads to a unified treatment of both discrete and
continuous random variables, as well as a general definition of conditional expectation.
Definition 1.3 The pair (Ω, ℱ) is called a measurable space. A probability measure ℙon a
measurable space (Ω, ℱ) is a function ℙ∶ℱ→[0, 1] such that:
(a) ℙ(∅) = 0;
(b) ℙ(Ω) = 1;
(c) if A1, A2, . . . ∈ℱand {Ai}∞
i=1 is disjoint such that Ai ∩Aj = ∅, i ≠j then ℙ(⋃∞
i=1 Ai) =
∑∞
i=1 ℙ(Ai).
The triple (Ω, ℱ, ℙ) is called a probability space. It is called a complete probability space
if ℱalso contains subsets B of Ω with ℙ-outer measure zero, that is ℙ∗(B) = inf{ℙ(A) ∶A ∈
ℱ, B ⊂A} = 0.
By treating 𝜎-algebras as a record of information, we have the following definition of a
filtration.
Definition 1.4 Let Ω be a non-empty sample space and let T be a fixed positive number, and
assume for each t ∈[0, T] there is a 𝜎-algebra ℱt. In addition, we assume that if s ≤t, then
every set in ℱs is also in ℱt. We call the collection of 𝜎-algebras ℱt, 0 ≤t ≤T, a filtration.
Below we look into the definition of a real-valued random variable, which is a function that
maps a probability space (Ω, ℱ, ℙ) to a measurable space ℝ.
Definition 1.5 Let Ω be a non-empty sample space and let ℱbe a 𝜎-algebra of subsets of Ω.
A real-valued random variable X is a function X ∶Ω →ℝsuch that {𝜔∈Ω ∶X(𝜔) ≤x}
∈ℱfor each x ∈ℝand we say X is ℱmeasurable.
In the study of stochastic processes, an adapted stochastic process is one that cannot “see
into the future” and in mathematical finance we assume that asset prices and portfolio positions
taken at time t are all adapted to a filtration ℱt, which we regard as the flow of information
up to time t. Therefore, these values must be ℱt measurable (i.e., depend only on information
available to investors at time t). The following is the precise definition of an adapted stochastic
process.
Definition 1.6 Let Ω be a non-empty sample space with a filtration ℱt, t ∈[0, T] and let Xt
be a collection of random variables indexed by t ∈[0, T]. We therefore say that this collection
of random variables is an adapted stochastic process if, for each t, the random variable Xt is
ℱt measurable.
Finally, we consider the concept of conditional expectation, which is extremely important
in probability theory and also for its wide application in mathematical finance such as pricing
options and other derivative products. Conceptually, we consider a random variable X defined
on the probability space (Ω, ℱ, ℙ) and a sub-𝜎-algebra 𝒢of ℱ(i.e., sets in 𝒢are also in ℱ).
Here X can represent a quantity we want to estimate, say the price of a stock in the future, while

1.1
INTRODUCTION
3
𝒢contains limited information about X such as the stock price up to and including the current
time. Thus, 𝔼(X|𝒢) constitutes the best estimation we can make about X given the limited
knowledge 𝒢. The following is a formal definition of a conditional expectation.
Definition 1.7 (Conditional Expectation) Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be
a sub-𝜎-algebra of ℱ(i.e., sets in 𝒢are also in ℱ). Let X be an integrable (i.e., 𝔼(|X|) < ∞)
and non-negative random variable. Then the conditional expectation of X given 𝒢, denoted
𝔼(X|𝒢), is any random variable that satisfies:
(a) 𝔼(X|𝒢) is 𝒢measurable;
(b) for every set A ∈𝒢, we have the partial averaging property
∫A
𝔼(X|𝒢) dℙ= ∫A
X dℙ.
From the above definition, we can list the following properties of conditional expectation.
Here (Ω, ℱ, ℙ) is a probability space, 𝒢is a sub-𝜎-algebra of ℱand X is an integrable random
variable.
• Conditional probability. If 1IA is an indicator random variable for an event A then
𝔼(1IA|𝒢) = ℙ(A|𝒢).
• Linearity. If X1, X2, . . . , Xn are integrable random variables and c1, c2, . . . , cn are con-
stants then
𝔼(c1X1 + c2X2 + . . . + cnXn|𝒢) = c1𝔼(X1|𝒢) + c2𝔼(X2|𝒢) + . . . + cn𝔼(Xn|𝒢).
• Positivity. If X ≥0 almost surely then 𝔼(X|𝒢) ≥0 almost surely.
• Monotonicity. If X and Y are integrable random variables and X ≤Y almost surely then
𝔼(X|𝒢) ≤𝔼(Y|𝒢).
• Computing expectations by conditioning. 𝔼[𝔼(X|𝒢)] = 𝔼(X).
• Taking out what is known. If X and Y are integrable random variables and X is 𝒢measur-
able then
𝔼(XY|𝒢) = X ⋅𝔼(Y|𝒢).
• Tower property. If ℋis a sub-𝜎-algebra of 𝒢then
𝔼[𝔼(X|𝒢)|ℋ] = 𝔼(X|ℋ).
• Measurability. If X is 𝒢measurable then 𝔼(X|𝒢) = X.
• Independence. If X is independent of 𝒢then 𝔼(X|𝒢) = 𝔼(X).
• Conditional Jensen’s inequality. If 𝜑∶ℝ→ℝis a convex function then
𝔼[𝜑(X)|𝒢] ≥𝜑[𝔼(X|𝒢)].

4
1.2.1
Probability Spaces
1.2
PROBLEMS AND SOLUTIONS
1.2.1
Probability Spaces
1. De Morgan’s Law. Let Ai, i ∈I where I is some, possibly uncountable, indexing set.
Show that
(a) (⋃
i∈I Ai
)c = ⋂
i∈I Ac
i .
(b) (⋂
i∈I Ai
)c = ⋃
i∈I Ac
i .
Solution:
(a) Let a ∈(⋃
i∈I Ai
)c which implies a ∉⋃
i∈I Ai, so that a ∈Ac
i for all i ∈I. Therefore,
(
⋃
i∈I
Ai
)c
⊆
⋂
i∈I
Ac
i .
On the contrary, if we let a ∈⋂
i∈IAc
i then a ∉Ai for all i ∈I or a ∈(⋃
i∈I Ai
)c and
hence
⋂
i∈I
Ac
i ⊆
(
⋃
i∈I
Ai
)c
.
Therefore, (⋃
i∈I Ai
)c = ⋂
i∈IAc
i .
(b) From (a), we can write
(
⋃
i∈I
Ac
i
)c
=
⋂
i∈I
(Ac
i
)c =
⋂
i∈I
Ai.
Taking complements on both sides gives
(
⋂
i∈I
Ai
)c
=
⋃
i∈I
Ac
i .
◽
2. Let ℱbe a 𝜎-algebra of subsets of the sample space Ω. Show that if A1, A2, . . . ∈ℱthen
⋂∞
i=1 Ai ∈ℱ.
Solution: Given that ℱis a 𝜎-algebra then Ac
1, Ac
2, . . . ∈ℱand ⋃∞
i=1 Ac
i ∈ℱ. Further-
more, the complement of ⋃∞
i=1 Ac
i is (⋃∞
i=1 Ac
i
)c ∈ℱ.
Thus, from De Morgan’s law (see Problem 1.2.1.1, page 4) we have (⋃∞
i=1 Ac
i
)c =
⋂∞
i=1
(Ac
i
)c = ⋂∞
i=1 Ai ∈ℱ.
◽
3. Show that if ℱis a 𝜎-algebra of subsets of Ω then {∅, Ω} ∈ℱ.
Solution: ℱis a 𝜎-algebra of subsets of Ω, hence if A ∈ℱthen Ac ∈ℱ.
Since ∅∈ℱthen ∅c = Ω ∈ℱ. Thus, {∅, Ω} ∈ℱ.
◽

1.2.1
Probability Spaces
5
4. Show that if A ⊆Ω then ℱ= {∅, Ω, A, Ac} is a 𝜎-algebra of subsets of Ω.
Solution: ℱ= {∅, Ω, A, Ac} is a 𝜎-algebra of subsets of Ω since
(i) ∅∈ℱ.
(ii) For ∅∈ℱthen ∅c = Ω ∈ℱ. For Ω ∈ℱthen Ωc = ∅∈ℱ. In addition, for A ∈ℱ
then Ac ∈ℱ. Finally, for Ac ∈ℱthen (Ac)c = A ∈ℱ.
(iii) ∅∪Ω = Ω ∈ℱ,
∅∪A = A ∈ℱ,
∅∪Ac = Ac ∈ℱ,
Ω ∪A = Ω ∈ℱ,
Ω ∪
Ac = Ω ∈ℱ, ∅∪Ω ∪A = Ω ∈ℱ, ∅∪Ω ∪Ac = Ω ∈ℱand Ω ∪A ∪Ac = Ω ∈ℱ.
◽
5. Let {ℱi}i∈I, I ≠∅be a family of 𝜎-algebras of subsets of the sample space Ω. Show that
ℱ= ⋂
i∈I ℱi is also a 𝜎-algebra of subsets of Ω.
Solution: ℱ= ⋂
i∈I ℱi is a 𝜎-algebra by taking note that
(a) Since ∅∈ℱi, i ∈I therefore ∅∈ℱas well.
(b) If A ∈ℱi for all i ∈I then Ac ∈ℱi, i ∈I. Therefore, A ∈ℱand hence Ac ∈ℱ.
(c) If A1, A2, . . . ∈ℱi for all i ∈I then ⋃∞
k=1 Ak ∈ℱi, i ∈I and hence A1, A2, . . . ∈ℱ
and ⋃∞
k=1 Ak ∈ℱ.
From the results of (a)–(c) we have shown ℱ= ⋂
i∈Iℱi is a 𝜎-algebra of Ω.
◽
6. Let Ω = {𝛼, 𝛽, 𝛾} and let
ℱ1 = {∅, Ω, {𝛼}, {𝛽, 𝛾}}
and
ℱ2 = {∅, Ω, {𝛼, 𝛽}, {𝛾}}.
Show that ℱ1 and ℱ2 are 𝜎-algebras of subsets of Ω.
Is ℱ= ℱ1 ∪ℱ2 also a 𝜎-algebra of subsets of Ω?
Solution: Following the steps given in Problem 1.2.1.4 (page 5) we can easily show ℱ1
and ℱ2 are 𝜎-algebras of subsets of Ω.
By setting ℱ= ℱ1 ∪ℱ2 = {∅, Ω, {𝛼}, {𝛾}, {𝛼, 𝛽}, {𝛽, 𝛾}}, and since {𝛼} ∈ℱand {𝛾} ∈
ℱ, but {𝛼} ∪{𝛾} = {𝛼, 𝛾} ∉ℱ, then ℱ= ℱ1 ∪ℱ2 is not a 𝜎-algebra of subsets of Ω.
◽
7. Let ℱbe a 𝜎-algebra of subsets of Ω and suppose ℙ∶ℱ→[0, 1] so that ℙ(Ω) = 1. Show
that ℙ(∅) = 0.
Solution: Given that ∅and Ω are mutually exclusive we therefore have
∅∩Ω = ∅and ∅∪Ω = Ω.
Thus, we can express
ℙ(∅∪Ω) = ℙ(∅) + ℙ(Ω) −ℙ(∅∩Ω) = 1.
Since ℙ(Ω) = 1 and ℙ(∅∩Ω) = 0 therefore ℙ(∅) = 0.
◽

6
1.2.1
Probability Spaces
8. Let (Ω, ℱ, ℙ) be a probability space and let ℚ∶ℱ→[0, 1] be defined by ℚ(A) = ℙ(A|B)
where B ∈ℱsuch that ℙ(B) > 0. Show that (Ω, ℱ, ℚ) is also a probability space.
Solution: To show that (Ω, ℱ, ℚ) is a probability space we note that
(a) ℚ(∅) = ℙ(∅|B) = ℙ(∅∩B)
ℙ(B)
= ℙ(∅)
ℙ(B) = 0.
(b) ℚ(Ω) = ℙ(Ω|B) = ℙ(Ω ∩B)
ℙ(B)
= ℙ(B)
ℙ(B) = 1.
(c) Let A1, A2, . . . be disjoint members of ℱand hence we can imply A1 ∩B, A2 ∩B, . . .
are also disjoint members of ℱ. Therefore,
ℚ
( ∞
⋃
i=1
Ai
)
= ℙ
( ∞
⋃
i=1
Ai
|||||
B
)
=
ℙ(⋃∞
i=1(Ai ∩B))
ℙ(B)
=
∞
∑
i=1
ℙ(Ai ∩B)
ℙ(B)
=
∞
∑
i=1
ℚ(Ai).
Based on the results of (a)–(c), we have shown that (Ω, ℱ, ℚ) is also a probability space.
◽
9. Boole’s Inequality. Suppose {Ai}i∈I is a countable collection of events. Show that
ℙ
(
⋃
i∈I
Ai
)
≤
∑
i∈I
ℙ(Ai
) .
Solution: Without loss of generality we assume that I = {1, 2, . . .} and define B1 = A1,
Bi = Ai\(A1 ∪A2 ∪. . . ∪Ai−1), i ∈{2, 3, . . .} such that {B1, B2, . . .} are pairwise dis-
joint and
⋃
i∈I
Ai =
⋃
i∈I
Bi.
Because Bi ∩Bj = ∅, i ≠j, i, j ∈I we have
ℙ
(
⋃
i∈I
Ai
)
= ℙ
(
⋃
i∈I
Bi
)
=
∑
i∈I
ℙ(Bi)
=
∑
i∈I
ℙ(Ai\(A1 ∪A2 ∪. . . ∪Ai−1))
=
∑
i∈I
{ℙ(Ai) −ℙ(Ai ∩(A1 ∪A2 ∪. . . ∪Ai−1))}
≤
∑
i∈I
ℙ(Ai).
◽

1.2.1
Probability Spaces
7
10. Bonferroni’s Inequality. Suppose {Ai}i∈I is a countable collection of events. Show that
ℙ
(
⋂
i∈I
Ai
)
≥1 −
∑
i∈I
ℙ(Ac
i
) .
Solution: From De Morgan’s law (see Problem 1.2.1.1, page 4) we can write
ℙ
(
⋂
i∈I
Ai
)
= ℙ
((
⋃
i∈I
Ac
i
)c)
= 1 −ℙ
(
⋃
i∈I
Ac
i
)
.
By applying Boole’s inequality (see Problem 1.2.1.9, page 6) we will have
ℙ
(
⋂
i∈I
Ai
)
≥1 −
∑
i∈I
ℙ(Ac
i
)
since ℙ(⋃
i∈IAc
i
) ≤∑
i∈Iℙ(Ac
i
).
◽
11. Bayes’ Formula. Let A1, A2, . . . , An be a partition of Ω, where
n⋃
i=1
Ai = Ω, Ai ∩Aj = ∅,
i ≠j and each Ai, i, j = 1, 2, . . . , n has positive probability. Show that
ℙ(Ai|B) =
ℙ(B|Ai)ℙ(Ai)
∑n
j=1 ℙ(B|Aj)ℙ(Aj).
Solution: From the definition of conditional probability, for i = 1, 2, . . . , n
ℙ(Ai|B) = ℙ(Ai ∩B)
ℙ(B)
=
ℙ(B|Ai)ℙ(Ai)
ℙ
(⋃n
j=1(B ∩Aj)
) = ℙ(B|Ai)ℙ(Ai)
∑n
j=1 ℙ(B ∩Aj) =
ℙ(B|Ai)ℙ(Ai)
∑n
j=1 ℙ(B|Aj)ℙ(Aj).
◽
12. Principle of Inclusion and Exclusion for Probability. Let A1, A2, . . . , An, n ≥2 be a col-
lection of events. Show that
ℙ(A1 ∪A2) = ℙ(A1) + ℙ(A2) −ℙ(A1 ∩A2).
From the above result show that
ℙ(A1 ∪A2 ∪A3) = ℙ(A1) + ℙ(A2) + ℙ(A3) −ℙ(A1 ∩A2) −ℙ(A1 ∩A3)
−ℙ(A2 ∩A3) + ℙ(A1 ∩A2 ∩A3).
Hence, using mathematical induction show that
ℙ
( n
⋃
i=1
Ai
)
=
n
∑
i=1
ℙ(Ai) −
n−1
∑
i=1
n
∑
j=i+1
ℙ(Ai ∩Aj) +
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
ℙ(Ai ∩Aj ∩Ak)

8
1.2.1
Probability Spaces
−. . . + (−1)n+1ℙ(A1 ∩A2 ∩. . . ∩An).
Finally, deduce that
ℙ
( n
⋂
i=1
Ai
)
=
n
∑
i=1
ℙ(Ai) −
n−1
∑
i=1
n
∑
j=i+1
ℙ(Ai ∪Aj) +
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
ℙ(Ai ∪Aj ∪Ak)
−. . . + (−1)n+1ℙ(A1 ∪A2 ∪. . . ∪An).
Solution: For n = 2, A1 ∪A2 can be written as a union of two disjoint sets
A1 ∪A2 = A1 ∪(A2\ A1) = A1 ∪(A2 ∩Ac
1).
Therefore,
ℙ(A1 ∪A2) = ℙ(A1) + ℙ(A2 ∩Ac
1)
and since ℙ(A2) = ℙ(A2 ∩A1) + ℙ(A2 ∩Ac
1) we will have
ℙ(A1 ∪A2) = ℙ(A1) + ℙ(A2) −ℙ(A1 ∩A2).
For n = 3, and using the above results, we can write
ℙ(A1 ∪A2 ∪A3) = ℙ(A1 ∪A2) + ℙ(A3) −ℙ[(A1 ∪A2) ∩A3]
= ℙ(A1) + ℙ(A2) + ℙ(A3) −ℙ(A1 ∩A2) −ℙ[(A1 ∪A2) ∩A3].
Since (A1 ∪A2) ∩A3 = (A1 ∩A3) ∪(A2 ∩A3) therefore
ℙ[(A1 ∪A2) ∩A3] = ℙ[(A1 ∩A3) ∪(A2 ∩A3)]
= ℙ(A1 ∩A3) + ℙ(A2 ∩A3) −ℙ[(A1 ∩A3) ∩(A2 ∩A3)]
= ℙ(A1 ∩A3) + ℙ(A2 ∩A3) −ℙ(A1 ∩A2 ∩A3).
Thus,
ℙ(A1 ∪A2 ∪A3) = ℙ(A1) + ℙ(A2) + ℙ(A3) −ℙ(A1 ∩A2) −ℙ(A1 ∩A3)
−ℙ(A2 ∩A3) + ℙ(A1 ∩A2 ∩A3).
Suppose the result is true for n = m, where m ≥2. For n = m + 1, we have
ℙ
(( m
⋃
i=1
Ai
)
∪Am+1
)
= ℙ
( m
⋃
i=1
Ai
)
+ ℙ(Am+1
) −ℙ
(( m
⋃
i=1
Ai
)
∩Am+1
)
= ℙ
( m
⋃
i=1
Ai
)
+ ℙ(Am+1
) −ℙ
( m
⋃
i=1
(Ai ∩Am+1
)
)
.

1.2.1
Probability Spaces
9
By expanding the terms we have
ℙ
(m+1
⋃
i=1
Ai
)
=
m
∑
i=1
ℙ(Ai) −
m−1
∑
i=1
m
∑
j=i+1
ℙ(Ai ∩Aj) +
m−2
∑
i=1
m−1
∑
j=i+1
m
∑
k=j+1
ℙ(Ai ∩Aj ∩Ak)
−. . . + (−1)m+1ℙ(A1 ∩A2 ∩. . . ∩Am)
+ℙ(Am+1
) −ℙ((A1 ∩Am+1) ∪(A2 ∩Am+1) . . . ∪(Am ∩Am+1))
=
m+1
∑
i=1
ℙ(Ai) −
m−1
∑
i=1
m
∑
j=i+1
ℙ(Ai ∩Aj) +
m−2
∑
i=1
m−1
∑
j=i+1
m
∑
k=j+1
ℙ(Ai ∩Aj ∩Ak)
−. . . + (−1)m+1ℙ(A1 ∩A2 ∩. . . ∩Am)
−
m
∑
i=1
ℙ(Ai ∩Am+1) +
m−1
∑
i=1
m
∑
j=i+1
ℙ(Ai ∩Aj ∩Am+1)
+ . . . −(−1)m+1ℙ(A1 ∩A2 ∩. . . ∩Am+1)
=
m+1
∑
i=1
ℙ(Ai) −
m
∑
i=1
m+1
∑
j=i+1
ℙ(Ai ∩Aj) +
m−1
∑
i=1
m
∑
j=i+1
m+1
∑
k=j+1
ℙ(Ai ∩Aj ∩Ak)
−. . . + (−1)m+2ℙ(A1 ∩A2 ∩. . . ∩Am+1).
Therefore, the result is also true for n = m + 1. Thus, from mathematical induction we
have shown for n ≥2,
ℙ
( n
⋃
i=1
Ai
)
=
n
∑
i=1
ℙ(Ai) −
n−1
∑
i=1
n
∑
j=i+1
ℙ(Ai ∩Aj) +
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
ℙ(Ai ∩Aj ∩Ak)
−. . . + (−1)n+1ℙ(A1 ∩A2 ∩. . . ∩An).
From Problem 1.2.1.1 (page 4) we can write
ℙ
( n
⋂
i=1
Ai
)
= ℙ
(( n
⋃
i=1
Ac
i
)c)
= 1 −ℙ
( n
⋃
i=1
Ac
i
)
.
Thus, we can expand
ℙ
( n
⋂
i=1
Ai
)
= 1 −
n
∑
i=1
ℙ(Ac
i ) +
n−1
∑
i=1
n
∑
j=i+1
ℙ(Ac
i ∩Ac
j ) −
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
ℙ(Ac
i ∩Ac
j ∩Ac
k)
+ . . . −(−1)n+1ℙ(Ac
1 ∩Ac
2 ∩. . . ∩Ac
n)
= 1 −
n
∑
i=1
(1 −ℙ(Ai)) +
n−1
∑
i=1
n
∑
j=i+1
(1 −ℙ(Ai ∪Aj))
−
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
(1 −ℙ(Ai ∪Aj ∪Ak))

10
1.2.1
Probability Spaces
+ . . . −(−1)n+1(1 −ℙ(A1 ∪A2 ∪. . . ∪An))
=
n
∑
i=1
ℙ(Ai) −
n−1
∑
i=1
n
∑
j=i+1
ℙ(Ai ∪Aj) +
n−2
∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
ℙ(Ai ∪Aj ∪Ak)
−. . . + (−1)n+1ℙ(A1 ∪A2 ∪. . . ∪An).
◽
13. Borel–Cantelli Lemma. Let (Ω, ℱ, ℙ) be a probability space and let A1, A2, . . . be sets in
ℱ. Show that
∞
⋂
m=1
∞
⋃
k=m
Ak ⊆
∞
⋃
k=m
Ak
and hence prove that
ℙ
( ∞
⋂
m=1
∞
⋃
k=m
Ak
)
=
⎧
⎪
⎪
⎨
⎪
⎪⎩
1
if Ai ∩Aj = ∅, i ≠j and
∞∑
k=1
ℙ(Ak) = ∞
0
if
∞∑
k=1
ℙ(Ak) < ∞.
Solution: Let Bm =
∞⋃
k=m
Ak and since
∞
⋂
m=1
Bm ⊆Bm therefore we have
∞
⋂
m=1
∞
⋃
k=m
Ak ⊆
∞
⋃
k=m
Ak.
From Problem 1.2.1.9 (page 6) we can deduce that
ℙ
( ∞
⋂
m=1
∞
⋃
k=m
Ak
)
≤ℙ
( ∞
⋃
k=m
Ak
)
≤
∞
∑
k=m
ℙ(Ak).
For the case
∞∑
k=1
ℙ(Ak) < ∞and given it is a convergent series, then lim
m→∞
∞∑
k=m
ℙ(Ak) = 0
and hence
ℙ
( ∞
⋂
m=1
∞
⋃
k=m
Ak
)
= 0
if
∞∑
k=1
ℙ(Ak) < ∞.
For the case Ai ∩Aj = ∅, i ≠j and
∞∑
k=1
ℙ(Ak) = ∞, since ℙ
( ∞
⋃
k=m
Ak
)
+ ℙ
(( ∞⋃
k=m
Ak
)c)
=
1 therefore from Problem 1.2.1.1 (page 4)
ℙ
( ∞
⋃
k=m
Ak
)
= 1 −ℙ
(( ∞
⋃
k=m
Ak
)c)
= 1 −ℙ
( ∞
⋂
k=m
Ac
k
)
.

1.2.2
Discrete and Continuous Random Variables
11
From independence and because
∞∑
k=1
ℙ(Ak) = ∞we can express
ℙ
( ∞
⋂
k=m
Ac
k
)
=
∞
∏
k=m
ℙ(Ac
k
) =
∞
∏
k=m
(1 −ℙ(Ak
)) ≤
∞
∏
k=m
e−ℙ(Ak) = e−∑∞
k=m ℙ(Ak) = 0
for all m and hence
ℙ
( ∞
⋃
k=m
Ak
)
= 1 −ℙ
( ∞
⋂
k=m
Ac
k
)
= 1
for all m. Taking the limit m →∞,
ℙ
( ∞
⋂
m=1
∞
⋃
k=m
Ak
)
= lim
m→∞ℙ
( ∞
⋃
k=m
Ak
)
= 1
for the case Ai ∩Aj = ∅, i ≠j and
∞∑
k=1
ℙ(Ak) = ∞.
◽
1.2.2
Discrete and Continuous Random Variables
1. Bernoulli Distribution. Let X be a Bernoulli random variable, X ∼Bernoulli(p), p ∈[0, 1]
with probability mass function
ℙ(X = 1) = p,
ℙ(X = 0) = 1 −p.
Show that 𝔼(X) = p and Var(X) = p(1 −p).
Solution: If X ∼Bernoulli(p) then we can write
ℙ(X = x) = px(1 −p)1−x,
x ∈{0, 1}
and by definition
𝔼(X) =
1
∑
x=0
xℙ(X = x) = 0 ⋅(1 −p) + 1 ⋅p = p
𝔼(X2) =
1
∑
x=0
x2ℙ(X = x) = 0 ⋅(1 −p) + 1 ⋅p = p
and hence
𝔼(X) = p,
Var(X) = 𝔼(X2) −𝔼(X)2 = p(1 −p).
◽

12
1.2.2
Discrete and Continuous Random Variables
2. Binomial Distribution. Let {Xi}n
i=1 be a sequence of independent Bernoulli random vari-
ables each with probability mass function
ℙ(Xi = 1) = p,
ℙ(Xi = 0) = 1 −p,
p ∈[0, 1]
and let
X =
n
∑
i=1
Xi.
Show that X follows a binomial distribution, X ∼Binomial(n, p) with probability mass
function
ℙ(X = k) =
(
n
k
)
pk(1 −p)n−k,
k = 0, 1, 2, . . . , n
such that 𝔼(X) = np and Var(X) = np(1 −p).
Using the central limit theorem show that X is approximately normally distributed, X ∻
𝒩(np, np(1 −p)) as n →∞.
Solution: The random variable X counts the number of Bernoulli variables X1, . . . , Xn
that are equal to 1, i.e., the number of successes in the n independent trials. Clearly X takes
values in the set N = {0, 1, 2, . . . , n}. To calculate the probability that X = k, where k ∈N
is the number of successes we let E be the event such that Xi1 = Xi2 = . . . = Xik = 1 and
Xj = 0 for all j ∈N\S where S = {i1, i2, . . . , ik}. Then, because the Bernoulli variables
are independent and identically distributed,
ℙ(E) =
∏
j∈S
ℙ(Xj = 1)
∏
j∈N\S
ℙ(Xj = 0) = pk(1 −p)n−k.
However, as there are
(
n
k
)
combinations to select sets of indices i1, . . . , ik from N, which
are mutually exclusive events, so
ℙ(X = k) =
(
n
k
)
pk(1 −p)n−k,
k = 0, 1, 2, . . . , n.
From the definition of the moment generating function of discrete random variables (see
Appendix B),
MX(t) = 𝔼(etX) =
∑
x
etxℙ(X = x)
where t ∈ℝand substituting ℙ(X = x) =
(
n
x
)
px(1 −p)n−x we have
MX(t) =
n
∑
x=0
etx
(
n
x
)
px(1 −p)n−x =
n
∑
x=0
(
n
x
)
(pet)x(1 −p)n−x = (1 −p + pet)n.

1.2.2
Discrete and Continuous Random Variables
13
By differentiating MX(t) with respect to t twice we have
M′
X(t) = npet(1 −p + pet)n−1
M′′
X(t) = npet(1 −p + pet)n−1 + n(n −1)p2e2t(1 −p + pet)n−2
and hence
𝔼(X) = M′
X(0) = np
Var(X) = 𝔼(X2) −𝔼(X)2 = M′′
X(0) −M′
X(0)2 = np(1 −p).
Given the sequence Xi ∼Bernoulli(p), i = 1, 2, . . . , n are independent and identically dis-
tributed, each having expectation 𝜇= p and variance 𝜎2 = p(1 −p), then as n →∞, from
the central limit theorem
∑n
i=1 Xi −n𝜇
𝜎
√
n
D
−−→𝒩(0, 1)
or
X −np
√
np(1 −p)
D
−−→𝒩(0, 1).
Thus, as n →∞, X ∻𝒩(np, np(1 −p)).
◽
3. Poisson Distribution. A discrete Poisson distribution, Poisson(𝜆) with parameter 𝜆> 0
has the following probability mass function:
ℙ(X = k) = 𝜆k
k! e−𝜆,
k = 0, 1, 2, . . .
Show that 𝔼(X) = 𝜆and Var(X) = 𝜆.
For a random variable following a binomial distribution, Binomial(n, p), 0 ≤p ≤1 show
that as n →∞and with p = 𝜆∕n, the binomial distribution tends to the Poisson distribution
with parameter 𝜆.
Solution: From the definition of the moment generating function
MX(t) = 𝔼(etX) =
∑
x
etxℙ(X = x)
where t ∈ℝand substituting ℙ(X = x) = 𝜆x
x! e−𝜆we have
MX(t) =
∞
∑
x=0
etx 𝜆x
x! e−𝜆= e−𝜆
∞
∑
x=0
(𝜆et)x
x!
= e𝜆(et−1).
By differentiating MX(t) with respect to t twice
M′
X(t) = 𝜆ete𝜆(et−1),
M′′
X(t) = (𝜆et + 1)𝜆ete𝜆(et−1)

14
1.2.2
Discrete and Continuous Random Variables
we have
𝔼(X) = M′
X(0) = 𝜆
Var(X) = 𝔼(X2) −𝔼(X)2 = M
′′
X(0) −M
′
X(0)2 = 𝜆.
If X ∼Binomial(n, p) then we can write
ℙ(X = k) =
n!
k!(n −k)!pk(1 −p)n−k
= n(n −1) . . . (n −k + 1)(n −k)!
k!(n −k)!
× pk
(
1 −(n −k)p + (n −k)(n −k −1)
2!
p2 + . . .
)
= n(n −1) . . . (n −k + 1)
k!
pk
(
1 −(n −k)p + (n −k)(n −k −1)
2!
p2 + . . .
)
.
For the case when n →∞so that n ≫k we have
ℙ(X = k) ≈nk
k! pk
(
1 −np + (np)2
2!
+ . . .
)
= nk
k! pke−np.
By setting p = 𝜆∕n, we have ℙ(X = k) ≈𝜆k
k! e−𝜆.
◽
4. Exponential Distribution. Consider a continuous random variable X following an expo-
nential distribution, X ∼Exp(𝜆) with probability density function
fX(x) = 𝜆e−𝜆x,
x ≥0
where the parameter 𝜆> 0. Show that 𝔼(X) = 1
𝜆and Var(X) = 1
𝜆2 .
Prove that X ∼Exp(𝜆) has a memory less property given as
ℙ(X > s + x|X > s) = ℙ(X > x) = e−𝜆x,
x, s ≥0.
For a sequence of Bernoulli trials drawn from a Bernoulli distribution, Bernoulli(p), 0 ≤
p ≤1 performed at time Δt, 2Δt, . . . where Δt > 0 and if Y is the waiting time for the
first success, show that as Δt →0 and p →0 such that p∕Δt approaches a constant 𝜆> 0,
then Y ∼Exp(𝜆).
Solution: For t < 𝜆, the moment generating function for a random variable X ∼Exp(𝜆) is
MX(t) = 𝔼(etX) = ∫
∞
0
etu𝜆e−𝜆udu = 𝜆∫
∞
0
e−(𝜆−t)udu =
𝜆
𝜆−t.

1.2.2
Discrete and Continuous Random Variables
15
Differentiation of MX(t) with respect to t twice yields
M
′
X(t) =
𝜆
(𝜆−t)2 ,
M
′′
X (t) =
2𝜆
(𝜆−t)3 .
Therefore,
𝔼(X) = M
′
X(0) = 1
𝜆,
𝔼(X2) = M
′′
X(0) = 2
𝜆2
and the variance of X is
Var(X) = 𝔼(X2) −[𝔼(X)]2 = 1
𝜆2 .
By definition
ℙ(X > x) = ∫
∞
x
𝜆e−𝜆udu = e−𝜆x
and
ℙ(X > s + x|X > s) = ℙ(X > s + x, X > s)
ℙ(X > s)
= ℙ(X > s + x)
ℙ(X > s)
=
∫∞
s+x 𝜆e−𝜆udu
∫∞
s
𝜆e−𝜆𝑣d𝑣
= e−𝜆x.
Thus,
ℙ(X > s + x|X > s) = ℙ(X > x).
If Y is the waiting time for the first success then for k = 1, 2, . . .
ℙ(Y > kΔt) = (1 −p)k.
By setting y = kΔt, and in the limit Δt →0 and assuming that p →0 so that p∕Δt →𝜆,
for some positive constant 𝜆,
ℙ(Y > y) = ℙ
(
Y >
( x
Δt
)
Δt
)
≈(1 −𝜆Δt)y∕Δt
= 1 −𝜆y +
(
y
Δt
) (
y
Δt −1
)
2!
(𝜆Δt)2 + . . .
≈1 −𝜆y + (𝜆y)2
2!
+ . . .
= e−𝜆x.
In the limit Δt →0 and p →0,
ℙ(Y ≤y) = 1 −ℙ(Y > y) ≈1 −e−𝜆y
and the probability density function is therefore
fY(y) = d
dyℙ(Y ≤y) ≈𝜆e−𝜆y, y ≥0.
◽

16
1.2.2
Discrete and Continuous Random Variables
5. Gamma Distribution. Let U and V be continuous independent random variables and let
W = U + V. Show that the probability density function of W can be written as
fW(𝑤) = ∫
∞
−∞
fV(𝑤−u)fU(u) du = ∫
∞
−∞
fU(𝑤−𝑣)fV(𝑣) d𝑣
where fU(u) and fV(𝑣) are the density functions of U and V, respectively.
Let X1, X2, . . . , Xn ∼Exp(𝜆) be a sequence of independent and identically distributed ran-
dom variables, each following an exponential distribution with common parameter 𝜆> 0.
Prove that if
Y =
n
∑
i=1
Xi
then Y follows a gamma distribution, Y ∼Gamma (n, 𝜆) with the following probability
density function:
fY(y) = (𝜆y)n−1
(n −1)!𝜆e−𝜆y,
y ≥0.
Show also that 𝔼(Y) = n
𝜆and Var(Y) = n
𝜆2 .
Solution: From the definition of the cumulative distribution function of W = U + V we
obtain
FW(𝑤) = ℙ(W ≤𝑤) = ℙ(U + V ≤𝑤) = ∫∫
u+𝑣≤𝑤
fUV(u, 𝑣) dud𝑣
where fUV(u, 𝑣) is the joint probability density function of (U, V). Since U ⟂⟂V therefore
fUV(u, 𝑣) = fU(u)fV(𝑣) and hence
FW(𝑤) = ∫∫
u+𝑣≤𝑤
fUV(u, 𝑣) dud𝑣
= ∫∫
u+𝑣≤𝑤
fU(u)fV(𝑣) dud𝑣
= ∫
∞
−∞
{
∫
𝑤−u
−∞
fV(𝑣) d𝑣
}
fU(u) du
= ∫
∞
−∞
FV(𝑤−u)fU(u) du.
By differentiating FW(𝑤) with respect to 𝑤, we have the probability density function
fW(𝑤) given as
fW(𝑤) = d
d𝑤∫
∞
−∞
FV(𝑤−u)fU(u) du = ∫
∞
−∞
fV(𝑤−u)fU(u) du.

1.2.2
Discrete and Continuous Random Variables
17
Using the same steps we can also obtain
fW(𝑤) = ∫
∞
−∞
fU(𝑤−𝑣)fV(𝑣) d𝑣.
To show that Y = ∑n
i=1 Xi ∼Gamma (n, 𝜆) where X1, X2, . . . , Xn ∼Exp(𝜆), we will prove
the result via mathematical induction.
For n = 1, we have Y = X1 ∼Exp(𝜆) and the gamma density fY(y) becomes
fY(y) = 𝜆e−𝜆y,
y ≥0.
Therefore, the result is true for n = 1.
Let us assume that the result holds for n = k and we now wish to compute the density for
the case n = k + 1. Since X1, X2, . . . , Xk+1 are all mutually independent and identically
distributed, by setting U = ∑k
i=1 Xi and V = Xk+1, and since U ≥0, V ≥0, the density of
Y = ∑k
i=1 Xi + Xk+1 can be expressed as
fY(y) = ∫
y
0
fV(y −u)fU(u) du
= ∫
y
0
𝜆e−𝜆(y−u) ⋅(𝜆u)k−1
(k −1)!𝜆e−𝜆u du
= 𝜆k+1e−𝜆y
(k −1)! ∫
y
0
uk−1 du
= (𝜆y)k
k! 𝜆e−𝜆y
which shows the result is also true for n = k + 1. Thus, Y = ∑n
i=1 Xi ∼Gamma (n, 𝜆).
Given that X1, X2, . . . , Xn ∼Exp(𝜆) are independent and identically distributed with com-
mon mean 1
𝜆and variance 1
𝜆2 , therefore
𝔼(Y) = 𝔼
( n
∑
i=1
Xi
)
=
n
∑
i=1
𝔼(Xi) = n
𝜆
and
Var(Y) = Var
( n
∑
i=1
Xi
)
=
n
∑
i=1
Var(Xi) = n
𝜆2 .
◽
6. Normal Distribution Property I. Show that for constants a, L and U such that t > 0 and
L < U,
1
√
2𝜋t ∫
U
L
e
a𝑤−1
2
(
𝑤
√
t
)2
d𝑤= e
1
2 a2t
[
Φ
(
U −at
√
t
)
−Φ
(
L −at
√
t
)]
where Φ(⋅) is the cumulative distribution function of a standard normal.

18
1.2.2
Discrete and Continuous Random Variables
Solution: Simplifying the integrand we have
1
√
2𝜋t ∫
U
L
e
a𝑤−1
2
(
𝑤
√
t
)2
d𝑤=
1
√
2𝜋t ∫
U
L
e
−1
2
(
𝑤2−2a𝑤t
t
)
d𝑤
=
1
√
2𝜋t ∫
U
L
e
−1
2
[(
𝑤−at
√
t
)2
−a2t
]
d𝑤
= e
1
2 a2t
√
2𝜋t ∫
U
L
e
−1
2
(
𝑤−at
√
t
)2
d𝑤.
By setting x = 𝑤−at
√
t
we can write
∫
U
L
1
√
2𝜋t
e
−1
2
(
𝑤−at
√
t
)2
d𝑤= ∫
U−at
√
t
L−at
√
t
1
√
2𝜋
e−1
2 x2dx = Φ
(
U −at
√
t
)
−Φ
(
L −at
√
t
)
.
Thus,
1
√
2𝜋t ∫
U
L
e
a𝑤−1
2
(
𝑤
√
t
)2
d𝑤= e
1
2 a2t
[
Φ
(
U −at
√
t
)
−Φ
(
L −at
√
t
)]
.
◽
7. Normal Distribution Property II. Show that if X ∼𝒩(𝜇, 𝜎2) then for 𝛿∈{ −1, 1},
𝔼[max{𝛿(eX −K), 0}] = 𝛿e𝜇+ 1
2 𝜎2Φ
(𝛿(𝜇+ 𝜎2 −log K)
𝜎
)
−𝛿KΦ
(𝛿(𝜇−log K)
𝜎
)
where K > 0 and Φ(⋅) denotes the cumulative standard normal distribution function.
Solution: We first let 𝛿= 1,
𝔼[max{eX −K, 0}] = ∫
∞
log K
(ex −K) fX(x) dx
= ∫
∞
log K
(ex −K)
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
dx
= ∫
∞
log K
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
+x dx −K ∫
∞
log K
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
dx.
By setting 𝑤= x −𝜇
𝜎
and z = 𝑤−𝜎we have
𝔼[max{eX −K, 0}] = ∫
∞
log K−𝜇
𝜎
1
√
2𝜋
e−1
2 𝑤2+𝜎𝑤+𝜇d𝑤−K ∫
∞
log K−𝜇
𝜎
1
√
2𝜋
e−1
2 𝑤2d𝑤

1.2.2
Discrete and Continuous Random Variables
19
= e𝜇+ 1
2 𝜎2
∫
∞
log K−𝜇
𝜎
1
√
2𝜋
e−1
2 (𝑤−𝜎)2d𝑤−K ∫
∞
log K−𝜇
𝜎
1
√
2𝜋
e−1
2 𝑤2d𝑤
= e𝜇+ 1
2 𝜎2
∫
∞
log K−𝜇−𝜎2
𝜎
1
√
2𝜋
e−1
2 z2 dz −K ∫
∞
log K−𝜇
𝜎
1
√
2𝜋
e−1
2 𝑤2d𝑤
= e𝜇+ 1
2 𝜎2Φ
(𝜇+ 𝜎2 −log K
𝜎
)
−KΦ
(𝜇−log K
𝜎
)
.
Using similar steps for the case 𝛿= −1 we can also show that
𝔼[max{K −eX, 0}] = KΦ
(log K −𝜇
𝜎
)
−e𝜇+ 1
2 𝜎2Φ
(log K −(𝜇+ 𝜎2)
𝜎
)
.
◽
8. For x > 0 show that
(1
x −1
x3
)
1
√
2𝜋
e−1
2 x2 ≤∫
∞
x
1
√
2𝜋
e−1
2 z2 dz ≤
1
x
√
2𝜋
e−1
2 x2.
Solution: Solving ∫
∞
x
1
√
2𝜋
e−1
2 z2 dz using integration by parts, we let u = 1
z , d𝑣
dz =
z
√
2𝜋
e−1
2 z2 so that du
dz = −1
z2 and 𝑣= −
1
√
2𝜋
e−1
2 z2. Therefore,
∫
∞
x
1
√
2𝜋
e−1
2 z2 dz = −
1
z
√
2𝜋
e−1
2 z2||||||
∞
x
−∫
∞
x
1
z2√
2𝜋
e−1
2 z2 dz
=
1
x
√
2𝜋
e−1
2 x2 −∫
∞
x
1
z2√
2𝜋
e−1
2 z2 dz
≤
1
x
√
2𝜋
e−1
2 x2
since ∫
∞
x
1
z2√
2𝜋
e−1
2 z2 dz > 0.
To obtain the lower bound, we integrate ∫
∞
x
1
z2√
2𝜋
e−1
2 z2 dz by parts where we let u =
1
z3 , d𝑣
dz =
z
√
2𝜋
e−1
2 z2 so that du
dz = −3
z4 and 𝑣= −
1
√
2𝜋
e−1
2 z2 and hence
∫
∞
x
1
z2√
2𝜋
e−1
2 z2 dz = −
1
z3√
2𝜋
e−1
2 z2||||||
∞
x
−∫
∞
x
3
z4√
2𝜋
e−1
2 z2 dz
=
1
x3√
2𝜋
e−1
2 x2 −∫
∞
x
3
z4√
2𝜋
e−1
2 z2 dz.

20
1.2.2
Discrete and Continuous Random Variables
Therefore,
∫
∞
x
1
√
2𝜋
e−1
2 z2 dz =
1
x
√
2𝜋
e−1
2 z2 −
1
x3√
2𝜋
e−1
2 x2 + ∫
∞
x
3
z4√
2𝜋
e−1
2 z2 dz
≥
(1
x −1
x3
)
1
√
2𝜋
e−1
2 x2
since ∫
∞
x
3
z4√
2𝜋
e−1
2 z2 dz > 0. By taking into account both the lower and upper bounds
we have
(1
x −1
x3
)
1
√
2𝜋
e−1
2 x2 ≤∫
∞
x
1
√
2𝜋
e−1
2 z2 dz ≤
1
x
√
2𝜋
e−1
2 x2.
◽
9. Lognormal Distribution I. Let Z ∼𝒩(0, 1), show that the moment generating function of
a standard normal distribution is
𝔼(e𝜃Z) = e
1
2 𝜃2
for a constant 𝜃.
Show that if X ∼𝒩(𝜇, 𝜎2) then Y = eX follows a lognormal distribution, Y = eX ∼
log-𝒩(𝜇, 𝜎2) with probability density function
fY(y) =
1
y𝜎
√
2𝜋
e
−1
2
( log y−𝜇
𝜎
)2
with mean 𝔼(Y) = e𝜇+ 1
2 𝜎2 and variance Var(Y) =
(
e𝜎2 −1
)
e2𝜇+𝜎2.
Solution: By definition
𝔼(e𝜃Z) = ∫
∞
−∞
e𝜃z ⋅
1
√
2𝜋
e−1
2 z2 dz
= ∫
∞
−∞
1
√
2𝜋
e−1
2 (z−𝜃)2+ 1
2 𝜃2 dz
= e
1
2 𝜃2
∫
∞
−∞
1
√
2𝜋
e−1
2 (z−𝜃)2 dz
= e
1
2 𝜃2.
For y > 0, by definition
ℙ(eX < y) = ℙ(X < log y) = ∫
log y
−∞
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
dx

1.2.2
Discrete and Continuous Random Variables
21
and hence
fY(y) = d
dyℙ(eX < y) =
1
y𝜎
√
2𝜋
e
−1
2
( log y−𝜇
𝜎
)2
.
Given that log Y ∼𝒩(𝜇, 𝜎2) we can write log Y = 𝜇+ 𝜎Z, Z ∼𝒩(0, 1) and hence
𝔼(Y) = 𝔼(e𝜇+𝜎Z) = e𝜇𝔼(e𝜎Z) = e𝜇+ 1
2 𝜎2
since 𝔼(e𝜃Z) = e
1
2 𝜃2 is the moment generating function of a standard normal distribution.
Taking second moments,
𝔼(Y2) = 𝔼(e2𝜇+2𝜎Z) = e2𝜇𝔼(e2𝜎Z) = e2𝜇+2𝜎2.
Therefore,
Var(Y) = 𝔼(Y2) −[𝔼(Y)]2 = e2𝜇+2𝜎2 −
(
e𝜇+ 1
2 𝜎2)2
=
(
e𝜎2 −1
)
e2𝜇+𝜎2.
◽
10. Lognormal Distribution II. Let X ∼log-𝒩(𝜇, 𝜎2), show that for n ∈ℕ
𝔼(Xn) = en𝜇+ 1
2 n2𝜎2.
Solution: Given X ∼log-𝒩(𝜇, 𝜎2) the density function is
fX(x) =
1
x𝜎
√
2𝜋
e
−1
2
( log x−𝜇
𝜎
)2
,
x > 0
and for n ∈ℕ,
𝔼(Xn) = ∫
∞
0
xnfX(x) dx
= ∫
∞
0
1
x𝜎
√
2𝜋
xne
−1
2
( log x−𝜇
𝜎
)2
dx.
By substituting z = log x −𝜇
𝜎
so that x = e𝜎z+𝜇and dz
dx = 1
x𝜎,
𝔼(Xn) = ∫
∞
−∞
1
x𝜎
√
2𝜋
en(𝜎z+𝜇)e−1
2 z2x𝜎dz
= ∫
∞
−∞
1
√
2𝜋
e−1
2 z2+n(𝜎z+𝜇) dz
= en𝜇+ 1
2 n2𝜎2
∫
∞
−∞
1
√
2𝜋
e−1
2 (z−n𝜎)2 dz

22
1.2.2
Discrete and Continuous Random Variables
= en𝜇+ 1
2 n2𝜎2
since ∫
∞
−∞
1
√
2𝜋
e−1
2 (z−n𝜎)2 dz = 1.
◽
11. Folded Normal Distribution. Show that if X ∼𝒩(𝜇, 𝜎2) then Y = |X| follows a folded
normal distribution, Y = |X| ∼𝒩f(𝜇, 𝜎2) with probability density function
fY(y) =
√
2
𝜋𝜎2 e
−1
2
(
y2+𝜇2
𝜎2
)
cosh
(𝜇y
𝜎2
)
with mean
𝔼(Y) = 𝜎
√
2
𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇
[
1 −2Φ
(
−𝜇
𝜎
)]
and variance
Var (Y) = 𝜇2 + 𝜎2 −
{
𝜎
√
2
𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇
[
1 −2Φ
(
−𝜇
𝜎
)]}2
where Φ(⋅) is the cumulative distribution function of a standard normal.
Solution: For y > 0, by definition
ℙ(|X| < y) = ℙ(−y < X < y) = ∫
y
−y
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
dx
and hence
fY(y) = d
dyℙ(|X| < y) =
1
𝜎
√
2𝜋
[
e
−1
2
( y+𝜇
𝜎
)2
+ e
−1
2
( y−𝜇
𝜎
)2]
=
√
2
𝜋𝜎2 e
−1
2
(
y2+𝜇2
𝜎2
)
cosh
(𝜇y
𝜎2
)
.
By definition
𝔼(Y) = ∫
∞
−∞
y fY(y) dy
= ∫
∞
0
y
𝜎
√
2𝜋
e
−1
2
( y+𝜇
𝜎
)2
dy + ∫
∞
0
y
𝜎
√
2𝜋
e
−1
2
( y−𝜇
𝜎
)2
dy.
By setting z = (y + 𝜇)∕𝜎and 𝑤= (y −𝜇)∕𝜎we have
𝔼(Y) = ∫
∞
𝜇∕𝜎
1
√
2𝜋
(𝜎z −𝜇)e−1
2 z2dz + ∫
∞
−𝜇∕𝜎
1
√
2𝜋
(𝜎𝑤+ 𝜇)e−1
2 𝑤2d𝑤

1.2.2
Discrete and Continuous Random Variables
23
=
𝜎
√
2𝜋
e
−1
2
( 𝜇
𝜎
)2
−𝜇
[
1 −Φ
(𝜇
𝜎
)]
+
𝜎
√
2𝜋
e
−1
2
( 𝜇
𝜎
)2
−𝜇
[
1 −Φ
(
−𝜇
𝜎
)]
=
2𝜎
√
2𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇
[
Φ
(𝜇
𝜎
)
−Φ
(
−𝜇
𝜎
)]
= 𝜎
√
2
𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇
[
1 −2Φ
(
−𝜇
𝜎
)]
.
To evaluate 𝔼(Y2) by definition,
𝔼(Y2) = ∫
∞
−∞
y2 fY(y) dy
= ∫
∞
0
y2
𝜎
√
2𝜋
e
−1
2
( y+𝜇
𝜎
)2
dy + ∫
∞
0
y2
𝜎
√
2𝜋
e
−1
2
( y−𝜇
𝜎
)2
dy.
By setting z = (y + 𝜇)∕𝜎and 𝑤= (y −𝜇)∕𝜎we have
𝔼(Y2) = ∫
∞
𝜇∕𝜎
1
√
2𝜋
(𝜎z −𝜇)2e−1
2 z2dz + ∫
∞
−𝜇∕𝜎
1
√
2𝜋
(𝜎𝑤+ 𝜇)2e−1
2 𝑤2d𝑤
=
𝜎2
√
2𝜋∫
∞
𝜇∕𝜎
z2e−1
2 z2dz −2𝜇𝜎
√
2𝜋∫
∞
𝜇∕𝜎
ze−1
2 z2dz + 𝜇2
∫
∞
𝜇∕𝜎
1
√
2𝜋
e−1
2 z2dz
+ 𝜎2
√
2𝜋∫
∞
−𝜇∕𝜎
𝑤2e−1
2 𝑤2d𝑤+ 2𝜇𝜎
√
2𝜋∫
∞
−𝜇∕𝜎
𝑤e−1
2 𝑤2d𝑤
+ 𝜇2
∫
∞
−𝜇∕𝜎
1
√
2𝜋
e−1
2 𝑤2d𝑤
=
𝜎2
√
2𝜋
[(𝜇
𝜎
)
e
−1
2
( 𝜇
𝜎
)2
+
√
2𝜋
(
1 −Φ
(𝜇
𝜎
))]
−2𝜇𝜎
√
2𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇2 [
1 −Φ
(𝜇
𝜎
)]
+ 𝜎2
√
2𝜋
[(
−𝜇
𝜎
)
e
−1
2
( 𝜇
𝜎
)2
+
√
2𝜋
(
1 −Φ
(
−𝜇
𝜎
))]
+ 2𝜇𝜎
√
2𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇2 [
1 −Φ
(
−𝜇
𝜎
)]
= (𝜇2 + 𝜎2) [
2 −Φ
(𝜇
𝜎
)
−Φ
(
−𝜇
𝜎
)]
= 𝜇2 + 𝜎2.

24
1.2.2
Discrete and Continuous Random Variables
Therefore,
Var(Y) = 𝔼(Y2) −[𝔼(Y)]2 = 𝜇2 + 𝜎2 −
{
𝜎
√
2
𝜋
e
−1
2
( 𝜇
𝜎
)2
+ 𝜇
[
1 −2Φ
(
−𝜇
𝜎
)]}2
.
◽
12. Chi-Square Distribution. Show that if X ∼𝒩(𝜇, 𝜎2) then Y = X −𝜇
𝜎
∼𝒩(0, 1).
Let Z1, Z2, . . . , Zn ∼𝒩(0, 1) be a sequence of independent and identically distributed ran-
dom variables each following a standard normal distribution. Using mathematical induc-
tion show that
Z = Z2
1 + Z2
2 + . . . + Z2
n ∼𝜒2(n)
where Z has a probability density function
fZ(z) =
1
2
n
2 Γ
(
n
2
)z
n
2 −1e−z
2 ,
z ≥0
such that
Γ
(n
2
)
= ∫
∞
0
e−xx
n
2 −1dx.
Finally, show that 𝔼(Z) = n and Var(Z) = 2n.
Solution: By setting Y = X −𝜇
𝜎
then
ℙ(Y ≤y) = ℙ
(X −𝜇
𝜎
≤y
)
= ℙ(X ≤𝜇+ 𝜎y).
Differentiating with respect to y,
fY(y) = d
dyℙ(Y ≤y)
= d
dy ∫
𝜇+𝜎y
−∞
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
dx
=
1
𝜎
√
2𝜋
e
−1
2
( 𝜇+𝜎y−𝜇
𝜎
)2
𝜎
=
1
√
2𝜋
e−1
2 y2
which is a probability density function of 𝒩(0, 1).
For Z = Z2
1 and given Z ≥0, by definition
ℙ(Z ≤z) = ℙ(Z2
1 ≤z) = ℙ
(
−
√
z < Z1 <
√
z
)
= 2 ∫
√
z
0
1
√
2𝜋
e−1
2 z2
1 dz1

1.2.2
Discrete and Continuous Random Variables
25
and hence
fZ(z) = d
dz
[
2 ∫
√
z
0
1
√
2𝜋
e−1
2 z2
1 dz1
]
=
1
√
2𝜋
z−1
2 e−z
2 ,
z ≥0
which is the probability density function of 𝜒2(1).
For Z = Z2
1 + Z2
2 such that Z ≥0 we have
ℙ(Z ≤z) = ℙ(Z2
1 + Z2
2 ≤z) = ∫∫
z2
1+z2
2≤z
1
2𝜋e−1
2 (z2
1+z2
2) dz1dz2.
Changing to polar coordinates (r, 𝜃) such that z1 = r cos 𝜃and z2 = r sin 𝜃with the Jaco-
bian determinant
|J| =
||||
𝜕(z1, z2)
𝜕(r, 𝜃)
||||
=
|||||||||
𝜕z1
𝜕r
𝜕z1
𝜕𝜃
𝜕z2
𝜕r
𝜕z2
𝜕𝜃
|||||||||
=
||||
cos 𝜃−r sin 𝜃
sin 𝜃
r cos 𝜃
||||
= r
then
∫∫
z2
1+z2
2≤z
1
2𝜋e−1
2 (z2
1+z2
2) dz1dz2 = ∫
2𝜋
𝜃=0 ∫
√
z
r=0
1
2𝜋e−1
2 r2r drd𝜃= 1 −e−1
2 z.
Thus,
fZ(z) = d
dz
⎡
⎢
⎢
⎢⎣
∫∫
z2
1+z2
2≤z
1
2𝜋e−1
2 (z2
1+z2
2) dz1dz2
⎤
⎥
⎥
⎥⎦
= 1
2 e−1
2 z,
z ≥0
which is the probability density function of 𝜒2(2).
Assume the result is true for n = k such that
U = Z2
1 + Z2
2 + . . . + Z2
k ∼𝜒2(k)
and knowing that
V = Z2
k+1 ∼𝜒2(1)
then, because U ≥0 and V ≥0 are independent, using the convolution formula the density
of Z = U + V = ∑k+1
i=1 Z2
i can be written as
fZ(z) = ∫
z
0
fV(z −u)fU(u) du
= ∫
z
0
{
1
√
2𝜋
(z −u)−1
2 e−1
2 (z−u)
}
⋅
⎧
⎪
⎨
⎪⎩
1
2
k
2 Γ
(
k
2
)u
1
2 k−1e−1
2 u
⎫
⎪
⎬
⎪⎭
du

26
1.2.2
Discrete and Continuous Random Variables
=
e−1
2 z
√
2𝜋2
k
2 Γ
(
k
2
) ∫
z
0
(z −u)−1
2 u
k
2 −1 du.
By setting 𝑣= u
z we have
∫
z
0
(z −u)−1
2 u
k
2 −1 du = ∫
1
0
(z −𝑣z)−1
2 (𝑣z)
k
2 −1z d𝑣
= z
k+1
2 −1
∫
1
0
(1 −𝑣)−1
2 𝑣
k
2 −1 d𝑣
and because ∫
1
0
(1 −𝑣)−1
2 𝑣
k
2 −1 d𝑣= B
(1
2, k
2
)
=
√
𝜋Γ
(
k
2
)
Γ
(
k+1
2
)
(see Appendix A) there-
fore
fZ(z) =
1
2
k+1
2 Γ
(
k+1
2
)z
k+1
2 −1e−1
2 z,
z ≥0
which is the probability density function of 𝜒2(k + 1) and hence the result is also true for
n = k + 1. By mathematical induction we have shown
Z = Z2
1 + Z2
2 + . . . + Z2
n ∼𝜒2(n).
By computing the moment generation of Z,
MZ(t) = 𝔼(etZ) = ∫
∞
0
etzfZ(z) dz =
1
2
n
2 Γ
(
n
2
) ∫
∞
0
z
n
2 −1e−1
2 (1−2t) dz
and by setting 𝑤= 1
2(1 −2t)z we have
MZ(t) =
1
2
n
2 Γ
(
n
2
)
(
2
1 −2t
) n
2
∫
∞
0
𝑤
n
2 −1e−𝑤d𝑤= (1 −2t)−n
2 ,
t ∈
(
−1
2, 1
2
)
.
Thus,
M
′
Z(t) = n(1 −2t)
−
( n
2 +1
)
,
M
′′
Z (t) = 2n
(n
2 + 1
)
(1 −2t)
−
( n
2 +2
)
such that
𝔼(Z) = M
′
Z(0) = n,
𝔼(Z2) = M
′′
Z (0) = 2n
(n
2 + 1
)
and
Var (Z) = 𝔼(Z2) −[𝔼(Z)]2 = 2n.
◽

1.2.2
Discrete and Continuous Random Variables
27
13. Marginal Distributions of Bivariate Normal Distribution. Let X and Y be jointly normally
distributed with means 𝜇x, 𝜇y, variances 𝜎2
x, 𝜎2
y and correlation coefficient 𝜌xy ∈(−1, 1)
such that the joint density function is
fXY(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
.
Show that X ∼𝒩(𝜇x, 𝜎2
x) and Y ∼𝒩(𝜇y, 𝜎2
y).
Solution: By definition,
fX(x) = ∫
∞
−∞
fXY(x, y) dy
= ∫
∞
−∞
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy
= ∫
∞
−∞
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
× e
−
1
2(1−𝜌2xy)
[
(1−𝜌2
xy)
( x−𝜇x
𝜎x
)2
+𝜌2
xy
( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy
=
1
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2
∫
∞
−∞
g(x, y) dy
where
g(x, y) =
1
√
1 −𝜌2
xy𝜎y
√
2𝜋
e
−
1
2(1−𝜌2xy)𝜎2y
[
y−
(
𝜇y+𝜌xy𝜎y
( x−𝜇x
𝜎x
))]2
is
the
probability
density
function
for
𝒩(𝜇y + 𝜌xy𝜎y
(x −𝜇x
) ∕𝜎x, (1 −𝜌2
xy)𝜎2
y
).
Therefore,
∫
∞
−∞
g(x, y) dy = 1
and hence
fX(x) =
1
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2
.
Thus, X ∼𝒩(𝜇x, 𝜎2
x). Using the same steps we can also show that Y ∼𝒩(𝜇y, 𝜎2
y).
◽

28
1.2.2
Discrete and Continuous Random Variables
14. Covariance of Bivariate Normal Distribution. Let X and Y be jointly normally distributed
with means 𝜇x, 𝜇y, variances 𝜎2
x, 𝜎2
y and correlation coefficient 𝜌xy ∈(−1, 1) such that the
joint density function is
fXY(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
.
Show that the covariance of X and Y, Cov(X, Y) = 𝜌xy𝜎x𝜎y and hence show that X and Y
are independent if and only if 𝜌xy = 0.
Solution: By definition, the covariance of X and Y is
Cov(X, Y) = 𝔼[(X −𝜇x)(Y −𝜇y)]
= ∫
∞
−∞∫
∞
−∞
(x −𝜇x)(y −𝜇y) fXY(x, y) dxdy
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy −𝜇y ∫
∞
−∞∫
∞
−∞
xfXY(x, y) dxdy
−𝜇x ∫
∞
−∞∫
∞
−∞
yfXY(x, y) dxdy + 𝜇x𝜇y ∫
∞
−∞∫
∞
−∞
fXY(x, y) dxdy
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy −𝜇y ∫
∞
−∞
x
[
∫
∞
−∞
fXY(x, y) dy
]
dx
−𝜇x ∫
∞
−∞
y
[
∫
∞
−∞
fXY(x, y) dx
]
dy + ∫
∞
−∞∫
∞
−∞
𝜇x𝜇yfXY(x, y) dxdy
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy −𝜇y ∫
∞
−∞
xfX(x) dx −𝜇x ∫
∞
−∞
yfY(y) dy
+ ∫
∞
−∞∫
∞
−∞
𝜇x𝜇yfXY(x, y) dxdy
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy −𝜇x𝜇y −𝜇x𝜇y + 𝜇x𝜇y
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy −𝜇x𝜇y
where
fX(x) = ∫∞
−∞fXY(x, y) dy
and
fY(y) = ∫∞
−∞fXY(x, y) dx.
Using
the
result
of
Problem 1.2.2.13 (page 27) we can deduce that
∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dxdy = ∫
∞
−∞
x
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2 (
∫
∞
−∞
yg(x, y) dy
)
dx
where
g(x, y) =
1
√
1 −𝜌2
xy𝜎y
√
2𝜋
e
−
1
2(1−𝜌2xy)𝜎2y
[
y−
(
𝜇y+𝜌xy𝜎y
( x−𝜇x
𝜎x
))]2

1.2.2
Discrete and Continuous Random Variables
29
is
the
probability
density
function
for
𝒩(𝜇y + 𝜌xy𝜎y
(x −𝜇x
) ∕𝜎x, (1 −𝜌2
xy)𝜎2
y
).
Therefore,
∫
∞
−∞
yg(x, y) dy = 𝜇y + 𝜌xy𝜎y
(x −𝜇x
𝜎x
)
.
Thus,
Cov(X, Y) = ∫
∞
−∞
x
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2 [
𝜇y + 𝜌xy𝜎y
(x −𝜇x
𝜎x
)]
dx −𝜇x𝜇y
= 𝜇x𝜇y +
𝜌xy𝜎y
𝜎x
∫
∞
−∞
x2
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2
dx −
𝜌xy𝜎y𝜇2
x
𝜎x
−𝜇x𝜇y
= 𝜇x𝜇y +
𝜌xy𝜎y
𝜎x
(𝜎2
x + 𝜇2
x
) −
𝜌xy𝜎y𝜇2
x
𝜎x
−𝜇x𝜇y
= 𝜌xy𝜎x𝜎y
where
𝔼(X2) = ∫
∞
−∞
x2
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2
dx = 𝜎2
x + 𝜇2
x.
To show that X and Y are independent if and only if 𝜌xy = 0 we note that if X ⟂⟂Y then
Cov(X, Y) = 0, which implies 𝜌xy = 0. On the contrary, if 𝜌xy = 0 then from the joint den-
sity of (X, Y) we can express it as
fXY(x, y) = fX(x) fY(y)
where fX(x) = ∫
∞
−∞
1
𝜎x
√
2𝜋
e
−1
2
( x−𝜇x
𝜎x
)2
dx and fY(y) = ∫
∞
−∞
1
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2
dy and so
X ⟂⟂Y.
Thus, if the pair X and Y has a bivariate normal distribution with means 𝜇x, 𝜇y, variances
𝜎2
x, 𝜎2
y and correlation 𝜌xy then X ⟂⟂Y if and only if 𝜌xy = 0.
◽
15. Minimum and Maximum of Two Correlated Normal Distributions. Let X and Y be jointly
normally distributed with means 𝜇x, 𝜇y, variances 𝜎2
x, 𝜎2
y and correlation coefficient 𝜌xy ∈
(−1, 1) such that the joint density function is
fXY(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
.
Show that the distribution of U = min{X, Y} is
fU(u) = Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇y +
𝜌xy𝜎y
𝜎x (u −𝜇x)
𝜎y
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fX(u) + Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇x +
𝜌xy𝜎x
𝜎y (u −𝜇y)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fY(u).

30
1.2.2
Discrete and Continuous Random Variables
and deduce that the distribution of V = max{X, Y} is
fV(𝑣) = Φ
⎛
⎜
⎜
⎜⎝
𝑣−𝜇y −
𝜌xy𝜎y
𝜎x (𝑣−𝜇x)
𝜎y
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fX(𝑣) + Φ
⎛
⎜
⎜
⎜⎝
𝑣−𝜇x −
𝜌xy𝜎x
𝜎y (u −𝜇y)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fY(𝑣)
where fX(z) =
1
𝜎x
√
2𝜋
e
−1
2
( z−𝜇x
𝜎x
)2
, fY(z) =
1
𝜎y
√
2𝜋
e
−1
2
(
z−𝜇y
𝜎y
)2
and Φ(⋅) denotes the
cumulative standard normal distribution function (cdf).
Solution: From Problems 1.2.2.13 (page 27) and 1.2.2.14 (page 28) we can show that
X ∼𝒩(𝜇x, 𝜎2
x), Y ∼𝒩(𝜇y, 𝜎2
y), Cov(X, Y) = 𝜌xy𝜎x𝜎y such that 𝜌xy ∈(−1, 1).
For U = min{X, Y} then by definition the cumulative distribution function (cdf) of U is
ℙ(U ≤u) = ℙ(min{X, Y} ≤u)
= 1 −ℙ(min{X, Y} > u)
= 1 −ℙ(X > u, Y > u) .
To derive the probability density function (pdf) of U we have
fU(u) = d
duℙ(U ≤u)
= −d
duℙ(X > u, Y > u)
= −d
du ∫
∞
x=u ∫
∞
y=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
× e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dydx
= g(u) + h(u)
where
g(u) = ∫
∞
y=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( u−𝜇x
𝜎x
)2
−2𝜌xy
( u−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy
and
h(u) = ∫
∞
x=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
u−𝜇y
𝜎y
)
+
(
u−𝜇y
𝜎y
)2]
dx.

1.2.2
Discrete and Continuous Random Variables
31
By focusing on
g(u) = ∫
∞
y=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( u−𝜇x
𝜎x
)2
−2𝜌xy
( u−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy
= ∫
∞
y=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
{[(
y−𝜇y
𝜎y
)
−𝜌xy
( u−𝜇x
𝜎x
)]2
+
( u−𝜇x
𝜎x
)2
(1−𝜌2
xy)
}
dy
=
1
𝜎x
√
2𝜋
e
−1
2
( u−𝜇x
𝜎x
)2
∫
∞
y=u
1
𝜎y
√
2𝜋(1 −𝜌2
xy)
e
−
1
2(1−𝜌2xy)
[(
y−𝜇y
𝜎y
)
−𝜌xy
( u−𝜇x
𝜎x
)]2
dy
and letting 𝑤=
y −𝜇y
𝜎y
−𝜌xy
(u −𝜇x
𝜎x
)
√
1 −𝜌2
xy
we have
g(u) =
1
𝜎x
√
2𝜋
e
−1
2
( u−𝜇x
𝜎x
)2
∫
∞
𝑤=
u−𝜇y
𝜎y
−𝜌xy
( u−𝜇x
𝜎x
)
√
1−𝜌2xy
1
√
2𝜋
e−1
2 𝑤2d𝑤
= Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇y +
𝜌xy𝜎y
𝜎x (u −𝜇x)
𝜎y
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fX(u).
In a similar vein we can also show
h(u) = ∫
∞
x=u
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
u−𝜇y
𝜎y
)
+
(
u−𝜇y
𝜎y
)2]
dx
=
1
𝜎y
√
2𝜋
e
−1
2
(
u−𝜇y
𝜎y
)2
∫
∞
𝑤=
u−𝜇x
𝜎x
−𝜌xy
( u−𝜇y
𝜎y
)
√
1−𝜌2xy
1
√
2𝜋
e−1
2 𝑤2d𝑤
= Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇x +
𝜌xy𝜎x
𝜎y (u −𝜇y)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fY(u).
Therefore,
fU(u) = Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇y +
𝜌xy𝜎y
𝜎x (u −𝜇x)
𝜎y
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fX(u) + Φ
⎛
⎜
⎜
⎜⎝
−u + 𝜇x +
𝜌xy𝜎x
𝜎y (u −𝜇y)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fY(u).

32
1.2.2
Discrete and Continuous Random Variables
As for the case V = max{X, Y}, then by definition the cdf of V is
ℙ(V ≤𝑣) = ℙ(max{X, Y} ≤𝑣)
= ℙ(X ≤𝑣, Y ≤𝑣) .
The pdf of V is
fV(𝑣) = d
d𝑣ℙ(V ≤𝑣)
= d
d𝑣ℙ(X ≤𝑣, Y ≤𝑣)
= d
d𝑣∫
x=𝑣
−∞∫
y=𝑣
−∞
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
× e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy dx
= ∫
y=𝑣
−∞
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( 𝑣−𝜇x
𝜎x
)2
−2𝜌xy
( 𝑣−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
dy
+ ∫
x=𝑣
−∞
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
𝑣−𝜇y
𝜎y
)
+
(
𝑣−𝜇y
𝜎y
)2]
dx.
Following the same steps as described above we can write
fV(𝑣) = Φ
⎛
⎜
⎜
⎜⎝
𝑣−𝜇y −
𝜌xy𝜎y
𝜎x (𝑣−𝜇x)
𝜎y
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fX(𝑣) + Φ
⎛
⎜
⎜
⎜⎝
𝑣−𝜇x −
𝜌xy𝜎x
𝜎y (𝑣−𝜇y)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
fY(𝑣).
◽
16. Bivariate Standard Normal Distribution. Let X ∼𝒩(0, 1) and Y ∼𝒩(0, 1) be jointly nor-
mally distributed with correlation coefficient 𝜌xy ∈(−1, 1) where the joint cumulative
distribution function is
𝚽(𝛼, 𝛽, 𝜌xy) = ∫
𝛽
−∞∫
𝛼
−∞
1
2𝜋
√
1 −𝜌2
xy
e
−1
2
(
x2−2𝜌xyxy+y2
1−𝜌2xy
)
dxdy.
By using the change of variables Y = 𝜌xyX +
√
1 −𝜌2
xyZ, Z ∼𝒩(0, 1), X ⟂⟂Z show that
𝚽(𝛼, 𝛽, 𝜌xy) = ∫
𝛼
−∞
fX(x)Φ
⎛
⎜
⎜
⎜⎝
𝛽−𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dx

1.2.2
Discrete and Continuous Random Variables
33
where fX(x) =
1
√
2𝜋
e−1
2 x2 and Φ(⋅) is the cumulative distribution function of a standard
normal.
Finally, deduce that 𝚽(𝛼, 𝛽, 𝜌xy) + 𝚽(𝛼, −𝛽, −𝜌xy) = Φ(𝛼).
Solution: Let y = 𝜌xyx +
√
1 −𝜌2
xyz. Differentiating y with respect to z we have dy
dz =
√
1 −𝜌2
xy, and hence
𝚽(𝛼, 𝛽, 𝜌xy) = ∫
𝛽
−∞∫
𝛼
−∞
1
2𝜋
√
1 −𝜌2
xy
e
−1
2
(
x2−2𝜌xyxy+y2
1−𝜌2xy
)
dx dy
= ∫
𝛽−𝜌xyx
√
1−𝜌2xy
−∞
∫
𝛼
−∞
1
2𝜋
√
1 −𝜌2
xy
× e
−1
2
⎛
⎜
⎜⎝
x2−2𝜌xyx(𝜌xyx+√
1−𝜌2xyz)+(𝜌xyx+√
1−𝜌2xyz)2
1−𝜌2xy
⎞
⎟
⎟⎠
√
1 −𝜌2
xy dx dz
= ∫
𝛽−𝜌xyx
√
1−𝜌2xy
−∞
∫
𝛼
−∞
1
2𝜋e−1
2 (x2+z2)dx dz
= ∫
𝛼
−∞
1
√
2𝜋
e−1
2 x2 ⎡
⎢
⎢⎣∫
𝛽−𝜌xyx
√
1−𝜌2xy
−∞
1
√
2𝜋
e−1
2 z2 dz
⎤
⎥
⎥⎦
dx
= ∫
𝛼
−∞
fX(x)Φ
⎛
⎜
⎜
⎜⎝
𝛽−𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dx.
Finally,
𝚽(𝛼, 𝛽, 𝜌xy) + 𝚽(𝛼, −𝛽, −𝜌xy) = ∫
𝛼
−∞
fX(x)Φ
⎛
⎜
⎜
⎜⎝
𝛽−𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dx
+ ∫
𝛼
−∞
fX(x)Φ
⎛
⎜
⎜
⎜⎝
−𝛽+ 𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dx
= ∫
𝛼
−∞
fX(x)
⎡
⎢
⎢
⎢⎣
Φ
⎛
⎜
⎜
⎜⎝
𝛽−𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
+ Φ
⎛
⎜
⎜
⎜⎝
−𝛽+ 𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
⎤
⎥
⎥
⎥⎦
dx

34
1.2.2
Discrete and Continuous Random Variables
= ∫
𝛼
−∞
fX(x) dx
= Φ(𝛼)
since Φ
⎛
⎜
⎜
⎜⎝
𝛽−𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
+ Φ
⎛
⎜
⎜
⎜⎝
−𝛽+ 𝜌xyx
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
= 1.
N.B. Similarly we can also show that
𝚽(𝛼, 𝛽, 𝜌xy) = ∫
𝛽
−∞
fY(y)Φ
⎛
⎜
⎜
⎜⎝
𝛼−𝜌xyy
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dy
where fY(y) =
1
√
2𝜋
e−1
2 y2 and 𝚽(𝛼, 𝛽, 𝜌xy) + 𝚽(−𝛼, 𝛽, −𝜌xy) = Φ(𝛽).
◽
17. Bivariate Normal Distribution Property. Let X and Y be jointly normally distributed with
means 𝜇x, 𝜇y, variances 𝜎2
x, 𝜎2
y and correlation coefficient 𝜌xy ∈(−1, 1) such that the joint
density function is
fXY(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
.
Show that
𝔼[max{eX −eY, 0}] = e𝜇x+ 1
2 𝜎2
x Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y + 𝜎x(𝜎x −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
−e𝜇y+ 1
2 𝜎2
y Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −𝜎y(𝜎y −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
.
Solution: By definition
𝔼[max{eX −eY, 0}] = ∫
y=∞
y=−∞∫
x=∞
x=y
(ex −ey) fXY(x, y) dxdy
= I1 −I2
where I1 = ∫
y=∞
y=−∞∫
x=∞
x=y
exfXY(x, y) dxdy and I2 = ∫
y=∞
y=−∞∫
x=∞
x=y
eyfXY(x, y) dxdy.

1.2.2
Discrete and Continuous Random Variables
35
For the case I1 = ∫
y=∞
y=−∞∫
x=∞
x=y
exfXY(x, y) dxdy we have
I1 = ∫
y=∞
y=−∞∫
x=∞
x=y
ex
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
= ∫
y=∞
y=−∞
1
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2
×
⎡
⎢
⎢
⎢⎣
∫
x=∞
x=y
ex
𝜎x
√
2𝜋(1 −𝜌2
xy)
e
−
1
2(1−𝜌2xy)
[
x−
(
𝜇x+𝜌xy𝜎x
(
y−𝜇y
𝜎y
))]2
dx
⎤
⎥
⎥
⎥⎦
dy
= ∫
y=∞
y=−∞
1
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2 [
∫
x=∞
x=y
exg(x, y) dx
]
dy
where g(x, y) =
1
𝜎x
√
2𝜋(1 −𝜌2
xy)
e
−
1
2(1−𝜌2xy)
[
x−
(
𝜇x+𝜌xy𝜎x
(
y−𝜇y
𝜎y
))]2
which is the probability
density function of 𝒩
[
𝜇x + 𝜌xy𝜎x
(y −𝜇y
𝜎y
)
, (1 −𝜌xy)2𝜎2
x
]
. Thus, from Problem 1.2.2.7
(page 18) we can deduce
∫
x=∞
x=y
exg(x, y) dx = e
𝜇x+𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
+ 1
2 (1−𝜌2
xy)𝜎2
x
× Φ
⎛
⎜
⎜
⎜
⎜⎝
𝜇x + 𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
+ (1 −𝜌xy)2𝜎2
x −y
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟
⎟⎠
.
Thus, we can write
I1 = e𝜇x+ 1
2 (1−𝜌2
xy)𝜎2
x
× ∫
y=∞
y=−∞
1
𝜎y
√
2𝜋
e
−1
2
[(
y−𝜇y
𝜎y
)2
−2𝜌xy𝜎x
(
y−𝜇y
𝜎y
)]
× Φ
⎛
⎜
⎜
⎜
⎜⎝
𝜇x + 𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
+ (1 −𝜌xy)2𝜎2
x −y
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟
⎟⎠
dy

36
1.2.2
Discrete and Continuous Random Variables
= e𝜇x+ 1
2 𝜎2
x
∫
y=∞
y=−∞
1
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y−𝜌xy𝜎x𝜎y
𝜎y
)2
× Φ
⎛
⎜
⎜
⎜
⎜⎝
𝜇x + 𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
+ (1 −𝜌xy)2𝜎2
x −y
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟
⎟⎠
dy.
Let u =
y −𝜇y −𝜌xy𝜎x𝜎y
𝜎y
then from the change of variables
I1 = e𝜇x+ 1
2 𝜎2
x
∫
u=∞
u=−∞
1
√
2𝜋
e−1
2 u2Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y + 𝜎x(𝜎x −𝜌xy𝜎y) −u(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
du
= e𝜇x+ 1
2 𝜎2
x
∫
u=∞
u=−∞∫
𝑣=
𝜇x−𝜇y+𝜎x(𝜎x−𝜌xy𝜎y)
𝜎x
√
1−𝜌2xy
−
u(𝜎y−𝜌xy𝜎x)
𝜎x
√
1−𝜌2xy
𝑣=−∞
1
2𝜋e−1
2 (u2+𝑣2) d𝑣du.
By setting 𝑤= 𝑣+
u(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
and 𝜎2 = 𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y,
I1 = e𝜇x+ 1
2 𝜎2
x
× ∫
u=∞
u=−∞∫
𝑤=
𝜇x−𝜇y+𝜎x(𝜎x−𝜌xy𝜎y)
𝜎x
√
1−𝜌2xy
𝑤=−∞
1
2𝜋
× e
−1
2
⎡
⎢
⎢⎣
𝑤2−2u𝑤
⎛
⎜
⎜⎝
𝜎y−𝜌xy𝜎x
𝜎x
√
1−𝜌2xy
⎞
⎟
⎟⎠
+
(
1+
(𝜎y−𝜌xy𝜎x)2
𝜎2x (1−𝜌2xy)
)
u2
⎤
⎥
⎥⎦d𝑤du
= e𝜇x+ 1
2 𝜎2
x
× ∫
u=∞
u=−∞∫
𝑤=
𝜇x−𝜇y+𝜎x(𝜎x−𝜌xy𝜎y)
𝜎x
√
1−𝜌2xy
𝑤=−∞
1
2𝜋
× e
−1
2
(
𝜎2
𝜎2x (1−𝜌2xy)
)⎡
⎢
⎢
⎢
⎢⎣
𝑤2
(
𝜎2
𝜎2x (1−𝜌2xy)
) −2u𝑤
𝜎x(𝜎y−𝜌xy𝜎x)√
1−𝜌2xy
𝜎2
+u2
⎤
⎥
⎥
⎥
⎥⎦d𝑤du.

1.2.2
Discrete and Continuous Random Variables
37
Finally, by setting 𝑤=
𝑤
⎛
⎜
⎜
⎜⎝
𝜎
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
,
I1 = e𝜇x+ 1
2 𝜎2
x
∫
u=∞
u=−∞∫
𝑤=
𝜇x−𝜇y+𝜎x(𝜎x−𝜌xy𝜎y)
𝜎
𝑤=−∞
1
2𝜋
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy) (𝑤2−2𝜌xyu𝑤+u2)
d𝑤du
where 𝜌xy =
𝜎y −𝜌xy𝜎x
𝜎
.
Therefore,
I1 = e𝜇x+ 1
2 𝜎2
x 𝚽
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y + 𝜎x(𝜎x −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
, ∞, 𝜌xy
⎞
⎟
⎟
⎟⎠
= e𝜇x+ 1
2 𝜎2
x Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y + 𝜎x(𝜎x −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
where 𝚽(𝛼, 𝛽, 𝜌) = ∫
𝛽
−∞∫
𝛼
−∞
1
2𝜋
√
1 −𝜌2 e
−1
2
(
x2−2𝜌xy+y2
1−𝜌2
)
dxdy is the cumulative distri-
bution function of a standard bivariate normal.
Using similar steps for the case I2 = ∫
y=∞
y=−∞∫
x=∞
x=y
eyfXY(x, y) dxdy we have
I2 = ∫
y=∞
y=−∞∫
x=∞
x=y
ey
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌xy
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
= ∫
y=∞
y=−∞
ey
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2
×
⎡
⎢
⎢
⎢⎣
∫
x=∞
x=y
1
𝜎x
√
2𝜋(1 −𝜌2
xy)
e
−
1
2(1−𝜌2xy)
[
x−
(
𝜇x+𝜌xy𝜎x
(
y−𝜇y
𝜎y
))]2
dx
⎤
⎥
⎥
⎥⎦
dy
= ∫
y=∞
y=−∞
ey
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2 [
∫
x=∞
x=y
g(x, y) dx
]
dy

38
1.2.2
Discrete and Continuous Random Variables
where g(x, y) =
1
𝜎x
√
2𝜋(1 −𝜌2
xy)
e
−
1
2(1−𝜌2xy)
[
x−
(
𝜇x+𝜌xy𝜎x
(
y−𝜇y
𝜎y
))]2
which is the probability
density function of 𝒩
[
𝜇x + 𝜌xy𝜎x
(y −𝜇y
𝜎y
)
, (1 −𝜌xy)2𝜎2
x
]
.
Thus,
I2 = ∫
y=∞
y=−∞
ey
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2
⎡
⎢
⎢
⎢
⎢
⎢⎣
∫
x=∞
x=y
1
𝜎x
√
2𝜋(1 −𝜌2
xy)
e
−1
2
⎛
⎜
⎜
⎜⎝
x−𝜇x−𝜌xy𝜎x
( y−𝜇y
𝜎y
)
𝜎x
√
1−𝜌2xy
⎞
⎟
⎟
⎟⎠
2
dx
⎤
⎥
⎥
⎥
⎥
⎥⎦
dy
and by setting z =
x −𝜇x −𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
𝜎x
√
1 −𝜌2
xy
,
I2 = ∫
y=∞
y=−∞
ey
𝜎y
√
2𝜋
e
−1
2
(
y−𝜇y
𝜎y
)2 ⎡
⎢
⎢
⎢⎣
∫
z=∞
z=
y−𝜇x−𝜌xy𝜎x
( y−𝜇y
𝜎y
)
𝜎x
√
1−𝜌2xy
1
√
2𝜋
e−1
2 z2 dz
⎤
⎥
⎥
⎥⎦
dy
= ∫
y=∞
y=−∞
1
𝜎y
√
2𝜋
e
−1
2
[(
y−𝜇y
𝜎y
)2
−2y
]
Φ
⎛
⎜
⎜
⎜
⎜⎝
−y + 𝜇x + 𝜌xy𝜎x
(
y−𝜇y
𝜎y
)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟
⎟⎠
dy.
By setting z =
y −𝜇y
𝜎y
therefore
I2 = e𝜇y+ 1
2 𝜎2
y
∫
z=∞
z=−∞
1
√
2𝜋
e−1
2 (z−𝜎y)2Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −z(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
dz
and substituting u = z −𝜎y,
I2 = e𝜇y+ 1
2 𝜎2
y
∫
u=∞
u=−∞
1
√
2𝜋
e−1
2 u2Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −(u + 𝜎y)(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
du
= e𝜇y+ 1
2 𝜎2
y
∫
u=∞
u=−∞
1
√
2𝜋
e−1
2 u2Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −𝜎y(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
−
u(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
du

1.2.2
Discrete and Continuous Random Variables
39
= e𝜇y+ 1
2 𝜎2
y
∫
u=∞
u=−∞∫
𝑣=
𝜇x−𝜇y−𝜎y(𝜎y−𝜌xy𝜎x)
𝜎x
√
1−𝜌2xy
−
u(𝜎y−𝜌xy𝜎x)
𝜎x
√
1−𝜌2xy
𝑣=−∞
1
2𝜋e−1
2 (u2+𝑣2) d𝑣du.
Using the same steps as described before, we let 𝑤= 𝑣+
u(𝜎y −𝜌xy𝜎x)
𝜎x
√
1 −𝜌2
xy
and 𝜎2 = 𝜎2
x −
2𝜌xy𝜎x𝜎y + 𝜎2
y so that
I2 = e𝜇y+ 1
2 𝜎2
y
× ∫
u=∞
u=−∞∫
𝑤=
𝜇x−𝜇y−𝜎y(𝜎y−𝜌xy𝜎x)
𝜎x
√
1−𝜌2xy
𝑤=−∞
1
2𝜋
× e
−1
2
⎡
⎢
⎢⎣
𝑤2−2u𝑤
⎛
⎜
⎜⎝
𝜎y−𝜌xy𝜎x
𝜎x
√
1−𝜌2xy
⎞
⎟
⎟⎠
+
(
1+
(𝜎y−𝜌xy𝜎x)2
𝜎2x (1−𝜌2xy)
)
u2
⎤
⎥
⎥⎦d𝑤du
= e𝜇y+ 1
2 𝜎2
y
∫
u=∞
u=−∞
× ∫
𝑤=
𝜇x−𝜇y−𝜎y(𝜎y−𝜌xy𝜎x)
𝜎x
√
1−𝜌2xy
𝑤=−∞
1
2𝜋
× e
−1
2
(
𝜎2
𝜎2x (1−𝜌2xy)
)⎡
⎢
⎢
⎢
⎢⎣
𝑤2
(
𝜎2
𝜎2x (1−𝜌2xy)
) −2u𝑤
𝜎x(𝜎y−𝜌xy𝜎x)√
1−𝜌2xy
𝜎2
+u2
⎤
⎥
⎥
⎥
⎥⎦d𝑤du.
By setting 𝑤=
𝑤
⎛
⎜
⎜
⎜⎝
𝜎
𝜎x
√
1 −𝜌2
xy
⎞
⎟
⎟
⎟⎠
,
I2 = e𝜇y+ 1
2 𝜎2
y
∫
u=∞
u=−∞∫
𝑤=
𝜇x−𝜇y−𝜎y(𝜎y−𝜌xy𝜎x)
𝜎
𝑤=−∞
1
2𝜋
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy) (𝑤2−2𝜌xyu𝑤+u2)
d𝑤du
where 𝜌xy =
𝜎y −𝜌xy𝜎x
𝜎
, thus
I2 = e𝜇y+ 1
2 𝜎2
y 𝚽
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −𝜎y(𝜎y −𝜌xy𝜎x)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
, ∞, 𝜌xy
⎞
⎟
⎟
⎟⎠
= e𝜇y+ 1
2 𝜎2
y Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −𝜎y(𝜎y −𝜌xy𝜎x)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
.

40
1.2.2
Discrete and Continuous Random Variables
By substituting I1 and I2 back into 𝔼[max{eX −eY, 0}] we have
𝔼[max {eX −eY, 0}] = e𝜇x+ 1
2 𝜎2
x Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y + 𝜎x(𝜎x −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
−e𝜇y+ 1
2 𝜎2
y Φ
⎛
⎜
⎜
⎜⎝
𝜇x −𝜇y −𝜎y(𝜎y −𝜌xy𝜎y)
√
𝜎2
x −2𝜌xy𝜎x𝜎y + 𝜎2
y
⎞
⎟
⎟
⎟⎠
.
◽
18. Markov’s Inequality. Let X be a non-negative random variable with mean 𝜇. For 𝛼> 0,
show that
ℙ(X ≥𝛼) ≤𝜇
𝛼.
Solution: Since 𝛼> 0, we can write
1I{X≥𝛼} =
{
1
X ≥𝛼
0
otherwise
and since X ≥0, we can deduce that
1I{X≥𝛼} ≤X
𝛼.
Taking expectations
𝔼(1I{X≥𝛼}
) ≤𝔼(X)
𝛼
and
since
𝔼(1I{X≥𝛼}
) = 1 ⋅ℙ(X ≥𝛼) + 0 ⋅ℙ(X ≤𝛼) = ℙ(X ≥𝛼)
and
𝔼(X) = 𝜇,
we have
ℙ(X ≥𝛼) ≤𝜇
𝛼.
N.B. Alternatively, we can also show the result as
𝔼(X) = ∫
∞
0
(1 −Fx(u)) du ≥∫
𝛼
0
(1 −Fx(u)) du ≥𝛼(1 −Fx(𝛼))
and hence it follows that ℙ(X ≥𝛼) = 1 −Fx(𝛼) ≤𝔼(X)
𝛼
.
◽
19. Chebyshev’s Inequality. Let X be a random variable with mean 𝜇and variance 𝜎2. Then
for k > 0, show that
ℙ(|X −𝜇| ≥k) ≤𝜎2
k2 .
Solution: Take note that |X −𝜇| ≥k if and only if (X −𝜇)2 ≥k2. Because (X −𝜇)2 ≥0,
and by applying Markov’s inequality (see Problem 1.2.2.18, page 40) we have
ℙ(
(X −𝜇)2 ≥k2) ≤
𝔼[
(X −𝜇)2]
k2
= 𝜎2
k2

1.2.3
Properties of Expectations
41
and hence the above inequality is equivalent to
ℙ(|X −𝜇| ≥k) ≤𝜎2
k2 .
◽
1.2.3
Properties of Expectations
1. Show that if X is a random variable taking non-negative values then
𝔼(X) =
⎧
⎪
⎪
⎨
⎪
⎪⎩
∞
∑
x=0
ℙ(X > x)
if X is a discrete random variable
∫
∞
0
ℙ(X ≥x) dx
if X is a continuous random variable.
Solution: We first show the result when X takes non-negative integer values only. By
definition
𝔼(X) =
∞
∑
y=0
yℙ(X = y)
=
∞
∑
y=0
y
∑
x=0
ℙ(X = y)
=
∞
∑
x=0
∞
∑
y=x+1
ℙ(X = y)
=
∞
∑
x=0
ℙ(X > x).
For the case when X is a continuous random variable taking non-negative values we have
𝔼(X) = ∫
∞
0
yfX(y) dy
= ∫
∞
0
{
∫
y
0
fX(y) dx
}
dy
= ∫
∞
0
{
∫
∞
x
fX(y) dy
}
dx
= ∫
∞
0
ℙ(X ≥x) dx.
◽
2. Hölder’s Inequality. Let 𝛼, 𝛽≥0 and for p, q > 1 such that 1
p + 1
q = 1 show that the fol-
lowing inequality:
𝛼𝛽≤𝛼p
p + 𝛽q
q

42
1.2.3
Properties of Expectations
holds. Finally, if X and Y are a pair of jointly continuous variables, show that
𝔼(|XY|) ≤{𝔼(|Xp|)}1∕p{𝔼(|Yq|)}1∕q.
Solution: The inequality certainly holds for 𝛼= 0 and 𝛽= 0. Let 𝛼= ex∕p and 𝛽= ey∕q
where x, y ∈ℝ. By substituting 𝜆= 1∕p and 1 −𝜆= 1∕q,
e𝜆x+(1−𝜆)y ≤𝜆ex + (1 −𝜆)ey
holds true since the exponential function is a convex function and hence
𝛼𝛽≤𝛼p
p + 𝛽q
q .
By setting
𝛼=
|X|
{𝔼(|Xp|)}1∕p ,
𝛽=
|Y|
{𝔼(|Yq|)}1∕q
hence
|XY|
{𝔼(|Xp|)}1∕p{𝔼(|Yq|)}1∕q ≤
|X|p
p𝔼(|Xp|) +
|Y|q
q𝔼(|Yq|).
Taking expectations we obtain
𝔼(|XY|) ≤{𝔼(|Xp|)}1∕p{𝔼(|Yq|)}1∕q.
◽
3. Minkowski’s Inequality. Let X and Y be a pair of jointly continuous variables, show that
if p ≥1 then
{𝔼(
|X + Y|p)}1∕p ≤{𝔼(|Xp|)}1∕p + {𝔼(|Yp|)}1∕p.
Solution: Since 𝔼(|X + Y|) ≤𝔼(|X|) + 𝔼(|Y|) and using Hölder’s inequality we can
write
𝔼(|X + Y|p) = 𝔼(|X + Y||X + Y|p−1)
≤𝔼(|X||X + Y|p−1) + 𝔼(|Y||X + Y|p−1)
≤{𝔼(|Xp|)}1∕p{𝔼(|X + Y|(p−1)q)}1∕q + {𝔼(|Yp|)}1∕p{𝔼(|X + Y|(p−1)q)}1∕q
= {𝔼(|Xp|)}1∕p{𝔼(|X + Y|p)}1∕q + {𝔼(|Yp|)}1∕p{𝔼(|X + Y|p)}1∕q
since 1
p + 1
q = 1.
Dividing the inequality by {𝔼(|X + Y|p)}1∕q we get
{𝔼(
|X + Y|p)}1∕p ≤{𝔼(|Xp|)}1∕p + {𝔼(|Yp|)}1∕p.
◽

1.2.3
Properties of Expectations
43
4. Change of Measure. Let Ω be a probability space and let ℙand ℚbe two probability
measures on Ω. Let Z(𝜔) be the Radon–Nikod´ym derivative defined as
Z(𝜔) = ℚ(𝜔)
ℙ(𝜔)
such that ℙ(Z > 0) = 1. By denoting 𝔼ℙand 𝔼ℚas expectations under the measure ℙand
ℚ, respectively, show that for any random variable X,
𝔼ℚ(X) = 𝔼ℙ(XZ),
𝔼ℙ(X) = 𝔼ℚ(X
Z
)
.
Solution: By definition
𝔼ℚ(X) =
∑
𝜔∈Ω
X(𝜔)ℚ(𝜔) =
∑
𝜔∈Ω
X(𝜔)Z(𝜔)ℙ(𝜔) = 𝔼ℙ(XZ).
Similarly
𝔼ℙ(X) =
∑
𝜔∈Ω
X(𝜔)ℙ(𝜔) =
∑
𝜔∈Ω
X(𝜔)
Z(𝜔)ℚ(𝜔) = 𝔼ℚ(X
Z
)
.
◽
5. Conditional Probability. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra
of ℱ(i.e., sets in 𝒢are also in ℱ). If 1IA is an indicator random variable for an event A
defined as
1IA(𝜔) =
{
1
if 𝜔∈A
0
otherwise
show that
𝔼(1IA|𝒢) = ℙ(A|𝒢).
Solution: Since 𝔼(1IA|𝒢) is 𝒢measurable we need to show that the following partial
averaging property:
∫B
𝔼(1IA|𝒢) dℙ= ∫B
1IA dℙ= ∫B
ℙ(A|𝒢) dℙ
is satisfied for B ∈𝒢. Setting
1IB(𝜔) =
{
1
if 𝜔∈B
0
otherwise
and
1IA∩B(𝜔) =
{
1
if 𝜔∈A ∩B
0
otherwise
and expanding ∫B
ℙ(A|𝒢) dℙwe have
∫B
ℙ(A|𝒢) dℙ= ℙ(A ∩B) = ∫Ω
1IA∩B dℙ= ∫Ω
1IA ⋅1IB dℙ= ∫B
1IA dℙ.

44
1.2.3
Properties of Expectations
Since 𝔼(1IA|𝒢) is 𝒢measurable we have
∫B
𝔼(1IA|𝒢) dℙ= ∫B
1IA dℙ
and hence 𝔼(1IA|𝒢) = ℙ(A|𝒢).
◽
6. Linearity. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e., sets
in 𝒢are also in ℱ). If X1, X2, . . . , Xn are integrable random variables and c1, c2, . . . , cn
are constants, show that
𝔼(c1X1 + c2X2 + . . . + cnXn|𝒢) = c1𝔼(X1|𝒢) + c2𝔼(X2|𝒢) + . . . + cn𝔼(Xn|𝒢).
Solution: Given 𝔼(c1X1 + c2X2 + . . . + cnXn|𝒢) is 𝒢measurable, and for any A ∈𝒢,
∫A
𝔼(c1X1 + c2X2 + . . . + cnXn|𝒢) dℙ= ∫A
(c1X1 + c2X2 + . . . + cnXn) dℙ
= c1∫A
X1 dℙ+ c2∫A
X2 dℙ
+ . . . + cn∫A
Xn dℙ.
Since ∫A
Xi dℙ= ∫A
𝔼(Xi|𝒢) dℙfor i = 1, 2, . . . , n therefore 𝔼(c1X1 + c2X2 + . . . +
cnXn|𝒢) = c1𝔼(X1|𝒢) + c2𝔼(X2|𝒢) + . . . + cn𝔼(Xn|𝒢).
◽
7. Positivity. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e., sets
in 𝒢are also in ℱ). If X is an integrable random variable such that X ≥0 almost surely
then show that
𝔼(X|𝒢) ≥0
almost surely.
Solution: Let A = {𝑤∈Ω ∶𝔼(X|𝒢) < 0} and since 𝔼(X|𝒢) is 𝒢measurable therefore
A ∈𝒢. Thus, from the partial averaging property we have
∫A
𝔼(X|𝒢) dℙ= ∫A
X dℙ.
Since X ≥0 almost surely therefore ∫A
X dℙ≥0 but ∫A
𝔼(X|𝒢) dℙ< 0, which is a con-
tradiction. Thus, ℙ(A) = 0, which implies 𝔼(X|𝒢) ≥0 almost surely.
◽
8. Monotonicity. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e.,
sets in 𝒢are also in ℱ). If X and Y are integrable random variables such that X ≤Y almost
surely then show that
𝔼(X|𝒢) ≤𝔼(Y|𝒢).

1.2.3
Properties of Expectations
45
Solution: Since 𝔼(X −Y|𝒢) is 𝒢measurable, for A ∈𝒢we can write
∫A
𝔼(X −Y|𝒢) dℙ= ∫A
(X −Y) dℙ
and since X ≤Y, from Problem 1.2.3.7 (page 44) we can deduce that
∫A
𝔼(X −Y|𝒢) dℙ≤0
and hence
𝔼(X −Y|𝒢) ≤0.
Using the linearity of conditional expectation (see Problem 1.2.3.6, page 44)
𝔼(X −Y|𝒢) = 𝔼(X|𝒢) −𝔼(Y|𝒢) ≤0
and therefore 𝔼(X|𝒢) ≤𝔼(Y|𝒢).
◽
9. Computing Expectations by Conditioning. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢
be a sub-𝜎-algebra of ℱ(i.e., sets in 𝒢are also in ℱ). Show that
𝔼[𝔼(X|𝒢)] = 𝔼(X).
Solution: From the partial averaging property we have, for A ∈𝒢,
∫A
𝔼(X|𝒢) dℙ= ∫A
X dℙ
or
𝔼[1IA ⋅𝔼(X|𝒢)] = 𝔼(1IA ⋅X)
where
1IA(𝜔) =
{
1
if 𝜔∈A
0
otherwise
is a 𝒢measurable random variable. By setting A = Ω we obtain 𝔼[𝔼(X|𝒢)] = 𝔼(X).
◽
10. Taking Out What is Known. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a
sub-𝜎-algebra of ℱ(i.e., sets in 𝒢are also in ℱ). If X and Y are integrable random
variables and if X is 𝒢measurable show that
𝔼(XY|𝒢) = X ⋅𝔼(Y|𝒢).

46
1.2.3
Properties of Expectations
Solution: Since X and 𝔼(Y|𝒢) are 𝒢measurable therefore X ⋅𝔼(Y|𝒢) is also 𝒢measur-
able and it satisfies the first property of conditional expectation. By calculating the partial
averaging of X ⋅𝔼(Y|𝒢) over a set A ∈𝒢and by defining
1IA(𝜔) =
{
1
if 𝜔∈A
0
otherwise
such that 1IA is a 𝒢measurable random variable we have
∫A
X ⋅𝔼(Y|𝒢) dℙ= 𝔼[1IA ⋅X𝔼(Y|𝒢)]
= 𝔼[1IA ⋅XY]
= ∫A
XY dℙ.
Thus, X ⋅𝔼(Y|𝒢) satisfies the partial averaging property by setting ∫A
XY dℙ=
∫A
𝔼(XY|𝒢) dℙ. Therefore, 𝔼(XY|𝒢) = X ⋅𝔼(Y|𝒢).
◽
11. Tower Property. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ
(i.e., sets in 𝒢are also in ℱ). If ℋis a sub-𝜎-algebra of 𝒢(i.e., sets in ℋare also in 𝒢)
and X is an integrable random variable, show that
𝔼[𝔼(X|𝒢)|ℋ] = 𝔼(X|ℋ).
Solution: For an integrable random variable Y, by definition we know that 𝔼(Y|ℋ) is
ℋmeasurable, and hence by setting Y = 𝔼(X|𝒢), and for A ∈ℋ, the partial averaging
property of 𝔼[𝔼(X|𝒢)|ℋ] is
∫A
𝔼[𝔼(X|𝒢)|ℋ] dℙ= ∫A
𝔼(X|𝒢) dℙ.
Since A ∈ℋand ℋis a sub-𝜎-algebra of 𝒢, A ∈𝒢. Therefore,
∫A
𝔼(X|ℋ) dℙ= ∫A
X dℙ= ∫A
𝔼(X|𝒢) dℙ.
This shows that 𝔼(X|ℋ) satisfies the partial averaging property of 𝔼[𝔼(X|𝒢)|ℋ], and
hence 𝔼[𝔼(X|𝒢)|ℋ] = 𝔼(X|ℋ).
◽
12. Measurability. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e.,
sets in 𝒢are also in ℱ). If the random variable X is 𝒢measurable then show that
𝔼(X|𝒢) = X.

1.2.3
Properties of Expectations
47
Solution: From the partial averaging property, for A ∈Ω,
∫A
𝔼(X|𝒢) dℙ= ∫A
X dℙ
and if X is 𝒢measurable then it satisfies
𝔼(X|𝒢) = X.
◽
13. Independence. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e.,
sets in 𝒢are also in ℱ). If X = 1IB such that
1IB(𝜔) =
{
1
if 𝜔∈B
0
otherwise
and 1IB is independent of 𝒢show that
𝔼(X|𝒢) = 𝔼(X).
Solution: Since 𝔼(X) is non-random then 𝔼(X) is 𝒢measurable. Therefore, we now need
to check that the following partial averaging property:
∫A
𝔼(X) dℙ= ∫A
X dℙ= ∫A
𝔼(X|𝒢) dℙ
is satisfied for A ∈𝒢.
Let X = 1IB such that
1IB(𝜔) =
{
1
if 𝜔∈B
0
otherwise
and the random variable 1IB is independent of 𝒢. In addition, we also define
1IA(𝜔) =
{
1
if 𝜔∈A
0
otherwise
where 1IA is 𝒢measurable. For all A ∈𝒢we have
∫A
X dℙ= ∫A
1IB dℙ= ∫A
ℙ(B) dℙ= ℙ(A)ℙ(B).
Furthermore, since the sets A and B are independent we can also write
∫A
X dℙ= ∫A
1IB dℙ= ∫Ω
1IA1IB dℙ= ∫Ω
1IA∩B dℙ= ℙ(A ∩B)

48
1.2.3
Properties of Expectations
where
1IA∩B(𝜔) =
{
1
if 𝜔∈A ∩B
0
otherwise
and hence
∫A
X dℙ= ℙ(A ∩B) = ℙ(A)ℙ(B) = ℙ(A)𝔼(X) = ∫A
𝔼(X) dℙ.
Thus, we have 𝔼(X|𝒢) = 𝔼(X).
◽
14. Conditional Jensen’s Inequality. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a
sub-𝜎-algebra of ℱ(i.e., sets in 𝒢are also in ℱ). If 𝜑∶ℝ→ℝis a convex function and
X is an integrable random variable show that
𝔼[𝜑(X)|𝒢] ≥𝜑[𝔼(X|𝒢)].
Deduce that if X is independent of 𝒢then the above inequality becomes
𝔼[𝜑(X)] ≥𝜑[𝔼(X)].
Solution: Given that 𝜑is a convex function,
𝜑(x) ≥𝜑(y) + 𝜑′(y)(y −x).
By setting x = X and y = 𝔼(X|𝒢) we have
𝜑(X) ≥𝜑[𝔼(X|𝒢)] + 𝜑′[𝔼(X|𝒢)][𝔼(X|𝒢) −X]
and taking conditional expectations,
𝔼[𝜑(X)|𝒢] ≥𝜑[𝔼(X|𝒢)].
If X is independent of 𝒢then from Problem 1.2.3.13 (page 47) we can set y = 𝔼(X|𝒢) =
𝔼(X). Using the same steps as described above we have
𝜑(X) ≥𝜑[𝔼(X)] + 𝜑′[𝔼(X)][𝔼(X) −X]
and taking expectations we finally have
𝔼[𝜑(X)] ≥𝜑[𝔼(X)].
◽

1.2.3
Properties of Expectations
49
15. Let (Ω, ℱ, ℙ) be a probability space and let 𝒢be a sub-𝜎-algebra of ℱ(i.e., sets in 𝒢are
also in ℱ). If X is an integrable random variable and 𝔼(X2) < ∞show that
𝔼[𝔼(X|𝒢)2] ≤𝔼(X2) .
Solution: From the conditional Jensen’s inequality (see Problem 1.2.3.14, page 48) we
set 𝜑(x) = x2 which is a convex function. By substituting x = 𝔼(X|𝒢) we have
𝔼(X|𝒢)2 ≤𝔼(X2|𝒢) .
Taking expectations
𝔼[𝔼(X|𝒢)2] ≤𝔼[𝔼(X2|𝒢)]
and from the tower property (see Problem 1.2.3.11, page 46)
𝔼[𝔼(X2|𝒢)] = 𝔼(X2) .
Thus, 𝔼[𝔼(X|𝒢)2] ≤𝔼(X2).
◽


2
Wiener Process
In mathematics, a Wiener process is a stochastic process sharing the same behaviour as Brow-
nian motion, which is a physical phenomenon of random movement of particles suspended in a
fluid. Generally, the terms “Brownian motion” and “Wiener process” are the same, although the
former emphasises the physical aspects whilst the latter emphasises the mathematical aspects.
In quantitative analysis, by drawing on the mathematical properties of Wiener processes to
explain economic phenomena, financial information such as stock prices, commodity prices,
interest rates, foreign exchange rates, etc. are treated as random quantities and then mathe-
matical models are constructed to capture the randomness. Given these financial models are
stochastic and continuous in nature, the Wiener process is usually employed to express the
random component of the model. Before we discuss the models in depth, in this chapter we
first look at the definition and basic properties of a Wiener process.
2.1
INTRODUCTION
By definition, a random walk is a mathematical formalisation of a trajectory that consists of
taking successive random steps at every point in time. To construct a Wiener process in con-
tinuous time, we begin by setting up a symmetric random walk – such as tossing a fair coin
infinitely many times where the probability of getting a head (H) or a tail (T) in each toss is
1
2. By defining the i-th toss as
Zi =
{
1
if toss is H
−1
if toss is T
and setting M0 = 0, the process
Mk =
k
∑
i=1
Zi,
k = 1, 2, . . .
is a symmetric random walk. In a continuous time setting, to approximate a Wiener process
for n ∈ℤ+ we define the scaled symmetric random walk as
W(n)
t
=
1
√
n
M⌊nt⌋=
1
√
n
⌊nt⌋
∑
i=1
Zi
such that in the limit of n →∞we can obtain the Wiener process where
lim
n→∞W(n)
t
= lim
n→∞
1
√
n
⌊nt⌋
∑
i=1
Zi
D
−−→𝒩(0, t).

52
2.1
INTRODUCTION
Given that a Wiener process is a limiting distribution of a scaled symmetric random walk,
the following is the definition of a standard Wiener process.
Definition 2.1 (Standard Wiener Process) Let (Ω, ℱ, ℙ) be a probability space. A stochas-
tic process {Wt ∶t ≥0} is defined to be a standard Wiener process (or ℙ-standard Wiener
process) if:
(a) W0 = 0 and has continuous sample paths;
(b) for each t > 0 and s > 0, Wt+s −Wt ∼𝒩(0, s) (stationary increment);
(c) for each t > 0 and s > 0, Wt+s −Wt ⟂⟂Wt (independent increment).
A standard Wiener process is a standardised version of a Wiener process, which need not
begin at W0 = 0, and may have a non-zero drift term 𝜇≠0 and a variance term not necessarily
equal to t.
Definition 2.2 (Wiener Process) A process { ̂Wt ∶t ≥0} is called a Wiener process if it can
be written as
̂Wt = 𝜈+ 𝜇t + 𝜎Wt
where 𝜈, 𝜇∈ℝ, 𝜎> 0 and Wt is a standard Wiener process.
Almost all financial models have the Markov property and without exception the Wiener
process also has this important property, where it is used to relate stochastic calculus to partial
differential equations and ultimately to the pricing of options. The following is an important
result concerning the Markov property of a standard Wiener process.
Theorem 2.3 (Markov Property) Let (Ω, ℱ, ℙ) be a probability space. The standard Wiener
process {Wt ∶t ≥0} is a Markov process such that the conditional distribution of Wt given
the filtration ℱs, s < t depends only on Ws.
Another generalisation from the Markov property is the strong Markov property, which is an
important result in establishing many other properties of Wiener processes such as martingales.
Clearly, the strong Markov property implies the Markov property but not vice versa.
Theorem 2.4 (Strong Markov Property) Let (Ω, ℱ, ℙ) be a probability space. If {Wt ∶t ≥
0} is a standard Wiener process and given ℱt is the filtration up to time t, then for s > 0,
Wt+s −Wt ⟂⟂ℱt.
Once we have established the Markov properties, we can use them to show that a Wiener
process is a martingale. Basically, a stochastic process is a martingale when its conditional
expected value of an observation at some future time t, given all the observations up to some
earlier time s, is equal to the observation at that earlier time s. In formal terms the definition
of this property is given as follows.

2.1
INTRODUCTION
53
Definition 2.5 (Martingale Property for Continuous Process) Let (Ω, ℱ, ℙ) be a probabil-
ity space. A stochastic process {Xt ∶t ≥0} is a continuous-time martingale if:
(a) 𝔼(Xt|ℱs) = Xs, for all 0 ≤s ≤t;
(b) 𝔼(|Xt|) < ∞;
(c) Xt is ℱt-adapted.
In addition to properties (b) and (c), the process is a submartingale if 𝔼(Xt|ℱs) ≥Xs and a
supermartingale if 𝔼(Xt|ℱs) ≤Xs for all 0 ≤s ≤t.
In contrast, we can also define the martingale property for a discrete process.
Definition 2.6 (Martingale Property for Discrete Process) A discrete process X = {Xn ∶
n = 0, 1, 2, . . .} is a martingale relative to (Ω, ℱ, ℙ) if for all n:
(a) 𝔼(Xn+1|ℱn) = Xn;
(b) 𝔼(|Xn|) < ∞;
(c) Xn is ℱn-adapted.
Together with properties (b) and (c), the process is a submartingale if 𝔼(Xn+1|ℱn) ≥Xn and a
supermartingale if 𝔼(Xn+1|ℱn) ≤Xn for all n.
A most important application of the martingale property of a stochastic process is in the area
of derivatives pricing where under the risk-neutral measure, to avoid any arbitrage opportu-
nities, all asset prices have the same expected rate of return that is the risk-free rate. As we
shall see in later chapters, by modelling an asset price with a Wiener process to represent the
random component, under the risk-neutral measure, the expected future price of the asset dis-
counted at a risk-free rate given its past information is a martingale. In addition, martingales
do have certain features even when they are stopped at random times, as given in the following
theorem.
Theorem 2.7 (Optional Stopping (Sampling)) Let (Ω, ℱ, ℙ) be a probability space. A ran-
dom time T ∈[0, ∞) is called a stopping time if {T ≤t} ∈ℱt for all t ≥0. If T < ∞is a
stopping time and {Xt} is a martingale, then 𝔼(XT) = 𝔼(X0) if any of the following are true:
(a) 𝔼(T) < ∞;
(b) there exists a constant K such that 𝔼(|Xt+Δt −Xt|) ≤K, for Δt > 0.
Take note that an important application of the optional stopping theorem is the first passage
time of a standard Wiener process hitting a level. Here one can utilise it to analyse American
options, where in this case the exercise time is a stopping time. In tandem with the stop-
ping time, the following reflection principle result allows us to find the joint distribution of
(max0≤s≤tWs, Wt) and the distribution of max0≤s≤tWs, which in turn can be used to price exotic
options such as barrier and lookback options.

54
2.1
INTRODUCTION
Theorem 2.8 (Reflection Principle) Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥
0} be a standard Wiener process. By setting T as a stopping time and defining
̃Wt =
{
Wt
if t ≤T
2WT −Wt
if t > T
then { ̃Wt ∶t ≥0} is also a standard Wiener process.
Finally, another useful property of the Wiener process is the quadratic variation,
where if {Wt ∶t ≥0} is a standard Wiener process then by expressing dWt as the
infinitesimal increment of Wt and setting ti = it∕n, i = 0, 1, 2, . . . , n −1, n > 0 such that
0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, the quadratic variation of Wt is defined as
lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)2 = ∫
t
0
dW2
u = t
which has a finite value. In addition, the cross-variation of Wt and t, the quadratic variation of
t and the p-order variation, p ≥3 of Wt can be expressed as
lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)(ti+1 −ti) = ∫
t
0
dWudu = 0,
lim
n→∞
n−1
∑
i=0
(ti+1 −ti)2 = ∫
t
0
du2 = 0
and
lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)p = ∫
t
0
dWp
u = 0,
p ≥3.
Informally, we can therefore write
(dWt)2 = dt,
dWtdt = 0,
(dt)2 = 0,
(dWt)p = 0,
p ≥3
where dWt and dt are the infinitesimal increment of Wt and t, respectively. The significance
of the above results constitutes the key ingredients in It¯o’s formula to find the differential of a
stochastic function and also in deriving the Black–Scholes equation to price options.
In essence, the many properties of the Wiener process made it a very suitable choice to
express the random component when modelling stock prices, interest rates, foreign currency
exchange rates, etc. One notable example is when pricing European-style options, where,
due to its inherent properties we can obtain closed-form solutions which would not be pos-
sible had it been modelled using other processes. In addition, we could easily extend the
one-dimensional Wiener process to a multi-dimensional Wiener process to model stock prices
that are correlated with each other as well as stock prices under stochastic volatility. How-
ever, owing to the normal distribution of the standard Wiener process it tends to provide stock
price returns that are symmetric and short-tailed. In practical situations, stock price returns are
skewed and have heavy tails and hence they do not follow a normal distribution. Nevertheless,

2.2.1
Basic Properties
55
even if there exist complex models to capture such attributes (for example, using a Lévy
process), a Wiener process is still a vital component to model sources of uncertainty in finance.
2.2
PROBLEMS AND SOLUTIONS
2.2.1
Basic Properties
1. Let (Ω, ℱ, ℙ) be a probability space. We consider a symmetric random walk such that the
j-th step is defined as
Zj =
⎧
⎪
⎨
⎪⎩
1
with probability 1
2
−1
with probability 1
2
where Zi ⟂⟂Zj, i ≠j. By setting 0 = k0 < k1 < k2 < . . . < kt, we let
Mki =
ki
∑
j=1
Zj,
i = 1, 2, . . . , t
where M0 = 0.
Show that the symmetric random walk has independent increments such that the random
variables
Mk1 −Mk0, Mk2 −Mk1, . . . , Mkt −Mkt−1
are independent.
Finally, show that 𝔼(Mki+1 −Mki) = 0 and Var(Mki+1 −Mki) = ki+1 −ki.
Solution: By definition
Mki −Mki−1 =
ki
∑
j=ki−1+1
Zj = si
and for m < n, m, n = 1, 2, . . . , t,
ℙ(Mkn −Mkn−1 = sn|Mkm −Mkm−1 = sm)
=
ℙ(Mkn −Mkn−1 = sn, Mkm −Mkm−1 = sm)
ℙ(Mkm −Mkm−1 = sm)
= ℙ(sum of walks in [kn−1 + 1, kn] ∩sum of walks in [km−1 + 1, km])
ℙ(sum of walks in [km−1 + 1, km])
.
Because m < n and since Zi is independent of Zj, i ≠j, there are no overlapping
events between the intervals [kn−1 + 1, kn] and [km−1 + 1, km] and hence for m < n,
m, n = 1, 2, . . . , t,
ℙ(Mkn −Mkn−1 = sn|Mkm −Mkm−1 = sm) = ℙ(sum of walks in [kn−1 + 1, kn])
= ℙ(Mkn −Mkn−1 = sn).

56
2.2.1
Basic Properties
Thus, we can deduce that Mk1 −Mk0, Mk2 −Mk1, . . . , Mkt −Mkt−1 are independent.
Since 𝔼(Zj) = 1 ⋅1
2 −1 ⋅1
2 = 0 and Var(Zj) = 𝔼(Z2
j ) = 1 ⋅1
2 + 1 ⋅1
2 = 1, and because Zj,
j = 1, 2, . . . are independent, we have
𝔼(Mki −Mki−1) =
ki
∑
j=ki−1+1
𝔼(Zj) = 0
and
Var(Mki −Mki−1) =
ki
∑
j=ki−1+1
Var(Zj) =
ki
∑
j=ki−1+1
1 = ki −ki−1.
◽
2. Let (Ω, ℱ, ℙ) be a probability space. For a symmetric random walk
Mk =
k
∑
i=1
Zi
with starting point at M0 = 0, ℙ(Zi = 1) = ℙ(Zi = −1) = 1
2, show that Mk is a martingale.
Solution: To show that Mk is a martingale we note that
(a) For j < k, j, k ∈ℤ+, using the independent increment property and 𝔼(Mk −Mj) = 0
as shown in Problem 2.2.1.1 (page 55),
𝔼(Mk|ℱj) = 𝔼(Mk −Mj + Mj|ℱj)
= 𝔼(Mk −Mj|ℱj) + 𝔼(Mj|ℱj)
= 𝔼(Mk −Mj) + Mj
= Mj.
(b) Given Mk = ∑k
i=1 Zi it follows that
|Mk| =
||||||
k
∑
i=1
Zi
||||||
≤
k
∑
i=1
|Zi| =
k
∑
i=1
1 = k < ∞.
(c) Mk is clearly ℱk-adapted.
From the results of (a)–(c), we have shown that Mk is a martingale.
◽
3. Donsker Theorem. Let (Ω, ℱ, ℙ) be a probability space. For a symmetric random walk
Mk =
k
∑
i=1
Zi

2.2.1
Basic Properties
57
where M0 = 0, ℙ(Zi = 1) = ℙ(Zi = −1) = 1
2 and by defining
W(n)
t
=
1
√
n
M⌊nt⌋=
1
√
n
⌊nt⌋
∑
i=1
Zi
for a fixed time t, show that
lim
n→∞W(n)
t
= lim
n→∞
1
√
n
⌊nt⌋
∑
i=1
Zi
D
−−→𝒩(0, t).
Solution: Since for all i = 1, 2, . . . ,
𝔼(Zi) = 0
and
Var(Zi) = 1
and given Zi ⟂⟂Zj, i ≠j,
𝔼
(
W(n)
t
)
=
1
√
n
⌊nt⌋
∑
i=1
𝔼(Zi) = 0
and
Var
(
W(n)
t
)
= 1
n
⌊nt⌋
∑
i=1
Var(Zi) = 1
n
⌊nt⌋
∑
i=1
1 = ⌊nt⌋
n .
For n →∞we can deduce that
lim
n→∞𝔼
(
W(n)
t
)
= 0
and
lim
n→∞Var
(
W(n)
t
)
= lim
n→∞
⌊nt⌋
n
= t.
Therefore, from the central limit theorem,
lim
n→∞W(n)
t
= lim
n→∞
1
√
n
⌊nt⌋
∑
i=1
Zi
D
−−→𝒩(0, t).
◽
4. Covariance of Two Standard Wiener Processes. Let (Ω, ℱ, ℙ) be a probability space and
let {Wt ∶t ≥0} be a standard Wiener process. Show that
Cov(Ws, Wt) = min{s, t}
and deduce that the correlation coefficient of Ws and Wt is
𝜌=
√
min{s, t}
max{s, t}.

58
2.2.1
Basic Properties
Solution: Since Wt ∼𝒩(0, t), Ws ∼𝒩(0, s) and by definition
Cov(Ws, Wt) = 𝔼(WsWt) −𝔼(Ws)𝔼(Wt) = 𝔼(WsWt).
Let s ≤t and because Ws ⟂⟂Wt −Ws,
𝔼(WsWt) = 𝔼(Ws(Wt −Ws) + W2
s
)
= 𝔼(Ws(Wt −Ws)) + 𝔼(W2
s
)
= 𝔼(Ws)𝔼(Wt −Ws) + 𝔼(W2
s
)
= s.
For s > t and because Wt ⟂⟂Ws −Wt,
𝔼(WsWt) = 𝔼(Wt(Ws −Wt) + W2
t
)
= 𝔼(Wt)𝔼(Ws −Wt) + 𝔼(W2
t
)
= t.
Therefore, Cov(Ws, Wt) = s ∧t = min{s, t}.
By definition, the correlation coefficient of Ws and Wt is defined as
𝜌=
Cov(Ws, Wt)
√
Var(Ws)Var(Wt)
= min{s, t}
√
st
.
For s ≤t,
𝜌=
s
√
st
=
√
s
t
whilst for s > t,
𝜌=
t
√
st
=
√
t
s.
Therefore, 𝜌=
√
min{s, t}
max{s, t}.
◽
5. Joint Distribution of Standard Wiener Processes. Let (Ω, ℱ, ℙ) be a probability space and
let {Wtk ∶tk ≥0}, k = 0, 1, 2, . . . , n be a standard Wiener process where 0 = t0 < t1 <
t2 < . . . < tn.
Find the moment generating function of the joint distribution (Wt1, Wt2, . . . , Wtn) and its
corresponding probability density function.

2.2.1
Basic Properties
59
Show that for t < T, the conditional distributions
Wt|WT = y ∼𝒩
(yt
T , t(T −t)
T
)
and
WT|Wt = x ∼𝒩(x, T −t).
Solution: By definition, the moment generating function for the joint distribution (Wt1,
Wt2, . . . , Wtn) is given as
MWt1,Wt2, ... ,Wtn
(
𝜃1, 𝜃2, . . . , 𝜃n
)
= 𝔼
(
e𝜃1Wt1+𝜃2Wt2+ ... +𝜃nWtn
)
,
𝜃1, 𝜃2, . . . , 𝜃n ∈ℝ.
Given that Wt1, Wt2 −Wt1, . . . , Wtn −Wtn−1 are independent and normally distributed, we
can write
𝜃1Wt1 + 𝜃2Wt2 + . . . + 𝜃nWtn = (𝜃1 + 𝜃2 + . . . + 𝜃n)Wt1 + (𝜃2 + . . . + 𝜃n)(Wt2 −Wt1)
+ . . . + 𝜃n(Wtn −Wtn−1)
and since for any 𝜃, s < t, e𝜃(Wt−Ws) ∼log-𝒩(0, 𝜃2(t −s)) therefore
𝔼
(
e𝜃1Wt1+𝜃2Wt2+ ... +𝜃nWtn
)
= 𝔼
[
e(𝜃1+𝜃2+ ... +𝜃n)Wt1+(𝜃2+ ... +𝜃n)
(
Wt2−Wt1
)
+ ... +𝜃n
(
Wtn−Wtn−1
)]
= 𝔼
[
e(𝜃1+𝜃2+ ... +𝜃n)Wt1
]
𝔼
[
e(𝜃2+ ... +𝜃n)(Wt2−Wt1)]
× . . . × 𝔼
[
e
𝜃n
(
Wtn−Wtn−1
)]
= e
1
2(𝜃1+𝜃2+ ... +𝜃n)2t1+ 1
2(𝜃2+ ... +𝜃n)2(t2−t1)+ ... + 1
2 𝜃2
n(tn−tn−1)
= e
1
2 𝜽T𝚺𝜽
where 𝜽T = (𝜃1, 𝜃2, . . . , 𝜃n) and 𝚺is the covariance matrix for the Wiener process. From
Problem 2.2.1.4 (page 57) we can express
𝚺=
⎡
⎢
⎢
⎢
⎢⎣
𝔼(W2
t1)
𝔼(Wt1Wt2) . . . 𝔼(Wt1Wtn)
𝔼(Wt2Wt1) 𝔼(W2
t2)
. . . 𝔼(Wt2Wtn)
⋮
⋮
⋱⋮
𝔼(WtnWt1) 𝔼(WtnWt2) . . . 𝔼(W2
tn)
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢⎣
t1 t1 . . . t1
t1 t2 . . . t2
⋮⋮⋱⋮
t1 t2 . . . tn
⎤
⎥
⎥
⎥⎦
.
Using elementary column operations we can easily show that the determinant
|𝚺| = ∏n
i=1(ti −ti−1) ≠0. Therefore, 𝚺−1 exists and the probability density function for
the joint distribution (Wt1, Wt2, . . . , Wtn) is given as
fWt1,Wt2, ... ,Wtn (x) =
1
(2𝜋)
n
2 |𝚺|
1
2
e−1
2 xT𝚺−1x
where xT = (x1, x2, . . . , xn).

60
2.2.1
Basic Properties
For the case of joint distribution of (Wt, WT), t < T we can deduce that
fWt,WT(x, y) =
1
(2𝜋)
√
t(T −t)
exp
[
−1
2
(Tx2 −2txy + ty2
t(T −t)
)]
with conditional density function of Wt given WT = y
fWt|WT(x|y) =
fWt,WT(x, y)
fWT(y)
=
1
√
2𝜋
(t(T −t)
T
)
exp
⎡
⎢
⎢
⎢⎣
−1
2
(
x −yt
T
)2
t(T −t)
T
⎤
⎥
⎥
⎥⎦
and conditional density function of WT given Wt = x
fWT|Wt(y|x) =
fWt,WT(x, y)
fWt(x)
=
1
√
2𝜋(T −t)
exp
[
−1
2
(y −x)2
T −t
]
.
Thus,
Wt|WT = y ∼𝒩
(yt
T , t(T −t)
T
)
and
WT|Wt = x ∼𝒩(x, T −t).
◽
6. Reflection. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener
process. Show that under reflection, Bt = −Wt is also a standard Wiener process.
Solution:
(a) B0 = −W0 = 0, and given that Wt has continuous sample paths we can deduce that Bt
has continuous sample paths as well.
(b) For t > 0, s > 0
Bt+s −Bt = −Wt+s + Wt = −(Wt+s −Wt) ∼𝒩(0, s).
(c) Since Wt+s −Wt ⟂⟂Wt therefore
𝔼[(Bt+s −Bt)Bt] = Cov(−Wt+s + Wt, −Wt) = −Cov(Wt+s −Wt, Wt) = 0.
Given Bt ∼𝒩(0, t) and Bt+s −Bt ∼𝒩(0, s) and the joint distribution of Bt and Bt+s −
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s −Bt, Bt) = 0
so Bt+s −Bt ⟂⟂Bt.
From the results of (a)–(c) we have shown that Bt = −Wt is also a standard Wiener pro-
cess.
◽

2.2.1
Basic Properties
61
7. Time Shifting. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. Show that under time shifting, Bt = Wt+u −Wu, u > 0 is also a standard
Wiener process.
Solution: By setting ̃t = t + u.
(a) B0 = Wu −Wu = 0, and given Wt has continuous sample paths therefore we can
deduce Bt has continuous sample paths as well.
(b) For t > 0, u > 0, s > 0
Bt+s −Bt = W̃t+s −W̃t+u ∼𝒩(0, s).
(c) Since Wt+s −Wt ⟂⟂Wt therefore
𝔼[(Bt+s −Bt)Bt] = Cov(Wt+u+s −Wt+u, Wt+u) = Cov(W̃t +s −W̃t , W̃t ) = 0.
Given Bt ∼𝒩(0, t) and Bt+s −Bt ∼𝒩(0, s) and the joint distribution of Bt and Bt+s −
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s −Bt, Bt) = 0
so Bt+s −Bt ⟂⟂Bt.
From the results of (a)–(c) we have shown that Bt = Wt+u −Wu is also a standard Wiener
process.
◽
8. Normal Scaling. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. Show that under normal scaling, Bt = cW t
c2 , c ≠0 is also a standard
Wiener process.
Solution:
(a) B0 = cW0 = 0 and it is clear that Bt has continuous sample paths for t ≥0 and c ≠0.
(b) For t > 0, s > 0
Bt+s −Bt = c
(
W t+s
c2 −W t
c2
)
with mean
𝔼(Bt+s −Bt) = c𝔼
(
W t+s
c2
)
−c𝔼
(
W t
c2
)
= 0
and variance
Var(Bt+s −Bt) = Var
(
cW t+s
c2
)
+ Var
(
cW t
c2
)
−2Cov
(
cW t+s
c2 , cW t
c2
)
= t + s + t −2min{t + s, t}
= s.
Since both W t+s
c2 ∼𝒩
(
0, t + s
c2
)
and W t
c2 ∼𝒩
(
0, t
c2
)
, then Bt+s −Bt ∼𝒩(0, s).

62
2.2.1
Basic Properties
(c) Finally, to show that Bt+s −Bt ⟂⟂Bt we note that since Wt has the independent incre-
ment property, so
𝔼[(Bt+s −Bt
) Bt
] = 𝔼(Bt+sBt) −𝔼(B2
t
)
= Cov(Bt+s, Bt) −𝔼(B2
t
)
= min{t + s, t} −t
= 0.
Since Bt ∼𝒩(0, t) and Bt+s −Bt ∼𝒩(0, s) and the joint distribution of Bt and Bt+s −
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s −Bt, Bt) = 0
so Bt+s −Bt ⟂⟂Bt.
From the results of (a)–(c) we have shown that Bt = cW t
c2 , c ≠0 is also a standard Wiener
process.
◽
9. Time Inversion. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. Show that under time inversion,
Bt =
{
0
if t = 0
tW 1
t
if t ≠0
is also a standard Wiener process.
Solution:
(a) B0 = 0 and it is clear that Bt has continuous sample paths for t > 0. From continuity
at t = 0 we can deduce that tW 1
t →0 as t →0.
(b) Since Wt ∼𝒩(0, t) we can deduce tW 1
t ∼𝒩(0, t), t > 0. Therefore,
Bt+s −Bt = (t + s)W 1
t+s −tW 1
t
with mean
𝔼(Bt+s −Bt) = 𝔼
(
(t + s)W 1
t+s
)
−𝔼
(
tW 1
t
)
= 0
and variance
Var(Bt+s −Bt) = Var
(
(t + s)W 1
t+s
)
+ Var
(
tW 1
t
)
−2Cov
(
(t + s)W 1
t+s , tW 1
t
)
= t + s + t −2min{t + s, s}
= s.
Since the sum of two normal distributions is also a normal distribution, so Bt+s −Bt ∼
𝒩(0, s).

2.2.1
Basic Properties
63
(c) To show that Bt+s −Bt ⟂⟂Bt we note that since Wt has the independent increment
property, so
𝔼[(Bt+s −Bt)Bt] = 𝔼(Bt+sBt) −𝔼(B2
t
)
= Cov(Bt+s, Bt) −𝔼(B2
t
)
= min{t + s, t} −t
= 0.
Since Bt ∼𝒩(0, t) and Bt+s −Bt ∼𝒩(0, s) and the joint distribution of Bt and Bt+s −
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s −Bt, Bt) = 0
so Bt+s −Bt ⟂⟂Bt.
From the results of (a)–(c) we have shown that Bt =
{
0
if t = 0
tW 1
t if t ≠0 is also a standard
Wiener process.
◽
10. Time Reversal. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. Show that under time reversal, Bt = W1 −W1−t is also a standard Wiener
process.
Solution:
(a) B0 = W1 −W1 = 0 and it is clear that Bt has continuous sample paths for t ≥0.
(b) Since Wt is a standard Wiener process, we have W1 ∼𝒩(0, 1) and W1−t ∼𝒩(0, 1 −t).
Therefore,
Bt+s −Bt = W1 −W1−(t+s) −W1 + W1−t = W1−t −W1−(t+s)
with mean
𝔼(Bt+s −Bt) = 𝔼(W1−t) −𝔼(W1−(t+s)) = 0
and variance
Var(Bt+s −Bt) = Var(W1−t) + Var(W1−(t+s)) −2Cov(W1−t, W1−(t+s))
= 1 −t + 1 −(t + s) −2min{1 −t, 1 −(t + s)}
= s.
Since the sum of two normal distributions is also a normal distribution, so Bt+s −Bt ∼
𝒩(0, s).
(c) To show that Bt+s −Bt ⟂⟂Bt we note that since Wt has the independent increment
property, so
𝔼[(Bt+s −Bt)Bt] = 𝔼(Bt+sBt) −𝔼(B2
t
)
= Cov(Bt+s, Bt) −𝔼(B2
t
)

64
2.2.1
Basic Properties
= min{t + s, t} −t
= 0.
Given Bt ∼𝒩(0, t) and Bt+s −Bt ∼𝒩(0, s) and the joint distribution of Bt and Bt+s −
Bt is a bivariate normal (see Problem 2.2.1.5, page 58), then if Cov(Bt+s −Bt, Bt) = 0
so Bt+s −Bt ⟂⟂Bt.
From the results of (a)–(c) we have shown that Bt = W1 −W1−t is also a standard Wiener
process.
◽
11. Multi-Dimensional Standard Wiener Process. Let (Ω, ℱ, ℙ) be a probability space and let
{W(i)
t
∶t ≥0}, i = 1, 2, . . . , n be a sequence of independent standard Wiener processes.
Show that the vector Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)T
is an n-dimensional standard Wiener
process with the following properties:
(a) W0 = 𝟎and Wt is a vector of continuous sample paths;
(b) for each t > 0 and s > 0, Wt+s −Wt ∼𝒩n(𝟎, sI) where I is an n × n identity matrix;
(c) for each t > 0 and s > 0, Wt+s −Wt ⟂⟂Wt.
Solution:
(a) Since W(i)
0 = 0 for all i = 1, 2, . . . , n therefore W0 =
(
W(1)
0 , W(2)
0 , . . . , W(n)
0
)T
= 𝟎
where 𝟎= (0, 0, . . . , 0)T is an n-vector of zeroes. In addition, Wt is a vector of con-
tinuous sample paths due to the fact that W(i)
t , t ≥0 has continuous sample paths.
(b) For s, t > 0 and for i, j = 1, 2, . . . , n we have
𝔼(W(i)
t+s −W(i)
t
) = 0,
i = 1, 2, . . . , n
and
𝔼
[(
W(i)
t+s −W(i)
t
) (
W(j)
t+s −W(j)
t
) ]
=
{
s
i = j
0
i ≠j.
Therefore, Wt+s −Wt ∼𝒩n(𝟎, sI) where I is an n × n identity matrix.
(c) For s, t > 0 and since W(i)
t+s −W(i)
t
⟂⟂W(i)
t , W(i)
t
⟂⟂W(j)
t , i ≠j, i, j = 1, 2, . . . , n we can
deduce that W(i)
t+s −W(i)
t
⟂⟂W(j)
t . Therefore, Wt+s −Wt ⟂⟂Wt.
From the results of (a)–(c) we have shown that Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)T
is an
n-dimensional standard Wiener process.
◽
12. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that the pair of random variables
(
Wt, ∫t
0 Ws ds
)
has the following covariance
matrix
𝚺=
⎡
⎢
⎢⎣
t
1
2t2
1
2t2
1
3t3
⎤
⎥
⎥⎦
with correlation coefficient
√
3
2 .

2.2.1
Basic Properties
65
Solution: By definition, the covariance matrix for the pair
(
Wt, ∫t
0 Ws ds
)
is
𝚺=
⎡
⎢
⎢⎣
Var(Wt)
Cov
(
Wt, ∫t
0 Ws ds
)
Cov
(
Wt, ∫t
0 Ws ds
)
Var
(
∫t
0 Ws ds
)
⎤
⎥
⎥⎦
.
Given that Wt ∼𝒩(0, t) we know
Var(Wt) = t.
For the case Cov
(
Wt, ∫t
0 Ws ds
)
we can write
Cov
(
Wt, ∫
t
0
Ws ds
)
= 𝔼
(
Wt ∫
t
0
Ws ds
)
−𝔼(Wt)𝔼
(
∫
t
0
Ws ds
)
= 𝔼
(
Wt ∫
t
0
Ws ds
)
= 𝔼
(
∫
t
0
WtWs ds
)
= ∫
t
0
𝔼(WtWs) ds
= ∫
t
0
𝔼[Ws(Wt −Ws) + W2
s
] ds.
From the independent increment property of a Wiener process we have
Cov
(
Wt, ∫
t
0
Ws ds
)
= ∫
t
0
𝔼(Ws)𝔼(Wt −Ws) ds + ∫
t
0
𝔼
(
W2
s
)
ds
= ∫
t
0
𝔼(W2
s
) ds
= ∫
t
0
s ds
= t2
2 .
Finally,
Var
(
∫
t
0
Ws ds
)
= 𝔼
[(
∫
t
0
Ws ds
)2]
−
[
𝔼
(
∫
t
0
Ws ds
)]2
= 𝔼
[(
∫
t
0
Ws ds
) (
∫
t
0
Wu du
)]
−
[
∫
t
0
𝔼(Ws)
]2

66
2.2.1
Basic Properties
= 𝔼
[
∫
s=t
s=0 ∫
u=t
u=0
WsWu duds
]
= ∫
s=t
s=0 ∫
u=t
u=0
𝔼(WsWu) duds.
From Problem 2.2.1.4 (page 57) we have
𝔼(WsWu
) = min{s, u}
and hence
Var
(
∫
t
0
Ws ds
)
= ∫
s=t
s=0 ∫
u=t
u=0
min{s, u} duds
= ∫
s=t
s=0 ∫
u=s
u=0
u duds + ∫
s=t
s=0 ∫
u=t
u=s
s duds
= ∫
t
0
1
2s2 ds + ∫
t
0
s(t −s) ds
= 1
3t3.
Therefore,
𝚺=
⎡
⎢
⎢⎣
t
1
2t2
1
2t2
1
3t3
⎤
⎥
⎥⎦
with correlation coefficient
𝜌=
Cov
(
Wt, ∫
t
0
Ws ds
)
√
Var(Wt)Var
(
∫
t
0
Ws ds
) =
t2∕2
t2∕
√
3
=
√
3
2 .
◽
13. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that the covariance of ∫s
0 Wu du and ∫t
0 W𝑣d𝑣, s, t > 0 is
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
= 1
3min{s3, t3} + 1
2|t −s|min{s2, t2}
with correlation coefficient
√
min{s3, t3}
max{s3, t3} + 3
2|t −s|
√
min{s, t}
max{s3, t3}.

2.2.1
Basic Properties
67
Solution: By definition
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
= 𝔼
[(
∫
s
0
Wu du
) (
∫
t
0
W𝑣d𝑣
)]
−𝔼
[
∫
s
0
Wu du
]
𝔼
[
∫
t
0
W𝑣d𝑣
]
= ∫
s
0 ∫
t
0
𝔼(WuW𝑣) dud𝑣−
[
∫
s
0
𝔼(Wu) du
] [
∫
t
0
𝔼(W𝑣) d𝑣
]
= ∫
s
0 ∫
t
0
min{u, 𝑣} dud𝑣
since 𝔼(WuW𝑣) = min{u, 𝑣} and 𝔼(Wu) = 𝔼(W𝑣) = 0.
For the case s ≤t, using the results of Problem 2.2.1.12 (page 64),
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
= ∫
s
0 ∫
t
0
min{u, 𝑣} dud𝑣
= ∫
s
0 ∫
s
0
min{u, 𝑣} dud𝑣+ ∫
s
0 ∫
t
s
min{u, 𝑣} dud𝑣
= 1
3s3 + ∫
s
0 ∫
t
s
𝑣dud𝑣
= 1
3s3 + ∫
s
0
𝑣(t −s) d𝑣
= 1
3s3 + 1
2(t −s)s2.
Following the same steps as described above, for the case s > t we can also show
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
= 1
3t3 + 1
2(s −t)t2.
Thus,
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
= 1
3min{s3, t3} + 1
2|t −s|min{s2, t2}.
From the definition of the correlation coefficient between ∫s
0 Wu du and ∫t
0 W𝑣d𝑣,
𝜌=
Cov
(
∫
s
0
Wu du, ∫
t
0
W𝑣d𝑣
)
√
Var
(
∫
s
0
Wu du
)
Var
(
∫
t
0
W𝑣d𝑣
)

68
2.2.2
Markov Property
=
1
3min{s3, t3} + 1
2|t −s|min{s2, t2}
1
3
√
s3t3
= min{s3, t3}
√
s3t3
+ 3
2
|t −s|min{s2, t2}
√
s3t3
.
For s ≤t we have
𝜌=
√
s3
t3 + 3
2(t −s)
√
s
t3
and for s > t
𝜌=
√
t3
s3 + 3
2(s −t)
√
t
s3 .
Therefore, we can deduce
𝜌=
√
min{s3, t3}
max{s3, t3} + 3
2|t −s|
√
min{s, t}
max{s3, t3}.
◽
2.2.2
Markov Property
1. The Markov Property of a Standard Wiener Process. Let (Ω, ℱ, ℙ) be a probability space
and let {Wt ∶t ≥0} be a standard Wiener process with respect to the filtration ℱt, t ≥0.
Show that if f is a continuous function then there exists another continuous function g
such that
𝔼[f(Wt)|ℱu] = g(Wu)
for 0 ≤u ≤t.
Solution: For 0 ≤u ≤t we can write
𝔼[f(Wt)|ℱu
] = 𝔼[f(Wt −Wu + Wu)|ℱu
] .
Since Wt −Wu ⟂⟂ℱu and Wu is ℱu measurable, by setting Wu = x where x is a constant
value
𝔼[f(Wt −Wu + Wu)|ℱu] = 𝔼[f(Wt −Wu + x)].
Because Wt −Wu ∼𝒩(0, t −u) we can write 𝔼[f(Wt −Wu + x)] as
𝔼[f(Wt −Wu + x)] =
1
√
2𝜋(t −u) ∫
∞
−∞
f(𝑤+ x)e−
𝑤2
2(t−u) d𝑤.

2.2.2
Markov Property
69
By setting 𝜏= t −u and y = 𝑤+ x, we can rewrite 𝔼[f(Wt −Wu + x)] = 𝔼[f(Wt −Wu +
Wu)] as
𝔼[f(Wt −Wu + Wu)] =
1
√
2𝜋𝜏∫
∞
−∞
f(y)e−(y−x)2
2𝜏dy
= ∫
∞
−∞
f(y)p(𝜏, Wu, y)dy
where the transition density
p(𝜏, Wu, y) =
1
√
2𝜋𝜏
e−(y−Wu)2
2𝜏
is the density of Y ∼𝒩(Wu, 𝜏). Since the only information from the filtration ℱu is Wu,
therefore
𝔼[f(Wt)|ℱu] = g(Wu)
where
g(Wu) = ∫
∞
−∞
f(y)p(𝜏, Wu, y)dy.
◽
2. The Markov Property of a Wiener Process. Let (Ω, ℱ, ℙ) be a probability space and let
{Wt ∶t ≥0} be a standard Wiener process with respect to the filtration ℱt, t ≥0. By
considering the Wiener process
̂Wt = a + bt + cWt,
a, b ∈ℝ,
c > 0
show that if f is a continuous function then there exists another continuous function g such
that
𝔼[f( ̂Wt)|ℱu] = g( ̂Wu)
for 0 ≤u ≤t.
Solution: For 0 ≤u ≤t we can write
𝔼
[
f( ̂Wt)|||ℱu
]
= 𝔼
[
f( ̂Wt −̂Wu + ̂Wu)|||ℱu
]
.
Since Wt −Wu ⟂⟂ℱu we can deduce that ̂Wt −̂Wu ⟂⟂ℱu. In addition, because Wu is ℱu
measurable, hence ̂Wu is also ℱu measurable. By setting ̂Wu = x where x is a constant
value,
𝔼
[
f( ̂Wt −̂Wu + ̂Wu)|||ℱu
]
= 𝔼
[
f( ̂Wt −̂Wu + x)
]
.

70
2.2.2
Markov Property
Taking note that ̂Wt −̂Wu ∼𝒩(b(t −u), c2(t −u)), we can write 𝔼
[
f( ̂Wt −̂Wu + x)
]
as
𝔼
[
f( ̂Wt −̂Wu + x)
]
=
1
c
√
2𝜋(t −u) ∫
∞
−∞
f(𝑤+ x)e
−1
2
[
(𝑤−b(t−u))2
c2(t−u)
]
d𝑤.
By setting 𝜏= t −u and y = 𝑤+ x, we can rewrite 𝔼
[
f( ̂Wt −̂Wu + x)
]
= 𝔼
[
f( ̂Wt −̂Wu +
̂Wu)
]
as
𝔼
[
f( ̂Wt −̂Wu + ̂Wu)
]
=
1
c
√
2𝜋𝜏∫
∞
−∞
f(y)e
−1
2
[
(y−b𝜏−̂Wu)2
c2𝜏
]
dy
= ∫
∞
−∞
f(y)p(𝜏, ̂Wu, y) dy
where the transition density
p(𝜏, Wu, y) =
1
√
2𝜋𝜏
e
−1
2
[
(y−b𝜏−̂Wu)2
c2𝜏
]
is the density of Y ∼𝒩(b𝜏+ ̂Wu, c2𝜏). Since the only information from the filtration ℱu
is Wu, therefore
𝔼
[
f( ̂Wt)|||ℱu
]
= g( ̂Wu)
where
g( ̂Wu) = ∫
∞
−∞
f(y)p(𝜏, ̂Wu, y) dy.
◽
3. The Markov Property of a Geometric Brownian Motion. Let (Ω, ℱ, ℙ) be a probability
space and let {Wt ∶t ≥0} be a standard Wiener process with respect to the filtration ℱt,
t ≥0. By considering the geometric Wiener process
St = S0e
(
𝜇−1
2 𝜎2)
t+𝜎Wt,
𝜇∈ℝ,
𝜎> 0
where S0 > 0 show that if f is a continuous function then there exists another continuous
function g such that
𝔼[f(St)|ℱu] = g(Su)
for 0 ≤u ≤t.
Solution: For 0 ≤u ≤t we can write
𝔼[f(St)|ℱu] = 𝔼
[
f
( St
Su
⋅Su
)|||||
ℱu
]

2.2.3
Martingale Property
71
where
log
( St
Su
)
∼𝒩
((
𝜇−1
2𝜎2)
(t −u), 𝜎2(t −u)
)
.
Since St∕Su ⟂⟂ℱu and Su is ℱu measurable, by setting Su = x where x is a constant value
𝔼
[
f
( St
Su
⋅Su
)|||||
ℱu
]
= 𝔼
[
f
( St
Su
⋅x
)]
.
By setting 𝜈= 𝜇−1
2𝜎2 so that St∕Su ∼log-𝒩(𝜈(t −u), 𝜎2(t −u)), we can write
𝔼
[
f
( St
Su
⋅x
)]
as
𝔼
[
f
( St
Su
⋅x
)]
=
1
𝜎𝑤
√
2𝜋(t −u) ∫
∞
−∞
f(𝑤⋅x)
e
−1
2
[
(log 𝑤−𝜈(t−u))2
𝜎2(t−u)
]
d𝑤.
By setting 𝜏= t −u and y = 𝑤⋅x, we can rewrite 𝔼
[
f
( St
Su
⋅x
)]
= 𝔼
[
f
( St
Su
⋅Su
)]
as
𝔼
[
f
( St
Su
⋅Su
)]
=
1
𝜎y
√
2𝜋𝜏∫
∞
−∞
f(y)e
−1
2
[
(log ( y
x)−𝜈𝜏)
2
𝜎2𝜏
]
dy
= ∫
∞
−∞
f(y)p(𝜏, Wu, y) dy
where the transition density
p(𝜏, Wu, y) =
1
𝜎y
√
2𝜋𝜏
e
−1
2
[
(log y−log Su−𝜈𝜏)2
𝜎2𝜏
]
is the density of Y ∼log-𝒩(log Su + 𝜈𝜏, 𝜎2𝜏). Since the only information from the filtra-
tion ℱu is Su, therefore
𝔼[f(St)|ℱu] = g(Su)
such that
g(Su) = ∫
∞
−∞
f(y)p(𝜏, Wu, y) dy.
◽
2.2.3
Martingale Property
1. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that Wt is a martingale.
Solution: Given Wt ∼𝒩(0, t).

72
2.2.3
Martingale Property
(a) For s ≤t and since Wt −Ws ⟂⟂ℱs, we have
𝔼(Wt|ℱs) = 𝔼(Wt −Ws + Ws|ℱs) = 𝔼(Wt −Ws|ℱs) + 𝔼(Ws|ℱs) = Ws.
(b) Since Wt ∼𝒩(0, t), |Wt| follows a folded normal distribution such that |Wt| ∼
𝒩f(0, t). From Problem 1.2.2.11 (page 22), we can deduce 𝔼(|Wt|) =
√
2t∕𝜋< ∞.
In contrast, we can also utilise Hölder’s inequality (see Problem 1.2.3.2, page 41) to
deduce that 𝔼(|Wt|) ≤
√
𝔼(W2
t ) =
√
t < ∞.
(c) Wt is clearly ℱt-adapted.
From the results of (a)–(c) we have shown that Wt is a martingale.
◽
2. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that Xt = W2
t −t is a martingale.
Solution: Given Wt ∼𝒩(0, t)
(a) For s ≤t and since Wt −Ws ⟂⟂ℱs, we have
𝔼(W2
t −t|ℱs
) = 𝔼
[(Wt −Ws + Ws
)2||| ℱs
]
−t
= 𝔼
[(Wt −Ws
)2||| ℱs
]
+ 2𝔼
[
Ws
(Wt −Ws
)||| ℱs
]
+ 𝔼
(
W2
s ||| ℱs
)
−t
= t −s + 0 + W2
s −t
= W2
s −s.
(b) Since |Xt| = |W2
t −t| ≤W2
t + t we can therefore write
𝔼
(|||W2
t −t|||
)
≤𝔼(W2
t + t) = 𝔼(W2
t
) + t = 2t < ∞.
(c) Since Xt = W2
t −t is a function of Wt, hence it is ℱt-adapted.
From the results of (a)–(c) we have shown that Xt = W2
t −t is a martingale.
◽
3. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
For 𝜆∈ℝshow that Xt = e𝜆Wt−1
2 𝜆2t is a martingale.
Solution: Given Wt ∼𝒩(0, t) we can write log Xt = 𝜆Wt −1
2𝜆2t ∼𝒩
(
−1
2𝜆2t, 𝜆2t
)
and
hence eXt ∼log-𝒩
(
−1
2𝜆2t, 𝜆2t
)
.
(a) For s ≤t and since Wt −Ws ⟂⟂ℱs, we have
𝔼
(
e𝜆Wt−1
2 𝜆2t||||
ℱs
)
= e−1
2 𝜆2t𝔼
(
e𝜆Wt||| ℱs
)
= e−1
2 𝜆2t𝔼
[
e𝜆(Wt−Ws)+𝜆Ws||| ℱs
]

2.2.3
Martingale Property
73
= e−1
2 𝜆2t𝔼
[
e𝜆(Wt−Ws)||| ℱs
]
𝔼
[
e𝜆Ws||| ℱs
]
= e−1
2 𝜆2t ⋅e
1
2 𝜆2(t−s) ⋅e𝜆Ws
= e𝜆Ws−1
2 𝜆2s.
(b) By setting |Xt| = ||||
e𝜆Wt−1
2 𝜆2t||||
= e𝜆Wt−1
2 𝜆2t,
𝔼(|Xt|) = 𝔼
(
e𝜆Wt−1
2 𝜆2t)
= e−1
2 𝜆2t𝔼(e𝜆Wt) = e−1
2 𝜆2t ⋅e
1
2 𝜆2t = 1 < ∞.
(c) Since Xt is a function of Wt, hence it is ℱt-adapted.
From the results of (a)–(c) we have shown that Xt = e𝜆Wt−1
2 𝜆2t is a martingale.
◽
4. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that Xt = W3
t −3tWt is a martingale.
Solution:
(a) For s ≤t and since Wt −Ws ⟂⟂ℱs, we have
𝔼(Xt|| ℱs
) = 𝔼
(
W3
t −3tWt||| ℱs
)
= 𝔼
[
Wt
((Wt −Ws + Ws
)2 −3t
)||||
ℱs
]
= 𝔼
[
Wt
(Wt −Ws
)2||| ℱs
]
+ 2Ws𝔼
[
Wt
(Wt −Ws
)||| ℱs
]
+W2
s 𝔼(Wt|| ℱs
) −3t𝔼(Wt|| ℱs
)
= 𝔼
[(Wt −Ws + Ws
) (Wt −Ws
)2||| ℱs
]
+2Ws𝔼
[(Wt −Ws + Ws
) (Wt −Ws
)||| ℱs
]
+ W3
s −3tWs
= 𝔼
[(Wt −Ws
)3||| ℱs
]
+ 𝔼
[
Ws
(Wt −Ws
)2||| ℱs
]
+2Ws𝔼
[(Wt −Ws
)2||| ℱs
]
+ 2Ws𝔼
[
Ws
(Wt −Ws
)||| ℱs
]
+ W3
s −3tWs
= Ws(t −s) + 2Ws(t −s) + W3
s −3tWs
= W3
s −3sWs
where, from Problem 2.2.1.5 (page 58), we can deduce that the moment generating
function of Wt is MWt(𝜃) = e
1
2 𝜃2t2 and 𝔼(W3
t ) = d3
d𝜃3 MWt(𝜃)||||𝜃=0
= 0.

74
2.2.3
Martingale Property
(b) Since |Xt| = |W3
t −3tWt| ≤|W3
t | + 3t|Wt| and from Hölder’s inequality (see Prob-
lem 1.2.3.2, page 41),
𝔼(|Xt|) ≤
√
𝔼
(
||W2
t ||
2)
𝔼
(
||Wt||
2)
+ 3t𝔼(|Wt|)
=
√
𝔼
(
W4
t
)
𝔼
(
W2
t
)
+ 3t𝔼
(
|Wt|
)
.
Since W2
t ∕t ∼𝜒(1) we have 𝔼(W4
t ) = 3t and utilising Hölder’s inequality again, we
have 𝔼(|Wt|) ≤
√
𝔼(W2
t
) =
√
t < ∞. Therefore, 𝔼(|Xt|) ≤t
√
2 + 3t
√
t < ∞.
(c) Since Xt is a function of Wt, hence it is ℱt-adapted.
From the results of (a)–(c) we have shown that Xt = W3
t −3tWt is a martingale.
◽
5. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
For 𝜆∈ℝshow that the following hyperbolic processes:
Xt = e−1
2 𝜆2t cosh(𝜆Wt)
Yt = e−1
2 𝜆2t sinh(𝜆Wt)
are martingales.
Solution: By definition we can write
Xt = e−1
2 𝜆2t cosh(𝜆Wt) = 1
2
(
e𝜆Wt−1
2 𝜆2t + e−𝜆Wt−1
2 𝜆2t)
.
Since for 𝜆∈ℝ, X(1)
t
= 1
2e𝜆Wt−1
2 𝜆2t and X(2)
t
= 1
2e−𝜆Wt−1
2 𝜆2t are martingales we have the
following properties:
(a) For s ≤t, 𝔼
(
X(1)
t
+ X(2)
t
||| ℱs
)
= X(1)
s
+ X(2)
s .
(b) Because 𝔼
(|||X(1)
t
|||
)
< ∞and 𝔼
(|||X(2)
t
|||
)
< ∞, so 𝔼
(|||X(1)
t
+ X(2)
t
|||
)
< ∞.
(c) Since Xt is a function of Wt, hence it is ℱt-adapted.
From the results of (a)–(c) we have shown that Xt = e−1
2 𝜆2t cosh(𝜆Wt) is a martingale.
For Yt = e−1
2 𝜆2t sinh(𝜆Wt) we note that
Yt = e−1
2 𝜆2t sinh(𝜆Wt) = 1
2
(
e𝜆Wt−1
2 𝜆2t −e−𝜆Wt−1
2 𝜆2t)
and similar steps can be applied to show that Yt is also a martingale.
◽
6. Let (Ω, ℱ, ℙ) be a probability space. Show that the sample paths of a standard Wiener
process {Wt ∶t ≥0} are continuous but not differentiable.

2.2.3
Martingale Property
75
Solution: The path Wt ∼𝒩(0, t) is continuous in probability if and only if, for every
𝛿> 0 and t ≥0,
lim
Δt→0ℙ(|Wt+Δt −Wt| ≥𝛿) = 0.
Given that Wt is a martingale and from Chebyshev’s inequality (see Problem 1.2.2.19,
page 40),
ℙ(|Wt+Δt −Wt| ≥𝛿) = ℙ(|Wt+Δt −𝔼(Wt+Δt|ℱt)| ≥𝛿)
≤Var(Wt+Δt|ℱt)
𝛿2
= Var(Wt+Δt −Wt + Wt|ℱt)
𝛿2
= Var(Wt+Δt −Wt)
𝛿2
+ Var(Wt|ℱt)
𝛿2
= Δt
𝛿2
since Wt+Δt −Wt ⟂⟂ℱt and Wt is ℱt measurable and hence Var(Wt|ℱt) = 0.
Taking the limit Δt →0, we have
ℙ(|Wt+Δt −Wt| ≥𝛿) →0
and therefore we conclude that the sample path is continuous. By setting
ΔWt = Wt+Δt −Wt = 𝜙
√
Δt
where 𝜙∼𝒩(0, 1) and taking limit Δt →0, we have
lim
Δt→0
ΔWt
Δt
= lim
Δt→0
𝜙
√
Δt
= ±∞
depending on the sign of 𝜙. Therefore, Wt is not differentiable.
◽
7. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process
with respect to the filtration ℱt, t ≥0.
Show that if 𝜑is a convex function and 𝔼[|𝜑(Wt)|] < ∞for all t ≥0, then 𝜑(Wt) is a
submartingale.
Finally, deduce that |Wt|, W2
t , e𝛼Wt, 𝛼∈ℝand W+
t = max{0, Wt} are non-negative sub-
martingales.
Solution: Let s < t and because Wt is a martingale we therefore have 𝔼(Wt|ℱs) = Ws
and hence 𝜑[𝔼(Wt|ℱs)] = 𝜑(Ws). From the conditional Jensen’s inequality (see Prob-
lem 1.2.3.14, page 48),
𝔼[𝜑(Wt)|ℱs] ≥𝜑[𝔼(Wt|ℱs)] = 𝜑(Ws).
In addition, since 𝔼[|𝜑(Wt)|] < ∞and 𝜑(Wt) is clearly ℱt-adapted we can conclude that
𝜑(Wt) is a submartingale.

76
2.2.4
First Passage Time
By setting 𝜑(x) = |x|, x ∈ℝand for 𝜃∈(0, 1) and x1, x2 ∈ℝsuch that x1 ≠x2,
we have
|𝜃x1 + (1 −𝜃)x2| ≤𝜃|x1| + (1 −𝜃)|x2|.
Therefore, |x| is a non-negative convex function. Because 𝔼(|Wt|) < ∞for all t ≥0, so
|Wt| is a non-negative submartingale.
On the contrary, by setting 𝜑(x) = x2, x ∈ℝand since 𝜑′′(x) = 2 ≥0 for all x , so x2 is a
non-negative convex function. Since 𝔼(|W2
t |) < ∞for all t ≥0, so W2
t is a non-negative
submartingale.
Using the same steps we define 𝜑(x) = e𝛼x where 𝛼, x ∈ℝand since 𝜑′′(x) = 𝛼2e𝛼x ≥0
for all x, so e𝛼x is a non-negative convex function. Since 𝔼(||e𝛼Wt||
) < ∞for all t ≥0 we
can conclude that e𝛼Wt is a non-negative submartingale.
Finally, by setting 𝜑(x) = max{0, x}, x ∈ℝand for 𝜃∈(0, 1) and x1, x2 ∈ℝsuch that
x1 ≠x2, we have
max{0, 𝜃x1 + (1 −𝜃)x2} ≤max{0, 𝜃x1} + max{0, (1 −𝜃)x2}
= 𝜃1max{0, x1} + (1 −𝜃1)max{0, x2}.
Therefore, x+ = max{0, x} is a non-negative convex function and since 𝔼(|W+
t |) < ∞for
all t ≥0, so W+
t = max{0, Wt} is a non-negative submartingale.
◽
2.2.4
First Passage Time
1. Doob’s Maximal Inequality. Let (Ω, ℱ, ℙ) be a probability space and let {Xt ∶0 ≤t ≤T}
be a continuous non-negative submartingale with respect to the filtration ℱt, 0 ≤t ≤T.
Given 𝜆> 0 and 𝜏= min{t ∶Xt ≥𝜆}, show that
𝔼(X0
) ≤𝔼
(
Xmin{𝜏,T}
)
≤𝔼(XT
) .
By writing
Xmin{𝜏,T} = X𝜏1I{𝜏≤T} + XT1I{𝜏>T}
show that
ℙ
(
sup
0≤t≤T
Xt ≥𝜆
)
≤
𝔼(XT
)
𝜆
.
Deduce that if {Yt ∶0 ≤t ≤T} is a continuous non-negative supermartingale with respect
to the filtration ℱt, 0 ≤t ≤T then
ℙ
(
sup
0≤t≤T
Yt ≥𝜆
)
≤
𝔼(Y0
)
𝜆
.
Solution: For 𝜆> 0 we let 𝜏= min{t ∶Xt ≥𝜆} so that 0 ≤min{𝜏, T} ≤T. Because Xt
is a non-negative submartingale we have
𝔼
(
XT|| ℱmin{𝜏,T}
)
≥Xmin{𝜏,T}

2.2.4
First Passage Time
77
or
𝔼
(
Xmin{𝜏,T}
)
≤𝔼
[
𝔼
(
XT|| ℱmin{𝜏,T}
)]
= 𝔼(XT
) .
Using the same steps we can deduce
𝔼(X0
) ≤𝔼
(
Xmin{𝜏,T}
)
≤𝔼(XT
) .
By definition
Xmin{𝜏,T} = X𝜏1I{𝜏≤T} + XT1I{𝜏>T}
where
1I{𝜏≤T} =
{
1
𝜏≤T
0
𝜏> T ,
1I{𝜏>T} =
{
1
𝜏> T
0
𝜏≤T.
Therefore,
𝔼
(
Xmin{𝜏,T}
)
= 𝔼
(
X𝜏1I{𝜏≤T}
)
+ 𝔼
(
XT1I{𝜏>T}
)
≥𝜆ℙ(𝜏≤T) + 𝔼
(
XT1I{𝜏>T}
)
.
Taking note that 𝔼
(
Xmin{𝜏,T}
)
≤𝔼(XT
), we can write
𝜆ℙ(𝜏≤T) ≤𝔼
(
Xmin{𝜏,T}
)
−𝔼
(
XT1I{𝜏>T}
)
≤𝔼(XT
) −𝔼
(
XT1I{𝜏>T}
)
≤𝔼(XT
) .
Since
{𝜏≤T} ⇐⇒
{
sup
0≤t≤T
Xt ≥𝜆
}
therefore
ℙ
(
sup
0≤t≤T
Xt ≥𝜆
)
≤𝔼(XT)
𝜆
.
If {Yt}0≤t≤T is a supermartingale then
𝔼(YT
) ≤𝔼
(
Ymin{𝜏,T}
)
≤𝔼(Y0
)
where in this case 𝜏= min{t ∶Yt ≥𝜆} and by analogy with the above steps, we have
ℙ
(
sup
0≤t≤T
Yt ≥𝜆
)
≤
𝔼(Y0
)
𝜆
.
◽
2. Doob’s Lp Maximal Inequality. Let (Ω, ℱ, ℙ) be a probability space and let Z be a contin-
uous non-negative random variable where for m > 0, 𝔼(Zm) < ∞. Show that
𝔼(Zm) = m ∫
∞
0
𝛼m−1ℙ(Z > 𝛼) d𝛼.

78
2.2.4
First Passage Time
Let {Xt ∶0 ≤t ≤T} be a continuous non-negative submartingale with respect to the fil-
tration ℱt, 0 ≤t ≤T. Using the above result, for p > 1 and if 𝔼(sup0≤t≤TXp
t
) < ∞show
that
𝔼
(
sup
0≤t≤T
Xp
t
)
≤
(
p
p −1
)p
𝔼(Xp
T
) .
Deduce that if {Yt}0≤t≤T is a continuous non-negative supermartingale with respect to the
filtration ℱt, 0 ≤t ≤T then
𝔼
(
sup
0≤t≤T
Yp
t
)
≤
(
p
p −1
)p
𝔼(Yp
0
) .
Solution: By defining the indicator function
1I{Z>𝛼} =
{
1
Z > 𝛼
0
Z ≤𝛼
we can prove
∫
∞
0
m𝛼m−1ℙ(Z > 𝛼) d𝛼= ∫
∞
0
m𝛼m−1𝔼(1I{Z>𝛼}) d𝛼
= 𝔼
[
∫
∞
0
m𝛼m−11I{Z>𝛼} d𝛼
]
= 𝔼
[
∫
Z
0
m𝛼m−1 d𝛼
]
= 𝔼(Zm).
Let 𝜏= min{t ∶Xt ≥𝜆}, 𝜆> 0 so that 0 ≤min{𝜏, T} ≤T. Therefore, we can deduce
{𝜏≤T} ⇐⇒
{
sup
0≤t≤T
Xt ≥𝜆
}
and from Problem 2.2.4.1 (page 76) we can show that
𝔼
(
sup
0≤t≤T
Xp
t
)
= ∫
∞
0
p𝜆p−1ℙ
(
sup
0≤t≤T
Xt > 𝜆
)
d𝜆
≤∫
∞
0
p𝜆p−2𝔼
(
XT1I{sup0≤t≤TXt≥𝜆}
)
d𝜆
= p 𝔼
(
XT ∫
∞
0
𝜆p−21I{sup0≤t≤TXt≥𝜆} d𝜆
)
= p 𝔼
(
XT ∫
sup0≤t≤TXt
0
𝜆p−2 d𝜆
)
=
p
p −1𝔼
(
XT ⋅
{
sup
0≤t≤T
Xp−1
t
})
.

2.2.4
First Passage Time
79
Using Hölder’s inequality and taking note that 𝔼(sup0≤t≤TXp
t
) < ∞, we can write
𝔼
(
sup
0≤t≤T
Xp
t
)
≤
p
p −1𝔼(Xp
T
) 1
p 𝔼
(
sup
0≤t≤T
Xp−1
t
) p−1
p
or
𝔼
(
sup
0≤t≤T
Xp
t
) 1
p
≤
p
p −1𝔼(Xp
T
) 1
p
and hence
𝔼
(
sup
0≤t≤T
Xp
t
)
≤
(
p
p −1
)p
𝔼(Xp
T
) .
From Problem 2.2.4.1 (page 76), if {Yt}0≤t≤T is a supermartingale then
𝔼(YT
) ≤𝔼
(
Ymin{𝜏,T}
)
≤𝔼(Y0
)
where in this case 𝜏= min{t ∶Yt ≥𝜆}. Following the same steps as discussed before, we
have
𝔼
(
sup
0≤t≤T
Yp
t
)
≤
(
p
p −1
)p
𝔼(Yp
0
) .
◽
3. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Using Doob’s maximal inequality show that for every T > 0 and 𝜆, 𝜃> 0, we can express
ℙ
(
sup
0≤t≤T
e𝜃Wt ≥e𝜃𝜆
)
≤e
1
2 𝜃2T−𝜃𝜆
and hence prove that
ℙ
(
sup
0≤t≤T
Wt ≥𝜆
)
≤e−𝜆2
2T .
Finally, deduce that
ℙ
(
sup
0≤t≤T
||Wt|| ≥𝜆
)
≤2e−𝜆2
2T .
Solution: Since 𝜆> 0 and for 𝜃> 0 we can write
ℙ
(
sup
0≤t≤T
Wt ≥𝜆
)
= ℙ
(
sup
0≤t≤T
𝜃Wt ≥𝜃𝜆
)
= ℙ
(
sup
0≤t≤T
e𝜃Wt ≥e𝜃𝜆
)
.
From Problem 2.2.3.7 (page 75) we have shown that e𝜃Wt is a submartingale and it is also
non-negative. Thus, from Doob’s maximal inequality theorem,
ℙ
(
sup
0≤t≤T
e𝜃Wt ≥e𝜃𝜆
)
≤
𝔼(e𝜃WT)
e𝜃𝜆
= e
1
2 𝜃2T−𝜃𝜆.

80
2.2.4
First Passage Time
By minimising e
1
2 𝜃2T−𝜃𝜆we obtain 𝜃= 𝜆∕T and substituting it into the inequality
we have
ℙ
(
sup
0≤t≤T
Wt ≥𝜆
)
≤e−𝜆2
2T .
Given that Wt ∼𝒩(0, t) we can deduce that ℙ(|Wt| ≥𝜆) = ℙ(Wt ≥𝜆) + ℙ(Wt ≤−𝜆) =
2ℙ(Wt ≥𝜆) and hence ℙ
(
sup
0≤t≤T
||Wt|| ≥𝜆
)
≤2e−𝜆2
2T .
◽
4. Let (Ω, ℱ, ℙ) be a probability space and let Wt be the standard Wiener process with respect
to the filtration ℱt, t ≥0.
By writing Xt = x0 + Wt where x0 ∈ℝshow that {Xt ∶t ≥0} is a continuous martingale.
Let Ta = inf{t ≥0 ∶Xt = a} and Tb = inf{t ≥0 ∶Xt = b} where a < b. Show, using the
optional stopping theorem, that
ℙ(Ta < Tb) = b −x0
b −a .
Solution: Let Xt = x0 + Wt where Wt is a standard Wiener process. Because Wt is a mar-
tingale (see Problem 2.2.3.1, page 71) and because x0 is a constant value, following the
same steps we can easily prove that Xt is also a martingale.
By writing T = inf{t ≥0 ∶Xt ∉(a, b)} as the first exit time from the interval (a, b), then
from the optional stopping theorem
𝔼(XT|X0 = x0) = 𝔼(x0 + WT|X0 = x0
) = x0
and by definition
𝔼(XT|X0 = x0
) = aℙ(XT = a|X0 = x0) + bℙ(XT = b|X0 = x0)
x0 = aℙ(Ta < Tb
) + bℙ(Ta ≥Tb
)
x0 = aℙ(Ta < Tb
) + b [1 −ℙ(Ta < Tb
)] .
Therefore,
ℙ(Ta < Tb) = b −x0
b −a .
◽
5. Let (Ω, ℱ, ℙ) be a probability space and let Wt be the standard Wiener process with respect
to the filtration ℱt, t ≥0.
By writing Xt = x + Wt where x ∈ℝshow that Xt and (Xt −x)2 −t are continuous mar-
tingales.
Let T = inf{t ≥0 ∶Xt ∉(a, b)} be the first exit time from the interval (a, b), a < x < b
and assuming T < ∞almost surely show, using the optional stopping theorem, that
ℙ(XT = a|X0 = x) = b −x
b −a
and
ℙ(XT = b|X0 = x) = x −a
b −a
with
𝔼(T) = (b −x)(x −a).

2.2.4
First Passage Time
81
Solution: Let Xt = x + Wt where Wt is a standard Wiener process. Because Wt and
W2
t −t are martingales (see Problems 2.2.3.1, page 71 and 2.2.3.2, page 72) and
because x0 is a constant value, following the same steps we can easily prove that Xt and
(Xt −x)2 −t are also martingales.
For x ∈(a, b) and because Xt is a martingale, from the optional stopping theorem
𝔼(XT|X0 = x) = 𝔼(x + WT|X0 = x) = x
and by definition
𝔼(XT|X0 = x) = aℙ(XT = a|X0 = x) + bℙ(XT = b|X0 = x)
x = aℙ(XT = a|X0 = x) + b [1 −ℙ(XT = a|X0 = x)] .
Therefore,
ℙ(XT = a|X0 = x) = b −x
b −a
and
ℙ(XT = b|X0 = x) = 1 −ℙ(XT = a|X0 = x) = x −a
b −a.
Since Yt = (Xt −x)2 −t is a martingale, from the optional stopping theorem we have
𝔼(YT|X0 = x) = 𝔼(Y0|X0 = x) = 0
or
𝔼
[(XT −x)2 −T||| X0 = x
]
= 𝔼
[(XT −x)2||| X0 = x
]
−𝔼(T|X0 = x) = 0.
Therefore,
𝔼(T|X0 = x) = 𝔼
[(XT −x)2||| X0 = x
]
= (a −x)2ℙ(XT = a|X0 = x) + (b −x)2ℙ(XT = b|X0 = x)
= (a −x)2 (b −x
b −a
)
+ (b −x)2 (x −a
b −a
)
= (b −x)(x −a).
◽
6. Let (Ω, ℱ, ℙ) be a probability space and let {Xt ∶t ≥0} be a continuous martingale on ℝ.
Show that if T = inf{t ≥0 ∶Xt ∉(a, b)} is the first exit time from the interval (a, b) and
assuming T < ∞almost surely then for 𝜃> 0,
Zt = (e𝜃b −e−𝜃a)e𝜃Xt−1
2 𝜃2t + (e−𝜃a −e𝜃b)e−𝜃Xt−1
2 𝜃2t
is a martingale. Using the optional stopping theorem and if a < x < b, show that
𝔼
(
e−1
2 𝜃2T||||
X0 = x
)
=
cosh
[
𝜃
(
x −1
2(a + b)
)]
cosh
[
1
2𝜃(b −a)
]
,
𝜃> 0.

82
2.2.4
First Passage Time
Solution: Given that Xt is a martingale and both e𝜃b −e−𝜃a and e−𝜃a −e𝜃b are indepen-
dent of Xt, then using the results of Problem 2.2.3.3 (page 72) we can easily show that
{Zt ∶t ≥0} is a martingale.
From the optional stopping theorem
𝔼(ZT|X0 = x) = 𝔼(Z0|X0 = x)
and using the identity sinh(x) + sinh(y) = 2 sinh
(x + y
2
)
cosh
(x −y
2
)
we have
𝔼(Z0|X0 = x) = e𝜃(b−x) −e−𝜃(b−x) + e−𝜃(a+x) −e𝜃(a−x)
= 2 {sinh [𝜃(b −x)] + sinh [−𝜃(a −x)]}
= 4 sinh
[1
2𝜃(b −a)
]
cosh
[
𝜃
(
x −1
2(a + b)
)]
and
𝔼(ZT|X0 = x) = 𝔼(ZT|XT = a, X0 = x)ℙ(XT = a|X0 = x)
+𝔼(ZT|XT = b, X0 = x)ℙ(XT = b|X0 = x)
= 𝔼(ZT|XT = a, X0 = x)ℙ(XT = a|X0 = x)
+𝔼(ZT|XT = b, X0 = x)(1 −ℙ(XT = a|X0 = x))
= [𝔼(ZT|XT = a, X0 = x) −𝔼(ZT|XT = b, X0 = x)]ℙ(XT = a|X0 = x)
+𝔼(ZT|XT = b, X0 = x)
= [e𝜃(b−a) −e−𝜃(b−a)] 𝔼
(
e−1
2 𝜃2T||||
X0 = x
)
= 2 sinh[𝜃(b −a)]𝔼
(
e−1
2 𝜃2T||||
X0 = x
)
since 𝔼(ZT|XT = a, X0 = x) = 𝔼(ZT|XT = b, X0 = x). Finally,
𝔼
(
e−1
2 𝜃2T||||
X0 = x
)
=
4 sinh
[
1
2𝜃(b −a)
]
cosh
[
𝜃
(
x −1
2(a + b)
)]
2 sinh[𝜃(b −a)]
=
cosh
[
𝜃
(
x −1
2(a + b)
)]
cosh
[
1
2𝜃(b −a)
]
.
◽

2.2.4
First Passage Time
83
7. Laplace Transform of First Passage Time. Let (Ω, ℱ, ℙ) be a probability space and let
{Wt ∶t ≥0} be a standard Wiener process.
Show that for 𝜆∈ℝ, Xt = e𝜆Wt−1
2 𝜆2t is a martingale.
By setting Ta,b = inf{t ≥0 ∶Wt = a + bt} as the first passage time of hitting the sloping
line a + bt, where a and b are constants, show that the Laplace transform of its distribution
is given by
𝔼(e−𝜃Ta,b) = e−a(b+
√
b2+2𝜃),
𝜃> 0
and hence show that
𝔼(Ta,b) =
(a
b
)
e−2ab
and
ℙ(Ta,b < ∞) = e−2ab.
Solution: To show that Xt = e𝜆Wt−1
2 𝜆2t is a martingale, see Problem 2.2.3.3 (page 72).
From the optional stopping theorem
𝔼(XTa,b) = 𝔼(X0
) = 1
and because at t = Ta,b, WTa,b = a + bTa,b we have
𝔼
[
e𝜆(a+bTa,b)−1
2 𝜆2Ta,b
]
= 1
or
𝔼
[
e
(
𝜆b−1
2 𝜆2)
Ta,b
]
= e−𝜆a.
By setting 𝜃= −
(
𝜆b −1
2𝜆2)
we have 𝜆= b ±
√
b2 + 2𝜃. Since 𝜃> 0 we must have
𝔼(e−𝜃Ta,b) ≤1 and therefore 𝜆= b +
√
b2 + 2𝜃. Thus,
𝔼(e−𝜃Ta,b) = e−a(b+
√
b2+2𝜃).
To find 𝔼(Ta,b) we differentiate 𝔼(e−𝜃Ta,b) with respect to 𝜃,
𝔼(Ta,be−𝜃Ta,b) =
a
√
b2 + 2𝜃
e−a(b+
√
b2+2𝜃)
and setting 𝜃= 0,
𝔼(Ta,b
) =
(a
b
)
e−2ab.
Finally, to find ℙ(Ta,b < ∞) by definition
𝔼(e−𝜃Ta,b) = ∫
∞
0
e−𝜃tfTa,b(t)dt

84
2.2.5
Reflection Principle
where fTa,b(t) is the probability density function of Ta,b. By setting 𝜃= 0,
ℙ(Ta,b < ∞) = 𝔼(e−𝜃Ta,b)|||𝜃=0 = e−2ab.
N.B. Take note that if b = 0 we will have 𝔼(Ta,b
) = ∞and ℙ(Ta,b < ∞) = 1 (we say
Ta,b is finite almost surely).
◽
2.2.5
Reflection Principle
1. Reflection Principle. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a stan-
dard Wiener process. By setting T as a stopping time and defining
̃Wt =
{
Wt
if t ≤T
2WT −Wt
if t > T
show that { ̃Wt ∶t ≥0} is also a standard Wiener process.
Solution: If T is finite then from the strong Markov property both the paths Yt = {Wt+T −
WT ∶t ≥0} and −Yt = {−(Wt+T −WT) ∶t ≥0} are standard Wiener processes and inde-
pendent of Xt = {Wt ∶0 ≤t ≤T}, and hence both (Xt, Yt) and (Xt, −Yt) have the same
distribution. Given the two processes defined on [0, T] and [0, ∞), respectively, we can
paste them together as follows:
𝜙∶(X, Y) →{Xt1I{t≤T} + (Yt−T + WT)1I{t≥T} ∶t ≥0}.
Thus, the process arising from pasting Xt = {Wt ∶0 ≤t ≤T} to Yt = {Wt+T −WT ∶t ≥
0} has the same distribution, which is {Wt ∶t ≥0}. In contrast, the process arising from
pasting Xt = {Wt ∶0 ≤t ≤T} to −Yt = {−(Wt+T −WT) ∶t ≥0} is { ̃Wt ∶t ≥0}. Thus,
{ ̃Wt ∶t ≥0} is also a standard Wiener process.
◽
2. Reflection Equality. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. By defining Tm = inf{t ≥0 ∶Wt = m}, m > 0 as the first passage time,
then for 𝑤≤m, m > 0, show that
ℙ(Tm ≤t, Wt ≤𝑤) = ℙ(Wt ≥2m −𝑤).
Solution: From the reflection principle in Problem 2.2.5.1 (page 84), since WTm = m,
ℙ(Tm ≤t, Wt ≤𝑤) = ℙ(Tm ≤t, 2WTm −Wt ≤𝑤)
= ℙ(Wt ≥2m −𝑤).
◽

2.2.5
Reflection Principle
85
3. First Passage Time Density Function. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶
t ≥0} be a standard Wiener process. By setting T𝑤as a stopping time such that T𝑤=
inf{t ≥0 ∶Wt = 𝑤}, 𝑤≠0 show, using the reflection principle, that
ℙ(T𝑤≤t) = 1 + Φ
(
−|𝑤|
√
t
)
−Φ
(
|𝑤|
√
t
)
and the probability density function of T𝑤is given as
fT𝑤(t) =
|𝑤|
t
√
2𝜋t
e−𝑤2
2t .
Solution: We first consider the case when 𝑤> 0. By definition
ℙ(T𝑤≤t) = ℙ(T𝑤≤t, Wt ≤𝑤) + ℙ(T𝑤≤t, Wt ≥𝑤).
For the case when Wt ≤𝑤, by the reflection equality
ℙ(T𝑤≤t, Wt ≤𝑤) = ℙ(Wt ≥𝑤).
On the contrary, if Wt ≥𝑤then we are guaranteed T𝑤≤t. Therefore,
ℙ(T𝑤≤t, Wt ≥𝑤) = ℙ(Wt ≥𝑤)
and hence
ℙ(T𝑤≤t) = 2ℙ(Wt ≥𝑤) = ∫
∞
𝑤
1
√
2𝜋t
e−y2
2t dy.
By setting x = y∕
√
2t we have
ℙ(T𝑤≤t) = 2 ∫
∞
𝑤
√
t
1
√
2𝜋
e−x2
2 dx = 1 + Φ
(
−𝑤
√
t
)
−Φ
(
𝑤
√
t
)
.
For the case when 𝑤≤0, using the reflection principle
ℙ(T𝑤≤t) = ℙ(T𝑤≤t, Wt ≤𝑤) + ℙ(T𝑤≤t, Wt ≥𝑤)
= ℙ(T𝑤≤t, −Wt ≥−𝑤) + ℙ(T𝑤≤t, −Wt ≤−𝑤)
= ℙ(T𝑤≤t, −Wt ≥−𝑤) + ℙ(T𝑤≤t, −Wt ≤−𝑤)
= ℙ(T𝑤≤t, ̃Wt ≥−𝑤) + ℙ(T𝑤≤t, ̃Wt ≤−𝑤)
where ̃Wt = −Wt is also a standard Wiener process. Therefore,
ℙ(T𝑤≤t) = 2ℙ( ̃Wt ≥−𝑤) = 1 + Φ
(
𝑤
√
t
)
−Φ
(
−𝑤
√
t
)

86
2.2.5
Reflection Principle
and hence
ℙ(T𝑤≤t) = 1 + Φ
(
−|𝑤|
√
t
)
−Φ
(
|𝑤|
√
t
)
.
To find the density of T𝑤we note that
fT𝑤(t) = d
dtℙ(T𝑤≤t) = d
dt
[
1 + Φ
(
−|𝑤|
√
t
)
−Φ
(
|𝑤|
√
t
)]
=
|𝑤|
t
√
2𝜋t
e−𝑤2
2t .
◽
4. Let (Ω, ℱ, ℙ) be a probability space and let Mt = max
0≤s≤t Ws be the maximum level reached
by a standard Wiener process {Wt ∶t ≥0} in the time interval [0, t]. Then for a ≥0, x ≤a
and for all t ≥0, show using the reflection principle that
ℙ(Mt ≤a, Wt ≤x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪⎩
Φ
(
x
√
t
)
−Φ
(
x −2a
√
t
)
a ≥0, x ≤a
Φ
(
a
√
t
)
−Φ
(
−a
√
t
)
a ≥0, x ≥a
0
a ≤0
and the joint probability density function of (Mt, Wt) is
fMt,Wt(a, x) =
⎧
⎪
⎨
⎪⎩
2(2a −x)
t
√
2𝜋t
e
−1
2
(
2a−x
√
t
)2
a ≥0, x ≤a
0
otherwise.
Solution: For a ≥0, x ≤a, let Ta = inf{t ≥0 ∶Wt = a} then
{Mt ≥a} ⇐⇒{Ta ≤t}.
Taking T = Ta in the reflection principle,
ℙ(Mt ≥a, Wt ≤x) = ℙ(Ta ≤t, Wt ≤x)
= ℙ(Ta ≤t, Wt ≥2a −x)
= ℙ(Wt ≥2a −x)
= 1 −Φ
(
2a −x
√
t
)
= Φ
(
x −2a
√
t
)
.

2.2.5
Reflection Principle
87
Because
ℙ(Wt ≤x) = ℙ(Mt ≤a, Wt ≤x) + ℙ(Mt ≥a, Wt ≤x)
then for a ≥0, x ≤a,
ℙ(Mt ≤a, Wt ≤x) = ℙ(Wt ≤x) −ℙ(Mt ≥a, Wt ≤x)
= Φ
(
x
√
t
)
−Φ
(
x −2a
√
t
)
.
For the case when a ≥0, x ≥a and because Mt ≥Wt, we have
ℙ(Mt ≤a, Wt ≤x) = ℙ(Mt ≤a, Wt ≤a) = ℙ(Mt ≤a)
and by substituting x = a into Φ
(
x
√
t
)
−Φ
(
x −2a
√
t
)
we have
ℙ(Mt ≤a, Wt ≤x) = Φ
(
a
√
t
)
−Φ
(
−a
√
t
)
.
Finally, for the case when a ≤0, and since Mt ≥W0 = 0, then ℙ(Mt ≤a, Wt ≤x) = 0.
To obtain the joint probability density function of (Mt, Wt), by definition
fMt,Wt(a, x) =
𝜕2
𝜕a𝜕xℙ(Mt ≤a, Wt ≤x)
and since ℙ(Mt ≤a, Wt ≤x) is a function in both a and x only for the case when a ≥0,
x ≤a, then
fMt,Wt(a, x) =
𝜕2
𝜕a𝜕x
[
Φ
(
x
√
t
)
−Φ
(
x −2a
√
t
)]
= 𝜕
𝜕a
⎡
⎢
⎢⎣
1
√
2𝜋t
e
−1
2
(
x
√
t
)2
−
1
√
2𝜋t
e
−1
2
(
x−2a
√
t
)2⎤
⎥
⎥⎦
= −2(x −2a)
t
√
2𝜋t
e
−1
2
(
x−2a
√
t
)2
= 2(2a −x)
t
√
2𝜋t
e
−1
2
(
2a−x
√
t
)2
.
◽

88
2.2.5
Reflection Principle
5. Let (Ω, ℱ, ℙ) be a probability space and let mt = min
0≤s≤t Ws be the minimum level reached
by a standard Wiener process {Wt ∶t ≥0} in the time interval [0, t]. Then for all t ≥0
show that
ℙ(mt ≥b, Wt ≥x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
Φ
(
−x
√
t
)
−Φ
(
2b −x
√
t
)
b ≤0, b ≤x
Φ
(
−b
√
t
)
−Φ
(
b
√
t
)
b ≤0, b ≥x
0
b ≥0
and the joint probability density function of (mt, Wt) is
fmt,Wt(b, x) =
⎧
⎪
⎨
⎪⎩
−2(2b −x)
t
√
2𝜋t
e
−1
2
(
2b−x
√
t
)2
b ≤0, b ≤x
0
otherwise.
Solution: Given mt = −max
0≤s≤t{−Wt} we have for b ≤0, b ≤x,
ℙ(mt ≥b, Wt ≥x) = ℙ
(
−max
0≤s≤t
{−Wt
} ≥b, Wt ≥x
)
= ℙ
(
−max
0≤s≤t
{−Wt
} ≥b, −Wt ≤−x
)
= ℙ
(
max
0≤s≤t
̃Wt ≤−b, ̃Wt ≤−x
)
,
where the last equality comes from the symmetric property of the standard Wiener process
such that ̃Wt = −Wt ∼𝒩(0, t) is also a standard Wiener process.
Since
ℙ( ̃Wt ≤−x) = ℙ
(
max
0≤s≤t
̃Wt ≤−b, ̃Wt ≤−x
)
+ ℙ
(
max
0≤s≤t
̃Wt ≥−b, ̃Wt ≤−x
)
and from Problem 2.2.5.4 (page 86), we can write
ℙ
(
max
0≤s≤t
̃Wt ≥−b, ̃Wt ≤−x
)
= Φ
(
2b −x
√
t
)
so
ℙ
(
max
0≤s≤t
̃Wt ≤−b, ̃Wt ≤−x
)
= ℙ( ̃Wt ≤−x) −ℙ
(
max
0≤s≤t
̃Wt ≥−b, ̃Wt ≤−x
)
= Φ
(
−x
√
t
)
−Φ
(
2b −x
√
t
)
.

2.2.6
Quadratic Variation
89
Consequently, if b ≤0, b ≤x we have
ℙ(mt ≥b, Wt ≥x) = Φ
(
−x
√
t
)
−Φ
(
2b −x
√
t
)
.
For the case when b ≤0, b ≥x and since mt ≤Wt,
ℙ(mt ≥b, Wt ≥x) = ℙ(mt ≥b, Wt ≥b) = ℙ(mt ≥b).
Setting x = b in Φ
(
−x
√
t
)
−Φ
(
2b −x
√
t
)
would therefore yield
ℙ(mt ≥b, Wt ≥x) = Φ
(
−b
√
t
)
−Φ
(
b
√
t
)
.
Finally, for b ≥0, since mt ≤W0 = 0 so ℙ(mt ≥b, Wt ≥x) = 0.
To obtain the joint probability density function by definition,
fmt,Wt(b, x) =
𝜕2
𝜕b𝜕xℙ(mt ≤b, Wt ≤x)
=
𝜕2
𝜕b𝜕xℙ(mt ≥b, Wt ≥x)
and since ℙ(mt ≥b, Wt ≥x) is a function in both b and x only for the case when b ≤0,
b ≤x, so
fmt,Wt(b, x) =
𝜕2
𝜕b𝜕x
[
Φ
(
−x
√
t
)
−Φ
(
2b −x
√
t
)]
= 𝜕
𝜕b
⎡
⎢
⎢⎣
−
1
√
2𝜋t
e
−1
2
(
x
√
t
)2
+
1
√
2𝜋t
e
−1
2
(
2b−x
√
t
)2⎤
⎥
⎥⎦
= −2(2b −x)
t
√
2𝜋t
e
−1
2
(
2b−x
√
t
)2
.
◽
2.2.6
Quadratic Variation
1. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that it has finite quadratic variation such that
⟨W, W⟩t = lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)2 = t
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Finally, deduce that dWt ⋅dWt = dt.

90
2.2.6
Quadratic Variation
Solution: Since the quadratic variation is a sum of random variables, we need to show
that its expected value is t and its variance converges to zero as n →∞.
Let ΔWti = Wti+1 −Wti ∼𝒩(0, t∕n) where 𝔼(ΔW2
ti
) = t∕n then, by taking expectations
we have
𝔼
(
lim
n→∞
n−1
∑
i=0
ΔW2
ti
)
= lim
n→∞
n−1
∑
i=0
𝔼
(
ΔW2
ti
)
= t.
Because ΔW2
ti∕(t∕n) ∼𝜒2(1) we have 𝔼(ΔW4
ti) = 3(t∕n)2. Therefore, by independence of
increments
Var
(
lim
n→∞
n−1
∑
i=0
ΔW2
ti
)
= 𝔼
⎡
⎢
⎢⎣
(
lim
n→∞
n−1
∑
i=0
ΔW2
ti −t
)2⎤
⎥
⎥⎦
= lim
n→∞
n−1
∑
i=0
𝔼
[(
ΔW2
ti −t
n
)2]
= lim
n→∞
n−1
∑
i=0
(
3t2
n2 −2t2
n2 + t2
n2
)
= lim
n→∞
2t2
n
= 0.
Since lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)2 = ∫t
0 dWs ⋅dWs = t and ∫t
0 ds = t, then by differentiating
both sides with respect to t we have dWt ⋅dWt = dt.
◽
2. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that the following cross-variation between Wt and t, and the quadratic variation of
t, are
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (ti+1 −ti
) = 0
lim
n→∞
n−1
∑
i=0
(ti+1 −ti
)2 = 0
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Finally, deduce that dWt ⋅dt = 0 and dt ⋅dt = 0.
Solution: Since Wti+1 −Wti ∼𝒩(0, t∕n) then taking expectation and variance we have
𝔼
[
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (ti+1 −ti
)
]
= lim
n→∞
n−1
∑
i=0
(ti+1 −ti
) 𝔼
(
Wti+1 −Wti
)
= 0

2.2.6
Quadratic Variation
91
and
Var
[
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (ti+1 −ti
)
]
= lim
n→∞
n−1
∑
i=0
(ti+1 −ti
)2Var
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
( t
n
)2 t
n
= lim
n→∞
t3
n
= 0.
Thus, lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
ti+1 −ti
)
= 0.
In addition,
lim
n→∞
n−1
∑
i=0
(ti+1 −ti)2 = lim
n→∞
n−1
∑
i=0
( t
n
)2
= lim
n→∞
t2
n = 0.
Finally, because
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (ti+1 −ti
) = ∫
t
0
dWs ⋅ds = 0
lim
n→∞
n−1
∑
i=0
(ti+1 −ti
)2 = ∫
t
0
ds ⋅ds = 0
then, by differentiating both sides with respect to t, we can deduce that dWt ⋅dt = 0 and
dt ⋅dt = 0.
◽
3. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that it has unbounded first variation such that
lim
n→∞
n−1
∑
i=0
|||Wti+1 −Wti
||| = ∞
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Solution: Since
n−1
∑
i=0
(
Wti+1 −Wti
)2
≤
max
0≤k≤n−1
|||Wtk+1 −Wtk
||| ⋅
n−1
∑
i=0
|||Wti+1 −Wti
|||, then
n−1
∑
i=0
|||Wti+1 −Wti
||| ≥
n−1
∑
i=0
(
Wti+1 −Wti
)2
max
0≤k≤n−1
|||Wtk+1 −Wtk
|||
.

92
2.2.6
Quadratic Variation
As Wt is continuous, lim
n→∞max
0≤k≤n−1
|||Wtk+1 −Wtk
||| = 0 and lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti
)2 = t < ∞,
we can conclude that lim
n→∞
n−1
∑
i=0
|||Wti+1 −Wti
||| = ∞.
◽
4. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Show that for p ≥3,
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)p
= 0
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Finally, deduce that (dWt)p = 0, p ≥3.
Solution: We prove this result via mathematical induction.
For p = 3, we can express
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)3||||||
≤
max
0≤k≤n−1
|||Wtk+1 −Wtk
||| ⋅
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)2||||||
=
max
0≤k≤n−1
|||Wtk+1 −Wtk
||| ⋅
n−1
∑
i=0
(
Wti+1 −Wti
)2
.
Because Wt is continuous, we have
lim
n→∞max
0≤k≤n−1
|||Wtk+1 −Wtk
||| = 0.
In addition, because lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)2
= t < ∞(see Problem 2.2.6.1, page 89), so
||||||
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)3||||||
≤0.
or
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)3
= 0.
Thus, the result is true for p = 3.
Assume the result is true for p = m, m > 3 so that lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti
)m = 0.
Then for p = m + 1,
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)m+1||||||
≤
max
0≤k≤n−1
|||Wtk+1 −Wtk
||| ⋅
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)m||||||

2.2.6
Quadratic Variation
93
and
because
Wt
is
continuous
such
that
lim
n→∞
max
0≤k≤n −1
(Wtk+1 −Wtk
) = 0
and
lim
n→∞
n −1
∑
i=0
(Wti+1 −Wti
)m = 0, so
||||||
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)m+1||||||
≤0
or
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)m+1
= 0.
Thus, the result is also true for p = m + 1. From mathematical induction we have shown
lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti
)p = 0, p ≥3.
Since for p ≥3, lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti
)p = ∫
t
0
(dWs
)p = 0, by differentiating both sides
with respect to t we have (dWt
)p = 0, p ≥3.
◽


3
Stochastic Differential Equations
A stochastic differential equation (SDE) is a differential equation in which one or more of
the terms has a random component. Within the context of mathematical finance, SDEs are
frequently used to model diverse phenomena such as stock prices, interest rates or volatilities
to name but a few. Typically, SDEs have continuous paths with both random and non-random
components and to drive the random component of the model they usually incorporate a Wiener
process. To enrich the model further, other types of random fluctuations are also employed in
conjunction with the Wiener process, such as the Poisson process when modelling discontin-
uous jumps. In this chapter we will concentrate solely on SDEs having only a Wiener process,
whilst in Chapter 5 we will discuss SDEs incorporating both Poisson and Wiener processes.
3.1
INTRODUCTION
To begin with, a one-dimensional stochastic differential equation can be described as
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
where Wt is a standard Wiener process, 𝜇(Xt, t) is defined as the drift and 𝜎(Xt, t) the volatility.
Many financial models can be written in this form, such as the lognormal asset random walk,
common spot interest rate and stochastic volatility models.
From an initial state X0 = x0, we can write in integrated form
Xt = X0 + ∫
t
0
𝜇(Xs, s) ds + ∫
t
0
𝜎(Xs, s) dWs
with ∫
t
0
[|𝜇(Xs, s)| + 𝜎(Xs, s)2] ds < ∞and the solution of the integral equation is called an
It¯o process or an It¯o diffusion.
In Chapter 2 we described the properties of a Wiener process and, given that it is
non-differentiable, there is a crucial difference between stochastic calculus and integral
calculus. We consider an integral with respect to a Wiener process and write
It = ∫
t
0
f(Ws, s) dWs
where the integrand f(Wt, t) is ℱt measurable (i.e., f(Wt, t) is a stochastic process adapted to
the filtration of a Wiener process) and is square-integrable
𝔼
(
∫
t
0
f(Ws, s)2 ds
)
< ∞
for all t ≥0.

96
3.1
INTRODUCTION
Let t > 0 be a positive constant and assume f(Wti, ti) is constant over the interval [ti, ti+1),
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t for n ∈ℕ. Here we call such a process
f an elementary process or a simple process, and for such a process the It¯o integral It can be
defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)(Wti+1 −Wti).
We now give a formal result of the It¯o integral as follows.
Theorem 3.1 Let {Wt ∶t ≥0} be a standard Wiener process on the probability space
(Ω, ℱ, ℙ), and let ℱt, t ≥0 be the associated filtration. The It¯o integral defined by
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)(Wti+1 −Wti)
where f is a simple process and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕhas
the following properties:
• The paths of It are continuous.
• For each t, It is ℱt measurable.
• If It = ∫t
0 f(Ws, s) dWs and Jt = ∫t
0 g(Ws, s) dWs where g is a simple process, then
It ± Jt = ∫
t
0
[ f(Ws, s) ± g(Ws, s)] dWs
and for a constant c
cIt = ∫
t
0
cf(Ws, s) dWs.
• It is a martingale.
• 𝔼[I2
t ] = 𝔼
(
∫
t
0
f(Ws, s)2 ds
)
.
• The quadratic variation ⟨I, I⟩t = ∫
t
0
f(Ws, s)2 ds.
In mathematics, It¯o’s formula (or lemma) is used in stochastic calculus to find the differential
of a function of a particular type of stochastic process. In essence, it is the stochastic calculus
counterpart of the chain rule in ordinary calculus via a Taylor series expansion. The formula is
widely employed in mathematical finance and its best-known application is in the derivation
of the Black–Scholes equation used to value options. The following is a formal result of It¯o’s
formula.
Theorem 3.2 (One-Dimensional It¯o’s Formula) Let {Wt ∶t ≥0} be a standard Wiener
process on the probability space (Ω, ℱ, ℙ) and let ℱt, t ≥0 be the associated filtration. Con-
sider a stochastic process Xt satisfying the following SDE
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt

3.1
INTRODUCTION
97
or in integrated form,
Xt = X0 + ∫
t
0
𝜇(Xs, s) ds + ∫
t
0
𝜎(Xs, s) dWs
with ∫
t
0
[|𝜇(Xs, s)| + 𝜎(Xs, s)2] ds < ∞. Then for any twice differentiable function g(Xt, t),
the stochastic process Yt = g(Xt, t) satisfies
dYt = 𝜕g
𝜕t (Xt, t) dt + 𝜕g
𝜕Xt
(Xt, t) dXt + 1
2
𝜕2g
𝜕X2
t
(Xt, t)(dXt)2
=
[
𝜕g
𝜕t (Xt, t) + 𝜇(Xt, t) 𝜕g
𝜕Xt
(Xt, t) + 1
2𝜎(Xt, t)2 𝜕2g
𝜕X2
t
(Xt, t)
]
dt + 𝜎(Xt, t) 𝜕g
𝜕Xt
(Xt, t) dWt
where (dXt)2 is computed according to the rule
(dWt)2 = dt,
(dt)2 = dWt dt = dt dWt = 0.
In integrated form,
Yt = Y0 + ∫
t
0
𝜕g
𝜕t (Xs, s) ds + ∫
t
0
𝜕g
𝜕Xt
(Xs, s) dXs + 1
2 ∫
t
0
𝜕2g
𝜕X2
t
(Xs, s) d⟨X, X⟩s
= Y0 + ∫
t
0
[
𝜕g
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕g
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2g
𝜕X2
t
(Xs, s)
]
ds
+ ∫
t
0
𝜎(Xs, s) 𝜕g
𝜕Xt
(Xs, s) dWs
where ⟨X, X⟩t = ∫t
0 𝜎(Xs, s)2 ds.
The theory of SDEs is a framework for expressing the dynamical models that include both
the random and non-random components. Many solutions to SDEs are Markov processes,
where the future depends on the past only through the present. For this reason, the solutions
can be studied using backward and forward Kolmogorov equations, which turn out to be lin-
ear parabolic partial differential equations of diffusion type. But before we discuss them, the
following Feynman–Kac theorem describes an important link between stochastic differential
equations and partial differential equations.
Theorem 3.3 (Feynman–Kac Formula for One-Dimensional Diffusion Process) Let
{Wt ∶t ≥0} be a standard Wiener process on the probability space (Ω, ℱ, ℙ) and let ℱt,
t ≥0 be the associated filtration. Let Xt be the solution of the following SDE:
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt

98
3.1
INTRODUCTION
and define r as a function of t. For t ∈[0, T] where T > 0 and if V(Xt, t) satisfies the partial
differential equation (PDE)
𝜕V
𝜕t (Xt, t) + 1
2𝜎(Xt, t)2 𝜕2V
𝜕X2
t
(Xt, t) + 𝜇(Xt, t) 𝜕V
𝜕Xt
(Xt, t) −r(t)V(Xt, t) = 0
subject to the boundary condition V(XT, T) = Ψ(XT), then under the filtration ℱt the solution
of the PDE is given by
V(Xt, t) = 𝔼
[
e−∫T
t
r(u)du Ψ(XT)||||
ℱt
]
.
The Feynman–Kac formula can be used to study the transition probability density function
of the one-dimensional stochastic differential equation
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt.
For t ∈[0, T], T > 0 and conditioning Xt = x we can write
V(x, t) = 𝔼
[
e−∫T
t
r(u)duΨ(XT)|||ℱt
]
= e−∫T
t
r(u)du
∫
∞
−∞
Ψ(y)p(x, t; y, T) dy
where the random variable XT has transition probability density p(x, t; y, T) in the y variable.
The transition probability density function p(x, t; y, T) satisfies two parabolic partial dif-
ferential equations, the forward Kolmogorov equation (or Fokker–Planck equation) and the
backward Kolmogorov equation. In the forward Kolmogorov function, it involves derivatives
with respect to the future state y and time T, whilst in the backward Kolmogorov function, it
involves derivatives with respect to the current state x and time t.
Theorem 3.4 (Forward Kolmogorov Equation for One-Dimensional Diffusion Pro-
cess) Let {Wt ∶t ≥0} be the standard Wiener process on the probability space (Ω, ℱ, ℙ).
Consider the stochastic differential equation
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
and for t ∈[0, T], T > 0 and by conditioning Xt = x, the random variable XT having a tran-
sition probability density p(x, t; y, T) in the y variable satisfies
𝜕
𝜕T p(x, t; y, T) = 1
2
𝜕2
𝜕y2 (𝜎(y, T)2p(x, t; y, T)) −𝜕
𝜕y(𝜇(y, T)p(x, t; y, T)).
Theorem 3.5 (Backward Kolmogorov Equation for One-Dimensional Diffusion Pro-
cess) Let {Wt ∶t ≥0} be the standard Wiener process on the probability space (Ω, ℱ, ℙ).
Consider the stochastic differential equation
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt

3.1
INTRODUCTION
99
and for t ∈[0, T], T > 0 and by conditioning Xt = x, the random variable XT having a tran-
sition probability density p(x, t; y, T) in the x variable satisfies
𝜕
𝜕tp(x, t; y, T) + 1
2𝜎(x, t)2 𝜕2
𝜕x2 p(x, t; y, T) + 𝜇(x, t) 𝜕
𝜕xp(x, t; y, T) = 0.
In contrast, a multi-dimensional diffusion process can be described by a set of stochastic
differential equations
dX(i)
t
= 𝜇(i)(X(i)
t , t) dt +
n
∑
j=1
𝜎(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where Wt =
[
W(1)
t , W(2)
t , . . . , W(n)
t
]T
is the n-dimensional standard Wiener process such that
dW(i)
t dW(j)
t
= 𝜌ijdt, 𝜌ij ∈(−1, 1) for i ≠j, i, j = 1, 2, . . . , n, 𝝁(Xt, t) =
[
𝜇(1)(X(1)
t , t), 𝜇(2)(X(2)
t ,
t), . . . , 𝜇(m)(X(m)
t
, t)
]T
is the m-dimensional drift vector and
𝝈(Xt, t) =
⎡
⎢
⎢
⎢
⎢⎣
𝜎(1,1)(X(1)
t , t) 𝜎(1,2)(X(1)
t , t) . . . 𝜎(1,n)(X(1)
t , t)
𝜎(2,1)(X(2)
t , t) 𝜎(2,2)(X(2)
t , t) . . . 𝜎(2,n)(X(2)
t , t)
⋮
⋮
⋱
⋮
𝜎(m,1)(X(m)
t
, t) 𝜎(m,2)(X(m)
t
, t) . . . 𝜎(m,n)(X(m)
t
, t)
⎤
⎥
⎥
⎥
⎥⎦
is the m × n volatility matrix. Given the initial state X(i)
0 , we can write in integrated form
X(i)
t
= X(i)
0 + ∫
t
0
𝜇(i)(X(i)
s , s) ds +
n
∑
j=1 ∫
t
0
𝜎(i,j)(X(i)
s , s) dW(j)
s
with ∫
t
0
[
|𝜇(i)(X(i)
s , s)| + |𝜎(i,p)(X(i)
s , s)𝜎(i,q)(X(i)
s , s)|
]
ds < ∞for i = 1, 2, . . . , m and p, q =
1, 2, . . . , n.
Definition 3.6 (Multi-Dimensional It¯o’s Formula) Let {W(i)
t
∶t ≥0}, i = 1, 2, . . . , n be a
sequence of standard Wiener processes on the probability space (Ω, ℱ, ℙ), and let ℱt, t ≥0
be the associated filtration. Consider a stochastic process X(i)
t
satisfying the following SDE
dX(i)
t
= 𝜇(i)(X(i)
t , t) dt +
n
∑
j=1
𝜎(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
or in integrated form
X(i)
t
= X(i)
0 + ∫
t
0
𝜇(i)(X(i)
s , s) ds +
n
∑
j=1 ∫
t
0
𝜎(i,j)(X(i)
s , s) dW(j)
s
with ∫t
0 |𝜇(i)(X(i)
s , s)| + |𝜎(i,p)(X(i)
s , s)𝜎(i,q)(X(i)
s , s)| ds < ∞for i = 1, 2, . . . , m and p, q =
1, 2, . . . , n. Then, for any twice differentiable function h(Xt, t), where Xt =
[
X(1)
t , X(2)
t , . . . ,

100
3.1
INTRODUCTION
X(m)
t
]T
, the stochastic process Zt = h(X(1)
t , X(2)
t , . . . , X(m)
t
, t) satisfies
dZt = 𝜕h
𝜕t (Xt, t) dt +
m
∑
i=1
𝜕h
𝜕X(i)
t
(Xt, t) dX(i)
t + 1
2
m
∑
i=1
m
∑
j=1
𝜕2h
𝜕X(i)
t 𝜕X(j)
t
(Xt, t) dX(i)
t dX(j)
t
=
[
𝜕h
𝜕t (Xt, t) +
m
∑
i=1
𝜇(i)(X(i)
t , t) 𝜕h
𝜕X(i)
t
(Xt, t)
+1
2
m
∑
i=1
m
∑
j=1
𝜌ij
( n
∑
k=1
n
∑
l=1
𝜎(i,k)(X(i)
t , t)𝜎(j,l)(X(j)
t , t)
)
𝜕2h
𝜕X(i)
t 𝜕X(j)
t
(Xt, t)
]
dt
+
m
∑
i=1
( n
∑
j=1
𝜎(i,j)(X(i)
t , t)
)
𝜕h
𝜕X(i)
t
(Xt, t) dW(i)
t
where dX(i)
t dX(j)
t
is computed according to the rule
dW(i)
t dW(j)
t
= 𝜌ijdt,
(dt)2 = dW(i)
t dt = dt dW(i)
t
= 0
such that 𝜌ij ∈(−1, 1) and 𝜌ii = 1. In integrated form
Zt = Z0 + ∫
t
0
𝜕h
𝜕t (Xs, s) ds + ∫
t
0
m
∑
i=1
𝜕h
𝜕X(i)
t
(Xs, s) dX(i)
s
+ ∫
t
0
1
2
m
∑
i=1
m
∑
j=1
𝜕2h
𝜕X(i)
t 𝜕X(j)
t
(Xs, s) d⟨X(i), X(j)⟩s
= Z0 + ∫
t
0
[
𝜕h
𝜕t (Xs, s) +
m
∑
i=1
𝜇(i)(X(i)
s , s) 𝜕h
𝜕X(i)
t
(Xs, s)
+1
2
m
∑
i=1
m
∑
j=1
𝜌ij
( n
∑
k=1
n
∑
l=1
𝜎(i,k)(X(i)
s , s)𝜎(j,l)(X(j)
s , s)
)
𝜕2h
𝜕X(i)
t 𝜕X(j)
t
(Xs, s)
]
ds
+ ∫
t
0
m
∑
i=1
( n
∑
j=1
𝜎(i,j)(X(i)
s , s)
)
𝜕h
𝜕X(i)
t
(Xs, s) dW(i)
s ,
where ⟨X(i), X(j)⟩t = ∫
t
0
𝜌ij
n
∑
k=1
n
∑
l=1
𝜎(i,k)(X(i)
s , s)𝜎(j,l)(X(j)
s , s) ds.
The Feynman–Kac theorem for a one-dimensional diffusion process also extends to a
multi-dimensional version.
Theorem 3.7 (Feynman–Kac Formula for Multi-Dimensional Diffusion Process) Let
{W(i)
t
∶t ≥0}, i = 1, 2, . . . , n be a sequence of standard Wiener processes on the probability

3.1
INTRODUCTION
101
space (Ω, ℱ, ℙ), and let ℱt, t ≥0 be the associated filtration. Let X(i)
t
be the solution of the
following SDE
dX(i)
t
= 𝜇(i)(X(i)
t , t) dt +
n
∑
j=1
𝜎(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= 𝜌ijdt, 𝜌ij ∈(−1, 1) for i ≠j, 𝜌ii = 1, i, j = 1, 2, . . . , n and define r to be
a function of t. By denoting Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, for t ∈[0, T] where T > 0 and if
V(Xt, t) satisfies the PDE
𝜕V
𝜕t (Xt, t) + 1
2
m
∑
i=1
m
∑
j=1
( n
∑
k=1
n
∑
l=1
𝜎(i,k)(X(i)
t , t)𝜎(j,l)(X(j)
t , t)
)
𝜕2V
𝜕X(i)
t 𝜕X(j)
t
(Xt, t)
+
m
∑
i=1
𝜇(i)(X(i)
t , t) 𝜕V
𝜕X(i)
t
(Xt, t) −r(t)V(Xt, t) = 0
subject to the boundary condition V(XT, T) = Ψ(XT), then under the filtration ℱt, the solution
of the PDE is given by
V(Xt, t) = 𝔼
[
e−∫T
t
r(u)du Ψ(XT)
||||
ℱt
]
.
Similarly, we have the Kolmogorov forward and backward equations for multi-dimensional
diffusion processes as well.
Theorem 3.8 (Forward Kolmogorov Equation for Multi-Dimensional Diffusion Pro-
cess) Let {W(i)
t
∶t ≥0}, i = 1, 2, . . . , n be a sequence of standard Wiener processes on the
probability space (Ω, ℱ, ℙ). Consider the stochastic differential equations
dX(i)
t
= 𝜇(i)(X(i)
t , t) dt +
n
∑
j=1
𝜎(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= 𝜌ijdt, 𝜌ij ∈(−1, 1) for i ≠j, 𝜌ii = 1, i, j = 1, 2, . . . , n. By denoting
Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, and for t ∈[0, T], T > 0 and by conditioning Xt = x where
x =
[
x(1), x(2), . . . , x(m)]T
, the m-dimensional random variable XT having a transition
probability density p(x, t; y, T) in the m-dimensional variable y =
[
y(1), y(2), . . . , y(m)]T
satisfies
𝜕
𝜕T p(x, t; y, T) = 1
2
m
∑
i=1
m
∑
j=1
𝜕2
𝜕y(i)𝜕y(j)
[ n
∑
k=1
n
∑
l=1
(𝜌kl𝜎(i,k)(y(i), T)𝜎(j,l)(y(j), T)) p(x, t; y, T)
]
−
m
∑
i=1
𝜕
𝜕y(i) (𝜇(i)(y(i), T)p(x, t; y, T)).

102
3.2.1
It¯o Calculus
Theorem 3.9 (Backward Kolmogorov Equation for Multi-Dimensional Diffusion Pro-
cess) Let {W(i)
t
∶t ≥0}, i = 1, 2, . . . , n be a sequence of independent standard Wiener pro-
cesses on the probability space (Ω, ℱ, ℙ). Consider the stochastic differential equations
dX(i)
t
= 𝜇(i)(X(i)
t , t) dt +
n
∑
j=1
𝜎(i,j)(X(i), t) dW(j)
t ,
i = 1, 2, . . . , m
where dW(i)
t dW(j)
t
= 𝜌ijdt, 𝜌ij ∈(−1, 1) for i ≠j, 𝜌ii = 1, i, j = 1, 2, . . . , n. By denoting
Xt =
[
X(1)
t , X(2)
t , . . . , X(m)
t
]T
, and for t ∈[0, T], T > 0 and by conditioning Xt = x where
x =
[
x(1), x(2), . . . , x(m)]T
, the m-dimensional random variable XT having a transition
probability density p(x, t; y, T) in the m-dimensional variable y =
[
y(1), y(2), . . . , y(m)]T
satisfies
𝜕
𝜕tp(x, t; y, T) + 1
2
m
∑
i=1
m
∑
j=1
( n
∑
k=1
n
∑
l=1
𝜌kl𝜎(i,k)(x(i), t)𝜎(j,l)(x(j), t)
)
𝜕2
𝜕x(i)𝜕x(j) p(x, t; y, T)
+
m
∑
i=1
𝜇(i)(x(i), t) 𝜕
𝜕x(i) p(x, t; y, T) = 0.
3.2
PROBLEMS AND SOLUTIONS
3.2.1
It¯o Calculus
1. It¯o Integral. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener
process. Let the It¯o integral of Wt dWt be defined as the following limit
I(t) = ∫
t
0
Ws dWs = lim
n→∞
n−1
∑
i=0
Wti
(
Wti+1 −Wti
)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t for n ∈ℕ.
Show that the quadratic variation of Wt is
⟨W, W⟩t = lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)2
= t
and hence
I(t) = 1
2
(W2
t −t) .
Finally, show that the It¯o integral is a martingale.
Solution: For the first part of the solution, see Problem 2.2.6.1 (page 89).
Given lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)2
= t and by expanding,
I(t) = lim
n→∞
n−1
∑
i=0
Wti
(
Wti+1 −Wti
)

3.2.1
It¯o Calculus
103
= lim
n→∞
n−1
∑
i=0
{
1
2
(
W2
ti+1 −W2
ti
)
−1
2
(
Wti+1 −Wti
)2}
= 1
2
[
lim
n→∞
(
W2
tn −W2
0
)
−t
]
= 1
2
(W2
t −t) .
To show that I(t) is a martingale, see Problem 2.2.3.2 (page 72).
N.B. Without going through first principles, we can also show that ∫t
0 Ws dWs =
1
2
(W2
t −t) by using It¯o’s formula on Xt = 1
2W2
t , where
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕Wt
dWt + 1
2
𝜕2Xt
𝜕W2
t
dW2
t + . . .
= WtdWt + 1
2dt.
Taking integrals,
∫
t
0
dXs = ∫
t
0
Ws dWs + 1
2 ∫
t
0
ds
Xt −X0 = ∫
t
0
Ws dWs + 1
2t
and since W0 = 0, so ∫
t
0
Ws dWs = 1
2
(W2
t −t).
◽
2. Stratonovich Integral. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a stan-
dard Wiener process. Let the Stratonovich integral of Wt ∘dWt be defined by the following
limit
S(t) = ∫
t
0
Ws ∘dWs = lim
n→∞
n−1
∑
i=0
1
2
(
Wti+1 + Wti
) (
Wti+1 −Wti
)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < . . . < tn−1 < tn = t, n ∈ℕ.
Show that S(t) = 1
2W2
t and show also that the Stratonovich integral is not a martingale.
Solution: Expanding,
S(t) = lim
n→∞
n−1
∑
i=0
1
2
(
Wti+1 + Wti
) (
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
1
2
(
W2
ti+1 −W2
ti
)
= lim
n→∞
1
2
(
W2
tn −W2
t0
)
= 1
2W2
t .

104
3.2.1
It¯o Calculus
Let S(u) = ∫
u
0
Ws ∘dWs, u < t and under the filtration ℱu and because Wt −Wu ⟂⟂ℱu,
we have
𝔼(S(t)|ℱu) = 1
2𝔼
(
W2
t ||| ℱu
)
= 1
2𝔼
[(Wt −Wu + Wu
)2||| ℱu
]
= 1
2𝔼
[(Wt −Wu
)2||| ℱu
]
+ 𝔼
[
Wu
(Wt −Wu
)||| ℱu
]
+ 1
2𝔼
(
W2
u||| ℱu
)
= 1
2(t −u) + 1
2W2
u
≠1
2W2
u.
Therefore, S(t) is not a martingale.
◽
3. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Let the integral of Wt ∗dWt be defined by the following limit
J(t) = ∫
t
0
Ws ∗dWs = lim
n→∞
n−1
∑
i=0
Wti+1
(
Wti+1 −Wti
)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Show that the quadratic variation of Wt is
⟨W, W⟩t = lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)2
= t
and hence
J(t) = 1
2
(W2
t + t) .
Finally, show also that the integral is not a martingale.
Solution: To show that lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)2
= t, see Problem 2.2.6.1 (page 89).
By expanding,
J(t) = lim
n→∞
n−1
∑
i=0
Wti+1
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
{
1
2
(
W2
ti+1 −W2
ti
)
+ 1
2
(
Wti+1 −Wti
)2}
= 1
2
[
lim
n→∞
(
W2
tn −W2
0
)
+ t
]
= 1
2
(W2
t + t) .

3.2.1
It¯o Calculus
105
To show that J(t) is not a martingale, we note from Problem 3.2.1.2 (page 103) that under
the filtration ℱu, u < t,
𝔼(J(t)|ℱu) = 𝔼
(1
2
(
W2
t + t
)|||ℱu
)
= 1
2t + 1
2𝔼
(
W2
t |||ℱu
)
= 1
2t + 1
2(t −u) + 1
2W2
u
≠1
2
(
W2
u + u
)
.
Therefore, J(t) is not a martingale.
◽
4. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
We define the Stratonovich integral of f(Wt, t) ∘dWt as the following limit
∫
t
0
f(Ws, s) ∘dWs = lim
n→∞
n−1
∑
i=0
f
(Wti + Wti+1
2
, ti
) (
Wti+1 −Wti
)
and the It¯o integral of f(Wt, t) dWt as
∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)(Wti+1 −Wti)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Show that
∫
t
0
f(Ws, s) ∘dWs = 1
2 ∫
t
0
𝜕f(Ws, s)
𝜕Ws
ds + ∫
t
0
f(Ws, s) dWs.
Solution: To prove this result we consider the difference between the two stochastic inte-
grals and using the mean value theorem,
∫
t
0
f(Ws, s) ∘dWs −∫
t
0
f(Ws, s) dWs
= lim
n→∞
n−1
∑
i=0
f
(Wti + Wti+1
2
, ti
) (
Wti+1 −Wti
)
−lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
[
f
(Wti + Wti+1
2
, ti
)
−f(Wti, ti)
] (
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
[
f
(
Wti +
Wti+1 −Wti+1
2
, ti
)
−f(Wti, ti)
] (
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
[
1
2
𝜕f(Wti, ti)
𝜕Wti
(
Wti+1 −Wti
)2
+ 1
4
𝜕2f(Wti, ti)
𝜕W2
ti
(
Wti+1 −Wti
)3
+ . . .
]

106
3.2.1
It¯o Calculus
since lim
n→∞
n−1
∑
i=0
(Wti+1 −Wti)p = 0 for p ≥3 and hence, for a simple process g(Wt, t),
||||||
lim
n→∞
n−1
∑
i=0
g(Wti, ti)
(
Wti+1 −Wti
)p||||||
≤lim
n→∞max
0≤k≤n−1|g(Wtk, tk)|
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)p||||||
= 0,
p ≥3
or
lim
n→∞
n−1
∑
i=0
g(Wti, ti)
(
Wti+1 −Wti
)p
= 0,
p ≥3.
Therefore,
∫
t
0
f(Ws, s) ∘dWs −∫
t
0
f(Ws, s) dWs
= lim
n→∞
n−1
∑
i=0
1
2
𝜕f(Wti, ti)
𝜕Wti
(
Wti+1 −Wti
)2
= lim
n→∞
n−1
∑
i=0
1
2
𝜕f(Wti, ti)
𝜕Wti
(
ti+1−ti
)
+ lim
n→∞
n−1
∑
i=0
1
2
𝜕f(Wti, ti)
𝜕Wti
[(
Wti+1−Wti
)2
−
(
ti+1 −ti
)]
= lim
n→∞
n−1
∑
i=0
1
2
𝜕f(Wti, ti)
𝜕Wti
(
ti+1 −ti
)
= 1
2 ∫
t
0
𝜕f (Ws, s)
𝜕Ws
ds
since, from Problem 2.2.6.1 (page 89),
lim
n→∞
n−1
∑
i=0
1
2
𝜕f(Wti, ti)
𝜕Wti
[(
Wti+1 −Wti
)2
−
(
ti+1 −ti
)]
≤lim
n→∞max
0≤k≤n−1
|||||
1
2
𝜕f(Wtk, tk)
𝜕Wtk
|||||
[n−1
∑
i=0
(
Wti+1 −Wti
)2
−
n−1
∑
i=0
(
ti+1 −ti
)]
= 0.
Thus,
∫
t
0
f(Ws, s) ∘dWs = 1
2 ∫
t
0
𝜕f(Ws, s)
𝜕Ws
ds + ∫
t
0
f(Ws, s) dWs.
◽

3.2.1
It¯o Calculus
107
5. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process
such that ℱt is the associated filtration. The It¯o integral with respect to the standard Wiener
process can be defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
where f is a simple process and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Show that the path of It is continuous and that it is also ℱt measurable for all t.
Solution: Given that Wti, ti = it∕n, n ∈ℕis both continuous and ℱti measurable, then
for a simple process, f,
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
.
The path of the It¯o integral is also continuous and ℱt measurable for all t.
◽
6. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process
such that ℱt is the associated filtration. The It¯o integrals with respect to the standard
Wiener process can be defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
and
Jt = ∫
t
0
g(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
g(Wti, ti)
(
Wti+1 −Wti
)
where f and g are simple processes and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t,
n ∈ℕ. Show that
It ± Jt = ∫
t
0
[
f(Ws, s) ± g(Ws, s)
]
dWs
and for constant c,
cIt = ∫
t
0
cf(Ws, s) dWs,
cJt = ∫
t
0
cg(Ws, s) dWs.
Solution: Using the sum rule in integration,
It ± Jt = ∫
t
0
f(Ws, s) dWs ± ∫
t
0
g(Ws, s) dWs
= lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
± lim
n→∞
n−1
∑
i=0
g(Wti, ti)
(
Wti+1 −Wti
)

108
3.2.1
It¯o Calculus
= lim
n→∞
n−1
∑
i=0
[
f(Wti, ti) ± g(Wti, ti)
](
Wti+1 −Wti
)
= ∫
t
0
[
f(Ws, s) ± g(Ws, s)
]
dWs.
For constant c,
cIt = c ∫
t
0
f(Ws, s) dWs
= c lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
c f(Wti, ti)
(
Wti+1 −Wti
)
= ∫
t
0
c f(Ws, s) dWs.
The same steps can be applied to show that cJt = ∫
t
0
cg(Ws, s) dWs.
◽
7. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process
such that ℱt is the associated filtration. The stochastic It¯o integral with respect to the
standard Wiener process can be defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
where f is a simple function and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Using the properties of the standard Wiener process, show that It is a martingale.
Solution: Given that Wt is a martingale, we note the following:
(a) Under the filtration ℱu, u < t, by definition
∫
t
0
f(Ws, s) dWs = ∫
u
0
f(Ws, s) dWs + ∫
t
u
f(Ws, s) dWs
= lim
n→∞
m−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
+ lim
n→∞
[n−1
∑
i=m
f(Wti, ti)
(
Wti+1 −Wti
)]
where
Iu = ∫
u
0
f(Ws, s) dWs = lim
n→∞
m−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
,
m < n −1
and
𝔼(Iu|ℱu) = Iu.

3.2.1
It¯o Calculus
109
Finally, because {Wt ∶t ≥0} is a martingale we have
𝔼(It|ℱu) = 𝔼
[
lim
n→∞
m−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)||||||
ℱu
]
+𝔼
[
lim
n→∞
[n−1
∑
i=m
f(Wti, ti)
(
Wti+1 −Wti
)]||||||
ℱu
]
= Iu + lim
n→∞𝔼
[ n−1
∑
i=m
f(Wti, ti)
(
Wti+1 −Wti
)||||||
ℱu
]
= Iu + lim
n→∞
n−1
∑
i=m
𝔼
[
f(Wti, ti)
(
Wti+1 −Wti
)|||ℱu
]
= Iu + lim
n→∞
n−1
∑
i=m
𝔼
[
𝔼
[
f(Wti, ti)
(
Wti+1 −Wti
)|||ℱti
] |||ℱu
]
= Iu + lim
n→∞
n−1
∑
i=m
𝔼
[
f(Wti, ti)
(
Wti −Wti
)|||ℱu
]
= Iu.
(b) Assuming | f(Wt, t)| < ∞, we have
|It| = lim
n→∞
||||||
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)||||||
≤lim
n→∞
n−1
∑
i=0
||||
f(Wti, ti)
(
Wti+1 −Wti
)||||
≤lim
n→∞
[
max
0≤k≤n−1
|||Wtk+1 −Wtk
|||
n−1
∑
i=0
||| f(Wti, ti)|||
]
.
Because Wt is continuous we have lim
n→∞max
0≤k≤n−1
|||Wtk+1 −Wtk
||| = 0, therefore we can
deduce that 𝔼(|It|) < ∞.
(c) Since It is a function of Wt, hence it is ℱt-adapted.
From the results of (a)–(c) we have shown that It is a martingale.
N.B. From the above result we can easily deduce that if {Mt}t≥0 is a martingale with
continuous sample paths and by defining the following stochastic It¯o integral:
It = ∫
t
0
f(Ms, s) dMs = lim
n→∞
n−1
∑
i=0
f(Mti, ti)
(
Mti+1 −Mti
)
where f is a simple process and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ,
then It is a martingale.
◽

110
3.2.1
It¯o Calculus
8. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
The stochastic It¯o integral with respect to a standard Wiener process can be defined as
∫
t
0
s dWs = lim
n→∞
n−1
∑
i=0
ti
(
Wti+1 −Wti
)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ. Prove that
∫
t
0
s dWs = tWt −∫
t
0
Ws ds.
Solution: By definition, W0 = 0 and we can expand
∫
t
0
s dWs = lim
n→∞
n−1
∑
i=0
ti
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
(
ti
(
Wti+1 −Wti
)
+ ti+1Wti+1 −ti+1Wti+1
)
= lim
n→∞
n−1
∑
i=0
(
ti+1Wti+1 −tiWti −(ti+1 −ti
) Wti+1
)
= lim
n→∞
n−1
∑
i=0
(
ti+1Wti+1 −tiWti
)
−lim
n→∞
n−1
∑
i=0
Wti+1
(ti+1 −ti
)
= tWt −lim
n→∞
n−1
∑
i=0
Wti+1
(ti+1 −ti
) .
By definition,
∫
t
0
Ws ds = lim
n→∞
n−1
∑
i=0
Wti
(ti+1 −ti
)
and to show
lim
n→∞
n−1
∑
i=0
Wti+1
(ti+1 −ti
) −lim
n→∞
n−1
∑
i=0
Wti
(ti+1 −ti
) = 0
we note from Problem 2.2.6.2 (page 90) that
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (ti+1 −ti
) = 0.
Therefore, we can deduce that
lim
n→∞
n−1
∑
i=0
Wti+1
(ti+1 −ti
) = ∫
t
0
Ws ds

3.2.1
It¯o Calculus
111
and hence
∫
t
0
s dWs = tWt −∫
t
0
Ws ds.
◽
9. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
The stochastic It¯o integral with respect to a standard Wiener process can be defined as
∫
t
0
W2
s dWs = lim
n→∞
n−1
∑
i=0
W2
ti
(
Wti+1 −Wti
)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ. Prove that
∫
t
0
W2
s dWs = 1
3W3
t −∫
t
0
Ws ds.
Solution: By definition,
lim
n→∞
n−1
∑
i=0
W2
ti
(
Wti+1 −Wti
)
= lim
n→∞
n−1
∑
i=0
1
3
(
W3
ti+1 −W3
ti
)
−lim
n→∞
n−1
∑
i=0
Wti
(
Wti+1 −Wti
)2
−lim
n→∞
n−1
∑
i=0
1
3
(
Wti+1 −Wti
)3
= lim
n→∞
n−1
∑
i=0
1
3
(
W3
ti+1 −W3
ti
)
−lim
n→∞
n−1
∑
i=0
Wti
(
ti+1 −ti
)
−lim
n→∞
n−1
∑
i=0
Wti
[(
Wti+1 −Wti
)2
−(ti+1 −ti
)]
−lim
n→∞
n−1
∑
i=0
1
3
(
Wti+1 −Wti
)3
= 1
3W3
t −∫
t
0
Ws ds
since from Problems 2.2.6.1 (page 89) and 2.2.6.4 (page 92),
||||||
lim
n→∞
n−1
∑
i=0
Wti
[(
Wti+1 −Wti
)2
−(ti+1 −ti)
]||||||
≤lim
n→∞max
0≤k≤n−1 |Wtk|
||||||
n−1
∑
i=0
(
Wti+1 −Wti
)2
−
n−1
∑
i=0
(ti+1 −ti
)||||||
= lim
n→∞max
0≤k≤n−1 |Wtk||t −t|
= 0

112
3.2.1
It¯o Calculus
and
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
)3
= 0.
Therefore, ∫
t
0
W2
s dWs = 1
3W3
t −∫
t
0
Ws ds.
N.B. Instead of going through first principles, we can also show ∫
t
0
W2
s dWs = 1
3W3
t −
∫
t
0
Ws ds by applying It¯o’s formula on Xt = 1
3W3
t , where
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕Wt
dWt + 1
2
𝜕2Xt
𝜕W2
t
dW2
t + . . .
= W2
t dWt + Wtdt.
Taking integrals,
∫
t
0
dXs = ∫
t
0
W2
s dWs + ∫
t
0
Ws ds
Xt −X0 = ∫
t
0
W2
s dWs + ∫
t
0
Ws ds
and since W0 = 0, therefore ∫
t
0
W2
s dWs = 1
3W3
t −∫
t
0
Ws ds.
◽
10. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
The stochastic It¯o integral with respect to a standard Wiener process can be defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
where f is a simple process and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Using the properties of a standard Wiener process, show that
𝔼
[
∫
t
0
f(Ws, s) dWs
]
= 0
and deduce that if {Mt ∶t ≥0} is a martingale then
𝔼
[
∫
t
0
g(Ms, s) dMs
]
= 0
where g is a simple process.

3.2.1
It¯o Calculus
113
Solution: By definition,
𝔼
[
∫
t
0
f(Ws, s) dWs
]
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)
(
Wti+1 −Wti
)]
= lim
n→∞
n−1
∑
i=0
𝔼
[
𝔼
[
f(Wti, ti)
(
Wti+1 −Wti
) |||ℱti
]]
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)
(
Wti −Wti
)]
= 0
since {Wt ∶t ≥0} is a martingale and therefore 𝔼
(
Wti+1
|||ℱti
)
= Wti.
Using the same steps as described above, if {Mt ∶t ≥0} is a martingale then
𝔼
[
∫
t
0
g(Ms, s) dMs
]
= 0
where g is a simple process.
◽
11. It¯o Isometry. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process. The stochastic It¯o integral with respect to a standard Wiener process can
be defined as
It = ∫
t
0
f(Ws, s) dWs = lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)
where f is a simple process and ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕ.
Using the properties of a standard Wiener process, show that
𝔼
[(
∫
t
0
f(Ws, s) dWs
)2]
= 𝔼
[
∫
t
0
f(Ws, s)2ds
]
.
Solution: Using the property of independent increments of a standard Wiener process as
well as the martingale properties of Wt and W2
t −t, we note that
𝔼
[(
∫
t
0
f(Ws, s) dWs
)2]
−𝔼
[
∫
t
0
f(Ws, s)2ds
]
= 𝔼
⎡
⎢
⎢⎣
{
lim
n→∞
n−1
∑
i=0
f(Wti, ti)
(
Wti+1 −Wti
)}2⎤
⎥
⎥⎦
−𝔼
[
lim
n→∞
n−1
∑
i=0
f(Wti, ti)2 (ti+1 −ti
)
]
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)2(
Wti+1 −Wti
)2]
−lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)2 (ti+1 −ti
)]

114
3.2.1
It¯o Calculus
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)2
{(
Wti+1 −Wti
)2
−(ti+1 −ti
)}]
= lim
n→∞
n−1
∑
i=0
𝔼
[
𝔼
[
f(Wti, ti)2
{(
Wti+1 −Wti
)2
−(ti+1 −ti
)}|||||
ℱti
]]
= lim
n→∞
n−1
∑
i=0
𝔼
[
𝔼
[
f(Wti, ti)2 {
W2
ti+1 −2Wti+1Wti + W2
ti −ti+1 + ti
}||||
ℱti
]]
= lim
n→∞
n−1
∑
i=0
{
𝔼
[
𝔼
[
f(Wti, ti)2 (
W2
ti+1 −ti+1
)||||
ℱti
]
−2𝔼
[
f(Wti, ti)2Wti+1Wti
|||ℱti
]
+ 𝔼
[
f(Wti, ti)2 (
W2
ti + ti
)||||
ℱti
]]}
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Wti, ti)2 (
W2
ti −ti −2W2
ti + W2
ti + ti
)]
= 0.
Therefore, 𝔼
[(
∫
t
0
f(Ws, s) dWs
)2]
= 𝔼
[
∫
t
0
f(Ws, s)2ds
]
.
◽
12. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
The stochastic It¯o integral with respect to a standard Wiener process can be defined as
It = ∫
t
0
f(Ws, s) dWs
where f is a simple process. Show that the It¯o integral has quadratic variation process
⟨I, I⟩t given by
⟨I, I⟩t = ∫
t
0
f(Ws, s)2 ds.
Solution: By definition,
⟨I, I⟩t = lim
m→∞
m−1
∑
k=0
(
Itk+1 −Itk
)2
where tk = kt∕m, 0 = t0 < t1 < t2 < . . . < tm−1 < tm = t, m ∈ℕ. We first concentrate on
one of the subintervals [tk, tk+1) on which f(Ws, s) = f(Wtk, tk) is a constant value for all
s ∈[tk, t+1). Partitioning
tk = s0 < s1 < . . . < sn = tk+1,
we can write
Isi+1 −Isi = ∫
si+1
si
f(Wtk, tk) dWu = f(Wtk, tk)
(
Wsi+1 −Wsi
)
.

3.2.1
It¯o Calculus
115
Therefore,
(Itk+1 −Itk)2 = lim
n→∞
n−1
∑
i=0
(
Isi+1 −Isi
)2
= lim
n→∞
n−1
∑
i=0
[
f(Wtk, tk)
(
Wsi+1 −Wsi
)]2
= f(Wtk, tk)2 lim
n→∞
n−1
∑
i=0
(
Wsi+1 −Wsi
)2
= f(Wtk, tk)2 (tk+1 −tk
)
since limn→∞
∑n−1
i=0
(
Wsi+1 −Wsi
)2
converges to the quadratic variation of a standard
Wiener process over [tk, tk+1) which is tk+1 −tk. Therefore,
(Itk+1 −Itk)2 = f(Wtk, tk)2 (tk+1 −tk
) = ∫
tk+1
tk
f(Ws, s)2 ds
where f(Ws, s) is a constant value for all s ∈[tk, tk+1).
Finally, to obtain the quadratic variation of the It¯o integral It,
⟨I, I⟩t = lim
m→∞
m−1
∑
k=0
(
Itk+1 −Itk
)2
= lim
m→∞
m−1
∑
k=0
f(Wtk, tk)2 (tk+1 −tk
) = ∫
t
0
f(Ws, s)2 ds.
N.B. In differential form we can write d⟨I, I⟩t = dItdIt = f(Wt, t)2dt. By comparing the
results of the quadratic variation ⟨I, I⟩t and 𝔼(I2
t ), we can see that the former is computed
path-by-path and hence the result is random. In contrast, the variance of the It¯o integral,
Var(It) = 𝔼(I2
t ) = 𝔼(⟨I, I⟩t), is the mean value of all possible paths of the quadratic varia-
tion and hence is non-random.
◽
13. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Using integration by parts, show that
∫
t
0
Ws ds = ∫
t
0
(t −s) dWs
and prove that ∫
t
0
Ws ds ∼𝒩
(
0, t3
3
)
.
Is
Bt =
⎧
⎪
⎨
⎪⎩
0
t = 0
√
3
t
∫
t
0
Ws ds
t > 0
a standard Wiener process?

116
3.2.1
It¯o Calculus
Solution: Using integration by parts,
∫u
(d𝑣
ds
)
ds = u𝑣−∫𝑣
(du
ds
)
ds.
We set u = Ws and d𝑣∕ds = 1. Therefore,
∫
t
0
Ws ds = sWs|||
t
0 −∫
t
0
s dWs
= tWt −∫
t
0
s dWs
= ∫
t
0
(t −s) dWs.
Taking expectations,
𝔼
(
∫
t
0
Ws ds
)
= 𝔼
(
∫
t
0
(t −s) dWs
)
= 0
𝔼
[(
∫
t
0
Ws ds
)2]
= 𝔼
(
∫
t
0
(t −s)2 ds
)
= t3
3 .
To show that ∫
t
0
Ws ds follows a normal distribution, let
Mt = ∫
t
0
Ws ds = ∫
t
0
(t −s) dWs
and using the properties of the It¯o integral (see Problems 3.2.1.7, page 108 and 3.2.1.12,
page 114), we can deduce that
Mt is a martingale
and
⟨M, M⟩t = ∫
t
0
(t −s)2 ds = t3
3
so that dMt ⋅dMt = t2dt.
By defining
Zt = e
𝜃Mt−1
2 𝜃2
(
t3
3
)
,
𝜃∈ℝ
then expanding dZt using Taylor’s theorem and applying It¯o’s lemma, we have
dZt = 𝜕Zt
𝜕t dt + 𝜕Zt
𝜕Mt
dMt + 1
2
𝜕2f
𝜕M2
t
(dMt)2 + . . .
= −1
2𝜃2t2ZTdt + 𝜃ZtdMt + 1
2𝜃2Zt(dMt)2 + . . .
= −1
2𝜃2t2ZTdt + 𝜃ZtdMt + 1
2𝜃2t2Ztdt
= 𝜃ZtdMt.

3.2.1
It¯o Calculus
117
Taking integrals, we can express
∫
t
0
dZu = ∫
t
0
𝜃Zu dMu
Zt −Z0 = 𝜃∫
t
0
Zu dMu
Zt = 1 + 𝜃∫
t
0
Zu dMu.
Finally, by taking expectations and knowing that Mt is a martingale, we have
𝔼(Zt) = 1 + 𝜃𝔼
(
∫
t
0
Zu dMu
)
= 1
and hence
𝔼(e𝜃Mt) = e
1
2 𝜃2
(
t3
3
)
which is the moment generating function of a normal distribution with mean zero and
variance t3
3 . Thus, ∫
t
0
Ws ds ∼𝒩
(
0, t3
3
)
.
Even though Bt ∼𝒩(0, t) for t > 0, Bt is not a standard Wiener process since for t, u > 0,
𝔼(Bt+u −Bt) = 𝔼(Bt+u) −𝔼(Bt)
= 𝔼
( √
3
t + u ∫
t+u
0
Ws ds
)
−𝔼
(√
3
t
∫
t
0
Ws ds
)
= 0
and using the result of Problem 2.2.1.13 (page 66),
Var(Bt+u −Bt) = Var(Bt+u) + Var(Bt) −2Cov(Bt+u, Bt)
= Var
( √
3
t + u ∫
t+u
0
Ws ds
)
+ Var
(√
3
t
∫
t
0
Ws ds
)
−2Cov
( √
3
t + u ∫
t+u
0
Ws ds,
√
3
t
∫
t
0
Ws ds
)
= t + u + t −
6
t(t + u)
[1
3t3 + 1
2ut2]
= 2t + u −t(2t + 3u)
t + u
=
u2
t + u
≠u

118
3.2.1
It¯o Calculus
which shows that Bt does not have the stationary increment property. Therefore, Bt is not
a standard Wiener process.
◽
14. Generalised It¯o Integral. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a
standard Wiener process. Given that f is a simple process, show
∫
t
0
f(Ws, s) dWs = Wt f(Wt, t) −∫
t
0
[
Ws
𝜕f
𝜕t (Ws, s) + 𝜕f
𝜕Wt
(Ws, s) + 1
2Ws
𝜕2f
𝜕W2
t
(Ws, s)
]
ds
−∫
t
0
Ws
𝜕f
𝜕Wt
(Ws, s) dWs
and
∫
t
0
f(Ws, s) ds = tf(Wt, t) −∫
t
0
s
[
𝜕f
𝜕t (Ws, s) + 1
2
𝜕2f
𝜕W2
t
(Ws, s)
]
ds
−∫
t
0
s 𝜕f
𝜕Wt
(Ws, s) dWs.
Solution: For the first result, using Taylor’s theorem on d(Wt f(Wt, t)) and subsequently
applying It¯o’s formula we have
d(Wt f(Wt, t)) = Wt
𝜕f
𝜕t (Wt, t) dt +
[
f(Wt, t) + Wt
𝜕f
𝜕Wt
(Wt, t)
]
dWt
+1
2
[
𝜕f
𝜕Wt
(Wt, t) + 𝜕f
𝜕Wt
(Wt, t) + Wt
𝜕2f
𝜕W2
t
(Wt, t)
]
dW2
t + . . .
= Wt
𝜕f
𝜕t (Wt, t) dt +
[
f(Wt, t) + Wt
𝜕f
𝜕Wt
(Wt, t)
]
dWt
+1
2
[
2 𝜕f
𝜕Wt
(Wt, t) + Wt
𝜕2f
𝜕W2
t
(Wt, t)
]
dt
=
[
Wt
𝜕f
𝜕t (Wt, t) + 𝜕f
𝜕Wt
(Wt, t) + 1
2Wt
𝜕2f
𝜕W2
t
(Wt, t)
]
dt
+
[
f(Wt, t) + Wt
𝜕f
𝜕Wt
(Wt, t)
]
dWt.
Taking integrals from 0 to t,
∫
t
0
d(Wsf(Ws, s)) = ∫
t
0
[
Ws
𝜕f
𝜕t (Ws, s) + 𝜕f
𝜕Wt
(Ws, s) + 1
2Ws
𝜕2f
𝜕W2
t
(Ws, s)
]
ds
+ ∫
t
0
[
f(Ws, s) + Ws
𝜕f
𝜕Wt
(Ws, s)
]
dWs

3.2.1
It¯o Calculus
119
and rearranging the terms, finally
∫
t
0
f(Ws, s) dWs = Wt f(Wt, t) −∫
t
0
[
Ws
𝜕f
𝜕t (Ws, s) + 𝜕f
𝜕Wt
(Ws, s) + 1
2Ws
𝜕2f
𝜕W2
t
(Ws, s)
]
ds
−∫
t
0
Ws
𝜕f
𝜕Wt
(Ws, s) dWs
since W0 = 0.
As for the second result, from Taylor’s theorem and It¯o’s formula
d(tf(Wt, t)) = f(Wt, t) dt + t𝜕f
𝜕t (Wt, t) dt + t 𝜕f
𝜕Wt
(Wt, t) dWt + 1
2t 𝜕2f
𝜕W2
t
(Wt, t) dW2
t + . . .
= f(Wt, t) dt + t𝜕f
𝜕t (Wt, t) dt + t 𝜕f
𝜕Wt
(Wt, t) dWt + 1
2t 𝜕2f
𝜕W2
t
(Wt, t) dt
=
[
f(Wt, t) + t𝜕f
𝜕t (Wt, t) + 1
2t 𝜕2f
𝜕W2
t
(Wt, t)
]
dt + t 𝜕f
𝜕Wt
(Wt, t) dWt.
Taking integrals from 0 to t we have
∫
t
0
d(sf(Ws, s)) = ∫
t
0
[
f(Ws, s) + s𝜕f
𝜕t (Ws, s) + 1
2s 𝜕2f
𝜕W2
t
(Ws, s)
]
ds
+ ∫
t
0
s 𝜕f
𝜕Wt
(Ws, s) dWs
and hence
∫
t
0
f(Ws, s) ds = tf(Wt, t) −∫
t
0
s
[
𝜕f
𝜕t (Ws, s) + 1
2
𝜕2f
𝜕W2
t
(Ws, s)
]
ds
−∫
t
0
s 𝜕f
𝜕Wt
(Ws, s) dWs.
◽
15. One-Dimensional Lévy Characterisation Theorem. Let (Ω, ℱ, ℙ) be a probability space
and let {Mt ∶t ≥0} be a martingale with respect to the filtration ℱt, t ≥0. By assuming
M0 = 0, Mt has continuous sample paths whose quadratic variation
lim
n→∞
n−1
∑
i=0
(
Mti+1 −Mti
)2
= t
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕshow that Mt is a standard
Wiener process.

120
3.2.1
It¯o Calculus
Solution: We first need to show that Mt ∼𝒩(0, t) or, using the moment generating func-
tion approach, we need to show that 𝔼(e𝜃Mt) = e
1
2 𝜃t2 for a constant 𝜃.
Let f(Mt, t) = e𝜃Mt−1
2 𝜃2t and since dMt ⋅dMt = dt and (dt)𝜈= 0, 𝜈≥2 from It¯o’s formula,
df(Mt, t) = 𝜕f
𝜕t dt + 𝜕f
𝜕Mt
dMt + 1
2
𝜕2f
𝜕M2
t
(dMt)2 + . . .
=
(
𝜕f
𝜕t + 1
2
𝜕2f
𝜕M2
t
)
dt + 𝜕f
𝜕Mt
dMt
=
(
−1
2𝜃2t + 1
2𝜃2t
)
f(Mt, t) dt + 𝜃f(Mt, t) dMt
= 𝜃f(Mt, t) dMt.
Taking integrals from 0 to t, and then taking expectations, we have
∫
t
0
df(Ms, s) = 𝜃∫
t
0
f(Ms, s) dMs
f(Mt, t) −f(M0, 0) = 𝜃∫
t
0
f(Ms, s) dMs
𝔼(f(Mt, t)) = 1 + 𝜃𝔼
[
∫
t
0
f(Ms, s) dMs
]
.
By definition of the stochastic It¯o integral we can write
𝔼
[
∫
t
0
f(Ms, s) dMs
]
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Mti, ti)
(
Mti+1 −Mti
)]
= lim
n→∞
n−1
∑
i=0
𝔼
[
𝔼
[
f(Mti, ti)
(
Mti+1 −Mti
) |||ℱti
]]
= lim
n→∞
n−1
∑
i=0
𝔼
[
f(Mti, ti)
(
Mti −Mti
)]
= 0
since {Mt}t≥0 is a martingale and hence
𝔼(f(Mt, t)) = 1
or
𝔼(e𝜃Mt) = e
1
2 𝜃2t
which is the moment generating function for the normal distribution with mean zero and
variance t. Therefore, Mt ∼𝒩(0, t).

3.2.1
It¯o Calculus
121
Since Mt is a martingale, for s < t
𝔼(Mt|| ℱs
) = 𝔼(Mt −Ms + Ms|| ℱs
)
= 𝔼(Mt −Ms|| ℱs
) + 𝔼(Ms|| ℱs
)
= 𝔼(Mt −Ms|| ℱs
) + Ms
= Ms.
Therefore, 𝔼(Mt −Ms|| ℱs
) = 𝔼(Mt −Ms
) = 0 and hence Mt −Ms ⟂⟂ℱs. So, we have
shown that Mt has the independent increment property.
Finally, to show that Mt has the stationary increment, for t > 0 and s > 0 we have
𝔼(Mt+s −Mt
) = 𝔼(Mt+s
) −𝔼(Mt
) = 0
and using the independent increment property of Mt
Var (Mt+s −Mt
) = Var (Mt+s
) + Var (Mt
) −2Cov (Mt+s, Mt
)
= t + s + t −2 [𝔼(Mt+sMt
) −𝔼(Mt+s
) 𝔼(Mt
)]
= 2t + s −2𝔼(Mt+sMt
)
= 2t + s −2𝔼
(
Mt
(
Mt+s −Mt
)
+ M2
t
)
= 2t + s −2𝔼(Mt
) 𝔼(Mt+s −Mt
) −2𝔼(M2
t
)
= 2t + s −2t
= s.
Therefore, Mt+s −Mt ∼𝒩(0, s).
Because M0 = 0 and also Mt has continuous sample paths with independent and stationary
increments, so Mt is a standard Wiener process.
◽
16. Multi-Dimensional Lévy Characterisation Theorem. Let (Ω, ℱ, ℙ) be a probability space
and let {M(1)
t
∶t ≥0}, {M(2)
t
∶t ≥0}, . . . , {M(n)
t
∶t ≥0} be martingales with respect to
the filtration ℱt, t ≥0. By assuming M(i)
0 = 0, M(i)
t
has continuous sample paths whose
quadratic variation
lim
n→∞
m
∑
k=0
(
M(i)
tk+1 −M(i)
tk
)2
= t
and cross-variation between M(i)
t
and M(j)
t , i ≠j, i, j = 1, 2, . . . , n
lim
m→∞
m
∑
k=0
(
M(i)
tk+1 −M(i)
tk
) (
M(j)
tk+1 −M(j)
tk
)
= 0
where tk = kt∕m, 0 = t0 < t1 < t2 < . . . < tm−1 < tm = t, m ∈ℕ, show that M(1)
t ,
M(2)
t , . . . , M(n)
t
are independent standard Wiener processes.

122
3.2.1
It¯o Calculus
Solution: Following
Problem
3.2.1.15
(page
119),
we
can
easily
prove
that
M(1)
t , M(2)
t , . . . , M(n)
t
are standard Wiener processes. In order to show M(1)
t , M(2)
t , . . . , M(n)
t
are mutually independent, we need to show the joint moment generating function of
M(1)
t , M(2)
t , . . . ,
M(n)
t
is
𝔼
(
e𝜃(1)M(1)
t
+𝜃(2)M(2)
t
+ ... 𝜃(n)M(n)
t
)
= e
1
2 (𝜃(1))2t ⋅e
1
2 (𝜃(2))2t · · · e
1
2 (𝜃(n))2t
for constants 𝜃(1), 𝜃(2), . . . , 𝜃(n).
Let f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = ∏n
i=1 e𝜃(i)M(i)
t −1
2 (𝜃(i))2t and since dM(i)
t ⋅dM(i)
t
= dt,
dM(i)
t
⋅dM(j)
t
= 0, i, j = 1, 2, . . . , n, i ≠j and (dt)𝜈= 0, 𝜈≥2, from It¯o’s formula
df(M(1)
t , M(2)
t , . . . , M(n)
t , t) = 𝜕f
𝜕t dt +
n
∑
i=1
𝜕f
𝜕M(i)
t
dM(i)
t
+1
2
n
∑
i=1
n
∑
j=1
𝜕2f
𝜕M(i)
t 𝜕M(j)
t
dM(i)
t dM(j)
t
= 𝜕f
𝜕t dt +
n
∑
i=1
𝜕f
𝜕M(i)
t
dM(i)
t
+ 1
2
n
∑
i=1
𝜕2f
𝜕(M(i)
t )2 (dM(i
t )2
=
(
𝜕f
𝜕t + 1
2
n
∑
i=1
𝜕2f
𝜕(M(i)
t )2
)
dt +
n
∑
i=1
𝜕f
𝜕M(i)
t
dM(i)
t
=
n
∑
i=1
𝜕f
𝜕M(i)
t
dM(i)
t
=
n
∑
i=1
𝜃(i)f(M(1)
t , M(2)
t , . . . , M(n)
t , t) dM(i)
t
since
𝜕f
𝜕t + 1
2
n
∑
i=1
𝜕2f
𝜕(M(i)
t )2 =
[
−1
2
n
∑
i=1
(𝜃(i))2 + 1
2
n
∑
i=1
(𝜃(i))2
]
f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = 0.
Integrating both sides from 0 to t and taking expectations, we have
∫
t
0
df(M(1)
s , M(2)
s , . . . , M(n)
s , s) =
n
∑
i=1 ∫
t
0
𝜃(i)f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
f(M(1)
t , M(2)
t , . . . , M(n)
t , t) = f(M(1)
0 , M(2)
0 , . . . , M(n)
0 , 0)
+
n
∑
i=1 ∫
t
0
𝜃(i)f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
𝔼[ f(M(1)
t , M(2)
t , . . . , M(n)
t , t)] = 1 +
n
∑
i=1
𝜃(i)𝔼
[
∫
t
0
f(M(1)
s , M(2)
s , . . . , M(n)
s , t) dM(i)
s
]
.

3.2.2
One-Dimensional Diffusion Process
123
Because M(i)
t
is a martingale, we can easily show (see Problem 3.2.1.15, page 119) that
𝔼
[
∫
t
0
f
(
M(1)
s , M(2)
s , . . . , M(n)
s , t
)
dM(i)
s
]
= 0
for i = 1, 2, . . . , n and hence
𝔼
(
e𝜃(1)M(1)
t
+𝜃(2)M(2)
t
+ ... 𝜃(n)M(n)
t
)
= e
1
2 (𝜃(1))2t ⋅e
1
2 (𝜃(2))2t · · · e
1
2 (𝜃(n))2t
where the joint moment generating function of M(1)
t , M(2)
t , . . . , M(n)
t
is a product of
moment generating functions of M(1)
t , M(2)
t , . . . , M(n)
t . Therefore, M(1)
t , M(2)
t , . . . , M(n)
t
are independent standard Wiener processes.
◽
3.2.2
One-Dimensional Diffusion Process
1. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Find the SDE for the random process Xt = Wn
t , n ∈ℤ+.
Show that
𝔼(Wn
t
) = 1
2n(n −1) ∫
t
0
𝔼
(
W(n−2)
s
)
ds
and using mathematical induction prove that
𝔼(Wn
t
) =
⎧
⎪
⎨
⎪⎩
n!t
n
2
2
n
2
(
n
2
)
!
n = 2, 4, 6, . . .
0
n = 1, 3, 5, . . .
Solution: By expanding dXt using Taylor’s formula and applying It¯o’s formula,
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕Wt
dWt + 1
2
𝜕2Xt
𝜕W2
t
dW2
t + . . .
dWn
t = nW(n−1)
t
dWt + 1
2n(n −1)W(n−2)
t
dt.
Taking integrals,
∫
t
0
dWn
s = ∫
t
0
nW(n−1)
s
dWs + 1
2n(n −1) ∫
t
0
W(n−2)
s
ds
Wn
t = ∫
t
0
nW(n−1)
s
dWs + 1
2n(n −1) ∫
t
0
W(n−2)
s
ds.
Finally, by taking expectations,
𝔼(Wn
t
) = 1
2n(n −1) ∫
t
0
𝔼
(
W(n−2)
s
)
ds.

124
3.2.2
One-Dimensional Diffusion Process
To prove the final result, we will divide it into two sections, one for even numbers n = 2k,
k ∈ℤ+ and another for odd numbers n = 2k + 1, k ∈ℤ+. We note that for n = 2 we have
𝔼(W2
t
) = 2!t
2 = t
and because Wt ∼𝒩(0, t), the result is true for n = 2.
We assume that the result is true for n = 2k, k ∈ℤ+. That is
𝔼(W2k
t
) = (2k)!tk
2kk! .
For n = 2(k + 1), k ∈ℤ+ we have
𝔼
(
W2(k+1)
t
)
= 1
2(2k + 2)(2k + 1) ∫
t
0
𝔼(W2k
s
) ds
= 1
2(2k + 2)(2k + 1) ∫
t
0
(2k)!sk
2kk!
ds
= (2k + 2)!
2k+1k!
∫
t
0
sk ds
= (2k + 2)!tk+1
2k+1(k + 1)!
=
(2(k + 1))!t
2(k+1)
2
2
2(k+1)
2
(2(k + 1)∕2)!
.
Thus, the result is also true for n = 2(k + 1), k ∈ℤ+.
For n = 1, we have
𝔼(Wt
) = 0
and because Wt ∼𝒩(0, t), the result is true for n = 1.
We assume the result is true for n = 2k + 1, k ∈ℤ+ such that
𝔼(W2k+1
t
) = 0.
For n = 2(k + 1) + 1, k ∈ℤ+
𝔼
(
W2(k+1)+1
t
)
= 1
2(2k + 3)(2k + 2) ∫
t
0
𝔼(W2k+1
s
) ds = 0
and hence the result is also true for n = 2(k + 1) + 1, k ∈ℤ+. Therefore, by mathematical
induction
𝔼(Wn
t
) =
⎧
⎪
⎨
⎪⎩
n!t
n
2
2
n
2
(
n
2
)
!
n = 2, 4, 6, . . .
0
n = 1, 3, 5, . . .
◽

3.2.2
One-Dimensional Diffusion Process
125
2. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
For a constant 𝜃find the SDE for the random process Xt = e𝜃Wt−1
2 𝜃2t.
By writing the SDE in integral form calculate 𝔼(e𝜃Wt), the moment generating function
of a standard Wiener process.
Solution: Expanding dXt using Taylor’s theorem and applying It¯o’s formula,
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕Wt
dWt + 1
2
𝜕2Xt
𝜕W2
t
dW2
t + . . .
=
(
𝜕Xt
𝜕t + 1
2
𝜕2Xt
𝜕W2
t
)
dt + 𝜕Xt
𝜕Wt
dWt
=
(
−1
2𝜃2Xt + 1
2𝜃2Xt
)
dt + 𝜃XtdWt
= 𝜃XtdWt.
Taking integrals, we have
∫
t
0
dXs = ∫
t
0
𝜃Xs dWs
Xt −X0 = ∫
t
0
𝜃Xs dWs.
Since X0 = 1 and taking expectations,
𝔼(Xt
) −1 = 𝔼
(
∫
t
0
𝜃Xs dWs
)
= 0
so
𝔼(Xt
) = 1
or
𝔼
(
e𝜃Wt−1
2 𝜃2t)
= 1.
Therefore, 𝔼(e𝜃Wt) = e
1
2 𝜃2t.
◽
3. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Consider the process
Zt = e𝜃Wt
where 𝜃is a constant parameter.
Using It¯o’s formula, find an SDE for Zt.
By setting mt = 𝔼(e𝜃Wt) show that the integrated SDE can be expressed as
dmt
dt −1
2𝜃2mt = 0.
Given W0 = 0, solve the first-order ordinary differential equation to find mt.

126
3.2.2
One-Dimensional Diffusion Process
Solution: Using It¯o’s formula we expand Zt as
dZt = 𝜕Zt
𝜕Wt
dWt + 1
2
𝜕2Zt
𝜕W2
t
(dWt)2 + . . .
= 𝜃e𝜃WtdWt + 1
2𝜃2e𝜃Wtdt
= 𝜃ZtdWt + 1
2𝜃2Ztdt.
Taking integrals and then expectations,
∫
t
0
dZs = ∫
t
0
𝜃Zs dWs + ∫
t
0
1
2𝜃2Zs ds
Zt −Z0 = ∫
t
0
𝜃Zs dWs + ∫
t
0
1
2𝜃2Zs ds
𝔼(Zt) −𝔼(Z0) = 𝔼
(
∫
t
0
𝜃Zs dWs
)
+ 𝔼
(
∫
t
0
1
2𝜃2Zs ds
)
𝔼(Zt) −1 = ∫
t
0
1
2𝜃2𝔼(Zs) ds
where Z0 = 1 and 𝔼
(
∫
t
0
𝜃Zs dWs
)
= 0. By differentiating the integral equation we have
d𝔼(Zt)
dt
= d
dt ∫
t
0
1
2𝜃2𝔼(Zs) ds
d𝔼(Zt)
dt
= 1
2𝜃2𝔼(Zt)
or
dmt
dt −1
2𝜃2mt = 0.
Setting the integrating factor as I = e−∫1
2 𝜃2dt = e−1
2 𝜃2t and multiplying the differential
equation with I, we have
d
dt
(
mte−1
2 𝜃2t)
= 0
or
e−1
2 𝜃2t𝔼(e𝜃Wt) = C
where C is a constant. Since 𝔼(e𝜃W0) = 1, so C = 1 and hence we finally obtain
𝔼
(
e𝜃Wt)
= e
1
2 𝜃2t.
◽
4. Let Mt = ∫
t
0
f(s) dWs and show that the SDE satisfied by
Xt = exp
{
𝜃Mt −1
2𝜃2
∫
t
0
f(s)2 ds
}
is
dXt = 𝜃f(t)XtdWt

3.2.2
One-Dimensional Diffusion Process
127
and show also that Mt ∼𝒩
(
0, ∫
t
0
f(s)2 ds
)
.
Solution: From It¯o’s formula,
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕Wt
dWt + 1
2
𝜕2Xt
𝜕W2
t
dW2
t + . . .
= 𝜕Xt
𝜕t dt +
( 𝜕Xt
𝜕Mt
⋅𝜕Mt
𝜕Wt
)
dWt + 1
2
𝜕
𝜕Wt
( 𝜕Xt
𝜕Mt
⋅𝜕Mt
𝜕Wt
)
dt
=
[𝜕Xt
𝜕t + 1
2
𝜕
𝜕Wt
( 𝜕Xt
𝜕Mt
⋅𝜕Mt
𝜕Wt
)]
dt +
( 𝜕Xt
𝜕Mt
⋅𝜕Mt
𝜕Wt
)
dWt.
Since
𝜕Xt
𝜕Mt
= 𝜃exp
{
𝜃Mt −1
2 ∫
t
0
f(s)2 ds
}
= 𝜃Xt,
𝜕Mt
𝜕Wt
= f(t)
𝜕
𝜕Wt
( 𝜕Xt
𝜕Mt
⋅𝜕Mt
𝜕Wt
)
= 𝜃2f(t)2Xt,
𝜕Xt
𝜕t = −1
2𝜃2f(t)2Xt
so
dXt = 𝜃f(t)XtdWt.
Writing in integral form and then taking expectations,
∫
t
0
dXs = ∫
t
0
𝜃f(s)Xs dWs
Xt −X0 = ∫
t
0
𝜃f(s)Xs dWs
𝔼(Xt
) −𝔼(X0
) = 𝔼
(
∫
t
0
𝜃f(s)Xs dWs
)
= 0
𝔼(Xt
) = 𝔼(X0
) = 1.
Therefore,
𝔼(e𝜃Mt) = exp
(
1
2𝜃2
∫
t
0
f(s)2 ds
)
which is the moment generating function for a normal distribution with mean zero and
variance ∫
t
0
f(s)2 ds. Hence Mt ∼𝒩
(
0, ∫
t
0
f(s)2 ds
)
.
◽
5. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Suppose Xt follows the generalised SDE
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
where 𝜇and 𝜎are functions of Xt and t. Show that Xt is a martingale if 𝜇(Xt, t) = 0.

128
3.2.2
One-Dimensional Diffusion Process
Solution: It suffices to show that under the filtration ℱs where s < t, for Xt to be a mar-
tingale
𝔼(Xt|ℱs
) = Xs.
By taking integrals of the SDE,
∫
t
s
dX𝑣= ∫
t
s
𝜇(X𝑣, t) d𝑣+ ∫
t
s
𝜎(X𝑣, 𝑣) dW𝑣
Xt −Xs = ∫
t
s
𝜇(X𝑣, t) d𝑣+ ∫
t
s
𝜎(X𝑣, 𝑣) dW𝑣.
Taking expectations,
𝔼(Xt −Xs
) = 𝔼
[
∫
t
s
𝜇(X𝑣, t) d𝑣
]
or
𝔼(Xt
) = 𝔼(Xs
) + 𝔼
[
∫
t
s
𝜇(X𝑣, t) d𝑣
]
.
Under the filtration ℱs, s < t
𝔼(Xt|ℱs
) = 𝔼(Xs|ℱs
) + 𝔼
[
∫
t
s
𝜇(X𝑣, t) d𝑣
|||||
ℱs
]
= Xs + 𝔼
[
∫
t
s
𝜇(X𝑣, t) d𝑣
|||||
ℱs
]
.
Therefore, if 𝜇(Xt, t) = 0 then Xt is a martingale.
◽
6. Bachelier Model (Arithmetic Brownian Motion). Let (Ω, ℱ, ℙ) be a probability space and
let {Wt ∶t ≥0} be a standard Wiener process. Suppose Xt follows the arithmetic Brown-
ian motion with SDE
dXt = 𝜇dt + 𝜎dWt
where 𝜇and 𝜎are constants. Taking integrals show that for t < T,
XT = Xt + 𝜇(T −t) + 𝜎WT−t
where WT−t = WT −Wt ∼𝒩(0, T −t). Deduce that XT, given Xt = x, follows a normal
distribution with mean
𝔼(XT|Xt = x) = x + 𝜇(T −t)
and variance
Var (XT|Xt = x) = 𝜎2(T −t).

3.2.2
One-Dimensional Diffusion Process
129
Solution: Taking integrals of dXt = 𝜇dt + 𝜎dWt we have
∫
T
t
dXs = ∫
T
t
𝜇ds + ∫
T
t
𝜎dWs
XT −Xt = 𝜇(T −t) + 𝜎(WT −Wt)
or
XT = Xt + 𝜇(T −t) + 𝜎WT−t
where WT−t ∼𝒩(0, T −t). Since Xt, 𝜇and 𝜎are deterministic components therefore XT,
given Xt = x follows a normal distribution with mean x + 𝜇(T −t) and variance 𝜎2(T −t).
◽
7. Black–Scholes Model (Geometric Brownian Motion). Let (Ω, ℱ, ℙ) be a probability space
and let {Wt ∶t ≥0} be a standard Wiener process. Suppose Xt follows the geometric
Brownian motion with SDE
dXt = 𝜇Xtdt + 𝜎XtdWt
where 𝜇and 𝜎are constants. By applying It¯o’s formula to Yt = log Xt and taking integrals
show that for t < T,
XT = Xte
(
𝜇−1
2 𝜎2)
(T−t)+𝜎WT−t
where WT−t ∼𝒩(0, T −t). Deduce that XT, given Xt = x follows a lognormal distribution
with mean
𝔼(XT|Xt = x) = xe𝜇(T−t)
and variance
Var (XT|Xt = x) = x2 (
e𝜎2(T−t) −1
)
e2𝜇(T−t).
Solution: From Taylor’s expansion and subsequently using It¯o’s formula,
d(log Xt) = 1
Xt
dXt −
1
2X2
t
(dXt)2 + . . .
= 𝜇dt + 𝜎dWt −
1
2X2
t
(𝜎2X2
t dt)
=
(
𝜇−1
2𝜎2)
dt + 𝜎dWt.
Taking integrals,
∫
T
t
d(log Xu) = ∫
T
t
(
𝜇−1
2𝜎2)
du + ∫
T
t
𝜎dWu
log XT −log Xt =
(
𝜇−1
2𝜎2)
(T −t) + 𝜎(WT −Wt)
XT = Xte
(
𝜇−1
2 𝜎2)
(T−t)+𝜎WT−t

130
3.2.2
One-Dimensional Diffusion Process
where WT −Wt = WT−t ∼𝒩(0, T −t). Therefore,
XT ∼log-𝒩
[
log Xt +
(
𝜇−1
2𝜎2)
(T −t), 𝜎2(T −t)
]
and from Problem 1.2.2.9 (page 20) the mean and variance of XT, given Xt = x are
𝔼(XT|Xt = x) = e
log x+
(
𝜇−1
2 𝜎2)
(T−t)+ 1
2 𝜎2(T−t)
= xe𝜇(T−t)
and
Var (XT|Xt = x) =
(
e𝜎2(T−t) −1
)
e
2 log x+2
(
𝜇−1
2 𝜎2)
(T−t)+𝜎2(T−t)
= x2 (
e𝜎2(T−t) −1
)
e2𝜇(T−t)
respectively.
◽
8. Generalised Geometric Brownian Motion. Let (Ω, ℱ, ℙ) be a probability space and let
{Wt ∶t ≥0} be a standard Wiener process. Suppose Xt follows the generalised geometric
Brownian motion with SDE
dXt = 𝜇tXtdt + 𝜎tXtdWt
where 𝜇t and 𝜎t are time dependent. By applying It¯o’s formula to Yt = log Xt and taking
integrals show that for t < T,
XT = Xt exp
{
∫
T
t
(
𝜇s −1
2𝜎2
s
)
ds + ∫
T
t
𝜎sdWs
}
.
Deduce that XT, given Xt = x follows a lognormal distribution with mean
𝔼(XT|Xt = x) = xe∫T
t
𝜇sds
and variance
Var (XT|Xt = x) = x2 (
e∫T
t
𝜎2
s ds −1
)
e2 ∫T
t
𝜇sds.
Solution: From Taylor’s expansion and using It¯o’s formula,
d(log Xt) = 1
Xt
dXt −
1
2X2
t
(dXt)2 + . . .
= 𝜇tdt + 𝜎dWt −
1
2X2
t
(𝜎2
t X2
t dt)
=
(
𝜇t −1
2𝜎2
t
)
dt + 𝜎tdWt.

3.2.2
One-Dimensional Diffusion Process
131
Taking integrals,
∫
T
t
d(log Xs) = ∫
T
t
(
𝜇s −1
2𝜎2
s
)
ds + ∫
T
t
𝜎s dWs
log XT −log Xt = ∫
T
t
(
𝜇s −1
2𝜎s
2)
ds + ∫
T
t
𝜎s dWs
XT = Xt exp
{
∫
T
t
(
𝜇s −1
2𝜎2
s
)
ds + ∫
T
t
𝜎dWs
}
.
Thus,
XT ∼log-𝒩
[
log Xt + ∫
T
t
(
𝜇s −1
2𝜎2
s
)
ds, ∫
T
t
𝜎2
s ds
]
and from Problem 1.2.2.9 (page 20) we have mean
𝔼(XT|Xt = x) = e
log x+∫T
t
(
𝜇s−1
2 𝜎2
s
)
ds+ 1
2 ∫T
t
𝜎2
s ds
= xe∫T
t 𝜇sds
and variance
Var (XT|Xt = x) =
(
e∫T
t
𝜎2
s ds −1
)
e
2
(
log x+∫T
t
(
𝜇s−1
2 𝜎2
s
)
ds
)
+∫T
t
𝜎2
s ds
= x2 (
e∫T
t
𝜎2
s ds −1
)
e2 ∫T
t
𝜇sds.
◽
9. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Suppose Xt follows the geometric Brownian motion with SDE
dXt = 𝜇Xtdt + 𝜎XtdWt
where 𝜇and 𝜎are constants. Show that if Yt = Xn
t , for some constant n, then Yt follows
the geometric Brownian process of the form
dYt
Yt
= n
(
𝜇+ 1
2(n −1)𝜎2)
dt + n𝜎dWt.
Deduce that given Yt, t < T, YT follows a lognormal distribution with the form
YT = Yte
n
(
𝜇−1
2 𝜎2)
(T−t)+n𝜎WT−t
where WT−t ∼𝒩(0, T −t) with mean
𝔼(YT|Yt = y) = ye
n
(
𝜇+ 1
2 (n−1)𝜎2)
(T−t)

132
3.2.2
One-Dimensional Diffusion Process
and variance
Var (YT|Yt = y) = y2(en2𝜎2(T−t) −1)e
2n
(
𝜇+ 1
2 (n−1)𝜎2)
(T−t).
Solution: From It¯o’s formula,
dYt = nXn−1
t
dXt + 1
2n(n −1)Xn−2
t
dX2
t
= nXn−1
t
(𝜇Xtdt + 𝜎XtdWt) + 1
2n(n −1)𝜎2Xn
t dt
= n
(
𝜇+ 1
2(n −1)𝜎2)
Xn
t dt + n𝜎Xn
t dWt.
By substituting Xn
t = Yt we have
dYt
Yt
= n
(
𝜇+ 1
2(n −1)𝜎2)
dt + n𝜎dWt.
Since Yt follows a geometric Brownian motion, by analogy with Problem 3.2.2.7 (page
129) and by setting 𝜇←n
(
𝜇+ 1
2(n −1)𝜎2)
and 𝜎←n𝜎, we can easily show that
YT = Yte
n
(
𝜇−1
2 𝜎2)
(T−t)+n𝜎WT−t
follows a lognormal distribution where WT−t ∼𝒩(0, T −t). In addition, we can also
deduce
𝔼(YT|Yt = y) = ye
n
(
𝜇+ 1
2 (n−1)𝜎2)
(T−t)
and
Var (YT|Yt = y) = y2 (
en2𝜎2(T−t) −1
)
e
2n
(
𝜇+ 1
2 (n−1)𝜎2)
(T−t).
◽
10. Ornstein–Uhlenbeck Process. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0}
be a standard Wiener process. Suppose Xt follows the Ornstein–Uhlenbeck process with
SDE
dXt = 𝜅(𝜃−Xt) dt + 𝜎dWt
where 𝜅, 𝜃and 𝜎are constants. By applying It¯o’s formula to Yt = e𝜅tXt and taking integrals
show that for t < T,
XT = Xte−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−s)dWs.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of XT, given Xt = x.
Deduce that XT follows a normal distribution.

3.2.2
One-Dimensional Diffusion Process
133
Solution: Expanding Yt = e𝜅tXt using Taylor’s formula and applying It¯o’s formula,
we have
d(e𝜅tXt) = 𝜅e𝜅tXtdt + e𝜅tdXt + 1
2𝜅2e𝜅tXt(dt)2 + . . .
= 𝜅e𝜅tXtdt + e𝜅t(𝜅(𝜃−Xt) dt + 𝜎dWt)
= 𝜅𝜃e𝜅tdt + 𝜎e𝜅tdWt.
Integrating the above expression,
∫
T
t
d(e𝜅tXs) = ∫
T
t
𝜅𝜃e𝜅sds + ∫
T
t
𝜎e𝜅sdWs
XT = Xte−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−s)dWs.
Given the fact that
𝔼
(
∫
T
t
𝜎e−𝜅(T−s)dWs
)
= 0
and
𝔼
[(
∫
T
t
𝜎e−𝜅(T−s)dWs
)2]
= 𝔼
(
∫
T
t
𝜎2e−2𝜅(T−s)ds
)
= 𝜎2
2𝜅
[1 −e−2𝜅(T−t)]
the mean and variance of XT, given Xt = x are
𝔼(XT|Xt = x) = xe−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)]
and
Var (XT|Xt = x) = 𝜎2
2𝜅
[1 −e−2𝜅(T−t)]
respectively.
Since ∫
T
t
𝜎e−𝜅(T−s)dWs can be written in the form
∫
T
t
𝜎e−𝜅(T−s)dWs = lim
n→∞
n−1
∑
i=0
𝜎e−𝜅(T−ti) (
Wti+1 −Wti
)
where ti = t + i(T −t)∕n, t = t0 < t1 < t2 < . . . < tn−1 < tn = T, n ∈ℕthen due to the
stationary increment of a standard Wiener process, we can see that each term of Wti+1 −
Wti ∼𝒩
(
0, T −t
n
)
is normal multiplied by a deterministic exponential term. Thus, the
product is normal and given that the sum of normal variables is normal we can deduce
XT ∼𝒩
(
xe−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] , 𝜎2
2𝜅
[1 −e−2𝜅(T−t)])
.
◽

134
3.2.2
One-Dimensional Diffusion Process
11. Geometric Mean-Reverting Process. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶
t ≥0} be a standard Wiener process. Suppose Xt follows the geometric mean-reverting
process with SDE
dXt = 𝜅(𝜃−log Xt)Xtdt + 𝜎XtdWt,
X0 > 0
where 𝜅, 𝜃and 𝜎are constants. By applying It¯o’s formula to Yt = log Xt show that the
diffusion process can be reduced to an Ornstein–Uhlenbeck process of the form
dYt =
[
𝜅(𝜃−Yt) −1
2𝜎2]
dt + 𝜎dWt.
Show also that for t < T,
log XT = (log Xt)e−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) (1 −e−𝜅(T−t)) + ∫
T
t
𝜎e−𝜅(T−s)dWs.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of XT, given Xt = x and deduce that XT follows a lognormal distribution.
Solution: By expanding Yt = log Xt using Taylor’s formula and subsequently applying
It¯o’s formula, we have
d(log Xt) = 1
Xt
dXt −
1
2X2
t
(dXt)2 + . . .
= 𝜅(𝜃−log Xt) dt + 𝜎dWt −1
2𝜎2dt
=
(
𝜅(𝜃−log Xt
) −1
2𝜎2)
dt + 𝜎dWt
and hence
dYt =
(
𝜅(𝜃−Yt) −1
2𝜎2)
dt + 𝜎dWt.
Using the same steps in solving the Ornstein–Uhlenbeck process, we apply It¯o’s formula
on Zt = e𝜅Yt such that
d(e𝜅tYt) = 𝜅e𝜅tYtdt + e𝜅tdYt + 1
2𝜅2e𝜅tYt(dt)2 + . . .
= 𝜅e𝜅tYtdt + e𝜅t [(
𝜅(𝜃−Yt) −1
2𝜎2)
dt + 𝜎dWt
]
=
(
𝜅𝜃e𝜅t −1
2𝜎2e𝜅t)
dt + 𝜎e𝜅tdWt.
Taking integrals from t to T, we have
∫
T
t
d(e𝜅sYs) = ∫
T
t
(
𝜅𝜃e𝜅s −1
2𝜎2e𝜅s)
ds + ∫
T
t
𝜎e𝜅sdWs

3.2.2
One-Dimensional Diffusion Process
135
or
YT = Yte−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) (1 −e−𝜅(T−t)) + ∫
T
t
𝜎e−𝜅(T−s)dWs.
By analogy with Problem 3.2.2.10 (page 132), we can deduce that YT = log XT follows a
normal distribution and hence
log XT ∼𝒩
(
log Xt
(e−𝜅(T−t)) +
(
𝜃−𝜎2
2𝜅
) (1 −e−𝜅(T−t)) , 𝜎2
2𝜅
(1 −e−2𝜅(T−t)))
or
XT ∼log −𝒩
(
log Xt
(e−𝜅(T−t)) +
(
𝜃−𝜎2
2𝜅
) (1 −e−𝜅(T−t)) , 𝜎2
2𝜅
(1 −e−2𝜅(T−t)))
with mean
𝔼(XT|Xt = x) = exp
{
e−𝜅(T−t) log x +
(
𝜃−𝜎2
2𝜅
)
(1 −e−𝜅(T−t)) + 𝜎2
4𝜅(1 −e−2𝜅(T−t))
}
and variance
Var (XT|Xt = x) = exp
{
𝜎2
2𝜅
(1 −e−2𝜅(T−t)) −1
}
× exp
{
2e−𝜅(T−t) log x + 2
(
𝜃−𝜎2
2𝜅
) (1 −e−𝜅(T−t)) +
𝜎2
2𝜅
(1 −e−2𝜅(T−t))}
.
◽
12. Cox–Ingersoll–Ross (CIR) Model. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥
0} be a standard Wiener process. Suppose Xt follows the CIR model with SDE
dXt = 𝜅(𝜃−Xt
) dt + 𝜎
√
XtdWt,
X0 > 0
where 𝜅, 𝜃and 𝜎are constants. By applying It¯o’s formula to Zt = e𝜅tXt and Z2
t = e2𝜅tX2
t
and taking integrals show that for t < T,
XT = Xte−𝜅(T−t) + 𝜃
[
1 −e−𝜅(T−t)]
+ ∫
T
t
𝜎e−𝜅(T−s)√
Xs dWs
and
X2
T = X2
t e−2𝜅(T−t) + (2𝜅𝜃+ 𝜎2)
∫
T
t
e−2𝜅(T−s)Xs ds + 2𝜎∫
T
t
e−2𝜅(T−s)X
3
2
s dWs.
Using the properties of stochastic integrals on the above two expressions, find the mean
and variance of XT, given Xt = x.

136
3.2.2
One-Dimensional Diffusion Process
Solution: Using Taylor’s formula and applying It¯o’s formula on Zt = e𝜅tXt, we have
d (e𝜅tXt
) = 𝜅e𝜅tXtdt + e𝜅tdXt + 1
2𝜅2e𝜅tXt(dt)2 + . . .
= 𝜅e𝜅tXtdt + e𝜅t (
𝜅(𝜃−Xt
) dt + 𝜎
√
XtdWt
)
= 𝜅𝜃e𝜅tdt + 𝜎e𝜅t√
XtdWt.
Integrating the above expression,
∫
T
t
d (e𝜅tXs
) = ∫
T
t
𝜅𝜃e𝜅s ds + ∫
T
t
𝜎e𝜅s√
Xs dWs
and therefore
XT = Xte−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−s)√
Xs dWs.
For the case of Z2
t = e2𝜅tX2
t , by the application of Taylor’s expansion and It¯o’s formula
we have
d (e2𝜅tX2
t
) = 2𝜅e2𝜅tX2
t dt + 2e2𝜅tXtdXt + 1
2
(4𝜅2) e2𝜅tX2
t (dt)2 + 1
2
(2e2𝜅t) (dXt
)2 + . . .
= 2𝜅e2𝜅tX2
t dt + 2e2𝜅tXt
(
𝜅(𝜃−Xt
) dt + 𝜎
√
XtdWt
)
+ e2𝜅t (𝜎2Xtdt)
= e2𝜅t (2𝜅𝜃+ 𝜎2) Xtdt + 2𝜎e2𝜅tX
3
2
t dWt.
By taking integrals,
∫
T
t
d (e2𝜅sX2
s
) = (2𝜅𝜃+ 𝜎2)
∫
T
t
e2𝜅sXs ds + 2𝜎∫
T
t
e2𝜅sX
3
2
s dWs
and we eventually obtain the following expression:
X2
T = e−2𝜅(T−t)X2
t + (2𝜅𝜃+ 𝜎2)
∫
T
t
e−2𝜅(T−s)Xs ds + 2𝜎∫
T
t
e−2𝜅(T−s)X
3
2
s dWs.
Given Xt = x, and by taking the expectation of the expression XT, we have
𝔼(XT|Xt = x) = xe−𝜅(T−t) + 𝜃(1 −e−𝜅(T−t)) .
To find the variance, Var (XT|Xt = x) we first take the expectation of X2
T,
𝔼(X2
T|Xt = x) = x2e−2𝜅(T−t) + (2𝜅𝜃+ 𝜎2)
∫
T
t
e−2𝜅(T−s)𝔼(Xs|Xt = x) ds
= x2e−2𝜅(T−t)

3.2.2
One-Dimensional Diffusion Process
137
+ (2𝜅𝜃+ 𝜎2)
∫
T
t
e−2𝜅(T−s) [xe−𝜅(s−t) + 𝜃(1 −e−𝜅(s−t))] ds
= x2e−2𝜅(T−t) +
(
2𝜅𝜃+ 𝜎2
𝜅
)
(x −𝜃) (e−𝜅(T−t) −e−2𝜅(T−t))
+𝜃(2𝜅𝜃+ 𝜎2)
2𝜅
(1 −e−2𝜅(T−t)) .
Therefore,
Var (XT|Xt = x) = 𝔼(X2
T|Xt = x) −[𝔼(XT|Xt = x)]2
= x𝜎2
𝜅
(e−𝜅(T−t) −e−2𝜅(T−t)) + 𝜃𝜎2
2𝜅
(1 −2e−𝜅(T−t) + e−2𝜅(T−t)) .
◽
13. Brownian Bridge Process. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a
standard Wiener process. Suppose Xt follows the Brownian bridge process with SDE
dXt = y −Xt
1 −t dt + dWt,
X1 = y
where the diffusion is conditioned to be at y at time t = 1. By applying It¯o’s formula to
Yt = (y −Xt)∕(1 −t) and taking integrals show that under an initial condition X0 = x and
for 0 ≤t < 1,
Xt = yt + (1 −t)
(
x + ∫
t
0
1
1 −s dWs
)
.
Using the properties of stochastic integrals on the above expression, find the mean and
variance of Xt, given X0 = x and show that Xt follows a normal distribution.
Solution: By expanding Yt = (y −Xt)∕(1 −t) using Taylor’s formula and subsequently
applying It¯o’s formula, we have
dYt = 𝜕Yt
𝜕t dt + 𝜕Yt
𝜕Xt
dXt + 1
2
𝜕2Yt
𝜕t2 (dt)2 + 1
2
𝜕2Yt
𝜕X2
t
(dXt)2 + . . .
= y −Xt
(1 −t)2 dt −
[(
1
1 −t
) (y −Xt
1 −t dt + dWt
)]
= −
(
1
1 −t
)
dWt.
Taking integrals,
∫
t
0
dYs = −∫
t
0
1
1 −s dWs
Yt −Y0 = −∫
t
0
1
1 −s dWs

138
3.2.2
One-Dimensional Diffusion Process
y −Xt
1 −t = y −x −∫
t
0
1
1 −s dWs.
Therefore,
Xt = yt + (1 −t)
(
x + ∫
t
0
1
1 −s dWs
)
.
Using the properties of stochastic integrals,
𝔼
(
∫
t
0
1
1 −s dWs
)
= 0
𝔼
[(
∫
t
0
1
1 −s dWs
)2]
= 𝔼
(
∫
t
0
1
(1 −s)2 ds
)
=
1
1 −t
therefore the mean and variance of Xt are
𝔼
(
Xt|X0 = x
)
= yt + x(1 −t)
and
Var (Xt|X0 = x) =
1
1 −t
respectively.
Since ∫
t
0
1
1 −s dWs is in the form ∫
t
0
f(s) dWs, from Problem 3.2.2.4 (page 126) we can
easily prove that ∫
t
0
1
1 −s dWs follows a normal distribution,
∫
t
0
1
1 −s dWs ∼𝒩
(
yt + x(1 −t),
1
1 −t
)
.
◽
14. Forward Curve from an Asset Price Following a Geometric Brownian Motion. Let
(Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process.
Suppose the asset price St at time t follows a geometric Brownian motion
dSt
St
= 𝜇dt + 𝜎dWt
where 𝜇is the drift parameter and 𝜎is the volatility. We define the forward price F(t, T)
as an agreed-upon price set at time t to be paid or received at time T, t ≤T and is given
by the relationship
F(t, T) = 𝔼(ST|| ℱt
) .
Show that the forward curve follows
dF(t, T)
F(t, T) = 𝜎dWt.

3.2.2
One-Dimensional Diffusion Process
139
Solution: Given that St follows a geometric Brownian motion then, following
Problem 3.2.2.7 (page 129), we can easily show
𝔼(ST|| ℱt
) = Ste𝜇(T−t).
Because F(t, T) = 𝔼(ST|| ℱt
) = Ste𝜇(T−t), and by expanding F(t, T) using Taylor’s theo-
rem and applying It¯o’s lemma,
dF(t, T) = 𝜕F
𝜕t dt + 𝜕F
𝜕St
dSt + 1
2
𝜕2F
𝜕S2
t
(dSt
)2 + . . .
= −𝜇Ste𝜇(T−t)dt + e𝜇(T−t)dSt.
Since dSt = 𝜇Stdt + 𝜎dWt we have
dF(t, T) = −𝜇Ste𝜇(T−t)dt + e𝜇(T−t) (𝜇Stdt + 𝜎StdWt
)
= 𝜎Ste𝜇(T−t)dWt
= 𝜎F(t, T) dWt.
◽
15. Forward Curve from an Asset Price Following a Geometric Mean-Reverting Process. Let
(Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard Wiener process. Sup-
pose the asset price St at time t follows a geometric mean-reverting process
dSt
St
= 𝜅(𝜃−log St
) dt + 𝜎dWt
where 𝜅is the mean-reversion rate, 𝜃is the long-term mean and 𝜎is the volatility param-
eter. We define the forward price F(t, T) as an agreed-upon price set at time t to be paid
or received at time T, t ≤T and is given by the relationship
F(t, T) = 𝔼(ST|| ℱt
) .
Show that the forward curve follows
dF(t, T)
F(t, T) = 𝜎e−𝜅(T−t)dWt.
Solution: Using the steps described in Problem 3.2.2.11 (page 134), the mean of ST given
St is
𝔼
(
ST|| ℱt
)
= exp
{
e−𝜅(T−t) log St +
(
𝜃−𝜎2
2𝜅
) (
1 −e−𝜅(T−t))
+ 𝜎2
4𝜅
(
1 −e−2𝜅(T−t))}
.
Because F(t, T) = 𝔼(ST|| ℱt
) and expanding F(t, T) using Taylor’s theorem, we have
dF(t, T) = 𝜕F
𝜕t dt + 𝜕F
𝜕St
dSt + 1
2
𝜕2F
𝜕S2
t
(dSt)2 + . . .

140
3.2.2
One-Dimensional Diffusion Process
=
[
𝜅e−𝜅(T−t) log St −
(
𝜃−𝜎2
2𝜅
)
𝜅e−𝜅(T−t) −𝜎2
2 e−2𝜅(T−t)
]
F(t, T) dt
+e−𝜅(T−t)
St
F(t, T) dSt + 1
2
[
−e−𝜅(T−t)
S2
t
+ e−2𝜅(T−t)
S2
t
]
F(t, T)(dSt)2 + . . .
By substituting dSt = 𝜅(𝜃−log St
) Stdt + 𝜎StdWt and applying It¯o’s lemma,
dF(t, T)
F(t, T) =
[
𝜅e−𝜅(T−t) log St −
(
𝜃−𝜎2
2𝜅
)
𝜅e−𝜅(T−t) −𝜎2
2 e−2𝜅(T−t)
]
dt
+e−𝜅(T−t) [𝜅(𝜃−log St
) dt + 𝜎dWt
] −𝜎2
2 e−𝜅(T−t)dt + 𝜎2
2 e−2𝜅(T−t)dt
= 𝜎e−𝜅(T−t)dWt.
◽
16. Forward–Spot Price Relationship I. Let {Wt ∶t ≥0} be the standard Wiener process on
the probability space (Ω, ℱ, ℙ). We define the forward curve F(t, T) following the SDE
dF(t, T)
F(t, T) = 𝜎(t, T) dWt
as an agreed-upon price of an asset with current spot price St to be paid or received at time
T, t ≤T where F(t, T) = 𝔼(ST|| ℱt
) such that ST is the spot price at time T and 𝜎(t, T) > 0
is a time-dependent volatility.
Show that the spot price has the following SDE
dSt
St
=
[𝜕log F(0, t)
𝜕t
−∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du + ∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu
]
dt + 𝜎(t, t) dWt.
Solution: By expanding log F(t, T) using Taylor’s theorem and then applying It¯o’s
lemma,
d log F(t, T) =
1
F(t, T)dF(t, T) −
1
2F(t, T)2 dF(t, T)2 + . . .
= 𝜎(t, T) dWt −1
2𝜎(t, T)2dt
and taking integrals,
∫
t
0
d log F(u, T) = ∫
t
0
𝜎(u, T) dWu −1
2 ∫
t
0
𝜎(u, T)2 du
so we have
F(t, T) = F(0, T)e−1
2 ∫t
0 𝜎(u,T)2du+∫t
0 𝜎(u,T)dWu.
By setting T = t, the spot price St = F(t, t) can be expressed as
St = F(0, t)e−1
2 ∫t
0 𝜎(u,t)2du+∫t
0 𝜎(u,t)dWu.

3.2.2
One-Dimensional Diffusion Process
141
Expanding using Taylor’s theorem,
dSt = 𝜕St
𝜕t dt + 𝜕St
𝜕Wt
dWt + 1
2
𝜕2St
𝜕t2 dt2 + 1
2
𝜕2St
𝜕W2
t
dW2
t + 𝜕2St
𝜕t𝜕Wt
dt dWt + . . .
and applying It¯o’s lemma again,
dSt =
(
𝜕St
𝜕t + 1
2
𝜕2St
𝜕W2
t
)
dt + 𝜕St
𝜕Wt
dWt.
From the spot price equation we have
𝜕St
𝜕t = 𝜕F(0, t)
𝜕t
F(0, t)−1St −1
2
[
𝜎(t, t)2 +∫
t
0
2𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du −2∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu
]
St
=
[𝜕log F(0, t)
𝜕t
−1
2𝜎(t, t)2 −∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du + ∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu
]
St,
𝜕St
𝜕Wt
=
𝜕
𝜕Wt
[
∫
t
0
𝜎(u, t) dWu
]
St = 𝜎(t, t)St
and
𝜕2St
𝜕W2
t
= 𝜎(t, t)2St.
By substituting the values of 𝜕St
𝜕t , 𝜕St
𝜕Wt
and 𝜕2St
𝜕W2
t
into dSt =
(
𝜕St
𝜕t + 1
2
𝜕2St
𝜕W2
t
)
dt +
𝜕St
𝜕Wt
dWt, we finally have
dSt
St
=
[𝜕log F(0, t)
𝜕t
−∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du + ∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu
]
dt + 𝜎(t, t) dWt.
◽
17. Clewlow–Strickland 1-Factor Model. Let {Wt ∶t ≥0} be the standard Wiener process on
the probability space (Ω, ℱ, ℙ). Suppose the forward curve F(t, T) follows the process
dF(t, T)
F(t, T) = 𝜎e−𝛼(T−t)dWt
where t ≤T, 𝛼is the mean-reversion parameter and 𝜎is the volatility. From the
forward–spot price relationship F(t, T) = 𝔼(ST|| ℱt
) where ST is the spot price at time
T, show that
dSt
St
=
[𝜕log F(0, t)
𝜕t
+ 𝛼(log F(0, t) −log St
) + 𝜎2
4
(1 −e−2𝛼t)]
dt + 𝜎dWt

142
3.2.2
One-Dimensional Diffusion Process
and the forward curve at time t is given by
F(t, T) = F(0, T)
[
St
F(0, t)
]e−𝛼(T−t)
e−𝜎2
4𝛼e−𝛼T(e2𝛼t−1)(e−𝛼t−e−𝛼T).
Finally, show that conditional on F(0, T), F(t, T) follows a lognormal distribution with
mean
𝔼[F(t, T)| F(0, T)] = F(0, T)
and variance
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
𝜎2
2𝛼
[e−2𝛼(T−t) −e−2𝛼T] −1
}
.
Solution: For the case when
dF(t, T)
F(t, T) = 𝜎(t, T) dWt
with 𝜎(t, T) a time-dependent volatility, from Problem 3.2.2.16 (page 140) the correspond-
ing spot price SDE is given by
dSt
St
=
[𝜕log F(0, t)
𝜕t
−∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du + ∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu
]
dt + 𝜎(t, t) dWt.
Let 𝜎(t, T) = 𝜎e−𝛼(T−t) and taking partial differentiation with respect to T, we have
𝜕𝜎(t, T)
𝜕T
= −𝛼𝜎e−𝛼(T−t).
Thus,
∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du = −∫
t
0
𝛼𝜎2e−2𝛼(t−u) du = −𝜎2
2
(1 −e−2𝛼t)
and
∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu = −∫
t
0
𝛼𝜎e−𝛼(t−u) dWu.
Using It¯o’s lemma,
d log F(t, T) =
1
F(t, T)dF(t, T) −
1
2F(t, T)2 dF(t, T)2 + . . .
= 𝜎e−𝛼(T−t)dWt −1
2𝜎2e−2𝛼(T−t)dt

3.2.2
One-Dimensional Diffusion Process
143
and taking integrals,
log F(t, T) = log F(0, T) −1
2 ∫
t
0
𝜎2e−2𝛼(T−u) du + ∫
t
0
𝜎e−𝛼(T−u) dWu.
By setting T = t such that F(t, t) = St, we can write
log St = log F(0, t) −1
2 ∫
t
0
𝜎2e−2𝛼(t−u) du + ∫
t
0
𝜎e−𝛼(t−u) dWu
therefore
∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu = 𝛼(log F(0, t) −log St
) −1
2 ∫
t
0
𝛼𝜎2e−2𝛼(t−u) du
= 𝛼(log F(0, t) −log St
) −𝜎2
4
(1 −e−2𝛼t) .
By substituting the values of ∫
t
0
𝜎(u, t)𝜕𝜎(u, t)
𝜕t
du and ∫
t
0
𝜕𝜎(u, t)
𝜕t
dWu into the spot
price SDE and taking note that 𝜎(t, t) = 𝜎, we eventually have
dSt
St
=
[𝜕log F(0, t)
𝜕t
+ 𝛼(log F(0, t) −log St
) + 𝜎2
4
(1 −e−2𝛼t)]
dt + 𝜎dWt.
Since
F(t, T) = F(0, T)e−1
2 ∫t
0 𝜎2e−2𝛼(T−u)du+∫t
0 𝜎e−𝛼(T−u)dWu
with
∫
t
0
𝜎2e−2𝛼(T−u) du = 𝜎2
2𝛼e−2𝛼T (e2𝛼t −1)
and using the spot price equation
∫
t
0
𝜎e−𝛼(T−u) dWu = e−𝛼(T−t)
∫
t
0
𝜎e−𝛼(t−u) dWu
= e−𝛼(T−t)
{
log
[
St
F(0, t)
]
+ 1
2 ∫
t
0
𝜎2e−2𝛼(t−u) du
}
= e−𝛼(T−t)
{
log
[
St
F(0, t)
]
+ 𝜎2
4𝛼
(1 −e−2𝛼t)}
therefore
F(t, T) = F(0, T)
[
St
F(0, t)
]e−𝛼(T−t)
e−𝜎2
4𝛼e−𝛼T(e2𝛼t−1)(e−𝛼t−e−𝛼T).
Finally, using
d log F(t, T) = 𝜎e−𝛼(T−t)dWt −1
2𝜎2e−2𝛼(T−t)dt

144
3.2.2
One-Dimensional Diffusion Process
and taking integrals we have
log F(t, T) = log F(0, T) + ∫
t
0
𝜎e−𝛼(T−u) dWu −𝜎2
4𝛼
[e−2𝛼(T−t) −e−2𝛼T] .
From the property of the It¯o integral,
𝔼
[
∫
t
0
𝜎e−𝛼(T−u) dWu
]
= 0
and
𝔼
[(
∫
t
0
𝜎e−𝛼(T−u) dWu
)2]
= 𝔼
[
∫
t
0
𝜎2e−2𝛼(T−u) du
]
= 𝜎2
2𝛼
[e−2𝛼(T−t) −e−2𝛼T] .
Since ∫
t
0
𝜎e−𝛼(T−u) dWu is in the form ∫
t
0
f(u) dWu, from Problem 3.2.2.4 (page 126)
we can easily show that ∫
t
0
𝜎e−𝛼(T−u) dWu follows a normal distribution.
In addition, since we can also write the It¯o integral as
∫
t
0
𝜎e−𝛼(T−u) dWu = lim
n→∞
n−1
∑
i=0
𝜎e−𝛼(T−ti)(Wti+1 −Wti)
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕand due to the stationary
increment of a standard Wiener process, each term of Wti+1 −Wti ∼𝒩
(
0, t
n
)
is normally
distributed multiplied by a deterministic term and we can deduce that
∫
t
0
𝜎e−𝛼(T−u) dWu ∼𝒩
(
0, 𝜎2
2𝛼
[e−2𝛼(T−t) −e−2𝛼T])
.
Hence,
log F(t, T) ∼𝒩
(
log F(0, T) −𝜎2
4𝛼
[e−2𝛼(T−t) −e−2𝛼T] , 𝜎2
2𝛼
[e−2𝛼(T−t) −e−2𝛼T])
which implies
𝔼[F(t, T)| F(0, T)] = F(0, T)
and
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
𝜎2
2𝛼
[e−2𝛼(T−t) −e−2𝛼T] −1
}
.
◽

3.2.2
One-Dimensional Diffusion Process
145
18. Constant Elasticity of Variance Model. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶
t ≥0} be a standard Wiener process with respect to the filtration ℱt, t ≥0. Suppose St > 0
follows a constant elasticity of variance (CEV) model of the form
dSt = rStdt + 𝜎(St, t)StdWt
where r is a constant and 𝜎(St, t) is a local volatility function. By setting 𝜎(St, t) = 𝛼S𝛽−1
t
with 𝛼> 0 and 0 < 𝛽< 1, show using It¯o’s formula that 𝜎(St, t) satisfies
d𝜎(St, t)
𝜎(St, t) = (𝛽−1)
{[
r + 1
2(𝛽−2)𝜎(St, t)2]
dt + 𝜎(St, t) dWt
}
.
Finally, conditional on St show that for t < T,
ST = erT
[
e−r(1−𝛽)tS1−𝛽
t
+ 𝛼∫
T
t
e−r(1−𝛽)u dWu
]
1
1−𝛽
.
Solution: From It¯o’s formula,
d𝜎(St, t) = 𝜕𝜎(St, t)
𝜕St
dSt + 1
2
𝜕2𝜎(St, t)
𝜕S2
t
(dSt)2 + . . .
= 𝛼(𝛽−1)S𝛽−2
t
dSt + 1
2𝛼(𝛽−1)(𝛽−2)S𝛽−3
t
(dSt)2 + . . .
= 𝛼(𝛽−1)S𝛽−2
t
(rSt dt + 𝛼S𝛽
t dWt) + 1
2𝛼(𝛽−1)(𝛽−2)(𝛼2S2𝛽
t dt)
= (𝛽−1)
[(
r𝜎(St, t) + 1
2(𝛽−2)𝜎(St, t)3)
dt + 𝜎(St, t)2dWt
]
.
Therefore,
d𝜎(St, t)
𝜎(St, t) = (𝛽−1)
{[
r + 1
2(𝛽−2)𝜎(St, t)2]
dt + 𝜎(St, t) dWt
}
.
To find the solution of the CEV model, let Xt = e−rtSt and by It¯o’s formula
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕St
dSt + 1
2
𝜕2Xt
𝜕S2
t
(dSt)2 + . . .
= −re−rtStdt + e−rt (
rStdt + 𝛼S𝛽
t dWt
)
= 𝛼e−rtS𝛽
t dWt
= 𝛼e−r(1−𝛽)tX𝛽
t dWt.
Taking integrals
∫
T
t
dXu
X𝛽
u
= 𝛼∫
T
t
e−r(1−𝛽)u dWu

146
3.2.2
One-Dimensional Diffusion Process
we have
X1−𝛽
T
= X1−𝛽
t
+ 𝛼∫
T
t
e−r(1−𝛽)u dWu
XT =
[
X1−𝛽
t
+ 𝛼∫
T
t
e−r(1−𝛽)u dWu
]
1
1−𝛽
or
ST = erT
[
e−r(1−𝛽)tS1−𝛽
t
+ 𝛼∫
T
t
e−r(1−𝛽)u dWu
]
1
1−𝛽
.
◽
19. Geometric Average. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0} be a standard
Wiener process with respect to the filtration ℱt, t ≥0. Suppose St satisfies the following
geometric Brownian motion model:
dSt
St
= 𝜇dt + 𝜎dWt,
where 𝜇and 𝜎are constant parameters. By considering a geometric average of St
Gt = e
1
t ∫t
0 Sudu,
G0 = S0
show that Gt satisfies the following SDE:
dGt
Gt
= 1
2
(
𝜇−1
6𝜎2)
dt + 𝜎
√
3
d ̃Wt
where ̃Wt =
√
3
t
∫
t
0
Wu du.
Under what condition is this SDE valid?
Solution: Using the steps described in Problem 3.2.2.7 (page 129) for u < t,
Su = S0e(𝜇−1
2 𝜎2)u+𝜎Wu
or
log Su = log S0 +
(
𝜇−1
2𝜎2)
u + 𝜎Wu.
Taking the natural logarithm of Gt,
log Gt = 1
t ∫
t
0
log Su du
= 1
t ∫
t
0
[
log S0 +
(
𝜇−1
2𝜎2)
u + 𝜎Wu
]
du

3.2.2
One-Dimensional Diffusion Process
147
= 1
t ∫
t
0
log S0 du + 1
t ∫
t
0
(
𝜇−1
2𝜎2)
u du + 𝜎
t ∫
t
0
Wu du
= log S0 + 1
2
(
𝜇−1
2𝜎2)
t + 𝜎
t ∫
t
0
Wu du.
From Problem 3.2.1.13 (page 115), using integration by parts we can write
∫
t
0
Wu du = ∫
t
0
(t −u) dWu
and we can deduce that ∫
t
0
Wu du ∼𝒩
(
0, 1
3t3)
and hence
𝜎
t ∫
t
0
Wu du ∼𝒩
(
0, 1
3𝜎2t
)
or
𝜎
√
3
̃Wt ∼𝒩
(
0, 1
3𝜎2t
)
where ̃Wt =
√
3
t
∫
t
0
Wu du ∼𝒩(0, t).
Thus,
log Gt = log S0 + 1
2
(
𝜇−1
2𝜎2)
t + 𝜎
√
3
̃Wt
or
dGt
Gt
= 1
2
(
𝜇−1
6𝜎2)
dt + 𝜎
√
3
d ̃Wt
with G0 = S0.
However, given that ̃Wt does not have the stationary increment property (see Problem
3.2.1.13, page 115), this SDE is only valid if the geometric average starts at time t = 0.
◽
20. Feynman–Kac Formula for One-Dimensional Diffusion Process. We consider the follow-
ing PDE problem:
𝜕V
𝜕t + 1
2𝜎(St, t)2 𝜕2V
𝜕S2
t
+ 𝜇(St, t) 𝜕V
𝜕St
−r(t)V(St, t) = 0
with boundary condition V(ST, T) = Ψ(ST) where 𝜇, 𝜎are known functions of St and t,
r and Ψ are functions of t and ST, respectively with t < T. Using It¯o’s formula on the
process
Zu = e−∫u
t r(𝑣)d𝑣V(Su, u)
where St satisfies the generalised SDE
dSt = 𝜇(St, t) dt + 𝜎(St, t) dWt

148
3.2.2
One-Dimensional Diffusion Process
such that {Wt ∶t ≥0} is a standard Wiener process on the probability space (Ω, ℱ, ℙ),
show that under the filtration ℱt the solution of the PDE is given by
V(St, t) = 𝔼
[
e−∫T
t
r(𝑣)d𝑣Ψ(ST)||||
ℱt
]
.
Solution: Let g(u) = e−∫u
t r(𝑣)d𝑣and hence we can write
Zu = g(u)V(Su, u).
By applying Taylor’s expansion and It¯o’s formula on dZu we have
dZu = 𝜕Zu
𝜕u du + 𝜕Zu
𝜕Su
dSu + 1
2
𝜕2Zu
𝜕S2
u
(dSu)2 + . . .
=
(
g(u)𝜕V
𝜕u + V(Su, u)𝜕g
𝜕u
)
du +
(
g(u) 𝜕V
𝜕Su
)
dSu + 1
2
(
g(u)𝜕2V
𝜕S2
u
)
(dSu)2
=
(
g(u)𝜕V
𝜕u −r(u)g(u)V(Su, u)
)
du +
(
g(u) 𝜕V
𝜕Su
) (𝜇(Su, u) du + 𝜎(Su, u)dWu
)
+1
2
(
g(u)𝜕2V
𝜕S2
u
)
(𝜎(Su, u)2du)
= g(u)
(
𝜕V
𝜕u + 1
2𝜎(Su, u)2 𝜕2V
𝜕S2
u
+ 𝜇(Su, u) 𝜕V
𝜕Su
−r(u)V(Su, u)
)
du
+g(u)𝜎(Su, u) 𝜕V
𝜕Su
dWu
= g(u)𝜎(Su, u) 𝜕V
𝜕Su
dWu
since
𝜕V
𝜕t + 1
2𝜎(Su, u)2 𝜕2V
𝜕S2
u
+ 𝜇(Su, u) 𝜕V
𝜕Su
−r(u)V(Su, u) = 0.
Taking integrals we have
∫
T
t
dZu = ∫
T
t
g(u)𝜎(Su, u) 𝜕V
𝜕Su
dWu
ZT −Zt = ∫
T
t
e−∫u
t r(𝑣)d𝑣𝜎(Su, u) 𝜕V
𝜕Su
dWu.
By taking expectations and using the properties of the It¯o integral,
𝔼(ZT −Zt
) = 0 or 𝔼(Zt
) = 𝔼(ZT
)

3.2.2
One-Dimensional Diffusion Process
149
and hence under the filtration ℱt,
𝔼(Zt|ℱt
) = 𝔼(ZT|ℱt)
𝔼
[
e−∫t
t r(𝑣)d𝑣V(St, t)||| ℱt
]
= 𝔼
[
e−∫T
t
r(𝑣)d𝑣V(ST, T)||||
ℱt
]
V(St, t) = 𝔼
[
e−∫T
t
r(𝑣)d𝑣Ψ(ST)
||||
ℱt
]
.
◽
21. Backward Kolmogorov Equation for One-Dimensional Diffusion Process. Let {Wt ∶t ≥
0} be a standard Wiener process on the probability space (Ω, ℱ, ℙ). For t ∈[0, T], T > 0
consider the generalised stochastic differential equation
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
where 𝜇(Xt, t) and 𝜎(Xt, t) are functions dependent on Xt and t. By conditioning Xt = x
and XT = y, let p(x, t; y, T) be the transition probability density of XT at time T starting at
time t at point Xt. For any function Ψ(XT), from the Feynman–Kac formula the function
f(x, t) = 𝔼
[
e−∫T
t
r(u)du Ψ(XT)
||||
ℱt
]
= e−∫T
t r(u)du
∫Ψ(y)p(x, t; y, T) dy
satisfies the partial differential equation
𝜕f
𝜕t + 1
2𝜎(x, t)2 𝜕2f
𝜕x2 + 𝜇(x, t)𝜕f
𝜕x −r(t)f(x, t) = 0
where r is a time-dependent function.
Show that the transition probability density p(t, x; T, y) in the y variable satisfies
𝜕
𝜕tp(x, t; y, T) + 1
2𝜎(x, t)2 𝜕2
𝜕x2 p(x, t; y, T) + 𝜇(x, t) 𝜕
𝜕xp(x, t; y, T) = 0.
Solution: From the Feynman–Kac formula, for any function Ψ(XT), the function
f(x, t) = 𝔼
[
e−∫T
t
r(u)du Ψ(XT)||||
ℱt
]
= e−∫T
t r(u)du
∫Ψ(y)p(x, t; y, T) dy
satisfies the following PDE:
𝜕f
𝜕t + 1
2𝜎(x, t)2 𝜕2f
𝜕x2 + 𝜇(x, t)𝜕f
𝜕x −r(t)f(x, t) = 0.

150
3.2.2
One-Dimensional Diffusion Process
By differentiation, we have
𝜕f
𝜕t = r(t)e−∫T
t
r(u)du
∫Ψ(y)p(x, t; y, T) dy + e−∫T
t
r(u)du
∫Ψ(y) 𝜕
𝜕tp(x, t; y, T) dy
= r(t)f(x, t) + e−∫T
t
r(u)du
∫Ψ(y) 𝜕
𝜕tp(x, t; y, T) dy
𝜕f
𝜕x = e−∫T
t
r(u)du
∫Ψ(y) 𝜕
𝜕xp(x, t; y, T) dy and
𝜕2f
𝜕x2 = e−∫T
t
r(u)du
∫Ψ(y) 𝜕2
𝜕x2 p(x, t; y, T) dy.
Substituting the above equations into the PDE, we obtain
e−∫T
t
r(u)du ×
∫Ψ(y)
[
𝜕
𝜕tp(x, t; y, T) + 1
2𝜎(x, t)2 𝜕2
𝜕x2 p(x, t; y, T) + 𝜇(x, t) 𝜕
𝜕xp(x, t; y, T)
]
dy = 0.
Finally, irrespective of the choice of Ψ(y) and r(t), the transition probability density func-
tion p(x, t; y, T) satisfies
𝜕
𝜕tp(x, t; y, T) + 1
2𝜎(x, t)2 𝜕2
𝜕x2 p(x, t; y, T) + 𝜇(x, t) 𝜕
𝜕xp(x, t; y, T) = 0.
◽
22. Forward Kolmogorov Equation for One-Dimensional Diffusion Process. Let {Wt ∶t ≥0}
be a standard Wiener process on the probability space (Ω, ℱ, ℙ). For t ∈[0, T], T > 0
consider the generalised stochastic differential equation
dXt = 𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
where 𝜇(Xt, t) and 𝜎(Xt, t) are functions dependent on Xt and t. Using It¯o’s formula on the
function f(Xt, t), show that
f(XT, T) = f(Xt, t) + ∫
T
t
(
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕X2
t
(Xs, s)
)
ds
+ ∫
T
t
𝜎(Xs, s) 𝜕f
𝜕Xt
(Xs, s) dWs
and taking the expectation conditional on Xt = x, show that in the limit T →t
∫
T
t
𝔼
[
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕Xt
2 (Xs, s)
|||||
Xt = x
]
ds = 0.

3.2.2
One-Dimensional Diffusion Process
151
Let XT = y and define the transition probability density p(x, t; y, T) of XT at time T starting
at time t at point Xt. By writing the conditional expectation in terms of p(x, t; y, T) and
integrating by parts twice, show that
𝜕
𝜕T p(x, t; y, T) = 1
2
𝜕2
𝜕y2 (𝜎(y, T)2p(x, t; y, T)) −𝜕
𝜕y(𝜇(y, T)p(x, t; y, T)).
Solution: For a suitable function f(Xt, t) and using It¯o’s formula,
df(Xt, t) = 𝜕f
𝜕t (Xt, t) dt + 𝜕f
𝜕Xt
(Xt, t) dXt + 1
2
𝜕2f
𝜕X2
t
(Xt, t)(dXt)2 + . . .
= 𝜕f
𝜕t (Xt, t) dt + 𝜕f
𝜕Xt
(Xt, t) (𝜇(Xt, t) dt + 𝜎(Xt, t) dWt
) + 1
2𝜎(Xt, t)2 𝜕2f
𝜕X2
t
(Xt, t) dt
=
(
𝜕f
𝜕t (Xt, t) + 𝜇(Xt, t) 𝜕f
𝜕Xt
(Xt, t) + 1
2𝜎(Xt, t)2 𝜕2f
𝜕X2
t
(Xt, t)
)
dt
+ 𝜎(Xt, t) 𝜕f
𝜕Xt
(Xt, t) dWt.
Taking integrals,
∫
T
t
df(Xs, s) = ∫
T
t
(
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕X2
t
(Xs, s)
)
ds
+ ∫
T
t
𝜎(Xs, s) 𝜕f
𝜕Xt
(Xs, s) dWs
f(XT, T) = f(Xt, t)
+ ∫
T
t
(
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕X2
t
(Xs, s)
)
ds
+ ∫
T
t
𝜎(Xs, s) 𝜕f
𝜕Xt
(Xs, s) dWs.
By taking conditional expectations given Xt = x we have
𝔼(f(XT, T)|| Xt = x) = 𝔼(f(Xt, t)|| Xt = x)
+ ∫
T
t
𝔼
(
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕X2
t
(Xs, s)
|||||
Xt = x
)
ds
and since in the limit
lim
T→t 𝔼(f(XT, T)|| Xt = x) = 𝔼(f(Xt, t)|| Xt = x)

152
3.2.2
One-Dimensional Diffusion Process
so
∫
T
t
𝔼
[
𝜕f
𝜕t (Xs, s) + 𝜇(Xs, s) 𝜕f
𝜕Xt
(Xs, s) + 1
2𝜎(Xs, s)2 𝜕2f
𝜕X2
t
(Xs, s)
|||||
Xt = x
]
ds = 0
or
∫
T
t
∫
∞
−∞
[𝜕f
𝜕s(y, s) + 𝜇(y, s)𝜕f
𝜕y(y, s) + 1
2𝜎(y, s)2 𝜕2f
𝜕y2 (y, s)
]
p(x, t; y, s) dyds = 0
where p(x, t; y, s) is the transition probability density function of Xt in the y-variable.
Integrating by parts,
∫
T
t ∫
∞
−∞
𝜕f
𝜕s(y, s)p(x, t; y, s) dyds
= ∫
∞
−∞∫
T
t
𝜕f
𝜕s(y, s)p(x, t; y, s) dsdy
=
∫
∞
−∞
f(y, s)p(x, t; y, s)||||
T
t
dy
−∫
∞
−∞∫
T
t
f(y, s) 𝜕
𝜕sp(x, t; y, s) dsdy
=
−∫
∞
−∞∫
T
t
f(y, s) 𝜕
𝜕sp(x, t; y, s) dsdy
=
−∫
T
t
∫
∞
−∞
f(y, s) 𝜕
𝜕sp(x, t; y, s) dyds
∫
T
t
∫
∞
−∞
𝜇(y, s)𝜕f
𝜕y(y, s)p(x, t; y, s) dyds
=
∫
T
t
f(y, s)𝜇(y, s)p(x, t; y, s)
|||||
∞
−∞
ds
−∫
T
t
∫
∞
−∞
f(y, s) 𝜕
𝜕y(𝜇(y, s)p(x, t; y, s)) dyds
= −∫
T
t
∫
∞
−∞
f(y, s) 𝜕
𝜕y(𝜇(y, s)p(x, t; y, s)) dyds
∫
T
t
∫
∞
−∞
1
2 𝜎(y, s)2 𝜕2f
𝜕y2 (y, s)p(x, t; y, s) dyds
= ∫
T
t
1
2
𝜕
𝜕yf(y, s)𝜎(y, s)2p(x, t; y, s)
|||||
∞
−∞
ds
−∫
T
t
∫
∞
−∞
1
2
𝜕
𝜕yf(y, s) 𝜕
𝜕y(𝜎(y, s)2p(x, t; y, s)) dyds
= −∫
T
t ∫
∞
−∞
1
2
𝜕
𝜕yf(y, s) 𝜕
𝜕y(𝜎(y, s)2p(x, t; y, s)) dyds
= −∫
T
t
1
2f(y, s) 𝜕
𝜕y
(𝜎(y, s)2p(x, t; y, s))|||||
∞
−∞
ds

3.2.2
One-Dimensional Diffusion Process
153
+ ∫
T
t
∫
∞
−∞
1
2f(y, s) 𝜕2
𝜕y2 (𝜎(y, s)2p(x, t; y, s)) dyds
= ∫
T
t
∫
∞
−∞
1
2f(y, s) 𝜕2
𝜕y2 (𝜎(y, s)2p(x, t; y, s)) dyds
and by substituting the above relationships into the integro partial differential equation,
we have
∫
T
t
∫
∞
−∞
f(y, s)×
[
𝜕
𝜕sp(x, t; y, s) + 𝜕
𝜕y (𝜇(y, s) p (x, t; y, s)) −1
2
𝜕2
𝜕y2
(𝜎(y, s)2p(x, t; y, s))]
dyds = 0.
Finally, by differentiating the above equation with respect to T, we obtain
∫
∞
−∞
f(y, T)×
[
𝜕
𝜕T p(x, t; y, T) + 𝜕
𝜕y (𝜇(y, T)p(x, t; y, T)) −1
2
𝜕2
𝜕y2
(𝜎(y, T)2p(x, t; y, T))]
dy = 0
and irrespective of the choice of f, the transition probability density function p(x, t; y, T)
satisfies
𝜕
𝜕T p(x, t; y, T) = 1
2
𝜕2
𝜕y2
(𝜎(y, T)2p(x, t; y, T)) −𝜕
𝜕y (𝜇(y, T)p(x, t; y, T)) .
◽
23. Backward Kolmogorov Equation for a One-Dimensional Random Walk. We consider a
one-dimensional symmetric random walk where at initial time t0, a particle starts at x0 and
is at position x at time t. At time t + 𝛿t, the particle can either move to x + 𝛿x or x −𝛿x each
with probability 1
2. Let p(x, t; x0, t0) denote the probability density of the particle position
x at time t starting at x0 at time t0.
By writing the backward equation in a discrete fashion and expanding it using Taylor’s
series, show that for 𝛿x =
√
𝛿t and in the limit 𝛿t →0
𝜕p(x, t; x0, t0)
𝜕t
= −1
2
𝜕2p(x, t; x0, t0)
𝜕x2
.
Solution: By denoting p(x, t; x0, t0) as the probability density function of the particle posi-
tion x at time t, hence the discrete model of the backward equation is
p(x, t; x0, t0) = 1
2p(x −𝛿x, t + 𝛿t; x0, t0) + 1
2p(x + 𝛿x, t + 𝛿t; x0, t0).
Using Taylor’s series, we have
p(x −𝛿x, t + 𝛿t; x0, t0) = p(x, t + 𝛿t; x0, t0) −𝜕p(x, t + 𝛿t; x0, t0)
𝜕x
𝛿x
+ 1
2
𝜕2p(x, t + 𝛿t; x0, t0)
𝜕x2
(−𝛿x)2 + O((𝛿x)3)

154
3.2.2
One-Dimensional Diffusion Process
and
p(x + 𝛿x, t + 𝛿t; x0, t0) = p(x, t + 𝛿t; x0, t0) + 𝜕p(x, t + 𝛿t; x0, t0)
𝜕x
𝛿x
+ 1
2
𝜕2p(x, t + 𝛿t; x0, t0)
𝜕x2
(𝛿x)2 + O((𝛿x)3).
Substituting these two equations into the backward equation,
p(x, t; x0, t0) = p(x, t + 𝛿t; x0, t0) + 1
2
𝜕2p(x, t + 𝛿t; x0, t0)
𝜕x2
(𝛿x)2 + O((𝛿x)3).
By setting 𝛿x =
√
𝛿t, dividing the equation by 𝛿t and taking limits 𝛿t →0
−lim
𝛿t→0
[p(x, t + 𝛿t; x0, t0) −p(x, t; x0, t0)
𝛿t
]
= lim
𝛿t→0
[
1
2
𝜕2p(x, t + 𝛿t; x0, t0)
𝜕x2
+ O(
√
𝛿t)
]
we finally have
𝜕p(x, t; x0, t0)
𝜕t
= −1
2
𝜕2p(x, t; x0, t0)
𝜕x2
.
◽
24. Forward Kolmogorov Equation for a One-Dimensional Random Walk. We consider a
one-dimensional symmetric random walk where at initial time t0, a particle starts at y0
and is at position y at terminal time T > 0. At time T −𝛿T, the particle can either move to
y + 𝛿y or y −𝛿y each with probability 1
2. Let p(y, T; y0, t0) denote the probability density
of the position y at time T starting at y0 at time t0.
By writing the forward equation in a discrete fashion and expanding it using Taylor’s
series, show that for 𝛿y =
√
𝛿T and in the limit 𝛿T →0
𝜕p(y, T; y0, t0)
𝜕T
= 1
2
𝜕2p(y, T; y0, t0)
𝜕y2
.
Solution: By denoting p(y, T; y0, t0) as the probability density function of the particle
position y at time T, hence the discrete model of the forward equation is
p(y, T; y0, t0) = 1
2p(y −𝛿y, T −𝛿T; y0, t0) + 1
2p(y + 𝛿y, T −𝛿T; y0, t0).
By expanding p(y −𝛿y, T −𝛿T; y0, t0) and p(y + 𝛿y, T −𝛿T; y0, t0) using Taylor’s series,
we have
p(y −𝛿y, T −𝛿T; y0, t0) = p(y, T −𝛿T; y0, t0) −𝜕p(y, T −𝛿T; y0, t0)
𝜕y
𝛿y
+ 1
2
𝜕2p(y, T −𝛿T; y0, t0)
𝜕y2
(−𝛿y)2 + O((𝛿y)3)

3.2.3
Multi-Dimensional Diffusion Process
155
and
p(y + 𝛿y, T −𝛿T; y0, t0) = p(y, T −𝛿T; y0, t0) + 𝜕p(y, T −𝛿T; y0, t0)
𝜕y
𝛿y
+ 1
2
𝜕2p(y, T −𝛿T; y0, t0)
𝜕y2
(𝛿y)2 + O((𝛿y)3).
Substituting the above two equations into the discrete forward equation,
p(y, T; y0, t0) = p(y, T −𝛿T; y0, t0) + 1
2
𝜕2p(y, T −𝛿T; y0, t0)
𝜕y2
(𝛿y)2 + O((𝛿y)3).
By setting 𝛿y =
√
𝛿T, dividing the equation by 𝛿T and taking limits 𝛿T →0
lim
𝛿T→0
[p(y, T; y0, t0) −p(y, T −𝛿T; y0, t0)
𝛿T
]
= lim
𝛿T→0
[
1
2
𝜕2p(y, T −𝛿T; y0, t0)
𝜕y2
+ O(
√
𝛿T)
]
we finally have
𝜕p(y, T; y0, t0)
𝜕T
= 1
2
𝜕2p(y, T; y0, t0)
𝜕y2
.
◽
3.2.3
Multi-Dimensional Diffusion Process
1. Let (Ω, ℱ, ℙ) be a probability space and consider n assets with prices S(i)
t , i = 1, 2, . . . , n
satisfying the SDEs
dS(i)
t
= 𝜇(i)S(i)
t dt + 𝜎(i)S(i)
t dW(i)
t
(
dW(i)
t
) (
dW(j)
t
)
= 𝜌(ij)dt
where {W(i)
t
∶t ≥0}, i = 1, 2, . . . , n are standard Wiener processes, 𝜌(ij) ∈(−1, 1), i ≠j
and 𝜌(ii) = 1.
By considering the function f(S(1)
t , S(2)
t , . . . , S(n)
t ), show using It¯o’s formula that
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
∑
i=1
𝜇(i)S(i)
t
𝜕f
𝜕S(i)
t
dt + 1
2
n
∑
i=1
n
∑
j=1
𝜌(ij)𝜎(i)𝜎(j)S(i)
t S(j)
t
𝜕2f
𝜕S(i)
t 𝜕S(j)
t
dt
+
n
∑
i=1
𝜎(i)S(i)
t
𝜕f
𝜕S(i)
t
dW(i)
t .
Solution: By expanding df(S(1)
t , S(2)
t , . . . , S(n)
t ) using Taylor’s formula,
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
∑
i=1
𝜕f
𝜕S(i)
t
dS(i)
t + 1
2
n
∑
i=1
n
∑
j=1
𝜕2f
𝜕S(i)
t S(j)
t
dS(i)
t dS(j)
t + . . .

156
3.2.3
Multi-Dimensional Diffusion Process
=
n
∑
i=1
𝜕f
𝜕S(i)
t
(
𝜇(i)S(i)
t dt + 𝜎(i)S(i)
t dW(i)
t
)
+1
2
n
∑
i=1
n
∑
j=1
𝜕2f
𝜕S(i)
t S(j)
t
(
𝜇(i)S(i)
t dt + 𝜎(i)S(i)
t dW(i)
t
) (
𝜇(j)S(j)
t dt + 𝜎(j)S(j)
t dW(j)
t
)
+ . . .
By setting (dt)2 = 0, (dW(i)
t )2 = dt, (dW(i)
t )(dW(j)
t ) = 𝜌(ij)dt, dW(i)
t dt = 0, i, j = 1, 2, . . . , n
we have
df(S(1)
t , S(2)
t , . . . , S(n)
t ) =
n
∑
i=1
𝜇(i)S(i)
t
𝜕f
𝜕S(i)
t
dt + 1
2
n
∑
i=1
n
∑
j=1
𝜌(ij)𝜎(i)𝜎(j)S(i)
t S(j)
t
𝜕2f
𝜕S(i)
t 𝜕S(j)
t
dt
+
n
∑
i=1
𝜎(i)S(i)
t
𝜕f
𝜕S(i)
t
dW(i)
t .
◽
2. Let (Ω, ℱ, ℙ) be a probability space. We consider two assets with prices S(1)
t , S(2)
t
satisfying
the SDEs
dS(1)
t
= 𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
dS(2)
t
= 𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
dW(1)
t dW(2)
t
= 𝜌dt
where 𝜇(1), 𝜇(2), 𝜎(1), 𝜎(2) are constants and {W(1)
t
∶t ≥0}, {W(2)
t
∶t ≥0} are standard
Wiener processes with correlation 𝜌.
By letting Ut = S(1)
t ∕S(2)
t
show that the SDE satisfied by Ut is
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) −𝜇(2) + (𝜎(2))2 −𝜌𝜎(1)𝜎(2), 𝜎=
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2) and
Vt =
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2) .
Finally, show that {Vt ∶t ≥0} is a standard Wiener process.
Solution: By using Taylor’s expansion and applying It¯o’s formula,
dUt = d
(
S(1)
t ∕S(2)
t
)
= 1
S(2)
t
dS(1)
t
−
S(1)
t
(S(2)
t )2 dS(2)
t
+
S(1)
t
(S(2)
t )3 (dS(2)
t )2 −
1
(S(2)
t )2 dS(1)
t dS(2)
t
+ . . .

3.2.3
Multi-Dimensional Diffusion Process
157
= 𝜇(1)Utdt + 𝜎(1)UtdW(1)
t
−
(
𝜇(2)Utdt + 𝜎(2)UtdW(2)
t
)
+(𝜎(2))2Utdt −𝜌𝜎(1)𝜎(2)Utdt
= (𝜇(1) −𝜇(2) + (𝜎(2))2 −𝜌𝜎(1)𝜎(2)) Utdt + Ut
(
𝜎(1)dW(1)
t
−𝜎(2)dW(2)
t
)
= (𝜇(1) −𝜇(2) + (𝜎(2))2 −𝜌𝜎(1)𝜎(2)) Utdt +
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)UtdVt.
Therefore,
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) −𝜇(2) + (𝜎(2))2 −𝜌𝜎(1)𝜎(2), 𝜎=
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2) and
Vt =
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2) .
To show that {Vt ∶t ≥0} is a standard Wiener process, we first show that Vt ∼𝒩(0, t).
Taking expectations,
𝔼(Vt) =
𝜎(1)𝔼(W(1)
t ) −𝜎(2)𝔼(W(2)
t )
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2) = 0
𝔼(V2
t ) = 𝔼
[
(𝜎(1))2(W(1)
t )2 + (𝜎(2))2(W(2)
t )2 −2𝜎(1)𝜎(2)W(1)
t W(2)
t
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
]
= (𝜎(1))2t + (𝜎(2))2t −2𝜌𝜎(1)𝜎(2)t
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
= t.
Given that W(1)
t
∼𝒩(0, t), W(2)
t
∼𝒩(0, t) and a linear combination of normal variates is
also normal, therefore Vt ∼𝒩(0, t).
To show that {Vt ∶t ≥0} is a standard Wiener process, we note the following:
(a) V0 = 0 and it is clear that Vt has continuous sample paths for t ≥0.
(b) For t > 0, s > 0 we have
W(1)
t+s −W(1)
t
∼𝒩(0, s),
W(2)
t+s −W(2)
t
∼𝒩(0, s)
Cov
(
W(1)
t+s −W(1)
t , W(2)
t+s −W(2)
t
)
= 𝜌s
and hence
Vt+s −Vt =
𝜎(1) (
W(1)
t+s −W(1)
t
)
−𝜎(2) (
W(2)
t+s −W(2)
t
)
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)

158
3.2.3
Multi-Dimensional Diffusion Process
with mean
𝔼(Vt+s −Vt
) =
𝜎(1)𝔼
(
W(1)
t+s −W(1)
t
)
−𝜎(2)𝔼
(
W(2)
t+s −W(2)
t
)
√
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
= 0
and variance
Var (Vt+s −Vt
) =
⎡
⎢
⎢⎣
(𝜎(1))2Var
(
W(1)
t+s −W(1)
t
)
+ (𝜎(2))2Var
(
W(2)
t+s −W(2)
t
)
−2𝜎(1)𝜎(2)Cov
(
W(1)
t+s −W(1)
t , W(2)
t+s −W(2)
t
)
⎤
⎥
⎥⎦
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
= s.
Therefore, Vt+s −Vt ∼𝒩(0, s).
(c) For t > 0, s > 0, to show that Vt+s −Vt ⟂⟂Vt we note that
𝔼[(Vt+s −Vt
) Vt
] = 𝔼(Vt+sVt
) −𝔼(V2
t
)
= 𝔼
⎡
⎢
⎢
⎢⎣
(
𝜎(1)W(1)
t+s −𝜎(2)W(2)
t+s
) (
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
)
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
⎤
⎥
⎥
⎥⎦
−𝔼(V2
t
)
=
⎡
⎢
⎢⎣
(𝜎(1))2𝔼
(
W(1)
t W(1)
t+s
)
−𝜎(1)𝜎(2)𝔼
(
W(1)
t+sW(2)
t
)
−𝜎(1)𝜎(2)𝔼
(
W(1)
t W(2)
t+s
)
+ (𝜎(2))2𝔼(W(2)
t W(2)
t+s)
⎤
⎥
⎥⎦
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
−t
=
[
(𝜎(1))2min {t, t + s} −𝜌𝜎(1)𝜎(2)min{t, t + s}
−𝜌𝜎(1)𝜎(2)min{t, t + s} + (𝜎(2))2min{t, t + s}
]
(𝜎(1))2 + (𝜎(2))2 −2𝜌𝜎(1)𝜎(2)
−t
= t −t
= 0.
Since Vt ∼𝒩(0, t), Vt+s −Vt ∼𝒩(0, s) and the joint distribution of Vt and Vt+s −Vt
is a bivariate normal (see Problem 2.2.1.5, page 58), if Cov (Vt+s −Vt, Vt
) = 0 then
Vt+s −Vt ⟂⟂Vt.
From the results of (a)–(c) we have shown that Vt is a standard Wiener process.
◽
3. Let (Ω, ℱ, ℙ) be a probability space. We consider two assets with prices S(1)
t , S(2)
t
satisfying
the SDEs
dS(1)
t
= 𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
dS(2)
t
= 𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
dW(1)
t dW(2)
t
= 𝜌dt

3.2.3
Multi-Dimensional Diffusion Process
159
where 𝜇(1), 𝜇(2), 𝜎(1), 𝜎(2) are constants and {W(1)
t
∶t ≥0}, {W(2)
t
∶t ≥0} are standard
Wiener processes with correlation 𝜌.
By letting Ut = S(1)
t S(2)
t , show that the SDE satisfied by Ut is
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) + 𝜇(2) + 𝜌𝜎(1)𝜎(2), 𝜎=
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2) and
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2) .
Show that {Vt ∶t ≥0} is a standard Wiener process.
Solution: From It¯o’s formula,
dUt = d(S(1)
t S(2)
t )
= S(1)
t dS(2)
t
+ S(2)
t dS(1)
t
+ (dS(1)
t )(dS(2)
t ) + . . .
= S(1)
t
(
𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
)
+ S(2)
t
(
𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
)
+𝜌𝜎(1)𝜎(2)S(1)
t S(2)
t dt
= (𝜇(1) + 𝜇(2) + 𝜌𝜎(1)𝜎(2)) Utdt + Ut
(
𝜎(1)dW(1)
t
+ 𝜎(2)dW(2)
t
)
= (𝜇(1) + 𝜇(2) + 𝜌𝜎(1)𝜎(2)) Utdt +
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2)UtdVt.
Therefore,
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) + 𝜇(2) + 𝜌𝜎(1)𝜎(2), 𝜎=
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2) and
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2) .
Following the same procedure as described in Problem 3.2.3.2 (page 156), we can show
that
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
√
(𝜎(1))2 + (𝜎(2))2 + 2𝜌𝜎(1)𝜎(2) ∼N(0, t)
and is also a standard Wiener process.
◽
4. Let (Ω, ℱ, ℙ) be a probability space. We consider three assets with prices S(1)
t , S(2)
t
and S(3)
t
satisfying the SDEs
dS(1)
t
= 𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
dS(2)
t
= 𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t

160
3.2.3
Multi-Dimensional Diffusion Process
dS(3)
t
= 𝜇(3)S(3)
t dt + 𝜎(3)S(3)
t dW(3)
t
dW(i)
t dW(j)
t
= 𝜌ijdt,
i ≠j
and
i, j = 1, 2, 3
where 𝜇(1), 𝜇(2), 𝜇(3), 𝜎(1), 𝜎(2), 𝜎(3) are constants and {W(1)
t
∶t ≥0}, {W(2)
t
∶t ≥0},
{W(3)
t
∶t ≥0} are standard Wiener processes with correlations dW(i)
t dW(j)
t
= 𝜌ijdt, i ≠j
and (dW(i)
t )2 = dt for i, j = 1, 2, 3.
By letting Ut = (S(1)
t S(2)
t )∕S(3)
t , show that the SDE satisfied by Ut is
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) + 𝜇(2) −𝜇(3) + 𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) −𝜌23𝜎(2)𝜎(3), 𝜎2 = (𝜎(1))2 +
(𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3) and
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3) .
Show that {Vt ∶t ≥0} is a standard Wiener process.
Solution: From It¯o’s lemma,
dUt = 𝜕Ut
𝜕S(1)
t
dS(1)
t
+ 𝜕Ut
𝜕S(2)
t
dS(2)
t
+ 𝜕Ut
𝜕S(3)
t
dS(3)
t
+1
2
𝜕2Ut
𝜕(S(1)
t )2 (dS(1)
t )2 + 1
2
𝜕2Ut
𝜕(S(2)
t )2 (dS(2)
t )2 + 1
2
𝜕2Ut
𝜕(S(3)
t )2 (dS(3)
t )2
+
𝜕2Ut
𝜕S(1)
t 𝜕S(2)
t
dS(1)
t dS(2)
t
+
𝜕2Ut
𝜕S(1)
t 𝜕S(3)
t
dS(1)
t dS(3)
t
+
𝜕2Ut
𝜕S(2)
t 𝜕S(3)
t
dS(2)
t dS(3)
t
+ . . .
= S(2)
t
S(3)
t
(
𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
)
+ S(1)
t
S(3)
t
(
𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
)
−S(1)
t S(2)
t
(S(3)
t )2
(
𝜇(3)S(3)
t dt + 𝜎(3)S(3)
t dW(3)
t
)
+ S(1)
t S(2)
t
(S(3)
t )3
(
(𝜎(3)S(3)
t )2dt
)
+ 1
S(3)
t
(
𝜌12𝜎(1)𝜎(2)S(1)
t S(2)
t dt
)
−
S(2)
t
(S(3)
t )2
(
𝜌13𝜎(1)𝜎(3)S(1)
t S(3)
t dt
)
−S(1)
t
(S(3)
t )2
(
𝜌23𝜎(2)𝜎(3)S(2)
t S(3)
t dt
)
= (𝜇(1) + 𝜇(2) −𝜇(3) + 𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) −𝜌23𝜎(2)𝜎(3)) Utdt
+Ut
(
𝜎(1)dW(1)
t
+ 𝜎(2)dW(2)
t
−𝜎(3)dW(3)
t
)
= (𝜇(1) + 𝜇(2) −𝜇(3) + 𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) −𝜌23𝜎(2)𝜎(3)) Utdt

3.2.3
Multi-Dimensional Diffusion Process
161
+
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3)UtdVt
which therefore becomes
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) + 𝜇(2) −𝜇(3) + 𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) −𝜌23𝜎(2)𝜎(3), 𝜎2 = (𝜎(1))2 +
(𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3) and
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3) .
Following the steps described in Problem 3.2.3.2 (page 156), we can easily show that
Vt =
𝜎(1)W(1)
t
+ 𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 + 2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) −2𝜌23𝜎(2)𝜎(3) ∼𝒩(0, 1)
and is also a standard Wiener process.
◽
5. Let (Ω, ℱ, ℙ) be a probability space. We consider three assets with prices S(1)
t , S(2)
t
and S(3)
t
satisfying the SDEs
dS(1)
t
= 𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
dS(2)
t
= 𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
dS(3)
t
= 𝜇(3)S(3)
t dt + 𝜎(3)S(3)
t dW(3)
t
dW(i)
t dW(j)
t
= 𝜌ijdt,
i ≠j
and
i, j = 1, 2, 3
where 𝜇(1), 𝜇(2), 𝜇(3), 𝜎(1), 𝜎(2), 𝜎(3) are constants and {W(1)
t
∶t ≥0}, {W(2)
t
∶t ≥0},
{W(3)
t
∶t ≥0} are standard Wiener processes with correlations dW(i)
t dW(j)
t
= 𝜌ijdt, i ≠j
and (dW(i)
t )2 = dt for i, j = 1, 2, 3.
By letting Ut = S(1)
t ∕(S(2)
t S(3)
t ), show that the SDE satisfied by Ut is
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) −𝜇(2) −𝜇(3) + (𝜎(2))2 + (𝜎(3))2 −𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) + 𝜌23𝜎(2)𝜎(3),
𝜎2 = (𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3) and
Vt =
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3) .
Show that {Vt ∶t ≥0} is a standard Wiener process.

162
3.2.3
Multi-Dimensional Diffusion Process
Solution: Applying It¯o’s lemma,
dUt = 𝜕Ut
𝜕S(1)
t
dS(1)
t
+ 𝜕Ut
𝜕S(2)
t
dS(2)
t
+ 𝜕Ut
𝜕S(3)
t
dS(3)
t
+1
2
𝜕2Ut
𝜕(S(1)
t )2 (dS(1)
t )2 + 1
2
𝜕2Ut
𝜕(S(2)
t )2 (dS(2)
t )2 + 1
2
𝜕2Ut
𝜕(S(3)
t )2 (dS(3)
t )2
+
𝜕2Ut
𝜕S(1)
t 𝜕S(2)
t
dS(1)
t dS(2)
t
+
𝜕2Ut
𝜕S(1)
t 𝜕S(3)
t
dS(1)
t dS(3)
t
+
𝜕2Ut
𝜕S(2)
t 𝜕S(3)
t
dS(2)
t dS(3)
t
+ . . .
=
1
S(2)
t S(3)
t
(
𝜇(1)S(1)
t dt + 𝜎(1)S(1)
t dW(1)
t
)
−
S(1)
t
(S(2)
t )2S(3)
t
(
𝜇(2)S(2)
t dt + 𝜎(2)S(2)
t dW(2)
t
)
−
S(1)
t
S(2)
t (S(3)
t )2
(
𝜇(3)S(3)
t dt + 𝜎(3)S(3)
t dW(3)
t
)
+
S(1)
t
(S(2)
t )3S(3)
t
(
(𝜎(2)S(2)
t )2dt
)
+
S(1)
t
S(2)
t (S(3)
t )3
(
(𝜎(3)S(3)
t )2dt
)
−
1
(S(2)
t )2S(3)
t
(
𝜌12𝜎(1)𝜎(2)S(1)
t S(2)
t dt
)
−
1
S(2)
t (S(3)
t )2
(
𝜌13𝜎(1)𝜎(3)S(1)
t S(3)
t dt
)
+
S(1)
t
(S(2)
t S(3)
t )2
(
𝜌23𝜎(2)𝜎(3)S(2)
t S(3)
t dt
)
= (𝜇(1) −𝜇(2) −𝜇(3) + (𝜎(2))2 + (𝜎(3))2 −𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) + 𝜌23𝜎(2)𝜎(3)) Utdt
+Ut
(
𝜎(1)dW(1)
t
−𝜎(2)dW(2)
t
−𝜎(3)dW(3)
t
)
= (𝜇(1) −𝜇(2) −𝜇(3) + (𝜎(2))2 + (𝜎(3))2 −𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) + 𝜌23𝜎(2)𝜎(3)) Utdt
+
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3)UtdVt.
We can therefore write
dUt = 𝜇Utdt + 𝜎UtdVt
where 𝜇= 𝜇(1) −𝜇(2) −𝜇(3) + (𝜎(2))2 + (𝜎(3))2 −𝜌12𝜎(1)𝜎(2) −𝜌13𝜎(1)𝜎(3) + 𝜌23𝜎(2)𝜎(3),
𝜎2 = (𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3) and
Vt =
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3) .
As described in Problem 3.2.3.2 (page 156) we can easily show that
Vt =
𝜎(1)W(1)
t
−𝜎(2)W(2)
t
−𝜎(3)W(3)
t
√
(𝜎(1))2 + (𝜎(2))2 + (𝜎(3))2 −2𝜌12𝜎(1)𝜎(2) −2𝜌13𝜎(1)𝜎(3) + 2𝜌23𝜎(2)𝜎(3) ∼𝒩(0, t)
and is a standard Wiener process.
◽

3.2.3
Multi-Dimensional Diffusion Process
163
6. Bessel Process. Let (𝛀, ℱ, ℙ) be a probability space and let Wt =
(
W(1)
t , W(2)
t , . . . , W(n)
t
)
be an n-dimensional standard Wiener process, with W(i)
t , i = 1, 2, . . . , n being independent
one-dimensional standard Wiener processes. By setting
Xt =
√
(W(1)
t )2 + (W(2)
t )2 + . . . + (W(n)
t )2
show that
Wt = ∫
t
0
W(1)
s
Xs
dW(1)
s
+ ∫
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t
0
W(n)
s
Xs
dW(n)
s
is a standard Wiener process.
Show also that Xt satisfies the n-dimensional Bessel process with SDE
dXt =
(
n −1
2Xt
)
dt + dWt
where X0 = 0.
Solution: We first need to show that Wt ∼𝒩(0, t). Using the properties of the stochastic
It¯o integral and the independence property of W(i)
t , i = 1, 2, . . . , n,
𝔼(Wt) = 𝔼
[
∫
t
0
W(1)
s
Xs
dW(1)
s
+ ∫
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t
0
W(n)
s
Xs
dW(n)
s
]
= 𝔼
(
∫
t
0
W(1)
s
Xs
dW(1)
s
)
+ 𝔼
(
∫
t
0
W(2)
s
Xs
dW(2)
s
)
+ . . . + 𝔼
(
∫
t
0
W(n)
s
Xs
dW(n)
s
)
= 0
𝔼(W2
t ) = 𝔼
⎡
⎢
⎢⎣
(
∫
t
0
W(1)
s
Xs
dW(1)
s
)2
+
(
∫
t
0
W(2)
s
Xs
dW(2)
s
)2
+ . . . +
(
∫
t
0
W(n)
s
Xs
dW(n)
s
)2
+ 2
n
∑
i=1
n
∑
j=1
i≠j
(
∫
t
0
W(i)
s
Xs
dW(i)
s
) (
∫
t
0
W(j)
s
Xs
dW(j)
s
)⎤
⎥
⎥
⎥⎦
= 𝔼
⎡
⎢
⎢⎣
(
∫
t
0
W(1)
s
Xs
dW(1)
s
)2⎤
⎥
⎥⎦
+ 𝔼
⎡
⎢
⎢⎣
(
∫
t
0
W(2)
s
Xs
dW(2)
s
)2⎤
⎥
⎥⎦
+ . . . + 𝔼
⎡
⎢
⎢⎣
(
∫
t
0
W(n)
s
Xs
dW(n)
s
)2⎤
⎥
⎥⎦

164
3.2.3
Multi-Dimensional Diffusion Process
= 𝔼
⎡
⎢
⎢⎣∫
t
0
(
W(1)
s
Xs
)2
ds
⎤
⎥
⎥⎦
+ 𝔼
⎡
⎢
⎢⎣∫
t
0
(
W(2)
s
Xs
)2
ds
⎤
⎥
⎥⎦
+ . . . + 𝔼
⎡
⎢
⎢⎣∫
t
0
(
W(n)
s
Xs
)2
ds
⎤
⎥
⎥⎦
= 𝔼
(
∫
t
0
ds
)
= t.
Because Wt is a function of n independent normal variates, so Wt ∼𝒩(0, t). To show that
Wt is also a standard Wiener process, we note the following:
(a) W0 = lim
t→0
[
∫
t
0
W(1)
s
Xs
dW(1)
s
+ ∫
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t
0
W(n)
s
Xs
dW(n)
s
]
= 0 and it is
clear that Wt has continuous sample paths for t ≥0.
(b) For t > 0, u > 0
Wt+u −Wt = ∫
t+u
0
W(1)
s
Xs
dW(1)
s
+ ∫
t+u
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t+u
0
W(n)
s
Xs
dW(n)
s
−∫
t
0
W(1)
s
Xs
dW(1)
s
−∫
t
0
W(2)
s
Xs
dW(2)
s
−. . . −∫
t
0
W(n)
s
Xs
dW(n)
s
= ∫
t+u
t
W(1)
s
Xs
dW(1)
s
+ ∫
t+u
t
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t+u
t
W(n)
s
Xs
dW(n)
s .
Taking expectations,
𝔼(Wt+u −Wt
) = 𝔼
(
∫
t+u
t
W(1)
s
Xs
dW(1)
s
)
+ 𝔼
(
∫
t+u
t
W(2)
s
Xs
dW(2)
s
)
+ . . . + 𝔼
(
∫
t+u
t
W(n)
s
Xs
dW(n)
s
)
= 0
and due to the independence of W(i)
t , i = 1, 2, . . . , n,
𝔼
[(Wt+u −Wt
)2]
= 𝔼
⎡
⎢
⎢⎣
(
∫
t+u
t
W(1)
s
Xs
dW(1)
s
)2
+
(
∫
t+u
t
W(2)
s
Xs
dW(2)
s
)2
+ . . . +
(
∫
t
0
W(n)
s
Xs
dW(n)
s
)2
+ 2
n
∑
i=1
n
∑
j=1
i≠j
(
∫
t+u
t
W(i)
s
Xs
dW(i)
s
) (
∫
t+u
t
W(j)
s
Xs
dW(j)
s
)⎤
⎥
⎥
⎥⎦

3.2.3
Multi-Dimensional Diffusion Process
165
= 𝔼
⎡
⎢
⎢⎣
(
∫
t+u
t
W(1)
s
Xs
dW(1)
s
)2⎤
⎥
⎥⎦
+ 𝔼
⎡
⎢
⎢⎣
(
∫
t+u
t
W(2)
s
Xs
dW(2)
s
)2⎤
⎥
⎥⎦
+ . . . + 𝔼
⎡
⎢
⎢⎣
(
∫
t+u
t
W(n)
s
Xs
dW(n)
s
)2⎤
⎥
⎥⎦
= 𝔼
⎡
⎢
⎢⎣∫
t+u
t
(
W(1)
s
Xs
)2
ds
⎤
⎥
⎥⎦
+ 𝔼
⎡
⎢
⎢⎣∫
t+u
t
(
W(2)
s
Xs
)2
ds
⎤
⎥
⎥⎦
+ . . . + 𝔼
⎡
⎢
⎢⎣∫
t+u
t
(
W(n)
s
Xs
)2
ds
⎤
⎥
⎥⎦
= 𝔼
[
∫
t+u
t
ds
]
= u.
Thus, Wt+u −Wt ∼𝒩(0, u).
(c) To show that Wt+u −Wt ⟂⟂Wt we note that from the independent increment property
of W(i)
t , i = 1, 2, . . . , n,
𝔼[(Wt+u −Wt
) Wt
]
= 𝔼(Wt+uWt
) −𝔼(W2
t
)
= 𝔼
[(
∫
t+u
0
W(1)
s
Xs
dW(1)
s
+ ∫
t+u
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t+u
0
W(n)
s
Xs
dW(n)
s
)
×
(
∫
t
0
W(1)
s
Xs
dW(1)
s
+ ∫
t
0
W(2)
s
Xs
dW(2)
s
+ . . . + ∫
t
0
W(n)
s
Xs
dW(n)
s
)]
−𝔼
(
W2
t
)
=
n
∑
i=1
𝔼
⎡
⎢
⎢⎣
(
∫
t
0
W(i)
s
Xs
dW(i)
s
)2⎤
⎥
⎥⎦
+
n
∑
i=1
𝔼
[(
∫
t+u
t
W(i)
s
Xs
dW(i)
s
) (
∫
t
0
W(i)
s
Xs
dW(i)
s
)]
+
n
∑
i=1
n
∑
j=1
i≠j
𝔼
[(
∫
t+u
t
W(i)
s
Xs
dW(i)
s
) (
∫
t
0
W(j)
s
Xs
dW(j)
s
)]
−𝔼
(
W2
t
)
=
n
∑
i=1
𝔼
⎡
⎢
⎢⎣∫
t
0
(
W(i)
s
Xs
)2
ds
⎤
⎥
⎥⎦
−𝔼
(
W2
t
)
= t −t
= 0.
Since Wt ∼𝒩(0, t), Wt+s −Wt ∼𝒩(0, s) and the joint distribution of Wt and Wt+s −
Wt is a bivariate normal (see Problem 2.2.1.5, page 58), if Cov(Wt+s −Wt, Wt) = 0
then Wt+s −Wt ⟂⟂Wt.

166
3.2.3
Multi-Dimensional Diffusion Process
Thus, from the results of (a)–(c) we have shown that Wt is a standard Wiener process.
From It¯o’s formula and given dW(i)
t dW(j)
t
= 0, i ≠j,
dXt =
n
∑
i=1
𝜕Xt
𝜕W(i)
t
dW(i)
t
+ 1
2
n
∑
i=1
𝜕2Xt
𝜕(W(i)
t )2 (dW(i)
t )2
=
n
∑
i=1
𝜕Xt
𝜕W(i)
t
dW(i)
t
+ 1
2
n
∑
i=1
𝜕2Xt
𝜕(W(i)
t )2 dt
=
n
∑
i=1
W(i)
t
Xt
dW(i)
t
+ 1
2
⎡
⎢
⎢
⎢⎣
n
∑
i=1
Xt −X−1
t
(
W(i)
t
)2
X2
t
⎤
⎥
⎥
⎥⎦
dt
= dWt + 1
2
⎡
⎢
⎢
⎢⎣
n
Xt
−
∑n
i=1
(
W(i)
t
)2
X3
t
⎤
⎥
⎥
⎥⎦
= dWt + 1
2
(
n
Xt
−X2
t
X3
t
)
= dWt +
(
n −1
2Xt
)
dt.
Therefore,
Xt =
√
(W(1)
t )2 + (W(2)
t )2 + . . . + (W(n)
t )2
satisfies the SDE
dXt =
(
n −1
2Xt
)
dt + dWt.
◽
7. Forward–Spot Price Relationship II. Let (Ω, ℱ, ℙ) be a probability space. We consider
the forward price F(t, T) of an asset St satisfying the SDE
dF(t, T)
F(t, T) = 𝜎1(t, T) dW(1)
t
+ 𝜎2(t, T) dW(2)
t
where 𝜎1(t, T) > 0, 𝜎2(t, T) > 0 are time-dependent volatilities and {W(1)
t
∶t ≥0},
{W(2)
t
∶t ≥0} are standard Wiener processes with correlation 𝜌∈(−1, 1). Given the
relationship F(t, T) = Ster(T−t) where r is the risk-free interest rate, show that
dSt
St
=
{𝜕log F(0, t)
𝜕t
−∫
t
0
[
𝜎1(u, t)𝜕𝜎1(u, t)
𝜕t
+ 𝜌
(
𝜎1(u, t)𝜕𝜎2(u, t)
𝜕t
+ 𝜎2(u, t)𝜕𝜎1(u, t)
𝜕t
)
+ 𝜎2(u, t)𝜕𝜎2(u, t)
𝜕t
]
du + ∫
t
0
𝜕𝜎1(u, t)
𝜕t
dW(1)
u
+ ∫
t
0
𝜕𝜎2(u, t)
𝜕t
dW(2)
u
}
dt + 𝜎(t, t) dWt

3.2.3
Multi-Dimensional Diffusion Process
167
where
𝜎(t, t) =
√
𝜎1(t, t)2 + 2𝜌𝜎1(t, t)𝜎2(t, t) + 𝜎2(t, t)2
and
Wt =
𝜎1(t, t)W(1)
t
+ 𝜎2(t, t)W(2)
t
√
𝜎1(t, t)2 + 2𝜌𝜎1(t, t)𝜎2(t, t) + 𝜎2(t, t)2
is a standard Wiener process.
Solution: Expanding log F(t, T) using Taylor’s theorem and then applying It¯o’s lemma,
we have
d log F(t, T) =
1
F(t, T)dF(t, T) −
1
2F(t, T)2 dF(t, T)2 + . . .
= 𝜎1(t, T) dW(1)
t
+ 𝜎2(t, T) dW(2)
t
−1
2
[𝜎1(t, T)2 + 𝜎2(t, T)2 + 2𝜌𝜎1(t, T)𝜎2(t, T)] dt
and taking integrals,
log
( F(t, T)
F(0, T)
)
= ∫
t
0
𝜎1(u, T) dW(1)
u
+ ∫
t
0
𝜎2(u, T) dW(2)
u
−1
2 ∫
t
0
[𝜎1(u, T)2 + 𝜎2(u, T)2 + 2𝜌𝜎1(u, T)𝜎2(u, T)] du
We finally have
F(t, T) = F(0, T)e−1
2 ∫t
0 [𝜎1(u,T)2+𝜎2(u,T)2+2𝜌𝜎1(u,T)𝜎2(u,T)]du+∫t
0 𝜎1(u,T)dW(1)
u +∫t
0 𝜎2(u,T)dW(2)
u .
By setting T = t and taking note that St = F(t, t), the spot price St has the expression
St = F(0, t)e−1
2 ∫t
0 [𝜎1(u,t)2+𝜎2(u,t)2+2𝜌𝜎1(u,t)𝜎2(u,t)]du+∫t
0 𝜎1(u,t)dW(1)
u +∫t
0 𝜎2(u,t)dW(2)
u .
To find the SDE for St, we apply It¯o’s lemma
dSt = 𝜕St
𝜕t dt +
𝜕St
𝜕W(1)
t
dW(1)
t
+
𝜕St
𝜕W(2)
t
dW(2)
t
+ 1
2
𝜕2St
𝜕(W(1)
t )2 (dW(1)
t )2 + 1
2
𝜕2St
𝜕(W(2)
t )2 (dW(2)
t )2 +
𝜕2St
𝜕W(1)
t 𝜕W(2)
t
dW(1)
t dW(2)
t
+ . . .
=
(
𝜕St
𝜕t + 1
2
𝜕2St
𝜕(W(1)
t )2 + 1
2
𝜕2St
𝜕(W(2)
t )2 + 𝜌
𝜕2St
𝜕W(1)
t 𝜕W(2)
t
)
dt +
𝜕St
𝜕W(1)
t
dW(1)
t
+ 𝜕St
𝜕W(2)
t
dW(2)
t .

168
3.2.3
Multi-Dimensional Diffusion Process
Taking partial differentiations of St, we have
𝜕St
𝜕t = 𝜕log F(0, t)
𝜕t
−1
2
[𝜎1(t, t)2 + 𝜎2(t, t)2 + 2𝜌𝜎1(t, t)𝜎2(t, t)]
−∫
t
0
[
𝜎1(u, t)𝜕𝜎1(u, t)
𝜕t
+ 𝜌
(
𝜎1(u, t)𝜕𝜎2(u, t)
𝜕t
+ 𝜎2(u, t)𝜕𝜎1(u, t)
𝜕t
)
+ 𝜎2(u, t)𝜕𝜎2(u, t)
𝜕t
]
du
+ ∫
t
0
𝜕𝜎1(u, t)
𝜕t
dW(1)
u
+ ∫
t
0
𝜕𝜎2(u, t)
𝜕t
dW(2)
u ,
𝜕St
𝜕W(1)
t
= 𝜎1(t, t)St,
𝜕St
𝜕W(2)
t
= 𝜎2(t, t)St,
𝜕2St
𝜕(W(1)
t )2 = 𝜎1(t, t)2St,
𝜕2St
𝜕(W(2)
t )2 = 𝜎2(t, t)2St,
𝜕2St
𝜕W(1)
t 𝜕W(2)
t
= 𝜎1(t, t)𝜎2(t, t)St
and substituting them into the SDE we eventually have
dSt
St
=
{𝜕log F(0, t)
𝜕t
−∫
t
0
[
𝜎1(u, t)𝜕𝜎1(u, t)
𝜕t
+ 𝜌
(
𝜎1(u, t)𝜕𝜎2(u, t)
𝜕t
+ 𝜎2(u, t)𝜕𝜎1(u, t)
𝜕t
)
+ 𝜎2(u, t)𝜕𝜎2(u, t)
𝜕t
]
du + ∫
t
0
𝜕𝜎1(u, t)
𝜕t
dW(1)
u
+ ∫
t
0
𝜕𝜎2(u, t)
𝜕t
dW(2)
u
}
dt
+ 𝜎1(t, t) dW(1)
t
+ 𝜎2(t, t) dW(2)
t
=
{𝜕log F(0, t)
𝜕t
−∫
t
0
[
𝜎1(u, t)𝜕𝜎1(u, t)
𝜕t
+ 𝜌
(
𝜎1(u, t)𝜕𝜎2(u, t)
𝜕t
+ 𝜎2(u, t)𝜕𝜎1(u, t)
𝜕t
)
+ 𝜎2(u, t)𝜕𝜎2(u, t)
𝜕t
]
du + ∫
t
0
𝜕𝜎1(u, t)
𝜕t
dW(1)
u
+ ∫
t
0
𝜕𝜎2(u, t)
𝜕t
dW(2)
u
}
dt
+ 𝜎(t, t) dWt
where
𝜎(t, t) =
√
𝜎1(t, t)2 + 2𝜌𝜎1(t, t)𝜎2(t, t) + 𝜎2(t, t)2
and from the steps discussed in Problem 3.2.3.2 (page 156) we can prove that
Wt =
𝜎1(t, t)W(1)
t
+ 𝜎2(t, t)W(2)
t
√
𝜎1(t, t)2 + 2𝜌𝜎1(t, t)𝜎2(t, t) + 𝜎2(t, t)2 ∼𝒩(0, t)
and is also a standard Wiener process.
◽

3.2.3
Multi-Dimensional Diffusion Process
169
8. Gabillon 2-Factor Model. Let {W(s)
t
∶t ≥0} and {W(l)
t
∶t ≥0} be standard Wiener pro-
cesses on the probability space (Ω, ℱ, ℙ) with correlation 𝜌∈(−1, 1). Suppose the for-
ward curve F(t, T) follows the process
dF(t, T)
F(t, T) = 𝜎se−𝛼(T−t)dW(s)
t
+ 𝜎l(1 −e−𝛼(T−t)) dW(l)
t
where t < T, 𝛼is the mean-reversion parameter and 𝜎s and 𝜎l are the short-term and
long-term volatilities, respectively.
By setting
dW(s)
t
= dW(1)
t
and dW(l)
t
= 𝜌dW(1)
t
+
√
1 −𝜌2dW(2)
t
where W(1)
t
and W(2)
t
are standard Wiener processes, W(1)
t
⟂⟂W(2)
t , show that
dF(t, T)
F(t, T) = 𝜎(t, T) dWt
where
𝜎(t, T) =
√
𝜎2
l + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−t) + 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−t)
and
Wt =
(𝜎se−𝛼(T−t) + 𝜌𝜎l
(1 −e−𝛼(T−t))) W(1)
t
+
√
1 −𝜌2 (1 −e−𝛼(T−t)) W(2)
t
√
𝜎2
l + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−t) + 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−t)
is a standard Wiener process.
Finally, show that conditional on F(0, T), F(t, T) follows a lognormal distribution with
mean
𝔼[F(t, T)| F(0, T)] = F(0, T)
and variance
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
𝜎2
l t + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) (
e−2𝛼(T−t) −e−2𝛼T
2𝛼
)
+2 (𝜌𝜎s𝜎l −𝜎2
l
) (
e−𝛼(T−t) −e−𝛼T
𝛼
)
−1
}
.
Solution: By defining dW(s)
t
= dW(1)
t
and dW(l)
t
= 𝜌dW(1)
t
+
√
1 −𝜌2dW(2)
t
such that
W(1)
t
⟂⟂W(2)
t , the SDE of F(t, T) can be expressed as
dF(t, T)
F(t, T) = 𝜎se−𝛼(T−t)dW(1)
t
+ 𝛼l
(1 −e−𝛼(T−t)) (
𝜌dW(1)
t
+
√
1 −𝜌2dW(2)
t
)
= (𝜎se−𝛼(T−t) + 𝜌𝜎l(1 −e−𝛼(T−t))) dW(1)
t
+
√
1 −𝜌2𝜎l
(1 −e−𝛼(T−t)) dW(2)
t
= 𝜎(t, T) dWt

170
3.2.3
Multi-Dimensional Diffusion Process
where, owing to the fact that dW(1)
t
⋅dW(2)
t
= 0, we have
𝜎(t, T)2 = (𝜎se−𝛼(T−t) + 𝜌𝜎l
(1 −e−𝛼(T−t)))2 + (1 −𝜌2) 𝜎2
l
(1 −e−𝛼(T−t))2
= 𝜎2
s e−2𝛼(T−t) + 2𝜌𝜎s𝜎le−𝛼(T−t) (1 −e−𝛼(T−t)) + 𝜎2
l
(1 −e−𝛼(T−t))2
= 𝜎2
l + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−t) + 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−t)
and using the steps described in Problem 3.2.3.2 (page 156), we can easily show that
Wt =
(𝜎se−𝛼(T−t) + 𝜌𝜎l
(1 −e−𝛼(T−t))) W(1)
t
+
√
1 −𝜌2 (1 −e−𝛼(T−t)) W(2)
t
√
𝜎2
l + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−t) + 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−t)
∼𝒩(0, t)
is also a standard Wiener process.
By expanding d log F(t, T) using Taylor’s theorem and applying It¯o’s formula, we have
d log F(t, T) = dF(t, T)
F(t, T) −1
2
(dF(t, T)
F(t, T)
)2
+ . . .
= 𝜎(t, T) dWt −1
2𝜎(t, T)2dt.
Taking integrals from 0 to t,
∫
t
0
d log F(u, T) = ∫
t
0
𝜎(u, T) dWu −1
2 ∫
t
0
𝜎(u, T)2 du
log F(t, T) = log F(0, T) + ∫
t
0
𝜎(u, T) dWu −1
2 ∫
t
0
𝜎(u, T)2 du.
From the property of It¯o’s integral, we have
𝔼
[
∫
t
0
𝜎(u, T) dWu
]
= 0
and
𝔼
[(
∫
t
0
𝜎(u, T) dWu
)2]
= 𝔼
[
∫
t
0
𝜎(u, T)2 du
]
= ∫
t
0
𝜎(u, T)2 du.
Since the It¯o integral ∫
t
0
𝜎(u, T) dWu is in the form ∫
t
0
f(u) dWu, from Problem 3.2.2.4
(page 126) we can easily prove that ∫
t
0
𝜎(u, T) dWu follows a normal distribution,
∫
t
0
𝜎(u, T) dWu ∼𝒩
(
0, ∫
t
0
𝜎(u, T)2 du
)
and hence
log F(t, T) ∼𝒩
(
log F(0, T) −1
2 ∫
t
0
𝜎(u, T)2 du, ∫
t
0
𝜎(u, T)2 du
)
.

3.2.3
Multi-Dimensional Diffusion Process
171
Solving the integral,
∫
t
0
𝜎(u, T)2 du = ∫
t
0
[𝜎2
l + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−u) + 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−u)] du
= 𝜎2
l u + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) e−2𝛼(T−u)
2𝛼
+ 2 (𝜌𝜎s𝜎l −𝜎2
l
) e−𝛼(T−u)
𝛼
||||
t
0
= 𝜎2
l t + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) (
e−2𝛼(T−t) −e−2𝛼T
2𝛼
)
+ 2 (𝜌𝜎s𝜎l −𝜎2
l
) (
e−𝛼(T−t) −e−𝛼T
𝛼
)
.
Since F(t, T) conditional on F(0, T) follows a lognormal distribution, we have
𝔼[F(t, T)|F(0, T)] = F(0, T)
and
Var [F(t, T)| F(0, T)] = F(0, T)2 exp
{
𝜎2
l t + (𝜎2
s −2𝜌𝜎s𝜎l + 𝜎2
l
) (
e−2𝛼(T−t) −e−2𝛼T
2𝛼
)
+ 2 (𝜌𝜎s𝜎l −𝜎2
l
) (
e−𝛼(T−t) −e−𝛼T
𝛼
)
−1
}
.
◽
9. Integrated Square-Root Process. Let (Ω, ℱ, ℙ) be a probability space and let {Wt ∶t ≥0}
be a standard Wiener process. Suppose Xt follows the CIR model with SDE
dXt = 𝜅(𝜃−Xt) dt + 𝜎
√
XtdWt,
X0 > 0
where 𝜅, 𝜃and 𝜎are constants. We consider the integral
Yt = ∫
t
t0
Xu du,
Xt0 > 0
as the integrated square-root process of Xt up to time t from initial time t0, t0 < t.
Show that for n ≥1, m ≥1, n, m ∈ℕ, the process Xn
t Ym
t satisfies the SDE
d(Xn
t Ym
t )
Xn
t Ym
t
=
[(
n𝜅(𝜃−Xt
) + 1
2n(n −1)𝜎2)
X−1
t
+ mXtY−1
t
]
dt + n𝜎X
−1
2
t
dWt
and hence show that 𝔼(Xn
t Ym
t |Xt0) satisfies the following first-order ordinary differential
equation
d
dt𝔼
(
Xn
t Ym
t || Xt0
)
= n𝜅𝜃𝔼
(
Xn−1
t
Ym
t ||| Xt0
)
−n𝜅𝔼
(
Xn
t Ym
t || Xt0
)
+1
2n(n −1)𝜎2𝔼
(
Xn
t Ym
t |||Xt0
)
+ m𝔼
(
Xn+1
t
Ym−1
t
|||Xt0
)
.
Finally, find the first two moments of Yt, given Xt0.

172
3.2.3
Multi-Dimensional Diffusion Process
Solution: From Taylor’s theorem and subsequently applying It¯o’s formula, we can write
d(Xn
t ) = nXn−1
t
dXt + 1
2n(n −1)Xn−2
t
(dX2
t ) + . . .
= nXn−1
t
[𝜅(𝜃−Xt) dt + 𝜎
√
XtdWt] + 1
2n(n −1)Xn−2
t
(𝜎2Xtdt)
=
[
n𝜅(𝜃−Xt
) Xn−1
t
+ 1
2n(n −1)𝜎2Xn−1
t
]
dt + n𝜎X
n−1
2
t
dWt
and
d(Ym
t ) = mYm−1
t
dYt + 1
2m(m −1)Ym−2
t
(dYt)2 + . . .
= mXtYm−1
t
dt.
Thus,
d(Xn
t Ym
t ) = Ym
t d(Xn
t ) + Xn
t d(Ym
t ) + d(Xn
t )d(Ym
t )
=
[(
n𝜅(𝜃−Xt
) + 1
2n(n −1)𝜎2)
Xn−1
t
Ym
t
]
dt + n𝜎X
n−1
2
t
Ym
t dWt
+ mXn+1
t
Ym−1
t
dt
or
d(Xn
t Ym
t )
Xn
t Ym
t
=
[(
n𝜅(𝜃−Xt
) + 1
2n(n −1)𝜎2)
X−1
t
+ mXtY−1
t
]
dt + n𝜎X
−1
2
t
dWt.
Taking integrals,
∫
t
t0
d(Xn
uYm
u ) = ∫
t
t0
[
n𝜅(𝜃−Xu
) Xn−1
u
Ym
u + 1
2n(n −1)𝜎2Xn−1
u
Ym
u + mXn+1
u
Ym−1
u
]
du
+ ∫
t
t0
n𝜎X
n−1
2
u
Ym
u dWu
Xn
t Ym
t = Xn
t0Ym
t0 +∫
t
t0
[
n𝜅(𝜃−Xu
) Xn−1
u Ym
u + 1
2n(n −1)𝜎2Xn−1
u
Ym
u + mXn+1
u
Ym−1
u
]
du
+∫
t
t0
n𝜎X
n−1
2
u
Ym
u dWu
and taking expectations given Xt0 and because Yt0 = ∫
t0
t0
Xudu = 0, we have
𝔼[Xn
t Ym
t |Xt0] = ∫
t
t0
𝔼[n𝜅(𝜃−Xu)Xn−1
u
Ym
u |Xt0]du + ∫
t
t0
𝔼
[1
2n(n −1)𝜎2Xn−1
u
Ym
u
||||
Xt0
]
du

3.2.3
Multi-Dimensional Diffusion Process
173
+ ∫
t
t0
𝔼[mXn+1
u
Ym−1
u
||| Xt0
]
du + ∫
t
t0
𝔼
[
n𝜎X
n−1
2
u
Ym
u
||||
Xt0
]
dWu
= ∫
t
t0
𝔼
[
n𝜅(𝜃−Xu
) Xn−1
u
Ym
u |||Xt0
]
du + ∫
t
t0
𝔼
[1
2n(n −1)𝜎2Xn−1
u
Ym
u |||Xt0
]
du
+ ∫
t
t0
𝔼
[
mXn+1
u
Ym−1
u
|||Xt0
]
du.
Differentiating with respect to t yields
d
dt𝔼
[
Xn
t Ym
t |||Xt0
]
= n𝜅𝔼
[(𝜃−Xt
) Xn−1
t
Ym
t |||Xt0
]
+ 1
2n(n −1)𝜎2𝔼
[
Xn−1
t
Ym
t |||Xt0
]
+ m𝔼
[
Xn+1
t
Ym−1
t
|||Xt0
]
= n𝜅𝜃𝔼
[
Xn−1
t
Ym
t |||Xt0
]
−n𝜅𝔼
[
Xn
t Ym
t |||Xt0
]
+ 1
2n(n −1)𝜎2𝔼
[
Xn−1
t
Ym
t |||Xt0
]
+ m𝔼
[
Xn+1
t
Ym−1
t
|||Xt0
]
.
To find the mean of Yt given Xt0, we let n = 0 and m = 1 so that
d
dt𝔼
[
Yt|||Xt0
]
= 𝔼
[
Xt|||Xt0
]
.
From Problem 3.2.2.12 (page 135), we have
𝔼
[
Xt|||Xt0
]
= Xt0e−𝜅(t−t0) + 𝜃(1 −e−𝜅(t−t0))
and therefore,
𝔼
[
Yt|||Xt0
]
= ∫
t
t0
[
Xt0e−𝜅(u−t0) + 𝜃
(
1 −e−𝜅(u−t0))]
du
= 𝜃(t −t0) + 1
𝜅
(
Xt0 −𝜃
) (1 −e−𝜅(t−t0)) .
For the second moment of Yt given Xt0, we set n = 0 and m = 2 so that
d
dt𝔼
[
Y2
t |||Xt0
]
= 2𝔼
[
XtYt|||Xt0
]
and to find 𝔼[XtYt|Xt0], we set n = 1 and m = 1 so that
d
dt𝔼
[
XtYt|||Xt0
]
= 𝜅𝜃𝔼
[
Yt|||Xt0
]
−𝜅𝔼
[
XtYt|||Xt0
]
+ 𝔼
[
X2
t |||Xt0
]
or
d
dt𝔼
[
XtYt|||Xt0
]
+ 𝜅𝔼
[
XtYt|||Xt0
]
= 𝜅𝜃𝔼
[
Yt|||Xt0
]
+ 𝔼
[
X2
t |||Xt0
]
.

174
3.2.3
Multi-Dimensional Diffusion Process
By setting the integrating factor I = e𝜅t, the solution of the first-order differential
equation is
d
dt
[
e𝜅t𝔼
[
XtYt|||Xt0
]]
= 𝜅𝜃𝔼
[
Yt|||Xt0
]
+ 𝔼
[
X2
t |||Xt0
]
or
𝔼
[
XtYt|||Xt0
]
= e−𝜅t
∫
t
t0
{
𝜅𝜃𝔼
[
Yu|||Xt0
]
+ 𝔼
[
X2
u|||Xt0
]}
du.
Since
𝔼
[
Yt|||Xt0
]
= 𝜃(t −t0) + 1
𝜅
(
Xt0 −𝜃
) (1 −e−𝜅(t−t0))
and from Problem 3.2.2.12 (page 135),
𝔼
[
X2
t |||Xt0
]
= X2
t0e−2𝜅(t−t0) +
(
2𝜅𝜃+ 𝜎2
𝜅
)
(Xt0 −𝜃) (e−𝜅(t−t0) −e−2𝜅(t−t0))
+
𝜃(2𝜅𝜃+ 𝜎2)
2𝜅
(1 −e−2𝜅(t−t0))
we can easily show that
𝔼
[
XtYt|||Xt0
]
= 1
2𝜅𝜃2(t −t0)2e−𝜅t + 𝜃(t −t0)
(
Xt0 + 𝜎2
2𝜅
)
e−𝜅t
+
(
𝜅𝜃+ 𝜎2
𝜅2
)
(Xt0 −𝜃)
(
1 −e−𝜅(t−t0))
e−𝜅t
+ 1
2𝜅
[
X2
t0 −
(
2𝜅𝜃+ 𝜎2
𝜅
) (
Xt0 −1
2𝜃
)] (1 −e−2𝜅(t−t0)) e−𝜅t.
Finally, by substituting the result of 𝔼[XtYt|Xt0] into
𝔼
[
Y2
t |||Xt0
]
= 2 ∫
t
t0
𝔼
[
XuYu|||Xt0
]
and using integration by parts, we eventually arrive at
𝔼
[
Y2
t |||Xt0
]
= 1
𝜅2
[
X2
t0 +
(
4𝜃+ 𝜎2
𝜅
)
Xt0 + 𝜃
(
𝜃−𝜎2
2𝜅
)] (1 −e−𝜅(t−t0)) e−𝜅t0
−𝜃
𝜅
[
2(Xt0 + 1) + 𝜎2
𝜅+ t −t0
]
(t −t0)e−𝜅t
−
(
𝜅𝜃+ 𝜎2
𝜅3
) (
Xt0 −𝜃
) (1 −e−2𝜅(t−t0)) e−𝜅t0
−1
3𝜅2
[
X2
t0 −
(
2𝜅𝜃+ 𝜎2
𝜅
) (
Xt0 −1
2𝜃
)] (1 −e−3𝜅(t−t0)) e−𝜅t0.
◽

3.2.3
Multi-Dimensional Diffusion Process
175
10. Heston Model. Let (Ω, ℱ, ℙ) be a probability space and let {WS
t ∶t ≥0}, {W𝜎
t ∶t ≥0}
be two standard Wiener processes with correlation 𝜌∈(−1, 1). Suppose the asset price St
takes the form
dSt = 𝜇Stdt + 𝜎tStdWS
t ,
S0 > 0
d𝜎2
t = 𝜅(𝜃−𝜎2
t ) dt + 𝛼𝜎tdW𝜎
t ,
𝜎0 > 0
dWS
t dW𝜎
t = 𝜌dt
where 𝜇, 𝜅, 𝜃and 𝛼are constants, and 𝜎t is the stochastic volatility of St. By defining
{Bt ∶t ≥0} as a standard Wiener process where Bt ⟂⟂W𝜎
t , show that we can write
WS
t = 𝜌W𝜎
t +
√
1 −𝜌2Bt.
Using the above relation show also that for 0 ≤t ≤T we can write
log
(ST∕𝜉T
St∕𝜉t
)
= 𝜇(T −t) −1
2(1 −𝜌2) ∫
T
t
𝜎2
u du +
√
1 −𝜌2
∫
T
t
𝜎u dBu
where 𝜉s = e𝜌∫s
0 𝜎udW𝜎
u −1
2 𝜌2 ∫s
0 𝜎2
udu.
Prove that the relation of 𝜉T and 𝜉t can be expressed as
𝜉T = 𝜉t + 𝜌∫
T
t
𝜎u𝜉u dW𝜎
u .
Conditional on ℱt and {𝜎u ∶t ≤u ≤T}, show that
log
(ST∕𝜉T
St∕𝜉t
)|||||
ℱt, {𝜎u ∶t ≤u ≤T} ∼𝒩
[(
𝜇−1
2𝜎2
RMS
)
(T −t), 𝜎2
RMS(T −t)
]
where 𝜎RMS =
√(1 −𝜌2
T −t
)
∫
T
t
𝜎2
u du.
Solution: From WS
t = 𝜌W𝜎
t +
√
1 −𝜌2Bt we have
𝔼(WS
t ) = 𝔼(𝜌W𝜎
t +
√
1 −𝜌2Bt) = 𝜌𝔼(W𝜎
t ) +
√
1 −𝜌2𝔼(Bt) = 0
and
Var (WS
t ) = Var (𝜌W𝜎
t +
√
1 −𝜌2Bt) = 𝜌2Var (W𝜎
t ) + (1 −𝜌2)Var (Bt) = t.
Given both W𝜎
t ∼𝒩(0, t) and Bt ∼𝒩(0, t), therefore
𝜌W𝜎
t +
√
1 −𝜌2Bt ∼𝒩(0, t).

176
3.2.3
Multi-Dimensional Diffusion Process
In addition, using It¯o’s formula and taking note that W𝜎
t ⟂⟂Bt,
dWS
t ⋅dW𝜎
t = d(𝜌W𝜎
t +
√
1 −𝜌2Bt) ⋅dW𝜎
t
= (𝜌dW𝜎
t +
√
1 −𝜌2dBt) ⋅dW𝜎
t
= 𝜌(dW𝜎
t )2 +
√
1 −𝜌2dBt ⋅dW𝜎
t
= 𝜌dt.
Thus, we can write WS
t = 𝜌W𝜎
t +
√
1 −𝜌2Bt.
Writing the SDEs of St and 𝜎2
t in terms of W𝜎
t and Bt,
dSt = 𝜇Stdt + 𝜎tSt(𝜌dW𝜎
t +
√
1 −𝜌2dBt)
d𝜎2
t = 𝜅(𝜃−𝜎2
t ) dt + 𝛼𝜎tdW𝜎
t
and following It¯o’s lemma,
d log St = dSt
St
−1
2
(dSt
St
)2
+ . . .
= 𝜇dt + 𝜎tSt(𝜌dW𝜎
t +
√
1 −𝜌2dBt) −1
2𝜎2
t (𝜌2dt + (1 −𝜌2) dt)
=
(
𝜇−1
2𝜎2
t
)
dt + 𝜌𝜎tdW𝜎
t +
√
1 −𝜌2𝜎tdBt
and taking integrals,
∫
T
t
d log Su = ∫
T
t
(
𝜇−1
2𝜎2
u
)
du + 𝜌∫
T
t
𝜎u dW𝜎
u +
√
1 −𝜌2
∫
T
t
𝜎u dBu
log ST = log St + 𝜇(T −t) −1
2 ∫
T
t
𝜎2
u du + 𝜌∫
T
t
𝜎u dW𝜎
u +
√
1 −𝜌2
∫
T
t
𝜎u dBu.
By setting
𝜉t = e𝜌∫t
0 𝜎u dW𝜎
u −1
2 𝜌2 ∫t
0 𝜎2
u du
and
𝜉T = e𝜌∫T
0 𝜎u dW𝜎
u −1
2 𝜌2 ∫T
0 𝜎2
u du
we have
log ST = log St + 𝜇(T −t) −1
2(1 −𝜌2) ∫
T
t
𝜎2
u du +
√
1 −𝜌2
∫
T
t
𝜎u dBu + log 𝜉T −log 𝜉t
or
log
(ST∕𝜉T
St∕𝜉t
)
= 𝜇(T −t) −1
2(1 −𝜌2) ∫
T
t
𝜎2
u du +
√
1 −𝜌2
∫
T
t
𝜎u dBu.

3.2.3
Multi-Dimensional Diffusion Process
177
To show the relation between 𝜉T and 𝜉t, from Taylor’s theorem and then using It¯o’s lemma,
d𝜉t = 𝜕𝜉t
𝜕t dt + 𝜕𝜉t
𝜕W𝜎
t
dW𝜎
t + 1
2
𝜕2𝜉t
𝜕t2 (dt)2 + 1
2
𝜕2𝜉t
𝜕(W𝜎
t )2 (dW𝜎
t )2 + . . .
= −1
2𝜌2𝜎2
t 𝜉tdt + 𝜌𝜎t𝜉tdW𝜎
t + 1
2𝜌2𝜎2
t 𝜉tdt
= 𝜌𝜎t𝜉tdW𝜎
t .
Taking integrals,
∫
T
t
d𝜉u = ∫
T
t
𝜌𝜎u𝜉udW𝜎
u
or
𝜉T = 𝜉t + 𝜌∫
T
t
𝜎u𝜉u dW𝜎
u .
Finally, conditional on ℱt and {𝜎u ∶t ≤u ≤T},
𝔼
[
log
(ST∕𝜉T
St∕𝜉t
)|||||
ℱt, {𝜎u ∶t ≤u ≤T}
]
= 𝜇(T −t) −1
2(1 −𝜌2) ∫
T
t
𝜎2
u du
=
(
𝜇−1
2𝜎2
RMS
)
(T −t)
and using the properties of It¯o’s integral,
Var
[
log
(ST∕𝜉T
St∕𝜉t
)|||||
ℱt, {𝜎u ∶t ≤u ≤T}
]
= (1 −𝜌2)Var
[
∫
T
t
𝜎u dBu
|||||
ℱt, {𝜎u ∶t ≤u ≤T}
]
= (1 −𝜌2)𝔼
[(
∫
T
t
𝜎u dBu
)2||||||
ℱt, {𝜎u ∶t ≤u ≤T}
]
−(1 −𝜌2)𝔼
[
∫
T
t
𝜎u dBu
|||||
ℱt, {𝜎u ∶t ≤u ≤T}
]2
= (1 −𝜌2)𝔼
[
∫
T
t
𝜎2
u du
|||||
ℱt, {𝜎u ∶t ≤u ≤T}
]
= (1 −𝜌2) ∫
T
t
𝜎2
u du
= 𝜎2
RMS(T −t)

178
3.2.3
Multi-Dimensional Diffusion Process
where 𝜎2
RMS =
(1 −𝜌2
T −t
)
∫
T
t
𝜎2
u du.
Since ∫
T
t
𝜎u dBu is normally distributed, therefore
log
(ST∕𝜉T
St∕𝜉t
)|||||
ℱt, {𝜎u ∶t ≤u ≤T} ∼𝒩
[(
𝜇−1
2𝜎2
RMS
)
(T −t), 𝜎2
RMS(T −t)
]
.
N.B. Given that the stochastic volatility 𝜎t follows a CIR process, from the results of Prob-
lem 3.2.3.9 (page 171) we can easily find the mean and variance of 𝜎2
RMS.
◽
11. Feynman–Kac Formula for Multi-Dimensional Diffusion Process. Let (Ω, ℱ, ℙ) be a prob-
ability space and let St = (S(1)
t , S(2)
t , . . . , S(n)
t ). We consider the following PDE problem:
𝜕V
𝜕t (St, t) + 1
2
n
∑
i=1
n
∑
j=1
𝜌ij𝜎(S(i)
t , t)𝜎(S(j)
t , t)
𝜕2V
𝜕S(i)
t 𝜕S(j)
t
(St, t)
+
n
∑
i=1
𝜇(S(i)
t , t) 𝜕V
𝜕S(i)
t
(St, t) −r(t)V(St, t) = 0
with boundary condition V(ST, T) = Ψ(ST) where 𝜇, 𝜎are known functions of S(i)
t
and t,
r and Ψ are functions of t and ST, respectively where t < T. Using It¯o’s formula on the
process,
Zu = e−∫u
t r(𝑣)d𝑣V(Su, u)
where S(i)
t satisfies the generalised SDE
dS(i)
t
= 𝜇(S(i)
t , t) dt + 𝜎(S(i)
t , t) dW(i)
t
such that {W(i)
t
∶t ≥0} is a standard Wiener process and dW(i)
t
⋅dW(j)
t
= 𝜌ij dt, 𝜌ij ∈
(−1, 1) for i ≠j and 𝜌ij = 1 for i = j where i, j = 1, 2, . . . , n, show that under the filtration
ℱt, the solution of the PDE is given by
V(St, t) = 𝔼
[
e−∫T
t
r(𝑣)d𝑣Ψ(ST)
||||
ℱt
]
.
Solution: In analogy with Problem 3.2.2.20 (page 147), we let g(u) = e−∫u
t r(𝑣)d𝑣and set
Zu = g(u)V(Su, u).
By applying Taylor’s expansion and It¯o’s formula on dZu, we have
dZu = 𝜕Zu
𝜕u du +
n
∑
i=1
𝜕Zu
𝜕S(i)
u
dS(i)
u + 1
2
n
∑
i=1
n
∑
j=1
𝜕2Zu
𝜕S(i)
u 𝜕S(j)
u
dS(i)
u dS(j)
u + . . .

3.2.3
Multi-Dimensional Diffusion Process
179
=
(
g(u)𝜕V
𝜕u + V(Su, u)𝜕g
𝜕u
)
du +
n
∑
i=1
(
g(u) 𝜕V
𝜕S(i)
u
)
dS(i)
u
+1
2
n
∑
i=1
n
∑
j=1
(
g(u)
𝜕2V
𝜕S(i)
u 𝜕S(j)
u
)
dS(i)
u dS(j)
u
=
(
g(u)𝜕V
𝜕u −r(u)g(u)V(Su, u)
)
du
+
n
∑
i=1
(
g(u) 𝜕V
𝜕S(i)
u
)
(𝜇(S(i)
u , u) du + 𝜎(S(i)
u , u) dW(i)
u )
+1
2
n
∑
i=1
n
∑
j=1
(
g(u)
𝜕2V
𝜕S(i)
u 𝜕S(j)
u
) (
𝜌ij𝜎(S(i)
u , u)𝜎(S(j)
u , u) dt
)
= g(u)
(
𝜕V
𝜕u (Su, u) + 1
2
n
∑
i=1
n
∑
j=1
𝜌ij𝜎(S(i)
u , t)𝜎(S(j)
u , u)
𝜕2V
𝜕S(i)
u 𝜕S(j)
u
(Su, u)
+
n
∑
i=1
𝜇(S(i)
u , u) 𝜕V
𝜕S(i)
u
(Su, u) −r(u)V(Su, u)
)
du
+g(u)
n
∑
i=1
𝜎(S(i)
u , u) 𝜕V
𝜕S(i)
u
dW(i)
u
= g(u)
n
∑
i=1
𝜎(S(i)
u , u) 𝜕V
𝜕S(i)
u
dW(i)
u
since
𝜕V
𝜕u (Su, u) + 1
2
n
∑
i=1
n
∑
j=1
𝜌ij𝜎(S(i)
u , t)𝜎(S(j)
u , u)
𝜕2V
𝜕S(i)
u 𝜕S(j)
u
(Su, u)
+
n
∑
i=1
𝜇(S(i)
u , u) 𝜕V
𝜕S(i)
u
(Su, u) −r(u)V(Su, u) = 0.
By integrating both sides of dZu we have
∫
T
t
dZu =
n
∑
i=1
{
∫
T
t
g(u)𝜎(S(i)
u , u) 𝜕V
𝜕S(i)
u
dW(i)
u
}
ZT −Zt =
n
∑
i=1
{
∫
T
t
e−∫u
t r(𝑣)d𝑣𝜎(S(i)
u , u) 𝜕V
𝜕S(i)
u
dW(i)
u
}
.
Taking expectations and using the property of It¯o calculus,
𝔼(ZT −Zt
) = 0 or 𝔼(Zt
) = 𝔼(ZT
) .

180
3.2.3
Multi-Dimensional Diffusion Process
Therefore, under the filtration ℱt,
𝔼(Zt|ℱt
) = 𝔼(ZT|ℱt
)
𝔼
[
e−∫t
t r(𝑣)d𝑣V(St, t)||| ℱt
]
= 𝔼
[
e−∫T
t
r(𝑣)d𝑣V(ST, T)||||
ℱt
]
V(St, t) = 𝔼
[
e−∫T
t
r(𝑣)d𝑣Ψ(ST)
||||
ℱt
]
.
◽
12. Backward Kolmogorov Equation for a Two-Dimensional Random Walk. We consider a
two-dimensional symmetric random walk where at initial time t0, a particle starts at (x0, y0)
and is at position (x, y) at time t. At time t + 𝛿t, the particle can either move to (x + 𝛿x, y),
(x −𝛿x, y), (x, y + 𝛿y) or (x, y −𝛿y) each with probability 1
4. Let p(x, y, t; x0, y0, t0) denote
the probability density of the particle position (x, y) at time t starting at (x0, y0) at time t0.
By writing the backward equation in a discrete fashion and expanding it using Taylor’s
series, show that for 𝛿x = 𝛿y =
√
𝛿t and in the limit 𝛿t →0,
𝜕p(x, y, t; x0, y0, t0)
𝜕t
= −1
4
(𝜕2p(x, y, t; x0, y0, t0)
𝜕x2
+ 𝜕2p(x, y, t; x0, y0, t0)
𝜕y2
)
.
Solution: By denoting p(x, y, t; x0, y0, t0) as the probability density function of the particle
position (x, y) at time t, the discrete model of the backward equation is
p(x, y, t; x0, y0, t0) = 1
4p(x −𝛿x, y, t + 𝛿t; x0, y0, t0) + 1
4p(x + 𝛿x, y, t + 𝛿t; x0, y0, t0)
+1
4p(x, y −𝛿y, t + 𝛿t; x0, y0, t0) + 1
4p(x, y + 𝛿y, t + 𝛿t; x0, y0, t0).
Expanding
p(x −𝛿x, y, t + 𝛿t; x0, y0, t0),
p(x + 𝛿x, y, t + 𝛿t; x0, y0, t0),
p(x, y −𝛿y, t +
𝛿t; x0, y0, t0) and p(x, y + 𝛿y, t + 𝛿t; x0, y0, t0) using Taylor’s series, we have
p(x −𝛿x, y, t + 𝛿t; x0, y0, t0) = p(x, y, t + 𝛿t; x0, y0, t0) −𝜕p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x
𝛿x
+ 1
2
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x2
(−𝛿x)2 + O((𝛿x)3)
p(x + 𝛿x, y, t + 𝛿t; x0, y0, t0) = p(x, y, t + 𝛿t; x0, y0, t0) + 𝜕p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x
𝛿x
+ 1
2
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x2
(𝛿x)2 + O((𝛿x)3)
p(x, y −𝛿y, t + 𝛿t; x0, y0, t0) = p(x, y, t + 𝛿t; x0, y0, t0) −𝜕p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y
𝛿y
+ 1
2
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y2
(−𝛿y)2 + O((𝛿y)3)

3.2.3
Multi-Dimensional Diffusion Process
181
and
p(x, y + 𝛿y, t + 𝛿t; x0, y0, t0) = p(x, y, t + 𝛿t; x0, y0, t0) + 𝜕p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y
𝛿y
+ 1
2
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y2
(𝛿y)2 + O((𝛿y)3).
By substituting the above equations into the discrete backward equation,
p(x, y, t; x0, y0, t0) = p(x, y, t + 𝛿t; x0, y0, t0) + 1
4
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x2
(𝛿x)2
+1
4
𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y2
(𝛿y)2 + O((𝛿x)3) + O((𝛿y)3).
Setting 𝛿x = 𝛿y =
√
𝛿t and dividing the equation by 𝛿t and in the limit 𝛿t →0
lim
𝛿t→0
[p(x, y, t + 𝛿t; x0, y0, t0) −p(x, y, t; x0, y0, t0)
𝛿t
]
= −lim
𝛿t→0
1
4
(𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕x2
)
−lim
𝛿t→0
1
4
(𝜕2p(x, y, t + 𝛿t; x0, y0, t0)
𝜕y2
)
+ lim
𝛿t→0 O(
√
𝛿t)
we eventually arrive at
𝜕p(x, y, t; x0, y0, t0)
𝜕t
= −1
4
(𝜕2p(x, y, t; x0, y0, t0)
𝜕x2
+ 𝜕2p(x, y, t; x0, y0, t0)
𝜕y2
)
.
◽
13. Forward Kolmogorov Equation for a Two-Dimensional Random Walk. We consider a
two-dimensional symmetric random walk where at initial time t0, a particle starts at
(X0, Y0) and is at position (X, Y) at terminal time T > 0. At time T −𝛿T, the particle can
either move to (X + 𝛿X, Y), (X −𝛿X, Y), (X, Y + 𝛿Y) or (X, Y −𝛿Y) each with probability
1
4. Let p(X, Y, T; X0, Y0, t0) denote the probability density of the position (X, Y) at time T
starting at (X0, Y0) at time t0.
By writing the forward equation in a discrete fashion and expanding it using Taylor’s
series, show that for 𝛿X = 𝛿Y =
√
𝛿T and in the limit 𝛿T →0
𝜕p(X, Y, T; X0, Y0, t0)
𝜕T
= 1
4
(𝜕2p(X, Y, T; X0, Y0, t0)
𝜕X2
+ 𝜕2p(X, Y, T; X0, Y0, t0)
𝜕Y2
)
.

182
3.2.3
Multi-Dimensional Diffusion Process
Solution: By denoting p(X, Y, T; X0, Y0, t0) as the probability density function of the par-
ticle position (X, Y) at time T starting at (X0, Y0) at initial time t0, the discrete model of
the forward equation is
p(X, Y, T; X0, Y0, t0) = 1
4p(X −𝛿X, Y, T −𝛿T; X0, Y0, t0)
+ 1
4p(X + 𝛿X, Y, T −𝛿T; X0, Y0, t0)
+ 1
4p(X, Y −𝛿Y, T −𝛿T; X0, Y0, t0)
+ 1
4p(X, Y + 𝛿Y, T −𝛿T; X0, Y0, t0).
Expanding p(X −𝛿X, Y, T −𝛿T; X0, Y0, t0), p(X + 𝛿X, Y, T −𝛿T; X0, Y0, t0), p(X, Y −
𝛿Y, T −𝛿T; X0, Y0, t0) and p(X, Y + 𝛿Y, T −𝛿T; X0, Y0, t0) using Taylor’s series, we have
p(X −𝛿X, Y, T −𝛿T; X0, Y0, t0) = p(X, Y, T −𝛿T; X0, Y0, t0)
−𝜕p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X
𝛿X
+1
2
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X2
(−𝛿X)2 + O((𝛿X)3)
p(X + 𝛿X, Y, T −𝛿T; X0, Y0, t0) = p(X, Y, T −𝛿T; X0, Y0, t0)
+ 𝜕p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X
𝛿X
+1
2
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X2
(𝛿X)2 + O((𝛿X)3)
p(X, Y −𝛿Y, T −𝛿T; X0, Y0, t0) = p(X, Y, T −𝛿T; X0, Y0, t0)
−𝜕p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y
𝛿Y
+1
2
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y2
(−𝛿Y)2 + O((𝛿Y)3)
and
p(X, Y + 𝛿Y, T −𝛿T; X0, Y0, t0) = p(X, Y, T −𝛿T; X0, Y0, t0)
+ 𝜕p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y
𝛿Y
+1
2
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y2
(𝛿Y)2 + O((𝛿Y)3).

3.2.3
Multi-Dimensional Diffusion Process
183
By substituting the above equations into the discrete forward equation,
p(X, Y, T; X0, Y0, t0) = p(X, Y, T −𝛿T; X0, Y0, t0) + 1
4
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X2
(𝛿X)2
+𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y2
(𝛿Y)2 + O((𝛿X)3) + O((𝛿Y)3).
Setting 𝛿X = 𝛿Y =
√
𝛿T, dividing the equation by 𝛿T and in the limit 𝛿T →0
lim
𝛿T→0
[p(X, Y, T; X0, Y0, t0) −p(X, Y, T −𝛿T; X0, Y0, t0)
𝛿T
]
= lim
𝛿T→0
1
4
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕X2
+ lim
𝛿T→0
1
4
𝜕2p(X, Y, T −𝛿T; X0, Y0, t0)
𝜕Y2
+ lim
𝛿T→0 O(
√
𝛿T)
we eventually arrive at
𝜕p(X, Y, T; X0, Y0, t0)
𝜕T
= 1
4
(𝜕2p(X, Y, T; X0, Y0, t0)
𝜕X2
+ 𝜕2p(X, Y, T; X0, Y0, t0)
𝜕Y2
)
.
◽


4
Change of Measure
In finance, derivative instruments such as options, swaps or futures can be used for both
hedging and speculation purposes. In a hedging scenario, traders can reduce their risk expo-
sure by buying and selling derivatives against fluctuations in the movement of underlying risky
asset prices such as stocks and commodities. Conversely, in a speculation scenario, traders can
also use derivatives to profit in the future direction of underlying prices. For example, if a trader
expects an asset price to rise in the future, then he/she can sell put options (i.e., the purchaser
of the put options pays an initial premium to the seller and has the right but not the obligation
to sell the shares back to the seller at an agreed price should the share price drop below it at the
option expiry date). Given the purchaser of the put option is unlikely to exercise the option,
the seller would be most likely to profit from the premium paid by the purchaser. From the
point of view of trading such contracts, we would like to price contingent claims (or payoffs
of derivative securities such as options) in such a way that there is no arbitrage opportunity
(or no risk-free profits). By doing so we will ensure that even though two traders may differ in
their estimate of the stock price direction, yet they will still agree on the price of the derivative
security. In order to accomplish this we can rely on Girsanov’s theorem, which tells us how
a stochastic process can have a drift change (but not volatility) under a change of measure.
With the application of this important result to finance we can convert the underlying stock
prices under the physical measure (or real-world measure) into the risk-neutral measure (or
equivalent martingale measure) where all the current stock prices are equal to their expected
future prices discounted at the risk-free rate. This is in contrast to using the physical measure,
where the derivative security prices will vary greatly since the underlying assets will differ in
degrees of risk from each other.
4.1
INTRODUCTION
From the seminal work of Black, Scholes and Merton in using diffusion processes and martin-
gales to price contingent claims such as options on risky asset prices, the theory of mathemat-
ical finance is one of the most successful applications of probability theory. Fundamentally,
the Black–Scholes model is concerned with an economy consisting of two assets, a risky asset
(stock) whose price St, t ≥0 is a stochastic process and a risk-free asset (bond or money mar-
ket account) whose value Bt grows at a continuously compounded interest rate. Here we can
assume that St and Bt satisfy the following equations
dSt
St
= 𝜇tdt + 𝜎tdWt, dBt
Bt
= rtdt
where 𝜇t is the stock price growth rate, 𝜎t is the stock price volatility, rt is the risk-free rate and
Wt is a standard Wiener process on the probability space (Ω, ℱ, ℙ). In the financial market we
would like to price a contingent claim Ψ(ST) at time t ≤T in such a way that no risk-free profit
or arbitrage opportunity exists. But before we define the notion of arbitrage we need some

186
4.1
INTRODUCTION
financial terminologies. The following first two definitions refer to the concepts of trading
strategy and self-financing trading strategy, which form the basis of creating a martingale
framework to price a contingent claim.
Definition 4.1(a) (Trading Strategy) In a continuous time setting, at time t ∈[0, T] we con-
sider an economy which consists of a non-dividend-paying risky asset St and a risk-free asset
Bt. A trading strategy (or portfolio) is a pair (𝜙t, 𝜓t) of stochastic processes which are adapted
to the filtration ℱt, 0 ≤t ≤T holding 𝜙t shares of St and 𝜓t units invested in Bt. Therefore, at
time t the value of the portfolio, Πt is
Πt = 𝜙tSt + 𝜓tBt.
Definition 4.1(b) (Self-Financing Trading Strategy) At time t ∈[0, T], the trading strategy
(𝜙t, 𝜓t) of holding 𝜙t shares of non-dividend-paying risky asset St and 𝜓t units in risk-free asset
Bt having a portfolio value
Πt = 𝜙tSt + 𝜓tBt
is called self-financing (or self-financing portfolio) if
dΠt = 𝜙tdSt + 𝜓tdBt
which implies the change in the portfolio value is due to changes in the market conditions and
not to either infusion or extraction of funds.
Note that the change in the portfolio value is only attributed to St and Bt rather than 𝜙t and
𝜓t. However, given that we cannot deposit or withdraw cash in the portfolio there will be
restrictions imposed on the pair (𝜙t, 𝜓t). In order to keep the trading strategy self-financing we
can see that if 𝜙t is increased then 𝜓t will be decreased, and vice versa. Thus, when we make
a choice of either 𝜙t or 𝜓t, the other can easily be found.
Definition 4.1(c) (Admissible Trading Strategy) At time t ∈[0, T], the trading strategy
(𝜙t, 𝜓t) of holding 𝜙t shares of non-dividend-paying risky asset St and 𝜓t units in risk-free
asset Bt having a portfolio value
Πt = 𝜙tSt + 𝜓tBt
is admissible if it is self-financing and if Πt ≥−𝛼almost surely for some 𝛼> 0 and t ∈[0, T].
From the above definition, the existence of a negative portfolio value which is bounded from
below almost surely shows that the investor cannot go too far into debt and thus have a finite
credit line. Once we have restricted ourselves to a class of admissible strategies, the absence
of arbitrage opportunities (“no free lunch”) can be ensured when pricing contingent claims.
But before we discuss its precise terminology we need to define first the concept of attainable
contingent claim.
Definition 4.1(d) (Attainable Contingent Claim) Consider a trading strategy (𝜙t, 𝜓t) at
time t ∈[0, T] of holding 𝜙t shares of non-dividend-paying risky asset St and 𝜓t units in
risk-free asset Bt having a portfolio value
Πt = 𝜙tSt + 𝜓tBt.

4.1
INTRODUCTION
187
The contingent claim Ψ(ST) is attainable if there exists an admissible strategy worth ΠT =
Ψ(ST) at exercise time T.
Definition 4.1(e) (Arbitrage) An arbitrage opportunity is an admissible strategy if the fol-
lowing criteria are satisfied:
(i) Π0 = 0
(no net investment initially)
(ii) ℙ(ΠT ≥0) = 1 (always win at time T)
(iii) ℙ(ΠT > 0) > 0 (making a positive return on investment at time T).
For risky assets which are traded in financial markets, the pricing of contingent claims based
on the underlying assets is determined based on the presence/absence of arbitrageable opportu-
nities as well as whether the market is complete/incomplete. The following definition discusses
the concept of a complete market.
Definition 4.1(f) (Complete Market) A market is said to be complete if every contingent
claim is attainable (i.e., can be replicated by a self-financing trading strategy). Otherwise, it
is incomplete.
In general, when we have NA number of traded assets (excluding risk-free assets) and NR
sources of risk, the financial “rule of thumb” is as follows:
• If NA < NR then the market has no arbitrage and is incomplete.
• If NA = NR then the market has no arbitrage and is complete.
• If NA > NR then the market has arbitrage.
Within the framework of a Black–Scholes model, the following theorem states the existence
of a trading strategy pair (𝜙t, 𝜓t) in a portfolio.
Theorem 4.2 (One-Dimensional Martingale Representation Theorem) Let {Wt ∶0 ≤t ≤
T} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ) and let ℱt, 0 ≤t ≤T
be the filtration generated by Wt. If Mt, 0 ≤t ≤T is a martingale with respect to this filtration
then there exists an adapted process 𝛾t, 0 ≤t ≤T such that
Mt = M0 + ∫
t
0
𝛾u dWu,
0 ≤t ≤T.
From the martingale representation theorem it follows that martingales can be represented
as It¯o integrals. However, the theorem only states that an adapted process 𝛾t exists but does
not provide a method to find it explicitly. Owing to the complexity of the proof which involves
functional analysis, the details are omitted in this book.
To show the relationship between the martingale representation theorem and the trading
strategy pair (𝜓t, 𝜙t), if we set B0 = 1 so that Bt = e∫t
0 rudu then the discounted portfolio value
can be written as
̃Πt = B−1
t Πt.

188
4.1
INTRODUCTION
Therefore, the trading strategy pair (𝜙t, 𝜓t) is self-financing if and only if the discounted port-
folio value can be written as a stochastic integral
̃Πt = ̃Π0 + ∫
t
0
𝜙𝑣d(B−1
𝑣S𝑣).
By assuming that (𝜙t, 𝜓t) is a self-financing trading strategy which replicates the contingent
claim Ψ(ST), it is hoped that the discounted risky asset value B−1
t St will be a martingale (or
ℙ-martingale), so that by taking expectations under the ℙmeasure
𝔼ℙ[
̃ΠT||| ℱt
]
= 𝔼ℙ[
̃Π0||| ℱt
]
+ 𝔼ℙ
[
∫
T
0
𝜙𝑣d(B−1
𝑣S𝑣)
|||||
ℱt
]
= ̃Π0 + ∫
t
0
𝜙𝑣d(B−1
𝑣S𝑣) = ̃Πt
or
Πt = 𝔼ℙ
[
e−∫T
t
rudu ΠT
||||
ℱt
]
= 𝔼ℙ
[
e−∫T
t
rudu Ψ(ST)
||||
ℱt
]
since ΠT = Ψ(ST). Although B−1
t St is not a ℙ-martingale on the probability space (Ω, ℱ, ℙ),
there exists another equivalent martingale measure ℚ(or risk-neutral measure) such that B−1
t St
is a ℚ-martingale on the probability space (Ω, ℱ, ℚ). Before we state the theorem we first
present a few intermediate results which will lead to the main result.
Definition 4.3 Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe
another probability measure on (Ω, ℱ, ℚ). Assume that for every A ∈ℱsatisfying ℙ(A) = 0,
we also have ℚ(A) = 0, then we say ℚis absolutely continuous with respect to ℙon ℱand we
write it as ℚ≪ℙ.
Theorem 4.4 (Radon–Nikod´ym Theorem) Let (Ω, ℱ, ℙ) be the probability space satisfying
the usual conditions. Let ℚbe another probability measure on (Ω, ℱ, ℚ). Under the assumption
that ℚ≪ℙ, there exists a non-negative random variable Z such that
dℚ
dℙ= Z
and we call Z the Radon–Nikod´ym derivative of ℚwith respect to ℙ.
Take note that in finance we need a stronger statement – that is, ℚto be equivalent to ℙ,
ℚ∼ℙwhich is ℚ≪ℙand ℙ≪ℚ. By imposing the condition ℚto be equivalent to ℙ, if an
event cannot occur under the ℙmeasure then it also cannot occur under the ℚmeasure and
vice versa. By doing so, we can now state the following definition of the equivalent martingale
measure for asset prices which pay no dividends.
Definition 4.5 (Equivalent Martingale Measure) Let (Ω, ℱ, ℙ) be the probability space
satisfying the usual conditions and let ℚbe another probability measure on (Ω, ℱ, ℚ). The
probability measure ℚis said to be an equivalent martingale measure (or risk-neutral mea-
sure) if it satisfies:
• ℚis equivalent to ℙ, ℚ∼ℙ;

4.1
INTRODUCTION
189
• the discounted price processes {B−1
t S(i)
t }, i = 1, 2, . . . , m are martingales under ℚ, that
is
𝔼ℚ[
B−1
u S(i)
u ||| ℱt
]
= B−1
t S(i)
t
for all 0 ≤t ≤u ≤T.
Note that for the case of dividend-paying assets, the discounted values of the asset prices with
the dividends reinvested are ℚ-martingales.
Once the equivalent martingale measure is defined, the next question is can we transform a
diffusion process into a martingale by changing the probability measure? The answer is yes
where the transformation can be established using Girsanov’s theorem, which is instrumental
in risk-neutral pricing for derivatives.
Theorem
4.6 (One-Dimensional
Girsanov
Theorem) Let
{Wt ∶0 ≤t ≤T}
be
a
ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ) and let ℱt, 0 ≤t ≤T be
the associated Wiener process filtration. Suppose 𝜃t is an adapted process, 0 ≤t ≤T and
consider
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds.
If
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
then Zt is a positive ℙ-martingale for 0 ≤t ≤T. By changing the measure ℙto a measure ℚ
such that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt,
then
̃Wt = Wt + ∫
t
0
𝜃u du
is a ℚ-standard Wiener process.
In probability theory, the importance of Girsanov’s theorem cannot be understated as it pro-
vides a formal concept of how stochastic processes change under changes in measure. The
theorem is especially important in the theory of financial mathematics as it tells us how to
convert from the physical measure ℙto the risk-neutral measure ℚ. In short, from the appli-
cation of this theorem we can change from a Wiener process with drift to a standard Wiener
process.
In contrast, the converse of Girsanov’s theorem says that every equivalent measure is given
by a change in drift. Thus, by changing the measure it is equivalent to changing the drift and
hence in the Black–Scholes model there is only one equivalent risk-neutral measure. Other-
wise we would have multiple arbitrage-free derivative prices.
Corollary 4.7 If {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on the probability space
(Ω, ℱ, ℙ) and ℚis equivalent to ℙthen there exists an adapted process 𝜃t, 0 ≤t ≤T such
that:

190
4.1
INTRODUCTION
(i) ̃Wt = Wt + ∫t
0 𝜃u du is a ℚ-standard Wiener process,
(ii) the Radon–Nikod´ym derivative of ℚwith respect to ℙis
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds
for 0 ≤t ≤T.
By comparing the martingale representation theorem with Girsanov’s theorem we can see
that in the former the filtration generated by the standard Wiener process is more restrictive
than the assumption given in Girsanov’s theorem. By including this extra restriction on the
filtration in Girsanov’s theorem we have the following corollary.
Corollary 4.8 Let {Wt ∶0 ≤t ≤T} be a ℙ-standard Wiener process on the probability
space (Ω, ℱ, ℙ) and let ℱt, 0 ≤t ≤T be the filtration generated by Wt. By assuming the
one-dimensional Girsanov theorem holds and if ̃Mt, 0 ≤t ≤T is a ℚ-martingale, there exists
an adapted process {̃𝛾t ∶0 ≤t ≤T} such that
̃Mt = ̃M0 + ∫
t
0
̃𝛾u d ̃Wu,
0 ≤t ≤T
where ̃Wt is a ℚ-standard Wiener process.
By extending the one-dimensional Girsanov theorem to multiple risky assets where each of
the assets is a random component driven by an independent standard Wiener process, we state
the following multi-dimensional Girsanov theorem.
Theorem 4.9 (Multi-Dimensional Girsanov Theorem) Let Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be an n-dimensional ℙ-standard Wiener process, with {W(i)
t }0≤t≤T, i = 1, 2, . . . , n being an
independent one-dimensional ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ)
and let ℱt, 0 ≤t ≤T be the associated Wiener process filtration. Suppose we have an
n-dimensional adapted process 𝜽t = (𝜃(1)
t , 𝜃(2)
t , . . . , 𝜃(n)
t )T, 0 ≤t ≤T and we consider
Zt = exp
{
−∫
t
0
𝜽u ⋅dWu −1
2 ∫
t
0
‖‖𝜽u‖‖
2
2 du
}
.
If
𝔼ℙ
(
exp
{
1
2 ∫
T
0
‖‖𝜽t‖‖
2
2 dt
})
< ∞
where
||𝜽t||2 =
√
(𝜃(1)
t )2 + (𝜃(2)
t )2 + . . . + (𝜃(n)
t )2
then Zt is a positive ℙ-martingale for 0 ≤t ≤T. By changing the measure ℙto a measure ℚ
such that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt

4.1
INTRODUCTION
191
for all 0 ≤t ≤T then
̃
Wt = Wt + ∫
t
0
𝜽u du
is an n-dimensional ℚ-standard Wiener process where ̃
Wt = ( ̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t )T and
̃W(i)
t
= W(i)
t
+ ∫
t
0
𝜃(i)
t du, i = 1, 2, . . . , n such that the component processes of ̃
Wt are
independent under ℚ.
By amalgamating the results of the multi-dimensional Girsanov theorem we also state the
following multi-dimensional martingale representation theorem.
Theorem 4.10 (Multi-Dimensional Martingale Representation Theorem) Let Wt =
(W(1)
t , W(2)
t , . . . , W(n)
t )T be an n-dimensional ℙ-standard Wiener process, with {W(i)
t }0≤t≤T,
i = 1, 2, . . . , n being an independent one-dimensional ℙ-standard Wiener process on the
probability space (Ω, ℱ, ℙ) and let ℱt, 0 ≤t ≤T be the filtration generated by Wt. If Mt,
0 ≤t ≤T is a martingale with respect to this filtration, then there exists an n-dimensional
adapted process 𝜸t = (𝛾(1)
t , 𝛾(2)
t , . . . , 𝛾(n)
t )T, 0 ≤t ≤T such that
Mt = M0 + ∫
t
0
𝜸u ⋅dWu,
0 ≤t ≤T.
By assuming the multi-dimensional Girsanov theorem holds and if ̃Mt, 0 ≤t ≤T is a
ℚ-martingale there exists an n-dimensional adapted process ̃𝜸t = (̃𝛾(1)
t ,̃𝛾(2)
t , . . . ,̃𝛾(n)
t )T,
0 ≤t ≤T} such that
̃Mt = ̃M0 + ∫
t
0
̃𝜸u ⋅d̃
Wu,
0 ≤t ≤T
where ̃Wt =
(
̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t
)T
is an n-dimensional ℚ-standard Wiener process.
In our earlier discussion we used the risk-free asset Bt to construct our trading strategy where,
under the risk-neutral measure ℚthe discounted risky asset price B−1
t St is a ℚ-martingale,
that is
𝔼ℚ[
B−1
T ST||| ℱt
]
= B−1
t St.
In finance terminology the risk-free asset Bt which does the discounting is called the numéraire.
Definition 4.11 A numéraire is an asset with positive price process which pays no dividends.
Suppose there is another non-dividend-paying asset Nt with strictly positive price pro-
cess and under the risk-neutral measure ℚ, the discounted price B−1
t Nt is a ℚ-martingale.
From Girsanov’s theorem we can define a new probability measure ℚN given by the
Radon–Nikod´ym derivative
dℚN
dℚ
||||ℱt
= Nt
N0
/ Bt
B0
.
By changing the ℚmeasure to an equivalent ℚN measure, the discounted price N−1
t St is a
ℚN-martingale such that
𝔼ℚN [
N−1
T ST||| ℱt
]
= N−1
t St.

192
4.2.1
Martingale Representation Theorem
Thus, we can deduce that
Bt𝔼ℚ
[ ST
BT
||||
ℱt
]
= Nt𝔼ℚN [ ST
NT
||||
ℱt
]
and by returning to the discussion of the self-financing trading strategy, for a contingent claim
Ψ(ST) we will also eventually obtain
Bt𝔼ℚ
[ Ψ(ST)
BT
||||
ℱt
]
= Nt𝔼ℚN [ Ψ(ST)
NT
||||
ℱt
]
.
The idea discussed above is known as the change of numéraire technique and is often used
to price complicated derivatives such as convertible bonds, foreign currency and interest rate
derivatives.
4.2
PROBLEMS AND SOLUTIONS
4.2.1
Martingale Representation Theorem
1. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process and let X be a real-valued random vari-
able on the probability space (Ω, ℱ, ℙ) such that 𝔼ℙ(|X|2) < ∞. For the process
Mt = 𝔼ℙ(X|ℱt
)
show that 𝔼ℙ(M2
t
) < ∞and that Mt is a ℙ-martingale with respect to the filtration ℱt,
0 ≤t ≤T generated by Wt.
Solution: By definition,
𝔼ℙ(M2
t
) = 𝔼ℙ[
𝔼ℙ(X|ℱt
)2]
.
From Jensen’s inequality (see Problem 1.2.3.14, page 48) we set 𝜑(x) = x2, which is a
convex function. By substituting x = 𝔼ℙ(
X|ℱt
)
we have
𝔼ℙ(X|ℱt
)2 ≤𝔼ℙ(X2|ℱt
) .
Taking the expectation,
𝔼ℙ[
𝔼ℙ(X|ℱt
)2]
≤𝔼ℙ[𝔼ℙ(X2|ℱt
)]
and from the tower property (see Problem 1.2.3.11, page 46)
𝔼ℙ[𝔼ℙ(X2|ℱt
)] = 𝔼ℙ(X2) .
Thus,
𝔼ℙ[
𝔼ℙ(X|ℱt
)2]
≤𝔼ℙ(X2)
and because 𝔼ℙ(|X|2) < ∞, so 𝔼ℙ(M2
t
) = 𝔼ℙ[
𝔼ℙ(X|ℱt
)2]
< ∞.

4.2.1
Martingale Representation Theorem
193
To show that Mt is a ℙ-martingale we note that:
(a) Under the filtration ℱs, 0 ≤s ≤t and using the tower property (see Problem 1.2.3.11,
page 46),
𝔼ℙ(Mt|| ℱs
) = 𝔼ℙ[
𝔼ℙ(X|ℱt
)||| ℱs
]
= 𝔼ℙ(X|ℱs
) = Ms.
(b) Since 𝔼ℙ(|X|2) < ∞we can deduce, using Hölder’s inequality, that
𝔼ℙ(|Mt|) = 𝔼ℙ[|||𝔼ℙ(X|ℱt
)|||
]
≤𝔼ℙ[𝔼ℙ(
|X| |ℱt
)] = 𝔼ℙ(|X|) ≤
√
𝔼ℙ(|X|2) < ∞.
(c) Mt is clearly ℱt-adapted for 0 ≤t ≤T.
From the results of (a)–(c) we have shown that Mt is a ℙ-martingale with respect to ℱt,
0 ≤t ≤T.
◽
2. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process and let X be a real-valued random vari-
able on the probability space (Ω, ℱ, ℙ) such that 𝔼ℙ(|X|2) < ∞. Given that the process
Mt = 𝔼ℙ(X|ℱt
)
is a ℙ-martingale with respect to the filtration ℱt, 0 ≤t ≤T generated by Wt, then from the
martingale representation theorem there exists an adapted process 𝛾u, 0 ≤u ≤T such that
Mt = M0 + ∫
t
0
𝛾u dWu,
0 ≤t ≤T.
Show that
(a) if X = WT then 𝛾t = 1,
(b) if X = W2
T then 𝛾t = 2Wt,
(c) if X = W3
T then 𝛾t = 3(W2
t + T −t),
(d) if X = e𝜎WT, 𝜎∈ℝthen 𝛾t = 𝜎e𝜎Wt+ 1
2 𝜎2(T−t),
for 0 ≤t ≤T.
Solution: To find the adapted process 𝛾t we note that from It¯o’s formula,
dMt = 𝜕Mt
𝜕t dt + 𝜕Mt
𝜕Wt
dWt + 1
2
𝜕2Mt
𝜕W2
t
dW2
t + . . . = 𝛾tdWt
for 0 ≤t ≤T.
(a) For Mt = 𝔼ℙ(WT|| ℱt
) and since Wt is a ℙ-martingale (see Problem 2.2.3.1, page 71),
Mt = 𝔼ℙ(WT|| ℱt
) = Wt
and hence, for 0 ≤t ≤T,
𝛾t = 𝜕Mt
𝜕Wt
= 1.

194
4.2.2
Girsanov’s Theorem
(b) For Mt = 𝔼ℙ(
W2
T
||| ℱt
)
and since W2
t −t is a ℙ-martingale (see Problem 2.2.3.2,
page 72),
𝔼ℙ(
W2
T −T||| ℱt
)
= W2
t −t
or
Mt = 𝔼ℙ(
W2
T
||| ℱt
)
= W2
t + T −t.
Thus,
𝛾t = 𝜕Mt
𝜕Wt
= 2Wt
for 0 ≤t ≤T.
(c) If Mt = 𝔼ℙ(
W3
T
||| ℱt
)
and since W3
t −3tWt is a ℙ-martingale (see Problem 2.2.3.4,
page 73),
𝔼ℙ(
W3
T −3TWT||| ℱt
)
= W3
t −3tWt
or
Mt = 𝔼ℙ(
W3
T
||| ℱt
)
= W3
t + 3(T −t)Wt.
Therefore,
𝛾t = 𝜕Mt
𝜕Wt
= 3 (W2
t + T −t)
for 0 ≤t ≤T.
(d) If Mt = 𝔼ℙ(e𝜎WT|| ℱt
) and since e𝜎Wt−1
2 𝜎2t is a ℙ-martingale (see Problem 2.2.3.3,
page 72),
𝔼ℙ
(
e𝜎WT−1
2 𝜎2T||||
ℱt
)
= e𝜎Wt−1
2 𝜎2t
or
Mt = 𝔼ℙ(
e𝜎WT||| ℱt
)
= e𝜎Wt+ 1
2 𝜎2(T−t).
Thus,
𝛾t = 𝜕Mt
𝜕Wt
= 𝜎e𝜎Wt+ 1
2 𝜎2(T−t)
for 0 ≤t ≤T.
◽
4.2.2
Girsanov’s Theorem
1. Novikov’s Condition I. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process on the probabil-
ity space (Ω, ℱ, ℙ) and let 𝜃t be an adapted process, 0 ≤t ≤T. By considering
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds

4.2.2
Girsanov’s Theorem
195
and if
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
then, without using It¯o’s formula, show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
Solution: Using the properties of stochastic integrals (see Problems 3.2.1.10, page 112;
3.2.1.11, page 113; and 3.2.2.4, page 126), we can deduce
∫
t
0
𝜃s dWs ∼𝒩
(
0, ∫
t
0
𝜃2
s ds
)
.
In the same vein, for any u < t, the random variable
∫
t
u
𝜃s dWs ∼𝒩
(
0, ∫
t
u
𝜃2
s ds
)
and ∫u
0 𝜃sdWs ⟂⟂∫t
u 𝜃sdWs. To show that Zt is a ℙ-martingale, we note the following.
(a) Under the filtration ℱu,
𝔼ℙ(Zt ||ℱu
) = 𝔼ℙ
(
e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds||||
ℱu
)
= e−1
2 ∫t
0 𝜃2
s ds 𝔼ℙ(
e−∫t
0 𝜃sdWs||| ℱu
)
= e−1
2 ∫t
0 𝜃2
s ds 𝔼ℙ(
e−∫u
0 𝜃sdWs−∫t
u 𝜃sdWs||| ℱu
)
= e−1
2 ∫t
0 𝜃2
s ds 𝔼ℙ(
e−∫u
0 𝜃sdWs||| ℱu
)
𝔼ℙ(
e−∫t
u 𝜃sdWs||| ℱu
)
= e−1
2 ∫t
0 𝜃2
s ds ⋅e−∫u
0 𝜃sdWs ⋅e
1
2 ∫t
u 𝜃2
s ds
= e−∫u
0 𝜃sdWs ⋅e−1
2 ∫t
0 𝜃2
s ds ⋅e−1
2 ∫u
t 𝜃2
s ds
= e−∫u
0 𝜃sdWs−1
2 ∫u
0 𝜃2
s ds.
Therefore, 𝔼ℙ(Zt ||ℱu
) = Zu.
(b) Taking the expectation of |Zt|,
𝔼ℙ(|Zt|) = 𝔼ℙ
(||||
e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds||||
)
= 𝔼ℙ(
e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds)
= e−1
2 ∫t
0 𝜃2
s ds𝔼ℙ(
e−∫t
0 𝜃sdWs
)
= e−1
2 ∫t
0 𝜃2
s ds ⋅e
1
2 ∫t
0 𝜃2
s ds
= 1 < ∞
since, for X ∼log-𝒩(𝜇, 𝜎2), 𝔼ℙ(X) = e𝜇+ 1
2 𝜎2.
(c) Because Zt is a function of Wt, it is ℱt-adapted.

196
4.2.2
Girsanov’s Theorem
From the results of (a)–(c) we have shown that {Zt ∶0 ≤t ≤T} is a ℙ-martingale and
because Zt > 0, {Zt}0≤t≤T is a positive ℙ-martingale.
◽
2. Novikov’s Condition II. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process on the proba-
bility space (Ω, ℱ, ℙ) and let 𝜃t be an adapted process, 0 ≤t ≤T. By considering
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds
and if
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
then, using It¯o’s formula, show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
Solution: Let Zt = f(Xt) = eXt where Xt = −∫t
0 𝜃s dWs −1
2 ∫t
0 𝜃2
s ds and by applying
Taylor’s expansion and subsequently It¯o’s formula,
dZt = 𝜕f
𝜕Xt
dXt + 1
2
𝜕2f
𝜕X2
t
(dXt)2 + . . .
= eXt
(
−𝜃tdWt −1
2𝜃2
t dt
)
+ 1
2eXt𝜃2
t dt
= −𝜃tZtdWt.
Integrating both sides of the equation from u to t, where u < t,
∫
t
u
dZs = −∫
t
u
𝜃sZs dWs
Zt = Zu −∫
t
u
𝜃sZs dWs.
Under the filtration ℱu and because ∫t
u 𝜃sZs dWs is independent of ℱu, we have
𝔼ℙ(Zt|ℱu
) = Zu
where 𝔼ℙ
(
∫
t
u
𝜃sZs dWs
|||||
ℱu
)
= 𝔼ℙ
(
∫
t
u
𝜃sZs dWs
)
= 0. Using the same steps as
described in Problem 4.2.2.1 (page 194), we can also show that 𝔼ℙ(|Zt|) < ∞. Since Zt
is ℱt-adapted and Zt > 0, we have shown that {Zt ∶0 ≤t ≤T} is a positive ℙ-martingale.
◽
3. Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe another
probability measure on (Ω, ℱ, ℚ) such that ℚis absolutely continuous with respect to ℙ
on ℱ. Using the Radon–Nikod´ym theorem show that if dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T,
then Zt is a positive ℙ-martingale.

4.2.2
Girsanov’s Theorem
197
Solution: By definition, the probability measures ℙand ℚare functions
ℙ∶Ω →[0, 1] and ℚ∶Ω →[0, 1].
Because ℚis absolutely continuous with respect to ℙon ℱthen, from the
Radon–Nikod´ym theorem,
dℚ
dℙ∶Ω →(0, ∞) and ℙ
(dℚ
dℙ> 0
)
= 1.
Furthermore,
𝔼ℙ
[
dℚ
dℙ
]
= ∫Ω
dℚ
dℙ⋅dℙ= ∫Ω
dℚ= 1.
Given dℚ
dℙ
||||ℱt
= Zt we can therefore deduce that Zt > 0 for all 0 ≤t ≤T. To show that Zt
is a positive martingale we have the following:
(a) Under the filtration ℱt we define the Radon–Nikod´ym derivative as
Zt = 𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
.
For 0 ≤s ≤t ≤T, and using the tower property of conditional expectation (see Prob-
lem 1.2.3.11, page 46),
𝔼ℙ(Zt|| ℱs
) = 𝔼ℙ
[
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)|||||
ℱs
]
= 𝔼ℙ
(
dℚ
dℙ
||||
ℱs
)
= Zs.
(b) Since dℚ
dℙ> 0, so |Zt| =
|||||
(
dℚ
dℙ
||||ℱt
)|||||
= dℚ
dℙ
||||ℱt
. Therefore, for 0 ≤t ≤T and using
the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
𝔼ℙ(|Zt|) = 𝔼ℙ
[
dℚ
dℙ
||||
ℱt
]
𝔼ℙ[𝔼ℙ(||Zt||
)] = 𝔼ℙ
[
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)]
we have
𝔼ℙ(||Zt||
) = 𝔼ℙ(dℚ
dℙ
)
= 1 < ∞.
(c) Zt is clearly ℱt-adapted for 0 ≤t ≤T.
From the results of (a)–(c) we have shown that Zt is a positive ℙ-martingale.
◽

198
4.2.2
Girsanov’s Theorem
4. Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe another
probability measure on (Ω, ℱ, ℚ) such that ℚis absolutely continuous with respect to ℙ
on ℱ. Let dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, so that Zt is a positive ℙ-martingale. By letting
{Xt ∶0 ≤t ≤T} be an ℱt measurable random variable, show using the Radon–Nikod´ym
theorem that
𝔼ℚ(Xt
) = 𝔼ℙ
[
Xt
(dℚ
dℙ
||||ℱt
)]
and hence deduce that for A ∈ℱt,
∫A
Xt dℚ= ∫A
Xt
(
dℚ
dℙ
||||ℱt
)
dℙ.
Solution: From the definition of the Radon–Nikod´ym derivative dℚ
dℙ= Z where Z is a
positive random variable, we can write
∫Ω
Xt dℚ= ∫Ω
XtZ dℙ
𝔼ℚ(Xt
) = 𝔼ℙ(XtZ) .
Using the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
𝔼ℚ(
Xt
)
= 𝔼ℙ(
XtZ
)
= 𝔼ℙ[𝔼ℙ(XtZ|ℱt
)]
= 𝔼ℙ[Xt𝔼ℙ(Z|ℱt
)] .
Under the filtration ℱt, we define the Radon–Nikod´ym derivative as
Zt = 𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
and therefore
𝔼ℚ(Xt
) = 𝔼ℙ(XtZt
)
or
𝔼ℚ(Xt
) = 𝔼ℙ
[
Xt
(dℚ
dℙ
||||ℱt
)]
.
By defining
1IA =
{
1
if A ∈ℱt
0
otherwise
and if Xt is ℱt measurable, then for any A ∈ℱt,
𝔼ℚ(1IAXt
) = 𝔼ℙ(1IAXtZt
)

4.2.2
Girsanov’s Theorem
199
which is equivalent to
∫A
Xt dℚ= ∫A
XtZt dℙ
or
∫A
Xt dℚ= ∫A
Xt
(
dℚ
dℙ
||||ℱt
)
dℙ.
◽
5. Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe another
probability measure on (Ω, ℱ, ℚ) such that ℚis absolutely continuous with respect to ℙ
on ℱ. Let dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, so that Zt is a positive ℙ-martingale. By letting
{Xt ∶0 ≤t ≤T} be an ℱt measurable random variable, show using the Radon–Nikod´ym
theorem that
𝔼ℙ(Xt
) = 𝔼ℚ
[
Xt
(dℚ
dℙ
||||ℱt
)−1]
and hence deduce that for A ∈ℱt,
∫A
Xt dℙ= ∫A
Xt
(
dℚ
dℙ
||||ℱt
)−1
dℚ.
Solution: From the definition of the Radon–Nikod´ym derivative dℚ
dℙ= Z where Z is a
positive random variable, we can write
∫Ω
Xt dℙ= ∫Ω
XtZ−1dℚ
𝔼ℙ(Xt
) = 𝔼ℚ(XtZ−1) .
Using the tower property of conditional expectation (see Problem 1.2.3.11, page 46),
𝔼ℙ(Xt
) = 𝔼ℚ(XtZ−1)
= 𝔼ℚ[𝔼ℚ(XtZ−1|ℱt
)]
= 𝔼ℚ[Xt𝔼ℚ(Z−1|ℱt
)] .
Under the filtration ℱt, we define the Radon–Nikod´ym derivative as
Zt = 𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
and therefore
𝔼ℙ(Xt
) = 𝔼ℚ(XtZ−1
t
)
or
𝔼ℙ(Xt
) = 𝔼ℚ
[
Xt
(dℚ
dℙ
||||ℱt
)−1]
.

200
4.2.2
Girsanov’s Theorem
By defining
1IA =
{
1
if A ∈ℱt
0
otherwise
and if Xt is ℱt measurable, then for any A ∈ℱt,
𝔼ℙ(1IAXt
) = 𝔼ℚ(1IAXtZ−1
t
)
which is equivalent to
∫A
Xt dℙ= ∫A
XtZ−1
t dℚ
or
∫A
Xt dℙ= ∫A
Xt
(
dℚ
dℙ
||||ℱt
)−1
dℚ.
◽
6. Let {Wt ∶0 ≤t ≤T} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ)
and let ℱt, 0 ≤t ≤T be the associated Wiener process filtration. By defining
dℚ
dℙ
||||ℱt
= Zt = e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
udu
where for all 0 ≤t ≤T the adapted process 𝜃t satisfies 𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞, show that ℚ
is a probability measure.
Solution: From Problem 4.2.2.4 (page 198) we can deduce that for A ∈ℱt,
∫A
dℚ= ∫A
Zt dℙ
or
ℚ(A) = 𝔼ℙ(Zt1IA
)
where
1IA =
{
1
if A ∈ℱt
0
otherwise.
In addition, from Problem 4.2.2.1 (page 194) we can deduce that the adapted process 𝜃t
follows ∫
t
0
𝜃s dWs ∼𝒩
(
0, ∫
t
0
𝜃2
s ds
)
so that
𝔼ℙ(Zt) = e−1
2 ∫t
0 𝜃2
udu𝔼ℙ(
e−∫t
0 𝜃udWu
)
= e−1
2 ∫t
0 𝜃2
udu ⋅e
1
2 ∫t
0 𝜃2
udu = 1.
Since ℙis a probability measure and based on the above results, ℚis also a probability
measure because

4.2.2
Girsanov’s Theorem
201
(a) ℚ(∅) = 𝔼ℙ(Zt1I∅
) = 𝔼ℙ(Zt)ℙ(∅) = 0 since ℙ(∅) = 0.
(b) ℚ(Ω) = 𝔼ℙ(Zt1IΩ) = 𝔼ℙ(Zt)ℙ(Ω) = 1 since ℙ(Ω) = 1 and 𝔼ℙ(Zt) = 1.
(c) For A1, A2, . . . ∈ℱt, Ai ∩Aj = ∅, i ≠j, i, j ∈{1, 2, . . .}
∞
⋃
i=1
ℚ(Ai
) =
∞
⋃
i=1
𝔼ℙ(
Zt1IAi
)
=
∞
⋃
i=1
𝔼ℙ(Zt
) ℙ(Ai
)
and since ⋃∞
i=1 ℙ(Ai) = ∑∞
i=1 ℙ(Ai),
∞
⋃
i=1
ℚ(Ai
) =
∞
∑
i=1
𝔼ℙ(Zt
) ℙ(Ai
) =
∞
∑
i=1
𝔼ℙ(
Zt1IAi
)
=
∞
∑
i=1
ℚ(Ai).
◽
7. Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe another
probability measure on (Ω, ℱ, ℚ) such that ℚis absolutely continuous with respect to ℙ
on ℱ. Let dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, so that Zt is a positive ℙ-martingale. By letting
{Xt ∶0 ≤t ≤T} be an ℱt measurable random variable and using the partial averaging
property given as
∫A
𝔼ℙ(Y|𝒢) dℙ= ∫A
Ydℙ,
for all
A ∈𝒢
where Y is a random variable on the probability space (Ω, ℱ, ℙ) and 𝒢is a sub-𝜎-algebra
of ℱshow that for 0 ≤s ≤t ≤T,
𝔼ℚ(Xt|ℱs
) =
(
dℚ
dℙ
||||ℱs
)−1
𝔼ℙ
[
Xt
(
dℚ
dℙ
||||ℱt
)|||||
ℱs
]
.
Solution: First, note that
(
dℚ
dℙ
||||ℱs
)−1
𝔼ℙ
[
Xt
(
dℚ
dℙ
||||ℱt
)|||||
ℱs
]
= Z−1
s 𝔼ℙ(XtZt|ℱs
) is
ℱs-measurable. For any A ∈ℱs, and using the results of Problem 4.2.2.4 (page 198) as
well as partial averaging, we have
∫A
Z−1
s 𝔼ℙ(XtZt|ℱs
) dℚ= ∫A
Z−1
s 𝔼ℙ(XtZt|ℱs
) ⋅Zs dℙ
= ∫A
XtZt dℙ
= ∫A
Xt dℚ.
Using the partial averaging once again, we have
∫A
Xt dℚ= ∫A
𝔼ℚ(Xt|ℱs
) dℚ.

202
4.2.2
Girsanov’s Theorem
Therefore,
∫A
Z−1
s 𝔼ℙ(XtZt|ℱs
) dℚ= ∫A
𝔼ℚ(Xt|ℱs
) dℚ
or
Z−1
s 𝔼ℙ(XtZt|ℱs
) = 𝔼ℚ(Xt|ℱs
) .
By substituting Zt = dℚ
dℙ
||||ℱt
we have
𝔼ℚ(Xt|ℱs) =
(
dℚ
dℙ
||||ℱs
)−1
𝔼ℙ
[
Xt
(
dℚ
dℙ
||||ℱt
)|||||
ℱs
]
.
◽
8. Let (Ω, ℱ, ℙ) be the probability space satisfying the usual conditions. Let ℚbe another
probability measure on (Ω, ℱ, ℚ) such that ℚis absolutely continuous with respect to ℙ
on ℱ. Let dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, so that Zt is a positive ℙ-martingale. By letting
{Xt ∶0 ≤t ≤T} be an ℱt measurable random variable and using the partial averaging
property given as
∫A
𝔼ℙ(Y|𝒢) dℙ= ∫A
Y dℙ,
for all
A ∈𝒢
where Y is a random variable on the probability space (Ω, ℱ, ℙ) and 𝒢is a sub-𝜎-algebra
of ℱshow that for 0 ≤s ≤t ≤T,
𝔼ℙ(Xt|ℱs
) =
(
dℚ
dℙ
||||ℱs
)
𝔼ℚ
[
Xt
(
dℚ
dℙ
||||ℱt
)−1|||||
ℱs
]
.
Solution: First, note that
(
dℚ
dℙ
||||ℱs
)
𝔼ℚ
[
Xt
(
dℚ
dℙ
||||ℱt
)−1|||||
ℱs
]
= Zs𝔼ℚ(XtZ−1
t |ℱs
) is
ℱs measurable. For any A ∈ℱs, and using the results of Problem 4.2.2.5 (page 199) as
well as partial averaging, we have
∫A
Zs𝔼ℚ(XtZ−1
t |ℱs
) dℙ= ∫A
Zs𝔼ℚ(XtZ−1
t |ℱs
) ⋅Z−1
s dℚ
= ∫A
XtZ−1
t dℚ
= ∫A
Xt dℙ.
Using the partial averaging once again, we have
∫A
Xt dℙ= ∫A
𝔼ℙ(Xt|ℱs
) dℙ.

4.2.2
Girsanov’s Theorem
203
Therefore,
∫A
Zs𝔼ℚ(XtZ−1
t |ℱs
) dℙ= ∫A
𝔼ℙ(Xt|ℱs
) dℙ
or
Zs𝔼ℚ(XtZ−1
t |ℱs
) = 𝔼ℙ(Xt|ℱs
) .
By substituting Zt = dℚ
dℙ
||||ℱt
we have
𝔼ℙ(Xt|ℱs
) =
(
dℚ
dℙ
||||ℱs
)
𝔼ℚ
[
Xt
(
dℚ
dℙ
||||ℱt
)−1|||||
ℱs
]
.
◽
9. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ) and
suppose 𝜃t is an adapted process, 0 ≤t ≤T. We consider
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds
and if
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
By changing the measure ℙto a measure ℚsuch that dℚ
dℙ
||||ℱt
= Zt, show that
̃Wt = Wt + ∫
t
0
𝜃udu
is a ℚ-martingale.
Solution: The first part of the result is given in Problem 4.2.2.1 (page 194) or Problem
4.2.2.2 (page 196). To show that ̃Wt is a ℚ-martingale we note the following:
(a) Let 0 ≤s ≤t ≤T, under the filtration ℱs and using the result of Problem 4.2.2.7 (page
201), we have
𝔼ℚ(
̃Wt||| ℱs
)
= 1
Zs
𝔼ℙ(
̃WtZt||| ℱs
)
.
From It¯o’s formula and using the results of Problem 4.2.2.2 (page 196),
d
(
̃WtZt
)
= ̃WtdZt + Ztd ̃Wt + d ̃WtdZt
= ̃Wt
(−𝜃tZtdWt
) + Zt
(dWt + 𝜃tdt) + (dWt + 𝜃tdt) (−𝜃tZtdWt
)
=
(
1 −𝜃t ̃Wt
)
ZtdWt.

204
4.2.2
Girsanov’s Theorem
By integrating both sides of the equation from s to t, where s < t,
∫
t
s
d
(
̃WuZu
)
= ∫
t
s
(
1 −𝜃u ̃Wu
)
Zu dWu
̃WtZt = ̃WsZs + ∫
t
s
(
1 −𝜃u ̃Wu
)
Zu dWu.
Under the filtration ℱs and because ∫
t
s
(
1 −𝜃u ̃Wu
)
Zu dWu ⟂⟂ℱs,
𝔼ℙ(
̃WtZt||| ℱs
)
= ̃WsZs
where 𝔼ℙ
(
∫
t
s
(
1 −𝜃u ̃Wu
)
Zu dWu
|||||
ℱs
)
= 𝔼ℙ
(
∫
t
s
(
1 −𝜃u ̃Wu
)
Zu dWu
)
= 0.
Thus,
𝔼ℚ(
̃Wt||| ℱs
)
= 1
Zs
𝔼ℙ(
̃WtZt||| ℱs
)
= 1
Zs
̃WsZs = ̃Ws.
(b) For 0 ≤t ≤T,
||| ̃Wt||| =
|||||
Wt + ∫
t
0
𝜃u du
|||||
≤||Wt|| +
|||||∫
t
0
𝜃u du
|||||
.
Taking expectations under the ℚmeasure, using Hölder’s inequality (see
Problem 1.2.3.2, page 41) and taking note that the positive random variable
Zt ∼log-𝒩
(
−1
2 ∫
t
0
𝜃2
s ds, ∫
t
0
𝜃2
s ds
)
under ℙ, we have
𝔼ℚ(||| ̃Wt|||
)
≤𝔼ℚ(||Wt||
) +
|||||∫
t
0
𝜃u du
|||||
= 𝔼ℙ(||Wt|| Zt
) +
|||||∫
t
0
𝜃u du
|||||
= 𝔼ℙ(||WtZt||
) +
|||||∫
t
0
𝜃u du
|||||
≤
√
𝔼ℙ(W2
t
) 𝔼ℙ(Z2
t
) +
|||||∫
t
0
𝜃u du
|||||
=
√
te∫t
0 𝜃2udu +
|||||∫
t
0
𝜃u du
|||||
< ∞
since 𝔼ℙ(
e
1
2 ∫T
0 𝜃2
udu)
< ∞.

4.2.2
Girsanov’s Theorem
205
(c) Because ̃Wt is a function of Wt, it is ℱt-adapted.
From the results of (a)–(c), we have shown that ̃Wt, 0 ≤t ≤T is a ℚ-martingale.
◽
10. One-Dimensional Girsanov Theorem. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process
on the probability space (Ω, ℱ, ℙ) and suppose 𝜃t is an adapted process, 0 ≤t ≤T. We
consider
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds
and if
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
By changing the measure ℙto a measure ℚsuch that dℚ
dℙ
||||ℱt
= Zt, show that
̃Wt = Wt + ∫
t
0
𝜃u du
is a ℚ-standard Wiener process.
Solution: The first part of the result is given in Problem 4.2.2.1 (page 194) or Problem
4.2.2.2 (page 196). As for the second part of the result, we can prove it using two methods.
Method 1. We first need to show that under the ℚmeasure, ̃Wt ∼𝒩(0, t) and to do this we
rely on the moment generating function. By definition, for a constant 𝜓,
M ̃Wt(𝜓) = 𝔼ℚ(
e𝜓̃Wt
)
= 𝔼ℙ(
e𝜓̃WtZt
)
= 𝔼ℙ(
e𝜓Wt+𝜓∫t
0 𝜃s ds ⋅e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds)
= e
∫t
0
(
𝜓𝜃s−1
2 𝜃2
s
)
ds 𝔼ℙ(
e𝜓Wt−∫t
0 𝜃sdWs
)
= e
∫t
0
(
𝜓𝜃s−1
2 𝜃2
s
)
ds 𝔼ℙ(
e∫t
0 𝜓dWs−∫t
0 𝜃sdWs
)
= e
∫t
0
(
𝜓𝜃s−1
2 𝜃2
s
)
ds 𝔼ℙ(
e∫t
0(𝜓−𝜃s)dWs
)
.
Using the properties of the stochastic It¯o integral under the ℙ-measure
∫
t
0
(𝜓−𝜃s) dWs ∼𝒩
(
0, ∫
t
0
(𝜓−𝜃s
)2 ds
)
therefore
𝔼ℙ(
e∫t
0(𝜓−𝜃s)dWs
)
= e
1
2 ∫t
0 (𝜓−𝜃s)2 ds

206
4.2.2
Girsanov’s Theorem
and hence
M ̃Wt(𝜓) = e
1
2 𝜓2t
which is a moment generating function of a normal distribution with mean zero and vari-
ance t.
Thus, under the ℚmeasure, ̃Wt ∼𝒩(0, t).
Finally, to show that ̃Wt, 0 ≤t ≤T is a ℚ-standard Wiener process we have the following:
(a) ̃W0 = 0 and ̃Wt certainly has continuous sample paths for t > 0.
(b) For t > 0 and s > 0, ̃Wt+s −̃Wt ∼𝒩(0, s) since
𝔼ℚ(
̃Wt+s −̃Wt
)
= 𝔼ℚ(
̃Wt+s
)
−𝔼ℚ(
̃Wt
)
= 0
and from the results of Problem 2.2.1.4 (page 57),
Varℚ(
̃Wt+s −̃Wt
)
= Varℚ(
̃Wt+s
)
+ Varℚ(
̃Wt
)
−2Covℚ(
̃Wt+s, ̃Wt
)
= t + s + t −2min{t + s, t}
= s.
(c) Because ̃Wt is a ℚ-martingale (see Problem 4.2.2.9, page 203), then for t > 0, s > 0
and under the filtration ℱt,
𝔼ℚ(
̃Wt+s||| ℱt
)
= ̃Wt.
Since we can write
𝔼ℚ(
̃Wt+s|||ℱt
)
= 𝔼ℚ(
̃Wt+s −̃Wt + ̃Wt||| ℱt
)
= 𝔼ℚ(
̃Wt+s −̃Wt||| ℱt
)
+ 𝔼ℚ(
̃Wt||| ℱt
)
= 𝔼ℚ(
̃Wt+s −̃Wt||| ℱt
)
+ ̃Wt
then
𝔼ℚ(
̃Wt+s −̃Wt||| ℱt
)
= 𝔼ℚ(
̃Wt+s −̃Wt
)
= 0
which implies ̃Wt+s −̃Wt ⟂⟂ℱt. Therefore, ̃Wt+s −̃Wt ⟂⟂̃Wt.
From the results of (a)–(c) we have shown ̃Wt, 0 ≤t ≤T is a ℚ-standard Wiener process.
Method 2. The process ̃Wt, 0 ≤t ≤T is a ℚ-martingale with the following properties:
(i) ̃W0 = 0 and has continuous paths for t > 0.
(ii) By setting ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn−1 < tn = t, n ∈ℕthe quadratic vari-
ation of ̃Wt is
lim
n→∞
n−1
∑
i=1
(
̃Wti+1 −̃Wti
)2
= lim
n→∞
n−1
∑
i=1
(
Wti+1 −Wti + ∫
ti+1
0
𝜃u du −∫
ti
0
𝜃u du
)2
= lim
n→∞
n−1
∑
i=1
(
Wti+1 −Wti
)2

4.2.2
Girsanov’s Theorem
207
+ lim
n→∞
n−1
∑
i=1
2
(
Wti+1 −Wti
) (
∫
ti+1
ti
𝜃u du
)
+ lim
n→∞
n−1
∑
i=1
(
∫
ti+1
ti
𝜃u du
)2
= t
since the quadratic variation of Wt is t and lim
n→∞∫
ti+1
ti
𝜃u du = 0. Informally, we can
write d ̃Wtd ̃Wt = dt.
Based on properties (i) and (ii), and from the Lévy characterisation theorem (see Problem
3.2.1.15, page 119), we can deduce that ̃Wt, 0 ≤t ≤T is a ℚ-standard Wiener process.
◽
11. Multi-Dimensional
Novikov
Condition.
Let
Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be
an
n-dimensional ℙ-standard Wiener process, with {W(i)
t
∶0 ≤t ≤T}, i = 1, 2, . . . , n
an independent one-dimensional ℙ-standard Wiener process on the probability space
(Ω, ℱ, ℙ). Suppose we have an n-dimensional adapted process 𝜽t = (𝜃(1)
t , 𝜃(2)
t , . . . , 𝜃(n)
t )T,
0 ≤t ≤T. By considering
Zt = exp
{
−∫
t
0
𝜽u ⋅dWu −1
2 ∫
t
0
‖‖𝜽u‖‖
2
2 du
}
and if
𝔼ℙ
(
exp
{
1
2 ∫
T
0
‖‖𝜽t‖‖
2
2 dt
})
< ∞
where
||𝜽t||2 =
√
(𝜃(1)
t )2 + (𝜃(2)
t )2 + . . . + (𝜃(n)
t )2
show that Zt is a positive ℙ-martingale.
Solution: To show that Zt is a positive ℙ-martingale, we have the following.
(a) We can write Zt as
Zt = exp
{
−∫
t
0
𝜽u ⋅dWu −1
2 ∫
t
0
‖‖𝜽u‖‖
2
2 du
}
= exp
{
−∫
t
0
n
∑
i=1
𝜃(i)
u dW(i)
u −1
2 ∫
t
0
n
∑
i=1
(𝜃(i)
u )2du
}
= exp
{ n
∑
i=1
(
−∫
t
0
𝜃(i)
u dW(i)
u −1
2 ∫
t
0
(𝜃(i)
u )2du
)}

208
4.2.2
Girsanov’s Theorem
=
n
∏
i=1
exp
{
−∫
t
0
𝜃(i)
u dW(i)
u −1
2 ∫
t
0
(𝜃(i)
u )2du
}
=
n
∏
i=1
Z(i)
t
where Z(i)
t
= e−∫t
0 𝜃(i)
u dW(i)
u −1
2 ∫t
0 (𝜃(i)
u )2du. From Problem 4.2.2.1 (page 194), we have
shown that Z(i)
t is a positive ℙ-martingale and since {W(i)
t
∶0 ≤t ≤T}, i = 1, 2, . . . , n
is an independent one-dimensional ℙ-standard Wiener process on the probability
space (Ω, ℱ, ℙ), so for 0 ≤s ≤t,
𝔼ℙ(Zt|ℱs
) = 𝔼ℙ
(
n
∏
i=1
Z(i)
t
|||||
ℱs
)
=
n
∏
i=1
𝔼ℙ(
Z(i)
t
||| ℱs
)
=
n
∏
i=1
Z(i)
s = Zs.
(b) Furthermore, because 𝔼ℙ(|Z(i)
t |) < ∞, hence
𝔼ℙ(||Zt||
) = 𝔼ℙ
(|||||
n
∏
i=1
Z(i)
t
|||||
)
≤𝔼ℙ
( n
∏
i=1
|||Z(i)
t
|||
)
≤
n
∏
i=1
𝔼ℙ(|||Z(i)
t
|||
)
< ∞.
(c) Zt is clearly ℱt-adapted for 0 ≤t ≤T.
From the results of (a)–(c) and since Zt > 0, we have shown that Zt is a positive
ℙ-martingale.
◽
12. Multi-Dimensional
Girsanov
Theorem.
Let
Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T
be
an
n-dimensional ℙ-standard Wiener process, with {W(i)
t
∶0 ≤t ≤T}, i = 1, 2, . . . , n
an independent one-dimensional ℙ-standard Wiener process on the probability space
(Ω, ℱ, ℙ). Suppose we have an n-dimensional adapted process 𝜽t = (𝜃(1)
t , 𝜃(2)
t , . . . , 𝜃(n)
t )T,
0 ≤t ≤T. By considering
Zt = exp
{
−∫
t
0
𝜽u ⋅dWu −1
2 ∫
t
0
‖‖𝜽u‖‖
2
2 du
}
and if
𝔼ℙ
(
exp
{
1
2 ∫
T
0
‖‖𝜽t‖‖
2
2 dt
})
< ∞
where
||𝜽t||2 =
√
(𝜃(1)
t )2 + (𝜃(2)
t )2 + . . . + (𝜃(n)
t )2
show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
By changing the measure ℙto a measure ℚsuch that dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, show
that
̃
Wt = Wt + ∫
t
0
𝜽u du

4.2.2
Girsanov’s Theorem
209
is an n-dimensional ℚ-standard Wiener process where ̃
Wt = ( ̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t )T and
̃W(i)
t
= W(i)
t
+ ∫t
0 𝜃(i)
t du, i = 1, 2, . . . , n such that the component processes of ̃
Wt are inde-
pendent under ℚ.
Solution: The first part of the result is given in Problem 4.2.2.11 (page 207).
As for the second part of the result, we note that given ̃
Wt = ( ̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t )T, and
following the steps given in Problem 4.2.2.9 (page 203), we can easily show that for each
i = 1, 2, . . . , n,
̃W(i)
t
= W(i)
t
+ ∫
t
0
𝜃(i)
u du
is a ℚ-martingale. Because ̃W(i)
0 = 0, and has continuous sample paths with quadratic
variation equal to t (see Problem 4.2.2.10, page 205), then from the multi-dimensional
Lévy characterisation theorem (see Problem 3.2.1.16, page 121) we can show that ̃W(i)
t ,
i = 1, 2, . . . , n is a ℚ-standard Wiener process. Finally, since W(i)
t
⟂⟂W(j)
t , i ≠j, i, j =
1, 2, . . . , n we have
d ̃W(i)
t
⋅d ̃W(j)
t
=
(
dW(i)
t
+ 𝜃(i)
t dt
) (
dW(j)
t
+ 𝜃(j)
t dt
)
= dW(i)
t dW(j)
t
+ 𝜃(j)
t dW(i)
t dt + 𝜃(i)
t dW(j)
t dt + 𝜃(i)
t 𝜃(j)
t dt2
= 0.
Thus, from the multi-dimensional Lévy characterisation theorem (see Problem 3.2.1.16,
page 121) we can deduce that ̃
Wt, 0 ≤t ≤T is an n-dimensional ℚ-standard Wiener pro-
cess such that each component process of ̃
Wt is independent under ℚ.
◽
13. Convergence Issue of Equivalent Measures. Let Wt = (W(1)
t , W(2)
t , . . . , W(n)
t )T be an
n-dimensional ℙ-standard Wiener process, with {W(i)
t
∶0 ≤t ≤T}, i = 1, 2, . . . , n
an independent one-dimensional ℙ-standard Wiener process on the probability space
(Ω, ℱ, ℙ). Suppose we have an n-dimensional adapted process 𝜽t = (𝜃(1)
t , 𝜃(2)
t , . . . , 𝜃(n)
t )T,
0 ≤t ≤T. We consider
Zt = exp
{
−∫
t
0
𝜽u ⋅dWu −1
2 ∫
t
0
‖‖𝜽u‖‖
2
2 du
}
and if
𝔼ℙ
(
exp
{
1
2 ∫
T
0
‖‖𝜽t‖‖
2
2 dt
})
< ∞
where
||𝜽t||2 =
√
(𝜃(1)
t )2 + (𝜃(2)
t )2 + . . . + (𝜃(n)
t )2
show that Zt is a positive ℙ-martingale for 0 ≤t ≤T.

210
4.2.2
Girsanov’s Theorem
By changing the measure ℙto a measure ℚsuch that dℚ
dℙ
||||ℱt
= Zt for all 0 ≤t ≤T, show
that
̃
Wt = Wt + ∫
t
0
𝜽u du
is an n-dimensional ℚ-standard Wiener process where ̃
Wt = ( ̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t )T and
̃W(i)
t
= W(i)
t
+ ∫t
0 𝜃(i)
t du, i = 1, 2, . . . , n such that the component processes of ̃
Wt are inde-
pendent under ℚ. Let X(i) = ∫
t
0
𝜃(i)
u dW(i)
u , i = 1, 2, . . . , n be a sequence of random vari-
ables on the probability space (Ω, ℱ, ℙ) and assume 𝜃(1)
t
= 𝜃(2)
t
= . . . = 𝜃(n)
t
= 𝜃t where
𝜃t is an adapted process, 0 ≤t ≤T. For n →∞show that under the ℙmeasure
ℙ
(
lim
n→∞
1
n
n
∑
i=1
X(i) = 0
)
= 1
and under the equivalent ℚmeasure
ℚ
(
lim
n→∞
1
n
n
∑
i=1
X(i) = 0
)
= 0.
Discuss the implications on the equivalent measures ℙand ℚ.
Solution: The first two results are given in Problem 4.2.2.12 (page 208).
As for the final result, because 𝜃(1)
t
= 𝜃(2)
t
= . . . = 𝜃(n)
t
= 𝜃t then under the ℙmeasure we
can deduce that
X(i) = ∫
t
0
𝜃(i)
u dW(i)
t
∼𝒩
(
0, ∫
t
0
𝜃2
u du
)
for i = 1, 2, . . . , n. Because {W(i)
t
∶0 ≤t ≤T}, i = 1, 2, . . . , n are independent and iden-
tically, distributed,
1
n
n
∑
i=1
X(i) = 1
n
n
∑
i=1
{
∫
t
0
𝜃(i)
u dW(i)
t
}
∼𝒩
(
0, 1
n ∫
t
0
𝜃2
u du
)
or
1
n
n
∑
i=1
X(i) = 1
n
n
∑
i=1
{
∫
t
0
𝜃(i)
u dW(i)
t
}
= 𝜙
√
1
n ∫
t
0
𝜃2
u du,
𝜙∼𝒩(0, 1).
Taking limits n →∞, we have
ℙ
(
lim
n→∞
1
n
n
∑
i=1
X(i) = 0
)
= 1.

4.2.2
Girsanov’s Theorem
211
On the contrary, under the measure ℚfor i = 1, 2, . . . , n,
̃W(i)
t
= W(i)
t
+ ∫
t
0
𝜃(i)
u du
is a ℚ-standard Wiener process and ̃W(1)
t , ̃W(2)
t , . . . , ̃W(n)
t
are independent and identically
distributed. Thus,
d ̃W(i)
t
= dW(i)
t
+ 𝜃(i)
t dt
and in turn we can write
∫
t
0
𝜃(i)
u dW(i)
u = ∫
t
0
𝜃(i)
u d ̃W(i)
u −∫
t
0
(𝜃(i)
u )2 du.
Because 𝜃(1)
t
= 𝜃(2)
t
= . . . = 𝜃(n)
t
= 𝜃t and under the ℚmeasure,
X(i) = ∫
t
0
𝜃(i)
u dW(i)
u ∼𝒩
(
−∫
t
0
𝜃2
u du, ∫
t
0
𝜃2
u du
)
.
Since { ̃W(i)
t }0≤t≤T, i = 1, 2, . . . , n are independent and identically distributed,
1
n
n
∑
i=1
X(i) = 1
n
n
∑
i=1
{
∫
t
0
𝜃(i)
u dW(i)
t
}
∼𝒩
(
−∫
t
0
𝜃2
u du, 1
n ∫
t
0
𝜃2
u du
)
or
1
n
n
∑
i=1
X(i) = 1
n
n
∑
i=1
{
∫
t
0
𝜃(i)
u dW(i)
t
}
= −∫
t
0
𝜃2
u du + 𝜙
√
1
n ∫
t
0
𝜃2
u du,
𝜙∼𝒩(0, 1).
By setting n →∞,
ℚ
(
lim
n→∞
1
n
n
∑
i=1
X(i) = 0
)
= 0.
Therefore, in the limit n →∞, the measures ℙand ℚfail to be equivalent.
◽
14. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ). By
considering a Wiener process with drift
̂Wt = 𝛼t + Wt
and by using Girsanov’s theorem to find a measure ℚunder which ̂Wt is a standard Wiener
process, show that the joint density of
̂Wt
and
̂Mt = max
0≤s≤t
̂Ws

212
4.2.2
Girsanov’s Theorem
under ℙis
f ℙ
̂Mt, ̂Wt(x, 𝑤) =
⎧
⎪
⎨
⎪⎩
2(2x−𝑤)
t
√
2𝜋t e𝛼𝑤−1
2 𝛼2t−1
2t (2x−𝑤)2
x ≥0, 𝑤≤x
0
otherwise.
Finally, deduce that the joint density of
̂Wt
and
̂mt = min
0≤s≤t
̂Ws
under ℙis
f ℙ
̂mt, ̂Wt(y, 𝑤) =
⎧
⎪
⎨
⎪⎩
−2(2y−𝑤)
t
√
2𝜋t e𝛼𝑤−1
2 𝛼2t−1
2t (2y−𝑤)2
y ≤0, y ≤𝑤
0
otherwise.
Solution: By defining
̂Wt = 𝛼t + Wt
and
̂Mt = max
0≤s≤t
̂Ws
using It¯o’s formula we can write
d ̂Wt = d ̃Wt
where ̃Wt = Wt + ∫t
0 𝛼du. From Girsanov’s theorem there exists an equivalent probability
measure ℚon the filtration ℱs, 0 ≤s ≤t defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝛼dWu−1
2 ∫s
0 𝛼2du
= e−𝛼Ws−1
2 𝛼2s
= e−𝛼( ̂Ws−𝛼s)−1
2 𝛼2s
= e−𝛼̂Ws+ 1
2 𝛼2s
so that ̂Wt is a ℚ-standard Wiener process.
From Problem 2.2.5.4 (page 86), under ℚ, the joint probability distribution of ( ̂Mt, ̂Wt)
can be written as
f ℚ
̂Mt, ̂Wt
(x, 𝑤) =
⎧
⎪
⎨
⎪⎩
2(2x−𝑤)
t
√
2𝜋t e−1
2t (2x−𝑤)2
x ≥0, 𝑤≤x
0
otherwise.
In order to find the joint cumulative distribution function of ( ̂Mt, ̂Wt) under ℙ, we use the
result given in Problem 4.2.2.5 (page 199)
ℙ
(
̂Mt ≤x, ̂Wt ≤𝑤
)
= 𝔼ℙ(
1I{ ̂Mt≤x, ̂Wt≤𝑤}
)
= 𝔼ℚ(
Z−1
t 1I{ ̂Mt≤x, ̂Wt≤𝑤}
)

4.2.2
Girsanov’s Theorem
213
= 𝔼ℚ(
e𝛼̂Wt−1
2 𝛼2t1I{ ̂Mt≤x, ̂Wt≤𝑤}
)
= ∫
𝑤
−∞∫
x
−∞
e𝛼𝑣−1
2 𝛼2tf ℚ
̂Mt, ̂Wt
(u, 𝑣) dud𝑣.
Since ̂W0 = 0, ̂Mt ≥0 therefore ̂Wt ≤̂Mt. By definition, the joint density of ( ̂Mt, ̂Wt) under
ℙis
f ℙ
̂Mt, ̂Wt(x, 𝑤) =
𝜕2
𝜕x𝜕𝑤ℙ
(
̂Mt ≤x, ̂Wt ≤𝑤
)
which implies
f ℙ
̂Mt, ̂Wt(x, 𝑤) =
{
2(2x−𝑤)
t
√
2𝜋t e𝛼𝑤−1
2 𝛼2t−1
2t (2x−𝑤)2 y ≥0, 𝑤≤x
0
otherwise.
For the case of the joint density of
̂Wt = 𝛼t + Wt
and
̂mt = min
0≤s≤t
̂Ws
from Problem 2.2.5.5 (page 88), the joint probability distribution of (̂mt, ̂Wt) under ℚis
f ℚ
̂mt, ̂Wt
(y, 𝑤) =
⎧
⎪
⎨
⎪⎩
−2(2y−𝑤)
t
√
2𝜋t e−1
2t (2y−𝑤)2
y ≤0, y ≤𝑤
0
otherwise.
Using the same techniques as discussed earlier, the joint cumulative distribution of (̂mt, ̂Wt)
under ℙis
ℙ
(
̂mt ≤y, ̂Wt ≤𝑤
)
= 𝔼ℙ(
1I{̂mt≤y, ̂Wt≤𝑤}
)
= 𝔼ℚ(
Z−1
t 1I{̂mt≤y, ̂Wt≤𝑤}
)
= 𝔼ℚ(
e𝛼̂Wt−1
2 𝛼2t1I{̂mt≤x, ̂Wt≤𝑤}
)
= ∫
𝑤
−∞∫
y
−∞
e𝛼𝑣−1
2 𝛼2tf ℚ
̂mt, ̂Wt
(u, 𝑣) dud𝑣.
Therefore,
f ℙ
̂mt, ̂Wt(y, 𝑤) =
𝜕2
𝜕y𝜕𝑤ℙ
(
̂mt ≤y, ̂Wt ≤𝑤
)
=
⎧
⎪
⎨
⎪⎩
−2(2y−𝑤)
t
√
2𝜋t e𝛼𝑤−1
2 𝛼2t−1
2t (2y−𝑤)2
y ≤0, y ≤𝑤
0
otherwise.
◽

214
4.2.2
Girsanov’s Theorem
15. Running Maximum and Minimum of a Wiener Process. Let {Wt ∶t ≥0} be a ℙ-standard
Wiener process on the probability space (Ω, ℱ, ℙ) and let
Xt = 𝜈+ 𝜇t + 𝜎Wt
be a Wiener process starting from 𝜈∈ℝwith drift 𝜇∈ℝand volatility 𝜎> 0. Let the
running maximum of the process Xt up to time t be defined as
MX
t = max
0≤s≤t Xs.
Using Girsanov’s theorem to find a measure ℚunder which Xt is a standard Wiener pro-
cess, show that the cumulative distribution function of the running maximum is
ℙ(MX
t ≤x) = Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
−e
2𝜇(x−𝜈)
𝜎2
Φ
(
−x + 𝜈−𝜇t
𝜎
√
t
)
,
x ≥𝜈
where Φ(⋅) is the standard normal cumulative distribution function.
Finally, deduce that the cumulative distribution function of the running minimum
mX
t = min
0≤s≤t Xs
is
ℙ(mX
t ≤x) = Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
+ e
2𝜇(x−𝜈)
𝜎2
Φ
(
x −𝜈+ 𝜇t
𝜎
√
t
)
,
x ≤𝜈.
Find the probability density functions for MX
t and mX
t .
Solution: Let Yt = Xt −𝜈
𝜎
such that
Yt = 𝛼t + Wt
where 𝛼= 𝜇∕𝜎. Following this, we can define
MY
t = max
0≤s≤t Ys
to be the running maximum of Yt = Xt −𝜈
𝜎
. From Problem 4.2.2.14 (page 211) we can
write the joint density of MY
t and Yt as
f ℙ
MY
t ,Yt(y, 𝑤y) =
⎧
⎪
⎨
⎪⎩
2(2y−𝑤y)
t
√
2𝜋t e𝛼𝑤y−1
2 𝛼2t−1
2t (2y−𝑤y)2
y ≥0, 𝑤y ≤y
0
otherwise.
Given that the pair of random variables (MY
t , Yt) only take values from the set
{(y, 𝑤y) ∶y ≥0, y ≥𝑤y}

4.2.2
Girsanov’s Theorem
215
Figure 4.1
Region of MY
t ≤y in shaded area.
then in order to get the cumulative distribution function of MY
t we compute over the shaded
region given in Figure 4.1
ℙ(MY
t ≤y) = ∫
0
−∞∫
y
0
f ℙ
MY
t ,Yt(u, 𝑣) dud𝑣+ ∫
y
0 ∫
y
𝑣
f ℙ
MY
t ,Yt(u, 𝑣) dud𝑣
such that
∫
0
−∞∫
y
0
f ℙ
MY
t ,Yt(u, 𝑣) dud𝑣= ∫
0
−∞∫
y
0
2(2u −𝑣)
t
√
2𝜋t
e𝛼𝑣−1
2 𝛼2t−1
2t (2u−𝑣)2dud𝑣
= −∫
0
−∞
1
√
2𝜋t
e𝛼𝑣−1
2 𝛼2t−1
2t (2u−𝑣)2||||||
u=y
u=0
d𝑣
= −
1
√
2𝜋t ∫
0
−∞
e𝛼𝑣−1
2 𝛼2t−1
2t (2y−𝑣)2d𝑣
+
1
√
2𝜋t ∫
0
−∞
e𝛼𝑣−1
2 𝛼2t−1
2t 𝑣2d𝑣
and using similar steps we have
∫
y
0 ∫
y
𝑣
f ℙ
MY
t ,Yt(u, 𝑣) dud𝑣= −
1
√
2𝜋t ∫
y
0
e𝛼𝑣−1
2 𝛼2t−1
2t (2y−𝑣)2d𝑣
+
1
√
2𝜋t ∫
y
0
e𝛼𝑣−1
2 𝛼2t−1
2t 𝑣2d𝑣.

216
4.2.2
Girsanov’s Theorem
Therefore,
ℙ(MY
t ≤y) = −
1
√
2𝜋t ∫
y
−∞
e𝛼𝑣−1
2 𝛼2t−1
2t (2y−𝑣)2d𝑣+
1
√
2𝜋t ∫
y
−∞
e𝛼𝑣−1
2 𝛼2t−1
2t 𝑣2d𝑣
and by completing the squares we have
𝛼𝑣−1
2𝛼2t −1
2t(2y −𝑣)2 = −1
2t(𝑣−2y −𝛼t)2 + 2𝛼y
𝛼𝑣−1
2𝛼2t −1
2t𝑣2 = −1
2t(𝑣−𝛼t)2.
Thus, for y ≥0,
ℙ(MY
t ≤y) = −e2𝛼y
√
2𝜋t ∫
y
−∞
e−1
2t (𝑣−2y−𝛼t)2d𝑣+ ∫
y
−∞
1
√
2𝜋t
e−1
2t (𝑣−𝛼t)2d𝑣
= −e2𝛼y
∫
−y−𝛼t
√
t
−∞
1
√
2𝜋
e−1
2 z2dz + ∫
y−𝛼t
√
t
−∞
1
√
2𝜋
e−1
2 z2dz
= −e2𝛼yΦ
(
−y −𝛼t
√
t
)
+ Φ
(
y −𝛼t
√
t
)
or
ℙ(MY
t ≤y) = Φ
(
y −𝛼t
√
t
)
−e2𝛼yΦ
(
−y −𝛼t
√
t
)
.
By substituting
MY
t = MX
t −𝜈
𝜎
,
y = x −𝜈
𝜎
,
and
𝛼= 𝜇
𝜎
we eventually have
ℙ(MX
t ≤x) = Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
−e
2𝜇(x−𝜈)
𝜎2
Φ
(
−x + 𝜈−𝜇t
𝜎
√
t
)
,
x ≥𝜈.
Finally, we focus on the minimum value of Xt. Take note that Y0 = 0 and hence by defining
mY
t = min
0≤s≤t Ys
we have mY
t ≤0. Therefore, for y ≤0 and following the symmetry of the standard Wiener
process,
ℙ(mY
t ≤y) = ℙ
(
min
0≤s≤t Ys ≤y
)
= ℙ
(
−max
0≤s≤t
{−Ys
} ≤y
)

4.2.2
Girsanov’s Theorem
217
= ℙ
(
−max
0≤s≤t
{−𝛼s −Ws
} ≤y
)
= ℙ
(
−max
0≤s≤t
{−𝛼s + Ws
} ≤y
)
= ℙ
(
max
0≤s≤t
̃Ys ≥−y
)
where ̃Yt = −𝛼t + Wt. Thus,
ℙ(mY
t ≤y) = 1 −ℙ
(
max
0≤s≤t
̃Ys ≤−y
)
= 1 −Φ
(
−y + 𝛼t
√
t
)
+ e2𝛼yΦ
(
y + 𝛼t
√
t
)
= Φ
(
y −𝛼t
√
t
)
+ e2𝛼yΦ
(
y + 𝛼t
√
t
)
.
Substituting
mY
t = mX
t −𝜈
𝜎
,
y = x −𝜈
𝜎
and
𝛼= 𝜇
𝜎
we have
ℙ(mX
t ≤x) = Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
+ e
2𝜇(x−a)
𝜎2
Φ
(
x −𝜈+ 𝜇t
𝜎
√
t
)
,
x ≤𝜈.
By denoting fMX
t (x) and fmX
t (x) as the probability density functions for MX
t and mX
t , respec-
tively, therefore
fMX
t (x) = d
dxℙ(MX
t ≤x)
= d
dx
[
Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
−e
2𝜇(x−𝜈)
𝜎2
Φ
(
−x + 𝜈−𝜇t
𝜎
√
t
)]
=
1
𝜎
√
2𝜋t
e
−1
2
(
x−𝜈−𝜇t
𝜎
√
t
)2
−2𝜇
𝜎2 e
2𝜇(x−𝜈)
𝜎2
Φ
(
−x + 𝜈−𝜇t
𝜎
√
t
)
+
1
𝜎
√
2𝜋t
e
2𝜇(x−𝜈)
𝜎2
−1
2
(
−x+𝜈−𝜇t
𝜎
√
t
)2
=
2
𝜎
√
2𝜋t
e
−1
2
(
x−𝜈−𝜇t
𝜎
√
t
)2
−2𝜇
𝜎2 e
2𝜇(x−𝜈)
𝜎2
Φ
(
−x + 𝜈−𝜇t
𝜎
√
t
)
,
x ≥𝜈
and

218
4.2.2
Girsanov’s Theorem
fmX
t (x) = d
dxℙ(mX
t ≤x)
= d
dx
[
Φ
(
x −𝜈−𝜇t
𝜎
√
t
)
+ e
2𝜇(x−a)
𝜎2
Φ
(
x −𝜈+ 𝜇t
𝜎
√
t
)]
=
1
𝜎
√
2𝜋t
e
−1
2
(
x−𝜈−𝜇t
𝜎
√
t
)2
+ 2𝜇
𝜎2 e
2𝜇(x−𝜈)
𝜎2
Φ
(
x −𝜈+ 𝜇t
𝜎
√
t
)
+
1
𝜎
√
2𝜋t
e
2𝜇(x−𝜈)
𝜎2
−1
2
(
x−𝜈+𝜇t
𝜎
√
t
)2
=
2
𝜎
√
2𝜋t
e
−1
2
(
x−𝜈−𝜇t
𝜎
√
t
)2
+ 2𝜇
𝜎2 e
2𝜇(x−𝜈)
𝜎2
Φ
(
x −𝜈+ 𝜇t
𝜎
√
t
)
,
x ≤𝜈.
◽
16. First Passage Time Density of a Standard Wiener Process Hitting a Sloping Line. Let
{Wt ∶t ≥0} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ). By set-
ting T𝛼+𝛽t as a stopping time such that T𝛼+𝛽t = inf {t ≥0 ∶Wt = 𝛼+ 𝛽t}, 𝛼, 𝛽∈ℝ, 𝛼≠0
show that the probability density function of T𝛼+𝛽t is given as
fT𝛼+𝛽t(t) =
|𝛼|
t
√
2𝜋t
e−1
2t (𝛼+𝛽t)2.
Solution: By defining ̃Wt = Wt −∫t
0 𝛽du such that
T𝛼+𝛽t = inf
{
t ≥0 ∶̃Wt = 𝛼
}
then, from Girsanov’s theorem, there exists an equivalent probability measure ℚon the
filtration ℱs, 0 ≤s ≤t defined by the Radon–Nikod´ym derivative
Zs = e∫s
0 𝛽dWu−1
2 ∫s
0 𝛽2du
= e𝛽Ws−1
2 𝛽2s
= e𝛽( ̃Ws+𝛽s)−1
2 𝛽2s
= e𝛽̃Ws+ 1
2 𝛽2s
so that ̃Wt is a ℚ-standard Wiener process. From Problem 2.2.5.3 (page 85), the probability
density function of T𝛼+𝛽t = inf {t ≥0 ∶̃Wt = 𝛼} under ℚis therefore
̃fT𝛼+𝛽t(t) =
|𝛼|
t
√
2𝜋t
e−1
2t 𝛼2.
To find the probability density of T𝛼+𝛽t under ℙwe note that
ℙ(T𝛼+𝛽t ≤t) = 𝔼ℙ(
1IT𝛼+𝛽t
)

4.2.2
Girsanov’s Theorem
219
= 𝔼ℚ(
Z−1
t 1IT𝛼+𝛽t
)
= 𝔼ℚ(
e−𝛽̃Wt−1
2 𝛽2t1IT𝛼+𝛽t
)
= 𝔼ℚ(
e−𝛼𝛽−1
2 𝛽2t1IT𝛼+𝛽t
)
= ∫
t
0
e−𝛼𝛽−1
2 𝛽2u ̃fT𝛼+𝛽t(u) du
= ∫
t
0
e−𝛼𝛽−1
2 𝛽2u
|𝛼|
u
√
2𝜋u
e−1
2u 𝛼2 du
= ∫
t
0
|𝛼|
u
√
2𝜋u
e−1
2u (𝛼+𝛽u)2 du.
Since
fT𝛼+𝛽t(t) = d
dtℙ(T𝛼+𝛽t ≤t)
we therefore have
fT𝛼+𝛽t(t) =
|𝛼|
t
√
2𝜋t
e−1
2t (𝛼+𝛽t)2.
◽
17. Let {Wt ∶t ≥0} be a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ), and
let ℱt be the filtration generated by Wt. Suppose 𝜃t is an adapted process, 0 ≤t ≤T and
by considering
Zt = e−∫t
0 𝜃sdWs−1
2 ∫t
0 𝜃2
s ds
and if
𝔼ℙ(
e
1
2 ∫T
0 𝜃2
t dt)
< ∞
then Zt is a positive ℙ-martingale for 0 ≤t ≤T. By changing the measure ℙto a measure
ℚsuch that dℚ
dℙ
||||ℱt
= Zt, from Girsanov’s theorem
̃Wt = Wt + ∫
t
0
𝜃udu,
0 ≤t ≤T
is a ℚ-standard Wiener process. If ̃Mt, 0 ≤t ≤T is a ℚ-martingale, then show that Mt =
Zt ̃Mt, 0 ≤t ≤T is a ℙ-martingale.
Using the martingale representation theorem, show that there exists an adapted process
{̃𝛾t ∶0 ≤t ≤T} such that
̃Mt = ̃M0 + ∫
t
0
̃𝛾u d ̃Wu,
0 ≤t ≤T.

220
4.2.2
Girsanov’s Theorem
Solution: To show that Mt = Zt ̃Mt is a ℙ-martingale, we note the following:
(a) From Problem 4.2.2.8 (page 202),
𝔼ℙ(Mt|ℱs
) = Zs𝔼ℚ(
MtZ−1
t ||| ℱs
)
= Zs ̃Ms = Ms.
(b) From Problem 4.2.2.5 (page 199),
𝔼ℙ(|Mt|) = 𝔼ℚ
(
|Mt| 1
Zt
)
= 𝔼ℚ(|||MtZ−1
t |||
)
= 𝔼ℚ(||| ̃Mt|||
)
< ∞
since ̃Mt is a ℚ-martingale.
(c) Mt = Zt ̃Mt is clearly ℱt-adapted.
From the results of (a)–(c) we have shown that Mt = Zt ̃Mt is a ℙ-martingale.
Using It¯o’s formula on d ̃Mt = d(MtZ−1
t ) and since dZt = −𝜃tZtdWt (see Problem 4.2.2.2,
page 196) and d ̃Wt = dWt + 𝜃tdt, we have
d
(
MtZ−1
t
)
= 1
Zt
dMt −Mt
Z2
t
dZt + Mt
Z3
t
(dZt)2 −1
Z2
t
dMtdZt
= 1
Zt
dMt −Mt
Z2
t
(−𝜃tZtdWt
) + Mt
Z3
t
(𝜃2
t Z2
t dt) −1
Z2
t
dMt
(−𝜃tZtdWt
)
= 1
Zt
dMt + 𝜃t
Zt
dMtdWt + Mt𝜃t
Zt
(dWt + 𝜃tdt)
= 1
Zt
dMt + 𝜃t
Zt
dMtdWt + Mt𝜃t
Zt
d ̃Wt.
Since Mt is a ℙ-martingale then, from the martingale representation theorem, there exists
an adapted process 𝛾t, 0 ≤t ≤T such that
Mt = M0 + ∫
t
0
𝛾u dWu,
0 ≤t ≤T
or
dMt = 𝛾tdWt
and hence
d ̃Mt = 𝛾t
Zt
dWt + 𝛾t𝜃t
Zt
dt + Mt𝜃t
Zt
d ̃Wt
= 𝛾t
Zt
(d ̃Wt −𝜃tdt) + 𝛾t𝜃t
Zt
dt + Mt𝜃t
Zt
d ̃Wt
=
(𝛾t + Mt𝜃t
Zt
)
d ̃Wt
= ̃𝛾td ̃Wt

4.2.3
Risk-Neutral Measure
221
where ̃𝛾t = 𝛾t + Mt𝜃t
Zt
. Integrating on both sides, we therefore have
̃Mt = ̃M0 + ∫
t
0
̃𝛾ud ̃Wu,
0 ≤t ≤T.
◽
4.2.3
Risk-Neutral Measure
1. Geometric Brownian Motion. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = 𝜇tStdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price drift rate, 𝜎t is the stock price volatility
(which are all time dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on
the probability space (Ω, ℱ, ℙ).
From the following discounted stock price process
Xt = e−∫t
0 ruduSt
show, using Girsanov’s theorem, that by changing the measure ℙto an equiva-
lent risk-neutral measure ℚ, Xt is a ℚ-martingale and by applying the martingale
representation theorem show also that
Xt = X0 + ∫
t
0
𝜎uXud ̃Wu,
0 ≤t ≤T
where ̃Wt = Wt + ∫
t
0
𝜆udu is a ℚ-standard Wiener process with 𝜆t = 𝜇t −rt
𝜎t
defined as
the market price of risk.
Finally, show that under the ℚmeasure the stock price follows
dSt = rtStdt + 𝜎tStd ̃Wt.
Solution: By expanding dXt using Taylor’s theorem and then applying It¯o’s formula,
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕St
dSt + 1
2
𝜕2Xt
𝜕t2 dt2 + 1
2
𝜕2Xt
𝜕S2
t
dS2
t + . . .
= −rtXtdt + e−∫t
0 rududSt
= (𝜇t −rt)Xtdt + 𝜎tXtdWt
= 𝜎tXt
[(𝜇t −rt
𝜎t
)
dt + dWt
]
= 𝜎tXtd ̃Wt

222
4.2.3
Risk-Neutral Measure
where ̃Wt = Wt + ∫
t
0
𝜆u du such that 𝜆t = 𝜇t −rt
𝜎t
. From Girsanov’s theorem there exists
an equivalent martingale measure or risk-neutral measure on the filtration ℱs, 0 ≤s ≤t
defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝜆udu−1
2 ∫s
0 𝜆2
udWu
so that ̃Wt is a ℚ-standard Wiener process. Given that under the risk-neutral measure ℚ,
the discounted stock price diffusion process
dXt = 𝜎tXtd ̃Wt
has no dt term, then Xt is a ℚ-martingale (see Problem 3.2.2.5, page 127). Thus, from the
martingale representation theorem, there exists an adapted process 𝛾u, 0 ≤u ≤T such that
Xt = X0 + ∫
t
0
𝛾u d ̃Wu,
0 ≤t ≤T
or
Xt = X0 + ∫
t
0
𝜎uXu d ̃Wu,
0 ≤t ≤T
such that under ℚ, the process ∫
t
0
𝜎uXu d ̃Wu is a ℚ-martingale.
Finally, by substituting dWt = d ̃Wt + 𝜆tdt into dSt = 𝜇tStdt + 𝜎tStdWt, the stock price dif-
fusion process under the risk-neutral measure becomes
dSt = rtStdt + 𝜎tStd ̃Wt.
◽
2. Arithmetic Brownian Motion. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = 𝜇tdt + 𝜎tdWt
where rt is the risk-free rate, 𝜇t is the stock price drift rate, 𝜎t is the stock price volatility
(which are all time dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on
the probability space (Ω, ℱ, ℙ).
From the following discounted stock price process
Xt = e−∫t
0 ruduSt
show, using Girsanov’s theorem, that by changing the measure ℙto an equiva-
lent risk-neutral measure ℚ, Xt is a ℚ-martingale and by applying the martingale
representation theorem show also that
Xt = X0 + ∫
t
0
𝜎uB−1
u d ̃Wu,
0 ≤t ≤T

4.2.3
Risk-Neutral Measure
223
where ̃Wt = Wt + ∫
t
0
𝜆u du is a ℚ-standard Wiener process such that
𝜆t = 𝜇t −rtSt
𝜎t
is defined as the market price of risk.
Finally, show that under the ℚmeasure the stock price follows
dSt = rtStdt + 𝜎td ̃Wt.
Solution: By expanding dXt using Taylor’s theorem and then applying It¯o’s formula,
dXt = 𝜕Xt
𝜕t dt + 𝜕Xt
𝜕St
dSt + 1
2
𝜕2Xt
𝜕t2 dt2 + 1
2
𝜕2Xt
𝜕S2
t
dS2
t + . . .
= −rtXtdt + e−∫t
0 rududSt
= e−∫t
0 rudu(𝜇t −rtSt)dt + 𝜎te−∫t
0 rududWt
= 𝜎te−∫t
0 rudu
[(𝜇t −rtSt
𝜎t
)
dt + dWt
]
= 𝜎tB−1
t d ̃Wt
where Bt = e∫t
0 rudu, ̃Wt = Wt + ∫
t
0
𝜆u du such that 𝜆t = 𝜇t −rtSt
𝜎t
. From Girsanov’s theo-
rem there exists an equivalent martingale measure or risk-neutral measure on the filtration
ℱs, 0 ≤s ≤t defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝜆udu−1
2 ∫s
0 𝜆2
udWu
so that ̃Wt is a ℚ-standard Wiener process. Given that under the risk-neutral measure ℚ,
the discounted stock price diffusion process
dXt = 𝜎tB−1
t d ̃Wt
has no dt term, then Xt is a ℚ-martingale (see Problem 3.2.2.5, page 127). Thus, from the
martingale representation theorem, there exists an adapted process 𝛾u, 0 ≤u ≤T such that
Xt = X0 + ∫
t
0
𝛾ud ̃Wu,
0 ≤t ≤T
or
Xt = X0 + ∫
t
0
𝜎uB−1
u d ̃Wu,
0 ≤t ≤T
such that under ℚ, the process ∫
t
0
𝜎uB−1
u d ̃Wu is a ℚ-martingale.

224
4.2.3
Risk-Neutral Measure
Finally, by substituting dWt = d ̃Wt + 𝜆tdt into dSt = 𝜇tdt + 𝜎tdWt, the stock price diffu-
sion process under the risk-neutral measure becomes
dSt = rtStdt + 𝜎td ̃Wt.
◽
3. Discounted Portfolio. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = 𝜇tStdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price drift rate, 𝜎t is the stock price volatility
(which are all time dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on
the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of stock
and 𝜓t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = e−∫t
0 ruduΠt
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent
risk-neutral measure ℚ, then the discounted portfolio Yt is a ℚ-martingale.
Solution: At time t, the portfolio Πt is valued as
Πt = 𝜙tSt + 𝜓tBt
and the differential of the portfolio is
dΠt = 𝜙tdSt + 𝜓trtBtdt
= 𝜙t
(𝜇tStdt + 𝜎tStdWt
) + rt𝜓tBtdt
= rtΠtdt + 𝜙t
(𝜇t −rt
) Stdt + 𝜙t𝜎tStdWt
= rtΠtdt + 𝜙t𝜎tSt
(𝜆tdt + dWt
)
where 𝜆t = 𝜇t −rt
𝜎t
. By expanding dYt using Taylor’s theorem and then applying It¯o’s
formula,
dYt = 𝜕Yt
𝜕t dt + 𝜕Yt
𝜕Πt
dSt + 1
2
𝜕2Yt
𝜕t2 dt2 + 1
2
𝜕2Yt
𝜕Π2
t
dΠ2
t + . . .
= −rtYtdt + e−∫t
0 rududΠt
= −rte−∫t
0 ruduΠtdt + e−∫t
0 rudu (rtΠtdt + 𝜙t𝜎tSt(𝜆tdt + dWt))
= 𝜙td
(
e−∫t
0 ruduSt
)

4.2.3
Risk-Neutral Measure
225
where 𝜆t = 𝜇t −rt
𝜎t
and from Problem 4.2.3.1 (page 221),
d
(
e−∫t
0 ruduSt
)
= 𝜎te−∫t
0 ruduSt(𝜆tdt + dWt))
= 𝜎te−∫t
0 ruduStd ̃Wt
where ̃Wt = Wt + ∫
t
0
𝜆u du.
From Girsanov’s theorem, there exists an equivalent martingale measure or risk-neutral
measure on the filtration ℱs, 0 ≤s ≤t defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝜆udu−1
2 ∫s
0 𝜆2
udWu
so that ̃Wt is a ℚ-standard Wiener process. Given that under the risk-neutral measure ℚ,
the discounted stock price diffusion process
dYt = 𝜙t𝜎te−∫t
0 ruduStd ̃Wt
has no dt term, then the discounted portfolio Yt is a ℚ-martingale (see Problem 3.2.2.5,
page 127).
◽
4. Self-Financing Trading Strategy. Consider an economy consisting of a risk-free asset and
a stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = 𝜇tStdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price drift rate, 𝜎t is the stock price volatility
(which are all time dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on
the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of stock
and 𝜓t units being invested in a risk-free asset. Under the risk-neutral measure ℚ, the
following discounted portfolio value
Yt = e−∫t
0 rudu Πt
is a ℚ-martingale. Using the martingale representation theorem, show that the portfolio
(𝜙t, 𝜓t) trading strategy has the values
𝜙t = 𝛾tBt
𝜎tSt
,
𝜓t = 𝜎tΠt −𝛾tBt
𝜎tBt
,
0 ≤t ≤T
where 𝛾t, 0 ≤t ≤T is an adapted process.
Solution: Since Yt is a ℚ-martingale with respect to the filtration ℱt, 0 ≤t ≤T (see Prob-
lem 4.2.3.3, page 224) and
dYt = 𝜙t𝜎te−∫t
0 ruduStd ̃Wt

226
4.2.3
Risk-Neutral Measure
where ̃Wt = Wt + ∫
t
0
𝜆u du, 𝜆t = 𝜇t −rt
𝜎t
then, from the martingale representation theo-
rem, there exists an adapted process 𝛾t, 0 ≤t ≤T such that
Yt = Y0 + ∫
t
0
𝛾𝑣d ̃W𝑣,
0 ≤t ≤T.
Taking integrals of dYt = 𝜙t𝜎te−∫t
0 ruduStd ̃Wt,
∫
t
0
dY𝑣= ∫
t
0
𝜙𝑣𝜎𝑣e−∫𝑣
0 ruduS𝑣d ̃W𝑣
Yt = Y0 + ∫
t
0
𝜙𝑣𝜎𝑣e−∫𝑣
0 ruduS𝑣d ̃W𝑣.
Therefore, for 0 ≤t ≤T, we can set
𝛾t = 𝜙t𝜎te−∫t
0 ruduSt
or
𝜙t = 𝛾te∫t
0 rudu
𝜎tSt
= 𝛾tBt
𝜎tSt
where Bt = e∫t
0 rudu. Since 𝜓tBt = Πt −𝜙tSt, therefore
𝜓t = 𝜎tΠt −𝛾tBt
𝜎tBt
.
◽
5. Self-Financing Portfolio. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = 𝜇tStdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price drift rate, 𝜎t is the stock price volatility
(which are all time dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on
the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of stock
and 𝜓t units being invested in a risk-free asset. For each of the following choices of 𝜙t:
(a) 𝜙t = 𝛼, where 𝛼is a constant
(b) 𝜙t = Sn
t , n > 0
(c) 𝜙t = ∫t
0 Sn
u du, n > 0
find the corresponding 𝜓t so that the trading strategy at time t, (𝜙t, 𝜓t) is self-financing.
Solution: By definition, the portfolio at time t, Πt = 𝜙tSt + 𝜓tBt is self-financing if dΠt =
𝜙tdSt + 𝜓tdBt. Taking note that Bt = e∫t
0 rudu satisfies dBt = rtBtdt and from It¯o’s lemma,
we have (dSt)2 = 𝜎2
t S2
t dt and (dSt)𝜈= 0 for 𝜈≥3. Then, for each of the following cases:

4.2.3
Risk-Neutral Measure
227
(a) If 𝜙t = 𝛼, we have Πt = 𝛼St + 𝜓tBt and in differential form
dΠt = 𝛼dSt + 𝜓tdBt + Btd𝜓t
and in order for the portfolio to be self-financing (i.e., dΠt = 𝛼dSt + 𝜓tdBt) we have
Btd𝜓t = 0 or 𝜓t = 𝛽for some constant 𝛽.
(b) If 𝜙t = Sn
t , we have Πt = Sn+1
t
+ 𝜓tBt and in differential form
dΠt = (n + 1)Sn
t dSt + 1
2n(n + 1)Sn−1
t
(dSt)2 + 𝜓tdBt + Btd𝜓t
and for the portfolio to be self-financing (i.e., dΠt = Sn
t dSt + 𝜓tdBt) we therefore set
nSn
t dSt + 1
2n(n + 1)Sn−1
t
(dSt)2 + Btd𝜓t = 0
or
𝜓t = −∫
t
0
Sn
u
Bu
dSu −∫
t
0
n(n + 1)𝜎2
uSn+1
u
Bu
du
= −∫
t
0
e−∫u
0 r𝑣d𝑣Sn
u dSu −∫
t
0
e−∫u
0 r𝑣d𝑣n(n + 1)𝜎2
uSn+1
u
du.
(c) If 𝜙t = ∫t
0 Sn
u du, we have Πt =
(
∫t
0 Sn
u du
)
St + 𝜓tBt and in differential form
dΠt = Sn+1
t
dt +
(
∫
t
0
Sn
u du
)
dSt + 𝜓tdBt + Btd𝜓t
and for the portfolio to be self-financing (i.e., dΠt =
(
∫t
0 Su du
)
dSt + 𝜓tdBt) we
require
Sn+1
t
dt + Btd𝜓t = 0
or
𝜓t = −∫
t
0
Sn+1
u
Bu
du = −∫
t
0
e−∫u
0 r𝑣d𝑣Sn+1
u
du.
◽
6. Stock Price with Continuous Dividend Yield (Geometric Brownian Motion). Consider an
economy consisting of a risk-free asset and a dividend-paying stock price (risky asset). At
time t, the risk-free asset Bt and the stock price St have the following diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt
such that rt is the risk-free rate, 𝜇t is the stock price drift rate, Dt is the continuous dividend
yield, 𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤t ≤T}
is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).

228
4.2.3
Risk-Neutral Measure
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of stock
and 𝜓t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = e−∫t
0 rudu Πt
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent
risk-neutral measure ℚ, then the discounted portfolio Yt is a ℚ-martingale.
Show also that under the ℚ-measure the stock price follows
dSt = (rt −Dt)Stdt + 𝜎tStd ̃Wt
where ̃Wt = Wt + ∫
t
0
𝜆u du is a ℚ-standard Wiener process such that
𝜆t = 𝜇t −rt
𝜎t
is defined as the market price of risk.
Solution: At time t, the portfolio Πt is valued as
Πt = 𝜙tSt + 𝜓tBt
and since the trader will receive DtStdt for every stock held and because the trader holds
𝜙t of the stock, then in differential form
dΠt = 𝜙tdSt + 𝜙tDtStdt + 𝜓tdBt
= 𝜙t
[(𝜇t −Dt)Stdt + 𝜎tStdWt
] + 𝜙tDtStdt + 𝜓trtBtdt
= rtΠtdt + 𝜙t(𝜇t −rt)Stdt + 𝜙t𝜎tStdWt
= rtΠtdt + 𝜙t𝜎tSt(𝜆tdt + dWt)
where 𝜆t = 𝜇t −rt
𝜎t
. Following Problem 4.2.3.3 (page 224),
dYt = d
(
e−∫t
0 rudu Πt
)
= 𝜙𝜎te−∫t
0 ruduStd ̃Wt
such that
̃Wt = Wt + ∫
t
0
𝜆u du.
By applying Girsanov’s theorem to change the measure ℙto an equivalent risk-neutral
measure ℚ, under which ̃Wt is a ℚ-standard Wiener process, the discounted portfolio Yt
is a ℚ-martingale.
By substituting dWt = d ̃Wt + 𝜆tdt into dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt, the stock price dif-
fusion process under the risk-neutral measure becomes
dSt = (rt −Dt)Stdt + 𝜎tStd ̃Wt.
◽

4.2.3
Risk-Neutral Measure
229
7. Stock Price with Continuous Dividend Yield (Arithmetic Brownian Motion). Consider an
economy consisting of a risk-free asset and a dividend-paying stock price (risky asset). At
time t, the risk-free asset Bt and the stock price St have the following diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)dt + 𝜎tdWt
such that rt is the risk-free rate, 𝜇t is the stock price drift rate, Dt is the continuous dividend
yield, 𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤t ≤T}
is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of stock
and 𝜓t units being invested in a risk-free asset. From the following discounted portfolio
value
Yt = e−∫t
0 rudu Πt
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent
risk-neutral measure ℚ, then the discounted portfolio Yt is a ℚ-martingale.
Show also that under the ℚ-measure the stock price follows
dSt = (rt −Dt)Stdt + 𝜎td ̃Wt
where ̃Wt = Wt + ∫
t
0
𝜆u du is a ℚ-standard Wiener process such that
𝜆t = 𝜇t −rtSt
𝜎t
is defined as the market price of risk.
Solution: At time t, the portfolio Πt is valued as
Πt = 𝜙tSt + 𝜓tBt
and since the trader will receive Dtdt for every stock held, then in differential form
dΠt = 𝜙t
(dSt + Dtdt) + 𝜓tdBt
= 𝜙t
[𝜇tdt + 𝜎tdWt
] + 𝜓trtBtdt
= rtΠtdt + 𝜙t
(𝜇t −rtSt
) dt + 𝜙t𝜎tdWt
= rtΠtdt + 𝜙t𝜎t
(𝜆tdt + dWt
)
where 𝜆t = 𝜇t −rtSt
𝜎t
.
Following Problem 4.2.3.3 (page 224), the discounted portfolio becomes
dYt = d
(
e−∫t
0 rudu Πt
)
= 𝜙t𝜎te−∫t
0 rudud ̃Wt
such that
̃Wt = Wt + ∫
t
0
𝜆u du.

230
4.2.3
Risk-Neutral Measure
By applying Girsanov’s theorem to change the measure ℙto an equivalent risk-neutral
measure ℚ, under which ̃Wt is a ℚ-standard Wiener process, the discounted portfolio Yt
is a ℚ-martingale.
By substituting dWt = d ̃Wt + 𝜆tdt into dSt = (𝜇t −Dt)dt + 𝜎tdWt, the diffusion process
under the risk-neutral measure becomes
dSt = (rt −Dt)Stdt + 𝜎td ̃Wt.
◽
8. Commodity Price with Cost of Carry. Consider an economy consisting of a risk-free asset
and a commodity price (risky asset). At time t, the risk-free asset Bt and the commodity
price St have the following diffusion processes
dBt = rtBtdt, dSt = 𝜇tStdt + Ctdt + 𝜎tStdWt
such that rt is the risk-free rate, 𝜇t is the commodity price drift rate, Ct > 0 is the cost
of carry for storage per unit time, 𝜎t is the commodity price volatility (which are all time
dependent) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on the probability space
(Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t units of com-
modity and 𝜓t units being invested in a risk-free asset. From the following discounted
portfolio value
Yt = e−∫t
0 rudu Πt
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent
risk-neutral measure ℚ, then the discounted portfolio Yt is a ℚ-martingale.
Show also that under the ℚ-measure the commodity price follows
dSt = rtStdt + Ctdt + 𝜎tStd ̃Wt
where ̃Wt = Wt + ∫
t
0
𝜆u du is a ℚ-standard Wiener process such that
𝜆t = 𝜇t −rt
𝜎t
is defined as the market price of risk.
Solution: At time t, the portfolio Πt is valued as
Πt = 𝜙tSt + 𝜓tBt
and since the trader will pay Ctdt for storage, and because the trader holds 𝜙t of the com-
modity, then in differential form
dΠt = 𝜙tdSt −𝜙tCtdt + 𝜓trtBtdt
= 𝜙t
[𝜇tStdt + Ctdt + 𝜎tStdWt
] −𝜙tCtdt + 𝜓trtBtdt

4.2.3
Risk-Neutral Measure
231
= rtΠtdt + 𝜙t
(𝜇t −rt
) Stdt + 𝜙t𝜎tStdWt
= rtΠtdt + 𝜙t𝜎tSt
(𝜆tdt + dWt
)
where 𝜆t = 𝜇t −rt
𝜎t
. Following Problem 4.2.3.3 (page 224),
dYt = d
(
e−∫t
0 ruduΠt
)
= 𝜙t𝜎te−∫t
0 ruduStd ̃Wt
such that
̃Wt = Wt + ∫
t
0
𝜆u du.
By applying Girsanov’s theorem to change the measure ℙto an equivalent risk-neutral
measure ℚ, under which ̃Wt is a ℚ-standard Wiener process, the discounted portfolio Yt
is a ℚ-martingale.
By substituting dWt = d ̃Wt + 𝜆tdt into dSt = 𝜇tStdt + Ctdt + 𝜎tStdWt, the commodity
price diffusion process under the risk-neutral measure becomes
dSt = rtStdt + Ctdt + 𝜎tStd ̃Wt.
◽
9. Pricing a Security Derivative. Consider an economy consisting of a risk-free asset and a
stock price (risky asset). At time t, the risk-free asset Bt and the stock price St have the
following diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt
where rt is the risk-free rate, Dt is the continuous dividend yield, 𝜇t is the stock price
growth rate, 𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤
t ≤T} is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt holding 𝜙t shares of
stock and 𝜓t units being invested in a risk-free asset. Let Ψ(ST) represents the payoff of a
derivative security at time T, 0 ≤t ≤T such that Ψ(ST) is ℱT measurable. Assuming the
trader begins with an initial capital Π0 and a trading strategy (𝜙t, 𝜓t), 0 ≤t ≤T such that
the portfolio value at time T is
ΠT = Ψ(ST) almost surely
show that under the risk-neutral measure ℚ,
Ψ(St) = 𝔼ℚ
[
e−∫T
t
rudu Ψ(ST)||||
ℱt
]
,
0 ≤t ≤T.
Solution: At time t, in order to calculate the price of a derivative security Ψ(St), 0 ≤t ≤T
with payoff at time T given as Ψ(ST), we note that because e−∫t
0 rudu Πt is a ℚ-martingale
and ΠT = Ψ(ST) almost surely,
e−∫t
0 rudu Πt = 𝔼ℚ
[
e−∫T
0 rudu ΠT
||||
ℱt
]
= 𝔼ℚ
[
e−∫T
0 rudu Ψ(ST)
||||
ℱt
]
.

232
4.2.3
Risk-Neutral Measure
Since the derivative security Ψ and the portfolio Π have identical values at T, and by
assuming the trader begins with an initial capital Π0, the value of the portfolio Πt = Ψ(St)
for 0 ≤t ≤T. Thus, we can write
Ψ(St) = 𝔼ℚ
[
e−∫T
t
rudu Ψ(ST)
||||
ℱt
]
,
0 ≤t ≤T.
◽
10. First Fundamental Theorem of Asset Pricing. Consider an economy consisting of a
risk-free asset and a stock price (risky asset). At time t, the risk-free asset Bt and the stock
price St have the following diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price growth rate, Dt is the continuous dividend
yield, 𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤t ≤T}
is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
At time t, 0 ≤t ≤T we consider a trader who has a self-financing portfolio valued at Πt
and we define an arbitrage strategy such that the following criteria are satisfied:
(i) Π0 = 0
(ii) ℙ(ΠT ≥0) = 1
(iii) ℙ(ΠT > 0) > 0.
Prove that if the trading strategy has a risk-neutral probability measure ℚ, then it does not
admit any arbitrage opportunities.
Solution: We prove this result via contradiction.
Suppose the trading strategy has a risk-neutral probability measure ℚand it admits arbi-
trage opportunities. Since the discounted portfolio e−∫t
0 rudu Πt is a ℚ-martingale, by let-
ting Π0 = 0, and from the optional stopping theorem,
𝔼ℚ(
e−∫T
0 rudu ΠT
)
= Π0 = 0.
Since ℙ(ΠT ≥0) = 1 and because ℙis equivalent to ℚ, therefore ℚ(ΠT ≥0) = 1. In addi-
tion, since ΠT ≥0 therefore e−∫T
0 ruduΠT ≥0. By definition,
𝔼ℚ(
e−∫T
0 rudu ΠT
)
= ∫
∞
0
ℚ
(
e−∫T
0 rudu ΠT ≥x
)
dx = ∫
∞
0
ℚ
(
ΠT ≥xe∫T
0 rudu)
dx = 0
which implies
ℚ(ΠT ≥0) = 0.
Because ℚis equivalent to ℙ, therefore
ℙ(ΠT ≥0) = 0
which is a contradiction to the fact that there is an arbitrage opportunity.
◽

4.2.3
Risk-Neutral Measure
233
11. Second Fundamental Theorem of Asset Pricing. Consider an economy consisting of a
risk-free asset and a stock price (risky asset). At time t, the risk-free asset Bt and the stock
price St have the following diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock price growth rate, Dt is the continuous dividend
yield, 𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤t ≤T}
is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
We consider a market model under the risk-neutral measure ℚwhere at each time t, 0 ≤
t ≤T, the portfolio Πt is constructed with 𝜓t number of stocks and 𝜙t units of non-risky
assets. Prove that if the market is complete (i.e., every derivative security can be hedged),
then the risk-neutral measure is unique.
Solution: Suppose the market model has two risk-neutral measures ℚ1 and ℚ2. Let A ∈
ℱT, and we consider the payoff of a derivative security
Ψ(ST) =
{
e∫T
0 rudu
A ∈ℱT
0
A ∉ℱT.
As the market model is complete, there is a portfolio value process Πt, 0 ≤t ≤T with the
same initial condition Π0 that satisfies ΠT = Ψ(ST). Since both ℚ1 and ℚ2 are risk-neutral
measures, the discounted portfolio e−∫t
0 rudu Πt is a martingale under ℚ1 and ℚ2.
From the optional stopping theorem we can write
ℚ1(A) = 𝔼ℚ1
[
e−∫T
0 rudu Ψ(ST)
]
= 𝔼ℚ1
(
e−∫T
0 rudu ΠT
)
= Π0
and
ℚ2(A) = 𝔼ℚ2
[
e−∫T
0 rudu Ψ(ST)
]
= 𝔼ℚ2
(
e−∫T
0 rudu ΠT
)
= Π0.
Therefore,
ℚ1 = ℚ2
since A is an arbitrary set.
◽
12. Change of Numéraire. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rtBtdt, dSt = (𝜇t −Dt)Stdt + 𝜎tStdWt
where rt is the risk-free rate, 𝜇t is the stock drift rate, Dt is the continuous dividend yield,
𝜎t is the stock price volatility (which are all time dependent) and {Wt ∶0 ≤t ≤T} is a
ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).

234
4.2.3
Risk-Neutral Measure
Let Ψ(ST) represent the payoff of a derivative security at time T, 0 ≤t ≤T such that Ψ(ST)
is ℱT measurable and under the risk-neutral measure ℚ, let the numéraire N(i)
t , i = 1, 2 be a
strictly positive price process for a non-dividend-paying asset with the following diffusion
process:
dN(i)
t
= rtN(i)
t dt + 𝜈(i)
t N(i)
t d ̃W(i)
t
where 𝜈(i)
t
is the volatility and ̃W(i)
t , 0 ≤t ≤T is a ℚ-standard Wiener process. Show that
for 0 ≤t ≤T,
N(1)
t 𝔼ℚ(1)
(
Ψ(ST)
N(1)
T
||||||
ℱt
)
= N(2)
t 𝔼ℚ(2)
(
Ψ(ST)
N(2)
T
||||||
ℱt
)
and
dℚ(1)
dℚ(2)
||||ℱt
= N(1)
t
N(1)
0
/N(2)
t
N(2)
0
where N(i) is a numéraire and ℚ(i) is the measure under which the stock price discounted
by N(i) is a martingale.
Solution: By solving dBt = rtBtdt we have Bt = e∫t
0 rudu. For i = 1, 2 and given
Bt = e∫t
0 rudu, from Problem 4.2.3.1 (page 221) we can deduce that
d(B−1
t N(i)
t ) = B−1
t N(i)
t 𝜈(i)
t d ̃W(i)
t
is a ℚ-martingale and from It¯o’s formula we have
d(log (B−1
t Nt)) = d(B−1
t N(i)
t )
B−1
t N(i)
t
−1
2
⎛
⎜
⎜
⎜⎝
d
(
B−1
t N(i)
t
)
B−1
t N(i)
t
⎞
⎟
⎟
⎟⎠
2
+ . . .
= 𝜈(i)
t d ̃W(i)
t
−1
2(𝜈(i)
t )2dt.
Taking integrals,
log
(
B−1
t Nt
B−1
0 N0
)
= ∫
t
0
𝜈(i)
u d ̃W(i)
u −∫
t
0
1
2(𝜈(i)
u )2 du
or
B−1
t N(i)
t
= N(i)
0 e∫t
0 𝜈(i)
u d ̃W(i)
u −1
2 ∫t
0 (𝜈(i)
u )2du
since B0 = 1. Using Girsanov’s theorem to change from the ℚmeasure to an equivalent
ℚ(i) measure, the Radon–Nikod´ym derivative is
dℚ(i)
dℚ
||||ℱt
= e−∫t
0 (−𝜈u)d ̃W(i)
u −1
2 ∫t
0 (𝜈(i)
u )2du =
N(i)
t
N(i)
0 Bt
so that W
(i)
t
= ̃W(i)
t
−∫
t
0
𝜈(i)
u du, 0 ≤t ≤T is a ℚ(i)-standard Wiener process.

4.2.3
Risk-Neutral Measure
235
Under the risk-neutral measure ℚ, the stock price follows
dSt = (rt −Dt)Stdt + 𝜎tStd ̃Wt
where ̃Wt = Wt + ∫
t
0
(𝜇u −ru
𝜎u
)
du is a ℚ-standard Wiener process. By changing the ℚ
measure to an equivalent ℚ(i) measure, the discounted stock price
{N(i)
t }−1St
is a ℚ(i)-martingale. Thus, for a derivative payoff Ψ(ST),
𝔼ℚ(Ψ(ST)|ℱt
) =
(
dℚ(i)
dℚ
||||ℱt
)
𝔼ℚ(i) ⎡
⎢
⎢⎣
Ψ(ST)
(
dℚ(i)
dℚ
||||ℱT
)−1||||||
ℱt
⎤
⎥
⎥⎦
= N(i)
t
N(i)
0 Bt
𝔼ℚ(i)
(
ΨT
N(i)
0 BT
N(i)
T
||||||
ℱt
)
= BTN(i)
t
Bt
𝔼ℚ(i)
(
Ψ(ST)
N(i)
T
||||||
ℱt
)
.
Thus,
N(1)
t 𝔼ℚ(1)
(
Ψ(ST)
N(1)
T
||||||
ℱt
)
= N(2)
t 𝔼ℚ(2)
(
Ψ(ST)
N(2)
T
||||||
ℱt
)
.
To find dℚ(1)
dℚ(2) on ℱt we note that
𝔼ℚ(1)
(
N(1)
0 Ψ(St)
N(1)
t
||||||
ℱ0
)
= 𝔼ℚ(2)
(
N(2)
0 Ψ(St)
N(2)
t
||||||
ℱ0
)
.
By letting Xt, 0 ≤t ≤T be an ℱt measurable random variable, from Problem 4.2.2.4
(page 198)
𝔼ℚ(1)(Xt) = 𝔼ℚ(2) [
Xt
(
dℚ(1)
dℚ(2)
||||ℱt
)]
.
We can therefore deduce that
N(1)
0 Ψ(St)
N(1)
t
(
dℚ(1)
dℚ(2)
||||ℱt
)
=
N(2)
0 Ψ(St)
N(2)
t
or
dℚ(1)
dℚ(2)
||||ℱt
= N(1)
t
N(1)
0
/N(2)
t
N(2)
0
.
N.B. An alternative method is to let dℚ(1)
dℚ(2) = dℚ(1)
dℚ
/
dℚ(2)
dℚ
and the result follows.
◽

236
4.2.3
Risk-Neutral Measure
13. Black–Scholes Equation. Consider an economy consisting of a risk-free asset and a stock
price (risky asset). At time t, the risk-free asset Bt and the stock price St have the following
diffusion processes
dBt = rBtdt, dSt = 𝜇Stdt + 𝜎StdWt
where r is the risk-free rate, 𝜇is the stock price drift rate, 𝜎is the stock price volatility
(which are all constants) and {Wt ∶0 ≤t ≤T} is a ℙ-standard Wiener process on the
probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt, given as
Πt = 𝜙tSt + 𝜓tBt
where he holds 𝜙t shares of stock and 𝜓t units being invested in a risk-free asset. Show
that the portfolio (𝜙t, 𝜓t) is self-financing if and only if Πt satisfies the Black–Scholes
equation
𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
+ rSt
𝜕Πt
𝜕St
−rΠt = 0.
Solution: By applying Taylor’s theorem on dΠt,
dΠt = 𝜕Πt
𝜕t dt + 𝜕Πt
𝜕St
dSt + 1
2
𝜕2Πt
𝜕S2
t
dS2
t + . . .
and since dS2
t = (𝜇Stdt + 𝜎StdWt)2 = 𝜎2S2
t dt such that (dt)𝜈= 0, 𝜈≥2, we have
dΠt = 𝜕Πt
𝜕t dt + 𝜕Πt
𝜕St
dSt + 1
2𝜎2 𝜕2Πt
𝜕S2
t
dt
= 𝜕Πt
𝜕St
dSt +
(
𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
)
dt.
In contrast, the portfolio (𝜙t, 𝜓t) is self-financing if and only if
dΠt = 𝜙tdSt + 𝜓tdBt = 𝜙tdSt + rBt𝜓tdt.
By equating both of the equations we have
rBt𝜓t = 𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
and
𝜙t = 𝜕Πt
𝜕St
and substituting the above two equations into
Πt = 𝜙tSt + 𝜓tBt
we have
𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
+ rSt
𝜕Πt
𝜕St
−rΠt = 0.

4.2.3
Risk-Neutral Measure
237
N.B. Take note that the Black–Scholes equation does not contain the growth parameter
𝜇, which means that the value of a self-financing portfolio is independent of how rapidly
or slowly a stock grows. In essence, the only parameter from the stochastic differential
equation dSt = 𝜇Stdt + 𝜎StdWt that affects the value of the portfolio is the volatility.
◽
14. Black–Scholes Equation with Stock Paying Continuous Dividend Yield. Consider an econ-
omy consisting of a risk-free asset and a stock price (risky asset). At time t, the risk-free
asset Bt and the stock price St have the following diffusion processes
dBt = rBtdt, dSt = (𝜇−D)Stdt + 𝜎StdWt
where r is the risk-free rate, 𝜇is the stock price drift rate, D is the continuous dividend
yield, 𝜎is the stock price volatility (which are all constants) and {Wt ∶0 ≤t ≤T} is a
ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
At time t, we consider a trader who has a portfolio valued at Πt, given as
Πt = 𝜙tSt + 𝜓tBt
where he holds 𝜙t shares of stock and 𝜓t units being invested in a risk-free asset. Show
that the portfolio (𝜙t, 𝜓t) is self-financing if and only if Πt satisfies the Black–Scholes
equation with continuous dividend yield
𝜕Πt
𝜕t + 1
2𝜎2 𝜕2Πt
𝜕S2
t
+ (r −D)St
𝜕Πt
𝜕St
−rΠt = 0.
Solution: By applying Taylor’s theorem on dΠt,
dΠt = 𝜕Πt
𝜕t dt + 𝜕Πt
𝜕St
dSt + 1
2
𝜕2Πt
𝜕S2
t
dS2
t + . . .
and since dS2
t = ((𝜇−D)Stdt + 𝜎StdWt)2 = 𝜎2S2
t dt such that (dt)𝜈= 0, 𝜈≥2, we have
dΠt = 𝜕Πt
𝜕t dt + 𝜕Πt
𝜕St
dSt + 1
2𝜎2 𝜕2Πt
𝜕S2
t
dt
= 𝜕Πt
𝜕St
dSt +
(
𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
)
dt.
Since the trader will receive DStdt for every stock held, the portfolio (𝜙t, 𝜓t) is
self-financing if and only if
dΠt = 𝜙tdSt + 𝜓tdBt + 𝜙tDStdt = 𝜙tdSt + (rBt𝜓t + 𝜙tDSt)dt.
By equating both of the equations we have
rBt𝜓t + 𝜙tDSt = 𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
and
𝜙t = 𝜕Πt
𝜕St

238
4.2.3
Risk-Neutral Measure
and substituting the above two equations into
Πt = 𝜙tSt + 𝜓tBt
we have
𝜕Πt
𝜕t + 1
2𝜎2S2
t
𝜕2Πt
𝜕S2
t
+ (r −D)St
𝜕Πt
𝜕St
−rΠt = 0.
◽
15. Foreign Exchange Rate under Domestic Risk-Neutral Measure. We consider a foreign
exchange (FX) market which at time t consists of a foreign-to-domestic FX spot rate Xt, a
risk-free asset in domestic currency Bd
t and a risk-free asset in foreign currency Bf
t . Here,
XtBf
t denotes the foreign risk-free asset quoted in domestic currency. Assume that the
evolution of these values has the following diffusion processes
dXt = 𝜇tXtdt + 𝜎tXtdWx
t , dBd
t = rd
t Bd
t dt and dBf
t = rf
t Bf
t
where 𝜇t is the drift parameter, 𝜎t is the volatility parameter, rd
t is the domestic risk-free
rate and rf
t is the foreign risk-free rate (which are all time dependent) and {Wx
t ∶0 ≤t ≤
T} is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
From the discounted foreign risk-free asset in domestic currency
̃Xt = XtBf
t
Bd
t
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent domestic
risk-neutral measure ℚd then ̃Xt is a ℚd-martingale.
Finally, show that under the ℚd measure the FX rate follows
dXt = (rd
t −rf
t )Xt + 𝜎tXtd ̃Wd
t
where ̃Wd
t = Wx
t + ∫
t
0
𝜆u du is a ℚd-standard Wiener process with 𝜆t = 𝜇t + rf
t −rd
t
𝜎t
.
Solution: By applying Taylor’s theorem on d̃Xt,
d̃Xt = 𝜕̃Xt
𝜕Xt
dXt + 𝜕̃Xt
𝜕Bf
t
dBf
t + 𝜕̃Xt
𝜕Bd
t
dBd
t + 1
2
𝜕2̃Xt
𝜕X2
t
(dXt)2 + 1
2
𝜕̃Xt
𝜕Bf
t
(dBf
t )2 + 1
2
𝜕̃Xt
𝜕Bd
t
(dBd
t )2
+ 𝜕2̃Xt
𝜕Xt𝜕Bf
t
(dXt)(dBf
t ) +
𝜕2̃Xt
𝜕Xt𝜕Bd
t
(dXt)(dBd
t ) +
𝜕2̃Xt
𝜕Bf
t 𝜕Bd
t
(dBf
t)(dBd
t ) + . . .

4.2.3
Risk-Neutral Measure
239
and from It¯o’s formula,
d̃Xt = 𝜕̃Xt
𝜕Xt
dXt + 𝜕̃Xt
𝜕Bf
t
dBf
t + 𝜕̃Xt
𝜕Bd
t
dBd
t + 1
2𝜎2
t X2
t
𝜕2̃Xt
𝜕X2
t
dt
=
(
Bf
t
Bd
t
)
(𝜇tXtdt + 𝜎tXtdWx
t ) +
(
Xt
Bd
t
)
Bf
trf
t dt −
(
XtBf
t
(Bd
t )2
)
rd
t Bd
t dt
=
(
𝜇t + rf
t −rd
t
)
̃Xtdt + 𝜎t̃XtdWx
t
= 𝜎t̃Xt
[(
𝜇t + rf
t −rd
t
𝜎t
)
dt + dWx
t
]
= 𝜎t̃Xtd ̃Wd
t
where ̃Wd
t = Wx
t + ∫
t
0
𝜆u du such that 𝜆t = 𝜇t + rf
t −rd
t
𝜎t
. From Girsanov’s theorem there
exists an equivalent domestic risk-neutral measure ℚd on the filtration ℱs, 0 ≤s ≤t
defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝜆udu−1
2 ∫s
0 𝜆2
udWx
u
so that ̃Wd
t is a ℚd-standard Wiener process.
Therefore, by substituting
dWx
t = d ̃Wd
t −
(
𝜇t + rf
t −rd
t
𝜎t
)
dt
into dXt = 𝜇tXt dt + 𝜎tXtdWx
t , under the domestic risk-neutral measure ℚd, the FX spot
rate SDE is
dXt = (rd
t −rf
t )Xt + 𝜎tXtd ̃Wd
t .
◽
16. Foreign Exchange Rate under Foreign Risk-Neutral Measure. We consider a foreign
exchange (FX) market which at time t consists of a foreign-to-domestic FX spot rate Xt, a
risk-free asset in domestic currency Bd
t and a risk-free asset in foreign currency Bf
t . Here,
Bd
t ∕Xt denotes the domestic risk-free asset quoted in foreign currency. Assume that the
evolution of these values has the following diffusion processes
dXt = 𝜇tXtdt + 𝜎tXtdWx
t , dBd
t = rd
t Bd
t dt and dBf
t = rf
t Bf
t
where 𝜇t is the drift parameter, 𝜎t is the volatility parameter, rd
t is the domestic risk-free
rate and rf
t is the foreign risk-free rate (which are all time dependent) and {Wx
t ∶0 ≤t ≤
T} is a ℙ-standard Wiener process on the probability space (Ω, ℱ, ℙ).
From the discounted domestic risk-free asset in foreign currency
̃Xt = Bd
t
XtBf
t

240
4.2.3
Risk-Neutral Measure
show, using Girsanov’s theorem, that by changing the measure ℙto an equivalent foreign
risk-neutral measure ℚf then ̃Xt is a ℚf-martingale.
Finally, show that under the ℚf measure the FX rate follows
dXt = (rd
t −rf
t + 𝜎2
t )Xt + 𝜎tXtd ̃Wf
t
where ̃Wf
t = Wx
t + ∫
t
0
𝜆u du is a ℚf -standard Wiener process with 𝜆t = 𝜇t + rf
t −rd
t −𝜎2
t
𝜎t
.
Solution: By applying Taylor’s theorem on d̃Xt,
d̃Xt = 𝜕̃Xt
𝜕Xt
dXt + 𝜕̃Xt
𝜕Bf
t
dBf
t + 𝜕̃Xt
𝜕Bd
t
dBd
t + 1
2
𝜕2̃Xt
𝜕X2
t
(dXt)2 + 1
2
𝜕̃Xt
𝜕Bf
t
(dBf
t )2 + 1
2
𝜕̃Xt
𝜕Bd
t
(dBd
t )2
+ 𝜕2̃Xt
𝜕Xt𝜕Bf
t
(dXt)(dBf
t ) +
𝜕2̃Xt
𝜕Xt𝜕Bd
t
(dXt)(dBd
t ) +
𝜕2̃Xt
𝜕Bf
t 𝜕Bd
t
(dBf
t)(dBd
t ) + . . .
and from It¯o’s formula,
d̃Xt = 𝜕̃Xt
𝜕Xt
dXt + 𝜕̃Xt
𝜕Bf
t
dBf
t + 𝜕̃Xt
𝜕Bd
t
dBd
t + 1
2𝜎2
t X2
t
𝜕2̃Xt
𝜕X2
t
dt
= −
(
Bd
t
X2
t Bf
t
)
(𝜇tXtdt + 𝜎tXtdWx
t ) −
(
Bd
t
Xt(Bf
t )2
)
Bf
t rf
t dt +
(
1
XtBf
t
)
rd
t Bd
t dt
+
(
Bd
t
X3
t Bf
t
)
𝜎2
t X2
t dt
=
(
rd
t −rf
t + 𝜎2
t −𝜇t
)
̃Xtdt −𝜎t̃XtdWx
t
= −𝜎t̃Xt
[(
𝜇t + rf
t −rd
t −𝜎2
t
𝜎t
)
dt + dWx
t
]
= −𝜎t̃Xtd ̃Wf
t
where ̃Wf
t = Wx
t + ∫
t
0
𝜆u du such that 𝜆t = 𝜇t + rf
t −rd
t −𝜎2
t
𝜎t
. From Girsanov’s theorem
there exists an equivalent foreign risk-neutral measure ℚf on the filtration ℱs, 0 ≤s ≤t
defined by the Radon–Nikod´ym derivative
Zs = e−∫s
0 𝜆udu−1
2 ∫s
0 𝜆2
udWx
u
so that ̃Wf
t is a ℚf -standard Wiener process.

4.2.3
Risk-Neutral Measure
241
Therefore, by substituting
dWx
t = d ̃Wf
t −
(
𝜇t + rf
t −rd
t −𝜎2
t
𝜎t
)
dt
into dXt = 𝜇tXt dt + 𝜎tXtdWx
t , under the foreign risk-neutral measure ℚf , the FX spot rate
SDE is
dXt = (rd
t −rf
t + 𝜎2
t )Xt + 𝜎tXtd ̃Wf
t .
◽
17. Foreign-Denominated Stock Price under Domestic Risk-Neutral Measure. Let (Ω, ℱ, ℙ)
be a probability space and let {Ws
t ∶0 ≤t ≤T} and {Wx
t ∶0 ≤t ≤T} be ℙ-standard
Wiener processes on the filtration ℱt, 0 ≤t ≤T. Let St and Xt denote the stock price
quoted in foreign currency and the foreign-to-domestic exchange rate, respectively, each
having the following SDEs
dSt = (𝜇s −Ds)Stdt + 𝜎sStdWs
t
dXt = 𝜇xXtdt + 𝜎xXtdWx
t
dWs
t ⋅dWx
t = 𝜌dt,
𝜌∈(−1, 1)
where 𝜇s, Ds and 𝜎s are the stock price drift, continuous dividend yield and volatility,
respectively, whilst 𝜇x and 𝜎x are the exchange rate drift and volatility, respectively. Here,
we assume Ws
t and Wx
t are correlated with correlation coefficient 𝜌∈(−1, 1). In addition,
let Bf
t and Bd
t be the risk-free assets in foreign and domestic currencies, respectively, having
the following differential equations
dBf
t = rf Bf
tdt and dBd
t = rdBd
t dt
where rf and rd are the foreign and domestic risk-free rates.
Show that for the stock price denominated in domestic currency, XtSt follows the diffusion
process
d(XtSt)
XtSt
= (𝜇s + 𝜇x + 𝜌𝜎s𝜎s −Ds)dt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xdWxs
t
where Wxs
t
=
𝜎sWs
t + 𝜎xWx
t
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
x
is a ℙ-standard Wiener process.
Using Girsanov’s theorem show that under the domestic risk-neutral measure ℚd, the stock
price denominated in domestic currency has the diffusion process
d(XtSt)
XtSt
= (rd −Ds)dt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xd ̃Wxs
t
where ̃Wxs
t
= Wxs
t +
⎛
⎜
⎜
⎜⎝
𝜇s + 𝜇x + 𝜌𝜎s𝜎s −rd
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
x
⎞
⎟
⎟
⎟⎠
t is a ℚd-standard Wiener process.

242
4.2.3
Risk-Neutral Measure
Solution: From Problem 3.2.3.3 (page 158) we can easily show that for XtSt, its diffusion
process is
d(XtSt)
XtSt
= (𝜇s + 𝜇x + 𝜌𝜎s𝜎x −Ds)dt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xdWxs
t
where Wxs
t
=
𝜎sWs
t + 𝜎xWx
t
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
x
is a ℙ-standard Wiener process.
At time t, we let the portfolio Πt be valued as
Πt = 𝜙tUt + 𝜓tBd
t
where 𝜙t and 𝜓t are the units invested in Ut = XtSt and the risk-free asset Bd
t , respectively.
Taking note that the holder will receive DsUtdt for every stock held, then
dΠt = 𝜙t(dUt + DsUtdt) + 𝜓trdBd
t dt
= 𝜙t
[
(𝜇s + 𝜇x + 𝜌𝜎s𝜎x)Utdt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xUtdWxs
t
]
+ 𝜓trdBd
t dt
= rdΠtdt + 𝜙t
[
(𝜇s + 𝜇x + 𝜌𝜎s𝜎x −rd)Utdt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xUtdWxs
t
]
= rdΠtdt + 𝜙t
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xUtd ̃Wxs
t
where ̃Wxs
t
= 𝜆t + Wxs
t such that 𝜆= 𝜇s + 𝜇x + 𝜌𝜎s𝜎x −rd
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
x
.
By applying Girsanov’s theorem to change the measure ℙto an equivalent risk-neutral
measure ℚd, under which ̃Wxs
t is a ℚd-standard Wiener process, then the discounted port-
folio e−rdtΠt is a ℚd-martingale.
Finally, by substituting dWxs
t
= d ̃Wxs
t −𝜆dt into
d(XtSt)
XtSt
= (𝜇s + 𝜇x + 𝜌𝜎s𝜎s −Ds)dt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xdWxs
t
the stock price diffusion process under the risk-neutral measure ℚd becomes
d(XtSt)
XtSt
= (rd −Ds)dt +
√
𝜎2
s + 2𝜌𝜎s𝜎x + 𝜎2
xd ̃Wxs
t .
◽

5
Poisson Process
In mathematical finance the most important stochastic process is the Wiener process, which
is used to model continuous asset price paths. The next important stochastic process is the
Poisson process, used to model discontinuous random variables. Although time is continuous,
the variable is discontinuous where it can represent a “jump” in an asset price (e.g., electricity
prices or a credit risk event, such as describing default and rating migration scenarios). In
this chapter we will discuss the Poisson process and some generalisations of it, such as the
compound Poisson process and the Cox process (or doubly stochastic Poisson process) that
are widely used in credit risk theory as well as in modelling energy prices.
5.1
INTRODUCTION
In this section, before we provide the definition of a Poisson process, we first define what a
counting process is.
Definition 5.1 (Counting Process) Let (Ω, ℱ, ℙ) be a probability space. A stochastic process
{Nt ∶t ≥0} where Nt denotes the number of events that have occurred in the interval (0, t] is
said to be a counting process if it has the following properties:
(a) Nt ≥0;
(b) Nt is an integer value;
(c) for s < t, Ns ≤Nt and Nt −Ns is the number of events occurring in (s, t].
Once we have defined a counting process we can now give a proper definition of a Poisson
process. Basically, there are three definitions of a Poisson process (or homogeneous Poisson
process) and they are all equivalent.
Definition 5.2(a) Let (Ω, ℱ, ℙ) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt ∶t ≥0} with intensity 𝜆> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
ℙ(Nt+h = i + j|Nt = i) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
j = 0
𝜆h + o(h)
j = 1
o(h)
j > 1.

244
5.1
INTRODUCTION
Definition 5.2(b) Let (Ω, ℱ, ℙ) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt ∶t ≥0} with intensity 𝜆> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) ℙ(Nt = k) = (𝜆t)ke−𝜆t
k!
, k = 0, 1, 2, . . .
Definition 5.2(c) Let (Ω, ℱ, ℙ) be a probability space. A Poisson process (or homogeneous
Poisson process) {Nt ∶t ≥0} with intensity 𝜆> 0 is a counting process with the following
properties:
(a) N0 = 0;
(b) the inter-arrival times of events 𝜏1, 𝜏2, . . . form a sequence of independent and identically
distributed random variables where 𝜏i ∼Exp(𝜆), i = 1, 2, . . . ;
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
ℙ(Nt+h = i + j|Nt = i) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
j = 0
𝜆h + o(h)
j = 1
o(h)
j > 1.
Like a standard Wiener process, the Poisson process has independent and stationary incre-
ments and so it is also a Markov process.
Theorem 5.3 (Markov Property) Let (Ω, ℱ, ℙ) be a probability space. The Poisson process
{Nt ∶t ≥0} is a Markov process such that the conditional distribution of Nt given the filtration
ℱs, s < t depends only on Ns.
In addition, the Poisson process also has a strong Markov property.
Theorem 5.4 (Strong Markov Property) Let (Ω, ℱ, ℙ) be a probability space. If {Nt ∶t ≥
0} is a Poisson process and given ℱt is the filtration up to time t then for s > 0, Nt+s −Nt ⟂⟂ℱt.
By modifying the intensity parameter we can construct further definitions of a Poisson pro-
cess and the following definitions (which are all equivalent) describe a non-homogeneous
Poisson process with intensity 𝜆t being a deterministic function of time.
Definition 5.5(a) Let (Ω, ℱ, ℙ) be a probability space. A Poisson process (or non-
homogeneous Poisson process) {Nt ∶t ≥0} with intensity function 𝜆t ∶ℝ+ →ℝ+ is a
counting process with the following properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;

5.1
INTRODUCTION
245
(c) the sample paths Nt have jump discontinuities of unit magnitude such that for h > 0
ℙ(Nt+h = i + j|Nt = i) =
⎧
⎪
⎨
⎪⎩
1 −𝜆th + o(h)
j = 0
𝜆th + o(h)
j = 1
o(h)
j > 1.
Definition 5.5(b) Let (Ω, ℱ, ℙ) be a probability space. A Poisson process (or non-
homogeneous Poisson process) {Nt ∶t ≥0} with intensity function 𝜆t ∶ℝ+ →ℝ+ is a
counting process with the following properties:
(a) N0 = 0;
(b) Nt has independent and stationary increments;
(c) ℙ(Nt = k) = (Λt)ke−Λt
k!
, k = 0, 1, 2, . . . where Λt = ∫
t
0
𝜆u du is known as the intensity
measure (hazard function or cumulative intensity).
It should be noted that if 𝜆t is itself a random process, then {Nt ∶t ≥0} is called a doubly
stochastic Poisson process, conditional Poisson process or Cox process. Given that 𝜆t is a
random process, the resulting stochastic process {Λt ∶t ≥0} defined as
Λt = ∫
t
0
𝜆u du
is known as the hazard process. In credit risk modelling, due to the stochastic process of the
intensity, the Cox process can be used to model the random occurrence of a default event, or
even the number of contingent claims in actuarial models (claims that can be made only if one
or more specified events occurs).
Given that the Poisson process Nt with intensity 𝜆is an increasing counting process, therefore
it is not a martingale. However, in its compensated form, Nt −𝜆t is a martingale.
Theorem 5.6 Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the prob-
ability space (Ω, ℱ, ℙ) with respect to the filtration ℱt. By defining the compensated Poisson
process ̂Nt as
̂Nt = Nt −𝜆t
then ̂Nt is a martingale.
In the following we define an important generalisation of the Poisson process known as a
compound Poisson process, where the jump size (or amplitude) can be modelled as a random
variable.
Definition 5.7 (Compound Poisson Process) Let {Nt ∶t ≥0} be a Poisson process with
intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt,
and let X1, X2, . . . be a sequence of independent and identically distributed random variables

246
5.1
INTRODUCTION
with common mean 𝔼(Xi) = 𝔼(X) = 𝜇and variance Var (Xi) = Var (X) = 𝜎2. Assume also that
X1, X2, . . . are independent of Nt. The compound Poisson process Mt is defined as
Mt =
Nt
∑
i=1
Xi,
t ≥0.
From the definition we can deduce that although the jumps in Nt are always of one unit, the
jumps in Mt are of random size since Xt is a random variable. The only similarity between
the two processes is that the jumps in Nt and Mt occur at the same time. Furthermore, like the
Poisson process, the compound Poisson process Mt also has the independent and stationary
increments property.
Another important generalisation of the Poisson process is to add a drift and a standard
Wiener process term to generate a jump diffusion process of the form
dXt = 𝜇(Xt−, t) dt + 𝜎(Xt−, t)dWt + J(Xt−, t)dNt
where J(Xt−, t) is a random variable denoting the size of the jump and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
In the presence of jumps, Xt is a càdlàg process, where it is continuous from the right
lim
u↑t Xu = lim
u→t+ Xu = Xt
which is also inclusive of any jumps at time t. To specify the value just before a jump we write
Xt−, which is the limit from the left:
lim
u↓t Xu = lim
u→t−Xu = Xt−.
Let Xt be a càdlàg process with jump times 𝜏1 < 𝜏2 < . . . The integral of a function 𝜓t with
respect to Xt is defined by the pathwise Lebesgue–Stieltjes integral
∫
t
0
𝜓s dXs =
∑
0<s≤t
𝜓sΔXs =
∑
0<s≤t
𝜓s(Xs −Xs−) =
Xt
∑
s=1
𝜓s
where the size of the jump, ΔXt is denoted by
ΔXt = Xt −Xt−
and given there is no jump at time zero we therefore set ΔX0 = 0.
Theorem 5.8 (One-Dimensional It¯o Formula for Jump Diffusion Process) Consider a
stochastic process Xt satisfying the following SDE:
dXt = 𝜇(Xt−, t) dt + 𝜎(Xt−, t)dWt + J(Xt−, t)dNt
or in integrated form
Xt = X0 + ∫
t
0
𝜇(Xs−, s) ds + ∫
t
0
𝜎(Xs−, s) dWs + ∫
t
0
J(Xs−, s) dNs

5.1
INTRODUCTION
247
with ∫
t
0
{|𝜇(Xs, s)| + 𝜎(Xs, s)2} ds < ∞. Then, for a function f(Xt, t), the stochastic process
Yt = f(Xt, t) satisfies
dYt = 𝜕f
𝜕t (Xt−, t) dt + 𝜕f
𝜕Xt
(Xt−, t)dXt + 1
2!
𝜕2f
𝜕X2
t
(Xt−, t)(dXt)2 + 1
3!
𝜕3f
𝜕X3
t
(Xt−, t)(dXt)3 + . . .
=
[
𝜕f
𝜕t (Xt−, t) + 𝜇(Xt−, t) 𝜕f
𝜕Xt
(Xt−, t) + 1
2𝜎(Xt−, t)2 𝜕2f
𝜕X2
t
(Xt−, t)
]
dt
+ 𝜎(Xt−, t) 𝜕f
𝜕Xt
(Xt−, t)dWt + [ f(Xt−+ Jt−, t) −f(Xt−, t)]dNt
where Jt−= J(Xt−, t), (dXt)m, m = 1, 2, . . . are expressed according to the rule
( dt)2 = ( dt)3 = . . . = 0
(dWt)2 = dt,
(dWt)3 = (dWt)4 = . . . = 0
(dNt)2 = (dNt)3 = . . . = dNt
dWt dt = dNt dt = dNtdWt = 0.
In integrated form
Yt = Y0 + ∫
t
0
𝜕f
𝜕t (Xs−, s) ds + ∫
t
0
𝜕f
𝜕Xt
(Xs−, s) dXs
+ 1
2! ∫
t
0
𝜕2f
𝜕X2
t
(Xs−, s)(dXs)2 + 1
3! ∫
t
0
𝜕3f
𝜕X3
t
(Xs−, s)(dXs)3 + . . .
= Y0 + ∫
t
0
[
𝜕f
𝜕t (Xs−, s) + 𝜇(Xs−, s) 𝜕f
𝜕Xt
(Xs−, s) + 1
2𝜎(Xs−, s)2 𝜕2f
𝜕X2
t
(Xs−, s)
]
ds
+ ∫
t
0
𝜎(Xs−, s) 𝜕f
𝜕Xt
(Xs−, s) dWs + ∫
t
0
[ f(Xs−+ Js−, s) −f(Xs−, s)] dNs
where
∫
t
0
[ f(Xs−+ Js−, s) −f(Xs−, s)] dNs =
∑
0<s≤t
[ f(Xs, s) −f(Xs−, s)] ΔNs
=
Nt
∑
s=1
[ f(Xs, s) −f(Xs−, s)].
The following is the multi-dimensional version of It¯o’s formula for a jump diffusion process.
Theorem 5.9 (Multi-Dimensional It¯o Formula for Jump Diffusion Process) Consider the
stochastic processes X(i)
t , i = 1, 2, . . . , n each satisfying the following SDE:
dX(i)
t
= 𝜇(X(i)
t−, t) dt + 𝜎(X(i)
t−, t)dWt + J(X(i)
t−, t)dNt

248
5.1
INTRODUCTION
or in integrated form
X(i)
t
= X(i)
0 + ∫
t
0
𝜇(X(i)
s−, s) ds + ∫
t
0
𝜎(X(i)
s−, s) dWs + ∫
t
0
J(X(i)
s−, s) dNs
with ∫
t
0
{
|𝜇(X(i)
s , s)| + 𝜎(X(i)
s , s)2}
ds < ∞. Then for a function f(X(1)
t , X(2)
t , . . . , X(n)
t , t), the
stochastic process Yt = f(X(1)
t , X(2)
t , . . . , X(n)
t , t) satisfies
dYt = 𝜕f
𝜕t (X(1)
t−, X(2)
t−, . . . , X(n)
t−, t) dt +
n
∑
i=1
𝜕f
𝜕X(i)
t
(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)dX(i)
t
+
n
∑
i=1
n
∑
j=1
1
2!
𝜕2f
𝜕X(i)
t 𝜕X(j)
t
(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)dX(i)
t dX(j)
t + . . .
=
[
𝜕f
𝜕t (X(1)
t−, X(2)
t−, . . . , X(n)
t−, t) +
n
∑
i=1
𝜇(X(i)
t−, t) 𝜕f
𝜕Xt
(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)
+
n
∑
i=1
n
∑
j=1
1
2𝜎(X(i)
t−, t)𝜎(X(j)
t−, t)
𝜕2f
𝜕X(i)
t 𝜕X(j)
t
(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)
]
dt
+
n
∑
i=1
𝜎(X(i)
t−, t) 𝜕f
𝜕X(i)
t
(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)dWt
+
[
f(X(1)
t−+ J(1)
t−, X(2)
t−+ J(2)
t−, . . . , X(n)
t−+ J(n)
t−, t) −f(X(1)
t−, X(2)
t−, . . . , X(n)
t−, t)
]
dNt
where J(i)
t−= J(X(i)
t−, t), (dXt)m, m = 1, 2, . . . are expressed according to the rule
( dt)2 = ( dt)3 = . . . = 0
(dWt)2 = dt,
(dWt)3 = (dWt)4 = . . . = 0
(dNt)2 = (dNt)3 = . . . = dNt
dWt dt = dNt dt = dNtdWt = 0.
In integrated form
Yt = Y0 + ∫
t
0
𝜕f
𝜕t (X(1)
s−, X(2)
s−, . . . , X(n)
s−, s) ds + ∫
t
0
[ n
∑
i=1
𝜕f
𝜕X(i)
t
(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s) dX(i)
s
]
+ ∫
t
0
[ n
∑
i=1
n
∑
j=1
1
2!
𝜕2f
𝜕X(i)
t 𝜕X(j)
t
(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s) dX(i)
s dX(j)
s
]
+ . . .
= Y0 + ∫
t
0
[
𝜕f
𝜕t (X(1)
s−, X(2)
s−, . . . , X(n)
s−, s) +
n
∑
i=1
𝜇(X(i)
s−, s) 𝜕f
𝜕Xt
(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)

5.1
INTRODUCTION
249
+
n
∑
i=1
n
∑
j=1
1
2𝜎(X(i)
s−, s)𝜎(X(j)
s−, s)
𝜕2f
𝜕X(i)
t 𝜕X(j)
t
(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)
]
ds
+ ∫
t
0
[ n
∑
i=1
𝜎(X(i)
s−, s) 𝜕f
𝜕X(i)
t
(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)dWs
]
+ ∫
t
0
[
f(X(1)
s−+ J(1)
s−, X(2)
s−+ J(2)
s−, . . . , X(n)
s−+ J(n)
s−, s) −f(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)
]
dNs
where
∫
t
0
[
f(X(1)
s−+ J(1)
s−, X(2)
s−+ J(2)
s−, . . . , X(n)
s−+ J(n)
s−, s) −f(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)
]
dNs
=
∑
0<s≤t
[
f(X(1)
s , X(2)
s , . . . , X(n)
s s) −f(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)
]
ΔNs
=
Nt
∑
s=1
[
f(X(1)
s , X(2)
s , . . . , X(n)
s s) −f(X(1)
s−, X(2)
s−, . . . , X(n)
s−, s)
]
.
By recalling that we can use Girsanov’s theorem to change the measure so that a Wiener
process (standard Wiener process with drift) becomes a standard Wiener process, we can also
change the measure for both Poisson and compound Poisson processes. For a Poisson process,
the outcome of the change of measure affects the intensity whilst for a compound Poisson
process, the change of measure affects both the intensity and the distribution of the jump ampli-
tudes. In the following we provide general results for the change of measure for Poisson and
compound Poisson processes.
Theorem 5.10 (Girsanov’s Theorem for Poisson Process) Let {Nt ∶0 ≤t ≤T} be a Pois-
son process with intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with respect to the
filtration ℱt, 0 ≤t ≤T. Let 𝜂> 0 and consider the Radon–Nikod´ym derivative process
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt.
By changing the measure ℙto a measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then, under the ℚmeasure, Nt ∼Poisson(𝜂t) with intensity 𝜂> 0.
Theorem 5.11 (Girsanov’s Theorem for Compound Poisson Process) Let {Nt ∶0 ≤t ≤
T} be a Poisson process with intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with
respect to the filtration ℱt, 0 ≤t ≤T and let X1, X2, . . . be a sequence of independent and
identically distributed random variables where each Xi, i = 1, 2, . . . has a probability density
function f ℙ(Xi) under the ℙmeasure. Let X1, X2, . . . also be independent of Nt and Wt. From
the definition of the compound Poisson process
Mt =
Nt
∑
i=1
Xi,
t ≥0

250
5.1
INTRODUCTION
we let 𝜂> 0 and consider the Radon–Nikod´ym derivative process
Zt =
⎧
⎪
⎪
⎨
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under the ℚ
measure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let ℚbe abso-
lutely continuous with respect to ℙon ℱ. By changing the measure ℙto measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then, under the ℚmeasure, Mt is a compound Poisson process with intensity 𝜂> 0 and Xi,
i = 1, 2, . . . is a sequence of independent and identically distributed random variables with
probability mass (density) functions ℚ(Xi) (f ℚ(Xi)), i = 1, 2, . . .
By augmenting the Wiener process to the Poisson and compound Poisson processes we have
the following results.
Theorem 5.12 (Girsanov’s Theorem for Poisson Process and Standard Wiener Pro-
cess) Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 and {Wt ∶0 ≤t ≤T}
be a standard Wiener process defined on the probability space (Ω, ℱ, ℙ) with respect to the
filtration ℱt, t ≥0. Suppose 𝜃t, 0 ≤t ≤T is an adapted process and 𝜂> 0. We consider the
Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
= e(𝜆−𝜂)t(𝜂
𝜆
)Nt
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du.
By changing the measure ℙto measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then, under the ℚmeasure, Nt is a Poisson process with intensity 𝜂> 0, the process ̃Wt =
Wt + ∫
t
0
𝜃u du is a ℚ-standard Wiener process and Nt ⟂⟂̃Wt.
Theorem 5.13 (Girsanov’s Theorem for Compound Poisson Process and Standard
Wiener Process) Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 and
{Wt ∶0 ≤t ≤T} be a standard Wiener process defined on the probability space (Ω, ℱ, ℙ)

5.2.1
Properties of Poisson Process
251
with respect to the filtration ℱt, 0 ≤t ≤T. Suppose 𝜃t, 0 ≤t ≤T is an adapted process,
𝜂> 0, X1, X2, . . . is a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function ℙ(Xi) > 0 (f ℙ(Xi) > 0)
under the ℙmeasure and assume also that the sequence X1, X2, . . . is independent of Nt. We
consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under the
ℚmeasure and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2
u du)
< ∞. For the case of continuous random variables Xi, i =
1, 2, . . . we also let ℚbe absolutely continuous with respect to ℙon ℱ. By changing the
measure ℙto measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then, under the ℚmeasure, the process Mt = ∑Nt
i=1 Xi is a compound Poisson process with
intensity 𝜂> 0 where Xi, i = 1, 2, . . . is a sequence of independent and identically distributed
random variables with probability mass (density) functions ℚ(Xi) (f ℚ(Xi)), i = 1, 2, . . . , the
process ̃Wt = Wt + ∫
t
0
𝜃u du is a ℚstandard Wiener process and Mt ⟂⟂̃Wt.
5.2
PROBLEMS AND SOLUTIONS
5.2.1
Properties of Poisson Process
1. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0. Show that Cov (Ns, Nt
) = 𝜆min{s, t} and deduce that the correlation coefficient
of Ns and Nt is
𝜌=
√
min{s, t}
max{s, t}.
Solution: Since Nt ∼Poisson(𝜆t) with 𝔼(Nt
) = 𝜆t, 𝔼(N2
t
) = 𝜆t + 𝜆2t2 and by definition
Cov (Ns, Nt
) = 𝔼(NsNt
) −𝔼(Ns
) 𝔼(Nt
) .

252
5.2.1
Properties of Poisson Process
For s ≤t, using the stationary and independent properties of a Poisson process,
𝔼(NsNt
) = 𝔼(Ns
(Nt −Ns
) + N2
s
)
= 𝔼(Ns
(Nt −Ns
)) + 𝔼(N2
s
)
= 𝔼(Ns
) 𝔼(Nt −Ns
) + 𝔼(N2
s
) ,
Ns ⟂⟂Nt −Ns
= 𝜆s ⋅𝜆(t −s) + 𝜆s + 𝜆2s2
= 𝜆2st + 𝜆s
which implies
Cov (Ns, Nt
) = 𝜆2st + 𝜆s −𝜆2st = 𝜆s.
Similarly, for t ≤s,
𝔼(NsNt
) = 𝔼(Nt
(Ns −Nt
) + N2
t
)
= 𝔼(Nt
(Ns −Nt
)) + 𝔼(N2
t
)
= 𝔼(Nt
) 𝔼(Ns −Nt
) + 𝔼(N2
t
) ,
Nt ⟂⟂Ns −Nt
= 𝜆t ⋅𝜆(s −t) + 𝜆t + 𝜆2t2
= 𝜆2st + 𝜆t
and hence
Cov(Ns, Nt) = 𝜆2st + 𝜆t −𝜆2st = 𝜆t.
Thus, Cov(Ns, Nt) = 𝜆min{s, t}.
By definition, the correlation coefficient of Ns and Nt is defined as
𝜌=
Cov (Ns, Nt
)
√
Var (Ns
) Var (Nt
)
= 𝜆min{s, t}
𝜆
√
st
= min{s, t}
√
st
.
For s ≤t,
𝜌=
s
√
st
=
√
s
t
whilst for s > t,
𝜌=
t
√
st
=
√
t
s.
Therefore, 𝜌=
√
min{s, t}
max{s, t}.
◽

5.2.1
Properties of Poisson Process
253
2. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0 satisfying the following conditions for small h > 0 and k ∈ℕ:
(a) N0 = 0 and Nt is a non-decreasing counting process;
(b) ℙ(Nt+h = i + j|Nt = i) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
j = 0
𝜆h + o(h)
j = 1
o(h)
j > 1;
(c) Nt has independent stationary increments.
By setting pk(t) = ℙ(Nt = k), show that
p′
k(t) =
{
−𝜆p0(t)
k = 0
𝜆pk−1(t) −𝜆pk(t)
k ≠0
with boundary condition
pk(0) =
{
1
k = 0
0
k ≠0.
By solving the differential-difference equation using mathematical induction or the
moment generating function, show that
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t,
k = 0, 1, 2, . . .
Solution: By definition, for k ≠0,
ℙ(Nt+h = k) =
∑
j
ℙ(Nt+h = k|Nt = j)ℙ(Nt = j)
= ℙ(Nt+h = k|Nt = k −1)ℙ(Nt = k −1)
+ ℙ(Nt+h = k|Nt = k)ℙ(Nt = k) + o(h)
= (𝜆h + o(h))ℙ(Nt = k −1) + (1 −𝜆h + o(h))ℙ(Nt = k).
By setting pk(t + h) = ℙ(Nt+h = k), pk−1(t) = ℙ(Nt = k −1) and pk(t) = ℙ(Nt = k), there-
fore for k ≠0,
pk(t + h) = 𝜆hpk−1(t) + (1 −𝜆h)pk(t) + o(h)
lim
h→0
pk(t + h) −pk(t)
h
= 𝜆pk−1(t) −𝜆pk(t) + lim
h→0
o(h)
h
and we will have
p′
k(t) = 𝜆pk−1(t) −𝜆pk(t),
k ≠0.
For the case when k = 0,
ℙ(Nt+h = 0) = ℙ(Nt+h = 0|Nt = 0)ℙ(Nt = 0) + o(h)
= (1 −𝜆h + o(h))ℙ(Nt = 0) + o(h)
or
p0(t + h) = (1 −𝜆h)p0(t) + o(h)

254
5.2.1
Properties of Poisson Process
where p0(t + h) = ℙ(Nt+h = 0) and p0(t) = ℙ(Nt = 0). By subtracting p0(t) from both
sides of the equation, dividing by h and letting h →0, we will have
p′
0(t) = −𝜆p0(t).
Knowing that ℙ(N0 = 0) = 1, hence
p′
k(t) =
{
−𝜆p0(t)
k = 0
𝜆pk−1(t) −𝜆pk(t)
k ≠0
with boundary condition
pk(0) =
{
1
k = 0
0
k ≠0.
The following are two methods to solve the differential-difference equation.
Method I: Mathematical Induction. Let k = 0. To solve
p′
0(t) + 𝜆p0(t) = 0
we let the integrating factor I = e𝜆t. By multiplying the integrating factor on both sides of
the equation and taking note that p0(0) = ℙ(N0 = 0) = 1, we have
ℙ(Nt = 0) = e−𝜆t.
Therefore, the result is true for k = 0.
Assume the result is true for k = j, j = 0, 1, 2, . . . such that
ℙ(Nt = j) = (𝜆t)je−𝜆t
j!
.
For k = j + 1 the differential-difference equation is
p′
j+1(t) + 𝜆pj+1(t) = 𝜆(𝜆t)je−𝜆t
j!
.
Setting the integrating factor I = e𝜆t and multiplying it by both sides of the differential
equation,
d
dt(e𝜆tpj+1(t)) = 𝜆j+1tj
j!
or
e𝜆tpj+1(t) = (𝜆t)j+1
(j + 1)! + C
where C is a constant. Knowing that pj+1(0) = ℙ(N0 = j + 1) = 0 we have C = 0 and
hence
ℙ(Nt = j + 1) = (𝜆t)j+1e−𝜆t
(j + 1)! .
Therefore, the result is also true for k = j + 1. Thus, using mathematical induction we
have shown
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t,
k = 0, 1, 2, . . .

5.2.1
Properties of Poisson Process
255
Method II: Moment Generating Function. Let
MNt(t, u) = 𝔼(euNt) =
∞
∑
k=0
eukℙ(Nt = k) =
∞
∑
k=0
eukpk(t)
be defined as the moment generating function of the Poisson process. Taking the first
partial derivative with respect to t,
𝜕MNt(t, u)
𝜕t
=
∞
∑
k=0
eukp′
k(t)
= 𝜆
∞
∑
k=1
eukpk−1(t) −𝜆
∞
∑
k=0
eukpk(t)
= 𝜆eu
∞
∑
k=1
eu(k−1)pk−1(t) −𝜆
∞
∑
k=0
eukpk(t)
or
𝜕MNt(t, u)
𝜕t
= 𝜆(eu −1)MNt(t, u)
with boundary condition MN0(0, u) = 1.
By setting the integrating factor I = e−𝜆(eu−1)t and multiplying it by the first-order differ-
ential equation, we have
𝜕
𝜕t
(
MNt(t, u)e−𝜆(eu−1)t)
= 0
or
MNt(t, u)e−𝜆(eu−1)t = C
where C is a constant. Since MN0(0, u) = 1 we have C = 1 and hence
MNt(t, u) = e𝜆(eu−1)t.
By definition, MNt(t, u) = ∑∞
k=0 eukℙ(Nt = k) and using Taylor’s series expansion we have
MNt(t, u) = e𝜆(eu−1)t = e−𝜆t
∞
∑
k=0
(𝜆teu)k
k!
=
∞
∑
k=0
euk
((𝜆t)k
k! e−𝜆t
)
and hence
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t,
k = 0, 1, 2, . . .
◽
3. Pure Birth Process. Let (Ω, ℱ, ℙ) be a probability space. If {Nt ∶t ≥0} is a Poisson
process with intensity 𝜆> 0 then for small h > 0 and k ∈ℕ, show that it satisfies the
following property:
ℙ(Nt+h = k + j|Nt = k) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
j = 0
𝜆h + o(h)
j = 1
o(h)
j > 1.

256
5.2.1
Properties of Poisson Process
Solution: We first consider j = 0 where
ℙ(Nt+h = k|Nt = k) = ℙ(Nt+h = k, Nt = k)
ℙ(Nt = k)
= ℙ(zero arrival between (t, t + h], Nt = k)
ℙ(Nt = k)
.
Since the event {Nt = k} relates to arrival during the time interval [0, t] and the event
{zero arrival between (t, t + h]} relates to arrivals after time t, both of the events are inde-
pendent and because Nt has stationary increments,
ℙ(Nt+h = k|Nt = k) = ℙ(zero arrival between (t, t + h])ℙ(Nt = k)
ℙ(Nt = k)
= ℙ(zero arrival between (t, t + h])
= ℙ(Nt+h −Nt = 0)
= e−𝜆h
= 1 −𝜆h + (𝜆h)2
2!
−(𝜆h)3
3!
+ . . .
= 1 −𝜆h + o(h).
Similarly,
ℙ(Nt+h = k + 1|Nt = k) = ℙ(1 arrival between (t, t + h])
= ℙ(Nt+h −Nt = 1)
= 𝜆he−𝜆h
= 𝜆h
(
1 −𝜆h + (𝜆h)2
2!
−(𝜆h)3
3!
+ . . .
)
= 𝜆h + o(h)
and finally
ℙ(Nt+h > k + 1|Nt = k) = 1 −ℙ(Nt+h = k|Nt+h = k) −ℙ(Nt+h = k + 1|Nt = k)
= 1 −(1 −𝜆h + o(h)) −𝜆h + o(h)
= o(h).
◽
4. Arrival Time Distribution. Let the inter-arrival times of “jump” events 𝜏1, 𝜏2, . . . be a
sequence of independent and identically distributed random variables where each 𝜏i ∼
Exp(𝜆), 𝜆> 0, i = 1, 2, . . . has a probability density function f𝜏(t) = 𝜆e−𝜆t, t ≥0. By
defining the arrival time of the n-th jump event as
Tn =
n
∑
i=1
𝜏i

5.2.1
Properties of Poisson Process
257
where 𝜏i = Ti −Ti−1, show that the arrival time Tn, n ≥1 follows a gamma distribution,
Tn ∼Gamma(n, 𝜆) with probability density function given as
fTn(t) = (𝜆t)n−1
(n −1)!𝜆e−𝜆t,
t ≥0.
Let Nt be the number of jumps that occur at or before time t. Explain why for k ≥1,
ℙ(Nt ≥k) = ℙ(Tk ≤t)
and for k = 0,
ℙ(Nt = 0) = ℙ(T1 > t).
Using the above results show that Nt is a Poisson process with intensity 𝜆having the
probability mass function
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t,
k = 0, 1, 2, . . .
Solution: The first part of the proof is analogous to Problem 1.2.2.5 (page 16) and we
shall omit it. As for the remaining part of the proof we note that for k ≥1, Nt ≥k if and
only if there are at least k jump events by time t, which implies that Tk (the time of the
k-th jump) is less than or equal to t. Thus,
ℙ(Nt ≥k) = ℙ(Tk ≤t),
k ≥1.
On the contrary, for k = 0, Nt = 0 if and only if there are zero jumps by time t, which
implies T1 (the time of the first jump) occurs after time t. Therefore,
ℙ(Nt = 0) = ℙ(T1 > t).
For k = 0,
ℙ(Nt = 0) = ℙ(T1 > t) = ℙ(𝜏1 > t) = ∫
∞
t
𝜆e−𝜆s ds = e−𝜆t.
For k ≥1,
ℙ(Nt = k) = ℙ(Nt ≥k) −ℙ(Nt ≥k + 1).
By solving
ℙ(Nt ≥k + 1) = ℙ(Tk+1 ≤t) = ∫
t
0
(𝜆s)k
k! 𝜆e−𝜆s ds
and using integration by parts ∫
b
a
u(x) d
dx𝑣(x) dx = u(x)𝑣(x)
||||
b
a
−∫
b
a
𝑣(x) d
dxu(x) dx, we
set
u = (𝜆s)k
k!
⇒du
ds = 𝜆ksk−1
(k −1)!

258
5.2.1
Properties of Poisson Process
d𝑣
ds = 𝜆e−𝜆s ⇒𝑣= −e−𝜆s.
Thus,
ℙ(Nt ≥k + 1) = −(𝜆s)k
k! e−𝜆s|||||
s=t
s=0
+ ∫
t
0
(𝜆s)k−1
(k −1)!𝜆e−𝜆s ds = −(𝜆t)k
k! e−𝜆t + ℙ(Nt ≥k).
Therefore, for k ≥1
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t.
Since 0! = 1, in general we can express the probability mass function of a Poisson process
with intensity 𝜆> 0 as
ℙ(Nt = k) = (𝜆t)k
k! e−𝜆t,
k = 0, 1, 2, . . .
◽
5. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0. If 𝜏1, 𝜏2, . . . is a sequence of inter-arrival times, show that the random variables
are mutually independent and each follows an exponential distribution with parameter 𝜆.
Solution: We prove this result via mathematical induction.
Since
ℙ(𝜏1 > t) = ℙ(Nt = 0) = e−𝜆t
the density of 𝜏1 is
f𝜏1(t) = d
dtℙ(𝜏1 < t) = d
dt(1 −e−𝜆t) = 𝜆e−𝜆t
and hence 𝜏1 ∼Exp(𝜆).
By conditioning on 𝜏1 and since 𝜏1 < 𝜏2,
ℙ(𝜏2 > t|𝜏1 = t1) = ℙ(zero arrival in (t1, t1 + t]|𝜏1 = t1).
Since the event {𝜏1 = t1} is only related to arrivals during the time interval [0, t1] and
the event {zero arrival in (t1, t1 + t]} relates to arrivals during the time interval (t1, t1 + t],
these two events are independent. Thus, we have
ℙ(𝜏2 > t|𝜏1 = t1) = ℙ(zero arrival in (t1, t1 + t]) = ℙ(𝜏2 > t) = e−𝜆t
with 𝜏2 ⟂⟂𝜏1 and 𝜏2 ∼Exp(𝜆).
We assume that for n > 1, 𝜏n ⟂⟂𝜏i and each 𝜏i ∼Exp(𝜆), i = 1, 2, . . . , n −1 such that
ℙ(𝜏n > t|𝜏1 = t1, 𝜏2 = t2, . . . , 𝜏n−1 = tn−1) = ℙ(zero arrival in (Tn−1, Tn−1 + t])
= e−𝜆t
where Tn−1 =
n−1
∑
i=1
ti.

5.2.1
Properties of Poisson Process
259
Conditional on 𝜏1 = t1, 𝜏2 = t2, . . . , 𝜏n = tn, such that 𝜏1 < 𝜏2 < . . . < 𝜏n < 𝜏n+1 we have
ℙ(𝜏n+1 > t|𝜏1 = t1, 𝜏2 = t2, . . . , 𝜏n = tn)
= ℙ(zero arrival in (Tn, Tn + t]|𝜏1 = t1, 𝜏2 = t2, . . . , 𝜏n = tn)
where Tn =
n∑
i=1
ti. Since the events {zero arrival in (Tn, Tn + t]} and {𝜏i = ti}, i = 1,
2, . . . , n are mutually independent, then
ℙ(𝜏n+1 > t|𝜏1 = t1, 𝜏2 = t2, . . . , 𝜏n = tn) = ℙ(zero arrival in (Tn, Tn + t])
= ℙ(𝜏n+1 > t)
= e−𝜆t
which implies 𝜏n+1 ⟂⟂𝜏i, i = 1, 2, . . . , n and 𝜏n+1 ∼Exp(𝜆).
Thus, from mathematical induction, the claim is true for all n ≥1.
◽
6. Stationary and Independent Increments. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶
t ≥0} be a Poisson process with intensity 𝜆> 0. Suppose 0 = t0 < t1 < . . . < tn, using
the property of the Poisson process
ℙ(Nti+h = 𝑣|Nti = 𝑤) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
𝑣= 𝑤
𝜆h + o(h)
𝑣= 𝑤+ 1
o(h)
𝑣> 𝑤+ 1
show that the increments
Nt1 −Nt0, Nt2 −Nt1, . . . , Ntn −Ntn−1
are stationary and independent with the distribution of Nti −Nti−1 the same as the distri-
bution of Nti−ti−1.
Solution: To show that {Nt ∶t ≥0} is stationary we note that for 0 ≤ti−1 < ti, m ≥0
ℙ(Nti −Nti−1 = m) =
∑
u
ℙ(Nti = u + m, Nti−1 = u)
=
∑
u
ℙ(Nti = u + m|Nti−1 = u)ℙ(Nti−1 = u)
=
∑
u
pu,u+m(ti−1, ti)ℙ(Nti−1 = u)
where pu,u+m(ti−1, ti) = ℙ(Nti = u + m|Nti−1 = u).
From the property of the Poisson process
ℙ(Nti+h = 𝑣|Nti = 𝑤) =
⎧
⎪
⎨
⎪⎩
1 −𝜆h + o(h)
𝑣= 𝑤
𝜆h + o(h)
𝑣= 𝑤+ 1
o(h)
𝑣> 𝑤+ 1

260
5.2.1
Properties of Poisson Process
we can write
ℙ(Nti+h = 𝑣|Nti−1 = u) = ℙ(Nti+h = 𝑣|Nti = 𝑣−1)ℙ(Nti = 𝑣−1|Nti−1 = u)
+ ℙ(Nti+h = 𝑣|Nti = 𝑣)ℙ(Nti = 𝑣|Nti−1 = u)
+
∑
𝑤<𝑣−1
ℙ(Nti+h = 𝑣|Nti = 𝑤)ℙ(Nti = 𝑤|Nti−1 = u)
= (𝜆h + o(h))ℙ(Nti = 𝑣−1|Nti−1 = u)
+ (1 −𝜆h + o(h))ℙ(Nti = 𝑣|Nti−1 = u) + o(h)
or
p′
0,0(ti−1, ti) = −𝜆p0,0(ti−1, ti)
p′
u,𝑣(ti−1, ti) = −𝜆pu,𝑣(ti−1, ti) + 𝜆pu,𝑣−1(ti−1, ti),
𝑣= u + 1, u + 2, . . .
where
p′
u,𝑣(ti−1, ti) = lim
h→0
ℙ(Nti+h = 𝑣|Nti−1 = u) −ℙ(Nti = 𝑣|Nti−1 = u)
h
.
Using the same steps as discussed in Problem 5.2.1.2 (page 253), we can deduce that
pu,𝑣(ti−1, ti) = 𝜆𝑣−u(ti −ti−1)𝑣−ue−𝜆(ti−ti−1)
(𝑣−u)!
,
𝑣= u + 1, u + 2, . . .
By substituting the above result into ℙ(Nti −Nti−1 = m),
ℙ(Nti −Nti−1 = m) =
∑
u
pu,u+m(ti−1, ti)ℙ(Nti−1 = u)
= 𝜆m(ti −ti−1)me−𝜆(ti−ti−1)
m!
∑
u
ℙ(Nti−1 = u)
= 𝜆m(ti −ti−1)me−𝜆(ti−ti−1)
m!
which is a probability mass function for the Poisson (𝜆(ti −ti−1)) distribution. Thus, for
all i = 1, 2, . . . , n, Nti −Nti−1 is a stationary process and has the same distribution as
Poisson(𝜆(ti −ti−1)).
Finally, to show {Nt ∶t ≥0} has independent increments we note for mi ≥0, mj ≥0,
i < j, i, j = 1, 2, . . . , n and from the stationary property of the Poisson process,
ℙ(Ntj −Ntj−1 = mj|Nti −Nti−1 = mi) =
ℙ(Ntj −Ntj−1 = mj, Nti −Nti−1 = mi)
ℙ(Nti −Nti−1 = mi)
=
ℙ(Ntj−tj−1 = mj, Nti−ti−1 = mi)
ℙ(Nti−ti−1 = mi)
=
ℙ(arrivals in (tj−1, tj] ∩arrivals in (ti−1, ti])
ℙ(arrivals in (ti−1, ti])
.

5.2.1
Properties of Poisson Process
261
Because i < j and since there are no overlapping events between the time intervals (ti−1, ti]
and (tj−1, tj], thus for i < j, i, j = 1, 2, . . . , n
ℙ(Ntj −Ntj−1 = mj|Nti −Nti−1 = mi) = ℙ(arrivals in (tj−1, tj]) = ℙ(Ntj−tj−1 = mj).
N.B. In general, by denoting ℱtk−1 as the 𝜎-algebra of information and observing the Pois-
son process Nt for 0 ≤t ≤tk−1, we can deduce that Ntk −Ntk−1 is independent of ℱtk−1.
◽
7. Superposition. Let N(1)
t , N(2)
t , . . . , N(n)
t
be n independent Poisson processes with intensities
𝜆(1), 𝜆(2), . . . , 𝜆(n), respectively, where the sequence of processes is defined on the proba-
bility space (Ω, ℱ, ℙ) with respect to the filtration ℱt. Show that N(1)
t
+ N(2)
t
+ . . . + N(n)
t
is a Poisson process with intensity 𝜆(1) + 𝜆(2) + . . . + 𝜆(n).
Solution: For N(i)
t
∼Poisson (𝜆(i)t), i = 1, 2, . . . , n the moment generating function for
a Poisson process is
MN(i)
t (t, u) = 𝔼
(
euN(i)
t
)
= e−𝜆(i)t
∞
∑
x=0
eux(𝜆(i)t)x
x!
= e−𝜆(i)t
∞
∑
x=0
(𝜆(i)teu)x
x!
= e𝜆(i)t(eu−1)
where u ∈ℝ.
By setting Nt = N(1)
t
+ N(2)
t
+ . . . + N(n)
t
such that N(i)
t
⟂⟂N(j)
t , i ≠j, i, j = 1, 2, . . . , n the
moment generating function for Nt is
MNt(t, u) = 𝔼(euNt)
= 𝔼
(
euN(1)
t
+uN(2)
t
+ ... +uN(n)
t
)
= 𝔼
(
euN(1)
t
)
⋅𝔼
(
euN(2)
t
)
· · · 𝔼
(
euN(n)
t
)
= e𝜆(1)t(eu−1) ⋅e𝜆(2)t(eu−1) · · · e𝜆(n)t(eu−1)
= e(𝜆(1)+𝜆(2)+ ... +𝜆(n))t(eu−1).
Therefore,
N(1)
t
+ N(2)
t
+ . . . + N(n)
t
∼Poisson ((𝜆(1) + 𝜆(2) + . . . + 𝜆(n))t) .
◽
8. Markov Property. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on
the probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt. Show that if f is a contin-
uous function then there exists another continuous function g such that
𝔼
[
f(Nt)|||ℱu
]
= g(Nu)
for 0 ≤u ≤t.
Solution: For 0 ≤u ≤t we can write
𝔼
[
f(Nt)|||ℱu
]
= 𝔼
[
f(Nt −Nu + Nu)|||ℱu
]
.

262
5.2.1
Properties of Poisson Process
Since Nt −Nu ⟂⟂ℱu and Nu is ℱu measurable, by setting Nu = x where x is a constant
value,
𝔼
[
f(Nt −Nu + Nu)|||ℱu
]
= 𝔼[ f(Nt −Nu + x)] .
Because Nt −Nu ∼Poisson(𝜆(t −u)), we can write 𝔼[ f(Nt −Nu + x)] as
𝔼[ f(Nt −Nu + x)] =
∞
∑
k=0
f(k + x)e−𝜆(t−u)(𝜆(t −u))k
k!
.
By setting 𝜏= t −u and y = k + x, we can rewrite 𝔼[ f(Nt−Nu+x)]=𝔼[ f(Nt −Nu+Nu)]
as
𝔼[ f(Nt −Nu + Nu)] =
∞
∑
y=Nu
f(y)e−𝜆𝜏(𝜆𝜏)y−Nu
(y −Nu)!
=
∞
∑
y=0
f(y + Nu)e−𝜆𝜏(𝜆𝜏)y
y!
.
Since the only information from the filtration ℱu is Nu, then
𝔼
[
f(Nt)|||ℱu
]
= g(Nu)
where
g(Nu) =
∞
∑
y=0
f(y + Nu)e−𝜆𝜏(𝜆𝜏)y
y!
.
◽
9. Compensated Poisson Process. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0
defined on the probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt. By defining the
compensated Poisson process, ̂Nt as
̂Nt = Nt −𝜆t
show that ̂Nt is a martingale.
Solution: To show that ̂Nt is a martingale, for 0 ≤s < t we note the following.
(a) Since the increment Nt −Ns is independent of ℱs and has expected value 𝜆(t −s), we
can write
𝔼
(
̂Nt||| ℱs
)
= 𝔼
(
̂Nt −̂Ns + ̂Ns||| ℱs
)
= 𝔼
(
̂Nt −̂Ns|||ℱs
)
+ 𝔼
(
̂Ns|||ℱs
)
= 𝔼
(
Nt −Ns −𝜆(t −s)|||ℱs
)
+ ̂Ns
= 𝔼(Nt −Ns
) −𝜆(t −s) + ̂Ns
= ̂Ns.

5.2.1
Properties of Poisson Process
263
(b) 𝔼
(|||̂Nt|||
)
= 𝔼(||Nt −𝜆t||
) ≤𝔼(Nt
) + 𝜆t < ∞since Nt ≥0 and hence 𝔼(||Nt||
) =
2𝜆t < ∞.
(c) The process ̂Nt = Nt −𝜆t is clearly ℱt-adapted.
From (a)–(c) we have shown that ̂Nt = Nt −𝜆t is a martingale.
◽
10. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt. By defining the compensated Poisson process,
̂Nt as
̂Nt = Nt −𝜆t
show that ̂N2
t −𝜆t is a martingale.
Solution: To show that ̂N2
t −𝜆t is a martingale, for 0 ≤s < t we note the following.
(a) Since the increment Nt −Ns ⟂⟂ℱs and has expected value 𝜆(t −s), and because Nt −
Ns ⟂⟂Ns, we can write
𝔼
(
̂N2
t −𝜆t|||ℱs
)
= 𝔼
[
(̂Nt −̂Ns + ̂Ns)2||| ℱs
]
−𝜆t
= 𝔼
[
(̂Nt −̂Ns)2||| ℱs
]
+ 2𝔼
[
̂Ns(̂Nt −̂Ns)||| ℱs
]
+ 𝔼
(
̂N2
s ||| ℱs
)
−𝜆t
= 𝔼
[((Nt −Ns) −𝜆(t −s))2||| ℱs
]
+ 2𝔼(Ns −𝜆s) 𝔼[(Nt −Ns) −𝜆(t −s)] + ̂N2
s −𝜆t
= 𝜆(t −s) + 0 + ̂N2
s −𝜆t
= ̂N2
s −𝜆s.
(b) Since |||̂N2
t −𝜆t||| ≤(Nt −𝜆t)2 + 𝜆t therefore
𝔼
(|||̂N2
t −𝜆t|||
)
≤𝔼
[(Nt −𝜆t)2]
+ 𝜆t = 2𝜆t < ∞
as 𝔼[(Nt −𝜆t)2] = Var (Nt
) = 𝜆t.
(c) The process ̂N2
t −𝜆= (Nt −𝜆t)2 −𝜆t is clearly ℱt-adapted.
From (a)–(c) we have shown that ̂N2
t −𝜆t is a martingale.
◽
11. Exponential Martingale Process. Let Nt be a Poisson process with intensity 𝜆> 0 defined
on the probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt. Show that for u ∈ℝ,
Xt = euNt−𝜆t(eu−1)
is a martingale.
Solution: Given that Nt ∼Poisson(𝜆t) then for u ∈ℝ,
𝔼(euNt) = e−𝜆t
∞
∑
x=0
eux(𝜆t)x
x!
= e−𝜆t
∞
∑
x=0
(𝜆teu)x
x!
= e𝜆t(eu−1)

264
5.2.1
Properties of Poisson Process
and hence for 0 ≤s < t and because Nt −Ns ∼Poisson(𝜆(t −s)),
𝔼(eu(Nt−Ns)) = e𝜆(t−s)(eu−1).
To show that Xt = euNt−𝜆t(eu−1) is a martingale, for 0 ≤s < t we have the following:
(a) Since Nt −Ns ⟂⟂ℱs and has stationary increment, therefore eNt−Ns ⟂⟂ℱs and eNt−Ns ⟂⟂
eNs. Thus, we can write
𝔼(Xt|ℱs
) = 𝔼
(
euNt−𝜆t(eu−1)||| ℱs
)
= e−𝜆t(eu−1)𝔼
(
euNt||| ℱs
)
= e−𝜆t(eu−1)𝔼
(
euNt−uNs+uNs||| ℱs
)
= e−𝜆t(eu−1)𝔼
(
euNs||| ℱs
)
𝔼
(
eu(Nt−Ns)||| ℱs
)
= e−𝜆t(eu−1) ⋅euNs ⋅e𝜆(t−s)(eu−1)
= euNs−𝜆s(eu−1)
= Xs.
(b) Since |Xt| = |||euNt−𝜆t(eu−1)||| = euNt−𝜆t(eu−1), we can write
𝔼(|Xt|) = 𝔼(euNt−𝜆t(eu−1)) = e−𝜆t(eu−1)𝔼(euNt) = e−𝜆t(eu−1) ⋅e𝜆t(eu−1) = 1 < ∞.
(c) The process Xt = euNt−𝜆t(eu−1) is clearly ℱt-adapted.
From (a)–(c) we have shown that Xt = euNt−𝜆t(eu−1) is a martingale.
◽
12. Compound Poisson Process. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆>
0 defined on the probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt, and let
X1, X2, . . . be a sequence of independent and identically distributed random variables with
common mean 𝔼(Xi) = 𝔼(X) = 𝜇and variance Var (Xi) = Var (X) = 𝜎2. Let X1, X2, . . . be
independent of Nt. By defining the compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
show that the moment generating function for Mt is
𝜑Mt(t, u) = e𝜆t(𝜑X(u)−1),
u ∈ℝ
where 𝜑X(u) = 𝔼(euX).
Further, show that 𝔼(Mt) = 𝜇𝜆t and Var (Mt) = (𝜇2 + 𝜎2)𝜆t.
Solution: By definition, for u ∈ℝ
𝜑Mt(t, u) = 𝔼(euMt)
= 𝔼
(
eu ∑Nt
i=1 Xi
)

5.2.1
Properties of Poisson Process
265
= 𝔼
[
𝔼
(
eu ∑Nt
i=1 Xi||||
Nt
)]
=
∞
∑
n=0
𝔼
(
eu ∑n
i=1 Xi||| Nt = n
)
ℙ(Nt = n).
Since X1, X2, . . . , Xn are independent and identically distributed, we can therefore write
𝜑Mt(t, u) =
∞
∑
n=0
𝔼(euX)nℙ(Nt = n)
=
∞
∑
n=0
(𝜑X(u))n e−𝜆t(𝜆t)n
n!
= e−𝜆t
∞
∑
n=0
(𝜆t𝜑X(t))n
n!
= e𝜆t(𝜑X(u)−1).
By taking first and second partial derivatives of 𝜑Mt(t, u) with respect to u, we have
𝜕𝜑Mt(t, u)
𝜕u
= 𝜆t𝜑′
X(u)e𝜆t(𝜑X(u)−1)
and
𝜕2𝜑Mt(t, u)
𝜕u2
= {𝜆t𝜑′′
X(u) + (𝜆t)2(𝜑′
X(u))2} e𝜆t(𝜑X(u)−1).
Since 𝜑X(0) = 1, therefore
𝔼(Mt) =
𝜕𝜑Mt(t, 0)
𝜕u
= 𝜆t𝜑′
X(0) = 𝜆t𝔼(X) = 𝜇𝜆t
and
𝔼(M2
t
) =
𝜕2𝜑Mt(t, 0)
𝜕u2
= 𝜆t𝜑′′
X(0) + (𝜆t)2(𝜑′
X(0))2 = 𝜆t𝔼(X2) + (𝜆t)2𝔼(X)2.
Thus, the variance of Y is
Var (Mt) = 𝔼(M2
t
) −𝔼(Mt
)2 = 𝜆t𝔼(X2) = 𝜆t [Var (X) + 𝔼(X)2] = (𝜇2 + 𝜎2)𝜆t.
◽
13. Martingale Properties of Compound Poisson Process. Let {Nt ∶t ≥0} be a Poisson
process with intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with respect
to the filtration ℱt, and let X1, X2, . . . be a sequence of independent and identically
distributed random variables with common mean 𝔼(Xi) = 𝔼(X) = 𝜇and variance
Var (Xi) = Var (X) = 𝜎2. Let X1, X2, . . . be independent of Nt. By defining the compound
Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0

266
5.2.1
Properties of Poisson Process
and assuming 𝔼(|X|) < ∞, show that for 0 ≤s < t
𝔼(Mt|ℱs)
⎧
⎪
⎨
⎪⎩
= Ms
if 𝜇= 0
≥Ms
if 𝜇> 0
≤Ms
if 𝜇< 0.
Solution: To show that Mt can be a martingale, submartingale or supermartingale, for
0 ≤s < t we have the following.
(a) 𝔼(Mt|ℱs) = 𝔼(Mt −Ms + Ms|ℱs) = 𝔼(Mt −Ms|ℱs) + 𝔼(Ms|ℱs) = 𝜇𝜆(t −s) + Ms
since the increment Mt −Ms is independent of ℱs and has mean 𝜇𝜆(t −s).
(b) 𝔼|Mt|) = 𝔼
(|||
∑Nt
i=1 Xi|||
)
≤𝔼
(∑Nt
i=1 ||Xi||
)
= 𝜆t𝔼(|X|) < ∞since 𝔼(|X|) < ∞.
(c) The process Mt = ∑Nt
i=1 Xi is clearly ℱt-adapted.
We can therefore deduce that Mt is a martingale, submartingale or supermartingale by
setting 𝜇= 0, 𝜇> 0 or 𝜇< 0, respectively.
◽
14. Compensated Compound Poisson Process. Let {Nt ∶t ≥0} be a Poisson process with
intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with respect to the filtration
ℱt. Let X1, X2, . . . be a sequence of independent and identically distributed random vari-
ables with common mean 𝔼(Xi) = 𝔼(X) = 𝜇and variance Var (Xi) = Var (X) = 𝜎2. Let
X1, X2, . . . be independent of Nt. By defining the compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
and assuming 𝔼(|X|) < ∞, show that the compensated compound Poisson process
̂Mt = Mt −𝜇𝜆t
is a martingale.
Solution: To show that ̂Mt is a martingale, for 0 ≤s < t we have the following.
(a) Given the increment Mt −Ms is independent of ℱs and has mean 𝜇𝜆(t −s),
𝔼
(
̂Mt||| ℱs
)
= 𝔼
(
̂Mt −̂Ms + ̂Ms||| ℱs
)
= 𝔼
(
Mt −Ms −𝜇𝜆(t −s) + Ms −𝜇𝜆s|||ℱs
)
= 𝔼
(
Mt −Ms|||ℱs
)
−𝜇𝜆(t −s) + Ms −𝜇𝜆s
= 𝜇𝜆(t −s) −𝜇𝜆(t −s) + Ms −𝜇𝜆s
= Ms −𝜇𝜆s.
(b) 𝔼
(||| ̂Mt|||
)
= 𝔼
(|||
∑Nt
i=1 Xi −𝜇𝜆t|||
)
≤𝔼
(∑Nt
i=1 ||Xi||
)
+ 𝜇𝜆t = 𝜆t𝔼(|X|) + 𝜇𝜆t < ∞
since 𝔼(|X|) < ∞.

5.2.1
Properties of Poisson Process
267
(c) The process ̂Mt = ∑Nt
i=1 Xi −𝜇𝜆t is clearly ℱt-adapted.
From the results of (a)–(c) we have shown that ̂Mt = ∑Nt
i=1 Xi −𝜇𝜆t is a martingale.
◽
15. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt. Let X1, X2, . . . be a sequence of independent
and identically distributed random variables with common mean 𝔼(Xi
) = 𝔼(X) = 𝜇and
variance Var (Xi
) = Var (X) = 𝜎2. Let X1, X2, . . . be independent of Nt. By defining the
compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
show that for 0 = t0 < t1 < t2 < . . . < tn the increments
Mt1 −Mt0, Mt2 −Mt1, . . . , Mtn −Mtn−1
are independent, stationary and the distribution of Mti −Mti−1 is the same as the distribu-
tion of Mti−ti−1.
Solution: We first show that the increment Mtk −Mtk−1, k = 1, 2, . . . , n is stationary and
has the same distribution as Mtk−tk−1.
Using the technique of moment generating function, for u ∈ℝ
𝜑Mtk−Mtk−1 (u) = 𝔼
(
eu(Mtk−Mtk−1))
= 𝔼
(
e
u
(∑Ntk
i=1 Xi−∑Ntk−1
i=1
Xi
))
= 𝔼
(
e
u ∑Ntk
Ntk−1 +1 Xi
)
= 𝜑Mtk−tk−1 (u).
Therefore, Mtk −Mtk−1, k = 1, 2, . . . , n is stationary and has the same distribution as
Mtk−tk−1.
Finally, to show that Mti −Mti−1 is independent of Mtj −Mtj−1, i, j = 1, 2, . . . , n and i ≠j,
using the joint moment generating function, for u, 𝑣∈ℝ
𝜑(Mti−Mti−1)(Mtj−Mtj−1)(u, 𝑣) = 𝜑(Mti−ti−1)(Mtj−tj−1)(u, 𝑣)
= 𝔼
(
e
u(Mti−ti−1)+𝑣(Mtj−tj−1))
= 𝔼
⎛
⎜
⎜⎝
e
u
(∑Nti
k=Nti−1 +1 Xk
)
+𝑣
(∑Ntj
k=Ntj−1 +1 Xk
)⎞
⎟
⎟⎠
= 𝔼
(
e
u
(∑Nti
k=Nti−1 +1 Xk
))
𝔼
⎛
⎜
⎜⎝
e
𝑣
(∑Ntj
k=Ntj−1 +1 Xk
)⎞
⎟
⎟⎠

268
5.2.1
Properties of Poisson Process
= 𝜑Mti−ti−1(u)𝜑Mtj−tj−1(𝑣)
since X1, X2, . . . is a sequence of independent and identically distributed random variables
and also the intervals [Nti−1 + 1, Nti] and [Ntj−1 + 1, Ntj] have no overlapping events.
Thus, Mti −Mti−1 ⟂⟂Mtj −Mtj−1, for all i, j = 1, 2, . . . , n, i ≠j.
N.B. Since the compound Poisson process Mt has increments which are independent and
stationary, therefore from Problem 5.2.1.8 (page 261) we can deduce that Mt is Markov.
That is, if f is a continuous function then there exists another continuous function g
such that
𝔼[ f(Mt)|ℱu
] = g(Mu)
for 0 ≤u ≤t.
◽
16. Decomposition of a Compound Poisson Process (Finite Jump Size). Let Nt be a Poisson
process with intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ) with respect to
the filtration ℱt. Let X1, X2, . . . be a sequence of independent and identically distributed
random variables, and let X1, X2, . . . be independent of Nt such that
ℙ(X = xk) = p(xk)
and
K
∑
k=1
ℙ(X = xk) = 1
where X
d= Xi, i = 1, 2, . . . and x1, x2, . . . , xK is a finite set of non-zero numbers. By defin-
ing the compound Poisson process as
Mt =
Nt
∑
i=1
Xi
show that Mt and Nt can be written as
Mt =
K
∑
k=1
xkN(k)
t
and
Nt =
K
∑
k=1
N(k)
t
where N(k)
t
∼Poisson (𝜆tp(xk)), k = 1, 2, . . . , K is a sequence of independent and identi-
cal Poisson processes.
Solution: From Problem 5.2.1.12 (page 264), the moment generating function for Mt is
𝜑Mt(t, u) = e𝜆t(𝜑X(u)−1),
u ∈ℝ
where 𝜑X(u) = 𝔼(euX). Given ℙ(X = xk) = p(xk), k = 1, 2, . . . , K we have
𝜑X(u) = 𝔼
(
euX)
=
K
∑
k=1
euxkℙ(X = xk)
=
K
∑
k=1
euxkp(xk).

5.2.1
Properties of Poisson Process
269
Substituting 𝜑X(u) into the moment generating function for Mt,
𝜑Mt(t, u) = e
𝜆t
(∑K
k=1 euxk p(xk)−1
)
.
Because ∑K
k=1 p(xk) = 1,
𝜑Mt(t, u) = e
𝜆t
(∑K
k=1 euxk p(xk)−∑K
k=1 p(xk)
)
= e𝜆t ∑K
k=1(euxk −1)p(xk)
=
K
∏
k=1
e𝜆tp(xk)(euxk −1)
=
K
∏
k=1
𝔼
(
euxkN(k)
t
)
which is a product of K independent moment generating functions of the random variable
xkN(k)
t
where N(k)
t
∼Poisson (𝜆tp(xk)). Thus,
𝜑Mt(t, u) =
K
∏
k=1
𝔼
(
euxkN(k)
t
)
= 𝔼
(
eu ∑K
k=1 xkN(k)
t
)
which implies Mt =
K∑
k=1
xkN(k)
t .
Given that N(k)
t
∼Poisson (𝜆tp(xk)), k = 1, 2, . . . , K is a sequence of independent and
identically distributed Poisson processes,
K∑
k=1
N(k)
t
∼Poisson
(
K∑
k=1
𝜆tp(xk)
)
or
K∑
k=1
N(k)
t
∼
Poisson(𝜆t) and hence Nt =
K∑
k=1
N(k)
t .
◽
17. Let {X1, X2, . . . ,} be a series of independent and identically distributed random variables
with moment generating function
MX(𝜉) = 𝔼(e𝜉X) ,
𝜉∈ℝ
where X
d= Xi, i = 1, 2, . . . and let {t1, t2, . . .} be its corresponding sequence of indepen-
dent and identically distributed random jump times of a Poisson process {Nt ∶t ≥0} with
intensity parameter 𝜆> 0 where ti ∼Exp(𝜆), i = 1, 2, . . . Let X1, X2, . . . be independent
of Nt.
Show that for n ≥1, 𝜅> 0 the process {Yt ∶t ≥0} with initial condition Y0 = 0 given as
Yt =
n
∑
i=1
e−𝜅(t−ti)Xi
has moment generating function MY(𝜉, t) = 𝔼(e𝜉Yt) given by
MY(𝜉, t) =
n
∏
i=1
{
∫
t
0
MXi (𝜉e−𝜅s) 𝜆e−𝜆(t−s) ds
}

270
5.2.1
Properties of Poisson Process
and
𝔼(Yt) = n𝜇
(
𝜆
𝜆−𝜅
) (e−𝜅t −e−𝜆t) (1 −e−𝜆t)n−1
Var (Yt) = n(𝜇2 + 𝜎2)
(
𝜆
𝜆−2𝜅
) (e−2𝜅t −e−𝜆t) (1 −e−𝜆t)n−1
+ n(n −1)𝜇2(
𝜆
𝜆−2𝜅
)2(e−2𝜅t −e−𝜆t)2(1 −e−𝜆t)n−2
−n2𝜇2(
𝜆
𝜆−𝜅
)2(e−𝜅t −e−𝜆t)2(1 −e−𝜆t)2(n−1)
where 𝔼(Xi) = 𝜇and Var (Xi) = 𝜎2 for i = 1, 2, . . .
Solution: By definition,
MY(𝜉, t) = 𝔼(e𝜉Yt) = 𝔼
(
e𝜉∑n
i=1 e−𝜅(t−ti)Xi
)
=
n
∏
i=1
𝔼
(
e𝜉e−𝜅(t−ti)Xi
)
=
n
∏
i=1
MXi(𝜉e−𝜅(t−ti)).
Using the tower property,
MXi(𝜉e−𝜅(t−ti)) = 𝔼
(
e𝜉e−𝜅(t−ti)Xi
)
= 𝔼
[
𝔼
(
e𝜉e−𝜅(t−ti)Xi||| ti = s
)]
= ∫
t
0
𝔼
(
e𝜉e−𝜅(t−s)Xi
)
𝜆e−𝜆s ds
= ∫
t
0
MXi(𝜉e−𝜅(t−s))𝜆e−𝜆s ds
= ∫
t
0
MXi(𝜉e−𝜅s)𝜆e−𝜆(t−s) ds.
Therefore,
MY(𝜉, t) =
n
∏
i=1
{
∫
t
0
MXi (𝜉e−𝜅s) 𝜆e−𝜆(t−s) ds
}
.
By setting 𝜉= 𝜉e−𝜅s and MXi(𝜉) = ∫
t
0
MXi(𝜉e−𝜅s)𝜆e−𝜆(t−s) ds, and differentiating MY(𝜉, t)
with respect to 𝜉, we have
𝜕MY(𝜉, t)
𝜕𝜉
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
𝜕MXi(𝜉)
𝜕𝜉
n
∏
j=1
j≠i
MXj(𝜉)
⎫
⎪
⎬
⎪⎭
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
[
∫
t
0
𝜕MXi(𝜉)
𝜕𝜉
⋅𝜕𝜉
𝜕𝜉⋅𝜆e−𝜆(t−s) ds
]
n
∏
j=1
j≠i
MXj(𝜉)
⎫
⎪
⎬
⎪⎭

5.2.1
Properties of Poisson Process
271
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
[
∫
t
0
𝜕MXi(𝜉)
𝜕𝜉
𝜆e−𝜆te(𝜆−𝜅)s ds
]
n
∏
j=1
j≠i
[
∫
t
0
MXi(𝜉)𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭
and
𝜕2MY(𝜉, t)
𝜕𝜉2
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
𝜕2MXi(𝜉)
𝜕𝜉2
n
∏
j=1
j≠i
MXj(𝜉)
⎫
⎪
⎬
⎪⎭
+
n
∑
i=1
n
∑
j=1
i≠j
⎧
⎪
⎨
⎪⎩
𝜕MXi(𝜉)
𝜕𝜉
𝜕MXj(𝜉)
𝜕𝜉
n
∏
k=1
k≠i,j
MXk(𝜉)
⎫
⎪
⎬
⎪⎭
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
[
∫
t
0
𝜕2MXi(𝜉)
𝜕𝜉
2
𝜆e−𝜆te(𝜆−2𝜅)s ds
]
n
∏
j=1
j≠i
[
∫
t
0
MXj(𝜉)𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭
+
n
∑
i=1
n
∑
j=1
i≠j
⎧
⎪
⎨
⎪⎩
[
𝜕MXi(𝜉)
𝜕𝜉
𝜆e−𝜆te(𝜆−𝜅)s ds
] ⎡
⎢
⎢⎣
𝜕MXj(𝜉)
𝜕𝜉
𝜆e−𝜆te(𝜆−𝜅)s ds
⎤
⎥
⎥⎦
×
n
∏
k=1
k≠i,j
[
∫
t
0
MXi(𝜉)𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭
.
By substituting 𝜉= 0 and taking note that MX(0) = 1, 𝜕MX(0)
𝜕𝜉
= 𝔼(X) = 𝜇and
𝜕2MX(0)
𝜕𝜉
2
= 𝔼(X2) = 𝜇2 + 𝜎2, we therefore have
𝔼(Yt) = 𝜕MY(0, t)
𝜕𝜉
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
[
∫
t
0
𝜇𝜆e−𝜆te(𝜆−𝜅)s ds
]
n
∏
j=1
j≠i
[
∫
t
0
𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭
= n𝜇
(
𝜆
𝜆−𝜅
) (
e−𝜅t −e−𝜆t) (
1 −e−𝜆t)n−1
and
𝔼(Y2
t ) = 𝜕2MY(0, t)
𝜕𝜉2
=
n
∑
i=1
⎧
⎪
⎨
⎪⎩
[
∫
t
0
(𝜇2 + 𝜎2) 𝜆e−𝜆te(𝜆−2𝜅)s ds
]
n
∏
j=1
j≠i
[
∫
t
0
𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭

272
5.2.1
Properties of Poisson Process
+
n
∑
i=1
n
∑
j=1
i≠j
⎧
⎪
⎨
⎪⎩
[𝜇𝜆e−𝜆te(𝜆−𝜅)s ds] [𝜇𝜆e−𝜆te(𝜆−𝜅)s ds]
n
∏
k=1
k≠i,j
[
∫
t
0
𝜆e−𝜆(t−s) ds
]⎫
⎪
⎬
⎪⎭
= n(𝜇2 + 𝜎2)
(
𝜆
𝜆−2𝜅
) (e−2𝜅t −e−𝜆t) (1 −e−𝜆t)n−1
+ n(n −1)𝜇2(
𝜆
𝜆−2𝜅
)2(e−2𝜅t −e−𝜆t)2(1 −e−𝜆t)n−2.
Therefore,
Var (Yt
) = 𝔼(Y2
t
) −𝔼(Yt
)2
= n(𝜇2 + 𝜎2)
(
𝜆
𝜆−2𝜅
) (e−2𝜅t −e−𝜆t) (1 −e−𝜆t)n−1
+ n(n −1)𝜇2(
𝜆
𝜆−2𝜅
)2(e−2𝜅t −e−𝜆t)2(1 −e−𝜆t)n−2
−n2𝜇2(
𝜆
𝜆−𝜅
)2(e−𝜅t −e−𝜆t)2(1 −e−𝜆t)2(n−1).
◽
18. Let {X1, X2, . . . , } be a series of independent and identically distributed random variables
with moment generating function
MX(𝜉) = 𝔼(e𝜉X) ,
𝜉∈ℝ
where X
d= Xi, i = 1, 2, . . . and let {t1, t2, . . .} be its corresponding sequence of random
jump times of a Poisson process {Nt ∶t ≥0} with intensity parameter 𝜆> 0 where ti ∼
Exp(𝜆), i = 1, 2, . . . In addition, let the sequence X1, X2, . . . be independent of Nt. Show
that for 𝜅> 0 the process {Yt ∶t ≥0} with initial condition Y0 = 0 given as
Yt =
Nt
∑
i=1
e−𝜅(t−ti)Xi
has moment generating function MY(𝜉, t) = 𝔼(e𝜉Yt) given by
MY(𝜉, t) = exp
{
𝜆∫
t
0
(MX(𝜉e−𝜅s) −1) ds
}
and
𝔼(Yt
) = 𝜆𝜇
𝜅
(1 −e−𝜅t)
Var (Yt
) = 𝜆
2𝜅
(𝜇2 + 𝜎2) (1 −e−2𝜅t)
where 𝔼(Xi
) = 𝜇and Var (Xi
) = 𝜎2 for i = 1, 2, . . .

5.2.1
Properties of Poisson Process
273
Solution: Given the mutual independence of the random variables Xi and Xj, i ≠j the
conditional expectation of the process Yt given the first jump time, t1 = s is
𝔼
(
e𝜉Yt||| t1 = s
)
= 𝔼
(
exp
{
𝜉
Nt
∑
i=1
e−𝜅(t−ti)Xi
}||||||
t1 = s
)
= 𝔼
(
exp {𝜉e−𝜅(t−s)X1
}||| t1 = s
)
𝔼
(
exp
{
𝜉
Nt
∑
i=2
e−𝜅(t−ti)Xi
}||||||
t1 = s
)
.
From the properties of the Poisson process, the sum conditioned on t1 = s has the same
unconditional distribution as the original sum, starting with the first jump i = 1 until time
t −s, and we therefore have
𝔼
(
e𝜉Yt||| t1 = s
)
= 𝔼(exp {𝜉e−𝜅(t−s)Xt
}) MY(𝜉, t −s)
= MX(𝜉e−𝜅(t−s))MY(𝜉, t −s).
Since the first jump time is exponentially distributed, t1 ∼Exp(𝜆) and from the tower
property
MY(𝜉, t) = 𝔼
[
𝔼
(
e𝜉Yt||| t1 = s
)]
= ∫
t
0
𝔼(e𝜉Yt|t1 = s) 𝜆e−𝜆s ds
= ∫
t
0
MX(𝜉e−𝜅(t−s))MY(𝜉, t −s)𝜆e−𝜆s ds
= ∫
t
0
MX(𝜉e−𝜅s)MY(𝜉, s)𝜆e−𝜆(t−s) ds.
Differentiating the integral with respect to t, we have
𝜕MY(𝜉, t)
𝜕t
= MX(𝜉e−𝜅t)MY(𝜉, t)𝜆−𝜆∫
t
0
MX(𝜉e−𝜅s)MY(𝜉, s)𝜆e−𝜆(t−s) ds
= 𝜆(MX(𝜉e−𝜅t) −1)MY(𝜉, t)
and solving the first-order differential equation,
MY(𝜉, t) = exp
(
𝜆∫
t
0
{MX (𝜉e−𝜅s) −1} ds
)
.
Setting 𝜉= 𝜉e−𝜅s and differentiating MY(𝜉, t) with respect to 𝜉, we have
𝜕MY(𝜉, t)
𝜕𝜉
= 𝜕
𝜕𝜉
(
𝜆∫
t
0
{
MX
(
𝜉
)
−1
}
ds
)
MY(𝜉, t)
= 𝜆
{
∫
t
0
e−𝜅s 𝜕MX(𝜉)
𝜕𝜉
ds
}
MY(𝜉, t)

274
5.2.1
Properties of Poisson Process
and differentiating the above equation with respect to 𝜉,
𝜕2MY(𝜉, t)
𝜕𝜉2
= 𝜆𝜕
𝜕𝜉
{
∫
t
0
e−𝜅s 𝜕MX(𝜉)
𝜕𝜉
ds
}
MY(𝜉, t) + 𝜆
{
∫
t
0
e−𝜅s 𝜕MX(𝜉)
𝜕𝜉
ds
}
𝜕MY(𝜉, t)
𝜕𝜉
= 𝜆
{
∫
t
0
e−2𝜅s 𝜕2MX(𝜉)
𝜕𝜉
2
ds
}
MY(𝜉, t) + 𝜆
{
∫
t
0
e−𝜅s 𝜕MX(𝜉)
𝜕𝜉
ds
}
𝜕MY(𝜉, t)
𝜕𝜉
.
By substituting 𝜉= 0, and since MY(0, t) = 1, we therefore have
𝔼(Yt
) = 𝜕MY(0, t)
𝜕𝜉
= 𝜆𝜇
𝜅
(1 −e−𝜅t)
𝔼(Y2
t
) = 𝜕2MY(0, t)
𝜕𝜉2
= 𝜆
2𝜅
(𝜇2 + 𝜎2) (1 −e−2𝜅t) + 𝔼(Yt)2
and hence
Var (Yt
) = 𝜆
2𝜅(𝜇2 + 𝜎2)(1 −e−2𝜅t)
where 𝔼(Xi
) = 𝜇and Var (Xi
) = 𝜎2 for i = 1, 2, . . .
◽
19. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt, and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
show that its quadratic variation is
⟨M, M⟩t = lim
n→∞
n−1
∑
i=0
(
Mti+1 −Mti
)2
=
Nt
∑
i=1
X2
i
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand deduce that dMt ⋅dMt =
X2
t dNt where Xt is the jump size if N jumps at t.
Solution: From the definition of quadratic variation
⟨M, M⟩t = lim
n→∞
n−1
∑
k=0
(
Mtk+1 −Mtk
)2
where tk = tk∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕ, we can set
⟨M, M⟩t = lim
n→∞
n−1
∑
k=0
⎛
⎜
⎜⎝
Ntk+1
∑
i=1
Xi −
Ntk
∑
i=1
Xi
⎞
⎟
⎟⎠
2
= lim
n→∞
Ntn
∑
i=Nt0
X2
i =
Nt
∑
i=1
X2
i
since Nt0 = N0 = 0.

5.2.1
Properties of Poisson Process
275
From the definition
lim
n→∞
n−1
∑
i=0
(
Mti+1 −Mti
)2
= ∫
t
0
(dMs
)2 =
Nt
∑
i=1
X2
i
and since
Nt
∑
i=1
X2
i = ∫
t
0
X2
u−dNu
where Xt−is the jump size before a jump event at time t, then by differentiating both sides
with respect to t we finally have dMt ⋅dMt = X2
t dNt where Xt is the jump size if N jumps
at t.
◽
20. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
show that the cross-variation between Mt and Nt is
⟨M, N⟩t = lim
n→∞
n−1
∑
i=0
(
Mti+1 −Mti
) (
Nti+1 −Nti
)
=
Nt
∑
i=1
Xi
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand deduce that dMt ⋅dNt =
XtdNt.
Solution: From the definition of cross-variation, for tk = kt∕n, 0 = t0 < t1 < t2 < . . . <
tn = t, n ∈ℕand since we can also write Nt = ∑Nt
i=1 1,
⟨M, N⟩t = lim
n→∞
n−1
∑
k=0
(
Mtk+1 −Mtk
) (
Ntk+1 −Ntk
)
= lim
n→∞
n−1
∑
k=0
⎛
⎜
⎜⎝
Ntk+1
∑
i=1
Xi −
Ntk
∑
i=1
Xi
⎞
⎟
⎟⎠
⎛
⎜
⎜⎝
Ntk+1
∑
i=1
1 −
Ntk
∑
i=1
1
⎞
⎟
⎟⎠
= lim
n→∞
Ntn
∑
i=Nt0
Xi ⋅1
=
Nt
∑
i=1
Xi
since Nt0 = N0 = 0.
Given
lim
n→∞
n−1
∑
i=0
(
Mti+1 −Mti
) (
Nti+1 −Nti
)
= ∫
t
0
dMs ⋅dNs =
Nt
∑
i=1
Xi

276
5.2.1
Properties of Poisson Process
and because we can define
Nt
∑
i=1
Xi = ∫
t
0
Xs−dNs
where Xt−is the jump size before a jump event at time t, then by differentiating the integrals
with respect to t we have dMt ⋅dNt = XtdNt where Xt is the jump size if N jumps at t.
◽
21. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt. Show that its quadratic variation is
⟨N, N⟩t = lim
n→∞
n−1
∑
i=0
(
Nti+1 −Nti
)2
= Nt
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand deduce that dNt ⋅dNt = dNt.
Solution: Since Nt is a counting process, we can define the compound Poisson process
Mt as
Mt =
Nt
∑
i=1
Xi
where Xi is a sequence of independent and identically distributed random variables which
are also independent of Nt. Using the results in Problem 5.2.1.19 (page 274),
⟨M, M⟩t =
Nt
∑
i=1
X2
i .
By setting Xi = 1 for all i = 1, 2, . . . , Nt, then Mt = Nt which implies
⟨N, N⟩t =
Nt
∑
i=1
1 = Nt.
By definition,
⟨N, N⟩t = lim
n→∞
n−1
∑
i=0
(
Nti+1 −Nti
)2
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand since
lim
n→∞
n−1
∑
i=0
(
Nti+1 −Nti
)2
= ∫
t
0
(dNs
)2 = Nt
then, by differentiating both sides with respect to t, we have dNt ⋅dNt = dNt.
◽

5.2.1
Properties of Poisson Process
277
22. Let {Wt ∶t ≥0} be a standard Wiener process and {Nt ∶t ≥0} be a Poisson process with
intensity 𝜆> 0 defined on the probability space (Ω, ℱ, ℙ). Show that the cross-variation
between Wt and Nt is
⟨W, N⟩t = lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Nti+1 −Nti
)
= 0
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand deduce that dWt ⋅dNt = 0.
Solution: Since
||||||
n−1
∑
i=0
(Wti+1 −Wti)(Nti+1 −Nti)
||||||
≤
max
0≤k≤n−1
|||Wtk+1 −Wtk
|||
||||||
n−1
∑
i=0
(Nti+1 −Nti)
||||||
and because Wt is continuous, we have
lim
n→∞max
0≤k≤n−1
|||Wtk+1 −Wtk
||| = 0.
Therefore, we conclude that
||||||
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Nti+1 −Nti
)||||||
≤0
and hence
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Nti+1 −Nti
)
= 0.
Finally, because
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Nti+1 −Nti
)
= ∫
t
0
dWs ⋅dNs = 0
then, by differentiating both sides with respect to t, we can deduce that dWt ⋅dNt = 0.
◽
23. Let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt and let X1, X2, . . . be a sequence of independent
and identically distributed random variables which are also independent of Nt. By defining
the compound Poisson process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0

278
5.2.1
Properties of Poisson Process
show that the cross-variation between Wt and Mt is
⟨W, M⟩t = lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Mti+1 −Mti
)
= 0
where ti = it∕n, 0 = t0 < t1 < t2 < . . . < tn = t, n ∈ℕand deduce that dWt ⋅dMt = 0.
Solution: Since we can write
||||||
n−1
∑
i=0
(
Wti+1 −Wti
) (
Mti+1 −Mti
)||||||
≤
max
0≤k≤n−1
|||Wtk+1 −Wtk
|||
||||||
n−1
∑
i=0
(Mti+1 −Mti)
||||||
and because Wt is continuous, we have
lim
n→∞max
0≤k≤n−1
|||Wtk+1 −Wtk
||| = 0.
Thus, we conclude that
||||||
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Mti+1 −Mti
)||||||
≤0
or
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Mti+1 −Mti
)
= 0.
Finally, because
lim
n→∞
n−1
∑
i=0
(
Wti+1 −Wti
) (
Mti+1 −Mti
)
= ∫
t
0
dWs ⋅dMs = 0
then, by differentiating both sides with respect to t, we can deduce that dWt ⋅dMt = 0.
◽
24. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0 and {Wt ∶t ≥0} be a standard Wiener process relative to the same filtration
ℱt, t ≥0. By considering the process
Zt = eu1Nt+u2Wt
where u1, u2 ∈ℝ, use It¯o’s formula to find an SDE for Zt.
By setting mt = 𝔼(Zt
) show that solution to the SDE can be expressed as
dmt
dt −
[(eu1 −1)𝜆+ 1
2u2
2
]
mt = 0.
Solve the differential equation to find 𝔼(Zt
) and deduce that Nt ⟂⟂Wt.

5.2.1
Properties of Poisson Process
279
Solution: By expanding Zt using Taylor’s theorem and taking note that (dWt)2 = dt,
(dNt)2 = (dNt)3 = · · · = dNt, dWtdNt = 0,
dZt = 𝜕Zt
𝜕Nt
dNt + 𝜕Zt
dWt
dWt + 1
2
𝜕2Zt
𝜕N2
t
(dNt)2 + 1
2
𝜕2Zt
𝜕W2
t
(dWt)2
+
𝜕2Zt
𝜕Nt𝜕Wt
(dNtdWt) + 1
3!
𝜕3Zt
𝜕N3
t
(dNt)3 + . . .
=
(
u1 + 1
2u2
1 + 1
3!u3
1 + . . .
)
ZtdNt + u2ZtdWt + 1
2u2
2Zt dt
= (eu1 −1)ZtdNt + u2ZtdWt + 1
2u2
2Zt dt.
Taking integrals, we have
∫
t
0
dZs = (eu1 −1) ∫
t
0
Zs dNs + u2 ∫
t
0
Zs dWs + 1
2u2
2 ∫
t
0
Zs ds
Zt −1 = (eu1 −1) ∫
t
0
Zs d̂Ns + u2 ∫
t
0
Zs dWs +
[
𝜆(eu1 −1) + 1
2u2
2
]
∫
t
0
Zs ds
where Z0 = 1 and ̂Nt = Nt −𝜆t. Taking expectations,
𝔼(Zt) = 1 +
[
𝜆(eu1 −1) + 1
2u2
2
]
∫
t
0
𝔼(Zs) ds
where since both Wt and ̂Nt are martingales we therefore have
𝔼
(
∫
t
0
Zs dWs
)
= 0
and
𝔼
(
∫
t
0
Zs d̂Ns
)
= 0.
By differentiating the integral equation,
d
dt𝔼(Zt) =
[
𝜆(eu1 −1) + 1
2u2
2
]
𝔼(Zt)
or
dmt
dt −
[
𝜆(eu1 −1) + 1
2u2
2
]
mt = 0
where mt = 𝔼(Zt).
By setting the integrating factor to be I = e−∫(𝜆(eu1−1)+ 1
2 u2
2) dt = e−(𝜆(eu1−1)+ 1
2 u2
2)t and mul-
tiplying the differential equation with I, we have
d
dt
(
mte−𝜆t(eu1−1)−1
2 u2
2t)
= 0
or
e−𝜆t(eu1−1)−1
2 u2
2t𝔼(eu1Nt+u2Wt) = C
where C is a constant. Since 𝔼(eu1N0+u2W0) = 1 therefore C = 1, and hence we finally
obtain
𝔼(eu1Nt+u2Wt) = e𝜆t(eu1−1)+ 1
2 u2
2t.

280
5.2.1
Properties of Poisson Process
Since the joint moment generating function of
𝔼(eu1Nt+u2Wt) = e𝜆t(eu1−1) ⋅e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Nt and Wt, respec-
tively, we can deduce that Nt and Wt are independent.
◽
25. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0 and {Wt ∶t ≥0} be a standard Wiener process relative to the same filtration
ℱt, t ≥0. Let X1, X2, . . . be a sequence of independent and identically distributed ran-
dom variables which are also independent of Nt and Wt. By defining a compound Poisson
process Mt as
Mt =
Nt
∑
i=1
Xi,
t ≥0
and by considering the process
Zt = eu1Mt+u2Wt
where u1, u2 ∈ℝ, use It¯o’s formula to find an SDE for Zt.
By setting mt = 𝔼(Zt
) show that solution to the SDE can be expressed as
dmt
dt −
[(e𝜑X(u1) −1) 𝜆+ 1
2u2
2
]
mt = 0
where 𝜑X(u1) = 𝔼(eu1Xt) is the moment generating function of Xt, which is the jump size
if N jumps at time t.
Finally, solve the differential equation to find 𝔼(Zt
) and deduce that Mt ⟂⟂Wt.
Solution: By expanding Zt using Taylor’s theorem and taking note that (dWt)2 = dt,
dMtdWt = 0, (dNt)k = dNt and (dMt)k = Xk
t dNt for k = 1, 2, . . . ,
dZt = 𝜕Zt
𝜕Mt
dMt + 𝜕Zt
dWt
dWt + 1
2
𝜕2Zt
𝜕M2
t
(dMt)2 + 1
2
𝜕2Zt
𝜕W2
t
(dWt)2
+
𝜕2Zt
𝜕Mt𝜕Wt
(dMtdWt) + 1
3!
𝜕3Zt
𝜕M3
t
(dMt)3 + . . .
=
(
u1Xt + 1
2u2
1X2
t + 1
3!u3
1X3
t + . . .
)
ZtdNt + u2ZtdWt + 1
2u2
2Zt dt
= (eu1Xt −1) ZtdNt + u2ZtdWt + 1
2u2
2Zt dt.
Integrating, we have
∫
t
0
dZs = ∫
t
0
(eu1Xt −1) Zs dNs + u2 ∫
t
0
Zs dWs + 1
2u2
2 ∫
t
0
Zs ds
Zt −1 = ∫
t
0
(eu1Xt −1) Zs d̂Ns + u2 ∫
t
0
Zs dWs + ∫
t
0
[
𝜆(eu1Xt −1) + 1
2u2
2
]
Zs ds

5.2.2
Jump Diffusion Process
281
where Z0 = 1 and ̂Nt = Nt −𝜆t. Taking expectations and because the jump size variable
Xt is independent of Nt and Wt, we have
𝔼(Zt) = 1 +
[
𝜆(𝔼(eu1Xt) −1) + 1
2u2
2
]
∫
t
0
𝔼(Zs
) ds
since 𝔼
(
∫
t
0
Zs dWs
)
= 0
and
𝔼
(
∫
t
0
Zs d̂Ns
)
= 0.
By differentiating the integral equation,
d
dt𝔼(Zt
) =
[
𝜆(𝔼(eu1Xt) −1) + 1
2u2
2
]
𝔼(Zt
)
or
dmt
dt −
[
𝜆(𝜑X(u1) −1) + 1
2u2
2
]
mt = 0
where mt = 𝔼(Zt
) and 𝜑X(u1) = 𝔼(eu1Xt).
By setting the integrating factor to be I = e−∫(𝜆(𝜑X(u1)−1)+ 1
2 u2
2) dt = e−(𝜆(𝜑X(u1)−1)+ 1
2 u2
2)t and
multiplying the differential equation with I, we have
d
dt
(
mte−𝜆t(𝜑X(u1)−1)−1
2 u2
2t)
= 0
or
e−𝜆t(𝜑X(u1)−1)−1
2 u2
2t𝔼(eu1Mt+u2Wt) = C
where C is a constant. Since 𝔼(eu1M0+u2W0) = 1 therefore C = 1, and hence we finally
obtain
𝔼(eu1Mt+u2Wt) = e𝜆t(𝜑X(u1)−1)+ 1
2 u2
2t.
Since the joint moment generating function of
𝔼(eu1Mt+u2Wt) = e𝜆t(𝜑X(u1)−1) ⋅e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Mt and Wt, respec-
tively, we can deduce that Mt and Wt, are independent.
◽
5.2.2
Jump Diffusion Process
1. Pure Jump Process. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson
process with intensity 𝜆> 0 relative to the filtration ℱt, t ≥0. Suppose St follows a pure
jump process
dSt
St−= (Jt −1)dNt
where Jt is the jump size variable if N jumps at time t, Jt ⟂⟂Nt and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Explain why the term in dNt is Jt −1 and not Jt.

282
5.2.2
Jump Diffusion Process
Show that the above differential equation can also be written as
dSt
St−= dMt
where Mt = ∑Nt
i=1(Ji −1) is a compound Poisson process such that Ji, i = 1, 2, . . . is a
sequence of independent and identically distributed random variables which are also inde-
pendent of Nt.
Solution: Let St−be the value of St just before a jump and assume there occurs an instan-
taneous jump (i.e., dNt = 1) in which St changes from St−to JtSt−where Jt is the jump
size. Thus,
dSt = JtSt−−St−= (Jt −1)St−
or
dSt
St−= (Jt −1).
Therefore, we can write
dSt
St−= (Jt −1)dNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Given the compound Poisson process
Mt =
Nt
∑
i=1
(Ji −1)
we let Mt−be the value of Mt just before a jump event. If N jumps at time t then
dMt = Mt −Mt−= Jt −1.
Thus, in general, we can write
dMt = (Jt −1)dNt
which implies the pure jump process can also be expressed as dSt
St−= dMt.
◽
2. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process with inten-
sity 𝜆> 0 relative to the filtration ℱt, t ≥0. Suppose St follows a pure jump process
dSt
St−= (Jt −1)dNt

5.2.2
Jump Diffusion Process
283
where Jt is the jump size variable if N jumps at time t and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Assume Jt follows a lognormal distribution such that log Jt ∼𝒩(𝜇J, 𝜎2
J) and Jt is also
independent of Nt. By applying It¯o’s formula on log St and taking integrals show that for
t < T,
ST = St
NT−t
∏
i=1
Ji
provided Ji ∈(0, 2], i = 1, 2, . . . is a sequence of independent and identically distributed
jump size random variables which are independent of Nt.
Given St and NT−t = n, show that ST follows a lognormal distribution with mean
𝔼(ST|| St, NT−t = n) = Sten(𝜇J+ 1
2 𝜎2
J )
and variance
Var (ST|| St, NT−t = n) = S2
t (en𝜎2
J −1)en(2𝜇J+𝜎2
J ).
Finally, given only St, show that
𝔼(ST|| St
) = St exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
and
Var (ST|| St
) = S2
t
[
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.
Solution: By letting St−denote the value of St before a jump event, and expanding
d(log St) using Taylor’s theorem and taking note that dNt ⋅dNt = dNt,
d(log St) = dSt
St−−1
2
(dSt
St−
)2
+ 1
3
(dSt
St−
)3
−1
4
(dSt
St−
)4
+ . . .
=
{
(Jt −1) −1
2(Jt −1)2 + 1
3(Jt −1)3 −1
4(Jt −1)4 + . . .
}
dNt
= log JtdNt
provided −1 < Jt −1 ≤1 or 0 < Jt ≤2.
By taking integrals, we have
∫
T
t
d(log Su) = ∫
T
t
log Ju dNu
log
(ST
St
)
=
NT−t
∑
i=1
log Ji

284
5.2.2
Jump Diffusion Process
or
ST = St
NT−t
∏
i=1
Ji
where Ji ∈(0, 2] is the jump size occurring at time instant ti and NT−t = NT −Nt is the
total number of jumps in the time interval (t, T].
Since log Ji ∼𝒩(𝜇J, 𝜎2
J), i = 1, 2, . . . , NT−t are independent and identically distributed,
and conditional on St and NT−t = n,
log ST = log St +
NT−t
∑
i=1
log Ji
follows a normal distribution. Therefore, the mean and variance of ST are
𝔼(ST ||St, NT−t = n) = Sten(𝜇J+ 1
2 𝜎2
J )
and
Var (ST ||St, NT−t = n) = S2
t
(
en𝜎2
J −1
)
en(2𝜇J+𝜎2
J )
respectively.
Finally, conditional only on St, by definition
𝔼(ST|| St
) = 𝔼
(
St
NT−t
∏
i=1
Ji
||||||
St
)
= St𝔼
(NT−t
∏
i=1
Ji
||||||
St
)
= St𝔼
(NT−t
∏
i=1
Ji
)
= St𝔼
(
e
∑NT−t
i=1
log Ji
)
.
By applying the tower property and from Problem 5.2.1.12 (page 264), we have
𝔼(ST|| St
) = St𝔼
[
𝔼
(
e
∑NT−t
i=1
log Ji||||
NT−t
)]
= St exp {𝜆(T −t) [𝔼(elog Jt) −1]}
= St exp {𝜆(T −t) [𝔼(Jt
) −1]} .
Since 𝔼(Jt) = e𝜇J+ 1
2 𝜎2
J therefore
𝔼(ST|St) = St exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
.

5.2.2
Jump Diffusion Process
285
For the case of variance of ST conditional on St, by definition
Var(ST|St) = Var
(
St
NT−t
∏
i=1
Ji
||||||
St
)
= S2
t Var
(NT−t
∏
i=1
Ji
)
= S2
t Var
(
e
∑NT−t
i=1
log Ji
)
= S2
t
{
𝔼
(
e2 ∑NT−t
i=1
log Ji
)
−𝔼
(
e
∑NT−t
i=1
log Ji
)2}
= S2
t
{
𝔼
(
e
∑NT−t
i=1
log J2
i
)
−𝔼
(
e
∑NT−t
i=1
log Ji
)2}
.
By applying the tower property and from Problem 5.2.1.12 (page 264), we have
Var(ST|St) = S2
t 𝔼
[
𝔼
(
e
∑NT−t
i=1
log J2
i ||||
NT−t
)]
−S2
t
{
𝔼
[
𝔼
(
e
∑NT−t
i=1
log Ji||||
NT−t
)]}2
= S2
t exp {𝜆(T −t)
[𝔼(J2
t
) −1]} −S2
t exp {2𝜆(T −t)
[𝔼(Jt) −1]} .
Since 𝔼(Jt) = e𝜇J+ 1
2 𝜎2
J and 𝔼(J2
t ) = e2(𝜇J+𝜎2
J ) therefore
Var(ST|St) = S2
t
[
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.
◽
3. Merton’s Model. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson
process with intensity 𝜆> 0 and {Wt ∶t ≥0} be a standard Wiener process relative to the
same filtration ℱt, t ≥0. Suppose St follows a jump diffusion process with the following
SDE
dSt
St−= (𝜇−D) dt + 𝜎dWt + (Jt −1)dNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt
with constants 𝜇, D and 𝜎being the drift, continuous dividend yield and volatility, respec-
tively, and Jt the jump variable such that log Jt ∼𝒩(𝜇J, 𝜎2
J). Assume that Jt, Wt and Nt are
mutually independent. In addition, we also let Ji, i = 1, 2, . . . be a sequence of indepen-
dent and identically distributed jump size random variables which are also independent of
Nt and Wt.

286
5.2.2
Jump Diffusion Process
By applying It¯o’s formula on log St and taking integrals show that for t < T,
ST = Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
provided Jt ∈(0, 2] and WT−t ∼𝒩(0, T −t).
Given St and NT−t = n, show that ST follows a lognormal distribution with mean
𝔼(ST|| St, NT−t = n) = Ste(𝜇−D)(T−t)+n(𝜇J+ 1
2 𝜎2
J )
and variance
Var (ST|| St, NT−t = n) = S2
t
(
e𝜎2(T−t)+n𝜎2
J −1
)
e2(𝜇−D)(T−t)+n(2𝜇J+𝜎2
J ).
Finally, conditional only on St, show that
𝔼(ST|| St
) = Ste(𝜇−D)(T−t) exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
and
Var (ST|| St
) = S2
t e2(𝜇−D)(T−t) (
e𝜎2(T−t) −1
) [
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.
Solution: By letting St−denote the value of St before a jump event, and expanding
d(log St) using both Taylor’s theorem and It¯o’s lemma,
d(log St) = dSt
St−−1
2
(dSt
St−
)2
+ 1
3
(dSt
St−
)3
−1
4
(dSt
St−
)4
+ . . .
= (𝜇−D) dt + 𝜎dWt + (Jt −1)dNt −1
2(𝜎2 dt + (Jt −1)2dNt)
+1
3(Jt −1)3dNt −1
4(Jt −1)4dNt + . . .
=
(
𝜇−D −1
2𝜎2)
dt + 𝜎dWt
+
{
(Jt −1) −1
2(Jt −1)2 + 1
3(Jt −1)3 −1
4(Jt −1)4 + . . .
}
dNt
=
(
𝜇−D −1
2𝜎2)
dt + 𝜎dWt + log JtdNt
provided −1 < Jt −1 ≤1 or 0 < Jt ≤2.
By taking integrals, we have
∫
T
t
d(log Su) = ∫
T
t
(
𝜇−D −1
2𝜎2)
du + ∫
T
t
𝜎dWu + ∫
T
t
log Ju dNu

5.2.2
Jump Diffusion Process
287
log
(ST
St
)
=
(
𝜇−D −1
2𝜎2)
(T −t) + 𝜎WT−t +
NT−t
∑
i=1
log Ji
or
ST = Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
where Ji ∈(0, 2] is the random jump size occurring at time ti and NT−t = NT −Nt is the
total number of jumps in the time interval (t, T].
Since WT−t ∼𝒩(0, T −t), log Ji ∼𝒩(𝜇J, 𝜎2
J), i = 1, 2, . . . and by independence,
log ST|{St, NT−t = n} ∼𝒩
[
log St +
(
𝜇−D −1
2𝜎2)
(T −t) + n𝜇J, 𝜎2(T −t) + n𝜎2
J
]
.
Therefore, conditional on St and NT−t = n, ST follows a lognormal distribution with mean
𝔼(ST ||St, NT−t = n) = Ste(𝜇−D)(T−t)+n(𝜇J+ 1
2 𝜎2
J )
and variance
Var (ST ||St, NT−t = n) = S2
t
(
e𝜎2(T−t)+n𝜎2
J −1
)
e2(𝜇−D)(T−t)+n(2𝜇J+𝜎2
J ).
Finally, given only St and from the mutual independence of a Wiener process and a com-
pound Poisson process, we have
𝔼(ST|St) = 𝔼
[
Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
||||||
St
]
= Ste
(
𝜇−D−1
2 𝜎2)
(T−t)𝔼(e𝜎WT−t)𝔼
(NT−t
∏
i=1
Ji
)
= Ste(𝜇−D)(T−t)𝔼
(NT−t
∏
i=1
Ji
)
.
Since 𝔼(e𝜎WT−t) = e
1
2 𝜎2(T−t), from Problem 5.2.2.2 (page 282) we therefore have
𝔼(ST|St) = Ste(𝜇−D)(T−t) exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
.
As for the variance of St conditional on St, we can write
Var(ST|St) = Var
[
Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
||||||
St
]
= S2
t e
2
(
𝜇−D−1
2 𝜎2)
(T−t)Var (e𝜎WT−t) Var
(NT−t
∏
i=1
Ji
)
.

288
5.2.2
Jump Diffusion Process
Since Var(e𝜎WT−t) = (e𝜎2(T−t) −1)e𝜎2(T−t) and using the results from Problem 5.2.2.3
(page 285), we finally have
Var(ST|St) = S2
t e2(𝜇−D)(T−t)(e𝜎2(T−t) −1)
[
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.
◽
4. Ornstein–Uhlenbeck Process with Jumps. Let (Ω, ℱ, ℙ) be a probability space and
let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 and {Wt ∶t ≥0} be a
standard Wiener process relative to the same filtration ℱt, t ≥0. Suppose St follows an
Ornstein–Uhlenbeck process with jumps of the form
dSt = 𝜅(𝜃−St−) dt + 𝜎dWt + log JtdNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt
with 𝜅, 𝜃and 𝜎being constant parameters such that 𝜅is the mean-reversion rate, 𝜃is
the long-term mean and 𝜎is the volatility. The random variable Jt is the jump amplitude
such that log Jt ∼𝒩(𝜇J, 𝜎2
J), and Wt, Nt and Jt are mutually independent. Suppose the
sequence of jump amplitudes Ji, i = 1, 2, . . . is independent and identically distributed,
and also independent of Nt and Wt.
Using It¯o’s lemma on e𝜅tSt show that for t < T,
ST = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−u)dWu +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
and conditional on St and NT−t = n,
𝔼(ST|St, NT−t = n) = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)]
+n𝜇J
(
𝜆
𝜆−𝜅
) [e−𝜅(T−t) −e−𝜆(T−t)] [1 −e−𝜆(T−t)]n−1
and
Var(ST|St, NT−t = n) = 𝜎2
2𝜅
[1 −e−2𝜅(T−t)]
+n (𝜇2
J + 𝜎2
J
) (
𝜆
𝜆−2𝜅
) [e−2𝜅(T−t) −e−𝜆(T−t)] [1 −e−𝜆(T−t)]n−1
+n(n −1)𝜇2
J
(
𝜆
𝜆−2𝜅
)2[e−2𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]n−2
−n2𝜇2
J
(
𝜆
𝜆−𝜅
)2[e−𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]2(n−1).

5.2.2
Jump Diffusion Process
289
Finally, conditional on St show also that
𝔼(ST|St
) = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + 𝜆𝜇J
𝜅
[1 −e−𝜅(T−t)]
and
Var (ST|St
) = 𝜎2
2𝜅
[1 −e−2𝜅(T−t)] + 𝜆
2𝜅
(𝜇2
J + 𝜎2
J
) [1 −e−𝜅(T−t)] .
What is the distribution of ST given St?
Solution: In order to solve the jump diffusion process we note that
d(e𝜅tSt) = 𝜅e𝜅tSt dt + e𝜅tdSt + 𝜅2e𝜅tSt( dt)2 + . . .
and using It¯o’s lemma,
d(e𝜅tSt) = 𝜅𝜃e𝜅t dt + 𝜎e𝜅tdWt + e𝜅t log JtdNt.
For t < T the solution is represented by
ST = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−u)dWu + ∫
T
t
e−𝜅(T−u) log JudNu.
Since
∫
T
t
e−𝜅(T−u) log JudNu =
NT
∑
i=Nt
e−𝜅(T−ti) log Ji =
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
where Ji is the random jump size occurring at time ti and NT−t = NT −Nt is the total
number of jumps in the time interval (t, T], therefore
ST = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−u)dWu +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji.
Given St, from the properties of It¯o’s integral we have
𝔼
[
∫
T
t
𝜎e−𝜅(T−u)dWu
]
= 0
and
𝔼
[(
∫
T
t
𝜎e−𝜅(T−u)dWu
)2]
= 𝔼
[
∫
T
t
𝜎2e−2𝜅(T−u) du
]
= 𝜎2
2𝜅
[1 −e−2𝜅(T−t)] .
Thus, given St and NT−t = n, from Problem 5.2.1.17 (page 269) we can easily obtain
𝔼
[NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
||||||
St, NT−t = n
]
= n𝜇J
(
𝜆
𝜆−𝜅
)[e−𝜅(T−t) −e−𝜆(T−t)][1 −e−𝜆(T−t)]n−1

290
5.2.2
Jump Diffusion Process
and
Var
[NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
||||||
St, NT−t = n
]
= n(𝜇2
J + 𝜎2
J)
(
𝜆
𝜆−2𝜅
) [e−2𝜅(T−t) −e−𝜆(T−t)] [1 −e−𝜆(T−t)]n−1
+ n(n −1)𝜇2
J
(
𝜆
𝜆−2𝜅
)2
× [e−2𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]n−2 −n2𝜇2
J
(
𝜆
𝜆−𝜅
)2
× [e−𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]2(n−1).
Therefore,
𝔼(ST|St, NT−t = n) = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)]
+n𝜇J
(
𝜆
𝜆−𝜅
) [e−𝜅(T−t) −e−𝜆(T−t)] [1 −e−𝜆(T−t)]n−1
and
Var (ST|St, NT−t = n) = 𝜎2
2𝜅
[1 −e−2𝜅(T−t)]
+n(𝜇2
J + 𝜎2
J)
(
𝜆
𝜆−2𝜅
) [e−2𝜅(T−t) −e−𝜆(T−t)] [1 −e−𝜆(T−t)]n−1
+n(n −1)𝜇2
J
(
𝜆
𝜆−2𝜅
)2[e−2𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]n−2
−n2𝜇2
J
(
𝜆
𝜆−𝜅
)2[e−𝜅(T−t) −e−𝜆(T−t)]2[1 −e−𝜆(T−t)]2(n−1).
Finally, conditional on St, from Problem 5.2.1.18 (page 272) we have
𝔼
[NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
]
= 𝜆𝜇J
𝜅
[1 −e−𝜅(T−t)]
and
Var
[NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
]
= 𝜆
2𝜅
(𝜇2
J + 𝜎2
J
) [1 −e−2𝜅(T−t)] .
Therefore,
𝔼(ST|St
) = Ste−𝜅(T−t) + 𝜃[1 −e−𝜅(T−t)] + 𝜆𝜇J
𝜅
[1 −e−𝜅(T−t)]
and
Var (ST|St
) = 𝜎2
2𝜅
[1 −e−2𝜅(T−t)] + 𝜆
2𝜅
(𝜇2
J + 𝜎2
J
) [1 −e−𝜅(T−t)] .
Without the jump component, ST given St is normally distributed (see Problem 3.2.2.10,
page 132). With the inclusion of a jump component, the term ∑NT−t
i=1 e−𝜅(T−t−ti) log Ji

5.2.2
Jump Diffusion Process
291
consists of summing over the product of random jump times in the exponent and jump
amplitudes where we do not have an explicit expression for the distribution. Thus, the
distribution of ST conditional on St is not known.
◽
5. Geometric Mean-Reverting Jump Diffusion Model. Let (Ω, ℱ, ℙ) be a probability space
and let {Nt ∶t ≥0} be a Poisson process with intensity 𝜆> 0 and {Wt ∶t ≥0} be a
standard Wiener process relative to the same filtration ℱt, t ≥0. Suppose St follows a
geometric mean-reverting jump diffusion process of the form
dSt
St−= 𝜅(𝜃−log St−) dt + 𝜎dWt + (Jt −1)dNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt
with 𝜅, 𝜃and 𝜎being constant parameters. Here, 𝜅is the mean-reversion rate, 𝜃is the
long-term mean and 𝜎is the volatility. The random variable Jt is the jump amplitude
such that log Jt ∼𝒩(𝜇J, 𝜎2
J), and Wt, Nt and Jt are mutually independent. Suppose the
sequence of jump amplitudes Ji, i = 1, 2, . . . is independent and identically distributed,
and also independent of Nt and Wt.
By applying It¯o’s formula on Xt = log St, show that
dXt =
(
𝜅(𝜃−Xt−) −1
2𝜎2)
dt + 𝜎dWt + log JtdNt.
Using It¯o’s lemma on e𝜅tXt and taking integrals show that for t < T,
log ST = (log St)e−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]
+ ∫
T
t
𝜎e−𝜅(T−s)dWs +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji.
Conditional on St and NT−t = n, show that
𝔼(ST|| St, NT−t = n) = Se−𝜅(T−t)
t
exp
{(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)] + 𝜎2
4𝜅
[1 −e−2𝜅(T−t)]}
×
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
and
Var (ST|| St, NT−t = n) = S2e−𝜅(T−t)
t
exp
{
2
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]}
×
[
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
−1
]
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}

292
5.2.2
Jump Diffusion Process
×
{[
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
−
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]2n}
.
Finally, given only St, show that
𝔼(ST|St) = St exp
{
e−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)] + 𝜎2
4𝜅
[1 −e−2𝜅(T−t)]
+𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s ds −𝜆(T −t)
}
and
Var(ST|St) = S2
t exp
{
2e−𝜅(T−t) + 2
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]}
×
[
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
−1
]
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
e−𝜆(T−t)
×
[
exp
{
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s (
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−𝜅s −2
)
ds
}
−e−𝜆(T−t)
]
.
Solution: We first let St−denote the value of St before a jump event and by expanding
d(log St) using Taylor’s theorem and subsequently applying It¯o’s lemma, we have
d(log St) = dSt
St−−1
2
(dSt
St−
)2
+ 1
3
(dSt
St−
)3
−1
4
(dSt
St−
)4
+ . . .
= 𝜅(𝜃−log St−) dt + 𝜎dWt + (Jt −1)dNt −1
2[𝜎2 dt + (Jt −1)2dNt]
+1
3(Jt −1)3dNt −1
4(Jt −1)4dNt + . . .
=
[
𝜅(𝜃−log St−) −1
2𝜎2]
dt + 𝜎dWt + log JtdNt
provided the random jump amplitude −1 < Jt −1 ≤1 or 0 < Jt ≤2.
Setting Xt = log St−we can redefine the above SDE into an Ornstein–Uhlenbeck process
with jumps
dXt = 𝜅(𝜃−Xt−) dt + 𝜎dWt + log JtdNt
and applying Taylor’s theorem and then Ito’s lemma on d(e𝜅tXt), we have
d(e𝜅tXt) = 𝜅e𝜅tXt−dt + e𝜅tdXt + 𝜅2e𝜅tXt−( dt)2 + . . .
= 𝜅𝜃e𝜅t dt + 𝜎e𝜅tdWt + e𝜅t log JtdNt.

5.2.2
Jump Diffusion Process
293
Taking integrals for t < T,
∫
T
t
d(e𝜅sXs) = ∫
T
t
(
𝜅𝜃e𝜅s −1
2𝜎2e𝜅s)
ds + ∫
T
t
𝜎e𝜅sdWs + ∫
T
t
e𝜅s log Js dNs
or
XT = Xte−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)] + ∫
T
t
𝜎e−𝜅(T−s)dWs +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji.
Substituting XT = log ST and Xt = log St, we finally have
log ST = (log St)e−𝜅(T−t) +
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]
+ ∫
T
t
𝜎e−𝜅(T−s)dWs +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
or
ST = Se−𝜅(T−t)
t
exp
{(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]
+ ∫
T
t
𝜎e−𝜅(T−s)dWs +
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}
.
To find the expectation and variance of ST given St, we note that from It¯o’s integral
∫
T
t
𝜎e−𝜅(T−u)dWu ∼𝒩
(
0, 𝜎2
2𝜅
[1 −e−2𝜅(T−t)])
and hence
𝔼
[
exp
{
∫
T
t
𝜎e−𝜅(T−u)dWu
}]
= exp
{
𝜎2
4𝜅
[1 −e−2𝜅(T−t)]}
and
Var
[
exp
{
∫
T
t
𝜎e−𝜅(T−u)dWu
}]
=
[
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
−1
]
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
.
Conditional on NT−t = n, from Problem 5.2.1.17 (page 269) and by setting 𝜉= 1, X =
log J and due to the independence of log Ji, i = 1, 2, . . . , n, we can write
𝔼
[
exp
{ n
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
=
n
∏
i=1
{
∫
T
t
Mlog Ji (e−𝜅s) 𝜆e−𝜆(T−t−s) ds
}
=
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n

294
5.2.2
Jump Diffusion Process
and
𝔼
[
exp
{
2
n
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
=
n
∏
i=1
{
∫
T
t
Mlog Ji (2e−𝜅s) 𝜆e−𝜆(T−t−s) ds
}
=
[
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
.
Therefore,
Var
[
exp
{ n
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
= 𝔼
[
exp
{
2
n
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
−𝔼
[
exp
{ n
∑
i=1
e−𝜅(T−t−ti) log Ji
}]2
=
[
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
−
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]2n
.
Conditional on St and NT−t = n and due to the independence of Wt and Nt, we can easily
show that
𝔼(ST|| St, NT−t = n) = Se−𝜅(T−t)
t
exp
{(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)] + 𝜎2
4𝜅
[1 −e−2𝜅(T−t)]}
×
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
and
Var (ST|| St, NT−t = n) = S2e−𝜅(T−t)
t
exp
{
2
(
𝜃−𝜎2
2𝜅
)
[1 −e−𝜅(T−t)]
}
×
[
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
−1
]
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
×
{[
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]n
−
[
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s−𝜆(T−t−s) ds
]2n}
.
Finally, by treating NT−t as a random variable, from Problem 5.2.1.18 (page 272) and by
setting 𝜉= 1 and X = log J, we can express
𝔼
[
exp
{NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
= exp
{
𝜆∫
T
t
[Mlog J(e−𝜅s) −1] ds
}
= exp
{
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s ds −𝜆(T −t)
}

5.2.2
Jump Diffusion Process
295
and
𝔼
[
exp
{
2
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
= exp
{
𝜆∫
T
t
[Mlog J(2e−𝜅s) −1] ds
}
= exp
{
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s ds −𝜆(T −t)
}
and subsequently
Var
[
exp
{NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
= 𝔼
[
exp
{
2
NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}]
−𝔼
[
exp
{NT−t
∑
i=1
e−𝜅(T−t−ti) log Ji
}]2
= exp
{
𝜆∫
T
t
e2𝜇Je−𝜅s+𝜎2
J e−2𝜅s ds −𝜆(T −t)
}
−exp
{
2𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s ds −2𝜆(T −t)
}
= e−𝜆(T−t)
[
exp
{
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s (
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−𝜅s −2
)
ds
}
−e−𝜆(T−t)
]
.
Conditional only on St and from the independence of Wt and Nt, we have
𝔼(ST|St) = Se−𝜅(T−t)
t
exp
{(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)] + 𝜎2
4𝜅
[1 −e−2𝜅(T−t)]
+𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s ds −𝜆(T −t)
}
and
Var(ST|St) = S2e−𝜅(T−t)
t
exp
{
2
(
𝜃−𝜎2
2𝜅
) [1 −e−𝜅(T−t)]}
×
[
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
−1
]
exp
{
𝜎2
2𝜅
[1 −e−2𝜅(T−t)]}
e−𝜆(T−t)
×
[
exp
{
𝜆∫
T
t
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−2𝜅s (
e𝜇Je−𝜅s+ 1
2 𝜎2
J e−𝜅s −2
)
ds
}
−e−𝜆(T−t)
]
.
◽
6. Kou’s Model. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶t ≥0} be a Poisson process
with intensity 𝜆> 0 and {Wt ∶t ≥0} be a standard Wiener process relative to the same

296
5.2.2
Jump Diffusion Process
filtration ℱt, t ≥0. Suppose St follows a jump diffusion process with the following SDE
dSt
St−= (𝜇−D) dt + 𝜎dWt + (Jt −1)dNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt
with constants 𝜇, D and 𝜎> 0 being the drift, continuous dividend yield and volatility,
respectively. The jump variable is denoted by Jt, where Xt = log Jt follows an asymmetric
double exponential distribution with density function
fXt(x) =
{
p𝛼e−𝛼x
x ≥0
(1 −p)𝛽e𝛽x
x > 0
where 0 ≤p ≤1, 𝛼> 1 and 𝛽> 0. Assume that Jt, Wt and Nt are mutually independent
and let Ji, i = 1, 2, . . . be a sequence of independent and identically distributed random
variables which are independent of Nt and Wt.
By applying It¯o’s formula on log St and taking integrals show that for t < T,
ST = Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
provided the random variable Ji ∈(0, 2], i = 1, 2, . . . , NT−t and WT−t ∼𝒩(0, T −t).
Given St and NT−t = n, show that the mean and variance of ST are
𝔼(ST|St, NT−t = n) = Ste(𝜇−D)(T−t)
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]n
and
Var (ST|St, NT−t = n) = S2
t e2(𝜇−D)(T−t) (
e𝜎2(T−t) −1
)
×
{
p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2 −
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]2}n
.
Finally, conditional only on St, show that
𝔼(ST|St) = Ste(𝜇−D)(T−t)
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]
exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
and
Var(ST|St) = S2
t e2(𝜇−D)(T−t) (
e𝜎2(T−t) −1
) [
p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2
]
×
[
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.

5.2.2
Jump Diffusion Process
297
Solution: Using the same steps as described in Problem 5.2.2.3 (page 285), for t < T we
can show the solution of the jump diffusion process is
ST = Ste
(
𝜇−D−1
2 𝜎2)
(T−t)+𝜎WT−t
NT−t
∏
i=1
Ji
provided Ji ∈(0, 2], i = 1, 2, . . . , NT−t.
To find the mean and variance of Jt we note that
𝔼(Jt
) = 𝔼(eXt)
= ∫
∞
−∞
exfXt(x) dx
= ∫
0
−∞
(1 −p)𝛽e(1+𝛽)xdx + ∫
∞
0
p𝛼e(1−𝛼)xdx
= p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
and
𝔼(J2
t
) = 𝔼(e2Xt)
= ∫
∞
−∞
e2xfXt(x) dx
= ∫
0
−∞
(1 −p)𝛽e(2+𝛽)xdx + ∫
∞
0
p𝛼e(2−𝛼)xdx
= p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2
so that
Var (Jt) = 𝔼(J2
t ) −𝔼(Jt)2
= p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2 −
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]2
.
Since WT−t ∼𝒩(0, T −t) so that
𝔼(e𝜎WT−t) = e
1
2 𝜎2(T−t)
and
Var (e𝜎WT−t) =
(
e𝜎2(T−t) −1
)
e𝜎2(T−t)
and conditional on St, NT−t = n and from independence, we have
𝔼(ST|St, NT−t = n) = Ste(𝜇−D)(T−t)−1
2 𝜎2(T−t)𝔼(e𝜎WT−t)
n
∏
i=1
𝔼(Ji)
= Ste(𝜇−D)(T−t)
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]n

298
5.2.3
Girsanov’s Theorem for Jump Processes
and
Var (ST|St, NT−t = n) = S2
t e2(𝜇−D)(T−t)−𝜎2(T−t)Var (e𝜎WT−t)
n
∏
i=1
Var (Ji)
= S2
t e2(𝜇−D)(T−t) (
e𝜎2(T−t) −1
)
×
{
p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2 −
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]2}n
.
Finally, conditional on St and from Problem 5.2.2.2 (page 282), we can show
𝔼(ST|St
) = Ste(𝜇−D)(T−t)−1
2 𝜎2(T−t)𝔼(e𝜎WT−t) 𝔼
(NT−t
∏
i=1
Ji
)
= Ste(𝜇−D)(T−t)
[
p
𝛼
𝛼−1 + (1 −p)
𝛽
𝛽+ 1
]
exp
{
𝜆
(
e𝜇J+ 1
2 𝜎2
J −1
)
(T −t)
}
and
Var (ST|St
) = S2
t e2(𝜇−D)(T−t)−𝜎2(T−t)Var (e𝜎WT−t) Var
(NT−t
∏
i=1
Ji
)
= S2
t e2(𝜇−D)(T−t) (
e𝜎2(T−t) −1
) [
p
𝛼
𝛼−2 + (1 −p)
𝛽
𝛽+ 2
]
×
[
exp
{
𝜆(T −t)
(
e2(𝜇J+𝜎2
J ) −1
)}
−exp
{
2𝜆(T −t)
(
e𝜇J+ 1
2 𝜎2
J −1
)}]
.
◽
5.2.3
Girsanov’s Theorem for Jump Processes
1. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Let 𝜂> 0 and consider the
Radon–Nikod´ym derivative process
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt.
Show that
dZt
Zt−= (𝜆−𝜂) dt −
(𝜆−𝜂
𝜆
)
dNt
for 0 ≤t ≤T.
Solution: From Taylor’s theorem,
dZt = 𝜕Zt
𝜕t dt + 𝜕Zt
𝜕Nt
dNt + 𝜕2Zt
𝜕t𝜕Nt
(dNt dt) + 1
2!
𝜕2Zt
𝜕N2
t
(dNt)2 + 1
3!
𝜕3Zt
𝜕N3
t
(dNt)3 + . . .

5.2.3
Girsanov’s Theorem for Jump Processes
299
Since dNt dt = 0, (dNt)2 = (dNt)3 = . . . = dNt, we have
dZt = 𝜕Zt
𝜕t dt +
(
𝜕Zt
𝜕Nt
+ 1
2!
𝜕2Zt
𝜕N2
t
+ 1
3!
𝜕3Zt
𝜕N3
t
+ . . .
)
dNt.
From Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt we can express
𝜕Zt
𝜕t = (𝜆−𝜂)e(𝜆−𝜂)t(𝜂
𝜆
)Nt = (𝜆−𝜂)Zt
𝜕Zt
𝜕Nt
= e(𝜆−𝜂)t log
(𝜂
𝜆
) (𝜂
𝜆
)Nt = log
(𝜂
𝜆
)
Zt
𝜕2Zt
𝜕N2
t
= log
(𝜂
𝜆
) 𝜕Zt
𝜕Nt
=
[
log
(𝜂
𝜆
)]2
Zt.
In general, we can deduce that
𝜕mZt
𝜕Nm
t
=
[
log
(𝜂
𝜆
)]m
Zt,
m = 1, 2, . . .
Therefore,
dZt = (𝜆−𝜂)Zt dt +
{
log
(𝜂
𝜆
)
+ 1
2!
[
log
(𝜂
𝜆
)]2
+ 1
3!
[
log
(𝜂
𝜆
)]3
+ . . .
}
ZtdNt
= (𝜆−𝜂)Zt dt +
{
e
log
( 𝜂
𝜆
)
−1
}
ZtdNt
= (𝜆−𝜂)Zt dt +
(𝜂−𝜆
𝜆
)
ZtdNt.
By letting Zt−denote the value of Zt before a jump event, we have
dZt
Zt−= (𝜆−𝜂) dt −
(𝜆−𝜂
𝜆
)
dNt.
◽
2. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Let 𝜂> 0 and consider the
Radon–Nikod´ym derivative process
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt.
Show that 𝔼ℙ(Zt
) = 1 and Zt is a positive ℙ-martingale for 0 ≤t ≤T.

300
5.2.3
Girsanov’s Theorem for Jump Processes
Solution: From Problem 5.2.3.1 (page 298), Zt satisfies
dZt
Zt−= (𝜆−𝜂) dt −
(𝜆−𝜂
𝜆
)
dNt
=
(𝜂−𝜆
𝜆
)
(dNt −𝜆dt)
=
(𝜂−𝜆
𝜆
)
d(Nt −𝜆t)
=
(𝜂−𝜆
𝜆
)
d̂Nt
where ̂Nt = Nt −𝜆t. Since ̂Nt = Nt −𝜆t is a ℙ-martingale, therefore
(𝜂−𝜆
𝜆
)
̂Nt is also
a ℙ-martingale.
Taking integrals,
∫
t
0
dZu = ∫
t
0
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
Zt = Z0 + ∫
t
0
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
and taking expectations,
𝔼ℙ(Zt
) = 𝔼ℙ(Z0
) + 𝔼ℙ
[
∫
t
0
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
]
= 1
since Z0 = 1 and due to ̂Nt being a ℙ-martingale, 𝔼ℙ
[
∫
t
0
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
]
= 0. Thus,
𝔼ℙ(Zt
) = 1 for all 0 ≤t ≤T.
To show that Zt is a positive ℙ-martingale, by taking integrals for s < t,
∫
t
s
dZu = ∫
t
s
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
Zt −Zs = ∫
t
s
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu.
Taking expectations under the filtration ℱs, s < t,
𝔼ℙ(Zt −Zs|ℱs
) = 𝔼ℙ
[
∫
t
s
Zu−
(𝜂−𝜆
𝜆
)
d̂Nu
|||||
ℱs
]
and because ̂Nt is a ℙ-martingale, therefore
𝔼ℙ(Zt −Zs|ℱs
) = 0
or
𝔼ℙ(Zt|ℱs
) = Zs.

5.2.3
Girsanov’s Theorem for Jump Processes
301
In addition, since Zt > 0 for all 0 ≤t ≤T, therefore |Zt| = Zt and hence 𝔼ℙ(|Zt|) =
𝔼ℙ(Zt
) = 1 < ∞for all 0 ≤t ≤T. Finally, because Zt is ℱt-adapted, we can conclude
that Zt is a positive ℙ-martingale for all 0 ≤t ≤T.
◽
3. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Let 𝜂> 0 and consider the
Radon–Nikod´ym derivative process
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt.
By changing the measure ℙto a measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
show that under the ℚmeasure, Nt ∼Poisson(𝜂t) with intensity 𝜂> 0 for 0 ≤t ≤T.
Solution: To solve this problem we need to show
𝔼ℚ(euNt) = e𝜂t(eu−1)
which is the moment generating function of a Poisson process, Nt ∼Poisson(𝜂t),
0 ≤t ≤T.
For u ∈ℝ, using the moment generating function approach,
𝔼ℚ(euNt) = 𝔼ℙ
(
euNt dℚ
dℙ
||||ℱt
)
= 𝔼ℙ
[
euNte(𝜆−𝜂)t(𝜂
𝜆
)Nt]
= e(𝜆−𝜂)t𝔼ℙ[
e(u+log( 𝜂
𝜆))Nt
]
= e(𝜆−𝜂)t exp
{
𝜆t
(
e
u+log
( 𝜂
𝜆
)
−1
)}
since the moment generating function of Nt ∼Poisson(𝜆t) is
𝔼ℙ(emNt) = e𝜆t(em−1)
for all m ∈ℝand hence
𝔼ℚ(euNt) = e𝜂t(eu−1)
which is the moment generating function of a Poisson process with intensity 𝜂. Therefore,
under the ℚmeasure, Nt ∼Poisson(𝜂t) for 0 ≤t ≤T.
◽

302
5.2.3
Girsanov’s Theorem for Jump Processes
4. Let {Nt ∶0 ≤t ≤0} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, t ≥0. Suppose 𝜃t is an adapted process,
0 ≤t ≤T and 𝜂> 0. We consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
= e(𝜆−𝜂)t(𝜂
𝜆
)Nt
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du.
Show that 𝔼ℙ(Zt) = 1 and Zt is a positive ℙ-martingale for 0 ≤t ≤T.
Solution: From Problem 4.2.2.1 (page 194) and Problem 5.2.3.2 (page 299), we have
shown that 𝔼ℙ(
Z(1)
t
)
= 1, 𝔼ℙ(
Z(2)
t
)
= 1 and both Z(1)
t
and Z(2)
t
are ℙ-martingales for
0 ≤t ≤T.
From It¯o’s lemma,
d(Z(1)
t Z(2)
t ) = Z(1)
t−dZ(2)
t
+ Z(2)
t dZ(1)
t
+ dZ(1)
t dZ(2)
t
where Z(1)
t−is the value of Z(1)
t
before a jump event. Since dZ(1)
t dZ(2)
t
= 0 thus, by taking
integrals,
∫
t
0
d(Z(1)
u Z(2)
u ) = ∫
t
0
Z(1)
u−dZ(2)
u
+ ∫
t
0
Z(2)
u dZ(1)
u
Z(1)
t Z(2)
t
= Z(1)
0 Z(2)
0
+ ∫
t
0
Z(1)
u−dZ(2)
u
+ ∫
t
0
Z(2)
u dZ(1)
u .
Taking expectations under the ℙmeasure,
𝔼ℙ(
Z(1)
t Z(2)
t
)
= 𝔼ℙ(
Z(1)
0 Z(2)
0
)
= 1
since Z(1)
0
= 1, Z(2)
0
= 1 and both Z(1)
t
and Z(2)
t
are ℙ-martingales.
To show that Zt = Z(1)
t
⋅Z(2)
t
is a positive ℙ-martingale, by taking integrals for s < t of
d(Z(1)
t Z(2)
t ) = Z(1)
t−dZ(2)
t
+ Z(2)
t dZ(1)
t
we have
Z(1)
t Z(2)
t
= Z(1)
s Z(2)
s
+ ∫
t
s
Z(1)
u−dZ(2)
u
+ ∫
t
s
Z(2)
u dZ(1)
u
and taking expectations with respect to the filtration ℱs, s < t
𝔼ℙ(
Z(1)
t Z(2)
t
|||ℱs
)
= Z(1)
s Z(2)
s
since 𝔼ℙ
(
∫
t
s
Z(1)
u−dZ(2)
u
|||||
ℱs
)
= 𝔼ℙ
(
∫
t
s
Z(2)
u dZ(1)
u
|||||
ℱs
)
= 0.

5.2.3
Girsanov’s Theorem for Jump Processes
303
In addition, since Z(1)
t
> 0 and Z(2)
t
> 0 for all 0 ≤t ≤T, therefore |||Z(1)
t Z(2)
t
||| = Z(1)
t Z(2)
t
and
hence 𝔼ℙ(||Zt||
) = 𝔼ℙ(Zt) = 1 < ∞for 0 ≤t ≤T. Finally, because Zt is also ℱt-adapted
we can conclude that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
◽
5. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probabil-
ity space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Suppose 𝜃t is an adapted
process, 0 ≤t ≤T and 𝜂> 0. We consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
= e(𝜆−𝜂)t(𝜂
𝜆
)Nt
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du.
By changing the measure ℙto measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
show that, under the ℚmeasure, Nt is a Poisson process with intensity 𝜂> 0, the process
̃Wt = Wt + ∫
t
0
𝜃u du is a ℚ-standard Wiener process and Nt ⟂⟂̃Wt for 0 ≤t ≤T.
Solution: The first two results are given in Problem 5.2.3.3 (page 301) and Problem
4.2.2.10 (page 205).
To prove the final result we need to show that, under the ℚmeasure,
𝔼ℚ(eu1Nt+u2 ̃Wt) = e𝜂t(eu1−1) ⋅e
1
2 u2
2t
which is a joint product of independent moment generating functions of Nt ∼Poisson(𝜂t)
and ̃Wt ∼𝒩(0, t).
From the definition
𝔼ℚ(
eu1Nt+u2 ̃Wt
)
= 𝔼ℙ(
eu1Nt+u2 ̃WtZt
)
= 𝔼ℙ(
eu1NtZ(1)
t
⋅eu2 ̃WtZ(2)
t
)
we let
Zt = Z
(1)
t
⋅Z
(2)
t
where
Z
(1)
t
= eu1NtZ(1)
t
and
Z
(2)
t
= eu2 ̃WtZ(2)
t .
Using It¯o’s lemma, we have
dZt = d
(
Z
(1)
t Z
(2)
t
)
= Z
(1)
t dZ
(2)
t
+ Z
(2)
t dZ
(1)
t
+ dZ
(1)
t dZ
(2)
t
where Z
(1)
t−is the value of Z
(1)
t
before a jump event.

304
5.2.3
Girsanov’s Theorem for Jump Processes
For dZ
(1)
t
we can expand using Taylor’s theorem,
dZ
(1)
t
= 𝜕Z
(1)
t
𝜕Nt
dNt + 𝜕Z
(1)
t
𝜕Z(1)
t
dZ(1)
t
+ 1
2!
⎡
⎢
⎢⎣
𝜕2Z
(1)
t
𝜕N2
t
(dNt)2 + 2 𝜕2Z
(1)
t
𝜕Nt𝜕Z(1)
t
(dNtdZ(1)
t ) + 𝜕2Z
(1)
t
𝜕(Z(1)
t )2 (dZ(1)
t )2
⎤
⎥
⎥⎦
+ 1
3!
⎡
⎢
⎢⎣
𝜕3Z
(1)
t
𝜕N3
t
(dNt)3 + 3 𝜕3Z
(1)
t
𝜕N2
t 𝜕Z(1)
t
(dNt)2(dZ(1)
t )
+3
𝜕3Z
(1)
t
𝜕Nt𝜕(Z(1)
t )2 (dNt)(dZ(1)
t )2 + 𝜕3Z
(1)
t
𝜕(Z(1)
t )3 (dZ(1)
t )3
⎤
⎥
⎥⎦
+ . . .
Because dZ(1)
t
=
(𝜂−𝜆
𝜆
)
Z(1)
t−d̂Nt, Z
(1)
t
= eu1NtZ(1)
t
and d̂Nt = dNt −𝜆dt, we can write
dZ
(1)
t
= u1Z
(1)
t−dNt + eu1NtdZ(1)
t
+ 1
2!
[
u2
1Z
(1)
t−dNt + 2
(𝜂−𝜆
𝜆
)
u1Z
(1)
t−dNtd̂Nt
]
+ 1
3!
[
u3
1Z
(1)
t−dNt + 3
(𝜂−𝜆
𝜆
)
u2
1Z
(1)
t−dNtd̂Nt
]
+ . . .
=
[
u1 + 1
2!u2
1 + 1
3!u3
1 + . . .
]
Z
(1)
t−dNt +
(𝜂−𝜆
𝜆
)
eu1NtZ(1)
t d̂Nt
+
(𝜂−𝜆
𝜆
) [
u1 + 1
2!u2
1 + 1
3!u3
1 + . . .
]
Z
(1)
t−dNtd̂Nt
= (eu1 −1) Z
(1)
t−dNt +
(𝜂−𝜆
𝜆
)
Z
(1)
t−d̂Nt +
(𝜂−𝜆
𝜆
)
(eu1 −1) Z
(1)
t−dNtd̂Nt
=
(𝜂
𝜆
)
(eu1 −1)Z
(1)
t−dNt +
(𝜂−𝜆
𝜆
)
Z
(1)
t−d̂Nt
since (dNt)2 = dNt and dNt dt = 0.
For the case of dZ
(2)
t , by applying Taylor’s theorem
dZ
(2)
t
= 𝜕Z
(2)
t
𝜕̃Wt
d ̃Wt + 𝜕Z
(2)
t
𝜕Z(2)
t
dZ(2)
t
+ 1
2!
⎡
⎢
⎢⎣
𝜕2Z
(2)
t
𝜕( ̃Wt)2 (d ̃Wt)2 + 2 𝜕2Z
(2)
t
𝜕̃Wt𝜕Z(2)
t
(d ̃Wt)(dZ(2)
t ) + 𝜕2Z
(2)
t
𝜕(Z(2)
t )2 (dZ(2)
t )2
⎤
⎥
⎥⎦
+ . . .

5.2.3
Girsanov’s Theorem for Jump Processes
305
and since d ̃Wt = dWt + 𝜃t dt and dZ(2)
t
= −𝜃tZ(2)
t dWt (see Problem 4.2.2.2, page 196), we
can write
dZ
(2)
t
= 1
2u2
2Z
(2)
t
dt + (u2 −𝜃t)Z
(2)
t dWt.
Since d̂Nt = dNt −𝜆dt, dNtdWt = 0, dWt dt = 0, dNt dt = 0, (dWt)2 = dt and ( dt)2 = 0,
we have
Z
(1)
t−dZ
(2)
t
= 1
2u2
2Zt dt + (u2 −𝜃t)ZtdWt,
Z
(2)
t dZ
(1)
t
=
(𝜂
𝜆
)
(eu1 −1)ZtdNt +
(𝜂−𝜆
𝜆
)
Ztd̂Nt
and
dZ
(1)
t dZ
(2)
t
= 0.
Thus, by letting dNt = d̂Nt + 𝜆dt, we have
dZt = 1
2u2
2Zt dt + (u2 −𝜃t)ZtdWt +
(𝜂
𝜆
)
(eu1 −1)ZtdNt +
(𝜂−𝜆
𝜆
)
Ztd̂Nt
=
[
𝜂(eu1 −1) + 1
2u2
2
]
Zt dt + (u2 −𝜃t)ZtdWt +
(𝜂eu1 −𝜆
𝜆
)
Ztd̂Nt.
Taking integrals, we have
Zt = 1 + ∫
t
0
[
𝜂(eu1 −1) + 1
2u2
2
]
Zs ds + ∫
t
0
(u2 −𝜃s)Zs dWs
+ ∫
t
0
(𝜂eu1 −𝜆
𝜆
)
Zs d̂Ns
where Z0 = 1, and because ̂Nt and Wt are ℙ-martingales, thus by taking expectations under
the ℙmeasure we have
𝔼ℙ(
Zt
)
= 1 + ∫
t
0
[
𝜂(eu1 −1) + 1
2u2
2
]
𝔼ℙ(
Zs
)
ds.
By differentiating the above equation with respect to t,
d
dt𝔼ℙ(
Zt
)
=
[
𝜂(eu1 −1) + 1
2u2
2
]
𝔼ℙ(
Zt
)
or
dmt
dt −
[
𝜂(eu1 −1) + 1
2u2
2
]
mt = 0
where mt = 𝔼ℙ(
Zt
)
.

306
5.2.3
Girsanov’s Theorem for Jump Processes
By setting the integrating factor to be I = e−∫(𝜂(eu1−1)+ 1
2 u2
2) dt = e−(𝜂(eu1−1)+ 1
2 u2
2)t and mul-
tiplying it by the first-order ordinary differential equation, we have
d
dt
(
mte−𝜂t(eu1−1)−1
2 u2
2t)
= 0
or
e−𝜂t(eu1−1)−1
2 u2
2t𝔼ℙ(Zt) = C
where C is a constant. Since 𝔼ℙ(
Z0
)
= 𝔼ℙ(
eu1N0Z(1)
0
⋅eu2 ̃W0Z(2)
0
)
= 1, therefore C = 1.
Thus, we will have
𝔼ℚ(
eu1Nt+u2 ̃Wt
)
= 𝔼ℙ(
Zt
)
= e𝜂t(eu1−1)+ 1
2 u2
2t.
Since the joint moment generating function of
𝔼ℚ(
eu1Nt+u2 ̃Wt
)
= e𝜂t(eu1−1) ⋅e
1
2 u2
2t
can be expressed as a product of the moment generating functions for Nt ∼Poisson(𝜂t)
and ̃Wt ∼𝒩(0, t), respectively, we can deduce that Nt and ̃Wt are independent.
◽
6. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, t ≥0. Let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function ℙ(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and ∑K
k=1 ℙ(X = xk) = 1. Let X1, X2, . . .
also be independent of Nt. From the definition of a compound Poisson process
Mt =
Nt
∑
i=1
Xi,
t ≥0
show that Mt and Nt can be expressed as
Mt =
K
∑
k=1
xkN(k)
t ,
Nt =
K
∑
k=1
N(k)
t
where N(k)
t
∼Poisson(𝜆tp(xk)) such that N(i)
t
⟂⟂N(j)
t , i ≠j.
Let 𝜂1, 𝜂2, . . . , 𝜂K > 0 and consider the Radon–Nikod´ym derivative process
Zt =
K
∏
k=1
Z(k)
t
where Z(k)
t
= e(𝜆k−𝜂k)t
( 𝜂k
𝜆k
)N(k)
t
such that 𝜆k = 𝜆p(xk), k = 1, 2, . . . , K.
Show that 𝔼ℙ(Zt) = 1 and Zt is a positive ℙ-martingale for 0 ≤t ≤T.
Solution: The first part of the result follows from Problem 5.2.1.16 (page 268).

5.2.3
Girsanov’s Theorem for Jump Processes
307
From Problem 5.2.3.1 (page 298), for each k = 1, 2, . . . , K, Z(k)
t
satisfies
dZ(k)
t
Z(k)
t−
=
(𝜂k −𝜆k
𝜆k
)
d̂N(k)
t
where ̂N(k)
t
= N(k)
t
−𝜆kt, 𝜆k = 𝜆tp(xk), therefore we can easily show that ̂N(k)
t
= N(k)
t
−
𝜆kt is a ℙ-martingale. Thus,
(𝜂k −𝜆k
𝜆k
)
̂N(k)
t
is also a ℙ-martingale and subsequently,
𝔼ℙ(
Z(k)
t
)
= 1 and Z(k)
t
is a ℙ-martingale for 0 ≤t ≤T.
For the case
Zt =
K
∏
i=1
Z(k)
t
and because the Poisson processes N(i)
t
and N(j)
t , i ≠j have no simultaneous jumps, we can
deduce that Z(i)
t
⟂⟂Z(j)
t , i ≠j. Therefore,
𝔼ℙ(Zt
) = 𝔼ℙ
( K
∏
k=1
Z(k)
t
)
=
K
∏
k=1
𝔼ℙ(
Z(k)
t
)
=
K
∏
k=1
1 = 1
for 0 ≤t ≤T.
Finally, to show that Zt is a positive ℙ-martingale, 0 ≤t ≤T we prove this result via math-
ematical induction. Let K = 1, then obviously Zt is a positive ℙ-martingale. Assume that
Zt is a positive ℙ-martingale for K = m, m ≥1. For K = m + 1 and because ∏m
k=1 Z(k)
t
and
Z(m+1)
t
have no simultaneous jumps, by It¯o’s lemma
d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
)
= Z(m+1)
t−
d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
+
(
Z(1)
t−Z(2)
t−. . . Z(m)
t−
)
dZ(m+1)
t
+ d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
dZ(m+1)
t
where Z(k)
t−is the value of Z(k)
t
before a jump event.
Since N(i)
t
⟂⟂N(j)
t , i ≠j therefore d
(
Z(1)
t Z(2)
t
. . . Z(m)
t
)
dZ(m+1)
t
= 0, and by taking integrals
for s < t,
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
= Z(1)
s Z(2)
s
. . . Z(m)
s
Z(m+1)
s
+ ∫
t
s
Z(m+1)
u−
d
(
Z(1)
u Z(2)
u
. . . Z(m)
u
)
+ ∫
t
s
(
Z(1)
u−Z(2)
u−. . . Z(m)
u−
)
dZ(m+1)
u
.
Because
m∏
k=1
Z(k)
t
and Z(m+1)
t
are positive ℙ-martingales and the integrands are
left-continuous, taking expectations with respect to the filtration ℱs, s < t
𝔼ℙ(
Z(1)
t Z(2)
t
. . . Z(m)
t
Z(m+1)
t
||| ℱs
)
= Z(1)
s Z(2)
s
. . . Z(m)
s
Z(m+1)
s
.

308
5.2.3
Girsanov’s Theorem for Jump Processes
In addition, since Z(i)
t
> 0, i = 1, 2, . . . , m + 1 for all t ≥0, therefore
||||||
m+1
∏
k=1
Z(k)
t
||||||
=
m+1
∏
k=1
Z(k)
t
and hence for t ≥0,
𝔼ℙ
(||||||
m+1
∏
k=1
Z(k)
t
||||||
)
= 𝔼ℙ
(m+1
∏
k=1
Z(k)
t
)
= 1 < ∞.
Finally, because
m+1
∏
k=1
Z(k)
t
is ℱt-adapted, the process
m+1
∏
k=1
Z(k)
t
is a positive ℙ-martingale.
Thus, from mathematical induction, Zt =
K∏
k=1
Z(k)
t
is a positive ℙ-martingale for
0 ≤t ≤T.
◽
7. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function ℙ(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and ∑K
k=1 ℙ(X = xk) = 1. Let X1, X2, . . .
also be independent of Nt. From the definition of a compound Poisson process
Mt =
Nt
∑
i=1
Xi,
t ≥0
we can set Mt and Nt as
Mt =
K
∑
k=1
xkN(k)
t ,
Nt =
K
∑
k=1
N(k)
t
where N(k)
t
∼Poisson (𝜆tp (xk
)) is the number of jumps in Mt of size xk up to and includ-
ing time t such that N(i)
t
⟂⟂N(j)
t , i ≠j.
Let 𝜂1, 𝜂2, . . . , 𝜂K > 0 and consider the Radon–Nikod´ym derivative process
Zt =
K
∏
k=1
Z(k)
t
where Z(k)
t
= e(𝜆k−𝜂k)t
( 𝜂k
𝜆k
)N(k)
t
such that 𝜆k = 𝜆p(xk), k = 1, 2, . . . , K. By changing the
measure ℙto a measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt

5.2.3
Girsanov’s Theorem for Jump Processes
309
show that under the ℚmeasure, Mt is a compound Poisson process with intensity 𝜂=
∑K
k=1 𝜂k > 0 and X1, X2, . . . is a sequence of independent and identically distributed ran-
dom variables with common probability mass function
ℚ(X = xk) = q(xk) =
𝜂k
𝜂1 + 𝜂2 + . . . + 𝜂K
such that
K∑
k=1
ℚ(X = xk) = 1.
Solution: From the independence of N(1)
t , N(2)
t , . . . , N(K)
t
under ℙ, for u ∈ℝand using
the moment generating function approach
𝔼ℚ(euMt) = 𝔼ℙ
(
euMt dℚ
dℙ
||||ℱt
)
= 𝔼ℙ
⎡
⎢
⎢⎣
eu ∑K
k=1 xkN(k)
t
⋅
K
∏
k=1
e(𝜆k−𝜂k)
( 𝜂k
𝜆k
)N(k)
t ⎤
⎥
⎥⎦
=
K
∏
k=1
e(𝜆k−𝜂k)t𝔼ℙ
[
e
(
uxk+log( 𝜂k
𝜆k )
)
N(k)
t
]
.
From Problem 5.2.1.14 (page 266), for a constant m > 0, 𝔼ℙ(
emN(k)
t
)
= e𝜆tp(xk)(em−1)
therefore
𝔼ℚ(euMt) =
K
∏
k=1
e(𝜆k−𝜂k)t ⋅e𝜆tp(xk)(e
uxk+log( 𝜂k
𝜆k
)
−1)
=
K
∏
k=1
e(𝜆k−𝜂k)t ⋅e
𝜆kt
( 𝜂t
k
𝜆k euxk −1
)
=
K
∏
k=1
e𝜂kt(euxk −1)
= e
𝜂t
(∑K
k=1 q(xk)euxk −1
)
which is the moment generating function for a compound Poisson process with intensity
𝜂> 0 and jump size distribution ℚ(Xi = xk) = q(xk) = 𝜂k
𝜂, i = 1, 2, . . .
◽
8. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent
of Nt, and define the compound Poisson process as
Mt =
Nt
∑
i=1
Xi,
0 ≤t ≤T.

310
5.2.3
Girsanov’s Theorem for Jump Processes
By changing the measure ℙto a measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
show that for 𝜂> 0 the Radon–Nikod´ym derivative process Zt can be written as
Zt =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where ℙ(X) (f ℙ(X)) and ℚ(X) (f ℚ(X)) are the probability mass (density) functions of ℙ
and ℚ, respectively. Note that for the case of continuous variables Xi, i = 1, 2, . . . , we
assume ℚis absolutely continuous with respect to ℙon ℱ.
Solution: We first consider the discrete case where we let X1, X2, . . . be a sequence of
independent and identically distributed random variables with common probability mass
function ℙ(X = xk) = p(xk) > 0, k = 1, 2, . . . , K and ∑K
k=1 ℙ(X = xk) = 1.
From Problem 5.2.3.7 (page 308), the compound Poisson process Mt and the Poisson
process Nt can be expressed as
Mt =
K
∑
k=1
xkN(k)
t ,
Nt =
K
∑
k=1
N(k)
t
where N(k)
t
∼Poisson (𝜆tp (xk
)) such that N(i)
t
⟂⟂N(j)
t , i ≠j. By setting 𝜂1, 𝜂2, . . . , 𝜂K > 0,
we consider the Radon–Nikod´ym derivative process
Zt =
K
∏
k=1
Z(k)
t
where Z(k)
t
= e(𝜆k−𝜂k)t
( 𝜂k
𝜆k
)N(k)
t
and 𝜆k = 𝜆p(xk) for k = 1, 2, . . . , K. Thus, under the
ℚmeasure, Mt is a compound Poisson process with intensity 𝜂= ∑K
k=1 𝜂k > 0 and
X1, X2, . . . is a sequence of independent and identically distributed random variables
with common probability mass function
ℚ(X = xk) = q(xk) =
𝜂k
𝜂1 + 𝜂2 + . . . + 𝜂K
.
Therefore,
Zt =
K
∏
k=1
Z(k)
t
=
K
∏
k=1
e(𝜆k−𝜂k)
( 𝜂k
𝜆k
)N(k)
t

5.2.3
Girsanov’s Theorem for Jump Processes
311
= e
∑K
k=1(𝜆k−𝜂k)t
K
∏
k=1
( 𝜂k
𝜆k
)N(k)
t
= e(𝜆−𝜂)t
K
∏
k=1
(𝜂q(xk)
𝜆p(xk)
)N(k)
t
= e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi).
By analogy with the discrete case, we can deduce that when Xi, i = 1, 2, . . . are continuous
random variables the Radon–Nikod´ym derivative process can also be written as
Zt = e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi),
0 ≤t ≤T
where f ℙ(X) and f ℚ(X) are the probability density functions of ℙand ℚmeasures, respec-
tively, and ℚis absolutely continuous with respect to ℙon ℱ.
◽
9. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T and let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent of
Nt. Under the ℙmeasure, each Xi, i = 1, 2, . . . has a probability mass (density) function
ℙ(Xi) (f ℙ(Xi)). From the definition of the compound Poisson process
Mt =
Nt
∑
i=1
Xi,
0 ≤t ≤T
we let 𝜂> 0 and consider the Radon–Nikod´ym derivative process
Zt =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the ℚmeasure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let ℚ
be absolutely continuous with respect to ℙon ℱ. By changing the measure ℙto measure
ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
show that under the ℚmeasure for 0 ≤t ≤T, Mt is a compound Poisson process
with intensity 𝜂> 0 and Xi, i = 1, 2, . . . is a sequence of independent and identically

312
5.2.3
Girsanov’s Theorem for Jump Processes
distributed random variables with probability mass (density) functions ℚ(Xi) (f ℚ(Xi)),
i = 1, 2, . . .
Solution: To solve this problem we need to show
𝔼ℚ(euMt) = e𝜂t(𝜑ℚ
X (u)−1),
u ∈ℝ
which is the moment generating function of a compound Poisson process with intensity 𝜂
such that
𝜑ℚ
X (u) = 𝔼ℚ(euX) =
⎧
⎪
⎪
⎨
⎪
⎪⎩
Nt
∑
i=1
euXiℚ(Xi)
if X is discrete
∫
∞
−∞
euxf ℚ(x) dx
if X is continuous.
Without loss of generality we consider only the continuous case where, by definition,
𝔼ℚ(euMt) = 𝔼ℙ
(
euMt dℚ
dℙ
||||ℱt
)
= 𝔼ℙ(euMtZt
)
= 𝔼ℙ
(
eu ∑Nt
i=1 Xie(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
)
= e(𝜆−𝜂)t𝔼ℙ
[
𝔼ℙ
(
eu ∑Nt
i=1 Xi
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
||||||
Nt
)]
= e(𝜆−𝜂)t
∞
∑
n=0
𝔼ℙ
(
eu ∑Nt
i=1 Xi
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
)
ℙ(Nt = n).
Since X1, X2, . . . are independent and identically distributed random variables and
because Nt ∼Poisson(𝜆t), we can express
𝔼ℚ(euMt) = e(𝜆−𝜂)t
∞
∑
n=0
𝔼ℙ
( Nt
∏
i=1
(𝜂
𝜆
)
euXi f ℚ(Xi)
f ℙ(Xi)
)
ℙ(Nt = n)
= e(𝜆−𝜂)t
∞
∑
n=0
[
𝔼ℙ
(𝜂
𝜆euX f ℚ(X)
f ℙ(X)
)]n e−𝜆t(𝜆t)n
n!
.
Because
𝔼ℙ
(𝜂
𝜆euX f ℚ(X)
f ℙ(X)
)
= ∫
∞
−∞
𝜂
𝜆eux f ℚ(x)
f ℙ(x) ⋅f ℙ(x) dx
= ∫
∞
−∞
𝜂
𝜆euxf ℚ(x) dx
= 𝜂
𝜆𝜑ℚ
X (u)

5.2.3
Girsanov’s Theorem for Jump Processes
313
where 𝜑ℚ
X (u) = ∫
∞
−∞
euxf ℚ(x) dx, then
𝔼ℚ(euMt) = e(𝜆−𝜂)t
∞
∑
n=0
(𝜂
𝜆𝜑ℚ
X (u)
)n e−𝜆t(𝜆t)n
n!
= e−𝜂t
∞
∑
n=0
(𝜂t𝜑ℚ
X (u))n
n!
= e𝜂t(𝜑ℚ
X (u)−1).
Based on the moment generating function we can deduce that under the ℚmeasure, Mt
is a compound Poisson process with intensity 𝜂> 0 and X1, X2, . . . are also indepen-
dent and identically distributed random variables with probability density function f ℚ(Xi),
i = 1, 2, . . .
◽
10. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 defined on the probability
space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T and let X1, X2, . . . be a sequence
of independent and identically distributed random variables which are also independent
of Nt. Under the ℙmeasure each Xi, i = 1, 2, . . . has a probability mass (density) function
ℙ(Xi) (f ℙ(Xi)). From the definition of the compound Poisson process
Mt =
Nt
∑
i=1
Xi,
0 ≤t ≤T
we let 𝜂> 0 and consider the Radon–Nikod´ym derivative process
Zt =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the ℚmeasure. For the case of continuous random variables Xi, i = 1, 2, . . . we also let
ℚbe absolutely continuous with respect to ℙon ℱ.
Show that 𝔼ℙ(Zt
) = 1 and Zt is a positive ℙmartingale for 0 ≤t ≤T.
Solution: Without loss of generality we consider only the case of continuous random
variables Xi, i = 1, 2, . . .
Let Zt = e(𝜆−𝜂)tGt, where Gt =
Nt∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi) is defined as a pure jump process, and let Zt−
denote the value of Zt before a jump event.

314
5.2.3
Girsanov’s Theorem for Jump Processes
From It¯o’s lemma,
dZt = 𝜕Zt
𝜕t dt + 𝜕Zt
𝜕Gt
dGt + 1
2
𝜕2Zt
𝜕G2
t
(dGt)2 + . . .
= (𝜆−𝜂)e(𝜆−𝜂)tGt dt + e(𝜆−𝜂)tdGt
= (𝜆−𝜂)Zt−+ e(𝜆−𝜂)tdGt.
Let Gt−be the value Gt just before a jump event and assume there occurs an instantaneous
jump at time t. Therefore,
dGt = Gt −Gt−
= 𝜂f ℚ(Xt)
𝜆f ℙ(Xt)Gt−−Gt−
=
(𝜂f ℚ(Xt)
𝜆f ℙ(Xt) −1
)
Gt−
where 𝜂f ℚ(Xt)
𝜆f ℙ(Xt) is the size of the jump variable if N jumps at time t.
By defining a new compound Poisson process
Ht =
Nt
∑
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
we let Ht−be the value of Ht just before a jump. At jump time t (which is also the jump
time of Mt and Gt), we have
dHt = Ht −Ht−= 𝜂f ℚ(Xt)
𝜆f ℙ(Xt).
Therefore, at jump times of N we can write
dGt
Gt−= dHt −1
and in general we can write
dGt
Gt−= dHt −dNt
where
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt
and
dHt = 𝜂f ℚ(Xt)
𝜆f ℙ(Xt)dNt.

5.2.3
Girsanov’s Theorem for Jump Processes
315
Thus,
dZt = (𝜆−𝜂)Zt−dt + e(𝜆−𝜂)t(dHt −dNt)Gt−
= (𝜆−𝜂)Zt−dt + Zt−dHt −Zt−dNt
or
dZt
Zt−= d(Ht −𝜂t) −d(Nt −𝜆t).
Since ̂Nt = Nt −𝜆t is a ℙ-martingale we now need to show that ̂Ht = Ht −𝜂t is also a
ℙ-martingale. By definition,
𝔼ℙ
[𝜂f ℚ(X)
𝜆f ℙ(X)
]
= 𝜂
𝜆∫
∞
−∞
f ℚ(x)
f ℙ(x) ⋅f ℙ(x) dx = 𝜂
𝜆
since ∫
∞
−∞
f ℚ(x) dx = 1. From Problem 5.2.1.14 (page 266), we can therefore deduce that
̂Ht = Ht −𝔼ℙ
[𝜂f ℚ(X)
𝜆f ℙ(X)
]
𝜆t = Ht −𝜂t
is a ℙ-martingale as well.
Substituting ̂Nt = Nt −𝜆t and ̂Ht = Ht −𝜂t into the stochastic differential equation, we
have
dZt
Zt−= d̂Ht −d̂Nt
and taking integrals, for t ≥0
∫
t
0
dZu = ∫
t
0
Zu−d̂Hu −∫
t
0
Zu−d̂Nu
Zt = Z0 + ∫
t
0
Zu−d̂Hu −∫
t
0
Zu−d̂Nu.
Taking expectations,
𝔼ℙ(Zt
) = 𝔼ℙ(Z0
) + 𝔼ℙ
[
∫
t
0
Zu−d̂Hu
]
−𝔼ℙ
[
∫
t
0
Zu−d̂Nu
]
= 1
since Z0 = 1, ̂Ht and ̂Nt are both ℙ-martingales.
To show that Zt is a positive ℙ-martingale, by taking integrals for s < t
∫
t
s
dZu = ∫
t
s
Zu−d̂Hu −∫
t
s
Zu−d̂Nu
Zt −Zs = ∫
t
s
Zu−d̂Hu −∫
t
s
Zu−d̂Nu

316
5.2.3
Girsanov’s Theorem for Jump Processes
and taking expectations under the filtration ℱs,
𝔼ℙ[Zt −Zs|ℱs
] = 𝔼ℙ
[
∫
t
s
Zu−d̂Hu
|||||
ℱs
]
−𝔼ℙ
[
∫
t
s
Zu−d̂Nu
|||||
ℱs
]
.
Because ̂Ht and ̂Nt are ℙ-martingales, therefore
𝔼ℙ[Zt|ℱs
] = Zs.
In addition, since Zt > 0 for 0 ≤t ≤T, then |Zt| = Zt and hence 𝔼ℙ(|Zt|) = 𝔼ℙ(Zt
) =
1 < ∞for 0 ≤t ≤T. Because Zt is also ℱt-adapted, we can conclude that Zt is a positive
ℙ-martingale for 0 ≤t ≤T.
◽
11. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 and {Wt ∶0 ≤t ≤T}
be a standard Wiener process defined on the probability space (Ω, ℱ, ℙ) with respect
to the filtration ℱt, 0 ≤t ≤T. Suppose 𝜃t is an adapted process, 0 ≤t ≤T and 𝜂> 0.
Let X1, X2, . . . be a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function ℙ(Xi) (f ℙ(Xi)) under
the ℙmeasure. Let X1, X2, . . . also be independent of Nt and Wt.
From the definition of the compound Poisson process
Mt =
Nt
∑
i=1
Xi,
0 ≤t ≤T
we consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the ℚmeasure and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2
u du)
< ∞. For the case of continuous random variables Xi,
i = 1, 2, . . . we also let ℚbe absolutely continuous with respect to ℙon ℱ.
Show that 𝔼ℙ(Zt
) = 1 and Zt is a positive ℙ-martingale for 0 ≤t ≤T.

5.2.3
Girsanov’s Theorem for Jump Processes
317
Solution: From Problem 4.2.2.1 (page 194) and Problem 5.2.3.10 (page 313), we have
shown that 𝔼ℙ(
Z(1)
t
)
= 1, 𝔼ℙ(
Z(2)
t
)
= 1, Z(1)
t
and Z(2)
t
are both ℙ-martingales. From It¯o’s
lemma,
d(Z(1)
t Z(2)
t ) = Z(1)
t−dZ(2)
t
+ Z(2)
t d + dZ(1)
t dZ(2)
t
where Zt−is the value of Z(1)
t
before a jump event. Since dZ(1)
t dZ(2)
t
= 0, by taking
integrals,
∫
t
0
d(Z(1)
u Z(2)
u ) = ∫
t
0
Z(1)
u−dZ(2)
u
+ ∫
t
0
Z(2)
u dZ(1)
u
Z(1)
t Z(2)
t
= Z(1)
0 Z(2)
0
+ ∫
t
0
Z(1)
u−dZ(2)
u
+ ∫
t
0
Z(2)
u dZ(1)
u .
Taking expectations under the ℙmeasure,
𝔼ℙ(
Z(1)
t Z(2)
t
)
= 𝔼ℙ(
Z(1)
0 Z(2)
0
)
= 1
since Z(1)
0
= 1, Z(2)
0
= 1 and both Z(1)
t
and Z(2)
t
are ℙ-martingales.
To show that Zt = Z(1)
t
⋅Z(2)
t
is a positive ℙ-martingale, by taking integrals for s < t of
d(Z(1)
t Z(2)
t ) = Z(1)
t−dZ(2)
t
+ Z(2)
t dZ(1)
t
we have
Z(1)
t Z(2)
t
= Z(1)
s Z(2)
s
+ ∫
t
s
Z(1)
u−dZ(2)
u
+ ∫
t
s
Z(2)
u dZ(1)
u
and taking expectations with respect to the filtration ℱs, s < t
𝔼ℙ(
Z(1)
t Z(2)
t
||| ℱs
)
= Z(1)
s Z(2)
s
since 𝔼ℙ
(
∫
t
s
Z(1)
u−dZ(2)
u
|||||
ℱs
)
= 𝔼ℙ
(
∫
t
s
Z(2)
u dZ(1)
u
|||||
ℱs
)
= 0.
In addition, since Z(1)
t
> 0 and Z(2)
t
> 0 for all 0 ≤t ≤T, therefore |||Z(1)
t Z(2)
t
||| = Z(1)
t Z(2)
t
and
hence 𝔼ℙ(|Zt|) = 𝔼ℙ(Zt
) = 1 < ∞for 0 ≤t ≤T. Finally, because Zt is also ℱt-adapted,
we can conclude that Zt is a positive ℙ-martingale for 0 ≤t ≤T.
◽
12. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity 𝜆> 0 and {Wt ∶0 ≤t ≤T}
be a standard Wiener process defined on the probability space (Ω, ℱ, ℙ) with respect
to the filtration ℱt, 0 ≤t ≤T. Suppose 𝜃t is an adapted process, 0 ≤t ≤T and 𝜂> 0.
Let X1, X2, . . . be a sequence of independent and identically distributed random variables
where each Xi, i = 1, 2, . . . has a probability mass (density) function ℙ(Xi) (f ℙ(Xi)) under
the ℙmeasure. Let X1, X2, . . . also be independent of Nt and Wt.
From the definition of the compound Poisson process
Mt =
Nt
∑
i=1
Xi,
0 ≤t ≤T

318
5.2.3
Girsanov’s Theorem for Jump Processes
we consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
and
Z(2)
t
= e−∫t
0 𝜃udWu−1
2 ∫t
0 𝜃2
u du
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the ℚmeasure and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2
u du)
< ∞. For the case of continuous random variables Xi,
i = 1, 2, . . . we also let ℚbe absolutely continuous with respect to ℙon ℱ.
By changing the measure ℙto measure ℚsuch that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
show that under the ℚmeasure, the process Mt =
Nt∑
i=1
Xi is a compound Poisson process
with intensity 𝜂> 0 where Xi, i = 1, 2, . . . is a sequence of independent and identically
distributed random variables with probability mass (density) functions ℚ(Xi) (f ℚ(Xi)), i =
1, 2, . . . , the process ̃Wt = Wt + ∫
t
0
𝜃u du is a ℚ-standard Wiener process and Mt ⟂⟂̃Wt.
Solution: The first two results are given in Problem 5.2.3.9 (page 311) and Problem
4.2.2.10 (page 205).
To prove the final result we need to show that, under the ℚmeasure,
𝔼ℚ(
eu1Mt+u2 ̃Wt
)
= e𝜂t(𝜑ℚ
X (u1)−1) ⋅e
1
2 u2
2t
which is a joint product of independent moment generating functions of Mt and ̃Wt ∼
𝒩(0, t).
By definition,
𝔼ℚ(
eu1Mt+u2 ̃Wt
)
= 𝔼ℙ(
eu1Mt+u2 ̃WtZt
)
= 𝔼ℙ(
eu1MtZ(1)
t
⋅eu2 ̃WtZ(2)
t
)
and we let
Zt = Z
(1)
t
⋅Z
(2)
t
where
Z
(1)
t
= eu1MtZ(1)
t
and
Z
(2)
t
= eu2 ̃WtZ(2)
t .

5.2.3
Girsanov’s Theorem for Jump Processes
319
By setting Z
(1)
t−to denote the value of Z
(1)
t
before a jump event and using It¯o’s lemma, we
have
dZt = d(Z
(1)
t Z
(2)
t ) = Z
(1)
t−dZ
(2)
t
+ Z
(2)
t dZ
(1)
t
+ dZ
(1)
t dZ
(2)
t .
For dZ
(1)
t
we can expand, using Taylor’s theorem,
dZ
(1)
t
= 𝜕Z
(1)
t
𝜕Mt
dMt + 𝜕Z
(1)
t
𝜕Z(1)
t
dZ(1)
t
+ 1
2!
⎡
⎢
⎢⎣
𝜕2Z
(1)
t
𝜕M2
t
(dMt)2 + 2 𝜕2Z
(1)
t
𝜕Mt𝜕Z(1)
t
(dMtdZ(1)
t ) + 𝜕2Z
(1)
t
𝜕(Z(1)
t )2 (dZ(1)
t )2
⎤
⎥
⎥⎦
+ 1
3!
⎡
⎢
⎢⎣
𝜕3Z
(1)
t
𝜕M3
t
(dMt)3 + 3
𝜕3Z
(1)
t
𝜕M2
t 𝜕Z(1)
t
(dMt)2(dZ(1)
t )
+3
𝜕3Z
(1)
t
𝜕Mt𝜕(Z(1)
t )2 (dMt)(dZ(1)
t )2 + 𝜕3Z
(1)
t
𝜕(Z(1)
t )3 (dZ(1)
t )3
⎤
⎥
⎥⎦
+ . . .
Because dMt = XtdNt, dZ(1)
t
= Z(1)
t−(d̂Ht −d̂Nt) where
d̂Ht =
⎧
⎪
⎪
⎨
⎪
⎪⎩
𝜂ℚ(Xt)
𝜆ℙ(Xt)dNt −𝜂dt
if Xt is discrete, ℙ(Xt) > 0
𝜂f ℚ(Xt)
𝜆f ℙ(Xt)dNt −𝜂dt
if Xt is continuous
and d̂Nt = Nt −𝜆dt (see Problem 5.2.3.10, page 313), we can write
dZ
(1)
t
= (u1Xt)Z
(1)
t−dNt + Z
(1)
t−(d̂Ht −d̂Nt) + 1
2!
[
(u1Xt)2Z
(1)
t−dNt + 2u1XtZ
(1)
t−dNt(d̂Ht −d̂Nt)
]
+ 1
3!
[
(u1Xt)3Z
(1)
t−dNt + 3(u1Xt)2Z
(1)
t−dNt(d̂Ht −d̂Nt)
]
+ . . .
=
[
(u1Xt) + 1
2!(u1Xt)2 + 1
3!(u1Xt)3 + . . .
]
Z
(1)
t−dNt + Z
(1)
t−(d̂Ht −d̂Nt)
+
[
(u1Xt) + 1
2!(u1Xt)2 + 1
3!(u1Xt)3 + . . .
]
Z
(1)
t−dNt(d̂Ht −d̂Nt)
= (eu1Xt −1)Z
(1)
t−dNt + Z
(1)
t−(d̂Ht −d̂Nt) + (eu1Xt −1)Z
(1)
t−dNt(d̂Ht −d̂Nt).
Since
d̂Ht −d̂Nt =
⎧
⎪
⎪
⎨
⎪
⎪⎩
[𝜂ℚ(Xt)
𝜆ℙ(Xt) −1
]
dNt −(𝜂−𝜆) dt
if Xt is discrete, ℙ(Xt) > 0
[𝜂f ℚ(Xt)
𝜆f ℙ(Xt) −1
]
dNt −(𝜂−𝜆) dt
if Xt is continuous

320
5.2.3
Girsanov’s Theorem for Jump Processes
and
dNt(d̂Ht −d̂Nt) =
⎧
⎪
⎪
⎨
⎪
⎪⎩
[𝜂ℚ(Xt)
𝜆ℙ(Xt) −1
]
dNt
if Xt is discrete, ℙ(Xt) > 0
[𝜂f ℚ(Xt)
𝜆f ℙ(Xt) −1
]
dNt
if Xt is continuous
therefore
dZ
(1)
t
=
⎧
⎪
⎪
⎨
⎪
⎪⎩
𝜂ℚ(Xt)
𝜆ℙ(Xt)
(eu1Xt −1) Z
(1)
t−dNt + Z
(1)
t−
(
d̂Ht −d̂Nt
)
if Xt is discrete, ℙ(Xt) > 0
𝜂f ℚ(Xt)
𝜆f ℙ(Xt)
(eu1Xt −1) Z
(1)
t−dNt + Z
(1)
t−
(
d̂Ht −d̂Nt
)
if Xt is continuous.
For the case of dZ
(2)
t , by applying Taylor’s theorem
dZ
(2)
t
= 𝜕Z
(2)
t
𝜕̃Wt
d ̃Wt + 𝜕Z
(2)
t
𝜕Z(2)
t
dZ(2)
t
+ 1
2!
⎡
⎢
⎢⎣
𝜕2Z
(2)
t
𝜕( ̃Wt)2 (d ̃Wt)2 + 2 𝜕2Z
(2)
t
𝜕̃Wt𝜕Z(2)
t
(d ̃Wt)(dZ(2)
t ) + 𝜕2Z
(2)
t
𝜕(Z(2)
t )2 (dZ(2)
t )2
⎤
⎥
⎥⎦
+ . . .
and since d ̃Wt = dWt + 𝜃t dt and dZ(2)
t
= −𝜃tZ(2)
t dWt (see Problem 4.2.2.2, page 196), we
can write
dZ
(2)
t
= 1
2u2
2Z
(2)
t
dt + (u2 −𝜃t)Z
(2)
t dWt.
Since dNtdWt = 0 we have
Z
(1)
t dZ
(2)
t
= 1
2u2
2Zt dt + (u2 −𝜃t)ZtdWt,
Z
(2)
t dZ
(1)
t
=
⎧
⎪
⎪
⎨
⎪
⎪⎩
𝜂ℚ(Xt)
𝜆ℙ(Xt)
(eu1Xt −1) ZtdNt + Zt
(
d̂Ht −d̂Nt
)
if Xt is discrete, ℙ(Xt) > 0
𝜂f ℚ(Xt)
𝜆f ℙ(Xt)
(eu1Xt −1) ZtdNt + Zt
(
d̂Ht −d̂Nt
)
if Xt is continuous
and
dZ
(1)
t dZ
(2)
t
= 0.
Without loss of generality, we let Xt be a continuous random variable and by letting dNt =
d̂Nt + 𝜆dt we have
dZt = 1
2u2
2Zt dt + (u2 −𝜃t)ZtdWt + 𝜂f ℚ(Xt)
𝜆f ℙ(Xt)
(eu1Xt −1) ZtdNt + Zt
(
d̂Ht −d̂Nt
)

5.2.3
Girsanov’s Theorem for Jump Processes
321
=
[𝜂f ℚ(Xt)
f ℙ(Xt)
(eu1Xt −1) + 1
2u2
2
]
Zt dt + (u2 −𝜃t)ZtdWt
+ 𝜂f ℚ(Xt)
𝜆f ℙ(Xt)
(eu1Xt −1) Ztd̂Nt + Zt(d̂Ht −d̂Nt).
Taking integrals,
Zt = 1 + ∫
t
0
[𝜂f ℚ(Xs)
f ℙ(Xs)
(eu1Xs −1) + 1
2u2
2
]
Zs ds + ∫
t
0
(u2 −𝜃s)ZsdWs
+ ∫
t
0
𝜂f ℚ(Xs)
𝜆f ℙ(Xs)
(eu1Xs −1) Zsd̂Ns + ∫
t
0
+Zs
(
d̂Hs −d̂Ns
)
where Z0 = 1 and because the jump size variable Xt is independent of Nt and Wt, and ̂Nt,
̂Ht and Wt are ℙ-martingales, by taking expectations under the ℙmeasure we have
𝔼ℙ(
Zt
)
= 1 + ∫
t
0
{
𝔼ℙ
[𝜂f ℚ(Xs)
f ℙ(Xs)
(eu1Xs −1)]
+ 1
2u2
2
}
𝔼ℙ(
Zs
)
ds.
By differentiating the integrals with respect to t,
d
dt𝔼ℙ(
Zt
)
=
{
𝔼ℙ
[𝜂f ℚ(Xt)
f ℙ(Xt)
(eu1Xt −1)]
+ 1
2u2
2
}
𝔼ℙ(
Zt
)
=
{
𝜂[𝔼ℚ(eu1Xt) −1] + 1
2u2
2
}
𝔼ℙ(
Zt
)
or
dmt
dt −
[
𝜂(𝜑ℚ
X (u1) −1) + 1
2u2
2
]
mt = 0
where mt = 𝔼ℙ(
Zt
)
and 𝜑ℚ
X (u1) = 𝔼ℚ(eu1Xt).
By setting the integrating factor to be I = e−∫(𝜂(𝜑X(u1)−1)+ 1
2 u2
2) dt = e−(𝜂(𝜑X(u1)−1)+ 1
2 u2
2)t and
multiplying the differential equation with I, we have
d
dt
(
mte−𝜂t(𝜑X(u1)−1)−1
2 u2
2t)
= 0
or
e−𝜂t(𝜑X(u1)−1)−1
2 u2
2t𝔼ℙ(
Zt
)
= C
where C is a constant. Since 𝔼ℙ(
Z0
)
= 𝔼ℙ(
eu1M0Z(1)
0
⋅eu2 ̃W0Z(2)
0
)
= 1, therefore C = 1.
Thus, we finally obtain
𝔼ℚ(
eu1Mt+u2 ̃Wt
)
= 𝔼ℙ(
Zt
)
= e𝜂t(𝜑X(u1)−1)+ 1
2 u2
2t.
Since the joint moment generating function of
𝔼ℚ(
eu1Mt+u2 ̃Wt
)
= e𝜂t(𝜑X(u1)−1) ⋅e
1
2 u2
2t

322
5.2.4
Risk-Neutral Measure for Jump Processes
can be expressed as a product of the moment generating functions for Mt (which is a com-
pound Poisson process with intensity 𝜂) and ̃Wt ∼𝒩(0, t), respectively, we can deduce
that Mt and ̃Wt are independent.
N.B. The same conclusion can also be obtained for the case of treating Xt as a discrete
random variable.
◽
5.2.4
Risk-Neutral Measure for Jump Processes
1. Simple Jump Process. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶0 ≤t ≤T} be a
Poisson process with intensity 𝜆> 0 relative to the filtration ℱt, 0 ≤t ≤T. Suppose the
asset price St follows a simple jump process
dSt
St−= (J −1)dNt
where J is a constant jump amplitude if N jumps at time t and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Let r be the risk-free interest rate.
By considering the Radon–Nikod´ym derivative process, for 𝜂> 0
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt,
0 ≤t ≤T
show that by changing the measure ℙto the risk-neutral measure ℚ, the above stochastic
differential equation is
dSt
St−= (J −1)dNt
where Nt ∼Poisson(𝜂t), 𝜂= r(J −1)−1 > 0 provided J > 1.
Is the market arbitrage free and complete under the ℚmeasure?
Solution: Let 𝜂> 0 and from the Radon–Nikod´ym derivative process
Zt = e(𝜆−𝜂)t(𝜂
𝜆
)Nt.
By changing the measure ℙto the risk-neutral measure ℚon the filtration ℱs, 0 ≤s ≤t
then under ℚ, Nt ∼Poisson(𝜂t) and the discounted asset price e−rtSt is a ℚ-martingale.
By setting St−to denote the value of St before a jump event and by expanding d(e−rtSt), we
have
d(e−rtSt) = −re−rtSt dt + e−rtdSt
= −re−rtSt dt + e−rtSt(J −1)dNt
= e−rtSt(−r + 𝜂(J −1)) dt + e−rtSt(J −1)(dNt −𝜂dt).

5.2.4
Risk-Neutral Measure for Jump Processes
323
Since the compensated Poisson process Nt −𝜂t is a ℚ-martingale (see Problem 5.2.1.9,
page 262), then, in order for e−rtSt to be a ℚ-martingale, we can set
𝜂= r(J −1)−1.
Since 𝜂> 0 we require J > 1. Thus, under the risk-neutral measure ℚ,
dSt
St−= (J −1)dNt
where Nt ∼Poisson(𝜂t).
The market is arbitrage free since we can construct a risk-neutral measure ℚon the filtration
ℱs, 0 ≤s ≤t. Since ℚis unique, the market is also complete.
◽
2. Pure Jump Process. Let (Ω, ℱ, ℙ) be a probability space and let {Nt ∶0 ≤t ≤T} be a
Poisson process with intensity 𝜆> 0 relative to the filtration ℱt, 0 ≤t ≤T. Suppose St
follows a pure jump process
dSt
St−= (Jt −1)dNt
where Jt is the jump variable with mean 𝔼ℙ(Jt) = J if N jumps at time t and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Let r be the risk-free interest rate. Assume that Jt is independent of Nt and consider the
Radon–Nikod´ym derivative process
Zt =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where 𝜂> 0, Xi = Ji −1, ℙ(X) (f ℙ(X)) and ℚ(X) (f ℚ(X)) are the probability mass (density)
functions of ℙand ℚ, respectively.
Show, by changing the measure ℙto the risk-neutral measure ℚ, the above SDE can be
written as
dSt
St−= dMt
where Mt = ∑Nt
i=1(Ji −1) is a compound Poisson process with Ji, i = 1, 2, . . . a sequence of
independent and identically distributed jump amplitude random variables having intensity
𝜂=
r
𝔼ℚ(Jt) −1 > 0
provided 𝔼ℚ(Jt) > 1.

324
5.2.4
Risk-Neutral Measure for Jump Processes
Is the market arbitrage free and complete under the ℚmeasure?
By assuming that the jump amplitude remains unchanged with the change of measure, find
the corresponding SDE under the equivalent martingale risk-neutral measure, ℚM.
Is the market arbitrage free and complete under the ℚM measure?
Solution: From Problem 5.2.2.1 (page 281) we can express the pure jump process as
dSt
St−= dMt
where Mt = ∑Nt
i=1(Ji −1) is a compound Poisson process with intensity 𝜆> 0.
Under the risk-neutral measure ℚon the filtration ℱs, 0 ≤s ≤t we let 𝜂> 0 and by con-
sidering the Radon–Nikod´ym derivative process
Zt =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
where Xi = Ji −1, ℙ(X) (f ℙ(X)) and ℚ(X) (f ℚ(X)) are the probability mass (density) func-
tions of ℙand, ℚrespectively, then by changing the measure ℙto the risk-neutral measure
ℚwe have that, under ℚ, Mt = ∑Nt
i=1(Ji −1) is a compound Poisson process with intensity
𝜂> 0 and the discounted asset price e−rtSt is a ℚ-martingale.
By letting St−= St to denote the value of St before a jump event and expanding d(e−rtSt),
we have
d(e−rtSt) = −re−rtSt dt + e−rtdSt
= −re−rtSt dt + e−rtStdMt
= e−rtSt
(−r + 𝜂𝔼ℚ(Jt −1)) dt + e−rtSt
(dMt −𝜂𝔼ℚ(Jt −1) dt) .
Given that the compensated compound Poisson process Mt −𝜂𝔼ℚ(Jt −1)t is a
ℚ-martingale (see Problem 5.2.1.14, page 266), and in order for e−rtSt to be a ℚ-martingale,
we can set
𝜂=
r
𝔼ℚ(Jt) −1
provided 𝔼ℚ(Jt) > 1.
Thus, under the risk-neutral measure ℚwe have
dSt
St−= (Jt −1)dNt
where Nt ∼Poisson(𝜂t), 𝜂= r(𝔼ℚ(Jt −1))−1 > 0.
The market is arbitrage free since we can construct a risk-neutral measure ℚon the filtration
ℱs, 0 ≤s ≤t. However, the market model is incomplete as ℚis not unique given that we
can vary the jump distribution Jt.

5.2.4
Risk-Neutral Measure for Jump Processes
325
By assuming that the distribution of the jump amplitude Jt remains unchanged under the
change of measure, there exists an equivalent martingale risk-neutral measure ℚM ∼ℚ
such that e−rtSt is a ℚM-martingale. Thus, under the equivalent martingale risk-neutral
measure ℚM,
𝔼ℚM (Jt −1) = 𝔼ℙ(Jt −1) = J −1
and the corresponding SDE is
dSt
St−= (Jt −1)dNt
where Nt ∼Poisson(𝜂t), 𝜂= r(J −1)−1 provided J > 1.
The market model is arbitrage free since we can construct an equivalent martingale
risk-neutral measure ℚM on the filtration ℱs, 0 ≤s ≤t. However, the market is incomplete
as ℚM is not unique given that we can still vary the jump size distribution Jt and hence J.
◽
3. Simple Jump Diffusion Process. Let {Nt ∶0 ≤t ≤T} be a Poisson process with intensity
𝜆> 0 and {Wt ∶0 ≤t ≤T} be a standard Wiener process defined on the probability space
(Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Suppose the stock price St follows a
jump diffusion process
dSt
St−= (𝜇−D) dt + 𝜎dWt + (J −1)dNt
where Wt ⟂⟂Nt, J is a constant jump amplitude if N jumps at time t and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
The constant parameters 𝜇, D and 𝜎are the drift, continuous dividend yield and volatil-
ity, respectively. In addition, let Bt be the risk-free asset having the following differential
equation:
dBt = rBt dt
where r is the risk-free interest rate.
By considering the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
= e(𝜆−𝜂)t(𝜂
𝜆
)Nt
and
Z(2)
t
= e−∫t
0 𝜃dWu−1
2 ∫t
0 𝜃2 du
where 𝜃∈ℝ, 𝜂> 0 and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2 du)
< ∞show that, under the risk-neutral measure
ℚ, the SDE can be written as
dSt
St−= (r −D −𝜂(J −1)) dt + 𝜎d ̃Wt + (J −1)dNt

326
5.2.4
Risk-Neutral Measure for Jump Processes
where
̃Wt = Wt +
(𝜇−r + 𝜂(J −1)
𝜎
)
t
is
the
ℚ-standard
Wiener
process
and
Nt ∼Poisson(𝜂t).
Is the market arbitrage free and complete under the ℚmeasure?
By assuming the jump component is uncorrelated with the market (i.e., the jump intensity
remains unchanged with the change of measure), find the corresponding SDE under the
equivalent martingale risk-neutral measure ℚM.
Is the market arbitrage free and complete under the ℚM measure?
Solution: Let 𝜃∈ℝand 𝜂> 0 and consider the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
= e(𝜆−𝜂)t(𝜂
𝜆
)Nt
and
Z(2)
t
= e−∫t
0 𝜃dWu−1
2 ∫t
0 𝜃2 du
where 𝔼ℙ(
e
1
2 ∫T
0 𝜃2 du)
< ∞. By changing the measure ℙto the risk-neutral measure ℚon
the filtration ℱs, 0 ≤s ≤t such that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then from Problem 5.2.3.5 (page 303), under the ℚmeasure, the Poisson process Nt ∼
Poisson(𝜂t) has intensity 𝜂> 0, the process ̃Wt = Wt + ∫
t
0
𝜃du is a ℚ-standard Wiener
process and Nt ⟂⟂̃Wt.
In the presence of dividends, the strategy of holding a single stock is no longer self-financing
as it pays out dividends at a rate DSt dt. By letting St−denote the value of St before a jump
event, at time t we let the portfolio Πt be valued as
Πt = 𝜙tSt + 𝜓tBt
where 𝜙t and 𝜓t are the units invested in St and the risk-free asset Bt, respectively. Given
that the holder will receive DSt dt for every stock held, then
dΠt = 𝜙t(dSt + DSt dt) + 𝜓trBt dt
= 𝜙t
[𝜇St dt + 𝜎StdWt + (J −1)StdNt
] + 𝜓trBt dt
= rΠt dt + 𝜙tSt
[(𝜇−r) dt + 𝜎dWt + (J −1)dNt
] .
By substituting Wt = ̃Wt −∫
t
0
𝜃du and taking note that the compensated Poisson process
Nt −𝜂t is a ℚ-martingale, we have
dΠt = rΠt dt + 𝜙tSt
[
(𝜇−r −𝜎𝜃+ 𝜂(J −1)) dt + 𝜎d ̃Wt + (J −1)
(dNt −𝜂dt)]
.

5.2.4
Risk-Neutral Measure for Jump Processes
327
Since both ̃Wt and Nt −𝜂t are ℚ-martingales and in order for the discounted portfolio e−rtΠt
to be a ℚmartingale
d(e−rtΠt) = −re−rtΠt dt + e−rtdΠt
= e−rt𝜙tSt
[
(𝜇−r −𝜎𝜃+ 𝜂(J −1)) dt + 𝜎d ̃Wt + (J −1)
(dNt −𝜂dt)]
we set
𝜃= 𝜇−r + 𝜂(J −1)
𝜎
.
Therefore, the jump diffusion process under ℚis
dSt
St−= (𝜇−D) dt + 𝜎
(
d ̃Wt −𝜃dt
)
+ (J −1)dNt
= (r −D −𝜂(J −1)) dt + 𝜎d ̃Wt + (J −1)dNt
where
̃Wt = Wt −
(𝜇−r + 𝜂(J −1)
𝜎
)
t is a ℚ-standard Wiener process and Nt ∼
Poisson(𝜂t).
The market is arbitrage free since we can construct a risk-neutral measure ℚon the filtration
ℱs, 0 ≤s ≤t. The market model is incomplete as ℚis not unique, since there are more
degrees of freedom in selecting J and 𝜂.
Under the assumption that the jump component is uncorrelated with the market, there exists
an equivalent martingale risk-neutral measure ℚM ∼ℚsuch that e−rtΠt is a ℚM-martingale.
Therefore, under ℚM,
Nt ∼Poisson(𝜆t)
and the SDE becomes
dSt
St−= (r −D −𝜆(J −1)) dt + 𝜎d ̃Wt + (J −1)dNt
where
̃Wt = Wt −
(𝜇−r + 𝜆(J −1)
𝜎
)
t
is
a
ℚM-standard
Wiener
process
and
Nt ∼Poisson(𝜆t).
The market is arbitrage free since we can construct an equivalent martingale risk-neutral
measure ℚM on the filtration ℱs, 0 ≤s ≤t. However, the market is not complete as ℚM is
not unique, since we can still vary the jump amplitude J.
◽
4. General Jump Diffusion Process (Merton’s Model). Let {Nt ∶0 ≤t ≤T} be a Poisson pro-
cess with intensity 𝜆> 0 and {Wt ∶0 ≤t ≤T} be a standard Wiener process defined on the
probability space (Ω, ℱ, ℙ) with respect to the filtration ℱt, 0 ≤t ≤T. Suppose the asset
price St follows a jump diffusion process
dSt
St−= (𝜇−D) dt + 𝜎dWt + (Jt −1)dNt

328
5.2.4
Risk-Neutral Measure for Jump Processes
where Jt is the jump variable with mean 𝔼ℙ(Jt) = J if N jumps at time t and
dNt =
{
1
with probability 𝜆dt
0
with probability 1 −𝜆dt.
Assume that Wt, Nt and Jt are mutually independent. The constant parameters 𝜇, D and 𝜎
are the drift, continuous dividend yield and volatility, respectively. In addition, let Bt be the
risk-free asset having the following differential equation:
dBt = rBt dt
where r is the risk-free interest rate.
By considering the Radon–Nikod´ym derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
and
Z(2)
t
= e−∫t
0 𝜃dWu−1
2 ∫t
0 𝜃2 du
where 𝜃∈ℝ, 𝜂> 0, ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi = Ji −1,
i = 1, 2, . . . under the ℚmeasure and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2 du)
< ∞show that, under risk-neutral
measure ℚ, the above SDE can be written as
dSt
St−= (r −D −𝜂𝔼ℚ(Jt −1)) dt + 𝜎d ̃Wt + (Jt −1)dNt
where ̃Wt = Wt −
(𝜇−r + 𝜂𝔼ℚ(Jt −1)
𝜎
)
t is the ℚ-standard Wiener process and Nt ∼
Poisson(𝜂t), 𝜂> 0.
Is the market model arbitrage free and complete under the ℚmeasure?
By assuming the jump component is uncorrelated with the market (i.e., the jump inten-
sity and jump size distribution remain unchanged with the change of measure), find the
corresponding SDE under the equivalent martingale risk-neutral measure ℚM.
Is the market model arbitrage free and complete under the ℚM measure?
Solution: From Problem 5.2.2.1 (page 281) we can express the jump diffusion process as
dSt
St−= (𝜇−D) dt + 𝜎dWt + dMt

5.2.4
Risk-Neutral Measure for Jump Processes
329
where Mt = ∑Nt
i=1(Ji −1) is a compound Poisson process with intensity 𝜆> 0 such that
the jump size (or amplitude) Ji, i = 1, 2, . . . is a sequence of independent and identically
distributed random variables.
Let 𝜃∈ℝ, 𝜂> 0 and Xi = Ji −1, i = 1, 2, . . . be a sequence of independent and identically
distributed random variables where each Xi, i = 1, 2, . . . has a probability mass (density)
function ℙ(Xi) > 0 (f ℙ(Xi) > 0) under the ℙmeasure. We consider the Radon–Nikod´ym
derivative process
Zt = Z(1)
t
⋅Z(2)
t
such that
Z(1)
t
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂ℚ(Xi)
𝜆ℙ(Xi)
if Xi is discrete, ℙ(Xi) > 0
e(𝜆−𝜂)t
Nt
∏
i=1
𝜂f ℚ(Xi)
𝜆f ℙ(Xi)
if Xi is continuous
and
Z(2)
t
= e−∫t
0 𝜃dWu−1
2 ∫t
0 𝜃2 du
where ℚ(Xi) (f ℚ(Xi)) is the probability mass (density) function of Xi, i = 1, 2, . . . under
the ℚmeasure and 𝔼ℙ(
e
1
2 ∫T
0 𝜃2 du)
< ∞. By changing the measure ℙto the risk-neutral
measure ℚon the filtration ℱs, 0 ≤s ≤t such that
𝔼ℙ
(
dℚ
dℙ
||||
ℱt
)
= dℚ
dℙ
||||ℱt
= Zt
then from Problem 5.2.3.12 (page 317), under ℚ, the compound Poisson process Mt =
∑Nt
i=1(Ji −1) has intensity 𝜂> 0, the process ̃Wt = Wt + ∫
t
0
𝜃du is a ℚ-standard Wiener
process and Mt ⟂⟂̃Wt.
In the presence of dividends, the simple strategy of holding a single risky asset is no longer
self-financing as the asset pays out dividends at a rate DSt dt. By letting St−denote the value
of St before a jump event at time t, we let the portfolio Πt be valued as
Πt = 𝜙tSt + 𝜓tBt
where 𝜙t and 𝜓t are the units invested in St and the risk-free asset Bd
t , respectively. Given
that the holder will receive DSt dt for every risky asset held, then
dΠt = 𝜙t(dSt + DSt dt) + 𝜓trBt dt
= 𝜙t
[𝜇St dt + 𝜎StdWt + dMt
] + 𝜓trBt dt
= rΠt dt + 𝜙tSt
[(𝜇−r) dt + 𝜎dWt + dMt
] .

330
5.2.4
Risk-Neutral Measure for Jump Processes
By substituting Wt = ̃Wt −∫
t
0
𝜃du and taking note that the compensated compound Pois-
son process Mt −𝜂𝔼ℚ(Jt −1)t is a ℚ-martingale, we have
dΠt = rΠt dt + 𝜙tSt
[(𝜇−r −𝜎𝜃+ 𝜂𝔼ℚ(Jt −1)) dt + 𝜎d ̃Wt + dMt −𝜂𝔼ℚ(Jt −1) dt
]
.
Since both ̃Wt and Mt −𝜂𝔼ℚ(Jt −1) t are ℚ-martingales and in order for the discounted
portfolio e−rtΠt to be a ℚ-martingale
d(e−rtΠt) = −re−rtΠt dt + e−rtdΠt
= e−rt𝜙tSt[(𝜇−r −𝜎𝜃+ 𝜂𝔼ℚ(Jt −1)) dt + 𝜎d ̃Wt + dMt −𝜂𝔼ℚ(Jt −1) dt]
we set
𝜃= 𝜇−r + 𝜂𝔼ℚ(Jt −1)
𝜎
.
Therefore, the jump diffusion process under ℚis
dSt
St−= (𝜇−D) dt + 𝜎
(
d ̃Wt −𝜃dt
)
+ (Jt −1)dNt
= (r −D −𝜂𝔼ℚ(Jt −1)) dt + 𝜎d ̃Wt + (Jt −1)dNt
where ̃Wt = Wt −
(𝜇−r + 𝜂𝔼ℚ(Jt −1)
𝜎
)
t and Nt ∼Poisson(𝜂t).
The market is arbitrage free since we can construct a risk-neutral measure ℚon the filtration
ℱs, 0 ≤s ≤t. However, the market model is incomplete as ℚis not unique, given that there
are more degrees of freedom in selecting 𝜂and Jt.
Under the assumption that the jump component is uncorrelated with the market, there exists
an equivalent martingale risk-neutral measure ℚM ∼ℚsuch that e−rtΠt is a ℚM-martingale
and hence
𝔼ℚM(Jt −1) = 𝔼ℙ(Jt −1) = J −1
and
Nt ∼Poisson(𝜆t).
Thus, under ℚM the jump diffusion process becomes
dSt
St−= (r −D −𝜆(J −1)) dt + 𝜎d ̃Wt + (Jt −1)dNt
where ̃Wt = Wt −
(
𝜇−r + 𝜆(J −1)
𝜎
)
t is a ℚM-standard Wiener process and Nt ∼
Poisson(𝜆t).
The market is arbitrage free since we can construct an equivalent martingale risk-neutral
measure ℚM on the filtration ℱs, 0 ≤s ≤t. However, the market is incomplete as ℚM is
not unique, since we can still vary the jump size distribution Jt.
◽

Appendix A
Mathematics Formulae
Indices
xaxb = xa+b,
xa
xb = xa−b,
(xa)b = (xb)a = xab
x−a = 1
xa ,
(
x
y
)a
= xa
ya ,
x0 = 1.
Surds
x
1
a =
a√
x,
a√xy =
a√
x a√y,
a√
x∕y =
a√
x
a√y
(
a√
x
)a
= x,
a√
b√
x =
ab√
x,
(
a√
x
)b
= x
b
a .
Exponential and Natural Logarithm
exey = ex+y,
(ex)y = (ey)x = exy,
e0 = 1
log (xy) = log x + log y,
log
(
x
y
)
= log x −log y,
log xy = y log x
log ex = x,
elog x = x,
ea log x = xa.
Quadratic Equation
For constants a, b and c, the roots of a quadratic equation ax2 + bx + c = 0 are
x = −b ±
√
b2 −4ac
2a
.
Binomial Formula
(
n
k
)
=
n!
k!(n −k)!,
(
n
k
)
+
(
n
k + 1
)
=
(
n + 1
k + 1
)
(x + y)n =
n
∑
k=0
(
n
k
)
xn−kyk =
n
∑
k=0
n!
k!(n −k)!xn−kyk.

332
Appendix A
Series
Arithmetic: For initial term a and common difference d, the n-th term is
Tn = a + (n −1)d
and the sum of n terms is
Sn = 1
2n[2a + (n −1)d].
Geometric: For initial term a and common ratio r, the n-th term is
Tn = arn−1
the sum of n terms is
Sn = a(1 −rn)
1 −r
,
and the sum of infinite terms is
lim
n→∞Sn =
a
1 −r,
|r| < 1.
Summation
For n ∈ℤ+,
n
∑
k=1
k = 1
2n(n + 1),
n
∑
k=1
k2 = 1
6n(n + 1)(2n + 1),
n
∑
k=1
k3 =
[1
2n(n + 1)
]2
.
Let a1, a2, . . . be a sequence of numbers:
• If ∑an < ∞⇒lim
n→∞an = 0.
• If lim
n→∞an ≠0 ⇒∑an = ∞.
Trigonometric Functions
sin(−x) = −sin x,
cos(−x) = cos x,
tan x = sin x
cos x
csc x =
1
sin x,
sec x =
1
cos x,
cot x =
1
tan x
cos2x + sin2x = 1,
tan2x + 1 = sec2x,
cot2x + 1 = csc2x
sin(x ± y) = sin x cos y ± cos x sin y
cos(x ± y) = cos x cos y ∓sin x sin y
tan(x ± y) = tan x ± tan y
1 ∓tan x tan y.

Appendix A
333
Hyperbolic Functions
sinh x = ex −e−x
2
,
cosh x = ex + e−x
2
,
tanh x = sinh x
cosh x = ex −e−x
ex + e−x
csch x =
1
sinh x,
sech x =
1
cosh x,
coth x =
1
tanh x
sinh(−x) = −sinh x,
cosh(−x) = cosh x,
tanh(−x) = −tanh x
cosh2x −sinh2x = 1,
coth2x −1 = csch2x,
1 −tanh2x = sech2x
sinh(x ± y) = sinh x cosh y ± cosh x sinh y
cosh(x ± y) = cosh x cosh y ± sinh x sinh y
tanh(x ± y) = tanh x ± tanh y
1 ± tanh x tanh y.
Complex Numbers
Let 𝑤= u + i𝑣and z = x + iy where u, 𝑣, x, y ∈ℝ, i =
√
−1 and i2 = −1 then
𝑤± z = (u ± x) + (𝑣± y)i,
𝑤z = (ux −𝑣y) + (𝑣x + uy)i,
𝑤
z =
(ux + 𝑣y
x2 + y2
)
+
(𝑣x −uy
x2 + y2
)
i
z = x −iy,
z = z,
𝑤+ z = 𝑤+ z,
𝑤z = 𝑤⋅z,
(
𝑤
z
)
= 𝑤
z .
De Moivre’s Formula: Let z = x + iy where x, y ∈ℝand we can write
z = r(cos 𝜃+ i sin 𝜃),
r =
√
x2 + y2,
𝜃= tan−1 (y
x
)
.
For n ∈ℤ
[r(cos 𝜃+ i sin 𝜃)]n = rn[cos(n𝜃) + i sin(n𝜃)].
Euler’s Formula: For 𝜃∈ℝ
ei𝜃= cos 𝜃+ i sin 𝜃.
Derivatives
If f(x) and g(x) are differentiable functions of x and a and b are constants
Sum Rule:
d
dx(af(x) + bg(x)) = af ′(x) + bg′(x)
Product/Chain Rule:
d
dx(f(x)g(x)) = f(x)g′(x) + f ′(x)g(x)

334
Appendix A
Quotient Rule:
d
dx
( f(x)
g(x)
)
= f ′(x)g(x) −f(x)g′(x)
g(x)2
,
g(x) ≠0
where d
dxf(x) = f ′(x) and d
dxg(x) = g′(x).
If f(z) is a differentiable function of z and z = z(x) is a differentiable function of x, then
d
dxf(z(x)) = f ′(z(x))z′(x).
If x = x(s), y = y(s) and F(s) = f(x(s), y(s)), then
d
dsF(s) = 𝜕f
𝜕x ⋅𝜕x
𝜕s + 𝜕f
𝜕y ⋅𝜕y
𝜕s.
If x = x(u, 𝑣), y = y(u, 𝑣) and F(u, 𝑣) = f(x(u, 𝑣), y(u, 𝑣)), then
𝜕F
𝜕u = 𝜕f
𝜕x ⋅𝜕x
𝜕u + 𝜕f
𝜕y ⋅𝜕y
𝜕u,
𝜕F
𝜕𝑣= 𝜕f
𝜕x ⋅𝜕x
𝜕𝑣+ 𝜕f
𝜕y ⋅𝜕y
𝜕𝑣.
Standard Differentiations
If f(x) and g(x) are differentiable functions of x and a and b are constants
d
dxa = 0,
d
dx[f(x)]n = n[f(x)]n−1f ′(x)
d
dxe f(x) = f ′(x)e f(x),
d
dx log f(x) = f ′(x)
f(x) ,
d
dxa f(x) = f ′(x)a f(x) log a
d
dx sin(ax) = a cos x,
d
dx cos(ax) = −a sin(ax),
d
dx tan(ax) = asec2x
d
dx sinh(ax) = a cosh(ax),
d
dx cosh(ax) = a sinh(ax),
d
dx tanh(ax) = asech2(ax)
where f ′(x) = d
dxf(x).
Taylor Series
If f(x) is an analytic function of x, then for small h
f(x0 + h) = f(x0) + f ′(x0)h + 1
2!f ′′(x0)h2 + 1
3!f ′′′(x0)h3 + . . .

Appendix A
335
If f(x, y) is an analytic function of x and y, then for small Δx, Δy
f(x0 + Δx, y0 + Δy) = f(x0, y0) + 𝜕f(x0, y0)
𝜕x
Δx + 𝜕f(x0, y0)
𝜕y
Δy
+ 1
2!
[𝜕2f(x0, y0)
𝜕x2
(Δx)2 + 2𝜕2f(x0, y0)
𝜕x𝜕y
ΔxΔy + 𝜕2f(x0, y0)
𝜕y2
(Δy)2
]
+ 1
3!
[𝜕3f(x0, y0)
𝜕x3
(Δx)3 + 3𝜕3f(x0, y0)
𝜕x2𝜕y
(Δx)2Δy
+3𝜕3f(x0, y0)
𝜕x𝜕y2
Δx(Δy)2 + 𝜕3f(x0, y0)
𝜕y3
(Δy)3
]
+ . . .
Maclaurin Series
Taylor series expansion of a function about x0 = 0
1
1 + x = 1 −x + x2 −x3 + . . . ,
|x| < 1
1
1 −x = 1 + x + x2 + x3 + . . . ,
|x| < 1
ex = 1 + x + 1
2!x2 + 1
3!x3 + . . . ,
for all x
e−x = 1 −x + 1
2!x2 −1
3!x3 + . . . ,
for all x
log(1 + x) = x −1
2x2 + 1
3x3 −1
4x4 + . . . ,
x ∈(−1, 1]
log(1 −x) = −x −1
2x2 −1
3x3 −1
4x4 + . . . ,
|x| < 1
sin x = x −1
3!x3 + 1
5!x5 −1
7!x7 + . . . ,
for all x
cos x = 1 −1
2!x2 + 1
4!x4 −1
6!x6 + . . . ,
for all x
tan x = x + 1
3x3 + 2
15x5 + 17
315x7 + . . . ,
|x| < 𝜋
2
sinh x = x + 1
3!x3 + 1
5!x5 + 1
7!x7 + . . . ,
for all x
cosh x = 1 + 1
2!x2 + 1
4!x4 + 1
6!x6 + . . . ,
for all x
tanh x = x −1
3x3 + 2
15x5 −17
315x7 + . . . ,
|x| < 𝜋
2 .
Landau Symbols and Asymptotics
Let f(x) and g(x) be two functions defined on some subsets of real numbers, then as x →x0

336
Appendix A
• f(x) = O(g(x)) if there exists a constant K > 0 and 𝛿> 0 such that |f(x)| ≤K|g(x)| for
|x −x0| < 𝛿.
• f(x) = o(g(x)) if lim
x→x0
f(x)
g(x) = 0.
• f(x) ∼g(x) if lim
x→x0
f(x)
g(x) = 1.
L’Hospital Rule
Let f and g be differentiable on a ∈ℝsuch that g′(x) ≠0 in an interval around a, except
possibly at a itself. Suppose that
lim
x→a f(x) = lim
x→a g(x) = 0
or
lim
x→a f(x) = lim
x→a g(x) = ±∞
then
lim
x→a
f(x)
g(x) = lim
x→a
f ′(x)
g′(x).
Indefinite Integrals
If F(x) is a differentiable function and f(x) is its derivative, then
∫f(x) dx = F(x) + c
where F
′(x) = d
dxF(x) = f(x) and c is an arbitrary constant.
If f(x) is a continuous function then
d
dx ∫f(x) dx = f(x).
Standard Indefinite Integrals
If f(x) is a differentiable function of x and a and b are constants
∫a dx = ax + c,
∫(ax + b)n dx = (ax + b)n+1
a(n + 1)
+ c,
n ≠−1
∫
f ′(x)
f(x) dx = log|f(x)| + c,
∫e f(x) dx =
1
f ′(x)e f(x) + c
∫log(ax) dx = x log(ax) −ax + c,
∫ax dx =
ax
log a + c
∫sin(ax) dx = −1
a cos(ax) + c,
∫cos(ax) dx = 1
a sin(ax) + c

Appendix A
337
∫sinh(ax) dx = 1
a cosh(ax) + c,
∫cosh(ax) dx = 1
a sinh(ax) + c
where c is an arbitrary constant.
Definite Integrals
If F(x) is a differentiable function and f(x) is its derivative and is continuous on a close interval
[a, b], then
∫
b
a
f(x) dx = F(b) −F(a)
where F
′(x) = d
dxF(x) = f(x).
If f(x) and g(x) are integrable functions then
∫
a
a
f(x) dx = 0,
∫
b
a
f(x) dx = −∫
a
b
f(x) dx
∫
b
a
[𝛼f(x) + 𝛽g(x)] dx = 𝛼∫
b
a
f(x) dx + 𝛽∫
b
a
g(x) dx,
𝛼, 𝛽are constants
∫
b
a
f(x) dx = ∫
c
a
f(x) dx + ∫
b
c
f(x) dx,
c ∈[a, b].
Derivatives of Definite Integrals
If f(t) is a continuous function of t and a(x) and b(x) are continuous functions of x
d
dx ∫
b(x)
a(x)
f(t) dt = f(b(x)) d
dxb(x) −f(a(x)) d
dxa(x)
d
dx ∫
b(x)
a(x)
dt = d
dxb(x) −d
dxa(x).
If g(x, t) is a differentiable function of two variables then
d
dx ∫
b(x)
a(x)
g(x, t) dt = g(x, b(x)) d
dxb(x) −g(x, a(x)) d
dxa(x) + ∫
b(x)
a(x)
𝜕g(x, t)
𝜕x
dt.
Integration by Parts
For definite integrals
∫
b
a
u(x)𝑣′(x) dx = u(x)𝑣(x)
|||||
b
a
−∫
b
a
𝑣(x)u′(x) dx.
where u′(x) = d
dxu(x) and 𝑣′(x) = d
dx𝑣(x).

338
Appendix A
Integration by Substitution
If f(x) is a continuous function of x and g′ is continuous on the closed interval [a, b] then
∫
g(a)
g(b)
f(x) dx = ∫
b
a
f(g(u))g′(u) du.
Gamma Function
The gamma function is defined as
Γ(z) = ∫
∞
0
tz−1e−t dt
such that
Γ(z + 1) = zΓ(z),
Γ
(1
2
)
=
√
𝜋,
Γ(n) = (n −1)! for n ∈ℕ.
Beta Function
The beta function is defined as
B(x, y) = ∫
1
0
tx−1(1 −t)y−1 dt
for x > 0, y > 0
such that
B(x, y) = B(y, x),
B(x, y) = Γ(x)Γ(y)
Γ(x + y) .
In addition
∫
u
0
tx−1(u −t)y−1 dt = ux+y−1B(x, y).
Convex Function
A set Ω in a vector space over ℝis called a convex set if for x, y ∈Ω, x ≠y and for any
𝜆∈(0, 1),
𝜆x + (1 −𝜆)y ∈Ω.
Let Ω be a convex set in a vector space over ℝ. A function f ∶Ω →ℝis called a convex
function if for x, y ∈Ω, x ≠y and for any 𝜆∈(0, 1),
f(𝜆x + (1 −𝜆)y) ≤𝜆f(x) + (1 −𝜆)f(y).
If the inequality is strict then f is strictly convex.
If f is convex and differentiable on ℝ, then
f(x) ≥f(y) + f ′(y)(x −y).
If f is a twice continuously differentiable function on ℝ, then f is convex if and only if f
′′ ≥0.
If f
′′ > 0 then f is strictly convex.

Appendix A
339
f is a (strictly) concave function if −f is a (strictly) convex function.
Dirac Delta Function
The Dirac delta function is defined as
𝛿(x) =
{
0 x ≠0
∞x = 0
and for a continuous function f(x) and a constant a, we have
∫
∞
−∞
𝛿(x) dx = 1,
∫
∞
−∞
f(x)𝛿(x) dx = f(0),
∫
∞
−∞
f(x)𝛿(x −a) dx = f(a).
Heaviside Step Function
The Heaviside step function, H(x) is defined as the integral of the Dirac delta function given as
H(x) = ∫
x
−∞
𝛿(s) ds =
{
0 x < 0
1 x > 0.
Fubini’s Theorem
Suppose f(x, y) is A × B measurable and if ∫A×B
|f(x, y)|d(x, y) < ∞then
∫A×B
f(x, y) d(x, y) = ∫A
(
∫B
f(x, y) dy
)
dx = ∫B
(
∫A
f(x, y) dx
)
dy.


Appendix B
Probability Theory Formulae
Probability Concepts
Let A and B be events of the sample space Ω with probabilities ℙ(A) ∈[0, 1] and ℙ(B) ∈
[0, 1], then
Complement:
ℙ(Ac) = 1 −ℙ(A).
Conditional:
ℙ(A|B) = ℙ(A ∩B)
ℙ(B)
.
Independence: The events A and B are independent if and only if
ℙ(A ∩B) = ℙ(A) ⋅ℙ(B).
Mutually Exclusive: The events A and B are mutually exclusive if and only if
ℙ(A ∩B) = 0.
Union:
ℙ(A ∪B) = ℙ(A) + ℙ(B) −ℙ(A ∩B).
Intersection:
ℙ(A ∩B) = ℙ(A|B)ℙ(B) = ℙ(B|A)ℙ(A).
Partition:
ℙ(A) = ℙ(A ∩B) + ℙ(A ∩Bc) = ℙ(A|B)ℙ(B) + ℙ(A|Bc)ℙ(Bc).
Bayes’ Rule
Let A and B be events of the sample space Ω with probabilities ℙ(A) ∈[0, 1] and ℙ(B) ∈
[0, 1], then
ℙ(A|B) = ℙ(B|A)ℙ(A)
ℙ(B)
.

342
Appendix B
Indicator Function
The indicator function 1IA of an event A of a sample space Ω is a function 1IA ∶Ω →ℝ
defined as
1IA(𝜔) =
{
1 if 𝜔∈A
0 if 𝜔∈Ac.
Properties: For events A and B of the sample space Ω
1IAc = 1 −1IA,
1IA∩B = 1IA1IB,
1IA∪B = 1IA + 1IB −1IA1IB
𝔼(1IA) = ℙ(A),
Var (1IA) = ℙ(A)ℙ(Ac),
Cov (1IA, 1IB) = ℙ(A ∩B) −ℙ(A)ℙ(B).
Discrete Random Variables
Univariate Case
Let X be a discrete random variable whose possible values are x = x1, x2, . . . , and let ℙ(X = x)
be the probability mass function.
Total Probability of All Possible Values:
∞
∑
k=1
ℙ(X = xk) = 1.
Cumulative Distribution Function:
ℙ(X ≤xn) =
n
∑
k=1
ℙ(X = xk).
Expectation:
𝔼(X) = 𝜇=
∞
∑
k=1
xkℙ(X = xk).
Variance:
Var (X) = 𝜎2
= 𝔼[(X −𝜇)2]
=
∞
∑
k=1
(xk −𝜇)2ℙ(X = xk)
=
∞
∑
k=1
x2
kℙ(X = xk) −𝜇2
= 𝔼(X2) −[𝔼(X)]2.
Moment Generating Function:
MX(t) = 𝔼(etX) =
∞
∑
k=1
etxkℙ(X = xk),
t ∈ℝ.

Appendix B
343
Characteristic Function:
𝜑X(t) = 𝔼(eitX) =
∞
∑
k=1
eitxkℙ(X = xk),
i =
√
−1 and t ∈ℝ.
Bivariate Case
Let X and Y be discrete random variables whose possible values are x = x1, x2, . . . and y =
y1, y2, . . . , respectively, and let ℙ(X = x, Y = y) be the joint probability mass function.
Total Probability of All Possible Values:
∞
∑
j=1
∞
∑
k=1
ℙ(X = xj, Y = yk) = 1.
Joint Cumulative Distribution Function:
ℙ(X ≤xn, Y ≤ym) =
n
∑
j=1
m
∑
k=1
ℙ(X = xj, Y = yk).
Marginal Probability Mass Function:
ℙ(X = x) =
∞
∑
k=1
ℙ(X = x, Y = yk),
ℙ(Y = y) =
∞
∑
j=1
ℙ(X = xj, Y = y).
Conditional Probability Mass Function:
ℙ(X = x|Y = y) = ℙ(X = x, Y = y)
ℙ(Y = y)
,
ℙ(Y = y|X = x) = ℙ(X = x, Y = y)
ℙ(X = x)
.
Conditional Expectation:
𝔼(X|Y) = 𝜇x|y =
n
∑
j=1
xjℙ(X = xj|Y = y),
𝔼(Y|X) = 𝜇y|x =
n
∑
k=1
ykℙ(Y = yk|X = x).
Conditional Variance:
Var (X|Y) = 𝜎2
x|y
= 𝔼[(X −𝜇x|y)2|Y]
=
∞
∑
j=1
(xj −𝜇x|y)2ℙ(X = xj|Y = y)
=
∞
∑
j=1
x2
j ℙ(X = xj|Y = y) −𝜇2
x|y

344
Appendix B
Var (Y|X) = 𝜎2
y|x
= 𝔼[(Y −𝜇y|x)2|X]
=
∞
∑
k=1
(yk −𝜇y|x)2ℙ(Y = yk|X = x)
=
∞
∑
k=1
y2
kℙ(Y = yk|X = x) −𝜇2
y|x.
Covariance: For 𝔼(X) = 𝜇x and 𝔼(Y) = 𝜇y
Cov (X, Y) = 𝔼[(X −𝜇x)(Y −𝜇y)]
=
∞
∑
j=1
∞
∑
k=1
(xj −𝜇x)(yk −𝜇y)ℙ(X = xj, Y = yk)
=
∞
∑
j=1
∞
∑
k=1
xjykℙ(X = xj, Y = yk) −𝜇x𝜇y
= 𝔼(XY) −𝔼(X)𝔼(Y).
Joint Moment Generating Function: For s, t ∈ℝ
MXY(s, t) = 𝔼(esX+tY) =
∞
∑
j=1
∞
∑
k=1
esxj+tykℙ(X = xj, Y = yk).
Joint Characteristic Function: For i =
√
−1 and s, t ∈ℝ
𝜑XY(s, t) = 𝔼(eisX+itY) =
∞
∑
j=1
∞
∑
k=1
eisxj+itykℙ(X = xj, Y = yk).
Independence: X and Y are independent if and only if
• ℙ(X = x, Y = y) = ℙ(X = x)ℙ(Y = y).
• MXY(s, t) = 𝔼(esX+tY) = 𝔼(esX)𝔼(etY) = MX(s)MY(t).
• 𝜑XY(s, t) = 𝔼(eisX+itY) = 𝔼(eisX)𝔼(eitY) = 𝜑X(s)𝜑Y(t).
Continuous Random Variables
Univariate Case
Let X be a continuous random variable whose values x ∈ℝand let fX(x) be the probability
density function.
Total Probability of All Possible Values:
∫
∞
−∞
fX(x) dx = 1.

Appendix B
345
Evaluating Probability:
ℙ(a ≤X ≤b) = ∫
b
a
fX(x) dx.
Cumulative Distribution Function:
FX(x) = ℙ(X ≤x) = ∫
x
−∞
fX(x) dx.
Probability Density Function:
fX(x) = d
dxFX(x).
Expectation:
𝔼(X) = 𝜇= ∫
∞
−∞
xfX(x) dx.
Variance:
Var (X) = 𝜎2 = ∫
∞
−∞
(x −𝜇)2fX(x) dx = ∫
∞
−∞
x2fX(x) dx −𝜇2 = 𝔼(X2) −[𝔼(X)]2.
Moment Generating Function:
MX(t) = 𝔼(etX) = ∫
∞
−∞
etxfX(x) dx,
t ∈ℝ.
Characteristic Function:
𝜑X(t) = 𝔼(eitX) = ∫
∞
−∞
eitxfX(x) dx,
i =
√
−1 and t ∈ℝ.
Probability Density Function of a Dependent Variable:
Let the random variable Y = g(X). If g is monotonic then the probability density function of
Y is
fY(y) = fX
(g−1(y)) ||||
d
dyg−1(y)
||||
−1
where g−1 denotes the inverse function.
Bivariate Case
Let X and Y be two continuous random variables whose values x ∈ℝand y ∈ℝ, and let
fXY(x, y) be the joint probability density function.
Total Probability of All Possible Values:
∫
∞
−∞∫
∞
−∞
fXY(x, y) dxdy = ∫
∞
−∞∫
∞
−∞
fXY(x, y) dydx = 1.

346
Appendix B
Joint Cumulative Distribution Function:
FXY(x, y) = ℙ(X ≤x, Y ≤y) = ∫
x
−∞∫
y
−∞
fXY(x, y) dydx = ∫
y
−∞∫
x
−∞
fXY(x, y) dxdy.
Evaluating Joint Probability:
ℙ(xa ≤X ≤xb, ya ≤Y ≤yb) = ∫
xb
xa
∫
yb
ya
fXY(x, y) dydx
= ∫
yb
ya
∫
xb
xa
fXY(x, y) dxdy
= FXY(xb, yb) −FXY(xb, ya) −FXY(xa, yb) + FXY(xa, ya).
Joint Probability Density Function:
fXY(x, y) =
𝜕2
𝜕x𝜕yFXY(x, y) =
𝜕2
𝜕y𝜕xFXY(x, y).
Marginal Probability Density Function:
fX(x) = ∫
∞
−∞
fXY(x, y) dy,
fY(y) = ∫
∞
−∞
fXY(x, y) dx.
Conditional Probability Density Function:
fX|Y(x|y) = fXY(x, y)
fY(y) ,
fY|X(y|x) = fXY(x, y)
fX(x) .
Conditional Expectation:
𝔼(X|Y) = 𝜇x|y = ∫
∞
−∞
xfX|Y(x|y) dx,
𝔼(Y|X) = 𝜇y|x = ∫
∞
−∞
yfY|X(y|x) dy.
Conditional Variance:
Var (X|Y) = 𝜎2
x|y
= 𝔼[(X −𝜇x|y)2|Y]
= ∫
∞
−∞
(x −𝜇x|y)2fX|Y(x|y) dx
= ∫
∞
−∞
x2fX|Y(x|y) dx −𝜇2
x|y
Var (Y|X) = 𝜎2
y|x
= 𝔼[(Y −𝜇y|x)2|X]
= ∫
∞
−∞
(y −𝜇y|x)2fY|X(y|x) dy

Appendix B
347
= ∫
∞
−∞
y2fY|X(y|x) dy −𝜇2
y|x.
Covariance: For 𝔼(X) = 𝜇x and 𝔼(Y) = 𝜇y
Cov (X, Y) = 𝔼[(X −𝜇x)(Y −𝜇y)]
= ∫
∞
−∞∫
∞
−∞
(x −𝜇x)(y −𝜇y)fXY(x, y) dy dx
= ∫
∞
−∞∫
∞
−∞
xyfXY(x, y) dy dx −𝜇x𝜇y
= 𝔼(XY) −𝔼(X)𝔼(Y).
Joint Moment Generating Function: For t, s ∈ℝ
MXY(s, t) = 𝔼(esX+tY) = ∫
∞
−∞∫
∞
−∞
esx+tyfXY(x, y) dy dx.
Joint Characteristic Function: For i =
√
−1 and t, s ∈ℝ
𝜑XY(s, t) = 𝔼(eisX+itY) = ∫
∞
−∞∫
∞
−∞
eisx+ityfXY(x, y) dy dx.
Independence: X and Y are independent if and only if
• fXY(x, y) = fX(x)fY(y).
• MXY(s, t) = 𝔼(esX+tY) = 𝔼(esX)𝔼(etY) = MX(s)MY(t).
• 𝜑XY(s, t) = 𝔼(eisX+itY) = 𝔼(eisX)𝔼(eitY) = 𝜑X(s)𝜑Y(t).
Joint Probability Density Function of Dependent Variables: Let the random variables U =
g(X, Y), V = h(X, Y). If u = g(x, y) and 𝑣= h(x, y can be uniquely solved for x and y in terms
of u and 𝑣with solutions given by, say, x = p(u, 𝑣) and y = q(u, 𝑣) and the functions g and h
have continuous partial derivatives at all points (x, y) such that the determinant
J(x, y) =
||||||||
𝜕g
𝜕x
𝜕g
𝜕y
𝜕h
𝜕x
𝜕h
𝜕y
||||||||
= 𝜕g
𝜕x
𝜕h
𝜕y −𝜕g
𝜕y
𝜕h
𝜕x ≠0
then the joint probability density function of U and V is
fUV(u, 𝑣) = fXY(x, y)|J(x, y)|−1
where x = p(u, 𝑣) and y = q(u, 𝑣).
Properties of Expectation and Variance
Let X and Y be two random variables and for constants a and b
𝔼(aX + b) = a𝔼(X) + b,
Var (aX + b) = a2Var (X)
𝔼(aX + bY) = a𝔼(X) + b𝔼(Y),
Var (aX + bY) = a2Var (X) + b2Var (Y) + 2abCov (X, Y).

348
Appendix B
Properties of Moment Generating and Characteristic Functions
If a random variable X has moments up to k-th order where k is a non-negative integer, then
𝔼(Xk) = dk
dtk MX(t)||||t=0
= i−k dk
dtk 𝜑X(t)||||t=0
where i =
√
−1.
If the bivariate random variables X and Y have moments up to m + n = k where m, n and k are
non-negative integers, then
𝔼(XmYn) =
dk
dsmdtn MXY(s, t)||||s=0,t=0
= i−k
dk
dsmdtn 𝜑XY(s, t)||||s=0,t=0
where i =
√
−1.
Correlation Coefficient
Let X and Y be two random variables with means 𝜇x and 𝜇y and variances 𝜎2
x and 𝜎2
y. The
correlation coefficient 𝜌xy between X and Y is defined as
𝜌xy =
Cov (X, Y)
√
Var (X)Var (Y)
=
𝔼[(X −𝜇x)(Y −𝜇y)]
𝜎x𝜎y
.
Important information:
• 𝜌xy measures only the linear dependency between X and Y.
• −1 ≤𝜌xy ≤1.
• If 𝜌xy = 0 then X and Y are uncorrelated.
• If X and Y are independent then 𝜌xy = 0. However, the converse is not true.
• If X and Y are jointly normally distributed then X and Y are independent if and only if
𝜌XY = 0.
Convolution
If X and Y are independent discrete random variables with probability mass functions ℙ(X = x)
and ℙ(Y = y), respectively, then the probability mass function for Z = X + Y is
ℙ(Z = z) =
∑
x
ℙ(X = x)ℙ(Y = z −x) =
∑
y
ℙ(X = z −y)ℙ(Y = y).
If X and Y are independent continuous random variables with probability density functions
fX(x) and fY(y), respectively, then the probability density function for Z = X + Y is
fZ(z) = ∫
∞
−∞
fX(x)fY(z −x) dx = ∫
∞
−∞
fX(z −y)fY(y) dy.

Appendix B
349
Discrete Distributions
Bernoulli: A random variable X is said to follow a Bernoulli distribution, X ∼Bernoulli(p)
where p ∈[0, 1] is the probability of success and the probability mass function is given as
P(X = x) = px(1 −p)1−x,
x = 0, 1
where 𝔼(X) = p and Var (X) = p −p2. The moment generating function is
MX(t) = 1 −p + pet,
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) = 1 −p + peit,
i =
√
−1 and t ∈ℝ.
Geometric: A random variable X is said to follow a geometric distribution, X ∼Geometric(p),
where p ∈[0, 1] is the probability of success and the probability mass function is given as
ℙ(X = x) = p(1 −p)x−1,
x = 1, 2, . . .
where 𝔼(X) = 1
p and Var (X) = 1 −p
p2 . The moment generating function is
MX(t) =
p
1 −(1 −p)et ,
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) =
p
1 −(1 −p)eit ,
i =
√
−1 and t ∈ℝ.
Binomial: A random variable X is said to follow a binomial distribution, X ∼Binomial(n, p),
p ∈[0, 1], where p ∈[0, 1] is the probability of success and n ∈ℕ0 is the number of trials and
the probability mass function is given as
ℙ(X = x) =
(
n
x
)
px(1 −p)n−x,
x = 0, 1, 2, . . . , n
where 𝔼(X) = np and Var (X) = np(1 −p). The moment generating function is
MX(t) = (1 −p + pet)n,
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) = (1 −p + peit)n,
i =
√
−1 and t ∈ℝ.

350
Appendix B
Negative Binomial: A random variable X is said to follow a negative binomial distribution,
X ∼NB(r, p) where p ∈[0, 1] is the probability of success and r is the number of successes
accumulated and the probability mass function is given as
ℙ(X = x) =
(
x −1
r −1
)
pr(1 −p)n−r,
x = r, r + 1, r + 2, . . .
where 𝔼(X) = r
p and Var (X) = r(1 −p)
p2
. The moment generating function is
MX(t) =
( 1 −p
1 −pet
)r
,
t < −log p
and the corresponding characteristic function is
MX(t) =
( 1 −p
1 −peit
)r
,
i =
√
−1 and t ∈ℝ.
Poisson: A random variable X is said to follow a Poisson distribution, X ∼Poisson(𝜆), 𝜆> 0
with probability mass function given as
ℙ(X = x) = e−𝜆𝜆x
x!
,
x = 0, 1, 2, . . .
where 𝔼(X) = 𝜆and Var (X) = 𝜆. The moment generating function is
MX(t) = e𝜆(et−1),
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) = e𝜆(eit−1),
i =
√
−1 and t ∈ℝ.
Continuous Distributions
Uniform: A random variable X is said to follow a uniform distribution, X ∼𝒰(a, b), a < b
with probability density function given as
fX(x) =
1
b −a,
a < x < b
where 𝔼(X) = a + b
2
and Var (X) = (b −a)2
12
. The moment generating function is
MX(t) = etb −eta
t(b −a) ,
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) = etb −eita
it(b −a) ,
i =
√
−1 and t ∈ℝ.

Appendix B
351
Normal: A random variable X is said to follow a normal distribution, X ∼𝒩(𝜇, 𝜎2), 𝜇∈ℝ,
𝜎2 > 0 with probability density function given as
fX(x) =
1
𝜎
√
2𝜋
e
−1
2
( x−𝜇
𝜎
)2
,
x ∈ℝ
where 𝔼(X) = 𝜇and Var (X) = 𝜎2. The moment generating function is
MX(t) = e𝜇t+ 1
2 𝜎2t2,
t ∈ℝ
and the corresponding characteristic function is
𝜑X(t) = ei𝜇t−1
2 𝜎2t2,
i =
√
−1 and t ∈ℝ.
Lognormal: A random variable X is said to follow a lognormal distribution, X ∼log-𝒩(𝜇, 𝜎2),
𝜇∈ℝ, 𝜎2 > 0 with probability density function given as
fX(x) =
1
x𝜎
√
2𝜋
e
−1
2
( log x−𝜇
𝜎
)2
,
x > 0
where 𝔼(X) = e𝜇+ 1
2 𝜎2 and Var (X) = (e𝜎2 −1)e2𝜇+𝜎2. The moment generating function is
MX(t) =
∞
∑
n=0
tn
n!en𝜇+ 1
2 n2𝜎2,
t ≤0
and the corresponding characteristic function is
𝜑X(t) =
∞
∑
n=0
(it)n
n! en𝜇+ 1
2 n2𝜎2,
i =
√
−1 and t ∈ℝ.
Exponential: A random variable X is said to follow an exponential distribution, X ∼Exp(𝜆),
𝜆> 0 with probability density function
fX(x) = 𝜆e−𝜆x,
x ≥0
where 𝔼(X) = 1
𝜆and Var (X) = 1
𝜆2 . The moment generating function is
MX(t) =
𝜆
𝜆−t,
t < 𝜆
and the corresponding characteristic function is
𝜑X(t) =
𝜆
𝜆−it,
i =
√
−1 and t ∈ℝ.

352
Appendix B
Gamma: A random variable X is said to follow a gamma distribution, X ∼Gamma(𝛼, 𝜆), 𝛼, 𝜆>
0 with probability density function given as
fX(x) = 𝜆e−𝜆x(𝜆x)𝛼−1
Γ(𝛼)
,
x ≥0
such that
Γ(𝛼) = ∫
∞
0
e−xx𝛼−1 dx
where 𝔼(X) = 𝛼
𝜆and Var (X) = 𝛼
𝜆2 . The moment generating function is
MX(t) =
(
𝜆
𝜆−t
)𝛼
,
t < 𝜆
and the corresponding characteristic function is
𝜑X(t) =
(
𝜆
𝜆−it
)𝛼
,
i =
√
−1 and t ∈ℝ.
Chi-Square: A random variable X is said to follow a chi-square distribution, X ∼𝜒2(𝜈), 𝜈∈ℕ
with probability density function given as
fX(x) =
1
2
𝜈
2 Γ
(
𝜈
2
)x
𝜈
2 −1e−x
2 ,
x ≥0
such that
Γ
(𝜈
2
)
= ∫
∞
0
e−xx
𝜈
2 −1 dx
where 𝔼(X) = 𝜈and Var (X) = 2𝜈. The moment generating function is
MX(t) = (1 −2t)−𝜈
2 ,
−1
2 < t < 1
2
and the corresponding characteristic function is
MX(t) = (1 −2it)−𝜈
2 ,
i =
√
−1 and t ∈ℝ.
Bivariate Normal: The random variables X and Y with means 𝜇x, 𝜇y, variances 𝜎2
x, 𝜎2
y,
and correlation coefficient 𝜌xy ∈(−1, 1) is said to follow a joint normal distribution,
(X, Y) ∼𝒩2 (𝝁, 𝚺) where 𝝁=
[𝜇x
𝜇y
]
and 𝚺=
[
Var(X)
Cov (X, Y)
Cov (X, Y)
Var(Y)
]
=
[
𝜎2
x
𝜌xy𝜎x𝜎y
𝜌xy𝜎x𝜎y
𝜎2
y
]
with joint probability density function given as
fXY(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
xy
e
−
1
2(1−𝜌2xy)
[( x−𝜇x
𝜎x
)2
−2𝜌
( x−𝜇x
𝜎x
)(
y−𝜇y
𝜎y
)
+
(
y−𝜇y
𝜎y
)2]
,
x, y ∈ℝ.

Appendix B
353
The moment generating function is
MXY(s, t) = e𝜇xs+𝜇yt+ 1
2 (𝜎2
x s2+2𝜌xy𝜎x𝜎yst+𝜎2
y t2),
s, t ∈ℝ
and the corresponding characteristic function is
𝜑XY(s, t) = ei𝜇xs+i𝜇yt−1
2 (𝜎2
x s2+2𝜌xy𝜎x𝜎yst+𝜎2
y t2),
i =
√
−1 and s, t ∈ℝ.
Multivariate Normal: The random vector X = (X1, X2, . . . , Xn) is said to follow a multivariate
normal distribution, X ∼𝒩n (𝝁, 𝚺) where
𝝁=
⎡
⎢
⎢
⎢⎣
𝔼(X1)
𝔼(X2)
⋮
𝔼(Xn)
⎤
⎥
⎥
⎥⎦
and 𝚺=
⎡
⎢
⎢
⎢⎣
Var (X1
)
Cov(X1, X2) . . . Cov(X1, Xn)
Cov(X1, X2)
Var(X2)
. . . Cov(X2, Xn)
⋮
⋮
⋱
⋮
Cov(X1, Xn) Cov(X2, Xn) . . .
Var(Xn)
⎤
⎥
⎥
⎥⎦
with probability density function given as
fX(x1, x2, . . . , xn) =
1
(2𝜋)
n
2 |𝚺|
1
2
e−1
2 XT𝚺−1X.
The moment generating function is
MX(t1, t2, . . . , tn) = e𝝁Tt+ 1
2 tT𝚺t,
t = (t1, t2, . . . , tn)T
and the corresponding characteristic function is
𝜑X(t1, t2, . . . , tn) = ei𝝁Tt−1
2 tT𝚺t,
t = (t1, t2, . . . , tn)T.
Integrable and Square Integrable Random Variables
Let X be a real-valued random variable
• If 𝔼(|X|) < ∞then X is an integrable random variable.
• If 𝔼(X2) < ∞then X is a square integrable random variable.
Convergence of Random Variables
Let X, X1, X2, . . . , Xn be a sequence of random variables. Then
(a) Xn
a.s.
−−−→X converges almost surely if
ℙ
(
lim
n→∞Xn = X
)
= 1.

354
Appendix B
(b) Xn
r−−→X converges in the r-th mean, r ≥1, if 𝔼(|Xr
n|) < ∞and
lim
n→∞𝔼(|Xn −X|r) = 0.
(c) Xn
P
−−→X converges in probability, if for all 𝜀> 0,
lim
n→∞ℙ(|Xn −X| ≥𝜀) = 0.
(d) Xn
D
−−→X converges in distribution, if for all x ∈ℝ,
lim
n→∞ℙ(Xn ≤x) = ℙ(X ≤x).
Relationship Between Modes of Convergence
For any r ≥1
{
Xn
a.s
−−−→X
Xn
r−−→X
}
⇒{Xn
P
−−→X} ⇒{Xn
D
−−→X}.
If r > s ≥1 then
{Xn
r−−→X} ⇒{Xn
s−−→X}.
Dominated Convergence Theorem
If Xn
a.s.
−−−→X and for any n ∈ℕwe have |Xn| < Y for some Y such that 𝔼(|Y|) < ∞, then
𝔼(|Xn|) < ∞and
lim
n→∞𝔼(Xn) = 𝔼(X).
Monotone Convergence Theorem
If 0 ≤Xn ≤Xn+1 and Xn
a.s.
−−−→X for any n ∈ℕthen
lim
n→∞𝔼(Xn) = 𝔼(X).
The Weak Law of Large Numbers
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean 𝜇∈ℝ. Then for any 𝜀> 0,
ℙ
(
lim
n→∞
||||
X1 + X2 + . . . + Xn
n
−𝜇
||||
≥𝜀
)
= 0.

Appendix B
355
The Strong Law of Large Numbers
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean 𝜇∈ℝ. Then
ℙ
(
lim
n→∞
X1 + X2 + . . . + Xn
n
= 𝜇
)
= 1.
The Central Limit Theorem
Let X1, X2, . . . , Xn be a sequence of independent and identically distributed random variables
with common mean 𝜇∈ℝand variance 𝜎2 > 0 and denote the sample mean as
X = X1 + X2 + . . . + Xn
n
where 𝔼
(
X
)
= 𝜇and Var
(
X
)
= 𝜎2
n . By defining
Zn =
X −𝔼
(
X
)
√
Var
(
X
) = X −𝜇
𝜎∕
√
n
then for n →∞,
lim
n→∞Zn = lim
n→∞
X −𝜇
𝜎∕
√
n
D
−−→𝒩(0, 1).
That is, Zn follows a standard normal distribution asymptotically.


Appendix C
Differential Equations Formulae
Separable Equations
The form
dy
dx = f(x)g(y)
has a solution
∫
1
g(y) dy = ∫f(x) dx.
If g(y) is a linear equation and if y1 and y2 are two solutions, then y3 = ay1 + by2 is also a
solution for constant a and b.
First-Order Ordinary Differential Equations
General Linear Equation: The general form of a first-order ordinary differential equation
dy
dx + f(x)y = g(x)
has a solution
y = I(x)−1
∫I(u)g(u) du + C
where I(x) = e∫f(x)dx is the integrating factor and C is a constant.
Bernoulli Differential Equation: For n ≠1, the Bernoulli differential equation has the form
dy
dx + P(x)y = Q(x)yn
which, by setting 𝑤=
1
yn−1 , can be transformed to a general linear ordinary differential
equation of the form
d𝑤
dx + (1 −n)P(x)𝑤= (1 −n)Q(x)
with a particular solution
𝑤= (1 −n)I(x)−1
∫I(u)Q(u) du

358
Appendix C
where I(x) = e(1−n) ∫P(x)dx is the integrating factor. The solution to the Bernoulli differential
equation becomes
y =
{
(1 −n)I(x)−1
∫I(u)Q(u) du
}−1
n−1
+ C
where C is a constant value.
Second-Order Ordinary Differential Equations
General Linear Equation: For a homogeneous equation,
ad2y
dx2 + bdy
dx + cy = 0.
By setting y = eux the differential equation has a general solution based on the characteristic
equation
au2 + bu + c = 0
such that m1 and m2 are the roots of the quadratic equation, and if
• m1, m2 ∈ℝ, m1 ≠m2 then y = Aem1x + Bem2x
• m1, m2 ∈ℝ, m1 = m2 = m then y = emx(A + Bx)
• m1, m2 ∈ℂ, m1 = 𝛼+ i𝛽, m2 = 𝛼−i𝛽then y = e𝛼x[A cos(𝛽x) + B sin(𝛽x)]
where A, B are constants.
Cauchy–Euler Equation: For a homogeneous equation,
ax2 d2y
dx2 + bxdy
dx + cy = 0.
By setting y = xu the Cauchy–Euler equation has a general solution based on the characteristic
equation
au2 + (b −a)u + c = 0
such that m1 and m2 are the roots of the quadratic equation, and if
• m1, m2 ∈ℝ, m1 ≠m2 then y = Axm1 + Bxm2
• m1, m2 ∈ℝ, m1 = m2 = m then y = xm(A + B log x)
• m1, m2 ∈ℂ, m1 = 𝛼+ i𝛽, m2 = 𝛼−i𝛽then y = x𝛼[A cos(𝛽log x) + B sin(𝛽log x)]
where A, B are constants.
Variation of Parameters: For a general non-homogeneous second-order differential equation,
a(x)d2y
dx2 + b(x)dy
dx + c(x) = f(x)

Appendix C
359
has the solution
y = yc + yp
where yc, the complementary function, satisfies the homogeneous equation
a(x)d2yc
dx2 + b(x)dyc
dx + c(x) = 0
and yp, the particular integral, satisfies
a(x)
d2yp
dx2 + b(x)
dyp
dx + c(x) = f(x).
Let yc = C1y(1)
c (x) + C2y(2)
c (x) where C1 and C2 are constants, then the particular solution to
the non-homogeneous second-order differential equation is
yp = −y(1)
c (x) ∫
y(2)
c (x)f(x)
a(x)W(y(1)
c (x), y(2)
c (x))
dx + y(2)
c (x) ∫
y(1)
c (x)f(x)
a(x)W(y(1)
c (x), y(2)
c (x))
dx
where W(y(1)
c (x), y(2)
c (x)) is the Wronskian defined as
W(y(1)
c (x), y(2)
c (x)) =
||||||||||
y(1)
c (x)
y(2)
c (x)
d
dxy(1)
c (x) d
dxy(2)
c (x)
||||||||||
= y(1)
c (x) d
dxy(2)
c (x) −y(2)
c (x) d
dxy(1)
c (x) ≠0.
Homogeneous Heat Equations
Initial Value Problem on an Infinite Interval: The diffusion equation of the form
𝜕u
𝜕t = 𝛼𝜕2u
𝜕x2 ,
𝛼> 0,
−∞< x < ∞,
t > 0
with initial condition u(x, 0) = f(x) has a solution
u(x, t) =
1
2
√
𝜋𝛼t ∫
∞
−∞
f(z)e−(x−z)2
4𝛼t
dz.
Initial Value Problem on a Semi-Infinite Interval: The diffusion equation of the form
𝜕u
𝜕t = 𝛼𝜕2u
𝜕x2 ,
𝛼> 0,
0 ≤x < ∞,
t > 0

360
Appendix C
with
• initial condition u(x, 0) = f(x) and boundary condition u(0, t) = 0 has a solution
u(x, t) =
1
2
√
𝜋𝛼t ∫
∞
0
f(z)
[
e−(x−z)2
4𝛼t
−e−(x+z)2
4𝛼t
]
dz
• initial condition u(x, 0) = f(x) and boundary condition ux(0, t) = 0 has a solution
u(x, t) =
1
2
√
𝜋𝛼t ∫
∞
0
f(z)
[
e−(x−z)2
4𝛼t
+ e−(x+z)2
4𝛼t
]
dz
• initial condition u(x, 0) = 0 and boundary condition u(0, t) = g(t) has a solution
u(x, t) =
x
2
√
𝜋𝛼∫
t
0
1
√
t −𝑤
g(𝑤)e−
x2
4𝛼(t−𝑤)
d𝑤.
Stochastic Differential Equations
Suppose Xt, Yt and Zt are It¯o processes satisfying the following stochastic differential
equations:
dXt = 𝜇(Xt, t)dt + 𝜎(Xt, t)dWx
t
dYt = 𝜇(Yt, t)dt + 𝜎(Yt, t))dWy
t
dZt = 𝜇(Zt, t)dt + 𝜎(Zt, t)dWz
t
where Wx
t , Wy
t and Wz
t are standard Wiener processes.
Reciprocal:
d
(
1
Xt
)
(
1
Xt
) = −dXt
Xt
+
(dXt
Xt
)2
.
Product:
d(XtYt)
XtYt
= dXt
Xt
+ dYt
Yt
+ dXt
Xt
dYt
Yt
.
Quotient:
d
(Xt
Yt
)
(Xt
Yt
) = dXt
Xt
−dYt
Yt
−dXt
Xt
dYt
Yt
+
(dYt
Yt
)2
.
Product and Quotient I:
d
(XtYt
Zt
)
(XtYt
Zt
) = dXt
Xt
+ dYt
Yt
−dZt
Zt
+ dXt
Xt
dYt
Yt
−dXt
Xt
dZt
Zt
−dYt
Yt
dZt
Zt
+
(dZt
Zt
)2
.

Appendix C
361
Product and Quotient II:
d
( Xt
YtZt
)
( Xt
YtZt
) = dXt
Xt
−dYt
Yt
−dZt
Zt
−dXt
Xt
dYt
Yt
−dXt
Xt
dZt
Zt
+ dYt
Yt
dZt
Zt
.
Black–Scholes Model
Black–Scholes Equation (Continuous Dividend Yield): At time t, let the asset price St follows
a geometric Brownian motion
dSt
St
= (𝜇−D)dt + 𝜎dWt
where 𝜇is the drift parameter, D is the continuous dividend yield, 𝜎is the volatility parameter
and Wt is a standard Wiener process. For a European-style derivative V(St, t) written on the
asset St, it satisfies the Black–Scholes equation with continuous dividend yield
𝜕V
𝜕t + 1
2𝜎2S2
t
𝜕2V
𝜕S2
t
+ (r −D)St
𝜕V
𝜕St
−rV(St, t) = 0
where r is the risk-free interest rate. The parameters 𝜇, r, D and 𝜎can be either constants,
deterministic functions or stochastic processes.
European Options: For a European option having the payoff
Ψ(ST) = max{𝛿(ST −K), 0}
where 𝛿∈{ −1, 1}, K is the strike price and T is the option expiry time, and if r, D and 𝜎are
constants the European option price at time t < T is
V(St, t; K, T) = 𝛿Ste−D(T−t)Φ(𝛿d+) −𝛿Ke−r(T−t)Φ(𝛿d−)
where d± =
log(St∕K) + (r −D ± 1
2𝜎2)(T −t)
𝜎
√
T −t
and Φ(⋅) is the cumulative distribution function
of a standard normal.
Reflection Principle: If V(St, t) is a solution of the Black–Scholes equation then for a constant
B > 0, the function
U(St, t) =
(St
B
)2𝛼
V
(
B2
St
, t
)
,
𝛼= 1
2
(
1 −r −D
1
2𝜎2
)
also satisfies the Black–Scholes equation.

362
Appendix C
Black Model
Black Equation: At time t, let the asset price St follows a geometric Brownian motion
dSt
St
= (𝜇−D)dt + 𝜎dWt
where 𝜇is the drift parameter, D is the continuous dividend yield, 𝜎is the volatility parameter
and Wt is a standard Wiener process. Consider the price of a futures contract maturing at time
T > t on the asset St as
F(t, T) = Ste(r−D)(T−t)
where r is the risk-free interest rate. For a European option on futures V(F(t, T), t) written on
a futures contract F(t, T), it satisfies the Black equation
𝜕V
𝜕t + 1
2𝜎2F(t, T)2 𝜕2V
𝜕F2 −rV(F(t, T), t) = 0.
The parameters 𝜇, r, D and 𝜎can be either constants, deterministic functions or stochastic
processes.
European Options on Futures: For a European option on futures having the payoff
Ψ(F(T, T)) = max{𝛿(F(T, T) −K), 0}
where 𝛿∈{ −1, 1}, K is the strike price and T is the option expiry time, and if r and 𝜎are
constants the price of a European option on futures at time t < T is
V(F(t, T), t; K, T) = 𝛿e−r(T−t)[F(t, T)Φ(𝛿d+) −KΦ(𝛿d−)]
where d± =
log(F(t, T)∕K) ± 1
2𝜎2(T −t)
𝜎
√
T −t
and Φ(⋅) is the cumulative distribution function of a
standard normal.
Reflection Principle: If V(F(t, T), t) is a solution of the Black equation then for a constant
B > 0, the function
U(F(t, T), t) = F(t, T)
B
V
(
B2
F(t, T), t
)
also satisfies the Black equation.
Garman–Kohlhagen Model
Garman–Kohlhagen Equation: At time t, let the foreign-to-domestic exchange rate Xt follows
a geometric Brownian motion
dXt
Xt
= 𝜇dt + 𝜎dWt

Appendix C
363
where 𝜇is the drift parameter, 𝜎is the volatility parameter and Wt is a standard Wiener
process. For a European-style derivative V(Xt, t) which depends on Xt, it satisfies the
Garman–Kohlhagen equation
𝜕V
𝜕t + 1
2𝜎2X2
t
𝜕2V
𝜕X2
t
+ (rd −rf )Xt
𝜕V
𝜕Xt
−rdV(Xt, t) = 0
where rd and rf are the domestic and foreign currency risk-free interest rates. The parameters
𝜇, rd, rf and 𝜎can be either constants, deterministic functions or stochastic processes.
European Options: For a European option having the payoff
Ψ(XT) = max{𝛿(XT −K), 0}
where 𝛿∈{ −1, 1}, K is the strike price and T is the option expiry time, and if rd, rf and 𝜎
are constants the European option price (domestic currency in one unit of foreign currency) at
time t < T is
V(Xt, t; K, T) = 𝛿Xte−rf (T−t)Φ(𝛿d+) −𝛿Ke−rd(T−t)Φ(𝛿d−)
where d± =
log(Xt∕K) + (rd −rf ± 1
2𝜎2)(T −t)
𝜎
√
T −t
and Φ(⋅) is the cumulative distribution func-
tion of a standard normal.
Reflection Principle: If V(Xt, t) is a solution of the Garman–Kohlhagen equation then for a
constant B > 0 the function
U(Xt, t) =
(Xt
B
)2𝛼
V
(
B2
Xt
, t
)
,
𝛼= 1
2
(
1 −
rd −rf
1
2𝜎2
)
also satisfies the Garman–Kohlhagen equation.


Bibliography
Abramovitz, M. and Stegun, I.A. (1970). Handbook of Mathematical Functions: with Formulas, Graphs
and Mathematical Tables. Dover Publications, New York.
Bachelier, L. (1900). Théorie de la spéculation. Annales Scientifiques de l’École Normale Supérieure,
3(17), pp. 21–86.
Baz, J. and Chacko, G. (2004). Financial Derivatives: Pricing, Applications and Mathematics.
Cambridge University Press, Cambridge.
Björk, T. (2009). Arbitrage Theory in Continuous Time, 3rd edn. Oxford Finance Series, Oxford Univer-
sity Press, Oxford.
Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political
Economy, 81, pp. 637–654.
Breze´zniak, Z. and Zastawniak, T. (1999). Basic Stochastic Processes. Springer-Verlag, Berlin.
Brown, R. (1828). A brief account of microscopical observations made in the months of June, July and
August 1827 on the particles contained in the pollen of plants. The Miscellaneous Botanical Works
of Robert Brown: Volume 1, John J. Bennet (ed), R. Hardwicke, London.
Capi´nski, M., Kopp, E. and Traple, J. (2012). Stochastic Calculus for Finance. Cambridge University
Press, Cambridge.
Clark, I.J. (2011). Foreign Exchange Option Pricing: A Practitioner’s Guide. John Wiley and Sons Ltd,
Chichester, United Kingdom.
Clark, I.J. (2014). Commodity Option Pricing: A Practitioner’s Guide. John Wiley and Sons Ltd, Chich-
ester, United Kingdom.
Clewlow, S. and Strickland, C. (1999). Valuing energy options in a one factor model fitted to for-
ward prices. Working Paper, School of Finance and Economics, University of Technology, Sydney,
Australia.
Cont, R. and Tankov, P. (2003). Financial Modelling with Jump Processes. Chapman and Hall/CRC,
London.
Cox, J.C. (1996). The constant elasticity of variance option pricing model. The Journal of Portfolio
Management, Special Issue, pp. 15–17.
Cox, J.C., Ingersoll, J.E. and Ross, S.A. (1985). A theory of the term structure of interest rates. Econo-
metrica, 53, pp. 385–407.
Dufresne, D. (2001). The integrated square-root process. Research Paper Number 90, Centre for Actuarial
Studies, Department of Economics, University of Melbourne, Australia.
Einstein, A. (1956). Investigations of the Theory of Brownian Movement. Republication of the original
1926 translation, Dover Publications Inc.
Fama, E. (1965). The behavior of stock market prices. Journal of Business, 38(1), pp. 34–105.
Gabillon, J. (1991). The term structures of oil futures prices. Working Paper, Oxford Institute for Energy
Studies, Oxford, UK.

366
Bibliography
Garman, M.B. and Kohlhagen, S.W. (1983). Foreign currency option values. Journal of International
Money and Finance, 2, pp. 231–237.
Geman, H. (2005). Commodities and Commodity Derivatives: Modelling and Pricing for Agriculturals,
Metals and Energy. John Wiley & Sons, Chichester.
Girsanov, I. (1960). On transforming a certain class of stochastic processes by absolutely continuous
substitution of measures. SIAM Theory of Probability and Applications, 5(3), pp. 285–301.
Gradshteyn, I.S. and Ryzhik, I.M. (1980). Table of Integrals, Series and Products, Jeffrey, A. (ed.).
Academic Press, New York.
Grimmett, G. and Strizaker, D. (2001). Probability and Random Processes, 3rd edn. Oxford University
Press, Oxford.
Harrison, J.M. and Kreps, D.M. (1979). Martingales and arbitrage in multiperiod securities markets.
Journal of Economic Theory, 20, pp. 381–408.
Harrison, J.M. and Pliska, S.R. (1981). Martingales and stochastic integrals in the theory of continuous
trading. Stochastic Processes and their Applications, 11, pp. 215–260.
Harrison, J.M. and Pliska, S.R. (1983). A stochastic calculus model of continuous trading: Complete
markets. Stochastic Processes and their Applications, 15, pp. 313–316.
Heston, S.L. (1993). A closed-form solution for options with stochastic volatility with applications to
bond and currency options. The Review of Financial Studies, 6(2), pp. 327–343.
Ho, S.Y. and Lee, S.B. (2004). The Oxford Guide to Financial Modeling: Applications for Capital-
Markets, Corporate Finance, Risk Management and Financial Institutions. Oxford University Press,
Oxford, United Kingdom.
Hsu, Y.L., Lin, T.I. and Lee, C.F. (2008). Constant elasticity of variance (CEV) option pricing model:
Integration and detail derivation. Mathematics and Computers in Simulation, 79(1), pp. 60–71.
Hull, J. (2011). Options, Futures, and Other Derivatives, 6th edn. Prentice Hall, Englewood Cliffs, NJ.
It¯o, K. (1951). On stochastic differential equations: Memoirs. American Mathematical Society, 4,
pp. 1–51.
Joshi, M. (2008). The Concepts and Practice of Mathematical Finance, 2nd edn. Cambridge University
Press, Cambridge.
Joshi, M. (2011). More Mathematical Finance. Pilot Whale Press, Melbourne.
Jowett, B. and Campbell, L. (1894). Plato’s Republic, The Greek Text, Edited with Notes and Essays.
Clarendon Press, Oxford.
Kac, M. (1949). On distributions of certain Wiener functionals. Transactions of the American Mathe-
matical Society, 65(1), pp. 1–13.
Karatzas, I. and Shreve, S.E. (2004). Brownian Motion and Stochastic Calculus, 2nd edn.
Springer-Verlag, Berlin.
Kluge, T. (2006). Pricing Swing Options and Other Electricity Derivatives. DPhil Thesis, University of
Oxford.
Kolmogorov, A. (1931), Über die analytischen Methoden in der Wahrscheinlichkeitsrechnung. Mathe-
matische Annalen, 104, pp. 415–458.
Kou, S.G. (2002). A jump-diffusion model for option pricing. Management Science, 48, pp. 1086–1101.
Kwok, Y.K. (2008). Mathematical Models of Financial Derivatives, 2nd edn. Springer-Verlag, Berlin.
Lucia, J.J. and Schwartz, E.S. (2002). Electricity prices and power derivatives: Evidence from the Nordic
Power Exchange. Review of Derivatives Research, 5, pp. 5–50.
Merton, R. (1973). The theory of rational option pricing. Bell Journal of Economics and Management
Science, 4, pp. 141–183.
Merton, R. (1976). Option pricing when underlying stock returns are discontinuous. Journal of Financial
Economics, 3, pp. 125–144.
Musiela, M. and Rutkowski, M. (2007). Martingale Methods in Financial Modelling, 2nd edn.
Springer-Verlag, Berlin.
Øksendal, B. (2003). Stochastic Differential Equations: An Introduction with Applications, 6th edn.
Springer-Verlag, Heidelberg.

Bibliography
367
Rice, J.A. (2007). Mathematical Statistics and Data Analysis, 3rd edn. Duxbury, Belmont, CA.
Ross, S. (2002). A First Course in Probability, 6th edn. Prentice Hall, Englewood Cliffs, NJ.
Samuelson, P.A. (1965). Rational theory of warrant pricing. Industrial Management Review, 6(2), pp.
13–31.
Shreve, S.E. (2005). Stochastic Calculus for Finance; Volume I: The Binomial Asset Pricing Models.
Springer-Verlag, New York.
Shreve, S.E. (2008). Stochastic Calculus for Finance; Volume II: Continuous-Time Models.
Springer-Verlag, New York.
Stulz, R.M. (1982). Options on the minimum or the maximum of two risky assets: Analysis and appli-
cations. Journal of Financial Economics, 10, pp. 161–185.
Wiener, N. (1923). Differential space. Journal of Mathematical Physics, 2, pp. 131–174.
Wilmott, P., Dewynne, J. and Howison, S. (1993). Option Pricing: Mathematical Models and Computa-
tion. Oxford Financial Press, Oxford.


Notation
SET NOTATION
∈
is an element of
∉
is not an element of
Ω
sample space
ℰ
universal set
∅
empty set
A
subset of Ω
Ac
complement of set A
|A|
cardinality of A
ℕ
set of natural numbers, {1, 2, 3, . . .}
ℕ0
set of natural numbers including zero, {0, 1, 2, . . .}
ℤ
set of integers, {0, ±1, ±2, ±3, . . .}
ℤ+
set of positive integers, {1, 2, 3, . . .}
ℝ
set of real numbers
ℝ+
set of positive real numbers, {x ∈ℝ∶x > 0}
ℂ
set of complex numbers
A × B
cartesian product of sets A and B, A × B = {(a, b) ∶a ∈A, b ∈B}
a ∼b
a is equivalent to b
⊆
subset
⊂
proper subset
∩
intersection
∪
union
\
difference
Δ
symmetric difference
sup
supremum or least upper bound
inf
infimum or greatest lower bound
[a, b]
the closed interval {x ∈ℝ∶a ≤x ≤b}
[a, b)
the interval {x ∈ℝ∶a ≤x < b}
(a, b]
the interval {x ∈ℝ∶a < x ≤b}
(a, b)
the open interval {x ∈ℝ∶a < x < b}
ℱ, 𝒢, ℋ
𝜎-algebra (or 𝜎-fields)

370
Notation
MATHEMATICAL NOTATION
x+
max{x, 0}
x−
min{x, 0}
⌊x⌋
largest integer not greater than and equal to x, max{m ∈ℤ| m ≤x}
⌈x⌉
smallest integer greater than and equal to x, min{n ∈ℤ| n ≥x}
x ∨y
max{x, y}
x ∧y
min{x, y}
i
√
−1
∞
infinity
∃
there exists
∃!
there exists a unique
∀
for all
≈
approximately equal to
p =⇒q
p implies q
p ⇐= q
p is implied by q
p ⇐⇒q
p implies and is implied by q
f ∶X →Y
f is a function where every element of X has an image in Y
f(x)
the value of the function f at x
lim
x→a f(x)
limit of f(x) as x tends to a
𝛿x, Δx
increment of x
f −1(x)
the inverse function of the function f(x)
f
′(x), f
′′(x)
the first and second-order derivative of the function f(x)
dy
dx, d2y
dx2
first and second-order derivative of y with respect to x
∫y dx, ∫b
a y dx
the indefinite and definite integral of y with respect to x
𝜕f
𝜕xi
, 𝜕2f
𝜕x2
i
first and second-order partial derivative of f with respect to xi
where f is a function on (x1, x2, . . . , xn)
𝜕2f
𝜕xi𝜕xj
second-order partial derivative of f with respect to xi and xj
where f is a function on (x1, x2, . . . , xn)
logax
logarithm of x to the base a
log x
natural logarithm of x
n∑
i=1
ai
a1 + a2 + . . . + an
n∏
i=1
ai
a1 × a2 × . . . × an
|a|
modulus of a
(
n√
a
)m
a
m
n
n!
n factorial
(n
k
)
n!
k!(n −k)! for n, k ∈ℤ+
𝛿(x)
Dirac delta function
H(x)
Heaviside step function

Notation
371
Γ(t)
gamma function
B(x, y)
beta function
a
a vector a
|a|
magnitude of a vector a
a ⋅b
scalar or dot product of vectors a and b
a × b
vector or cross-product of vectors a and b
M
a matrix M
MT
transpose of a matrix M
M−1
inverse of a square matrix M
|M|
determinant of a square matrix M
PROBABILITY NOTATION
A, B, C
events
1IA
indicator of the event A
ℙ, ℚ
probability measures
ℙ(A)
probability of event A
ℙ(A|B)
probability of event A conditional on event B
X, Y, Z
random variables
X, Y, Z
random vectors
ℙ(X = x)
probability mass function of a discrete random variable X
fX(x)
probability density function of a continuous random variable X
FX(x), ℙ(X ≤x)
cumulative distribution function of a random variable X
MX(t)
moment generating function of a random variable X
𝜑X(t)
characteristic function of a random variable X
P(X = x, Y = y)
joint probability mass function of discrete variables X and Y
fXY(x, y)
joint probability density function of continuous random
variables X and Y
FXY(x, y), ℙ(X ≤x, Y ≤y)
joint cumulative distribution function of random variables X
and Y
MXY(s, t)
joint moment generating function of random variables X and Y
𝜑XY(s, t)
joint characteristic function of random variables X and Y
p(x, t; y, T)
transition probability density of y at time T starting at time t at
point x
∼
is distributed as
≁
is not distributed as
∻
is approximately distributed as
a.s
−−−→
converges almost surely
r−−→
converges in the r-th mean
P
−−→
converges in probability
D
−−→
converges in distribution
X
d= Y
X and Y are identically distributed random variables

372
Notation
X ⟂⟂Y
X and Y are independent random variables
X ∕⟂⟂Y
X and Y are not independent random variables
𝔼(X)
expectation of random variable X
𝔼ℚ(X)
expectation of random variable X under the probability
measure ℚ
𝔼[g(X)]
expectation of g(X)
𝔼(X|ℱ)
conditional expectation of X
Var(X)
variance of random variable X
Var(X|ℱ)
conditional variance of X
Cov(X, Y)
covariance of random variables X and Y
𝜌xy
correlation between random variables X and Y
Bernoulli(p)
Bernoulli distribution with mean p and variance p(1 −p)
Geometric(p)
geometric distribution with mean p−1 and variance (1 −p)p−2
Binomial(n, p)
binomial distribution with mean np and variance np(1 −p)
BN(n, r)
negative binomial distribution with mean rp−1 and variance
r(1 −p)p−2
Poisson(𝜆)
Poisson distribution with mean 𝜆and variance 𝜆
Exp(𝜆)
exponential distribution with mean 𝜆−1 and variance 𝜆−2
Gamma(𝛼, 𝜆)
gamma distribution with mean 𝛼𝜆−1 and variance 𝛼𝜆−2
𝒰(a, b)
uniform distribution with mean 1
2(a + b) and variance
1
12(b −a)2
𝒩(𝜇, 𝜎2)
normal distribution with mean 𝜇and variance 𝜎2
log-𝒩(𝜇, 𝜎2)
lognormal distribution with mean e𝜇+ 1
2 𝜎2 and variance
(e𝜎2 −1)e2𝜇+𝜎2
𝜒2(k)
chi-square distribution with mean k and variance 2k
𝒩n(𝝁, 𝚺)
multivariate normal distribution with n-dimensional mean
vector 𝜇and n × n covariance matrix 𝚺
Φ(⋅), Φ(x)
cumulative distribution function of a standard normal
𝚽(x, y, 𝜌xy)
cumulative distribution function of a standard bivariate normal
with correlation coefficient 𝜌xy
Wt
standard Wiener process, Wt ∼𝒩(0, t)
Nt
Poisson process, Nt ∼Poisson(𝜆t)

Index
adapted stochastic processes, 2, 303
admissible trading strategy, 186
American options, 53
appendices, 1, 331–63
arbitrage, 53, 185–7, 232, 322–30
arithmetic Brownian motion, 128–9, 222–3,
229–30
see also Bachelier model
stock price with continuous dividend
yield, 229–30
arithmetic series, formulae, 332
arrival time distribution, Poisson
process, 256–8
see also stock...
fundamental theories, 232–5
attainable contingent claim, 186–7
Bachelier model
see also arithmetic Brownian motion
definition and formulae, 128–9
backward Kolmogorov equation, 97, 98–102,
149–50, 153–4,
180–1
see also diffusion; parabolic...
definition, 97, 98–9, 149–50, 153–4, 180–1
multi-dimensional diffusion
process, 99–102, 155–83
one-dimensional diffusion process, 87–8,
123–55
one-dimensional random walk, 153–4
two-dimensional random walk, 180–1
Bayes’ Formula, 7
Bayes’ rule, 341
Bernoulli differential equation, 357–8
Bernoulli distribution, 11, 12, 13, 14–15, 349
Bessel process, 163–6
beta function, 338
binomial distribution, 12–14, 349–50
bivariate continuous random variables, 345–7
see also continuous...
bivariate discrete random variables, 343–4
see also discrete...
bivariate normal distribution, 27–9, 34–40,
60–2, 158, 165, 352–3
see also normal distribution
covariance, 28–9, 57–9, 158, 165
marginal distributions, 27
Black equation, 362
Black model, 362
Black–Scholes equation, 54, 96, 236–8, 361
see also partial differential equations
reflection principle, 54–5, 361
Black–Scholes model, 129–30, 185, 236–8,
361
see also geometric Brownian motion
Bonferroni’s inequality, 7
Boole’s inequality, 6, 7, 10
Borel–Cantelli lemma, 10–11
Brownian bridge process, 137–8
Brownian motion, 51–93, 95–183, 185–92,
221–4, 227–8, 361, 362–3
see also arithmetic...; diffusion...;
geometric...; random walks; Wiener
processes
definitions and formulae, 51, 128–32,
138–9, 221–4, 227–30
càdlàg process, 246
Cauchy–Euler equation, 358–9
cdf see cumulative distribution function
central limit theorem, 12, 13, 57, 355
CEV see constant elasticity of variance model
change of measure, 43, 185–242, 249–51,
301–30
see also Girsanov’s theorem
definitions, 185–92, 249–51
Chebyshev’s inequality, 40–1, 75
chi-square distribution, 24–6, 352
CIR see Cox–Ingersoll–Ross model
Clewlow–Strickland 1-factor model, 141–4

374
Index
compensated Poisson process, 245, 262–3,
266–7, 323–6, 330
see also Poisson...
complement, probability concepts, 1, 4–5, 341
complete market, 187, 233, 322–30
compound Poisson process, 243, 245–6,
249–51, 264–88, 306–22, 323–30
see also Poisson...
decomposition, 268–9, 306–8
Girsanov’s theorem, 249–51
martingales, 265–7, 323–5
concave function, 339
conditional, probability concepts, 341
conditional expectation, 2–3, 43–9, 52–5,
75–6, 150–3, 192–3, 197–200, 273–4,
284–5, 341, 343, 346
conditional Jensen’s inequality, properties of
conditional expectation, 3, 48–9, 75–6,
192–4
conditional probability
density function formulae, 346
mass function formulae, 343
properties of conditional expectation, 3,
43–5
conditional variance, 343, 346
constant elasticity of variance model
(CEV), 145–6
contingent claims, 185–92, 245
see also derivatives
continuous distributions, 350–3
convergence of random variables, 10–11,
209–11, 353–4
convex function, 3, 42, 48–9, 75–6, 192–4,
338–9
convolution formulae, 25–6, 348
countable unions, 1–2
counting process, 243–330
see also Poisson...
definition and formulae, 243–4
covariance, 28–9, 57–60, 64–8, 117–18,
157–8, 165–6, 251–2, 344, 347
matrices, 59, 64–8
covariance of bivariate normal
distribution, 28–9, 57–60, 158, 165–6
covariance of two standard Wiener
processes, 57–60, 64–8
Cox process (doubly stochastic Poisson
process), 243, 245
see also Poisson process
Cox–Ingersoll–Ross model (CIR), 135–7,
171–4, 178
cumulative distribution function (cdf), 16–20,
22–4, 30–2, 37–40, 214–18, 342, 343,
345, 361–3
cumulative intensity, 245
see also intensity...
De Moivre’s formula, 333
De Morgan’s law, 4, 7, 10
decomposition of a compound Poisson
process, 268–9, 306–8
differential equations, 52, 90–3, 95–183,
224–42, 253–5, 305–30, 357–63
see also ordinary...; partial...; stochastic...
differential-difference equations, 253–5
Dirac delta function, 339
see also Heaviside step function
discounted portfolio value, 187–92, 224–7,
228–32, 242, 324–30
discrete distributions, 349–50
discrete-time martingales, 53
dominated convergence theorem, 354
Donsker theorem, 56–7
Doob’s maximal inequality, 76–80
elementary process, 96
equivalent martingale measure, 185, 188–9,
209–11, 222–5, 230–1, 234–5, 238–42,
325–30
see also risk-neutral...
Euler’s formula, 333
events, definition, 1, 243–51
exclusion for probability, definition, 7–10
expectation, 2–3, 40–9, 52–5, 75–6, 90–1,
122–3, 125–6, 148–9, 157–8, 164–6,
192–4, 197–205, 270–2, 279–81,
305–30, 342, 343–7
exponential martingale process, 263–4
see also martingales
Feynman–Kac theorem, 97–102, 147–9,
178–80
see also diffusion
multi-dimensional diffusion process, 178–80
one-dimensional diffusion process, 147–9,
178
fields, definition, 1–2
filtration, 2–3, 52–5, 68–71, 75–82, 95–123,
128, 145–9, 178–80, 186–242, 244–51,
261–330
first fundamental theorem of asset pricing,
definition, 232
first passage time density function, 85–9,
218–19
first passage time of a standard Wiener
processes, 53, 76–89, 218–19
hitting a sloping line, 218–19
Laplace transform, 83–4
first-order ordinary differential
equations, 125–6, 171–74, 255, 265,
273–4, 306, 357–8
Fokker–Planck equation see forward
Kolmogorov equation

Index
375
folded normal distribution, 22–4, 72
see also normal distribution
foreign exchange (FX), 51, 54, 192, 238–42, 362
foreign-denominated stock price under
domestic risk-neutral measure, 241–2
risk-neutral measure, 238–42
forward curve from an asset price following a
geometric Brownian motion, 138–9
forward curve from an asset price following a
geometric mean-reverting
process, 139–40, 169–71
forward curves, 138–44
forward Kolmogorov equation, 97, 98–102,
150–3, 154–5, 181–3
see also diffusion; parabolic...
multi-dimensional diffusion process, 102
one-dimensional diffusion process, 150–3
one-dimensional random walk, 154–5
two-dimensional random walk, 180–3
Fubini’s theorem, 339
FX see foreign exchange
Gabillon 2-factor model, 169–71
gamma distribution, 16–17, 257, 352
Garman–Kohlhagen equation/model, 362–3
general probability theory, concepts, 1–49,
185–92, 341–55
generalised Brownian motion, 130–2
generalised It¯o integral, 118–19
geometric average, 146–7
geometric Brownian motion, 70–1, 129–32,
138–9, 146–7, 221–2, 227–8, 361, 362–3
see also Black–Scholes options pricing
model; Brownian motion
definition and formulae, 129–32, 138–9,
146–7, 221–2, 227–8
forward curve from an asset price, 138–9
Markov property, 70–1
stock price with continuous dividend
yield, 227–8
geometric distribution, 349
geometric mean-reverting process, 134–5,
139–40, 169–71, 291–5
definition and formulae, 134–5, 139–40,
169–71, 291–5
forward curve from an asset price, 139–40,
169–71
jump-diffusion process, 291–5
geometric series, formulae, 332
Girsanov’s theorem, 185, 189–92, 194–242,
249–51, 298–322
see also real-world measure; risk-neutral
measure
corollaries, 189–90
definitions, 185, 189–92, 194–225, 249–51,
298–322
formulae, 189–92, 194–225, 249–51,
298–322
jump processes, 298–322
Poisson process, 249–51, 298–322
running maximum and minimum of a Wiener
process, 214–18
hazard function, 245
see also intensity...
hazard process, 245
heat equations, 359–60
Heaviside step function, 339
see also Dirac delta function
Heston stochastic volatility model, 175–8
Hölder’s inequality, 41–2, 72–4, 79, 193,
204–5
homogeneous heat equations, 359–60
homogeneous Poisson process see Poisson
process
hyperbolic functions, 74, 333
inclusion and exclusion for probability,
definition, 7–10
incomplete markets, 187, 324–30
independence, properties, 3, 11, 47–8, 163–6,
210, 259–61
independent events, probability
concepts, 259–61, 341, 344, 347
integrable random variable, 3, 44, 45–9, 353
see also random variables
integral calculus, definition, 95–6
integrated square-root process, 171–4
integration by parts, 19–20, 115–18, 147, 151,
174, 257–8, 337
It¯o integral, 102–7, 108–116, 118–20, 144,
148–9, 163–6, 177–8, 187–92, 195–6,
205–7
It¯o isometry, 113–14
It¯o processes, 54, 95–123, 360–1
It¯o’s formula see It¯o’s lemma
It¯o’s lemma, 96–7, 99–100, 102–27, 129–33,
134–7, 139–46, 147–53, 155–62, 166–8,
170–80, 196, 203–5, 212–13, 220–7,
234–41, 246–51, 278–80, 283–98,
307–8, 314–22
see also stochastic differential equations;
Taylor series
multi-dimensional It¯o formulae for
jump-diffusion process, 247–9
one-dimensional It¯o formulae for
jump-diffusion process, 246–7
Jensen’s inequality, properties of conditional
expectation, 3, 48–9, 75–6, 192–4
joint characteristic function, 344, 347

376
Index
joint cumulative distribution function, 16–17,
32, 212–13, 343, 345
joint distribution of standard Wiener
processes, 58–60, 64, 158, 165–6
joint moment generating function, 122–3, 267,
280–1, 306, 318–19, 321–2, 344, 347
joint probability density function, 16–17,
27–34, 86–9, 211–18, 345–6, 352
joint probability mass function, definition, 343
jump diffusion process, 246–51, 281–98,
298–30
see also diffusion; Poisson...; Wiener...
concepts, 246–51, 281–98, 325–30
geometric mean-reverting process, 291–5
Merton’s model, 285–8, 327–30
multi-dimensional It¯o formulae, 247–9
one-dimensional It¯o formulae, 246–7
pure jump process, 281–5, 323–5, 327–30
simple jump-diffusion process, 325–7
jumps, 95, 243–330
see also Poisson process
Girsanov’s theorem, 298–322
risk-neutral measure, 322–30
Kou’s model, 295–8
Laplace transform of first passage time, 83–4
laws of large numbers, formulae, 354–5
Lebesgue–Stieltjes integral, 246
Lévy processes, 55, 119–23, 207, 209
L’Hospital rule, formulae, 336
linearity, properties of conditional
expectation, 3, 44, 45
lognormal distribution, 20–2, 129–32, 134–5,
142–4, 169–71, 283–8, 351
Maclaurin series, 335
marginal distributions of bivariate normal
distribution, 27
marginal probability density function, 27–9,
346
marginal probability mass function, 343
Markov property of a geometric Brownian
motion, 70–1
Markov property of Poisson process, 244–5,
261–2, 268–9
Markov property of Wiener processes, 52,
68–71, 97
definition and formulae, 51–2, 68–71
Markov’s inequality, definition and
formulae, 40
martingale representation theorem, 187–8,
190, 192–4, 219–26
martingales, 52–3, 71–84, 96–123, 127–8,
185–242, 245, 262–7, 279–81, 299–330
see also equivalent...; stochastic processes;
Wiener processes
compound Poisson process, 265–8, 322–4
continuous processes property, 53
discrete processes property, 53
theorems, 53–5, 187–92
maximum of two correlated normal
distributions, definition, 29–32
mean value theorem, 105–6
mean-reversion, 134–5, 139–44, 169–71,
288–95
see also geometric mean-reverting process
measurability, properties of conditional
expectation, 3, 43–4, 45, 46, 199–200
measurable space, definition, 2
measure theory, definition, 2
measures, 2, 185–242, 249–51,
301–30
see also Girsanov’s theorem; real-world...;
risk-neutral...
change of measure, 185–242, 249–51,
301–30
Merton’s model, definition and
formulae, 285–8, 327–30
minimum and maximum of two correlated
normal distributions, 29–32
Minkowski’s inequality, 42
monotone convergence theorem, 354
monotonicity, properties of conditional
expectation, 3, 44–45
multi-dimensional diffusion process
see also diffusion
backward Kolmogorov equation, 101–2
definition and formulae, 99–102,
155–83
Feynman–Kac theorem, 178–80
forward Kolmogorov equation, 101
problems and solutions, 155–83
multi-dimensional Girsanov theorem, 190–1,
208–9, 249–51
multi-dimensional It¯o formulae, 99–100
multi-dimensional It¯o formulae jump-diffusion
process, 247–9
multi-dimensional Lévy characterisation
theorem, 121–3, 209
multi-dimensional martingale representation
theorem, 191–2
multi-dimensional Novikov condition,
207–8
multi-dimensional Wiener processes, 54, 64–8,
99–102, 163–6
multiplication, probability concepts, 341
multivariate normal distribution, 353
see also normal distribution
mutually exclusive events, probability
concepts, 12, 341

Index
377
negative binomial distribution, 350
normal distribution, 17–24, 29–32, 54, 62–3,
117–18, 128–9, 132–3, 135–8, 144,
170–1, 178, 206–7, 284–5, 348, 351,
361–3
see also bivariate...; folded...; log...;
multivariate...
minimum and maximum of two correlated
normal distributions, 29–32
Novikov’s conditions, 194–6, 207–8
numéraire, definition, 191–2
one-dimensional diffusion process, 97–9,
123–55, 178
see also diffusion
backward Kolmogorov equation, 149–50
definition and formulae, 97–9, 147–53
Feynman–Kac formulae, 147–9, 178
forward Kolmogorov equation, 150–53
one-dimensional Girsanov theorem, 189–90,
205–7
one-dimensional It¯o formulae for
jump-diffusion process, 246–7
one-dimensional Lévy characterisation
theorem, 119–21, 207
one-dimensional martingale representation
theorem, 187–8, 190
one-dimensional random walk
see also random walks
backward Kolmogorov equation, 153–4
forward Kolmogorov equation, 154–5
optional stopping (sampling) theorem, 53,
80–4, 218–19, 232
ordinary differential equations, 125–6, 357–9
Ornstein–Uhlenbeck process,
definition, 132–3, 134–5, 288–91, 292–5
parabolic partial differential equations, 97
see also backward Kolmogorov...;
Black–Scholes...; diffusion...; forward
Kolmogorov...
partial averaging property, 3, 43, 45, 46–7,
201–3
partial differential equations (PDEs), 52,
97–102, 149–53, 178–80
see also backward Kolmogorov...;
Black–Scholes...; forward
Kolmogorov...; parabolic...
concepts, 97–102, 147–53, 168, 178–80
stochastic differential equations, 97–102,
168, 178–80
partition, probability concepts, 341
PDEs see partial differential equations
pdf see probability density function
physical measure see real-world measure
Poisson distribution, 13–14, 350
Poisson process, 95, 243–330
see also compensated...; compound...; Cox...;
jump...
Girsanov’s theorem, 249–51, 298–322
Markov property, 244–51, 261–3, 268
positivity, properties of conditional
expectation, 3, 44
principle of inclusion and exclusion for
probability, definition, 7–10
probability density function (pdf), 14–17,
20–2, 24–30, 35–40, 58–60, 84–9, 98,
150–3, 180–3, 214–18, 249–51, 256–9,
344, 345, 348–53
probability mass function, 11–13, 250, 257–8,
306–30, 342, 348, 349
probability spaces, 2–3, 4–11, 43–9, 52–93,
187–242
definition, 2
probability theory, 1–49, 185, 189–92, 341–55
formulae, 341–55
properties of characteristic function, 348
properties of conditional expectation, 3, 41–9,
192–4, 197–202, 269–72, 284–5
properties of expectation, 3, 40–9, 75–6,
192–4, 197–202, 270–2, 284–5, 347
definition and formulae, 40–9, 347
problems and solutions, 40–9
properties of moment generating, 348
properties of normal distribution, 17–20,
34–40
properties of the Poisson process, problems and
solutions, 243–51, 251–81
properties of variance, 347
pure birth process, 255–6
pure jump process, definition and
formulae, 281–85, 323–5
quadratic variation property of Wiener
processes, 54, 89–93, 96–123, 206–7,
209, 274–7
Radon–Nikod´ym derivative, 43, 188, 190,
191–2, 196–200, 212–13, 218–19,
222–5, 234–5, 239–42, 249–51, 298–330
random walks, 51–93, 95, 153–5, 180–3
see also Brownian motion; continuous-time
processes; symmetric...; Wiener
processes
definition, 51–5, 153–5, 180–3
real-world measure, 185, 189–242
see also Girsanov’s theorem
definition, 185
reflection principle, 53–5, 84–9, 361–3
Black equation, 362
Black–Scholes equation, 54, 361
definition, 53–4, 60, 84–9

378
Index
reflection principle (continued)
Garman–Kohlhagen equation, 362–3
Wiener processes, 53–5, 60, 84–9
risk-neutral measure, 53, 185, 188–242,
322–30
see also equivalent martingale...; Girsanov’s
theorem
definition, 185, 188–9, 221–42, 322–30
FX, 238–42
jump processes, 322–30
problems and solutions, 221–42,
322–30
running maximum and minimum of a Wiener
process, Girsanov’s theorem, 214–18
sample space, definition, 1, 4, 341–2
scaled symmetric random walk, 51–5
see also random walks
SDEs see stochastic differential equations
second fundamental theorem of asset pricing,
definition, 233
second-order ordinary differential equations
formulae, 358–9
variation of parameters, 358–9
self-financing trading strategy, 186–8, 192,
225–7, 236–8, 326–30
sets, 1, 2–11
definition, 1
𝜎-sigma-algebra, 1–5, 43–9, 201–5, 261
simple jump process, 322–3
simple jump-diffusion process, 325–7
simple process see elementary process
skew, 54
speculation uses of derivatives, 185
square integrable random variable, 95–6, 353
standard Wiener processes, 51–93, 95–183,
185–242, 244–51, 277–81, 285–98,
303–30
covariance of two standard Wiener
processes, 57–60, 64–8, 117–18
definition, 52, 54, 189–90, 250–1, 325–6
joint distribution of standard Wiener
processes, 58–63, 64, 158–9, 165–6
stationary and independent increments, Poisson
process, 259–81
stochastic differential equations
(SDEs), 95–183, 237–42, 246–51,
315–30, 360–1
definition, 95–102, 246–9
integral calculus contrasts, 95–6
partial differential equations, 97–102, 168,
178–80
stochastic processes, 2, 51, 52, 185–242,
243–330
see also martingales; Poisson...; Wiener...
definitions, 2, 51, 52
stochastic volatility, 54, 95, 175–8
stopping times, Wiener processes, 53–5, 80–9,
218–19, 232
Stratonovich integral, 103–6
the strong law of large numbers, formulae, 355
strong Markov property, 52, 84–9
submartingales, 53, 75–80
supermartingales, 53, 76–9
symmetric random walk, 51–68, 88–9, 153–5,
180–3
see also random walks
time inversion, Wiener processes, 62–3
time reversal, Wiener processes, 63–4
time shifting, Wiener processes, 61
total probability of all possible values,
formulae, 342–5
tower property, conditional expectation, 3, 46,
49, 192–4, 197–9, 270–4, 284–5
trading strategy, 186–8, 191–2, 225–7, 236–8,
326–30
transition probability density function, 98–102,
149–53
see also backward Kolmogorov...; forward
Kolmogorov...
two-dimensional random walk
see also random walks
backward Kolmogorov equation, 180–1
forward Kolmogorov equation, 181–3
uniform distribution, 350
union, probability concepts, 341
univariate continuous random variables, 344–7
see also continuous...
univariate discrete random variables, 342–4
see also discrete...
variance, 13–49, 52–93, 115–83, 264–330,
342, 343, 345, 346, 347
see also covariance...
constant elasticity of variance model, 145–6
volatilities, 54, 95–183, 185–242, 285–330,
361–3
see also local...; stochastic...
the weak law of large numbers, formulae, 354
Wiener processes, 51–93, 95–183, 185–242,
242, 246–51, 277–81, 285–98, 303–30,
360–63
see also Brownian motion; diffusion...;
martingales; random walks
covariance of two standard Wiener
processes, 57–60, 64–8, 115–18

Index
379
first passage time, 53, 76–89, 218–19
joint distribution of standard Wiener
processes, 58–60, 64, 158, 165–6
Markov property, 52, 68–71, 97–102
multi-dimensional Wiener processes, 54,
64–8, 99–102, 163–6
quadratic variation property, 54, 89–93,
96–123, 206–7, 209, 274–5
reflection principle, 54–5, 60, 84–9
running maximum and minimum of a Wiener
process, 214–18

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.


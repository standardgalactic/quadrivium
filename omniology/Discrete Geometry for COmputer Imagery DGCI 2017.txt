Walter G. Kropatsch
Nicole M. Artner
Ines Janusch (Eds.)
 123
LNCS 10502
20th IAPR International Conference, DGCI 2017
Vienna, Austria, September 19–21, 2017
Proceedings
Discrete Geometry
for Computer Imagery

Lecture Notes in Computer Science
10502
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7412

Walter G. Kropatsch
• Nicole M. Artner
Ines Janusch (Eds.)
Discrete Geometry
for Computer Imagery
20th IAPR International Conference, DGCI 2017
Vienna, Austria, September 19–21, 2017
Proceedings
123

Editors
Walter G. Kropatsch
TU Wien
Vienna
Austria
Nicole M. Artner
TU Wien
Vienna
Austria
Ines Janusch
TU Wien
Vienna
Austria
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-66271-8
ISBN 978-3-319-66272-5
(eBook)
DOI 10.1007/978-3-319-66272-5
Library of Congress Control Number: 2017950082
LNCS Sublibrary: SL6 – Image Processing, Computer Vision, Pattern Recognition, and Graphics
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Vienna was the location of the 20th international conference on Discrete Geometry for
Computer Imagery (DGCI). It followed the tradition of previous events, which have
been held alternately in France and outside France at an approximate interval of 18
months. DGCI 2017 attracted a number of research contributions from academic and
research institutions in our ﬁeld. A total of 36 papers were submitted from all over the
world. Papers were ﬁrst assigned to one of the main topics of the conference, each one
led by two or three members of the Program Committee (PC). Then, the members
of the PC selected at least three reviewers for each paper. After all the reviews had been
collected by the PC members, they generated a meta-review based on which the authors
could explain potential misunderstandings and propose some corrections for the ﬁnal
version. Following this “rebuttal” phase, 28 papers were accepted by the program
chairs and their authors were invited to present their work at DGCI 2017.
The program consisted of 18 oral and 10 poster presentations. In addition, three
internationally well-known speakers were invited for keynote lectures: Helmut Pott-
mann (TU Wien, Austria), Michael Wilkinson (Univ. of Groningen, The Netherlands),
and Eric Andres (Univ. Poitiers, France). We are happy that all invited speakers were
willing to also contribute a written article to the proceedings. These proceedings also
follow the tradition of DGCI and appear in the LNCS series of Springer, a publisher
celebrating its 175th birthday this year.
We are also very thankful to Springer for sponsoring a best student paper award for
DGCI 2017. Furthermore, DGCI 2017 received the sponsorship of the International
Association of Pattern Recognition (IAPR). It is planned to produce a special issue
of the Journal of Mathematical Imaging and Vision from extended versions of excellent
contributions presented at DGCI 2017.
We would like to thank all contributors, the invited speakers, all PC members and
reviewers, the members of the Steering Committees, and all those who made this
conference happen. Furthermore, we are grateful to our ﬁnancial support institutions.
Last but not least, we thank all the participants and hope that everyone found great
interest in the DGCI 2017 program and also had a very good time in our city of Vienna.
July 2017
Walter G. Kropatsch
Nicole M. Artner
Ines Janusch

Organization
Program Chairs
Walter G. Kropatsch
TU Wien, Austria
Nicole M. Artner
TU Wien, Austria
Ines Janusch
TU Wien, Austria
David Coeurjolly
LIRIS, France
Program Committee
Joost K. Batenburg
CWI, Amsterdam, The Netherlands
Isabelle Bloch
LTCI, Télécom ParisTech, Paris, France
Srecko Brlek
Université du Québec à Montréal, Canada
Sara Brunetti
University of Siena, Italy
Michel Couprie
LIGM, University of Paris-Est, France
Guillaume Damiand
LIRIS, CNRS, Lyon, France
Yan Gerard
ISIT, Auvergne University, France
Rocio Gonzalez-Diaz
University of Seville, Spain
Yukiko Kenmochi
LIGM, University of Paris-Est, France
Bertrand Kerautret
Loria, University of Lorraine, France
Christer Kiselman
Uppsala University, Sweden
Jacques-Olivier Lachaud
LAMA, University Savoie Mont Blanc, France
Pawel Pilarczyk
IST Austria
Xavier Provençal
LAMA, University Savoie Mont Blanc, France
Gabriella Sanniti di Baja
ICAR, CNR, Italy
Robin Strand
Centre for Image Analysis, Uppsala, Sweden
Imants Svalbe
Monash University, Melbourne, Australia
Reviewers
Andreas Alpers
Frosini Andrea
Vialard Anne
Teo Asplund
Peter Balazs
Fabien Baldacci
Reneta Barneva
Etienne Baudrier
Partha Bhowmick
Silvia Biasotti
Arindam Biswas
Gunilla Borgefors
Nicolas Boutry
Shekhar Chandra
Isabelle
Debled-Rennesson
Eric Domenjoud
Paolo Dulio
Henri-Alex Esbelin
Jean-Marie Favreau
Fabien Feschet
Largeteau-Skapin Gaëlle
Yan Gerard
Aldo Gonzalez-Lorenzo
Jeanpierre Guedon
Lajos Hajdu
Cris L. Luengo Hendriks
Damien Jamet
Maria Jose Jimenez
Andrew Kingston

Ullrich Köthe
Jean-Philippe Labbé
Claudia Landi
Pascal Lienhardt
Joakim Lindblad
Filip Malmberg
Loïc Mazo
Christian Mercat
Benoît Naegel
Benedek Nagy
Laurent Najman
Phuc Ngo
Nicolas Normand
Laszlo Nyul
Silvia Pagani
Nicolas Passat
Samuel Peltier
Christophe Picouleau
Daniel Prusa
Eric Remy
Ana Romero
Tristan Roussillon
Apurba Sarkar
Isabelle Sivignon
Natasa Sladoje
Robin Strand
Akihiro Sugimoto
Hugues Talbot
Oriol Ramos Terrades
Edouard Thiel
Darren Thompson
Laure Tougne
Antoine Vacavant
Jonathan Weber
Günter M. Ziegler
Rita Zrour
Henri der Sarkissian
VIII
Organization

Contents
Invited Talks
Freeform Architecture and Discrete Differential Geometry . . . . . . . . . . . . . .
3
Helmut Pottmann and Johannes Wallner
A Guided Tour of Connective Morphology: Concepts, Algorithms,
and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
Michael H.F. Wilkinson
Collaborating with an Artist in Digital Geometry . . . . . . . . . . . . . . . . . . . .
19
Eric Andres, Gaelle Largeteau-Skapin, and Aurélie Mourier
Geometric Transforms
Honeycomb Geometry: Rigid Motions on the Hexagonal Grid . . . . . . . . . . .
33
Kacper Pluta, Pascal Romon, Yukiko Kenmochi, and Nicolas Passat
Large Families of “Grey” Arrays with Perfect Auto-Correlation
and Optimal Cross-Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Imants Svalbe, Matthew Ceko, and Andrew Tirkel
The Minimum Barrier Distance: A Summary of Recent Advances. . . . . . . . .
57
Robin Strand, Krzysztof Chris Ciesielski, Filip Malmberg,
and Punam K. Saha
Convexity-Preserving Rigid Motions of 2D Digital Objects . . . . . . . . . . . . .
69
Phuc Ngo, Yukiko Kenmochi, Isabelle Debled-Rennesson,
and Nicolas Passat
Weighted Distances on the Trihexagonal Grid . . . . . . . . . . . . . . . . . . . . . .
82
Gergely Kovács, Benedek Nagy, and Béla Vizvári
An Integer Programming Approach to Characterize Digital Disks
on the Triangular Grid. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Gergely Kovács, Benedek Nagy, and Béla Vizvári
Discrete Tomography
High-Level Algorithm Prototyping: An Example Extending
the TVR-DART Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
Axel Ringh, Xiaodong Zhuge, Willem Jan Palenstijn,
Kees Joost Batenburg, and Ozan Öktem

A Parametric Level-Set Method for Partially Discrete Tomography . . . . . . . .
122
Ajinkya Kadu, Tristan van Leeuwen, and K. Joost Batenburg
Maximal N-Ghosts and Minimal Information Recovery
from N Projected Views of an Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
Imants Svalbe and Matthew Ceko
Ambiguity Results in the Characterization of hv-convex Polyominoes
from Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Elena Barcucci, Paolo Dulio, Andrea Frosini, and Simone Rinaldi
Mojette Transform on Densest Lattices in 2D and 3D . . . . . . . . . . . . . . . . .
159
Vincent Ricordel, Nicolas Normand, and Jeanpierre Guédon
Fuzzy Directional Enlacement Landscapes . . . . . . . . . . . . . . . . . . . . . . . . .
171
Michaël Clément, Camille Kurtz, and Laurent Wendling
Discrete Modelling and Visualization
An Introduction to Gamma-Convergence for Spectral Clustering . . . . . . . . . .
185
Aditya Challa, Sravan Danda, B.S. Daya Sagar, and Laurent Najman
Digital Surface Regularization by Normal Vector Field Alignment . . . . . . . .
197
David Coeurjolly, Pierre Gueth, and Jacques-Olivier Lachaud
Morphological Analysis
Opening Holes in Discrete Objects with Digital Homotopy . . . . . . . . . . . . .
213
Aldo Gonzalez-Lorenzo, Alexandra Bac, and Jean-Luc Mari
Well-Composedness in Alexandrov Spaces Implies Digital
Well-Composedness in Zn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Nicolas Boutry, Laurent Najman, and Thierry Géraud
Discrete Shape Representation, Recognition and Analysis
Heat Kernel Laplace-Beltrami Operator on Digital Surfaces . . . . . . . . . . . . .
241
Thomas Caissard, David Coeurjolly, Jacques-Olivier Lachaud,
and Tristan Roussillon
Efficiently Updating Feasible Regions for Fitting Discrete
Polynomial Curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
Fumiki Sekiya and Akihiro Sugimoto
A New Shape Descriptor Based on a Q-convexity Measure . . . . . . . . . . . . .
267
Péter Balázs and Sara Brunetti
X
Contents

Recognition of Digital Polyhedra with a Fixed Number of Faces
Is Decidable in Dimension 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Yan Gérard
Reconstructions of Noisy Digital Contours with Maximal Primitives
Based on Multi-scale/Irregular Geometric Representation
and Generalized Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
Antoine Vacavant, Bertrand Kerautret, Tristan Roussillon,
and Fabien Feschet
Discrete and Combinatorial Topology
Euclidean and Geodesic Distance Profiles . . . . . . . . . . . . . . . . . . . . . . . . .
307
Ines Janusch, Nicole M. Artner, and Walter G. Kropatsch
Greyscale Image Vectorization from Geometric Digital
Contour Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
Bertrand Kerautret, Phuc Ngo, Yukiko Kenmochi, and Antoine Vacavant
Discrete Models and Tools
The Boolean Map Distance: Theory and Efficient Computation . . . . . . . . . .
335
Filip Malmberg, Robin Strand, Jianming Zhang, and Stan Sclaroff
Fast and Efficient Incremental Algorithms for Circular and Spherical
Propagation in Integer Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
Shivam Dwivedi, Aniket Gupta, Siddhant Roy, Ranita Biswas,
and Partha Bhowmick
Models for Discrete Geometry
Study on the Digitization Dual Combinatorics and Convex Case. . . . . . . . . .
363
Loïc Mazo and Étienne Baudrier
Algorithmic Construction of Acyclic Partial Matchings
for Multidimensional Persistence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Madjid Allili, Tomasz Kaczynski, Claudia Landi, and Filippo Masoni
Digital Primitives Defined by Weighted Focal Set. . . . . . . . . . . . . . . . . . . .
388
Eric Andres, Ranita Biswas, and Partha Bhowmick
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
Contents
XI

Invited Talks

Freeform Architecture and Discrete
Diﬀerential Geometry
Helmut Pottmann1(B) and Johannes Wallner2
1 Center for Geometry and Computational Design, TU Wien,
Wiedner Hauptstraße 8–10, 1040 Wien, Austria
pottmann@geometrie.tuwien.ac.at
2 Institut f¨ur Geometrie, TU Graz, Kopernikusgasse 24, 8010 Graz, Austria
Abstract. Freeform structures play an important role within contempo-
rary architecture. While there is a wealth of excellent tools for the digital
design of free-form geometry, the actual fabrication on the architectural
scale is a big challenge. Key issues in this context are free-form surfaces
composed of panels which can be manufactured at reasonable cost, and
the geometry and statics of the support structure. The present article is
an extended abstract of a talk on the close relation between geometric
computing for free-form architecture and discrete diﬀerential geometry.
It addresses topics such as skins from planar, in particular quadrilateral
panels, geometry and statics of supporting structures, structures in force
equilibrium.
1
Introduction
The mathematical and computational challenges posed by free-form shapes in
architecture are twofold. One is rationalization which means approximating a
given design surface by a collection of smaller parts which can be individually
manufactured and put together. There is a great variety of constraints imposed
on the individual parts, most having to do with manufacturing. The second
challenge is design of free forms. The goal here is to develop tools which allow
the user to interactively design free forms, such that key aspects of statics and
fabrication are taken into account directly in the design phase. Meanwhile there is
a wealth of results on these topics, and we want to point to the survey article [13].
2
Freeform Skins from Planar Panels and Associated
Support Structures
Steel-glass constructions usually require a decomposition of freeform skins into
ﬂat panels, which leads us to the question of rationalization with polyhedral sur-
faces, and designing with polyhedral surfaces. The combinatorics of meshes plays
an important role here: It is very easy to represent a given shape by a triangle
mesh, and in fact the majority of freeform skins which exist are based on trian-
gle meshes. However there are drawbacks: On average 6 edges meet in a vertex,
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 3–8, 2017.
DOI: 10.1007/978-3-319-66272-5 1

4
H. Pottmann and J. Wallner
so structures based on triangle meshes have complicated nodes. Further, they tend
to be heavier than structures based on quad meshes. This has led to a new line of
research into PQ (planar quad) meshes, which are meshes whose faces are planar
quadrilaterals. Here an important link to discrete diﬀerential geometry is estab-
lished: Combinatorially regular quad meshes decompose into two families of mesh
polylines which can be seen as discrete versions of the parameter lines on a smooth
surface. A quad mesh is then interpreted as a discrete version of a parametric sur-
face. Properties of quad meshes relevant to architecture turn out to be equiva-
lent to properties relevant in discrete diﬀerential geometry (in particular, the inte-
grable systems viewpoint of discrete diﬀerential geometry, see [3]).
This connection between smooth surfaces and discrete surfaces is very impor-
tant in investigating the degrees of freedom available for rationalization and
design: E.g. a PQ mesh constitutes a discrete version of a so-called conjugate net-
work of curves [9]. Meshes where the edge polylines appear smooth will need to
approximate a conjugate network of curves. The conjugate networks are known
and in theory there are many, but we nevertheless can draw the conclusion that
in connection with practical considerations (e.g. angles between edges) there
might be little ﬂexibility or even no satisfactory network at all which serves as
guidance for a PQ mesh (see Fig. 1).
Fig. 1. Diﬀerential geometry informing rationalization. (a) The Cour Visconti roof in
the Louvre (image courtesy Waagner-Biro Stahlbau). It was intended to be built in
a lightweight way, possibly as a quad mesh. (b) A rationalization of this surface as
a quad mesh with planar faces and smooth edge polylines must follow a conjugate
network of curves, but these networks have unacceptable singularities. (c) If zigzags
are allowed, rationalization as a PQ mesh with regular combinatorics is possible. For
the actual roof, however, a diﬀerent solution with both triangular and quadrilateral
faces was found.

Feeform Architecture and DDG
5
An important special case are nearly rectangular panels. Aside from aesthetics
there are fundamental geometric reasons for constraining a PQ mesh to some
form of orthogonality between edges. As it turns out, such meshes are discrete
versions of principal curve networks, and the known nice behaviour of the surface
normals along the curves of such a network translates to good properties of the
support structure associated with the quad mesh, see Fig. 2. One is able to design
so-called torsion-free nodes [9,11]. Research in this direction also led to progress
in discrete diﬀerential geometry itself, in particular a new curvature theory for
discrete surfaces [4]. Direct design of torsion-free support structures with quad
combinatorics is related to special parametrizations of congruences. This word
refers to a 2-dimensional system of straight lines and constitutes a classical topic
of diﬀerential geometry. Its discrete incarnation turned out to be quite useful
and has been explored systematically. We used it in connection with shading
and guiding light by reﬂection, see [17] and Fig. 3.
Fig. 2. Torsion-free support structures. The Chadstone shopping mall in Melbourne
features a steel-glass roof in the shape of a planar quad mesh. The member corre-
sponding to an edge is aligned along the support plane (yellow) of that edge, and the
intersection of members in a node is deﬁned by the node axis (red) where support planes
meet. This behaviour of node axes is analogous to the behaviour of surface normals
along principal curvature lines (original photo: T. Burgess, imageplay).
Fig. 3. Torsion-free support structures for shading and lighting. By cutting out and
assembling the strips shown above one creates a torsion-free support structure capable
of reﬂecting light into prescribed patterns. This arrangement of planes and lines dis-
cretizes the notion of torsal parametrization of a line congruence. The strips correspond
to the two families of developable surfaces which make up the congruence (the system
of normals of a surface along principal curvature lines is a special instance of this).

6
H. Pottmann and J. Wallner
The previous paragraphs did not give an exhaustive list of the correspon-
dences between discrete surfaces and smooth parametric surfaces which have
already been used in the context of freeform architecture. In particular we did
not mention semidiscrete surfaces relevant to structures with bent glass [12].
3
Structures in Static Equilibrium
Statics obviously is of paramount importance in architecture and building con-
struction. It is therefore important that aspects of statics play a role already
in the ﬁrst stages of design. It is a long-term goal to create design tools which
incorporate constraints relating to geometry, fabrication and statics while being
still fast enough to allow interactive modeling. We are currently far from this
goal, but partial results have been achieved. We start our summary by mention-
ing the thrust network method [1,2]: Maxwell’s ideas on graphical statics are the
basis of a method to treat systems of equilibrium forces which act in surface-like
geometries. By separating vertical and horizontal components one is led to a
discrete Airy potential polyhedron, which is a ﬁnite element discretization of the
Airy stress potential well known in 2D elasticity theory. Compressive stresses
are characterized by convexity of the stress potential.
A particularly nice application of this method is self-supporting masonry
which is stable even without mortar, see Fig. 4. It is possible to interpret forces
resp. stresses in diﬀerential-geometric terms, and we refer to [14,16] for this
“geometrization” of the force balance condition, and for a treatment of the so-
called isotropic diﬀerential geometry which occurs here. The direct interactive
design of meshes (in particular polyhedral surfaces) with additional force balance
conditions is a special case of constrained geometric modeling, see [15].
Recently we have worked on material-minimizing structures, see Fig. 4. This
optimization problem was originally proposed in a groundbreaking paper by
Fig. 4. Self-supporting and weight-optimal structures. (a) This masonry vault with
holes contains a network of compressive forces which is in equilibrium with the dead-
load, implying the remarkable fact of stability of the structure when built of bricks even
without mortar. Interactive design of such self-supporting surfaces is possible [15,16].
(b) The search for quad meshes with planar faces and minimal weight in the sense of
M.G.M. Michell’s limit of economy is converted into computing a variant of principal
curves, by a suitable diﬀerential-geometric interpretation [8].

Feeform Architecture and DDG
7
M.G.M. Michell [10] and is meanwhile formulated in modern language [18]. Our
work, like others mentioned in this paper, is based on a diﬀerential-geometric
interpretation of the subject of interest which, in this case, is the volume of
members of a structure based on a mesh, and also the forces acting in these
members. For example, 2-dimensional optimal trusses are characterized by Airy
potential surfaces of minimal total curvature in the sense of isotropic geometry.
This topic and its extension to shells is treated by [8] (Fig. 4).
Fig. 5. Nonstandard notions of fairness. Here a given smooth surface is approximated
by a polyhedral surface (a) of prescribed local combinatorics (b). The concept of fairness
employed in the computation is based on existence of local approximate symmetries.
4
On Fairness, the Importance of Regularizers and
Structures Beyond Discrete Diﬀerential Geometry
In all examples mentioned above, fairness plays an important role in identifying
those discrete structures which meaningfully correspond to smooth objects. On
a technical level, fairness functionals are used as regularizers in optimization and
in iterative constraint solvers. There are, however, many diﬀerent ways to express
fairness computationally. The standard quadratic fairness energies composed of
iterated diﬀerences might not be appropriate for meshes like the one shown by
Fig. 1c. The zigzag polylines might be fully intentional, but they cause high (bad)
values of such a fairness energy. Recently, alternative approaches to fairness have
been successfully employed in creating polyhedral patterns [6]. They are based on
existence of local approximate symmetries. An interpretation in terms of standard
concepts of discrete diﬀerential geometry is still open. A diﬃcult topic in general
are fairness functionals of high nonlinearity, e.g. those involving kink angles. A
fairness measure based on angles only [7] has led to a new concept of smoothness
of discrete surfaces [5]. Recently we have investigated a functional deﬁned as the
sum of edge lengths times absolute value of kink angles. Its “isotropic” version
surprisingly turns up in connection with material minimization (see previous
paragraph). The shape of minimizers is a topic of current research; we conjecture
that at least in negatively curved areas they are principal meshes.

8
H. Pottmann and J. Wallner
Acknowledgments. This research has been supported by the Austrian Science Fund
(FWF) under grant no. I-2978, within the framework of the SFB-Transregio Pro-
gramme Discretization in Geometry and Dynamics.
References
1. Adriaenssens, S., Block, P., Veenendaal, D., Williams, C. (eds.): Shell Structures
for Architecture. Taylor & Francis, Routledge (2014)
2. Block, P., Ochsendorf, J.: Thrust network analysis: a new methodology for three-
dimensional equilibrium. J. Int. Assoc. Shell Spatial Struct. 48, 167–173 (2007)
3. Bobenko, A., Suris, Y.: Discrete diﬀerential geometry: Integrable structure. Amer-
ican Math. Soc. (2009)
4. Bobenko, A., Pottmann, H., Wallner, J.: A curvature theory for discrete surfaces
based on mesh parallelity. Math. Annalen 348, 1–24 (2010)
5. G¨unther, F., Jiang, C., Pottmann, H.: Smooth polyhedral surfaces. Preprint
(arXiv:1703.05318)
6. Jiang, C., Tang, C., Vaxman, A., Wonka, P., Pottmann, H.: Polyhedral patterns.
ACM Trans. Graph. 34(6), article 172 (2015)
7. Jiang, C., G¨unther, F., Wallner, J., Pottmann, H.: Measuring and controlling
fairness of triangulations. In: Advances in Architectural Geometry 2016, VDF
Hochschulverlag, ETH Z¨urich, 2016, pp. 24–39 (2016)
8. Kilian, M., Pellis, D., Wallner, J., Pottmann, H.: Material-minimizing forms and
structures (2017, submitted for publication)
9. Liu, Y., Pottmann, H., Wallner, J., Yang, Y.-L., Wang, W.: Geometric modeling
with conical meshes and developable surfaces. ACM Trans. Graph. 25(3), 681–689
(2006)
10. Michell, A.G.M.: The limit of economy of material in frame-structures. Phil. Mag.,
Ser. VI 8, 589–597 (1904)
11. Pottmann, H., Liu, Y., Wallner, J., Bobenko, A., Wang, W.: Geometry of multi-
layer freeform structures for architecture. ACM Trans. Graph. 26(3), article 65
(2007)
12. Pottmann, H., Schiftner, A., Bo, P., Schmiedhofer, H., Wang, W., Baldassini, N.,
Wallner, J.: Freeform surfaces from single curved panels. ACM Trans. Graph.
27(3), article 76 (2008)
13. Pottmann, H., Eigensatz, M., Vaxman, A., Wallner, J.: Architectural geometry.
Comput. Graph. 47, 145–164 (2015)
14. Strubecker, K.: Airy’sche Spannungsfunktion und isotrope Diﬀerentialgeometrie.
Math. Zeitschrift 78, 189–198 (1962)
15. Tang, C., Sun, X., Gomes, A., Wallner, J., Pottmann, H.: Form-ﬁnding with poly-
hedral meshes made simple. ACM Trans. Graph. 33(4), article 70 (2014)
16. Vouga, E., H¨obinger, M., Wallner, J., Pottmann, H.: Design of self-supporting
surfaces. ACM Trans. Graph. 31(4), article 87 (2012)
17. Wang, J., Jiang, C., Bompas, P., Wallner, J., Pottmann, H.: Discrete line congru-
ences for shading and lighting. Comput. Graph. Forum 32, 53–62 (2013)
18. Whittle, P.: Networks - Optimisation and Evolution. Cambridge University Press,
Cambridge (2007)

A Guided Tour of Connective Morphology:
Concepts, Algorithms, and Applications
Michael H.F. Wilkinson(B)
Johann Bernoulli Institute for Mathematics and Computer Science,
University of Groningen, P.O. Box 407, 9700 AK Groningen, The Netherlands
m.h.f.wilkinson@rug.nl
Abstract. Connective morphology has been an active area of research
for more than two decades. Based on an abstract notion of connectivity,
it allows development of perceptual grouping of pixels using diﬀerent con-
nectivity classes. Images are processed based on these perceptual groups,
rather than some rigid neighbourhood imposed upon the image in the
form of a ﬁxed structuring element. The progress in this ﬁeld has been
threefold: (i) development of a mathematical framework; (ii) develop-
ment of fast algorithms, and (iii) application of the methodology in very
diverse ﬁelds. In this talk I will review these developments, and describe
relationships to other image-adaptive methods. I will also discuss the
opportunities for use in multi-scale analysis and inclusion of machine
learning within connected ﬁlters.
Keywords: Connectivity
·
Connected
ﬁlters
·
Mathematical
morphology · Algorithms
1
Introduction
Connected ﬁlters, and more generally connective morphology [1,2] are relative
newcomers in the ﬁeld of mathematical morphology. Rather than being based on
adjunctions of erosions and dilations by structuring elements, they are based on
a generalised notion of connectivity, and connectivity openings form the building
blocks of operators. The key property of these image operators is that they do
not consider the image as a collection of pixels, but more as a collection of
connected structures, referred to as (quasi) ﬂat-zones or connected components.
They work by merging these connected structures and assigning new grey-levels
or colours to them, based on certain criteria. This property means that they are
strictly edge preserving: they cannot introduce new edges in an image. Their
use ranges from simple image ﬁltering, through detection and enhancement of
speciﬁc structures to hierarchical segmentation.
As so often happens, the development of these ﬁlters originally stemmed
from practical needs, and only later a rigorous mathematical framework was
set up. Likewise, the initial algorithms were comparatively ineﬃcient, but as
the potential power of these ﬁlters became apparent, a great deal of eﬀort was
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 9–18, 2017.
DOI: 10.1007/978-3-319-66272-5 2

10
M.H.F. Wilkinson
made into development of eﬃcient algorithms. In this review I will ﬁrst describe
the history of these image operators, introducing the most important types of
connected operators. After that, I will present the lattice-theoretical framework
of connectivity classes, and several generalisations that have been proposed. This
is followed by a discussion of the most important algorithms, both sequential and
parallel, for eﬃcient computation of these operators. Finally, I will discuss future
perspectives.
2
A Brief History
The very ﬁrst connected ﬁlter was published in the thesis of Klein [3], in which
he presented reconstruction operators. The basic reconstruction operator is the
reconstruction f from marker g. In the binary case, all connected components
of f which have a non-empty intersection with g re preserved, all others are
removed. The opening-by-reconstruction is obtained by choosing
g = f ⊖B
(1)
with B some structuring element. This preserves all connected components of
f into which some translate of B ﬁts entirely. If B is a ball of radius r, this
preserves only those connected components with an erosion width larger than
2r. An example is shown in Fig. 1. Quite clearly, the opening by reconstruction
preserves shapes exactly, whereas the regular, structural opening does not.
Fig. 1. A comparison of openings: (a) original image of bacteria; (c) structural opening
by 11 × 11 square; (b) reconstruction of (a) by (b); (d) moment-of-inertia opening with
λ = 114/4.
Area openings were proposed in the early 1990s [4,5] as ﬁlters which remove
connected components with an area smaller than some threshold λ from images.
After this, Breen and Jones [6] proposed attribute openings as a generalisation.
In this case some increasing measure μ(C) of each connected component is com-
puted, and compared to some threshold. An example is shown in Fig. 1(d), where
the measure is the trace of the moment of inertia, and the threshold is set to that
of an 11 × 11 square. The latter is clearly more selective for elongated features.
The above ﬁlters are anti-extensive, i.e. the remove bright, or foreground
structures. Extensive ﬁlters which remove dark, or background structures can

A Guided Tour of Connective Morphology
11
be deﬁned by duality, i.e., by inverting the image, applying the opening (or
thinning) and inverting the result.
The grey-scale case can be obtained by threshold superposition [7]. The eas-
iest way to visualise this process is through the concept variously known as
component trees [8,9], max-trees and min-trees [10], and opening trees [11]. An
example of a max-tree of a simple grey scale image is shown in Fig. 2. It can
be constructed by thresholding the grey level image at all threshold levels, and
computing the connected components, called peak components, of every thresh-
old set. It is easy to see that each peak component at grey level h is nested within
a single peak component at a lower level. Therefore, the peak components can
be organized into a tree structure called the max-tree, because the leaves are
the local maxima. A min-tree is is simply a max-tree of the inverted image, and
component tree may be either.
Input image
Thresholded sets(peaks)
Max-Tree
Fig. 2. A simple grey-scale image, its peak components and max-tree
Once a max-tree has been computed ﬁltering can be done by applying some
decision function which determines which nodes of the tree to keep, and which to
remove. In the simplest case, some increasing attribute, such as area, is computed
for each node, and this is compared to some threshold. In this case we end up
pruning those branches of the tree which contain nodes that are too small to meet
the criterion. All pixels in the removed nodes are assigned the grey levels of the
nearest surviving ancestor. In more complicated cases, the attributes are non-
increasing, and removed and retained nodes along the root path may alternate.
In this we need to make more complicated ﬁltering rules to assign new grey
levels [10,12]. Although we add complexity, it does allow implementation of
scale invariant shape ﬁlters [12,13], which allow ﬁltering of features such as
blood vessels based on shape, rather than size [14,15]. Non-increasing ﬁltering
also takes place in so-called vector-attribute ﬁlters [16,17]. In this case each
node of the tree contains a feature vector describing size, shape, or grey-level
statistics. In the simplest case, we can then retain or remove nodes based on
how closely they resemble some prototype vector, using distance measures like
euclidean or Mahalanobis distances. Machine learning and clustering can also be
used to distinguish diﬀerent classes of nodes [18]. An interesting development is
that of building a component tree of a component tree, and steering the ﬁltering
process from that second tree [19].

12
M.H.F. Wilkinson
Several self-dual ﬁlters have been developed: levelings [20,21] as self-dual
versions of the reconstruction operators, and level-line tree [22–24] also called
tree-of-shapes [25] methods in the case of attribute ﬁlters, which can be seen
as combining a max-tree and min-tree in such a way that the extrema are the
leaves.
All these tree structures allow more than just ﬁltering. As these trees are multi-
scale representations of the image, all kinds of multi-scale analysis is very readily
performed using these trees, including computation of pattern spectra [26–29] and
morphological proﬁles [30,31]. There are also important relations between the var-
ious partition trees and hierarchical segmentation methods [32–34].
Component trees and their relatives require a total order of the pixel values
in order to work, so adaptations are need for work in colour, vector and tensor
images. Approaches include total pre-orders in component trees [35], merging
of trees of shapes from diﬀerent colour bands [36], and area scale spaces for
colour images using local extremal values [37,38]. However, the most common
way to deal with colour, vector or tensor images in connective morphology is
through hierarchies of quasi ﬂat zones [32,39–41]. In principle we start out at
the ﬂat-zones of the image, i.e. connected regions of maximal extent of constant
colour. We then hierarchically merge these into larger components in increasing
order of dissimilarity. The simplest approach only performs binary merges, and
thus creates a binary partition trees [39]. Alternatively, we can allow mergers of
multiple regions simultaneously, creating so-called alpha trees [40].
3
Connectivity Theory
In the discussion above, we implicitly assumed the usual deﬁnition of a connected
component, as a maximal connected subset of some set X. We did not, however
specify what we actually mean by “connected”. Serra [42,43] introduced the gen-
eral notion of connectivity classes. These are essentially families of all elements of
the lattice under consideration that are connected. In the case of binary images
that means the family of all connected sets. In all cases, a connectivity class C
must have the following three properties
1. 0 ∈C
2. C is sup-generating
3. For any set {Ci} ⊂C, with i from some index set, 
i Ci ̸= 0
⇒

i Ci ∈C
This means that the least element of the lattice (∅in the set theoretical case)
is connected, that any element of the lattice can be generated by taking the
supremum of elements of C, and that if the inﬁmum of elements of C is not
0, their supremum is connected. In the set theoretical case this means that if
the intersection of any number of connected sets is not empty, their union is
connected.
It is easily veriﬁed that the standard notions of (path) connectivity in images
adhere to these three axioms. Many others have been proposed in a framework
often called second-generation connectivity, where the standard connectivity is

A Guided Tour of Connective Morphology
13
modiﬁed either by applying operators like dilation, opening or closings to the
image [42,44,45], or by using some second image in so-called mask-based second-
generation connectivity [46]. These methods allow objects broken up by noise to
be considered a single, connected entity, or objects linked by single pixels to be
considered separate. This is done by considering whether pixels are connected
at a given grey level in a modiﬁed image, rather than in the original. A further
extension also assigns grey levels to the edges between pixels, allowing further
freedom in deciding what is connected [47].
3.1
Beyond Connectivity
In [42] suggested that there are generalisations to connectivity that have been
explored further by other researchers. In particular, the notion of hyperconnectiv-
ity has received much attention [48–52]. Initially, Serra changed the third axiom
of connectivity classes, from merely requiring a non-empty intersection for a
group of connected sets to have a connected union, to some stronger require-
ment called the overlap criterion. This might be that the intersection contains a
ball of a certain diameter, leading to viscous hyperconnections [53,54] It has been
shown that this can lead to a family of ﬁlters that bridges the gap between con-
nected ﬁlters and structural morphological ﬁlters [53]. It has since been shown
that the overlap criterion is not required to deﬁne these hyperconnections, but
that other axiomatics can be used [48,52].
Other extensions of connectivity are partial connections [34], and attribute-
space connections [55].
4
Algorithms
In support of all the theoretical developments, algorithms for fast computation of
connected operators have been an important area of research [5,9,10,40,56,57].
For a recent review see [58]. Many of the algorithms centre around component
trees (min-tree, max-tree, tree-of-shapes) on the one hand, and partition trees on
the other (binary partition trees and alpha trees, mainly). In the former category,
there were two diﬀerent approaches, i.e. building the tree from root to leaf,
by recursive or non-recursive ﬂood-ﬁlling [10,59,60], and the reverse approach,
starting from the leaves joining up branches, and ultimately aggregating them
into a single tree. These latter fall into two categories: (i) those using a priority
queue [8], derived from the area opening algorithm of Vincent [5], and (ii) using
Tarjan’s union-ﬁnd [9,61,62] based on the area opening algorithm in [57]. All
algorithms have their pros and cons as reviewed in [58].
An important aspect of eﬃcient computation on modern compute archi-
tectures is the ability to use parallellism, available either through multi-core
processors, or even the graphics processing unit (GPU). Connected ﬁlters are
particularly troublesome for parallel implementation since the processing order
of most algorithms is very much data driven, and the usual approach of dividing
the data over diﬀerent cores (with some degree of overlap) is hampered by the

14
M.H.F. Wilkinson
fact that we do not a priori know how much overlap is needed, because we do
not know the location or borders of connected components. This problem was
addressed in [63], in which it was shown that max-trees or min-trees of disjoint
image regions could be merged into a complete tree, complete with the correct
attribute information. The cost of merging is proportional to the number of grey
levels, worst case, so the algorithm does not work eﬃciently beyond 16 bits per
pixel. A solution has recently been presented by Moschini et al. [64], in which
ﬁrst a root-to-leaf ﬂood-ﬁlling algorithm algorithm is used on a uniformly quan-
tised approximation image, in which the number of grey levels equals the number
of processors, after which a variant of Berger’s leaf-to-root algorithm is used to
reﬁne the tree. These algorithms have been used in various multi-scale analysis
tasks in remote sensing [27,65] and astronomical imaging [28,66].
A limitation of both the above parallel algorithms is that they only work
on shared-memory parallel computers. In practice this means that images up to
a few gigapixel can be processed. This is because the entire max-tree must be
stored in memory. In a recent development, a distributed memory algorithm
suitable for use on clusters has been developed [67]. This does not build a single
max-tree, but rather a forest of trees, each for one tile of the image, which can
be stored on one node of the cluster. After building these trees, only the sub-
tree connected to the tile boundaries are exchanged and merged hierarchically
to correct the topology. Once these corrections have been made, ﬁltering the
individual trees yields the same result as ﬁltering the entire max-tree. The results
show a speed up of up to 103 on 128 cores, in the case of a 153 Gpixel image.
The above algorithms focus on regular path connectivity, but they have been
extended to the case of second-generation connectivity, without loss of eﬃciency
[46,68], and even some hyperconnections [54,69].
Other important algorithms concern building the binary partition tree [39],
the sequential alpha tree algorithm [40], and a parallel algorithm for alpha trees
[70], based on the same merge strategy as [63].
5
Conclusions and Perspectives
Connected ﬁlters are versatile tools for image ﬁltering, feature extraction, object
recognition, and segmentation. An attractive property of these methods is that
they allow modelling of perceptual grouping through changing the notion of
connectivity, and simultaneously allow modelling of object properties through
the choice of attributes used. Finally, it is relatively straightforward to introduce
machine learning into the framework, in the form of a decision function used to
accept or reject connected components.
There are still many issues to be addressed. Algorithmically, we still not
have a solution that would allow us to use the power of GPUs on these methods.
Likewise, the new distributed algorithm for attribute ﬁlters only works up to
16-bits per pixel images, because it uses the same merging algorithm as in [63].
The same holds for [70]. Therefore, extreme-dynamic range versions of these
algorithms are needed, capable of handling ﬂoating point values.

A Guided Tour of Connective Morphology
15
Integration of machine learning is also an area of research that is still in its
infancy, and more work is needed here. This is not restricted to the decision
function needed in attribute ﬁlters, but also in e.g. computation of connectivity
masks, where currently only ad hoc approaches are used in determining the best
connectivity mask to achieve a particular task. Whether it is possible to learn
the best mask is an open question.
References
1. Salembier, P., Wilkinson, M.H.F.: Connected operators: a review of region-based
morphological image processing techniques. IEEE Signal Process. Mag. 26(6), 136–
157 (2009)
2. Salembier, P., Serra, J.: Flat zones ﬁltering, connected operators, and ﬁlters by
reconstruction. IEEE Trans. Image Proc. 4, 1153–1160 (1995)
3. Klein, J.C.: Conception et r´ealisation d’une unit´e logique pour l’analyse quantita-
tive d’images. PhD thesis, Nancy University, France (1976)
4. Cheng, F., Venetsanopoulos, A.N.: An adaptive morphological ﬁlter for image
processing. IEEE Trans. Image Proc. 1, 533–539 (1992)
5. Vincent, L.: Morphological area openings and closings for grey-scale images. In: O,
Y.L., Toet, A., Foster, D., Heijmans, H.J.A.M., Meer, P. (eds.) Shape in Picture.
NATO ASI Series (Series F: Computer and Systems Sciences), vol. 126, pp. 197–
208. Springer, Heidelberg (1993)
6. Breen, E.J., Jones, R.: Attribute openings, thinnings and granulometries. Comp.
Vis. Image Understand. 64(3), 377–389 (1996)
7. Maragos, P., Ziﬀ, R.D.: Threshold decomposition in morphological image analysis.
IEEE Trans. Pattern Anal. Mach. Intell. 12(5), 498–504 (1990)
8. Jones, R.: Connected ﬁltering and segmentation using component trees. Comp.
Vis. Image Understand. 75, 215–228 (1999)
9. Najman, L., Couprie, M.: Building the component tree in quasi-linear time. IEEE
Trans. Image Proc. 15, 3531–3539 (2006)
10. Salembier, P., Oliveras, A., Garrido, L.: Anti-extensive connected operators for
image and sequence processing. IEEE Trans. Image Proc. 7, 555–570 (1998)
11. Vincent, L.: Granulometries and opening trees. Fundamenta Informaticae 41, 57–
90 (2000)
12. Urbach, E.R., Roerdink, J.B.T.M., Wilkinson, M.H.F.: Connected shape-size pat-
tern spectra for rotation and scale-invariant classiﬁcation of gray-scale images.
IEEE Trans. Pattern Anal. Mach. Intell. 29, 272–285 (2007)
13. Urbach, E.R., Wilkinson, M.H.F.: Shape-only granulometries and grey-scale shape
ﬁlters. In: Proceeding of International Symposium Mathematical Morphology
(ISMM) 2002, pp. 305–314 (2002)
14. Wilkinson, M.H.F., Westenberg, M.A.: Shape preserving ﬁlament enhancement
ﬁltering. In: Niessen, W.J., Viergever, M.A. (eds.) MICCAI 2001. LNCS, vol. 2208,
pp. 770–777. Springer, Heidelberg (2001). doi:10.1007/3-540-45468-3 92
15. Westenberg, M.A., Roerdink, J.B.T.M., Wilkinson, M.H.F.: Volumetric attribute
ﬁltering and interactive visualization using the max-tree representation. IEEE
Trans. Image Proc. 16, 2943–2952 (2007)
16. Urbach, E.R., Boersma, N.J., Wilkinson, M.H.F.: Vector-attribute ﬁlters. In: Math-
ematical Morphology: 40 Years On, Proceedings of International Symposium Math-
ematical Morphology (ISMM) 2005, Paris, 18–20 April 2005 95–104

16
M.H.F. Wilkinson
17. Naegel, B., Passat, N., Boch, N., Kocher, M.: Segmentation using vector-attribute
ﬁlters: methodology and application to dermatological imaging. In: Proceeding
International Symposium on Mathematical Morphology (ISMM) 2007, pp. 239–
250 (2007)
18. Kiwanuka, F., Wilkinson, M.: Cluster based vector attribute ﬁltering. Math. Mor-
phol. Theory Appl. 1(1), 116–135 (2016)
19. Xu, Y., Carlinet, E., G´eraud, T., Najman, L.: Hierarchical segmentation using
tree-based shape space. IEEE Trans. Pattern Anal. Mach. Intell. 39(3), 457–469
(2016)
20. Meyer, F.: From connected operators to levelings. In: Fourth International Sympo-
sium on Mathematical Morphology, ISMM 1998, pp. 191–198. Kluwer, Amsterdam,
The Netherlands (1998)
21. Meyer, F.: Levelings, image simpliﬁcation ﬁlters for segmentation. J. Math. Imag.
Vis. 20(1–2), 59–72 (2004)
22. Monasse, P., Guichard, F.: Fast computation of a contrast invariant image repre-
sentation. IEEE Trans. Image Proc. 9, 860–872 (2000)
23. Monasse, P., Guichard, F.: Scale-space from a level lines tree. J. Vis. Commun.
Image Repres. 11, 224–236 (2000)
24. Caselles, V., Monasse, P.: Grain ﬁlters. J. Math. Imag. Vis. 17, 249–270 (2002)
25. G´eraud, T., Carlinet, E., Crozet, S., Najman, L.: A Quasi-linear algorithm to
compute the tree of shapes of nD images. In: Hendriks, C.L.L., Borgefors, G.,
Strand, R. (eds.) ISMM 2013. LNCS, vol. 7883, pp. 98–110. Springer, Heidelberg
(2013). doi:10.1007/978-3-642-38294-9 9
26. Urbach, E.R., Roerdink, J., Wilkinson, M.H.F.: Connected rotation-invariant size-
shape granulometries. In: Proceeding 17th International Conference on Pattern
Recognition, vol. 1, pp. 688–691 (2004)
27. Wilkinson, M.H.F., Moschini, U., Ouzounis, G.K., Pesaresi, M.: Concurrent com-
putation of connected pattern spectra for very large image information mining.
In: Proceeding of ESA-EUSC-JRC 8th Conference on Image Information Mining,
Oberpfaﬀenhofen, Germany, pp. 21–25 (2012)
28. Moschini, U., Teeninga, P., Trager, S.C., Wilkinson, M.H.F.: Parallel 2D local
pattern spectra of invariant moments for galaxy classiﬁcation. In: Azzopardi, G.,
Petkov, N. (eds.) CAIP 2015. LNCS, vol. 9257, pp. 121–133. Springer, Cham
(2015). doi:10.1007/978-3-319-23117-4 11
29. Bosilj, P., Wilkinson, M.H.F., Kijak, E., Lef`evre, S.: Local 2D pattern spectra
as connected region descriptors. In: Benediktsson, J.A., Chanussot, J., Najman,
L., Talbot, H. (eds.) ISMM 2015. LNCS, vol. 9082, pp. 182–193. Springer, Cham
(2015). doi:10.1007/978-3-319-18720-4 16
30. Pesaresi, M., Benediktsson, J.: A new approach for the morphological segmentation
of high-resolution satellite imagery. IEEE Trans. Geosci. Remote Sens. 39(2), 309–
320 (2001)
31. Benediktsson, J., Palmason, J., Sveinsson, J.: Classiﬁcation of hyperspectral data
from urban areas based on extended morphological proﬁles. IEEE Trans. Geosci.
Remote Sens. 43(3), 480–491 (2005)
32. Soille, P.: Constrained connectivity and connected ﬁlters. IEEE Trans. Pattern
Anal. Mach. Intell. 30(7), 1132–1145 (2008)
33. Najman, L.: On the equivalence between hierarchical segmentations and ultrametric
watersheds. J. Math. Imag. Vis. 40(3), 231–247 (2011)
34. Ronse, C.: Partial partitions, partial connections and connective segmentation. J.
Math. Imag. Vis. 32(2), 97–125 (2008)

A Guided Tour of Connective Morphology
17
35. Naegel, B., Passat, N.: Component-trees and multi-value images: a comparative
study. In: Wilkinson, M.H.F., Roerdink, J.B.T.M. (eds.) ISMM 2009. LNCS, vol.
5720, pp. 261–271. Springer, Heidelberg (2009). doi:10.1007/978-3-642-03613-2 24
36. Carlinet, E., G´eraud, T.: MToS: a tree of shapes for multivariate images. IEEE
Trans. Image Process. 24(12), 5330–5342 (2015)
37. Evans, A.N.: Color area morphology scale-spaces. Adv Imaging Electr. Phys. 160,
35–74 (2010)
38. Gimenez, D., Evans, A.N.: An evaluation of area morphology scale-spaces for colour
images. Comp. Vis. Image Understand. 110, 32–42 (2008)
39. Salembier, P., Garrido, L.: Binary partition tree as an eﬃcient representation
for image processing, segmentation and information retrieval. IEEE Trans. Image
Proc. 9(4), 561–576 (2000)
40. Ouzounis, G.K., Soille, P.: The Alpha-Tree algorithm. Publications Oﬃce of the
European Union, December 2012
41. Aptoula, E., Pham, M.-T., Lef`evre, S.: Quasi-ﬂat zones for angular data simpliﬁ-
cation. In: Angulo, J., Velasco-Forero, S., Meyer, F. (eds.) ISMM 2017. LNCS, vol.
10225, pp. 342–354. Springer, Cham (2017). doi:10.1007/978-3-319-57240-6 28
42. Serra, J.: Connectivity on complete lattices. J. Math. Imag. Vis. 9(3), 231–251
(1998)
43. Serra, J.: Connections for sets and functions. Fundam. Inf. 41(1–2), 147–186 (2000)
44. Braga-Neto, U., Goutsias, J.: A theoretical tour of connectivity in image processing
and analysis. J. Math. Imag. Vis. 19, 5–31 (2003)
45. Braga-Neto, U., Goutsias, J.: Object-based image analysis using multiscale con-
nectivity. IEEE Trans. Pattern Anal. Mach. Intell. 27(6), 892–907 (2005)
46. Ouzounis, G.K., Wilkinson, M.H.F.: Mask-based second generation connectivity
and attribute ﬁlters. IEEE Trans. Pattern Anal. Mach. Intell. 29, 990–1004 (2007)
47. Oosterbroek, J., Wilkinson, M.H.F.: Mask-edge connectivity: Theory, computation,
and application to historical document analysis. In: Proceeding 21st International
Conference on Pattern Recognition, pp. 3112–3115 (2012)
48. Wilkinson, M.H.F.: An axiomatic approach to hyperconnectivity. In: Wilkinson,
M.H.F., Roerdink, J.B.T.M. (eds.) ISMM 2009. LNCS, vol. 5720, pp. 35–46.
Springer, Heidelberg (2009). doi:10.1007/978-3-642-03613-2 4
49. Wilkinson, M.H.F.: Hyperconnections and openings on complete lattices. In: Soille,
P., Pesaresi, M., Ouzounis, G.K. (eds.) ISMM 2011. LNCS, vol. 6671, pp. 73–84.
Springer, Heidelberg (2011). doi:10.1007/978-3-642-21569-8 7
50. Perret, B., Lefevre, S., Collet, C., Slezak, E.: Hyperconnections and hierarchical
representations for grayscale and multiband image processing. IEEE Trans. Image
Process. 21(1), 14–27 (2012)
51. Perret, B., Lef`evre, S., Collet, C.: Toward a new axiomatic for hyper-connections.
In: Soille, P., Pesaresi, M., Ouzounis, G.K. (eds.) ISMM 2011. LNCS, vol. 6671,
pp. 85–95. Springer, Heidelberg (2011). doi:10.1007/978-3-642-21569-8 8
52. Perret, B.: Inf-structuring functions: a unifying theory of connections and con-
nected operators. J. Math. Imaging Vis. 51(1), 171–194 (2015)
53. Wilkinson, M.H.F.: Hyperconnectivity, attribute-space connectivity and path
openings: theoretical relationships. In: Wilkinson, M.H.F., Roerdink, J.B.T.M.
(eds.) ISMM 2009. LNCS, vol. 5720, pp. 47–58. Springer, Heidelberg (2009). doi:10.
1007/978-3-642-03613-2 5
54. Moschini, U., Wilkinson, M.H.F.: Viscous-hyperconnected attribute ﬁlters: a ﬁrst
algorithm. In: Benediktsson, J.A., Chanussot, J., Najman, L., Talbot, H. (eds.)
ISMM 2015. LNCS, vol. 9082, pp. 669–680. Springer, Cham (2015). doi:10.1007/
978-3-319-18720-4 56

18
M.H.F. Wilkinson
55. Wilkinson, M.H.F.: Attribute-space connectivity and connected ﬁlters. Image Vis.
Comput. 25, 426–435 (2007)
56. Vincent, L.: Morphological grayscale reconstruction in image analysis: application
and eﬃcient algorithm. IEEE Trans. Image Proc. 2, 176–201 (1993)
57. Meijster, A., Wilkinson, M.H.F.: A comparison of algorithms for connected set
openings and closings. IEEE Trans. Pattern Anal. Mach. Intell. 24(4), 484–494
(2002)
58. Carlinet, E., G´eraud, T.: A comparative review of component tree computation
algorithms. IEEE Trans. Image Proc. 23(9), 3885–3895 (2014)
59. Hesselink, W.H.: Salembier’s Min-tree algorithm turned into breadth ﬁrst search.
Inf. Process. Lett. 88(5), 225–229 (2003)
60. Wilkinson, M.H.F.: A fast component-tree algorithm for high dynamic-range
images and second generation connectivity. In: Proceeding International Confer-
ence on Image Processing 2011, pp. 1041–1044 (2011)
61. Tarjan, R.E.: Eﬃciency of a good but not linear set union algorithm. J. ACM 22,
215–225 (1975)
62. Berger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E.: Eﬀec-
tive component tree computation with application to pattern recognition in astro-
nomical imaging. In: Proceeding of International Conference Image Processing
2007, San Antonio, Texas, USA, 16–19 September 2007, vol. IV, pp. 41–44 (2007)
63. Wilkinson, M.H.F., Gao, H., Hesselink, W.H., Jonker, J.E., Meijster, A.: Concur-
rent computation of attribute ﬁlters using shared memory parallel machines. IEEE
Trans. Pattern Anal. Mach. Intell. 30(10), 1800–1813 (2008)
64. Moschini, U., Meijster, A., Wilkinson, M.H.F.: A hybrid shared-memory parallel
max-tree algorithm for extreme dynamic-range images. IEEE Trans. Pattern Anal.
Mach, Intell. (2017, in press)
65. Wilkinson, M.H.F., Pesaresi, M., Ouzounis, G.K.: An eﬃcient parallel algorithm
for multi-scale analysis of connected components in gigapixel images. ISPRS Int.
J. Geo-Inf. 5(3), 22 (2016)
66. Moschini, U., Teeninga, P., Wilkinson, M.H.F., Giese, N., Punzo, D., Van der
Hulst, J.M., Trager, S.C.: Towards better segmentation of large ﬂoating point 3d
astronomical data sets: ﬁrst results. In: Proceedings of the 2014 conference on Big
Data from Space BiDS14, pp. 232–235. Publications Oﬃce of the European Union
(2014)
67. Kazemier, J.J., Ouzounis, G.K., Wilkinson, M.H.F.: Connected morphological
attribute ﬁlters on distributed memory parallel machines. In: Angulo, J., Velasco-
Forero, S., Meyer, F. (eds.) ISMM 2017. LNCS, vol. 10225, pp. 357–368. Springer,
Cham (2017). doi:10.1007/978-3-319-57240-6 29
68. Ouzounis, G.K., Wilkinson, M.H.F.: A parallel dual-input max-tree algorithm for
shared memory machines. In: Proceeding of International Symposium Mathemat-
ical Morphology (ISMM) 2007, pp. 449–460 (2007)
69. Ouzounis, G.K., Wilkinson, M.H.F.: Hyperconnected attribute ﬁlters based on
k-ﬂat zones. IEEE Trans. Pattern Anal. Mach. Intell. 33(2), 224–239 (2011)
70. Havel, J., Merciol, F., Lef`evre, S.: Eﬃcient tree construction for multiscale image
representation and processing. J. Real-Time Image Process. 1–18 (2016)

Collaborating with an Artist in Digital
Geometry
Eric Andres(B), Gaelle Largeteau-Skapin, and Aur´elie Mourier
Laboratory XLIM, Team ASALI, UMR CNRS 6712, University of Poitiers,
BP 30179, 86962 Futuroscope Chasseneuil Cedex, France
{eric.andres, gaelle.largeteau.skapin}@univ-poitiers.fr
contact@aureliemourier.net
Abstract. In this invited paper, we are going to present an ongoing
collaboration with a local artist, Aur´elie Mourier. The artist works with
voxel shapes and this led our digital geometry team to develop new shape
modeling tools and explore a particular class of unfolding problems.
1
Introduction and Context of the Artistic Work
In this paper, we are going to present the results of an ongoing collaboration
of our Digital Geometry Research team with a local graphical artist, Aur´elie
Mourier, that happens to be working with cubes. Let us present the motivations
behind her work and then we’ll present some examples of our research that were
partly or completely driven by the artists demands and questions.
An artist typically tries to understand the world around him by proposing
his own reproduction and/or by dissecting speciﬁc elements of it in order to
propose an original point of view. The artist will embody his unique and personal
point of view in an art work so his experience of the world can be shared with
the public. Art is not meant to be didactic but singular. The inspiration for
his work may come from many diﬀerent sources and points of view including
scientiﬁc ones. There are many similarities in the approach used by artists and
scientists in particular in the attempts to propose an abstract representation of
the world. The artist will use his art form while the scientist will use mathematics
as common shared language.
In A. Mourier’s case, she focuses on shapes. Those shapes can be extracted
from reality or invented. In order to study only the shape of things, she willingly
discards parameters such as color, texture and size: a planet, a ball and a mar-
ble have all the same shape. It is interesting here to make a parallel with the
ideas behind the invention of topology. In her case however, the geometric shape
(although in a digital abstract form) still plays an important role. She proceeds
by injecting the object she wants to reproduce into a 3D cubic grid similar to
what happens when one pixelizes a shape in 2D. A resolution was chosen: the
grid size of 25 × 25 × 25. This sizes was chosen arbitrarily so that the shapes
are just big enough to be recognizable and allow some expressibility. A shape is
well formed for A. Mourier if it is in one piece, with each cube touching another
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 19–30, 2017.
DOI: 10.1007/978-3-319-66272-5 3

20
E. Andres et al.
one by face and touches at least two opposite sides of the grid. She orders the
shapes according to their volume (their number of voxels). She did not know,
in the beginning of her artistic work, that her process was called digitization
and her one piece, face-connected constraint translates in topology as a single
6-connected component.
Her inspiration came from the observation that when we look at the world
through computer screens, our world view is formed of digital images with a
ﬁnite ﬁxed resolution. There exists only ﬁnite, although very big, number of
possible images. If we could create a catalog of all the possible images, it would,
for instance, include all the images of all the people in the world, dead, alive and
yet to be born. Let us note here that our biological eye has also a ﬁnite number
of cones and that our brain has a limit on the number of diﬀerent colors it is able
to distinguish. A. Mourier works on a shape repository but more importantly,
on how a shape can be represented or coded and how such shapes can then lead
to sculptures that reinstate a size and a material with physical properties. This
idea of repository is inspired by the novel written in 1941 by J.-L. Borges, “La
biblioteca de Babel” [6]. The story of Borges describes a library with all the
possible books of 410 pages, each made of 40 lines of about 80 characters each
in a alphabet composed of 22 regular letters, and the characters space, coma
and point (check out https://libraryofbabel.info/ for a virtual example of this
library). Of course, just as for the Library of Babel, and even though well formed
objects represent only a fraction of the 2253 ≈3.9×104703 possible digital objects
that can be represented in her grid, it gives a setting for shape exploration.
Before we met, A. Mourier used to model her voxel objects with generic
modeling software. She then printed and cut the result by hand to obtain either
the unfolded pattern of the shape or its slices. An example of a stereo microscope
shape is presented Fig. 1. The number in the name of the shape corresponds to
its number of voxels (797) and the order of the object among all the shapes
having the same number of voxels. Once a shape has been deﬁned, A. Mourier
is interested in all the ways such a shape can be represented: as a set of voxels
in 3D, as a set of 2D slices, unfolded as a net, etc.
Fig. 1. A. Mourier stereo microscope shape 00797.001: the unfolded net, the slices in
the z axis and the 3D numeric object.

Collaborating with an Artist in Digital Geometry
21
2
Creating Digital Shapes for an Artist
As an artist working on her shape depository, A. Mourier used to design her
digital objects by hand. Coming from a completely diﬀerent background, terms
like digitization, 6-connectivity or digital geometry were unknown to her, on the
other hand she had manipulated such shapes and representation forms for a
couple of years and often she has a better intuition than we could possibly have
on how to do certain operations. For her, it was interesting to put scientiﬁc
words on notions she was manipulating intuitively. For us, it was interesting to
discover new classes of problems with speciﬁc applicative constraints.
The starting point of our collaboration was through a student project
that created a simple voxel modeling software that lets you freely create
voxel objects. This tool can be found online at http://www.aureliemourier.
net/logiciel/25aucube.html. However, with this tool, shapes still had to be con-
structed by hand. We proposed to design algorithms to help create speciﬁc classes
of voxel objects. For this, several constraints had to be met so that our client, A.
Mourier, could use these objects: one of the main constraints was to incorporate
a manual design possibility so that the artist may express herself. As a ﬁrst pro-
posal, we developed a online software to generate digital surfaces of revolution
based on a hand-drawn generatrix and hand-drawn curves of revolution. This
work will be presented in Sect. 2.1. We then proposed a method for creating
tubes where the 3D curve and the section of the tube can be designed freely.
This will be presented in Sect. 2.2. More recently, we looked into the problem of
unfolding the surface of a voxel object. This was motivated by the nets created,
by the artist, by hand. This will be presented in Subsect. 2.3.
2.1
Digital Surface of Revolution with Hand-Drawn Generatrix and
Curve of Revolution
Working with a visual art artist, our goal was to propose ﬂexible, intuitive to
use tools for designing 3D voxel surfaces. For this we considered 2D (hand-)
drawings which are a natural shape representation form for most artists. A recent
paper proposed a digitization method for surfaces of revolution [3] based on a
very simple and straightforward method for digitizing almost any implicit nD
surfaces [16]. It is based on a morphological type digitization method called
ﬂake digitization. The ﬂake digitization allows to deﬁne digital surfaces with a
controlled topology (control on tunnels of the digital surface). The paper [16]
can be seen as an extension to all tunnel connectivities and dimensions of the
paper of S. Laine [14].
A surface of revolution is deﬁned by two 2D curves: the curve of revolution
and the generatrix. We propose three ways of deﬁning a curve of revolution
and three ways of deﬁning a generatrix in order to allow maximum ﬂexibility.
A curve of revolution can be deﬁned as an implicit curve (that separates space
into positive and negative valued regions), as a closed hand-drawn curve or as
binary pixel image that serves as a look-up matrix. The curve of revolution
is not limited to the traditional circle and not limited to a unique connected
component, which oﬀers a great liberty in designing complex shapes. Contrary

22
E. Andres et al.
to the curve of revolution, the generatrix is not necessarily a closed curve. Three
similar ways of deﬁning generatrix curves are proposed: as an explicit function,
as hand-drawn curves or as contour curves extracted from a binary image. This
leads to nine diﬀerent ways of deﬁning digital surfaces of revolution. Two of those
methods have already been published by Andres et al. respectively in [3,4].
Let us detail a little bit the overall method. A curve of revolution can be basi-
cally deﬁned in any way as long as we are able, slice by slice, to deﬁne regions
where with a interior and an exterior. The curve(s) of revolution are then deﬁned
as the boundaries between those regions. For the digitization, we determine if a
digital point belongs to the digital surface of revolution by considering the vertices
of the three dimensional structuring element (k-Flakes [16]) centered on this point
and computing their position relatively to the curve of revolution. If some of the
vertices are inside and some outside then the corresponding voxel is cut by the sur-
face and therefore belongs to the Digital Surface of Revolution. When the curve
of revolution is implicitly deﬁned the vertex localisation is straightforward. When
the curve of revolution is given as a hand-drawn curve, which is more natural for
an artist, we record the sequence of Euclidean points, while the artist draws the
curve of revolution. We ensure that this curve is a closed one (to deﬁne one or
several interior(s) and exterior(s)) by adding the ﬁrst point at the end of the list.
This list of point is then treated as a closed polygon and the critical localisation
information can be obtained using a Point In Polygon (PIP) algorithm [12].
The other curve, the generatrix g, typically plays the role of an homothetic
factor for the curve of revolution. A slice z = z0 of the surface of revolution is the
curve of revolution scaled by a factor g(z0) for a generatrix that is deﬁned by an
explicit function y = g(z). When the generatrix is drawn and represented by a list
of points, there are two main issues: ﬁrstly, there can be more than one value g(z)
per z and secondly, the curve may not be closed. To handle the multiplicity of
g(z) values, the point sequence is divided into strictly monotonic (in z, increasing
or decreasing) or horizontal subsequences. The end point of one subsequence
is duplicated as the starting point of the next one. Each subsequence can be
digitized completely independently because the ﬂake digitization method we use
is a morphological type digitization (i.e. consistant with the union operator).
One last problem had to be addressed: the generatrix may be an open curve
and therefore it may have extremities. This case needs to be handled speciﬁcally
because the digitization of the surface of revolution supposes that we are able, for
all points, to compute its localisation relatively to the surface. At the generatrix
extremities, some of the vertices of the voxels’ ﬂake are outside the domain of
deﬁnition of the generatrix. There can be, in this case, a deﬁned localisation
for some vertices and not for others. In this case, we consider only the parts
of the ﬂake that are inside the domain and therefore we take, as substitute for
the undeﬁned vertices, the endpoints of a cropped ﬂake line segments. See [4]
for more details. See http://imgur.com/a/eDFbY for some examples of swept
digital tubes and digital surfaces of revolution. The method is not limited to
curves of revolution of dimension two. You can see an example at http://imgur.
com/a/eDFbY of a four-dimensional torus deﬁned by a 3D surface of revolution
(a sphere) and a 2D generatrix (a circle) (Fig. 2).

Collaborating with an Artist in Digital Geometry
23
HD
implicit
explicit
implicit
HD
HD
BMP
HD
BMP
HD
Fig. 2. Examples of 3D surfaces of revolution (HD: hand-drawn, BMP: bitmap image).

24
E. Andres et al.
Fig. 3. Spinning top sculptor
The software that implements these methods is available at http://xlim-sic.
labo.univ-poitiers.fr/demonstrateurs/DSoR Generator/. This software has been
used by A.Mourier to build chess pieces that have been 3D printed (See Fig. 4
for an example), and a sculptor representing a spinning top (Fig. 3).
Fig. 4. The 3D printed chess white queen. This piece was built using an implicit curve
of revolution and a hand-drawn generatrix.
2.2
Swept Tubular Surfaces
As a complement to surfaces of revolution, we designed a tool that allows to
create Swept Tubular surfaces. Such tubes can also be deﬁned as digital implicit
surfaces [16]. These digital surfaces have applications well beyond our visual art
interest: modeling of body parts [13], medicine [7], etc.
Formally, a swept tube is deﬁned by a 3D curve, the spine curve, and a 2D
closed curve, the proﬁle. The proﬁle (sometimes called cross-section in the litera-
ture) is swept out in a plane normal to the 3D spine curve (or trajectory). Using

Collaborating with an Artist in Digital Geometry
25
a mapping of the proﬁle on the spine together with the implicit surface digiti-
zation method [16], it is actually quite easy to build digitized swept tubes. In
Fig. 5, one can see three interlaced digital swept tubes generated by a parametric
3D spine curve and a bitmap image of three disks that served as proﬁle (please
check http://imgur.com/a/eDFbY for an animated version). For the mapping,
we chose the Frenet-Serret formulas [10] which describes the motion of a particle
along a 3D continuous curve (any other mapping intended to deﬁne a moving
frame like the Darboux frame for instance could also be used). The Serret-Frenet
frame is deﬁned by the tangent, the normal and the binormal unit vectors in
any point of the 3D curve.
Fig. 5. An example of a swept tube deﬁned by a parametric closed 3D spine curve (on
the upper right side) and a bitmap image of three disks as proﬁle (lower right side)
For the animations that we created or that the artist created, the idea was
simply to rotate the proﬁle (see http://imgur.com/a/eDFbY for some examples).
2.3
Unfolding Voxel Surfaces
A central question in graphical arts has always been the representation of 3D so-
called reality in 2D. For a long time Hierarchical representation (where important
personae tended to be painted in big and in the middle of a painting while others
where painted in small and in the periphery) or perspective representation was
the norm. Let us not that for hierarchical representation, although less realistic,
conteined additional information with an added, otherwise abstract, social and
political dimension. Cubists, at the beginning of the twentieth century, proposed
a new form of representation that tried to incorporate multiple view points in
an abstract recomposition. One of the motivations was to represent elements
that would otherwise be hidden. The abtract nature of such recomposition make
the paintings sometimes diﬃcult to understand while at the same time they

26
E. Andres et al.
potentially represent a more complete representation of the reality than a more
classical representation with only one point of view. That was the starting point
of A. Mouriers’ interest in nets and the problem of unfolding voxel surfaces. A
net shows the complete 3D object in 2D with all the complete surface visible
although the net makes it diﬃcult to imagine what the corresponding 3D object
looks like. Representing a complete 3D reality comes with a price.
The unfolding problem is an old problem already discussed by A. Durer
[11]. Since then, unfolding problems have been extensively studied with a wide
range of applications ranging from industrial manufacturing, storage problems,
to texture mapping, etc. The unfolding problem can be stated as follows: can
the surface of a 3D closed object be unfolded ﬂat to a single component without
overlap? [5]. There are two ways of unfolding: edge-following unfolding and gen-
eral unfoldings. In edge-unfoldings, one can only make cuts along the edges of
the polyhedra while for general unfoldings, one can cut through faces (Fig. 6).
Fig. 6. General unfolding net and edge unfolding net of a cube.
2.4
State of the Art
It has been shown that a non convex polyhedra can not always be edge unfolded
[5] and that a convex polyhedra has always a general unfolding [1]. To our best
knowledge, it is not known if a convex polyhedra always has an edge unfolding
or if a non convex polyhedra always has a general unfolding. There is however a
subclass of unfolding problems where does not deal with an arbitrary polyhedra
but with what is called, orthogonal polyhedra. An Orthogonal polyhedron is a
polyhedron whose faces meet only with a 0 angle (both faces are coplanar), a π/2
angle (both faces have a so-called valley fold) or a 3π/2 angle (both faces have a
so-called mountain fold). The terms valley and mountain fold comes originally
from the origami community. In 2007, it has been shown that arbitrary genus 0
orthogonal polyhedra always have a general unfolding [9]. More recently, in 2016,
an algorithm for genus 2 orthogonal orthogonal polyhedra has been proposed [8]
as well as a one layer genus g method [15]. A particular case of general unfoldings
that are considered are so called grid-unfoldings where cuts across orthogonal
faces are only allowed along the edges of a subgrid that may be arbitrarily small
in some cases.

Collaborating with an Artist in Digital Geometry
27
2.5
Our Unfolding Problem
Our work on these problems have been driven by the questions and needs as
presented to us by A. Mourier. An art-and-science project funded by the regional
direction of Art and Culture (DRAC Nouvelle Aquitaine) was proposed with
the question of peeling (unfolding) genus 0 egg shaped objects (not necessarily
convex). For this, we proposed, ﬁrstly, a new way of generating digital objects
based on Focus points which was accepted for DGCI 2017 [2]. Once we had a
way of generating such objects, we were asked if it was possible to generate nets
randomly. The problem, for A. Mourier, being the question of having diﬀerent
nets that represent the same ﬁnal 3D object.
In our case, we consider a 6-connected voxel object whose surface is divided
into square voxel faces. We are therefore looking into an edge unfolding problem
of orthogonal polyhedra that bears some resemblance with the general grid-
unfolding of orthogonal polyhedra class of problems. The key diﬀerence here is
that the voxel faces can not be subdivided. The classical examples counterexam-
ples, to prove that edge-unfolding for orthogonal polyhedra is not always pos-
sible, have nets in our case. The question of the existence of an edge-unfolding
solution for all such voxel surface object remains open. And if such a solution
always exists, what algorithm could be proposed in the general case ?
For the problem on hand, with the demand of the artist to not propose a
deterministic solution, we developed an algorithm based on the following basis:
1. we start with an empty 2D grid in which the voxel faces will be set in order
to deﬁne a net.
2. First we extract a list of the surface faces of the 6-connected voxel object.
3. for each face we numerotate the edges. An edge is of course shared by two
and only two faces.
4. We start with a random face and put it in the center of our grid.
5. We compute all the faces that can be put down next as neighbors for the
already settled faces. The key point is that you can put down a new face next
to a face in the grid if the edge corresponds. Of course it may happen that
a grid square next to a settled face can not be ﬁlled because the face that
corresponds to this edge has already been put down elsewhere in the grid for
another edge.
6. we choose randomly, among those faces, the next face that can be put down
as long as not all the faces have been put down or that we are not blocked.
7. if we are not blocked or haven’t ﬁnished we go to step 5.
This method of building a net does not always provide a solution. It can be
blocked simply because all the (four) grid places where we could put down a face
are already occupied by other faces.
There are however several ways to improve the convergence: If we are blocked
and have several faces left, we can try to correct our net. For the faces that we
could not put down, we can determine where we could have put them. If the
location where we could have put down the face is an isolated face or a face
in a cycle such that it is not the place where the cycle is linked to the other

28
E. Andres et al.
faces of the net, then we can remove the isolated or cycle face and put down our
blocked face. We have now swapped one face for another. The idea is that, may
be, this new face will ﬁnd a free grid spot to put it down. The reason why we
can only swap with isolated or non splitting cycle faces is that otherwise the face
represents the root of a tree of faces that would be disconnected from the net.
Since the aim is to create a net in one piece, that can not be allowed. In some
cases, a face can not be placed at any spot where we would have isolated faces.
We have then the choice, either to abandon and start over or to extract all the
faces of the smallest sub-tree and start over with trying to place all those new
faces.
A last method we have implemented to get out of a blocking situation, with-
out starting all over, is to remove all the isolated and non splitting cycle faces
from the blocked net and start over at step 6.
2.6
Results
To our surprise, the algorithm we have developed works surprisingly well with
what we thought would be topologically complicated voxel objects. Figure 7
shows the net for a voxel object formed of a 53 voxel cube with 4 traversing
tunnels on each side. There are 270 faces and the algorithm takes only a cou-
ple of seconds to ﬁnd a net for such an object with very often only a couple of
attempts. The algorithm has also found a solution for the much more complex
digital object shown below in the ﬁgure. We didn’t include here the image of the
net because it is simply too big to distinguish any details. It can take usually
up to a couple of hours to ﬁnd a solution for such an object. There are 1350 faces
Fig. 7. Voxel cube with 4 holes by face and one of the resulting nets. Below Cube with
16 holes where a solution was found as well.

Collaborating with an Artist in Digital Geometry
29
in this case. The other surprise was that the method works not very well with
topologically simple objects. The algorithm struggles with simple objects such as
digital spheres of relatively small radii. It is however not completely surprising,
all things considered. Our net generator is based on random choices which tends
to produce relatively compact nets. The algorithm will typically not stretch the
net in one direction to provide more space to put down faces.
3
Conclusion and Perspectives
In this paper we have presented some of the scientiﬁc work that has resulted from
the collaboration between a scientiﬁc team working in digital geometry and a
visual art artist. As an artist, A. Mourier explores shapes in a voxel form. Several
modeling tools for digital objects have been developed with a focus on giving the
artist the possibility to express herself graphically. This has led to develop new
methods for generating digital surfaces of revolution and digital tubular swept
objects. A more recent collaboration has focused on the unfolding problem for
voxel surface objects. A new algorithm for generating nets has been proposed.
There are several open questions that have been raised in these diﬀerent works:
we have developed a method where the generatrix may be an open curve. What
about open curves of revolution? The same question can be asked for digital
swept tubes. Can they be deﬁned with open proﬁle curves? Lastly, we have
considered a particular type of unfolding problem: orthogonal edge based grid
unfolding. Is there always a solution for this particular case? Right now we have
developed a random search algorithm to generate nets that are diﬀerent each
time. This method may need several attempts before it proposes a result. Is
there a way to create deterministic algorithms for the general case of convex,
non-convex voxel surface unfoldings?
References
1. Agarwal, P.K., Aronov, B., O’Rourke, J., Schevon, C.A.: Star unfolding of a poly-
tope with applications. In: Gilbert, J.R., Karlsson, R. (eds.) SWAT 1990. LNCS,
vol. 447, pp. 251–263. Springer, Heidelberg (1990). doi:10.1007/3-540-52846-6 94
2. Andres, E., Biswas, R., Bhowmick, P.: Digital primitives deﬁned by weighted focal
set. In: Kropatsch, W.G., Artner, N.M., Janusch, I. (eds.) DGCI 2017. LNCS, vol.
10502, pp. 388–398. Springer, Heidelberg (2017)
3. Andres, E., Largeteau-Skapin, G.: Digital surfaces of revolution made simple. In:
Normand, N., Gu´edon, J., Autrusseau, F. (eds.) DGCI 2016. LNCS, vol. 9647, pp.
244–255. Springer, Cham (2016). doi:10.1007/978-3-319-32360-2 19
4. Andres, E., Richaume, L., Largeteau-Skapin, G.: Digital surface of revolution with
hand-drawn generatrix. J. Math. Imaging Vis. 59(1), 40–51 (2017)
5. Biedl, T., Demaine, E., Demaine, M., Lubiw, A., Overmars, M., O’Rourke, J.,
Robbins, S., Whitesides, S.: Unfolding some classes of orthogonal polyhedra. In:
Proceedings of the 10th Canada Conference on Computing and Geometry, pp.
70–71 (1998)

30
E. Andres et al.
6. Borges, J.L.: La Biblioteca de Babel: pr´ologos. Obras de Borges, Emec´e Editores
(2000)
7. Bornik, A., Reitinger, B., Beichel, R.: Simplex-mesh based surface reconstruction
and representation of tubular structures. In: Meinzer, H.P., Handels, H., Horsch,
A., Tolxdorﬀ, T. (eds.) Bildverarbeitung f¨ur die Medizin 2005. Informatik aktuell,
pp. 143–147. Springer, Heidelberg (2005)
8. Damian, M., Demaine, E., Flatland, R., O’Rourke, J.: Unfolding genus-2 orthogo-
nal polyhedra with linear reﬁnement. CoRR, abs/1611.00106 (2016)
9. Damian, M., Flatland, R., O’Rourke, J.: Epsilon-unfolding orthogonal polyhedra.
Graphs Comb. 23, 179–194 (2007)
10. Chand De, U.: Diﬀerential Geometry of Curves and Surfaces in E3. Anshan (2007)
11. D¨urer, A.: Unterweysung der Messung mit dem Zirkel un Richtscheyt in Linien
Ebnen uhnd Gantzen Corporen (1525)
12. Michael
Galetzka
and
Patrick
O
Glauner.
A
correct
even-odd
algorithm
for the point-in-polygon (pip) problem for complex polygons. arXiv preprint
arXiv:1207.3502 (2012)
13. Hyun, D.E., Yoon, S.H., Kim, M.S., Juttler, B.: Modeling and deformation of arms
and legs based on ellipsoidal sweeping. In: 11th Paciﬁc Conference on Computer
Graphics and Applications, Proceedings, pp. 204–212. IEEE (2003)
14. Laine, S.: A topological approach to voxelization. Comput. Graph. Forum 32(4),
77–86 (2013)
15. Liou, M.-H., Poon, S.-H., Wei, Y.-J.: On edge-unfolding one-layer lattice polyhe-
dra with cubic holes. In: Cai, Z., Zelikovsky, A., Bourgeois, A. (eds.) COCOON
2014. LNCS, vol. 8591, pp. 251–262. Springer, Cham (2014). doi:10.1007/
978-3-319-08783-2 22
16. Toutant, J.-L., Andres, E., Largeteau-Skapin, G., Zrour, R.: Implicit digital sur-
faces in arbitrary dimensions. In: Barcucci, E., Frosini, A., Rinaldi, S. (eds.)
DGCI 2014. LNCS, vol. 8668, pp. 332–343. Springer, Cham (2014). doi:10.1007/
978-3-319-09955-2 28

Geometric Transforms

Honeycomb Geometry: Rigid Motions
on the Hexagonal Grid
Kacper Pluta1(B), Pascal Romon2, Yukiko Kenmochi3, and Nicolas Passat4
1 LIGM (UMR 8049), LAMA (UMR 8050), UPEM, UPEC, CNRS,
Universit´e Paris-Est, Marne-la-Vall´ee, France
kacper.pluta@univ-paris-est.fr
2 LAMA (UMR 8050), UPEM, UPEC, CNRS, Universit´e Paris-Est,
Marne-la-Vall´ee, France
pascal.romon@u-pem.fr
3 LIGM (UMR 8049), UPEM, CNRS, ESIEE Paris, ENPC, Universit´e Paris-Est,
Marne-la-Vall´ee, France
yukiko.kenmochi@esiee.fr
4 CReSTIC, Universit´e de Reims Champagne-Ardenne, Reims, France
nicolas.passat@univ-reims.fr
Abstract. Euclidean rotations in R2 are bijective and isometric maps,
but they generally lose these properties when digitized in discrete spaces.
In particular, the topological and geometric defects of digitized rigid
motions on the square grid have been studied. This problem is related to
the incompatibility between the square grid and rotations; in general, one
has to accept either relatively high loss of information or non-exactness
of the applied digitized rigid motion. Motivated by these facts, we study
digitized rigid motions on the hexagonal grid. We establish a framework
for studying digitized rigid motions in the hexagonal grid—previously
proposed for the square grid and known as neighborhood motion maps.
This allows us to study non-injective digitized rigid motions on the hexag-
onal grid and to compare the loss of information between digitized rigid
motions deﬁned on the two grids.
1
Introduction
Rigid motions on R2 are fundamental transformations in 2D image processing.
They are isometric and bijective. Nevertheless, digitized rigid motions are, in
general, non-bijective. Despite many eﬀorts during the last twenty years, the
topological and geometric defects of digitized rigid motions on the square grid are
still not fully understood. According to Nouvel and R´emila, this problem is not
related to a limitation of arithmetic precision, but arises as a deep incompatibility
between the square grid and rotations [1].
Pioneering contributions to two-dimensional digital geometry in the square
grid were made in 1970s (see [2,3] for a survey). The main reason for taking
this Cartesian approach was that objects deﬁned on the square grid are easy to
address in the computer memory, as well as image acquisition devices, whose
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 33–45, 2017.
DOI: 10.1007/978-3-319-66272-5 4

34
K. Pluta et al.
sensors are generally organized on square grids. Moreover, the square grid has
a direct extension to higher dimensions. However, such a Cartesian framework
suﬀers from fundamental topological problems and therefore one has to choose
diﬀerent connectivity relations for objects of interest and their complements [2],
or use a cell-complex framework [4], for example.
On the contrary, the regular hexagonal grid (called hereafter the hexagonal
grid) suﬀers less from these problems since it possesses the following properties:
equidistant neighbors – each hexagon has six equidistant neighbors; uniform con-
nectivity – there is only one type of connectivity [5,6]. However, operations such
as a sampling of continuous signals with the hexagonal grids are often com-
putationally more expensive than ones deﬁned on the square grid. Indeed, a
digitization operator on the square grid can be deﬁned by a rounding function,
while there is no such counterpart on the hexagonal grid. Nevertheless, a practi-
cally useful digitization operator on a hexagonal grid has been proposed by Her
[7].
Motivated by aforementioned issues, we study the diﬀerences between dig-
itized rigid motions deﬁned on both types of grids. To understand these dif-
ferences, we establish a framework for studying digitized rigid motions on the
hexagonal grid at a local scale. Our approach is an extension to the hexagonal
grid of the former study of digitized rigid motions of the square grid, which is
based on neighborhood motion maps [8]. This enables us to study the structure
of groups induced by images of digitized rigid motions. It also allows us to focus
on the issue of the information preservation under digitized rigid motions deﬁned
on the two grids. We provide a comparison of the information loss between the
hexagonal and square grids. In addition, we present a complete list of neighbor-
hood motion maps for the 6-neighborhood in Appendix.
2
Digitizing on the Hexagonal Grid
2.1
Hexagonal Grid
A hexagonal grid H is a grid formed by a tessellation of R2 by regular hexagons
of side length 1/
√
3, as illustrated in Fig. 1. Let us consider the center points
of these hexagons, ı.e. the points of the underlying lattice Λ = Zϵ1 ⊕Zϵ2 (see
Fig. 1(a)) where ϵ1 = (1, 0)t and ϵ2 =

−1
2,
√
3
2
t
. We can then regard H as the
boundary of the Voronoi tessellation of R2 associated with the lattice Λ.
2.2
Digitized Rigid Motions
Rigid motions on R2 are isometric maps deﬁned as

U : R2 →R2
x
→Rx + t
(1)
where t ∈R2 is a translation vector and R is a rotation matrix.

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
35
Fig. 1. A visualization of a part of the hexagonal grid (a) The arrows represent the base
of the underlying lattice Λ with the white dots representing its elements. (b) Examples
of three diﬀerent point statuses induced by a rigid motion U on the hexagonal grid:
digitization cells corresponding to 0- and 2-points are marked by green lined, red dotted
patterns and labeled, with their status numbers, respectively. The white dots indicate
the positions of the images of the points of the initial set Λ under U
According to Eq. (1), we generally have U(Λ) ⊈Λ. As a consequence, in
order to deﬁne digitized rigid motions as maps from Λ to Λ, we commonly
apply rigid motions on Λ as a part of R2 and then combine the results with a
digitization operator. To deﬁne such a digitization operator on Λ, let us ﬁrst
deﬁne a digitization cell of κ ∈Λ
C(κ) =
 x ∈R2

 ∀β ∈B, ( ∥x −κ∥≤∥x −κ + β∥) ∧(∥x −κ∥< ∥x −κ −β∥)

,
where B = {ϵ1, ϵ2, ϵ1 + ϵ2} ⊂Λ. The digitization operator is then deﬁned as
a function D : R2 →Λ such that ∀x ∈R2, ∃!D(x) ∈Λ and x ∈C(D(x)). We
ﬁnally deﬁne digitized rigid motions as U = D◦U|Λ. Note that this deﬁnition of
the digitization operator is rather theoretical but not computationally relevant.
For readers who are interested in implementing digitization operators in the
hexagonal grid, we advise the method proposed by Her [7].
Due to the behavior of D, that maps R2 onto Λ, digitized rigid motions are,
most of the time, non-bijective. This leads us to deﬁne a notion of point status
with respect to digitized rigid motions, as in the case of the square grid [9].
Deﬁnition 1. Let λ ∈Λ. The set of preimages of λ ∈Λ with respect to U is
deﬁned as SU(λ) = {κ ∈Λ | U(κ) = λ}, and λ is referred to as an s-point
where s = |SU(λ)| and is called the status of λ.
Remark 2. For any λ ∈Λ, |SU(λ)| ∈{0, 1, 2}. From the digitization cell geom-
etry, we have that |SU(λ)| = 2 only when two preimages κ, σ ∈SU(λ) satisfy
∥κ −σ∥2 = 1.

36
K. Pluta et al.
The non-injective and non-surjective behaviors of digitized rigid motions result
in the existence of 2- and 0-points, respectively (see Fig. 1(b)).
3
Neighborhood Motion Maps and Remainder Range
Partitioning
3.1
Neighborhood Motion Maps
In R2, an intuitive way to deﬁne the neighborhood of a point x is to consider the
set of points that lie within a ball of a given radius centered at x. This metric
deﬁnition actually remains valid in Λ and it allows us to retrieve the classical
notion of neighborhood based on adjacency relations.
Deﬁnition 3 (Neighbourhood). The neighborhood of κ ∈Λ (of a squared
radius r ∈R+), denoted Nr(κ), is deﬁned as Nr(κ) =

κ + δ ∈Λ | ∥δ∥2
2 ≤r

.
Remark 4. N1 corresponds to the 6-neighborhood which is the smallest, non-
trivial and isotropic neighborhood of κ ∈Λ.
As stated above, the non-injective and/or non-surjective behavior of a digitized
rigid motion may result in the existence of 2- and/or 0-points. In other words, given
a point κ ∈Λ, the image of its neighborhood Nr(κ) may be distributed in a non-
homogeneous fashion within the neighborhood of the image Nr′(U(κ)), r ≤r′,
of κ with respect to the digitized rigid motion U.
In order to track these local alterations of the neighborhood of κ ∈Λ, we
consider the notion of a neighborhood motion map on Λ deﬁned as a set of vectors,
each representing the rigid motion of a neighbor.
Deﬁnition 5 (Neighborhood motion map).
The neighborhood motion
map of κ ∈Λ with respect to a digitized rigid motion U and r ∈R+ is the
function deﬁned as

GU
r (κ) : Nr(0) −→Nr′(0)
δ
−→U(κ + δ) −U(κ)
where r′ ≥r.
In other words, GU
r (κ) associates to each relative position of a point κ + δ
in the neighborhood of κ, the relative position of the image U(κ + δ) in the
neighborhood of U(κ).
Remark 6. The maximal squared radius r′ of the new neighborhood U(Nr(κ))
is slightly larger than r, due to digitization eﬀect. In particular, we have r′ = 4
for r = 1.
For a better understanding of GU
r (κ), we will consider a visual representation
of the GU
r (κ) functions as label maps. A ﬁrst—reference—map Lr associates a
speciﬁc color label to each point δ of Nr(0) for a given squared radius r (see

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
37
Fig. 2. (a) The reference label map L1. (b–c) Examples of label maps LU
1 (κ). (b) Each
point contains at most one label: the rigid motion U|N1(κ) is then locally injective. (c)
One point contains two labels: U|N1(κ) is then non-injective
Fig. 2(a) for the map L1). A second map LU
r (κ)—associated to GU
r (κ), i.e. to a
point κ and a digitized rigid motion U—associates to each point σ of Nr′(0) the
labels of all the points δ ∈Nr(0) such that U(κ + δ) −U(κ) = σ. Each σ may
contain 0, 1 or 2 labels, due to the various possible statuses of points of Λ under
digitized rigid motions (see Remark 2). Figures 2(b–c) provide some examples.
Note that a similar idea was previously proposed to study local alterations
of the neighborhood N1 under 2D digitized rotations [1], and local alterations of
Nr under 2D digitized rigid motions [8] deﬁned on the square grid.
3.2
Remainder Range Partitioning
Digitized rigid motions U = D ◦U|Λ are piecewise constant, which is a conse-
quence of the nature of D. In other words, the neighborhood motion map GU
r (κ)
evolves non-continuously according to the parameters of U that underlies U.
Our purpose is now to express how GU
r (κ) evolves.
Let us consider a point κ + δ ∈Λ in the neighborhood Nr(κ) of κ. From
Formula (1), we have
U(κ + δ) = Rδ + U(κ).
(2)
We know that U(κ) lies in a digitization cell C(U(κ)) centered at U(κ), which
implies that there exists a value ρ(κ) = U(κ) −U(κ) ∈C(0).
Deﬁnition 7. The coordinates of ρ(κ), called the remainder of κ under U, are
the fractional parts of the coordinates of U(κ) and ρ is called the remainder map
under U.
As ρ(κ) ∈C(0), this range C(0) is called the remainder range. Using ρ, we can
rewrite Eq. (2) as U(κ + δ) = Rδ + ρ(κ) + U(κ).
Without loss of generality, we can consider that U(κ) is the origin of a local
coordinate frame of the image space, i.e. U(κ) ∈C(0). In such local coordinate
frame, the former equation rewrites as U(κ + δ) = Rδ + ρ(κ). Still under this
assumption, studying the non-continuous evolution of the neighborhood motion

38
K. Pluta et al.
Fig. 3. The remainder range C(0) intersected with the translated hexagonal grids
H −Rδ, δ ∈N1(0) for rotation angle θ =
π
12. Each hexagonal grid H −Rδ is colored
with respect to each δ ∈N1(0) in the reference label map L1 (see Fig. 2)
map GU
r (κ) is equivalent to studying the behavior of U(κ + δ) = D ◦U(κ + δ)
for δ ∈Nr(0) and κ ∈Λ, with respect to the rotation angle θ deﬁning R and the
translation embedded in ρ(κ) = (x, y) ∈C(0), that deterministically depends on
(x, y, θ). The discontinuities of U(κ+δ) occur when U(κ+δ) is on the boundary
of a digitization cell. These critical cases related to U(κ+δ) can be observed via
the relative positions of ρ(κ), which are formulated by the translated hexagonal
grid H −Rδ, more precisely C(0) ∩(H −Rδ). Let us consider H −Rδ for all
δ ∈Nr(0) in C(0), namely H =

δ∈Nr(0)
(H −Rδ). Then C(0) ∩H subdivides
the remainder range into regions, as illustrated in Figs. 3 and 4 for r = 1, called
frames. Compared to the remainder range of the square grid [8], the geometry
of the frames is relatively complex (see Fig. 4). From the deﬁnition, we have the
following proposition.
Proposition 8. For any λ, κ ∈Λ, GU
r (λ) = GU
r (κ) iﬀρ(λ) and ρ(κ) are in the
same frame.
3.3
Generating Neighborhood Motion Maps for r = 1
From the above discussion, it is plain that a partition of the remainder range
given by C(0) ∩H depends on the rotation angle θ. In order to detect the
set of all neighborhood motion maps for r = 1, i.e. equivalence classes of rigid
motions U|N1(κ), we need to consider critical angles that lead to topological
changes of C(0)∩H. Indeed, from Proposition 8, we know that this is equivalent
to computing all diﬀerent frames in the remainder range. Such changes occur
when at least one frame has a null area, i.e. when at least two parallel line
segments which bound a frame have their intersection equal to a line segment.

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
39
Fig. 4. Three diﬀerent partitions of the remainder range C(0) depending on rotation
angles. (a) θ1 = π/12 ∈(α0, α1), together with non-injective zones marked by a lined
pattern. (b) θ2 = 357/1000 ∈(α1, α2) and (c) θ3 = 19/40 ∈(α2, α3). The color of each
H −Rδ corresponds to that of each neighbor δ in the label map L1
To illustrate this issue, let us consider the minimal distance among the distances
between all pairs of the parallel line segments. Thanks to rotational symmetries
by an angle of π
6 , and based on the above discussion, we can restrict, without
loss of generality, the parameter space of (x, y, θ) to C(0) ×

0, π
6
	
. Then, we
found that there exist two critical angles between 0 and π
6 , denoted by α1 and
α2, with 0 < α1 < α2 < π
6 . Note that the angles 0 and π
6 are also critical, and
denoted α0 and α3, respectively. Figure 4 presents three diﬀerent partitions of
the remainder range for angles θ1 ∈(α0, α1), θ2 ∈(α1, α2) and θ3 ∈(α2, α3),
respectively.
Based on this knowledge on critical angles, we can observe the set of all
distinct neighborhood motion maps Mr = 
U∈U

κ∈Λ

GU
r (κ)

where U is the set
of all rigid motions U deﬁned by the restricted parameter space C(0) ×

0, π
6
	
.
The cardinality of M1 is equal to 67. It should be noticed that
 
κ∈Λ

GU
1 (κ)

is constant: 49 for any U, except for θ = αi, i ∈{0, 1, 2, 3}. Indeed, we have
 
κ∈Λ

GU
1 (κ)
 = 1 for θ = α0, 43 for θ = α1, 37 for θ = α2 and 30 for θ = α3.
Such elements of the set M1 are presented in Appendix.
4
Eisenstein Rational Rotations
In this section, we study the images of the remainder map ρ with respect to
the parameters of the underlying rigid motion. In the case of the square grid,
it is known that only for rotations with rational cosine and sine, i.e. rotations
given by Pythagorean primitive triples, the remainder map does have a ﬁnite
number of images; furthermore these images form a group [1,8,10]. When, on
the contrary, cosine or/and sine are irrational, the images form an inﬁnite and
dense set: any ball of non-zero radius in the remainder range intersects images of

40
K. Pluta et al.
the remainder map. We will show that, in the hexagonal grid case, a similar result
is obtained for rotations corresponding to the counterparts of the Pythagorean
primitive triples in the hexagonal lattice. These are called Eisenstein primitive
triples, namely triples (a, b, c) ∈Z3 such that 0 < a < c < b, gcd(a, b, c) = 1 and
a2 −ab + b2 = c2 [11,12]. We shall say that the rotation matrix R is Eisenstein
rational if
R =

2a−b
2c
−
√
3b
2c
√
3b
2c
2a−b
2c

= a
c

1 0
0 1

+ b
c

−1
2 −
√
3
2
√
3
2
1
2

where (a, b, c) is an Eisenstein primitive triple. We must note that any rotation
matrix can be written in this form, with a, b, c real numbers, not Eisenstein
triples (and not even integers in general).
To begin, we focus on the rotation part of a rigid motion and deﬁne a group
G = ZRϵ1 ⊕ZRϵ2 ⊕Zϵ1 ⊕Zϵ2 and its translation G′ = G + t. We state the
following result.
Proposition 9. The group G is a rank two lattice if and only if the rotation
matrix R is Eisenstein rational.
Proof. First, we note that the density properties of the underlying group is not
aﬀected by aﬃne transformations, i.e. a lattice (resp. dense group) is transformed
into another lattice (resp. dense group). Here we consider X = [ϵ1 | ϵ2]−1 so that
{Xκ | κ ∈Λ} = Z2. Then we obtain
ˇR = XRX−1 = X

2a−b
2c
−
√
3b
2c
√
3b
2c
2a−b
2c

X−1 =
 a
c −b
c
b
c
a−b
c

,
(3)
and study ˇG = ˇRZ ( 1
0 ) ⊕ˇRZ ( 0
1 ) ⊕Z ( 1
0 ) ⊕Z ( 0
1 ), instead of G.
The generators of ˇG are given by the columns of the rational matrix B =
 ˇR
 I2
	
where I2 stands for the 2 × 2 identity matrix. As B is a rational, full
row rank matrix, it can be brought to its Hermite normal form H = [T | 02,2].
The problem of computing the Hermite normal form H of the rational matrix B
reduces to that of computing the Hermite normal form of an integer matrix: c ∈Z
is the least common multiple of all the denominators of B; compute the Hermite
normal form H′ for the integer matrix cB; ﬁnally, the Hermite normal form H
of B is obtained by c−1H′. The columns of H are the minimal generators of ˇG.
Since the rank of B is equal to 2, H gives a base (σσσ,φφφ), so that ˇG = Zσσσ ⊕Zφφφ. As
H′ gives an integer base, cˇG is an integer lattice. Finally, G = ZX−1σσσ ⊕ZX−1φφφ.
Conversely, let us prove that G is dense if (a, b, c) is not an Eisenstein prim-
itive triple (up to scaling). Again, we consider instead ˇR = [bbb1 | bbb2], and prove
that for any ε > 0 there exists eee,eee′ ∈ˇG, linearly independent, such that ∥eee∥< ε,
∥eee′∥< ε. Let {.} stand for the fractional part function. We study the images of
{Zbbb1} ∈[−1/2, 1/2)2, where bbb1 =

a/c
b/c

denotes the ﬁrst column of ˇR. If bbb1 con-
tains irrational elements, the {Zbbb1} contains inﬁnitely many distinct points. By
compactness of [−1/2, 1/2]2, we can extract a subsequence ({nkbbb1})k∈N, converg-
ing to some point in [−1/2, 1/2]2. Thus {(nk+1−nk)bbb1} = {nk+1bbb1}−{nkbbb1} con-
verges to (0, 0). In particular, we can ﬁnd integers m, p, q, where m = nk+1 −nk

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
41
for k large enough, such that eee = mbbb1 + (p, q) ∈ˇG has norm smaller than ε
3.
Note now that the second column of ˇR satisﬁes bbb2 =
 0 −1
1 −1
	
bbb1. Then we claim
that eee′ =
 0 −1
1 −1
	
eee = mbbb2 + (−q, p −q) also lies in ˇG, has norm less than 3∥eee∥
(hence less than ε) and is linearly independent from eee (the matrix ˇR has no
eigenvectors).
Consequently, bbb1 has rational coeﬃcients, and we may take a, b, c integers
with gcd equal to 1. Since cos θ = 2a−b
c
and sin θ =
√
3b
2c , we conclude that these
form an Eisenstein primitive triple.
⊓⊔
In the next section, we focus on a subgroup of G obtained from the intersection
with the remainder range, i.e. ¯G = G/Λ and its translation by t (modulo Λ).
Corollary 10. If cos θ =
2a−b
2c
and sin θ =
√
3b
2c
where (a, b, c) is a primitive
Eisenstein triple, the group ¯G = G/Λ is cyclic and |¯G| = c.
Proof. Up to the aﬃne transformation of (3), we may consider the quotient group
¨G = ˇG/Z2 and we note that for a primitive Eisenstein triple (a, b, c), any two
integers in the triple are coprime [12, p. 12]. Let us ﬁrst give a characterization of
¨G. From the proof of Proposition 9, we know that any element x ∈ˇG is a rational
vector of the form ( q1
c , q2
c ). By deﬁnition, {x} ∈¨G iﬀthere exist n, m ∈Z such
that {x} = {nbbb1 + mbbb2}, i.e. there exist integers n, m, u, v such that
 q1
c + u = n a
c −m b
c,
q2
c + v = n b
c + m a−b
c
or, equivalently,

q1 + uc = an −bm,
q2 + vc = bn + am −bm .
A linear combination of both lines yields directly bq1 −aq2 = c(−cm −bu + av),
hence bq1 −aq2 ≡0 (mod c) is a necessary condition. It is also suﬃcient. Indeed,
let us suppose that bq1 −aq2 = kc, k ∈Z.
Then, since gcd(a, b) = 1, we know that the solutions to this Diophantine
equation are of the form (q1, q2) = ℓ(a, b)+kc(β, −α), where αa+βb = 1 (B´ezout
identity) and ℓ∈Z. Consequently, ( q1
c , q2
c ) = ℓbbb1 + (kβ, −kα) lies in ˇG.
Moreover, we deduce that ¨G is cyclic with generator bbb1. Finally, {ℓbbb1} = (0, 0)
implies ℓa = uc and ℓb = vc for some integers u, v. Applying the Gauss’ lemma
to coprimes a, c, we see that ℓneeds to be a multiple of c. Therefore |¨G| = c. ⊓⊔
5
Non-injective Digitized Rigid Motions
5.1
Non-injective Frames of the Remainder Range
In Remark 2, we observed that some frames of the remainder range correspond
to neighborhood motion maps which exhibit non-injectivity of the correspond-
ing digitized rigid motion. The set of all the neighborhood motion maps M1,
presented in Appendix, allows us to identify such non-injective zones i.e., unions
of frames of the remainder range. For instance, the frames related to the neigh-
borhood motion maps of the axial coordinates; (−2, 4), (−1, 4) and (0, 3) (see
Fig. 6(top)), constitute a case of such a non-injective zone for rotation angles in

42
K. Pluta et al.
(α0, α1). Based on this observation, we can characterize the non-injectivity of a
digitized rigid motion by the presence of ρ(κ) in these speciﬁc zones (illustrated
in Fig. 4(a)).
Conjecture 11. Let C6 stand for the 6-fold discrete rotation symmetry group.
Given U ∈U and κ ∈Λ, U(κ), has two preimages κ and κ + δ where δ ∈
N1(κ) iﬀρ(κ) is in one of the zones ck(F ), ck ∈C6, where F is the parallelogram
region whose vertices are:

cos θ −1, cos θ
√
3

,

0,
1
√
3

,
 1
2

2 −cos θ −
√
3 sin θ

, 1
6
√
3 cos θ + 3 sin θ
 
,
 1
2

cos θ −
√
3 sin θ

, 1
6

3 sin(θ) + 3
√
3 cos(θ) −2
√
3
 
.
5.2
Non-injective Digitized Rigid Motions in Square and Hexagonal
Grids
In this section, we compare the loss of information induced by rigid motions on
the square and the hexagonal grids. Indeed, we aim to determine on which type
of grid digitized rigid motions preserve more information.
In accordance with the discussion in Sect. 4 and the similar discussion for
the square grid in [1,13], the density of images of the remainder map in the
non-injective zones is related to the cardinality and the structure of ¯G. On
the one hand, when ¯G is dense, it is considered as the ratio between the area
of the union of all the non-injective zones

k=1,...,6
ck(F ), and the area of the
remainder range [13]. On the other hand, when ¯G forms a lattice, it is estimated
as
|¯G∩(
  ck(F ))|
c
, k = 1, . . . , 6, where c is an element of a primitive Eisenstein
(resp. Pythagorean) triple, i.e. |¯G| [13].
Nevertheless, to facilitate our study, we will consider the former ratio between
the areas as an approximation of the information loss measure. Indeed, the rota-
Fig. 5. Comparison of the loss of information induced by digitized rigid motions on
the hexagonal and the square grids. The red and blue curves correspond to the ratios
between the areas of non-injective zones and the remainder range for the square and
the hexagonal grids, respectively. The “x” (resp. “o”) markers correspond to the values
of the information loss rate 1 −|U(SZ)|
|SZ|
(resp. 1 −|U(SL)|
|SL| )

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
43
tions induced by primitive Eisenstein (resp. Pythagorean) triples are dense; one can
always ﬁnd a relatively near rotation angle such that c is relatively high. The area-
ratio density measure can be then seen as a limit for the cardinality based ratio.
Figure 5 presents the analytical curves of the area ratios for the hexagonal and the
square grids with respect to rotation angles.
In order to validate this approximation, we also measure the loss of points for
diﬀerent sampled rotations and the following ﬁnite sets SL = Λ ∩[−100, 100]2
(resp. SZ = Z2∩[−100, 100]2) in the hexagonal (resp. square) grid. In this setting,
we use 1 −|U(SL)|
|SL|
(resp. 1 −|U(SZ)|
|SZ| ) as the measure. They are plotted as well
in Fig. 5. We note that the experimental results follow the area-ratio measure
that provides consequently a good approximation1. The obtained results allow
us to conclude that digitized rigid motions on the hexagonal grid preserve more
information than their counterparts deﬁned on the square grid.
6
Conclusion
In this article, we have extended the framework of neighborhood motion maps to
rigid motions deﬁned on the hexagonal grid—previously proposed for digitized
rigid motions deﬁned on Z2 [8,10]. Then, we studied the density of images of
the remainder map and the structure of the induced groups. Finally, we have
shown that the loss of information induced by digitization of rigid motions on
the hexagonal grid is relatively low, in comparison with those on the square grid.
Our main perspective is to use the proposed framework for studying bijective
digitized rigid motions and geometric and topological alterations induced by
digitized rigid motions on the hexagonal grid.
Following a paradigm of reproducible research, the source code of the
tool
designed
for
studying
neighborhood
motion
maps
on
the
hexago-
nal grids is provided at the following URL: https://github.com/copyme/
NeighborhoodMotionMapsTools
Appendix: Neighborhood motion maps for GU
1 and their
graph
In Sect. 3.3, we observed that there exist three topologically diﬀerent partitions of
the remainder range. They depend on rotation angles: θ1 ∈(α1, α2), θ2 ∈(α2, α3)
and θ3 ∈(α2, α3), as illustrated in Fig. 4. From Proposition 8, we also know
that each frame corresponds to a diﬀerent neighborhood motion map. Figure 6
illustrates all the neighborhood motion maps of M1, with the diﬀerent rotation
angles: θ1 ∈(α1, α2), θ2 ∈(α2, α3) and θ3 ∈(α2, α3).
The dual graph of the remainder range partitioning, in Fig. 4, is consid-
ered. In this graph G1 = {M1, E}, each node is represented by a neighborhood
motion map (or a frame), while each edge between two nodes corresponds to a
line segment shared by frames in the remainder range. Moreover, each edge is
1 Similar attempt was made in [14], but with a diﬀerent approach of measuring.

44
K. Pluta et al.
Fig. 6. All the neighborhood motion maps

κ∈Λ

GU
r (κ)

for any angle in (α0, α1) (top),
(α1, α2) (bottom, left), and (α2, α3) (bottom, right), visualized by the label map LU
1
(see Fig. 2). Neighborhood motions maps are considered as graph vertices and linked by
edges with respect to the adjacency relations of respective frames, i.e. each edge in the
graph represents a side shared by two frames in the remainder range. The neighborhood
motion maps which correspond to non-injective zones are surrounded by pink ellipses.
The elements which are changed with respect to the rotation angles in the bottom
ﬁgures are surrounded by black squares, while those which are not changed are faded.
A high resolution version of the ﬁgure can be found via the URL: https://doi.org/10.
5281/zenodo.820911

Honeycomb Geometry: Rigid Motions on the Hexagonal Grid
45
labeled with the color of the corresponding line segment (see Fig. 3). We note
that an edge color denotes the color of a point in the transition between the two
corresponding neighboring motion maps. For instance, if there exists a red hori-
zontal edge between two nodes, then we observe a transition of the red neighbor
(δ = (1, 0)) between two neighborhood motion maps connected by this edge. We
invite the reader to consider Fig. 6(top) and neighborhood motion maps of the
indices (−1, 2) and (0, 2).
Note that, neighborhood motion maps in Fig. 6 are arranged with respect to
the hexagonal lattice and each can be identiﬁed thanks to its axial coordinates
[5]. We also observe that, in such an arrangement, neighborhood motion maps
are symmetric with respect to the origin—the frame of index (0, 0). For example,
the neighborhood motion map of index (−4, 3) is symmetric to that of the index
(4, −3) (see Fig. 6).
References
1. Nouvel, B., R´emila, E.: Conﬁgurations induced by discrete rotations: periodicity
and quasi-periodicity properties. Discrete Appl. Math. 147, 325–343 (2005)
2. Kong, T., Rosenfeld, A.: Digital topology: introduction and survey. Comput. Vis.
Graph. Image Process. 48, 357–393 (1989)
3. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Picture
Analysis. Elsevier, Amsterdam (2004)
4. Kovalevsky, V.A.: Finite topology as applied to image analysis. Comput. Vis.
Graph. Image Process. 46, 141–161 (1989)
5. Middleton, L., Sivaswamy, J.: Hexagonal Image Processing: A Practical Approach.
Advances in Pattern Recognition. Springer, Berlin (2005)
6. Serra, J.: Image Analysis and Mathematical Morphology. Academic Press, London
(1982)
7. Her, I.: Geometric transformations on the hexagonal grid. IEEE Trans. Image
Process. 4, 1213–1222 (1995)
8. Pluta, K., Romon, P., Kenmochi, Y., Passat, N.: Bijective digitized rigid motions
on subsets of the plane. J. Math. Imaging Vis. 59, 84–105 (2017)
9. Ngo, P., Kenmochi, Y., Passat, N., Talbot, H.: Topology-preserving conditions for
2D digital images under rigid transformations. J. Math. Imaging Vis. 49, 418–433
(2014)
10. Nouvel, B., R´emila, ´E.: On colorations induced by discrete rotations. In: Nystr¨om,
I., Sanniti di Baja, G., Svensson, S. (eds.) DGCI 2003. LNCS, vol. 2886, pp. 174–
183. Springer, Heidelberg (2003). doi:10.1007/978-3-540-39966-7 16
11. Gilder, J.: Integer-sided triangles with an angle of 60◦. Math. Gaz. 66, 261–266
(1982)
12. Gordon, R.A.: Properties of Eisenstein triples. Math. Mag. 85, 12–25 (2012)
13. Berth´e, V., Nouvel, B.: Discrete rotations and symbolic dynamics. Theoret. Com-
put. Sci. 380, 276–285 (2007)
14. Thibault, Y.: Rotations in 2D and 3D discrete spaces. Ph.D. thesis, Universit´e
Paris-Est (2010)

Large Families of “Grey” Arrays with Perfect
Auto-Correlation and Optimal Cross-Correlation
Imants Svalbe1, Matthew Ceko1(B), and Andrew Tirkel2
1 School of Physics and Astronomy, Monash University, Melbourne, Australia
{imants.svalbe,matthew.ceko}@monash.edu
2 Scientiﬁc Technology, Melbourne, Australia
atirkel@bigpond.net.au
Abstract. Digital watermarking applications have a voracious demand
for large sets of distinct 2D arrays of variable size that possess both
strong auto-correlation and weak cross-correlation. We use the discrete
Finite Radon Transform to construct “perfect” p × p arrays, for p any
prime. Here the array elements are comprised of the integers {0, ±1, +2}.
Each array exhibits perfect periodic auto-correlation, having peak corre-
lation value p2, with all oﬀ-peak values being exactly zero. Each array,
by design, contains just 3(p −1)/2 zero elements, the minimum number
possible when using this “grey” alphabet. The grey alphabet and the low
number of zero elements maximises the eﬃciency with which these per-
fect arrays can be embedded into discrete data. The most useful aspect
of this work is that large families of such arrays can be constructed. Here
the family size, M, is given by M = p2 −1. Each of the M(M −1)/2
intra-family periodic cross-correlations is guaranteed to have one of the
three lowest possible merit factors for arrays with this alphabet. The
merit factors here are given by v2/(p2 −v2), for v = 2, 3 and 4. Whilst
the strength of the auto-correlation rises with array size p as p2, the
strength of the many (order p4) cross-correlations between all M family
members falls as 1/p2.
Keywords: Perfect arrays · Low cross-correlation arrays · Discrete pro-
jection · Finite Radon Transform · Watermarking
1
Introduction
We are motivated by the many watermarking applications, like [2,7,8], for which
one needs large families of arrays that have both low oﬀ-peak auto-correlation
and cross-correlation. For example, to provide watermark tags for each frame
of a 5 min YouTube 120 fps video requires about 36,000 arrays. If all of these
tags are unique and have a low cross-correlation, it is possible to easily isolate
and verify any individual frame within a 5 min sequence. A family comprised of
39,600 perfect arrays, each of size 199×199, would suﬃce for such an application.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 46–56, 2017.
DOI: 10.1007/978-3-319-66272-5 5

Large Families of “Grey” Arrays with Perfect Auto-Correlation
47
The cross-correlation between functions f and g is given by
Cfg(s) = f ⊗g =

f(s) · g(s −r)
(1)
where r, a shift variable, is taken over all coordinates of g, and s covers the
domain of f. Auto-correlation corresponds to the case where f = g. Perfect
arrays have periodic auto-correlation with constant oﬀ-peak values. For a p × p
array, the peak is p2 with zero elsewhere (or p2 −1 peak and −1 elsewhere), and
cross-correlation between all family members have only ±p values.
Previous work [11] used the Finite Radon Transform (FRT) to construct p×p
pseudo-noise arrays in families of size M = p (where p is a 4N −1 prime) that
had optimal periodic auto-correlation and cross-correlation, that meet the Welch
correlation bounds [12]. These families of (Legendre) arrays have an alphabet of
a single zero element with the remainder being equal numbers of ±1 elements.
“Grey” versions of these array families were also constructed that have integer
alphabets (with integer values ranging between ±√p). Recovery of a “grey”
array, A, embedded in “grey” data, B, can be advantageous, as A ⊗(A + B) ≈
2A ⊗A if we choose to embed A in those parts of B where A ≈B.
Subsequent work [10] extended the size of these array families (M) to multi-
ples of p, typically M ≈3p. This was done by blending the original array family
with distinct arrays either derived from the original array auto-correlations, or
with new arrays, also built using the FRT, but with their families generated using
diﬀerent (but equivalent) Hadamard matrices. The only concession made when
extending the family size beyond p is that the strength of each cross-correlation
now lies in a range of statistically predictable values, at or just above the lowest
possible levels.
Further extension of the size of a family of arrays well beyond p is diﬃcult
as it is hard to constrain the range of cross-correlation values. The rapidity of
this rise is, in part, due to the depth of the array alphabet. A binary array
(or any array with mostly ±1 values) has only so many combinations that can
simultaneously sustain high auto- and low cross-correlation. The “grey” versions
of the p × p Legendre arrays constructed in [11] can support a much larger and
more diverse range of well-correlated structures. The combinatorial diversity of
grey perfect arrays also makes them signiﬁcantly more secure and resistant to
hacking.
However, the balance theorem ensures that the sum of the array values dic-
tates the sum over all correlation values [3]. This ensures that alphabets spanning
a wide range of greys also require a rapid increase in the number of zero ele-
ments in those perfect arrays (see Sect. 5). The number of zero elements in a
perfect-correlation array increases with the square of the values of the non-zero
elements. Arrays containing a large number of zero elements have reduced oper-
ational eﬃciency, as the zero terms “change nothing” when embedded into any
local data.
For these reasons, we have investigated construction of perfect p × p arrays
with a restricted grey alphabet of just {0, ±1, +2}. We introduce a minimal
number, (p −1)/2, of elements having value +2, thus requiring 3(p −1)/2 zero

48
I. Svalbe et al.
terms in each array. The balance of these array values (always a clear majority)
are either ±1. The presence of a relatively few extra zeroes reduces the eﬃciency
of these arrays, by O(1/p), but this becomes less signiﬁcant for large p. Very large
numbers of such arrays can be made, where each array contains a ﬁxed proportion
of each grey element. We can then select large families of arrays, where the intra-
family array cross-correlations are restricted to the lowest possible levels.
Section 2 reviews the important link between the correlations of arrays and
the correlations between projection of those arrays. This link permits the con-
struction of 2D perfect arrays from 1D perfect projections. Section 3 reviews
the Finite Radon Transform, a discrete projection scheme whose inverse back-
projection permits exact reconstruction of any p × p set from p + 1 discrete pro-
jections, for p prime. Section 4 reviews the use of aﬃne transforms as a means to
produce many distinct variants of a perfect array that retain the original array
and correlation properties. Section 5 introduces a method to construct perfect
arrays with a ﬁxed “grey” alphabet and tightly bounded cross-correlation values.
It then shows how to assemble a large family of such arrays. Section 6 presents
some results for example array families. Ways to improve this technique and
future work are highlighted in Sect. 7.
2
Projection Preserves Moments and Correlations
The central slice theorem [1] states that projected views of a distribution pre-
serve the Fourier transform of the distribution. This is the main result underlying
image reconstruction methods for computed tomography. As a corollary of the
central slice theorem, moments and correlations of a distribution are also pre-
served under projection. This means, for example, that the auto-correlation of
the projected view of some object is equal to the same projected view of the full
object auto-correlation [4,6]. The projections of any distribution inherit that
distribution’s correlation properties.
We use this result in reverse to construct arrays with any desired correlation
properties. For example, we assemble a set of 1D projections, each having perfect
auto-correlation. We then reconstruct from that set a 2D object that inherits
their perfect auto-correlation [10,11].
The cross-correlation Cfg between function f and g is deﬁned in Eq. (1).
Correlations are termed periodic where the sum of the products of overlapped
functions is taken over cyclic boundary conditions and termed aperiodic when
zeroes extend the function boundaries. Auto-correlation is the case where f = g.
The correlation results as presented here are for periodic arrays. In practice,
the aperiodic correlation results are more relevant. However, there the boundary
conditions are usually not zero, but depend on the values of the data in which
the arrays are embedded.
3
Using 1D FRT Projections to Build 2D Arrays
We exploit the correlation-preserving property of projections as given in Sect. 2.
The discrete Finite Radon Transform [5] is used to provide a unique and exact

Large Families of “Grey” Arrays with Perfect Auto-Correlation
49
reconstruction of any 2D p × p object from its p + 1 1D projected views. Here
p must be prime to ensure the (cyclically wrapped) projections are uncoupled,
as each projection fully tiles a p × p array, exactly once, at all positions, in a
distinct pattern.
Projection R(t, m) of an image I(x, y) starts from translate t, 0 ≤t < p.
Usually t is deﬁned as one of the pixels along the top row of a p × p image. Each
1D projection is comprised of p parallel rays, where each ray sums p pixel values
in I(x, y) that are located at p steps beginning from t, each step being m pixels
across and one pixel down, wrapping periodically around the ray as required,
where 0 ≤m ≤p. Projections 0 and p are column and row sums of I(x, y),
respectively.
R(t, m) =

I(⟨t + my⟩p, y)
(2)
where ⟨j⟩p means j modulus p. Back-projecting each of the p + 1 1D projections
across a zeroed p × p array at the complemented angles (m′ = p −m) and
normalising the result recovers, exactly, the 2D data that was projected.
4
Aﬃne Transforms Preserve Correlations
Just as discrete projection preserves correlation, so too does aﬃne transforma-
tion. For example, a 2D aﬃne transformation (reversibly) maps each pixel (x, y)
of a prime p×p 2D image to a new location (x′, y′). Under matrix multiplication
in homogeneous coordinates (modulus p):
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
a b e
c d f
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
where the values, 0 ≤a, b, c, d, e, f < p, are arbitrary integer transform coeﬃ-
cients, provided only that the upper matrix [a b; c d] has non-zero determinant
(modulus p).
The coeﬃcients e and f serve as a discrete translation vector; hence we always
set these to 0, as simple translations of an array exhibit the same (periodic)
correlations. When [a b; c d] = [j −i; i j], the aﬃne transform rotates the array
by the discrete angle i:j, when [a b; c d] = [j i; i j], the aﬃne transform skews
the array by vector i:j.
With 4 arbitrary aﬃne transform coeﬃcients, a single 13 × 13 perfect array
A thus has 124 distinct aﬃne variations, each of which has perfect correlation,
since the original A is perfect. The very many cross-correlations between these
arrays will vary from being optimally low through to many cases where the trans-
formed array is a cyclic shift of A (equivalent under periodic correlations). For
watermarking applications, sign changed, reﬂected or transposed arrays should
be avoided.
For p×p arrays, we know in advance the exact set of angles i:j that correspond
to the complete set of p + 1 discrete projections of the FRT for a p × p array [9].
If we avoid the simple axial rotations 1:0 (90◦) and 0:1 (0◦), we can, without

50
I. Svalbe et al.
redundancy, rotate each original array A by aﬃne coeﬃcients i:j to obtain up to
p −3 distinct copies A′ of each A, whilst preserving the original correlation
properties. Aﬃne skews for angles ±1:1 are skipped, because they yield a zero
determinant. Similarly, for all p = 4n + 1 primes, there is one set of degenerate
rotations ±i:j (and ±j:i) that are skipped, for the case i2 + j2 = p2, which also
has zero determinant [9].
5
Construction of a Family of Perfect Arrays with
Alphabet {0, ±1, +2}
In this section, we detail the construction of families of perfect p×p arrays using
the alphabet {−1, 0, +1, +2}. The FRT is employed to construct perfect arrays
using distinct cyclic shifts. Aﬃne rotations can then be used to extend the size
of the array family.
5.1
Array Construction
Discrete 1D “delta” functions (or unit impulses) of length p, for example [1 0
0 0 0 0 0], can be used to create the p + 1 FRT projections (as done in [11]).
A delta function has a perfect 1D auto-correlation, hence so too will any 2D
array reconstructed from these 1D delta projections by applying the FRT inverse
transform.
We want to minimise the number of zeroes in the 2D array reconstructed
from these projections. This requires that the 1D rays back-projected from each
view angle (m1) must intersect with the rays from other angles (m2) at as many
distinct array positions as possible. This condition can be achieved by judicious
adjustment of the (circular) phase shift of each delta function, for example from
[1 0 0 0 0 0 0] to something like [0 0 0 1 0 0 0].
In the FRT, each ray (m, t) is back-projected [5] as the line that passes
through the image points (x, y), where
x = −my + t.
(3)
We want to ensure that the delta impulse from projection m1 intersects with
the delta impulse from projection m2 at a distinct point (x12, y12) for each pair
m1, m2. We assign
t = 1/m
(4)
for 1 ≤m ≤p −1, and substitute into (3) so the rays for projections m1 and
m2 have
x12 = −m1y12 + 1
m1
= −m2y12 + 1
m2
y12(m2 −m1) =
1
m2
−1
m1
= −(m2 −m1)
m1m2

Large Families of “Grey” Arrays with Perfect Auto-Correlation
51
which intersects at (x12, y12) =

1
m1 +
1
m2 ,
−1
m1m2

. Alternatively, we can assign
t = m2
(5)
for 1 ≤m ≤p −1, and substitute into (3) so the rays m1 and m2 have
x12 = −m1y12 + m2
1 = −m2y12 + m2
2
y12(m2 −m1) = m2
2 −m2
1 = (m2 −m1)(m2 + m1)
y = m1 + m2 and x = −m1m2
which intersects at (x12, y12) = (−m1m2, m1 + m2).
Assigning projection m to have cyclic shift t = 1/m or t = m2 imposes a
strong symmetry on the FRT matrix, as each projection m has a negative coun-
terpart m′ = p −m = −m, and then m2 = m′
2, and 1/m = −1/m′. The “near-
orthogonality” of these shift assignments is evident in the “pseudo-Hadamard”
constructed from the p × p matrix product of the shifted delta functions of the
FRT, B, with the shifted impulses of its transpose, shown as B∗BT , in Fig. 1 (a)
for t = 1/m and (b) for t = m2. We use the term near-orthogonal to reference
the fact that there are few non-zero elements that lie oﬀthe diagonal of B ∗BT .
Fig. 1. FRT projection matrices for a 2D array built from 1D phase shifted delta
functions, for p = 7, (a) t = 1/m, (b) t = m2. The pseudo-orthogonality of these phase
shifts is shown on the right via the matrix product of their 2D FRT arrays

52
I. Svalbe et al.
Typical arrays reconstructed from FRT’s that are built using delta functions
where the phase-shift t for projection m are given by t = 1/m are shown in
Fig. 2a and for t = m2 in Fig. 2b. Note that these perfect p × p arrays all have
sum = p. All arrays made this way will have the same histograms: for the 49
elements in each 7 × 7 array, 18 elements have value −1, 9 are zero, 19 are +1
and 3 are +2 elements, giving sum = 7.
We extend the size of the array families by computing FRT matrices with
shifts, t, that are linear multiples of (4, 5), modulus p, which then undergo many
aﬃne rotations and skews. Combining these operations can produce some dupli-
cated arrays. Although every distinct FRT set corresponds to a unique array,
some scaled mapping of the FRT variables m and t can be degenerate. For exam-
ple, the FRT of the transpose of a p × p array, BT (m, t), can be obtained from
shuﬄing the FRT of the original array, B(m, t), by m′ = 1/m and t′ = −t/m (as
the transpose maps projection angle x:y to y:x). The shifts of Eq. (4) are thus
very close to a transpose operation. Similar structural overlaps in reconstructed
arrays can result from axial rotations or symmetric reﬂections.
Fig. 2. 7 × 7 arrays reconstructed from (a) FRT Ba from Fig. 1 (a). (b) Bb from Fig. 1
(b). (c) The auto-correlation for arrays (a) and (b) is perfect. (d) The cross-correlation
between (a) and (b) has type L2. This example shows the strongest cross we accept.
We assign frequencies k, l, m and n to the occurrence of grey elements −1,
0, +1 and +2 respectively in any p × p array. The FRT translates, t, arrange
the (p −1) projections to have distinct intersections as pairs, yielding (p −1)/2
elements with value +2 in the ﬁnal array, thus ﬁxing n = (p −1)/2. The sum
over all p2 elements of a p × p array is then
−1 · k + 0 · l + 1 · m + 2 · (p −1)/2 = p.
(6)

Large Families of “Grey” Arrays with Perfect Auto-Correlation
53
The sum of the array auto-correlation values, by the balance theorem [3],
means
(−1)2 · k + (0)2 · l + (+1)2 · m + (+2)2 · (p −1)/2 = p2.
(7)
From (6), m−k = 1 and from (7), m+k = p2−2p+2, giving m = (p−1)2/2,
k = m + 1 and l = 3(p −1)/2. Any p × p array (with p prime) made using the
FRT with these values of t will have a ﬁxed histogram for its element values, −1
through +2, as [(p −1)2/2, 3(p −1)/2, (p −1)2/2 + 1, (p −1)/2].
The ﬁxed histogram of element values permits quantiﬁcation of the merit
factors for the periodic cross-correlation of these arrays. The merit factor (MF)
is deﬁned as the square of the peak correlation value divided by the sum of all
p2 −1 oﬀ-peak values. Perfect arrays are, by deﬁnition, spectrally ﬂat. All cross-
correlations between pairs of spectrally ﬂat arrays are also spectrally ﬂat by the
convolution theorem, and hence those cross-correlations are also perfect arrays
themselves.
The lowest possible maximum value of any cross-correlation is ±1 · 2 = ±2
(it cannot be ±1, as one of the 2’s will line up at least once with the majority
of ±1 terms). The sum of all array terms squared is always p2, hence L0, the
lowest possible MF value, is given by L0 = 22/(p2 −22) = 4/(p2 −4).
The next level possible cross level, L1, corresponds to a maximum cross-
correlation sum of 3, giving L1 = 32/(p2 −32). The next possible level has
cross-correlation value = 4, thus L2 = 16/(p2 −16). The next level L3 = (p −
1)2/(2p −1), corresponds to arrays where all the ones line up, and ﬁnally L4 =
p2/0 = ∞, when the two arrays are identical (and the cross becomes a perfect
auto-correlation).
Here L0, L1, L2, L3 and L4 are the only possible periodic cross-correlation
values between these arrays when built using symmetric 1D projections, for any
array size p. For p = 7, L3 corresponds to a peak cross value of p −1 = 6. A cor-
relation peak value of 5 is not possible for a cross between these arrays. However
we can construct diﬀerent arrays (that require an asymmetric set of cyclic shifts
for diﬀerent projections m in the FRT) to give an alphabet {0, ±1, 2, 5}. This
array, for p = 7, contains 27 zeroes, compared to just 9 zeroes for the arrays with
alphabet {0, ±1, 2}. These “asymmetrically made” grey arrays also, of course,
still retain perfect auto-correlations.
Note the merit factors L0, L1 and L2 are all < 1 for any prime p > 5, while L3
denotes a strong cross-correlation with MF of order p/2 and L4 means the two
arrays are a perfect match. When selecting arrays to build an extended family,
we restrict the choice of arrays to be only those that yield cross-correlations of
L0, L1 or L2, preferably choosing the sets of arrays that have a larger fraction
of crosses being either L0 or L1.
5.2
Building Array Families
To construct a family of arrays, a set A1 of p −1 seed arrays is made using the
FRT with delta functions as 1D projections. Each array is made using (4) and
a distinct cyclic shift t = α/m, for 1 ≤α ≤p −1. A second set A2 of p −1 seed

54
I. Svalbe et al.
arrays is made using (5) and cyclic shifts t = αm2, again for 1 ≤α ≤p −1. The
translates chosen for the remaining FRT projections, for m = 0 and p in set A1,
can be ﬁxed independently of the assigned shifts necessary for the (p−1) paired
intersecting rays. If the m = 0 and p rays are all set to t = 0, the arrays A2 are
the transpose of the arrays in A1, listed in reverse order. A free choice is possible
in the assignments of parameter t for the perpendicular rays m = 0 and m = p
when constructing the arrays for set A1. This permits several distinct perfect A1
families to be constructed whilst using the same pairing rules that ﬁx the array
histogram.
The sets A1 and A2 are pooled to form AP . The seed family AP is then
aﬃne rotated by a selection of the FRT projection angles i:j for that prime p to
produce array family AR. The seed family AP is skewed by selected valid skew
vectors to form array family AS. The two families AR and AS are then pooled
to form a (large) family AT . Any duplicate copies of arrays formed by matched
aﬃne transformations are removed. Each array in AT has perfect periodic auto-
correlation. All intra-family correlations for AT are checked and those pairings
that produce correlation values L3 and L4 are discarded from AT to produce
the ﬁnal optimal family A.
This method of pooling rotated and skewed arrays ensures all valid possible
variants of A1 and A2 are produced. The ﬁnal thinned set A then is always
comprised of p2 −1 arrays. There are also many diﬀerent ways to thin and
discard pairs of arrays from AT to select the ﬁnal family A. We select arrays
with the most favourable distribution of cross-correlation values L0, L1 and L2
that best suit a given application.
6
Results
Table 1 presents example results for families of p × p arrays where p = 7, 23
and 43 with family sizes 48, 528 and 1848, respectively. In each case, all auto-
correlations within each family are perfect, having peak values of 49, 529 and
1849 respectively. The merit factor (MF) for any intra-family periodic cross-
correlations is either L0, L1 or L2, having the values as listed. The relative
frequencies of the L0, L1 or L2 occurrences are given to highlight the distribution
of cross-correlation values between all family members.
The family of arrays for p = 23 is generated as follows: each of the initial
seed sets A1 and A2 contains p −1 = 22 perfect arrays. Each of these 23 × 23
arrays contains 529 elements; 242 elements being −1, 33 zeroes, 243 +1 elements
and 11 +2 values. Their periodic auto-correlation peak = 529, with all oﬀ-peak
entries = 0. Each of the 231 cross-correlations between the 22 arrays in A1 (and
also between all A2 members) has the minimal MF L0 = 0.00762.
Set AP = (A1 + A2) has 44 distinct arrays. Aﬃne rotation of the set AP ,
by 11 FRT angles, produces 484 more perfect arrays, 396 of which are distinct
(duplicate free). When AP is skewed by 10 FRT angles, another 440 perfect
arrays are produced, of which 308 are duplicate-free. The resulting 396 + 308 +
44 = 748, 23×23 arrays are pooled as set AT . AT is checked for cross-correlations,

Large Families of “Grey” Arrays with Perfect Auto-Correlation
55
Table 1. Cross-correlation values and their relative frequencies for two sets.
48 7 × 7 perfect arrays
MF
L0 = 0.08889 L1 = 0.22500 L2 = 0.48485
Set 1 0.44681
0.44681
0.10638
Set 2 0.36171
0.57447
0.06383
528 23 × 23 perfect arrays
MF
L0 = 0.00762 L1 = 0.01731 L2 = 0.03119
Set 1 0.03985
0.84788
0.11227
Set 2 0.03985
0.83428
0.12582
1848 43 × 43 perfect arrays
MF
L0 = 0.00227 L1 = 0.00489 L2 = 0.00873
Set 1 0.02220
0.83881
0.15216
Set 2 0.02220
0.83452
0.14328
from which 220 arrays, whose crosses give an MF > 1, are discarded, leaving the
ﬁnal family A of 748 −220 = 528 perfect arrays. Two diﬀerent families of 528
arrays were selected from AT , each has a slightly diﬀerent distribution of cross-
correlations over the same low cross-correlation merit factors.
Table 1 shows that the MF for the bulk of the intra-family cross-correlation
values are mainly of type L1, but that selecting arrays in diﬀerent ways can,
at least marginally, alter the proportion of L0, L1 and L2 results. However, the
p = 7 case shows that it is possible to select diﬀerent families of arrays from AT
that favour the production of more L0 crosses and that minimise the number of
L2 crosses.
7
Conclusions and Further Work
The families of p × p arrays constructed here have perfect auto-correlation
with guaranteed low cross-correlation values between all family members. These
arrays have a restricted grey alphabet (elements of {0, ±1, +2}) with a small
number, 3(p−1)/2, of zero elements. This makes them highly eﬃcient and secure
when embedded as watermarks. The ﬁxed frequency of array element values per-
mits selection of p2 −1 family members that have cross-correlation values with
just three of the lowest possible merit factors, MF = v2/(p2 −v2), for v = 2, 3
and 4.
The size of AT needs to be kept small to avoid excessive computation to do
the cross-correlation checks required to remove arrays that have MF > 1. If AT
has size M, M(M −1)/2 crosses of size p × p need to be checked. However,
pooling AT to be too small may also restrict the range of distinct or partially
overlapped solution sets A that can be selected from AT . It may be possible to
deﬁne a set of aﬃne rotations and skews to directly produce a ﬁnal set A without
computing the larger intermediate set AT .

56
I. Svalbe et al.
At present, we select the ﬁnal A arrays by deleting of one of each pair of
arrays in AT that has MF > 1. The order of these deletions can be done in
several diﬀerent ways. This selection and deletion process could be organised
more strategically.
The FRT can be adapted to reconstruct higher dimensional arrays from 1D
projections. The technique developed here can then be extended to produce
families of nD arrays.
We have yet to examine perfect arrays over other, larger alphabets, even
in 2D, to see if the merit factors of their cross-correlations can also be ﬁxed
algebraically.
Acknowledgments. The School of Physics and Astronomy at Monash University,
Australia, has supported and provided funds for this work. M.C. has the support of
the Australian government’s Research Training Program (RTP) and the J.L. William
scholarship from the School of Physics and Astronomy at Monash University.
References
1. Bracewell, R.: Strip integration in radio astronomy. Aust. J. Phys. 9(2), 198–217
(1956)
2. Chauhan, S., Rizvi, S.: A survey: Digital audio watermarking techniques and appli-
cations. In: 2013 4th International Conference on Computer and Communication
Technology (ICCCT), pp. 185–192. IEEE (2013)
3. Golomb, S., Gong, G.: Signal Design for Good Correlation: For Wireless Com-
munication, Cryptography, and Radar. Cambridge University Press, Cambridge
(2005)
4. Gu´edon, J.: The Mojette Transform: Theory and Applications. ISTE John Wiley
& Sons, London (2009)
5. Mat´uˇs, F., Flusser, J.: Image representation via a ﬁnite Radon transform. IEEE
Trans. Pattern Anal. Mach. Intell. 15(10), 996–1006 (1993)
6. Phillip´e, O.: Image representation for joint source channel coding for QoS networks
(Ph.D. Thesis). University of Nantes (1998)
7. Potdar, V., Han, S., Chang, E.: A survey of digital image watermarking techniques.
In: 2005 3rd IEEE International Conference on Industrial Informatics, INDIN 2005,
pp. 709–716. IEEE (2005)
8. Sethuraman, P., Srinivasan, R.: Survey of digital video watermarking techniques
and its applications. Eng. Sci. 1(1), 22–27 (2016)
9. Svalbe, I.: Sampling properties of the discrete Radon transform. Discrete App.
Math. 139(1), 265–281 (2004)
10. Svalbe, I., Tirkel, A.: Extended families of 2D arrays with near optimal auto and
low cross-correlation. EURASIP J. Adv. Sig. Process. 2017(1), 18 (2017)
11. Tirkel, A., Cavy, B., Svalbe, I.: Families of multi-dimensional arrays with optimal
correlations between all members. Electron. Lett. 51(15), 1167–1168 (2015)
12. Welch, L.: Lower bounds on the maximum cross correlation of signals. IEEE Trans.
Inf. Theory 20(3), 397–399 (1974)

The Minimum Barrier Distance:
A Summary of Recent Advances
Robin Strand1(B), Krzysztof Chris Ciesielski2,3, Filip Malmberg1,
and Punam K. Saha4
1 Centre for Image Analysis, Uppsala University, Uppsala, Sweden
robin@cb.uu.se
2 Department of Mathematics, West Virginia University, Morgantown, USA
3 Department of Radiology, MIPG, University of Pennsylvania, Philadelphia, USA
4 Department of Electrical and Computer Engineering and Department of Radiology,
University of Iowa, Iowa City, USA
Abstract. In this paper we present an overview and summary of recent
results of the minimum barrier distance (MBD), a distance operator
that is a promising tool in several image processing applications. The
theory constitutes of the continuous MBD in Rn, its discrete formula-
tion in Zn (in two diﬀerent natural formulations), and of the discussion
of convergence of discrete MBDs to their continuous counterpart. We
describe two algorithms that compute MBD, one very fast but returning
only approximate MBD, the other a bit slower, but returning the exact
MBD. Finally, some image processing applications of MBD are presented
and the directions of potential future research in this area are indicated.
1
Introduction
Distance functions and their transforms (DTs, where each pixel is assigned the
distance to the closest seed pixel) have been used extensively in image process-
ing applications. Since Rosenfeld and Pfaltz deﬁned distance transforms on the
binary images in the 1960’s [16,17], a huge number of distance transform meth-
ods have been developed in theoretical and application setting. The classical,
binary DTs are useful, for example, for measurement and description of binary
objects. Distance functions and transforms that are deﬁned as minimal cost
paths in general images include geodesic distance [9], fuzzy distances [19], and
the minimax cost function; they have been used, for example, in segmentation
and saliency detection [18]. The most common algorithms for computing dis-
tance transforms are: (i) raster scanning methods, where distance values are
propagated by sequentially scanning the image in a pre-deﬁned order [1,5]; (ii)
wave-front propagation methods, where distance values are propagated from
low distance value points (the object border) to the points with higher distance
values, using data structure containing previously visited points and their cor-
responding distance values, until all points have been visited [15,21]; and (iii)
separable algorithms, where one-dimensional subsets of the image are scanned
separately, until all principal directions have been scanned [4,20].
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 57–68, 2017.
DOI: 10.1007/978-3-319-66272-5 6

58
R. Strand et al.
Recently, we introduced in [22] the minimal barrier distance (MBD) func-
tion, based on the minimal length of the interval of intensity values along a
path between two points, see Fig. 1. The MBD diﬀers from traditional distance
functions in a number of aspects. For example, the length of a path may remain
constant during its growth until a new stronger barrier is met on the path. This
subtle shift in the notion of path length allows the new distance function to cap-
ture separation between two points in a “connectivity”-sense. Our experiments
have shown that the MBD is stable to noise, seed point position [3,10,22,23].
The MBD has many interesting theoretical properties, and has been shown to
be a potentially useful tool in image processing [3,7,10,22,25].
Fig. 1. One-dimensional example of the minimum barrier. Left: The graph of a function
(blue) and, for each x on the horizontal axis, the maximum and minimum value attained
in [0, x] (red). Right: The pointwise diﬀerence between the maximum and minimum
values – the minimum barrier. The minimum barrier distance between two points in
an image is deﬁned by a path with minimum barrier between the points. (Color ﬁgure
online)
This paper collects and compares published results on the MBD and presents
them in a uniﬁed framework. In Sect. 2, the MBD is deﬁned on Rn with the
two equivalent formulations ρ (Eq. 2) and ϕ (Eq. 3). In Sect. 3, their discrete
counterparts ρ (Eq. 4) and ϕ (Eq. 5) are deﬁned in Zn. Section 4 gives results on
convergence between the diﬀerent versions of MBD. The diﬀerent MBD versions
are related as follows:
The arrows indicate conceptual relations. The top row shows the continuous
formulations and the bottom row the two diﬀerent discrete versions.
Section 5 describes the algorithms for computation of DTs. The MBD based
on ρ is, similar to [8], not smooth in the sense of [2,6]. As a consequence, exact
DTs for ρ cannot be computed as eﬃciently as for ϕ, but both approximate
algorithms and eﬃcient algorithms for exact computation have been developed.

The Minimum Barrier Distance: A Summary of Recent Advances
59
These are given in Sect. 5.1. Section 5.2 shows that DTs for ϕ can be eﬃciently
computed by standard wave-front propagation techniques. Applications in image
processing are presented in Sect. 6.
2
The Minimum Barrier Distance in Rn
We consider bounded maps f : D →R and their graphs A = {(x, f(x)) : x ∈D}.
When f : D →[0, 1] the set A can be seen as a fuzzy subset of D with f being
its membership function.
A path from p to q (where p, q ∈D and D ⊂Rn) is any continuous function
π: [0, 1] →D with p = π(0) and q = π(1). The symbol Πp,q (without subscripts,
when p and q are clear from the context) is used to denote the family of all such
paths. We consider functions f : D →R such that
– f is bounded;
– f is continuous;
– D ⊂Rn is path connected, that is, for every p, q ∈D there exists a path
π: [0, 1] →D from p to q.
The barrier of a path π: [0, 1] →D is the number
τ(π) = max
t
f(π(t)) −min
t
f(π(t)) = max
t0,t1

f(π(t1)) −f(π(t0))

.
(1)
The maxima and minima in Eq. 1 are attained due to the Extreme Value Theo-
rem. The minimum barrier distance between p, q ∈D is the number
ρ(p, q) =
inf
π∈Πp,q τ(π).
(2)
2.1
Metricity
Deﬁnition 1. A function d: D × D →[0, ∞) is a metric on a set D provided,
for every x, y, z ∈D,
(i) d(x, x) = 0 (identity);
(ii) d(x, y) > 0 for all x ̸= y (positivity);
(iii) d(x, y) = d(y, x) (symmetry);
(iv) d(x, z) ≤d(x, y) + d(y, z) (triangle inequality).
It is easy to see that metricity property (ii) does not hold for the minimum
barrier distance (take a constant function f for example). Metricity properties
(i), (iii), and (iv) are obeyed, and the minimum barrier distance is therefore a
pseudo-metric [22].

60
R. Strand et al.
2.2
Alternative Formulation
Now, the mapping ϕ: D × D →[0, ∞) is deﬁned by two separate paths, via the
formula
ϕ(p, q) =
inf
π1∈Πp,q max
t
f(π1(t)) −
sup
π0∈Πp,q
min
t
f(π0(t)).
(3)
In Theorem 1 below, we see that the mappings ϕ and ρ are identical under mild
assumptions on the set D.
Recall, that a set D ⊂Rn is simply connected, provided it is path connected
and for all p, q ∈D the paths π0, π1 ∈Πp,q are homotopic, that is, there exists
a continuous function h: [0, 1]2 →D, known as a homotopy between π0 and
π1, such that h(·, 0) = π0(·), h(·, 1) = π1(·), and the maps h(0, ·), h(1, ·) are
constant. Intuitively, the homotopy condition means that D has no holes.
Theorem 1 ([22]). If D ⊂Rn is simply connected, then the mappings ρ and ϕ
are equal, that is, ρ(p, q) = ϕ(p, q) for all p, q ∈D.
3
The Minimum Barrier Distance in Zn
In the digital setting, we consider the (bounded) functions f : D →R, where
the digital scene D is a ﬁnite subset of a digital space ⟨φZn, α⟩, where φ is a
positive number and α is an adjacency relation such that two points in φZn
are α-adjacent provided that no coordinate diﬀers by more than φ and that
the points diﬀer in exactly one coordinate. Note that this is equivalent to the
standard 6-adjacency [11] in a 3D digital space.
A digital path in a subset D of the space ⟨φZn, α⟩is any ordered sequence
π = ⟨π(0), π(1), . . . , π(k)⟩of points in D such that π(i) and π(i −1) are α-
adjacent for all i ∈{1, 2, . . . , k}. If π(0) = p and π(k) = q, we say that the path
π is from p to q. For a ﬁxed set D, a family of all paths in D from p to q is
denoted by Πp,q (with the subscripts omitted when p and q are clear from the
context). Note that the digital paths are denoted by π, while the paths in the
continuous space Rn are denoted by π.
In what follows, we assume that the digital scenes D are of the rectangular
form Dφ = D ∩φZn, where D = {x ∈Rn : Li ≤x(i) ≤Ui} for some real
numbers Li, Ui such that Li < Ui for all i. In particular, any two points in D
are connected by a path.
In view of Theorem 1, there are two natural ways of deﬁning the discrete
minimum barrier distance for f : D →R, the discretization of the formula for
ρ(p, q) and of that for ϕ(p, q):

The Minimum Barrier Distance: A Summary of Recent Advances
61
– Discretization I
ρ(p, q) = min
π∈Πp,q

max
i

f(π(i))

−min
j

f(π(j))

,
(4)
– Discretization II
ϕ(p, q) =
min

π1∈Πp,q
max
i

f(π1(i))

−max

π0∈Πp,q
min
j

f(π0(j))

.
(5)
We know from [22] that each of the functions ρ and ϕ is a pseudo-metric on D
and that ϕ(p, q) ≤ρ(p, q) for all p, q ∈D.
4
Convergence Properties
Next we will see, in Theorem 2, that if f : Dφ →R is a discretization of a
continuous function f deﬁned on a rectangular region D, then, for a suﬃciently
small φ, the numbers ϕ(p, q) and ρ(p, q) well approximate ϕ(p, q) = ρ(p, q).
Theorem 2 (Theorem 2 in [22]). Let D be a rectangular region in Rn and
f : D →R be continuous. Let ρ and ϕ be the discrete minimum barrier distance
functions for the sampling f of f on Dφ, that is, with f(p) = f(p) for all p ∈Dφ.
Then, for every ε > 0 there exists a φ0 > 0 such that for every φ ∈(0, φ0]
|ρ(p, q) −ρ(p, q)| < ε and |ϕ(p, q) −ϕ(p, q)| < ε for all p, q ∈Dφ.
More precisely, this holds for any φ0 > 0 such that |f(x) −f(y)| < ε/4 for any
x, y ∈D with ∥x −y∥≤φ0
√n/2.
Since, by Theorem 2, the values ρ(p, q) and ϕ(p, q) converge, as φ →0, to
ρ(p, q) = ϕ(p, q), we obtain the following corollary.
Corollary 1 (Corollary 1 in [22] and Theorem 1 in [3]). Let Dφ, f, ρ
and ϕ be as in Theorem 2. Then
max
p,q∈
Dφ
|ρ(p, q) −ϕ(p, q)| →0 as φ →0.
More precisely, let ε = max
	
| f(x) −f(y)|: x, y ∈Dφ are (3n −1)-adjacent

,
with f as in Theorem 2. Then
0 ≤ρ(p, q) −ϕ(p, q) ≤2ε for all p, q ∈Dφ.

62
R. Strand et al.
5
Discrete Distance Transform Computation
Eﬃcient distance transform computation is crucial for most applications of dis-
tance transforms. As described in the introduction, many diﬀerent computation
approaches have been proposed, including raster scanning and wave-front prop-
agation. In image segmentation, it is natural to compute DTs from seed points
in the background and in the object and then assign each point to a seed from
which it has the minimal distance. In practice, this is often computed eﬃciently
by propagating diﬀerent labels from object and background seed points together
with the distance values. In this way, the points get labeled during the DT
computation, resulting in eﬃcient computation of the labeling/segmentation.
As described above, the MBD originally is formulated in the continuous space
and oﬀers two natural discretizations. This leads to two diﬀerent problems to
solve when developing methods for computing the DT, one for each discretiza-
tion. Computing the DT of Discretization I turns out to be fairly easy, whereas
computing it for Discretization II is an intricate problem.
5.1
Distance Transform Computation of Discretization I, ρ
Approximate Computation. By propagating the minimum barrier, i.e., the
minimal interval of minimum and maximum value by a wave-front propagation
approach using auxiliary data structures that hold the minimal and maximal
attained values, an approximate distance transform can be computed [3,22]. We
call the algorithm the Dijkstra approximation algorithm (Algorithm 1 in [3]).
The minimum barrier distance based on ρ is not smooth in the sense of [2,6]
and, as a result, the obtained distance transforms is not error-free [3,14,22,25]
with this approach. However, Zhang et al. [25] gave an error bound and also
showed that for a very restricted class of 2D images, the approach gives exact
minimum barrier distance transforms, see Theorem 3 which is here adapted to
the Dijkstra approximation algorithm.
Theorem 3. [25] Let f be an integer-valued 2D image (n = 2) on a rectangular
domain D and let
f ′ =
 f
ϵ

ϵ,
where ϵ = max
	
| f(x) −f(y)|: x, y ∈D are (3n −1)-adjacent

. If the set of seed
points is an α-connected set, then the absolute error in the distance map obtained
by the Dijkstra approximation algorithm on f ′ is strictly less than ϵ.
The following Corollary holds since if ϵ = 1 in Theorem 3, then the maximum
absolute error is integer valued and strictly less than 1, that is, equal 0.

The Minimum Barrier Distance: A Summary of Recent Advances
63
Corollary 2. Let f be an integer-valued 2D image (n = 2) on a rectangular
domain D. If max
	
| f(x) −f(y)|: x, y ∈D are (3n −1)-adjacent

= 1 (i.e., if
ϵ in Theorem 3 is 1) and if the set of seed points is an α-connected set, then
the absolute error in the distance map obtained by the Dijkstra approximation
algorithm on f is error-free.
Exact Distance Transform Computation. Computing the exact discrete
MBD eﬃciently for general images is far from trivial, but it is fairly easy to
check if there exists a path between two points in a digital space within a given
interval: threshold the image at the lower and upper limits of the interval and
check if the two points are connected in the so-obtained connected regions. This
approach gives a simple, but computationally very ineﬃcient way to compute
the minimum barrier distance transform: for a given seed point, compute for
each interval the set of points it is connected to after the thresholding procedure
explained above. The distance between the seed point and another given point
is the minimum of all such intervals, for which they are connected.
A slightly more eﬃcient approach is to loop over all possible lower bounds
of the interval and compute a minimax transform from a given seed point. The
minimum barrier distance is then obtained by a min-operator of the so-obtained
distance maps.
This idea can be further optimized by sorting the priority queue in an eﬃcient
way. By popping points from the queue based on the upper limit of the attained
barrier and propagating and pushing points to the queue based on the lower
limit of the barrier while storing the minimum barrier attained at each point, it
has been proved that an error-free algorithm can be obtained [3].
5.2
Distance Transform Computation of Discretization II, ϕ
The transform ϕ(p, ·) can be eﬃciently computed since the path cost functions
maxi

f(π1(i))

and minj

f(π0(j))

are smooth in the sense of [2,6] and can
therefore be computed by Dijkstra’s algorithm, where wave-front propagation is
used to compute lowest cost paths by local propagation [3,22].
6
The Minimum Barrier Distance in Image Processing
Applications
Since color images are used in most applications, an important extension of the
MBD is the vectorial MBD, where vectorial, i.e., multi-band images, are used
as input. Diﬀerent versions of the vectorial MBD were developed by K˚arsn¨as et
al. [10] and applied to interactive color image segmentation, where images are
segmented by manually placed seed points in the image background and in the

64
R. Strand et al.
object together with a label propagation approach to segment color images.
Instead of considering the one-dimensional interval, K˚arsn¨as et al. consider
the diameter and volume of the bounding box as well as the diameter of the
(hyper-)volume of a convex hull in feature space (e.g. the RGB color space) as
the basis for the path cost. This shift in the MBD cost function deﬁnition leads to
additional problems in how the exact distance transform is computed. However,
in [10], an approximate method based on Dijkstra-like wave-front propagation
is used to compute the vectorial MBD.
The vectorial MBD on color images is used as a pre-processing step by Grand-
Brochier et al. [7] in a comparison of diﬀerent pre-processing step methods for
image segmentation. They assume that the object is centered within the image
and compute the DT from a single seed point in the center of the image. The
pixels with low distance values are then assumed to be object pixels.
A slightly diﬀerent, and more successful, approach based on a real-time imple-
mentation of the MBD by raster-scanning until convergence is presented by
Zhang et al. [25]. They detect salient objects in images as follows. As initial-
ization, the image border pixels are set to seed points. Using the assumption
that the object does not touch the image border, the pixels with high distance
values are those that belong to the object. They use a raster-scanning approach,
where each row is scanned, ﬁrst from upper left to lower right and the from lower
right to upper left, until convergence, which is very well suited for parallel imple-
mentations. By a GPU implementation based on the raster-scanning technique,
real-time performance, 80 MBD DTs per second, is achieved.
6.1
Example Applications of Diﬀerent Versions of the Minimum
Barrier Distance
In this section, diﬀerent MBD DTs with diﬀerent sets of seed points are illus-
trated by DTs of a single image from the MSRA database ([13]). In Fig. 2, all
border pixels of the image are set to seed points (c.f. [25]) and in Fig. 3, a single
seed point is centered in the image (as used in [7]).
In Fig. 2 and Fig. 3, some of the methods described in this paper are illus-
trated by applying them to a color image and its gray-scale version. The examples
show:
– how restrictive the conditions in Corollary 2 are in order to guarantee that
the obtained MBD DT is error-free,
– the exact MBD DT of Discretization I, ρ (Sect. 5.1),
– the Dijkstra approximation of Discretization I, ρ (Sect. 5.1),
– the exact MBD DT of Discretization II, ϕ (Sect. 5.2),
– the color/vectorial MBD DT using the L1-diameter of the bounding box in
the RGB and Lab color spaces, see [10] for details (Sect. 6).

The Minimum Barrier Distance: A Summary of Recent Advances
65
a
b
c
d
e
f
g
h
i
j
k
l
Fig. 2. Diﬀerent versions of Minimum Barrier Distance computed on (a) when all
border pixels are set to seed points. a: Original color image; b: Gray scale image; c: b
smoothed; d: c quantized such that the condition in Corollary 2 holds; e: exact MBD
DT, Discretization I, of b; f: Dijkstra approximation of MBD, Discretization I, of b; g:
MBD, Discretization I of d (exact MBD computed by Dijkstra approximation); h: MBD
DT, Discretization II, of b; i: Absolute pointwise diﬀerence between e and f (gray scale
between 0 (no diﬀerence) and 28 (maximum diﬀerence)) j: Absolute pointwise diﬀerence
between e and h (gray scale between 0 (no diﬀerence) and 99 (maximum diﬀerence))
k: Color MBD of a in the RGB color space. l: Color MBD of a in the Lab color space.
(Color ﬁgure online)

66
R. Strand et al.
a
b
c
d
e
f
g
h
i
j
k
l
Fig. 3. Diﬀerent versions of Minimum Barrier Distance computed on (a) when only
the center pixel is set to seed point. a: Original color image; b: Gray scale image; c: b
smoothed; d: c quantized such that the condition in Corollary 2 holds; e: exact MBD
DT, Discretization I, of b; f: Dijkstra approximation of MBD, Discretization I, of b; g:
MBD, Discretization I of d (exact MBD computed by Dijkstra approximation); h: MBD
DT, Discretization II, of b; i: Absolute pointwise diﬀerence between e and f (gray scale
between 0 (no diﬀerence) and 38 (maximum diﬀerence)) j: Absolute pointwise diﬀerence
between e and h (gray scale between 0 (no diﬀerence) and 62 (maximum diﬀerence))
k: Color MBD of a in the RGB color space. l: Color MBD of a in the Lab color space.
(Color ﬁgure online)

The Minimum Barrier Distance: A Summary of Recent Advances
67
7
Discussion
The basic idea behind the MBD is very easy to explain and MBD is straight-
forward to deﬁne, also for color images. Still, the theory of MBD is surprisingly
intricate and holds many interesting and to some extent surprising results, such
as the equivalence between the two formulations ρ and ϕ. Eﬃcient algorithms
for DT computation, together with the algorithm for exact computation, makes
MBD easy to apply in real-life applications. Even though the methods presented
and illustrated here are rather simple applications of MBD, it is a promising
tool for image processing such as segmentation and saliency detection. A more
complex method based on a minimum spanning tree representation together
with the MBD for saliency detection was recently presented in [24].
In Rn, we assume that we have continuous images f : D →R, which we in
practice do not have. However, most image acquisition methods induce smooth-
ing of the image scene by a point spread function, leading to continuous f. See
for example [12] for a discussion. A smooth f can also be found by interpolation
of a digital function (often, just an image intensity function).
Open problems include properties and eﬃcient algorithms for DT computa-
tion of the vectorial MBD, using other feature spaces than the RGB color space,
error bound for the Dijkstra approximation method in arbitrary dimensions, and
parallel implementation of the algorithm for exact DT computation.
References
1. Borgefors, G.: Distance transformations in digital images. Comput. Vision Graph.
Image Process. 34, 344–371 (1986)
2. Ciesielski, K.C., Falc˜ao, A.X., Miranda, P.A.V.: Path-value functions for which
Dijkstra’s algorithm returns optimal mapping (2017, submitted)
3. Ciesielski, K.C., Strand, R., Malmberg, F., Saha, P.K.: Eﬃcient algorithm for ﬁnd-
ing the exact minimum barrier distance. Comput. Vis. Image Underst. 123, 53–64
(2014)
4. Coeurjolly, D., Vacavant, A.: Separable distance transformation and its applica-
tions. In: Brimkov, V., Barneva, R. (eds.) Digital Geometry Algorithms. LNCVB,
vol. 2, pp. 189–214. Springer, Dordrecht (2012)
5. Danielsson, P.E.: Euclidean distance mapping. Comput. Graph. Image Process.
14, 227–248 (1980)
6. Falc˜ao, A.X., Stolﬁ, J., de Alencar Lotufo, R.: The image foresting transform: the-
ory, algorithms, and applications. IEEE Trans. Pattern Anal. Mach. Intell. 26(1),
19–29 (2004)
7. Grand-Brochier, M., Vacavant, A., Strand, R., Cerutti, G., Tougne, L.: About the
impact of pre-processing tools on segmentation methods applied for tree leaves
extraction. In: 2014 International Conference on Computer Vision Theory and
Applications (VISAPP), vol. 1, pp. 507–514. IEEE (2014)
8. Holuˇsa, M., Sojka, E.: A k-max geodesic distance and its application in image
segmentation. In: Azzopardi, G., Petkov, N. (eds.) CAIP 2015. LNCS, vol. 9256,
pp. 618–629. Springer, Cham (2015). doi:10.1007/978-3-319-23192-1 52
9. Ikonen, L., Toivanen, P.: Shortest routes on varying height surfaces using grey-level
distance transforms. Imag. Vis. Comp. 23(2), 133–141 (2005)

68
R. Strand et al.
10. K˚arsn¨as, A., Strand, R., Saha, P.K.: The vectorial minimum barrier distance. In:
2012 21st International Conference on Pattern Recognition (ICPR), pp. 792–795.
IEEE (2012)
11. Kong, T.Y., Rosenfeld, A.: Digital topology: introduction and survey. Comput.
Vis. Graph. Image Process. 48(3), 357–393 (1989)
12. K¨othe, U.: What can we learn from discrete images about the continuous world?
In: Coeurjolly, D., Sivignon, I., Tougne, L., Dupont, F. (eds.) DGCI 2008. LNCS,
vol. 4992, pp. 4–19. Springer, Heidelberg (2008). doi:10.1007/978-3-540-79126-3 2
13. Liu, T., Sun, J., Zheng, N.N., Tang, X., Shum, H.Y.: Learning to detect a salient
object. In: 2007 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1–8 (2007)
14. Mansilla, L.A., Miranda, P.A.V., Cappabianco, F.A.: Image segmentation by image
foresting transform with non-smooth connectivity functions. In: 2013 XXVI Con-
ference on Graphics, Patterns and Images, pp. 147–154. IEEE (2013)
15. Piper, J., Granum, E.: Computing distance transformations in convex and non-
convex domains. Pattern Recogn. 20(6), 599–615 (1987)
16. Rosenfeld, A., Pfaltz, J.L.: Distance functions on digital pictures. Pattern Recogn.
1, 33–61 (1968)
17. Rosenfeld, A., Pfaltz, J.L.: Sequential operations in digital picture processing. J.
ACM 13(4), 471–494 (1966)
18. Saha, P.K., Strand, R., Borgefors, G.: Digital topology and geometry in medical
imaging: a survey. IEEE Trans. Med. Imaging 34(9), 1940–1964 (2015)
19. Saha, P.K., Wehrli, F.W., Gomberg, B.R.: Fuzzy distance transform: theory, algo-
rithms, and applications. Comput. Vis. Image Underst. 86, 171–190 (2002)
20. Saito, T., Toriwaki, J.I.: New algorithms for Euclidean distance transformation
of an n-dimensional digitized picture with applications. Pattern Recogn. 27(11),
1551–1565 (1994)
21. Sethian, J.A.: A fast marching level set method for monotonically advancing fronts.
In: Proceedings of National Academy of Sciences, vol. 93, pp. 1591–1595 (1996)
22. Strand, R., Ciesielski, K.C., Malmberg, F., Saha, P.K.: The minimum barrier dis-
tance. Comput. Vis. Image Underst. 117(4), 429–437 (2013). Special Issue on Dis-
crete Geometry for Computer Imagery
23. Strand, R., Malmberg, F., Saha, P.K., Linn´er, E.: The minimum barrier distance
– stability to seed point position. In: Barcucci, E., Frosini, A., Rinaldi, S. (eds.)
DGCI 2014. LNCS, vol. 8668, pp. 111–121. Springer, Cham (2014). doi:10.1007/
978-3-319-09955-2 10
24. Wei-Chih, T., Shengfeng He, Q.Y., Chien, S.Y.: Real-time salient object detection
with a minimum spanning tree. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR 2016), pp. 2334–2342 (2016)
25. Zhang, J., Sclaroﬀ, S., Lin, Z., Shen, X., Price, B., Mech, R.: Minimum barrier
salient object detection at 80 fps. In: Proceedings of the IEEE International Con-
ference on Computer Vision, pp. 1404–1412 (2015)

Convexity-Preserving Rigid Motions
of 2D Digital Objects
Phuc Ngo1(B), Yukiko Kenmochi2, Isabelle Debled-Rennesson1,
and Nicolas Passat3
1 Universit´e de Lorraine, LORIA, UMR, 7503, Villers-l`es-Nancy, France
hoai-diem-phuc.ngo@loria.fr
2 Universit´e Paris-Est, LIGM, CNRS, Paris, France
3 Universit´e de Reims Champagne-Ardenne, CReSTIC,
Reims, France
Abstract. Rigid motions on R2 are isometric and thus preserve the
geometry and topology of objects. However, this important property is
generally lost when considering digital objects deﬁned on Z2, due to the
digitization process from R2 to Z2. In this article, we focus on the convex-
ity property of digital objects, and propose an approach for rigid motions
of digital objects which preserves this convexity. The method is extended
to non-convex objects, based on the concavity tree representation.
Keywords: Digital rigid motion · Digital convexity · Half-plane
representation · Concavity tree · Quasi-regularity
1
Introduction
Rigid motions (i.e., transformations based on translations and rotations) in Rn
are well-known topology- and geometry-preserving operations. They are fre-
quently used in image processing and image analysis. Due to digitization eﬀects,
these important properties are generally lost when considering digital images
deﬁned on Zn, as illustrated in Fig. 1.
A part of these problems, topological preservation, was studied for 2D images
under rigid transformations [19]. In a similar context, we investigate the geomet-
rical issue, in particular convexity, in this article.
Convexity plays an important role in geometry. This notion is well studied
and understood in the continuous space. A convex object in Rn is deﬁned as a
region such that the segment joining any two points within the region is also
within the region. Nevertheless, this notion is not adapted to Zn, due to the
digitization process required to convert continuous objects to digital objects, as
illustrated in Fig. 2.
In the literature, several deﬁnitions have been proposed for convexity deﬁned
on Zn, namely digital convexity, such as MP-convexity [18], S-convexity [22],
D-convexity [12], H-convexity [12]. The latter will be used in this article due
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 69–81, 2017.
DOI: 10.1007/978-3-319-66272-5 7

70
P. Ngo et al.
(a)
(b)
(c)
(d)
Fig. 1. Convexity alterations under rotation: (a) a convex digital object in 2D, and (b)
its non-convex transformed image by a rotation; (c) a convex digital object in 3D, and
(d) its non-convex transformed image by a rotation.
(a)
(b)
(c)
(d)
Fig. 2. (a–b) Convexity deﬁned on Rn is not applicable to Zn due to the digitization.
(a) A convex continuous object deﬁned on R2 and the grid Z2, (b) its digital image
represented by pixels. The segments in red and blue belong fully to (a) but not to
(b) H-convex (c) and non H-convex (d) objects. Red polygons are their convex hulls.
(Color ﬁgure online)
to its simplicity and usefulness. Roughly speaking, a digital object X ⊂Z2 is
H-convex if its convex hull does contain exactly the points of X (see Fig. 2(c–d)).
In this article, we propose a method allowing us to preserve the H-convexity of
digital objects under any rigid motions. Such a method relies on a representation
of an H-convex digital object by half-planes. The rigid motion is then applied
on such half-plane representation and followed by a digitization. Furthermore,
we investigate the condition under which the H-convexity of digital objects is
preserved by arbitrary rigid motions.
The paper is organized as follows. After recalling in Sect. 2 basic notions
about rigid motions on digital objects, Sect. 3 presents a representation of digital
object, well-adapted to H-convexity recognition. A convexity-preserving method
for digital objects under rigid motions is then proposed in Sect. 4. Section 5
investigates the problem of rigid motions of non-convex digital objects and opens
perspectives summarized in the conclusion.

Convexity-Preserving Rigid Motions of 2D Digital Objects
71
2
Digital Objects and Rigid Motions on Z2
2.1
Objects and Rigid Motions on R2
Let us consider an object X in the Euclidean plane R2 as a closed connected
ﬁnite subset of R2. Rigid motions on R2 are deﬁned by a mapping:

T : R2 →R2
x
→Rx + t
(1)
where R is a rotation matrix and t ∈R2 is a translation vector. Such bijective
transformation T is isometric and orientation-preserving. Thus, T(X) has the
same shape (i.e., the same geometry and topology) as X.
2.2
Digitization of Objects and Topology Preservation
As the digitization process may cause topological alterations, conditions for guar-
anteeing the topology of shape boundaries have been studied [16,20].
Deﬁnition 1. An object X ⊂R2 is r-regular if for each boundary point of X,
there exist two tangent open balls of radius r, lying entirely in X and its comple-
ment X, respectively.
This notion, based on classical concepts of diﬀerential geometry, establishes a
topological link between a continuous shape and its digital counterpart as follows.
Proposition 1 ([20]). An r-regular object X ⊂R2 has the same topological
structure as its digitized version X ∩Z2 if r ≥
√
2
2 .
Note that the digitization deﬁned by the intersection of a continuous object
X and Z2 is called Gauss digitization [15]. It was shown that the digitization
process of an r-regular object yields a well-composed shape [16], whose deﬁnition
relies on the following concepts of digital topology (see e.g., [15]). Given a point
p ∈Z2, the k-neighborhood of p is deﬁned by Nk(p) = {q ∈Z2 : ∥p −q∥ℓ≤1}
for k = 4 (resp. 8) where ℓ= 1 (resp. ∞). We say that a point q is k-adjacent
to p if q ∈Nk(p) \ {p}. From the reﬂexive-transitive closure of this k-adjacency
relation on a ﬁnite subset S ⊂Z2, we derive the k-connectivity relation on S.
The k-connectivity relation on S is an equivalence relation. If there is exactly one
equivalence class for this relation, namely S, then we say that S is k-connected.
Deﬁnition 2 ([16]). A ﬁnite subset S ⊂Z2 is well-composed if each 8-connected
component of S and of its complement S is also 4-connected.
This deﬁnition implies that the boundary1 of S is a set of 1-manifolds when-
ever S is well-composed. As stated above, there exists a strong link between
r-regularity and well-composedness.
1 Here, the boundary of S is associated to the continuous boundary induced by the
pixels of S, i.e., the union of pixel edges shared by pixels of S and S.

72
P. Ngo et al.
Proposition 2 ([16]). If an object X ⊂R2 is r-regular with r ≥
√
2
2 , then
X ∩Z2 is well-composed.
We deﬁne a digital object X as a connected ﬁnite subset of Z2. In the context
of topological coherence of shape boundary, we hereafter assume that X is well-
composed, and thus 4-connected.
2.3
Problem of Point-by-Point Rigid Motions on Z2
If we simply apply a rigid motion T of Eq. (1) to every point in Z2, we gener-
ally have T(Z2) ̸⊂Z2. In order to get the points back on Z2, we then need a
digitization operator D : R2 →Z2, commonly deﬁned as a standard rounding
function. A discrete analogue of T is then obtained by Tpoint = D ◦T|Z2, so that
the discrete analogue of the point-by-point rigid motion of a digital object X on
Z2 is given by Tpoint(X).
Figure 3 illustrates some examples of Tpoint of digital lines with diﬀerent
thicknesses. These examples show that the topology and geometry of digital
objects are not always preserved, even when the initial shapes are very simple.
If a digital line is suﬃciently thick, then it preserves topology, but not always
geometry. This led us to consider a digital counterpart of regularity condition for
guaranteeing topology during point-by-point rigid motions on Z2 [19]. However,
ﬁnding rigid motions on Z2 preserving geometry is still an open problem. In this
article, we focus on one important geometrical property, namely convexity. We
present a new method for rigid motions on Z2 that preserves convexity.
(a)
(b)
(c)
(d)
Fig. 3. Well-composed digital lines (a, c) with diﬀerent thicknesses, which remain well-
composed (d) or not (b) after a point-by-point digitized rigid motion Tpoint. In both
cases, the convexity of the digital lines is lost by Tpoint.
3
Digital Convexity
In R2, an object X is said to be convex if and only if for any pair of points
x, y ∈X, every point on the straight line segment joining x and y, deﬁned by
[x, y] = {λx + (1 −λ)y | 0 ≤λ ≤1}, is also within X. This continuous notion,
however, cannot be directly applied to digital objects X in Z2 since for p, q ∈X,
we generally have [p, q] ̸⊂Z2. In order to tackle this issue, several extensions of
the notion of convexity from R2 to Z2 have been proposed.

Convexity-Preserving Rigid Motions of 2D Digital Objects
73
3.1
Deﬁnitions
First of all, we introduce the deﬁnition of Minsky and Papert [18], called MP-
convexity, which is a straightforward extension of the continuous notion.
Deﬁnition 3. A digital object X is MP-convex if for any pair of points p, q ∈X,
∀r ∈[p, q] ∩Z2, r ∈X.
Sklansky proposed a diﬀerent deﬁnition based on digitization process [22], called
S-convexity.
Deﬁnition 4. A digital object X is S-convex if there exists a convex object X ⊂
R2 such that X = X ∩Z2.
Later, Kim gave a geometrical deﬁnition that is based on convex hull [12], named
H-convexity. The convex hull of X is deﬁned by
Conv(X) =

x ∈R2
 x =
|X|

i=1
λipi ∧
|X|

i=1
λi = 1 ∧λi ≥0 ∧pi ∈X

.
Deﬁnition 5. A digital object X (connected, by deﬁnition) is H-convex if X =
Conv(X) ∩Z2.
Kim also showed the equivalence between MP-convexity and H-convexity for
4-connected digital objects [12, Theorem 5]. Similar results under the assumption
of 8-connectivity can be found in [10] via the chord property, which relates
the MP- and H-convexities to another digital convexity notion based on digital
lines, called D-convexity [13]. On the other hand, discussing the relation between
S-convexity and H-convexity requires some conditions. An element p of a digital
object X is said to be isolated if |N(p) ∩X| ≤1. Under the condition that X has
no isolated element, it was proven that X is H-convex if and only if X is S-convex
[12, Theorem 4]. A detailed description of various notions of digital convexity
can be found in [5, Chap. 9]. In this article, we adopt the H-convexity notion,
as it allows us to propose a method for rigid motions of digital objects which
preserves H-convexity thanks to the half-plane representation (see Sect. 4).
It should be also mentioned that a similar property to the intersection prop-
erty known from ordinary convex sets, is preserved; a proof based on an approach
of the chord property can be found in [5, Corollary 3.5.1].
Property 1. Let X and Y be two digital objects. If X and Y are H-convex and
X ∩Y is connected, then X ∩Y is H-convex.
Let us remark that convexity does not imply connectivity in Z2 by contrast
to R2. This is a reason why an additional connectivity condition is reasonable
in the discrete setting.

74
P. Ngo et al.
3.2
Digital Half-Plane Representation
If a digital object X consists of more than two elements, then the convex hull
Conv(X)2 is a convex polygon whose vertices are some points of X. As these
vertices are grid points, Conv(X) is thus represented by the union of closed half-
planes with integer coeﬃcients such that
Conv(X) =

H∈R(Conv(X))
H,
where R(P) is the minimal set of closed half-planes that constitute a convex
polygon P and each H is a closed half-plane in the following form:
H = {(x, y) ∈R2 | ax + by + c ≤0, a, b, c ∈Z, gcd(a, b) = 1}.
(2)
Note that the integer coeﬃcients of H are uniquely obtained by the pairs of the
corresponding consecutive vertices of Conv(X), denoted by u, v ∈Z2, which are
in the clockwise order, such that (a, b) =
1
gcd(wx,wy)(−wy, wx), and c = (a, b) · u
where (wx, wy) = v −u ∈Z2.
Therefore, from Deﬁnition 5, if X is H-convex, then we have
X =


H∈R(Conv(X))
H

∩Z2 =

H∈R(Conv(X))

H ∩Z2

(3)
where each H∩Z2 is called a digital half-plane. It is obvious that any digital half-
plane is H-convex. An example of an H-convex digital object with its half-plane
representation is illustrated in Fig. 4.
Fig. 4. Example of the digital half-plane representation of a digital object. Left: Digital
object and its convex hull (convex hull vertices are in red). Right: Digital half-planes
extracted from this convex hull. (Color ﬁgure online)
3.3
Veriﬁcation and Recognition Algorithms
To determine if a digital object is H-convex, several discrete methods were pro-
posed with diﬀerent approaches. In particular, they are based on the observation
of the 8-connected curve corresponding to the inner contour of digital object.
2 In this paper, we consider only X such that the area of Conv(X) is not null.

Convexity-Preserving Rigid Motions of 2D Digital Objects
75
In [6], a linear algorithm determines if a given polyomino is convex and, in
this case, it returns its convex-hull. It relies on the incremental digital straight
line recognition algorithm [7], and uses the geometrical properties of leaning
points of maximal discrete straight line segments on the contour. The algorithm
scans the contour curve and decomposes it into discrete segments whose extrem-
ities must be leaning points. The tangential cover of the curve [11] can be used
to obtain this decomposition. Then, the half-planes corresponding to Eq. (3) are
directly deduced from the characteristics of discrete segments obtained in the
curve decomposition. Similar approaches [8,21] were used to decompose a dis-
crete shape contour into a faithful polygonal representation respecting convex
and concave parts.
On the other hand, a combinatorial approach presented in [4] uses tools of
combinatorics on words to study contour words: the linear Lyndon factorization
algorithm [9] and the Christoﬀel words. A linear time algorithm veriﬁes convexity
of polyominoes and can also compute the convex hull of a digital object. It is
presented as a discrete version of the classical Melkman algorithm [17]. This
latter can also be used to compute the convex hull of a digital object, so that
the half-planes of R(Conv(X)) of a digital object X are then deduced from the
consecutive vertices (see Eq. (2)). Note that R(P) is the minimal set of half-
planes whose support lines are the edges of a convex polygon P. Then, the
H-convexity is tested with elementary operations (see Eq. (3)).
4
Convexity-Preserving Rigid Motions of Digital
Objects
In order to preserve the H-convexity of a given H-convex digital object X, we
will not apply rigid motions to each grid point of X, as discussed in Sect. 2.3,
but to each half-plane H of R(Conv(X)) according to Eq. (3). We ﬁrst explain
how to perform a rigid motion of a closed integer half-plane. Then, we propose
a method for rigid motions of the whole H-convex digital object.
4.1
Rational Rigid Motions of a Digital Half-Plane
A notion of rigid motion of a digital half-plane H ∩Z2 is given by:
TConv(H ∩Z2) := T(H) ∩Z2
(4)
where T(H) is deﬁned analytically as follows.
Digital objects and digital half-planes involve exact computations with inte-
gers. Thus, we assume hereafter that all the parameters R and t of rigid motions
T are rational. More precisely, we consider R =
1
r

p −q
q p

where p, q, r ∈Z
constitute a Pythagorean triple, i.e., p2 + q2 = r2, r ̸= 0, and t = (tx, ty) ∈Q2.
This assumption is reasonable, as we can always ﬁnd rational parameter values
suﬃciently close to any real values (see [2] for ﬁnding such a Pythagorean triple).

76
P. Ngo et al.
An integer half-plane H deﬁned by Eq. (2) is transformed by such T to the
rational half-plane:
T(H) = {(x, y) ∈R2 | αx + βy + γ ≤0, α, β, γ ∈Q},
(5)
whose coeﬃcients α, β, γ are given by

α
β

= R

a
b

and γ = c + αtx + βty.
Note that any rational half-plane can be easily rewritten as an integer half-
plane in the form of Eq. (2).
4.2
Rigid Motions of H-convex Digital Objects
In the previous subsection, we showed how to transform a digital half-plane with
Eq. (4) via Eq. (5). Since an H-convex digital object X is represented by a ﬁnite
set of digital half-planes H, as shown in Eq. (3), we can deﬁne a rigid motion of
X on Z2 using its associated digital half-planes such that
TConv(X) := T


H∈R(Conv(X))
H

∩Z2 =


H∈R(Conv(X))
T(H)

∩Z2.
(6)
For each H ∈R(Conv(X)), we obtain T(H) by Eq. (5), and then re-digitize
the transformed convex polygon P =
	
H∈R(Conv(X))
T(H).
We now show that TConv(X) is H-convex under a condition on the transformed
convex polygon P, called quasi-r-regularity. Thanks to Property 1, what we need
here is to characterize convex polygons P whose re-discritization P ∩Z2 preserves
the 4-connectivity since any digital half-plane is H-convex.
Property 2. Let P be a (closed) convex polygon in R2. If P includes a (closed)
ball of diameter
√
2, then P ∩Z2 is not empty.
This property is trivial due to the fact that we work on the grid Z2 of size 1.
From a morphological point of view, it can be reformulated as follows.
Property 3. If the erosion of P by a ball B √
2
2
of radius
√
2
2
is not empty,
namely, P ⊖B √
2
2 ̸= ∅, then P ∩Z2 is not empty.
Indeed, if P ⊖B √
2
2 ̸= ∅, then the opening (P ⊖B √
2
2 ) ⊕B √
2
2 is the (
√
2
2 )-regular
part of P, noted Reg(P). Its digitization with grid interval 1 is guaranteed to
have the same topological structure of P (see Proposition 1). In this context,
the regular part of P is deﬁned by
Reg(P) = (P ⊖B √
2
2 ) ⊕B √
2
2 ,
and P \ Reg(P) is called non-regular part. If, for each boundary half-plane of
H ∈R(P), there exists a ball B √
2
2 ⊂P being tangent to H, then we can deﬁne

Convexity-Preserving Rigid Motions of 2D Digital Objects
77
(a)
(b)
(c)
Fig. 5. Convex polygons P, which are quasi-(
√
2
2 )-regular (a) and not (b-c): (b) there
is a vertex of P with angle < π
2 ; (c) there is an half-plane H ∈R(P) where no ball
B √
2
2
⊂P exists such that H is tangent to B √
2
2 . In the ﬁgures, P are in blue, B √
2
2
are in red, P ⊖B √
2
2
is in yellow, Reg(P) are in pink, and Cor(P) are in green. (Color
ﬁgure online)
the corner parts of P as P \ Reg(P). More precisely, each connected component
of P \ Reg(P) corresponds to a corner part of P. In other words, the set of the
corner parts is deﬁned by Cor(P) = C(P \ Reg(P)) where C is the (continuous)
connected component function (see Fig. 5).
Property 4 Let us consider a corner part A ∈Cor(P). If the angle θ(A) at the
corresponding vertex of P veriﬁes θ(A) ≥π
2 and A ∩Z2 is not empty, then any
point p ∈A ∩Z2 has at least one 4-adjacent point in Reg(P).
Proof. As the Euclidean distance from p to Reg(P) cannot be higher than
√
2
2 ,
thus inferior to 1, the circle of radius 1 whose center is p always has a non-empty
intersection with Reg(P). Due to the angle condition of θ(A), this intersection
forms a circular arc whose central angle is not less than π
2 . The 4-neighbours of
p are on the circle of radius 1 and at least one of them appears on this circular-
arc intersection with Reg(P). Therefore, there always exists at least one of the
4-neighbours of p in Reg(P).
Let us now introduce the notion of quasi-regularity.
Deﬁnition 6. A convex polygon P is quasi-r-regular if ∀H ∈R(P), ∃Br ⊂P
such that H is tangent to Br, and if ∀A ∈Cor(P), θ(A) ≥π
2 .
From Property 4, we then obtain the following lemma.
Lemma 1. Let X be an H-convex digital object. If Conv(X) is quasi-(
√
2
2 )-
regular, then TConv(X) is 4-connected.
Finally, we obtain the main proposition from this lemma and Property 1.
Proposition 3. Let X be an H-convex digital object. If Conv(X) is quasi-(
√
2
2 )-
regular, then TConv(X) is H-convex.

78
P. Ngo et al.
Moreover, the transformed convex polygon P =
	
H∈R(Conv(X))
T(H) generally
does not correspond to the convex hull P ′ of TConv(X), i.e., P ′ = Conv(P ∩Z2).
However, we always have the following inclusion relation.
Property 5. Let X be an H-convex digital object. Let us consider the two convex
polygons such that P =
	
H∈R(Conv(X))
T(H) and P ′ = Conv(TConv(X)). Then, we
always have P ′ ⊆P.
5
Rigid Motions of a Non-convex Digital Object
In the previous section, we showed how to carry out rigid motions of H-convex
digital objects with preservation of their H-convexity under the quasi-(
√
2
2 )-
regular condition. In this section, we ﬁrst present a hierarchical representation
of a non-convex digital objects based on convex hulls, called a concavity tree
[23]. Then, we show how to perform rigid motions of a non-convex digital object
via this concavity-tree representation.
5.1
Concavity Tree of a Digital Object
A concavity tree, initially introduced in [23], is a hierarchical representation of
the convex and concave parts of the contour of a digital object, with diﬀerent
levels of detail. This tree structure has been used, e.g., for minimum-perimeter
polygon computation [14,23], hierarchical shape analysis [3], polygonal approx-
imation [1]. The root of the tree corresponds to a given digital object. Then,
each node corresponds to a concavity part of its parent. Note that each con-
cavity part X′ of a digital object X is obtained as a connected component of
the subtraction of X from the digitized convex hull Conv(X) ∩Z2. This is writ-
ten by X′ ∈C((Conv(X) ∩Z2) \ X) where C(S) denotes the set of all connected
components of a ﬁnite set S ∈Z2. In other words, we have
X =

Conv(X)

Z2

\


X′∈C((Conv(X)∩Z2)\X)
X′

,
(7)
where each concavity part X′ is recursively replaced by the subtraction of the
concavity parts of X′ from Conv(X′) 	 Z2 until no concavity part is found. An
illustration of concavity tree is given in Fig. 6.
5.2
Digital Object Rigid Motions Using Concavity Tree
As observed in Eq. (7), a digital object X can be decomposed into the set of
digitized convex hulls of hierarchical concavity parts, Conv(X) ∩Z2, Conv(X′) ∩
Z2, . . ., via the subtraction operations. As we are rather interested in such a
hierarchical decomposition by digitized convex polygons, we consider a concavity
tree such that each node corresponds to the half-plane representation of the

Convexity-Preserving Rigid Motions of 2D Digital Objects
79
(a)
(b)
(c)
Fig. 6. (a) A digital object. (b) The convex and concave parts of (a) with their convex
hulls. (c) The concavity tree of (a) where each colored node corresponds to a concavity
part of its parent and the root (in orange) corresponds to (a). (Color ﬁgure online)
convex hull of a concavity part Y, Conv(Y) =
	
H∈R(Conv(Y))
H, instead of Y itself.
Once the hierarchical object decomposition is obtained, we simply apply TConv
to each digitized convex polygon Conv(Y)∩Z2, as shown in the previous section,
and then carry out the set subtraction operations guided by the tree.
Figure 7 illustrates comparisons between applications of point-by-point trans-
formation Tpoint and convexity-preserving transformation TConv using concavity
tree to non-convex digital objects.
From Proposition 3, we can preserve the H-convexity of each digitized con-
vex polygon in the tree if the convex polygon is quasi-(
√
2
2 )-regular. Practically,
such hypothesis is not satisﬁed in general (see Fig. 6(b)). However up-sampling
strategies on the digital objects can allow us to generate convex polygons which
guarantee the quasi-(
√
2
2 )-regularity. This subject is one of our perspectives.
Fig. 7. Original digital objects (a, d) with their point-by-point digital rigid motions
(b, e) and their convex-preserving digital rigid motions using concavity tree (d, f).

80
P. Ngo et al.
6
Conclusion
In this article, we proposed a method for rigid motions of digital objects which
preserves the H-convexity. The method is based on the half-plane representation
of H-convex digital objects. In order to guarantee the H-convexity, we intro-
duced the notion of quasi-r-regularity for convex polygons, and showed that
the H-convexity is preserved under rigid motions if the convex hull of an initial
H-convex digital object is quasi-(
√
2
2 )-regular. The necessity of such condition
is caused by the fact that the convexity does not imply the connectivity in Z2
contrary to R2. As we need to re-discretize the transformed convex polygon at
the end, it is natural to have a similar notion of the r-regularity of continu-
ous objects, which guarantees the topology after its digitization. The method
was also extended to non-convex digital objects using the hierarchical object
representation.
The proposed method works only with digital objects satisfying the quasi-
(
√
2
2 )-regular condition on the convex hull. In practice, this condition is diﬃcult
to obtain, and this would limit the direct use of the proposed method. However,
an up-sampling approach would provide a promising strategy to guarantee the
quasi-(
√
2
2 )-regularity of the convex hull of any digital object. It is also observed
that the quasi-(
√
2
2 )-regularity is suﬃcient but not necessary for the H-convexity
preservation. It would be one of our perspectives to ﬁnd such a suﬃcient and
necessary condition. Another perspective would be to extend the method into
higher dimensions.
References
1. Aguilera-Aguilera, E., Carmona-Poyato, A., Madrid-Cuevas, F., Medina-Carnicer,
R.: The computation of polygonal approximations for 2D contours based on a
concavity tree. J. Vis. Commun. Image Represent. 25(8), 1905–1917 (2014)
2. Anglin, W.S.: Using Pythagorean triangles to approximate angles. Am. Math.
Monthly 95(6), 540–541 (1988)
3. Borgefors, G., di Baja, G.S.: Analyzing nonconvex 2D and 3D patterns. Comput.
Vis. Image Underst. 63(1), 145–157 (1996)
4. Brlek, S., Lachaud, J., Proven¸cal, X., Reutenauer, C.: Lyndon + Christoﬀel =
digitally convex. Pattern Recogn. 42(10), 2239–2246 (2009)
5. Cristescu, G., Lupsa, L.: Non-Connected Convexities and Applications. Kluwer
Academic Publishers, Dordrecht (2002)
6. Debled-Rennesson, I., R´emy, J.L., Rouyer-Degli, J.: Detection of the discrete con-
vexity of polyominoes. Discrete Appl. Math. 125(1), 115–133 (2003)
7. Debled-Rennesson, I., Reveill`es, J.: A linear algorithm for segmentation of digital
curves. Int. J. Pattern Recognit Artif Intell. 9(4), 635–662 (1995)
8. Dorksen-Reiter, H., Debled-Rennesson, I.: Convex and concave parts of digital
curves. J. Geom. Prop. Incomplete Data 31, 145–159 (2004)
9. Duval, J.: Factorizing words over an ordered alphabet. J. Algorithms 4(4), 363–381
(1983)

Convexity-Preserving Rigid Motions of 2D Digital Objects
81
10. Eckhardt, U.: Digital lines and digital convexity. In: Bertrand, G., Imiya, A., Klette,
R. (eds.) Digital and Image Geometry. LNCS, vol. 2243, pp. 209–228. Springer,
Heidelberg (2001). doi:10.1007/3-540-45576-0 13
11. Feschet, F., Tougne, L.: Optimal time computation of the tangent of a discrete
curve: application to the curvature. In: Bertrand, G., Couprie, M., Perroton, L.
(eds.) DGCI 1999. LNCS, vol. 1568, pp. 31–40. Springer, Heidelberg (1999). doi:10.
1007/3-540-49126-0 3
12. Kim, C.E.: On the cellular convexity of complexes. IEEE Trans. Pattern Anal.
Mach. Intell. 3(6), 617–625 (1981)
13. Kim, C.E., Rosenfeld, A.: Digital straight lines and convexity of digital regions.
IEEE Trans. Pattern Anal. Mach. Intell. 4(2), 149–153 (1982)
14. Klette, G.: Recursive computation of minimum-length polygons. Comput. Vis.
Image Underst. 117(4), 386–392 (2013)
15. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Picture
Analysis. Elsevier, Amsterdam (2004)
16. Latecki, L.J., Conrad, C., Gross, A.: Preserving topology by a digitization process.
J. Math. Imaging Vis. 8(2), 131–159 (1998)
17. Melkman, A.A.: On-line construction of the convex hull of a simple polyline.
Inform. Process. Lett. 25(1), 11–12 (1987)
18. Minsky, M., Papert, S.: Perceptrons: An Introduction to Computational Geometry.
MIT Press, Reading (1969)
19. Ngo, P., Passat, N., Kenmochi, Y., Talbot, H.: Topology-preserving rigid transfor-
mation of 2D digital images. IEEE Trans. Image Process. 23(2), 885–897 (2014)
20. Pavlidis, T.: Algorithms for Graphics and Image Processing. Berlin: Springer, and
Rockville: Computer Science Press (1982)
21. Roussillon, T., Sivignon, I.: Faithful polygonal representation of the convex and
concave parts of a digital curve. Pattern Recogn. 44(10–11), 2693–2700 (2011)
22. Sklansky, J.: Recognition of convex blobs. Pattern Recogn. 2, 3–10 (1970)
23. Sklansky, J.: Measuring concavity on a rectangular mosaic. IEEE Trans. Comput.
C-21(12), 1355–1364 (1972)

Weighted Distances on the Trihexagonal Grid
Gergely Kov´acs1(B), Benedek Nagy2, and B´ela Vizv´ari3
1 Edutus College, Tatab´anya, Hungary
kovacs.gergely@edutus.hu
2 Department of Mathematics, Faculty of Arts and Sciences,
Eastern Mediterranean University, Mersin-10, Famagusta, North Cyprus, Turkey
nbenedek.inf@gmail.com
3 Department of Industrial Engineering, Eastern Mediterranean University,
Mersin-10, Famagusta, North Cyprus, Turkey
Abstract. Recently chamfer distances have been developed not only on
the usual integer grids, but also on some non traditional grids including
grids which are not lattices. In this paper the trihexagonal grid is con-
sidered which is a kind of mix of the hexagonal and triangular grids: its
pixels are hexagons and two shaped (oriented) triangles. Three types of
‘natural’ neighborhood relations are considered on the grid, consequently
three weights are used to describe the chamfer distances. Formulae to
compute the minimal weights of a connecting path, i.e., the distance
of any two pixels, are provided to various cases depending on the rela-
tive ratio of the weights. Some properties of these distances, including
metricity are also analysed.
1
Introduction
Digital geometry is an important theoretical part of digital image processing.
Discrete, digital spaces have diﬀerent properties than the Euclidean space, e.g.,
neighborhood of points play important role. Consequently, digital (path based)
distance functions have various advantages, see, e.g., [4]. Various digital distance
functions are developed since the 1960’s, where the two basic digital distances
based on the two usual neighborhood on the square grid were investigated [16].
Various grids have various properties and various advantages and disadvantages
in applications. E.g., the square grid is easy to use, it has hardware and soft-
ware support, other grids have more symmetries, may provide better topological
properties. Digital geometry and distance functions are developed for various
traditional and non traditional grids, both in 2D and in higher dimensions. To
move some of the results from a point lattice to other point lattice may not be
trivial. In case of at least one of the grids is not a point lattice, we need to ﬁnd
newer and newer approaches, the translation of the results cannot go automati-
cally. Even some results seem to be similar, the details could be very diﬀerent.
A nice symmetric coordinate frame for the hexagonal grid was presented in [3],
while the simplest digital distances are investigated in [7] for that grid. The tri-
angular grid was also described by symmetric coordinate system with integer
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 82–93, 2017.
DOI: 10.1007/978-3-319-66272-5 8

Weighted Distances on the Trihexagonal Grid
83
triplets, see, e.g., [9,11]. The hexagonal and triangular grids can be seen as one
and two parallel oblique planes of the cubic grid [10], and with three such planes
another ‘triangular grid’ can be obtained [12]. This grid is called trihexagonal
grid in this paper since it is mixing the properties of the hexagonal and triangular
grids (see Fig. 1). Each node has the same rank and each node is surrounded by
the same set of regular polygons in the same order. By its symmetric properties
this grid is denoted by T(6, 3, 6, 3) in [15], where 4 is the nodal rank and 6 or
3 is the number of edges of the i-th polygon.
Chamfer (or weighted) distances are providing a relatively good approxima-
tion of the Euclidean distance with good algorithmic properties [1]. The concept
was also investigated on some non-traditional 2D grids including the triangular
grid [13], Khalimsky grid [5,6]; and various 3D grids [17,18]. In this paper cham-
fer distances on the 2D trihexagonal grid are investigated; our main motivation
is to show the basic properties.
2
Description of Trihexagonal Tiling (6, 3, 6, 3)
Figure 1 shows a usual representation of the trihexagonal tiling. The grid is
T(6, 3, 6, 3) if dual notation is used, see, e.g., [15]. In this representation hexagons
represent the points for which 6-neighborhood is used and triangles the points
for which 3-neighborhood is used.
Each pixel of the trihexagonal grid (6, 3, 6, 3) is a hexagon or a triangle, we
also call it as a point of the grid. A hexagon has 6 neighbors with common sides
(6 triangles). A triangle has 3 neighbors with common sides (3 hexagons).
Similarly to the hexagonal grid or to the triangular grid, each pixel of the
trihexagonal grid (6, 3, 6, 3) can also be described by a unique coordinate-triplet.
There is a triangle having coordinate triplet (0, 0, 0), and the directions of
the axes can be seen on the Fig. 1. At every time when we step from a triangle
to a hexagon (or from a hexagon to a triangle) crossing their common side, the
step is done parallel to one of the axes. If the step is in the direction of an axis,
then the respective coordinate value is increased by 1, while in case the step is in
opposite direction to an axis, the respective coordinate value is decreased by 1.
In this way every point gets a unique coordinate triplet with integer values.
However the three values are not independent (we are in a two dimensional space,
i.e., plane). Their sum is either 0, 1 or 2. There are two orientations of the used
triangles: there are triangles of shape △and there are triangles of shape ▽.
The sum of the coordinate values that address a triangle is 0 or 2 depending
the orientation (shape) of the triangle. The sum of the coordinate values of a
hexagon is 1.
There are two types of (commonly used) neighborhood on this grid. Two pix-
els are neighbors if they share a side. Two pixels are semi-neighbors if they share
at least a point on their boundaries (e.g., a corner point). Using the coordinate
triplets one can give the neighborhood relations in the following formal form.
The hexagon p(p(1), p(2), p(3)) and the triangle q(q(1), q(2), q(3)) are neighbors
if |p(1) −q(1)| + |p(2) −q(2)| + |p(3) −q(3)| = 1. (See also Fig. 1.)

84
G. Kov´acs et al.
0,-1,3
1,-1,2
2,-1,1
3,-1,0
4,-1,-1
1,2                   1,-1,1                 
-1,-1,2
0,-1,1
1,-1,0
2,-1,-1
3,-1,-2
1,0,2                    0,0,1                    1,0,0                    2,0,-1                  3,0,-2 
-1,1,2
1,1,0
2,1,-1
3,1,-2
                   0,1,0                     1,1,-1                  2,1,-2 
-1,1,0
0,1,-1
1,1,-2
2,1,-3
0,-
-
- 2,2,1                 -1,2,0                    0,2,-1                   1,2,-2                  2,2,-3 
y
-1,0,1
0,0,0
1,0,-1
2,0,-2
-1,2,1
0,2,0
2,2,-2
0,0,2
1,0,1
2,0,0
3,0,-1
-2,2,0
-1,2,-1
1,2,-3
z
x
0,1,1
-2,1,1
-1,1,1
1,2,-1
0,2,-2
2,-1,0                   3,-1,-1 
Fig. 1. Coordinate axes and coordinate values assigned to cells of a segment of the
trihexagonal grid. The yellow hexagon and the orange triangles are neighbors (α), the
pink hexagons are semi-neighbors (β), the blue triangles are semi-neighbors (γ). (Color
ﬁgure online)
There are no triangles with common side, but every triangle p has 3 semi-
neighbor triangle, for example q, for which |p(1) −q(1)| + |p(2) −q(2)| + |p(3) −
q(3)| = 2.
There are no hexagons with common side, but every hexagon p has 6 semi-
neighbor hexagon, for example q, for which |p(1) −q(1)| + |p(2) −q(2)| + |p(3) −
q(3)| = 2.
3
Deﬁnition of Weighted Distances
Let α, β, γ ∈R+ be positive real weights. The simplest weighted distances allow
to step to a neighbor (from a triangle to a hexagon and vice versa) by changing
only one coordinate value by ±1 with weight α. Let the weight of a step from
a hexagon to a semi-neighbor hexagon be β. This step changes two coordinates,
one by +1, and another one by −1. Let the weight of a step from a triangle to a
semi-neighbor triangle be γ. This step changes two coordinates, both by +1, or
both by −1. (See also Fig. 1.)
We can deﬁne the weighted distance of any two points (hexagons or triangles)
of the grid. Let p and q be two points of the grid. A ﬁnite point sequence of points
of the form p = p0, p1, . . . , pm = q, where pi−1, pi are neighbors or semi-neighbors
for 1 ≤i ≤m, is called a path from p to q. A path can be seen as consecutive
steps to neighbors or semi-neighbors. Then the cost of the path is the sum of
the weights of its steps.

Weighted Distances on the Trihexagonal Grid
85
Finally, let the weighted distance d(p, q; α, β, γ) of p and q by the weights
α, β, γ be the cost of the minimal (basic) weighted paths between p and q.
Usually, there can be several shortest paths from a point to another. The
order of steps can be varied, e.g., if a shortest path from (0, 0, 1) to (2, −1, 0)
contains two β steps containing the point (1, 0, 0), then the path having two β
steps through (1, −1, 1) is also a shortest path. However, since we are not on
a lattice, we are not free to use any order of the steps. One need to take care
about the following conditions:
– α steps are allowed on any point, and a point of the opposite type is reached
by the step (where hexagon and triangle are the types).
– β step can only be used from a hexagon (the sum of coordinates is equal to
1), and it goes to a hexagon.
– γ steps are valid only from a triangle (the sum of the coordinates is equal to
0, or 2), and another triangle is reached (but the sum of the coordinates of
the new triangle is diﬀerent from the original one: the triangles have opposite
orientation).
A technical deﬁnition is used through the paper. The diﬀerence wp,q =
(w(1), w(2), w(3)) of two points p and q is deﬁned by w(i) = q(i) −p(i), where
i ∈{1, 2, 3}.
4
Minimal Weighted Paths
There are various paths with various sums of weights that can be found between
any two points. When the weights α, β and γ are known the optimal search (the
Dijkstra algorithm) can be used. However depending on the actual ratios and
values of the weights α, β, γ, one can compute a minimum weighted paths in
a more direct way. Using a combinatorial approach, we give methods for these
computations for each possible case.
We use the natural condition 0 ≤α ≤β, γ for the used weight values. We
know that with a neighbor step (by weight α) only 1 of the coordinates changes
by ±1; with a semi-neighbor step (by weight β or γ) exactly 2 of the coordinates
change by 1 and/or −1, respectively. Therefore it is important to measure the rel-
ative weight of a step, that is the cost of the change of a coordinate value by ±1.
These relative weights give the separation of the possible cases.
4.1
Case 2α ≤β and 2α ≤γ
Lemma 1. If 2α ≤β and 2α ≤γ, then the length of the minimal path between
the points p and q is
d(p, q; α, β, γ) = α(|w(1)| + |w(2)| + |w(3)|).
(1)
Proof. In this case the use of β steps and γ steps is not eﬃcient (their usage
do not lead to shorter paths than we have without them). A minimal path can
be constructed only by α steps. In every step the absolute value of a coordinate
diﬀerence is decreasing by 1 implying the formula.
⊓⊔

86
G. Kov´acs et al.
For example the distance of the triangle (0, 0, 0) and the hexagon (1, −1, 1)
is 3α.
4.2
Case γ ≤2α ≤β
Lemma 2. If α ≤γ ≤2α ≤β, then the length of the minimal path between the
points p and q is
d(p, q; α, β, γ) = γ · min{|w(i)|} + α ·
 3

i=1
|w(i)| −2 · min{|w(i)|}

,
(2)
but if wp,q contains one 0 and two 1 values, or one 0 and two −1 values, then
d(p, q; α, β, γ) = γ.
(3)
Proof. The value of the sum of coordinate diﬀerences
3
i=1
w(i) is equal to 0, ±1
or ±2. (Moreover it is ±2 only if the two points are diﬀerent shaped triangles.)
Clearly, in this case β steps cannot appear (when 2α ≤β) in any shortest
path or substituting them by two α steps a path with same weight is obtained,
we deal with paths containing only α and γ steps. It follows from Lemma 1 there
is always a path between p and q consisting of α steps only and their number is
|w(1)| + |w(2)| + |w(3)|. Obviously, a path with this property has the minimal
length among all paths using only α steps as each such step changes the absolute
values of diﬀerences of the coordinates by 1. If there are two coordinates where
the diﬀerence has the same sign, then it is possible to make a γ step at the
beginning.
If min{|w(i)|} > 0, then the signs of two diﬀerences (from three) are the same,
and one of them has the smallest absolute value, because the sum of diﬀerences
is between −2 and 2. It means that we are able to use γ steps min{|w(i)|} times
instead of twice many α steps, but no more. The path can be constructed that
between two γ steps come two α steps (for example between two (1, 1, 0) steps
come two (0, 0, −1) steps).
If min{|w(i)|} = 0 and the sign of the other two diﬀerences are diﬀerent,
then the use of a γ step does not decrease the sum of the absolute values of the
coordinate diﬀerences, i.e. the use of γ step is not eﬃcient (there is a shortest
path without it).
If min{|w(i)|} = 0, then the signs of the other two diﬀerences are the same
only if both values are equal to 1 or both values are equal to −1, for example
(1, 1, 0). In this case the distance is γ.
⊓⊔
For example the distance of the triangle (0, 0, 0) and the hexagon (1, −1, 1) is
γ + α, the distance of the triangles (0, 0, 0) and (2, 0, −2) is 4α, but the distance
of the triangles (0, 0, 0) and (1, 0, 1) is γ with the given conditions on the weights.

Weighted Distances on the Trihexagonal Grid
87
4.3
Case β ≤2α ≤γ
The use of γ steps instead of α steps is not eﬃcient, but one β step can be better
than two α steps.
Lemma 3. If β ≤2α ≤γ, then the length of the minimal path between the
hexagons p and q is
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)|
2
.
(4)
Proof. If p and q are hexagons, then the sum of the coordinate diﬀerences is 0,
and every β step is a 0-sum step. In this case a minimal path can be constructed
only by β steps. In every step the sum of the absolute values of the coordinate
diﬀerences is decreasing by 2, and this is better than the use of two α steps. ⊓⊔
For example the distance of the hexagons (0, 0, 1) and (2, −1, 0) is 2β.
If every point of a path is a hexagon, then it is called a hexagonal path in
the following. The next lemmas can be proven in similar manner as Lemma 3.
Lemma 4. If β ≤2α ≤γ, and p and q are a triangle and a hexagon, then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −1
2
+ α.
(5)
Lemma 5. If β ≤2α ≤γ, and p and q are triangles, then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −2
2
+ 2α.
(6)
For example the distance of the triangle (0, 0, 0) and the hexagon (1, −1, 1)
is β + α, and the distance of the triangles (0, 0, 0) and (2, 0, −2) is β + 2α.
4.4
Case β ≤γ ≤2α
Lemma 6. If β ≤γ ≤2α, then the length of the minimal path between p
and q is given in (4), (5) and (6), but if p and q are adjacent triangles, then
d(p, q; α, β, γ) = γ.
Proof. In every β step and γ step the sum of the absolute values of the coordinate
diﬀerences is decreasing by 2. But β ≤γ means that the use of γ steps is not
eﬃcient if one of the points is a hexagon. Then this case is the same as the
previous case.
But if p and q are triangles and in the minimal path of the above case the
α steps are consecutive steps, then it is feasible and eﬃcient to change them
with a γ step. The α steps are consecutive only if there are no β steps, and the
triangles are semi-neighbors.
⊓⊔
For example the distance of the triangles (0, 0, 0) and (2, 0, −2) is β + 2α,
but the distance of the triangles (0, 0, 0) and (1, 0, 1) is γ.

88
G. Kov´acs et al.
4.5
Case γ ≤β ≤2α
Subcase p and q are Hexagons
Lemma 7. If α ≤γ ≤β ≤2α and p, q are hexagons, then the minimal path
contains at least one γ step if and only if wp,q has two coordinates with the same
sign and γ + 2α ≤2β.
Proof. Of course we cannot use a γ step instead of a β step, or two γ steps
instead of two β steps. But we can use a γ step and two α steps instead of two
appropriate β steps.
If p and q are hexagons, then the sum of coordinate diﬀerences is equal to 0.
In this case min{|w(i)} = 0 if and only if the sign of the other two diﬀerences
are diﬀerent. Then the use of a γ step does not decrease the sum of the absolute
values of the coordinate diﬀerences, thus the use of γ step is not eﬃcient. The
minimal path is the hexagonal path between the two hexagons, which uses only
β steps. min{|w(i)} > 0 if and only if wpq has two coordinates with the same
sign. The hexagonal path is minimal only if 2β < γ + 2α, when the use of γ step
is not eﬃcient. But if 2β ≥γ + 2α, then the minimal path uses γ step: we can
begin the minimal path with α, γ, α steps, and the sum of the absolute value of
the coordinate diﬀerences is decreased by 4.
⊓⊔
Lemma 8. If α ≤γ ≤β ≤2α and p, q are hexagons, then
d(p, q; α, β, γ) = β ·
3
i=1 |w(i)|
2
(7)
if 2β ≤γ + 2α, and
d(p, q; α, β, γ) = γ · min{|w(i)|} + 2α · min{|w(i)|} + β ·
3
i=1
|w(i)| −4 min{|w(i)|}
2
(8)
if γ + 2α ≤2β.
Proof. If p and q are hexagons, then the sum of the coordinate diﬀerences is 0.
If min{|w(i)|} = 0, then the signs of the other two diﬀerences are diﬀerent.
It means that the use of γ steps is not eﬃcient.
But if min{|w(i)|} > 0, then wp,q has two coordinates with the same sign, for
example positive coordinates: the ﬁrst and the second one. There exists a β-path
between the two hexagons, which ﬁrst 2 min{|w(i)|} steps are the alternate β
steps (1, 0, −1) and (0, 1, −1). Changing two appropriate β steps to one γ step
and two α-steps may be eﬃcient min{|w(i)|} times if γ + 2α ≤2β.
Of course 4 min{|w(i)|} ≤
3
i=1
|w(i)| holds, because of
3
i=1
w(i) = 0.
⊓⊔
For example the distance of the hexagons (0, 0, 1) and (2, −1, 0) is 2β or
2α + γ.

Weighted Distances on the Trihexagonal Grid
89
Subcase p and q are a Hexagon and a Triangle. If p and q are a hexagon
and a triangle, then there exists the above mentioned minimal path between
the hexagon and another hexagon, which is the closest hexagon neighbor of the
triangle, and there is a ﬁnal α step between the closest hexagon and the triangle.
If the ﬁnal step of the path between the two hexagons is a β step, then it can
be eﬃcient to change the ﬁnal β and α steps to an α step and a γ step. This
way we can increase the number of γ steps by 1. For example we can change the
steps (0, 1, −1) and (1, 0, 0) to (0, 0, −1) and (1, 1, 0). When are we able to do
this substitution?
In this case the sum of the coordinate diﬀerences is ±1.
Lemma 9. If α ≤γ ≤β ≤2α, p and q are a hexagon and a triangle, and wp,q
contains only one coordinate with the sign of
3
i=1
w(i), then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −1
2
+ α
(9)
if 2β ≤γ + 2α, and
d(p, q; α, β, γ) = γ · min{|w(i)|}
+ α · (2 min{|w(i)|} + 1) + β ·
3
i=1
|w(i)| −4 min{|w(i)|} −1
2
(10)
if γ + 2α ≤2β.
Proof. Let us assume, that
3
i=1
w(i) = 1 and p is a hexagon and q is a triangle.
(The proof of the case −1, or the case of a triangle and a hexagon are similar.)
In this case wp,q contains only one positive coordinate, for example the ﬁrst
one, then the ﬁnal α step of the above mentioned path (based on the closest
neighbor hexagon of the triangle) is (1, 0, 0). (For example if the starting hexagon
is (1, 0, 0) and the triangle is (7, −3, −2), then the closest neighbor hexagon of
the triangle is (6, −3, −2) and not (7, −4, −2) or (7, −3, −3), because the sum of
the absolute value of the coordinate diﬀerences are here 10, 12 and 12.)
The use of a γ step, which ﬁrst coordinate is +1 is not eﬃcient, because
this γ step does not decrease the sum of the absolute values of the coordinate
diﬀerences. Thus the use of the above mentioned substitution is not eﬃcient, i.e.
the number of γ steps is equal to the number of γ steps of the path based on
the closest neighbor hexagon (in the previous lemma).
If this path uses an α step, then it’s coordinates are (1, 0, 0). These are the
same as the coordinates of the ﬁnal α step. Thus we are not able to increase the
number of β steps.
⊓⊔

90
G. Kov´acs et al.
For example the distance of the triangle (0, 0, 0) and the hexagon (3, −1, −1)
is 2β + α or 3α + γ.
In a similar manner one can also prove the following lemma.
Lemma 10. If α ≤γ ≤β ≤2α, p and q are a hexagon and a triangle, and
wp,q contains two coordinates with the sign of
3
i=1
w(i), then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −3
2
+ α + γ
(11)
if 2β ≤γ + 2α, and
d(p, q; α, β, γ) = γ · min{|w(i)|}
+ α · (2 min{|w(i)|} −1) + β ·
3
i=1
|w(i)| −4 min{|w(i)|} + 1
2
(12)
if γ + 2α ≤2β.
For example the distance of the triangle (0, 0, 0) and the hexagon (2, 1, −2)
is β + α + γ in both cases.
Subcase p and q are Triangles. If p and q are triangles, then there exists
the above mentioned minimal path between the closest hexagon neighbors of the
triangles, and there are starting and ﬁnal α steps between the closest hexagons
and the triangles. This case is similar to the previous one, but sometimes we are
able to do the above mentioned changes (at the end of the path) two times (at
the beginning, too).
Lemma 11. If α ≤γ ≤β ≤2α, p and q are triangles,
3
i=1
w(i) ̸= 0, and wp,q
contains only one coordinate with the sign of
3
i=1
w(i), then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −2
2
+ 2α
(13)
if 2β ≤γ + 2α, and
d(p, q; α, β, γ) = γ · min{|w(i)|}
+ α · (2 min{|w(i)|} + 2) + β ·
3
i=1
|w(i)| −4 min{|w(i)|} −2
2
(14)
if γ + 2α ≤2β.

Weighted Distances on the Trihexagonal Grid
91
The proof of this lemma is similar to the proof of Lemma 9.
For example the distance of the triangles (0, 0, 0) and (3, −1, 0) is 2α + β in
both cases.
Lemma 12. If α ≤γ ≤β ≤2α, p and q are triangles,
3
i=1
w(i) ̸= 0 and wp,q
contains two coordinates with the sign of
3
i=1
w(i), then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −6
2
+ 2α + 2γ
(15)
if 2β ≤γ + 2α, and
d(p, q; α, β, γ) = γ · min{|w(i)|}
+ α · (2 min{|w(i)|} −2) + β ·
3
i=1
|w(i)| −4 min{|w(i)|} + 2
2
(16)
if γ + 2α ≤2β. But if moreover
3
i=1
|w(i)| = 2, then d(p, q; α, β, γ) = γ, and if
3
i=1
|w(i)| = 4, then d(p, q; α, β, γ) = γ + 2α.
The proof of this lemma is similar to the proof of Lemma 10.
For example the distance of the triangles (0, 0, 0) and (2, 2, −2) is 2α + 2γ in
both cases.
Lemma 13. If α ≤γ ≤β ≤2α, p and q are triangles, and
3
i=1
w(i) = 0, then
d(p, q; α, β, γ) = β ·
3
i=1
|w(i)| −2
2
+ 2α
(17)
if 2β ≤γ + 2α or min{|w(i)|} = 0 and
d(p, q; α, β, γ) = γ · min{|w(i)|}
+ α · min{|w(i)|} + β ·
3
i=1
|w(i)| −4 min{|w(i)|}
2
(18)
if γ + 2α ≤2β and min{|w(i)|} > 0.
The proof of this lemma is similar to the proof of Lemma 10.
For example the distance of the triangles (0, 0, 0) and (0, 2, −2) is 2α + β.

92
G. Kov´acs et al.
5
Properties of Distances
Minimal weighted paths can be obviously computed by Dijkstra algorithm [2].
However, based on the regular structure of the grid and by the help of an appro-
priate coordinate system direct formulae are provided to compute them. There
are various cases depending on the relation of the used weights; a summary of
the results is shown in Table 1.
Table 1. Value of d(p, q; α, β, γ) depending on the cases of respective relations of the
weights
Between
Cases
Two hexagons A hexagon and a triangle
Two triangles
4.1: 2α ≤β, γ
α  |w(i)|
4.2: γ ≤2α ≤β
γ min{|w(i)|} + α ( |w(i)| −2 min{|w(i)|}) Subcases
4.3, 4.4: β ≤γ, 2α β
 |w(i)|
2
β
 |w(i)|−1
2
+ α
Subcases
4.5: γ ≤β ≤2α
Subcases
Subcases
Subcases
For the sake of completeness, we recall the deﬁnition of metricity for digital
distances. A distance function d(·, ·) is a metric if the three properties, the posi-
tive deﬁniteness, the symmetry and the triangular inequality, are fulﬁlled for any
points p, q, r of a space, that is, in this paper the trihexagonal grid (6, 3, 6, 3).
There are some non-metrical digital distances, e.g., distances based on neigh-
borhood sequences [9], weight sequences [14]. However, in some applications it
is important to use metric distances, therefore, it is important to note that, as
usual at weighted distances, for any values of α, β, γ ∈R+, the distance function
d(·, ·; α, β, γ) is a metric.
One can also prove the following about the translation invariance of our
distance functions.
Theorem 1. Let t = (x, y, z) be a grid vector, i.e., the diﬀerence of the coordi-
nates of two points of the grid. Then, the distance d(p, q; α, β, γ) = d(p + t, q +
t; α, β, γ) for all pairs of points p, q of the grid and for any α, β, γ ∈R+ if and
only if t is an integer triplet with 0-sum.
We can conclude that we have done the ﬁrst steps to include the trihexagonal
grid in digital geometry, namely we have studied chamfer distances. Digital or
path-based distance functions are well known and widely used. However, the
properties of these functions depend highly on the underlying grid: while chamfer
polygons on the square grid are well known any relatively easy to describe, it is
not the case on the triangular grid (based on the three classical neighborhood,
e.g., 63-gons can also be obtained [8]). Future works and possible applications
include cases when, e.g., γ < α, distance transforms, and also studies about the
digital disks (chamfer polygons) and their interesting phenomena: conditions for
concavities, holes and islands (somewhat similarly to [6]).

Weighted Distances on the Trihexagonal Grid
93
References
1. Borgefors, G.: Distance transformations in digital images. Comput. Vis. Graph.
Image Process. 34, 344–371 (1986)
2. Dijkstra, E.W.: A note on two problems in connexion with graphs. Numerische
Mathematik 1, 269–271 (1959). doi:10.1007/BF01386390
3. Her, I.: A symmetrical coordinate frame on the hexagonal grid for computer graph-
ics and vision. ASME J. Mech. Design 115, 447–449 (1993)
4. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Picture
Analysis. Elsevier, Amsterdam (2004)
5. Kov´acs, G., Nagy, B., Vizv´ari, B.: On weighted distances on the Khalimsky grid.
In: Normand, N., Gu´edon, J., Autrusseau, F. (eds.) DGCI 2016. LNCS, vol. 9647,
pp. 372–384. Springer, Cham (2016). doi:10.1007/978-3-319-32360-2 29
6. Kov´acs, G., Nagy, B., Vizv´ari, B.: Weighted distances and digital disks on the
Khalimsky grid: disks with holes and islands. J. Math. Imaging Vis. 59, 2–22
(2017). doi:10.1007/s10851-016-0701-5
7. Luczak, E., Rosenfeld, A.: Distance on a hexagonal grid. Trans. Comput C–25(5),
532–533 (1976)
8. Mir-Mohammad-Sadeghi, H., Nagy, B.: On the chamfer polygons on the triangular
grid. In: Brimkov, V.E., Barneva, R.P. (eds.) IWCIA 2017. LNCS, vol. 10256, pp.
53–65. Springer, Cham (2017). doi:10.1007/978-3-319-59108-7 5
9. Nagy, B.: Metrics based on neighbourhood sequences in triangular grids. Pure
Math. Appl. - PU.M.A 13, 259–274 (2002)
10. Nagy, B.: A family of triangular grids in digital geometry. In: 3rd International
Symposium on Image and Signal Processing and Analysis (ISPA 2003), Rome,
Italy, pp. 101–106 (2003)
11. Nagy, B.: Characterization of digital circles in triangular grid. Pattern Recognit.
Lett. 25, 1231–1242 (2004)
12. Nagy, B.: Generalized triangular grids in digital geometry. Acta Mathematica
Academiae Paedagogicae Ny´ıregyh´aziensis 20, 63–78 (2004)
13. Nagy, B.: Weighted distances on a triangular grid. In: Barneva, R.P., Brimkov,
V.E., ˇSlapal, J. (eds.) IWCIA 2014. LNCS, vol. 8466, pp. 37–50. Springer, Cham
(2014). doi:10.1007/978-3-319-07148-0 5
14. Nagy, B., Strand, R., Normand, N.: A weight sequence distance function. In: Hen-
driks, C.L.L., Borgefors, G., Strand, R. (eds.) ISMM 2013. LNCS, vol. 7883, pp.
292–301. Springer, Heidelberg (2013). doi:10.1007/978-3-642-38294-9 25
15. Radv´anyi, A.G.: On the rectangular grid representation of general CNN networks.
Int. J. Circuit Theory Appl. 30, 181–193 (2002)
16. Rosenfeld, A., Pfaltz, J.L.: Distance functions on digital pictures. Pattern Recognit.
1, 33–61 (1968)
17. Strand, R., Nagy, B.: Weighted neighbourhood sequences in non-standard three-
dimensional grids – metricity and algorithms. In: Coeurjolly, D., Sivignon, I.,
Tougne, L., Dupont, F. (eds.) DGCI 2008. LNCS, vol. 4992, pp. 201–212. Springer,
Heidelberg (2008). doi:10.1007/978-3-540-79126-3 19
18. Strand, R., Nagy, B., Borgefors, G.: Digital distance functions on three-dimensional
grids. Theor. Comput. Sci. 412, 1350–1363 (2011)

An Integer Programming Approach
to Characterize Digital Disks on the Triangular
Grid
Gergely Kov´acs1(B), Benedek Nagy2, and B´ela Vizv´ari3
1 Edutus College, Tatab´anya, Hungary
kovacs.gergely@edutus.hu
2 Department of Mathematics, Faculty of Arts and Sciences,
Eastern Mediterranean University, Mersin-10, Famagusta, North Cyprus, Turkey
nbenedek.inf@gmail.com
3 Department of Industrial Engineering, Eastern Mediterranean University,
Mersin-10, Famagusta, North Cyprus, Turkey
Abstract. Generally, the integer hull of a polyhedral set is the convex
hull of the integer points of the set. In most of the cases, for example
when the set is bounded, the integer hull is a polyhedral set, as well.
The integer hull can be determined in an iterative way by Chv´atal cuts.
Weighted (or chamfer) distances are popular digital distances used in
various grids. They are based on the weights assigned to steps to var-
ious neighborhood. In the triangular grid there are three usually used
neighborhood, consequently, chamfer distances based on three weights
are deﬁned. A digital disk (or a chamfer ball) of a grid is the set of the
elements which are not on a longer distance from the origin than a given
ﬁnite bound, radius. These disks are well known and well characterized
on the square grid (with even larger neighborhood than the usual 3 × 3),
and recently they become a topic of a current research on the triangular
grid. The shapes of the disks in the latter case have a great variability. In
this paper, the inequalities satisﬁed by the elements of a disk are analyzed
if their Chv´atal rank is 1. The most popular coordinate system of the
triangular grid uses three coordinates. Individual bounds are described
completely. It also gives the complete description of some disks. Further
inequalities having Chv´atal rank 1 are also discussed.
Keywords: Weighted distances · Chamfer balls · Non-traditional grids ·
Integer programming · Optimization
1
Introduction
A grid consists of several tiles/pixels. A step is moving from one pixel to another,
neighbor one. Each step has a positive length depending on the two pixels and
their relative positions. The distance of two pixels of the grid is measured by the
length of the minimal path between them. A disk of a grid is a set of pixels such
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 94–106, 2017.
DOI: 10.1007/978-3-319-66272-5 9

An Integer Programming Approach to Characterize Digital Disks
95
that their distance from a ﬁxed pixel, say P is not greater than a given value.
In most of the cases P is considered to be the origin of grid.
The shape of a disk can be deﬁned as follows. Assume that the pixels of the
grid are symmetric and have a center point. Then the shape of the disk is the
shape of the convex hull of the center points of the pixels which are the elements
of the grid. It is well known that the shape of the disk is always an octagon
on the square grid based on the usual two neighborhood [1,2]. There is a wide
variety of shapes of the disks on the triangular grid [8,12]. On Fig. 1 we show an
example.
The pixels of a grid are identiﬁed by integer coordinates depending on the
type of the grid, e.g., each pixel has its 2 or 3 coordinates which uniquely identify
the pixels, in the square and the triangular grids, respectively. These coordinates
can be considered as the coordinates of the center point of the pixel. The disk
is determined by some inequalities and equations. One inequality restricts the
total distance. There are grids where the coordinates must also satisfy some
conditions. For example, the sum of the coordinates can be either 0 or 1 in the
case of the triangular grid. Finally, the path from the origin to the pixel must
be described as well. These constraints are satisﬁed by many values including
even non-integer ones. The problem which is the main topic of this paper is,
how can be these constraint used to determine the convex hull of the integer
points satisfying the constraints. It is a general problem in integer programming
(optimization).
2
The Triangular Grid
The triangular grid is a complete, non-overlapping coverage of the plane by
regular triangles [9,10]. Each pixel of the grid can be addressed by an integer
coordinate triplet having zero or one sum. These two types of vectors diﬀerenti-
ate the two types of the orientations of the pixels of the grid. Zero sum vectors
address the even pixels (they have shape △in this paper), while one sum triplets
address the odd pixels (their shape is ▽). As Fig. 1 shows, the coordinate axes
have angle 120◦pairwise. The set of pixels where one of the coordinate values is
ﬁxed, is called a lane and it is orthogonal to the axis with ﬁxed value. For exam-
ple, the pixels of the top of the Fig. 1 belong to the lane y = −4. This symmetric
coordinate system captures well the well-known neighborhood relations [3] of the
grid (Fig. 1, right). Formally, the points (i.e., pixels) p = (p(1), p(2), p(3)) and
q = (q(1), q(2), q(3)) of the triangular grid are
– m-neighbors (m = 1, 2, 3), if
(i) |p(i) −q(i)| ≤1, for i = 1, 2, 3, and
(ii) |p(1) −q(1)| + |p(2) −q(2)| + |p(3) −q(3)| ≤m.
– strict m-neighbors, if there is an equality in (ii) for the value of m.
Let p = (p(1), p(2), p(3)) and q = (q(1), q(2), q(3)) be two points of the trian-
gular grid. A ﬁnite sequence of points of the form p = p0, p1, . . . , pm = q, where
pi−1, pi are 3-neighbor points for 1 ≤i ≤m, is called a path from p to q. The term

96
G. Kov´acs et al.
Fig. 1. A part of the triangular grid with the symmetric coordinate system, an example
to (the convex hull of) a disk and a pixel (X) with its various neighbors (right left
corner).
k-step will be used to abbreviate the sentence ‘step to a strict k-neighbor point’
(k ∈{1, 2, 3}).
There are three types of steps in the problem which are called according to
their length 1-step, 2-step, and 3-step. We use the notation given in Table 1 for
various possible k-steps.
Table 1. 1-steps, 2-steps, and 3-steps on the triangular grid.
1-steps
2-steps
3-steps
u1 u2 u3
u4
u5
u6
v1
v2
v3
v4
v5
v6
w1
w2
w3
w4
w5
w6
1
0
0 −1
0
0
1
1
0 −1 −1
0
1
1 −1 −1 −1
1
0
1
0
0 −1
0 −1
0
1
1
0 −1
1 −1
1 −1
1 −1
0
0
1
0
0 −1
0 −1 −1
0
1
1 −1
1
1
1 −1 −1
In this paper without loss of generality, we can deal with shortest paths from
the origin (0, 0, 0) to point (x, y, z).
Chamfer distances on the triangular grid were investigated in [11] based on
various weights for the 3 types of steps on the grid.

An Integer Programming Approach to Characterize Digital Disks
97
3
Chamfer Distances
Using the 3 types of neighbors there could be various paths from a point p to a
point q in the triangular grid. Let the weights for the various steps be ﬁxed as
a, b, c for 1-, 2-, and 3-steps, respectively, with the condition c ≥b ≥a > 0. Then,
the sum of the weights of the steps of a path is considered as a weighted path.
The chamfer distance (or, in another term, the weighted distance) of two points
is the weight of the weighted shortest path between them, i.e., the minimal value
among weights of the paths between the points: d((x1, y1, z1), (x2, y2, z2); a, b, c).
In [11] it is proven, that this distance function satisﬁes the metric conditions: it
is positive deﬁnite, symmetric and the triangle inequality holds.
Let the chamfer ball, disk(r) be the set of pixels (x, y, z) for which the
weighted distance between the origin and (x, y, z) is at most r, formally,
disk(r) = {(x, y, z) | d((0, 0, 0), (x, y, z); a, b, c) ≤r}.
Chamfer distances are discussed for some other grid in [13]. They are con-
nected to the Frobenius problem of three variables [4,13].
4
Integer Hull and Chv´atal Cuts
Let m and n be two positive integers, A an m×n matrix and b an m-dimensional
vector. The set P = {x | Ax ≤b} is a polyhedron in the n-dimensional space.
The integer hull of P is the convex hull of its integer points, i.e. the set int(P) =
conv(P ∩Zn), where Zn is the lattice of n-dimensional integer vectors.
The set int(P) is not necessarily a polyhedral set [5]. However, it is a poly-
hedral set in many cases including the case when P is bounded or A and b have
rational elements [7]. It means that the disks are always polygons.
The set int(P) can be determined by an iterative procedure. The key tool of
the algorithm is the Chv´atal cut. Assume that λ ∈

R≥0m is an m-dimensional
vector such that the product λT A is an integer vector. Then all integer vector x
of P must satisfy the inequality
λT Ax ≤

λT b

,
(1)
where the usual ﬂoor function ⌊·⌋is used. If λT b ̸=

λT b

then (1) is not an alge-
braic consequence of the original inequalities deﬁning P. This inequality is the
Chv´atal cut which can be added to other inequalities without cutting any integer
point from P. Further details of the method can be obtained from [14]. What is
important here is that for there are ﬁnitely many signiﬁcantly diﬀerent Chv´atal
cuts of every inequality system. One iteration is to generate all of them. When
the inequality system is enlarged by the generated inequalities, the procedure
can be repeated. The integer hull is obtained after ﬁnitely many iterations.
The Chv´atal rank of an inequality is the number of the iteration in which
it was generated. The rank of the inequalities deﬁning P is 0. The rank of the

98
G. Kov´acs et al.
inequalities generated by rank 0 inequalities is 1, etc. In this paper, only inequal-
ities having rank 1 are investigated. In one iteration the number of Chv´atal cuts
could be even exponential on the number of original equations (Chap. 23 in
[14]), therefore their analysis gives already becomes very time consuming and
also gives some nice results. The Chv´atal cuts are analyzed systematically only
in few papers. [6] discusses the Chv´atal cuts of the knapsack polytope of rank 1.
5
Linear Programming Model
There are several methods to describe a chamfer disk. A new approach is dis-
cussed in this paper. The polygon of a disk for a given radius r consists of
the feasible solutions of an integer programming problem. It means that the
coordinate vectors must satisfy certain linear inequalities and must have integer
components. The size of the model, i.e. number of rows and columns, is ﬁxed.
What is changing is only the right-hand side. The theory of linear program-
ming gives a complete description of the potential optimal solutions. The form
of the disks can be one of the elements of a ﬁnite set according to our empirical
observation. Thus it seems possible to give a complete structural description of
the disks by uncovering the integer hull of the feasible sets of the related LP
problems. The structural description eliminate the necessity of any algorithm.
A 1-step changes one coordinate by 1 unit. A 2-step changes two coordinates
into the opposite direction by 1 unit. Finally, a 3-step changes all coordinates by
1 unit, however the direction of the three changes are not the same. Thus, the
matrix of the steps is speciﬁed in Table 1. Let ui, vi and wi (for i = 1, . . . , 6) are
the numbers of various steps. The disk constraint is given in the following form:
au1 + · · · + au6 + bv1 + · · · + bv6 + cw1 + · · · + cw6 ≤r,
where c ≥b ≥a > 0. It should be note here that the triangular grid is not a
lattice, therefore the steps of a path are usually not free to permute. However,
when a multiset of steps is speciﬁed corresponding to a path connecting the
origin to another pixel, then because of the constraint on the sum of coordinate
values, there exists always a path on the grid which built up by those steps if
the sum of the coordinate changes is equal to 0 or 1.
Now, we are ready to continue to show other constraints. All step variables
are non-negative:
u1, . . . , u6, v1, . . . , v6, w1, . . . , w6 ≥0.
If a point (x, y, z)T is reached from the origin by the steps, then the step numbers
satisfy the equation system
u1 −u4 + v1 + v2 −v4 −v5 + w1 + w2 −w3 −w4 −w5 + w6 = x,
u2 −u5 −v1 + v3 + v4 −v6 + w1 −w2 + w3 −w4 + w5 −w6 = y,
u3 −u6 −v2 −v3 + v5 + v6 −w1 + w2 + w3 + w4 −w5 −w6 = z.

An Integer Programming Approach to Characterize Digital Disks
99
The sum of the coordinates x, y, and z are between 0 and 1:
x + y + z ≤1,
x + y + z ≥0.
The polyhedral set is deﬁned by the system of constraints shown in Table 2,
where the index of the (in)equality is used in further analysis as is indicated
there. Using the same index set, the multipliers of the inequalities of the original
constraint set are denoted by λ0, . . . , λ23, i.e., they are the elements of vector λ.
The facet deﬁning inequalities concern to the coordinates of the points only,
i.e. to the variables x, y, and z. It is assumed that the facet-deﬁning inequality
of the disk is given in the form
ex + fy + gz ≤h
(2)
with e, f, g, h ∈Z. The rank of the deﬁning inequalities is 0 by deﬁni-
tion. If (2) is not among the deﬁning inequalities of the disk polytope, then
its Chv´atal rank is 1 only if multipliers λ0, . . . , λ23 can be chosen such that
λ0, . . . , λ18, λ22, λ23 are nonnegative, and the coeﬃcients on the left-hand side of
the generated inequality are e, f, g, 0, ..., 0, while the right-hand side is less than
h+1. If there are several options that can be employed to generate the left-hand
side, then the preferred choice is that which gives the minimal right-hand side
result. This observation leads to the following linear programming model:
Table 2. The inequalities deﬁning the disk polytope, RHS stands for Right-Hand Side.
0
au1 + · · · +au6 +bv1 + · · · +bv6 +cw1 + · · · +cw6 ≤r
1
−u1
≤0
6
−u6
≤0
7
−v1
≤0
12
−u6
≤0
13
−w1
≤0
18
−w6 ≤0
19
−x
+u1
· · ·
+v1
· · ·
+w1
· · ·
+w6 = 0
20
−y
· · ·
−v1
· · ·
−v6
+w1
· · ·
−w6 = 0
21
−z
· · ·
−u6
· · ·
+v6
−w1
· · ·
−w6 = 0
22
x +y +z
≤1
23
−x −y −z
≤0

100
G. Kov´acs et al.
min rλ0 + λ22
−λ19 + λ22 −λ23 = e
(3)
−λ20 + λ22 −λ23 = f
(4)
−λ21 + λ22 −λ23 = g
(5)
aλ0 −λ1 + λ19 = 0
(6)
...
aλ0 −λ6 −λ21 = 0
(7)
bλ0 −λ7 + λ19 −λ20 = 0
(8)
...
bλ0 −λ12 −λ20 + λ21 = 0
(9)
cλ0 −λ13 + λ19 + λ20 −λ21 = 0
(10)
...
cλ0 −λ18 + λ19 −λ20 −λ21 = 0
(11)
λ0, . . . λ18, λ22, λ23 ≥0.
(12)
6
Construction of Chv´atal Cuts
In general, a cut (1) is not necessarily facet deﬁning cut as even stronger cuts
might be generated in farther iterations. In this section, some cuts of rank 1 are
generated.
The next lemma goes independently of the fact that we are using the trian-
gular grid, that is, we do not take into account about the constraints i = 22, 23,
we set their weights λ22 = λ23 = 0.
Lemma 1. Chv´atal rank of (2) is 1 if
r · max

max
i∈{e,f,g}
|i|
a

;
max
i,j∈{e,f,g}
|i −j|
b

; M3

< h + 1,
where M3 =
max
i,j,k∈{e,f,g}

|i−j−k|
c
; |i+j−k|
c
	
.
Proof. If λ22 = λ23 = 0, then we must minimize λ0 for the best RHS.
In this case from (3), (4), and (5) we get, that λ19 = −e, λ20 = −f, and
λ21 = −g.
The minimal value of λ0 for the Eqs. (6) and (7) is
max{|e|, |f|, |g|}
a
=
max
i∈{e,f,g}
|i|
a

≤λ0,
because λi’s are nonnegative for i = 1, . . . , 6.

An Integer Programming Approach to Characterize Digital Disks
101
Similarly, the minimal value of λ0 for the Eqs. (8) and (9) is
max{|e −f|, |e −g|, |f −g|}
b
=
max
i,j∈{e,f,g}
|i −j|
b

≤λ0,
because λi’s are nonnegative for i = 7, . . . , 12.
Finally, the minimal value of λ0 for the Eqs. (10) and (11) is
max{|e + f −g|, |e −f + g|, | −e + f + g|}
c
= M3 ≤λ0,
because λi’s are nonnegative for i = 13, . . . , 18.
⊓⊔
Corollary 1. If 2a ≤b, 3a ≤c and
r ·
max
i∈{e,f,g}
|i|
a

≤h + 1,
then the Chv´atal rank of (2) is 1.
Theorem 1. Chv´atal rank of the inequality x ≤k is 1 if and only if
r + a
2a
< k + 1 if 2a ≤b and 3a ≤c,
r + b −a
b
< k + 1 if b ≤2a and 2b ≤a + c,
2r + c −a
c + a
< k + 1 if c ≤3a and a + c ≤2b.
Proof. In this case, let e = 1, f = g = 0 in (2). We need to minimize rλ0 + λ22,
where (3)–(12) hold. Only (3)–(5) contain λ23 and always in the form λ22 −λ23
not counting (12). It means that because of minimizing λ22 we need to choose
λ23 = 0, because λ22 is nonnegative. Let us denote λ22 = s. Thus from (3)
we get, that λ19 = s −1 (it may be negative). From (4) and (5) we get, that
λ20 = λ21 = s.
Then the minimal value for λ0 satisﬁes (6) and (7) is the following:
max{s, −s, s −1, 1 −s}
a
= max
s
a, 1 −s
a

≤λ0.
(13)
The minimal value for λ0 satisﬁes (8)–(9) is
max{s −s, s −(s −1), (s −1) −s}
b
= 1
b ≤λ0.
(14)
Finally, the minimal value for λ0 satisﬁes (10)–(11) is
max{s −s −(s −1), s −s + (s −1), −s −s + (s −1), s + s −(s −1)}
c
=
= s + 1
c
≤λ0.
(15)

102
G. Kov´acs et al.
Then from (13)–(15) we get
max
s
a, 1 −s
a
, 1
b , s + 1
c

≤λ0.
It means that to minimize rλ0 + s we need to minimize the following in s:
max
rs
a + s, r(1 −s)
a
+ s, r
b + s, r(s + 1)
c
+ s

.
There are four functions of s in the above maximum, let us denote them by
f1(s), . . . , f4(s).
If r < a, then our disk contains only the origin. We can assume that a ≤r.
In this case f2(s) is decreasing, but other three functions are increasing. If s = 0,
then the value of the second function is greater than the others: f2(0) = r
a ≥
fi(0), where i = 1, 3, 4, because we assumed that a ≤b ≤c.
The minimal value of the maximum of the above functions is obtained when
the decreasing one (f2(s)) has the same value as one of the increasing functions
(we say that the decreasing function and one of the increasing functions are
intersecting each other).
Case 1. The intersection of f1(s) and f2(s) is s = 1
2. In this case the value of
the functions is
f1

1
2

= f2

1
2

= r + a
2a .
It is maximal for all fi in s = 1
2 if f1(s) = f2(s) ≥f3(s), thus
r
2a + 1
2 ≥r
b + 1
2,
and this holds if 2a ≤b. f1(s) = f2(s) ≥f4(s) means that
r
2a + 1
2 ≥
3
2r
c + 1
2,
and this holds if 3a ≤c.
Case 2. The intersection of f2(s) and f3(s) is s = b−a
b
from 1−s
a
= 1
b. In this
case the value of the functions is
f2

b −a
b

= f3

b −a
b

= r + b −a
b
.
It is maximal for all fi in s = b−a
b
if f2(s) = f3(s) ≥f1(s), thus
r
b + b −a
b
≥r(b −a)
ba
+ b −a
b
,
and this holds if b ≤2a. f2(s) = f3(s) ≥f4(s) means that
r
b + b −a
b
≥r(2b −a)
bc
+ b −a
b
,
and this holds if 2b ≤a + c.

An Integer Programming Approach to Characterize Digital Disks
103
Case 3. The intersection of f2(s) and f4(s) is s = c−a
c+a from 1−s
a
= s+1
c . In this
case the value of the functions is
f2

c −a
c + a

= f4

c −a
c + a

= 2r + c −a
c + a
.
It is maximal for all fi in s = c−a
c+a if f2(s) = f4(s) ≥f1(s), thus
2r
c + a + c −a
c + a ≥r(c −a)
a(c + a) + c −a
c + a,
and this holds if c ≤3a. f2(s) = f4(s) ≥f3(s) means that
2r
c + a + c −a
c + a ≥r
b + c −a
c + a,
and this holds if a + c ≤2b.
⊓⊔
The conditions of the above three cases contain all possibilities. If 2a ≤b
and 3a ≤c, then this is Case 1. If b ≤2a and 3a ≤c, then 2b ≤4a ≤a + c,
thus this subcase is part of Case 2. If 2a ≤b and c ≤3a, then c + a ≤4a ≤2b,
thus this subcase is part of Case 3. If b ≤2a and c ≤3a, then a + c ≤2b or
2b ≤a + c is possible, thus one part of this subcase belongs to Case 2, another
part of this subcase belongs to Case 3. Further we will refer to these cases as we
have described them here.
Theorem 2. Chv´atal rank of the inequality −x ≤l is 1 if and only if
r
2a < l + 1 if 2a ≤b and 3a ≤c,
r
b < l + 1 if b ≤2a and 2b ≤a + c,
2r
c + a < l + 1 if c ≤3a and a + c ≤2b.
The proof follows the same idea as the previous one with e = −1, f = g = 0
in (2).
7
Facet-Deﬁning Inequalities
Theorem 3. The above mentioned inequalities x ≤k and −x ≤l are facet-
deﬁning in Case 1 and in Case 2.
Proof. Case 2a ≤b and 3a ≤c. This case belongs to Case 1 of the above two
Theorems. It is in [11] that the distance function between (0, 0, 0) and (x, y, z)
is d(a, b, c) = a(|x| + |y| + |z|). If we want to create the point of the disk, which
has minimal or maximal value in x, we need to solve the following problem:
min(or max)x
d(a, b, c) ≤r
(16)

104
G. Kov´acs et al.
x + y + z ≤1
−x −y −z ≤0
x, y, z ∈Z
If x + y + z = 0, then the maximal value of |x| can be equal to |x|+|y|+|z|
2
. In this
case 2|x| = |x| + |y| + |z| ≤r
a and it means that −x ≤
 r
2a

is a facet-deﬁning
equation.
If x+y +z = 1, then the maximal value of x can be equal to |x|+|y|+|z|+1
2
. In
this case 2x −1 = |x| + |y| + |z| ≤r
a and x ≤
 r+a
2a

is a facet-deﬁning equation.
Case b ≤2a and 3a ≤c. In this case 2b ≤a + c, and this case belongs to
Case 2 of the above two Theorems. The distance function between (0, 0, 0) and
(x, y, z) is in [11]:
d(a, b, c) =

b |x|+|y|+|z|
2
,
if x + y + z = 0;
a + b |x|+|y|+|z|−1
2
, if x + y + z = 1.
If x + y + z = 0, then from (16) and from the maximal value of |x| we get
that 2|x| = |x| + |y| + |z| ≤2r
b and it means that −x ≤
 r
b

is a facet-deﬁning
equation.
If x + y + z = 1, then from (16) and from the maximal value of x we get that
2x −1 = |x| + |y| + |z| ≤2r−2a+b
b
and x ≤
 r+b−a
b

is a facet-deﬁning equation.
Case b ≤2a, c ≤3a and a + b ≤c. In this case 2b ≤a + c, and this case
belongs to Case 2. In this case function d is the same as in the previous case,
i.e., our statements hold.
Case b ≤2a, c ≤3a, c ≤a + b and 2b ≤a + c. This case belongs to Case
2, too. If x + y + z = 0, then function d is the same as in the previous case.
If x + y + z = 1 and we want to maximize the value of |x|, then x is positive
and y and z are negative. In this case [11] uses the same distance function as
in the previous case. There is a third distance function in [11] in the case of
x + y + z = 1 for the subcase a negative and two positive coordinates, but in
this subcase the value of |x| is not maximal.
Case b ≤2a, c ≤3a, c < a+b and a+c ≤2b and Case 2a ≤b and c ≤3a.
These cases belong to Case 3.
⊓⊔
The result of the theorem can give at most 6 of the sides of the chamfer disk,
that is, actually, the embedded hexagon of the disk.
Example. The inequalities provided by the theorem can be both facet deﬁning
and non-facet deﬁning in Case 3. If a = 4, b = 7, c = 8 and r = 30 (see Fig. 1),
then this case belongs to Case 3.
(a) In this case the Chv´atal rank of the inequality of −x ≤l is 1 if and
only if
 60
12

= 5 ≤l. Let v(i) be the sorted coordinate values of (x, y, z) in a
non-increasing way by their absolute values, i.e., |v(3)| ≤|v(2)| ≤|v(1)|. The
distance function between (0, 0, 0) and (x, y, z) is in [11]:
d(a, b, c) = |v(1)| b
2 + |v(2)| b
2 + |v(3)|(a + c −3
2b),
(17)

An Integer Programming Approach to Characterize Digital Disks
105
if x+y+z = 0. In this case a+c−3
2b ≤b
2, thus the value of d(a, b, c) is minimal for
a given x if |v(3)| is close to |v(2)|, i.e., if x is even, then |v(2)| = |v(3)| = |x|
2 ; and
if x is odd, then |v(2)| = |v(3)| + 1 = |x|+1
2
. For example if (x, y, z) = (−5, 3, 2),
then v(1) = −5, v(2) = 3, v(3) = 2, and in this case d(4, 7, 8) = 31, thus there
is no point with x = −5 for this distance function to satisfy d(.) ≤r.
If x + y + z = 1, then the distance function between (0, 0, 0) and (x, y, z) is
diﬀerent from (17). If x + y + z = 1 and x = −5, then by decreasing one of the
positive coordinates by 1, the new point has the same x value and the distance
of the new point from (0, 0, 0) is less than the distance of the original one, thus
there is no point with x = −5 for the distance functions of case x + y + z = 1
to satisfy d(.) ≤r. It means that the disk of the value r = 30 has no point with
x = −5, i.e., −x ≤5 is not facet-deﬁning.
(b) The Chv´atal rank of the inequality of x ≤5 is 1. If (x, y, z) = (5, −1, −3),
then d(4, 7, 8) = 30 based on [11], thus x ≤5 is a facet-deﬁning equation.
(c) (5, −1, −3) and (4, 1, −4) are points of the disk, the inequality x −z ≤8
holds for these pixels of the disk. The Chv´atal rank of this inequality is 1 if h = 8
in Lemma 1, thus x −z ≤8 is a facet-deﬁning equation with Chv´atal rank of 1.
The disk of Fig. 1 has 12 facet-deﬁning equation: there are three 1-ranked
inequalities similar to x ≤5; six 1-ranked inequalities similar to x −z ≤8; and
three not 1-ranked inequalities similar to −x ≤4.
A further analysis of 1-ranked conditions is planned. We believe that we will
ﬁnd connection between the chamfer radius of the disk and the Eucledian radius
by using the theory of 1-ranked conditions.
References
1. Borgefors, G.: Distance transformations in digital images. Comput. Vis. Graph.
Image Process. 34(3), 344–371 (1986)
2. Butt, M.A., Maragos, P.: Optimum design of chamfer distance transforms. IEEE
Trans. Image Process. 7(10), 1477–1484 (1998)
3. Deutsch, E.S.: Thinning algorithms on rectangular, hexagonal and triangular
arrays. Comm. ACM 15, 827–837 (1972)
4. Hujter, M., Vizv´ari, B.: The exact solutions to the Frobenius problem with three
variables. J. Ramanujan Math. Soc. 2, 117–143 (1987)
5. Jeroslow, R.G.: Comments on integer hull of two linear constraints. Oper. Res. 19,
1061–1069 (1971)
6. Kov´acs, G., Vizv´ari, B.: On Chv´atal complexity of knapsack problems, RUTCOR,
Rutgers University, Research report, 15–2008 (2008)
7. Meyer, R.R.: On the existence of optimal solutions to IP and MIP problems. Math.
Program. 7, 223–235 (1974)
8. Mir-Mohammad-Sadeghi, H., Nagy, B.: On the chamfer polygons on the triangular
grid. In: Brimkov, V.E., Barneva, R.P. (eds.) IWCIA 2017. LNCS, vol. 10256, pp.
53–65. Springer, Cham (2017). doi:10.1007/978-3-319-59108-7 5
9. Nagy, B.: Shortest path in triangular grids with neighbourhood sequences. J. Com-
put. Inf. Technol. 11, 111–122 (2003)
10. Nagy, B.: Characterization of digital circles in triangular grid. Pattern Recogn.
Lett. 25, 1231–1242 (2004)

106
G. Kov´acs et al.
11. Nagy, B.: Weighted distances on a triangular grid. In: Barneva, R.P., Brimkov,
V.E., ˇSlapal, J. (eds.) IWCIA 2014. LNCS, vol. 8466, pp. 37–50. Springer, Cham
(2014). doi:10.1007/978-3-319-07148-0 5
12. Nagy, B., Mir-Mohammad-Sadeghi, H.: Digital disks by weighted distances
in the triangular grid. In: Normand, N., Gu´edon, J., Autrusseau, F. (eds.)
DGCI 2016. LNCS, vol. 9647, pp. 385–397. Springer, Cham (2016). doi:10.1007/
978-3-319-32360-2 30
13. Remy, E., Thiel, E.: Computing 3D medial axis for chamfer distances. In: Borgefors,
G., Nystr¨om, I., Baja, G.S. (eds.) DGCI 2000. LNCS, vol. 1953, pp. 418–430.
Springer, Heidelberg (2000). doi:10.1007/3-540-44438-6 34
14. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, Chichester (1986)

Discrete Tomography

High-Level Algorithm Prototyping: An Example
Extending the TVR-DART Algorithm
Axel Ringh1(B), Xiaodong Zhuge2, Willem Jan Palenstijn2,
Kees Joost Batenburg2,3, and Ozan ¨Oktem1
1 Department of Mathematics, KTH Royal Institute of Technology,
Stockholm, Sweden
{aringh,ozan}@kth.se
2 Computational Imaging, Centrum Wiskunde & Informatica (CWI),
Amsterdam, The Netherlands
{x.zhuge,willem.jan.palenstijn,joost.batenburg}@cwi.nl
3 Mathematical Institute, Leiden University, Leiden, The Netherlands
Abstract. Operator Discretization Library (ODL) is an open-source
Python library for prototyping reconstruction methods for inverse prob-
lems, and ASTRA is a high-performance Matlab/Python toolbox for
large-scale tomographic reconstruction. The paper demonstrates the fea-
sibility of combining ODL with ASTRA to prototype complex recon-
struction methods for discrete tomography. As a case in point, we con-
sider the total-variation regularized discrete algebraic reconstruction tech-
nique (TVR-DART). TVR-DART assumes that the object to be imaged
consists of a limited number of distinct materials. The ODL/ASTRA
implementation of this algorithm makes use of standardized building
blocks, that can be combined in a plug-and-play manner. Thus, this imple-
mentation of TVR-DART can easily be adapted to account for application
speciﬁc aspects, such as various noise statistics that come with diﬀerent
imaging modalities.
1
Introduction
Inverse problems refer to the task of reconstructing parameters characterizing
the system under investigation from indirect observations. Such problems arise
in several areas of science and engineering, and in particular for tomographic
imaging. The idea here is to expose the object to penetrating waves or particles
A. Ringh and O. ¨Oktem—The authors are supported by the Swedish Research Coun-
cil (VR) grant 2014-5870, and the Swedish Foundation of Strategic Research (SSF)
grant AM13-0049.
X. Zhuge—The author is supported by the Stichting voor de Technische Weten-
schappen (STW) through a personal grant (Veni, 13610).
W.J. Palenstijn—The author is supported by the Stichting voor de Technische
Wetenschappen (STW), project 13314.
K.J. Batenburg—The author is supported by the Netherlands Organization for Sci-
entiﬁc Research (NWO), project 639.073.506.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 109–121, 2017.
DOI: 10.1007/978-3-319-66272-5 10

110
A. Ringh et al.
from diﬀerent directions. The measured transmission (or emission) data is then
used as input to a reconstruction scheme that computes an estimate of the
interior structure of the object.
Computed tomography (CT) has a wide range of applications, e.g., X-ray CT
[9] in medical imaging and electron tomography (ET) [8,10] in biology and mate-
rial science. A key element is to model the interaction between the object and the
wave/particle probe with suﬃcient accuracy. The resulting inverse problems are
often ill-posed, for example in the sense that small errors in data get ampliﬁed.
Hence, one must stabilize the reconstruction (regularization) by exploiting a pri-
ori knowledge of the unknown interior structure. Discrete tomography considers
a speciﬁc type of prior knowledge, where it is assumed that the unknown object
consists of a small number of diﬀerent materials, each corresponding to a char-
acteristic, approximately constant grey level in the reconstruction. A variety of
reconstruction algorithms have been proposed for discrete tomography problems
including primal-dual subgradient algorithms [12], network ﬂow algorithms [3],
statistical methods [2,7] and algebraic methods [4,17].
At ﬁrst sight it may seem that most aspects of a reconstruction method are
problem-speciﬁc. This is fortunately not the case. In fact, the general theory
developed during the last three decades provides a number of generic frame-
works that are adaptable to speciﬁc ill-posed inverse problems. When properly
adapted, the methods derived from the general framework compare favorably
with application-speciﬁc approaches. Furthermore, using general mathematical
tools to address speciﬁc problems also provides new insights that may go unno-
ticed if one uses an entirely problem-speciﬁc approach. An example is sparsity
promoting regularization, which is a general framework that outperforms, or
matches, many application-speciﬁc state-of-the-art approaches for inverse prob-
lems where data are highly noisy or under-sampled.
Despite the above, most concrete implementations of reconstruction methods
are tied to a speciﬁc application in the sense that minor mathematical modiﬁ-
cations lead to a large amount of low level implementations and modiﬁcations,
which require substantial dedicated algorithmic and programming eﬀorts. Oper-
ator Discretization Library (ODL) [1] and ASTRA toolbox [14] are two open-
source software libraries developed to assist fast prototyping of reconstruction
algorithms. When used together, a user may implement a generic reconstruc-
tion method and use it on diﬀerent tomographic real-world problems without
having to re-implement all necessary parts from the bottom up. This becomes
especially useful for complex reconstruction methods.This paper demonstrates
the capabilities of ODL and ASTRA on a recently proposed discrete tomography
algorithm, total variation regularized discrete algebraic reconstruction technique
(TVR-DART).
2
Inverse Problems and Tomography
Mathematically, an inverse problem in imaging can be stated as the problem of
reconstructing an image f ∈X representing the object under study from data

High-Level Algorithm Prototyping
111
g ∈Y where
g = A(f) + “noise”.
(1)
Here, A : X →Y (forward operator) models how an image gives rise to data
in the absence of noise. Moreover, X is a suitable Hilbert space of real valued
functions supported on a ﬁxed domain Ω ⊂Rn whose elements represent atten-
uation of emission values. Likewise, Y is a Hilbert space of real-valued functions
that represent data and that are deﬁned on some manifold M (data manifold).
In tomographic imaging, data can often be modeled as line integrals of the
function f that describes the object along a line, i.e., data is a real-valued func-
tion on some set of lines M in Rn. We can now introduce coordinates on this
data manifold. A line in Rn can be described by a directional vector in the unit
sphere Sn−1 and a point that it passes through. This provides coordinates on
M where a line is given by (ω, x) ∈Sn−1 × Rn with x ∈ω⊥. Here, ω⊥⊂Rn is
the unique plane through the origin with ω ∈Sn−1 as its normal vector. The
corresponding forward operator A is the ray transform, which is expressible in
the aforementioned coordinates as
A(f)(ω, x) :=
 ∞
−∞
f(x + tω)dt
for f ∈X.
(2)
Tomographic data can then be seen as values of A(f)(ω, x) for a sampling of
ω ∈Sn−1 (angular sampling) and x ∈ω⊥(detector sampling). With slight abuse
of terminology, one refers to these data as the “projection” of f along the line
given by (ω, x).
3
Overview of ODL and ASTRA
Many reconstruction schemes can be formulated in a generic, yet adaptable, man-
ner by stating them in an abstract coordinate-free setting using the language of
functional analysis. The adaptability stems from “parametrizing” the scheme in
terms of the forward operator, the data noise model, and the type of a priori infor-
mation that one seeks to exploit. These can be further broken down into more
generic components, each representing a well-deﬁned generally applicable mathe-
matical structure or operation. Another advantage that comes with a generic for-
mulation is that it makes the reconstruction scheme more transparent.
These considerations form a natural blueprint for a modular software library
for inverse problems where the forward operator, data noise model, and prior
model are treated as independent exchangeable components. ODL is such a
software library whose design principles are modularity, abstraction, and com-
partmentalization [1] that is freely available at http://github.com/odlgroup/odl.
To realize these design principles, ODL separates which mathematical object or
operation one seeks to represent from how it is implemented using concrete
computational routines. Mathematical objects and operations are represented
by abstract classes with abstract methods and speciﬁc implementations are rep-
resented by concrete subclasses. Hence, these abstract classes form a domain

112
A. Ringh et al.
speciﬁc language for functional analysis and corresponding subclasses couple to
relevant numerical libraries. In this way one can express abstraction in a way
that allows combining generic and application-speciﬁc code. The generic part,
such as an optimization method, can be formulated in a coordinate-free manner
using abstract classes and methods, whereas application-speciﬁc parts, such as
evaluating the forward operator, are contained in speciﬁc subclasses. Hence, one
can express reconstruction schemes using a clean near-mathematical syntax and
involved implementation speciﬁc details are hidden in concrete subclasses.
A key part of ODL is the notion of an operator between two vector spaces.
ODL oﬀers operator calculus for constructing operators from existing ones, typ-
ically using composition. Furthermore, an operator may also have a number of
additional associated operators, like its (Fr´echet) derivative, inverse, and adjoint.
Whenever possible, such associated operators are automatically generated when
an operator is deﬁned using the operator calculus in ODL, e.g., derivative oper-
ators are formed using the chain rule. This is a very powerful part of ODL that
reduces the risk for errors and simpliﬁes testing.
Another important part of ODL is its usage of external software libraries
for performing speciﬁc tasks. When working with tomographic inverse prob-
lems, one such task is computing the 2D/3D ray transform and its adjoint. For
this ODL employs the ASTRA toolbox [11,13,14], which is a high-performance,
GPU accelerated toolbox for tomographic reconstruction freely available from
http://www.astra-toolbox.com. The toolbox supports many diﬀerent data man-
ifolds arising in tomography, including for example circular cone beam, laminog-
raphy, tomosynthesis, and electron tomography, see [13] for details. It also pro-
vides both Matlab and Python interfaces, that expose the core tomographic
operations. The latter is used for seamless integration between ODL and ASTRA
in the sense that forward and backprojection routines in ASTRA are available as
operators in ODL. Likewise, the tomographic data acquisition model in ASTRA
is fully reﬂected by the corresponding data model in ODL. This open up for
using ASTRA routines from ODL without unnecessary data copying between
GPU and CPU.
The reconstruction methods available in ASTRA are mostly iterative meth-
ods. In ODL, on the other hand, one can easily formulate a variational recon-
struction algorithm. In this work we will consider ODL/ASTRA to formulate a
variational reconstruction algorithm and solve the corresponding optimization
problem.
4
Discrete Algebraic Reconstruction
A large class of reconstruction methods for ill-posed inverse problems can be
formulated as solving an optimization problem:
min
f∈X

L

A(f), g

+ λ R(f)

.
(3)
Here, R: X →R+ is the regularization term that accounts for the a priori
knowledge by penalizing unfeasible solution candidates, L: Y × Y →R+ is the

High-Level Algorithm Prototyping
113
data-ﬁt term that quantiﬁes how well two points in the data space agree with
each other, and λ > 0 is the regularization parameter that weights the a priori
knowledge against the need to minimize the data-ﬁt term.
Discrete tomography (DT) is a class of tomographic reconstruction meth-
ods that are based on the assumption that the unknown object f consists of
a few distinct materials, each producing a (almost) constant gray value in the
reconstruction. The total variation regularized discrete algebraic reconstruction
technique (TVR-DART) is a recent approach that is adapted towards discrete
tomography [16,17], which has proven to be more robust than the original DART
algorithm [4]. In particular, using the TVR-DART method allows one to signif-
icantly improve reconstruction quality and to drastically reduce the number of
required projection images and/or exposure to the sample. The idea in TVR-
DART is that the image to be recovered is step-function like with a transition
between regions that is not necessarily sharp. This is typically the case when
one images specimens consisting of only a few diﬀerent material compositions
where each material has a distinct gray value in the corresponding image.
TVR-DART aims to capture such a priori information by combining princi-
ples from discrete tomography and compressive sensing. The original formulation
in [16,17] uses L2-norm as the data-ﬁt term, which comes from the assumption
that noise in data is additive Gaussian. This is however not always the case, e.g.,
in HAADF-STEM tomography [8] the noise in data is predominantly Poisson
distributed, especially under very low exposure (electron dose) [10].
In the following, we will ﬁrst formulate TVR-DART in an abstract manner
using the language of functional analysis. Next, this abstract version is imple-
mented in ODL/ASTRA using the operator calculus in ODL. Thereby, we can
encapsulate all application-speciﬁc parts and use high-level ODL solvers that
usually expect an operator as argument. In this way, the same problem can be
solved with diﬀerent methods by simply calling diﬀerent solvers. When a new
solver or application-speciﬁc code is needed, it needs to be written only once
at one place, and can be tested separately. At the lower level of ODL, eﬃcient
forward and backward projections are performed using GPU accelerated code
provided by the ASTRA toolbox.
In summary, we oﬀer a generic, yet adaptable, version of TVR-DART with
a plug-and-play structure that allows one to change the forward operator and
the noise model. We demonstrate this ﬂexibility by switching the data-ﬁt term
to one that better matches data with Poisson noise, which is more appropriate
for tomographic data under low dose conditions.
5
ODL Implementation of TVR-DART
Bearing in mind the functional analytic viewpoint in ODL, our starting point is
to formulate the TVR-DART scheme in an abstract setting (Sect. 5.1, Eq. (7)).
In the following four sections (Sects. 5.2, 5.3, 5.4 and 5.5) we show how the
abstract TVR-DART scheme is implemented using the ODL operator calculus
with ASTRA as computational backend for computing projections and corre-
sponding backprojections. In order to emphasis the important points, and due

114
A. Ringh et al.
to space limitations, we have left out some of the code indicated with “[...]”.
The full source code is available at http://github.com/aringh/TVR-DART. We
conclude by showing some reconstructions. Note however that the goal of the
paper is not to evaluate TVR-DART, for that see [17]. It is to show the ﬂexibility
in using ODL/ASTRA as a prototyping tool. Here TVR-DART merely severs
as an example of a complex reconstruction method.
5.1
Abstract Formulation
The key assumption in TVR-DART is that the image we seek consists of n gray-
scale levels that are separated by a narrow, but smooth, transition layer. Thus,
we introduce a (parametrized) segmentation operator T : X × Θ →X that acts
as a kind of segmentation map. It is here given as
T (f, θ)(x) =
n−1

i=1
(ρi −ρi−1)uki

f(x) −τi

for x ∈Ω and θ ∈Θ.
(4)
The parameter space Θ := (R × R × R)n deﬁnes the transition characteristics of
the n layers (the background ρ0 is often set to 0). Concretely, θ = (θ1, . . . , θn) ∈
Θ with θi := (ρi, τi, Ki) where ρi is the gray-scale level of the i:th level, τi is the
mid-point gray-scale value, ki := Ki/(ρi −ρi−1) is the sharpness of the smooth
gray-scale transition, and u: R →[0, 1] is the logistic function that models the
transition itself
uk(s) :=
1
1 + e−2ks
for s ∈R.
(5)
The TVR-DART algorithm for solving (1) is now deﬁned as a method that yields
a minimizer to
min
f∈X, θ∈Θ

L

[A ◦T ](f, θ), g

+ λ[S ◦T ](f, θ)
	
.
(6)
In the above, L: Y × Y →R+ is an appropriate data-ﬁt term and S : X →
R+ is the regularization. The variant considered in [17] uses a data-ﬁt term
L( ·, g) = ∥· −g∥2
2 and a regularizing functional TVε = [Hε ◦∇], where Hε is
the Huber norm and ∇is the spatial gradient operator. The Huber norm is a
smooth surrogate functional for the L1-norm, and Hε ◦∇is thus a smoothed
version of TV. Hence, (6) becomes
min
f∈X,θ∈Θ


[A ◦T ](f, θ) −g


2
2 + λ[Hε ◦∇◦T ](f, θ)
	
.
(7)
Gradient based methods can be used to solve (7) since its objective functional is
smooth. In [17] one such solution method was presented, based on an alternating
optimization over f and θ. We take a similar approach here, and to this end deﬁne
the two operator T θ : X →X, deﬁned by T θ(f) = T (f, θ), and T f : Θ →X,
deﬁned T f(θ) = T (f, θ), where θ and f are seen as ﬁx parameters, respectively.
In the current implementation we view the sharpness parameter as ﬁxed, but
optimize over gray-scale value and mid-point.

High-Level Algorithm Prototyping
115
5.2
Deﬁning the Inverse Problem
We begin by deﬁning the reconstruction space X = L2(Ω) assuming digitization
by uniform sampling in Ω with 320 × 320 pixels:
X = odl . uniform discr ( min pt =[−200,
−200] ,
max pt =[200 ,
200] ,
shape =[320 ,
320])
Next is to deﬁne the forward operator as the ray transform A : X →Y in (2).
M angle part = odl . u n i f o r m p a r t i t i o n (0 ,
np . pi ,
18 ,
nodes on bdry=
True )
M detector part = odl . u n i f o r m p a r t i t i o n ( −200 ,
200 ,
500)
M = odl . tomo . Parallel2dGeometry ( M angle part ,
M detector part )
A = odl . tomo . RayTransform (X, M,
impl=’ astra cuda ’ )
Note that there is no need to explicitly specify the range Y of the ray trans-
form A, which are functions deﬁned on M (data manifold). Y is given indirectly
by the geometry-object, which deﬁnes M through M angle part for the angu-
lar sampling of the lines and M detector part for the detector sampling. This
information is typically provided by the experimental setup.
5.3
Deﬁning the Objective Functional
To deﬁne the objective functional in (7), we begin by setting up the soft seg-
mentation operator T θ : X →X (see Sect. 5.5 for its implementation):
T theta = SoftSegmentationOperator (X,
base value ,
thresholds ,
values ,
sharpness )
The regularization term R : X →X in (7), when optimizing over f, is given
by Rθ := Hε ◦∇◦T θ, which can be implemented using the operator calculus in
ODL:
gradient = odl . Gradient (X)
gradient = odl . PointwiseNorm ( gradient . range ) ∗
gradient
H = HuberNorm(X,
0.0001)
R theta = H ∗
gradient
∗T theta
In the above, PointwiseNorm is used to deﬁne the isotropic TV-like term and a
description of how to implement the Huber norm is given in Sect. 5.5. Next the
data-ﬁt term in (7) as a function f →


[A ◦T θ](f) −g


2
2 is implemented as
l2 norm = odl . s o l v e r s . L2NormSquared (A. range )
l2 norm = l2 norm . translated ( data )
d a t a f i t t h e t a = l2 norm ∗A ∗T theta
The l2 norm.translated(data) command shifts the origin of the L2-norm
functional, i.e., it changes the functional ∥·∥2
2 into ∥·−g∥2
2. Hence, the complete
objective functional in (7), when optimizing over f, can be assembled as

116
A. Ringh et al.
obj theat = d a t a f i t t h e t a + reg param ∗R theta
The implementation for optimizing over θ is analogous.
5.4
Solving the Optimization Problem
Since the objective functional in (7), both when seen as a functional over f and
over θ, is smooth we can use a smooth solver such as limited-memory BFGS [6,
Sect. 13.5] with backtracking line-search [6, Sect. 11.5] in the alternating opti-
mization. A BFGS solver and backtracking line-search is built into ODL, and
the alternating optimization can thus be implemented as follows.
reco = fbp
theta = t h e t a i n i t
f o r
i
in
range (10) :
[ . . . ]
l i n e s e a r c h = odl . s o l v e r s . BacktrackingLineSearch ( obj theta )
odl . s o l v e r s . bfgs method ( f=obj theta ,
x=reco ,
l i n e s e a r c h=
linesearch ,
maxiter =10,
t o l=1e −8, num store=10)
[ . . . ]
l i n e s e a r c h = odl . s o l v e r s . BacktrackingLineSearch ( o b j f )
odl . s o l v e r s . bfgs method ( f=obj f ,
x=theta ,
l i n e s e a r c h=
linesearch ,
maxiter =2,
t o l=1e −8, num store=2)
The command x=reco speciﬁes the initial iterate for the BFGS solver, and in
the ﬁrst outer iteration is taken as a reconstruction fbp obtained from standard
ﬁltered backprojection (FBP) using a Hann ﬁlter. Example reconstructions using
this algorithm are shown later in Fig. 1.
5.5
Implementing the Huber Norm and Soft Segmentation
Operator
The Huber norm and soft segmentation operator are not part of ODL and need to
be added. They are implemented as an ODL Functional and Operator object,
respectively.
Starting with the Huber norm, its mathematical deﬁnition is
Hε(f) =

Ω
fε(x)dx
where
fε(x) =
⎧
⎨
⎩
|f(x)| −ε
2
if |f(x)| ≥ε
f(x)2
2ε
if |f(x)| < ε.
In the ODL implementation below we uses q part to denote the quadratic region,
i.e., where |f(x)| < ε.

High-Level Algorithm Prototyping
117
c l a s s
HuberNorm( Functional ) :
[ . . . ]
def
c a l l ( s e l f ,
f ) :
””” Evaluating
the
f u n c t i o n a l . ”””
q part = f . ufuncs . absolute () . asarray () < s e l f . e p s i l o n
q part = np . f l o a t 3 2 ( q part )
f e p s = (( f
∗
q part ) ∗∗2 /
( 2 . 0
∗
s e l f . e p s i l o n ) +
( f . ufuncs . absolute () −s e l f . e p s i l o n
/
2 . 0 )
∗
(1−q part ) )
# This
l i n e
takes
the
inner
product
with
the
one−function .
return
f e p s . inner ( s e l f . domain . one () )
Since we use a smooth solver, we also need to provide the gradient associated
with the Huber norm, which is an element ∇Hε(f) ∈X that satisﬁes
H′
ε(f)(h) = ⟨∇Hε(f), h⟩X.
In the above, the bounded linear operator H′
ε(f) : X →R is the Fr´echet deriv-
ative of Hε at f. For the Huber norm, the gradient at f ∈X is
∇Hε(f)(x) = ∂
∂f fε(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
if f(x) ≥ε
−1
if f(x) ≤−ε
f(x)
ε
if |f(x)| < ε.
The above is implemented in ODL as a property of the Huber norm functional:
@property
def
gradient ( s e l f ) :
””” Gradient
operator
of
the
f u n c t i o n a l . ”””
func = s e l f
c l a s s
HuberNormGradient ( Operator ) :
[ . . . ]
def
c a l l ( s e l f ,
f ) :
q part = f . ufuncs . absolute () . asarray () < func . e p s i l o n
q part = np . f l o a t 3 2 ( q part )
f e p s d i f f = (( f
∗q part ) / ( func . e p s i l o n ) +
f . ufuncs . sign ()
∗(1−q part ) )
return
f e p s d i f f
return
HuberNormGradient ()
The soft segmentation operator T θ : X →X, implicitly deﬁned in (4), can
be implemented in a similar way. Below we compute the Fr´echet derivative with
respect to f (since this operator is not a functional, it does not have a gradient)
that can be implemented in ODL as a property. In ODL the derivative of T θ
with respect to the function, at f ∈X, is itself a linear operator T ′
θ(f) : X →X
given by
T ′
θ(f)(h)(x) =
n−1

i=1
h(x)(ρi −ρi−1)u′
ki

f(x) −τi

for h ∈X,

118
A. Ringh et al.
where u′
k is the derivative of the logistic function (5)
u′
k(t) =
2ke−2kt
(1 + e−2kt)2 .
The operator T f : Θ →X is implemented analogously, where the Fr´echet deriv-
ative of T f with respect to θ can be derived similarly.
5.6
Extension to Handle Data with Poisson Noise
It is well-known that minimizing the Kullback-Leibler (KL) divergence of count
data is equivalent to maximum likelihood estimation when the noise in data
g is Poisson distributed [5]. The original notion of KL divergence comes from
information theory, and is thus deﬁned for probability measures. The one used
in inverse problems is the generalization below to nonnegative functions:
DKL(g | h) =
⎧
⎨
⎩

Ω

g(y) log
 g(y)
h(y)

+ h(y) −g(y)

dy
g(y) ≥0, h(y) > 0
+∞
else.
(8)
Noise in low count data is often better modeled using a Poisson distribution
rather than an additive Gaussian distribution, and many electron tomography
applications are low count data [10]. Hence, using (8) as data-ﬁt term in (6), i.e.,
L([A ◦T θ](f), g) = DKL

g | [A ◦T θ](f)

,
is of interest to applications where the TVR-DART algorithm will be used. Since
the KL divergence is already available as a functional in ODL, in order to use
KL instead of L2 we only need to change the data-ﬁt functional:
kl = odl . s o l v e r s . KullbackLeibler (A. range ,
p r i o r=data )
d a t a f i t t h e t a = kl
∗A ∗T theta
5.7
Reconstructions
The resulting reconstructions from running the TVR-DART is summarized in
Fig. 1, which compares TVR-DART and TV reconstructions, both approaches
using L2-norm and the KL as data-ﬁt term, the former more suitable for data
with additive Gaussian noise and the latter more suitable for data with Poisson
noise. Tomographic data used for the tests is simulated using both Gaussian
and Poisson noise, and for both TV and TVR-DART we have used an FBP
reconstruction as an initial starting iterate. In Fig. 1 we also give some ﬁgure of
merits for the reconstructions, namely Relative Mean Error (RME) (see, e.g.,
[17, p. 460]) and Structural Similarity index (SSIM) [15].

High-Level Algorithm Prototyping
119
(a) Phantom
(b) Filtered backprojection.
RME: 1.96, and SSIM: 0.111.
(c) Phantom
(d) Filtered backprojection.
RME: 0.504, and SSIM: 0.303.
(e) TV with L2, λ = 3.
RME: 0.184, and SSIM: 0.915.
(f) TVR-DART with L2, λ = 30.
RME: 0.370, and SSIM: 0.772.
(g) TV with L2, λ = 10.
RME: 0.109, and SSIM: 0.907.
(h) TVR-DART with L2, λ = 70.
RME: 0.101, and SSIM: 0.919.
(i) TV with KL, λ = 0.07.
RME: 0.420, and SSIM: 0.829.
(j) TVR-DART with KL, λ = 1.
RME: 0.529, and SSIM: 0.800.
(k) TV with KL, λ = 0.1.
RME: 0.141, and SSIM: 0.894.
(l) TVR-DART with KL, λ = 1.
RME: 0.133, and SSIM: 0.899.
Fig. 1. Phantoms are shown in 1a and 1c. Data is generated from the phantoms and then perturbed by noise. In 1b and 1d, FBP
reconstructions from the data with Poisson noise are shown. The reconstructions in the second row, 1e through 1h, are from data with
white Gaussian noise, and the reconstructions in the third row, 1i through 1l, are from data with Poisson noise. The images are of size
320 × 320, and data is acquired from a parallel beam geometry, with 18 equidistant angles between 0 and π and with 500 discretization
points on the detector. For data with white Gaussian noise, the noise has a norm that is 5% of the norm of data, and for data with
Poisson noise, the data is the outcome of a Poisson distributed variable with parameter given by the noise-free data.

120
A. Ringh et al.
6
Conclusions
We have shown how TVR-DART can be implemented in ODL/ASTRA, and
utilizing the modularity and ﬂexibility of ODL we extended the algorithm by
changing the data-ﬁt functional L in (6). In the same way, it is straightforward to
change to forward operator A in (6) in order to use the algorithm in other imag-
ing modalities. As an example, the ODL implementation of TVR-DART can be
applied to magnetic resonance imaging by merely changing the forward opera-
tor A to the Fourier transform instead of the ray transform. To conclude, the
combination of ODL and ASTRA allows users to specify advanced tomographic
reconstruction methods using a high-level mathematical description that facili-
tates rapid prototyping.
References
1. Adler, J., Kohr, H., ¨Oktem, O.: ODL - a Python framework for rapid prototyping
in inverse problems. Royal Institute of Technology (2017) (in Preparation)
2. Alpers, A., Poulsen, H.F., Knudsen, E., Herman, G.T.: A discrete tomography
algorithm for improving the quality of three-dimensional X-ray diﬀraction grain
maps. J. Appl. Crystallogr. 39(4), 582–588 (2006)
3. Batenburg, K.J.: A network ﬂow algorithm for reconstructing binary images from
continuous X-rays. J. Math. Imaging Vis. 30(3), 231–248 (2008)
4. Batenburg, K.J., Sijbers, J.: DART: a practical reconstruction algorithm for dis-
crete tomography. IEEE Trans. Image Process. 20(9), 2542–2553 (2011)
5. Bertero, M., Lant´eri, H., Zanni, L.: Iterative image reconstruction: a point of view.
Math. Methods Biomed. Imaging Intensity-Modulated Radiat. Ther. (IMRT) 7,
37–63 (2008)
6. Griva, I., Nash, S.G., Sofer, A.: Linear and Nonlinear Optimization, 2nd edn. SIAM
(2009)
7. Liao, H.Y., Herman, G.T.: A coordinate ascent approach to tomographic recon-
struction of label images from a few projections. Disc. Appl. Math. 151(1), 184–197
(2005)
8. Midgley, P.A., Dunin-Borkowski, R.E.: Electron tomography and holography in
materials science. Nat. Mater. 8(4), 271 (2009)
9. Natterer, F., W¨ubbeling, F.: Mathematical Methods in Image Reconstruction.
SIAM (2001)
10. ¨Oktem, O.: Mathematics of electron tomography. In: Scherzer, O. (ed.) Handbook
of Mathematical Methods in Imaging, pp. 937–1031. Springer, New York (2015)
11. Palenstijn, W.J., Batenburg, K.J., Sijbers, J.: Performance improvements for iter-
ative electron tomography reconstruction using graphics processing units (GPUs).
J. Struct. Biol. 176(2), 250–253 (2011)
12. Sch¨ule, T., Schn¨orr, C., Weber, S., Hornegger, J.: Discrete tomography by convex-
concave regularization and DC programming. Disc. Appl. Math. 151(1), 229–243
(2005)
13. van Aarle, W., Palenstijn, W.J., Cant, J., Janssens, E., Bleichrodt, F., Dabravolski,
A., De Beenhouwer, J., Batenburg, K.J., Sijbers, J.: Fast and ﬂexible X-ray tomog-
raphy using the astra toolbox. Opt. Express 24(22), 25129–25147 (2016)

High-Level Algorithm Prototyping
121
14. van Aarle, W., Palenstijn, W.J., De Beenhouwer, J., Altantzis, T., Bals, S.,
Batenburg, K.J., Sijbers, J.: The ASTRA Toolbox: A platform for advanced algo-
rithm development in electron tomography. Ultramicroscopy 157, 35–47 (2015)
15. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13(4),
600–612 (2004)
16. Zhuge, X., Jinnai, H., Dunin-Borkowski, R.E., Migunov, V., Bals, S., Cool, P.,
Bons, A.J., Batenburg, K.J.: Automated discrete electron tomography - towards
routine high-ﬁdelity reconstruction of nanomaterials. Ultramicroscopy 175, 87–96
(2017)
17. Zhuge, X., Palenstijn, W.J., Batenburg, K.J.: TVR-DART: a more robust algo-
rithm for discrete tomography from limited projection data with automated gray
value estimation. IEEE Trans. Image Process. 25(1), 455–468 (2016)

A Parametric Level-Set Method for Partially
Discrete Tomography
Ajinkya Kadu1(B), Tristan van Leeuwen1, and K. Joost Batenburg2
1 Mathematical Institute, Utrecht University, Utrecht, The Netherlands
a.a.kadu@uu.nl
2 Centrum Wiskunde & Informatica, Amsterdam, The Netherlands
Abstract. This paper introduces a parametric level-set method for
tomographic reconstruction of partially discrete images. Such images
consist of a continuously varying background and an anomaly with a
constant (known) grey-value. We express the geometry of the anomaly
using a level-set function, which we represent using radial basis func-
tions. We pose the reconstruction problem as a bi-level optimization
problem in terms of the background and coeﬃcients for the level-set func-
tion. To constrain the background reconstruction, we impose smoothness
through Tikhonov regularization. The bi-level optimization problem is
solved in an alternating fashion; in each iteration we ﬁrst reconstruct
the background and consequently update the level-set function. We test
our method on numerical phantoms and show that we can successfully
reconstruct the geometry of the anomaly, even from limited data. On
these phantoms, our method outperforms Total Variation reconstruc-
tion, DART and P-DART.
Keywords: Discrete tomography · Level-set method · Model splitting ·
Geometric inversion
1
Introduction
The need to reconstruct (quantitative) images of an object from tomographic
measurements appears in many applications. At the heart of many of these appli-
cations is a projection model based on the Radon transform. Characterizing the
object under investigation by a function u(x) with x ∈D = [0, 1]2, tomographic
measurements are modeled as
pi =

D
u(x)δ(si −n(θi) · x) dx,
(1)
where si ∈[0, 1] denotes the shift, θi ∈[0, 2π) denotes the angle and n(θ) =
(cos θ, sin θ). The goal is to retrieve u from a number, m, of such measurements
for various shifts and directions.
If the shifts and angles are regularly and densely sampled, the transform can
be inverted directly by Filtered back-projection or Fourier reconstruction [9].
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 122–134, 2017.
DOI: 10.1007/978-3-319-66272-5 11

A Parametric Level-Set Method for Partially Discrete Tomography
123
A common approach for dealing with non-regularly sampled or missing data, is
algebraic reconstruction. Here, we express u in terms of a basis
u(x) =
n

j=1
ujb(x −xj),
where b are piece-wise polynomial basis functions and {xj}n
j=1 is a regular (pixel)
grid. This leads to a set of m linear equations in n unknowns
p = Wu,
with wij =

D b(x −xj)δ(si −n(θi) · x) dx. Due to noise in the data or errors
in the projection model the system of equations is typically inconsistent, so
a solution may not exist. Furthermore, there may be many solutions that ﬁt
the observations equally well when the system is underdetermined. A standard
approach to mitigate these issues is to formulate a regularized least-squares
problem
min
u
1
2∥Wu −p∥2
2 + λ
2 ∥Ru∥2
2,
where R is the regularization operator with parameter λ balancing the data-
misﬁt and regularity of the solution. Such a formulation is popular mainly
because very eﬃcient algorithms exist for solving it. Depending on the choice of
R, however, this formulation forces the solution to have certain properties which
may not reﬂect the truth. For example, setting R to be the discrete Laplace
operator will produce a smooth reconstruction, whereas setting R to be the
identity matrix forces the individual coeﬃcients ui to be small. In many appli-
cations such quadratic regularization terms do not reﬂect the characteristics of
the object we are reconstructing. For example, if we expect u to be piecewise
constant, we could use a Total Variation regularization term ∥Ru∥1 where R is
a discrete gradient operator [13]. Recently, a lot of progress has been made in
developing eﬃcient algorithms for solving such non-smooth optimization prob-
lems [6]. If the object under investigation is known to consist of only two distinct
materials, the regularization can be formulated in terms of a non-convex con-
straint u ∈{u0, u1}n. The latter leads to a combinatorial optimization problem,
solutions to which can be approximated using heuristic algorithms [3].
In this paper, we consider tomographic reconstruction of partially discrete
objects that consist of a region of constant density embedded in a continuously
varying background. In this case, neither the quadratic, Total Variation nor non-
convex constraints by themselves are suitable. We therefore propose the following
parametrization
u(x) =
 u1
if x ∈Ω,
u0(x) otherwise.
The inverse problem now consists of ﬁnding u0(x), u1 and the set Ω. We can sub-
sequently apply suitable regularization to u0 separately. To formulate a tractable
optimization algorithm, we represent the set Ω using a level-set function φ(x)
as follows:
Ω = {x | φ(x) > 0}.

124
A. Kadu et al.
In the following sections, we assume knowledge of u1 and discuss how to for-
mulate a variational problem to reconstruct Ω and u0 based on a parametric
level-set representation of Ω.
The outline of the paper is as follows. In Sect. 2 we discuss the paramet-
ric level-set method and propose some practical heuristics for choosing various
parameters that occur in the formulation. A joint background-anomaly recon-
struction algorithm for partially discrete tomography is discussed in Sect. 3. The
results on a few moderately complicated numerical phantoms are presented in
Sect. 4. We provide some concluding remarks in Sect. 5.
2
Level-Set Methods
In terms of the level-set function, we can express u as
u(x) = (1 −h(φ(x)))u0(x) + h(φ(x))u1,
where h is the Heaviside function.
Level-set methods have received much attention in geometric inverse prob-
lems, interface tracking, segmentation and shape optimization. In the classical
level-set method, introduced by Sethian and Osher [11], the level-set is evolved
according to the Hamilton-Jacobi equation
∂φ
∂t + v|∇φ| = 0,
where φ(x, t) now denotes the level-set function as a time-dependent quantity for
representing the shape and v denotes the normal velocity at the boundary of the
shape. In the inverse-problems setting, the velocity v is often derived from the
gradient of the cost function with respect to the model parameter [5,7]. There
are various numerical issues associated with the numerical solution of level-set
equation, e.g. reinitialization of the level-set. We refer the interested reader to
a seminal paper in level-set methods [11] and its application to computational
tomography [10].
Instead of taking this classical level-set approach, we employ a parametric
level-set approach, ﬁrst introduced by Aghasi et al. [1]. In this method, the
level-set function is parametrized using radial basis functions (RBF)
φ(x) =
n′

j=1
αjΨ(βj∥x −χj∥2),
where Ψ(·) is a radial basis function, {αj}n′
j=1 and {χj}n′
j=1 are the amplitudes and
nodes respectively, and the parameters {βj}n′
j=1 control the widths. Introducing
the kernel matrix A(χ, β) with elements
aij = Ψ(βj∥xi −χj∥2),

A Parametric Level-Set Method for Partially Discrete Tomography
125
we can now express u on the computational grid {xi}n
i=1 as
u = (1 −h(A(χ, β)α)) ⊙u0 + h(A(χ, β)α)u1,
(2)
where h is applied element-wise to the vector A(χ, β)α and ⊙denotes the
element-wise (Hadamard) product. By choosing the parameters (χ, β, α) appro-
priately we can represent any (smooth) shape. To simplify matters and make the
resulting optimization problem more tractable, we consider a ﬁxed regular grid
{χj}n′
j=1 and a ﬁxed width βj ≡β. In the following we choose β in accordance
with the grid spacing Δχ as β = 1/(ηΔχ), where η corresponds to the width of
the RBF in grid points.
Example. To illustrate the representation of a shape using ﬁnitely many radial
basis functions, we consider the green shape shown in Fig. 1(a). Starting from
an initial guess (Fig. 1(a), red) we obtain the coeﬃcients α by solving a non-
linear least-squares problem minα ∥h(Aα)−y∥2
2, where y ∈{0, 1}n indicates the
true shape. This leads to the representation shown in Fig. 1(b). With n′ = 196
RBFs, it is possible to reconstruct a smooth shape discretized on a grid with
n = 256 × 256 pixels.
(a)
(b)
Fig. 1. Any (suﬃciently) smooth level-set can be reconstructed from radial basis func-
tions. (a) The shape to be reconstructed is denoted in green. The initial shape (dash-
dotted line) is generated by some positive RBF coeﬃcients (denoted by red pluses)
near the center and negative coeﬃcients elsewhere (denoted by blue dots). Also shown
is the corresponding initial level-set function. The reconstructed shape denoted with a
dash-dotted line, the sign of the RBF-coeﬃcients as well as the corresponding level-set
function are shown in (b). (Color ﬁgure online)
Finally, the discretized reconstruction problem for determining the shape is
now formulated as
min
α

f(α) = ∥W[(u11 −u0) ⊙hϵ(Aα)] −(p −Wu0)∥2
2

,
(3)
where hϵ is a smooth approximation of the Heaviside function. The gradient and
Gauss-Newton Hessian of f(α) are given by
∇f(α) = AT DT
αW T r(α),
HGN(f(α)) = AT DT
αW T WDαA,
(4)

126
A. Kadu et al.
where the diagonal matrix and residual vectors are given by
Dα = diag((u11 −u0) ⊙h′
ϵ(Aα)),
r(α) = W[(u11 −u0) ⊙hϵ(Aα)] −(p −Wu0).
Using a Gauss-Newton method, the level-set parameters are updated as
α(k+1) = α(k) −μk

HGN(f(α(k)))
	−1
∇f(α(k)),
(5)
where μk is a stepsize chosen to satisfy the weak Wolfe conditions [15] and α(0)
is a given initial estimate of the shape. The weak Wolfe conditions consist of
suﬃcient decrease and curvature conditions and ensure global convergence to a
local minimum.
From Eq. (4), it can be observed that the ability to update the level-set
parameters depends on two main factors: (1) The diﬀerence between u0 and
u1, and (2) the derivative of the Heaviside function. Hence, the support and
smoothness of h′
ϵ plays a crucial role in the sensitivity. More details on the
choice of hϵ are discussed in Sect. 2.1.
Example. We demonstrate the parametric level-set method on a (binary) dis-
crete tomography problem. We consider the model described in Fig. 2(a). For a
full-angle case (0 ≤θ ≤π) with a large number of samples, Fig. 2(c) shows that
it is possible to accurately reconstruct a complex shape. The model is recon-
structed by iteratively updating α using Eq. (5).
2.1
Approximation to Heaviside Function
The update of the level-set function depends crucially on the choice of the Heav-
iside function. In Eq. (4) we see that h′
ϵ acts as a windowing function that con-
trols which part of the level-set function is updated. The windowing function
should achieve the following: (i) limit the update to a small region around the
boundary of the shape; (ii) have a uniform amplitude in the boundary region;
and (iii) guarantee a minimum width of the boundary region. Failure to meet
these requirements may result in poor updates for the level-set parameter α and
premature break-down of the algorithm.
Requirement (i) is easily fulﬁlled as any smooth approximation of the Heav-
iside will have a rapidly decaying derivative. To satisfy the second requirement
we construct the Heaviside function by smoothing the piece-wise linear function
1
2 + x
2ϵ for |x| ≤ϵ. This approximation is shown in Fig. 3 alongside two common
smooth approximations of the Heaviside. We now discuss how we can satisfy the
third requirement, starting with a formal deﬁnition of the width of the level-set
boundary layer as shown in Fig. 3(c).
Deﬁnition 1. In accordance with the compact approximation of the Heaviside
function with width ϵ, we deﬁne the minimum width of the level-set boundary
layer as Δ = minx0,x1 ∥x0 −x1∥2 such that φ(x0) = 0 and |φ(x1)| = ϵ.

A Parametric Level-Set Method for Partially Discrete Tomography
127
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 2. Parametric level-set method for Discrete tomography problem. (a) True model
(n = 256×256) (b) RBF grid (n′ = 27×27) with initial level-set denoted by green line,
positive and negative RBFs are denoted by red pluses and blue dots respectively (c)
Final level-set denoted by the green line, and the corresponding positive and negative
RBFs (d) Initial level-set function (e) level-set function after 10 iterations (f) ﬁnal
level-set function after 25 iterations. (Color ﬁgure online)
Lemma 1. For any smooth and compact approximation of the Heaviside func-
tion with ﬁnite width ϵ, the width of the level-set boundary layer, Δ, satisﬁes
Δ ≥ϵ/∥∇φ∥∞.
Proof. A Taylor series expansion of φ(x) around x0 for which φ(x0) = 0, we get
φ(x) = (x −x0)T ∇φ(ξ),
with ξ = tx0 + (1 −t)x for some t ∈[0, 1]. This leads to
|φ(x)| ≤∥x −x0∥2 · ∥∇φ(ξ)∥2 ≤∥x −x0∥2 · ∥∇φ∥∞.
Choosing x = x1 with |φ(x1)| = ϵ, we have ∥x1 −x0∥2 ≥ϵ/∥∇φ∥∞. Since this
holds for all x0, x1 we obtain the desired result.
⊓⊔
To ensure a minimum width of the boundary layer, Lemma 1 suggest to
choose ϵ proportional to ∥∇φ∥∞. For computational simplicity, we approximate
this using upper and lower bounds [8] and set:
ϵ = κ

max(φ(x)) −min(φ(x))
Δx

= κ

max(Aα) −min(Aα)
Δx

,
(6)
where κ controls the width of level-set boundary in terms of the underlying
computational grid. A small value of κ leads to the narrow boundary while big
value leads a wide boundary.

128
A. Kadu et al.
-1.5
-1
-0.5
0
0.5
1
1.5
x
0
0.2
0.4
0.6
0.8
1
h(x)
new formulation
compact
global
-1.5
-1
-0.5
0
0.5
1
1.5
x
0
0.2
0.4
0.6
0.8
1
(x)
new formulation
compact
global
(a)
(b)
(c)
Fig. 3. New formulation for approximating the Heaviside function. The Heaviside func-
tions (a) and corresponding Dirac-Delta functions (b) with ϵ = 1. Global approximation
is constructed from inverse tangent function ( 1
2(1+ 2
π arctan(π x
ϵ ))), while compact one
is composed of linear and sinusoid functions. (c) level-set boundary (orange region)
around zero level-set denoted by blue line, n represents the normal direction at x0.
(Color ﬁgure online)
3
Joint Reconstruction Algorithm
Reconstructing both the shape and the background parameter can be cast as a
bi-level optimization problem
min
u0,α

f(α, u0) := 1
2∥W[(1 −h(Aα))u0 + h(Aα)u1] −p∥2
2 + λ
2 ∥Lu0∥2
2

,
(7)
where L is of form [LT
x
LT
y ]T where Lx and Ly is the second-order ﬁnite-
diﬀerence operators in the x and y direction, respectively. This optimization
problem is separable; it is quadratic in u0 and non-linear in α. In order to
exploit the fact that the problem has a closed-form solution in u0 for each α,
we introduce a reduced objective
f(α) = min
u0 f(α, u0).
The gradient and Hessian of this reduced objective are given by
∇f(α) = ∇αf(α, u0),
(8)
∇2f(α) = ∇2
αf −∇2
α,u0f

∇2
u0f
−1 ∇2
α,u0f,
(9)
where u0 = argminu0 f(α, u0) [2].
Using a modiﬁed Gauss-Newton algorithm to ﬁnd a minimizer of f, it leads
to the following alternating algorithm
u(k+1)
0
= arg min
u0
f(α(k), u0)
(10)
α(k+1) = α(k) −μk

HGN(f(α(k)))
	−1
∇αf(α(k), u(k+1)
0
),
(11)
where the expressions for the gradient and Gauss-Newton Hessian are given
by (4). Convergence of this alternating approach to a local minimum of (7) is
guaranteed as long as the step-length satisﬁes the strong Wolfe conditions [15].

A Parametric Level-Set Method for Partially Discrete Tomography
129
The reconstruction algorithm based on this iterative scheme is presented in
Algorithm 1. We use the LSQR method in step 3, with a pre-deﬁned maximum
number of iterations (typically 200) and a tolerance value. A trust-region method
is applied to compute α(k+1) in step 4 restricting the conjugate gradient to only
10 iterations. We perform a total of K = 50 iterations to reconstruct the model.
Algorithm 1. Joint reconstruction algorithm
Require: p - data, W - forward modeling operator, u1 - anomaly property, A - RBF
Kernel matrix, α0 - initial RBF weights, κ - Heaviside parameter
Ensure: αK - ﬁnal weights, u - corresponding model
1: for k = 0 to K −1 do
2:
compute Heaviside ϵ from Equation (6)
3:
compute background parameter u(k+1)
0
by solving Eq. (10)
4:
compute level-set parameter α(k+1) from Eq. (11)
5: end for
6: compute u from Eq. (2).
4
Numerical Experiments
The numerical experiments are performed on 4 phantoms shown in Fig. 4. We
scale the phantoms such that u1 = 1. For the ﬁrst two phantoms, the background
varies from 0 to 0.5, while for the next two, it varies from 0 to 0.8. In order to
avoid the inverse crime, we use two diﬀerent discretization schemes for Eq. (1)
(namely, line kernel [4] for data generation, and Joseph kernel [4] for forward
modeling). We use ASTRA toolbox to compute the forward and backward pro-
jections [4]. First, we show the results on the full-view data and later we compare
various methods on a limited-angle case.
For the parametric level-set method, we use Wendland compactly supported
radial basis functions [8]. The RBF nodes are placed on a 5 times coarser grid
than the model grid, with an extension of two points outside the model grid
to avoid boundary eﬀects. To constrain the initial level-set boundary to 4 grid-
points, the Heaviside width parameter κ is set to be 0.01.
(a) Model A
(b) Model B
(c) Model C
(d) Model D
Fig. 4. Phantoms for Simulations. All the models have resolution of 256 × 256 pixels.

130
A. Kadu et al.
The level-set parameter α is optimized using the fminunc package (trust-
region algorithm) in Matlab. A total of 50 iterations are performed for predict-
ing the α, while 200 iterations are performed for predicting u0(x) using LSQR
at each step.
4.1
Regularization Parameter Selection
The reconstruction with the proposed algorithm is inﬂuenced by the regulariza-
tion parameter λ (cf. (7)). In general, there are various strategies to choose this
parameter, e.g., [14]. As our problem formulation is non-linear, many of these
strategies do not apply. Instead we analyze the inﬂuence of the regularization
parameter numerically as follows.
We deﬁne two measures (in the least-squares sense) to quantify the resid-
uals: the data residual (DR), which determines the ﬁt between the true data
and reconstructed data, and the model residual (MR), which determines the ﬁt
between reconstructed model and true model. Finally, we use the Jaccard index
(JI), deﬁned as a similarity coeﬃcient between two sets, to capture the error
in the reconstructed shape. In practice, one can only use the data residual to
select the regularization parameter λ. It is evident from Fig. 5 that there exists
a suﬃciently large region of λ for which the reconstructions are equally good.
Moreover, this region is easily identiﬁable from the data residual plot for various
λ values.
4.2
Full-View Test
For the full-view case, the projection data is generated on a 256 × 256 grid with
256 detectors and 180 equidistant projections (0 ≤θ ≤π). The Gaussian noise of
10 dB Signal-to-Noise ratio (SNR) is added to the data. The results on phantoms
A, B, C and D with the full-view data are shown in Fig. 6. The geometry of the
(a)
104
105
106
107
108
109
101
102
103
104
residual
 =1.83e+05 
 =6.95e+06 
 =5.46e+08 
 =3.79e+05 
  =2.07e+04
Data Residual
Model Residual
(b)λ = 1.83 × 105 (c)λ = 3.79 × 105
(d) λ = 6.95 × 106 (e) λ = 5.46 × 108
Fig. 5. Variation of residuals with regularization parameter for Tikhonov. Appropriate
region for choosing λ exists between 3.79 × 105 and 6.95 × 106. (a) behavior of DR
and MR over λ for model A with noisy limited-angle data. (b), (c), (d), (e) show
reconstructions for various λ values.

A Parametric Level-Set Method for Partially Discrete Tomography
131
anomalies in all of these reconstructed models are very close to the ground truth,
as is indicated by the Jaccard index shown above the ﬁgures. The background,
though, has been smoothened out with the Tikhonov regularization.
JI = 0.99
JI = 0.98
JI = 0.99
JI = 0.96
(λ = 2.97 × 107) (λ = 1.13 × 109) (λ = 2.97 × 107) (λ = 1.27 × 108)
Fig. 6. Full-view test: reconstructions with full-view data for the regularization para-
meter λ shown below it.
4.3
Limited-Angle Test
In this case, we generate synthetic data using only 5 projections, namely,
θ = {0, π/6, π/3, π/2, 2π/3}. We add Gaussian noise of 10 dB SNR to the syn-
thetic data. To check the performance of the proposed method, we compare it
to Total Variation method [4], DART [3] and its modiﬁed version for partially
discrete tomography, P-DART [12]. For Total Variation, we determine the shape
from the ﬁnal reconstruction via a simple segmentation step (thresholding). A
total of 200 iterations were performed with the Total Variation method and
the regularization parameter determined such that it optimally reconstructs the
shape. In DART, the background is modeled using 20 discrete grey-values for
model A and B, while 30 discrete grey-values for model C and D. True model has
been segmented per mentioned grey-values to generate data for DART. 40 DART
iterations were performed in each case. For P-DART, a total of 150 iterations
were performed.
The results on limited-angle data are presented in Fig. 7. The proposed
method is able to capture most of the ﬁne details (evident from the Jaccard
Index) in the phantoms even with the very limited data with moderate noise.
The P-DART method achieves the least amount of data residual in all the cases,
but fails to capture the complete geometry of the anomaly. The Total varia-
tion method gives surprisingly good reconstructions of the shape. However, we
obtained these results by selecting the best over a large range of regularization
parameters. The level-set method consistently gives the best reconstruction of
the shape.

132
A. Kadu et al.
Phantom
Total Variation
DART
P-DART
Proposed Method
(λ = 3.36)
(λ = 3.793 × 105)
DR = 59.91
DR = 100.36
DR = 12.8
DR = 70.2
JI = 0.92
JI = 0.49
JI = 0.76
JI = 0.96
(λ = 1.438)
(λ = 3.793 × 105)
DR = 39.39
DR = 32.8
DR = 10.9
DR = 52.6
JI = 0.78
JI = 0.4
JI = 0.69
JI = 0.91
(λ = 3.36)
(λ = 7.438 × 105)
DR = 70.35
DR = 115.28
DR = 16.54
DR = 117.56
JI = 0.92
JI = 0.19
JI = 0.39
JI = 0.95
(λ = 0.6158)
(λ = 3.793 × 105)
DR = 21.52
DR = 78.30
DR = 8.92
DR = 50.10
JI = 0.80
JI = 0.25
JI = 0.49
JI = 0.87
Fig. 7. Reconstructions with noisy limited data. The ﬁrst column shows the true mod-
els, while the last 4 columns show the reconstructions with various methods. Red dotted
line shows the contour of the segmented model in Total Variation method. Diﬀerent
measures are also shown below each reconstructed model. (Color ﬁgure online)
5
Conclusions and Discussion
We discussed a parametric level-set method for partially discrete tomography.
We model such objects as a constant-valued shape embedded in a continuously
varying background. The shape is represented using a level-set function, which
in turn is represented using radial basis functions. The reconstruction problem

A Parametric Level-Set Method for Partially Discrete Tomography
133
is posed as a bi-level optimization problem for the background and level-set
parameters. This reconstruction problem can be eﬃciently solved using a vari-
able projection approach, where the shape is iteratively updated. Each iteration
requires a full reconstruction of the background. The algorithm includes some
practical heuristics for choosing various parameters that are introduced as part
of the parametric level-set method. Numerical experiments on a few numerical
phantoms show that the proposed approach can outperform other popular meth-
ods for (partially) discrete tomography in terms of the reconstruction error. As
the proposed algorithm requires repeated full reconstructions, it is currently an
order of magnitude slower than the other methods. Future research is directed
at making the method more eﬃcient.
Acknowledgments. This work is part of the Industrial Partnership Programme
(IPP) ‘Computational sciences for energy research’ of the Foundation for Fundamental
Research on Matter (FOM), which is part of the Netherlands Organisation for Scientiﬁc
Research (NWO). This research programme is co-ﬁnanced by Shell Global Solutions
International B.V. The second and third authors are ﬁnancially supported by the NWO
as part of research programmes 613.009.032 and 639.073.506 respectively.
References
1. Aghasi, A., Kilmer, M., Miller, E.L.: Parametric level set methods for inverse prob-
lems. SIAM J. Imaging Sci. 4(2), 618–650 (2011)
2. Aravkin, A.Y., Van Leeuwen, T.: Estimating nuisance parameters in inverse prob-
lems. Inverse Prob. 28(11), 115016 (2012)
3. Batenburg, K.J., Sijbers, J.: Dart: a practical reconstruction algorithm for discrete
tomography. IEEE Trans. Image Process. 20(9), 2542–2553 (2011)
4. Bleichrodt, F., van Leeuwen, T., Palenstijn, W.J., van Aarle, W., Sijbers, J.,
Batenburg, K.J.: Easy implementation of advanced tomography algorithms using
the astra toolbox with spot operators. Numer. Algorithms 71(3), 673–697 (2016)
5. Burger, M.: A level set method for inverse problems. Inverse Prob. 17(5), 1327
(2001)
6. Chambolle, A., Pock, T.: A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. J. Math. Imaging Vis. 40(1), 120–145 (2011)
7. Dorn, O., Lesselier, D.: Level set methods for inverse scattering. Inverse Prob.
22(4), R67 (2006)
8. Kadu, A., Van Leeuwen, T., Mulder, W.A.: Salt reconstruction in full waveform
inversion with a parametric level-set method. IEEE Trans. Comput. Imaging 3(2),
305–315 (2016)
9. Kak, A.C., Slaney, M.: Principles of Computerized Tomographic Imaging. SIAM,
Philadelphia (2001)
10. Klann, E., Ramlau, R., Ring, W.: A mumford-shah level-set approach for the inver-
sion and segmentation of SPECT/CT data. Inverse Probl. Imaging 5(1), 137–166
(2011)
11. Osher, S., Fedkiw, R.: Level Set Methods and Dynamic Implicit Surfaces, vol. 153.
Springer, New York (2006). doi:10.1007/b98879
12. Roelandts, T., Batenburg, K., Biermans, E., K¨ubel, C., Bals, S., Sijbers, J.: Accu-
rate segmentation of dense nanoparticles by partially discrete electron tomography.
Ultramicroscopy 114, 96–105 (2012)

134
A. Kadu et al.
13. Sidky, E.Y., Pan, X.: Image reconstruction in circular cone-beam computed tomog-
raphy by constrained, total-variation minimization. Phys. Med. Biol. 53(17), 4777
(2008)
14. Thompson, A.M., Brown, J.C., Kay, J.W., Titterington, D.M.: A study of methods
of choosing the smoothing parameter in image restoration by regularization. IEEE
Trans. Pattern Anal. Mach. Intell. 13(4), 326–339 (1991)
15. Wright, S., Nocedal, J.: Numerical Optimization. Springer Series in Operations
Research and Financial Engineering. Springer, New York (1999). doi:10.1007/b98874

Maximal N-Ghosts and Minimal Information
Recovery from N Projected Views of an Array
Imants Svalbe and Matthew Ceko(B)
School of Physics and Astronomy, Monash University, Melbourne, Australia
{imants.svalbe,matthew.ceko}@monash.edu
Abstract. Digital data is now frequently stored privately and securely
in the “cloud”. One repository stores several diﬀerent sets of projections
of the original data. Each set is kept on a separate, remote server. The
information residing on any local server, purposefully, is insuﬃcient to
exactly reconstruct the full data. Here we ask: how much useful informa-
tion can be gleaned from one local projection set? We answer that ques-
tion by examining projection ghosts. A ghost is an assembly of signed
pixels positioned to have zero sums along chosen discrete directions. The
shape of each ghost is deﬁned uniquely by its distinct set of N direc-
tions. An N-ghost with a shape that ﬁts snuggly inside the boundary
of an array deﬁnes precisely all of the array locations that cannot be
exactly reconstructed from those N projected views. Minimal N-ghosts
contain 2N elements: one (−1/+1) pair is needed for zero-sums along
each of the N directions. Maximal N-ghosts contain 2N elements: the
number of (−1/+1) elements can double N times, once for each of the N
directions. Here we construct maximal N-ghosts that cover a large area
of their bounding array. By maximising the number of unrecoverable
ghosted pixels, we minimise the information that can be reconstructed
from N projected views. We show that at least 60% of the data in an
m × m array, for m ≈N 2/4, can be masked or made “unreadable”, for
a maximal set of N noise-free projections of the original m × m data.
Keywords: Discrete projection · Cloud storage · Data security ·
Mojette transform · Tomographic reconstruction
1
Introduction
In tomography, absorption of beams with ﬁnite width are measured passing
through a continuum of absorbing atoms. Absorption proﬁles are recorded in sep-
arate bins to be reconstructed on discrete lattices, and therefore the reconstruc-
tions must be approximated. For projections on a discrete array, each projected
ray can simply sum the intensity at each array site. On a discrete array, projec-
tion angles are not continuous. The rays follow paths that link regularly staggered
lattice sites. For a square lattice we take p steps across and q steps down to deﬁne
angle p:q, where p and q are relatively prime, giving angle θp:q = tan−1(q/p).
Sums of pixels that lie on p:q lines are known as Dirac-Mojette projections
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 135–146, 2017.
DOI: 10.1007/978-3-319-66272-5 12

136
I. Svalbe and M. Ceko
(also called X-rays). If p takes signed integer values, 0◦≤θp:q < 180◦, the set of
discrete angles native to a P ×Q pixel array can be found using the Farey-Harros
set of fractions, as FP Q [10].
An N-ghost is a set of signed elements placed in a particular pattern on a
digital array. For each of N chosen discrete angles, any parallel rays that pass
across these patterns will always intersect pairs of ghost elements that sum to
exactly zero. By design, ghosts remain invisible when viewed from any of these
N angles. Ghosts are also known as switching elements or the null set. A ghost
whose (+/−) values are scaled by a constant, or a ghost with its signs reversed,
remains a ghost. Exact reconstruction of any array of elements from N (noise-
free) projected views is possible if and only if their N-ghost extends beyond the
array boundary when the ghost is superimposed over the array. This is the Katz
criterion [12]. An N-ghost with any of its elements positioned outside the image
space cannot be superimposed on the image and also remain invisible in all N
directions. For a set of projections {pi:qi}, 1 ≤i ≤N, on a P × Q image, the
Katz criterion permits exact reconstruction of that array if and only if

|pi| ≥P or

|qi| ≥Q.
(1)
This paper considers methods to make tomographic reconstruction as diﬃcult
as possible. This inverts the usual problem of choosing sets of projection angles
to facilitate image recovery. A ghost projection contains no new information
about internal parts of an object, because the N projections of those parts, by
design, have zero sums. An N-ghost adds nothing to the content for any object
viewed in those N directions.
The aim here is to minimise the information that can be recovered from a
partial set of array projections. The motivation of this work is to improve the
security and privacy of information that is stored as partial sets of projected
data in a distributed manner across multiple servers. For example, the RozoFS
system [18], or for digital data communications via packet networks [17].
This paper is structured as follows: Sect. 2 provides a short review of the the-
ory and construction of ghost projections, including methods to make minimal
ghosts and methods for the exact reconstruction of images from discrete noise-
free projections. Section 3 presents techniques to construct maximal N-ghosts
and discusses their properties. It shows how well these ghosts mask given loca-
tions to inhibit exact array recovery. Section 4 discusses how this work might be
extended to cover a wider range of practical applications, whilst Sect. 5 provides
a brief summary and conclusion.
2
Ghost Review
2.1
Ghosts and Tomography
The notion of ghosts arose with the development of practical schemes to imple-
ment the inverse Radon transform. The central slice theorem makes it possible
to recover internal object details from the intensity proﬁles of beams transmitted

Maximal N-Ghosts and Minimal Information Recovery
137
through the object at several angles. Tomography has found rapid application in
diverse areas such as astrophysics, archaeology, biology, medical imaging, geol-
ogy, part inspection for manufacturing and security scanning.
Early work on ghosts for image reconstruction in the digital domain (using
discrete data from ﬁnite detector arrays) was presented by [12,13,15]. The non-
unique mapping between arrays and their projections, in the mathematical sense,
was ﬂagged by G.G. Lorenz as early as 1949 [14], as did related work on the
recovery of matrices from their row and column sums. It was soon realised that
the uniqueness of a reconstructed image depends on the absence of ghosts (or
switching elements). The presence of a ghost means the constraints of any ﬁnite
projection set may be equally well satisﬁed by arbitrary multiple (i.e. diﬀerently
ghosted) objects.
Recent work on ghosts has aimed to minimise the redundant information in
sets of projections. The objective was to achieve shorter acquisition times and
require fewer probing rays [5,11]. Along the way, many valuable theoretical links
have been established, such as the use of polynomials, cyclotomic equations and
the Vandermonde matrix to represent discrete projection data. A wide variety
of methods to reconstruct images from projections have been developed [1,4,16].
Work has been done to recover images from sets with several missing projections
that has resulted in several image de-ghosting algorithms [6], or to recover data
from lossy transmission of packets across communication networks [16].
2.2
Constructing N-Ghosts
An elementary ghost for a discrete angle p:q, denoted gp:q, is a pair of signed
points separated by p columns and q rows. This is deﬁned as
gp:q(i, j) =
⎧
⎪
⎨
⎪
⎩
+1
if (i, j) = (0, 0)
−1
if (i, j) = (q, p)
0
elsewhere.
(2)
A ghost G, over N directions can then be constructed through N −1 discrete
convolutions of N elementary ghosts.
G = gp1:q1 ∗... ∗gpN :qN
(3)
Geometrically, this corresponds to starting with a pair of oppositely signed
points that deﬁne a ghost in the ﬁrst chosen direction and then dilating, (add a
translated copy with reversed signs) that ghost in the second selected direction.
The result for the composite 2-ghost is then dilated in the third direction, and
so on, for N directions. Figure 1 shows this process to make the 4-ghost with
directions {1:0, 0:1, 1:1, −1:1}.
The bounding polygon of the N-ghost, given by the convex hull of ghost
points, is then comprised of 2N vectors from the angles p:q (including reﬂec-
tions). Therefore the convex hull of all N-ghosts built through convolutions of
elementary ghosts are 180◦symmetric. Ghosts deﬁned through Eq. (3) are the

138
I. Svalbe and M. Ceko
+1 −1
+1 −1
−1 +1
+1 −1 0
−1 0 +1
0 +1 −1
0 +1 −1 0
−1 0
0 +1
+1 0
0 −1
0 −1 +1 0
g1:0
g1:0 ∗g0:1
g1:0 ∗g0:1 ∗g1:1
g1:0 ∗g0:1 ∗g1:1 ∗g−1:1
Fig. 1. Construction of an N-ghost by sequential dilation. Left to right: the 1D ghost
with direction 1:0, dilated in direction 0:1, then 1:1 followed by −1:1 to form the N = 4
(minimal) ghost with 8 non-zero elements. Boxes show the location of the previous ghost
before sign reversal and shift by p:q.
minimal ghost conﬁgurations for given directions {pi:qi} [11]. It is also possible
to construct ghosts through other methods, such as using U-polygons [8]. How-
ever, in this work we consider only ghosts deﬁned by (3) as these are the ghosts
that arise as errors in Mojette reconstruction.
2.3
Reconstruction from Projections
The Dirac-Mojette projections of known, ﬁxed elements of a discrete array create
noise-free projections. Such arrays can be reconstructed exactly from a suﬃcient
set of their Mojette projections using the Corner Based Inversion (CBI) method
of [16]. This method begins by matching pixels to pairs of projection bins from
the corners of the image. These rays contain one or two summed elements. We
can iteratively subtract the un-summed pixel values from the summed bins to
progressively unpack the content of projected values, working from the array
edges into the array centre. CBI is not robust even for small, isolated levels of
noise. Any error, even at one bin location, will propagate into all subsequent
unpacked values, as studied in [2].
For a set of N sub-Katz projections, where  |pi| < P and  |qi| < Q,
the CBI method correctly unpacks as many image pixels as possible, leaving
unknown values for the remaining pixels. The unrecovered data corresponds to
the (P − |pi|)(Q − |qi|) possible translations of N-ghost pixels inside the
reconstructed image [3,7]. Figure 2 demonstrates the simplest case where the
N-ghost is the same size as the image, and therefore the errors are simply an
embedded N-ghost with scaled values.
There are also additional “shadow” pixels located along the N directions that
are blocked by a ghost element and cannot be further unpacked (see Fig. 4). All
values that lie within the convex hull of translated ghost pixels are initially
unsolved. Using ghost solving [6] or by assigning an arbitrary (hence usually
incorrect) value to one of these ghost pixels, it is possible to re-apply CBI to
remove many of the shadow pixels (Fig. 2d). The remaining unreconstructed
pixels are, at least, all of the non-zero positions of the N-ghost. We wish to
choose a set of N directions that maximises the number of these N-ghost pixels
and thus maximise the number of unrecoverable array pixels.

Maximal N-Ghosts and Minimal Information Recovery
139
0:1 84 110 83 83 105
1:0 72 58
53 81 106 95
1:1 4
27
46 54 89
48 87 48 42 20
−1:1 25 57
30 76 42
79 76 48 24 8
1:2 4
3
31 37 49
72 37 38 45 27 37 37 28 20
a)
8
19 17 24
4
5
10 18 22
3
21 11 12
2
7
23 13
1
29 15
28 16 26
6
30
20 14
9
27 25
8
0
0 24
4
0
0
0
0
3
0
0
0
0
7
23
0
0
0
0
28
0
0
0
0
20 14 0
0
25
8
10
26
24
4
14 10
18
13
3
12 11
21
2
7
23 13 −8 29 24
28 25
26
6
21
20 14
0
36 25
)
d
)
c
)
b
Fig. 2. CBI for a 5 × 6 array. (a) N = 5 projections {0:1, 1:0, 1:1, −1:1, 1:2}. (b) The
original array values. (c) The raw CBI result using the array projections. Initially only
the corner values of the array are solved. The ghost pixels for this N = 5 set are
underlined. (d) Setting (arbitrarily) the middle ghost pixel on the bottom row to 0
and re-applying CBI yields the remaining array values. Note the underlined values
at the locations of the ghost elements are all wrong (here by either +9 or −9). Here
18/30 = 60% of the array can be recovered, 40% cannot.
2.4
Minimal Ghosts
A projection ghost increases the pixel sum (by +v) of an object at one point
along a ray, whilst equally decreasing the object sum (by −v) at some other
point along the same ray, leaving the total number of pixels unchanged. Clearly
the object has been changed, but the net projected view remains the same. To
do this in N-directions requires at least N increments and N decrements, one
pair for each of the N directions.
A 2D ghost has a ﬁnite area and enclosing perimeter. Each ghost element
located on the perimeter of the ghost area must be linked to at least one other
perimeter ghost element of the opposite sign, by a line pi:qi in one of the N direc-
tions. The same must happen for the parallel ray passing through the opposite
edge of the perimeter. The polygonal boundary formed by the vertices of the
convex hull of any ghost is then symmetric and always contains at least 2N
points. A minimal N-ghost thus traces the perimeter of a symmetric polygon,
with 2N sides, made from just N (+1) and N (−1) entries. The area inside the
perimeter of a minimal ghost polygon is free of ghost elements (Fig. 3a).
Minimal ghosts in 2D only exist for N = 1, 2, 3, 4, 6 [9]. All other N-ghosts
contain additional points on their polygon interiors. Near-minimal ghosts for
N = 5 and all N > 6 contain as close as possible to 2N points. Near-minimal
ghosts are comprised of a symmetric bounding perimeter points, but they always
contain interior points, usually most of these have value ±1. Fig. 3b shows the
near-minimal case for N = 5, this is the same ghost seen in Fig. 2.

140
I. Svalbe and M. Ceko
)
c
)
b
)
a
Fig. 3. (a) A minimal ghost for N = 6 has 6 (+1) and 6 (−1) points on its boundary
and an empty interior. (b) A near-minimal ghost for N = 5 contains 12 non-zero points,
made up of 10 perimeter points and 2 points interior to the bounding polygon. (c) A
214 N = 16 point near-minimal ghost for in a 34 × 30 array, 108 (−1), 104 (+1) and 2
(+2) values, ghost volume 655.
Ghost elements that have the same sign when overlapped at any location
by the dilation process can be summed to produce “grey” ghosts, with internal
integer entries of ±2, ..., ±n. The value of n for near-minimal ghosts grows
rapidly with N [19]. Grey values where summed ghost points overlap are known
as multiple points. For grey ghosts, we can ﬁnd sets of angles that minimise the
N-ghost “volume”. Here, the volume is computed as the area of the bounding
2N-sided polygon multiplied by its height. The height can be taken as the mean
absolute grey scale of all the ±1 to ±n entries (Fig. 4a). There is experimental
evidence that ghost volumes have an asymptotic limiting density [19], i.e. that
all 2D ghosts have a minimum 3D volume. The height of a grayscale ghost can be
traded against the area of its footprint. Ghost elements that have opposite signs
at any location when they are overlapped by the dilation process will cancel and
thus can reduce the count of internal points (as shown in Fig. 3b and c).
The theoretical design and practical construction of projection sets that pro-
duce minimal ghosts is important. It helps to minimise the size and number of
projections required to meet the Katz criterion and thus permit exact image
reconstruction. Any set of N projections will fail to exactly reconstruct any
image that is larger than the size of the corresponding N-ghost. As a minimal
(or near-minimal) N-ghost contains at least 2N elements, at least 2N pixels of
any image larger than this ghost cannot be (exactly) reconstructed. As the aim
here is to maximise the number of un-reconstructed pixels, we need to build
N-ghosts that contain not as few but as many ghost elements as possible.
3
Maximal Ghosts
Minimal (and near-minimal) ghosts are important for designing tomographic
systems in which we would like to make reconstruction as easy as possible.
However, for applications such as data fragmentation for distributed cloud stor-
age, we wish to do the opposite. If any server is compromised, and insuﬃcient

Maximal N-Ghosts and Minimal Information Recovery
141
projection data is obtained for an object, the amount of data that can be recon-
structed should be minimised. Maximal ghosts contain exactly 2N ghost points,
including multiple points. Therefore, they create a maximum amount of error
in reconstructed data. These 2N points should also cover the maximum area of
their bounding array, to minimise the regions of uniqueness that can be exactly
reconstructed. Although the same amount of information is missing for any set
of projections where  |pi| and  |qi| are equal, the data that can be exactly
reconstructed without said information depends on the ghost constructed by the
set of directions.
3.1
Maximal Ghost Examples
The maximal and minimal ghosts are identical for N = 1 (for example {0:1})
and N = 2 (for example {0:1, 1:0}). Any aﬃne rotation of these ghosts is also a
ghost. For N = 3, a minimal ghost set is {0:1, 1:0, 1:1} or {0:1, 1:0, −1:1}, with
each 3-ghost having 2·3 = 6 boundary points and no interior points. In contrast,
the set {0:1, 1:1, −1:1} forms a compact 3-maximal ghost comprised of 23 = 8
points, with no holes in its interior.
We assume here that the original data is an array that ﬁts inside an approx-
imately square m × m or m × (m + 1) region, where the more valuable content
(when the data is image-like) can be reasonably expected to lie mostly across
the centre region of the array. The maximal N-ghosts are made here to have
matching size m × m, to overlap the given data as much as possible.
When the data is of text or other formats, long, thin, rectangular boxes, of
size n × m, with n ≪m may be more appropriate. The shape of the maximal
ghosts then needs to be adapted to accommodate the appropriate image shape.
Any discrete angle has four-fold symmetric angles, p:q, −p:q, q:p and −q:p, except
for the pairs {0:1, 1:0, 1:1, −1:1}. Changing the symmetry of the p:q angles that
are selected within the set N can be used to deliberately increase or shorten the
growth of a ghost preferentially along the x- or y-axis, thus changing the aspect
ratio of the part of the image they reconstruct.
The number of ghost elements double after each dilation into a new p:q
direction. We can easily achieve the maximal count of ghost elements (2N) by
choosing p and q to be large enough that the ghosts never overlap under suc-
cessive dilations. However, this approach also introduces large gaps between the
dilated patterns. These gaps will reduce the masking eﬃciency as many un-
ghosted internal pixels will be recoverable using simple CBI reconstruction. Our
reﬁned aim is then to have the maximal number of ghost elements ghost elements
closely packed, with few or no holes inside the ghost perimeter.
3.2
Constructing Maximal N-Ghosts
The projection sets {0:1, 1:0} and {1:0, 1:1, −1:1} construct maximal N-ghosts
for N = 2 and N = 3 respectively, as shown in Fig. 4. Therefore, we will use
these ghost as starting points from which we can build larger N-ghosts such that
overlapping points never cancel over successive dilation directions.

142
I. Svalbe and M. Ceko
a)
+1 −1
−1 +1
b)
0
+1 −1
0
−1 +1 −1 +1
0
+1 −1
0
Fig. 4. (a) Maximal ghost for N = 2 given by {0:1, 1:0}. (b) Maximal ghost for N = 3
given by {1:0, 1:1, −1:1}.
Notice that the pattern of positive and negative values for the N = 2 ghost
form a checkerboard. We now look for another angle, such that convolving its
elementary ghost does not result in the cancelling of any ghost points. This
corresponds to ﬁnding the translation for which placing the N = 2 ghost with
reversed signs never aligns positive with negative points. If we choose p to be
even, this occurs when q is odd. Likewise, if q is even, then p must be odd. Due
to the alignment of positive and negative ghost points over the new dilation, the
resulting N = 3 ghost is also a checkerboard pattern (excluding zero points).
Hence this can be repeated to obtain any maximal N-ghost with 2N points.
Similar reasoning can be used to construct N-ghosts starting from the set
{1:0, 1:1, −1:1}. For this set, the columns alternate between negative and pos-
itive. Therefore, to construct any maximal N-ghost starting from this set, we
must ensure the reversed, translated columns align with similar signs. As long
as p is chosen to be odd, then ghost points will never cancel. Equivalently, this
can also be performed using {0:1, 1:1, −1:1}, by choosing odd q.
For maximally ineﬃcient reconstruction, we wish to cover as many possible
pixel sites with non-zero ghost values. We have seen that the ghost from the
projection set {0:1, 1:0} covers its entire bounding array with non-zero values
due to the directions aligning with the lattice. Since for N > 2 we can no
longer choose directions that align with the grid, we must choose angles that
approximate it. Therefore we choose ±p:1 ±1:q for the largest valid p and q
possible without disconnecting ghost points. Then maximal ghosts for even N
can be constructed using symmetric discrete projection sets
SE = {0:1, 1:0, 1:2n, −1:2n, 2n:1, −2n:1}
(4)
or subsets thereof for n ∈N. Similarly, odd N maximal ghosts can be built using
the projection set
SO = {0:1, 1:1, −1:1, 1:(2n + 1), −1:(2n + 1), (2n + 1):1, −(2n + 1):1}
(5)
or any subsets with n ∈N. SE and SO in their entirety produce ghosts that are
extremely compact and dense. They also result in some ambiguity in choice of
angles when N ̸= 4n + 2 for even N, and N ̸= 4n + 3 for odd N. Therefore
we oﬀer some subsets GN as examples, to produce approximately square ghosts.
For even N, we can choose
GN = {0:1, 1:0, 1:2, 2:1, 1:4, 4:1, ..., 1:(N −2), (N −2):1}
(6)
or GN = {0:1, 1:0, 1:2, −2:1, 1:4, −4:1, ..., 1:(N −2), −(N −2):1}.
(7)

Maximal N-Ghosts and Minimal Information Recovery
143
Note (6) and (7) result in square m × m ghosts. For odd N maximal ghosts,
we choose a similar symmetric pattern that give m × (m + 1) ghosts.
GN = {0:1, 1:1, −1:1, 1:3, 3:1, 1:5, 5:1, ..., 1:(N −2), (N −2):1}
(8)
or GN = {0:1, 1:1, −1:1, 1:3, −3:1, 1:5, −5:1, ..., 1:(N −2), −(N −2):1}
(9)
The choice (6) constructs ghosts that have no holes inside the 2N point
symmetric polygon that forms the boundary of any N-ghost. All interior points
are ﬁlled by non-zero ghost elements. Using choice (7) inserts just four interior
holes, these holes always lie near the corners of the polygon. Examples of these
maximal ghosts, for N = 8 and N = 9, are shown in Fig. 5. For odd N, the
situation is similar, although the boundaries are less sharply deﬁned, with the
arrays from option (9) having 8 interior holes. As N grows, the largest angle
in the 2N-polygon perimeter closer approximates 1:0 (or 0:1). Hence the ghost
boundary will approach the array boundary, and the ﬁll will approach 100%.
a)
b)
c)
d)
Fig. 5. Maximal 8-ghosts as 17 × 17 arrays, and maximal 9-ghosts as 21 × 22 arrays
with. (a) G8 using Eq. (6), has 191 non-zero elements. (b) G8 using Eq. (7), has 205
non-zero elements, with 4 holes. (c) G9 using Eq. (8), contains 312 non-zero elements.
(d) G9 using Eq. (9), contains 322 non-zero elements, with 8 interior holes near the
corners of the array.
Tables 1 and 2 give the image sizes and percentage of the array that the N-
ghost ﬁlls for even and odd N, respectively. The size of the array can be computed
from the sum of the absolute values of p and q values in the angle set. For N even
(6), (7), this sum gives array size m = N(N −2)/4 + (N −2)/2 + 2 = (N 2 + 4).
Similarly m = (N 2 + 5)/4 for N odd (8), (9). The entry “count” in Tables 1 and
2 is the number of non-zero elements in the ghost, the ﬁll factor is the percentage
of non-zero pixels in the full array. The pair of rows in the table at each N show
results using choice (6) then (7) for even N, and (8) then (9) for odd N.
To maintain closely spaced elements with few holes, the overlap of ghost
elements will also increase the grey level of the ghost. The range of greys in
a ghost must be less than the grey values in the image. The sum of the ghost
values is always zero. The sum of the absolute values on the ghost array is 2N as
elements at each location accumulate from the overlap of N successive dilations.

144
I. Svalbe and M. Ceko
Table 1. N-ghost array sizes, redun-
dancy and ﬁll factors for even N = 4 to
N = 14.
N
Array size Range
Count Fill (%)
4 5 × 5 (6)
{−1, ..., 2}
15
60.00
(7)
{−1, ..., 1}
16
64.00
6 10 × 10
{−1, ..., 1}
60
60.00
{−1, ..., 1}
64
64.00
8 17 × 17
{−3, ..., 4}
191
61.09
{−4, ..., 3}
205
70.09
10 26 × 26
{−5, ..., 5}
484
71.60
{−4, ..., 4}
512
75.74
12 37 × 37
{−10, ..., 10} 1039
75.89
{−9, ..., 9}
1085
79.25
14 50 × 50
{−26, ..., 26} 1980
79.20
{−25, ..., 25} 2048
81.92
Table 2. N-ghost array sizes, redun-
dancy and ﬁll factors for odd N = 5 to
N = 15.
N
Array size Range
Count Fill (%)
5 7 × 8 (8)
{−1, ..., 1}
32
57.14
(9)
{−1, ..., 1}
32
57.14
7 13 × 14
{−2, ..., 2}
112
61.54
{−2, ..., 2}
112
61.54
9 21 × 22
{−4, ..., 4}
312
67.53
{−4, ..., 4}
322
69.70
11 31 × 32
{−7, ..., 7}
720
72.58
{−8, ..., 8}
744
75.00
13 43 × 44
{−17, ..., 17} 1448
76.53
{−18, ..., 18} 1490
78.75
15 57 × 58
{−44, ..., 44} 2632
79.61
{−43, ..., 43} 2696
81.55
3.3
Binary Maximal Ghosts
Maximal N-ghosts with an alphabet of just ±1 elements can also be constructed.
Here the 7 angles were chosen, by eye, to place successive dilations so they do not
cause any overlap between ghosts points while not creating holes. This dense set
for N = 7, p:q = {0:1, 1:0, 1:2, −1:2, −3:2, 5:2, −1:6}, has array size 16×13, where
values are all −1/+1. This array has 27 = 128 non-zero elements, achieving a
61.54% ﬁll of the array area. The angle −11:2 can be appended to this set to
construct a maximal binary 8-ghost. It can be seen in Fig. 6 that this translates
the binary 7-ghost to ﬁt with itself in a jigsaw-like fashion to produce a ﬁll of
59.26%. It is not yet known if maximal binary ghosts exist for all N.
Fig. 6. A maximal 7-ghost, p:q = {0:1, 1:0, 2:1, −2:1, 2:3, 6:1, −10:1} (left), comprised
of 128±1 points. The direction −11:2 is added to give a maximal binary 8-ghost (right).

Maximal N-Ghosts and Minimal Information Recovery
145
4
Further Work
We have, so far, only tested how these N-ghosts aﬀect array recovery using the
CBI reconstruction method. We next want to apply several statistical/iterative
inversion methods to evaluate the potential for “exact” or nearly-exact recover-
ability of arrays from “ghosted” partial sets of projection data. In many cases,
the stored data will have already been encrypted, so any inversion process is
unlikely to be aided by using structural or statistical clues from any sections of
partially recovered data. However the part played by the range of data values
needs further scrutiny, as the constraints on valid reconstructions are tighter
(even for random arrays) when the data values are drawn from a restricted
alphabet, for example for binary or ternary arrays.
The maximal N-ghost examples given in Fig. 6 was built by hand. It would
be useful to devise an algorithm to produce ghosts for any N that are comprised
of only −1/+1 values and at the same time ensure the lowest possible number
of zero elements (holes) occur inside the bounding polygon. It may be possible
to also control the location of holes, as they too can be seen as parts of a
complementary ghost structure that is enclosed by the same N angles.
When two or more partial sets of projections that are kept on separate servers
are pooled, exact and eﬃcient recovery of the exact data must still be possible.
Individual sets of N-ghosts must then be distinct, but compatible in the joint
reconstruction sense. Maximal ghosts for even N made from Eqs. (6) or (7) would
pair well with the odd ghosts for N ±1 (via Eqs. (8) or (9)), as they have just
the 0:1 projection in common and these partial sets are all similarly robust.
5
Summary
This paper considers methods to assemble sets of N discrete projections {pi:qi}
that are insuﬃcient for an exact reconstruction, and structured to yield the
least possible information about the original array. The unrecoverable pixels of
an array are shown to be the elements of the locally-held ghost for those N
directions. The geometry of elementary ghost convolutions is used to construct
maximal N-ghosts that are comprised of symmetric sets of discrete angles built
from 1:n for n < N, where n and N are either odd or even. These ghosts
maximise the fraction (f) of an array that is not able to be reconstructed from
those N projected views. In practice, we can assemble sets where f ≥60% for
N ≥4.
Acknowledgments. The School of Physics and Astronomy at Monash University,
Australia, has supported and provided funds for this work. M.C. has the support of
the Australian government’s Research Training Program (RTP) and the J.L. William
scholarship from the School of Physics and Astronomy at Monash University.

146
I. Svalbe and M. Ceko
References
1. van Aarle, W., Palenstijn, W.J., De Beenhouwer, J., Altantzis, T., Bals, S.,
Batenburg, K.J., Sijbers, J.: The ASTRA toolbox: a platform for advanced algo-
rithm development in electron tomography. Ultramicroscopy 157, 35–47 (2015)
2. Alpers, A., Gritzmann, P.: On stability, error correction, and noise compensation
in discrete tomography. SIAM J. Discrete Math. 20(1), 227–239 (2006)
3. Alpers, A., Larman, D.G.: The smallest sets of points not determined by their
X-rays. Bull. Lond. Math. Soc. 47(1), 171–176 (2015)
4. Batenburg, K.J., Plantagie, L.: Fast approximation of algebraic reconstruction
methods for tomography. IEEE Trans. Image Process. 21(8), 3648–3658 (2012)
5. Brunetti, S., Dulio, P., Hajdu, L., Peri, C.: Ghosts in discrete tomography. J. Math.
Imaging Vis. 53(2), 210–224 (2015)
6. Chandra, S., Svalbe, I., Gu´edon, J., Kingston, A., Normand, N.: Recovering missing
slices of the discrete Fourier transform using ghosts. IEEE Trans. Image Process.
21(10), 4431–4441 (2012)
7. Dulio, P., Frosini, A., Pagani, S.M.: A geometrical characterization of regions of
uniqueness and applications to discrete tomography. Inverse Probl. 31(12), 125011
(2015)
8. Dulio, P., Peri, C.: On the geometric structure of lattice U-polygons. Discrete Math.
307(19), 2330–2340 (2007)
9. Gardner, R., Gritzmann, P.: Discrete tomography: determination of ﬁnite sets by
x-rays. Trans. Am. Math. Soc. 349(6), 2271–2295 (1997)
10. Gu´edon, J.: The Mojette Transform: Theory and Applications. ISTE Wiley, New
York (2009)
11. Hajdu, L., Tijdeman, R.: Algebraic aspects of discrete tomography. Journal fur die
Reine und Angewandte Mathematik 534, 119–128 (2001)
12. Questions of Uniqueness and Resolution in Reconstruction from Projections. Lec-
ture Notes in Biomathematics, vol. 26. Springer, Heidelberg (1978). doi:10.1007/
978-3-642-45507-0
13. Kuba, A.: The reconstruction of two-directionally connected binary patterns from
their two orthogonal projections. Comput. Vis. Graph. Image Process. 27(3), 249–
265 (1984)
14. Lorentz, G.: A problem of plane measure. Am. J. Math. 71(2), 417–426 (1949)
15. Louis, A., T¨ornig, W.: Ghosts in tomography-the null space of the Radon trans-
form. Math. Methods Appl. Sci. 3(1), 1–10 (1981)
16. Normand, N., Kingston, A., ´Evenou, P.: A geometry driven reconstruction algo-
rithm for the Mojette transform. In: Kuba, A., Ny´ul, L.G., Pal´agyi, K. (eds.)
DGCI 2006. LNCS, vol. 4245, pp. 122–133. Springer, Heidelberg (2006). doi:10.
1007/11907350 11
17. Normand, N., Svalbe, I., Parrein, B., Kingston, A.: Erasure coding with the
ﬁnite Radon transform. In: Wireless Communications and Networking Conference
(WCNC), pp. 1–6. IEEE (2010)
18. Pertin, D.: Mojette erasure code for distributed storage (Ph.D Thesis). University
of Nantes (2016)
19. Svalbe, I., Chandra, S.: Growth of discrete projection ghosts created by iteration.
In: Debled-Rennesson, I., Domenjoud, E., Kerautret, B., Even, P. (eds.) DGCI
2011. LNCS, vol. 6607, pp. 406–416. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-19867-0 34

Ambiguity Results in the Characterization
of hv-convex Polyominoes from Projections
Elena Barcucci1, Paolo Dulio2, Andrea Frosini1(B), and Simone Rinaldi3
1 Dipartimento di Matematica e Informatica “U. Dini”, Universit`a di Firenze,
viale Morgagni 65, 50134 Florence, Italy
{elena.barcucci,andrea.frosini}@unifi.it
2 Dipartimento di Matematica “F. Brioschi”, Politecnico di Milano,
Piazza Leonardo da Vinci 32, 20133 Milan, Italy
paolo.dulio@polimi.it
3 Dipartimento di Ingegneria dell’Informazione e Scienze Matematiche,
Universit`a di Siena, Via Roma, 56, 53100 Siena, Italy
rinaldi@unisi.it
Abstract. In 1997 R. Gardner and P. Gritzmann proved a milestone
result for uniqueness in Discrete Tomography: a ﬁnite convex discrete
set can be uniquely determined by projections taken in any set of seven
planar directions. The number of required directions can be reduced to
4, providing their cross-ratio, arranged in order of increasing angle with
the positive x-axis, does not belong to the set {4/3, 3/2, 2, 3, 4}.
Later studies, supported by experimental evidence, allow us to conjec-
ture that a similar result may also hold for the wider class of hv-convex
polyominoes.
In this paper we shed some light on the diﬀerences between these two
classes, providing new 4-tuples of discrete directions that do not lead to
a unique reconstruction of hv-convex polyominoes. We reach our main
result by a constructive process. This generates switching components
along four directions by a recursive composition of only three of them,
and then by shifting the obtained structure along the fourth one.
Furthermore, we stress the role that the horizontal and the vertical
directions have in preserving the hv-convexity property. This is pointed
out by showing that these often appear in the 4-tuples of directions that
allow uniqueness.
A ﬁnal characterization theorem for hv-convex polyominoes is still left
as open question.
Keywords: Discrete geometry · Discrete tomography · hv-convex set ·
Uniqueness problem · Switching component
1
Introduction
We consider the problem of the characterization of ﬁnite discrete set of points of
the 2D integer lattice from projections, i.e., from the knowledge of the number of
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 147–158, 2017.
DOI: 10.1007/978-3-319-66272-5 13

148
E. Barcucci et al.
points lying on parallel lines along a given set of discrete directions. This research
is of great relevance in Discrete Tomography, and it is motivated by the need of
a faithful reconstruction of the internal structure of an object that is not directly
accessible. The general problem has been studied since the beginning of the 50’s,
both from theoretical and computational perspectives (see [12] for a survey on
the topic). Due to the existence of diﬀerent sets of points that are consistent
with the projections along any set of discrete directions (in [8], Fishburn et
al. gave several characterizations of the ﬁnite subsets of Zn that are uniquely
determined by their horizontal and vertical projections), constraints have been
added to restrict the space of solutions to obtain faithfulness, say uniqueness, in
the reconstruction process.
In [10], the authors provided the following milestone result that answers the
uniqueness problem in case of convex polyominoes, i.e. the class C of connected
sets of points that match their convex hulls:
Theorem 1. Let us consider the class C; it holds that
1. if U is a set of four discrete directions not having cross ratio {4/3, 3/2, 2, 3, 4},
then C is characterized by U (i.e., uniquely determined by the projections along
the directions of U);
2. C is characterized by any set of seven mutually nonparallel directions;
3. there is a set of six discrete directions not characterizing C;
4. C cannot be characterized by any set of three discrete directions.
For every set of four directions U={u1, u2, u3, u4}, the cross ratio is deﬁned as
ρ(U) = (h3 −h1)(h4 −h2)
(h3 −h2)(h4 −h1),
where ui = (uix, uiy) and hi = uiy
uix , for i = 1, 2, 3, 4 and 0 ≤h1 < h2 < h3 < h4.
If u4 = (0, 1), say by abuse of notation h4 = ∞, then we only keep the terms
not containing h4, and the cross ratio reduces to ρ(U) = (h3−h1)
(h3−h2).
A major task in proving the above result relies on the analysis of the class
of lattice U-polygons, consisting of any non-degenerate convex polygon P such
that, for any vertex v of P, and for any direction u ∈U, the line through v and
parallel to u meets a diﬀerent vertex v′ of P. The proof of Theorem 1 comes
out from a number of lemmas and properties that combine tools from p-adic
valuations, projective geometry, algebraic number theory and convexity.
After the paper of Gardner and Gritzmann appeared, the leading idea was
that, when a kind of convexity information is available, ambiguous reconstruc-
tions are somehow related to the existence of U-polygons. Therefore, understand-
ing the properties of such structures can help in achieving uniqueness results and
reconstruction hints (see [1–6] for interesting examples). In [4] it was proved that
if |U| ≥4, and the values of the cross ratio of any set of four directions in U,
arranged in order of increasing angle with the positive x-axis, are in the set
{4/3, 3/2, 2, 3, 4}, then a lattice U-polygon does exist. All the lattice U-polygons
exhibited in [4] are dodecagons, produced by tiling aﬃnely regular hexagons.

Ambiguity Results in the Characterization of hv-Convex Polyominoes
149
Such kind of hexagonal tiling has been investigated and generalized in [6,7],
where several results concerning the symmetries of U-polygons, and their geo-
metric structure have been also provided.
In [4], and later on in [9], some classes of polyominoes that are uniquely
determined by projections are provided, combining the notion of horizontal and
vertical convexity with additional geometrical constraints. The characterization
of the class of hv-convex polyominoes, say HV, from projections is still open:
again in [4], it was conjectured that Theorem 1 continues to hold for the elements
of this class.
Our study sheds some new light on this problem: we restrict the conjecture
by showing that ambiguities in the reconstruction do persist for any set of direc-
tions whose elements belong to the same quadrant and do not include both the
horizontal and the vertical ones. Then, we go further by showing that no char-
acterization is possible for any set of four directions including the two axial ones
and such that the remaining two form a suﬃciently small angle.
The overall organization of the paper includes, in the next section, a review
to the basics of discrete tomography and the related uniqueness problem. Then,
Sect. 3 presents our main results, i.e., the construction of speciﬁc hv-switchings
along a class of 4-tuples of directions. The prominent role of the projections
along the horizontal and vertical directions is also underlined. Finally, in Sect. 4
we provide perspectives and open problems related to the deﬁnition of hv-convex
polyominoes from a generic set of given projections.
2
Deﬁnitions and Known Results
A ﬁnite discrete subset of points S in the integer lattice is usually represented
by a set of cells (unit squares whose centers are the points in the set itself) on a
squared surface, and its dimensions are those of the minimal bounding rectangle.
We choose to identify the lower leftmost cell of such rectangle with the origin of
the integer lattice, so that each set of points will be considered up to translation.
A polyomino is a ﬁnite union of cells whose interior is connected (see
Fig. 1(a)). A column (row) of a polyomino is the intersection between the poly-
omino and an inﬁnite strip of cells whose centers lie on a vertical (horizontal)
line.
Nevertheless, several subclasses of interest were considered by putting on
polyominoes constraints deﬁned by the notion of convexity along diﬀerent direc-
tions. In particular, a polyomino is h-convex (resp. v-convex) if each of its rows
(resp. columns) is connected. A polyomino is hv-convex, if it is both h-convex
and v-convex (see Fig. 1(b)). Finally, a polyomino is convex if it is convex w.r.t.
all the discrete directions, i.e., if it equals its discrete convex hull (see Fig. 1(c)).
Let u = (ux, uy) be a discrete direction i.e., a couple of coprime integers.
To each discrete set S and direction u, we associate an integer vector Xu(S)
that stores the number of points of S that lie on each line parallel to u and
intersecting the minimal bounding rectangle of S; we indicate such a vector as
the (vector of) projections of S along u (see Fig. 1(c)).

150
E. Barcucci et al.
32 333
(a)
(b)
(c)
Fig. 1. (a) a polyomino; (b) an hv-convex polyomino; (c) a convex polyomino and its
projections along the direction u = (1, 1).
Let U be a ﬁnite set of discrete directions. We say that S is determined by (the
projections along the directions of) U, if Xu(R) = Xu(S) implies R = S. We
say that S is ambiguous w.r.t. U otherwise. Finally, a class of discrete sets S is
characterized by U if all its elements are non ambiguous w.r.t. U inside the class.
One of the main aims in the ﬁeld of Discrete Tomography is the achievement
of a faithful reconstruction of an unknown object, regarded as a discrete set of
points at a certain resolution, from a set of projections. As one can imagine,
the existence of diﬀerent sets of points sharing the same set of projections may
dramatically change into meaningless the whole process, so the relevance of the
following problem:
Uniqueness (S, U)
Instance: a class of discrete sets S, and a set of discrete directions U.
Question: Does there exist two distinct elements of S having the same projections
along the directions of U?
2.1
The Notion of Switching Components
Many authors studied Uniqueness (S, U) in terms of the cardinality and the
characteristics of the class of discrete sets sharing the same projections along U
and, since the very beginning, they relied on the idea of switching component
(in a very ﬁrst study, Ryser [14] called it interchange) i.e., a rearranging of some
elements of a set that preserves the projections along U.
More precisely, a discrete set S contains a switching component along U if
there is a set of cells S′ in the corresponding minimal bounding rectangle such
that:
– S′ = S0 ∪S1, S0 ∩S1 = ∅and |S0| = |S1|;
– if a ∈S0 and b ∈S1, then exactly one among a and b belongs to S;
– for each u ∈U, Xu(S0) = Xu(S1) i.e., each line parallel to one direction of
U contains the same number of elements in S0 and S1.
We call S′ a switching component along U.

Ambiguity Results in the Characterization of hv-Convex Polyominoes
151
u n
(a)
(b)
(c)
Fig. 2. (a) the switching components S′ and its two sets S0 and S1 along the directions
(1, 0) and (1, 2). The dark squares are elements of S; (b) the switching component S
′
obtained by S′ after changing the values of its elements; (c) the recursive step of the
construction of a switching along the n-th direction un.
Figure 2(a), shows the two diﬀerent switching components S0 and S1 along
two directions, (the light gray cells are not elements of S).
From the deﬁnition of switching component, it holds that if a set S has a
switching component S′ along U, then by changing the values of the elements of
S′, we obtain another set having the same projections. We denote this discrete
set by ˆS. Furthermore, we call the switching component obtained by changing
the values of all the elements of S′, its dual, and we indicate it by S
′ (see
Fig. 2(b)). Obviously the dual operator has period 2. We recall the following
result (see [13]):
Theorem 2. Let S be a class of discrete sets. An element S ∈S is ambiguous
(in S) w.r.t. to a set of directions U, if and only if S has a switching component
along U such that ˆS ∈S.
The following simple property, rediscovered several times (see [12]), intro-
duces a recursive way to generate switching components along a generic set of
discrete directions:
Property 1. Let S′ be a switching component along u1, u2, . . . , un−1, and S′′ be
the dual of the translation of S′ along the further direction un so that S′∩S′′ = ∅,
i.e., S′′ = {kun+(i, j) : (i, j) ∈S
′}, for k suﬃciently large integer. The set S′∪S′′
is a switching component along the directions u1, u2, . . . , un.
From this property, two main consequences follow:
(i) we can construct a switching component along a generic set U of directions
in the recursive way shown in Fig. 2;
(ii) since we can construct a switching component along every U, then the class
of all the possible discrete sets cannot be characterized by any set of discrete
directions.

152
E. Barcucci et al.
a
b
a
b
Fig. 3. An example of a (dodecagonal) switching component along the directions (1, 0),
(0, 1), (a, b), (−a, b), (a, 2b), and (2a, b).
We stress that the switching components may also be composed in order to
achieve complex switching conﬁgurations that connect all the elements sharing
the same projections to form a connected graph structure.
If we restrict our study to the class C of convex polyominoes, Theorem 1
states necessary and suﬃcient conditions in order for a discrete set to be charac-
terized by projections. In particular, any set of seven directions provides unique-
ness, while six do not always suﬃce as the U-dodecagon in Fig. 3 shows, since
every set of four directions in U returns a value of the cross ratio belonging to
{4/3, 3/2, 2, 3, 4}.
On the other hand, if we relax too much the constraints on the convexity of
the sets, for example preserving convexity only along the horizontal direction,
then a negative result holds, as shown in [4], Theorem 3.7:
Theorem 3. No ﬁnite set of directions characterizes the class of horizontally
(resp. vertically) convex polyominoes.
Our intent is to inspect the cut-oﬀline between those results that seems to
show up when both the horizontal and vertical convexity are present.
In the same paper, it has been proposed the following
Conjecture 1. The class HV is characterized by a set of four discrete directions
U = {(1, 0), (0, 1), u3, u4} such that ρ(U) does not belong to {4/3, 3/2, 2, 3, 4}.
that has been supported by computational evidence: hv-convex polyominoes
were randomly generated and each of them reconstructed using its projec-
tions along a set of four directions whose cross ratio does not belong to
{4/3, 3/2, 2, 3, 4}. It was veriﬁed that the algorithm uniquely reconstructed the
generated hv-convex polyominoes. In the next section, we modify the conjecture
by showing a new class of 4-tuples of directions that allow switching components
in hv-convex polyominoes.
3
A New Class of Switching Components
The following result from [4,11], provides a step forward to the study of the
uniqueness problem on the class HV:

Ambiguity Results in the Characterization of hv-Convex Polyominoes
153
u
3
2
1
u
u
Fig. 4. The construction of the hexagonal switching along three directions u1, u2,
and u3.
Property 2. Let u1, u2, and u3 be three discrete directions. There is a hexagonal
switching H along U = {u1, u2, u3}.
Proof. We prove the statement by construction. First of all we construct the
switching S along u1 and u2, following Property 1. Then, we extend both the
sides of S until they equal a k multiple of u3, and ﬁnally we add to S its dual
along ku3, i.e. H = S ∪ku3 + S. The two coincident points are deleted (see
Fig. 4).
⊓⊔
Observe that hexagonal switching components have just six distinct points,
which is the minimal cardinality for a three directions’ switching, while the
general construction along three directions deﬁned in Property 1 requires eight
points. Furthermore, each hexagonal switching is convex in the sense that it is a
switching of a convex set.
However, from Property 2 it is easy to realize that hexagonal switching com-
ponents are not diﬀerent from those deﬁned in Property 1: as a matter of fact,
they can be obtained by translating along the direction u3 an appropriate mag-
niﬁcation of the switching components along u1 and u2, so that two opposite of
the eight points coincide and so annihilate.
Let U = {u1, u2, u3} be a set of three directions in R2. The group SU
3 which
permutes the indices {1, 2, 3} can be represented as a group of symmetries ﬁxing
a triangle T with edges parallel to {u1, u2, u3}. In [7, Theorem 6], the following
characterization of lattice U-polygons has been determined
Theorem 4. Let U = {u1, u2, u3} be any set of three lattice directions. Let P
be a lattice hexagon. Then P is a U-polygon if and only if SU
3 (P) = P.
As a consequence, a hexagonal switching along U = {u1, u2, u3} always
returns a rational magniﬁcation of a lattice U-hexagon P, where the triangle
T reduces to its barycenter.
Property 3. The class HV cannot be characterized by a ﬁnite set of discrete
directions U such that:
(i) at most one among the directions h = (1, 0), and v = (0, 1) belongs to U;
and
(ii) all the directions in U belong to the same quadrant.

154
E. Barcucci et al.
This property predicts the basic role played by the two convexity directions h
and v when they belong to U; for brevity we omit its proof, that is related to
the possibility of connecting suﬃciently distant parts of a switching component
deﬁned as in Property 1 with a path that preserves hv-convexity. It is also easy
to verify that the same construction holds when the axes direction is replaced
by a generic one:
Property 4. The class HV cannot be characterized by a ﬁnite set of discrete
directions U not containing (1, 0) and (0, 1) and such that all but one lie in the
same quadrant.
Relying on these results, we focus on the characterization of HV by means of a
set of four directions U = {h, v, u3, u4} such that ρ(U) ̸∈{4/3, 3/2, 2, 3, 4}.
3.1
Composing Hexagonal Switching Components Along a Diagonal
Direction
The deﬁnition of hexagonal switching component along three directions suggests
a possible construction of hv-convex switching operations along four directions.
From [7, Corollary 7] we know that an hexagon H is a lattice U-polygon if and
only if for any two diagonals of H, having direction v1, v2, there exists a symme-
try σ ∈SU
3 such that σ({u1, u2, u3}) = {u1, u2, u3}, and σ(v1) = v2. This induces
to explore further switchings of a hexagonal one when performed along a diag-
onal direction. We deﬁne such switchings as diagonal-hexagon switchings. Also,
up to aﬃne transformation, we can always assume that U = {(a, 0), (0, b), (a, b)}.
Property 5. For any three integer numbers a, b, k
(i) there exists an hv-convex hexagonal switching Hk along the set of three
directions U = {(a, 0), (0, b), (a, b)};
(ii) there exists an hv-convex diagonal-hexagon switching H′
k along U ′
k = U ∪
{(2a, 2b) + k(2a, b)}. The same holds if U ′
k = U ∪{(2a, 2b) + k(a, 2b)}.
Proof. Let us consider the set U = {(a, 0), (0, b), (a, b)} and construct the related
hexagonal switching H. Then, we translate it along the direction (2a, b), and we
annihilate the incident points having opposite values. The obtained ten points
conﬁguration, say H1 = H∪((2a, b)+H), is still an hv-convex switching along U.
Now we deﬁne H′
1 = H1∪((4a, 3b)+H1). It is immediate to see that H′
1 is an hv-
convex switching along U ′ = U ∪{(4a, 3b)}. Now, by iterating the construction
for k times, we get
Hk = H
k
i=1
(k(2a, b) + H)
and
H′
k = Hk ∪((2a, 2b) + k(2a, b) + Hk).
(1)
Then H′
k provides an hv-convex switching component with respect to the set
U ′
k = U ∪{((2a, 2b) + k(2a, b))}. In the case U ′
k = U ∪{((2a, 2b) + k(a, 2b))}, the
statement follows with a quite analogous argument.
⊓⊔

Ambiguity Results in the Characterization of hv-Convex Polyominoes
155
Theorem 5. The cross ratio of any diagonal-hexagon switching U ′
k is
ρ(U ′
k) = 2k + 1
k
.
Proof. Suppose that U ′
k = U ∪{((2a, 2b) + k(2a, b))}. Then, being a, b, k > 0,
the slope mk of the k-dependent direction of U ′
k satisﬁes
0 < mk = (k + 2)b
2(k + 1)a < b
a.
Assume the slope as a projective coordinate. Then, by arranging increasingly the
four slopes of the directions in U ′
k, the computation of their cross ratio ρ(U ′
k)
provides
ρ(U ′
k) = (0, mk, b
a, ∞) = 2k + 1
k
.
In case U ′
k = U ∪{((2a, 2b) + k(a, 2b))}, it results
0 < b
a < m4 = 2(k + 1)b
(k + 2)a ,
so that
ρ(U ′
k) = (0, b
a, mk, ∞) = 2k + 1
k
,
and the statement follows.
⊓⊔
Remark 1. Note that ρ(U ′
1) = 4, ρ(U ′
2) = 3, and
lim
k→∞ρ(U ′
k) = 2.
Moreover, up to rearranging the order of the directions, the values 4/3 and 3/2
can be turned to 4 and 3, respectively. This points out that the set of cross ratio
determined by diagonal-hexagon switching

2k + 1
k
, k ∈N,

,
naturally reﬁnes the set {4/3, 3/2, 2, 3, 4}, that prevents uniqueness in the class
C of convex lattice sets.
Figure 5 shows the above result in the case when k = 3 and the related
switching along the directions U ′ = {(a, 0), (0, b), (a, b), (8a, 5b)} whose cross
ratio is 8/3.

156
E. Barcucci et al.
a
b
Fig. 5. The composition of four hexagonal switchings, i.e., k = 3 w.r.t. the set U =
{(a, 0), (b, 0), (a, b)} along the direction (2a, b) and the related switching components.
The fourth obtained direction is (8a, 5b).
3.2
General Construction of the Switching Components
Finally, we push to the last step our construction of a new class of hv-convex
switching components by observing that successive shifts along one of the two
directions u = (2a, b) or v = (a, 2b) of the hexagon H can also generate new
4-tuples of elements for the set of directions U ′.
In particular, let us consider a sequence of k1 successive shifts of H along
u and k2 along v: the obtained switching along U, say Hk1,k2 is represented in
Fig. 6(a). Following Eq. (1), we deﬁne
H′
k1,k2 = Hk1,k2 ∪((2a, 2b) + k1(2a, b) + k2(a, 2b) + Hk1,k2).
The following holds
Theorem 6. The set H′
k1,k2 is an hv-convex switching along the directions U ′ =
U ∪{u4}, with u4 = (2a, 2b) + k1u + k2v.
The proof can be achieved after observing that Hk1,k2 is an hv-convex switching
along U, and the union with its dual, once translated along the direction u4,
preserve both the projections along u4, by deﬁnition, and the hv-convexity.
Figure 6(b) shows an example of the switching when k1 = 1 and k2 = 3. We
again underline that the cross ratio of the obtained direction u4 = (2a + k12a +
k2, 2b + k1b + k22b) = (7a, 9b) does not belong to the set {4/3, 3/2, 2, 3, 4}.
After deﬁning the set of discrete directions
D = {(2a + k12a + k2, 2b + k1b + k22b) : k1, k2 ∈N},
Theorem 6 allows us to modify Conjecture 1 as follows:
Conjecture 2. The class of hv-convex polyominoes is characterized by a set of
four discrete directions U = {(1, 0), (0, 1), (a, b), u4} such that:
– ρ(U) ̸∈{4/3, 3/2, 2, 3, 4};
– u4 ̸∈D.

Ambiguity Results in the Characterization of hv-Convex Polyominoes
157
k 1
2k
(a)
(b)
Fig. 6. (a) a generic composition of k1 hexagonal switchings along the direction (2a, b)
and k2 along the direction (a, 2b); (b) an example when k1 = 1 and k2 = 3. The
obtained fourth direction is (7a, 9b).
4
Conclusions and Perspectives
In this paper we have addressed the problem of generalizing the results of [10],
related to the lattice convex sets, to the wider class HV of hv-convex polyomi-
noes. We have obtained a few preliminary properties towards a detailed answer
to Conjecture 1. In particular we have obtained a complete characterization of
diagonal-hexagon switching in HV, and we have determined all the possible val-
ues of the cross ratio involved in the corresponding ambiguous reconstructions.
These have been explicitly exhibited also from the geometric point of view.
As a further step we wish to investigate diﬀerent extensions of the presented
constructions, in order to get a general description of all switchings preserving
hv-convexity. This would allow to get a uniqueness result for hv-convex polyomi-
noes, as well as to get new information on the geometric meaning of the values
of the cross ratio related to ambiguous reconstructions
Acknowledgment. This study has been partially supported by INDAM - GNCS
Project 2016.
References
1. Alpers, A., Larman, D.G.: The smallest sets of points not determined by their
X-rays. Bull. London Math. Soc. 47, 171–176 (2015)
2. Alpers, A., Tijdeman, R.: The two-dimensional Prouhet-Tarry-Escott problem. J.
Number Theor. 123, 403–412 (2007)
3. Brunetti, S., Dulio, P., Hajdu, L., Peri, C.: Ghosts in discrete tomography. J. Math.
Imaging Vis. 53(2), 210–224 (2015)
4. Barcucci, E., Del Lungo, A., Nivat, M., Pinzani, R.: X-rays characterizing some
classes of discrete sets. Linear Algebra Appl. 339, 3–21 (2001)
5. Cipolla, M., Lo Bosco, G., Millonzi, F., Valenti, C.: An island strategy for memetic
discrete tomography reconstruction. Inform. Sci. 257, 357–368 (2104)

158
E. Barcucci et al.
6. Dulio, P.: Convex decomposition of U-polygons. Theor. Comput. Sci. 406, 80–89
(2008)
7. Dulio, P., Peri, C.: On the geometric structure of lattice U-polygons. Discrete Math.
307, 2330–2340 (2007)
8. Fishburn, P., Lagarias, J., Reeds, J., Shepp, L.: Sets uniquely determined by pro-
jections on axes II. Discrete Appl. Math. 91, 149–159 (1991)
9. Castiglione, G., Frosini, A., Restivo, A., Rinaldi, S.: Enumeration of L-convex
polyominoes by rows and columns. Theor. Comput. Sci. 347(1-2), 336–352 (2005)
10. Gardner, R.J., Gritzmann, P.: Discrete tomography: determination of ﬁnite sets
by X-rays. Trans. Amer. Math. Soc. 349, 2271–2295 (1997)
11. Gardner, R., Gritzmann, P.: Uniqueness and complexity in discrete tomography.
In: Herman, G., Kuba, A. (eds.) Discrete Tomography: Foundations, Algorithms,
and Applications, pp. 85–113. Birkh¨auser, Basel (1999)
12. Herman, G.T., Kuba, A. (eds.): Discrete tomography: Foundations algorithms and
applications. Birkhauser, Boston (1999)
13. Kuba, A.: Reconstruction of unique binary matrices with prescribed elements. Acta
Cybern. 12, 57–70 (1995)
14. Ryser, H.: Combinatorial Mathematics, The Carus Mathematical Monographs No.
14, The Mathematical Association of America, Rahway (1963)

Mojette Transform on Densest Lattices
in 2D and 3D
Vincent Ricordel(B), Nicolas Normand(B), and Jeanpierre Gu´edon(B)
Universit´e de Nantes, LS2N UMR CNRS 6004 Polytech Nantes,
Rue Christian Pauc, BP 50609, 44306 Nantes Cedex 3, France
{vincent.ricordel,nicolas.normand,jeanpierre.guedon}@univ-nantes.fr
Abstract. The Mojette Transform (MT) is an exact discrete form of
the Radon transform. It has been originally deﬁned on the lattice Zn
(where n is the dimension). We propose to study this transform when
using the densest lattices for the dimensions 2 and 3, namely the lattice
A2 and the face-centered cubic lattice A3. In order to compare the legacy
MT using Zn, versus the new MT using An, we deﬁne a fair comparison
methodology between the two MT schemes. In particular we detail how
to generate the projection angles by exploiting the lattice symmetries
and by reordering the Haros-Farey series. Statistic criteria have been also
deﬁned to analyse the information distribution on the projections. The
experimental results study shows the speciﬁc nature of the information
distribution on the MT projections due to the high compacity of the An
lattices.
Keywords: Mojette Transform · Discrete tomography · Lattices · Dens-
est lattices · Haros-Farey series
1
Objectives of the Study
The Mojette transform is an exact discrete form of the Radon transform [1]
deﬁned for speciﬁc rational projection angles. Gu´edon et al. originally developed
this transform and its corresponding inverse in 1995, in order to represent an
image as a set of discrete projections which can be chosen highly redundant (i.e.
a frame description). Since 1995, the MT proprieties have been largely explored
(spline MT, reconstructability of convex regions, MT in high dimensions, multi-
resolution MT, etc.), and a lot of applications have been found (data communi-
cation and storage, Mojette discrete tomography, Mojette based security, etc.).
Nevertheless, the MT has been mainly deﬁned, studied and applied using the
lattice Zn (where n is the dimension of the initial lattice to transform). We
propose to study this transform when using densest lattices, because we expect
that the lattice high compacity will improve the MT performances when repre-
senting the data. In the paper we naturally start the study by considering the
ﬁrst dimensions 2 and 3 for which the densest lattices are known.
Thank to Cl´ement Rougale, Jimmy Thomas, Ugo Maury and Maxime Pineau who
worked on this project for their MSc at Polytech Nantes.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 159–170, 2017.
DOI: 10.1007/978-3-319-66272-5 14

160
V. Ricordel et al.
The paper is organised as follows: in the second section, basics on MT and
lattices are given. We focus on the MT proprieties (direct/inverse transform,
projection matrix, conditions of reconstructability) that are used in the paper,
and the densest lattices for the dimensions 2 and 3 (namely the lattice A2 and
the face-centered cubic lattice A3) are also presented, the lattice density will
be also deﬁned at this level. A fair comparison method between the two MT
schemes has to be deﬁned, in order to compare the legacy MT using Zn, versus
the new MT using An. The comparison methodology is detailed in the third
section where we explain in particular how to generate the truncated lattice
containing the data to transform, and how to generate the projection angles by
exploiting the lattice symmetries and by reordering the Haros-Farey series. The
fourth section gives the experimental results, and it explains the statistic criteria
used to analyse the speciﬁc nature of the information distribution on the MT
projections. A conclusion and perspectives are given in the last section.
2
Basics on Mojette Transform and Lattices
2.1
The Mojette Transform
Direct Transform. The Mojette transform is an exact discrete form of the
Radon transform deﬁned for speciﬁc rational projection angles. Following the
work of M. Katz [2], Gu´edon et al. originally developed this transform and
its corresponding inverse in 1995, in order to represent an image as a set of
discrete projections. The rational projection angles θi are deﬁned by a set of
discrete vectors (pi, qi) as θi = tan(qi, pi), with the condition that qi and pi are
coprime (i.e. gcd(pi, qi) = 1), and qi is restricted to be positive except for the
case {pi, qi} = (1, 0). The transform domain of an image (or any truncated 2D
lattice) is a set of projections where each element (called bin) corresponds to the
sum of the pixels centered on the line of projection. This is a linear transform
deﬁned for each projection angle by the operator:
[Mf](b, p, q) = projp,q(b) =
∞

k=−∞
∞

l=−∞
f(k, l)Δ(b + kq −lp);
(1)
where (k, l) deﬁnes the location of an image pixel, b is the index of a bin, and
Δ(n) is the Kronecker delta function, equals to 1 when n = 0 and 0 otherwise.
The line of projection is represented by b = kq −lp, and then Δ(b + kq −lp) is
equal to 1 only for the pixels on this line. The previous Eq. 1 can be rewritten
in a matrix form:
Mft(b, p, q) =

k

l
f(k, l)Δ

B −P2→1

k
l

;
=

k

l
f(k, l)Δ

b

−

−q p
 
k
l

;
(2)

Mojette Transform on Densest Lattices in 2D and 3D
161
where P2→1
k
l

is the projection matrix.
Equation (2) can be generalised to higher dimensions. In 3D, a projection
plane is deﬁned by a discrete vector (p, q, r) with gcd(p, q, r) = 1. In the same
way, the projection planes are built from a discrete 3D volume f(k, l, m). Bins
are discrete points onto the projected plane, indexed by a vector B =
b1 b2
t.
The 3D Mojette transform can then be deﬁned as [1, Chap. 3], [3]:
Mf(b1, b2, p, q, r) =

k,l,m
f(k, l, m)Δ
⎛
⎝B −P3→2
⎡
⎣
k
l
m
⎤
⎦
⎞
⎠;
=

k,l,m
f(k, l, m)Δ
⎛
⎝

b1
b2

−P3→2
⎡
⎣
k
l
m
⎤
⎦
⎞
⎠.
(3)
Moreover, in order to obtain a simple and unique index method for the vector of
projection, the following conventions are taken [1, Chap. 3]: r ≥0 and q ≥0 if
r = 0. The projection P3→2 matrix can then be deﬁned as following [1, Chap. 3]:
P3→2 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

1 0 −p
r
0 1 −q
r

if r ̸= 0 and q ̸= 0;

1 −p
q 0
0 0 1

if r = 0 and q ̸= 0;

0 1 0
0 0 1

if r = 0 and q = 0.
(4)
This matrix is not optimal as it does not use entire displacement (i.e. ratios
are used in this matrix) which creates point with non integer coordinates. Other
matrices P exist and can be generated from the direction projections as presented
in DGCI 2005 [4]. The full and detailed explanations in order to generate the
matrix P from the direction projection (v1, v2, . . . , vn) are given in [4,5].
The projection of a 3D regular lattice on a plane with the vector (p, q, r)
always produces a 2D regular lattice, according to the vector (p, q, r) [3].
Projection Matrix and Reconstructibility. The reconstructability is the
ability to ensure the exact reconstruction of any information using only a set of
viewpoints. In other words, a region is reconstructible by a set of projections if
a unique correspondence exists between the region and the set of projections [6].
The conditions determining if a set of Mojette projections is suﬃcient for invert-
ing the transform depends strongly on the discrete shape of the region support
under projection. Simple rules exist when the shape is convex [1, Chap. 4]. For
rectangular regions, the Katz criterion solves this problem [2]. The reconstruc-
tion conditions for any convex region were derived by Normand [1, Chap. 4].

162
V. Ricordel et al.
In 2D, each projection direction vector (pi, qi) is associated with a two-pixels
structuring element Bi (2PSE). Taking G as the region support formed by the
successive dilations of the structuring elements Bi, the convex region is recon-
structible if and only if it can not contain G [6]. In other words, a convex region
(i.e. an image) is not reconstructible if and only if the dilation result by 2PSE
is not included in the image support [1, Chap. 4]. This can be also rephrased
as: an image of convex support C is reconstructible if and only if the successive
erosions of the C formed by the structuring elements Bi gives an empty set [6].
In 3D, the method is extended [3, Chap. 4]: each projection direction vector
(pi, qi, ri) is associated with a two-voxels structuring element, and any convex
3D region R is reconstructible by a set of projections SI, if the dilation of the
two-voxels structuring elements produces a form D than is not included in R.
2.2
The Lattices
Lattices. A lattice Λ is a regular arrangement of points in a n-dimensional
space. Λ is characterised by its basis [7, (Chap. 1)] or correspondingly by its
generator matrix:
MΛ =
⎡
⎢⎢⎢⎣
v11 v12 . . . v1m
v21 v22 . . . v2m
...
...
...
...
vn1 vn2 . . . vnm
⎤
⎥⎥⎥⎦.
(5)
By combination of the basis vectors, the lattice fundamental parallelotope is
constructed. This parallelotope, when repeated, can ﬁll the whole space with
just one lattice point in each copy.
Diﬀerent lattices have been studied to solve diﬀerent problems as sphere
packing problem, sphere covering problem, kissing number, fast quantization,
etc. In the paper we focus on the Zn lattice, and on the densest lattices for the
dimensions 2 and 3, respectively A2 and A3.
A sphere packing is an arrangement of non-overlapping identical spheres
within a containing space. The lattice is then constituted with the spheres cen-
ters, and the densest packing maximises the volume occupied by the spheres.
The lattice density can be deﬁned by:
Δ =
vol. of one sphere
vol. of the fundamental region = vol. of one sphere
det(MM tr)
1
2
.
(6)
Z n lattices. The integer lattice is deﬁned as [7, (Chap. 4)]:
Zn = {(x1, . . . , xn)|xi ∈Z}.
(7)
Its generator matrix is the identity matrix. The densities of the lattices Z2 and
Z3 are respectively ΔZ2 = π
4 = 0.785... and ΔZ3 = π
6 = 0.524...
We will exploit the lattice symmetries for the MT. The Zn automorphism
group consists of all possible symmetries that are obtained by vector coordinate

Mojette Transform on Densest Lattices in 2D and 3D
163
permutation and/or sign change, the set of permutations has a cardinality of
(2nn!). So, after removing the sign changes (e.g. (a, b) and (−a, −b) are the
same vector but with opposite direction), Z2 and Z3 counts respectively 4 (see
Fig. 1) and 24 symmetries (see Fig. 2).
Fig. 1. Symmetries in Z2 (without sign change).
Fig. 2. Symmetry in Z3 (without sign change).
An lattices. The An lattice (for n ≥1) can be deﬁned as [7, Chap. 4]:
An = {(x0, x1, . . . , xn) ∈Zn+1|x0 + x1 + · · · + xn = 0}.
(8)
The generator matrix is
MAn =
⎡
⎢⎢⎢⎢⎢⎣
−1 1
0 0 . . . 0 0
0 −1 1 0 . . . 0 0
0
0 −1 1 . . . 0 0
...
...
...
... ...
...
...
0
0
0 0 . . . −1 1
⎤
⎥⎥⎥⎥⎥⎦
.
(9)

164
V. Ricordel et al.
An is the densest lattice for dimensions 2 and 3. A3 is also known as the
face-centered cubic lattice (FCC). The densities of the lattices A2 and A3 are
respectively ΔA2 =
π
√
12 = 0.9069... and ΔA3 =
π
√
18 = 0.7405....
The set of permutations of the automorphism group of the lattice A2 (respect.
A3) has a cardinality of 16 (respect. 48). After removing the sign changes, 6
symmetries (respect. 12 symmetries) remain [7, Chap. 4] (see also the Figs. 3
and 4).
Fig. 3. Symmetries in A2 (without sign changes).
Fig. 4. Neighbors of the point (0, 0, 0) in A3 [8]
2.3
Projections and Haros-Farey Sequences
The Haros-Farey sequence gives the set of rational angles in a centered square
or cube, this sequence is used to enumerate the MT projections (up to the
reconstructability conditions).

Mojette Transform on Densest Lattices in 2D and 3D
165
In 2D, the Haros-Farey sequence of order N, denoted FN, is the ordered
sequence of irreducible ratios included between 0 and 1, where the denominator
is less than or equal to N. In order to get FN+1 from FN, a median ratio a12
b12
is inserted between each ratio
a1
b1 and
a2
b2 of FN, such as a12 = a1 + a2 and
b12 = b1 + b2 if a12 < N + 1 and b12 < N + 1 [5]. F1, F2, F3 are given as an
example:
F1 =
0
1, 1
1

; F2 =
0
1, 1
2, 1
1

; F3 =
0
1, 1
3, 1
2, 2
3, 1
1

.
(10)
Each ratio ( q
p) of the sequence is used in order to generate a corresponding
projection (p, q) of the 2D MT.
In 3D, according to [5], the Haros-Farey sequence of order N, denoted by FN,
is the set of points ( y
x, z
x) such that gcd(x, y, z) = 1, between [0, 0] and [1, 1], and
which denominator x does not exceed N. In other words, a point ( y
x, z
x) ∈FN
if x ≤N, 0 ≤y ≤x, 0 ≤z ≤x and gcd(x, y, z) = 1. Let A1( y1
x1 , z1
x1 ) and
A2( y2
x2 , z2
x2 ), two points of FN−1 such as x1 + x2 = N. The middle point between
A1 and A2 has the coordinates ( y1+y2
x1+x2 , z1+z2
x1+x2 ) [5]. Below, F1, F3, F3 are given as
an example, where each point ( y
x, z
x) is written as (x, y, z) [5]:
F1 = {(1, 0, 0), (1, 1, 0), (1, 1, 1)} ;
F2 = {(1, 0, 0), (1, 1, 0), (1, 1, 1), (2, 1, 0), (2, 1, 1), (2, 2, 1)} ;
F3 = F2

{(3, 1, 0), (3, 1, 1), (3, 2, 0), (3, 2, 1), (3, 2, 2),
(3, 3, 1), (3, 3, 1)} .
(11)
The sequence is used in order to generate the (p, q, r) projections of the 3D MT,
with (p, q, r) representing the point ( q
p, r
p).
3
Comparison
This section explains how the truncated lattices were constructed and how the
projections were selected. The proposed criteria of comparison are also presented.
3.1
Methodology of Comparison
The goal is to compare the legacy MT using the Zn lattice, with the MT using
the densest lattice. Each lattice (Zn or An) is truncated such that they have the
same number of points Npoints.
Construction of the Truncated Lattices. The ﬁrst step is to create the
truncated lattice. In order to do that, an iterative process is used, where from
the 0 point, at each loop, we ﬁnd the lattice points on successive embedded
spheres. Locally, a basic pattern is used which gives for a lattice point its closed
neighbours (see Figs. 2 and 4). The growing lattice process is stoped when the
number of points Npoints is reached (Npoints is given by the user). Each point of
the truncated lattices is set to a unitary value.

166
V. Ricordel et al.
Selection of Projections. The minimal number of projections are chosen
for the truncated lattice to be exactly reconstructible. Projection vectors are
produced by sorting fractions of Haros-Farey series according to their squared
Euclidean norms, i.e., respectively x2 + y2, x2 + y2 + z2, k2 + l2 −kl and xx +
yy + zz −yz −xz in lattices Z2, Z3, A2 and A3. For example, with F5 sorted:
F5 =
0
1, 1
5, 1
4, 1
3, 2
5, 1
2, 3
5, 2
3, 3
4, 4
5, 1
1

;
SortZ2(F5) =
0
1, 1
1, 1
2, 1
3, 2
3, 1
4, 3
4, 1
5, 2
5, 3
5, 4
5

;
SortA2(F5) =
0
1, 1
1, 1
2, 1
3, 2
3, 1
4, 3
4, 2
5, 3
5, 1
5, 4
5

.
(12)
Before choosing another projection in the Haros-Farey, all equivalent projec-
tions by rotation are generated. The number of equivalent projections by rotation
depends on the lattice (for instance, 3 other projections for the Z2 lattice, and
5 other projections for the A2 lattice).
In order to know if the truncated lattice is exactly reconstructible using the
set of selected projections, the shape of successive dilatations of the projections
directions is generated, as explained in Sect. 2.1. The truncated lattice is exactly
reconstructible only when its radius is inferior or equal to the radius of the
generated ﬁgure.
3.2
Comparison Criteria
Global criteria were used to compare the information distribution on the MT
projections.
Redundancy. Redundancy is given by the following equation [1, Chap. 3]:
Red = nbbins
nbpoints
−1.
(13)
If redundancy is positive, it represents the percentage of extra bins compared to
the number of points. If redundancy is negative, then there is no reconstructabil-
ity of the truncated lattice. Here, by construction, Red is positive but small.
Number of Bins. Bi is the number of bins on the i-th projection (i.e. the
i-th projection length). The mean ¯B, and the variance V ar(B), can be then
calculated as following:
¯B = 1
n
n

i=1
Bi,
V ar(B) = 1
n
n

i=1
(Bi −¯B)2,
(14)
where n is the number of projections in the set. This criteria measures the
diﬀerence of the number of bins in the projections. The smaller V ar(B) for
diﬀerent projections, the higher those projections carry the same amount of
information.

Mojette Transform on Densest Lattices in 2D and 3D
167
Number of Points per Bin. For the test case, each lattice point is set to
1, so each bin value (on every projections) equals to the number of points that
contribute to the bin. The mean (considering all projections) is computed as:
Mean(points per bin) = ¯b = 1
m
m

i=1
bi,
(15)
where m is the total number of bins (considering all projections) and bi is the
value of the i-th bin. The higher the mean, the higher bins represent more points.
This is directly related to the density of the lattice.
4
Experimental Results
In this section, the main results are presented and discussed. All the experiments
were done in 2D and 3D, but we shall concentrate here onto the 3D case because
it generalises the 2D case.
Fig. 5. Truncated lattice radius (a), Total number of bins (b), and Redundancy (c).
The blue (resp. red) curves characterise the A3 (resp. Z3) lattice. (Color ﬁgure online)

168
V. Ricordel et al.
The ﬁrst feature displayed on Fig. 5(a) is the truncated lattice radius value
computed from Npoints, the number of generated points. For Npoints > 400, the
curves show the higher compacity of A3 on Z3.
The higher total number of bins of Z3 (see Fig. 5(b)) explains its higher
redundancy (see Fig. 5(c)).
The next features are the number of bins Mean (Fig. 6(a)) and Variance
(Fig. 6(b)) according to the number of generated points. Concerning these 2
features, it seems that the two lattices perform almost equally, but the Fig. 6(c)
shows that the projections number is diﬀerent for the 2 lattices, a ﬁner analysis
at the projections level is then necessary.
Fig. 6. Number of bins: Mean (a) and Variance (b), and Number of projections (c).
The blue (resp. red) curves characterise the A3 (resp. Z3) lattice. (Color ﬁgure online)
We then use histograms. The Fig. 7 compares the projections densities con-
sidering the number of points Mean per projection. And the Fig. 8 compares
the projections densities considering their lengths. The histograms with A3 are
slightly more uniform than the ones with Z3, it shows the higher regularity of
the projections when using A3, these results are due to the high compacity of
this lattice.

Mojette Transform on Densest Lattices in 2D and 3D
169
Fig. 7. Histograms of the number of points Mean per projection for Z3 (a), and A3 (b).
Fig. 8. Histograms of the projections length for Z3 (a), and A3 (b).
5
Conclusion and Perspectives
In this paper, we examined the behaviour of densest lattices in 2D and 3D from
their discrete Mojette projections point of view. Exactly, the study focused on
the 3D case because it generalises the 2D one. The analysis of Mojette Transform
projections, when comparing the legacy MT with Z3 versus the MT with A3,
shows some interesting diﬀerences both in terms of dimensions and in terms
of projections regularity. Since the software has been developed to manage any
dimension and lattice, future work will focus on higher dimensions. Indeed it
seems interesting to try higher dimensions in order to see if the gap between An
and Zn lattices still increases.
References
1. Guedon, J.-P.: The Mojette transform: theory and applications. Wiley-ISTE (2009).
ISBN 978-1-84821-080-6. https://hal.archivesouvertes.fr/hal-00367681

170
V. Ricordel et al.
2. Katz, M.B.: Questions of Uniqueness and Resolution in Reconstruction from Pro-
jections. Lecture Notes in Biomathematics, vol. 26. Springer, Heidelberg (1978).
doi:10.1007/978-3-642-45507-0
3. Guedon, J.-P., Normand, N., Lecoq, S.: Transformation Mojette en 3D: Mise en oeu-
vre et application en synth´eses d image. GRETSI, Groupe d’Etudes du Traitement
du Signal et des Images, September 1999. http://hdl.handle.net/2042/13101
4. Normand, N., Servi`eres, M., Gu´edon, J.P.: How to obtain a lattice basis from a
discrete projected space. In: Andres, E., Damiand, G., Lienhardt, P. (eds.) DGCI
2005. LNCS, vol. 3429, pp. 153–160. Springer, Heidelberg (2005). doi:10.1007/
978-3-540-31965-8 15
5. Servieres,
M.:
Reconstruction
Tomographique
Mojette.
Theses,
Universit´e
de
Nantes;
Ecole
Centrale
de
Nantes
(ECN),
December
2005.
https://tel.
archives-ouvertes.fr/tel-00426920
6. Normand, N., Guedon, J.-P.: La transformee mojette: Une representation redon-
dante pour l’image. In: Comptes Rendus de l’Academie des Sciences - Series I
- Mathematics, vol. 326, no. 1, pp. 123–126 (1998). http://dx.doi.org/10.1016/
S0764-4442(97)82724-3, ISSN 0764–4442
7. Conway, J.H., Sloane, N.J.A.: Sphere Packings, Lattices and Groups. Grundlehren
der mathematischen Wissenschaften, vol. 290. Springer, New York (1993). doi:10.
1007/978-1-4757-6568-7
8. Rashid, M.A., Iqbal, S., Khatib, F., Hoque, M.T., Sattar, A.: Guided macro-
mutation in a graded energy based genetic algorithm for protein structure predic-
tion. Comput. Biol. Chem. 61, 162–177 (2016). doi:10.1016/j.compbiolchem.2016.
01.008

Fuzzy Directional Enlacement Landscapes
Micha¨el Cl´ement(B), Camille Kurtz, and Laurent Wendling
Universit´e Paris Descartes, Sorbonne Paris Cit´e, LIPADE (EA 2517),
45 Rue des Saints-P´eres, 75006 Paris, France
michael.clement@parisdescartes.fr
Abstract. Spatial relations between objects represented in images are
of high importance in various application domains related to pattern
recognition and computer vision. By deﬁnition, most relations are vague,
ambiguous and diﬃcult to formalize precisely by humans. The issue of
describing complex spatial conﬁgurations, where objects can be imbri-
cated in each other, is addressed in this article. A novel spatial relation,
called enlacement, is presented and designed using a directional fuzzy
landscape approach. We propose a generic fuzzy model that allows to
visualize and evaluate complex enlacement conﬁgurations between crisp
objects, with directional granularity. The interest and the behavior of
this approach is highlighted on several characteristic examples.
1
Introduction
The spatial organization of objects is fundamental to increase the understanding
of the perception of similarity between scenes or situations. Despite the fact that
humans seem capable of apprehending spatial conﬁgurations, in many cases it
is exceedingly diﬃcult to quantitatively deﬁne these relations, mainly because
they are highly prone to subjectivity. Standard all-or-nothing mathematical rela-
tions are clearly not suitable, and the interest of fuzzy relations was initially
suggested by Freeman in the 70s [9], since they allow to take imprecision into
account. Over the last few decades, numerous works were proposed on the analy-
sis of spatial relationships in various domains, ranging from shape recognition to
computer vision, with the main purpose of describing the relative positioning of
objects in images [3]. These approaches provide a set of interesting features able
to describe eﬃciently most of spatial situations. However, some conﬁgurations
remain challenging to describe without ambiguities, especially when the objects
are imbricated, or composed of multiple connected components. In this context,
we propose to study new relations dedicated to the imbrication of objects.
This article is organized as follows. Section 2 presents related works to our
approach. Section 3 recalls the model of directional enlacement, proposed in [6]
for the description of complex spatial conﬁgurations. From the latter, we propose
in Sect. 4 a generic model relying on fuzzy landscapes that allows to evaluate
relative enlacement conﬁgurations between crisp objects, with directional gran-
ularity. Section 5 presents experimental results on diﬀerent illustrative examples
that allow to highlight the behavior and the interest of this model. Section 6
provides conclusions and perspectives.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 171–182, 2017.
DOI: 10.1007/978-3-319-66272-5 15

172
M. Cl´ement et al.
2
Related Work
In the domain of spatial relations, two major research axes can be distinguished
in the literature, based on two dual concepts: the one of spatial relationship and
that of relative position. On the one hand, it is possible to formulate a fuzzy
evaluation of a spatial relation (for example “to the left of ”) for two objects, in
order to describe their relative position. The fuzzy landscape model is a widely
used method for providing this type of assessments [2]. This approach relies on
the fuzzy modeling of a given spatial relation, directly in the image space, using
morphological operators. Applications of this model can be found in various
domains such as spatial reasoning in medical images [7] or the recognition of
handwriting [8]. On the other hand, the location of an object with regards to
another can be modeled by a quantitative representation, in the form of a relative
position descriptor. Diﬀerent spatial relations can be assessed from this repre-
sentation and the associated descriptors can be integrated in pattern recognition
systems to match similar spatial conﬁgurations. Among the various relative posi-
tion descriptors, the histograms of forces [14] are widely used due to their ability
to process pairwise information following a set of directions. They are applied
in diﬀerent works, such as the linguistic description of spatial relations [11] or
image retrieval [5]. To summarize, fuzzy landscapes consist in determining the
region of space matching a speciﬁc spatial relation, and relative position descrip-
tors consist in characterizing the position of an object with regards to another,
by combining diﬀerent spatial features into a standalone descriptor.
Although these two types of approaches allow to interpret many spatial rela-
tions between objects, they usually fail at properly describing more complex
spatial conﬁgurations, in particular when objects are concave, or composed of
multiple connected components [4]. A typical complex spatial relation is the “sur-
rounded by” relation, which was ﬁrst studied by Rosenfeld [15] and deepened by
Vanegas [16] with a dedicated approach based on fuzzy landscapes. Another spe-
ciﬁc spatial relation is “between”. This relation has been studied in details in [4],
involving deﬁnitions based on convex hulls and speciﬁc morphological opera-
tors. Applications of this spatial conﬁguration for the analysis of histological
images have been proposed by [10]. Work has also been done to characterize the
“alignment” and “parallelism” of objects in satellite images [17]. Recent works
introduced the φ-descriptor [12,13], which is a powerful generic framework to
assess any spatial relation from a set of speciﬁc operators, inspired by Allen
intervals [1]. This descriptor can determine if two objects are imbricated or not,
but it is not able to measure the depth of imbrication (such as, for instance,
when two spirals are interlaced).
In this context, recent works [6] introduced both enlacement and interlace-
ment descriptors, from the relative position point of view, in order to obtain a
robust modeling of the imbricated parts of objects. Based on this model, in this
article we propose to tackle the dual point of view, by considering fuzzy enlace-
ment landscapes instead of enlacement descriptors. The goal of fuzzy enlacement
landscapes is to visualize and evaluate these spatial conﬁgurations directly in the
image space, by considering the concavities of the objects in a directional fashion.

Fuzzy Directional Enlacement Landscapes
173
3
Directional Enlacement Model
In this section, we present the model used to describe the relative enlacement of
objects. This model was initially introduced in [6], mostly from the point of view
of the relative position descriptors. Here, we recall the intuitive idea behind what
is intended with the term enlacement, and we provide some useful deﬁnitions and
notations for this model.
A two-dimensional object A of the Euclidean space is deﬁned by its char-
acteristic function fA : R2 →R. This generic deﬁnition allows to handle both
crisp and fuzzy objects. Let θ ∈R be an orientation angle, and ρ ∈R a distance
from the origin. We deﬁne the oriented line of angle θ at the altitude ρ by the
non-ﬁnite set Δ(θ,ρ) = {eiθ(t + iρ), t ∈R}. The subset A ∩Δ(θ,ρ) represents a
one-dimensional slice of the object A, also called a longitudinal cut. In the case
of crisp objects, such a longitudinal cut of A is either empty (the oriented line
does not cross the object) or composed of a ﬁnite number of segments. In the
general case, a longitudinal cut of A along the line Δ(θ,ρ) can be deﬁned as:
f (θ,ρ)
A
: R −→R
t −→fA(eiθ(t + iρ)).
(1)
Let (A, B) be a couple of objects. The goal is to describe how A is enlaced
by B. The intuitive idea is therefore to capture the occurrences of points of
A being between points of B. In order to determine such occurrences, objects
are handled in a one-dimensional case, using longitudinal cuts along oriented
lines. For a given oriented line Δ(θ,ρ), the idea is to combine the quantity of
object A (represented by f (θ,ρ)
A
) located simultaneously before and after object
B (represented by f (θ,ρ)
B
). Let f and g be two bounded measurable functions
with compact support from R to R. The enlacement of f with regards to g is
deﬁned as:
E(f, g) =
 +∞
−∞
g(x)
 +∞
x
f(y)
 +∞
y
g(z) dz dy dx.
(2)
The scalar value E(f (θ,ρ)
A
, f (θ,ρ)
B
) represents the enlacement of A by B along
the oriented line Δ(θ,ρ). For crisp objects (i.e., each point is either 0 or 1), it
corresponds to the total number of ordered triplets of points on the oriented
line, which can be seen as arguments to put in favor of the proposition “A is
enlaced by B” in the direction θ. Algorithmically, this value can be derived by
an appropriate distribution of segments lengths along the longitudinal cuts of
both objects (see [6] for more details).
The set of all parallel lines {Δ(θ,ρ), ρ ∈R} in the direction θ slices the
objects into sets of longitudinal cut functions. To measure the global enlace-
ment of an object with regards to another in this direction, we aggregate the
one-dimensional enlacement values obtained for each of these longitudinal cuts.
The enlacement of A by B in direction θ is deﬁned by:
EAB(θ) =
1
∥A∥1∥B∥1
 +∞
−∞
E(f (θ,ρ)
A
, f (θ,ρ)
B
) dρ,
(3)

174
M. Cl´ement et al.
where ∥A∥1 and ∥B∥1 denote the areas of A and B. This normalization allows
to achieve scale invariance. In the binary case, this deﬁnition corresponds to
a number of triplets of points to put in favor of “A is enlaced by B” along
the longitudinal cuts in this direction. Intuitively, it can be interpreted as the
quantity of B traversed while sliding the object A in the direction θ, with regards
to the quantity of B located on the opposite direction.
In [6] the enlacement model EAB was considered from the point of view of
the relative position descriptors by building a directional enlacement histogram,
allowing to characterize how an object A is enlaced by another object B. In the
next section, we involve this model in a novel evaluation point of view based on
a fuzzy approach that allows to evaluate enlacement conﬁgurations directly in
the image space, with directional granularity.
4
Fuzzy Enlacement Landscapes
We present here how to extend the directional enlacement model to evaluate the
enlacement of objects in the image space from a local point of view, inspired by
the works of Bloch [2] on fuzzy landscapes for classical spatial relations.
4.1
Deﬁnition
A fuzzy enlacement landscape of an object A should be a representation of the
region of space that is enlaced by A. Since the initial enlacement model is essen-
tially directional, we also propose to deﬁne directional enlacement landscapes.
Let A be a crisp object (i.e., represented as fA : R2 →{0, 1}). In a given direc-
tion θ, for a point outside of A located at (ρ, t) coordinates in the rotated frame,
its local enlacement value can be deﬁned as:
EA(θ)(ρ, t) =
1
∥A∥1
 +∞
t
f (θ,ρ)
A
(x) dx
 t
−∞
f (θ,ρ)
A
(x) dx.
(4)
Therefore, EA(θ) can be seen as a landscape representing the local enlacement
values of the points outside of the object A. This image can be normalized into
the [0, 1] range of values in order to be interpreted as a fuzzy set, which we call
a Fuzzy Directional Enlacement Landscape (Fuzz-DEL) of the object:
μA
E (θ)(ρ, t) =
EA(θ)(ρ, t)
max
ρ,t EA(θ)(ρ, t).
(5)
Such a landscape allows to assess and visualize to which degree each point
is enlaced by the object A in a ﬁxed direction θ. It is interesting to note that
the non-zero values of this landscape are necessarily located inside the object’s
concavities. This is particularly interesting from an algorithmic point of view,
since it allows to restrict the computation to points located in the convex hull of
A (and outside of A). Another point to highlight is that enlacement landscapes
are symmetric, with period π (i.e., μA
E (θ + π) = μA
E (θ)).

Fuzzy Directional Enlacement Landscapes
175
Fig. 1. Fuzzy directional enlacement landscapes of a crisp object A, with a ﬁxed direc-
tion θ and an increasing width ω. In (b, c and d), A is outlined in white.
Since μA
E (θ) is focused on a single direction, we propose to aggregate such
fuzzy landscapes across multiple orientation angles. Let θ ∈[0, π] be an orien-
tation angle and ω ∈[0, π] a width parameter. The Fuzz-DEL on the interval
[θ −ω
2 , θ + ω
2 ] is deﬁned as follows:
μA
E (θ, ω)(ρ, t) = 1
ω
 θ+ ω
2
θ−ω
2
μA
E (α)(ρ, t) dα,
(6)
where θ represents the direction on which the fuzzy landscape is focused, while ω
controls the width of the interval, allowing to measure either a narrow direction
or a more global one. In particular, the landscape that aggregates all directions
is denoted by μA
E = μA
E ( π
2 , π).
In order to illustrate such deﬁnitions, Figs. 1 and 2 show the Fuzz-DELs
obtained for two diﬀerent objects. On the one hand, Fig. 1 illustrates the impact
of the width parameter ω for a given vertical direction (θ = π
2 ). Note that the
landscape would be identical for the opposite vertical direction (θ = 3π
2 ) because
of symmetry. We can observe the zero-valued points in the center of the object
for ω = 0, representing the fact that these points are not enlaced vertically by
A. This can be interpreted by the idea that if another object was located here, it
would be able to move in the vertical direction without crossing the other object
(i.e., the object could slide downwards). We also observe that when ω increases,
the fuzzy landscape progressively gets smoother, taking into account a wider
range of directions. On the other hand, Fig. 2 shows enlacement landscapes on
another object for diﬀerent directions θ (with a ﬁxed width ω = π
3 ). From these
examples, one can note how a Fuzz-DEL allows to capture the object directional
concavities. In the horizontal direction (θ = 0), the local enlacement values are
relatively high, and the values are higher the deeper we get inside the “snaked”
shape. In the vertical direction (θ = π
2 ), the Fuzz-DEL is mostly empty, except
on some small concavities.
4.2
Fuzzy Evaluation
In the previous deﬁnitions, a reference object A is considered, and diﬀerent
Fuzz-DELs can be derived from it. These fuzzy landscapes allow to visualize the

176
M. Cl´ement et al.
Fig. 2. Fuzzy directional enlacement landscapes of a crisp object A, with a ﬁxed width
ω and for diﬀerent directions θ. In (b, c and d), A is outlined in white.
interaction area of A. In the following, we show how to exploit these landscapes
to evaluate to which degree a target object B is enlaced by the reference object
A, using classical fuzzy operators.
Let μA and μB be two fuzzy sets over R2. A typical way to evaluate how
μB matches with μA is the necessity-possibility measure. The necessity N and
possibility Π can be respectively deﬁned as follows:
Π(μA, μB) = sup
x,y t(μA(x, y), μB(x, y)),
(7)
N(μA, μB) = inf
x,y T(μA(x, y), 1 −μB(x, y)),
(8)
where t is a fuzzy intersection (t-norm) and T is a fuzzy union (t-conorm). For
the rest of this article, the min and max operators are chosen for t-norm and
t-conorm respectively, but other fuzzy operators could be considered.
In our context, this fuzzy matching measure can be applied to evaluate how
a target object B (represented by its membership function μB) matches with
a Fuzz-DEL μA
E (θ, ω) of a reference object A. The necessity-possibility inter-
val [N(μA
E (θ, ω), μB), Π(μA
E (θ, ω), μB)] constitutes a fuzzy evaluation of how B
is enlaced by A in direction θ, with the necessity being a pessimist point of
view, while the possibility represents an optimist point of view. The mean value
M(μA
E (θ, ω), μB) can also be considered. This evaluation strategy will be further
studied in the upcoming experiments.
5
Experimental Results
We present diﬀerent illustrative examples to highlight the interest of our app-
roach. These experiments are organized around two main applications. The ﬁrst
one is to evaluate the speciﬁc relation “surrounded by”. As mentioned previously,
this relation can be considered as a particular case that can be derived from the
directional enlacement model. The second application is to evaluate the spatial
relation “enlaced by” in a more generic sense, in particular when the reference
object has multiple degrees of concavities. We also propose some preliminary
results on interlacement landscapes.

Fuzzy Directional Enlacement Landscapes
177
Fig. 3. Examples of typical surrounding conﬁgurations (gray: reference object A; white:
target object B).
5.1
Surrounding
The “surrounded by” relation is easily apprehended by human perception, but
is particularly challenging to evaluate quantitatively. It is usually modeled by
the “all directions” point of view, i.e., an object surrounds another object if it
is located in all directions. In the following, we adopt the same insight, but we
adapt it to the enlacement model: an object is surrounded if it is enlaced by the
other object in all directions.
Figure 3 presents characteristic examples of surrounding conﬁgurations that
we assessed using the proposed fuzzy evaluation strategy. In each image, the
reference object A is in gray and the target object B is the white circle.
For this application, we propose a speciﬁc way to apply our approach. The
target object B is projected into a Fuzz-DEL of A, and further normalized as a
fuzzy set. Such a projection is deﬁned as:
μAB
E
(θ, ω) =
min
ρ,t (μA
E (θ, ω)(ρ, t), μB(ρ, t))
max
ρ,t μA
E (θ, ω)(ρ, t)
.
(9)
Then, the necessity N(μAB
E
(θ, ω), μB) and possibility Π(μAB
E
(θ, ω), μB) evalua-
tions are performed for diﬀerent values of θ ∈[0, π]. This results in informative
directional necessity and possibility proﬁles, which can be also then exploited
to derive a global evaluation of how B is surrounded by A. For the rest of this
study, we ﬁxed ω to a low value of
π
36 (5◦) to take into account diﬀerent direc-
tions individually, while smoothing out some discretization issues. Following the
“all directions” point of view, this global evaluation can be obtained with the
following:
N AB
S
= 1
π
 π
0
N(μAB
E
(θ, ω), μB) dθ,
(10)
ΠAB
S
= 1
π
 π
0
Π(μAB
E
(θ, ω), μB) dθ.
(11)
Figure 4 shows the directional necessity and possibility proﬁles obtained for
the conﬁgurations of Fig. 3. In situation (a), the object is only partially sur-
rounded. Both the pessimist and possibility evaluations agree that the reference

178
M. Cl´ement et al.
Fig. 4. Directional necessity, possibility and mean proﬁles measuring the surrounding
conﬁgurations of Fig. 3.
object A is surrounded in the vertical directions, but not in the horizontal direc-
tions. The gradual transition of the situation is captured along the diagonal
directions. The object is also partially surrounded in situation (b), where half
of the surrounding circle has been cut out. In situation (c), small parts were
added preventing the object to leave without crossing the surrounding object,
and therefore the optimist point of view is 1 while the pessimist one oscillates
but is never zero. Finally, evaluations tend to agree that the object is surrounded
in situation (d). The optimist point of view is always 1, yet it takes into account
that the object could escape by crossing a small portion of the surrounding
object, resulting in low pessimist evaluations for the vertical directions.
Table 1. Fuzzy surrounding evaluations (necessity-possibility intervals and mean val-
ues) obtained for the conﬁgurations of Figs. 3 and 5.
Vanegas et al. [16] Enlacement EAB [6] [N AB
S
, ΠAB
S
]
(a)
[0.70, 0.79], 0.76
[0.50, 0.63], 0.55
[0.36, 0.64], 0.48
(b)
[0.50, 0.54], 0.52
[0.40, 0.49], 0.45
[0.25, 0.53], 0.39
(c)
[0.93, 1.00], 0.97
[0.75, 1.00], 0.95
[0.54, 1.00], 0.79
(d)
[0.94, 1.00], 0.99
[0.48, 1.00], 0.82
[0.77, 1.00], 0.85
(arcachon) [0.68, 0.85], 0.79
[0.35, 1.00], 0.62
[0.63, 1.00], 0.80

Fuzzy Directional Enlacement Landscapes
179
Fig. 5. Applicative example of a complex surrounding conﬁguration. The satellite
image represents the Bassin d’Arcachon (France). (b) Object A is gray and object
B is white. (c–f) A is outlined in white and B is outlined in red. (Color ﬁgure online)
Fig. 6. Directional necessity, possibility and mean proﬁles measuring the surrounding
of the island by the bay in Fig. 5.
To complement these results, Table 1 presents the surrounding necessity-
possibility intervals [N AB
S
, ΠAB
S
], to evaluate the global surrounding of the target
objects of Fig. 3. For comparison purposes, we also present the results of the app-
roach of [16], which is also based on a fuzzy landscape framework, but dedicated
to surrounding relation. It is based on a speciﬁc fuzzy landscape, considering
only the visible concavities of the reference object. We also present the results
obtained by [6] with the initial enlacement descriptors. Considering the fact that
surrounding evaluations are highly subjective, our goal here is not to argue that
an approach is better than another, but to illustrate that the proposed Fuzz-
DELs can provide interesting point of views regarding this surrounding spatial
relation.

180
M. Cl´ement et al.
Fig. 7. Fuzzy enlacement landscape of a spiral (reference object A) and evaluation for
diﬀerent target objects inside the spiral (represented in diﬀerent colors). (Color ﬁgure
online)
To show the potential of our approach on real data, we evaluated the “sur-
rounded by” relation on geographical objects extracted from a satellite image
(Fig. 5(a)). This image1 represents the Bassin d’Arcachon (France) and has been
acquired by the Formosat-2 satellite. The image was segmented to produce a
3-class image (Fig. 5(b)) composed of an island enclosed into the bay (refer-
ence object A) and the land coast (target object B). For illustrative purposes,
Fig. 5(c–f) present the Fuzz-DELs of the bay object for diﬀerent directions θ
and widths ω. In particular, (c) shows the overall landscape μA
E that aggre-
gates all directions, and (e) shows the direction where the target object is the
least enlaced (i.e., for θ = π
3 ). The related directional necessity and possibility
proﬁles are shown in Fig. 6, and the respective fuzzy surrounding evaluations
[N AB
S
, ΠAB
S
] are reported in Table 1.
5.2
Global Enlacement
To pursue our study and to go further the surrounding spatial relation, we
consider in a more generic sense the spatial relation “enlaced by”, in particu-
lar when the reference object has multiple degrees of concavities. Figure 7 (a)
presents a complex spatial conﬁguration involving a spiral and diﬀerent target
objects enclosed into it, from the center of the spiral to its “tail”. The spiral is
the reference object A, and we consider here its Fuzz-DEL μA
E that aggregates
all directions. From this landscape (Fig. 7 (b)), we can observe the decreas-
ing pattern (from white pixels to dark gray pixels) as we shift away from the
center of the spiral. To assess this behavior, Fig. 7 (c) presents the intervals
[N(μA
E , μB), Π(μA
E , μB)] measuring the global enlacement for the diﬀerent tar-
get objects inside the spiral. Note that other surrounding approaches cannot
take into account the depth within the spiral. For instance, the approach of
[16] provides the same evaluations for the green, blue, yellow and pink objects
(i.e., around 0.50), because it does not consider the reference object as a whole,
but only looks at the visible concavities from the target object.
1 Thanks to the CNES agency and the Kalideos project (http://kalideos.cnes.fr/).

Fuzzy Directional Enlacement Landscapes
181
Fig. 8. Examples of fuzzy interlacement landscapes (mapped into a “heat” color scale)
obtained for diﬀerent images (in (b), white: object A; gray: object B). (Color ﬁgure
online)
5.3
Towards Fuzzy Interlacement Landscapes
We also propose some preliminary results on interlacement landscapes. The term
interlacement is intended as a mutual enlacement of two objects. If we aggregate
all directions, a fuzzy interlacement landscape between two objects A and B can
be obtained by: μAB
I
= μA
E +μB
E . Fig. 8 shows the fuzzy interlacement landscapes
obtained for two illustrative images, which have been respectively segmented into
3 classes. The ﬁrst landscape is obtained from an image of a zebra whose coat
features an alternating stripes pattern. We can observe the high interlacement
values concentrated in the center of the animal’s coat. The second landscape is
obtained from a ASTER satellite image2 covering a large delta river. Notice that
the interlacement is mainly located around the ramiﬁcations between the river
and the mangrove. Such interlacement visualization could be useful for instance
for ecological landscape monitoring.
6
Conclusion
We introduced a generic fuzzy model for the evaluation of complex spatial con-
ﬁgurations of binary objects represented in images. In particular, we focused on
the enlacement spatial relation, which can be considered as a generalization of
the notions of surrounding and imbrication of objects. Based on the directional
enlacement model [6], our proposed evaluation approach exploits the concept of
fuzzy landscapes to assess the enlacement of objects in the image space from
a local point of view. An experimental study carried out on diﬀerent illustra-
tive examples highlighted the interest of this model to evaluate complex spatial
2 U.S./Japan ASTER Science Team, NASA/GSFC/METI/ERSDAC/JAROS.

182
M. Cl´ement et al.
relations. In future works, we plan to further study how to exploit fuzzy interlace-
ment landscapes, in particular with overlapping objects. We also plan to extend
the model by integrating a measure of spacing in interlacement conﬁgurations,
allowing to better take into account the distance between the objects.
References
1. Allen, J.F.: Maintaining knowledge about temporal intervals. Commun. ACM
26(11), 832–843 (1983)
2. Bloch, I.: Fuzzy relative position between objects in image processing: a morpho-
logical approach. IEEE Trans. Pattern Anal. Mach. Intell. 21(7), 657–664 (1999)
3. Bloch, I.: Fuzzy spatial relationships for image processing and interpretation: a
review. Image Vis. Computing 23(2), 89–110 (2005)
4. Bloch, I., Colliot, O., Cesar, R.M.: On the ternary spatial relation “Between”.
IEEE Trans. Syst. Man Cybern. B Cybern. 36(2), 312–327 (2006)
5. Cl´ement, M., Kurtz, C., Wendling, L.: Bags of spatial relations and shapes features
for structural object description. In: Proceeding of ICPR (2016)
6. Cl´ement, M., Poulenard, A., Kurtz, C., Wendling, L.: Directional enlacement his-
tograms for the description of complex spatial conﬁgurations between objects.
IEEE Trans. Pattern Anal. Mach. Intell. (2017, in press)
7. Colliot, O., Camara, O., Bloch, I.: Integration of fuzzy spatial relations in
deformable models - application to brain MRI segmentation. Pattern Recogn.
39(8), 1401–1414 (2006)
8. Delaye, A., Anquetil, E.: Learning of fuzzy spatial relations between handwritten
patterns. Int. J. Data Min Model. Manage. 6(2), 127–147 (2014)
9. Freeman, J.: The modelling of spatial relations. Comput. Graph. Image Process.
4(2), 156–171 (1975)
10. Lom´enie, N., Racoceanu, D.: Point set morphological ﬁltering and semantic spa-
tial conﬁguration modeling: application to microscopic image and bio-structure
analysis. Pattern Recogn. 45(8), 2894–2911 (2012)
11. Matsakis, P., Keller, J.M., Wendling, L., Marjamaa, J., Sjahputera, O.: Linguistic
description of relative positions in images. IEEE Trans. Syst. Man Cybern. B
Cybern. 31(4), 573–88 (2001)
12. Matsakis, P., Naeem, M.: Fuzzy models of topological relationships based on the
PHI-descriptor. In: Proceeding of FUZZ-IEEE, pp. 1096–1104 (2016)
13. Matsakis, P., Naeem, M., Rahbarnia, F.: Introducing the Φ-descriptor - a most
versatile relative position descriptor. In: Proceeding of ICPRAM, pp. 87–98 (2015)
14. Matsakis, P., Wendling, L.: A new way to represent the relative position between
areal objects. IEEE Trans. Pattern Anal. Mach. Intell. 21(7), 634–643 (1999)
15. Rosenfeld, A., Klette, R.: Degree of adjacency or surroundedness. Pattern Recogn.
18(2), 169–177 (1985)
16. Vanegas, M.C., Bloch, I., Inglada, J.: A fuzzy deﬁnition of the spatial relation
“surround” - application to complex shapes. In: Proceeding of EUSFLAT, pp.
844–851 (2011)
17. Vanegas, M.C., Bloch, I., Inglada, J.: Alignment and parallelism for the description
of high-resolution remote sensing images. IEEE Trans. Geosci. Remote Sens. 51(6),
3542–3557 (2013)

Discrete Modelling and Visualization

An Introduction to Gamma-Convergence
for Spectral Clustering
Aditya Challa1(B), Sravan Danda1, B.S. Daya Sagar1, and Laurent Najman2
1 Systems Science and Informatics Unit, Indian Statistical Institute, 8th Mile,
Mysore Road, Bangalore 560059, India
aditya.challa.20@gmail.com
2 Universit´e Paris-Est, LIGM, Equipe A3SI, ESIEE, Paris, France
Abstract. The problem of clustering is to partition the dataset into
groups such that elements belonging to the same group are similar and
elements belonging to the diﬀerent groups are dissimilar. The unsuper-
vised nature of the problem makes it widely applicable and also tough
to solve objectively. Clustering in the context of image data is referred
to as image segmentation. Distance based methods such as K-means fail
to detect the non-globular clusters and hence spectral clustering was
proposed to overcome this problem. This method detects the non glob-
ular structures by projecting the data set into a subspace, in which the
usual clustering methods work well. Gamma convergence is the study of
asymptotic behavior of minimizers of a family of minimization problems.
Such a limit of minimizers is referred to as the gamma limit. Calculating
the gamma limit for various variational problems has been proved use-
ful - giving a diﬀerent algorithm and insights into why existing methods
work. In this article, we calculate the gamma limit of the spectral clus-
tering methods, analyze its properties, and compare them with minimum
spanning tree based clustering methods and spectral clustering methods.
1
Introduction
The problem of clustering is deﬁned as - given a set of elements {xi}, partition
the set into non overlapping groups such that elements belonging to the same
group are “similar”, and elements belonging to diﬀerent groups are “dissimilar”.
The importance of a solution to the problem of clustering is due to its wide range
of applications [1,14,15]. Clustering in image data is also referred to as image
segmentation. There exists several methods to solve the problem of clustering
[2,10,11,18]. A comprehensive textbook on the subject is [1]. One of the most
commonly used clustering methods is K-means [2,17]. However, it suﬀers from
the problem of not being able to detect the non-globular structures. Spectral
clustering methods were proposed to overcome this problem. Loosely speaking
spectral clustering methods embed the data in a lower dimensional subspace,
in which usual methods K-means clustering would be able to detect the non
globular clusters as well.
Recently, in [6], seeded clustering/segmentation methods in [7,8,12,24] were
extended by taking the limit of minimizers. This is referred to as the Γ-limit [19].
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 185–196, 2017.
DOI: 10.1007/978-3-319-66272-5 16

186
A. Challa et al.
Γ-convergence is a tool to study the asymptotic behavior of families of minimum
problems [3]. The aim of Γ-convergence is to replace a family of minimum prob-
lems with a single problem whose minima exhibits some interesting properties.
For instance in [6] it has been shown that the Γ-limit revealed a new segmenta-
tion method which performs at least as well as graph cuts, random walker and
shortest paths, if not better.
In this article our aim is to calculate the Γ-limit of the ratio cut spectral clus-
tering. The Γ-limit of the ratio cut is referred to as PRcut and an algorithm to
calculate the PRcut has been proposed. Due to various numerical precision errors
and other constraints, a variant of the algorithm is implemented instead. Thus,
during the exposition, we stick to the philosophy that theory is developed to be
as general as possible, while the experiments are conducted with slightly modi-
ﬁed theory based on practical considerations such as “small” clusters described
later.
2
Background
Let {vi} be the given set of points in Rn which we would like to cluster.
Taking each of these points as vertices, one can construct a similarity graph
G = (V, E, W) with vertex set V , edge set E, and W : E →R+ denotes weights
assigned to each edge. Here R+ denotes the set of positive real numbers. With
slight abuse of notation, we can write the weights as a |V | × |V | matrix, with
wij denoting the edge weight between vi and vj. The degree of a vertex, di is
given by
di =

j
wij
(1)
Let D be the diagonal matrix diag(d1, d2, · · · , dn). The Laplacian of a graph is
then deﬁned by
L = D −W
(2)
We know that the Laplacian is a symmetric positive-semi deﬁnite matrix, and
hence has non negative real eigenvalues, represented by 0 = λ0 ≤λ1 ≤· · · ≤
λn−1 [16]. The corresponding eigenvectors are denoted by (e0, e1, · · · , en−1). Let
A ⊆V . Then the vector 1A(x) is given by
1A(x) =

1
if x ∈A
0
otherwise
(3)
Let p be a real parameter. Let W (p) be the matrix such that W (p)
ij
= wp
ij. Let D(p)
denote the matrix as constructed in (1) with weights wp
ij. Let L(p) = D(p)−W (p).
2.1
Spectral Clustering
This section brieﬂy reviews spectral clustering methods. For more details please
refer to [20,23,25]. As noted before, spectral clustering methods work by embed-
ding the data into a lower dimensional subspace. The three main steps are - (1)

An Introduction to Gamma-Convergence for Spectral Clustering
187
Given a set of points {vi} (dataset), construct a similarity graph with each point
as a vertex. (2) Construct the Laplacian for the obtained graph and calculate
the ﬁrst k eigenvectors. The value of k is ﬁxed based on the number of clusters
one would like to obtain. Let K be the matrix such that the ith column of K is
the ith eigenvector ei−1. (3) Using rows of the matrix K as new representation of
the points vi, use traditional clustering methods such as K-means to obtain the
ﬁnal clusters. Note that as a part of K-means step, the algorithm is run several
times with random initialization of seeds.
Why does spectral clustering work? Although the deﬁnitive answer to this
question still remains open, there exists several analyses which provide insights
into this question [25]. One approach is to interpret the spectral clustering in an
optimization framework. One of the measures to validate the appropriateness of
the clusters is
cut(A1, A2, · · · , Ak) = 1
2
k

i=1
W(Ai, Ai)
(4)
where W(A, B) = 
i∈A;j∈B wij, A denotes the complement of A in the vertex
set V . cut(., .) measures how dissimilar the clusters are by taking the sum of
the weights of the edges connecting distinct clusters. In practice minimizing
the cut(., .) does not give good results, since it generally separates one vertex,
and gives degenerate solutions. To solve this, it was proposed to use a slight
modiﬁcation of the above cost function. Ratio-cut, [25], is given by
Ratio −cut(A1, A2, · · · , Ak) = 1
2
k

i=1
W(Ai, Ai)
|Ai|
(5)
where |Ai| is the cardinality of set Ai. It can be shown that minimizing the
Ratio−cut(., .) for k clusters is approximately equivalent to solving the following
optimization problem.
minimize
H∈Rn×k
Tr(HtLH)
subject to
HtH = I
(6)
Here I is the identity matrix and L is the laplacian as deﬁned in (2). From the
Rayleigh-Ritz theorem [16] we know that the solution to this optimization problem
is obtained by considering the ﬁrst k eigenvectors of L as columns of H.
2.2
Gamma Convergence
Let min{Fp(x) : x ∈X ⊂Rn} be a family of minimum problems. Let x∗
p be a
minimum of Fp(x). We are interested in calculating the limit
x∗= lim
p→∞arg min Fp
(7)
In other words, we are interested in the limit of a sequence of minimizers of the
family {Fp(x)}. Note that there could be many such sequences. Now, consider a
special case where

188
A. Challa et al.
Fp(x) =
n

i=1
αp
i Qi(x)
(8)
where 1 ≥αn > αn−1 > · · · > α1 > 0., and Qi(x) are smooth functions. We also
assume that there exists a compact set C such that x∗
p belongs to C for all p.
It turns out that, in this case, one can ﬁnd a simple algorithm to calculate the
limit of minimizers as described in the following Theorem 1 [19]. Deﬁne
Mn = arg min Qn(x)
x ∈C
(9)
Recursively deﬁne for k = n −1, n −2, · · · , 1
Mk = arg min Qk(x)
x ∈Mk+1
(10)
Theorem 1. Let Fp(x) be as deﬁned in (8) and x∗be the Γ-limit. Then x∗∈M1.
Refer to [19] for proof of Theorem 1. The main consequence of Theorem 1 is
that it provides a method to calculate an “approximate” Γ-limit. One starts at
the highest scale, optimizes the cost function at this scale, then moves on to the
lower scale and repeats the process. Theorem 1 essentially states that one can
obtain an approximate solution to the Γ-limit by this process. Approximate, in
the sense that the Γ-limit belongs to the set M1. The question of how good is
the approximation, needs to be analyzed in the speciﬁc case.
3
Gamma Limit of Ratio - Cut
In this section, we calculate the approximate Gamma limit of the Ratio-cut. A
few more notations are required. For a given graph G, let Gk denote the graph
(V, Ek, Wk). Ek ⊆E denotes the set of edges whose weight is wk. According to
the existence of the edge with weight wk, Wk takes the values in {0, 1}. We refer
to this graph as scale graph/level graph at level k. Just as for the original graph,
one can construct a laplacian, Lk, for Gk. A point to observe is that the entries
in weight matrices of a level graph are either 1 or 0. We assume that the graph
has distinct weights w1 < w2 < · · · < wj, where j < |E|. Given the notation as
above, we have
Tr(HtLH) =
j

k=1
wkTr(HtLkH)
(11)
Drawing a parallel with the Γ-convergence framework, we have that Qk(H) =
Tr(HtLkH) and αk = wk. We are thus interested in calculating the limit of
minimizers of j
k=1 wp
kTr(HtLkH) as p →∞, subject to HtH = I
Let Pk denote the following optimization problem.
minimize
H∈Rn×m
Tr(HtLkH)
subject to
HtH = I
(12)
Thanks to Theorem 1, we have the following method to calculate the Γ-limit
for spectral clustering.

An Introduction to Gamma-Convergence for Spectral Clustering
189
1. Let G be a graph with distinct weights w1 < w2 < · · · < wj. Let Mj+1 be the
set of the all n × m matrices.
2. For each k going from j to 1, let Mk be the set of solutions for arg min Pk
which belong to Mk+1.
The set M1 is the output of the method. Note that the above steps are not
implementable.
The main problem in ﬁnding an implementable version is to characterize all
the solutions for the problem in (12). Let λ(m) denote the mth smallest eigen-
value. Let Ak be the matrix [e1, e2, · · · el], where ei is the ith eigenvector, ordered
in increasing order of the corresponding eigenvalue. The number of eigenvectors
considered, l, are the number of eigenvalues less than or equal to λ(m). Let K
be the matrix
K =
Il1×l1
0
0
kl2×m−l1

(13)
where KtK = I. l2 is the number of eigenvalues equal to λ(m), and l = l1 + l2.
Let X be an orthogonal matrix such that XtX = I. Then,
Algorithm 1. Eﬃcient algorithm to compute Γ-limit for ratio-cut.
Input: A weighted graph, G, with distinct weights w1 < w2 < · · · < wj. Number of clusters,
m.
Output: N - A representation of the subspace spanned by the Γ-limit of the minimizers.
1: Set k := j.
2: while Number of connected components of G≥k is greater than or equal to m do
3:
Set k := k −1 {We refer to this as an MST-Phase}
4: end while
5: Construct N by stacking the vectors 1Ai/

|Ai| in columns, where Ai is a connected
component of G≥k.
6: Set l1 := 0 and l2 := number of connected components in G≥k
7: Consider the graph Gk and let Lk be the corresponding laplacian.
8: Set C = [NtLkN]l2,l2
9: Calculate the ﬁrst eigenvectors of eigenvalue problem whose eigenvalue is less than or equal
to λ(m).
Cx = λx
(14)
10: Let A be the matrix obtained by stacking the eigenvectors as columns.
11: Construct ˆ
A as
ˆ
A =
Il1×l1 0
0
A

(15)
12: Update l1 and l2.
13: N := N × ˆ
A
14: Set k := k −1
15: if k = 0 or number of columns of N is equal to m then
16:
return N
17: else
18:
Goto Step (7)
19: end if

190
A. Challa et al.
Proposition 1. The set of all solutions to the optimization problem in (12) is
of the form AkKX.
Proposition 1 results in an algorithm which is implementable. However, one
requires to calculate all the eigenvectors at every stage, which is computationally
expensive. Proposition 2 results in an eﬃcient implementation of the algorithm
as given in Algorithm 1. We refer the result obtained by this algorithm as Power
Rcut. Also, G≥k denotes the graph with vertex set V and edge set E≥k = ∪i≥kEi.
Proposition 2. Given a graph G, Let G≥k denote the graph, whose vertex set
is V and edge set Ek containing all the edges whose weight is greater than or
equal to wk. At stage k, if A is a maximal connected component of G≥k, then 1A
is an eigenvector with eigenvalue 0 of the optimization problem (12).
The proofs for the propositions and details of simpliﬁcation to obtain the
eﬃcient Algorithm 1 will be discussed in detail in a complete version of the
paper [4]. Observe that Algorithm 1 only gives an element of the set to which
the Γ-limit actually belongs. The appropriateness of the solution thus obtained
must be proved, which Proposition 3 gives.
Proposition 3. Let x be the solution obtained by Algorithm 1 and let x∗be a
Γ-limit. Then
Fp(x∗) = Fp(x)
for all p
(16)
Where Fp is as given in (8), where Qi(x) = Tr(xtLix).
The above proposition implies that any Γ-limit, and the approximate one calcu-
lated have the same cost. In the context of spectral clustering, this implies that
the solutions are equivalent and thus the solution obtained from Algorithm 1 is
a good approximation. The proof of the above proposition will be detailed in
later work.
4
Analysis with Experiments
In the rest of the article, we discuss how the algorithm works, its practical imple-
mentation and its similarities and dissimilarities with the MST based clustering
and spectral clustering methods.
4.1
How the Algorithm Works?
Recall that the output of spectral clustering is a projection onto a subspace,
and thus the algorithm produces a representation of the points in a subspace
(denoted by N in Algorithm 1) which is a gamma limit. We assume that we need
m clusters. In steps 2–4, we progressively add all the edges while there are at least
m connected components in the threshold graph G≥k. Thanks to Proposition 2
we know the ﬁrst eigenvectors of the laplacian of the threshold graph are the

An Introduction to Gamma-Convergence for Spectral Clustering
191
indicator vectors as in (3) where A denotes each of the connected components.
We construct the initial representation of the points N by taking the eigenvec-
tors in step 5. Steps 7–14 update the representation N with respect to the lower
weight edges. Note that, once the number of columns of the matrix N is equal
to m, we need not update the representation anymore, since any other represen-
tation would just be an orthogonal transformation of the points and thus the
clustering results would not change. This condition is checked in steps 15–19.
One issue with the gamma limit is that the property of non-trivial clusters for
the Ratio-cut is not preserved in the limit. In the above algorithm, in practice,
we get a lot of outliers and this results in small clusters. To avoid this, in steps 2–
4, instead of calculating the number of connected components, we calculate the
number of connected components whose size is greater than a given parameter
threshComp. At the moment the algorithm jumps out of the while loop, all the
components which have less than threshComp number of vertices are ignored.
Recall that after the representation of the points is obtained, one has to perform
K-means clustering to get the ﬁnal partition. At this stage the ignored “small”
clusters can either be returned as a diﬀerent cluster, or combined with one of
the larger clusters at random.
A simple application of the algorithm is illustrated in Fig. 1, where one can see
that the algorithm correctly detects the object (ﬂower in this case). Since small
clusters are ignored, for the applications of image segmentation we do not get
closed contours for the segments. Thus, for image segmentation the clusters are
post-processed with an operator. Observe in Fig. 1(b) a few parts of the ﬂower are
missing. In this case, simple operators such as an opening works well, which gives
the result as in Fig. 1(c). In general connected operators [22] preserve contours
and are better for the post-processing of the image. Another important property
of the Power Rcut clustering method is that, it results in smoother contours
compared to the spectral clustering method. This is illustrated in Fig. 1(d)–(f).
4.2
Relation to MST-clustering
MST (Maximum Spanning Tree) based clustering is one of the earliest graph
based clustering approach [5,13,21,26]. There exists several variations of the
method. We consider here the simplest method - (a) Construct an MST (b)
Iteratively remove the least weight edges until we get the required number of
clusters. One of the most useful properties of spectral clustering is its ability
to detect non-convex clusters in the data. This property is shared by the MST
based clustering methods as well.
However, the problem with MST based clustering is breaking ties between
edges of equal weight, which it does arbitrarily. Spectral clustering on the other
hand ensures a single clustering (up to the arbitrariness of k-means step). In this
sense, Power Rcut can be considered to be a method between these two cluster-
ing methods. Power Rcut follows a similar procedure as MST based clustering,
and it breaks ties with spectral clustering on a subgraph. For example consider
the graph in Fig. 2(a). Power Rcut segments the graph into two equal clusters,
Fig. 2(b). The same behavior is also exhibited by spectral clustering. MST based

192
A. Challa et al.
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 1. (a) Original Image (Flower) (b) Power Rcut segment result (c) Power Rcut seg-
ment result post processing with opening (d) Original Image (Cameraman) (e) Con-
tours obtained by Power R cut and (f) Contours obtained by Normalized spectral
clustering.
a
b
1
c
1
d
1
e
1
f
1
(a)
a
b
1
c
1
d
e
1
f
1
(b)
a
b
1
c
d
1
e
1
f
1
(c)
Fig. 2. (a) Basic Graph (b) Power Rcut Clustering (c) MST based clustering. Observe
that Power Rcut splits the graph into two equal halves, where as MST based clustering
does not.
clustering on the other hand segments the graph into non-equal parts since it
breaks the ties arbitrarily, Fig. 2(c).
This is because, Power Rcut takes into consideration the sizes of the cluster
while breaking ties. Consider another synthetic example in Fig. 3(a). An MST is
highlighted in bold edges in Fig. 3(a). Since MST based clustering would break
the ties randomly, it could result in clusters as in Fig. 3(b). Power Rcut clustering
on the other would deﬁnitely not give clusters as in Fig. 3(b). An example Power
Rcut clusters is given in Fig. 3(c).

An Introduction to Gamma-Convergence for Spectral Clustering
193
a
b
2
c
2
d
1
g
1
2
e
2
f
2
1
2
(a)
a
b
2
d
1
c
2
e
2
f
2
g
(b)
a
b
2
g
1
c
2
d
e
2
f
2
(c)
Fig. 3. (a) Basic Graph and MST edges highlighted in bold (b) Clustering with MST
(c) Power Rcut Clustering. Observe that Power Rcut clustering tries to split the graph
into equal parts.
Proposition 4. If at a threshold t, we have that G≥t has exactly m clusters,
then MST clustering and Power Rcut clustering results in the same clusters.
Power Rcut clustering and MST based clustering are in fact closely related as
suggested by the Proposition 4. The proof follows from the following observation
- for any two vertices belonging to the same connected component has the same
value in the embedded space, and hence belong to the same cluster. Since, we
require m clusters, and there are m connected components, the k-means step
results in each of these m components as a cluster.
4.3
Relation to Spectral Clustering
The Power Rcut solution can also be interpreted as being obtained by spectral
clustering on every level graph Gk. Since, the ﬁrst few eigenvectors are the indi-
cator of the connected components, this gives a heuristic explanation for steps
2–4 in Algorithm 1. This points out the similarity between Power Rcut solution
and spectral clustering.
In low noise conditions, spectral clustering and Power Rcut clustering results
are similar. However, as noise level increases, spectral clustering will not be able
to identify the regions anymore. In Fig. 4(a),(b) and (c) data points are sampled
from two concentric circles with noise. Figure 4(c) shows the results obtained by
spectral clustering are shown. Notice that the structure of the two circles is not
preserved. Figure 4(a) show the results obtained with Power Rcut. To generate
the results of Power Rcut, as a post processing step, we assign each of the points
the “small” clusters to the closest cluster, which results in Fig. 4(b). Although
not perfect, Power Rcut results preserve some structure. For a quantitative view
we calculate the Fowlkes-Mallows(FW) scores, given by the formula
Score =
TP

(TP + FP)

(TP + FN)
(17)
where TP is true positives, FP is false positives and FN is false negatives. The
FW scores for various noise levels is plotted in Fig. 4(d). Note that Power Rcut
performs better in high noise scenarios compared to spectral clustering.

194
A. Challa et al.
(a)
(b)
(c)
(d)
Fig. 4. Data is sampled from two concentric circles with noise. The clusters obtained
are represented by red dots and blue circles. (a) Power Rcut clustering. The green
stars indicate the “noise” clusters obtained. (b) Power Rcut clustering obtained by
adjusting the noise clusters to the nearest cluster. (c) Spectral clustering (d) Fowlkes-
Mallows scores for varying noise. The dashed line indicates spectral clustering score
and continuous line indicate Power Rcut score. The higher the score, the better the
procedure.
5
Conclusion and Future Work
In this article we outlined the basics of spectral clustering and discussed the
concept of Γ-convergence. Important results to calculate the Γ-limit of Ratio-
cut were outlined and the algorithm to calculate the gamma limit was obtained.
The correctness of the algorithm was shown via a proposition. The similarities
and dissimilarities between Power Rcut, MST clustering and Spectral clustering
are analyzed stating the result that Power Rcut clustering is a speciﬁc kind
of MST clustering. Power Rcut clustering was shown to be superior to MST
clustering in dealing with ramp eﬀects of the image, and superior to spectral
clustering in noisy scenarios.

An Introduction to Gamma-Convergence for Spectral Clustering
195
Note that none of the steps in spectral clustering methods indicate directly
as to why spectral clustering methods could obtain non-convex clusters. This
has been an open question for a long time. We believe that, the fact that gamma
limit of spectral clustering is a method close to MST clustering provides an
insight into this question. In particular, this provides a bridge between spectral
clustering and MST clustering and allows us to dive into the question of why
spectral methods work.
Note that the algorithm starts with combining the edges of highest weight
until the number of clusters are obtained. This can be interepreted as a greedy
method of “combining all the points which deﬁnitely belong to the same cluster”,
thus reducing the size of the dataset and allowing for faster computation. This
in theory can also be done in parallel. This can allow spectral clustering to be
applicable in the case of large datasets and also have eﬃcient implementations.
The error bounds and the exact algorithm to do this are a subject of future
research.
In [9], the authors study in depth hierarchies and their equivalence with
MST. Observe that the Algorithm 1 is inherently hierarchical. The question of
how Algorithm 1 is related to the concepts in [9] is also a subject of future
research.
Acknowledgements. AC and SD would like to thank Indian Statistical Institute.
This work has been partly funded by ANR-15-CE40-0006 CoMeDiC and ANR-14-
CE27-0001 GRAPHSIP research grants. BSDS would like to acknowledge the support
received from the Science and Engineering Research Board (SERB) of the Department
of Science and Technology (DST) with the grant number EMR/2015/000853, and the
Indian Space Research Organization (ISRO) with the grant number ISRO/SSPO/Ch-
1/2016-17.
References
1. Aggarwal, C.C., Reddy, C.K.: Data Clustering: Algorithms and Applications, 1st
edn. Chapman & Hall/CRC, Boca Raton (2013)
2. Arthur, D., Vassilvitskii, S.: k-means++: the advantages of careful seeding. In:
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algo-
rithms, pp. 1027–1035. Society for Industrial and Applied Mathematics (2007)
3. Braides, A.: Gamma-convergence for Beginners, vol. 22. Clarendon Press (2002)
4. Challa, A., Danda, S., Daya Sagar, B.S., Najman, L.: Power spectral cluster-
ing. Technical report, Universit´e Paris-Est, LIGM, ESIEE Paris (working paper).
https://hal.archives-ouvertes.fr/hal-01516649/
5. Chowdhury, N., Murthy, C.: Minimal spanning tree based clustering technique:
relationship with bayes classiﬁer. Pattern Recogn. 30(11), 1919–1929 (1997)
6. Couprie, C., Grady, L., Najman, L., Talbot, H.: Power watershed: a unifying graph-
based optimization framework. IEEE Trans. Pattern Anal. Mach. Intell. 33(7),
1384–1399 (2011)
7. Cousty, J., Bertrand, G., Najman, L., Couprie, M.: Watershed cuts: minimum
spanning forests and the drop of water principle. IEEE Trans. Pattern Anal. Mach.
Intell. 31(8), 1362–1374 (2009)

196
A. Challa et al.
8. Cousty, J., Bertrand, G., Najman, L., Couprie, M.: Watershed cuts: thinnings,
shortest path forests, and topological watersheds. IEEE Trans. Pattern Anal. Mach.
Intell. 32(5), 925–939 (2010)
9. Cousty, J., Najman, L., Kenmochi, Y., Guimares, S.: Hierarchical segmenta-
tions with graphs: quasi-ﬂat zones, minimum spanning trees, and saliency maps.
Research report, LIGM, July 2016. https://hal.archives-ouvertes.fr/hal-01344727
10. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete
data via the EM algorithm. J. Roy. Stat. Soc, Ser. B (Methodol.) 39(1), 1–38
(1977)
11. Ester, M., Kriegel, H.P., Sander, J., Xu, X., et al.: A density-based algorithm for
discovering clusters in large spatial databases with noise. KDD 96, 226–231 (1996)
12. Grady, L.: Random walks for image segmentation. IEEE Trans. Pattern Anal.
Mach. Intell. 28(11), 1768–1783 (2006)
13. Grygorash, O., Zhou, Y., Jorgensen, Z.: Minimum spanning tree based clustering
algorithms. In: 2006 18th IEEE International Conference on Tools with Artiﬁcial
Intelligence (ICTAI 2006), pp. 73–81. IEEE (2006)
14. Jain, A.K.: Data clustering: 50 years beyond k-means. Pattern Recogn. Lett. 31(8),
651–666 (2010)
15. Jain, A.K., Murty, M.N., Flynn, P.J.: Data clustering: a review. ACM Comput.
Surv. (CSUR) 31(3), 264–323 (1999)
16. L¨utkepohl, H.: Handbook of Matrices, 1st edn. Wiley, Chichester (1997)
17. MacQueen, J., et al.: Some methods for classiﬁcation and analysis of multivariate
observations. In: Proceedings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, Oakland, CA, USA, vol. 1, pp. 281–297 (1967)
18. McLachlan, G., Peel, D.: Finite Mixture Models. Wiley, New York (2004)
19. Najman, L.: Extending the powerwatershed framework thanks to Γ-convergence.
Technical report, Universit´e Paris-Est, LIGM, ESIEE Paris (to appear in
SIAM Journal on Imaging Sciences). https://hal-upec-upem.archives-ouvertes.fr/
hal-01428875
20. Ng, A.Y., Jordan, M.I., Weiss, Y., et al.: On spectral clustering: analysis and an
algorithm. Adv. Neural Inform. Process. Syst. 2, 849–856 (2002)
21. Peter, S.J.: Minimum spanning tree-based structural similarity clustering for image
mining with local region outliers. Int. J. Comput. Appl. (0975–8887) Volume (2010)
22. Serra, J.: A lattice approach to image segmentation. J. Math. Imag. Vis. 24(1),
83–130 (2006)
23. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 22(8), 888–905 (2000)
24. Sinop, A.K., Grady, L.: A seeded image segmentation framework unifying graph
cuts and random walker which yields a new algorithm. In: IEEE 11th International
Conference on Computer Vision, ICCV 2007, pp. 1–8. IEEE (2007)
25. Von Luxburg, U.: A tutorial on spectral clustering. Stat. Comput. 17(4), 395–416
(2007)
26. Zahn, C.T.: Graph-theoretical methods for detecting and describing gestalt clus-
ters. IEEE Trans. Comput. 100(1), 68–86 (1971)

Digital Surface Regularization by Normal Vector
Field Alignment
David Coeurjolly1(B), Pierre Gueth1, and Jacques-Olivier Lachaud2
1 Universit´e de Lyon, CNRS, LIRIS UMR 5205, 69622 Lyon, France
david.coeurjolly@liris.cnrs.fr
2 Laboratoire de Math´ematiques, CNRS, UMR 5127,
University Savoie Mont Blanc, Chambery, France
Abstract. Digital objects and digital surfaces are isothetic structures
per se. Such surfaces are thus not adapted to direct visualization with
isothetic quads, or to many geometry processing methods. We propose
a new regularization technique to construct a piecewise smooth quad-
rangulated surface from a digital surface. More formally we propose a
variational formulation which eﬃciently regularizes digital surface ver-
tices while complying with a prescribed, eventually anisotropic, input
normal vector ﬁeld estimated on the digital structure. Beside visualiza-
tion purposes, such regularized surface can then be used in any geometry
processing tasks which operates on triangular or quadrangular meshes
(e.g. compression, texturing, anisotropic smoothing, feature extraction).
1
Introduction
Objective. This paper addresses the problem of approximating the boundary
of an object given as a 3d binary image (see Fig. 4 top for an example). Such
input data is called a 3d digital object and its surface is called a digital surface.
They arise as digitizations of continuous objects and segmentation of 3d images.
Compared to gray-level volumetric images, such binary image model is much
more complex to handle: the function takes only two values, it is not continuous
and of course has no gradient, it is known only at regularly sampled places.
For this data, direct use of former methods gives triangulated meshes with poor
geometry (staircasing eﬀects, few possible orientations).
Contribution. We propose an original method to construct a piecewise smooth
approximation of the boundary of a digital object. It follows a convex variational
principle that regularizes the digital surface according to three criteria: First, it
must stay close to the input data; second, it should comply to a well-chosen
normal vector ﬁeld u; third, its cells must be as regular as possible. Our method
is not iterative and provides excellent piecewise smooth reconstructions when
the normal vector ﬁeld u is a good approximation of the normal vector ﬁeld of
the original continuous object. This is why we use recent digital normal vec-
tor estimators which oﬀer multigrid convergence guarantees [5,9] and may also
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 197–209, 2017.
DOI: 10.1007/978-3-319-66272-5 17

198
D. Coeurjolly et al.
detect normal discontinuities [4]. By this way, we are able to take into account
the discrete and arithmetic nature of digital objects while using the powerful
framework of convex optimization. Experiments show that the output surface is
aligned with the prescribed normal vector ﬁeld and is closer to the underlying
continuous object than was the input digitized surface. Sharp features are nicely
delineated and the quality of mesh faces is very good.
Related works. First of all, the numerous methods that extract isosurfaces from
a volume image or function provide poor results with such input data, since
they rely on the exact value of the image /function to determine the position
of vertices. This includes marching cubes, dual marching cubes and the many
variants designed to reconstruct meshes from implicit surfaces [12–14]. Standard
mesh denoising methods could also be considered for removing staircasing eﬀects
of digital surfaces [8,15–17]. However, they tend either to consider all steps as
features or to smooth everything out. Very few approaches take into account the
peculiar nature of digital data. In 2d, we may quote early works for digital con-
tour polygonalization, which use digital straightness properties to align digital
points onto their estimated tangent line [2]. In 3d, reversible polyhedrization of
digital surfaces can be achieved with greedy digital plane segmentation followed
by Marching-Cubes sewing [3]. Although they are theoretically reversible, none
of these techniques achieve similar visual quality compared to our proposal (see
Fig. 3). Note that we experimentally show that our regularized surface is also
very close to the original digital one.
Outline. Section. 2 gives the necessary notions and notations of digital geometry
used throughout the paper. Section 3 presents the variational formulation of
our digital surface regularization method. The convex optimization algorithm
is described in Sect. 4. Experiments illustrating the qualities of the method are
given in Sect. 5, as well as a quantitative asymptotic analysis of the surface
reconstruction. Limitations and possible extensions of this method are discussed
in Sect. 6.
2
Preliminaries
A 3d binary image is simply a function from Z3 toward {0, 1}. The set Z ⊂Z3
of digital points where the function is not null represents the digital object.
It is naturally embedded in R3 as the union of unit cubes centered on these
digital points. Its topological boundary is called its digital boundary, and is a
special case of digital surface. Since a digital surface is a set of unit squares sewn
together, it has a cellular representation in cartesian cubical grid as a set E0 of
0−cells (vertices), a set E1 of 1−cells (edges), and a set E2 of 2−cells (square
faces). We do not detail the topological combinatorial structure, ´a-la Alexandrov,
associated with subsets of the Cartesian cubic grid (and more speciﬁcally digital
surfaces). Interested readers may refer to [10]. We just assume in the following
that (E0, E1, E2) deﬁnes a proper combinatorial 2-manifold without boundaries.

Digital Surface Regularization by Normal Vector Field Alignment
199
Our objective is to reconstruct a piecewise smooth surface from a digital
boundary. Therefore we have in mind that the digital shape comes from some
digitization or sampling of a continuous shape X, and our objective is to infer the
original shape boundary ∂X with solely its digitization as input. The multigrid
convergence framework is thus well adapted to evaluate objectively the qualities
of our method (e.g. see [10,11]). For some continuous shape X ⊂R3 and a
digitization process parameterized by a sampling grid step h, we will evaluate if
our regularized surface ∂∗
hX (in red in Fig. 1) is closer to the continuous surface
∂X than the digitized boundary ∂hX (in orange). And of course, we wish that the
ﬁner is the resolution (h →0) the better is the approximation. Classical notations
are recalled in Fig. 1. It is already known that, for the Gauss digitization (denoted
Gh(X) in Fig. 1), the digitized boundary ∂hX of a compact subset X of Rd with
positive reach is Hausdorﬀclose to ∂X, with a distance no greater than
√
dh/2
[11]. Hence our regularized surface ∂∗
hX should also be Hausdorﬀclose to ∂X.
In Sect. 5, we show that this is indeed the case, and with a better constant
than
√
3
2 h.
h
X
∂X
Gh(X)
E1
E0
ξ(·)
h
p∗
(p∗−p)
Fig. 1. Illustration of the digitization models and notations in dimension 2. ξ(y) maps
y ∈R2 \ MA(X) to the closest point on ∂X (MA(X) being the medial axis of X).
(Color ﬁgure online)

200
D. Coeurjolly et al.
Furthermore, the normals of the regularized surface ∂∗
hX should also tend
toward the normal of the original continuous surface ∂X. We achieve this prop-
erty by adding as input to our regularization process a normal vector ﬁeld associ-
ated to faces of the input digital surface. By choosing normal estimates given by
some multigrid convergent normal estimator (like [5,9]), our variational formu-
lation makes the normals of the regularized surface ∂∗
hX align with the normals
of ∂X.
3
Variational Formulation
Our variational model will simply move the vertices E0 of the input digital surface
in order to regularize it. Although the cellular topology of the digital surface is
used in the process, the output regularized surface has the same cellular topology
as the input digital surface.
In the following, we denote p := {pi} the canonical coordinates in R3 of the
0−cells {σ0
i } in the embedding Euclidean space. Similarly we introduce p∗:=
{p∗
i } ∈R3 the set of regularized points coordinates in R3 associated with each
0−cell. Furthermore, we denote by u := {uk} the input discrete normal vector
ﬁeld associated with the faces of the cubical complex (vector uk is the estimated
normal of the cell σ2
k). In the following sections, we provide more details about
the structure of these sets allowing us to deﬁne a reliable and eﬃcient calculus
on the digital surface. We also consider S as the embedded quadrangulated
surface associated with the cubical complex (with vertices p, edges induced by
E1 and faces by E2). We denote by S∗the quadrangulated surface with the same
structure as S with vertices p∗. Please note that once vertices are regularized,
the quads of S∗may not be planar anymore. The surface S is, by deﬁnition,
∂hX if we are considering the digitization of a shape X. The surface S∗then
corresponds to ∂∗
hX in this case.
Before formally deﬁning p∗, let us consider a given point set ˆp ⊂R3 and
its associated quadrangulation ˆS. We evaluate the energy associated with this
point set. First, we want ˆp to be close to p (data attachment). Then, we want the
discrete normal vector ﬁeld associated with ˆp to comply with the input vector
ﬁeld u. Since quads associated with ˆS may not be planar, we place this constraint
onto the edges of each quad: we want each edge dˆpj (where dˆpj := ˆpj0 −ˆpj1
with ∂σ1
j = {σ0
j0, σ0
j1}, σ0
j0 being the tip of edge σ1
j and σ0
j1 being the origin of
edge σ1
j )1 to be as orthogonal as possible to each neighboring face normal vector
uk (normal vector alignment). Those constraints are handled by the ﬁrst and
second terms of the following energy function :
E(ˆp) := α ∥ˆp −p∥2



Edata(ˆp)
+ β ∥dˆp · u∥2



Ealign(ˆp)
+ γ ∥ˆp −ˆb∥2



Efairness(ˆp)
.
(1)
1 For readers familiar with Discrete Exterior Calculus (DEC), d is similar to an exte-
rior derivative operator on (triplets of) primal 0−forms of the cubical complex.

Digital Surface Regularization by Normal Vector Field Alignment
201
where ˆbi holds the coordinates of the barycenter of the neighboring vertices of
pi. The last term (fairness) ensures that vertices of ˆS are well distributed along
the surface: it moves the points onto their tangent planes so that the sampling
is as regular as possible. In Sects. 3.1, 3.2 and 3.3, we detail the norms involved
in each term of this functional.
From this formulation, we deﬁne the optimal regularized coordinates p∗as
p∗:= argmin
ˆp
E(ˆp) .
(2)
We detail now each energy term in a discrete calculus setting, which allows
an eﬃcient minimization of (2) (see Sect. 4).
3.1
Data Attachment Term
Since p contains point coordinates, it can be interpreted as a triplet of maps px,
py and pz from E0 to R containing the vertex coordinates in embedding space.
Thanks to a numbering of cells in E0, we use a vector representation of p as a
single column vector concatenating vectors associated with px, py and pz. In
other words p = [pT
x , pT
y , pT
z ]T . Data attachement term keeps p∗close to p and
guarantees the convexity of the problem. It is deﬁned as follows:
Edata(ˆp) := ∥ˆp −p∥2 .
(3)
The norm in (3) is the norm of discrete E0 →R3 maps and is deﬁned from
the scalar products between discrete E0 →R maps ⟨•, •⟩0:
∥ˆp −p∥2 = ∥a∥2 = ⟨ax, ax⟩0 + ⟨ay, ay⟩0 + ⟨az, az⟩0 .
(4)
To shorten notation in the previous equation, a is the diﬀerence map (ˆp −p).
One can simply consider classical Euclidean scalar products between vectors
in R|E0| to deﬁne (4). In Sect. 3.4, we propose an alternative deﬁnition which is
more consistent with discrete calculus on combinatorial structures.
3.2
Normal Vector Alignment Term
The second term is the most complex one and tends to orthogonalize the direc-
tion of each edge with adjacent face normal vectors:
Ealign(ˆp) := ∥dˆp · u∥2 .
(5)
First let us look at dˆp : E1 →R3 (see deﬁnition in Sect. 3). There exists a
linear operator D : (E0 →R3) →(E1 →R3) such that
dˆp = Dˆp .
(6)
Thanks to the linearization of ˆp as a 3|E0| vector, such linear operator can be
represented as a 3|E1|×3|E0| matrix. dˆp·u : E2 →R4 holds the scalar products
for each edge adjacent to all faces in embedding space:
(dˆp · u)k = (dˆpk0 · uk, dˆpk1 · uk, dˆpk2 · uk, dˆpk3 · uk)
(7)

202
D. Coeurjolly et al.
where ∂σ2
k = {σ1
k0, σ1
k1, σ1
k2, σ1
k3} (always four edges in face border).
Furthermore, there exists a linear operator U : (E1 →R3) →(E2 →R4)
such that :
dˆp · u = U D ˆp .
(8)
Again, such operator can be represented as a 4|E2|×3|E1| matrix in our discrete
calculus.
The norm in (5) is the norm of discrete E2 →R4 maps and is deﬁned from
the scalar products between discrete E2 →R maps ⟨•, •⟩2 :
∥dˆp · u∥2 = ∥b∥2 = ⟨bk0, bk0⟩2 + ⟨bk1, bk1⟩2 + ⟨bk2, bk2⟩2 + ⟨bk3, bk3⟩2 .
(9)
In the previous equation, b is a shorthand for dˆp·u. Finally, the alignment term
is thus simply expressed as
Ealign(ˆp) = ∥U D ˆp∥2 .
(10)
3.3
Fairness Term
The last term is the fairness term which tends to ﬂatten the regularized complex
and to distribute the vertex positions with tangential displacements (see Fig. 2):
Efairness(ˆp) := ∥ˆp −ˆb∥2 .
(11)
The norm is the same as in (3) and relies on the same scalar product ⟨•, •⟩0.
ˆbi is the barycenter of neighboring vertices to pi. More formally2:
ˆbi :=
1
|link(σ0
i )|

σ0
j ∈link(σ0
i )
ˆpj .
(12)
This deﬁnes B : (E0 →R3) →(E0 →R3) a linear operator from and to
E0 →R3 (3|E0| × 3|E0| matrix) allowing us to write ˆb, expressed as a linearized
column vector of positions, as the matrix-vector multiplication,
ˆb := Bˆp .
(13)
Note that matrix B is sparse which leads to eﬃcient factorization (see Sect. 4).
The fairness term reduces to
Efairness(ˆp) = ∥(I −B)ˆp∥2 ,
(14)
where I is the identity operator (identity matrix 3|E0| × 3|E0|).
2 Link(σ0
i ) is the link operator on cubical complexes. As a consequence, 0−cells in this
set are connected to σ0
i by a 1−cell in the complex.

Digital Surface Regularization by Normal Vector Field Alignment
203
Fig. 2. Impact of the Efairness term on the regularized quadrangulation (without on the
left, with on the right).
3.4
Scalar Products
In previous deﬁnitions, scalar products ⟨•, •⟩0 and ⟨•, •⟩2 must be speciﬁed.
More precisely, we need to specify the metric tensor (deﬁnite positive, therefore
symmetric matrix) W0 (resp. W2) associated with maps x, y : E0 →R (resp.
maps u, v : E2 →R):
⟨x, y⟩0 := xT W0y ,
(15)
⟨u, v⟩2 := uT W2v .
(16)
A simple choice consists in considering identity matrices for W0 and W2. How-
ever, speciﬁc weights can be set if embedding priors of the digital surface are
known (see [6] for a complete discussion and Sect. 6).
Since we consider triplets or quadruple of respectively 0−and 2−forms in
previous formulations (due to the linearization of positions into a single vector
and the linearization of the four dot products associated with each face edges), we
need to extend scalar products of vector is R|E0| to vectors in R3|E0|(respectively
to vectors in R4|E2|). We simply deﬁne
¯
W0 :=
⎡
⎣
W0
0
0
0
W0
0
0
0
W0
⎤
⎦, ¯
W2 :=
⎡
⎢⎢⎣
W2
0
0
0
0
W2
0
0
0
0
W2
0
0
0
0
W2
⎤
⎥⎥⎦.
(17)
4
Energy Minimization
Since (1) is convex, we can compute p∗as the unique solution to
∇ˆpE(p∗) = α ∇ˆpEdata(p∗) + β ∇ˆpEalign(p∗) + γ ∇ˆpEfairness(p∗) = 0 .
(18)

204
D. Coeurjolly et al.
From (3), (10) and (14) gradients of each energy term can be expressed as follows:
∇ˆpEdata(ˆp) = 2 ¯
W0(ˆp −p) ,
(19)
∇ˆpEalign(ˆp) = 2DT UT ¯
W2UDˆp ,
(20)
∇ˆpEfairness(ˆp) = 2(I −B)T ¯
W0(I −B)ˆp .
(21)
Weighting up all these gradients in Eq. (18) leads to the following linear
system in p∗:
Rp∗= α ¯
W0p ,
(22)
with
R :=

α ¯
W0 + βDT UT ¯
W2UD + γ(I −B)T ¯
W0(I −B)

.
(23)
As long as weights are strictly positive, the operator R is a 3|E0|×3|E0| matrix,
which is symmetric deﬁnite-positive (see Appendix A). The linear system (22) is
thus eﬃciently solved using classical linear algebra solvers [7]. In our experiments,
we have used the following rules to balance the energy terms:
β = 1, 0 < α ≪γ ≪β .
We choose parameters with diﬀerent order of magnitudes to sequence the eﬀect
of the minimization of the three energy terms of Eq. (23). First, we wish to build
a smooth surface leading to the highest value for β, the alignment term. Second,
the α energy term is necessary to achieve uniqueness of the optimization problem.
Its lowest order of magnitude ensures that the result is very close to the set of
solutions of the alignment term alone. Finally, the user has freedom for the γ
energy term, depending on the desired regularity of the output quadrangulation
(see Fig. 2).
5
Experiments
In this section, we evaluate the quality of the regularization. We have considered
two diﬀerent normal vector ﬁelds as input: the ﬁrst one is given by Integral Invari-
ant approaches [5] and is known to be multigrid convergent. The second corre-
spond to a piecewise smooth reconstruction of the normal vector ﬁeld using [4].
This approach performs a normal vector ﬁeld regularization while detecting and
preserving sharp features. The discrete operators have been implemented using
the DGtal DEC package [1] with Eigen backend for the linear algebra solver [7].
In Fig. 4 we ﬁrst illustrate the overall regularization on a Standford-bunny
object using an Integral Invariant based normal vector ﬁeld. In Fig. 5, we show
that using an anisotropic, piecewise smooth normal vector ﬁeld from [4], the
regularized surface is able to capture sharp features. In Fig. 6, we demonstrate
the robustness of the regularization in presence of noise. Note that beside the
fact that the input normal vector ﬁeld is robust for the alignment term, the

Digital Surface Regularization by Normal Vector Field Alignment
205
Fig. 3. Regularization example on a digital sphere (r = 10). From left to right the
original Marching-Cubes surface, the simpliﬁed one using [3] and our regularization.
Fig. 4. Regularization example on a 1283 Stanford-bunny. From left to right, input
digital surface and regularized surface using the input normal vector ﬁeld from [5]
(r = 6).
fairness term allows us to obtain a smooth quadrangulation even in this case.
If not speciﬁed, α = 10−3, β = 1 and γ = 10−2 values have been used in these
tests.
We also evaluate the asymptotic behavior of the regularization operator. As
discussed in Sect. 2, we know that the Hausdorﬀdistance between S and ∂X
for some smooth shape X is in O(h). We experimentally observe that S∗has
the same asymptotic behavior and is even closer to the original surface than S.
We have considered asymptotic plane and sphere objects in Fig. 7 for various h
(abscissa) tending to zero. In the ﬁrst row, we show that the normal vector ﬁeld
u∗of the regularized surface S∗seems to converge to the input ﬁeld u as h tends
to zero3. The second row shows the proximity of p (i.e. the vertices of S = ∂hX)
to ∂X (Theorem 1 of [11]). The O(h) convergence speed is thus experimentally
conﬁrmed. In the third row, we evaluate the distance between p∗(the vertices of
S∗) and its closest point on ∂X. We also observe an experimental convergence
speed in O(h). Finally, the last row compares the two approximations. In this
case, we see that p∗is experimentally closer to ∂X than p.
3 Since quads of S∗are not coplanar anymore, each vector of u∗is deﬁned by averaging
the two normal vectors of the quad triangles.

206
D. Coeurjolly et al.
Fig. 5. Isotropic vs anistropic normal vector ﬁeld as input data. From left to right: the
original object, the regularization with Integral Invariant based normal vector ﬁeld [5]
(r = 4), the regularization after a piecewise smoothing of the same normal vector ﬁeld
using [4] (ϵ = [2, 0.25], λ = 0.03, α = 0.006 and the Integral Invariant normal vector
ﬁeld with r = 4).
Fig. 6. Regularization examples without (left) or with noise (right) (Octaflower, 2563).
A highly specular surface has been used to highlight the smoothness of the reconstruc-
tion (normal vectors are given by the face geometry). The input normal vector ﬁeld is
given by [4].

Digital Surface Regularization by Normal Vector Field Alignment
207
Fig. 7. Convergence results for plane and sphere with α = 10−5h2, β = 1, γ = 0. Errors
(y−axis) are given for both the L2 and L∞norms. The grid resolution h (in abscissa)
in the range [6 · 10−3, 0.2]. As h tends to zero, convergence graphs must be read from
right to left.

208
D. Coeurjolly et al.
6
Conclusion and Future Works
In this article, we have proposed a variational approach to regularize a digital
surface. The regularized surface is consistent with respect to an input normal
vector ﬁeld, and has a smooth embedding. If the input normal vector ﬁeld is
piecewise smooth (i.e. with singularities), the regularization preserves these fea-
tures. Finally, we have experimentally demonstrated that the regularized vertices
are closer to the underlying continuous object than the Gauss digitization in a
multigrid convergent framework.
Future works are twofold: First, similarly to Theorem 1 of [11], a formal
proximity result is needed between ˆp and ∂X. At this point, we have been
able to derived a formal proof for an L2 proximity in O(h) (average error).
Further developments are required to obtain a L∞proximity in O(h) (worst-
case). Secondly, we would like to exploit the regularity and smoothness of the
regularized surface in various geometry processing problems. Last, we would like
to study the inﬂuence of metrics ¯
W0 and ¯
W2 in the quality of reconstruction.
For now we have only used identity matrices, but we believe that metrics based
on estimated local areas would improve results.
Acknowlegments. This work has been partly funded by the COMEDIC ANR-15-
CE40-0006 research grant.
A
Details on Operator R
The R operator deﬁned in (23) is a sum of three symmetric matrices,
α ¯
W0 + β DT UT ¯
W2 U D + γ(I −B)T ¯
W0(I −B) .
(24)
The ﬁrst one is a positive deﬁnite matrix since it stands for a scalar prod-
uct. From (5) and (11), one can clearly see that Ealign ≥0 and Efairness ≥0
as they are calculated as the sum of positive or null terms. Noting that
Ealign(ˆp) = ˆpT DT UT ¯
W2UDˆp and Efairness(ˆp) = ˆpT (I −B)T ¯
W0(I −B)ˆp, the
second and third matrices are therefore positive semi-deﬁnite. Therefore we have
∀x ∈R3|E0| \ {0}:
xT 
α ¯
W0

x > 0 ,
(25)
xT 
β DT UT ¯
W2U D

x ≥0 ,
(26)
xT 
γ(I −B)T ¯
W0(I −B)

x ≥0 .
(27)
Assuming α > 0, β > 0 and γ > 0, we have xT Rx > 0 and R is positive
deﬁnite. R is thus invertible and eﬃcient inversion algorithms exist (e.g. using
LDLT Cholesky factorization [7]).

Digital Surface Regularization by Normal Vector Field Alignment
209
References
1. DGtal: Digital geometry tools and algorithms library. http://dgtal.org
2. Braquelaire, J., Vialard, A.: Euclidean paths: a new representation of bound-
ary of discrete regions. Graph. Models Image Process. 61(1), 16–43 (1999).
http://dx.doi.org/10.1006/gmip.1999.0488
3. Coeurjolly, D., Dupont, F., Jospin, L., Sivignon, I.: Optimization schemes for the
reversible discrete volume polyhedrization using marching cubes simpliﬁcation. In:
Kuba, A., Ny´ul, L.G., Pal´agyi, K. (eds.) DGCI 2006. LNCS, vol. 4245, pp. 413–424.
Springer, Heidelberg (2006). doi:10.1007/11907350 35
4. Coeurjolly, D., Foare, M., Gueth, P., Lachaud, J.: Piecewise smooth reconstruction
of normal vector ﬁeld on digital data. Comput. Graph. Forum 35(7), 157–167
(2016). http://dx.doi.org/10.1111/cgf.13013
5. Coeurjolly, D., Lachaud, J., Levallois, J.: Multigrid convergent principal curvature
estimators in digital geometry. Comput. Vis. Image Underst. 129, 27–41 (2014).
http://dx.doi.org/10.1016/j.cviu.2014.04.013
6. Grady, L.J., Polimeni, J.: Discrete Calculus: Applied Analysis on Graphs for Com-
putational Science. Springer, London (2010)
7. Guennebaud, G., Jacob, B., et al.: Eigen v3. (2010). http://eigen.tuxfamily.org
8. He, L., Schaefer, S.: Mesh denoising via l0 minimization. ACM Trans. Graph.
(TOG) 32(4), 64 (2013)
9. Jacques-Olivier Lachaud, D.C., Levallois, J.: Robust and convergent curvature and
normal estimators with digital integral invariants. In: Modern Approaches to Dis-
crete Curvature, vol. 2184. LNM, Springer International Publishing (to appear,
2017)
10. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Pic-
ture Analysis. Series in Computer Graphics and Geometric Modelin. Morgan Kauf-
mann, San Francisco (2004)
11. Lachaud,
J.,
Thibert,
B.:
Properties
of
gauss
digitized
shapes
and
dig-
ital
surface
integration.
J.
Math.
Imaging
Vis.
54(2),
162–180
(2016).
http://dx.doi.org/10.1007/s10851-015-0595-7
12. Lorensen, W.E., Cline, H.E.: Marching cubes: a high resolution 3d surface con-
struction algorithm. In: ACM Siggraph computer Graphics, vol. 21, pp. 163–169.
ACM (1987)
13. Ohtake, Y., Belyaev, A.G.: Dual/primal mesh optimization for polygonized implicit
surfaces. In: Proceedings of the Seventh ACM Symposium on Solid Modeling and
Application, pp. 171–178. ACM (2002)
14. Schaefer, S., Warren, J.: Dual marching cubes: primal contouring of dual grids. In:
Proceedings of the 12th Paciﬁc Conference on Computer Graphics and Applica-
tions, PG 2004, pp. 70–76. IEEE (2004)
15. Wang, R., Yang, Z., Liu, L., Deng, J., Chen, F.: Decoupling noise and features
via weighted l1-analysis compressed sensing. ACM Trans. Graph. (TOG) 33(2),
18 (2014)
16. Wu, X., Zheng, J., Cai, Y., Fu, C.W.: Mesh denoising using extended rof model
with l1 ﬁdelity. Comput. Graph. Forum 34(7), 35–45 (2015)
17. Zhang, H., Wu, C., Zhang, J., Deng, J.: Variational mesh denoising using total vari-
ation and piecewise constant function space. IEEE Trans. Visual Comput. Graphics
21(7), 873–886 (2015)

Morphological Analysis

Opening Holes in Discrete Objects
with Digital Homotopy
Aldo Gonzalez-Lorenzo(B), Alexandra Bac, and Jean-Luc Mari
Aix-Marseille Universit´e, CNRS, LSIS UMR 7296, Marseille, France
aldo.gonzalez-lorenzo@univ-amu.fr
Abstract. Discrete objects are sets of pixels, voxels or their analog in
higher dimension. A three-dimensional discrete object can contain holes
such as tunnels, handles or cavities. Opening the holes of an object con-
sists in erasing all its holes by removing some parts of it. The main idea
is to take a point of the object and to dilate it inside the object with-
out changing its homotopy type: the remaining points in the object are
those which have to be removed. This process does not require the com-
putation of the homology groups of the object and is only based on the
identiﬁcation of simple points.
In this experimental paper we propose two algorithms for opening the
holes of a discrete object endowed with any adjacency relation in arbi-
trary dimension. Both algorithms are based on the distance transform
of the object and diﬀer in how the dilation is performed, favoring either
time complexity or the quality of the output. Moreover, these algorithms
contain a parameter that controls the thickness of the removed parts.
1
Introduction
Topological features such as connected components, tunnels, handles or cavities
allow us to understand the essential structure of an object, which is invari-
ant under a continuous deformation. They are deﬁned through algebraic topol-
ogy. Roughly speaking, homology theory deﬁnes a q-dimensional hole as a q-
dimensional sphere which is not the boundary of a (q + 1)-dimensional ball.
Hence, connected components are 0-dimensional holes; tunnels and handles are
1-dimensional holes and cavities are 2-dimensional holes. On the other hand,
homotopy theory detects q-dimensional holes with q-dimensional spheres that
cannot be continuously deformed into a point.
In order to erase a hole, one can add matter to the object so it disappears.
This is called closing a hole. For instance, we can remove a connected component
by adding a bridge to another connected component, a handle by adding a patch
or a cavity by ﬁlling its interior. Intuitively, we can erase a q-dimensional hole
by adding a (q + 1)-dimensional ball.
Another way to erase a hole is by removing matter from the object. We
call this opening a hole. As an illustration, we can open a 0-dimensional hole
by removing the whole connected component, a handle by cutting a slice or a
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 213–224, 2017.
DOI: 10.1007/978-3-319-66272-5 18

214
A. Gonzalez-Lorenzo et al.
cavity by digging a well. In this case, if the object is embedded in the three-
dimensional space, we say that we can open a q-dimensional hole by removing a
(3 −q)-dimensional ball.
The problem of closing holes has been addressed in [1,7,8]. We recall that an
object is contractible if it has the homotopy type of a point [6]. Hence, a con-
tractible object has no holes at all. A trivial way of ﬁlling the holes of an object is
to consider a contractible object that contains it: the diﬀerence between them is
the matter that we have to add to close its holes. In order to optimize the quan-
tity of matter that we add, we can shrink it while preserving its contractibility
property and keeping the object inside.
In this experimental article we study the problem of opening the holes in an
object. The main idea is to ﬁnd a contractible subset of the object by taking a
point in its interior and expanding it without changing its homotopy type. This
has been shortly investigated in [10] for segmenting the brain cortex in magnetic
resonance images. In this paper we develop farther this problem and design two
algorithms for opening the holes of a discrete object using additional geometric
conditions to obtain a visually pleasant output.
2
Preliminaries
2.1
Discrete Object
A d-dimensional discrete object is a (ﬁnite) subset of Zd. Its elements are called
pixels when d = 2, voxels when d = 3 or points in general.
We endow a discrete object with an adjacency relation. Let us mention two
of them: two points x, y ∈Zd are (2d)-adjacent (resp. (3d −1)-adjacent) if
∥x −y∥1 ≤1 (resp. ∥x −y∥∞≤1). Let α ∈

2d, 3d −1

, the α-neighborhood
of a point x, denoted Nα(x), is the set of its α-adjacent points. We also denote
N ∗
α(x) := Nα(x)−{x}. The outer α-boundary of a discrete object X is N ∗
α(X) :=
(
x∈X Nα(x)) −X, the set of points in Zd −X having an α-adjacent point in X.
2.2
Simple Points
Roughly speaking, a point is simple for a discrete object X if its addition or
removal from X does not change the homotopy type of X. As a discrete object is
just a set of isolated points, this only makes sense if we endow the discrete object
with a topological space. This is usually done in terms of the adjacency relation
considered for the object, so there are diﬀerent characterizations of simple points.
See [2,3,5,11] to cite a few.
We consider in this paper the (3d −1)-adjacency relation and its associated
notion of simple point as described in [5], though any other deﬁnition of simple
point can be used.

Opening Holes in Discrete Objects with Digital Homotopy
215
3
Homotopic Opening
A topological space is contractible if it has the homotopy type of a point. Intu-
itively, this means that we can shrink it to a point. We can translate this concept
into the discrete context by using the notion of simple point. We recall that a
point in a discrete object is simple if its removal does not change the homo-
topy type of the object. Consequently, a discrete object is contractible if we can
reduce it to a point by a sequence of simple points deletions.
A contractible discrete object has no holes. Given a discrete object X, we say
that Y ⊂X is a homotopic opening if X−Y is contractible. Hence, the homotopic
opening consists of a set of cuts that remove the holes from the object. Note
that, if the object is not connected, then all the connected components except
one will be in the homotopic opening, so it may be more interesting to consider
the homotopic opening of each connected component separately.
The homotopic opening is clearly not unique. We could be interested in
obtaining a minimal (in the number of points) homotopic opening, but this
is not useful in the discrete context (see Fig. 1). Also, it can be useful to have
thick cuts for better representing the holes. For this reason, we evaluate the
homotopic opening by visual inspection and we do not try to deﬁne a unique or
optimal homotopic opening.
Fig. 1. Two homotopic openings (black) for the same object (gray) with the same size.
This example shows why we cannot judge a homotopic opening by its size.
4
Computing Homotopic Openings
A simple algorithm for computing a homotopic opening, which is the base for
the more elaborate algorithms that we introduce later, is the following. Let X be
a discrete object, choose some point x ∈X and set C = {x}. Then ﬁnd a point
in X −C which is simple for C and add it to C. Repeat this operation while
there are such points. At the end C is obviously contractible and thus X −C
is a homotopic opening for X. Note that, according to [10], C is a homotopic
dilation of {x} constrained by X.
Figure 2 illustrates the output of such algorithm. While the cuts are thin,
they are far too long. A more elaborate algorithm consists in using the distance
transform.

216
A. Gonzalez-Lorenzo et al.
Fig. 2. A 2D discrete object (gray) and a homotopic opening (black) obtained by
choosing simple points at random.
The (Euclidean) distance transform of a discrete object X is the map dtX :
X −→R that assigns to each point of X its distance to Zd −X or, in other
words, its depth in the object. Hence, at the beginning we can set C = {x}
where x is a maximum for dtX and then we give priority to points with higher
dtX value. By using the distance transform, we lead the propagation in order to
obtain the cuts in the thinner parts of the object. In the following we describe
two ways of doing this.
4.1
Random Propagation
A main concern when using the distance transform in the propagation of the
contractible subset is how to deal with points with equal value. A simple idea
for having an isotropic homotopic opening is to randomly pick a point among the
points with equal distance transform value. Algorithm 1 describes this approach.
Algorithm 1. Homotopic opening with random propagation
Input: X ⊂Zd
Output: Homotopic opening for X
C ←{x}
for a random x ∈X such that dtX(x) is maximal;
S ←N ∗
α(x) ∩(X −C);
while S ̸= ∅do
x ←random point in S such that dtX(x) is maximal;
S ←S −{x};
if x is simple for C then
C ←C ∪{x};
S ←S ∪(N ∗
α(x) ∩(X −C));
return X −C;

Opening Holes in Discrete Objects with Digital Homotopy
217
Complexity of Algorithm 1. Let n denote the number of points in the bounding
box of the discrete object. The distance transform of X can be computed in
O(n) [4,9]. Since every point is inserted into S each time a neighbor is added
to C, then it is added at most (3d −1) times. Thus, the while loop is executed
at most 3dn times. The set S can be implemented as a priority queue using the
distance transform as key. Hence, insertion and deletion can be performed in
logarithmic time while ﬁnding the deepest point is done in constant time. Con-
sequently, Algorithm 1 has complexity O(3dn(log n + f(d)), where f(d) denotes
the complexity of checking if a point is simple in Zd.
For dimension d ≤3, we can use a look-up table for recognizing simple points,
and hence the complexity of Algorithm 1 is O(n log n).
Figure 3 illustrates some results of Algorithm 1. The holey disk (top-left)
presents short cuts, though they do not seem straight segments, specially the
bottom-left cut. This kind of issue is treated in Sect. 4.3. The hollow ball (top-
center) and the sculpture (top-right) present well-shaped cuts. Regarding the
torus (bottom), we can appreciate two diﬀerentiated cuts. One is around the
central hole, which is well-shaped, while the other one is far from looking like a
ring around the cavity.
Fig. 3. Homotopic openings obtained with Algorithm 1.
This kind of problem is due to the propagation of the contractible subset. If
we randomly propagate a contractible object inside a plane, we do not obtain
a uniform disk, but a tree-shaped object. A 3D discrete object can contain a
two-manifold-like set of points which are equidistant to the complement of the
object, so the propagation has a similar behavior when it reaches their depth.

218
A. Gonzalez-Lorenzo et al.
A concrete example is shown in Fig. 4 (top), which depicts a thickened torus—
so its set of deepest points is a torus—and three steps in the propagation of
the contractible subset. Hence, it is natural that Algorithm 1 produces strange
homotopic openings on such objects. Note that this problem does not happen in
2D discrete objects.
4.2
Propagation by Layers
The previous example on the torus motivates the following algorithm. In this case,
we treat the object by layers. The idea is that at each step we traverse the sim-
ple points in the outer boundary of the contractible subset to ﬁnd the maximum
distance transform value. Then we mark the points in the outer boundary whose
distance transform value is close to that maximum and we try to add them to the
object. This is described in Algorithm 2. This alternative strategy produces a more
uniform propagation of the contractible subset. Figure 4 illustrates the diﬀerence
between the propagations performed by Algorithms 1 and 2.
Fig. 4. Diﬀerent steps of the propagation performed in Algorithms 1 (top) and 2 (bot-
tom) for the same object.
Complexity of Algorithm 2. An upper bound for the number of executions of
the repeat loop is n, since at each execution (except the last one), at least
one point of X −C is added to C. At each iteration we must ﬁnd the outer
boundary of the contractible subset and check if the points are simple, thus
needing O(3dnf(d)) operations. The later traversal of the outer boundary does
not aﬀect the complexity of the iteration. Hence, the complexity of Algorithm 2
is O(3dn2f(d)).
Again, the complexity of Algorithm 2 is O(n2) for dimension d ≤3.
At each step, we put in the list L not only the deepest simple points in
the outer boundary of the contractible subset, but all the points with distance
transform value close to the maximum. Concretely, we consider the points whose

Opening Holes in Discrete Objects with Digital Homotopy
219
Algorithm 2. Homotopic opening with propagation by layers
Input: X ⊂Zd
Output: Homotopic opening for X
C ←{x}
for a random x ∈X with highest dtX value;
repeat
m ←max {dtX(x) | x ∈N ∗
α(C) ∩X, x simple for C};
L ←dt−1
X ([m −1, m]) ∩N ∗
α(C);
foreach x ∈L do
if x is simple for C then
C ←C ∪{x};
until idempotency;
return X −C;
distance transform value is in the interval [m −a, m], for a = 1. We have chosen
this value of a after visual inspection of several examples, but a diﬀerent value
can also be considered.
Some results of Algorithm 2 are depicted in Fig. 5. The cuts of the holey
disk (top-left) look slightly better. There is no remarkable improvement for the
hollow ball (top-center) nor the sculpture (top-right). However, the homotopic
opening of the torus (bottom) is deﬁnitely better.
In conclusion, Algorithm 2 can be better than Algorithm 1, but it has worse
complexity.
Fig. 5. Homotopic openings obtained with Algorithm 2.

220
A. Gonzalez-Lorenzo et al.
4.3
More Than Simple Points
Both previous algorithms add as many simple points as possible to the con-
tractible subset, which produces thin cuts. However, leaving thicker cuts can be
interesting for two reasons:
1. Cuts usually look like polygonal lines instead of discretized straight lines.
When two wavefronts collide, the next added simple points depend strongly
on their neighborhood. This produces a chain reaction that makes the cut be
far from what we would expect.
2. One-dimensional thin cuts in big objects can be very diﬃcult to see.
We propose a simple method for obtaining thick cuts. Instead of adding simple
points, we check if we can add a discrete ball without changing the homotopy
type of the contractible subset, and then we just add its center point (which
must be a simple point). Given r ∈N, we consider Br =

x ∈Zd | ∥x∥2 ≤r

,
the discrete ball of radius r. Hence, given a discrete object X, a contractible
subset C, a point p and a radius r, we check if we can reduce (C ∪(p + Br)) ∩X
to C via a sequence of simple points deletions. A simple way of doing this is to
put the O(rd) points of ((p + Br) ∩X) −C in a list and repeatedly traverse it
and remove simple points until stability. Hence, the complexity of checking if a
ball is simple is O(r2df(d)).
By using simple balls instead of simple points, Algorithms 1 and 2 are
enriched with a parameter r which is the radius of the discrete ball. Thus, their
complexities are O(3dn(log n + r2df(d)) and O(3dn2r2df(d)) respectively.
Intuitively, using simple balls instead of simple points should be suﬃcient
to obtain thick cuts, but using a big radius can lead to the tangency problem
illustrated in Fig. 6.
The discrete ball (red) is not simple for the contractible object (black) since
it creates a one-pixel hole. Consequently, the center point (green) is not added.
This kind of phenomenon is quite frequent when we use a big ball. It prevents
a correct propagation of the contractible subset which can eventually produce a
Fig. 6. Tangency problem. The discrete ball (red) is not simple for the object (black).
(Color ﬁgure online)

Opening Holes in Discrete Objects with Digital Homotopy
221
Fig. 7. Homotopic openings computed by Algorithms 1 (top) and 2 (bottom) with
radiuses 10 (left), 20 (center) and 50 (right).
strange homotopic opening. This is illustrated in Fig. 7, which depicts the output
of Algorithms 1 (top) and 2 (bottom) with radiuses 10, 20 and 50.
An attempt to avoid this kind of conﬁgurations is to center the ball not
only in the point, but also in its (2d)-neighborhood. If one of the 2d + 1 balls is
simple for the contractible subset, then we add the point. We call this the relaxed
test for simple balls. Figure 8 shows how the homotopic openings of Fig. 7 are
when we use a relaxed test for simple balls. We can appreciate that, after using
this approach, the outputs of Algorithms 1 (top) and 2 (bottom) are almost
indistinguishable. Sadly, this idea does not seem to work for 3D objects. Figure 9
depicts the homotopic openings computed by Algorithms 1 (top) and 2 (bottom)
using a ball of radius 5. The relaxed test for simple balls yet produces strange
results, but we can appreciate that Algorithm 2 behaves notably better.
We observe that using a positive radius prevents fronts from approaching
too much, so we obtain quite smooth cuts. We can then continue to propagate
the contractible subset with balls of smaller radiuses in order to obtain thin
and smooth cuts. We have chosen to divide the radius by two at each step.
Figure 10 shows the homotopic openings computed by Algorithm 1 when using
initial radiuses 0, 2, 4 and 8. We can appreciate how the cuts look more and
more well-shaped as we increase the initial radius.

222
A. Gonzalez-Lorenzo et al.
Fig. 8. Homotopic openings computed by Algorithms 1 (top) and 2 (bottom) with
radiuses 10 (left), 20 (center) and 50 (right) and relaxed test for simple balls.
Fig. 9. Homotopic openings computed by Algorithms 1 (top) and 2 (bottom) with
radius 5 and relaxed test for simple balls.

Opening Holes in Discrete Objects with Digital Homotopy
223
Fig. 10. Homotopic openings computed by Algorithm 1 with initial radiuses 0, 2, 4
and 8 and successive reductions. The cuts have been thickened for better visibility.
5
Conclusion and Future Work
This paper addresses the problem of erasing the holes of a discrete object by
removing parts of it. We have deﬁned the homotopic opening and we have pre-
sented two algorithms for computing it. The outputs of these algorithms are
evaluated by visual inspection. While the second algorithm has worse complex-
ity than the ﬁrst one, it seems to produce better results. Then we have enriched
these algorithms with a parameter corresponding to the radius of the balls used
in the propagation of the contractible subset.
We do not obtain satisfactory results for big radiuses in 3D discrete objects,
due to the tangency problem. We think that this problem deserves more atten-
tion. Our algorithms can also be generalized by considering other metrics and
other structural elements for the propagation. Moreover, we suspect that the
complexity of Algorithm 2 can be improved by smartly managing the layer
update.
References
1. Aktouf, Z., Bertrand, G., Perroton, L.: A three-dimensional holes closing algorithm.
Pattern Recogn. Lett. 23(5), 523–531 (2002)
2. Bertrand, G.: New notions for discrete topology. In: Proceedings of the 8th Interna-
tional Conference Discrete Geometry for Computer Imagery, DCGI 1999, Marne-
la-Vall´ee, France, 17–19 March 1999, pp. 218–228 (1999)
3. Bertrand, G., Malandain, G.: A new characterization of three-dimensional simple
points. Pattern Recogn. Lett. 15(2), 169–175 (1994)
4. Coeurjolly, D., Montanvert, A.: Optimal separable algorithms to compute the
reverse Euclidean distance transformation and discrete medial axis in arbitrary
dimension. IEEE Trans. Pattern Anal. Mach. Intell. 29(3), 437–448 (2007)
5. Couprie, M., Bertrand, G.: New characterizations of simple points in 2D, 3D, and
4D discrete spaces. IEEE Trans. Pattern Anal. Mach. Intell. 31(4), 637–648 (2009)
6. Hatcher, A.: Algebraic Topology. Cambridge University Press, Cambridge (2002)
7. Janaszewski, M., Couprie, M., Babout, L.: Geometric approach to hole segmenta-
tion and hole closing in 3D volumetric objects. In: Bayro-Corrochano, E., Eklundh,
J.-O. (eds.) CIARP 2009. LNCS, vol. 5856, pp. 255–262. Springer, Heidelberg
(2009). doi:10.1007/978-3-642-10268-4 30

224
A. Gonzalez-Lorenzo et al.
8. Janaszewski, M., Couprie, M., Babout, L.: Hole ﬁlling in 3D volumetric objects.
Pattern Recogn. 43(10), 3548–3559 (2010)
9. Maurer Jr., C.R., Qi, R., Raghavan, V.: A linear time algorithm for computing
exact Euclidean distance transforms of binary images in arbitrary dimensions.
IEEE Trans. Pattern Anal. Mach. Intell. 25(2), 265–270 (2003)
10. Rueda, A., Acosta, O., Couprie, M., Bourgeat, P., Fripp, J., Dowson, N., Romero,
E., Salvado, O.: Topology-corrected segmentation and local intensity estimates for
improved partial volume classiﬁcation of brain cortex in MRI. J. Neurosci. Methods
188(2), 305–315 (2010)
11. Saha, P.K., Chaudhuri, B.B., Chanda, B., Majumder, D.D.: Topology preservation
in 3D digital space. Pattern Recogn. 27(2), 295–300 (1994)

Well-Composedness in Alexandrov Spaces
Implies Digital Well-Composedness in Zn
Nicolas Boutry1,2(B), Laurent Najman2, and Thierry G´eraud1
1 EPITA Research and Development Laboratory (LRDE),
Le Kremlin-Bicˆetre, France
2 Universit´e Paris-Est, LIGM, ´Equipe A3SI, ESIEE, Champs-sur-Marne, France
nicolas.boutry@lrde.epita.fr
Abstract. In digital topology, it is well-known that, in 2D and in 3D,
a digital set X ⊆Zn is digitally well-composed (DWC), i.e., does not
contain any critical conﬁguration, if its immersion in the Khalimsky grids
Hn is well-composed in the sense of Alexandrov (AWC), i.e., its boundary
is a disjoint union of discrete (n −1)-surfaces. We show that this is still
true in n-D, n ≥2, which is of prime importance since today 4D signals
are more and more frequent.
Keywords: Well-composed · Discrete surfaces · Alexandrov spaces ·
Critical conﬁgurations · Digital topology
1
Introduction
We recall that a subset of Zn, n ≥1, is said to be digital if it is ﬁnite or
if its complement in Zn is ﬁnite. When n ∈{2, 3}, the immersion I(X) into
the Khalimsky grid Hn of a digital subset X of Zn based on the miss-strategy
is known to be well-composed in the Alexandrov sense (AWC) [13], i.e., the
connected components of the boundary of I(X) are discrete (n −1)-surfaces,
iﬀX is digitally well-composed (DWC) [5], i.e., X does not contain any critical
conﬁguration (see Fig. 1). The aim of this paper is to show that AWCness (of
the immersion of a set) implies DWCness (of the initial set) in n-D, n ≥2.
In order to do so, we show that we can reformulate AWCness in a local
way: a subset Y of Hn is AWC iﬀ, for any z in the boundary N of Y in Hn,
the subspace |β□
N(z)| is a discrete (n −2 −dim(z))-surface. This property is of
prime importance since we compare AWCness with DWCness, which is a local
property. It is then suﬃcient to proceed by counterposition and to show that, if
a digital subset X of Zn contains a critical conﬁguration in a block S such that
X ∩S = {p, p′} (primary case) or S \ X = {p, p′} (secondary case) with p and
p′ two antagonists in S, then there exists some z∗in the boundary N of I(X)
satisfying that |β□
N(z∗)| is not a (n −2 −dim(z∗))-surface. In fact, by choosing a
particular z∗related to p+p′
2 , we obtain that |β□
N(z∗)| is the union of two disjoint
(n −2 −dim(z∗))-surfaces, the ﬁrst is a function of p and z∗and the second is
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 225–237, 2017.
DOI: 10.1007/978-3-319-66272-5 19

226
N. Boutry et al.
Fig. 1. From a digital set X ⊆Z2 to its immersion I(X) in H2; X and I(X) are
depicted in black, and the boundary of I(X) in H2 is depicted in gray and red. (Color
ﬁgure online)
a function of p′ and z∗. This way, |β□
N(z∗)| is not a (n −2 −dim(z∗))-surface,
which concludes the proof.
In Sect. 2, we recall some needed basics principles to formalize DWCness and
AWCness. In Sect. 3, we present some new mathematical tools supporting our
proof. In Sect. 4, after having detailed some properties of Zn and Hn, we show
how they are related. In Sect. 5, we outine the proof of the paper’s main result,
while we conclude our work in Sect. 6.
2
Basic Concepts of Digital Topology
Let us reintroduce the notions of DWCness and AWCness.
2.1
Digital Topology and DWCness
Let B = {e1, . . . , en} be the canonical basis of Zn. We use the notation xi,
where i belongs to 1, n, to determine the ith coordinate of x ∈Zn. We
recall that the L1-norm of a point x ∈Zn is denoted by ∥.∥1 and is equal
to 
i∈1,n |xi| where |.| is the absolute value. Also, the L∞-norm is denoted
by ∥.∥∞and is equal to maxi∈1,n |xi|. For a given point x ∈Zn, an ele-
ment of the set N ∗
2n(x) = {y ∈Zn ;
∥x −y∥1 = 1} (resp. of the set
N ∗(x) = {y ∈Zn ; ∥x −y∥∞= 1}) is a 2n-neighbor (resp. a (3n −1)-neighbor)
of x. For any z ∈Zn and any F = (f 1, . . . , f k) ⊆B, we denote by S(z, F)
the set

z + 
i∈1,k λif i  λi ∈{0, 1}, ∀i ∈1, k

. We call this set the block
associated with the pair (z, F); its center is z + 
f∈F
f
2 , and its dimension,
denoted by dim(S), is equal to k. More generally, a set S ⊂Zn is said to be a
block iﬀthere exists a pair (z, F) ∈Zn × P(B) such that S = S(z, F). Then, we
say that two points p, q ∈Zn belonging to a block S are antagonists in S iﬀthe
distance between them equals the maximal distance using the L1 norm between
two points in S; in this case we write p = antagS(q). Note that the antagonist
of a point p in a block S containing p exists and is unique. Two points that
are antagonists in a block of dimension k ≥0 are said to be k-antagonists; k is
then called the order of antagonism between these two points. We say that a
digital subset X of Zn contains a critical conﬁguration in a block S of dimension

Well-Composedness in Alexandrov Spaces
227
k ∈2, n iﬀthere exists two points {p, p′} ∈Zn that are antagonists in S s.t.
X ∩S = {p, p′} (primary case) or s.t. S \ X = {p, p′} (secondary case). Then,
a digital set X ⊂Zn is said to be digitally well-composed (DWC) [5] iﬀit does
not contain any critical conﬁguration.
2.2
Axiomatic Digital Topology and AWCness
Let X be any set, and let U be a set of subsets of X satisfying that X, ∅belong
to U, any union of any family of elements of U belongs to U, and any ﬁnite inter-
section of any family of elements of U belongs to U. Then U is a topology [1,10],
and the pair (X, U) is called a topological space. We abusively say that X is a
topological space, assuming it is supplied with a topology U. The elements of U
are called the open sets of (X, U), and the complement of an open set is said to
be a closed set [1]. A set N containing an element p of a topological space X
s.t. there exists U ∈U satisfying p ∈U ⊆N is said to be a neighborhood of p
in X. We say that a topological space (X, U) satisﬁes the T0 axiom of separa-
tion [1,3,10] iﬀfor any two diﬀerent elements in X, for at least one of them there
is an open neighborhood not containing the other element. A topological space
which satisﬁes the T0 axiom of separation is said to be a T0-space, a topological
space X is called discrete [2] iﬀthe intersection of any family of open sets of X
is open in X, and a discrete T0-space is said to be an Alexandrov space [8].
Let Λ be an arbitrary set. A binary relation [4] R on Λ is a subset of Λ × Λ,
and for any x, y ∈Λ, we denote by x R y the fact that (x, y) ∈R, or equivalently
x ∈R(y). A binary relation R is called reﬂexive iﬀ, ∀x ∈Λ, x R x, is called
antisymmetric iﬀ, ∀x, y ∈Λ, x R y and y R x imply x = y, and is called transitive
iﬀ, ∀x, y, z ∈Λ, x R y and y R z imply x R z. Also, we denote by R□the
binary relation deﬁned such that, ∀x, y ∈Λ,

x R□y

⇔{x R y and x ̸= y}.
An order relation [4] on Λ is a binary relation which is reﬂexive, antisymmetric,
and transitive; a set Λ of arbitrary elements supplied with an order relation R on
Λ is denoted (Λ, R) or |Λ| and is called a poset [4]; Λ is called the domain of |Λ|.
According to Alexandrov (Theorem 6.52, p. 28 of [1]), we can identify any poset
|X| = (X, R) with the Alexandrov space induced by the order relation R. Let
(X, αX) be a poset and p an element of X, the combinatorial closure αX(p) of p
in |X| is the set {q ∈X ; (q, p) ∈αX}, the combinatorial opening βX(p) of p in
|X| is the set {q ∈X ; (p, q) ∈αX}, and θX(p) := αX(p) ∪βX(p); αX(p) (resp.
βX(p)) is then the smallest closed (resp. open) set containing {p} in X. Also,
∀S ⊆X, αX(S) := ∪p∈SαX(p), βX(S) := ∪p∈SβX(p), and θX(S) := ∪p∈SθX(p).
Assuming that |X| is a poset and S is a subset of X, the suborder [4] of |X|
relative to S is the poset |S| = (S, αS) with αS := αX ∩S ×S; we have then, for
any x ∈S, αS(x) = αX(x) ∩S, βS(x) = βX(x) ∩S, and θS(x) = θX(x) ∩S. For
any suborder |S| of |X|, we denote by IntX(S) the open set {h ∈X ; βX(h) ⊆
S}. A set S ⊆X is said to be a regular open set (resp. a regular closed set) iﬀ
S = IntX(αX(S)) (resp. S = αX(IntX(S))). We call relative topology [8] induced
in S by U the set of all the sets of the form U ∩S where U ∈U. A set which
is open in the relative topology of S is said to be a relatively open set [8]. A
set S ⊆X is then said to be connected iﬀit is not the disjoint union of two

228
N. Boutry et al.
non-empty relatively open subsets w.r.t. S. The largest connected set in (X, U)
containing p ∈X is called the component [1] of the point p in (X, U) and we
denote it CC(X, p). When (X, U) is non-empty, the set of maximal components of
X in the inclusion sense is denoted by CC(X) and is called the set of connected
components of X. We call path [4] into S ⊆X a ﬁnite sequence (p0, . . . , pk)
such that for all i ∈1, k, pi ∈θ□
X(pi−1), and we say that a set S ⊆X is
path-connected [4] iﬀfor any points p, q in S, there exists a path into S joining
them. When |X| is an Alexandrov space, any subset S of X is connected iﬀit is
path-connected [4,8].
The Khalimsky grid [11] of dimension n is denoted |Hn| = (Hn, ⊆) and
is the poset deﬁned such that H1
0 = {{a} ; a ∈Z}, H1
1 = {{a, a + 1} ; a ∈Z},
H1 = H1
0 ∪H1
1, and Hn =

h1 × · · · × hn ; ∀i ∈1, n, hi ∈H1
. For any h ∈Hn,
we have the following equalities: α(h) := αHn(h) = {h′ ∈Hn ; h′ ⊆h}, β(h) :=
βHn(h) = {h′ ∈Hn ; h ⊆h′}, and θ(h) := θHn(h) = {h′ ∈Hn ; h′ ⊆h or h ⊆
h′}. For any suborder |X| of |Hn|, we obtain that αX(h) = {h′ ∈X ; h′ ⊆h},
βX(h) = {h′ ∈X ; h ⊆h′}, and θX(h) = {h′ ∈X ; h′ ⊆h or h ⊆h′}. Any
element h of Hn which is the Cartesian product of k elements, with k ∈0, n, of
H1
1 and of (n−k) elements of H1
0 is said to be of dimension k [12], which is denoted
by dim(h) = k, and the set of all the elements of Hn which are of dimension k
is denoted by Hn
k. Furthermore, for any n ≥1, |Hn| is an Alexandrov space [4].
Finally, let A, B be two subsets of Hn; we say that A and B are separated iﬀ
(A∩(β(B))∪(β(A)∩B) = ∅, or equivalently iﬀA∩θ(B) = ∅. The rank ρ(x, |X|) of
an element x in |X| is 0 if α□
X(x) = ∅and is equal to maxy∈α□
X(x)(ρ(y, |X|)) + 1
otherwise. The rank of a poset |X| is denoted by ρ(|X|) and is equal to the
maximal rank of its elements. An element x of X such that ρ(x, |X|) = k is
called k-face [4] of X. In Khalimsky grids, the dimension is equal to the rank.
Let |X| = (X, αX) be a poset. |X| is said to be countable iﬀits domain X
is countable. Also, |X| is called locally ﬁnite iﬀfor any element x ∈X, the set
θX(x) is ﬁnite. A poset which is countable and locally ﬁnite is said to be a CF-
order [4]. Now let us recall the deﬁnition of n-surfaces [9]. Let |X| = (X, αX) be
a CF-order; the poset |X| is said to be a (−1)-surface iﬀX = ∅, or a 0-surface
iﬀX is made of two diﬀerent elements x, y ∈X such that x ̸∈θ□
X(y), or an n-
surface, n ≥1, iﬀ|X| is connected and for any x ∈X, |θ□
X(x)| is a (n−1)-surface.
According to Evako et al. [9], |Hn| is an n-surface. Also, any n-surface |X| is
homogeneous [6], i.e., ∀x ∈X, βX(x) contains an n-face. The boundary [13] of
a digital subset S in an Alexandrov space X is deﬁned as αX(S) ∩αX(X \ S),
and S is said to be well-composed in the sense of Alexandrov (AWC) iﬀthe
connected components of its boundary are discrete (n −1)-surfaces where n ≥0
is the rank of X. Also, let us recall some properties about n-surfaces that will
be useful in the sequel. Let |X|, |Y | be two posets; it is said that |X| and |Y |
can be joined [4] if X ∩Y = ∅. If |X| and |Y | can be joined, the join of |X| and
|Y | is denoted |X|∗|Y | and is equal to (X ∪Y, αX ∪αY ∪X × Y ).
Property 1 ([7]). Let |X| and |Y | be two orders that can be joined, and let
n be an integer. The poset |X|∗|Y | is a (n + 1)-surface iﬀthere exists some
p ∈−1, n + 1 such that |X| is a p-surface and |Y | is a (n −p)-surface.

Well-Composedness in Alexandrov Spaces
229
Property 2 (Property 10 in [6]). Let |X| = (X, αX) be a poset. Then |X| is
an n-surface iﬀfor any x ∈X, |α□
X(x)| is a (k −1)-surface and |β□
X(x)| is a
(n −k −1)-surface, with k = ρ(x, |X|).
Fig. 2. Bijection between (Z/2) and H1.
2.3
A Bijection Between (Z/2)n and Hn
For A, B two arbitrary families of sets, we set A ⊗B := {a × b ; a ∈A, b ∈B},
where × is the Cartesian product. For any a ∈Hn and any i ∈1, n, we
denote by ai the ith coordinate of a into Hn. Then, as a consequence of the
Cartesian product, we obtain that ∀a ∈Hn, α(a) = ⊗m∈1,nα(am) (resp. β(a) =
⊗m∈1,nβ(am)). Also, we deﬁne the bijection H : (Z/2) →H1 s.t. ∀z ∈(Z/2),
H(z) = {z, z + 1} if z ∈Z and H(z) = {z + 1/2} otherwise (see Fig. 2). Its
inverse is denoted by Z. Finally, we deﬁne the bijection Hn :
 Z
2
	n →Hn as the
n-ary Cartesian product of H and Zn : Hn →
 Z
2
	n its inverse.
3
Introducing a New Mathematical Background
Let us introduce new mathematical properties which show how Zn and Hn are
related to each other.
3.1
Complements About Antagonism in Zn
Lemma 1. Let x, y be two elements of Zn. Then, x and y are antagonists in a
block of Zn of dimension k ∈0, n iﬀ:

Card {m ∈1, n ; xm = ym} = n −k, (1)
Card {m ∈1, n ; |xm −ym| = 1} = k. (2)
Proof: Let x, y be two elements of Zn satisfying (1) and (2) with k ∈0, n.
Now, let us take c ∈Zn such that ∀i ∈1, n, ci := min(xi, yi), Ix :=
{i ∈1, n ; ci ̸= xi} and Iy := {i ∈1, n ; ci ̸= yi}. Obviously, Ix∩Iy = ∅, and
then by (1), Card(Ix ∪Iy) = k. Since by (2) we have x = c + 
i∈Ix ei and y =
c + 
i∈Iy ei, then x and y belong to S(c, F) where F :=

ei ∈B ; i ∈Ix ∪Iy

is of cardinality k. Furthermore, the L1 norm of x −y is equal to k, and thus x
and y maximize the L1-distance between two points into S(c, F). So, x and y are
antagonists in S(c, F). Conversely, let us assume that x, y ∈Zn are antagonists
in a block S(c, F) of dimension k ∈0, n. For any i ∈1, n, ei belongs to F
and hence |xi −yi| = 1, or it does not belong to F and hence xi = yi. Since
Card(F) = k by hypothesis, this concludes the proof.
⊓⊔

230
N. Boutry et al.
3.2
General Facts Between Zn and Hn
Let us present some properties relating
 Z
2
	n and Hn that are induced by our
bijection Hn.
Lemma 2. Let c be a value in (Z/2) \ Z, and let y be a value in Z. Then,
y ∈

c −1
2, c + 1
2

iﬀβ(H(y)) ⊆β(H(c)).
Proof: When c belongs to (Z/2) \ Z, H(c) = {c + 1
2} ∈H1
0, and β(H(c)) =
{{c −1/2, c + 1/2}, {c + 1/2}, {c + 1/2, c + 3/2}}. Also, when y ∈Z, H(y) =
{y, y + 1} ∈H1
1, and β(H(y)) = {{y, y + 1}}. If y belongs to {c −1
2, c + 1
2},
we obtain that β(H(y)) ⊆β(H(c)). Conversely, if {{y, y + 1}} ⊆{{c −1/2, c +
1/2}, {c + 1/2}, {c + 1/2, c + 3/2}}, it means that y ∈{c −1/2, c + 1/2}.
⊓⊔
Proposition 1. Let S be a block in Zn, and let c be its center in
 Z
2
	n. Then
S = Zn(β(Hn(c)) ∩Hn
n).
Proof: Let us remark that S =

c + 
i∈1
2 (c) λiei ; ∀i ∈1
2(c), λi ∈

−1
2, 1
2

where
1
2(c) denotes the set of indices of the coordinates i ∈1, n satisfying
ci ∈(Z/2) \ Z. Then, for any y ∈S, if i ∈1, n \ 1
2(c), then yi = ci, if
i ∈
1
2(c) such that λi = 1/2, then yi = ci + 1/2 with ci ∈(Z/2) \ Z, and if
i ∈1
2(c) such that λi = −1/2, hence yi = ci −1/2 with ci ∈(Z/2) \ Z. Then,
for any i ∈1, n, by Lemma 2, H(yi) ∈β(H(ci)), and then Hn(y) ∈β(Hn(c)).
Because y ∈Zn, Hn(y) ∈Hn
n, and then Hn(y) ∈β(Hn(c)) ∩Hn
n, which leads to
y ∈Zn(β(Hn(c)) ∩Hn
n). Conversely, let us assume that y ∈Zn(β(Hn(c)) ∩Hn
n).
Then, Hn(y) ∈β(Hn(c))∩Hn
n, which means that y ∈Zn, and Hn(y) ∈β(Hn(c)).
In other words, for any i ∈1, n, H(yi) ∈β(H(ci)). Two cases are then possible:
ci ∈Z, hence yi = ci, or ci ∈(Z/2)\Z and thus by Lemma 2, yi ∈{ci−1
2, ci+ 1
2}.
This way, y ∈S.
⊓⊔
3.3
Inﬁmum of Two Faces in Hn
Let X be a subset of Hn. If there exists one element x ∈X such that for any
y ∈X, y ⊆x, we say that x is the supremum of X, and we denote it sup(X).
Now, let a, b be two elements of Hn. When sup(α(a) ∩α(b)) is well-deﬁned, we
denote it a ∧b and we call it the inﬁmum between a and b.
Lemma 3. Let a, b be two elements of Hn. Then, α(a) ∩α(b) ̸= ∅iﬀa ∧b is
well-deﬁned. Furthermore, when a ∧b is well-deﬁned, a ∧b = ×i∈1,n(ai ∧bi),
and α(a ∧b) = α(a) ∩α(b).
Proof: Let a1, b1 be two elements of H1, then it is easy to show by a case-by-case
study that α(a1) ∩α(b1) ̸= ∅is equivalent to saying that a1 ∧b1 is well-deﬁned,
and that α(a1) ∩α(b1) = α(a1 ∧b1) when a1 ∧b1 is well-deﬁned. Then, when
a, b belong to Hn, n ≥1 with α(a) ∩α(b) ̸= ∅, we obtain that α(a) ∩α(b)
is equal to ⊗i∈1,n (α(ai) ∩α(bi)) which is non-empty, which means that for
any i ∈1, n, α(ai) ∩α(bi) is not empty, and then ai ∧bi is well-deﬁned and

Well-Composedness in Alexandrov Spaces
231
Fig. 3. Inﬁma between images by Hn of the k-antagonists p and p′ in 3D.
α(ai) ∩α(bi) = α(ai ∧bi). This way, α(a) ∩α(b) is equal to ⊗i∈1,nα(ai ∧bi),
and then is equal to α(×i∈1,n(ai ∧bi)), and then the supremum of α(a) ∩α(b)
is ×i∈1,n(ai ∧bi), i.e., exists and is unique, and so can be denoted by a ∧b.
Furthermore, it satisﬁes α(a ∧b) = α(a) ∩α(b). Conversely, when a ∧b is well-
deﬁned, the supremum of α(a) ∩α(b) exists and thus α(a) ∩α(b) ̸= ∅.
⊓⊔
Lemma 4. ∀p, p′ ∈Zn, p and p′ are k-antagonists, k ∈0, n, iﬀHn(p)∧Hn(p′)
is well-deﬁned and belongs to Hn
n−k.
Proof: The intuition of the proof is depicted on Fig. 3. Let p, p′ be deﬁned in
Zn and k ∈0, n such that p and p′ are antagonists in a block of dimension
k ∈0, n. By Lemma 1, there exists I ⊆1, n or cardinality k, and s.t. ∀i ∈I,
|pi −p′
i| = 1, and ∀i ∈1, n \ I, pi = p′
i. Since for each i ∈1, n, we have
pi, p′
i ∈Z, then H(pi) = {pi, pi + 1}, and H(p′
i) = {p′
i, p′
i + 1}. Let us denote
zi = H(pi), and z′
i = H(p′
i), then zi, z′
i ∈H1
1. When i is in I, p′
i = pi −1,
and α(zi) ∩α(z′
i) = {{pi}}, and then zi ∧z′
i = {pi} ∈H1
0, or p′
i = pi + 1, and
α(zi)∩α(z′
i) = {{p′
i}} and then zi ∧z′
i = {p′
i} ∈H1
0. When i belongs to 1, n\I,
zi = z′
i and α(zi) ∩α(z′
i) = α(zi) and then zi ∧z′
i = zi ∈H1
1. It follows then that
×i∈1,n(zi ∧z′
i) belongs to Hn
n−k. Also, since α(zi)∩α(z′
i) ̸= ∅for any i ∈1, n,
α(Hn(p))∩α(Hn(p′)) is equal to ⊗i∈1,n(α(zi)∩α(z′
i)) which is non-empty, and
then, by Lemma 3, Hn(p) ∧Hn(p′) exists and is equal to ×i∈1,n(zi ∧z′
i), which
belongs to Hn
n−k. Let us now proceed to the converse implication. Let p, p′ be
two points of Zn, and z = Hn(p), z′ = Hn(p′) such that z ∧z′ is well-deﬁned
and belongs to Hn
n−k. Then, we deﬁne I = {i ∈1, n ; zi ∧z′
i ∈H1
0}, whose
cardinality is equal to k thanks to Lemma 3. Now, let us observe that, for any
i ∈1, n, pi ∈{p′
i −1, p′
i + 1} iﬀzi ∧z′
i ∈H1
0, then p and p′ have exactly k
diﬀerent coordinates, and they diﬀer from one. Then, p and p′ are antagonists
in a block of dimension k by Lemma 1.
⊓⊔
Lemma 5. Let a, b be two elements of Zn such that a and b are (3n −1)-
neighbors in Zn or equal. Then, Hn((a + b)/2) = Hn(a) ∧Hn(b).
Proof: Since a and b are (3n−1)-neighbors in Zn, they are antagonists in a block
of dimension k ∈0, n, and then by Lemma 4, Hn(a) ∧Hn(b) is well-deﬁned.
Now, let us prove that (a + b)/2 = Zn(Hn(a) ∧Hn(b)). This is equivalent to say
that for any i ∈1, n, we have (ai + bi)/2 = Z(H(ai) ∧H(bi)) by Lemma 3.
Starting from the equality H(ai)∧H(bi) = {ai, ai+1}∧{bi, bi+1} and observing
that, since a and b are (3n −1)-neighbors in Zn or equal, they satisfy for any

232
N. Boutry et al.
Fig. 4. When y ̸∈S centered at
Zn(z∗), α(Hn(y)) ∩β(z∗) = ∅.
Fig. 5. α□(Hn(p)) is composed of the faces
Hn(p) ∧Hn(vp) such that vp ∈N ∗(p).
i ∈1, n that ai ∈{bi −1, bi, bi + 1}, we have 3 possible cases: ai = bi −1, and
then H(ai)∧H(bi) = {bi −1, bi}∧{bi, bi +1} = {bi}, whose image by Z is equal
to bi −1
2 = (ai +bi)/2, or we have bi = ai −1, and then a symmetrical reasoning
leads to the same result, or bi = ai, and then the result is immediate.
⊓⊔
Proposition 2. Let S be a block and let p, p′ ∈S be any two antagonists in S.
Then the center of the block S is equal to p+p′
2 . Furthermore, its image by Hn
into Hn is equal to Hn(p) ∧Hn(p′).
Proof: Starting from the two antagonists p, p′ in S, we can compute z ∈Zn
and F ⊆B such that S = S(z, F). In fact, for all i ∈1, n, zi = min(pi, p′
i), and
F = {ei ; i ∈1, n, pi ̸= p′
i}. Then, it is clear that p = (p−z)+z = z+
pi̸=zi ei,
and that p′ = (p′ −z) + z = z + 
p′
i̸=zi ei. Then, p + p′ = 2z + 
f∈F f, which
shows that p+p′
2
is the center of S in (Z/2)n. The second part of the proposition
follows from Lemma 5.
⊓⊔
Lemma 6. Let S be a block of Zn, and let z∗∈Hn be the image by Hn of the
center of S. For all y ∈Zn, y ̸∈S implies that α(Hn(y)) ∩β(z∗) is empty.
Proof: This proof can be followed on Fig. 4. Let y be an element of Zn s.t.
α(Hn(y)) ∩β(z∗) is not empty. Then, for all i ∈1, n, α(H(yi)) ∩β(z∗
i ) is not
empty. Now, let us show that y belongs to S. Since there exists pi ∈α(H(yi)) ∩
β(z∗
i ), then H(yi) ∈β(pi) and pi ∈β(z∗
i ), which leads to H(yi) ∈β(z∗
i ), and
then Hn(y) ∈β(z∗). Since y ∈Zn, Hn(y) ∈Hn
n, and then H(y) ∈β(z∗) ∩Hn
n,
which is equivalent to y ∈Zn(β(z∗) ∩Hn
n), which is the reformulation of a block
centered at z∗by Lemma 1.
⊓⊔
Lemma 7. ∀p ∈Zn, α□(Hn(p)) = 
v∈N ∗(p) α(Hn(p) ∧Hn(v)).
Proof: This proof is depicted on Fig. 5. Since p ∈Zn, it can be easily proved that
α□(Hn(p)) is equal to the set of elements f of Hn satisfying ∥Zn(f) −p∥∞= 1
2,
i.e., ∥vp−p∥∞= 1 with vp := 2Zn(f)−p. Then, α□(Hn(p)) is equal to the set of
elements f ∈Hn satisfying vp ∈N ∗(p) and f = Hn((vp+p)/2). By Lemma 5, we
obtain that α□(Hn(p)) is equal to {Hn(vp) ∧Hn(p) ∈Hn ; vp ∈N ∗(p)}, which
leads to the required formula by applying the α operator.
⊓⊔

Well-Composedness in Alexandrov Spaces
233
Lemma 8. Let S be a block in Zn of dimension k ≥2. Now, let p, p′ be two
antagonists in S, and v be a 2n-neighbor of p in S. Then, we have the following
relation: Hn(p) ∧Hn(p′) ∈α(Hn(p) ∧Hn(v)).
Proof: By Lemma 4, Hn(p) ∧Hn(p′) and Hn(p) ∧Hn(v) are well-deﬁned (p
and v are antagonists in a block of dimension 1). By Lemma 3, the ﬁrst term
of the relation is equal to ×i∈1,n(H(pi) ∧H(p′
i)). Likewise, the second term
is equal to ⊗i∈1,n (α(H(pi)) ∩α(H(vi))). Then we want to show that for all
i ∈1, n, H(pi) ∧H(p′
i) belongs to α(H(pi)) ∩α(H(vi)). Let I be the family of
indices {i ∈1, n ; pi ̸= p′
i} . Since v is a 2n-neighbor of p into S, there exists
an index i∗in I such that vi∗̸= pi∗, i.e., vi∗= p′
i∗, and ∀i ∈1, n\{i∗}, vi = pi.
When i ∈1, n \ I or when i = i∗, the property is obviously true. When
i ∈I \ {i∗}, then vi = pi, which implies α(H(pi)) ∩α(H(vi)) = α(H(pi)) =
{{pi}, {pi + 1}, {pi, pi + 1}}. However, either H(pi)∧H(p′
i) = {pi} (if p′
i = pi−1)
or H(pi) ∧H(p′
i) = {pi + 1} (if p′
i = pi + 1), then H(pi) ∧H(p′
i) ∈α(H(pi)) ∩
α(H(vi)).
⊓⊔
3.4
Some Additional Background Concerning n-surfaces
The following proposition results from the proof of Property 11 (p. 55) in [6].
Proposition 3. Let |X| = (X, αX) and |Y | = (Y, αY ) be two n-surfaces, n ≥0.
Then, if |X| is a suborder of |Y |, then |X| = |Y |.
Proof: Let us proceed by induction. Initialization (n = 0): when |X| and |Y |
are two 0-surfaces, the inclusion X ⊆Y implies directly that X = Y since they
have the same cardinality, and then |X| = |Y |. Heredity (n ≥1): we assume
that when two (n −1)-surfaces satisfy an inclusion relationship, they are equal.
Now, let |X| and |Y | be two n-surfaces, n ≥1, such that |X| is a suborder of |Y |.
Then, for all x ∈X, x ∈Y and so we can write θ□
X(x) ⊆θ□
Y (x) since X ⊆Y .
However, |θ□
X(x)| and |θ□
Y (x)| are (n −1)-surfaces and |θ□
X(x)| is a suborder of
|θ□
Y (x)|, then |θ□
X(x)| = |θ□
Y (x)|. Now, let us assume that we have X ⊊Y . Then
let x be a point of X and y a point of Y \ X. Since |Y | is connected as an
n-surface with n ≥1, it is connected by path, and so x, y ∈Y implies that there
exists a path π joining them into Y . This way, there exist x′ ∈X and y′ ∈Y \X
s.t. y′ ∈θ□(x′). In other words, y′ ∈θ□
Y (x′) = θ□
X(x′) since x′ ∈X. This leads
to y′ ∈X. We obtain a contradiction. Thus we have X = Y , and consequently
|X| = |Y |.
⊓⊔
Corollary 1. Let |X1| and |X2| be two k-surfaces, k ≥0, with X1 ∩X2 = ∅.
Then |X1 ∪X2| is not a k-surface.
Proposition 4. Let a, b be two elements of Hn with a ∈β□(b). Then |α□(a) ∩
β□(b)| is a (dim(a) −dim(b) −2)-surface.

234
N. Boutry et al.
Proof: Since |Hn| is an n-surface, then |α□(a)| is a (ρ(a, |Hn|) −1)-surface by
Property 2, and then is a (dim(a)−1)-surface. Now, we can remark that because
b belongs to α□(a), we can write that α□(a)∩β□(b) = β□
α□(a)(b), and then, again
by Property 2, |α□(a) ∩β□(b)| is a ((dim(a) −1) −ρ(b, |α□(a)|) −1)-surface.
Since ρ(b, |α□(a)|) = ρ(b, |Hn|) = dim(b), the proof is done.
⊓⊔
4
Properties Speciﬁc to the Proof
From now on, we suppose n is an integer greater than or equal to 2, that X is a
digital subset of Zn, that Y is the complement of X into Zn; also, we deﬁne the
sets X := Hn(X) and Y := Hn(Y ), and the immersion of X into Hn using the
miss strategy: I(X) := Int(α(X)); its boundary is N := α(I(X))∩α(Hn\I(X)).
Fig. 6. α(Hn(X)) ∩α(Hn(Y )) vs. α(I(X)) ∩α(Hn \ I(X)) in 1D.
Proposition 5. N is equal to α(X) ∩α(Y).
Proof: An intuition of the proof is given in Fig. 6. Let us ﬁrst remark that
α(X) is a regular closed set. Eﬀectively, Int(α(X))) ⊆α(X) implies that
α(Int(α(X))) ⊆α(X) by monotonicity of α. Conversely, any element x ∈X sat-
isﬁes β(x) = {x} ⊆X, and so Int(α(X)), which is equal to {h ∈α(X) ; β(h) ⊆
α(X)}, contains X. This implies that α(Int(α(X))) ⊇α(X). Thus α(X) is a
regular closed set. We can then simplify the formula of N; by deﬁnition, N is
equal to α(I(X)) ∩α(Hn \ I(X)), which is then equal to α(X) ∩α(Hn \ I(X)).
Since I(X) is open, α(Hn \ I(X)) = Hn \ I(X). Thus, N = α(X) \ I(X), which
is equal to α(X) ∩(Int(α(X)))c, and so N = α(X) ∩α(Int(X c)). Let us show
that α(X) ∩α(Int(X c)) is equal to α(X) ∩α(Y). Since Y = Hn
n \ X ⊆Hn \ X, it
is clear that Int(Y) ⊆Int(Hn \ X). Since Y is open as a set of n-faces, we obtain
Y ⊆Int(Hn \ X), and thus α(Y) ⊆α(Int(Hn \ X)) = α(Int(X c)). This way,
α(X)∩α(Y) ⊆α(X)∩α(Int(X c)). Now, let z be an element of α(X)∩α(Int(X c)),
then β(z)∩Hn
n ⊆X (1), or β(z)∩Hn
n ⊆Y (2), or β(z)∩Hn
n∩X ̸= ∅̸= β(z)∩Hn
n∩Y
(3). Before treating the ﬁrst case, let us prove that α(β(z)) = α(β(z) ∩Hn
n) (P).
The converse inclusion is obvious. Concerning the direct inclusion, let a be an
element of α(β(z)). There exists p ∈β(z) such that a ∈α(p). Also, Hn is an
n-surface, and so is homogeneous. This implies that there exists pn ∈β(p) s.t.
pn ∈Hn
n. Since pn ∈β(p) and p ∈β(z), pn ∈β(z) ∩Hn
n, and the fact that a
belongs to α(p) implies that a ∈α(β(z)∩Hn
n). This way, (P) is true. Now, we can

Well-Composedness in Alexandrov Spaces
235
treat the ﬁrst case: β(z) ∩Hn
n ⊆X implies that Int(α(β(z) ∩Hn
n)) ⊆Int(α(X)).
Using (P), we obtain Int(α(β(z))) ⊆Int(α(X)). Since β(z) is an open regular
set, we obtain β(z) ⊆Int(α(X)). Yet, β(z) ⊆α(β(z)) ⊆α(X), since α(X) is a
regular closed set. However, this imples that β(z) = Int(β(z)) ⊆Int(α(X)), and
so z ̸∈α(Int(X c)), which is a contradiction. In the second case, β(z) ∩Hn
n ⊆Y,
which means that no x ∈X exists such that x ∈β(z), which means that
z ̸∈α(X), which leads once more to a contradiction. In the third case,
β(z) ∩Hn
n
∩X ̸= ∅and β(z) ∩Hn
n ∩Y ̸= ∅implies that there exists
some x ∈X and y ∈Y such that z ∈α(x) ∩α(y), and so z ∈α(X) ∩α(Y).
⊓⊔
Proposition 6. For any z ∈N, |α□
N(z)| is a (dim(z) −1)-surface.
Proof: Since N is closed, ∀z ∈N, |α□
N(z)| = |α□(z)|, which is a (ρ(z, |Hn|)−1)-
surface by Property 2 since Hn is an n-surface. Since ρ(z, |Hn|) = dim(z), |α□
N(z)|
is a (dim(z) −1)-surface.
⊓⊔
Lemma 9. I(X) is AWC iﬀ∀z ∈N, |β□
N(z)| is a (n −2 −dim(z))-surface.
Proof: Let us recall that two disjoint components C1 and C2 of N are separated:
C1 ∩θ(C2) = ∅. For this reason, for any z ∈N, |θ□
N(z)| = |θ□(z) ∩
C∈CC(N) C|
= | 
C∈CC(N)(θ□(z) ∩C)| = |θ□
CC(N,z)(z)|. Since n ≥2, I(X) is AWC iﬀ∀C ∈
CC(N), C is a (n −1)-surface, i.e., ∀C ∈CC(N), ∀z ∈C, |θ□
C(z)| is a (n −2)-
surface, which means that ∀C ∈CC(N), ∀z ∈C, |θ□
N(z)| is a (n −2)-surface,
or, in other words, by Property 1 and Proposition 6, ∀z ∈N, |β□
N(z)| is a
(n −2 −dim(z))-surface.
⊓⊔
Proposition 7. Let S be a block of dimension k ∈2, n s.t. X ∩S = {p, p′}
(resp. Y ∩S = {p, p′}) and p′ = antagS(p), then Hn

p+p′
2

∈N.
Proof: Let v be a 2n-neighbor of p in S, then, by Lemma 4, Hn(p) ∧Hn(v) is
well-deﬁned, and by Lemma 3, α(Hn(p) ∧Hn(v)) = α(Hn(p)) ∩α(Hn(v)). Since
dim(S) ≥2, v ∈Y , and so α(Hn(p)) ∩α(Hn(v)) ⊆N by Proposition 5. Now,
using Proposition 2, Hn

p+p′
2

is equal to Hn(p) ∧Hn(p′), which belongs to
α(Hn(p) ∧Hn(v)) by Lemma 8, and thus to N.
⊓⊔
Fig. 7. Summary of the proof; f(q, z∗) := α□(Hn(q)) ∩β□(z∗), with q ∈{p, p′}.

236
N. Boutry et al.
5
Proof of the Main Result
Fig. 8. β□
N(Hn(c)) (in blue) when X
admits a 2D/3D critical conﬁguration
in the block of center c (whose image by
Hn is depicted in black) when n = 3.
(Color ﬁgure online)
We want to prove that AWCness implies
DWCness in n-D (see Fig. 7 for the sum-
mary of the proof). To this aim, we
will show that if X is not DWC, then
I(X) is not AWC. Since by Lemma 9,
I(X) is AWC iﬀ∀z ∈N, |β□
N(z)| is a
(n −2 −dim(z))-surface, it is suﬃcient
to prove that when X is not DWC, then
there exists an element z∗of N such that
|β□
N(z∗)| is not a (n−2−dim(z∗))-surface.
So, let us assume that X is not DWC.
Now, X admits a critical conﬁguration. Let us treat the primary case, since the
reasoning for the secondary case is similar: let us assume that there exists a block
S of dimension k ∈2, n such that X ∩S = {p, p′} with p′ = antagS(p). This
way, we can compute the image z∗by Hn into Hn of the center of S. By Proposi-
tion 2, z∗= Hn(p)∧Hn(p′). Let us show that |β□
N(z∗)| is not a (n−2−dim(z))-
surface. By Proposition 7, z∗∈N, so the expression β□
N(z∗) is well-deﬁned.
Now, let us compute |β□
N(z∗)|. Using Lemmas 7 and 6, we obtain that f(p, z∗) :=
α□(Hn(p)) ∩β□(z∗) is equal to 
y∈S\{p} α(Hn(p) ∧Hn(y)) ∩β□(z∗). Then,
since we know that α(Hn(p) ∧Hn(p′)) ∩β□(z∗) = ∅, thus f(p, z∗) is equal to

y∈S\{p,p′} α(Hn(p) ∧Hn(y)) ∩β□(z∗). By Lemma 3, f(p, z∗) = α(Hn(p)) ∩
α(Hn(Y ∩S)) ∩β□(z∗). With a similar calculation based on p′, we obtain that
f(p′, z∗) := α□(Hn(p′)) ∩β□(z∗) is equal to α(Hn(p′)) ∩α(Hn(Y ∩S)) ∩β□(z∗).
Next, f(p, z∗) ∪f(p′, z∗) is equal to α(Hn(X∩S))∩α(Hn(Y ∩S))∩β□(z∗), which is
equal by Lemma 6 to α(X)∩α(Y)∩β□(z∗), and then to β□
N(z∗) by Proposition 5.
Finally, we have that |β□
N(z∗)| is equal to |f(p, z∗) ∪f(p′, z∗)|. Figure 8 depicts
examples of β□
N(z∗) in the case n = 3. Let us now remark that |β□
N(z∗)| is the dis-
joint union of |f(p, z∗)| and of |f(p′, z∗)|: α□(Hn(p)) ∩α□(Hn(p′)) ∩β□(z∗) = ∅.
However, by Proposition 4, |f(p, z∗)| and |f(p′, z∗)| are both (n −dim(z∗) −2)-
surfaces. Finally, by Corollary 1, |β□
N(z∗)| is not a (n−dim(z∗)−2)-surface, and
then I(X) is not AWC.
⊓⊔
6
Conclusion
Now that we have proved that AWCness implies DWCness for sets, we naturally
conclude that, thanks to cross-section topology, this implication is also true for
gray-level images: a gray-level image u : Zn →Z will be DWC if the span-
based immersion of u is AWC. As future work, we propose to study the converse
implication, i.e., if DWCness implies AWCness in n-D, n ≥2.

Well-Composedness in Alexandrov Spaces
237
References
1. Alexandrov, P.: Combinatorial topology, vol. 1–3. Graylock (1956)
2. Alexandrov, P.: Diskrete R¨aume. Matematicheskii Sbornik 2(44), 501–519 (1937)
3. Alexandrov, P., Hopf, H.: Topologie I. Springer-Verlag, Heidelberg (2013)
4. Bertrand, G.: New notions for discrete topology. In: Bertrand, G., Couprie, M.,
Perroton, L. (eds.) DGCI 1999. LNCS, vol. 1568, pp. 218–228. Springer, Heidelberg
(1999). doi:10.1007/3-540-49126-0 17
5. Boutry, N., G´eraud, T., Najman, L.: How to make nD functions digitally well-
composed in a self-dual way. In: Benediktsson, J.A., Chanussot, J., Najman, L.,
Talbot, H. (eds.) ISMM 2015. LNCS, vol. 9082, pp. 561–572. Springer, Cham
(2015). doi:10.1007/978-3-319-18720-4 47
6. Daragon, X.: Surfaces discr`etes et fronti`eres d’objets dans les ordres. Ph.D. thesis,
Universit´e de Marne-la-Vall´ee, France (2005)
7. Daragon, X., Couprie, M., Bertrand, G.: Discrete frontiers. In: Nystr¨om, I., Sanniti
di Baja, G., Svensson, S. (eds.) DGCI 2003. LNCS, vol. 2886, pp. 236–245. Springer,
Heidelberg (2003). doi:10.1007/978-3-540-39966-7 22
8. Eckhardt, U., Latecki, L.: Digital topology. Technical report, Institut f¨ur Ange-
wandte Mathematik (1994)
9. Evako, A.V., Kopperman, R., Mukhin, Y.V.: Dimensional properties of graphs and
digital spaces. J. Math. Imaging Vis. 6(2–3), 109–119 (1996)
10. Kelley, J.L.: General Topology, Graduate Texts in Mathematics, vol. 27. Springer,
Heidelberg (1955)
11. Khalimsky, E., Kopperman, R., Meyer, P.R.: Computer graphics and connected
topologies on ﬁnite ordered sets. Topol. Appl. 36(1), 1–17 (1990)
12. Kovalevsky, V.: Axiomatic digital topology. J. Math. Imaging Vis. 26(1), 41–58
(2006)
13. Najman, L., G´eraud, T.: Discrete set-valued continuity and interpolation. In:
Hendriks, C.L.L., Borgefors, G., Strand, R. (eds.) ISMM 2013. LNCS, vol. 7883,
pp. 37–48. Springer, Heidelberg (2013). doi:10.1007/978-3-642-38294-9 4

Discrete Shape Representation,
Recognition and Analysis

Heat Kernel Laplace-Beltrami Operator
on Digital Surfaces
Thomas Caissard1(B), David Coeurjolly1, Jacques-Olivier Lachaud2,
and Tristan Roussillon1
1 Univ Lyon, CNRS, INSA-Lyon, LIRIS, UMR 5205, 69621 Lyon, France
{thomas.caissard,david.coeurjolly,tristan.roussillon}@liris.cnrs.fr
2 Universit´e de Savoie, CNRS, LAMA UMR 5127, 73776 Chamb´ery, France
jacques-olivier.lachaud@univ-smb.fr
Abstract. Many problems in image analysis, digital processing and
shape optimization can be expressed as variational problems involving
the discretization of the Laplace-Beltrami operator. Such discretizations
have been widely studied for meshes or polyhedral surfaces. On digital
surfaces, direct applications of classical operators are usually not sat-
isfactory (lack of multigrid convergence, lack of precision. . . ). In this
paper, we ﬁrst evaluate previous alternatives and propose a new digi-
tal Laplace-Beltrami operator showing interesting properties. This new
operator adapts Belkin et al. [2] to digital surfaces embedded in 3D. The
core of the method relies on an accurate estimation of measures associ-
ated to digital surface elements. We experimentally evaluate the interest
of this operator for digital geometry processing tasks.
1
Introduction
Objectives. In geometry processing, Partial Diﬀerential Equations (PDEs) con-
taining Laplace-Beltrami operator arise in many applications such as surface
fairing, mesh smoothing, mesh parametrization, remeshing, mesh compression,
feature extraction or shape matching (see [16] for an extensive survey). On dig-
ital surfaces, few digital Laplace-Beltrami operators have been proposed and
none has been evaluated in terms of multigrid convergence (convergence of the
operator toward the continuous one in digitization of smooth manifolds on grid
with decreasing gridstep).
Contributions. In this article, we propose a discrete Laplace-Beltrami operator
on digital surfaces (boundaries of subsets of Z2 embedded in 3D). This new
operator adapts Belkin et al. [2] on our speciﬁc data. The method uses an accu-
rate estimation of areas associated with digital surface elements. This estimation
is achieved through a convergent digital normal estimator described in [5]. We
show experimental convergence of our operator but also that none of the exist-
ing approaches adapted to digital surfaces achieves such convergence. Finally,
we illustrate the interest of the discretized Laplacian on digital surface geometry
processing.
This work has been partly funded by CoMeDiC ANR-15-CE40-0006 research grant.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 241–253, 2017.
DOI: 10.1007/978-3-319-66272-5 20

242
T. Caissard et al.
Related works. The Laplace-Beltrami operator being a second order diﬀerential
operator (divergence of the function gradient, see Sect. 2) a discrete calculus frame-
work is required to deﬁne such operator on embedded combinatorial structures
such as meshes or digital surfaces. First works on discrete calculus may be found in
the Regge Calculus [21] for quantum physics, where tetrahedra in combination with
edge lengths are used. Works on geometric acquisition devices and models drove
studies toward calculus working on meshes and more generally on simplicial com-
plexes. Early works include a deﬁnition of the Laplace-Beltrami operator using the
classical cotangent formula [19] for solving the problem of minimal surfaces, which
is an analog of the standard ﬁnite element method [16]. Exact calculus generalizing
the cotangent discretization in 2D based on ﬁnite elements [20] emerged from the
German school but with a restriction to triangular complexes.
In a more generic discrete calculus perspective, the Discrete Exterior Calculus
(DEC) framework was then developed in the computational mathematics and
geometry processing community. Another more recent formulation of the DEC
comes from Hirani’s thesis [11] and later by the monograph [8]. On triangular
meshes, DEC based Laplace-Beltrami operator and the cotangent based one
coincide.
In [10], authors show that under some strong assumptions, the cotangent
Laplacian on a triangular mesh converges to the continuous one when the mesh
interpolates a smooth manifold with increasing precision (with a continuous one-
to-one map between the mesh and the manifold, which would not be the case
on digital surfaces). The operator converges in the sense of distributions and the
authors show that pointwise convergence in the l2 sense does not generally hold.
On triangular meshes, Belkin et al. [2] have proposed a ﬁrst Laplace-Beltrami
operator that converges in the l∞uniform case. The digital Laplacian operator
we propose is an extension to digital surfaces of such operator.
In digital geometry, many estimators of diﬀerential quantities have been pro-
posed and there exists multigrid convergent estimators for many quantities such
as length, tangent and curvature in 2D (see [6] for a complete survey), surface
area [14], normal vectors and curvature tensor in dimension 3 [5]. A preliminary
approach can be found in [4,17]. It focuses on the conformal map computa-
tion of a digital surface, which is a related problem involving the deﬁnition of a
Laplace-Beltrami operator. However, their deﬁnition is based on the cotangent
formula, which lacks pointwise convergence. When designing a discrete version
of the Laplacian, not all properties of the continuous one can be expected at
the same time. In [24], entitled “Discrete Laplace operators: No free lunch.”, the
authors have proposed a formal evaluation of such properties. We position our
new digital Laplace-Beltrami operator with respect to this analysis.
Outline. After introducing mathematical deﬁnitions, we review the classical
approaches to deﬁne a discrete Laplacian and compare their properties in Sect. 2.
We then formalize our operator in Sect. 3. In Sect. 4, we experimentally evaluate
our proposal in terms of multigrid convergence and geometry processing appli-
cations.

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
243
2
Discretizations of the Laplace-Beltrami Operator and
Their Properties
We ﬁrst describe various discretizations of the Laplace-Beltrami operator on
triangular meshes. We then check several desired properties of Laplacian using
[24] as a baseline for comparisons.
2.1
Preliminaries and Classical Discretizations on Triangular
Meshes
Let M be a 2-smooth manifold, with or without boundary, embedded in R3. The
intrinsic smooth Laplace-Beltrami operator [22] is deﬁned as:
Δ : C2(M) →C2(M)
u →div(∇u),
(1)
where C2 is the set of functions which are twice diﬀerentiable and with the sec-
ond derivative continuous (in the literature, alternative deﬁnitions may consider
“−div(∇u)” for Δ u).
Let Γ be a combinatorial structure (a triangular mesh for instance), V (Γ)
its set of vertices and F(Γ) its faces. Let u : M →R be a twice diﬀerentiable
function. We suppose that V (Γ) is a sampling of M (i.e. V (Γ) ⊂M). In other
words, u(w) is perfectly deﬁned for w ∈V (Γ).
A ﬁrst simple discretization only considers the combinatorial structure of Γ.
Such Laplacian is either called graph Laplacian or combinatorial Laplacian of Γ
[25]:
(LCOMBI u)(w) := −deg(w)u(w) +

p∈link0(w)
u(p) ,
(2)
for all w ∈V (Γ) where link0(w) is the set of points V (Γ) adjacent to w and
deg(w) is the degree of w in Γ.
A more complex approach can be deﬁned using DEC operators [8,11]. Using
an arbitrarily embedded dual structure of Γ, the Laplace operator can be shown
to be a classical weighted double ﬁnite diﬀerence:
(LDEC u)(w) :=
1
| ⋆w|

p∈link0(w)
| ⋆ewp|
|ewp| (u(p) −u(w)),
(3)
where ⋆is the Hodge-duality star operator acting on discrete forms (see [11]),
and | · | the measure of a k-cell. As illustrated in Fig. 1, | ⋆ewp| would be the
length of the segment orthogonal to ewp. If we set all measures to one, LDEC
coincides with LCOMBI.
By ﬁxing the dual of Γ to be the Voronoi diagram of its vertices and by
computing the measures as Euclidean lengths and areas of such dual complex,
the DEC operator coincides exactly with the famous cotan Laplacian [19]:
(LCOT u)(w) :=
1
2Aw

p link0(w)
(cot(αwp) + cot(βwp)) (u(p) −u(w)) ,
(4)

244
T. Caissard et al.
Fig. 1. Illustration of LDEC (left), and LCOT (right) on triangular meshes. For LCOT
the area of integration Aw is one third the area of all triangles incident on vertex w
in green. For LDEC, the dual structure is in orange and the dual of the edge ewp is in
blue. (Color ﬁgure on line)
where Aw is one third of the area of all incident triangles to vertex w, αwp and
βwp are the angles opposing the corresponding edge ewp (see Fig. 1).
Finally, we detail the deﬁnition of the mesh Laplacian from [2]. Let g : M ×
(0, T) →R be a time-dependent function which solves the partial diﬀerential
equation called the heat equation:
Δg(x, t) = ∂
∂tg(x, t),
(5)
with initial condition g0 = g(·, 0) : M →R which is the initial temperature
distribution. An exact solution [22] is:
g(x, t) =

y∈M
p(t, x, y)g0(y)dy,
(6)
where p ∈C∞(R+ × M × M) is called the heat kernel. The construction of
the heat kernel involves complex technics [22]. Fortunately, there exists many
approximations of p as t tends toward 0 (called small time asymptotics). Early
work includes the famous Varadhan formula [23] and later extensions on a wider
class of shapes [18]. Recently, an approximation using the ambient metric (i.e.
the L2 norm) have been proposed by Belkin et al. It is known to converge toward
the real heat kernel for small t (see Lemma 5 of [1]):
g(x, t) ∼
t→0
1
4πt

y∈M
e−||x−y||2
4t
g0(y)dy.
(7)
Injecting Eq. (7) into Eq. (5), applying a ﬁnite time diﬀerence and knowing
that the integral of e over M is one:
Δg(x, t) = lim
t→0
1
4πt2

y∈M
e−||x−y||2
4t
(g0(y) −g0(x))dy.
(8)

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
245
The previous equation can be seen as a convolution between diﬀerences of g and
a time dependent Gaussian. Then, the mesh Laplace operator [2] on Γ is:
(LMESH u)(w) :=
1
4πt2

f∈F (Γ )
Af
3

p∈V (f)
e−||p−w||2
4t
(u(p) −u(w)),
(9)
where Af is the area associated to the face f.
2.2
Desired Properties of a Discrete Laplacian
As discussed in [24], all properties of the continuous Laplace-Beltrami operator
may not be preserved when discretizing it. We consider the discrete Laplacian
as a linear operator acting on values u := {up} on V (Γ) (represented as a vector
in R|V (Γ )|). Such operator can thus be denoted as a matrix L with components
lij. Hence, v := Lu would be the resulting Laplacian of u. Expected properties
of the discrete Laplace-Beltrami operators are:
Symmetry (SYM). ℓij = ℓji for 0 ≤i < |V (Γ)|. This is very useful when it
comes to solve linear systems as solvers are usually more performant when
the matrix is symmetric.
Locality (LOC). ℓij ̸= 0 if and only if vertices i and j share a common edge.
The locality property gives very sparse matrices decreasing drastically mem-
ory consumption. It also opens a panel of very fast linear system solvers.
Linear Precision (LIN). Lu = 0 whenever u is a linear function restricted to
a plane.
Positive Weights (POS). ℓij ≥0 for i ̸= j. Furthermore, for each vertex i,
there exists a vertex j such that ℓij > 0.
Positive Semi-Deﬁniteness (PSD). The matrix is symmetric positive semi-
deﬁnite regarding the standard inner product, and has a one-dimensional
kernel. (SYM) and (POS) imply (PSD), but (PSD) does not imply (POS).
This property ensures that the basis generated by the eigenvectors of L is
orthogonal and that the eigenvalues are real.
Dirichlet Convergence (CON). When considering a sequence of meshes
{Γi} converging to M in a given sense as i →∞, we want the Laplacian
sequence Li to converge to Δ with respect to the discrete Dirichlet problem.
The convergence is mandatory when we seek approximate solutions of partial
diﬀerential equations.
The (CON) property requires a formal deﬁnition of the sequence {Mi}. In addi-
tion to [24], we add the following property
Pointwise Convergence (PCON). For a given sequence meshes {Γi}, we
want the associated Laplacians {Li} to converge to Δ in a pointwise l2 or l∞
sense. This notion of convergence is stronger than (CON) which is implied by
(PCON).

246
T. Caissard et al.
For the Laplacian on digital surfaces (e.g. L⋆
h we deﬁne in Sect. 3), we consider
a sequence of combinatorial structures deﬁned as the boundaries of the Gauss
digitizations of M (see next section). We thus consider the (DPCON) property
as the pointwise convergence of the operator for multigrid digital surfaces.
Table 1 compares classical Laplacian discretizations with respect to these
properties. The (DPCON) property has been evaluated experimentally in Sect. 4.
LCOMBI being purely combinatorial, the same operator can be used for both
meshes and digital surfaces. For LCOT and LMESH, we have considered the
marching-cubes representation of the digital surfaces (more precisely, a contin-
uous analog of the dual surface [13]). Properties of L⋆
h are discussed in the next
section.
Table 1. Properties of various Laplacians. See text for the description of each prop-
erty.
means that the property is valid, whereas
means not. N.A. stands for Not
Applicable and ? means that we do not know whether it is true or not. Finally exp.
holds for experimental convergence with no known theoretical proof.
Ref
SYM
LOC
LIN
POS
PSD
CON
PCON DPCON
MEAN VALUE
[9]
?
N.A
INTRINSINC DEL
[3]
?
?
N.A
LCOMBI
[25]
LCOT
[8,10]
LMESH
[2]
?
L⋆
h
here
?
exp
?
N.A
exp
3
New Laplace-Beltrami Operator on Digital Surfaces
In the discretization schemes discussed above, the points of the combinator-
ial structure interpolate the underlying manifold. In order to deﬁne a discrete
Laplace-Beltrami operator on digital surfaces, a challenge is to work with a
combinatorial structure that only approximates the underlying manifold in a
Hausdorﬀsense. First, we extend Eq. (9) to digital surfaces. Proper deﬁnitions
of a digital surface can be found in [12,14]. Let us recall the Gauss digitization
process:
Deﬁnition 1. (Gauss digitization). Let h > 0 be the sampling grid step.
The Gauss Digitization of an Euclidean shape X ⊂Rd is deﬁned as Dh(X) :=
X ∩(hZ)d where d is the dimension.
For smooth object X in dimension 3 with boundary M := ∂X, the digital surface
is deﬁned as the topological boundary of Dh(X), denoted ∂hX (see [14] for details).
More precisely, the digital surface has a cellular representation in a Cartesian cubi-
cal grid and is composed of points of dimension 0 (pointels, E0), straight segments
of dimension 1 (linels, E1) and squares of dimension 2 (surfels, E2).

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
247
The topological boundary ∂hX is an O(h)-Hausdorﬀapproximation of M [14].
As a consequence, we need to map the smooth function u deﬁned on M to ∂hX:
Deﬁnition 2. (Extension of u to ∂hX). Given a smooth function u on M,
we deﬁne the extension ˜u of u to ∂hX as
˜u(s) := u(ξ(˙s)) ,
where ˙s is the centroid of the surfel s ∈E2, and ξ is the map that projects a
point of ∂hX onto the closest point of M.
We show below how to adapt the deﬁnition of [2], recalled in Eq. (9), to digital
surfaces. We chose this approach because LMESH has an interesting pointwise
convergence for triangular meshes. Our Laplace-Beltrami operator is thus deﬁned
as follows
Deﬁnition 3. (Digital Laplace-Beltrami operator). The digital Laplace-
Beltrami operator is deﬁned on ∂hX as:
(L⋆
h ˜u)(s) :=
1
4πt2
h

r∈E2
e−||˙r−˙s||2
4th
μ(r)(˜u(r) −˜u(s)),
(10)
where the sum is taken over all surfels of ∂hX, ˙r is the centroid of the surfel
r, μ(s) is equal to the dot product between an estimated normal and the trivial
normal orthogonal to the surfel s and th is a function of h tending to zero as h
tends to zero.
The quantity μ(s) is called the measure of the surfel s: it is the area of the
projected surfel s onto the tangent plane induced by the estimated normal. Nor-
mal vectors are estimated using the estimator presented in [5,15] which has the
multigrid convergence property. Note that summing μ for each surfel of the sur-
face leads to an estimation of the global area of the shape boundary, which itself
has a multigrid convergence property [14]. This surfel measure is a key ingredient
of the digital formalization of the operator leading to an experimental multigrid
convergence and isotropic properties when used for diﬀusion (see Sect. 4).
If we index surfels in E2, L⋆
h has a |E2|×|E2| matrix representation L⋆
h deﬁned
as follows:
(L⋆
h)ij =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
4πt2
h
e−
|| ˙sj −˙si||2
4th
μ(sj) if i ̸= j
−

k̸=i
(L⋆
h)ik
if i = j
(11)
In other words, if ˜u is constant and equal to 1, for a surfel s ∈E2 of index i,
we have (L⋆
h˜u)i = (L⋆
h˜u)(˙si).
For the properties listed in Table 1, we can ﬁrst observe that Eq. (11) implies
that we have property (POS) but not (SYM). As L⋆
h
performs a convolution
on the complete surface with a Gaussian kernel, we do not have the (LOC)
(similarly to LMESH). Although we do not provide a theoretical proof of (PSD)
for L⋆
h, eigenvalues of L⋆
h, have always been positive through all experiments. The
(PCON) property is not applicable to our framework. The pointwise convergence
(DPCON) is observed experimentally and discussed in Sect. 4.1.

248
T. Caissard et al.
4
Experiments
4.1
Experimental Convergence
We ﬁrst evaluate the multigrid convergence of our Laplace-Beltrami operator.
We consider a unit sphere S2 and three diﬀerent smooth functions u : S2 →R,
namely z, x2 and ex (see Fig. 2). Let θ be the azimutal angle, and φ the polar
angle. The spherical Laplacian is then:
ΔS2u(θ, φ) =
1
sin2 φ
∂2u
∂θ2 +
1
sin φ
∂
∂φ

sin φ∂u
∂φ
	
.
(12)
We compute the Gauss digitization Dh(S2) of the sphere for decreasing grid
steps h. Since the elements of ∂hS2 do not interpolate the sphere, u is extended
to ˜u as deﬁned in Sect. 3. We compute LCOMBI and L⋆
h directly on ∂hS2, but
LCOT and LMESH on the associated marching-cubes triangulation. Since the
vertices of this mesh coincide with the centroids of the surfels of ∂hS2, all these
operators LCOMBI, LMESH and L⋆
h are evaluated at the same points.
For L⋆
h, we use the normal vector estimator described in [5] to estimate the
measure of the surfels. In addition, for both L⋆
h and LMESH, the parameter th
is set to 0.1 × h
1
3 . As the discretization becomes ﬁner, the standard deviation
σ := √2th of the Gaussian function decreases and the number of points within
the standard deviation σ increases. The constant factor 0.1 is a scale term derived
from the unit sphere that sets the kernel to 1/10 of the sphere.
For comparison, in order to mimic the setting of [2], we have also considered
the Laplacian LP
MESH, which corresponds to LMESH when the vertices of the
marching-cubes are projected onto the sphere. In our framework, this operator
is the gold standard, since the position errors are corrected by the projection,
which is usually unknown.
For all the above operators, we plot in Fig. 2 the l2 and l∞error between the
computed Laplacians and the true spherical one against the grid step h. First,
we observe that errors for LCOT (in blue
) and LCOMBI (in green
)
are constant: clearly both operators are non-convergent. Non-convergence is also
observed for LMESH (in red
) but with lower errors. On the opposite, LP
MESH
(in orange
) shows convergence behavior for both l2 and l∞error, as expected
in [2]. Concerning L⋆
h (in purple
), experimental convergence holds for the
three functions. For the periodic function z, the convergence speed is slower than
LP
MESH whereas for the non-linear functions, we can see that convergence speed
is the same. Moreover, the l2 error for L⋆
h tends toward LP
MESH, and its l∞error
is close to that of LP
MESH.

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
249
0.01
0.1
0.0001
0.001
0.01
0.1
1
z
h
error (px)
0.01
0.1
0.1
1
x2
h
0.01
0.1
0.1
1
ex
h
error (px)
l∞
l2
l∞
l2
l∞
l2
l∞
l2
l∞
l2
LCOT
LCOMBI
LMESH
LP
MESH
L⋆
h
Fig. 2. Multigrid convergence graphs for various functions on S2, the unit sphere. Both
l2 error in plain line and l∞in dashed line are displayed for LCOMBI, LMESH, LP
MESH,
and L⋆
h.
4.2
Shape Approximation Using Eigenvectors Decomposition
In this section, we consider the spectral analysis framework to process shapes
geometry [16]. Given a shape and its Laplace-Beltrami operator, we compute
the eigenvalues and eigenvectors of the operator and project the geometry onto
the eigenvector basis of the ﬁrst k eigenvalues. More formally, given an operator
L, we denote by e1, e2, . . . , en its normalized eigenvectors and the matrix E
whose columns are those eigenvectors. By λ1, λ2, . . . , λn we denote the associated
increasing eigenvalues where n is the number of rows of L. Given three input
vectors (X, Y , Z) encoding the vertex positions in R3, we can approximate the
input shape using a ﬁxed number k of eigenvectors:

250
T. Caissard et al.
X(k) = E(k)(E(k))T X,
Y (k) = E(k)(E(k))T Y ,
Z(k) = E(k)(E(k))T Z,
where E(k) is a matrix of size n×k containing the ﬁrst k eigenvectors columnwise.
We compute the eigen decomposition on Dh(M) for LCOMBI, and L⋆
h in Fig. 3
on a bunny object (643, 13236 eigenvectors). We illustrate the reconstruction for
increasing number of eigenvectors k. For low frequencies (k ≤100), we observe
that L⋆
h captures more geometrical details than LCOMBI. For k = 100, we clearly
have a better approximation of both ears of the bunny shape. As k increases,
both reconstructions converge to the original bunny shape (E(n)(E(n))T = I).
In Fig. 4, we show the ﬁrst 20 eigenvectors on a axis aligned cube.
10
50
100
10000
10
50
100
10000
Fig. 3. Images of the reconstruction using an increasing number k of eigenvectors.
(First row) using LCOMBI, (second row) with L⋆
h (r = 6 for [5] and th = 3).
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Fig. 4. Eigenfunctions are displayed on a simple cube with faces aligned with the grid
axes (with a red to blue colormap and zero-crossing in white). (Color ﬁgure on line)

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
251
4.3
Heat Diﬀusion
In this section, we highlight an interesting isotropic property of L⋆
h compared to
the combinatorial one. We compute a heat diﬀusion when the source is a Dirac
in the center of a rotated cube face (heat diﬀusion is a preliminary step of [7] to
estimate geodesics on a manifold). We can derive an expression of g(x, t) from
Eq. (5):
(I −tL)g(x, t) = g0(x),
(13)
where I is the identity matrix, and L is the Laplacian matrix associated to any
discrete laplace operator. This diﬀusion only makes sense for small t and around
the Dirac g0(x). As the computed heat diﬀusion decreases exponentially, we
display the absolute value of its log. For LCOMBI (ﬁrst column in Fig. 5), both
small and large values of t lead anisotropic estimations of the intrinsic metric
due to the staircase eﬀect of the face (concentric rhombi or concentric ellipses
depending on t, see similar discussion in [7]). When using Eq. (13) with our
matrix based operator L⋆
h (second column), even if numerical instabilities occur
far from the Dirac depending on t, the intrinsic metric is perfectly estimated
(concentric circles). Note that both LMESH and L⋆
h already compute a diﬀusion
without the approximation of Eq. (13). The third column shows the associated
diﬀusion. Even if LMESH does not have multigrid convergence properties, on
this speciﬁc object, both methods provide good isotropic behaviors.
t = 0.01
t = 0.01
L⋆
h
t = 10
t = 10
LMESH
Fig. 5. Heat diﬀusion on a cube aligned with R3 axis. (First column) using LCOMBI,
(second column) using L⋆
h and (third column, with th = 4) using the diﬀusion computed
through the ambient heat kernel. The rightmost picture shows staircases on the rotated
cube.

252
T. Caissard et al.
5
Conclusion and Future-Works
In this paper, we have investigated diﬀerent discretization schemes of the
Laplace-Beltrami operator on digital surfaces. The contribution is twofold: ﬁrst,
we have shown that classical schemes either do not asymptotically converge to
the expected operator, or contain anisotropic artifacts when used for geometry
processing tasks. Second, we have proposed a new Laplace-Beltrami operator
that incorporates multigrid convergent surfel measures allowing us to have both
an experimental multigrid convergence and isotropic properties on digital sur-
faces.
A natural future-work consists of focusing on the multigrid convergence proof
of the operator. In dimension 2, a preliminary proof has been derived using digital
integration results from [14] but it is still an open problem in dimension 3.
References
1. Belkin, M., Niyogi, P.: Towards a theoretical foundation for laplacian-based man-
ifold methods. J. Comput. Syst. Sci. 74(8), 1289–1308 (2008)
2. Belkin, M., Sun, J., Wang, Y.: Discrete laplace operator on meshed surfaces. In:
Teillaud, M. (ed.) Proceedings of the 24th ACM Symposium on Computational
Geometry, College Park, MD, USA, pp. 278–287. ACM, 9–11 June (2008)
3. Bobenko, A.I., Springborn, B.: A discrete laplace-beltrami operator for simplicial
surfaces. Discrete Comput. Geom. 38(4), 740–756 (2007)
4. Cartade, C., Mercat, C., Malgouyres, R., Samir, C.: Mesh parameterization with
generalized discrete conformal maps. J. Math. Imaging. Vis. 46(1), 1–11 (2013)
5. Coeurjolly, D., Lachaud, J.O., Levallois, J.: Multigrid convergent principal cur-
vature estimators in digital geometry. Comput. Vis. Image Underst. 129, 27–41
(2014)
6. Coeurjolly, D., Lachaud, J.O., Roussillon, T.: Multigrid Convergence of Discrete
Geometric Estimators, pp. 395–424. Springer, Netherlands (2012)
7. Crane, K., Weischedel, C., Wardetzky, M.: Geodesics in heat: a new approach to
computing distance based on heat ﬂow. ACM Trans. Graph. (TOG) 32(5), 152
(2013)
8. Desbrun, M., Hirani, A.N., Leok, M., Marsden, J.E.: Discrete exterior calculus.
arXiv preprint math/0508341 (2005)
9. Dodgson, N.A., Floater, M.S., Sabin, M.A. (eds.): Advances in Multiresolution for
Geometric Modelling. Springer, Heidelberg (2005)
10. Hildebrandt, K., Polthier, K., Wardetzky, M.: On the convergence of metric
and geometric properties of polyhedral surfaces. Geom. Dedicata. 123(1), 89–112
(2006)
11. Hirani, A.N.: Discrete exterior calculus. Ph.D. thesis, California Institute of Tech-
nology (2003)
12. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Picture
Analysis. The Morgan Kaufmann Series in Computer Graphics and Geometric
Modeling. Elsevier, Amsterdam (2004)
13. Lachaud, J.O., Montanvert, A.: Continuous analogs of digital boundaries: a topo-
logical approach to iso-surfaces. Graph. Models Image Process. 62, 129–164 (2000)

Heat Kernel Laplace-Beltrami Operator on Digital Surfaces
253
14. Lachaud, J.O., Thibert, B.: Properties of gauss digitized shapes and digital surface
integration. J. Math. Imaging Vis. 54(2), 162–180 (2016)
15. Levallois, J., Coeurjolly, D., Lachaud, J.-O.: Parameter-Free and Multigrid Conver-
gent Digital Curvature Estimators. In: Barcucci, E., Frosini, A., Rinaldi, S. (eds.)
DGCI 2014. LNCS, vol. 8668, pp. 162–175. Springer, Cham (2014). doi:10.1007/
978-3-319-09955-2 14
16. L´evy, B., Zhang, H.: Spectral Mesh Processing. Technical. report, SIGGRAPH
Asia 2009 Courses (2008)
17. Mercat, C.: Discrete Complex Structure on Surfel Surfaces. In: Coeurjolly, D.,
Sivignon, I., Tougne, L., Dupont, F. (eds.) DGCI 2008. LNCS, vol. 4992, pp. 153–
164. Springer, Heidelberg (2008). doi:10.1007/978-3-540-79126-3 15
18. Molchanov, S.A.: Diﬀusion processes and riemannian geometry. Russ. Math. Surv.
30(1), 1 (1975)
19. Pinkall, U., Polthier, K.: Computing discrete minimal surfaces and their conjugates.
Exp. Math. 2(1), 15–36 (1993)
20. Polthier, K., Preuss, E.: Identifying vector ﬁeld singularities using a discrete Hodge
decomposition. Vis. Math. 3, 113–134 (2003)
21. Regge, T.: General relativity without coordinates. Il Nuovo Cimento Series 10
19(3), 558–571 (1961)
22. Rosenberg, S.: The Laplacian on a Riemannian Manifold. Cambridge University
Press, Cambridge Books Online (1997)
23. Varadhan, S.: On the behavior of the fundamental solution of the heat equation
with variable coeﬃcients. Commun. Pure Appl. Math. 20(2), 431–455 (1967)
24. Wardetzky, M., Mathur, S., Kaelberer, F., Grinspun, E.: Discrete Laplace opera-
tors: No free lunch. Eurographics Symposium on Geometry Processing, pp. 33–37
(2007)
25. Zhang, H.: Discrete combinatorial Laplacian operators for digital geometry process-
ing. In: SIAM Conference on Geometric Design, pp. 575–592. (2004, press)

Eﬃciently Updating Feasible Regions for Fitting
Discrete Polynomial Curve
Fumiki Sekiya1(B) and Akihiro Sugimoto2
1 Department of Informatics, SOKENDAI (The Graduate University
for Advanced Studies), Tokyo, Japan
2 National Institute of Informatics, Tokyo, Japan
{sekiya,sugimoto}@nii.ac.jp
Abstract. We deal with the problem of ﬁtting a discrete polynomial
curve to 2D data in the presence of outliers. Finding a maximal inlier
set from given data that describes a discrete polynomial curve is equiv-
alent with ﬁnding the feasible region corresponding to the set in the
parameter space. When iteratively adding a data point to the current
inlier set, how to update its feasible region is a crucial issue. This work
focuses on how to track vertices of feasible regions in accordance with
newly coming inliers. When a new data point is added to the current
inlier set, a new vertex is obtained as the intersection point of an edge
(or a face) of the feasible region for the current inlier set and a facet
(or two facets) of the feasible region for the data point being added.
Evaluating all possible combinations of an edge (or a face) and a facet
(or two facets) is, however, computationally expensive. We propose an
eﬃcient computation in this incremental evaluation that eliminates com-
binations producing no vertices of the updated feasible region. This com-
putation facilitates collecting the vertices of the updated feasible region.
Experimental results demonstrate our proposed computation eﬃciently
reduces practical running time.
1
Introduction
Contour detection is unavoidable for many image processing and/or computer
vision tasks such as object recognition, image segmentation and shape approxi-
mation. A contour is usually represented as a curve and, thus, contour detection
is reduced to ﬁtting a curve to noisy data. Since curves are discretized in the
digital image, discrete curve ﬁtting has been studied for decades for diﬀerent
classes of curves and diﬀerent discretization models [1–6,9,11–13]. An impor-
tant advantage of using a discrete curve over a continuous one, when used for
ﬁtting, is that it requires no empirical threshold in error to deﬁne an inlier that
aﬀects the output. We note that an underlying threshold that a discrete model
uses to collect its points is usually designed only to achieve some properties such
as connectivity (see [7,10] for example), and thus such a threshold is clearly
justiﬁed.
This paper deals with the problem of ﬁtting a discrete polynomial curve to
2D data in the presence of outliers, which is formulated as follows [8]: For a given
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 254–266, 2017.
DOI: 10.1007/978-3-319-66272-5 21

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
255
(a) (i, j) ∈D(a).
(b) (i, j) /∈D(a).
Fig. 1. Integer point in (not in) D(a). The black curves depict the underlying contin-
uous polynomial curve y = f(x) = d
l=0 alxl.
data set P = {(ip, jp) ∈Z2 | p = 1, . . . , n} (n < ∞) and a degree d, the discrete
polynomial curve ﬁtting is to ﬁnd the discrete polynomial curve D(a) that has
the maximum number of inliers, i.e., data points in D(a). Here D (a) is deﬁned
[10] by, with coeﬃcients a = (a0, . . . , ad) ∈Rd+1,
D (a) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i, j) ∈Z2

min
s∈{1,...,4}

(j + ys) −
d

l=0
al (i + xs)l
	
≤0 ≤
max
s∈{1,...,4}

(j + ys) −
d

l=0
al (i + xs)l
	
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎭
,
(1)
where (x1, y1) = (−1
2, −1
2), (x2, y2) = ( 1
2, −1
2), (x3, y3) = ( 1
2, 1
2), (x4, y4) =
(−1
2, 1
2). Equation (1) collects (i, j) ∈Z2 iﬀthe four points (i + xs, j + ys) for
s ∈{1, . . . , 4} lie on the both sides (y > d
l=0 alxl and y < d
l=0 alxl) of the
underlying continuous polynomial curve y = d
l=0 alxl, or at least one of them
is on y = d
l=0 alxl (Fig. 1). We employ this discretization model because the
connectivity is guaranteed [7].
This problem can be discussed in the parameter (coeﬃcient) space. For the
data point (ip, jp) (p ∈{1, . . . , n}), the feasible region Rp ⊆Rd+1 is deﬁned by
Rp =

a ∈Rd+1

min
s∈{1,...,4} h(p,s) (a) ≤0 ≤
max
s∈{1,...,4} h(p,s) (a)

,
where h(p,s) (a) = (jp + ys) −d
l=0 (ip + xs)l al (Fig. 2(a)). We remark that
(ip, jp) ∈D(a) iﬀa ∈Rp. Rp is an unbounded concave polytope, which is the
union of two unbounded convex polytopes deﬁned by h(p,1)(a) ≤0 ≤h(p,3)(a)
and h(p,2)(a) ≤0 ≤h(p,4)(a) (cf. Fig. 3(a)). For Π ⊆{1, . . . , n}, the feasible
region RΠ is deﬁned as the intersection of the feasible regions each of which is
for a data point (ip, jp) where p ∈Π: RΠ = 
p∈Π Rp (Fig. 2(b)). Since RΠ = ∅if
there exists no a ∈Rd+1 that satisﬁes {(ip, jp) | p ∈Π} ⊆D(a), we may assume
RΠ ̸= ∅below. Accordingly, our ﬁtting problem is formulated in the parameter

256
F. Sekiya and A. Sugimoto
Fig. 2. Feasible region. (a) shows Rp for d = 2 and (ip, jp) = (0, 0). (b) shows inter-
sections among the feasible regions for four data points indexed from 1 to 4. A darker
region has a larger number of inliers.
space as follows: Given P and d, ﬁnd Π ⊆{1, . . . , n} with the maximum |Π|
and a ∈RΠ for that Π.
This problem requires evaluating RΠ for all Π ⊂{1, . . . , n}, which is reduced
to classify each data point into an inlier or an outlier (we have 2n instances).
To this end, a heuristic based incremental approach [8] was proposed where it
iteratively evaluates whether a data point can be added to the current inlier
set until the inlier set does not have its superset. In this approach, the feasi-
ble region for the current inlier set is tracked by its vertices: when a new data
point is added to the current inlier set, a vertex of the new feasible region can be
obtained from the intersection points of an edge (or a face) of the current feasible
region and a facet (or two facets) of the feasible region for the data point being
added. Evaluating such possible combinations all is, however, computationally
expensive. The contribution of this paper is to facilitate this incremental evalu-
ation by introducing an eﬃcient computation of the vertices of the new feasible
region. Our introduced computation eliminates combinations producing no ver-
tex of the new feasible region, based on the property that an edge or face of
a bounded feasible region is inside the convex hull of its vertices. Though the
computational complexity is not reduced, our introduced computation eﬃciently
reduces running time in practice, as shown in experiments.
2
Brief Review of Incremental Approach
The incremental approach [8] starts with computing the feasible region for an
initialized inlier set. It then evaluates each data point one by one to update the
feasible region. If the updated feasible region is not empty, the data point is
added to the inlier set; it is regarded as an outlier otherwise. How to represent
and update the feasible region is a key issue there, which is brieﬂy explained
below.

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
257
2.1
Representing a Feasible Region Using Its Vertices
A vertex of a feasible region is deﬁned as an intersection point of its facets. For
p = 1, . . . , n, and s = 1, . . . , 4, a facet F(p, s) of Rp is deﬁned by
F (p, s) =

a ∈Rd+1

h(p,s) (a) = 0 and
s ∈arg min
s′∈{1,...,4}
h(p,s′) (a) ∪arg max
s′∈{1,...,4}
h(p,s′) (a)

.
F(p, s) is a part of the hyperplane h(p,s)(a) = 0 supporting Rp (Fig. 3(a)).
Similarly, a facet FΠ(p, s) of RΠ is deﬁned by FΠ (p, s) = F (p, s) ∩RΠ for
Π ⊆{1, . . . , n} and (p, s) ∈Π × {1, . . . , 4} (Fig. 3(b)). Note that FΠ(p, s) may
be empty for some (p, s).
A vertex of RΠ is given as the intersection of d + 1 facets. The set VΠ of the
vertices of RΠ is deﬁned by
VΠ =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
a ∈Rd+1

a ∈d+1
λ=1 FΠ (pλ, sλ) for some
(p1, s1) , . . . , (pd+1, sd+1) ⊆Π × {1, . . . , 4}
such that h(pλ,sλ) (a) = 0 for λ = 1, . . . , d + 1
are linearly independent
⎫
⎪
⎪
⎬
⎪
⎪
⎭
.
(2)
See Fig. 4 for an illustration of VΠ. Each combination of d+1 facets determining
an element in VΠ is indicated by an element in ΨΠ, which is deﬁned by
ΨΠ =
⎧
⎨
⎩
{(p1, s1) , . . . , (pd+1, sd+1)}
⊆Π × {1, . . . , 4}

h(pλ,sλ) (a) = 0 for λ = 1, . . . , d + 1
are linearly independent and
their solution is in d+1
λ=1 FΠ (pλ, sλ)
⎫
⎬
⎭.
(3)
In this way, the feasible region of the inlier set Π can be represented by
its vertices VΠ with the help of ΨΠ. Note that diﬀerent elements in ΨΠ may
determine the same element of VΠ.
...
(a) F(p, s).
...
(b) FΠ(p, s) (Π = {1, 2, 3}).
Fig. 3. Facets of a feasible region. (a): h(p,s)(a) = 0, s = 1, ...4 are depicted in blue
lines. mins∈{1,...,4} h(p,s)(a) = 0 is depicted in yellow, while maxs∈{1,...,4} h(p,s)(a) = 0
is depicted in pink. (Color ﬁgure online)

258
F. Sekiya and A. Sugimoto
...
Fig. 4. Vertices VΠ (yellow) of RΠ (Π = {1, 2, 3}). (Color ﬁgure online)
2.2
Tracking the Vertices of the Feasible Region
Theorem 1 indicates that ΨΠ plays an important role in tracking the vertices
of the updated feasible region when a new inlier (represented by p∗) comes in.
Note that RΠ is almost always bounded (see [8]).
Theorem 1 (Sekiya and Sugimoto[8]).
For Π ⊊{1, . . . , n} such that RΠ
is bounded and p∗∈{1, . . . , n}\Π, ΨΠ∪{p∗} ⊆ΨΠ ∪Φ1
Π,p∗∪Φ2
Π,p∗, where
Φ1
Π,p∗=
{(p1, s1) , . . . , (pd, sd)}
∪{(p∗, s∗)}

{(p1, s1) , . . . , (pd, sd)} is a subset of
an element in ΨΠ and s∗= 1, . . . , 4

,
Φ2
Π,p∗=
⎧
⎨
⎩
{(p1, s1) , . . . , (pd−1, sd−1)}
∪{(p∗, s∗
1) , (p∗, s∗
2)}

{(p1, s1) , . . . , (pd−1, sd−1)} is
a subset of an element in ΨΠ
and (s∗
1, s∗
2) ∈{(1, 2) , (3, 4)}
⎫
⎬
⎭.
{(p1, s1) , . . . , (pω, sω)} where ω = d and d −1 respectively corresponds to
an edge and a (2-dimensional) face of RΠ: ω
λ=1 FΠ(pλ, sλ) (the intersection
of ω facets). An element in Φ1
Π,p∗(resp. Φ2
Π,p∗) is thus considered to be the
combination of an edge (resp. a face) of RΠ and a facet (resp. two facets) of
Rp∗. Theorem 1 therefore indicates that a vertex of RΠ∪{p∗} is a vertex of RΠ
or otherwise obtained as the intersection point of an edge (resp. a 2-dimensional
face) of RΠ and a facet (resp. two facets) of Rp∗.
3
Eﬃcient Update of Vertices of Feasible Region
Sekiya and Sugimoto [8] evaluates whether each element in ΨΠ ∪Φ1
Π,p∗∪Φ2
Π,p∗
satisﬁes the condition in Eq. (3) to extract elements in ΨΠ∪{p∗}. When RΠ is
bounded, any edge or face of RΠ is inside the convex hull of the vertices of RΠ
on that edge or face (Lemma 1). Based on this, we introduce a computation to
eliminate elements in Φ1
Π,p∗∪Φ2
Π,p∗that cannot be in ΨΠ∪{p∗}. This enables us
to compute ΨΠ∪{p∗} eﬃciently.
We deﬁne a set of edges (or faces) of RΠ. Namely, for ω = d (edge), d −
1 (face), we deﬁne
Ψ (ω)
Π
=
 {(p1, s1) , . . . , (pω, sω)}
⊆Π × {1, . . . , 4}
 {(p1, s1) , . . . , (pω, sω)} ⊆ξ such that ξ ∈ΨΠ

.

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
259
...
Fig. 5. Illustration of AΠ,p∗(ψ). For ψ ∈Ψ (ω)
Π , 
(p,s)∈ψ FΠ(p, s) is depicted in green
and VΠ(ψ) in yellow. For s∗= 1, . . . , 4, h(p∗,s∗)(a) = 0 is depicted in a solid and dotted
red line where the solid part depicts F(p∗, s∗). In this example, s∗satisﬁes (i) in Eq. (4)
if h(p∗,s∗)(a) = 0 runs between the two yellow vertices, and (ii) if h(p∗,s∗)(a) = 0 is
depicted in a solid line on either of the dotted black lines which are parallel to the
a0 axis and passes through yellow vertices. AΠ,p∗(ψ) = {3}, accordingly. (Color ﬁgure
online)
We note that an edge (or face) is determined as the intersection of d (or d −1)
facets of RΠ. For an edge (or face) ψ ∈Ψ (ω)
Π
(ω = d, d −1), we denote by VΠ(ψ)
the vertices on ψ:
VΠ (ψ) = {a ∈VΠ | a is determined by ξ ∈ΨΠ such that ψ ⊆ξ} .
Let Conv(VΠ(ψ)) denote the convex hull of VΠ(ψ). Then we have Lemma 1
whose proof is provided in Appendix A.
Lemma 1. For Π ⊆{1, . . . , n} for which RΠ is bounded and ψ ∈Ψ (ω)
Π
(ω =
d, d −1), 
(p,s)∈ψ FΠ(p, s) ⊆Conv(VΠ(ψ)).
Suppose we are adding an new inlier p∗to the current inlier set Π. For each
ψ ∈Ψ (d)
Π , Φ1
Π,p∗has four elements ψ∪{(p∗, s∗)}, s∗= 1, . . . , 4. Lemma 1 allows to
identify ˜s∗∈{1, . . . , 4} such that 
(p,s)∈ψ FΠ(p, s) ∩F(p∗, ˜s∗) = ∅(the facet of
Rp∗corresponding to ˜s∗does not intersect with the edge ψ). We can thus deﬁne
the subset of Φ1
Π,p∗by eliminating ψ ∪{(p∗, ˜s∗)} and use the subset instead of
Φ1
Π,p∗itself in computing ΨΠ∪{p∗}. The same argument can be applied to Φ2
Π,p∗.
To exclude ˜s∗above, we deﬁne AΠ,p∗(ψ) for ψ ∈Ψ (ω)
Π
(ω = d, d −1), by
AΠ,p∗(ψ) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
s∗∈{1, . . . , 4}

(i)
min
a∈VΠ(ψ) h(p∗,s∗) (a) ≤0 ≤
max
a∈VΠ(ψ) h(p∗,s∗) (a) and
(ii) s∗∈arg min
s′∈{1,...,4}
h(p∗,s′) (a) ∪arg max
s′∈{1,...,4}
h(p∗,s′) (a)
for some a ∈VΠ (ψ)
⎫
⎪
⎪
⎬
⎪
⎪
⎭
.
(4)
With AΠ,p∗(ψ), we can identify the facets of Rp∗that potentially intersect
with the edge/face ψ (see Fig. 5). (i) in Eq. (4) means that the hyperplane

260
F. Sekiya and A. Sugimoto
h(p∗,s∗)(a) = 0 runs between two vertices of ψ, or passes through one of its ver-
tices. Since the edge or face is inside the convex hull of its vertices (Lemma 1),
it follows that the hyperplane intersects with ψ. This however does not indicate
that the facet determined by s∗intersects with ψ (the facet is only a part of the
hyperplane). We therefore have to evaluate whether the facet indeed intersects
with ψ, for which (ii) plays the role. (ii) means that, for at least one vertex
a ∈VΠ(ψ), s∗achieves the maximum or minimum of h(p∗,s′)(a), s′ ∈{1, . . . , 4};
unless (ii) is satisﬁed, the intersection of the hyperplane with ψ is out of the
facet.
Using only s∗∈{1, . . . , 4} in AΠ,p∗(ψ) for each ψ, we can deﬁne the subsets
of Φ1
Π,p∗and Φ2
Π,p∗that can be used in computing ΨΠ∪{p∗}.
X1
Π,p∗=

ψ ∪{(p∗, s∗)}
 ψ ∈Ψ (d)
Π
and s∗∈AΠ,p∗(ψ)

,
X2
Π,p∗=

ψ ∪{(p∗, s∗
1) , (p∗, s∗
2)}

ψ ∈Ψ (d−1)
Π
and s∗
1, s∗
2 ∈AΠ,p∗(ψ)
where (s∗
1, s∗
2) = (1, 2) or (3, 4)

.
Now we formally prove that no element in Φ1
Π,p∗\X1
Π,p∗or Φ2
Π,p∗\X2
Π,p∗is in
ΨΠ∪{p∗}.
Theorem 2. For Π
⊊
{1, . . . , n} such that RΠ
is bounded and p∗
∈
{1, . . . , n}\Π, ΨΠ∪{p∗} ⊆ΨΠ ∪X1
Π,p∗∪X2
Π,p∗.
Proof. Consider ψ ∈Ψ (ω)
Π
(ω = d, d−1). We show 
(p,s)∈ψ FΠ(p, s)∩F(p∗, s∗) =
∅for any s∗∈{1, . . . , 4}\AΠ,p∗(ψ). Note that this means ψ′ /∈ΨΠ∪{p∗} for any
ψ′ ∈Φd+1−ω
Π,p∗
\Xd+1−ω
Π,p∗
.
We ﬁrst assume that s∗does not satisfy (i) in Eq. (4): h(p∗,s∗)(a) < 0 for
∀a ∈VΠ(ψ) or h(p∗,s∗)(a) > 0 for ∀a ∈VΠ(ψ). From Lemma 1 and the linearity
of h(p∗,s∗), we have h(p∗,s∗)(a) < 0 for ∀a ∈
(p,s)∈ψ FΠ(p, s) or h(p∗,s∗)(a) > 0
for ∀a ∈
(p,s)∈ψ FΠ(p, s). It follows that 
(p,s)∈ψ FΠ(p, s) ∩F(p∗, s∗) = ∅.
We next assume that s∗does not satisfy (ii): s∗/∈arg mins′∈{1,...,4} h(p∗,s′)(a)
∪arg maxs′∈{1,...,4} h(p∗,s′)(a) for ∀a ∈VΠ(ψ). Lemma 1 and the linearity of
h(p∗,s∗) imply s∗/∈arg mins′∈{1,...,4} h(p∗,s′)(a) ∪arg maxs′∈{1,...,4} h(p∗,s′)(a)
for ∀a ∈
(p,s)∈ψ FΠ(p, s). It follows that 
(p,s)∈ψ FΠ(p, s) ∩F(p∗, s∗) = ∅.
⊓⊔
4
Algorithm
Algorithm 1 [8]1 is the incremental approach to solve the discrete polynomial
curve ﬁtting for a given data point set P and a given degree d. It classiﬁes each
data index into two classes: Π (inlier) and Π∁(outlier). Π is ﬁrst initialized to
be a set of d + 1 data indices for which VΠ and ΨΠ are computed at low cost
1 Because the initial inlier selection is not the scope of this paper, Algorithm 1 is
presented without any initial inlier set.

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
261
Algorithm 1. Incremental algorithm (Sekiya+ [8]).
Require: P, d.
Ensure: Π ⊆{1, . . . , n} and VΠ.
1: Initialize Π := any d + 1 data indices in {1, . . . , n} for which RΠ is bounded.
2: Initialize Π∁:= ∅.
3: Compute VΠ and ΨΠ using Eqs. (2) and (3).
4: while {1, . . . , n} \ (Π ∪Π∁) ̸= ∅do
5:
p∗:= any data index in {1, . . . , n} \ (Π ∪Π∁).
6:
Compute VΠ∪{p∗} and ΨΠ∪{p∗}
7:
if VΠ∪{p∗} ̸= ∅then
8:
Π := Π ∪{p∗} and update VΠ and ΨΠ.
9:
else
10:
Π∁:= Π∁∪{p∗}.
11:
end if
12: end while
13: return Π and VΠ.
(see [8] for the suﬃcient condition that RΠ is bounded). In the following loop
(Steps 4–12), we add new data indices to either Π or Π∁one by one. When Π
is updated, VΠ and ΨΠ are also updated. Since ΦΠ∪{p∗} ̸= ∅if RΠ∪{p∗} ̸= ∅(see
[8]), an inlier set obtained by Algorithm 1 is guaranteed to have no superset.
The purpose of Algorithm 2 is to eﬃciently compute VΠ∪{p∗} and ΨΠ∪{p∗} in
Step 6 of Algorithm 1: eﬃcient computation of the vertices of the feasible region
updated by an additional data point. Some of the vertices are inherited from the
current feasible region. So, the ﬁrst loop (Steps 2–7) evaluates each vertex of the
current feasible region to check if it serves as a vertex of the updated feasible
region, where it suﬃces only to verify that the vertex is inside the feasible region
for the additional data point (Step 5), since FΠ(p, s) ∩Rp∗= FΠ∪{p∗}(p, s) for
any (p, s) ∈Π × {1, . . . , 4}. The vertices appearing only in the updated feasible
region are obtained from X1
Π,p∗∪X2
Π,p∗in the second loop (Steps 9–16). For
each element in X1
Π,p∗∪X2
Π,p∗, we ﬁrst check if the hyperplanes corresponding
to the element intersect at a unique point (Step 10), and if so, we then check if
that intersection point serves as a vertex of the updated feasible region (Step 12).
Note that the condition in Step 12 is equivalent with a ∈
(p,s)∈ψ FΠ∪{p∗}(p, s);
a ∈
(p,s)∈ψ F(p, s) implies a ∈Rp∗since any ψ ∈X1
Π,p∗∪X2
Π,p∗contains (p, s)
such that p = p∗. The most eﬃciently working part is Step 8, which reduces the
number of iterations in the second loop.
The computational complexity for this method is the same with [8]: O(nd+2)
for a variable number n of data and a ﬁxed degree d, because O(|Xα
Π,p∗|) =
O(|Φα
Π,p∗|) = O(|Ψ (ω)
Π |) for α = 1, 2 where ω = d+1−α. The practical eﬃciency
of the method is evaluated in the next section.

262
F. Sekiya and A. Sugimoto
Algorithm 2. Eﬃcient update of VΠ and ΨΠ for an additional inlier.
Require: P, d, Π ⊊{1, . . . , n}, p∗∈{1, . . . , n} \ Π, VΠ and ΨΠ.
Ensure: VΠ∪{p∗} and ΨΠ∪{p∗}.
1: Initialize V := ∅and Ψ := ∅.
2: for all ψ ∈ΨΠ do
3:
a := vertex determined by ψ.
4:
if a ∈Rp∗then
5:
V := V ∪{a} and Ψ := Ψ ∪{ψ}.
6:
end if
7: end for
8: Compute AΠ,p∗(ψ) for all ψ ∈Ψ (ω)
Π
(ω = d, d −1) to have X1
Π,p∗and X2
Π,p∗.
9: for all ψ ∈X1
Π,p∗∪X2
Π,p∗do
10:
if {h(p,s)(a) = 0 | (p, s) ∈ψ} has a unique solution a then
11:
a := the unique solution to {h(p,s)(a) = 0 | (p, s) ∈ψ}
12:
if a ∈
(p,s)∈ψ F(p, s) ∩RΠ then
13:
V := V ∪{a} and Ψ := Ψ ∪{ψ}.
14:
end if
15:
end if
16: end for
17: return V = VΠ∪{p∗} and Ψ = ΨΠ∪{p∗}.
5
Experiments
For d = 2, we generated input data sets P for n = 200, 400, 600, 800, 1000 as
follows: setting (a0, a1, a2) = (450, −3.2, 0.0064), we randomly generated n inte-
ger points within [0, 499] × [0, 499] so that 80% of the points, called ground-
truth inliers, are in D(a0, a1, a2) while the other 20% points, called ground-truth
outliers, are not in D(a0, a1, a2), where each ground-truth outlier was gener-
ated so that its Euclidean distance from its closest point in D(a0, a1, a2) is
in [1, 4]. In the same way, we generated data sets for d = 3 where we used
(a0, a1, a2, a3) = (250, 5, −0.03, 4.0×10−5) to generate their ground-truth inliers
and outliers. P is shown in Fig. 6 for n = 200, 600, 1000. In the experiments, we
used a PC with an Intel Xeon 3.7 GHz processor with 64 GB memory.
We applied our proposed method (Algorithm 1 together with Algorithm 2)
100 times to each P to see the eﬃciency of our introduced computation. At
each trial, we randomly initialized Π (Step 1) and selected p∗(Step 5), where
the d + 1 data indices in the initial Π are chosen only from the ground-truth
inliers. For comparison, we also applied Algorithm 1 alone (Sekiya+ [8]) 100
times to each P using the same initialization and data point selection. We then
evaluated the recall (the ratio of ground-truth inliers in the output against the
whole ground-truth inliers) and the computational time (i.e., processing time).
Tables 1 and 2 show the average of recalls over 100 trials for each P and the
average of computational times over 100 trials for the two methods. We remark
that the outputs by our proposed method are exactly the same as those by
Algorithm 1 alone.

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
263
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(a) d = 2, n = 200.
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(b) d = 2, n = 600.
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(c) d = 2, n = 1000.
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(d) d = 3, n = 200.
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(e) d = 3, n = 600.
 0
 100
 200
 300
 400
 500
 0
 100
 200
 300
 400
 500
(f) d = 3, n = 1000.
Fig. 6. Examples of input data sets. Ground-truth inliers are depicted in green while
ground-truth outliers in red. (Color ﬁgure online)
We see in Table 2 that our proposed method achieves the same results much
faster than Algorithm 1 alone. We see that Algorithm 2 indeed eﬃciently updates
feasible regions in computational time thanks to |X1
Π,p∗∪X2
Π,p∗| < |Φ1
Π,p∗∪
Φ2
Π,p∗|. To visualize this eﬃciency, we compared |X1
Π,p∗∪X2
Π,p∗| and |Φ1
Π,p∗∪
Φ2
Π,p∗| in each iteration in a trial in the case of n = 400 (Fig. 7). We observe
that (1) |X1
Π,p∗∪X2
Π,p∗| is signiﬁcantly smaller than |Φ1
Π,p∗∪Φ2
Π,p∗| and that
(2) |X1
Π,p∗∪X2
Π,p∗| is almost constant independent of the size of the inlier set.
We remark that X1
Π,p∗∪X2
Π,p∗= ∅indicates AΠ,p∗(ψ) = ∅for all ψ ∈Ψ (ω)
Π
(ω = d, d −1).
Table 1. Recall of ground-truth inliers (average over 100 trials).
n
200
400
600
800
1000
d = 2 0.870 0.860 0.810 0.859 0.855
d = 3 0.817 0.805 0.773 0.806 0.822
Table 2. Computational time (ms) (average over 100 trials).
n
200
400
600
800
1000
d = 2 Sekiya+[8]
36.92
69.16
90.92 138.32 177.28
Proposed
1.16
1.88
2.32
3.28
3.60
d = 3 Sekiya+[8] 154.16 267.32 366.16 488.36 627.16
Proposed
8.68
9.12
10.36
9.76
11.12

264
F. Sekiya and A. Sugimoto
 0
 20
 40
 60
 80
 100
 120
 0
 50
 100
 150
 200
 250
 300
 350
 400
# of data evaluated (iteration)
(a) d = 2.
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0
 50
 100
 150
 200
 250
 300
 350
 400
# of data evaluated (iteration)
(b) d = 3.
Fig. 7. |X1
Π,p∗∪X2
Π,p∗| (red) and |Φ1
Π,p∗∪Φ2
Π,p∗| (green) in each iteration. The hori-
zontal axis is the number of data points already evaluated (i.e., |Π ∪Π∁|). The results
are from a trial of n = 400. (Color ﬁgure online)
6
Conclusion
We dealt with the problem of ﬁtting a discrete polynomial curve to 2D data in
the presence of outliers. We discussed how to eﬃciently compute the vertices
of the feasible region for an incrementally updated inlier set in the parameter
space. Based on the property that an edge or face of a bounded feasible region is
inside the convex hull of its vertices, we introduced a computation to facilitate
updating the vertices of the feasible region when a new data point is added to the
current inlier set. The eﬃciency of our proposed computation was demonstrated
by our experimental results.
Acknowledgements. This work is in part supported by Grant-in-Aid for Scientiﬁc
Research (Grant No. 16H02851) of the Ministry of Education, Culture, Sports, Science
and Technology of Japan.
A
Appendix: Proof of Lemma 1
Proof. For any b ∈
(p,s)∈ψ FΠ(p, s), we prove that b ∈Conv(VΠ(ψ)), i.e.,
b is represented as a convex combination of some vertices in VΠ(ψ). In fact, it
suﬃces to show that there exist c1, c2 ∈
(p,s)∈ψ FΠ(p, s) and (p1, s1), (p2, s2) ∈
Π × {1, . . . , 4} that satisfy the following: c1 ∈FΠ(p1, s1) and c2 ∈FΠ(p2, s2),
the linear systems {h(p,s)(a) = 0 | (p, s) ∈ψ ∪{(p1, s1)}} and {h(p,s)(a) =
0 | (p, s) ∈ψ ∪{(p2, s2)}} are respectively independent, and b is represented
as a convex combination of c1 and c2. Why this proves b ∈Conv(VΠ(ψ)) is
explained as follows. For ψ ∈Ψ (d)
Π , it is obvious since we have c1, c2 ∈VΠ(ψ)
immediately. For ψ ∈Ψ (d−1)
Π
, next, each of c1 and c2 can be seen as b in the case
of ψ ∈Ψ (d)
Π ; ψ∪{(p1, s1)}, ψ∪{(p2, s2)} ∈Ψ (d)
Π is proven by Lemma 1 in [8]. From
the result already obtained for ψ ∈Ψ (d)
Π , therefore, c1 and c2 are respectively
represented as a convex combination of two vertices in VΠ(ψ ∪{(p1, s1)}) and

Eﬃciently Updating Feasible Regions for Fitting Discrete Polynomial Curve
265
VΠ(ψ ∪{(p2, s2)}). b is represented as a convex combination of the four vertices
in VΠ(ψ), accordingly.
We therefore prove for the existence of c1, c2, (p1, s1), (p2, s2) as described
above. If there exists (p′, s′) ∈(Π × {1, . . . , 4})\ψ such that b ∈FΠ(p′, s′) and
{h(p,s)(a) = 0 | (p, s) ∈ψ∪{(p′, s′)}} is independent, then the required condition
is immediately satisﬁed for c1 = c2 = b and (p1, s1) = (p2, s2) = (p′, s′). We
therefore give the proof for the other case. Let us ﬁrst consider the (d + 1 −ω)-
dimensional ﬂat {a ∈Rd+1 | h(p,s)(a) = 0 for (p, s) ∈ψ}, which includes

(p,s)∈ψ FΠ(p, s). We then consider an arbitrary half-line on the ﬂat running
from b. Note that such a half-line necessarily exists since d + 1 −ω ≥1. A point
in the half-line is represented by c(r) = b + rv for some vector v ∈Rd+1\{0}
and a parameter r ∈R≥0. For such a half-line, it has been already shown in
the proof of Lemma 1 in [8] that, for some r1 > 0 (r1 < ∞), c1 = c(r1)
satisﬁes c1 ∈FΠ(p1, s1) for ∃(p1, s1) ∈(Π×{1, . . . , 4})\ψ such that {h(p,s)(a) =
0 | (p, s) ∈ψ ∪{(p1, s1)}} is independent. c2, on the other hand, is found on the
half-line running in the opposite direction from b, whose point is represented by
c′(r) = b + r(−v). Since it is also a half-line on the same ﬂat, for some r2 > 0
(r2 < ∞), c2 = c′(r2) satisﬁes c2 ∈FΠ(p2, s2) for ∃(p2, s2) ∈(Π ×{1, . . . , 4})\ψ
such that {h(p,s)(a) = 0 | (p, s) ∈ψ ∪{(p2, s2)}} is independent. Now, b is on
the line segment connecting c1 and c2, which means that b is represented as a
convex combination of c1 and c2.
⊓⊔
References
1. Buzer, L.: An incremental linear time algorithm for digital line and plane recogni-
tion using a linear incremental feasibility problem. In: Braquelaire, A., Lachaud,
J.-O., Vialard, A. (eds.) DGCI 2002. LNCS, vol. 2301, pp. 372–381. Springer,
Heidelberg (2002). doi:10.1007/3-540-45986-3 33
2. Kenmochi, Y., Buzer, L., Talbot, H.: Eﬃciently computing optimal consensus of
digital line ﬁtting. In: International Conference on Pattern Recognition (ICPR
2010), pp. 1064–1067. IEEE (2010)
3. Largeteau-Skapin, G., Zrour, R., Andres, E.: O(n3logn) time complexity for the
optimal consensus set computation for 4-connected digital circles. In: Gonzalez-
Diaz, R., Jimenez, M.-J., Medrano, B. (eds.) DGCI 2013. LNCS, vol. 7749,
pp. 241–252. Springer, Heidelberg (2013). doi:10.1007/978-3-642-37067-0 21
4. Largeteau-Skapin, G., Zrour, R., Andres, E., Sugimoto, A., Kenmochi, Y.: Opti-
mal consensus set and preimage of 4-connected circles in a noisy environment. In:
Proceedings of the International Conference on Pattern Recognition (ICPR 2012),
pp. 3774–3777. IEEE (2012)
5. Provot, L., Gerard, Y.: Recognition of digital hyperplanes and level layers with
forbidden points. In: Aggarwal, J.K., Barneva, R.P., Brimkov, V.E., Koroutchev,
K.N., Korutcheva, E.R. (eds.) IWCIA 2011. LNCS, vol. 6636, pp. 144–156.
Springer, Heidelberg (2011). doi:10.1007/978-3-642-21073-0 15
6. Sekiya, F., Sugimoto, A.: Fitting discrete polynomial curve and surface to noisy
data. Ann. Math. Artif. Intell. 75(1–2), 135–162 (2015)
7. Sekiya, F., Sugimoto, A.: On connectivity of discretized 2D explicit curve. In:
Ochiai, H., Anjyo, K. (eds.) Mathematical Progress in Expressive Image Synthesis
II. MI, vol. 18, pp. 33–44. Springer, Tokyo (2015). doi:10.1007/978-4-431-55483-7 4

266
F. Sekiya and A. Sugimoto
8. Sekiya, F., Sugimoto, A.: Discrete polynomial curve ﬁtting guaranteeing inclusion-
wise maximality of inlier set. In: Chen, C.-S., Lu, J., Ma, K.-K. (eds.) ACCV
2016. LNCS, vol. 10117, pp. 477–492. Springer, Cham (2017). doi:10.1007/
978-3-319-54427-4 35
9. Sere, A., Sie, O., Andres, E.: Extended standard Hough transform for analytical line
recognition. In: Proceedings of International Conference on Sciences of Electronics,
Technologies of Information and Telecommunications (SETIT 2012), pp. 412–422.
IEEE (2012)
10. Toutant, J.-L., Andres, E., Largeteau-Skapin, G., Zrour, R.: Implicit digital sur-
faces in arbitrary dimensions. In: Barcucci, E., Frosini, A., Rinaldi, S. (eds.)
DGCI 2014. LNCS, vol. 8668, pp. 332–343. Springer, Cham (2014). doi:10.1007/
978-3-319-09955-2 28
11. Zrour, R., Kenmochi, Y., Talbot, H., Buzer, L., Hamam, Y., Shimizu, I., Sugimoto,
A.: Optimal consensus set for digital line and plane ﬁtting. Int. J. Imaging Syst.
Technol. 21(1), 45–57 (2011)
12. Zrour, R., Largeteau-Skapin, G., Andres, E.: Optimal consensus set for annulus
ﬁtting. In: Debled-Rennesson, I., Domenjoud, E., Kerautret, B., Even, P. (eds.)
DGCI 2011. LNCS, vol. 6607, pp. 358–368. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-19867-0 30
13. Zrour, R., Largeteau-Skapin, G., Andres, E.: Optimal consensus set for nD ﬁxed
width annulus ﬁtting. In: Barneva, R.P., Bhattacharya, B.B., Brimkov, V.E. (eds.)
IWCIA 2015. LNCS, vol. 9448, pp. 101–114. Springer, Cham (2015). doi:10.1007/
978-3-319-26145-4 8

A New Shape Descriptor Based
on a Q-convexity Measure
P´eter Bal´azs1(B) and Sara Brunetti2
1 Department of Image Processing and Computer Graphics, University of Szeged,
´Arp´ad t´er 2., Szeged 6720, Hungary
pbalazs@inf.u-szeged.hu
2 Dipartimento di Ingegneria dell’Informazione e Scienze Matematiche, Via Roma,
56, 53100 Siena, Italy
sara.brunetti@unisi.it
Abstract. In this paper we deﬁne a new measure for shape descriptor.
The measure is based on the concept of convexity by quadrant, called
Q-convexity. Mostly studied in Discrete Tomography, this convexity gen-
eralizes hv-convexity to any two or more directions, and presents inter-
esting connections with “total” convexity. The new measure generalizes
that proposed by Bal´azs and Brunetti (A measure of Q-convexity, LNCS
9647 (2016) 219–230), and therefore it has the same desirable features:
(1) its values range intrinsically from 0 to 1; (2) its values equal 1 if and
only if the binary image is Q-convex; (3) its eﬃcient computation can
be easily implemented; (4) it is invariant under translation, reﬂection,
and rotation by 90◦. We test the new measure for assessing sensitivity
using a set of synthetic polygons with rotation and translation of intru-
sions/protrusions and global skew, and for a ranking task using a variety
of shapes. Based on the geometrical properties of Q-convexity, we also
provide a characterization of any binary image by the matrix of its “gen-
eralized salient points”, and we design a linear-time algorithm for the
construction of the binary image from its associated matrix.
Keywords: Shape descriptor · Convexity measure · Q-convexity ·
Salient point
1
Introduction
Shape descriptors are widely used in image processing and computer vision for
object detection, classiﬁcation, and recognition [8,12]. One class of descriptors
captures single geometrical or topological characteristics of shapes, like moments
[7], orientation and elongation [15], circularity [10], just to mention a few. Among
them, probably the most often studied descriptor is the measure of convexity.
Depending on whether the interior or the boundary of the shape is investigated
in order to determine the degree of convexity, these measures can be grouped
into area-based [2,12,13] and boundary-based [14] categories.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 267–278, 2017.
DOI: 10.1007/978-3-319-66272-5 22

268
P. Bal´azs and S. Brunetti
In [1] we proposed a convexity measure which uses both boundary and area
information, thus falls between the two above mentioned classes. It is based
on the concept of Q-convexity [4,5], mostly studied in Discrete Tomography
[9] for its good properties (it generalizes so-called hv-convexity to any two or
more directions, and has interesting connections with “total” convexity). The
notion of salient points of a Q-convex image has been introduced in [6] as the
analogue of extremal points of a convex set. They have similar features, and in
particular a Q-convex image is characterized by its salient points. Salient points
can be generalized for any binary image, and they have been studied to model
the “complexity” of a binary image which led to the convexity measure of [1,3].
The novel idea of this paper is to consider generalized salient points to have
diﬀerent weights – depending on “how” far they are from the boundary – when
calculating the convexity measure. In this way we provide a ﬂexible extension
of the measure of [1]. For this purpose we introduce the matrix of generalized
salient points of a binary image (shortly, GS matrix) and study its properties.
We provide a linear-time algorithm for the construction of the binary image from
its GS matrix and also describe how the measure can also be computed in linear
time.
The structure of the paper is the following. In Sect. 2 we present the basic
concepts and give the deﬁnition of the new measure of Q-convexity. In Sect. 3
we introduce the matrix of generalized salient points (GS matrix, for short) and
describe its properties. We also design a linear-time algorithm for the construc-
tion of the binary image from its GS matrix. The aim of Sect. 4 is to brieﬂy show
how the introduced measure can be eﬃciently computed, in linear time in the
size of the image. In Sect. 5 we present experimental results. Finally, Sect. 6 is
for the conclusion.
2
New Q-convexity Measure
Any binary image F is a m × n binary matrix, and it can be represented by
a set of black, foreground pixels denoted by F, and white, background pixels
(unit squares) (see Fig. 1 left). Equivalently, foreground pixels can be regarded
as points of Z2 contained in a lattice grid G (rectangle of size m × n) up to a
translation so that any binary image can be viewed as a subset of G, also called
lattice set (see Fig. 1 right). Throughout the paper, we will consider images dif-
ferent from the emptyset and use both representations as interchangeable, since
notation for the latter one is more suitable to describe geometrical properties
(even if the order of the points in the lattice and the order of the items in a
matrix are diﬀerent). For our convenience when not confusing, we use F for
both the image and its representations, and we denote by Fc the complement
of F, i.e., the image obtained as the complement of its pixel values reversing
foreground and background pixels. In the lattice representation, Fc corresponds
to G\F.

A New Shape Descriptor Based on a Q-convexity Measure
269
Fig. 1. A binary image represented as black and white pixels (left), and by a lattice
set (right).
2.1
Background
Let us introduce the main deﬁnitions concerning Q-convexity [4,6]. In order to
simplify our explanation, let us consider the horizontal and vertical directions,
and denote the coordinate of any point M of the grid G by (xM, yM). Then, M
and the directions determine the following four quadrants:
Z0(M) = {N ∈G : 0 ≤xN ≤xM, 0 ≤yN ≤yM}
Z1(M) = {N ∈G : xM ≤xN < m, 0 ≤yN ≤yM}
Z2(M) = {N ∈G : xM ≤xN < m, yM ≤yN < n}
Z3(M) = {N ∈G : 0 ≤xN ≤xM, yM ≤yN < n}.
Deﬁnition 1. A lattice set F is Q-convex if Zp(M) ∩F ̸= ∅for all p = 0, . . . , 3
implies M ∈F.
If Zp(M) ∩F = ∅, we say that Zp(M) is a background quadrant. Thus, in other
words, a binary image is Q-convex if there exists at least a background quadrant
Zp(M) for every pixel M in the background of F. Figure 2 illustrates the above
concepts.
Z M
0(
)
Z M
1(
)
Z M
2(
)
Z M
3(
)
M
Z M
0(
)
Z M
1(
)
Z M
2(
)
Z M
3(
)
M
Fig. 2. Illustration of the concept of Q-convexity. A Q-convex (left) and a non-Q-
convex (right) lattice set. Note that the image on the left is the Q-convex hull of the
image on the right.

270
P. Bal´azs and S. Brunetti
The Q-convex hull of F can be deﬁned as follows:
Deﬁnition 2. The Q-convex hull Q(F) of a lattice set F is the set of points
M ∈G such that Zp(M) ∩F ̸= ∅for all p = 0, . . . , 3.
By Deﬁnitions 1 and 2, if F is Q-convex then F = Q(F). Diﬀerently, if F is
not Q-convex, then Q(F)\F ̸= ∅(see Fig. 2, again, where for the lattice set F
on the right, Q(F)\F = {M}).
We deﬁne a new measure in between region- and boundary-based measures
exploiting some geometrical properties of the “shape”.
Deﬁnition 3. Let F be a lattice set. A point M ∈F is a salient point of F if
M /∈Q(F\{M}).
Denote the set of salient points of F by S(F). Of course S(F) = ∅if and only
if F = ∅. In particular, Daurat proved in [6] that the salient points of F are the
salient points of the Q-convex hull Q(F) of F, i.e. S(F) = S(Q(F)). This means
that if F is Q-convex, its salient points completely characterize F. If it is not,
there are other points belonging to the Q-convex hull of F but not in F that
“track” the non-Q-convexity of F. These points are called generalized salient
points (abbreviated by g.s.p.). The set of generalized salient points Sg(F) of F
is obtained by iterating the deﬁnition of salient points on the sets obtained each
time by discarding the points of the set from its Q-convex hull, i.e., using the
set notation:
Deﬁnition 4. If F is a lattice set, then the set of its generalized salient
points (g.s.p.) Sg(F) is deﬁned by Sg(F) = 
i S(Fi), where F1 = F, Fi =
Q(Fi−1)\Fi−1.
With the obvious meaning we may denote the binary images related to Q(F),
S(F), and Fi by Q(F), S(F), and Fi, respectively. Figure 3 illustrates the deﬁn-
ition in the lattice representation. We notice that Fi is contained in Fc
i−1 (more
precisely, in Q(Fi−1)\Fi−1), and if i is even, Fi is contained in Fc
1, else if i is odd
Fi is contained in F1. In the pixel-representation, this corresponds to say that
foreground and background pixels in Fi correspond to white and black pixels
for i even, and to black and white pixels for i odd, respectively. In this view,
Fig. 3. Generalized salient points are in black. Leftmost: F1. Centre-left: F2. Centre-
right: F3. Rightmost: F4.

A New Shape Descriptor Based on a Q-convexity Measure
271
the Q-convex hull of the foreground pixels of Fi−1 contains the Q-convex hull of
the foreground pixels of Fi. Moreover if F and F′ are two binary images, then
Sg(F) is diﬀerent from Sg(F′) (see Theorem 9 of [6]).
Let k be the index such that Fk+1 = ∅. By deﬁnition, S(F) = S(F1) ⊆
Sg(F) = S(F1) ∪S(F2) ∪. . . ∪S(Fk) and the equality holds when F is Q-
convex. Moreover, the points of Sg(F) are chosen among the points of subsets
of Q(F), thus Sg(F) ⊆Q(F).
2.2
The Generalized Shape Descriptor
In [1], we deﬁned a shape measure in terms of proportion between salient points
and generalized salient points. Denoting the cardinality of an arbitrary set P of
points by |P|, here we generalize the measure as follows:
Deﬁnition 5. For a given binary image F, its Q-convexity measure Ψ(ci)(F) is
deﬁned by
Ψ(ci)(F) =
|S(F)|

i ci|S(Fi)|,
where S(F) and S(Fi) are as in Deﬁnition 4, c1 = 1, and each ci is a non-
negative real number.
Notice that c1 = 1 must hold in order to get value 1 for Q-convex sets.
Note also that the measure is purely qualitative because is independent from
the size of the image. It coincides with the measure in [1] if ci = 1, for all i, and,
more generally, if the g.s.p. are many with respect to salient points, then F is
far to be Q-convex. Besides, the dependence on successive |S(Fi)| depends on
the choice of ci: if ci is a decreasing function, then the measure scores heavily
g.s.p. in the boundary with respect to the g.s.p. in the interior, and vice-versa in
case of an increasing function. This approach provides, in fact, a family of shape
descriptors (by setting the weights diﬀerently). Each member of the family could
complement other ones, thus, giving a ﬁnely tuneable tool for solving pattern
recognition issues, as diﬀerent weightings can capture diﬀerent aspects of the
shapes (see the ranking examples in Sect. 5).
Since S(F) ⊆Sg(F) ⊆Q(F), the Q-convexity measure satisﬁes the following
properties:
– the Q-convexity measure ranges from 0 to 1;
– the Q-convexity measure equals 1 if and only if F is Q-convex.
In particular, S(F) = Sg(F), and hence Ψ(ci)(F) = 1, (for instance when F is
a full rectangle as the rightmost image in Fig. 6) if and only if F is Q-convex.
If Sg(F) = Q(F) (for instance if F is a chessboard), Ψ(ci)(F) decreases with
the inverse of the size of Q(F) in the case where ci = 1 for all i. So, if we
consider the sequence of images starting from the full rectangle and ending with
the chessboard, and having intermediate images obtained by deleting each time
one suitable pixel iteratively row by row, they have measure values decreasing
from 1 to
4
mn (circa 6 · 10−5 for m = n = 256).

272
P. Bal´azs and S. Brunetti
Moreover, since S(F), S(Fi) and Q(F) are invariant under translation,
reﬂection, and rotation by 90◦for the horizontal and vertical directions, the
measure is also invariant.
3
The GS Matrix
Let F be a m×n binary image, and Sg(F) = S(F1)∪S(F2)∪. . .∪S(Fk). Consider
the matrix representation of F = (fij). We may associate F to the m×n integer
matrix B of its generalized salient points deﬁned as follows: bij = h, if and only if
fij is a g.s.p. of Fh; bij = 0 otherwise. Informally, items 0 < h(≤k) of the integer
matrix B correspond to g.s.p in S(Fh); items 0 do not correspond to any g.s.p.
of F. For example, the rightmost matrix in Fig. 4 is the GS matrix associated
to the leftmost binary matrix (which corresponds to F in Fig. 3). We call B, the
GS matrix associated to F, where GS stands for “generalized salient”. The GS
matrix is well-deﬁned since by Deﬁnition 4, we have that ∩iS(Fi) = ∅.
Theorem 1. Any two binary images are equal if and only if their GS matrices
are equal.
Proof. The following construction permits to determine the binary image by
its GS matrix: For all item i in the GS matrix considered in decreasing order
(of i), compute the Q-convex hull of the pixels corresponding to the items i and
ﬁll the corresponding pixels not already considered with the foreground w.r.t. i.
It is easy to see that the construction is correct. Let k be the maximum
value in the GS matrix; then, Fk+1 = ∅, and Fk is Q-convex. Therefore, the
Q-convex hull of its g.s.p. is Fk, and so the ﬁrst step i = k of the construction
determines Fk = Q(S(Fk)). In the second step i = k −1, the construction
determines Fk−1: it computes the Q-convex hull of the pixels corresponding to
the items k −1 in the GS matrix, i.e. Q(S(Fk−1)) = Q(Fk−1), and ﬁlls the
corresponding pixels not already considered with the foreground w.r.t. k −1, i.e.
Q(Fk−1)\Fk. By deﬁnition, Fk = Q(Fk−1)\Fk−1, and since Q(Fk−1) = Fk−1∪Fk
and Fk−1 ∩Fk = ∅, we have Fk−1 = Q(Fk−1)\Fk. By proceeding in this way, in
the last step i = 1, the construction determines F = F1 since F1 = Q(F1)\F2.
Finally, since two diﬀerent binary images have diﬀerent GS matrices, there is a
one-to-one correspondence between images and matrices.
⊓⊔
In order to design an eﬃcient algorithm based on the constructive proof of the
theorem, we extend the deﬁnition of Q-convex hull as follows: The Q-convex hull
Q(Fi) of the lattice set Fi is the set of points M ∈G such that Zp(M) ∩Fi ̸= ∅
for all p = 0, . . . , 3.
Therefore, pixel M belongs to the Q-convex hull of Fi if there is an item i in
the GS matrix associated to Fi in each zone of M. Since the Q-convex hull of
the foreground pixels of Fi contains the Q-convex hull of the foreground pixels
of Fi+1, pixel M belongs to Q(Fi)\Q(Fi+1), if i is the minimum among the
maximum items in the GS matrix in each zone in M. This ensures that every
item is considered once. Let Zt = (zt
ij) such that zt
ij = h iﬀh is the maximum
item in the submatrix Zt(bij), for t = 0, 1, 2, 3.

A New Shape Descriptor Based on a Q-convexity Measure
273
1: procedure (B)
▷Construct the binary image F from its GS matrix B
2:
for each bij = 0 do
3:
ﬁnd the maximum item of B in Z0(bij) and store in z0
ij of matrix Z0
4:
end for
5:
for each bij = 0 do
6:
ﬁnd the maximum item of B in Z1(bij) and store in z1
ij of matrix Z1
7:
end for
8:
for each bij = 0 do
9:
ﬁnd the maximum item of B in Z2(bij) and store in z2
ij of matrix Z2
10:
end for
11:
for each bij = 0 do
12:
ﬁnd the maximum item of B in Z3(bij) and store in z3
ij of matrix Z3
13:
end for
14:
for each bij = 0 do
15:
h ←min(z0
ij, z1
ij, z2
ij, z3
ij)
16:
if h is odd then fij ←1
17:
elsefij ←0
18:
end if
19:
end for
20:
for each bij ̸= 0 do
21:
if bij is odd then fij ←1
22:
elsefij ←0
23:
end if
24:
end for
25: end procedure
Starting from the GS matrix B in input, Algorithm 1 constructs F by using
Z0, Z1, Z2 and Z3. For example:
B=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 1 2 2 1 0
0 0 3 3 0 0
1 0 0 0 0 1
0 0 0 0 3 2
2 3 0 4 0 1
1 0 0 0 0 0
0 0 0 4 0 0
0 0 0 3 0 0
0 6 1 2 1 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, Z0=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
2 0 0 0 2 4
2 3 0 0 4 4
0 2 3 4 4 0
2 3 3 4 0 0
0 0 3 0 4 0
0 1 1 4 4 4
0 0 1 0 4 0
0 0 1 0 3 0
0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, Z1=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
4 0 0 0 0 2
4 4 0 0 4 4
0 4 4 4 3 0
4 4 4 4 0 0
0 0 4 0 1 0
0 4 4 4 1 0
0 0 4 0 1 0
0 0 3 0 1 0
0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, Z2=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
2 0 0 0 0 0
3 3 0 0 1 0
0 3 3 3 1 0
3 3 3 3 0 0
0 0 4 0 3 0
0 4 4 4 3 2
0 0 4 0 3 0
0 0 4 0 3 0
0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, Z3=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 0 0 0 0 2
0 1 0 0 3 3
0 1 3 3 3 0
1 1 3 3 0 0
0 0 3 0 4 0
0 3 3 4 4 4
0 0 3 0 4 0
0 0 3 0 4 0
0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
If we consider for instance b22 = 0, since min{z0
22 = 3, z1
22 = 4, z2
22 = 3, z3
22 =
3} = 3, then f22 is in Q(F3) and so f22 = 1. Note that Algorithm 1 reconstructs
F of Fig. 4. The correctedness of the algorithm derives by previous discussion.
Theorem 2. Algorithm 1 computes the binary image from its GS matrix in
linear time.
Proof. Let B = (bij) be the GS matrix in input, and F = (fij) be the binary
matrix representation of the image associated to B. Initially, fij = 0, for all i, j.

274
P. Bal´azs and S. Brunetti
The computation of the maximum in any zone for each item bij = 0 (statements
2–4) can be done in linear time in the size of the image. Indeed consider zone
Z0: for bij, by deﬁnition, Z0(bij) = Z0(bi−1j) ∪Z0(bij−1). Therefore the maxi-
mum in Z0(bij) can be computed by previous computations for Z0(bi−1j) and
Z0(bij−1), and stored in a matrix Z0. (Analogous, relations hold for Z1, Z2, Z3.)
For any item bij, the minimum among four corresponding values stored in the
four matrices Z0, Z1, Z2, Z3 (statement 15), and the determination of the parity
of the minimum cost O(1) (statements 16–18, 21–23). Hence, the complexity of
the algorithm is linear in the size of matrix B.
⊓⊔
4
Computation of Ψ
The GS matrix and the shape measure Ψ(c1,...,ck) can be computed in linear
time in the size of the binary image by the algorithm designed in [3] for the
determination of generalized salient pixels.
Here we brieﬂy describe the algorithm. The basic idea is that salient points
and generalized salient points of a binary image F can be determined by implicit
computation of the Q(F). Indeed, the authors in [3] proved that Q(F) is the
complement of the union of maximal background quadrants. At each step i, the
algorithm ﬁnds the foreground (generalized) salient pixels of Fi by computing the
maximal background quadrants of Fi. Pixels in the background quadrants are
discarded and the remaining complemented image is considered in the next step
being the Q-convex hull of Fi (recall that Fi+1 = Q(Fi)\Fi). During the com-
putation of generalized salient points, the algorithm constructs the GS matrix
B = (bij). Indeed, bij = h, if fij is a g.s.p. in Fh and the algorithm ﬁnds it
at step h (and bij = 0 for any item which is not a g.s.p.). Therefore, B is the
matrix of the steps at which every g.s.p. is found. Figure 4 shows an example of
the execution of the algorithm and the corresponding GS matrix.
0 1 0 0 1 0
0 1 1 1 1 0
1 1 1 1 1 1
1 1 1 1 1 0
0 1 1 0 1 1
1 1 1 0 1 0
0 0 1 0 1 0
0 0 1 1 1 0
0 0 1 0 1 0
0 1 0 0 1 0
0 1 1 1 1 0
1 1 1 1 1 1
1 1 1 1 1 0
0 1 1 0 1 1
1 1 1 0 1 0
0 0 1 0 1 0
0 0 1 1 1 0
0 0 1 0 1 0
0 1 0 0 1 0
0 1 1 1 1 0
1 1 1 1 1 1
1 1 1 1 1 0
0 1 1 0 1 1
1 1 1 0 1 0
0 0 1 0 1 0
0 0 1 1 1 0
0 0 1 0 1 0
0 1 0 0 1 0
0 1 1 1 1 0
1 1 1 1 1 1
1 1 1 1 1 0
0 1 1 0 1 1
1 1 1 0 1 0
0 0 1 0 1 0
0 0 1 1 1 0
0 0 1 0 1 0
0 1 2 2 1 0
0 0 3 3 0 0
1 0 0 0 0 1
0 0 0 0 3 2
2 3 0 4 0 1
1 0 0 0 0 0
0 0 0 4 0 0
0 0 0 3 0 0
0 0 1 2 1 0
k =1
k =2
k =3
k =4
GS
Fig. 4. Illustrative example of the algorithm for ﬁnding g.s.p. of image F (ﬁrst matrix).
In each step (ﬁrst four matrices, from left to right) the identiﬁed g.s.p. are drawn bold
and pixels of the background quadrants are grey. The positions inside the polygon
constitute the Q-convex hull of the g.s.p. Q(S(Fi))\S(Fi) and will be investigated in
the successive step. The rightmost matrix is the GS matrix of F.

A New Shape Descriptor Based on a Q-convexity Measure
275
5
Experiments
Depending on the choice of ci we obtain shape measures that score diﬀerently
pixels closer to the boundary and those internal. In [3] we considered the case
where ci = 1 for all i, thus weighting all the g.s.p. in the same way. Here we
investigate two pairs of opposite choices:
– ci = i, and ci = i2 for i = 1, . . . , k
– ci = 1/i, and ci = (1/i)2 for i = 1, . . . , k.
In the ﬁrst experiment, we used the set of synthetic polygons in [11] to study the
behavior of the measures in case of rotation, translation of intrusions/protrusions
and global skew. In Fig. 5 are illustrated the results. We observe that Ψ(ci=i) and
Ψ(ci=i2) assign values lower than those assigned by Ψ(ci=1), whereas Ψ(ci=1/i) and
Ψ(ci=1/i2) assign values greater than those assigned by Ψ(ci=1). In this experi-
ment, all the measures rank the images in the same order except Ψci=i2 (which
exchanges ﬁrst with second image). Let us notice that measures are invariant
under translation of intrusions and protrusions (see fourth and ﬁfth shapes, for
example), but are sensitive to rotations of angles diﬀerent from 90◦(see second
shape).
Ψ(ci=1)
0.0175 0.0322 0.0645 0.0645 0.0645 0.0895 0.0895 0.1071 0.8638
Ψ(ci=i)
0.0079 0.0099 0.0211 0.0211 0.0211 0.0250 0.0250 0.0394 0.7245
Ψ(ci=i2)
0.0033 0.0029 0.0057 0.0057 0.0057 0.0064 0.0064 0.0137 0.5119
Ψ(ci=1/i)
0.0368 0.0899 0.1576 0.1576 0.1576 0.2163 0.2163 0.2571 0.9361
Ψ(ci=1/i2) 0.0737 0.2152 0.3091 0.3091 0.3091 0.3980 0.3980 0.4954 0.9703
Fig. 5. Synthetic shapes ranked into ascending order by shape measures. Values are
rounded to four digits.
In the second experiment, we considered a variety of shapes, and we ranked
them by each measure. In Fig. 6 the ranking in ascending order and the values
for measure Ψ(ci=1) are illustrated, whereas in Fig. 7 we report on the results for
Ψ(ci=i), Ψ(ci=i2), Ψ(ci=1/i), and Ψ(ci=1/i2). Note that all the measures correctly
assign value 1 to the “L” and rectangular shapes. Moreover, by deﬁnition, Ψ(ci=i)
and Ψ(ci=i2) assign lower values to shapes with the majority of g.s.p in the interior
(thus, to images with many narrows but deep intrusions) than to shapes with the

276
P. Bal´azs and S. Brunetti
majority of g.s.p in the boundary, whereas Ψ(ci=1/i) and Ψ(ci=1/i2) behave on the
contrary. This is shown for example by the eagle and spiral images (fourth and
eighth in Fig. 6, respectively). Indeed the spiral shape has most of its g.s.p. in the
boundary so that it is in position four and two in the ranking for Ψ(ci=1/i) and
Ψ(ci=1/i2), respectively, whereas it is in position nine in the ranking for Ψ(ci=i)
and Ψ(ci=i2) in Fig. 7. The contrary happens for the eagle shape, where most of
its g.s.p. are in the interior. This is in accordance of our expectations and shows
that diﬀerent weightings can be appropriate for diﬀerent classiﬁcation tasks.
In both experiments we used the images with original size (in both dimensions
varying between 100 and 555 pixels), even if we illustrate them rescaled for better
presentation quality. We also investigated scale invariance. This time we omitted
the two fully convex images as their convexities are naturally scale-invariant.
Taking the vectorized versions of the remaining 12 original images we digitized
them on diﬀerent scales (32 × 32, 64 × 64, 128 × 128, 256 × 256, and 512 × 512).
Then, for each image we computed the convexity values and compared them to
the convexity of the original sized image. Formally, we measured the normalized
diﬀerence
ΔΨ = |Ψ(Fo) −Ψ(Fr)|
Ψ(Fo)
,
where Ψ(Fo) and Ψ(Fr) is the convexity of the original sized and the rescaled
image, respectively. Table 1 shows the average of the measured convexity diﬀer-
ences over the 12 pair of images. Of course, in lower resolutions the small details
of the shapes disappear, therefore the shown diﬀerence values are higher. As
expected, Ψ(ci=i) and Ψ(ci=i2) are fairly intolerant to rescaling which has a high
impact on the narrow and deep intrusions. On the other hand, the values for
Ψ(ci=1/i) and Ψ(ci=1/i2) in Table 1 are small (less than 0.1), from which we can
deduce a reasonable scale-invariance of these convexity measures.
Table 1. Average diﬀerence of the convexity value of the original and rescaled images.
Size
Δψ(ci=i) Δψ(ci=i2) Δψ(ci=1/i) Δψ(ci=1/i2)
32 × 32
8.9945
73.4749
0.0950
0.0601
64 × 64
2.2382
10.1241
0.0866
0.0674
128 × 128 0.7901
2.3950
0.0756
0.0459
256 × 256 0.1936
0.4378
0.0541
0.0227
512 × 512 0.1853
0.3452
0.0806
0.0312
0.0335 0.0353 0.0571 0.1171 0.1924 0.2040 0.2531 0.2766 0.3267 0.3366 0.5392 0.6516 1.0000 1.0000
Fig. 6. Shapes ranked into ascending order by Ψ(ci=1). Values are rounded to four
digits.

A New Shape Descriptor Based on a Q-convexity Measure
277
0.0010 0.0011 0.0060 0.0081 0.0157 0.0205 0.0546 0.0600 0.1046 0.1492 0.3452 0.3602 1.0000 1.0000
0.0000 0.0000 0.0003 0.0004 0.0007 0.0011 0.0048 0.0098 0.0304 0.0533 0.1146 0.2104 1.0000 1.0000
0.2927 0.3453 0.3529 0.5236 0.5746 0.5754 0.5821 0.6043 0.6819 0.7061 0.7118 0.8491 1.0000 1.0000
0.6390 0.7380 0.7543 0.7553 0.7705 0.8160 0.8303 0.8329 0.8818 0.8923 0.9113 0.9357 1.0000 1.0000
Fig. 7. Shapes ranked into ascending order by Ψ(ci=i), Ψ(ci=i2), Ψ(ci=1/i), and Ψ(ci=1/i2)
measures, from top to bottom, respectively. Values are rounded to four digits.
6
Further Work
In this paper we presented a ﬂexible extended version of the measure of Q-
convexity deﬁned in [1]. By introducing the matrix of generalized salient points
we can give diﬀerent weights for diﬀerent groups of g.s.p. when calculating the
degree of convexity.
Choosing the proper weightings depends, of course, always on the classiﬁca-
tion/recognition/image precessing problem and whether we are more interested
in diﬀerentiating the images based on their boundaries or on their interiors. To
ﬁnd the proper weights, either trial-and-fail or more sophisticated methods, such
as stochastic search or machine learning constructions can be used. This issue
can be investigated in more detail in a further paper as well as the performance
of the measures in real-life pattern recognition applications.
Acknowledgements. The authors thank P.L. Rosin for providing the dataset used
in [11]. The collaboration of the authors was supported by the COST Action MP1207
“EXTREMA: Enhanced X-ray Tomographic Reconstruction: Experiment, Modeling,
and Algorithms”. The research of P´eter Bal´azs was supported by the NKFIH OTKA
[grant number K112998].
References
1. Bal´azs, P., Brunetti, S.: A measure of Q-convexity. In: Normand, N., Gu´edon, J.,
Autrusseau, F. (eds.) DGCI 2016. LNCS, vol. 9647, pp. 219–230. Springer, Cham
(2016). doi:10.1007/978-3-319-32360-2 17

278
P. Bal´azs and S. Brunetti
2. Boxter, L.: Computing deviations from convexity in polygons. Pattern Recogn.
Lett. 14, 163–167 (1993)
3. Brunetti, S., Bal´azs, P.: A measure of Q-convexity for shape analysis (submitted)
4. Brunetti, S., Daurat, A.: An algorithm reconstructing convex lattice sets. Theoret.
Comput. Sci. 304(1–3), 35–57 (2003)
5. Brunetti, S., Daurat, A.: Reconstruction of convex lattice sets from tomographic
projections in quartic time. Theoret. Comput. Sci. 406(1–2), 55–62 (2008)
6. Daurat, A.: Salient points of Q-convex sets. Int. J. Pattern Recognit. Artif. Intell.
15, 1023–1030 (2001)
7. Flusser, J., Suk, T.: Pattern recognition by aﬃne moment invariants. Patt. Rec.
26, 167–174 (1993)
8. Gonzalez, R.C., Woods, R.E.: Digital Image Processing, 3rd edn. Prentice Hall,
Upper Saddle River (2008)
9. Herman, G.T., Kuba, A. (eds.): Advances in Discrete Tomography and Its Appli-
cations. Birkh¨auser Basel, Boston (2007). doi:10.1007/978-0-8176-4543-4
10. Proﬃtt, D.: The measurement of circularity and ellipticity on a digital grid. Patt.
Rec. 15, 383–387 (1982)
11. Rosin, P.L., Zunic, J.: Probabilistic convexity measure. IET Image Process. 1(2),
182–188 (2007)
12. Sonka, M., Hlavac, V., Boyle, R.: Image Processing, Analysis, and Machine Vision,
3rd edn. Thomson Learning, Toronto (2008)
13. Stern, H.: Polygonal entropy: a convexity measure. Pattern Recogn. Lett. 10, 229–
235 (1998)
14. Zunic, J., Rosin, P.L.: A new convexity measure for polygons. IEEE T. Pattern
Anal. 26(7), 923–934 (2004)
15. Zunic, J., Rosin, P.L., Kopanja, L.: On the orientability of shapes. IEEE Trans.
Image Process. 15(11), 3478–3487 (2006)

Recognition of Digital Polyhedra with a Fixed
Number of Faces Is Decidable in Dimension 3
Yan G´erard(B)
LIMOS - UMR 6158 CNRS/Universit´e Clermont Auvergne,
Clermont-ferrand, France
yan.gerard@uca.fr
Abstract. We consider a conjecture on lattice polytopes Q ⊂Rd (the
vertices are integer points) or equivalently on ﬁnite subsets S ⊂Zd, Q
and S being related by Q ∩Zd = S or Q = conv(S): given the vertices of
Q or the list of points of S and an integer n, the problem to determine
whether there exists a (rational) polyhedron P ⊂Rd with at most n
faces and verifying P ∩Zd = S is decidable.
In terms of computational geometry, it’s a problem of polyhedral sep-
arability of S and Zd\S but the inﬁnite number of points of Zd \S makes
it intractable for classical algorithms. This problem of digital geometry
is however very natural since it is a kind of converse of Integer Linear
Programming.
The conjecture is proved in dimension d = 2 and in arbitrary dimen-
sion for non hollow lattice polytopes Q [6]. The purpose of the paper is
to extend the result to hollow polytopes in dimension d = 3. An impor-
tant part of the work is already done in [5] but it remains three special
cases for which the set of outliers can not be reduced to a ﬁnite set: pla-
nar sets, pyramids and marquees. Each case is solved with a particular
method which proves the conjecture in dimension d = 3.
Keywords: Pattern recognition · Geometry of numbers · Polyhedral
separation · Digital polyhedron · Hollow lattice polytopes
1
Introduction
The recognition of digital primitives is a classical task of pattern recognition
and digital geometry. It is usually question of recognizing digital primitives such
as digital straight segments, conics or more generally some families of shapes in
several dimensions. These problems can be stated in the following terms:
Problem 1 (Recognition(d, F, S)). Input: Let F be a family of subsets F of Rd
and S be a subset of Zd.
Output: Does there exists a set F of F verifying F ∩Zd = S?
We focus in this paper on the problem [Recognition(d, F, S)] where the fam-
ily F is a set of polyhedra with a prescribed number of faces. By denoting
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 279–290, 2017.
DOI: 10.1007/978-3-319-66272-5 23

280
Y. G´erard
Fig. 1. An instance of [Recognition(d, Pn, S)] is the given of dimension d (here d = 2),
a number of faces n (here n = 4) and a ﬁnite set of integer points. The problem is to ﬁnd
a polyhedron P ⊂Pn with the prescribed number of faces containing S but no other
integer point: P ∩Zd = S. The problem can be restated as a problem of polyhedral
separability [PolyhedralSeparability(d, n, S, Zd \ S)] of S and its complementary Zd \ S
by a polyhedron of Pn.
Pn the set of polyhedra of Rd deﬁned as intersections of at most n linear
half-spaces (by convention, a polyhedron of Pn is in Pn+1), we investigate
[Recognition(d, Pn, S)] (Fig. 1).
This problem [Recognition(d, Pn, S)] is mentioned in the review of open
questions in digital geometry [1]. Until 2015, it has been only investigated in
dimension 2 with speciﬁc polyhedra such as squares and rectangles [4,7,10]. The
diﬃculty of [Recognition(d, Pn, S)] is that even its decidability is not straight-
forward. By deﬁnition, [Recognition(d, Pn, S)] enters in the class of problems of
polyhedral separability investigated in Computational Geometry:
Problem 2 (PolyhedralSeparability(d, n, S, T)).
Input: A dimension d, an integer n, a set S ⊂Rd of inliers and a set T ⊂Rd of
outliers.
Output: Find a polyhedron P ⊂Pn such that all the points of S and none
point of T are in P: S ⊂P ⊂Rd \ T.
The problem [Recognition(d, Pn, S)] can be stated as [PolyhedralSeparability
(d, n, S, Zd \ S)] where the set of outliers T is the complementary of S in Zd. For
ﬁnite sets S and T, polyhedral separability can be solved in linear time if n = 1 [8]
or in O((|S| + |T|) log(|S| + |T|)) if d = 2 for any n [3]. It becomes NP-complete
in arbitrary dimension even with n = 2 [9]. With any ﬁnite sets S and T, the
decidability of [PolyhedralSeparability(d, n, S, T)] is completely straightforward
but it is no more the case with the inﬁnite set of outliers T = Zd \ S considered
for [Recognition(d, Pn, S)]. The problem is intractable for classical algorithms of
polyhedral separability.
This problem is however interesting because it is a kind of converse prob-
lem of Integer Linear Programming [11]. In ILP, the input is a set of n lin-
ear inequalities and the purpose is to provide the integer points which satisfy
them. In [Recognition(d, Pn, S)], we have the set of integer points and we want
to recover a prescribed number of inequalities which characterize it. Although
the geometry of numbers provides a powerful framework to work on lattice

Recognition of Digital Polyhedra with a Fixed Number
281
polytopes, the classical algorithms of this ﬁeld do not allow to solve directly
[Recognition(d, Pn, S)].
We can however imagine a direct strategy by reducing the inﬁnite set of
outliers T = Zd \ S to the subset of its minimal elements according to a partial
order relation “is in the shadow of” [5,6]. The minimal elements of Zd \ S are
called the lattice jewels of S while the non minimal elements do not need to
be taken into account. This approach allows to prove the decidability of the
problem if the number of lattice jewels is ﬁnite. It holds in dimension d =
2 or in arbitrary dimension, if the polytope Q = conv(S) is not hollow (its
interior contains at least an integer point) [6]. In dimension d = 3, the hollow 3-
polytopes with a ﬁnite number of lattice jewels are characterized in [5]. For them,
[Recognition(3, Pn, S)] is also decidable. In this dimension, it just remains the
cases of the hollow 3-polytopes with an inﬁnite number of lattice jewels (Fig. 2),
namely if
– S is coplanar,
– or S is a pyramid of lattice height 1, namely the lattice width of S is 1 and
one of the two consecutive planar sections of S is reduced to a point,
– or S is a marquee: the lattice width of S is 1 and one of the two consecutive
planar sections of S in the thin direction is reduced to a segment.
Fig. 2. The three types of 3-polytopes S with inﬁnitely many lattice jewels (thus the
decidability of [Recognition(3, Pn, S)] in dimensions d = 3 is an open question). On
the left, S is planar. In the middle, S is a pyramid of lattice height 1. On the right,
the lattice width of S is again 1 but one of its consecutive planar sections in the thin
direction is reduced to a segment. We refer to this case as a (circus) marquee.
In both cases, we provide an algorithm to decide [Recognition(3, Pn, S)] in
a ﬁnite time. It solves the last cases allowing to prove the following theorem:
Theorem 1. The problem [Recognition(3, Pn, S)] is decidable.
Section 2 is devoted to the easiest cases of planar sets and pyramids while
marquees are investigated in Sect. 3.
2
Decidability for Planar Sets and Pyramids
For planar set S of Z3, the strategy to solve [Recognition(3, Pn, S)] is to con-
sider the problem [Recognition(2, Pn, S)] in the sublattice of Z3 containing S.

282
Y. G´erard
If [Recognition(2, Pn, S)] admits no solution, neither [Recognition(3, Pn, S)].
If [Recognition(2, Pn, S)] has a solution P, then we have to expand P in a 3-
polytope without adding any integer point and by preserving the number of
faces.
2.1
Polytope’s Expansion in Higher Dimensions
The following lemma is not easy to ﬁnd in the state of the art may be while its
dual is trivial.
Lemma 1. Any polytope P of Rd with n ≥d + 2 faces is the planar section of
a polytope of Rd+1 with the same number n of faces (Fig. 3).
Fig. 3. According to Theorem 1, any polytope of Rd with at least d + 2 faces is the
planar section of a polytope of Rd+1with the same number of faces.
Proof. Although its dual formulation is not far to be trivial, this sketch of proof
(Fig. 4) is necessary to follow the proof of Lemma 2.
We assume that P is of full dimension in Rd. Otherwise, we proceed by induc-
tion. Since P has n faces, the polar polytope P ∗of P is the convex hull of a ﬁnite
set A ⊂Rd of n points (we consider as origin O or pole an interior point of P) [12].
We can elevate the points of A in a set A′ of full dimension in Rd+1. The intersec-
tion of the convex hull of A′ with the vertical line passing through the origin is
not reduced to a point (since the interior of the projection of the convex hull of
A contains the origin). It’s a segment. If it does not contain the origin, we can
translate A′ to obtain this property. It provides a polytope (P +)∗. Its polar P + is
a polytope with n faces and P as planar section (Fig. 4).
⊓⊔
There is n degrees of freedom which allow to build an expanded polyhedron
with complementary constraints. We express it in the following lemma which
holds in general dimension but is given here for d = 2.
Lemma 2. Given a polytope P ⊂R2 with n non parallel faces (n ≥4), embed-
ded in the plane z = 0 and a point X with zX ̸= 0, there exists a pyramid P X
with n faces, X as apex, a basis as close as we want from P and P as planar
section of height z = 0.

Recognition of Digital Polyhedra with a Fixed Number
283
Fig. 4. In (a), we consider a d-polytope with n faces and n ≥d+2. In (b), we introduce
its polar polytope P ∗for instance with respect to the barycenter of P. In (c), we
introduce the elevation lines where we can move the vertices of P ∗. In (d), we provide
a vertical expansion of P ∗but the origin is not in the d+1-polytope. In (e), we translate
it vertically so that the origin enters in its interior. In (f), we obtain a d + 1-polytope
(P +)∗whose polar polytope has n faces and P as planar section.
Proof. The construction follows from the proof of Lemma 1. The property that
P has no parallel faces means that there are no collinear vertices in its polar
polytope. We denote A the set of the vertices of P ∗. According to Carath´eodory
Theorem, we can remove a vertex of A (it remains at least 3 vertices) and provide
a reduced set of vertice A′ with the origin in its convex hull. The condition of
non collinearity of the points of A guarantees that the origin is in the interior
of the convex hull of A′. Then we elevate the points of A′ at level +1 and send
the point of A \ A′ with a negative height h. This elevation puts the points in
the position of the vertices of a pyramid P ∗h ⊂R3. There is a limit value h0 for
which the origin enters in the pyramid P ∗h. Its polar polyhedron with h < h0 is
a pyramid P h with the prescribed horizontal section. We can change the apex
of P h with linear transformations preserving the horizontal plane and send it to
X. And as h is tending to −∞, the lower face of the solution P h -its basis- is
becoming closer and closer and as close as we want from P.
⊓⊔
2.2
Proof of Decidability for Planar Sets
We prove the decidability of [Recognition(3, Pn, S)] for planar subsets of Z3:
Lemma 3. If S is a ﬁnite planar set of Z3 and n ≥4, [Recognition(3, Pn, S)]
is decidable.
Notice that polyhedra with only n = 2 faces and a ﬁnite intersection with the
lattice Z3 exist but they are a bit pathological (they are planes with irrational
normal directions and an intersection with the lattice reduced to a point).
Proof. If S is reduced to a point or to a segment, the answer is positive for n ≥4.
Then we assume that the dimension of S is 2. We consider the sublattice of a

284
Y. G´erard
support plane HS of S. According to [5], [Recognition(d, Pn, S)] is decidable in
dimension d = 2. It allows to determine whether there exists a two dimensional
polyhedron P ⊂HS with n faces separating S from the other integer points. If
there is no polyhedron of R2 solution in the sublattice, there exists no solution
in R3. If there is a polyhedron solution P, as S is ﬁnite, P is a polytope (the only
unbounded polyhedra of R2 having a ﬁnite intersection with Z2 have necessarily
a recession cone reduced to an irrational direction. It’s only possible if S is a
singleton). Then Lemma 1 allows to expand it in dimension 3. It provides a
solution of [Recognition(3, Pn, S)].
⊓⊔
2.3
Proof of Decidability for Pyramids
We prove the decidability of [Recognition(3, Pn, S)] for pyramids of Z3 of lattice
height 1:
Lemma 4. If S is an pyramid of Z3 of lattice height 1, [Recognition(3, Pn, S)]
is decidable (we assume n ≥4).
Proof. Up to an unimodular aﬃne isomorphism preserving Z3, we can assume
that S is the union of a basis B in the horizontal plane z = 0 and the point
(0, 0, 1). We decompose the pyramid in its basis that we denote B and its ver-
tex y. The basis B is a planar set embedded in a plane HB. As for Lemma 3,
we consider the problem [Recognition(2, Pn, B)] in the sublattice of the plane
z = 0. If [Recognition(2, Pn, B)] is not feasible, neither [Recognition(3, Pn, S)].
Conversely if [Recognition(2, Pn, B)] has a solution P, we have to consider the
parallelism of the faces of P in order to provide a three dimensional solution.
If P has no parallel faces, then Lemma 2 allows to expand with a ﬁnite num-
ber of computations P in a pyramid having (0, 0, 1) as apex and a basis as
close as we want from the plane z = 0. It allows to provide solutions whose
only integer points are (0, 0, 1) and the ones of B. In other words, the expan-
sion does not introduce any new integer point in the polytope. It proves that
[Recognition(3, Pn, S)] is feasible.
If P has parallel faces, we can perturb it to avoid the diﬃculty that it occurs.
By deﬁnition, a solution P of [Recognition(2, Pn, B)] contains all the points of
B and none point of Z2 \ B. As it is compact, its minimal distance to Z2 \ B is
strictly positive. It means that there exists an ϵ > 0 for which P and (1 + ϵ)P
are both solutions of [Recognition(2, Pn, B)]. The space between P and (1+ϵ)P
allows to move vertices and break the parallelism of the faces (Fig. 5).
Withaﬁnitenumberofcomputations,wehavereduced[Recognition(3, Pn, S)]
to [Recognition(2, Pn, B)] which is a decidable problem [6].
⊓⊔
3
Decidability for Marquees
The approach to prove the decidability of [Recognition(3, Pn, S)] for marquees
can not be done just by considering its basis. Up to an aﬃne isomorphism of Zd
sending S in a reference position, we can assume that the set S is covered by the

Recognition of Digital Polyhedra with a Fixed Number
285
Fig. 5. In (a), we consider a pyramid as instance of [Recognition(3, P4, S)] of basis
B. In (b), we provide a two-dimensional solution P of [Recognition(2, P4, B)] but its
faces are parallel, which does not allow to use Lemma 2. In (c), we can pertub P in
P ′ in order to break the faces parallelism by remaining between P and (1 + ϵ)P. Then
Lemma 2 allows to expand P ′. The only new integer point in the expanded polyhedron
is the apex of the pyramid.
two consecutive planes z = 0 and z = 1 and that the section S1 of S is a segment
containing at least two integer points on the line passing through (0, 0, 1) in the
direction x. The section of S in the lower plane z = 0 is denoted S0. We are
interested in the width widthy(S0) of S0 in the y direction. We decompose the
proof of the decidability of [Recognition(3, Pn, S)] for marquees according to
the value of the width widthy(S0):
– if widthy(S0) = 0, the marquee S is a planar set (previously solved).
– if widthy(S0) = 1, the basis of the marquee is reduced to two consecutive
segments in the x direction. It is a particular case to which we refer as a
prism (Fig. 6).
– if widthy(S0) ≥2, we have a general case which requires some speciﬁc work.
3.1
Decidability for Prisms
The problem for prisms is particular because there are three lines of lattice jewels
(the three lines in the x direction passing through the points (0, −1, 1), (0, 1, −1)
and (0, 1, 1)) but it is easy to solve. They are intersections of a tetrahedron with
the lattice:
Lemma 5. If S ⊂Z3 is a prism namely a ﬁnite set unimodularly equivalent to
the union of three segments in the x direction passing through the three points
(0, 0, 0), (0, 1, 0) and (0, 0, 1), then [Recognition(3, Pn, S)] is feasible for any
n ≥4.

286
Y. G´erard
Fig. 6. In (a), we consider a prism S contained by three lines in the x direction and
passing through the points (0, 0, 0) (red), (0, 1, 0) (blue) and (0, 1, 0) (green). In (b) we
introduce a vertex Q (red) on a line in the x direction passing through the interior of the
prism. We build a tetrahedron TQ containing S. In (c), we notice that by construction,
the spike (the right part of the tetrahedron in red) can not contain any integer point.
By choosing Q far enough, we can guarantee that the tetrahedron does not contain
other integer points than the ones of the prism. (Color ﬁgure online)
Proof. Notice that the notion of prism has here a very precise meaning. We
introduce a vertex Q on a line in the x direction crossing the interior of the
prism. Then we consider the tetrahedron containing S, with Q as apex and
the plane of the opposite face of Q in the prism as basis (Fig. 6). We choose a
ﬁrst position of Q0. The corresponding tetrahedron TQ0 might contain a ﬁnite
number of unwanted integer points (the important point is here that it is ﬁnite).
Then we push Q far enough to remove these integer points from the tetrahedron
TQ. The key-point is that by driving away the point Q in the x direction, the
spike is increasing but by construction, it does not contain any integer point.
The rear part of the tetrahedron is decreasing. It means that by choosing Q far
enough, we can exclude all the unwanted integer points from TQ.
3.2
Strategy for General Marquees
It remains to establish the decidability of [Recognition(3, Pn, S)] for the general
marquees.
Lemma 6. If S ⊂Z3 is a marquee and not a prism, then [Recognition(3, Pn, S)]
is decidable.
Before sketching a proof of Lemma 6, let us consider the particular case where
the basis S0 is of dimension 1. The marquee is made of two segments whose
convex hull is a tetrahedron. In this case, the convex hull of S is a solution
of [Recognition(3, Pn, S)] for any n ≥4. In the remaining case, a ﬁrst result
provides a localization of an inﬁnite set of the lattice jewels of S (Fig. 7).
Lemma 7. For a ﬁnite marquee S ⊂Z3 which is not a prism, with a non degener-
ated basis and placed in the reference position, we denote T −= {(k, −1, 1)|k ∈Z}
and T + = {(k, 1, 1)|k ∈Z} (Fig. 7). We have two properties:

Recognition of Digital Polyhedra with a Fixed Number
287
Fig. 7. On the left a marquee which is not a prism and on the right, the sets of points
T −and T + are the two main sets of lattice jewels (with a ﬁnite number of other integer
points not colored here).
– For any k ∈K, the points (k, 1, 1) and (k, −1, 1) are lattice jewels of S.
– The set of the other jewels T 0 = jewels(S) \ (T −∪T +) is ﬁnite.
The proof of Lemma 7 is based on the same arguments of compacity than
the ones used in [5] but due to the lack of space, it is absent from the paper.
Let us prove now Lemma 6 in the case of a basis S0 of dimension 2:
Proof. The approach provided in [5,6] allows to reduce [Recognition(3, Pn, S)]
to [PolyhedralSeparability(3, n, S, jewels(S))]. With Lemma 7, we rewrite it [Poly
hedralSeparability(3, n, S, T 0 ∪T −∪T +)]. Then the strategy is to process dif-
ferently with the constraints coming from S and T 0 than for the ones excluding
the points of T −and T +.
As T 0 is ﬁnite, the problem [PolyhedralSeparability(3, n, S, T 0)] is decid-
able. The ﬁrst key point is to decompose [PolyhedralSeparability(3, n, S, T 0)]
in n instances [PolyhedralSeparability(3, n, S, Ti)] with an index i going from
1 to n where the sets Ti deﬁne a partition of T. We notice that any solu-
tion P of [PolyhedralSeparability(3, n, S, T 0)] is the intersection of n half-planes
Hi respectively solutions of some instances [PolyhedralSeparability(3, n, S, Ti)]
where the sets Ti deﬁne a partition of T. Conversely, since any solution can
be decomposed in this way, our strategy is to consider all the partitions of T 0
in sets Ti. Given such a partition, each one of the n half-space Hi has to be
chosen in a set of half-spaces Ki deﬁned by the linear constraints expressing
[PolyhedralSeparability(3, n, S, Ti)]. By denoting aix+biy+ciz ≤hi an equation
of the half-space Hi, the set Ki is a convex cone deﬁned by the linear inequalities
aiu + biv + ciw ≤hi where (u, v, w) is in S and aiu′ + biv′ + ciw′ > hi where
(u′, v′, w′) is in Ti. Choosing the n half-spaces Hi in Ki guarantees that their
intersection contains S and no point of T 0. The separation from S and T 0 being
already taken into account, it remains to add the constraints of exclusion of the
points of T −and T +.
The restriction of Hi to the lines z = 1 and y = δ with δ = ±1 (these
two lines contain respectively T −and T +) is given by the linear inequalities
Hi : aix + biδ + ci ≤hi. Our task is to determine coeﬃcients ai, bi, ci and hi in
each Ki so that no integer x ∈Z with δ = +1 or −1 satisﬁes the n conditions.
The sets Ki are polyhedral cones in the space of dimension 4 of coordinates
(ai, bi, ci, hi). They can be described by their three sections by the hyperplanes

288
Y. G´erard
ai = 1, ai = 0 or ai = −1. The section of Ki with the hyperplane ai = α is
denoted Kα
i with α = −1 or 0 or +1.
For ai = 1, the linear inequality becomes Hi : x ≤hi −biδ −ci with linear
constraints on the coeﬃcients hi, bi and ci. For ai = 0, we have Hi : 0 ≤
hi −biδ −ci and for ai = −1, x ≥biδ +ci −hi. Our problem is to decide if we can
choose the coeﬃcients ai equal to −1, 0 or 1 and the coeﬃcients hi, bi and ci so
that we can exclude all the points of T −and T + namely all the integers x with
δ ∈{−1, 1}. We can decompose again the problem in the following questions:
1. given Ki, does there exist an half-space in K0
i excluding all the points of T +?
of T −? of both?
2. given Kα
i
and K−α
j
with i ̸= j, does there exist a pair of half-spaces in
Kα
i ×K−α
j
excluding all the points of T +? of T −? of both? (α and −α because
their orientation in the direction x should not be the same)?
3. given Kα
i , K−α
j
and K−α
j′
with diﬀerent indices, does there exist a triplet of
half-spaces excluding T −and T +?
4. same questions with a pair excluding T + and a pair excluding T −, but it can
be reduced to the second question.
There is no reason to increase the size of the tuple considered in these ques-
tions, because if three intervals of the form ]∞, β] and [γ, +∞[ and [γ′, +∞[ are
excluding the integers, one of them is redundant. Then if we cannot exclude T +
(or T −) with two half-spaces, we cannot exclude them at all.
The case 1 is solved by comparing hi −biδ −ci to 0 with δ = +1 for T + and
δ = −1 for T −.
Let us focus now on the case 2. The equation of Hi can be rewriten x ≤
−biδ + di with di = hi −ci for Hi ∈K+1
i
and x ≥bjδ −dj for Hj ∈K−1
j
with
dj = hj −cj. By replacing the coordinates c and h by the coordinate d = h −c,
we proceed to a projection of the convex sets Kα
i . Its image by this projection
in the space of parameters (b, d) is denoted K′α
i . It is a two-dimensional convex
set described by a ﬁnite number of inequalities which can be obtained from the
inequalities characterizing Kα
i by Fourier-Motzkin elimination.
We notice now that two constraints issued from K+1
i
and K−1
j
exclude T +
if and only if there is no integer x verifying bjδ −dj ≤x ≤−biδ + di for
δ = +1. We can determine the existence of such pair of points (bi, di) ∈K′+1
i
and (bj, dj) ∈K′−1
j
by computing the maximum max+
+1
i
and max+
−1
j
of b −d
for (bi, di) ∈K′+1
i
and (bj, dj) ∈K′−1
j
. It follows that T + can be excluded
by a pair of constraints coming from K+1
i
and K−1
j
if and only if the interval
[max+
+
j , −max+
+
i ] does not contain any integer. This last question can be solved
by Linear Programming (a similar approach holds for T −with −b −d instead
of b −d) (Fig. 8).
We end the proof with the case 3. In order to determine whether three
constraints coming from K′+1
i
, K′−1
j
and K′−1
j′
can exclude T −and T +, we
use Linear Programming in the same manner. We compute again the max-
imum max+
−1
j
of b −d for (bj, dj) ∈K′−1
j
and the maximum max−
−1
j′
of
−b −d for (bj, dj) ∈K′−1
j
. Then, we determine whether the set −K′+1
i
has

Recognition of Digital Polyhedra with a Fixed Number
289
Fig. 8. In the cases (2) and (2’), excluding all the outliers of T + with two half-spaces
in K+1
i
and K−1
j
is equivalent with ﬁnding a pair of points (bi, di) ∈−K′+1
i
and
(bj, dj) ∈K′−1
j
with no integer x verifying bj −dj ≤x ≤−bi + di. It’s not possible in
(2) because there is a diagonal line b−d = x ∈Z with K′−1
j
above and −K′+1
i
below. It
is possible in (2). This possibility is determined by the extreme points in the diagonal
direction namely the result of the comparison of ⌈max+
+
j ⌉and −max+
+
i . In the cases
(3) and (3’), we deal with the possibility to exclude all the outliers of T −and T + with
only three constraints coming from the sets K+1
i
, K−1
j
and K−1
j′ . We can compute a
quadrant Q determined by the extreme points of K′−1
j
and K′−1
j′
and determine its
intersection with K′+1
i
. The outliers of T −∪T + can all be excluded in this manner iﬀ
the intersection is non empty (as in (3) and not in (3’)).
a non empty intersection with the quadrant Q = {(bi, di) ∈R2| −bi + di <
⌈max+
−1
j ⌉and bi + di < ⌈max+
−1
j ⌉}. It can be done with a linear program.
⊓⊔
3.3
Perspectives
We have proved the decidability of the recognition of digital polyhedra
[Recognition(3, Pn, S)] in dimension 3. This result is weak and the three cases
considered in the paper can be considered as marginal. They are not because
they require to deal with an inﬁnite number of irreducible constraints but we
have proved that by using their geometry, it is possible to decide in a ﬁnite time.
The reader can however believe that this standalone approach is not appropri-
ate and that some known results coming from the lattice polytope’s theory allow
to prove stronger results with less work. The problem [Recognition(d, Pn, S)] is
a kind of converse of Integer Linear Programming but the idea that some kinds
of ILP approaches could avoid the diﬃculty requires more than an intuition.

290
Y. G´erard
One of the most interesting results related with the conjecture could be the exis-
tence of the ﬁniteness threshold width [2]: for larger width than the threshold
denoted w∞(d) (we have for instance w∞(3) = 1), there exists only a ﬁnite
number of lattice polytopes (up to lattice preserving aﬃne isomorphisms) con-
taining a prescribed number of integer points. Such a deep result could be used
to prove that for lattice polytopes Q verifying width(Q) > w∞(d), the num-
ber of lattice jewels is ﬁnite which makes the problem decidable. It remains the
mystery of what happens below the threshold namely exactly where inﬁnite-
ness occures. We can at last notice that as for the conjecture of decidability
of [Recognition(d, Pn, S)] which is unsolved in dimension d ≥4 for hollow
polytopes, the inﬁniteness threshold width is also related with hollow poly-
topes... It can explain that for this speciﬁc class of objects, the decidability
of [Recognition(d, Pn, S)] which seems to be a so weak question remains chal-
lenging.
References
1. Asano, T., Brimkov, V.E., Barneva, R.P.: Some theoretical challenges in digital
geometry: a perspective. Discrete Appl. Math. 157(16), 3362–3371 (2009)
2. Blanco, M., Haase, C., Hofmann, J., Santos, F.: The ﬁniteness threshold width of
lattice polytopes (2016)
3. Edelsbrunner, H., Preparata, F.: Minimum polygonal separation. Inform. Comput.
77(3), 218–232 (1988)
4. Forchhammer, S., Kim, C.: Digital squares, vol. 2, pp. 672–674. IEEE (1988)
5. Gerard, Y.: About the decidability of polyhedral separability in the lattice zd. J.
Math. Imaging Vis. (2017)
6. G´erard, Y.: Recognition of digital polyhedra with a ﬁxed number of faces. In:
Normand, N., Gu´edon, J., Autrusseau, F. (eds.) DGCI 2016. LNCS, vol. 9647, pp.
415–426. Springer, Cham (2016). doi:10.1007/978-3-319-32360-2 32
7. Krishnaswamy, R., Kim, C.E.: Digital parallelism, perpendicularity, and rectangles.
IEEE Trans. Pattern Anal. Mach. Intell. 9(2), 316–321 (1987)
8. Megiddo, N.: Linear-time algorithms for linear programming in r3 and related
problems. SIAM J. Comput. 12(4), 759–776 (1983)
9. Megiddo, N.: On the complexity of polyhedral separability. Discrete Comput.
Geom. 3(4), 325–337 (1988)
10. Nakamura, A., Aizawa, K.: Digital squares. Comput. Vis. Graph. Image Process.
49(3), 357–368 (1990)
11. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1986)
12. Ziegler, G.: Lectures on Polytopes. Graduate Texts in Mathematics. Springer, New
York (1995)

Reconstructions of Noisy Digital Contours
with Maximal Primitives Based
on Multi-scale/Irregular Geometric
Representation and Generalized Linear
Programming
Antoine Vacavant1(B), Bertrand Kerautret2, Tristan Roussillon3,
and Fabien Feschet1
1 Universit´e Clermont Auvergne, CNRS, SIGMA Clermont, Institut Pascal,
63000 Clermont-Ferrand, France
{antoine.vacavant,fabien.feschet}@uca.fr
2 LORIA, UMR CNRS 7503, Universit´e de Lorraine,
54506 Vandœuvre-l`es-Nancy, France
bertrand.kerautret@univ-lorraine.fr
3 Univ Lyon, INSA-LYON, LIRIS UMR 5205, 69622 Villeurbanne, France
tristan.roussillon@liris.cnrs.fr
Abstract. The reconstruction of noisy digital shapes is a complex ques-
tion and a lot of contributions have been proposed to address this prob-
lem, including blurred segment decomposition or adaptive tangential
covering for instance. In this article, we propose a novel approach combin-
ing multi-scale and irregular isothetic representations of the input con-
tour, as an extension of a previous work [Vacavant et al., A Combined
Multi-Scale/Irregular Algorithm for the Vectorization of Noisy Digital
Contours, CVIU 2013]. Our new algorithm improves the representation
of the contour by 1-D intervals, and achieves afterwards the decomposi-
tion of the contour into maximal arcs or segments. Our experiments with
synthetic and real images show that our contribution can be employed
as a relevant option for noisy shape reconstruction.
Keywords: Digital
shape
analysis ·
Irregular
isothetic
grids ·
Multi-scale analysis · Decomposition into maximal arcs · Decomposition
into maximal segments
1
Introduction
The representation of digital contours is an important task in image analysis
applications, since binary shapes obtained by image processing algorithms (pre-
processing and segmentation) may be altered by noise. A lot of eﬀorts have been
made on these algorithms to produce smooth contours, by developing sophisti-
cated deblurring and denoising algorithms [14], or by integrating regularization
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 291–303, 2017.
DOI: 10.1007/978-3-319-66272-5 24

292
A. Vacavant et al.
terms in segmentation process for instance [26]. However, these approaches sig-
niﬁcantly raise the computational complexity of the complete image analysis
pipeline, and include other input-noise-dependent parameters to be tediously
set for any new speciﬁc applications.
Hence, another approach consists in obtaining a faithful geometrical rep-
resentation directly from any noisy digital contours. A lot of research works
have addressed this question by ﬁtting parametric curves (e.g. B-splines, ratio-
nal Gaussian curves) to the input points [4,8,10]. These approaches require a
parameter depending on input noise scale, in order to ﬁt the objective function
at best. In general, they do not use the fact that digital points belong to Z2, as
this is always the case in the image plane.
Fig. 1. Global pipeline of our approach. Input: a noisy contour. Output: maximal geo-
metric primitives. Stages are: 1- Extraction of a multi-scale representation (unsuper-
vised geometric noise detector); 2- Irregular isothetic representation (non overlapping
cells) in X and Y directions; 3- 1-D intervals representation; 4- Fusion of the two
directions to achieve a faithful geometric structure of the input contour.
In the digital geometry community, an important literature has been dedi-
cated to this problem since the 80’s, by representing contours with several kinds
of primitives (segments, arcs of circle). Thanks to theoretical concepts designed
in digital geometry, these approaches extend the scheme of vectorization, well-
known in document analysis [3], consisting in converting pixels to line segments.
In particular, some publications tackle the issue of ﬁtting both straight segments
and circular arcs to digital contours at the same time. The famous approach of
Rosin and West [18] relies on least square ﬁtting and is non parametric. Another
parametric technique has been designed by Hilaire and Tombre [9], based on the

Reconstructions of Noisy Digital Contours with Maximal Primitives
293
notions of fuzzy digital segments and fuzzy digital arcs, and Faure and Feschet
[5] by using α-thick decomposition and combinatorial optimization. All of them
are robust and accurate whilst the former two suﬀer from a high time complex-
ity and are restricted to one pixel wide digital curves. Since multi-primitives
decomposition can be viewed as a competition between primitives, the complex-
ity can be tackled with an eﬃcient and uniﬁed representation of the multiple
decompositions with all individuals primitives. Relying on the work of Feschet
and Tougne [6], each decomposition can be represented by a circular arc-graph in
linear-time. Decomposition into several primitives can be solved in O(qn) where
q is the minimum number of intersecting primitives in the graph [2]. Other recent
methods from state-of-the-art opt for diﬀerent strategies, such as the adaptation
of tangential cover [15] or the detection of dominant points [16].
In this article, we propose a novel unsupervised approach for reconstructing
noisy digital contours by combining multi-scale and irregular isothetic repre-
sentations as presented in Fig. 1. The complete pipeline of our approach works
as follows. From an input (supposedly noisy) closed contour obtained from any
image (ﬁrst column), we ﬁrst extract a multi-scale object, containing overlapping
boxes, with an unsupervised geometric noise detector (second column). We then
represent this structure by irregular isothetic objects that is with cells with-
out any overlapping (third column). This is done by following two directions
(X and Y axes) simultaneously. Then these two X and Y axis aligned boxes
are represented as lists of 1-D intervals between irregular cells (fourth column).
Then, we combine both intervals to achieve a faithful geometric structure of the
input contour using both X and Y oriented segments (ﬁfth column) in a uniﬁed
representation of the contour. At the end (sixth column) we compute maximal
primitives within this last representation using the same Generalized Linear Pro-
gramming approach for segments and arcs allowing us to produce decomposition
of the contour into maximal straight line segments or circular arcs.
The article is organized as follows. In Sect. 2, we present the ﬁrst step of
our approach, aiming at representing the input contour as a multi-scale set of
bounding boxes. These are then analyzed and converted into irregular isothetic
structures, exposed in Sect. 3. Then, we describe the way to obtain decomposi-
tions into maximal primitives (Sect. 4), and experimental results with real and
synthetic contours (Sect. 5) before concluding this article in Sect. 6.
2
Multi-scale Noise Detection
The noise level detection on a digital contour is an important problem, which
can inﬂuence the quality of geometric estimators or contour representation algo-
rithms. From the digital geometry domain, a method was proposed to automati-
cally detect the amount of noise present on a digital structure [11]. This detection
is based on the meaningful scale detection computed from asymptotic proper-
ties of the maximal segments. In particular, it is based on a theorem describing
the evolution of the lengths of the maximal segments computed on the border
of a shape on ﬁner and ﬁner grid sizes [13]. From such a multiscale analysis,

294
A. Vacavant et al.
Fig. 2. Conversion of two noisy digital contours into meaningful scales (ﬁrst row) and
illustration of the multiscale proﬁles for two points A and B (second row)
the proposed algorithm consists in constructing, for each contour points C, a
multiscale proﬁle (P(C)) deﬁned by the segment length of all segments covering
P for larger and larger grid sizes h (see graph of Fig. 2 second row). From each
proﬁle, the noise level is determined by the ﬁrst scale (nu) minus one for which
the slope of P is decreasing and follows the awaited theoretical bounds between
h−1
2 and h−1
3 if C is on a non null curvature area and near h−1 on ﬂat part. On
the examples of Fig. 2, the noise levels of points A are 0 since P(A) is always
decreasing and B for the circle (resp. ﬂower) has a noise level of 3 (resp. 3) since
P(B) is increasing until scale 4 (resp. 3). This uncertainty can be represented as
boxes and as exposed in Fig. 2 ﬁrst row, a high noise in the contour will lead to
a large box, and vice-versa. The algorithm can be tested on-line from any digital
contour given by a netizen [12].
3
Irregular Isothetic Cyclic Representation
In this section, we ﬁrst recall the I-grid (Irregular Isothetic grid) model [22]:
Deﬁnition 1 (2-D I-grid). Let R be a closed rectangular subset of R2. A 2-D
I-grid G is a tiling of R with closed rectangular cells whose edges are parallel to
the X and Y axes, and whose interiors have a pairwise empty intersection. The
position of each cell R is given by its center point (xR, yR) ∈R2 and its length
along X and Y axes by (lx
R, ly
R) ∈R∗
+
2.
This model permits to generalize many irregular image representations such as
quad-trees, kd-trees, run-length encodings, and the geometry of frames encoded
within video coding standards like MPEG, H.264, etc. For the rest of the article,
we consider the following deﬁnitions for I-grids.

Reconstructions of Noisy Digital Contours with Maximal Primitives
295
Deﬁnition 2 (ve-adjacency and e-adjacency). Let R1 and R2 be two cells.
R1 and R2 are ve-adjacent (vertex and edge adjacent) if:
or

|xR1 −xR2| =
lx
R1+lx
R2
2
and |yR1 −yR2| ≤
ly
R1+ly
R2
2
|yR1 −yR2| =
ly
R1+ly
R2
2
and |xR1 −xR2| ≤
lx
R1+lx
R2
2
R1 and R2 are e-adjacent (edge adjacent) if we consider an exclusive “or” and
strict inequalities in the above ve-adjacency deﬁnition. The letter k may be inter-
preted as e or ve in the following deﬁnitions.
A k-path from R to R′ is a sequence of cells (Ri)1≤i≤n with R = R1 and R′ = Rn
such that for any i, 2 ≤i < n, Ri is k-adjacent to Ri−1 and Ri+1.
Fig. 3. From the meaningful scales of the Circle sample, we reconstruct a set of k-arcs
converted to a k-curve thanks to the underlying graph (b). (c) presents a part of the
cells obtained with intervals (red) (Color ﬁgure online)
Deﬁnition 3 (k-curve). Let A = (Ri)1≤i≤n be a k-path from R1 to Rn. Then
A is a k-curve iﬀeach cell Ri has exactly two k-adjacent cells in A.
As shown in Fig. 2, the meaningful boxes (denoted afterwards by the set M)
overlap and thus cannot be viewed as an irregular isothetic object directly (Def-
inition 1). However each one contains a given number of pixels (at the initial
resolution) so that the set of boxes M covers a subset of the input image. This
subset P, which is an irregular isothetic object, is transformed into k-arcs, i.e.
open k-curves, and the respective adjacencies relations between arcs is repre-
sented by a Reeb graph structure [22], as illustrated in Fig. 3a [21,23]. In that
graph, each edge is associated to an irregular k-arc reconstructed. This process is
driven by considering a given order relation, along X or Y axis (in this ﬁgure, X
axis has been chosen). With the support of the Reeb graph, we are then able to
produce a cyclic representation of the contour by parsing k-arcs in a given order
(e.g. clock-wise from the top-most element). We also remove extra branches of

296
A. Vacavant et al.
the graph, i.e. edges corresponding to k-arcs not belonging to the cycle (asso-
ciated to red parts in Fig. 3a). In particular, graph edges comporting a node of
degree 1 are removed. At the end of this process, we obtain a single k-curve
(Fig. 3b), associated to a cyclic graph, and we consider the interface (Euclidean
segment shared) between two consecutive cells in the k-curve, i.e. 1-D intervals.
By combining the intervals computed from both X and Y axes, we have thus
two lists of segments representing the input contour, denoted by SX and SY ,
as shown in Fig. 4a. We set the internal and external points of these straight
segments (black and white points in Fig. 4b) by considering the barycenter of
the global shape as we did in [21].
Fig. 4. A part of the two sets SX (black) and SY (red) intervals from the Flower sample
(a), and converted into a single set of intervals SXY (b). Internal points are dotted in
white, external ones in black. We also illustrate some cases of our process, with speciﬁc
examples of segments s, sx and sy (see text) in (c) and (d). In the images, red intervals
are already in place in SXY and green ones are to be processed from SX, SY (Color
ﬁgure online)
We then build a single list of intervals SXY = {SXY [i]}1≤i≤n with X- and Y -
aligned elements by ﬁrst removing segments from SY intersecting one or several
ones in SX (see again Fig. 4a and b). The converse choice can done (i.e. removing
segments from SX overlapping some of SY ), nevertheless, our option leads to a
faithful representation of the input contour, as exposed later in experiments.
Then, we simultaneously parse the sets SX and SY to construct this list by
a linear and incremental approach, according to the size of these two lists. At
an iteration of this process, consider the last segment added in SXY , denoted
by s, and the next segments to be added from SX and SY , denoted by sx and
sy respectively. We add in SXY the closest interval from s. As an illustration, in
Fig. 4c, we add sy, and in Fig. 4d, sx.
During this process, adding segments of SX and SY in SXY is realized in in
O(|SX| + |SY |). We can observe that we build a valid list of segments in SXY ,

Reconstructions of Noisy Digital Contours with Maximal Primitives
297
since two successive segments in the ﬁnal list (not sharing the same end point)
respect this condition:
−−−−→
SXY [i].
−−−−−−−−−−−−−−→
SXY

(i + 1)mod n

≥0, ∀i = 1, . . . , n,
(1)
wherein each segment is considered as a vector with the orientation given by
internal and external endpoints. For instance, in Fig. 4c, −→s .−→
sy = 0 and are added
successively in SXY , in Fig. 4d, −→s .−→
sx = 0, and any two consecutive parallel
segments SXY [i] and SXY [i + 1] will respect a strict positive dot product in
Eq. 1. The validity of the list SXY also means that the list of internal points are
ordered in the clockwise order, and follow the curvilinear abscissa of the input
contour (and this is the same for the list of external points).
4
Recognition of Straight Segments and Circular Arcs
Even if the arrangement of straight segments in SXY is not completely random,
it lacks regularity. For instance, the X-coordinates of the endpoints do not nec-
essarily increase and for this reason, we cannot use the algorithm of O’Rourke
[17] for the recognition of straight segments.
In this work, you use a general algorithm for the recognition of both straight
segments and circular arcs, formulated as two instances of a Generalized Linear
Programming (GLP) problem [1].
Our notations and deﬁnitions follow [1]. A GLP problem is a family H of
constraints and an objective function ω from subfamilies of H to some totally
ordered set S. In addition, H and ω must be such that:
(C1) Monotonicity: ∀F ⊆G ⊆H, ω(F) ≤ω(G),
(C2) Locality: ∀F ⊆G ⊆H s.t. ω(F) = ω(G) and for each h ∈H: ω(F ∪h) >
ω(F) iﬀω(G ∪h) > ω(G).
Note that the set S must contain a special maximal element Ω so that G ⊆H
is unfeasible if ω(G) = Ω and feasible otherwise.
In our framework, the constraint set H is given by the endpoints of the set
of the n straight segments of SXY , with n ≥1. Each straight segment has two
endpoints: one with label “white”, the other with label “black”, as depicted
in Fig. 4b. Let us denote the set of white (resp. black) endpoints by P ◦:=
{p◦
i }i=1...n (resp. P • := {p•
i }i=1...n). Let P (resp. D) be the set of all possible
half-planes (resp. disks). For a given X ∈{P, D}, we want to ﬁnd a shape X ∈X
that contains one point set, e.g. P ◦, but not the other. In other words, we want
to ﬁnd X ∈X under the constraint set H := {h2i−1, h2i}i=1...n, where
∀i = 1, . . . , n, h2i−1 := p•
i ∈X, h2i := p◦
i /∈X.
(2)
The problem is unfeasible if it does not exist such a X, but feasible otherwise.
In the latter case, we search for X minimizing a given objective function.
For any X ∈{P, D}, there exists an objective function ωX so that (C1) and
(C2) are true, which means that the above problem reduces to a GLP problem.

298
A. Vacavant et al.
The objective function ωD is chosen to either return Ω if the problem is unfeasible
or the radius of a smallest separating disk for H otherwise. By deﬁnition, the
pair (H, ωD) satisﬁes the monotonicity condition (C1), which coarsely says that
the larger the constraint set is, the larger the smallest separating disk for this set
is. In addition, since n ≥1, the smallest separating disk, if it exists, is unique,
which implies locality (C2).
The objective function ωP returns Ω if the problem is unfeasible. Otherwise,
the convex hulls of the point set to enclose and the point set to not enclose are
well-deﬁned because n ≥1 and do not intersect. In this case, ωP returns the
inverse of the minimal distance between the two convex hulls. The inverse is
taken so that adding a non-redundant constraint makes the objective function
increase. Again, the pair (H, ωP) satisﬁes conditions (C1) and (C2). As a result,
depending on ωX, we have to solve two diﬀerent kinds of GLP problem.
There exists an easy-to-implement and randomized algorithm that solves
these two kinds of GLP problem in expected linear-time [20]. It comes from the
well-known randomized algorithm for the smallest enclosing circle problem [25].
It takes a pair (H, ωX) and returns a basis, i.e. a minimal subfamily B ∈H
such that ωX(B) = ωX(H). The combinatorial dimension d of the problem is the
maximum size of any basis for any feasible family. For instance, d = 3 for ωD
(resp. ωP) because at most three constraints uniquely deﬁne a disk (resp. the
width between two convex polygons).
The algorithm is incremental and recursive. It may be coarsely described as
follows. We iteratively add constraints. For each constraint, we check whether
the new constraint violates the current basis or not. If yes, then we try to update
the basis from the new constraint by recursively calling the same algorithm with
all the previous constraints.
It is useful to have an on-line algorithm in order to compute the whole set of
maximal segments [7] or arcs [19]. Since the original algorithm [20] is incremental,
adding the constraints in order straightforwardly leads to an on-line algorithm.
The drawback is that the random order can be used only in the recursive calls but
not during the constraint discovery, which results to an increase of the expected
time-complexity from linear to quadratic. However, we experimentally observe
short running times. The next section shows results of our pipeline, employing
this on-line algorithm for the reconstruction of maximal segments or arcs.
5
Experimental Results
We ﬁrst present in Fig. 5 the whole set of maximal segments and arcs for
the Flower image. Contrary to the previous work dedicated to pure vector-
ization [23], we do not calculate a unique polyline from a complex structure of
k-arcs. Thanks to the cyclic irregular representation of the input contour, we are
capable of reconstructing maximal primitives, bounded by 1-D intervals, whose
lengths only depend on local input noise. Without any parameter, we obtain
faithful representations of noisy shapes. Moreover, the results do not depend on
any starting point, as it could be the case for other methodologies employing

Reconstructions of Noisy Digital Contours with Maximal Primitives
299
Fig. 5. Top: Decomposition into maximal segments (a) and maximal arcs (b) in a part
of the Flower sample (green primitives), superimposed on input intervals (presented as
in Fig. 4). Center and bottom: The maximal primitives are shown over original digital
contour of Flower and Circle (Color ﬁgure online)

300
A. Vacavant et al.
greedy algorithms. Even in the case of an high amount of local noise, our algo-
rithm successfully reconstructs sets of primitives, as illustrated in Fig. 5 (bottom)
wherein the input digital contour is signiﬁcantly corrupted and contains large
discontinuities and holes (top-right of the shape). As in [21], we can also obtain
the circle passing through the contour, by choosing maximal arcs (Fig. 5b).
We ﬁnally present the meaningful representation, and sets of maximal
straight segments and circular arcs obtained with our algorithm, for two real
images, in Fig. 6 (one contour) and Fig. 7 (two contours). The Char image leads
to a noisy contour (Fig. 6b), which is accurately represented thanks to our algo-
rithm. Maximal primitives (Fig. 6e, f) represent the complete contour, while one
of our previous contributions (d) [23], an accurate vectorization by MLP (or
Minimum Length Polyline), misses a part of the object, and produces abrupt
angles in round parts. The Sign image (350 × 350 pixels) allows us to test the
scalability of our method. The contours we have extracted generate a high num-
ber of meaningful boxes (1,364 boxes for external part, 1,234 for internal part)
that we have processed without any extra eﬀort.
Fig. 6. Results of our algorithm with the real image Char of size 185 × 85 pixels

Reconstructions of Noisy Digital Contours with Maximal Primitives
301
Fig. 7. Results of our algorithm with the real image Sign of size 350 × 350 pixels
6
Conclusion and Future Works
In this article, we have proposed a novel approach combining multi-scale and
irregular isothetic representations for the geometrical reconstruction of digital
noisy contours. Our algorithm calculates a set of 1-D bounding intervals of the
input shape, which permits to apply an on-line and incremental recognition
algorithm. Our contribution has been successfully applied on synthetic and real
images, encouraging us to exploit it in concrete image analysis contexts, and to
investigate several lines of research.
Our ﬁrst concern will consists in adapting the tangential cover approach [6] to
our cyclic irregular representation. This will enable the calculation of a structure
containing successive primitives, instead of overlapping maximal segments or
arcs. Second, we would like to compare our contribution with other methods
selected from state-of-the-art, e.g. [15,16], and to test their robustness [24] with
challenging data-sets of binary shapes, such as KIMIA. As a longer term, we
plan to investigate the more general question of reconstructing digital shapes
with other geometrical primitives, like B-splines and other parametric curves,
with a similar framework we have presented herein.
References
1. Amenta, N.: Helly-type theorems and generalized linear programming. Discrete
Comput. Geom. 12(3), 241–261 (1994)
2. Atallah, M.J., et al.: An optimal algorithm for shortest paths on weighted interval
and circular-arc graphs. Appl. Algorithmica 14(5), 429–441 (1995)
3. H.S., B.: Structured Document Image Analysis. Springer, Heidelberg (1992).
doi:10.1007/978-3-642-77281-8
4. Bo, P., et al.: A graph-based method for ﬁtting planar b-spline curves with inter-
sections. J. Comput. Des. Eng. 3(1), 14–23 (2016)
5. Faure, A., Feschet, F.: Multi-primitive analysis of digital curves. In: Wiederhold, P.,
Barneva, R.P. (eds.) IWCIA 2009. LNCS, vol. 5852, pp. 30–42. Springer, Heidelberg
(2009). doi:10.1007/978-3-642-10210-3 3

302
A. Vacavant et al.
6. Feschet, F., Tougne, L.: On the min DSS problem of closed discrete curves. Discrete
Appl. Math. 151(1–3), 138–153 (2005)
7. Feschet, F., Tougne, L.: Optimal time computation of the tangent of a discrete
curve: application to the curvature. In: Bertrand, G., Couprie, M., Perroton, L.
(eds.) DGCI 1999. LNCS, vol. 1568, pp. 31–40. Springer, Heidelberg (1999). doi:10.
1007/3-540-49126-0 3
8. Goshtasby, A.A.: Fitting parametric curves to dense and noisy points. In: Interna-
tional Conference on Curves and Surfaces (1999)
9. Hilaire, X., Tombre, K.: Robust and accurate vectorization of line drawings. IEEE
Trans. Pattern Anal. Mach. Intell 28(6), 890–904 (2006)
10. Karasalo, M., et al.: Contour reconstruction using recursive smoothing splines -
algorithms and experimental validation. Robot. Auton. Syst. 57(6–7), 617–628
(2009)
11. Kerautret, B., Lachaud, J.O.: Meaningful scales detection along digital contours
for unsupervised local noise estimation. IEEE Trans. Pattern Anal. Mach. Intell.
34(12), 2379–2392 (2012)
12. Kerautret, B., Lachaud, J.O.: Meaningful scales detection: an unsupervised noise
detection algorithm for digital contours. Image Process. On Line 4, 98–115 (2014)
13. Lachaud, J.O., Non-Euclidiens, E., d’Image, A.: Mod`eles D´eformables Riemanniens
et Discrets, Topologie et G´eom´etrie Discr`ete. Habilitation `a Diriger des Recherches,
Universit´e Bordeaux 1 (2006). (en francais)
14. Lebrun, M., et al.: Secrets of image denoising cuisine. Acta Numer. 21, 475–576
(2012)
15. Ngo, P., Nasser, H., Debled-Rennesson, I., Kerautret, B.: Adaptive tangential cover
for noisy digital contours. In: Normand, N., Gu´edon, J., Autrusseau, F. (eds.)
DGCI 2016. LNCS, vol. 9647, pp. 439–451. Springer, Cham (2016). doi:10.1007/
978-3-319-32360-2 34
16. Nguyen, T.P., Debled-Rennesson, I.: Decomposition of a curve into arcs and line
segments based on dominant point detection. In: Heyden, A., Kahl, F. (eds.) SCIA
2011. LNCS, vol. 6688, pp. 794–805. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-21227-7 74
17. O’Rourke, J.: An on-line algorithm for ﬁtting straight lines between data ranges.
Commun. ACM 24(9), 574–578 (1981)
18. Rosin, P.L., West, G.A.W.: Nonparametric segmentation of curves into various
representations. IEEE Trans. Pattern Anal. Mach. Intell. 17(12), 1140–1153 (1995)
19. Roussillon, T., Lachaud, J.-O.: Accurate curvature estimation along digital con-
tours with maximal digital circular arcs. In: Aggarwal, J.K., Barneva, R.P.,
Brimkov, V.E., Koroutchev, K.N., Korutcheva, E.R. (eds.) IWCIA 2011. LNCS,
vol. 6636, pp. 43–55. Springer, Heidelberg (2011). doi:10.1007/978-3-642-21073-0 7
20. Sharir, M., Welzl, E.: A combinatorial bound for linear programming and related
problems. In: Finkel, A., Jantzen, M. (eds.) STACS 1992. LNCS, vol. 577, pp.
567–579. Springer, Heidelberg (1992). doi:10.1007/3-540-55210-3 213
21. Toutant, J.-L., Vacavant, A., Kerautret, B.: Arc recognition on irregular isothetic
grids and its application to reconstruction of noisy digital contours. In: Gonzalez-
Diaz, R., Jimenez, M.-J., Medrano, B. (eds.) DGCI 2013. LNCS, vol. 7749, pp.
265–276. Springer, Heidelberg (2013). doi:10.1007/978-3-642-37067-0 23
22. Vacavant, A., Coeurjolly, D., Tougne, L.: Topological and geometrical reconstruc-
tion of complex objects on irregular isothetic grids. In: Kuba, A., Ny´ul, L.G.,
Pal´agyi, K. (eds.) DGCI 2006. LNCS, vol. 4245, pp. E1–E1. Springer, Heidelberg
(2006). doi:10.1007/11907350 58

Reconstructions of Noisy Digital Contours with Maximal Primitives
303
23. Vacavant, A., et al.: A combined multi-scale/irregular algorithm for the vector-
ization of noisy digital contours. Comput. Vis. Image Underst. 117(4), 438–450
(2013)
24. Vacavant, A.: A novel deﬁnition of robustness for image processing algorithms. In:
Kerautret, B., Colom, M., Monasse, P. (eds.) RRPR 2016. LNCS, vol. 10214, pp.
75–87. Springer, Cham (2017). doi:10.1007/978-3-319-56414-2 6
25. Welzl, E.: Smallest enclosing disks (balls and ellipsoids). In: Maurer, H. (ed.)
New Results and New Trends in Computer Science. LNCS, vol. 555, pp. 359–370.
Springer, Heidelberg (1991). doi:10.1007/BFb0038202
26. Wirjadi, O.: Survey of 3D image segmentation methods. Berichte des Fraunhofer
ITWM, p. 23 (2007)

Discrete and Combinatorial Topology

Euclidean and Geodesic Distance Proﬁles
Ines Janusch(B), Nicole M. Artner, and Walter G. Kropatsch
Pattern Recognition and Image Processing Group,
Institute of Computer Graphics and Algorithms, TU Wien, Vienna, Austria
{ines,artner,krw}@prip.tuwien.ac.at
Abstract. This paper presents a boundary-based, topological shape
descriptor: the distance proﬁle. It is inspired by the LBP (= local binary
pattern) scale space – a topological shape descriptor computed by a ﬁl-
tration with concentric circles around a reference point. For rigid objects,
the distance proﬁle is computed by the Euclidean distance of each bound-
ary pixel to a reference point. A geodesic distance proﬁle is proposed for
articulated or deformable shapes: the distance is measured by a combina-
tion of the Euclidean distance of each boundary pixel to the nearest pixel
of the shape’s medial axis and the geodesic distance along the shape’s
medial axis to the reference point. In contrast to the LBP scale space, it
is invariant to deformations and articulations and the persistence of the
extrema in the proﬁles allows pruning of spurious branches (i.e. robust-
ness against noise on the boundary). The distance proﬁles are applicable
to any shape, but the geodesic distance proﬁle is especially well-suited
for articulated or deformable objects (e.g.applications in biology).
Keywords: Distance proﬁle · Shape description · Local topology ·
Persistence · Local binary patterns · LBP scale space · Medial axis
1
Introduction
Shape is a widely used feature to describe and distinguish objects for a vari-
ety of computer vision tasks such as image retrieval, object classiﬁcation and
recognition, segmentation or tracking. A shape descriptor should be invariant
to transformations, distortions and occlusions of the shape. Furthermore, it is
desirable for a shape descriptor to be independent of the application and to be
low in computational complexity (especially for online image retrieval).
Zhang et al. [21] divide shape descriptors into two classes: region-based and
boundary-based (contour-based) approaches. Each class can be further divided
into structural and global approaches. Structural approaches describe shapes by
segments while global approaches describe shapes as a whole.
The distance proﬁles presented in this paper are boundary-based descriptors
for 2D shapes without holes. They allow a global description of the shape, but
are also able to divide the shape into segments based on intervals of topologi-
cal persistence (see Sects. 2.2 and 3.2). Two distance proﬁles are presented: (I)
Euclidean distance proﬁle (DP) and (II) geodesic distance proﬁle (DP∗).
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 307–318, 2017.
DOI: 10.1007/978-3-319-66272-5 25

308
I. Janusch et al.
These proﬁles are based on the idea of the LBP scale space [11] (see Sect. 2),
which describes a shape based on a ﬁltration process from a chosen reference
point. This ﬁltration yields a translation- and rotation-invariant topological
description of the shape.
In comparison to the LBP scale space, the distance proﬁles (Euclidean and
geodesic) in this paper speed up the computation of the descriptor and are in
addition invariant against scaling, articulation and deformation (in case of DP∗).
Furthermore, they allow the pruning of spurious branches based on persistence.
1.1
State of the Art
The proposed distance proﬁles are mostly related to topological shape descrip-
tors, shape signatures and spectral descriptors.
Verri et al. [20] use topological persistence for shape description in the form
of size functions, which represent the persistence of the connected components.
Carlsson et al. [4] propose persistence barcodes for shape description and clas-
siﬁcation. Barcodes visualize the lifetime for which features persist. For shape
retrieval, shapes may be compared based on their persistence diagram using the
matching distance as presented by Cerri et al. [5]. Another topological shape
representation are Reeb graphs [1]. The simplest way to obtain a Reeb graph
of a 2D or 3D shape is to use a height function as Morse function. In the same
way persistence diagrams or barcodes can be determined using a ﬁltration (see
Sect. 2.2) based on a height function. Topological shape representations in gen-
eral depend on the ﬁltration. Height functions for example are not invariant to
rotations and therefore lack in representational power.
Distance proﬁles are also related to shape signatures, which are one dimensional
functions derived from the shape’s boundary. There are diﬀerent kinds of shape sig-
natures [7,19,21]: centroidal proﬁle, complex coordinates, centroid distance, tan-
gent angle, cumulative angle, curvature, area and chord-length. Shape signatures
are sensitive to noise on the boundary. Small changes in the boundary may cause
large errors when matching the shapes (e.g. image retrieval). Considering the per-
sistence of extrema, it is possible to ﬁlter out noise (i.e. spurious branches of the
medial axis) and make distance proﬁles invariant to noise on the boundary.
Spectral descriptors, such as Fourier descriptors (FD) [6,9,22] and wavelet
descriptors (WD) [18], are another kind of boundary-based shape descriptors,
which are usually derived from a spectral transform on a shape signature. They
overcome the problem of noisy boundaries by analyzing shapes in the spectral
domain. Spectral descriptors are derived from spectral transforms on one dimen-
sional shape signatures.
1.2
Overview of the Paper
Sect. 2 recalls the LBP scale space and its persistence. Section 3 presents the pro-
posed distance proﬁles and their local extrema. Furthermore, Sect. 3 explains how
persistence is deﬁned on the distance proﬁles. First experiments on the distance
proﬁles of discrete shapes are presented in Sect. 4. Conclusions are given in Sect. 5.

Euclidean and Geodesic Distance Proﬁles
309
2
Recall: LBP Scale Space
Originally, LBPs were introduced for texture classiﬁcation in 1996 [14]. A LBP
describes the local texture around a pixel p = (x, y) by a bit pattern BP. This bit
pattern, results from a comparison of the grayvalue of the center pixel g(p) and
the grayvalues g(qi) along a circle sub-sampled with P points qi, i = 1, . . . , P:
BP(qi) =

1
if g(p) −g(qi) ≥0
0,
otherwise
(1)
Two parameters determine the computation of a LBP: r deﬁnes the radius of
the circular neighborhood around p, and P = |BP| ﬁxes the number of sampling
points along the circle, i.e. the number of bits BP = (s1, s2, . . . , sP ) [15].
2.1
Describing Local Topology with LBPs
LBPs can be employed to describe the local topology of a binary image region
around a reference point p. Based on the bit pattern LBP types can be deﬁned
by counting the number of transitions from 0 to 1 and vice versa. A transition
equals an intersection bi of the boundary B with the LBP circle of radius r at
position p: B ∩c(p, r). Following topological types can be derived from LBPs:
(local) maximum: no transition, bit pattern contains only 0 s and g(p) = 1;
(local) minimum: no transition, bit pattern contains only 1 s and g(p) = 0;
plateau: no transition, bit pattern contains only 1 s and g(p) = 1;
slope: two transitions (compare uniform patterns [15]);
saddle point: four or more transitions [10].
Fig. 1. (a) Increasing radius r shifts intersections along the boundary B (blue arrows).
(b) LBP scale space in the continuous case – critical points marked. (Color ﬁgure
online)

310
I. Janusch et al.
2.2
LBP Based Persistence
The persistence of a feature (e.g. LBP type) – its lifetime, is measured by the
ﬁltration of a space, which is “a nested sequence of subspaces that begins with
the empty and ends with the complete space” [8, p. 5].
Filtration Based on LBPs: One possibility to perform a ﬁltration using LBPs
is by varying the radius r of the LBP computation for a ﬁxed reference point
p and a boundary B [11]. Varying r corresponds to a movement along B (blue
arrows in Fig. 1a). This ﬁltration process changes the intersection points of the
circle c with the boundary B, i.e. the bit patterns BP (number of transitions)
and thus inﬂuences the topological LBP types.
The persistence of an LBP type is measured by these 2n intersection points,
which divide the shape in n + 1 regions and the boundary in 2n segments (per-
sistence diagram: “birth” for each region/segment). By increasing r the intersec-
tion points move along B. Once two intersection points coincide the LBP type
changes (persistence diagram: “death” of the respective segment/region).
2.3
LBP Scale Space
The LBP scale space was proposed as shape descriptor in [11]. From a chosen
reference point p inside a shape, a ﬁltration based on LBPs is performed. The
ﬁltration may start with a radius r = 0, which is increased according to a
predeﬁned sampling scheme (for discrete case: r increased by 1 covers all integer
radii). For the shape descriptor, the number of transitions observed for each of
the LBP radii is stored, i.e. the changes in the local topology. Figure 1b illustrates
the LBP scale space for sampling at critical points.
Besides applications in classiﬁcation or recognition, the LBP scale space
enables the reconstruction of a shape when extended by polar coordinates [11].
3
Distance Proﬁles
The brute force computation (see Fig. 1) of the LBP scale space starts with
a small circle for which the intersections with the shape’s boundary are com-
puted. Then the radius of the LBP circle is increased and the computation of
the intersections is done again. This process is repeated until the shape is com-
pletely inside the LBP circle and no more intersections can be observed for a
connected shape. Such an implementation of the LBP scale space is computa-
tionally expensive. Hence, this paper proposes an alternative and more eﬃcient
way to compute the LBP scale space based on the deﬁnition of distance proﬁles.
First, the distances d of all points bi along the shape’s boundary B to the
reference point p are computed:
DP(bi, p) = ||bi −p||, bi, p ∈ℜ2
(2)

Euclidean and Geodesic Distance Proﬁles
311
Fig. 2. LBP scale space computation using the (a) Euclidean distance proﬁle DP and
(b) geodesic distance proﬁle DP ∗. (Color ﬁgure online)
We call DP : B →ℜ+ the Euclidean distance proﬁle for each bi ∈B (see Eq. (2)
and Fig. 2a). The LBP scale space [11] corresponds to this Euclidean distance
proﬁle DP. By changing the metric and using the medial axis of the shape S,
a geodesic distance proﬁle DP∗: B →ℜ+ can be deﬁned in two parts: (I) the
distance of a point bi ∈B to the closest point al of the shape’s medial axis
MA(S) plus (II) the geodesic distance along this axis from al to the reference
point p:
DP∗(bi, p) = ||bi −al|| + arclength(al, p), p ∈MA(S)
(3)
A visualization of the geodesic distance proﬁle DP ∗is given in Fig. 2b.
Note that a connected medial axis MA is essential for this approach. There-
fore, we assume that a connected medial axis can be derived for a given binary
shape and do not focus on the computation of the MA itself. Furthermore, the
input of the proposed method is currently limited to binary shapes without holes.
3.1
Local Extrema Along a Distance Proﬁle
Let DT : S ⊂ℜ2 →ℜ+ denote the Euclidean distance transform [16] of the
shape. It assigns to each point p ∈S inside the shape the radius of the largest
inscribed circle with radius DT(p), which touches the shape’s boundary B at a
boundary point bi : ||p −bi|| = DT(p). Let p be the LBP scale space center.
Lemma 1. If the reference point p ∈S is inside the shape S, then there is at
least one local minimum along the distance proﬁle.
Since p ∈S, DT(p) is the radius of a circle touching B. Increasing the (maximal)
circle at p would cross the boundary assigning larger distance values to neighbors
of bi. Hence, the touching point is a local minimum of DP.
For the geodesic distance proﬁle DP∗we consider the medial axis MA ⊂S:
every circle with center on the MA touches the shape’s boundary B at two or
more points.

312
I. Janusch et al.
Leyton’s curvature-symmetry duality relates each local curvature maximum
with an end point ae ∈MA [12]. Such a maximum in curvature also produces a
maximum in both distance proﬁles for reference points p along the MA-branch
of ae. This is generally the case if p is located inside the shape farther away from
the curvature maximum than the radius of the osculating circle.
The maximally inscribed circle at a branching point of MA touches the
boundary of the shape in at least as many points as there are branches. If the
branching point is taken as the reference point for the distance proﬁle, it shows
a local minimum at each of these touching points. Maximal circles touch the
shape’s boundary in two points, if their centers are located at MA of the shape,
but not at an end or at a branching point of the MA.
The extrema of the distance proﬁles may be used for shape description and
representation. At these extrema the topology changes: a connected component
either starts or ends at the distance associated with the local extremum for this
distance proﬁle. The LBP scale space [11] similarly describes a shape based on
a sequence of changes in the topology of the shape through a ﬁltration for a
certain LBP center. The Euclidean distance proﬁle DP and the LBP scale space
representations are identical, but the computation of the DP is more eﬃcient.
In contrast, the geodesic distance proﬁle DP ∗provides a similar representa-
tion based on a diﬀerent metric, which is more robust against articulations and
deformations.
The number of maxima in a distance proﬁle is determined by the number of
end points of MA (i.e. the number of positive local curvature maxima), whereas
the number of minima must be equal to the number of maxima along B since a
minimum has to be located between every pair of maxima. This is the smallest
number of extrema for a shape and it depends exclusively on the number of
MA-branches.
Consequently, spurious branches of MA generate extra extrema, which can
be removed by the concept of persistence (see Sect. 3.2). The Euclidean DP may
contain more extrema than the geodesic DP∗due to bent branches.
3.2
Persistence Deﬁned on the Distance Proﬁle
As in classical persistence [8], we consider the lifetime of connected components
generated by thresholding the distance proﬁle. The corresponding space is the
boundary B of the shape S and the sub-level sets of the proﬁle function give the
ﬁltration. This corresponds to the choice of a particular radius in the LBP scale
space. The transitions from 0 to 1 and vice versa in the LBP code for a certain
radius correspond to the transition between the diﬀerent connected components.
The extrema E = (b1, b2, . . . , b2M), bi ∈B derived from DP are alternating
(i.e. maximum – minimum – maximum – ...), where M is the number of maxima.
The persistence P of each of the extrema bj, j = 1 . . . 2M, is deﬁned by the
smallest diﬀerence to the adjacent two extrema:
P(bj) = min{|DP(bj−1, p) −DP(bj, p)|, |DP(bj+1, p) −DP(bj, p)|}.
(4)

Euclidean and Geodesic Distance Proﬁles
313
x
r
x1
x3
x2
r1
r3
r2
r
b1
b2
b3
Fig. 3. A spurious branch generates non-persistent extrema. MA in yellow. (Color
ﬁgure online)
If an extremum is close to an adjacent extremum, then this diﬀerence is small
and a small modiﬁcation of the threshold used for ﬁltration would suﬃce to
change the intersections between an LBP circle and the shape’s boundary.
Figure 3 shows part of an elongated true branch (in direction x) and a typical
spurious branch generated by a small bump (circle with small radius r′) along
the boundary of the actual branch. The bump itself (centered at position x2) is
a local maximum of the distance proﬁle (we denote by bi the boundary point
corresponding to the axis point xi)
DP ∗(b2) = x2 + r2 = (x1 + r′) + (r1 + 2r′) = (x1 + r1) + 3r′
(5)
= DP ∗(b1) + 3r′ > DP ∗(b1)
while the return to the main branch at x3 is a local minimum with distance
DP ∗(b3) = x3 + r3 = (x1 + 2r′) + r1 = (x1 + r1) + 2r′
(6)
= DP ∗(b1) + 2r′ < DP ∗(b2)
b2 is clearly a local maximum and b3 a local minimum since DP ∗(b(x)) >
DP ∗(b3) for x > x3 and the width of the branch is constant, but the geodesic
distance to the reference point increases.
The diﬀerence between the two extrema b2 and b3 (their persistence) is related
to the size of the bump, e.g. r′, but independent of the width of the main branch.
As in many pruning strategies for the MA, branches with a long axis are con-
sidered reliable while short branches often occur due to noise. Large diﬀerences
between local minima and maxima of the distance proﬁle also indicate long dis-
tances along the axis and thus long branches. A highly persistent reference point
(LBP center) should induce a small number of extrema of the distance proﬁle
and favor the center of the diameter of the MA as primary locus.

314
I. Janusch et al.
4
Experiments
The DP and the DP∗deﬁned in the continuous case in Sect. 3 have been imple-
mented for a practical evaluation in the discrete case. Morphological thinning
has been used as a pre-processing step to obtain a connected skeleton. The two
distance proﬁles together with persistence applied to DP ∗have been evaluated
in experiments on binary shapes of the Kimi99 [17] and the Myth [2,3] dataset.
For the DP the Euclidean distance from every pixel along a shape’s boundary
to a ﬁxed reference point on the skeleton of the shape is calculated. The DP∗is
computed as the geodesic distance along the skeleton from the reference point
to every skeleton pixel plus the medial axis radius (computed using a distance
transformation) at each skeleton position. Figure 4 shows a binary input shape,
the skeleton of the shape, as well as the DP and the DP∗.
Extrema of the distance proﬁles are determined as local minima and maxima
in the ordered sequence of distances. For the DP, this sequence is the sequence
of distances observed when tracing a shape’s boundary in either clockwise or
counter-clockwise direction. In the case of the DP∗, the ordered sequence of
distances is retrieved by tracing the skeleton clockwise or counter-clockwise.
Note that due to this diﬀerence in the computation the length of the sequences
of distances of the two distance proﬁles may vary.
Figures 5a and b show the extrema marked along DP and DP∗for the binary
hand shape shown in Fig. 4a. The DP shows a higher number of extrema, as it is
prone to noise due to small variations along the boundary. Additional extrema
in the DP can also be caused by the deformation of a shape through bending.
This deformation however does not aﬀect the extrema of DP∗. Note that no
ﬁltering based on persistence has been done for Fig. 5a and b.
These observations have further been evaluated on all shapes of the Kimia99
dataset. In total 4824 extrema were found along DP for the 99 shapes and
2162 extrema in total along DP ∗. The minimum diﬀerence in the number of
extrema per shape on the Kimia99 dataset between DP and DP ∗is 0, however
the maximum diﬀerence is 96. In average DP ∗produces 27.8 less extrema than
DP for every shape in the Kimia99 dataset.
This diﬀerence in number of extrema of the two distance proﬁles Δ = |DP −
DP ∗| has further been used to cluster the shapes in the Kimia99 dataset and
(a) hand shape
(b) skeleton
(c) DP
(d) DP ∗
Fig. 4. Binary input shape and obtained distances for DP and DP ∗, reference point =
white square, blue indicates small distances, red large distances. (Color ﬁgure online)

Euclidean and Geodesic Distance Proﬁles
315
(a) distances along the Euclidean DP
(b) distances along the geodesic DP ∗
(c) distances along the DP ∗
pers (threshold = 5)
Fig. 5. Distance proﬁles (a) DP and (b) DP ∗and (c) persistence (threshold = 5)
applied to DP ∗: DP ∗
pers for the hand shape. ◦indicate maxima, × indicate minima.
(Color ﬁgure online)
Table 1. Shapes in the Kimia99 dataset clustered according to the diﬀerence in number
of extrema in the distance proﬁles Δ = |DP −DP ∗|.
Δ =
[0, 10]
[11, 30]
[31, 50]
[51, 96]
# of images
25
32
33
10
representative image
recall of class
100%
82%
73%
75%
to identify common features among shapes within a cluster. The dataset was
partitioned into four cluster with Δ = [0, 10], Δ = [11, 30], Δ = [31, 50] or

316
I. Janusch et al.
Δ = [51, 96]. Table 1 shows the number of shapes in each of these intervals. Since
the Kimia99 dataset is meant for shape classiﬁcation and retrieval experiments,
the images of the dataset are mainly grouped into classes. The dataset consists
of 6 major classes of shapes with minimum 7 and maximum 11 images per
class. The remaining 40 images are either single shapes or shape classes with
only two or three images per class. Interestingly, the shapes within one of these
major classes of the Kimia99 dataset are with a high percentage (73% or more)
also in the same cluster regarding Δ. Table 1 shows a shape for each cluster,
representing the class of the Kimia99 dataset with the highest number of images
in the respective cluster. Furthermore, the recall, the percentage of images of
(a)
(b) DP ∗
pers
(c)
(d) DP ∗
pers
(e)
(f) DP ∗
pers
Fig. 6. Geodesic distance proﬁles under application of persistence (threshold = 8 for
the three shapes shown in the left column respectively. ◦indicate maxima, × indicate
minima. (Color ﬁgure online)

Euclidean and Geodesic Distance Proﬁles
317
each such class in the cluster to the total number of images in the class, is given
in the last row of Table 1 (recall of class).
The persistence deﬁned on the distance proﬁle DP ∗(see Sect. 3.2) has been
subject to further experiments. If persistence is applied to the distance proﬁle
DP ∗(DP ∗
pers) with a very small distance threshold of 5, it further reduces the
total number of extrema for all shapes in the Kimia99 dataset to 1174 DP ∗
pers
(compared to 2162 for DP ∗and 4824 for DP). In average the number of extrema
in DP ∗is already reduced by 54% for this small threshold in DP ∗
pers. Figure 5c
shows DP ∗
pers for the hand shape introduced in Fig. 4a.
The robustness of the geodesic distance proﬁle with persistence based pruning
DP ∗
pers to articulated deformations is demonstrated on three shapes of the Myth
dataset. The three shapes together with their respective distance proﬁles DP ∗
pers
are shown in Fig. 6. While legs and tail of the horse move from Fig. 6a to c and
the horse is angling its torso upwards from Fig. 6c to e, the respective distance
proﬁles given in the Figs. 6b, d and f show high similarity for all three shapes.
5
Conclusion and Future Work
This paper presented the Euclidean and geodesic distance proﬁles, which are con-
sistent with Leyton’s shape evolution as expressed in his process-grammar [13].
The distance proﬁles are shape descriptors based on the ideas of the LBP scale
space. Both proﬁles are invariant against translation, rotation and scaling. The
geodesic distance proﬁle is also invariant against articulations and deformations.
Local extrema along the distance proﬁles can be ﬁltered with their persistence
to prune spurious branches of the MA. Hence, the ﬁltered distance proﬁles are
robust against noise on the boundary. First experiments on the computation
of the proﬁles result in the expected alternating sequence of local minima and
maxima.
References
1. Biasotti, S., Giorgi, D., Spagnuolo, M., Falcidieno, B.: Reeb graphs for shape analy-
sis and applications. Theor. Comput. Sci. 392(1–3), 5–22 (2008)
2. Bronstein, A.M., Bronstein, M.M., Bruckstein, A.M., Kimmel, R.: Analysis of two-
dimensional non-rigid shapes. Int. J. Comput. Vis. 78(1), 67–88 (2008)
3. Bronstein, A.M., Bronstein, M.M., Kimmel, R.: Numerical Geometry of Non-rigid
Shapes. Springer, New York (2008). doi:10.1007/978-0-387-73301-2
4. Carlsson, G., Zomorodian, A., Collins, A., Guibas, L.J.: Persistence barcodes for
shapes. Int. J. Shape Model. 11(02), 149–187 (2005)
5. Cerri, A., Fabio, B., Medri, F.: Multi-scale approximation of the matching distance
for shape retrieval. In: Ferri, M., Frosini, P., Landi, C., Cerri, A., Fabio, B. (eds.)
CTIC 2012. LNCS, vol. 7309, pp. 128–138. Springer, Heidelberg (2012). doi:10.
1007/978-3-642-30238-1 14
6. Dalitz, C., Brandt, C., Goebbels, S., Kolanus, D.: Fourier descriptors for broken
shapes. EURASIP J. Adv. Signal Process. 2013(1), 161 (2013)

318
I. Janusch et al.
7. Davies, E.R.: Machine Vision: Theory, Algorithms, Practicalities. Elsevier,
Amsterdam (2004)
8. Edelsbrunner, H.: Persistent homology: theory and practice (2014)
9. El-ghazal, A., Basir, O., Belkasim, S.: A new shape signature for Fourier descrip-
tors. In: 2007 IEEE International Conference on Image Processing, vol. 1, pp.
I-161–I-164, September 2007
10. Gonzalez-Diaz, R., Kropatsch, W.G., Cerman, M., Lamar, J.: Characterizing con-
ﬁgurations of critical points through LBP. In: SYNASC 2014 Workshop on Com-
putational Topology in Image Context (2014)
11. Janusch, I., Kropatsch, W.G.: Persistence based on LBP scale space. In: Bac,
A., Mari, J.-L. (eds.) CTIC 2016. LNCS, vol. 9667, pp. 240–252. Springer, Cham
(2016). doi:10.1007/978-3-319-39441-1 22
12. Leyton, M.: Symmetry-curvature duality. Comput. Vis. Graph. Image Process.
38(3), 327–341 (1987)
13. Leyton, M.: A Generative Theory of Shape. LNCS, vol. 2145. Springer, Heidelberg
(2001). doi:10.1007/3-540-45488-8
14. Ojala, T., Pietik¨ainen, M., Harwood, D.: A comparative study of texture measures
with classiﬁcation based on featured distributions. Pattern Recognit. 29(1), 51–59
(1996)
15. Pietik¨ainen, M., Hadid, A., Zhao, G., Ahonen, T.: Computer Vision Using Local
Binary Patterns. Computational Imaging and Vision, vol. 40. Springer, London
(2011). doi:10.1007/978-0-85729-748-8
16. Rosenfeld, A., Pfaltz, J.L.: Sequential operations in digital picture processing. J.
ACM (JACM) 13(4), 471–494 (1966)
17. Sebastian, T., Klein, P., Kimia, B.: Recognition of shapes by editing shock graphs.
In: IEEE International Conference on Computer Vision, vol. 1, p. 755. IEEE Com-
puter Society (2001)
18. Tieng, Q.M., Boles, W.W.: Recognition of 2D object contours using the wavelet
transform zero-crossing representation. IEEE TPAMI 19(8), 910–916 (1997)
19. van Otterloo, P.J.: A Contour-Oriented Approach to Shape Analysis. Prentice Hall
International Series in Acoustics, Speech & Si. Prentice Hall, Englewood (1991)
20. Verri, A., Uras, C., Frosini, P., Ferri, M.: On the use of size functions for shape
analysis. Biol. Cybern. 70(2), 99–107 (1993)
21. Zhang, D., Lu, G.: Review of shape representation and description techniques.
Pattern Recognit. 37(1), 1–19 (2004)
22. Zhang, D., Lu, G., et al.: A comparative study on shape retrieval using Fourier
descriptors with diﬀerent shape signatures. In: Proceedings of the International
Conference on Intelligent Multimedia and Distance Education, pp. 1–9. Citeseer
(2001)

Greyscale Image Vectorization from Geometric
Digital Contour Representations
Bertrand Kerautret1(B), Phuc Ngo1, Yukiko Kenmochi2,
and Antoine Vacavant3
1 LORIA, UMR CNRS 7503, Universit´e de Lorraine,
54506 Vandœuvre-l`es-Nancy, France
Bertrand.Kerautret@univ-lorraine.fr
2 LIGM, UMR CNRS 8049, Universit´e Paris-Est, 77454 Marne-la-Vall´ee, France
3 Institut Pascal, Universit´e Clermont Auvergne, UMR 6602 CNRS/UCA/SIGMA,
63171 Aubi`ere, France
Abstract. In the ﬁeld of digital geometry, numerous advances have been
recently made to eﬃciently represent a simple polygonal shape; from
dominant points of a curvature-based representation, a binary shape is
eﬃciently represented even in presence of noise. In this article, we exploit
recent results of such digital contour representations and propose an
image vectorization algorithm allowing a geometric quality control. All
the results presented in this paper can also be reproduced online.
1
Introduction
Image vectorization is a classic problem with potential impacts and applications
in computer graphic domains. Taking a raw bitmap image as input, this process
allows us to recover geometrical primitives such as straight segments, arcs of
circles and B´ezier curved parts. Such a transformation is exploited in design
softwares such as Illustrator or Inkscape in particular when users need to import
bitmap images. This domain is large and also concerns document analyses and
a variety of graphical objects such as engineering drawings [1], symbols [2], line
drawings [3], or circular arcs [4]. Related to these applications, diﬀerent theo-
retical advances are regularly proposed in the domains of pattern recognition
and digital geometry. In particular, their focus on digital contour representa-
tions concerns various methodologies: dominant point detection [5,6], relaxed
straightness properties [7], multi-scale analysis [8], irregular isothetic grids and
meaningful scales [9] or curvature-based approach [10].
These last advances are limited to digital shapes (i.e. represented within a
crisp binary image). Indeed, even if a digital contour can be extracted from a
non-binary image, their adaptation to other types of images such as greyscale and
color images is not straightforward. There are other methods, which are designed
to handle greyscale images; for example, Swaminarayan and Prasad proposed a
method to vectorize an image by using its contours (from simple Canny or Sobel
ﬁlters) and by performing Delaunay triangulation [11]. Despite its interest, the
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 319–331, 2017.
DOI: 10.1007/978-3-319-66272-5 26

320
B. Kerautret et al.
proposed approach does not take into account the geometrical characteristics of
edge contours and other information obtained from low intensity variations. More
oriented to segmentation methods, Lecot and Levy introduced an algorithm
based on Mumford and Shah’s minimization process to decompose an image
into a set of vector primitives and gradients [12]. Inspired from this technique,
which includes a triangulation process, other methods were proposed to represent
a digital image by using a gradient mesh representation allowing to preserve the
topological image structure [13]. We can also mention the method proposed
by Demaret et al. [14], which is based on linear spline over adaptive Delaunay
triangulation or the work of Xia et al. [15] exploiting a triangular mesh followed
of B´ezier curves patch based representation. From a more interactive process, the
vectorization method of Price and Barrett [16] was designed for interactive image
edition by exploiting graph cut segmentation and hierarchical object selections.
In this article, we propose to investigate the solutions exploiting the geometric
nature of image contours, and apply these advanced representations to handle
greyscale images. In particular, we will mainly focus on recent digital-geometry
based approaches of diﬀerent natures: (i) the approach based on dominant point
detection by exploiting the maximal straight segment primitives [6,17], (ii) the
one from the digital level layers with the algorithms proposed by Provot et al.
[18,19], (iii) the one using the Fr´echet distance [20,21], and (iv) the curvature
based polygonalization [10].
In the next section (Sect. 2), we ﬁrst present a strategy to reconstruct the
image by considering the greyscale image intensities. Diﬀerent strategies are
presented with pros and cons. Afterwards, in Sect. 3, we recall diﬀerent polygo-
nalization methods. Then, we present the main results and comparisons obtained
with diﬀerent polygonalization techniques (Sect. 4).
2
Vectorizing Images from Level Set Contours
The proposed method is composed of two main steps. The ﬁrst one is to extract
the level set contours of the image. This step is parameterized by the choice of the
intensity step (δI), which deﬁnes the intensity variations from two consecutive
intensity levels. From these contours, the geometric information can then be
extracted and selected by polygonalization algorithms (as described in Sect. 3).
The second step concerns the vectorial image reconstruction from these contours.
Step 1: Extracting Level Set Contours and Geometrical Information
The aim of this step is to extract geometrical information from the image inten-
sity levels. More precisely, we propose to extract all the level set contours by
using diﬀerent intensity thresholds deﬁned from an intensity step δI. For this
purpose, we apply a simple boundary extraction method deﬁned from an image
intensity predicate and from a connectivity deﬁnition. The method is based on
the deﬁnition of the Khalimsky space and can work in N dimension [22]. This
algorithm has the advantage to be implemented in the DGtal library [23] and
can be tested online [24].

Greyscale Image Vectorization from Geometric Digital Contour
321
(a)
(b)
(c)
(d)
Fig. 1. Extraction of the level set contours (b, c) from the input image (a) and sampling
resulting contours (d)
Figure 1 illustrates such a level set extraction (Fig. 1(b, c)) with a basic con-
tour sampling deﬁned from a point selection taken at the frequency of 20 pixels
(Fig. 1(d)). Naturally, the more advanced contour polygonalization algorithms,
as described in section Sect. 3, will be exploited to provide better contours, with
relevant geometrical properties.
Step 2: Vectorial Image Reconstruction
We explore several choices for the reconstruction of the resulting vectorial
images: the application of a triangulation based process, a reconstruction from
sequence of intensity intervals, and a component tree representation.
Representation from Delaunay triangulation: A ﬁrst solution was inspired
from the method proposed by Swaminarayan and Prasad [11], which uses a
Delaunay triangulation deﬁned on the edges detected in the image. However the
strategy is diﬀerent since the triangulation is performed after an extraction of
the main geometric features like dominant points, curvature or Fr´echet distance.
The ﬁrst direct integration of the triangulation was performed by using the
Triangle software [25] and by reconstructing the image with the mean intensity
represented in the triangle barycenter. Though, even if such strategy appears
promising, the quality is degraded if we have a closer view to the digital structure,
as illustrated for the DGCI logo in Fig. 2. To obtain a better quality of image
reconstruction, the level set intensity has to be integrated into the reconstruction
structure in order to make a better decision for the triangle intensity value.
Representation by ﬁlling single intensity intervals: The ﬁlling of the
polygonal region boundaries of the input image could be a possibility to obtain
a better image reconstruction. For instance, the image in Fig. 4(a) can be recon-
structed by ﬁlling inside the region R0 deﬁned from its borders (δ0
R0, δ1
R0, and
δ2
R0) and in the same way for the other regions R1, R2, R3, R4 and R5. However,
such a strategy may depend on the quality of the polygonalization, which has no
guarantee to generate the polygonal contours with correspondence between the
polygon vertices (according to the border of the current region and the border of
their adjacent region). For instance, the vertices of the polygonal representations

322
B. Kerautret et al.
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 2. Illustration of the Delaunay based reconstruction from sampled level set con-
tours: resulting mesh representation (a–c) and ﬁnal vectorial rendering (d–f)
of δ2
R0 and δ0
R2 do not necessarily share the same vertices. Such limitations are
illustrated in the Fig. 3(a–e) where numerous defects are present with null-color
areas which are not ﬁlled by any color.
Component tree based representation: To overcome the limitations of the
previous reconstruction, we propose to exploit a mathematical morphology based
representation deﬁned on the component tree [26,27]. This method allows to rep-
resent an image by considering the successive intensity levels and by maintain-
ing the tree representing inclusions of connected components. As an illustration,
Fig. 4(d) shows such a representation with the root R0 associated to the ﬁrst
level; it contains all the pixels of the image (with values higher or equal to 0).
The edge between the nodes R4 and R2 indicates that the region R4 is included
in the region R2.
With such a representation, each region is -by deﬁnition- included in its
anchors and as a consequence a reconstruction starting from the root region
can be executed without any non-ﬁlled areas (even with poor-quality polygonal-
ization algorithms). As shown in Fig. 3(f–j), the reconstruction results have no
empty-color region.
Algorithm 1 describes the global process of the proposed method where we
can use the diﬀerent polygonalization methods that are detailed in the next
section.

Greyscale Image Vectorization from Geometric Digital Contour
323
Fig. 3. Comparisons of the quality of the two reconstructions types: single intensity
interval (a–e) and component tree based (f–j)

324
B. Kerautret et al.
Fig. 4. Image border extraction from a simple intensity predicate (images (a–c)) and
illustration of its component tree representation
Algorithm 1. Algorithm to vectorize a bitmap image based on component tree
representation and polygonalization algorithm.
Input
Image2D im
▷The input bitmap image.
Int iStep
▷Intensity step.
polygonalizationAlgo(cnt) ▷Polygonalization algorithm, which takes as input a
contour and return a list of polygon vertices.
Output
VectorialImage vectIm
▷Resulting vectorial image
Begin
for i = 0; i < 255 −iStep; i = i + iStep do
listCnt = ExtractAllCnt(0, iStep)
▷Extract all contours of connected
regions with pixel predicate: I(p) >= i
for all contour cnt in listCnt do
polygonalizationAlgo(cnt)
addToImageVector(vectIm, cnt)
End
3
Overview of Polygonalization Methods
In the proposed vectorization method, the polygonalization step can play an
important role in the resulting image quality. Before showing some results, we
overview several methods with diﬀerent principles.

Greyscale Image Vectorization from Geometric Digital Contour
325
3.1
Dominant Points Based Polygonalization
The polygonalization method proposed by Nguyen and Debled-Rennesson allows
to ﬁnd the characteristic points on a contour, called dominant points, to build
a polygon representing a given contour. The method consists in using a dis-
crete curve structure based on width ν tangential cover [28] (see Fig. 5(a)). This
structure is widely used for studying geometrical characteristics of a discrete
curve. More precisely, the width ν tangential cover of a contour is composed of
a sequence of all maximal blurred segments of width ν [29] along the contour.
Using this structure, a method proposed in [6,17] detects the dominant points
of a contour. The key idea of the method is that the candidates to be dominant
points are located in common zones of successive maximal blurred segments. By
a simple measure of angle, the dominant point in each common zone is deter-
mined as the point having the smallest angle with respect to the left and right
endpoints of the left and right maximal blurred segments constituting the com-
mon zone. Then, the polygon representing the contour is constructed from the
detected dominant points (see Fig. 5(b)). In the next section the acronym DPP
will refer to this method.
Fig. 5. (a) Width 2 tangential cover of a curve. (b) Dominant points detected on the
curve in (a) and the associated polygonal representation. (c) Polygonalization result
based on Fr´echet distance
3.2
DLL Based Polygonalization [18,19]
A method of curve decomposition using the notion of digital level layers (DLL)
has been proposed in [18,19]. Roughly speaking, DLL is an analytical primitive
which is modeled by a double inequality −ω ≤f(x) ≤ω. Thanks to this analytic
model of DLL, the method allows a large possible classes of primitives such as
lines, circles and conics.
Based on a DLL recognition algorithm, the method consists in decomposing
a given curve into the consecutive DLL and, by this way, an analytical represen-
tation of the curve is obtained. Figure 6 illustrates the DLL decomposition using

326
B. Kerautret et al.
Fig. 6. Decomposing results of DLL based method using diﬀerent primitives
line, circle and conic primitives. As the shape contains many line segments and
arcs, the decompositions based on circle and conic primitives uses less primitives
than the one with line primitives.
3.3
Fr´echet Based Polygon Representation
Using the exact Fr´echet distance, the algorithm proposed in [20,21] computes
the polygonal simpliﬁcation of a curve. More precisely, given a polygonal curve
P, the algorithm simpliﬁes P by ﬁnding the shortcuts in P such that the Fr´echet
distance between the shortcut and the associated original part of the curve is
less than a given error e. As a consequence, this method guarantees a minimum
number of vertices in the simpliﬁed curve according to the Fr´echet distance and
a maximal allowed error e (see Fig. 5(c)).
3.4
Visual Curvature Based Polygon Representation
The method was introduced by Liu, Latecki and Liu [10]. It is based on the
measure of the number of extreme points presenting on a height function deﬁned
from several directions. In particular, the convex-hull points are directly visible
as extreme points in the function and a scale parameter allows to retain only
the extreme points being surrounded by large enough concave or convex parts.
Thus, the non signiﬁcant parts can be ﬁltered from the scale parameter. The
main advantage of this estimator is the feature detection that gives an interest-
ing multi-scale contour representation even if the curvature estimation is only
qualitative. Figure 7 illustrates some examples.

Greyscale Image Vectorization from Geometric Digital Contour
327
Fig. 7. Illustration of the visual curvature obtained at diﬀerent scales
4
Results and Comparisons
In this section, we present some results obtained with the proposed method
using the previous polygonalization algorithms. Figure 8 shows the results with
their execution times (t) in link to the number of polygon points (#). The
comparisons were obtained by applying some variations on the scale parameter
(when exists: ν for DPP, e for Fr´echet) and on the intensity interval size s (s =
255/iStep, with iStep of Algorithm 1). From the presented results, we can observe
that the Fr´echet polygonalization methods provides a faster reconstruction but
contains more irregular contours. By comparing DLL and DPP, the DLL looks,
at low scale, slightly smoother than the others (Fig. 8(a) and (f)). The execution
time mainly depends of the choice of the input parameter s and the choice
of the method (see time measure given in Fig. 8). Due to page limitation the
results obtained with the visual-curvature based polygonalization method are not
presented but they can be obtained from an Online demonstration available
at:
http://ipol-geometry.loria.fr/∼kerautre/ipol demo/DGIV IPOLDemo/
To conclude this section we also performed comparisons with Inkscape, a free
drawing software [30] (Fig. 9). We can observe that our representation provides
ﬁne results, with a lighter PDF ﬁle size.
5
Conclusion and Perspectives
Exploiting recent advances from the representation of digital contours, we have
proposed a new framework to construct a vectorial representation from an input
bitmap image. The proposed algorithm is simple, can integrate many other polyg-
onalization algorithms and can be reproduced online. Moreover, we propose a vec-
torial representation of the image with a competitive eﬃciency, w.r.t. Inkscape [30].

328
B. Kerautret et al.
Fig. 8. Experiments of the proposed level set based method on the Lena image with
three polygonalization algorithms: (a–d) dominant points (DPP), (e–h) digital level
layers with line primitive (DLL) and (i–l) Fr´echet distance based

Greyscale Image Vectorization from Geometric Digital Contour
329
Fig. 9. Comparisons of vectorization obtained with Inkscape and Fr´echet (resp. DLL)
based vectorization (ﬁrst row, resp. second row).
This work opens new perspectives with for instance the integration of the mean-
ingful scale detection to ﬁlter only the signiﬁcant intensity levels [31]. Also, we
would like to evaluate the robustness of our approach with diﬀerent polygonaliza-
tion algorithms, by testing images corrupted by noise.
References
1. Tombre, K.: Analysis of engineering drawings: state of the art and challenges. In:
Tombre, K., Chhabra, A.K. (eds.) GREC 1997. LNCS, vol. 1389, pp. 257–264.
Springer, Heidelberg (1998). doi:10.1007/3-540-64381-8 54
2. Cordella, L., Vento, M.: Symbol recognition in documents: a collection of tech-
niques? Int. J. Doc. Anal. Recognit. 3, 73–88 (2000)
3. Hilaire, X., Tombre, K.: Robust and accurate vectorization of line drawings. IEEE
Trans. Pattern Anal. Mach. Intell. 28, 890–904 (2006)
4. Nguyen, T.P., Debled-Rennesson, I.: Arc segmentation in linear time. In: Real,
P., Diaz-Pernil, D., Molina-Abril, H., Berciano, A., Kropatsch, W. (eds.) CAIP
2011. LNCS, vol. 6854, pp. 84–92. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-23672-3 11
5. Marji, M., Siy, P.: Polygonal representation of digital planar curves through domi-
nant point detection – a nonparametric algorithm. Pattern Recognit. 37, 2113–2130
(2004)
6. Nguyen, T.P., Debled-Rennesson, I.: A discrete geometry approach for dominant
point detection. Pattern Recognit. 44, 32–44 (2011)

330
B. Kerautret et al.
7. Bhowmick, P., Bhattacharya, B.B.: Fast polygonal approximation of digital curves
using relaxed straightness properties. IEEE Trans. Pattern Anal. Mach. Intell. 29,
1590–1602 (2007)
8. Feschet, F.: Multiscale analysis from 1D parametric geometric decomposition of
shapes. In: International Conference on Pattern Recognition, pp. 2102–2105 (2010)
9. Vacavant, A., Roussillon, T., Kerautret, B., Lachaud, J.O.: A combined multi-
scale/irregular algorithm for the vectorization of noisy digital contours. Comput.
Vis. Image Underst. 117, 438–450 (2013)
10. Liu, H., Latecki, L.J., Liu, W.: A uniﬁed curvature deﬁnition for regular, polygonal,
and digital planar curves. Int. J. Comput. Vis. 80, 104–124 (2008)
11. Swaminarayan, S., Prasad, L.: Rapid automated polygonal image decomposition.
In: 35th IEEE Applied Imagery and Pattern Recognition Workshop (AIPR 2006),
p. 28 (2006)
12. Lecot, G., Levy, B.: Ardeco: automatic region detection and conversion. In: 17th
Eurographics Symposium on Rendering-EGSR 2006, pp. 349–360 (2006)
13. Sun, J., Liang, L., Shum, H.Y.: Image vectorization using optimized gradient
meshes. ACM Trans. Graph. 26, 11 (2007). ACM
14. Demaret, L., Dyn, N., Iske, A.: Image compression by linear splines over adaptive
triangulations. Signal Process. 86, 1604–1616 (2006)
15. Xia, T., Liao, B., Yu, Y.: Patch-based image vectorization with automatic curvi-
linear feature alignment. Trans. Graph. 28, 115 (2009). ACM
16. Price, B., Barrett, W.: Object-based vectorization for interactive image editing.
Vis. Comput. 22, 661–670 (2006)
17. Ngo, P., Nasser, H., Debled-Rennesson, I.: Eﬃcient dominant point detection based
on discrete curve structure. In: Barneva, R.P., Bhattacharya, B.B., Brimkov, V.E.
(eds.) IWCIA 2015. LNCS, vol. 9448, pp. 143–156. Springer, Cham (2015). doi:10.
1007/978-3-319-26145-4 11
18. G´erard, Y., Provot, L., Feschet, F.: Introduction to digital level layers. In: Debled-
Rennesson, I., Domenjoud, E., Kerautret, B., Even, P. (eds.) DGCI 2011. LNCS,
vol. 6607, pp. 83–94. Springer, Heidelberg (2011). doi:10.1007/978-3-642-19867-0 7
19. Provot, L., Gerard, Y., Feschet, F.: Digital level layers for digital curve decompo-
sition and vectorization. Image Process. On Line 4, 169–186 (2014)
20. Sivignon, I.: A near-linear time guaranteed algorithm for digital curve simpli-
ﬁcation under the Fr´echet distance. In: Debled-Rennesson, I., Domenjoud, E.,
Kerautret, B., Even, P. (eds.) DGCI 2011. LNCS, vol. 6607, pp. 333–345. Springer,
Heidelberg (2011). doi:10.1007/978-3-642-19867-0 28
21. Sivignon, I.: A near-linear time guaranteed algorithm for digital curve simpliﬁca-
tion under the Fr´echet distance. Image Process. On Line 4, 116–127 (2014)
22. Lachaud, J.O.: Coding Cells of Digital Spaces: A Framework to Write Generic
Digital Topology Algorithms, vol. 12, pp. 337–348. Elsevier (2003)
23. DGTal-Team: DGtal: digital geometry tools and algorithms library (2017). http://
dgtal.org
24. Coeurjolly, D., Kerautret, B., Lachaud, J.O.: Extraction of connected region
boundary in multidimensional images. Image Process. On Line 4, 30–43 (2014)
25. Shewchuk, J.R.: Delaunay reﬁnement algorithms for triangular mesh generation.
Comput. Geom. 22, 21–74 (2002)
26. Najman, L., Couprie, M.: Building the component tree in quasi-linear time. Trans.
Image Process. 15, 3531–3539 (2006)
27. Bertrand, G.: On the dynamics. Image Vis. Comput. 25, 447–454 (2007)
28. Faure, A., Buzer, L., Feschet, F.: Tangential cover for thick digital curves. Pattern
Recognit. 42, 2279–2287 (2009)

Greyscale Image Vectorization from Geometric Digital Contour
331
29. Debled-Rennesson, I., Feschet, F., Rouyer-Degli, J.: Optimal blurred segments
decomposition of noisy shapes in linear time. Comput. Graph. 30, 30–36 (2006)
30. https://inkscape.org/fr/
31. Kerautret, B., Lachaud, J.O.: Meaningful scales detection along digital contours
for unsupervised local noise estimation. IEEE Trans. Pattern Anal. Mach. Intell.
34, 2379–2392 (2012)

Discrete Models and Tools

The Boolean Map Distance:
Theory and Eﬃcient Computation
Filip Malmberg1(B), Robin Strand1, Jianming Zhang2, and Stan Sclaroﬀ3
1 Department of Information Technology, Centre for Image Analysis,
Uppsala University, Uppsala, Sweden
filip.malmberg@it.uu.se
2 Adobe Research, San Jose, USA
3 Department of Computer Science, Boston University, Boston, USA
Abstract. We propose a novel distance function, the boolean map dis-
tance (BMD), that deﬁnes the distance between two elements in an image
based on the probability that they belong to diﬀerent components after
thresholding the image by a randomly selected threshold value. This con-
cept has been explored in a number of recent publications, and has been
proposed as an approximation of another distance function, the mini-
mum barrier distance (MBD). The purpose of this paper is to introduce
the BMD as a useful distance function in its own right. As such it shares
many of the favorable properties of the MBD, while oﬀering some addi-
tional advantages such as more eﬃcient distance transform computation
and straightforward extension to multi-channel images.
1
Introduction
Distance functions and their transforms (DTs, where each pixel is assigned the
distance to a set of seed pixels) are used extensively in many image process-
ing applications. Here, we introduce a novel distance function, the boolean map
distance (BMD), that deﬁnes the distance between two elements in an image
based on the probability that they belong to diﬀerent components after thresh-
olding the image by a randomly selected threshold value. The idea of considering
connectivity with respect to randomly selected thresholds was ﬁrst introduced
by Zhang and Sclaroﬀ[5], who considered the probability that a pixel does not
belong to a component that touches the image border after thresholding with
a randomly selected value. This probability was used to identify salient objects
in an image, and the concept was named boolean map saliency. Here, we con-
sider the probability that any pair of pixels are not connected with respect
to a random threshold, and thus use the term boolean map distance (BMD).
Ideas related to this concept have further been explored in a number of recent
publications [1,4,6], and have been used as an approximation of another dis-
tance function, the minimum barrier distance (MBD) ﬁrst proposed by Strand
et al. [4].
The purpose of this paper is to introduce the BMD as a useful distance func-
tion in its own right. As such the BMD shares many of the favorable properties
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 335–346, 2017.
DOI: 10.1007/978-3-319-66272-5 27

336
F. Malmberg et al.
of the MBD, while oﬀering some additional advantages such as more eﬃcient
distance transform computation and straightforward extension to multi-channel
images. Speciﬁcally, our contributions are as follows:
– We provide a formal deﬁnition of the BMD, for both continuous and discrete
images.
– We show that the (continuous and discrete domain) BMD is a pseudo-metric,
thus motivating the name boolean map distance.
– We prove the equivalence between the (continuous and discrete domain) BMD
and the ϕ mapping introduced by Strand et al. [4] as an approximation to
the MBD. Thereby, we strengthen the connection between the BMD and
the MBD. Previously, this equivalence was only established in the discrete
case [6].
– We summarize available algorithms for computing distance transforms for
the discrete BMD. Speciﬁcally, we note that the equivalence between the dis-
crete BMD and ϕ functions allows the BMD to be expressed as the diﬀerence
between two well-known distance functions, whose distance transforms can
be computed using the image foresting transform [2], a generalization of Dijk-
stra’s algorithm. We demonstrate empirically that the resulting algorithm is
an order of magnitude faster than previously reported algorithms, while still
producing exact results.
2
The Boolean Map Distance in Rn
We deﬁne a n-dimensional gray-scale image I as a pair I = (D, f), where D ⊂Rn
and f : D →[0, 1] is a continuous function. The restriction of the image values
to the range [0, 1] does, for the purposes considered here, not imply a loss of
generality [6].
For p, q ∈D, a path from p to q (in D) is any continuous function π : [0, 1] →
D with π(0) = p and π(1) = q. We use the symbol ΠD
p,q to denote the family of
all such paths. The reverse π−1 of a path π is deﬁned as π−1(s) = π(1 −s) for
all s ∈[0, 1]. Recall that D ⊂Rn is path connected if for every p, q ∈D there
exists a path π : [0, 1] →D from p to q. For the remainder of this section, we
assume that the set D is path connected.
Let t ∈[0, 1]. Given an image I = (D, f), we deﬁne the function TI,t : D →
{0, 1} by
TI,t(p) =

0,
if f(p) < t
1,
otherwise
,
(1)
for all p ∈D. We refer to any function that maps image elements to the set
{0, 1} as a boolean map. The boolean map TI,t represents the thresholding of
the image I by t. For any p, q ∈D, we say that p and q belong to the same
component of TI,t if there exists a path π ∈ΠD
p,q such that either TI,t(π(s)) = 0
for all s ∈[0, 1] or TI,t(π(s)) = 1 for all s ∈[0, 1]. A path satisfying either of
these criteria is called a connecting path. Otherwise, p and q belong to diﬀerent

The Boolean Map Distance: Theory and Eﬃcient Computation
337
components of TI,t. We use the notation p ∼
t q to indicate that p and q belong to
the same component of TI,t, while the notation p ≁
t q indicates that they belong
to diﬀerent components.
Deﬁnition 1. Let t be a random value sampled from a uniform probability distri-
bution over [0, 1]. The continuous domain boolean map distance BMD : D×D →
[0, 1] is deﬁned as
BMD(p, q) = P(p ≁
t q) = 1 −P(p ∼
t q)
(2)
for all p, q ∈D, where P(A) denotes the probability of the event A.
Deﬁnition 2. A function d : D × D →[0, ∞) is a pseudo-metric on a set D if,
for every p, q, r ∈D,
(i) d(p, p) = 0 (identity)
(ii) d(p, q) ≥0 (non-negativity)
(iii) d(p, q) = d(p, q) (symmetry)
(iv) d(p, r) ≤d(p, q) + d(q, r) (triangle inequality)
If additionally it holds that d(p, q) = 0 ⇔p = q for all p, q, then d is a metric.
In the proof that BMD is a pseudo-metric, we will use the following notion.
The concatenation π1 · π2 of the paths π1 and π2 such that π1(1) = π2(0) is
(π1 · π2)(s) =

π1(2s)
if s ∈[0, 1/2]
π2(2s)
otherwise
,
(3)
Theorem 1. BMD is a pseudo-metric.
Proof. First, we show that BMD obeys property (i). Consider a path π(s) such
that π(s) = p for any s ∈[0, 1]. This path is obviously connecting p to itself.
Thus, p ∼
t p for all t, and so P(p ≁
t q) = 0.
Since BMD is deﬁned as a probability, we have BMD(p, q) ∈[0, 1] for all p, q.
Thus, BMD clearly obeys property (ii).
Next, we show that BMD obeys property (iii). If, for a given threshold t,
there exists a connecting path π from p to q then the reverse path π−1 is a
connecting path from q to p. Thus, p ≁
t q ⇔q ≁
t p, and so P(p ≁
t q) = P(q ≁
t p).
Finally, we show that BMD obeys property (iv). If, for a given threshold t,
there exists a connecting path π1 from p to q and another connecting path π2
from q to r, then the concatenation π1 · π2 of these two paths is a connecting
path from p to r. Thus, the set of thresholds t for which q ∼
t p and q ∼
t r is a
subset of the set of thresholds for which p ∼
t r, and so BMD(p, r) ≤BMD(p, q)+
BMD(q, r).
⊓⊔
Note that for a constant function f we have P(p ≁
t q) = 0 for all p, q ∈D,
and thus BMD is not in general a metric.

338
F. Malmberg et al.
2.1
Equivalence Between BMD and ϕ
In this section, we prove the equivalence between the proposed BMD mapping
and the ϕ mapping deﬁned by Strand et al. (Deﬁnition 3 in [4]). We recall the
deﬁnition of ϕ:
Deﬁnition 3. Let I = (D, f). The mapping ϕ : D × D →[0, ∞) is deﬁned as
ϕ(p, q) =
inf
π1∈ΠD
p,q
max
s
f(π1(s)) −
sup
π2∈ΠD
p,q
min
s
f(π2(s))
(4)
for all p, q ∈D.
As stated by Strand et al. [4] the minimum/maximum over the numbers s ∈[0, 1]
are attained, while neither the inﬁmum nor supremum operators can be replaced
by maximum/minimum.
Theorem 2. The mappings BMD and ϕ are equal, i.e. ϕ(p, q) = BMD(p, q) for
all p, q ∈D.
Proof. We begin our proof by observing that
BMD(p, q) = P(p ≁
t q) = 1 −P(p ∼
t q) = 1 −P(A ∨B)
(5)
where A is the event (p ∼
t
q ∧f(p) < t ∧f(q) < t) and B is the event
(p ∼
t q ∧f(p) ≥t ∧f(q) ≥t). Since the events A and B are mutually exclusive,
it follows that
BMD(p, q) = 1 −P(A) −P(B).
(6)
First, we study the probability P(A) that p and q belong to the same com-
ponent of TI,t, and f(p) and f(q) are both less than t. This is true if there exists
a path π ∈ΠD
p,q such that
f(π(s)) < t for all s ∈[0, 1].
(7)
Let c =
inf
π1∈ΠD
p,q
max
s
f(π1(s)). Then a path π satisfying the condition given in
Eq. (7) exists if t > c, but does not exist if t < c. If t = c, then the existence
of π is not possible to determine in the general case but depends on whether,
for the speciﬁc f, p and q at hand, there exists a path whose maximum value
max
s
f(π(s)) attains the inﬁmum over all paths in ΠD
p,q. Let φ : [0, 1] →{0, 1}
be an indicator function for the event A, deﬁned by
φ(t) =

1
if A
0
otherwise
(8)
Depending on the speciﬁc values of f, p and q, we have either φ = φ1 or
φ = φ2 where
φ1(t) =

1
if t ≥c
0
otherwise
φ2(t) =

1
if t > c
0
otherwise .
(9)

The Boolean Map Distance: Theory and Eﬃcient Computation
339
We note that
P(A) =
 1
0
φ(t)dt =
 1
0
φ1(t)dt =
 1
0
φ2(t)dt = 1−c = 1−
inf
π1∈ΠD
p,q
max
s
f(π1(s)).
(10)
Thus, regardless of the existence of the path π in the case where t = c, the
probability of the event A occurring is P(A) = 1 −
inf
π1∈ΠD
p,q
max
s
f(π1(s)).
Next, we study the probability P(B) that p and q belong to the same com-
ponent of TI,t, and that f(p) and f(q) are both greater than or equal to t. This
is true if there exists a path π ∈ΠD
p,q such that f(π(s)) ≥t for all s ∈[0, 1].
Such a path exists iﬀt <
sup
π2∈ΠD
p,q
min
s
f(π2(s)). The probability of the event B
occurring is P(B) =
sup
π2∈ΠD
p,q
min
s
f(π2(s)).
From Eq. (6), it thus follows that
BMD(p, q) =
inf
π1∈ΠD
p,q
max
s
f(π1(s)) −
sup
π2∈ΠD
p,q
min
s
f(π2(s)) = ϕ(p, q).
(11)
⊓⊔
3
The Discrete Boolean Map Distance
In this section, we introduce a discrete formulation of the BMD.
We deﬁne a discrete gray-scale digital image ˆI as a pair ˆI = ( ˆD, f) consisting
of a ﬁnite set ˆD of image elements and a mapping f : ˆD →[0, 1]. We will
refer to elements of ˆD as pixels, regardless of the dimensionality of the image.
Additionally, we deﬁne a mapping N : ˆD →P( ˆD) specifying an adjacency
relation over the set of pixels ˆD. For any p, q ∈ˆD, we refer to N(p) as the
neighborhood of p and say that q is adjacent to p if q ∈N(p). We require the
adjacency relation to be symmetric, so that q ∈N(p) ⇔p ∈N(q) for all
p, q ∈ˆD.
A discrete path ˆπ = ⟨ˆ
π(0), ˆπ(1), . . .
ˆ
π(k)⟩of length |ˆπ| = k + 1 from
ˆ
π(0) to
ˆπ(k) is an ordered sequence of pixels in ˆD where each consecutive pair of pixels
are adjacent. We use the symbol ˆΠ ˆ
D
p,q to denote the set of all discrete paths from
p to q. For a set of pixels S ⊆ˆD, the symbol ˆΠ ˆ
D
p,S denotes the set of all discrete
paths from p to any q ∈S. The reverse ˆπ−1 of a discrete path ˆπ is deﬁned as
ˆπ−1(i) = ˆπ(k −i) for all i ∈{0, 1, . . . , k}. Given two discrete paths ˆπ1 and ˆπ2
such that the endpoint of ˆπ1 equals the starting point of ˆπ2, we denote by ˆπ1 · ˆπ2
the concatenation of the two paths.
Throughout, we assume that the combination of the set ˆD and the adjacency
relation N deﬁnes a connected graph, so that for every pair of pixels p, q ∈ˆD
there exists a path between them.

340
F. Malmberg et al.
Let t ∈[0, 1]. Given a discrete image ˆI = ( ˆD, f), we deﬁne the thresholding
TˆI,t : ˆD →{0, 1} of ˆI by t as
TˆI,t(p) =

0,
f(p) < t
1,
otherwise ,
(12)
For any p, q ∈ˆD, we say that p and q belong to the same component of TˆI,t if
there exists a path ˆπ ∈ˆΠ ˆ
D
p,q such that either TˆI,t(ˆπ(i)) = 0 for all i ∈{0, 1, . . . , k}
or TˆI,t(ˆπ(i)) = 1 for all i ∈{0, 1, . . . , k}. A path satisfying either of these criteria
is called a connecting path. As before, we use the notation p ∼
t q to indicate that
p and q belong to the same component of TˆI,t, while the notation p ≁
t q indicates
that they belong to diﬀerent components. Additionally, for a set of pixels S, the
notation p ∼
t S indicates that p ∼
t q for at least one q ∈S while the notation
p ≁
t S indicates that p ≁
t q for all q ∈S.
Deﬁnition 4. Let t be a random value sampled from a uniform probability dis-
tribution over [0, 1]. For any set of pixels S ⊆ˆD and pixel p ∈ˆD, the discrete
boolean map distance
ˆ
BMD : ˆD × P( ˆD) →[0, 1] is deﬁned as
ˆ
BMD(p, S) = P(p ≁
t S)
(13)
In the above deﬁnition, P( ˆD) denotes the power set of ˆD. If the set S consists
of a single element q, we can consider
ˆ
BMD to be a mapping from ˆD × ˆD to
[0, 1], and the deﬁnition can in this case be reduced to
ˆ
BMD(p, q) = P(p ≁
t q).
Theorem 3. Let S ⊆ˆD consist of a single element q and let p ∈ˆD. Then the
discrete
ˆ
BMD, viewed as a mapping from ˆD × ˆD to [0, 1], is a pseudo-metric.
The proof of Theorem 3 is identical to that of Theorem 1, provided that the
relevant continuous notions deﬁned in Sect. 2 are swapped out for their discrete
counterparts deﬁned in this section.
3.1
Equivalence Between the Discrete
ˆ
BMD and ˆϕ
In this section, we prove the equivalence between the discrete
ˆ
BMD mapping and
the ˆϕ mapping deﬁned by Strand et al. (Eq. (4) in [4]) as a discrete counterpart
of the ϕ mapping. We provide a slightly extended deﬁnition of ˆϕ:
Deﬁnition 5. The mapping ˆϕ : ˆD × P( ˆD) →[0, 1] is deﬁned as
ˆϕ(p, S) =
min
ˆπ1∈ˆ
Π ˆ
D
p,S

max
i∈{0,1,...,k}I(ˆπ1(i))

−
max
ˆπ2∈ˆ
Π ˆ
D
p,S

min
i∈{0,1,...,k}I(ˆπ2(i))

(14)
for all p ∈ˆD and S ⊆ˆD.

The Boolean Map Distance: Theory and Eﬃcient Computation
341
Note that if S consists of a single element q, the above deﬁnition of ˆϕ reduces
to the deﬁnition given by Strand et al. [4].
Theorem 4. The mappings
ˆ
BMD and ˆϕ are equal, i.e., ˆϕ(p, S) =
ˆ
BMD(p, S)
for all p ∈ˆD and S ⊆ˆD.
Proof. We start by observing that
ˆ
BMD(p, S) = P(p ≁
t S) = 1 −P(p ∼
t S) = 1 −P(A ∨B)
(15)
where A is the event (p ∼
t q1 ∧f(p) < t∧f(q1) < t) for some q1 ∈S and B is the
event (p ∼
t q2 ∧f(p) ≥t ∧f(q2) ≥t) for some q2 ∈S. Since the events A and B
are mutually exclusive, it follows that
ˆ
BMD(p, S) = 1 −P(A) −P(B).
(16)
First, we study the probability P(A) that p and q1 belong to the same com-
ponent of TˆI,t, and f(p) and f(q1) are both less than t. This is true for some p1
if there exists a path ˆπ ∈ˆΠD
p,S such that f(ˆπ(i)) < t for all i ∈{0, 1, . . . , |ˆπ|−1}.
Such a path exists iﬀt ≥
min
ˆπ1∈ˆ
Π ˆ
D
p,S
max
i∈{0,1,...,k} f(ˆπ1(i)). The probability of this
event occurring is P(A) = 1 −
min
ˆπ1∈ˆ
Π ˆ
D
p,S
max
i∈{0,1,...,k} f(ˆπ1(i)).
Next, we study the probability P(B) that p and q2 belong to the same com-
ponent of TˆI,t, and that f(p) and f(q2) are both greater than or equal to t. This
is true for some q2 if there exists a path ˆπ ∈ˆΠ ˆ
D
p,S such that f(ˆπ(i)) ≥t for all
i ∈{0, 1, . . . , |ˆπ| −1}. Such a path exists iﬀt <
max
ˆπ2∈ˆ
Π ˆ
D
p,S
min
i∈{0,1,...,k} f(ˆπ2(i)). The
probability of the event B occurring is P(B) =
max
ˆπ2∈ˆ
Π ˆ
D
p,q
min
i∈{0,1,...,k} f(ˆπ2(i)).
From Eq. (16), it thus follows that
ˆ
BMD(p, q) =
min
ˆπ1∈ˆ
Π ˆ
D
p,S
max
i∈{0,1,...,k} f(ˆπ1(i)) −
max
ˆπ2∈ˆ
Π ˆ
D
p,S
min
i∈{0,1,...,k} f(ˆπ2(i)) = ˆϕ(p, S).
(17)
⊓⊔
A proof of Theorem 4 was previously provided by Zhang and Sclaroﬀ[6].
4
Computing Distance Transforms for the Discrete BMD
Given a discrete image ˆI = ( ˆD, f) and a set of seed pixels S ⊆ˆD, the distance
transform for the discrete BMD is a map assigning to each pixel p ∈ˆD the
value BMD(p, S), i.e., each pixel is assigned the discrete boolean map distance
to the set S. In this section, we study various methods for computing distance
transforms for the discrete BMD.

342
F. Malmberg et al.
4.1
Monte Carlo Approximation
From the deﬁnition of the BMD, it is straightforward to devise a Monte Carlo
algorithm for approximating the BMD distance transform by iteratively selecting
a random threshold, performing thresholding, and using a ﬂood-ﬁll operation to
ﬁnd the set of pixels connected to at least one seed-point. As the number of
iterations increases, the relative frequency with which each pixel belongs to the
complement of this set approaches the correct distance transform value.
4.2
The Zhang-SclaroﬀAlgorithm
Assume that all intensities present in a given image can be written as i/k, for
some ﬁxed integer k and some i in the set {1, 2, . . . , k}. This situation occurs in
practice if we, e.g., remap an image with integer intensity values to the range
[0, 1]. If gray levels are stored as 8-bit integers, for example, we can take k = 256.
Then the algorithm proposed by Zhang and Sclaroﬀfor calculating Boolean
Map Saliency [6] can be used directly for computing the exact BMD distance
transform from any set of seed-points. Pseudo-code for this algorithm is listed
in Algorithm 1.
In this algorithm each iteration of the foreach-loop requires O(n) operations,
where n is the number of image pixels. Thus, the entire algorithm terminates
in O(nk) operations which, since k can be considered a constant, equals O(n)
operations.
Algorithm 1. The Zhang-Sclaroﬀalgorithm for computing the discrete
BMD distance transform.
Input: An image I, a set of seed-points S, an integer k
Output: Distance transform D
1 Set D(p) = 0 for all pixels p in I;
2 foreach i ∈{1, 2, . . . , k} do
3
Set B ←TI,i/k;
4
Perform a ﬂood-ﬁll operation to identify the set of pixels belonging to the
same component as at least one seed-point in B;
5
Increase D by 1/(k) for all pixels not in this set;
6 end
4.3
Dijkstra’s Algorithm
As shown in Sect. 3.1, the discrete BMD can be written as the diﬀerence between
two functions:
min
ˆπ∈ˆ
Π ˆ
D
p,S

max
i∈{0,1,...,k}]I(ˆπ(i))

(18)
and
max
ˆπ∈ˆ
Π ˆ
D
p,S

min
i∈{0,1,...,k}I(ˆπ(i))

.
(19)

The Boolean Map Distance: Theory and Eﬃcient Computation
343
Fig. 1. Left: “Cameraman” image used in the experiments. The location of the single
seed-point is indicated in green. Right: The corresponding BMD distance transform.
(Color ﬁgure online)
Fig. 2. Left: Approximation error of Monte Carlo estimation as a function of the num-
ber of samples. Right: Empirical comparison of running time for the Zhang-Sclaroﬀ
algorithm and Dijkstra’s algorithm.
Both of these functions are path based distance functions, and are smooth in the
sense deﬁned by Falc˜ao et al. [2]. Therefore, distance transforms for each term can
be computed in O(n log n) operations using the image foresting transform [2], a
generalization of Dijkstra’s algorithm. In the case where the magnitude of the
set of all intensities present in the image is bounded by a ﬁxed integer, this can
further be reduced to O(n) operations [2].
4.4
Empirical Comparison of Running Time
In this section, we perform an empirical comparison of the algorithms described
above.
First, we consider the Monte Carlo approximation method. This algorithm
diﬀers from the others in that it only produces approximate results. The error
of the approximation decreases as the number of iterations is increased, but this
also increases the computation time. We are thus interested in investigating the

344
F. Malmberg et al.
trade-oﬀbetween number of iterations and approximation error. To this end,
we use the Monte Carlo approximation method to compute an approximate
BMD distance transform of the “Cameraman” image shown in Fig. 1, from the
single seed-point indicated in the ﬁgure for a varying number of iterations. Each
result was compared to the true distance transform, computed using Dijkstra’s
algorithm. Figure 2 (left) shows the average error per pixel as a function of the
number of iterations. Due to the stochastic nature of the algorithm, the mean
error is itself noisy. To increase clarity the ﬁgure therefore shows, for each number
of samples, the average error obtained when repeating the experiment 20 times.
As Fig. 2 (left) shows, a large number of samples is required to obtain an accurate
approximation.
The Zhang-Sclaroﬀalgorithm and the Dijkstra based method both have lin-
ear time complexity, but the constants involved diﬀer substantially. To compare
the algorithms empirically we calculated distance transforms for the “Camera-
man” image shown in Fig. 1, scaled to various sizes using bi-cubic interpolation,
using both algorithms. The gray-levels in this image are stored as 8-bit inte-
gers, so we take k = 256 for the Zhang-Sclaroﬀalgorithm. In all cases, a single
seed-point was placed at the top left corner of the image. The resulting run-
ning times are shown in Fig. 2 (right). Both algorithms show the expected linear
dependence between image size and running time, but the approach based on
Dijkstra’s algorithm is faster by about a factor 30–40.
5
Extension to Multi-channel Images
To extend the BMD to multi-channel images, we consider the following procedure
for creating a boolean map:
1. Randomly select one of the image channels according to some probability
distribution over the set of image channels.
2. Randomly select a threshold from a uniform distribution over [0, 1] and
threshold the selected image channel at this value.
The multi-channel BMD between two pixels in an image with m channels is
then deﬁned as the probability that they belong to diﬀerent components of the
boolean map obtained by the above procedure. This probability is given by
BMD(p, q) = w1BMD1(p, q) + w2BMD2(p, q) + . . . + wmBMDm(p, q),
(20)
where wi denotes the probability of selecting channel i and BMDi denotes the
single channel BMD deﬁned on channel i. For example, we may chose wi = 1/m
for all i ∈{1, 2, ..., m}. Note that the above result applies to both the continuous
and discrete BMD. In the discrete case, this means that we can compute BMD
distance transforms for multi-channel images by computing the single channel
BMD on each channel, and forming a weighted average of the results.
An illustration of computing a multi-channel BMD distance transform on a
color image is shown in Fig. 3.

The Boolean Map Distance: Theory and Eﬃcient Computation
345
Fig. 3. Top left: “Flower” image (from Rhemann et al. [3]) with seedpoints overlaid
in white. Top right: Gray-scale image. Bottom left: BMD distance transform of the
color image, after transformation to the CIEL*a*b* color space. Bottom right: BMD
distance transform of the gray-scale image. The values of both distance transforms have
been scaled for display purposes. Compared to the single-channel distance transform,
the multi-channel BMD distance transform better captures the contrast between the
ﬂower and the background.
6
Conclusion
We have introduced the Boolean map distance (BMD), a pseudo-metric that
measures the distance between elements in an image based on the probability
that they belong to diﬀerent components after thresholding the image by a
randomly selected value. Formal deﬁnitions of the BMD have been given in both
the continuous and discrete settings. The equivalence between the BMD and
the ϕ mapping proposed by Strand et al. was previously shown in the discrete
case [6]. We have extended this proof to also cover the continuous case, thereby
further strengthening the connection between the BMD and the MBD.
We have summarized available algorithms for computing distance transforms
for the discrete BMD. From the empirical comparison, we conclude that the
Monte Carlo approximation method is not suitable for practical applications,
given the existence of eﬃcient exact algorithms. In the comparison between
exact algorithms, we found that the approach based on Dijkstra’s algorithm was
faster than the Zhang-Sclaroﬀalgorithm by an order of magnitude for calcu-
lating distance transforms on gray-scale images stored using 8-bit pixels. With
increased color depth, the diﬀerence in computation time will increase.
Various aspects of the ideas presented have been explored in previous publi-
cations [1,4–6]. The BMD, however, has not to our knowledge previously been
proposed as a distance function in its own right. By compiling and extending

346
F. Malmberg et al.
ideas previously scattered across multiple publications, we hope to highlight the
BMD as a valuable distance function for image processing tasks.
References
1. Strand, R., Malmberg, F., Saha, P.K.: Eﬃcient algorithm for ﬁnding the exact
minimum barrier distance. Comput. Vis. Image Underst. 123, 53–64 (2014)
2. Falc˜ao, A.X., Stolﬁ, J., de Alencar Lotufo, R.: The image foresting transform: theory,
algorithms, and applications. IEEE Trans. Pattern Anal. Mach. Intell. 26(1), 19–29
(2004)
3. Rhemann, C., Rother, C., Wang, J., Gelautz, M., Kohli, P., Rott, P.: A perceptually
motivated online benchmark for image matting. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR 2009), pp. 1826–1833. IEEE (2009)
4. Strand, R., Ciesielski, K.C., Malmberg, F., Saha, P.K.: The minimum barrier dis-
tance. Comput. Vis. Image Underst. 117(4), 429–437 (2013)
5. Zhang, J., Sclaroﬀ, S.: Saliency detection: a boolean map approach. In: Proceedings
of the IEEE International Conference on Computer Vision, pp. 153–160 (2013)
6. Zhang, J., Sclaroﬀ, S.: Exploiting surroundedness for saliency detection: a boolean
map approach. IEEE Trans. Pattern Anal. Mach. Intell. 38(5), 889–902 (2016)

Fast and Eﬃcient Incremental Algorithms
for Circular and Spherical Propagation
in Integer Space
Shivam Dwivedi1, Aniket Gupta1, Siddhant Roy1, Ranita Biswas1(B),
and Partha Bhowmick2
1 Department of Computer Science and Engineering,
Indian Institute of Technology, Roorkee, India
shivamdw22@gmail.com, aniketguptaknp@gmail.com,
raj.siddhant.rohit@gmail.com, biswas.ranita@gmail.com
2 Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur, India
bhowmick@gmail.com
Abstract. Space ﬁlling circles and spheres have various applications
in mathematical imaging and physical modeling. In this paper, we ﬁrst
show how the thinnest (i.e., 2-minimal) model of digital sphere can be
augmented to a space ﬁlling model by ﬁxing certain “simple voxels” and
“ﬁller voxels” associated with it. Based on elementary number-theoretic
properties of such voxels, we design an eﬃcient incremental algorithm
for generation of these space ﬁlling spheres with successively increas-
ing radius. The novelty of the proposed technique is established further
through circular space ﬁlling on 3D digital plane. As evident from a pre-
liminary set of experimental result, this can particularly be useful for
parallel computing of 3D Voronoi diagrams in the digital space.
Keywords: Digital circle · Digital sphere · Space ﬁlling curve · Space
ﬁlling surface · Spherical propagation · Voronoi diagram
1
Introduction
Space ﬁlling curves and surfaces ﬁnd numerous applications in scientiﬁc com-
puting, especially when the space is discretized by a well-deﬁned grid or lattice.
In digital geometry, the discretization usually refers to a collection of isotropic
pixels in 2D and isotropic voxels in 3D. Our domain of interest is discrete 3D
space or a discrete 3D plane, which eventually needs to be ﬁlled by voxels in
a spherical or in a circular fashion. There exist few techniques in the literature
on this; see, for example, [17,18], where a marching sphere approach to wall
distance calculation is proposed. It uses a modiﬁed mid-point circle algorithm to
produce spheres at each level while ensuring that no gap is produced in between
two consecutive concentric digital spheres. However, the adopted model of digi-
tal sphere conforms to only 16-symmetry and hence is not uniform with respect
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 347–359, 2017.
DOI: 10.1007/978-3-319-66272-5 28

348
S. Dwivedi et al.
Table 1. List of symbols used and their meaning
Symbol Meaning
S(r)
Real sphere of radius r
S(r)
Naive sphere of radius r
S(r)
(isothetic) 1
2-oﬀset digital sphere of r
S(r)
Space ﬁlling digital sphere of radius r
S∗(r)
Solid digital sphere of radius r
χ(r)
Set of simple voxels corresponding to S(r)
F(r)
Set of ﬁller voxels between S(r) and S(r + 1)
B(r)
Set of inter-octant boundary voxels of S(r)
to diﬀerent coordinates. Nonetheless, that work suggests the usability of sphere
propagation in a real-life situation where distance from a moving or deforming
body needs be estimated with reasonable eﬃciency in computation.
During spherical propagation, gaps or absentee voxels is a crucial issue, which
needs to be tackled with guarantee and eﬃciency. Work on absentee characteri-
zation between concentric circles and spheres in digital space can be seen in [4–6].
Gap-free solid sphere generation using layering approach is also reported in [7],
which is however suitable for voxel-based rapid prototyping but not for spherical
propagation. In this paper, we show how space ﬁlling digital sphere can be prop-
agated with eﬃcient computation, which can subsequently be used for successful
creation of Voronoi diagrams in 3D digital space.
For the basic terms and deﬁnitions useful for our work, we refer to [16].
For more speciﬁc terms and concepts related to metrics and topology in the
voxel space, we refer to [8,9,13–15]. For formal deﬁnitions and details on paths,
tunnels, and gaps in discrete objects, we refer to [11].
For a discrete object A ⊂Z3, a coordinate plane, say, xy, is functional for
A, if for every voxel v(x0, y0, z0) ∈A there is no other voxel in A with the same
ﬁrst two coordinates. For example, let A = {(2, 5, 3), (2, 6, 3), (3, 5, 3)}; here the
functional plane of A is the xy-plane, since a bijection exists between A and its
projection on the xy-plane.
In Table 1, we have shown the list of important symbols used in this paper.
Some related images for easy understanding are shown in Fig. 1.
Our contribution. A naive sphere of integer radius and integer center is 2-
minimal and unique by composition. It has 48 symmetric parts (termed quadrag-
inta octants or q-octants in short), each with a deﬁnite functional plane, and
hence can be constructed very eﬃciently [8,9]. However, being not space ﬁll-
ing, as we show in this paper, it cannot readily be used for spherical or circular
propagations in the integer space. To circumvent this, we show how to aug-
ment the 2-minimal naive sphere to a space ﬁlling digital sphere. We exploit
certain number-theoretic properties of naive sphere to characterize the voxels
lying in between two consecutive naive spheres. Once this is ﬁxed, the space

Circular and Spherical Propagation in Integer Space
349
(a) S(r)
(b) S(r)
(c) S(r)
Fig. 1. Hemispheres of radius 11 (voxels: naive = white, simple = blue, ﬁller = red).
(Color ﬁgure online)
ﬁlling digital spheres are generated in succession very eﬃciently. We design an
incremental algorithm that uses the current solid sphere to build the next one
with simple integer operations. This, in turn, makes out algorithms for spherical
propagation in 3D space and circular propagation on any 3D digital plane. We
show several results to demonstrate its usefulness.
2
Space Filling Digital Sphere
There are diﬀerent discretization models for representing geometric objects in
digital space, each with its own advantages and disadvantages. Sphere is one
of the most important primitive objects and computationally interesting due to
its non-linearity. Its usual discretization models are naive, standard, supercover,
arithmetic, and τ-oﬀset. Among these, standard and supercover models produce
overlapping spheres for consecutive integer radii and hence not computationally
attractive for space ﬁlling. The τ-oﬀset model gives rise to diﬀerent models with
varying values of τ taken in the Euclidean metric space, as recently shown in [10].
An arithmetic sphere is identical with
1
2-oﬀset sphere and produces distinct
voxel sets for consecutive radii. However, the algorithm for its construction is
computationally heavy due to its generality to the real domain [1]. As tested
by us, the integer algorithm for arithmetic sphere in [1] takes around thrice as
much time as the algorithm proposed in this paper (Sect. 3). For example, with
radius 95, it takes 2.2 ms against 0.8 ms needed by our algorithm on the same
computing platform.
As shown in [8,9], a naive sphere is computationally very eﬃcient, as its
construction is based on simple integer operations like addition, increment, and
comparison, but devoid of any multiplication or division. This owes to the min-
imality of naive sphere. Due to its very minimality, though space ﬁlling is not
possible by successive naive spheres, they can be augmented by requisite ﬁller
voxels to serve the purpose. These ﬁller voxels admit interesting characterization,
as we show in this section.

350
S. Dwivedi et al.
Each q-octant has one of the coordinate planes as the functional plane [9].
This allows us to represent the q-octant merely as a 2D array, which results in
its quick computation. We show in this paper that the ﬁller voxels can eﬃciently
be found from few circular arcs in 3D, which are eﬀectively elliptical arcs in 2D
projection of those circular arcs on the functional plane.
2.1
Properties of Naive Sphere
We recall in particular some basic properties and characterization of the naive
(i.e., 2-minimal or thinnest) model of digital sphere [8]. These concepts help us
formalize the notion of ﬁller voxels and eventually propose the concept of space
ﬁlling spheres and spherical propagation while generating a higher-radius sphere
from a lower-radius one.
We use S(r) to denote a naive sphere of radius r and S1(r) to denote its
1st q-octant (0 ⩽x ⩽y ⩽z ⩽r). As explained in [8] and recently proved
in [9], a naive sphere S(r) is an irreducible 2-separating set of 3D integer points
(equivalently, voxels) such that max
p∈S(r) d⊥(p, S(r)) is minimized. Furthermore, the
set S(r) is unique and its closed form is as follows.
Deﬁnition 1 (Naive Sphere [8]). A naive sphere of radius r is given by
S(r) =

p ∈Z3 :

r2 −λ ⩽s < r2 + λ

∧

s ̸= r2 + λ −1

∨(μ ̸= λ)

, where,
p = (i, j, k), s = i2 + j2 + k2, λ = max{|i|, |j|, |k|}, and μ = med{|i|, |j|, |k|}.
By med{·}, we denote the median of values. The upper bound for S(r) is
r2 + λ whereas the lower bound for S(r + 1) is (r + 1)2 −λ. Therefore, all the
voxels falling in between these two values do not belong to S(r) or S(r + 1) or
any other naive sphere. Therefore, we have the following observation.
Fig. 2. Consecutive naive hemispheres from S(4) to S(15) shown in alternate gray
shades; voxels in between them are shown in red (ﬁller voxels) and blue (simple voxels).
(Color ﬁgure online)

Circular and Spherical Propagation in Integer Space
351
Observation 1. Naive sphere is not space ﬁlling.
Figure 2 shows consecutive naive spheres in two alternate shades of gray;
voxels not belonging to any naive sphere are in red and blue. Red voxels denote
the ﬁller voxels, which are more than 1
2 isothetic distance away from any real
sphere of integer radius (as explained in Sect. 2.2). Blue ones are the simple
voxels, which remain sandwiched between S(r) and S(r + 1), and lie within
isothetic distance
1
2 from the real sphere of radius r [8]. Characterization of
simple voxels and ﬁller voxels eventually simpliﬁes the generation of space ﬁlling
digital spheres, as we show in this paper. From the deﬁnition of simple voxels
given in [8], we make the following observation.
Observation 2. The set of simple voxels corresponding to S(r) is given by
χ(r) =

p ∈Z3 :

s = r2 + λ −1

∧(μ = λ)

, where, p = (i, j, k), s = i2 + j2 +
k2, λ = max{|i|, |j|, |k|} and μ = med{|i|, |j|, |k|}.
2.2
Characterization of Space Filling Digital Sphere
We deﬁne the (isothetic) 1
2-oﬀset digital sphere of radius r as the set of voxels
whose isothetic distance from S(r) is at most 1
2. For brevity, henceforth we refer
this model simply as ‘digital sphere’. This voxel set is eﬀectively S(r) in union
with all the simple voxels corresponding to radius r, and deﬁned precisely as
follows based on Observation 2.
Deﬁnition 2 ( 1
2-oﬀset digital sphere). For any positive integer r, the 1
2-oﬀset
digital sphere of radius r is given by S(r) =

p ∈Z3 : r2 −λ ⩽s < r2 + λ

, where,
p = (i, j, k), s = i2 + j2 + k2, and λ = max{|i|, |j|, |k|}.
The voxels other than χ(r) which lie in between S(r) and S(r + 1) comprise
the set of ﬁller voxels. This is the same set of voxels lying in between S(r) and
S(r + 1) and given as follows.
Observation 3. The set of ﬁller voxels lying in between S(r −1) and S(r) is
given by F(r) =

p ∈Z3 : (r −1)2 + λ ⩽s < r2 −λ

, where, p = (i, j, k), s =
i2 + j2 + k2, and λ = max{|i|, |j|, |k|}.
We derive a few interesting properties pertaining to the set of ﬁller voxels,
which leads to an eﬃcient algorithm for the construction of space ﬁlling spheres.
We denote by F1(r) the ﬁrst q-octant of F(r), and require the following lemmas
for this.
Lemma 1. For any voxel in F1(r), we have j + k ⩾r.
Proof. By Observation 3, for any voxel (i, j, k) in F1(r), we have
(r −1)2 + k ⩽i2 + j2 + k2 < r2 −k
⇒r2 −2r + 1 + k −k2 ⩽i2 + j2 < r2 −k −k2.
(1)

352
S. Dwivedi et al.
Since i ⩽j in the ﬁrst q-octant, from Eq. 1, we get 2j2 ⩾i2 + j2 ⩾r2 −
2r + 1 + k −k2. Let us assume, to the contrary of the lemma statement, that
j + k < r, or, j < r −k, which yields 2(r −k)2 ⩾r2 −2r + 1 + k −k2 ⇒f(k) :=
3k2 −(4r + 1)k + (r2 + 2r −1) ⩾0. For a constant value of r, the function
f(k) = 0 is an upward facing parabola, and hence the maximum values can be
reached only at the extremes. The domain of k satisﬁed by F1(r) is ( r
√
3, r], and
we get the values of f(k) at these extreme points as follows.
f( r
√
3) : 3r2 −4r2 −r + r2 + 2r −1 = −r −1 < 0
f(r) : r2 −
4
√
3r2 −
r
√
3 + r2 + 2r −1
= −( 4
√
3 −2)r2 −(2 +
1
√
3)r −1 < 0
The above negative values indicate a contradiction, whence the proof.
⊓⊔
Lemma 2. The ﬁller voxels in the ﬁrst q-octant are functional on both xy- and
xz-planes.
Proof. The lemma tells that if (i, j, k) ∈F1(r), then (i, j, k + 1) /∈F1(r) (for
being functional on xy-plane) and (i, j + 1, k) /∈F1(r) (for being functional on
xz-plane). Let us assume the contrary. Hence, ﬁrst we take both (i, j, k) and
(i, j, k + 1) belong to F1(r).
r2 −2r + k + 1 ⩽i2 + j2 + k2 < r2 −k
(2)
r2 −2r + k + 2 ⩽i2 + j2 + k2 + 2k + 1 < r2 −k −1
⇒r2 −2r + k + 2 −2k −1 ⩽i2 + j2 + k2 < r2 −k −2k −1
⇒r2 −2r −k + 1 ⩽i2 + j2 + k2 < r2 −3k −1
(3)
To make the two inequalities simultaneously true, we must have r2−2r+k+1 <
r2 −3k −1, which implies 2k < r −1. By Lemma 1, j + k ⩾r for F1(r). Since
j ⩽k in the 1st octant, we get 2k ⩾r, which contradicts our initial assumption.
The other part of the theorem regarding the functionality on xz-plane can also
be proved in a similar way.
⊓⊔
Lemma 3. The ﬁller voxels in the ﬁrst q-octant follow circular paths.
Proof. The voxels in F1(r) satisﬁes (r −1)2 + k ⩽i2 + j2 + k2 < r2 −k. For a
ﬁxed value of k, we get r2 −k2 + k ⩽i2 + j2 < (r + 1)2 −k2 −k ⇒r2 −k2 + k ⩽
i2 + j2 < r2 −k2 + 2r −k + 1. For a constant value of k, the set of voxels are
bounded by the range [r2 −k2 + k, r2 −k2 + 2r −k + 1) and has x-axis as the
functional axis (as F1(r) is functional on both xy- and xz-planes by Lemma 2).
Hence, for each k, the ﬁller voxels belong to a discrete circle.
⊓⊔
The properties of ﬁller voxels are used to deﬁne space ﬁlling digital spheres
suitable for spherical propagation. This is evident from the following theorem.
Theorem 1. S(r) := S(r) ∪F(r) ∪χ(r) = S(r) ∪F(r) is a space ﬁlling digital
sphere, and hence given by S(r) =

p ∈Z3 : (r −1)2 + λ ⩽s < r2 + λ

, where,
p = (i, j, k), s = i2 + j2 + k2, and λ = max{|i|, |j|, |k|}.

Circular and Spherical Propagation in Integer Space
353
Proof. From Deﬁnition 2, Observation 3, and Lemma 3, we get
S(r) = F(r) ∪S(r) ∪χ(r)
= F(r) ∪S(r)
=

p ∈Z3 : (r −1)2 + λ ⩽s < r2 −λ

∪

p ∈Z3 : r2 −λ ⩽s < r2 + λ

=

p ∈Z3 : ((r −1)2 + λ ⩽s < r2 −λ) ∨(r2 −λ ⩽s < r2 + λ)

=

p ∈Z3 : (r −1)2 + λ ⩽s < r2 + λ

.
⊓⊔
3
Frontier Propagation
By ‘frontier’, we mean the boundary of a voxel set, which is typically a solid
digital sphere or union of many such in our work. We propose an incremental
algorithm for frontier propagation, which uses the voxel set of the immediate
lower-radius sphere or circle to generate the higher-radius sphere or circle. For
this, the diﬀerence between these two are characterized very speciﬁcally. By
Theorem 1, S(r) is the union of F(r) and S(r). Circular characterization of F(r)
by Lemma 3 helps to generate the same in an eﬃcient manner. If we generate the
digital spheres independently of each other over successive radii, the algorithm
will not be eﬃcient. So, we use the voxel set of S(r −1) along with F(r) to
generate S(r) by the incremental algorithm. In this section, we ﬁrst identify the
challenges in this approach and then propose the algorithm by overcoming those.
Consider S1(r −1). Observe that it is functional on xy-plane. Hence, for
each (i, j) pair, the k value gets increased in S1(r) by 1 if (i, j, k + 1) is not a
ﬁller voxel or by 2 otherwise. Based on this observation, we ﬁrst increase the
k value for each (i, j) pair by 1 to produce a new voxel set, namely, Γ, which
contains all the voxels of F1(r) excepting the ones that have j = k. Note that
(i, k, k −1) /∈S1(r −1) and the remaining voxels of F1(r) are always 2-adjacent
to the voxels of S1(r −1). Each of the voxels of F1(r) are also 2-adjacent to
some voxels of S1(r). We traverse the circles of ﬁller voxels and add the voxel
(i, j, k+1) (both (i, j, k+1) and (i, j, k) if j = k) to Γ if (i, j, k) is the ﬁller voxel,
as others are already included in the last step. Note that the voxel set of Γ now
contains a set of voxels (i, j, k) for which we always have (i, j, k −1) ∈S1(r) or
(i, j, k −1) ∈F1(r). Therefore, the voxels of S(r) having the form (i, k, k) are
not added in Γ yet, as (i, k, k −1) /∈S1(r) or F1(r). Hence, the explained two
steps (along with the other 47 symmetric voxels for each voxel added) produce
almost all the voxels of S(r) excepting these voxels lying on the inter-octant
boundaries having max{|i|, |j|, |k|} = med{|i|, |j|, |k|}. Clearly these voxels do
not have immediate successor on their same q-octant. We have the following
observation regarding these extra voxels which are to be added in Γ to make it
equivalent to S1(r).
Observation 4. The set of inter-octant boundary voxels of S(r) is given by
B(r) =

p ∈Z3 : r2 −λ ⩽s < r2 + λ ∧(μ = λ)

, where, p = (i, j, k), s = i2 +
j2 + k2, and λ = max{|i|, |j|, |k|}.

354
S. Dwivedi et al.
Theorem 2. First q-octant of the space ﬁlling digital sphere of radius r is given by
S1(r) =

(i, j, k) ∈Z3 : (i, j, k −1) ∈S1(r −1) ∨(i, j, k −1) ∈F1(r)
∨(j = k ∧(i, j, k) ∈F1(r)) ∨(i, j, k) ∈B1(r)

(4)
Proof. Follows from Theorem 1, Observation 4, and our discussion before Obser-
vation 4.
⊓⊔
Clearly, a solid digital sphere, which is given by S∗(r) =

p ∈Z3 : s < r2 + λ

,
is the union of the solid digital sphere of radius r −1 and the space ﬁlling digital
sphere of radius r, i.e., S∗(r) = S∗(r −1) ∪S(r). During spherical propagation,
we increase the radius of the solid digital sphere continuously by unit increment
in each iteration. This is discussed in the forthcoming section.
3.1
Spherical Propagation
The algorithm for spherical propagation uses the characterization of ﬁller and
inter-octant boundary voxels to generate the successive space ﬁlling spheres in
a very eﬃcient manner. In this section, for simplicity, we discuss the algorithm
for building S∗(r) from S∗(r −1) without going into much detail. We resort to
a new notion of axis-parallel 2-neighborhood of length l deﬁned as follows.
N (2)(p, l) =

q ∈Z3 : (dx(p, q) + dy(p, q) + dz(p, q)) = d⊥(p, q) ⩽l

Here dx(p, q), dy(p, q), and dz(p, q) denote the axis-parallel distances between
points p and q along x-, y-, and z-axes and d⊥(p, q) denotes the isothetic distance.
From the analysis of the space ﬁlling digital sphere, it is easy to observe
that any new voxel to be added in S∗(r) falls in N (2)(p, 2) of some voxel p
lying on the surface of S∗(r −1). Also observe that the voxels lying on the
surface of S∗(r −1) are none other than the voxels of S(r −1) and can easily
be found by checking if any of the voxels lying in N (2)(p, 1) for voxel p is not
yet included in S∗(r). This neighborhood characterization readily simpliﬁes the
steps for spherical propagation, as shown in Algorithm 1.
Algorithm 1. Spherical Propagation
Input: S∗(r −1), r
Output: S∗(r)
1 A ←S∗(r −1), B ←φ
2 for (p ∈A) ∧(q ∈N (2)(p, 1)) ∧(q /∈A) do
3
B ←B ∪p
4 end
5 for (p ∈B) ∧(q ∈N (2)(p, 2)) ∧(q /∈A) do
6
if q ∈S(r) then
7
A ←A ∪q
8
end
9 end
10 return A

Circular and Spherical Propagation in Integer Space
355
3.2
Circular Propagation
Our characterization of space ﬁlling digital sphere can also be utilized for circular
propagation on 3D digital planes. Along with checking the membership of a voxel
in space ﬁlling digital sphere, we also check for its membership in the given digital
plane. This makes the neighborhood to be considered slightly larger, as now we
cannot say about surface voxels by just checking their 2-adjacent neighbors or
even newly added voxels does not maintain axis-parallel 2-neighborhood with
the existing voxels. Hence, we deﬁne the axis-parallel 0-neighborhood of a voxel
p, within an isothetic distance l from it, as follows.
N (0)(p, l) =

q ∈Z3 : d⊥(p, q) ⩽l

We utilize this neighborhood for propagating circularly from a given point.
Algorithm 2 takes a disc C∗(r −1) of radius r −1 on a given digital plane P and
produces C∗(r) by adding the voxels belonging to both P and S(r).
Algorithm 2. Circular Propagation
Input: C∗(r −1), r, P
Output: C∗(r)
1 A ←C∗(r −1), B ←φ
2 for (p ∈A) ∧(q ∈N (0)(p, 1)) ∧(q ∈P) ∧(q /∈A) do
3
B ←B ∪p
4 end
5 for (p ∈B) ∧(q ∈N (0)(p, 2)) ∧(q ∈P) ∧(q /∈A) do
6
if q ∈S(r) then
7
A ←A ∪q
8
end
9 end
10 return A
4
Test Result
In this section, we furnish some preliminary test results on both spherical prop-
agation in 3D space and circular propagation on 3D discrete plane. We use these
propagation techniques to generate digital Voronoi diagram from a given set of
seed points.
A Voronoi diagram (VD) is a partitioning of a space into regions based on
distance from a speciﬁc set of seed points as input. For each seed, there is a
corresponding region consisting of all points closer to that seed than to any other.
These regions are called Voronoi cells or Voronoi regions. For a given distance
metric d, the Voronoi region Ri corresponding to a seed pi (1 ⩽i ⩽n) can be
deﬁned as follows.
Ri = {q : d(q, pi) ⩽d(q, pj) ∀j = 1, 2, . . . , n}
(5)

356
S. Dwivedi et al.
)
b
(
)
a
(
)
d
(
)
c
(
Fig. 3. Spherical propagation for Voronoi diagram in a conﬁned 3D space. (a) Seed
points, (b) After iteration 3, (c) After iteration 7, (d) After iteration 10.
As known, Voronoi diagram (VD) in 2D and in 3D real spaces is a well-
researched topic in computational geometry [2,3], but there has not been any
signiﬁcant progress on this to date for 3D digital space. Some interesting work
have been reported on characteristics and digital-geometric properties of VD in
recent time on the 2D digital plane [12,19].

Circular and Spherical Propagation in Integer Space
357
)
b
(
)
a
(
)
d
(
)
c
(
Fig. 4. Circular propagation for Voronoi diagram on a discrete plane. (a) Seed points,
(b) After iteration 4, (c) After iteration 9, (d) After iteration 19.
We use Algorithms 1 and 2 to grow in parallel the Voronoi regions over all
seeds. The eﬀectiveness of the parallel and incremental region growing algorithm
can be utilized for generating the boundaries of the Voronoi regions where the
spheres or circles meet. Figure 3(b–d) shows the generation of Voronoi diagram
in a conﬁned 3D space by parallel spherical propagation from all the seed points
as given in Fig. 3(a). Similarly, in Fig. 4(a), a set of seed points is given on a 3D
discrete plane, and Fig. 4(b–d) show the circular propagation to form Voronoi
regions on the discrete plane.
5
Concluding Note
We have proposed a model of space ﬁlling digital sphere that can be used for
eﬃcient propagation of spherical or circular frontier in the digital space. Neces-
sary theoretical analysis and characterization have been provided with related

358
S. Dwivedi et al.
proofs. As an immediate application, we have also shown how this space ﬁlling
model of digital sphere can be used for construction of digital Voronoi diagrams
in 3D space or along a 3D plane. Naturally, this brings in several interesting
issues like digital convexity of the Voronoi regions, thus formed, which needs to
be addressed in future work.
On real-world applications, a very speciﬁc use of space ﬁlling digital sphere
can be related to discrete 3D terrains for solving various computational problems
related to geography information system. A suitable distance measure for con-
struction of well-deﬁned Voronoi diagram on an arbitrary digital surface, e.g., a
digital terrain whose underlying real surface is unknown, seems to be an inter-
esting and challenging task. The work presented in this paper can be used to
meet these challenges in future.
References
1. Andres, E., Jacob, M.-A.: The discrete analytical hyperspheres. IEEE TVCG 3,
75–86 (1997)
2. Aurenhammer, F.: Voronoi diagrams–a survey of a fundamental geometric data
structure. ACM Comput. Surv. 23, 345–405 (1991)
3. Aurenhammer, F., Klein, R., Lee, D.: Voronoi Diagrams and Delaunay Triangula-
tions. World Scientiﬁc, Singapore (2013)
4. Bera, S., Bhowmick, P., Bhattacharya, B.B.: A digital-geometric algorithm for
generating a complete spherical surface in Z3. In: Gupta, P., Zaroliagis, C. (eds.)
ICAA 2014. LNCS, vol. 8321, pp. 49–61. Springer, Cham (2014). doi:10.1007/
978-3-319-04126-1 5
5. Bera, S., Bhowmick, P., Bhattacharya, B.B.: On the characterization of absentee-
voxels in a spherical surface and volume of revolution in Z3. JMIV 56, 535–553
(2016)
6. Bera, S., Bhowmick, P., Stelldinger, P., Bhattacharya, B.B.: On covering a digital
disc with concentric circles in Z2. TCS 506, 1–16 (2013)
7. Biswas, R., Bhowmick, P.: Layer the sphere. Vis. Comput. 31, 787–797 (2015)
8. Biswas, R., Bhowmick, P.: From prima quadraginta octant to lattice sphere through
primitive integer operations. TCS 624, 56–72 (2016)
9. Biswas, R., Bhowmick, P.: On the functionality and usefulness of quadraginta
octants of naive sphere. JMIV (2017). doi:10.1007/s10851-017-0718-4
10. Biswas, R., Bhowmick, P., Brimkov, V.E.: On the connectivity and smoothness
of discrete spherical circles. In: Barneva, R.P., Bhattacharya, B.B., Brimkov, V.E.
(eds.) IWCIA 2015. LNCS, vol. 9448, pp. 86–100. Springer, Cham (2015). doi:10.
1007/978-3-319-26145-4 7
11. Brimkov, V.E.: Formulas for the number of (n −2)-gaps of binary objects in arbi-
trary dimension. DAM 157, 452–463 (2009)
12. Cao, T., Edelsbrunner, H., Tan, T.: Triangulations from topologically correct dig-
ital Voronoi diagrams. Comput. Geom. 48, 507–519 (2015)
13. Cohen-Or, D., Kaufman, A.: 3D line voxelization and connectivity control. IEEE
Comput. Graph 17, 80–87 (1997)
14. Gouraud, H.: Continuous shading of curved surfaces. IEEE Trans. Comput. 20,
623–629 (1971)

Circular and Spherical Propagation in Integer Space
359
15. Kaufman, A.: Eﬃcient algorithms for 3D scan-conversion of parametric curves,
surfaces, and volumes. SIGGRAPH 21, 171–179 (1987)
16. Klette, R., Rosenfeld, A.: Digital Geometry: Geometric Methods for Digital Picture
Analysis. Morgan Kaufmann, San Francisco (2004)
17. Roget, B., Sitaraman, J.: Wall distance search algorithm using voxelized marching
spheres. In: ICCFD 2012, pp. 1–23 (2012)
18. Roget, B., Sitaraman, J.: Wall distance search algorithm using voxelized marching
spheres. J. Comput. Phys. 241, 76–94 (2013)
19. Rong, G., Tan, T.: Jump ﬂooding in GPU with applications to Voronoi diagram
and distance transform. In: Symposium on International 3D Graphics & Games,
pp. 109–116 (2006)

Models for Discrete Geometry

Study on the Digitization Dual Combinatorics
and Convex Case
Lo¨ıc Mazo(B) and ´Etienne Baudrier
ICube-UMR 7357, 300 Bd S´ebastien Brant - CS 10413,
67412 Illkirch Cedex, France
loic.mazo@unistra.fr
Abstract. The action of a translation on a continuous object before its
digitization generates several digitizations. The dual, introduced by the
authors in a previous paper, stands for these digitizations in function
of the translation parameters. This paper focuses on the combinatorics
of the dual by making a link between the digitization number and the
boundary curve, especially through its dual representation. The convex
case is then studied and a few signiﬁcant examples are exhibited.
1
Introduction
For a given grid step and a given digitization method, a planar object produces
several digitizations in function of its position on the grid. The object digital
properties and digitally estimated characteristics depend on the obtained digi-
tization. Thereby, this study of the digitization variability is an important issue
in image analysis.
This ﬁeld has been explored for some geometrical primitives. For instance,
the set of straight segment digitizations in function of the segment slope and
oﬀset is known as the segment preimage and is used for digital straight segment
recognition [3]. Several papers are also dedicated to the study of the generation
and combinatorics of the disc digitization set in function of its radius and center
position [4,6–10,12–14] and the combinatorics of the strictly convex sets [5]. In
the general case, the digitization set can be seen as the consequence of a group
action on the object. A function, so-called dual, linking the group action and
the produced digitization is used by the authors to study the digitization set
up to a translation, for function graphs in [1] and for planar object in [11]. In
the latter case, the dual has been proved to be piecewise constant in function of
the translation. This paper focuses on the dual combinatorics.
Two upper bounds are given for the number of digitizations of a planar
object whose boundary is a Jordan curve. The ﬁrst one is expressed in terms of
the number of grid cells crossed by the boundary and the second one in terms of
the intersection number when plotting the boundary on the torus IR2/ZZ2. The
latter bound is proved to be quadratic in the convex case. Some examples are
provided in order to compare the two upper bounds both in the convex and the
non-convex cases. A conclusion and some perspectives end the paper.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 363–374, 2017.
DOI: 10.1007/978-3-319-66272-5 29

364
L. Mazo and ´E. Baudrier
2
Background
Let us consider a connected compact set S in IR2 hose boundary is a simple
closed (Jordan) curve Γ. Thanks to the Jordan curve theorem, we may assume
a continuous map f : IR2 →IR such that Γ and S are implicitly deﬁned by
Γ = {f(x) = 0 | x ∈IR2}
and
S = {x ∈IR2 | f(x) ≤0}.
We are interested in the variability of the Gauss digitization when the group
of the translations acts on S, that is on the sets (u + S) ∩ZZ2, u ∈IR2. In
this paper, we focus on the combinatorial aspects of this variance. Of course, the
variance has to be understood “up to integer translations”. This is the reason why
we deﬁned in a previous paper [11] the dual by translation1 of the digitizations
of S as a set-valued function ϕS deﬁned on the torus T = IR2/ZZ2 which maps
each point t ∈T to the digitization, up to integer translations, of u + S where
the vector u is any representative of t in IR2. Let us pick a representative in each
class of T so as to form a connected set M called the structuring element. We
note C the symmetric of M with respect to the origin: C = −M. The family of
sets p + C, p ∈ZZ2, is a tiling of IR2. For instance, we can take M = [0, 1)2,
C = (−1, 0]2. From now on, to simplify the notations, we identify t ∈T with its
representative in M and the subsets of ZZ2 with their orbits for the action of
the integer translations so we can write ϕS(t) = (t + S) ∩ZZ2. For any point or
set X in IR2, we denote by proj(X) its projection on the quotient space T. We
proved in [11] that the plot of the curve Γ on the torus T, proj(Γ), delineates
regions on which the dual function ϕS is constant.
We deﬁne the grid boundary B as the set of grid points that lie in the (mor-
phological) dilation of the boundary Γ of S by the structuring element M:
B = (Γ ⊕M) ∩ZZ2,
where ⊕denotes the Minkowski sum. The set B contains all the points of ZZ2
whose membership to the digitization may change when the set S is shifted by
a vector u ∈M. Nevertheless, since IR2 is connected, M is not open so there
may exist some points in B ∩S not liable to change, namely those points p in
B for which p + C ⊆S (see Fig. 1). That is why we have in fact to consider
the toggling boundary B as the set of grid points whose membership eﬀectively
toggle for some translation by a vector u ∈M:
B = B \ {p ∈ZZ2 | p + C ⊆S}.
The set S ∩ZZ2 \ B of the grid points that are in any digitization of u + S,
u ∈M, is called the digitization core.
1 The “dual” term is an analogy with the algebraic dual because our construction
transforms a set of binary images on a discrete set (ZZ2) in a labeled image on a set
of transformations (the translations).

Study on the Digitization Dual Combinatorics and Convex Case
365
Γp
p
r
q
Fig. 1. (a) A Jordan curve Γ and a set of tiles z + C, z ∈ZZ2 and C = (−1, 0]2. The
point p is in B for the square p + C intersects both Γ and ZZ2 \ S; the point q is in
B \ B for the square q + C intersects the boundary Γ but is included in S; the point r
is in the digitization core for the square r + C is included in S and does not intersect
Γ (b) Black: a Jordan curve, medium red discs: the toggling boundary, big blue discs:
two points that lie in the grid boundary but that cannot change their membership,
small green discs: the digitization core. (Color ﬁgure online)
Finally, for any p ∈ZZ2, we set
Γp = −p +

Γ ∩(p + C)

= (−p + Γ) ∩C,
so Γp = ∅if p /∈B, and we denote by 1lp the indicator function of the set Γp, so
1lp is not constant iﬀp ∈B. Then,
ϕS(t) = {p ∈ZZ2 | 1lp(−t) = 1}.
In the rest of the article, we use the notation |E| for the cardinal of a set E
(|E| ∈IN∪{∞}), CC(A) for the family of the connected components of a subset
A of IR2 and I ⊔J for the disjoint union of the sets I and J.
In the following section, upper bounds are given for the number of digitiza-
tions up to a translation. The ﬁrst upper bound is naive. For the second one,
the idea is to link the digitization number to the number of the intersection of
the translated curves Γp. Indeed, the dual induces a partition of the torus whose
cells frontiers are arcs of the curve proj(Γ). Then, we bound from above the
partition size by counting the number of curve intersections in proj(Γ).
3
Contribution
3.1
Bounding up by Counting the Crossed Tiles
Since the grid boundary B contains any integer point whose value may change
when shifting the set S, we have a ﬁrst, obvious, upper bound on the number
of Gauss digitizations given by 2|B|. As |B| is also the number of tiles p + C,
p ∈ZZ2, crossed by the frontier of S, we can state the following proposition.

366
L. Mazo and ´E. Baudrier
Proposition 1. The number of Gauss digitizations, up to integer translations,
is upper bounded by 2 a(Γ ) where a(Γ) is the number of tiles crossed by Γ.
Generally, the digitization enumeration provided by Proposition 1 includes
false positives and multiple counts. For instance, the grid boundary of a circle
with diameter 1.7 involves 4 to 8 pixels, depending on the grid position, which
gives an upper bound according to Proposition 1 equals to 16, while there exists
only 8 digitizations (see Fig. 2). Nevertheless, from any set, it is possible to build
a new set that avoids false positives by replacing the initial boundary by a family
of Hilbert curves and it is possible to extend the set to prevent multiple counts
so that the theoretical upper bound 2B is obtained (see Appendix B).
Fig. 2. (a) A circle with diameter 1.7 and the four pixels in the toggling boundary
(which is equal to the grid boundary): a, b, c, d. (b) The (ﬂatten) dual of the closed
disc bounded by the circle. Among the 16 = 24 potential digitizations provided by
the 4-pixels a, b, c, d, three of them does not actually appear (∅, ac and bd) and
some others appear multiple times (the four singular digitizations, which are obviously
congruent, and the vertical and horizontal pairs which each appear twice. Eventually,
there are only 8 digitizations (and actually the (sewed) dual is divided into 8 regions).
3.2
Bounding up by Counting the Intersections
In Sect. 3.2, we assume a parametrization of Γ. It induces an order on the points
of the curve Γ (for Γ is simple) that is used in the proof of the following propo-
sition.
The dual ϕS can be regarded as the projection, on the torus T of a ﬁnite
labeled partition of the tile C (whose cells need not be connected). For the
order by reﬁnement on the partitions, this partition is lower bounded by the
inﬁmum of the binary partitions Pp associated to the indicator functions 1lp,
p ∈B (actually, proj(1lp), p ∈B). Indeed some cells of 
p∈B Pp need to be
merged whenever the corresponding digitizations are equivalent up to an inte-
ger translation. Then, the next proposition proposes an upper bound on the
size of the partition 
p∈B Pp—and thereby an upper bound to the number of
digitizations—by inductively counting the intersections between the curves Γp,
p ∈B. The idea is to count the partition cells created when adding a curve
Γp. To do this we count the intersection of Γp with the already added curves.

Study on the Digitization Dual Combinatorics and Convex Case
367
Nevertheless, such an intersection can be with one or more curves and can be a
singleton, a set of several points or an arc. Then, let us specify how we handle
intersections in this context (the deﬁnition is illustrated in Fig. 3).
Deﬁnition 1. Let B = {b1, · · · , bi, · · · , bn}, n ≥1. Let m ∈[2, n]. Then,
interγ,I = CC

γ ∩
 
i∈I
Γbi \

j∈J
Γbj

where γ ∈CC(Γbm) and I ⊔J = [1, m −1],
# interm =
	
γ∈CC(Γbm)
∅⊂I⊆[1,m−1]
wI |interγ,I|
where wI = min(|I| , 2).
Note that the components of all the interγ,I are two by two disjoint and that
# interm may be inﬁnite.
The set interγ,I stands for the intersection of the curve γ with the curves
whose indexes are in a subset I, excluding any other curve Γbi whose index is
not in I. The necessity to have all the index subsets comes from the fact that
intersections between multiple curves can occur.
Fig. 3. (a) Blue: Γb1. (b) Green: Γb2; # inter2 =
interΓb2 ,{1}
 = 4. (c) Brown: Γb3;
# inter3 =
interΓb3 ,{1}
 +
interΓb3 ,{2}
 + 2
interΓb3 ,{1,2}
 = 0 + 0 + 2 = 2. (d) Red:
Γb4; # inter4 =
interΓb4 ,{1}
 + 2
interΓb4 ,{1,2,3}
 = 2 + 2 = 4. (Color ﬁgure online)
We are now able to state the proposition that relates the number of digiti-
zations and the number of intersections # interm.
Proposition 2. The size of the partition 
p∈B Pp is upper bounded by
2 +
n
	
m=2
# interm + |CC(Γbm)| .
(1)
Proof. The proposition is proved by induction. For m = 1, the result is
obvious since, for any p
∈
B, Pp is a binary partition. Let m
>
1.

368
L. Mazo and ´E. Baudrier
We assume that the number of cells of the partition m−1
i=1 Pbi is upper bounded
by 2 + 
m−1
k=2 # interk + |CC(Γbk)|. The cells of m−1
i=1 Pbi which are included in
one of the two cells of Pbm, namely the sets (1lbm = 0) and (1lbm = 1), stay
unchanged in the partition m
i=1 Pbi. Conversely, the cells of m−1
i=1 Pbi that are
intersected by both (1lbm = 0) and (1lbm = 1), which obviously are cells inter-
sected by Γbm, are each divided in two new cells. Thereby, the number Nm of
new cells is upper bounded by N ′
m, the number of cells in m−1
i=1 Pbi intersected
by Γbm. Besides, the partition m−1
i=1 Pbi of the tile C induces a partition Q
of Γbm, as a subset of C, with N ′
m cells (these cells of Γbm need not be con-
nected). Then, the idea of the proof is to map each cell of Q to its supremum,
for the order induced by the parametrization of Γ—namely to the intersection,
as deﬁned in Deﬁnition 1—its supremum belongs to, or to the empty set when
no such intersection exists. Then, though this mapping is not one-to-one, a care-
ful examination of the diﬀerent cases will permit to conclude that N ′
m is upper
bounded by # interm + |CC(Γbm)|.
Let γ be a connected component of Γbm and s∞be the supremum of γ for
the order induced by the parametrization of Γ. Let Qγ be the restriction of Q
to γ. We set interγ = 
∅⊂I⊆[1,m−1] interγ,I, and we assume the following facts
that will be prove further:
(a) each component in interγ is included in a cell of Qγ;
(b) if a cell of Qγ has a supremum s distinct from s∞, then s belongs to some
component of interγ;
(c) if two cells of Qγ have their supremums in the same component K of interγ,
then these supremums are equal to the inﬁmum of K.
(d) if three cells of Qγ have the same supremum s, then interγ has inﬁnitely
many components;
(e) if two cells of Qγ have the same supremum s and interm is ﬁnite then either
s = s∞and s∞belongs to some component of interγ, or s belongs to at
least two curves Γbi and Γbj where i, j < m.
Then, with these ﬁve assumptions, we deduce the desired upper bound as follows.
We map each cell of Qγ to the component of interγ its supremum belongs to, if
any. The other cells are map on the empty set. We denote by ψ this mapping.
Hence, from Fact b,the supremum of the preimages by ψ of ∅, if any, is s∞
and so, s∞/∈interγ if such preimages exist. If a component K of interγ has
exactly two preimages K1 and K2 by the mapping ψ, then from Fact c, we
derive that the two cells share the same supremum and from Fact e, and the
very deﬁnition of # interm, we see that the weight of K in # interm is 2, which
corresponds to the number of its preimages, or s = s∞and the empty set has no
preimage. If the empty set has two or more preimages by ψ, then # interm = ∞
(Fact e). If a component K of interγ has three or more preimages by ψ, these
preimages share the same supremum (Fact c) and from Fact d we derive that
# interm = ∞. We readily conclude that the number of cells of Qγ is upper
bounded by 1 + 
∅⊂I⊆[1,m−1] wI |interγ,I| where wI = min(2, |I|). By summing
on all the connected components of Γbm, we derive that N ′
m, the number of cells
of Q, is upper bounded by # interm + |CC(Γbm)| which achieves the proof.

Study on the Digitization Dual Combinatorics and Convex Case
369
Let us now prove the ﬁve facts stated above.
(a) We demonstrate Fact a by contradiction. Suppose it exists K in interγ,I, I ⊆
[1, m −1], and two distinct cells K1, K2 of m−1
i=1 Pbi such that K ∩K1 ̸= ∅
and K ∩K2 ̸= ∅. Let c1 ∈K ∩K1 and c2 ∈K ∩K2. Since K1 ̸= K2, there
exists i ∈[1, m −1] such that 1lbi(c1) ̸= 1lbi(c2). Then, the segment [c1, c2]
in γ contains a point c3 ∈Γbi which also belongs to K for K is connected.
Then, on the one hand i ∈I for c3 ∈Γbi ∩K and, on the other hand, c1, or
c2, is in (1lbi = 0), that is i /∈I. Contradiction.
(b) Suppose that the supremum s of the cell K of Qγ is not in any component
of interm. Then, for any i ∈[1, m −1], s /∈Γbi. In other words, for any
i ∈[1, m −1], s is in the open set (1lbi = 0) or in the interior of (1lbi = 1).
Thus, there exists an open neighborhood U of s in the tile C which does
not intersect any curve Γbi, 1 ≤i < m. Thereby, U is included in a cell of
the partition m−1
i=1 Pbi. Since s = sup(K), there is no point t in γ ∩U such
that t > s. Thus, s = s∞.
(c) Let K1, K2 be two cells of Qγ whose supremums are in K ∈interγ. Let K3 be
the cell of Qγ which includes this component (Fact a). If s is not the inﬁmum
of K then there exists an interval [u, s], with u < s, in K ⊆K3 (for K is
connected) and s is not the supremum of both K1 and K2. Contradiction.
(d) Let K1, K2, K3 be three cells of Qγ that share the same supremum s. Since
s belongs to at most one of these three cells, it is a limit point for the two
others. For instance, assume that s is a limit point for K1 and K2. Let
k ∈[1, m −1] such that 1lbk takes two distinct values on K1 and K2. For
instance, K1 ⊆(1lbk = 0) and K2 ⊆(1lbk = 1). Then, we can inductively
build an inﬁnite sequence c1 < c2 < · · · < ci < · · · < s such that c2i−1 ∈K1
and K2i ∈K2 for any i ≥1. In particular, we have 1lbk(c2i−1) = 0 and
1lbk(c2i) = 1 for any i ≥1. Then, c2i−1 /∈Γbk and [c2i−1, c2∗i] intersects Γbk.
Therefore, interγ contains inﬁnitely may components between c1 and s.
(e) We assume # interm < ∞. Let K1, K2 be two cells of Qγ that share the
same supremum s. This supremum cannot be a limit point both in K1 and
K2 otherwise we could make the same reasoning as in the previous item
and concludes that # interm = ∞which contradicts our assumption. So,
s is an isolated point in one of the two cells, for instance K1 (therefore,
s ∈K1, s /∈K2 and s is a limit point in K2). From Fact b, there exist a
subset I of [1, m −1] and a component K in interγ,I such that s ∈K. As
in the proof of Fact b, we derive that there exists an open neighborhood
U of s in C which does not intersect any curve Γbi, i /∈I. Suppose that I
is a singleton, say I = {b1}. Then, taking a point in K2 ∩U (recall that
s is a limit point of K2), we found that necessarily 1lb1 = 0 on K2, while
1lb1 = 1 on K1, and 1lbi, i ̸= 1, coincides on K1 and K2. If s ̸= s∞, there
exists a point u in γ ∩U greater than s. This point is not in K1 nor in K2
for s = sup(K1) = sup(K2). Thereby, 1lb1(u) ̸= 0 and 1lb1(u) ̸= 1 which is
absurd. Thus, either s∞= s ∈K1 or I is not a singleton.
⊓⊔
In the simplest case where the curve segments Γbm all have one connected
component and where the intersections are sets of points belonging to at most

370
L. Mazo and ´E. Baudrier
two curves, Formula 1 reduces to 1 + |B| + I where I is the number of inter-
section points between the curve segments Γbm. Furthermore, when projecting
the partition n
i=1 Pbi on the torus T, the cells that touch the boundary of C
are identiﬁed two by two, which decreases the number of cells in the partition.
Unfortunately, it is diﬃcult to count these cells in the general case.
3.3
The Convex Case
It seems plain that the structure of the dual should be simpler when the set S is
convex compared to a set with a winding boundary. We could even hope that the
digitizations coincide with the cells of the partition 
p∈B Γp, after sewing the
boundary of the tile C, and with the connected regions of the torus T delineated
by the curve Γ. Figure 4 annihilates this hope by exhibiting a convex object and
one of its digitizations whose inverse image by the dual is not connected.
Fig. 4. (a) A triangle and its toggling boundary. (b) The dual of the (ﬁlled) triangle.
The four colored regions correspond to the same digitization: the horizontal pair. Red
region: cell bc of the partition 
p∈B Γp. The red region is disconnected by the unique
point of the cell abcd. Orange region: cell cd. Pink region: cell ae. (c) The conﬁgura-
tions of the shifted triangle in the two red components. Since they are disconnected, it
is not possible to continuously move to one conﬁguration to the other without hurting
a black toggling point. Note that the triangle could be slightly inﬂated so as to provide
a strictly convex counterexample. (Color ﬁgure online)
Nevertheless, in this section we show that the structure of the sets interγ,I
is simple when the set S is strictly convex and permits to obtain a quadratic
bound for the complexity of the dual in terms of the grid boundary size (to be
compare to the exponential bound of Proposition 1).
Proposition 3. We assume a convex quadrilateral structuring element M. Let
n = |B|. The number of Gauss digitizations of a strictly convex planar object
up to a translation is upper bounded by
4n2 + 4n −6.

Study on the Digitization Dual Combinatorics and Convex Case
371
Proof. The proof invokes Lemma 3 which is stated and proved in the appendix.
Let N be the digitization number. Thanks to Proposition 2, we have
N ≤2 +
n
	
m=2
# interm + |CC(Γbm)| .
(2)
From Lemma 3, we derive that, for any m ∈[1, n], γ ∈CC(Γbm), i ∈[1, m −1],
	
I∋i
|interγ,I| ≤2 |CC(Γbi)| .
Then,
# interm ≤2
m−1
	
k=1
|CC(Γbk)| .
Eventually, Eq. (2) turns into
N ≤2 + 2
n
	
m=2
m−1
	
k=1
|CC(Γbk)| +
n
	
m=2
|CC(Γbm)|
≤2

1 +
n
	
k=1
(n −k + 1) |CC(Γbk)| −|CC(Γb1)|

.
It can easily be seen (from Lemma 2 for instance) that, for any convex curve Γ
and any convex polygon P with c edges, the number of connected arcs in the
intersection Γ ∩P is upper bounded by c. Then, if we assume that the tile C is
a convex quadrilateral, we straightforwardly obtain the desired bound.
⊓⊔
The term 4n2 in Proposition 3 comes down to n2 for suﬃciently high reso-
lutions because each curve Γbi then have just one connected component instead
of possibly 4 in the general case. Then, when S a disc of radius r, the result
of Proposition 3 is close to the one in [7] which states that the number of dig-
itizations of the disc is asymptotically 4πr2 + O(r). As the ratio between the
radius r and the size of the grid boundary is π/4 for the disc, our upper bound
in function of the radius r of the disc is asymptotically equivalent to (16/π2)r2.
4
Conclusion
We present in this paper two upper bounds on the number of digitizations
obtained from all the translated of a continuous object. The ﬁrst one is expo-
nential in the number of toggling object-boundary pixels and a generic example
reaching this bound is given. The second one is based on the passage from the
dual connected-component count to the curve intersection count which, if the
curve is parameterized, comes down to count some equation solutions. In the
convex case, the latter upper bound is shown to yield a quadratic digitization
number in term of the grid boundary size. An example of a convex object is

372
L. Mazo and ´E. Baudrier
given where the set of translation parameter classes corresponding to a given
digitization is not connected.
The perspectives of this work are ﬁrst to explicit the second upper bound
under assumptions less restrictive that the convexity, e.g. bounded curvature;
then to study the combinatorics of the digitization under the rigid transforma-
tions and to propose an algorithm for the digitization generation.
Acknowledgements. We thank Renan Lauretti for his idea to link the dual regions
to the dual region border crossings in his study of the function graph dual.
Appendix
A
Convex Sets
The proof of Lemma 3 relies on the two following lemmas about convex sets that
seem obvious at the ﬁrst glance. Nevertheless, since we did not ﬁnd any result
related to these lemmas in the literature, we provide our own justiﬁcations of
the two statements.
Lemma 1 (Chords of convex sets). Let [a, b] be a chord of the boundary Γ
of a closed convex set S. If [a, b] ⊈Γ, then (ab)∩S = [a, b] and (ab)∩Γ = {a, b}.
Proof. Since [a, b] ⊈Γ, the line (ab) does not support S at any point (the notion
of supporting line of a convex set is exposed for instance in [2]). So, there exists
two supporting lines of S at a and b that cross the line (ab). Then, (ab) ∩S is
included in [a, b]. Let c ∈[a, b] ∩Γ, c ̸= a. Applying the ﬁrst part of the proof
to the chord [a, c], we derive that (ac) ∩S ⊆[a, c] and, since b ∈(ac) ∩S, we
conclude that b = c.
⊓⊔
Lemma 2 (Cuts of convex sets). Let a, b be two points of the boundary Γ
of a closed convex set S. If [a, b] ⊈Γ, then the open curve segments of Γ, σ1, σ2,
whose extremities are a and b are included in distinct open half-planes bounded
by the line (ab).
Proof. Let H−and H+ be the two open half-planes bounded by (ab). Since
[a, b] ⊈Γ, from Lemma 1, (ab) ∩Γ = {a, b}. Thereby, by connectivity, either
σ1 ⊂H−or σ1 ⊂H+ and σ2 ⊂H−or σ2 ⊂H+. Suppose for instance that
σ1 ⊂H−and σ2 ⊂H−. Then, S, which is the connected subset of IR2 bounded
by σ1 ∪σ2 ∪{a, b} is included in H−∪(ab) and, since (ab) ∩S = [a, b] from
Lemma 1, [a, b] ⊂Γ: contradiction.
⊓⊔
Lemma 3 (Intersection of two segments of a convex curve).
Let Γ
be a Jordan curve whose interior is convex. Let Γ1 and Γ2 two disjoint closed
segments of the curve Γ and τ a vector of IR2. Then the intersection of Γ1 and
τ + Γ2 is composed of none, one, two points or a line segment.

Study on the Digitization Dual Combinatorics and Convex Case
373
Proof. Let p, q be two distinct points in Γ1 ∩(τ + Γ2) if such a pair exists.
We denote by Σ1 the open segment of Γ1 between p and q. Alike, Σ2 is the
open segment of Γ2 between −τ + p and −τ + q. We set Σ1 = Σ1 ∪{p, q} and
Σ2 = Σ2 ∪{p, q}. Firstly, we prove that Σ1 ∪(τ + Σ2) is a straight line segment
whenever it contains more than two points. First case: Σ1 ∪(τ + Σ2) ⊆(pq).
Then, since Σ1 and Σ2 are connected and Γ is simple, Σ1 = Σ2 = [p, q]. Second
case: ∃x ∈

Σ1 ∪(τ + Σ2)

\ (pq). For instance, we assume x ∈Σ1 \ (pq). By
Lemma 2, Σ1 = Γ ∩H1, where H1 is the open half-plane bounded by the line
(pq) and containing x, and −τ +p is in IR2 \H1. Then, it can easily be seen that
−τ +H1 is the open half-plane bounded by the line joining −τ +p and −τ +q and
including p. Thanks to Lemma 2, we derive that Σ2 does not intersect −τ + H1.
Thus, τ + Σ2 does not intersect H1. In particular, (τ + Σ2) ∩Σ1 = ∅. This
achieves the ﬁrst part of the proof. Now, let r be a point in Γ1 ∩(τ + Γ2) which
is not in Σ1 (if such a point exists). For instance, p belongs to the segment of Γ1
between q and r. Then, the ﬁrst part of the proof, applied to the points q and
r, implies that Γ1 ∩(τ + Γ2) includes the segments [q, r]. We straightforwardly
concludes that either the intersection of Γ1 and τ + Γ2 is composed of at most
two points or it is a line segment.
⊓⊔
B
Examples and Counterexamples
B.1
Building Examples Without Proper Congruent Digitizations in
the Image of the Dual
Let u and v be two vectors in [0, 1)2 such that the sets u+S and v+S have distinct
but congruent digitizations. Then, there exists an integer vector w, w ̸= 0, such
that (u + S) ∩ZZ2 = w + ((v + S) ∩ZZ2) = (w + v + S) ∩ZZ2. Let p be a point in
the digitization core. Then, p ∈(u + S) ∩ZZ2 and p ∈(v + S) ∩ZZ2. Therefore,
w + p ∈(u + S) ∩ZZ2 and −w + p ∈(v + S) ∩ZZ2, which can be rewritten as
p ∈((−w + u) + S) ∩ZZ2 and p ∈((w + v) + S) ∩ZZ2. Then, at least one of
the vectors w + v or −w + u has one of its coordinates which is negative. We
derive that if there exists a point in the digitization core which is maximal in S
for both coordinates then there is no proper congruent digitizations in the dual.
B.2
Building Toric Partitions in One-to-one Correspondence with
the Power Set of the Toggling Boundary
In this section, we exhibit a way to modify the boundary of the set S in order to
ensure that any subset of the toggling boundary is represented in the dual. To
do so, we move along B, ordered in the same way as in Deﬁnition 1. Then, a new
boundary is built thanks to the approximations of the Hilbert ﬁlling curve: the
segment of Γ intersecting the n-th cell of B is replaced by a n-th approximation of
the Hilbert ﬁlling curve Hn (extended at its extremities to ensure the continuity
of the boundary). We consider the family of binary partitions Pn of the unit
square that comes with the curves Hn. We claim that each curve Hn+1 crosses

374
L. Mazo and ´E. Baudrier
each cell of the partition n
i=1 Pi so that the size of the ﬁnal torus partition is
2N where N is the cardinal of the toggling boundary. To justify our claim, we
divide the unit square in a family of 2n ×2n small squares (Kn
i,j)1≤i,j≤2n (n ≥0)
whose sizes are
1
2n ×
1
2n . It can be seen that on the one hand, the Hilbert curve
Hn passes through the center of each of the squares Kn
i,j and, on the other hand,
does not intersect any of the interior of the squares Kn+1
i,j
(H0 is just the center
of the unit square). Thereby, the partition n
i=1 Pi is coarser than the partition
{Kn+1
i,j
| 1 ≤i, j < n} (the boundaries of the squares Kk
i,j are assigned to the
cells so as to coincide with Pn). Since Hn+1 passes through the center of each
of the squares Kn+1
i,j , it passes in each cell of n
i=1 Pi which gives the claim.
References
1. Baudrier, ´E., Mazo, L.: Curve digitization variability. In: Normand, N., Gu´edon,
J., Autrusseau, F. (eds.) DGCI 2016. LNCS, vol. 9647, pp. 59–70. Springer, Cham
(2016). doi:10.1007/978-3-319-32360-2 5
2. Berger, M.: Geometry, vol. 1 (1987)
3. Dorst, L., Smeulders, A.W.M.: Discrete Representation of Straight Lines. IEEE
Trans. Pattern Anal. Mach. Intell. 6(4), 450–463 (1984). http://ieeexplore.ieee.org/
lpdocs/epic03/wrapper.htm?arnumber=4767550
4. Heath-Brown, D.: Lattice points in the sphere. Number Theory Prog. 2, 883–892
(1997)
5. Huxley, M.N.: The number of conﬁgurations in lattice point counting I. Forum
Mathematicum 22(1), 127–152 (2010)
6. Huxley, M.N., ˇZuni´c, J.: On the number of digitizations of a disc depending on
its position. In: Klette, R., ˇZuni´c, J. (eds.) IWCIA 2004. LNCS, vol. 3322, pp.
219–231. Springer, Heidelberg (2004). doi:10.1007/978-3-540-30503-3 17
7. Huxley, M.N., ˇZuni´c, J.D.: Diﬀerent digitisations of displaced discs. Found Comput.
Math. 6(2), 255–268 (2006). doi:10.1007/s10208-005-0177-y
8. Huxley, M.N., Zuni´c, J.D.: The number of N-point digital discs. IEEE Trans. Pat-
tern Anal. Mach. Intell. 29(1), 159–161 (2007). doi:10.1109/TPAMI.2007.250606
9. Huxley, M.N., Zuni´c, J.D.: The number of diﬀerent digital N-discs. J. Math. Imag-
ing Vis. 56(3), 403–408 (2016). doi:10.1007/s10851-016-0643-y
10. Mazo, J.E., Odlyzko, A.M.: Lattice points in high-dimensional spheres. Monat-
shefte f¨ur Mathematik 110(1), 47–61 (1990)
11. Mazo, L., Baudrier, ´E.: Object digitization up to a translation, September 2016.
preprint https://hal.archives-ouvertes.fr/hal-01384377
12. Nagy, B.: An algorithm to ﬁnd the number of the digitizations of discs with a ﬁxed
radius. Electron. Notes Discr. Math. 20, 607–622 (2005). doi:10.1016/j.endm.2005.
04.006
13. ˇZuni´c, J.D.: On the number of digital discs. J. Math. Imaging Vis. 21(3), 199–204
(2004). doi:10.1023/B:JMIV.0000043736.15525.ed
14. ˇZuni´c, J.D.: On the number of ways to occupy n lattice points by balls in d-
dimensional space. J. Number Theor. 110(2), 396–402 (2004)

Algorithmic Construction of Acyclic Partial
Matchings for Multidimensional Persistence
Madjid Allili1(B), Tomasz Kaczynski2, Claudia Landi3, and Filippo Masoni3
1 Department of Computer Science, Bishop’s University,
Sherbrooke, QC J1M1Z7, Canada
mallili@ubishops.ca
2 Department of Mathematics, Universit´e de Sherbrooke,
Sherbrooke, QC J1K2R1, Canada
t.kaczynski@usherbrooke.ca
3 Dipartimento di Scienze e Metodi dell’Ingegneria,
Universit`a di Modena e Reggio Emilia, Reggio Emilia, Italy
claudia.landi@unimore.it
Abstract. Given a simplicial complex and a vector-valued function on
its vertices, we present an algorithmic construction of an acyclic par-
tial matching on the cells of the complex. This construction is used to
build a reduced ﬁltered complex with the same multidimensional persis-
tent homology as of the original one ﬁltered by the sublevel sets of the
function. A number of numerical experiments show a substantial rate of
reduction in the number of cells achieved by the algorithm.
Keywords: Multidimensional persistent homology · Discrete Morse
theory · Acyclic partial matchings · Matching algorithm · Reduced
complex
1
Introduction
Persistent homology has been established as an important tool for the topological
analysis of discrete data. However, its eﬀective computation remains a challenge
due to the huge size of complexes built from data. Some recent works focussed
on algorithms that reduce the original complexes generated from data to much
smaller cellular complexes, homotopically equivalent to the initial ones by means
of acyclic partial matchings of discrete Morse theory.
Although algorithms computing acyclic partial matchings have primarily
been used for persistence of one-dimensional ﬁltrations, see e.g. [11,13,16], there
is currently a strong interest in combining persistence information coming from
multiple functions in multiscale problems, e.g. in biological applications [20],
which motivates extensions to generalized types of persistence. The extension
T. Kaczynski—This work was partially supported by NSERC Canada Discovery
Grant and IMA Minnesota.
C. Landi—Work performed under the auspices of INdAM-GNSAGA.
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 375–387, 2017.
DOI: 10.1007/978-3-319-66272-5 30

376
M. Allili et al.
of persistent homology to multiﬁltrations is studied in [5]. Other related direc-
tions are explored e.g. by the authors of [19] who do statistics on a set of one-
dimensional persistence diagrams varied as coordinate system rotates, and in [8],
where persistence modules on quiver complexes are studied.
Our attempt parallel to [8] is [2], where an algorithm given by King et al.
in [11] is extended to multiﬁltrations. The algorithm produces a partition of the
initial complex into three sets (A, B, C) and an acyclic partial matching m : A →B.
Any simplex which is not matched is added to C and deﬁned as critical. The
matching algorithm of [2] is used for reducing a simplicial complex to a smaller
one by elimination of matched simplices. First experiments with ﬁltrations of
triangular meshes show that there is a considerable amount of cells identiﬁed by
the algorithm as critical but which seem to be spurious, in the sense that they
appear in clusters of adjacent critical faces which do not seem to carry signiﬁcant
topological information.
The aim of this paper is to improve our previous matching method for opti-
mality, in the sense of reducing the number of spurious critical cells. Our new
matching algorithm extends the one given in [16] for cubical complexes, which
processes lower stars rather than lower links, and improves the result of [2] for
optimality. Next, the new matching algorithm presented here emerges from the
observation that, in the multidimensional setting, it is not enough to look at
lower stars of vertices: one should take into consideration the lower stars of
simplices of all dimensions, as there may be vertices of a simplex which are not
comparable in the partial order of the multiﬁltration. The vector-valued function
initially given on vertices of a complex is ﬁrst extended to simplices of all dimen-
sions. Then the algorithm processes the lower stars of all simplices, not only the
vertices. The resulting acyclic partial matching is used as in [2] to construct a
reduced ﬁltered Lefschetz complex with the same multidimensional persistent
homology as the original simplicial complex ﬁltered by the sublevel sets of the
function. Our reduction is derived from the works of [10,13,14]. A recent related
work on reduction techniques can be found among others in [6].
The paper is organized as follows. In Sect. 2, the preliminaries are introduced.
In Sect. 3, the main Algorithm 2 is presented and its correctness is discussed. At
the section end, the complementing reduction method is recalled from [2]. In
Sect. 4 experiments on synthetic and real 3D data are presented. In Sect. 5, we
comment on open questions and prospects for future work.
2
Preliminaries
Let K be a ﬁnite geometric simplicial complex, that is a ﬁnite set composed
of vertices, edges, triangles, and their q-dimensional counterparts, called sim-
plices. A q-dimensional simplex is the convex hull of aﬃnely independent ver-
tices v0, . . . vq ∈Rn and is denoted by σ = [v0, . . . vq]. The set of q-simplices of
K is denoted by Kq. A face of a q-simplex σ ∈K is a simplex τ whose vertices
constitute a subset of {v0, v1, . . . , vq}. If dim τ = q −1, it is called a facet of σ.
In this case, σ is called a cofacet of τ, and we write τ < σ.

Algorithmic Construction of Acyclic Partial Matchings
377
A partial matching (A, B, C, m) on K is a partition of K into three sets A, B, C
together with a bijective map m : A →B, also called discrete vector ﬁeld, such
that, for each τ ∈A, m(τ) is a cofacet of τ. The intuition behind is that pro-
jection from τ to the complementing part of the boundary of m(τ) induces
a homotopy equivalence between K and a smaller complex. An m-path is a
sequence σ0, τ0, σ1, τ1, . . . , σp, τp, σp+1 such that, for each i = 0, . . . , p, σi+1 ̸= σi,
τi = m(σi), and τi is a cofacet of σi+1.
A partial matching (A, B, C, m) on K is called acyclic if there does not exist a
closed m-path, that is an m-path such that, σp+1 = σ0.
The main goal of this paper is to produce an acyclic partial matching which
preserves the ﬁltration of a simplicial complex K by sublevel sets of a vector-
valued function f : K0 →Rk given on the set of vertices of K. We assume
that f : K0 →Rk is a function which is component-wise injective, that is,
whose components fi are injective. This assumption is used in [3] for proving
correctness of the algorithm. Given any function ˜f : K0 →Rk, we can obtain
a component-wise injective function f which is arbitrarily close to ˜f via the
following procedure. Let n denote the cardinality of K0. For i = 1, . . . , k, let
us set ηi = min{| ˜fi(v) −˜fi(w)| : v, w ∈K0 ∧˜fi(v) ̸= ˜fi(w)}. For each i with
1 ≤i ≤k, we can assume that the n vertices in K0 are indexed by an integer
index j, with 1 ≤j ≤n, increasing with ˜fi. Thus, the function fi : K0 →R
can be deﬁned by setting fi(vj) = ˜fi(vj) + jηi/ns, with s ≥1 (the larger s, the
closer f to ˜f). Finally, it is suﬃcient to set f = (f1, f2, . . . , fk). We extend f to
a function f : K →Rk as follows:
f(σ) = (f1(σ), . . . , fk(σ))
with
fi(σ) =
max
v∈K0(σ) fi(v).
(1)
Any function f : K →Rk that is an extension of a component-wise injective
function f : K0 →Rk deﬁned on the vertices of the complex K in such a way
that f satisﬁes Eq. (1) will be called admissible. In Rk we consider the following
partial order. Given two values a = (ai), b = (bi) ∈Rk we set a ⪯b if and only
if ai ≤bi for every i with 1 ≤i ≤k. Moreover we write a ⪵b whenever a ⪯b
and a ̸= b. The sublevel set ﬁltration of K induced by an admissible function f
is the family {Ka}a∈Rk of subsets of K deﬁned as follows:
Ka = {σ = [v0, v1, . . . , vq] ∈K | f(vi) ⪯a, i = 0, . . . , q}.
It is clear that, for any parameter value a ∈Rk and any simplex σ ∈Ka, all
faces of σ are also in Ka. Thus Ka is a simplical subcomplex of K for each a. The
changes of topology of Ka as we change the multiparameter a permit recognizing
some features of the shape of |K| if f is appropriately chosen. For this reason, the
function f is called in the literature a measuring function or, more speciﬁcally, a
multidimensional measuring function [4]. The lower star of a simplex is the set
L(σ) = {α ∈K | σ ⊆α
and
f(α) ⪯f(σ)},
and the reduced lower stars is the set L∗(σ) = L(σ) \ {σ}.

378
M. Allili et al.
2.1
Indexing Map
An indexing map on the simplices of the complex K of cardinality N, compatible
with an admissible function f, is a bijective map I : K →{1, 2, . . . , N} such that,
for each σ, τ ∈K with σ ̸= τ, if σ ⊆τ or f(σ) ⪵f(τ) then I(σ) < I(τ).
To build an indexing map I on the simplices of the complex K, we will revisit
the algorithm introduced in [2] that uses the topological sorting of a Directed
Acyclic Graph (DAG) to build an indexing for vertices of a complex that is
compatible with the ordering of values of a given function deﬁned on the vertices.
We will extend the algorithm to build an indexing for all cells of a complex that
is compatible with both the ordering of values of a given admissible function
deﬁned on the cells and the ordering of the dimensions of the cells. We recall
that a topological sorting of a directed graph is a linear ordering of its nodes
such that for every directed edge (u, v) from node u to node v, u precedes v in
the ordering. This ordering is possible if and only if the graph has no directed
cycles, that is, if it is a DAG. A simple well known algorithm (see [17]) for this
task consists of successively ﬁnding nodes of the DAG that have no incoming
edges and placing them in a list for the ﬁnal sorting. Note that at least one such
node must exist in a DAG, otherwise the graph must have at least one directed
cycle.
Algorithm 1. Topological sorting
1: Input: A DAG whose list of nodes with no incoming edges is I
2: Output: The list L containing the sorted nodes
3: while there are nodes remaining in I do
4:
remove a node u from I
5:
add u to L
6:
for each node v with an edge e from u to v do
7:
remove edge e from the DAG
8:
if v has no other incoming edges then
9:
insert v into I
10:
end if
11:
end for
12: end while
When the graph is a DAG, there exists at least one solution for the sorting
problem, which is not necessarily unique. We can easily see that each node and
each edge of the DAG is visited once by the algorithm, therefore its running
time is linear in the number of nodes plus the number of edges in the DAG. The
following lemma whose proof is based on Algorithm 1 is proved in [3].
Lemma 1. Let f : K →Rk be an admissible function. There exists an injective
function I : K →N such that, for each σ, τ ∈K with σ ̸= τ, if σ ⊆τ or
f(σ) ⪵f(τ) then I(σ) < I(τ).

Algorithmic Construction of Acyclic Partial Matchings
379
3
Matching Algorithm
The main contribution of this paper is the Matching Algorithm 2. It uses as input
a ﬁnite simplicial complex K of cardinality N, an admissible function f : K →Rk
built from a component-wise injective function f : K0 →Rk using the extension
formula given in Eq. (1), and an indexing map I compatible with f. It can be
precomputed using the topological sorting Algorithm 1. Given a simplex σ, we
use unclass facetsσ(α) to denote the set of facets of a simplex α that are in
L(σ) and have not been classiﬁed yet, that is, not inserted in either A, B, or C, and
num unclass facetsσ(α) to denote the cardinality of unclass facetsσ(α). We
initialize classified(σ) = false for every σ ∈K. We use priority queues PQzero
and PQone which store candidates for pairings with zero and one unclassiﬁed
facets respectively in the order given by I. We initialize both as empty sets. The
algorithm processes cells in the increasing order of their indexes. Each cell σ can
be set to the states of classified(σ) = true or classified(σ) = false so that
if it is processed as part of a lower star of another cell it is not processed again by
the algorithm. The algorithm makes use of extra routines to calculate the cells
in the lower star L(σ) and the set of unclassiﬁed facets unclass facetsσ(α) of
α in L∗(σ) for each cell σ ∈K and each cell α ∈L∗(σ).
The goal of the process is to build a partition of K into three lists A, B, and C
where C is the list of critical cells and in which each cell in A is paired in a one-to-
one manner with a cell in B which deﬁnes a bijective map m : A →B. When a cell
σ is considered, each cell in its lower star L(σ) is processed exactly once as shown
in [3]. The cell σ is inserted into the list of critical cells C if L∗(σ) = ∅. Otherwise,
σ is paired with the cofacet δ ∈L∗(σ) that has minimal index value I(δ). The
algorithm makes additional pairings which can be interpreted topologically as the
process of constructing L∗(σ) with simple homotopy expansions or the process of
reducing L∗(σ) with simple homotopy contractions. When no pairing is possible
a cell is classiﬁed as critical and the process is continued from that cell. A cell α is
candidate for a pairing when unclass facetsσ(α) contains exactly one element
λ that belongs to PQzero. For this purpose, the priority queues PQzero and PQone
which store cells with zero and one available unclassiﬁed faces respectively are
created. As long as PQone is not empty, its front is popped and either inserted
into PQzero or paired with its single available unclassiﬁed face. When PQone
becomes empty, the front cell of PQzero is declared as critical and inserted in C.
We illustrate the algorithm by a simple example. We use the simplicial com-
plex S from our ﬁrst paper [2, Figure 2] to compare the outputs of the previous
matching algorithm and the new one. Figure 1(a) displays S and the output of
[2, Algorithm 6]. The coordinates of vertices are the values of the function con-
sidered in [2]. Since that function is not component-wise injective, we denote it
by ˜f and we start from constructing a component-wise injective approximation
f discussed at the beginning of Sect. 2. If we interpret the passage from ˜f to
f as a displacement of the coordinates of vertices, the new complex K is illus-
trated by Fig. 1(b). The partial order relation is preserved when passing from
˜f to f, and the indexing of vertices in [2, Figure 2] may be kept for f. Hence,
it is easy to see that [2, Algorithm 6] applied to K gives the same result as

380
M. Allili et al.
Algorithm 2. Matching
1: Input: A ﬁnite simplicial complex K with an admissible function f : K →Rk and
an indexing map I : K →{1, 2, . . . , N} on its simplices compatible with f.
2: Output: Three lists A, B, C of simplices of K, and a function m : A →B.
3: for i = 1 to N do
4:
σ := I−1(i)
5:
if classified(σ)=false then
6:
if L∗(σ) contains no cells then
7:
add σ to C, classified(σ)=true
8:
else
9:
δ := the cofacet in L∗(σ) of minimal index I(δ)
10:
add σ to A and δ to B and deﬁne m(σ) = δ, classified(σ)=true,
classified(δ)=true
11:
add all α ∈L∗(σ) −{δ} with num unclass facetsσ(α) = 0 to PQzero
12:
add all α ∈L∗(σ) with num unclass facetsσ(α) = 1 and α > δ to PQone
13:
while PQone ̸= ∅or PQzero ̸= ∅do
14:
while PQone ̸= ∅do
15:
α := PQone.pop front
16:
if num unclass facetsσ(α) = 0 then
17:
add α to PQzero
18:
else
19:
add λ ∈unclass facetsσ(α) to A, add α to B and deﬁne m(λ) = α,
classified(α)=true, classified(λ)=true
20:
remove λ from PQzero
21:
add all β ∈L∗(σ) with num unclass facetsσ(β) = 1 and either
β > α or β > λ to PQone
22:
end if
23:
end while
24:
if PQzero ̸= ∅then
25:
γ := PQzero.pop front
26:
add γ to C, classified(γ)=true
27:
add all τ ∈L∗(σ) with num unclass facetsσ(τ) = 1 and τ > γ to
PQone
28:
end if
29:
end while
30:
end if
31:
end if
32: end for
that displayed in Fig. 1(a). In order to apply our new Algorithm 2, we need to
index all 14 simplices of K. For convenience of presentation, we label the vertices
wi, edges ei, and triangles ti by the index values i = 1, 2, ..., 14. The result is
displayed in Fig. 1(b). The sequence of vertices (v0, v1, v2, v3, v4) is replaced by
(w1, w2, w4, w8, w12). Here are the main steps of the algorithm:

Algorithmic Construction of Acyclic Partial Matchings
381
i = 1 L∗(w1) = ∅, w1 ∈C
i = 2 L∗(w2) = {e3}, m(w2) = e3
i = 3 e3 classiﬁed
i = 4 L∗(w4) = {e5, e6, t7},
m(w4) = e5,
e6 ∈PQzero, t7 ∈PQone,
line 15, α = t7 ̸∈PQone,
line 19, λ = e6, m(e6) = t7,
e6 ̸∈PQzero.
5, 6, 7 e5, e6, t7 classiﬁed
i = 8
L∗(w8) = {e9, e10, t11},
m(w8) = e9,
e10 ∈PQzero, e11 ∈PQone,
line 15, α = t11 /∈PQone,
line 19, λ = e10, m(e10) = t11,
e10 ̸∈PQzero.
9, 10, 11 e9, e10, t11 classiﬁed
i = 12
L∗(w12) = {e13, e14},
m(w12) = e13,
e14 ∈PQzero, PQone = ∅,
Line 25, γ = e14 ∈C.
13, 14
e13, e14 classiﬁed
The output is displayed in Fig. 1(b).
v0 = (0, 0)
v1 = (1, 0)
v2 = (1, 1)
v3 = (2, 0)
v4 = (2, 1)
w1
w2
w4
w12
w8
e3
e5
e6
e9
e10
e13
e14
t7
t11
(a)
(b)
Fig. 1. In (a), the complex and output of Algorithm 6 of [2] are displayed. Gray-shaded
triangles are those which are present in the simplicial complex. Critical simplices are
marked by red circles and the matched simplices are marked by arrows. In (b), the
complex is modiﬁed so to satisfy the coordinate-wise injectivity assumption. Labeling
of all simplices by the indexing function and the output of Algorithm 2 are displayed.
(Color ﬁgure online)
3.1
Correctness
The full and detailed proof of correctness of Algorithm 2 is given in [3]. We
present here the main steps of the proof. Recall that f = (f1, . . . , fk) : K →Rk
is an admissible function. We start with some preparatory results that allow to
make a proof by induction that every cell of the complex is classiﬁed exactly
once by the algorithm and the algorithm produces eﬀectively an acyclic partial
matching of the complex.
– First, we note that lower stars of distinct simplices are not necessarily disjoint.
However when two lower stars meet, they get automatically classiﬁed at the
same time by the algorithm (see [3, Lemma 3.1]).

382
M. Allili et al.
– If a cell σ is unclassiﬁed when Algorithm 2 reaches line 5, then either L∗(σ) is
empty for which case σ is classiﬁed as critical. Alternatively, L∗(σ) contains at
least one cofacet of σ that has σ as a unique facet in L(σ). Then σ is paired
with the cofacet with minimal index and the remainder of its cofacets in
L∗(σ), have no unclassiﬁed faces in L∗(σ) and hence they must enter PQzero
at line 11 of Algorithm 2. Moreover, if for every cell α with I(α) < I(σ),
L(α) consists only of classiﬁed cells, then all cells in L(σ) are also unclassiﬁed
(see [3, Lemmas 3.2, 3.4]).
– Finally, we prove by induction on the index of the cells that every cell is
classiﬁed in a unique fashion by the algorithm. The proof is simple when the
index takes values 1 and 2 since the cells can be only vertices or edges (see [3,
Lemma 3.3]). For the general index, we ﬁrst prove that for any cell popped
from PQone, its unique unclassiﬁed facet belongs to PQzero and the two cells
get paired at line 19 of the algorithm (see [3, Lemma 3.5]). Afterwards, we
state that each cell that is still unclassiﬁed after line 10 of the algorithm ulti-
mately enters PQone or PQzero and gets classiﬁed. This requires an argument
based on the dimension of the cell and the number of its unclassiﬁed faces in
the considered lower star. Moreover, we prove that if a cell is already clas-
siﬁed, it cannot be considered again for classiﬁcation or enter the structures
PQone or PQzero (see [3, Lemma 3.6]).
– The correctness proof is concluded by proving that the algorithm produces
an acyclic partial matching of the complex (see [3, Proposition 3.7 and The-
orem 3.8]).
3.2
Filtration Preserving Reductions
Lefchetz complexes introduced by Lefschetz in [12] are developed further in [14]
under the name S-complex. In our context, these complexes are produced by
applying the reduction method [10,13,14] to an initial simplicial complex K,
with the use of the matchings produced by our main Algorithm 2. Both concepts
of partial matchings and sublevel set ﬁltration of K induced by f : K →Rk
introduced in Sect. 2 naturally extend to Lefschetz complexes as proved in [2].
Persistence is based on analyzing the homological changes occurring along the
ﬁltration as the multiparameter a ∈Rk varies. This analysis is carried out by
considering, for a ⪯b, the homomorphism H∗(j(a,b)) : H∗(Sa) →H∗(Sb) induced
by the inclusion map j(a,b) : Sa 	→Sb. The image of the map Hq(j(a,b)) is known
as the q-th multidimensional persistent homology group of the ﬁltration at (a, b)
and we denote it by Ha,b
q (S). It contains the homology classes of order q born
not later than a and still alive at b.
If we assume that (A, B, C, m) is an acyclic matching on a ﬁltered Lefschetz
complex S obtained from the original simplicial complex K by reduction, the fol-
lowing result holds which asserts that the multidimensional persistent homology
of the reduced complex is the same as of the initial complex (see [2]).
Corollary 2. For every a ⪯b ∈Rk, Ha,b
∗(C) ∼= Ha,b
∗(K).

Algorithmic Construction of Acyclic Partial Matchings
383
3.3
Complexity Analysis
Given a simplex σ ∈K, the coboundary cells of σ are given by cb (σ) := {τ ∈
K | σ is a face of τ}. It is immediate from the deﬁnitions that L∗(σ) ⊂cb (σ).
We deﬁne the coboundary mass γ of K as γ = maxσ∈K card cb (σ), where card
denotes cardinality. While γ is trivially bounded by N, the number of cells in K,
this upper bound is a gross estimate of γ for many complexes of manifolds and
approximating surface boundaries of objects. For the simplicial complex K, we
assume that the boundary and coboundary cells of each simplex are computed
oﬄine and stored in such a way that access to every cell is done in constant
time. Given an admissible function f : K →Rk, the values by f of simplices
σ ∈K are stored in the structure that stores the complex K in such a way that
they are accessed in constant time. We assume that adding cells to the lists
A, B, and C is done in constant time. Algorithm 2 processes every cell σ of the
simplicial complex K and checks whether it is classiﬁed or not. In the latter case,
the algorithm requires a function that returns the cells in the reduced lower star
L∗(σ) which is read directly from the structure storing the complex. In the best
case, L∗(σ) is empty and the cell is declared critical. Since L∗(σ) ⊂cb (σ), it
follows that card L∗(σ) ≤γ. From Algorithm 2, we can see that every cell in
L∗(σ) enters at most once in PQzero and PQone. It follows that the while loops
in the algorithm are executed all together in at most 2γ steps. We may consider
the operations such as ﬁnding the number of unclassiﬁed faces of a cell to have
constant time except for the priority queue operations which are logarithmic in
the size of the priority queue when implemented using heaps. Since the sizes of
PQzero and PQone are clearly bounded by γ, it follows that L∗(σ) is processed in
at most O(γ log γ) steps. Therefore processing the whole complex incurs a worst
case cost of O(N · γ log γ).
4
Experimental Results
We have successfully applied the algorithms from Sect. 3 to diﬀerent sets of
triangle meshes. In each case the input data is a 2-dimensional simplicial complex
K and a function f deﬁned on the vertices of K with values in R2. The ﬁrst step
is to slightly perturb f in order to achieve injectivity on each component as
described in Sect. 2. The second step is to construct an index function deﬁned
on all the simplices of the complex and satisfying the properties of Lemma 1.
Then we build the acyclic matching m and the partition (A, B, C) in the simplices
of the complex using Algorithm 2. In particular, the number of simplices in C
out of the total number of simplices of K is relevant, because it determines the
amount of reduction obtained by our algorithm to speed up the computation of
multidimensional persistent homology.
4.1
Examples on Synthetic Data
We consider two well known 2-dimensional manifolds — the sphere and the
torus. For the sphere, we consider its triangulations of ﬁve diﬀerent sizes and we

384
M. Allili et al.
Table 1. Reduction performance on diﬀerent triangulations of a sphere and torus.
sphe 1 sphe 2 sphe 3 sphe 4 sphe 5 tor 96 tor 4608 tor 7200
#K 38
242
962
1538
2882
96
4608
7200
#C
4
20
98
178
278
8
128
156
%
10.53
8.26
10.19
11.57
9.65
8.33
2.78
2.17
take f(x, y, z) = (x, y). The triangulated sphere with the maximum number of
simplices is shown in Fig. 2(left), where cells found critical by the algorithm are
colored. The comparison with other triangulations of the sphere is shown in the
Table 1: the ﬁrst row shows the number of simplices in each considered mesh K;
the middle row shows the number of critical cells obtained by using our matching
algorithm to reduce K; the bottom row shows the ratio between the second and
the ﬁrst lines, expressing them in percentage points. In the case of the torus, we
again consider triangulations of diﬀerent sizes and we take f(x, y, z) = (x, y). The
numerical results are shown in the same table and also displayed in Fig. 2(right).
Fig. 2. A triangulation a sphere with 2882 simplices and one of the torus with 7200 sim-
plices, with respect to a component-wise injective perturbation of the function deﬁned
of its vertices by f(x, y, z) = (x, y). Critical vertices are in yellow, critical edges in blue,
and critical triangles in red. (Color ﬁgure online)
In conclusion, our experiments on synthetic data conﬁrm that the current
simplex-based matching algorithm scales well with the size of the complex.
4.2
Examples on Real Data
We consider four triangle meshes (available at [1]). For each mesh the input
2-dimensional measuring function f takes each vertex v of coordinates (x, y, z)
to the pair f(v) = (|x|, |y|). In Table 2, the ﬁrst row shows on the top line
the number of vertices in each considered mesh, and in the middle line the
reduction ratio achieved by our algorithm on those cells, expressing them in
percentage points. Finally, it also displays in the bottom line the analogous ratio
achieved by our previous algorithm [2]. The second and the third rows show

Algorithmic Construction of Acyclic Partial Matchings
385
Table 2. Percentage of reduction achieved by Algorithm 2 on some natural triangle
meshes compared to that of [2].
Dataset tie
space shuttle x wing space station
#K0
2014
2376
3099
5749
%
27.5
9.5
19.8
30.8
% [2]
11.3
5.1
5.6
32.7
#K1
5944
6330
9190
15949
%
20.1
3.8
13.4
16.0
%[2]
56.2
58.4
39.2
70.0
#K2
3827
3952
6076
10237
%
14.1
0.4
9.9
8.0
%[2]
78.7
90.5
56.2
56.2
#K
11785 12658
18365
31935
%
19.4
3.8
13.3
16.1
% [2]
55.9
58.4
39.2
70.0
similar information for the edges and the faces. Finally, the fourth row show the
same information for the total number of cells of each considered mesh K.
Our experiments conﬁrm that the current simplex-based matching algorithms
produce a fair rate of reduction for simplices of any dimension also on real
data. In particular, it shows a clear improvement with respect to the analogous
result presented in [2] and obtained using a vertex-based and recursive matching
algorithm.
4.3
Discussion
The experiments on synthetic data conﬁrmed two aspects: (1) The discrete case
seems to behave much as the diﬀerentiable case for two functions [18] because
critical cells are still localized along curves; (2) The number of critical cells scales
well with the total number of cells, indicating that we are not detecting too many
spurious critical cells.
We should point here a fundamental diﬀerence between Morse theory for
one function whose critical points are isolated and extensions of Morse theory
to vector-valued functions where, even in the generic case, critical points form
stratiﬁed submanifolds. For example, for two functions on a surface, they form
curves. Hence the topological complexity depends not on the number of critical
points but on the number of such curves. As a consequence, the ﬁner the tri-
angulation, the ﬁner the discretization of such curves and the larger number of
critical cells we get.
On the other hand, experiments on real data show the improvement with
respect to our previous algorithm [2] already observed in the toy example of

386
M. Allili et al.
Fig. 1. We think that the new algorithm performs better because it is simplex-
based rather than vertex-based. So, the presence of many non-comparable ver-
tices has a limited impact on it.
5
Conclusion
The point of this paper is the presentation of Algorithm 2 to construct an acyclic
partial matching from which a gradient compatible with multiple functions can
be obtained. As such, it can be useful for speciﬁc purposes such as multidi-
mensional persistence computation, whereas it is not meant to be a competitive
algorithm to construct an acyclic partial matching for general purposes.
Some questions remain open. First, since indexing map is not unique, one
may ask what is the eﬀect of its choice on the output. We believe that the size of
the resulting complex should be independent. This is a subject for future work.
A deeper open problem arises from the fact that the optimality of reductions
is not yet well deﬁned in the multidimensional setting, although the improve-
ment is observed in practice. As commented earlier, even in the classical smooth
case, the singularities of vector-valued functions on manifolds are not isolated.
An appropriate application-driven extension of the Morse theory to multidimen-
sional functions is not much investigated yet. Some related work is that of [15]
on Jacobi sets and of [7] on preimages of maps between manifolds. However
there are essential diﬀerences between those concepts and our sublevel sets with
respect to the partial order relation.
The experiments presented in this paper show an improvement with respect
to the algorithm in [2] that, to the best of our knowledge, is still the only other
algorithm available for this task. Such experiments were obtained with a non-
optimized implementation. For an optimized implementation of it, we defer the
reader to [9], where experiments on larger data sets can be found.
References
1. The GTS Library. http://gts.sourceforge.net/samples.html
2. Allili, M., Kaczynski, T., Landi, C.: Reducing complexes in multidimensional per-
sistent homology theory. J. Symbolic Comput. 78, 61–75 (2017)
3. Allili, M., Kaczynski, T., Landi, C., Masoni, F.: A new matching algorithm for
multidimensional persistence. arXiv:1511.05427v3
4. Biasotti, S., Cerri, A., Frosini, P., Giorgi, D., Landi, C.: Multidimensional size
functions for shape comparison. J. Math. Imaging Vis. 32(2), 161–179 (2008)
5. Carlsson, G., Zomorodian, A.: The theory of multidimensional persistence. In:
Proceedings of the 23rd Annual Symposium on Computational Geometry (SCG
2007), pp. 184–193, New York, NY, USA. ACM (2007)
6. Dlotko, P., Wagner, H.: Simpliﬁcation of complexes for persistent homology com-
putations. Homology Homotopy Appl. 16(1), 49–63 (2014)
7. Edelsbrunner, H., Harer, J.: Jacobi sets of multiple Morse functions. In: Founda-
tions of Computational Mathematics (FoCM 2002), Minneapolis, pp. 37–57. Cam-
bridge Univ. Press, Cambridge (2002)

Algorithmic Construction of Acyclic Partial Matchings
387
8. Escolar, E.G., Hiraoka, Y.: Computing persistence modules on commutative lad-
ders of ﬁnite type. In: Hong, H., Yap, C. (eds.) ICMS 2014. LNCS, vol. 8592, pp.
144–151. Springer, Heidelberg (2014). doi:10.1007/978-3-662-44199-2 25
9. Iuricich, F., Scaramuccia, S., Landi, C., De Floriani, L.: A discrete morse-based
approach to multivariate data analysis. In: SIGGRAPH ASIA 2016 Symposium on
Visualization (SA 2016), New York, NY, USA, pp. 5:1–5:8. ACM (2016)
10. Kaczynski, T., Mrozek, M., Slusarek, M.: Homology computation by reduction of
chain complexes. Comput. Math. Appl. 35(4), 59–70 (1998)
11. King, H., Knudson, K., Mramor, N.: Generating discrete Morse functions from
point data. Exp. Math. 14(4), 435–444 (2005)
12. Lefschetz, S.: Algebraic Topology, vol. 27. American Mathematical Society, Provi-
dence (1942). Colloquium Publications
13. Mischaikow, K., Nanda, V.: Morse theory for ﬁltrations and eﬃcient computation
of persistent homology. Discrete Comput. Geometry 50(2), 330–353 (2013)
14. Mrozek, M., Batko, B.: Coreduction homology algorithm. Discrete Comput. Geom-
etry 41, 96–118 (2009)
15. Patel, A.: Reeb spaces and the robustness of preimages. Ph.D. thesis, Duke Uni-
versity (2010)
16. Robins, V., Wood, P.J., Sheppard, A.P.: Theory and algorithms for constructing
discrete Morse complexes from grayscale digital images. IEEE Trans. Pattern Anal.
Mach. Intell. 33(8), 1646–1658 (2011)
17. Sedgewick, R., Wayne, K.: Algorithms. Addison-Wesley, Boston (2011)
18. Smale, S.: Global analysis and economics. I. Pareto optimum and a generalization
of Morse theory. In: Dynamical systems (Proceedings of Symposium, Univ. Bahia,
Salvador, 1971), pp. 531–544. Academic Press, New York (1973)
19. Turner, K., Murkherjee, A., Boyer, D.M.: Persistent homology transform for mod-
eling shapes ans surfaces. Inf. Inference 3(4), 310–344 (2014)
20. Xia, K., Wei, G.-W.: Multidimensional persistence in biomolecular data. J. Com-
put. Chem. 36(20), 1502–1520 (2015)

Digital Primitives Deﬁned by Weighted
Focal Set
Eric Andres1(B), Ranita Biswas2, and Partha Bhowmick3
1 Universit´e de Poitiers, Laboratoire XLIM, ASALI, UMR CNRS 7252,
Futuroscope, BP 30179, 86962 Chasseneuil, France
eric.andres@univ-poitiers.fr
2 Department of Computer Science and Engineering,
Indian Institute of Technology, Roorkee, India
biswas.ranita@gmail.com
3 Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur, India
bhowmick@gmail.com
Abstract. This papers introduces a deﬁnition of digital primitives based
on focal points and weighted distances (with positive weights). The pro-
posed deﬁnition is applicable to general dimensions and covers in its
gamut various regular curves and surfaces like circles, ellipses, digital
spheres and hyperspheres, ellipsoids and k-ellipsoids, Cartesian k-ovals,
etc. Several interesting properties are presented for this class of digital
primitives such as space partitioning, topological separation, and connec-
tivity properties. To demonstrate further the potential of this new way
of deﬁning digital primitives, we propose, as extension, another class of
digital conics deﬁned by focus-directrix combination.
Keywords: Digital primitive · Focus · Hypersphere · Ellipse ·
Ellipsoid · k-ellipse · Cartesian oval · Conic
1
Introduction
In this paper we introduce digital primitives deﬁned by a weighted focal set.
Continuous geometric objects deﬁned by foci have been well studied but nothing,
to our knowledge, has been proposed so far in digital geometry.
The word focus is the Latin word for ﬁreplace. This comes from classical
experiment which consists in converging the sunlight, in the focal point of a
lens, on a piece of paper to ignite it. Focal points play a fundamental role in
the geometry of lenses and study of lenses played an important role in the early
development of mathematical physics. The historic importance of the research
in optics can even be traced in our common language with expressions such as
“staying focused”.
Classically a foci based continuous geometric object is deﬁned as all the
points such that the sum of the distances to the foci is a constant. The distances
may have diﬀerent weights in an even more general deﬁnition [9]. In this paper
c
⃝Springer International Publishing AG 2017
W.G. Kropatsch et al. (Eds.): DGCI 2017, LNCS 10502, pp. 388–398, 2017.
DOI: 10.1007/978-3-319-66272-5 31

Digital Primitives Deﬁned by Weighted Focal Set
389
we introduce foci deﬁned digital geometric primitives. The deﬁnition we propose
covers digital objects with multiple foci, in arbitrary dimension. Our deﬁnition
includes weighted distances with positive weights. Contrary to the continuous
deﬁnition, in the deﬁnition that we propose, the weighted sum of the distances
to the foci is not a constant but lies in an interval. This deﬁnition generalizes the
Andres digital hyperspheres [1,2] that has only one focal point. We show that
with a well deﬁned interval, we can prove some important topological properties
for our digital objects such as (n −1)-separation and (n −2)-connectivity prop-
erties. With an appropriate sequence of such intervals, it is easy to see that we
can provide a space partition by such foci based digital primitives.
In a second part of the paper, we propose two extensions. Firstly, we pro-
pose an immediate extension of the deﬁnition that allows to deﬁne m-separating
digital foci based primitives. As a second extension, we propose a new type of
digital conics whose deﬁnition is based on a focal point and a directrix. Again,
it is possible to show that we have topological separation properties. This illus-
trates that the exploration on the possibilities provided by this new approach
oﬀers many opportunities for further research.
After this introduction and basic notions and notations, we introduce in
Sect. 2.1, our deﬁnition of foci based digital objects and propose several funda-
mental properties of such objects. In Sect. 3, we propose two types of extensions:
foci and directrix deﬁned digital objects with properties, and boundary foci
based digital objects with properties. We conclude and discuss perspectives in
Sect. 4.
Basic Notions and Notations
Let {e1, . . . , en} denote the canonical basis of the n-dimensional Euclidean vector
space. Let Zn be the subset of Rn that consists of all the integer points. A digital
(resp. Euclidean) point is an element of Zn (resp. Rn). We denote by xi the i-th
coordinate of a point or a vector x, that is its coordinate associated to ei. We
denote by ci the i-th element of a list or sequence C. A digital (resp. Euclidean)
object is a set of digital (resp. Euclidean) points. When not otherwise stated,
the distance we are considering in this paper is the Euclidean distance d(·) with
d(p, q) =
n
i=1(pi −qi)2 for p, q ∈Rn.
For all k ∈{0, . . . , n−1}, two integer points p and q are said to be k-adjacent
or k-neighbors, if for all i ∈{1, . . . , n}, |pi−qi| ≤1 and n
j=1 |pj −qj| ≤n−k. In
the 2-dimensional plane, the 0- and 1-neighborhood notations correspond respec-
tively to the classical 8- and 4-neighborhood notations. In the 3-dimensional
space, the 0-, 1- and 2-neighborhood notations correspond respectively to the
classical 26-, 18- and 6-neighborhood notations [8].
A k-path is a sequence of integer points such that every two consecutive points
in the sequence are k-adjacent. A digital object C is k-connected if there exists a
k-path in C between any two points of C. Let us suppose that the complement of
a digital object E, Zn\E admits a set of k-connected components C, or in other
words that there exists no k-path joining integer points of any two connected
components of the set C then E is said to be k-separating, or k-tunnel free, in Zn.

390
E. Andres et al.
If there is no path from any two connected components of the set C then E is
said to be 0-separating or simply separating.
2
Foci Based Digital Primitives
2.1
Deﬁnition
The classical continuous primitives that are deﬁned by a set of focal points can
be summarized by the following deﬁnition.
Deﬁnition 1. A foci based continuous nD primitive is deﬁned as all the con-
tinuous points:

x ∈Rn :
k

i=1
αid (x, fi) = r

where f is the list of foci f = (f1, . . . , fk) with fi ∈Rn, αi ∈R∗+ the weight of
the distance to focus fi, and r ∈R the generalized radius.
This deﬁnition covers hyperspheres (with one only focus traditionally called
the center of the hypersphere), ellipsoids and k-ellipsoids (with respectively two
and k foci, and all weights equal to unity), Cartesian ovals and k-Cartesian ovals
(with respectively two and k focal points and arbitrary non-zero weights).
We are now going to introduce a digital version of this deﬁnition with
some restrictions. Firstly, let us note that a digital primitive is generally not
deﬁned mathematically by an implicit equation {p ∈Zn : g(p) = 0} because
there is usually no particular reason for an integer point to lie on the con-
tinuous curve g(x) = 0. Instead, J.-P. Reveill´es [3,7] proposed to deﬁne a
digital line as the digital points in a band deﬁned by a thickness interval

p ∈Z2 : 0 ≤ap1 + bp2 + c < ω

. This captures the general idea that digital
primitives are based on grid points where neighborhoods are deﬁned by points
that are at a certain, non-zero, distance from each other. To deﬁne topologically
sound objects, this distance between neighboring points has to be part of the
digital deﬁnition. This idea of deﬁning primitives as points with an interval has
been extended by E. Andres to circles and hyperspheres [1,2] with deﬁnitions
based on annulus in 2D, concentric hyperspheres in nD. We now propose a new
extension for foci based primitives.
Deﬁnition 2. A foci based digital nD primitive Fn
k (f, α, r) is deﬁned as all the
integer points verifying
F n
k (f, α, r) =

p ∈Zn :
 k

i=1
αi
 
r −1
2

≤
k

i=1
αid (p, fi) <
 k

i=1
αi
 
r + 1
2

where, f is the list of foci f = (f1, . . . , fk) with fi ∈Rn, αi ∈R∗+ the weight
of the distance to focus fi, and r ∈R the generalized radius.

Digital Primitives Deﬁned by Weighted Focal Set
391
Let us note that Deﬁnition 2 is slightly less general than Deﬁnition 1, since
we are considering only strictly positive weights for the digital primitives. This
restriction comes from the fact that we are seeking digital primitives with some
speciﬁc topological properties (see Sect. 2.2) that do not stand with this def-
inition if some weights are taken negative. Let us note that if the general
radius is too small then the digital object might be empty. For instance, if
k
i=1
	
r + 1
2

< mink
i=1
k
j=1 d (fi, fj)

, then the digital primitive will be
empty.
The Andres digital hypersphere [1,2] of center c and radius r has been deﬁned
as the set H(c, r):
H(c, r) =

p ∈Zn :

r −1
2
2
≤
n

i=1
(pi −ci)2 <

r + 1
2
2
It is easy to see that H(c, r) = Fn
1 ((c) , (1), r) (see Fig. 1). A Andres digital
hypersphere is a foci based digital primitive with one focal point, classically
called the center of the hypersphere.
Fig. 1. Andres circle F 2
1 (((0.1, 0.2)) , (1), 3.5) and Andres sphere F 3
1 (((0.1, 0.2, 0.3)),
(1), 3.5)
Deﬁnition 2 deﬁnes a new type of 2D digital ellipse, the foci based digital
ellipse F2
2((f1, f2), (1, 1), r) with two focal points and equal weights of 1 (or
any strictly positive equal weights) and a new type of foci based digital ellipsoid
En
2 ((f1, f2), (1, 1), r) (see Fig. 2). Contrary to some classical or more recent digital
ellipse deﬁnitions [4–6], the digital ellipses are not limited to axis aligned ellipses.
Another major property of this deﬁnition is that it is dimension independent.
With Deﬁnition 2, we propose the ﬁrst deﬁnition of a digital k-ellipse and
k-ellipsoid: Fn
k ((f1, . . . , fk) , (1, . . . , 1), r) (see Fig. 3). And lastly, it allows to
deﬁne foci based digital Cartesian ovals with weighted distances F2
k(F, α, r)
(See Fig. 4). As we can see, this simple deﬁnition allows to deﬁne a wide range
of new types of digital objects.

392
E. Andres et al.
Fig. 2. Foci based digital ellipse F 2
2 (((0.1, 0.2), (5.1, 3.1)) , (1, 1), 4.5) and ellipsoid
F3
2 (((0.1, 0.2, 0.3), (5.1, 3.1, 1.1)) , (1, 1), 4.5)
Fig. 3.
3-ellipse
F 2
3 (((0.1, 0.3), (2.1, 2.3), (−2.1, 4.3)) , (1, 1, 1), 3.5)
and
3-ellipsoid
F3
3 (((0.1, 0.3, 0.5), (2.1, 2.3, −2.5), (−2.1, 4.3, −5.5)) , (1, 1, 1), 5.5)
Fig. 4.
Cartesian
oval
F 2
3 (((0.1, 0.3), (2.1, 2.3), (−2.1, 4.3)) , (2, 0.5, 0.5), 3.5)
and
F3
3 (((0.1, 0.3, 0.5), (2.1, 2.3, −2.5), (−2.1, 4.3, −5.5)) , (2, 0.5, 0.5), 5.5)

Digital Primitives Deﬁned by Weighted Focal Set
393
2.2
Properties
Let us have a look now at the properties of such digital objects. As we will
see in what follows, the foci based digital primitives have interesting structural
properties:
Theorem 1. A foci based digital nD primitive Fn
k (f, α, r) is (n −1)-separating
in Zn.
Let us, for the sake of simplicity of language, call the Euclidean region(s)
deﬁned by k
i=1 αid (x, fi) <
k
i=1 αi
 	
r −1
2

the interior of the digital object
and the region(s) deﬁned by k
i=1 αid (x, fi) ≥
k
i=1 αi
 	
r + 1
2

the outside.
Let us note that nothing in Deﬁnition 2 requires the inside or outside regions
to be composed of a singular connected component. A good example is given by
hyperbolic type curves that divide space into three regions and not two.
Proof. Let us consider two digital points a and b and E = Fn
k (f, α, r) a foci based
digital nD object such that a is inside E and b outside E: k
i=1 αid (a, fi) <
k
i=1 αi
 	
r −1
2

and k
i=1 αid (b, fi) ≥
k
i=1 αi
 	
r + 1
2

.
Here we suppose that the inside of E contains at least one digital point.
Since k
i=1 αid (a, fi) <
k
i=1 αi
 	
r −1
2

, we have −k
i=1 αid (a, fi) ≥
−
k
i=1 αi
 	
r −1
2

+ ϵ with ϵ a strictly positive real value.
This means that k
i=1 αi (d (b, fi) −d (a, fi)) >
k
i=1 αi

.
Since d is a distance, it veriﬁes the triangular inequality d (fi, a) + d(a, b) ≥
d (fi, b). With αi > 0, this means that αid(a, b) ≥αi (d (fi, b) −d (fi, a)).
Therefore
k
i=1 αi

d(a, b) ≥k
i=1 (αi (d (fi, b) −d (fi, a))) >
k
i=1 αi

,
and thus d(a, b) > 1. Now, it is easy to see that if there exist a (n −1) path
linking a to b without intersecting the object then there has to be a point on
the path inside that is (n−1)-neighbor to a point outside. The distance between
two such points is at least 1 which proves that E is (n −1)-separating in Zn. ⊓⊔
Let us note some important points here. Nowhere in this proof (or more
generally in the deﬁnition) appears the type of distance. So far in the images we
have considered the Euclidean distance but that any distance. It is the triangular
inequality property of the distance that is used in the proof.
The next proposition concerns partitioning properties similar to those already
seen for the Andres circles and hyperspheres [1,2]. For the sake of simplicity,
we are going to consider, in what follows, foci based digital primitives with
consecutive integer general radii. In all generality, it can be any set of consecutive
sequence of radii as long as the diﬀerence between two consecutive radii is one.
Proposition 1. A set of foci based digital nD objects with consecutive general
radii is partitioning space:
For r1, r2 ∈Z, r1 ̸= r2, we have Fn
k (f, α, r1) ∩Fn
k (f, α, r2) = ∅,
and for r ∈N, ⊎∞
r=−∞Fn
k (f, α, r) = Zn.

394
E. Andres et al.
Proof. The property is a direct consequence from the deﬁnition: the intervals

r1 −1
2, r1 + 1
2 [ and

r2 −1
2, r2 + 1
2 [ are disjoint for r1, r2 ∈Z, r1 ̸= r2. And
the intervals of consecutive integer general radii r partition the natural number
set ⊎∞
r=−∞

r −1
2, r + 1
2 [ = Z.
⊓⊔
This property is a direct extension of the space partitioning property already
seen for the Andres hyperspheres [2]. It is interesting here to note that this prop-
erty comes directly in contradiction with another property that is often sought
which is minimal thickness. It is easy to see that it is not possible to parti-
tion space with, for example digital ellipses, without having local non-uniform
thickness. Even for the most regular of all those type of ﬁgure, circles, this is
not possible. Another point to be made about the local thickness of this digital
curves and surfaces comes from the proof of Theorem 1. In the proof, the radius
disappears when we consider the distance between a point inside and outside the
digital object. What that means is that the proposed bounds
k
i=1 αi
 	
r ± 1
2

are the bounds that ensure that all the curves and surfaces that partition space
(independently of r) are (n−1)-separating. From a general perspective, this can
be understood quite easily. For a very large generalized radius r, all the focus
points become basically one focus point and the shape of the focus based prim-
itives becomes a hypersphere. It is not very diﬃcult to see that the minimal
thickness to ensure the (n −1)-separation property is r + 1
2 −(r −1
2) = 1 (one
can always divide the formula of Deﬁnition 2 by k
i=1 αi).
Fig. 5. Partitioning ellipses and partitioning ellipsoids.
Figure 5 illustrates the partitioning property of foci based digital primitives.
Note that in the 2D case presented in the ﬁgure with focal points (0.1, 0.3) and
(5.1, 3.3), the ellipses of radii 0, 1 and 2 are of course empty since the distance
between both foci is around 5.83 and with two foci and weights of 1, the deﬁnition
is given by 2r −1 ≤d(p, f1) + d(p, f2) < 2r + 1.
3
Extensions
Let us know look at some extensions of proposed Deﬁnition 2 of digital foci based
primitives. The ﬁrst extension is an immediate extension of the deﬁnition to more

Digital Primitives Deﬁned by Weighted Focal Set
395
general thicknesses which allows more general separation properties. The second
extension corresponds to the classical approach where a digital conic is deﬁned
by a focal point and a directrix.
3.1
m-Separating Digital Foci Based Primitives
At ﬁrst, let us expand Deﬁnition 2 to include foci based nD primitives that are
m-separating, for 0 ≤m < n −1 rather than only (n −1).-separating (Fig. 6).
Fig. 6. Foci based digital 0-separating ellipse F 2,0
2
(((0.1, 0.2), (5.1, 3.1)) , (1, 1), 4.5)
Deﬁnition 3. The m-separating foci based digital nD primitive is deﬁned by:
Fn,m
k
(f, α, r) =

p ∈Zn :
 k

i=1
|αi|
 
r −
√n −m
2

≤
k

i=1
αid (p, fi) <
 k

i=1
|αi|
 
r +
√n −m
2

Proposition 2. The digital primitive Fn,m
k
(f, α, r) is m-separating in Zn.
Proof. The proof for the separation property is similar to the one of Proposition
1 with simply a diﬀerent constant. It results in d(a, b) > √n −m which proves
that the digital object is m-separating.
⊓⊔
3.2
Primitives with One Focal Point and a Directrix
There are many diﬀerent ways of deﬁning 2D conics. One way is to deﬁne a
conic with a focal point, a directrix (a straight line) and a constant e called the
eccentricity. We are going to propose now a digital deﬁnition of conics based on
such parameters:
Deﬁnition 4. A digital conic C(f, L, e) in 2D is given by
C(f, L, e) =

p ∈Z2 : −e + 1
2
≤d(p, f) −e · d(p, L) < e + 1
2

(1)
where f ∈R2, L ⊂R2, e > 0 denote the respective focal point, directrix, and
eccentricity of the corresponding real conic.

396
E. Andres et al.
Fig. 7. Conics C((−2, 2), L, e), for directrix L passing through (−5, −5) and (5, 2) and
eccentricity e = 0.5, 0.7, 1.0, 2.0 from left to right.
Theorem 2. A digital conic C(f, L, e) given by Eq. 1 is 1-separating in Z2.
A
B
F
L
Proof. Let a and b be two integer points, the former lying
in the interior and the latter in the exterior of C(f, L, e), as
shown in the inset ﬁgure. Then,
d(a, f) −e · d(a, L) < −e + 1
2
=⇒−d(a, f) + e · d(a, L) > e + 1
2
(2)
and d(b, f) −e · d(b, L) ≥e + 1
2
.
(3)
Adding Eqs. 2 and 3, we get
d(b, f) −d(a, f) + e(d(a, L) −d(b, L)) > e + 1.
(4)
We have two possible cases:
(i) d(b, f) −d(a, f) > 1. by triangle inequality, d(a, b) > 1.
(ii) d(b, f)−d(a, f) ≤1. By Eq. 4, e(d(a, L)−d(b, L)) > e, or, d(a, L)−d(b, L) >
1, which implies by Pythagorean theorem, d(a, b) > 1.
As d(a, b) > 1 for either case, C(f, L, e) is 1-separating.
4
Conclusion and Perspectives
In this paper we are proposing a new class of digital primitives with deﬁnitions
based on focal points. The deﬁnition allows any number of foci and weighted
distances. The proposed deﬁnition generalizes Andres hyperspheres [1,2]. These
primitives are deﬁned in dimension n, have a space partitioning property, and
their thickness can be controlled so that they are guaranteed to be m-separating
in space. We propose an extension based on a similar principle, where we deﬁne
a new class of digital conics deﬁned by a directrix (a straight line), a focal point,
and a parameter e called eccentricity. What we would like to highlight with this

Digital Primitives Deﬁned by Weighted Focal Set
397
way of deﬁning digital primitives is that it allows power and ﬂexibility to the
design of digital primitives.
This work has opened many possibilities for future work and further exten-
sions. Instead of focal points, one can imagine considering distances to objects
which could make an interesting link with distance transforms and skeletoniza-
tion. Can we keep topological m-separation properties? As we can see in Fig. 7
for instance, the foci based primitives do not, in general, have a constant thick-
ness. One could imagine primitives that are deﬁned as the outer or the inner
k-connected boundary of the foci based primitives as we have deﬁned them. As
mentioned, the proposed formula ensures that, for a set of foci and weights, the
digital primitives separate space regardless of the generalized radius. Now, what
would we have to change in order to ensure separation for a primitive of a given
generalized radius? It would be interesting to compare such primitives to more
classically deﬁned digital primitives. For that matter, do the classically deﬁned
ellipses, parabola, hyperbola, etc. respect distance sum properties to some focal
point?
In the proof of Theorem 1, the only thing that appears is a notion of distance
and the minimal distance to ensure a separation property. As one can see in
Fig. 8. Foci based digital primitives on arbitrary digital surfaces
Fig. 9. A digital ellipse based on the Chebychev distance

398
E. Andres et al.
Fig. 10. Conics C((0, 0, 5), L, e), for directrix plane L of equation ax + by + cz + d = 0
with (a, b, c, d) = (18.02, −33.10, 92.62, 0) and eccentricity e = 0.5, 0.7, 1, 1.4 from left
to right.
Fig. 8, one can deﬁne such focus based digital objects on arbitrary digital sur-
faces. It would be interesting to extend such notions to graphs and triangular
meshes (as long as the triangles in the mesh are somewhat regular). Nothing in
the deﬁnition limits us to the Euclidean distance. Experimentation with diﬀerent
distances could be very interesting as well (See Fig. 9 as an example).
It is interesting to notice that nothing in Deﬁnition 4 or in the proof of Theo-
rem 2 limits our deﬁnition to dimension two. As one can see in Fig. 10, with a 3D
plane as directrix for instance, that one can create digital ellipsoids, paraboloids,
and hyperboloids. One can imagine replacing the plane in 3D by a 3D straight
line.
Lastly, general questions can be raised: how can such primitives be recog-
nized? At what more precise conditions are such primitives empty?
References
1. Andres, E.: Discrete circles, rings and spheres. Comput. Graph. 18(5), 695–706
(1994)
2. Andres, E., Jacob, M.-A.: The discrete analytical hyperspheres. IEEE Trans. Vis.
Comput. Graph. 3(1), 75–86 (1997)
3. Brimkov, V.E., Coeurjolly, D., Klette, R.: Digital planarity - a review. Discrete
Appl. Math. 155(4), 468–495 (2007)
4. Fellner, D.W., Helmberg, C.: Robust rendering of general ellipses and elliptical arcs.
ACM Trans. Graph. 12(3), 251–276 (1993)
5. Mahato, P., Bhowmick, P.: Construction of digital ellipse by recursive integer inter-
vals. In: Normand, N., Gu´edon, J., Autrusseau, F. (eds.) DGCI 2016. LNCS, vol.
9647, pp. 295–308. Springer, Cham (2016). doi:10.1007/978-3-319-32360-2 23
6. McIlroy, M.D.: Getting raster ellipses right. ACM Trans. Graph. 11(3), 259–275
(1992)
7. Reveill`es, J.-P.: Calcul en Nombres Entiers et Algorithmique. Ph.D. thesis, Universit´e
Louis Pasteur, Strasbourg, France (1991)
8. Rosenfeld, A.: Adjacency in digital pictures. Inf. Control 26(1), 24–33 (1974)
9. Sz-Nagy, G.: Tschirnhaus’sche eiﬂachen und eikurven. Acta Mathematica Acad-
emiae Scientiarum Hungarica 1(2), 36–45 (1950)

Author Index
Allili, Madjid
375
Andres, Eric
19, 388
Artner, Nicole M.
307
Bac, Alexandra
213
Balázs, Péter
267
Barcucci, Elena
147
Batenburg, K. Joost
122
Batenburg, Kees Joost
109
Baudrier, Étienne
363
Bhowmick, Partha
347, 388
Biswas, Ranita
347, 388
Boutry, Nicolas
225
Brunetti, Sara
267
Caissard, Thomas
241
Ceko, Matthew
46, 135
Challa, Aditya
185
Ciesielski, Krzysztof Chris
57
Clément, Michaël
171
Coeurjolly, David
197, 241
Danda, Sravan
185
Daya Sagar, B.S.
185
Debled-Rennesson, Isabelle
69
Dulio, Paolo
147
Dwivedi, Shivam
347
Feschet, Fabien
291
Frosini, Andrea
147
Gérard, Yan
279
Géraud, Thierry
225
Gonzalez-Lorenzo, Aldo
213
Guédon, Jeanpierre
159
Gueth, Pierre
197
Gupta, Aniket
347
Janusch, Ines
307
Kaczynski, Tomasz
375
Kadu, Ajinkya
122
Kenmochi, Yukiko
33, 69, 319
Kerautret, Bertrand
291, 319
Kovács, Gergely
82, 94
Kropatsch, Walter G.
307
Kurtz, Camille
171
Lachaud, Jacques-Olivier
197, 241
Landi, Claudia
375
Largeteau-Skapin, Gaelle
19
Malmberg, Filip
57, 335
Mari, Jean-Luc
213
Masoni, Filippo
375
Mazo, Loïc
363
Mourier, Aurélie
19
Nagy, Benedek
82, 94
Najman, Laurent
185, 225
Ngo, Phuc
69, 319
Normand, Nicolas
159
Öktem, Ozan
109
Palenstijn, Willem Jan
109
Passat, Nicolas
33, 69
Pluta, Kacper
33
Pottmann, Helmut
3
Ricordel, Vincent
159
Rinaldi, Simone
147
Ringh, Axel
109
Romon, Pascal
33
Roussillon, Tristan
241, 291
Roy, Siddhant
347
Saha, Punam K.
57
Sclaroff, Stan
335
Sekiya, Fumiki
254
Strand, Robin
57, 335
Sugimoto, Akihiro
254
Svalbe, Imants
46, 135
Tirkel, Andrew
46
Vacavant, Antoine
291, 319
van Leeuwen, Tristan
122
Vizvári, Béla
82, 94

Wallner, Johannes
3
Wendling, Laurent
171
Wilkinson, Michael H.F.
9
Zhang, Jianming
335
Zhuge, Xiaodong
109
400
Author Index


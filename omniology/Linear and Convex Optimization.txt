Lars-√Öke Lindahl
Linear and Convex Optimization
Convexity and Optimization ‚Äì Part II
Download free books at

ii
 
LARS-√ÖKE LINDAHL
LINEAR AND CONVEX 
OPTIMIZATION
CONVEXITY AND 
OPTIMIZATION ‚Äì PART II
Download free eBooks at bookboon.com

iii
Linear and Convex Optimization: Convexity and Optimization ‚Äì Part II
1st edition
¬© 2016 Lars-√Öke Lindahl & bookboon.com
ISBN 978-87-403-1383-3
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
iv
Contents
iv
CONTENTS
	
To see Part I download: Convexity: Convexity and Optimization ‚Äì Part I
	
Part I. Convexity
1	
Preliminaries	
Part I
2	
Convex sets	
Part I
2.1	
Affine sets and affine maps	
Part I
2.2	
Convex sets	
Part I
2.3	
Convexity preserving operations	
Part I
2.4	
Convex hull	
Part I
2.5	
Topological properties	
Part I
2.6	
Cones	
Part I
2.7	
The recession cone	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
v
Contents
3	
Separation	
Part I
3.1	
Separating hyperplanes	
Part I
3.2	
The dual cone	
Part I
3.3	
Solvability of systems of linear inequalities	
Part I
	
Exercises	
Part I
4	
More on convex sets	
Part I
4.1	
Extreme points and faces	
Part I
4.2	
Structure theorems for convex sets	
Part I
	
Exercises	
Part I
5	
Polyhedra	
Part I
5.1	
Extreme points and extreme rays	
Part I
5.2	
Polyhedral cones	
Part I
5.3	
The internal structure of polyhedra	
Part I
5.4	
Polyhedron preserving operations	
Part I
5.5	
Separation	
Part I
	
Exercises	
Part I
6	
Convex functions	
Part I
6.1	
Basic definitions	
Part I
6.2	
Operations that preserve convexity	
Part I
6.3	
Maximum and minimum	
Part I
6.4	
Some important inequalities	
Part I
6.5	
Solvability of systems of convex inequalities	
Part I
6.6	
Continuity	
Part I
6.7	
The recessive subspace of convex functions	
Part I
6.8	
Closed convex functions	
Part I
6.9	
The support function	
Part I
6.10	
The Minkowski functional	
Part I
	
Exercises	
Part I
7	
Smooth convex functions	
Part I
7.1	
Convex functions on R	
Part I
7.2	
Differentiable convex functions	
Part I
7.3	
Strong convexity	
Part I
7.4	
Convex functions with Lipschitz continuous derivatives	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
vi
Contents
8	
The subdifferential	
Part I
8.1	
The subdifferential	
Part I
8.2	
Closed convex functions	
Part I
8.3	
The conjugate function	
Part I
8.4	
The direction derivative	
Part I
8.5	
Subdifferentiation rules	
Part I
	
Exercises	
Part I
	
Bibliografical and historical notices	
Part I
	
References	
Part I
	
Answers and solutions to the exercises	
Part I
	
Index	
Part I
	
Endnotes	
Part I
	
Part II. Linear and Convex Optimization
	
Preface	
viii
	
List of symbols	
ix
9	
Optimization	
1
9.1	
Optimization problems	
1
9.2	
Classification of optimization problems	
5
9.3	
Equivalent problem formulations	
10
9.4	
Some model examples	
15
	
Exercises	
29
10	
The Lagrange function	
32
10.1	
The Lagrange function and the dual problem	
32
10.2	
John‚Äôs theorem	
41
	
Exercises	
47
11	
Convex optimization	
49
11.1	
Strong duality	
49
11.2	
The Karush-Kuhn-Tucker theorem	
52
11.3	
The Lagrange multipliers	
54
	
Exercises	
58
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
vii
Contents
12	
Linear programming	
62
12.1	
Optimal solutions	
62
12.2	
Duality	
67
	
Exercises	
80
13	
The simplex algorithm	
82
13.1	
Standard form	
82
13.2	
Informal description of the simplex algorithm	
84
13.3	
Basic solutions	
91
13.4	
The simplex algorithm	
101
13.5	
Bland‚Äôs anti cycling rule	
116
13.6	
Phase 1 of the simplex algorithm	
121
13.7	
Sensitivity analysis	
128
13.8	
The dual simplex algorithm	
132
13.9	
Complexity	
136
	
Exercises	
138
	
Bibliografical and historical notices	
144
	
References	
146
	
Answers and solutions to the exercises	
150
	
Index	
154
	
Part III. Descent and Interior-point Methods
14	
Descent methods	
Part III
14.1	
General principles	
Part III
14.2	
The gradient descent method	
Part III
15	
Newton‚Äôs method	
Part III
15.1	
Newton decrement and Newton direction	
Part III
15.2	
Newton‚Äôs method	
Part III
15.3	
Equality constraints	
Part III
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
viii
Contents
16	
Self-concordant functions	
Part III
16.1	
Self-concordant functions	
Part III
16.2	
Closed self-concordant functions	
Part III
16.3	
Basic inequalities for the local seminorm	
Part III
16.4	
Minimization	
Part III
16.5	
Newton‚Äôs method for self-concordant functions	
Part III
17	
The path-following method	
Part III
17.1	
Barrier and central path	
Part III
17.2	
Path-following methods	
Part III
18	
The path-following method with self-concordant barrier	
Part III
18.1	
Self-concordant barriers	
Part III
18.2	
The path-following method	
Part III
18.3	
LP problems	
Part III
18.4	
Complexity	
Part III
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
ix
Preface
Preface
Part II in this series of three on convexity and optimization is about linear
and convex optimization.
We start by studying some equivalent ways to formulate a given opti-
mization problem and then present some classical model examples.
Duality is an important principle in many areas of mathematics, so also in
optimization theory. To each minimization problem we can associate a dual
maximization problem by means of the so-called Lagrange function, and the
two problems have the same optimal value provided certain conditions are
fulÔ¨Ålled. We devote two chapters to the study of duality for general convex
optimization problems and then treat the special case of linear programming
in a separate chapter.
The simplex algorithm, which until the mid 1980‚Äôs was the only practical
algorithm for solving large linear optimization problems, is studied in the
last chapter.
Uppsala, April 2016
Lars-Àö
Ake Lindahl
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
x
List of symbols
List of symbols
con X
conic hull of X, see Part I
cvx X
convex hull of X, see Part I
dom f
the eÔ¨Äective domain of the function f, i.e.
{x | ‚àí‚àû< f(x) < ‚àû}
ext X
set of extreme points of X, see Part I
recc X
recession cone of X, see Part I
f ‚Ä≤
derivate or gradient of f, see Part I
vmax, vmin
optimal values, p. 2
B(a; r)
open ball centered at a with radius r
B(a; r)
closed ball centered at a with radius r
I(x)
set of active constraints at x, p. 42
L(x, Œª)
Lagrange function, p. 32
MÀÜr[x]
object obtained by replacing the element in M
at location r by x, p. 93
R+, R++
{x ‚ààR | x ‚â•0}, {x ‚ààR | x > 0}
R‚àí
{x ‚ààR | x ‚â§0}
R, R, R
R ‚à™{‚àû}, R ‚à™{‚àí‚àû}, R ‚à™{‚àû, ‚àí‚àû}
X+
dual cone of X, see Part I
1
the vector (1, 1, . . . , 1)
œÜ(Œª)
dual function infx L(x, Œª), p. 33
‚àáf
gradient of f
[x, y]
line segment between x and y
]x, y[
open line segment between x and y
‚à•¬∑‚à•1, ‚à•¬∑‚à•2, ‚à•¬∑‚à•‚àû
‚Ñì1-norm, Euclidean norm, maximum norm, see Part I
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
1
Optimization
Chapter 9
Optimization
The Latin word optimum means ‚Äôthe best‚Äô. The optimal alternative among
a number of diÔ¨Äerent alternatives is the one that is the best in some way.
Optimization is therefore, in a broad sense, the art of determining the best.
Optimization problems occur not only in diÔ¨Äerent areas of human plan-
ning, but also many phenomena in nature can be explained by simple opti-
mization principles. Examples are light propagation and refraction in diÔ¨Äer-
ent media, thermal conductivity and chemical equilibrium.
In everyday optimization problems, it is often diÔ¨Écult, if not impossible,
to compare and evaluate diÔ¨Äerent alternatives in a meaningful manner. We
shall leave this diÔ¨Éculty aside, for it can not be solved by mathematical
methods. Our starting point is that the alternatives are ranked by means of
a function, for example a proÔ¨Åt or cost function, and that the option that
gives the maximum or minimum function value is the best one.
The problems we will address are thus purely mathematical ‚àíto min-
imize or maximize given functions over sets that are given by a number of
constraints.
9.1
Optimization problems
Basic notions
We begin by recalling the following notation from Part I:
R = R ‚à™{+‚àû}
R = R ‚à™{‚àí‚àû}
R = R ‚à™{‚àí‚àû, +‚àû}.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
2
Optimization
For the problem of minimizing a function f : ‚Ñ¶‚ÜíR over a subset X of
the domain ‚Ñ¶of the function, we use the notation
min f(x)
s.t.
x ‚ààX.
Here, s.t. is an abbreviation for the phrase subject to the condition.
The elements of the set X are called the feasible points or feasible solutions
of the optimization problem. The function f is the objective function.
Observe that vi allow +‚àûas a function value of the objective function
in a minimization problem.
The (optimal) value vmin of the minimization problem is by deÔ¨Ånition
vmin =

inf {f(x) | x ‚ààX}
if X Ã∏= ‚àÖ,
+‚àû
if X = ‚àÖ.
The optimal value is thus a real number if the objective function is bounded
below and not identically equal to +‚àûon the set X, the value is ‚àí‚àûif the
function is not bounded below on X, and the value is +‚àûif the objective
function is identically equal to +‚àûon X or if X = ‚àÖ.
Of course, we will also study maximization problems, and the problem of
maximizing a function f : ‚Ñ¶‚ÜíR over X will be written
max f(x)
s.t.
x ‚ààX.
The (optimal) value vmax of the maximization problem is deÔ¨Åned by
vmax =

sup {f(x) | x ‚ààX}
if X Ã∏= ‚àÖ,
‚àí‚àû
if X = ‚àÖ.
The optimal value of a minimization or maximization problem is in this
way always deÔ¨Åned as a real number, ‚àí‚àûor +‚àû, i.e. as an element of
the extended real line R. If the value is a real number, we say that the
optimization problem has a Ô¨Ånite value.
A feasible point x0 for an optimization problem with objective function
f is called an optimal point or optimal solution if the value of the problem
is Ô¨Ånite and equal to f(x0). An optimal solution of a minimization problem
is, in other words, the same as a global minimum point (with a Ô¨Ånite value).
Of course, problems with Ô¨Ånite optimal values need not necessarily have any
optimal solutions.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
3
Optimization
3
From a mathematical point of view, there is no diÔ¨Äerence in principle be-
tween maximization problems and minimization problems, since the optimal
values vmax and vmin of the problems
max f(x)
s.t.
x ‚ààX
and
min ‚àíf(x)
s.t.
x ‚ààX
,
respectively, are connected by the simple relation vmax = ‚àívmin, and x0 is a
maximum point of f if and only if x0 is a minimum point of ‚àíf. For this
reason, we usually only formulate results for minimization problems.
Finally, a comment as to why we allow +‚àûand ‚àí‚àûas function values
of the objective functions as this seems to complicate matters. The most
important reason is that sometimes we have to consider functions that are
deÔ¨Åned as pointwise suprema of an inÔ¨Ånite family of functions, and the supre-
mum function may assume inÔ¨Ånite values even if all functions in the family
assume only Ô¨Ånite values. The alternative to allowing functions with values
in the extended real line would be to restrict the domain of these supremum
functions, and this is neither simpler nor more elegant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
4
Optimization
General comments
There are some general and perhaps completely obvious comments that are
relevant for many optimization problems.
Existence of feasible points
This point may seem trivial, for if a problem has no feasible points then there
is not much more to be said. It should however be remembered that the set
of feasible points is seldom given explicitly. Instead it is often deÔ¨Åned by a
system of equalities and inequalities, which may not be consistent.
If the problem comes from the ‚Äùreal world‚Äù, simpliÔ¨Åcations and defects
in the mathematical model may lead to a mathematical problem that lacks
feasible points.
Existence of optimal solutions
Needless to say, a prerequisite for the determination of the optimal solution
of a problem is that there is one. Many theoretical results are of the form
‚ÄôIf x0 is an optimal solution, then x0 satisÔ¨Åes these conditions.‚Äô Although
this usually restricts the number of potential candidates for optimal points,
it does not prove the existence of such points.
From a practical point of view, however, the existence of an optimal so-
lution ‚àíand its exact value, if such a solution exists ‚àímay not be that
important. In many applications one is often satisÔ¨Åed with a feasible solu-
tions that is good enough.
Uniqueness
Is the optimal solution, if such a solution exists, unique?
The answer is
probably of little interest for somebody looking for the solution of a practical
problem ‚àíhe or she should be satisÔ¨Åed by having found a best solution even
if there are other solutions that are just as good. And if he or she would
consider one of the optimal solutions better than the others, then we can only
conclude that the optimization problem is not properly set from the start,
because the objective function apparently does not include everything that
is required to sort out the best solution.
However, uniqueness of an optimal solution may sometimes lead to inter-
esting properties that can be of use when looking for the solution.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
5
Optimization
Dependence on parameters and sensitivity
Sometimes, and in particular in problems that come directly from ‚Äùreality‚Äù,
objective functions and constraints contain parameters, which are only given
with a certain accuracy and, in the worst case, are more or less coarse esti-
mates. In such cases, it is not suÔ¨Écient to determine the optimal solution,
but it is at least as important to know how the solution changes when pa-
rameters are changed. If a small perturbation of one parameter alters the
optimal solution very much, there is reason to consider the solution with
great skepticism.
Qualitative aspects
Of course, it is only for a small class of optimization problems that one
can specify the optimum solution in exact form, or where the solution can
be described by an algorithm that terminates after Ô¨Ånitely many iterations.
The mathematical solution to an optimization problem often consists of a
number of necessary and/or suÔ¨Écient conditions that the optimal solution
must meet. At best, these can be the basis for useful numerical algorithms,
and in other cases, they can perhaps only be used for qualitative statements
about the optimal solutions, which however in many situations can be just
as interesting.
Algorithms
There is of course no numerical algorithm that solves all optimization prob-
lems, even if we restrict ourselves to problems where the constraint set is
deÔ¨Åned by a a Ô¨Ånite number of inequalities and equalities. However, there
are very eÔ¨Écient numerical algorithms for certain subclasses of optimization
problems, and many important applied optimization problems happen to be-
long to these classes. We shall study some algorithms of this type in the last
chapter of this Part II and in Part III.
The development of good algorithms has been just as important as the
computer development for the possibility of solving big optimization prob-
lems, and much of the algorithm development has occurred in recent decades.
9.2
ClassiÔ¨Åcation of optimization problems
To be able to say anything sensible about the minimization problem
(P)
min f(x)
s.t.
x ‚ààX
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
6
Optimization
6
we must make various assumptions about the objective function f : ‚Ñ¶‚ÜíR
and about the set X of feasible points.
We will always assume that ‚Ñ¶is a subset of Rn and that the set X can
be expressed as the solution set of a number of inequalities and equalities,
i.e. that
X = {x ‚àà‚Ñ¶| g1(x) ‚â§0, . . . , gp(x) ‚â§0, gp+1(x) = 0, . . . , gm(x) = 0}
where g1, g2, . . . , gm are real valued functions deÔ¨Åned on ‚Ñ¶.
We do not exclude the possibility that all constraints are equalities, i.e.
that p = 0, or that all constraints are inequalities, i.e. that p = m, or that
there are no constraints at all, i.e. that m = 0.
Since the equality h(x) = 0 can be replaced by the two inequalities
¬±h(x) ‚â§0, we could without loss of generality assume that all constraints
are inequalities, but it is convenient to formulate results for optimization
problems with equalities among the constraints without Ô¨Årst having to make
such rewritings.
If ÀÜx is a feasible point and gi(ÀÜx) = 0, we say that the i:th constraint is
active at the point ÀÜx. All constraints in the form of equalities are, of course,
active at all feasible points.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
7
Optimization
The condition x ‚àà‚Ñ¶is (in the case ‚Ñ¶Ã∏= Rn) of course also a kind of
constraint, but it plays a diÔ¨Äerent role than the other constraints. We will
sometimes call it the implicit constraint in order to distinguish it from the
other explicit constraints. If ‚Ñ¶is given as the solution set of a number of
inequalities of type hi(x) ‚â§0 and the functions hi, the objective function
and the explicit constraint functions are deÔ¨Åned on the entire space Rn, we
can of course include the inequalities hi(x) ‚â§0 among the explicit conditions
and omit the implicit constraint.
The domain ‚Ñ¶will often be clear from the context, and it is in these cases
not mentioned explicitly in the formulation of the optimization problem. The
minimization problem (P) will therefore often be given in the following form
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m.
Linear programming
The problem of maximizing or minimizing a linear form over a polyhedron,
which is given in the form of an intersection of closed halvspaces in Rn,
is called linear programming, abbreviated LP. The problem (P) is, in other
words, an LP problem if the objective function f is linear and X is the set
of solutions to a Ô¨Ånite number of linear equalities and inequalities.
We will study LP problems in detail in Chapter 12.
Convex optimization
The minimization problem
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with implicit constraint x ‚àà‚Ñ¶is called convex, if the set ‚Ñ¶is convex, the
objective function f : ‚Ñ¶‚ÜíR is convex, and the constraint functions gi are
convex for i = 1, 2, . . . , p and aÔ¨Éne for i = p + 1, . . . , m.
The aÔ¨Éne conditions gp+1(x) = 0, . . . , gm(x) = 0 in a convex problem
can of course be summarized as Ax = b, where A is an (m ‚àíp) √ó n-matrix.
The set X of feasible points is convex in a convex minimization problem,
for
X =
p
i=1
{x ‚àà‚Ñ¶| gi(x) ‚â§0} ‚à©
m

i=p+1
{x | gi(x) = 0},
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
8
Optimization
and this expresses X as an intersection of sublevel sets of convex functions
and hyperplanes.
A maximization problem
max f(x)
s.t.
x ‚ààX
is called convex if the corresponding equivalent minimization problem
min ‚àíf(x)
s.t.
x ‚ààX
is convex, which means that the objective function f has to be concave.
LP problems are of course convex optimization problems. General convex
optimization problems are studied in Chapter 11.
Convex quadratic programming
We get a special case of convex optimization if X is a polyhedron and the
objective function f is a sum of a linear form and a positive semideÔ¨Ånite
quadratic form, i.e. has the form f(x) = ‚ü®c, x‚ü©+ ‚ü®x, Qx‚ü©, where Q is a
positive semideÔ¨Ånite matrix. The problem (P) is then called convex quadratic
programming. LP problems constitute a subclass of the convex quadratic
problems, of course.
Non-linear optimization
Non-linear optimization is about optimization problems that are not sup-
posed to be LP problems.
Since non-linear optimization includes almost
everything, there is of course no general theory that can be applied to an
arbitrary non-linear optimization problem.
If f is a diÔ¨Äerentiable function and X is a ‚Äùdecent‚Äù set in Rn, one can of
course use diÔ¨Äerential calculus to attack the minimization problem (P). We
recall in this context the Lagrange theorem, which gives a necessary condition
for the minimum (and maximum) when
X = {x ‚ààRn | g1(x) = g2(x) = ¬∑ ¬∑ ¬∑ = gm(x) = 0}.
A counterpart of Lagrange‚Äôs theorem for optimization problems with con-
straints in the form of inequalities is given in Chapter 10.
Integer programming
An integer programming problem is a mathematical optimization problem in
which some or all of the variables are restricted to be integers. In particular,
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
9
Optimization
9
a linear integer problem is a problem of the form
min ‚ü®c, x‚ü©
s.t.
x ‚ààX ‚à©(Zm √ó Rn‚àím)
where ‚ü®c, x‚ü©is a linear form and X is a polyhedron in Rn.
Many problems dealing with Ô¨Çows in networks, e.g. commodity distribu-
tion problems and maximum Ô¨Çow problems, are linear integer problems that
can be solved using special algorithms.
Simultaneous optimization
The title refers to a type of problems that are not really optimization prob-
lems in the previous sense. There are many situations, where an individual
may aÔ¨Äect the outcome through his actions without having full control over
the situation. Some variables may be in the hands of other individuals with
completely diÔ¨Äerent desires about the outcome, while other variables may be
of a completely random nature. The problem to in some sense optimize the
outcome could then be called simultaneous optimization.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
10
Optimization
Simultaneous optimization is the topic of game theory, which deals with
the behavior of the various agents in conÔ¨Çict situations. Game theoretical
concepts and results have proved to be very useful in various contexts, e.g.
in economics.
9.3
Equivalent problem formulations
Let us informally call two optimization problems equivalent if it is possible to
determine in an automatical way an optimal solution to one of the problems,
given an optimal solution to the other, and vice versa.
A trivial example of equivalent problems are, as already mentioned, the
problems
max f(x)
s.t.
x ‚ààX
and
min ‚àíf(x).
s.t.
x ‚ààX
We now describe some useful transformations that lead to equivalent op-
timization problem
Elimination of equalities
Consider the problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m.
If it is possible to solve the subsystem of equalities and express the solution
in the form x = h(y) with a parameter y running over some subset of Rd,
then we can eliminate the equalities and rewrite problem (P) as
(P‚Ä≤)
min f(h(y))
s.t.
gi(h(y)) ‚â§0,
i = 1, 2, . . . , p
If ÀÜy is an optimal solution to (P‚Ä≤), then h(ÀÜy) is of course an optimal solution
to (P). Conversely, if ÀÜx is an optimal solution to (P), then ÀÜx = h(ÀÜy) for some
value ÀÜy of the parameter, and this value is an optimal solution to (P‚Ä≤).
The elimination is always possible (by a simple algorithm) if all constraint
equalities are aÔ¨Éne, i.e. if the system can be written in the form Ax = b for
some (m ‚àíp) √ó n-matrix A. Assuming that the system is consistent, the
solution set is an aÔ¨Éne subspace of dimension d = n ‚àírank A, and there
exists an n√ód-matrix C of rank d and a particular solution x0 to the system
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
11
Optimization
such that Ax = b if and only if x = Cy + x0 for some y ‚ààRd. The problem
(P) is thus in this case equivalent to the problem
min f(Cy + x0)
s.t.
gi(Cy + x0) ‚â§0,
i = 1, 2, . . . , p
(with implicit constraint Cy + x0 ‚àà‚Ñ¶).
In convex optimization problems, and especially in LP problems, we can
thus, in principle, eliminate the equalities from the constraints and in this
way replace the problem by an equivalent optimization problem without any
equality constraints.
Slack variables
The inequality g(x) ‚â§0 holds if and only if there is a number s ‚â•0 such
that g(x) + s = 0. By thus replacing all inequalities in the problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with equalities, we obtain the following equivalent problem
(P‚Ä≤)
min f(x)
s.t.
Ô£±
Ô£≤
Ô£≥
gi(x) + si = 0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
si ‚â•0,
i = 1, 2, . . . , p
with n + p variables, m equality constraints and p simple inequality con-
straints. The new variables si are called slack variables.
If ÀÜx is an optimal solution to (P), we get an optimal solution (ÀÜx, ÀÜs) to (P‚Ä≤)
by setting ÀÜsi = ‚àígi(ÀÜx). Conversely, if (ÀÜx, ÀÜs) is an optimal solution to the last
mentioned problem, then ÀÜx is of course an optimal solution to the original
problem.
If the original constraints are aÔ¨Éne, then so are all new constraints. The
transformation thus transforms LP problems to LP problems.
Inequalities of the form g(x) ‚â•0 can of course similarly be written as
equalities g(x)‚àís = 0 with nonnegative variables s. These new variables are
usually called surplus variables.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
12
Optimization
12
Nonnegative variables
Every real number can be written as a diÔ¨Äerence between two nonnegative
numbers. In an optimization problem, we can thus replace an unrestricted
variable xi, i.e. a variable that a priori may assume any real value, with two
nonnegative variables x‚Ä≤
i and x‚Ä≤‚Ä≤
i by setting
xi = x‚Ä≤
i ‚àíx‚Ä≤‚Ä≤
i , x‚Ä≤
i ‚â•0, x‚Ä≤‚Ä≤
i ‚â•0.
The number of variables increases with one and the number of inequalities
increases with two for each unrestricted variable that is replaced, but the
transformation leads apparently to an equivalent problem. Moreover, convex
problems are transfered to convex problems and LP problems are transformed
to LP problems.
Example 9.3.1. The LP problem
min x1 + 2x2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 ‚â•2
2x1 ‚àíx2 ‚â§3
x1 ‚â•0
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
13
Optimization
is transformed, using two slack/surplus variables and by replacing the un-
restricted variable x2 with a diÔ¨Äerence of two nonnegative variables, to the
following equivalent LP problem in which all variables are nonnegative and
all remaining constraints are equalities.
min x1 + 2x‚Ä≤
2 ‚àí2x‚Ä≤‚Ä≤
2 + 0s1 + 0s2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x‚Ä≤
2 ‚àíx‚Ä≤‚Ä≤
2 ‚àís1
= 2
2x1 ‚àíx‚Ä≤
2 + x‚Ä≤‚Ä≤
2
+ s2 = 3
x1, x‚Ä≤
2, x‚Ä≤‚Ä≤
2, s1, s2 ‚â•0.
Epigraph form
Every optimization problem can be replaced by an equivalent problem with
a linear objective function, and the trick to accomplish this is to utilize the
epigraph of the original objective function. The two problems
(P)
min f(x)
s.t.
x ‚ààX
and
(P‚Ä≤)
min t
s.t.
f(x) ‚â§t
x ‚ààX
are namely equivalent, and the objective function in (P‚Ä≤) is linear. If ÀÜx is an
optimal solution to (P), then (ÀÜx, f(ÀÜx)) is an optimal solution to (P‚Ä≤), and if
(ÀÜx, ÀÜt) is an optimal solution to (P‚Ä≤), then ÀÜx is an optimal solution to (P).
If problem (P) is convex, i.e. has the form
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with convex functions f and gi for 1 ‚â§i ‚â§p, and aÔ¨Éne functions gi for
i ‚â•p + 1, then the epigraph variant
min t
s.t.
Ô£±
Ô£≤
Ô£≥
f(x) ‚àít ‚â§0,
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
is also a convex problem.
So there is no restriction to assume that the objective function of a convex
program is linear when we are looking for general properties of such programs.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
14
Optimization
Piecewise aÔ¨Éne objective functions
Suppose that X is a polyhedron (given as an intersection of closed halfspaces)
and consider the convex optimization problem
(P)
min f(x)
s.t.
x ‚ààX
where the objective function f(x) is piecewise aÔ¨Éne and given as
f(x) = max{‚ü®ci, x‚ü©+ bi | i = 1, 2, . . . , m}.
The epigraph transformation results in the equivalent convex problem
min t
s.t.

max
1‚â§i‚â§m(‚ü®ci, x‚ü©+ bi) ‚â§t
x ‚ààX,
and since max1‚â§i‚â§m Œ±i ‚â§t if and only if Œ±i ‚â§t for all i, this problem is in
turn equivalent to the LP problem
(P‚Ä≤)
min t
s.t.
‚ü®ci, x‚ü©‚àít + bi ‚â§0,
i = 1, 2, . . . , m
x ‚ààX.
The constraint set of this LP problem is a polyhedron in Rn √ó R.
If instead the objective function in problem (P) is a sum
f(x) = f1(x) + f2(x) + ¬∑ ¬∑ ¬∑ + fk(x)
of piecewise aÔ¨Éne functions fi, then problem (P) is equivalent to the convex
problem
min t1 + t2 + ¬∑ ¬∑ ¬∑ + tk
s.t.
 fi(x) ‚â§ti
i = 1, 2, . . . , k
x ‚ààX
and this problem becomes an LP problem if every inequality fi(x) ‚â§ti is
expressed as a system of linear inequalities in a similar way as above.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
15
Optimization
15
9.4
Some model examples
Diet problem
Let us start with a classical LP problem that was formulated and studied
during the childhood of linear programming. The goal of the diet problem
is to select a set of foods that will satisfy a set of daily nutritional require-
ments at minimum cost.
There are n foods L1, L2, . . . , Ln available at a
cost of c1, c2, . . . , cn dollars per unit. The foods contain various nutrients
N1, N2, . . . , Nm (proteins, carbohydrates, fats, vitamins, etc.). The number
of units of nutrients per unit of food is shown by the following table:
L1
L2
. . .
Ln
N1
a11
a12
. . .
a1n
N2
a21
a22
. . .
a2n
...
Nm
am1
am2
. . .
amn
Buying x1, x2, . . . , xn units of the foods, one thus obtains
ai1x1 + ai2x2 + ¬∑ ¬∑ ¬∑ + ainxn
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
16
Optimization
units of nutrient Ni at a cost of
c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn.
Suppose that the daily requirement of the diÔ¨Äerent nutrients is b1, b2,
. . . , bm and that it is not harmful to have too much of any substance. The
problem to meet the daily requirement at the lowest possible cost is called
the diet problem. Mathematically, it is of the form
min c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn ‚â•b1
a21x1 + a22x2 + ¬∑ ¬∑ ¬∑ + a2nxn ‚â•b2
...
am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn ‚â•bm
x1, x2, . . . , xn ‚â•0.
The diet problem is thus an LP problem. In addition to determining
the optimal diet and the cost of this, it would be of interest to answer the
following questions:
1. How does a price change of one or more of the foods aÔ¨Äect the optimal
diet and the cost?
2. How is the optimal diet aÔ¨Äected by a change of the daily requirement
of one or more nutrients?
3. Suppose that pure nutrients are available on the market. At what price
would it be proÔ¨Åtable to buy these and satisfy the nutritional needs by
eating them instead of the optimal diet? Hardly a tasty option for a
gourmet but perhaps possible in animal feeding.
Assume that the cost of the optimal diet is z, and that its cost changes
to z + ‚àÜz when the need for nutrient N1 is changed from b1 to b1 + ‚àÜb1,
ceteris paribus. It is obvious that the cost can not be reduced when demand
increases, so therefore ‚àÜb1 > 0 entails ‚àÜz ‚â•0. If it is possible to buy the
nutrient N1 in completely pure form to the price p1, then it is economically
advantageous to meet the increased need by taking the nutrient in pure form,
provided that p1‚àÜb1 ‚â§‚àÜz. The maximum price of N1 which makes nutrient
in pure form an economical alternative is therefore ‚àÜz/‚àÜb1, and the limit as
‚àÜb1 ‚Üí0, i.e. the partial derivative ‚àÇz
‚àÇb1, is called the dual price or the shadow
price in economic literature.
It is possible to calculate the nutrient shadow prices by solving an LP
problem closely related to the diet problem. Assume again that the market
provides nutrients in pure form and that their prices are y1, y2, . . . , ym. Since
one unit of food Li contains a1i, a2i, . . . , am units of each nutrient, we can
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
17
Optimization
‚Äùmanufacture‚Äù one unit of food Li by buying just this set of nutrients, and
hence it is economically advantageous to replace all foods by pure nutrients
if
a1iy1 + a2iy2 + ¬∑ ¬∑ ¬∑ + amym ‚â§ci
for i = 1, 2, . . . , n. Under these conditions the cost of the required daily ration
b1, b2, . . . , bm is at most equal to the maximum value of the LP problem
max b1y1 + b2y2 + ¬∑ ¬∑ ¬∑ + bmym
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11y1 + a21y2 + . . . + am1ym ‚â§c1
a12y1 + a22y2 + . . . + am2ym ‚â§c2
...
a1ny1 + a2ny2 + . . . + amnym ‚â§cn
y1, y2, . . . , ym ‚â•0.
We will show that this so called dual problem has the same optimal value
as the original diet problem and that the optimal solution is given by the
shadow prices.
Production planning
Many problems related to production planning can be formulated as LP
problems, and a pioneer in the Ô¨Åeld was the Russian mathematician and
economist Leonid Kantorovich, who studied and solved such problems in the
late 1930s. Here is a typical such problem.
A factory can manufacture various goods V1, V2, . . . , Vn. This requires
various inputs (raw materials and semi-Ô¨Ånished goods) and diÔ¨Äerent types of
labor, something which we collectively call production factors P1, P2, . . . , Pm.
These are available in limited quantities b1, b2, . . . , bm. In order to manufac-
ture, market and sell one unit of the respective goods, production factors are
needed to an extent given by the following table:
V1
V2
. . .
Vn
P1
a11
a12
. . .
a1n
P2
a21
a22
. . .
a2n
...
Pm
am1
am2
. . .
amn
Every manufactured product Vj can be sold at a proÔ¨Åt which is cj dollars per
unit, and the goal now is to plan the production x1, x2, . . . , xn of the various
products so that the proÔ¨Åt is maximized.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
18
Optimization
18
Manufacturing x1, x2, . . . , xn units of the goods consumes ai1x1 + ai2x2 +
¬∑ ¬∑ ¬∑ + ainxn units of production factor Pi and results in a proÔ¨Åt equal to
c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn. The optimization problem that we need to solve is
thus the LP problem
max c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a12x2 + . . . + a1nxn ‚â§b1
a21x1 + a22x2 + . . . + a2nxn ‚â§b2
...
am1x1 + am2x2 + . . . + amnxn ‚â§bm
x1, x2, . . . , xn ‚â•0.
Here it is reasonable to ask similar questions as for the diet problem, i.e. how
is the optimal solution and the optimal proÔ¨Åt aÔ¨Äected by
1. altered pricing c1, c2, . . . , cn;
2. changes in the resource allocation.
If we increase a resource Pi that is already fully utilized, so does (nor-
mally) the proÔ¨Åt. What will the price of this resource be for the expansion
to pay oÔ¨Ä? The critical price is called the shadow price, and it can be inter-
preted as a partial derivative, and as the solution to a dual problem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
19
Optimization
Transportation problem
The transportation problem is another classical LP problem that was formu-
lated and solved before the invention of the simplex algorithm
A commodity (e.g. gasoline) is stored at m places S1, S2, . . . , Sm and de-
manded at n other locations D1, D2, . . . , Dn. The quantity of the commodity
available at Si is ai units, while bj units are demanded at Dj. To ship 1 unit
from storage place Si to demand center Dj costs cij dollars.
...
...
...
...
ai
bn
bj
b1
cij
xij
Figure 9.1. The transportation problem
The total supply, i.e. m
i=1 ai, is assumed for simplicity to be equal to the
total demand n
j=1 bj, so it is possible to meet the demand by distributing
xij units from Si to Dj. To do this at the lowest transportation cost gives
rise to the LP problem
min
m

i=1
n

j=1
cijxij
s.t.
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
n
j=1 xij = ai,
i = 1, 2, . . . , m
m
i=1 xij = bj,
j = 1, 2, . . . , n
xij ‚â•0,
all i, j.
An investment problem
An investor has 1 million dollars, which he intends to invest in various
projects, and he has found m interesting candidates P1, P2, . . . , Pm for this.
The return will depend on the projects and the upcoming economic cycle.
He thinks he can identify n diÔ¨Äerent economic situations E1, E2, . . . , En, but
it is impossible for him to accurately predict what the economy will look like
in the coming year, after which he intends to collect the return. However,
one can accurately assess the return of each project during the various eco-
nomic cycles; each invested million dollars in project Pi will yield a return
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
20
Optimization
of aij million dollars during business cycle Ej. We have, in other words, the
following table of return for various projects and business cycles:
E1
E2
. . .
En
P1
a11
a12
. . .
a1n
P2
a21
a22
. . .
a2n
...
Pm
am1
am2
. . .
amn
Our investor intends to invest x1, x2, . . . , xm million dollars in the various
projects, and this will give him the return
a1jx1 + a2jx2 + ¬∑ ¬∑ ¬∑ + amjxm
million dollars, assuming that the economy will be in state Ej. Since our in-
vestor is a very cautious person, he wants to guard against the worst possible
outcome, and the worst possible outcome for the investment x1, x2, . . . , xm is
min
1‚â§j‚â§n
m

i=1
aijxi.
He therefore wishes to maximize this outcome, which he does by solving the
problem
max min
1‚â§j‚â§n
m

i=1
aijxi
s.t.
x ‚ààX
where X is the set {(x1, x2, . . . , xm) ‚ààRm
+ | m
i=1 xi = 1} of all possible ways
to distribute one million on the various projects.
In this formulation, the problem is a convex maximization problem with
a piecewise aÔ¨Éne concave objective function. However, we can transform it
into an equivalent LP problem by making use of a hypograph formulation.
Utilizing the techniques of the previous section, we see that the investor‚Äôs
problem is equivalent to the LP problem
max v
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a21x2 + . . . + am1xm ‚â•v
a12x1 + a22x2 + . . . + am2xm ‚â•v
...
a1nx1 + a2nx2 + . . . + amnxm ‚â•v
x1 +
x2 + . . . +
xm = 1
x1, x2, . . . , xm ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
21
Optimization
21
Two-person zero-sum game
Two persons, row player Rick and column player Charlie, each choose, inde-
pendently of each other, an integer. Rick chooses a number i in the range
1 ‚â§i ‚â§m and Charlie a number j in the range 1 ‚â§j ‚â§n. If they choose
the pair (i, j), Rick wins aij dollars of Charlie, and to win a negative amount
is of course the same as to loose the corresponding positive amount.
The numbers m, n and aij are supposed to be known by both players, and
the objective of each player is to win as much as possible (or equivalently, to
loose as little as possible). There is generally no best choice for any of the
players, but they could try to maximize their expected winnings by selecting
their numbers at random with a certain probability distribution.
Suppose Rick chooses the number i with probability xi, and Charlie
chooses the number j with probability yj. All probabilities are of course
nonnegative numbers, and m
i=1 xi = n
j=1 yj = 1. Let
X = {x ‚ààRm
+ |
m

i=1
xi = 1}
and
Y = {y ‚ààRn
+ |
n

j=1
yj = 1}.
The elements in X are called the row player‚Äôs mixed strategies, and the ele-
ments in Y are the column player‚Äôs mixed strategies.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
22
Optimization
Since the players choose their numbers independently of each other, the
outcome (i, j) will occur with probability xiyj. Rick‚Äôs pay-oÔ¨Äis therefore a
random variable with expected value
f(x, y) =
m

i=1
n

j=1
aijxiyj.
Row player Rick can now conceivably argue like this: ‚ÄùThe worst that can
happen to me, if I choose the probability distribution x, is that my opponent
Charlie happens to choose a probability distribution y that minimizes my
expected proÔ¨Åt f(x, y)‚Äù. In this case, Rick will obtain the amount
g(x) = min
y‚ààY f(x, y) = min
y‚ààY
n

j=1
yj
 m

i=1
aijxi

.
The sum n
j=1 yj
m
i=1 aijxi

is a weighted arithmetic mean of the n numbers
m
i=1 aijxi, j = 1, 2, . . . , n, with the weights y1, y2, . . . , yn, and such a mean
is greater than or equal to the smallest of the n numbers, and equality is
obtained by putting all weight on this smallest number. Hence,
g(x) = min
1‚â§j‚â§n
m

i=1
aijxi.
Rick, who wants to maximize his outcome, should therefore choose to
maximize g(x), i.e. Rick‚Äôs problem becomes
max g(x)
s.t.
x ‚ààX.
This is exactly the same problem as the investor‚Äôs problem. Hence, Rick‚Äôs op-
timal strategy, i.e. optimal choice of probabilities, coincides with the optimal
solution to the LP problem
max v
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a21x2 + . . . + am1xm ‚â•v
a12x1 + a22x2 + . . . + am2xm ‚â•v
...
a1nx1 + a2nx2 + . . . + amnxm ‚â•v
x1 +
x2 + . . . +
xm = 1
x1, x2, . . . , xm ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
23
Optimization
The column player‚Äôs problem is analogous, but he will of course minimize
the maximum expected outcome f(x, y). Charlie must therefore solve the
problem
min
max
1‚â§i‚â§m
n

j=1
aijyj
s.t.
y ‚ààY
to Ô¨Ånd his optimal strategy, and this problem is equivalent to the LP problem
min
u
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11y1 + a12y2 + . . . + a1nyn ‚â§u
a21y1 + a22y2 + . . . + a2nyn ‚â§u
...
am1y1 + am2y2 + . . . + amnyn ‚â§u
y1 +
y2 + . . . +
yn = 1
y1, y2, . . . , yn ‚â•0.
The two players‚Äô problems are examples of dual problems, and it follows
from results that will appear in Chapter 12 that they have the same optimal
value.
Consumer Theory
The behavior of consumers is studied in a branch of economics known as
microeconomics. Assume that there are n commodities V1, V2, . . . , Vn on
the market and that the price of these goods is given by the price vector
p = (p1, p2, . . . , pn). A basket x consisting of x1, x2, . . . , xn units of the goods
thus costs ‚ü®p, x‚ü©= p1x1 + p2x2 + ¬∑ ¬∑ ¬∑ + pnxn.
A consumer values her beneÔ¨Åt of the commodity bundle x by using a
subjective utility function f, where f(x) > f(y) means that she prefers x to
y. A reasonable assumption about the utility function is that every convex
combination Œªx + (1 ‚àíŒª)y of two commodity bundles should be valued as
being at least as good as the worst of the two bundles x and y, i.e. that
f(Œªx + (1 ‚àíŒª)y) ‚â•min

f(x), f(y)

. The utility function f is assumed, in
other words, to be quasiconcave, and a stronger assumption, which is often
made in the economic literature and that we are making here, is that f is
concave.
Suppose now that our consumer‚Äôs income is I, that the entire income
is disposable for consumption, and that she wants to maximize her utility.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
24
Optimization
24
Then, the problem that she needs to solve is the convex optimization problem
max f(x)
s.t.
‚ü®p, x‚ü©‚â§I
x ‚â•0.
To determine empirically a consumer‚Äôs utility function is of course al-
most impossible, so microtheory is hardly useful for quantitative calculations.
However, one can make qualitative analyzes and answer questions of the type:
How does an increase in income change the consumer behavior? and How
does changes in the prices of the goods aÔ¨Äect the purchasing behavior?
Portfolio optimization
A person intends to buy shares in n diÔ¨Äerent companies C1, C2, . . . , Cn for S
dollars. One dollar invested in the company Cj gives a return of Rj dollars,
where Rj is a random variable with known expected value
¬µj = E[Rj].
The covariances
œÉij = E[(Ri ‚àí¬µi)(Rj ‚àí¬µj)]
are also assumed to be known.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
25
Optimization
The expected total return e(x) from investing x = (x1, x2, . . . , xn) dollars
in the companies C1, C2, . . . , Cn is given by
e(x) = E
 n

j=1
xjRj

=
n

j=1
¬µjxj,
and the variance of the total return is
v(x) = Var
 n

j=1
xjRj

=
n

i,j=1
œÉijxixj.
Note that v(x) is a positive semi-deÔ¨Ånite quadratic form.
It is not possible for our person to maximize the total return, because
the return is a random variable, i.e. depends on chance. However, he can
maximize the expected total return under appropriate risk conditions, i.e.
requirements for the variance. Alternatively, he can minimize the risk with
the investment given certain requirements on the expected return.
Thus
there are several possible strategies, and we will formulate three such.
(i) The strategy to maximize the expected total return, given an upper
bound B on the variance, leads to the convex optimization problem
max e(x)
s.t.
Ô£±
Ô£≤
Ô£≥
v(x) ‚â§B
x1 + x2 + ¬∑ ¬∑ ¬∑ + xn = S
x ‚â•0.
(ii) The strategy to minimize the variance of the total return, given a lower
bound b on the expected return, gives rise to the convex quadratic program-
ming problem
min v(x)
s.t.
Ô£±
Ô£≤
Ô£≥
e(x) ‚â•b
x1 + x2 + ¬∑ ¬∑ ¬∑ + xn = S
x ‚â•0.
(iii) The two strategies can be considered together in the following way. Let
œµ ‚â•0 be a (subjective) parameter, and consider the convex quadratic problem
min œµv(x) ‚àíe(x)
s.t.
x1 + x2 + ¬∑ ¬∑ ¬∑ + xn = S
x ‚â•0
with optimal solution x(œµ). We leave as an exercise to show that
v(x(œµ1)) ‚â•v(x(œµ2))
and
e(x(œµ1)) ‚â•e(x(œµ2))
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
26
Optimization
if 0 ‚â§œµ1 ‚â§œµ2. The parameter œµ is thus a measure of the person‚Äôs attitude
towards risk; the smaller the œµ, the greater the risk (= variance) but also the
greater expected return.
Snell‚Äôs law of refraction
We will study the path of a light beam which passes through n parallel
transparent layers. The j:th slice Sj is assumed to be aj units wide and to
consist of a homogeneous medium in which the speed of light is vj. We choose
a coordinate system as in Ô¨Ågure 9.2 and consider a light beam on its path
from the origin on the surface of the Ô¨Årst slice to a point with y-coordinate
b on the outer surface of the last slice.
Œ∏j
S1
S2
Sj
Sn
aj
yj
(x, b)
x
y
Figure 9.2. The path of a light beam through
layers with diÔ¨Äerent refraction indices.
According to Fermat‚Äôs principle, the light chooses the fastest route. The
path of the beam is therefore determined by the optimal solution to the
convex optimization problem
min
n

j=1
v‚àí1
j

y2
j + a2
j
s.t.
n

j=1
yj = b,
and we obtain Snell‚Äôs law of refraction
sin Œ∏i
sin Œ∏j
= vi
vj
by solving the problem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
27
Optimization
27
Overdetermined systems
If a system of linear equations Ax = b with n unknowns and m equations
is inconsistent, i.e. has no solutions, you might want to still determine the
best approximate solution, i.e. the n-tuple x = (x1, x2, . . . , xn) that makes
the error as small as possible. The error is by deÔ¨Ånition the diÔ¨Äerence Ax‚àíb
between the left and the right hand side of the equation, and as a measure
of the size of the error we use ‚à•Ax ‚àíb‚à•for some suitably chosen norm.
The function x ‚Üí‚à•Ax ‚àíb‚à•is convex, so the problem of minimizing
‚à•Ax ‚àíb‚à•over all x ‚ààRn is a convex problem regardless of which norm is
used, but the solution depends on the norm, of course. Let as usual aij denote
the element at location i, j in the matrix A, and let b = (b1, b2, . . . , bm).
1. The so-called least square solution is obtained by using the Euclidean
norm ‚à•¬∑‚à•2. Since ‚à•Ax ‚àíb‚à•2
2 = m
i=1(ai1x1 + ai2x2 + ¬∑ ¬∑ ¬∑ + ainxn ‚àíbi)2, we get
the least square solution as the solution of the convex quadratic problem
minimize
m

i=1
(ai1x1 + ai2x2 + ¬∑ ¬∑ ¬∑ + ainxn ‚àíbi)2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
28
Optimization
The gradient of the objective function is equal to zero at the optimal point,
which means that the optimal solution is obtained as the solution to the
linear system
ATAx = ATb.
2.
By instead using the ‚à•¬∑‚à•‚àûnorm, one obtains the solution that gives the
smallest maximum deviation between the left and the right hand side of the
linear system Ax = b. Since
‚à•Ax ‚àíb‚à•‚àû= max
1‚â§i‚â§m |ai1x1 + ai2x2 + ¬∑ ¬∑ ¬∑ + ainxn ‚àíbi|,
the objective function is now piecewise aÔ¨Éne, and the problem is therefore
equivalent to the LP problem
min
t
s.t.
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
¬±(a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn ‚àíb1) ‚â§t
...
¬±(am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn ‚àíbm) ‚â§t.
3.
Instead of minimizing the sum of squares of the diÔ¨Äerences between left
and right sides, we can of course minimize the sum of the absolute value of
the diÔ¨Äerences, i.e. use the ‚à•¬∑‚à•1-norm. Since the objective function
‚à•Ax ‚àíb‚à•1 =
m

i=1
|ai1x1 + ai2x2 + ¬∑ ¬∑ ¬∑ + ainxn ‚àíbi|
is a sum of convex piecewise aÔ¨Éne functions, our convex minimization prob-
lem is in this case equivalent to the LP problem
min t1 + t2 + ¬∑ ¬∑ ¬∑ + tm
s.t.
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
¬±(a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn ‚àíb1) ‚â§t1
...
¬±(am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn ‚àíbm) ‚â§tm.
Largest inscribed ball
A convex set X with nonempty interior is given in Rn, and we want to
determine a ball B(x, r) in X (with respect to a given norm) with the largest
possible radius r. We assume that X can be described as the solution set to
a system of inequalities, i.e. that
X = {x ‚ààRn | gi(x) ‚â§0, i = 1, 2, . . . , m},
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
29
Optimization
with convex functions gi.
The ball B(x, r) lies in X if and only if gi(x + ry) ‚â§0 for all y with
‚à•y‚à•‚â§1 and i = 1, 2, . . . , m, which makes it natural to consider the functions
hi(x, r) = sup
‚à•y‚à•‚â§1
gi(x + ry),
i = 1, 2, . . . , m.
The functions hi are convex since they are deÔ¨Åned as suprema of convex
functions in the variables x and r.
The problem of determining the ball with the largest possible radius has
now been transformed into the convex optimization problem
max r
s.t.
hi(x, r) ‚â§0,
i = 1, 2, . . . , m.
For general convex sets X, it is of course impossible to determine the
functions hi explicitly, but if X is a polyhedron, gi(x) = ‚ü®ci, x‚ü©‚àíbi, and the
norm in question is the ‚Ñìp-norm, then it follows from H¬®older‚Äôs inequality that
hi(x, r) = sup
‚à•y‚à•p‚â§1
(‚ü®ci, x‚ü©+ r‚ü®ci, y‚ü©‚àíbi) = ‚ü®ci, x‚ü©+ r‚à•ci‚à•q ‚àíbi
for r ‚â•0, where ‚à•¬∑‚à•q denotes the dual norm.
The problem of determining the center x and the radius r of the largest
ball that is included in the polyhedron
X = {x ‚ààRn | ‚ü®ci, x‚ü©‚â§bi, i = 1, 2, . . . , m}
has now been reduced to the LP problem
max r
s.t.
‚ü®ci, x‚ü©+ r‚à•ci‚à•q ‚â§bi,
i = 1, 2, . . . , m.
Exercises
9.1 In a chemical plant one can use four diÔ¨Äerent processes P1, P2, P3, and P4 to
manufacture the products V1, V2, and V3. Produced quantities of the various
products, measured in tons per hour, for the various processes are shown in
the following table:
P1
P2
P3
P4
V1
‚àí1
2
2
1
V2
4
1
0
2
V3
3
1
2
1
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
30
Optimization
(Process P1 thus consumes 1 ton of V1 per hour!) Running processes P1,
P2, P3, and P4 costs 5 000, 4 000, 3 000, and 4 000 dollars per per hour,
respectively. The plant intends to produce 16, 40, and 24 tons of products V1,
V2, and V3 at the lowest possible cost. Formulate the problem of determining
an optimal production schedule.
9.2 Bob has problems with the weather. The weather occurs in the three states
pouring rain, drizzle and sunshine. Bob owns a raincoat and an umbrella,
and he is somewhat careful with his suit. The raincoat is diÔ¨Écult to carry,
and the same applies ‚àíthough to a lesser degree ‚àíto the umbrella; the
latter, however, is not fully satisfactory in case of pouring rain. The following
table reveals how happy Bob considers himself in the various situations that
can arise (the numbers are related to his blood pressure, with 0 corresponding
to his normal state).
Pouring rain
Drizzle
Sunshine
Raincoat
2
1
‚àí2
Umbrella
1
2
‚àí1
Only suit
‚àí4
‚àí2
2
In the morning, when Bob goes to work, he does not know what the weather
will be like when he has to go home, and he would therefore choose the clothes
that optimize his mind during the walk home. Formulate Bob‚Äôs problem as
an LP problem.
9.3 Consider the following two-person game in which each player has three al-
ternatives and where the payment to the row player is given by the following
payoÔ¨Ämatrix.
1
2
3
1
1
0
5
2
3
3
4
3
2
4
0
In this case, it is obvious which alternatives both players must choose. How
will they play?
9.4 Charlie and Rick have three cards each. Both have the ace of diamonds
and the ace of spades. Charlie also has the two of diamonds, and Rick has
the two of spades. The players play simultaneously one card each. Charlie
wins if both these cards are of the same color and loses in the opposite case.
The winner will receive as payment the value of his winning card from the
opponent, with ace counting as 1. Write down the payoÔ¨Ämatrix for this two-
person game, and formulate column player Charlie‚Äôs problem to optimize his
expected proÔ¨Åt as an LP problem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
31
Optimization
31
9.5 The overdetermined system
Ô£±
Ô£≤
Ô£≥
x1 + x2 = 2
x1 ‚àíx2 = 0
3x1 + 2x2 = 4
has no solution.
a) Determine the least square solution.
b) Formulate the problem of determining the solution that minimizes the
maximum diÔ¨Äerence between the left and the right hand sides of the
system.
c) Formulate the problem of determining the solution that minimizes the
sum of the absolut values of the diÔ¨Äerences between the left and the right
hand sides.
9.6 Formulate the problem of determining
a) the largest circular disc,
b) the largest square with sides parallel to the coordinate axes,
that is contained in the triangle bounded by the lines x1‚àíx2 = 0, x1‚àí2x2 = 0
and x1 + x2 = 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
32
The Lagrange function
Chapter 10
The Lagrange function
10.1
The Lagrange function and the dual prob-
lem
The Lagrange function
To the minimization problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with x ‚àà‚Ñ¶as implicit condition and m explicit constraints, the Ô¨Årst p of
which in the form of inequalities, we shall associate a dual maximization
problem, and the tool to accomplish this is the Lagrange function deÔ¨Åned
below. To avoid trivial matters we assume that dom f Ã∏= ‚àÖ, i.e. that the
objective function f : ‚Ñ¶‚ÜíR is not identically equal to ‚àûon ‚Ñ¶.
X denotes as before the set of feasible points in the problem (P), i.e.
X = {x ‚àà‚Ñ¶| g1(x) ‚â§0, . . . , gp(x) ‚â§0, gp+1(x) = 0, . . . , gm(x) = 0},
and vmin(P) is the optimal value of the problem.
DeÔ¨Ånition. Let
Œõ = Rp
+ √ó Rm‚àíp.
The function L: ‚Ñ¶√ó Œõ ‚ÜíR, deÔ¨Åned by
L(x, Œª) = f(x) +
m

i=1
Œªigi(x),
is called the Lagrange function of the minimization problem (P), and the
variables Œª1, Œª2, . . . , Œªm are called Lagrange multipliers.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
33
The Lagrange function
For each x ‚ààdom f, the expression L(x, Œª) is the sum of a real number
and a linear form in Œª1, Œª2, . . . , Œªm. Hence, the function Œª ‚ÜíL(x, Œª) is aÔ¨Éne
(or rather, the restriction to Œõ of an aÔ¨Éne function on Rm). The Lagrange
function is thus especially concave in the variable Œª for each Ô¨Åxed x ‚ààdom f.
If x ‚àà‚Ñ¶\ dom f, then obviously L(x, Œª) = ‚àûfor all Œª ‚ààŒõ. Hence,
inf
x‚àà‚Ñ¶L(x, Œª) =
inf
x‚ààdom f L(x, Œª) < ‚àû
for all Œª ‚ààŒõ.
DeÔ¨Ånition. For Œª ‚ààŒõ, we deÔ¨Åne
œÜ(Œª) = inf
x‚àà‚Ñ¶L(x, Œª)
and call the function œÜ: Œõ ‚ÜíR the dual function associated to the mini-
mization problem (P).
It may of course happen that the domain
dom œÜ = {Œª ‚ààŒõ | œÜ(Œª > ‚àí‚àû}
of the dual function is empty; this occurs if the functions x ‚ÜíL(x, Œª) are
unbounded below on ‚Ñ¶for all Œª ‚ààŒõ.
Theorem 10.1.1. The dual function œÜ of the minimization problem (P) is
concave and
œÜ(Œª) ‚â§vmin(P)
for all Œª ‚ààŒõ.
Hence, dom œÜ = ‚àÖif the objective function f in the original problem (P) is
unbounded below on the constraint set, i.e. if vmin(P) = ‚àí‚àû.
Proof. The functions Œª ‚ÜíL(x, Œª) are concave for x ‚ààdom f, which means
that the function œÜ is the inÔ¨Åmum of a family of concave functions.
It
therefore follows from Theorem 6.2.4 in Part I that œÜ is concave.
Suppose Œª ‚ààŒõ and x ‚ààX; then Œªigi(x) ‚â§0 for i ‚â§p and Œªigi(x) = 0 for
i > p, and it follows that
L(x, Œª) = f(x) +
n

i=1
Œªigi(x) ‚â§f(x),
and that consequently
œÜ(Œª) = inf
x‚àà‚Ñ¶L(x, Œª) ‚â§inf
x‚ààX L(x, Œª) ‚â§inf
x‚ààX f(x) = vmin(P).
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
34
The Lagrange function
The following optimality criterion is now an immediate consequence of
the preceding theorem.
Theorem 10.1.2 (Optimality criterion). Suppose ÀÜx is a feasible point for the
minimization problem (P) and that there is a point ÀÜŒª ‚ààŒõ such that
œÜ(ÀÜŒª) = f(ÀÜx).
Then ÀÜx is an optimal solution.
Proof. The common value f(ÀÜx) belongs to the intersection R‚à©R = R of the
codomains of f and œÜ, and it is thus a real number, and by Theorem 10.1.1,
f(ÀÜx) ‚â§vmin(P). Hence, f(ÀÜx) = vmin(P).
Example 10.1.1. Let us consider the simple minimization problem
min f(x) = x2
1 ‚àíx2
2
s.t.
x2
1 + x2
2 ‚â§1.
The Lagrange function is
L(x1, x2, Œª) = x2
1 ‚àíx2
2 + Œª(x2
1 + x2
2 ‚àí1)
= (Œª + 1)x2
1 + (Œª ‚àí1)x2
2 ‚àíŒª
with (x1, x2) ‚ààR2 and Œª ‚ààR+.
The Lagrange function is unbounded below when 0 ‚â§Œª < 1, and it
attains the minimum value ‚àíŒª for x1 = x2 = 0 when Œª ‚â•1, so the dual
function œÜ is given by
œÜ(Œª) =

‚àí‚àû,
if 0 ‚â§Œª < 1
‚àíŒª ,
if Œª ‚â•1.
We Ô¨Ånally note that the optimality condition œÜ(ÀÜŒª) = f(ÀÜx) is satisÔ¨Åed by
the point ÀÜx = (0, 1) and the Lagrange multiplier ÀÜŒª = 1. Hence, (0, 1) is an
optimal solution.
The optimality criterion gives a suÔ¨Écient condition for optimality, but it
is not necessary, as the following trivial example shows.
Example 10.1.2. Consider the problem
min f(x) = x
s.t.
x2 ‚â§0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
35
The Lagrange function
35
There is only one feasible point, ÀÜx = 0, which is therefore the optimal solu-
tion. The Lagrange function L(x, Œª) = x + Œªx2 is bounded below for Œª > 0
and
œÜ(Œª) = inf
x‚ààR(x + Œªx2) =

‚àí1/4Œª,
if Œª > 0
‚àí‚àû,
if Œª = 0.
But œÜ(Œª) < 0 = f(ÀÜx) for all Œª ‚ààŒõ = R+, so the optimality criterion in
Theorem 10.1.2 is not satisÔ¨Åed by the optimal point.
For the converse of Theorem 10.1.2 to hold, some extra condition is thus
needed, and we describe such a condition in Chapter 11.1.
The dual problem
In order to obtain the best possible lower estimate of the optimal value of
the minimization problem (P), we should, in the light of Theorem 10.1.1,
maximize the dual function. This leads to the following deÔ¨Ånition.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
36
The Lagrange function
DeÔ¨Ånition. The optimization problem
(D)
max œÜ(Œª)
s.t.
Œª ‚ààŒõ
is called the dual problem of the minimization problem (P).
The dual problem is a convex problem, irrespective of whether the prob-
lem (P) is convex or not, because the dual function is concave. The value of
the dual problem will be denoted by vmax(D) with the usual conventions for
¬±‚àû-values.
Our next result is now an immediate corollary of Theorem 10.1.1.
Theorem 10.1.3 (Weak duality). The following inequality holds between the
optimal values of the problem(P) and its dual problem (D):
vmax(D) ‚â§vmin(P).
The inequality in the above theorem is called weak duality.
If the two
optimal values are equal, i.e. if
vmax(D) = vmin(P)
then we say that strong duality holds for problem (P).
Weak duality thus holds for all problems while strong duality only holds
for special types of problems. Of course, strong duality prevails if the opti-
mality criterion in Theorem 10.1.2 is satisÔ¨Åed.
Example 10.1.3. Consider the minimization problem
min x3
1 + 2x2
s.t.
x2
1 + x2
2 ‚â§1.
It is easily veriÔ¨Åed that the minimum is attained for x = (0, ‚àí1) and that
the optimal value is vmin(P) = ‚àí2. The Lagrange function
L(x1, x2, Œª) = x3
1 + 2x2 + Œª(x2
1 + x2
2 ‚àí1) = x3
1 + Œªx2
1 + 2x2 + Œªx2
2 ‚àíŒª
tends, for each Ô¨Åxed Œª ‚â•0, to ‚àí‚àûas x2 = 0 and x1 ‚Üí‚àí‚àû. The Lagrange
function is in other words unbounded below on R2 for each Œª, and hence
œÜ(Œª) = ‚àí‚àûfor all Œª ‚ààŒõ.
The value of the dual problem is therefore
vmax(D) = ‚àí‚àû, so strong duality does not hold in this problem.
The Lagrange function, the dual function and the dual problem of a
minimization problem of the type (P) are deÔ¨Åned in terms of the constraint
functions of the problem.
Therefore, it may be worth emphasizing that
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
37
The Lagrange function
problems that are equivalent in the sense that they have the same objective
function f and the same set X of feasible points do not necessarily have
equivalent dual problems.
Thus, strong duality may hold for one way of
framing a problem but fail to hold for other ways. See exercise 10.2.
Example 10.1.4. Let us Ô¨Ånd the dual problem of the LP problem
(LP-P)
min ‚ü®c, x‚ü©
s.t.
Ax ‚â•b
x ‚â•0.
Here A is an m √ó n-matrix, c is a vector in Rn and b a vector in Rm. Let us
rewrite the problem in the form
min ‚ü®c, x‚ü©
s.t.
 b ‚àíAx ‚â§0
x ‚ààRn
+
with x ‚ààRn
+ as an implicit constraint. The matrix inequality b ‚àíAx ‚â§0
consists of m linear inequalities, and the Lagrangefunction is therefore deÔ¨Åned
on the product set Rn
+ √ó Rm
+, and it is given by
L(x, Œª) = ‚ü®c, x‚ü©+ ‚ü®Œª, b ‚àíAx‚ü©= ‚ü®c ‚àíATŒª, x‚ü©+ ‚ü®b, Œª‚ü©.
For Ô¨Åxed Œª, L(x, Œª) is bounded below on the set Rn
+ if and only if c‚àíATŒª ‚â•0,
with minimum value equal to ‚ü®b, Œª‚ü©attained at x = 0. The dual function
œÜ: Rm
+ ‚ÜíR is thus given by
œÜ(Œª) =

‚ü®b, Œª‚ü©,
if ATŒª ‚â§c
‚àí‚àû,
otherwise.
The dual problem to the LP problem (LP-P) is therefore also an LP problem,
namely (after renaming the parameter Œª to y) the LP problem
(LP-D)
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c
y ‚â•0.
Note the beautiful symmetry between the two problems.
By weak duality, we know for sure that the optimal value of the maximiza-
tion problem is less than or equal to the optimal value of the minimization
problem. As we shall see later, strong duality holds for LP problems, i.e. the
two problems above have the same optimal value, provided at least one of
the problems has feasible points.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
38
The Lagrange function
We now return to the general minimization problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with X as the set of feasible points, Lagrange function L: ‚Ñ¶√ó Œõ ‚ÜíR,
and dual function œÜ. Our next theorem shows that the optimality criterion
in Theorem 10.1.2 can be formulated as a saddle point condition on the
Lagrange function.
Theorem 10.1.4. Suppose (ÀÜx, ÀÜŒª) ‚àà‚Ñ¶√óŒõ. The following three conditions are
equivalent for the optimization problem (P):
(i) ÀÜx ‚ààX and f(ÀÜx) = œÜ(ÀÜŒª), i.e. the optimality criterion is satisÔ¨Åed.
(ii) For all (x, Œª) ‚àà‚Ñ¶√ó Œõ,
L(ÀÜx, Œª) ‚â§L(ÀÜx, ÀÜŒª) ‚â§L(x, ÀÜŒª),
i.e. (ÀÜx, ÀÜŒª) is a saddle point for the Lagrange function.
(iii) ÀÜx ‚ààX, ÀÜx minimizes the function x ‚ÜíL(x, ÀÜŒª) when x runs through ‚Ñ¶,
and
ÀÜŒªigi(ÀÜx) = 0
for i = 1, 2, . . . , p.
Thus, ÀÜx is an optimal solution to the problem (P) if any of the equivalent
conditions (i)‚Äì(iii) is satisÔ¨Åed.
The condition in (iii) that ÀÜŒªigi(ÀÜx) = 0 for i = 1, 2, . . . , p is called com-
plementarity. An equivalent way to express this, which explains the name,
is
ÀÜŒªi = 0
or
gi(ÀÜx) = 0.
A constraint with a positive Lagrange multiplier is thus necessarily active at
the point ÀÜx.
Proof. (i) ‚áí(ii): For ÀÜx ‚ààX and arbitrary Œª ‚ààŒõ (= Rp
+ √ó Rn‚àíp) we have
L(ÀÜx, Œª) = f(ÀÜx) +
m

i=1
Œªigi(ÀÜx) = f(ÀÜx) +
p

i=1
Œªigi(ÀÜx) ‚â§f(ÀÜx),
since Œªi ‚â•0 and gi(ÀÜx) ‚â§0 for i = 1, 2, . . . , p. Moreover,
œÜ(ÀÜŒª) = inf
z‚àà‚Ñ¶L(z, ÀÜŒª) ‚â§L(x, ÀÜŒª)
for all x ‚àà‚Ñ¶.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
39
The Lagrange function
39
If f(ÀÜx) = œÜ(ÀÜŒª), then consequently
L(ÀÜx, Œª) ‚â§f(ÀÜx) = œÜ(ÀÜŒª) ‚â§L(x, ÀÜŒª)
for all (x, Œª) ‚àà‚Ñ¶√ó Œõ, and by the particular choice of x = ÀÜx, Œª = ÀÜŒª in
this inequality, we see that f(ÀÜx) = L(ÀÜx, ÀÜŒª). This proves the saddle point
inequality in (ii) with L(ÀÜx, ÀÜŒª) = f(ÀÜx).
(ii) ‚áí(iii): It is obvious that ÀÜx minimizes the function L( ¬∑ , ÀÜŒª) if and only
if the right part of the saddle point inequality holds. The minimum value is
moreover Ô¨Ånite (due to our tacit assumption dom f Ã∏= ‚àÖ), and hence f(ÀÜx) is
a Ô¨Ånite number.
The left part of the saddlepoint inequality means that
f(ÀÜx) +
m

i=1
Œªigi(ÀÜx) ‚â§f(ÀÜx) +
m

i=1
ÀÜŒªigi(ÀÜx)
for all Œª ‚ààŒõ, or equivalently that
m

i=1
(Œªi ‚àíÀÜŒªi)gi(ÀÜx) ‚â§0
for all Œª ‚ààŒõ.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
40
The Lagrange function
Now Ô¨Åx the index k and choose in the above inequality the number Œª so
that Œªi = ÀÜŒªi for all i except i = k. It follows that
(10.1)
(Œªk ‚àíÀÜŒªk)gk(ÀÜx) ‚â§0
for all such Œª.
If k > p, we choose Œªk = ÀÜŒªk ¬± 1 with the conclusion that ¬±gk(ÀÜx) ‚â§0, i.e.
that gk(ÀÜx) = 0. For k ‚â§p we instead choose Œªk = ÀÜŒªk +1, with the conclusion
that gk(ÀÜx) ‚â§0. Thus, ÀÜx satisÔ¨Åes all the constraints, i.e. ÀÜx ‚ààX.
For k ‚â§p we Ô¨Ånally choose Œªk = 0 and Œªk = 2ÀÜŒªk, respectively, in the
inequality (10.1) with ¬±ÀÜŒªkgk(ÀÜx) ‚â§0 as result. This means that ÀÜŒªkgk(ÀÜx) = 0
for k ‚â§p, and the implication (ii) ‚áí(iii) is now proved.
(iii) ‚áí(i): From (iii) follows at once
œÜ(ÀÜŒª) = inf
x‚àà‚Ñ¶L(x, ÀÜŒª) = L(ÀÜx, ÀÜŒª) = f(ÀÜx) +
m

i=1
ÀÜŒªigi(ÀÜx) = f(ÀÜx),
which is condition (i).
If the objective and constraint functions f and g1, g2, . . . , gm are diÔ¨Äer-
entiable, so is the Lagrange function L(x, Œª) = f(x) + m
i=1 Œªigi(x), and we
use L‚Ä≤
x(x0, Œª) as the notation for the value of the derivative of the function
x ‚ÜíL(x, Œª) at the point x0, i.e.
L‚Ä≤
x(x0, Œª) = f ‚Ä≤(x0) +
m

i=1
Œªig‚Ä≤
i(x0).
If the diÔ¨Äerentiable function x ‚ÜíL(x, Œª) has a minimum at an inte-
rior point x0 in ‚Ñ¶, then L‚Ä≤
x(x0, Œª) = 0. The following corollary is thus an
immediate consequence of the implication (i) ‚áí(iii) in Theorem 10.1.4.
Corollary 10.1.5. Suppose that ÀÜx is an optimal solution to the minimization
problem (P), that ÀÜx is an interior point of the domain ‚Ñ¶, that the objec-
tive and constraint functions are diÔ¨Äerentiable at ÀÜx, and that the optimality
criterion f(ÀÜx) = œÜ(ÀÜŒª) is satisÔ¨Åed by some Lagrange multiplier ÀÜŒª ‚ààŒõ. Then
(KKT)

L‚Ä≤
x(ÀÜx, ÀÜŒª) = 0
and
ÀÜŒªigi(ÀÜx) = 0
for i = 1, 2, . . . , p.
The system (KKT) is called the Karush‚ÄìKuhn‚ÄìTucker condition.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
41
The Lagrange function
The equality L‚Ä≤
x(ÀÜx, ÀÜŒª) = 0 means that
f ‚Ä≤(ÀÜx) +
m

i=1
ÀÜŒªig‚Ä≤
i(ÀÜx) = 0,
which written out in more detail becomes
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àÇf
‚àÇx1
(ÀÜx) +
m

i=1
ÀÜŒªi
‚àÇgi
‚àÇx1
(ÀÜx) = 0
...
‚àÇf
‚àÇxn
(ÀÜx) +
m

i=1
ÀÜŒªi
‚àÇgi
‚àÇxn
(ÀÜx) = 0.
Example 10.1.5. In Example 10.1.1 we found that ÀÜx = (0, 1) is an optimal
solution to the minimization problem
min x2
1 ‚àíx2
2
s.t.
x2
1 + x2
2 ‚â§1
and that the optimality criterion is satisÔ¨Åed with ÀÜŒª = 1.
The Lagrange
function is L(x, Œª) = x2
1 ‚àíx2
2 + Œª(x2
1 + x2
2 ‚àí1), and indeed, x = (0, 1) and
Œª = 1 satisfy the KKT-system
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àÇL(x, Œª)
‚àÇx1
= 2(Œª + 1)x1 = 0
‚àÇL(x, Œª)
‚àÇx1
= 2(Œª ‚àí1)x2 = 0
Œª(x2
1 + x2
2 ‚àí1) = 0.
10.2
John‚Äôs theorem
Conditions which guarantee that the KKT condition is satisÔ¨Åed at an optimal
point, are usually called constraint qualiÔ¨Åcation conditions, and in the next
chapter we will describe such a condition for convex problems. In this section
we will study a diÔ¨Äerent qualifying condition, John‚Äôs condition, for general
optimization problems with constraints in the form of inequalities.
Let us therefore consider a problem of the form
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , m
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
42
The Lagrange function
42
with implicit constraint set ‚Ñ¶, i.e. domain for the objective and the constraint
functions.
Whether a constraint is active or not at an optimal point plays a ma-
jor role, and aÔ¨Éne constraints are thereby easier to handle than other con-
straints. Therefore, we introduce the following notations:
IaÔ¨Ä(x) = {i | the function gi is aÔ¨Éne and gi(x) = 0},
Ioth(x) = {i | the function gi is not aÔ¨Éne and gi(x) = 0},
I(x) = IaÔ¨Ä(x) ‚à™Ioth(x).
So IaÔ¨Ä(x) consists of the indices of all active aÔ¨Éne constraints at the point
x, Ioth(x) consists of the indices of all other active constraints at the point,
and I(x) consists of the indices of all active constraints at the point.
Theorem 10.2.1 (John‚Äôs theorem). Suppose ÀÜx is a local minimum point for
the problem (P), that ÀÜx is an interior point in ‚Ñ¶, and that the functions f
and g1, g2, . . . , gm are diÔ¨Äerentiable at the point ÀÜx. If there exists a vector
z ‚ààRn such that
(J)

‚ü®g‚Ä≤
i(ÀÜx), z‚ü©‚â•0
for all i ‚ààIaÔ¨Ä(ÀÜx)
‚ü®g‚Ä≤
i(ÀÜx), z‚ü©> 0
for all i ‚ààIoth(ÀÜx),
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
43
The Lagrange function
then there exist Lagrange parameters ÀÜŒª ‚ààRm
+ such that
(KKT)

L‚Ä≤
x(ÀÜx, ÀÜŒª) = 0
ÀÜŒªigi(ÀÜx) = 0
for i = 1, 2, . . . , m.
Remark 1. According to Theorem 3.3.5 in Part I, the system (J) is solvable
if and only if
(J‚Ä≤)
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥

i‚ààI(ÀÜx)
uig‚Ä≤
i(ÀÜx) = 0
u ‚â•0
‚áíui = 0
for all i ‚ààIoth(ÀÜx).
The system (J) is thus in particular solvable if the gradient vectors ‚àági(ÀÜx)
are linearly independent for i ‚ààI(ÀÜx).
Remark 2. If Ioth(ÀÜx) = ‚àÖ, then (J) is trivially satisÔ¨Åed by z = 0.
Proof. Let Z denote the set of solutions to the system (J). The Ô¨Årst part
of the proof consists in showing that Z is a subset of the conic halfspace
{z ‚ààRn | ‚àí‚ü®f ‚Ä≤(ÀÜx), z‚ü©‚â•0}.
Assume therefore that z ‚ààZ and consider the halÔ¨Çine ÀÜx ‚àítz for t ‚â•0.
We claim that ÀÜx ‚àítz ‚ààX for all suÔ¨Éciently small t > 0.
If g is an aÔ¨Éne function, i.e. has the form g(x) = ‚ü®c, x‚ü©+b, then g‚Ä≤(x) = c
and g(x + y) = ‚ü®c, x + y‚ü©+ b = ‚ü®c, x‚ü©+ b + ‚ü®c, y‚ü©= g(x) + ‚ü®g‚Ä≤(x), y‚ü©for all x
and y. Hence, for all indices i ‚ààIaÔ¨Ä(ÀÜx),
gi(ÀÜx ‚àítz) = gi(ÀÜx) ‚àít‚ü®g‚Ä≤
i(ÀÜx), z‚ü©= ‚àít‚ü®g‚Ä≤
i(ÀÜx), z‚ü©‚â§0
for all t ‚â•0.
For indices i ‚ààIoth(ÀÜx), we obtain instead, using the chain rule, the in-
equality
d
dt gi(ÀÜx ‚àítz)|t=0 = ‚àí‚ü®g‚Ä≤
i(ÀÜx), z‚ü©< 0.
The function t ‚Üígi(ÀÜx ‚àítz) is in other words decreasing at the point t = 0,
whence gi(ÀÜx ‚àítz) < gi(ÀÜx) = 0 for all suÔ¨Éciently small t > 0.
If the i:th constraint is inactive at ÀÜx, i.e. if i /‚ààI(ÀÜx), then gi(ÀÜx) < 0, and
it follows from continuity that gi(ÀÜx ‚àítz) < 0 for all suÔ¨Éciently small t > 0.
We have thus proved that the points ÀÜx ‚àítz belong to the constraint set
X if t > 0 is suÔ¨Éciently small. Since ÀÜx is a local minimum point of f, it
follows that f(ÀÜx ‚àítz) ‚â•f(ÀÜx) for all suÔ¨Éciently small t > 0. Consequently,
‚àí‚ü®f ‚Ä≤(ÀÜx), z‚ü©= d
dt f(ÀÜx ‚àítz)

t=0= lim
t‚Üí0+
f(ÀÜx ‚àítz) ‚àíf(ÀÜx)
t
‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
44
The Lagrange function
g1(x) = 0
g2(x) = 0
‚àág1(ÀÜx)
‚àág2(ÀÜx)
-‚àáf(ÀÜx)
X
ÀÜx
Figure 10.1. Illustration for Example 10.2.1: The vector ‚àí‚àáf(ÀÜx) does
not belong to the cone generated by the gradients ‚àág1(ÀÜx) and ‚àág2(ÀÜx).
This proves the alleged inclusion
Z ‚äÜ{z ‚ààRn | ‚àí‚ü®f ‚Ä≤(ÀÜx), z‚ü©‚â•0} = {‚àíf ‚Ä≤(ÀÜx)}+ =

con{‚àíf ‚Ä≤(ÀÜx)}
+,
and it now follows from Theorem 3.2.1, Corollary 3.2.4 and Theorem 3.3.4
in Part I that
con{‚àíf ‚Ä≤(ÀÜx)} ‚äÜZ+ = con{g‚Ä≤
i(ÀÜx) | i ‚ààI(ÀÜx)}.
So the vector ‚àíf ‚Ä≤(ÀÜx) belongs to the cone generated by the vektors g‚Ä≤
i(ÀÜx),
i ‚ààI(ÀÜx), which means that there are nonnegative integers ÀÜŒªi, i ‚ààI(ÀÜx), such
that
‚àíf ‚Ä≤(ÀÜx) =

i‚ààI(ÀÜx)
ÀÜŒªig‚Ä≤
i(ÀÜx).
If we Ô¨Ånally deÔ¨Åne ÀÜŒªi = 0 for i /‚ààI(ÀÜx), then
f ‚Ä≤(ÀÜx) +
m

i=1
ÀÜŒªig‚Ä≤
i(ÀÜx) = 0
and ÀÜŒªigi(ÀÜx) = 0 for i = 1, 2, . . . , m. This means that the KKT-condition is
satisÔ¨Åed.
The condition in John‚Äôs statement that the system (J) has a solution
can be replaced with other qualifying constraints but can not be completely
removed without the conclusion being lost. This is shown by the following
example.
Example 10.2.1. Consider the problem
min f(x) = x1
s.t.
g1(x) = ‚àíx3
1 + x2 ‚â§0
g2(x) =
‚àíx2 ‚â§0
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
45
The Lagrange function
45
with Lagrange function L(x, Œª) = x1+Œª1(x2‚àíx3
1)‚àíŒª2x2. The unique optimal
solution is ÀÜx = (0, 0), but the system L‚Ä≤
x(ÀÜx, Œª) = 0, i.e.

1 = 0
Œª1 ‚àíŒª2 = 0,
has no solutions. This is explained by the fact that the system (J), i.e.
‚àíz2 ‚â•0
z2 > 0,
has no solutions.
Example 10.2.2. We will solve the problem
min x1x2 + x3
s.t.
2x1 ‚àí2x2 + x3 + 1 ‚â§0
x2
1 + x2
2 ‚àíx3
‚â§0
using John‚Äôs theorem. Note Ô¨Årst that the constraints deÔ¨Åne a compact set
X, for the inequalities
x2
1 + x2
2 ‚â§x3 ‚â§‚àí2x1 + 2x2 ‚àí1
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
46
The Lagrange function
imply that (x1 + 1)2 + (x2 ‚àí1)2 ‚â§1, and consequently, ‚àí2 ‚â§x1 ‚â§0,
0 ‚â§x2 ‚â§2, and 0 ‚â§x3 ‚â§7. Since the objective function is continuous,
there is indeed an optimal solution.
Let us now Ô¨Årst investigate whether the system (J) is solvable. We use the
equivalent version (J‚Ä≤) in the remark after the theorem. First note that the
gradients of the constraint functions are never equal to zero. The condition
(J‚Ä≤) is thus met in the points where only one of the constraints is active.
Assume therefore that x is a point where I(x) = {1, 2}, i.e. where both
constraints are active, and that u1(2, ‚àí2, 1) + u2(2x1, 2x2, ‚àí1) = (0, 0, 0). If
u2 > 0, we conclude from the above equation that u1 = u2, x1 = ‚àí1 and
x2 = 1. Inserting x1 = ‚àí1 and x2 = 1 into the two active constraints yields
x3 = 3 and x3 = 2, respectively, which is contradictory. Thus, u2 = 0, which
means that the condition (J‚Ä≤) is fulÔ¨Ålled at all feasible points.
We conclude that the optimal point satisÔ¨Åes the KKT-condition, which
in this instance is as follows
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x2 + 2Œª1 + 2x1Œª2 = 0
(i)
x1 ‚àí2Œª1 + 2x2Œª2 = 0
(ii)
1 + Œª1 ‚àí
Œª2 = 0
(iii)
Œª1(2x1 ‚àí2x2 + x3 + 1) = 0
(iv)
Œª2(x2
1 + x2
2 ‚àíx3) = 0
(v)
The further investigation is divided into two cases.
Œª1 = 0 :
Equation (iii) implies that Œª2 = 1, which inserted into (i) and (ii)
gives x1 = x2 = 0, and from (v) now follows x3 = 0. But this is a false
solution, since (0, 0, 0) /‚ààX.
Œª1 > 0 : Equation (iv) now implies that
2x1 ‚àí2x2 + x3 + 1 = 0.
(vi)
From (i) and (ii) follows (x1 + x2)(1 + 2Œª2) = 0, and since Œª2 ‚â•0,
x1 + x2 = 0.
(vii)
By (iii,) Œª2 > 0. Condition (v) therefore implies that
x2
1 + x2
2 ‚àíx3 = 0.
(viii)
The system consisting of equations (vi), (vii), (viii) has two solutions, namely
ÀÜx = (‚àí1+

1/2, 1‚àí

1/2, 3‚àí2
‚àö
2) and x = (‚àí1‚àí

1/2, 1+

1/2, 3+2
‚àö
2).
Using (i) and (iii), we compute the corresponding Œª and obtain
ÀÜŒª = (‚àí1/2 +

1/2, 1/2 +

1/2) and Œª = (‚àí1/2 ‚àí

1/2, 1/2 ‚àí

1/2),
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
47
The Lagrange function
respectively. Note that ÀÜŒª ‚â•0 and Œª < 0. The system KKT thus has a unique
solution (x, Œª) with Œª ‚â•0, namely x = ÀÜx, Œª = ÀÜŒª. By John‚Äôs theorem, ÀÜx is
the unique optimal solution of our minimization problem, and the optimal
value is 3/2 ‚àí
‚àö
2.
Exercises
10.1 Determine the dual function for the optimization problem
min
x2
1 + x2
2
s.t.
x1 + x2 ‚â•2,
and prove that (1, 1) is an optimal solution by showing that the optimality
criterion is satisÔ¨Åed by ÀÜŒª = 2. Also show that the KKT-condition is satisÔ¨Åed
at the optimal point.
10.2 Consider the two minimization problems
(Pa)
min e‚àíx1
x2
1/x2 ‚â§0
and
(Pb)
min e‚àíx1
|x1| ‚â§0
both with ‚Ñ¶= {(x1, x2) | x2 > 0} as implicit domain. The two problems
have the same set X = {(0, x2) | x2 > 0} of feasible points and the same
optimal value vmin = 1. Find their dual functions and dual problems, and
show that strong duality holds for (Pb) but not for (Pa).
10.3 Suppose the function f : X √ó Y ‚ÜíR has two saddle points (ÀÜx1, ÀÜy1) and
(ÀÜx2, ÀÜy2). Prove that
a) f(ÀÜx1, ÀÜy1) = f(ÀÜx2, ÀÜy2);
b) (ÀÜx1, ÀÜy2) and (ÀÜx2, ÀÜy1) are saddle points, too.
10.4 Let f : X √ó Y ‚ÜíR be an arbitrary function.
a) Prove that
sup
y‚ààY
inf
x‚ààX
f(x, y) ‚â§inf
x‚ààX
sup
y‚ààY
f(x, y).
b) Suppose there is a point (ÀÜx, ÀÜy) ‚ààX √ó Y such that
sup
y‚ààY
inf
x‚ààX
f(x, y) = inf
x‚ààX
f(x, ÀÜy)
and
inf
x‚ààX
sup
y‚ààY
f(x, y) = sup
y‚ààY
f(ÀÜx, y).
Prove that (ÀÜx, ÀÜy) is a saddle point of the function f if and only if
inf
x‚ààX
f(x, ÀÜy) = sup
y‚ààY
f(ÀÜx, y),
and that the common value then is equal to f(ÀÜx, ÀÜy).
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
48
The Lagrange function
48
10.5 Consider a minimization problem
min
f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , m
with convex diÔ¨Äerentiable constraint functions g1, g2, . . . , gm, and suppose
there is a point x0 ‚ààX = {x | g1(x) ‚â§0, . . . , gm(x) ‚â§0} which satisÔ¨Åes
all non-aÔ¨Éne constraints with strict inequality. Show that the system (J) is
solvable at all points ÀÜx ‚ààX.
[Hint: Show that z = ÀÜx ‚àíx0 satisÔ¨Åes (J).]
10.6 Solve the following optimization problems
a) min
x3
1 + x1x2
2
s.t.
x2
1 + 2x2
2 ‚â§1
x2 ‚â•0
b) max x2
1 + x2
2 + arctan x1x2
s.t.
 x2
1 + x2
2 ‚â§2
0 ‚â§x1 ‚â§x2
c) min
x1x2
s.t.
x2
1 + x1x2 + 4x2
2 ‚â§1
x1 + 2x2 ‚â•0
d) max x2
1x2x3
s.t.
2x1 + x1x2 + x3 ‚â§1
x1, x2, x3 ‚â•0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
49
Convex optimization
Chapter 11
Convex optimization
11.1
Strong duality
We recall that the minimization problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
is called convex if
‚Ä¢ the implicit constraint set ‚Ñ¶is convex,
‚Ä¢ the objective function f is convex,
‚Ä¢ the constraint functions gi are convex for i = 1, 2, . . . , p and aÔ¨Éne for
i = p + 1, . . . , m.
The set X of feasible points is convex in a convex optimization problem,
and the Lagrange function
L(x, Œª) = f(x) +
m

i=1
Œªigi(x)
is convex in the variable x for each Ô¨Åxed Œª ‚ààŒõ = Rp
+ √ó Rm‚àíp, since it is a
conic combination of convex functions.
We have already noted that the optimality criterion in Theorem 10.1.2
need not be fulÔ¨Ålled at an optimal point, not even for convex problems,
because of the trivial counterexample in Example 10.1.2. For the criterion
to be met some additional condition is needed, and a weak one is given in
the next deÔ¨Ånition.
DeÔ¨Ånition. The problem (P) satisÔ¨Åes Slater‚Äôs condition if there is a feasible
point x in the relative interior of ‚Ñ¶such that gi(x) < 0 for each non-aÔ¨Éne
constraint function gi.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
50
Convex optimization
Slater‚Äôs condition is of course vacously fulÔ¨Ålled if all constraint functions
are aÔ¨Éne.
For convex problems that satisfy Slater‚Äôs condition, the optimality cri-
terion is both suÔ¨Écient and necessary for optimality. We have namely the
following result.
Theorem 11.1.1 (Duality theorem). Suppose that the problem (P) is convex
and satisÔ¨Åes Slater‚Äôs condition, and that the optimal value vmin is Ô¨Ånite. Let
œÜ: Œõ ‚ÜíR denote the dual function of the problem. Then there is a point
ÀÜŒª ‚ààŒõ such that
œÜ(ÀÜŒª) = vmin.
Proof. First suppose that all constraints are inequalities, i.e. that p = m, and
renumber the constraints so that the functions gi are convex and non-aÔ¨Éne
for i = 1, 2, . . . , k and aÔ¨Éne for i = k + 1, . . . , m.
Because of Slater‚Äôs condition, the system
gi(x) < 0,
i = 1, 2, . . . , k
gi(x) ‚â§0,
i = k + 1, . . . , m
has a solution in the relative interior of ‚Ñ¶, whereas the system
Ô£±
Ô£≤
Ô£≥
f(x) ‚àívmin < 0
gi(x) < 0,
i = 1, 2, . . . , k
gi(x) ‚â§0,
i = k + 1, . . . , m
lacks solutions in ‚Ñ¶, due to the deÔ¨Ånition of vmin. Therefore, it follows from
Theorem 6.5.1 in Part I that there exist nonnegative scalars ÀÜŒª0, ÀÜŒª1, . . . , ÀÜŒªm
such that at least one of the numbers ÀÜŒª0, ÀÜŒª1, . . . , ÀÜŒªk is positive and
ÀÜŒª0(f(x) ‚àívmin) + ÀÜŒª1g1(x) + ÀÜŒª2g2(x) + ¬∑ ¬∑ ¬∑ + ÀÜŒªmgm(x) ‚â•0
for all x ‚àà‚Ñ¶. Here, the coeÔ¨Écient ÀÜŒª0 has to be positive, because if ÀÜŒª0 = 0
then ÀÜŒª1g1(x) + ¬∑ ¬∑ ¬∑ + ÀÜŒªmgm(x) ‚â•0 for all x ‚àà‚Ñ¶, which contradicts the fact
that the Ô¨Årst mentioned system of inequalities has a solution in ‚Ñ¶. We may
therefore assume, by dividing by ÀÜŒª0 if necessary, that ÀÜŒª0 = 1, and this gives
us the inequality
L(x, ÀÜŒª) = f(x) +
m

i=1
ÀÜŒªigi(x) ‚â•vmin
for all x ‚àà‚Ñ¶. It follows that
œÜ(ÀÜŒª) = inf
x‚àà‚Ñ¶L(x, ÀÜŒª) ‚â•vmin,
which combined with Theorem 10.1.1 yields the desired equality œÜ(ÀÜŒª) = vmin.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
51
Convex optimization
51
If the problem has aÔ¨Éne equality constraints, i.e. if p < m, we replace
each equality gi(x) = 0 with the two inequalities ¬±gi(x) ‚â§0, and it follows
from the already proven case of the theorem that there exist nonnegative
Lagrange multipliers ÀÜŒª1, . . . , ÀÜŒªp, ÀÜ¬µp+1, . . . , ÀÜ¬µm, ÀÜŒΩp+1, . . . , ÀÜŒΩm such that
f(x) +
p

i=1
ÀÜŒªigi(x) +
m

i=p+1
(ÀÜ¬µi ‚àíÀÜŒΩi)gi(x) ‚â•vmin
for all x ‚àà‚Ñ¶, By deÔ¨Åning ÀÜŒªi = ÀÜ¬µi ‚àíÀÜŒΩi for i = p + 1, . . . , m, we obtain a
point ÀÜŒª ‚ààŒõ = Rp
+ √ó Rm‚àíp which satisÔ¨Åes œÜ(ÀÜŒª) ‚â•vmin, and this completes
the proof of the theorem.
By combining Theorem 11.1.1 with Theorem 10.1.2 we get the following
corollary.
Corollary 11.1.2. Suppose that the problem (P) is convex and that it satisÔ¨Åes
Slater‚Äôs condition. Then, a feasible point ÀÜx is optimal if and only if it satisÔ¨Åes
the optimality criterion, i.e. if and only if there exists a ÀÜŒª ‚ààŒõ such that
œÜ(ÀÜŒª) = f(ÀÜx).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
52
Convex optimization
11.2
The Karush‚ÄìKuhn‚ÄìTucker theorem
Variants of the following theorem were Ô¨Årst proved by Karush and Kuhn‚Äì
Tucker, and the theorem is therefore usually called the Karush‚ÄìKuhn‚ÄìTucker
theorem.
Theorem 11.2.1. Let
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
be a convex problem, and suppose that the objective and constraint functions
are diÔ¨Äerentiable at the feasible point ÀÜx.
(i) If ÀÜŒª is a point in Œõ and the pair (ÀÜx, ÀÜŒª) satisÔ¨Åes the KKT-condition

L‚Ä≤
x(ÀÜx, ÀÜŒª) = 0
ÀÜŒªigi(ÀÜx) = 0
for i = 1, 2, . . . , p
then strong duality prevails; ÀÜx is an optimal solution to the problem (P)
and ÀÜŒª is an optimal solution to the dual problem.
(ii) Conversely, if Slater‚Äôs condition is fulÔ¨Ålled and ÀÜx is an optimal solution,
then there exist Lagrange multipliers ÀÜŒª ‚ààŒõ such that (ÀÜx, ÀÜŒª) satisÔ¨Åes the
KKT-condition.
Proof. (i) The KKT-condition implies that ÀÜx is a stationary point of the
convex function x ‚ÜíL(x, ÀÜŒª), and an interior stationary point of a convex
function is a minimum point, according to Theorem 7.2.2 in Part I. Condition
(iii) in Theorem 10.1.4 is thus fulÔ¨Ålled, and this means that the optimality
criterion is satisÔ¨Åed by the pair (ÀÜx, ÀÜŒª).
(ii) Conversely, if Slater‚Äôs condition is satisÔ¨Åed and ÀÜx is an optimal solu-
tion, then the optimality criterion f(ÀÜx) = œÜ(ÀÜŒª) is satisÔ¨Åed by some ÀÜŒª ‚ààŒõ,
according to Theorem 11.1.1. The KKT-condition is therefore met because
of Corollary 10.1.5.
The KKT-condition has a natural geometrical interpretation. Assume for
simplicity that all constraints are inequalities, i.e. that p = m, and let I(ÀÜx)
denote the index set for the constraints that are active at the optimal point
ÀÜx. The KKT-condition means that ÀÜŒªi = 0 for all indices i /‚ààI(ÀÜx) and that
‚àí‚àáf(ÀÜx) =

i‚ààI(ÀÜx)
ÀÜŒªi‚àági(ÀÜx),
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
53
Convex optimization
where all coeÔ¨Écients ÀÜŒªi occuring in the sum are nonnegative. The geometrical
meaning of the above equality is that the vector ‚àí‚àáf(ÀÜx) belongs to the cone
generated by the gradients ‚àági(ÀÜx) of the active inequality constraints. Cf.
Ô¨Ågure 11.1 and Ô¨Ågure 11.2.
f(x) = 3
f(x) = 2
f(x) = 1
ÀÜx
‚àí‚àáf(ÀÜx)
‚àág1(ÀÜx)
‚àág2(ÀÜx)
X
g1(x) ‚â§0
g2(x) ‚â§0
Figure 11.1. The point ÀÜx is opti-
mal since both constraints are ac-
tive at the point and
‚àí‚àáf(ÀÜx) ‚ààcon{‚àág1(ÀÜx), ‚àág2(ÀÜx)}.
f(x) = 2
f(x) = 1
‚àág1(ÀÜx)
‚àág2(ÀÜx)
‚àí‚àáf(ÀÜx)
ÀÜx
x
X
g1(x) ‚â§0
g2(x) ‚â§0
Figure 11.2.
Here the point ÀÜx is
not optimal since
‚àí‚àáf(ÀÜx)
Ã∏‚àà
con{‚àág1(ÀÜx), ‚àág2(ÀÜx)}.
The optimum is instead attained at
x, where ‚àí‚àáf(x) = Œª1‚àág1(x) for
some Œª1 > 0.
Example 11.2.1. Consider the problem
min e x1‚àíx3 + e‚àíx2
(x1 ‚àíx2)2 ‚àíx3 ‚â§0
x3 ‚àí4 ‚â§0.
The objective and the constraint functions are convex. Slater‚Äôs condition is
satisÔ¨Åed, since for instance (1, 1, 1) satisÔ¨Åes both constraints strictly. Accord-
ing to Theorem 11.2.1, x is therefore an optimal solution to the problem if
and only if x solves the system
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
e x1‚àíx3 + 2Œª1(x1 ‚àíx2) = 0
(i)
‚àíe‚àíx2 ‚àí2Œª1(x1 ‚àíx2) = 0
(ii)
‚àíe x1‚àíx3 ‚àíŒª1 + Œª2 = 0
(iii)
Œª1

(x1 ‚àíx2)2 ‚àíx3

= 0
(iv)
Œª2(x3 ‚àí4) = 0
(v)
Œª1, Œª2 ‚â•0
(vi)
It follows from (i) and (vi) that Œª1 > 0, from (iii) and (vi) that Œª2 > 0,
and from (iv) and (v) that x3 = 4 and x1 ‚àíx2 = ¬±2. But x1 ‚àíx2 < 0,
because of (i) and (vi), and hence x1 ‚àíx2 = ‚àí2. By comparing (i) and (ii)
we see that x1 ‚àíx3 = ‚àíx2, i.e.
x1 + x2 = 4. It follows that x = (1, 3, 4)
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
54
Convex optimization
54
and Œª = (e‚àí3/4, 5e‚àí3/4) is the unique solution of the system. The problem
therefore has a unique optimal solution, namely (1, 3, 4). The optimal value
is equal to 2e‚àí3.
11.3
The Lagrange multipliers
In this section we will study how the optimal value vmin(b) of an arbitrary
minimization problem of the type
(Pb)
min f(x)
s.t.
gi(x) ‚â§bi,
i = 1, 2, . . . , p
gi(x) = bi,
i = p + 1, . . . , m
depends on the constraint parameters b1, b2, . . . , bm. The functions f and
g1, g2, . . . , gm are, as previously, deÔ¨Åned on a subset ‚Ñ¶of Rn, b = (b1, . . . , bm)
is a vector in Rm, and
X(b) = {x ‚àà‚Ñ¶| gi(x) ‚â§bi for 1 ‚â§i ‚â§p and gi(x) = bi for p < i ‚â§m}
is the set of feasible points.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
55
Convex optimization
The Lagrange function and the dual function associated to the minimiza-
tion problem (Pb) are denoted by Lb and œÜb, respectively. By deÔ¨Ånition,
Lb(x, Œª) = f(x) +
m

i=1
Œªi(gi(x) ‚àíbi),
and the relationship between the Lagrange functions Lb and Lb belonging to
two diÔ¨Äerent parameter vectors b and b, is therefore given by the equation
Lb(x, Œª) = Lb(x, Œª) +
m

i=1
Œªi(bi ‚àíbi) = Lb(x, Œª) + ‚ü®Œª, b ‚àíb‚ü©.
By forming the inÔ¨Åmum over x ‚àà‚Ñ¶, we immediately get the following relation
for the dual functions:
(11.1)
œÜb(Œª) = œÜb(Œª) + ‚ü®Œª, b ‚àíb‚ü©.
The following theorem gives an interpretation of the Lagrange parameters
in problems which satisfy the optimality criterion in Theorem 10.1.2, and thus
especially for convex problems which satisfy Slater‚Äôs condition.
Theorem 11.3.1. Suppose that the minimization problem (Pb) has an optimal
solution x and that the optimality criterion is satisÔ¨Åed at the point, i.e. that
there are Lagrange multipliers Œª such that œÜb(Œª) = f(x). Then:
(i) The objective function f is bounded below on X(b) for each b ‚ààRm,
so the optimal value vmin(b) of problem (Pb) is Ô¨Ånite if the set X(b) of
feasible points is nonempty, and equal to +‚àûif X(b) = ‚àÖ.
(ii) The vector ‚àíŒª is a subgradient at the point b of the optimal value func-
tion vmin : Rm ‚ÜíR.
(iii) Suppose that the optimality criterion is satisÔ¨Åed in the problem (Pb) for
all b in an open convex set U. The restriction of the function vmin to
U is then a convex function.
Proof. By using weak duality for problem (Pb), the identity (11.1) and the
optimality criterion for problem (Pb), we obtain the following inequality:
vmin(b) =
inf
x‚ààX(b) f(x) ‚â•œÜb(Œª) = œÜb(Œª) + ‚ü®Œª, b ‚àíb‚ü©= f(x) + ‚ü®Œª, b ‚àíb‚ü©
= vmin(b) ‚àí‚ü®Œª, b ‚àíb‚ü©.
It follows, Ô¨Årst, that the optimal value vmin(b) can not be equal to ‚àí‚àû, and
second, that ‚àíŒª is a subgradient of the function vmin at the point b.
If the optimality criterion is satisÔ¨Åed at all b ‚ààU, then vmin has a sub-
gradient at all points in U, and such a function is convex.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
56
Convex optimization
Now suppose that the function vmin is diÔ¨Äerentiable at the point b. The
gradient at the point b is then, by Theorem 8.1.3 in Part I, the unique
subgradient at the point, so it follows from (ii) in the above theorem that
v‚Ä≤
min(b) = ‚àíŒª. This gives us the approximation
vmin(b1 + ‚àÜb1, . . . , bm + ‚àÜbm) ‚âàvmin(b1, . . . , bm) ‚àíŒª1‚àÜb1 ¬∑ ¬∑ ¬∑ ‚àíŒªm‚àÜbm
for small increments ‚àÜbj. So the Lagrange multipliers provide information
about how the optimal value is aÔ¨Äected by small changes in the parameters.
Example 11.3.1. As an illustration of Theorem 11.3.1, let us study the con-
vex problem
min x2
1 + x2
2
s.t.
 x1 + 2x2 ‚â§b1
2x1 + x2 ‚â§b2.
Since it is about minimizing the distance squared from the origin to a poly-
hedron, there is certainly an optimal solution for each right-hand side b, and
since the constraints are aÔ¨Éne, it follows from the Karush‚ÄìKuhn‚ÄìTucker
theorem that the optimal solution satisÔ¨Åes the KKT-condition, which in the
present case is the system
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
2x1 + Œª1 + 2Œª2 = 0
(i)
2x2 + 2Œª1 + Œª2 = 0
(ii)
Œª1(x1 + 2x2 ‚àíb1) = 0
(iii)
Œª2(2x1 + x2 ‚àíb2) = 0
(iv)
Œª1, Œª2 ‚â•0.
We now solve this system by considering four separate cases:
Œª1 = Œª2 = 0 : In this case, x1 = x2 = 0 is the unique solution to the KKT-
system. Thus, the point (0, 0) is optimal provided it is feasible, and so is the
case if and only if b1 ‚â•0 and b2 ‚â•0. The optimal value for these parameter
values is vmin(b) = 0.
Œª1 > 0, Œª2 = 0 : From (i) and (ii), it follows Ô¨Årst that x2 = 2x1 = ‚àíŒª1, and
(iii) then gives x = 1
5(b1, 2b1). This point is feasible if 2x1 + x2 = 4
5b1 ‚â§b2,
and for the Lagrange multiplier Œª1 = ‚àí2
5b1 to be positive, we must also have
b1 < 0. Thus, the point x = 1
5(b1, 2b1) is optimal if b1 < 0 and 4b1 ‚â§5b2, and
the corresponding value is vmin(b) = 1
5b2
1.
Œª1 = 0, Œª2 > 0 : From (i) and (ii), it now follows that x1 = 2x2 = ‚àíŒª2,
which inserted into (iv) gives x =
1
5(2b2, b2).
This is a feasible point if
x1 +2x2 = 4
5b2 ‚â§b1. The Lagrange multiplier Œª2 = ‚àí2
5b2 is positive if b2 < 0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
57
Convex optimization
57
Hence, the point x = 1
5(2b2, b2) is optimal and the optimal value is v(b) = 1
5b2
2,
if b2 < 0 och 4b2 ‚â§5b1.
Œª1 > 0, Œª2 > 0: By solving the subsystem obtained from (iii) and (iv), we
get x = 1
3(2b2 ‚àíb1, 2b1 ‚àíb2), and the equations (i) and (ii) then result in
Œª =
2
9(4b2 ‚àí5b1, 4b1 ‚àí5b2). The two Lagrange multipliers are positive if
5
4b1 < b2 <
4
5b1. For these parameter values, x is the optimal point and
vmin(b) = 1
9(5b2
1 ‚àí8b1b2 + 5b2
2) is the optimal value.
The result of our investigation is summarized in the following table:
vmin(b)
‚àíŒª1 = ‚àÇv
‚àÇb1
‚àíŒª2 = ‚àÇv
‚àÇb2
b1 ‚â•0, b2 ‚â•0
0
0
0
b1 < 0, b2 ‚â•4
5b1
1
5b2
1
2
5b1
0
b2 < 0, b2 ‚â§5
4b1
1
5b2
2
0
2
5b2
5
4b1 < b2 < 4
5b1
1
9(5b2
1 ‚àí8b1b2 + 5b2
2)
2
9(5b1 ‚àí4b2)
2
9(5b2 ‚àí4b1)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
58
Convex optimization
Exercises
11.1 Let b > 0 and consider the following trivial convex optimization problem
min
x2
s.t.
x ‚â•b.
Slater‚Äôs condition is satisÔ¨Åed and the optimal value is attained at the point
ÀÜx = b. Find the number ÀÜŒª which, according to Theorem 11.1.1, satisÔ¨Åes the
optimality criterion.
11.2 Verify in the previous exercise that v‚Ä≤(b) = ÀÜŒª.
11.3 Consider the minimization problem
(P)
min f(x)
s.t.
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
with x ‚àà‚Ñ¶as implicit constraint, and the equivalent epigraph formulation
(P‚Ä≤)
min
t
s.t.
Ô£±
Ô£≤
Ô£≥
f(x) ‚àít ‚â§0,
gi(x) ‚â§0,
i = 1, 2, . . . , p
gi(x) = 0,
i = p + 1, . . . , m
of the problem with (t, x) ‚ààR √ó ‚Ñ¶as implicit constraint.
a) Show that (P‚Ä≤) satisÔ¨Åes Slater‚Äôs condition if and only if (P) does.
b) Determine the relation between the Lagrange functions of the two prob-
lems and the relation between their dual functions.
c) Prove that the two dual problems have the same optimal value, and that
the optimality criterion is satisÔ¨Åed in the minimization problem (P) if
and only if it is satisÔ¨Åed in the problem (P‚Ä≤).
11.4 Prove for convex problems that Slater‚Äôs condition is satisÔ¨Åed if and only if,
for each non-aÔ¨Éne constraint gi(x) ‚â§0, there is a feasible point xi in the
relative interior of ‚Ñ¶such that gi(xi) < 0.
11.5 Let
(Pb)
min
f(x)
s.t.
gi(x) ‚â§bi,
i = 1, 2, . . . , p
gi(x) = bi,
i = p + 1, . . . , m
be a convex problem, and suppose that its optimal value vmin(b) is > ‚àí‚àû
for all right-hand sides b that belong to some convex subset U of Rm. Prove
that the restriction of vmin to U is a convex function.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
59
Convex optimization
11.6 Solve the following convex optimization problems.
a) min ex1‚àíx2 + ex2 ‚àíx1
s.t.
x ‚ààR2
b) min ex1‚àíx2 + ex2 ‚àíx1
s.t.
x2
1 + x2
2 ‚â§
1
x1 + x2 ‚â•‚àí1
c) min
‚àíx1 ‚àí2x2
s.t.
ex1 + x2 ‚â§1
x2 ‚â•0
d) min x1 + 2x2
s.t.
x2
1 + x2
2 ‚â§5
x1 ‚àíx2 ‚â§1
e) min
x1 ‚àíx2
s.t.
0 < x1 ‚â§2
0 ‚â§x2 ‚â§ln x1
f) min ex1 + ex2 + x1x2
s.t.
x1 + x2 ‚â•1
x1, x2 ‚â•0
11.7 Solve the convex optimization problem
min
x2
1 + x2
2 ‚àíln(x1 + x2)
s.t.
Ô£±
Ô£≤
Ô£≥
(x1 ‚àí1)2 + x2
2 ‚â§9
x1 + x2 ‚â•2
x1, x2 ‚â•0.
11.8 Solve the convex optimization problem
min
n

j=1
v‚àí1
j

y2
j + a2
j
s.t.
n
j=1 yj = b
y ‚ààRn
that occurred in our discussion of light refraction in Section 9.4, and verify
Snell‚Äôs law of refraction: sin Œ∏i/ sin Œ∏j = vi/vj, where Œ∏j = arctan yj/aj.
11.9 Lisa has inherited 1 million dollars that she intends to invest by buying
shares in three companies: A, B and C. Company A manufactures mobile
phones, B manufactures antennas for mobile phones, and C manufactures ice
cream. The annual return on an investment in the companies is a random
variable, and the expected return for each company is estimated to be
A
B
C
Expected return:
20%
12%
4%
Lisa‚Äôs expected return if she invests x1, x2, x3 million dollars in the three
companies, is thus equal to
0.2 x1 + 0.12 x2 + 0.04 x3.
The investment risk is by deÔ¨Ånition the variance of the return. To calculate
this we need to know the variance of each company‚Äôs return and the corre-
lation between the returns of the various companies. For obvious reasons,
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
60
Convex optimization
60
there is a strong correlation between sales in companies A and B, while sales
of the company C only depend on whether the summer weather is beau-
tiful or not, and not on the number of mobile phones sold. The so-called
covariance matrix is in our case the matrix
Ô£Æ
Ô£∞
50
40
0
40
40
0
0
0
10
Ô£π
Ô£ª
For those who know some basic probability theory, it is now easy to calculate
the risk ‚àíit is given by the expression
50x2
1 + 80x1x2 + 40x2
2 + 10x2
3.
Lisa, who is a careful person, wants to minimize her investment risk but she
also wants to have an expected return of at least 12 %. Formulate and solve
Lisa‚Äôs optimization problem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
61
Convex optimization
11.10 Consider the consumer problem
max f(x)
s.t.
‚ü®p, x‚ü©‚â§I
x ‚â•0
discussed in Section 9.4, where f(x) is the consumer‚Äôs utility function, as-
sumed to be concave and diÔ¨Äerentiable, I is her disposable income, p =
(p1, p2, . . . , pn) is the price vector and x = (x1, x2, . . . , xn) denotes a con-
sumption bundle.
Suppose that ÀÜx is an optimal solution. The optimal utility v, as well as
ÀÜx, depends on the income I, of course; let us assume that v = v(I) is a
diÔ¨Äerentiable function. Show that under these assumptions
ÀÜxj, ÀÜxk > 0 ‚áí
1
pj
‚àÇf
‚àÇxj

ÀÜx = 1
pk
‚àÇf
‚àÇxk

ÀÜx = dv
dI
ÀÜxj = 0, ÀÜxk > 0 ‚áí
1
pj
‚àÇf
‚àÇxj

ÀÜx ‚â§1
pk
‚àÇf
‚àÇxk

ÀÜx.
In words, this means:
The ratio between the marginal utility and the price of a commodity is for
the optimal solution the same for all goods that are actually purchased, and
it equals the marginal increase of utility at an increase of income. For goods
that are not purchased, the corresponding ratio is not larger.
The conclusion is rather trivial, for it xk > 0 and 1
pj
‚àÇf
‚àÇxj
> 1
pk
‚àÇf
‚àÇxk
, then the
consumer beneÔ¨Åts from changing a small quantity œµ/pk of commodity no. k
to the quantity œµ/pj of commodity no. j.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
62
Linear programming
Chapter 12
Linear programming
Linear programming (LP) is the art of optimizing linear functions over poly-
hedra, described as solution sets to systems of linear inequalities. In this
chapter, we describe and study the basic mathematical theory of linear pro-
gramming, above all the very important duality concept.
12.1
Optimal solutions
The optimal value of a general optimization problem was deÔ¨Åned in Chap-
ter 9. In particular, each LP problem
(P)
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
has an optimal value, which in this section will be denoted by vmin(c) to
indicate its dependence of the objective function.
LP problems with Ô¨Ånite optimal values always have optimal solutions.
The existence of an optimal solution is of course obvious if the polyhedron of
feasible points is bounded, i.e. compact, since the objective function is con-
tinuous. For arbitrary LP problems, we rely on the representation theorem
for polyhedra to prove the existence of optimal solutions.
Theorem 12.1.1. Suppose that the polyhedron X of feasible solutions in the
LP problem (P) is nonempty and a subset of Rn. Then we have:
(i) The value function vmin : Rn ‚ÜíR is concave with eÔ¨Äective domain
dom vmin = (recc X)+.
The objective function ‚ü®c, x‚ü©is, in other words, bounded below on X if
and only if c belongs to the dual cone of the recession cone of X.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
63
Linear programming
(ii) The problem has optimal solutions for each c ‚àà(recc X)+, and the set
of optimal solutions is a polyhedron. Moreover, the optimum is attained
at some extreme point of X if X is a line-free polyhedron.
Proof. By deÔ¨Ånition, the optimal value vmin(c) = inf{‚ü®c, x‚ü©| x ‚ààX} is
the pointwise inÔ¨Åmum of a family of concave functions, namely the linear
functions c ‚Üí‚ü®c, x‚ü©, with x running through X. So the value function vmin
is concave by Theorem 6.2.4 in Part I.
Let us now determine dom vmin, i.e. the set of c such that vmin(c) > ‚àí‚àû.
By the structure theorem for polyhedra (Theorem 5.3.1 in Part I), there is a
Ô¨Ånite nonempty set A such that X = cvx A + recc X, where A = ext X if the
polyhedron is line-free. The optimal value vmin(c) can therefore be calculated
as follows:
vmin(c) = inf{‚ü®c, y + z‚ü©| y ‚ààcvx A, z ‚ààrecc X}
(12.1)
= inf{‚ü®c, y‚ü©| y ‚ààcvx A} + inf{‚ü®c, z‚ü©| z ‚ààrecc X}
= min{‚ü®c, y‚ü©| y ‚ààA} + inf{‚ü®c, z‚ü©| z ‚ààrecc X},
The equality inf{‚ü®c, y‚ü©| y ‚ààcvx A} = min{‚ü®c, y‚ü©| y ‚ààA} holds because of
Theorem 6.3.3 in Part I, since linear functions are concave.
If c belongs to the dual cone (recc X)+, then ‚ü®c, z‚ü©‚â•0 for all vectors
z ‚ààrecc X with equality for z = 0, and it follows from equation (12.1) that
vmin(c) = min{‚ü®c, y‚ü©| y ‚ààA} > ‚àí‚àû.
This proves the inclusion (recc X)+ ‚äÜdom vmin, and that the optimal value
is attained at a point in A, and then in particular at some extreme point of
X if the polyhedron X is line-free.
If c /‚àà(recc X)+, then ‚ü®c, z0‚ü©< 0 for some vector z0 ‚ààrecc X. Since
tz0 ‚ààrecc X for t > 0 and limt‚Üí‚àû‚ü®c, tz0‚ü©= ‚àí‚àû, it follows that
inf{‚ü®c, z‚ü©| z ‚ààrecc X} = ‚àí‚àû,
and equation (12.1) now implies that vmin(c) = ‚àí‚àû. This concludes the
proof of the equality dom vmin = (recc X)+.
The set of minimum points to an LP problem with Ô¨Ånite value vmin is
equal to the intersection
X ‚à©{x ‚ààRn | ‚ü®c, x‚ü©= vmin}
between the polyhedron X and a hyperplane, and it is consequently a poly-
hedron.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
64
Linear programming
c
‚ü®c, x‚ü©= k‚Ä≤
‚ü®c, x‚ü©= k
‚ü®c, x‚ü©= vmin
ÀÜx
X
Figure 12.1. The minimum of ‚ü®c, x‚ü©over the line-
free polyhedron X is attained at an extreme point.
Example 12.1.1. The polyhedron X of feasible points for the LP problem
min x1 + x2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 ‚àíx2 ‚â•‚àí2
x1 + x2 ‚â•
1
‚àíx1
‚â•‚àí3
has three extreme points, namely (3, 5), (‚àí1
2, 3
2) and (3, ‚àí2). The values of
the objective function f(x) = x1 + x2 at these points are f(3, 5) = 8 and
f(‚àí1
2, 3
2) = f(3, ‚àí2) = 1. The least of these is 1, which is the optimal value.
The optimal value is attained at two extreme points, (1
2, 3
2) och (3, ‚àí2), and
thus also at all points on the line segment between those two points.
(‚àí1
2 , 3
2 )
(3, 5)
(3, ‚àí2)
x1 + x2 = k
x1
x2
Figure 12.2. Illustration for Example 12.1.1.
Suppose that X = {x ‚ààRn | Ax ‚â•b} is a line-free polyhedron and
that we want to minimize a given linear function over X. To determine the
optimal value of this LP problem, we need according to the previous theorem,
assuming that the objective function is bounded below on X, only calculate
function values at the Ô¨Ånitely many extreme points of X. In theory, this
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
65
Linear programming
65
is easy, but in practice it can be an insurmountable problem, because the
number of extreme points may be extremely high. The number of potential
extreme points of X when A is an m √ó n-matrix, equals
m
n

, which for
m = 100 and n = 50 is a number that is greater than 1029. The simplex
algorithm, which we will study in Chapter 13, is based on the idea that
it is not necessary to search through all the extreme points; the algorithm
generates instead a sequence x1, x2, x3, . . . of extreme points with decreasing
objective function values ‚ü®c, x1‚ü©‚â•‚ü®c, x2‚ü©‚â•‚ü®c, x3‚ü©‚â•. . . until the minimum
point is found. The number of extreme points that needs to be investigated
is therefore generally relatively small.
Sensitivity analysis
Let us rewrite the polyhedron of feasible points in the LP problem
(P)
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
as
X = cvx A + con B
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
66
Linear programming
with Ô¨Ånite sets A and B. We know from the preceding theorem and its proof
that a feasible point x is optimal for the LP problem if and only if

‚ü®c, a‚ü©‚â•‚ü®c, x‚ü©
for all a ‚ààA
‚ü®c, b‚ü©‚â•0
for all b ‚ààB,
and these inequalities deÔ¨Åne a convex cone Cx in the variable c. The set of
all c for which a given feasible point is optimal, is thus a convex cone.
Now suppose that x is indeed an optimal solution to (P). How much
can we change the coeÔ¨Écients of the objective function without changing
the optimal solution? The study of this issue is an example of sensitivity
analysis.
Expressed in terms of the cone Cx, the answer is simple: If we change
the coeÔ¨Écients of the objective function to c + ‚àÜc, then x is also an optimal
solution to the perturbed LP problem
(P‚Ä≤)
min ‚ü®c + ‚àÜc, x‚ü©
s.t.
x ‚ààX
if and only if c + ‚àÜc belongs to the cone Cx, i.e. if and only if ‚àÜc lies in the
polyhedron ‚àíc + Cx.
In summary, we have thus come to the following conclusions.
Theorem 12.1.2.
(i) The set of all c for which a given feasible point is
optimal in the LP problem (P), is a convex cone.
(ii) If x is an optimal solution to problem (P), then there is a polyhedron
such that x is also an optimal solution to the perturbed LP problem (P ‚Ä≤)
for all ‚àÜc in the polyhedron.
The set {‚àÜck | ‚àÜc ‚àà‚àíc + Cx and ‚àÜcj = 0 for j Ã∏= k} is a (possibly un-
bounded) closed interval [‚àídk, ek] around 0. An optimal solution to the prob-
lem (P) is therefore also optimal for the perturbed problem that is obtained
by only varying the objective coeÔ¨Écient ck, provided that the perturbation
‚àÜck lies in the interval ‚àídk ‚â§‚àÜck ‚â§ek.
Many computer programs for
LP problems, in addition to generating the optimal value and the optimal
solution, also provide information about these intervals.
Sensitivity analysis will be studied in connection with the simplex algo-
rithm in Chapter13.7.
Example 12.1.2. The printout of a computer program that was used to solve
an LP problem with c = (20, 30, 40, . . . ) contained among other things the
following information:
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
67
Linear programming
Optimal value: 4000
Optimal solution: x = (50, 40, 10, . . . )
Sensitivity report:
Variable
Value
Objective
Allowable
Allowable
coeÔ¨Ä.
decrease
increase
x1
50
20
15
5
x2
40
30
10
10
x3
10
40
15
20
...
...
...
...
...
Use the printout to determine the optimal solution and the optimal value
if the coeÔ¨Écients c1, c2 and c3 are changed to 17, 35 and 45, respectively, and
the other objective coeÔ¨Écients are left unchanged.
Solution: The columns ‚ÄùAllowable decrease‚Äù and ‚ÄùAllowable increase‚Äù show
that the polyhedron of changes ‚àÜc that do not aÔ¨Äect the optimal solution
contains the vectors (‚àí15, 0, 0, 0, . . . ), (0, 10, 0, 0, . . . ) and (0, 0, 20, 0, . . . ).
Since
(‚àí3, 5, 5, 0, . . . ) = 1
5(‚àí15, 0, 0, 0, . . . ) + 1
2(0, 10, 0, 0, . . . ) + 1
4(0, 0, 20, 0, . . . )
and 1
5 + 1
2 + 1
4 = 19
20 < 1, ‚àÜc = (‚àí3, 5, 5, 0, . . . ) is a convex combination of
changes that do not aÔ¨Äect the optimal solutions, namley the three changes
mentioned above and (0, 0, 0, 0, . . . ).
The solution x = (50, 40, 10, . . . ) is
therefore still optimal for the LP problem with c = (17, 35, 45, . . . ). However,
the new optimal value is of course 4000 ‚àí20 ¬∑ 3 + 30 ¬∑ 5 + 40 ¬∑ 5 = 4290.
12.2
Duality
Dual problems
By describing the polyhedron X in a linear minimization problem
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
as the solution set of a system of linear inequalities, we get a problem with
a corresponding Lagrange function, and hence also a dual function and a
dual problem. The description of X as a solution set is of course not unique,
so the dual problem is not uniquely determined by X as a polyhedron, but
whichever description we choose, we get, according to Theorem 11.1.1, a dual
problem, where strong duality holds, because Slater‚Äôs condition is satisÔ¨Åed
for convex problems with aÔ¨Éne constraints.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
68
Linear programming
68
In this section, we describe the dual problem for some commonly occur-
ring polyhedron descriptions, and we give an alternative proof of the duality
theorem. Our premise is that the polyhedron X is given as
X = {x ‚ààU + | Ax ‚àíb ‚ààV +},
where
‚Ä¢ U and V are Ô¨Ånitely generated cones in Rn and Rm, respectively;
‚Ä¢ A is an m √ó n-matrix;
‚Ä¢ b is a vector in Rm.
As usual, we identify vectors with column matrices and matrices with linear
transformations. The set X is of course a polyhedron, for by writing
X = U + ‚à©A‚àí1(b + V +)
we see that X is an intersection of two polyhedra ‚àíthe conical polyhe-
dron U + and the inverse image A‚àí1(b + V +) under the linear map A of the
polyhedron b + V +.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
69
Linear programming
The LP problem of minimizing ‚ü®c, x‚ü©over the polyhedron X with the
above description will now be written
(P)
min ‚ü®c, x‚ü©
s.t.
Ax ‚àíb ‚ààV +, x ‚ààU +
and in order to form a suitable dual problem we will perceive the condition
x ‚ààU + as an implicit constraint and express the other condition Ax‚àíb ‚ààV +
as a system of linear inequalities. Assume therefore that the Ô¨Ånitely generated
cone V is generated by the columns of the m √ó k-matrix D, i.e. that
V = {Dz | z ‚ààRk
+}.
The dual cone V + can then be written as
V + = {y ‚ààRm | DTy ‚â•0},
and the constraint Ax ‚àíb ‚ààV + can now be expressed as a system of in-
equalities, namely DTAx ‚àíDTb ‚â•0.
Our LP problem (P) has thus been transformed into
min ‚ü®c, x‚ü©
s.t.
DTb ‚àíDTAx ‚â§0, x ‚ààU +.
The associated Lagrange function L: U + √ó Rk
+ ‚ÜíR is deÔ¨Åned by
L(x, Œª) = ‚ü®c, x‚ü©+ ‚ü®Œª, DTb ‚àíDTAx‚ü©= ‚ü®c ‚àíATDŒª, x‚ü©+ ‚ü®b, DŒª‚ü©,
and the corresponding dual function œÜ: Rk
+ ‚ÜíR is given by
œÜ(Œª) = inf
x‚ààU+ L(x, Œª) =

‚ü®b, DŒª‚ü©,
if c ‚àíATDŒª ‚ààU
‚àí‚àû,
otherwise.
This gives us a dual problem of the form
max ‚ü®b, DŒª‚ü©
s.t.
c ‚àíATDŒª ‚ààU, Œª ‚ààRk
+.
Since DŒª describes the cone V as Œª runs through Rk
+, we can by setting
y = DŒª reformulate the dual problem so that it becomes
max ‚ü®b, y‚ü©
s.t.
c ‚àíATy ‚ààU, y ‚ààV.
It is therefore natural to deÔ¨Åne duality for LP problems of the form (P) as
follows.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
70
Linear programming
DeÔ¨Ånition. Given the LP problem
(P)
min ‚ü®c, x‚ü©
s.t.
Ax ‚àíb ‚ààV +, x ‚ààU +,
which we call the primal problem, we call the problem
(D)
max ‚ü®b, y‚ü©
s.t.
c ‚àíATy ‚ààU, y ‚ààV
the dual LP problem.
The optimal values of the two problems are denoted by vmin(P) and
vmax(D). The polyhedron of feasible points will be denoted by X for the
primal problem and by Y for the dual problem.
Example 12.2.1. DiÔ¨Äerent choices of the cones U and V give us diÔ¨Äerent
concrete dual problems (P) and (D). We exemplify with four important spe-
cial cases.
1. The choice U = {0}, U + = Rn and V = V + = Rm
+ gives us the following
dual pair:
(P1)
min ‚ü®c, x‚ü©
s.t.
Ax ‚â•b
and
(D1)
max ‚ü®b, y‚ü©
s.t.
ATy = c, y ‚â•0.
Every LP problem can be expressed in the form (P1), because every poly-
hedron can be expressed as an intersection of halfspaces, i.e. be written as
Ax ‚â•b.
2. The choice U = U + = Rn
+ and V = V + = Rm
+ gives instead the dual pair:
(P2)
min ‚ü®c, x‚ü©
s.t.
Ax ‚â•b, x ‚â•0
and
(D2)
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c, y ‚â•0.
This is the most symmetric formulation of duality, and the natural formu-
lation for many application problems with variables that represent physical
quantities or prices, which of course are nonnegative. The diet problem and
the production planning problem in Chapter 9.4 are examples of such prob-
lems.
3. U = U + = Rn
+, V = Rm and V + = {0} result in the dual pair:
(P3)
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
and
(D3)
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c.
The formulation (P3) is the natural starting point for the simplex algo-
rithm.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
71
Linear programming
71
4. The choice U = {0}, U + = Rn, V = Rm and V + = {0} gives us the pair
(P4)
min ‚ü®c, x‚ü©
s.t.
Ax = b
and
(D4)
max ‚ü®b, y‚ü©
s.t. ATy = c.
Example 12.2.2. A trivial example of dual LP problems in one variable is
min 5x
s.t.
2x ‚â•4
and
max 4y
s.t.
2y = 5, y ‚â•0
Both problems have the optimal value 10.
Example 12.2.3. The problems
min x1 + x2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 ‚àíx2 ‚â•‚àí2
x1 + x2 ‚â•
1
‚àíx1
‚â•‚àí3
and
max ‚àí2y1 + y2 ‚àí3y3
s.t.
Ô£±
Ô£≤
Ô£≥
y1 + y2 ‚àíy3 = 1
‚àíy1 + y2
= 1
y1, y2, y3 ‚â•0
are dual. The optimal solutions to the primal minimization problem were
determined in Example 12.1.1 and the optimal value was found to be 1. The
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
72
Linear programming
feasible points for the dual maximization problem are of the form
y = (t, 1 + t, 2t)
with t ‚â•0, and the corresponding values of the objective function are 1‚àí7t.
The maximum value is attained for t = 0 at the point (0, 1, 0), and the
maximum value is equal to 1.
The Duality Theorem
The primal and dual problems in Examples 12.2.2 and 12.2.3 have the same
optimal value, and this is no coincidence but a consequence of the duality
theorem, which is formulated below and is a special case of the duality the-
orem for general convex problems (Theorem 11.1.1). In this section we give
an alternative proof of this important theorem, and we start with the trivial
result about weak duality.
Theorem 12.2.1 (Weak duality). The optimal values of the two dual LP prob-
lems (P) and (D) satisfy the inequality
vmax(D) ‚â§vmin(P).
Proof. The inequality is trivially satisÔ¨Åed if any of the two polyhedra X and
Y of feasible points is empty, because if Y = ‚àÖthen vmax(D) = ‚àí‚àû, by
deÔ¨Ånition, and if X = ‚àÖthen vmin(P) = +‚àû, by deÔ¨Ånition.
Assume therefore that both problems have feasible points. If x ‚ààX and
y ‚ààY , then y ‚ààV , (Ax‚àíb) ‚ààV +, (c‚àíATy) ‚ààU and x ‚ààU +, by deÔ¨Ånition,
and hence ‚ü®Ax ‚àíb, y‚ü©‚â•0 and ‚ü®c ‚àíATy, x‚ü©‚â•0. It follows that
‚ü®b, y‚ü©‚â§‚ü®b, y‚ü©+ ‚ü®c ‚àíATy, x‚ü©= ‚ü®b, y‚ü©+ ‚ü®c, x‚ü©‚àí‚ü®y, Ax‚ü©
= ‚ü®c, x‚ü©+ ‚ü®b, y‚ü©‚àí‚ü®Ax, y‚ü©= ‚ü®c, x‚ü©‚àí‚ü®Ax ‚àíb, y‚ü©‚â§‚ü®c, x‚ü©.
The objective function ‚ü®b, y‚ü©in the maximization problem (D) is in other
words bounded above on Y by ‚ü®c, x‚ü©for each x ‚ààX, and hence
vmax(D) = sup
y‚ààY
‚ü®b, y‚ü©‚â§‚ü®c, x‚ü©.
The objective function ‚ü®c, x‚ü©in the minimization problem (P) is therefore
bounded below on X by vmax(D). This implies that vmax(D) ‚â§vmin(P) and
completes the proof of the theorem.
The following optimality criterion follows from weak duality.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
73
Linear programming
Theorem 12.2.2 (Optimality criterion). Suppose that ÀÜx is a feasible point for
the minimization problem (P), that ÀÜy is a feasible point for the dual maxi-
mization problem (D), and that
‚ü®c, ÀÜx‚ü©= ‚ü®b, ÀÜy‚ü©.
Then ÀÜx and ÀÜy are optimal solutions of the respective problems.
Proof. The assumptions on ÀÜx and ÀÜy combined with Theorem 12.2.1 give us
the following chain of inequalities
vmax(D) ‚â•‚ü®b, ÀÜy‚ü©= ‚ü®c, ÀÜx‚ü©‚â•vmin(P) ‚â•vmax(D).
Since the two extreme ends are equal, there is equality everywhere, which
means that ÀÜy is a maximum point and ÀÜx is a minimum point.
Theorem 12.2.3 (Duality theorem). Suppose that at least one of the two dual
LP problems
min ‚ü®c, x‚ü©
s.t.
Ax ‚àíb ‚ààV +, x ‚ààU +
(P)
and
max ‚ü®b, y‚ü©
s.t.
c ‚àíATy ‚ààU, y ‚ààV
(D)
has feasible points. Then, the two problem have the same optimal value.
Thus, provided that at least one of the two dual problems has feasible points:
(i) X = ‚àÖ‚áîthe objective function ‚ü®b, y‚ü©is not bounded above on Y .
(ii) Y = ‚àÖ‚áîthe objective function ‚ü®c, x‚ü©is not bounded below on X.
(iii) If X Ã∏= ‚àÖand Y Ã∏= ‚àÖ, then there exist points ÀÜx ‚ààX and ÀÜy ‚ààY such
that ‚ü®b, y‚ü©‚â§‚ü®b, ÀÜy‚ü©= ‚ü®c, ÀÜx‚ü©‚â§‚ü®c, x‚ü©for all x ‚ààX and all y ‚ààY .
The duality theorem for linear programming problems is a special case
of the general duality theorem for convex problems, but we give here an
alternative proof based directly on the following variant of Farkas‚Äôs lemma.
Lemma. The system
(12.2)
‚ü®c, x‚ü©‚â§Œ±
x ‚ààX
has a solution if and only if the systems
‚ü®b, y‚ü©> Œ±
y ‚ààY
and
(12.3-B)
Ô£±
Ô£≤
Ô£≥
‚ü®b, y‚ü©= 1
‚àíATy ‚ààU
y ‚ààV
(12.3-A)
both have no solutions,
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
74
Linear programming
74
Proof. The system (12.2), i.e.
Ô£±
Ô£≤
Ô£≥
‚ü®c, x‚ü©‚â§Œ±
Ax ‚àíb ‚ààV +
x ‚ààU +,
is solvable if and only if the following homogenized system is solvable:
(12.2‚Ä≤)
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚ü®c, x‚ü©‚â§Œ±t
Ax ‚àíbt ‚ààV +
x ‚ààU +
t ‚ààR
t > 0.
(If x solves the system (12.2), then (x, 1) solves the system (12.2‚Ä≤), and if
(x, t) solves the system (12.2‚Ä≤), then x/t solves the system (12.2).) We can
write the system (12.2‚Ä≤) more compactly by introducing the matrix
ÀúA =
 Œ±
‚àícT
‚àíb
A

Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
75
Linear programming
and the vectors Àúx = (t, x) ‚ààR √ó Rn and d = (‚àí1, 0) ‚ààR √ó Rn, namely as
(12.2‚Ä≤‚Ä≤)
Ô£±
Ô£≤
Ô£≥
ÀúAÀúx ‚ààR+ √ó V +
Àúx ‚ààR √ó U +
dTÀúx < 0.
By Theorem 3.3.2 in Part I, the system (12.2‚Ä≤‚Ä≤) is solvable if and only if
the following dual system has no solutions:
(12.3‚Ä≤‚Ä≤)

d ‚àíÀúATÀúy ‚àà{0} √ó U
Àúy ‚ààR+ √ó V .
Since
ÀúAT =
 Œ±
‚àíbT
‚àíc
AT

,
we obtain the following equivalent system from (12.3‚Ä≤‚Ä≤) by setting Àúy = (s, y)
with s ‚ààR and y ‚ààRm:
(12.3‚Ä≤)
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àí1 ‚àíŒ±s + ‚ü®b, y‚ü©= 0
cs ‚àíATy ‚ààU
y ‚ààV
s ‚â•0.
The system (12.2) is thus solvable if and only if the system (12.3‚Ä≤) has no
solutions, and by considering the cases s > 0 and s = 0 separately, we see
that the system (12.3‚Ä≤) has no solutions if and only if the two systems
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚ü®b, y/s‚ü©= Œ±+1/s
c ‚àíAT(y/s) ‚ààU
y/s ‚ààV
s > 0
and
Ô£±
Ô£≤
Ô£≥
‚ü®b, y‚ü©= 1
‚àíATy ‚ààU
y ‚ààV
have no solutions, and this is obviously the case if and only if the systems
(12.3-A) and (12.3-B) both lack solutions.
Proof of the duality theorem. We now return to the proof of the duality
theorem, and because of weak duality, we only need to show the inequality
(12.4)
vmin(P) ‚â§vmax(D).
We divide the proof of this inequality in three separate cases.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
76
Linear programming
Case 1. Y Ã∏= ‚àÖand the system (12.3-B) has no solution.
The inequality (12.4) is trivially true if vmax(D) = ‚àû. Therefore, assume
that vmax(D) < ‚àû. Then, because of the deÔ¨Ånition of vmax(D), the system
(12.3-A) has no solution when Œ± = vmax(D). So neither of the two systems in
(12.3) has a solution for Œ± = vmax(D). Thus, the system (12.2) has a solution
for this Œ±-value by the lemma, which means that there is a feasible point ÀÜx
such that ‚ü®c, ÀÜx‚ü©‚â§vmax(D). Consequently, vmin(P) ‚â§‚ü®c, ÀÜx‚ü©‚â§vmax(D).
Note that it follows from the proof that the minimization problem actually
has an optimal solution ÀÜx.
Case 2. Y = ‚àÖand the system (12.3-B) has no solution.
The system (12.3-A) now lacks solutions for all values of Œ±, so it follows
from the lemma that the system (12.2) is solvable for all Œ±-values, and this
means that the objective function ‚ü®c, x‚ü©is unbounded below on X. Hence,
vmin(P) = ‚àí‚àû= vmax(D) in this case.
Case 3. The system (12.3-B) has a solution
It now follows from the lemma that the system (12.2) has no solution for all
values of Œ±, and this implies that the set X of feasible solutions is empty.
The polyhedron Y of feasible points in the dual problem is consequently
nonempty. Choose a point y0 ‚ààY , let y be a solution to the system (12.3-B)
and consider the points yt = y0 + ty for t > 0. The vectors yt belong to V ,
because they are conical combinations of vectors in V . Moreover, the vectors
c‚àíATyt = (c‚àíATy0)‚àítATy are conic combinations of vectors in U and thus
belong to U. This means that the vector yt lies in Y for t > 0, and since
‚ü®b, yt‚ü©= ‚ü®b, y0‚ü©+ t‚ü®b, y‚ü©= ‚ü®b, y0‚ü©+ t ‚Üí+‚àû
as t ‚Üí‚àû, we conclude that vmax(D) = ‚àû. The inequality (12.4) is in other
words trivially fulÔ¨Ålled.
The Complementary Theorem
Theorem 12.2.4 (Complementary theorem). Suppose that ÀÜx is a feasible point
for the LP problem (P) and that ÀÜy is a feasible point for the dual LP problem
(D). Then, the two points are optimal for their respective problems if and
only if
‚ü®c ‚àíATÀÜy, ÀÜx‚ü©= ‚ü®AÀÜx ‚àíb, ÀÜy‚ü©= 0.
Proof. Note Ô¨Årst that due to the deÔ¨Ånition of the polyhedra X and Y of
feasible points, we have ‚ü®Ax‚àíb, y‚ü©‚â•0 for all points x ‚ààX and y ‚ààV , while
‚ü®c ‚àíATy, x‚ü©‚â•0 for all points y ‚ààY and x ‚ààU.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
77
Linear programming
In particular, ‚ü®AÀÜx ‚àíb, ÀÜy‚ü©‚â•0 and ‚ü®c ‚àíATÀÜy, ÀÜx‚ü©‚â•0 if ÀÜx is an optimal
solution to the primal problem (P) and ÀÜy is an optimal solution to the dual
problem (D). Moreover, ‚ü®c, ÀÜx‚ü©= ‚ü®b, ÀÜy‚ü©because of the Duality theorem, so it
follows that
‚ü®c, ÀÜx‚ü©‚àí‚ü®AÀÜx‚àíb, ÀÜy‚ü©‚â§‚ü®c, ÀÜx‚ü©= ‚ü®b, ÀÜy‚ü©‚â§‚ü®b, ÀÜy‚ü©+‚ü®c‚àíATÀÜy, ÀÜx‚ü©= ‚ü®c, ÀÜx‚ü©‚àí‚ü®AÀÜx‚àíb, ÀÜy‚ü©.
Since the two extreme ends of this inequality are equal, we have equality
everywhere, i.e. ‚ü®AÀÜx ‚àíb, ÀÜy‚ü©= ‚ü®c ‚àíATÀÜy, ÀÜx‚ü©= 0.
Conversely, if ‚ü®c ‚àíATÀÜy, ÀÜx‚ü©= ‚ü®AÀÜx ‚àíb, ÀÜy‚ü©= 0, then ‚ü®c, ÀÜx‚ü©= ‚ü®ATÀÜy, ÀÜx‚ü©and
‚ü®b, ÀÜy‚ü©= ‚ü®AÀÜx, ÀÜy‚ü©, and since ‚ü®ATÀÜy, ÀÜx‚ü©= ‚ü®AÀÜx, ÀÜy‚ü©, we conclude that ‚ü®c, ÀÜx‚ü©=
‚ü®b, ÀÜy‚ü©. The optimality of the two points now follows from the Optimality
criterion.
Let us for clarity formulate the Complementarity theorem in the impor-
tant special case when the primal and dual problems have the form described
as Case 2 in Example 12.2.1.
Corollary 12.2.5. Suppose that ÀÜx and ÀÜy are feasible points for the dual prob-
lems
min ‚ü®c, x‚ü©
s.t.
Ax ‚â•b, x ‚â•0
(P2)
and
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c, y ‚â•0.
(D2)
respectively. Then, they are optimal solutions if and only if
(12.5)

(AÀÜx)i > bi
‚áí
ÀÜyi = 0
ÀÜxj > 0 ‚áí(ATÀÜy)j = cj
In words we can express condition (12.5) as follows, which explains the term
‚Äôcomplementary slackness‚Äô: If ÀÜx satisÔ¨Åes an individual inequality in the sys-
tem Ax ‚â•b strictly, then the corresponding dual variable ÀÜyi has to be equal
to zero, and if ÀÜy satisÔ¨Åes an individual inequality in the system ATy ‚â§c
strictly, then the corresponding primal variable xj has to be equal to zero.
Proof. Since ‚ü®AÀÜx‚àíb, ÀÜy‚ü©= m
i=1((AÀÜx)i ‚àíbi)ÀÜyi is a sum of nonnegative terms,
we have ‚ü®AÀÜx ‚àíb, ÀÜy‚ü©= 0 if and only if all the terms are equal to zero, i.e. if
and only if (AÀÜx)i > bi ‚áíÀÜyi = 0.
Similarly, ‚ü®c ‚àíATÀÜy, ÀÜx‚ü©= 0 if and only if ÀÜxj > 0 ‚áí(ATÀÜy)j = cj. The
corollary is thus just a reformulation of Theorem 12.2.4 for dual problems of
type (P2)‚Äì(D2).
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
78
Linear programming
78
The curious reader may wonder whether the implications in the condition
(12.5) can be replaced by equivalences. The following trivial example shows
that this is not the case.
Example 12.2.4. Consider the dual problems
min x1 + 2x2
s.t.
x1 + 2x2 ‚â•2, x ‚â•0
and
max 2y
s.t.
 y ‚â§1
2y ‚â§2, y ‚â•0
with A = cT =

1
2

and b = [2]. The condition (12.5) is not fulÔ¨Ålled with
equivalence at the optimal points ÀÜx = (2, 0) and ÀÜy = 1, because ÀÜx2 = 0 and
(ATÀÜy)2 = 2 = c2.
However, there are other optimal solutions to the minimization problem;
all points on the line segment between (2, 0) and (0, 1) are optimal, and the
optimal pairs ÀÜx = (2 ‚àí2t, t) and ÀÜy = 1 satisfy the condition (12.5) with
equivalence for 0 < t < 1.
The last conclusion in the above example can be generalized. All dual
problems with feasible points have a pair of optimal solutions ÀÜx and ÀÜy that
satisfy the condition (12.5) with implications replaced by equivalences. See
exercise 12.8.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
79
Linear programming
Example 12.2.5. The LP problem
min ‚àíx1 + 2x2 + x3 + 2x4
s.t.
Ô£±
Ô£≤
Ô£≥
‚àíx1 ‚àíx2 ‚àí2x3 + x4 ‚â•4
‚àí2x1 + x2 + 3x3 + x4 ‚â•8
x1, x2, x3, x4 ‚â•0
is easily solved by Ô¨Årst solving the dual problem
max 4y1 + 8y2
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àíy1 ‚àí2y2 ‚â§‚àí1
‚àíy1 + y2 ‚â§
2
‚àí2y1 + 3y2 ‚â§
1
y1 + y2 ‚â§
2
y1, y2 ‚â•
0
graphically and then using the Complementary theorem.
4y1 + 8y2 = 12
1
2
y1
1
2
y2
Figure 12.3. A graphical solution to
the maximization problem in Ex. 12.2.5.
A graphical solution is obtained from Ô¨Ågure 12.3, which shows that ÀÜy =
(1, 1) is the optimal point and that the value is 12. Since ÀÜy satisÔ¨Åes the Ô¨Årst
two constraints with strict inequality and ÀÜy1 > 0 and ÀÜy2 > 0, we obtain the
optimal solution ÀÜx to the minimization problem as a solution to the system
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
‚àíx1 ‚àíx2 ‚àí2x3 + x4 = 4
‚àí2x1 + x2 + 3x3 + x4 = 8
x1
= 0
x2
= 0
x1, x2, x3, x4 ‚â•0.
The solution to this system is ÀÜx =

0, 0, 4
5, 28
5

, and the optimal value is 12,
which it of course has to be according to the Duality theorem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
80
Linear programming
Exercises
12.1 The matrix A and the vector c are assumed to be Ô¨Åxed in the LP problem
min
‚ü®c, x‚ü©
s.t.
Ax ‚â•b
but the right hand side vector b is allowed to vary. Suppose that the problem
has a Ô¨Ånite value for some right hand side b. Prove that for each b, the value
is either Ô¨Ånite or there are no feasible points. Show also that the optimal
value is a convex function of b.
12.2 Give an example of dual problems which both have no feasible points.
12.3 Use duality to show that (3, 0, 1) is an optimal solution to the LP problem
min 2x1 + 4x2 + 3x3
s.t.
Ô£±
Ô£≤
Ô£≥
2x1 + 3x2 + 4x3 ‚â•10
x1 + 2x2
‚â•
3
2x1 + 7x2 + 2x3 ‚â•
5, x ‚â•0.
12.4 Show that the column player‚Äôs problem and the row player‚Äôs problem in a
two-person zero-sum game (see Chapter 9.4) are dual problems.
12.5 Investigate how the optimal solution to the LP problem
max x1 + x2
s.t.
Ô£±
Ô£≤
Ô£≥
tx1 + x2 ‚â•‚àí1
x1
‚â§
2
x1 ‚àíx2 ‚â•‚àí1
depends on the parameter t.
12.6 The Duality theorem follows from Farkas‚Äôs lemma (Corollary 3.3.3 in Part I).
Show conversely that Farkas‚Äôs lemma follows from the Duality theorem by
considering the dual problems
min
‚ü®c, x‚ü©
s.t.
Ax ‚â•0
and
max ‚ü®0, y‚ü©
s.t.
ATy = c, y ‚â•0
12.7 Let Y = {y ‚ààRm | c ‚àíATy ‚ààU, y ‚ààV }, where U and V are closed convex
cones, and suppose that Y Ã∏= ‚àÖ.
a) Show that recc Y = {y ‚ààRm | ‚àíATy ‚ààU, y ‚ààV }.
b) Show that the system (12.3-B) has a solution if and only if the vector ‚àíb
does not belong to the dual cone of recc Y .
c) Show, using the result in b), that the conclusion in case 3 of the proof of
the Duality theorem follows from Theorem 12.1.1, i.e. that vmax(D) = ‚àûif
(and only if) the system (12.3-B) has a solution.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
81
Linear programming
81
12.8 Suppose that the dual problems
min
‚ü®c, x‚ü©
s.t.
Ax ‚â•b, x ‚â•0
and
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c, y ‚â•0
both have feasible points. Prove that there exist optimal solutions ÀÜx and ÀÜy
to the problems that satisfy
(AÀÜx)i > bi ‚áî
ÀÜyi = 0
ÀÜxj > 0 ‚áî(ATÀÜy)j = cj.
[Hint: Because of the Complementarity theorem it suÔ¨Éces to show that the
following system of inequalities has a solution: Ax ‚â•b, x ‚â•0, ATy ‚â§c,
y ‚â•0, ‚ü®b, y‚ü©‚â•‚ü®c, x‚ü©, Ax + y > b, Ay ‚àíc < x. And this system is solvable if
and only if the following homogeneous system is solvable: Ax‚àíbt ‚â•0, x ‚â•0,
‚àíATy +ct ‚â•0, y ‚â•0, ‚àí‚ü®c, x‚ü©+‚ü®b, y‚ü©‚â§0, Ax+y ‚àíbt > 0, x‚àíATy +ct > 0,
t > 0. The solvability can now be decided by using Theorem 3.3.7 in Part I.]
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
82
The simplex algorithm
Chapter 13
The simplex algorithm
For practical purposes, there are somewhat simpliÔ¨Åed two kinds of methods
for solving LP problems. Both generate a sequence of points with progres-
sively better objective function values. Simplex methods, which were intro-
duced by Dantzig in the late 1940s, generate a sequence of extreme points of
the polyhedron of feasible points in the primal (or dual) problem by moving
along the edges of the polyhedron. Interior-point methods generate instead,
as the name implies, points in the interior of the polyhedron. These methods
are derived from techniques for non-linear programming, developed by Fiacco
and McCormick in the 1960s, but it was only after Karmarkars innovative
analysis in 1984 that the methods began to be used for LP problems.
In this chapter, we describe and analyze the simplex algorithm.
13.1
Standard form
The simplex algorithm requires that the LP problem is formulated in a special
way, and the variant of the algorithm that we will study assumes that the
problem is a minimization problem, that all variables are nonnegative and
that all other constraints are formulated as equalities.
DeÔ¨Ånition. An LP problem has standard form if it has the form
min c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn = b1
...
am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn = bm
x1, x2, . . . , xn ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
83
The simplex algorithm
By introducing the matrices
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
b =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
b1
b2
...
bm
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
and
c =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c1
c2
...
cn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
we get the following compact writing for an LP problem in standard form:
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0.
We noted in Chapter 9 that each LP problem can be transformed into an
equivalent LP problem in standard form by using slack/surplus variables and
by replacing unrestricted variables with diÔ¨Äerences of nonnegative variables.
Duality
We gave a general deÔ¨Ånition of the concept of duality in Chapter 12.2 and
showed that dual LP problems have the same optimal value, except when
both problems have no feasible points. In our description of the simplex
algorithm, we will need a special case of duality, and to make the presenta-
tion independent of the results in the previous chapter, we now repeat the
deÔ¨Ånition for this special case.
DeÔ¨Ånition. The LP problem
(D)
max ‚ü®b, y‚ü©
s.t. ATy ‚â§c
is said to be dual to the LP problem
(P)
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0.
We shall use the following trivial part of the Duality theorem.
Theorem 13.1.1 (Weak duality). If x is a feasible point for the minimization
problem (P) and y is a feasible point for the dual maximization problem (D),
i.e. if Ax = b, x ‚â•0 and ATy ‚â§c, then
‚ü®b, y‚ü©‚â§‚ü®c, x‚ü©.
Proof. The inequalities ATy ‚â§c and x ‚â•0 imply that att ‚ü®x, ATy‚ü©‚â§‚ü®x, c‚ü©,
and hence
‚ü®b, y‚ü©= ‚ü®Ax, y‚ü©= ‚ü®x, ATy‚ü©‚â§‚ü®x, c‚ü©= ‚ü®c, x‚ü©.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
84
The simplex algorithm
84
Corollary 13.1.2 (Optimality criterion). Suppose that ÀÜx is a feasible point for
the minimization problem (P), that ÀÜy is a feasible point for the dual maxi-
mization problem (D), and that ‚ü®c, ÀÜx‚ü©= ‚ü®b, ÀÜy‚ü©. Then ÀÜx and ÀÜy are optimal
solutions to the respective problems.
Proof. It follows from the assumptions and Theorem 13.1.1, applied to the
point y and an arbitrary feasible point x for the minimization problem, that
‚ü®c, x‚ü©= ‚ü®b, y‚ü©‚â§‚ü®c, x‚ü©
for all feasible points x.
This shows that x is a minimum point, and an
analogous argument shows that y is a maximum point.
13.2
Informal description of the simplex al-
gorithm
In this section we describe the main features of the simplex algorithm with
the help of some simple examples. The precise formulation of the algorithm
and the proof that it works is given in sections 13.4 and 13.5.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
85
The simplex algorithm
Example 13.2.1. We start with a completely trivial problem, namely
min f(x) = x3 + 2x4
s.t.
x1
+ 2x3 ‚àíx4 = 2
x2 ‚àíx3 + x4 = 3, x ‚â•0.
Since the coeÔ¨Écients of the objective function f(x) are positive and x ‚â•0, it
is clear that f(x) ‚â•0 for all feasible points x. There is also a feasible point
x with x3 = x4 = 0, namely x = (2, 3, 0, 0). The minimum is therefore equal
to 0, and (2, 3, 0, 0) is the (unique) minimum point.
Now consider an arbitrary problem of the form
(13.1)
min f(x) = cm+1xm+1 + ¬∑ ¬∑ ¬∑ + cnxn + d
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1 + a1 m+1xm+1 + . . . + a1nxn = b1
x2 + a2 m+1xm+1 + . . . + a2nxn = b2
...
xm + am m+1xm+1 + . . . + amnxn = bm, x ‚â•0
where
b1, b2, . . . , bm ‚â•0.
If cm+1, cm+2, . . . , cn ‚â•0, then obviously f(x) ‚â•d for all feasible points x,
and since x = (b1, . . . , bm, 0, . . . , 0) is a feasible point and f(x) = d, it follows
that d is the optimal value.
The constraint system in LP problem (13.1) has a very special form, for it
is solved with respect to the basic variables x1, x2, . . . , xm, and these variables
are not present in the objective function. Quite generally, we shall call a set
of variables basic to a given system of linear equations if it is possible to solve
the system with respect to the variables in the set.
Example 13.2.2. Let us alter the objective function in Example 13.2.1 by
changing the sign of the x3-coeÔ¨Écient. Our new problem thus reads as fol-
lows:
(13.2)
min f(x) = ‚àíx3 + 2x4
s.t.
x1
+ 2x3 ‚àíx4 = 2
x2 ‚àíx3 + x4 = 3, x ‚â•0.
The point (2, 3, 0, 0) is of course still feasible and the corresponding value
of the objective function is 0, but we can get a smaller value by choosing
x3 > 0 and keeping x4 = 0.
However, we must ensure that x1 ‚â•0 and
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
86
The simplex algorithm
x2 ‚â•0, so the Ô¨Årst constraint equation yields the bound x1 = 2 ‚àí2x3 ‚â•0,
i.e. x3 ‚â§1.
We now transform the problem by solving the system (13.2) with respect
to the variables x2 and x3, i.e. by changing basic variables from x1, x2 to
x2, x3. Using Gaussian elimination, we obtain

1
2x1
+ x3 ‚àí1
2x4 = 1
1
2x1 + x2
+ 1
2x4 = 4.
The new basic variable x3 is then eliminated from the objectiv function by
using the Ô¨Årst equation in the new system. This results in
f(x) = 1
2x1 + 3
2x4 ‚àí1,
and our problem has thus been reduced to a problem of the form (13.1),
namely
min
1
2x1 + 3
2x4 ‚àí1
s.t.

1
2x1
+ x3 ‚àí1
2x4 = 1
1
2x1 + x2
+ 1
2x4 = 4, x ‚â•0
with x2 and x3 as basic variables and with nonnegative coeÔ¨Écients for the
other variables in the objectiv function. Hence, the optimal value is equal to
‚àí1, and (0, 4, 1, 0) is the optimal point.
The strategy for solving a problem of the form (13.1), where some co-
eÔ¨Écient cm+k is negative, consists in replacing one of the basic variables
x1, x2, . . . , xm with xm+k so as to obtain a new problem of the same form. If
the new c-coeÔ¨Écients are nonnegative, then we are done. If not, we have to
repeat the procedure. We illustrate with another example.
Example 13.2.3. Consider the problem
(13.3)
min f(x) = 2x1 ‚àíx2 + x3 ‚àí3x4 + x5
s.t.
Ô£±
Ô£≤
Ô£≥
x1
+ 2x4 ‚àíx5 = 5
x2
+ x4 + 3x5 = 4
x3 ‚àíx4 + x5 = 3, x ‚â•0.
First we have to eliminate the basic variables x1, x2, x3 from the objective
function with
(13.4)
f(x) = ‚àí5x4 + 5x5 + 9
as result. Since the coeÔ¨Écient of x4 is negative, x4 has to be eliminated
from the objective function and from two constraint equations in such a way
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
87
The simplex algorithm
87
that the right hand side of the transformed system remains nonnegative.
The third equation in (13.3) can not be used for this elimination, since the
coeÔ¨Écient of x4 is negative. Eliminating x4 from the Ô¨Årst equation by using
the second equation results in the equation x1 ‚àí2x2 ‚àí7x5 = 5 ‚àí2 ¬∑ 4 = ‚àí3,
which has an illegal right-hand side. It therefore only remains to use the Ô¨Årst
of the constraints in (13.3) for the elimination. We then get the following
equivalent system
(13.5)
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
1
2x1
+ x4 ‚àí1
2x5 = 5
2
‚àí1
2x1 + x2
+ 7
2x5 = 3
2
1
2x1
+ x3
+ 1
2x5 = 11
2 , x ‚â•0
with x2, x3, x4 as new basic variables.
The reason why the right-hand side of the system remains positive when
the Ô¨Årst equation of (13.3) is used for the elimination of x4, is that the ratio
of the right-hand side and the x4-coeÔ¨Écient is smaller for the Ô¨Årst equation
than for the second (5/2 < 4/1).
We now eliminate x4 from the objective function, using equation (13.4)
and the Ô¨Årst equation of the system (13.5), and obtain
f(x) = 5
2x1 + 5
2x5 ‚àí7
2
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
88
The simplex algorithm
which is to be minimized under the constraints (13.5). The minimum value
is clearly equal to ‚àí7
2, and (0, 3
2, 11
2 , 5
2, 0) is the minimum point.
To reduce the writing it is customary to omit the variables and only work
with coeÔ¨Écients in tabular form. The problem (13.3) is thus represented by
the following simplex tableau:
1
0
0
2
‚àí1
5
0
1
0
1
3
4
0
0
1
‚àí1
1
3
2
‚àí1
1
‚àí3
1
f
The upper part of the tableau represents the system of equations, and the
lower row represents the objective function f. The vertical line corresponds
to the equality signs in (13.3).
To eliminate the basic variables x1, x2, x3 from the objective function we
just have to add ‚àí2 times row 1, row 2 and ‚àí1 times row 3 to the objective
function row in the above tableau. This gives us the new tableau
1
0
0
2
‚àí1
5
0
1
0
1
3
4
0
0
1
‚àí1
1
3
0
0
0
‚àí5
5
f ‚àí9
The bottom row corresponds to equation (13.4). Note that the constant term
9 appears on the other side of the equality sign compared to (13.4), and this
explains the minus sign in the tableau. We have also highlighted the basic
variables by underscoring.
Since the x4-coeÔ¨Écient of the objective function is negative, we have to
transform the tableau in such a way that x4 becomes a new basic variable.
By comparing the ratios 5/2 and 4/1 we conclude that the Ô¨Årst row has to
be the pivot row, i.e. has to be used for the eliminations. We have indicated
this by underscoring the coeÔ¨Écient in the Ô¨Årst row and the fourth column of
the tableau, the so-called pivot element.
Gaussian elimination gives rise to the new simplex tableau
1
2
0
0
1
‚àí1
2
5
2
‚àí1
2
1
0
0
7
2
3
2
1
2
0
1
0
1
2
11
2
5
2
0
0
0
5
2
f + 7
2
Since the coeÔ¨Écients of the objective function are now nonnegative, we can
read the minimum, with reversed sign, in the lower right corner of the tableau.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
89
The simplex algorithm
The minimum point is obtained by assigning the value 0 to the non-basic
variables x1 and x5, which gives x = (0, 3
2, 11
2 , 5
2, 0).
Example 13.2.4. Let us solve the LP problem
min x1 ‚àí2x2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + 2x2 + 2x3 + x4
= 5
x1
+ x3
+ x5
= 2
x2 ‚àí2x3
+ x6 = 1, x ‚â•0.
The corresponding simplex tableau is
1
2
2
1
0
0
5
1
0
1
0
1
0
2
0
1
‚àí2
0
0
1
1
1
‚àí2
1
0
0
0
f
with x4, x5, x6 as basic variables, and these are already eliminated from the
objective function. Since the x2-coeÔ¨Écient of the objective function is nega-
tive, we have to introduce x2 as a new basic variable, and we have to use the
underscored element as pivot element, since 1/1 < 5/2. Using the third row,
the tableau is transformed into
1
0
6
1
0
‚àí2
3
1
0
1
0
1
0
2
0
1
‚àí2
0
0
1
1
1
0
‚àí3
0
0
2
f + 2
and this tableau corresponds to the problem
min x1 ‚àí3x3 + 2x6 ‚àí2
s.t.
Ô£±
Ô£≤
Ô£≥
x1
+ 6x3 + x4
‚àí2x6 = 3
x1
+ x3
+ x5
= 2
x2 ‚àí2x3
+ x6 = 1, x ‚â•0.
Since the x3-coeÔ¨Écient in the objective function is now negative, we have
to repeat the procedure. We must thus introduce x3 as a new basic variable,
and this time we have to use the Ô¨Årst row as pivot row, for 3/6 < 2/1. The
new tableau has the following form
1
6
0
1
1
6
0
‚àí1
3
1
2
5
6
0
0
‚àí1
6
1
1
3
3
2
1
3
1
0
1
3
0
1
3
2
3
2
0
0
1
2
0
1
f + 7
2
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
90
The simplex algorithm
We can now read oÔ¨Äthe minimum ‚àí7
2 and the minimum point (0, 2, 1
2, 0, 3
2, 0).
So far, we have written the function symbol f in the lower right corner of
our simplex tableaux. We have done this for pedagogical reasons to explain
why the function value in the box gets a reverse sign. Remember that the
last row of the previous simplex tableau means that
3
2x1 + 1
2x4 + x6 = f(x) + 7
2.
Since the symbol has no other function, we will omit it in the future.
Example 13.2.5. The problem
min f(x) = ‚àí2x1 + x2
s.t.

x1 ‚àíx2 + x3
= 3
‚àíx1 + x2
+ x4 = 4, x ‚â•0
gives rise to the following simplex tableaux:
1
‚àí1
1
0
3
‚àí1
1
0
1
4
‚àí2
1
0
0
0
1
‚àí1
1
0
3
0
0
1
1
7
0
‚àí1
2
0
6
Since the objective function has a negative x2-coeÔ¨Écient, we are now
supposed to introduce x2 as a basic variable, but no row will work as a
pivot row since the entire x2-column is non-positive. This implies that the
objective function is unbounded below, i.e. there is no minimum. To see this,
we rewrite the last tableau with variables in the form
min f(x) = ‚àíx2 + 2x3 ‚àí6
s.t.
x1 = x2 ‚àíx3 + 3
x4 =
‚àíx3 + 7.
By choosing x2 = t and x3 = 0 we get a feasible point xt = (3 + t, t, 0, 7) for
each t ‚â•0, and since f(xt) = ‚àít ‚àí6 ‚Üí‚àí‚àûas t ‚Üí‚àû, we conclude that the
objective function is unbounded below.
Examples 13.2.4 and 13.2.5 are typical for LP problems of the form (13.1).
In Section 13.5, namely, we show that one can always perform the iterations
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
91
The simplex algorithm
91
so as to obtain a Ô¨Ånal tableau similar to the one in Example 13.2.4 or the one
in Example 13.2.5, and in Section 13.6 we will show how to get started, i.e.
how to transform an arbitrary LP problem in standard form into a problem
of the form (13.1).
13.3
Basic solutions
In order to describe and understand the simplex algorithm it is necessary
Ô¨Årst to know how to produce solutions to a linear system of equations. We
assume that Gaussian elimination is familiar and concentrate on describing
how to switch from one basic solution to another. We begin by reviewing the
notation that we will use in the rest of this chapter.
The columns of an m √ó n-matrix A will be denoted A‚àó1, A‚àó2, . . . , A‚àón so
that
A =

A‚àó1 A‚àó2 . . . A‚àón

.
We will often have to consider submatrices comprised of certain columns
in an m √ó n-matrix A. So if 1 ‚â§k ‚â§n and
Œ± = (Œ±1, Œ±2, . . . , Œ±k)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
92
The simplex algorithm
is a permutation of k elements chosen from the set {1, 2, . . . , n}, we let A‚àóŒ±
denote the m √ó k-matrix consisting of the columns A‚àóŒ±1, A‚àóŒ±2, . . . , A‚àóŒ±k in
the matrix A, i.e.
A‚àóŒ± =

A‚àóŒ±1 A‚àóŒ±2 . . . A‚àóŒ±k

.
And if
x =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x1
x2
...
xn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
is a column matrix with n entries, then xŒ± denotes the column matrix
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
xŒ±1
xŒ±2...
xŒ±k
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
As usual, we make no distinction between column matrices with n entries
and vectors in Rn.
We consider permutations Œ± = (Œ±1, Œ±2, . . . , Œ±k) as ordered sets and allow
us therefore to write j ‚ààŒ± if j is any of the numbers Œ±1, Œ±2, . . . , Œ±k. This
also allows us to write sums of the type
k

i=1
xŒ±iA‚àóŒ±i
as

j‚ààŒ±
xjA‚àój,
or with matrices as
A‚àóŒ±xŒ±.
DeÔ¨Ånition. Let A be an m√ón-matrix of rank m, and let Œ± = (Œ±1, Œ±2, . . . , Œ±m)
be a permutation of m numbers from the set {1, 2, . . . , n}. The permutation
Œ± is called a basic index set of the matrix A if the columns of the m√óm-matrix
A‚àóŒ± form a basis for Rm.
The condition that the columns A‚àóŒ±1, A‚àóŒ±2, . . . , A‚àóŒ±m form a basis is equiv-
alent to the condition that the submatrix
A‚àóŒ± =

A‚àóŒ±1 A‚àóŒ±2 . . . A‚àóŒ±m

Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
93
The simplex algorithm
is invertible. The inverse of the matrix A‚àóŒ± will be denoted by A‚àí1
‚àóŒ±. This
matrix, which thus means (A‚àóŒ±)‚àí1, will appear frequently in the sequel ‚àído
not confuse it with (A‚àí1)‚àóŒ±, which is not generally well deÔ¨Åned.
If Œ± = (Œ±1, Œ±2, . . . , Œ±m) is a basic index set, so too is of course every
permutation of Œ±.
Example 13.3.1. The matrix
3
1
1
‚àí3
3
‚àí1
2
‚àí6

has the following basic index sets: (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (4, 1)
(2, 3), (3, 2), (2, 4), and (4, 2).
We also need a convenient way to show the result of replacing an element
in an ordered set with some other element. Therefore, let M = (a1, a2, . . . , an)
be an arbitrary n-tuple (ordered set). The n-tuple obtained by replacing the
item ar at location r with an arbitrary object x will be denoted by MÀÜr[x]. In
other words,
MÀÜr[x] = (a1, . . . , ar‚àí1, x, ar+1, . . . , an).
An m √ó n-matrix can be regarded as an ordered set of columns. If b is
a column matrix with m entries and 1 ‚â§r ‚â§n, we therefore write AÀÜr[b] for
the matrix

A‚àó1 . . . A‚àór‚àí1 b A‚àór+1 . . . A‚àón

.
Another context in which we will use the above notation for replacement
of elements, is when Œ± = (Œ±1, Œ±2, . . . , Œ±m) is a permutation of m elements
taken from the set {1, 2, . . . , n}. If 1 ‚â§r ‚â§m, 1 ‚â§k ‚â§n and k /‚ààŒ±, then
Œ±ÀÜr[k] denotes the new permutation
(Œ±1, . . . , Œ±r‚àí1, k, Œ±r+1, . . . , Œ±m).
Later we will need the following simple result, where the above notation
is used.
Lemma 13.3.1. Let E be the unit matrix of order m, and let b be a column
matrix with m elements. The matrix EÀÜr[b] is invertible if and only if br Ã∏= 0,
and in this case
EÀÜr[b]‚àí1 = EÀÜr[c],
where
cj =

‚àíbj/br
for j Ã∏= r,
1/br
for j = r.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
94
The simplex algorithm
94
Proof. The proof is left to the reader as a simple exercise.
Example 13.3.2.
Ô£Æ
Ô£∞
1
4
0
0
3
0
0
5
1
Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞
1 ‚àí4/3
0
0
1/3
0
0 ‚àí5/3
1
Ô£π
Ô£ª
Systems of linear equations and basic solutions
Consider a system of linear equations
(13.6)
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn = b1
a21x1 + a22x2 + ¬∑ ¬∑ ¬∑ + a2nxn = b2
...
am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn = bm
with coeÔ¨Écient matrix A of rank m and right-hand side matrix b. Such a
system can equivalently be regarded as a vector equation
(13.6‚Ä≤)
n

j=1
xjA‚àój = b
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
95
The simplex algorithm
or as a matrix equation
(13.6‚Ä≤‚Ä≤)
Ax = b.
Both alternative approaches are, as we shall see, fruitful.
We solve the system (13.6), preferably using Gaussian elimination, by
expressing m of the variables, xŒ±1, xŒ±2, . . . , xŒ±m say, as linear combinations
of the remaining n ‚àím variables xŒ≤1, xŒ≤2, . . . , xŒ≤n‚àím and b1, b2, . . . , bm. Each
assignment of values to the latter Œ≤-variables results in a unique set of values
for the former Œ±-variables. In particular, we get a unique solution by setting
all Œ≤-variables equal to 0.
This motivates the following deÔ¨Ånition.
DeÔ¨Ånition. Let Œ± = (Œ±1, Œ±2, . . . , Œ±m) be a permutation of m numbers chosen
from the set {1, 2, . . . , n}, and let Œ≤ = (Œ≤1, Œ≤2, . . . , Œ≤n‚àím) be a permutation
of the remaining n ‚àím numbers. The variables xŒ±1, xŒ±2, . . . , xŒ±m are called
basic variables and the variables xŒ≤1, xŒ≤2, . . . , xŒ≤n‚àím are called free variables
in the system (13.6), if for each c = (c1, c2, . . . , cn‚àím) ‚ààRn‚àím there is a
unique solution x to the system (13.6) such that xŒ≤ = c. The unique solution
obtained by setting all free variables equal to 0 is called a basic solution.
Any m variables can not be chosen as basic variables; to examine which
ones can be selected, let Œ± = (Œ±1, Œ±2, . . . , Œ±m) be a permutation of m numbers
from the set {1, 2, . . . , n} and let Œ≤ = (Œ≤1, Œ≤2, . . . , Œ≤n‚àím) be an arbitrary
permutation of the remaining n ‚àím numbers, and rewrite equation (13.6‚Ä≤)
as
(13.6‚Ä≤‚Ä≤‚Ä≤)
m

j=1
xŒ±jA‚àóŒ±j = b ‚àí
n‚àím

j=1
xŒ≤jA‚àóŒ≤j.
If Œ± is a basic index set, i.e. if the columns A‚àóŒ±1, A‚àóŒ±2, . . . , A‚àóŒ±m form a
basis of Rm, then equation (13.6‚Ä≤‚Ä≤‚Ä≤) has clearly a unique solution for each
assignment of values to the Œ≤-variables, and (xŒ±1, xŒ±2, . . . , xŒ±m) is in fact the
coordinates of the vector b ‚àín‚àím
j=1 xŒ≤jA‚àóŒ≤j in this basis. In particular, the
coordinates of the vector b are equal to (xŒ±1, xŒ±2, . . . , xŒ±m), where x is the
corresponding basic solution, deÔ¨Åned by the condition that xŒ≤j = 0 for all j.
Conversely, suppose that each assignment of values to the Œ≤-variables
determines uniquely the values of the Œ±-variables. In particular, the equation
(13.7)
m

j=1
xŒ±jA‚àóŒ±j = b
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
96
The simplex algorithm
has then a unique solution, and this implies that the equation
(13.8)
m

j=1
xŒ±jA‚àóŒ±j = 0
has no other solution then the trivial one, xŒ±j = 0 for all j, because we would
otherwise get several solutions to equation (13.7) by to a given one adding
a non-trivial solution to equation (13.8). The columns A‚àóŒ±1, A‚àóŒ±2, . . . , A‚àóŒ±m
are in other words linearly independent, and they form a basis for Rm since
they are m in number. Hence, Œ± is a basic index set.
In summary, we have proved the following result.
Theorem 13.3.2. The variables xŒ±1, xŒ±2, . . . , xŒ±m are basic variables in the
system (13.6) if and only if Œ± is a basic index set of the coeÔ¨Écient matrix A.
Let us now express the basic solution corresponding to the basic index
set Œ± in matrix form. By writing the matrix equation (13.6‚Ä≤‚Ä≤) in the form
A‚àóŒ±xŒ± + A‚àóŒ≤xŒ≤ = b
and multiplying from the left by the matrix A‚àí1
‚àóŒ±, we get
xŒ± + A‚àí1
‚àóŒ±A‚àóŒ≤xŒ≤ = A‚àí1
‚àóŒ±b,
i.e.
xŒ± = A‚àí1
‚àóŒ±b ‚àíA‚àí1
‚àóŒ±A‚àóŒ≤xŒ≤,
which expresses the basic variables as linear combinations of the free variables
and the coordinates of b. The basic solution is obtained by setting xŒ≤ = 0
and is given by
xŒ± = A‚àí1
‚àóŒ±b , xŒ≤ = 0.
We summarize this result in the following theorem.
Theorem 13.3.3. Let Œ± be a basic index set of the matrix A. The corre-
sponding basic solution x to the system Ax = b is given by the conditions
xŒ± = A‚àí1
‚àóŒ±b
and
xk = 0
for k /‚ààŒ±.
The n‚àím free variables in a basic solution are equal to zero by deÔ¨Ånition.
Of course, some basic variable may also happen to be equal to zero, and since
this results in certain complications for the simplex algorithm, we make the
following deÔ¨Ånition.
DeÔ¨Ånition. A basic solution x is called non-degenerate if xi Ã∏= 0 for m indices
i and degenerate if xi Ã∏= 0 for less than m indices i.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
97
The simplex algorithm
97
Two basic index sets Œ± and Œ±‚Ä≤, which are permutations of each other,
naturally give rise to the same basic solution x. So the number of diÔ¨Äerent
basic solutions to a system Ax = b with m equations and n unknowns is at
most equal to the number of subsets with m elements that can be chosen
from the set {1, 2, . . . , n}, i.e. at most equal to
n
m

. The number is smaller
if the matrix A contains m linearly dependent columns.
Example 13.3.3. The system
3x1 + x2 + x3 ‚àí3x4 = 3
3x1 ‚àíx2 + 2x3 ‚àí6x4 = 3
has ‚àíapart from permutations ‚àíthe following basic index sets: (1, 2), (1, 3),
(1, 4), (2, 3) and (2, 4), and the corresponding basic solutions are in turn
(1, 0, 0, 0), (1, 0, 0, 0), (1, 0, 0, 0), (0, 1, 2, 0) and (0, 1, 0, ‚àí2
3). The basic so-
lution (1, 0, 0, 0) is degenerate, and the other two basic solutions are non-
degenerate.
The reason for our interest in basic index sets and basic solutions is that
optimal values of LP problems are attained at extreme points, and these
points are basic solutions, because we have the following characterisation of
extreme points.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
98
The simplex algorithm
Theorem 13.3.4. Suppose that A is an m√ón-matrix of rank m, that b ‚ààRm
and that c ‚ààRn. Then:
(i) x is an extreme point of the polyhedron X = {x ‚ààRn | Ax = b, x ‚â•0}
if and only if x is a nonnegative basic solution to the system Ax = b,
i.e. if and only if there is a basic index set Œ± of the matrix A such that
xŒ± = A‚àí1
‚àóŒ±b ‚â•0 and xk = 0 for k /‚ààŒ±.
(ii) y is an extreme point of the polyhedron Y = {y ‚ààRm | ATy ‚â§c} if and
only if ATy ‚â§c and there is a basic index set Œ± of the matrix A such
that y = (A‚àí1
‚àóŒ±)TcŒ±.
Proof. (i) According to Theorem 5.1.1 in Part I, x is an extreme point of the
polyhedron X if and only if x ‚â•0 and x is the unique solution of a system
of linear equations consisting of the equation Ax = b and n ‚àím equations
out of the n equations x1 = 0, x2 = 0, . . . , xn = 0. Let Œ±1, Œ±2, . . . , Œ±m be the
indices of the m equations xi = 0 that are not used in this system. Then,
Œ± = (Œ±1, Œ±2, . . . , Œ±m) is a basic index set and x is the corresponding basic
solution.
(ii) Because of the same theorem, y is an extreme point of the polyhedron
Y if and only if y ‚ààY and y is the unique solution of a quadratic system of
linear equations obtained by selecting m out of the n equations in the system
ATy = c. Let Œ±1, Œ±2, . . . , Œ±m denote the indices of the selected equations.
The quadratic system is then of the form (A‚àóŒ±)Ty = cŒ±, and this system
of equations has a unique solution y = (A‚àí1
‚àóŒ±)TcŒ± if and only if A‚àóŒ± is an
invertible matrix, i.e. if and only if Œ± = (Œ±1, Œ±2, . . . , Œ±m) is a basic index set
of A.
Example 13.3.4. It follows from Theorem 13.3.4 and Example 13.3.3 that
the polyhedron X of solutions to the system
3x1 + x2 + x3 ‚àí3x4 = 3
3x1 ‚àíx2 + 2x3 ‚àí6x4 = 3, x ‚â•0
has two extreme points, namely (1, 0, 0, 0) and (0, 1, 2, 0).
The ‚Äùdual‚Äù polyhedron Y of solutions to the system
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
3y1 + 3y2 ‚â§
2
y1 ‚àíy2 ‚â§
1
y1 + 2y2 ‚â§
1
‚àí3y1 ‚àí6y2 ‚â§‚àí1
has three extreme points, namely (5
6, ‚àí1
6), ( 1
3, 1
3) and ( 7
9, ‚àí2
9), corresponding
to the basic index sets (1, 2), (1, 3) and (2, 4). (The points associated with
the other two basic index sets (1, 4) and (2, 3), y = (1, ‚àí1
3) and y = (1, 0),
respectively, are not extreme points since they lie outside Y .)
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
99
The simplex algorithm
Changing basic index sets
We will now discuss how to generate a suite of basic solutions by successively
replacing one element at a time in the basic index set.
Theorem 13.3.5. Suppose that Œ± = (Œ±1, Œ±2, . . . , Œ±m) is a basic index set of
the system Ax = b and let x denote the corresponding basic solution. Let k
be a column index not belonging to the basic index set Œ±, and let v ‚ààRn be
the column vector deÔ¨Åned by
vŒ± = A‚àí1
‚àóŒ±A‚àók,
vk = ‚àí1
and
vj = 0
for j /‚ààŒ± ‚à™{k}.
(i) Then Av = 0, so it follows that x‚àítv is a solution to the system Ax = b
for all t ‚ààR.
(ii) Suppose that 1 ‚â§r ‚â§m and deÔ¨Åne a new ordered set Œ±‚Ä≤ by replacing
the element Œ±r in Œ± with the number k, i.e.
Œ±‚Ä≤ = Œ±ÀÜr[k] = (Œ±1, . . . , Œ±r‚àí1, k, Œ±r+1, . . . , Œ±m).
Then, Œ±‚Ä≤ is a basic index set if and only if vŒ±r Ã∏= 0. In this case,
A‚àí1
‚àóŒ±‚Ä≤ = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ±
and if x‚Ä≤ is the basic solution corresponding to the basic index set Œ±‚Ä≤,
then
x‚Ä≤ = x ‚àíœÑv,
where œÑ = xŒ±r/vŒ±r.
(iii) The two basic solutions x and x‚Ä≤ are identical if and only if œÑ = 0. So
if x is a non-degenerate basic solution, then x Ã∏= x‚Ä≤.
We will call v the search vector associated with the basic index set Œ± and
the index k, since we obtain the new basic solution x‚Ä≤ from the old one x by
searching in the direction of minus v.
Proof. (i)
It follows immediately from the deÔ¨Ånition of v that
Av =

j‚ààŒ±
vjA‚àój +

j /‚ààŒ±
vjA‚àój = A‚àóŒ±vŒ± ‚àíA‚àók = A‚àók ‚àíA‚àók = 0.
(ii) The set Œ±‚Ä≤ is a basic index set if and only if A‚àóŒ±‚Ä≤ is an invertible matrix.
But
A‚àí1
‚àóŒ±A‚àóŒ±‚Ä≤ = A‚àí1
‚àóŒ±

A‚àóŒ±1 . . . A‚àóŒ±r‚àí1 A‚àók A‚àóŒ±r+1 . . . A‚àóŒ±m

=

A‚àí1
‚àóŒ±A‚àóŒ±1 . . . A‚àí1
‚àóŒ±A‚àóŒ±r‚àí1 A‚àí1
‚àóŒ±A‚àók A‚àí1
‚àóŒ±A‚àóŒ±r+1 . . . A‚àí1
‚àóŒ±A‚àóŒ±m

=

E‚àó1 . . . E‚àór‚àí1 vŒ± E‚àór+1 . . . E‚àóm

= EÀÜr[vŒ±],
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
100
The simplex algorithm
100
where of course E denotes the unit matrix of order m. Hence
A‚àóŒ±‚Ä≤ = A‚àóŒ±EÀÜr[vŒ±].
The matrix A‚àóŒ±‚Ä≤ is thus invertible if and only if the matrix EÀÜr[vŒ±] is invertible,
and this is the case if and only if vŒ±r Ã∏= 0, according to Lemma 13.3.1. If the
inverse exists, then
A‚àí1
‚àóŒ±‚Ä≤ =

A‚àóŒ±EÀÜr[vŒ±]
‚àí1 = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ±.
Now, deÔ¨Åne xœÑ = x ‚àíœÑv. Then xœÑ is a solution to the equation Ax = b,
by part (i) of the theorem, so in order to prove that xœÑ is the basic solution
corresponding to the basic index set Œ±‚Ä≤, it suÔ¨Éces to show that xœÑ
j = 0 for all
j /‚ààŒ±‚Ä≤, i.e. for j = Œ±r and for j /‚ààŒ± ‚à™{k}.
But xœÑ
Œ±r = xŒ±r ‚àíœÑvŒ±r = 0, because of the deÔ¨Ånition of œÑ, and if j /‚ààŒ±‚à™{k}
then xj and vj are both equal to 0, whence xœÑ
j = xj ‚àíœÑvj = 0.
(iii) Since vk = ‚àí1, we have œÑv = 0 if and only if œÑ = 0. Hence , x‚Ä≤ = x if
and only if œÑ = 0.
If the basic solution x is non-degenerate, then xj Ã∏= 0 for all j ‚ààŒ± and in
particular xŒ±r Ã∏= 0, which implies that œÑ Ã∏= 0, and that x‚Ä≤ Ã∏= x.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
101
The simplex algorithm
Corollary
13.3.6. Keep the asumptions of Theorem 13.3.5 and suppose in
addition that x ‚â•0, that the index set
I+ = {j ‚ààŒ± | vj > 0}
is nonempty, and that the index r is chosen so that Œ±r ‚ààI+ and
œÑ = xŒ±r/vŒ±r = min{xj/vj | j ‚ààI+}.
Then x‚Ä≤ ‚â•0.
Proof. Since x‚Ä≤
j = 0 for all j /‚ààŒ±‚Ä≤, it suÔ¨Éces to show that x‚Ä≤
j ‚â•0 for all
j ‚ààŒ± ‚à™{k}.
We begin by noting that œÑ ‚â•0 since x ‚â•0, and therefore
x‚Ä≤
k = xk ‚àíœÑvk = 0 + œÑ ‚â•0.
For indices j ‚ààŒ± \ I+ we have vj ‚â§0, and this implies that
x‚Ä≤
j = xj ‚àíœÑvj ‚â•xj ‚â•0.
Finally, if j ‚ààI+, then xj/vj ‚â•œÑ, and it follows that
x‚Ä≤
j = xj ‚àíœÑvj ‚â•0.
This completes the proof.
13.4
The simplex algorithm
The variant of the simplex algorithm that we shall describe assumes that the
LP problem is given in standard form. So we start from the problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
where A is an m √ó n-matrix, b ‚ààRm and c ‚ààRn.
We assume that
rank A = m = the number of rows in A.
Of course, this is no serious restriction, because if rank A < m and the system
Ax = b is consistent, then we can delete (m ‚àírank A) constraint equations
without changing the set of solutions, and this leaves us with an equivalent
system A‚Ä≤x = b, where the rank of A‚Ä≤ is equal to the number of rows in A‚Ä≤.
Let us call a basic index set Œ± of the matrix A and the corresponding
basic solution x to the system Ax = b feasible, if x is a feasible point for our
standard problem, i.e. if x ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
102
The simplex algorithm
The simplex algorithm starts from a feasible basic index set Œ± of the
matrix A, and we shall show in Section 13.6 how to Ô¨Ånd such an index set
by applying the simplex algorithm to a so-called artiÔ¨Åcial problem.
First compute the corresponding feasible basic solution x, i.e.
xŒ± = A‚àí1
‚àóŒ±b ‚â•0,
and then the number Œª ‚ààR and the column vectors y ‚ààRm and z ‚ààRn,
deÔ¨Åned as
Œª = ‚ü®c, x‚ü©= ‚ü®cŒ±, xŒ±‚ü©
y = (A‚àí1
‚àóŒ±)TcŒ±
z = c ‚àíATy.
The number Œª is thus equal to the value of the objective function at x.
Note that zŒ± = cŒ± ‚àí(ATy)Œ±‚àó= cŒ± ‚àí(A‚àóŒ±)Ty = cŒ± ‚àícŒ± = 0, so in order
to compute the vector z we only have to compute its coordinates
zj = cj ‚àí(A‚àój)Ty = cj ‚àí‚ü®A‚àój, y‚ü©
for indices j /‚ààŒ±. The numbers zj are usually called reduced costs.
Lemma 13.4.1. The number Œª and the vectors x, y and z have the following
properties:
(i) ‚ü®z, x‚ü©= 0, i.e. the vectors z and x are orthogonal.
(ii) Ax = 0 ‚áí‚ü®c, x‚ü©= ‚ü®z, x‚ü©.
(iii) Ax = b ‚áí‚ü®c, x‚ü©= Œª + ‚ü®z, x‚ü©.
(iv) If v is the search vector corresponding to the basic index set Œ± and the
index k /‚ààŒ±, then ‚ü®c, x ‚àítv‚ü©= Œª + tzk.
Proof. (i) Since zj = 0 for j ‚ààŒ± and xj = 0 for j /‚ààŒ±,
‚ü®z, x‚ü©=

j‚ààŒ±
zjxj +

j /‚ààŒ±
zjxj = 0 + 0 = 0.
(ii) It follows immediately from the deÔ¨Ånition of z that
‚ü®z, x‚ü©= ‚ü®c, x‚ü©‚àí‚ü®ATy, x‚ü©= ‚ü®c, x‚ü©‚àí‚ü®y, Ax‚ü©= ‚ü®c, x‚ü©
for all x satisfying the equation Ax = 0.
(iii) If Ax = b, then
‚ü®c, x‚ü©‚àí‚ü®z, x‚ü©= ‚ü®ATy, x‚ü©= ‚ü®y, Ax‚ü©= ‚ü®(A‚àí1
‚àóŒ±)TcŒ±, b‚ü©= ‚ü®cŒ±, A‚àí1
‚àóŒ±b‚ü©
= ‚ü®cŒ±, xŒ±‚ü©= Œª.
(iv) Since Av = 0, it follows from (ii) that
‚ü®c, x ‚àítv‚ü©= ‚ü®c, x‚ü©‚àít‚ü®c, v‚ü©= Œª ‚àít‚ü®z, v‚ü©= Œª + tzk.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
103
The simplex algorithm
The following theorem contains all the essential ingredients of the simplex
algorithm.
Theorem 13.4.2. Let Œ±, x, Œª, y and z be deÔ¨Åned as above.
(i) (Optimality) If z ‚â•0, then x is an optimal solution to the minimiza-
tion problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
and y is an optimal solution to the dual maximization problem
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c
with Œª as the optimal value. The optimal solution x to the minimization
problem is unique if zj > 0 for all j /‚ààŒ±.
(ii) Suppose that z Ã∏‚â•0, and let k be an index such that zk < 0. Let further
v be the search vector associated to Œ± and k, i.e.
vŒ± = A‚àí1
‚àóŒ±A‚àók,
vk = ‚àí1,
vj = 0
for j /‚ààŒ± ‚à™{k},
and set xt = x ‚àítv for t ‚â•0. Depending on whether v ‚â§0 or v Ã∏‚â§0,
the following applies:
(ii a) (Unbounded objective function) If v ‚â§0, then the points
xt are feasible for the minimization problem for all t ‚â•0 and
‚ü®c, xt‚ü©‚Üí‚àí‚àûas t ‚Üí‚àû. The objective function is thus unbounded
below, and the dual maximization problem has no feasible points.
(ii b) (Iteration step) If v Ã∏‚â§0, then deÔ¨Åne a new basic index set
Œ±‚Ä≤ and the number œÑ as in Theorem 13.3.5 (ii) with the index r
chosen as in Corollary 13.3.6. The basic index set Œ±‚Ä≤ is feasible
with x‚Ä≤ = x ‚àíœÑv as the corresponding feasible basic solution, and
‚ü®c, x‚Ä≤‚ü©= ‚ü®c, x‚ü©+ œÑzk ‚â§‚ü®c, x‚ü©.
Hence, ‚ü®c, x‚Ä≤‚ü©< ‚ü®c, x‚ü©, if œÑ > 0.
Proof. (i) Suppose that z ‚â•0 and that x is an arbitrary feasible point for
the minimization problem. Then ‚ü®z, x‚ü©‚â•0 (since x ‚â•0), and it follows
from part (iii) of Lemma 13.4.1 that ‚ü®c, x‚ü©‚â•Œª = ‚ü®c, x‚ü©. The point x is thus
optimal and the optimal value is equal to Œª.
The condition z ‚â•0 also implies that ATy = c ‚àíz ‚â§c, i.e. y is a feasible
point for the dual maximization problem, and
‚ü®b, y‚ü©= ‚ü®y, b‚ü©= ‚ü®(A‚àí1
‚àóŒ±)TcŒ±, b‚ü©= ‚ü®cŒ±, A‚àí1
‚àóŒ±b‚ü©= ‚ü®cŒ±, xŒ±‚ü©= ‚ü®c, x‚ü©,
so if follows from the optimality criterion (Corollary 13.1.2) that y is an
optimal solution to the dual problem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
104
The simplex algorithm
104
Now suppose that zj > 0 for all j /‚ààŒ±. If x is a feasible point Ã∏= x, then
xj0 > 0 for some index j0 /‚ààŒ±, and it follows that ‚ü®z, x‚ü©= 
j /‚ààŒ± zjxj ‚â•
zj0xj0 > 0. Hence, ‚ü®c, x‚ü©= Œª + ‚ü®z, x‚ü©> Œª = ‚ü®c, x‚ü©, by Lemma 13.4.1 (iii).
This proves that the minimum point is unique.
(ii a) According to Theorem 13.3.5, xt is a solution to the equation Ax = b
for all real numbers t, and if v ‚â§0 then xt = x ‚àítv ‚â•x ‚â•0 for t ‚â•0. So
the points xt are feasible for all t ‚â•0 if v ‚â§0, and by Lemma 13.4.1 (iv),
lim
t‚Üí‚àû‚ü®c, xt‚ü©= Œª + lim
t‚Üí‚àûzkt = ‚àí‚àû.
The objective function is thus not bounded below.
Suppose that the dual maximization problem has a feasible point y. Then,
‚ü®b, y‚ü©‚â§‚ü®c, xt‚ü©for all t ‚â•0, by the weak duality theorem, and this is contra-
dictory since the right hand side tends to ‚àí‚àûas t ‚Üí‚àû. So it follows that
the dual maximization problem has no feasible points.
(ii b) By Corollary 13.3.6, Œ±‚Ä≤ is a feasible basic solution with xœÑ as the cor-
responding basic solution, and the inequality ‚ü®c, x‚Ä≤‚ü©‚â§‚ü®c, x‚ü©now follows
directly from Lemma 13.4.1 (iv), because œÑ ‚â•0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
105
The simplex algorithm
Theorem 13.4.2 gives rise to the following algorithm for solving the stan-
dard problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0.
The simplex algorithm
Given a feasible basic index set Œ±.
1. Compute the matrix A‚àí1
‚àóŒ±, the corresponding feasible basic solution x,
i.e. xŒ± = A‚àí1
‚àóŒ±b and xj = 0 for j /‚ààŒ±, and the number Œª = ‚ü®cŒ±, xŒ±‚ü©.
Repeat steps 2‚Äì8 until a stop occurs.
2. Compute the vector y = (A‚àí1
‚àóŒ±)TcŒ± and the numbers zj = cj ‚àí‚ü®A‚àój, y‚ü©
for j /‚ààŒ±.
3. Stopping criterion: quit if zj ‚â•0 for all j /‚ààŒ±.
Optimal solution: x. Optimal value: Œª. Optimal dual solution: y.
4. Choose otherwise an index k such that zk < 0, compute the corre-
sponding search vector v, i.e. vŒ± = A‚àí1
‚àóŒ±A‚àók, vk = ‚àí1 and vj = 0 for
j /‚ààŒ± ‚à™{k}, and put I+ = {j ‚ààŒ± | vj > 0}.
5. Stopping criterion: quit if I+ = ‚àÖ.
Optimal value: ‚àí‚àû.
6. DeÔ¨Åne otherwise œÑ = min{xj/vj | j ‚ààI+} and determine an index r so
that Œ±r ‚ààI+ and xŒ±r/vŒ±r = œÑ.
7. Put Œ±‚Ä≤ = Œ±ÀÜr[k] and compute the inverse A‚àí1
‚àóŒ±‚Ä≤ = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ±.
8. Update: Œ±:= Œ±‚Ä≤, A‚àí1
‚àóŒ± := A‚àí1
‚àóŒ±‚Ä≤, x:= x ‚àíœÑv, and Œª:= Œª + œÑzk.
Before we can call the above procedure an algorithm in the sense of a
mechanical calculation that a machine can perform, we need to specify how
to choose k in step 4 in the case when zj < 0 for several indices j, and r in
step 6 when xj/vj = œÑ for more than one index j ‚ààI+.
A simple rule that works well most of the time, is to select the index j
that minimizes zj (and if there are several such indices the least of these) as
the index k, and the smallest of all indices i for which xŒ±i/vŒ±i = œÑ as the
index r. We shall return to the choice of k and r later; for the immediate
discussion of the algorithm, it does not matter how to make the choice.
We also need a method to Ô¨Ånd an initial feasible basic index set to start
the simplex algorithm from.
We shall treat this problem and solve it in
Section 13.6.
Now suppose that we apply the simplex algorithm to an LP problem
in standard form, starting from a feasible basic index set. It follows from
Theorem 13.4.2 that the algorithm delivers an optimal solution if it stops
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
106
The simplex algorithm
during step 3, and that the objective function is unbounded from below if
the algorithm stops during step 5.
So let us examine what happens if the algorithm does not stop. Since
a feasible basic index set is generated each time the algorithm comes to
step 7, we will obtain in this case an inÔ¨Ånite sequence Œ±1, Œ±2, Œ±3, . . . of feasible
basic index sets with associated feasible basic solutions x1, x2, x3, . . .. As the
number of diÔ¨Äerent basic index sets is Ô¨Ånite, some index set Œ±p has to be
repeated after a number of additional, say q, iterations. This means that Œ±p =
Œ±p+q and xp = xp+q and in turn implies that the sequence Œ±p, Œ±p+1, . . . , Œ±p+q‚àí1
is repeated periodically in all inÔ¨Ånity. We express this by saying that the
algorithm cycles. According to (ii)
¬Ø
in Theorem 13.4.2,
‚ü®c, xp‚ü©‚â•‚ü®c, xp+1‚ü©‚â•¬∑ ¬∑ ¬∑ ‚â•‚ü®c, xp+q‚ü©= ‚ü®c, xp‚ü©,
and this implies that
‚ü®c, xp‚ü©= ‚ü®c, xp+1‚ü©= ¬∑ ¬∑ ¬∑ = ‚ü®c, xp+q‚àí1‚ü©.
The number œÑ is hence equal to 0 for all the iterations of the cycle, and
this implies that the basic solutions xp, xp+1, . . . , xp+q‚àí1 are identical and
degenerate. If the simplex algorithm does not stop, but continues indeÔ¨Ånitely,
it is so because the algortihm has got stuck in a degenerate basic solution.
The following theorem is now an immediate consequence of the above
discussion.
Theorem 13.4.3. The simplex algorithm stops when applied to an LP prob-
lem in which all feasible basic solutions are non-degenerate.
Cycling can occur, and we shall give an example of this in the next sec-
tion. Theoretically, this is a bit troublesome, but cycling seems to be a rare
phenomenon in practical problems and therefore lacks practical signiÔ¨Åcance.
The small rounding errors introduced during the numerical treatment of an
LP problem also have a beneÔ¨Åcial eÔ¨Äect since these errors usually turn de-
generate basic solutions into non-degenerate solutions and thereby tend to
prevent cycling. There is also a simple rule for the choice of indices k and r,
Bland‚Äôs rule, which prevents cycling and will be described in the next section.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
107
The simplex algorithm
107
Example
Example 13.4.1. We now illustrate the simplex algorithm by solving the
minimization problem
min x1 ‚àíx2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
‚àí2x1 + x2 + x3 ‚â§3
‚àíx1 + x2 ‚àí2x3 ‚â§3
2x1 ‚àíx2 + 2x3 ‚â§1, x ‚â•0.
We start by writing the problem in standard form by introducing three
slack variables:
min x1 ‚àíx2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
‚àí2x1 + x2 + x3 + x4
= 3
‚àíx1 + x2 ‚àí2x3
+ x5
= 3
2x1 ‚àíx2 + 2x3
+ x6 = 1, x ‚â•0.
Using matrices, this becomes
min cTx
s.t.
Ax = b, x ‚â•0
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
108
The simplex algorithm
with
A =
Ô£Æ
Ô£∞
‚àí2
1
1
1
0
0
‚àí1
1 ‚àí2
0
1
0
2 ‚àí1
2
0
0
1
Ô£π
Ô£ª,
b =
Ô£Æ
Ô£∞
3
3
1
Ô£π
Ô£ª
and
cT =

1 ‚àí1
1
0
0
0

.
We note that we can start the simplex algorithm with
Œ± = (4, 5, 6),
A‚àí1
‚àóŒ± =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª,
xŒ± =
Ô£Æ
Ô£∞
3
3
1
Ô£π
Ô£ª,
Œª = ‚ü®cŒ±, xŒ±‚ü©= c T
Œ±xŒ± =

0
0
0

Ô£Æ
Ô£∞
3
3
1
Ô£π
Ô£ª= 0.
1st iteration:
y = (A‚àí1
‚àóŒ±)TcŒ± =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª
z1,2,3 = c1,2,3 ‚àí(A‚àó1,2,3)Ty =
Ô£Æ
Ô£∞
1
‚àí1
1
Ô£π
Ô£ª‚àí
Ô£Æ
Ô£∞
‚àí2 ‚àí1
2
1
1 ‚àí1
1 ‚àí2
2
Ô£π
Ô£ª
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
‚àí1
1
Ô£π
Ô£ª.
Since z2 = ‚àí1 < 0, we have to select k = 2 and then
vŒ± = A‚àí1
‚àóŒ±A‚àók =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
1
‚àí1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
1
‚àí1
Ô£π
Ô£ª,
v2 = ‚àí1
I+ = {j ‚ààŒ± | vj > 0} = {4, 5}
œÑ = min{xj/vj | j ‚ààI+} = min{x4/v4, x5/v5} = min{3/1, 3/1} = 3
for Œ±1 = 4, i.e.
r = 1
Œ±‚Ä≤ = Œ±ÀÜr[k] = (4, 5, 6)ÀÜ1[2] = (2, 5, 6)
EÀÜr[vŒ±]‚àí1 =
Ô£Æ
Ô£∞
1
0
0
1
1
0
‚àí1
0
1
Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞
1
0
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
A‚àí1
‚àóŒ±‚Ä≤ = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ± =
Ô£Æ
Ô£∞
1
0
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
0
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
109
The simplex algorithm
x‚Ä≤
Œ±‚Ä≤ = xŒ±‚Ä≤ ‚àíœÑvŒ±‚Ä≤ =
Ô£Æ
Ô£∞
0
3
1
Ô£π
Ô£ª‚àí3
Ô£Æ
Ô£∞
‚àí1
1
‚àí1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
3
0
4
Ô£π
Ô£ª
Œª‚Ä≤ = Œª + œÑzk = 0 + 3 (‚àí1) = ‚àí3.
Update: Œ±:= Œ±‚Ä≤, A‚àí1
‚àóŒ± := A‚àí1
‚àóŒ±‚Ä≤, xŒ± := x‚Ä≤
Œ±‚Ä≤ and Œª:= Œª‚Ä≤.
2nd iteration:
y = (A‚àí1
‚àóŒ±)TcŒ± =
Ô£Æ
Ô£∞
1 ‚àí1
1
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
0
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
0
0
Ô£π
Ô£ª
z1,3,4 = c1,3,4 ‚àí(A‚àó1,3,4)Ty =
Ô£Æ
Ô£∞
1
1
0
Ô£π
Ô£ª‚àí
Ô£Æ
Ô£∞
‚àí2 ‚àí1
2
1 ‚àí2
2
1
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
0
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
2
1
Ô£π
Ô£ª.
Since z1 = ‚àí1 < 0,
k = 1
vŒ± = A‚àí1
‚àóŒ±A‚àók =
Ô£Æ
Ô£∞
1
0
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí2
‚àí1
2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí2
1
0
Ô£π
Ô£ª,
v1 = ‚àí1
I+ = {j ‚ààŒ± | vj > 0} = {5}
œÑ = x5/v5 = 0/1 = 0
for Œ±2 = 5, i.e.
r = 2
Œ±‚Ä≤ = Œ±ÀÜr[k] = (2, 5, 6)ÀÜ2[1] = (2, 1, 6)
EÀÜr[vŒ±]‚àí1 =
Ô£Æ
Ô£∞
1
‚àí2
0
0
1
0
0
0
1
Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞
1
2
0
0
1
0
0
0
1
Ô£π
Ô£ª
A‚àí1
‚àóŒ±‚Ä≤ = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ± =
Ô£Æ
Ô£∞
1
2
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
0
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
2
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
x‚Ä≤
Œ±‚Ä≤ = xŒ±‚Ä≤ ‚àíœÑvŒ±‚Ä≤ =
Ô£Æ
Ô£∞
3
0
4
Ô£π
Ô£ª‚àí0
Ô£Æ
Ô£∞
‚àí2
‚àí1
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
3
0
4
Ô£π
Ô£ª
Œª‚Ä≤ = Œª + œÑzk = ‚àí3 + 0 (‚àí1) = ‚àí3.
Update: Œ±:= Œ±‚Ä≤, A‚àí1
‚àóŒ± := A‚àí1
‚àóŒ±‚Ä≤, xŒ± := x‚Ä≤
Œ±‚Ä≤ and Œª:= Œª‚Ä≤.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
110
The simplex algorithm
110
3rd iteration:
y = (A‚àí1
‚àóŒ±)
TcŒ± =
Ô£Æ
Ô£∞
‚àí1 ‚àí1
1
2
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
1
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
‚àí1
0
Ô£π
Ô£ª
z3,4,5 = c3,4,5 ‚àí(A‚àó3,4,5)Ty =
Ô£Æ
Ô£∞
1
0
0
Ô£π
Ô£ª‚àí
Ô£Æ
Ô£∞
1 ‚àí2
2
1
0
0
0
1
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
0
‚àí1
0
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
0
1
Ô£π
Ô£ª.
Since z3 = ‚àí1 < 0,
k = 3
vŒ± = A‚àí1
‚àóŒ±A‚àók =
Ô£Æ
Ô£∞
‚àí1
2
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
‚àí2
2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí5
‚àí3
3
Ô£π
Ô£ª,
v3 = ‚àí1
I+ = {j ‚ààŒ± | vj > 0} = {6}
œÑ = x6/v6 = 4/3
for Œ±3 = 6, i.e.
r = 3
Œ±‚Ä≤ = Œ±ÀÜr[k] = (2, 1, 6)ÀÜ3[3] = (2, 1, 3)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
111
The simplex algorithm
EÀÜr[vŒ±]‚àí1 =
Ô£Æ
Ô£∞
1
0 ‚àí5
0
1 ‚àí3
0
0
3
Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞
1
0
5
3
0
1
1
0
0
1
3
Ô£π
Ô£ª
A‚àí1
‚àóŒ±‚Ä≤ = EÀÜr[vŒ±]‚àí1A‚àí1
‚àóŒ± =
Ô£Æ
Ô£∞
1
0
5
3
0
1
1
0
0
1
3
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
2
0
‚àí1
1
0
1
0
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
2
3
2
5
3
0
1
1
1
3
0
1
3
Ô£π
Ô£ª
x‚Ä≤
Œ±‚Ä≤ = xŒ±‚Ä≤ ‚àíœÑvŒ±‚Ä≤ =
Ô£Æ
Ô£∞
3
0
0
Ô£π
Ô£ª‚àí4
3
Ô£Æ
Ô£∞
‚àí5
‚àí3
‚àí1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
29
3
4
4
3
Ô£π
Ô£ª
Œª‚Ä≤ = Œª + œÑzk = ‚àí3 + 4
3(‚àí1) = ‚àí13
3 .
Update: Œ±:= Œ±‚Ä≤, A‚àí1
‚àóŒ± := A‚àí1
‚àóŒ±‚Ä≤,
xŒ± := x‚Ä≤
Œ±‚Ä≤ and Œª:= Œª‚Ä≤.
4th iteration:
y = (A‚àí1
‚àóŒ±)
TcŒ± =
Ô£Æ
Ô£∞
2
3
0
1
3
2
1
0
5
3
1
1
3
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
1
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
3
‚àí1
‚àí1
3
Ô£π
Ô£ª
z4,5,6 = c4,5,6 ‚àí(A‚àó4,5,6)Ty =
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª‚àí
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
3
‚àí1
‚àí1
3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
3
1
1
3
Ô£π
Ô£ª.
The solution x = (4, 29
3 , 4
3, 0, 0, 0) is optimal with optimal value ‚àí13
3 since
z4,5,6 > 0. The original minimization problem has the same optimal value, of
course, and (x1, x2, x3) = (4, 29
3 , 4
3) is the optimal solution.
The version of the simplex algorithm that we have presented is excellent
for computer calculations, but it is unnecessarily complicated for calculations
by hand.
Then it is better to use the tableau form which we utilized in
Section 13.2, even if this entails performing unnecessary calculations. To the
LP problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
we associate the following simplex tableau:
(13.9)
A
b
E
cT
0
0T
We have included the column on the far right of the table only to explain
how the tableau calculations work; it will be omitted later on.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
112
The simplex algorithm
Let Œ± be a feasible basic index set with x as the corresponding basic
solution. The upper part [ A
b
E ] of the tableau can be seen as a matrix,
and by multiplying this matrix from the left by A‚àí1
‚àóŒ±, we obtain the following
new tableau:
A‚àí1
‚àóŒ±A
A‚àí1
‚àóŒ±b
A‚àí1
‚àóŒ±
cT
0
0T
Now subtract the upper part of this tableau multiplied from the left by
c T
Œ± from the bottom row of the tableau. This results in the tableau
A‚àí1
‚àóŒ±A
A‚àí1
‚àóŒ±b
A‚àí1
‚àóŒ±
cT ‚àíc T
Œ±A‚àí1
‚àóŒ±A
‚àíc T
Œ±A‚àí1
‚àóŒ±b
‚àíc T
Œ±A‚àí1
‚àóŒ±
Using the notation introduced in the deÔ¨Ånition of the simplex algorithm,
we have A‚àí1
‚àóŒ±b = xŒ±, c T
Œ±A‚àí1
‚àóŒ± = ((A‚àí1
‚àóŒ±)TcŒ±)T = yT, cT‚àíc T
Œ±A‚àí1
‚àóŒ±A = cT‚àíyTA = zT
and c T
Œ±A‚àí1
‚àóŒ±b = c T
Œ±xŒ± = ‚ü®cŒ±, xŒ±‚ü©= Œª, which means that the above tableau can
be written in the form
(13.10)
A‚àí1
‚àóŒ±A
xŒ±
A‚àí1
‚àóŒ±
zT
‚àíŒª
‚àíyT
Note that the columns of the unit matrix appear as columns in the matrix
A‚àí1
‚àóŒ±A, because column number Œ±j in A‚àí1
‚àóŒ±A is identical with unit matrix
column E‚àój. Moreover, zŒ±j = 0.
When performing the actual calculations, we use Gaussian elimination to
get from tableau (13.9) to tableau (13.10).
If zT ‚â•0, which we can determine with the help of the bottom line in
(13.10), then x is an optimal solution, and we can also read oÔ¨Äthe optimal
solution y to the dual maximization problem. (The matrix A will in many
cases contain the columns of the unit matrix, and if so then it is of course
possible to read oÔ¨Äthe solution to the dual problem in the Ô¨Ånal simplex
tableau without Ô¨Årst having to add the unit matrix on the right side of
tableau (13.9).)
If zT Ã∏‚â•0, then we choose a column index k with zk < 0, and consider the
corresponding column a = A‚àí1
‚àóŒ±A‚àók (= vŒ±) in the upper part of the tableau.
The minimization problem is unbounded if a ‚â§0. In the opposite case,
we choose an index i = r that minimizes xŒ±i/ai (= xŒ±i/vŒ±i) among all ratios
with positive ai. This means that r is the index of a row with the least ratio
xŒ±i/ai among all rows with positive ai. Finally, we transform the simplex
tableau by pivoting around the element at location (r, k).
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
113
The simplex algorithm
113
Example 13.4.2. We solve Example 13.4.1 again ‚àíthis time by performing
all calculations in tabular form. Our Ô¨Årst tableau has the form
‚àí2
1
1
1
0
0
3
‚àí1
1
‚àí2
0
1
0
3
2
‚àí1
2
0
0
1
1
1
‚àí1
1
0
0
0
0
and in this case it is of course not necessary to repeat the columns of the
unit matrix in a separate part of the tableau in order also to solve the dual
problem.
The basic index set Œ± = (4, 5, 6) is feasible, and since A‚àóŒ± = E and c T
Œ± =

0
0
0

, we can directly read oÔ¨ÄzT =

1
‚àí1
1
0
0
0

and ‚àíŒª = 0
from the bottom line of the tableau.
The optimality criterion is not satisÔ¨Åed since z2 = ‚àí1 < 0, so we proceed
by choosing k = 2. The positive ratios of corresponding elements in the
right-hand side column and the second column are in this case the same and
equal to 3/1 for the Ô¨Årst and the second row. Therefore, we can choose r = 1
or r = 2, and we decide to use the smaller of the two numbers, i.e. we put
r = 1.
The tableau is then transformed by pivoting around the element at
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
114
The simplex algorithm
location (1, 2). By then continuing in the same style, we get the following
sequence of tableaux:
‚àí2
1
1
1
0
0
3
1
0
‚àí3
‚àí1
1
0
0
0
0
3
1
0
1
4
‚àí1
0
2
1
0
0
3
Œ± = (2, 5, 6),
k = 1,
r = 2
0
1
‚àí5
‚àí1
2
0
3
1
0
‚àí3
‚àí1
1
0
0
0
0
3
1
0
1
4
0
0
‚àí1
0
1
0
3
Œ± = (2, 1, 6),
k = 3,
r = 3
0
1
0
2
3
2
5
3
29
3
1
0
0
0
1
1
4
0
0
1
1
3
0
1
3
4
3
0
0
0
1
3
1
1
3
13
3
Œ± = (2, 1, 3)
The optimality criterion is now satisÔ¨Åed with x = (4, 29
3 , 4
3, 0, 0, 0) as op-
timal solution and ‚àí13
3 as optimal value. The dual problem has the optimal
solution (‚àí1
3, ‚àí1, ‚àí1
3).
Henceforth, we will use the tableau variant of the simplex algorithm to
account for our calculations, because it is the most transparent method.
The optimality condition in step 2 of the simplex algorithm is a suÔ¨Écient
condition for optimality, but the condition is not necessary. A degenerate
basic solution can be optimal without the optimality condition being satisÔ¨Åed.
Here is a trivial example of this.
Example 13.4.3. The problem
min ‚àíx2
s.t.
x1 + x2 = 0, x ‚â•0
has only one feasible point, x = (0, 0), which is therefore optimal. There are
two feasible basic index sets, Œ± = (1) and Œ±‚Ä≤ = (2), both with (0, 0) as the
corresponding degenerate basic solution.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
115
The simplex algorithm
The optimality condition is not fulÔ¨Ålled at the basic index set Œ±, because
y = 1 ¬∑ 0 = 0 and z2 = ‚àí1 ‚àí1 ¬∑ 0 = ‚àí1 < 0. At the other basic index set Œ±‚Ä≤,
y = 1 ¬∑ (‚àí1) = ‚àí1 and z2 = 0 ‚àí1 ¬∑ (‚àí1) = 1 > 0, and the optimality criterion
is now satisÔ¨Åed.
The corresponding simplex tableaux are
1
1
0
0
‚àí1
0
Œ± = (1)
and
1
1
0
1
0
0
Œ± = (2)
We shall now study a simple example with a non-unique optimal solution.
Example 13.4.4. The simplex tableaux associated with the problem
min x1 + x2
s.t.
x1 + x2 ‚àíx3
= 1
2x2 ‚àíx3 + x4 = 1, x ‚â•0
are as follows:
1
1
‚àí1
0
1
0
2
‚àí1
1
1
1
1
0
0
0
Œ± = (1, 4)
1
1
‚àí1
0
1
0
2
‚àí1
1
1
0
0
1
0
‚àí1
Œ± = (1, 4)
The optimality condition is met; x = (1, 0, 0, 1) is an optimal solution,
and the optimal value is 1. However, coeÔ¨Écient number 2 in the last row,
i.e. z2, is equal to 0, so we can therefore perform another iteration of the
simplex algorithm by choosing the second column as the pivot column and
the second row as the pivot row, i.e. k = 2 and r = 2. This gives rise to the
following new tableau:
1
0
‚àí1
2
‚àí1
2
1
2
0
1
‚àí1
2
1
2
1
2
0
0
1
0
‚àí1
Œ± = (1, 2)
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
116
The simplex algorithm
116
The optimality condition is again met, now with ÀÜx = ( 1
2, 1
2, 0, 0) as optimal
solution. Since the set of optimal solutions is convex, each point on the line
segment between ÀÜx and x is also an optimal point.
13.5
Bland‚Äôs anti cycling rule
We begin with an example of Kuhn showing that cycling can occur in de-
generate LP problems if the column index k and the row index r are not
properly selected.
Example 13.5.1. Consider the problem
min ‚àí2x1 ‚àí3x2 + x3 + 12x4
s.t.
Ô£±
Ô£≤
Ô£≥
‚àí2x1 ‚àí9x2 + x3 + 9x4 + x5
= 0
1
3x1 + x2 ‚àí1
3x3 ‚àí2x4
+ x6
= 0
2x1 + 3x2 ‚àíx3 ‚àí12x4
+ x7 = 2, x ‚â•0.
We use the simplex algorithm with the additional rule that the column index
k should be chosen so as to make zk as negative as possible and the row index
r should be the least among all allowed row indices. Our Ô¨Årst tableau is
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
117
The simplex algorithm
‚àí2
‚àí9
1
9
1
0
0
0
1
3
1
‚àí1
3
‚àí2
0
1
0
0
2
3
‚àí1
‚àí12
0
0
1
2
‚àí2
‚àí3
1
12
0
0
0
0
with Œ± = (5, 6, 7) as feasible basic index set. According to our rule for the
choice of of k, we must choose k = 2.
There is only one option for the
row index r, namely r = 2, so we use the element located at (2, 2) as pivot
element and obtain the following new tableau
1
0
‚àí2
‚àí9
1
9
0
0
1
3
1
‚àí1
3
‚àí2
0
1
0
0
1
0
0
‚àí6
0
‚àí3
1
2
‚àí1
0
0
6
0
3
0
0
with Œ± = (5, 2, 7). This time k = 1, but there are two row indices i with
the same least value of the ratios xŒ±i/vŒ±i, namely 1 and 2. Our additional
rule tells us to choose r = 1. Pivoting around the element at location (1, 1)
results in the next tableau
1
0
‚àí2
‚àí9
1
9
0
0
0
1
1
3
1
‚àí1
3
‚àí2
0
0
0
0
2
3
‚àí1
‚àí12
1
2
0
0
‚àí2
‚àí3
1
12
0
0
with Œ± = (1, 2, 7), k = 4, r = 2.
The algorithms goes on with the following sequence of tableaux:
1
9
1
0
‚àí2
‚àí9
0
0
0
1
1
3
1
‚àí1
3
‚àí2
0
0
0
‚àí3
1
0
0
‚àí6
1
2
0
3
‚àí1
0
0
6
0
0
Œ± = (1, 4, 7), k = 3, r = 1
1
9
1
0
‚àí2
‚àí9
0
0
‚àí1
3
‚àí2
0
1
1
3
1
0
0
‚àí1
‚àí12
0
0
2
3
1
2
1
12
0
0
‚àí2
‚àí3
0
0
Œ± = (3, 4, 7), k = 6, r = 2
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
118
The simplex algorithm
‚àí2
‚àí9
1
9
1
0
0
0
‚àí1
3
‚àí2
0
1
1
3
1
0
0
0
‚àí6
0
‚àí3
1
0
1
2
0
6
0
3
‚àí1
0
0
0
Œ± = (3, 6, 7), k = 5, r = 1
‚àí2
‚àí9
1
9
1
0
0
0
1
3
1
‚àí1
3
‚àí2
0
1
0
0
2
3
‚àí1
‚àí12
0
0
1
2
‚àí2
‚àí3
1
12
0
0
0
0
Œ± = (5, 6, 7)
After six iterations we are back to the starting tableau.
The simplex
algorithm cycles!
We now introduce a rule for the choice of indices k and r that prevents
cycling.
Bland‚Äôs rule: Choose k in step 4 of the simplex algorithm so that
k = min{j | zj < 0}
and r in step 6 so that
Œ±r = min{j ‚ààI+ | xj/vj = œÑ}.
Example 13.5.2. Consider again the minimization problem in the previous
example and now use the simplex algorithm with Bland‚Äôs rule. This results
in the following sequence of tableaux:
‚àí2
‚àí9
1
9
1
0
0
0
1
3
1
‚àí1
3
‚àí2
0
1
0
0
2
3
‚àí1
‚àí12
0
0
1
2
‚àí2
‚àí3
1
12
0
0
0
0
Œ± = (5, 6, 7), k = 1, r = 2
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
119
The simplex algorithm
119
0
‚àí3
‚àí1
‚àí3
1
6
0
0
1
3
‚àí1
‚àí6
0
3
0
0
0
‚àí3
1
0
0
‚àí6
1
2
0
3
‚àí1
0
0
6
0
0
Œ± = (5, 1, 7), k = 3, r = 3
0
‚àí6
0
‚àí3
1
0
0
2
1
0
0
‚àí6
0
‚àí3
1
2
0
‚àí3
1
0
0
‚àí6
1
2
0
0
0
0
0
12
1
2
Œ± = (5, 1, 3)
The optimality criterion is met with x = (2, 0, 2, 0, 2, 0, 0) as optimal
solution and ‚àí2 as optimal value.
Theorem 13.5.1. The simplex algorithm always stops if Bland‚Äôs rule is used.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
120
The simplex algorithm
Proof. We prove the theorem by contradiction. So suppose that the simplex
algorithm cycles when applied to some given LP problem, and let x be the
common basic solution during the iterations of the cycle.
Let C denote the set of indices k of the varibles xk that change from being
basic to being free during the iterations of the cycle. Since these variables
have to return as basic variables during the cycle, C is of course also equal
to the set of indices of the variables xk that change from being free to being
basic during the cycle. Moreover, xk = 0 for all k ‚ààC.
Let
q = max{j | j ‚ààC},
and let Œ± be the basic index set which is in use during the iteration in the
cycle when the variabel xq changes from being basic to being free, and let xk
be the free variable that replaces xq. The index q is in other words replaced
by k in the basic index set that follows after Œ±. The corresponding search
vector v and reduced cost vector z satisfy the inequalities
zk < 0
and
vq > 0,
and
zj ‚â•0
for j < k.
since the index k is chosen according to Bland‚Äôs rule. Since k ‚ààC, we also
have k < q, because of the deÔ¨Ånition of q.
Let us now consider the basic index set Œ±‚Ä≤ that belongs to an iteration
when xq returns as a basic variables after having been free. Because of Bland‚Äôs
rule for the choice of incoming index, in this case q, the corresponding reduced
cost vector z‚Ä≤ has to satisfy the following inequalities:
(13.11)
z‚Ä≤
j ‚â•0
for j < q
and
z‚Ä≤
q < 0.
Especially, thus z‚Ä≤
k ‚â•0.
Since Av = 0, vk = ‚àí1 and vj = 0 for j /‚ààŒ± ‚à™{k}, and zj = 0 for j ‚ààŒ±,
it follows from Lemma 13.4.1 that

j‚ààŒ±
z‚Ä≤
jvj ‚àíz‚Ä≤
k = ‚ü®z‚Ä≤, v‚ü©= ‚ü®c, v‚ü©= ‚ü®z, v‚ü©=

j‚ààŒ±
zjvj + zkvk = ‚àízk > 0,
and hence

j‚ààŒ±
z‚Ä≤
jvj > z‚Ä≤
k ‚â•0.
There is therefore an index j0 ‚ààŒ± such that z‚Ä≤
j0vj0 > 0. Hence z‚Ä≤
j0 Ã∏= 0,
which means that j0 can not belong to the index set Œ±‚Ä≤. The variable xj0 is
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
121
The simplex algorithm
in other words basic during one iteration of the cycle and free during another
iteration. This means that j0 is an index in the set C, and hence j0 ‚â§q, by
the deÔ¨Ånition of q. The case j0 = q is impossible since vq > 0 and z‚Ä≤
q < 0.
Thus j0 < q, and it now follows from (13.11) that z‚Ä≤
j0 > 0. This implies in
turn that vj0 > 0, because the product z‚Ä≤
j0vj0 is positive. So j0 belongs to the
set I+ = {j ‚ààŒ± | vj > 0}, and since xj0/vj0 = 0 = œÑ, it follows that
min{j ‚ààI+ | xj/vj = œÑ} ‚â§j0 < q.
The choice of q thus contradicts Bland‚Äôs rule for how to choose index to leave
the basic index set Œ±, and this contradiction proves the theorem.
Remark. It is not necessary to use Bland‚Äôs rule all the time in order to prevent
cycling; it suÔ¨Éces to use it in iterations with œÑ = 0.
13.6
Phase 1 of the simplex algorithm
The simplex algorithm assumes that there is a feasible basic index set to start
from. For some problems we will automatically get one when the problem is
written in standard form. This is the case for problems of the type
min ‚ü®c, x‚ü©
s.t.
Ax ‚â§b, x ‚â•0
where A is an m √ó n-matrix and the right-hand side vector b is nonnegative.
By introducing m slack variables sn+1, sn+2, . . . , sn+m and deÔ¨Åning
s = (sn+1, sn+2, . . . , sn+m),
we obtain the standard problem
min ‚ü®c, x‚ü©
s.t.
Ax + Es = b, x, s ‚â•0,
and it is now obvious how to start; the slack variables will do as basic vari-
ables, i.e. Œ± = (n + 1, n + 2, . . . , n + m) is a feasible basic index set with
x = 0, s = b as the corresponding basic solution.
In other cases, it is not at all obvious how to Ô¨Ånd a feasible basic index set
to start from, but one can always generate such a set by using the simplex
algorithm on a suitable artiÔ¨Åcial problem.
Consider an arbitrary standard LP problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0,
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
122
The simplex algorithm
122
where A is an m √ó n-matrix. We can assume without restriction that b ‚â•0,
for if any bj is negative, we just multiply the corresponding equation by ‚àí1.
We begin by choosing an m √ó k-matris B so that the matrix
A‚Ä≤ =

A
B

gets rank equal to m and the system
A‚Ä≤
x
y

= Ax + By = b
gets an obvious feasible basic index set Œ±0. The new y-variables are called
artiÔ¨Åcial variables, and we number them so that y = (yn+1, yn+2, . . . , yn+k).
A trivial way to achieve this is to choose B equal to the unit matrix E
of order m, for Œ±0 = (n + 1, n + 2, . . . , n + m) is then a feasible basic index
set with (x, y) = (0, b) as the corresponding feasible basic solution. Often,
however, A already contains a number of unit matrix columns, and then it
is suÔ¨Écient to add the missing unit matrix columns to A.
Now let
1 =

1
1
. . .
1
T
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
123
The simplex algorithm
be the k √ó 1-matrix consisting of k ones, and consider the following artiÔ¨Åcial
LP problems:
min ‚ü®1, y‚ü©= yn+1 + ¬∑ ¬∑ ¬∑ + yn+k
s.t.
Ax + By = b, x, y ‚â•0
.
The optimal value is obviously ‚â•0, and the value is equal to zero if and only
if there is a feasible solution of the form (x, 0), i.e. if and only if there is a
nonnegative solution to the system Ax = b.
Therefore, we solve the artiÔ¨Åcial problem using the simplex algorithm with
Œ±0 as the Ô¨Årst feasible basic index set. Since the objective function is bounded
below, the algorithm stops after a Ô¨Ånite number of iterations (perhaps we
need to use Bland‚Äôs supplementary rule) in a feasible basic index set Œ±, where
the optimality criterion is satisÔ¨Åed. Let (x, y) denote the corresponding basic
solution.
There are now two possibilities.
Case 1. The artiÔ¨Åcial problem‚Äôs optimal value is greater than zero.
In this case, the original problem has no feasible solutions.
Case 2. The artiÔ¨Åcial problem‚Äôs value is equal to zero.
In this case, y = 0 and Ax = b.
If Œ± ‚äÜ{1, 2, . . . , n}, then Œ± is also a feasible basic index set of the matrix
A, and we can start the simplex algorithm on our original problem from Œ±
and the corresponding feasible basic solution x.
If Œ± Ã∏‚äÜ{1, 2, . . . , n}, we set
Œ±‚Ä≤ = Œ± ‚à©{1, 2, . . . , n}.
The matrix columns {A‚àók | k ‚ààŒ±‚Ä≤} are now linearly independent, and we
can construct an index set Œ≤ ‚äáŒ±‚Ä≤, which is maximal with respect to the
property that the columns {A‚àók | k ‚ààŒ≤} are linearly independent.
If rank A = m, then Œ≤ will consist of m elements, and Œ≤ is then a basic
index set of the matrix A. Since xj = 0 for all j /‚ààŒ±‚Ä≤, and thus especially
for all j /‚ààŒ≤, it follows that x is the basic solution of the system Ax = b
that corresponds to the basic index set Œ≤. Hence, Œ≤ is a feasible basic index
set for our original problem. We can also note that x is a degenerate basic
solution.
If rank A < m, then Œ≤ will consist of just p = rank A elements, but we
can now delete m ‚àíp equations from the system Ax = b without changing
the set of solutions. This results in a new equivalent LP problem with a
coeÔ¨Écient matrix of rank p, and Œ≤ is a feasible basic index set with x as the
corresponding basic solution in this problem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
124
The simplex algorithm
To solve a typical LP problem, one thus normally needs to use the simplex
algorithm twice. In Phase 1, we use the simplex algorithm to generate a
feasible basic index set Œ± for the original LP problem by solving an artiÔ¨Åcial
LP problem, and in phase 2, the simplex algorithm is used to solve the
original problem starting from the basic index set Œ±.
Example 13.6.1. We illustrate the technique on the simple problem
min x1 + 2x2 + x3 ‚àí2x4
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 + x3 ‚àíx4 = 2
2x1 + x2 ‚àíx3 + 2x4 = 3
x1, x2, x3, x4 ‚â•0.
Phase 1 consists in solving the artiÔ¨Åcial problem
min y5 + y6
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 + x3 ‚àíx4 + y5
= 2
2x1 + x2 ‚àíx3 + 2x4
+ y6 = 3
x1, x2, x3, x4, y5, y6 ‚â•0.
The computations are shown in tabular form, and the Ô¨Årst simplex tableau
is the following one.
1
1
1
‚àí1
1
0
2
2
1
‚àí1
2
0
1
3
0
0
0
0
1
1
0
We begin by eliminating the basic variables from the objective function
and then obtain the following sequence of tableaux:
1
1
1
‚àí1
1
0
2
2
1
‚àí1
2
0
1
3
‚àí3
‚àí2
0
‚àí1
0
0
‚àí5
Œ± = (5, 6),
k = 1,
r = 2
0
1
2
3
2
‚àí2
1
‚àí1
2
1
2
1
1
2
‚àí1
2
1
0
1
2
3
2
0
‚àí1
2
‚àí3
2
2
0
3
2
‚àí1
2
Œ± = (5, 1),
k = 3,
r = 1
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
125
The simplex algorithm
125
0
1
3
1
‚àí4
3
2
3
‚àí1
3
1
3
1
2
3
0
1
3
1
3
1
3
5
3
0
0
0
0
1
1
0
Œ± = (3, 1)
The above Ô¨Ånal tableau for the artiÔ¨Åcial problem shows that Œ± = (3, 1)
is a feasible basic index set for the original problem with x = ( 5
3, 0, 1
3, 0) as
corresponding basic solution. We can therefore proceed to phase 2 with the
following tableau as our Ô¨Årst tableau.
0
1
3
1
‚àí4
3
1
3
1
2
3
0
1
3
5
3
1
2
1
‚àí2
0
By eliminating the basic variables from the objective function, we obtain
the following tableau:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
126
The simplex algorithm
0
1
3
1
‚àí4
3
1
3
1
2
3
0
1
3
5
3
0
1
0
‚àí1
‚àí2
Œ± = (3, 1),
k = 4,
r = 2
One iteration is enough to obtain a tableau satisfying the optimality criterion.
4
3
1
0
7
3
2
0
1
5
3
3
0
0
3
Œ± = (3, 4)
The optimal value is thus equal to ‚àí3, and x = (0, 0, 7, 5) is the optimal
solution.
Since the volume of work grows with the number of artiÔ¨Åcial variables,
one should not introduce more artiÔ¨Åcial variables than necessary. The mini-
mization problem
min ‚ü®c, x‚ü©
s.t.
Ax ‚â§b, x ‚â•0
requires no more than one artiÔ¨Åcial variable. By introducing slack variables
s = (sn+1, sn+2, . . . , sn+m), we Ô¨Årst obtain an equivalent standard problem
min ‚ü®c, x‚ü©
s.t.
Ax + Es = b, x, s ‚â•0
.
If b ‚â•0, this problem can be solved, as we have already noted, without
artiÔ¨Åcial variables. Let otherwise i0 be the index of the most negative coor-
dinate of the right-hand side b, and subtract equation no. i0 in the system
Ax + Es = b from all other equations with negative right-hand side, and
change Ô¨Ånally the sign of equation no. i0.
The result is a system of equations of the form A‚Ä≤x + E‚Ä≤s = b‚Ä≤, which is
equivalent to the system Ax+Es = b and where b‚Ä≤ ‚â•0 and all the columns of
the matrix E‚Ä≤, except column no. i0, are equal to the corresponding columns
of the unit matrix E. Phase 1 of the simplex algorithm applied to the problem
min ‚ü®c, x‚ü©
s.t.
A‚Ä≤x + E‚Ä≤s = b‚Ä≤, x, s ‚â•0
therefore requires only one artiÔ¨Åcial variable.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
127
The simplex algorithm
Existence of optimal solutions and the duality theorem
The simplex algorithm is of course Ô¨Årst and foremost an eÔ¨Écient algorithm
for solving concrete LP problems, but we can also use it to provide alternative
proofs of important theoretical results. These are corollaries to the following
theorem.
Theorem 13.6.1. Each standard LP problem with feasible points has a fea-
sible basic index set where one of the two stopping criteria in the simplex
algorithm is satisÔ¨Åed.
Proof. Bland‚Äôs rule ensures that phase 1 of the simplex algorithm stops with
a feasible basic index set from where to start phase 2, and Bland‚Äôs rule also
ensures that this phase stops in a feasible basic index set, where one of the
two stopping criteria is satisÔ¨Åed.
As Ô¨Årst corollary we obtain a new proof that every LP problem with Ô¨Ånite
value has optimal solutions (Theorem 12.1.1).
Corollary 13.6.2. Each linear minimization problem with feasible solutions
and downwards bounded objective function has an optimal solution.
Proof. Since each LP problem can be replaced by an equivalent LP problem
in standard form, it is suÔ¨Écient to consider such problems. The only way
for the simplex algorithm to stop, when the objective function is bounded
below, is to stop at a basic solution which satisÔ¨Åes the optimality criterion.
So it follows at once from the above theorem that there exists an optimal
solution if the objective function is bounded below and the set of feasible
solutions is nonempty.
We can also give an algorithmic proof of the Duality theorem.
Corollary 13.6.3 (Duality theorem). If the linear optimization problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
has feasible solutions, then it has the same optimal value as the dual maxi-
mization problem
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c.
Proof. Let Œ± be the feasible basic index set where the simplex algorithm
stops. If the optimality criterion is satisÔ¨Åed at Œ±, then it follows from Theo-
rem 13.4.2 that the minimization problem and the dual maximization prob-
lem have the same Ô¨Ånite optimal value. If instead the algorithm stops because
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
128
The simplex algorithm
the objective function is unbounded below, then the dual problem has no fea-
sible points according to Theorem 13.4.2, and the value of both problems is
equal to ‚àí‚àû, by deÔ¨Ånition.
By writing general minimization problems in standard form, one can also
deduce the general form of the duality theorem from the above special case.
13.7
Sensitivity analysis
In Section 12.1, we studied how the optimal value and the optimal solution
depend on the coeÔ¨Écients of the objective function. In this section we shall
study the same issue in connection with the simplex algorithm and also study
how the solution to the LP problem
(P)
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
depends on the right-hand side b. In real LP problems, the coeÔ¨Écients of the
objective function and the constraints are often not exactly known, some of
them might even be crude estimates, and it is then of course important to
know how sensitive the optimal solution is to errors in input data. And even
if the input data are accurate, it is of course interesting to know how the
optimum solution is aÔ¨Äected by changes in one or more of the coeÔ¨Écients.
Let Œ± be a basic index set of the matrix A, and let x(b) denote the
corresponding basic solution to the system Ax = b, i.e.
x(b)Œ± = A‚àí1
‚àóŒ±b
and
x(b)j = 0 for all j /‚ààŒ±.
Suppose that the LP problem (P) has an optimal solution for certain
given values of b and c, and that this solution has been obtained because the
simplex algorithm stopped at the basic index set Œ±. For that to be the case,
the basic solution x(b) has to be feasible, i.e.
(i)
A‚àí1
‚àóŒ±b ‚â•0,
and the optimality criterion z ‚â•0 in the simplex algorithm has to be satisÔ¨Åed.
Since
z = c ‚àíATy
and
y = (A‚àí1
‚àóŒ±)TcŒ±,
we have z = c ‚àí(A‚àí1
‚àóŒ±A)TcŒ±, which means that the optimality criterion can
be written as
(ii)
z(c) = c ‚àí(A‚àí1
‚àóŒ±A)TcŒ± ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
129
The simplex algorithm
129
Conversely, x(b) is an optimal solution to the LP problem (P) for all b
and c that satisfy the conditions (i) and (ii), because the optimality criterion
in the simplex algorithm is then satisÔ¨Åed.
Condition (i) is a system of homogeneous linear inequalities in the vari-
ables b1, b2, . . . , bm, and it deÔ¨Ånes a polyhedral cone BŒ± in Rm, while (ii)
is a system of homogeneous linear inequalities in the variables c1, c2, . . . , cn
and deÔ¨Ånes a polyhedral cone CŒ± in Rn. In summary, we have the following
result:
x(b) is an optimal solution to the LP problem (P) for all b ‚ààBŒ± and all
c ‚ààCŒ±.
Now suppose that we have solved the problem (P) for given values of b
and c with x = x(b) as optimal solution and Œª as optimal value. Condition
(ii) determines how much we are allowed to change the coeÔ¨Écients of the
objective function without changing the optimal solution; x is still an optimal
solution to the perturbed problem
(P‚Ä≤)
min ‚ü®c + ‚àÜc, x‚ü©
s.t.
Ax = b, x ‚â•0
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
130
The simplex algorithm
if z(c + ‚àÜc) = z(c) + z(‚àÜc) ‚â•0, i.e. if
(13.12)
‚àÜc ‚àí(A‚àí1
‚àóŒ±A)T(‚àÜc)Œ± ‚â•‚àíz(c).
The optimal value is of course changed to Œª + ‚ü®‚àÜc, x‚ü©.
Inequality (13.12) deÔ¨Ånes a polyhedron in the variables ‚àÜc1, ‚àÜc2, . . . ,
‚àÜcn.
If for instance ‚àÜcj = 0 for all j except j = k, i.e. if only the ck-
coeÔ¨Écient of the objective function is allowed to change, then inequality
(13.12) determines a (possibly unbounded) closed interval [‚àídk, ek] around 0
for ‚àÜck.
If instead we change the right-hand side of the constraints replacing the
vector b by b+‚àÜb, then x(b+‚àÜb) becomes an optimal solution to the problem
min ‚ü®c, x‚ü©
s.t.
Ax = b + ‚àÜb, x ‚â•0
as long as the solution is feasible, i.e. as long as A‚àí1
‚àóŒ±(b + ‚àÜb) ‚â•0. After
simpliÔ¨Åcation, this results in the condition
A‚àí1
‚àóŒ±(‚àÜb) ‚â•‚àíx(b)Œ±,
which is a system of linear inequalities that determines how to choose ‚àÜb. If
‚àÜbi = 0 for all indices except i = k, then the set of solutions for ‚àÜbk is an
interval around 0 of the form [‚àídk, ek].
The printouts of softwares for the simplex algorithm generally contain
information on these intervals.
Example 13.7.1. A person is studying the diet problem
min ‚ü®c, x‚ü©
s.t.
Ax ‚â•b, x ‚â•0
in a speciÔ¨Åc case with six foods and four nutrient requirements. The fol-
lowing computer printout is obtained when cT = (1, 2, 3, 4, 1, 6) and bT =
(10, 15, 20, 18).
Optimal value:
8.52
Optimal solution:
Food 1:
5.73
Food 2:
0.00
Food 3:
0.93
Food 4:
0.00
Food 5:
0.00
Food 6:
0.00
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
131
The simplex algorithm
Sensitivity report
Variable
Value
Objective-
Allowable
Allowable
coeÔ¨Ä.
decrease
increase
Food 1:
5.73
1.00
0.14
0.33
Food 2:
0.00
2.00
1.07
‚àû
Food 3:
0.93
3.00
2.00
0.50
Food 4:
0.00
4.00
3.27
‚àû
Food 5:
0.00
1.00
0.40
‚àû
Food 6:
0.00
6.00
5.40
‚àû
Constraint
Final
Shadow
Bounds
Allowable
Allowable
value
price
r.h. side
decrease
increase
Nutrient 1:
19.07
0.00
10.00
‚àû
9.07
Nutrient 2:
31.47
0.00
15.00
‚àû
16.47
Nutrient 3:
20.00
0.07
20.00
8.00
7.00
Nutrient 4:
18.00
0.40
18.00
4.67
28.67
The sensitivity report shows that the optimal solution remains unchanged
as long as the price of food 1 stays in the interval [5.73 ‚àí0.14, 5.73 + 0.33],
ceteris paribus. A price change of z units in this range changes the optimal
value by 5.73 z units.
A price reduction of food 4 with a maximum of 3.27, or an unlimited
price increase of the same food, ceteris paribus, does not aÔ¨Äect the optimal
solution, nor the optimal value.
The set of price changes that leaves the optimal solution unchanged is
a convex set, since it is a polyhedron according to inequality (13.12). The
optimal solution of our example is therefore unchanged if for example the
prices of foods 1, 2 and 3 are increased by 0.20, 1.20 and 0.10, respectively,
because ‚àÜc = (0.20, 1.20, 0.10, 0, 0, 0) is a convex combination of allowable
increases, since
0.20
0.33 + 1.20
‚àû+ 0.10
0.50 ‚â§1.
The sensitivity report also shows how the optimal solution is aÔ¨Äected
by certain changes in the right-hand side b. The optimal solution remains
unchanged, for example, if the need for nutrient 1 would increase from 10
to 15, since the constraint is not binding and the increase 5 is less than the
permitted increase 9.07.
The sensitivity report also tells us that the new optimal solution will still
be derived from the same basic index set as above, if b4 is increased by say 20
units from 18 to 38, an increase that is within the scope of the permissible.
So in this case, the optimal diet will also only consist of foods 1 and 3, but
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
132
The simplex algorithm
132
the optimal value will increase by 20 ¬∑ 0.40 to 16.52 since the shadow price of
nutrient 4 is equal to 0.40.
13.8
The dual simplex algorithm
The simplex algorithm, applied to a problem
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
with a bounded optimal value, starts from a given feasible basic index set
Œ±0 and then generates a Ô¨Ånite sequence (Œ±k, xk, yk)p
k=0 of basic index sets Œ±k,
corresponding basic solutions xk and vectors yk with the following properties:
(i) The basic solutions xk are extreme points of the polyhedron
X = {x ‚ààRn | Ax = b, x ‚â•0}
of feasible solutions.
(ii) The line segments [xk, xk+1] are edges of the polyhedron X.
(iii) The objective function values (‚ü®c, xk‚ü©)p
k=0 form a decreasing sequence.
(iv) ‚ü®b, yk‚ü©= ‚ü®c, xk‚ü©for all k.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
133
The simplex algorithm
(v) The algorithm stops after p iterations when the optimality criterion is
met, and yp is then an extreme point of the polyhedron
Y = {y ‚ààRm | ATy ‚â§c}.
(vi) xp is an optimal solution to the primal problem, and yp is an optimal
solution to the dual problem
max ‚ü®b, y‚ü©
s.t.
ATy ‚â§c.
(vii) The vectors yk do not, however, belong to Y for 0 ‚â§k ‚â§p ‚àí1.
The optimal solution xp is obtained by moving along edges of the poly-
hedron X until an extreme point has been reached that also corresponds to
an extreme point of the polyhedron Y . Instead, we could move along edges
of the polyhedron Y , and this observation leads to the following method for
solving the minimization problem.
The dual simplex algorithm
Given a basic index set Œ± such that z = c ‚àíATy ‚â•0, where y = (A‚àí1
‚àóŒ±)TcŒ±.
Repeat steps 1‚Äì4 until a stop occurs.
1. Compute the basic solution x corresponding to Œ±.
2. Stopping criterion: quit if x ‚â•0.
Optimal solution: x. Optimal dual solution: y.
Also quit if any of the constraint equations has the form a‚Ä≤
i1x1+a‚Ä≤
i2x2+
¬∑ ¬∑ ¬∑ + a‚Ä≤
inxn = b‚Ä≤
i with b‚Ä≤
i > 0 and a‚Ä≤
ij ‚â§0 for all j, because then there
are no feasible solutions to the primal problem.
3. Generate a new basic index set Œ±‚Ä≤ by replacing one of the indices of Œ±
in such a way that the new reduced cost vector z‚Ä≤ remains nonnegative
and ‚ü®b, y‚Ä≤‚ü©‚â•‚ü®b, y‚ü©, where y‚Ä≤ = (A‚àí1
‚àóŒ±‚Ä≤)TcŒ±‚Ä≤.
4. Update: Œ±:= Œ±‚Ä≤, y:= y‚Ä≤.
We refrain from specifying the necessary pivoting rules. Instead, we con-
sider a simple example.
Example 13.8.1. We shall solve the minimization problem
min x1 + 2x2 + 3x3
s.t.
Ô£±
Ô£≤
Ô£≥
2x1
+ x3 ‚â•
9
x1 + 2x2
‚â•12
x2 + 2x3 ‚â•15, x ‚â•0
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
134
The simplex algorithm
by using the dual simplex algorithm, and we begin by reformulating the
problem in standard form as follows:
min x1 + 2x2 + 3x3
s.t.
Ô£±
Ô£≤
Ô£≥
2x1
+ x3 ‚àíx4
=
9
x1 + 2x2
‚àíx5
= 12
x2 + 2x3
‚àíx6 = 15, x ‚â•0.
The corresponding simplex tableau now looks like this:
2
0
1
‚àí1
0
0
9
1
2
0
0
‚àí1
0
12
0
1
2
0
0
‚àí1
15
1
2
3
0
0
0
0
For comparison, we also state the corresponding dual maximization prob-
lem:
max 9y1 + 12y2 + 15y3
s.t.
Ô£±
Ô£≤
Ô£≥
2y1 + y2
‚â§1
2y2 + y3 ‚â§2
y1
+ 2y3 ‚â§3, y ‚â•0.
We can start the dual simplex algorithm from the basic index set Œ± =
(4, 5, 6), and as usual, we have underlined the basic columns.
The cor-
responding basic solution x is not feasible since the coordinates of xŒ± =
(‚àí9, ‚àí12, ‚àí15) are negative. The bottom row [ 1 2 3 0 0 0 ] of the tableau
is the reduced cost vector zT = cT ‚àíyTA. The row vector yT = c T
Œ±A‚àí1
‚àóŒ± =

0
0
0

can also be read in the bottom row; it is found below the matrix
‚àíE, and y belongs to the polyhedron Y of feasible solutions to the dual
problem, since zT ‚â•0.
We will now gradually replace one element at a time in the basic index
set. As pivot row r, we choose the row that corresponds to the most negative
coordinate of xŒ±, and in the Ô¨Årst iteration, this is the third row in the above
simplex tableau. To keep the reduced cost vector nonnegative, we must select
as pivot column a column k, where the matrix element ark is positive and
the ratio zk/ark is as small as possible. In the above tableau, this is the third
column, so we pivot around the element at location (3, 3). This leads to the
following tableau:
2
‚àí1
2
0
‚àí1
0
1
2
3
2
1
2
0
0
‚àí1
0
12
0
1
2
1
0
0
‚àí1
2
15
2
1
1
2
0
0
0
3
2
‚àí45
2
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
135
The simplex algorithm
135
In this new tableau, Œ± = (4, 5, 3), xŒ± = (‚àí3
2, ‚àí12, 15
2 ) and y = (0, 0, 3
2).
The most negative element of xŒ± is to be found in the second row, and the
least ratio zk/a‚Ä≤
2k with a positive denominator a‚Ä≤
2k is obtained for k = 2.
Pivoting around the element at location (2, 2) leads to the following simplex
tableau:
9
4
0
0
‚àí1
‚àí1
4
1
2
9
2
1
2
1
0
0
‚àí1
2
0
6
‚àí1
4
0
1
0
1
4
‚àí1
2
9
2
3
4
0
0
0
1
4
3
2
‚àí51
2
Now, Œ± = (4, 2, 3), xŒ± = (‚àí9
2, 6, 9
2) and y = (0, 1
4, 3
2). This time, we should
select the element in the Ô¨Årst row and the Ô¨Årst column as pivot element, which
leads to the next tableau.
1
0
0
‚àí4
9
‚àí1
9
2
9
2
0
1
0
2
9
‚àí4
9
‚àí1
9
5
0
0
1
‚àí1
9
2
9
‚àí4
9
5
0
0
0
1
3
1
3
4
3
‚àí27
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
136
The simplex algorithm
Here, Œ± = (1, 2, 3), xŒ± = (2, 5, 5) and y = ( 1
3, 1
3, 4
3), and the optimality
criterion is met since xŒ± ‚â•0. The optimal value is 27 and (2, 5, 5, 0, 0, 0)
is the optimal point. The dual maximization problem attains its maximum
at (1
3, 1
3, 4
3). The optimal solution to our original minimization problem is of
course x = (2, 5, 5).
13.9
Complexity
How many iterations are needed to solve an LP problem using the simplex
algorithm? The answer will depend, of course, on the size of the problem.
Experience shows that the number of iterations largely grows linearly with
the number of rows m and sublinearly with the number of columns n for
realistic problems, and in most real problems, n is a small multiple of m,
usually not more than 10m. The number of iterations is therefore usually
somewhere between m and 4m, which means that the simplex algorithm
generally performs very well.
The worst case behavior of the algorithm is bad, however (for all known
pivoting rules). Klee and Minty has constructed an example where the num-
ber of iterations grows exponentially with the size of the problem.
Example 13.9.1 (Klee and Minty, 1972). Consider the following LP problem
in n variables and with n inequality constraints:
max 2n‚àí1x1 + 2n‚àí2x2 + ¬∑ ¬∑ ¬∑ + 2xn‚àí1 + xn
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1
‚â§
5
4x1 +
x2
‚â§25
8x1 +
4x2 + x3
‚â§125
...
...
2nx1 + 2n‚àí1x2 + . . . + 4xn‚àí1 + xn ‚â§5n
The polyhedron of feasible solutions has in this case 2n extreme points.
Suppose that we apply the simplex algorithm to the equivalent standard
problem, in each iteration choosing as pivot column the column with the most
negative value of the reduced cost. If we start from the feasible basic solution
that corresponds to x = 0, then we have to go through all the 2n feasible
basic solutions before we Ô¨Ånally reach the optimal solution (0, 0, . . . , 5n). The
number of iterations is therefore equal to 2n and thus increases exponentially
with n.
An algorithm for solving a problem in n variables is called strictly polyno-
mial if there exists a positive integer k such that the number of elementary
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
137
The simplex algorithm
arithmetic operations in the algorithm grows with n as at most O(nk). In
many algorithms, the number of operations also depends on the size of the
input data. An algorithm is called polynomial if the number of arithmetic
operations is growing as a polynomial in L, where L is the number of bi-
nary bits needed to represent all input (i.e. the matrices A, b and c in linear
programming).
Gaussian elimination is a strictly polynomial algorithm, because a system
of linear equations with n equations and n unknowns is solved with O(n3)
arithmetic operations.
Klee‚ÄìMinty‚Äôs example and other similar examples demonstrate that the
simplex algorithm is not strictly polynomial. But all experience shows that
the simplex algorithm works very well, even if the worst case behavior is bad.
This is also supported by probabilistic analyzes, made by Borgwardt (1987),
Smale (1983), Adler and Megiddo (1985), among others. Such an analysis
shows, for example, that (a variant of) the simplex algorithm, given a certain
special probability distribution of the input data, on average converges after
O(m2) iterations, where m is the number of constraints.
The existence of a polynomial algorithm that solves LP problems (with
rational coeÔ¨Écients as input data) was Ô¨Årst demonstrated in 1979 by Leonid
Khachiyan.
His so-called ellipsoid algorithm reduces LP problems to the
problem of Ô¨Ånding a solution to a system Ax > b of strict inequalities with a
bounded set of solutions, and the algorithm generates a sequence of shrinking
ellipsoids, all guaranteed to contain all the solutions to the system. If the
center of an ellipsoid satisÔ¨Åes all inequalities of the system, then a solution
has been found, of course. Otherwise, the process stops when a generated
ellipsoid has too small volume to contain all solutions, if there are any, with
the conclusion that there are no solutions.
LP problems in standard form with n variables and input size L are solved
by the ellipsoid method in O(n4L) arithmetic operations. However, in spite
of this nice theoretical result, it was soon clear that the ellipsoid method
could not compete with the simplex algorithm on real problems of moderate
size due to slow convergence. (The reason for this is, of course, that the
implicit constant in the O-estimate is very large.)
A new polynomial algorithm was discovered in 1984 by Narendra Kar-
markar. His algorithm generates a sequence of points, which lie in the interior
of the set of feasible points and converge towards an optimal point. The algo-
rithm uses repeated centering of the generated points by a projective scaling
transformation. The theoretical complexity bound of the original version of
the algorithm is also O(n4L).
Karmarkar‚Äôs algorithm turned out to be competitive with the simplex
algorithm on practical problems, and his discovery was the starting point for
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
138
The simplex algorithm
138
an intensive development of alternative interior point methods for LP prob-
lems and more general convex problems. We will study such an algorithm in
Chapter 18.
It is still an open problem whether there exists any strictly polynomial
algorithm for solving LP problems.
Exercises
13.1 Write the following problems in standard form.
a) min 2x1 ‚àí2x2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 ‚àíx3 ‚â•3
x1 + x2 ‚àíx3 ‚â§2
x1, x2, x3 ‚â•0
b) min x1 + 2x2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 ‚â•
1
x2 ‚â•‚àí2
x1
‚â•
0.
13.2 Find all nonnegative basic solutions to the following systems of equations.
a)
5x1 + 3x2 + x3 = 40
x1 + x2 + x3 = 10
b)
 x1 ‚àí2x2 ‚àíx3 + x4 = 3
2x1 + 5x2 ‚àí3x3 + 2x4 = 6.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
139
The simplex algorithm
13.3 State the dual problem to
min
x1 + x2 + 4x3
s.t.
x1
‚àíx3 = 1
x1 + 2x2 + 7x3 = 7, x ‚â•0
and prove that (1, 3, 0) is an optimal solution and that (1
2, 1
2) is an optimal
solution to the dual problem.
13.4 Solve the following LP problems using the simplex algorithm.
a) min
‚àíx4
s.t.
Ô£±
Ô£≤
Ô£≥
x1
+ x4 = 1
x2
+ 2x4 = 2
x3 ‚àíx4 = 3, x ‚â•0
b) max 2x1 ‚àíx2 + x3 ‚àí3x4 + x5
s.t.
Ô£±
Ô£≤
Ô£≥
x1
+ 2x4 ‚àíx5 = 15
x2
+ x4 + x5 = 12
x3 ‚àí2x4 + x5 =
9, x ‚â•0
c) max 15x1 + 12x2 + 14x3
s.t.
Ô£±
Ô£≤
Ô£≥
3x1 + 2x2 + 5x3 ‚â§6
x1 + 3x2 + 3x3 ‚â§3
5x3 ‚â§4, x ‚â•0
d) max 2x1 + x2 + 3x3 + x4 + 2x5
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + 2x2 + x3
+ x5 ‚â§10
x2 + x3 + x4 + x5 ‚â§
8
x1
+ x3 + x4
‚â§
5, x ‚â•0
e) min
x1 ‚àí2x2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 ‚àí2x3 ‚â§3
x1 ‚àíx2 + x3 ‚â§2
‚àíx1 ‚àíx2 + x3 ‚â§0, x ‚â•0
f) min
x1 ‚àíx2 + 2x3 ‚àí3x4
s.t.
2x1 + 3x2 + x3
= 2
x1 + 3x2 + x3 + 5x4 = 4, x ‚â•0.
13.5 Carry out in detail all the steps of the simplex algorithm for the problem
min
‚àíx2 + x4
s.t.
Ô£±
Ô£≤
Ô£≥
x1
+ x4 + x5 = 1
x2
‚àí2x4 ‚àíx5 = 1
x3 + 2x4 + x5 = 3, x ‚â•0.
Is the optimal solution unique?
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
140
The simplex algorithm
13.6 Use artiÔ¨Åcial variables to solve the LP problem
max x1 + 2x2 + 3x3 ‚àíx4
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + 2x2 + 3x3
= 15
2x1 + x2 + 5x3
= 20
x1 + 2x2 + x3 + x4 = 10, x ‚â•0.
13.7 Use the simplex algorithm to show that the following systems of equalities
and inequalities are consistent.
a)
3x1 + x2 + 2x3 + x4 + x5 = 2
2x1 ‚àíx2 + x3 + x4 + 4x5 = 3, x ‚â•0
b)
Ô£±
Ô£≤
Ô£≥
x1 ‚àíx2 + 2x3 + x4 ‚â•
6
‚àí2x1 + x2 ‚àí2x3 + 7x4 ‚â•
1
x1 ‚àíx2 + x3 ‚àí3x4 ‚â•‚àí1, x ‚â•0.
13.8 Solve the LP problem
min
x1 + 2x2 + 3x3
s.t.
Ô£±
Ô£≤
Ô£≥
2x1
+ x3 ‚â•3
x1 + 2x2
‚â•4
x2 + 2x3 ‚â•5, x ‚â•0.
13.9 Write the following problem in standard form and solve it using the simplex
algorithm.
min 8x1 ‚àíx2
s.t.
Ô£±
Ô£≤
Ô£≥
3x1 + x2 ‚â•
1
x1 ‚àíx2 ‚â§
2
x1 + 2x2 = 20, x ‚â•0.
13.10 Solve the following LP problems using the dual simplex algorithm.
a) min 2x1 + x2 + 3x3
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 + x3 ‚â•2
2x1 ‚àíx2
‚â•1
x2 + 2x3 ‚â•2, x ‚â•0
b) min
x1 + 2x2
s.t.
Ô£±
Ô£≤
Ô£≥
x1
‚àí2x3 ‚â•‚àí5
‚àí2x1 + 3x2 ‚àíx3 ‚â•‚àí4
‚àí2x1 + 5x2 ‚àíx3 ‚â•
2, x ‚â•0
c) min 3x1 + 2x2 + 4x3
s.t.
4x1
+ 2x3 ‚â•5
x1 + 3x2 + 2x3 ‚â•4, x ‚â•0.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
141
The simplex algorithm
141
13.11 Suppose b2 ‚â•b1 ‚â•0.
Show that x =

b1, 1
2(b2 ‚àíb1), 0

is an optimal
solution to the problem
min
x1 + x2 + 4x3
s.t.
x1
‚àíx3 = b1
x1 + 2x2 + 7x3 = b2, x ‚â•0.
13.12 Investigate how the optimal solution to the LP problem
max 2x1 + tx2
s.t.
 x1 + x2 ‚â§5
2x1 + x2 ‚â§7, x ‚â•0
varies as the real parameter t varies.
13.13 A shoe manufacturer produces two shoe models A and B. Due to limited
supply of leather, the manufactured number of pairs xA and xB of the two
models must satisfy the inequalities
xA ‚â§1000,
4xA + 3xB ‚â§4100,
3xA + 5xB ‚â§5000.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
142
The simplex algorithm
The sale price of A and B is 500 SEK and 350 SEK, respectively per pair. It
costs 200 SEK to manufacture a pair of shoes of model B. However, the cost
of producing a pair of shoes of model A is uncertain due to malfunctioning
machines, and it can only be estimated to be between 300 SEK and 410
SEK. Show that the manufacturer may nevertheless decide how many pairs
of shoes he shall manufacture of each model to maximize his proÔ¨Åt.
13.14 Joe wants to meet his daily requirements of vitamins P, Q and R by only
living on milk and bread. His daily requirement of vitamins is 6, 12 and 4
mg, respectively. A liter of milk costs 7.50 SEK and contains 2 mg of P, 2 mg
of Q and nothing of R; a loaf of bread costs 20 SEK and contains 1 mg of P,
4 mg of Q and 4 mg of R. The vitamins are not toxic, so a possible overdose
does not harm. Joe wants to get away as cheaply as possible. Which daily
bill of fare should he choose? Suppose that the price of milk begins to rise.
How high can it be without Joe having to change his bill of fare?
13.15 Using the assumptions of Lemma 13.4.1, show that the reduced cost zk
is equal to the direction derivative of the objective function ‚ü®c, x‚ü©in the
direction ‚àív.
13.16 This exercise outlines an alternative method to prevent cycling in the sim-
plex algorithm. Consider the problem
(P)
min ‚ü®c, x‚ü©
s.t.
Ax = b, x ‚â•0
and let Œ± be an arbitrary feasible basic index set with corresponding basic
solution x. For each positive number œµ, we deÔ¨Åne new vectors x(œµ) ‚ààRn
and b(œµ) ‚ààRm as follows:
x(œµ)Œ± = xŒ± + (œµ, œµ2, . . . , œµm)
and
x(œµ)j = 0 for all j /‚ààŒ±,
b(œµ) = Ax(œµ).
Then x(œµ) is obviously a nonnegative basic solution to the system Ax = b(œµ)
with Œ± as the corresponding basic index set, and the coordinates of the vector
b(œµ) are polynomials of degree m in the variable œµ.
a) Prove that all basic solutions to the system Ax = b(œµ) are non-degenerate
except for Ô¨Ånitely many numbers œµ > 0. Consequently, there is a number
œµ0 > 0 so that all basic solution are non-degenerate if 0 < œµ < œµ0.
b) Prove that if 0 < œµ < œµ0, then all feasible basic index sets for the problem
(Pœµ)
min
‚ü®c, x‚ü©
s.t.
Ax = b(œµ), x ‚â•0
are also feasible basic index sets for the original problem (P).
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
143
The simplex algorithm
143
c) The simplex algorithm applied to problem (Pœµ) will therefore stop at a
feasible basic index set Œ≤, which is also feasible for problem (P), provided
œµ is a suÔ¨Éciently small number. Prove that Œ≤ also satisÔ¨Åes the stopping
condition for problem (P).
Cycling can thus be avoided by the following method: Perturb the right-
hand side by forming x(œµ) and the column matrix b(œµ), where œµ is a small
positive number. Use the simplex algorithm on the perturbed problem. The
algorithm stops at a basic index set Œ≤.
The corresponding unperturbed
problem stops at the same basic index set.
13.17 Suppose that A is a polynomial algorithm for solving systems Cx ‚â•b of
linear inequalities. When applied to a solvable system, the algorithm Ô¨Ånds a
solution x and stops with the output A(C, b) = x. For unsolvable systems,
it stops with the output A(C, b) = ‚àÖ. Use the algorithm A to construct a
polynomial algorithm for solving arbitrary LP problems
min
‚ü®c, x‚ü©
s.t.
Ax ‚â•b, x ‚â•0.
13.18 Perform all the steps of the simplex algorithm for the example of Klee and
Minty when n = 3.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
144
Bibliogracal and historical notices
BibliograÔ¨Åcal and historical
notices
The theory of convex programs has its roots in a paper by Kuhn‚ÄìTucker [1],
which deals with necessary and suÔ¨Écient conditions for optimality in nonlin-
ear problems. Kuhn‚ÄìTucker noted the connection between Lagrange multi-
pliers and saddle points, and they focused on the role of convexity. A related
result with Lagrange multiplier conditions had otherwise been shown before
by John [1] for general diÔ¨Äerentiable constraints, and KKT conditions are
present for the Ô¨Årst time in an unpublished master‚Äôs thesis by Karush [1].
Theorem 11.2.1 can be found in Uzawa [1].
The duality theorem in linear programming was known as a result of game
theory by John von Neumann, but the Ô¨Årst published proof of this theorem
appears in Gale‚ÄìKuhn‚ÄìTucker [1].
The earliest known example of linear programming can be found in works
by Fourier [1] from the 1820s and deals with the problem of determining the
best, with respect to the maximum norm, Ô¨Åt to an overdetermined system of
linear equations. Fourier reduced this problem to minimizing a linear form
over a polyhedron, and he also hinted a method, equivalent to the simplex
algorithm, to compute the minimum.
It was to take until the 1940s before practical problems on a larger scale
began to be formulated as linear programming. The transportation prob-
lem was formulated by Hitchcock [1], who also gave a constructive solution
method, and the diet problem was studied by Stigler [1], who, however, failed
to compute the exact solution. The Russian mathematician and economist
Kantorovich [1] had some years before formulated and solved LP problems
in production planning, but his work was not noticed outside the USSR and
would therefore not inÔ¨Çuence the subsequent development.
The need for mathematical methods for solving military planning prob-
lems had become apparent during the Second World War, and in 1947 a
group of mathematicians led by George Dantzig and Marshall Wood worked
at the U.S. Department of the Air Force with such problems. The group‚Äôs
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
145
Bibliogracal and historical notices
work resulted in the realization of the importance of linear programming,
and the Ô¨Årst version of the simplex algorithm was described by Dantzig [1]
and Wood‚ÄìDantzig [1].
The simplex algorithm is contemporary with the Ô¨Årst computers, and
this suddenly made it possible to treat large problems numerically and con-
tributed to a breakthrough for linear programming. A conference on linear
programming, arranged by Tjalling Koopmans 1949 in Chicago, was also an
important step in the popularization of linear programming. During this
conference, papers on linear programming were presented by economists,
mathematicians, and statisticians. The papers were later published in Koop-
mans [1], and this book became the start for a rapidly growing literature on
linear programming.
Dantzig‚Äôs [2] basic article 1951 treated the non-degenerate case of the sim-
plex algorithm, and the possibility of cycling in the degenerate case caused
initially some concern.
The Ô¨Årst example with cycling was constructed
by HoÔ¨Äman [1], but even before this discovery Charnes [1] had proposed
a method for avoiding cycling.
Other such methods were then given by
Dantzig‚ÄìOrden‚ÄìWolfe [1] and Wolfe [2]. Bland‚Äôs [1] simple pivoting rule is
relatively recent.
It is easy to modify the simplex algorithm so that it is directly applicable
to LP problems with bounded variables, which was Ô¨Årst noted by Charnes‚Äì
Lemke [1] and Dantzig [3].
The dual simplex algorithm was developed by Beale [1] and Lemke [1].
The currently most eÔ¨Écient variants of the simplex algorithm are primal-dual
algorithms.
Convex quadratic programs can be solved by a variant of the simplex
algorithm, formulated by Wolfe [1].
Khachiyan‚Äôs [1] complexity results was based on the ellipsoid algorithm,
which was Ô¨Årst proposed by Shor [1] as a method in general convex optimiza-
tion. See Bland‚ÄìGoldfarb‚ÄìTodd [1] for an overview of the ellipsoid method.
Many variants of Karmarkar‚Äôs [1] algorithm were developed after his pub-
lication in 1984. Algorithms for LP problems with O(n3L) as complexity
bound are described by Gonzaga [1] and Ye [1].
There are numerous textbooks on linear programming. Two early such
books, written by pioneers in the Ô¨Åeld, are Dantzig [4], which in addition to
the mathematical material also contains a thorough historical overview, many
applications and an extensive bibliography, and Gale [1], which provides a
concise but mathematically rigorous presentation of linear programming with
an emphasis on economic applications. More recent books are Chvatal [1]
and Luenberger [1].
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
146
References
146
References
Adler, I. & Megiddo, N.
[1] A simplex algorithm whose average number of steps is bounded between
two quadratic functions of the smaller dimension. J. ACM 32 (1985),
871‚Äì895.
Beale, E.M.L
[1] An alternative method of linear programming, Proc. Cambridge Philos.
Soc. 50 (1954), 513‚Äì523.
Bland, R.G.
[1] New Finite Pivoting Rules for the Simplex Method, Math. Oper. Res. 2
(1977), 103‚Äì107.
Bland, R.G., Goldfarb, D. & Todd, M.J.
[1] The ellipsoid method: A survey, Oper. Res. 29 (1981), 1039‚Äì1091.
Borgwardt, K.H.
[1] The Simplex Method ‚àía probabilistic analysis, Springer-Verlag, 1987.
Boyd, S. & Vandenberghe, L.
[1] Convex Optimization, Cambridge Univ. Press, Cambridge, UK, 2004.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
147
References
Charnes, A.
[1] Optimality and Degeneracy in Linear Programming, Econometrica 20
(1952), 160‚Äì170.
Charnes, A. & Lemke, C.E.
[1] The bounded variable problem. ONR memorandum 10, Carnegie Insti-
tute of Technology, 1954.
Chv¬¥atal, V.
[1] Linear Programming. W.H. Freeman, 1983.
Dantzig, G.B.
[1] Programming of Interdependent Activities.
II. Mathematical Model,
Econometrica 17 (1949), 200‚Äì211.
[2] Maximization of Linear Functions of Variables Subject to Linear In-
equalities. Pages 339‚Äì347 in T.C. Koopmans (ed.), Activity Analysis of
Production and Allocation, John Wiley, 1951.
[3] Upper Bounds, Secondary Constraints and Block Triangularity in Linear
Programming, Econometrica 23 (1955), 174‚Äì183.
[4] Linear Programming and Extensions. Princeton University Press, 1963.
Dantzig, G.B., Orden, A. & Wolfe, P.
[1] The genereralized simplex method for minimizing a linear form under
linear inequality constraints, PaciÔ¨Åc J. Math. 5 (1955), 183‚Äì195.
Fourier, J.-B.
[1] Solution d‚Äôune question particuli`ere du calcul des in¬¥egalit¬¥es. Sid 317‚Äì328
i Oeuvres de Fourier II, 1890.
Gale, D.
[1] The Theory of Linear Economic Models. McGraw‚ÄìHill, 1960.
Gale, D., Kuhn, H.W. & Tucker, A.W.
[1] Linear programming and the theory of games. Pages 317‚Äì329 in Koop-
mans, T.C. (ed.), Activity Analysis of Production and Allocation, John
Wiley & Sons, 1951.
Gonzaga, C.C.
[1] An algorithm for solving linear programming problems in O(n3L) oper-
ations. Pages 1‚Äì28 in Megiddo, N. (ed.), Progress in Mathematical Pro-
gramming: Interior-Point and Related Methods, Springer-Verlag, 1988.
Hitchcock, F.L.
[1] The distribution of a product from several socurces to numerous locali-
ties, J. Math. Phys. 20 (1941), 224‚Äì230.
HoÔ¨Äman, A. J.
[1] Cycling in the Simplex Algorithm. Report No. 2974, National Bureau of
Standards, Gaithersburg, MD, USA, 1953.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
148
References
John, F.
[1] Extremum problems with inequalities as subsidiary conditions, 1948.
Sid 543‚Äì560 i Moser J. (ed.), Fritz John, Collected Papers, Birkh¬®auser
Verlag, 1985.
Kantorovich, L.V.
[1] Mathematical methods of organizing and planning production, Lenin-
grad State Univ. (in Russian), 1939. Engl. translation in Management
Sci. 6 (1960), 366-422.
Karmarkar, N.
[1] A new polynomial-time algorithm for linear programming, Combinator-
ica 4 (1984), 373‚Äì395.
Karush, W.
[1] Minima of Functions of Several Variables with Inequalities as Side Con-
straints. M. Sc. Disseratation. Dept. of Mathematics, Univ. of Chicago,
Chicago, Illinois, 1939.
Khachiyan, L.G.
[1] A polynomial algorithm in linear programming, Dokl. Akad. Nauk SSSR
244 (1979), 1093‚Äì1096.
Engl. translation in Soviet Math. Dokl.
20
(1979), 191-194.
Klee, V. & Minty, G.J.
[1] How Good is the Simplex Algorithm? Pages 159‚Äì175 in Shisha, O. (ed.),
Inequalities, III, Academic Press, 1972.
Koopmans, T.C., ed.
[1] Activity Analysis of Production and Allocation.
John Wiley & Sons,
1951.
Kuhn, H.W. & Tucker, A.W.
[1] Nonlinear programming. Pages 481‚Äì492 in Proc. of the second Berkeley
Symposium on Mathematical Statistics and Probability. Univ. of Cali-
fornia Press, 1951.
Lemke, C.E.
[1] The dual method of solving the linear programming problem, Naval Res.
Logist. Quart. 1 (1954), 36‚Äì47.
Luenberger, D.G.
[1] Linear and Nonlinear Programming. Addison‚ÄìWesley, 1984
Shor, N.Z.
[1] Utilization of the operation of space dilation in the minimization of
convex functions, Cybernet. System Anal. 6 (1970), 7‚Äì15.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
149
References
149
Smale, S.
[1] On the average number of steps in the simplex method of linear pro-
gramming, Math. Program. 27 (1983), 241‚Äì262.
Stigler, G.J.
[1] The Cost of Subsistence, J. Farm Econ. 27 (1945), 303‚Äì314.
Uzawa, H.
[1] The Kuhn‚ÄìTucker theorem in concave programming. In Arrow, K.J.,
Hurwicz, L. & H. Uzawa, H. (eds.), Studies in Linear and Non-linear
Programming, Stanford Univ. Press, 1958.
Wolfe, P.
[1] The Simplex Method for Quadratic Programming, Econometrica 27
(1959), 382‚Äì398.
[2] A Technique for Resolving Degeneracy in Linear Programming, J. of the
Soc. for Industrial and Applied Mathematics 11 (1963), 205‚Äì211.
Wood, M.K. & Dantzig, G.B.
[1] Programming of Interdependent Activities. I. General discussion, Econo-
metrica 17 (1949), 193‚Äì199.
Ye, Y.
[1] An O(n3L) potential reduction algorithm for linear programming. Math.
Program. 50 (1991), 239‚Äì258.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
150
Answers and solutions to the exercises
Answers and solutions to the
exercises
Chapter 9
9.1 min 5000x1 + 4000x2 + 3000x3 + 4000x4
s.t.
Ô£±
Ô£≤
Ô£≥
‚àíx1 + 2x2 + 2x3 + x4 ‚â•16
4x1 + x2
+ 2x4 ‚â•40
3x1 + x2 + 2x3 + x4 ‚â•24, x ‚â•0
9.2 max v
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
2x1 + x2 ‚àí4x3 ‚â•v
x1 + 2x2 ‚àí2x3 ‚â•v
‚àí2x1 ‚àíx2 + 2x3 ‚â•v
x1 + x2 + x3 = 1, x ‚â•0
9.3 The row player should choose row number 2 and the column player
column number 1.
9.4 PayoÔ¨Ämatrix:
Sp E
Ru E
Ru 2
Sp E
‚àí1
1
‚àí1
Ru E
1
‚àí1
‚àí2
Sp 2
‚àí1
2
2
The column players problem can be formulated as
min u
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àíy1 + y2 + y3 ‚â§u
y1 ‚àíy2 ‚àí2y3 ‚â§u
‚àíy1 + 2y2 + 2y3 ‚â§u
y1 + y2 + y3 = 1, y ‚â•0
9.5 a) (4
5, 13
15)
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
151
Answers and solutions to the exercises
9.6 a) max r
s.t.
Ô£±
Ô£≤
Ô£≥
‚àíx1 + x2 + r
‚àö
2 ‚â§0
x1 ‚àí2x2 + r
‚àö
5 ‚â§0
x1 + x2 + r
‚àö
2 ‚â§1
b) max r
s.t.
Ô£±
Ô£≤
Ô£≥
‚àíx1 + x2 + 2r ‚â§0
x1 ‚àí2x2 + 3r ‚â§0
x1 + x2 + 2r ‚â§1
Chapter 10
10.1 œÜ(Œª) = 2Œª ‚àí1
2Œª2
10.2 The dual functions œÜa and œÜb of the two problems are given by:
œÜa(Œª) = 0 for all Œª ‚â•0 and
œÜb(Œª) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
0
if Œª = 0,
Œª ‚àíŒª ln Œª
if 0 < Œª < 1,
1
if Œª ‚â•1.
10.5 The inequality gi(x0) ‚â•gi(ÀÜx)+‚ü®g‚Ä≤
i(ÀÜx), x0‚àíÀÜx‚ü©= ‚ü®g‚Ä≤
i(ÀÜx), x0‚àíÀÜx‚ü©holds for
all i ‚ààI(ÀÜx). It follows that ‚ü®g‚Ä≤
i(ÀÜx), ÀÜx‚àíx0‚ü©‚â•‚àígi(x0) > 0 for i ‚ààIoth(ÀÜx),
and ‚ü®g‚Ä≤
i(ÀÜx), ÀÜx ‚àíx0‚ü©‚â•‚àígi(x0) ‚â•0 for i ‚ààIaÔ¨Ä(ÀÜx).
10.6 a) vmin = ‚àí1 for x = (‚àí1, 0)
b) vmax = 2 + œÄ
4 for x = (1, 1)
c) vmin = ‚àí1
3 for x = ¬±( 2
‚àö
6, ‚àí1
‚àö
6)
d) vmax =
1
54 for x = ( 1
6, 2, 1
3)
Chapter 11
11.1 ÀÜŒª = 2b
11.3 b) Let L: ‚Ñ¶√ó Œõ ‚ÜíR
and
L1 : (R √ó ‚Ñ¶) √ó (R+ √ó Œõ) ‚ÜíR be
the Lagrange functions of the problems (P) and (P‚Ä≤), respectively, and
let œÜ and œÜ1 be the corresponding dual functions. The two Lagrange
functions are related as follows:
L1(t, x, Œª0, Œª) = (1 ‚àíŒª0)(t ‚àíf(x)) + L(x, Œª).
The Lagrange function L1 is for Ô¨Åxed (Œª0, Œª) ‚ààR+ √ó Œõ bounded below
if and only if Œª0 = 1 and Œª ‚ààdom œÜ. Hence, dom œÜ1 = {1} √ó dom œÜ.
Moreover, œÜ1(1, Œª) = œÜ(Œª) for all Œª ‚ààdom œÜ.
11.4 Let I be the index set of all non-aÔ¨Éne constraints, and let k be the
number of elements of I. Slater‚Äôs condition is satisÔ¨Åed by the point
x = k‚àí1 
i‚ààI xi.
11.5 Let b(1) and b(2) be two points in U, and let 0 < Œª < 1. Choose, given
œµ > 0, feasible points x(i) for the problems (Pb(i)) so that f(x(i)) <
vmin(b(i)) + œµ.
The point x = Œªx(1) + (1 ‚àíŒª)x(2) is feasible for the
problem (Pb), where b = Œªb(1) + (1 ‚àíŒª)b(2). Therefore,
vmin(Œªb(1) + (1 ‚àíŒª)b(2)) ‚â§f(x) ‚â§Œªf(x(1)) + (1 ‚àíŒª)f(x(2))
< Œªvmin(b(1)) + (1 ‚àíŒª)vmin(b(2)) + œµ,
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
152
Answers and solutions to the exercises
and since œµ > 0 is arbitrary, this shows that the function vmin is convex
on U.
11.6 a) vmin = 2 for x = (0, 0)
b) vmin = 2 for x = (0, 0)
c) vmin = ln 2‚àí1 for x = (‚àíln 2, 1
2)
d) vmin = ‚àí5 for x = (‚àí1, ‚àí2)
e) vmin = 1 for x = (1, 0)
f) vmin = 2 e1/2 + 1
4 for x = ( 1
2, 1
2)
11.7 vmin = 2 ‚àíln 2 for x = (1, 1)
11.9 min 50x2
1 + 80x1x2 + 40x2
2 + 10x2
3
s.t.
0.2x1 + 0.12x2 + 0.04x3 ‚â•0.12
x1 +
x2 +
x3 = 1,
x ‚â•0
Optimum for x1 = x3 = 0.5 miljon dollars.
Chapter 12
12.1 All nonempty sets X(b) = {x | Ax ‚â•b} of feasible points have the
same recession cone, since recc X(b) = {x | Ax ‚â•0} if X(b) Ã∏= ‚àÖ.
Therefore, it follows from Theorem 12.1.1 that the optimal value v(b)
is Ô¨Ånite if X(b) Ã∏= ‚àÖ. The convexity of the optimal value function v is a
consequence of the same theorem, because
v(b) = min{‚ü®‚àíb, y‚ü©| ATy ‚â§c, y ‚â•0},
according to the duality theorem.
12.2 E.g. min x1 ‚àíx2
s.t.
‚àíx1
‚â•1
x2 ‚â•1, x ‚â•0
and
max y1 + y2
s.t.
‚àíy1
‚â§
1
y2 ‚â§‚àí1, y ‚â•0
12.5 vmax =
Ô£±
Ô£≤
Ô£≥
t ‚àí3
t + 1
for x =

‚àí
2
t + 1, t ‚àí1
t + 1

if t < ‚àí2,
5
for x = (2, 3) if t ‚â•‚àí2.
Chapter 13
13.1 a) min 2x1 ‚àí2x2 + x3
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x2 ‚àíx3 ‚àís1
= 3
x1 + x2 ‚àíx3
+ s2 = 2
x1, x2, x3, s1, s2 ‚â•0
b) min x1 + 2x‚Ä≤
2 ‚àí2x‚Ä≤‚Ä≤
2
s.t.
Ô£±
Ô£≤
Ô£≥
x1 + x‚Ä≤
2 ‚àíx‚Ä≤‚Ä≤
2 ‚àís1
=
1
x‚Ä≤
2 ‚àíx‚Ä≤‚Ä≤
2
‚àís2 = ‚àí2
x1, x‚Ä≤
2, x‚Ä≤‚Ä≤
2, s1, s2 ‚â•
0
13.2 a) (5, 5, 0) and (71
2, 0, 21
2)
b) (3, 0, 0, 0) and (0, 0, 0, 3)
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
153
Answers and solutions to the exercises
13.3 max y1 + 7y2
s.t.
Ô£±
Ô£≤
Ô£≥
y1 + y2 ‚â§1
2y2 ‚â§1
‚àíy1 + 7y2 ‚â§4
13.4 a) vmin = ‚àí1 for x = (0, 0, 4, 1)
b) vmax = 56 for x = (24, 0, 0, 1, 11)
c) vmax = 306
7 for x = (1 5
7, 3
7, 0)
d) vmax = 23 for x = (2, 0, 3, 0, 5)
e) vmin = ‚àí‚àû
f) vmin = ‚àí1 13
15 for x = (0, 2
3, 0, 2
5)
13.5 vmin = ‚àí2 is attained at all points on the line segment between the
points (0, 3, 1, 1, 0) and (0, 2, 2, 0, 1).
13.6 vmax = 15 for x = (2 1
2, 2 1
2, 2 1
2, 0)
13.8 vmin = 9 for x = ( 2
3, 1 2
3, 1 2
3)
13.9 vmin = ‚àí40 3
5 for x = (‚àí3 3
5, 114
5)
13.10 a) vmin = 41
4 for x = ( 3
4, 1
2, 3
4)
b) vmin = 4
5 for x = (0, 2
5, 0)
c) vmin = 5 7
12 for x = (1 1
4, 11
12, 0)
13.12 vmax =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
7
for x = (3 1
2, 0) if t ‚â§1,
4 + 3t
for x = (2, 3) if 1 < t < 2,
5t
for x = (0, 5) if t ‚â•2.
13.13 500 pairs of model A and 700 pairs of model B.
13.14 4 liters of milk and 1 loaf. The milk price could rise to 10 SEK/l.
13.17 First, use the algorithm A on the system consisting of the linear in-
equalities Ax ‚â•b, x ‚â•0, ATy ‚â§c, y ‚â•0, ‚ü®c, x‚ü©‚â§‚ü®b, y‚ü©.
If the
algorithm delivers a solution (x, y), then x is an optimal solution to the
minimization problem because of the complementarity theorem.
If the algorithm instead shows that the system has no solution, then we
use the algorithm on the system Ax ‚â•b, x ‚â•0 to determine whether
the minimization problem has feasible points or not. If this latter sys-
tem has feasible points, then it follows from our Ô¨Årst investigation that
the dual problem has no feasible points, and we conclude that the ob-
jective function is unbounded below, because of the duality theorem.
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
154
Index
Index
active constraint, 6
artiÔ¨Åcial variable, 122
basic index set, 92
feasible ‚Äî, 101
basic solution, 95
degenerate ‚Äî, 96
feasible ‚Äî, 101
basic variable, 85, 95
Bland‚Äôs rule, 118
complementarity, 38
complementary theorem, 76
constraint qualiÔ¨Åcation condition, 41
convex
optimization, 7
quadratic programming, 8
cycling, 106
diet problem, 15
dual
function, 33
price, 16
problem, 36, 70
simplex algorithm, 133
duality, 70
strong ‚Äî, 36
weak ‚Äì, 72
duality theorem, 50, 73
ellipsoid algorithm, 137
feasible
point, 2
solution, 2
free variable, 95
implicit constraint, 7
integer programming, 8
John‚Äôs theorem, 42
Karush‚ÄìKuhn‚ÄìTucker
condition, 40
theorem, 52
Lagrange
function, 32
multiplier, 32
linear
integer programming, 9
programming, 7
objective function, 2
optimal
point, 2
solution, 2
value, 2
optimality criterion, 34, 73, 84
optimization
convex ‚Äî, 7
convex quadratic ‚Äî, 8
linear ‚Äî, 7
non-linear ‚Äî, 8
phase 1, 121
pivot element, 88
polynomial algorithm, 137
reduced cost, 102
Download free eBooks at bookboon.com

LINEAR AND CONVEX OPTIMIZATION: 
CONVEXITY AND OPTIMIZATION ‚Äì PART II
155
Index
155
saddle point, 38
search vector, 99
sensitivity analysis, 66
simplex algorithm, 105
dual ‚Äî, 133
phase 1, 121
simplex tableau, 88
slack variable, 11
Slater‚Äôs condition, 49
standard form, 82
strong duality, 36
surplus variable, 11
transportation problem, 19
two-person zero-sum game, 21
weak duality, 36, 83
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

To see Part III, download:
Descent and Interior-point Methods:  
Convexity and Optimization ‚Äì Part III
Download free eBooks at bookboon.com


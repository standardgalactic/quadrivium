
PROBABILITY AND
STATISTICAL INFERENCE

STATISTICS: Textbooks and Monographs
D. B. Owen, Founding Editor, 1972–1991
1.
The Generalized Jackknife Statistic, H. L. Gray and W. R. Schucan
2.
Multivariate Analysis, Anant M. Kshirsagar
3.
Statistics and Society, Walter T. Federer
4.
Multivariate Analysis: A Selected and Abstracted Bibliography, 1957–1972,
Kocherlakota Subrahmaniam and Kathleen Subrahmaniam
5.
Design of Experiments: A Realistic Approach, Virgil L. Anderson and Robert A.
McLean
6.
Statistical and Mathematical Aspects of Pollution Problems, John W. Pratt
7.
Introduction to Probability and Statistics (in two parts), Part I: Probability; Part II:
Statistics, Narayan C. Giri
8.
Statistical Theory of the Analysis of Experimental Designs, J. Ogawa
9.
Statistical Techniques in Simulation (in two parts), Jack P. C. Kleijnen
10.
Data Quality Control and Editing, Joseph I. Naus
11.
Cost of Living Index Numbers: Practice, Precision, and Theory, Kali S. Banerjee
12.
Weighing Designs: For Chemistry, Medicine, Economics, Operations Research,
Statistics, Kali S. Banerjee
13.
The Search for Oil: Some Statistical Methods and Techniques, edited by D. B. Owen
14.
Sample Size Choice: Charts for Experiments with Linear Models, Robert E. Odeh
and Martin Fox
15.
Statistical Methods for Engineers and Scientists, Robert M. Bethea, Benjamin S.
Duran, and Thomas L. Boullion
16.
Statistical Quality Control Methods, Irving W. Burr
17.
On the History of Statistics and Probability, edited by D. B. Owen
18.
Econometrics, Peter Schmidt
19.
Sufficient Statistics: Selected Contributions, Vasant S. Huzurbazar (edited by Anant
M. Kshirsagar)
20.
Handbook of Statistical Distributions, Jagdish K. Patel, C. H. Kapadia, and D. B.
Owen
21.
Case Studies in Sample Design, A. C. Rosander
22.
Pocket Book of Statistical Tables, compiled by R. E. Odeh, D. B. Owen, Z. W.
Bimbaum, and L. Fisher
23.
The Information in Contingency Tables, D. V. Gokhale and Solomon Kullback
24.
Statistical Analysis of Reliability and Life-Testing Models: Theory and Methods, Lee
J. Bain
25.
Elementary Statistical Quality Control, Irving W. Burr
26.
An Introduction to Probability and Statistics Using BASIC, Richard A. Groeneveld
27.
Basic Applied Statistics, B. L. Raktoe and J. J. Hubert
28.
A Primer in Probability, Kathleen Subrahmaniam
29.
Random Processes: A First Look, R. Syski
30.
Regression Methods: A Tool for Data Analysis, Rudolf J. Freund and Paul D. Minton
31.
Randomization Tests, Eugene S. Edgington
32.
Tables for Normal Tolerance Limits, Sampling Plans and Screening, Robert E. Odeh
and D. B. Owen
33.
Statistical Computing, William J. Kennedy, Jr., and James E. Gentle
34.
Regression Analysis and Its Application: A Data-Oriented Approach, Richard F.
Gunst and Robert L. Mason
35.
Scientific Strategies to Save Your Life, I. D. J. Bross
36.
Statistics in the Pharmaceutical Industry, edited by C. Ralph Buncher and Jia-Yeong
Tsay
37.
Sampling from a Finite Population, J. Hajek

38.
Statistical Modeling Techniques, S. S. Shapiro and A. J. Gross
39.
Statistical Theory and Inference in Research, T. A. Bancroft and C.-P. Han
40.
Handbook of the Normal Distribution, Jagdish K. Patel and Campbell B. Read
41.
Recent Advances in Regression Methods, Hrishikesh D. Vinod and Aman Ullah
42.
Acceptance Sampling in Quality Control, Edward G. Schilling
43.
The Randomized Clinical Trial and Therapeutic Decisions, edited by Niels Tygstrup,
John M Lachin, and Erik Juhl
44.
Regression Analysis of Survival Data in Cancer Chemotherapy, Walter H. Carter, Jr.,
Galen L. Wampler, and Donald M. Stablein
45.
A Course in Linear Models, Anant M. Kshirsagar
46.
Clinical Trials: Issues and Approaches, edited by Stanley H. Shapiro and Thomas H.
Louis
47.
Statistical Analysis of DNA Sequence Data, edited by B. S. Weir
48.
Nonlinear Regression Modeling: A Unified Practical Approach, David A. Ratkowsky
49.
Attribute Sampling Plans, Tables of Tests and Confidence Limits for Proportions,
Robert E. Odeh and D. B. Owen
50.
Experimental Design, Statistical Models, and Genetic Statistics, edited by Klaus
Hinkelmann
51.
Statistical Methods for Cancer Studies, edited by Richard G. Comell
52.
Practical Statistical Sampling for Auditors, Arthur J. Wilbum
53.
Statistical Methods for Cancer Studies, edited by Edward J. Wegman and James G.
Smith
54.
Self-Organizing Methods in Modeling: GMDH Type Algorithms, edited by Stanley J.
Farlow
55.
Applied Factorial and Fractional Designs, Robert A. McLean and Virgil L. Anderson
56.
Design of Experiments: Ranking and Selection, edited by Thomas J. Santner and Ajit
C. Tamhane
57.
Statistical Methods for Engineers and Scientists: Second Edition, Revised and
Expanded, Robert M. Bethea, Benjamin S. Duran, and Thomas L. Boullion
58.
Ensemble Modeling: Inference from Small-Scale Properties to Large-Scale Systems,
Alan E. Gelfand and Crayton C. Walker
59.
Computer Modeling for Business and Industry, Bruce L. Bowerman and Richard T.
O’Connell
60.
Bayesian Analysis of Linear Models, Lyle D. Broemeling
61.
Methodological Issues for Health Care Surveys, Brenda Cox and Steven Cohen
62.
Applied Regression Analysis and Experimental Design, Richard J. Brook and
Gregory C. Arnold
63.
Statpal: A Statistical Package for Microcomputers—PC-DOS Version for the IBM
PC and Compatibles, Bruce J. Chalmer and David G. Whitmore
64.
Statpal: A Statistical Package for Microcomputers—Apple Version for the II, II+, and
Ile, David G. Whitmore and Bruce J. Chalmer
65.
Nonparametric Statistical Inference: Second Edition, Revised and Expanded, Jean
Dickinson Gibbons
66.
Design and Analysis of Experiments, Roger G. Petersen
67.
Statistical Methods for Pharmaceutical Research Planning, Sten W. Bergman and
John C. Gittins
68.
Goodness-of-Fit Techniques, edited by Ralph B. D’Agostino and Michael A. Stephens
69.
Statistical Methods in Discrimination Litigation, edited by D. H. Kaye and Mikel
Aickin
70.
Truncated and Censored Samples from Normal Populations, Helmut Schneider
71.
Robust Inference, M. L. Tiku, W. Y. Tan, and N. Balakrishnan
72.
Statistical Image Processing and Graphics, edited by Edward J. Wegman and Douglas
J. DePriest
73.
Assignment Methods in Combinatorial Data Analysis, Lawrence J. Hubert
74.
Econometrics and Structural Change, Lyle D. Broemeling and Hiroki Tsurumi
75.
Multivariate Interpretation of Clinical Laboratory Data, Adelin Albert and
Eugene K. Harris

76.
Statistical Tools for Simulation Practitioners, Jack P. C. Kleijnen
77.
Randomization Tests: Second Edition, Eugene S. Edgington
78.
A Folio of Distributions: A Collection of Theoretical Quantile-Quantile Plots, Edward B.
Fowlkes
79.
Applied Categorical Data Analysis, Daniel H. Freeman, Jr.
80.
Seemingly Unrelated Regression Equations Models: Estimation and Inference, Virendra
K. Srivastava and David E. A. Giles
81.
Response Surfaces: Designs and Analyses, Andre I. Khuri and John A. Cornell
82.
Nonlinear Parameter Estimation: An Integrated System in BASIC, John C. Nash and
Mary Walker-Smith
83.
Cancer Modeling, edited by James R. Thompson and Barry W. Brown
84.
Mixture Models: Inference and Applications to Clustering, Geoffrey J. McLachlan and
Kaye E. Basford
85.
Randomized Response: Theory and Techniques, Arijit Chaudhuri and Rahul Mukerjee
86.
Biopharmaceutical Statistics for Drug Development, edited by Karl E. Peace
87.
Parts per Million Values for Estimating Quality Levels, Robert E. Odeh and D. B. Owen
88.
Lognormal Distributions: Theory and Applications, edited by Edwin L. Crow and Kunio
Shimizu
89.
Properties of Estimators for the Gamma Distribution, K. O. Bowman and L. R. Shenton
90.
Spline Smoothing and Nonparametric Regression, Randall L. Eubank
91.
Linear Least Squares Computations, R. W. Farebrother
92.
Exploring Statistics, Damaraju Raghavarao
93.
Applied Time Series Analysis for Business and Economic Forecasting, Sufi M. Nazem
94.
Bayesian Analysis of Time Series and Dynamic Models, edited by James C. Spall
95.
The Inverse Gaussian Distribution: Theory, Methodology, and Applications, Raj S.
Chhikara and J. Leroy Folks
96.
Parameter Estimation in Reliability and Life Span Models, A. Clifford Cohen and Betty
Jones Whitten
97.
Pooled Cross-Sectional and Time Series Data Analysis, Terry E. Dielman
98.
Random Processes: A First Look, Second Edition, Revised and Expanded, R. Syski
99.
Generalized Poisson Distributions: Properties and Applications, P. C. Consul
100.
Nonlinear Lp-Norm Estimation, Rene Gonin and Arthur H. Money
101.
Model Discrimination for Nonlinear Regression Models, Dale S. Borowiak
102.
Applied Regression Analysis in Econometrics, Howard E. Doran
103.
Continued Fractions in Statistical Applications, K. O. Bowman and L. R. Shenton
104.
Statistical Methodology in the Pharmaceutical Sciences, Donald A. Berry
105.
Experimental Design in Biotechnology, Perry D. Haaland
106.
Statistical Issues in Drug Research and Development, edited by Karl E. Peace
107.
Handbook of Nonlinear Regression Models, David A. Ratkowsky
108.
Robust Regression: Analysis and Applications, edited by Kenneth D. Lawrence and
Jeffrey L. Arthur
109.
Statistical Design and Analysis of Industrial Experiments, edited by Subir Ghosh
110.
U-Statistics: Theory and Practice, A. J. Lee
111.
A Primer in Probability: Second Edition, Revised and Expanded, Kathleen
Subrahmaniam
112.
Data Quality Control: Theory and Pragmatics, edited by Gunar E. Liepins and V. R. R.
Uppuluri
113.
Engineering Quality by Design: Interpreting the Taguchi Approach, Thomas B. Barker
114.
Survivorship Analysis for Clinical Studies, Eugene K. Harris and Adelin Albert
115.
Statistical Analysis of Reliability and Life-Testing Models: Second Edition, Lee J. Bain
and Max Engelhardt
116.
Stochastic Models of Carcinogenesis, Wai-Yuan Tan
117.
Statistics and Society: Data Collection and Interpretation, Second Edition, Revised and
Expanded, Walter T. Federer
118.
Handbook of Sequential Analysis, B. K. Ghosh and P. K. Sen
119.
Truncated and Censored Samples: Theory and Applications, A. Clifford Cohen

120.
Survey Sampling Principles, E. K. Foreman
121.
Applied Engineering Statistics, Robert M. Bethea and R. Russell Rhinehart
122.
Sample Size Choice: Charts for Experiments with Linear Models: Second Edition,
Robert E. Odeh and Martin Fox
123.
Handbook of the Logistic Distribution, edited by N. Balakrishnan
124.
Fundamentals of Biostatistical Inference, Chap T. Le
125.
Correspondence Analysis Handbook, J.-P. Benzécri
126.
Quadratic Forms in Random Variables: Theory and Applications, A. M. Mathai and
Serge B. Provost
127.
Confidence Intervals on Variance Components, Richard K. Burdick and Franklin A.
Graybill
128.
Biopharmaceutical Sequential Statistical Applications, edited by Karl E. Peace
129.
Item Response Theory: Parameter Estimation Techniques, Frank B. Baker
130.
Survey Sampling: Theory and Methods, Arijit Chaudhuri and Horst Stenger
131.
Nonparametric Statistical Inference: Third Edition, Revised and Expanded, Jean
Dickinson Gibbons and Subhabrata Chakraborti
132.
Bivariate Discrete Distribution, Subrahmaniam Kocherlakota and Kathleen
Kocherlakota
133.
Design and Analysis of Bioavailability and Bioequivalence Studies, Shein-Chung Chow
and Jen-pei Liu
134.
Multiple Comparisons, Selection, and Applications in Biometry, edited by Fred M.
Hoppe
135.
Cross-Over Experiments: Design, Analysis, and Application, David A. Ratkowsky,
Marc A. Evans, and J. Richard Alldredge
136.
Introduction to Probability and Statistics: Second Edition, Revised and Expanded,
Narayan C. Giri
137.
Applied Analysis of Variance in Behavioral Science, edited by Lynne K. Edwards
138.
Drug Safety Assessment in Clinical Trials, edited by Gene S. Gilbert
139.
Design of Experiments: A No-Name Approach, Thomas J. Lorenzen and Virgil L.
Anderson
140.
Statistics in the Pharmaceutical Industry: Second Edition, Revised and Expanded,
edited by C. Ralph Buncher and Jia-Yeong Tsay
141.
Advanced Linear Models: Theory and Applications, Song-Gui Wang and Shein-Chung
Chow
142.
Multistage Selection and Ranking Procedures: Second-Order Asymptotics, Nitis
Mukhopadhyay and Tumulesh K. S. Solanky
143.
Statistical Design and Analysis in Pharmaceutical Science: Validation, Process
Controls, and Stability, Shein-Chung Chow and Jen-pei Liu
144.
Statistical Methods for Engineers and Scientists: Third Edition, Revised and Expanded,
Robert M. Bethea, Benjamin S. Duran, and Thomas L. Boullion
145.
Growth Curves, Anant M. Kshirsagar and William Boyce Smith
146.
Statistical Bases of Reference Values in Laboratory Medicine, Eugene K. Harris and
James C. Boyd
147.
Randomization Tests: Third Edition, Revised and Expanded, Eugene S. Edgington
148.
Practical Sampling Techniques: Second Edition, Revised and Expanded, Ranjan K.
Som
149.
Multivariate Statistical Analysis, Narayan C. Giri
150.
Handbook of the Normal Distribution: Second Edition, Revised and Expanded, Jagdish
K. Patel and Campbell B. Read
151.
Bayesian Biostatistics, edited by Donald A. Berry and Dalene K. Stangl
152.
Response Surfaces: Designs and Analyses, Second Edition, Revised and Expanded,
André I. Khuri and John A. Cornell
153.
Statistics of Quality, edited by Subir Ghosh, William R. Schucany, and William B. Smith
154.
Linear and Nonlinear Models for the Analysis of Repeated Measurements, Edward F.
Vonesh and Vernon M. Chinchilli
155.
Handbook of Applied Economic Statistics, Aman Ullah and David E. A. Giles

156.
Improving Efficiency by Shrinkage: The James-Stein and Ridge Regression
Estimators, Marvin H. J. Gruber
157.
Nonparametric Regression and Spline Smoothing: Second Edition, Randall
L. Eubank
158.
Asymptotics, Nonparametrics, and Time Series, edited by Subir Ghosh
159.
Multivariate Analysis, Design of Experiments, and Survey Sampling, edited by
Subir Ghosh
160.
Statistical Process Monitoring and Control, edited by Sung H. Park and
G. Geoffrey Vining
161.
Statistics for the 21st Century: Methodologies for Applications of the Future,
edited by C. R. Rao and Gábor J. Székely
162.
Probability and Statistical Inference, Nitis Mukhopadhyay
Additional Volumes in Preparation

PROBABILITY AND
STATISTICAL INFERENCE
NITIS MUKHOPADHYAY
University of Connecticut
Storrs, Connecticut

Library of Congress Cataloging-in-Publication Data
Mukhopadhyay, Nitis.
Probability and statistical inference/Nitis Mukhopadhyay.
   p. cm. – (Statistics, textbooks and monographs; v. 162)
Includes bibliographical references and index.
ISBN 0-8247-0379-0 (alk. paper)
1. Probabilities. 2. Mathematical statistics. I. Title. II. Series.
QA273 .M85 2000
519.2—dc2100-022901
This book is printed on acid-free paper.
Headquarters
Marcel Dekker, Inc.
270 Madison Avenue, New York, NY 10016
tel: 212-696-9000; fax: 212-685-4540
Eastern Hemisphere Distribution
Marcel Dekker AG
Hutgasse 4, Postfach 812, CH-4001 Basel, Switzerland
tel: 41-61-261-8482; fax: 41-61-261-8896
World Wide Web
http://www.dekker.com
The publisher offers discounts on this book when ordered in bulk quantities. For
more information, write to Special Sales/Professional Marketing at the
headquarters address above.
Copyright © 2000 by Marcel Dekker, Inc. All Rights Reserved.
Neither this book nor any part may be reproduced or transmitted in any form or
by any means, electronic or mechanical, including photocopying, microfilming,
and recording, or by any information storage and retrieval system, without
permission in writing from the publisher.
Current printing (last digit)
10 9 8 7 6 5 4 3 2
PRINTED IN THE UNITED STATES OF AMERICA

With love and affection,
this book is dedicated to my parents
The late Mr. Manindra Chandra Mukherjee,
Mrs. Snehalata Mukherjee
It is my homage to the two best teachers I have ever known

This page intentionally left blank

Preface
ix
phies, in spite of some limitations/selection bias/exclusions, will hopefully
inspire and energize the readers.
I assure the readers that a lot of effort has gone into this work. Several
readers and reviewers have been very helpful. I remain eternally grateful
to them. But, I alone am responsible for any remaining mistakes and errors.
I will be absolutely delighted if the readers kindly point out errors of any
kind or draw my attention to any part of the text requiring more explanation
or improvement.
It has been a wonderful privilege on my part to teach and share my
enthusiasm with the readers. I eagerly await to hear comments, criticisms,
and suggestions (Electronic Mail: mukhop@uconnvm.uconn.edu). In the
meantime,
Enjoy and Celebrate Statistics!!
I thank you, the readers, for considering my book and wish you all the
very best.
Nitis Mukhopadhyay
January 1, 2000

Preface
This textbook aims to foster the theory of both probability and statistical
inference for first-year graduate students in statistics or other areas in which
a good understanding of statistical concepts is essential. It can also be used
as a textbook in a junior/senior level course for statistics or mathematics/
statistics majors, with emphasis on concepts and examples. The book includes
the core materials that are usually taught in a two-semester or three-quarter
sequence.
A distinctive feature of this book is its set of examples and exercises.
These are essential ingredients in the total learning process. I have tried to
make the subject come alive through many examples and exercises.
This book can also be immensely helpful as a supplementary text in a
significantly higher level course (for example, Decision Theory and Advanced
Statistical Inference) designed for second or third year graduate students in
statistics.
The prerequisite is one year’s worth of calculus. That should be enough
to understand a major portion of the book. There are sections for which
some familiarity with linear algebra, multiple integration and partial
differentiation will be beneficial. I have reviewed some of the important
mathematical results in Section 1.6.3. Also, Section 4.8 provides a selected
review of matrices and vectors.
The first four chapters introduce the basic concepts and techniques in
probability theory, including the calculus of probability, conditional probability,
independence of events, Bayes’s Theorem, random variables, probability
distributions, moments and moment generating functions (mgf), probability
generating functions (pgf), multivariate random variables, independence of
random variables, standard probability inequalities, the exponential family
of distributions, transformations and sampling distributions. Multivariate
normal, t and F distributions have also been briefly discussed. Chapter 5
develops the notions of convergence in probability, convergence in distribution,
the central limit theorem (CLT) for both the sample mean and sample
variance, and the convergence of the density functions of the Chi-square, t
and F distributions.
The remainder of the book systematically develops the concepts of
statistical inference. It is my belief that the concept of “sufficiency” is the
heart of statistical inference and hence this topic deserves appropriate care
and respect in its treatment. I introduce the fundamental notions of sufficiency,
Neyman factorization, information, minimal sufficiency, completeness, and
ancillarity very early, in Chapter 6. Here, Basu’s Theorem and the location,
v

vi
Preface
scale and location-scale families of distributions are also addressed.
The method of moment estimator, maximum likelihood estimator (MLE),
Rao-Blackwell Theorem, Rao-Blackwellization, Cramér-Rao inequality,
uniformly minimum variance unbiased estimator (UMVUE) and Lehmann-
Scheffé Theorems are developed in Chapter 7. Chapter 8 provides the
Neyman-Pearson theory of the most powerful (MP) and uniformly most
powerful (UMP) tests of hypotheses as well as the monotone likelihood
ratio (MLR) property. The concept of a UMP unbiased (UMPU) test is
briefly addressed in Section 8.5.3. The confidence interval and confidence
region methods are elaborated in Chapter 9. Chapter 10 is devoted entirely
to the Bayesian methods for developing the concepts of the highest posterior
density (HPD) credible intervals, the Bayes point estimators and tests of
hypotheses.
Two-sided alternative hypotheses, likelihood ratio (LR) and other tests
are developed in Chapter 11. Chapter 12 presents the basic ideas of large-
sample confidence intervals and test procedures, including variance stabilizing
transformations and properties of MLE. In Section 12.4, I explain how one
arrives at the customary sin–1 (       ), , and tanh–1 (ρ) transformations in the
case of Binomial (p), Poisson (λ), and the correlation coefficient ρ,
respectively.
Chapter 13 introduces two-stage sampling methodologies for determining
the required sample size needed to solve two simple problems in statistical
inference for which, unfortunately, no fixed-sample-size solution exists. This
material is included to emphasize that there is much more to explore beyond
what is customarily covered in a standard one-year statistics course based
on Chapters 1 -12.
Chapter 14 (Appendix) presents (i) a list of notation and abbreviations,
(ii) short biographies of selected luminaries, and (iii) some of the standard
statistical tables computed with the help of MAPLE. One can also find
some noteworthy remarks and examples in the section on statistical tables.
An extensive list of references is then given, followed by a detailed index.
In a two-semester sequence, probability theory is covered in the first
part, followed by statistical inference in the second. In the first semester,
the core material may consist of Chapters 1-4 and some parts of Chapter 5.
In the second semester, the core material may consist of the remainder of
Chapter 5 and Chapters 6-10 plus some selected parts of Chapters 11-13.
The book covers more than enough ground to allow some flexibility in the
selection of topics beyond the core. In a three-quarter system, the topics
will be divided somewhat differently, but a year’s worth of material taught
in either a two-semester or three-quarter sequence will be similar.
p

Preface
vii
Obviously there are competing textbooks at this level. What sets this
book apart from the others? Let me highlight some of the novel features
of this book:
1. The material is rigorous, both conceptually and mathematically, but I
have adopted what may be called a “tutorial style.” In Chapters 1-12, the
reader will find numerous worked examples. Techniques and concepts are
typically illustrated through a series of examples and related exercises,
providing additional opportinities for absorption. It will be hard to find another
book that has even one-half the number of worked examples!
2. At the end of each chapter, a long list of exercises is arranged according
to the section of a concept’s origin (for example, Exercise 3.4.2 is the second
exercise related to the material presented in Section 3.4). Many exercises
are direct follow-ups on the worked-out examples. Hints are frequently
given in the exercises. This kind of drill helps to reinforce and emphasize
important concepts as well as special mathematical techniques. I have found
over the years that the ideas, principles, and techniques are appreciated
more if the student solves similar examples and exercises. I let a reader
build up his/her own confidence first and then challenge the individual to
approach harder problems, with substantial hints when appropriate. I try to
entice a reader to think through the examples and then do the problems.
3. I can safely remark that I often let the examples do the talking. After
giving a series of examples or discussing important issues, I routinely
summarize within a box what it is that has been accomplished or where one
should go from here. This feature, I believe, should help a reader to focus
on the topic just learned, and move on.
4. There are numerous figures and tables throughout the book. I have
also used computer simulations in some instances. From the layout, it should
be obvious that I have used the power of a computer very liberally.
I should point out that the book contains unique features throughout. Let
me highlight a few examples:
a) In Section 2.4, the “moment problem” is discussed in an elementary
fashion. The two given density functions plotted in Figure 2.4.1 have identical
moments of all orders. This example is not new, but the two plots certainly
should grab one’s attention! Additionally, Exercise 2.4.6 guides a reader in
the construction of other examples. Next, at this level, hardly any book
discusses the role of a probability generating function. Section 2.5 does
precisely that with the help of examples and exercises. Section 3.5 and
related exercises show how easily one can construct examples of a collection
of dependent random variables having certain independent subsets within

viii
Preface
the collection. With the help of interesting examples and discussions, Section
3.7 briefly unfolds the intricate relationship between “zero correlation” and
“independence” for two random variables.
b) In Chapter 4, the Helmert transformation for a normal distribution,
and the transformation involving the spacings for an exponential distribution,
have both been developed thoroughly. The related remarks are expected to
make many readers pause and think. Section 4.6 exposes readers to some
continuous multivariate distributions other than the multivariate normal.
Section 4.7 has special messages – in defining a random variable having
the Student’s t or F distribution, for example, one takes independent random
variables in the numerator and denominator. But, what happens when the
random variables in the numerator and denominator are dependent? Some
possible answers are emphasized with the help of examples. Exercise 4.7.4
shows a way to construct examples where the distribution of a sample
variance is a multiple of Chi-square even though the random samples do
not come from a normal population!
c) The derivation of the central limit theorem for the sample variance
(Theorem 5.3.6) makes clever use of several non-trivial ingredients from
the theory of probability. In other words, this result reinforces the importance
of many results taught in the preceding sections. That should be an important
aspect of learning. No book at this level highlights this in the way I have. In
Section 5.4, various convergence properties of the densities and percentage
points of the Student’s t and F distributions, for example, are laid out. The
usefulness of such approximations is emphasized through computation. In
no other book like this will one find such engaging discussions and
comparisons.
d) No book covers the topics of Chapter 6, namely, sufficiency,
information, and ancillarity, with nearly as much depth or breadth for the
target audience. In particular, Theorem 6.4.2 helps in proving the sufficiency
property of a statistic via its information content. The associated simple
examples and exercises then drive the point home. One will discover out-
of-the-ordinary remarks, ideas and examples throughout the book.
e) The history of statistics and statistical discoveries should not be sep-
arated from each other since neither can exist without the other. It may be
noted that Folks (1981) first added some notable historical remarks within
the material of his textbook written at the sophomore level. I have found
that at all levels of instructions, students enjoy the history very much and
they take more interest in the subject when the human element comes
alive. Thus, I have added historical remarks liberally throughout the text.
Additionally, in Section 14.2, I have given selected biographical notes on some
of the exceptional contributors to the development of statistics. The biogra-

This page intentionally left blank

Acknowledgments
A long time ago, in my transition from Salkia A. S. High School to
Presidency College, Calcutta, followed by the Indian Statistical Institute,
Calcutta, I had the good fortune of learning from many great teachers. I
take this opportunity to express my sincerest gratitude to all my teachers,
especially to Mr. Gobinda Bandhu Chowdhury, Professors Debabrata Basu,
Biren Bose, Malay Ghosh, Sujit K. Mitra, and to Professor Anis C.
Mukhopadhyay, who is my elder brother.
In good times and not so good times, I have been lucky to be able to
count on my mentors, Professors P. K. Sen, Malay Ghosh, Bimal K. Sinha
and Bikas K. Sinha, for support and guidance. From the bottom of my
heart, I thank them for their kindness and friendship.
During the past twenty-five years, I have taught this material at a number
of places, including Monash University in Melbourne, Australia, as well as
the University of Minnesota-Minneapolis, the University of Missouri-Columbia,
the Oklahoma State University-Stillwater and the University of Connecticut-
Storrs. Any time a student asked me a question, I learned something. When
a student did not ask questions whereas he/she perhaps should have, I have
wondered why no question arose. From such soul searching, I learned
important things, too. I have no doubt that the students have made me a
better teacher. I thank all my students, both inside and outside of classrooms.
I am indebted tremendously to Dr. William T. Duggan. He encouraged
me in writing this book since its inception and he diligently read several
versions and caught many inconsistencies and errors. It is my delight to
thank Bill for all his suggestions, patience, and valuable time.
My son, Shankha, has most kindly gone through the whole manuscript
to “test” its flow and readability, and he did so during perhaps the busiest
time of his life, just prior to his entering college. He suggested many stylistic
changes and these have been very valuable. Shankha, thank you.
I thank Professor Tumulesh K. S. Solanky, for going through an earlier
draft and for encouraging me throughout this project. I am also indebted to
Professor Makoto Aoshima and I thank him for the valuable suggestions he
gave me.
Without the support of the students, colleagues and staff of the
Department of Statistics at the University of Connecticut-Storrs, this project
could not have been completed. I remain grateful for this support, particularly
to Professor Dipak K. Dey, the Head of the Department.
xi

I am grateful to Mr. Greg Cicconetti, one of the graduate students in the
Department of Statistics at the University of Connecticut-Storrs, for teaching
me some tricks with computer graphics. He also enthusiastically helped me
with some of the last minute details. Greg, thanks for the support.
I take this opportunity to especially thank my colleague, Professor Joe
Glaz, who gave me constant moral support.
Those who know me personally may not believe that I have typed this
book myself. I gathered that unbelievable courage because one special
individual, Mrs. Cathy Brown, our department’s administrative assistant,
told me I could do it and that she would help me with Latex any time I
needed help. She has helped me with Latex, and always with a smile, on
innumerable occasions during the most frustrating moments. It is impossible
for me to thank Cathy enough.
I remain grateful to the anonymous reviewers of the manuscript in various
stages as well as to the editorial and production staff at Marcel Dekker,
Inc. I am particularly indebted to Ms. Maria Allegra and Ms. Helen Paisner
for their help and advice at all levels.
Last but not least, I express my heartfelt gratitude to my wife, Mahua,
and our two boys, Shankha and Ranjan. During the past two years, we
have missed a number of activities as a family. No doubt my family sacrificed
much, but I did not hear many complaints. I was “left alone” to complete
this project. I express my deepest appreciation to the three most sensible,
caring, and loving individuals I know.

Contents
Preface
v
Acknowledgments
xi
1
Notions of Probability
1
1.1
Introduction
1
1.2
About Sets
3
1.3
Axiomatic Development of Probability
6
1.4
The Conditional Probability and Independent Events
9
1.4.1
Calculus of Probability
12
1.4.2
Bayes’s Theorem
14
1.4.3
Selected Counting Rules
16
1.5
Discrete Random Variables
18
1.5.1
Probability Mass and Distribution Functions
19
1.6
Continuous Random Variables
23
1.6.1
Probability Density and Distribution Functions
23
1.6.2
The Median of a Distribution
28
1.6.3
Selected Reviews from Mathematics
28
1.7
Some Standard Probability Distributions
32
1.7.1
Discrete Distributions
33
1.7.2
Continuous Distributions
37
1.8
Exercises and Complements
50
2
Expectations of Functions of Random Variables
65
2.1
Introduction
65
2.2
Expectation and Variance
65
2.2.1
The Bernoulli Distribution
71
2.2.2
The Binomial Distribution
72
2.2.3
The Poisson Distribution
73
2.2.4
The Uniform Distribution
73
2.2.5
The Normal Distribution
73
2.2.6
The Laplace Distribution
76
2.2.7
The Gamma Distribution
76
2.3
The Moments and Moment Generating Function
77
2.3.1
The Binomial Distribution
80
2.3.2
The Poisson Distribution
81
2.3.3
The Normal Distribution
82

xiv
Contents
2.3.4
The Gamma Distribution
84
2.4
Determination of a Distribution via MGF
86
2.5
The Probability Generating Function
88
2.6
Exercises and Complements
89
3
Multivariate Random Variables
99
3.1
Introduction
99
3.2
Discrete Distributions
100
3.2.1
The Joint, Marginal and Conditional Distributions
101
3.2.2
The Multinomial Distribution
103
3.3
Continuous Distributions
107
3.3.1
The Joint, Marginal and Conditional Distributions
107
3.3.2
Three and Higher Dimensions
115
3.4
Covariances and Correlation Coefficients
119
3.4.1
The Multinomial Case
124
3.5
Independence of Random Variables
125
3.6
The Bivariate Normal Distribution
131
3.7
Correlation Coefficient and Independence
139
3.8
The Exponential Family of Distributions
141
3.8.1
One-parameter Situation
141
3.8.2
Multi-parameter Situation
144
3.9
Some Standard Probability Inequalities
145
3.9.1
Markov and Bernstein-Chernoff Inequalities
145
3.9.2
Tchebysheff’s Inequality
148
3.9.3
Cauchy-Schwarz and Covariance Inequalities
149
3.9.4
Jensen’s and Lyapunov’s Inequalities
152
3.9.5
Hölder’s Inequality 156
3.9.6
Bonferroni Inequality
157
3.9.7
Central Absolute Moment Inequality
158
3.10
Exercises and Complements
159
4
Functions of Random Variables and Sampling
Distribution
177
4.1
Introduction
177
4.2
Using Distribution Functions
179
4.2.1
Discrete Cases
179
4.2.2
Continuous Cases
181
4.2.3
The Order Statistics
182
4.2.4
The Convolution
185
4.2.5
The Sampling Distribution
187
4.3
Using the Moment Generating Function
190
4.4
A General Approach with Transformations
192
4.4.1
Several Variable Situations
195
4.5
Special Sampling Distributions
206

Contents
xv
4.5.1
The Student’s t Distribution
207
4.5.2
The F Distribution
209
4.5.3
The Beta Distribution
211
4.6
Special Continuous Multivariate Distributions
212
4.6.1
The Normal Distribution
212
4.6.2
The t Distribution
218
4.6.3
The F Distribution
219
4.7
Importance of Independence in Sampling Distributions
220
4.7.1
Reproductivity of Normal Distributions
220
4.7.2
Reproductivity of Chi-square Distributions
221
4.7.3
The Student’s t Distribution
223
4.7.4
The F Distribution
223
4.8
Selected Review in Matrices and Vectors
224
4.9
Exercises and Complements
227
5
Concepts of Stochastic Convergence
241
5.1
Introduction
241
5.2
Convergence in Probability
242
5.3
Convergence in Distribution
253
5.3.1
Combination of the Modes of Convergence
256
5.3.2
The Central Limit Theorems
257
5.4
Convergence of Chi-square, t, and F Distributions
264
5.4.1
The Chi-square Distribution
264
5.4.2
The Student’s t Distribution
264
5.4.3
The F Distribution
265
5.4.4
Convergence of the PDF and Percentage Points
265
5.5
Exercises and Complements
270
6
Sufficiency, Completeness, and Ancillarity
281
6.1
Introduction
281
6.2
Sufficiency
282
6.2.1
The Conditional Distribution Approach
284
6.2.2
The Neyman Factorization Theorem
288
6.3
Minimal Sufficiency
294
6.3.1
The Lehmann-Scheffé Approach
295
6.4
Information
300
6.4.1
One-parameter Situation
301
6.4.2
Multi-parameter Situation
304
6.5
Ancillarity
309
6.5.1
The Location, Scale, and Location-Scale Families
314
6.5.2
Its Role in the Recovery of Information
316
6.6
Completeness
318
6.6.1
Complete Sufficient Statistics
320
6.6.2
Basu’s Theorem
324
6.7
Exercises and Complements
327

xvi
Contents
7
Point Estimation
341
7.1
Introduction
341
7.2
Finding Estimators
342
7.2.1
The Method of Moments
342
7.2.2
The Method of Maximum Likelihood
344
7.3
Criteria to Compare Estimators
351
7.3.1
Unbiasedness, Variance and Mean Squared Error
351
7.3.2
Best Unbiased and Linear Unbiased Estimators
354
7.4
Improved Unbiased Estimator via Sufficiency
358
7.4.1
The Rao-Blackwell Theorem
358
7.5
Uniformly Minimum Variance Unbiased Estimator
365
7.5.1
The Cramér-Rao Inequality and UMVUE
366
7.5.2
The Lehmann-Scheffé Theorems and UMVUE
371
7.5.3
A Generalization of the Cramér-Rao Inequality
374
7.5.4
Evaluation of Conditional Expectations
375
7.6
Unbiased Estimation Under Incompleteness
377
7.6.1
Does the Rao-Blackwell Theorem Lead
to UMVUE?
377
7.7
Consistent Estimators
380
7.8
Exercises and Complements
382
8
Tests of Hypotheses
395
8.1
Introduction
395
8.2
Error Probabilities and the Power Function
396
8.2.1
The Concept of a Best Test
399
8.3
Simple Null Versus Simple Alternative Hypotheses
401
8.3.1
Most Powerful Test via the Neyman-Pearson
Lemma
401
8.3.2
Applications: No Parameters Are Involved
413
8.3.3
Applications: Observations Are Non-IID
416
8.4
One-Sided Composite Alternative Hypothesis
417
8.4.1
UMP Test via the Neyman-Pearson Lemma
417
8.4.2
Monotone Likelihood Ratio Property
420
8.4.3
UMP Test via MLR Property
422
8.5
Simple Null Versus Two-Sided Alternative Hypotheses
425
8.5.1
An Example Where UMP Test Does Not Exist
425
8.5.2
An Example Where UMP Test Exists
426
8.5.3
Unbiased and UMP Unbiased Tests
428
8.6
Exercises and Complements
429
9
Confidence Interval Estimation
441
9.1
Introduction
441
9.2
One-Sample Problems
443
9.2.1
Inversion of a Test Procedure
444

Contents
xvii
9.2.2
The Pivotal Approach
446
9.2.3
The Interpretation of a Confidence Coefficient
451
9.2.4
Ideas of Accuracy Measures
452
9.2.5
Using Confidence Intervals in the Tests
of Hypothesis
455
9.3
Two-Sample Problems
456
9.3.1
Comparing the Location Parameters
456
9.3.2
Comparing the Scale Parameters
460
9.4
Multiple Comparisons
463
9.4.1
Estimating a Multivariate Normal Mean Vector
463
9.4.2
Comparing the Means
465
9.4.3
Comparing the Variances
467
9.5
Exercises and Complements
469
10 Bayesian Methods
477
10.1
Introduction
477
10.2
Prior and Posterior Distributions
479
10.3
The Conjugate Priors
481
10.4
Point Estimation
485
10.5
Credible Intervals
488
10.5.1 Highest Posterior Density
489
10.5.2 Contrasting with the Confidence Intervals
492
10.6
Tests of Hypotheses
493
10.7
Examples with Non-Conjugate Priors
494
10.8
Exercises and Complements
497
11 Likelihood Ratio and Other Tests
507
11.1
Introduction
507
11.2
One-Sample Problems
508
11.2.1 LR Test for the Mean
509
11.2.2 LR Test for the Variance
512
11.3
Two-Sample Problems
515
11.3.1 Comparing the Means
515
11.3.2 Comparing the Variances
519
11.4
Bivariate Normal Observations
522
11.4.1 Comparing the Means: The Paired Difference
t Method
522
11.4.2 LR Test for the Correlation Coefficient
525
11.4.3 Tests for the Variances
528
11.5
Exercises and Complements
529
12 Large-Sample Inference
539
12.1
Introduction
539
12.2
The Maximum Likelihood Estimation
539
12.3
Confidence Intervals and Tests of Hypothesis
542

xviii
Contents
12.3.1 The Distribution-Free Population Mean
543
12.3.2 The Binomial Proportion
548
12.3.3 The Poisson Mean
553
12.4
The Variance Stabilizing Transformations
555
12.4.1 The Binomial Proportion
556
12.4.2 The Poisson Mean
559
12.4.3 The Correlation Coefficient
560
12.5
Exercises and Complements
563
13 Sample Size Determination: Two-Stage
Procedures
569
13.1
Introduction
569
13.2
The Fixed-Width Confidence Interval
573
13.2.1 Stein’s Sampling Methodology
573
13.2.2 Some Interesting Properties
574
13.3
The Bounded Risk Point Estimation
579
13.3.1 The Sampling Methodology
581
13.3.2 Some Interesting Properties
582
13.4
Exercises and Complements
584
14 Appendix
591
14.1
Abbreviations and Notation
591
14.2
A Celebration of Statistics: Selected Biographical Notes
593
14.3
Selected Statistical Tables
621
14.3.1 The Standard Normal Distribution Function
621
14.3.2 Percentage Points of the Chi-Square Distribution
626
14.3.3 Percentage Points of the Student’s t Distribution
628
14.3.4 Percentage Points of the F Distribution
630
References
633
Index
649

1
Notions of Probability
1.1 Introduction
In the study of the subject of probability, we first imagine an appropriate
random experiment. A random experiment has three important components
which are:
a)
multiplicity of outcomes,
b)
uncertainty regarding the outcomes, and
c)
repeatability of the experiment in identical fashions.
Suppose that one tosses a regular coin up in the air. The coin has two sides,
namely the head (H) and tail (T). Let us assume that the tossed coin will land
on either H or T. Every time one tosses the coin, there is the possibility of the
coin landing on its head or tail (multiplicity of outcomes). But, no one can say
with absolute certainty whether the coin would land on its head, or for that
matter, on its tail (uncertainty regarding the outcomes). One may toss this coin
as many times as one likes under identical conditions (repeatability) provided
the coin is not damaged in any way in the process of tossing it successively.
All three components are crucial ingredients of a random experiment. In
order to contrast a random experiment with another experiment, suppose that in
a lab environment, a bowl of pure water is boiled and the boiling temperature is
then recorded. The first time this experiment is performed, the recorded tem-
perature would read 100° Celsius (or 212° Fahrenheit). Under identical and
perfect lab conditions, we can think of repeating this experiment several times,
but then each time the boiling temperature would read 100° Celsius (or 212°
Fahrenheit). Such an experiment will not fall in the category of a random ex-
periment because the requirements of multiplicity and uncertainty of the out-
comes are both violated here.
We interpret probability of an event as the relative frequency of the oc-
currence of that event in a number of independent and identical replications
of the experiment. We may be curious to know the magnitude of the prob-
ability p of observing a head (H) when a particular coin is tossed. In order to
gather valuable information about p, we may decide to toss the coin ten times,
for example, and suppose that the following sequence of H and T is

1. Nations of Probability
observed:
Let nk be the number of H’s observed in a sequence of k tosses of the coin while
nk/k refers to the associated relative frequency of H. For the observed sequence
in (1.1.1), the successive frequencies and relative frequencies of H are given in
the accompanying Table 1.1.1. The observed values for nk/k empirically pro-
vide a sense of what p may be, but admittedly this particular observed sequence
of relative frequencies appears a little unstable. However, as the number of
tosses increases, the oscillations between the successive values of nk/k will
become less noticeable. Ultimately nk/k and p are expected to become indistin-
guishable in the sense that in the long haul nk/k will be very close to p. That is,
our instinct may simply lead us to interpret p as 
∞
→
klim (nk/k).
Table 1.1.1. Behavior of the Relative Frequency of the H’s
k
nk
nk/k
k
nk
nk/k
1
0
0
6
2
1/3
2
1
1/2
7
3
3/7
3
1
1/3
8
3
3/8
4
2
1/2
9
3
1/3
5
2
2/5
10
4
2/5
A random experiment provides in a natural fashion a list of all possible
outcomes, also referred to as the simple events. These simple events act like
“atoms” in the sense that the experimenter is going to observe only one of these
simple events as a possible outcome when the particular random experiment is
performed. A sample space is merely a set, denoted by S, which enumerates
each and every possible simple event or outcome. Then, a probability scheme is
generated on the subsets of S, including S itself, in a way which mimics the
nature of the random experiment itself. Throughout, we will write P(A) for the
probability of a statement A(⊆ S). A more precise treatment of these topics is
provided in the Section 1.3. Let us look at two simple examples first.
Example 1.1.1 Suppose that we toss a fair coin three times and record
the outcomes observed in the first, second, and third toss respectively from
left to right. Then the possible simple events are HHH, HHT,
HTH, HTT, THH, THT, TTH or TTT. Thus the sample space is given by
2

1. Nations of Probability
Since the coin is assumed to be fair, this particular random experiment gener-
ates the following probability scheme: P(HHH) = P(HHT) = P(HTH) = P(HTT)
= P(THH) = P(THT) = P(TTH) = P(TTT) = 1/8. !
Example 1.1.2 Suppose that we toss two fair dice, one red and the other
yellow, at the same time and record the scores on their faces landing upward.
Then, each simple event would constitute, for example, a pair ij where i is the
number of dots on the face of the red die that lands up and j is the number of
dots on the face of the yellow die that lands up. The sample space is then given
by S = {11, 12, ..., 16, 21, ..., 26, ..., 61, ..., 66} consisting of exactly 36 pos-
sible simple events. Since both dice are assumed to be fair, this particular ran-
dom experiment generates the following probability scheme: P(ij) = 1/36 for
all i, j = 1, ..., 6. !
Some elementary notions of set operations are reviewed in the Section 1.2.
The Section 1.3 describes the setup for developing the formal theory of prob-
ability. The Section 1.4 introduces the concept of conditional probability fol-
lowed by the notions such as the additive rules, multiplicative rules, and Bayes’s
Theorem. The Sections 1.5-1.6 respectively introduces the discrete and con-
tinuous random variables, and the associated notions of a probability mass
function (pmf), probability density function (pdf) and the distribution function
(df). The Section 1.7 summarizes some of the standard probability distributions
which are frequently used in statistics.
1.2 About Sets
A set S is a collection of objects which are tied together with one or more
common defining properties. For example, we may consider a set S = {x : x is
an integer} in which case S can be alternately written as {..., –2, –1, 0, 1, 2,
...}. Here the common defining property which ties in all the members of the set
S is that they are integers.
Let us start with a set S. We say that A is a subset of S, denoted by A ⊆ S,
provided that each member of A is also a member of S. Consider A and B which
are both subsets of S. Then, A is called a subset of B, denoted by A ⊆ B, pro-
vided that each member of A is also a member of B. We say that A is a proper
subset of B, denoted by A ⊂ B, provided that A is a subset of B but there is at
least one member of B which does not belong to A. Two sets A and B are said to
be equal if and only if A ⊆ B as well as B ⊆ A.
Example 1.2.1 Let us define S = {1, 3, 5, 7, 9, 11, 13},A = {1, 5, 7}, B =
{1, 5, 7, 11, 13}, C = {3, 5, 7, 9}, and D = {11, 13}. Here, A, B, C and D are
3

1. Nations of Probability
all proper subsets of S. It is obvious that A is a proper subset of B, but A is
not a subset of either C or D. !
Suppose that A and B are two subsets of S. Now, we mention some custom-
ary set operations listed below:
Figure 1.2.1. Venn Diagrams: Shaded Areas
Correspond to the Sets (a) Ac ∩ B (b) A ∪ B
Figure 1.2.2. Venn Diagrams: Shaded Areas
Correspond to the Sets (a) A ∩ B (b) A ∆ B
The union and intersection operations also satisfy the following laws: For
any subsets A, B, C of S, we have
We say that A and B are disjoint if and only if there is no common element
between A and B, that is, if and only if A ∩ B = ϕ, an empty set. Two disjoint
sets A and B are also referred to as being mutually exclusive.
4

1. Nations of Probability
Example 1.2.2 (Example 1.2.1 Continued) One can verify that A∪B = {1,
5, 7, 11, 13}, B∪C = S, but C and D are mutually exclusive. Also, A and D are
mutually exclusive. Note that Ac = {3, 9, 11, 13}, Α∆C = {1, 3, 9} and A∆B =
{11, 13}. !
Now consider {Ai; i ∈ I}, a collection of subsets of S. This collection may
be finite, countably infinite or uncountably infinite. We define
The equation (1.2.3) lays down the set operations involving the union and
intersection among arbitrary number of sets. When we specialize I = {1, 2, 3,
...} in the definition given by (1.2.3), we can combine the notions of countably
infinite number of unions and intersections to come up with some interesting
sets. Let us denote
Interpretation of the set B: Here the set B is the intersection of the collec-
tion of sets , 
   j = 1. In other words, an element x will belong to B if and
only if x belongs to 
 Aifor each j = 1 which is equivalent to saying that
there exists a sequence of positive integers i1 < i2 < ... < ik < ... such that x ∈
Aik for all k = 1, 2, ... . That is, the set B corresponds to the elements which
are hit infinitely often and hence B is referred to as the limit (as n → ∞)
supremum of the sequence of sets An, n = 1, 2, ... .
Interpretation of the set C: On the other hand, the set C is the union of the
collection of sets 
 Ai  j = 1. In other words, an element x will belong to C if
and only if x belongs to  
 Ai for some j ≥ 1 which is equivalent to saying that
x belongs to Aj, Aj+1, ... for some j ≥ 1. That is, the set C corresponds to the
elements which are hit eventually and hence C is referred to as the limit
(as n → ∞) infimum of the sequence of sets An, n = 1, 2, ... .
Theorem 1.2.1 (DeMorgan’s Law) Consider {Ai; i ∈ I}, a collection of
subsets of S. Then,
Proof Suppose that an element x belongs to the lhs of (1.2.5). That is, x ∈ S
but x ∉ ∪i∈I Ai, which implies that x can not belong to any of the
5

1. Nations of Probability
sets Ai, ∈ I. Hence, the element x must belong to the set ∩i∈  I (Ai
c). Thus, we
have (∪i∈ I Ai)c ⊆ ∩i∈ I (Ai
c)
Suppose that an element x belongs to the rhs of (1.2.5). That is x ∈ Ai
c for
each i ∈ I, which implies that x can not belong to any of the sets Ai, i ∈ I. In
other words, the element x can not belong to the set ∪i∈ I Ai so that x must
belong to the set (∪i∈ I  Ai)c Thus, we have (∪i∈ I  Ai)c ⊇ ∩i∈ I (Ai
c). The proof is
now complete. !
Definition 1.2.1 The collection of sets {Ai ; i ∈ I} is said to consist of
disjoint sets if and only if no two sets in this collection share a common ele-
ment, that is when Ai ∩ Aj = ϕ  for all i ≠  j ∈ I. The collection {Ai ; ∈ I} is
called a partition of S if and only if
(i)
{Ai ; i ∈ I} consists of disjoint sets only, and
(ii)
{Ai ; i ∈ I} spans the whole space S, that is ∪i∈ I Ai = S.
Example 1.2.3 Let S = (0,1] and define the collection of sets {Ai ; i∈ I}
where Ai = (
 i ∈ I = {1,2,3, ...}. One should check that the given
collection of intervals form a partition of (0,1]. !
1.3 Axiomatic Development of Probability
The axiomatic theory of probability was developed by Kolmogorov in his 1933
monograph, originally written in German. Its English translation is cited as
Kolmogorov (1950b). Before we describe this approach, we need to fix some
ideas first. Along the lines of the examples discussed in the Introduction, let us
focus on some random experiment in general and state a few definitions.
Definition 1.3.1 A sample space is a set, denoted by S, which enumerates
each and every possible outcome or simple event.
In general an event is an appropriate subset of the sample space S, includ-
ing the empty subset ϕ and the whole set S. In what follows we make this
notion more precise.
Definition 1.3.2 Suppose that ß = {Ai : Ai ⊆ S,i ∈ I} is a collection of
subsets of S. Then, ß is called a Borel sigma-field or Borel sigma-algebra if
the following conditions hold:
(i)
The empty set ϕ ∈ ß;
(ii)
If A ∈ ß, then Ac ∈ ß;
(iii) If Ai ∈ ß for i = 1,2, ..., then  
 ∈ ß.
In other words, the Borel sigma-field ß is closed under the operations of
complement and countable union of its members. It is obvious that the
6

1. Nations of Probability
whole space S belongs to the Borel sigma-field ß since we can write S = ϕc ∈
ß, by the requirement (i)-(ii) in the Definition 1.3.2. Also if Ai ∈ ß for i = 1,2,
..., k, then 
  Ai ∈ ß, since with Ai = ϕ for i = k+1, k+2, ..., we can express
 Ai as 
 Ai which belongs to ß in view of (iii) in the Definition 1.3.2.
That is, ß is obviously closed under the operation of finite unions of its mem-
bers. See the Exercise 1.3.1 in this context.
Definition 1.3.3 Suppose that a fixed collection of subsets ß = {Ai : Ai ⊆
S, i ∈ I} is a Borel sigma-field. Then, any subset A of S is called an event if
and only if A ∈ ß.
Frequently, we work with the Borel sigma-field which consists of all sub-
sets of the sample space S but always it may not necessarily be that way. Hav-
ing started with a fixed Borel sigma-field ß of subsets of S, a probability scheme
is simply a way to assign numbers between zero and one to every event while
such assignment of numbers must satisfy some general guidelines. In the next
definition, we provide more specifics.
Definition 1.3.4 A probability scheme assigns a unique number to a set A
∈ ß, denoted by P(A), for every set A ∈ ß in such a way that the following
conditions hold:
Now, we are in a position to claim a few basic results involving probability.
Some are fairly intuitive while others may need more attention.
Theorem 1.3.1 Suppose that A and B are any two events and recall that φ
denotes the empty set. Suppose also that the sequence of events {Bi; i = 1}
forms a partition of the sample space S. Then,
(i)
P(ϕ) = 0 and P(A) = 1;
(ii)
P(Ac) = 1 – P(A);
(iii) P(B ∩ Ac) = P(B) – P(B ∩ A);
(iv) P(A ∪ B) = P(A) + P(B) – P(A ∩ B);
(v)
If A ⊆ B, then P(A) ≤ P(B);
(vi) P(A) = 
 P(A ∩ Bi).
Proof (i) Observe that ϕ ∪ ϕc = S and also ϕ, ϕc are disjoint events.
Hence, by part (iii) in the Definition 1.3.4, we have 1 = P(S) = P(ϕ∪ϕc) =
P(ϕ) + P(ϕc). Thus, P(ϕ) = 1 – P(ϕc = 1 – P(S) = 1 – 1 = 0, in view of part
(i) in the Definition 1.3.4. The second part follows from part (ii). "
7

1. Nations of Probability
(ii)
Observe that A ∪ Ac = S and then proceed as before. Observe that A
and Ac are disjoint events. "
(iii) Notice that B = (B ∩ A) ∪ (B ∩ Ac) where B ∩ A and B ∩ Ac are
disjoint events. Hence by part (iii) in the Definition 1.3.4, we claim that
Now, the result is immediate. "
(iv) It is easy to verify that A ∪ B = (A ∩ Bc) ∪ (B ∩ Ac) ∪ (A ∩ B)
where the three events A ∩ Bc, B ∩ Ac, A ∩ B are also disjoint. Thus, we
have
which leads to the desired result. "
(v)
We leave out its proof as the Exercise 1.3.4. "
(vi) Since the sequence of events {Bi ; i ≥ 1} forms a partition of the
sample space S, we can write 
where the events A ∩ Bi, i = 1,2, ... are also disjoint. Now, the result follows
from part (iii) in the Definition 1.3.4. !
Example 1.3.1 (Example 1.1.1 Continued) Let us define three events as
follows:
How can we obtain the probabilities of these events? First notice that as sub-
sets of S, we can rewrite these events as A = {HHT, HTH, THH}, B = {HHT,
HTH, HTT, THH, THT, TTH, TTT}, and C = {TTT}. Now it becomes obvious
that P(A) = 3/8, P(B) = 7/8, and P(C) = 1/8. One can also see that A ∩ B =
{HHT, HTH, THH} so that P(A ∩ B) = 3/8, whereas A ∪ C = {HHT, HTH,
THH, TTT} so that P(A ∪ C) = 4/8 = 1/2.
Example 1.3.2 Example 1.1.2 Continued) Consider the following events:
Now, as subsets of the corresponding sample space S, we can rewrite these
events as D = {26, 35, 44, 53, 62} and E = {31, 42, 53, 64}. It is now obvious
that P(D) = 5/36 and P(E) = 4/36 = 1/9. !
8

1. Nations of Probability
Example 1.3.3 In a college campus, suppose that 2600 are women out of
4000 undergraduate students, while 800 are men among 2000 undergraduates
who are under the age 25. From this population of undergraduate students if
one student is selected at random, what is the probability that the student will
be either a man or be under the age 25? Define two events as follows
A:
the selected undergradute student is male
B:
the selected undergradute student is under the age 25
and observe that P(A) = 1400/4000, P(B) = 2000/4000, P(A ∩ B) = 800/4000.
Now, apply the Theorem 1.3.3, part (iv), to write P(A ∪ B) = P(A) + P(B) –
P(A ∩ B) = (1400 + 2000 – 800)/4000 = 13/20. !
Having a sample space S and appropriate events from a Borel
sigma-field ß of subsets of S, and a probability scheme satisfying
(1.3.1), one can evaluate the probability of the legitimate events
only. The members of ß are the only legitimate events.
1.4 The Conditional Probability and Independent
Events
Let us reconsider the Example 1.3.2. Suppose that the two fair dice, one red
and the other yellow, are tossed in another room. After the toss, the experi-
menter comes out to announce that the event D has been observed. Recall that
P(E) was 1/9 to begin with, but we know now that D has happened, and so the
probability of the event E should be appropriately updated. Now then, how
should one revise the probability of the event E, given the additional informa-
tion?
The basic idea is simple: when we are told that the event D has been ob-
served, then D should take over the role of the “sample space” while the origi-
nal sample space S should be irrelevant at this point. In order to evaluate the
probability of the event E in this situation, one should simply focus on the
portion of E which is inside the set D. This is the fundamental idea behind the
concept of conditioning.
Definition 1.4.1 Let S and ß be respectively the sample space and the
Borel sigma-field. Suppose that A, B are two arbitrary events. The condi-
tional probability of the event A given the other event B, denoted by P(A | B),
is defined as
9

1. Nations of Probability
In the same vein, we will write P(B | A) = P(A ∩ B) | P(A) provided that
P(A) > 0.
Definition 1.4.2 Two arbitrary events A and B are defined independent if
and only if P(A | B), that is having the additional knowledge that B has been
observed has not affected the probability of A, provided that P(B) > 0. Two
arbitrary events A and B are then defined dependent if and only if P(A | B) ≠
P(A), in other words knowing that B has been observed has affected the prob-
ability of A, provided that P(B) > 0.
In case the two events A and B are independent, intuitively it means that
the occurrence of the event A (or B) does not affect or influence the probabil-
ity of the occurrence of the other event B (or A). In other words, the occur-
rence of the event A (or B) yields no reason to alter the likelihood of the other
event B (or A).
When the two events A, B are dependent, sometimes we say that B is favor-
able to A if and only if P(A |  B) > P(A) provided that P(B) > 0. Also, when the
two events A, B are dependent, sometimes we say that B is unfavorable to A if
and only if P(A | B) < P(A) provided that P(B) > 0.
Example 1.4.1 (Example 1.3.2 Continued) Recall that P(D ∩ E) = P(53) =
1/36 and P(D) = 5/36, so that we have P(E | D) = P(D∩ E) | P(D) = 1/5. But,
P(E) = 1/9 which is different from P(E | D). In other words, we conclude that D
and E are two dependent events. Since P(E | D) > P(E), we may add that the
event D is favorable to the event E. !
The proof of the following theorem is left as the Exercise 1.4.1.
Theorem 1.4.1 The two events B and A are independent if and only if A
and B are independent. Also, the two events A and B are independent if and
only if P(A ∩ B) = P(A) P(B).
We now state and prove another interesting result.
Theorem 1.4.2 Suppose that A and B are two events. Then, the following
statements are equivalent:
(i)
The events A and B are independent;
(ii)
The events Ac and B are independent;
(iii) The events A and Bc are independent;
(iv) The events Ac and Bc are independent.
Proof It will suffice to show that part (i) ⇒ part (ii) ⇒ part (iii) ⇒ part (iv)
⇒ part (i).
(i) ⇒ (ii) : Assume that A and B are independent events. That is, in
view of the Theorem 1.4.1, we have P(A ∩ B) = P(A)P(B). Again in view
of the Theorem 1.4.1, we need to show that P(Ac∩B) = P(Ac)P(B). Now,
10

1. Nations of Probability
we apply parts (ii)-(iii) from the Theorem 1.3.1 to write
(ii) ⇒ (iii) ⇒ (iv) : These are left as the Exercise 1.4.2.
(iv) ⇒ (i) : Assume that Ac and Bc are independent events. That is, in view
of the Theorem 1.4.1, we have P(Ac ∩ Bc) = P(Ac)P(Bc). Again in view of the
Theorem 1.4.1, we need to show that P(A ∩ B) = P(A)P(B). Now, we combine
DeMorgan’s Law from (1.2.5) as well as the parts (ii)-(iv) from the Theorem
1.3.1 to write
which is the same as P(A)P(B), the desired claim. !
Definition 1.4.3 A collection of events A1, ..., An are called mutually inde-
pendent if and only if every sub-collection consists of independent events, that
is
for all 1 = i1 ≤  i2 < ... < ik ≤ n and 2 ≤ k ≤ n.
A collection of events A1, ..., An may be pairwise independent,
that is, any two events are independent according to
the Definition 1.4.2, but the whole collection of sets may not
be mutually independent. See the Example 1.4.2.
Example 1.4.2 Consider the random experiment of tossing a fair coin twice.
Let us define the following events:
A1:
Observe a head (H) on the first toss
A2:
Observe a head (H) on the second toss
A3:
Observe the same outcome on both tosses
The sample space is given by S = {HH, HT, TH, TT} with each outcome being
equally likely. Now, rewrite A1 = {HH, HT}, A2 = {HH, TH}, A3 = {HH, TT}.
Thus, we have P(A1) = P(A2) = P(A3) = 1/2. Now, P(A1 ∩ A2) = P(HH) = 1/4 =
P(A1)P(A2), that is the two events A1, A2 are independent. Similarly, one should
verify that A1, A3 are independent, and so are also A2, A3. But, observe that
P(A1 ∩ A2 ∩ A3) = P(HH) = 1/4 and it is not the same as P(A1)P(A2)P(A3). In
other words, the three events A1, A2, A3 are not mutually independent, but they
are pairwise independent. !
11

1. Nations of Probability
1.4.1
Calculus of Probability
Suppose that A and B are two arbitrary events. Here we summarize some of
the standard rules involving probabilities.
Additive Rule:
Conditional Probability Rule:
Multiplicative Rule:
The additive rule was proved in the Theorem 1.3.1, part (iv). The conditional
probability rule is a restatement of (1.4.1). The multiplicative rule follows
easily from (1.4.1). Sometimes the experimental setup itself may directly indi-
cate the values of the various conditional probabilities. In such situations, one
can obtain the probabilities of joint events such as A ∩ B by cross multiplying
the two sides in (1.4.1). At this point, let us look at some examples.
Example 1.4.3 Suppose that a company publishes two magazines, M1 and
M2. Based on their record of subscriptions in a suburb they find that sixty
percent of the households subscribe only for M1, forty five percent subscribe
only for M2, while only twenty percent subscribe for both M1 and M2. If a
household is picked at random from this suburb, the magazines’ publishers
would like to address the following questions: What is the probability that the
randomly selected household is a subscriber for (i) at least one of the maga-
zines M1, M2, (ii) none of those magazines M1, M2, (iii) magazine M2 given
that the same household subscribes for M1? Let us now define the two events
A: The randomly selected household subscribes for the magazine M1
B: The randomly selected household subscribes for the magazine M2
We have been told that P(A) = .60, P(B) = .45 and P(A ∩ B) = .20. Then,
P(A ∪ B) = P(A) + P(B) – P(A ∩ B) = .85, that is there is 85% chance that
the randomly selected household is a subscriber for at least one of the
magazines M1, M2. This answers question (i). In order to answer ques-
tion (ii), we need to evaluate P(Ac∩ Bc) which can simply be written as 1
– P(A ∪ B) = .15, that is there is 15% chance that the randomly selected
household subscribes for none of the magazines M1, M2. Next, to answer
12

1. Nations of Probability
question (iii), we obtain P(B | A) = P(A ∩ B)/P(A) = 1/3, that is there is one-
in-three chance that the randomly selected household subscribes for the maga-
zine M2 given that this household already receives the magazine M1. !
In the Example 1.4.3, P(A ∩ B) was given to us, and so we
could use (1.4.3) to find the conditional probability P(B | A).
Example 1.4.4 Suppose that we have an urn at our disposal which contains
eight green and twelve blue marbles, all of equal size and weight. The follow-
ing random experiment is now performed. The marbles inside the urn are mixed
and then one marble is picked from the urn at random, but we do not observe its
color. This first drawn marble is not returned to the urn. The remaining marbles
inside the urn are again mixed and one marble is picked at random. This kind of
selection process is often referred to as sampling without replacement. Now,
what is the probability that (i) both the first and second drawn marbles are
green, (ii) the second drawn marble is green? Let us define the two events
A: The randomly selected first marble is green
B: The randomly selected second marble is green
Obviously, P(A) = 8/20 = .4 and P (B |  A) = 7/19. Observe that the experimen-
tal setup itself dictates the value of P(B | A). A result such as (1.4.3) is not very
helpful in the present situation. In order to answer question (i), we proceed by
using (1.4.4) to evaluate P(A ∩ B) = P(A) P(B | A) = 8/20 7/19 = 14/95. Obvi-
ously, {A, Ac} forms a partition of the sample space. Now, in order to answer
question (ii), using the Theorem 1.3.1, part (vi), we write
But, as before we have P(Ac ∩ B) = P(AcP(B | Ac) = 12/20 8/19 = 24/95. Thus,
from (1.4.5), we have P(B) = 14/95 + 24/95 = 38/95 = 2/5 = .4. Here, note that
P(B | A) ≠ P(B) and so by the Definition 1.4.2, the two events A, B are depen-
dent. One may guess this fact easily from the layout of the experiment itself.
The reader should check that P(A) would be equal to P(B) whatever, be the
configuration of the urn. Refer to the Exercise 1.4.11. !
In the Example 1.4.4, P(B | A) was known to us, and so we
could use (1.4.4) to find the joint probability P(A ∩ B).
13

1. Nations of Probability
1.4.2
Bayes’s Theorem
We now address another type of situation highlighted by the following
example.
Example 1.4.5 Suppose in another room, an experimenter has two urns at
his disposal, urn #1 and urn #2. The urn #1 has eight green and twelve blue
marbles whereas the urn #2 has ten green and eight blue marbles, all of same
size and weight. The experimenter selects one of the urns at random with equal
probability and from the selected urn picks a marble at random. It is announced
that the selected marble in the other room turned out blue. What is the prob-
ability that the blue marble was chosen from the urn #2? We will answer this
question shortly. !
The following theorem will be helpful in answering questions such as the
one raised in the Example 1.4.5.
Theorem 1.4.3 (Bayes’s Theorem) Suppose that the events {A1, ..., Ak}
form a partition of the sample space S and B is another event. Then,
Proof Since {A1, ..., Ak} form a partition of S, in view of the Theorem 1.3.1,
part (vi) we can immediately write
by using (1.4.4). Next, using (1.4.4) once more, let us write
The result follows by combining (1.4.6) and (1.4.7). !
This marvelous result and the ideas originated from the works of Rev. Tho-
mas Bayes (1783). In the statement of the Theorem 1.4.3, note that the condi-
tioning events on the rhs are A1, ..., Ak, but on the Ihs one has the conditioning
event B instead. The quantities such as P(Ai), i = 1, ..., k are often referred to as
the apriori or prior probabilities, whereas P(Aj | B) is referred to as the poste-
rior probability. In Chapter 10, we will have more opportunities to elaborate
the related concepts.
Example 1.4.6 (Example 1.4.5 Continued) Define the events
Ai: The urn #i is selected, i = 1, 2
B: The marble picked from the selected urn is blue
14

1. Nations of Probability
It is clear that P(Ai) = 1/2 for i = 1, 2, whereas we have P(B | A1) = 12/20 and
P(B | A2) = 8/18. Now, applying the Bayes Theorem, we have
which simplifies to 20/47. Thus, the chance that the randomly drawn blue marble
came from the urn #2 was 20/47 which is equivalent to saying that the chance
of the blue marble coming from the urn #1 was 27/47.
The Bayes Theorem helps in finding the conditional
probabilities when the original conditioning events
A1, ..., Ak and the event B reverse their roles.
Example 1.4.7 This example has more practical flavor. Suppose that 40%
of the individuals in a population have some disease. The diagnosis of the pres-
ence or absence of this disease in an individual is reached by performing a type
of blood test. But, like many other clinical tests, this particular test is not per-
fect. The manufacturer of the blood-test-kit made the accompanying informa-
tion available to the clinics. If an individual has the disease, the test indicates
the absence (false negative) of the disease 10% of the time whereas if an indi-
vidual does not have the disease, the test indicates the presence (false positive)
of the disease 20% of the time. Now, from this population an individual is
selected at random and his blood is tested. The health professional is informed
that the test indicated the presence of the particular disease. What is the prob-
ability that this individual does indeed have the disease? Let us first formulate
the problem. Define the events
A1:
The individual has the disease
A1
c :
The individual does not have the disease
B:
The blood test indicates the presence of the disease
Suppose that we are given the following information: P (A1)  = .4, P (A1
c) = .6,
P(B | A1) = .9, and P(B | A1
c) = .2. We are asked to calculate the conditional
probability of A1 given B. We denote A2 = A1
c  and use the Bayes Theorem. We
have
which is 3/4. Thus there is 75% chance that the tested individual has the dis-
ease if we know that the blood test had indicated so. !
15

1. Nations of Probability
1.4.3
Selected Counting Rules
In many situations, the sample space S consists of only a finite number of
equally likely outcomes and the associated Borel sigma-field ß is the collection
of all possible subsets of S, including the empty set ϕ and the
whole set S. Then, in order to find the probability of an event A, it will be
important to enumerate all the possible outcomes included in the sample space
S and the event A. This section reviews briefly some of the customary count-
ing rules followed by a few examples.
The Fundamental Rule of Counting: Suppose that there are k different
tasks where the ith task can be completed in ni ways, i = 1, ..., k. Then, the total
number of ways these k tasks can be completed is given by 
.
Permutations: The word permutation refers to arrangements of some ob-
jects taken from a collection of distinct objects.
The order in which the objects are laid out is important here.
Suppose that we have n distinct objects. The number of ways we can arrange k
of these objects, denoted by the symbol nPk, is given by
The number of ways we can arrange all n objects is given by nPn which is
denoted by the special symbol
We describe the symbol n! as the “n factorial”. We use the convention to inter-
pret 0! = 1.
Combinations: The word combination refers to the selection of some ob-
jects from a set of distinct objects without regard to the order.
The order in which the objects are laid out is not important here.
Suppose that we have n distinct objects. The number of ways we can select k of
these objects, denoted by the symbol 
, is given by
But, observe that we can write n! = n(n – 1)...(n – k + 1)(n – k)! and hence we
can rewrite (1.4.10) as
16

1. Nations of Probability
We use the convention to interpret 
, whatever be the positive integer n.
The following result uses these combinatorial expressions and it goes by
the name
Theorem 1.4.4 (Binomial Theorem) For any two real numbers a, b and
a positive integer n, one has
Example 1.4.8 Suppose that a fair coin is tossed five times. Now the out-
comes would look like HHTHT, THTTH and so on. By the fundamental rule of
counting we realize that the sample space S will consist of 25(=2× 2 × 2 × 2 ×
2), that is thirty two, outcomes each of which has five components and these
are equally likely. How many of these five “dimensional” outcomes would in-
clude two heads? Imagine five distinct positions in a row and each position will
be filled by the letter H or T. Out of these five positions, choose two positions
and fill them both with the letter H while the remaining three positions are
filled with the letter T. This can be done in 
 ways, that is in (5)(4)/2 = 10
ways. In other words we have P(Two Heads) = 
 / 25 = 10/32 = 5/16. !
Example 1.4.9 There are ten students in a class. In how many ways can the
teacher form a committee of four students? In this selection process, naturally
the order of selection is not pertinent. A committee of four can be chosen in 
ways, that is in (10)(9)(8)(7)/4! = 210 ways. The sample space S would then
consist of 210 equally likely outcomes. !
Example 1.4.10 (Example 1.4.9 Continued) Suppose that there are six men
and four women in the small class. Then what is the probability that a ran-
domly selected committee of four students would consist of two men and two
women? Two men and two women can be chosen in 
  ways, that is in 90
ways. In other words, P(Two men and two women are selected) = 
 / 
 =
90/120 = 3/7. !
Example 1.4.11 John, Sue, Rob, Dan and Molly have gone to see a
movie. Inside the theatre, they picked a row where there were exactly five
empty seats next to each other. These five friends can sit in those chairs in
exactly 5! ways, that is in 120 ways. Here, seating arrangement is naturally
pertinent. The sample space S consists of 120 equally likely outcomes. But
the usher does not know in advance that John and Molly must sit next to
each other. If the usher lets the five friends take those seats randomly, what
is the probability that John and Molly would sit next to each other? John
and Molly may occupy the first two seats and other three friends may
permute in 3! ways in the remaining chairs. But then John and Molly may
17

1. Nations of Probability
occupy the second and third or the third and fourth or the fourth and fifth
seats while the other three friends would take the remaining three seats in any
order. One can also permute John and Molly in 2! ways. That is, we have
P(John and Molly sit next to each other) = (2!)(4)(3!)/5! = 2/5.!
Example 1.4.12 Consider distributing a standard pack of 52 cards to four
players (North, South, East and West) in a bridge game where each player gets
13 cards. Here again the ordering of the cards is not crucial to the game. The
total number of ways these 52 cards can be distributed among the four players
is then given by 
. Then, P(North will receive 4 aces and 4
kings) = 
 = 
,
because North is given 4 aces and 4 kings plus 5 other cards from the remaining
44 cards while the remaining 39 cards are distributed equally among South,
East and West. By the same token, P(South will receive exactly one ace) =
 = 
, since South would re-
ceive one out of the 4 aces and 12 other cards from 48 non-ace cards while the
remaining 39 cards are distributed equally among North, East and West. !
1.5 Discrete Random Variables
A discrete random variable, commonly denoted by X, Y and so on, takes a
finite or countably infinite number of possible values with specific probabili-
ties associated with each value. In a collection agency, for example, the man-
ager may look at the pattern of the number (X) of delinquent accounts. In a
packaging plant, for example, one may be interested in studying the pattern of
the number (X) of the defective packages. In a truckload of shipment, for ex-
ample, the receiving agent may want to study the pattern of the number (X) of
rotten oranges or the number (X) of times the shipment arrives late. One may
ask: how many times (X) one must drill in a oil field in order to hit oil? These
are some typical examples of discrete random variables.
In order to illustrate further, let us go back to the Example 1.1.2. Sup-
pose that X is the total score from the red and yellow dice. The possible
values of X would be any number from the set consisting of 2, 3, ..., 11, 12.
We had already discussed earlier how one could proceed to evaluate P(X =
x), x = 2, ..., 12. In this case, the event [X = 2] corresponds to the subset
{11}, the event [X = 3] corresponds to the subset {12, 21}, the event [X =
4] corresponds to the subset {13, 22, 31}, and so on. Thus, P(X = 2) = 1/36,
P(X = 3) = 2/36, P(X = 4) = 3/36, and so on. The reader should
18

1. Notions of Probability
19
easily verify the following entries:
Here, the set of the possible values for the random variable X happens to be
finite.
On the other hand, when tossing a fair coin, let Y be the number of tosses of
the coin required to observe the first head (H) to come up. Then, P(Y = 1) =
P(The H appears in the first toss itself) = P(H) = 1/2, and P(Y = 2) = P(The first
H appears in the second toss) = P(TH) =
 P(TTH) = 1/8, ..., that is
Here, the set of the possible values for the random variable Y is countably
infinite.
1.5.1
Probability Mass and Distribution Functions
In general, a random variable X is a mapping (that is, a function) from the
sample space S to a subset χ of the real line ℜ which amounts to saying
that the random variable X induces events (∈ ß) in the context of S. We may
express this by writing X : S → χ. In the discrete case, suppose that X takes
the possible values x1, x2, x3, ... with the respective probabilities pi = P(X = xi),
i = 1, 2, ... . Mathematically, we evaluate P(X = xi) as follows:
In (1.5.1), we found P(X = i) for i = 2, 3, 4 by following this approach
whereas the space χ ={2, 3, ..., 12} and S = {11, ..., 16, 21, ..., 61, ..., 66}.
While assigning or evaluating these probabilities, one has to make sure that
the following two conditions are satisfied:
When both these conditions are met, we call an assignment such as
1
2
(
(
1
2
(
(
1
4
(
(
=
Similarly, P(Y = 3) =

20
1. Notions of Probability
a discrete distribution or a discrete probability distribution of the random
variable X.
The function f(x) = P(X = x) for x ∈ χ = {x1, x2, ..., xi, ...} is
customarily known as the probability mass function (pmf) of X.
We also define another useful function associated with a random variable
X as follows:
which is customarily called the distribution function (df) or the cumulative
distribution function (cdf) of X. Sometimes we may instead write FX(x) for the
df of the random variable X.
A distribution function (df) or cumulative distribution function (cdf)
F(x) for the random variable X is defined for all real numbers x
Once the pmf f(x) is given as in case of (1.5.4), one can find the probabili-
ties of events which are defined through the random variable X. If we denote a
set A(⊆ ℜ;), then
Example 1.5.1 Suppose that X is a discrete random variable having the
following probability distribution:
The associated df is then given by
This df looks like the step function in the Figure 1.5.1. From the Figure
1.5.1, it becomes clear that the jump in the value of F(x) at the points
x = 1, 2, 4 respectively amounts to .2, .4 and .4. These jumps obviously
correspond to the assigned values of P(X = 1), P(X = 2) and P(X = 4).
Also, the df is non-decreasing in x and it is discontinuous at the points

1. Notions of Probability
21
x = 1, 2, 4, namely at those points where the probability distribution laid out in
(1.5.7) puts any positive mass. !
Figure 1.5.1. Plot of the DF F(x) from (1.5.8)
Example 1.5.2 For the random variable X whose pmf is given by (1.5.1), the
df is easily verified to be as follows:
x
F(x)
x
F(x)
–∞ < x < 2
0
7 ≤  x < 8
21/36
2 ≤ x < 3
1/36
8 ≤ x < 9
26/36
3 ≤ x < 4
3/36
9 ≤ x < 10
30/36
4 ≤ x < 5
6/36
10 ≤ x < 11 33/36
5 ≤ x < 6
10/36
11 ≤ x < 12 35/36
6 ≤ x < 7
15/36
12 ≤ x < ∞
1
From this display, again few facts about the df F(x) become clear. The df is
non-decreasing in x and it is discontinuous at the points x = 2, 3, ..., 12,
namely at those points where the probability distribution laid out in (1.5.1)
puts any positive mass. Also, the amount of jump of the df F(x) at the point x
= 2 equals 
 which corresponds to P(X = 2), and the amount of jump of F(x)
at the point x = 3 equals 
 which corresponds to P(X = 3), and so
on. !
In the two preceding examples, we gave tips on how the pmf could be
constructed from the expression of the df F(x). Suppose that we write

22
1. Notions of Probability
This F(x–) is the limit of F(h) as h converges to x from the left hand side of
x. In the Example 1.5.1, one can easily verify that F(1–) = 0, F(2–) = .2 and
F(4–) = .6. In other words, in this example, the jump of the df or the cdf F(x)
at the point
There is this natural correspondence between the jumps of a df, the left-
limit of a df, and the associated pmf. A similar analysis in the case of the Ex-
ample 1.5.2 is left out as the Exercise 1.5.4.
For a discrete random variable X, one can obtain P(X = x) as
F(x) – F(x–) where the left-limit F(x–) comes from (1.5.9).
Example 1.5.3 (Example 1.5.2 Continued) For the random variable X whose
pmf is given by (1.5.1), the probability that the total from the two dice will be
smaller than 4 or larger than 9 can be found as follows. Let us denote an event
A = {X < 4 ∪ X > 9} and we exploit (1.5.6) to write P(A) = P(X > 4) + P(X >
9) = P(X = 2, 3) + P(X = 10, 11, 12) = 9/36 = 1/4. What is the probability that
the total from the two dice will differ from 8 by at least 2? Let us denote an
event B = {|X – 8| ≥ 2} and we again exploit (1.5.6) to write P(B) = P(X ≤ 6) +
P(X ≥ 10) = P(X = 2, 3, 4, 5, 6) + P(X = 10, 11, 12) = 21/36 = 7/12.  !
Example 1.5.4 (Example 1.5.3 Continued) Consider two events A, B de-
fined in the Example 1.5.3. One can obtain P(A), P(B) alternatively by using
the expression of the df F(x) given in the Example 1.5.2. Now, P(A) = P(X < 4)
+ P(X > 9) = F(3) + {1 – F(9)} = 3/36 + {1 – 30/36} = 9/36 = 1/4. Also, P(B)
= P(X ≤ 6) + P(X ≥ 10) = F(6) + {1 – F(9)} = 15/36 + {1 – 30/36} = 21/36 =
7/12. !
Next, let us summarize some of the important properties of a distribution
function defined by (1.5.5) associated with an arbitrary discrete random vari-
able.
Theorem 1.5.1 Consider the df F(x) with x ∈ ℜ, defined by (1.5.5), for a
discrete random variable X. Then, one has the following properties:
(i) F(x) is non-decreasing, that is F(x) ≤ F(y) for all x ≤ y where x, y ∈
ℜ;
(ii)
;
(iii) F(x) is right continuous, that is F(x + h) ↓ F(x) as h ↓ 0, for all x ∈ ℜ.

1. Notions of Probability
23
This is a simple result to prove. We leave its proof as Exercise 1.5.6.
It is true that any function F(x), x ∈ ℜ, which satisfies the
properties (i)-(iii) stated in the Theorem 1.5.1 is in fact a df
of some uniquely determined random variable X.
1.6 Continuous Random Variables
A continuous random variable, commonly denoted by X, Y and so on, takes
values only within subintervals of ℜ or within subsets generated by some ap-
propriate subintervals of the real line ℜ. At a particular location in a lake, for
example, the manager of the local parks and recreation department may be
interested in studying the pattern of the depth (X) of the water level. In a high-
rise office building, for example, one may be interested to investigate the pat-
tern of the waiting time (X) for an elevator on any of its floors. At the time of the
annual music festival in an amusement park, for example, the resident manager
at a nearby senior center may want to study the pattern of the loudness factor
(X). These are some typical examples of continuous random variables.
1.6.1
Probability Density and Distribution Functions
We assume that the sample space S itself is a subinterval of ℜ. Now, a con-
tinuous real valued random variable X is a mapping of S into the real line ℜ. In
this scenario, we no longer talk about P(X = x) because this probability will be
zero regardless of what specific value x ∈ ℜ one has in mind. For example,
the waiting time (X) at a bus stop would often be postulated as a continuous
random variable, and thus the probability of one’s waiting exactly five minutes
or exactly seven minutes at that stop for the next bus to arrive is simply zero.
In order to facilitate modeling a continuous stochastic situation, we start
with a function f(x) associated with each value x ∈ ℜ satisfying the following
two properties:
Observe that the interpretations of (1.5.3) and (1.6.1) are indeed very simi-
lar. The conditions listed in (1.6.1) are simply the continuous analogs of
those in (1.5.3). Any function f(x) satisfying (1.6.1) is called a probability

24
1. Notions of Probability
density function (pdf).
A probability mass function (pmf) or probability density function
(pdf) f(x) is respectively defined through (1.5.3) and (1.6.1).
Once the pdf f(x) is specified, we can find the probabilities of various
events defined in terms of the random variable X. If we denote a set A( ⊆ ℜ),
then
where the convention is that we would integrate the function f(x) only on that
part of the set A wherever f(x) is positive.
In other words, P(X ∈ A) is given by the area under the curve {(x, f(x)); for
all x ∈ A wherever f(x) > 0}. In the Figure 1.6.1, we let the set A be the interval
(a, b) and the shaded area represents the corresponding probability, P(a < X <
b).
Figure 1.6.1. Shaded Area Under the PDF f(x) Is P(a < X < b)
We define the distribution function (df) of a continuous random variable X
by modifying the discrete analog from (1.5.5). We let
which also goes by the name, cumulative distribution function (cdf). Again,
note that F(x) is defined for all real values x. As before, sometimes we also
write FX(x) for the df of the random variable X.
Now, we state a rather important characteristic of a df without supplying
its proof. The result is well-known. One will find its proof in Rao (1973, p.
85), among other places.

1. Notions of Probability
25
Theorem 1.6.1 Suppose that F(x), x ∈ ℜ, is the df of an arbitrary random
variable X. Then, the set of points of discontinuity of the distribution function
F(x) is finite or at the most countably infinite.
Example 1.6.1 Consider the following discrete random variable X first:
In this case, the df FX(x) is discontinuous at the finite number of points x = –
1, 2 and 5. !
Example 1.6.2 Look at the next random variable Y. Suppose that
In this case, the corresponding df FY(y) is discontinuous at the countably
infinite number of points y = 1, 2, 3, ... .
Example 1.6.3 Suppose that a random variable U has an associated non-
negative function f(u) given by
Observe that 
 and thus f(u) happens to
be the distribution of U. This random variable U is neither discrete nor con-
tinuous. The reader should check that its df FU(u) is discontinuous at the count-
ably infinite number of points u = 1, 2, 3, ... . !
If a random variable X is continuous, then its df F(x) turns out to be continu-
ous at every point x ∈ ℜ. However, we do not mean to imply that the df F(x)
will necessarily be differentiable at all the points. The df F(x) may not be differ-
entiable at a number of points. Consider the next two examples.
Example 1.6.4 Suppose that we consider a continuous random variable W
with its pdf given by
In this case, the associated df FW(w) is given by

26
1. Notions of Probability
The expression for FW(w) is clear for w ≤ 1 since in this case we inte-
grate the zero density. The expression for FW(w) is also clear for w ≥ 2
because the associated integral can be written as 
  For 1 < w < 2, the expression for FW(w can be
written as 
. It should
be obvious that the df FW(w) is continuous at all points
w ∈ ℜ but FW(w) is not differentiable specifically at the two points w = 1, 2. In
this sense, the df FW(w) may not be considered very smooth. !
The pmf in (1.6.6) gives an example of a discrete random variable
whose df has countably infinite number of discontinuity points. The
pdf in (1.6.9) gives an example of a continuous random variable
whose df is not differentiable at countably infinite number of points.
Example 1.6.5 Consider the function f(x) defined as follows:
We leave it as the Exercise 1.6.4 to verify that (i) f(x) is a genuine pdf, (ii) the
associated df F(x) is continuous at all points x ∈ ℜ, and (iii) F(x) is not differ-
entiable at the points x belonging to the set 
 which is countably
infinite. In the Example 1.6.2, we had worked with the point masses at count-
ably infinite number of points. But, note that the present example is little differ-
ent in its construction. !
In each case, observe that the set of the discontinuity points of the associ-
ated df F(.) is at most countably infinite. This is exactly what one will expect in
view of the Theorem 1.6.1. Next, we proceed to calculate probabilities of events.
The probability of an event or a set A is given by the area
under the pdf f(x) wherever f(x) is positive, x ∈ A.
Example 1.6.6 (Example 1.6.4 Continued) For the continuous dis-
tribution defined by (1.6.7), suppose that the set A stands for the interval
(–1.5, 1.8). Then, P(A) = 
 =
. Alternately, we may also apply the form of the df given by
(1.6.8) to evaluate the probability, P(A) as follows: P(A) = P(1 < W < 1.8) =
FW(1.8) – FW(1) = 1/7{(1.8)3 – 1} – 0 = 
. !

1. Notions of Probability
27
At all points x ∈ ℜ wherever the df F(x) is differentiable, one must have
the following result:
This is a simple restatement of the fundamental theorem of integral calculus.
Example 1.6.7 (Example 1.6.4 Continued) Consider the df FW(w) of the
random variable W from (1.6.8). Now, FW(w) is not differentiable at the two
points w = 1, 2. Except at the points w = 1, 2, the pdf f(w) of the random
variable W can be obtained from (1.6.8) as follows: f(w) = d/dw FW(w) which
will coincide with zero when –∞ < w < 1 or 2 < w < ∞, whereas for 1 < w < 2
we would have d{1/7(w3 – 1)}/dw = 3/7w2. This agrees with the pdf given by
(1.6.7) except at the points w = 1, 2. !
For a continuous random variable X with its pdf f(x and
its df F(x), x ∈ ℜ, one has: (i) P(X < a) or P(X ≤ a) is
given by F(a) = 
, (ii) P(X > b) or P(X ≥ b) is
given by 1-F(b) = 
, and (iii) P(a < X < b) or
P(a ≤ X < b) or P (a < X ≤ b) or P(a ≤ X ≤ b) is given
by F(b) - F(a) = 
. The general understanding,
however, is that these integrals are carried out within the
appropriate intervals wherever the pdf f(x) is positive.
In the case of the Example 1.6.7, notice that the equation (1.6.10) does not
quite lead to any specific expression for f(w) at w = 1, 2, the points where FW(w)
happens to be non-differentiable.. So, must f(w) be defined at w = 1, 2 in exactly
the same way as in (1.6.7)?
Let us write I(.) for the indicator function of (.). Since we only handle
integrals when evaluating probabilities, without any loss of generality, the pdf
given by (1.6.7) is considered equivalent to any of the pdf’s such as 3/7w2I(1
≤ w < 2) or 3/7w2I(1 < w ≤ 2) or 3/7w2I(1 ≤ w ≤ 2). If we replace the pdf f(w)
from (1.6.7) by any of these other pdf’s, there will be no substantive changes
in the probability calculations.
From this point onward, when we define the various pieces of the
pdf f(x) for a continuous random variable X, we will not attach
any importance on the locations of the equality signs placed to
identify the boundaries of the pieces in the domain of the variable x.

28
1. Notions of Probability
1.6.2
The Median of a Distribution
Often a pdf has many interesting characteristics. One interesting characteris-
tic is the position of the median of the distribution. In statistics, when com-
paring two income distributions, for example, one may simply look at the
median incomes from these two distributions and compare them. Consider a
continuous random variable X whose df is given by F(x). We say that xm is the
median of the distribution if and only if F(xm) = 1/2, that is P(X ≤ xm) = P(X ≥
xm) = 1/2. In other words, the median of a distribution is that value xm of X
such that 50% of the possible values of X are below xm and 50% of the
possible values of X are above xm.
Example 1.6.8 (Example 1.6.4 Continued) Reconsider the pdf f(w) from
(1.6.7) and the df F(w) from (1.6.8) for the random variable W. The median wm
of this distribution would be the solution of the equation F(wm) = 1/2. In view
of (1.6.8) we can solve this equation exactly to obtain um = (4.5)1/3 ≈ 1.651. !
1.6.3
Selected Reviews from Mathematics
A review of some useful results from both the differential and integral calculus
as well as algebra and related areas are provided here. These are not laid out in
the order of importance. Elaborate discussions and proofs are not included.
Finite Sums of Powers of Positive Integers:
Infinite Sums of Reciprocal Powers of Positive Integers: Let us write
Then, ζ (p) = ∞ if p ≤ 1, but ζ(p) is finite if p > 1. It is known that ζ(2) =
1/6π2, ζ(4) = 1/90π4, and ζ(3) ≈ 1.2020569. Refer to Abramowitz and Stegun
(1972, pp. 807-811) for related tables.

1. Notions of Probability
29
Results on Limits:
The Big O(.) and o(.) Terms: Consider two terms an and bn, both are real
valued but depend on n = 1, 2, ... . The term an is called O(bn) provided that an/
bn → c, a constant, as n → ∞. The term an is called o(bn) provided that an/bn →
0 as n → ∞. Let an = n – 1, bn=2n+ n and dn = 5n8/7 + n, n = 1, 2, ... . Then,
one has, for example, an = O(bn), an = o(dn), bn = o(dn). Also, one may write,
for example, an = O(n), bn = O(n), dn = O(n8/7), dn = o(n2), dn = o(n9/7).
Taylor Expansion: Let f(.) be a real valued function having the finite nth
derivative dnf(x)/dxn , denoted by f(n)(.), everywhere in an open interval (a, b)
⊆ ℜand assume that f(n–1)(.) is continuous on the closed interval [a, b]. Let c
∈ [a, b]. Then, for every x ∈ [a, b], x ≠ c, there exists a real number ξ
between the two numbers x and c such that
Some Infinite Series Expansions:
Differentiation Under Integration (Leibnitz’s Rule): Suppose that
f(x, θ), a(θ), and b(θ) are differentiable functions with respect to θ for x ∈
ℜ, θ ∈ ℜ. Then,

30
1. Notions of Probability
Differentiation Under Integration: Suppose that f(x, θ) is a differen-
tiable function in θ, for x ∈ ℜ, θ ∈ ℜ. Let there be another function g(x, θ)
such that (i) |∂f(x, θ)/∂θ|θ=θ0 | ≤ g(x, θ) for all θ0 belonging to some interval
(θ – ε, θ + ε), and (ii)  
. Then
Monotone Function of a Single Real Variable: Suppose that f(x) is a
real valued function of x ∈ (a, b) ⊆ ℜ. Let us assume that d f(x)/dx  exists at
each x ∈ (a, b) and that f(x) is continuous at x = a, b. Then,
Gamma Function and Gamma Integral: The expression Γ(α), known
as the gamma function evaluated at α, is defined as
The representation given in the rhs of (1.6.19) is referred to as the gamma
integral. The gamma function has many interesting properties including the
following:
Stirling’s Approximation: From equation (1.6.19) recall that Γ(α) =
 with α > 0. Then,
Writing α = n + 1 where n is a positive integer, one can immediately claim
that
The approximation for n! given by (1.6.22) works particularly well even for n
as small as five or six. The derivation of (1.6.22) from (1.6.21) is left as the
Exercise 1.6.15.

1. Notions of Probability
31
Ratios of Gamma Functions and Other Approximations: Recall that
. Then , one has
with fixed numbers c(> 0) and d, assuming that the gamma function itself is
defined. Also, one has
with fixed numbers c and d, assuming that the gamma functions involved are
defined.
Other interesting and sharper approximations for expressions involving the
gamma functions and factorials can be found in the Section 6.1 in Abramowitz
and Stegun (1972).
Beta Function and Beta Integral: The expression b(α, β), known as the
beta function evaluated at α and β in that order, is defined as
The representation given in the rhs of (1.6.25) is referred to as the beta inte-
gral. Recall the expression of Γ(α) from (1.6.19). We mention that b(α, β) can
alternatively 
be 
expressed 
as 
follows:
Maximum and Minimum of a Function of a Single Real Variable: For
some integer n ≥ 1, suppose that f(x) is a real valued function of a single real
variable x ∈ (a, b) ⊆ R, having a continuous nth derivative 
, denoted
by f(n) (x), everywhere in the open interval (a, b). Suppose also that
for some point ξ ∈ (a, b), one has f(1) (ξ) = f(2) (ξ) = ... = f(n–
1) (ξ) = 0, but f(n) (ξ) ≠ 0. Then,
Maximum and Minimum of a Function of Two Real Variables:
Suppose that f(x) is a real valued function of a two-dimensional variable

32
1. Notions of Probability
x = (x1, x2) ∈ (a1, b1) × (a2, b2) ⊆ ℜ2. The process of finding where this
function f(x) attains its maximum or minimum requires knowledge of matri-
ces and vectors. We briefly review some notions involving matrices and vec-
tors in the Section 4.8. Hence, we defer to state this particular result from
calculus in the Section 4.8. One should refer to (4.8.11)-(4.8.12) regarding this.
Integration by Parts: Consider two real valued functions f(x), g(x) where
x ∈ (a, b), an open subinterval of ℜ. Let us denote d/dx f(x) by f (x) and the
indefinite integral ∫ g(x)dx by h(x). Then,
assuming that all the integrals and f’ (x) are finite.
L’Hôpital’s Rule: Suppose that f(x) and g(x) are two differentiable real
valued functions of x ∈ ℜ. Let us assume that 
 and 
where a is a fixed real number, –∞ or +∞. Then,
where f’ (x) = df(x)/dx, g’ (x) = dg(x)/dx.
Triangular Inequality: For any two real numbers a and b, the following
holds:
From the triangular inequality it also follows that
One may use (1.6.30) and mathematical induction to immediately write:
where a1, a2, ..., ak are real numbers and k = 2.
1.7 Some Standard Probability Distributions
In this section we list a number of useful distributions. Some of these distri-
butions will appear repeatedly throughout this book.
As a convention, we often write down the pmf or the pdf f(x)
only for those x ∈ χ where f(x) is positive.

1. Notions of Probability
33
1.7.1
Discrete Distributions
In this subsection, we include some standard discrete distributions. A few of
these appear repeatedly throughout the text.
The Bernoulli Distribution: This is perhaps one of the simplest possible
discrete random variables. We say that a random variable X has the Bernoulli
(p) distribution if and only if its pmf is given by
where 0 < p < 1. Here, p is often referred to as a parameter. In applications, one
may collect dichotomous data, for example simply record whether an item is
defective (x = 0) or non-defective (x = 1), whether an individual is married (x =
0) or unmarried (x = 1), or whether a vaccine works (x = 1) or does not work (x
= 0), and so on. In each situation, p stands for P(X = 1) and 1 – p stands for P(X
= 0).
The Binomial Distribution: We say that a discrete random variable X has
the Binomial(n, p) distribution if and only if its pmf is given by
where 0 < p < 1. Here again p is referred to as a parameter. Observe that the
Bernoulli (p) distribution is same as the Binomial(1, p) distribution.
The Binomial(n, p) distribution arises as follows. Consider repeating the
Bernoulli experiment independently n times where each time one observes the
outcome (0 or 1) where p = P(X = 1) remains the same throughout. Let us
obtain the expression for P(X = x). Consider n distinct positions in a row
where each position will be filled by the number 1 or 0. We want to find the
probability of observing x many 1’s, and hence additionally exactly (n – x)
many 0’s. The probability of any such particular sequence, for example the
first x many 1’s followed by (n – x) many 0’s or the first 0 followed by x
many 1’s and then (n – x – 1) many 0’s would each be px(1 – p)n–x. But, then
there are 
 ways to fill the x positions each with the number 1 and n – x
positions each with the number 0, out of the total n positions. This verifies the
form of the pmf in (1.7.2).
Recall the requirement in the part (ii) in (1.5.3) which demands that all the
probabilities given by (1.7.2) must add up to one. In order to verify this directly,
let us proceed as follows. We simply use the binomial expansion to write

34
1. Notions of Probability
Refer to the Binomial Theorem from (1.4.12). Next let us look at some ex-
amples.
Example 1.7.1 In a short multiple choice quiz, suppose that there are
ten unrelated questions, each with five suggested choices as the possible
answer. Each question has exactly one correct answer given. An unpre-
pared student guessed all the answers in that quiz. Suppose that each cor-
rect (wrong) answer to a question carries one (zero) point. Let X stand for
the student’s quiz score. We can postulate that X has the Binomial(n = 10,
p = 1/5) distribution. Then, 
Also, 
. In other words, the student may earn
few points by using a strategy of plain guessing, but it will be hard to earn B or
better in this quiz. !
Example 1.7.2 A study on occupational outlook reported that 5% of all
plumbers employed in the industry are women. In a random sample of 12
plumbers, what is the probability that at most two are women? Since we
are interested in counting the number of women among twelve plumbers,
let us use the code one (zero) for a woman (man), and let X be the number
of women in a random sample of twelve plumbers. We may assume that X
has the Binomial(n = 12, p = .05) distribution. Now, the probability that at
most two are women is the same as 
 . !
The Poisson Distribution: We say that a discrete random variable X has
the Poisson(λ) distribution if and only if its pmf is given by
where 0 < λ < ∞;. Here, λ is referred to as a parameter.
Recall the requirement in the part (ii) in (1.5.3) which demands that all the
probabilities given by (1.7.4) must add up to one. In order to verify this directly,
let us proceed as follows. We simply use the infinite series expansion of ex
from (1.6.15) to write
The Poisson distribution may arise in the following fashion. Let us
reconsider the binomial distribution defined by (1.7.2) but pretend that we
have a situation like this: we make n → ∞ and p → 0 in such a way that np
remains a constant, say, λ(> 0). Now then, we can rewrite the binomial

1. Notions of Probability
35
probability for any fixed x = 0, 1, 2, ... ,
Observe that (1 – k/n) → 1 as n → ∞ for all fixed k = 1, 2, ..., x – 1, λ. Also,
(1 – λ/n)n → e–λ  as n → ∞. See (1.6.13) as needed. Now, from (1.7.6) we
can conclude that 
.
Next let us look at some examples.
Example 1.7.3 In a certain manufacturing industry, we are told that minor
accidents tend to occur independently of one another and they occur at a
constant rate of three (= λ) per week. A Poisson(λ = 3) is assumed to ad-
equately model the number of minor accidents occurring during a given week.
Then, the probability that no minor accidents will occur during a week is
given by P(X = 0) = e–3 ≈ 4.9787 × 10–2. Also, the probability that more than
two minor accidents will occur during a week is given by P(X > 2) = 1 – P(X
≤ 2) = 1 – {e–330/0!) + (e–33/1!) = + (e–332/2!)} = 1 – 8.5e–3 ≈ .57681 !
Example 1.7.4 We are inspecting a particular brand of concrete slab speci-
mens for any visible cracks. Suppose that the number (X) of cracks per concrete
slab specimen has approximately a Poisson distribution with λ =
2.5. What is the probability that a randomly selected
slab will have at least two cracks? We wish to evaluate
P(X ≥ 2) which is the same as 1 – P(X ≤ 1) = 1 – [{e–2.5 (2.5)0/0!} + {e–2.5
(2.5)1/1!}] = 1 – (3.5)e–2.5 ≈ .7127. !
The Geometric Distribution: A discrete random variable X is said to
have the Geometric(p) distribution if and only if its pmf is given by
where 0 < p < 1. Here, p is referred to as a parameter.
The Geometric(p) distribution arises as follows. Consider repeating the
Bernoulli experiment independently until we observe the value x = 1 for the
first time. In other words, let X be number of trials needed for the indepen-
dent run of Bernoulli data to produce the value 1 for the first time. Then, we
have P(X = x) = P(Observing x – 1 many 0’s followed a single occurrence of
1 in the xth trial) = p(1 – p)x–1, x = 1, 2, ... .

36
1. Notions of Probability
Recall the requirement in the part (ii) in (1.5.3) which demands that all the
probabilities given by (1.7.7) must add up to one. In order to verify this
directly, let us proceed as follows. We denote q = 1 – p and simply use the
expression for the sum of an infinite geometric series from (1.6.15), with m =
1, to write
The expression of the df for the Geometric (p) random variable is also quite
straight forward. One can easily verify that
Example 1.7.5 Some geological exploration may indicate that a well drilled
for oil in a region in Texas would strike oil with probability .3. Assuming that
the oil strikes are independent from one drill to another, what is the probability
that the first oil strike will occur on the sixth drill? Let X be the number of drills
until the first oil strike occurs. Then, X is distributed as Geometric (p = .3) so
that one has P(X = 6) = (.3)(.7)5 ≈ 5.0421 × 10–2. !
Example 1.7.6 An urn contains six blue and four red marbles of identical
size and weight. We reach in to draw a marble at random, and if it is red, we
throw it back in the urn. Next, we reach in again to draw another marble at
random, and if it is red then it is thrown back in the urn. Then, we reach in for
the third draw and the process continues until we draw the first blue marble.
Let X be total number of required draws. Then, this random variable X has the
Geometric(p = .6) distribution. What is the probability that we will need to
draw marbles fewer than four times? Using the expression of the df F(x)
from (1.7.8) we immediately obtain P(X < 4) = F(3) = 1 – (.4)3 = .936.!
The kind of sampling inspection referred to in the Example 1.7.6
falls under what is called sampling with replacement.
The Negative Binomial Distribution: A discrete random variable X is
said to have the negative binomial distribution with µ and k, customarily de-
noted by NB(µ, k), if and only if its pmf is given by
where 0 < μ, k < ∞. Here, µ and k are referred to as parameters. The
parameterization given in (1.7.9) is due to Anscombe (1949). This form of

1. Notions of Probability
37
the pmf is widely used in the areas such as entomology, plant science,
and soil science. The parameters μ and k have physical interpretations in
many applications in these areas.
Denoting p = k/(µ + k), q = µ/(µ + k), the pmf given by (1.7.9) can be
rewritten in a more traditional way as follows:
where 0 < p < 1, q = 1 – p and k is a positive integer. This form of the
negative binomial distribution arises as follows. Suppose that we have the
same basic setup as in the case of a geometric distribution, but instead we let
X be the number of 0’s observed before the kth occurrence of 1. Then, we
have P(X = 0) = P(Observing k many 1’s right away) = pk. Next, P(X =
P(The last trial yields 1, but we observe k – 1 many 1’s and a single 0 before
the occurrence of the 1 in the last trial) = 
  
 .  Also,
P(X = 2) = P(The last trial yields 1, but we observe k – 1 many 1’s and two
0’s before the occurrence of the 1 in the last trial) =
. Analogous arguments will eventually justify
(1.7.10) in general.
Example 1.7.7 (Example 1.7.5 Continued) Some geological exploration
indicates that a well drilled for oil in a region in Texas may strike oil with
probability .3. Assuming that the oil strikes are independent from one drill to
another, what is the probability that the third oil strike will occur on the tenth
well drilled? Let X be the number of drilled wells until the third oil strike
occurs. Then, using (1.7.10) we immediately get P(X = 10) = (.7)7(.3)3 ≈
8.0048 × 10–2. !
The Discrete Uniform Distribution: Let X be a discrete random variable
which takes the only possible values x1, ..., xk each with the same probability
1/k. Such X is said to have a discrete uniform distribution. We may write
down its pmf as follows.
Example 1.7.8 Suppose that we roll a fair die once and let X be the num-
ber of dots on the face of the die which lands up. Then, obviously f(x) = 1/6
for x = 1, ..., 6 which corresponds to the pmf given by (1.7.11) with k = 6
and x1 = 1, x2 = 2, ..., x6. !
1.7.2
Continuous Distributions
In this subsection we include some standard continuous distributions. A few
of these appear repeatedly throughtout the text.

38
1. Notions of Probability
The Uniform Distribution: A continuous random variable X has the uni-
form distribution on the interval (a, b), denoted by Uniform (a, b), if and only if
its pdf is given by
where –∞ < a, b < ∞. Here, a, b are referred to as parameters.
Figure 1.7.1. Uniform (0, 1) Density
Let us ask ourselves: How can one directly check that f(x) given by (1.7.12)
is indeed a pdf? The function f(x) is obviously non-negative for all x ∈ ℜ.
Next, we need to verify directly that the total integral is one. Let
us write 
 since b ≠ a. In other words, (1.7.12) defines a genuine
pdf. Since this pdf puts equal weight at each point x ∈ (a, b), it is called the
Uniform (a, b) distribution. The pdf given by (1.7.12) when a = 0, b = 1 has
been plotted in the Figure 1.7.1.
Example 1.7.9 The waiting time X at a bus stop, measured in minutes,
may be uniformly distributed between zero and five. What is the probability
that someone at that bus stop would wait more than 3.8 minutes for the bus?
We have   
.24.!
The Normal Distribution: A continuous random variable X has the nor-
mal distribution with the parameters µ and σ2, denoted by N(µ, σ2), if and only
if its pdf is given by
where –∞ < µ < ∞ and 0 < σ < ∞. Among all the continuous distributions, the
normal distribution is perhaps the one which is most widely used in modeling
data.

1. Notions of Probability
39
C. F. Gauss, the celebrated German mathematician of the eighteenth cen-
tury, had discovered this distribution while analyzing the measurement errors
in astronomy. Hence, the normal distribution is alternatively called a Gaussian
distribution.
Figure 1.7.2. Normal Densities: (a) N(0,.25) (b) N(1,.25)
Let us ask ourselves: How can one directly check that f(x) given by (1.7.13)
is indeed a pdf? The function f(x) is obviously positive for all x ∈ ℜ. Next, we
need to verify directly that
Recall the gamma function Γ(.) defined by (1.6.19). Let us substitute u = (x –
µ)/σ, v = –u, w = 1/2v2 successively, and rewrite the integral from (1.7.14) as
In the last step in (1.7.15) since the two integrals are the same, we can claim
that

40
1. Notions of Probability
This proves (1.7.14). An alternative way to prove the same result using the
polar coordinates has been indicated in the Exercise 1.7.20.
The pdf given by (1.7.13) is symmetric around x = µ, that is we have f(x
– µ) = f(x + µ) for all fixed x ∈ ℜ. In other words, once the curve (x, f(x)) is
plotted, if we pretend to fold the curve around the vertical line x = µ, then the
two sides of the curve will lie exactly on one another. See the Figure 1.7.2.
The Standard Normal Distribution: The normal distribution with µ = 0,
σ = 1 is customarily referred to as the standard normal distribution and the
standard normal random variable is commonly denoted by Z. The standard
normal pdf and the df are respectively denoted by
Figure 1.7.3. Standard Normal: (a) Density φ(z) (b) DF Φ(z)
In the Figure 1.7.3 we have shown plots of the pdf φ(z) and the df Φ(z). In
these plots, the variable z should stretch from –∞ to ∞. But, the standard
normal pdf φ(z), which is symmetric around z = 0, falls off very sharply. For
all practical purposes, there is hardly any sizable density beyond the interval
(–3, 3). This is also reflected in the plot of the corresponding df Φ(z).
Unfortunately, there is no available simple analytical expression for the df
Φ(z). The standard normal table, namely the Table 14.3.1, will facilitate find-
ing various probabilities associated with Z. One should easily verify the fol-
lowing:
How is the df of the N(µ, σ2) related to the df of the standard normal
random variable Z? With any fixed x ∈ ℜ, v = (u – µ)/σ, observe that we

1. Notions of Probability
41
can proceed as in (1.7.15) and write
with the Φ(.) function defined in (1.7.16).
In the Figure 1.7.2, we plotted the pdf’s corresponding to the N(0, .25)
and N(1, .25) distributions. By comparing the two plots in the Figure 1.7.2 we
see that the shapes of the two pdf’s are exactly same whereas in (b) the
curve’s point of symmetry (x = 1) has moved by a unit on the right hand side.
By comparing the plots in the Figures 1.7.2(a)-1.7.3(a), we see that the N(0,
.25) pdf is more concentrated than the standard normal pdf around their points
of symmetry x = 0.
In (1.7.13), we added earlier that the parameter µ indicates the point of
symmetry of the pdf. When σ is held fixed, this pdf’s shape remains intact
whatever be the value of µ. But when µ is held fixed, the pdf becomes more
(less) concentrated around the fixed center µ as σ becomes smaller (larger).
Example 1.7.10 Suppose that the scores of female students on the recent
Mathematics Scholastic Aptitude Test were normally distributed with µ = 520
and σ = 100 points. Find the proportion of female students taking this exam
who scored (i) between 500 and 620, (ii) more than 650. Suppose that F(.)
stands for the df of X. To answer the first part, we find P(500 < X < 620) =
F(620) – F(500) = Φ((620–520)/100) – Φ((500–520)/100) = Φ(1) – Φ(–.2) =
Φ(1) + Φ(.2) –1, using (1.7.18). Thus, reading the entries from the standard
normal table (see Chapter 14), we find that P(500 < X < 620) = .84134 +
.57926 – 1 = .4206. Next, we again use (1.7.18) and the standard normal table
to write P(X > 650) = 1 – F(650) = 1 – Φ(650–520/100) = 1 – Φ(1.3) = 1 –
.90320 = .0968. !
Theorem 1.7.1 If X has the N(µ, σ2) distribution, then W = (X – µ)/σ has
the standard normal distribution.
Proof Let us first find the df of W. For any fixed w ∈ ℜ, in view of
(1.7.18), we have
Since Φ(w) is differentiable for all w ∈ ℜ, using (1.6.10) we can claim that the
pdf of W would be given by dΦ(w)/dw = φ(w) which is the pdf of the standard
normal random variable. !

42
1. Notions of Probability
The Gamma Distribution: The expression Γ(α) was introduced in
(1.6.19). We say that a positive continuous random variable X has the gamma
distribution involving α and β, denoted by Gamma(α, β), if and only if its pdf
is given by
where 0 < α, β < ∞. Here, α and β are referred to as parameters. By varying
the values of α and β, one can generate interesting shapes for the associated
pdf.
Figure 1.7.4. (a) Gamma(3,.1) Density (b) Gamma(3.2, .5) Density
In the Figure 1.7.4, the two pdf’s associated with the Gamma(α = 3, β =
.1) and Gamma(α = 3.2, β = .5) distributions have been plotted. The gamma
distribution is known to be skewed to the right and this feature is apparent
from the plots provided. As αβ increases, the point where the pdf attains its
maximum keeps moving farther to the right hand side. This distribution ap-
pears frequently as a statistical model for data obtained from reliability and
survival experiments as well as clinical trials.
Let us ask ourselves: How can one directly check that f(x) given by (1.7.20)
is indeed a pdf? The f(x) is obviously non-negative for all x ∈ ℜ+. Next, we
need to verify directly that
Let us substitute u = x/β which is one-to-one and rewrite the integral from
(1.7.21) as
which verifies (1.7.21) since

1. Notions of Probability
43
The exponential Distribution: This is a very special distribution which
coincides with the Gamma(1,β) distribution where β(> 0) is referred to as a
parameter. This distribution is widely used in reliability and survival analyses.
The associated pdf is given by
where 0 < β < ∞. This distribution is also skewed to the right.
Figure 1.7.5 Exponential Densities: (a) β = 1 (b) β = 2
The pdf given by (1.7.23) has been plotted in the Figure 1.7.5 with β = 1, 2.
The plot (a) corresponds to the pdf of the standard exponential variable. The
plot (b) corresponds to β = 2 and it is clear that this pdf has moved to the rhs
compared with the standard exponential pdf.
As the parameter β increases, the pdf given by (1.7.23) will move farther to
the right and as a result the concentration in the right hand vicinity of zero will
decrease. In other words, as β increases, the probability of failure of items in
the early part of a life test would decrease if X represents the life expectancy of
the tested items.
The reader can easily check that the corresponding df is given by
 In the context of a reliability experiment, suppose that X represents the
length of life of a particular component in a machine and that X has the
exponential distribution defined by (1.7.23). With any two fixed positive

44
1. Notions of Probability
numbers a and b, we then have
This conveys the following message: Given that a component has lasted up
until the time a, the conditional probability of its surviving beyond the time
a + b is same as P(X > b}, regardless of the magnitude of a. In other words,
the life of the component ignores the aging process regardless of its own age.
This interesting feature of the exponential distribution is referred to as its
memoryless property. The recently edited volume of Balakrishnan and Basu
(1995) gives a synthesis of the gamma, exponential, and other distributions.
The Chi-square Distribution: We say that a positive continuous random
variable X has the Chi-square distribution with ν degrees of freedom denoted
by 
 , with ν = 1, 2, 3, ..., if X has the Gamma(1/2ν, 2) distribution. Here,
the parameter ν is referred to as the degree of freedom. By varying the values
of ν, one can generate interesting shapes for the associated pdf.
Figure 1.7.6. PDF’s: (a)
Thin;
Thick (b)
Thin;
Thick
A Chi-square random variable is derived from the Gamma family and so it
should not be surprising to learn that Chi-square distributions are skewed to
the right too. In the Figure 1.7.6, we have plotted the pdf’s corresponding
of the 
 random variable when ν = 5, 10, 25, 30. From these figures, it
should be clear that as the degree of freedom ν increases, the pdf tends to
move more toward the rhs. From the Figure 1.7.6 (b) it appears that the
shape of the pdf resembles more like that of a symmetric distribution when
x2
5
x 2
25
2
35
x
x 2
10

1. Notions of Probability
45
ν  = 25 and 35. In the Section 5.4.1, the reader will find a more formal
statement of this empirical observation for large values of ν. One may refer to
(5.4.2) for a precise statement of the relevant result.
The Lognormal Distribution: We say that a positive continuous random
variable X has the lognormal distribution if and only if its pdf is given by
where – ∞ < µ < ∞ and 0 < σ < ∞ are referred to as parameters. The pdf given
by (1.7.27) when µ = 0 and σ = 1 has been plotted in the Figure 1.7.8 and it also
looks fairly skewed to the right. We leave it as the Exercise 1.7.15 to verify that
We may immediately use (1.7.28) to claim that
That is, the pdf of log(X) must coincide with that of the N(µ, σ2) random
variable. Thus, the name “lognormal” appears quite natural in this situation.
Figure 1.7.7. Lognormal Density: µ = 0, σ = 1
The Student’s t Distribution: The pdf of a Student’s t random variable
with ν degrees of freedom, denoted by tν, is given by
with 
 ν = 1, 2, 3, ... . One can
easily verify that this distribution is symmetric about x = 0. Here, the param-
eter ν is referred to as the degree of freedom. This distribution plays a key

46
1. Notions of Probability
role in statistics. It was discovered by W. S. Gosset under the pseudonym
“Student” which was published in 1908. By varying the values of ν, one can
generate interesting shapes for the associated pdf.
Figure 1.7.8. Student’s t PDF’s: (a) t1 and t5 (b) t7 and t50
In the Figure 1.7.8, we have plotted the Student’s tν variable’s pdf given by
(1.7.30) when ν = 1, 5, 7 and 50. As the degree of freedom ν increases, one
can see that the pdf has less spread around the point of symmetry x = 0. If
one compares the Figure 1.7.8 (b) with ν = 50 and the Figure 1.7.3 (a) for the
pdf of the standard normal variable, the naked eyes may not find any major
differences. In the Section 5.4.2, the reader will find a more formal statement
of this empirical observation for large values of ν. One may refer to (5.4.3)
for a precise statement.
Table 1.7.1. Comparison of the Tail Probabilities for the
Student’s tν and the Standard Normal Distributions
P(Z > 1.5):
P(Z > 1.96):
P(Z > 2.5):
P(Z > 5):
6.6807 × 10–2
 2.4998 × 10–2  6.2097 × 10–3
2.8665 × 10–7
ν
P(tν > 1.5)
P(tν > 1.96)
P(tν > 2.5)
P(tν > 5)
10
8.2254 × 10–2
3.9218 × 10–2 1.5723 × 10–2
2.6867 × 10–4
15
7.7183 × 10–2
3.4422 × 10–2 1.2253 × 10–2
7.9185 × 10–5
30
7.2033 × 10–2
2.9671 × 10–2 9.0578 × 10–3
1.1648 × 10–5
100
6.8383 × 10–2
2.6389 × 10–2 7.0229 × 10–3
1.2251 × 10–6
500
6.7123 × 10–2
2.5275 × 10–2 6.3693 × 10–3
3.973 × 10–7
1000 6.6965 × 10–2
2.5137 × 10–2 6.2893 × 10–3
3.383 × 10–7
It is true, however, that the tails of the t distributions are “heavier” than
those of the standard normal distribution. In order to get a feeling for this,
one may look at the entries given in the Table 1.7.1. It is clear that P(tν >
1.5) decreases as ν is successively assigned the value 10, 15, 30, 100, 500

1. Notions of Probability
47
and 1000. Even when ν = 1000, we have P(tν > 1.5) = .066965 whereas P(Z
> 1.5) = .066807, that is P(tν > 1.5) > P(Z > 1.5). In this sense, the tail of the
tν distribution is heavier than that of the standard normal pdf. One may also
note that the discrepancy between P(tν > 5) and P(Z > 5) stays large even
when ν = 1000.
The Cauchy Distribution: It corresponds to the pdf of a Student’s tν
random variable with ν = 1, denoted by Cauchy(0, 1). In this special case, the
pdf from (1.7.30) simplifies to
Figure 1.7.9. Cauchy (0, 1) Density
The pdf from (1.7.31) has been plotted in the Figure 1.7.9 which naturally
coincides with the Figure 1.7.8 (a) with ν = 1. The pdf from (1.7.31) is obvi-
ously symmetric about x = 0.
Let us ask ourselves: How can one directly check that f(x) given by (1.7.31)
is indeed a pdf? Obviously, f(x) is positive for all x ∈ ℜ. Next, let us evaluate
the whole integral and write
The df of the Cauchy distribution is also simple to derive. One can verify that
The Cauchy distribution has heavier tails compared with those of the stan-
dard normal distribution. From (1.7.31) it is clear that the Cauchy pdf f(x) ≈ 1/
πx–2 for large values of |x|. It can be argued then that 1/πx–2 → 0 slowly com-
pared with 
 when |x| → ∞.
The F Distribution: We say that a positive continuous random variable
X has the F distribution with ν1, ν2 degrees of freedom, in that order and

48
1. Notions of Probability
denoted by Fν1, ν2, if and only if its pdf is given by
with 
 . Here,
ν1 and ν2 are referred to as the parameters. By varying the values of ν1 and ν2,
one can generate interesting shapes for this pdf.
Figure 1.7.10. F Densities: (a) F1, 1 and F1, 5
(b) F4, 5 and F3, 4
The pdf from (1.7.34) has been plotted in the Figure 1.7.10 when we fix (ν1,
ν2) = (1, 1), (1, 5), (4, 5), (3, 4). One realizes that the F distribution is skewed
to the right.
The Beta Distribution: Recall the expression Γ(α), the beta function b(α,
β), and that b(α, β) = Γ(α)Γ(β){Γ(α + β)}–1 respectively from (1.6.19), (1.6.25)-
(1.6.26). A continuous random variable X, defined on the interval (0, 1), has
the beta distribution with parameters α and β, denoted by Beta(α, β), if and
only if its pdf is given by
where 0 < α, β < ∞. By varying the values of α and β, one can generate
interesting shapes for this pdf. In general, the Beta distributions are fairly
skewed when α ≠ β. The beta pdf from (1.7.35) has been plotted in the
Figure 1.7.11 for (α, β) = (2, 5), (4, 5). The pdf in the Figure 1.7.11 (b),
however, looks almost symmetric. It is a simple matter to verify that a
random variable distributed as Beta(1, 1) is equivalent to the Uniform(0, 1)

1. Notions of Probability
49
random variable.
Figure 1.7.11. Beta Densities: (a) α = 2, β = 5 (b) α = 4, β = 5
The Negative Exponential Distribution: We say that a continuous ran-
dom variable X has the negative exponential distribution involving γ and β, if
and only if its pdf is given by
where –∞ < γ < ∞, 0 < β < ∞. Here, γ and β are referred to as parameters.
This distribution is widely used in modeling data arising from experiments in
reliability and life tests. When the minimum threshold parameter γ is assumed
zero, one then goes back to the exponential distribution introduced earlier in
(1.7.23).
In the Exercise 1.7.23, we have asked to plot this pdf for different values of
β and γ. Based on these plots, for some fixed number a, the reader should think
about the possible monotonicity property of Pγ, β{X > a} as (i) a function of β
when γ is kept fixed, or (ii) a function of γ when β is kept fixed.
Figure 1.7.12. Weibull Densities: (a) α = 3, β = .5 (b) α = 3, β = 1

50
1. Notions of Probability
The Weibull Distribution: We say that a positive continuous random
variable X has the Weibull distribution if and only if its pdf is given by
where α(> 0) and β(> 0) are referred to as parameters. This pdf is also
skewed to the right. By varying the values of α and β, one can generate
interesting shapes for the associated pdf. The Weibull pdf from (1.7.37) has
been plotted in the Figure 1.7.12 for (α, β) = (3, .5), (3, 1). These figures are
skewed to the right too.
The Rayleigh Distribution We say that a positive continuous random
variable X has the Rayleigh distribution if and only if its pdf is given by
where θ(> 0) is referred to as a parameter. In reliability studies and related
areas, this distribution is used frequently.
Figure 1.7.13. Raleigh Densities: (a) θ = 1 (b) θ = 4
The pdf from (1.7.38) has been plotted in the Figure 1.7.13 for θ = 1, 4. This
pdf is again skewed to the right. The tail in the rhs became heavier as θ in-
creased from 1 to 4.
1.8 Exercises and Complements
1.1.1 (Example 1.1.1 Continued) In the set up of the Example 1.1.1, find
the probability of observing no heads or two heads.
1.1.2 (Example 1.1.2 Continued) In the set up of the Example 1.1.2, find
the probability of observing a difference of one between the scores which

1. Notions of Probability
51
come up on the two dice. Also, find the probability of observing the red die
scoring higher than that from the yellow die.
1.1.3 Five equally qualified individuals consisting of four men and one woman
apply for two identical positions in a company. The two positions are filled by
selecting two from this applicant pool at random.
(i)
Write down the sample space S for this random experiment;
(ii)
Assign probabilities to the simple events in the sample space S;
(iii) Find the probability that the woman applicant is selected for a posi-
tion.
1.2.1 One has three fair dice which are red, yellow and brown. The three
dice are rolled on a table at the same time. Consider the following two events:
A : The sum total of the scores from all three dice is 10
B : The sum total of the scores from the red and brown dice exceeds 8
Find P(A), P(B) and P(A n B).
1.2.2 Prove the set relations given in (1.2.2).
1.2.3 Suppose that A, B, C are subsets of S. Show that
(i)
A ∆ C ⊆ (A ∆ B) ∪ (B ∆ C);
(ii) (A ∆ B) ∪ (B ∆ C)
= [(A ∪ B) ∩ (A ∩ B)c] ∪ [(B ∪ C) ∩ (B ∩ C)c].
{Hint: Use the Venn Diagram.}
1.2.4 Suppose that A1, ..., An are Borel sets, that is they belong to ß. Define
the following sets: B1 = A1,B2 = A2 ∩ B3 = A3 ∩ 
 (A1 ∪ A2)c, ..., Bn = An ∩ (A1
∪ ... ∪ An–1)c. Show that
(i)
B1, ..., Bn are Borel sets;
(ii)
B1, ..., Bn are disjoint sets;
(iii)
1.2.5 Suppose that S = {(x, y) : x2 + y2 ≤ 1}. Extend the ideas from the
Example 1.2.3 to obtain a partition of the circular disc, S. {Hint: How about
considering Ai = {(x, y) : 1/2i < x2 + y2 = 1/2i–1}, i = 1, 2, ...? Any other
possibilities?}
1.3.1 Show that the Borel sigma-field ß is closed under the operation of (i)
finite intersection, (ii) countably infinite intersection, of its members. {Hint:
Can DeMorgan’s Law (Theorem 1.2.1) be used here?}
1.3.2 Suppose that A and B are two arbitrary Borel sets. Then, show that
A ∆ B is also a Borel set.

52
1. Notions of Probability
1.3.3 Suppose that A1, ..., Ak are disjoint Borel sets. Then show that
. Contrast this result with the requirement in part (iii)
of the Definition 1.3.4.
1.3.4 Prove part (v) in the Theorem 1.3.1.
1.3.5 Consider a sample space S and suppose that ß is the Borel sigma-
field of subsets of S. Let A, B, C be events, that is these belong to ß. Now,
prove the following statements:
(i)
P(A ∪ B) = P(A) + P(B);
(ii) P(A ∩ B) = P(A) + P(B) – 1;
(iii) P(A ∩ C) = min{P(A), P(C)};
(iv) P(A ∪ B ∪ C) = P(A) + P(B) + P(C) – P(A ∩ B)
– P(A ∩ C) – P(B ∩ C) + P(A ∩ B ∩ C).
{Hint: Use the Theorem 1.3.1 to prove Parts (i)-(iii). Part (iv) should fol-
low from the Theorem 1.3.1, part (iv). The result in part (ii) is commonly
referred to as the Bonferroni inequality. See the Theorem 3.9.10.}
1.3.6 (Exercise 1.3.5 Continued) Suppose that A1, ..., An are Borel sets,
that is they belong to ß. Show that
{Hint: Use mathematical induction and part (iv) in the Exercise 1.3.5.}
1.3.7 (Exercise 1.3.5 Continued) Suppose that A1, ..., An are Borel sets, that
is they belong to ß. Show that
This is commonly referred to as the Bonferroni inequality. {Hint: Use math-
ematical induction and part (ii) in the Exercise 1.3.5. See the Theorem 3.9.10.}
1.3.8 Suppose that A1, A2 are Borel sets, that is they belong to ß. Show that
where recall from (1.2.1) that A1∆A2 stands for the symmetric difference of
A1, A2.
1.3.9 Suppose that A1, A2, A3 are Borel sets, that is they belong to ß. Recall
from (1.2.1) that Ai∆ Aj stands for the symmetric difference of Ai, Aj. Show that
(i)
A1∆A3 ⊆ (A1∆A2) ∪ (A2∆A3);
(ii)
P(A1∆A3) ≤ P(A1∆A2) + P(A2∆A3).

1. Notions of Probability
53
1.4.1 Prove the Theorem 1.4.1.
1.4.2 In the Theorem 1.4.2, prove that (ii) ⇒ (iii) ⇒ (iv).
1.4.3 Consider a sample space S and suppose that A, B are two events
which are mutually exclusive, that is, A ∩ B = ϕ. If P(A) > 0, P(B) > 0, then
can these two events A, B be independent? Explain.
1.4.4 Suppose that A and B are two events such that P(A) = .7999 and
P(B) = .2002. Are A and B mutually exclusive events? Explain.
1.4.5 A group of five seniors and eight first year graduate students are
available to fill the vacancies of local news reporters at the radio station in a
college campus. If four students are to be randomly selected from this pool
for interviews, find the probability that at least two first year graduate stu-
dents are among the chosen group.
1.4.6 Four cards are drawn at random from a standard playing pack of
fifty two cards. Find the probability that the random draw will yield
(i)
an ace and three kings;
(ii)
the ace, king, queen and jack of clubs;
(iii)
the ace, king, queen and jack from the same suit;
(iv)
the four queens.
1.4.7 In the context of the Example 1.1.2, let us define the three events:
E :
The sum total of the scores from the two up faces exceeds 8
F :
The score on the red die is twice that on the yellow die
G :
The red die scores one point more than the yellow die
Are the events E, F independent? Are the events E, G independent?
1.4.8 (Example 1.4.7 Continued) In the Example 1.4.7, the prevalence of
the disease was 40% in the population whereas the diagnostic blood test had
the 10% false negative rate and 20% false positive rate. Instead assume that the
prevalence of the disease was 100p% in the population whereas the diagnostic
blood test had the 100α% false negative rate and 100β% false positive rate, 0 <
p, α, β < 1. Now, from this population an individual is selected at random and
his blood is tested. The health professional is informed that the test indicated
the presence of the particular disease. Find the expression of the probability,
involving p, α and β, that this individual does indeed have the disease. {Hint:
Try and use the Beyes’s Theorem.}
1.4.9 Let us generalize the scenario considered in the Examples 1.4.5-
1.4.6. The urn #1 contains eight green and twelve blue marbles whereas
the urn #2 has ten green and eight blue marbles. Suppose that we also
have the urn #3 which has just five green marbles. These marbles have
the same size and weight. Now, the experimenter first randomly selects
an urn, with equal probability, and from the selected urn draws one marble

54
1. Notions of Probability
at random. Given that the selected marble was green, find the probability that
(i)
the urn #1 was selected;
(ii)
the urn #2 was selected;
(iii)
the urn #3 was selected.
{Hint: Try and use the Bayes Theorem.}
1.4.10 (Exercise 1.2.1 Continued) One has three fair dice which are red,
yellow and brown. The three dice are rolled on a table at the same time. Con-
sider the following events:
A :
 The total score from all three dice is 10
B :
 The total score from the red and brown dice exceeds 8
Are A, B independent events?
1.4.11 (Example 1.4.4 Continued) Show that P(A) = P(B) assuming that
there are respectively m, n green and blue marbles in the urn to begin with.
This shows that the result P(A) = P(B) we had in the Example 1.4.4 was not
just a coincidence after all.
1.4.12 Show that 
. {Hint: Can the binomial theorem, namely
(a+b)n = 
, be used here?}
1.4.13 Suppose that we have twenty beads of which eight are red, seven are
green and five are blue. If we set them up in a row on a table, how many
different patterns are possible?
1.4.14 Suppose that four boys and four girls are waiting to occupy a chair
each, all placed in a row adjacent to each other. If they are seated randomly,
what is the probability that the boys and girls will alternate? {Hint: The total
possible arrangement is 8!. Note that each arrangement could start with a boy
or a girl.}
1.4.15 Suppose that we have n different letters for n individuals as well as n
envelopes correctly addressed to those n individuals. If these letters| are ran-
domly placed in these envelopes so that exactly one letter goes in an envelope,
then what is the probability that at least one letter will go in the correct enve-
lope? Obtain the expression of this probability when we let n go to ∞. {Hint:
Let Ai be the event that the ith letter is stuffed in its correct envelope, i = 1, ..., n.
We are asked to evaluate P(A1 ∪ ... ∪ An). Apply the result from the Exercise
1.3.6. Observe also that P(Ai) = (n–1)!/n!, P(Ai ∩ Aj) = (n–2)!/n!, P(Ai ∩ Aj
∩ Ak) = (n–3)!/n!, ..., P(A1 ∩ ... ∩ An) = 1/n!, for i ≠ j ≠ k... . Now all there
is left is to count the number of terms in the single, double, triple sums and so
on. The answer for P(A1 ∪...∪ An) should simplify to 1 – 1/2! + 1/3! – ... +
(–1)n–1 1/n! which is approximately 1 – e–1 for very large n. See (1.6.13).}

1. Notions of Probability
55
1.4.16 Consider the digits 0, 1, ..., 9. Use these digits at random to form a
four (five) digit number. Then, find the probability of forming
(i)
a four digit random number, not starting with a zero, which
would be an even number while each digit appears exactly once;
(ii)
a four digit random number which would be an even number
where the digits can be repeated, and starting with a zero is
allowed;
(iii)
a five digit random number, not starting with a zero, which would
be divisible by the number five while each digit appears only
once.
1.4.17 In a twin engine plane, we are told that the two engines (#1, #2)
function independently. We are also told that the plane flies just fine when at
least one of the two engines are working. During a flying mission, individually
the engine #1 and #2 respectively may fail with probability .001 and .01.
Then, during a flying mission, what is the probability that the plane would
crash? The plane would complete its mission?
1.4.18 Suppose that A1, ..., Ak are disjoint events. Let B be another event.
Then, show that
1.4.19 Suppose that A1, A2 are events. Then, show that
1.4.20 Suppose that A1, A2 are disjoint events. Then, show that
(i)
P(A1 | 
)= P(A1)/{1 – P(A2)} if P(A2) ≠ 1;
(ii)
P(A1 | A1 ∪ A2) = P(A1)/{P(A1) + P(A2)}.
1.5.1 Suppose that a random variable X has the following pmf:
X values:
–2
0
1
3
8
Probabilities:
.2
p
.1
2p
.4
where p ∈ (0, .1].
(i)
Is it possible to determine p uniquely?
(ii)
Find P{|X – .5| > 2} and P{|X – .5| = 2.5}.
1.5.2 Suppose that a random variable X has the following pmf:
X values:
–2
0
1
    3
8
Probabilities:
.2
p
.1
.3 – p
.4

56
1. Notions of Probability
where p ∈ (0, .3). Is it possible to determine p uniquely? With a fixed but
arbitrary p ∈ (0, .3), find P{|X – .5| > 2}, P{X2 < 4} and P{|X – .3| = 1.7}.
1.5.3 Suppose that a random variable X has the following pmf:
X values:
  –4
–2
1
3
6
Probabilities:
.3– p2
p2
.1
2p
.4
where p2 ≤ .3. Is it possible to determine p uniquely?
1.5.4 (Example 1.5.2 Continued) Use the form of the df F(x) from the
Example 1.5.2 and perform the types of calculations carried out in (1.5.10).
1.5.5 A large envelope has twenty cards of same size. The number two is
written on ten cards, the number four is written on six cards, and the number
five is written on the remaining four cards. The cards are mixed inside the
envelope and we go in to take out one card at random. Let X be the number
written on the selected card.
(i)
Derive the pmf f(x) of the random variable X;
(ii)
Derive the df F(x) of the random variable X;
(iii)
Plot the df F(x) and check its points of discontinuities;
(iv)
Perform the types of calculations carried out in (1.5.10).
1.5.6 Prove the Theorem 1.5.1.
1.5.7 An urn contains m red and n blue marbles of equal size and weight.
The marbles are mixed inside the urn and then the experimenter selects four
marbles at random. Suppose that the random variable X denotes the number of
red marbles selected. Derive the pmf of X when initially the respective number
of marbles inside the urn are given as follows:
(i)
m = 5, n = 3;
(ii)
m = 3, n = 5;
(iii)
m = 4, n = 2.
In each case, watch carefully the set of possible values of the random
variable X wherever the pmf is positive. Next write down the pmf of X when
m and n are arbitrary. This situation is referred to as sampling without re-
placement and the corresponding distribution of X is called the Hypergeomet-
ric distribution.
1.5.8 A fair die is rolled n times independently while we count the number
of times (X) the die lands on a three or six. Derive the pmf f(x) and the df F(x)
of the random variable X. Evaluate P(X < 3) and P(|X – 1| > 1).
1.6.1 Let c be a positive constant and consider the pdf of a random

1. Notions of Probability
57
variable X given by
The Figure 1.8.1 gives a plot of f(x) when c = 4/3. It is implicit that x, f(x) are
respectively plotted on the horizontal and vertical axes.
Find the correct value of c. Find the df F(x) and plot it. Does F(x) have any
points of discontinuity? Find the set of points, if non-empty, where F(x) is not
differentiable. Calculate P(– .5 < X ≤ .8). Find the median of this distribution.
Figure 1.8.1. The PDF f(x) from the Exercise 1.6.1 with c = 4/3
1.6.2 Let c be a constant and consider the pdf of a random variable X given
by
Find the value of c. Find the df F(x) and plot it. Does F(x) have any points of
discontinuity? Find the set of points, if non-empty, where F(x) is not differen-
tiable. Calculate P(– 1.5 < X ≤ 1.8). Find the median of this distribution.
1.6.3 Let c be a positive constant and consider the pdf of a random vari-
able X given by

58
1. Notions of Probability
Find the value of c. Find the df F(x) and plot it. Does F(x) have any points of
discontinuity? Find the set of points, if nonempty, where F(x) is not differen-
tiable. Calculate P(– 1.5 < X ≤ 1.8). Find the median of this distribution.
1.6.4 (Example 1.6.5 Continued) Consider the function f(x) defined as
follows:
Let us denote the set A = {1, 1/2, 1/22, 1/23, ...}. Show that
(i)
the function f(x) is a genuine pdf;
(ii)
the associated df F(x) is continuous at all points x ∈ A;
(iii)
Find the set of points, if nonempty, where F(x) is not differen-
tiable.
1.6.5 Along the lines of the construction of the specific pdf f(x) given in the
Exercise 1.6.4, examine how one can find other examples of pdf’s so that the
associated df’s are non-differentiable at countably infinite number of points.
1.6.6 Is f(x) = x–2I(1 < x < ∞) a genuine pdf? If so, find P{2 < X ≤ 3},
P{|X – 1| ≤ .5} where X is the associated random variable.
1.6.7 Consider a random variable X having the pdf f(x) = c(x – 2x2 + x3)I(0
< x < 1) where c is a positive number and I(.) is the indicator function. Find c
first and then evaluate P(X > .3). Find the median of this distribution.
1.6.8 Suppose that a random variable X has the Rayleigh distribution with
where θ(> 0) is referred to as a parameter. First show directly that f(x) is
indeed a pdf. Then derive the expression of the df explicitly. Find the median
of this distribution. {Hint: Try substitution u = x2/θ during the integration.}
1.6.9 Suppose that a random variable X has the Weibull distribution with
where α(> 0) and β(> 0) are referred to as parameters. First show directly
that f(x) is indeed a pdf. Then derive the expression of the df explicitly. Find
the median of this distribution. {Hint: Try the substitution v = xβ/α during the
integration.}

1. Notions of Probability
59
1.6.10 Consider the function f(x) = e–x2 (1 + x2) for x ∈ ℜ. Use (1.6.18) to
examine the monotonicity of f(x) in x.
1.6.11 Consider the function f(x) = exp{x – x1/2 – 1/2log(x)} for x > 0. Use
(1.6.18) to show that f(x) is increasing (decreasing) when |x1/2 + 1/2| < (>)3/4.
1.6.12 Consider the function f(x) = e–|x| (1 + x2) for x ∈ ℜ. Use (1.6.18) to
examine the monotonicity of f(x) in x.
1.6.13 Use the method of integration by parts from (1.6.28) to evaluate
1.6.14 By the appropriate substitutions, express the following in the form
of a gamma integral as in (1.6.19). Then evaluate these integrals in terms of
the gamma functions.
1.6.15 Use Stirling’s approximation, namely that for large values of α, to
prove: 
 for large positive integral values of n. {Hint: Observe
that n! =Γ(n+1) ~ √2 e -(n+1)(n+1)n+1/2 But, one can rewrite the last expression
as 
 
  and then appeal to (1.6.13).}
1.7.1 Consider the random variable which has the following discrete uni-
form distribution:
Derive the explicit expression of the associated df F(x), x ∈ Β. Evaluate
 and 
. {Hint: Use (1.6.11).}
1.7.2 The probability that a patient recovers from a stomach infection is 9.
Suppose that ten patients are known to have contracted this infection. Then
(i)
what is the probability that exactly seven will recover?
(ii)
what is the probability that at least five will recover?
(iii)
what is the probability that at most seven will recover?
1.7.3 Suppose that a random variable X has the Binomial(n, p) distrib-

60
1. Notions of Probability
ution, 0 < p < 1. Show that
{Hint: Use (1.4.3), (1.7.2) and direct calculations.}
1.7.4 Suppose that a random variable X has the Binomial(n, p) distribution,
0 < p < 1. For all x = 0, 1, ..., n, show that
This recursive relation helps enormously in computing the binomial probabili-
ties successively for all n, particularly when n is large. {Hint: Use (1.7.2) and
direct calculations.}
1.7.5 (Exercise 1.7.4 Continued) Suppose that a random variable X has the
Binomial(n, p) distribution, 0 < p < 1. Find the value of x at which P(X = x) is
maximized. {Hint: Use the result from the Exercise 1.7.4 to order the prob-
abilities P(X = x) and P(X = x + 1) for each x first.}
1.7.6 The switchboard rings at a service desk according to a Poisson distri-
bution on the average five (= λ) times in a ten minute interval. What is the
probability that during a ten minute interval, the service desk will receive
(i)
no more than three calls?
(ii)
at least two calls?
(iii)
exactly five calls?
1.7.7 Suppose that a random variable X has the Poisson (λ) distribution, 0
< λ < ∞. Show that
{Hint: Use (1.4.3), (1.7.4) and direct calculations.}
1.7.8 Suppose that a random variable X has the Poisson (λ) distribution, 0
< λ < ∞. For all x = 0, 1, 2, ..., show that
This recursive relation helps enormously in computing the Poisson probabili-
ties successively for all x, particularly when x is large. {Hint: Use (1.7.4) and
direct calculations.}

1. Notions of Probability
61
1.7.9 In this exercise, you are given the expressions of the pdf of different
random variables. In each case, identify the random variable by its standard
name and specify the values of the associated parameters. Also, find the value
of c in each case.
(i)
 f(x) = cexp(–πx)I(x > 0), c is a positive constant;
(ii)
 f(x) = cexp(–πx2), x ∈ ℜ, and c is a positive constant;
(iii)
f(x) = cexp(–x2 – 1/4x), x ∈ ℜ, and c is a positive constant;
(iv
 f(x) = 4xcexp(–2x)I(x > 0), c is a positive constant;
(v)
f(x) = 128/3x4exp(–cx)I(x > 0), c is a positive constant;
(vi)
f(x) = 105x4(1 – x)cI(0 < x < 1), c is a positive constant.
1.7.10 Suppose that a random variable X has the Gamma(2, 1) distribution
with its pdf given by (1.7.20). Find the expressions for P(X < a), P(X > b) in the
simplest form where a, b are positive numbers. {Hint: Observe that
 and evaluate this integral by parts.}
1.7.11 Suppose that a random variable X has the Gamma(3, 2) distribution
with its pdf given by (1.7.20). Find the expressions for P(X < a), P(X > b) in
the simplest form where a, b are positive numbers. {Hint: Observe that  P(X
< a) = 1/16 
x2 e-x/2 dx and evaluate this integral by parts.}
1.7.12 Suppose that a random variable X has the 
  distribution. Derive
the expression of the associated df explicitly. Also, find P(X > a + b | X > a)
where a, b are positive numbers. Does this answer depend on the number a?
Explain.
Figure 1.8.2. The PDF f(x) from the Exercise 1.7.13 with θ = 0:
Thick β = 1; Thin β = 2
1.7.13 Suppose that a random variable X has the following pdf involving

62
1. Notions of Probability
θ and β
where θ ∈ ℜ and β ∈ℜ+. Here, θ and β are referred to as parameters. In the
statistical literature, this is known as the Laplace or double exponential distri-
bution. The Figure 1.8.2 gives a plot of this pdf when θ = 0 and β = 1, 2. It is
implicit that x, f(x) are respectively plotted on the horizontal and vertical axes.
(i)
Show that the pdf f(x) is symmetric about x = θ
(ii)
Derive the expression of the df explicitly;
(iii)
Show that P{| X – θ| > a + b | |X – θ| > a} does not depend on
a, for any two positive numbers a and b;
(iv)
Let θ = 0 and β = 1. Obtain an expression for the right hand tail
area probability, P(X > a), a > 0. Then compare tail area prob-
ability with that of the Cauchy distribution from (1.7.31). Which
distribution has “heavier” tails? Would this answer change if θ =
0 and β = 2 instead? {Hint: Recall the types of discussions we
had around the Table 1.7.1.}
1.7.14 (Exercise 1.6.7 Continued) Consider a random variable X having the
pdf f(x) = c(x – 2x2 + x3)I(0 < x < 1) where c is a positive number and I(.) is the
indicator function. Find the value of c. {Hint: In order to find c, should you
match this f(x) up against the Beta density?}
1.7.15 Show that 
0) given by (1.7.27) is a bona-fide pdf with µ ∈ ℜ, σ ∈ ℜ+. Also verify that
(i)
P(X ≤ x) = Φ({log(x) – µ}/σ), for all x ∈ ℜ+;
(ii)
P{log(X) ≤ x} = Φ({x – µ}/σ), for all x ∈ ℜ.
{Hint: Show that (a) f(x) is always positive and (b) evaluate 
 by
making the substitution u = log(x).}
1.7.16 The neighborhood fishermen know that the change of depth (X,
measured in feet) at a popular location in a lake from one day to the next is a
random variable with the pdf given below:
where c is a positive number. Then,
(i)
find the value of the constant c;
(ii)
determine the expression of the df;
(iii)
find the median of this distribution.
1.7.17 The neighborhood fishermen know that the change of depth (X,
measured in feet) at a popular location in a lake from one day to the next

1. Notions of Probability
63
is a random variable with the pdf given below:
where k is a positive number and a is some number. But, it is known that P(X
≤ 2) = .9. Then,
(i)
find the values of the constants k and a;
(ii)
determine the expression of the df;
(iii)
find the median of this distribution.
1.7.18 Suppose that the shelf life (X, measured in hours) of certain brand
of bread has the exponential pdf given by (1.7.23). We are told that 90% of
this brand of bread stays suitable for sale at the most for three days from the
moment they are put on the shelf. What percentage of this brand of bread
would last for sale at the most for two days from the moment they are put on
the shelf?
1.7.19 Let X be distributed as N(µ, σ2) where µ ∈ ℜ, σ ∈ ℜ+.
(i)
Suppose that P(X ≤ 10) = .25 and P(X ≤ 50) = .75. Can µ and σ
be evaluated uniquely?
(ii)
Suppose that median of the distribution is 50 and P(X > 100) =
.025. Can µ and σ be evaluated uniquely?
1.7.20 Let 
 z ∈ ℜ. Along the lines of (1.7.15)-
(1.7.16) we claim that φ(z) is a valid pdf. One can alternately use the polar
coordinates to show that 
 by going through the following steps.
(i)
Show that it is enough to prove: 
(ii)
Verify: 
 Hence,
show that 
(iii)
In the double integral found in part (ii), use the substitutions u =
r cos(θ), υ = r sin(θ), 0 < r < ∞ and 0 < θ < 2π. Then, rewrite,
 ;
(iv)
Evaluate explicitly to show that 
(v)
Does part (iii) now lead to part (ii)?
1.7.21 A soft-drink dispenser can be adjusted so that it may fill µ ounces
of the drink per cup. Suppose that the ounces of fill (X) are normally distrib-
uted with parameters µ ounce and σ = .25 ounce. Obtain the setting for µ so
that 8-ounce cups will overflow only 1.5% of the time.

64
1. Notions of Probability
1.7.22 Consider an arbitrary random variable Y which may be discrete or
continuous. Let A be an arbitrary event (Borel set) defined through the ran-
dom variable Y. For example, the event A may stand for the set where Y ≥ 2 or
|Y| > 4 ∪ |Y| ≤ 1/2 or one of the many other possibilities. Define a new random
variable X = I(A), the indicator variable of the set A, that is:
Argue that X is a Bernoulli variable, defined in (1.7.1), with p = P(A).
1.7.23 Consider the negative exponential pdf
from (1.7.36) where β ∈ ℜ+, γ ∈ ℜ. Plot the pdf for several values of β and
γ. Answer the following questions by analyzing these plots.
(i)
If β is held fixed, will Pγ=1{X > 3} be larger than
Pγ=2{X > 3}?
(ii)
If γ is held fixed, will Pβ=2{X > 4} be larger than
Pβ=3{X > 4}?

2
Expectations of Functions of
Random Variables
2.1 Introduction
In Chapter 1 we introduced the notions of discrete and continuous random
variables. We start Chapter 2 by discussing the concept of the expected value
of a random variable. The expected value of a random variable is sometimes
judged as the “center” of the probability distribution of the variable. The vari-
ance of a random variable then quantifies the average squared deviation of a
random variable from its “center”. The Section 2.2 develops these concepts.
The Section 2.3 introduces the notion of the expected value of a general
function of a random variable which leads to the related notions of moments
and moment generating functions (mgf) of random variables. In Section 2.4
we apply a powerful result involving the mgf which says that a finite mgf
determines a probability distribution uniquely. We also give an example which
shows that the finiteness of all moments alone may not determine a probabil-
ity distribution uniquely. The Section 2.5 briefly touches upon the notion of a
probability generating function (pgf) which consequently leads to the idea of
factorial moments of a random variable.
2.2 Expectation and Variance
Let us consider playing a game. The house will roll a fair die. The player
will win $8 from the house whenever a six comes up but the player will pay
$2 to the house anytime a face other than the six comes up. Suppose, for
example, that from ten successive turns we observed the following up
faces on the rolled die: 4, 3, 6, 6, 2, 5, 3, 6, 1, 4. At this point, the player is
ahead by $10(= $24 – $14) so that the player’s average win per game thus
far has been exactly one dollar. But in the long run, what is expected to be
the player’s win per game? Assume that the player stays in the game k
times in succession and by that time the face six appears nk times while a
non-six face appears k – nk times. At this point, the player’s win will then
amount to 8nk – 2(k – nk) so that the player’s win per game (Wk) should
65

66
2. Expectations of Functions of Random Variables
be {8nk – 2(k – nk)}/k which is rewritten as follows:
What will Wk amount to when k is very large? Interpreting probabilities as the
limiting relative frequencies we can say that 
 should coincide with the
probability of seeing the face six in a single roll of a fair die which is nothing
but 1/6. Hence, the player’s ultimate win per game is going to be
In other words, in this game the player will lose one-third of a dollar in the long
run. It is seen readily from (2.2.2) that we multiplied the possible value of the
win with its probability and added up these terms.
In (2.2.2), the final answer is really the weighted average of the two pos-
sible values of the win where the weights are the respective probabilities. This
is exactly how we interpret this player’s expected win (per game) in the long
haul. The process is intrinsically a limiting one and in general we will proceed
to define the expected value of a random variable X as simply the weighted
average of all possible values of the random variable. More precise statements
would follow.
Let us begin with a random variable X whose pmf or pdf is f(x) for x ∈ ⊆ χ
ℜ. We use the following convention:
In some examples when X is a discrete random variable, the space ÷ will
consist of finitely many points. One may recall (1.5.1) as an example. On the
other hand, in a continuous case in general, the space χ will be the union of
subintervals of the real line ℜ. However, in many examples the space χ will be
ℜ, ℜ+ or the interval (0, 1). In general, χ is called the support of the distribution
of X whether it is a discrete random variable or a continuous random variable.
Definition 2.2.1 The expected value of the random variable X, denoted
by E(X), E{X} or E[X], is defined as:

2. Expectations of Functions of Random Variables
67
where χ is the support of X. The expected value is also called the mean of the
distribution and is frequently assigned the symbol µ.
In the Definition 2.2.1, the term Σi:xi ∈ χ xif(xi) is interpreted as the infinite
sum, x1f(x1) + x2f(x2) + x3f(x3) + ... provided that x1, x2, x3, ... belong to χ, the
support of the random variable X.
In statistics, the expected value of X is often referred to as the center of the
probability distribution f(x). Let us look at the following discrete probability
distributions for the two random variables X and Y respectively.
Here we have χ = y= {–1, 1, 3, 5, 7}. Right away one can verify that
We may summarize the same information by saying that µX = µY = 3.
In a continuous case the technique is not vastly different. Consider a ran-
dom variable X whose pdf is given by
Here we have χ = (0, 2). The corresponding expected value can be found as
follows:
We will give more examples shortly. But before doing so, let us return to the
two random variables X and Y defined via (2.2.4). Both can take the same set of
possible values with positive probabilities but they have different probability
distributions. In (2.2.5) we had shown that their expected values were same
too. In other words the mean by itself may not capture all the interesting fea-
tures in a distribution.
We may ask ourselves: Between X and Y, which one is more variable? In
order to address this question, we need an appropriate measure of variation in
a random variable.
Definition 2.2.2 The variance of the random variable X, denoted by
V(X), V{X} or V[X], is defined to be E{(X – µ)2} which is the same as:

68
2. Expectations of Functions of Random Variables
where χ is the support of X and µ = E(X). The variance is frequently assigned
the symbol σ2. The positive square root of σ2, namely σ, is called the standard
deviation of X.
The two quantities µ and σ2 play important roles in statistics. The variance
measures the average squared distance of a random variable X from the center
µ of its probability distribution. Loosely speaking, a large value of σ2 indicates
that on the average X has a good chance to stray away from its mean µ,
whereas a small value of σ2 indicates that on the average X has a good chance
to stay close to its mean µ.
Next, we state a more general definition followed by a simple result. Then,
we provide an alternative formula to evaluate σ2.
Definition 2.2.3 Start with a random variable X and consider a function
g(X) of the random variable. Suppose that f(x) is the pmf or pdf of X where x
∈ χ. Then, the expected value of the random variable g(X), denoted by E(g(X)),
E{g(X)} or E[g(X)], is defined as:
where χ is the support of X.
Theorem 2.2.1 Let X be a random variable. Suppose that we also have
real valued functions gi(x) and constants ai, i = 0, 1, ..., k. Then, we have
as long as all the expectations involved are finite. That is, the expectation is a
linear operation.
Proof We supply a proof assuming that X is a continuous random variable
with its pdf f(x), x ∈ χ. In the discrete case, the proof is similar. Let us write
constant and using property of the integral operations.
Hence, we have

2. Expectations of Functions of Random Variables
69
since ai ‘s are all constants and ∫χ f(x)dx = 1. Now, we have the desired
result.!
Theorem 2.2.2 Let X be a random variable. Then, we have
Proof We prove this assuming that X is a continuous random variable with
its pdf f(x), x ∈ χ. In a discrete case, the proof is similar. Note that µ, which is
E(X), happens to be a constant. So, from the Definition 2.2.2 we have
Hence, in view of the Theorem 2.2.1, we have
which is the desired result. !
Example 2.2.1 We now go back to the two discrete random variables X, Y
defined in (2.2.4). Recall that µX = µY = 3. Now, using the Definition 2.2.3 we
note that
Then using the Theorem 2.2.2, we have the corresponding variances 
 4.8
and 
 = 7.6. We find that the random variable Y is more variable than the
random variable X. Incidentally, the associated standard deviations are σX ≈
2.19, σY ≈ 2.76 respectively. !
Example 2.2.2 Next we go back to the continuous random variable X
defined in (2.2.6). Recall from (2.2.7) that X had its mean equal 1.5. Next,
using the Definition 2.2.3 we note that
Thus, using the Theorem 2.2.2, we have 
and the associated standard deviation is σX ≈ .387. !
Next, we state two simple but useful results. Proofs of Theorems 2.2.3-
2.2.4 have respectively been included as Exercises 2.2.18-2.2.19 with some
hints.
Theorem 2.2.3 Let X and Y be random variables. Then, we have

70
2. Expectations of Functions of Random Variables
(i)
a ≤ E(X) ≤ b if the support χ of X is the interval [a, b];
(ii)
E(X) ≤ E(Y) if X ≤ Y w.p.1.
Theorem 2.2.4 Let X be a random variable. Then, we have
where a and b are any two fixed real numbers.
We now consider another result which provides an interesting perspective
by expressing the mean of a continuous random variable X in terms of its tail
area probabilities when X is assumed non-negative.
Theorem 2.2.5 Let X be a non-negative continuous random variable with
its distribution function F(x). Suppose that 
. Then, we
have:
Proof We have assumed that X ≥ 0 w.p.1 and thus
The proof is now complete. !
We can write down a discrete analog of this result as well. Look at the
following result.
Theorem 2.2.6 Let X be a positive integer valued random variable with its
distribution function F(x). Then, we have
Proof Recall that F(x) = P(X ≤ x) so that 1 – F(x) = P(X > x) for x = 1, 2,
.... Let us first verify that

2. Expectations of Functions of Random Variables
71
where as usual I(X > x) stands for the indicator function of the event X > x. If
X happens to be one, then I(X > 0) = 1, but I(X > x) = 0 for all x = 1, 2, ... .
Hence, the rhs of (2.2.12) is also one. In other words when X = 1, the two
sides of (2.2.12) agree. Now if X happens to be two, then I(X > 0) = I(X > 1)
= 1, but I(X > x) = 0 for all x = 2, 3, ... . Hence, the rhs of (2.2.12) is also then
two. In other words when X = 2, the two sides of (2.2.12) agree. The reader
should check that (2.2.12) holds whatever be the value of X observed from
the set {1, 2, ...}.
For any fixed x ∈ {0, 1, 2, ...} the random variable I(X > x) takes only one
of the values 0 or 1. Hence from the Definition 2.2.1 of the expectation or using
the Example 2.2.3 which follows shortly, we can claim that E[I(X > x)] = P(X
> x) and this equals 1 – F(x).
Now the desired result will follow provided that the expectation operation
“E” can be taken inside the summation in order to write 
 x)] =
 Since I(X > x) ≥ 0 w.p.1, in-
deed by applying the Monotone Convergence Theorem (refer to the Exercise
2.2.24) one can justify exchanging the expectation and the infinite summation.
The proof is now complete. !
The Exercise 2.2.21 provides an alternate sufficient condition for the con-
clusion of the Theorem 2.2.5 to hold. The Exercises 2.2.22-2.2.23 give inter-
esting applications of the Theorems 2.2.5-2.2.6.
At this point we consider some of the standard distributions from Section
1.7 and derive the expressions for their means and variances. The exact calcu-
lation of the mean and variance needs special care and attention to details. In
what follows, we initially emphasize this.
2.2.1
The Bernoulli Distribution
From (1.7.1) recall that a Bernoulli(p) random variable X takes the values 1
and 0 with probability p and 1 – p respectively. Then according to the Defini-
tions 2.2.1-2.2.3 and using (2.2.9), we have
Example 2.2.3 (Exercise 1.7.22 Continued) Consider an arbitrary ran-
dom variable Y which may be discrete or continuous. Let A be an arbitrary
event (Borel set) defined through the random variable Y. For example, the
event A may stand for the set where Y ≥ 2 or |Y| > 4 ∪ |Y| ≤ 1/2 or one of

72
2. Expectations of Functions of Random Variables
the many other possibilities. Define a new random variable X = I(A), the
indicator variable of the set A, that is:
Then X is a Bernoulli random variable, defined in (1.7.1), with p = P(A).
Hence, applying (2.2.13) we conclude that µ = P(A) and σ2 = P(A) {1 –
P(A)}. Consider selecting a random digit from 0, 1, 2, ..., 9 with equal prob-
ability and let A be the event that the random digit is divisible by 4. Then, P(A)
= 2/10 = .2 so that for the associated Bernoulli random variable X one con-
cludes that µ = .2 and σ2 = .16. !
2.2.2
The Binomial Distribution
Suppose that X has the Binomial(n, p) distribution with its pmf
     where x = 0, 1, ..., n, 0 < p < 1, given by (1.7.2). Now,
observe that x! = x(x – 1)!, n! = n(n – 1)! for x ≥ 1, n ≥ 1 and so we can write
Thus, using the Binomial theorem, we obtain
In order to evaluate the variance, let us use the Theorems 2.2.1-2.2.2 and
note that
We now proceed along the lines of (2.2.14) by omitting some of the interme-
diate steps and write
Next, we combine (2.2.15)-(2.2.16) to obtain
In other words, for the Binomial(n, p) variable, one has
f(x) =      px(1-p)n-x
n
x
(   )

2. Expectations of Functions of Random Variables
73
2.2.3
The Poisson Distribution
Suppose that X has the Poisson (λ) distribution with its pmf f(x) = e–λλx/x!
where x = 0, 1, ..., 0 < λ < ∞, given by (1.7.4). In order to find the mean and
variance of this distribution, one may proceed in the same way as in the bino-
mial case. After the dust settles, one has to find the sums of the following two
infinite series:
Now, 
 and similarly II = λ2.
Here, the exponential series expansion from (1.6.15) helps. The details are left
out as the Exercise 2.2.7. Finally, one has
2.2.4
The Uniform Distribution
Suppose that X has the Uniform (α, β) distribution with its pdf f(x) = (β – α)–1
for α < x < β, given by (1.7.12). Here α, β are two real numbers. In order to
find the mean and variance of this distribution, we proceed as follows. Let us
write
The preceding answer should not be surprising. The uniform distribution puts
the same density or weight, uniformly on each point x ∈ (α, β), so that we
should expect the midpoint of the interval (α, β) to be designated as the mean or
the center of this distribution. Next, we write
Now we combine the Theorem 2.2.2 and (2.2.19)-(2.2.20) to claim that
Finally, we summarize:
2.2.5
The Normal Distribution
First, let us consider the standard normal random variable Z having its pdf
 for –∞ < z < ∞, given by (1.7.16).

74
2. Expectations of Functions of Random Variables
In order to find the mean and variance of this distribution, we make the one-
to-one substitution u = –z along the lines of (1.7.15) when z < 0, and write
But, the two integrals I1, J1 are equal. Hence, E(Z) = 0. Next, in order to evalu-
ate E(Z2) we first write
Then, we proceed as before with the one-to-one substitution u = –z when z <
0, and rewrite
since the two integrals in the previous step are equal.
(2.2.23)
Now, in order to evaluate the last integral from (2.2.23), we further make the
one-to-one substitution υ = 1/2u2 when u > 0, and proceed as follows:
which reduces to 1/2 since 
 Refer to (1.6.20). Now, combining
(2.2.23)-(2.2.24) we see clearly that E(Z2) = 1. In other words, the mean and
variance of the standard normal variable Z are given by
The result in (2.2.22), namely that E(Z) = 0, may not surprise anyone
because the pdf ϕ(z) is symmetric about z = 0 and E(Z) is finite. Refer to the
Exercise 2.2.13 in this context.

2. Expectations of Functions of Random Variables
75
Example 2.2.4 Now, how should one evaluate, for example, E[(Z – 2)2]?
Use the Theorem 2.2.1 and proceed directly to write E[(Z – 2)2] = E[Z2] –
4E[Z] + 4 = 1 – 0 + 4 = 5. How should one evaluate, for example, E[(Z –
2)(2Z + 3)]? Again use the Theorem 2.2.1 and proceed directly to write E[(Z
– 2)(2Z + 3)] = 2E[Z2] – E[Z] –6 = 2 – 0 – 6 = –4. !
Next suppose that X has the N(µ, σ2) distribution with its pdf f(x) =
 exp{–(x – µ)2)/(2σ2)} for –∞ < x < ∞, given by (1.7.13). Here we
recall that (µ, σ2) ∈ ℜ× ℜ+.
Let us find the mean and variance of this distribution. One may be tempted
to use a result that (X – µ)/σ is standard normal and exploit the expressions
summarized in (2.2.25). But, the reader may note that we have not yet derived
fully the distribution of (X – µ)/σ. We came incredibly close to it in (1.7.18).
This matter is delegated to Chapter 4 for a fuller treatment through transforma-
tions and other techniques.
Instead we give a direct approach. In order to calculate the mean and vari-
ance of X, let us first evaluate E{(X – µ)/σ} and E{(X – µ)2/σ2} by making the
one-to-one substitution w = (x – µ)/σ in the respective integrals where x ∈ ℜ:
Now, by appealing to the Theorem 2.2.1, we can see that
since σ > 0. Again, with the same substitution we look at
Next, by appealing to the Theorem 2.2.1 again, we can write
We combine (2.2.27), (2.2.29) and summarize by saying that for the random
variable X distributed as N(µ, σ2), we have

76
2. Expectations of Functions of Random Variables
2.2.6
The Laplace Distribution
Suppose that a random variable X has the Laplace or the double exponential
pdf f(x) = 1/2β exp{– |x – θ|/β} for all x ∈ ℜ where θ ∈ ℜ and β ∈ ℜ+. This
pdf is symmetric about x = θ. In order to evaluate E(X), let us write
This reduces to θ since 
. Next, the variance, V(X)
is given by
One can see that and hence the last step of (2.2.32) can be rewritten as
with Γ(.) defined in (1.6.19). For the Laplace distribution, we then summarize
our findings as follows:
2.2.7
The Gamma Distribution
We consider a random variable X which has the Gamma (α, β) distribution
with its pdf f(x) = {βαΓ(α)}–1e–x/βxα–1 for 0 < x < ∞, given by (1.7.20). Here, we
have (α, β) ∈ ℜ+ × ℜ+. In order to derive the mean and variance of this distri-
bution, we proceed with the one-to-one substitution u = x/β along the lines of
(1.7.22) where x > 0, and express E(X) as
In other words, the mean of the distribution simplifies to

2. Expectations of Functions of Random Variables
77
Then, we express E(X2) analogously as
In other words, E(X2) simplifies to
Hence, V(X) = E(X2) – E2(X) = (α + 1)αβ2 – α2β2 = αβ2. In summary, for
the random variable X distributed as Gamma(α, β), we have
2.3 The Moments and Moment Generating
Function
Start with a random variable X and consider a function of the random variable,
g(X). Suppose that f(x) is the pmf or pdf of X where x ∈ χ is the support of the
distribution of X. Now, recall the Definition 2.2.3 for the expected value of the
random variable g(X), denoted by E[g(X)].
We continue to write µ = E(X). When we specialize g(x) = x – µ, we obvi-
ously get E[g(X)] = 0. Next, if we let g(x) = (x – µ)2, we get E[g(X)] = σ2. Now
these notions are further extended by considering two other special choices of
functions, namely, g(x) = xr or (x – µ)r for fixed r = 1, 2, ... .
Definition 2.3.1 The rth moment of a random variable X, denoted by ηr, is
given by ηr = E[Xr], for fixed r = 1, 2, ... . The first moment η1 is the mean or
the expected value µ of the random variable X.
Definition 2.3.2 The rth central moment of a random variable X around its
mean µ, denoted by µr, is given by µr = E[(X – µ)r] with fixed r = 1, 2, ... .
Recall that the first central moment µ1 is zero and the second
central moment µ2 turns out to be the variance σ2 of X,
assuming that µ and σ2 are finite.
Example 2.3.1 It is known that the infinite series 
 converges
if p > 1, and it diverges if p ≤ 1. Refer back to (1.6.12). With some fixed

78
2. Expectations of Functions of Random Variables
p > 1, let us write 
, a positive and finite real number, and
define a random variable X which takes the values i ∈ χ = {1, 2, 3, ...} such
that P(X = i) = 
, i = 1, 2, ... . This is obviously a discrete probabil-
ity mass function. Now, let us fix p = 2. Since 
 is not finite, it
is clear that η1 or E(X) is not finite for this random variable X. This example
shows that it is fairly simple to construct a discrete random variable X for
which even the mean µ is not finite. !
The Example 2.3.1 shows that the moments of a random
variable may not be finite.
Now, for any random variable X, the following conclusions should be fairly
obvious:
The part (iii) in (2.3.1) follows immediately from the parts (i) and (ii).
The finiteness of the rth moment ηr of X does not necessarily
imply the finiteness of the sth moment ηs of X when s > r.
Look at the Example 2.3.2.
Example 2.3.2 (Example 2.3.1 Continued) For the random variable X de-
fined in the Example 2.3.1, the reader should easily verify the claims made in
the adjoining table:
Table 2.3.1. Existence of Few Lower Moments
But Non-Existence of Higher Moments
p
Finite ηr
Infinite ηr
2
none
r = 1, 2, ...
3
r = 1
r = 2, 3, ...
4
r = 1,2
r = 3, 4, ...
k
r = 1, ..., k – 2
r = k – 1, k, ...
The Table 2.3.1 shows that it is a simple matter to construct discrete
random variables X for which µ may be finite but its variance σ2 may not
be finite, or µ and σ2 both could be finite but the third moment η3 may not
be finite, and so on. !

2. Expectations of Functions of Random Variables
79
It will be instructive to find simple examples of continuous random vari-
ables and other discrete random variables with interesting features analogous
to those cited in the Example 2.3.2. These are left as Exercises 2.3.8-2.3.10.
When the Definition 2.2.3 is applied with g(x) = etx, one comes up with a
very useful and special function in statistics. Look at the following definition.
Definition 2.3.3 The moment generating function (mgf) of a random vari-
able X, denoted by MX(t), is defined as
provided that the expectation is finite for | t | < a with some a > 0.
As usual, the exact expression of the mgf MX(t) would then be derived
analytically using one of the following expressions:
The function MX(t) bears the name mgf because one can derive all the mo-
ments of X by starting from its mgf. In other words, all moments of X can be
generated from its mgf provided that the mgf itself is finite.
Theorem 2.3.1 If a random variable X has a finite mgf MX(t), for | t | < a
with some a > 0, then the rth moment ηr of X, given in the Definition 2.3.1, is
the same as dr MX(t)/dtr when evaluated at t = 0.
Proof Let us first pretend that X is a continuous random variable so that
MX(t) = ∫χ etx f(x)dx. Now, assume that the differentiation operator of MX(t) with
respect to t can be taken inside the integral with respect to x. One may refer to
(1.6.16)-(1.6.17) for situations where such interchanges are permissible. We
write
and then it becomes clear that dMX(t)/dt when evaluated at t = 0 will coincide
with ∫χ xf(x)dx which is η1 (= µ). Similarly let us use (2.3.3) to claim that
Hence, d2MX(t)/dt2 when evaluated at t = 0 will coincide with ∫χ x2 f(x)dx
which is η2. The rest of the proof proceeds similarly upon successive differ-
entiation of the mgf MX(t). A discrete scenario can be handled by replacing
the integral with a sum. !

80
2. Expectations of Functions of Random Variables
A finite mgf MX(t) determines a unique infinite sequence
of moments of a random variable X.
Remark 2.3.1 Incidentally, the sequence of moments {ηr = E(Xr): r = 1,
2, ...} is sometimes referred to as the sequence of positive integral moments
of X. The Theorem 2.3.1 provides an explicit tool to restore all the positive
integral moments of X by successively differentiating its mgf and then letting
t = 0 in the expression. It is interesting to note that the negative integral mo-
ments of X, that is when r = –1, –2, ... in the Definition 2.3.1, are also hidden
inside the same mgf. These negative moments of X can be restored by imple-
menting a process of successive integration of the mgf, an operation which is
viewed as the opposite of differentiation. Precise statements of the regularity
conditions under which the negative moments of X can be derived with this
approach are found in Cressie et al. (1981).
The positive integral moments of X can be found by
successively differentiating its mgf with respect to t
and letting t = 0 in the final derivative.
The following result is very useful and simple. We leave its proof as an exer-
cise. There will be ample opportunities to use this theorem in the sequel.
Theorem 2.3.2 Suppose that a random variable X has the mgf MX(t), for |
t | < a with some a > 0. Let Y = cX + d be another random variable where c, d
are fixed real numbers. Then, the mgf MY(t) of Y is related to the mgf MX(t) as
follows:
In the following subsections, we show the derivations of the mgf in the
case of a few specific distributions. We also exhibit some immediate applica-
tions of the Theorem 2.3.1.
2.3.1
The Binomial Distribution
Suppose that X has the Binomial (n, p) distribution with its pmf f(x) = px 
 (1
– p)n–x for x = 0, 1, ..., n and 0 < p < 1, given by (1.7.2). Here, for all fixed t ∈
ℜ we can express MX(t) as

2. Expectations of Functions of Random Variables
81
Observe that log(MX(t)) = nlog{(1 – p) + pet} so that we obtain
Next, using the chain rule of differentiation, from (2.3.6) we can write
From (2.3.7) it is obvious that dMX(t/dt, when evaluated at t = 0, reduces to
We found the mean of the distribution in (2.2.14). Here we have another
way to check that the mean is np.
Also, the Theorem 2.3.1 would equate the expression of E(X2) with d2 MX(t)/
dt2 evaluated at t = 0. From (2.3.6) it is obvious that d2 MX(t)/dt2, when evalu-
ated at t = 0, should lead to η2, that is
Hence, one has V(X) = E(X2) – µ2 = np – np2 + n2p2 – (np)2 = np–np2 = np(1
– p). This matches with the expression of σ2 given in (2.2.17).
2.3.2
The Poisson Distribution
Suppose that X has the Poisson(λ) distribution with its pmf f(x) = e–λ λx/x! for x
= 0, 1, ... and 0 < λ < ∞, given by (1.7.4). Here, for all fixed t ∈ ℜ we can
express MX(t) as
Observe that log(MX(t)) = –λ + λet so that one has

82
2. Expectations of Functions of Random Variables
Next, using the chain rule of differentiation, from (2.3.11) one obtains
Since MX(0) = 1, from (2.3.11) it is obvious that dMX(t)/dt, when evaluated
at t = 0, reduces to η1 = λ which matches with the expression of µ found earlier
in (2.2.18). Here is another way to check that the mean of the distribution is λ.
Also, the Theorem 2.3.1 would equate the expression of η2 or E(X2) with
d2 MX(t)/dt2, evaluated at t = 0. From (2.3.12) it should become obvious that
d2 MX(t)/dt2, evaluated at t = 0, should be η2 = λ + λ2 since MX(0) = 1. Hence,
we have V(X) = E(X2) – µ2 = λ + λ2 – λ2 = λ. This again matches with the
expression of σ2 given in (2.2.18).
2.3.3
The Normal Distribution
Let us first suppose that Z has the standard normal distribution with its pdf
 exp(–z2/2) for – ∞ < z < ∞, given by (1.7.16). Now, for all
fixed t ∈ ℜ we can express the mgf as
Observe that the form of the function h(u) used in the last step in (2.3.13)
resembles the pdf of a random variable having the N(t, 1) distribution for all
fixed t ∈ Β. So, we must have 
 h(u)du = 1 for all fixed t ∈ Β. In other
words, (2.3.13) leads to the following conclusion:
Now, suppose that X is distributed as N(µ, σ2). We can write X = σY + µ
where Y = (X – µ)/σ. We can immediately use the Theorem 2.3.2 and claim that
Now, by substituting y = (x – µ)/σ in the integral, the expression of MY(t) can
be found as follows:

2. Expectations of Functions of Random Variables
83
which was exactly the same integral evaluated in (2.3.13). In other words,
from (2.3.14) we can immediately claim that MY(t) = exp(1/2t2). Thus, one
can rewrite (2.3.15) as follows:
Now, log(MX(t)) = tµ + 1/2t2σ2 so that dMX(t)/dt = (µ + tσ2) MX(t). Hence,
dMX(t)/dt, when evaluated at t = 0, reduces to η1 = µ because one has
MX(0) = 1.
Also, using the chain rule of differentiation we have d2MX(t)/dt2 = σ2MX(t) +
(µ + tσ2)2 MX(t) so that d2 MX(t)/dt2, when evaluated at t = 0, reduces to η2 = σ2
+ µ2. In view of the Theorem 2.3.1, we can say that µ is the mean of X and V(X)
= E(X2) – µ2 = σ2 + µ2 – µ2 = σ2. These answers were verified earlier in (2.2.30).
One can also easily evaluate higher order derivatives of the mgf. In view of
the Theorem 2.3.1 one can claim that dk MX(t)/dtk with k = 3 and 4, when
evaluated at t = 0, will reduce to η3 or E(X3) and η4 or E(X4) respectively. Then,
in order to obtain the third and fourth central moments of X, one should pro-
ceed as follows:
by the Theorem 2.2.1. Similarly,
Look at the related Exercises 2.3.1-2.3.2.
Example 2.3.3 Suppose that Z is the standard normal variable. How should
one directly derive the expression for E(|Z|)? Here, the mgf of Z from (2.3.14) is
not going to be of much help. Let us apply the Definition 2.2.3 and proceed as
follows. As before, we substitute u = –z when z < 0, and express E(|Z|) as
In the last step of (2.3.19), one immediately realizes that the two integrals are
really the same that is, 
 and
hence one has
α

84
2. Expectations of Functions of Random Variables
Next, along the lines of (2.2.22)-(2.2.24) we can rewrite the integral on the
rhs of (2.3.20) as a gamma integral to finally claim that
Also look at the closely related Exercises 2.3.3-2.3.4. !
Example 2.3.4 Suppose that Z is the standard normal variable. How should
one directly derive the expression for E(e|Z|)? Here, the mgf of Z from (2.3.14)
is not going to be of much help. Let us apply the Definition 2.2.3 and proceed
as follows:
Look at the related Exercise 2.3.16 where we ask for the mgf of |Z|. !
2.3.4
The Gamma Distribution
Let us suppose that a random variable X has the Gamma(α, β) distribution
with its pdf 
 where 0 < x < ∞ and (α, β) ∈ ℜ+
× ℜ+, given by (1.7.20). Now, let us denote β* = β(1 – βt)–1 for all t < β–1 so
that β* is positive. Thus, we can express MX(t) as
Observe that the function h(u) used in the last step in (2.3.22) resembles the
pdf of a random variable having the Gamma(α, β*) distribution for u ∈ Β+
where β* is positive. So, we must have 
 In other words,
(2.3.22) leads to the following conclusion.
Now, log(MX(t)) = –αlog(1 – βt) so that one can immediately have dMX(t)/
dt = αβ(1 – βt)–1 MX(t). Hence, dMX(t)/dt, when evaluated at t = 0, re-
duces to αβ because MX(0) = 1. Next, we use the chain rule of
differentiation in order to write d2 MX(t)/dt2 = α(1 + α)β2(1 – βt)–2 MX(t)

2. Expectations of Functions of Random Variables
85
so that d2 MX(t)/dt2, when evaluated at t = 0, reduces to α(1 + α)β2. In view
of the Theorem 2.3.1, we can say that αβ is the mean of X and V(X) = E(X2)
– µ2 = α(1 + α)β2 – µ2 = αβ2. These same answers were derived earlier in
(2.2.39).
How should one derive the expression for E(Xr) when r is arbitrary? We
no longer restrict r to be a positive integer. Now, the expression of the mgf of
X given by (2.3.23) may not be of immediate help. One may pursue a direct
approach along the lines of (2.2.35)-(2.2.38). Let us write
and observe that the integrand is the kernel (that is the part involving x only) of
the gamma pdf provided that α + r > 0. In other words, we must have
Next, we combine (2.3.24)-(2.3.25) to conclude that
regardless of what r is. When r is positive, the expression for E(Xr) stays the
same, but no additional condition on α is warranted. The result quoted in (2.3.26)
would be very helpful in the sequel.
Suppose that X is distributed as Gamma(α, β). Then, E(Xr) is finite
if and only if α > –r. Look at (2.3.24)-(2.3.26).
Special Case 1: The Exponential Distribution
The exponential distribution was defined in (1.7.23). This random variable
X has the pdf f(x) = β–1 e–x/β I(x > 0) which is equivalent to saying that X is
distributed as Gamma(1, β) with β ∈ ℜ+. In this special situation, we can
summarize the following results.
These can be checked out easily using (2.3.23) as well as (2.3.26).
Special Case 2: The Chi-square Distribution
The Chi-square distribution was also introduced in the Section 1.7.
This random variable X has the pdf f(x) = {2ν/2Γ(ν/2)}–1e–x/2x(ν/2)–1 I(x > 0)
which is equivalent to saying that X is distributed as Gamma(ν/2, 2) with

86
2. Expectations of Functions of Random Variables
positive integral values of ν. Recall that ν is referred to as the degree of
freedom and X is often denoted by eν
2. Then, in this special situation, we can
summarize the following results.
These can again be checked out easily from (2.3.23) as well as (2.3.26).
2.4 Determination of a Distribution via MGF
Next, we emphasize the role of a moment generating function in uniquely
determining the probability distribution of a random variable. We state an im-
portant and useful result below. Its proof is out of scope for this book. We
will have ample opportunities to rely upon this result in the sequel.
Theorem 2.4.1 Let M(t) be a finite mgf for | t | < a with some a > 0. Then
M(t) corresponds to the mgf associated with the probability distribution of a
uniquely determined random variable.
This simple looking result however has deep implications. Suppose that U
is a discrete random variable and assume that somehow one knows the ex-
pression of its mgf MU(t). Now if MU(t), for example, looks exactly like the
mgf of a Binomial random variable, then we can conclude that U indeed has
the Binomial distribution. But on the other hand if MU(t) looks exactly like the
mgf of a Poisson random variable, then again we will conclude that U has the
Poisson distribution, and so on. If U has a continuous distribution instead and
MU(t), for example, looks exactly like the mgf of a Gamma random variable,
then U indeed must have a Gamma distribution. These conclusions are sup-
ported by the Theorem 2.4.1. We will exploit such implications in the sequel.
Example 2.4.1 Suppose that a random variable X takes the possible val-
ues 0, 1 and 2 with the respective probabilities 1/8, 1/4 and 5/8. The mgf of
X is obviously given by MX(t) = 1/8 + 1/4 et + 5/8 e2t = 1/8(1 + 2et + 5e2t).
Observe how the mgf of a discrete random variable is formed and how easy
it is to identify the probability distribution by inspecting the appearance of
the terms which together build up the function MX(t). Now suppose that we
have a random variable U whose mgf MU(t) = 1/5(1 + et + 3e2t)e–t. What is
the probability distribution of U? Let us rewrite this mgf as .2e–t + .2 + .6et.
We can immediately claim that U takes the possible values –1, 0 and 1 with
the respective probabilities .2, .2 and .6. In view of the Theorem 2.4.1, we

2. Expectations of Functions of Random Variables
87
can claim that the random variable U and its distribution we have indicated are
unique. !
Example 2.4.2 Suppose that U is a random variable such that MU(t) = 1/
16(1 + et)4. We can rewrite MU(t) = (1/2 + 1/2 et)4 which agrees with the ex-
pression of MX(t) given by (2.3.5) where n = 4 and p = 1/2. Hence U must be
distributed as Binomial(4, 1/2) by the Theorem 2.4.1. !
Example 2.4.3 Suppose that U is a random variable such that MU(t) =
exp{πt2} which agrees with the expression of MX(t) given by (2.3.16) with µ =
0, σ2 = 2π. Hence, U must be distributed as N(0, 2π). !
A finite mgf determines the distribution uniquely.
Before we move on, let us attend to one other point involving the moments.
A finite mgf uniquely determines a probability distribution, but on the other
hand, the moments by themselves alone may not be able to identify a unique
random variable associated with all those moments. Consider the following
example.
Example 2.4.4 Rao (1973, p.152) had mentioned the construction of the
following two pdf’s, originally due to C. C. Heyde. Consider two positive
continuous random variables X and Y whose pdf’s are respectively given by
where c is a fixed number, –1 ≤ c ≤ 1 and c ≠ 0. We leave it as the Exercise
2.4.4 to show that E[Xr] = E[Yr] for all r = 1, 2, ... . But, certainly we can
Figure 2.4.1. The Two PDF’s from (2.4.1): (a) f(x) (b) g(y)
Where c = 1/2

88
2. Expectations of Functions of Random Variables
claim that X and Y have different probability distributions because their two
pdf’s are obviously different when c ≠ 0. In the Figure 2.4.1, these two pdf’s
have been plotted when c = 1/2. Here all the moments of X are finite, but in the
Exercise 2.4.5 one would verify that the mgf for the random variable X is not
finite. In another related Exercise 2.4.6, examples of other pairs of random
variables with analogous characteristics can be found. !
Finite moments alone may not determine a distribution
uniquely. The Example 2.4.4 highlights this point.
Depending on the particular situation, however, an infinite sequence of all
finite moments may or may not characterize a probability distribution uniquely.
Any elaborate discussion of such issues would be out of scope of this book. The
readers may additionally consult the Section 3, Chapter 7 in Feller (1971) on
“Moment Problems.” Chung (1974) also includes relevant details.
2.5 The Probability Generating Function
We have seen that a mgf generates the moments. Analogously, there is another
function which generates the probabilities. Suppose that X is a non-negative
random variable. A probability generating function (pgf) of X is defined by
The explicit form of a pgf is often found when the random variable X is
integer-valued. From this point onward, let us include only non-negative inte-
ger-valued random variables in this discussion.
Why this function PX(t) is called a pgf should become clear once we write
it out fully as follows:
where pi = P(X = i) for i = 1, 2, ... . We see immediately that the coefficient
of ti is pi, the probability that X = i for i = 1, 2, ... .
In the light of the Theorem 2.3.1 one can justify the following result:

2. Expectations of Functions of Random Variables
89
One should verify that
where PX(t) is the pgf of X defined in (2.5.1).
2.6 Exercises and Complements
2.2.1 A game consists of selecting a three-digit number. If one guesses the
number correctly, the individual is paid $800 for each dollar the person bets.
Each day there is a new winning number. Suppose that an individual bets $1
each day for a year, then how much money can this individual be expected to
win?
2.2.2 An insurance company sells a life insurance policy with the face value
of $2000 and a yearly premium of $30. If 0.4% of the policy holders can be
expected to die in the course of a year, what would be the company’s expected
earnings per policyholder in any year?
2.2.3 (Exercise 1.5.1 Continued) Suppose that a random variable X has the
following pmf:
X values:
–2
0
1
3
8
Probabilities:
.2
p
.1
2p
.4
where p ∈(0, .1]. Evaluate µ and σ for the random variable X.
2.2.4 (Exercise 1.5.2 Continued) Suppose that a random variable X has the
following pmf:
X values :
–2
0
3
    5
8
Probabilities:
.2
p
.1
.3 – p
.4
where p ∈ (0, .3). With a fixed but arbitrary p ∈ (0, .3), find the expressions
of µ and σ for the random variable X depending upon p. Is there some p ∈ (0,
.3) for which the V(X) is minimized?
2.2.5 Consider a random variable X which has the following discrete uni-
form distribution along the lines of (1.7.11):
Derive the explicit expressions of µ and σ for this distribution. {Hint: Use the
first two expressions from (1.6.11).}

90
2. Expectations of Functions of Random Variables
2.2.6 Let X be a discrete random variable having the Geometric(p) distri-
bution whose pmf is f(x) = p(1 – p)x–1 for x = 1, 2, 3, ... and 0 < p < 1, given
by (1.7.7). Derive the explicit expressions of µ and σ for this distribution. An
alternative way to find µ has been provided in the Exercise 2.2.23. {Hint: Use
the expressions for sums of the second and third infinite series expansions
from (1.6.15).}
2.2.7 Suppose that X has the Poisson(λ) distribution with 0 < λ< ∞. Show
that µ = σ2 = λwhich would verify (2.2.18). See Section 2.2.3 for some
partial calculations.
2.2.8 For the negative binomial pmf defined by (1.7.10), find the expres-
sions for the mean and variance.
2.2.9 Let c be a positive constant and consider the pdf of a random variable
X given by
(i)
Explicitly evaluate c, µ and σ;
(ii)
Evaluate E{(1 – X)–1/2};
(iii)
Evaluate E{X3(1 – X)3/2}.
{Hints: In parts (ii)-(iii) first derive the expressions using direct integration
techniques. Alternatively reduce each expected value in parts (ii)-(iii) as a com-
bination of the appropriate beta functions defined in (1.6.25).}
2.2.10 Let c be a constant and consider the pdf of a random variable X given
by
Explicitly evaluate c, µ and σ.
2.2.11 Suppose that X has the Uniform(0, 1) distribution. Evaluate
(i)
E[X–1/2(1 – X)10];
(ii)
E[X3/2(1 – X)5/2];
(iii)
E[(X2 + 2X1/2 – 3X5/2)(1 – X)10].
{Hint: First express these expectations as the integrals using the Definition
2.2.3. Can these integrals be evaluated using the forms of beta integrals de-
fined in (1.6.25)?}
2.2.12 Consider a random variable X having the pdf f(x) = c(x – 2x2 + x3)
I(0 < x < 1) where c is a positive number. Explicitly evaluate µ and σ for this
distribution. {Hint: In order to find c, should one match this f(x) with a beta
density?}

2. Expectations of Functions of Random Variables
91
2.2.13 Suppose that X is a continuous random variable having its pdf f(x)
with the support χ = (a, b), –∞ ≤ a < b ≤ ∞. Additionally suppose that f(x) is
symmetric about the point x = c where a < c < b. Then, show that both the mean
and median of this distribution will coincide with c as long as the mean is finite.
{Hint: First show that the point of symmetry c for f(x) must be the midpoint
between a and b. Consider splitting the relevant integrals as the sum of the
integrals over the intervals (–∞, a), (a, c), (c, b) and (b, ∞). A word of caution
may be in order. A pdf may be symmetric about c, but that may not imply µ = c
simply because µ may not even be finite. Look at the Exercise 2.3.8.}
2.2.14 Suppose that a random variable X has the Rayleigh distribution, that
is its pdf is given by
where θ(> 0). Evaluate µ and σ for this distribution. {Hint: Try the substitution
u = x2/θ during the integration. Is this substitution one-to-one?}
2.2.15 Suppose that a random variable X has the Weibull distribution, that
is its pdf is given by
where α(> 0) and β(> 0). Evaluate µ and σ for this distribution. {Hint: Try the
substitution u = [x/β]α during the integration. Is this substitution one-to-one?}
2.2.16 Suppose that we have a random variable X which has the lognormal
pdf 
 for 0 < x < ∞, given by (1.7.27).
Derive the expression of the mean and variance. {Hint: While considering the
integrals over (0, ∞), try the substitution y = log(x) and look at the integrands
as functions of the new variable y which varies between (–∞, ∞). Try to
express these integrands in the variable y to resemble the pdf’s of normal
random variables.}
2.2.17 For the negative exponential distribution with its pdf f(x) = β–1e–(x–
γ)/β for γ < x < ∞, defined in (1.7.36), show that µ = γ + β and σ = β.
2.2.18 Prove Theorem 2.2.3. {Hint: First suppose that f(x) is the pdf of X
with its support χ = [a, b]. To prove part (i), multiply each x throughout the
interval [a, b] by f(x) to write af(x) ≤ xf(x) ≤ bf(x) and then integrate all sides
(with respect to x) from a to b. The result will follow since 
.
The discrete case can be handled analogously. In part (ii), note that Y – X ≥ 0
w.p.1 and hence the result follows from part (i).}

92
2. Expectations of Functions of Random Variables
2.2.19 Prove Theorem 2.2.4. {Hint: By the Theorem 2.2.1, note that E(aX
+ b) = aE(X) + b. Next, apply the Definition 2.2.2 and write V(aX + b) =
E{(aX + b) – [aE(X) + b]}2 = E{[aX – aE(X)]2} = a2E{[X – E(X)]2} which
simplifies to a2V(X).}
2.2.20 Suppose that X is a random variable whose second moment is
finite. Let g(a) = E[(X – a)2], a ∈ ℜ. Show that g(a) is minimized with respect
to a when a = µ, the mean of X. {Hint: Write out E[(X – a)2] = E[X2] –
2aE[X] + a2. Treat this as a function of a single variable a and minimize this
with respect to a. Refer to (1.6.27).}
2.2.21 In the Theorem 2.2.5, we had assumed that 
for the df F(x) of a non-negative continuous random variable X. Write x{1 –
F(x)} = {1 – F(x)}/x–1 and then it is clear that 
 unfortu-
nately takes the form of 0/0. Hence, by applying the L’Hôpital’s rule (refer to
(1.6.29)) show that we may assume instead 
 where f(x) is
the associated pdf.
2.2.22 (Exercise 2.2.21 Continued) Suppose that X has the exponential
distribution with its pdf f(x) = β–1e–x/β for x > 0, β > 0. Obtain the expression
of µ by applying the integral expression found in the Theorem 2.2.5. Is the
sufficient condition 
 satisfied here? How about the suffi-
cient condition 
?
2.2.23 This exercise provides an application of the Theorem 2.2.6. Sup-
pose that X has the Geometric(p) distribution, defined in (1.7.7), with its pmf
f(x) = pqx–1, x = 1, 2, ..., 0 < p < 1, q = 1 – p.
(i)
Show that F(x) = 1 – qx, x = 1, 2, ...;
(ii)
Perform the infinite summation 
 using the ex-
pression for 
, and hence find µ
A direct approach using the Definition 2.2.1 to find µ was sought out in
the Exercise 2.2.6.
2.2.24 (Monotone Convergence Theorem) Let {Xn; n = 1} be a se-
quence of non-negative random variables. Then, show that
{Note: We state this result here for completeness. It has been utilized in the
proof of the Theorem 2.2.6. It may be hard to prove this result at the level of
this book. See also the Exercise 5.2.22 (a).}
2.3.1 Suppose that Z has the standard normal distribution. By directly
evaluating the appropriate integrals along the lines of (2.2.22)-(2.2.24) and
Example 2.3.4 show that

2. Expectations of Functions of Random Variables
93
(i)
E(Zr) = 0 for any odd integer r > 0;
(ii)
E(Zr) = π–1/22r/2Γ(1/2r + 1/2) for any even integer r > 0;
(iii)
µ4 = 3 using part (ii).
2.3.2 (Exercise 2.3.1 Continued) Suppose that a random variable X has the
N(µ, σ2) distribution. By directly evaluating the relevant integrals, show that
(i)
E{(X – µ)r} = 0 for all positive odd integer r;
(ii)
E{(X – µ)r} = π–1/2σr2r/2Γ (1/2r + 1/2) for all positive
even integer r;
(iii)
the fourth central moment µ4 reduces to 3σ4.
2.3.3 Suppose that Z has the standard normal distribution. Along the lines
of the Example 2.3.4, that is directly using the techniques of integration, show
that
for any number r > 0.
2.3.4 (Exercise 2.3.3 Continued) Why is it that the answer in the Exercise
2.3.1, part (ii) matches with the expression of E(|Z|r) given in the Exercise
2.3.3? Is it possible to somehow connect these two pieces?
2.3.5 (Exercise 2.2.13 Continued) Suppose that X is a continuous random
variable having its pdf f(x) with the support χ = (a, b), –∞ = a < b = ∞.
Additionally suppose that f(x) is symmetric about the point x = c where a < c
< b. Then, show that E{(X – c)r} = 0 for all positive odd integer r as long as
the rth order moment is finite. {Hint: In principle, follow along the derivations
in the part (i) in the Exercise 2.3.2.}
2.3.6 (Exercise 2.3.1 Continued) Derive the same results stated in the
Exercise 2.3.1 by successively differentiating the mgf of Z given by (2.3.14).
2.3.7 (Exercise 2.3.2 Continued) Suppose that X has the N(µ, σ2) distribu-
tion. Show that µ3 = 0 and µ4 = 3σ4 by successively differentiating the mgf of
X given by (2.3.16).
2.3.8 Consider the Cauchy random variable X, defined in (1.7.31), whose
pdf is given by f(x) = π–1(1 + x2)–1 for x ∈ ℜ. Show that E(X) does not exist.
2.3.9 Give an example of a continuous random variable, other than Cauchy,
for which the mean µ is infinite. {Hint: Try a continuous version of the Ex-
ample 2.3.1. Is f(x) = x–2I(1 < x < ∞) a genuine pdf? What happens to the
mean of the corresponding random variable?}
2.3.10 Give different examples of continuous random variables for which

94
2. Expectations of Functions of Random Variables
(i)
the mean µ is finite, but the variance σ2 is infinite;
(ii)
the mean µ and variance σ2 are both finite, but µ3 is infinite;
(iii)
µ10 is finite, but µ11 is infinite.
{Hint: Extend the ideas from the Example 2.3.2 and try other pdf’s similar
to that given in the “hint” for the Exercise 2.3.9.}
2.3.11 For the binomial and Poisson distributions, derive the third and
fourth central moments by the method of successive differentiation of their
respective mgf’s from (2.3.5) and (2.3.10).
2.3.12 Consider the expression of the mgf of an exponential distribution
given in (2.3.27). By successively differentiating the mgf, find the third and
fourth central moments of the exponential distribution.
2.3.13 Consider the expression of the mgf of a Chi-square distribution
given in (2.3.28). By successively differentiating the mgf, find the third and
fourth central moments of the Chi-square distribution.
2.3.14 (Exercise 2.2.16 Continued) Suppose that X has the lognormal pdf
for 0 < x < ∞, given by (1.7.27).
Show that the rth moment ηr is finite and find its expression for each r = 1, 2,
3, ... . {Hint: Substitute y = log(x) in the integrals and see that the integrals
would resemble the mgf with respect to the normal pdf for some appropriate
values t.}
2.3.15 Suppose that a random variable X has the Laplace or the double
exponential pdf f(x) = 1/2β exp{– |x|/β} for all x ∈ ℜ where β ∈ ℜ+. For this
distribution.
(i)
derive the expression of its mgf;
(ii)
derive the expressions of the third and fourth central moments.
{Hint: Look at the Section 2.2.6. Evaluate the relevant integrals along the
lines of (2.2.31)-(2.2.33).}
2.3.16 Suppose that Z has the standard normal distribution. Along the lines
of the Example 2.3.4, derive the expression of the mgf M|Z|(t) of the random
variable |Z| for t belonging to ℜ.
2.3.17 Prove the Theorem 2.3.2.
2.3.18 (Exercise 2.2.14 Continued) Suppose that we have a random vari-
able X which has the Rayleigh distribution, that is its pdf is given by f(x) = 2θ–
1xexp(–x2/θ)I(x > 0) where θ(> 0). Evaluate E(Xr) for any arbitrary but fixed
r > 0. {Hint: Try the substitution u = x2/θ during the integration.}
2.3.19 (Exercise 2.2.15 Continued) Suppose that we have a random vari-
able X which has the Weibull distribution, that is its pdf is given by

2. Expectations of Functions of Random Variables
95
f(x) = αβ–α xα–1 exp(–[x/β]α)I(x > 0) where α(> 0) and β(> 0). Evaluate E(Xr)
for any arbitrary but fixed r > 0. {Hint: Try the substitution u = [x/β]α during
the integration.}
2.4.1 In this exercise, you are given the expressions of the mgf of differ-
ent random variables. In each case, (i) identify the random variable either by
its standard name or by explicitly writing down its pmf or the pdf, (ii) find the
values of both µ and σ for the random variable X.
(i)
MX (t) = e5t, for t ∈ ℜ;
(ii)
MX (t) = 1, for t ∈ ℜ;
(iii)
MX (t) = 1/2(1 + et), for t ∈ ℜ;
(iv)
MX (t) = 1/3(e–2t + 1 + et), for t ∈ ℜ;
(v)
MX (t) = 1/10(e2t + 3 + 6e4t), for t ∈ ℜ;
(vi)
MX (t) = 1/2401(3et + 4)4, for t ∈ ℜ.
{Hint: Think of a discrete random variable and how one actually finds its
mgf. Then, use Theorem 2.4.1.}
2.4.2 In this exercise, you are given the expressions for the mgf of a
random variable X. In each case, (i) identify the random variable X either by
its standard name or by explicitly writing down its pmf or the pdf, (ii) find the
values of both µ and σ for the random variable X.
(i)
E[etX] = e25t2, for t ∈ ℜ;
(ii)
E[etX] = et2, for t ∈ ℜ;
(iii)
MX (t) = (1 – 6t + 9t2)–2, for t < 1/3.
{Hint: Think of a continuous random variable from the Section 1.7 and
see if that helps in each part. Then, use Theorem 2.4.1.}
2.4.3 A random variable X has its mgf given by
Find µ and σ2. Can the distribution of X be identified?
2.4.4 (Example 2.4.4 Continued) Show that E(Xr)=E(Yr) for all r=1,
2, ... where the pdf of X is f(x)=(2π)–1/2x–1 exp[–1/2(log(x))2]I(x > 0) and
that of Y is g(y) = f(y)[1+csin(2πlog(y))]I(y > 0). Here, c is a fixed num-
ber, –1 ≤ c ≤ 1 and c ≠ 0. {Hint: Note that the pdf f(x) actually matches
with the lognormal density from (1.7.27). When handling g(y), first show
that the multiplier of f(y) is always positive. Then while evaluating
g(y)dy, all one needs to show is that 
 In

96
2. Expectations of Functions of Random Variables
the process of evaluating the relevant integrals, try the substitution u = log(x).}
2.4.5 (Exercise 2.4.4 Continued) Suppose a random variable X has the
lognormal pdf f(x) = (2π)–1/2x–1 exp[–1/2(log(x))2]I(x > 0). Show that the mgf
of X does not exist. {Hint: In the process of evaluating the relevant integral,
one may try the substitution u = log(x).}
2.4.6 (Exercise 2.4.4 Continued) Consider the two pdf’s f(x) and g(y)
with x > 0, y > 0, as defined in the Exercise 2.4.4 Let a(x) with x > 0 be any
other pdf which has all its (positive integral) moments finite. For example,
a(x) may be the pdf corresponding to the Gamma(α, β) distribution. On the
other hand, there is no need for a(x) to be positive for all x > 0. Consider now
two non-negative random variables U and V with the respective pdf’s f0(u)
and g0(v) with u > 0, v > 0 where
Show that U and V have the same infinite sequence of moments but they
naturally have different distributions. We plotted the two pdf’s f0(u) and g0(v)
with c = p = 1/2 and a(x) = 1/10e–x/10. In comparison with the plots given in
the Figure 2.4.1, the two present pdf’s appear more skewed to the right. The
reader should explore different shapes obtainable by using different choices
of the function a(x) and the numbers c and p. In view of the Exercise 2.4.5,
one should check that neither U and V has a finite mgf.
Figure 2.6.1. The PDF’s from the Exercise 2.4.6: (a) f0(u) (b) g0(v)
Where c = p = 1/2, a(x) = 1/10 exp(–x/10)
2.4.7 (Exercise 2.2.5 Continued) Consider a random variable X which has
the following discrete uniform distribution along the lines of (1.7.11):

2. Expectations of Functions of Random Variables
97
Derive the explicit expression of the mgf of X and hence derive µ and σ for
this distribution. {Hint: Note that E(etx) = Σi=1et11/n = 1/n et[1+et+
. Can this be simplified using the geometric progression?}
2.4.8 A random variable X has its pdf
where c(> 0) and d are appropriate constants. We are also told that E(X) =
–3/2.
(i)
Determine c and d;
(ii)
Identify the distribution by its name;
(iii)
Evaluate the third and fourth central moments of X.
2.4.9 A random variable X has its pdf
where c(> 0) and d(> 0) are appropriate constants. We are also told that E(X)
= 10.5 and V(X) = 31.5.
(i)
Determine c and d;
(ii)
Identify the distribution by its name;
(iii)
Evaluate the third and fourth moments, η3 and η4, for X.
2.4.10 A random variable X has its mgf given by
Evaluate P(X = 4 or 5). {Hint: What is the mgf of a geometric random vari-
able?}
2.5.1 Verify (2.5.4).
2.5.2 Let X be a discrete random variable having the Geometric(p) distri-
bution whose pmf is f(x) = P(X = x) = p(1 – p)x–1 for x = 1, 2, 3, ... and 0 <
p < 1, given by (1.7.7). Show that pgf of X is given by
Evaluate the first two factorial moments of X and hence find the expressions
of µ and σ.
2.5.3 Let X be a discrete random variable having the Poisson(λ) distribu-
tion with 0 < λ < ∞. Derive the expression of the pgf of X. Hence, evaluate the
kth order factorial moment of X and show that it reduces to λk.
2.5.4 Let X be a discrete random variable having the Binomial(n, p)
distribution with 0 < p < 1. Derive the expression of the pgf of X. Hence,
n

98
2. Expectations of Functions of Random Variables
evaluate the kth order factorial moment of X and show that it reduces to n(n –
1)...(n – k + 1)pk.
2.5.5 Consider the pgf PX(t) defined by (2.5.1). Suppose that PX(t) is
twice differentiable and assume that the derivatives with respect to t and
expectation can be interchanged. Let us denote 
 and
. Then, show that
(i)
(ii)
2.5.6 Suppose that a random variable X has the Beta(3, 5) distribution.
Evaluate the third and fourth factorial moments of X. {Hint: Observe that
E[X(X – 1)(X – 2)] = E[X(X – 1)2 – X(X – 1)] = E[X(X – 1)2] – E[X(X – 1)].
Write each expectation as a beta integral and evaluate the terms accordingly to
come up with the third factorial moment. The other part can be handled by
extending this idea.}

3
Multivariate Random Variables
3.1 Introduction
Suppose that we draw two random digits, each from the set {0, 1, ...,
9}, with equal probability. Let X1, X2 be respectively the sum and the differ-
ence of the two selected random digits. Using the sample space technique,
one can easily verify that P(X1 = 0) = 1/100, P(X1 = 1) = 2/100, P(X1 = 2)
= 3/100 and eventually obtain the distribution, namely P(X1 = i) for i = 0, 1,
..., 18. Similarly, one can also evaluate P(X2 = j) for j = –9, –8, ..., 8, 9.
Now, suppose that one has observed X1 = 3. Then there is no chance for X2
to take a value such as 2. In other words, the bivariate discrete random
variable (X1, X2) varies together in a certain way. This sense of joint varia-
tion is the subject matter of the present chapter.
Generally speaking, we may have k(≥ 2) real valued random variables X1,
..., Xk which vary individually as well as jointly. For example, during the
“health awareness week” we may consider a population of college students
and record a randomly selected student’s height (X1), weight (X2), age (X3)
and blood pressure (X4). Each individual random variable may be assumed
to follow some appropriate probability distribution in the population, but it
may also be quite reasonable to assume that these four variables together
follow a certain joint probability distribution. This example falls in the cat-
egory of multivariate continuous random variables.
The Section 3.2 introduces the ideas of joint, marginal and conditional
distributions in a discrete situation including the multinomial distribution.
The Section 3.3 introduces analogous topics in the continuous case. We
begin with the bivariate scenario for simplicity. The notion of a compound
distribution and the evaluation of the associated mean and variance are ex-
plored in the Examples 3.3.6-3.3.8. Multidimensional analysis is included in
the Section 3.3.2. The notions of the covariances and correlation coeffi-
cients between random variables are explored in the Section 3.4. We briefly
revisit the multinomial distribution in the Section 3.4.1. The Section 3.5
introduces the concept of the independence of random variables. The cel-
ebrated bivariate normal distribution is discussed in the Section 3.6 and the
associated marginal as well as the conditional pdf’s are found in this special
situation. The relationships between the zero correlation and possible
99

100
3. Multivariate Random Variables
independence are explained in the Section 3.7. The Section 3.8 summarizes
both the one- and multi-parameter exponential families of distributions. This
chapter draws to an end with some of the standard inequalities which are
widely used in statistics. The Sections 3.9.1-3.9.4 include more details than
Sections 3.9.5-3.9.7.
3.2   Discrete Distributions
Let us start with an example of bivariate discrete random variables.
Example 3.2.1 We toss a fair coin twice and define Ui = 1 or 0 if the ith toss
results in a head (H) or tail (T), i = 1, 2. We denote X1 = U1 + U2 and X2 = U1
– U2. Utilizing the techniques from the previous chapters one can verify that
X1 takes the values 0, 1 and 2 with the respective probabilities 1/4, 1/2 and 1/
4, whereas X2 takes the values –1, 0 and 1 with the respective probabilities 1/
4, 1/2 and 1/4. When we think of the distribution of Xi alone, we do not worry
much regarding the distribution of Xj, i ≠ j = 1, 2. But how about studying the
random variables (X1, X2) together? Naturally, the pair (X1, X2) takes one of
the nine (= 32) possible pairs of values: (0, –1), (0, 0), (0, 1), (1, –1), (1, 0),
(1, 1), (2, –1), (2, 0), and (2, 1).
Table 3.2.1. Joint Distribution of X1 and X2
X1 values
Row Total
0
1
2
–1
0
.25
0
.25
X2 values
0
.25
0
.25
.50
1
0
.25
0
.25
Col. Total
.25
.50
.25
1.00
Note that X1 = 0 implies that we must have observed TT, in other words U1 =
U2 = 0, so that P(X1 = 0 ∩ X2 = 0) = P(TT) = 1/4. Whereas P(X1 = 0 ∩ X2 =
–1) = P(X1 = 0 ∩ X2 = 1) = 0. Other entries in the Table 3.2.1 can be verified
in the same way. The entries in the Table 3.2.1 provide the joint probabilities,
P(X1 = i ∩ X2 = j) for all i = 0, 1, 2 and j = –1, 0, 1. Such a representation is
provides what is known as the joint distribution of X1, X2. The column and
row totals respectively line up exactly with the individual distributions of X1,
X2 respectively which are also called the marginal distributions of X1, X2.!

3. Multivariate Random Variables
101
3.2.1   The Joint, Marginal and Conditional Distributions
Suppose that we have k(≥ 2) discrete random variables X1, ..., Xk where Xi
takes one of the possible values xi belonging to its support χi, i = 1, ..., k.
Here, χi can be at most countably infinite. The joint probability mass function
(pmf) of X = (X1, ..., Xk) is then given by
A function such as f(x) would be a genuine joint pmf if and only if the follow-
ing two conditions are met:
These are direct multivariable extensions of the requirements laid out earlier in
(1.5.3) in the case of a single real valued random variable.
The marginal distribution of Xi corresponds to the marginal probability
mass function defined by
Example 3.2.2 (Example 3.2.1 Continued) We have χ1 = {0, 1, 2}, χ2 =
{–1, 0, 1}, and the joint pmf may be summarized as follows: f(x1, x2) = 0
when (x1, x2) = (0, –1), (0, 1), (1, 0), (2, –1), (2, 1), but f(x1, x2) = .25 when
(x1, x2) = (0, 0), (1, –1), (1, 1), (2, 0). Let us apply (3.2.3) to obtain the
marginal pmf of X1.
which match with the respective column totals in the Table 3.2.1. Similarly,
the row totals in the Table 3.2.1 will respectively line up exactly with the
marginal distribution of X2. !
In the case of k-dimensions, the notation becomes cumbersome in defin-
ing the notion of the conditional probability mass functions. For simplicity,
we explain the idea only in the bivariate case.

102
3. Multivariate Random Variables
For i≠ j, the conditional pmf of Xi at the point xi given that Xj = xj, denoted
by fi|j(xi), is defined by
Now, suppose that P(Xj = xj), which is equivalent to fj(xj), is positive. Then,
using the notion of the conditional probability from (1.4.1) we can express
fi|j(xi) as
for xi ∈ χi, i ≠ j. In the two-dimensional case, we have two conditional
pmf’s, namely f1|2(x1) and f2|1(x2) which respectively represent the conditional
pmf of X1 given X2 = x2 and that of X2 given X1 = x1.
Once the conditional pmf f1|2(x1) of X1 given X2 = x2 and the conditional
pmf f2|1(x2) of X2 given X1 = x1 are found, the conditional mean of Xi given Xj
= xj, denoted by E[Xi | Xj = xj], is defined as follows:
This is not really any different from how we interpreted the expected value of
a random variable in the Definition 2.2.1. To find the conditional expectation
of Xi given Xj = xj, we simply multiply each value xi by the conditional prob-
ability P{Xi = xi | Xj = xj} and then sum over all possible values xi∈ χi with any
fixed xj ∈ χj, for i ≠ j = 1, 2.
Example 3.2.3 (Example 3.2.2 Continued) One can verify that given X2 =
–1, the conditional pmf f1|2(x1) corresponds to 0, 1 and 0 respectively when x1
= 0, 1, 2. Other conditional pmf’s can be found analogously. !
Example 3.2.4 Consider two random variables X1 and X2 whose joint dis-
tribution is given as follows:
Table 3.2.2. Joint Distribution of X1 and X2
X1 values
Row Total
–1
2
5
1
.12
.18
 .25
.55
X2 values
2
.20
 .09 .
16
.45
Col. Total
.32
.27
.41
1.00

3. Multivariate Random Variables
103
One has χ1 = {–1, 2, 5}, χ2 = {1, 2}. The marginal pmf’s of X1, X2 are
respectively given by f1(–1) = .32, f1(2) = .27, f1(5) = .41, and f2(1) = .55,
f2(2) = .45. The conditional pmf of X1 given that X2 = 1, for example, is given
by f1|2(–1) = 12/55, f1|2(2) = 18/55, f1|2(5) = 25/55. We can apply the notion of
the conditional expectation from (3.2.5) to evaluate E[X1 | X2 = 1] and write
That is, the conditional average of X1 given that X2 = 1 turns out to be approxi-
mately 2.7091. Earlier we had found the marginal pmf of X1 and hence we
have
Obviously, there is a conceptual difference between the two expressions E[X1]
and E[X1 | X2 = 1]. !
If we have a function g(x1, x2) of two variables x1, x2 and we wish to
evaluate E[g(X1, X2)], then how should we proceed? The approach involves a
simple generalization of the Definition 2.2.3. One writes
Example 3.2.5 (Example 3.2.4 Continued) Suppose that g(x1, x2) = x1x2
and let us evaluate E[g(X1, X2)]. We then obtain
Instead, if we had , then one can similarly check that E[h(X1, X2)]
= 16.21. !
3.2.2
 The Multinomial Distribution
Now, we discuss a special multivariate discrete distribution which ap-
pears in the statistical literature frequently and it is called the multinomial
distribution. It is a direct generalization of the binomial setup introduced
earlier in (1.7.2). First, let us look at the following example.
Example 3.2.6 Suppose that we have a fair die and we roll it twenty
times. Let us define Xi = the number of times the die lands up with the
face having i dots on it, i = 1, ..., 6. It is not hard to see that individually

104
3. Multivariate Random Variables
Xi has the Binomial (20, 1/6) distribution, for every fixed i = 1, ..., 6. It is
clear, however, that
      must be exactly twenty in this example and hence
we conclude that X1, ..., X6 are not all free-standing random variables. Sup-
pose that one has observed X1 = 2, X2 = 4, X3 = 4, X5 = 2 and X6 = 3, then X4
must be 5. But, when we think of X4 alone, its marginal distribution is Bino-
mial (20, 1/6) and so by itself it can take any one of the possible values 0, 1,
2, ..., 20. On the other hand, if we assume that X1 = 2, X2 = 4, X3 = 4, X5 = 2
and X6 = 3, then X4 has to be 5. That is to say that there is certainly some kind
of dependence among these random variables X1, ..., X6. What is the exact
nature of this joint distribution? We discuss it next. !
We denote a vector valued random variable by X = (X1, ..., Xk) in general.
A discrete random variable X is said to have a multinomial distribution in-
volving n and p1, ..., pk, denoted by Multk(n, p1, ..., pk), if and only if the joint
pmf of X is given by
We may think of the following experiment which gives rise to the multino-
mial pmf. Let us visualize k boxes lined up next to each other. Suppose that
we have n marbles which are tossed into these boxes in such a way that each
marble will land in one and only one of these boxes. Now, let pi stand for the
probability of a marble landing in the ith box, with 0 < pi < 1, i = 1, ..., k,
      . These p’s are assumed to remain same for each tossed marble.
Now, once these n marbles are tossed into these boxes, let Xi = the number of
marbles which land inside the box ≠i, i = 1, ..., k. The joint distribution of X
= (X1, ..., Xk) is then characterized by the pmf described in (3.2.8).
How can we verify that the expression given by (3.2.8) indeed corre-
sponds to a genuine pmf? We must check the requirements stated in (3.2.2).
The first requirement certainly holds. In order to verify the requirement (ii),
we need an extension of the Binomial Theorem. For completeness, we state
and prove the following result.
Theorem 3.2.1 (Multinomial Theorem) Let a1, ..., ak be arbitrary real
numbers and n be a positive integer. Then, we have

3. Multivariate Random Variables
105
Proof First consider the case k = 2. By the Binomial Theorem, we get
which is the same as (3.2.9). Now, in the case k = 3, by using (3.2.10)
repeatedly let us write
which is the same as (3.2.9). Next, let us assume that the expansion (3.2.9)
holds for all n ≤r, and let us prove that the same expansion will then hold for
n = r + 1. By mathematical induction, the proof will then be complete. Let us
write
which is the same as (3.2.9). The proof is now complete. ¢
The fact that the expression in (3.2.8) satisfies the requirement (ii) stated
in (3.2.2) follows immediately from the Multinomial Theorem. That is, the
function f(x) given by (3.2.8) is a genuine multivariate pmf.
The marginal distribution of the random variable Xi is
Binomial (n,pi) for each fixed i = 1, ..., k.
In the case of the multinomial distribution (3.2.8), suppose that we focus on
the random variable Xi alone for some arbitrary but otherwise fixed

106
3. Multivariate Random Variables
i = 1, ..., k. The question is this: Out of the n marbles, how many (that is, Xi)
would land in the ith box? We are simply counting how many marbles would fall
in the ith box and how many would fall outside, that is in any one of the other k
– 1 boxes. This is the typical binomial situation and hence we observe that the
random variable Xi has the Binomial(n, pi) distribution for each fixed i = 1, ..., k.
Hence, from our discussions on the binomial distribution in Section 2.2.2 and
(2.2.17), it immediately follows that
Example 3.2.7  (Example 3.2.6 Continued) In the die rolling example, X =
(X1, ..., Xk) has the Multk(n, p1, ..., pk) distribution where n = 20, k = 6, and p1
= ... = p6 = 1/6. !
The derivation of the moment generating function (mgf) of the
multinomial distribution and some of its applications are
highlighted in Exercise 3.2.8.
The following theorems are fairly straightforward to prove. We leave their
proofs as the Exercises 3.2.5-3.2.6.
Theorem 3.2.2  Suppose that the random vector X = (X1, ..., Xk) has the
Multk(n, p1, ..., pk) distribution. Then, any subset of the X variables of size
r, namely (Xi1, ..., Xir) has a multinomial distribution in the sense that (Xi1, ...,
Xir,
) is Multr+1(n, pi1, ..., pir,
      ) where 1 ≤ i1 < i2 < ... < ir ≤ k
and .
Theorem 3.2.3  Suppose that the random vector X = (X1, ..., Xk) has the
Multk(n, p1, ..., pk) distribution. Consider any subset of the X variables of
size r, namely (Xi1, ..., Xir). The conditional joint distribution of (Xi1, ..., Xir)
given all the remaining X’s is also multinomial with its conditional pmf
Example 3.2.8 (Example 3.2.7 Continued) In the die rolling example,
suppose that we are simply interested in counting how many times the faces
with the numbers 1 and 5 land up. In other words, our focus is on the three

3. Multivariate Random Variables
107
dimensional random variable (X1, X5,
) where
             ).By
the Theorem 3.2.2, the three dimensional variable of interest (X1, X5,    )has
the Mult3(n, 1/6, 1/6, 2/3) distribution where n = 20. !
3.3   Continuous Distributions
Generally speaking, we may be dealing with k(≥ 2) real valued random vari-
ables X1, ..., Xk which vary individually as well as jointly. We may go back to
the example cited earlier in the introduction. Consider a population of college
students and record a randomly selected student’s height (X1), weight (X2),
age (X3), and blood pressure (X4). It may be quite reasonable to assume that
these four variables together has some multivariate continuous joint probabil-
ity distribution in the population.
The notions of the joint, marginal, and conditional distributions in the
case of multivariate continuous random variables are described in the Section
3.3.1. This section begins with the bivariate scenario for simplicity. The no-
tion of a compound distribution and the evaluation of the associated mean and
variance are explored in the Examples 3.3.6-3.3.8. Multidimensional analysis
is included in the Section 3.3.2.
3.3.1
The Joint, Marginal and Conditional Distributions
For the sake of simplicity, we begin with the bivariate continuous random
variables. Let X1, X2 be continuous random variables taking values respec-
tively in the spaces χ1, χ2 both being subintervals of the real line ℜ. Consider
a function f(x1, x2) defined for (x1, x2) ∈ χ1 × χ2. Throughout we will use the
following convention:
Let us presume that χ1 × χ2 is the support of the probability
distribution in the sense that f(x1, x2) > 0 for all (x1, x2)
∈ χ1 × χ2 and f(x1, x2) = for all (x1, x2)  ∈ℜ2 – (χ1 × χ2).
Such a function f(x1, x2) would be called a joint pdf of the random vector
X = (X1, X2) if the following condition is satisfied:
Comparing (3.3.1) with the requirement (ii) in (3.2.2) and (1.6.1), one
will notice obvious similarities. In the case of one-dimensional random vari-
ables, the integral in (1.6.1) was interpreted as the total area under the
density curve y = f(x). In the present situation involving two random

108
3. Multivariate Random Variables
variables, the integral in (3.3.1) will instead represent the total volume under
the density surface z = f(x1, x2).
The marginal distribution of one of the random variables is obtained by
integrating the joint pdf with respect to the remaining variable. The marginal
pdf of Xi is then formally given by
Visualizing the notion of the conditional distribution in a continuous sce-
nario is little tricky. In the discrete case, recall that fi(xi) was simply inter-
preted as P(Xi = xi) and hence the conditional pmf was equivalent to the
corresponding conditional probability given by (3.2.4) as long as P(Xi = x1) >
0. In the continuous case, however, P(Xi = xi) = 0 for all xi ∈ χi with i = 1,
2. So, conceptually how should one proceed to define a conditional pdf?
Let us first derive the conditional df of X1 given that x2≤ X2 ≤ x2 + h where
h(> 0) is a small number. Assuming that P(x2 ≤X2 ≤x2 + h) > 0, we have
From the last step in (3.3.3) it is clear that as h ↓ 0, the limiting value of this
conditional probability takes the form of 0/0. Thus, by appealing to the
L’Hôpital’s rule from (1.6.29) we can conclude that
Next, by differentiating the last expression in (3.3.4) with respect to x1, one
obtains the expression for the conditional pdf of X1 given that X2 = x2.

3. Multivariate Random Variables
109
Hence, the conditional pdf of X1 given that X2 = x2 is given by
Once these conditional pdf’s are obtained, one can legitimately start talk-
ing about the conditional moments of one of the random variables given the
other. For example, in order to find the conditional mean and the conditional
variance of X1 given that X2 = x2, one should proceed as follows:
It should be clear that µ1/2 and
  respectively from (3.3.7) would corre-
spond to the conditional mean and the conditional variance of X1 given that
X2 = x2. In general, we can write
Example 3.3.1  Consider two random variables X1 and X2 whose joint
continuous distribution is given by the following pdf:
Obviously, one has χ1 = χ2 = (0, 1). Next, we integrate this joint pdf with
respect to x2 to obtain the marginal pdf of X1 and write
for 0 < x1 < 1. In the same fashion, we obtain the marginal pdf of X2 as
follows:

110
3. Multivariate Random Variables
for 0 < x2 < 1. Next, we can simply use the marginal pdf f2(x2) of X2 to
obtain
so that
The reader may similarly evaluate E(X1) and V(X1). !
Example 3.3.2 (Example 3.3.1 Continued) One should easily verify that
are respectively the two conditional pdf’s. For fixed 0 < x1 < 1, the conditional
expectation of X2 given that X1 = x1 is obtained as follows:
One can evaluate the expression
of as follows:
so that using (3.3.14) and after considerable simplifications, we find:
The details are left out as Exercise 3.3.1.!
Example 3.3.3 Consider two random variables X1 and X2 whose joint con-
tinuous distribution is given by the following pdf:
Obviously, one has χ1 = χ2 = (0, 1). One may check the following expressions
for the marginal pdf’s easily:

3. Multivariate Random Variables
111
One should derive the conditional pdf’s and the associated conditional means
and variances. See the Exercise 3.3.2. !
In a continuous bivariate distribution, the joint, marginal, and
conditional pdf’s were defined in (3.3.1)-(3.3.2) and (3.3.5)-(3.3.6).
In the case of a two-dimensional random variable, anytime we wish to
calculate the probability of an event A(⊆ ℜ2), we may use an approach which
is a generalization of the equation (1.6.2) in the univariate case: One would
write
We emphasize that the convention is to integrate f(x1, x2) only on that part of
the set A where f(x1, x2) is positive.
If we wish to evaluate the conditional probability of an event B(⊆ ℜ)
given, say, X1 = x1, then we should integrate the conditional pdf f2/1(x2) of X2
given X1 = x1 over that part of the set B where f2/1(x2) is positive. That is, one
has
Example 3.3.4 (Example 3.3.2 Continued) In order to appreciate the
essence of what (3.3.20) says, let us go back for a moment to the Ex-
ample 3.3.2. Suppose that we wish to find the probability of the set or
the event A where A = {X1 = .2 ∩.3 < X2 = .8}. Then, in view of (3.3.20),
we obtain
dx1 = 1.2(.845 – .62) = .27. !
Example 3.3.5 Consider two random variables X1 and X2 whose joint
continuous distribution is given by the following pdf:
Obviously, one has χ1 = χ2 = (0, 1). One may easily check the following
expressions for the marginal pdf’s:
Now, suppose that we wish to compute the conditional probability that
X2 < .2 given that X1 = .5. We may proceed as follows. We have

112
3. Multivariate Random Variables
and hence using (3.3.24), one can write
One may, for example, evaluate µ2/1 too given that X1 = .5. We leave it out as
the Exercise 3.3.3. !
We defined the conditional mean and variance of a random variable X1
given the other random variable X2 in (3.3.7). The following result gives the
tool for finding the unconditional mean and variance of X1 utilizing the expres-
sions of the conditional mean and variance.
Theorem 3.3.1  Suppose that X = (X1, X2) has a bivariate pmf or pdf,
namely f(x1, x2) for xi ∈ χi, the support of Xi, i = 1, 2. Let E1[.] and V1[.]
respectively denote the expectation and variance with respect to the marginal
distribution of X1. Then, we have
(i) E[X2] = E1[E2/1 {X2 | X1}];
(ii) V(X2) = V1[E2/1{X2 | X1}] + E1[V1/V2/1{X2 | X1}].
Proof (i) Note that
Next, we can rewrite
which is the desired result.
(ii) In view of part (i), we can obviously write
where g(x1) = E{(X2 – χ2)2 | X1 = x1}. As before, denoting E{X2 | X1 = x1} by
µ2/1, let us rewrite the function g(x1) as follows:

3. Multivariate Random Variables
113
But, observe that the third term in the last step (3.3.29) can be simplified as
follows:
which is zero. Now, we combine (3.3.29)-(3.3.30) and obtain
At this point, (3.3.31) combined with (3.3.29) then leads to the desired result
stated in part (ii). ¢
The next three Examples 3.3.6-3.3.8, while applying
the Theorem 3.3.1, introduce what are otherwise
known in statistics as the compound distributions.
Example 3.3.6  Suppose that conditionally given X1 = x1, the random vari-
able X2 is distributed as N(β0 + β1x1,    ) for any fixed x1 ∈ ℜ. Here, β0, β1 are
two fixed real numbers. Suppose also that marginally, X1 is distributed as N(3,
10). How should we proceed to find E[X2] and V[X2]? The Theorem 3.3.1 (i)
will immediately imply that
which reduces to β0 + 3β1. Similarly, the Theorem 3.3.1 (ii) will imply that
which reduces to
    Refer to the Section 2.3.3 as needed. In a situa-
tion like this, the marginal distribution of X2 is referred to as a compound
distribution. Note that we have been able to derive the expressions of the
mean and variance of X2 without first identifying the marginal distribution of
X2. !
Example 3.3.7  Suppose that conditionally given X1 = x1, the random vari-
able X2 is distributed as Poisson (x1) for any fixed x1 ∈ ℜ+. Suppose also that
marginally, X1 is distributed as Gamma (α = 3, β = 10). How should we
proceed to find E[X2] and V[X2]? The Theorem 3.3.1 (i) will immediately
imply that

114
3. Multivariate Random Variables
Similarly, the Theorem 3.3.1 (ii) will imply that
which reduces to 330. Refer to the Sections 2.3.2 and 2.3.4 as needed. In a
situation like this, again the marginal distribution of X2 is referred to as a
compound distribution. Note that we have been able to derive the expressions
of the mean and variance of X2 without first identifying the marginal distribu-
tion of X2. !
Example 3.3.8  Suppose that conditionally given X1 = x1, the random vari-
able X2 is distributed as Binomial(n, x1) for any fixed x1 ∈ (0, 1). Suppose also
that marginally, X1 is distributed as Beta(α = 4, β = 6). How should we pro-
ceed to find E[X2] and V[X2]? The Theorem 3.3.1 (i) will immediately imply
that
Similarly, the Theorem 3.3.1 (ii) will imply that
Refer to the Sections 2.3.1 and equation (1.7.35) as needed. One can verify
that
        Also, we have .
          Hence,
we have
In a situation like this, again the marginal distribution of X2 is referred to as a
compound distribution. Note that we have been able to derive the expressions
of the mean and variance of X2 without first identifying the marginal distribu-
tion of X2. !
Example 3.3.9 (Example 3.3.2 Continued) Let us now apply the Theo-
rem 3.3.1 to reevaluate the expressions for E[X2] and V[X2]. Combining
(3.3.15) with the Theorem 3.3.1 (i) and using direct integration with respect

3. Multivariate Random Variables
115
to the marginal pdf f1(x1) of X1 from (3.3.10), we get
which matches with the answer found earlier in (3.3.12). Next, combining
(3.3.15) and (3.3.17) with the Theorem 3.3.1 (ii), we should then have
Now, we combine (3.3.39) and (3.3.40). We also use the marginal pdf f1(x1)
of X1 from (3.3.10) and direct integration to write
since E[(1 – X1)] = 3/4 and E[(1 – X1)2] = 3/5. The answer given earlier in
(3.3.13) and the one found in (3.3.41) match exactly. !
3.3.2  Three and Higher Dimensions
The ideas expressed in (3.3.2) and (3.3.5)-(3.3.6) extend easily in a multi-
variate situation too. For example, let us suppose that a random vector X =
(X1, X2, X3, X4) has the joint pdf f(x1, x2, x3, x4) where xi ∈ χi, the support of
Xi, i = 1, 2, 3, 4. The marginal pdf of (X1, X3), that is the joint pdf of X1 and
X3, for example, can be found as follows:
The conditional pdf of (X2, X4) given that X1 = x1, X3 = x3 will be of the form
and the x’s belong to the χ’s. The notions of expectations and conditional ex-
pectations can also be generalized in a natural fashion in the case of a

116
3. Multivariate Random Variables
multidimensional random vector X by extending the essential ideas from (3.3.7)-
(3.3.8).
If one has a k-dimensional random variable X = (X1, ..., Xk), the joint pdf
would then be written as f(x) or f(x1, ..., xk). The joint pdf of any subset of
random variables, for example, X1 and X3, would then be found by integrating
f(x1, ..., xk) with respect to the remaining variables x2, x4, ..., xk. One can also
write down the expressions of the associated conditional pdf’s of any subset
of random variables from X given the values of any other subset of random
variables from X.
Theorem 3.3.2  Let X = (X1, ..., Xk) be any k-dimensional discrete or
continuous random variable. Suppose that we also have real valued functions
hi(x) and constants ai, i = 0, 1, ..., p. Then, we have
as long as all the expectations involved are finite. That is, the expectation is
a linear operation.
Proof  Let us write
   and hence we have
Now, the proof is complete. ¢
Next, we provide two specific examples.
Example 3.3.10  Let us denote χ1 = χ3 = (0, 1), χ2 = (0, 2) and define
Note that these are non-negative functions and ∫χi ai(xi)dxi = 1 for all

3. Multivariate Random Variables
117
i = 1, 2, 3. With x = (x1, x2, x3)
   and , let us denote
Now, one can easily see that
and
Hence, for the function f(x) defined in (3.3.47) we have
and also observe that f(x) is always non-negative. In other words, f(x) is a
pdf of a three dimensional random variable, say, X = (X1, X2, X3). The joint
pdf of X1, X3 is then given by
One may directly check by double integration that g(x1, x3) is a genuine pdf
with its support being χ1 × χ3. For any 0 < x2 < 2, with g(x1, x3) from (3.3.49),
the conditional pdf of X2 given that X1 = x1, X3 = x3 is then given by

118
3. Multivariate Random Variables
Utilizing (3.3.49), it also follows that for any 0 < x3 < 1, the marginal pdf of X3
is given by
Utilizing the expressions of g(x1, x3), g3(x3) from (3.3.49) and (3.3.51), for
any 0 < x1 < 1, we can write down the conditional pdf of X1 given that X3 = x3
as follows:
After obtaining the expression of g(x1, x3) from (3.3.49), one can easily
evaluate, for example E(X1 X3) or E(X1    ) respectively as the double integrals
. Look
at the Exercises 3.3.6-3.3.7. !
In the two Examples 3.3.10-3.3.11, make a special note of how
the joint densities f(x) of more than two continuous random
variables have been constructed. The readers should create a
few more of their own. Is there any special role of the specific
forms of the functions ai(xi)’s? Look at the Exercise 3.3.8.
Example 3.3.11  Let us denote χ1 = χ3 = χ4 = χ5 = (0, 1), χ2 = (0, 2) and
recall the functions a1(x1), a2(x2) and a3(x3) from (3.3.45)-(3.3.46). Addition-
ally, let us denote
Note that these are non-negative functions and also one has ∫χi ai(xi)dxi = 1 for
all i = 1, ..., 5. With x = (x1, x2, x3, x4, x5) and
, let us denote
It is a fairly simple matter to check that ,

3. Multivariate Random Variables
119
2, ∫ ∫ ∫χ ∫ ∫
        dxi = 2, and ∫ ∫ ∫χ ∫ ∫ a4(x4)
dxi = 2.
Hence, one verifies that ∫ ∫ ∫χ ∫ ∫
  = 1 so that (3.3.53) does
provide a genuine pdf since f(x) is non-negative for all x ∈ χ. Along the lines
of the example 3.3.10, one should be able to write down (i) the marginal joint
pdf of (X2, X3, X5), (ii) the conditional pdf of X3, X5 given the values of X1, X4,
(iii) the conditional pdf of X3 given the values of X1, X4, (iv) the conditional
pdf of X3 given the values of X1, X2, X4, among other similar expressions.
Look at the Exercises 3.3.9-3.3.10. Look at the Exercise 3.3.11 too for other
possible choices of ai’s. !
3.4  Covariances and Correlation Coefficients
Since the random variables X1, ..., Xk vary together, we may look for a mea-
sure of the strength of such interdependence among these variables. The
covariance aims at capturing this sense of joint dependence between two real
valued random variables.
Definition 3.4.1 The covariance between any two discrete or continuous
random variables X1 and X2, denoted by Cov (X1, X2), is defined as
where µi = E(Xi), i = 1, 2.
On the rhs of (3.4.1) we note that it is the expectation of a specific func-
tion g(X1, X2) = (X1 – µ1)(X2 – µ2) of the two random variables X1, X2. By
combining the ideas from (3.2.6) and (3.3.8) we can easily rewrite (3.4.1).
Then, using the Theorem 3.3.2 we obtain
We now rewrite (3.4.2) in its customary form, namely
whether X1, X2 are discrete or continuous random variables, provided that the
expectations E(X1 X2), E(X1) and E(X2) are finite. From (3.4.3) it also be-
comes clear that

120
3. Multivariate Random Variables
where X1, X2 are any two arbitrary discrete or continuous random variables,
that is the covariance measure is symmetric in the two variables. From (3.4.3)
it obviously follows that
where X1 is any arbitrary random variable having a finite second moment.
Theorem 3.4.1  Suppose that Xi, Yi, i = 1, 2 are any discrete or continuous
random variables. Then, we have
(i)
Cov(X1, c) = 0 where c ∈ ℜ is fixed, if E(X1) is finite;
(ii) Cov(X1 + X2, Y1 + Y2) = Cov(X1, Y1) + Cov(X1, Y2) + Cov(X2, Y1) +
Cov(X2, Y2) provided that Cov(Xi, Yj)
is finite for i, j = 1, 2.
In other words, the covariance is a bilinear operation which means that it
is a linear operation in both coordinates.
Proof (i)  Use (3.4.3) where we substitute X2 = c w.p.1. Then, E(X2) = c
and hence we have
(ii) First let us evaluate Cov(X1 + X2, Y1). One gets
Next, we exploit (3.4.6) repeatedly to obtain
This leads to the final conclusion by appealing again to (3.4.4). ¢

3. Multivariate Random Variables
121
Example 3.4.1  (Examples 3.2.4 and 3.2.5 Continued) We already know
that E(X1) = 2.27 and also E(X1X2) = 3.05. Similarly, we can find E(X2) =
(1)(.55) + 2(.45) = 1.45 and hence, Cov(X1, X2) = 3.05 – (2.27)(1.45) = –
0.2415. !
Example 3.4.2  (Example 3.3.3 Continued) It is easy to see that E(X1) =
(1/3)(1/2) = 1/6. Next, we appeal to (3.4.3) and observe that Cov(X1, X2) =
E(X1X2) – E(X1)E(X2) = 1/6 – (1/3)(1/2) = 0. !
Example 3.4.3  (Example 3.3.5 Continued) It is easy to see that E(X1) =
peal to (3.4.3) and observe that Cov(X1, X2) = E(X1X2) – E(X1)E(X2) = 3/10 –
(3/4)(3/8) = 3/160. !
The term Cov(X1, X2) can be any real number, positive, negative or zero.
However, if we simply look at the value of Cov(X1, X2), it will be hard for us
to say very much about the strength of the dependence between the two
random variables X1 and X2 under consideration. It should be apparent that the
term Cov(X1, X2) has the unit of measurement which is same as that for the
variable X1X2, the product of X1 and X2. If X1, X2 are both measured in inches,
then Cov(X1, X2) would have to be recorded in square inches.
A standardized version of the covariance term, commonly known as the
correlation coefficient, was made popular by Karl Pearson. Refer to Stigler
(1989) for the history of the invention of correlation. We discuss related his-
torical matters later.
Definition 3.4.2  The correlation coefficient between any two random
variables X1 and X2, denoted by ρX1, X2, is defined as follows:
whenever one has –∞ < Cov(X1, X2) < ∞,
      and

122
3. Multivariate Random Variables
When we consider two random variables X1 and X2 only, we may supress the
subscripts from ρX1, X2 and simply write ρ instead.
Definition 3.4.3  Two random variables X1, X2 are respectively called
negatively correlated, uncorrelated, or positively correlated if and only if ρX1,
X2 negative, zero or positive.
Before we explain the role of a correlation coefficient any further, let us
state and prove the following result.
Theorem 3.4.2  Consider any two discrete or continuous random vari-
ables X1 and X2 for which we can assume that –∞ < Cov(X1, X2) < ∞, 0 <
V(X1) < ∞ and 0 < V(X2) < ∞. Let ρX1, X2, defined by (3.4.8), stand for the
correlation coefficient between X1 and X2. We have the following results:
(i)
Let Yi = ci + diXi where –∞ < ci < ∞ and 0 < di < ∞ arefixed
numbers, i = 1, 2. Then, ρY1,Y2 = ρX1, X2;
(ii)
|ρX1, X2| ≤ 1;
(iii)
In part (ii), the equality holds, that is ρ is +1 or –1, if and only
if X1 and X2 are linearly related. In other words, ρX1, X2 is +1or –
1 if and onlyif X1 = a + bX2 w.p.1 for some real numbers a and b.
Proof (i) We apply the Theorem 3.3.2 and Theorem 3.4.1 to claim that
Also, we have
Next we combine (3.4.8)-(3.4.10) to obtain
(ii) We apply the Cauchy-Schwarz inequality (Theorem 3.9.5) or directly
the covariance inequality (Theorem 3.9.6) from the Section 3.9 and imme-

3. Multivariate Random Variables
123
diately note that
From (3.4.12) we conclude that
Hence, one has |ρX1, X2| = 1.
(iii) We conclude that ,             X2 = 1 provided that we have equality throughout
(3.4.13). The covariance inequality (Theorem 3.9.6) dictates that we can
have equality in (3.4.13) if and only if the two random variables (X1 – µ1) and
(X2 – µ2) are linearly related w.p.1. In other words, one will have ,             X2 = 1
if and only if (X1 – µ1) = c(X2 – µ2) + d w.p.1 where c and d are two real
numbers. The result follows with a = µ1 – cµ2 + d and b = c. One may
observe that ρX1, X2 = +1 or –1 according as b is positive or negative. ¢
From the Definition 3.4.3, recall that in the case when ρX1, X2 = 0, the two
random variables X1, X2 are called uncorrelated. The case of zero correlation
is addressed in more detail in the Section 3.7.
If the correlation coefficient ρ is one in magnitude, then the
two random variables are linearly related with probability one.
The zero correlation is referred to as uncorrelation.
Example 3.4.4 (Example 3.4.1 Continued) One can check that V(X1) = 6.4971
and V(X2) = .6825. We also found earlier that Cov(X1, X2) = –0.2415. Thus, using
(3.4.8) we get ρX1, X2 = Cov(X1, X2)/(σ1σ2) =
       ≈ –
. 11468. !
Example 3.4.5 (Example 3.4.2 Continued) We already had shown that
Cov(X1, X2) = 0. One can also check that both V(X1) and V(X2) are finite, and
hence ρX1, X2 = 0. !
Example 3.4.6 (Example 3.4.3 Continued) We already had shown that
Cov(X1, X2) = 3/160 whereas E(X1) = 3/4, E(X2) = 3/8. Proceeding analo-
gously, one can check that
               Thus, V(X1) = 3/5 – (3/4)2 = 3/80 and V(X2) = 1/5 – (3/8)2 = 19/320.
Hence, ρX1, X2 = (3/160)
  / ≈ .39736. !

124
3. Multivariate Random Variables
Next, we summarize some more useful results for algebraic manipulations
involving expectations, variances, and covariances.
Theorem 3.4.3 Let us write a1, ..., ak and b1, ..., bk for arbitrary but fixed
real numbers. Also recall the arbitrary k-dimensional random variable X =
(X1, ..., Xk) which may be discrete or continuous. Then, we have
Proof The parts (i) and (ii) follow immediately from the Theorem 3.3.2
and (3.4.3). The parts (iii) and (iv) follow by successively applying the bilin-
ear property of the covariance operation stated precisely in the Theorem 3.4.1,
part (ii). The details are left out as the Exercise 3.4.5. ¢
One will find an immediate application of Theorem 3.4.3
in a multinomial distribution.
3.4.1   The Multinomial Case
Recall the multinomial distribution introduced in the Section 3.2.2. Sup-
pose that X = (X1, ..., Xk) has the Multk(n, p1, ..., pk) distribution with the
associated pmf given by (3.2.8). We had mentioned that Xi has the Binomial(n,
pi) distribution for all fixed i = 1, ..., k. How should we proceed to derive the
expression of the covariance between Xi and Xj for all fixed i ≠ j = 1, ..., k?
Recall the way we had motivated the multinomial pmf given by (3.2.8).
Suppose that we have n marbles and these are tossed so that each marble
lands in one of the k boxes. The probability that a marble lands in box #i is,
say, pi, i = 1, ..., k,
Then, let Xl be the number of marbles land-
ing in the box #l, l = i, j. Define
Then, utilizing (3.4.14) we can write

3. Multivariate Random Variables
125
so that we have
It is not hard to see that E[Y1Z1] = 0, E[Y1] = pi, E[Z1] = pj, and hence we can
rewrite (3.4.16) to claim that
for all fixed i ≠ j = 1, ..., k.
This negative covariance should intuitively make sense because out of the
n marbles, large number of marbles in box #i would necessarily force the
number of marbles in the box #j to be small. Next, one can simply obtain
for all fixed i ≠ j = 1, ..., k.
3.5   Independence of Random Variables
Suppose that we have a k-dimensional random variable X = (X1, ..., Xk) whose
joint pmf or pdf is written as f(x) or f(x1, ..., xk) with xi ∈ χi(⊆ ℜ), i = 1, ...,
k. Here χi is the support of the random variable Xi, i = 1, ..., k, where these
random variables can be discrete or continuous.
Definition 3.5.1  Let fi(xi) denote the marginal pmf or pdf of Xi, i = 1, ...,
k. We say that X1, ..., Xk form a collection of independent random variables if
and only if

126
3. Multivariate Random Variables
Also, X1, ..., Xk form a collection of dependent random variables if and only
if these are not independent.
In order to show that X1, ..., Xk are dependent, one simply needs
to show the existence of a particular set of values x1, ..., xk,
       with xi ∈ χi, i = 1, ..., k such that
Example 3.5.1  (Example 3.2.4 Continued) We immediately note that P(X1
= –1 ∩ X2 = 1) = .12, P(X1 = –1) = .32 and P(X2 = 1) = .55. But, P(X1 = –
1)P(X2 = 1) = .176 which is different from P(X1 = –1 ∩ X2 = 1). We have
shown that when x1 = –1, x2 = 1 are chosen, then, f(x1,x2) ≠ f1(x1)f2(x2. Thus,
the two random variables X1, X2 are dependent. !
Example 3.5.2 (Example 3.3.1 Continued) From (3.3.9)-(3.3.11) we have
f(x1, x2) = 6(1 – x2) for 0 < x1 < x2 < 1,
   fi(xi) = 18x2(1 – x1)2(1 – x2) for 0
< x1, x2 < 1. For x1 = .4, x2 = .5, we have f(x1, x2) = 3, but       fi(xi) = 1.62
≠ f(x1, x2). Thus, the two random variables X1, x2 are dependent. !
Example 3.5.3 Consider two random variables X1, X2 having their joint
pdf given by
First, one can easily show that fi(xi) = 2xi for 0 < xi < 1, i = 1, 2. Since one
obviously has f(x1, x2) = f1(x1) f2(x2) for all 0 < x1, x2 < 1, it follows that X1, X2
are independent. !
It is important to understand, for example, that in order for
the k random variables X1, X2, ..., Xk to be independent, their
joint and the marginal pmf’s or the pdf’s must satisfy (3.5.1).
Look at the next example for the case in point.
Example 3.5.4 It is easy to construct examples of random variables X1,
X2, X3 such that (i) X1 and X2 are independent, (ii) X2 and X3 are independent,
and (iii) X1 and X3 are also independent, but (iv) X1, X2, X3 together are depen-
dent. Suppose that we start with the non-negative integrable functions fi(xi)
for 0 < xi < 1 which are not identically equal to unity such that we have
                 , i = 1, 2, 3. Let us then define

3. Multivariate Random Variables
127
for 0 < xi < 1, i = 1, 2, 3. It is immediate to note that g(x1, x2, x3) is a bona fide
pdf. Now, focus on the three dimensional random variable X = (X1, X2, X3)
whose joint pdf is given by g(x1, x2, x3). The marginal pdf of the random
variable Xi is given by gi(xi) = 1/2{fi(xi) + 1} for 0 < xi < 1, i = 1, 2, 3. Also the
joint pdf of (Xi, Xj) is given by gi, j(xi, xj) = 1/4{fi(xi)fj(xj) + fi(xi) + fj(xj) + 1} for
i ≠ j = 1, 2, 3. Notice that gi, j(xi, xj) = gi(xi)gj(xj) for i ≠ j = 1, 2, 3, so that we
can conclude: Xi and Xj are independent for i ≠ j = 1, 2, 3. But, obviously g(x1,
x2, x3) does not match with the expression of the product
 for 0 <
xi < 1, i = 1, 2, 3. Refer to the three Exercises 3.5.4-3.5.6 in this context.!
In the Exercises 3.5.7-3.5.8 and 3.5.17, we show ways to construct
a four-dimensional random vector X = (X1, X2, X3, X4) such that
X1, X2, X3 are independent, but X1, X2, X3, X4 are dependent.
For a set of independent vector valued random variables X1, ..., Xp, not
necessarily all of the same dimension, an important consequence is summa-
rized by the following result.
Theorem 3.5.1 Suppose that X1, ..., Xp are independent vector valued
random variables. Consider real valued functions gi(xi), i = 1, ..., p. Then, we
have
as long as E[gi(Xi)] is finite, where this expectation corresponds to the inte-
gral with respect to the marginal distribution of Xi, i = 1, ..., p. Here, the Xi’s
may be discrete or continuous.
Proof Let the random vector Xi be of dimension ki and let its support be χi
⊆ ℜki, i = 1, ..., k. Since X1, ..., Xp are independent, their joint pmf or pdf is
given by
Then, we have

128
3. Multivariate Random Variables
which is the desired result. ¢
One important consequence of this theorem is this: if two real valued
random variables are independent, then their covariance, when finite, is
necessarily zero. This in turn then implies that the correlation coefficient
between those two random variables is indeed zero.
In other words, independence between the random variables X1 and X2
would lead to the zero correlation between X1 and X2 as long as ρ x1, x2 is
finite. We state this more generally as follows.
Theorem 3.5.2 Suppose that X1, X2 are two independent vector valued
random variables, not necessarily of the same dimension. Then,
(i)
Cov(g1(X1),g2(X2)) = 0 where g1(.),g2(.) are real valued func
tions, if E[g1g2], E[g1] and E[g2] are all finite;
(ii)
g1(X1),g2(X2) are independent where g1(.),g2(.) do not need to be
restricted as only real valued.
Proof (i) Let us assume that E[g1g2], E[g2] and E[g2] are all finite. Then,
we appeal to the Theorem 3.5.1 to assert that E[g1(X1)g2(X2)] =
E[g1(X1)]E[g2(X2)]. But, since Cov(g1(X1), g2(X2)) is E[g1(X1)g2)] –
E[g1(X1)]E[g2)], we thus have Cov(g1(X1,g2(X2)) obviously reducing to zero.
(ii) We give a sketch of the main ideas. Let Ai be any Borel set in the range
space of the function gi(.), i = 1, 2. Now, let Bi = {xi ∈ χi : gi(xi) ∈ Ai}. Now,
which is the desired result. ¢
In the Theorem 3.5.2, one may be tempted to prove part (i) using the
result from part (ii) plus the Theorem 3.5.1, and wonder whether the require-
ments of the finite moments stated therein are crucial for the
conclusion to hold. See the following example for the case in point.
Example 3.5.5 Suppose that X1 is distributed as the standard normal
variable with its pdf
= φ(x 1) defined in
(1.7.16) and X2 is distributed as the Cauchy variable with its pdf f2(x2) = π–1
        defined in (1.7.31), –∞ < x1, x2 < ∞. Suppose also that

3. Multivariate Random Variables
129
these two random variables are independent. But, we can not claim that Cov(X1,
X2) = 0 because E[X2] is not finite. !
Two random variables X1 and X2 may be independent,
but that may not necessarily imply that Cov(X1, X2) = 0.
This has been emphasized in the Example 3.5.5.
Example 3.5.6 What will happen to the conclusions in the Theorem
3.3.1, parts (i)-(ii), when the two random variables X1 and X2 are indepen-
dent? Let X1 and X2 be independent. Then, E2|1{X2 | X1} = E[X2] which is a
constant, so that E1[E{X2}] = E[X2]. Also, V1[E2|1{X2 | X1}] = V1[E{X2}] =
0 since the variance of a constant is zero. On the other hand,
V[X2] which is a constant. Thus, E1[V2|1{X2 | X1}] = E1[V{X2}] = V[X2].
Hence, V1[E2|1{X2 | X1}] + E1[V2|1{X2 | X1}] simplifies to V[X2]. !
We have more to say on similar matters again in the Section 3.7. We end
this section with yet another result which will come in very handy in verifying
whether some jointly distributed random variables are independent. In some
situations, applying the Definition 3.5.1 may be a little awkward because (3.5.1)
requires one to check whether the joint pmf or the pdf is the same as the
product of the marginal pmf’s or pdf’s. But, then one may not yet have
identified all the marginal pmf’s or pdf’s explicitly! What is the way out?
Look at the following result. Its proof is fairly routine and we leave it as the
Exercise 3.5.9.
Suppose that we have the random variables X1, ..., Xk where χi is the
support of Xi, i = 1, ..., k. Next, we formalize the notion of the supports χi’s
being unrelated to each other.
Definition 3.5.2 We say that the supports χi’s are unrelated to each other if
the following condition holds: Consider any subset of the random variables,
Xij, j = 1, ..., p, p = 1, ..., k – 1. Conditionally, given that Xij = xij, the support
of the remaining random variable Xl stays the same as χl for any l ≠ ij, j = 1,
..., p, p = 1, ..., k – 1.
Theorem 3.5.3 Suppose that the random variables X1, ..., Xk have the
joint pmf or pdf f(x1, ..., xk), xi ∈ χl, the support of Xi, i = 1, ..., k. Assume
that these supports are unrelated to each other according to the Definition
3.5.2. Then, X1, ..., Xk are independent if and only if
for some functions h1(x1), ..., hk(xk).

130
3. Multivariate Random Variables
Example 3.5.7 Suppose that the joint pdf of X1, X2 is given by
for some θ > 0 where k(> 0) is a constant. Obviously, we can write f(x1, x2) =
kh1(x1)h2(x2) for all positive numbers x1, x2 where h1(x1) = exp{–θx1} and h2(x2)
= exp{–1/θx2}. Also the supports χ1 = ℜ+, χ2 = ℜ+ are unrelated to each other.
Hence, by appealing to the Theorem 3.5.3 we can claim that X1 and X2 are
independent random variables. One should note two interesting facts here. The
two chosen functions h1(x1) and h2(x2) are not even probability densities to
begin with. The other point is that the choices of these two functions are not
unique. One can easily replace these by kdh1(x1) and kd–1h2(x2) respectively
where d(> 0) is any arbitrary number. Note that we did not have to determine k
in order to reach our conclusion that X1 and X2 are independent random vari-
ables. !
Example 3.5.8 (Example 3.5.7 Continued) One can check that f1(x1) = θ
exp{–θx1} and f2(x2) = θ–1exp{–1/θ . x2}, and then one may apply (3.5.1) in-
stead to arrive at the same conclusion. In view of the Theorem 3.5.3 it becomes
really simple to write down the marginal pdf’s in the case of independence. !
Example 3.5.9 Suppose that the joint pdf of X1, X2, X3 is given by
where c(> 0) is a constant. One has f(x1, x2, x3) = ch1(x1)h2(x2)h3(x3) with
                exp{–2x1},
   and h3(x3) = exp
 , for all values of x1 ∈ ℜ+, (x2, x3) Î ℜ2. Also, the supports χ1 ∈ ℜ+, χ2 = ℜ, χ3
= ℜ are unrelated to each other. Hence, by appealing to the Theorem 3.5.3 we
can claim that X1, X2 and X3 are independent random variables. One should
again note the interesting facts. The three chosen functions h1(x1), h2(x2) and
h3(x3) are not even probability densities to begin with. The other point is that the
choices of these three functions are not unique. !
Example 3.5.10 (Example 3.5.9 Continued) One should check these out:
By looking at h1(x1) one can immediately guess that X1 has the Gamma(α =
1/2, β = 1/2) distribution so that its normalizing constant           .   By
looking at h2(x2) one can immediately guess that X2 has the Cauchy dis-
tribution so that its normalizing constant 1/π. Similarly, by looking at
h3(x3) one can immediately guess that X3 has the N(0, 1/2π) distribution
so that its normalizing constant is unity. Hence,
      (1/π)(1)=

3. Multivariate Random Variables
131
     Refer to (1.7.13), (1.7.20) and (1.7.31) as needed. Note that the
Theorem 3.5.3 really makes it simple to write down the marginal pdf’s in
the case of independence. !
It is crucial to note that the supports χi’s are assumed to
be unrelated to each other so that the representation given
by (3.5.6) may lead to the conclusion of independence among
the random variables Xi’s. Look at the Example 3.5.11
to see what may happen when the supports are related.
Example 3.5.11  (Examples 3.3.1 and 3.5.2 Continued) Consider two
random variables X1 and X2 whose joint continuous distribution is given by
the following pdf:
Obviously, one has χ1 = χ2 = (0, 1). One may be tempted to denote,
for example, h1(x1) = 6, h2(x2) = (1 – x2). At this point, one may be
tempted to claim that X1 and X2 are independent. But, that will be
wrong! On the whole space χ1 × χ2, one can not claim that f(x1, x2)
= h1(x1)h2(x2). One may check this out by simply taking, for ex-
ample, x1 = 1/2, x2 = 1/4 and then one has f(x1, x2) = 0 whereas
h 1(x 1)h 2(x 2) = 9/2. But, of course the relationship f(x 1, x 2) =
h1(x1)h2(x2) holds in the subspace where 0 < x1 < x2 < 1. From the
Example 3.5.2 one will recall that we had verified that in fact the
two random variables X1 and X2 were dependent. !
3.6   The Bivariate Normal Distribution
Let (X1, X2) be a two-dimensional continuous random variable with
the following joint pdf:
 with
The pdf given by (3.6.1) is known as the bivariate normal or two-dimensional
normal density. Here, µ1, µ2, σ1, σ2 and ρ are referred to as the parameters

132
3. Multivariate Random Variables
of the distribution. A random variable (X1, X2) having the pdf given by (3.6.1)
is denoted by N2(µ1, µ2,        , ρ). The pdf f(x1, x2) from (3.6.1) is centered
around the point (µ1, µ2) in the x1, x2 plane. The bivariate normal pdf given by
(3.6.1) has been plotted in the Figures 3.6.1-3.6.2.
The Figure 3.6.2 (a) appears more concentrated around its center than
its counterpart in the Figure 3.6.1 (a). This is due to the fact that we have
taken ρ = .5 to draw the former picture in contrast with the value ρ = 0 in
the latter picture, while the other parameters are held fixed. Clearly, the
ordinate at the center in the Figure 3.6.1 (a) happens to be 1/2π (≈ .15915)
whereas the ordinate at the center in the Figure 3.6.2 (a) happens to be (1/
2π)        (≈ .18378) which is larger. This justifies the preceding claim. One
observes a similar feature when the Figures 3.6.1 (b)-3.6.2 (b)

3. Multivariate Random Variables
133
are visually compared.
How can one show directly that f(x1, x2) from (3.6.1) is
indeed a genuine pdf? The derivation follows shortly.
The function f(x1, x2) is always positive. So, we merely need to verify that
the double integral of f(x1, x2) over the whole space ℜ2 is unity. With u1, u2
from (3.6.2), let us then rewrite
Hence, with c defined in (3.6.2) we obtain
Now, for all fixed x2 ∈ ℜ, let us denote
so that with
       we obtain
Next, look at the expression of ag(x1, x2) obtained from (3.6.5) and note that
for all fixed x2, it resembles the pdf of a univariate normal variable with mean
µ1 + ρσ1(x2 – µ2)/σ2 and variance
    at the point x1 ∈ ℜ. Hence, we
must have

134
3. Multivariate Random Variables
Again note that with
   , the expression bh(x2) obtained from
(3.6.5) happens to be the pdf of a normal variable with mean µ2 and variance
      at the point x2 ∈ ℜ. Hence, we must have
so that (3.6.8) can be rewritten as
by the definition of c from (3.6.2). Thus, we have directly verified that the
function f(x1, x2) given by (3.6.1) is indeed a genuine pdf of a two-dimen-
sional random variable with its support ℜ2.
Theorem 3.6.1  Suppose that (X1, X2) has the N2 (µ1, µ2,        , ρ)
distribution with its pdf f(x1, x2) given by (3.6.1). Then,
(i)
the marginal distribution of Xi is given by N(µi,     ), i = 1, 2;
(ii)
the conditional distribution of X1 | X2 = x2 is normal with mean
µ1+
 (x2 – µ2) and variance ),         for all
fixed x2∈ℜ;
(iii)
the conditional distribution of X2 | X1 = x1 is normal with
mean µ2 + 
 (x1 – µ1) and variance
  , for all fixed
x1∈ℜ.
Proof (i)  We simply show the derivation of the marginal pdf of the ran-
dom variable X2. Using (3.3.2) one gets for any fixed x2 ∈ ℜ,
which can be expressed as
This shows that X2 is distributed as N(µ2,    ). The marginal pdf of X1 can be
found easily by appropriately modifying (3.6.5) first. We leave this as the
Exercise 3.6.1.

3. Multivariate Random Variables
135
(ii) Let us denote
  and a = {2π(1–
.
       Utilizing (3.3.5), (3.6.4)-(3.6.5) and (3.6.11), the conditional pdf
of X1 | X2 = x2 is given by
The expression of f1|2(x1) found in the last step in (3.6.12) resembles the
pdf of a normal random variable with mean
 
and
variance
(iii) Its proof is left as the Exercise 3.6.2. ¢
Example 3.6.1 Suppose that (X1, X2) is distributed as N2 (0,0,4,1,ρ) where
ρ = 1/2. From the Theorem 3.6.1 (i) we already know that E[X2] = 0 and
V[X2] = 1. Utilizing part (iii), we can also say that E[X2 | X1 = x1] = 1/4x1 and
V[X2 | X1 = x1] = 3/4. Now, one may apply the Theorem 3.3.1 to find indi-
rectly the expressions of E[X2] and V[X2]. We should have E[X2] = E{E[X2 |
X1 = x1]} = E[1/4 X1] = 0, whereas V[X2] = V{E[X2 | X1 = x1]} + E{V[X2 | X1
= x1]} = V[1/4X1] + E[3/4] = 1/16V[X1] + 3/4 = 1/16(4) + 3/4 = 1. Note that
in this example, we did not fully exploit the form of the conditional distribu-
tion of X2 | X1 = x1. We merely used the expressions of the conditional mean
and variance of X2 | X1 = x1. !
In the Example 3.6.2, we exploit more fully the form
of the conditional distributions.
Example 3.6.2 Suppose that (X1, X2) is distributed as N2(0,0,1,1,ρ) where
ρ = 1/2. We wish to evaluate E{exp[1/2X1X2]}. Using the Theorem 3.3.1 (i),
we can write
But, from the Theorem 3.6.1 (iii) we already know that the conditional distri-
bution of X2 | X1 = x1 is normal with mean 1/2x1 and variance 3/4. Now,
E(exp1/2x1X2] | X1 = x1) can be viewed as the conditional mgf of X2 | X1 = x1.
Thus, using the form of the mgf of a univariate normal random variable from
(2.3.16), we obtain

136
3. Multivariate Random Variables
From the Theorem 3.6.1 (i), we also know that marginally X1 is distributed as
N(0,1). Thus, we combine (3.6.13)-(3.6.14) and get
With σ2 = 16/5, let us denote
         exp{–5/32u2}, u ∈ ℜ. Then,
h(u) is the pdf of a random variable having the N(0,σ2) distribution so that
∫ℜh(u)du = 1. Hence, from (3.6.15) we have
In the same fashion one can also derive the mgf of the random variable
X1X2, that is the expression for the E{exp[tX1X2]} for some appropriate range
of values of t. We leave this as the Exercise 3.6.3. !
The reverse of the conclusion given in the Theorem 3.6.1, part (i)
is not necessarily true. That is, the marginal distributions of both
X1 and X2 can be univariate normal, but this does not imply that
(X1,X2) is jointly distributed as N2. Look at the next example.
Example 3.6.3  In the bivariate normal distribution (3.6.1), each random
variable X1,X2 individually has a normal distribution. But, it is easy to con-
struct two dependent continuous random variables X1 and X2 such that mar-
ginally each is normally distributed whereas jointly (X1,X2) is not distributed
as N2.
Let us temporarily write f(x1, x2;µ1,µ2,            ρ) for the pdf given in (3.6.1).
Next, consider any arbitrary 0 < α , ρ < 1 and fix them. Let us now define
for–∞ < x1, x2 < ∞ Since the non-negative functions f(x1, x2; 0,0,1,1,ρ) and
f(x1, x2; 0, 0, 1, 1, – ρ) are both pdf’s on ℜ2, we must have

3. Multivariate Random Variables
137
Hence, we can express ∫ℜ2∫ g(x1, x2; ρ) dx1 dx2 as
Also, g(x1, x2;ρ) is non-negative for all (x1, x2) ∈ ℜ2. Thus, g(x1, x2) is a
genuine pdf on the support ℜ2.
Let (X1, X2) be the random variables whose joint pdf is g(x1, x2;ρ) for all
(x1, x2) ∈ ℜ2. By direct integration, one can verify that marginally, both X1 and
X2 are indeed distributed as the standard normal variables.
The joint pdf g(x1, x2; ρ) has been plotted in the Figures 3.6.3 (a) and (b) with
α = .5,.1 respectively and α = .5. Comparing these figures visually with those
plotted in the Figures 3.6.1-3.6.2, one may start wondering whether g(x1, x2;
ρ) may correspond to some bivariate normal pdf after all!
Figure 3.6.3. The PDF g(x1, x2; ρ) from (3.6.17):
(a) ρ = .5, α = .5 (b) ρ = .5, α = .1
But, the fact of the matter is that the joint pdf g(x1, x2; ρ) from (3.6.17) does
not quite match with the pdf of any bivariate normal distribution. Look at the
next example for some explanations. !
How can one prove that the joint pdf g(x1, x2;ρ) from (3.6.17) can not
match with the pdf of any bivariate normal distribution?
Look at the Example 3.6.4.
Example 3.6.4 (Example 3.6.3 Continued) Consider, for example, the situ-
ation when ρ = .5, α = .5. Using the Theorem 3.6.1 (ii), one can check

138
3. Multivariate Random Variables
that
whatever be fixed x2 ∈ ℜ, since α = .5. Suppose that it is possible for the pair
(X1, X2) to be distributed as the bivariate normal variable, N2(0,0,1,1, ρ*) with
some ρ* ∈ (–1, 1). But, then E[X1 | X2 = x2] must be ρ*x2 which has to match
with the answer zero obtained in (3.6.19), for all x2 ∈ ℜ. In other words, ρ*
must be zero. Hence, for all (x1, x2) ∈ ℜ2 we should be able to write
Now, g(0,0; ρ = .5) =
   , but h(0,0) = 1/2π. It is obvious that g(0,0; π =.5)
≠ h(0,0). Hence, it is impossible for the random vector (X1, X2) having the pdf
g(x1, x2; ρ = .5) to be matched with any bivariate normal random vector. !
The Exercise 3.6.8 gives another pair of random variables X1 and
X2 such that marginally each is normally distributed whereas
jointly (X1, X2) is not distributed as N2.
For the sake of completeness, we now formally define what is known as
the regression function in statistics.
Definition 3.6.1 Suppose that (X1, X2) has the N2 (µ1, µ2,           , ρ)
distribution with its pdf f(x1, x2) given by (3.6.1). The conditional mean of X1
| X2 = x2, that is, µ1 +
 (x2–µ2) is known as the regression function of X1 on
X2. Analogously, the conditional mean of X2 | X1 = x1, that is, µ2 + 
 (x1 – µ1)
is known as the regression function of X2 on X1.
Even though linear regression analysis is out of scope for this textbook,
we simply mention that it plays a very important role in statistics. The readers
have already noted that the regression functions in the case of a bivariate
normal distribution turn out to be straight lines.
We also mention that Tong (1990) had written a whole book devoted en-
tirely to the multivariate normal distribution. It is a very valuable resource,
particularly because it includes the associated tables for the percentage points
of the distribution.

3. Multivariate Random Variables
139
3.7 Correlation Coefficient and Independence
We begin this section with a result which clarifies the role of the zero
correlation in a bivariate normal distribution.
Theorem 3.7.1 Suppose that (X1, X2) has the bivariate normal distri-
bution N2(µ1, µ2,            , ρ) with the joint pdf given by (3.6.1) where –∞
< µ1, µ2 < ∞, 0 < σ1, σ2 < ∞ and –1 < ρ < 1. Then, the two random
variables X1 and X2 are independent if and only if the correlation coeffi-
cient ρ = 0.
Proof We first verify the “necessary part” followed by the “sufficiency
part”.
Only if part: Suppose that X1 and X2 are independent. Then, in view of
the Theorem 3.5.2 (i), we conclude that Cov(X1, X2) = 0. This will imply
that ρ = 0.
If part: From (3.6.1), let us recall that the joint pdf of (X1, X2) is given
by
with
But, when ρ = 0, this joint pdf reduces to
where
                exp{–1/2(xi – µi)2/      }, i = 1, 2. By appealing
to the Theorem 3.5.3 we conclude that X1 and X2 are independent. ¢
However, the zero correlation coefficient between two arbitrary
random variables does not necessarily imply that these two variables
are independent. Examples 3.7.1-3.7.2 emphasize this point.
Example 3.7.1 Suppose that X1 is N(0, 1) and let `X2 =    . Then,
Cov(X1, X2) = E(X1X2) – E(X1)E(X2) =           – E(X1)E(X2) = 0, since
E(X1) = 0 and
= 0. That is, the correlation coefficient ρX1,X2 is
zero. But the fact that X1 and X2 are dependent can be easily verified as
follows. One can claim that P {X2 > 4} > 0, however, the conditional
probability, P {X2 > 4 | –2 ≤ X1 ≤ 2} is same as P
     –2 ≤ X1 ≤ 2}

140
3. Multivariate Random Variables
which happens to be zero. Thus, we note that P {X2 > 4 | –2 ≤ X1 ≤ 2} ≠ P
{X2 > 4}. Hence, X1 and X2 are dependent variables. !
One can easily construct similar examples in a discrete
situation. Look at the Exercises 3.7.1-3.7.3.
Example 3.7.2 Suppose that Θ is distributed uniformly on the interval
[0, 2π). Let us denote X1 = cos(Θ), X2 = sin(Θ). Now, one has E[X1] =
             Also, one can write E[X1X2] =
. Thus,
Cov(X1, X2) = E(X1X2) – E(X1)E(X2) = 0 – 0 = 0. That is, the correlation
coefficient ρX1,X2 is zero. But the fact that X1 and X2 are dependent can be
easily verified as follows. One observes that
and hence condi-
tionally given X1 = x1, the random variable X2 can take one of the possible
values,
   or
          with probability 1/2 each. Suppose that we
fix x1 = . Then, we argue that P{–1/4 < X2 < 1/4 | X1 =
} = 0, but obvi-
ously P{–1/4 < X2 < 1/4} > 0. So, the random variables X1 and X2 are depen-
dent. !
Theorem 3.7.1 mentions that ρX1,X2 = 0 implies independence
between X1 and X2 when their joint distribution is N2. But,
ρX1,X2 = 0 may sometimes imply independence between
X1 and X2 even when their joint distribution is different from
the bivariate normal. Look at the Example 3.7.3.
Example 3.7.3 The zero correlation coefficient implies independence not
merely in the case of a bivariate normal distribution. Consider two random
variables X1 and X2 whose joint probability distribution is given as follows:
Each expression in the Table 3.7.1 involving the p’s is assumed positive and
smaller than unity.
Table 3.7.1. Joint Probability Distribution of X1 and X2
X1values
Row
0
1
Total
0
1 – p1
p1+ p
1 – p2
X2
 – p1 +p
values
1
 p2 – p
p
p2
Col. Total
1 – p1
p1
1
Now, we have Cov(X1, X2) = E(X1X2)–E(X1)E(X2) = P{X1 = 1∩X2 = 1} –
P{X1 = 1} P{X2 = 1} = p – p1p2, and hence the zero correlation

3. Multivariate Random Variables
141
coefficient between X1 and X2 will amount to saying that p = p1p2 where so far
p, p1 and p2 have all been assumed to lie between (0, 1) but they are otherwise
arbitrary. Now, we must have then P(X1 = 1∩X2 = 0) = p1 – p, and P(X1 =
0nX2 = 0) = 1–p1–p2+p, and P(X1=0∩X2=1) = p2–p. But, now P(X1 = 0nX2 =
1) = p2–p = p2–p1p2 = p2(1–p1) = P(X1=0) P(X2=1); P(X1 = 1 ∩ X2 = 0) = p1 –
p = p1 – p1p2 = p1(1 – p2) = P(X1 = 1) P(X2 = 0); and P(X1=0 ∩ X2=0) = 1–p1–
p2+p = 1–p1–p2+p1p2 = (1–p1)(1–p2) = P(X1=0) P(X2=0). Hence, the two such
random variables X1 and X2 are independent. Here, the zero correlation coef-
ficient implied independence, in other words the property that “the zero cor-
relation coefficient implies independence” is not a unique characteristic prop-
erty of a bivariate normal distribution. !
Example 3.7.4  There are other simple ways to construct a pair of random
variables with the zero correlation coefficient. Start with two random vari-
ables U1, U2 such that V(U1) = V(U2). Let us denote X1 = U1 + U2 and X2 = U1
– U2. Then, use the bilinear property of the covariance function which says
that the covariance function is linear in both components. This property was
stated in the Theorem 3.4.3, part (iv). Hence, Cov(X1, X2) = Cov(U1 + U2, U1
– U2) = Cov(U1, U1) – Cov(U1, U2) + Cov(U2, U1) – Cov(U2, U2) = V(U1) –
V(U2) = 0. !
3.8   The Exponential Family of Distributions
The exponential family of distributions happens to be very rich when it comes
to statistical modeling of datasets in practice. The distributions belonging to
this family enjoy many interesting properties which often attract investigators
toward specific members of this family in order to pursue statistical studies.
Some of those properties and underlying data reduction principles, such as
sufficiency or minimal sufficiency, would impact significantly in Chapter 6
and others. To get an idea, one may simply glance at the broad ranging results
stated as Theorems 6.2.2, 6.3.3 and 6.3.4 in Chapter 6. In this section, we
discuss briefly both the one-parameter and multi-parameter exponential fami-
lies of distributions.
3.8.1   One-parameter Situation
Let X be a random variable with the pmf or pdf given by f(x; θ), x ∈ χ ⊆ ℜ,
θ ∈ Θ ⊆ ℜ. Here, θ is the single parameter involved in the expression of f(x;
θ) which is frequently referred to as a statistical model.

142
3. Multivariate Random Variables
Definition 3.8.1 We say that f(x;θ) belongs to the one-parameter expo-
nential family if and only if we can express
with appropriate forms of real valued functions a(θ) ≥ 0, b(θ), g(x) ≥ 0 and
R(x), where χ ⊆ ℜ and Θ is a subinterval of the real line ℜ. It is crucial to
note that the expressions of a(θ) and b(θ) can not involve x, while the expres-
sions of g(x) and R(x) can not involve θ.
Many standard distributions, including several listed in Section 1.7, belong
to this rich class. Let us look at some examples.
Example 3.8.1 Consider the Bernoulli distribution from (1.7.1). Let us
rewrite the pmf f(x; p) as
which now resembles (3.8.1) where θ = p, a(θ) = 1 – θ, g(x) = 1, b(θ) =
log{θ(1 – θ)–1}, R(x) = x, and Θ = (0, 1), χ = {0, 1}. !
Example 3.8.2 Let X be distributed as Poisson(λ), defined in (1.7.4) where
λ(> 0). With χ = {0, 1, 2, ...}, θ = λ, and Θ = (0, ∞), the pmf f(x; θ) = e–θ θx/
x! has the same representation given in (3.8.1) where g(x) = (x!)–1, a(θ) =
exp{–θ}, b(θ) = log(θ) and R(x) = x. !
Example 3.8.3 Let X be distributed as N(µ, 1), defined in (1.7.13) where
µ ∈ ℜ. With χ = ℜ, θ = µ, and Θ = ℜ, the pdf
  exp{–
1/2(x – θ)2} has the same representation given in (3.8.1) where g(x) = exp{–
1/2x2},
               exp{–1/2θ2}, b(θ) = θ and R(x) = x. !
One should not however expect that every possible distribution would
necessarily belong to this one-parameter exponential family. There are many
examples where the pmf or the pdf does not belong to the class of distribu-
tions defined via (3.8.1).
All one-parameter distributions do not necessarily belong to the
exponential family (3.8.1). Refer to the Examples 3.8.4-3.8.6.
In the Table 3.8.1, we explicitly show the correspondence between some
of the standard distributions mentioned in Section 1.7 and the associated rep-
resentations showing their memberships in the one-parameter exponential fam-
ily. While verifying the entries given in the Table 3.8.1, the reader will imme-
diately realize that the expressions for a(θ), b(θ), g(x) and R(x) are not really
unique.

3. Multivariate Random Variables
143
Example 3.8.4 Consider a random variable X having a negative exponen-
tial pdf, defined in (1.7.36). The pdf is θ–1 exp{–(x – θ)/θ}I(x > θ) with θ > 0,
where I(.) is an indicator function. Recall that I(A) is 1 or 0 according as the
set A or Ac is observed. The term I(x > θ) can not be absorbed in the expres-
sions for a(θ), b(θ), g(x) or R(x), and hence this distribution does not belong
to a one-parameter exponential family. !
Example 3.8.5 Suppose that a random variable X has the uniform distribu-
tion, defined in (1.7.12), on the interval (0, θ) with θ > 0. The pdf can be
rewritten as f(x; θ) = θ–1I(0 < x < θ). Again, the term I(x > θ) can not be
absorbed in the expressions for a(θ), b(θ), g(x) or R(x), and hence this distri-
bution does not belong to a one-parameter exponential family. !

144
3. Multivariate Random Variables
In the two Examples 3.8.4-3.8.5, the support χ depended
on the single parameter θ. But, even if the support χ does
not depend on θ, in some cases the pmf or the pdf may not
belong to the exponential family defined via (3.8.1).
Example 3.8.6  Suppose that a random variable X has the N(θ, θ2) distri-
bution where θ(> 0) is the single parameter. The corresponding pdf f(x; θ)
can be expressed as
which does not have the same form as in (3.8.1). In other words, this distri-
bution does not belong to a one-parameter exponential family. !
A distribution such as N(θ, θ2) with θ(> 0) is said to belong to a
curved exponential family, introduced by Efron (1975, 1978).
3.8.2   Multi-parameter Situation
Let X be a random variable with the pmf or pdf given by f(x; θθθθθ), x ∈ χ⊆ ℜ, θθθθθ
= (θ1, ..., θk) ∈ Θ ⊆ ℜk. Here, θθθθθ is a vector valued parameter having k compo-
nents involved in the expression of f(x; θθθθθ) which is again referred to as a
statistical model.
Definition 3.8.2  We say that f(x; θθθθθ) belongs to the k-parameter exponen-
tial family if and only if one can express
with some appropriate forms for g(x) ≥ 0, a(θθθθθ) ≥ 0, bi(θθθθθ) and Ri(x), i = 1, ...,
k. It is crucial to note that the expressions of a(θθθθθ) and bi(θθθθθ), i = 1, ..., k, can
not involve x, while the expressions of g(x) and R1(x), ..., Rk(x) can not in-
volve θθθθθ.
Many standard distributions, including several listed in Section 1.7, belong
to this rich class. In order to involve only statistically meaningful
reparameterizations while representing f(x; θθθθθ) in the form given by (3.8.4),
one would assume that the following regulatory conditions are satisfied:

3. Multivariate Random Variables
145
In the contexts of both the one-parameter and multi-parameter exponential
families, there are such notions referred to as the natural parameterization,
the natural parameter space, and the natural exponential family. A serious
discussion of these topics needs substantial mathematical depth beyond the
assumed prerequisites. Elaborate discussions of intricate issues and related
references are included in Chapter 2 of both Lehmann (1983, 1986) and
Lehmann and Casella (1998), as well as Barndorff-Nielson (1978). Let us
again consider some examples.
Example 3.8.7  Let X be distributed as N(µ, σ2), with k = 2, θθθθθ = (µ, σ) ∈
ℜ × ℜ+ where µ and σ are both treated as parameters. Then, the correspond-
ing pdf has the form given in (3.8.4) where x ∈ ℜ, θ1 = µ, θ2 = σ, R1(x) = x,
R2(x) = x2, and
 ,           g(x) = 1,
      ,
and .
   !
Example 3.8.8  Let X be distributed as Gamma(α, β) where both α(> 0)
and β (> 0) are treated as parameters. The pdf of X is given by (1.7.20) so
that f(x; α, β) = {βα  Γ(α)}–1exp(–x/β)xα–1, where we have k = 2, θθθθθ = (α, β) ∈
ℜ+ × ℜ+, x ∈ ℜ+. We leave it as an exercise to verify that this pdf is also of the
form given in (3.8.4). !
The regularity conditions stated in (3.8.5) may sound too mathematical.
But, the major consolation is that many standard and useful distributions in
statistics belong to the exponential family and that the mathematical condi-
tions stated in (3.8.5) are routinely satisfied.
3.9   Some Standard Probability Inequalities
In this section, we develop some inequalities which are frequently encoun-
tered in statistics. The introduction to each inequality is followed by a few
examples. These inequalities apply to both discrete and continuous random
variables.
3.9.1   Markov and Bernstein-Chernoff Inequalities
Theorem 3.9.1  (Markov Inequality)  Suppose that W is a real valued
random variable such that P(W = 0) = 1 and E(W) is finite. Then, for any
fixed δ (> 0), one has:
Proof Suppose that the event A stands for the set [W ≥ δ], and then

146
3. Multivariate Random Variables
we can write
where recall that IA is the indicator function of A. One can check the validity
of (3.9.2) as follows. On the set A, since one has IA = 1, the rhs of (3.9.2) is
ä and it is true then that W ≥ δ. On the set Ac, however, one has IA = 0, so that
the rhs of (3.9.2) is zero, but we have assumed that W ≥ 0 w.p.1. Next,
observe that IA is a Bernoulli random variable with p = P(A). Refer to the
Example 2.2.3 as needed. Since, W – δIA ≥ 0 w.p.1, we have E(W – δIA) = 0.
But, E(W – δIA) = E(W) – δP(A) so that E(W) – δP(A) = 0, which implies that
P(A) ≥ δ–1 E(W). ¢
Example 3.9.1 Suppose that X is distributed as Poisson(λ = 2). From the
Markov inequality, we can claim, for example, that P{X ≥ 1} ≤ (1)(2) = 2.
But this bound is useless because we know that P{X ≥ 1} lies between 0 and
1. Also, the Markov inequality implies that P{X ≥ 2} ≤ (1/2)(2) = 1, which is
again a useless bound. Similarly, the Markov inequality implies that P{X = 10}
= (1/10)(2) = .2, whereas the true probability, P{X ≥ 10} = 1 – .99995 =
.00005. There is a serious discrepancy between the actual value of P{X ≥ 10}
and its upper bound. However, this discussion may not be very fair because
after all the Markov inequality provides an upper bound for P(W ≥ δ) without
assuming much about the exact distribution of W. !
Example 3.9.2 Consider a random variable with its distribution as follows:
One may observe that E[X] = (–1/7)(.7)+(1)(.1)+(10)(.2) = 2 and P{X ≥ 10}
= .2. In this case, the upper bound for P{X ≥ 10} obtained from (3.9.1) is
also .2, which happens to match with the exact value of P{X ≥ 10}. But, that
should not be the key issue. The point is this: The upper bound provided by
the Markov inequality is distribution-free so that it works for a broad range of
unspecified distributions. !
Note that the upper bound for P(W ≥ δ) given by (3.9.1)
is useful only when it is smaller than unity.
The upper bound given by (3.9.1) may appear crude, but even so,
the Markov inequality will work like a charm in some derivations.
The next Theorem 3.9.2 highlights one such application.
Theorem 3.9.2 (Bernstein-Chernoff Inequality) Suppose that X is
a real valued random variable whose mgf Mx (t) is finite for some t ∈ T ⊆

3. Multivariate Random Variables
147
(0, ∞). Then, for any fixed real number a, one has
Proof Observe that for any fixed t > 0, the three sets [X ≥ a], [tX ≥ ta] and
[etX ≥ eta] are equivalent. Hence, for t ∈ Γ, we can rewrite P{X ≥ a} as
using (3.9.1) with W = etX, δ = eta. Now, since we are looking for an upper
bound, it makes sense to pick the smallest upper bound available in this class.
Thus, we take
           as the upper bound. ¢
Example 3.9.3  Suppose that the random variable X has the exponential
distribution with its pdf f(x) = e–xI(x > 0). The corresponding mgf is given by
MX(t) = (1 – t)–1 for t ∈ e = (–∞, 1). For any fixed a > 1, the Theorem 3.9.2
implies the following:
Now, let us denote g(t) = e–ta(1 – t)–1 and h(t) = log(g(t)) for 0 < t < 1. It is
easy to check that h’(t) = –a + (1 – t)–1, h’’(t) = (1 – t)–2 so that h’(t) = 0 when
t ≡ t0 = 1 – a–1 which belongs to (0, 1). Also, h’’(t0) is positive so that the
function h(t) attains the minimum at the point t = t0. Refer to (1.6.27) as
needed. Since g(t) is a one-to-one monotone function of h(t), we conclude
that the function g(t) attains the minimum at the point t = t0. In other words,
we can rewrite (3.9.5) as
Does the upper bound in (3.9.6) lie between 0 and 1? In order to answer this
question, next look at the function m(x) = (x – 1) – log(x) for x ≥ 1. Obvi-
ously, m(1) = 0 and m’(x) = 1 – x–1 ≥ 0, where equality holds if and only if x
= 1. That is, for all x > 1, we have an increasing function m(x) But, since m(1)
= 0, we claim that m(x) > 0 for x > 1. In other words, for a > 1, we have
log(a) < a – 1 so that a < ea–1. Hence, the upper bound given by (3.9.6) is a
number which lies between zero and one. !
The types of bounds given by (3.9.5) are customarily used in
studying the rate of convergence of tail area probabilities.

148
3. Multivariate Random Variables
Example 3.9.4 Suppose that X is a random variable whose mgf MX(t) is
finite for some t ∈ T ⊆ (–∞, 0). Then, it follows from the Theorem 3.9.2 that
for any fixed real number a, one can claim:
{e–ta MX(t)}. Its verification is left as the Exercise 3.9.1. !
The following section provides yet another application of the Markov
inequality.
3.9.2   Tchebysheff’s Inequality
This inequality follows from a more general inequality (Theorem 3.9.4)
which is stated and proved a little later. We take the liberty to state the simpler
version separately for its obvious prominence in the statistical literature.
Theorem 3.9.3 (Tchebysheff’s Inequality) Suppose that X is a real val-
ued random variable with the finite second moment. Let us denote its mean ì
and variance α 2(> 0). Then, for any fixed real number ε(> 0), one has
We know that P{|X – µ| < kσ} = 1 – P{|X – µ| ≥ kσ}. Thus, with k > 0,
if we substitute ε = kσ in (3.9.7), we can immediately conclude:
In statistics, sometimes (3.9.8) is also referred to as the Tchebysheff’s in-
equality. Suppose we denote pk = P{|X – µ| < kσ}. Again, (3.9.7) or equiva-
lently (3.9.8) provide distribution-free bounds for some appropriate probabil-
ity. Yet, let us look at the following table:
Table 3.9.1. Values of pk and the Tchebysheff’s Lower Bound (3.9.8)
k = 1       k = 2
k = 3
k = 4
Tchebysheff’s Bound
0       3/4= .75
8/9 ≈ .88889
15/16 = .9375
pk: X is N(0, 1)
.68268     .95450
.99730
 .99994
In the case of the standard normal distribution, the Tchebysheff’s lower bound
for pk appears quite reasonable for k = 3, 4. In the case k = 1, the Tchebysheff’s
inequality provides a trivial bound whatever be the distribution of X.

3. Multivariate Random Variables
149
Theorem 3.9.4 Suppose that X is a real valued random variable such that
with some r > 0 and T ∈ T(⊆ ℜ), one has ør = E{|X – T|r} which is finite. Then,
for any fixed real number ε(> 0), one has
Proof Note that P{|X – T| ≥ ε} = P{W ≥ εr} where W = |X – T|r. Now, the
inequality (3.9.9) follows immediately by invoking the Markov inequality. ¢
The Tchebysheff’s inequality follows immediately
from (3.9.9) by substituting r = 2 and T = µ.
3.9.3  Cauchy-Schwarz and Covariance Inequalities
If we have independent random variables X1 and X2, then we know from the
Theorem 3.5.1 that E[X1X2] = E[X1]E[X2]. But, if X1 and X2 are dependent,
then it is not always so simple to evaluate E[X1X2]. The Cauchy-Schwarz
inequality allows us to split E[X1X2] in the form of an upper bound having two
separate parts, one involving only X1 and the other involving only X2.
Theorem 3.9.5 (Cauchy-Schwarz Inequality) Suppose that we have
two real valued random variables X1 and X2, such that ,
       and
E[X1X2] are all finite. Then, we have
In (3.9.10), the equality holds if and only if X1 = kX2 w.p.1 for some constant
k.
Proof First note that if
          , then X2 = 0 w.p.1 so that both sides
of (3.9.10) will reduce to zero. In other words, (3.9.10) holds when                .
Now we assume that . Let λ be any real number. Then we can write
But note that (X1 + λ X2)2 is a non-negative random variable whatever be λ,
and so E[(X1 + λX2)2] ≥ 0 whatever be λ. If we substitute λ ≡ λ0 =

150
3. Multivariate Random Variables
–E[X1X2]
 in the last step of (3.9.11), we get
that is
                   since                 . Thus, (3.9.10) has
been proven.
Next, let us suppose that equality holds in (3.9.10), that is,
E2[X1X2] = 0. Then, from (3.9.10) we conclude that
It should be clear from (3.9.12) that with λ ≡ λ0 = –E[X1X2]/
we have
and again since (X1 + λ0X2)2 is a non-negative random variable w.p.1, one can
conclude that X1 + λ0X2 = 0 w.p.1. Hence, X1 = kX2 w.p.1 with some real
number k = –λ0.
Let us suppose that X1 = kX2 w.p.1 so that  E[X1X2] = k          and
              .  Hence, one immediately has E 2[X 1X 2] –
              . Now the proof is complete. ¢
Theorem 3.9.6 (Covariance Inequality) Suppose that we have two real
valued random variables X1, X2, such that
and E[X1X2] are
all finite. Then, we have
In (3.9.13), the equality holds if and only if X1 = a + bX2 w.p.1 for some
constants a and b.
Proof Recall from (3.4.1) that Cov(X1, X2) = E{(X1 – µ1)(X2 – µ2)} where
µi = E[Xi], i = 1, 2. Let us denote Ui = Xi – µi, i = 1, 2, and then by the
Cauchy-Schwarz inequality (3.9.10), we have
which verifies (3.9.13). The covariance inequality will become an equality if and
only if we have equality throughout (3.9.14), that is if and only if . From the
Cauchy-Schwarz inequality it then follows that we will have equality in (3.9.13) if
and only if U1 = kU2 w.p.1 with some constant k, which will be equivalent to the
claim that X1 = a + bX2 w.p.1. where a = µ1 – kµ2 and b = k. ¢

3. Multivariate Random Variables
151
Earlier we had seen applications of this covariance inequality in proving
the parts (ii)-(iii) in the Theorem 3.4.2 where we had shown that |ρX1,X2| ≤ 1
whereas |ρX1,X2 | = 1 if and only if X1 = a + bX2 w.p.1.
Example 3.9.5 (Examples 3.2.4, 3.2.5, 3.4.1 Continued) For the two ran-
dom variables on hand, recall that we had V(X1) = 6.4971 and V(X2) = .6825.
Thus, Cov2(X1, X2) ≤ (6.4971)(.6825) ≈ 4.4343. But, we knew that Cov(X1,
X2) = –0.2415. !
Example 3.9.6 (Example 3.4.3, 3.4.6 Continued) For the two random
variables on hand, recall that we had V(X1) = 3/80 and V(X2) = 19/320. Thus,
Cov2(X1, X2) ≤ (3/80)(19/320) = 57/25600 ≈ .00223. But, we had found ear-
lier that Cov(X1, X2) = 3/160 ≈ .01875. !
Example 3.9.7 Suppose that X2 is distributed as N(0, 5) and let X1 = 2X2 –
10–3
       Note that V[X2] = 5, but
The covariance inequality (3.9.13) would imply that
However, we know that Cov(X1, X2) = Cov(X2, X1) and hence, by appealing
to the Theorem 3.4.3 (ii), we can express Cov(X2, X1) as
The upper bound for Cov2(X1, X2) given by the covariance inequality was
100.00025 whereas the exact value of Cov2(X1, X2) is 100. This upper bound
and the actual value of Cov2(X1, X2) are almost indistinguishable because, for
all practical purposes, X1 is a linear function of X2. !

152
3. Multivariate Random Variables
3.9.4   Jensen’s and Lyapunov’s Inequalities
Consider a real valued random variable X with finite variance. It is clear that
which would imply that E[X2] ≥ E2[X]. That is, if we let f(x) = x2, then we can
claim
The question is this: Can we think of a large class of functions f(.) for which
the inequality in (3.9.16) would hold? In order to move in that direction, we
start by looking at the rich class of convex functions f(x) of a single real
variable x.
Definition 3.9.1 Consider a function f: ℜ → ℜ. The function f is called
convex if and only if
for all u, v ∈ ℜ and 0 ≤ α ≤ 1. The function f is called concave if and only if
– f is convex.
Figure 3.9.1. A Convex Function f(x)
The situation has been depicted in the Figure 3.9.1. The definition of the
convexity of a function demands the following geometric property: Start
with any two arbitrary points u, υ on the real line and consider the chord
formed by joining the two points (u, f(u)) and (υ, f(υ)). Then, the function
f evaluated at any intermediate point z, such as αu + (1 – α)υ with 0 ≤

3. Multivariate Random Variables
153
α ≤ 1, will lie under the chord.
Figure 3.9.2. A Convex Function f(x) and Tangent Lines (a), (b)
We may also look at a convex function in a slightly different way. A func-
tion f is convex if and only if the curve y = f(x) never goes below the tangent
of the curve drawn at the point (z, f(z)), for any z ∈ ℜ. This has been depicted
in the Figure 3.9.2 where we have drawn two tangents labelled as (a) and (b).
In the Figure 3.9.3, we have shown two curves, y = f(x) and y = g(x), x ∈ ℜ.
The function f(x) is convex because it lies above its tangents at any point
whatsoever. But, the function g(x) is certainly not convex because a
Figure 3.9.3. Two Functions: f(x) Is Convex, g(x) Is Not Convex
part of the curve y = g(x) lies below the chord as shown which is a violation
of the requirement set out in (3.9.17). In the Figure 3.9.3, if we drew the
tangent to the curve y = g(x) at the point (u, g(u)), then a part of the curve will
certainly lie below this tangent!
Example 3.9.8 Consider the following functions: (i) f(x) = x2 for x ∈ ℜ; (ii)
f(x) = ex for x ∈ ℜ; (iii) f(x) = max(0, x) for x ∈ ℜ; and (iv) f(x) = log(x) for
x ∈ ℜ+. The reader should verify geometrically or otherwise that the first
three functions are convex, while the fourth one is concave. !

154
3. Multivariate Random Variables
Figure 3.9.4. Plot of f(x) Defined in (3.9.19)
Example 3.9.9 The convexity or concavity of the functions cited in parts
(i), (ii) and (iv) of the Example 3.9.8 can be easily checked by verifying the
sufficient condition given in (3.9.18). But, a function f(x) may not even be
differentiable at all points x ∈ ℜ, and yet it may be convex (or concave). The
function cited in part (iii) of the Example 3.9.8 is of this type. Let us define
another function
which has been plotted in the Figure 3.9.4. One should verify that at the point
x = 0, the left hand derivative of f(x) is –1 whereas the right hand derivative of
f(x) is zero. In other words, the function f(x) is not differentiable at the point
x = 0, that is the condition laid out in (3.9.18) is not useful here.
But, after examining the plot of f(x) in the Figure 3.9.4, it becomes appar-
ent that f(x) is indeed a convex function. One should, however, check this
claim more formally. !
Theorem 3.9.7 (Jensen’s Inequality) Suppose that X is a real valued
random variable and let f(x), x ∈ ℜ be a convex function. Assume that E[X]
is finite. Then, one has

3. Multivariate Random Variables
155
Proof Note that if E[f(X)] is infinite, then the required inequality certainly
holds. So, let us assume now that E[f(X)] is finite. Since f(x) is convex, the
curve y = f(x) must lie above the tangents at all points (u, f(u)) for any u ∈ ℜ.
With u = E[X], consider specifically the point (u, f(u)) at which the equation
of the tangent line would be given by y = f(E[X]) + b{x – E[X]} for some
appropriate b. But, then we can claim that
Thus, using (3.9.21), we have
E[f(X)] = E[f(E(X)) + b{X – E(X)}] = f(E(X)) + b{E(X) – E(X)},
which reduces to f(E(X)). Thus, we have the desired result. ¢
In the statement of the Jensen’s inequality (3.9.20), equality
holds only when f(x) is linear in x ∈ ℜ. Also, the inequality
in (3.9.20) gets reversed when f(x) is concave.
Example 3.9.10 Suppose that X is distributed as Poisson (λ), λ > 0. Then,
using the Jensen’s inequality, for example, we immediately claim that E[X3] >
λ3. This result follows because the function f(x) = x3, x ∈ ℜ+ is convex and
E[X] = λ. !
Example 3.9.11 Suppose that X is distributed as N(–1, σ2), σ > 0. Then,
for example, we have E[|X|] > 1. This result follows immediately from the
Jensen’s inequality because the function f(x) = |x|, x ∈ ℜ is convex and |E[X]|
= |–1| = 1. !
Example 3.9.12 Suppose that X is distributed as N(µ, σ2), – ∞ < µ < ∞,
σ > 0. Then, for example, we have E[(X – µ)4] ≥ {E[(X – µ)2]}2, by the
Jensen’s inequality with f(x) = x2, x ∈ ℜ+ so that one can claim E[(X – µ)4]
≥ (σ2)2 = σ4. !
Example 3.9.13 Suppose that one is proctoring a makeup examination for
two students who got started at the same time. Let Xi be the time to complete
the examination for the ith student, i = 1, 2. Let X = max (X1, X2), the duration
of time the proctor must be present in the room. Then, one has

156
3. Multivariate Random Variables
so that E[X] ≥ max(E[X1], E[X2]). !
Example 3.9.14 Suppose that X is exponentially distributed with mean λ >
0. Then, what can we say about E[X1/2]? Look at the function
for x > 0. This function is concave because f 
//(x) = –1/4 x–3/2 < 0. Hence, from the
Jensen’s inequality we can immediately claim that
Similarly, one can also show that E[log(X)] ≥ log(E[X]) = log(λ). !
What follows is yet another nice application of the Jensen’s inequality.
Theorem 3.9.8 (Lyapunov’s Inequality) Suppose that X is a real valued
random variable. Let mr = E1/r[|X|r]. Then, mr is increasing for r ≥ 1.
Proof We want to show that
Observe that
using Jensen’s inequality since f(x) = xr/s, x > 0, is convex. Thus, one has
which is the desired result. ¢
3.9.5   Hölder’s Inequality
In the Cauchy-Schwarz inequality (3.9.10), the upper bound consisted of the
product of E[X2] and E[Y2]. However, what if we have a situation like this:
For the random variable X, higher than second moments may be assumed
finite, but for the random variable Y, lower than second moments may be
assumed finite. Under this scenario, the Cauchy-Schwarz inequality does not
help much to obtain an upper bound for E2[X1X2]. The following inequality is
more flexible. For brevity, we do not supply its proof.
Theorem 3.9.9 (Hölder’s Inequality) Let X and Y be two real valued
random variables. Then, with r > 1, s > 1 such that r–1 + s–1 = 1, one has
provided that E[|X|r] and E[|Y|s] are finite.
Example 3.9.15 Suppose that we have two dependent random vari-
ables X and Y where X is distributed as N(0, 1), but the pdf of Y is given by

3. Multivariate Random Variables
157
g(y) = 5/3y–8/3 I(1 < y < ∞). Now, by direct integration, one can check that
E[Y4/3] is finite but E[Y2] is not. Suppose that we want to get an upper bound
for |E[XY]|. The Cauchy-Schwarz inequality does not help because E[Y2] is
not finite even though E[X2] is. Let us try and apply the Hölder’s inequality
with r = 4 and s = 4/3 in order to obtain
Note that this upper bound is finite whatever be the nature of dependence
between these two random variables X and Y. !
3.9.6   Bonferroni Inequality
We had mentioned this inequality in the Exercise 1.3.5, part (ii), for two events
only. Here we state it more generally.
Theorem 3.9.10 (Bonferroni Inequality) Consider a sample space S
and suppose that ß is the Borel sigma-field of subsets of S. Let A1, ..., Ak be
events, that is these belong to ß. Then,
Proof In Chapter 1, we had proved that
so that we can write
This is valid because P(A1 ∪ A2) ≤ 1. Similarly, we can write
So, we first verified the desired result for k = 2 and assuming this, we
then proved the same result for k = 3. The result then follows by the
mathematical induction. ¢
The Exercise 3.9.10 gives a nice application combining the Tchebysheff’s
inequality and Bonferroni inequality.

158
3. Multivariate Random Variables
3.9.7   Central Absolute Moment Inequality
Let X be a real valued random variable from some population having its pmf
or pdf f(x). Suppose that E[X], that is the population mean, is µ. In statistics,
E[| X – µ |k] is customarily called the kth central absolute moment of X, k = 1,
2, ... . For example, suppose that X has the following distribution:
X values:
–1
0
2
2.5
Probabilities:
.1
.2
 .4
.3
One can check that E[X] = –.1 + 0 + .8 + .75 = 1.45 which is µ. Now let us
fix k = 3 and evaluate the 3
rd central absolute moment of X. We have
Now, one may ask: how is the 3rd central absolute moment of X any different
from the 3rd central moment of X? One should check that
One thing should be clear. The k
th central absolute moment of X should al-
ways be non-negative for any k = 1, 2, ... .
Now consider this scenario. Let X1, ..., Xn be real valued random samples
from a population having its pmf or pdf f(x). Let the population mean be µ.
One may be tempted to evaluate the average magnitude of the discrepancy
between the sample mean ,
    and the population
mean µ which will amount to E[|    – µ|]. One may even like to evaluate
E[|     – µ |k] for k = 1, 2, ... . But, then we need the exact distribution of      to
begin with. From Chapter 4 it will be clear that the determination of the exact
distribution of can be hard, even for some of the simplest looking population
pmf’s or pdf’s f(x).
The beauty and importance of the next inequality will be appreciated
more once we realize that it provides an upper bound for E[|      – µ|k]

3. Multivariate Random Variables
159
even though we may not know the exact distribution of ,    , k = 1, 2, ... .
The following result actually gives an upper bound for E[|      – µ |2ξ] with
ξ ≥ 1/2.
Theorem 3.9.11 (Central Absolute Moment Inequality) Suppose that
we have iid real valued random variables X1, X2, ... having the common mean
µ. Let us also assume that E[| X1 |2ξ] < ∞ for some ξ ≥ 1/2. Then, we have
gwhere k does not depend on n, and T = 2ξ – 1 or ξ according as 1/2 ≤ ξ < 1
or ξ ≥ 1 respectively.
The methods for exact computations of central moments for the sample
mean      were systematically developed by Fisher (1928). The classic text-
book of Cramér (1946a) pursued analogous techniques extensively. The par-
ticular inequality stated here is the special case of a more general large devia-
tion inequality obtained by Grams and Serfling (1973) and Sen and Ghosh
(1981) in the case of Hoeffding’s (1948) U-statistics. A sample mean      turns
out to be one of the simplest U-statistics.
3.10   Exercises and Complements
3.2.1 (Example 3.2.2 Continued) Evaluate f2(i) for i = –1, 0, 1.
3.2.2 (Example 3.2.4 Continued) Evaluate E[X1 | X2 = x2] where x2 = 1, 2.
3.2.3 (Example 3.2.5 Continued) Check that
= 16.21. Also
evaluate
3.2.4 Suppose that the random variables X1 and X2 have the following joint
distribution.
X2 values
X1 values
0
1
 2
0
0
3/15
3/15
1
2/15
6/15
0
2
1/15
0
 0
Using this joint distribution.

160
3. Multivariate Random Variables
(i)
find the marginal pmf’s for X1 and X2;
(ii) find the conditional pmf’s f1/2(x1) and f2/1(x2);
(iii) evaluate E[X1 | X2 = x2] and E[X2 | X1 = x1];
(iv) evaluate
         and
3.2.5 Prove the Theorem 3.2.2.
3.2.6 Prove the Theorem 3.2.3.
3.2.7 Consider an urn containing sixteen marbles of same size and weight
of which two are red, five are yellow, three are green, and six are blue. The
marbles are all mixed and then randomly one marble is picked from the urn
and its color is recorded. Then this marble is returned to the urn and again all
the marbles are mixed, followed by randomly picking a marble from the urn
and its color recorded. Again this marble is also returned to the urn and the
experiment continues in this fashion. This process of selecting marbles is
called sampling with replacement. After the experiment is run n times, sup-
pose that one looks at the number of red (X1), yellow (X2), green (X3) and blue
(X4) marbles which are selected.
(i)
What is the joint distribution of (X1, X2, X3, X4)?
(ii) What is the mean and variance of X3?
(iii) If n = 10, what is the joint distribution of (X1, X2, X3, X4)?
(iv) If n = 15, what is the conditional distribution of (X1, X2, X3) given
that X4 = 5?
(v)
If n = 10, calculate P{X1 = 1 ∩ X2 = 3 ∩ X3 = 6 ∩ X4 = 0}.
{Hint: In parts (i) and (v), try to use (3.2.8). In part (ii), can you use
(3.2.13)? For part (iii)-(iv), use the Theorem 3.2.2-3.2.3 respectively.}
3.2.8 The mgf of a random vector X = (X1, ..., Xk), denoted by MX(t) with
t = (t1, ..., tk), is defined as E[exp{t1X1 + ... + tkXk}]. Suppose that X is
distributed as Multk(n, p1, ..., pk). Then,
(i)
show that MX(t) = {p1et
k}n;
(ii) find the mgf MU(t) of U = X1 + X3 by substituting t1 = t3 = t and ti
= 0 for all i ≠ 1, 3 in the part (i). What is the distribution of U?
(iii) use the procedure from part (ii) to find the mgf of the sum of any
subset of random variables from X.
3.2.9 Suppose that the random variables X1 and X2 have the following joint
pmf:
where x1, x2 are integers, 0 ≤ x1 ≤ 3, 0 ≤ x2 ≤ 3, but 1 ≤ x1 + x2 ≤ 3. Then,

3. Multivariate Random Variables
161
(i)
find the marginal pmf’s for X1 and X2;
(ii) find the conditional pmf’s f1/2(x1) and f2/1(x2);
(iii) evaluate E[X1 | X2 = x2] and E[X2 | X1 = x1];
(iv) evaluate E[X1 X2],
        , and
3.2.10 Suppose that X = (X1, ..., X5) has the Mult5 (n, p1, ..., p5) with 0 <
pi < 1, i = 1, ..., 5,
     . What is the distribution of Y = (Y1, Y2, Y3)
where Y1 = X1 + X2, Y2 = X3, Y3 = X4 + X5? {Hint: Use the Theorem 3.2.2.}
3.2.11 Suppose that X = (X1, X2, X3) has the Mult3 (n, p1, p2, p3) with 0 <
pi < 1, i = 1, 2, 3,
     . Show that the conditional distribution of X1
given that X1 + X2 = r is Bin (r, p1/(p1 + p2)). {Hint: Use the Theorem 3.2.3.}
3.2.12 Suppose that X1 and X2 have the joint pmf
with 0 < p < 1, q = 1 – p. Find the marginal distributions of X1 and X2 and
evaluate P{X2 – X1 ≤ 1}.
3.2.13 Suppose that X1 and X2 have the joint pmf
(i)
Show that f1(x1) = 1/21 (2x1 + 3) for x1 = 1, 2, 3;
(ii) Show that f2(x2) = 1/7 (x2 + 2) for x2 = 1, 2;
(iii) Evaluate P{X1 ≤ 2} and P{X2 < 2}.
3.2.14 Suppose that X1 and X2 have the joint pdf given by
The surface represented by this pdf is given in the Figure 3.10.1. Show
that P{X1 X2 > a} = 1 – a + alog(a) for any 0 < a < 1. {Hint: Note that

162
3. Multivariate Random Variables
Figure 3.10.1. Plot of the PDF from the Exercise 3.2.14
3.3.1 (Examples 3.3.1-3.3.2 Continued) Evaluate E(X1) and V(X1). Also,
evaluate µ1/2 and
3.3.2 (Examples 3.3.3 Continued) Answer the following questions.
(i)
Evaluate E(Xi) and V(Xi), i = 1, 2 using (3.3.19);
(ii) Evaluate E[X1(1 – X1)],
           and E[(X1 + X2)2];
(iii) Show that fi|j(xi) = fi(xi) for i ≠ j = 1, 2.
3.3.3 (Example 3.3.5 Continued) Evaluate E(Xi) and V(Xi), i = 1, 2. Evalu-
ate µi|j and         for i ≠ j, i, j = 1, 2. Also evaluate
3.3.4 Let c be a positive constant such that X1 and X2 have the joint pdf
given by
Find the value of c. Derive the expressions of f1(x1), f2(x2), f1/2(x1), and
f2/1(x2). Evalute E(Xi) and V(Xi), i = 1, 2. Evaluate µi|j and      for i ≠ j,
i, j = 1, 2. Also evaluate E[X1 (1 – X1)],
     and E[(X1 +

3. Multivariate Random Variables
163
X2)2]. The surface represented by this pdf is given in the Figure 3.10.2.
Figure 3.10.2. Plot of the PDF from the Exercise 3.3.4 with c = 3/16
3.3.5 Let c be a positive constant such that X1 and X2 have the joint pdf
given by
Find the value of c. Derive the expressions of f1(x1), f2(x2), f1/2(x1), and f2/1(x2).
Does either of X1, X2 have a standard distribution which matches with one of
those distributions listed in Section 1.7? Evaluate E(Xi) and V(Xi), i = 1, 2.
Evaluate µi/j and    for i ≠ j, i, j = 1, 2. Also evaluate E[X1(1 – X1)],
and E[(X1 + X2)2]. The surface represented by this pdf is
given in the Figure 3.10.3.
Figure 3.10.3. Plot of the PDF from the Exercise 3.3.5 with c = 1
3.3.6 (Example 3.3.10 Continued) Evaluate E[X1 X2] and E[X1
     Also
evaluate E[X1(1 – X1)], E[X1 X2      ] and E[X1 + X2 + X3)2].
3.3.7 (Example 3.3.10 Continued) Find the expressions for E[X1
| X2 = x2, X3 = x3] and E[X1      | X2 = x2] with 0 < x2 < 2, 0 < x3 < 1.

164
3. Multivariate Random Variables
{Hints: First find the conditional pdf’s of X1 given X2 = x2, X3 = x3 and that of
(X1, X3) given X2 = x2. Next, express the expected values as the respective
integrals and then evaluate them.}
3.3.8 (Example 3.3.10 Continued) Choose any three specific pdf’s on three
finite intervals as ai(xi), i = 1, 2, 3. Then, construct an appropriate combina-
tion of these ai’s to define a pdf f(x) along the line of (3.3.47). Try putting
these components in a certain order and adjust the coefficients so that the
whole integral is one. {Hint: Examine closely why we had the success in the
equation (3.3.48).}
3.3.9 (Example 3.3.11 Continued) Evaluate E[X1 X2 X5] and E[       X5].
Also evaluate E[X1(1 – X5)], E[X1 X2(2 – X3)2] and E[(X1 + X2 + X3 + X4 +
X5)2].
3.3.10 (Example 3.3.11 Continued) Find the expressions for E[X1(1 – X5)
| X2 = x2, X4 = x4] and E[X1      | X5 = x5] with 0 < x2 < 2, 0 < x4, x5 < 1. {Hints:
First find the conditional pdf’s of (X1, X5) given X2 = x2, X4 = x4 and that of
(X1, X3) given X5 = x5. Next, express the expected values as the respective
integrals and then evaluate them.}
3.3.11 (Example 3.3.11 Continued) Choose any five specific pdf’s on five
finite intervals as ai(xi), i = 1, ..., 5. Then, construct an appropriate combina-
tion of these ai’s to define a pdf f(x) along the line of (3.3.53). Try putting
these components in a certain order and adjust the coefficients so that the
whole integral is one. {Hint: Examine closely how we had defined the func-
tion f in the equation (3.3.53).}
3.3.12 Let c be a positive constant such that X1, X2 and X3 have the joint
pdf given by
Find the value of c. Derive the marginal pdf’s of X1, X2 and X3. Does either of
X1, X2, X3 have a standard distribution which matches with one of those
distributions listed in Section 1.7? Evaluate E[X1 X2] and E[X1    ]. Also
evaluate E[X1(1 – X1)], E[X1 X2        ] and E[(X1 + X2 + X3)2].
3.4.1 (Example 3.2.1 Continued) In the case of the random variables X1,
X2 whose joint pdf was defined in the Table 3.2.1, calculate ρ, the correlation
coefficient between X1 and X2.
3.4.2 (Exercise 3.2.4 Continued) In the case of the random variables X1,
X2 whose joint pdf was defined in the Exercise 3.2.4, calculate ñ, the correla-
tion coefficient between X1 and X2.

3. Multivariate Random Variables
165
3.4.3 (Example 3.3.10 Continued) Evaluate Cov(Xi, Xj), i ≠ j = 1, 2, 3.
Then, evaluate ρXi, Xj, i ≠ j = 1, 2, 3.
3.4.4 (Example 3.3.11 Continued) Evaluate Cov(Xi, Xj), i ≠ j = 1, ..., 5.
Then, evaluate ρXi, Xj, i ≠ j = 1, ..., 5.
3.4.5 Prove the Theorem 3.4.3.
3.4.6 With any two random variables X1 and X2, show that
provided that Cov(X1, X2), V(X1), and V(X2) are finite.
3.4.7 (Exercise 3.4.6 Continued) Consider any two random variables X1
and X2 for which one has V(X1) = V(X2). Then, for this pair of random vari-
ables X1 and X2 we must have Cov(X1, X2) = 0. Next, using this ammunition,
construct several pairs of random variables which are uncorrelated.
3.4.8 (Examples 3.2.6-3.2.7 Continued) Consider the multinomial random
variable X = (X1, ..., X6) defined in the Example 3.2.6. Evaluate ρXi, Xj for all i
≠ j = 1, ..., 6.
3.4.9 (Example 3.2.8 Continued) Consider the multinomial random vari-
able X = (X1, X5,     ) defined in the Example 3.2.6 with      = n – (X1 + X5).
Evaluate ρX1, X5, ρX1,      and ρX5,      .
3.4.10 (Exercise 3.2.7 Continued) Consider the multinomial random vari-
able X = (X1, X2, X3, X4) defined in the Exercise 3.2.7. Evaluate ρXi, Xj for all i
≠ j = 1, 2, 3, 4.
3.4.11 (Exercise 3.2.10 Continued) Recall that X = (X1, ..., X5) has the
Mult5(n, p1, ..., p5) with 0 < pi < 1, i = 1, ..., 5,
whereas Y = (Y1,
Y2, Y3) where Y1 = X1 + X2, Y2 = X3, Y3 = X4 + X5. Evaluate ρYi, Yj for all i ≠ j =
1, 2, 3.
3.4.12 Suppose that X1 and X2 have the joint pdf given by
(i)
Show that f1(x1) = x1 + 1/2, f2(x2) = x2 + 1/2 for
0 < x1, x2 < 1;
(ii) Evaluate P{1/2 ≤ X1 ≤ 3/4 | 1/3 ≤ X2 ≤ 2/3}.
3.4.13 Suppose that X1 and X2 have the joint pdf given by

166
3. Multivariate Random Variables
Show that ρX1, X2 = – 5/139.
3.5.1 (Exercise 3.3.4 Continued) Let c be a positive constant such that X1
and X2 have the joint pdf given by
Prove whether or not X1 and X2 are independent. Solve this exercise first by
directly applying the Definition 3.5.1. Then, repeat this exercise by applying
the Theorem 3.5.3.
3.5.2 (Exercise 3.3.5 Continued) Let c be a positive constant such that X1
and X2 have the joint pdf given by
Prove whether or not X1 and X2 are independent. Solve this exercise first by
directly applying the Definition 3.5.1. Then, repeat this exercise by applying
the Theorem 3.5.3.
3.5.3 (Exercise 3.3.12 Continued) Let c be a positive constant such that
X1, X2 and X3 have the joint pdf given by
Prove whether or not X1, X2 and X3 are independent. Solve this exercise first
by directly applying the Definition 3.5.1. Then, repeat this exercise by apply-
ing the Theorem 3.5.3.
3.5.4 (Example 3.5.4 Continued) Verify all the steps in the Example 3.5.4.
3.5.5 (Example 3.5.4 Continued) In the Example 3.5.4, suppose that we
fix f1(x1) = 2x1,
        and ,
      0 < x1, x2, x3 < 1.
Then, form the function g(x1, x2, x3) as in (3.5.2).
(i)
Directly by integration, check, that g(x1, x2, x3) is a pdf;
(ii) Directly by integration, find the expressions of all pairwise marginal
pdf’s gi,j(xi, xj) and single marginal pdf’s gi(xi) for i ≠ j = 1, 2, 3;
(iii) Show directly that the X’s are pairwise independent, but X1, X2, X3
are not independent.

3. Multivariate Random Variables
167
3.5.6 (Example 3.5.4 Continued) Construct some other specific functions
fi(xi), i = 1, 2, 3 satisfying the conditions laid out in the Example 3.5.4 and
observe what happens in some of those special situations.
3.5.7 Let us start with the non-negative integrable functions fi(xi) for 0 < xi
< 1 which are not identically equal to unity such that we can claim:
fi(xi)dxi = 1, i = 1, 2, 3, 4. With x = (x1, x2, x3, x4), let us then define
for 0 < xi < 1, i = 1, 2, 3, 4.
(i)
Directly by integration, check that g(x1, x2, x3, x4) is a pdf;
(ii) Directly by integration, find the expression of the marginal pdf
g1, 2,3(x1, x2, x3);
(iii) Show directly that X1, X2, X3 are independent;
(iv) Consider the random variables X1, X2, X3, X4 whose joint pdf is
given by g(x1, x2, x3, x4). Show that X1, X2, X3, X4 are not indepen
dent.
3.5.8 (Exercise 3.5.7 Continued) Along the lines of the Exercise 3.5.6,
construct some specific functions fi(xi), i = 1, 2, 3, 4 satisfying the conditions
laid out in the Exercise 3.5.7 and observe what happens in some of those
special situations.
3.5.9 Prove the Theorem 3.5.3.
3.5.10 Suppose that X1 and X2 have the joint pmf
Prove whether or not X1 and X2 are independent.
3.5.11 Suppose that X1 and X2 have the joint pmf
Prove that X1 and X2 are dependent.
3.5.12 Suppose that X1 and X2 have the joint pmf

168
3. Multivariate Random Variables
Prove whether or not X1 and X2 are independent.
3.5.13 Suppose that X1 and X2 have the joint pmf such that P{X1 = 2 ∩ X2
= 3} = 1/3, P{X1 = 2 ∩ X2 = –1} = a, P{X1 = –1 ∩ X2 = 3} = b and P{X1 =
–1 ∩ X2 = –1} = 1/6 where a and b are appropriate numbers. Determine a and
b when X1 and X2 are independent.
3.5.14 Suppose that X1 and X2 have the joint pdf
where k is a positive constant.
(i)
Show that k = 2;
(ii) Show that f1(x1) = 2{e–x
1 – e–2x
1} I(0 < x1 < ∞) and f2(x2) = 2e–2x2
I(0 < x2 < ∞). Find P(X2 ≥ 3);
(iii) Show that f1/2(x1) = e–(x
1
 – x
2
) I(x2 < x1 < ∞) and f2/1(x2) = e–x
2
(1 –e–x
1)–1 I(0 < x2 < x1);
(iv)  Prove whether or not X1 and X2 are independent.
3.5.15 Suppose that X1 and X2 have the joint pdf
(i)
Show that f1(x1) = ¼(3 – x1) I(0 < x1 < 2) and f2(x2) = ¼(5 – x2) I(2
<x2 < 4);
(ii) Show that f1/2(x1) =  
 I(0 < x1 < 2, 2 < x2 < 4) and
f2/1(x2) = 
 I(0 < x1 < 2, 2 < x2 < 4);
(iii)  Prove whether or not X1 and X2 are independent.
3.5.16 Suppose that X1 and X2 have the joint pdf
The surface represented by this pdf is given in the Figure 3.10.4.

3. Multivariate Random Variables
169
(i)
Show that f1(x1) = ¼π{cos(½πx1) + sin(½πx1)} for 0 < x1 < 1,
and f2(x2) = ¼π{cos(½πx2) + sin(½πx2)} for 0 < x2 < 1;
(ii) Prove whether or not X1 and X2 are independent.
Figure 3.10.4. Plot of the PDF from the Exercise 3.5.16
3.5.17 Suppose that the random vector X = (X1, X2, X3, X4) has its joint
pdf given by
(i)
Find the marginal pdf’s fi(xi), i = 1, 2, 3, 4. Show that each Xi is
distributed as N(0, 4), i = 1, 2, 3, but X4 is distributed as N(0, 1);
(ii) By integrating f(x1, x2, x3, x4) with respect to x4 alone, obtain the
joint pdf of (X1, X2, X3);
(iii) Combine the parts (i) and (ii) to verify that X1, X2, X3 form a set of
independent random variables;
(iv) Show that X1, X2, X3, X4 do not form a set of independent random
variables.
3.5.18 (Exercise 3.5.17 Continued) Consider the random vector X = (X1,
X2, X3, X4) with its joint pdf given in the Exercise 3.5.17. Using this random
vector X, find few other four-dimensional random vectors Y = (Y1, Y2, Y3, Y4)
where Y1, Y2, Y3 are independent, but Y1, Y2, Y3, Y4 are not.
3.5.19 Suppose that X has the following pdf with –∞ < µ < ∞, 0 < σ < ∞:

170
3. Multivariate Random Variables
One will recall from (1.7.27) that this pdf is known as the lognormal density
and the corresponding X is called a lognormal random variable. Suppose that
X1, X2 are iid having the common pdf f(x). Let r and s be arbitrary, but fixed
real numbers. Then, obtain the expression for
3.6.1 Derive the marginal pdf of X1 in the Theorem 3.6.1, part (i).
3.6.2 Prove Theorem 3.6.1, part (iii).
3.6.3 (Example 3.6.2 Continued) Suppose that (X1, X2) is distributed as
N2(0, 0, 1, 1, ρ) where one has ρ ∈ (–1, 1). Find the expression of the mgf of
the random variable X1 X2, that is E{exp[tX1 X2]} for t belonging to an appro-
priate subinterval of ℜ.
3.6.4 Suppose that the joint pdf of (X1, X2) is given by
for –∞ < x1, x2 < ∞. Evaluate E[Xi], V[Xi] for i = 1, 2, and ρX1, X2.
3.6.5 Suppose that the joint pdf of (X1, X2) is given by
for –∞ < x1, x2 < ∞ where k is a positive number. Evaluate E[Xi], V[Xi]
for i = 1, 2, and ρX1, X2.
3.6.6 Suppose that (X1, X2) is distributed as N2(3, 1, 16, 25, 3/5). Evaluate
P{3 < X2 < 8 | X1 = 7} and P{–3 < X1 < 3 | X2 = –4}.
3.6.7 Suppose that X1 is distributed as N(µ, σ2) and conditionally the distri-
bution of X2 given that X1 = x1 is N(x1, σ2). Then, show that the joint distribu-
tion of (X1, X2) is given by N2(µ, µ, σ2, 2σ2,
).
3.6.8 Suppose that the joint pdf of (X1, X2) is given by
for –∞ < x1, x2 < ∞.
(i)
By direct integration, verify that f(x1, x2) is a genuine pdf;
(ii) Show that the marginal distributions of X1, X2 are both univariate
normal;
(iii) Does this f(x1, x2) match with the density given by (3.6.1)?
3.6.9 Suppose that the joint pdf of (X1, X2, X3) is given by

3. Multivariate Random Variables
171
for –∞ < x1, x2, x3 < ∞. Show that X1, X2, X3 are dependent random variables.
Also show that each pair (X1, X2), (X2, X3) and (X1, X3) is distributed as a
bivariate normal random vector.
3.7.1 This exercise provides a discrete version of the Example 3.7.1. Sup-
pose that a random variable X1 has the following probability distribution.
X1 values:
–5
–2
0
2
5
Probabilities:
.25
.2
.1
.2
 .25
Define
  Then, show that
(i)
E[X1] =           ] = 0;
(ii)
Cov(X1, X2) = 0, that is the random variables X1 and X2 are
uncorrelated;
(iii) X1 and X2 are dependent random variables.
3.7.2 (Exercise 3.7.1 Continued) From the Exercise 3.7.1, it becomes
clear that a statement such as “the zero correlation need not imply indepen-
dence” holds not merely for the specific situations handled in the Examples
3.7.1-3.7.2. Find a few other examples of the type considered in the Exercise
3.7.1.
3.7.3 In view of the Exercises 3.7.1-3.7.2, consider the following gener-
alization. Suppose that X1 has an arbitrary discrete distribution, symmetric
about zero and having its third moment finite. Define .                Then, show that
(i)
E[X1] = E[     ] = 0;
(ii) Cov(X1, X2) = 0, that is the random variables X1 and X2 are
uncorrelated;
(iii) X1 and X2 are dependent random variables.
3.7.4 (Example 3.7.1 Continued) In the Example 3.7.1, we used nice tricks
with the standard normal random variable. Is it possible to think of another
example by manipulating some other continuous random variable to begin
with? What if one starts with X1 that has a continuous symmetric distribution
about zero? For example, suppose that X1 is distributed uniformly on the
interval (–1, 1) and let
Are X1 and X2 uncorrelated, but dependent?
Modify this situation to find other examples, perhaps with yet some other
continuous random variable X1 defined on the whole real line ℜ and
3.7.5 In the types of examples found in the Exercises 3.7.1-3.7.4, what if
someone had defined
                 instead? Would X1 and X2 thus
defined, for example, be uncorrelated, but dependent in the earlier exercises?

172
3. Multivariate Random Variables
3.7.6 (Example 3.7.3 Continued) The zero correlation between two ran-
dom variables sometimes does indicate independence without bivariate nor-
mality. The Example 3.7.3 had dealt with a situation like this. In that example,
will the same conclusion hold if both X1 and X2 were allowed to take two
arbitrary values other than 0 and 1? {Hint: First recall from the Theorem
3.4.2, part (i) that the correlation coefficient between X1 and X2 would be the
same as that between Y1 and Y2 where we let Yi = (Xi – aibi, with ai ∈ ℜ, bi ∈
ℜ+ being any fixed numbers. Then, use this result to reduce the given problem
to a situation similar to the one in the Example 3.7.3.}
3.7.7 Suppose that X1 and X2 have the joint pdf
where k is some positive constant. Show that X1 and X2 are uncorrelated, but
these are dependent random variables.
3.7.8 Suppose that X1 and X2 have the joint pdf
for –∞ < x1, x2 < ∞. Show that X1 and X2 are uncorrelated, but these are
dependent random variables.
3.7.9 (Example 3.7.4 Continued) Suppose that (U1, U2) is distributed as
N2(5, 15, 8, 8, ρ) for some ρ ∈ (–1, 1). Let X1 = U1 + U2 and X2 = U1 – U2.
Show that X1 and X2 are uncorrelated.
3.8.1 Verify the entries given in the Table 3.8.1.
3.8.2 Consider the Beta(α, β) distribution defined by (1.7.35). Does the
pdf belong to the appropriate (that is, one-or two-parameter) exponential family
when
(i)
α is known, but β is unknown?
(ii)
β is known, but α is unknown?
(iii) α and β are both unknown?
3.8.3 Consider the Beta(α, β) distribution defined by (1.7.35). Does the
pdf belong to the appropriate (that is, one- or two-parameter) exponential
family when
(i)
α = β = θ, but θ(> 0) is unknown?
(ii)
α = θ, β = 2θ, but θ(> 0) is unknown?
3.8.4 Suppose that X has the uniform distribution on the interval (–θ, θ)
with θ(> 0) unknown. Show that the corresponding pdf does not belong to

3. Multivariate Random Variables
173
the one-parameter exponential family defined by (3.8.1). {Hint: Use ideas similar
to those from the Example 3.8.5.}
3.8.5 Consider two random variables X1, X2 whose joint pdf is given by
with θ(> 0) unknown.
(i)
Are X1, X2 independent?
(ii)
Does this pdf belong to the one-parameter exponential family?
3.8.6 Does the multinomial pmf with unknown parameters p1, ..., pk de-
fined in (3.2.8) belong to the multi-parameter exponential family? Is the num-
ber of parameters k or k – 1?
3.8.7 Express the bivariate normal pdf defined in (3.6.1) in the form of an
appropriate member of the one-parameter or multi-parameter exponential family
in the following situations when the pdf involves
(i)
all the parameters µ1, µ2, σ1, σ2, ρ;
(ii)
µ1 = µ2 = 0, and the parameters σ1, σ2, ρ;
(iii) σ1 = σ2 = 1, and the parameters µ1, µ2, ρ;
(iv) σ1 = σ2 = σ, and the parameters µ1, µ2, σ, ρ;
(v)
µ1 = µ2 = 0, σ1 = σ2 = 1, and the parameter ρ.
3.8.8 Consider the Laplace or the double exponential pdf defined as
for –∞ < x, θ < ∞ where θ is referred to as a parameter. Show that f(x; θ)
does not belong to the one-parameter exponential family.
3.8.9 Suppose that a random variable X has the Rayleigh distribution with
where θ(> 0) is referred to as a parameter. Show that f(x; θ) belongs to the
one-parameter exponential family.
3.8.10 Suppose that a random variable X has the Weibull distribution with
where α(> 0) is referred to as a parameter, while β(> 0) is assumed known.
Show that f(x; α) belongs to the one-parameter exponential family.
3.9.1 (Example 3.9.4 Continued) Prove the claim made in the Example
3.9.4 which stated the following: Suppose that X is a random variable
whose mgf MX(t) is finite for some t ∈ Τ ⊆ (–∞, 0). Then, it follows

174
3. Multivariate Random Variables
from the Theorem 3.9.2 that for any fixed real number a, one has P{X ≤
3.9.2 Suppose that X has the Gamma(4, 1/2) distribution.
(i)
Use Markov inequality to get an upper bound for P{X ≥ 12};
(ii) Use Bernstein-Chernoff inequality to get an upper bound for P{X ≥
12};
(iii) Use Tchebysheff’s inequality to obtain an upper bound for P{X ≥
12}.
3.9.3 (Exercise 3.6.6 Continued) Suppose that (X1, X2) is distributed as
N2(3, 1, 16, 25, 3/5).
(i)
Use Tchebysheff’s inequality to get a lower bound for P{–2 < X2 <
10 | X1 = 7};
(ii) Use Cauchy-Schwarz inequality to obtain an upper bound for (a)
E[|X1X2|] and (b)
{Hint: In part (i), work with the conditional distribution of X2 | X1 = 7 and
from X2, subtract the right conditional mean so that the Tchebysheff’s in-
equality can be applied. In part (ii), apply the Cauchy-Schwarz inequality in a
straightforward manner.}
3.9.4 Suppose that X is a positive integer valued random variable and
denote pk = P(X = k), k = 1, 2, ... . Assume that the sequence {pk; k ≥ 1}
is non-increasing. Show that pk ≥ 2k–2 E[X] for any k = 1, 2, ... . {Hint:
E[X] ≥
  Refer to (1.6.11).}
3.9.5 Suppose that X is a discrete random variable such that P(X = –a) =
P(X = a) = 1/8 and P(X = 0) = 3/4 where a(> 0) is some fixed number.
Evaluate µ, σ and P(|X| ≥ 2σ). Also, obtain Tchebysheff’s upper bound for
P(|X| ≥ 2σ). Give comments.
3.9.6 Verify the convexity or concavity property of the following func-
tions.
(i)
h(x) = x5 for –1 < x < 0;
(ii)
h(x) = |x|3 for x ∈ ℜ;
(iii) h(x) = x4e–2x for x ∈ ℜ+;
(iv) h(x) = e–1/2x2 for x ∈ ℜ;
(v)
h(x) = x–1 for x ∈ ℜ+.
3.9.7 Suppose that 0 < α, β < 1 are two arbitrary numbers. Show that
{Hint: Is log(x) with x > 0 convex or concave?}

3. Multivariate Random Variables
175
3.9.8 First show that f(x) = x–1 for x ∈ ℜ+ is a convex function. Next,
suppose that X is a real valued random variable such that P(X > 0) = 1 and
E[X] is finite. Show that
(i)
E[1/X] > 1/E[X];
(ii) Cov(X, 1/X) < 0.
3.9.9 Suppose that X and Y are two random variables with finite second
moments. Also, assume that P(X + Y = 0) < 1. Show that
{Hint: Observe that (X + Y)2 ≥ |X|{|X + Y|} + |Y|{|X + Y|}. Take expectations
throughout and then apply the Cauchy-Schwarz inequality on the rhs.}
3.9.10 Suppose that X1, ..., Xn are arbitrary random variables, each with
zero mean and unit standard deviation. For arbitrary ε(> 1), show that
{Hint: Let Ai be the event that
, i = 1, ..., n. By Tchebysheff’s
inequality, P(Ai) ≥ 1 – (nε2)–1, i = 1, ..., n. Then apply Bonferroni inequality,
namely,
≥ n{1 – (nε2)–1} – (n–1) = 1 – ε–2.}
3.9.11 Suppose that Y is a random variable for which E[Y] = 3 and E[Y2]
= 13. Show that P{–2 < Y < 8} > 21/25. {Hint: Check that V[Y] = 4 so that
P{–2 < Y < 8} = P{|Y – 3| < 5} > 1 – 4/25 = 21/25, by Tchebysheff’s
inequality.}

This page intentionally left blank

4
Functions of Random Variables
and Sampling Distribution
4.1 Introduction
It is often the case when we need to determine the distribution of a function
of one or more random variables. There is, however, no one unique ap-
proach to achieve this goal. In a particular situation, one or more approaches
given in this chapter may be applicable. Which method one would ultimately
adopt in a particular situation may largely depend on one’s taste. For com-
pleteness, we include standard approaches to handle various types of prob-
lems involving transformations and sampling distributions.
In Section 4.2 we start with a technique involving distribution functions.
This approach works well in the case of discrete as well as continuous
random variables. Section 4.2 also includes distributions of order statistics
in the continuous case. Next, Section 4.3 demonstrates the usefulness of a
moment generating function (mgf) in deriving distributions. Again, this ap-
proach is shown to work well in the case of discrete as well as continuous
random variables. Section 4.4 exclusively considers continuous distribu-
tions and transformations involving one, two, and several random variables.
One highlight in this section consists of the Helmert (Orthogonal) Transfor-
mation in the case of a normal distribution, and another consists of a trans-
formation involving the spacings between the successive order statistics in
the case of an exponential distribution. In both these situations, we deal
with one-to-one transformations from n random variables to another set of
n random variables, followed by discussions on Chi-square, Student’s t,
and F distributions. Section 4.5 includes sampling distributions for both
one-sample and two-sample problems. Section 4.6 briefly touches upon the
multivariate normal distributions and provides the distribution of the Pearson
correlation coefficient in the bivariate normal case. Section 4.7 shows, by
means of several examples, the importance of independence of various ran-
dom variables involved in deriving sampling distributions such as the Student’s
t and F distributions as well as in the reproductive properties of normal and
Chi-square random variables.
The following example shows the immediate usefulness of results which
evolve via transformations and sampling distributions. We first solve the
177

178
4. Functions of Random Variables and Sampling Distribution
problem using the direct calculation, and then provide an alternative quick
and painless way to solve the same problem by applying a result derived later
in this chapter.
Example 4.1.1 Let us consider a popular game of “hitting the bull’s eye” at
the point of intersection of the horizontal and vertical axes. From a fixed
distance, one aims a dart at the center and with the motion of the wrist, lets it
land on the game board, say at the point with the rectangular coordinates (Z1,
Z2). Naturally then, the distance (from the origin) of the point on the game
board where the dart lands is
  A smaller distance would indicate
better performance of the player. Suppose that Z1 and Z2 are two indepen-
dent random variables both distributed as N(0, 1). We wish to calculate
  where a > 0, that is the probability that the thrown dart
lands within a distance of 
 from the aimed target. The joint pdf of (Z1, Z2)
would obviously amount to
           )} with
–∞ < z1, z2 < ∞. We then go back to the heart of (3.3.20) as we proceed to
evaluate the double integral,
Let us substitute
which transform a point (z1, z2) on the plane in the rectangular coordinates
system to the point (r, θ) on the same plane, but in the polar coordinates
system. Now, the matrix of the first partial derivatives of z1 and z2 with re-
spect to θ and r would be
so that its determinant, det(A) = ½. Now, we can rewrite the double integral
from (4.1.1) as
But, later in this chapter (Example 4.3.6), we show that the distribution
of
    is indeed      , that is a Chi-square with two degrees of

4. Functions of Random Variables and Sampling Distribution
179
freedom. Thus the pdf of W is given by g(w) = 1/2exp{–1/2ω}I(ω > 0).
Refer to Section 1.7 as needed. The required probability, P(W ≤ a) is simply
then
which matches with the answer found earlier in (4.1.3). This second ap-
proach appears much simpler. !
In the Exercise 4.1.1, we have proposed a direct approach to evaluate
         ,with some fixed but arbitrary b > 0, where Z1, Z2 are iid stan-
dard normal. The Exercise 4.5.4, however, provides an easier way to evaluate
the same probability by considering the sampling distribution of the random
variable Z1/Z2 which just happens (Exercise 4.5.1 (iii)) to have the Cauchy
distribution defined in (1.7.31). The three Exercises 4.5.7-4.5.9 also fall in the
same category.
4.2 Using Distribution Functions
Suppose that we have independent random variables X1, ..., Xn and let us
denote Y = g (X1, ..., Xn), a real valued function of these random variables.
Our goal is to derive the distribution of Y.
If X1, ..., Xn or Y happen to be discrete random variables, we set out to
evaluate the expression of P(Y = y) for all appropriate y ∈ y, and then by
identifying the form of P(Y = y), we are often led to one of the standard
distributions.
In the continuous case, the present method consists of first obtaining the
distribution function of the new random variable Y, denoted by F(y) = P(Y ≤
y) for all appropriate values of y ∈ y. If F(y) is a differentiable function of Y,
then by differentiating F(y) with respect to y, one can obtain the pdf of Y at
the point y. In what follows, we discuss the discrete and continuous cases
separately.
4.2.1   Discrete Cases
Here we show how some discrete situations can be handled. The exact tech-
niques used may vary from one problem to another.
Example 4.2.1 Suppose that X1, X2 are two independent random variables
having respectively the following probability distributions:
X1 values:
 0
1
3
Probabilities:
.2
.3
 .5

180
4. Functions of Random Variables and Sampling Distribution
X2 values:
–1
0
2
 2.5
Probabilities:
.1
 .2
.4
 .3
Now, P(X1 = x1 ∩ X2 = x2) = P(X1= x1)P(X2 = x2) for all X1 = 0, 1, 3 and X2 =
–1, 0, 2, 2. 5. Suppose that we want to obtain the pmf for the random variable
Y = X1 + X2. The possible values Y of Y belong to the set γ = {–1, 0, 1, 2, 2.5,
3, 3.5, 5, 5.5}. Now P(Y = 0) = P(X1 = 0 ∩ X2 = 0) + P(X1 = 1 ∩ X2 = –1) =
1 ∩ X2 = –1) = (.2)(.2) + (.3)(.1) = .07. Also, P(Y = 2) = P(X1 = 0 ∩ X2 = 2)
+ P(X1 = 3 ∩ X2 = –1) = (.2)(.4) + (.5)(.1) = .13, and this way the pmf
function g(y) = P(Y = y) can be obtained for all y ∈ y. "
Example 4.2.2 Let X1 and X2 be independent random variables, where X1
is Binomial(n1, p) and X2 is Binomial(n2, p) with 0 < p < 1. Let us find the
distribution of Y = X1 + X2. With 0 ≤ Y ≤ n1 + n2, one can express P(Y = Y) as
In order to evaluate the summation in the last step in (4.2.1), one needs to
compare the coefficients of py(1 – p)n1+n2–y on both sides of the identity:
Hence,
can be replaced bY
   , a fact that follows
from the Binomial Theorem (see (1.4.12)). That is, we can simply rewrite
which is the pmf of the Binomial(n1 + n2, p) variable. Hence, the random
variable Y has the Binomial (n1 + n2 p) distribution. !
Example 4.2.3 Suppose that X1, ..., Xk are independent, Xi is distributed as
the Binomial(ni, p), 0 < p < 1, i = 1, ..., k. Obviously X1 + X2 + X3 = {X1 + X2}
+ X3, and of course X1 + X2 and X3 are independently distributed as
the binomials with the same p. Hence, using the Example 4.2.2, we
conclude that X1 + X2 + X3 is Binomial({n1 + n2} + n3, p), because we
are simply adding two independent Binomial random variables

4. Functions of Random Variables and Sampling Distribution
181
respectively with parameters (n1 + n2, p) and (n3, p). Next, one may use
mathematical induction to claim that
    is thus distributed as the Bino-
mial (
         ). !
4.2.2   Continuous Cases
The distribution function approach works well in the case of continuous ran-
dom variables. Suppose that X is a continuous real valued random variable
and let Y be a real valued function of X. The basic idea is first to express the
distribution function G(y) = P(Y ≤ y) of Y in the form of the probability of an
appropriate event defined through the original random variable X. Once the
expression of G(y) is found in a closed form, one would obviously obtain
dG(y)/dy, whenever G(y) is differentiable, as the pdf of the transformed ran-
dom variable Y in the appropriate domain space for y. This technique is ex-
plained with examples.
Example 4.2.4 Suppose that a random variable X has the pdf
Let Y = X2 and we first obtain G(y) = P(Y ≤ y) for all y in the real line.
Naturally, G(y) = 0 if y ≤ 1 and G(y) = 1 if y ≥ 4. But, for 1 < y < 4, we have
      . Hence,
which is the pdf of Y. !
In a continuous case, for the transformed variable Y = g(X),
first find the df G(y) = P(Y ≤ y) of Y in the appropriate
space for Y. Then, 
G(y), whenever G(y) is differentiable,
would be pdf of Y for the appropriate y values.
Example 4.2.5 Let X have an arbitrarY continuous distribution with its pdf
f(x) and the df F(x) on the interval (a, b) ⊆ ℜ. We first find the pdf of the
random variable F(X) and denote W = F(X). Let F–1(.) be the inverse function
of F(.), and then one has for 0 < w < 1:
and thus the pdf of W is given by

182
4. Functions of Random Variables and Sampling Distribution
That is, W has a uniform distribution on the interval (0, 1). Next, define
U = – log (F(X)) and V = –2log(F(X)). The domain space of U and V are both
(0, ∞). Now, for 0 < u < ∞, one has the df of U,
so that U has the pdf f(u) = 
G(u) = e–u, which corresponds to the pdf of a
standard exponential random variable which is defined as Gamma(1, 1). Re-
fer to (1.7.24). Similarly, for 0 < v < ∞, we write the df of V,
so that V has the pdf g(v) = 
H(v) = ½e–1/2v, which matches with the pdf of
a Chi-square random variable with two degrees of freedom. Again refer back
to Section 1.7 as needed. !
Example 4.2.6 Suppose that Z has a standard normal distribution and let Y
= Z2. Denote ∅(.) and Φ (.) respectively for the pdf and df of Z. Then, for 0
< y < ∞, we have the df of Y,
and hence the pdf of Y will be given by
Now g(y) matches with the pdf of a Chi-square random variable with one
degree of freedom. That is, Z2 is distributed as a Chi-square with one degree
of freedom. Again, refer back to Section 1.7. !
Suppose that X has a continuous random variable with its df
F(x). Then, F(X) is Uniform on the interval (0, 1), –log{F(X)}
is standard exponential, and –2log{F(X)} is
4.2.3   The Order Statistics
Let us next turn to the distributions of order-statistics. Consider independent
and identically distributed (iid) continuous random variables X1, ..., Xn having
the common pdf f(x) and the df F(x). Suppose that Xn:1 ≤ Xn:2 ≤ ... ≤ Xn:n stand
for the corresponding ordered random variables where Xn:i is referred to as
the ith order statistic. Let us denote Yi = Xn:i for i = 1, ..., n and Y = (y1, ..., yn).
The joint pdf of Y1, ..., Yn, denoted by fY(y1, ..., yn), is given by

4. Functions of Random Variables and Sampling Distribution
183
In (4.2.2), the multiplier n! arises because y1, ..., yn can be arranged among
themselves in n! ways and the pdf for any such single arrangement amounts
to
        . Often we are specifically interested in the smallest and largest
order statistics. For the largest order statistic Yn, one can find the distribution
as follows:
and hence the pdf of Yn would be given by
in the appropriate space for the Y values. In the same fashion, for the smallest
order statistic Y1, we can write:
and thus the pdf of Y1 would be given by
in the appropriate space for the Y values.
In the Exercise 4.2.5, we have indicated how one can find the
joint pdf of anY two order statistics Yi = Xn:i and Yj = Xn:j.
Using the Exercise 4.2.5, one can derive the joint pdf of Y1 and Yn. In order
to write down the joint pdf of (Y1, Yn) at a point (y1, yn) quickly, we adopt the
following heuristic approach. Since y1, yn are assumed fixed, each of the
remaining n – 2 order statistics can be anywhere between y1 and yn, while
these could be any n – 2 of the original n random X’s. Now, P{y1 < Xi < yn} =
F(yn) – F(y1), for each i = 1, ..., n. Hence, with 
the joint pdf of (Y1,
Yn) would be given by:

184
4. Functions of Random Variables and Sampling Distribution
In the same fashion, one can easily write down the joint pdf of any subset
of the order statistics. Now, let us look at some examples.
In the Exercises 4.2.7-4.2.8, we show how one can find the pdf
of the range, Xn:n – Xn:1 when one has the random samples
X1, ..., Xn from the Uniform distribution on the interval (0, θ) with
θ ∈ ℜ+ or on the interval (θ, θ + 1) with θ ∈ ℜ respectivelY.
Example 4.2.7 Suppose that X1, ..., Xn are iid random variables distributed
as Uniform on the interval (0, θ) with θ > 0. Consider the largest and smallest
order statistics Yn and Y1 respectively. Note that
and in view of (4.2.4) and (4.2.6), the marginal pdf of Yn and Y1 will be
respectively given by g(y) = nyn–1θ–n and h(y) = n(θ – y)n–1θ–n for 0 < y < θ. In
view of (4.2.7), the joint pdf of (Y1, Yn) would be given by f(y1, yn) = n(n –
1)(yn – y1)n–2θ–n for 0 < y1 < yn < θ. !
The next example points out the modifications needed in (4.2.4)
and (4.2.6)-(4.2.7) in order to find the distributions of various
order statistics from a set of independent, but not identically
distributed continuous random variables.
Example 4.2.8 Consider independent random variables X1 and X2 where
their respective pdf’s are given by f1(x) = 1/3x2I(–1 < x < 2) and
f2(x) = 5/33x4I(–1 < x < 2). Recall that here and elsewhere I(.) stands
for the indicator function of (.). One has the distribution functions
      for
–1 < x < 2. Hence, for –1 < y < 2, the distribution function F(y) of Y2 = X2:2,
the larger order statistic, can be found as follows:
since the X’s are independent. By differentiating F(y) with respect to y, one
can immediately find the pdf of X2:2. Similarly one can handle the distribution
of X2:1, the smaller order statistic. The same idea easily extends for n indepen-
dent, but not identically distributed continuous random variables. !

4. Functions of Random Variables and Sampling Distribution
185
4.2.4   The Convolution
We start with what is also called the convolution theorem. This result will
sometimes help to derive the distribution of sums of independent random
variables. In this statement, we assume that the support of (X1, X2) is ℜ2.
When the support is a proper subset of ℜ2, the basic result would still hold
except that the range of the integral on the rhs of (4.2.9) should be appropri-
ately adjusted on a case by case basis.
Theorem 4.2.1 (Convolution Theorem) Suppose that X1 and X2 are in-
dependent continuous random variables with the respective pdf’s f1(x1) and
f2(x2), for (x1, x2) ∈ ℜ2. Let us denote U = X1 + X2. Then, the pdf of U is given
by
for u ∈ ℜ.
Proof First let us obtain the df of the random variable U. With u ∈ ℜ, we
write
Thus, by differentiating the df G(u) from (4.2.10) with respect to u, we get
the pdf of U and write
which proves the result. ¢
If X1 and X2 are independent continuous random variables,
then the pdf of U given by (4.2.9) can be equivalently
        written as 
 instead.

186
4. Functions of Random Variables and Sampling Distribution
Example 4.2.9 Let X1 and X2 be independent exponential random
variables, with their respective pdf’s given by f1(x1) = e–x1I(x1 ∈ ℜ+)
and f2(x2) = e–x2I(x2 ∈ ℜ+). What is the pdf of U = X1 + X2? It is obvious
that the pdf g(u) will be zero when u ≤ 0, but it will be positive when
u > 0. Using the convolution theorem, for u > 0, we can write
which coincides with the gamma pdf in (1.7.20) with α = 2 and β = 1. In
other words, the random variable U is distributed as Gamma(2, 1). See the
related Exercise 4.2.19. !
A result analogous to the Theorem 4.2.1 for the product of two
continuous independent random variables X1 and X2 has been
set aside as the Exercise 4.2.13.
Example 4.2.10 Suppose that X1 and X2 are independent standard normal
random variables with the respective pdf’s f1(x1) = ∅ (x1) and f2(x2) = ∅(x2)
for (x1, x2) ∈ ℜ2. What is the pdf of U = X1 + X2? It is obvious that the pdf g(u)
will be positive for all u ∈ ℜ. Using the convolution theorem, we can write
                                    Now, we can simplify the expression of
g(u) as follows:
where we let
   But, note that for fixed u, the function
h(x) is the pdf of a N(1/2u, 1/2) variable. Thus,
        must be one.
Hence, from (4.2.12), we can claim that
  for u ∈ ℜ. But,
g(u) matches with the pdf of a N(0, 2) variable and hence U is distributed as
N(0, 2). See the related Exercise 4.2.11. !
In the next example, X1 and X2 are not independent but the
convolution or the distribution function approach still works.
Example 4.2.11 Suppose that X1 and X2 have their joint pdf given by
What is the pdf of U = X1 – X2? We first obtain the df G(u) of U.

4. Functions of Random Variables and Sampling Distribution
187
Obviously, G(u) = 0 for u ≤ 0 or u ≥ 1. With 0 < u < 1, we write
Thus, the pdf of U is given by
We leave out some of the intermediate steps as the Exercise 4.2.12. !
The convolution approach leads to the distribution of U = X1 + X2
where X1, X2 are iid Uniform(0, 1). The random variable U is often
said to have the triangular distribution. See the Exercise 4.2.9.
4.2.5   The Sampling Distribution
Suppose that we consider a large population of all adult men in a city and we
wish to gather information regarding the distribution of heights of these indi-
viduals. Let Xj stand for the height of the jth individual selected randomly from
the whole population, j = 1, ..., n. Here, n stands for the sample size. Since the
population is large, we may assume from a practical point of view, that these
n individuals are selected independently of each other. The simple random
sampling with replacement (see the Example 1.7.6) would certainly gener-
ate independence. What is the sampling distribution of, say, the sample mean,
           ? How can we proceed to understand what this theoretical
distribution of    may possibly mean to us in real life? A simple minded ap-
proach may consist of randomly selecting n individuals from the relevant
population and record each individual’s height. The data would look like n
numbers X1,1, ..., X1,n which will give rise to an observed value     , namely
                          From the same population, if we contemplate selecting an-
other batch of n individuals, drawn independently from the first set, and record
their heights, we will end up with another data of n numbers x2,1, ..., x2,n leading
to another observed value of the sample mean    , namely
.
Though we do not physically have to go through this process of independent resampling
with the same sample of size n, we may think of such a hypothetical process of the
identical replication, infinitely many times. Through these replications, we will finally

188
4. Functions of Random Variables and Sampling Distribution
arrive at the specific observed values      i = 1, 2, ... for the sample mean
    based on a sample of size n. In real life, however, it will be impossible to
replicate the process infinitely many times, but suppose instead that we repli-
cate one hundred times thereby coming up with the observed values      i = 1,
2, ..., 100. One can easily draw a relative frequency histogram for this data
consisting of the observed values     i = 1, 2, ..., 100. The shape of this
histogram will give us important clues regarding the nature of the theoretical
distribution of the sample mean,     . If we generated more than one hundred
    values, then the observed relative frequency histogram and the true pmf or
pdf of      would have more similarities. With this perception of the indepen-
dent resampling again and again, the adjective “sampling” is attached when
we talk about the “distribution” of    . In statistical applications, the sample
mean frequently arises and its theoretical distribution, as told by its pmf or the
pdf, is customarily referred to as the sampling distribution of     . In the same
vein, one can think about the sampling distributions of many other character-
istics, for example, the sample median, sample standard deviation or the sample
maximum in a random sample of size n from an appropriate population under
consideration. In practice, for example, it is not uncommon to hear about the
sampling distribution of the median income in a population or the sampling
distribution of the record rainfall data in a particular state over a period. We
would use both phrases, the sampling distribution and distribution, quite inter-
changeably.
Example 4.2.12 (Example 4.2.4 Continued) Let X be a random variable
with its pdf
Now, suppose that we wish to select 1000 random samples from this popula-
tion. How can we accomplish this? Observe that the df of X is
and we know that F(X) must be distributed as the Uniform(0, 1) random
variable. Using the MINITAB Release 12.1 and its uniform distribution gen-
erator, we first obtain 1000 observed values u1, u2, ..., u1000 from the Uni-
form(0, 1) distribution. Then, we let

4. Functions of Random Variables and Sampling Distribution
189
Figure 4.2.1. The Histogram of 1000 Sample Values Drawn
According to the PDF f(x) from (4.2.13)
These xi’s form a random sample of size 1000 from a population with its pdf
f(x) given by (4.2.13). Next, using the MINITAB Release 12.1, a histogram
was constructed from these 1000 sample values x1, x2, ..., x1000 which is
presented in the Figure 4.2.1. This histogram provides a sample-based picture
approximating the true pdf given by (4.2.13).
Figure 4.2.2. The Plot of the PDF f(x) from (4.2.13)
In this situation, we know the exact form of the pdf f(x) which is plotted
between –0.5 and 2.5 in the Figure 4.2.2 with the help of MINITAB, Release
12.1. Visual examinations reveal that there are remarkable similarities between
the histogram from the Figure 4.2.1 and the plot of f(x) from the Figure
4.2.2. In practice, a sample-based histogram is expected to be a good ap-
proximation of the population distribution, particularly if the sample size is
not too small. !

190
4. Functions of Random Variables and Sampling Distribution
4.3   Using the Moment Generating Function
The moment generating function (mgf) of a random variable was introduced
in the Section 2.3. In the Section 2.4 we had emphasized that a finite mgf
indeed pinpoints a unique distribution (Theorem 2.4.1). The implication we
wish to reinforce is that a finite mgf corresponds to the probability distribu-
tion of a uniquely determined random variable. For this reason alone, the finite
mgf’s can play important roles in deriving the distribution of functions of
random variables.
One may ponder over what kinds of functions of random variables we should
be looking at in the first place. We may point out that in statistics one fre-
quently faces the problem of finding the distribution of a very special type of
function, namely a linear function of independent and identically distributed
(iid) random variables. The sample mean and its distribution, for example, fall
in this category. In this vein, the following result captures the basic idea. It
also provides an important tool for future use.
Theorem 4.3.1 Consider a real valued random variable Xi having its mgf
MXi (t) = E(etX
i) for i = 1, ..., n. Suppose that X1, ..., Xn are independent. Then,
the mgf of
is given by
   where a1, ..., an are any
arbitrary but otherwise fixed real numbers.
Proof The proof merely uses the Theorem 3.5.1 which allows us to split
the expected value of a product of n independent random variables as the
product of the n individual expected values. We write
which completes the proof. ¢
First, find the mgf MU(t). Visually match this mgf with that of one
of the standard distributions. Then, U has that same distribution.
A simple looking result such as the Theorem 4.3.1 has deep implications.
First, the mgf MU(t) of U is determined by invoking Theorem 4.3.1. In view
of the Theorem 2.4.1, since the mgf of U and the distribution of U corre-
spond uniquely to each other, all we have to do then is to match the form of
this mgf MU(t) with that of one of the standard distributions. This way we
will identify the distribution of U. There are many situations where this simple
approach works just fine.

4. Functions of Random Variables and Sampling Distribution
191
Example 4.3.1 (Example 4.2.3 Continued) Let X1, ..., Xk be independent
and let Xi be Binomial(ni, p), i = 1, ..., k. Write
                  Then invoking Theorem 4.3.1 and using the expression
of the mgf of the Binomial (ni, p) random variable from (2.3.5),
one has
   The
final expression of MU(t) matches with the expression for the mgf of the
Binomial (n, p) random variable found in (2.3.5). Thus, we can claim that the
sampling distribution of U is Binomial with parameters n and p since the
correspondence between a distribution and its finite mgf is unique. !
Example 4.3.2 Suppose that we roll a fair die n times and let Xi denote the
score, that is the number on the face of the die which lands upward on the ith
toss, i = 1, ..., n. Let
 the total score from the n tosses. What is
the probability that U = 15? The technique of full enumeration will be very
tedious. On the other hand, let us start with the mgf of any of the Xi’s which
is given by MXi(t) = 1/6et (1 + et + ... + e5t) and hence MU(t) = 1/6nent (1 + et +
... + e5t)n. In the full expansion of MU(t), the coefficient of ekt must coincide
with P(X = k) for k = n, n + 1, ..., 6n–1, 6n. This should be clear from the
way the mgf of any discrete distribution is constructed. Hence, if n = 4, we
have P(X = 15) = 1/64{coefficient of e11t in the expansion of (1 + et + ... +
e5t)10} = 140/64} = But, if n = 10, we have P(X = 15) = 1/610{coefficient of e5t
in the expansion of (1 + et + ... + et)4} = 2002/610. One can find the pmf of U
as an exercise. !
Example 4.3.3 Let X1, ..., Xn be independent random variables, Xi dis-
tributed as
i = 1, ..., n. Write 
 and
                      Then invoking Theorem 4.3.1 and using the expression for
the mgf of the             random variable from (2.3.16), one has
                          The final ex-
pression of MU(t) matches with the expression for the mgf of the N(µ, σ2)
random variable found in (2.3.16). Thus, we can claim that the sampling
distribution of U is normal with parameters µ and σ2 since the correspon-
dence between a distribution and its finite mgf is unique. !
Example 4.3.4 Let X1, ..., Xn be independent random variables, Xi having a
Gamma distribution with the pdf fi(x) = cie–x/βxαi–1 with
for 0 < x, αi, β < ∞, i = 1, ..., n. Let us write U =
                    Then, for 0 <
t < β–1, as before one has MU(t) =
Here we used the expression for the mgf of the Gamma(αi, β) random variable
from (2.3.23). Thus, we can claim that the sampling distribution of U is Gamma(α,
β) since the correspondence between a distribution and its finite mgf is unique. !
Example 4.3.5 Let X1, ..., Xn be independent random variables, Xi be

192
4. Functions of Random Variables and Sampling Distribution
distributed as N(µi,    ), i = 1, ..., n. With fixed but otherwise arbitrary real
numbers a1, ..., an, we write
    and then denoting
and
     we claim along the lines of the Example 4.3.3 that the sam-
pling distribution of U turns out to be N(µ, σ2). It is left as the Exercise 4.3.3.!
For the record, we now state the following results. Each part has already
been verified in one form or another. It will, however, be instructive to supply
direct proofs using the mgf technique under these special situations. These
are left as the Exercise 4.3.8.
Theorem 4.3.2 (Reproductive Property of Independent Normal,
Gamma and Chi-square Distributions) Let X1, ..., Xn be independent ran-
dom variables. Write
          and     = n–1U. Then, one can conclude the
following:
(i)
If Xi’s have the common N(µ, σ2) distribution, then U is distributed
as N(nµ, nσ2) and hence      is distributed as N(µ, 1/nσ2);
(ii)
If Xi’s have the common Gamma (α, β) distribution, then U is dis
tributed as Gamma(nα, β);
(iii) If Xi has a Gamma(½νi, 2) distribution, that is a Chi-square distri
bution with νi degrees of freedom for i = 1, ..., n, then U is distrib
uted as Gamma(½ν, 2) with
     which is a Chi-square
distribution with ν degrees of freedom.
Example 4.3.6 (Example 4.1.1 Continued) Now, let us briefly go back to
the Example 4.1.1. There, we had Z1, Z2 iid N(0, 1) and in view of the Ex-
ample 4.2.6 we can claim that
     are iid      . Thus, by using the repro-
ductive property of the independent Chi-squares, we note that the random
variable 
 is distributed as the 
  random variable. The pdf of
W happens to be g(w) = 1/2e–w/2I(w > 0). Hence, in retrospect, (4.1.4) made
good sense. !
4.4    A General Approach with Transformations
This is a more elaborate methodology which can help us to derive distribu-
tions of functions of random variables. We state the following result without
giving its proof.
Theorem 4.4.1 Consider a real valued random variable X whose pdf is
f(x) at the point x belonging to some subinterval χ of the real line ℜ. Sup-
pose that we have a one-to-one function g: χ → ℜ and let g(x) be differ-
entiable with respect to x(∈ χ). Define the transformed random variable

4. Functions of Random Variables and Sampling Distribution
193
Y = g(X). Suppose that the domain space  of Y is also a subinterval of ℜ.
Then, the pdf of Y is given by
for y ∈ γ.
That is, in the expression of f(x), the x variable is first replaced by the new
variable g–1(y) everywhere and then we multiply the resulting expression by the abso-
lute value of the Jacobian of transformation which is
          Incidentally, note
that
         is evaluated as the absolute value of dx/dy and then replacing
the variable x ∈ χ in terms of the new variable Y ∈ .
Figure 4.4.1. A Mapping of x to y
Recall the convention that when a pdf is written down with its
support, it is understood that the densitY is zero elsewhere.
Example 4.4.1 (Example 4.2.5 Continued) We write W = F(X) where we
have g(x) = F(x) for x ∈ (a, b), so that x = F–1(w) for w for w ∈ (0, 1). This
transformation from x to w is one-to-one. Now, dw/dx = f(x) = f(F–1(w)).
Then, using (4.4.1) the pdf of W becomes
In other words, W = F(X) is distributed uniformly on the interval (0, 1). Next,
we have U = q(W) where q(w) = –log(w) for 0 < w < 1. This transformation
from w to u is also one-to-one. Using (4.4.1) again, the pdf of U is obtained as
follows:
which shows that U is distributed as a standard exponential variable or
Gamma(1, 1). !

194
4. Functions of Random Variables and Sampling Distribution
Example 4.4.2 Let U be a Uniform(0, 1) variable, and define V = –log(U),
W = –2log(U). From the steps given in the Example 4.4.1, it immediately
follows that V has the standard exponential distribution. Using (4.4.1) again,
one should verify that the pdf of W is given by g(w) = 1/2e–w/2I(w > 0) which
coincides with the pdf of a Chi-square random variable with 2 degrees of
freedom. !
In the case when g(.) considered in the statement of the Theorem 4.4.1 is
not one-to-one, the result given in the equation (4.4.1) needs some minor
adjustments. Let us briefly explain the necessary modifications. We begin by
partitioning the χ space into A1, ..., Ak in such a way that the individual trans-
formation g : Ai →  becomes one-to-one for each i = 1, ..., k. That is, when
we restrict the mapping g on Ai → , given a specific value y ∈ , we can
find a unique corresponding x in Ai which is mapped into y, and suppose that
we denote, 
i = 1, ..., k. Then one essentially applies the Theorem
4.4.1 on each set in the partition where gi, which is the same as g restricted on
Ai → , is one-to-one. The pdf h(y) of Y is then obtained as follows:
Example 4.4.3 Suppose that Z is a standard normal variable and Y = Z2.
This transformation from z to y is not one-to one, but the two pieces of
transformations from z ∈ [0, ∞) to y ∈ ℜ+ and from z ∈ (–∞, 0) to y ∈ ℜ+ are
individually one-to-one. Also recall that the pdf of Z, namely, ∅(z) is symmet-
ric about z = 0. Hence, for 0 < y < ∞, we use (4.4.2) to write down the pdf
h(y) of Y as follows:
which coincides with the pdf of a Chi-square variable with one degree of
freedom. Recall the earlier Example 4.2.6 in this context where the same pdf
was derived using a different technique. See also the Exercise 4.4.3-4.4.4.!
Example 4.4.4 Suppose that X has the Laplace or double exponential dis-
tribution with its pdf given bY f(x) = 1/2e–|x| for x ∈ ℜ. Let the transformed
variable be Y = |X|. But, g(x) = |x| : ℜ → ℜ+ is not one-to-one. Hence, for 0 <
y < ∞, we use (4.4.2) to write down the pdf h(y) of Y as follows:
which coincides with the pdf of a standard exponential variable. See also the
Exercise 4.4.5. !

4. Functions of Random Variables and Sampling Distribution
195
4.4.1   Several Variable Situations
In this subsection, we address the case of several variables. We start with the
machinery available for one-to-one transformations in a general case. Later,
we include an example when the transformation is not one-to-one. First, we
clarify the techniques in the two-dimensional case. One may review the Sec-
tion 4.8 for some of the details about matrices.
Suppose that we start with real valued random variables X1, ..., Xn and we
have the transformed real valued random variables Yi = gi(X1, ..., Xn), i = 1, ...,
n where the transformation from (x1, ..., xn) ∈ χ to (y1, ..., yn), ∈  is assumed
to be one-to one. For example, with n = 3, we may have on hand three
transformed random variables Y1 = X1 + X2, Y2 = X1 – X2, and Y3 = X1 + X2 +
X3.
We start with the joint pdf f(x1, ..., xn) of X1, ..., Xn and first replace all the
variables x1, ..., xn appearing within the expression of f(x1, ..., xn) in terms of
the transformed variables
 i = 1, ..., n. In other words, since
the transformation (x1, ..., xn) to (y1, ..., yn) is one-to one, we can theoreticallY
think of uniquely expressing xi in terms of (y1, ..., yn) and write xi = bi(y1, ...,
yn), i = 1, ..., n. Thus, f(x1, ..., xn) will be replaced by f(b1, ..., bn). Then, we
multiply f(b1, ..., bn) by the absolute value of the determinant of the Jacobian
matrix of transformation, written exclusively involving the transformed vari-
ables y1, ..., yn only. Let us define the matrix
The understanding is that each xi variable is replaced by bi(y1, ..., yn), i = 1, ...,
n, while forming the matrix J. Let det(J) stand for the determinant of the
matrix J and | det(J) | stand for the absolute value of det(J). Then the joint pdf
of the transformed random variables Y1, ..., Yn is given by
for y ∈ γ.
On the surface, this approach may appear complicated but actually it is not
quite so. The steps involved are explained by means of couple of examples.
Example 4.4.5 Let X1, X2 be independent random variables and Xi be
distributed as Gamma(αi, β), αi > 0, β > 0, i = 1, 2. Define the trans-
formed variables Y1 = X1 + X2, Y2 = X1/(X1 + X2). Then one can uniquely

196
4. Functions of Random Variables and Sampling Distribution
express x1 = y1y2 and x2 = y1 (1 – y2). It is easy to verify that
so that det(J) = –y1y2 – y1(1 – y2) = –y1. Now, writing the constant c instead of
the expression {βα
1
+α
2Γ(α1)Γ(α2)}–1, the joint pdf of X1 and X2 can be written
as
for 0 < x1, x2 < ∞. Hence using (4.4.4) we can rewrite the joint pdf of Y1 and
Y2 as
for 0 < y1 < ∞, 0 < y2 < 1. The terms involving y1 and y2 in (4.4.5) factorize
and also either variable’s domain does not involve the other variable. Refer
back to the Theorem 3.5.3 as needed. It follows that Y1 and Y2 are indepen-
dent random variables, Y1 is distributed as Gamma(α1 + α2,β) and Y2 is dis-
tributed as Beta(α1, α2) since we can rewrite c as {βα
1
+α
2Γ(α1 + α2)}–1 {b(α1,
α2)}–1 where b(α1, α2) stands for the beta function, that is
One may refer to (1.6.19) and (1.6.25)-(1.6.26) to review the gamma and
beta functions. !
Example 4.4.6 (Example 4.4.5 Continued) Suppose that X1, ..., Xn are
independent random variables where Xi is Gamma(αi, β), i = 1, ..., n. In view
of the Example 4.4.5, we can conclude that X1 + X2 + X3 = (X1+X2)+X3 is then
Gamma(α1+α2+α3, β) whereas X1/(X1+X2+X3) is Beta(α1, α2+α3) and these are
independent random variables. Thus, by the mathematical induction one can claim
that
      is Gamma
     is Beta
whereas these are also distributed independently. !
In the next two examples one has X1 and X2 dependent.
Also, the transformed variables Y1 and Y2 are dependent.
Example 4.4.7 Suppose that X1 and X2 have their joint pdf given by

4. Functions of Random Variables and Sampling Distribution
197
Let Y1 = X1 and Y2 = X1 + X2. We first wish to obtain the joint pdf of Y1 and Y2.
Then, the goal is to derive the marginal pdf’s of Y1, Y2.
The one-to-one transformation (x1, x2) → (y1, y2) leads to the inverse: x1 =
y1, x2 = y2 – y1 so that |det(J)| = 1. Now, x2 > 0 implies that 0 < 2y1 < y2 < ∞
since y1 < y2 – y1. Thus, (4.4.4) leads to the following joint pdf of Y1 and Y2:
The marginal pdf’s of Y1, Y2 can be easily verified as the following:
We leave out some of the intermediate steps as the Exercise 4.4.6.
Example 4.4.8 Suppose that X1 and X2 have their joint pdf given by
Let Y1 = X1 + X2 and Y2 = X1 – X2. We first wish to obtain the joint pdf of Y1
and Y2. Then, the goal is to derive the marginal pdf’s of Y1, Y2.
The one-to-one transformation (x1, x2) → (y1, y2) leads to the inverse: x1 =
1/2(y1 + y2), x2 = 1/2(y1 – y2) so that |det(J)| = 1/2. Observe that: 0 < x1 < 1 ⇒
0 < y1 + y2 < 2; 0 < x2 < 1 ⇒ 0 < y1 – y2 < 2; 0 < x1 + x2 < 1 ⇒ 0 < y1 < 1. Let
γ = {(y1, y2) ∈ ℜ : 0 < y1 < 1, 0 < y1 + y2 < 2, 0 < y1 – y2 < 2}. The joint pdf of
Y1 and Y2 is then given by
The marginal pdf’s of Y1, Y2 can be easily verified as the following:
We leave out some of the intermediate steps as the Exercise 4.4.7. !
Example 4.4.9 The Helmert Transformation: This consists of a very
special kind of orthogonal transformation from a set of n iid N(µ, σ2)

198
4. Functions of Random Variables and Sampling Distribution
random variables X1, ..., Xn, with n ≥ 2, to a new set of n random variables Y1,
..., Yn defined as follows:
Y1, ..., Yn so defined are referred to as the Helmert variables.
Let us denote the matrix
Then, one has Y = Ax where x’ = (x1, ..., xn) and y’ = (y1, ..., yn).
An×n is an orthogonal matriX. So, A’ is the inverse of A. This
       implies that
In ℜn, a sphere in the x-coordinates continues to look
like a sphere in the y-coordinates when the x axes are rotated
orthogonally to match with the new y axes.
Observe that the matrix J defined in (4.4.3) coincides with the matrix A’ in
the present situation and hence one can immediately write | det(J) |= | det(A’)
|=| {det(AA’)}½ |= 1.
Now, the joint pdf of X1, ..., Xn is given by
for –∞ < x1, ..., xn < ∞, and thus using (4.4.4) we obtain the joint pdf of Y1, ...,
Yn as follows:

4. Functions of Random Variables and Sampling Distribution
199
where –∞ < y1, ..., yn < ∞. In (4.4.8), the terms involving y1, ..., yn factorize
and none of the variables’ domains involve the other variables. Thus it is clear
(Theorem 3.5.3) that the random variables Y1, ..., Yn are independently dis-
tributed. Using such a factorization, we also observe that Y1 is distributed as
     whereas Y2, ..., Yn are iid N(0, σ2). !
The following result plays a crucial role in many statistical analyses. Inci-
dentallY, one should note that sometimes a population is also referred to as a
universe in the statistical literature. The result we are about to mention is also
indispensable in much of the distribution theory. These points will become
clear in the sequel.
Theorem 4.4.2 (Joint Sampling Distribution of     and S2 from a
Normal Universe) Suppose that X1, ..., Xn are iid N(µ, σ2) random vari-
ables, n ≥ 2. Define the sample mean ,
        and the sample
variance
Then, we have:
(i)
The sample mean      is distributed independently of the sample
variance S2;
(ii) The sampling distribution of     is               and that of (n – 1)S2/
σ2 is Chi-square with (n – 1) degrees of freedom.
Proof (i) Using the Helmert transformation from (4.4.6), we can rewrite
= n–1/2 Y1. Next, observe that
using the same Helmert variables. It is clear that      is a function of Y1 alone,
whereas from (4.4.9) we note that S2 depends functionallY on (Y2, ..., Yn)
only. But, since Y1, ..., Yn are all independent, we conclude that     is distrib-
uted independentlY of S2. Refer to the Theorem 3.5.2, part (ii) as needed.
(ii) Recall that      = n–1/2 Y1 where Y1 is distributed as N(     µ, σ2) and so
the sampling distribution of      follows immediately. From (4.4.9) again, it is
clear that
      which is the sum of (n – 1) indepen-
dent Chi-square random variables each having one degree of freedom. Hence,
using the reproductive property of independent Chi-square variables (Theo-
rem 4.3.2, part (iii)), it follows that (n – 1)S2/σ2 has a Chi-square distribution
with (n – 1) degrees of freedom. ¢
Remark 4.4.1 Let us reconsider the setup in the Example 4.4.9 and Theo-
rem 4.4.2. It is important to note that the sample variance S2 is the average of
  and each Helmert variable independently contributes one de-
gree of freedom toward the total (n – 1) degrees of freedom. Having n
observations X1, ..., Xn, the decomposition in (4.4.9) shows how ex-
actly S2 can be split up into (n – 1) independent and identically distributed

200
4. Functions of Random Variables and Sampling Distribution
components. This is the central idea which eventually leads to the Analyses of
Variance techniques, used so widely in statistics.
The Exercise 4.6.7 gives another transformation which proves that
     and S2 are independent when the X’s are iid N(µ, σ2).
Remark 4.4.2 Let us go back one more time to the setup considered in
the Example 4.4.9 and Theorem 4.4.2. Another interesting feature can be
noticed among the Helmert variables Y2, ..., Yn. Only the Helmert variable Yn
functionally depends on the last observed variable Xn. This particular feature has
an important implication. Suppose that we have an additional observation Xn+1 at
our disposal beYond X1, ..., Xn. Then, with         =                         , the
new sample variance          and its decomposition would be expressed as
where
  Here, note that Y2, ..., Yn
based on X1, ..., Xn alone remain exactly same as in (4.4.6). In other words,
the Helmert transformation shows exactly how the sample variance is af-
fected in a sequential setup when we let n increase successively.
Remark 4.4.3 By extending the two-dimensional polar transformation
mentioned in (4.1.2) to the n-dimension, one can supply an alternative proof
of the Theorem 4.4.2. Indeed in Fisher’s writings, one often finds derivations
using the n-dimensional polar transformations and the associated geometry.
We may also add that we could have used one among many choices of or-
thogonal matrices instead of the specific Helmert matrix A given by (4.4.7) in
proving the Theorem 4.4.2. If we did that, then we would be hard pressed to
claim useful interpretations like the ones given in our Remarks 4.4.1-4.4.2
which guide the readers in the understanding of some of the deep-rooted
ideas in statistics.
Suppose that X1, ..., Xn are iid random variables with n ≥ 2. Then,
 and S2 are independent ⇒ Xi’s are normally distributed.
Remark 4.4.4 Suppose that from the iid random samples X1, ..., Xn, one
obtains the sample mean      and the sample variance S2. If it is then assumed
that      and S2 are independently distributed, then effectively one is not assum-
ing anY less than normality of the original iid random variables. In other words,
the independence of      and S2 is a characteristic property of the normal
distribution alone. This is a deep result in probability theory. For a
proof of this fundamental characterization of a normal distribution

4. Functions of Random Variables and Sampling Distribution
201
and other historical notes, one may refer to Zinger (1958), Lukacs (1960),
and Ramachandran (1967, Section 8.3), among a host of other sources. Look
at the Exercises 4.4.22-4.4.23 which can be indirectly solved using this char-
acterization.
Example 4.4.10 The Exponential Spacings: Let X1, ..., Xn be iid with the
common pdf given by
  
Define the order statistics Xn:1 ≤ Xn:2 ≤ ... ≤ Xn:n and let us write Yi = Xn:i, i =
1, ..., n. The Xi’s and Xn:i’s may be interpreted as the failure times and the
ordered failure times respectively. Let us denote
and these are referred to as the spacings between the successive order statis-
tics or failure times. In the context of a life-testing experiment, U1 corre-
sponds to the waiting time for the first failure, and Ui corresponds to the time
between the (i – 1)th and the ith failures, i = 2, ..., n. Here, we have a one-to-
one transformation on hand from the set of n variables (y1, ..., yn) to another
set of n variables (u1, ..., un). Now, in view of (4.4.4), the joint pdf of the
order statistics Y1, ..., Yn is given by
Note that 
. One can
also verify that | det(J) |= 1, and thus using (4.4.4) and (4.4.13), the joint pdf
of U1, ..., Un can be written as
  
for 0 < u1, ..., un < ∞. Next, (4.4.14) can be easily rewritten as
 
for 0 < u1, ..., un < ∞. In (4.4.15), the terms involving u1, ..., un factorize and
none of the variables’ domains involve any other variables. Thus it is clear
(Theorem 3.5.3) that the random variables U1, ..., Un are independently dis-
tributed. Using such a factorization, we also observe that Ui has an exponen-
tial distribution with the mean (n – i + 1)–1β,i = 1, ..., n. !

202
4. Functions of Random Variables and Sampling Distribution
Example 4.4.11 (Example 4.4.10 Continued) Suppose that the X’s are iid
and the common pdf is the same as the one in (4.4.11). For all k = 1, ..., n –
1, observe that
z
and hence
Since 
 we have
It is worthwhile to note that we have succeeded in deriving an expression for
the expected value of the kth order statistic Xn:k without finding the marginal
distribution of Xn:k. These techniques are particularly useful in the areas of
reliability and survival analyses. !
If the X’s are iid normal, the Helmert transformation provides
a natural way to consider intricate properties of 
 and S2. If the
X’s are iid exponential or negative exponential, the transformation
involving the spacings between the successive order statistics
is a natural one to consider instead.
Example 4.4.12 The Negative Exponential Distribution: Suppose that
X1, ..., Xn are iid random variables having a common pdf given by
where µ is the location parameter and σ is the scale parameter. In reliability
applications, µ is often referred to as the minimum guarantee time or the
minimum threshold, and hence µ is assumed positive in such applications.
Refer back to (1.7.36) as needed. We will, however, continue to assume that
µ is an arbitrary real number. Let us consider the two random variables
and look at their distributions. Denote Wi = Xi – µ and then it is clear that
W1, ..., Wn are iid having the common pdf given by in (4.4.11) with β
replaced by σ. Following the Example 4.4.10, we write U1 = Wn:1, U2 =

4. Functions of Random Variables and Sampling Distribution
203
Wn:2 – Wn:1, ..., Un = Wn:n – Wn:n–1, and realize that n(Xn:1 – µ)/σ is same as
nU1/σ, which has the standard exponential distribution. That is, n(Xn:1 – µ)/σ
is distributed as the standard exponential distribution which is the same as
Gamma (1, 1). Also, recall that U1, ..., Un must be independent random vari-
ables. Next, one notes that
Now, it is clear that T is distributed independently of Xn:1 because T function-
ally depends only on (U2, ..., Un) whereas Xn:1 functionally depends only on
U1. But, U1 is independent of (U2, ..., Un). Also, note that
where Z2n, ..., Znn are iid random variables having the Chi-square distribution
with two degrees of freedom. Thus, using the reproductive property of inde-
pendent Chi-square variables (Theorem 4.3.2, part (iii)), we conclude that
 has the Chi-square distribution with 2(n – 1) degrees of
freedom. !
Suppose that X1, ..., Xn are iid random variables.
If their common distribution is negative exponential, then
Xn:1 and
 are independent.
Remark 4.4.5 If we compare (4.4.20) with the representation given in
(4.4.9), it may appear that in principle, the basic essence of the Remark 4.4.1
holds in this case too. It indeed does, but only partially. One realizes fast that
in the present situation, one is forced to work with the spacings between the
successive order statistics. Thus, the decomposition of 2Tσ–1 into unit inde-
pendent components consists of (n – 1) random terms, each depending on the
sample size n. This is fundamentally different from what we had observed in
(4.4.9) and emphasized earlier in the Remark 4.4.2.
In the next example X1 and X2 are independent, but the
transformed variables Y1 and Y2 are dependent.
Example 4.4.13 (Example 4.4.5 Continued) Suppose that X1 and X2 are iid
standard exponential random variables. Thus,

204
4. Functions of Random Variables and Sampling Distribution
We denote y1 = x1 + x2, y2 = x2 so that for this one-to-one transformation we
have x1 = y1 – y2, x2 = y2 where 0 < y2 < y1 < ∞. One can verify that | det(J)
| = 1, and hence the joint pdf of Y1 and Y2 would become
Then, from (4.4.21) we obtain the marginal pdf of Y1 as
In other words, Y1 = X1 + X2 has the Gamma(2, 1) distribution. We leave out
the intermediate steps as the Exercise 4.4.16. !
In the next example X1, X2 and X3 are independent, but the
transformed variables Y1, Y2 and Y3 are dependent.
Example 4.4.14 (Example 4.4.13 Continued) Suppose that X1, X2 and X3
are iid standard exponential random variables. Thus,
We denote y1 = x1 + x2 + x3, y2 = x2, y3 = x3 so that for this one-to-one
transformation we have x1 = y1 – y2 – y3, x2 = y2, x3 = y3 where 0 < y2 < y1 ∞,
0 < y3 < y1 < ∞ and y2 + y3 < y1. One can verify that | det(J) | = 1, and hence
the joint pdf of Y1, Y2 and Y3 would become
Then, from (4.4.22) we obtain the marginal pdf of Y1 as
In other words, Y1 = X1 + X2 + X3 has the Gamma(3, 1) distribution. We leave
out the intermediate steps as the Exercise 4.4.17. !
In the next example X1 and X2 are dependent, but the transformed
variables Y1 and Y2 are independent.
Example 4.4.15 Suppose that the random vector (X1, X2) has the bivari-
ate normal distribution, N2(0, 0, σ2, σ2, ρ) with 0 < σ < ∞, –1 < ρ < 1.

4. Functions of Random Variables and Sampling Distribution
205
Let us derive the joint pdf of Y1 = X1 + X2 and Y2 = X1 – X2. Refer to
the Section 3.6 as needed. With c = {2πσ2(1 – ρ2)–1/2}–1, we start
with
where –∞ < x1, x2 < ∞. For the one-to-one transformation on hand, we have
x1 = 1/2(y1 + y2), x2 = 1/2(y1 – y2) with 0 < y1, y2 < ∞. One can easily verify
that | det(J)| = 1/2, and hence the joint pdf of Y1 and Y2 would become
for –∞ < y1, y2 < ∞. In (4.4.23), the terms involving y1, y2 factorize and none
of the variables’ domains involve the other variable. Thus it is clear (Theorem
3.5.3) that the random variables Y1, Y2 are independently distributed. Using
such a factorization, we also observe that Y1 = X1 + X2 is distributed as N(0,
2σ2(1 + ρ)) whereas Y2 = X1 – X2 is distributed as N(0, 2σ2(1 – ρ)). !
Suppose that the transformation from (X1, ..., Xn) → (Y1, ..., Yn) is not
one-to-one. Then, the result given in the equation (4.4.4) will need slight
adjustments. Let us briefly explain the necessary modifications. We begin by
partitioning the χ space, that is the space for (x1, ..., xn), into A1, ..., Ak in
such a way that the associated transformation from Ai → y, that is the space
for (y1, ..., yn), becomes separately one-to-one for each i = 1, ..., k. In other
words, when we restrict the original mapping of (x1, ..., xn) → (y1, ..., yn) on
Ai → y, given a specific (y1, ..., yn) ∈ y, we can find a unique (x1, ..., xn) in Ai
which is mapped into (y1, ..., yn). Given (y1, ..., yn), let the associated xj =
bij(y1, ..., yn), j = 1, ..., n, i = 1, ..., k. Suppose that the corresponding
Jacobian matrix is denoted by Ji, i = 1, ..., k. Then one essentially applies
(4.4.4) on each piece A1, ..., Ak and the pdf g(y1, ..., yn) of Y is obtained as
follows:
for y ∈ y. For a clear understanding, let us look at the following ex-
ample.
Example 4.4.16 Suppose that X1 and X2 are iid N(0, 1) variables.
Let us denote Y1 = X1/X2 and Y2 = 
  Obviously, the transformation
(x1, x2) → (y1, y2) is not one-to-one. Given (y1, y2), the inverse solu-
tion would be 
 or 
  We should take A1 =
{(x1, x2) ∈ ℜ2 : x2 > 0}, A2 = {(x1, x2) ∈ ℜ2 : x2 < 0}. Now, we apply

206
4. Functions of Random Variables and Sampling Distribution
(4.4.24) directly as follows: With –∞ < y1 < ∞, 0 < y2 < ∞, we get
With –∞ < y1 < ∞ and the substitution 
  the marginal pdf
g1(y1) of Y1 is then obtained from (4.4.25) as
which coincides with the Cauchy pdf defined in (1.7.31). That is, X1/X2
has the Cauchy distribution. 
Next, with the substitution
 ∞, the marginal pdf g2(y2) of Y2 is then obtained from (4.4.25) as
But, note that h(y1) resembles the pdf of the 
 variable for any fixed
0 < y2 < ∞. Hence, 
 must be one for any fixed 0 < y2 < ∞.
That is, for all fixed 0 < y2 < ∞, we get
which coincides with the pdf of the x2
1 variable. In other words, has the
 distribution. !
The Exercise 4.4.18 provides a substantial generalization in the
sense that X1/X2 can be claimed to have a Cauchy distribution
even when (X1, X2) has the N2(0, 0, σ2, σ2, ρ) distribution. The
transformation used in the Exercise 4.4.18 is also different
from the one given in the Example 4.4.16.
4.5   Special Sampling Distributions
In the Section 1.7, we had listed some of the standard distributions by
including their pmf or pdf. We recall some of the continuous distributions,

4. Functions of Random Variables and Sampling Distribution
207
particularly the normal, gamma, Chi-square, Student’s t, the F and beta distri-
butions. At this point, it will help to remind ourselves some of the techniques
used earlier to find the moments of normal, Chi-square or gamma variates.
In this section, we explain how one constructs the Student’s t and F vari-
ables. Through examples, we provide some justifications for the importance
of both the Student’s t and F variates. When we move to the topics of statis-
tical inference, their importance will become even more convincing.
4.5.1
The Student’s t Distribution
Definition 4.5.1 Suppose that X is a standard normal variable, Y is a Chi-
square variable with í degrees of freedom, and that X and Y are independently
distributed. Then, the random variable 
 is said to have the
Student’s t distribution, or simply the t distribution, with í degrees of freedom.
Theorem 4.5.1 The pdf of the random variable W mentioned in the Defi-
nition 4.5.1, and distributed as tí, is given by
with 
Proof The joint pdf of X and Y is given by
for –∞ < x < ∞, 0 < y < ∞ where
  Let us denote u
= y and 
  so that the inverse transformation is given by
   and y = u. Note that  
  From (4.5.1), the joint pdf
of U and W can be written as
for 0 < u < ∞, –∞ < w < ∞. Thus, for –∞ < w < ∞ and with the substitution s
= u(1 + w2ν–1)/2, the pdf h(w) of W is given by
which matches with the intended result. Some of the details are left out as the
Exercise 4.5.1.¢

208
4. Functions of Random Variables and Sampling Distribution
Once the pdf h(w) is simplified in the case ν = 1, we find that h(w) = π–1
(1+w2)–1 for –∞ < w < ∞. In other words, when ν = 1, the Student’s t distri-
bution coincides with the Cauchy distribution. Verification of this is left as the
Exercise 4.5.2.
In some related problems we can go quite far without looking at
the pdf of the Student’s t variable. This point is emphasized next.
A case in point: note that –W thus defined can simply be written as
 Since (i) –X is distributed as standard normal, (ii) –X is
distributed independently of Y, we can immediately conclude that W and –W
have identical distributions, that is the Student’s t distribution is symmetric
around zero. In other words, the pdf h(w) is symmetric around w = 0. To
conclude this, it is not essential to look at the pdf of W. On the other hand, one
may arrive at the same conclusion by observing that the pdf h(w) is such that
h(w) = h(–w) for all w > 0.
How about finding the moments of the Student’s t variable? Is the pdf
h(w) essential for deriving the moments of W? The answer is: we do not really
need it. For any positive integer k, observe that
as long as E[Y–k/2] is finite. We could split the expectation because X and Y are
assumed independent. But, it is clear that the expression in (4.5.2) will lead to
finite entities provided that appropriate negative moments of a Chi-square
variable exist. We had discussed similar matters for the gamma distributions
in (2.3.24)-(2.3.26).
By appealing to (2.3.26) and (4.5.2), we claim that for the Student’s t
variable W given in the Definition 4.5.1, we have E(W) finite if 1/2ν > –(–1/2)
that is if ν > 1, whereas E(W2) is finite, that is V(W) is finite when 1/2ν > –(–
1) or ν > 2. One should verify the following claims:
and also,
Example 4.5.1 The One-Sample Problem: Suppose that X1, ..., Xn are
iid N(µ, σ2), –∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. Let us recall that 
 is
distributed as N(0, 1), (n – 1)S2/σ2 is 
 and these two are also indepen-
dent (Theorem 4.4.2). Then, we rewrite

4. Functions of Random Variables and Sampling Distribution
209
In other words, 
 has the same representation as that of W with
ν = n – 1. Hence, we can claim that
A standardized variable such as 
 is widely used in the sequel
when the population mean µ and variance σ2 are both unknown.
W. S. Gosset was a pioneer in the development of statistical methods for
design and analysis of experiments. He is perhaps better known under the
pseudonym “Student” than under his own name. In most of his papers, he
preferred to use the pseudonym “Student” instead of his given name. His
path-breaking 1908 paper gave the foundation of this t-distribution. !
Example 4.5.2 The Two-Sample Problem: Suppose that the random vari-
ables Xi1, ..., Xini are iid N(µi, σ2), i = 1, 2, and that the X1j’s are independent
of the X2j’s. With ni ≥ 2, let us denote
for i = 1, 2.
 is called the pooled sample variance.
Now, 
  2, and these are also independent. Us-
ing the reproductive property of independent Chi-squares (Theorem 4.3.2,
part (iii)) we claim that 
 has a Chi-square dis-
tribution with (n1 + n2 – 2) degrees of freedom. Also, 
 and 
 are
independent. Along the lines of the Example 4.5.1, we can claim that
This two-sample Student’s t distribution is widely used in the statistical litera-
ture. !
4.5.2   The F Distribution
Definition 4.5.2 Let X, Y be independent Chi-square random variables dis-
tributed respectively with í1 and í2 degrees of freedom. Then, the random
variable U = (X/ ν1) ÷ (Y/ ν2) is said to have the F distribution with degrees
of freedom ν1, ν2, in that order.

210
4. Functions of Random Variables and Sampling Distribution
Theorem 4.5.2 The pdf of the random variable U mentioned in the Defi-
nition 4.5.2, and distributed as Fí1, í2, is given by
with k = k(ν1, ν2) = (ν1/ν2)1/2ν
1 Γ((ν1 + ν2)/2) {Γ(ν1/2)Γ(ν2/2)}–1 and ν1, ν2
= 1, 2, ... .
Proof The joint pdf of X and Y is given by
for 0 < x, y < ∞ where c = {2(ν
1
+ν
2
)/2Γ(ν1/2)Γ(ν2/2)}–1. Let us denote
u = ν2-ν1 x/y and v = y, so that the inverse transformation is given by x = ν1-ν2 uv
and y = v. Note that |J| = ν1-ν2v. From (4.5.9), the joint pdf of U and V can be
written as
for 0 < u, v < ∞. Thus, for 0 < u < ∞, the pdf h(u) of U is given by
by viewing the integrand as a gamma pdf,
which matches with the intended result. ¢
In some related problems we can go quite far without looking at
the pdf of the F variable. This point is emphasized next.
From the Definition 4.5.1 of the Student’s t random variable W, it is clear
that W2 = X2(Y/ν)–1. Since (i) X2, Y are distributed as Chi-squares respec-
tively with one and ν degrees of freedom, (ii) X2 and Y are also indepen-
dent, we can conclude that W2 has the same form as in the Definition
4.5.2 for U. Thus, the pdf of U is not essential to arrive at the conclusion:
 has the same distribution as that of F1, ν
The F distribution is not symmetric in the same sense as the t distri-
bution is. But, from the Definition 4.5.2, it follows immediately though
that 1/U, which is the same as (Y/ ν2) ÷ (X/ ν1), should be distributed

4. Functions of Random Variables and Sampling Distribution
211
as Fν2, ν1. That is, 1/F has a F distribution too. This feature may be intuitively
viewed as the “symmetry property” of the pdf h(u). The explicit form of the
pdf has not played any crucial role in this conclusion.
How about finding the moments of the Fν1, ν2 variable? The pdf h(u) is not
essential for deriving the moments of U. For any positive integer k, observe
that
as long as E[Y–k] is finite. We can split the expectation in (4.5.10) because X
and Y are assumed independent. But, it is clear that the expression in (4.5.10)
will lead to finite entities provided that appropriate negative moment of a Chi-
square variable exists. We had discussed similar matters for the gamma distri-
butions in (2.3.24)-(2.3.26).
By appealing to (2.3.26) and (4.5.10), we claim that for the Fν1, ν2 variable
given in the Definition 4.5.2, we have E(U) finite if ν2 > 2, whereas E(U2) is
finite, that is V(U) is finite when ν2 > 4. One should verify the following
claims:
and also
Example 4.5.3 The Two-Sample Problem:  Let Xi1, ..., Xini be iid
 i = 1, 2, and that the X1j’s are independent of the X2j’s. For ni ≥ 2,
we denote 
 as in the
Example 4.5.2. Now,
, (ii) they are also
independent, and hence in view of the Definition 4.5.2, the random variable U
= (S1/σ1)2 ÷ (S2/σ2) has the F distribution with degrees of freedom ν1 = n1 –
1, ν2 = n2 – 1. !
4.5.3   The Beta Distribution
Recall the random variable U mentioned in the Definition 4.5.2. With k =
k(ν1,ν2) = (ν1/ν2)1/2 ν
1 Γ((ν1 + ν2)/2) {Γ(ν1/2)Γ(ν2/2)}–1, the pdf of U was
for 0 < u < ∞. Now, let us define Z = [(ν1/ν2)U]/[1 + (ν1/ν2)U]. The
transformation from u → z is one-to-one and we have (ν1/ν2)u = z(1 –

212
4. Functions of Random Variables and Sampling Distribution
z)–1, 0 < u < ∞, 0 < z < 1. One may also check that (ν1/ν2)  du-dz = (1 – z)–2. Thus,
combining (4.4.1) and (4.5.13) in a straightforward fashion, we can write
down the pdf of Z as follows: For 0 < z < 1, we have
which simplifies to
It coincides with the beta pdf defined in (1.7.35) where α = 1/2ν1 and β = 1/
2ν2. That is,
[(ν1/ν2)Fν1, ν2]/[1 + (ν1/ν2) Fν1, ν2] has Beta (1/2ν1, 1/2ν2) distribution.
4.6   Special Continuous Multivariate Distributions
We now include some interesting aspects of the multivariate normal, t, and F
distributions. It will become clear shortly that both the multivariate t and F
distributions are close associates of the multivariate normal distribution. One
may review the Section 4.8 for some of the details about matrices.
Tong’s (1990) book is devoted to the multivariate normal distributions and
includes valuable tables. It briefly discusses the multivariate t and F distribu-
tions too. The references to the tables and other features for the multivariate
normal, t and F distributions can be found in Johnson and Kotz (1972).
We included important properties of a bivariate normal distribution in the
Section 3.6. The sampling distributions in the context of a bivariate normal
population, however, is included in the present section.
4.6.1 The Normal Distribution
The bivariate normal density was given in (3.6.1). The general multivariate
normal density is more involved. But, without explicitly referring to the pdf,
one can derive many interesting and useful properties. The following broad
definition of the p-dimensional normality can be found in Rao (1973, p. 518.)
The advantage of adopting this definition over another relying explicitly on the
multivariate pdf will be clear from the Examples 4.6.1-4.6.2.
Definition 4.6.1 A p(≥ 1) random vector X = (X1, ..., Xp) is said to
have a p-dimensional normal distribution, denoted by Np, if and only if

4. Functions of Random Variables and Sampling Distribution
213
each linear function 
 has the univariate normal distribution for all
fixed, but arbitrary real numbers a1, ..., ap.
Example 4.6.1 Suppose that X1, ..., Xn are iid N (µ, σ2), –∞ < µ < ∞, 0 <
σ < ∞. Consider the sample mean 
 and let us look at the two-dimensional
joint distribution of (X1,
). Obviously, E(X1)=E(
)=µ,V(X1)
=σ2 and V(
) = 
1-nσ2. Also, we can write Cov(X1,
) =
 Refer to the Theorem 3.4.3,
part (iii) as needed. But, the covariance between X1 and Xj is zero for 1 < j ≤
n. That is, Cov(X1, 
) = 1-n Cov(X1, X1) = 1-nσ2 and hence 
 Any
linear function L of X1 and 
 is also a linear function of n original iid normal
random variables X1, ..., Xn, and hence L itself is distributed normally. This
follows from the reproductive property of independent normal variables (Theo-
rem 4.3.2, part (i)). So, by the Definition 4.6.1 it follows that (X1, 
) is
jointly distributed as a bivariate normal variable. Thus, the conditional distri-
bution of X1 given
is 
 is N ( , σ2 (1 – 1/n)) which provides the
following facts: 
 and V(X1 | 
) = σ2(1 – 1-n). Refer
to the Theorem 3.6.1 as needed. Look at the Exercise 4.6.1 !
Example 4.6.2 (Example 4.6.1 Continued) One can easily find the condi-
tional expectation of aX1 + bX2 given 
 in the following way where a
and b are fixed non-zero real numbers. Observe that E(aX1 + bX2 |  
)
= E(aX1 | 
) + E(bX2 | 
) = (a + b) . Also note, for example, that
 = V(X1 |  
) + {E(X1 | 
)}2 = σ2(1 – 1/n) + 
2 .
One can exploit similar techniques to obtain other expected values in this
context. Look at the Exercise 4.6.2. !
Example 4.6.3 (Example 4.6.1 Continued) A result such as E(X1| 
=  can be alternately derived as follows. Obviously,E(X1 | 
) = , and
this can be rewritten as 
 
)
E(X1 | 
) which was the intended result in the first place. This argument
works here because E{Xi | 
} = E(X1 | 
) for each i = 1, ..., n.
Observe that this particular approach merely used the fact that the Xi’s are iid,
but it did not exploit the fact that the Xi’s are iid normal to begin with. !
Example 4.6.4 (Example 4.6.2 Continued) A result such as 
 ) = σ2(1 – 1/n) +
2 can be alternately derived as follows. Obviously,
σ2 = E(S2) = E(S2 | 
 ) since S2 and 
 are independent. Here, we have
indeed used the full power of the assumption of normality. Recall the Remark
4.4.4 in this context. So one can write and hence 

214
4. Functions of Random Variables and Sampling Distribution
} = 
 + (n – 1)σ2. This leads to the desired conclusion: 
 }
= 
 + (1 – 1/n)σ2.
For completeness, we now give the pdf of the p-dimensional normal ran-
dom vector. To be specific, let us denote a p-dimensional column vector X
whose transpose is X’ = (X1, ..., Xp), consisting of the real valued random
variable Xi as its ith component, i = 1, ..., p. We denote E[Xi] = µi, V[Xi] = σii,
and Cov(Xi, Xj) = σij, 1 ≤ i ≠ j ≤ p. Suppose that we denote the mean vector
µµµµµ where µ′
µ′µ′
µ′
µ′ = (µ1, ..., µp) and then write down σij as the (i, j)th element of the
p × p matrix ΣΣΣΣΣ, 1 ≤ i ≠ j = p. Then, ΣΣΣΣΣ = (σij)p×p is referred to as the variance-
covariance matrix or the dispersion matrix of the random vector X.
Assume that the matrix ΣΣΣΣΣ has the full rank which is equivalent to saying
that ΣΣΣΣΣ–1 exists. Then, the random vector X has the p-dimensional normal
distribution with mean vector µµµµµ and dispersion matrix ΣΣΣΣΣ, denoted by Np(µµµµµ,
ΣΣΣΣΣ), provided that the joint pdf of X1, ..., Xp is given by
where x = (x1, ..., xp). One should check that the bivariate normal pdf from
(3.6.1) can be written in this form too. We leave it as the Exercise 4.6.3.
C. F. Gauss originally derived the density function given in (4.6.1) from
that of linear functions of independent normal variables around 1823-1826. In
many areas, including the sciences and engineering, the normal distributions
are also frequently referred to as the Gaussian distributions.
We leave the proofs of (4.6.2)-(4.6.3) as the Exercise 4.6.4.
Example 4.6.5 (Exercise 3.5.17 Continued) Suppose that the random
vector X = (X1, X2, X3, X4) has the 4-dimensional normal distribution

4. Functions of Random Variables and Sampling Distribution
215
namely, N4(µµµµµ, ΣΣΣΣΣ) with µµµµµ = 0 and
with the matrices and vectors P3×3 = 4I, Q’1×3 = (111) and S1×1 = 1. The
matrix ΣΣΣΣΣ is p.d. and it is easy to check that det(ΣΣΣΣΣ) = 16.
Let us denote the matrices
Thus, applying (4.8.10), we have
and hence
In other words, in this particular situation, the pdf from (4.6.1) will reduce to
for (x1, x2, x3, x4) ∈ ℜ4. Thus, in the Exercise 3.5.17, the given random vector
X actually had this particular 4-dimensional normal distribution even though
at that time we did not explicitly say so.
Sampling Distributions: The Bivariate Normal Case
For the moment, let us focus on the bivariate normal distribution. Suppose
that (X1, Y1), ..., (Xn, Yn) are iid 
 where –∞ < µ1, µ2 < ∞,
0 < 
 < ∞ and –1 < ρ < 1, n ≥ 2. Let us denote

216
4. Functions of Random Variables and Sampling Distribution
Here, r is defined as the Pearson correlation coefficient
or simply the sample correlation coefficient.
By separately using the marginal distributions of X1, ..., Xn and Y1, ..., Yn, we
can right away claim the following sampling distributions:
The joint distribution of 
 is not very difficult to obtain by means of
transformations. We leave this out as the Exercise 4.6.8.
Alternately, note, however, that any linear function of 
 and 
 is also a
linear function of the original iid random vectors (Xi, Yi), i = 1, ..., n. Then, by
appealing to the Definition 4.6.1 of a multivariate normal distribution, we can
immediately claim the following result:
How does one find ρ*? Invoking the bilinear property of covariance from
Theorem 3.4.3, part (iii), one can express Cov 
  as
so that the population correlation coefficient between 
 and 
 is simplified
to n–1 ρσ1σ2 / 
.
The distribution of the Pearson correlation coefficient
is quite complicated, particularly when ρ ≠ 0. Without explicitly writing down
the pdf of r, it is still simple enough to see that the distribution of r can not
depend on the values of µ1, 
 and 
. To check this claim, let us
denote Ui = (Xi – µ1)/σ1, Vi = (Yi – µ2)/σ2, and then observe that the random
vectors (Ui, Vi), i = 1, ..., n, are distributed as iid N2(0, 0, 1, 1, ρ). But, r can
be equivalently expressed in terms of the Ui’s and Vi’s as follows:
From (4.6.8), it is clear that the distribution of r depends only on ρ.

4. Functions of Random Variables and Sampling Distribution
217
Francis Galton introduced a numerical measure, r, which he termed “re-
version” in a lecture at the Royal Statistical Society on February 9, 1877 and
later called “regression”. The term “cor-relation” or “correlation” probably
appeared first in Galton’s paper to the Royal Statistical Society on December
5, 1888. At that time, “correlation” was defined in terms of deviations from
the median instead of the mean. Karl Pearson gave the definition and calcula-
tion of correlation as in (4.6.7) in 1897. In 1898, Pearson and his collabora-
tors discovered that the standard deviation of r happened to be 
when n was large. “Student” derived the “probable error of a correlation
coefficient” in 1908. Soper (1913) gave large sample approximations for the
mean and variance of r which were better than those proposed earlier by
Pearson. Refer to DasGupta (1980) for some of the historical details.
The unsolved problem of finding the exact pdf of r for normal variates
came to R. A. Fisher’s attention via Soper’s 1913 paper. The pdf of r was
published in the year 1915 by Fisher for all values of ρ ∈ (–1, 1). Fisher, at the
age of 25, brilliantly exploited the n-dimensional geometry to come up with
the solution, reputedly within one week. Fisher’s genius immediately came
into limelight. Following the publication of Fisher’s results, however, Karl
Pearson set up a major cooperative study of the correlation. One will notice
that in the team formed for this cooperative project [Soper et al. (1917)]
studying the distribution of the sample correlation coefficient, the young Fisher
was not included. This happened in spite of the fact that Fisher was right
there and he already earned quite some fame. Fisher felt hurt as he was left
out of this project. One thing led to another. R. A. Fisher and Karl Pearson
continued criticizing each other even more as each held on to his philosophi-
cal stand.
We will merely state the pdf of r when ρ = 0. This pdf is given by
where 
 for n ≥ 3. Using (4.6.9) and
some simple transformation techniques, one can easily derive the following
result:
The verification of the claim in (4.6.10) is left as the Exercise 4.6.9. Fisher
(1915) also gave the exact pdf of r in the form of an infinite power series for
all values of ρ ∈ (–1, 0) ∪ (0, 1).

218
4. Functions of Random Variables and Sampling Distribution
Sampling Distributions : The Multivariate Normal Case
Now, we briefly touch upon some aspects of sampling distributions in the
context of a multivariate normal population. Suppose that X1, ..., Xn are iid
Np(µµµµµ, ΣΣΣΣΣ) where µµµµµ ∈ ℜp and ΣΣΣΣΣ is a p × p p.d. matrix, n ≥ 2. Let us denote
Observe that 
 is a p-dimensional column vector whereas W is a p × p
matrix, both functionally depending on the random samples X1, ..., Xn.
Theorem 4.6.1 Suppose that X1, ..., Xn are iid Np(µµµµµ, ΣΣΣΣΣ) where µµµµµ ∈ ℜp and
ΣΣΣΣΣ is a p × p p.d. matrix, n ≥ 2. Then, we have the following sampling distri-
butions:
The part (i) in this theorem is easily proved by applying the Definition
4.6.1. Also, the part (ii) can be easily proved when p = 2. We leave their
verifications out in the Exercise 4.6.10. Proofs of parts (ii)-(iv) in their fullest
generality are, however, out of scope for this book. The readers, however,
should exploit these results to avoid laborious calculations whenever possible.
4.6.2   The t Distribution
This distribution comes up frequently in the areas of multiple comparisons
and selection and ranking. Let the random vector X, where X’ = (X1, ...,
Xp), have a p-dimensional normal distribution with the mean vector 0 and
the p × p dispersion matrix ΣΣΣΣΣ. We assume that each diagonal entry in ΣΣΣΣΣ is 1.
That is, the random variables X1, ..., Xp have each been standardized. In
other words, the (i, j)th entry in the matrix ΣΣΣΣΣ corresponds to the population
correlation coefficient ρij between the two variables Xi, Xj, 1 ≤ i ≠ j ≤ p. In
this special situation, one also refers to ΣΣΣΣΣ as a correlation matrix. Suppose
that Y is a positive real valued random variable distributed as 
. It is also
assumed that Y and (X1, ..., Xp) are independent. Let us denote p new ran-
dom variables

4. Functions of Random Variables and Sampling Distribution
219
The marginal distribution of each Ti has the Student’s t distribution with ν
degrees of freedom. This follows from the definition of the Student’s t ran-
dom variable. Jointly, however, (T1, ..., Tp) is said to have the p-dimensional
t distribution which depends on the correlation matrix ΣΣΣΣΣ, denoted by Mtp(ν,
ΣΣΣΣΣ). One may refer to Johnson and Kotz (1972, p. 134). If we assume that
ΣΣΣΣΣ–1 exists, then the joint pdf of T1, ..., Tp, distributed as Mtp(ν, ΣΣΣΣΣ), is given by
with t’ = (t1, ..., tp). The derivation of this joint pdf is left as the Exercise
4.6.11.
Fundamental works in this area include references to Cornish (1954, 1962),
Dunnett (1955), and Dunnett and Sobel (1954, 1955), among others. One
may also refer to Johnson and Kotz (1972) and Tong (1990) to find other
sources.
Example 4.6.6 Suppose that Ui1, ..., Uini are iid N(µi, σ2), i = 1, 2, 3 as in
the setup of a one-way analysis of variance. Let the three treatments be inde-
pendent too. From the ith treatment, we obtain the sample mean 
 and the
sample variance 
. Since the population variances are all equal to
σ2, we obtain the pooled sample variance
Let 
. Then,
(X1, X2) is distributed as N2(0, ΣΣΣΣΣ) with 
 where σ11 =
σ22 = 1 and σ12 = σ21 = 
. Also,
 is distributed as a Chi-square random vari-
able with the degree of freedom ν = n1 + n2 + n3 – 3. Next, the random
variable Y and the random vector (X1, X2) are independently distributed.
Thus, with 
, i = 1, 2, the corresponding random vector
(T1, T2) is distributed as Mt2(ν, ΣΣΣΣΣ). !
4.6.3   The F Distribution
This distribution also arises frequently in the areas of analysis of variance,
multiple comparisons, and selection and ranking. Suppose that we have

220
4. Functions of Random Variables and Sampling Distribution
independent random variables X0, X1, ..., Xp where Xi is distributed as
, i = 0, 1, ..., p. This is often the situation in the case of analysis of
variance, where these X’s customarily stand for the sums of squares due to
the error and treatments. We denote the new random variables
The marginal distribution of the random variable Ui is F νi, ν0. This follows
from the definition of a F random variable. Jointly, however, (U1, ..., Up) is
said to have the p-dimensional F distribution, denoted by MFp(ν0, ν1, ..., νp).
One may refer to Johnson and Kotz (1972, p. 240). The joint pdf of U1, ...,
Up, is distributed as MFp(ν0, ν1, ..., νp), is given by
with u’ = (u1, ..., up) and 
. The derivation of this joint pdf is left
as the Exercise 4.6.13.
Fundamental works in this area include references to Finney (1941) and
Kimbal (1951), among others. One may also refer to Johnson and Kotz (1972)
and Tong (1990) to find other sources.
4.7 Importance of Independence in Sampling
Distributions
In the reproductive property of the normal distributions and Chi-square distri-
butions (Theorem 4.3.2, parts (i) and (iii)) or in the definitions of the Student’s
t and F distributions, we assumed independence among various random vari-
ables. Now, we address the following important question. If those indepen-
dence assumptions are violated, would one have similar standard distributions
in the end? In general, the answer is “no” in many circumstances. In this
section, some of the ramifications are discussed in a very simple way.
4.7.1   Reproductivity of Normal Distributions
In the Theorem 4.3.2, part (i), we learned that independent normal variables
add up to another normal random variable. According to the Definition
4.6.1, as long as (X1, ..., Xn) is distributed as the n-dimensional normal Nn,
this reproductive property continues to hold. But, as soon as we stray

4. Functions of Random Variables and Sampling Distribution
221
away from the structure of the multivariate normality for (X1, ..., Xn), the
marginals of X1, ..., Xn can each be normal in a particular case, but their sum
need not be distributed as a normal variable. We recall an earlier example.
Example 4.7.1 (Example 3.6.3 Continued) Let us rewrite the bivariate
normal pdf given in (3.6.1) as 
. Next, consider any
arbitrary 0 < α, ρ < 1 and fix them. We recall that we had
for –∞ < x1, x2 ∞. Here, (X1, X2) has a bivariate non-normal distribution, but
marginally both X1, X2 have the standard normal distribution. However one
can show that X1 + X2 is not a univariate normal variable. These claims are
left as the Exercises 4.7.1.-4.7.2. !
4.7.2   Reproductivity of Chi-square Distributions
Suppose that X1 and X2 are independent random variables which are respec-
tively distributed as 
 and 
. From the Theorem 4.3.2, part (iii), we can
then claim that X1 + X2 will be distributed as 
. But, will the same con-
clusion necessarily hold if X1 and X2 are respectively distributed as 
 and
, but X1 and X2 are dependent? The following simple example shows that
when X1 and X2 are dependent, but X1 and X2 are both Chi-squares, then X1 +
X2 need not be Chi-square.
Example 4.7.2 Suppose that (U1, V1) is distributed as N2(0, 0, 1, 1, ρ).
We assume that –1 < ρ < 1. Let 
 and thus the marginal
distributions of X1, X2 are both 
. Let us investigate the distribution of Y =
X1 + X2. Observe that
where the joint distribution of (U1 + V1, U1 – V1) is actually N2, because any
linear function of U1 + V1, U1 – V1 is ultimately a linear function of U1 and V1,
and is thus univariate normal. This follows (Definition 4.6.1) from the fact
that (U1, V1) is assumed N2. Now, we can write
and hence from our earlier discussions (Theorem 3.7.1), it follows that
U1 + V1 and U1 – V1 are independent random variables. Also, U1 + V1 and
U1 – V1 are respectively distributed as N (0, 2(1 + ρ)) and N (0, 2(1 – ρ)).

222
4. Functions of Random Variables and Sampling Distribution
Thus, with 0 < t < min {(2(1 + ρ))–1, (2(1 – ρ))–1}, the mgf MY(t) of Y is
given by
Hence, we can express MY(t) as
which will coincide with (1 – 2t)–1, the mgf of 
, if and only if ρ = 0. We
have shown that Y is distributed as  
 if and only if ρ = 0, that is if and only
if X1, X2 are independent. !
We have not said anything yet about the difference of random variables
which are individually distributed as Chi-squares.
The difference of two Chi-square random variables may or may
not be another Chi-square variable. See the Example 4.7.3.
Example 4.7.3 Suppose that we have X1, X2, X3, X4 which are iid N(0, 1).
Let us denote 
. Since 
s are iid 
 i
= 1, 2, 3, 4, by the reproductive property of Chi-square distributions we claim
that U1 is distributed as 
 Obviously, U2 is distributed as 
 At the same
time, we have 
 which is distributed as 
 Here, the dif-
ference of two Chi-square random variables is distributed as another Chi-
square variable. Next, let us we denote 
 which is distributed as
 But, 
 and this random variable cannot have a
Chi-square distribution. We can give a compelling reason to support this con-
clusion. We note that U1 – U3 can take negative values with positive probabil-
ity. In order to validate this claim, we use the pdf of F2,1 and express
 as
which simplifies to 1 – 2–1/2 ≈ .29289. !
Let X1, ..., Xn be iid N(µ, µ2) and S2 be the sample variance.
Then, (n – 1)S2/σ2 is distributed as Chi-square. Is it possible
that (n – 1)S2/σ2 is a Chi-square random variable without
the assumed normality of the X’s? See the Exercise 4.7.4.

4. Functions of Random Variables and Sampling Distribution
223
4.7.3   The Student’s t Distribution
In the Definition 4.5.1, we introduced the Student’s t distribution. In defining
W, the Student’s t variable, how crucial was it for the standard normal vari-
able X and the Chi-square variable Y to be independent? The answer is: inde-
pendence between the numerator and denominator was indeed very crucial.
We elaborate this with the help of two examples.
Example 4.7.4 Suppose that Z stands for a standard normal variable and
we let X = Z, Y = Z2. Clearly, X is N(0, 1) and Y is 
 but they are dependent
random variables. Along the line of construction of the random variable W, let
us write 
 and it is clear that W can take only two
values, namely –1 and 1, each with probability 1/2 since P(X > 0) = P(X ≤ 0)
= 1/2. That is, W is a Bernoulli variable instead of being a t variable. Here, W
has a discrete distribution! !
Example 4.7.5 Let X1, X2 be independent, X1 being N(0, 1) and X2 being
. Define X = X1, 
, and obviously we have X2 < Y w.p. 1, so
that X and Y are indeed dependent random variables. Also by choice, X is N(0,
1) and Y is 
. Now, consider the Definition 4.5.1 again and write
so that we have | W |≤ (n + 1)1/2. A random variable W with such a restricted
domain space can not be distributed as the Student’s t variable. The domain
space of the t variable must be the real line, ℜ. Note, however, that the ran-
dom variable W in (4.7.4) has a continuous distribution unlike the scenario in
the previous Example 4.7.4. !
4.7.4   The F Distribution
In the Definition 4.5.2, the construction of the F random variable was ex-
plained. The question is whether the independence of the two Chi-squares,
namely X and Y, is essential in that definition. The answer is “yes” as the
following example shows.
Example 4.7.6 Let U1, U2 be independent, U1 be 
 and U2 be 
. Define
X = U1, Y = U1 + U2, and we note that Y > X w.p.1. Hence, obviously X,Y are
dependent random variables. Also, X is 
 and Y is 
. Now, we look at
the Definition 4.5.2 and express U as
so that we have 0 < U < (m + n)/m. A random variable U with such a
restricted domain space cannot have F distribution. The domain space

224
4. Functions of Random Variables and Sampling Distribution
of F variable must be ℜ+. Note, however, that the random variable U in (4.7.5)
has a continuous distribution.
4.8   Selected Review in Matrices and Vectors
We briefly summarize some useful notions involving matrices and vectors
made up of real numbers alone. Suppose that Am×n is such a matrix having m
rows and n columns. Sometimes, we rewrite A as (a1, ..., an) where ai stands
for the ith column vector of A, i = 1, ..., n.
The transpose of A, denoted by A’, stands for the matrix whose rows
consist of the columns a1, ..., an of A. In other words, in the matrix A’, the
row vectors are respectively a1, ..., an. When m = n, we say that A is a square
matrix.
Each entry in a matrix is assumed to be a real number.
The determinant of a square matrix An×n is denoted by det(A). If we have
two square matrices An×n and Bn×n, then one has
The rank of a matrix Am×n = (a1, ..., an), denoted by R(A), stands for the
maximum number of linearly independent vectors among a1, ..., an. It can be
shown that
A square matrix An×n = (a1, ..., an) is called non-singular or full rank if
and only if R(A) = n, that is all its column vectors a1, ..., an are linearly
independent. In other words, An×n is non-singular or full rank if and only if the
column vectors a1, ..., an form a minimal generator of ℜn.
A square matrix Bn×n is called an inverse of An×n if and only if AB = BA =
In×n, the identity matrix. The inverse matrix of An×n, if it exists, is unique and
it is customarily denoted by A–1. It may be worthwhile to recall the following
result.
For a matrix An×n: A–1 exists ⇔ R(A) = n ⇔ det(A) ≠ 0.
For a 2 × 2 matrix 
, one has det(A) = a11a22 – a12a21.
Let us suppose that a11a22 ≠ a12a21, that is det(A) ≠ 0. In this situation, the
inverse matrix can be easily found. One has:

4. Functions of Random Variables and Sampling Distribution
225
A square matrix An×n is called orthogonal if and only if A’, the transpose of
A, is the inverse of A. If An×n is orthogonal then it can be checked that det(A)
= ±1.
A square matrix A = (aij)n×n is called symmetric if and only if A’ = A, that
is aij = aji, 1 ≤ i ≠ j ≤ n.
Let A = (aij, i,j = 1, ..., n, be a n×n matrix. Denote the l×l sub-matrix Bl =
(apq), p, q = 1, ..., l,l = 1, ..., n. The lth principal minor is defined as the
det(Bl), l = 1, ..., n.
For a symmetric matrix An×n, an expression such as x’ Ax with x ∈ ℜn is
customarily called a quadratic form.
A symmetric matrix An×n is called positive semi definite (p.s.d.) if (a) the
quadratic form x’ Ax ≥ 0 for all x ∈ ℜk, and (b) the quadratic form x’ Ax =0
for some non-zero x ∈ ℜn.
A symmetric matrix An×n is called positive definite (p.d.) if (a) the qua-
dratic form x’ Ax ≥ 0 for all x ∈ ℜn, and (b) the quadratic form x’ Ax =0 if
and only if x = 0.
A symmetric matrix An×n is called negative definite (n.d.) if (a) the qua-
dratic form x’ Ax ≤0 for all x ∈ ℜn, and (b) the quadratic form x’ Ax =0 if and
only if x = 0. In other words, a symmetric matrix An×n is n.d. if and only if –
A is positive definite.
Let us now look at some partitioned matrices A, B where
where m, n, u, v, w, t, c, d are all positive integers such that u + w = m, u + t
= n and c + d = q. Then, one has

226
4. Functions of Random Variables and Sampling Distribution
Again, let us consider a partitioned matrix A from (4.8.7) where Pu×u and
Sw×w are square matrices where u + w = n. Then, one has
Let us reconsider the partitioned matrix An×n which was used in (4.8.9). Then,
we can write
Next, we summarize an important tool which helps us to find the maxi-
mum or minimum of a real valued function of two variables. The result
follows:
Suppose that f(x) is a real valued function of a two-dimensional variable x
= (x1, x2) ∈ (a1, b1) × (a2, b2) ⊆ ℜ2, having continuous second-order partial
derivatives 
 and 
 everywhere
in an open rectangle (a1, b1) × (a2, b2). Suppose also that for some point ξ =
(ξ1, ξ2) ∈ (a1, b1) × (a2, b2), one has
Now, let us denote the matrix of the second-order partial derivatives
Then,

4. Functions of Random Variables and Sampling Distribution
227
4.9   Exercises and Complements
4.1.1 (Example 4.1.1 Continued) Suppose that Z1, Z2 are iid standard
normal variables. Evaluate 
 with some fixed but arbitrary b >
0. {Hint: Express the required probability as the double integral,
. Use the substitutions from (4.1.1)
to rewrite this as  
 which will change to
. This last integral can then be re-
placed by 
Also, look at the Exercise 4.5.7.}
4.2.1 (Example 4.2.1 Continued) Consider the two random variables X1,
X2 from the Example 4.2.1. Find the pmf’s of the following random variables
which are functions of X1, X2.
4.2.2 Let X1, X2 be iid Poisson(λ), λ > 0. Derive the distribution of the
random variable U = X1 + X2. Then, use mathematical induction to show that
 has the Poisson(nλ) distribution if X1, ..., Xn are iid Poisson(λ).
Next, evaluate P(X1 – X2 = v) explicitly when v = 0, 1, 2, 3. Proceeding this
way, is it possible to write down the pmf of the random variable V = X1 – X2?
{Hint: In the first part, follow along the Examples 4.2.2-4.2.3.}
4.2.3 In a Bernoulli experiment, suppose that X1 = number of trials needed to
observe the first success, and X2 = number of trials needed since the first
success to observe the second success, where the trials are assumed indepen-
dent having the common success probability p, 0 < p < 1. That is, X1, X2 are
assumed iid having the Geometric(p) distribution defined earlier in (1.7.7). First,
find the distribution of the random variable U = X1 + X2. Next, with k such iid
random variables X1, ..., Xk, derive the pmf of the random variable
. The distribution of U is called Negative Binomial with param-
eters (k, p). A different parameterization was given in (1.7.9).
4.2.4 Let the pdf of X be 
 with β > 0. Find the
pdf’s of the following random variables which are functions of X.
(i)
U = X2;
(ii)
V = X3.

228
4. Functions of Random Variables and Sampling Distribution
4.2.5 In general, suppose that X1, ..., Xn are iid continuous random variables with the
common pdf and df respectively given by f(x) and F(x). Derive the joint pdf of the ith
order statistic Yi = Xn:i and the jth order statistic Yj = Xn:j, 1 ≤ i < j ≤ n by solving the
following 
parts 
in 
the 
order 
they 
are 
given. 
Define
 u2), U3 = n – U1 – U2. Now, show
that
(i)
(U1, U2, U3) is distributed as multinomial with k = 3, p1 = F(u1),
p2 = F(u2) – F(u1), and p3 = 1 – p1 – p2 = 1 – F(u2);
(ii)
P{Yi ≤ u1 ∩ Yj ≤ u2} = P{U1 ≥ i ∩ (U1 + U2) ≥ j};
(iii)
U2 = l} + P{U ≥ j};
(iv)
the expression in part (iii) can be rewritten as
(v)
the joint pdf of Yi, Yj, denoted by f(yi, yj), can be directly
obtained by evaluating 
 first,
and then simplifying it.
4.2.6 Verify (4.2.7). {Hint: Use the Exercise 4.2.5.}
4.2.7 Suppose that X1, ..., Xn are iid uniform random variables on the
interval (0, θ), θ ∈ ℜ. Denote the ith order statistic Yi = Xn:i where Xn:1 = Xn:2
≤ ... ≤ Xn:n. Define U = Yn – Y1, V = 1/2(Yn + Y1) where U and V are
respectively referred to as the range and midrange for the data X1, ..., Xn.
This exercise shows how to derive the pdf of the range, U.
(i)
Show that the joint pdf of Y1 and Yn is given by h(y1, yn) = n(n –
1)θ–n (yn – y1)n–2 I(0 < y1 < yn < θ). {Hint: Refer to the equation
(4.2.7).};
(ii)
Transform (Y1, Yn) to (U, V) where U = Yn – Y1, V = 1/2(Yn +
Y1). Show that the joint pdf of (U, V) is given by g(u, v) = n(n – 1)
θ–n un–2 when 0 < u < θ, 1/2u < v < θ – 1/2u, and zero otherwise;
(iii)
Show that the pdf of U is given by g1(u) = n(n – 1)θ–n × (θ – u)
un–2 I(0 < u < θ). {Hint: g1(u) = 
,
for 0 < u < θ.};
(iv)
When θ = 1, does the pdf of the range, U, derived in part (iii)
correspond to that of one of the standard random variables from
the Section 1.7?
4.2.8 Suppose that X1, ..., Xn are iid uniform random variables on the
interval (θ, θ + 1), θ ∈ ℜ. Denote the ith order statistic Yi = Xn:i where

4. Functions of Random Variables and Sampling Distribution
229
Xn:1 ≤ Xn:2 ≤ ... ≤ Xn:n. Define U = Yn – Y1, V = 1/2(Yn + Y1) where recall that
U and V are respectively referred to as the range and midrange for the data
X1, ..., Xn. This exercise shows how to derive the pdf of the range, U.
(i)
Show that the joint pdf of Y1 and Yn is given by h(y1, yn) = n(n –
1)(yn – y1)n–2 I(θ < y1 < yn < θ + 1). {Hint: Refer to the equation
(4.2.7).};
(ii)
Transform (Y1, Yn) to (U, V) where U = Yn – Y1, V = 1/2(Yn +
Y1). Show that the joint pdf of (U, V) is given by g(u, v) = n(n
– 1)un–2 when 0 < u < 1, θ + 1/2u < v < θ + 1 – 1/2u, and zero
otherwise;
(iii)
Show that the pdf of U is given by g1(u) = n(n – 1)×(1 – u)
un–2 I(0 < u < 1). {Hint:g1(u)=
,for 0<u< 1.};
(iv)
Does the pdf of the range, U, derived in part (iii) correspond to
that of one of the standard random variables from the Section
1.7?
4.2.9 (Triangular Distribution) Suppose that X1 and X2 are independent
random variables, distributed uniformly on the interval (0, 1) with the respec-
tive pdf’s f1(x1) = I(0 < x1 < 1) and f2(x2) = I(0 < x2 < 1). Show that the pdf
of U = X1 + X2 is given by
The random variable U is said to follow the triangular distribution. Look at
the plot of g(u) in the Figure 4.9.1. {Hint: It is obvious that the pdf g(u) of

230
4. Functions of Random Variables and Sampling Distribution
U will be zero when u ≤ 0 or u ≥ 2, but it will be positive when 0 < u < 2.
Thus, using the convolution result, for 0 < u < 1, we can write the df of U as
. Hence, for 0 < u
< 1 we have 
. On the other hand, for 1 ≤ u < 2, we can
write the df of U as 
 so that 
4.2.10 Suppose that X1 and X2 are independent random variables, distrib-
uted uniformly on the intervals (0, 1) and (0, 2) with the respective pdf’s
f1(x1) = I(0 < x1 < 1) and f2(x2) = 1/2I(0 < x2 < 2). Use the convolution result
to derive the form of the pdf g(u) of U = X1 + 1/2X2 with 0 < u < 2.
4.2.11 (Example 4.2.10 Continued) Let U, V be iid N(µ, σ2). Find the pdf
of W = U + V by the convolution approach. Repeat the exercise for the
random variable T = U – V.
4.2.12 (Example 4.2.11 Continued) Provide all the intermediate steps in the
Example 4.2.11.
4.2.13 Prove a version of the Theorem 4.2.1 for the product, V = X1X2, of
two independent random variables X1 and X2 with their respective pdf’s given
by f1(x1) and f2(x2). Show that the pdf of V is given by h(v) =
 for v ∈ ℜ. Show that h(v) can be equivalently
written as 
. {Caution: Assume that V can not take
the value zero.}
4.2.14 Suppose that X1, X2, X3 are iid uniform random variables on the
interval (0, 1).
(i)
Find the joint pdf of X3:1, X3:2, X3:3;
(ii)
Derive the pdf of the median, X3:2;
(iii)
Derive the pdf of the range, X3:3 – X3:1.
{Hint: Use the Exercise 4.2.7}
4.2.15 Suppose that X1, ..., Xn are iid N(0, 1). Let us denote Yi = 4Φ(Xi)
where 
.
(i)
Find the pdf of the random variable 
(ii) Evaluate E[U] and V[U];
(iii) Find the marginal pdf’s of Yn:1, Yn:n and Yn:n – Yn:1.
4.2.16 Suppose that X1 is uniform on the interval (0, 2), X2 has its pdf 1/
2xI(0 < x < 2), and X3 has its pdf 3/8x2I(0 < x < 2). Suppose also that X1, X2,
X3 are independent. Derive the marginal pdf’s of X3:1 and X3:3. {Hint: Follow
along the Example 4.2.8.}

4. Functions of Random Variables and Sampling Distribution
231
4.2.17 Suppose that Z is the standard normal random variable. Denote X =
max{|Z|, 1/|Z|}.
(i)
Find the expression of P{X ≤ x} for x ≥ 1. Observe that P{X ≤
x} = 0 for x < 1;
(ii)
Use part (i) to show that the pdf of X is given by f(x) = 2{φ(x)
+ x–2φ(x–1)}I(x ≥ 1).
{Hint: For x = 1, one can write P{X ≤ x} = P{x–1 ≤ |Z|≤ x} = 2P{x–1 ≤ Z =
x} = 2{Φ(x) – Φ(x–1)}. This is part (i). Differentiating this expression with
respect to x will lead to part (ii).}
4.2.18 (Exercise 4.2.9 Continued) Suppose that X1, X2 and X3 are inde-
pendent random variables, distributed uniformly on the interval (0, 1) with the
respective pdf’s f1(x1) = I(0 < x1 < 1), f2(x2) = I(0 < x2 < 1) and f3(x3) = I(0
< x3 < 1). Use the convolution result to derive the form of the pdf h(v) of V =
X1 + X2 + X3 with 0 < v < 3. {Hint: Note that V = U + X3 where U = X1 + X2
and its pdf g(u) was derived in the Exercise 4.2.9 for 0 < u < 2. Now, use the
convolution result again.}
4.2.19 (Example 4.2.9 Continued) Suppose that X1, X2 and X3 are inde-
pendent random variables, distributed exponentially with the respective pdf’s
f1(x1) = e–x
1I(x1 ∈ ℜ+), f2(x2) = e–x
2I(x2 ∈ ℜ+) and f3(x3) = e–x
3I(x3 ∈ ℜ+). Use
the convolution result to show that the pdf h(v) of V = X1 + X2 + X3 with v ∈
ℜ+ matches with that of the Gamma(3, 1) distribution. {Hint: Note that V = U
+ X3 where U = X1 + X2 and its pdf g(u) was derived in the Example 4.2.9 for
u ∈ ℜ+. Now, use the convolution result again.}
4.2.20 Suppose that X1, ..., Xn are iid Uniform(0, 1) random variables
where n = 2m + 1 for some positive integer m. Consider the sample median,
that is U = Xn:m+1 which is the order statistic in the middle.
(i)
Show that the pdf of U is given by g(u) = cum(1 – u)m × I(0 < u
< 1) where c = (2m + 1)!/(m!)2. Is this one of the standard
distributions listed in the Section 1.7?
(ii)
Show that E(U) = 1/2 and 
.
4.3.1 Suppose that X1, ..., Xn are iid Poisson(λ). Use the mgf technique to
show that 
 has the Poisson(nλ) distribution. {Hint: Follow along
the Examples 4.3.1-4.3.2.}
4.3.2 Suppose that X1, ..., Xk are iid Geometric(p), 0 < p < 1. Find the
distribution of 
. The distribution of U is called Negative Binomial
with parameters (k, p). A different parameterization was given in (1.7.9).
4.3.3 Complete the arguments in the Example 4.3.5.

232
4. Functions of Random Variables and Sampling Distribution
4.3.4 Consider the two related questions.
(i)
Suppose that a random variable U has its pdf given by g(u) =
1-2σe–|u|/σI(–∞ < u < ∞). First show that the mgf of U is given by
(1– σ2t2)–1 for |t|< σ–1, with 0 < σ < ∞;
(ii)
Next, suppose that X1, X2 are iid with the common pdf f(x) ,
 with β > 0. Using part (i), derive the pdf
of V =  X1 – X2.
4.3.5 Let Y1, Y2 be iid N (0, 4). Use the mgf technique to derive the distri-
bution of 
. Then, obtain the exact algebraic expressions for
(i)
P(U > 1);
(ii)
P(1 < U < 2);
(iii)
P(| U – 2 |≥ 2.3).
4.3.6 Suppose that Z is the standard normal variable. Derive the expres-
sion of E[exp(tZ2)] directly by integration. Hence, obtain the distribution of
Z2.
4.3.7 (Exercise 4.3.6 Continued) Suppose that the random variable X is
distributed as N(µ, σ2), µ ∈ ℜ, σ ∈ ℜ+. Obtain the expression of the mgf of
the random variable X2, that is E[exp(tX2)].
4.3.8 Prove the Theorem 4.3.2.
4.3.9 Suppose that U1, ..., Un are iid random variables with the common
pdf given by 
. Then, use the
part (ii) or (iii) from the Theorem 4.3.2 to obtain the distribution of the ran-
dom variable 
.
4.4.1 Let X be a random variable with the pdf
Find the marginal pdf’s of U, V, and W defined as follows:
(i) U = 2X – 1; (ii) V = X2; (iii) 
4.4.2 Suppose that a random variable X has the Raleigh density given by
Derive the pdf of U = X2.
4.4.3 (Example 4.4.3 Continued) Suppose that Z has the standard nor-
mal distribution. Let us denote Y = |Z|. Find the pdf of Y. {Caution: The

4. Functions of Random Variables and Sampling Distribution
233
transformation from Z to Y is not one-to-one. Follow along the Example 4.4.3.}
4.4.4 (Example 4.4.3 Continued) Suppose that Z has the standard normal
distribution. Let us denote Y = |Z|3. Find the pdf of Y. {Caution: The transfor-
mation from Z to Y is not one-to-one. Follow along the Example 4.4.3.}
4.4.5 (Example 4.4.4 Continued) Suppose that X has the pdf f(x) = 1/
2exp{– |x|}I(x ∈ ℜ). Obtain the pdf of Y = |X|3. {Caution: The transformation
from X to Y is not one-to-one. Follow along the Example 4.4.4.}
4.4.6 (Example 4.4.7 Continued) Verify all the details.
4.4.7 (Example 4.4.8 Continued) Verify all the details.
4.4.8 Suppose that X1, ..., Xn are iid Uniform(0, 1). Let us define Yi = Xn:i/
Xn:i–1, i = 1, ..., n with Xn:0 = 1. Find the joint distribution of Y2, ..., Yn. {Hint:
First, consider taking log and then use the results from Examples 4.4.2 and
4.4.10.}
4.4.9 (Example 4.4.9 Continued) Suppose that X1, ..., Xn are iid N(µ, σ2)
with n ≥ 2. Use the Helmert variables Y1, ..., Yn from the Example 4.4.9 to
show that (
 – 1)2 and S + S2/3 are independently distributed.
4.4.10 Suppose that (U, V) has the following joint pdf:
where θ > 0. Define X = UV and Y = U/V.
(i)
Find the joint pdf of (X, Y);
(ii)
Find the marginal pdf’s of X, Y;
(iii)
Find the conditional pdf of Y given X = x.
4.4.11 Let X1, X2 be iid having the Gamma(α, β) distribution with α > 0, β
> 0. Let us denote U = X1 + X2, V = X2/X1. Find the marginal distributions of
U, V.
4.4.12 Let X1, X2 be iid N(µ, σ2). Define U = X1 + X2, V = X1 – X2.
(i)
Find the joint pdf of (U, V);
(ii)
Evaluate the correlation coefficient between U and V;
(iii)
Evaluate the following: E{(X1 – X2)2 &pipe; X1 + X2 = x}; E{(X1
+ X2)2 | X1 = X2}; V{(X1 – X2)2 | X1 + X2 = x}.
4.4.13 Suppose that X has the following pdf with –∞ < µ < ∞, 0 < µ < ∞:

234
4. Functions of Random Variables and Sampling Distribution
One will recall from (1.7.27) that this pdf is known as the lognormal density
and the corresponding X is called a lognormal random variable. Suppose that
X1, X2 are iid having the common pdf f(x). Let r and s be arbitrary, but fixed
real numbers. Then, find the pdf of the random variable 
 {Hint: Does
taking log help?}
4.4.14 Suppose that X1, X2, X3 are iid Gamma(α, β), α > 0, β > 0. Define
U1 = X1 + X2 + X3, U2 = X2/(X1 + X2), and U3 = X3/(X1 + X2 + X3). Solve the
following problems.
(i)
Show that one can express x1 = u1(1 – u2)(1 – u3), x2 = u1u2(1 –
u3), x3. Is this transformation one-to-one?
(ii)
Determine the matrix J from (4.4.3) and show that
| det(J) | = 
;
(iii)
Start out with the joint pdf of X1, X2, X3. Use the transformation
(x1, x2, x3) → (u1, u2, u3) to obtain the joint pdf of U1, U2, U3;
(iv)
Argue that U1, U2, U3 are distributed independently. Show that
(a) U1 is distributed as Gamma(3α, β), (b) U2 is distributed as
Beta(α, α), and (c) U3 is distributed as Beta(α, 2α). In this
part, recall the beta function and the Beta pdf from (1.6.25)-
(1.6.26) and (1.7.35) respectively.
4.4.15 Suppose that X1, X2, X3, X4 are iid Gamma(α, β), α > 0, β > 0.
Define U1 = X1 + X2 + X3 + X4, U2 = X2/(X1 + X2), U3 = X3/(X1 + X2 + X3),
and U4 = X4/(X1 + X2 + X3 + X4). Solve the following problems.
(i)
Show that one can express x1 = u1(1 – u2)(1 – u3)(1 – u4), x2 =
u1u2(1 – u3)(1 – u4), x3 = u1u3(1 – u4), x4 = u1u4. Is this transfor
mation one-to-one?
(ii)
Determine the matrix J from (4.4.3) and show that | det(J)
| = u1(1 – u2)(1 – u3)(1 – u4);
(iii)
Start out with the joint pdf of X1, X2, X3, X4. Use the transforma
tion (x1, x2, x3, x4) →u1, u2, u3, u4) to obtain the joint pdf of U1,
U2, U3, U4;
(iv)
Argue that U1, U2, U3, U4 are distributed independently Show
that (a) U1 is distributed as Gamma(4α, β), (b) U2 is distributed
as Beta(α, α), (c) U3 is distributed as Beta(α, 2α), and (d) U4 is
Beta(α, 3α). Recall the beta function and Beta pdf from (1.6.25)-
(1.6.26) and (1.7.35) respectively.
4.4.16 (Example 4.4.13 Continued) Provide all the details in the Example
4.4.13.
4.4.17 (Example 4.4.14 Continued) Provide all the details in the Example
4.4.14.

4. Functions of Random Variables and Sampling Distribution
235
4.4.18 (Example 4.4.16 Continued) Suppose that (X1, X2) is distributed as
N2(0, 0, σ2, σ2, ρ) with 0 < σ < ∞, –1 < ρ < 1. Define U = X1/X2 and V = X1.
(i)
Find the joint pdf of U and V;
(ii)
In part (i), integrate out V and derive the marginal pdf of U;
(iii)
When ρ = 0, the pdf in part (ii) coincides with the standard
Cauchy pdf, namely, π–1(1 + u2)–1 for –∞ < u < ∞
(iv)
When ρ ≠ 0, the pdf in part (ii) coincides with the Cauchy pdf
with appropriate location and scale depending on ρ.
4.4.19 Suppose that (X1, X2) has the following joint pdf:
Find the pdf of U = X1 – X2. {Hint: Use the transformation u = x1 – x2 and v
= x1.}
4.4.20 Suppose that (X1, X2) has the following joint pdf:
Show that the pdf of 
 is given by
4.4.21 Suppose that (X1, X2) has the following joint pdf:
Let U = X1 + X2 and V = X1X2.
(i) Show that the pdf of U is a(u) 
(ii)Show that the pdf of V is b(v) 
4.4.22 Suppose that X1, X2 are iid random variables. We are told that X1 +
X2 and X1 – X2 are independently distributed. Show that the common pdf of
X1, X2 must be same as the normal density. {Hint: Can the Remark 4.4.4 be
used to solve this problem?}
4.4.23 Suppose that X1, X2, X3 are three iid random variables. We
are also told that X1 + X2 + X3 and (X1 – X2, X1 – X3) are independently

236
4. Functions of Random Variables and Sampling Distribution
distributed. Show that the common pdf of X1, X2, X3 must be the normal pdf.
{Hint: Can the Remark 4.4.4 be used to solve this problem?}
4.5.1 In the Theorem 4.5.1, provide all the details in the derivation of the
pdf of tí, the Student’s t with ν degrees of freedom.
4.5.2 Consider the pdf h(w) of tí, the Student’s t with ν degrees of free-
dom, for w ∈ ℜ. In the case ν = 1, show that h(w) reduces to 
 the
Cauchy density.
4.5.3 Generalize the result given in the Example 4.5.2 in the case of the k-
sample problem with k(≥ 3).
4.5.4 With n(≥ 4) iid random variables from N(0, σ2), consider the Helmert
variables Y1, ..., Y4 from Example 4.4.9. Hence, obtain the distribution
of 
 and that of W2?
4.5.5 Suppose that X1, ..., Xn are iid random variables having a common
pdf given by f(x) = σ–1 exp{–(x – µ)/σ}I(x > µ) for – ∞ < µ < ∞, 0 < σ < ∞.
Here, µ is the location parameter and ó is the scale parameter. Consider (4.4.18)
and let U = n(Xn:1 – µ) / σ, V = 2T / σ.
(i)
Find the pdf of U and E(Ur) for all fixed numbers r > 0;
(ii)
Evaluate E(Vr) for all fixed real numbers r;
(iii)
What is the distribution of W = σU ÷ [T/(n – 1)]?
{Caution: In parts (i)-(ii), watch for the condition on n as needed.}
4.5.6 Suppose that X1, ..., Xn are iid N(µ, σ2). Recall the Helmert variables
Y1, ..., Yn from the Example 4.4.9.
(i)
Find the pdf of Yi/Yj for i ≠ j = 2, ..., n;
(ii)
Find the pdf of Yi/ |Yj| for i ≠ j = 2, ..., n;
(iii)
Find the pdf of Yi
2 / Yj
2 for i ≠ j = 2, ..., n;
(iv)
Find the pdf of Yi
2 / T where 
(v)
Find the pdf of 
/U where 
for i ≠ j = 2, ..., n.
{Hint: Recall the distributional properties of the Helmert variables.}
4.5.7 (Exercise 4.1.1 Continued) Let Z1, Z2 be iid standard normal. From
the Exercise 4.5.2, note that the random variable Z1/Z2, denoted by X, has
the Cauchy pdf given in (1.7.31). Use this sampling distribution to evaluate
 where b(> 0) is fixed but arbitrary. {Hint: Express the re-
quired probability as 
 and then integrate the Cauchy pdf
within the appropriate interval. Does this match with the answer given for the
Exercise 4.1.1?}
4.5.8 (Exercise 4.5.7 Continued) Let Z1, Z2 be iid standard normal.
Evaluate 
 where c is a fixed but arbitrary real number.

4. Functions of Random Variables and Sampling Distribution
237
4.5.9 (Exercise 4.5.8 Continued) Let Z1, Z2 be iid standard normal. Evalu-
ate  
 where 0 < c < 1 is a fixed but arbitrary number.
4.5.10 Verify the expression of the variance of W, which is distributed as
tí, given in (4.5.4).
4.5.11 Verify the expression of the mean and variance of U, which is
distributed as Fí1,í2, given in (4.5.11)-(4.5.12).
4.6.1 (Example 4.6.1 Continued) Use transformations directly to show
that (X1, 
) is distributed as 
 when X1, ..., Xn are iid
N(µ, σ2).
4.6.2 (Example 4.6.2 Continued) Use transformations directly to find the
bivariate normal distribution of (aX1 + bX2, 
) when X1, ..., Xn are iid N(µ
σ2). Here, a and b are fixed non-zero real numbers.
4.6.3 Show that the bivariate normal density from (3.6.1) can be expressed
in the form given by (4.6.1).
4.6.4 Verify the properties given in (4.6.2)-(4.6.3) for the multivariate nor-
mal distribution.
4.6.5 Let X1, ..., Xn be iid N(µ, σ2). Then, find the joint distributions of
(i)
Y2, ..., Yn where Yi = Xi – X1, i = 2, ..., n;
(ii)
U2, ..., Un where Ui = Xi – Xi–1, i = 2, ..., n.
{Hint: Use the Definition 4.6.1 for the multivariate normality.}
4.6.6 (Exercise 4.6.5 Continued) Solve the Exercise 4.6.5 by using direct
transformation techniques.
4.6.7 Suppose that X1, ..., Xn are iid N(µ, σ2), n ≥ 2. Use the variables U1,
..., Un where 
(i)
Show that U = (U1, ..., Un) has the n-dimensional normal distri
bution;
(ii)
Show that U1 and (U2, ..., Un) are independent;
(iii)
Use parts (i)-(ii) to derive the distribution of 
;
(iv)
Express the sample variance S2 as a function of U2, ..., Un alone.
Hence, show 
 that and S2 are independently distributed.
{Hint: Use the Definition 4.6.1 for the multivariate normality to solve part (i).
Also, observe that 
 can be rewritten as 
4.6.8 Suppose that (X1, Y1), ..., (Xn, Yn) are iid 
with –∞ < µ1, µ2 < ∞, 0 < 
 < ∞ and –1 < ρ < 1, n ≥ 2. Then,

238
4. Functions of Random Variables and Sampling Distribution
by appealing to the Definition 4.6.1 of a multivariate normal distribution, show
that
  is distributed as 
 with some ρ*.
{Hint: Look at (4.6.6).}
4.6.9 Suppose that (X1, Y1), ..., (Xn, Yn) are iid  
 with
 and –1 < ρ < 1, n ≥ 3. Consider the
Pearson correlation coefficient r defined by (4.6.7). The pdf of r when ρ = 0
is given by
where 
 Use transformation tech-
niques to derive the following result:
4.6.10 Suppose that X1, ..., Xn are iid N2(µ, ΣΣΣΣΣ) where µ ∈ ℜ2 and ΣΣΣΣΣ is a 2
× 2 p.d. matrix, n ≥ 2. Let us denote 
 and
 Prove the following sampling distributions:
(i)
 is distributed as N2(µ, n–1 ΣΣΣΣΣ);
(ii)
n(
 – µ)′ ΣΣΣΣΣ–1 (
 – µ) is distributed as 
.
{Hint: Apply the Definition 4.6.1 in part (i). To prove part (ii), without any
loss of generality, assume that n = 1 and µµµµµ = 0. Now, show that n(
 – µ)′ ΣΣΣΣΣ–
1 (
 – µ) can be written as 
 which is further split
up 
 as . At this stage, can the re-
productive property of Chi-squares be applied?}
4.6.11 Suppose that the random vector X, where X′ = (X1, ..., Xp), has a p-
dimensional normal distribution with the mean vector 0 and the p × p p.d. disper-
sion matrix ΣΣΣΣΣ, denoted by Np(0, ΣΣΣΣΣ). We assume that each diagonal entry in ΣΣΣΣΣ is 1.
In other words, the (i, j)th entry in the matrix ΣΣΣΣΣ corresponds to the population
correlation coefficient ρij between the variables Xi, Xj, 1≤ i ≠ j = p. Suppose
that Y is a positive real valued random variable distributed as χν
2. It is also
assumed that Y and (X1, ..., Xp) are independent. Let us denote p new random
variables Ti = Xi ÷ (Y/ν)1/2, i = 1, ..., p. Jointly, however, (T1, ..., Tp) is said to
have the p-dimensional t distribution which depends on the correlation matrix ΣΣΣΣΣ,
denoted by Mtp(ν, ΣΣΣΣΣ). Assume that ΣΣΣΣΣ–1 exists. Then, show that the joint

4. Functions of Random Variables and Sampling Distribution
239
pdf of T1, ..., Tp, distributed as Mtp(ν, ΣΣΣΣΣ), is given by
with t’ = (t1, ..., tp). {Hint: Start with the joint pdf of X1, ..., Xp and Y. From
this, find the joint pdf of T1, ..., Tp and Y by transforming the variables (x1, ...,
xp, y) to (t1, ..., tp, y). Then, integrate this latter joint pdf with respect to y.}
4.6.12 (Exercise 4.6.5 Continued) Let X1, ..., Xn be iid N(µ, σ2). Recall
that Yi = Xi – X1, i = 2, ..., n. Suppose that U is distributed as 
 and also
U is independent of the X’s. Find the joint pdf of Yi/U for i = 2, ..., n.
4.6.13 Consider the independent random variables X0, X1, ..., Xp where Xi
is distributed as 
. We denote the new random variables Ui =
(Xi/νi) ÷ (X0/ν0), i = 1, ..., p. Jointly, (U1, ..., Up) is said to have the p-
dimensional F distribution, denoted by MFp(ν0, ν1, ..., νp). Using transforma-
tion techniques, show that the joint pdf of U1, ..., Up, distributed as MFp (ν0,
ν1, ..., νp), is given by
with u’ = (u1, ..., up) and 
. {Hint: Start with the joint pdf of X0,
X1, ..., Xp. From this, find the joint pdf of U1, ..., Up and X0 by transforming
(x1, ..., xp, x0) to (u1, ..., up, x0). Then, integrate this latter pdf with respect to
x0.}
4.6.14 Suppose that (U1, U2) is distributed as N2(5, 15, 8, 8, ρ) for some
ρ ∈ (–1, 1). Let X1 = U1 + U2 and X2 = U1 – U2. Show that X1 and X2 are
independently distributed. {Hint: Is (X1, X2) jointly distributed as a bivariate
normal random vector?}
4.7.1 (Examples 3.6.3 and 4.7.1 Continued) Suppose that a pair of random
variables (X1, X2) has the joint pdf given by
with 0 < α < 1, where 
 stands for the bivariate nor-
mal pdf defined in (3.6.1) with means µ1, µ2, variances  
, and the
correlation coefficient ρ with 0 < ρ < 1. Show that X1 + X2 is not normally

240
4. Functions of Random Variables and Sampling Distribution
distributed. {Hint: Let u = x1 + x2, v = x1 – x2, and find the joint pdf of the
transformed random variable (U, V). Integrate this joint pdf with respect to v
and show that the marginal pdf of U does not match with the pdf of a normal
random variable.}
4.7.2 (Example 3.6.3 and Exercise 4.7.1 Continued) Suppose that a pair of
random variables (X1, X2) has the joint pdf given by
with 0 < α < 1, where f(x1, x2; ν, θ, σ2, T2, ρ) stands for the bivariate normal
pdf defined in (3.6.1) with 
 and 0 < ρ < 1.
Show that marginally X1 is N(µ, σ2), X2 is N(θ,T2), but X1 + X2 is not normally
distributed.
4.7.3 (Example 4.7.2 Continued) Provide the missing details in the Ex-
ample 4.7.2.
4.7.4 If X1, ..., Xn were iid N(µ, σ2) and 
 
was the sample variance, then we could claim that (n – 1)S2/σ2 was distrib-
uted as a Chi-square random variable with (n–1) degrees of freedom for n ≥
2. Does such a conclusion necessarily need the assumption of the normality
or the iid nature of the X sequence? In general there can be different kinds of
scenarios on hand. The following problems indicate some of the possibilities.
Think through each case carefully and summarize the story each situation
unfolds.
(i)
Consider Y0, Y1, ..., Yn which are iid N(0, 1), and define then Xi
= Yi + Y0, i = 1, ..., n. Obviously, 
 = 
 + Y0 where 
 and  
are respectively the sample means of X1, ..., Xn and Y1, ..., Yn.
Now, 
, which
would be distributed as a Chi-square random variable with
(n – 1) degrees of freedom. Note that X1, ..., Xn happen to
be normal variables, but these are dependent;
(ii)
In part (i), observe that Y0 can be allowed to have any arbitrary
distribution while on the other hand Y0 and (Y1, ..., Yn) need not
be independent either. Yet, the conclusion that (n – 1)S2 is dis
tributed as a Chi-square random variable with (n – 1) degrees of
freedom holds. Y0 can even be a discrete random variable!

5
Concepts of Stochastic
Convergence
5.1 Introduction
The concept of the convergence of a sequence of real numbers {an; n ≥ 1} to
another real number a as n → ∞ is well understood. Now, we are about to
define the notions of “convergence” of a sequence of real valued random
variables {Un; n ≥ 1} to some constant u or a random variable U, as n → ∞.
But, before we do so, let us adopt a minor change of notation. In the previous
chapters, we wrote 
, S2 and so on, because the sample size n was held
fixed. Instead, we will prefer writing 
 
 and so on, in order to make the
dependence on the sample size n more explicit.
First, let us see why we need to explore the concepts of stochastic con-
vergence. We may, for example, look at 
 or Xn:n to gather information
about the average or the record value (e.g. the average or record rainfall) in a
population. In a situation like this, we are looking at a sequence of random
variables {Un; n ≥ 1} where Un = 
 or Xn:n. Different probability calculations
would then involve n and the distribution of Un. Now, what can we say about
the sampling distribution of Un? It turns out that an exact answer for a simple-
minded question like this is nearly impossible to give under full generality.
When the population was normal or gamma, for example, we provided some
exact answers throughout Chapter 4 when Un = 
. On the other hand, the
distribution of Xn:n is fairly intractable for random samples from a normal or
gamma population. If the population distribution happened to be uniform, we
found the exact pdf of Xn:n in Chapter 4, but in this case the exact pdf of  
becomes too complicated even when n is three or four! In situations like
these, we may want to examine the behavior of the random variables 
or Xn:n, for example, when n becomes large so that we may come up with
useful approximations.
But then having to study the sequence {
; n ≥ 1} or {Xn:n; n ≥ 1}, for
example, is not the same as studying an infinite sequence of real numbers!
The sequences {
; n ≥ 1} or {Xn:n; n ≥ 1}, and in general {Un; n ≥ 1}, are
stochastic in nature.
This chapter introduces two fundamental concepts of convergence for a
sequence of real valued random variables {Un; n ≥ 1}. First, in the Section
241

242
5. Concepts of Stochastic Convergence
5.2, we discuss the notion of the convergence in probability (denoted by 
)
and the weak law of large numbers (WLLN). We start with a weak version of
the WLLN, referred to as the Weak WLLN, followed by a stronger version,
referred to as Khinchine’s WLLN. In the Section 5.3, we discuss the notion
of the convergence in distribution or law (denoted by ). We provide Slutsky’s
Theorem in Section 5.3 which sets some of the ground rules for manipula-
tions involving these modes of convergence. The central limit theorem (CLT)
is discussed in Section 5.3 for both the sample mean 
 and sample variance
. This chapter ends (Section 5.4) with some interesting large-sample prop-
erties of the Chi-square, t, and F distributions.
5.2 Convergence in Probability
Definition 5.2.1 Consider a sequence of real valued random variables {Un; n
≥ 1}. Then, Un is said to converge in probability to a real number u as n → ∞,
denoted by 
, if and only if the following condition holds:
In other words, 
 means this: the probability that Un will stay away
from u, even by a small margin e, can be made arbitrarily small for large
enough n, that is for all n ≥ n0 for some n0 ≡ n0(ε). The readers are familiar
with the notion of convergence of a sequence of real numbers {an; n ³ 1} to
another real number a, as n → ∞. In (5.2.1), for some fixed ε (> 0), let us
denote 
 which turns out to be a non-negative real
number. In order for Un to converge to u in probability, all we ask is that pn(ε)
> 0 as n → ∞, for all fixed ε(> 0).
Next we state another definition followed by some important results. Then,
we give a couple of examples.
Definition 5.2.2 A sequence of real valued random variables {Un; n ≥ 1} is
said to converge to another real valued random variable U in probability as n
→ ∞ if and only if  
 as n → ∞.
Now, we move to prove what is known as the weak law of large numbers
(WLLN). We note that different versions of WLLN are available. Let us begin
with one of the simplest versions of the WLLN. Later, we introduce a stron-
ger version of the same result. To set these two weak laws of large numbers
apart, the first one is referred to as the Weak WLLN.
Theorem 5.2.1 (Weak WLLN) Let X1, ..., Xn be iid real valued
random variables with E(X1) = µ and V(X1) = σ2, −∞ < µ < ∞,0 <

5. Concepts of Stochastic Convergence
243
σ < ∞. Write 
 for the sample mean. Then, 
 as n →
∞.
Proof Consider arbitrary but otherwise fixed ε (> 0) and use Tchebysheff’s
inequality (Theorem 3.9.3) to obtain
Thus, we have 0 ≤ P{| 
 – µ |≥ ε } ≤ σ2/(nε2) → 0 as n → ∞. Hence, P
{|
 – µ | ≥ε} → 0 as n → ∞ and by the Definition 5.2.1 one claims that
  !
Intuitively speaking, the Weak WLLN helps us to conclude that the sample
mean 
 of iid random variables may be expected to hang around the popu-
lation average µ with high probability, if the sample size n is large enough and
0 < σ < ∞.
Example 5.2.1 Let X1, ..., Xn be iid N(µ, σ2), –∞ < µ < ∞, 0 < σ < ∞, and write
 for n ≥ 2. Then, we show
that 
 as n→∞. From (4.4.9) recall that we can express 
 as
  where Y2,..., Yn are the Helmert variables. These Helmert variables
are iid N(0, σ2). Observe that 
 
 ,
which is finite. In other words, the sample variance 
 has the representation
of a sample mean of iid random variables with a finite variance. Thus, the
Weak WLLN immediately implies that 
 as n→∞. !
Under very mild additional conditions, one can conclude that 
as n → ∞, without the assumption of normality of the X’s.
Refer to Example 5.2.11.
Example 5.2.2 Let X1, ..., Xn be iid Bernoulli(p), 0 < p < 1. We know that
E(X1) = p and V(X1) = p(1 – p), and thus by the Weak WLLN, we conclude
that  
 as n → ∞. !
In the following, we state a generalized version of the Weak WLLN. In
some problems, this result could come in handy.
Theorem 5.2.2 Let {Tn; n ≥ 1} be a sequence of real valued random
variables such that with some r(> 0) and a ∈ℜ, one can claim that ξr,n = E{|Tn
– a |r} → 0 as n → ∞. Then, 
  as n → ∞.

244
5. Concepts of Stochastic Convergence
Proof For arbitrary but otherwise fixed ε (> 0), we apply the Markov
inequality (Theorem 3.9.1) and write
Thus, we have 0 ≤ P{| Tn – a |≥ ε} ≤ ξr,n ε–r → 0 as n → ∞. That is, P{| Tn –
a |≥ε} ≤ ξr,n ε–r → 0 as n → ∞. !
In Example 5.2.3, the Weak WLLN does not apply,
but Theorem 5.2.2 does.
Example 5.2.3 Let X1, ..., Xn be identically distributed with –∞ < µ < ∞, 0
< σ < ∞, but instead of assuming independence among these X’s, we only
assume that Cov(Xi, Xj) = 0, for i ≠ j = 1, ..., n, n ≥ 1. Again, we have V(
)
= n–1 σ2 which converges to zero as n → ∞.  Thus, by the Theorem 5.2.2,
 as n → ∞. !
Example 5.2.3 shows that the conclusion from the Weak WLLN
holds under less restrictive assumption of uncorrelation among
the X’s rather than the independence among those X’s.
Example 5.2.4 Here we directly apply the Theorem 5.2.2. Let X1, ..., Xn
be iid Uniform(0, θ) with θ > 0. From Example 4.2.7, recall that Tn = Xn:n, the
largest order statistic, has the pdf given by
Thus, 
, and similarly we
get 
. That is, E{(Tn – θ)2} = 2(n + 1)–1 (n + 2)–1 θ2
which converges to zero as n → ∞. Now, we apply the Theorem 5.2.2 with
a = θ and r = 2 to conclude that 
 as n → ∞. !
We can, however, come to the same conclusion without referring to the
Theorem 5.2.2. Look at the next example.
Example 5.2.5 (Example 5.2.4 Continued) Let X1, ..., Xn be iid uniform on
the interval (0, θ) with θ > 0. Let us directly apply the Definition 5.2.1 to
show that Tn= 
 as n → ∞. Recall the pdf of Tn from (5.2.3). Now,
for arbitrary but otherwise fixed ε > 0, one has
where pn is positive. We simply need to evaluate the lim pn as n → ∞.
Now, with 0 < ε < θ, we have

5. Concepts of Stochastic Convergence
245
(1 – ε/θ)n → 0 as n → ∞, since 0 < 1 – εθ–1 < 1. Hence, by the Definition
5.2.1, we can claim that 
 as n → ∞.
Weak WLLN (Theorem 5.2.1) can be strengthened considerably.
One claims: 
 as n → ∞ so long as the Xi’s are
iid with finite µ. Assuming the finiteness of σ2 is not essential.
In what follows, we state a stronger version of the weak law of large
numbers. Its proof is involved and hence it is omitted. Some references are
given in the Exercise 5.2.2.
Theorem 5.2.3 (Khinchine’s WLLN) Let X1, ..., Xn be iid real valued
random variables with E(X1) = µ, –∞ < µ < ∞. Then, 
 as n → ∞.
Example 5.2.6 In order to appreciate the importance of Khinchine’s WLLN
(Theorem 5.2.3), let us consider a sequence of iid random variables {Xn; n =
1} where the distribution of X1 is given as follows:
with 
 so that 0 < c < ∞. This is indeed a probability distribution.
Review the Examples 2.3.1-2.3.2 as needed. We have 
 which
is finite. However, 
, but this infinite series is not finite. Thus,
we have a situation where µ is finite but ó2 is not finite for the sequence of the
iid X’s. Now, in view of Khinchine’s WLLN, we conclude that as
 n → ∞. From the Weak WLLN, we would not be able to
reach this conclusion.
We now move to discuss other important aspects. It may be that a se-
quence of random variables Un converges to u in probability as n → ∞, but
then one should not take it for granted that E(Un) → u as n → ∞. On the other
hand, one may be able to verify that E(Un) → u as n → ∞ in a problem, but
then one should not jump to conclude that 
 as n → ∞. To drive the
point home, we first look at the following sequence of random variables:
It is simple enough to see that E(Un) = 2 – 1/n → 2 as n → ∞, but 
 as
n → ∞. Next, look at a sequence of random variables Vn:

246
5. Concepts of Stochastic Convergence
Here again, it easily follows that 
 as n → ∞, but E(Vn) = 1 – 1/n+n →
∞ as n → ∞. The relationship between the two notions involving the conver-
gence in probability (that is, 
 as n → ∞) and the convergence in mean
(that is, E(Un) → u as n → ∞) is complicated. The details are out of scope for
this book. Among other sources, one may refer to Sen and Singer (1993,
Chapter 2) and Serfling (1980).
{Un; n ≥ 1} may converge to u in probability, but this fact by
itself may not imply that E(Un) → u. Also, a fact that E(Un)
→ u alone may not imply that 
. Carefully study the
random variables given in (5.2.6) and (5.2.7).
The Exercise 5.2.22 gives some sufficient conditions under which
one claims: 
 as n → ∞ ⇒ E[g(Un)] → g(u) as n → ∞.
The Exercise 5.2.23 gives some applications.
In textbooks at this level, the next theorem’s proof is normally left out as
an exercise. The proof is not hard, but if one is mindful to look into all the
cases and sub-cases, it is not that simple either. We include the proof with the
hope that the readers will think through the bits and pieces analytically, thereby
checking each step with the care it deserves.
Theorem 5.2.4 Suppose that we have two sequences of real valued ran-
dom variables {Un, Vn; n = 1} such that 
. Then,
we have:
(i)
(ii)
 as n → ∞;
(iii)
 as n → ∞ if P(Vn = 0) = 0 for all n = 1 and v ≠ 0.
Proof (i) We start with an arbitrary but otherwise fixed ε(> 0) and write
The second inequality follows because 
implies that  [| Un – u | + | Vn – v | < ε], so that [| Un – u | + | Vn – v | ≥ ε]

5. Concepts of Stochastic Convergence
247
implies [| Un – u | ≥ ½ ε ∪ | Vn – v | = 1/2ε]. The third inequality follows
because P(A ∪ B) ≤ P(A) + P(B) for any two events A and B. From the fact
that 
 as n → ∞, both the probabilities in the last step in (5.2.8)
converge to zero as n → ∞, and hence the lhs in (5.2.8), which is non-
negative, converges to zero as n → ∞, for all fixed but arbitrary ε(> 0). Thus,
 as n → ∞. The case of Un – Vn can be tackled similarly. It
is left as the Exercise 5.2.6. This completes the proof of part (i). "
(ii) We start with arbitrary but otherwise fixed ε(> 0) and write
Case 1: u = 0, v = 0
Observe that 
  implies that [| Un Vn | < ε], that
is [| Un Vn | ≥ ε] implies 
. Hence, we have
,
 and both these probabilities converge to zero as n → ∞, and
thus P{| Un Vn | ≥ ε} → 0 as n → ∞.
Case 2: u ≠ 0, v = 0
Observe by triangular inequality that | Un | = | (Un – u) + u | ≤ | Un – u | +
| u | and hence P{| Un | ≥ 2 | u |} ≤ P{| Un – u | + | u | ≥ 2 | u |} = P{| Un – u
| ≥ | u |} which converges to zero as n → ∞, since 
 as n →  ∞. Thus,
we claim that P{| Un | ≥ 2 | u |} →  0 as n →  ∞. Let us now write
But, we had verified earlier that the first term in the last step converges to zero
as n →  ∞, while the second term also converges to zero as n →  ∞ simply
because 
 as n →  ∞. Thus the lhs converges to zero as n →  ∞.
Case 3: u = 0, v ≠ 0

248
5. Concepts of Stochastic Convergence
The proof in this case is similar to the one in the previous case and hence
it is left out as the Exercise 5.2.6.
Case 4: u ≠ 0, v ≠ 0
From (5.2.9), using similar arguments as before, we observe that
In the last step in (5.2.10), the first and third terms certainly converge to zero
since 
,  
  ≠  0 as n → ∞. For the middle term in the last step
of (5.2.10), let us again use the triangular inequality and proceed as follows:
which implies that P{| Un | > 2 | u |} → 0 as n → ∞, since 
 as n
→ ∞. Hence the lhs in (5.2.10), converges to zero as n → ∞. This completes
the proof of part (ii). "
(iii) In view of part (ii), we simply proceed to prove that 
 as n
→ ∞. Note that | Vn – v | ≥ | v | – | Vn | which implies that
Observe that the lhs of (5.2.11) converges to unity as n → ∞ because 
as n → ∞. This implies
Let us denote Wn = | Vn – v | {| Vn || v |}–1. Then, for any arbitrary but
otherwise fixed ε(> 0), we can write

5. Concepts of Stochastic Convergence
249
which → 0 as n → ∞. The proof of part (iii) is now complete. !
Convergence in probability property is closed under the
operations: addition, subtraction, multiplication and division.
Caution: Division by 0 or ∞ is not allowed.
Example 5.2.7 Suppose that X1, ..., Xn are iid N(µ, σ2), –∞ < µ < ∞, 0 < σ2 < ∞,
n ≥ 2. Let us consider the sample mean 
 and the sample variance 
. We know
that 
 as n → ∞. Thus by the Theorem 5.2.4, part (i), we
conclude that 
 as n → ∞, and 
 as n → ∞.!
Let us again apply the Theorem 5.2.4. Suppose that 
 as n → ∞.
Then, by the Theorem 5.2.4, part (i), we can obviously claim, for example,
that 
 as n → ∞. Then, one may write
in view of the Theorem 5.2.4, part (ii). That is, one can conclude: 
as n → ∞. On the other hand, one could alternatively think of 
where Vn = Un and directly apply the Theorem 5.2.4, part (ii) also. The fol-
lowing theorem gives a more general result.
Theorem 5.2.5 Suppose that we have a sequence of real valued random
variables {Un; n = 1} and that 
 as n → ∞. Let g(.) be a real valued
continuous function. Then, 
 as n → ∞.
Proof A function g(x) is continuous at x = u provided the following holds.
Given arbitrary but otherwise fixed ε(> 0), there exists some positive number
δ → δ(ε) for which
Hence for large enough n(≥ n0 ≡ n0(ε)), we can write
and the upper bound → 0 as n → ∞. Now, the proof is complete. !
Example 5.2.8 (Example 5.2.4 Continued) Let X1, ..., Xn be iid uniform on
the interval (0, θ) with θ > 0. Recall from the Example 5.2.4 that Tn = Xn:n,
the largest order statistic, converges in probability to θ. In view of the
Theorem 5.2.5, obviously 
 as n → ∞, by considering the

250
5. Concepts of Stochastic Convergence
continuous function g(x) = x2 for x > 0. Similarly, for example, 
 as n
→ ∞, by considering the continuous function 
. By the
same token, we can also claim, for example, that sin 
 as
n → ∞. !
Example 5.2.9 Let X1, ..., Xn be iid Poisson (λ) with λ > 0 and look at 
,
the sample mean. By the Weak WLLN, we immediately claim that 
 as
n → ∞. By combining the results from the two Theorems 5.2.4 and 5.2.5, it
also immediately follows, for example, that
But then can one also claim that
Before jumping into a conclusion, one should take into consideration the fact
that 
0, which is positive for every fixed
n ≥ 1 whatever be λ(> 0). One can certainly conclude that
We ask the reader to sort out the subtle difference between the question raised
in (5.2.15) and the statement made in (5.2.16). !
Under fair bit of generality, we may claim that 
 as n → ∞.
But, inspite of the result in the Theorem 5.2.4, part (iii), we may not
be able to claim that 
 as n → ∞ when α > 0.
The reason is that µ may be zero or P(
 = 0) may be positive for all n.
See the Example 5.2.9.
Example 5.2.10 Let X1, ..., Xn be iid random variables with E(X1) = µ and
V(X1) = σ2, –∞ < µ < ∞, 0 < σ < ∞. Define Un = nγ 
 with 0 < γ <
1. Note that Un > 0 w.p. 1. Now, for any arbitrary ε(> 0), by the Markov
inequality, we observe that
Hence, by the Definition 5.2.1, 
 as n → ∞. !

5. Concepts of Stochastic Convergence
251
Remark 5.2.1 Under the setup of the Example 5.2.10, using the Theo-
rems 5.2.4-5.2.5, it is obvious that 
 0 as n → ∞. But note
that the term  nγ → ∞ as n → ∞ and the term Vn is inflated by this growth
factor. It is noteworthy in the Example 5.2.10 that the inflated random vari-
able 
 as n → ∞ if 0 < γ < 1 is held fixed.
Example 5.2.11 Let X1, ..., Xn be iid random variables with E(X1) = µ and
V(X1) = σ2, – ∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. As usual, write 
 for the
sample mean and variance respectively. It is easy to see that
which reduces to (n – 1)σ2 so that 
, without assuming any spe-
cial distribution. But, the calculation of 
 is not so simple when the distri-
bution of the X’s is unspecified. In the Section 2.3, we had defined the central
moments. In particular, we denoted the third and fourth central moments by
 respectively. Assume that 0 < µ4 <
∞ and µ4 > σ4. Since 
 does not depend on the specific value of µ,
without any loss of generality, let us pretend from this point onward that µ =
0. Look at the Exercise 5.2.17. Now,
which implies that
But, the X’s are iid and hence
and also,

252
5. Concepts of Stochastic Convergence
The variance of each term in (5.2.20) would be equal to 
, since (i) we pretend
that µ = 0, and (ii) the X’s are assumed independent. Also, the covariances
such as
and hence from (5.2.20) we get
See (1.6.11) for the sum of successive positive integers. Next, by taking each
covariance term into consideration, similarly one has
Now combining (5.2.17)-(5.2.22), we obtain
so that one writes
Now, obviously 
 as n → ∞. Using the Theorem 5.2.2 with r = 2
and a = σ2, it follows that 
 as n → ∞. By applying the Theorem
5.2.5, we can also conclude, for example, that 
 as n → ∞ or 
as n → ∞.
Remark 5.2.2 If the X’s considered in the Example 5.2.11 were assumed
iid N(µ,σ2), then we would have had µ4 = 3σ4 so that (5.2.23) would reduce to
Note that (5.2.24) is also directly verified by observing that 
 is distributed
as 
 so that one can write 
.
Example 5.2.12 (Example 5.2.11 Continued) Let us continue working under
the non-normal case and define 
 where 0 < γ < 1. Now, by
the Markov inequality and (5.2.23), with kn = µ4 – (n – 3)(n – 1)–1σ4 and for
any fixed ε(> 0), we can write:
Hence, by the Definition 5.2.1, 
 as n → ∞, whenever µ4 is finite and
µ4 > σ4. !

5. Concepts of Stochastic Convergence
253
5.3 Convergence in Distribution
Definition 5.3.1 Consider a sequence of real valued random variables {Un; n
≥ 1} and another real valued random variable U with the respective distribu-
tion functions Fn(u) = P(Un ≤ u), F(u) = P(U ≤ u), u ∈ ℜ. Then, Un is said to
converge in distribution to U as n → ∞, denoted by 
, if and only if
Fn(u) → F(u) pointwise at all continuity points u of F(.). The distribution of
U is referred to as the limiting or asymptotic (as n → ∞) distribution of Un
In other words, 
 means this: with every fixed u, once we compute
P(Un ≤ u), it turns out to be a real number, say, an. For the convergence in
distribution of Un to U, all we ask for is that the sequence of non-negative real
numbers an → a as n → ∞ where a = P(U ≤ u), u being any continuity point
of F(.).
It is known that the totality of all the discontinuity points of any
df F can be at most countably infinite. Review Theorem 1.6.1
and the examples from (1.6.5)-(1.6.9).
Example 5.3.1 Let X1, ..., Xn be iid Uniform (0, θ) with θ > 0. From
Example 4.2.7, recall that the largest order statistic Tn = Xn:n has the pdf given
by g(t) = ntn–1 θ–n I(0 < t < θ). The df of Tn is given by
Now let Un = n(θ – Tn)/θ. Obviously, P(Un > 0) = 1 and from (5.3.1) we can
write:
so that
since 
 m(1 – u/n)n = e–u. See (1.6.13) for some of the results on limits.
Now consider a random variable U with its pdf given by f(u) = e–uI(u > 0), so
that U is the standard exponential random variable, while its distribution func-
tion is given by

254
5. Concepts of Stochastic Convergence
It is clear that all points u ∈ (–∞, 0) ∪ (0, ∞) are continuity points of the
function F(u), and for all such u, 
. Fn(u)=F(u)Hence, we would say that
 as n → ∞.Hence, asymptotically Un has the standard exponential
distribution, that is the Gamma(1,1) distribution.!
Figure 5.3.1. PDF h(u; n) of Un When n = 10 and PDF
h(u) from the Example 5.3.2
Example 5.3.2 (Example 5.3.1 Continued) Using the expression of the df of
Un from (5.3.2), one can immediately check that the pdf of Un is given by h(u;n)
= (1 – u/n)n–1 I(u > 0) for n = 1,2, ... . The pdf of the limiting distribution is given
by h(u) = e–u I(u > 0). How quickly does the distribution of Un converge to the
distribution of U? When we compare the plots of h(u;10) and h(u) in the Figure
5.3.1, we hardly notice any difference between them. What it implies, from a
practical point of view, is this: Even if the sample size n is as small as ten, the
distribution of Un is approximated remarkably well by the limiting distribution. !
In general, it may be hard to proceed along the lines of our Example 5.3.1.
There may be two concerns. First, we must have the explicit expression of
the df of Un and second, we must be able to examine this df’s asymptotic
behavior as n → ∞. Between these two concerns, the former is likely to
create more headache. So we are literally forced to pursue an indirect ap-
proach involving the moment generating functions.
Let us suppose that the mgf’s of the real valued random variables Un and U
are both finite and let these be respectively denoted by Mn( t) ≡ MUn(t), M(t) ≡
MU(t) for | t | < h with some h(> 0).
Theorem 5.3.1 Suppose that Mn(t) → M(t) for | t | < h as n → ∞. Then,

5. Concepts of Stochastic Convergence
255
A proof of this result is beyond the scope of this book. One may refer to
Serfling (1980) or Sen and Singer (1993).
The point is that once M(t) is found, it must correspond to a unique ran-
dom variable, say, U. In the Section 2.4, we had talked about identifying a
distribution uniquely (Theorem 2.4.1) with the help of a finite mgf. We can
use the same result here too.
Also, one may recall that the mgf of the sum of n independent random
variables is same as the product of the n individual mgf’s (Theorem 4.3.1).
This result was successfully exploited earlier in the Section 4.3. In the follow-
ing example and elsewhere, we will repeatedly exploit the form of the mgf of
a linear function of iid random variables all over again.
Let X1, ..., Xn be independent real valued random variables. Let
Xi have its finite mgf Mxi(t) = E(etX
i) for i = 1, ..., n. Then, the
mgf of is 
 given by 
 where a1, ..., an
are any arbitrary but otherwise fixed real numbers.
Example 5.3.3 Let X1, ..., Xn be iid Bernoulli(p) with p = 1/2, and we
denote 
. Now, the mgf Mn(t) of Un is given by
since E (exp(tX1)) = ½(1 + et). In other words, one has
since ex = 1 + x/1! + x2/2! + x3/3! + ... and the remainder term Rn is of the
order O(n–2) so that 
 n2
n is finite. In (5.3.5), the expression in the last step
converges to exp(½t2) as n → ∞, because (1 + a/n)n → ea as n → ∞. But recall
from (2.3.16) that M(t) = e1/2t2 is the mgf of a standard normal variable, and
thus we claim that 
 as n → ∞. !
The Example 5.3.4 uses the mgf technique to show that
for large n, the distribution of 
 approaches
the standard normal distribution. The Section 5.4 gives
another justification for the same result.
Example 5.3.4 Suppose that Xn is distributed as the Chi-square with n
degrees of freedom. Denote 
 . The question

256
5. Concepts of Stochastic Convergence
is whether 
, some appropriate random variable, as n → ∞. For
, let us start with the mgf Mn(t) and write
Here, we use the form of the mgf of  
 from (2.3.28), namely, E {exp (sXn)}
= (1 – 2s)–n/2 for s < ½. Hence, with 
 we obtain
Thus, for large n, we can write
Here, we used the series expansion of log(1 – x) from (1.6.15). We have thus
proved that Mn(t) → exp(½t2) as n → ∞. Hence, by the Theorem 5.3.1, we
conclude that 
 as n → ∞, since M(t) = exp(½t2) hap-
pens to be the mgf of the standard normal distribution. !
5.3.1 Combination of the Modes of Convergence
In this section, we summarize some important results without giving their
proofs. One would find the proofs in Sen and Singer (1993, Chapter 2). One
may also refer to Serfling (1980) and other sources.
Theorem 5.3.2 Suppose that a sequence of real valued random variables
Un converges in probability to another real valued random variable U as n →
∞. Then,  
 as n → ∞.
The converse of Theorem 5.3.2 is not necessarily true.
That is, it is possible to have 
, but 
 as n → ∞.
Refer to Exercise 5.3.14.

5. Concepts of Stochastic Convergence
257
Theorem 5.3.3 (Slutsky’s Theorem) Let us consider two sequences of
real valued random variables {Un, Vn; n ≥ 1}, another real valued random
variable U, and a fixed real number v. Suppose that  
 v as
n → ∞. Then, we have as n → ∞:
(i)
(ii)
(iii)
 provided that P{Vn = 0} = 0 for all n
and v ≠ 0.
Example 5.3.5 (Examples 5.3.1-5.3.2 Continued) Recall Tn = Xn:n in the
Uniform(0, θ) situation and let us define 
. We already
know that 
. By using the Theorem 5.2.4 we can conclude
that 
. We had proved that 
 U as n → ∞
where U has the standard exponential distribution. Hence, by Slutsky’s Theo-
rem, we can immediately claim that 
as n → ∞. We
can also claim that 
 as n → ∞. It is not easy to
obtain the df of Hn and then proceed directly with the Definition 5.3.1 to show
that 
. !
In the Exercise 5.3.2, we had suggested a direct approach using
the Definition 5.3.1 to show that 
5.3.2
The Central Limit Theorems
First we discuss the central limit theorems for the standardized sample mean
and the sample variance.
Let X1, ..., Xn be iid random samples from a population with
mean µ and variance σ2, n = 2. Consider the sample mean 
and the sample variance 
. In the literature, 
is called the standardized version of the sample mean when
σ is known, and 
  is called the standardized
version of the sample mean when σ is unknown.
The following theorem, known as the central limit theorem (CLT),
provides under generality the asymptotic distribution for the standardized

258
5. Concepts of Stochastic Convergence
version of the sample mean 
 of iid real valued random variables having a
positive and finite variance.
Theorem 5.3.4 (Central Limit Theorem) Let X1, ..., Xn be iid real val-
ued random variables having the common mean µ and variance σ2, –∞ < µ <
∞ and 0 < σ < ∞. Then, as n → ∞, we have
A careful treatment of the proof of the CLT under such generality requires
working knowledge of the characteristic functions and hence it is out of
scope of this book. The reader may look at Sen and Singer (1993, pp. 107-
108) for a proof of this result and other versions of the CLT.
If one however assumes additionally that the mgf of X1 exists, then a proof
of the CLT can be constructed fairly easily, essentially along the lines of the
Examples 5.3.3-5.3.4. The details are left out as the Exercise 5.3.3.
Figure 5.3.2. Histogram of 100 Values of  u=
-100)/2
from the N(10, 4) Population When n = 10
In case the common pdf happened to be normal with mean µ and vari-
ance σ2, we had shown by means of the Helmert transformation in Chapter
4 that 
would be distributed exactly as the standard normal
variable whatever be the sample size n. Using the MINITAB Release 12.1,
we have drawn random samples from the N(µ = 10, σ2 = 4) population
with n = 10 and replicated the process. Having first fixed n, in the ith rep-
lication we drew n random samples x1i, ..., xni which led to the value of
the sample mean, 
 for i = 1, ..., 100. We then obtained

5. Concepts of Stochastic Convergence
259
the histogram of the 100 randomly observed values of the standardized sample
mean, namely, 
, ..., 100. The Figure 5.3.2 shows
the histogram which gives an impression of a standard normal pdf.
Figure 5.3.3. Histogram of 100 Values of  
-10)/2
from the Gamma(25, .4) Population When n = 10
The CLT on the other hand talks about the asymptotic distribution of
, whatever be the distribution of the parent population with finite
σ(> 0). So, we ran the following experiments with MINITAB Release 12.1.
We considered the Gamma(α = 25, β = .4) population which has its mean µ
= αβ = 10 and variance σ2 = αβ2 = 4. Then, having first fixed n = 10, in the ith
replication we drew n random samples x1i, ..., xni which led to the value of the
sample mean, 
 for i = 1,..., 100. We have plotted the histogram
of the 100 randomly observed values of the standardized sample mean, namely,
, i = 1, ..., 100. We give this plot in the Figure 5.3.3.
The CLT claims that the histogram in the Figure 5.3.3 should approxi-
mately resemble the pdf of a standard normal variable for large n. We note
that this histogram, though a little skewed to the right, creates an impression
of a standard normal pdf when the sample size is 10.
Next, we considered the Uniform(10 – a, 10+a) population with 
which has its mean µ = 10 and variance σ2 = 4. This uniform distribution
is symmetric about µ. Then, having first fixed n = 10, in the ith replication
we drew n random samples x1i, ..., xni which led to the value of the sample
mean, 
 for i = 1, ..., 100. We have then plotted the histo-
gram of the 100 randomly observed values of the standardized sample
mean, namely,  
 , i = 1, ..., 100. We give this plot in the

260
5. Concepts of Stochastic Convergence
Figure 5.3.4. We add that the earlier comments made in the context of the
Figure 5.3.3 remain valid when the Figure 5.3.4 is inspected.
In general, with a larger (than 10) sample size n, one will notice more
clearly the symmetry in the resulting histograms around zero, whatever be the
parent population pmf or pdf with finite σ(> 0).
Figure 5.3.4. Histogram of 100 Random 
 Values
from the Uniform(10 – a, 10 + a) Population when 
The three histograms presented in the Figures 5.3.2-5.3.4 are comparable
to each other in the sense that the three parent populations under consider-
ation have the same mean, µ = 10 and variance, σ2 = 4. The reader should
pursue more explorations like these as exercises.
Example 5.3.6 Let X1, ..., Xn be iid N(µ, σ2), –∞ < µ < ∞, 0 < σ < ∞. For
n ≥ 2, let 
 be the sample variance. From
(4.4.9), recall that 
 where these Yi’s are the Helmert
variables distributed as iid N(0, σ2). This implies that 
 is indeed a sample
mean of (n – 1) iid random variables. The CLT then immediately leads us to
conclude that 
 as n → ∞. But,
one has 
 3σ4 – σ4 = 2σ4.
That is in this case, we have 
 N (0, 2σ4) as n → ∞.
Let us now denote Vn = {n/(n – 1)}1/2 and view it as a sequence of degenerate
random variables to claim: 
 as n → ∞. Next, we apply Slutsky’s Theo-
rem to conclude that 
 
 as n → ∞. !

5. Concepts of Stochastic Convergence
261
Example 5.3.7 First note that we can express
and hence by Slutsky’s Theorem, under the same conditions as the CLT, we
can conclude that 
 This
is so because 
 as n →
∞. Similarly, we can easily handle the convergence in distribution for the
sequence of random variables (i) 
 if µ ≠ 0 or (ii)
 if µ > 0 and 
 is positive w.p.1 for all n. These are
left out as the Exercise 5.3.4. !
The following result is a very useful follow-up on the CLT. Suppose that
one is able to use the CLT to verify that 
 converges in distribution
to N(0, σ2) as n → ∞, with some appropriate θ and σ(> 0), where {Tn; n ≥ 1}
is a sequence of real valued random variables. But then, does 
converge to an appropriate normal variable for reasonable g(.) functions?
Review the Example 5.3.7. The following theorem, a nice blend of the CLT
(Theorem 5.3.4) and Slutsky’s Theorem, answers this question affirmatively.
Theorem 5.3.5 (Mann-Wald Theorem) Suppose that {Tn; n ≥ 1} is a
sequence of real valued random variables such that 
as n → ∞ where σ2(> 0) may also depend on θ. Let g(.) be a continuous real
valued function such that 
g(θ), denoted by g’(θ), is finite and nonzero.
Then, we have:
Proof We proceed along the proof given in Sen and Singer (1993, pp. 231-
232). Observe that
where we denote
Next, note that
and since 
 as n → ∞, by Slutsky’s Theorem, part (ii), we
conclude that 
 as n → ∞. Thus, 
 as n → ∞, by the

262
5. Concepts of Stochastic Convergence
definition of g’(θ) itself. Recall that for some fixed x, one has: 
. We again apply Slutsky’s Theorem, part (ii), in con-
junction with (5.3.6) to complete the proof. !
Remark 5.3.1 Suppose that the X’s are as in the CLT. Then we immedi-
ately conclude that 
 as n → ∞, once we plug in
g(x) = x2, x ∈ ℜ and apply the Mann-Wald Theorem. We had shown this result
in a different way in the Example 5.3.7. Now, we can also claim that
 as n → ∞, by plugging in g(x) = xq, x ∈
ℜ, for any fixed non-zero real number q (≠ 1). We leave verification of this
result as an exercise. Of course, we tacitly assume that q is chosen in such a
way that both 
 and µq remain well-defined. Note that for negative values of
q, we need to assume that the probability of 
 being exactly zero is in fact
zero and that µ is also non-zero.
Example 5.3.8 Let X1, ..., Xn be iid Poisson (λ) with λ > 0. Observe that
in this case µ = σ2 = λ and then, by the CLT, 
  N(0, 1) as
n → ∞. Then, by using the Remark 5.3.1 and Mann-Wald Theorem, we
immediately see, for example, that 
 as n → ∞. !
Now we move towards an appropriate CLT for the standardized sample
variance. This result again utilizes a nice blend of the CLT (Theorem 5.3.4)
and Slutsky’s Theorem.
Theorem 5.3.6 (Central Limit Theorem for the Sample Variance) Let
X1, ..., Xn be iid random variables with mean µ, variance σ2(> 0), µ4 = E{(X1
– µ)4}, and we assume that 0 < µ4 < ∞ as well as µ4 > σ4. Denote
  for n ≥ 2. Then,
we have:
Proof Let us first work with 
. We denote Yi = (Xi –
µ)2, i = 1, ..., n, 
, and write
From (5.3.7), observe that

5. Concepts of Stochastic Convergence
263
Notice that Y’s are iid with mean = E(Y1) = σ2 and variance = E(Y1
2) – E2(Y1)
= µ4 – σ4 which is assumed finite and positive. Hence, by the CLT, we have
 as n → ∞. Also, from the Example 5.2.10 it follows that
 as n → ∞. Thus by Slutsky’s Theorem, part (i), we conclude that
 as n → ∞. Next, we write
and reapply Slutsky’s Theorem. The result then follows since 
 and 
  as n → ∞, so that 
 as n
→ ∞. !
Remark 5.3.2 One can obtain the result mentioned in the Example 5.3.6
from this theorem by noting that µ4 = 3σ4 in the normal case.
Example 5.3.9 Under the setup of the Theorem 5.3.6, using Example
5.2.11 and Slutsky’s Theorem, we can immediately conclude the following
result when the X’s are iid but non-normal: 
 as n → ∞. In the literature, µ4σ–4 is traditionally denoted by
β2, which is customarily referred to as a measure of the kurtosis in the parent
population. !
The following result is along the lines of the Theorem 5.2.5. It shows that
the property of the convergence in distribution is preserved under a continu-
ous transformation. We state it without giving its proof.
Theorem 5.3.7 Suppose that we have a sequence of real valued random
variables {Un; n ≥ 1} and another real valued random variable U. Suppose
also that 
 as n → ∞. Let g(.) be a real valued continuous function.
Then, 
 as n → ∞.
Example 5.3.10 Let X1, ..., Xn be iid real valued random variables having
the common mean µ and variance σ2, –∞ < µ < ∞ and 0 < σ < ∞. From the
CLT we know that 
 as n → ∞. Thus, using the
Theorem 5.3.7 with g(x) = x2, x ∈ ℜ, we can immediately conclude that
 as n → ∞, since the square of a standard normal
variable has a Chi-square distribution with one degree of freedom.!
Example 5.3.11 (Example 5.3.10 Continued) Let X1, ..., Xn be iid real
valued random variables having the common mean µ and variance σ2, –∞
< µ < ∞ and 0 < σ < ∞. From the CLT we know that 
N(0, 1) as n → ∞. Thus, using the Theorem 5.3.7 with g(x) = |x|, x ∈ ℜ,

264
5. Concepts of Stochastic Convergence
we can immediately conclude that 
 as n → ∞ where Z
is the standard normal variable. !
For the approximation of the Binomial(n, p) distribution with the
N(np, np(1 – p)) distribution, for large n and fixed 0 < p < 1,
refer to the Exercise 5.3.18.
5.4 Convergence of Chi-square, t, and F
Distributions
In this section, we discuss some asymptotic properties of the Chi-square, t,
and F distributions. One will notice that most of the probabilistic and statisti-
cal tools introduced thus far come together in these topics.
5.4.1
The Chi-square Distribution
First, consider a random variable Uν which is distributed as 
. Now,
E(ν–1Uν ) = 1, V(ν–1Uν) = ν–2V(Uν) = 2ν–1 → 0 as ν → ∞, and hence by
applying the Weak WLLN, we can claim immediately that
Let X1, ..., Xν, ... be iid so that we may view 
 as 
, and hence we
can write 
. Now, we can apply the CLT (Theo-
rem 5.3.4) to claim that 
 as ν → ∞ with µ = E(X1)
= 1, σ2 = V(X1) = 2, and hence, 
 as ν  → ∞. In
the Example 5.3.4, we arrived at the same conclusion with a fairly different
approach. For practical purposes, we would say:
5.4.2
The Student’s t Distribution
The Student’s t random variable was described in the Definition 4.5.1.
Let Wν = X ÷ (Yνν–1)1/2 where X is the standard normal variable and Yν is
, while X and Yν are assumed independent. In other words, Wν has the

5. Concepts of Stochastic Convergence
265
Student’s t distribution with ν degrees of freedom. Now, since 
according to (5.4.1), so does 
 as ν → ∞ by the Theorem 5.2.5
with g(x) = x1/2, x > 0. Hence, applying Slutsky’s Theorem, we conclude that
 as ν  → ∞. For practical purposes, we would say:
5.4.3
The F Distribution
The Fν1,ν2 random variable was introduced by the Definition 4.5.2 as ν1
–1 Xν1
÷ ν2
–1 Yν2 where Xν1, Yν2 are independent, Xν1 is 
 and Yν2 is  
. Now, if ν1
is held fixed, but ν2 is allowed to go to infinity, then, 
.
Next, we apply Slutsky’s Theorem. Hence, as ν2 → ∞, we conclude that
, that is, for practical purposes, we would say:
5.4.4
Convergence of the PDF and Percentage Points
From (1.7.30), let us recall the pdf of the Student’s t random variable with ν
degrees of freedom, denoted by tν. The pdf of tν, indexed by ν, is given by
with 
 We had
seen in (5.4.3) that 
, the standard normal variable, as ν  → ∞. But, is it
true that fν(x) → φ(x), the pdf of the standard normal variable, for each fixed
x ∈ ℜ, as ν  → ∞? The answer follows.
Using the limiting value of the ratio of gamma functions from (1.6.24), we
obtain

266
5. Concepts of Stochastic Convergence
Next, we rewrite
Hence, one obtains
using (5.4.6) and the facts that 
 and 
 (1 +
(1/ν) x2)–1/2 = 1. Refer to the Section 1.6 as needed. Thus, we conclude:
With 0 < α < 1, let us define the upper 100α% point tν,α for the Student’s tν
distribution through the following relationship:
that is, the area under the pdf of tν on the rhs of the point tν,α is α. Similarly
let zα stand for the upper 100α% point for the standard normal distribution.
These percentage points have been tabulated in the Appendix. It is known that
for any fixed 0 < α < 1, one has
But what is interesting is that this convergence is also monotone. For any
fixed 0 < α < 1, it is known that

5. Concepts of Stochastic Convergence
267
One should refer to Ghosh (1973) and DasGupta and Perlman (1974) for
many related details.
For large ν, one can expand tν,α in terms of zα and ν.   E.A.Cornish and R.
A. Fisher systematically developed techniques for deriving expansions of the
percentile of a distribution in terms of that of its limiting form. These expan-
sions are customarily referred to as the Cornish-Fisher expansions. The Cor-
nish-Fisher expansion of tν,α in terms of zα and ν is given below: For large
values of ν,
The reader may refer to Johnson and Kotz (1970, p. 102) for related details.
At this point, the reader may ask the following question: For some fixed 0
< α < 1, how good is the expression in the rhs of (5.4.11) as an approxima-
tion for tν,α when ν is small or moderate? Let us write
The Table 5.4.1 provides the values of zα, correct up to four decimal places,
and those of tν,α, correct up to three decimal places. We have computed
tν,α,approx, correct up to five decimal places, for α = .05, .10 and ν = 10, 20, 30.
The values of zα and tν,α are respectively obtained from Lindley and Scott
(1995) and the Student’s t Table 14.3.3. The entries in the Table 5.4.1 show
clearly that the approximation (5.4.12) works well.
Table 5.4.1. Comparing tν,α with tν,α,approx
α = .10
zα = 1.2816
α = .05
zα = 1.6449
ν
tν,α
tν,α,approx
tν,α
tν,α,approx
10
1.3722
1.37197
1.8125
1.81149
20
1.3253
1.32536
1.7247
1.72465
30
1.3104
1.31046
1.6973
1.69727
Next, recall from (1.7.34) that for the F random variable with ν1, ν2 de-
grees of freedom, written as Fν1,ν2, the pdf is given by,
indexed by ν1, ν2, with b = b(ν1, ν2) = (ν1/ν2)1/2ν1 Γ((ν1 + ν2)/2){Γ(ν1/2) ×
Γ(ν2/2)}–1, ν1, ν2 = 1, 2, 3, ... . We had seen earlier in (5.4.4) that 
  when ν1 is held fixed, but ν2 ? ∞. But, is it true that fν1,ν2 (x) →

268
5. Concepts of Stochastic Convergence
g(x) where g(x) is the pdf of 
 at x, for each fixed x ∈ ℜ+, as ν2 → ∞? The
answer follows.
Using the limiting value of the ratio of gamma functions from (1.6.24), we
again obtain
Next, we rewrite
and obtain
using (5.4.14) as well as the facts that 
 (1 + ν1/ν2x)–1/2ν2 = e–1/2ν1
x and
 (1 + ν1/ν2x)–1/2ν1 = 1. Refer to the Section 1.6 as needed. We conclude:
Let Fν1,ν2,α be the upper 100α% point of the Fν1,ν2 distribution for any fixed
0 < α < 1. Unlike the percentiles tν,α for the tν  distribution, the corresponding
F percentiles Fν1,ν2,α do not satisfy a clear-cut monotonicity property. The sce-
nario here is complicated and one may consult the two articles of Ghosh (1973)
and DasGupta and Perlman (1974) cited earlier for details. The percentile Fν1,ν2,α
does have the Cornish-Fisher expansion in terms of the upper 100α% point
from the 
 distribution depending upon ν1,ν2,α for large ν2 when ν1 is
kept fixed. The explicit details are due to Scheffé and Tukey (1944). Refer to
Johnson and Kotz (1970, p. 84) for specific details in this regard.

5. Concepts of Stochastic Convergence
269
In the same vein, when ν1 = 2, but ν2 → ∞, we can easily study the behav-
ior of F2,ν2,α in terms of 
 for any fixed 0 < α < 1. We rely upon very simple
and familiar tools for this purpose. In this special case, we provide the interest-
ing details so that we can reinforce the techniques developed in this textbook.
Let us denote c for the upper 100α% point of the distribution of 
,
that is
and hence we have c = c(α) = –log(α). Suppose that X and Y are respectively
distributed as 
 and 
, and suppose also that they are independent. Let h(y)
be the pdf of Y at y(> 0). Let us simply write d instead of F2,ν2,α so that
Since X and Y are independent, one obtains
writing the mgf of 
 from (2.3.28). Thus, one has the following exact
relationship between d and α:
One can show that d is a strictly decreasing function of ν2. We leave this out
as an exercise. Next, let us explore how we can expand d as a function of ν2.
We use a simple trick. Observe that

270
5. Concepts of Stochastic Convergence
using the expansion of ex. Thus, one gets
which is the Cornish-Fisher expansion of F2,ν2,α for large ν2.
One may ask the following question: How good is the expression in (5.4.18)
as an approximation for F2,ν2,α when ν2 is small or moderate? Let us write
The Table 5.4.2 provides the values of log(α), correct up to five decimal
places, and those of F2,ν2,α , correct up to three decimal places. We have
computed F2,ν2,α,approx , correct up to five decimal places, for α = .05, .10 and
ν2 = 10, 20, 30. The values of F2,ν2,α are obtained from Lindley and Scott
(1995) and the F Table 14.3.4. The entries in the Table 5.4.2 show clearly that
the approximation (5.4.19) works well.
Table 5.4.2. Comparing F2,ν2,α with F2,ν2,α,approx
α = .10
log(α) = –2.30259
α = .05
log(α) = –2.99573
ν2
F2,ν2,α
F2,ν2,α,approx
F2,ν2,α
F2,ν2,α,approx
10
2.9245
2.91417
4.1028
4.07240
20
2.5893
2.58803
3.4928
3.48926
30
2.4890
2.48836
3.3160
3.31479
5.5 Exercises and Complements
5.2.1 Let X1, ..., Xn be iid having the following common discrete probabil-
ity mass function:
X values:
–2
0
1
3
4
Probabilities:
.2
.05
.1
.15
.5
Is there some real number a such that 
 as n → ∞? Is there some
positive real number b such that 
 as n ? ∞? {Hint: Find the values of µ,
σ and then try to use the Weak WLLN and Example 5.2.11.}
5.2.2 Prove Khinchine’s WLLN (Theorem 5.2.3). {Hint: For specific ideas,
see Feller (1968, pp. 246-248) or Rao (1973, p.113).}

5. Concepts of Stochastic Convergence
271
5.2.3 Let X1, ..., Xn be iid N(µ, 1), –∞ < µ < ∞. Consider the following
cases separately:
Is there some real number a in each case such that 
 as n → ∞?
{Hint: Can one use the Theorems 5.2.4-5.2.5 together?}
5.2.4 Let X1, ..., Xn be iid having the common pdf
Is there a real number a such that 
 as n → ∞? {Hint: Is the Weak
WLLN applicable here?}
5.2.5 (Example 5.2.6 Continued) Suppose that X1, ..., Xn are iid having the
following common distribution: P(X1 = i) = c/ip,i = 1, 2, 3, ... and 2 < p < 3.
Here, c ≡ c(p) (> 0) is such that 
. Is there a real number
a ≡ a(p) such that 
 as n → ∞, for all fixed 2 < p < 3?
5.2.6 In the Theorem 5.2.4, part (i), construct a proof of the result that
 Also, prove part (ii) when u = 0, v ≠ 0.
5.2.7 (Example 5.2.5 Continued) Let X1, ..., Xn be iid Uniform (0, θ) with
θ > 0. Consider Xn:n, the largest order statistic. Find the range of γ(> 0) such
that 
  as n → ∞. Does 
 as n → ∞?
{Hint: In the first part, follow the approach used in the Example 5.2.10 and
later combine with the Theorem 5.2.4.}
5.2.8 Obtain the expression of 
 using (5.2.23) when the X’s are iid
with
(i)    Bernoulli(p), 0 < p < 1;
(ii) Poisson(λ), λ > 0;
(iii)  Geometric(p), 0 < p < 1;
(iv) N(µ µ2), µ > 0.
In part (ii), show that V[S2] > V [
]for all fixed λ(> 0).
5.2.9 (Example 4.4.12 Continued) Suppose that X1, ..., Xn are iid random
variables having the negative exponential distribution with the common pdf
given by f(x) = σ–1 exp{–(x – µ)/σ}I(x > µ) with –∞ < µ < ∞, 0 < σ < ∞. Let
Xn:1 be the smallest order statistic and Tn = 
 Show that

272
5. Concepts of Stochastic Convergence
5.2.10 Consider a sequence of real valued random variables {Tn; n > k},
and suppose that 
 as n → ∞. Is it true that for some real
number a, the random variable 
 as n → ∞? {Hint: Can one apply
Slutsky’s Theorem after taking the natural logarithm?}
5.2.11 (Exercise 5.2.10 Continued) Let X1, ..., Xn be iid Poisson(λ) with λ
> 0, and let 
 for n > k. Show that 
 as n →
∞. Also, find the number c(> 0) such that 
 as n → ∞.
5.2.12 Consider a sequence of real valued random variables {Tn; n ≥ 1},
and suppose that 
 as n → ∞. Let us define Xn = I(Tn > 1/2a),
n ≥ 1. Does 
 as n → ∞? Suppose that Yn = I(Tn > 3/2a), n ≥ 1.
Does 
 as n → ∞? {Hint: Try and apply the Definition 5.2.1 di-
rectly. Can one use the Markov inequality here?}
5.2.13 (i) Consider the two sequences of random variables {Un; n ≥ 1}
and {Vn; n ≥ 1} respectively defined in (5.2.6)-(5.2.7). Verify that 
 as n → ∞. Using Theorem 5.2.4, it will immediately follow that
 and 
 as n → ∞.
(ii) Additionally suppose that Un and Vn are independent for all n ≥ 1. In
this situation, first obtain the probability distributions of 
 and Un Vn.
Hence, show directly, that is without appealing to Slutsky’s Theorem, that
 and  
 as n → ∞.
5.2.14 Suppose that (Xi, Yi), i = 1, ..., 2n, are iid N2(0, 0, 1, 1, ρ) with –1
< ρ < 1. Recall the bivariate normal distribution from the Section 3.6. Let us
denote
with i = 1, 2, ..., n. Consider now the sample mean 
, and
denote 
(i)  Show that the Ui’s are iid Bernoulli with p = 1/2 (1 + ρ );
(ii) Show that 
 as n → ∞.
{Hint: Note tht p = P(U1 = 1) = P(X1Y1 + X2Y2 > 0). But, one can
write, for example, X1Y1 = ¼{(X1 + Y1)2 – (X1 – Y1)2}, so that p = P{(X1 +
Y1)2 – (X1 – Y1)2 + (X2 + Y2)2 – (X2 – Y2)2 > 0} = P{(X1 + Y1)2 + (X2 + Y2)2
> (X1 – Y1)2 + (X2 – Y2)2}. Verify that U = (X1 + Y1)2 + (X2 + Y2)2 is
independent of V = (X1 – Y1)2 + (X2 – Y2)2. Find the distributions of U
and V. Then, rewrite p as the probability of an appropriate event defined

5. Concepts of Stochastic Convergence
273
in terms of the F2,2 random variable and ρ. Thus, p can be evaluated with the
help of the integration of the pdf of F2,2.}
5.2.15 Suppose that X1, ..., Xn are iid having the common N(0, 1) distribu-
tion, and let us denote 
.
Determine the positive real numbers a and b such that as 
n → ∞. {Hint: We write 
so that 
 which can be rewritten as
 by substituting v = 1/2u2. Can one of the
weak laws of large numbers be applied now?}
5.2.16 Suppose that X1, ..., Xn are iid with the pdf f(x) = 1/8e–|x|/4 I(x ∈ ℜ)
and n ≥ 3. Denote 
 and
  Determine the real numbers a, b and c such that
 as n → ∞.
5.2.17 (Example 5.2.11 Continued) Let X1, ..., Xn be iid random variables
with E(X1) = µ and V(X1) = σ2, –∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. Let
 be the sample variance. Suppose that Yi =
aXi + b where a(> 0) and b are fixed numbers, i = 1, ..., n. Show that the new
sample variance based on the Yi’s is given by 
5.2.18 Suppose that (X1, Y1), ..., (Xn, Yn) are iid 
 where
–∞ < µ1, µ2 < ∞, 0 < 
 < ∞ and – 1 < ρ < 1. Let us denote Pearson’s
sample correlation coefficient defined in (4.6.7) by rn instead of r. Show that
 as n → ∞. {Hint: Use Theorem 5.2.5.}
5.2.19 (Exercise 5.2.11 Continued) Denote Tn = I n = 1. Does 
 or 1
as n →∞? Prove your claim.
5.2.20 Let X1, ..., Xn, ... be iid random variables with the finite variance.
Let us denote
Show that 
 as n → ∞. {Hint: Verify that 
 for all n.
In this problem, one will need expressions for both 
 Re-
view (1.6.11) as needed.}
5.2.21 Let X1, ..., Xn, ... be iid random variables where

274
5. Concepts of Stochastic Convergence
Does 
  as n → ∞ for some appropriate c? {Hint: Can one of the weak
laws of large numbers be applied here? Is it true that alog(r) = rlog(a) for any two
positive numbers a and r?}
5.2.22 (i) (Monotone Convergence Theorem) Consider {Un; n = 1}, a
sequence of real valued random variables. Suppose that as 
 as n → ∞.
Let g(x), x ∈ ℜ, be an increasing real valued function. Then, E[g(Un)] → g(u)
as n → ∞.
(ii) (Dominated Convergence Theorem) Consider {Un; n ≥ 1}, a se-
quence of real valued random variables. Suppose that as 
 n → ∞. Let
g(x), x ∈ ℜ, be a real valued function such that |g(Un)| ≤ W and E[W] is finite.
Then, E[g(Un)] → g(u) as n → ∞.
{Note: Proofs of these two results are beyond the scope of this book. The
part (i) is called the Monotone Convergence Theorem, a special form of which
was stated earlier in Exercise 2.2.24. The part (ii) is called the Dominated
Convergence Theorem. The usefulness of these two results is emphasized in
Exercise 5.2.23.}
5.2.23 Let X1, ..., Xn be iid non-negative random variables with µ ∈ ℜ+, σ
∈ ℜ+. We denote 
.
(i)
Does E{eU
n} converge to some real number as n → ∞?
(ii)
Does E{e–2Un} converge to some real number as n → ∞?
(iii)
Does E{sin(Un)} converge to some real number as n → ∞?
(iv)
Does E{Φ(Un)} converge to some real number as n → ∞ where
(v)
Does E(Tn) converge to some real number as n → ∞?
{Hints: By the WLLN, we first claim that 
 µ as n → ∞. Let g(x) = ex,
x ∈ ℜ+ be our increasing real valued function. Then, by the Exercise 5.2.22,
part (i), we conclude that E[g(Un)] = E{eU
n} → eµ as n → ∞. For the second
part, note that g(x) = e–2x, x ∈ ℜ+ is bounded between two fixed numbers, zero
and one. For the third part, note that g(x) = sin(x), x ∈ ℜ+ is bounded between
± 1. Thus, by the Exercise 5.2.22, part (ii), we conclude that E[g(Un)] will
converge to e–2µ (or sin (µ)) as n → ∞ in part (ii) (or (iii)) respectively. How
about parts (iv)-(v)?}
5.2.24 Let X1, ..., Xn be iid Poisson (λ), 0 < λ < ∞. We denote Un =
, n ≥ 1. From Exercise 5.2.23, it follows that E[eUn] → eλ and E[e–2U
n]
→ e–2λ as n → ∞. Verify these two limiting results directly by using the
mgf of a Poisson random variable. {Hint: Observe that et/n = 1 + t/n + 1/
2!(t/n)2 + ... . Then, note that E[etU
n] = [exp{– λ + λet/n}]n = exp{–nλ +

5. Concepts of Stochastic Convergence
275
nλet/n} → etλ as n → ∞.}
5.3.1 (Exercise 5.2.1 Continued) Find a and b(> 0) such that we can
claim: 
 as n → ∞.
5.3.2 (Example 5.3.5 Continued) Let X1, ..., Xn be iid Uniform (0, θ)
with θ > 0 and suppose that Tn = Xn:n, the largest order statistic. First, find
the df of the random variable Gn = n(θ – Tn)/Tn, and then use the Definition
5.3.1 to show directly that the limiting distribution of Gn is the standard
exponential.
5.3.3 Prove CLT using the mgf technique assuming that the Xi’s are iid having
a finite mgf for | t |< h with some h > 0. {Hint: Follow the derivations in Examples
5.3.3-5.3.4 closely.}
5.3.4 (Example 5.3.7 Continued) Under the same conditions as the CLT, along
the lines of Example 5.3.7, derive the asymptotic distribution of
(i)
 as n → ∞ if µ ≠ 0;
(ii)
 as n → ∞ if µ > 0 and 
 is positive w.p.1.
In this problem, avoid using the Mann-Wald Theorem.
5.3.5 (Exercise 5.3.2 Continued) Let X1, ..., Xn be iid Uniform (0, θ) with
θ > 0 and suppose that Tn = Xn:n, the largest order statistic. Does n{θ –
 converge to some appropriate random variable in distribution as
n ? ∞? {Hint: Can Slutsky’s Theorem be used here?}
5.3.6 (Exercise 5.2.3 Continued) Let X1, ..., Xn be iid N(µ, 1), –∞ < µ < ∞.
Consider the following cases separately:
Find suitable an, bn (> 0) associated with each Tn such that  
N (0, 1) as n → ∞. {Hint: Is the Mann-Wald Theorem helpful here?}
5.3.7 (Example 5.3.8 Continued) Find the number η ( > 0) such that
 as n → ∞. Here, 
 is the sample variance obtained from iid
random variables X1, ..., Xn, n ≥ 2. Solve this problem separately when the Xi’s are (i)
normal and (ii) non-normal, under appropriate moment assumptions. {Hint: Is the
Mann-Wald Theorem helpful here?}
5.3.8 (Exercise 5.3.7 Continued) Find the number η( > 0) such that
 N(0, η2) as n → ∞, where u (≠ 0) is some fixed
real number. Hence, that is without referring to Slutsky’s Theorem, find the
number ξ(> 0) such that 
 as n → ∞ where
u (≠ 0) is some fixed real number. Solve this problem separately when

276
5. Concepts of Stochastic Convergence
the X’s are (i) normal and (ii) non-normal, under appropriate moment
assumptions.
5.3.9 (Exercise 5.2.9 Continued) Suppose that X1, ..., Xn are iid random
variables having the negative exponential distribution with the common pdf
given by
Let Xn:1 be the smallest order statistic and 
. Show that
 the standard exponential random variable, as n → ∞.
5.3.10 Let X1, ..., Xn be iid with the lognormal distribution having the
following pdf with –∞ < µ < ∞, 0 < σ < ∞:
Denote 
, the geometric mean of X1, ..., Xn.
(i)
Find the real number c(> 0) such that 
(ii) Find the positive real numbers an, bn such that (Tn – an)/bn
 as n → ∞.
5.3.11 Let X1, ..., Xn be iid with the uniform distribution on the interval (0,
θ), θ > 0. Denote 
, the geometric mean of X1, ..., Xn.
(i)
Find c(> 0) such that 
 as n → ∞;
(ii) Find an, bn(> 0) such that 
 as n → ∞.
5.3.12 (Exercise 5.2.15 Continued) Suppose that X1, ..., Xn are iid having
the common N(0, 1) distribution. We denote 
 and
. Find suitable numbers cn, dn(> 0), cn, dn(> 0) such
that 
 and 
 as n → ∞.
5.3.13 (Exercise 5.2.16 Continued) Suppose that X1, ..., Xn are iid with the
pdf f(x) = 1/8e–|x|/4 I(x ∈ ℜ) and n ≥ 3. Denote 
 and 
. Find suitable numbers
 such that 
 and 
 as n → ∞.
5.3.14 This exercise shows that the converse of the Theorem 5.3.2 is
not necessarily true. Let {Un, U; n ≥ 1} be a sequence of random variables

5. Concepts of Stochastic Convergence
277
such that Un and U are independent, Un is N(0, 1 + 1/n), and U is N(0, 1), for
each n ≥ 1. Show that
(i)
 as n → ∞. {Hint: For x ∈ ℜ, check that P(Un ≤ x)
 → Φ (x) as n → ∞.};
(ii)
 as n → ∞. {Hint: Since Un and U are independent,
Un – U is distributed as N(0, 2 + 1/n). Hence, 
as n → ∞}.
5.3.15 Let X1, ..., Xn be iid Uniform (0, 1). Let us denote a sequence of
random variables, 
.
(i)
Show that the mgf, MX1 (t) = 1/t(et – 1), t ≠ 0;
(ii) Show that the mgf, MUn (t) = {1/2(ea – e–a)/a}n where a
(iii) Show that 
= exp{t2/24};
(iv) Use part (iii) to argue that 
 as n → ∞.
5.3.16 Let X1, ..., Xk be iid N(0, 1) where k = 2n, n ≥ 1. We denote
Find the limiting (as n → ∞) distribution of Wn = Un/Vn. {Hint: Let 
,
j = 1, ..., n. Each random variable Yj has the Cauchy pdf f(y) = π–1 (1 + y2)–1,
–∞ < y < ∞. Also, 1/n Un has the same Cauchy pdf. Is it possible to conclude
that 
 as n → ∞ where W has the same Cauchy pdf? Would Slutsky’s
Theorem suffice?}
5.3.17 Let X1, ..., Xn be iid N(0, 1), n ≥ 1. We denote:
Show that 
 as n → ∞. {Hint: Use both CLT and Slutsky’s
Theorem.}
5.3.18 (Normal Approximation to the Binomial Distribution)
Suppose that X1, ..., Xn are iid Bernoulli(p), 0 < p < 1, n ≥ 1. We know

278
5. Concepts of Stochastic Convergence
that 
 is distributed as Binomial(n, p), n ≥ 1. Apply the CLT to
show that
In other words, for practical problems, the Binomial(n, p) distribution can be
approximated by the N(np, np(1 – p)) distribution, for large n and fixed 0 < p
< 1.
5.3.19 (Exercise 5.3.18 Continued) Let X1, ..., Xn be iid Bernoulli(p) with 0
< p < 1, n = 1. Let us denote 
. Show that
{Hint: In view of the Exercise 5.3.18, would the Mann-Wald Theorem help?}
5.3.20 (Exercises 5.3.18-5.3.19 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p), 0 < p < 1, n ≥ 1. Let us denote 
 and Wn = Vn(1
– Vn). Show that
when p ≠ 1/2. {Hint: In view of the Exercise 5.3.18, would the Mann-Wald
Theorem help?}
5.3.21 Suppose that X1, ..., Xn are iid with the common pdf
f (x) = cerp {3x – 4x
2} for x ∈ ℜ,
where c(> 0) is an appropriate constant.
(i)
Find k such that 
 as n → ∞;
(ii) Find an, bn(> 0) such that 
 as n → ∞.
5.4.1 (Example 4.5.2 Continued) Suppose that the random variables Xi1,
..., Xinz are iid N(µ, σ2, i, = 1, 2, and that the X1j’s are independent of the X2j’s.
With ni ≥ 2, n = (n1, n2), let us denote
for i = 1, 2. Consider the two-sample t random variable tν = 
 which has the Student’s t distribu-
tion with ν  ≡ ν(n) = (n1 + n2 –2) degrees of freedom. Does t
ν converge to

5. Concepts of Stochastic Convergence
279
some appropriate random variable in distribution as both n1 and n2 → ∞ in
such a way that n1/n2 → k(> 0)?
5.4.2 Let X1, ..., Xn be iid N(µ1, σ2), Y1, ..., Yn be iid N(µ2, 3σ2) where
–∞ < µ1, µ2 < ∞, 0 < σ < ∞. Also suppose that the X’s and Y’s are
independent. Denote for n ≥ 2, 
(i)
Show that 
 is distributed as N (0, 1);
(ii) Show that (n – 1)Tn/σ2 is distributed as 
;
(iii) Are Vn, Tn independent?
(iv) Show that 
 is distributed as
the Student’s t2(n–1);
(v)
Show that 
;
(vi) Show that 
 as n → ∞.
5.4.3 Suppose that X1, ..., Xm are iid exponential with mean β(> 0), Y1, ...,
Yn are iid exponential with mean η(> 0), and that the X’s are independent of
the Y’s. Define Tm, n = . Then,
(i)
Show that Tm, n is distributed as β/η F2m, 2n;
(ii)
Determine the asymptotic distribution of Tm, n as n → ∞, when m is
kept fixed;
(iii) Determine the asymptotic distribution of Tm, n as m → ∞, when n is
kept fixed.
5.4.4 Verify the limiting ratio of the gamma functions in (5.4.6).
5.4.5 Verify the limiting ratio of the gamma functions in (5.4.14).
5.4.6 Show that the percentile point d ≡ d(ν2, α) given by (5.4.17) is a
strictly decreasing function of ν2 whatever be fixed α ∈ (0, 1). {Hint: Take the
derivative of d with respect to ν2 and show that this derivative is negative. This
approach is not entirely fair because ν2 is after all a discrete variable ∈ {1, 2, 3,
...}. A rigorous approach should investigate the behavior of d(ν2 + 1, α) – d(ν2,
α) for ν2 ∈ {1, 2, 3, ...}. The “derivative approach” however should drive the
point home.}
5.4.7 Consider the random variables T1, ..., Tp defined in (4.6.12) whose
joint distribution was the multivariate t, denoted by Mtp(ν, Σ). Derive the limit-
ing distribution of (T1, ..., Tp) as ν → ∞. Show that the pdf of the Mtp(ν, Σ)
distribution given by (4.6.13) converges to the pdf of the corresponding limit-
ing random variable U as ν → ∞. Identify this random variable U by name.
{Hint: In the second part, use techniques similar to those used in Section 5.4.4.}

280
5. Concepts of Stochastic Convergence
5.4.8 Consider the random variables U1, ..., Up defined in (4.6.15) whose
joint distribution was the multivariate F, denoted by MFp(ν0, ν1, ..., νp). De-
rive the limiting distribution of (U1, ..., Up) as ν0 → ∞ but ν1, ..., νp are held
fixed. Show that the pdf of the MFp(ν0, ν1, ..., νp) distribution given by (4.6.16)
converges to the pdf of the corresponding limiting random variable W as ν0
→ ∞ but ν1, ..., νp are held fixed. Identify this random variable W by name.
{Hint: In the second part, use techniques similar to those used in Section
5.4.4.}
5.4.9 (Exercise 5.4.1 Continued) Suppose that the random vari-
ables Xi1, ..., Xin are iid N(µi, σ2), i = 1, 2, and that the X1j’s are
independent of the X2j’s. With n ≥ 2, let us denote
for i = 1, 2. Consider the random variable 
1, 2. Show that (T1n, T2n) has an appropriate bivariate t distribution,
for all fixed n ≥ 2. Find the limiting distribution of (T1n, T2n) as n →
∞.
5.4.10 (Exercise 5.4.9 Continued) Suppose that the random vari-
ables Xi1, ..., Xin are iid N(µi, σ2), i = 1, ..., 4, and that the Xij’s are
independent of the Xlj’s for all i ≠ l = 1, ..., 4. With n ≥ 2, let us
denote
for i = 1, ..., 4. Consider the random variable 
1, ..., 4. Show that (T1n, ..., T4n) has an appropriate four-dimensional t
distribution, for all fixed n ≥ 2. Find the limiting distribution of (T1n, ...,
T4n) as n → ∞.
5.4.11 (Exercise 5.4.10 Continued) Suppose that the random vari-
ables Xi1, ..., Xinz are iid N(µi, σ2), i = 1, ..., 4, and that the Xij’s are
independent of the Xlj’s for all i ≠ l = 1, ..., 4. With ni ≥ 2, let us
denote
for i = 1, ..., 4. Suppose that n1 = n2 = n3 = k and n4 = n. Consider the
random variable 
. Show that (T1n, T2n, T3n) has
an appropriate three-dimensional F distribution, for all fixed n ≥ 2.
Find the limiting distribution of (T1n, T2n, T3n) as n → ∞.

6
Sufficiency, Completeness, and
Ancillarity
6.1 Introduction
Sir Ronald Aylmer Fisher published several path-breaking articles in the 1920’s
which laid the foundation of statistical inference. Many fundamental concepts
and principles of statistical inference originated in the works of Fisher. The
most exciting thing about these concepts is that these are still alive, well, and
indispensable. Perhaps the deepest of all statistical concepts and principles is
what is known as sufficiency. The concept of sufficiency originated from
Fisher (1920) and later it blossomed further, again in the hands of Fisher
(1922). First we introduce the notion of sufficiency which helps in summa-
rizing data without any loss of information.
Consider a scenario like this. From past experience, suppose that a market
analyst postulates the monthly income per household in a small town to be
normally distributed with the unknown population mean µ and population
standard deviation σ = $800. In order to guess the unknown µ, twenty one
households are randomly selected, independently of each other, from the popu-
lation, leading to the observations X1 = x1, X2 = x2, ..., X21 = x21. At this point,
the market analyst may be debating between the appropriateness of using ,
the observed value of the sample mean 
, as the guess or using x21:11, the
observed value of the sample median X21:11 instead. Now, the question is this:
which guess should the market analyst use in this situation? Since the income
distribution is assumed normal, 
 should be used because  
 is sufficient for
µ as we will see later. On the other hand, the sample median, X21:11 is not
sufficient for µ. Once we develop the idea of sufficiency in Section 6.2, it will
be clear that the summary obtained via  
 preserves all the information con-
tained in the whole data X = (X1, ..., X21), whereas in the alternative summary
obtained via X21:11, some information from the data X will be lost. The com-
mon phrases such as the estimator, statistic, information, and sufficiency
would all be defined shortly.
Section 6.2 includes two ways to find sufficient statistics in a statistical
model. The first method involves the direct calculation of the conditional
distribution of the data given the value of a particular statistic, while the
281

282
6. Sufficiency, Completeness, and Ancillarity
second approach consists of the classical Neyman factorization of a likeli-
hood function. We include specific examples to highlight some approaches to
verify whether a statistic is or is not sufficient.
In Section 6.3, the notion of minimal sufficiency is introduced and a fun-
damental result due to Lehman and Scheffé (1950) is discussed. This result
helps us in locating, in some sense, the best sufficient statistic, if it exists. We
had seen in Section 3.8 that many standard statistical models such as the
binomial, Poisson, normal, gamma and several others belong to an exponen-
tial family. It is often a simple matter to locate the minimal sufficient statistic
and its distribution in an exponential family. One gets a glimpse of this in the
Theorems 6.3.3-6.3.4.
The Section 6.4 provides the idea of quantifying information in both one-
and two-parameter situations, but we do so in a fairly elementary fashion. By
means of examples, we show that the information contained in the whole data
is indeed preserved by the sufficient statistics. In the one-parameter case, we
compare the information content in a non-sufficient statistic with that in the
data and find the extent of the lost information if a non-sufficient statistic is
used as a summary.
The topic of ancillarity is discussed in Section 6.5, again moving deeper
into the concepts and highlighting the fact that ancillary statistics can be use-
ful in making statistical inferences. We include the location, scale, and loca-
tion-scale families of distributions in Section 6.5.1. The Section 6.6 intro-
duces the concept of completeness and discusses some of the roles complete
sufficient statistics play within the realm of statistical inference. Section 6.6.2
highlights Basu’s Theorem from Basu (1955a).
6.2 Sufficiency
Suppose that we start a statistical investigation with observable iid random
variables X1, ..., Xn, having a common pmf or pdf f(x), x ∈ χ, the domain
space for x. Here, n is the sample size which is assumed known. Practically
speaking, we like to think that we are going to observe X1, ..., Xn from a
population whose distribution is approximated well by f(x). In the example
discussed in the introduction, the market analyst is interested in the income
distribution of households per month which is denoted by f(x), with some
appropriate space χ for x. The income distribution may be indexed by some
parameter (or parameter vector) θ (or θθθθθ) which captures important features
of the distribution. A practical significance of indexing with the parameter θ
(or θθθθθ) is that once we know the value of θ (or θθθθθ), the population distribution
f(x) would then be completely specified.

6. Sufficiency, Completeness, and Ancillarity
283
For simplicity, however, let us assume first that θ is a single parameter and
denote the population pmf or pdf by f(x; θ) so that the dependence of the
features of the underlying distribution on the parameter θ becomes explicit. In
classical statistics, we assume that this parameter θ is fixed but otherwise
unknown while all possible values of θ ∈ Θ, called the parameter space, Θ ⊆
ℜ, the real line. For example, in a problem, we may postulate that the X’s are
distributed as N(µ, σ2) where µ is the unknown parameter, –∞ < µ < ∞, but
σ(> 0) is known. In this case we may denote the pdf by f(x; θ) with θ = µ
while the parameter space Θ = ℜ. But if both the parameters µ and σ2 are
unknown, the population density would be denoted by f(x; θθθθθ) where the pa-
rameter vector is θθθθθ = (µ, σ2) ∈ Θ = ℜ×ℜ+. This is the idea behind indexing a
population distribution by the unknown parameters in general.
From the context, it should become clear whether the unknown
parameter is real valued (θ) or vector valued (θθθθθ).
Consider again the observable real valued iid random variables X1, ..., Xn
from a population with the common pmf or pdf f(x; θ) where θ(∈ Θ) is the
unknown parameter. Our quest for gaining information about the unknown
parameter θ can safely be characterized as the core of statistical inference.
The data, of course, has all the information about θ even though we have not
yet specified how to quantify this “information.” In Section 6.4, we address
this. A data can be large or small, and it may be nice or cumbersome, but it is
ultimately incumbent upon the experimenter to summarize this data so that all
interesting features are captured by its summary. That is, the summary should
preferably have the exact same “information” about the unknown parameter θ
as does the original data. If one can prepare such a summary, then this would
be as good as the whole data as far as the information content regarding the
unknown parameter θ is concerned. We would call such a summary suffi-
cient for θ and make this basic idea more formal as we move along.
Definition 6.2.1 Any observable real or vector valued function T ≡
T(X1, ..., Xn), of the random variables X1, ..., Xn is called a statistic.
Some examples of statistics are 
, X1 (X2 – Xn:n), 
 S2 and so on.
As long as the numerical evaluation of T, having observed a specific data
X1 = x1, ..., Xn = xn, does not depend on any unknown quantities, we will
call T a statistic. Supposing that X1, ..., Xn are iid N(µ, σ2) where µ is
unknown, but σ is known, T = 
 is a statistic because the value of T
associated with any observed data x1, ..., xn can be explicitly calculated. In
the same example, however, the standardized form of 
, namely 

284
6. Sufficiency, Completeness, and Ancillarity
µ)/σ, is not a statistic because it involves the unknown parameter µ, and
hence its value associated with any observed data x1, ..., xn can not be calcu-
lated.
Definition 6.2.2 A real valued statistic T is called sufficient (for the un-
known parameter θ) if and only if the conditional distribution of the random
sample X = (X1, ..., Xn) given T = t does not involve θ, for all t ∈ 
, the
domain space for T.
In other words, given the value t of a sufficient statistic T, conditionally
there is no more “information” left in the original data regarding the unknown
parameter θ. Put another way, we may think of X trying to tell us a story
about θ, but once a sufficient summary T becomes available, the original
story then becomes redundant. Observe that the whole data X is always suf-
ficient for θ in this sense. But, we are aiming at a “shorter” summary statistic
which has the same amount of information available in X. Thus, once we find
a sufficient statistic T, we will focus only on the summary statistic T. Before
we give other details, we define the concept of joint sufficiency of a vector
valued statistic T for an unknown parameter θ.
Definition 6.2.3 A vector valued statistic T ≡ (T1, ..., Tk) where Ti ≡ Ti(X1,
..., Xn), i = 1, ..., k, is called jointly sufficient (for the unknown parameter θ)
if and only if the conditional distribution of X = (X1, ..., Xn) given T = t does
not involve θ, for all t ∈ 
 ⊆ ℜk.
The Section 6.2.1 shows how the conditional distribution of X given T =
t can be evaluated. The Section 6.2.2 provides the celebrated Neyman factor-
ization which plays a fundamental role in locating sufficient statistics.
6.2.1
The Conditional Distribution Approach
With the help of examples, we show how the Definition 6.2.2 can be applied
to find sufficient statistics for an unknown parameter θ.
Example 6.2.1 Suppose that X1, ..., Xn are iid Bernoulli(p), where p is the
unknown parameter, 0 < p < 1. Here, χ = {0, 1}, θ = p, and Θ = (0, 1). Let us
consider the specific statistic 
. Its values are denoted by t ∈  =
{0, 1, 2, ..., n}. We verify that T is sufficient for p by showing that the
conditional distribution of (X1, ..., Xn) given T = t does not involve p, what-
ever be t ∈ . From the Examples 4.2.2-4.2.3, recall that T has the Binomial(n,
p) distribution. Now, we obviously have:
But, when 
, since 
 is a subset of B = {T = t},

6. Sufficiency, Completeness, and Ancillarity
285
we can write
Thus, one has
which is free from p. In other words, 
 is a sufficient statistic for the
unknown parameter p. !
Example 6.2.2 Suppose that X1, ..., Xn are iid Poisson(λ) where λ is the
unknown parameter, 0 < λ < ∞. Here, χ = {0, 1, 2, ...}, θ = λ, and Θ = (0, ∞).
Let us consider the specific statistic 
. Its values are denoted by t
∈ 
 = {0, 1, 2, ...}. We verify that T is sufficient for λ by showing that the
conditional distribution of (X1, ..., Xn) given T = t does not involve λ, what-
ever be t ∈ 
. From the Exercise 4.2.2 recall that T has the Poisson(nλ)
distribution. Now, we obviously have:
But, when 
, since 
 is a subset of B = {T = t},
we can write
Hence, one gets

286
6. Sufficiency, Completeness, and Ancillarity
which is free from λ. In other words, 
 is a sufficient statistic for the
unknown parameter λ. !
Example 6.2.3 Suppose that X has the Laplace or double exponential pdf
given by f(x; θ) = 1/2θe–|x|/θI(–∞ < x < ∞) where θ (> 0) is an unknown
parameter. Let us consider the statistic T = | X |. The difference between X
and T is that T provides the magnitude of X, but not its sign. Conditionally
given T = t (> 0), X can take one of the two possible values, namely t or –t,
each with probability 1/2. In other words, the conditional distribution of X
given T = t does not depend on the unknown parameter θ. Hence, | X | is a
sufficient statistic for θ. !
Example 6.2.4 Suppose that X1, X2 are iid N(θ, 1) where θ is unknown, –
∞ < θ < ∞. Here, χ = ℜ and Θ = ℜ. Let us consider the specific statistic T =
X1 + X2. Its values are denoted by t ∈  = ℜ. Observe that the conditional pdf
of (X1, X2), at (x1, x2) would be zero if x1 + x2 ≠ t when T = t has been
observed. So we may work with data points (x1, x2) such that x1 + x2 = t once
T = t is observed. Given T = t, only one of the two X’s is a free-standing
variable and so we will have a valid pdf of one of the X’s. Let us verify that T
is sufficient for θ by showing that the conditional distribution of X1 given T =
t does not involve θ. Now following the Definition 4.6.1 of multivariate nor-
mality and the Example 4.6.1, we can claim that the joint distribution of (X1,
T) is N2(θ, 2θ, 1, 2, 
), and hence the conditional distribution of X1 given
T = t is normal with its mean = θ + 
 = 1/2t and conditional
variance = 1 – ( 
)2 = 1/2, for all t ∈ ℜ. Refer to the Theorem 3.6.1 for the
expressions of the conditional mean and variance. This conditional distribu-
tion is clearly free from θ. In other words, T is a sufficient statistic for θ. !
How can we show that a statistic is not sufficient for θ?
Discussions follow.
If T is not sufficient for θ, then it follows from the Definition 6.2.2 that the
conditional pmf or pdf of X1, ..., Xn given T = t must depend on the unknown
parameter θ, for some possible x1, ..., xn and t.
In a discrete case, suppose that for some chosen data x1, ..., xn,
the conditional probability P{X1 = x1, ..., Xn = xn | T = t},
involves the parameter θ. Then, T can not be sufficient for θ.
Look at Examples 6.2.5 and 6.2.7.
Example 6.2.5 (Example 6.2.1 Continued) Suppose that X1, X2, X3 are
iid Bernoulli(p) where p is unknown, 0 < p < 1. Here, χ = {0, 1}, θ = p,

6. Sufficiency, Completeness, and Ancillarity
287
and Θ = (0, 1). We had verified that the statistic 
 was sufficient
for θ. Let us consider another statistic U = X1X2 + X3. The question is whether
U is a sufficient statistic for p. Observe that
Now, since {X1 = 1 n X2 = 0 n X3 = 0} is a subset of {U = 0}, we have
This conditional probability depends on the true value of p and so we claim
that the statistic U is not sufficient for p. That is, after the completion of the
n trials of the Bernoulli experiment, if one is merely told the observed value of
the statistic U, then some information about the unknown parameter p would
be lost.!
In the continuous case, we work with the same basic idea.
If for some data x1, ..., xn, the conditional pdf given T = t,
 fX|T=t(x1, ..., xn), involves the parameter θ, then the statistic T
can not be sufficient for θ. Look at the Example 6.2.6.
Example 6.2.6 (Example 6.2.4 Continued) Suppose that X1, X2 are iid
N(θ, 1) where θ is unknown, –∞ < θ < ∞. Here, χ = ℜ and Θ = ℜ. Let us
consider a statistic, for example, T = X1 + 2X2 while its values are denoted
by t ∈  = ℜ. Let us verify that T is not sufficient for θ by showing that the
conditional distribution of X1 given T = t involves θ. Now, following the
Definition 4.6.1 and the Example 4.6.1, we can claim that the joint distribu-
tion of (X1, T) is N2(θ, 3θ, 1, 5, 
), and hence the conditional distribu-
tion of X1 given T = t is normal with its mean = θ + 
  
 (t – 3θ) =
1/5(t + 2θ) and variance = 1 – ( 
)
2 = 4/5, for t ∈ ℜ. Refer to the
Theorem 3.6.1 as needed. Since this conditional distribution depends on
the unknown parameter θ, we conclude that T is not sufficient for θ. That

288
6. Sufficiency, Completeness, and Ancillarity
is, merely knowing the value of T after the experiment, some information
about the unknown parameter θ would be lost. !
Example 6.2.7 Suppose that X has the exponential pdf given by f(x) = λe–
λxI(x > 0) where λ(> 0) is the unknown parameter. Instead of the original data,
suppose that we are only told whether X ≤ 2 or X > 2, that is we merely
observe the value of the statistic T ≡ I(X > 2). Is the statistic T sufficient for
λ? In order to check, let us proceed as follows: Note that we can express
P{X > 3 | T = 1} as
which depends on λ and hence T is not a sufficient statistic for λ. !
The methods we pursued in the Examples 6.2.1-6.2.7 closely followed
the definition of a sufficient statistic. But, such direct approaches to verify
the sufficiency or non-sufficiency of a statistic may become quite cumber-
some. More importantly, in the cited examples we had started with specific
statistics which we could eventually prove to be either sufficient or non-
sufficient by evaluating appropriate conditional probabilities. But, what is
one supposed to do in situations where a suitable candidate for a sufficient
statistic can not be guessed readily? A more versatile technique follows.
6.2.2
The Neyman Factorization Theorem
Suppose that we have at our disposal, observable real valued iid random vari-
ables X1, ..., Xn from a population with the common pmf or pdf f(x; θ). Here,
the unknown parameter is θ which belongs to the parameter space Θ.
Definition 6.2.4 Consider the (observable) real valued iid random vari-
ables X1, ..., Xn from a population with the common pmf or pdf f(x; θ), where
the unknown parameter θ ∈ Θ. Once we have observed Xi = xi, i = 1, ..., n, the
likelihood function is given by
In the discrete case, L(θ) stands for Pθ{X1 = x1 ∩ ... ∩ Xn = xn}, that is the
probability of the data on hand when θ obtains. In the continuous case, L(θ)
stands for the joint pdf at the observed data point (x1, ..., xn) when θ obtains.
It is not essential however for the X’s to be real valued or that they be
iid. But, in many examples, they will be so. If the X’s happen to be vector
valued or if they are not iid, then the corresponding joint pmf or pdf of

6. Sufficiency, Completeness, and Ancillarity
289
Xi = xi, i = 1, ..., n, would stand for the corresponding likelihood function
L(θ). We will give several examples of L(θ) shortly.
One should note that once the data {xi; i = 1, ..., n} has been observed,
there are no random quantities in (6.2.4), and so the likelihood L(.) is simply
treated as a function of the unknown parameter θ alone.
The sample size n is assumed known and fixed before
the data collection begins.
One should note that θ can be real or vector valued in this general discus-
sion, however, let us pretend for the time being that θ is a real valued param-
eter. Fisher (1922) discovered the fundamental idea of factorization. Neyman
(1935a) rediscovered a refined approach to factorize the likelihood function in
order to find sufficient statistics for θ. Halmos and Savage (1949) and Bahadur
(1954) gave more involved measure-theoretic treatments.
Theorem 6.2.1 (Neyman Factorization Theorem) Consider the likeli-
hood function L(θ) from (6.2.4). A real valued statistic T = T(X1, ..., Xn) is
sufficient for the unknown parameter θ if and only if the following factoriza-
tion holds:
where the two functions g(.; θ) and h(.) are both nonnegative, h(x1, ..., xn) is
free from θ, and g(T(x1, ..., xn);θ) depends on x1, ..., xn only through the
observed value T(x1, ..., xn) of the statistic T.
Proof For simplicity, we will provide a proof only in the discrete case. Let
us write X = (X1, ..., Xn) and x = (x1, ..., xn). Let the two sets A and B
respectively denote the events X = x and T(X) = T(x), and observe that A ⊆ B.
Only if part: Suppose that T is sufficient for θ. Now, we write
Comparing (6.2.5)-(6.2.6), let us denote g(T(x1, ..., xn);θ) = Pθ{T(X) = T(x)}
and h(x1, ..., xn) = Pθ{X = x |T(X) = T(x)}. But, we have assumed that T is
sufficient for θ and hence by the Definition 6.2.2 of sufficiency, the condi-
tional probability Pθ{X = x |T(X) = T(x)} cannot depend on the parameter θ.
Thus, the function h(x1, ..., xn) so defined may depend only on x1, ..., xn.
The factorization given in (6.2.5) thus holds. The “only if” part is now
complete.¿

290
6. Sufficiency, Completeness, and Ancillarity
If part: Suppose that the factorization in (6.2.5) holds. Let us denote the
pmf of T by p(t; θ). Observe that the pmf of T is given by p(t; θ) = Pθ{T(X)
= t} = 
. It is easy to see that
For all x ∈ χ such that T(x) = t and p(t; θ) ≠ 0, we can express Pθ{X = x
|T(X) = t} as
because of factorization in (6.2.5). Hence, one gets
where q(x) does not depend upon θ. Combining (6.2.7)-(6.2.8), the proof of
the “if part” is now complete. !
In the statement of the Theorem 6.2.1, notice that we do not
demand that g(T(x1, ..., xn);θ) must be the pmf or the pdf of
T(X1, ..., Xn). It is essential, however, that the function
h(x1, ..., xn) must be entirely free from θ.
It should be noted that the splitting of L(θ) may not be unique, that is
there may be more than one way to determine the function h(.) so that
(6.2.5) holds. Also, there can be different versions of the sufficient statis-
tics.
Remark 6.2.1 We mentioned earlier that in the Theorem 6.2.1, it was
not essential that the random variables X1, ..., Xn and the unknown param-
eter θ be all real valued. Suppose that X1, ..., Xn are iid p-dimensional ran-
dom variables with the common pmf or pdf f(x; θθθθθ) where the unknown
parameter θθθθθ is vector valued, θθθθθ ∈ Θ ⊆ ℜq. The Neyman Factorization Theo-
rem can be stated under this generality. Let us consider the likelihood
function, 

6. Sufficiency, Completeness, and Ancillarity
291
Example 6.2.8 (Example 6.2.1 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p) where p is unknown, 0 < p < 1. Here, χ = {0, 1}, θ = p, and Θ =
(0, 1). Then,
which looks like the factorization provided in (6.2.5) where 
 and h(x1, ..., xn) = 1 for all x1, ..., xn ∈ {0, 1}. Hence,
the statistic T = T(X1, ..., Xn) = 
 is sufficient for p. From (6.2.10), we
could instead view L(θ) = g(x1, ..., xn; p) h(x1, ..., xn) with, say, g(x1, ..., xn; p)
= 
 and h(x1, ..., xn) = 1. That is, one could claim that X =
(X1, ..., Xn) was sufficient too for p. But, 
 provides a significantly
reduced summary compared with X, the whole data. We will have more to
say on this in the Section 6.3.!
Example 6.2.9 (Example 6.2.2 Continued) Suppose that X1, ..., Xn are iid
Poisson(λ) where λ is unknown, 0 < λ < ∞. Here, χ = {0, 1, 2, ...}, θ = λ,
and Θ = (0, ∞). Then,
which looks like the factorization provided in (6.2.5) with 
 and h(x1, ..., xn) = 
 for all x1, ..., xn ∈ {0, 1, 2, ...}.
Hence, the statistic T = T(X1, ..., Xn) = 
 is sufficient for λ. Again, from
(6.2.11) one can say that the whole data X is sufficient too, but 
 pro-
vides a significantly reduced summary compared with X. !
Example 6.2.10 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ and σ are
both assumed unknown, –∞ < µ < ∞, 0 < σ < ∞. Here, we may denote θθθθθ = (µ,
σ) so that χ = ℜ and Θ = ℜ × ℜ+. We wish to find jointly sufficient statistics
for θθθθθ. Now, we have
which looks like the factorization provided in (6.2.9) where one writes
 and

292
6. Sufficiency, Completeness, and Ancillarity
h(x1, ..., xn) = 
 for all (x1, ..., xn) ∈ ℜn. In other words, T = T(X1, ...,
Xn) = 
 is jointly sufficient for (µ, σ2). !
If T is a sufficient statistic for θθθθθ, then any statistic T’ which
is a one-to-one function of T is also sufficient for θθθθθ.
Example 6.2.11 (Example 6.2.10 Continued) We have 
,
, and so it is clear that the transfor-
mation from 
 to T’ = (
, S2) is one-to-one. Hence, in
the Example 6.2.10, we can also claim that (
, S2) is jointly sufficient for θθθθθ =
(µ, σ2). !
Let us emphasize another point. Let T be a sufficient statistic for
θθθθθ. Consider another statistic T’, an arbitrary function of T. Then,
the statistic T’ itself is not necessarily sufficient for θθθθθ.
Look at the earlier Example 6.2.7.
An arbitrary function of a sufficient statistic T need not be sufficient for θθθθθ.
Suppose that X is distributed as N(θ, 1) where –∞ < θ < ∞ is the unknown
parameter. Obviously, T = X is sufficient for θ. One should check that the
statistic T’ = | X |, a function of T, is not sufficient for θ.
Remark 6.2.2 In a two-parameter situation, suppose that the Neyman
factorization (6.2.9) leads to a statistic T = (T1, T2) which is jointly sufficient
for θθθθθ = (θ1, θ2). But, the joint sufficiency of T should not be misunderstood to
imply that T1 is sufficient for θ1 or T2 is sufficient for θ2.
From the joint sufficiency of the statistic T = (T1, ..., Tp)
for θθθθθ = (θ1, ..., θp), one should not be tempted to claim that the
statistic Ti is componentwise sufficient for θi, i = 1, ..., p.
Look at the Example 6.2.12. In some cases, the statistic T
and θθθθθ may not even have the same number of components!
Example 6.2.12 (Example 6.2.11 Continued) In the N(µ, σ2) case when
both the parameters are unknown, recall from the Example 6.2.11 that (
,
S2) is jointly sufficient for (µ, σ2). This is very different from trying to answer
a question like this: Is 
 sufficient for µ or is S2 sufficient for σ2? We can
legitimately talk only about the joint sufficiency of the statistic (
, S2) for θθθθθ
= (µ, σ2).

6. Sufficiency, Completeness, and Ancillarity
293
To appreciate this fine line, let us think through the example again and
pretend for a moment that one could claim componentwise sufficiency. But,
since (
, S2) is jointly sufficient for (µ, σ2), we can certainly claim that
(S2, 
) is also jointly sufficient for θθθθθ = (µ, σ2). Now, how many readers
would be willing to push forward the idea that componentwise, S2 is suffi-
cient for µ or 
 is sufficient for σ2! Let us denote U = 
 and let g(u; n) be
the pdf of U when the sample size is n.
Figure 6.2.1. Two PDF’s of Where n = 16
In the Figure 6.2.1, the two pdf’s of 
 when µ = 0 and n = 16, for example,
are certainly very different from one another. Relatively speaking, the darker
pdf gives the impression that σ is “small” whereas the lighter pdf gives the
impression that σ is “large”. There should be no doubt that 
 provides some
information about σ2. In fact 
 has some information about both µ and σ2,
whereas S2 has information about σ2 alone. !
Example 6.2.13 Suppose that X1, ..., Xn are iid Uniform(0, θ), where θ (>
0) is unknown. Here, χ = (0, θ) and Θ = ℜ+. We wish to find a sufficient
statistic for θ. Now,
where xn:1 and xn:n are respectively the observed smallest and largest order
statistics. The last step in (6.2.13) looks exactly like the Neyman factorization
provided in (6.2.5) where g(xn:n; θ) = θ–n I (0 < xn:n < θ) and h(x1, ..., xn) = I(0
< xn:1 < xn:n) for all x1, ..., xn ∈ (0, θ). Hence, the statistic T = T(X1, ..., Xn) =
Xn:n is sufficient for θ. !
It is not crucial that the X’s be iid for the Neyman factorization
of the likelihood function to lead to a (jointly) sufficient
statistic (T)T. Look at the Example 6.2.14.

294
6. Sufficiency, Completeness, and Ancillarity
In many examples and exercises the X’s are often assumed iid. But, if the
X’s are not iid, then all we have to do is to carefully write down the likelihood
function L(θ) as the corresponding joint pmf or pdf of the random variables
X1, ..., Xn. The Neyman factorization would then hold.
Example 6.2.14 Suppose that X1, X2 are independent random variables,
with their respective pdf’s f(x1; θ) = θe–θx
1 I(0 < x1 < ∞) and g(x2; θ) = 2θe–2θx
2
I(0 < x2 < ∞), where θ > 0 is an unknown parameter. For 0 < x1, x2 < ∞, the
likelihood function is given by the joint pdf, namely
From (6.2.14) it is clear that the Neyman factorization holds and hence the
statistic T = X1 + 2X2 is sufficient for θ. Here, X1, X2 are not identically
distributed, and yet the factorization theorem has been fruitful.!
The following result shows a simple way to find sufficient statistics when
the pmf or the pdf belongs to the exponential family. Refer back to the Sec-
tion 3.8 in this context. The proof follows easily from the factorization (6.2.9)
and so we leave it out as the Exercise 6.2.15.
Theorem 6.2.2 (Sufficiency in the Exponential Family) Suppose that
X1, ..., Xn are iid with the common pmf or the pdf belonging to the k-param-
eter exponential family defined by (3.8.4), namely
with appropriate forms for g(x) ≥ 0, a(θθθθθ) ≥ 0, bi(θθθθθ) and Ri(x), i = 1, ..., k.
Suppose that the regulatory conditions stated in (3.8.5) hold. Denote the sta-
tistic 
  j = 1, ..., k. Then, the statistic T = (T1, ..., Tk) is
jointly sufficient for θθθθθ.
The sufficient statistics derived earlier in the Examples 6.2.8-6.2.11 can
also be found directly by using the Theorem 6.2.2. We leave these as the
Exercise 6.2.14.
6.3 Minimal Sufficiency
We noted earlier that the whole data X must always be sufficient for the
unknown parameter θθθθθ. But, we aim at reducing the data by means of sum-
mary statistics in lieu of considering X itself. From the series of examples
6.2.8-6.2.14, we found that the Neyman factorization provided sufficient

6. Sufficiency, Completeness, and Ancillarity
295
statistics which were substantially “reduced” compared with X. As a prin-
ciple, we should use the “shortest sufficient” summary in lieu of handling the
original data. Two pertinent questions may arise: What is a natural way to
define the “shortest sufficient” summary statistic? The next question is how
to get hold of such a “shortest sufficient” summary, if indeed there is one?
Lehmann and Scheffé (1950) developed a precise mathematical formula-
tion of the concept known as minimal sufficiency and they gave a technique
that helps to locate minimal sufficient statistics. Lehmann and Scheffé (1955,
1956) included important followups.
Definition 6.3.1 A statistic T is called minimal sufficient for the unknown
parameter θθθθθ or simply minimal sufficient if and only if
(i)
T is sufficient for θθθθθ, and
(ii)
T is minimal or “shortest” in the sense that T is a function
of any other sufficient statistic.
Let us think about this concept for a moment. We do want to summarize
the whole data X by reducing it to some appropriate statistic such as 
, the
median (M), or the histogram, and so on. Suppose that in a particular situa-
tion, the summary statistic T = (
, M) turns out to be minimal sufficient for
θθθθθ. Can we reduce this summary any further? Of course, we can. We may
simply look at, for example, T1 = 
 or T2 = M or T3 = 
 1/2 ( + M). Can T1,
T2, or T3 individually be sufficient for θθθθθ? The answer is no, none of these
could be sufficient for θθθθθ. Because if, for example T1 by itself was sufficient
for θθθθθ, then T = (
, M) would have to be a function of T1 in view of the
requirement in part (ii) of the Definition 6.3.1. But, T = (
, M) can not be a
function of T1 because we can not uniquely specify the value of T from our
knowledge of the value of T1 alone. A minimal sufficient summary T can not
be reduced any further to another sufficient summary statistic. In this sense,
a minimal sufficient statistic T may be looked upon as the best sufficient
statistic.
In the Definition 6.3.1, the part (i) is often verified via Neyman factoriza-
tion, but the verification of part (ii) gets more involved. In the next subsec-
tion, we state a theorem due to Lehmann and Scheffé (1950) which provides
a direct approach to find minimal sufficient statistics for θθθθθ.
6.3.1
The Lehmann-Scheffé Approach
The following theorem was proved in Lehmann and Scheffé (1950). This
result is an essential tool to locate a minimal sufficient statistic when it exists.
Its proof, however, requires some understanding of the correspondence

296
6. Sufficiency, Completeness, and Ancillarity
between a statistic and so called partitions it induces on the sample space.
Let us look at the original data X = (X1, ..., Xn) where x = (x1, ..., xn) ∈ χn.
Consider a statistic T ≡ T(X1, ..., Xn), that is T is a mapping from χn onto some
space 
 say. For t ∈ 
, let χt = {x : x ∈ χn such that T(x) = t} which are
disjoint subsets of χn and also χn = ?t∈ χt. In other words, the collection of
subsets {χt : t ∈ 
} forms a partition of the space χn. Often, {χt : t ∈ } is
also called the partition of χn induced by the statistic T.
Theorem 6.3.1 (Minimal Sufficient Statistics) Let us consider the func-
tion  
, the ratio of the likelihood func-
tions from (6.2.9) at x and y, where θθθθθ is the unknown parameter and x, y ∈ χn.
Suppose that we have a statistic T ≡ T(X1, ..., Xn) = (T1, ..., Tk) such that the
following conditions hold:
Then, the statistic T is minimal sufficient for the parameter θθθθθ.
Proof We first show that T is a sufficient statistic for θθθθθ and then we verify
that T is also minimal. For simplicity, let us assume that f(x; θθθθθ) is positive for
all x ∈ χn and θθθθθ.
Sufficiency part: Start with {χt : t ∈ 
} which is the partition of χn in-
duced by the statistic T. In the subset χt, let us select and fix an element xt. If
we look at an arbitrary element x ∈ χn, then this element x belongs to χt for
some unique t so that both x and xt belong to the same set χt. In other words,
one has T(x) = T(xt). Thus, by invoking the “if part” of the statement in
(6.3.1), we can claim that h(x, xt; θθθθθ) is free from θθθθθ. Let us then denote h(x)
≡ h(x, xt; θθθθθ), x ∈ χn. Hence, we write
where xt = (xt1, ..., xtn). In view of the Neyman Factorization Theorem, the
statistic T(x) is thus sufficient for θθθθθ.¿
Minimal part: Suppose U = U(X) is another sufficient statistic for θθθθθ. Then,
by the Neyman Factorization Theorem, we can write

6. Sufficiency, Completeness, and Ancillarity
297
for some appropriate g0(.; θθθθθ) and h0(.). Here, h0(.) does not depend upon θθθθθ.
Now, for any two sample points x = (x1, ..., xn), y = (y1, ..., yn) from χn such
that U(x) = U(y), we obtain
Thus, h(x, y; θθθθθ) is free from θθθθθ. Now, by invoking the “only if” part of the
statement in (6.3.1), we claim that T(x) = T(y). That is, T is a function of U.
Now, the proof is complete. !
Example 6.3.1 (Example 6.2.8 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p), where p is unknown, 0 < p < 1. Here, χ = {0, 1}, θ = p, and Θ
= (0, 1). Then, for two arbitrary data points x = (x1, ..., xn) and y = (y1, ..., yn),
both from χ, we have:
From (6.3.2), it is clear that 
 would become free
from the unknown parameter p if and only if 
, that is, if
and only if 
  Hence, by the theorem of Lehmann-Scheffé,
we claim that 
 is minimal sufficient for p. !
We had shown non-sufficiency of a statistic U in the Example 6.2.5.
One can arrive at the same conclusion by contrasting U with
a minimal sufficient statistic. Look at the Examples 6.3.2 and 6.3.5.
Example 6.3.2 (Example 6.2.5 Continued) Suppose that X1, X2, X3 are
iid Bernoulli(p), where p is unknown, 0 < p < 1. Here, χ = {0, 1}, θ = p,
and Θ = (0, 1). We know that the statistic 
 is minimal suffi-
cient for p. Let U = X1 X2 + X3, as in the Example 6.2.5, and the question is
whether U is a sufficient statistic for p. Here, we prove again that the
statistic U can not be sufficient for p. Assume that U is sufficient for p,
and then 
  must be a function of U, by the Definition 6.3.1 of
minimal sufficiency. That is, knowing an observed value of U, we must be
able to come up with a unique observed value of T. Now, the event {U = 0}

298
6. Sufficiency, Completeness, and Ancillarity
consists of the union of possibilities such as {X1 = 0 ∩ X2 = 0 ∩ X3 = 0}, {X1
= 0 ∩ X2 = 1 ∩ X3 = 0}, and {X1 = 1 ∩ X2 = 0 ∩ X3 = 0}. Hence, if the event
{U = 0} is observed, we know then that either T = 0 or T = 1 must be
observed. But, the point is that we cannot be sure about a unique observed
value of T. Thus, T can not be a function of U and so there is a contradiction.
Thus, U can not be sufficient for p. !
Example 6.3.3 (Example 6.2.10 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2), where θθθθθ = (µ, σ) and both µ, σ are unknown, –∞ < µ < ∞, 0 < σ <
∞. Here, χ = ℜ and Θ = ℜ × ℜ+. We wish to find a minimal sufficient statistic
for θθθθθ. Now, for two arbitrary data points x = (x1, ..., xn) and y = (y1, ..., yn),
both from χ, we have:
From (6.3.3), it becomes clear that the last expression would not involve the
unknown parameter θθθθθ = (µ, σ) if and only if 
  as well as
, that is, if and only if 
=
 and
. Hence, by the theorem of Lehmann-Scheffé, we claim
that 
 is minimal sufficient for (µ, σ). !
Theorem 6.3.2 Any statistic which is a one-to-one function of a minimal
sufficient statistic is itself minimal sufficient.
Proof Suppose that a statistic S is minimal sufficient for θθθθθ. Let us consider
another statistic T = h(S) where h(.) is one-to-one. In Section 6.2.2, we
mentioned that a one-to-one function of a (jointly) sufficient statistic is (jointly)
sufficient and so T is sufficient. Let U be any other sufficient statistic for θθθθθ.
Since, S is minimal sufficient, we must have S = g(U) for some g(.). Then,
we obviously have T = h(S) = h(g(U)) = h  g(U) which verifies the minimality
of the statistic T. !
Example 6.3.4 (Example 6.3.3 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2), where θθθθθ = (µ, σ) and both µ, σ are unknown, –∞ < µ < ∞, 0 < σ <
∞. We know that 
 is minimal sufficient for (µ, σ).
Now, (
, S2) being a one-to-one function of T, we can claim that (
, S2) is
also minimal sufficient. !
Example 6.3.5 (Example 6.3.3 Continued) Suppose that X1, X2, X3 are
iid N(µ, σ2) where µ is unknown, but σ is assumed known, –∞ < µ < ∞,

6. Sufficiency, Completeness, and Ancillarity
299
0 < σ < ∞. Here, we write θ = µ, χ = ℜ and Θ = ℜ. It is easy to verify that the
statistic 
 is minimal sufficient for θ. Now consider the statistic U
= X1 X2 + X3 and suppose that the question is whether U is sufficient for θ.
Assume that U is sufficient for θ. But, then 
 must be a function of U,
by the Definition 6.3.1 of minimal sufficiency. That is, knowing an observed
value of U, we must be able to come up with a unique value of T. One can
proceed in the spirit of the earlier Example 6.3.2 and easily arrive at a contra-
diction. So, U cannot be sufficient for θ. !
Example 6.3.6 (Example 6.2.13 Continued) Suppose that X1, ..., Xn are iid
Uniform(0, θ), where θ (> 0) is unknown. Here, χ = (0, θ) and Θ = ℜ+. We
wish to find a minimal sufficient statistic for θ. For two arbitrary data points
x = (x1, ..., xn) and y = (y1, ..., yn), both from χ, we have:
Let us denote a(θ) = I(0 < xn:n < θ)/I(0 < yn:n < θ). Now, the question is
this: Does the term a(θ) become free from θ if and only if xn:n = yn:n?
If we assume that xn:n = yn:n, then certainly a(θ) does not involve θ. It
remains to show that when a(θ) does not involve θ, then we must have xn:n =
yn:n. Let us assume that xn:n ≠ yn:n, and then show that a(θ) must depend on the
value of θ. Now, suppose that xn:n = 2, yn:n = .5. Then, a(θ) = 0, 1 or 0/0 when
θ = 1, 3 or .1 respectively. Clearly, a(θ) will depend upon the value of θ
whenever xn:n ≠ yn:n. We assert that the term a(θ) becomes free from θ if and
only if xn:n = yn:n. Hence, by the theorem of Lehmann-Scheffé, we claim that
T = Xn:n, the largest order statistic, is minimal sufficient for θ. !
Remark 6.3.1 Let X1, ..., Xn be iid with the pmf or pdf f(x; θθθθθ), x ∈ χ ⊆ ℜ,
θθθθθ = (θ1, ..., θk) ∈ Θ ⊆ ℜk. Suppose that a statistic T = T(X) = (T1(X), ...,
Tr(X)) is minimal sufficient for θθθθθ. In general can we claim that r = k? The
answer is no, we can not necessarily say that r = k. Suppose that f(x; θ)
corresponds to the pdf of the N(θ, θ) random variable with the unknown
parameter θ > 0 so that k = 1. The reader should verify that
  is a minimal sufficient for θ so that r = 2. Here,
we have r > k. On the other hand, suppose that X1 is N(µ, σ2) where µ and σ2
are both unknown parameters. In this case one has θθθθθ = (µ, σ2) so that k = 2.
But, T = X1 is minimal sufficient so that r = 1. Here, we have r < k. In many
situations, of course, one would find that r = k. But, the point is that there is
no guarantee that r would necessarily be same as k.
The following theorem provides a useful tool for finding minimal sufficient

300
6. Sufficiency, Completeness, and Ancillarity
statistics within a rich class of statistical models, namely the exponential fam-
ily. It is a hard result to prove. One may refer to Lehmann (1983, pp. 43-44)
or Lehmann and Casella (1998) for some of the details.
Theorem 6.3.3 (Minimal Sufficiency in the Exponential Family) Sup-
pose that X1, ..., Xn are iid with the common pmf or the pdf belonging to the
k-parameter exponential family defined by (3.8.4), namely
with some appropriate forms for g(x) ≥ 0, a(θθθθθ) ≥ 0, bi(θθθθθ) and Ri(x), i = 1, ...,
k. Suppose that the regulatory conditions stated in (3.8.5) hold. Denote the
statistic 
. Then, the statistic T = (T1, ..., Tk) is
(jointly) minimal sufficient for θθθθθ.
The following result provides the nature of the distribution itself of a mini-
mal sufficient statistic when the common pmf or pdf comes from an expo-
nential family. Its proof is beyond the scope of this book. One may refer to
the Theorem 4.3 of Lehmann (1983) and Lemma 8 in Lehmann (1986). One
may also review Barankin and Maitra (1963), Brown (1964), Hipp (1974),
Barndorff-Nielsen (1978), and Lehmann and Casella (1998) to gain broader
perspectives.
Theorem 6.3.4 (Distribution of a Minimal Sufficient Statistic in the
Exponential Family) Under the conditions of the Theorem 6.3.3, the pmf or
the pdf of the minimal sufficient statistic (T1, ..., Tk) belongs to the k-param-
eter exponential family.
In each example, the data X was reduced enormously by the
minimal sufficient summary. There are situations where no
significant data reduction may be possible.
See the Exercise 6.3.19 for specific examples.
6.4 Information
Earlier we have remarked that we wish to work with a sufficient or mini-
mal sufficient statistic T because such a statistic will reduce the data and
preserve all the “information” about θθθθθ contained in the original data. Here,
θθθθθ may be real or vector valued. But, how much information do we have in
the original data which we are trying to preserve? Now our major concern
is to quantify the information content within some data. In order to keep
the deliberations simple, we discuss the one-parameter and two-parameter

6. Sufficiency, Completeness, and Ancillarity
301
situations separately. The notion of the information about an unknown param-
eter θ contained in the data was introduced by F. Y. Edgeworth in a series of
papers, published in the J. Roy. Statist. Soc., during 1908-1909. Fisher (1922)
articulated the systematic development of this concept. The reader is referred
to Efron’s (1998, p.101) recent commentaries on (Fisher) information.
6.4.1
One-parameter Situation
Suppose that X is an observable real valued random variable with the pmf or
pdf f (x; θ) where the unknown parameter θ ∈ Θ, an open subinterval of ℜ,
while the χ space is assumed not to depend upon θ. We assume throughout
that the partial derivative 
 (x; θ) is finite for all x ∈ χ, θ ∈ Θ. We also
assume that we can interchange the derivative (with respect to θ) and the
integral (with respect to x).
Definition 6.4.1 The Fisher information or simply the information about
θ, contained in the data, is given by
Example 6.4.1 Let X be Poisson(λ), λ > 0. Now,
which implies that 
. Thus, we have
since Eλ[(X – λ)2] = V(X) = λ. That is, as we contemplate having larger and
larger values of λ, the variability built in X increases, and hence it seems
natural that the information about the unknown parameter λ contained in the
data X will go down further and further.
Example 6.4.2 Let X be N(µ, σ2) where µ ∈ (–∞, ∞) is the unknown pa-
rameter. Here, σ ∈ (0, ∞) is assumed known. Now,
which implies that 
. Thus we have

302
6. Sufficiency, Completeness, and Ancillarity
since Eµ[(X – µ)2] = V(X) = σ2. That is, as we contemplate having larger and
larger values of σ, the variability built in X increases, and hence it seems
natural that the information about the unknown parameter µ contained in the
data X will go down further and further. !
The following result quantifies the information about the unknown param-
eter θ contained in a random sample X1, ..., Xn of size n.
Theorem 6.4.1 Suppose that X1, ..., Xn are iid with the common pmf or
pdf given by f(x; θ). We denote 
, the infor-
mation contained in the observation X1. Then, the information IX(θ), con-
tained in the random sample X = (X1, ..., Xn), is given by
Proof Denote the observed data X = (x1, ..., xn) and rewrite the likelihood
function from (6.2.4) as
Hence we have 
. Now, utilizing (6.4.1), one can
write down the information contained in the data X as follows:
Since the X’s are iid, we have 
 for each i =
1, ..., n, and hence the first term included in the end of (6.4.6) amounts to
nIX1(θ). Next, the second term in the end of (6.4.6) can be expressed as
  
since the X’s are identically distributed.

6. Sufficiency, Completeness, and Ancillarity
303
Now, let us write
so that 0 = 
 dx. Hence one
obtains
Next, combining (6.4.6)-(6.4.8), we conclude that IX(θ) = nIX1(θ). !
Suppose that we have collected random samples X1, ..., Xn from a popula-
tion and we have evaluated the information IX(θ) contained in the data X =
(X1, ..., Xn). Next, suppose also that we have a statistic T = T(X) in mind for
which we have evaluated the information IT(θ) contained in T. If it turns out
that IT(θ), can we then claim that the statistic T is indeed sufficient for θ? The
answer is yes, we certainly can. We state the following result without supply-
ing its proof. One may refer to Rao (1973, result (iii), p. 330) for details. In a
recent exchange of personal communications, C. R. Rao has provided a simple
way to look at the Theorem 6.4.2. In the Exercise 6.4.15, we have given an
outline of Rao’s elegant proof of this result. In the Examples 6.4.3-6.4.4, we
find opportunities to apply this theorem.
Theorem 6.4.2 Suppose that X is the whole data and T = T(X) is some
statistic. Then, IX(θ) ≥ IT(θ) for all θ ∈ Θ. The two information measures
match with each other for all θ if and only if T is a sufficient statistic for θ.
Example 6.4.3 (Example 6.4.1 Continued) Suppose that X1, ..., Xn are iid
Poisson(λ), where λ (> 0) is the unknown parameter. We already know that
 is a minimal sufficient statistic for λ and T is distributed as
Poisson(nλ). But, let us now pursue T from the information point of view.
One can start with the pmf g(t; λ) of T and verify that
as follows: Let us write log{g(t; λ)} = –nλ+tlog(nλ) – log(t!) which implies
that 
 log{g(t;λ)} = –n + tλ-1. So, IT(λ) = Eλ [{
 log{g(T;λ)}}2] = Eλ [(T -
nλ)2/λ2] = nλ-1 since Eλ[(T - nλ)2] = V(T) = nλ.
On the other hand, from (6.4.4) and Example 6.4.1, we can write IX(λ)
= nIX1(λ) = nλ-1. That is, T preserves the available information from the
whole data X. The Theorem 6.4.2 implies that the statistic T is indeed suf-
ficient for λ. !

304
6. Sufficiency, Completeness, and Ancillarity
Example 6.4.4 (Example 6.4.2 Continued) Let X1, ..., Xn be iid N(µ, σ2)
where µ ∈ ( –∞, ∞) is the unknown parameter. Here σ ∈ (0, ∞) is assumed
known. We had shown earlier that the statistic T = 
 was sufficient for µ.
Let us now pursue T from the information point of view. The statistic T is
distributed as N(µ, n-1σ2) so that one can start with the pdf g(t; µ) of T and
verify that
as follows: Let us write log{g(t; µ)} = – ½{n(t – µ)2/σ2} – 
which implies that 
 = n(t – µ)/σ2. Hence, we have IT(µ)
  = Eµ [n2(T – µ)2/σ4] = nσ-2 since Eµ[(T – µ)2] =
V
 = n-1 σ2. From the Example 6.4.2, however, we know that the informa-
tion contained in one single observation is IX1 (µ) = nσ-2 and thus in view of
(6.4.4), we have IX(µ) = nIX1 (µ) = nσ-2. That is, T preserves the available
information from the whole data X. Now, the Theorem 6.4.2 would imply
that the statistic T is indeed sufficient for λ. !
Remark 6.4.1 Suppose that the pmf or pdf f(x; θ) is such that 
is finite for all x ∈ χ and Eθ 
 is finite for all θ ∈ Θ. Then the Fisher
information defined earlier can be alternatively evaluated using the following
expression:
We leave its verification as an exercise.
Example 6.4.5 (Example 6.4.1 Continued) Use (6.4.9) and observe that
 log f(x; λ) = –xλ-2 so that IX(λ) = –Eλ 
 = – Eλ[-Xλ-2] =
λ-1.!
Example 6.4.6 (Example 6.4.2 Continued) Use (6.4.9) and observe that  
log f(x; µ) = –σ–2 so that IX(λ) = –Eλ [
 log f(X; µ)] = –Eλ[–σ–2] = σ–2. !
In the Exercise 6.4.16, we pursue an idea like this: Suppose
that a statistic T is not sufficient for θ. Can we say something
about how non-sufficient T is for θ?
6.4.2
Multi-parameter Situation
When the unknown parameter θ is multidimensional, the definition of the
Fisher information gets more involved. To keep the presentation simple,

6. Sufficiency, Completeness, and Ancillarity
305
we only discuss the case of a two-dimensional parameter. Suppose that X is
an observable real valued random variable with the pmf or pdf f(x;θ) where
the parameter θ = (θ1, θ2) ∈ Θ, an open rectangle ⊆ ℜ2, and the χ space does
not depend upon θ. We assume throughout that 
 f(x; θ) exists, i = 1, 2, for
all x ∈ χ, θ ∈ Θ, and that we can also interchange the partial derivative (with
respect to θ1, θ2) and the integral (with respect to x).
Definition 6.4.2 Let us extend the earlier notation as follows. Denote
Iij(θ) = Eθ 
, for i, j = 1, 2. The Fisher
information matrix or simply the information matrix about θ is given by
Remark 6.4.2 In situations where 
 f(x; θ) exists for all x ∈ χ, for all
i, j = 1, 2, and for all θ ∈ Θ, we can alternatively write
and express the information matrix IX(θ) accordingly. We have left this as an
exercise.
Having a statistic T = T(X1, ..., Xn), however, the associated information
matrix about θ will simply be calculated as IT(θ) where one would replace the
original pmf or pdf f(x; θ) by that of T, namely g(t;θ), t ∈ 
. When we
compare two statistics T1 and T2 in terms of their information content about a
single unknown parameter θ, we simply look at the two one-dimensional quan-
tities IT1 (θ) and IT2 (θ), and compare these two numbers. The statistic associ-
ated with the larger information content would be more appealing. But, when
θ is two-dimensional, in order to compare the two statistics T1 and T2, we
have to consider their individual two-dimensional information matrices IT1(θ)
and IT2(θ). It would be tempting to say that T1 is more informative about θ
than T2 provided that

306
6. Sufficiency, Completeness, and Ancillarity
We add that a version of the Theorem 6.4.2 holds in the multiparameter
case as well. One may refer to Section 5a.3 of Rao (1973).
Example 6.4.7 Let X1, ..., Xn be iid N(µ, σ2) where µ ∈ (–∞, ∞) and σ2 ∈
(0, ∞) are both unknown parameters. Denote θ = (µ, σ2), X = (X1, ..., Xn). Let
us evaluate the information matrix for X. First, a single observation X1 has its
pdf
so that one has
Hence we obtain
since σ–2(X1 – µ)2 is 
 so that E[σ–2(X1 – µ)2] = 1, V[σ–2(X1 – µ)2] = 2. Next,
we have
so that combining (6.4.14)-(6.4.15) with (6.4.10), we obtain the following
information matrix for one single observation X1:
Utilizing (6.4.12), we obtain the information matrix,
for the whole data X. !

6. Sufficiency, Completeness, and Ancillarity
307
Example 6.4.8 (Example 6.4.7 Continued) Let 
 the
sample mean. We are aware that 
 is distributed as N(µ, n–1σ2) and its pdf is
given by
so that one has
Hence we have
We can again show that I12(θ) = I21(θ) = 0 corresponding to 
. Utilizing
(6.4.18), we obtain the following information matrix corresponding to the
statistic 
:
Comparing (6.4.17) and (6.4.19), we observe that
which is a positive semi definite matrix. That is, if we summarize the whole
data X only through 
, then there is some loss of information. In other
words, 
 does not preserve all the information contained in the data X when
µ and σ2 are both assumed unknown. !
Example 6.4.9 (Example 6.4.7 Continued) Suppose that we consider the
sample variance, S2 = (n – 1)–1 
 . We are aware that Y = (n– 1)
S2/σ2 is distributed as 
 for n ≥ 2, and so with c = {2(n–1)/2Γ(½(n – 1))}–1,
the pdf of Y is given by
Hence with d = (n - 1)(n-1)/2c, the pdf of S2 is given by

308
6. Sufficiency, Completeness, and Ancillarity
so that one has
Hence we obtain
Obviously, I12(θ) = I21(θ) = 0 corresponding to S2. Utilizing (6.4.21), we ob-
tain the information matrix corresponding to the statistic S2, namely,
Comparing (6.4.17) and (6.4.22), we observe that
which is a positive semi definite matrix. That is, if we summarize the whole
data X only through S2, then there is certainly some loss of information when
µ and σ2 are both assumed unknown. !
Example 6.4.10 (Examples 6.4.8-6.4.9 Continued) Individually, whether
we consider the statistic 
 or S2, both lose some information in comparison
with IX(θ), the information contained in the whole data X. This is clear from
(6.4.20) and (6.4.23). But recall that 
 and S2 are independently distributed,
and hence we note that
That is, the lost information when we consider only 
 or S2 is picked up by
the other statistic. !
In the Example 6.4.10, we tacitly used a particular result which is fairly
easy to prove. For the record, we merely state this result while its proof is left
as the Exercise 6.4.11.
Theorem 6.4.3 Suppose that X1, ..., Xn are iid with the common pmf or
pdf given by f(x; θ). We denote the whole data X = (X1, ..., Xn). Suppose that
we have two statistics T1 = T1(X), T2 = T2(X) at our disposal and T1, T2 are
distributed independently. Then, the information matrix IT(θ) is given by

6. Sufficiency, Completeness, and Ancillarity
309
Let us now go back for a moment to (6.4.10) for the definition of the
information matrix IX(θ). Now suppose that Y = h(X) where the function
h(.) : χ → Y is one-to-one. It should be intuitive enough to guess that IX(θ) =
IY(θ). For the record, we now state this result formally.
Theorem 6.4.4 Let X be an observable random variable with its pmf or
pdf f(x; θ) and the information matrix IX(θ). Suppose that Y = h(X) where the
function h(.) : χ → Y is one-to-one. Then, the information matrix about the
unknown parameter θ contained in Y is same as that in X, that is
Proof In order to keep the deliberations simple, we consider only a real
valued continuous random variable X and a real valued unknown parameter θ.
Recall that we can write 
. Note that x = h-1(y)
is well-defined since h(.) is assumed one-to-one. Now, using the transforma-
tion techniques from (4.4.1), observe that the pdf of Y can be expressed as
Thus, one immediately writes
 
The vector valued case and the case of discrete X can be disposed off with
minor modifications. These are left out as Exercise 6.4.12. !
6.5 Ancillarity
The concept called ancillarity of a statistic is perhaps the furthest away
from sufficiency. A sufficient statistic T preserves all the information about
θ contained in the data X. To contrast, an ancillary statistic T by itself
provides no information about the unknown parameter θ. We are not im-
plying that an ancillary statistic is necessarily bad or useless. Individually,
an ancillary statistic would not provide any information about θ, but

310
6. Sufficiency, Completeness, and Ancillarity
such statistics can play useful roles in statistical methodologies. In the mid
1920’s, R. A. Fisher introduced this concept and he frequently revisited it in
his writings. This concept evolved from Fisher (1925a) and later it blossomed
into the vast area of conditional inferences. In his 1956 book, Fisher empha-
sized many positive aspects of ancillarity in analyzing real data. Some of these
ideas will be explored in this and the next section. For fuller discussions of
conditional inference one may look at Basu (1964), Hinkley (1980a) and Ghosh
(1988). The interesting article of Reid (1995) also provides an assessment of
conditional inference procedures.
Consider the real valued observable random variables X1, ..., Xn from some
population having the common pmf or pdf f(x; θ), where the unknown pa-
rameter vector θ belongs to the parameter space Θ ⊆ ℜp. Let us continue
writing X = (X1, ..., Xn) for the data, and denote a vector valued statistic by T
= T(X).
Definition 6.5.1 A statistic T is called ancillary for θ or simply ancillary
provided that the pmf or the pdf of T, denoted by g(t) for t ∈ 
, does not
involve the unknown parameter θ ∈ Θ.
Example 6.5.1 Suppose that X1, ..., Xn are iid N(θ, 1) where θ is the
unknown parameter, –∞ < θ < ∞, n ≥ 3. A statistic T1 = X1 – X2 is distributed
as N(0, 2) whatever be the value of the unknown parameter θ. Hence T1 is
ancillary for θ. Another statistic T2 = X1 + ... + Xn–1 – (n – 1)Xn is distributed
as N(0, n(n – 1)) whatever be the value of the unknown parameter θ. Hence
T2 is also ancillary for θ. The sample variance S2 is distributed as 
whatever be the value of the unknown parameter θ and hence S2 is ancillary
too for θ. !
Example 6.5.2 Suppose that X1, ..., Xn are iid N(µ, σ2), θ = (µ, σ2), –∞ <
µ < ∞, 0 < σ2 < ∞, n ≥ 2. Here both the parameters µ and σ are assumed
unknown. Let us reconsider the statistics T1 or T2 defined in the Example
6.5.1. Now, T1, T2 are respectively distributed as N(0, 2σ2) and N(0, n(n –
1)σ2) respectively, and both these distributions clearly depend upon θ. Thus,
T1 or T2 is no longer ancillary for θ in this situation. The sample variance S2 is
distributed as 
 and hence S2 is not ancillary either for θ. But,
consider another statistic T3 = (X1 – X2)/S where S2 is the sample variance.
Denoting Yi = (Xi – µ)/σ, observe that in terms of the Y’s, we can equivalently
rewrite T3
2 as
Since Y1, ..., Yn are iid N(0, 1), starting with the likelihood function of Y =
(Y1, ..., Yn), and then by using transformations, one can find the pdf of U3.
We leave it as an exercise. But, since the likelihood function of Y would not

6. Sufficiency, Completeness, and Ancillarity
311
involve θ to begin with, the pdf of U3 would not involve θ. In other words, T3
is an ancillary statistic here. Note that we do not need the explicit pdf of U3 to
conclude this. !
In Example 6.5.2, note that 
 has a Student’s
t distribution with (n - 1) degrees of freedom which is free from
θ. But, we do not talk about its ancillarity or non-ancillarity since
T4 is not a statistic. T3, however, was a statistic. The expression
used in (6.5.1) was merely a device to argue that the distribution
of the statistic T3 in the Example 6.5.2 was free from θ.
Example 6.5.3 Suppose that X1, ..., Xn are iid with the common pdf f(x;
λ) = λe–λx I(x > 0) where λ(> 0) is the unknown parameter with n = 2. Let us
write S2 = (n - 1)-1 
 and denote 
 T1 = X1/Xn, T2 =
X2/U, T3 = (X1 + X3)/S. Define Yi = λXi for i = 1, ..., n and it is obvious that the
joint distribution of Y = (Y1, ..., Yn) does not involve the unknown parameter
λ. Next, one can rewrite the statistic T1 as Y1/Yn and its pdf cannot involve λ.
So, T1 is ancillary. Also, the statistic T2 can be rewritten as Y2/{
Yi} and
its pdf cannot involve λ. So, T2 is ancillary. Similarly one can argue that T3 is
also ancillary. The details are left out as Exercise 6.5.2. !
Example 6.5.4 (Example 6.5.1 Continued) Suppose that X1, X2, X3 are iid
N(θ, 1) where θ is the unknown parameter, –∞ < θ < ∞. Denote T1 = X1 - X2,
T2 = X1 + X2 - 2X3, and consider the two dimensional statistic T = (T1, T2).
Note that any linear function of T1, T2 is also a linear function of X1, X2, X3,
and hence it is distributed as a univariate normal random variable. Then, by
the Definition 4.6.1 of the multivariate normality, it follows that the statistic T
is distributed as a bivariate normal variable. More specifically, one can check
that T is distributed as N2(0, 0, 2, 6, 0) which is free from θ. In other words,
T is an ancillary statistic for θ. !
Example 6.5.5 (Example 6.5.2 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2), θ = (µ, σ2), –∞ < µ < ∞, 0 < σ2 < ∞, n ≥ 4. Here, both the
parameters µ and σ are assumed unknown. Let S2 be the sample variance and
T1 = (X1 - X3)/S, T2 = (X1 + X3 - 2X2)/S, T3 = (X1 – X3 + 2X2 – 2X4)/S, and
denote the statistic T = (T1, T2, T3). Follow the technique used in the Example
6.5.2 to show that T is ancillary for θ. !
We remarked earlier that a statistic which is ancillary for the unknown
parameter θ can play useful roles in the process of inference making. The
following examples would clarify this point.

6. Sufficiency, Completeness, and Ancillarity
312
Example 6.5.6 (Example 6.5.1 Continued) Suppose that X1, X2 are iid N(θ,
1) where θ is the unknown parameter, –∞ < θ < ∞. The statistic T1 = X1 – X2
is ancillary for θ. Consider another statistic T2 = X1. One would recall from
Example 6.4.4 that 
 (θ) = 1 whereas IX(θ) = 2, and so the statistic T2 can
not be sufficient for θ. Here, while T2 is not sufficient for θ, it has some
information about θ, but T1 itself has no information about θ. Now, if we are
told the observed value of the statistic T = (T1, T2), then we can reconstruct
the original data X = (X1, X2) uniquely. That is, the statistic T = (T1, T2) turns
out to be jointly sufficient for the unknown parameter θ. !
Example 6.5.7 This example is due to D. Basu. Suppose that (X, Y) is a
bivariate normal variable distributed as N2(0, 0, 1, 1, ρ), introduced in Section
3.6, where the unknown parameter is the correlation coefficient ρ ∈ (–1, 1).
Now consider the two statistics T1 = X and T2 = Y. Since T1 and T2 have
individually both standard normal distributions, it follows that T1 is ancillary
for ρ and so is T2. But, note that the statistic T = (T1, T2) is minimal sufficient
for the unknown parameter ρ. What is remarkable is that the statistic T1 has
no information about ρ and the statistic T2 has no information about ρ, but the
statistic (T1, T2) jointly has all the information about ρ. !
The situation described in (6.5.2) has been highlighted in the Example
6.5.6 where we note that (T1, T2) is sufficient for θ, but (T1, T2) is not minimal
sufficient for θ. Instead, 2T2 - T1 is minimal sufficient in the Example 6.5.6.
The situation described in (6.5.3) has been highlighted in the Example 6.5.7
where we note especially that (T1, T2) is minimal sufficient for θ. In other
words, there are remarkable differences between the situations described by
these two Examples.
Let us now calculate the information 
 where ρ is the
correlation coefficient in a bivariate normal population.

6. Sufficiency, Completeness, and Ancillarity
313
With 
, from (3.6.2), one can write down the joint
pdf of (X, Y):
In order to derive the expression for the Fisher information IX,Y(ρ), one may
proceed with the natural logarithm of f(x, y; ρ). Next, differentiate the natural
logarithm with respect to ρ, followed by squaring it, and then evaluating the
expectation of that expression after replacing (x, y) with (X, Y). This direct
approach becomes quite involved and it is left as an exercise. Let us, how-
ever, adopt a different approach in the following example.
Example 6.5.8 (Example 6.5.7 Continued) Suppose that (X, Y) is distrib-
uted as N2(0, 0, 1, 1, ρ), where the unknown parameter is the correlation
coefficient ρ ∈ (–1, 1). Define U = X - Y, V = X &plus; Y, and notice that (U,
V) can be uniquely obtained from (X, Y) and vice versa. In other words,
IU,V(ρ) and IX,Y(ρ) should be exactly same in view of the Theorem 6.4.4. But,
observe that (U, V) is distributed as N2(0, 0, 2(1 – ρ), 2(1 + ρ), 0), that is U
and V are independent random variables. Refer back to the Section 3.7 as
needed. Hence, using the Theorem 6.4.3, we immediately conclude that IU,V(ρ)
= IU(ρ) + IV(ρ). So, we can write:
That is, it will suffice to evaluate IU(ρ) and IV(ρ) separately. It is clear that U
is N(0, 2(1 – ρ)) while V is N(0, 2(1 + ρ)). The pdf of U is given by
so that one has
Hence, we write
since Eρ[U2] = 2(1 – ρ), 
. Similarly, one would
verify that IV(ρ) = ½(1 + ρ)–2. Then, from (6.5.5), we can obviously write:

6. Sufficiency, Completeness, and Ancillarity
314
A direct verification of (6.5.6) using the expression of f(x, y; ρ) is left as the
Exercise 6.5.17. !
Example 6.5.9 Let X1, X2, ... be a sequence of iid Bernoulli(p), 0 < p < 1.
One may think of Xi = 1 or 0 according as the ith toss of a coin results in a
head (H) or tail (T) where P(H) = 1 – P(T) = p in each independent toss, i =
1, ..., n. Once the coin tossing experiment is over, suppose that we are only
told how many times the particular coin was tossed and nothing else. That is,
we are told what n is, but nothing else about the random samples X1, ..., Xn. In
this situation, knowing n alone, can we except to gain any knowledge about
the unknown parameter p? Of course, the answer should be “no,” which
amounts to saying that the sample size n is indeed ancillary for p. !
6.5.1
The Location, Scale, and Location-Scale Families
In Chapter 3, we discussed a very special class of distributions, namely the
exponential family. Now, we briefly introduce a few other special ones which
are frequently encountered in statistics. Let us start with a pdf g(x), x ∈ ℜ
and construct the following families of distributions defined through the
g(.) function:
The understanding is that θ, δ may belong to some appropriate subsets of ℜ,
ℜ+ respectively. The reader should check that the corresponding members
f(.) from the families F1, F2, F3 are indeed pdf’s themselves.
The distributions defined via parts (i), (ii), and (iii) in (6.5.7)
are respectively said to belong to the
location, scale, and location-scale family.
We often say that the families F1, F2, F3 are respectively indexed by (i) the
parameter θ, (ii) the parameter δ, and (iii) the parameters θ and δ. In part (i) θ
is called a location parameter, (ii) δ is called a scale parameter, and (iii) θ, δ
are respectively called the location and scale parameters.
For example, the collection of N(µ, 1) distributions, with µ ∈ ℜ, forms
a location family. To see this, let g(x) = φ (x) = 
 exp{–x2/2}, x ∈ ℜ

6. Sufficiency, Completeness, and Ancillarity
315
and then write F1 = {f(x; µ) = φ (x – µ) : µ ∈ ℜ, x ∈ ℜ}. Next, the collection
of N(0, σ2) distributions, with σ ∈ ℜ+, forms a scale family. To see this, let us
simply write F2 = {f(x; σ) = σ–1 φ (x/σ) : σ ∈ ℜ+, x ∈ ℜ}. The collection of
N(µ, σ2) distributions, with µ ∈ ℜ, σ ∈ ℜ+, forms a location-scale family. We
simply write F3 = {f(x; µ, σ) = σ–1 φ ((x – µ)/σ) : µ ∈ ℜ, σ ∈ ℜ+, x ∈ ℜ}. We
have left other examples as exercises.
In a location family, the role of the location parameter θ is felt in the “move-
ment” of the pdf along the x-axis as different values of θ are contemplated.
The N(0, 1) distribution has its center of symmetry at the point x = 0, but the
N(µ, 1) distribution’s center of symmetry moves along the x-axis, to the right
or left of x = 0, depending upon whether µ is positive or negative, without
changing anything with regard to the shape of the probability density curve.
In this sense, the mean µ of the normal distribution serves as a location
parameter. For example, in a large factory, we may look at the monthly wage
of each employee and postulate that the distribution of wage as N(µ, σ2)
where σ = $100. After the negotiation of a new contract, suppose that each
employee receives $50 monthly raise. Then the distribution moves to the right
with its new center of symmetry at µ + 50. The intrinsic shape of the distri-
bution can not change in a situation like this.
In a scale family, the role of the scale parameter δ is felt in “squeezing or
expanding” the pdf along the x-axis as different values of δ are contemplated.
The N(0, 1) distribution has its center of symmetry at the point x = 0. The
N(0, δ2) distribution’s center of symmetry stays put at the point x = 0, but
depending on whether δ is larger or smaller than one, the shape of the density
curve will become more flat or more peaked, compared with the standard
normal, around the center. In this sense, the variance δ2 of the normal distri-
bution serves as a scale parameter. Suppose that we record the heights (in
inches) of individuals and we postulate the distribution of these heights as
N(70, σ2). If heights are measured in centimeters instead, then the distribution
would appear more spread out around the new center. One needs to keep in
mind that recording the heights in centimeters would amount to multiplying
each original observation X measured in inches by 2.54.
In a location-scale family, one would notice movement of the distribu-
tion along the x-axis as well as the squeezing or expansion effect in the
shape. We may be looking at a data on the weekly maximum temperature
in a city recorded over a period, in Fahrenheit (F) or Celsius (C). If one
postulates a normal distribution for the temperatures, changing the unit of
measurements from Fahrenheit to Celsius would amount to shifts in both
the origin and scale. One merely needs to recall the relationship 1/5C =

6. Sufficiency, Completeness, and Ancillarity
316
1-9 (F - 32) between the two units, Fahrenheit and Celsius.
At this point, one may ask the following question. What is the relevance of
such special families of distributions in the context of ancillarity? It may help
if one goes back to the Examples 6.5.1-6.5.5 and thinks through the process
of how we had formed some of the ancillary statistics. Suppose that X1, ...,
Xn are iid random variables having the common pdf f(x), indexed by some
appropriate parameter(s). Then, we can conclude the following.
One should not, however, get the impression that (6.5.8)-(6.5.10) list the
unique or in some sense the “best” ancillary statistics. These summary state-
ments and ancillary statistics should be viewed as building blocks to arrive at
many forms of ancillary statistics.
6.5.2
Its Role in the Recovery of Information
In Examples 6.5.6-6.5.7, we had seen how ancillary statistics could play
significant roles in conjunction with non-sufficient statistics. Suppose that
T1 is a non-sufficient statistic for θ and T2 is ancillary for θ. In other
words, in terms of the information content, 
 < IX(θ) where X is the
whole data and 
 = 0 for all θ ∈ Θ. Can we recover all the information
contained in X by reporting T1 while conditioning on the observed value of
T2? The answer is: we can do so and it is a fairly simple process. Such a
process of conditioning has far reaching implications as emphasized by

6. Sufficiency, Completeness, and Ancillarity
317
Fisher (1934, 1956) in the famous “Nile” example. One may also refer to
Basu (1964), Hinkley (1980a), Ghosh (1988) and Reid (1995) for fuller dis-
cussions of conditional inference.
The approach goes through the following steps. One first finds the condi-
tional pdf of T1 at the point T1 = u given that T2 = v, denoted by 
.
Using this conditional pdf, one obtains the information content, namely 
following the Definition 6.4.1. In other words,
In general, the expression of 
 would depend on v, the value of the
ancillary statistic T2. Next, one averages 
 over all possible values v, that
is to take 
 Once this last bit of averaging is done, it will coincide
with the information content in the joint statistic (T1, T2), that is
This analysis provides a way to recover, in the sense of (6.5.12), the lost
information due to reporting T1 alone via conditioning on the ancillary statistic
T2. Few examples follow.
Example 6.5.10 (Example 6.5.1 Continued) Let X1, X2 be iid N(θ, 1) where
θ ∈ (–∞, ∞) is an unknown parameter. We know that 
 is sufficient for θ.
Now,  
 is distributed as N(θ, ½) so that we can immediately write
 = 2. Now,  T1 = X1 is not sufficient for θ since 
That is, if we report only X1 after the data (X1, X2) has been collected,
there will be some loss of information. Next, consider an ancillary sta-
tistic, T2 = X1 - X2 and now the joint distribution of (T1, T2) is N2(θ, 0,
1, 2, ρ =  
). Hence, using Theorem 3.6.1, we find that the condi-
tional distribution of T1 given T2 = v is N(θ + ½v, ½), v ∈ (–∞, ∞).
Thus, we first have 
= 2 and since this
expression does not involve v, we then have 
 which equals
. In other words, by conditioning on the ancillary statistic T2, we have
recovered the full information which is 
.
Example 6.5.11 (Example 6.5.7 Continued) Suppose that (X, Y) is distrib-
uted as N2(0, 0, 1, 1, ρ) where the unknown parameter is the correlation
coefficient ρ ∈ (–1, 1). Now consider the two statistics X and Y. Individually,
both T1 = X and T2 = Y are ancillary for ρ. Again, we utilize (6.5.11)-(6.5.12).
Using the Theorem 3.6.1, we note that the conditional distribution of X given
Y = y is N(ρy, 1 – ρ2) for y ∈ (–∞, ∞). That is, with x ∈ (–∞, ∞), we can write

6. Sufficiency, Completeness, and Ancillarity
318
so that we have
In other words, the information about ρ contained in the conditional distribu-
tion of T1 | T2 = v, v ∈ ℜ, is given by
which depends on the value v unlike what we had in the Example 6.5.10.
Then, the information contained in (X, Y) will be given by
In other words, even though the statistic X tells us nothing about ?, by aver-
aging the conditional (on the statistic Y) information in X, we have recovered
the full information about ρ contained in the whole data (X, Y). Refer to the
Example 6.5.8. !
6.6
Completeness
Consider a real valued random variable X whose pmf or pdf is given by f(x; θ)
for x ∈ χ and θ ∈ Θ. Let T = T(X) be a statistic and suppose that its pmf or pdf
is denoted by g(t; θ) for t ∈ T and θ ∈ Θ.
Definition 6.6.1 The collection of pmf’s or pdf’s denoted by {g(t; θ): θ ∈
Θ} is called the family of distributions induced by the statistic T.
Definition 6.6.2 The family of distributions {g(t; θ): θ ∈ Θ}, induced
by a statistic T, is called complete if and only if the following condition

6. Sufficiency, Completeness, and Ancillarity
319
holds. Consider any real valued function h(t) defined for t ∈ T, having finite
expectation, such that
In other words,
Definition 6.6.3 A statistic T is said to be complete if and only if the
family of distributions {g(t; θ): θ ∈ Θ} induced by T is complete.
In these definitions, observe that neither the statistic T nor the parameter θ
has to be necessarily real valued. If we have, for example, a vector valued
statistic T = (T1, T2, ..., Tk) and θ is also vector valued, then the requirement
in (6.6.1) would obviously be replaced by the following:
Here, h(t) is a real valued function of t = (t1, ..., tk) ∈ T and g(t; θ) is the pmf
or the pdf of the statistic T at the point t.
The concept of completeness was introduced by Lehmann and Scheffé
(1950) and further explored measure-theoretically by Bahadur (1957). Next
we give two examples. More examples will be given in the subsection
6.6.1.
Example 6.6.1 Suppose that a statistic T is distributed as Bernoulli(p), 0
< p < 1. Let us examine if T is a complete statistic according to the Defini-
tion 6.6.3. The pmf induced by T is given by g(t; p) = pt(1 - p)1-t, t = 0, 1.
Consider any real valued function h(t) such that Ep[h(T)] = 0 for all 0 < p <
1. Now, let us focus on the possible values of h(t) when t = 0, 1, and write
Observe that the middle step in (6.6.3) is linear in p and so it may be zero
for exactly one value of p between zero and unity. But we are demanding
that p{h(1) - h(0)} + h(0) must be zero for infinitely many values of p in
(0, 1). Hence, this expression must be identically equal to zero which
means that the constant term as well as the coefficient of p must indi-
vidually be both zero. That is, we must have h(0) = 0 and h(1) - h(0) =
0 so that h(1) = 0. In other words, we have h(t) = 0 for t = 0, 1. Thus,
T is a complete statistic. !

6. Sufficiency, Completeness, and Ancillarity
320
Example 6.6.2 Suppose g(t; 0, σ) = 
 exp {-½t2/σ2}, –∞ < t <
∞, σ ∈ ℜ+. Is the family of distributions {g(t; 0, σ): σ > 0} complete? The
answer is “no,” it is not complete. In order to prove this claim, consider the
function h(t) = t and then observe that Eσ[h(T)] = E
σ[T] = 0 for all 0 < σ < ∞,
but h(t) is not identically zero for all t ∈ ℜ. Hence, the claim is true. !
6.6.1
Complete Sufficient Statistics
The completeness property of a statistic T looks like a mathematical con-
cept. From the statistical point of view, however, this concept can lead to
important results when a complete statistic T also happens to be sufficient.
Definition 6.6.4 A statistic T is called complete sufficient for an un-
known parameter θ if and only if (i) T is sufficient for θ and (ii) T is com-
plete.
In Section 6.6.3, we present a very useful theorem, known as Basu’s
Theorem, which needs a fusion of both concepts: sufficiency and com-
pleteness. Theorem 6.6.2 and (6.6.8) would help one to claim the minimality
of a complete sufficient statistic. Other important applications which ex-
ploit the existence of a complete sufficient statistic will be mentioned in later
chapters.
Example 6.6.3 Let X1, ..., Xn be iid Bernoulli(p), 0 < p < 1. We already
know that 
 is a sufficient statistic for p. Let us verify that T is
complete too. Since T is distributed as Binomial(n, p), the pmf induced by T is
given by g(t; p) = 
 pt(1 - p)n-t, t ∈ T = {0, 1, ..., n}. Consider any real valued
function h(t) such that Ep[h(T)] = 0 for all 0 < p < 1. Now, let us focus on the
possible values of h(t) for t = 0, 1, ..., n. With γ = p(1 - p)-1, we can write
From (6.6.4) we observe that Ep[h(T)] has been expressed as a polynomial of
the nth degree in terms of the variable γ ∈ (0, ∞). A nth degree polynomials in
? may be equal to zero for at most n values of γ ∈ (0, ∞). If we are forced to
assume that
then we conclude that 
 that is 
 for all t = 0,
1, ..., n. In other words, h(t) ≡ 0 for all t = 0, 1, ..., n, proving the complete-
ness of the sufficient statistic T. !

6. Sufficiency, Completeness, and Ancillarity
321
Example 6.6.4 Let X1, ..., Xn be iid Poisson(λ) where λ(> 0) is the un-
known parameter. We know that 
 is a sufficient statistic for λ.
Let us verify that T is also complete. Since T is distributed as Poisson(nλ), the
pmf induced by T is given by g(t; λ) = e–nλ(nλ)t/t!, t ∈ T = {0, 1, 2, ...}.
Consider any real valued function h(t) such that Eλ[h(T)] = 0 for all 0 < λ < ∞.
Now, let us focus on the possible values of h(t) for t = 0, 1, 2, ... . With k(t)
= h(t) nt/t!, we can write
From (6.6.5) we observe that Eλ[h(T)] has been expressed as an infinite power
series in the variable λ belonging to (0, ∞). The collection of such infinite
power series forms a vector space generated by the set  = {1, λ, λ2, λ3, ...,
λt, ...}. Also,  happens to be the smallest generator in the sense that if any
element is dropped from , then  will be unable to generate all infinite power
series in λ. In other words, the vectors belonging to  are linearly indepen-
dent. So, if we are forced to assume that
we will then conclude that 
 that is k(t) = 0 for all t = 0, 1, 2,
... . Hence, we conclude that h(t) ≡ 0 for all t = 0, 1, 2, ... , proving the
completeness of the sufficient statistic T. !
In general, a sufficient or minimal sufficient statistic T for
an unknown parameter θ is not necessarily complete.
Look at the Examples 6.6.5-6.6.6.
Example 6.6.5 Let X1, ..., Xn be iid Normal(θ, θ) where θ(> 0) is the
unknown parameter. One should verify that T = (
, S2) is a minimal suffi-
cient statistic for θ, but note that Eθ (
) = θ and Eθ(
, S2) = θ for all θ > 0.
That is, for all θ > 0, we have Eθ(
 – S2) = 0. Consider h(T) = 
 – S2 and
then we have Eθ(h(T)) ≡ 0 for all θ > 0, but h(t) =  – s2 is not identically zero
for all t ∈ ℜ × ℜ+. Hence, the minimal sufficient statistic T can not be com-
plete. !
Example 6.6.6 Let X1, ..., Xn be iid with the common pdf given by θ-1
exp{– (x – θ)/θ}I(x > θ) where θ(> 0) is the unknown parameter. We note that
the likelihood function L(θ) is given by
and so it follows that U = 
 is a minimal sufficient statistic for
θ. But, the statistic T = 
 is a one-to-one function of

6. Sufficiency, Completeness, and Ancillarity
322
U, and hence by the Theorem 6.3.2, T itself is a minimal sufficient statistic for
θ. But recall that Eθ(Xn:1) = (1 + n-1)θ and Eθ[(n – 1)–1 ∑n
i=1(Xi-Xn:1)] = θ for all
θ > 0. Let us denote h(t) = (1 + n–1)–1xn:1 – (n – 1)–1 
 for all t
∈ T = (θ, ∞) × ℜ+, so that Eθ[h(T)] = Eθ[(1 + n–1)–1 Xn:1 – (n – 1)–1
 = 0 for all θ > 0. But obviously h(t) is not identically zero
for all t ∈ T ≡ (θ, ∞) × ℜ+. Hence, the minimal sufficient statistic T can not be
complete. !
In the Examples 6.6.5-6.6.6, one can easily find other nontrivial real val-
ued functions h(t) such that Eθ[h(T)] ≡ 0 for all θ, but h(t) is not identically
zero. We leave these as exercises.
Theorem 6.6.1 Suppose that a statistic T = (T1, ..., Tk) is complete. Let U
= (U1, ..., Uk) be another statistic with U = f(T) where g: T → U is one-to one.
Then, U is complete.
Proof For simplicity, let us pretend that T, U have continuous distributions
and that T, U and θ are all real valued. Let the pdf of T be denoted by g(t; θ).
Since f(.) is one-to-one, we can write:
Now, assume that U is not complete. Then, there is a function a(U) such that
Eθ[a(U)] ≡ 0 but Pθ{u : a(u) ≠ 0} > 0 for all θ ∈ Θ. From (6.6.7) we can then
claim that Eθ[h(T)] ≡ 0 for all θ ∈ Θ where h ≡ aof stands for the composition
mapping. But, this function h(.) is non-zero with positive probability which
contradicts the assumed completeness property of T. !
Now, we state a remarkably general result (Theorem 6.6.2) in the case of
the exponential family of distributions. One may refer to Lehmann (1986, pp.
142-143) for a proof of this result.
Theorem 6.6.2 (Completeness of the Minimal Sufficient Statistic in
the Exponential Family) Suppose that X1, ..., Xn are iid with the common
pmf or the pdf belonging to the k-parameter exponential family defined by
with some appropriate forms for p(x) ≥ 0, q(θ) ≥ 0, θi and Ri(x), i = 1, ...,
k. Suppose that the regulatory conditions stated in (3.8.5) hold. Denote

6. Sufficiency, Completeness, and Ancillarity
323
the statistic 
 Rj(Xi), j = 1, ..., k. Then, the (jointly) minimal suffi-
cient statistic T = (T1, ..., Tk) for θ is complete.
Theorem 6.6.2 will sometimes provide an easy route to claim
the completeness of a sufficient statistic for θ as long as f(x; θ)
is given by (6.6.9) and the parameter space Θ(⊆ ℜk)
includes a k-dimensional rectangle.
Example 6.6.7 Let X1, ..., Xn be iid N(µ, σ2) with (µ, σ) ∈ Θ = ℜ×ℜ+
where µ and σ are both assumed unknown. The pdf of N(µ, σ2) has the form
given in (6.6.9) where k = 2, θ = (θ1, θ2) with θ1 = µ/σ2, θ2 = 1/σ2, R1(x) = x,
R2(x) = x2, so that the minimal sufficient statistic is T = T(X) = (T1(X), T2(X))
where 
 and 
. In view of the Theorem 6.6.2,
the statistic T(X) is complete. !
Example 6.6.8 Let X1, ..., Xn be iid Gamma(α, β) so that f(x; α, β) =
{β
αΓ(α)}-1 exp(–x/β)xα-1 with (α, β) ∈ ℜ+ × ℜ+, x ∈ ℜ+ where the parameters
α and β are both assumed unknown. This pdf has the form given in (6.6.9)
where k = 2, θ = (θ1, θ2) with θ1 = 1/β, θ2 = α, R1(x) = –x, R2(x) = log(x), so
that the sufficient statistic is T(X) = (T1(X), T2(X)) where 
and 
. In view of the Theorem 6.6.2,
the statistic T is complete. !
Example 6.6.9 In the Example 6.6.7, if µ is assumed unknown, but σ is
known, then 
 or equivalently 
 is a complete sufficient sta-
tistic for µ. In that same example, if µ is known, but σ is unknown, then
 is a complete sufficient statistic for σ. In the Ex-
ample 6.6.8, if α is unknown, but β is known, then 
 is a
complete sufficient statistic for α. In this case if α is known, but β is un-
known, then 
 or 
 equivalently is a complete sufficient sta-
tistic for β. These results follow immediately from the Theorem 6.6.2. !
Example 6.6.10 (Example 6.6.4 Continued) Suppose that we have X1, ...,
Xn iid Poisson(λ) where λ(> 0) is the unknown parameter. The common pmf
f(x; λ) = e–λλx/x! with χ = {0, 1, 2, ...}, θ = log(λ) has the same representa-
tion given in (6.6.9) where k = 1, p(x) = (x!)-1, q(λ) = exp{e–θ} and R(x) = x.
Hence, in view of the Theorem 6.6.2, the sufficient statistic 
 is
then complete. !
Theorem 6.6.2 covers a lot of ground by helping to prove
the completeness property of sufficient statistics. But it fails to
reach out to non-exponential family members. The case in
point will become clear from the Example 6.6.11.

6. Sufficiency, Completeness, and Ancillarity
324
Example 6.6.11 (Example 6.2.13 Continued) Suppose that X1, ..., Xn are
iid Uniform(0, θ), θ(> 0) being the unknown parameter. We know that T(X) =
Xn:n is a minimal sufficient statistic for θ and the pdf of T is given by f(t; θ) =
ntn-1θ–n I(0 < t < θ) which does not belong to the exponential family defined by
(6.6.9) with k = 1. But, we show directly that T is complete. Let h(t), 0 < t <
θ, be any arbitrary real valued function such that Eθ[h(T)] = 0 for all θ > 0 and
we can write
which proves that h(θ) ≡ 0 for all θ > 0. We have now shown that the minimal
sufficient statistic T is complete. See (1.6.16)-(1.6.17) for the rules on differ-
entiating an integral. !
6.6.2
Basu’s Theorem
Suppose that X = (X1, ..., Xn) has the likelihood function L which depends on
some unknown parameter ! and the observed value x. It is not essential to
assume that X1, ..., Xn are iid in the present setup. Consider now two statistics
U = U(X) and W = W(X). In general, showing that the two statistics U and W
are independent is a fairly tedious process. Usually, one first finds the joint
pmf or pdf of (U, W) and then shows that it can be factored into the two
marginal pmf’s or pdf’s of U and W.
The following theorem, known as Basu’s Theorem, provides a scenario
under which we can prove independence of two appropriate statistics pain-
lessly. Basu (1955a) came up with this elegant result which we state here
under full generality.
Theorem 6.6.3 (Basu’s Theorem) Suppose that we have two vector val-
ued statistics, U = U(X) which is complete sufficient for θθθθθ and W = W(X)
which is ancillary for θθθθθ. Then, U and W are independently distributed.
Proof For simplicity, we supply a proof only in the discrete case. The
proof in the continuous situation is similar. Suppose that the domain spaces
for U and W are respectively denoted by U and W.
In order to prove that U and W are independently distributed, we need to
show that
Now, for w ∈ W, let us denote Pθ(W = W) = h(W). Obviously h(W) is
free from θ since W’s distribution does not involve the parameter θ.

6. Sufficiency, Completeness, and Ancillarity
325
But, observe that Pθ{W = w | U = u} must be free from θ since U is a
sufficient statistic for θ. So, let us write g(u) = Pθ{W = w | U = u}. Now,
Eθ[g(U)] = Pθ(W = w) which we had denoted earlier by h(w), and this is free
from θ. Hence, we have verified that {g(U) - h(w)} is a genuine statistic.
Now, we note that Eθ[g(U) – h(w)] ≡ 0 for all θ ∈ Θ and use the fact that
the statistic U is complete too. Thus, by the Definition 6.6.3 we must have
g(u) – h(w) = 0 w.p.1, that is, g(u) ≡ h(w) for all w ∈ W, u ∈ U. In other
words, we have shown the validity of (6.6.10). !
Example 6.6.12 Let X1, ..., Xn be iid N(µ, σ2) with n ≥ 2, (µ, σ) ∈ ℜ × ℜ+
where µ is unknown, but σ is known. Let U = 
 which is complete sufficient
statistic for µ. Observe that W = S2 is an ancillary statistic for µ. The fact that
W is ancillary can be claimed from its explicit distribution or by appealing to
the location family of distributions. Now, by Basu’s Theorem, the two statis-
tics  
 and S2 are then independently distributed. The Theorem 4.4.2 showed
this via Helmert transformation. !
Example 6.6.13 (Example 6.6.12 Continued) Let V = Xn:n - Xn:1, the sample
range. Then, 
 and S/V are independently distributed. Also, 
 and (Xn:n – 
)
are independent. In the same spirit, and (Xn:1 – 
)2 are independent. Is  
independent of | Xn:n –  
 | /S? The ancillarity of the corresponding statistics
can be verified by appealing to the location family of distributions. We leave
out the details as exercises. !
Example 6.6.14 Let X1, ..., Xn be iid N(µ, σ2) with n ≥ 2, (µ, σ) ∈ ℜ × ℜ+
where σ is unknown, but µ is known. Let U
2 = 
 which is a
complete sufficient statistic for σ2. Observe that W = (X1 – 
)/U is ancillary
for σ. Immediately we can claim that 
 and W are independently
distributed. Also, 
 and (Xn:1 – 
)/S are independent where S2 is
the customary sample variance. Here, ancillarity of the corresponding statis-
tics can be verified by appealing to the scale family of distributions. We leave
out the details as exercises. !
In the N(µ, s2) case with µ, s both unknown, Basu’s Theorem can
be used to show that 
 and S2 are independent. See the Example
6.6.15. Compare its elegance with the brute-force approach
given in the Example 4.4.9 via Helmert transformations.
Example 6.6.15 Suppose that X1, ..., Xn are iid N(µ, σ2) with n ≥ 2, (µ,
σ) ∈ ℜ × ℜ+ where µ and σ are both assumed unknown. Let us see how
we can apply Basu’s Theorem to show that 
 and S2 are independently
distributed. This clever application was originated by D. Basu. Let us have

6. Sufficiency, Completeness, and Ancillarity
326
any two sets A ⊆ ℜ, B ⊆ ℜ+ and we wish to verify that
We now work with a fixed but otherwise arbitrary value σ = σ0(> 0). In this
situation, we may pretend that µ is really the only unknown parameter so that
we are thrown back to the setup considered in the Example 6.6.12, and hence
claim that 
 is complete sufficient for µ and S2 is ancillary for µ. Thus,
having fixed σ = σ0, using Basu’s Theorem we claim that 
 and S2 will be
independently distributed. That is, for all µ ∈ ℜ and fixed σ0(> 0), we have so
far shown that
But, then (6.6.12) holds for any fixed value σ0 ∈ ℜ+. That is, we can claim
the validity of (6.6.12) for all (µ, σ0) ∈ ℜ × ℜ+. There is no difference be-
tween what we have shown and what we started out to prove in (6.6.11).
Hence, (6.6.11) holds. !
From the Example 6.6.15, the reader may think that we
used the sufficiency property of 
 and ancillarity property
of S2. But if µ, s are both unknown, then certainly 
 is
not sufficient and S2 is not ancillary. So, one may think
that the previous proof must be wrong. But, note that we
used the following two facts only: when σ = σ0 is fixed
but arbitrary,  
 is sufficient and S2 is ancillary.
Example 6.6.16 (Example 6.6.15 Continued) In the Example 6.6.15, the
statistic U = (
, S2) is complete sufficient for θ = (µ, σ2) while W = (X1 –
X2)/S is a statistic whose distribution does not depend upon θ. Here, we may
use the characteristics of a location-scale family. To check directly that W
has a distribution which is free from θ, one may pursue as follows: Let Yi =
(Xi – µ)/σ which are iid standard normal, i = 1, ..., n, and then note that the
statistic W can also be expressed as (Y1 - Y2)/S* where S*2 = (n – 1)-1
 with 
. The statistics U and W are independent
by virtue of Basu’s Theorem. In this deliberation, the ancillary statistic W
can be vector valued too. For example, with n ≥ 3, suppose that we define
W* = ({X1 – X2}/S, {X2 – X3}/|X1 + X2 – 2X3|). As before, we can rewrite
W* as ({Y1 – Y2}/S*, {Y2 – Y3}/|Y1 + Y2 – 2Y3|) where we recall that the Y’s
are iid standard normal, and hence W* is ancillary for θ. Hence, U and W*
are independent by virtue of Basu’s Theorem. !

6. Sufficiency, Completeness, and Ancillarity
327
In the Examples 6.6.12-6.6.15, the arguments revolved around
sufficiency and completeness to come face to face with the result
that 
 and S2 are independent. A reader may get the wrong
impression that the completeness property is essential to claim
that 
 and S2 are independent. But, note that (
, S2) is not a
complete statistic when we have random samples from N(θ, θ)
or N(θ, θ2) population, with θ > 0. Yet it is true that 
 and S2
are independent in such situations. Refer to the Example 4.4.9.
Example 6.6.17 (Example 6.6.15 Continued) Suppose that X1, ..., Xn are
iid N(µ, σ2) with (µ, σ) ∈ ℜ × ℜ+ where µ and σ are both assumed unknown.
By Basu’s Theorem, one can immediately claim that the statistic (
, S2) and
(X1 – 
)/(Xn:n – Xn:1) are independent. !
Example 6.6.18 (Example 6.6.11 Continued) Suppose that X1, ..., Xn are
iid Uniform(0, θ) with n ≥ 2, θ(> 0) being the unknown parameter. We know
that U = Xn:n is a complete sufficient statistic for θ. Let W = Xn:1/Xn:n which is
ancillary for θ. Hence, by Basu’s Theorem, Xn:n and Xn:1/Xn:n are indepen-
dently distributed. Also, Xn:n and 
/S are independent, since 
/S is ancillary
for θ where S2 stands for the sample variance. Using a similar argument, one
can also claim that (Xn:n – Xn:1/S and Xn:n are independent. One may look at the
scale family of distributions to verify the ancillarity property of the appropri-
ate statistics. !
Remark 6.6.1 We add that a kind of the converse of Basu’s Theorem was
proved later in Basu (1958). Further details are omitted for brevity.
6.7
Exercises and Complements
6.2.1 Suppose that X1, X2 are iid Geometric(p), that is the common pmf is
given by f(x;p) = p(1 – p)x, x = 0, 1, 2, ... where 0 < p < 1 is the unknown
parameter. By means of the conditional distribution approach, show that
X1 + X2 is sufficient for p.
6.2.2 Suppose that X1, ..., Xm are iid Bernoulli(p), Y1, ...., Yn are iid
Bernoulli(q), and that the X’s are independent of the Y’s where 0 < p < 1 is
the unknown parameter with q = 1 - p. By means of the conditional distribu-
tion approach, show that 
 is sufficient for p. {Hint: In-
stead of looking at the data (X1, ..., Xm, Y1, ...., Yn), can one justify looking
at (X1, ..., Xm, 1 - Y1, ...., 1 - Yn)?}

6. Sufficiency, Completeness, and Ancillarity
328
6.2.3 Suppose that X is distributed as N(0, σ2) where 0 < σ < ∞ is the
unknown parameter. Along the lines of the Example 6.2.3, by means of the
conditional distribution approach, show that the statistic | X | is sufficient for
σ2.
6.2.4 Suppose that X is N(θ, 1) where –∞ < θ < ∞ is the unknown param-
eter. By means of the conditional distribution approach, show that | X | can
not be sufficient for θ.
6.2.5 (Exercise 6.2.2 Continued) Suppose that m = 2, n = 3. By means of
the conditional distribution approach, show that X1 + Y1Y2 can not be suffi-
cient for p.
6.2.6 Suppose that X1, ..., Xm are distributed as iid Poisson(λ), Y1, ..., Yn
are iid Poisson(2λ), and that the X’s are independent of the Y’s where 0 < λ <
∞ is the unknown parameter. By means of the conditional distribution ap-
proach, show that 
 is sufficient for ?.
6.2.7 (Exercise 6.2.6 Continued) Suppose that m = 4, n = 5. Show that X1
+ Y1 can not be sufficient for λ.
6.2.9 (Example 6.2.5 Continued) Let X1, ..., X4 be iid Bernoulli(p) where 0
< p < 1 is the unknown parameter. Consider the statistic U = X1(X3 + X4) + X2.
By means of the conditional distribution approach, show that the statistic U is
not sufficient for p.
6.2.10 Let X1, ..., Xn be iid N(µ, σ2) where –∞ < µ < ∞, 0 < σ < ∞. Use the
Neyman Factorization Theorem to show that
(i)
 is sufficient for µ if σ is known;
(ii)
 is sufficient for σ if µ is known.
6.2.11 Let X1, ..., Xn be iid having the common pdf σ–1 exp{–(x – µ)/σ}I(x
> µ) where –∞ < µ < ∞, 0 < σ < ∞. Use the Neyman Factorization Theorem
to show that
(i)
Xn:1, the smallest order statistic, is sufficient for µ if σ is known;
(ii)
 is sufficient for σ if µ is known;
(iii)
 is jointly sufficient for (µ, σ) if both
parameters are unknown.
6.2.12 Let X1, ..., Xn be iid having the Beta(α, β) distribution with the
parameters α and β, so that the common pdf is given by

6. Sufficiency, Completeness, and Ancillarity
329
where b(α, β) = Γ(α)Γ(β){Γ(α + β)}–1. Refer to (1.7.35) as needed. Use the
Neyman Factorization Theorem to show that
(i)
 is sufficient for α if β is known;
(ii)
 is sufficient for β if α is known;
(iii)
 is jointly sufficient for (α, β) if both
parameters are unknown.
6.2.13 Suppose that X1, ..., Xn are iid with the Uniform distribution on the
interval (θ - ½, θ + ½), that is the common pdf is given by f(x; θ) = I(θ – ½
< x < θ + ½) where θ(> 0) is the unknown parameter. Show that (Xn:1, Xn:n) is
jointly sufficient for θ.
6.2.14 Solve the Examples 6.2.8-6.2.11 by applying the Theorem 6.2.2
based on the exponential family.
6.2.15 Prove the Theorem 6.2.2 using the Neyman Fctorization Theorem.
6.2.16 Suppose that X1, ..., Xn are iid with the Rayleigh distribution, that is
the common pdf is
where θ(> 0) is the unknown parameter. Show that 
 is sufficient for
θ.
6.2.17 Suppose that X1, ..., Xn are iid with the Weibull distribution, that is
the common pdf is
where α(> 0) is the unknown parameter, but β(> 0) is assumed known that
 is sufficient for α. 
6.3.1 (Exercise 6.2.10 Continued) Let X1, ..., Xn be iid N(µ, σ2) where –∞
< µ < ∞, 0 < σ < ∞. Show that
(i)
is minimal sufficient for µ if σ is known;
(ii)
is minimal sufficient for σ if µ is known.
6.3.2 (Exercise 6.2.11 Continued) Let X1, ..., Xn be iid having the common
pdf σ–1 exp{–(x – µ)/σ}I(x > µ) where –∞ < µ < ∞, 0 < σ < ∞. Show that
(i)
Xn:1, the smallest order statistic, is minimal sufficient for µ if
σ is known;
(ii)
 is minimal sufficient for s if µ is known;
(iii)
 is minimal sufficient for (µ, σ) if both
parameters are unknown.

6. Sufficiency, Completeness, and Ancillarity
330
6.3.3 Solve the Exercises 6.2.5-6.2.8 along the lines of the Examples 6.3.2
and 6.3.5.
6.3.4 Show that the pmf or pdf corresponding to the distributions such as
Binomial(n, p), Poisson(λ), Gamma(α, β), N(µ, σ2), Beta(α, β) belong to the
exponential family defined in (6.3.5) when
(i)
0 < p < 1 is unknown;
(ii)
λ ∈ ℜ+ is unknown;
(iii)
µ ∈ ℜ is unknown but σ ∈ ℜ+ is known;
(iv)
σ ∈ ℜ+ is unknown but µ ∈ ℜ is known;
(v)
µ ∈ ℜ and σ ∈ ℜ+ are both unknown;
(vi)
α ∈ ℜ+ is known but β ∈ ℜ+ is unknown;
(vii)
α ∈ ℜ+, β ∈ ℜ+ are both unknown.
In each case, obtain the minimal sufficient statistic(s) for the associated
unknown parameter(s).
6.3.5 (Exercise 6.2.1 Continued) Let X1, ..., Xn be Geometric(p), that is the
common pmf is given by f(x;p) = p(1 - p)x, x = 0, 1, 2, ... where 0 < p < 1 is the
unknown parameter. Show that this pmf belongs to the exponential family de-
fined in (6.3.5). Hence, show that 
 is minimal sufficient for p.
6.3.6. Show that the common pdf given in the Exercises 6.2.16-6.2.17
respectively belongs to the exponential family.
6.3.7 Suppose that X1, ..., Xn are iid with the Uniform distribution on the
interval (θ – ½, θ + ½), that is the common pdf is given by f(x;θ) = I(θ – ½ <
x < θ + ½) where θ(> 0) is the unknown parameter. Show that (Xn:1, Xn:n) is
jointly minimal sufficient for θ.
6.3.8 Let X1, ..., Xn be iid N(θ, θ2) where 0 < θ < ∞ is the unknown
parameter. Derive the minimal sufficient statistic for ∞. Does the common
pdf belong to the exponential family (6.3.5)? {Hint: If it does belong to the
exponential family, it should be a 2-parameter family, but it is not so. Does the
parameter space include a 2-dimensional rectangle?}
6.3.9 Let X1, ..., Xn be iid N(θ, θ) where 0 < θ < ∞ is the unknown param-
eter. Derive the minimal sufficient statistic for θ. Does the common pdf be-
long to the exponential family (6.3.5)? {Hint: If it does belong to the exponen-
tial family, it should be a 2-parameter family, but it is not so. Does the param-
eter space include a 2-dimensional rectangle?}
6.3.10 Let X1, ..., Xn be iid having a negative exponential distribution
with the common pdf θ–1 exp{–(x – θ)/θ}I(x > θ) where 0 < θ < ∞ is the
unknown parameter. Derive the minimal sufficient statistics for θ. Does
the common pdf belong to the exponential family (6.3.5)? {Hint: If it does

6. Sufficiency, Completeness, and Ancillarity
331
belong to the exponential family, it should be a 2-parameter family, but it is not
so. Does the parameter space include a 2-dimensional rectangle?}
6.3.11 Let X1, ..., Xn be iid having a negative exponential distribution with
the common pdf θ–2 exp{–(x – θ)/θ2}I(x > θ) where 0 < θ < ∞ is the unknown
parameter. Derive the minimal sufficient statistics for θ. Does the common
pdf belong to the exponential family (6.3.5)? {Hint: If it does belong to the
exponential family, it should be a 2-parameter family, but it is not so. Does the
parameter space include a 2-dimensional rectangle?}
6.3.12 Let X1, ..., Xn be iid having the common Uniform distribution on the
interval (–θ, θ) where 0 < θ < ∞ is the unknown parameter. Derive the mini-
mal sufficient statistic for θ.
6.3.13 Let X1, ..., Xm be iid N(µ1, σ2), Y1, ..., Yn be iid N(µ2, σ2), and also let
the X’s be independent of the Y’s where –∞ < µ1, µ2 < ∞, 0 < σ < ∞ are the
unknown parameters. Derive the minimal sufficient statistics for (µ1, µ2, σ2).
6.3.14 (Exercise 6.3.13 Continued) Let X1, ..., Xm be iid N(µ1, σ2), Y1, ...,
Yn be iid N(µ2, kσ2), and also let the X’s be independent of the Y’s where –∞
< µ1, µ2 < ∞, 0 < σ < ∞ are the unknown parameters. Assume that the number
k (> 0) is known. Derive the minimal sufficient statistic for (µ1, µ2, σ2).
6.3.15 Let X1, ..., Xm be iid Gamma(α, β), Y1, ..., Yn be iid Gamma(α, kβ),
and also let the X’s be independent of the Y’s with 0 < α, β < ∞ where β is the
only unknown parameter. Assume that the number k (> 0) is known. Derive
the minimal sufficient statistic for β.
6.3.16 (Exercise 6.2.12 Continued) Let X1, ..., Xn be iid having a Beta
distribution with its parameters α = β = θ where θ (> 0) is unknown. Derive
the minimal sufficient statistic for θ.
6.3.17 Let X1, ..., Xm be iid N(µ, σ2), Y1, ..., Yn be iid N(0, σ2), and also let
the X’s be independent of the Y’s where –∞ < µ < ∞, 0 < σ < ∞ are the
unknown parameters. Derive the minimal sufficient statistics for (µ, σ2).
6.3.18 Let X1, ..., Xm be iid N(µ, σ2), Y1, ..., Yn be iid N(0, kσ2), and also let
the X’s be independent of the Y’s where –∞ < µ < ∞, 0 < σ < ∞ are the
unknown parameters. Assume that the number k (> 0) is known. Derive the
minimal sufficient statistics for (µ, σ2).
6.3.19 Suppose that X1, ..., Xn are iid with the common pdf given by one
of the following:

6. Sufficiency, Completeness, and Ancillarity
332
(i)
f(x; θ) = exp(–(x – θ)}/[1 + exp{–(x – θ)}]2, x ∈ ℜ, θ ∈ ℜ,
which is called the logistic distribution;
(ii)
f(x; θ) = 1/π{1 + (x – θ)2}–1, x ∈ ℜ, ? ∈ ℜ;
(iii)
f(x; θ) = ½ exp{– | x – θ |}, x ∈ ℜ, ? ∈ ℜ.
In each case, show that the order statistics (Xn:1, ..., Xn:n) is minimal suf-
ficient for the unknown location parameter θ. That is, we do not achieve any
significant reduction of the original data X = (X1, ..., Xn). {Hint: Part (i) is
proved in Lehmann (1983, p. 43). Parts (ii) and (iii) can be handled similarly.}
6.4.1 Let X1, ..., Xn be iid Bernoulli(p) where 0 < p < 1 is the unknown
parameter. Evaluate IX(p), the information content in the whole data X = (X1,
..., Xn). Compare IX(p) with 
 . Can the Theorem 6.4.2 be used here to
claim that 
 is sufficient for p?
6.4.2 (Exercise 6.4.1 Continued) Let X1, ..., Xn be iid Bernoulli(p) where 0
< p < 1 is the unknown parameter, n ≥ 3. Let T = X1 + X2, U = X1 + X2 + 2X3.
Compare IX(p) with IT(p) and IU(p). Can T be sufficient for p? Can U be
sufficient for p? {Hint: Try to exploit the Theorem 6.4.2}
6.4.3 Verify the results given in equations (6.4.9) and (6.4.11).
6.4.4 (Exercise 6.2.1 Continued) Let X1, X2, be iid Geometric(p) where 0
< p < 1 is the unknown parameter. Let X = (X1, X2), and T = X1 + X2 which is
sufficient for p. Evaluate IX(p) and IT(p), and then compare these two infor-
mation contents.
6.4.5 (Exercise 6.2.10 Continued) In a N(µ, σ2) distribution where –∞ < µ
< ∞, 0 < σ < ∞, suppose that only µ is known. Show that IU2(s2) > IS2(s2)
where U2 = n–1 
 and S2 = (n – 1)–1 
 
)2, n ≥ 2.
Would it then be fair to say that there is no point in using the statistic S2 which
making inferences about σ2 when µ is assumed known?
6.4.6 (Exercise 6.2.11 Continued) In the two-parameter negative expo-
nential distribution, if only µ is known, show that IV(σ) > IT(σ) where V = n–
1 
 and T = (n - 1)-1 
, n ≥ 2. Would it then be fair
to say that there is no point in using the statistic T while making inferences
about σ when µ is assumed known?
6.4.7 (Exercise 6.3.17 Continued) Suppose that we have X1, ..., Xm are iid
N(µ, σ2), Y1, ..., Yn are iid N(0, σ2), the X’s are independent of the Y’s where
–∞ < µ < ∞, 0 < σ < ∞ are the unknown parameters. Suppose that T = T(X,
Y) is the minimal sufficient statistic for θ = (µ, σ2). Evaluate the expressions
of the information matrices IX,Y(θ) and IT(θ), and then compare these two
information contents.
6.4.8 (Example 6.2.5 Continued) Consider the statistics X1X2 and U.

6. Sufficiency, Completeness, and Ancillarity
333
Find the pmf of X1X2 and then using the fact that X1X2 is independent of X3,
derive the pmf of U. Hence, find the expressions of IX(p), and 
IU(p) where X = (X1, X2, X3). Show that IU(p) < 
 < IX(p). This
shows that U or (X1X2, X3) can not be sufficient for p.
6.4.9 (Exercise 6.2.16 Continued) Suppose that X1, ..., Xn are iid with the
Rayleigh distribution, that is the common pdf is
where θ(> 0) is the unknown parameter. Denote the statistic 
.
Evaluate IX(θ) and IT(θ). Are these two information contents the same? What
conclusions can one draw from this comparison?
6.4.10 (Exercise 6.2.17 Continued) Suppose that X1, ..., Xn are iid with the
Weibull distribution, that is the common pdf is
where α (> 0) is the unknown parameter, but β(> 0) is assumed known.
Denote the statistic T=Σn
i=1X2
i. Evaluate IX(θ) and IT(θ). Are these two infor-
mation contents the same? What conclusions can one draw from this com-
parison?
6.4.11 Prove the Theorem 6.4.3. {Hint: One may proceed along the lines
of the proof given in the case of the Theorem 6.4.1.}
6.4.12 Prove the Theorem 6.4.4 when
(i)
X, θ are both vector valued, but X has a continuous distribution;
(ii) X, θ are both real valued, but X has a discrete distribution.
6.4.13 (Exercise 6.3.8 Continued) Let X1, ..., Xn be iid N(θ, θ2) where 0 <
θ < ∞ is the unknown parameter. Evaluate IX(θ) and 
, and compare
these two quantities. Is it possible to claim that 
 is sufficient for θ?
6.4.14 (Exercise 6.3.9 Continued) Let X1, ..., Xn be iid N(θ, θ) where 0 <
θ < ∞ is the unknown parameter. Evaluate IX(θ) and 
, and compare
these two quantities. Is it possible to claim that 
 is sufficient for θ?
6.4.15 (Proof of the Theorem 6.4.2) Let θ be a real valued parameter.
Suppose that X is the whole data and T = T(X) is some statistic. Then, show
that IX(θ) ≥ IT(θ) for all θ ∈ Θ. Also verify that the two information mea-
sures match with each other for all θ if and only if T is a sufficient statis-
tic for θ. {Hint: This interesting idea was included in a recent personal

6. Sufficiency, Completeness, and Ancillarity
334
communication from C. R. Rao. Note that the likelihood function f(x; θ) can
be written as g(t; θ)h(X | T = t; θ) where g(t; θ) is the pdf or pmf of T and h(x
| T = t; θ) is the conditional pdf or pmf of X given that T = t. From this
equality, first derive the identity: IX(θ) = IT(θ) + IX|T(θ) for all θ ∈ Θ. This
implies that IX(θ) ≥ IT(θ) for all θ ∈ Θ. One will have IX(θ) = IT(θ) if and only
if IX|T(θ) = 0, that is h(x | T = t;θ) must be free from θ. It then follows from
the definition of sufficiency that T is sufficient for θ.}
6.4.16 Suppose that X1, X2 are iid N(θ, 1) where the unknown parameter θ
∈ ℜ. ‘We know that T = X1 + X2 is sufficient for θ and also that IT(θ) = 2.
Next, consider a statistic Up = X1 + pX2 where p is a known positive number.
(i)
Show that IUp(θ) = (1 + p)2/(1 + p2);
(ii)
Show that 1 < IUp(θ) ≤ 2 for all p(> 0);
(iii)
Show that IUp(θ) = 2 if and only if p = 1;
(iv)
An expression such as IUp(θ)/IT(θ) may be used to quantify the ex-
tent of non-sufficiency or the fraction of the lost information due to
using Up instead of utilizing the sufficient statistic T. Analytically,
study the behavior of IUp(θ)/IT(θ) as a function of p(> 0);
(v)
Evaluate IUp(θ)/IT(θ) for p = .90, .95, .99, 1.01, 1.05, 1.1. Is Up too
non-sufficient for θ in practice when p = .90, .95, .99, 1.01?
(vi)
In practice, if an experimenter is willing to accept at the most 1%
lost information compared with the full information IT(θ), find the
range of values of p for which the non-sufficient statistic Up will
attain the goal.
{Note: This exercise exploits much of what has been learned conceptually
from the Theorem 6.4.2.}
6.5.1 (Example 6.5.2 Continued) Find the pdf of U3 defined in (6.5.1).
Hence, show that T3 is ancillary for θ.
6.5.2 (Example 6.5.3 Continued) Show that T3 is ancillary for λ.
6.5.3 (Example 6.5.4 Continued) Show that T = (T1, T2) is distributed as
N2(0, 0, 2, 6, 0). Are T1, T2 independent?
6.5.4 (Example 6.5.5 Continued) Show that T is ancillary for θ.
6.5.5 (Exercise 6.3.7 Continued) Let X1, ..., Xn be iid having the common
Uniform distribution on the interval (θ – ½, θ + ½) where –∞ < θ < ∞ is the
unknown parameter. Show that Xn:n – Xn:1 is an ancillary statistic for θ.

6. Sufficiency, Completeness, and Ancillarity
335
6.5.6 (Exercise 6.3.12 Continued) Let X1, ..., Xn be iid having the common
Uniform distribution on the interval (–θ, θ) where 0 < θ < ∞ is the unknown
parameter. Show that Xn:n/Xn:1 is an ancillary statistic for θ. Also show that
Xn:n/(Xn:n – Xn:1) is an ancillary statistic for θ.
6.5.7 (Curved Exponential Family) Suppose that (X, Y) has a particular
curved exponential family of distributions with the joint pdf given by
where θ(> 0) is the unknown parameter. This distribution was discussed by
Fisher (1934, 1956) in the context of the famous “Nile” example. Denote U =
XY, V = X/Y. Show that U is ancillary for θ, but (U, V) is minimal sufficient
for θ, whereas V by itself is not sufficient for θ. {Hint: The answers to the
Exercise 4.4.10 would help in this problem.}
6.5.8 Show that the families F1, F2, F3 defined via (6.5.7) include only
genuine pdf’s.
6.5.9 (Exercise 6.3.8 Continued) Let X1, ..., Xn be iid N(θ, θ2) where 0 < θ
< ∞ is the unknown parameter. Construct several ancillary statistics for θ.
Does the common pdf belong to one of the special families defined via (6.5.7)?
6.5.10 (Exercise 6.3.9 Continued) Let X1, ..., Xn be iid N(θ, θ) where 0 <
θ < 8 is the unknown parameter. Construct several ancillary statistics for θ.
Does the common pdf belong to one of the special families defined via (6.5.7)?
6.5.11 (Exercise 6.3.10 Continued) Let X1, ..., Xn be iid having the com-
mon pdf θ–1 exp{ – (x – θ)/θ}I(x > θ) where 0 < θ < 8 is the unknown
parameter. Construct several ancillary statistics for θ. Does the common pdf
belong to one of the special families defined via (6.5.7)?
6.5.12 (Exercise 6.3.11 Continued) Let X1, ..., Xn be iid having the com-
mon pdf θ–2 exp{ – (x – θ)/θ2}I(x > θ) where 0 < θ < ∞ is the unknown
parameter. Construct several ancillary statistics for θ. Does the common pdf
belong to one of the special families defined via (6.5.7)?
6.5.13 Use the Definition 6.4.1 of the information and the pdf from (6.5.4)
to derive directly the expression for IX,Y(ρ). {Hint: Take the log of the pdf
from (6.5.4). Then take the partial derivative of this with respect to ?. Next,
square this partial derivative and take its expectation.}
6.5.14 Suppose that (X, Y) is distributed as N2(0, 0, σ2, σ2, ρ) where
-1 < ρ < 1 is the unknown parameter while 0 < σ (≠ 1) < ∞ is assumed
known. Evaluate the expression for the information matrix IX,Y(ρ) along

6. Sufficiency, Completeness, and Ancillarity
336
the lines of the Example 6.5.8. Next, along the lines of the Example 6.5.11,
recover the lost information in X by means of conditioning on the ancillary
statistic Y.
6.5.15 (Example 6.5.8 Continued) Suppose that (X, Y) is distributed as
N2(0, 0, σ2, σ2, ρ) where θ = (σ2, ρ) with the unknown parameters σ2, ρ
where 0 < σ < ∞, –1 < ρ < 1. Evaluate the expression for the information
matrix IX,Y(θ). {Hint: Does working with U = X + Y, V = X - Y help in the
derivation?}
6.5.16 Suppose that X′ = (X1, ..., Xp) where X is distributed as multivariate
normal Np (0, σ2[(1 – ρ)Ip×p + ρ11′]) with 1′ = (1, 1, ....1), σ ∈ ℜ+ and – (p –
1)–1 < ρ < 1. We assume that θ = (σ2, ρ) where σ2, ρ are the unknown
parameters, 0 < σ < ∞, –1 < ρ < 1. Evaluate the expression for the informa-
tion matrix IX(θ). {Hint: Try the Helmert transformation from the Example
2.4.9 on X to generate p independent normal variables each with zero mean,
and variances depending on both σ2 and ρ, while (p - 1) of these variances are
all equal but different from the pth one. Is it then possible to use the Theorem
6.4.3?}
6.5.17 (Example 6.5.8 Continued) Suppose that (X, Y) is distributed as
N2(0, 0, 1, 1, ρ) where ρ is the unknown parameter, –1 < ρ < 1. Start with the
pdf of (X, Y) and then directly apply the equivalent formula from the equation
(6.4.9) for the evaluation of the expression of IX,Y(ρ).
6.6.1 Suppose that X1, X2 are iid Poisson(λ) where λ(> 0) is the unknown
parameter. Consider the family of distributions induced by the statistic T =
(X1, X2). Is this family, indexed by λ, complete?
6.6.2 (Exercise 6.3.5 Continued) Let X1, ..., Xn be iid Geometric(p), that is
the common pmf is given by f(x; p) = p(1 - p)x, x = 0, 1, 2, ... where 0 < p <
1 is the unknown parameter. Is the statistic 
 complete sufficient for p?
{Hint: Is it possible to use the Theorem 6.6.2 here?}
6.6.3 Let X1, ..., Xn be iid N(θ, θ2) where 0 < θ < ∞ is the unknown
parameter. Is the minimal sufficient statistic complete? Show that 
 and S2
are independent. {Hint: Can the Example 4.4.9 be used here to solve the
second part?}
6.6.4 Let X1, ..., Xn be iid N(θ, θ) where 0 < θ < ∞ is the unknown param-
eter. Is the minimal sufficient statistic complete? Show that 
 and S2 are
independent. {Hint: Can the Example 4.4.9 be used here to solve the second
part?}
6.6.5 Let X1, ..., Xn be iid having a negative exponential distribution
with the common pdf θ–1 exp{–(x – θ)/θ}I(x > θ) where 0 < θ < ∞ is the
unknown parameter. Is the minimal sufficient statistic complete? Show

6. Sufficiency, Completeness, and Ancillarity
337
that Xn:1 and 
 are independent. {Hint: Can the Example 4.4.12
be used here to solve the second part?}
6.6.6 Let X1, ..., Xn be iid having a negative exponential distribution with
the common pdf θ–2exp{–(x – θ)/θ2}I(x > θ) where 0 < θ < ∞ is the unknown
parameter. Is the minimal sufficient statistic complete? Show that Xn:1 and
 are independent. {Hint: Can the Example 4.4.12 be used
here to solve the second part?}
6.6.7 Let X1, ..., Xn be iid having the common Uniform distribution on the
interval (θ – ½, θ + ½) where –∞ < θ < ∞ is the unknown parameter. Is the
minimal sufficient statistic T = (Xn:1, Xn:n) complete?
6.6.8 Let X1, ..., Xn be iid having the common Uniform distribution on the
interval (–θ, θ) where 0 < θ < ∞ is the unknown parameter. Find the minimal
sufficient statistic T for θ. Is the statistic T complete? {Hint: Use indicator
functions appropriately so that the problem reduces to the common pdf’s of
the random variables | Xi|, i = 1, ..., n.}
6.6.9 Let X1, ..., Xm be iid N(µ1, σ2), Y1, ..., Yn be iid N(µ2, σ2), the X’s are
independent of the Y’s where –∞ < µ1, µ2 < ∞, 0 < σ < ∞ are all unknown
parameters. Is the minimal sufficient statistic for (µ1, µ2, σ2) complete? {Hint:
Is it possible to use the Theorem 6.6.2 here?}
6.6.10 (Exercise 6.3.13 Continued) Let X1, ..., Xm be iid N(µ1, σ2), Y1, ...,
Yn be iid N(µ2, kσ2), the X’s be independent of the Y’s where –∞ < µ1, µ2 < ∞,
0 < σ < ∞ are all unknown parameters, but k (> 0) is known. Is the minimal
sufficient statistic for (µ1, µ2, σ2) complete? {Hint: Is it possible to use the
Theorem 6.6.2 here?}
6.6.11 (Exercise 6.3.15 Continued) Let X1, ..., Xm be iid Gamma(α, β), Y1,
..., Yn be iid Gamma(α, kβ), the X’s be independent of the Y’s, with 0 < α, β
< ∞ where β is the only unknown parameter. Assume that the number k (> 0)
is known. Is the minimal sufficient statistic for β complete? {Hint: Is it pos-
sible to use the Theorem 6.6.2 here?}
6.6.12 (Exercise 6.3.16 Continued) Let X1, ..., Xn be iid having a Beta
distribution with its parameters α = β = θ where θ (> 0) is unknown. Is the
minimal sufficient statistic for θ complete? {Hint: Is it possible to use the
Theorem 6.6.2 here?}
6.6.13 (Exercise 6.6.9 Continued) Let the X’s be independent of the
Y’s, X1, ..., Xm be iid N(µ1, σ2), Y1, ..., Yn be iid N(µ2, σ2) where –∞ < µ1,
µ2 < ∞, 0 < σ < ∞ are all unknown parameters. Use Basu’s Theorem
along the lines of the Example 6.6.15 to show that 
 is distributed

6. Sufficiency, Completeness, and Ancillarity
338
independently of 
. Is (
 –
)3 distrib-
uted independently of T? Is {(Xn:n – 
) – (Yn:n – 
)}2/(Xn:n – Xn:1)2/T distrib-
uted independently of T? Is {(Xn:n – 
) – (Yn:n – 
)}2/(Xn:n – Xn:1)2 distributed
independently of T? {Hint: Can the characteristics of a location-scale family
and (6.5.10) be used here?}
6.6.14 (Exercise 6.6.8 Continued) Let X1, ..., Xn be iid having the common
Uniform distribution on the interval (–θ, θ) where 0 < θ < ∞ is the unknown
parameter. Let us denote 
. Is T distributed independently of
| Xn:1|/Xn:n? Is T distributed independently of (X1 – X2)2/{Xn:1Xn:n}? {Hint: Can
the characteristics of a scale family and (6.5.9) be used here?}
6.6.15 (Exercise 6.6.14 Continued) Let X1, ..., Xn be iid having the com-
mon Uniform distribution on the interval (–θ, θ) where 0 < θ < ∞ is the
unknown parameter. Let us denote 
. Is T distributed indepen-
dently of the two-dimensional statistic (U1, U2) where U1 = | Xn:1 |/Xn:n, U2 =
(X1 – X2)2/{Xn:1Xn:n}? {Hint: Can the characteristics of a scale family and
(6.5.9) be used here?}
6.6.16 (Exercise 6.6.13 Continued) Let X1, ..., Xm be iid N(µ1, σ2), Y1, ...,
Yn be iid N(µ2, σ2), the X’s be independent of the Y’s where –∞ < µ1, µ2 < ∞,
0 < σ < ∞ are all unknown parameters. Use Basu’s Theorem to check whether
the following two-dimensional statistics
are distributed independently where 
.
{Hint: Can the characteristics of a location-scale family and (6.5.10) be used
here?}
6.6.17 Let X1, ..., Xn be iid having a negative exponential distribution with
the common pdf σ–1exp{–(x – θ)/σ}I(x > θ) where θ and σ are both unknown
parameters, –∞ < θ < ∞, 0 < σ < ∞. Argue as in the Example 6.6.15 to show
that Xn:1 and 
 are independent.
6.6.18 (Exercise 6.3.2 Continued) Let X1, ..., Xn be iid having the common
pdf σ–1exp{–(x – µ)/σ}I(x > µ) where –∞ < µ < ∞, 0 < σ < ∞. Show that
(i)
Xn:1, the smallest order statistic, is complete if µ is unknown
but σ is known;
(ii)
n-1 
 is complete if σ is unknown but µ is known;
(iii)
(Xn:1, 
 is complete if both µ, σ are unknown.

6. Sufficiency, Completeness, and Ancillarity
339
{Hint: Part (ii) follows from the Theorem 6.6.2. One may look at Lehmann
(1983, pp. 47-48) for details.}
6.6.19 Let X1, ..., X4 be iid N(µ, σ2) where –∞ < µ < ∞, 0 < σ < ∞ are both
unknown parameters. Evaluate the expression for
where 
 and S2 = 1/3 
. {Note: Can Basu’s Theo-
rem be used here?}

This page intentionally left blank

7
Point Estimation
7.1 Introduction
We begin with iid observable random variables X1, ..., Xn having a common
pmf or pdf f(x), x ∈ χ, the domain space for x. Here, n is assumed known and
it is referred to as the (fixed) sample size. We like to think of a population
distribution which in practice may perhaps be approximated by f(x). For ex-
ample, in a population of two thousand juniors in a college campus, we may
be interested in the average GP A(= X) and its distribution which is denoted
by f(x) with an appropriate domain space χ = (0, 4) for x. The population
distribution of X may be characterized or indexed by some parameter θθθθθ, for
example the median GP A of the population. The practical significance of θθθθθ is
that once we fix a value of θθθθθ, the population distribution f(x) would then be
completely specified. Thus, we denote the pmf or pdf by f(x; θθθθθ), instead of
just f(x), so that its dependence on a few specific population-features (that is,
θθθθθ) is now made explicit. The idea of indexing a population distribution by the
unknown parameter θθθθθ was also discussed in Chapter 6.
We suppose that the parameter θθθθθ is fixed but otherwise unknown and that
the possible values of θθθθθ belong to a parameter space Θ ⊆ ℜk. For example, we
may be able to postulate that the X’s are distributed as N (µ, σ2) where µ is the
only unknown parameter, −∞ < µ < ∞, 0 < σ < ∞. In this case we may denote
the pdf of X by f(x; µ) where θ = µ ∈ Θ = ℜ, χ = ℜ. If µ and σ2 are both
unknown, then the population density would be denoted by f(x; θθθθθ) where θθθθθ =
(µ, σ2) ∈ Θ = ℜ × ℜ+, χ = ℜ.
The Definition 7.2.1 gives a formal statement of what an estimator of a
parameter is. In this chapter, we consider only point estimation problems. In
Section 7.2, we first apply the method of moments, originated by Karl Pearson
(1902), to find estimators of θθθθθ. This approach is ad hoc in nature, and hence
we later introduce a more elaborate way of finding estimators by the method
of maximum likelihood. The latter approach was pioneered by Fisher (1922,
1925a, 1934).
One may arrive at different choices of estimators for the unknown pa-
rameter θθθθθ and hence some criteria to compare their performances are ad-
dressed in Section 7.3. One criterion which stands out more than anything
341

342
7. Point Estimation
else is called unbiasedness of an estimator and we follow this up with a notion
of the best estimator among all unbiased estimators. Sections 7.4 and 7.5
include several fundamental results, for example, the Rao-Blackwell Theo-
rem, Cramér-Rao inequality, and Lehmann-Scheffé Theorems. These ma-
chineries are useful in finding the best unbiased estimator of θ in different
situations. The Section 7.6 addresses a situation which arises when the Rao-
Blackwellization technique is used but the minimal sufficient statistic is not
complete. In Section 7.7, an attractive large sample criterion called consis-
tency, proposed by Fisher (1922), is discussed.
7.2   Finding Estimators
Consider iid and observable real valued random variables X1, ..., Xn from a
population with the common pmf or pdf f(x; θθθθθ) where the unknown param-
eter θθθθθ ∈ Θ ⊆ ℜk. It is not essential for the X’s to be real valued or iid. But, in
many examples they will be so and hence we assume that the X’s are real
valued and iid unless specified otherwise. As before, we denote X = (X1, ...,
Xn).
Definition 7.2.1 An estimator or a point estimator of the unknown pa-
rameter θθθθθ is merely a function T = T(X1, ..., Xn) which is allowed to depend
only on the observable random variables X1, ..., Xn. That is, once a particular
data X = x has been observed, the numerical value of T(x) must be comput-
able. We distinguish between T(X) and T(x) by referring to them as an estima-
tor and an estimate of θθθθθ  respectively.
An arbitrary estimator T of a real valued parameter θ, for example, can be
practically any function which depends on the observable random variables
alone. In some problem, we may think of X1, 
 S2 and so
on as competing estimators. At this point, the only restriction we have to
watch for is that T must be computable in order to qualify to be called an
estimator. In the following sections, two different methods are provided for
locating competing estimators of θθθθθ.
7.2.1 The Method of Moments
During the late nineteenth and early twentieth centuries, Karl Pearson was
the key figure in the major methodological developments in statistics. Dur-
ing his long career, Karl Pearson pioneered on many fronts. He originated
innovative ideas of curve fitting to observational data and did fundamental
research with correlations and causation in a series of multivariate data

7. Point Estimation
343
from anthropometry and astronomy. The method of moments was introduced
by Pearson (1902).
The methodology is very simple. Suppose that θθθθθ = (θ1, ..., θk). Derive the
first k theoretical moments of the distribution f(x; θθθθθ) and pretend that they are
equal to the corresponding sample moments, thereby obtaining k equations in
k unknown parameters θ1, ..., θk. Next, simultaneously solve these k equa-
tions for θ1, ..., θk. The solutions are then the estimators of ?1, ..., ?k. Refer
back to the Section 2.3 as needed. To be more specific, we proceed as fol-
lows: We write
Now, having observed the data X = x, the expressions given in the rhs of
(7.2.1) can all be evaluated, and hence we will have k separate equations in k
unknown parameters θ1, ..., θk. These equations are solved simultaneously.
The following examples would clarify the technique.
Often  is written for an estimator of the unknown parameter θθθθθ.
Example 7.2.1 (Example 6.2.8 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p) where p is unknown, 0 < p < 1. Here χ = {0, 1}, θ = p and Θ =
(0, 1). Observe that η1 = η1(θ) = Ep[X1] = p, and let us pretend that
  the sample mean. Hence, 
 would be the esti-
mator of p obtained by the method of moments. We write 
 which
happens to be sufficient for the parameter p too.!
Example 7.2.2 (Example 6.2.10 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ, σ2 are both unknown with n ≥ 2, θθθθθ = (µ, σ2), θθθθθ1 = µ, θ2 =
σ2, −∞ < µ < ∞, 0 < σ < ∞. Here χ = ℜ and Θ = ℜ × ℜ+. Observe that η1 =
η1(θ1, θ2) = E?[X1] = µ and 
 so that (7.2.1)
would lead to the two equations,
After solving these two equations simultaneously for µ and σ2, we obtain
the estimators 
 and 
. The

344
7. Point Estimation
estimator of σ2 coincides with (n - 1)n-1 S2. Note, however, that 
 is
sufficient for θθθθθ too. !
Example 7.2.3 Suppose that X1, ..., Xn are iid N(0, σ2) where θ = σ2 is
unknown, 0 < σ < ∞. Here χ = ℜ+ and Θ = ℜ+. Observe that η1 ≡ η1(θ) =
Eθ[X1] = 0 for all θθθθθ. In this situation, it is clear that the equation given
by the first moment in (7.2.1) does not lead to anything interesting and
one may arbitrarily move to use the second moment. Note that 
 so that (7.2.1) will now lead to 
After such ad hoc adjustment, the method of moment estimator turns out to
be sufficient for σ2. !
The method of moments is an ad hoc way to find estimators. Also,
 this method may not lead to estimators which are functions of
 minimal sufficient statistics for θ. Look at Examples 7.2.4-7.2.5.
If any of the theoretical moments η1, η2, ..., ηk is zero, then one will continue
to work with the first k non-zero theoretical moments so that (7.2.1) may
lead to sensible solutions. Of course, there is a lot of arbitrariness in this
approach.
Example 7.2.4 Suppose that X1, ..., Xn are iid Poisson(λ) where θ = λ is
unknown, 0 < λ < ∞. Here χ = {0, 1, 2, ...} and Θ = ℜ+. Now, η1 ≡ η1(θ) =
Eθ[X1] = λ and 
  Suppose that instead of
starting with η1 in (7.2.1), we start with η2 and equate this with 
This then provides the equation
which leads to the estimator 
  However, if
we had started with η1, we would have ended up with the estimator 
, a
minimal sufficient statistic for η. The first estimator is not sufficient for η.
From this example, one can feel the sense of arbitrariness built within this
methodology. !
Example 7.2.5 Suppose that X1, ..., Xn are iid Uniform (0, θ) where θ(> 0)
is the unknown parameter. Here η1 ≡ η1(θ) = E?[X1] = ½θ so that by equating
this with we obtain which is not sufficient for ?. Recall that Xn:n is a minimal
sufficient statistic for θ. !
7.2.2   The Method of Maximum Likelihood
The method of moments appeared quite simple but it was ad hoc and arbi-
trary in its approach. In the Example 7.2.3 we saw that we could not equate

7. Point Estimation
345
η1 and 
 to come up with an estimator of σ2. From the Examples 7.2.4-7.2.5,
it is clear that the method of moments may lead to estimators which will
depend upon non-sufficient statistics. Next, on top of this, if we face situa-
tions where theoretical moments are infinite, we can not hope to apply this
method. R. A. Fisher certainly realized the pitfalls of this methodology and
started criticizing Karl Pearson’s way of finding estimators early on. Fisher
(1912) was critical on Pearson’s approach of curve fitting and wrote on page
54 that “The method of moments ... though its arbitrary nature is apparent”
and went on to formulate the method of maximum likelihood in the same
paper. Fisher’s preliminary ideas took concrete shapes in a path-breaking ar-
ticle appearing in 1922 and followed by more elaborate discussions laid out in
Fisher (1925a, 1934).
Consider X1, ..., Xn which are iid with the common pmf or pdf f(x; θ)
where x ∈ χ ⊆ ℜ and θ = (θ1, ..., θk) ∈ Θ ⊆ ℜk. Here θ1, ..., θk are all assumed
unknown and thus θθθθθ is an unknown vector valued parameter. Recall the no-
tion of a likelihood function defined in (6.2.4). Having observed the data X =
x, we write down the likelihood function
Note that the observed data x = (x1, ..., xn) is arbitrary but otherwise held
fixed.
Throughout this chapter and the ones that follow, we essentially
 pay attention to the likelihood function when it is positive.
Definition 7.2.2 The maximum likelihood estimate of θθθθθ is the value
 for which 
  The maximum likelihood estimator
(MLE) of θ is denoted by 
  If we write 
  the context will dictate
whether it is referring to an estimate or an estimator of θθθθθ.
When the X’s are discrete, L(θθθθθ) stands for Pθ{X = x}, that is the probabil-
ity of observing the type of data on hand when ? is the true value of the
unknown parameter. The MLE is interpreted as the value of θθθθθ which maxi-
mizes the chance of observing the particular data we already have on hand.
Instead when the X’s are continuous, a similar interpretation is given by re-
placing the probability statement with an analogous statement using the joint
pdf of X.
As far as the definition of MLE goes, there is no hard and fast dictum
regarding any specific mathematical method to follow in order to locate
where L(θθθθθ) attains its supremum. If L(θθθθθ) is a twice differentiable function
of θθθθθ, then we may apply the standard techniques from differential calculus

346
7. Point Estimation
to find   See the Section 1.6 for some review. Sometimes we take the natural
logarithm of L(θθθθθ) first, and then maximize the logarithm instead to obtain 
There are situations where one finds a unique solution 
 or situations
where we find more than one solution 
 which globally maximize L(θθθθθ). In
some situations there may not be any solution  which will globally maximize
L(θθθθθ). But, from our discourse it will become clear that quite often a unique
MLE  of θθθθθ will exist and we would be able to find it explicitly. In the sequel,
we temporarily write c throughout to denote a generic constant which does
not depend upon the unknown parameter θθθθθ.
Example 7.2.6 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is un-
known but σ2 is known. Here we have −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ. The
likelihood function is given by
which is to be maximized with respect to µ. This is equivalent to maximizing
logL(µ) with respect to µ. Now, we have
and hence, 
 Next, equate 
logL(µ) to zero and solve for µ. But,
 logL(µ) = 0 implies that 
  and so we would say that 
  At this
step, our only concern should be to decide whether 
 really maximizes
logL(µ). Towards that end, observe that 
 which is
negative, and this shows that L(µ) is globally maximized at 
  Thus the
MLE for µ is 
, the sample mean.
Suppose that we had observed the following set of data from a normal
population: x1 = 11.4058, x2 = 9.7311, x3 = ∞.2280, x4 = 8.5678 and x5 =
8.6006 with n = 5 and σ = 1.
Figure 7.2.1. Likelihood Function L(µ) When the Mean µ
Varies from 2.5 - 19.7.

7. Point Estimation
347
In the Figure 7.2.1, the variable µ runs from 2.5 through 19.7. It becomes
practically clear from the Figure 7.2.1 that L(µ) attains its maximum at only
one point which is around 9.3. Using MAPLE, we found that the likelihood
function L(µ) was maximized at µ = 9.3067. For the observed data, the sample
mean happens to be 
 = 9.30666 In the end, we may add that the
observed data was generated from a normal population with µ = 10 and σ = 1
using MINITAB Release 12.1. !
Example 7.2.7 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ and σ2 are
both unknown, θ = (µ, σ2), −∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. Here we have χ =
ℜ and Θ = ℜ × ℜ+. We wish to find the MLE for θ. In this problem, the
likelihood function is given by
which is to be maximized with respect to both µ and σ2. This is equivalent to
maximizing logL(µ, σ2) with respect to both µ and σ2. Now, one has
which leads to
Then, we equate both these partial derivatives to zero and solve the re-
sulting equations simultaneously for µ and σ2. But, 
 logL(µ, σ2) = 0
and 
2 logL(µ, σ2) = 0 imply that 
 so that 
  as
well as 
 thereby leading to σ 2 =
 say. Next, the only concern is whether L(µ, σ2) given
by (7.2.4) is globally maximized at 
  We need to obtain the
matrix H of the second-order partial derivatives of logL(µ, σ2) and show that
H evaluated at 
 is negative definite (n.d.). See (4.8.12) for
some review. Now, we have
which evaluated at reduces to the matrix

348
7. Point Estimation
The matrix G would be n.d. if and only if its odd order principal minors are
negative and all even order principal minors are positive. Refer to (4.8.6) as
needed. In this case, the first diagonal is -nu-1 which is negative and det(G) =
½n2u-3 which is positive. In other words, G is a n.d. matrix. Thus, L(µ, s2) is
globally maximized at 
 That is, the MLE of µ and σ2 are re-
spectively 
 and 
 !
Next we include few examples to highlight the point that L(θ) may not be
a differentiable function of θ where L(θ) attains the global maximum. In such
situations, the process of finding the MLE turns out little different on a case
by case basis.
Example 7.2.88888 Suppose that we have a single observation X which is
distributed as Bernoulli(p) where 0 ≤ p ≤ 1 is the unknown parameter. Here,
we have
Whether we observe x = 0 or 1, the resulting likelihood function L(p) is not
differentiable at the end points. But, by simply drawing a picture of L(p) one
can verify that (i) when x = 0 then L(p) is maximized if p is the smallest, that
is if p = 0, and (ii) when x = 1 then L(p) is maximized if p is the largest, that
is if p = 1. Hence the MLE of p is 
 when Θ = [0, 1].
But, if the parameter space happens to be Θ = [1/3, 2/3] instead, then
what will be the MLE of p? Again, L(p) is maximized at the end points
where L(p) is not differentiable. By examining the simple picture of L(p)
in the Figure 7.2.2, it becomes clear in this situation that (i) when x = 0,
L(p) is maximized if p is the smallest, that is if p = 1/3, and (ii) when x =
1, L(p) is maximized if p is the largest, that is if p = 2/3. Hence the MLE of
p is 
 if the parameter space happens to be Θ = [1/3, 2/3]. !
Example 7.2.9 Suppose that X1, ..., Xn are iid Uniform(0, θ) where 0
< θ < ∞ is the unknown parameter. Here χ = (0, θ) and Θ = ℜ+. We

7. Point Estimation
349
wish to find the MLE for θ. The likelihood function is
which is maximized at an end point. By drawing a simple picture of L(θ) it
should be apparent that L(θ) is maximized when θ = xn:n. That is the MLE of
θ is 
  the largest order statistic. !
Example 7.2.10 Suppose that X1, ..., Xn are iid Poisson(λ) where 0 < λ <
∞ is the unknown parameter. Here χ = {0, 1, 2, ...} and Θ = ℜ+. We wish to
find the MLE for θ. The likelihood function is
so that one has
Now L(λ) is to be maximized with respect to λ and that is equivalent to
maximizing logL(λ) with respect to λ. We have 
 logL(µ) = −n + 
which when equated to zero provides the solution 
  If 
  > 0, then d/dµ
logL(µ) = 0 when . In this situation it is easy to verify that 
 is
negative. Hence the MLE is 
 whenever 
 But there is a fine point
here. If 
  which is equivalent to saying that x1 = ... = xn = 0, the
likelihood function in (7.2.5) does not have a global maximum. In this case
L(λ) = e-nλ which becomes larger as λ(> 0) is allowed to become smaller. In
other words if 
  an MLE for λ can not be found. Observe, however,
that 
 for all i = 1, ..., n} = exp( -nλ) which will be
negligible for “large” values of nλ.
Table 7.2.1. Values of 
nλ:
 3
 4
5
 6
7
 :
.0498  .01∞3
.0067
 .0025
.0009
By looking at these entries, one may form some subjective opinion about
what values of nλ should perhaps be considered “large” in a situation like
this.!
In the Binomial(n,p) situation where 0 < p < 1 is the unknown parameter,
the problem of deriving the MLE of p would hit a snag similar to what we
found in the Example 7.2.10 when 
is 0 or 1. Otherwise the MLE of p
would be 
 If the parameter space is replaced by 0 ≤ p ≤ 1, then of
course the MLE of p would be 
  We leave this as the Exercise 7.2.8.

350
7. Point Estimation
Remark 7.2.1 In the Examples 7.2.6-7.2.10, the MLE of the unknown
parameter θ turned out to be either a (minimal) sufficient statistic or its func-
tion. This observation should not surprise anyone. With the help of Neyman
factorization one can rewrite L(θ) as g(T; θ)h(x) where T is a (minimal) suf-
ficient statistic for θ. Now, any maximizing technique for the likelihood func-
tion with respect to θ would reduce to the problem of maximizing g(T; θ) with
respect to θ. Thus the MLE of θ in general will be a function of the associated
(minimal) sufficient statistic T. The method of moment estimator of θ may
lack having this attractive feature. Refer to the Examples 7.2.4-7.2.5.
The MLE  has a very attractive property referred to as its
 invariance property: That is, if is the MLE of θ,
then 
 is the MLE of g(θ).
The MLE can be quite versatile. It has a remarkable feature which is known
as its invariance property. This result will be useful to derive the MLE of
parametric functions of θ without a whole lot of effort as long as one already
has the MLE  . We state this interesting result as a theorem without giving its
proof. It was proved by Zehna (1966).
Theorem 7.2.1 (Invariance Property of MLE) Consider the likelihood
function L(θ) defined in (7.2.2). Suppose that the MLE of θ (∈ Θ ⊆ ℜk) exists
and it is denoted by  . Let g(.) be a function, not necessarily one-to-one,
from ℜk to a subset of ℜm. Then, the MLE of the parametric function g(θ) is
given by 
 .
Example 7.2.11 (Example 7.2.7 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) with θ = (µ σ2) where µ, σ2 are both unknown, −∞ < µ < ∞, 0 < σ <
∞, n ≥ 2. Here χ = ℜ and Θ = ℜ × ℜ+. We recall that the MLE of µ and σ2 are
respectively 
 and 
 Then, in view of
the MLE’s invariance property, the MLE of (i) σ would be 
 (ii) µ + σ
would be 
 (iii) µ2/σ2 would be 
 !
Note that the function g(.) in the Theorem 7.2.1 is not necessarily
one-to-one for the invariance property of the MLE to hold.
Example 7.2.12 (Example 7.2.9 Continued) Suppose that X1, ..., Xn are iid
Uniform(0, θ) where 0 < θ < ∞ is the unknown parameter. Here χ = (0, θ) and
Θ = ℜ+. We recall that the MLE for θ is Xn:n. In view of the MLE’s invariance
property, one can verify that the MLE of (i) θ2 would be 
 (ii) 1/θ would
be 1/Xn:n, (iii) 
 would be 
 !

7. Point Estimation
351
7.3 Criteria to Compare Estimators
Let us assume that a population’s pmf or pdf depends on some unknown
vector valued parameter θθθθθ ∈ Θ (⊆ ℜk). In many instances, after observing the
random variables X1, ..., Xn, we will often come up with several competing
estimators for T(θθθθθ), a real valued parametric function of interest. How should
we then proceed to compare the performances of rival estimators and then
ultimately decide which one is perhaps the “best”? The first idea is introduced
in Section 7.3.1 and the second one is formalized in Sections 7.3.2-7.3.3.
7.3.1 Unbiasedness, Variance, and Mean Squared Error
In order to set the stage, right away we start with two simple definitions.
These are followed by some examples as usual. Recall that T(θθθθθ) is a real
valued parametric function of θθθθθ.
Definition 7.3.1 A real valued statistic T ≡ T(X1, ..., Xn) is called an
unbiased estimator of T(θθθθθ) if and only if Eθ(T) = T(θθθθθ) for all θθθθθ ∈ Θ. A statistic
T ≡ T(X1, ..., Xn) is called a biased estimator of T(θθθθθ) if and only if T is not
unbiased for T(θθθθθ).
Definition 7.3.2 For a real valued estimator T of T(θθθθθ), the amount of bias
or simply the bias is given by
Gauss (1821) introduced originally the concept of an unbiased estimator in
the context of his theory of least squares. Intuitively speaking, an unbiased
estimator of T(θθθθθ) hits its target T(θθθθθ) on the average and the corresponding bias
is then exactly zero for all θθθθθ ∈ Θ. In statistical analysis, the unbiasedness
property of an estimator is considered very attractive. The class of unbiased
estimators can be fairly rich. Thus, when comparing rival estimators, initially
we restrict ourselves to consider the unbiased ones only. Then we choose the
estimator from this bunch which appears to be the “best” according to an
appropriate criteria.
In order to clarify the ideas, let us consider a specific population or
universe which is described as N(µ, σ2) where µ ∈ ℜ is unknown but s ∈
ℜ+ is assumed known. Here χ = ℜ and Θ = ℜ. The problem is one of
estimating the population mean µ. From this universe, we observe the iid
random variables X1, ..., X4. Let us consider several rival estimators of µ

352
7. Point Estimation
defined as follows:
Based on X1, ..., X4, one can certainly form many other rival estimators for µ.
Observe that Eµ(T1) = 2µ, Eµ(T2) = µ, Eµ(T3) = µ, Eµ(T4) = 2/3µ, Eµ(T5) = µ
and Eµ(T6) = µ. Thus, T1 and T4 are both biased estimators of µ, but T2, T3, T5
and T6 are unbiased estimators of µ. If we wish to estimate µ unbiasedly, then
among T1 through T6, we should only include T2, T3, T5, T6 for further consid-
erations.
Definition 7.3.3 Suppose that the real valued statistic T ≡  T(X1, ..., Xn) is
an estimator of T(θ). Then, the mean squared error (MSE) of the estimator T,
sometimes denoted by MSET, is given by Eθ[(T - T(θ))2]. If T is an unbiased
estimator of T(θ), then the MSE is the variance of T, denoted by Vθ(T).
Note that we have independence between the X’s. Thus, utilizing the Theo-
rem 3.4.3 in the case of the example we worked with little earlier, we obtain
In other words, these are the mean squared errors associated with the unbi-
ased estimators T2, T3, T5 and T6.
How would one evaluate the MSE of the biased estimators T1 and T4? The
following result will help in this regard.
Theorem 7.3.1 If a statistic T is used to estimate a real valued parametric
function T(θ), then MSET, the MSE associated with T, is given by
which amounts to saying that the MSE is same as the variance plus the square
of the bias.

7. Point Estimation
353
Proof Let us write Eθ(T) = ξ(θ). Then, we have
Now, since ξ(θ) - T(θ) is a fixed real number, we can write Eθ[{T - ξ(θ)}{ξ(θ)
- T(θ)}] = {ξ(θ) - T(θ)} Eθ[{T - θ(θ)}] = 0. Hence the result follows. !
Now, we can evaluate the MSET1 as Vµ(T1) + [Eµ(T1) - µ]2 = 2σ2 + (2µ -
µ)2 = µ2 + 2σ2. The evaluation of MSET4 is left as the Exercise 7.3.6.
It is possible sometimes to have T1 and T2 which are respectively
biased and unbiased estimators of T(θ), but MSET1 < MSET4. In
other words, intuitively a biased estimator may be preferable if its
average squared error is smaller. Look at the Example 7.3.1.
Example 7.3.1 Let X1, ..., Xn be iid N(µ, σ2) where µ, σ2 are both un-
known, θ = (µ, σ2), −∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. Here χ = ℜ and Θ = ℜ ×
ℜ+. Our goal is the estimation of a parametric function T(θ) = σ2, the popula-
tion variance S2 = 
  Consider the customary sample
variance We know that S2 is unbiased for σ2. One will also recall that (n - 1)S2/
σ2 is distributed 
 as and hence Vθ(S2) = 2σ4(n - 1)-1. Next, consider an-
other estimator for σ2, namely T = (n + 
 which can be
rewritten as (n - 1)(n + 1)-1 S2. Thus, Eθ(T) = (n - 1)(n + 1)-1 σ2 ≠ σ2 and so
T is a biased estimator of σ2. Next, we evaluate
Then we apply the Theorem 7.3.1 to express MSET as
which is smaller than Vθ(S2). That is, S2 is unbiased for σ2 and T is biased for
σ2 but MSET is smaller than MSEs2 for all θ. Refer to the Exercise 7.3.2 to see
how one comes up with an estimator such as T for σ2. !
In the context of the example we had been discussing earlier in this
section, suppose that we consider two other estimators T7 = ½X1 and

354
7. Point Estimation
T8 = 0 for µ. Obviously both are biased estimators of µ. In view of the Theo-
rem 7.3.1, the MSET7 would be ¼(σ2 + µ2) whereas the MSET8 would simply
be µ2. Between these two estimators T7 and T∞, as far as the smaller MSE
criterion goes, T7 will be deemed better if and only if σ2 < 3µ2.
Figure 7.3.1. Curves Corresponding to MSET7 and MSET8
When σ = 1
In the Figure 7.3.1, we have plotted both MSET7 (thick curve) and MSET8
(thin curve) assuming that σ = 1. In this case, we claim that T7 is better than
T8 (that is, MSET7 < MSET8) if and only if 
 This conclusion
is validated by the Figure 7.3.1. This means that between the two estimators
T7 and T8, we should prefer T7 if and only if 
 But, we do not know
µ to begin with! In other words, it will be impossible to choose between T7
and T8 in practice. These two estimators are not comparable. The reader
should try and find other estimators in this case which are not comparable
among themselves.
Sometimes estimators’ MSE’s may not be comparable to each
other. Look at the Figure 7.3.1.
In the next section, we consider unbiased estimators of T(θθθθθ) and compare
the performance among those unbiased estimators only.
7.3.2 Best Unbiased and Linear Unbiased Estimators
Let us discuss how to define the phrase “best” unbiased estimator or the “best”
linear unbiased estimator of a real valued parametric function T(θθθθθ) as long as
there is at least one unbiased estimator of T(θθθθθ). In the previous section, ex-
amples were given where we could find competing unbiased estimators of T(θθθθθ).

7. Point Estimation
355
There are situations where a parametric function of interest
has no unbiased estimator. In such situations, naturally we do not
proceed to seek out the best unbiased estimator.
Look at the Example 7.3.2.
Example 7.3.2 Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 < p <
1 is unknown and θ = p. We now wish to estimate the parametric function
T(θ) = 1/p. Of course 
 is sufficient for p and we also know that T
is distributed as Binomial(n,p) for all fixed 0 < p < 1. In this case, there is no
unbiased estimator of T(θ). In order to prove this result, all we need to show
is that no function of T can be unbiased for T(θ). So let us assume that h(T) is
an unbiased estimator of T(θ) and write, for all 0 < p < 1,
which can be rewritten as
But the lhs in (7.3.3) is a polynomial of degree n+1 in the variable p and it
must be zero for all p ∈ (0, 1). That is the lhs in (7.3.3) must be identically
equal to zero and hence we must have −1 ≡ 0 which is a contradiction. That
is, there is no unbiased estimator of 1/p based on T. In this situation, there is
no point in trying to find the “best” unbiased estimator of 1/p. Look at a
closely related Exercise 7.3.7. !
In the Example 7.3.2, we could not find any unbiased estimator
of p-1. But, one can appropriately modify the sampling
scheme itself and then find an unbiased estimator of p-1.
Look at the Example 7.3.3.
Example 7.3.3 (Example 7.3.2 Continued) In many areas, for example,
ecology, entomology, genetics, forestry and wild life management, the
problem of estimating p-1 is very important. In the previous example, the
sample size n was held fixed and out of the n independent runs of the
Bernoulli experiment, we counted the number (T) of observed successes.
If the data was collected in that fashion, then we had shown the nonexist-
ence of any unbiased estimator of p-1. But we do not suggest that p-1 can
not be estimated unbiasedly whatever be the process of data collection.

356
7. Point Estimation
One may run the experiment a little differently as follows. Let N = the number
of runs needed to observe the first success so that the pmf of the random
variable N is given by P(N = y) = p(1 - p)y-1, y = 1, 2, 3, ... which means that
N is a Geometric(p) random variable. One can verify that Ep[N] = p-1 for all p
∈ (0, 1). That is the sample size N is an unbiased estimator of p-1. This method
of data collection, known as the inverse binomial sampling, is widely used in
applications mentioned before. In his landmark paper, Petersen (1896) gave
the foundation of capture-recapture sampling. The 1956 paper of the famous
geneticist, J. B. S. Haldane, is cited frequently. Look at the closely related
Exercise 7.3.8. !
How should we go about comparing performances of
any two unbiased estimators of T(θ)?
This is done by comparing the variances of the rival estimators. Since the
rival estimators are assumed unbiased for T(θ), it is clear that a smaller vari-
ance will indicate a smaller average (squared) error. So, if T1, T2 are two
unbiased estimators of T(θθθθθ), then T1 is preferable to (or better than) T2 if
Vθ(T1) ≤ Vθ(T2) for all θθθθθ ∈ Θ but Vθ(T1) < Vθ(T2) for some θθθθθ ∈ Θ. Now, in the
class of unbiased estimators of T(θθθθθ), the one having the smallest variance is
called the best unbiased estimator of T(θθθθθ). A formal definition is given shortly.
Using such a general principle, by looking at (7.3.2), it becomes apparent
that among the unbiased estimators T2, T3, T5 and T6 for the unknown mean µ,
the estimator T3 is the best one to use because it has the smallest variance.
Definition 7.3.4 Assume that there is at least one unbiased estimator of
the unknown real valued parametric function T(θθθθθ). Consider the class C of all
unbiased estimators of T(θθθθθ). An estimator T ∈ C is called the best unbiased
estimator or the uniformly minimum variance unbiased estimator (UMVUE)
of T(θθθθθ) if and only if for all estimators T* ∈ C, we have
In Section 7.4 we will introduce several approaches to locate the UMVUE.
But first let us focus on a smaller subset of C for simplicity. Suppose that we
have located estimators T1, ..., Tk which are all unbiased for T(θθθθθ) such that
V?(Ti) = δ2 and the Ti’s are pairwise uncorrelated, 0 < δ < ∞, i = 1, ..., k.
Denote a new subclass of estimators

7. Point Estimation
357
For any T ∈ D we have: 
for all θθθθθ which shows that any estimator T chosen from D is an unbiased
estimator of T(θθθθθ). Hence, it is obvious that D ⊆ C. Now we wish to address
the following question.
What is the best estimator of T(θθθθθ) within the smaller class D?
That is, which estimator from class D has the smallest variance? From the
following theorem one will see that the answer is indeed very simple.
Theorem 7.3.2 Within the class of estimators D, the one which has the
smallest variance corresponds to αi = k-1, i = 1, ..., k. That is, the best unbi-
ased estimator of T(θθθθθ) within the class D turns out to be 
 
 which
is referred to as the best linear (in T1, ..., Tk) unbiased estimator (BLUE) of
T(θθθθθ).
Proof Since the Ti’s are pairwise uncorrelated, for any typical estimator T
from the class D, we have
From (7.3.6) it is now clear that we need to
minimize 
 subjected to the restriction that 
But, observe that
Hence, 
 for all choices of αi, i = 1, ..., k such that 
But, from (7.3.7) we see that 
 the smallest possible value, if
and only if 
 That is, Vθ(T) would be minimized if and
only if αi = k-1, i = 1, ..., k. !
Example 7.3.4 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ, σ are both
unknown, −∞ < µ < ∞, 0 < σ < ∞. Among all linear (in X1, ..., Xn) unbiased
estimators of µ, the BLUE turns out to be 
 , the sample mean. This follows
immediately from the Theorem 7.3.2.!

358
7. Point Estimation
7.4 Improved Unbiased Estimators via Sufficiency
It appears that one of the first examples of UMVUE was found by Aitken and
Silverstone (1942). The UMVUE’s as suitable and unique functions of suffi-
cient statistics were investigated by Halmos (1946), Kolmogorov (1950a),
and more generally by Rao (1947).
In a problem suppose that we can get hold of at least one unbiased estima-
tor for a real valued parametric function T(θθθθθ). Now the question is this: If we
start with some unbiased estimator T for T(θθθθθ), however trivial this estimator
may appear to be, can we improve upon T? That is, can we revise the initial
unbiased estimator in order to come up with another unbiased estimator T’ of
T(θθθθθ) such that Vθ(T’) < Vθ(T) for all θθθθθ ∈ Θ?
Let us illustrate. Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 < p <
1 is unknown. We wish to estimate the parameter p unbiasedly. Consider T =
X1 and obviously T is an unbiased estimator of p. But note that T can take one
of two possible values 0 or 1. Such an estimator, even though unbiased, may
appear naive and useless. T may be unreasonable as an estimator of the pa-
rameter p which lies between zero and one. But, this criticism against T should
not be too bothersome because T is not the estimator which would be recom-
mended for use in practice. The question is, even if we start with T, can we
hope to improve upon this initial estimator in the sense of reducing the vari-
ance? For all practical purposes, the answer is in the affirmative. The general
machinery comes next.
7.4.1 The Rao-Blackwell Theorem
The technique to improve upon an initial unbiased estimator of T(θθθθθ) is custom-
arily referred to as the Rao-Blackwellization in the statistical literature. C. R.
Rao and D. Blackwell independently published fundamental papers respec-
tively in 1945 and 1947 which included this path-breaking idea. Neither Rao
nor Blackwell knew about the other’s paper for quite some time because of
disruptions due to the war. We first state and prove this fundamental result.
Theorem 7.4.1 (Rao-Blackwell Theorem) Let T be an unbiased estima-
tor of a real valued parametric function T(θθθθθ) where the unknown parameter θθθθθ
∈ Θ ⊂ ℜk. Suppose that U is a jointly sufficient statistic for θθθθθ. Define g(u) =
Eθ[T &pipe; U = u], for u belonging to U, the domain space of U. Then, the
following results hold:
(i)
Define W = g(U). Then, W is an unbiased estimator of T(θθθθθ);
(ii)
Vθ[W] ≤ Vθ[T] for all θθθθθ ∈ Θ, with the equality holding if and only
if T is the same as W w. p. 1.

7. Point Estimation
359
Proof (i) Since U is sufficient for θθθθθ, the conditional distribution of T given
U = u can not depend upon the unknown parameter θθθθθ, and this remains true
for all u ∈ u. This clearly follows from the Definition 6.2.3 of sufficiency.
Hence, g(u) is a function of u and it is free from θθθθθ for all u ∈ U. In other
words, W = g(U) is indeed a real valued statistic and so we can call it an
estimator. Using the Theorem 3.3.1, part (i), we can write E[X] =
EY[E(X | Y)] where X and Y are any two random variables with finite expec-
tations. Hence we have for all θθθθθ ∈ Θ,
which shows that W is an unbiased estimator of T(θθθθθ). !
(ii) Let us now proceed as follows for all θθθθθ ∈ Θ:
since we have
Now, from (7.4.2), the first conclusion in part (ii) is obvious since {T - W}2
is non-negative w.p.1 and thus Eθ[{T - W}2] ≥ 0 for all θθθθθ ∈ Θ. For the second
conclusion in part (ii), notice again from (7.4.2) that Vθ[W] = Vθ[T] for all θθθθθ ∈
Θ if and only if Eθ[{T - W}2] = 0 for all θθθθθ ∈ Θ, that is if and only if T is the
same as W w.p.1. The proof is complete.!
One attractive feature of the Rao-Blackwell Theorem is that there is no
need to guess the functional form of the final unbiased estimator of T(θθθθθ).
Sometimes guessing the form of the final unbiased estimator of T(θθθθθ) may be
hard to do particularly when estimating some unusual parametric function.
One will see such illustrations in the Examples 7.4.5 and 7.4.7.
Example 7.4.1 Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 <
p < 1 is unknown. We wish to estimate T(p) = p unbiasedly. Consider T =
X1 which is an unbiased estimator of p. We were discussing this example

360
7. Point Estimation
just before we introduced the Rao-Blackwell Theorem. The possible values of
T are 0 or 1. Of course 
 is sufficient for p. The domain space for
U is u = {0, 1, 2, ..., n}. Let us write for u ∈ u,
Next, observe that 
 is Binomial(n,p) and 
 is Binomial(n - 1, p).
Also X1 and 
 are independently distributed. Thus, we can immediately
rewrite (7.4.3) as
That is, the Rao-Blackwellized version of the initial unbiased estimator X1
turns out to be 
 , the sample mean, even though T was indeed a very naive
and practically useless initial estimator of p. Now note that Vp[T] = p(1-p) and
Vp[W] = p(1 - p)/n so that Vp[W] < Vp[T] if n ≥ 2. When n = 1, the sufficient
statistic is X1 and so if one starts with T = X1, then the final estimator obtained
through Rao-Blackwellization would remain X1. That is, when n = 1,
we will not see any improvement over T through the Rao-Blackwellization
technique.!
Start with an unbiased estimator T of a parametric function T(θθθθθ).
The process of conditioning T given a sufficient (for θθθθθ) statistic U
is referred to as Rao-Blackwellization. The refined estimator W
is often called the Rao-Blackwellized version of T. This technique
is remarkable because one always comes up with an improved
unbiased estimator W for T(θθθθθ) except in situations where the initial
estimator T itself is already a function of the sufficient statistic U.
Example 7.4.2 (Example 7.4.1 Continued) Suppose that X1, ..., Xn are
iid Bernoulli(p) where 0 < p < 1 is unknown, with n ≥ 2. Again, we wish
to estimate T(p) = p unbiasedly. Now consider a different initial estima-
tor T = ½(X1 + X2) and obviously T is an unbiased estimator of p. The
possible values of T are 0, ½ or 1 and again 
 is a sufficient

7. Point Estimation
361
statistic for p. Then, Ep[T |  U = u] = ½{Ep[X1 |  U = u] + Ep[X2 |
 In other words, if one starts with ½(X1 + X2)
as the initial unbiased estimator of p, then after going through the process of
Rao-Blackwellization, one again ends up with 
 as the refined unbiased esti-
mator of p. Observe that Vp[T] = p(1 - p)/2 and 
 
 so that
 if n = 3. When n = 2, the sufficient statistic is T, and so if one
happens to start with T, then the final estimator obtained through the process
of Rao-Blackwellization would remain T. In other words, when n = 2, we will
not see any improvement over T through the Rao-Blackwellization technique.!
Example 7.4.3 (Example 7.4.1 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p) where 0 < p < 1 is unknown, with n ≥ 2. We wish to estimate T(p)
= p(1 - p) unbiasedly. Consider T = X1(1 - X2) which is an unbiased estimator
of T(p). The possible values of T are 0 or 1. Again, 
 is the suffi-
cient statistic for p with the domain space u = {0, 1, 2, ..., n}. Let us denote
 and then write for u ∈ u:
since X1(1 - X2) takes the value 0 or 1 only. Thus, we express Ep[X1(1 - X2) |
U = u] as
Next observe that 
 is Binomial(n, p), 
 is Binomial(n - 2, p), and
also that X1, 
 are independently distributed. Thus, we can re-
write (7.4.4) as
which is the same as 
 That is, the Rao-Blackwellized ver-
sion of the initial unbiased estimator X1(1 - X2) turns out to be n(n-
 For the Bernoulli random samples, since the X’s are either
0 or 1, observe that the sample variance in this situation turns out to be
 which coincides with the Rao-
Blackwellized version. !

362
7. Point Estimation
Example 7.4.4 Suppose that X1, ..., Xn are iid Poisson(λ) where 0 < l < ∞
is unknown. We wish to estimate T(λ) = λ unbiasedly. Consider T = X1 which
is an unbiased estimator of λ 
 and is a sufficient statistic for ?.
The domain space for U is u = {0, 1, 2, ..}. Now, for u ∈ U, conditionally
given U = u, the statistic T can take one of the possible values from the set T
= {0, 1, 2, ..., u}. Thus, for u ∈ u, we can write
Now, we have to find the expression for Pλ{T = t |  U = u} for all fixed u ∈ u
and t ∈ . Let us write
But observe that 
 is Poisson(nλ)
 is Poisson((n - 1)λ), and also
that X1,
 are independently distributed. Thus, from (7.4.6) we can
express Pλ{T = t |  U = u} as
That is, the conditional distribution of T given U = u is Binomial(u, 1/n).
Hence, combining (7.4.5) and (7.4.7), we note that 
 In other words, the Rao-Blackwellized version of T is 
, the sample mean.!
We started with trivial unbiased estimators in the Examples 7.4.1
-7.4.4. In these examples, perhaps one could intuitively guess the
 the improved unbiased estimator. The Rao-Blackwell Theorem did
not lead to any surprises here. The Examples 7.4.5 and 7.4.7 are
however, different because we are forced to begin with naive
unbiased estimators. Look at the Exercises 7.4.6 and
7.4.8 for more of the same.
Example 7.4.5 (Example 7.4.4 Continued) Suppose that X1, ..., Xn
are iid Poisson(λ) where 0 < λ < ∞ is unknown and n ≥ 2. We wish to
estimate T(λ) = e-λ unbiasedly. Consider T = I(X1 = 0) which is an
unbiased estimator of T(λ) since Eλ[T] = Pλ{X1 = 0} = e-λ. Consider

7. Point Estimation
363
 the sufficient statistic for λ. The domain space for U is U =
{0, 1, 2, ..}. Now, for all u ∈ u, conditionally given U = u, the statistic T can
take one of the possible values 0 or 1. Thus, for u ∈ u, we can write
Note that 
 is Poisson(nλ), 
 is Poisson((n − 1)λ) whereas X1
and 
 are independently distributed. Thus, from (7.4.8) we rewrite Eλ[T
|  U = u] as
and hence the Rao-Blackwellized version of the estimator T is W = (1 -
 Now we know the expression for W and so we can directly
evaluate Eλ[W]. We use the form of the mgf of a Poisson random variable,
namely Eλ [esU] = exp{nλ(es - 1)}, and then replace s with log(1 - n-1) to write
In other words, W is an unbiased estimator of e-λ, but this should not be
surprising. The part (i) of the Rao-Blackwell Theorem leads to the same con-
clusion. Was there any way to guess the form of the estimator W before
actually going through Rao-Blackwellization? How should one estimate e-2λ or
e-3λ? One should attack these problems via Rao-Blackwellization or mgf as we
just did. We leave these and other related problems as Exercise 7.4.3. !
Example 7.4.6 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is un-
known but σ2 is known with −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ. We wish to
estimate T(µ) = µ unbiasedly. Consider T = X1 which is an unbiased estimator
of µ. Consider 
 the sufficient statistic for µ. The domain
space for U is u = ℜ. Now, for u ∈ u, conditionally given U = u, the distribu-
tion of the statistic T is N (u, σ2(1 - n-1)). Refer to the Section 3.6 on the
bivariate normal distribution as needed. Now, 
 That
is, the Rao-Blackwellized version of the initial unbiased estimator T turns out
to be 
.!

364
7. Point Estimation
Example 7.4.7 (Example 7.4.6 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ is unknown but σ2 is known with −∞ < µ < ∞, 0 < σ < ∞
and χ = ℜ. We wish to estimate the parametric function T(µ) = Pµ{X1 > a}
unbiasedly where a is a fixed and known real number. Consider T = I(X1 > a)
and obviously T is an unbiased estimator of T(µ). Consider 
the sufficient statistic for µ. The domain space for U is u = ℜ. For u ∈ u,
conditionally given U = u, the distribution of the statistic T is N (u, σ2(1 - n-1)).
In other words, we have
That is, the Rao-Blackwellized version of the initial unbiased estimator T is
 Was there a way to guess the form
of the improved estimator W before we used Rao-Blackwellization? The Exer-
cise 7.4.4 poses a similar problem for estimating the two-sided probability. A
much harder problem is to estimate the same parametric function T(µ) when
both µ and σ are unknown. Kolmogorov (1950a) gave a very elegant solution
for this latter problem. The Exercise 7.4.6 shows the important steps. !
Example 7.4.88888 (Example 7.4.6 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ is unknown but σ2 is known with −∞ < µ < ∞, 0 < σ < ∞
and χ = ℜ. We wish to estimate T(µ) = µ2 unbiasedly. Consider 
which is an unbiased estimator of T(µ). Consider again 
 a
sufficient statistic for µ. The domain space for U is u = ℜ. As before, for u
∈ u, conditionally given U = u, the distribution of the statistic T is N (u, σ2(1
- n-1)). In other words, we can express Eµ[T |  U = u] as
That is, the Rao-Blackwellized version of the initial unbiased estimator T of µ2
is 
 What initial estimator T would one start with if we wished
to estimate T(µ) = µ3 unbiasedly? Try to answer this question first before
looking at the Exercise 7.4.9. !
Remark 7.4.1 In the Example 7.4.8, we found 
 as
the final unbiased estimator of µ2. Even though W is unbiased for µ2, one
may feel somewhat uneasy to use this estimator in practice. It is true that
Pµ{W < 0} is positive whatever be n, µ and σ. The parametric function
µ2 is non-negative, but the final unbiased estimator W can be negative
with positive probability! From the Figure 7.4.1 one can see the behavior

7. Point Estimation
365
of Pµ{W < 0} as a function of n when µ = .1 but σ = 1, 2, 3. The unbiasedness
criterion at times may create an awkward situation like this. In this particular
case, however, the MLE of µ2 is 
 by virtue of the invariance property
(Theorem 7.2.1). The MLE is a biased estimator but it is always non-nega-
tive.
Figure 7.4.1. Plot of n Versus Pµ{W < 0} When µ = .1
In the preceding Examples 7.4.1-7.4.∞, we got started with a naive unbi-
ased estimator for the parametric functions of interest. But once the initial
unbiased estimator was refined through Rao-Blackwellization, the improved
unbiased estimator appeared quickly. Now, the question is whether such a
refined estimator is in fact the UMVUE of the associated parametric function.
We have indeed found the unique UMVUE in the given examples, but we can
not validate this claim by relying upon the Rao-Blackwell Theorem alone. We
give extra machineries next to pinpoint the UMVUE. In Section 7.6.1, we
briefly discuss a scenario where the refinement via Rao-Blackwellization does
not lead to the UMVUE.
7.5
 Uniformly Minimum Variance Unbiased
 Estimator
This section provides specific tools to derive the uniformly minimum vari-
ance unbiased estimator (UMVUE) of parametric functions when there is a
UMVUE. The first approach relies upon the celebrated Cramér-Rao inequal-
ity. C. R. Rao and H. Cramér independently discovered, under mild regu-
larity conditions, a lower bound for the variance of an unbiased estimator
of a real valued parametric function T(θ) where θ ∈ Θ(⊆ ℜ) in their clas-
sic papers, Rao (1945) and Cramér (1946b). Neither of them was aware

366
7. Point Estimation
about the other’s results for quite some time due to the war. We will first
prove this famous inequality.
Next, we discuss a fundamental result (Theorem 7.5.2) due to Lehmann
and Scheffé (1950) which leaps out of the Rao-Blackwell Theorem and it
helps us go very far in our search for the UMVUE. The Lehmann-Scheffé
(1950,1955,1956) series of papers is invaluable in this regard.
7.5.1
The Cramér-Rao Inequality and UMVUE
Lehmann (1983) referred to this inequality as the “information inequality,” a
name which was suggested by Savage (1954). Lehmann (1983, p. 145, Sec-
tion 9) wrote, “The first version of the information inequality appears to have
been given by Fréchet (1943). Early extensions and rediscoveries are due to
Darmois (1945), Rao (1945), and Cramér (1946b).” We will, however, con-
tinue to refer to this inequality by its commonly used name, the Cramér-Rao
inequality, for the ease of (i) locating cross-references and (ii) going for a
literature search among the available books and other sources.
This bound for the variance, customarily called the Cramér-Rao lower
bound (CRLB), for unbiased estimators of T(θ) is appreciated in many prob-
lems where one can (i) derive the expression of the CRLB, and (ii) easily
locate an unbiased estimator of T(θ) whose variance happens to coincide with
the CRLB. In situations like these, one has then precisely found the UMVUE
for T(θ).
Consider iid real valued and observable random variables X1, ..., Xn from a
population with the common pmf or pdf f(x; θ) where the unknown param-
eter θ ∈ Θ ⊆ ℜ and x ∈ χ ⊆ ℜ. Recall that we denote X = (X1, ..., Xn). Let us
pretend that we are working with the pdf and hence the expectations of func-
tions of random variables would be written as appropriate multiple integrals.
In the case of discrete random variables, one would replace the integrals by
the appropriate finite or infinite sums, as the case may be.
Standing Assumptions: Let us assume that the support χ does not in-
volve θ and the first partial derivative of f(x; θ) with respect to θ and the
integrals with respect to X = (x1, ..., xn) are interchangeable.
Theorem 7.5.1 (Cramér-Rao Inequality) Suppose that T = T(X) is an
unbiased estimator of a real valued parametric function T(θ), that is Eθ(T) =
T(θ) for all θ ∈ Θ. Assume also that 
T(θ), denoted by T’(?), exists and it is
finite for all θ ∈ Θ. Then, for all θ ∈ Θ, under the standing assumptions we
have:

7. Point Estimation
367
The expression on the rhs of the inequality in (7.5.1) is called the Cramér-Rao
lower bound (CRLB).
Proof Without any loss of generality, let us assume that 0 < Vθ(T) < ∞. By
the definition of expectation, we can write
which implies that
Now, observe that
so that by using the chain rule of differentiation, we get
Next, let us denote Y = 
.] Note that Y is not an observable
random variable because it involves the unknown parameter θ. We now com-
bine (7.5.4) and (7.5.5) to rewrite
Also, one obviously has ∫χ f(x; θ)dx = 1 so that one writes
Hence, we have
for all θ ∈ Θ, since the X’s have identical distributions. Thus, (7.5.6) leads us
to conclude that

368
7. Point Estimation
which can be rewritten as
by virtue of the Cauchy-Schwarz inequality or the covariance inequality (Theo-
rems 3.9.5-3.9.6). Recall that Y is the sum of n iid random variables and thus
in view of (7.5.7), we obtain
Now the inequality (7.5.1) follows by combining (7.5.8) and (7.5.9). "
Remark 7.5.1 It is easy to see that the CRLB given by the rhs of the
inequality in (7.5.1) would be attained by the variance of the estimator T for
all θ ∈ Θ if and only if we can conclude the strict equality in (7.5.8), that is if
and only if the statistic T and the random variable Y are linearly related w.p.1.
That is, the CRLB will be attained by the variance of T if and only if
with some fixed real valued functions a(.) and b(.).
Remark 7.5.2 By combining the CRLB from (7.5.1) and the expression
for the information IX(θ), defined in (6.4.1), we can immediately restate the
Cramér-Rao inequality under the same standard assumptions as follows:
where IX1(θ) is the information about the unknown parameter θ in one single
observation X1.
We will interchangeably use the form of the Cramér-Rao inequality
 given by (7.5.1) or (7.5.11). The CRLB would then correspond to
 the expressions found on the rhs of either of these two equations.
Since the denominator in the CRLB involves the information, we would
rely heavily upon some of the worked out examples from Section 6.4.
Example 7.5.1 Let X1, ..., Xn be iid Poisson(λ) where λ(> 0) is the
unknown parameter. Let us consider T(λ) = λ so that T′(λ) = 1. Now,
is an unbiased estimator of λ Vλ(
) = n-1λ and . Recall from the Ex-
ample 7.4.4 that we could not claim that 
 was the UMVUE of λ. Can
we now claim that 
 is the UMVUE of λ? In (6.4.2) we find IX1(λ) = λ-1
and hence from the rhs of (7.5.11) we see that the CRLB = 1/(nλ-1) = λ/n

7. Point Estimation
369
which coincides with the expression for 
 That is, the estimator attains
the smallest possible variance among all unbiased estimators of ?. Then, 
must be the UMVUE of λ.!
Example 7.5.2 Let X1, ..., Xn be iid Bernoulli(p) where 0 < p < 1 is the
unknown parameter. Let us consider T(p) = p so that T′(p) = 1. Now, 
 is an
unbiased estimator of p and 
  Recall from the Example
7.4.1 that we could not claim that was the UMVUE of p. Can we now claim
that 
 is the UMVUE of p? Let us first derive the expression for IX1(p).
Observe that
so that  
 log f(x; p) = xp-1 - (1 - x)(1 - p)-1 = (x - p){p(1 - p)}-1. Hence, one
evaluates IX1(p) as
Thus from (7.5.11), we have the CRLB = 1/(n{p(1 - p)}-1) = p(1 - p)/n which
coincides with the expression for 
 That is,
 attains the smallest pos-
sible variance among all unbiased estimators of p. Then, 
 must be the UMVUE
of p. !
Example 7.5.3 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is un-
known but σ2 is known. Here we have −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ. We
wish to estimate T(µ) = µ unbiasedly. Consider 
 which is obviously an unbi-
ased estimator of µ. Is 
 the UMVUE of µ? Example 7.4.6 was not decisive
in this regard. In (6.4.3) we find IX1 (µ) = σ-2 so that from (7.5.11) we have
the CRLB = 1/(n σ--2) = σ2/n which coincides with the expression for
Then, 
 must be the UMVUE of µ. !
In these examples, we thought of a “natural” unbiased estimator of the
parametric function of interest and this estimator’s variance happened to co-
incide with the CRLB. So, in the end we could claim that the estimator we
started with was in fact the UMVUE of T(µ). The reader has also surely noted
that these UMVUE’s agreed with the Rao-Blackwellized versions of some of
the naive initial unbiased estimators.
We are interested in unbiased estimators of T(µ). We found the
Rao-Blackwellized version W of an initial unbiased estimator T.
But, the variance of this improved estimator W may not attain
the CRLB. Look at the following example.
Example 7.5.4 (Example 7.4.5 Continued) Suppose that X1, ..., Xn
are iid Poisson(λ) where 0 < λ < ∞ is unknown with n ≥ 2. We wish to

370
7. Point Estimation
estimate T(λ) = e-λ unbiasedly. In the Example 7.4.5, we had started with T =
I(X1 = 0) but its Rao-Blackwellized version was in fact W = 
Does Vλ(W) attain the CRLB? Recall that 
 is Poisson(nλ) so that its
mgf is given by
Let us use (7.5.13) with s = 2log(1 - n-1) to claim that Eλ[W2] = exp{nλ(es -
1)} = exp{nλ[(n-1/n)2 − 1]} = exp{−2λ + n-1λ}. Hence, we obtain
Now, we have T’(λ) = -e-λ and, from (6.4.2), IX1(λ) = λ-1. Utilizing (7.5.11) we
obtain the CRLB = {T’(λ)}2/(nλ-1) = n-1λe-2λ. Now, for x > 0, observe that ex >
1 + x. Hence, from (7.5.14) we obtain
In other words, Vλ[W] does not attain the CRLB. !
Question remains whether the estimator W in the Example 7.5.4
 is the UMVUE of e-λ. It is clear that the CRLB alone may not
point toward the UMVUE. Example 7.5.5 is also similar.
Example 7.5.5 (Example 7.4.8 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ is unknown but σ2 is known with −∞ < µ < ∞, 0 < σ < ∞
and χ = ℜ. We wish to estimate T(µ) = µ2 unbiasedly. In the Example 7.4.8,
we found the Rao-Blackwellized unbiased estimator 
Let us first obtain the expression of the variance of the estimator W as fol-
lows:
The first term in (7.5.15) is evaluated next. Recall that 
 = 0
 and 
 since
 has N(µ, n-1 σ2) distribution. Thus, we have

7. Point Estimation
371
and in view of (7.5.15) we obtain
From (7.5.11), we have the CRLB = {T′(µ)}2/(nσ-2) = 4n-1 µ2σ2 and compar-
ing this with (7.5.16) it is clear that Vµ[W] > CRLB for all µ. That is, the
CRLB is not attained.!
7.5.2   The Lehmann-Scheffé Theorems and UMVUE
In situations similar to those encountered in the Examples 7.5.4-7.5.5, it is
clear that neither the Rao-Blackwell Theorem nor the Cramér-Rao inequality
may help in deciding whether an unbiased estimator W is the UMVUE of T(?).
An alternative approach is provided in this section.
If the Rao-Blackwellized version in the end always comes up with the same
refined estimator regardless of which unbiased estimator of T(θ) one initially
starts with, then of course one has found the UMVUE for T(θ). Lehmann and
Scheffé’s (1950) notion of a complete statistic, introduced in Section 6.6,
plays a major role in this area. We first prove the following result from Lehmann
and Scheffé (1950).
Theorem 7.5.2 (Lehmann-Scheffé Theorem I) Suppose that T is an
unbiased estimator of the real valued parametric function T(θ) where the un-
known parameter θθθθθ ∈ Θ ⊆ ℜk. Suppose that U is a complete (jointly) suffi-
cient statistic for θθθθθ. Define g(u) = Eθ[T |  U = u], for u belonging to U, the
domain space of U. Then, the statistic W = g(U) is the unique (w.p.1) UMVUE
of T(θ).
Proof The Rao-Blackwell Theorem assures us that in order to search for
the best unbiased estimator of T(θ), we need only to focus on unbiased esti-
mators which are functions of U alone. We already know that W is a function
of U and it is an unbiased estimator of T(θ). Suppose that there is another
unbiased estimator W* of T(θ) where W* is also a function of U. Define h(U)
= W − W* and then we have
Now, we use the Definition 6.6.2 of the completeness of a statistic. Since U is
a complete statistic, from (7.5.17) it follows that h(U) = 0 w.p.1, that is we
must have W = W* w.p.1. The result then follows. !
In our quest for finding the UMVUE of T(θ), we need not always have to
go through the conditioning with respect to the complete sufficient statistic
U. In some problems, the following alternate and yet equivalent result may

372
7. Point Estimation
be more directly applicable. We state the result without proving it. Its proof
can be easily constructed from the preceding proof of Theorem 7.5.2.
Theorem 7.5.3 (Lehmann-Scheffé Theorem II) Suppose that U is a
complete sufficient statistic for ? where the unknown parameter θθθθθ ∈ Θ ⊆ ℜk.
Suppose that a statistic W = g(U) is an unbiased estimator of the real valued
parametric function T(θ). Then, W is the unique (w.p.1) UMVUE of T(θ).
In the Examples 7.5.6-7.5.8, neither the Rao-Blackwell Theorem
nor the Cramér-Rao inequality helps in identifying the UMVUE.
 But, the Lehmann-Scheffé approach is right on the money.
Example 7.5.6 (Example 7.5.4 Continued) Suppose that X1, ..., Xn are iid
Poisson(λ) where 0 < λ < ∞ is unknown with n ≥ 2. We wish to estimate T(µ) =
e-λ unbiasedly. The Rao-Blackwellized unbiased estimator of T(λ) was
 and its variance was strictly larger than the CRLB. But, W
depends only on the complete sufficient statistic U = 
 Hence, in view of
the Lehmann-Scheffé Theorems, W is the unique (w.p.1) UMVUE for T(λ). !
Example 7.5.7 (Example 7.5.5 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ is unknown but σ2 is known with −∞ < µ < ∞, 0 < σ < ∞
and χ = ℜ. We wish to estimate T(µ) = µ2 unbiasedly. We found the Rao-
Blackwellized unbiased estimator 
 and but its vari-
ance was strictly larger than the CRLB. Now, W depends only on the com-
plete sufficient statistic 
  Hence, in view of the Lehmann-Scheffé
Theorems, W is the unique (w.p.1) UMVUE for T(µ). !
Example 7.5.88888 (Example 7.4.3 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p) where 0 < p < 1 is unknown with n ≥ 2. We wish to estimate T(p)
= p(1 - p) unbiasedly. Recall that the Rao-Blackwellized version of the unbi-
ased estimator turned out to be 
  But, W depends only on
the complete sufficient statistic 
 Hence, in view of the Lehmann-
Scheffé Theorems, W is the unique (w.p.1) UMVUE for T(p). !
Let us add that the Rao-Blackwellized estimator in the Example 7.4.7 is
also the UMVUE of the associated parametric function. The verification is left
as Exercise 7.5.2.
Next we give examples where the Cramér-Rao inequality is not
applicable but we can conclude the UMVUE property of the
 natural unbiased estimator via the Lehmann-Scheffé Theorems.
Example 7.5.9 Let X1, ..., Xn be iid Uniform(0, θ) where θ(> 0) is the
unknown parameter. Now, U = Xn:n, the largest order statistic, is complete

7. Point Estimation
373
sufficient for θ. We wish to estimate T(θ) = θ unbiasedly. Since U has its pdf
given by nun-1θ-n I(0 < u < θ) we have Eθ[Xn:n] = n(n + 1)-1 θ. Hence, Eθ[(n +
1)n-1 Xn:n] = θ so that W = (n + 1)n-1 Xn:n is an unbiased estimator of θ. Thus,
by the Lehmann-Scheffé Theorems, (n + 1)n-1 Xn:n is the UMVUE for θ. Here,
the domain space χ for the X variable depends on ? itself. Hence, an approach
through the Cramér-Rao inequality is not feasible. !
Example 7.5.10 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ and s are
both unknown, −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ. Let us write θ = (µ, θ2) ∈
Θ = ℜ × ℜ+ and we wish to estimate T (θ) = µ unbiasedly. Recall from the
Example 6.6.7 that U = (
, S2) is a complete sufficient statistic for θθθθθ. Obvi-
ously, 
 is an unbiased estimator of T (θ) whereas is a function of U only.
Thus, by the Lehmann-Scheffé Theorems, 
 is the UMVUE for µ. Similarly,
one can show that S2 is the UMVUE for σ2. The situation here involves two
unknown parameters µ and σ. Hence, an approach through the Cramér-Rao
inequality as stated will not be appropriate. !
Example 7.5.11 (Example 7.5.10 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) where µ and σ are both unknown, −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ.
Let us write θθθθθ = (µ, θ2) ∈ Θ = ℜ × ℜ+ and we wish to estimate T(?) = µ + s
unbiasedly. Now, U = (
, S2) is a complete
sufficient statistic for θθθθθ. From (2.3.26) for the moments of a gamma variable,
it follows that Eθ[S] = ans where 
  Thus
  is an unbiased estimator of T(θ) and W depends only on U.
Hence, by the Lehmann-Scheffé Theorems, 
 is the UMVUE for µ +
σ. Similarly one can derive the UMVUE for the parametric function µσ? where
? is any real number. For k < 0, however, one should be particularly cautious
about the minimum required n. We leave this out as Exercise 7.5.7. !
Example 7.5.12 (Example 4.4.12 Continued) Suppose that X1, ..., Xn are
iid with the common negative exponential pdf f(x; θθθθθ) = σ-1 exp{−(x − µ)/σ}I(x
> µ) where µ and σ are both unknown, θθθθθ = (µ, σ) ∈ Θ = ℜ × ℜ+, n = 2. Here,
we wish to estimate T(θ) = µ + σ, that is the mean of the population, unbi-
asedly. 
Recall 
from 
Exercises 
6.3.2 
and 
6.6.18 
that
 is a complete sufficient statistic for θθθθθ. Now,
is an unbiased estimator of T(θ) and note that we can rewrite 
 = Xn:1
+
 which shows that 
 depends only on U. Thus, by the
Lehmann-Scheffé Theorems, 
 is the UMVUE for µ+σ. Exploiting (2.3.26),
one can derive the UMVUE for the parametric function µσk where K is any
real number. For ? < 0, however, one should be particularly cautious about the
minimum required n. We leave this out as Exercise 7.5.8. !

374
7. Point Estimation
7.5.3   A Generalization of the Cramér-Rao Inequality
In the statement of the Cramér-Rao inequality, Theorem 7.5.1, the random
variables X1, ..., Xn do not necessarily have to be iid. The inequality holds with
minor modifications even if the X’s are not identically distributed but they are
independent.
Let us suppose that we have iid real valued observable random variables Xj1,
..., Xjn, from a population with the common pmf or pdf fj(x; θ) where Xj’s and
Xl’s are independent for all j ≠ l = 1, ..., k, x ∈ χ ⊆ ℜ. Here, we have the
unknown parameter θ ∈ Θ ⊆ ℜ. We denote X = (X11, ..., X1n1, ..., Xk1, ..., Xknk)
and X = (x11, ..., x1n1, ..., xk1, ..., xknk) with 
  x ∈ χn. Let
us pretend that we are working with the pdf’s and hence the expectations of
functions of the random variables would be written as appropriate multiple
integrals. In the case of discrete random variables, one will replace the inte-
grals by the corresponding finite or infinite sums, as the case may be. Let us
denote the information content of Xj1 by IX3 (θ) which is calculated as E?
[(
[logfj(X;θ)])2]. We now state a generalized version of the Theorem 7.5.1.
Its proof is left as Exercise 7.5.16.
Standing Assumptions: Let us assume that the support χ does not in-
volve θ and the first partial derivative of fj(x;θ), j = 1, ..., k with respect to θ
and the integrals with respect to X are interchangeable.
Theorem 7.5.4 Suppose that T = T(X) is an unbiased estimator of a real
valued parametric function T(θ), that is Eθ(T) = T(θ) for all θ ∈ Θ. Assume
also that 
T (θ), denoted by T′(θ), exists and is finite for all θ ∈ Θ. Then, for
all θ ∈ Θ, under the standing assumptions we have:
The expression on the rhs of the inequality in (7.5.18) is called the Cramér-
Rao lower bound (CRLB) as before.
Example 7.5.13 Let X11, ..., X1n1 be iid Exponential(θ) so that their common
pdf is given by f1(x;θ) = θ-1e-x/θ with the unknown parameter θ ∈ Θ = (0, ∞) and
χ = (0, ∞). Also, suppose that X21, ..., X2n2 are iid Gamma(α, θ), that is, their
common pdf is given by f2(x; θ) = {θαΓ(α)}-1e-x/θxα-1 with the same unknown
parameter θ and χ = (0, ∞), but α(> 0) is assumed known. Let us assume that
the X1’s are independent of the X2’s. In an experiment on reliability and survival
analyses, one may have a situation like this where a combination of two or more
statistical models, depending on the same unknown parameter θ, may be ap-
propriate. By direct calculations we obtain IX1(θ) = θ-2 and IX2(θ) = αθ-2. That

7. Point Estimation
375
is, the CRLB from (7.5.18) for the variance of unbiased estimators of T(θ)
would be θ2(n1 + n2α)-1. Obviously, 
 is sufficient for
θ and Eθ{U} = (n1 + n2α)θ so that (n1 + n2α)-1U is an unbiased estimator of θ.
But, note that Vθ{(n1 + n2α)-1U} = (n1 + n2α)-2(n1θ2 + n2αθ2) = θ2(n1 + n2α)-1,
the same as the CRLB. Hence, we conclude that (n1 + n2α)-1
 is the UMVUE of θ. !
Example 7.5.14 Suppose that X11, ..., X1n1 are iid N(µ, 1) with the un-
known parameter µ ∈ (−∞, ∞) and χ = (−∞, ∞). Also, suppose that X21, ...,
X2n2 are iid N(2µ, σ2) involving the same unknown parameter µ and χ = (−∞,
∞), but σ(> 0) is assumed known. Let us assume that the X1’s are indepen-
dent of the X2’s. By direct calculations we have IX1(µ) = 1 and IX2(µ) = 4σ-2.
That is, the CRLB from (7.5.18) for the variance of unbiased estimators of
T(µ) = µ is given by (n1 + 4n2σ-2)-1. Obviously, 
is sufficient for µ and Eµ{U} = (n1 + 4n2σ-2)µ so that (n1 + 4n2σ-2)-1U is an
unbiased estimator of µ. But, note that Vµ{(n1 + 4n2σ-2)-1U} = (n1 + 4n2σ-2)-
2(n1 + 4n2σ-2) = (n1 + 4n2σ-2)-1, the same as the CRLB. Hence, we conclude
that (n1 + 4n2σ-2)-1 
 is the UMVUE of µ.!
7.5.4   Evaluation of Conditional Expectations
Thus far we aimed at finding the UMVUE of a real valued parametric function
T(θ). With some practice, in many problems one will remember the well-
known UMVUE for some of the parametric functions. Frequently, such
UMVUE’s will depend on complete sufficient statistics. Combining these in-
formation we may easily find the expressions of special types of conditional
expectations.
Suppose that T and U are statistics and we wish to find the expression for
Eθ[T |  U = u]. The general approach is involved. One will first derive the
conditional distribution of T given U and then directly evaluate the integral
∫T tf(t |  U = u; θ)dt). In some situations we can avoid this cumbersome
process. The following result may be viewed as a restatement of the Lehmann-
Scheffé Theorems but it seems that it has not been included elsewhere in its
present form.
Theorem 7.5.5 Suppose that X1, ..., Xn are iid real valued random vari-
ables with the common pmf or pdf given by f(x; θ) with x ∈ χ ⊆ ℜ, θ ∈ Θ ⊆
ℜk. Consider two statistics T and U where T is real valued but U may be
vector valued such that Eθ[T] = T(θ), a real valued parametric function.
Suppose that U is complete sufficient for θ and a known real valued

376
7. Point Estimation
statistic g(U) is the UMVUE for T(θθθθθ). Then, we have:
Proof Since the statistic U is assumed both complete and sufficient, by
virtue of the Lehmann-Scheffé Theorems it follows that Eθ[T | U] must be the
best unbiased estimator of T(θθθθθ). But g(U) is the unique UMVUE of T(?) and
hence the result follows immediately. !
Example 7.5.15 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is un-
known but σ2 is known with n = 4, −∞ < µ < ∞, 0 < σ < ∞ and χ = ℜ. Let
 which is unbiased for T(µ) = 2µ and U = 
 . But, U is
complete sufficient for µ and hence g(U) = 2U is the unique UMVUE for T(µ).
Thus, in view of the Theorem 7.5.5 we can immediately write Eµ[T | 
 .2
Instead if we had T = (X1 + X2)2, then Eµ[T] = 4µ2 + 2σ2 = T(µ). But, the
unique UMVUE of µ2 was earlier (Example 7.5.7) found to be 
Hence in view of the Theorem 7.5.5, we can immediately write
  !
Suppose that X1, ..., Xn are random samples from Poisson(λ),
0 < λ < ∞. Among other things, the following example shows
easily that Vλ(S2) > Vλ 
 for all λ.
Example 7.5.16 Suppose that X1, ..., Xn are iid Poisson(λ) with 0 < λ < ∞
unknown and n ≥ 2. Let us denote T = S2, the sample variance, and U = 
Obviously, Eλ[T] = λ = T(λ). But, U is complete sufficient and so U is the
unique UMVUE for T(λ). Thus, in view of the Theorem 7.5.5, we can write
Eλ[T | 
 ] = 
 .
Now, we use the Theorem 3.3.1, part (ii) to rewrite Vθ(S2) as
which exceeds Vλ[
] , for all λ, because V(S2 |
) is a positive random
variable whose expectation is positive. For the direct calculation of Vθ(S2),
however, refer to the expression given by (5.2.23). !
Example 7.5.17 Let X1, ..., Xn be iid Uniform (0, θ) where θ(> 0) is the
unknown parameter with n ≥ 2. Then, U = Xn:n is complete sufficient for θ.
Consider 
  Obviously, Eθ[T] = ½θ+1/3θ2 = T (θ). Now, since U
has its pdf given by nun-1θ-nI (0 < u < θ), we have Eθ[Xn:n] = n(n+1)-1θ and
  Hence, the unique UMVUE of T(θ) is given by
  Thus, in view of the Theorem 7.5.5,

7. Point Estimation
377
we can write 
  If one
first obtains the conditional pdf of T given Xn:n and evaluates Eθ[T | Xn:n]
directly, then one will realize what a clever approach the present one has
been. !
Example 7.5.18 (Example 7.5.17 Continued) Let X1, ..., Xn be iid random
variables distributed uniformly on (0, θ) where θ(> 0) is the unknown param-
eter with n ≥ 2. Again U = Xn:n is complete sufficient for θ. Consider T =
Obviously, Eθ[T] = ¼θ2(1 + 1/3n) = T(θ). One should verify that
  We leave it out as Exercise 7.5.21.
Again, if one first obtains the conditional pdf of T given Xn:n and evaluates
Eθ[T | Xn:n] directly, then one will realize what a clever approach the present
one has been. !
7.6    Unbiased Estimation Under Incompleteness
Suppose that it we start with two statistics T and T’, both estimating the same
real valued parametric function T(θ) unbiasedly. Then, we individually condi-
tion T and T’ given the sufficient statistic U and come up with the refined
Rao-Blackwellized estimators W = Eθ[T | U] and W’ = Eθ[T’ | U] respectively.
It is clear that (i) both W, W’ would be unbiased estimators of T(θ), (ii) W will
have a smaller variance than that of T, and (iii) W’ will have a smaller variance
than that of T’.
An important question to ask here is this: how will the two
variances Vθ(W) and Vθ(W’) stack up against each other?
If the statistic U happens to be complete, then the Lehmann-Scheffé Theo-
rems will settle this question because W and W’ will be identical estimators
w.p.1. But if U is not complete, then it may be a different story. We highlight
some possibilities by means of examples.
7.6.1 Does the Rao-Blackwell Theorem Lead to UMVUE?
Suppose that X1, ..., Xn are iid N(θ, θ2) having the unknown parameter θ ∈ Θ
= (0, ∞), χ = (−∞, ∞), n ≥ 2. Of course,
 is sufficient for θ but
U is not complete since one can check that 
  for
all θ ∈ Θ and yet 
 is not identically zero w.p.1. Now, let us
work with 
 where 
From (2.3.26) it follows that Eθ[S] = anθ and hence T’ is unbiased for

378
7. Point Estimation
θ while obviously so is T itself. Now, 
 , S2 are independently distributed and
hence we claim the following:
Thus, the Rao-Blackwellized versions of T and T’ are respectively 
and 
  Clearly W and W’ are both unbiased estimators for the
unknown parameter θ. But observe that Vθ(W) = n-1θ2 while Vθ(W′ ) = (an
-2 −
1)θ2. Next, note that Γ(x + 1) = xΓ(x) for x > 0, 
 and evaluate
Vθ(W′ ). Look at the Table 7.6.1.
Table 7.6.1. Comparing the Two Variances Vθ(W) and Vθ(W’)
n
 θ-2Vθ(W)
 Exact θ-2(Vθ(W′ )
Approx. θ-2Vθ(W′ )
2
 .5000
 (π/2) − 1
.5708
3
.3333
(4/π) − 1
.2732
4
.2500
 (3π/8) − 1
.1781
5
.2000
{32/(9π)} − 1
.1318
Upon inspecting the entries in the Table 7.6.1, it becomes apparent that W
is better than W′ in the case n = 2. But when n = 3, 4, 5, we find that W′ is
better than W.
For large n, however, Vθ(W′ ) can be approximated. Using (1.6.24) to
approximate the ratios of gamma functions, we obtain
Now, we apply (7.6.1) with x = ½n, a = -½ and b = 0 to write
In other words, from (7.6.2) it follows that for large n, we have
Using MAPLE, the computed values of θ-2Vθ(W′ ) came out to be 2.6653 ×
10-2, 1.7387 × 10-2, 1.2902 × 10-2, 1.0256 × 12-2, 5.0632 × 103 and 2.5157 ×
10-3 respectively when n = 20, 30, 40, 50, 100 and 200. The approximation
given by (7.6.3) seems to work well for n ≥ 40.

7. Point Estimation
379
Khan (1968) considered this estimation problem but he first focussed on
the status of the MLE. Our goal is slightly different. Utilizing the two initial
unbiased estimators T, T′ we focussed on the Rao-Blackwellized versions and
compared their variances. This was not emphasized in Khan (1968). We find
that W′ can be better than W even if n is small (≥ 3) but for large n, the
estimator W′ can have approximately fifty percent less variation than the esti-
mator W.
Khan (1968) proceeded to derive another unbiased estimator for θ which
performed better than both W and W′. The basic idea was simple. Let us look
at the following class of unbiased estimators of θ :
For each α ∈ [0, 1], T* (α) is unbiased for θ and hence the one having the
smallest variance should be more attractive than either W or W′ . Since 
 and
S2 are independent, we have
Now, Vθ(T*) can be minimized directly with respect to the choice of α. The
optimal choice of α is given by
In view of (7.6.3), α* reduces to 1/3 for large n. With α* determined by
(7.6.5), the corresponding unbiased estimator T* (α*) would have its vari-
ance smaller than that of both W and W′ . One will find interesting decision
theoretic considerations in Gleser and Healy (1976). Lehmann (1983, page
89) mentions an unpublished thesis of Unni (1978) which showed that a
UMVUE of θ did not exist in the present situation.
Example 7.6.1 Let us suppose that one has X1, ..., Xn iid with the common
pdf f(x; θ) = θ−1 exp{−(x−θ)/θ}I(x > θ) with θ ∈ Θ = ℜ+ where θ is the
unknown parameter. Here, we wish to estimate θ unbiasedly. Recall from the
Exercise 6.6.5 that the statistic 
 is not complete
but it is sufficient for θ. One should check into the ramifications of the pre-
ceding discussions in the context of the present problem. We leave this out as
Exercise 7.6.1. !

380
7. Point Estimation
7.7   Consistent Estimators
The consistency is a large sample property of an estimator. R. A. Fisher had
introduced the concept of consistency in his 1922 paper. Let us suppose that
we have {Tn = Tn(X1, ..., Xn); n ≥ 1}, a sequence of estimators for some
unknown real valued parametric function T(θθθθθ) where θθθθθ ∈ Θ ⊆ ℜk. We empha-
size the dependence of the estimators on the sample size n by indexing with
subscript n. It may help to recall the concept of the convergence in probabil-
ity, denoted by 
 which was laid out by the Definition 5.2.1.
Definition 7.7.1 Consider {Tn ≡ Tn(X1, ..., Xn); n ≥ 1}, a sequence of
estimators for some unknown real valued parametric function T(θθθθθ) where θθθθθ ∈
Θ ⊆ ℜk. Then, Tn is said to be consistent for T(θθθθθ) if and only if 
 as
n → ∞ . Also, Tn is called inconsistent for T(θθθθθ) if Tn is not consistent for T(θθθθθ).
An estimator Tn may be biased for T(θθθθθ) and yet Tn
may be consistent for T(θθθθθ).
Estimators found by the method of moments are often smooth functions
of averages of powers of the X’s and so they are consistent for the associated
parametric functions. One should verify that all the estimators derived in the
Examples 7.2.1-7.2.5 are indeed consistent for the parameters of interest.
The MLE’s derived in the Examples 7.2.6-7.2.7 and also in the Examples
7.2.9-7.2.12 are all consistent.
Let us particularly draw attention to the Examples 7.2.5 and 7.2.9. In the
Uniform(0, θ) case, the method of moment estimator (Example 7.2.5) for θ
turned out to be 
 and by the Weak WLLN (Theorem 5.2.1) it follows
that 
 as n → ∞. That is, Tn is a consistent estimator of θ even though
Tn is not exclusively a function of the minimal sufficient statistic Xn:n. The
MLE (Example 7.2.9) of θ is Xn:n and recall from Example 5.2.5 
that as n → ∞. Thus, Xn:n is a consistent estimator for θ. Note that Xn:n is not
unbiased for θ.
All the UMVUE’s derived in Section 7.5 are consistent for the parameters
of interest. Their verifications are left as exercises. Let us, however, look at
the Example 7.5.4 where the unbiased estimator 
 was
found for the parametric function e-λ in the case of the Poisson(λ) distribu-
tion. Let us rewrite
where 
  Vn = (1-n-1)n → e-1 as n → ∞. In this deliberation,

7. Point Estimation
381
it will help to think of Vn as a sequence of degenerate random variables. Then,
one can repeatedly apply the Theorems 5.2.4-5.2.5 and conclude that
 and hence 
 as n → ∞. That is,
 is a consistent estimator of e-θ.
One should also verify that the estimators W, W′, and T*(α*) defined via
(7.6.4)-(7.6.5) are also consistent for the unknown parameter θ in the
N(θ, θ2) population. Details are left out as Exercise 7.7.5.
Having found a number of estimators in so many statistical models which
are consistent for the parameters of interest, we wish to emphasize again that
consistency is after all a large sample property. In statistics, one invariably
works with data of fixed size n and hence one should not rely upon the con-
sistency property of an estimator alone to claim any superior performance on
behalf of the estimator under consideration.
D. Basu, among others, emphasized the limited usefulness of the concept
of consistency of an estimator. Basu gave the following forceful example. Let
X1, ..., Xn be iid N(θ, 1) where θ ∈ (−∞, ∞) is the unknown parameter. Define
a sequence of estimators of θ as follows:
where k is fixed but presumably very large. Since 
 is consistent
for θ, it follows from the definition of the estimator Tn that it would be consis-
tent for θ too. But, Basu would argue that in real life how many times does
one encounter a sample of size n larger than a million! So, he would focus on
the estimator Tn when k = 106. Now, if an experimenter is committed to use
the estimator Tn from (7.7.1), then for all practical purposes, regardless of
what the observed data dictates, one will end up guessing that θ is zero and
yet such an estimator would be consistent for θ ! This construction of Tn
unfortunately created an impression that there was something inherently wrong
with the concept of consistency. Can a reader think of any practical scenario
where Tn defined by (7.7.1) will be used to estimate θ?
Basu wanted to emphasize that the consistency of an estimator is not a
useful property in the practice of statistics. But, on the other hand, the given
Tn will almost certainly never be used in practice, and so it remains vague
what this example actually conveys. What this example might have demon-
strated was this: The consistency of an estimator may not be a useful prop-
erty in itself in the practice of statistics. This is exactly what we had said
before we gave Basu’s example! In other words, Basu’s example has not been
nearly as damaging to the concept of consistency as some non-Fisherians
may have liked to believe.

382
7. Point Estimation
It has now become a common place for some authors of Bayesian papers
to claim that a proposed “Bayes estimator” or “Bayes decision” also satisfies
the “consistency” property in some sense. In the Bayesian literature, some
authors refer to this as the Bayesian-Frequentist compromise. Will it then be
fair to say that the (Fisherian) consistency property has not been such a bad
concept after all? The “history” does have an uncanny ability to correct its
own course from time to time! At the very least, it certainly feels that way.
A simple truth is that R. A. Fisher never suggested that one should choose
an estimator because of its consistency property alone. The fact is that in
many “standard” problems, some of the usual estimators such as the MLE,
UMVUE or estimators obtained via method of moments are frequently con-
sistent for the parameter(s) of interest.
The common sense dictates that the consistency property has to take a
back seat when considered in conjunction with the sufficiency property. The
estimator Tn defined in (7.7.1) is not sufficient for θ for any reasonable fixed-
sample-size n when k = 106. On the other hand, the sample mean
 is sufficient for θ and it also happens to be consistent for θ.
We should add that, in general, an MLE  may not be consistent for ?.
Some examples of inconsistent MLE’s were constructed by Neyman and
Scott (1948) and Basu (1955b). In Chapter 12, under mild regularity condi-
tions, we quote a result in (12.2.3) showing that an MLE  is indeed consis-
tent for the parameter θ it is supposed to estimate in the first place. This
result, in conjunction with the invariance property (Theorem 7.2.1), make the
MLE’s very appealing in practice.
7.8   Exercises and Complements
7.2.1 (Example 7.2.3 Continued) Suppose that X
1, ..., X
n are iid N(0, σ
2)
where q = σ
2 is unknown, 0 < s < ¥. Here we have χ = Θ = ℜ and h
1 ≡ h
1(q)
= E
q[X
1] = 0, 
 for all q. It is clear that the equations
given by the first and third moments in (7.2.1) do not lead to anything inter-
esting. In the Example 7.2.3 we had used the expression of η
2(θ). Now we
may 
arbitrarily 
move 
to 
the 
fourth 
moment 
and 
write
  Using η
4, find an appropriate estimator of σ
2.
How is this estimator different from the one obtained in the Example 7.2.3?
7.2.2 (Exercise 6.3.5 Continued) Suppose that X1, ..., Xn are iid dis-
tributed as Geometric(p) random variables with the common pmf given
by f(x; p) = p(1 - p)x, x = 0, 1, 2, ... , and 0 < p < 1 is the unknown

7. Point Estimation
383
parameter. Find an estimator of p by the method of moments.
7.2.3 Suppose that X1, ..., Xn are iid distributed as Gamma(α, β) random
variables where α and β are both unknown parameters, 0 < α, β < ∞. Derive
estimators for α and β by the method of moments.
7.2.4 Suppose that X1, ..., Xn are iid whose common pdf is given by
where θ(> 0) is the unknown parameter. Derive an estimator for θ by the
method of moments.
7.2.5 Suppose that X1, ..., Xn are iid distributed as Beta(θ, θ) random vari-
ables where θ(> 0) is the unknown parameter. Derive an estimator for θ by
the method of moments.
7.2.6 (Exercise 7.2.2 Continued) Suppose that X1, ..., Xn are iid distributed
as Geometric(p) random variables with the common pmf given by f(x; p) =
p(1 - p)x, x = 0, 1, 2, ... , and 0 < p < 1 is the unknown parameter. Find the
MLE of p. Is the MLE sufficient for p?
7.2.7 Suppose that X1, ..., Xn are iid Bernoulli(p) random variables where p
is the unknown parameter, 0 ≤ p ≤ 1.
(i)
Show that 
 is the MLE of p. Is the MLE sufficient for p?
(ii)
Derive the MLE for p2;
(iii)
Derive the MLE for p/q where q = 1 - p;
(iv)
Derive the MLE for p(1 - p).
{Hint: In parts (ii)-(iv), use the invariance property of the MLE from Theo-
rem 7.2.1.}
7.2.8 (Exercise 7.2.7 Continued) Suppose that we have iid Bernoulli(p)
random variables X1, ..., Xn where p is the unknown parameter, 0 < p < 1.
Show that 
 is the MLE of p when 
 is not zero or one. In the light of the
Example 7.2.10, discuss the situation one faces when 
 is zero or one.
7.2.9 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is known but σ2 is
unknown, θ = σ2, −∞ < µ < ∞, 0 < σ < ∞, n ≤ 2.
(i)
Show that the MLE of σ2 is 
 ;
(ii)
Is the MLE in part (i) sufficient for σ2?
(iii)
Derive the MLE for 1/σ;
(iv)
Derive the MLE for (σ + σ-1)½.
{Hint: In parts (iii)-(iv), use the invariance property of the MLE from Theo-
rem 7.2.1.}

384
7. Point Estimation
7.2.10 Suppose that X1, ..., Xn are iid with the Rayleigh distribution, that is the
common pdf is
f(x;θ) = 
where θ(> 0) is the unknown parameter. Find the MLE for θ. Is the MLE
sufficient for θ?
7.2.11 Suppose that X1, ..., Xn are iid with the Weibull distribution, that is
the common pdf is
f(x;α) =
where α(> 0) is the unknown parameter, but β(> 0) is assumed known. Find
the MLE for α. Is the MLE sufficient for α?
7.2.12 (Exercise 6.2.11 Continued) Let X1, ..., Xn be iid having the com-
mon pdf σ-1exp{−(x − µ)/σ}I(x > µ) where µ and σ are both unknown, −∞ <
µ < ∞, 0 < σ < ∞, n ≥ 2. Show that the MLE for µ and σ are respectively Xn:1,
the smallest order statistic, and 
  Then, derive the MLE’s
for µ/σ, µ/σ2 and µ + σ.
7.2.13 (Exercise 7.2.12 Continued) Let X1, ..., Xn be iid having the com-
mon pdf σ-1exp{ −(x − µ)/σ}I(x > µ) where µ is known but σ is unknown, −
∞ < µ < ∞, 0 < σ < ∞. Show that the MLE for σ is given by 
7.2.14 (Exercise 6.3.12 Continued) Let X1, ..., Xn be iid having the com-
mon Uniform distribution on the interval (−θ, θ) where 0 < θ < ∞ is the
unknown parameter. Derive the MLE for θ. Is the MLE sufficient for θ? Also,
derive the MLE’s for θ2 and θ−2.
7.2.15 (Exercise 6.3.13 Continued) Let X1, ..., Xm be iid N(µ1, σ2), Y1, ...,
Yn be iid N(µ2, σ2), and also let the X’s be independent of the Y’s where −∞ <
µ1, µ2 < ∞, 0 < σ < ∞ are the unknown parameters. Derive the MLE for (µ1,
µ2, σ2). Is the MLE sufficient for (µ1, µ2, σ2)? Also, derive the MLE for (µ1 -
µ2)/σ.
7.2.16 (Exercise 6.3.14 Continued) Let X1, ..., Xm be iid N(µ1, σ2), Y1, ...,
Yn be iid N(µ2, kσ2), and also let the X’s be independent of the Y’s where −∞
< µ1, µ2 < ∞, 0 < σ < ∞ are the unknown parameters. Assume that the number
k (> 0) is known. Derive the MLE for (µ1, µ2, σ2). Is the MLE sufficient for
(µ1, µ2, σ2)? Also, derive the MLE for (µ1 − µ2)/σ.
7.2.17 (Exercise 7.2.4 Continued) Suppose that X1, ..., Xn are iid whose
common pdf is given by

7. Point Estimation
385
where θ(> 0) is the unknown parameter. Derive the MLE for θ. Compare this
MLE with the method of moments estimator obtained earlier. Is the MLE
sufficient for θ?
7.2.18 (Exercise 7.2.12 Continued) Let X1, ..., Xn be iid having the com-
mon pdf σ-1exp{ −(x − µ)/σ}I(x > µ) where µ and σ are both unknown, −∞
< µ < ∞, 0 < σ < ∞, n ≥ 2. Derive the method of moment estimators for µ
and σ.
7.2.19 Suppose that Y1, ..., Yn are independent random variables where Yi
is distributed as N(β0 + β1xi, σ2) with unknown parameters β0, β1. Here, σ(>
0) is assumed known and xi’s are fixed real numbers with θ = (β0, β1) ∈ ℜ2,
i = 1, ..., n, n ≥ 2.  Denote 
 
 with and assume that
a > 0. Suppose that the MLE’s for β0 and β1 are respectively denoted by
  Now, consider the following linear regression problems.
(i)
Write down the likelihood function of Y1, ..., Yn;
(ii)
Show that 
 and 
 is normally distributed
with mean β1 and variance σ2/a;
(iii)
Show that 
 and 
 is normally distributed with
mean β0 and variance σ2{1/n + 
2/a}.
7.2.20 Suppose that X1, ..., Xn are iid with the common pdf f(x; θ) =
θ−1xexp{−x2/(2θ)}I(x > 0) where 0 < θ < ∞ is the unknown parameter. Esti-
mate θ by the method of moments separately using the first and second popu-
lation moments respectively. Between these two method of moments estima-
tors of the parameter θ, which one should one prefer and why?
7.3.1 Suppose that X1, ..., X4 are iid N(0, σ2) where 0 < σ < ∞ is the
unknown parameter. Consider the following estimators:
(i)
Is Ti unbiased for σ2, i = 1, ..., 4?
(ii)
Among the estimators T1, T2, T3, T4 for σ2, which one has the
smallest MSE?
(iii)
Is T5 unbiased for σ? If not, find a suitable multiple of T5 which
is unbiased for σ. Evaluate the MSE of T5.
7.3.2 (Example 7.3.1 Continued) Let X1, ..., Xn be iid N(µ, σ2) where µ, σ
are both unknown with −∞ < µ < ∞, 0 < σ < ∞, n ≥ 2. Denote
  Let V = cU be an estimator of σ2 where c(> 0) is a
constant.

386
7. Point Estimation
(i)
Find the MSE of V. Then, minimize this MSE with respect to c.
Call this latter estimator W which has the smallest MSE among
the estimators of σ2 which are multiples of U;
(ii)
Show that estimator W coincides with 
which was used in the Example 7.3.1.
7.3.3 (Exercise 7.2.13 Continued) Let X1, ..., Xn be iid having the common
pdf σ-1exp{−(x − µ)/σ}I(x > µ) where µ, σ are both unknown, −∞ < µ < ∞ 0
< σ < ∞, n ≥ 2. Denote 
  Let V = cU be an estimator of
σ where c(> 0) is a constant.
(i)
Find the MSE of V. Then, minimize this MSE with respect to c.
Call this latter estimator W which has the smallest MSE among
the estimators of s which are the multiples of U;
(ii)
Hdow do the two estimators W and 
compare relative to their respective bias and MSE?
7.3.4 Suppose that X1, ..., Xn are iid from the following respective
populations. Find the expressions for the BLUE of θ, the parameter of
interest in each case.
(i)
The population is Poisson(λ) where θ = λ ∈ ℜ+;
(ii)
The population is Binomial(n,p) where θ = p ∈ (0, 1);
(iii)
The population has the pdf f(x) =
exp(- |x|/σ), x ∈ ℜ where
θ = σ ∈ ℜ+.
7.3.5 (Exercise 7.3.1 Continued) Suppose that X1, ..., Xn are iid N(0, σ2)
where 0 < σ < ∞ is the unknown parameter. Consider estimating θ unbiasedly
by linear functions of |Xi|, i = 1, ..., n. Within this class of estimators, find the
expression of the BLUE of σ. Next, evaluate the variance of the BLUE of σ.
7.3.6 Look at the estimator T4 defined in (7.3.1). Evaluate its MSE.
7.3.7 (Example 7.3.2 Continued) Suppose that we have iid Bernoulli(p)
random variables X1, ..., Xn where 0 < p < 1 is an unknown parameter. Show
that there is no unbiased estimator for the parametric function (i) T(p) = p-1(1
− p)-1, (ii) T(p) = p/(1 - p).
7.3.8 (Example 7.3.3 Continued) Suppose that we have iid Bernoulli(p)
random variables X1, X2, ... where 0 < p < 1 is an unknown parameter. Con-
sider the parametric function T(p) = p-2 and the observable random variable N
defined in the Example 7.3.3. Use the expressions for the mean and variance
of the Geometric distribution to find an estimator T involving N so that T is
unbiased for T(p).
7.4.1 Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 < p < 1 is
an unknown parameter with n ≥ 2. Consider the parametric function

7. Point Estimation
387
T(p) = p2. Start with the estimator T = X1X2 which is unbiased for T(p) and then
derive the Rao-Blackwellized version of T. {Hint: Proceed along the lines of
the Examples 7.4.1 and 7.4.3.}
7.4.2 (Example 7.4.3 Continued) Suppose that we have iid Bernoulli(p)
random variables X1, ..., Xn where 0 < p < 1 is an unknown parameter with n
≥ 3. Consider the parametric function T(p) = p2(1 − p). Start with the estima-
tor T = X1X2(1 − X3) which is unbiased for T(p) and then derive the Rao-
Blackwellized version of T. {Hint: Proceed along the lines of the Example
7.4.3.}
7.4.3 (Example 7.4.5 Continued) Suppose that we have iid Poisson(λ)
random variables X1, ..., Xn where 0 < λ < ∞ is an unknown parameter with n
≥ 4. Consider the parametric function T(λ) and the initial estimator T defined
in each part and then derive the corresponding Rao-Blackwellized version W
of T to estimate T(λ) unbiasedly. Consider
(i)
T(λ) = λe-λ and start with T = I(X1 = 1). Verify first that T is
unbiased for T(λ). Derive W;
(ii)
T(λ) = e-2λ and start with T = I(X1 = 0∩ X2 = 0). Verify first that
T is unbiased for T(λ). Derive W;
(iii)
T(λ) = e-3λ and start with T = I(X1 = 0 ∩ X2 = 0∩ X3 = 0). Verify
first that T is unbiased for T(λ). Derive W;
(iv)
T(λ) = λ2e-λ and start with T = 2I(X1 = 2). Verify first that T is
unbiased for T(λ). Derive W.
7.4.4 (Example 7.4.7 Continued) Suppose that X1, ..., Xn are iid N(µ, σ2)
where µ is unknown but σ is assumed known with µ ∈ ℜ, σ ∈ ℜ+. Let T(µ) =
Pµ{|X1| ≤ a} where a is some known positive real number. Find an initial
unbiased estimator T for T(µ). Next, derive the Rao-Blackwellized version of
T to estimate T(µ) unbiasedly.
7.4.5 (Exercise 7.4.4 Continued) Suppose that X1, ..., Xn are iid N(µ, σ2)
where µ is unknown but σ is assumed known with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 3. Let
T(µ) = Pµ{X1 + X2 ≤ a} where a is some known real number. Find an initial
unbiased estimator T for T(µ). Next, derive the Rao-Blackwellized version of
T to estimate T(µ) unbiasedly.
7.4.6 (Example 7.4.7 Continued) Suppose that X1, ..., Xn are iid N(µ,
σ2) where µ, σ are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let
T(µ) = Pµ{X1 > a} where a is some known real number. Start with the
initial unbiased estimator T = I(X1 > a) for T(µ). Next, derive the Rao-
Blackwellized version W of T to estimate T(µ) unbiasedly. {Hint:
Kolmogorov (1950a) first found the form of the final unbiased estimator W.

388
7. Point Estimation
This elegant proof is due to Kolmogorov. Observe that one has the sufficient
statistic U = (
 , S*) for θ = (µ, σ) where 
Let g(u) = Eθ{T |
 , S*} = Pθ{X1 > a|
 , S*} = Pθ{Y > yo |
 , S*} with
 and 
Next, verify that Y is distributed independently of U, and the pdf of Y is given
by k(1 − y2)(n-4)/2 for –1 < y < 1 and some known positive constant k. Hence,
 }
7.4.7 (Exercise 7.4.6 Continued) Suppose that X1, ..., Xn are iid N(µ, σ2)
where µ, σ are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let T(µ) = Pµ{X1 >
a} where a is some known real number. Consider the Rao-Blackwellized ver-
sion W from the Exercise 7.4.6 which estimates T(µ) unbiasedly. Derive the
form of W in its simplest form when n = 4, 6 and 8.
7.4.8 Suppose that X1, ..., Xn, Xn+1 are iid Bernoulli(p) where 0 < p < 1 is an
unknown parameter. Denote the parametric function T(p) = 
Now, consider the initial unbiased estimator T = 
 for T(p).
The problem is to find the Rao-Blackwellized version W of T. Here, it will be hard
to guess the form of the final estimator. Now, proceed along the following steps.
(i)
Note that 
 is sufficient for p and that T is an
unbiased estimator of T(p);
(ii)
Observe that W = Ep{T |U = u} = Pp{T = 1| U = u}, and find
the expression for W; {Hint: 
(iii)
Directly evaluate the ratio of the two probabilities in part (ii),
and show that g(u) = 0, n/(n + 1), (n - 1)/(n + 1), respectively
when u = 0, 1, 2, 3, ..., n, n + 1;
(iv)
Using the explicit distribution of W given in part (iii), show
that Ep[W] = 1 − np2q − qn where q = 1 − p;
(v)
It is possible that so far the reader did not feel any urge to
think about the explicit form of the parametric function T(p).
In part (iv), one first gets a glimpse of the expression of T(p).
Is the expression of T(p) correct? In order to check, the reader
is now asked to find the expression of T(p) directly from its
definition. {Hint: 

7. Point Estimation
389
> 1 ∩ Xn+1 = 1} = (1 − qn)q + (1 − qn − npqn-1)p = 1 − np2q −
qn. Even if one had first found the expression of T (p), could
one intuitively guess the form of W found in part (ii)?}
7.4.9 (Example 7.4.8 Continued) Suppose that X1, ..., Xn are iid N(µ, σ2)
where µ is unknown but σ is assumed known with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let
T (µ) = µ3. Find an initial unbiased estimator T for T (µ). Next, derive the Rao-
Blackwellized version of T to estimate T (µ) unbiasedly. {Hint: Start with the
fact that Eµ[(X1 − µ)3] = 0 and hence show that 
 is an unbi-
ased estimator for T(µ). Next, work with the third moment of the conditional
distribution of X1 given that 
 }
7.5.1 Prove Theorem 7.5.3 exploiting the arguments used in the proof of
Theorem 7.5.2.
7.5.2 (Example 7.4.7 Continued) Show that the Rao-Blackwellized estima-
tor W is the UMVUE of T(µ).
7.5.3 (Exercise 6.2.16 Continued) Suppose that X1, ..., Xn are iid with the
Rayleigh distribution, that is the common pdf is
where θ(> 0) is the unknown parameter. Find the UMVUE for (i) θ, (ii) θ2 and
(iii) θ-1. {Hint: Start with 
 which is sufficient for θ. Show that
the distribution of U/θ is a multiple of 
  Hence, derive unbiased estimators
for θ, θ2 and θ-1 which depend only on U. Can the completeness of U be
justified with the help of the Theorem 6.6.2?}
7.5.4 (Exercise 7.5.3 Continued) Find the CRLB for the variance of unbi-
ased estimators of (i) θ, (ii) θ2 and (iii) θ-1. Is the CRLB attained by the vari-
ance of the respective UMVUE obtained in the Exercise 7.5.3?
7.5.5 (Exercise 6.2.17 Continued) Suppose that X1, ..., Xn are iid with the
Weibull distribution, that is the common pdf is
where α(> 0) is the unknown parameter, but β(> 0) is assumed known. Find the
UMVUE for (i) α, (ii) α2 and (iii) α-1. {Hint: Start with U =
 which is
sufficient for θ. Show that the distribution of U/α is a multiple of 
  Hence,
derive unbiased estimators for α, α2 and α-1 which depend only on U. Can the
completeness of U be justified with the help of the Theorem 6.6.2?}.
7.5.6 (Exercise 7.5.5 Continued) Find the CRLB for the variance of
unbiased estimators of (i) α, (ii) α2 and (iii) α-1. Is the CRLB attained
by the variance of the respective UMVUE obtained in the Exercise 7.5.5?

390
7. Point Estimation
7.5.7 (Example 7.5.11 Continued) Let X1, ..., Xn be iid N(µ, σ2) where µ, σ
are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let θθθθθ = (µ σ) and T(θθθθθ) = µσk
where k is a known and fixed real number. Derive the UMVUE for T(θθθθθ). Pay
particular attention to any required minimum sample size n which may be
needed. {Hint: Use (2.3.26) and the independence between 
 and S to first
derive the expectation of 
S k where S2 is the sample variance. Then make
some final adjustments.}
7.5.8 (Example 7.5.12 Continued) Let X1, ..., Xn be iid having the common
pdf σ-1exp{−(x - µ)/σ}I(x > µ) where µ, σ are both unknown with −∞ < µ <
∞, 0 < σ < ∞, n ≥ 2. Let θθθθθ = (µ, σ) and T(θθθθθ) = µσk where k is a known and
fixed real number. Derive the UMVUE for T(θθθθθ). Pay particular attention to any
required minimum sample size n which may be needed. {Hint: Use (2.3.26)
and the independence between Xn:1 and 
 to first derive
the expectation of 
Yk. Then make some final adjustments.}
7.5.9 Suppose that X1, ..., Xn are iid Uniform (−θ, θ) where θ is the un-
known parameter, θ ∈ ℜ+. Let T(θ) = θk where k is a known and fixed positive
real number. Derive the UMVUE for T(θ). {Hint: Verify that U = |Xn:n| is
complete sufficient for θ. Find the pdf of U/θ to first derive the expectation of
Uk. Then make some final adjustments.}
7.5.10 In each Example 7.4.1-7.4.8, argue that the Rao-Blackwellized es-
timator W is indeed the UMVUE for the respective parametric function. In the
single parameter problems, verify whether the variance of the UMVUE attains
the corresponding CRLB.
7.5.11 Suppose that X1, ..., Xn, Xn+1 are iid N(µ, θ2) where µ is unknown
but σ is assumed known with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Consider the parametric
function 
 The problem is to find the UMVUE
for T(µ). Start with 
 which is an unbiased estimator for
T(θ). Now, proceed along the following steps.
(i)
Note that 
 is complete sufficient for µ and that T
is an unbiased estimator of T(θ);
(ii)
Observe that W = Eµ{T |U = u} = Pµ{T = 1| U = u}, and find
the expression for W. {Hint: Write down explicitly the bivari
ate normal distribution of 
 and U. Then,
find the conditional probability, Pµ{T1 > 0| U = u} utilizing the
Theorem 3.6.1. Next, argue that T1 > 0 holds if and only if T
= 1.}.
7.5.12 Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 < p < 1 is an
unknown parameter. Consider the parametric function T(p) = p + qe2 with q
= 1 − p.

7. Point Estimation
391
(i)
Find a suitable unbiased estimator T for T(p);
(ii)
Since the complete sufficient statistic is 
 use the
Lehmann-Scheffé theorems and evaluate the conditional expec
tation, Ep[T | U = u];
(iii)
Hence, derive the UMVUE for T(p).
{Hint: Try and use the mgf of the X’s appropriately.}
7.5.13 Suppose that X1, ..., Xn are iid Bernoulli(p) where 0 < p < 1 is an
unknown parameter. Consider the parametric function T(p) = (p+qe3)2 with
q = 1 - p.
(i)
Find a suitable unbiased estimator T for T for T(p);
(ii)
Since the complete sufficient statistic is 
 use the
Lehmann-Scheffé theorems and evaluate the conditional expec
tation, Ep[T | U = u];
(iii)
Hence, derive the UMVUE for T(p).
{Hint: Try and use the mgf of the X’s appropriately.}
7.5.14 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ is unknown but σ
is assumed known with µ ∈ ℜ, σ ∈ ℜ+. Consider the parametric function T(µ)
= e2µ. Derive the UMVUE for T(µ). Is the CRLB attained in this problem?
7.5.15 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ and σ are both
assumed unknown with µ ∈ ℜ, σ ∈ ℜ+, θθθθθ = (µ, σ), n ≥ 2. Consider the
parametric function T(θθθθθ) = e2(µ+σ2). Derive the UMVUE for T(θθθθθ).
7.5.16 Prove the Theorem 7.5.4 by appropriately modifying the lines of
proof given for the Theorem 7.5.1.
7.5.17 Let X1, ..., Xm be iid N(0, σ2), Y1, ..., Yn be iid N(2, 3σ2) where σ is
unknown with σ ∈ ℜ+. Assume also that the X’s are independent of the Y’s.
Derive the UMVUE for σ2 and check whether the CRLB given by the Theo-
rem 7.5.4 is attained in this case.
7.5.18 Suppose that X1, ..., Xm are iid Gamma(2, β), Y1, ..., Yn are iid
Gamma(4, 3β) where β is unknown with β ∈ ℜ+. Assume also that the X’s
are independent of the Y’s. Derive the UMVUE for β and check whether the
CRLB given by the Theorem 7.5.4 is attained in this case.
7.5.19 Suppose that X is distributed as N(0, σ2), Y has its pdf given by g(y;
σ2) = (2σ2)-1 exp{− |y| /σ2}I(y ∈ ℜ) where σ is unknown with σ ∈ ℜ+. Assume
also that the X is independent of the Y. Derive the UMVUE for σ2 and check
whether the CRLB given by the Theorem 7.5.4 is attained in this case.
7.5.20 Let X1, ..., Xn be iid having the common pdf σ-1 exp{ −(x
− µ)/σ}I(x > µ) where µ is known but σ is unknown with −∞ < µ <

392
7. Point Estimation
∞, 0 < σ < ∞, n ≥ 5. Let U = Xn:1, the smallest order statistic, which is
complete sufficient for µ.
(i)
Find the conditional expectation, Eµ[
 | U];
(ii)
Find the conditional expectation, Eµ[X1 | U];
(iii)
Find the conditional expectation, 
{Hint: Try and use the Theorem 7.5.5 appropriately.}
7.5.21 (Example 7.5.18 Continued) Let X1, ..., Xn be iid random vari-
ables distributed uniformly on (0, θ) where θ(> 0) is the unknown param-
eter with n ≥ 2. Show that 
  {Hint:
Try and use the Theorem 7.5.5 appropriately.}
7.6.1 (Example 7.6.1 Continued) Suppose that X1, ..., Xn are iid with
the common pdf f(x;θ) = θ-1 exp{ −(x − θ)/θ}I(x > θ) with θ ∈ Θ = ℜ+
where θ is the unknown parameter and n ≥ 2. Recall from Exercise 6.3.10
that 
 is minimal sufficient for θ but U is not
complete. Here, we wish to estimate θ unbiasedly.
(i)
Show that T = n(n + 1)-1 Xn:1 is an unbiased estimator for θ;
(ii)
Show that 
 is also an unbiased
estimator for θ;
(iii)
Along the lines of our discussions in the Section 7.6.1, derive
the Rao-Blackwellized versions W and W′ of the estimators T
and T′  respectively;
(iv)
Compare the variances of W and W′. Any comments?
(v)
Define a class of unbiased estimators of θ which are convex
combinations of W and W′ along the lines of (7.6.4). Within this
class find the estimator T* whose variance is the smallest;
(vi)
Summarize your comments along the lines of the remarks made
in (7.6.6).
7.7.1 Show that the estimators derived in the Examples 7.2.1-7.2.5 are
consistent for the parametric function being estimated.
7.7.2 Show that the MLE’s derived in the Examples 7.2.6-7.2.7 are con-
sistent for the parametric function being estimated.
7.7.3 Show that the MLE’s derived in the Examples 7.2.9-7.2.12 are con-
sistent for the parametric function being estimated.
7.7.4 Show that the UMVUE’s derived in the Section 7.5 are consistent for
parametric function being estimated.
7.7.5 Show that the estimators W, W′ and T*(α*) defined in (7.6.4)-(7.6.5)
are consistent for the parameter θ being estimated.

7. Point Estimation
393
7.7.6 (Exercises 7.2.10 and 7.5.4) Suppose that X1, ..., Xn are iid with the
Rayleigh distribution, that is the common pdf is
where θ(> 0) is the unknown parameter. Show that the MLE’s and the
UMVUE’s for θ, θ2 and θ-1 are all consistent.
7.7.7 (Exercises 7.2.11 and 7.5.6) Suppose that X1, ..., Xn are iid with the
Weibull distribution, that is the common pdf is
where α(> 0) is the unknown parameter but β(> 0) is assumed known. Show
that the MLE’s and the UMVUE’s for α, α2 and α-1 are all consistent.
7.7.8 (Exercises 7.2.4 and 7.2.17 Continued) Suppose that X1, ..., Xn are
iid whose common pdf is given by
where θ(> 0) is the unknown parameter. Show that the method of moment
estimator and the MLE for θ are both consistent.
7.7.9 Suppose that X1, ..., Xn are iid whose common pdf is given by
where µ, α are both assumed unknown with −∞ < µ < ∞ 0 < α < ∞, θθθθθ = (µ,
σ). One will recall from (1.7.27) that this pdf is known as the lognormal
density.
(i)
Evaluate the expression for 
 denoted by the parametric
function T(θ), for any fixed k(> 0);
(ii)
Derive the MLE, denoted by Tn, for T(θ);
(iii)
Show that Tn is consistent for T(θ).
7.7.10 Suppose that X1, ..., Xn are iid N(µ, θ2) where µ and σ are both
assumed unknown with µ ∈ ℜ, σ ∈ ℜ+, θθθθθ = (µ, σ), n ≥ 2. First find the
UMVUE T = Tn for the parametric function T(µ) = µ + µ2. Show that Tn is
consistent for T(µ).
7.7.11 (Exercise 7.5.4) Suppose that X1, ..., Xn are iid with the Rayleigh
distribution, that is the common pdf is

394
7. Point Estimation
where θ(> 0) is the unknown parameter. Let 
 with c > 0 and
consider estimating θ with Tn(c). First find the MSE of Tn(c) with c(> 0)
fixed. Then, minimize the MSE with respect to c. Denote the optimal choice
for c by 
  Show that the minimum MSE estimator Tn(c*) is consistent
for θ.
7.7.12 (Example 7.4.6 Continued) Let X1, ..., Xn be iid N(µ, σ2) where µ, σ
are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let T(µ) = Pµ{X1 < a} where
a is some known real number. First find the UMVUE T ≡ Tn for T(µ). Show
that Tn is consistent for T(µ).
7.7.13 (Example 7.4.6 Continued) Let X1, ..., Xn be iid N(µ, σ2) where µ, σ
are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let T(µ) = Pµ{|X1| < a} where
a is some known positive number. First find the UMVUE T ≡ Tn for T(µ).
Show that Tn is consistent for T(µ).
7.7.14 Suppose that X1, ..., Xn are iid Uniform(0, θ) where θ(> 0) is the
unknown parameter. Let Tn(c) = cXn:n with c > 0 and consider estimating θ
with Tn(c). First find the MSE of Tn(c) with c(> 0) fixed. Then, minimize the
MSE with respect to c. Denote the optimal choice for c by 
  Show
that the minimum MSE estimator Tn(c*) is consistent for θ.
7.7.15 (Example 7.7.13 Continued) Let X1, ..., Xn be iid N(µ, σ2) where µ,
σ are both unknown with µ ∈ ℜ, σ ∈ ℜ+, n ≥ 2. Let T(µ) = Pµ{|X1| < a} where
a is some known positive number. Obtain the expression for T(µ) and thereby
propose a consistent estimator Un for T(µ). But, Un must be different from
the UMVUE Tn proposed earlier in the Exercise 7.7.13. {Note: A consistent
estimator does not have to be unbiased.}

8
Tests of Hypotheses
8.1
Introduction
Suppose that a population pmf or pdf is given by f(x;θ) where x ∈ χ ⊆ ℜ and
θ is an unknown parameter which belongs to a parameter space Θ ⊆ ℜ.
Definition 8.1.1 A hypothesis is a statement about the unknown param-
eter θ.
In a problem the parameter θ may refer to the population mean and the
experimenter may hypothesize that θ  ≥ 100 and like to examine the plausibil-
ity of such a hypothesis after gathering the sample evidence. But, a formula-
tion of testing the plausibility or otherwise of a single hypothesis leads
to some conceptual difficulties. Jerzy Neyman and Egon S. Pearson discov-
ered fundamental approaches to test statistical hypotheses. The Neyman-
Pearson collaboration first emerged (1928a,b) with formulations and con-
structions of tests through comparisons of likelihood functions. These blos-
somed into two landmark papers of Neyman and Pearson (1933a,b).
Neyman and Pearson formulated the problem of testing of hypotheses as
follows. Suppose one is contemplating to choose between two plausible hy-
potheses.
where Θ0 ⊂ Θ, Θ1 ⊂ Θ and Θ0 ∩ Θ1 = ϕ, the empty set. Based on the evidence
collected from random samples X1, ..., Xn, obtained from the relevant popu-
lation under consideration, the statistical problem is to select one hypoth-
esis which seems more reasonable in comparison with the other. The experi-
menter may, for example, decide to opt for H0 compared with H1 based on
the sample evidence. But we should not interpret this decision to indicate that
H0 is thus proven to be true. An experimenter’s final decision to opt for the
hypothesis H0 (or H1) will simply indicate which hypothesis appears more
favorable based on the collected sample evidence. In reality it is possible,
however, that neither H0 nor H1 is actually true. The basic question is this:
Given the sample evidence, if one must decide in favor of H0 or H1, which
hypothesis is it going to be? This chapter will build the methods of such
decision-making.
We customarily refer to H0 and H1 respectively as the null and alterna-
tive hypothesis. The null hypothesis H0 or the alternative hypothesis H1 is
395

396
8. Tests of Hypotheses
called simple provided that Θ0, Θ1 are singleton subsets of Θ. That is, a
hypothesis such as H0 : θ = θ0 or H1 : θ = θ1, with θ0,θ1 known, would be
called a simple hypothesis. A hypothesis which is not simple is called compos-
ite. A hypothesis such as H0 : θ > θ0, H0 : θ < θ0, H1 : θ ≥ θ1, H1 : θ ≤ θ1, for
example, are referred to as one-sided composite hypotheses. A hypothesis
such as H1 : θ ≠ θ0 or H1 : θ ≤ θ0 ∪ θ ≥ θ1 with θ0 < θ1, for example,
is referred to as a two-sided composite hypothesis. In Section 8.2,
we first formulate the two types of errors in the decision making and focus
on the fundamental idea of a test. The Section 8.3 develops the concept of the
most powerful (MP) test for choosing between a simple null versus simple
alternative hypotheses. In Section 8.4, the idea of a uniformly most powerful
(UMP) test is pursued when H0 is simple but H1 is one-sided. The Section 8.5
gives examples of two-sided alternative hypotheses testing situations and ex-
amines the possibilities of finding the UMP test. Section 8.5 touches upon the
ideas of unbiased and uniformly most powerful unbiased (UMPU) tests.
8.2 Error Probabilities and the Power Function
It is emphasized again that H0 and H1 are two statements regarding the un-
known parameter θ. Then we gather around the random samples X1, ..., Xn
from the appropriate population and learn about the unknown value of θ.
Intuitively speaking, the experimenter would favor H0 or H1 if the estimator
 seems more likely to be observed under H0 or H1 respec-
tively. But, at the time of decision making, the experimenter may make mis-
takes by favoring the wrong hypothesis simply because the decision is based
on the evidence gathered from a random sample.
To understand the kinds of errors one can commit by choosing one hy-
pothesis over the other, the following table may be helpful. An examination
Table 8.2.1. Type I and Type II Errors
Test Result
Nature’s Choice
or Decision
H0 True
H1 True
Accept H0
No Error
Type II Error
Accept H1
Type I Error
No Error
of the entries reveals that the experimenter may commit one of the two
possible errors. One ends up rejecting the null hypothesis H0 while H0 is
actually true or ends up accepting the null hypothesis H0 while H1 is true.
Table 8.2.1 clearly suggests that the other two possible decisions are correct

8. Tests of Hypotheses
397
ones under given circumstances.
Once and for all, let us add that we freely interchange statements, for
example, “Reject Hi” and “Accept Hj” for i ≠ j ∈ {0, 1}.
Next, let us explain what we mean by a test of H0 versus H1. Having
observed the data X = (X1, ..., Xn), a test will guide us unambiguously to reject
H0 or accept H0. This is accomplished by partitioning the sample space ℜn
into two parts R and Rc corresponding to the respective final decision: “reject
H0” and “accept H0”
R is constructed so that we reject H0 whenever X ∈ R. The subset
R is called the rejection region or the critical region.
Example 8.2.1 Let us walk through a simple example. Consider a popula-
tion with the pdf N(θ, 1) where θ ∈ℜ is unknown. An experimenter postu-
lates two possible hypotheses H0 : θ = 5.5 and H1 : θ = 8. A random sample X
= (X1, ..., X9) is collected and denote 
. Some examples of
“tests” are given below:
We may summarize these tests in a different fashion: Let us rewrite
Here, Ri is that part of the sample space ℜ9 where H0 is rejected by means of
the Test #i, i = 1, ..., 4. !
Whenever H0, H1 are both simple hypotheses, we respectively write a and
ß for the Type I and II error probabilities:

398
8. Tests of Hypotheses
where R denotes a generic rejection region for H0.
Example 8.2.2 (Example 8.2.1 Continued) In the case of the Test #1,
writing Z for a standard normal variable, we have:
Proceeding similarly, we used MAPLE to prepare the following table for the
values of α and β associated with the Tests #1-4 given by (8.2.1).
Table 8.2.2. Values of a and ß for Tests #1-4 from (8.2.1)
Test #1
Test #2
Test #3
Test #4
R1
R2
R3
R4
α = .06681
α = .01696
α = .06681
α = .00000
β = .15866
β = .07865
β = .00000
β = .06681
Upon inspecting the entries in the Table 8.2.2, we can immediately conclude a
few things. Between the Tests #1 and #2, we feel that the Test #2 appears
better because both its error probabilities are smaller than the ones associated
with the Test #1. Comparing the Tests #1 and #3 we can similarly say that
Test #3 performs much better. In other words, while comparing Tests #1-3,
we feel that the Test #1 certainly should not be in the running, but no clear-cut
choice between Tests #2 and #3 emerges from this. One of these has a smaller
value of a but has a larger value of ß. If we must pick between the Tests #2-
3, then we have to take into consideration the consequences of committing
either error in practice. It is clear that an experimenter may not be able to
accomplish this by looking at the values of a and ß alone. Tests #3-4 point out
a slightly different story. By down-sizing the rejection region R for the Test #4
in comparison with that of Test #3, we are able to make the a value for Test
#4 practically zero, but this happens at the expense of a sharp rise in the value
of β. !
From Table 8.2.2, we observe some special features which also hold in
general. We summarize these as follows:
All tests may not be comparable among themselves such
as tests #2-3. By suitably adjusting the rejection region R,
we can make α (or ß) as small as we would like, but then
β (or a) will be on the rise as the sample size n is kept fixed.
So, then how should one proceed to define a test for H0 versus H1 which
can be called the “best”? We discuss the Neyman-Pearson formulation of the
testing problem in its generality in the next subsection.

8. Tests of Hypotheses
399
Let us, however, discuss few other notions for the moment. Sometimes
instead of focussing on the Type II error probability β, one considers what is
known as the power of a test.
Definition 8.2.1 The power or the power function of a test, denoted by
Q(θ), is the probability of rejecting the null hypothesis H0 when θ ∈ Θ is the
true parameter value. The power function is then given by
In a simple null versus simple alternative testing situation such as the one
we had been discussing earlier, one obviously has Q(θ0) = α and Q(θ1) =
1 - β.
Example 8.2.3 (Example 8.2.1 Continued) One can verify that for the
Test #4, the power function is given by Q(θ) = 1 - Φ (22.5 - 3θ) for all θ
∈ ℜ.
A test is frequently laid out as in (8.2.1) or (8.2.2). There is yet another
way to identify a test by what is called a critical function or test function.
Recall that proposing a test is equivalent to partitioning the sample space χn
into two parts, critical region R and its complement Rc.
Definition 8.2.2 A function Ψ (.) : χn → [0, 1] is called a critical function
or test function where ψ(X) stands for the probability with which the null
hypothesis H 0 is rejected when the data X = x has been observed,
x ∈ χ n.
In general, we can rewrite the power function defined in (8.2.4) as follows:
8.2.1
The Concept of a Best Test
In (8.2.3) we pointed out how Type I and II error probabilities are evaluated
when testing between a simple null and simple alternative hypotheses. When
the null and alternative hypotheses are composite, we refocus on the defini-
tion of the two associated error probabilities. Let us consider testing H0 : θ ∈
Θ0 versus H1 : θ ∈ Θ1 where Θ0 ⊂ Θ, Θ1 ⊂ Θ and Θ0 ∩ Θ1 = ϕ, the empty
set.
Definition 8.2.3 We start with a fixed number α ∈ (0, 1). A test for H0 : θ
∈ Θ0 versus H1 : θ ∈ Θ1 with its power function Q(θ) defined in (8.2.2) is
called size a or level a according as 
 Q(θ) = α or ≤ α respec-
tively.

400
8. Tests of Hypotheses
In defining a size α or level α test, what we are doing is quite simple. For
every θ in the null space Θ0, we look at the associated Type I error probability
which coincides with Q(θ), and then consider the largest among all Type I
error probabilities. 
 may be viewed as the worst possible Type I
error probability. A test is called size α if 
 = α whereas it is
called level α if  
 ≤ α. We may equivalently restrict our attention
to a class of test functions such that 
 or
≤ α. It should be obvious that a size a test is also a level α
test.
It is important to be able to compare all tests for H0 versus H1 such that
each has some common basic property to begin with. In Chapter 7, for ex-
ample, we wanted to find the best estimator among all unbiased estimators of
θ. The common basic property of each estimator was its unbiasedness. Now,
in defining the “best” test for H0 versus H1, we compare only the level a tests
among themselves.
Definition 8.2.4 Consider the collection C of all level α tests for H0 : θ ∈
Θ0 versus H1 : θ ∈ Θ1 where 
 A test belonging to C
with its power function Q(θ) is called the best or the uniformly most powerful
(UMP) level α test if and only if Q (θ) ≥ Q* (θ) for all θ ∈ Θ
 where Q* (θ)
is the power function of any other test belonging to C. If the alternative
hypothesis is simple, that is if Θ1 is a singleton set, then the best test is called
the most powerful (MP) level a test.
Among all level a tests our goal is to find the one whose power hits the
maximum at each point θ ∈ Θ1. Such a test would be UMP level α. In the
simple null versus simple alternative case first, Neyman and Pearson (1933a,b)
gave an explicit method to determine the MP level α test. This approach is
described in the next section. We end this section with an example.
Example 8.2.4 (Example 8.2.1 Continued) Consider a population with the
pdf N(θ, 1) where θ ∈ ℜ is unknown. Again an experimenter postulates two
hypotheses, H0 : θ = 5.5 and H1 : θ = 8. A random sample X = (X1, ..., X9) is
collected. Let us look at the following tests:
Let us write Qi(θ) for the power function of Test #i, i = 1, 2, 3. Using
MAPLE, we verified that Q1(5.5) = Q2(5.5) = Q3(5.5) = .049995. In other
words, these are all level a tests where α = .049995. In the Figure 8.2.1,
we have plotted these three power functions. It is clear from this plot that

8. Tests of Hypotheses
401
among the three tests under investigation, the Test #3 has the maximum power
at each point θ > 5.5.
Figure 8.2.1. Power Functions for Tests #1, #2 and #3
We may add that the respective power at the point θ = 8 is given by Q1(8)
= .80375, Q2(8) = .9996, and Q3(8) = 1.0. !
8.3
Simple Null Versus Simple Alternative
Hypotheses
Here we elaborate the derivation of the MP level a test to choose between a
simple null hypothesis H0 and a simple alternative hypothesis H1. We first
prove the celebrated Neyman-Pearson Lemma which was originally formu-
lated and proved by Neyman and Pearson (1933a).
8.3.1
Most Powerful Test via the Neyman-Pearson Lemma
Suppose that X1, ..., Xn are iid real valued random variables with the pmf or
pdf f(x; θ), θ ∈ Θ ⊆ ℜ. Let us continue to write X = (X1, ..., Xn), X = (x1, ...,
xn). The likelihood function is denoted by L(X; θ) when θ is the true value. We
wish to test
where θ0 ≠ θ1 but both θ0, θ1 ∈ Θ, the parameter space. Under Hi : θ = θi, the
data x has its likelihood function given by L(X; θi), i = 0, 1.
Let us focus our attention on comparing the powers associated with all
level a tests where 0 < α < 1 is preassigned. Customarily the number α

402
8. Tests of Hypotheses
is choosen small. In practice, one often chooses α = .10, .05 or .01 unless
otherwise stated. But the experimenter is free to choose any appropriate a
value.
Note that L(x; θi), i = 0, 1, are two completely specified likelihood func-
tions. Intuitively speaking, a test for H0 versus H1 comes down to the com-
parison of L(x; θ0) with L(x; θ1) and figure out which one is significantly
larger. We favor the hypothesis associated with the significantly larger likeli-
hood as the more plausible one. The following result gives a precise state-
ment.
Theorem 8.3.1 (Neyman-Pearson Lemma) Consider a test of H0 versus
H1 stated in (8.3.1) with its rejection and acceptance regions for the null
hypothesis H0 defined as follows:
or equivalently, suppose that the test function has the form
where the constant k(≥ 0) is so determined that
Any test satisfying (8.3.2)-(8.3.3) is a MP level α test.
Proof We give a proof assuming that the X’s are continuous random vari-
ables. The discrete case can be disposed off by replacing the integrals with
the corresponding sums. First note that any test which satisfies (8.3.3) has
size α and hence it is level a too.
We already have a level a test function ψ(x) defined by (8.3.2)-(8.3.3). Let
ψ*(x) be the test function of any other level α test. Suppose that Q(θ), Q*(θ)
are respectively the power functions associated with the test functions ψ, ψ*.
Now, let us first verify that
Suppose that x ∈ χn is such that ψ(x) = 1 which implies L(x; θ1) - kL(x;
θ0) > 0, by the definition of ψ in (8.3.2). Also for such x, one obviously
has ψ(x) - ψ*(x) ≥ 0 since ψ*(x) ∈ (0, 1). That is, if x ∈ χn is such that
χ (x) = 1, we have verified (8.3.4). Next, suppose that x ∈ θn is such
that χ (x) = 0 which implies L(x; θ1) - kL(x; θ0) < 0, by the definition

8. Tests of Hypotheses
403
of ψ in (8.3.2). Also for such x one obviously has ψ(x) - ψ*(x) ≤ 0 since
ψ*(x) ∈ (0, 1). Again (8.3.4) is validated.. Now, if x ∈ χn is such that 0 <
ψ(x) < 1, then from (8.3.2) we must have L(x; θ1) - kL(x; θ0) = 0, and
again (8.3.4) is validated. That is, (8.3.4) surely holds for all x ∈ θn. Hence
we have
Now recall that Q(θ0) is the Type I error probability associated with the test
ψ defined in (8.3.2) and thus Q(θ0) = α from (8.3.3). Also, Q*(θ0) is the
similar entity associated with the test ψ* which is assumed to have the level
α, that is Q*(θ0) ≤ α. Thus, Q(θ0) – Q*(θ0) ≥ 0 and hence we can rewrite
(8.3.5) as
which shows that Q(θ1) = Q*(θ1). Hence, the test associated with ψ is at least
as powerful as the one associated with ψ*. But, ψ* is any arbitrary level a test
to begin with. The proof is now complete.¾
Remark 8.3.1 Observe that the Neyman-Pearson Lemma rejects H0 in
favor of accepting H1 provided that the ratio of two likelihoods under H1 and
H0 is sufficiently large, that is if and only if L(X; θ1)/L(X; θ0) > k for some
suitable k(≥ 0).
Convention: The ratio c/0 is interpreted as infinity if c > 0
and one if c = 0.
Remark 8.3.2 In the statement of the Neyman-Pearson Lemma, note
that nothing has been said about the data points x which satisfy the equa-
tion L(x; θ1) = kL(x; θ0). First, if the X’s happen to be continuous random
variables, then the set of such points X would have the probability zero. In
other words, by not specifying exactly what to do when L(x; θ1) = kL(x;
θ0) in the continuous case amounts to nothing serious in practice. In a
discrete case, however, one needs to randomize on the set of X’s for
which L(x; θ1) = kL(x; θ0) holds so that the MP test has the size

404
8. Tests of Hypotheses
α. See the Examples 8.3.6-8.3.7.
A final form of the MP or UMP level a test is written down in the
simplest implementable form. That is, having fixed α ∈ (0, 1), the
cut-off point of the test defining the regions R, Rc must explicitly
be found analytically or from a standard statistical table.
Remark 8.3.3 We note that, for all practical purposes, the MP level α test
given by the Neyman-Pearson Lemma is unique. In other words, if one finds
another MP level α test with its test function ψ* by some other method, then
for all practical purposes, ψ* and ψ will coincide on the two sets {X ∈ χn :
L(x; θ1) > kL(x; θ0)} and {x ∈ χn : L(x; θ1) < kL(x; θ0)}.
Convention: k is used as a generic and nonstochastic constant.
k may not remain same from one step to another.
Example 8.3.1 Let X1, ..., Xn be iid N(µ, σ2) with unknown µ ∈ ℜ, but
assume that σ ∈ ℜ&plus; is known. With preassigned α ∈ (0, 1) we wish to
derive the MP level α test for H0 : µ = µ0 versus H1 : µ = µ1 where µ1 > µ0 and
µ0, µ1 are two known real numbers. Both H0, H1 are simple hypotheses and
the Neyman-Pearson Lemma applies. The likelihood function is given by
Figure 8.3.1. (a) Standard Normal PDF: Upper 100a% Point
(b) Probability on the right of za Is Larger Under H1 (darker plus
lighter shaded areas) than Under H0 (lighter shaded area)
The MP test will have the following form:
that is, we will reject the null hypothesis H0 if and only if

8. Tests of Hypotheses
405
Now since µ1 > µ0, the condition in (8.3.6) can be rephrased as:
Since Eµ[X] = µ, it does make sense to reject H0 when 
 is large (> k)
because the alternative hypothesis postulates a value µ1 which is larger than
µ0. But the MP test given in (8.3.7) is not yet in the implementable form and
the test must also be size a. Let us equivalently rewrite the same test as fol-
lows:
where zα is the upper 100α% point of the standard normal distribution. See
the Figure 8.3.1. The form of the test given in (8.3.7) asks us to reject H0 for
large enough values of 
 while (8.3.8) equivalently asks us to reject H0 for
large enough values of 
 Under H0, observe that 
is a statistic, referred to as the test statistic, which is distributed as a standard
normal random variable.
Here, the critical region 
.
Now, we have:
Thus, we have the MP level a test by the Neyman-Pearson Lemma. !
Example 8.3.2 (Example 8.3.1 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) with unknown µ ∈ ℜ, but σ ∈ ℜ+ is known. With preassigned α ∈
(0, 1) we wish to derive the MP level α test for H0 : µ = µ0 versus H1 : µ = µ1
where µ1 < µ0 and µ0, µ1 are two real numbers.
Figure 8.3.2. Standard Normal PDF: Lower 100α% Point

406
8. Tests of Hypotheses
Since both H0, H1 are simple hypotheses, the Neyman-Pearson Lemma
applies. The equation (8.3.6) will continue to hold but we now have µ1 – µ0 <
0. Thus the large values of 
 will correspond to the
small values of 
. In other words, the MP test would look like
this:
This simplifies to the following form of the MP level a test:
See the Figure 8.3.2. Since Eµ[X] = µ, it does make sense to reject H0 when
 is small because the alternative hypothesis postulates a value µ1 which is
smaller than µ0. Under H0, again observe that 
 is a statistic,
referred to as the test statistic, which has a standard normal distribution.
Here, the critical region 
– z
α.
Figure 8.3.3. The Shaded Area is a, the Type I Error Probability:
(a) R ≡ {t ∈ T : t > k} (b) R ≡ {t ∈ T : t < k}
Suppose that we wish to test H0 : θ = θ0 versus H1 : θ = θ1 at the level α
where θ1 > θ0 and θ0, θ1 are two known real numbers. In a number of prob-
lems, we may discover that we reject H0 when an appropriate test statistic T
exceeds some number k. This was the situation in the Example 8.3.1 where
we had 
 and k = zα. Here, the alternative hypothesis was
on the upper side (of µ0) and the rejection region R (for H0) fell on the upper
side too.
Instead we may wish to test H0 : θ = θ0 versus H1 : θ = θ1 at the level
α where θ1 < θ0 and θ0, θ1 are two known real numbers. In a number of

8. Tests of Hypotheses
407
problems, we may discover that we reject H0 when an appropriate test statis-
tic T falls under some number k. This was the situation in the Example 8.3.2
where we had 
 and k = -zα. Here, the alternative hypoth-
esis was on the lower side (of µ0) and the rejection region R (for H0) fell on
the lower side too.
In general, the cut-off point k has to be determined from the distribution
g(t), that is the pmf or pdf of the test statistic T under H0. The pmf or pdf of
T under H0 specifies what is called a null distribution. We have summarized
the upper- and lower-sided critical regions in the Figure 8.3.3.
Example 8.3.3 Suppose that X1, ..., Xn are iid with the common pdf b-1
exp(-x/b) for x ∈ ℜ+ with unknown b ∈ ℜ+. With preassigned α ∈ (0, 1), we
wish to obtain the MP level a test for H0 : b = b0 versus H1 : b = b1(> b0) where
b0, b1 are two positive numbers. Both H0, H1 are simple hypothesis and the
Neyman-Pearson Lemma applies. The likelihood function is given by
The MP test will have the following form:
that is, we will reject the null hypothesis H0 if and only if
Now since b1 > b0, the condition in (8.3.10) can be rephrased as:
Figure 8.3.4. Chi-Square Upper 100a% Point with
Degrees of Freedom 2n
But, the MP test described by (8.3.11) is not yet in the implementable
form. Under H0, observe that 
 are iid standard exponential random

408
8. Tests of Hypotheses
variables, so that 
 are iid 
. Hence, 
 is distributed as 
when H0 is true. This test must also be size a. Let us equivalently rewrite the
same test as follows:
where 
 is the upper 100a% point of the  
 distribution. See, for ex-
ample, the Figure 8.3.4. The test from (8.3.11) asks us to reject H0 for large
enough values of 
 whereas (8.3.12) asks us to reject H0 for large
enough values of 
 We call 
 the associated test statis-
tic. The order of “largeness”, that is the choice of k, depends on the normal-
ized test statistic and its distribution under H0. Under H0, since 
 is
distributed as a  
 random variable, we have:
Thus, we have the MP level a test. Here, the critical region R = {X =
!
Example 8.3.4 Suppose that X1, ..., Xn are iid having the Uniform(0, θ)
distribution with the unknown parameter θ (> 0). With preassigned α ∈ (0,
1), we wish to obtain the MP level α test for H0 : θ = θ0 versus H1 : θ = θ1(>
θ0) where θ0, θ1 are two positive numbers. Both H0, H1 are simple hypothesis
and the Neyman-Pearson Lemma applies. The likelihood function is given by
In view of the Remark 8.3.1, the MP test will have the following form:
that is, we will reject the null hypothesis H0 if and only if
Note that the MP test given by (8.3.13) is not in the implementable form.
Under H0, the pdf of the statistic T = Xn:n is given by ntn-1/θ n
0. Hence we can
determine k as follows:

8. Tests of Hypotheses
409
provided that we choose k = θ0(1 – α)1/n and implement test defined by
(8.3.13). Here, the critical region R = {X = (x1, ..., xn) ∈ ℜ+n : xn:n > θ0(1 –
α)1/n}.
In the Neyman-Pearson Lemma, we assumed that θ was real valued.
This assumption was not crucial. The unknown parameter can be
vector valued too. It is crucial that the likelihood function involves
no unknown component of ? under either hypothesis Hi, i = 0, 1
if θ is vector valued. Look at Example 8.3.5.
Example 8.3.5 Suppose that X1, ..., Xn are iid having the common pdf
b-δ[G(δ)]-1 xδ-1 exp(–x/b) with two unknown parameters (δ, b) ∈ ℜ+ × ℜ+, x ∈
ℜ+. With preassigned α ∈ (0, 1), we wish to obtain the MP level α test for H0
: (b = b0, δ = δ*) versus H1 : (b = b1, δ = δ*) where b1 > b0 are two positive
numbers and δ* is also a positive number. Both H0 and H1 are simple hypoth-
esis and hence the Neyman-Pearson Lemma applies. The likelihood function
is given by
The MP test will have the following form:
that is, we will reject the null hypothesis H0 if and only if
This test must also have size a. Observe that, under H0, the statistic 
3
has the Gamma (nδ*, b0) distribution which is completely known for fixed
values of n, δ*, b0 and α. Let us equivalently write the test as follows:
where gn,δ*, b0,a is the upper 100a% point of the Gamma(nδ*, b0) distribution.
In the Table 8.3.1, gn,δ*, b0,a values are given for α = .01, .05, b0 = 1,
Table 8.3.1. Selected Values of gnδ*, b0,α with b0 = 1
α = .05
α = .01
n = 2
n = 5
n = 6
n = 2
n = 5
n = 6
δ* = 2
1.3663
5.4254
6.9242
0.8234
4.1302
5.4282

δ* = 3
2.6130
9.2463
11.6340
1.7853
7.4767
9.6163

410
8. Tests of Hypotheses
n = 2, 5, 6 and δ* = 2, 3. Express the critical region explicitly. !
In a discrete case, one applies the Neyman-Pearson Lemma, but
employs randomization. Look at Examples 8.3.6-8.3.7.
In all the examples, we have so far dealt with continuous distributions only.
The reader should recall the Remark 8.3.2. If the X’s are discrete, then we
carefully use randomization whenever L(x; θ1) = kL(x; θ0). The next two
examples emphasize this concept.
Example 8.3.6 Suppose that X1, ..., Xn are iid Bernoulli(p) where p ∈ (0,
1) is the unknown parameter. With preassigned α ∈ (0, 1), we wish to derive
the MP level a test for H0 : p = p0 versus H1 : p = p1(> p0) where p0, p1 are two
numbers from the interval (0, 1). Both H0 and H1 are simple hypothesis and
hence the Neyman-Pearson Lemma applies. Then, writing
 the likelihood function is given by
The MP test will have the following form:
that is, we will reject the null hypothesis H0 if
Now since p1 > p0, we have [p1(1 - p0)]/[p0(1 - p1)] > 1. So, the “large values”
of the lhs in (8.3.16) correspond to the “large values” of 
 Hence, the
MP test defined in (8.3.16) can be rephrased as:
We may then write down the corresponding test function as follows:
where a positive integer k and γ ∈ (0, 1) are to be chosen in such a way that
the test has the size α. Observe that 
 has the Binomial(n, p0) distribu-
tion under H0. First, we determine the smallest integer k such that
 < α and let

8. Tests of Hypotheses
411
where one has
Now, with k and γ defined by (8.3.18), one can check that the Type I error
probability is
Thus, we have the MP level α test. If 
 then one rejects H0 with
probability γ. For example, if γ = .135, then consider three-digit random num-
bers 000, 001, ..., 134, 135, ..., 999 and look at a random number table to
draw one three-digit number. If we come up with one of the numbers 000,
001, ... or 134, then and only then H0 will be rejected. This is what is known
as randomization. The following table provides the values of k and γ for some
specific choices of n and α.
Table 8.3.2. Values of k and γ in the Bernoulli Case
n = 10 α = .10
n = 10 α = .05
p0
k
γ
p0
k
γ
.2
4
.763
.2
4
.195
.4
6
.406
.6
8
030
n = 20 α = .05
n = 25 α = .10
p0
k
γ
p0
k
γ
.3
9
.031
.5
16
.757
.5
14
.792
.6
18
.330
The reader should verify some of the entries in this table by using the expres-
sions given in (8.3.19). !
Example 8.3.7 Suppose that X1, ..., Xn are iid Poisson(λ) where λ ∈ ℜ+ is
the unknown parameter. With preassigned α ∈ (0, 1), we wish to derive the
MP level α test for H0 : λ = λ0 versus H1 : λ = λ1 (> λ0) where λ0, λ1 are two
positive numbers. Both H0 and H1 are simple hypothesis and the Neyman-
Pearson Lemma applies Then, writing 
the likeli-
hood function is given by
The MP test will have the following form:

412
8. Tests of Hypotheses
that is, we will reject the null hypothesis H0 if
Now since λ1 > λ0, the “large values” of the lhs in (8.3.20) correspond to the
“large values” of 
 Hence, the MP test defined by (8.3.20) can be
rephrased as:
We then write down the test function as follows:
where a positive integer k and γ ∈ (0, 1) are to be chosen in such a way that
the test has the size α. Observe that 
 has Poisson(nλ0) distribution
under H0. First, we determine the smallest integer value of k such that
 and let
where
Now, with k and γ defined by (8.3.22), one can check that the Type I error
probability is
Thus, we have the MP level a test. If  
 = k, then one would employ
appropriate randomization and reject H0 with probability γ. The following
table provides some values of k, γ for specific choices of n and α. The reader
Table 8.3.3. Values of k and γ in the Poisson Case
n = 10 α = .10
n = 10 α = .05
λ0
k
γ
λ0
k
γ
.15
3
.274
.15
4
.668
.30
5
.160
.35
7
.604
n = 20 α = .05
n = 25 α = .10
λ0
k
γ
λ0
k
γ
.40
13
.534
.28
10
.021
.50
15
.037
.40
14
.317

8. Tests of Hypotheses
413
should verify some of the entries in this table by using the expressions given
in (8.3.23). !
A MP α level a test always depends on (jointly) sufficient statistics.
In each example, the reader has noticed that the MP level α test always de-
pended only on the (jointly) sufficient statistics. This is not a coincidence.
Suppose that T = T(X1, ..., Xn) is a (jointly) sufficient statistic for θ. By the
Neyman factorization (Theorem 6.2.1), we can split the likelihood function as
follows:
where h(X) does not involve θ. Next, recall that the MP test rejects H0 : θ = θ0
in favor of accepting H1 : θ = θ1 for large values of L(X; θ1)/L(X; θ0). But, in
view of the factorization in (8.3.24), we can write
which implies that the MP test rejects H0 : θ = θ0 in favor of accepting H1 : θ
= θ1, for large values of g(T(x); θ1))/g(T(x); θ0)). Thus, it should not surprise
anyone to see that the MP tests in all the examples ultimately depended on the
(jointly) sufficient statistic T.
8.3.2 Applications: No Parameters Are Involved
In the statement of the Neyman-Pearson Lemma, we assumed that θ was a
real valued parameter and the common pmf or pdf was indexed by θ. We
mentioned earlier that this assumption was not really crucial. It is essential to
have a known and unique likelihood function under both H0, H1. In other
words, as long as H0, H1 are both simple hypothesis, the Neyman-Pearson
Lemma will provide the MP level α test explicitly Some examples follow.
Example 8.3.8 Suppose that X is an observable random variable with its
pdf given by f(x), x ∈ ℜ. Consider two functions defined as follows:
We wish to determine the MP level α test for
In view of the Remark 8.3.1, the MP test will have the following form:

414
8. Tests of Hypotheses
But, f1(x)/f0(x) = 4x-3/2 and hence the test given in (8.3.26) can be rewritten
as:
This test must also have the size a, that is we require:
so that k = 4α1/3. With k = 4α1/3, one would implement the MP level α test
given by (8.3.27). The associated power calculation can be carried out as
follows:
when k = 4α1/3. When a = .05, .01 and .001, the power is respectively .223606,
.1 and .031623. !
Example 8.3.9 (Example 8.3.8 Continued) Suppose that X1 and X2 are
observable random variables with the common pdf given by f(x), x ∈ ℜ.
Consider the two functions f0(x) and f1(x) defined in the Example 8.3.8. As in
the earlier example, we wish to determine the MP level α test for
The reader should check that the MP test has the following form:
Let us write F0(x) for the distribution function which corresponds to the pdf
f0(x) so that
Under H0, it is known that -2log{F0(Xi)}, i = 1, 2, is distributed as iid 
 One
may go back to the Example 4.2.5 in this context. The test defined via (8.3.28)
must also have size a, that is, we require:

8. Tests of Hypotheses
415
Thus, one implements the MP level a test as follows:
where 
 is the upper 100α% point of the 
 distribution. We leave out the
power calculation as an exercise. !
Example 8.3.10 Suppose that X is an observable random variable with the
pdf f(x), x ∈ ℜ. We wish to test
That is, to decide between the simple null hypothesis which specifies that X is
distributed as N(0, ½) versus the simple alternative hypothesis which speci-
fies that X has the Cauchy distribution. In view of the Remark 8.3.1, the MP
test will have the following form:
Now, we want to determine the points x for which the function (1 + x2)-1
exp(x2) becomes “large”. Let us define a new function, g(y) = (1 + y)-1 exp(y),
0 < y < 8. We claim that g(y) is increasing in y. In order to verify this claim, it
will be simpler to consider h(y) = log(g(y)) = y – log(1 + y) instead. Note that
 = y/(1 + y) > 0 for 0 < y < 8. That is, h(y) is increasing in y so that g(y)
is then increasing in y too, since the log operation is monotonically increasing.
Hence, the “large” values of (1 + x2)-1 exp(x2) would correspond to the “large”
values of x2 or equivalently, the “large” values of |x|. Thus, the MP test given
by (8.3.29) will have the following simple form:
This test must also have the size a, that is
so that we have 
 where zα/2 stands for the upper 50α% point of the
standard normal distribution. With this choice of k, one would implement the
MP level a test given by (8.3.30). For example, if a = .05, then zα/2 = 1.96 so
that k ≈ 1.385929. The associated power calculation can be carried out as
follows:

416
8. Tests of Hypotheses
since Cauchy(0, 1) pdf is symmetric around x = 0. Hence, the test’s power is
given by 1 – 2/π arctan(k) with 
 When α = .05, the power turns
out to be .39791. !
Look at the Exercises 8.3.8-8.3.10 and the Exercise 8.3.20.
8.3.3
Applications: Observations Are Non-IID
In the formulation of the Neyman-Pearson Lemma, it is not crucial that the
X’s be identically distributed or they be independent. In the general situation,
all one has to work with is the explicit forms of the likelihood function under
both simple hypotheses H0 and H1. We give two examples.
Example 8.3.11 Suppose that X1 and X2 are independent random variables
respectively distributed as N(µ, σ2) and N(µ, 4σ2) where µ ∈ ℜ is unknown
and σ ∈ ℜ&plus; is assumed known. Here, we have a situation where the X’s are
independent but they are not identically distributed. We wish to derive the MP
level a test for H0 : µ = µ0 versus H1 : µ = µ1(> µ0). The likelihood function is
given by
where x = (x1,x2). Thus, one has
In view of the Neyman-Pearson Lemma, we would reject H0 if and only if
L(x; µ1)/L(x; µ0) is “large”, that is when 4X1 + X2 > k. This will be the MP
level α test when k is chosen so that Pµ0{4X1 + X2 > k} = α. But, under H0,
the statistic 4X1 + X2 is distributed as N(5µ0, 20σ2). So, we can rephrase the
MP level a test as follows:
We leave out the power calculation as Exercise 8.3.15. !
Example 8.3.12 Let us denote X’ = (X1, X2) where X is assumed to have
the bivariate normal distribution, 
 with the unknown param-
eter µ ∈ ℜ. Refer to Section 3.6 for a review of the bivariate normal distribu-
tion. We wish to derive the MP level a test for H0 : µ = µ0 versus H1 : µ = µ1(>
µ0). The likelihood function is given by
Thus, one has

8. Tests of Hypotheses
417
In view of the Neyman-Pearson Lemma, we would reject H0 if and only if
L(x; µ1)/L(x; µ0) is “large”, that is when X1 + X2 > k. This will be the MP level
α test when k is chosen so that Pµ0{X1 + X2 > k} = α. But, under H0, the
statistic X1 &plus; X2 is distributed as 
 So, we can rephrase
the MP level a test as follows:
We leave out the power calculation as Exercise 8.3.17. !
Look at the related Exercises 8.3.16, 8.3.18 and 8.3.21.
8.4
One-Sided Composite Alternative Hypothesis
So far we have developed the Neyman-Pearson methodology to test a simple
null hypothesis versus a simple alternative hypothesis. Next suppose that we
have a simple null hypothesis H0 : θ = θ0, but the alternative hypothesis is H1 :
θ > θ0. Here, H1 : θ > θ0 represents an upper-sided composite hypothesis. It is
conceivable that the alternative hypothesis in another situation may be H1 : θ <
θ0 which is a lower-sided composite hypothesis.
We recall the concept of the UMP level a test of H0 versus H1, earlier laid
out in the Definition 8.2.4. A test with its power function Q(θ) would be
called UMP level a provided that (i) Q(θ0) ≤ α and (ii) Q(θ) is maximized at
every point θ satisfying H1.
In other words, a test would be UMP level a provided that it has level α
and its power Q(θ) is at least as large as the power Q*(θ) of any other level a
test, at every point θ > θ0 or θ < θ0, as the case may be. The following
subsections lay down a couple of different approaches to derive the UMP
level θ tests.
8.4.1
UMP Test via the Neyman-Pearson Lemma
This method is very simple. We wish to obtain the UMP level θ test for H0
: θ = θ0 versus H1 : θ > θ0. We start by fixing an arbitrary value θ1 ∈ T such
that θ1 > θ0. Now, we invoke the Neyman-Pearson Lemma to conclude
that there exists a MP level a test for deciding between the choices of two
simple hypotheses θ0 and θ1. If this particular test, so determined, hap-
pens to remain unaffected by the choice of the specific value θ1, then by
the Definition 8.2.4, we already have on hand the required UMP level

418
8. Tests of Hypotheses
α test for H0 versus H1.
How can one prove this claim? One may argue as follows.
Suppose that Q(θ) is the power function of the MP level α test between θ0
and some fixed θ1(> θ0). Suppose that there exists a level θ test with its power
function Q*(θ) for choosing between θ0 and some fixed θ* (> θ0) such that
Q*(θ*) > Q(θ*). But, we are working under the assumption that the MP level
α test between θ0 and θ1 is also the MP level α test between θ0 and θ*, that is
Q(θ*) is the maximum among all level α tests between θ0 and θ*. This leads
to a contradiction.
The case of a lower-sided composite alternative is handled analogously.
Some examples follow.
Example 8.4.1 (Example 8.3.1 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) with unknown µ ∈ ℜ, but assume that σ ∈ ℜ+ is known. With
preassigned α ∈ (0, 1), we wish to obtain the UMP level a test for H0 : µ = µ0
versus H1 : µ > µ0 where µ0 is a real number. Now, fix a value µ1(> µ0) and
then from (8.3.8) recall that the MP level a test between µ0 and arbitrarily
chosen µ1 will have the following form:
where zα is the upper 100α% point of the standard normal distribution. See
the Figure 8.3.1. Obviously this test does not depend on the specific choice of
µ1(> µ0). Hence, the test given by (8.4.1) is UMP level α for testing H0 : µ =
µ0 versus H1 : µ > µ0. !
Figure 8.4.1. Chi-Square Lower 100a% or Upper 100(1 - α)%
Point with Degrees of Freedom n
Example 8.4.2 Let X1, ..., Xn be iid N(0, σ2) with unknown σ ∈ ℜ+.
With preassigned α ∈ (0, 1), we wish to obtain the UMP level α test for
H0 : σ = σ0 versus H1 : σ < σ0 where σ0 is a positive number. Now, fix a

8. Tests of Hypotheses
419
value σ1(< σ0) and then verify that the MP level a test between σ0 and arbi-
trarily chosen σ1 will have the following form:
where 
 is the lower 100α% point of the 
 distribution. See, for ex-
ample, the Figure 8.4.1. Obviously this test does not depend on the specific
choice of σ1(< σ0). Hence, the test given by (8.4.2) is UMP level a for testing
H0 : σ = σ0 versus H1 : σ < σ0. !
Example 8.4.3 (Example 8.3.6 Continued) Suppose that X1, ..., Xn are iid
Bernoulli(p) where p ∈ (0, 1) is the unknown parameter. With preassigned α ∈
(0, 1), we wish to obtain the UMP level α test for H0 : p = p0 versus H1 : p >
p0 where p0 is a number from the interval (0, 1). Now, fix a value p1(> p0) and
then from (8.3.17) recall that the test function associated with the MP level a
test between p0 and arbitrarily chosen p1 will have the following form:
where k, a positive integer, and γ ∈ (0, 1) are chosen so that the test described
by (8.4.3) has size α. Obviously the test does not depend on the specific
choice of p1(> p0). Hence, the test given by (8.4.3) is UMP level α for testing
H0 : p = p0 versus H1 : p > p0. !
One can similarly argue that the MP level α tests derived in the Examples
8.3.2-8.3.5 are also the UMP level α tests for the same simple null hypothesis
versus the corresponding upper- or lower-sided composite alternative hy-
pothesis. Verifications of such claims are left out as exercises.
The p-value of a test:
In tests of hypothesis, one often reports the final result of the test, that is
whether one accepts H0 or H1 at a chosen level α. There is, however, another
way to report the final result in practice using the notion called the p-value.
We emphasize that the p-value is a fully data driven quantification of “error”
and its interpretation is sometimes difficult.
Consider the Example 8.4.1. Suppose that we have the observed value 
for the sample mean and we have 
  the calculated value
of the test statistic, 
Now, we may ask the following question:
What is the probability of observing a data which is similar to
the one on hand or more extreme, if H0 happens to be true?
This probability will be called the p-value.

420
8. Tests of Hypotheses
Since the alternative hypothesis is µ > µ0, here the phrase more extreme is
interpreted as “
 > ” and hence the p-value is given by
which can be evaluated using the standard normal table.
If the alternative hypothesis is µ < µ0, then the phrase more extreme will be
interpreted as “
 > ” and hence the p-value is going to be
This probability can again be evaluated using the standard normal table.
A test with a “small” p-value indicates that the null hypothesis is less
plausible than the alternative hypothesis and in that case H0 is rejected.
A test would reject the null hypothesis H0 at the chosen level α
if and only if the associated p-value < α.
Example 8.4.4 (Example 8.4.1 Continued) Suppose that X1, ..., X15 are iid
N(µ, σ2) with unknown µ ∈ ℜ but s2 = 9. We wish to test H0 : µ = 3.1 versus
H1 : µ > 3.1 at the 5% level. Now, suppose that the observed data gave  =
5.3 so that 
 Here, we have zα = 1.645
and hence according to the rule given by (8.4.1), we will reject H0 at the 5%
level since zcalc > zα.
On the other hand, the associated p-value is given by P{Z > 2.8402} ≈
2.2543 × 10–3. If we were told that the p-value was 2.2543 × 10–3 in the first
place, then we would have immediately rejected H0 at any level α > 2.2543 ×
10–3.
8.4.2
Monotone Likelihood Ratio Property
We now define the monotone likelihood ratio (MLR) property for a family of
pmf or pdf denoted by f(x; θ), θ ∈ Θ ⊆ ℜ. In the next subsection, we exploit
this property to derive the UMP level α tests for one-sided null against one-
sided alternative hypotheses in some situations. Suppose that X1, ..., Xn are iid
with a common distribution f(x; θ) and as before let us continue to write X =
(X1, ..., Xn), x = (x1, ..., xn).
Definition 8.4.1 A family of distributions {f(x; θ): θ ∈ Θ} is said to have
the monotone likelihood ratio (MLR) property in a real valued statistic T =
T(X) provided that the following holds: for all {θ*, θ} ⊂ Θ and x ∈ χ, we
have

8. Tests of Hypotheses
421
Remark 8.4.1 In this definition, all we need is that L(x; θ*)/L(x; θ) should
be non-increasing or non-decreasing in T(x) whenever θ* > θ. If the likeli-
hood ratio L(x; θ*)/L(x; θ) is non-increasing instead of being non-decreas-
ing, then we shall see later that its primary effect would be felt in placing the
rejection region R in the upper or lower tail of the distribution of the test
statistic under H0. One should realize that the statistic T would invariably be
sufficient for the unknown parameter θ.
Example 8.4.5 Suppose that X1, ..., Xn are iid N(µ, σ2) with unknown µ ∈
ℜ, but assume that σ ∈ ℜ+ is known. Consider arbitrary real numbers µ, µ*
(> µ), and then with 
 let us write
which is increasing in T. Then, we have the MLR property in T. !
Example 8.4.6 Suppose that X1, ..., Xn are iid with the common pdf f(x;b)
= 
 (x > 0) with unknown b ∈ ℜ+. Consider arbitrary real numbers b,
b* (> b), and 
 with let us write
which is increasing in T. Then, we have the MLR property in T. !
Example 8.4.7 Suppose that X1, ..., Xn are iid Uniform(0, θ) with un-
known θ ∈ ℜ+. Consider arbitrary real numbers θ, θ* (> θ), and with T(x) =
xn:n, the largest order statistic, let us write
which is non-decreasing in T(x). Then, we have the MLR property in T. !
Next let us reconsider the real valued sufficient statistic T = T(X) for the
unknown parameter θ, and suppose that its family of pmf or pdf is given by
{g(t; θ): θ ∈ Θ ⊆ ℜ}. Here the domain space for T is indicated by t ∈ T ⊆ ℜ.
Suppose that g(t; θ) belongs to the one-parameter exponential family, given
by the Definition 3.8.1. That is, we can express
where b(θ) is an increasing function of θ. Then, the family {g(t; θ): θ ∈ Θ
⊆ ℜ} has the MLR property in T. Here, a(θ) and b(θ) can not involve t
whereas c(t) can not involve θ. In many distributions involving only a
single real valued unknown parameter θ, including the binomial, Poisson,

422
8. Tests of Hypotheses
normal, and gamma, the family of pmf or pdf of the associated sufficient
statistic T would be MLR in T. We leave out the verifications as exercises.
Some families of distributions may not enjoy the MLR property.
The Exercise 8.4.6 gives an example.
8.4.3
UMP Test via MLR Property
The following useful result is due to Karlin and Rubin (1956). We simply state
the result without giving its proof.
Theorem 8.4.1 (Karlin-Rubin Theorem) Suppose that we wish to
test H0 : θ = θ0 versus H1 : θ > θ0. Consider a real valued sufficient statistic
T = T(X) for θ ∈ Θ (⊆ ℜ). Suppose that the family {g(t; θ) : θ ∈ Θ} of the
pdf’s induced by T has the MLR (non-decreasing) property. Then, the test
function
corresponds to the UMP level a test if k is so chosen that 
If the null hypothesis in the Karlin-Rubin Theorem is replaced by
H0: θ ≤ θ0, the test given by (8.4.6) continues to be UMP level α.
Remark 8.4.2 Suppose that the null hypothesis in Theorem 8.4.1 is re-
placed by H0 : θ ≤ θ0. Recall that the level of the test is defined by
 One may, however, note that the maximum Type
I error probability is attained at the boundary point θ = θ0 because of the MLR
(non-decreasing) property in T. If the distribution of T happens to be discrete,
then the theorem continues to hold but it will become necessary to randomize
on the set {T = k} as we did in the Examples 8.3.6-8.3.7. In the Karlin-Rubin
Theorem, if the family {f(x; θ) : θ ∈ Θ} has instead the MLR non-increasing
property in T(X), then the UMP level α test function ψ(X) would instead be 1
or 0 according as T(X) < k or T(X) > k respectively.
Example 8.4.8 (Example 8.4.1 Continued) Suppose that X1, ..., Xn are
iid N(µ, σ2) with unknown µ ∈ ℜ, but assume that σ ∈ ℜ&plus; is known. Let
 which is sufficient for µ and its pdf has the MLR increas-
ing property in T. One may use the representation in (8.4.5) and the remark
about the one-parameter exponential family. Now, we wish to test the null
hypothesis H0 : µ ≤ µ0 versus H1 : µ > µ0 with level α where µ0 is a fixed

8. Tests of Hypotheses
423
real number. In view of the Karlin-Rubin Theorem and Remark 8.4.2, the
UMP level α test will look like this:
or equivalently it can be written as
One will note that the Type I error probability at the boundary point µ = µ0 in
the null space is exactly α. One may also check directly that the same for any
other µ < µ0 is smaller than α as follows: Writing Z for a standard normal
variable, for µ < µ0, we get
Since 
, we can now conclude that P{Z > za +
!
Example 8.4.9 Suppose that X1, ..., Xn are iid N(µ, σ2) with known µ ∈ ℜ,
but assume that σ ∈ ℜ+ is unknown. Let 
 and observe
that σ-2T(X) is distributed as the 
 random variable. Thus, the pdf of T,
which is a sufficient statistic for σ, has the MLR increasing property in T.
One may use (8.4.5) and the remark about the one-parameter exponential
family. We wish to test the null hypothesis H0 : σ ≤ σ0 versus H1 : σ > σ0 with
level α where σ0 is a fixed positive real number. In view of the Karlin-Rubin
Theorem and Remark 8.4.2, the UMP level α test will look like this:
or equivalently it can be written as
where recall that 
 is the upper 100α % point of the 
 distribution. See,
for example, the Figure 8.3.4. One will note that the Type I error probability at
the boundary point σ = σ0 in the null space is exactly α. One may also check
directly that the same for any other σ < σ0 is smaller than α as follows: For σ
< σ0, we get

424
8. Tests of Hypotheses
Since 
 we claim that 
Example 8.4.10 Suppose that X1, ..., Xn are iid with the common pdf
f(x; λ) = λe-?x I (x > 0) with unknown λ ∈ ℜ+. Let us consider 
which is sufficient for λ and its pdf has the MLR decreasing property in T. We
wish to test the null hypothesis H0 : λ ≤ λ0 versus H1 : λ > λ0 with level α
where λ0 is a fixed positive real number. In view of the Karlin-Rubin Theorem
and Remark 8.4.2, the UMP level a test will look like this:
or equivalently it can be written as
where recall that 
 is the lower 100α% point of the 
 distribution.
See, for example, the Figure 8.4.1. One will note that the Type I error prob-
ability at the boundary point λ = λ0 in the null space is exactly α. One may also
check directly, as in the two previous examples, that the same for any other
λ < λ0 is smaller than α. Note that for small values of 
 we are rejecting
the null hypothesis H0 : λ ≤ λ0 in favor of the alternative hypothesis H1 :
λ > λ0. This is due to the fact that we have the MLR decreasing property in T
instead of the increasing property. Also, observe that Eλ(X) = λ-1 and thus
“small” values of 
 would be associated with “large” values of ? which
should lead to the rejection of H0. Recall Remark 8.4.2 in this context. !
Example 8.4.11 Suppose that X1, ..., Xn are iid Uniform(0, θ) with un-
known θ ∈ ℜ+. Let us consider T(X) = Xn:n which is sufficient for θ and its
pdf has the MLR increasing property in T. We wish to test the null hypothesis
H0 : θ ≤ θ0 versus H1 : θ > θ0 with level α where θ0 is a fixed positive real
number. In view of the Karlin-Rubin Theorem and Remark 8.4.2, the UMP
level a test will look like this:
Let us now choose k so that the Type I error probability at the boundary point
θ = θ0 in the null space is exactly α. The pdf of T is given by ntn-1 θ-n I (0 < t
< θ) when θ is the true value. So, we proceed as follows.

8. Tests of Hypotheses
425
so that k = (1 – α)1/nθ0. One may also check directly that the Type I error
probability for any other θ < θ0 is smaller than α. !
Sample size determination is important in practice when we wish to
control both Type I and II error probabilities. See Exercise 8.4.25.
8.5
Simple Null Versus Two-Sided Alternative
8.5
Hypotheses
Consider testing a simple null hypothesis H0 : θ = θ0 versus a two-sided alter-
native hypothesis H1 : θ ≠ θ0 where θ0 is a fixed value in the parameter space
Θ. Will there exist a UMP level α test? The answer is “yes” in some situations
and “no” in some others. We refrain from going into general discussions of
what may or may not happen when the alternative hypothesis is two-sided.
In the case of the N(µ, σ2) distribution with µ unknown but σ known, a
UMP level α test fails to exist for deciding between a simple null hypothesis
H0 : µ = µ0 and a two-sided alternative hypothesis H1 : µ ≠ µ0. On the other
hand, for the Uniform(0, θ) distribution with θ unknown, a UMP level θ test
exists for deciding between a simple null hypothesis H0 : θ = θ0 and a two-
sided alternative hypothesis H1 : θ ≠ θ0. In the next two subsections, we
provide the details.
8.5.1
An Example Where UMP Test Does Not Exist
Suppose that X1, ..., Xn are iid N(µ, σ2) with unknown µ ∈ ℜ but known σ ∈
ℜ+. Consider the statistic 
 which is sufficient for µ. We wish
to test the simple null hypothesis H0 : µ = µ0 against the two-sided alternative
H1 : µ ≠ µ0 with level α where µ0 is a fixed real number. We wish to show that
there exists no UMP level α test in this situation.
Suppose that there is a UMP level α test and let its test function be denoted
by ψ* (X). Observe that ψ* (X) is then a UMP level α test for deciding
between H0 : µ = µ0 versus H1 : µ > µ0. In the Example 8.4.1, however, the
UMP level a for deciding between H0 versus H’1 was written as
where zα is the upper 100α% point of the standard normal distribution. See,
for example, the Figure 8.3.1. The two test functions ψ* and ψ1 must coin-
cide on the sets where ψ1 is zero or one. One can similarly show that the

426
8. Tests of Hypotheses
test function corresponding to the UMP level α test for deciding between H0
: µ = µ0 versus 
 can be written as
But, ψ* is also a UMP level a test for deciding between H0 versus H”1. Again,
the two test functions ψ* and ψ2 must also then coincide whenever ψ2 is zero
or one. Now, we argue as follows.
Suppose that the observed data X is such that the test statistic’s cal-
culated value 
 does not exceed –zα. Then, for such X, we
must have ψ1 (x) = 0, ψ2(x) = 1. That is, on the part of the sample space
where 
 the test function ψ*(X) will fail to coincide
with both ψ1(x), ψ2(x). So, we have arrived at a contradiction. In other
words, there is no UMP level a test for deciding between H0 : µ = µ0
against H1 : µ ≠ µ0.
8.5.2
An Example Where UMP Test Exists
Suppose that X1, ..., Xn are iid Uniform(0, θ) with unknown θ ∈ ℜ+. Consider
T(X) = Xn:n, the largest order statistic, which is sufficient for θ. We wish to
test the simple null hypothesis H0 : θ = θ0 against the two-sided alternative
hypothesis H1 : θ ≠ θ0 with level α where θ0 is a fixed positive number. We will
show that there exists a UMP level α test in this situation. This is one of many
celebrated exercises from Lehmann (1986, p. 111).
As a follow-up of the earlier Example 8.4.11, let us first show that any test
function ψ*(X) such that
corresponds to a UMP level a test for deciding between H’0 : θ ≤ θ0 and H’1 :
θ  > θ0.
Using the MLR property, from the Example 8.4.11, one may write down
the test function of the UMP level a test for H’0 versus H’1 as follows:
Now, for any ψ > ψ0, the power function associated with the test function

8. Tests of Hypotheses
427
ψ*0 is given by
Of course, the test function ψ* corresponds to a level α test for H’0
versus H’1. Let us now compute the power function associated with ψ* and
show that its power coincides with the expression given in (8.5.5). Let us
write g(t) = E[ψ*(X) | T(X) = t] which can not depend upon θ because T is
sufficient for θ and thus we have
Now, ψ*(x) = 1 for any t > θ0 hence g(t) = 1 if t > θ0. Thus, for any θ > θ0,
we can write
This last expression is the same as in (8.5.5). So, any test function ψ* defined
via (8.5.3) is UMP level a for testing H’0 versus H’0. Now we are in position
to state and prove the following result.
Theorem 8.5.1 In the Uniform(0, θ) case, for testing the simple null hy-
pothesis H0 : θ  = θ0 against the two-sided alternative H1 : θ  ≠ θ0 where θ0 is
a fixed positive number, the test associated with
is UMP level α.
Proof First note that since Pθ0{T(X) ≥ θ0} = 0, one obviously has
Along the lines of the Example 8.4.11, we can easily show that a UMP level a
test for deciding between H0 : θ = θ0 and 
 would have its test
function as follows:

428
8. Tests of Hypotheses
One can use the test function ψ(X) for testing H0 versus 
 too. In
view of (8.5.7), we realize that ψ(X) has level α and so for θ < θ0 the two test
functions 
 must coincide. That is, ψ(X) corresponds to a UMP
level α test for H0 versus 
 Next, by comparing the form of ψ(X)
described in the statement of the theorem with ψ*(X) given in (8.5.3), we
can claim that ψ(X) also corresponds to a UMP level α test for H0 versus
 Hence the result follows.¾
8.5.3
Unbiased and UMP Unbiased Tests
It should be clear that one would prefer a UMP level a test to decide between
two hypotheses, but the problem is that there may not exist such a test. Since,
the whole class of level α tests may be very large in the first place, one may
be tempted to look at a sub-class of competing tests and then derive the best
test in that class. There are, however, different ways to consider smaller
classes of tests. We restrict our attention to the unbiased class of tests.
Definition 8.5.1 Consider testing H0 : θ ∈ Θ0 versus H1 : θ ∈ Θ1 where Θ0,
Θ1 are subsets of Θ and Θ0, Θ1 are disjoint. A level a test is called unbiased if
Q(θ), the power of the test, that is the probability of rejecting H0, is at least α
whenever θ ∈ Θ1.
Thus, a test with its test function ψ(X) would be called unbiased level a
provided the following two conditions hold:
The properties listed in (8.5.9) state that an unbiased test rejects the null
hypothesis more frequently when the alternative hypothesis is true than in a
situation when the null hypothesis is true. This is a fairly minimal demand on
any reasonable test used in practice.
Next, one may set out to locate the UMP level a test within the class of
level a unbiased tests. Such a test is called the uniformly most powerful unbi-
ased (UMPU) level a test. An elaborate theory of unbiased tests was given by
Neyman as well as Neyman and Pearson in a series of papers. The readers
will find a wealth of information in Lehmann (1986).
When testing a simple null hypothesis H0 : θ = θ0 versus a simple alterna-
tive H1 : θ = θ1, the Neyman-Pearson Lemma came up with the MP level a test
implemented by the test function ψ(x) defined by (8.3.2). Now, consider
another test function ψ*(x) ≡ α, that is this randomized test rejects H0 with
probability a whatever be the observed data x ∈ χn.

8. Tests of Hypotheses
429
Note that ψ* corresponds to a level a test and hence the power associated
with ψ must be at least as large as the power associated with ψ* since ψ is
MP level a. Thus,
In other words, the MP level a test arising from the Neyman-Pearson Lemma
is in fact unbiased.
Remark 8.5.1 In the Section 8.5.1, we had shown that there was no
UMP level a test for testing a simple null against the two-sided alternative
hypotheses about the unknown mean of a normal population with its variance
known. But, there is a good test available for this problem. In Chapter 11, we
will derive the likelihood ratio test for the same hypotheses testing problem.
It would be shown that the level a likelihood ratio test would
This coincides with the customary two-tailed test which is routinely used in
practice. It will take quite some effort to verify that the test given by (8.5.10)
is indeed the UMPU level α test. This derivation is out of scope at the level of
this book. The readers nonetheless should be aware of this result.
8.6
Exercises and Complements
8.2.1 Suppose that X1, X2, X3, X4 are iid random variables from the N(θ, 4)
population where θ(∈ ℜ) is the unknown parameter. We wish to test H0 : θ =
2 versus H1 : θ = 5. Consider the following tests:
Find Type I and Type II error probabilities for each test and compare the
tests.
8.2.2 Suppose that X1, X2, X3, X4 are iid random variables from a popula-
tion with the exponential distribution having unknown mean θ (∈ ℜ+). We
wish to test H0 : θ = 4 versus H1 : θ = 8. Consider the following tests:

430
8. Tests of Hypotheses
Find Type I and Type II error probabilities for each test and compare the
tests.
8.2.3 Suppose that X1, X2, X3, X4 are iid random variables from the N(θ, 4)
population where θ(∈ ℜ) is the unknown parameter. We wish to test H0 : θ =
3 versus H1 : θ = 1. Consider the following tests:
Find Type I and Type II error probabilities for each test and compare the
tests.
8.2.4 Suppose that X1, X2, X3, X4 are iid random variables from a popula-
tion with the exponential distribution having unknown mean θ(∈ ℜ+). We
wish to test H0 : θ = 6 versus H1 : θ = 2. Consider the following possible
tests:
Find Type I and Type II error probabilities for each test and compare the
tests.
8.2.5 Suppose that X1, X2 are iid with the common pdf
where θ(> 0) is the unknown parameter. In order to test the null hypothesis
H0 : θ = 1 against the alternative hypothesis H1 : θ = 2, we propose the critical
region 
(i)
Show that the level α = ¼ + ¾log(¾);
(ii)
Show that power at θ = 2 is 7/16 + 9/8log(¾).
{Hints: Observe that 
 which is written
as 
 Similarly, power

8. Tests of Hypotheses
431
8.2.6 Suppose that X1, ..., X10 are iid N(θ, 4) where θ(∈ ℜ) is the unknown
parameter. In order to test H0 : θ = –1 against H1 : θ = 1, we propose the
critical region 
 Find the level α and evaluate
the power at θ = 1.
8.3.1 Suppose that X1, ..., Xn are iid random variables from the N(µ, σ2)
population where µ is assumed known but σ is unknown, µ ∈ ℜ, σ ∈ ℜ+. We
fix a number α ∈ (0, 1) and two positive numbers σ0, σ1.
(i)
Derive the MP level a test for H0 : σ = σ0 versus H1 : σ = σ1 (> σ0)
in the simplest implementable form;
(ii)
Derive the MP level α test for H0 : σ = σ0 versus H1 : σ = σ1 (< σ0)
in the simplest implementable form;
In each part, draw the power function.
8.3.2 (Example 8.3.4 Continued) Suppose that X1, ..., Xn are iid having the
Uniform(0, θ) distribution with unknown θ(> 0). With preassigned α ∈ (0, 1)
and two positive numbers θ1 < θ0, derive the MP level a test for H0 : θ = θ0
versus H1 : θ = θ1 in the simplest implementable form. Perform the power
calculations.
8.3.3 (Example 8.3.5 Continued) Suppose that X1, ..., Xn are iid with the
common pdf b-δ[Γ(δ)]-1xδ-1exp(-x/b), with two unknown parameters (δ, b) ∈
ℜ+2. With preassigned α ∈ (0, 1), derive the MP level α test, in the simplest
implementable form, for H0 : (b = b0, δ = δ*) versus H1 : (b = b1, δ = δ*)
where b1 < b0 are two positive numbers and δ* is a positive number.
8.3.4 Suppose that X1, ..., Xn are iid random variables from the N(µ, σ2)
population where µ is unknown but σ is assumed known, µ ∈ ℜ, σ ∈ ℜ+. In
order to choose between the two hypotheses H0 : µ = µ0 versus H1 : µ = µ1(>
µ0), suppose that we reject H0 if and only if 
 where c is a fixed number.
Is there any α ∈ (0, 1) for which this particular test is MP level α?
8.3.5 (Exercise 8.3.2 Continued) Suppose that X1, ..., Xn are iid having
the Uniform(0, θ) distribution with unknown θ (> 0). In order to choose
between the two hypotheses H0 : θ = θ0 versus H1 : θ = θ1 where θ1 < θ0 are
two positive numbers, suppose that we reject H0 if and only if Xn:n < c where
c is a fixed positive number. Is there any α ∈ (0, 1) for which this test is MP
level α ?
8.3.6 (Example 8.3.6 Continued) Suppose that X1, ..., Xn are iid ran-
dom variables having the Bernoulli(p) distribution where p ∈ (0, 1) is the

432
8. Tests of Hypotheses
unknown parameter. With preassigned α ∈ (0, 1), derive the randomized MP
level α test for H0 : p = p0 versus H1 : p = p1 where p1 < p0 are two numbers
from (0, 1). Explicitly find k and γ numerically when n = 10, 15, 25, α = .05,
.10 and p0 = .1, .5, .7.
8.3.7 (Example 8.3.7 Continued) Suppose that X1, ..., Xn are iid random
variables having the Poisson(λ) distribution where λ ∈ ℜ+ is the unknown
parameter. With preassigned α ∈ (0, 1), derive the randomized MP level α test
for H0 : λ = λ0 versus H1 : λ = λ1 where λ1 < λ0 are two positive numbers.
Explicitly find k and γ numerically when n = 10, 15, 25, α = .05, .10 and λ0 =
1, 2.
8.3.8 Suppose that X is an observable random variable with its pdf given
by f(x), x ∈ ℜ. Consider the two functions defined as follows:
Determine the MP level α test for
in the simplest implementable form. Perform the power calculations. {Hint:
Follow the Example 8.3.8.}
8.3.9 Suppose that X1, X2, X3 are observable iid random variables with the
common pdf given by f(x), x ∈ ℜ. Consider the two functions defined as
follows:
Determine the MP level α test for
in the simplest implementable form. Perform the power calculations. {Hint:
Follow the Example 8.3.9.}
8.3.10 Suppose that X is an observable random variable with its pdf given
by f(x), x ∈ ℜ. Consider the two functions defined as follows:
Show that the MP level a test for

8. Tests of Hypotheses
433
will reject H0 if and only if | X | < k. Determine k as a function of α. Perform
the power calculations. {Hint: Follow the Example 8.3.10.}
8.3.11 Suppose that X1, ..., Xn are iid having Uniform (–θ, θ) distribution
with the unknown parameter θ (> 0). In order to choose between the two
hypotheses H0 : θ = θ0 versus H1 : θ = θ1(> θ0) with two positive numbers θ0,
θ1, suppose that we reject H0 if and only if | Xn:n | > c where c is a fixed
positive number. Is there any α ∈ (0, 1) for which this test is MP level α?
{Hint: Write the likelihood function and look at the minimal sufficient statistic
for θ.}
8.3.12 Suppose that X1, ..., Xn are iid Geometric(p) where p ∈ (0, 1) is the
unknown parameter. With preassigned α ∈ (0, 1), derive the randomized MP
level α test for H0 : p = p0 versus H1 : p = p1(> p0) where p0, p1 are two
numbers from (0, 1). Explicitly find k and γ numerically when n = 4, 5, 6, α
= .05 and p0 = .1, .5, .7.
8.3.13 Let X1, ..., Xn be iid having the Rayleigh distribution with the com-
mon pdf f(x; θ) = 2θ -1xexp(-x2/θ)I(x > 0) where θ(> 0) is the unknown
parameter. With preassigned α ∈ (0, 1), derive the MP level α test for H0 : θ
= θ0 versus H1 : θ = θ1(< θ0) where θ0, θ1 are two positive numbers, in the
simplest implementable form.
8.3.14 Let X1, ..., Xn be iid having the Weibull distribution with the com-
mon pdf f(x;a) = a-1bxb-1exp(-xb/a)I(x > 0) where a(> 0) is an unknown pa-
rameter but b(> 0) is assumed known. With preassigned α ∈ (0, 1), derive the
MP level a test for H0 : a = a0 versus H1 : a = a1(> a0) where a0, a1 are two
positive numbers, in the simplest implementable form.
8.3.15 (Example 8.3.11 Continued) Evaluate the power of the MP level α
test described by (8.3.32).
8.3.16 (Example 8.3.11 Continued) Suppose that X1 and X2 are indepen-
dent random variables respectively distributed as N(µ, σ2), N(3µ, 2σ2) where
µ ∈ ℜ is the unknown parameter and σ ∈ ℜ+ is assumed known. Derive the
MP level a test for H0 : µ = µ0 versus H1 : µ = µ1(< µ0). Evaluate the power of
the test. {Hint: Write the likelihood function along the line of (8.3.31) and
proceed accordingly.}
8.3.17 (Example 8.3.12 Continued) Evaluate the power of the MP level a
test described by (8.3.34).
8.3.18 (Example 8.3.12 Continued) Let us denote X’ = (X1, X2) where X is
assumed to have the bivariate normal distribution, 
 Here, we
consider µ(∈ ℜ) as the unknown parameter. Derive the MP level a test for H0
: µ = µ0 versus H1 : µ = µ1(< µ0). Evaluate the power of the test. {Hint: Write
the likelihood function along the line of (8.3.33) and proceed accordingly.}

434
8. Tests of Hypotheses
8.3.19 Let X1, ..., Xn be iid positive random variables having the common
lognormal pdf 
 0) with
– ∞ < µ < ∞, 0 < σ < ∞. Here, µ is the only unknown parameter.
(i)
Find the minimal sufficient statistic for µ;
(ii)
Given α ∈ (0, 1), find the MP level a test for deciding between the
null hypothesis H0 : µ = µ0 and the alternative hypothesis H1 : µ =
µ1(> µ0) where µ0, µ1 are fixed real numbers.
8.3.20 Suppose that X is an observable random variable with its pdf given
by f(x), x ∈ ℜ. Consider the two functions defined as follows:
Derive the MP level α test for
Perform the power calculations. {Hint: Follow the Example 8.3.10.}
8.3.21 (Exercise 8.3.18 Continued) Let us denote X’ = (X1, X2) where X is
assumed to have the bivariate normal distribution, 
 Here,
we consider µ(∈ ℜ) as the unknown parameter. Derive the MP level α test for
H0 : µ = µ0 versus H1 : µ = µ1(< µ0). Evaluate the power of the test. {Hint:
Write down the likelihood function along the line of (8.3.33) and proceed
accordingly.}
8.4.1 Let X1, ..., Xn be iid having the common Laplace pdf f(x;b) =
½b–1exp(- | x - a |/b)I(x ∈ ℜ) where b(> 0) is an unknown parameter but
a(∈ ℜ) is assumed known. Show that the family of distributions has the MLR
increasing property in T, the sufficient statistic for b.
8.4.2 Let X1, ..., Xn be iid having the Weibull distribution with the common
pdf f(x;a) = a-1bxb-1exp(-xb/a)I(x > 0) where a(> 0) is an unknown parameter
but b(> 0) is assumed known. Show that the family of distributions has the
MLR increasing property in T, the sufficient statistic for a.
8.4.3 Let X1, ..., Xn be iid random variables with the Poisson(λ) distribu-
tion where λ ∈ ℜ+ is the unknown parameter. Show that the family of distri-
butions has the MLR increasing property in T, the sufficient statistic for λ.
8.4.4 Suppose that X1, ..., Xn are iid having the Uniform(–θ, θ) distribution
with unknown θ(> 0). Show that the family of distributions has the MLR
increasing property in T = | Xn:n|, the sufficient statistic for θ.

8. Tests of Hypotheses
435
8.4.5 Let X1, ..., Xn be iid having the common negative exponential pdf
f(x;µ) = σ-1exp{-(x - µ)/σ}I(x > µ) where µ(∈ ℜ) is an unknown parameter
but σ (∈ ℜ&plus;) is assumed known. Show that the family of distributions has
the MLR increasing property in T = Xn:1, the sufficient statistic for µ.
8.4.6 Let X1, ..., Xn be iid having the common Cauchy pdf f(x; θ) = 1/π{1
+ (x - θ)2}-1I(x ∈ ℜ) where θ(∈ ℜ) is an unknown parameter. Show that this
family of distributions does not enjoy the MLR property. {Hint: Note that for
any θ* > ?, the likelihood ratio L(x; θ*)/L(x; θ) θ → 1 as xi’s converge to ±∞.)
8.4.7 Suppose that X1, ..., Xn are iid random variables from the N(µ, σ2)
population where µ is assumed known but σ is unknown, µ ∈ ℜ, σ ∈ ℜ+. We
have fixed numbers α ∈ (0, 1) and σ0(> 0). Derive the UMP level α test for H0
: σ > σ0 versus H1 : σ = σ0 in the simplest implementable form.
(i)
Use the Neyman-Pearson approach;
(ii)
Use the MLR approach.
8.4.8 Suppose that X1, ..., Xn are iid having the Uniform(0, θ) distribution
with unknown θ(> 0). With preassigned α ∈ (0, 1), derive the UMP level α
test for H0 : θ > θ0 versus H1 : θ ≤ θ0 is a positive number, in the simplest
implementable form.
(i)
Use the Neyman-Pearson approach;
(ii)
Use the MLR approach.
8.4.9 Let X1, ..., Xn be iid with the common gamma pdf f(x; δ, b) =
b-δ[Γ(δ)]-1xd1exp(-x/b)I(x > 0) with two unknown parameters (δ, b) ∈ ℜ+ × ℜ+.
With preassigned α ∈ (0, 1), derive the UMP level α test, in the simplest
implementable form, for H0 : (b < b0, δ = δ*) versus H1 : (b ≥ b0, δ = δ*)
where b0 is a positive number and δ* is also positive number.
(i)
Use the Neyman-Pearson approach;
(ii)
Use the MLR approach.
8.4.10 Suppose that X1, ..., Xn are iid random variables from a N(µ, σ2)
population where µ is unknown but σ is assumed known, µ ∈ ℜ, σ ∈ ℜ+. In
order to choose between the two hypotheses H0 : µ < µ0 versus H1 : µ ≥ µ0,
suppose that we reject H0 if and only if 
 > c where c is a fixed number. Is
there any α ∈ (0, 1) for which this test is UMP level α?
8.4.11 Suppose that X1, ..., Xn are iid having the Uniform(0, θ) distribution
with unknown θ(> 0). In order to choose between the two hypotheses H0 : θ
> θ0 versus H1 : θ ≤ θ0 where θ0 is a positive number, suppose that we reject
H0 if and only if Xn:n < c where c is a fixed positive number. Is there any α ∈
(0, 1) for which this test is UMP level α?

436
8. Tests of Hypotheses
8.4.12 Suppose that X1, ..., Xn are iid random variables having the
Bernoulli(p) distribution where p ∈ (0, 1) is the unknown parameter. With
preassigned α ∈ (0, 1), derive the randomized UMP level α test for H0 : p = p0
versus H1 : p > p0 where p0 is a number between 0 and 1.
8.4.13 Suppose that X1, ..., Xn are iid random variables having the Poisson(λ)
distribution where λ ∈ ℜ+ is the unknown parameter. With preassigned α ∈
(0, 1), derive the randomized UMP level α test for H0 : λ = λ0 versus H1 : λ <
λ0 where λ0 is a positive number.
8.4.14 Suppose that X1, ..., Xn are iid having the Uniform(–θ, θ) distribu-
tion with unknown θ(> 0). In order to choose between the two hypotheses H0
: θ ≤ θ0 versus H1 : θ > θ0 where θ0 is a positive number, suppose that we
reject H0 if and only if | Xn:n | > c where c is a fixed known positive number. Is
there any α ∈ (0, 1) for which this test is UMP level α?
8.4.15 Suppose that X1, ..., Xn are iid Geometric(p) where p ∈ (0, 1) is the
unknown parameter. With preassigned α ∈ (0, 1), derive the randomized UMP
level α test for H0 : p ≥ p0 versus H1 : p < p0 where p0 is a number between 0
and 1.
8.4.16 Let X1, ..., Xn be iid having the Rayleigh distribution with the com-
mon pdf f(x; θ) = 2θ–1xexp(–x2/θ)I(x > 0) where θ(> 0) is the unknown pa-
rameter. With preassigned α ∈ (0, 1), derive the UMP level α test for H0 : θ ≤
θ0 versus H1 : θ > θ0 where θ0 is a positive number, in the simplest implementable
form.
(i)
Use the Neyman-Pearson approach;
(ii)
Use the MLR approach.
8.4.17 Let X1, ..., Xn be iid having the Weibull distribution with the com-
mon pdf f(x; a) = a-1bxb-1exp(-xb/a)I(x > 0) where a(> 0) is an unknown
parameter but b(> 0) is assumed known. With preassigned a ∈ (0, 1), derive
the UMP level a test for H0 : a ≤ a0 versus H1 : a > a0 where a0 is a positive
number, in the simplest implementable form.
(i)
Use the Neyman-Pearson approach;
(ii)
Use the MLR approach.
8.4.18 (Exercise 8.4.5 Continued) Let X1, ..., Xn be iid having the common
negative exponential pdf f(x; µ) = σ–1exp{-(x - µ)/σ}I(x > µ) where µ(∈ ℜ) is
an unknown parameter but σ(∈ ℜ+) is assumed known. With preassigned α ∈
(0, 1), derive the UMP level α test for H0 : µ ≤ µ0 versus H1 : µ > µ0 where µ0
is a real number, in the simplest implementable form.
8.4.19 (Exercise 8.4.1 Continued) Let X1, ..., Xn be iid having the common
Laplace pdf f(x; b) = ½b-1exp(– | x – a | /b)I(x ∈ ℜ) where b(> 0) is an
unknown parameter but a(∈ ℜ) is assumed known. With preassigned

8. Tests of Hypotheses
437
α ∈ (0, 1), derive the UMP level a test for H0 : b ≤ b0 versus H1 : b > b0 where
b0 is a positive real number, in the simplest implementable form.
8.4.20 (Exercise 8.3.19 Continued) Suppose that X1, ..., Xn are iid positive
random variables having the common lognormal pdf
with x > 0, –∞ < µ < ∞, 0 < σ < ∞. Here, µ is the only unknown parameter.
Given the value α ∈ (0, 1), derive the UMP level α test for deciding between
the null hypothesis H0 : µ = µ0 and the alternative hypothesis H1 : µ > µ0
where µ0 is a fixed real number. Describe the test in its simplest implementable
form.
8.4.21 (Exercise 8.4.20 Continued) Let us denote the lognormal pdf
with w > 0, –∞ < µ < ∞, 0 < σ < ∞. Suppose that X1, ..., Xm are iid positive
random variables having the common pdf f(x; µ, 2), Y1, ..., Yn are iid positive
random variables having the common pdf f(y; 2µ, 3), and also that the X’s
and Y’s are independent. Here, µ is the unknown parameter and m ≠ n. De-
scribe the tests in their simplest implementable forms.
(i)
Find the minimal sufficient statistic for µ;
(ii)
Given α ∈ (0, 1), find the MP level a test to choose between the
null hypothesis H0 : µ = µ0 and the alternative hypothesis H1 : µ =
µ1(> µ0) where µ0, µ1 are fixed real numbers;
(iii) Given α ∈ (0, 1), find the UMP level a test to choose between the
null hypothesis H0 : µ = 1 and the alternative hypothesis Ha : µ > 1.
{Hints: In part (i), use the Lehmann-Scheffé Theorems from Chapter 6 to
claim that 
 is the minimal sufficient statis-
tic. In parts (ii)-(iii), use the likelihood functions along the lines of the Ex-
amples 8.3.11-8.3.12 and show that one will reject H0 if and only if
8.4.22 Denote the gamma pdf f(w; µ) = [µaΓ(a)]-1exp{-w/µ} wa-1 with w >
0, 0 < µ < ∞, 0 < a < ∞. Suppose that X1, ..., Xm are iid positive random
variables having the common pdf f(x; µ, 1), Y1, ..., Yn are iid positive random
variables having the common pdf f(y; µ, 3), and also that the X’s and Y’s are
independent. Here, µ is the only unknown parameter and m ≠ n.

438
8. Tests of Hypotheses
(i)
Find the minimal sufficient statistic for µ;
(ii)
Given α ∈ (0, 1), find the MP level α test to choose between the
null hypothesis H0 : µ = µ0 and the alternative hypothesis H1 : µ =
µ1(> µ0) where µ0, µ1 are fixed real numbers;
(iii) Given α ∈ (0, 1), find the UMP level α test to choose between
the null hypothesis H0 : µ = 1 and the alternative hypothesis
Ha : µ > 1.
{Hints: In part (i), use the Lehmann-Scheffé Theorems from Chapter 6 to
claim that 
 is the minimal sufficient statistic. In parts
(ii)-(iii), use the likelihood functions along the lines of the Examples 8.3.11-
8.3.12 and show that one will reject H0 if and only if T > k.}
8.4.23 Let X1, X2 be iid with the common pdf θ-1exp(-x/θ)I(x > 0) where θ
> 0. In order to test H0 : θ = 2 against H1 : θ > 2, we propose to use the critical
region R = {X ∈ ℜ+2 : X1 + X2 > 9.5}. Evaluate the level α and find the power
function. Find the Type II error probabilities when θ = 4, 4.5.
8.4.24 Let X1, ..., Xn be iid random variables having the common pdf f(x;
θ) = θ-1x(1-θ)/θI(0 < x < 1) with 0 < θ < ∞. Here, θ is the unknown parameter.
Given the value α ∈ (0, 1), derive the UMP level a test in its simplest
implementable form, if it exists, for deciding between the null hypothesis H0 :
θ ≤ θ0 and the alternative hypothesis H1 : θ > θ0 where θ0 is a fixed positive
number.
8.4.25 (Sample Size Determination) Let X1, ..., Xn be iid N(µ, σ2) where
µ is assumed unknown but σ is known, µ ∈ ℜ, σ ∈ ℜ+. We have shown that
given α ∈ (0, 1), the UMP level α test for H0 : µ = µ0 versus H1 : µ > µ0 rejects
H0 if and only if 
 The UMP test makes sure that it has
the minimum possible Type II error probability at µ = µ1(> µ0) among all level
α tests, but there is no guarantee that this minimum Type II error probability
at µ = µ1 will be “small” unless n is appropriately determined.
Suppose that we require the UMP test to have Type II error probability ≤
β ∈ (0, 1) for some specified value µ = µ1(> µ0). Show that the sample size n
must be the smallest integer ≥ {(zα + zβ)σ/(µ1 – µ0)}2.
8.5.1 Let X1, ..., Xn be iid having the common negative exponential pdf f(x;
θ) = b-1exp{–(x – θ)/b}I(x > θ) where b(> 0) is assumed known and θ(∈ ℜ)
is the unknown parameter. With preassigned α ∈ (0, 1), derive the UMP level
α test for H0 : θ = θ0 versus H1 : θ ≠ θ0 where θ0 is a fixed number, in the
simplest implementable form. {Hint: Observe that Yi = exp{-Xi/b}, i = 1, ...,
n, are iid Uniform(0, δ) where the parameter δ = exp{–θ/b}. Now, exploit the
UMP test from the Section 8.5.2.}

8. Tests of Hypotheses
439
8.5.2 (Exercise 8.4.21 Continued) Denote the lognormal pdf f(w; µ) =
 with w > 0, –∞ < µ < ∞, 0 < σ < ∞.
Suppose that X1, ..., Xm are iid positive random variables having the common
pdf f(x; µ, 2), Y1, ..., Yn are iid positive random variables having the common
pdf f(y; 2µ, 3), and also that the X’s and Y’s are independent. Here, µ is the
only unknown parameter and m ≠ n. Argue whether there does or does not
exist a UMP level a test for deciding between the null hypothesis H0 : µ = 1
and the alternative hypothesis Ha : µ ≠ 1. {Hint: Try to exploit arguments
similar to those used in the Section 8.5.1.}
8.5.3 Let X1, X2, X3, X4 be iid Uniform(0, θ) where θ > 0. In order to test
H0 : θ = 1 against H1 : θ ≠ 1, suppose that we propose to use the critical region
R = {X ∈ ℜ+4 : X4:4 < ½ or X4:4 > 1}. Evaluate the level α and the power
function.
8.5.4 Let X1, ..., Xn be iid having the common exponential pdf f(x; θ) =
θ -1exp{-x/θ}I(x > 0) where θ(> 0) is assumed unknown. With preassigned α
∈ (0, 1), show that no UMP level α test for H0 : θ = θ0 versus H1 : θ ≠ θ0 exists
where ?0 is a fixed positive number. {Hint: Try to exploit arguments similar to
those used in the Section 8.5.1.}
8.5.5 Let X1, ..., Xn be iid N(0, σ2) with unknown σ ∈ ℜ+. With preas-
signed α ∈ (0, 1), we wish to test H0 : σ = σ0 versus H1 : σ ≠ σ0 at the level
α, where σ0 is a fixed positive number. Show that no UMP level α test exists
for testing H0 versus H1. {Hint: Try to exploit arguments similar to those
used in the Section 8.5.1.}

This page intentionally left blank

441
9
Confidence Interval Estimation
9.1
Introduction
As the name confidence interval suggests, we will now explore methods to
estimate an unknown parameter θ ∈ Θ ⊆ ℜ with the help of an interval. That
is, we would construct two statistics TL(X), TU(X) based on the data X and
propose the interval (TL(X), TU(X)) as the final estimator of θ. Sometimes the
lower bound TL(X) may coincide with –∞ and in that case the associated
interval (–∞, TU(X)) will be called an upper confidence interval for θ. On the
other hand, sometimes the upper bound TU(X) may coincide with ∞ and in
that case the associated interval (TL(X), ∞) will be called a lower confidence
interval for θ.
We may add that the concepts of the “fiducial distribution” and “fiducial
intervals” originated with Fisher (1930) which led to persistent and substan-
tial philosophical arguments. Among others, J. Neyman came down hard on
Fisher on philosophical grounds and proceeded to give the foundation of the
theory of confidence intervals. This culminated in Neyman’s (1935b,1937)
two path–breaking papers. After 1937, neither Neyman nor Fisher swayed
from their respective philosophical stance. However, in the 1961 article, Sil-
ver jubilee of my dispute with Fisher, Neyman was a little kinder to Fisher in
his exposition. It may not be entirely out of place to note that Fisher died on
July 29, 1962. The articles of Buehler (1980), Lane (1980) and Wallace (1980)
gave important perspectives of fiducial probability and distribution.
We prefer to have both the lower and upper end points of the
confidence interval J, namely TL(X) and TU(X), depend
exclusively on the (minimal) sufficient statistics for θ.
Now, let us examine how we should measure the quality of a proposed
confidence interval. We start with one fundamental concept defined as fol-
lows.
Definition 9.1.1 The coverage probability associated with a confidence
interval J = (TL(X), TU(X)) for the unknown parameter θ is measured by

442
9. Confidence Interval Estimation
The confidence coefficient corresponding to the confidence interval J is de-
fined to be
However, the coverage probability Pθ {θ ∈ (TL(X), TU(X))} will not in-
volve the unknown parameter θ in many standard applications. In those situ-
ations, it will be easy to derive Infθ∈TPθ {θ ∈ (TL(X), TU(X))} because then it
will coincide with the coverage probability itself. Thus, we will interchange-
ably use the two phrases, the confidence coefficient and the coverage prob-
ability when describing a confidence interval.
Customarily, we fix a small preassigned number α ∈ (0, 1) and require a
confidence interval for θ with the confidence coefficient exactly (1 – α). We
refer to such an interval as a (1 – θ) or 100(1 – α)% confidence interval
estimator for the unknown parameter θ.
Example 9.1.1 Suppose that X1, X2 are iid N(µ, 1) where µ(∈ ℜ) is the
unknown parameter.
Figure 9.1.1. Standard Normal PDF: The Shaded Area Between
–zα/2 and zα/2 = 1.96 Is 1 – α Where α = 0.05
First consider TL(X) = X1 – 1.96, TU(X) = X1 + 1.96, leading to the confi-
dence interval
The associated coverage probability is given by
which is .95 and it does not depend upon µ. So, the confidence coefficient
associated with the interval J1 is .95.

9. Confidence Interval Estimation
443
Next consider 
 leading to the confi-
dence interval 
. Let 
 which is dis-
tributed as N(0, 1) if µ is the true population mean. Now, the confidence
coefficient is given by
which is the same as P{| Z | < 1.96} = .95, whatever be µ. Between the two
95% confidence intervals J1 and J2 for µ, the interval J2 appears superior
because J2 is shorter in length than J1. Observe that the construction of J2 is
based on the sufficient statistic 
, the sample mean. !
In Section 9.2, we discuss some standard one–sample problems. The
first approach discussed in Section 9.2.1 involves what is known as the
inversion of a suitable test procedure. But, this approach becomes compli-
cated particularly when two or more parameters are involved. A more flex-
ible method is introduced in Section 9.2.2 by considering pivotal random
variables. Next, we provide an interpretation of the confidence coefficient
in Section 9.2.3. Then, in Section 9.2.4, we look into some notions of accu-
racy measures of confidence intervals. The Section 9.3 introduces a number
of two–sample problems via pivotal approach. Simultaneous confidence
regions are briefly addressed in Section 9.4.
It will become clear from this chapter that our focus lies in the methods
of construction of exact confidence intervals. In discrete populations, for
example binomial or Poisson, exact confidence interval procedures are of-
ten intractable. In such situations, one may derive useful approximate tech-
niques assuming that the sample size is “large”. These topics fall in the
realm of large sample inferences and hence their developments are delegated
to the Chapter 12.
9.2
One-Sample Problems
The first approach involves the inversion of a test procedure. Next, we pro-
vide a more flexible method using pivots which functionally depend only on
the (minimal) sufficient statistics. Then, an interpretation is given for the con-
fidence interval, followed by notions of accuracy measures associated with a
confidence interval.

444
9. Confidence Interval Estimation
9.2.1
Inversion of a Test Procedure
In general, for testing a null hypothesis H0 : θ = θ0 against the alternative
hypothesis H1 : θ > θ0 (or H1 : θ < θ0 or H1 : θ  ≠ θ0), we look at the subset of
the sample space Rc which corresponds to the acceptance of H0. In Chapter
8, we had called the subset R the critical or the rejection region. The subset
Rc which corresponds to accepting H0 may be referred to as the acceptance
region. The construction of a confidence interval and its confidence coeffi-
cient are both closely tied in with the nature of the acceptance region Rc and
the level of the test.
Example 9.2.1 (Example 8.4.1 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) with the unknown parameter µ ∈ ℜ. We assume that σ ∈ ℜ+ is
known. With preassigned α ∈ (0, 1), the UMP level a test for H0 : µ = µ0
versus H1 : µ > µ0 where µ0 is a fixed real number, would be as follows:
where zα is the upper 100α% point of the standard normal distribution. Refer
to the Figure 9.2.1. The acceptance region (for H0) then corresponds to
Figure 9.2.1. Standard Normal PDF: The Shaded Area
on the Right of zα Is α
Since the test described by (9.2.2) has the level α, we can write
In other words, we can claim that

9. Confidence Interval Estimation
445
Now, the equation (9.2.3) can be rewritten as
and thus, we can claim that 
 is a 100(1 – α)% lower
confidence interval estimator for µ. !
The upper–sided a level test ⇒ 100(1 – α)% lower confidence
interval estimator (TL(X), ∞) for θ.
The lower–sided a level test ⇒ 100(1 – α)% upper confidence
interval estimator (–∞, TU(X)) for θ.
Example 9.2.2 Let X1, ..., Xn be iid with the common exponential pdf θ–
1exp{–x/θ}I(x > 0) with the unknown parameter θ ∈ ℜ+. With preassigned α
∈ (0, 1), the UMP level α test for H0 : θ = θ0 versus H1 : θ > θ0 where θ0 is a
fixed positive real number, would be as follows:
where 
,a is the upper 100α% point of the Chi–square distribution with
2n degrees of freedom. See the Figure 9.2.2.
Figure 9.2.2. The Shaded Area on the Right of  
, α Is a
The acceptance region (for H0) then corresponds to
Since this test has level a, we can write

446
9. Confidence Interval Estimation
In other words, we can claim that
The equation (9.2.7) can be rewritten as
and thus we can claim that 
 lower
confidence interval estimator for θ. !
Example 9.2.3 (Example 9.2.2 Continued) Let X1, ..., Xn be iid with the
common exponential pdf θ–1 exp{ –x/θ}I(x > 0) with the unknown parameter
θ ∈ ℜ+. With preassigned α ∈ (0,1), suppose that we invert the UMP level α
test for H0 : θ = θ0 versus H1 : θ < θ0, where θ0 is a positive real number.
Figure 9.2.3. The Area on the Left (or Right) of 
,
1– α Is a (or 1 – α)
Then, one arrives at 
 a 100(1 – α)% upper confi-
dence interval estimator for θ, by inverting the UMP level α test. See the
Figure 9.2.3. We leave out the details as an exercise. !
9.2.2
The Pivotal Approach
Let X1, ..., Xn be iid real valued random variables from a population with the
pmf or pdf f(x; θ) for x ∈ χ where θ(∈ Θ) is an unknown real valued
parameter. Suppose that T ≡ T(X) is a real valued (minimal) sufficient statis-
tic for θ.
The family of pmf or pdf induced by the statistic T is denoted by g(t; θ)
for t ∈ T and θ ∈ Θ. In many applications, g(t; θ) will belong to an appropriate
location, scale, or location–scale family of distributions which were discussed
in Section 6.5.1. We may expect the following results:

9. Confidence Interval Estimation
447
The Location Case: With some a(θ), the distribution of
{T – a(θ)} would not involve θ for any θ ∈ Θ.
The Scale Case: With some b(θ), the distribution of
T/b(θ) would not involve θ for any θ ∈ Θ.
The Location–Scale Case: With some a(θ), b(θ), the
distribution of {T – a(θ)}/b(θ) would not involve θ for any θ ∈ Θ.
Definition 9.2.1 A pivot is a random variable U which functionally de-
pends on both the (minimal) sufficient statistic T and θθθθθ, but the distribution
of U does not involve θθθθθ for any θθθθθ ∈ Θ.
In the location, scale, and location–scale situations, when U, T and θ are
all real valued, the customary pivots are appropriate multiples of {T – a(θ)},
T/b(θ) or {T – a(θ)}/b(θ) respectively with suitable expressions of a(θ) and
b(θ).
We often demand that the distribution of the pivot must
coincide with one of the standard distributions so that a
standard statistical table can be utilized to determine the
appropriate percentiles of the distribution.
Example 9.2.4 Let X be a random variable with its pdf f(x; θ) = θ–1 exp{
–x/θ}I(x > 0) where θ(> 0) is the unknown parameter. Given some α ∈ (0,1),
we wish to construct a (1 – α) two–sided confidence interval for θ. The
statistic X is minimal sufficient for θ and the pdf of X belongs to the scale
family. The pdf of the pivot U = X/θ is given by g(u) = e–uI(u > 0). One can
explicitly determine two positive numbers a < b such that P(U < a) = P(U >
b) = ½α so that P(a < U < b) = 1 – α. It can be easily checked that a = –log(1
– ½α) and b = –log(½α).
Since the distribution of U does not involve θ, we can
determine both a and b depending exclusively upon α.
Now observe that
which shows that J = (b–1 X, a–1 X) is a (1 – α) two–sided confidence interval
estimator for θ. !

448
9. Confidence Interval Estimation
Example 9.2.5 Let X1, ..., Xn be iid Uniform(0, θ) where θ(> 0) is the
unknown parameter. Given some α ∈ (0,1), we wish to construct a (1 – α)
two-sided confidence interval for θ. The statistic T ≡ Xn:n, the largest order
statistic, is minimal sufficient for θ and the pdf of T belongs to the scale
family. The pdf of the pivot U = T/θ is given by g(u) = nun–1I(0 < u <1). One
can explicitly determine two numbers 0 < a < b < 1 such that P(U < a) = P(U
> b) = ½α so that P(a < U < b) = 1 – α. It can be easily checked that a =
(½α)1/n and b = (1 – ½α)1/n. Observe that
which shows that J = (b–1 Xn:n, a–1 Xn:n) is a (1 – α) two–sided confidence
interval estimator for θ. !
Example 9.2.6 Negative Exponential Location Parameter with Known
Scale: Let X1, ..., Xn be iid with the common negative exponential pdf f(x; θ)
= σ–1 exp{–(x – θ)/σ}I(x > θ). Here, θ ∈ ℜ is the unknown parameter and we
assume that σ ∈ ℜ+ is known. Given some α ∈ (0,1), we wish to construct a
(1 – α) two–sided confidence interval for θ. The statistic T ≡ Xn:1, the small-
est order statistic, is minimal sufficient for θ and the pdf of T belongs to the
location family. The pdf of the pivot U = n(T – θ)/σ is given by g(u) = e–uI(u
> 0). One can explicitly determine a positive number b such that P(U > b) =
a so that we will then have P(0 < U < b) = 1 – α. It can be easily checked that
b = –log(α). Observe that
which shows that J = (Xn:1 – bn–1 σ, Xn:1) is a (1 – α) two-sided confidence
interval estimator for θ.
Figure 9.2.4. Standard Normal PDF: The Area on the
Right (or Left) of zα/2 (or – zα/2) Is α/2
Example 9.2.7 Normal Mean with Known Variance: Let X1, ...,
Xn be iid N(µ, σ2) with the unknown parameter µ ∈ ℜ. We assume that

9. Confidence Interval Estimation
449
σ ∈ ℜ+ is known. Given some α ∈ (0,1), we wish to construct a (1 – α) two-
sided confidence interval for µ. The statistic T = 
, the sample mean, is
minimal sufficient for µ and T has the N(µ, 1/nσ2) distribution which belongs
to the location family. The pivot U = 
 has the standard normal
distribution. See the Figure 9.2.4. We have P{ –zα/2 < U < zα/2} = 1 – α which
implies that
In other words,
is a (1 – α) two–sided confidence interval estimator for µ. !
Example 9.2.8 Normal Mean with Unknown Variance: Suppose that
X1, ..., Xn are iid N(µ, α2) with both unknown parameters µ ∈ ℜ and σ ∈
ℜ+, n ≥ 2. Given some α ∈ (0,1), we wish to construct a (1 – α) two-
sided confidence interval for µ. Let 
 be the sample mean and
 be the sample variance. The statistic T ≡
(
, S) is minimal sufficient for (µ, σ). Here, the distributions of the X’s
belong to the location–scale family. The pivot 
 has the
Student’s t distribution with (n – 1) degrees of freedom. So, we can say
that P{ –tn–1,α/2 < U < tn–1,α/2} = 1 – α where tn–1,α/2 is the upper 100(1 –
½α)% point of the Student’s t distribution with (n – 1) degrees of free-
dom. See the Figure 9.2.5.
Figure 9.2.5. The Area on the Right (or Left) of tn–1,α/2
(or –tn–1,α/2) Is α/2
Thus, we claim that

450
9. Confidence Interval Estimation
In other words,
is a (1 – α) two–sided confidence interval estimator for µ. !
Example 9.2.9 Normal Variance: Suppose that X1, ..., Xn are iid N(µ,
σ2) with both unknown parameters µ ∈ ℜ and σ ∈ ℜ+, n ≥ 2. Given some α
∈ (0,1), we wish to construct a (1 – α) two–sided confidence interval for σ2.
Let 
 be the sample mean and 
 be the sample
variance. The statistic T ≡ (
, S) is minimal sufficient for (µ, σ). Here, the
distributions of the X’s belong to the location–scale family. The pivot U = (n
– 1) S2 / σ2 has the Chi–square distribution with (n – 1) degrees of freedom.
Recall that 
 is the upper 100γ% point of the Chi–square distribution with
v degrees of freedom.
Figure 9.2.6. The Area on the Right (or Left) of 
 is α/2
The distribution of S2 belongs to the scale family. So, we can claim that
 See the Figure 9.2.6. Thus, we
claim that
In other words,
is a (1 – α) two–sided confidence interval estimator for σ2. !

9. Confidence Interval Estimation
451
Example 9.2.10 Joint Confidence Intervals for the Normal Mean and
Variance: Suppose that X1, ..., Xn are iid N(µ, σ2) with both unknown pa-
rameters µ ∈ ℜ and σ ∈ ℜ+, n ≥ 2. Given some α ∈ (0, 1), we wish to
construct (1 – α) joint two-sided confidence intervals for both µ and σ2. Let
 be the sample mean and 
 be the sample vari-
ance. The statistic T ≡ (
, S) is minimal sufficient for (µ, σ).
From the Example 9.2.8, we claim that
is a (1 – γ) confidence interval for µ for any fixed γ ∈ (0, 1). Similarly, from
the Example 9.2.9, we claim that
is a (1 – δ) confidence interval for σ2 for any δ ∈ (0, 1). Now, we can write
Now, if we choose 0 < γ , δ < 1 so that γ + δ = α, then we can think of {J1,
J2} as the two-sided joint confidence intervals for the unknown parameters
µ, σ2 respectively with the joint confidence coefficient at least (1 – α). Cus-
tomarily, we pick γ = δ = ½α. !
One will find more closely related problems on joint confidence
intervals in the Exercise 9.2.7 and Exercises 9.2.11-9.2.12.
9.2.3
The Interpretation of a Confidence Coefficient
Next, let us explain in general how we interpret the confidence coefficient
or the coverage probability defined by (9.1.1). Consider the confidence
interval J for θ. Once we observe a particular data X = x, a two-sided
confidence interval estimate of θ is going to be (TL(x), TU(x)), a fixed
subinterval of the real line. Note that there is nothing random about this
observed interval estimate (TL(x), TU(x)) and recall that the parameter θ is
unknown (∈ Θ) but it is a fixed entity. The interpretation of the phrase
“(1 – α) confidence” simply means this: Suppose hypothetically that
we keep observing different data X = x1, x2, x3, ... for a long time, and we

452
9. Confidence Interval Estimation
keep constructing the corresponding observed confidence interval estimates
(TL(x1), TU(x1)), (TL(x2), TU(x2)), (TL(x3), TU(x3)), .... In the long run, out of
all these intervals constructed, approximately 100(1 – α)% would include the
unknown value of the parameter θ. This goes hand in hand with the relative
frequency interpretation of probability calculations explained in Chapter 1.
In a frequentist paradigm, one does not talk about the probability of a fixed
interval estimate including or not including the unknown value of θ.
Example 9.2.11 (Example 9.2.1 Continued) Suppose that X1, ..., Xn are iid
N(µ, σ2) with the unknown parameter µ ∈ ℜ. We assume that s ∈ ℜ&plus; is
known. We fix α = .05 so that 
 will be a 95% lower
confidence interval for µ. Using the MINITAB Release 12.1, we generated a
normal population with µ = 5 and σ = 1. First, we considered n = 10. In the
ith replication, we obtained the value of the sample mean  and computed the
lower end point 
 of the observed confidence interval, i = 1, ..., k
with k = 100, 200, 500. Then, approximately 5% of the total number (k) of
intervals so constructed can be expected not to include the true value µ = 5.
Next, we repeated the simulated exercise when n = 20. The following table
summarizes the findings.
Table 9.2.1. Number of Intervals Not Including the True Value
µ = 5 Out of k Simulated Confidence Intervals
k = 100
k = 200
k = 500
n = 10
4
9
21
n = 20
4
8
22
The number of simulations (k) is not particularly very high in this example.
Yet, we notice about four or five percent non–coverage which is what one
may expect. Among k constructed intervals, if nk denotes the number of
intervals which do not include the true value of µ, then we can claim that
9.2.4
Ideas of Accuracy Measures
In the two Examples 9.2.7–9.2.8 we used the equal tail percentage points
of the standard normal and the Student’s t distributions. Here, both piv-
otal distributions were symmetric about the origin. Both the standard
normal and the Student’s tn–1 pdf’s obviously integrate to (1 – α) re-
spectively on the intervals (–zα/2,zα/2) and (–tn–1,α/2,tn–1,α/2). But, for these
two distributions, if there existed any asymmetric (around zero) and shorter

9. Confidence Interval Estimation
453
interval with the same coverage probability (1 – α), then we should have
instead mimicked that in order to arrive at the proposed confidence inter-
vals.
Let us focus on the Example 9.2.7 and explain the case in point. Suppose
that Z is the standard normal random variable. Now, we have P{–zα/2 < Z <
zα/2} = 1 – α. Suppose that one can determine two other positive numbers a,
b such that P{–a < Z < b} = 1 – α and at the same time 2zα/2 > b + a. That is,
the two intervals (–zα/2,zα/2) and (–a, b) have the same coverage probability
(1 – α), but (–a, b) is a shorter interval. In that case, instead of the solution
proposed in (9.2.9), we should have suggested 
as the confidence interval for µ. One may ask: Was the solution in (9.2.9)
proposed because the interval (–zα/2,zα/2) happened to be the shortest one
with (1 – α) coverage in a standard normal distribution? The answer is: yes,
that is so. In the case of the Example 9.2.8, the situation is not quite the same
but it remains similar. This will become clear if one contrasts the two Ex-
amples 9.2.12–9.2.13. For the record, let us prove the Theorem 9.2.1 first.
Figure 9.2.7. The Area Between –a and a Is (1 – θ). The Area
Between –(a + g) and (a – h) Is (1 – θ)
Theorem 9.2.1 Suppose that X is a continuous random variable having a
unimodal pdf f(x) with the support ℜ. Assume that f(x) is symmetric around x
= 0, that is f(–x) = f(x) for all x > 0. Let P(–a < X < a) = 1 – a for some 0 <
α < ½. Now, suppose that the positive numbers g, h are such that one has P(–
a – g < X < a – h) = 1 – α. Then, the interval (–a – g, a – h) must be wider
than the interval (–a, a).
Proof It will suffice to show that g > h. The mode of f(x) must be at

454
9. Confidence Interval Estimation
x = 0. We may assume that a > h since we have 0 < α < ½. Since we have
P(–a < X < a) = P(–a – g < X < a – h), one obviously, has
But, the pdf f(x) is symmetric about x = 0 and f(x) is assumed positive for all
x ∈ ℜ. The Figure 9.2.7 describes a situation like this. Hence, the integrals in
(9.2.15) can be equal if and only if 
 which can
happen as long as the interval (a – h, a) is shorter than the interval (a, a +g).
The result then follows. The details are left out as an exercise. !
Remark 9.2.1 The Theorem 9.2.1 proved that among all (1 – α) intervals
going to the left of (–a, a), the interval (–a, a) was the shortest one. Since,
f(x) is assumed symmetric about x = 0, it also follows from this result that
among all (1 – α) intervals going to the right of (–a, a), the interval (–a, a) is
again the shortest one.
Example 9.2.12 (Example 9.2.7 Continued) The length Ln of the confi-
dence interval from (9.2.9) amounts to 2zα/2n–½σ which is the shortest width
among all (1 – α) confidence intervals for the unknown mean µ in a N(µ, σ2)
population with σ(> 0) known. The Theorem 9.2.1 immediately applies. Also
observe that the shortest width, namely 2zα/2n–½σ, is a fixed number here.
Example 9.2.13 (Example 9.2.8 Continued) The length Ln of the confi-
dence interval from (9.2.10) amounts to 2tn–1,α/2n–½S which is a random vari-
able to begin with. So, we do not discuss whether the confidence interval
from (9.2.10) has the shortest width among all (1 – α) confidence intervals
for the unknown mean µ. The width Ln is not even a fixed number! Observe
that 
 where Y has the 
 distribution. But,
the expression for 
 is a function of n only. Now, Theorem 9.2.1 di-
rectly implies that the confidence interval from (9.2.10) has the shortest ex-
pected width among all (1 – a) confidence intervals for µ in a N(µ, s2) popu-
lation when σ2(> 0) is also unknown. !
Both examples handled the location parameter estimation problems and we
could claim the optimality properties (shortest width or shortest expected width)
associated with the proposed (1 – α) confidence intervals. Since the pivotal
pdf’s were symmetric about zero and unimodal, these intervals had each tail
area probability ½α. Recall the Figures 9.2.4–9.2.5.
In the location parameter case, even if the pivotal pdf is skewed but
unimodal, a suitable concept of “optimality” of (1 – α) confidence intervals
can be formulated. The corresponding result will coincide with Theorem

9. Confidence Interval Estimation
455
9.2.1 in the symmetric case. But, any “optimality” property in the skewed
unimodal case does not easily translate into something nice and simple under
the scale parameter scenario. For skewed pivotal distributions, in general, we
can not claim useful and attractive optimality properties when a (1 – α) con-
fidence interval is constructed with the tail area probability ½α on both sides.
Convention: For standard pivotal distributions such as
Normal, Student’s t, Chi–square and F, we customarily
assign the tail area probability ½α on both sides in order
to construct a 100(1 – α)% confidence interval.
9.2.5
Using Confidence Intervals in the Tests of Hypothesis
Let us think of testing a null hypothesis H0 : θ = θ0 against an alternative
hypothesis H1 : θ > θ0 (or H1 : θ < θ0 or H1 : θ ≠ θ0). Depending on the nature
of the alternative hypothesis, the rejection region R respectively becomes up-
per or lower or two-sided. The reader has observed that a confidence interval
for θ can also be upper or lower or two-sided.
Suppose that one has constructed a (1 – α) lower confidence interval
estimator J1 = (TL(X), ∞) for θ. Then any null hypothesis H0 : θ = θ0 will be
rejected at level α in favor of the alternative hypothesis H1 : θ > θ0 if and only
if θ0 falls outside the confidence interval J1.
Suppose that one has constructed a (1 – α) upper confidence interval
estimator J2 = (–∞, TU(X)) for θ. Then any null hypothesis H0 : θ = θ0 will be
rejected at level α in favor of the alternative hypothesis H1 : θ < θ0 if and only
if θ0 falls outside the confidence interval J2.
Suppose that one has constructed a (1 – α) two–sided confidence interval
estimator J3 = (TL(X), TU(X)) for θ. Then any null hypothesis H0 : θ = θ0 will
be rejected at level a in favor of the alternative hypothesis H1 : θ ≠ θ0 if and
only if θ0 falls outside the confidence interval J3.
Once we have a (1 – α) confidence interval J, it is clear that any
null hypothesis H0 : θ = θ0 will be rejected at level α as long as
θ0 ∉ J. But, rejection of H0 leads to acceptance of H1 whose
nature depends upon whether J is upper-, lower- or two-sided.
Sample size determination is crucial in practice if we wish to have
a preassigned “size” of a confidence region. See Chapter 13.

456
9. Confidence Interval Estimation
9.3
Two-Sample Problems
Tests of hypotheses for the equality of means or variances of two indepen-
dent populations need machineries which are different in flavor from the theory
of MP or UMP tests developed in Chapter 8. These topics are delegated to
Chapter 11. So, in order to construct confidence intervals for the difference
of means (or location parameters) and the ratio of variances (or the scale
parameters), we avoid approaching the problems through the inversion of test
procedures. Instead, we focus only on the pivotal techniques.
Here, the basic principles remain same as in Section 9.2.2. For an un-
known real valued parametric function κ(θ), suppose that we have a point
estimator 
 which is based on a (minimal) sufficient statistic T for θ. Of-
ten, the exact distribution of 
 will become free from θ for all
θ ∈ Θ with some τ(> 0).
If τ happens to be known, then we obtain two suitable numbers a and b
such that Pθ[a < { 
 – κ(θ)}/τ < b] = 1 – α. This will lead to a (1 – α) two-
sided confidence interval for κ(θ).
If τ happens to be unknown, then we may estimate it by  and use the
new pivot 
 instead. If the exact distribution of {
 – 
become free from θ for all θ ∈ Θ, then again we obtain two suitable numbers
a and b such that 
 Once more this
will lead to a (1 – α) two-sided confidence interval for κ(θ). This is the way
we will handle the location parameter cases.
In a scale parameter case, we may proceed with a pivot 
 whose
distribution will often remain the same for all θ ∈ Θ. Then, we obtain two
suitable numbers a and b such that 
. This
will lead to a (1 – α) two–sided confidence interval for κ(θ).
9.3.1
Comparing the Location Parameters
Examples include estimation of the difference of (i) the means of two inde-
pendent normal populations, (ii) the location parameters of two independent
negative exponential populations, and (iii) the means of a bivariate normal
population.
Example 9.3.1 Difference of Normal Means with a Common Un-
known Variance: Recall the Example 4.5.2 as needed. Suppose that the
random variables Xi1, ..., Xin, are iid N(µi, σ2), i = 1, 2, and that the X1j’s
are independent of the X2j’s. We assume that all three parameters are un-
known and θ = (µ1, µ2, σ) ∈ ℜ × ℜ × ℜ+. With fixed α ∈ (0, 1), we wish
to construct a (1 – α) two–sided confidence interval for µ1 – µ2(= κ(θ))

9. Confidence Interval Estimation
457
based on the sufficient statistics for θ. With ni ≥ 2, let us denote
for i = 1, 2. Here, 
 is the pooled sample variance.
Based on (4.5.8), we define the pivot
which has the Student’s tv distribution with v = (n1 + n2 – 2) degrees of
freedom. Now, we have P{–tv,α/2 < U < tv,α/2} = 1 – α where tv,α/2 is the upper
100(1 – ½α)% point of the Student’s t distribution with v degrees of free-
dom. Thus, we claim that
Now, writing 
, we have
as our (1 – α) two–sided confidence interval for µ1 – µ2. !
Example 9.3.2 Difference of Negative Exponential Locations with a
Common Unknown Scale: Suppose that the random variables Xi1, ..., Xin
are iid having the common pdf f(x; µi, σ), i = 1, 2, where we denote f(x; µ,
σ) = σ–1 exp{ – (x – µ)/σ}I(x > µ). Also let the X1j’s be independent of the
X2j’s. We assume that all three parameters are unknown and α = (µ1, µ2, σ) ∈
ℜ×ℜ×ℜ+. With fixed α ∈ (0, 1), we wish to construct a (1 – α) two–sided
confidence interval for µ1 – µ2(= κ(α)) based on the sufficient statistics for
α. With n ≥ 2, let us denote
for i = 1, 2.
Here, WP is the pooled estimator of σ. It is easy to verify that WP estimates
σ unbiasedly too. It is also easy to verify the following claims:
That is, V[WP] < V[Wi], i = 1, 2 so that the pooled estimator WP is indeed a
better unbiased estimator of s than either Wi, i = 1, 2.

458
9. Confidence Interval Estimation
From the Example 4.4.12, recall that 2(n – 1)Wi/σ is distributed as 
,
i = 1, 2, and these are independent. Using the reproductive property of inde-
pendent Chi-squares (Theorem 4.3.2, part (iii)) we can claim that 4(n – 1)WPσ–
1 = {2(n – 1)W1 + 2(n – 1)W2}σ–1 which has a Chi-square distribution with
4(n – 1) degrees of freedom. Also, 
 and WP are independent. Hence,
we may use the following pivot
Now, let us look into the distribution of U. We know that
Hence, the pdf of Q = Y1 – Y2 would be given by ½e–|q|I(q ∈ ℜ) so that the
random variable | Q | has the standard exponential distribution. Refer to the
Exercise 4.3.4, part (ii). In other words, 2| Q | is distributed as 
. Also Q, WP
are independently distributed. Now, we rewrite the expression from (9.3.5)
as
that is, the pivotal distribution of |U| is given by F2,4n–4. Let F2,4n–4,α be the
upper 100α% point of the F distribution with 2 and (4n – 4) degrees of
freedom. See the Figure 9.3.1. We can say that P{ –F2,4n–4,α < U < F2,4n–4,α}
= 1 – α and claim that
In other words,
is a (1 – α) two–sided confidence interval estimator for (µ1 – µ2). !
Figure 9.3.1. The Shaded Area on the Right of F2,4n–4,α Is α

9. Confidence Interval Estimation
459
Confidence interval estimation of (µ1 – µ2)/σ in the two Examples
9.3.1-9.3.2 using the Bonferroni inequality are left as Exercises
9.3.3 and 9.3.5. Look at the Example 9.2.10 as needed.
Example 9.3.3 The Paired Difference t Method: Sometimes the two
populations may be assumed normal but they may be dependent. In a large
establishment, for example, suppose that X1j, X2j respectively denote the job
performance score before and after going through a week–long job enhance-
ment program for the jth employee, j = 1, ..., n(≥ 2). We assume that these
employees are selected randomly and independently of each other. We wish to
compare the average “before and after” job performance scores in the popu-
lation. Here, observe that X1j, X2j, are dependent random variables. The meth-
odology from the Example 9.3.1 will not apply here.
Suppose that the pairs of random variables (X1j, X2j) are iid bivariate
normal, 
 j = 1, ..., n(≥ 2). Let all five parameters be
unknown, (µi, σi) ∈ ℜ × ℜ+, i = 1, 2 and –1 < ρ < 1. With fixed α ∈ (0, 1),
we wish to construct a (1 – α) two–sided confidence interval for µ1 – µ2
based on the sufficient statistics for θ ( = (µ1, µ2, σ1, σ2, ρ)). Let us denote
In (9.3.7), observe that Y1, ..., Yn are iid N(µ1 – µ2, σ2) where 
 Since both mean µ1 – µ2 and variance σ2 of the common normal
distribution of the Y’s are unknown, the original two-sample problem reduces
to a one–sample problem (Example 9.2.8) in terms of the random samples on
the Y’s.
We consider the pivot
which has the Student’s t distribution with (n – 1) degrees of freedom. As
before, let tn–1,α/2 be the upper 100(1 – ½α)% point of the Student’s t distribu-
tion with (n – 1) degrees of freedom. Thus, along the lines of the Example
9.2.8, we propose
as our (1 – α) two–sided confidence interval estimator for (µ1 – µ2). !

460
9. Confidence Interval Estimation
9.3.2
Comparing the Scale Parameters
The examples include estimation of the ratio of (i) the variances of two inde-
pendent normal populations, (ii) the scale parameters of two independent nega-
tive exponential populations, and (iii) the scale parameters of two independent
uniform populations.
Example 9.3.4 Ratio of Normal Variances: Recall the Example 4.5.3 as
needed. Suppose that the random variables Xi1, ..., Xini are iid 
 ni ≥
2, i = 1, 2, and that the X1j’s are independent of the X2j’s. We assume that all
four parameters are unknown and (µi, σi) ∈ ℜ × ℜ+, i = 1, 2. With fixed α ∈
(0, 1), we wish to construct a (1 – α) two-sided confidence interval for
 based on the sufficient statistics for θ(= (µ1, µ2, σ1, σ2)). Let us denote
for i = 1, 2 and consider the pivot
It should be clear that U is distributed as Fn1–1,n2–1 since (ni – 1) 
 is
distributed as 
 i = 1, 2, and these are also independent. As before, let us
denote the upper 100(α/2)% point of the Fn1–1,n2–1 distribution by Fn1–1,n2–1,α/
2. See the Figure 9.3.2.
Figure 9.3.2. Area on the Right (or Left) of Fn1–1,n2–1,α/2}
(or Fn1–1mn2–1,1–α/2) Is α/2
Thus, we can write P{Fn1–1,n2–1,1–α/2 < U Fn1–1,n2–1,α/2} = 1 – α and claim
that

9. Confidence Interval Estimation
461
Hence, we conclude that
is a (1 – α) two-sided confidence interval estimator for the variance ratio
 By taking the square root throughout, it follows immediately from
(9.3.11) that
is a (1 – α) two–sided confidence interval estimator for the ratio σ1/σ2. !
Example 9.3.5 Ratio of Negative Exponential Scales: Suppose that
the random variables Xi1, ..., Xini are iid having the common pdf f(x; µi,
si), ni ≥ 2, i = 1, 2, where f(x; µ, σ) = σ–1 exp{–(x – µ)/σ}I(x > µ). Also
let the X1j’s be independent of the X2j’s. Here we assume that all four
parameters are unknown and (µi, σi) ∈ ℜ × ℜ+, i = 1, 2. With fixed α ∈ (0,
1), we wish to construct a (1 – α) two–sided confidence interval for σ1/
σ2 based on the sufficient statistics for θ ( = (µ1, µ2, σ1, σ2)). We denote
for i = 1, 2 and consider the pivot
It is clear that U is distributed as F2n1–2,2n2–2 since 2(nl – 1)Wl/σi is distrib-
uted as 
 and these are also independent. As before, let us
denote the upper 100(α/2)% point of the F2n1–2,2n2 – 2 distribution by
Fn1–2,2n2–2,α/2. Thus, we can write P{F2n1–2,2n2–2,1–α/2 < U < F2n1–2,2n2–2,α/2} =
1 – α and hence claim that

462
9. Confidence Interval Estimation
This leads to the following conclusion:
is a (1 – α) two–sided confidence interval estimator for σ1/σ2. !
Example 9.3.6 Ratio of Uniform Scales: Suppose that the random vari-
ables Xi1, ..., Xin are iid Uniform(0, θi), i = 1, 2. Also let the X1j’s be indepen-
dent of the X2j’s. We assume that both the parameters are unknown and θ =
(θ1, θ2) ∈ ℜ+ × ℜ+. With fixed α ∈ (0, 1), we wish to construct a (1 – α) two-
sided confidence interval for θ1/θ2 based on the sufficient statistics for (θ1,
θ2). Let us denote 
 Xij for i = 1, 2 and we consider the
following pivot:
It should be clear that the distribution function of 
 is simply tn for 0 <
t < 1 and zero otherwise, i = 1, 2. Thus, one has
distributed as iid standard exponential random variable. Recall the Example
4.2.5. We use the Exercise 4.3.4, part (ii) and claim that
has the pdf 1
2- e–|w|I(w ∈ ℜ). Then, we proceed to solve for a(> 0) such that
P(|– nlog(U)| < a} = 1 – α. In other words, we need 
which leads to the expression a = log(1/α). Then, we can claim that
 Hence, one has:
which leads to the following conclusion:
is a (1 – α) two–sided confidence interval estimator for the ratio θ1/θ2. Look
at the closely related Exercise 9.3.11. !

9. Confidence Interval Estimation
463
9.4
Multiple Comparisons
We first briefly describe some confidence region problems for the mean vec-
tor of a p–dimensional multivariate normal distribution when the p.d. disper-
sion matrix (i) is known and (ii) is of the form σ2H with a known p × p matrix
H but σ is unknown. Next, we compare the mean of a control with the means
of independent treatments followed by the analogous comparisons among the
variances. Here, one encounters important applications of the multivariate
normal, t and F distributions which were introduced earlier in Section 4.6.
9.4.1
Estimating a Multivariate Normal Mean Vector
Example 9.4.1 Suppose that X1, ..., Xn are iid p–dimensional multivariate
normal, Np(µ, Σ), random variables. Let us assume that the dispersion matrix
Σ is p.d. and known. With given α ∈ (0, 1), we wish to derive a (1 – α)
confidence region for the mean vector µ. Towards this end, from the Theo-
rem 4.6.1, part (ii), let us recall that
Now, consider the non–negative expression 
 as the
pivot and denote
We write 
,α for the upper 100α% point of the 
 distribution, that is
=1-α. Thus, we define a p–dimensional confidence region Q
for µ as follows:
One can immediately claim that
and hence, Q is a (1 – α) confidence region for the mean vector µ. Geometri-
cally, the confidence region Q will be a p–dimensional ellipsoid having its
center at the point 
, the sample mean vector. !

464
9. Confidence Interval Estimation
Figure 9.4.1. The Elliptic Confidence Regions Q from (9.4.3)
and Q* from (9.4.4)
Example 9.4.2 (Example 9.4.1 Continued) Suppose that X1, ..., X10 are iid
2–dimensional normal, N2(µ, Σ), random variables with 
 We
fix α = .05, that is we require a 95% confidence region for µ. Now, one has
 –2log(.05) = 5.9915. Suppose also that the observed value of 
′ is
(1, 2). Then, the confidence region from (9.4.2) simplifies to
which should be elliptic with its center at the point (1, 2). The Figure 9.4.1 gives
a picture (solid curve) of the region Q which is the inner disk of the ellipse. The
horizontal (x) and vertical (y) axis respectively correspond to µ1 and µ2.
Instead, if we had 
 then one can check that a 95% con-
fidence region for µ will also turn out to be elliptic with its center at the point
(1, 2). Let us denote
The Figure 9.4.1 gives a picture (dotted curve) of the region Q* which is the
inner disk of the ellipse. !
Example 9.4.3 Suppose that X1, ..., Xn are iid p–dimensional multivariate
normal, Np(µ, Σ), random variables with n ≥ 2. Let us assume that the disper-
sion matrix Σ = σ2H where H is a p.d. and known matrix but the scale multi-
plier σ2(∈ ℜ+), is unknown. With fixed α ∈ (0, 1), we wish to derive a (1 – α)
confidence region for the mean vector µ.

9. Confidence Interval Estimation
465
Let us denote
which respectively estimate µ and σ2. Recall from Theorem 4.6.1, part (ii)
that
One can show easily that
(pn – p)S2/s2 is distributed as 
 and that the mean
vector 
 and S2 are independently distributed.
(9.4.6)
Combining (9.4.5) and (9.4.6) we define the pivot
But, we note that we can rewrite U as
so that U has the Fp,pn–p distribution. We write Fp,pn–p,α for the upper 100α%
point of the Fp,pn–p distribution, that is P{U < Fp,pn–p,α} = 1 – α. Thus, we
define a p–dimensional confidence region Q for µ as follows:
Thus, we can immediately claim that
and hence Q is a (1 – α) confidence region for the mean vector µ. Again,
geometrically the confidence region Q will be a p–dimensional ellipsoid with
its center at the point 
. !
9.4.2
Comparing the Means
Example 9.4.4 Suppose that Xi1, ..., Xin are iid random samples from the N(µi,
σ2) population, i = 0, 1, ..., p. The observations X01, ..., X0n refer to a control
with its mean µ0 whereas Xi1, ..., Xin are the observations from the ith treat-
ment, i = 1, ..., p. Let us also assume that all the observations from the treat-
ments and control are independent and that all the parameters are unknown.

466
9. Confidence Interval Estimation
The problem is one of comparing the treatment means µ1, ..., µp with the
control mean µ0 by constructing a (1 – σ) joint confidence region for esti-
mating the parametric function (µ1 – µ0, ..., µp – µ0). Let us denote
The random variables Y1, ..., Yn are obviously iid. Observe that any linear
function of Yi is a linear function of the independent normal variables X1i, X2i,
..., Xpi and X0i. Thus, Y1, ..., Yn are iid p–dimensional normal variables. The
common distribution is given by Np(µ, Σ) where
and Σ = σ2H where
Let us denote 
 i =
0, 1, ..., p. The customary unbiased estimator of σ2 is the pooled sample
variance,
and it is known that (p + 1)(n – 1)S2/σ2 is distributed as 
 with the degree of
freedom v = (p + 1)(n – 1). Also, S2 and 
 are independently
distributed, and hence S2 and 
 are independently distributed. Next, let us
consider the pivot
But, observe that 
 is distributed as Np(0, Σ) where the cor-
relation matrix
Thus, the pivot U has a multivariate t distribution, Mtp(v = (p &plus; 1)(n –
1), Σ), defined in Section 4.6.2. Naturally, we have the case of equicorrelation
ρ where ρ = ½. So, we may determine a positive number h ≡ hv,α which is the
upper equicoordinate 100α% point of the Mtp(v, Σ) distribution in the follow-
ing sense:

9. Confidence Interval Estimation
467
The tables from Cornish (1954, 1962), Dunnett (1955), Dunnett and Sobel
(1954, 1955), Krishnaiah and Armitage (1966), and Hochberg and Tamhane
(1987) provide the values of “h” in various situations. For example, if α = .05
and p = 2, then from Table 5 of Hochberg and Tamhane (1987) we can read
off h = 2.66, 2.54 respectively when n = 4, 5. Next, we simply rephrase
(9.4.13) to make the following joint statements:
The simultaneous confidence intervals given by (9.4.14) jointly have 100(1 –
α)% confidence. !
Example 9.4.5 Suppose that Xi1, ..., Xin are iid random samples from the
N(µi, σ2) population, i = 1, ..., 4. The observations Xi1, ..., Xin, refer to the ith
treatment, i = 1, ..., 4. Let us assume that all the observations from the treat-
ments are independent and that all the parameters are unknown.
Consider, for example, the problem of jointly estimating the parameters θ1
= µ1 – µ2, θ2 = µ2 + µ3 – 2µ4 by means of a simultaneous confidence region.
How should one proceed? Let us denote
The random variables Y1, ..., Yn are obviously iid. Also observe that any
linear function of Yi is a linear function of the independent normal variables
X1i, ..., X4i. Thus, Y1, ..., Yn are iid 2–dimensional normal variables. The
common distribution is given by N2(θ, Σ) where θ ′ = (θ1, θ2) and
Now, along the lines of the Example 9.4.3, one can construct a 100(1 – α)%
joint elliptic confidence region for the parameters θ1, θ2. On the other hand,
one may proceed along the lines of the Example 9.4.4 to derive a 100(1 – α)%
joint confidence intervals for the parameters θ1, θ2. The details are left out as
Exercise 9.4.1.  !
9.4.3
Comparing the Variances
Example 9.4.6 Suppose that Xi1, ..., Xin are iid random samples from the
 population, i = 0, 1, ..., p. The observations X01, ..., X0n refer to
a control population with its mean µ0 and variance σ2
0 whereas Xi1, ..., Xin

468
9. Confidence Interval Estimation
refer to the ith treatment, i = 1, ..., p. Let us also assume that the all observa-
tions from the treatments and control are independent.
The problem is one of comparing the treatment variances σ2
1,...,σ2
p  with
the control variance σ2
0 by constructing a joint confidence region of the para-
metric function 
 Let us denote 
 the ith sample variance which estimates 
 i = 0, 1, ..., p. These sample
variances are all independent and also 
 is distributed as 
 i =
0, 1, ..., p. Next, let us consider the pivot
which has a multivariate F distribution, M Fp(v0, v1, ..., vp) with v0 = v1 = ...
= vp = n – 1, defined in Section 4.6.3. With σ = (σ0, σ1, ..., σp), one may find
a positive number b = bp,n,α such that
The Tables from Finney (1941) and Armitage and Krishnaiah (1964) will pro-
vide the b values for different choices of n and α. Next, we may rephrase
(9.4.18) to make the following joint statements:
The simultaneous confidence intervals given by (9.4.19) jointly have 100(1 –
α)% confidence. !
Example 9.4.7 (Example 9.4.6 Continued) From the pivot and its distribu-
tion described by (9.4.17), it should be clear how one may proceed to obtain
the simultaneous (1 – α) two–sided confidence intervals for the variance
ratios 
 i = 1, ..., p. We need to find two positive numbers a and b , a <
b, such that
so that with σ′ = (s0, s1, ..., sp) we have
Using (9.4.20), one can obviously determine simultaneous (1 – α) two–sided
confidence intervals for all the variance ratios 
 i = 1, ..., p. We leave the
details out as Exercise 9.4.4.

9. Confidence Interval Estimation
469
But, the numerical determination of the numbers a and b is not so simple.
The following result of Hewett and Bulgren (1971) may help:
Now, equating P{a < Fn–1,n–1 < b} with (1 – α)1/p, we may determine ap-
proximate choices of a and b corresponding to equal tails
 (= ½(1 – (1 – α)1/p)) of the Fn–1,n–1 distribution. This approximation works
well when n ≤ 21.
9.5
Exercises and Complements
9.1.1 Suppose that X1, X2 are iid with the common exponential pdf f(x; θ) =
θ–1exp{–x/θ}I(x > 0) where θ(> 0) is the unknown parameter. We are given
α ∈ (0, 1).
(i)
Based on X1 alone, find an appropriate upper (lower) (1 – α) confi-
dence interval for α;
(ii)
Based on X1, X2, find an appropriate upper (lower) and two–sided
(1 – α) confidence interval for θ.
Give comments and compare the different confidence intervals.
9.1.2 Let X have the Laplace pdf f(x; θ) = ½exp{– |x – θ |}I(x ∈ ℜ)
where θ(∈ ℜ) is the unknown parameter. We are given α ∈ (0, 1). Based on
X, find an appropriate upper (lower) and two–sided (1 – α) confidence
interval for θ.
9.1.3 Let X have the Cauchy pdf f(x; θ) = 1/π{1 + (x – θ)2}–1I(x ∈ ℜ)
where θ(∈ ℜ) is the unknown parameter. We are given α ∈ (0, 1). Based on
X, find an appropriate upper (lower) and two–sided (1 – α) confidence inter-
val for θ.
9.1.4 Suppose that X has the Laplace pdf f(x; θ) = 1-2θexp{– |x|/θ}I(x ∈ ℜ)
where θ(∈ ℜ+) is the unknown parameter. We are given α ∈ (0, 1). Based on
X, find an appropriate upper (lower) and two–sided (1 – α) confidence inter-
val for θ.
9.2.1 Suppose that X has N(0, σ2) distribution where σ(∈ ℜ+) is the un-
known parameter. Consider the confidence interval J = (|X|, 10 |X|) for the
parameter σ.
(i)
Find the confidence coefficient associated with the interval J;
(ii)
What is the expected length of the interval J?

470
9. Confidence Interval Estimation
9.2.2 Suppose that X has its pdf f(x; θ) = 2(θ – x)θ–2 I(0 < x < θ) where
θ(∈ ℜ+) is the unknown parameter. We are given α ∈ (0, 1). Consider the
pivot U = X/θ and derive a (1 – α) two–sided confidence interval for θ.
9.2.3 Suppose that X1, ..., Xn are iid Gamma(a, b) where a(> 0) is known
but b(> 0) is assumed unknown. We are given α ∈ (0, 1). Find an appropriate
pivot based on the minimal sufficient statistic and derive a two-sided (1 – α)
confidence interval for b.
9.2.4 Suppose that X1, ..., Xn are iid N(µ, σ2) where µ(∈ ℜ) is known but
σ(∈ ℜ+) is assumed unknown. We are given α ∈ (0, 1). We wish to obtain a
(1 – α) confidence interval for σ.
(i)
Find both upper and lower confidence intervals by inverting appro-
priate UMP level θ tests;
(ii)
Find a two–sided confidence interval by considering an appropriate
pivot based only on the minimal sufficient statistic.
9.2.5 Let X1, ..., Xn be iid with the common negative exponential pdf
f(x; σ) = σ–1 exp{– (x – θ)/σ} I(x > θ). We suppose that θ(∈ ℜ) is known but
σ(∈ ℜ+) is unknown. We are given α ∈ (0, 1). We wish to obtain a (1 – θ)
confidence interval for σ.
(i)
Find both upper and lower confidence intervals by inverting appro-
priate UMP level θ tests;
(ii)
Find a two–sided confidence interval by considering an appropriate
pivot based only on the minimal sufficient statistic.
{Hint: The statistic 
 is minimal sufficient for σ.}
9.2.6 Let X1, ..., Xn be iid with the common negative exponential pdf f(x;
θ, σ) = σ–1 exp{– (x – θ)/σ} I(x > θ). We suppose that both the parameters
θ(∈ ℜ) and σ(∈ ℜ&plus;) are unknown. We are given α ∈ (0, 1). Find a (1 – θ)
two-sided confidence interval for σ by considering an appropriate pivot based
only on the minimal sufficient statistics. {Hint: Can a pivot be constructed
from the statistic 
 where 
 the smallest order
statistic?}
9.2.7 Let X1, ..., Xn be iid with the common negative exponential pdf f(x;
θ, σ) = σ–1 exp{ – (x – θ)/σ} I(x > θ). We suppose that both the parameters
θ(∈ ℜ) and σ(∈ ℜ&plus;) are unknown. We are given α ∈ (0, 1). Derive the
joint (1 – α) two–sided confidence intervals for θ and σ based only on the
minimal sufficient statistics. {Hint: Proceed along the lines of the Example
9.2.10.}
9.2.8 Let X1, ..., Xn be iid Uniform(–θ, θ) where θ(∈ ℜ+) is assumed
unknown. We are given α ∈ (0, 1). Derive a (1 – α) two-sided confidence
interval for θ based only on the minimal sufficient statistics. {Hint: Can

9. Confidence Interval Estimation
471
one justify working with |Xn:n| to come up with a suitable pivot?}
9.2.9 Let X1, ..., Xn be iid having the Rayleigh distribution with the com-
mon pdf f(x; θ) = 2θ–1 xexp(–x2/θ)I(x > 0) where θ(> 0) is the unknown
parameter. We are given α ∈ (0, 1). First find both upper and lower (1 – α)
confidence intervals for θ by inverting appropriate UMP level a tests. Next,
consider an appropriate pivot and hence determine a (1 – α) two-sided confi-
dence interval for θ based only on the minimal sufficient statistic.
9.2.10 Let X1, ..., Xn be iid having the Weibull distribution with the com-
mon pdf f(x; a) = a–1 bxb–1 exp(–xb/a)I(x > 0) where a(> 0) is an unknown
parameter but b(> 0) is assumed known. We are given α ∈ (0, 1). First find
both upper and lower (1 – α) confidence intervals for a by inverting appropri-
ate UMP level α tests. Next, consider an appropriate pivot and hence deter-
mine a (1 – α) two–sided confidence interval for a based only on the minimal
sufficient statistic.
9.2.11 (Example 9.2.10 Continued) Suppose that X1, ..., Xn are iid N(µ,
σ2) with both unknown parameters µ ∈ ℜ and σ ∈ ℜ+, n ≥ 2. Given
α ∈ (0, 1), we had found (1 – α) joint confidence intervals J1, J2 for µ and σ2.
From this, derive a confidence interval for each parametric function given
below with the confidence coefficient at least (1 – α).
(i) µ + σ;
(ii) µ + σ2;
(iii) µ/σ;
(iv) µ/σ2.
9.2.12 (Exercise 9.2.7 Continued) Let X1, ..., Xn be iid with the common
negative exponential pdf f(x; θ, σ) = σ–1 exp{–(x – θ)/σ}I(x > θ). We sup-
pose that both the parameters θ(∈ ℜ) and σ(∈ ℜ+) are unknown. We are
given α ∈ (0, 1). In the Exercise 9.2.7, the joint (1 – α) two–sided confi-
dence intervals for θ and σ were derived. From this, derive a confidence
interval for each parametric function given below with the confidence coef-
ficient at least (1 – α).
(i) θ + σ;
(ii) θ + σ2;
(iii) θ/σ;
(iv) θ/σ2.
9.2.13 A soda dispensing machine automatically fills the soda cans. The
actual amount of fill must not vary too much from the target (12 fluid ounces)
because the overfill will add extra cost to the manufacturer while the underfill
will generate complaints from the customers. A random sample of 15 cans
gave a standard deviation of .008 ounces. Assuming a normal distribution for
the fill, estimate the true population variance with the help of a 95% two–
sided confidence interval.
9.2.14 Ten automobiles of the same make and model were tested by
drivers with similar road habits, and the gas mileage for each was recorded
over a week. The summary results were  = 22 miles per gallon and s = 3.5
miles per gallon. Construct a 90% two–sided confidence interval for the true

472
9. Confidence Interval Estimation
average (µ) gas mileage per gallon. Assume a normal distribution for the gas
mileage.
9.2.15 The waiting time (in minutes) at a bus stop is believed to have an
exponential distribution with mean θ(> 0). The waiting times on ten occasions
were recorded as follows:
6.2
5.8
4.5
6.1
4.6
 4.8
5.3
5.0
3.8
4.0
(i)
Construct a 95% two–sided confidence interval for the true average
waiting time;
(ii)
Construct a 95% two–sided confidence interval for the true vari-
ance of the waiting time;
(iii)
At 5% level, is there sufficient evidence to justify the claim that the
average waiting time exceeds 5 minutes?
9.2.16 Consider a normal population with unknown mean µ and σ = 25.
How large a sample size n is needed to estimate µ within 5 units with 99%
confidence?
9.2.17 (Exercise 9.2.10 Continued) In the laboratory, an experiment was
conducted to look into the average number of days a variety of weed takes to
germinate. Twelve seeds of this variety of weed were planted on a dish. From
the moment the seeds were planted, the time (days) to germination was re-
corded for each seed. The observed data follows:
4.39
6.04
6.43
6.98
2.61
5.87
2.73
7.74
5.31
3.27
4.36
4.61
The team’s expert in areas of soil and weed sciences believed that the time to
germination had a Weibull distribution with its pdf
where a(> 0) is unknown but b = 3. Find a 90% two–sided confidence inter-
val for the parameter a depending on the minimal sufficient statistic.
9.2.18 (Exercise 9.2.17 Continued) Before the lab experiment was con-
ducted, the team’s expert in areas of soil and weed sciences believed that the
average time to germination was 3.8 days. Use the confidence interval found
in the Exercise 9.2.17 to test the expert’s belief. What are the respective null
and alternative hypotheses? What is the level of the test?
9.3.1 (Example 9.3.1 Continued) Suppose that the random variables
Xi, ..., Xini are iid 
 i = 1, 2, and that the X1j’s are independent of
the X2j’s. Here we assume that the means µ1, µ2 are unknown but σ1, σ2
are known, (µi, σi) ∈ ℜ × ℜ+, i = 1, 2. With fixed α ∈ (0, 1), construct a

9. Confidence Interval Estimation
473
(1 – α) two–sided confidence interval for µ1 – µ2 based on sufficient statis-
tics for (µ1, µ2). Is the interval shortest among all (1 – α) two–sided confi-
dence intervals for µ1 – µ2 depending only on the sufficient statistics for
(µ1, µ2)?
9.3.2 (Example 9.3.1 Continued) Suppose that the random variables Xi1,
..., Xini are iid N(µi, kiσ2), ni ≥ 2, i = 1, 2, and that the X1j’s are independent
of the X2j’s. Here we assume that all three parameters µ1, µ2, σ are unknown
but k1, k2 are positive and known, (µ1, µ2, σ) ∈ ℜ × ℜ × ℜ+. With fixed α ∈
(0, 1), construct a (1 – α) two–sided confidence interval for µ1 – µ2 based on
sufficient statistics for (µ1, µ2, σ). Is the interval shortest on the average
among all (1 – α) two–sided confidence intervals for µ1 – µ2 depending on
sufficient statistics for (µ1, µ2, σ)? {Hint: Show that the sufficient statistic is
 Start creating a pivot by
standardizing 
 with the help of an analog of the pooled sample vari-
ance. For the second part, refer to the Example 9.2.13.}
9.3.3 (Example 9.3.1 Continued) Suppose that the random variables Xi1,
..., Xini are iid N(µi, σ2), ni ≥ 2, i = 1, 2, and that the X1j’s are independent of
the X2j’s. Here we assume that all three parameters µ1, µ2, σ are unknown,
(µ1, µ2, σ) ∈ ℜ × ℜ × ℜ+. With fixed α ∈ (0, 1), construct a (1 – α) two-sided
confidence interval for (µ1 – µ2)/σ based on sufficient statistics for (µ1, µ2,
σ). {Hint: Combine the separate estimation problems for (µ1 – µ2) and σ via
the Bonferroni inequality.}
9.3.4 (Example 9.3.2 Continued) Suppose that the random variables Xi1,
..., Xin are iid having the common pdf f(x; µi, σi), i = 1, 2, where we denote
f(x; µ, s) = σ–1 exp{–(x – µ)/σ}I(x > µ). Also let the X1j’s be independent of
the X2j’s. Here we assume that the location parameters µ1, µ2 are unknown
but σ1, σ2 are known, (µi, σi) ∈ ℜ × ℜ+, i = 1, 2. With fixed α ∈ (0, 1),
construct a (1 – α) two–sided confidence interval for µ1 – µ2 based on suffi-
cient statistics for (µ1, µ2).
9.3.5 (Example 9.3.2 Continued) Suppose that the random variables Xi1,
..., Xin are iid having the common pdf f(x; µi, σ), i = 1, 2 and n ≥ 2, where we
denote f(x; µ, σ) = σ–1 exp{–(x – µ)/σ}I(x > µ). Also let the X1j’s be indepen-
dent of the X2j’s. Here we assume that all three parameters are unknown, (µ1,
µ2, σ) ∈ ℜ × ℜ × ℜ+. With fixed α ∈ (0, 1), construct a (1 – α) two-sided
confidence interval for (µ1 – µ2)/σ based on sufficient statistics for (µ1, µ2,
σ). {Hint: Combine the separate estimation problems for (µ1 – µ2) and σ via
the Bonferroni inequality.}
9.3.6 Two types of cars were compared for their braking distances. Test
runs were made for each car in a driving range. Once a car reached the
stable speed of 60 miles per hour, the brakes were applied. The distance

474
9. Confidence Interval Estimation
(feet) each car travelled from the moment the brakes were applied to the
moment the car came to a complete stop was recorded. The summary statis-
tics are shown below:
Car
Sample Size
s
Make A
nA = 12
37.1
3.1
Make B
nB = 10
39.6
4.3
Construct a 95% two–sided confidence interval for µB – µA based on sufficient
statistics. Assume that the elapsed times are distributed as N(µA, σ2) and N(µB,
σ2) respectively for the Make A and B cars with all parameters unknown. Is the
interval shortest on the average among all 95% two–sided confidence intervals
for µB – µA depending on sufficient statistics for (µA, µB, σ)?
9.3.7 In an experiment in nutritional sciences, a principal investigator con-
sidered 8 overweight men with comparable backgrounds which included eat-
ing habits, family traits, health condition and job related stress. A study was
conducted to estimate the average weight reduction for overweight men fol-
lowing a regimen involving nutritional diet and exercise. The technician weighed
in each individual before they entered this program. At the conclusion of the
two–month long study, each individual was weighed. The data follows:
ID# of
Weight (x1, pounds)
Weight (x2, pounds)
Individual
Before Study
After Study
1
235
220
2
189
175
3
156
150
4
172
160
5
165
169
6
180
170
7
170
173
8
195
180
Here, it is not reasonable to assume that in the relevant target population, X1
and X2 are independent. The nutritional scientist believed that the assumption
of a bivariate normal distribution would be more realistic. Obtain a 95% two–
sided confidence interval for µ1 – µ2 where E(Xi) = µi, i = 1, 2.
9.3.8 (Exercise 9.3.7 Continued) Suppose that the scientist believed be-
fore running this experiment that on the average the weight would go down
by at least 10 pounds when such overweight men went through their regimen
of diet and exercise for a period of two months. Can the 95% confidence

9. Confidence Interval Estimation
475
interval found in the Exercise 9.3.7 be used to test the scientist’s belief at 5%
level? If not, then find an appropriate 95% confidence interval for µ1 – µ2 in
the Exercise 9.3.7 and use that to test the hypothesis at 5% level.
9.3.9 Two neighboring towns wanted to compare the variations in the time
(minutes) to finish a 5k–run among the first place winners during each town’s
festivities such as the heritage day, peach festival, memorial day, and other
town–wide events. The following data was collected recently by the town
officials:
Town A (xA):
18
20
17
22
19
18
20
18
17
Town B (xB):
20
17
25
24
18
23
Assume that the performances of these first place winners are independent
and that the first place winning times are normally distributed within each
town’s festivities. Obtain a 90% two–sided confidence interval for σA/σB
based on sufficient statistics for (µA, µB, σA, σB). Can we conclude at 10%
level that σA = σB?
9.3.10 Suppose that the random variables Xi1, ..., Xini are iid having the
common pdf f(x; θi), i = 1, 2, where we denote the exponential pdf f(x; a) =
a–1 exp{–x/a}I(x > 0), a ∈ ℜ+. Also, let the X1j’s be independent of the X2j’s.
We assume that both the parameters θ1, θ2 are unknown, (θ1, θ2) ∈ ℜ+ × ℜ+.
With fixed α ∈ (0, 1), derive a (1 – α) two-sided confidence interval for θ2/θ1
based on sufficient statistics for (θ1, θ2). {Hint: Consider creating a pivot out
of 
9.3.11 (Example 9.3.6 Continued) Suppose that the random variables Xi1,
..., Xin are iid Uniform(0, θi), i = 1, 2. Also, let the X1j’s be independent of the
X2j’s. We assume that both the parameters are unknown, (θ1, θ2) ∈ ℜ+ × ℜ+.
With fixed α ∈ (0, 1), derive a (1 – α) two-sided confidence interval for θ1/
(θ1 + θ2) based on sufficient statistics for (θ1, θ2).
9.4.1 (Example 9.4.5 Continued) Suppose that Xi1, ..., Xin are iid random
samples from the N(µi, σ2) population, i = 1, ..., 4. The observations Xi1, ...,
Xin refer to the ith treatment, i = 1, ..., 4. Let us assume that the treatments are
independent too and that all the parameters are unknown. With fixed α ∈ (0,
1), derive the (1 – α) joint confidence intervals for estimating the parameters
θ1 = µ1 – µ2, θ2 = µ2 + µ3 – 2µ4.
9.4.2 (Example 9.4.4 Continued) Suppose that Xi1, ..., Xin are iid random
samples from the N(µi, σ2) population, i = 1, ..., 5. The observations Xi1, ...,
Xin refer to the ith treatment, i = 1, ..., 5. Let us assume that the treatments
are independent too and that all the parameters are unknown. With fixed α ∈
(0, 1), derive the (1 – α) joint ellipsoidal confidence region for estimating the
parameters θ1 = µ1 – µ2, θ2 = µ2 – µ3 and θ3 = µ3 + µ4 – 2µ5.

476
9. Confidence Interval Estimation
9.4.3 (Exercise 9.4.2 Continued) Suppose that Xi1, ..., Xin are iid random
samples from the N(µi, σ2) population, i = 1, ..., 5. The observations Xi1, ...,
Xin refer to the ith treatment, i = 1, ..., 5. Let us assume that the treatments are
independent too and that all the parameters are unknown. With fixed α ∈ (0,
1), derive the (1 – α) joint confidence intervals for estimating the parameters
θ1 = µ1 – µ2, θ2 = µ2 – µ3 and θ3 = µ3 + µ4 – 2µ5.
9.4.4 Use the equation (9.4.20) to propose the simultaneous (1 – α) two-
sided confidence intervals for the variance ratios.
9.4.5 Let Xi1, ..., Xin be iid having the common pdf f(x; µi, σi), n ≥ 2,
where f(x; µ, σ) = σ–1 exp{–(x – µ)/σ}I(x > µ), (µi, σi) ∈ ℜ × ℜ+, i = 0, 1,
..., p. The observations X01, ..., X0n refer to a control whereas Xi1, ..., Xin
refer to the ith treatment, i = 1, ..., p. Suppose that all the observations are
independent and all the parameters are unknown. Start with an appropriate p–
dimensional pivot depending on the sufficient statistics and then, with fixed α
∈ (0, 1), derive the simultaneous (1 – α) two-sided confidence intervals for
the ratios σi/σ0, i = 1, ..., p. {Hint: Proceed along the lines of the Examples
9.4.6–9.4.7.}
9.4.6 (Exercise 9.3.10 Continued) Suppose that the random variables Xi1,
..., Xin are iid having the common pdf f(x; θi) where we denote the exponen-
tial pdf f(x; a) = a–1 exp{–x/a}I(x > 0), θi > 0, i = 0, 1, ..., p. The observations
X01, ..., X0n refer to a control whereas Xi1, ..., Xin refer to the ith treatment, i
= 1, ..., p. Suppose that all the observations are independent and all the param-
eters are unknown. Start with an appropriate p–dimensional pivot depending
on the sufficient statistics and then, with fixed α ∈ (0, 1), derive the simulta-
neous (1 – α) two-sided confidence intervals for the ratios θi/θ0, i = 1, ..., p.
{Hint: Proceed along the lines of the Examples 9.4.6–9.4.7.}

477
10
Bayesian Methods
10.1 Introduction
The nature of what we are going to discuss in this chapter is conceptually
very different from anything we had included in the previous chapters. Thus
far, we have developed methodologies from the point of view of a frequentist.
We started with a random sample X1, ..., Xn from a population having the pmf
or pdf f(x; ) where x ∈ χ and  ∈ Θ. The unknown parameter v is assumed
fixed. A frequentist’s inference procedures depended on the likelihood func-
tion denoted earlier by 
 where  is unknown but fixed.
In the Bayesian approach, the experimenter believes from the very begin-
ning that the unknown parameter 
 is a random variable having its own
probability distribution on the space Θ. Now that  is assumed random, the
likelihood function will be same as L(θ) given that  = θ. Let us denote the
pmf or pdf of  by h(θ) at the point  = θ which is called the prior distribu-
tion of .
In previous chapters, we wrote f(x; θ) for the pmf or pdf of X.
Now, f(x; θ) denotes the conditional pmf or pdf of X given  = θ.
The unknown parameter v is assumed a random variable with its
own distribution h(θ) when  = θ ∈ Θ, the parameter space.
The prior distribution h(θ) often reflects an experimenter’s subjective be-
lief regarding which v values are more (or less) likely when one considers the
whole parameter space Θ. The prior distribution is ideally fixed before the
data gathering begins. An experimenter may utilize related expertise and other
knowledge in order to come up with a realistic prior distribution h(θ).
The Bayesian paradigm requires one to follow along these lines: perform
all statistical inferences and analysis after combining the information about
 contained in the collected data as evidenced by the likelihood function
L(θ) given that  = θ, as well as that from the prior distribution h(θ). One
combines the evidences about v derived from both the prior distribution and
the likelihood function by means of the Bayes’s Theorem (Theorem 1.4.3).
After combining the information from these two sources, we come up

478
10. Bayesian Methods
with what is known as a posterior distribution. All Bayesian inferences are
then guided by this posterior distribution. This approach which originated
from Bayes’s Theorem was due to Rev. Thomas Bayes (1783). A strong
theoretical foundation evolved due to many fundamental contributions of de
Finetti (1937), Savage (1954), and Jeffreys (1957), among others. The con-
tributions of L. J. Savage were beautifully synthesized in Lindley (1980).
From the very beginning, R. A. Fisher was vehemently opposed to any-
thing Bayesian. Illuminating accounts of Fisher’s philosophical arguments as
well as his interactions with some of the key Bayesian researchers of his time
can be found in the biography written by his daughter, Joan Fisher Box (1978).
Some interesting exchanges between R. A. Fisher and H. Jeffreys as well as
L. J. Savage are included in the edited volume of Bennett (1990). The articles
of Buehler (1980), Lane (1980) and Wallace (1980) gave important perspec-
tives on possible connections between Fisher’s fiducial inference and the Baye-
sian doctrine.
A branch of statistics referred to as “decision theory” provides a formal
structure to find optimal decision rules whenever possible. Abraham Wald
gave the foundation to this important area during the early to mid 1940’s.
Wald’s (1950) book, Statistical Decision Functions, is considered a classic in
this area. Berger (1985) treated modern decision theory with much emphasis
on Bayesian arguments. Ferguson (1967), on the other hand, gave a more
balanced view of the area. The titles on the cover of these two books clearly
emphasize this distinction.
In Section 10.2, we give a formal discussion of the prior and posterior
distributions. The Section 10.3 first introduces the concept of conjugate pri-
ors and then posterior distributions are derived in some standard cases when
 has a conjugate prior. In this section, we also include an example where the
assumed prior for  is not chosen from a conjugate family. In Section 10.4,
we develop the point estimation problems under the squared error loss func-
tion and introduce the Bayes estimator for . In the same vein, Section 10.5.1
develops interval estimation problems. These are customarily referred to as
credible interval estimators of v. Section 10.5.2 highlights important concep-
tual differences between a credible interval estimator and a confidence inter-
val estimator. Section 10.6 briefly touches upon the concept of a Bayes test of
hypotheses whereas Section 10.7 gives some examples of Bayes estimation
under non-conjugate priors.
We can not present a full-blown discussion of the Bayes theory at this
level. It is our hope that the readers will get a taste of the underlying basic
principles from a brief exposure to this otherwise vast area.

10. Bayesian Methods
479
10.2 Prior and Posterior Distributions
The unknown parameter  itself is assumed to be a random variable having
its pmf or pdf h(θ) on the space Θ. We say that h(θ) is the prior distribution
of v. In this chapter, the parameter  will represent a continuous real valued
variable on Θ which will customarily be a subinterval of the real line ℜ. Thus
we will commonly refer to h(θ) as the prior pdf of .
The evidence about v derived from the prior pdf is then combined with
that obtained from the likelihood function by means of the Bayes’s Theorem
(Theorem 1.4.3). Observe that the likelihood function is the conditional joint
pmf or pdf of X = (X1, ..., Xn) given that  = θ. Suppose that the statistic T is
(minimal) sufficient for θ in the likelihood function of the observed data X
given that  = θ. The (minimal) sufficient statistic T will be frequently real
valued and we will work with its pmf or pdf g(t; θ), given that v = θ, with t ∈
Τ where T is an appropriate subset of ℜ.
For the uniformity of notation, however, we will treat T as a continuous
variable and hence the associated probabilities and expectations will be writ-
ten in the form of integrals over the space T. It should be understood that
integrals will be interpreted as the appropriate sums when T is a discrete
variable instead.
The joint pdf of T and v is then given by
The marginal pdf of T will be obtained by integrating the joint pdf from (10.2.1)
with respect to θ. In other words, the marginal pdf of T can be written as
Thus, we can obtain the conditional pdf of v given that T = t as follows:
In passing, let us remark that the expression for k(θ; t) follows directly by
applying the Bayes’s Theorem (Theorem 1.4.3). In the statement of the Bayes’s
Theorem, simply replace 
 by k(θ; t),
h(θ), g(t; θ) and 
 respectively.
Definition 10.2.1 The conditional pdf k(θ; t) of 
 given that T = t is
called the posterior distribution of .

480
10. Bayesian Methods
Under the Bayesian paradigm, after having the likelihood function and the
prior, the posterior pdf k(θ;t) of v epitomizes how one combines the informa-
tion about 
 obtained from two separate sources, the prior knowledge and
the collected data. The tractability of the final analytical expression of k(θ;t)
largely depends on how easy or difficult it is to obtain the expression of m(t).
In some cases, the marginal distribution of T and the posterior distribution
can be evaluated only numerically.
Example 10.2.1 Let X1, ..., Xn be iid Bernoulli(θ) given that v = θ where v
is the unknown probability of success, 0 < v < 1. Given v = θ, the statistic
 is minimal sufficient for θ, and one has
where T = {0, 1, ..., n}. Now, suppose that the prior distribution of v on the
space Θ = (0, 1) is taken to be Uniform(0, 1), that is h(θ) = I(0 < θ < 1). From
(10.2.2), for t ∈ T, we then obtain the marginal pmf of T as follows:
Thus, for any fixed value t ∈ T, using (10.2.3) and (10.2.5), the posterior pdf
of v given the data T = t can be expressed as
That is, the posterior distribution of the success probability v is described by
the Beta(t + 1, n - t + 1) distribution. !
In principle, one may carry out similar analysis even if the
unknown parameter v is vector valued. But, in order to keep
the presentation simple, we do not include any such example
here. Look at the Exercise 10.3.8 for a taste.
Example 10.2.2 (Example 10.2.1 Continued) Let X1, ..., X10 be iid
Bernoulli(θ) given that  = θ where v is the unknown probability of suc-
cess, 0 < v < 1. The statistic 
 is minimal sufficient for θ given
that  = θ. Assume that the prior distribution of  on the space Θ = (0, 1)
is Uniform(0, 1). Suppose that we have observed the particular value T = 7,

10. Bayesian Methods
481
the number of successes out of ten Bernoulli trials. From (10.2.6) we know
that the posterior pdf of the success probability  is that of the Beta(8, 4)
distribution. This posterior density has been plotted as a solid curve in the
Figure 10.2.1. This curve is skewed to the left. If we had observed the data T
= 5 instead, then the posterior distribution of the success probability v will be
described by the Beta(6, 6) distribution. This posterior density has been plot-
ted as a dashed curve in the same Figure 10.2.1. This curve is symmetric
about θ = .5. The Figure 10.2.1 clearly shows that under the same uniform
prior
Figure 10.2.1. Posterior PDF’s of v Under Two Prior
Distributions of v When t = 7
regarding the success probability  but with different observed data, the shape
of the posterior distribution changes one’s perception of what values of  are
more (or less) probable. !
10.3 The Conjugate Priors
If the prior h(θ) is such that the integral in the equation (10.2.2) can not be
analytically found, then it will be nearly impossible to derive a clean expres-
sion for the posterior pdf k(θ; t). In the case of many likelihood functions, we
may postulate a special type of prior h(θ) so that we can achieve technical
simplicity.
Definition 10.3.1 Suppose that the prior pdf h(θ) for the unknown pa-
rameter v belongs to a particular family of distributions, P. Then, h(θ) is
called a conjugate prior for 
 if and only if the posterior pdf k(θ; t) also
belongs to the same family P.
What it means is this: if h(θ) is chosen, for example, from the family of

482
10. Bayesian Methods
beta distributions, then k(θ; t) must correspond to the pdf of an appropriate
beta distribution. Similarly, for conjugacy, if h(θ) is chosen from the family of
normal or gamma distributions, then k(θ; t) must correspond to the pdf of an
appropriate normal or gamma distribution respectively. The property of
conjugacy demands that h(θ) and k(θ; t) must belong to the same family of
distributions.
Example 10.3.1 (Example 10.2.1 Continued) Suppose that we have the
random variables X1, ..., Xn which are iid Bernoulli(θ) given that  = θ, where
 is the unknown probability of success, 0 <  < 1. Given that  = θ, the
statistic 
 is minimal sufficient for θ, and recall that one has
 for t ∈ T = {0, 1, ..., n}.
In the expression for g(t; θ), carefully look at the part which depends on θ,
namely θ t(1 - θ)n-t. It resembles a beta pdf without the normalizing constant.
Hence, we suppose that the prior distribution of v on the space Θ = (0, 1) is
Beta(α, β) where α (> 0) and β(> 0) are known numbers. From (10.2.2), for
t ∈ T, we then obtain the marginal pmf of T as follows:
Now, using (10.2.3) and (10.3.1), the posterior pdf of v given the data T = t
simplifies to
and fixed values t ∈ T. In other words, the posterior pdf of the success
probability v is same as that for the Beta(t + α, n – t + β) distribution.
We started with the beta prior and ended up with a beta posterior.
Here, the beta pdf for v is the conjugate prior for .
In the Example 10.2.1, the uniform prior was actually the Beta(1, 1) distri-
bution. The posterior found in (10.3.2) agrees fully with that given by (10.2.6)
when α = β = 1. !
Example 10.3.2 Let X1, ..., Xn be iid Poisson(θ) given that  = θ, where
(> 0) is the unknown population mean. Given that 
 = θ, the statistic
 is minimal sufficient for θ, and recall that one has g(t; θ) = e-
n?(nθ)t/t! for t ∈ T = {0, 1, 2, ...}.
In the expression of g(t; θ), look at the part which depends on θ, namely
e-nθ?t. It resembles a gamma pdf without the normalizing constant. Hence,
we suppose that the prior distribution of  on the space Θ = (0, ∞) is

10. Bayesian Methods
483
Gamma(α, β) where α(> 0) and β(> 0) are known numbers. From (10.2.2),
for t ∈ T, we then obtain the marginal pmf of T as follows:
Now, using (10.2.3) and (10.3.3), the posterior pdf of  given the data T = t
simplifies to
and fixed values t ∈ T. In other words, the posterior pdf of  is the same as
that for the Gamma(t + α, β(nβ + 1)–1) distribution.
We plugged in a gamma prior and ended up with a gamma posterior.
In this example, observe that the gamma pdf for  is the conjugate prior
for 
. !
Example 10.3.3 (Example 10.3.1 Continued) Suppose that we have the
random variables X1, ..., Xn which are iid Bernoulli(θ) given that  = θ, where
 is the unknown probability of success, 0 <  < 1. As before, consider the
statistic 
 which is minimal sufficient for θ given that  = θ. Let
us assume that the prior distribution of  on the space Θ = (0, 1) is described
as Beta(α, β) where α(> 0) and β(> 0) are known numbers. In order to find
the posterior distribution of , there is no real need to determine m(t) first.
The joint distribution of ( , T) is given by
for 0 < θ < 1, t = 0, 1, ..., n. Now, upon close examination of the rhs of
(10.3.5), we realize that it does resemble a beta density without its normaliz-
ing constant. Hence, the posterior pdf of the success probability  is going to
be that of the Beta(t + α, n – t + β) distribution. !
Example 10.3.4 Let X1, ..., Xn be iid Poisson(θ) given that  = θ, where
(> 0) is the unknown population mean. As before, consider the statistic
 which is minimal sufficient for θ given that  = θ. Let us sup-
pose that the prior distribution of v on the space Θ = (0, ∞) is Gamma(α, β)
where α(> 0) and β(> 0) are known numbers. In order to find the posterior
distribution of , again there is no real need to determine m(t) first. The joint
distribution of ( , T) is given by

484
10. Bayesian Methods
for 0 < θ < ∞. Now, upon close examination of the rhs of (10.3.6), we realize
that it does resemble a gamma density without its normalizing constant. Hence,
the posterior pdf of v is same as that for the Gamma(t &plus; a, β(nβ + 1)–1)
distribution. !
For this kind of analysis to go through, the observations X1, ..., Xn
do not necessarily have to be iid to begin with. Look at the
Exercises 10.3.1, 10.3.7, 10.4.2, 10.5.1 and 10.5.8.
Example 10.3.5 Let X1, ..., Xn be iid N(θ, σ2) given that  = θ, where v(∈
ℜ) is the unknown population mean and σ(> 0) is assumed known. Consider
the statistic 
 which is minimal sufficient for θ given that v = θ.
Let us suppose that the prior distribution of  on the space Θ = ℜ is N(T, δ2)
where τ (∈ ℜ) and δ (> 0) are known numbers. In order to find the posterior
distribution of , again there is no real need to determine m(t) first. The joint
distribution of ( , T) is proportional to
for θ ∈ ℜ. Now, upon close examination of the last expression in (10.3.7), we
realize that it does resemble a normal density without its normalizing constant.
We started with a normal prior and ended up in a normal posterior.
With 
 the posterior
distribution of 
 turns out to be . In this example again, observe that the
normal pdf for  is the conjugate prior for . !
A conjugate prior may not be reasonable in every problem.
Look at the Examples 10.6.1-10.6.4.
In situations where the domain space T itself depends on
the unknown parameter , one needs to be very careful in
the determination of the posterior distribution. Look at the
Exercises 10.3.4-10.3.6, 10.4.5-10.4.7 and 10.5.4-10.5.7.
The prior h(θ) used in the analysis reflects the experimenter’s subjec-
tive belief regarding which -subsets are more (or less) likely. An experi-
menter may utilize hosts of related expertise to arrive at a realistic prior
distribution h(θ). In the end, all types of Bayesian inferences follow from the

10. Bayesian Methods
485
posterior pdf k(θ; t) which is directly affected by the choice of h(θ). This is
why it is of paramount importance that the prior pmf or pdf h(θ) is fixed in
advance of data collection so that both the evidences regarding  obtained
from the likelihood function and the prior distribution remain useful and cred-
ible.
10.4 Point Estimation
In this section, we explore briefly how one approaches the point estimation
problems of an unknown parameter  under a particular loss function. Recall
that the data consists of a random sample X = (X1, ..., Xn) given that = θ.
Suppose that a real valued statistic T is (minimal) sufficient for θ given that 
= θ. Let T denote the domain of t. As before, instead of considering the
likelihood function itself, we will only consider the pmf or pdf g(t; θ) of the
sufficient statistic T at the point T = t given that v = θ, for all t ∈ T. Let h(θ)
be the prior distribution of , θ ∈ Θ.
An arbitrary point estimator of  may be denoted by δ ≡ δ(T) which takes
the value δ(t) when one observes T = t, t ∈ T. Suppose that the loss in estimat-
ing  by the estimator θ(T) is given by
which is referred to as the squared error loss.
The mean squared error (MSE) discussed in Section 7.3.1 will correspond
to the weighted average of the loss function from (10.4.1) with respect to the
weights assigned by the pmf or pdf g(t; θ). In other words, this average is
actually a conditional average given that  = θ. Let us define, conditionally
given that  = θ, the risk function associated with the estimator θ:
This is the frequentist risk which was referred to as MSEd in the Section 7.3.
In Chapter 7, we saw examples of estimators δ1 and δ2 with risk functions
R*(θ, δi), i = 1, 2 where R*(θ, δ1) > R*(θ, δ2) for some parameter values θ
whereas R*(θ, δ1) ≤ R*(θ, δ2) for other parameter values θ. In other words,
by comparing the two frequentist risk functions of δ1 and δ2, in some situa-
tions one may not be able to judge which estimator is decisively superior.
The prior h(θ) sets a sense of preference and priority of some values of
v over other values of 
. In a situation like this, while comparing two
estimators δ1 and δ2, one may consider averaging the associated frequentist

486
10. Bayesian Methods
risks R*(θ, δi), i = 1, 2, with respect to the prior h(θ) and then check to see
which weighted average is smaller. The estimator with the smaller average
risk should be the preferred estimator.
So, let us define the Bayesian risk (as opposed to the frequentist risk)
Suppose that D is the class of all estimators of  whose Bayesian risks are
finite. Now, the best estimator under the Bayesian paradigm will be δ* from D
such that
Such an estimator will be called the Bayes estimator of . In many standard
problems, the Bayes estimator δ* happens to be unique.
Let us suppose that we are going to consider only those estimators δ
and prior h(θ) so that both R*(θ, δ) and r*(v, δ) are finite, θ ∈ Θ.
Theorem 10.4.1 The Bayes estimator δ* = δ*(T) is to be determined in
such a way that the posterior risk of δ*(t) is the least possible, that is
for all possible observed data t ∈ T.
Proof Assuming that m(t) > 0, let us express the Bayesian risk in the
following form:
In the last step, we used the relation g(t; θ)h(θ) = k(t; θ)m(t) and the fact that
the order of the double integral ∫Θ ∫T can be changed to ∫T ∫Θ because the inte-
grands are non-negative. The interchanging of the order of the integrals is
allowed here in view of a result known as Fubini’s Theorem which is stated
as Exercise 10.4.10 for the reference.
Now, suppose that we have observed the data T = t. Then, the Bayes
estimate δ*(t) must be the one associated with the 
that is the smallest posterior risk. The proof is complete. !
An attractive feature of the Bayes estimator δ* is this: Having observed
T = t, we can explicitly determine δ*(t) by implementing the process of
minimizing the posterior risk as stated in (10.4.5). In the case of the squared

10. Bayesian Methods
487
error loss function, the determination of the Bayes estimator happens to be
very simple.
Theorem 10.4.2 In the case of the squared error loss function, the Bayes
estimate δ* ≡ δ*(t) is the mean of the posterior distribution k(θ; t), that is
for all possible observed data t ∈ T.
Proof In order to determine the estimator δ*(T), we need to minimize
∫ΘL*(θ, δ(t))k(δ; t)dδ with respect to δ, for every fixed t ∈ T. Let us now
rewrite
where we denote 
 In (10.4.8) we used the fact that
∫Θk(δ; t)dδ = 1 because k(δ; t) is a probability distribution on Θ. Now, we look
at the expression a(t) – 
 as a function of δ ≡ δ(t) and wish to
minimize this with respect to δ. One can accomplish this task easily. We leave
out the details as an exercise. !
Example 10.4.1 (Example 10.3.1 Continued) Suppose that we have the
random variables X1, ..., Xn which are iid Bernoulli(θ) given that  = θ, where
 is the unknown probability of success, 0 <  < 1. Given that  = θ, the
statistic 
 is minimal sufficient for θ. Suppose that the prior distri-
bution of  is Beta(α, β) where α(> 0) and β(> 0) are known numbers. From
(10.3.2), recall that the posterior distribution of v given the data T = t happens
to be Beta(t + α, n – t + β) for t ∈ T = {0, 1, ..., n}. In view of the Theorem
10.4.2, under the squared error loss function, the Bayes estimator of  would
be the mean of the posterior distribution, namely, the mean of the Beta(t + α,
n – t + β) distribution. One can check easily that the mean of this beta distri-
bution simplifies to (t + α)/(α + β + n) so that we can write:
Now, we can rewrite the Bayes estimator as follows:
From the likelihood function, that is given  = θ, the maximum likelihood
estimate or the UMVUE of θ would be 
 whereas the mean of the prior

488
10. Bayesian Methods
distribution is α/(α + β). The Bayes estimator is an appropriate weighted
average of  and α/(α + β). If the sample size n is large, then the classical
estimator  gets more weight than the mean of the prior belief, namely α/(α
+ β) so that the observed data is valued more. For small sample sizes, gets
less weight and thus the prior information is valued more. That is, for large
sample sizes, the sample evidence is weighed in more, whereas if the sample
size is small, the prior mean of  is trusted more. !
Example 10.4.2 (Example 10.3.5 Continued) Let X1, ..., Xn be iid N(θ, σ2)
given that v = θ, where (∈ ℜ) is the unknown population mean and σ(> 0)
is assumed known. Consider the statistic 
 which is minimal suf-
ficient for θ given that  = θ. Let us suppose that the prior distribution of v is
N(τ, δ2) where τ(∈ ℜ) and β(> 0) are known numbers. From the Example
10.3.5, recall that the posterior distribution of 
 is 
 where
 In view of the Theorem
10.4.2, under the squared error loss function, the Bayes estimator of  would
be the mean of the posterior distribution, namely, the  
 distribution. In
other words, we can write:
Now, we can rewrite the Bayes estimator as follows:
From the likelihood function, that is given  = θ, the maximum likelihood
estimate or the UMVUE of θ would be 
 whereas the mean of the prior
distribution is τ. The Bayes estimate is an appropriate weighted average of 
and τ. If n/σ2 is larger than 1/δ2, that is if the classical estimate  is more
reliable (smaller variance) than the prior mean τ, then the sample mean  gets
more weight than the prior belief. When n/σ2 is smaller than 1/δ2, the prior
mean of v is trusted more than the sample mean . !
10.5 Credible Intervals
Let us go back to the basic notions of Section 10.2. After combining the
information about  gathered separately from two sources, namely the ob-
served data X = x and the prior distribution h(θ), suppose that we have ob-
tained the posterior pmf or pdf k(θ; t) of  given T = t. Here, the statistic T is
assumed to be (minimal) sufficient for θ, given that  = θ.
Definition 10.5.1 With fixed α, 0 < α < 1, a subset Θ* of the parameter
space Θ is called a 100(1 – α)% credible set for the unknown parameter 

10. Bayesian Methods
489
if and only if the posterior probability of the subset Θ* is at least (1 – α), that
is
In some situations, a 100(1 – α)% credible set Θ*(⊂ Θ) for the unknown
parameter  may not be a subinterval of Θ. The geometric structure of Θ*
will largely depend upon the nature of the prior pdf h(θ) on Θ. When Θ* is an
interval, we naturally refer to it as a 100(1 – α)% credible interval for .
10.5.1
Highest Posterior Density
It is conceivable that one will have to choose a 100(1 – α)% credible set Θ*
from a long list of competing credible sets for v. Naturally, we should include
only those values of v in a credible set which are “very likely” according to
the posterior distribution. Also, the credible set should be “small” because we
do not like to remain too uncertain about the value of v. The concept of the
highest posterior density (HPD) credible set combines these two ideas.
Definition 10.5.2 With some fixed α, 0 < α < 1, a subset Θ* of the param-
eter space Θ is called a 100(1 – α)% highest posterior density (HPD) credible
set for the unknown parameter v if and only if the subset Θ* has the following
form:
where a ≡ a(α, t) is the largest constant such that the posterior probability of
Θ is at least (1 – α).
Now, suppose that the posterior density k(θ; t) is unimodal. In this case, a
100(1 – α)% HPD credible set happens to be an interval. If we assume that
the posterior pdf is symmetric about its finite mean (which would coincide
with the its median too), then the HPD 100(1 – α)% credible interval Θ* will
be the shortest as well as equal tailed and symmetric about the posterior mean.
In order to verify this conclusion, one may apply the Theorem 9.2.1 in the
context of the posterior pdf k(θ; t).
Example 10.5.1 (Examples 10.3.5 and 10.4.2 Continued) Let X1, ...,
Xn be iid N(θ, σ2) given that  = θ where (∈ ℜ) is the unknown popula-
tion mean and s(> 0) is assumed known. Consider the statistic
 which is minimal sufficient for θ given that  = θ. Let us
suppose that the prior distribution of v is N(τ, δ2) where τ(∈ ℜ) and δ(>
0) are known numbers. From the Example 10.3.5, recall that the posterior distribution

490
10. Bayesian Methods
of 
 is 
 where 
Also, from the Example 10.4.2, recall that under the squared error loss func-
tion, the Bayes estimate of v would be the mean of the posterior distribution,
namely 
 Let zα/2 stands for the upper 100(α/
2)% point of the standard normal distribution. The posterior distribution N(µ,
σ2
0) being symmetric about µ, the HPD 100(1 – α)% credible interval Θ* will
become
This interval is centered at the posterior mean 
 and it stretches either way
by za/2 times the posterior standard deviation σ0. !
Example 10.5.2 (Example 10.5.1 Continued) In an elementary statistics
course with large enrollment, the instructor postulated that the midterm ex-
amination score (X) should be distributed normally with an unknown mean θ
and variance 30, given that  = θ. The instructor assumed the prior distribu-
tion N(73, 20) for  and then looked at the midterm examination scores of n
= 20 randomly selected students. The observed data follows:
85
78
87
92
66
59
88
61
59
78
82
72
75
79
63
67
69
77
73
81
One then has 
 We wish to construct a 95% HPD credible interval
for the population average score v so that we have zα/2 = 1.96. The posterior
mean and variance are respectively
From (10.5.3), we claim that the 95% HPD credible interval for  would be
which will be approximately the interval (72.127, 76.757). !
Example 10.5.3 (Example 10.4.1 Continued) Suppose that we have
the random variables X1, ..., Xn which are iid Bernoulli(θ) given that v = θ
where v is the unknown probability of success, 0<  <1. Given that v = θ,
the statistic 
 is minimal sufficient for θ. Suppose that the prior
distribution of  is Beta(α, β) where α(>0) and β(> 0) are known num-
bers. From (10.3.2), recall that the posterior distribution of v is
Beta(t + α, n – t + β) for t ∈ T = {0, 1, ..., n}. Using the Definition 10.5.2,

10. Bayesian Methods
491
one can immediately write down the HPD 100(1 – α)% credible interval Θ*
for : Let us denote
with the largest positive number a such that P{Θ* |T = t) ≥ 1 – α. !
Example 10.5.4 (Example 10.5.3 Continued) In order to gather infor-
mation about a new pain-killer, it was administered to n = 10 comparable
patients suffering from the same type of headache. Each patient was checked
after one-half hour to see if the pain was gone and we found seven patients
reporting that their pain vanished. It is postulated that we have observed the
random variables X1, ..., X10 which are iid Bernoulli(θ) given that 
 = θ
where 0 <  < 1 is the unknown probability of pain relief for each patient, 0
<  < 1. Suppose that the prior distribution of  was fixed in advance of
data collection as Beta(2, 6). Now, we have observed seven patients who
got the pain relief. From (10.5.5), one can immediately write down the form
of the HPD 95% credible interval Θ* for  as
with the largest positive number a such that P{Θ* |T = t) = 1 – α. Let us
rewrite (10.5.6) as
We will choose the positive number b in (10.5.7) such that P{Θ*(b) |T = t) =
1 – α. We can simplify (10.5.7) and equivalently express, for 0 < b < ¼,
Now, the posterior probability content of the credible interval Θ*(b) is given
by
We have given a plot of the function q(b) in the Figure 10.5.1. From this
figure, we can guess that the posterior probability of the credible interval

492
10. Bayesian Methods
Θ*(b) is going to be 95% when we choose b ≈ .2.
Figure 10.5.1. Plot of the Probability Content q(b) from (10.5.9)
With the help of MAPLE, we numerically solved the equation q(b) = .95 and
found that b ≈ .20077. In other words, (10.5.8) will lead to the HPD 95%
credible interval for v which is (.27812, .72188). !
10.5.2
Contrasting with the Confidence Intervals
Now, we emphasize important conceptual differences in the notions of a 100
(1–α)% confidence interval and a 100(1–α)% (HPD) credible interval for an
unknown parameter . In either case, one starts with an appropriate likeli-
hood function. In the case of a confidence interval, the unknown parameter 
is assumed fixed. In the case of a credible interval, however, the unknown
parameter  is assumed to be a random variable having some pdf h(θ) on Θ.
Under the frequentist paradigm, we talk about the unknown parameter 
belonging to a constructed confidence interval, only from the point of view of
limiting relative frequency idea. Recall the interpretation given in Section 9.2.3.
On the other hand, under the Bayesian paradigm we can evaluate the poste-
rior probability that v belongs to a credible interval by integrating the posterior
pdf on the interval. In this sense, the confidence coefficient may be regarded
as a measure of initial accuracy which is fixed in advance by the experi-
menter whereas the posterior probability content of a credible interval may be
regarded as the final accuracy. The latter conclusion may look more appeal-
ing in some applications but we may also remind the reader that a HPD cred-
ible interval arises at the expense of assuming that (i) the unknown parameter
 is a random variable and (ii)  has a known prior distribution h(α) for α ∈
Θ.
Example 10.5.5 (Example 10.5.2 Continued) With the sample of
size n = 20 midterm examination scores, we had 
 and σ2 = 30. So

10. Bayesian Methods
493
a 95% confidence interval for the population mean 
 would be 74.55 ±
 which will lead to the interval (72.15, 76.95). It is not meaning-
ful for a frequentist to talk about the chance that the unknown parameter 
belongs or does not belong to the interval (72.15, 76.95) because  is not a
random variable. In order to make any such statement meaningful, the phrase
“chance” would have to be tied in with some sense of randomness about v in
the first place! A frequentist will add, however, that in a large collection of
such confidence intervals constructed in the same fashion with n = 20, ap-
proximately 95% of the intervals will include the unknown value of .
On the other hand, the 95% HPD credible interval for  turned out to be
(72.127, 76.757). Here, one should have no confusion about the interpreta-
tion of this particular interval’s “probability” content. The posterior pdf puts
exactly 95% posterior probability on this interval. This statement is verifiable
because the posterior is completely known once the observed value of the
sample mean becomes available.
In this example, one may be tempted to say that the credible interval is
more accurate than the confidence interval because it is shorter of the two.
But, one should be very careful to pass any swift judgement. Before one
compares the two intervals, one should ask: Are these two interval estimates
comparable at all, and if they are, then in what sense are they comparable?
This sounds like a very simple question, but it defies any simple answer. We
may mention that philosophical differences between the frequentist and Baye-
sian schools of thought remains deep-rooted from the days of Fisher and
Jeffreys. !
10.6 Tests of Hypotheses
We would like to recall that a concept of the p-value associated with the
Neyman-Pearson’s frequentist theory of tests of hypotheses was briefly dis-
cussed in Section 8.4.1. In a Bayesian framework, one may argue that mak-
ing a decision to reject or accept H0 based on the p-value is not very reason-
able. Berger et al. (1997) gave an interesting exposition. A Bayes test of hy-
potheses is then formulated as follows.
Let us consider iid real valued random variables X1, ..., Xn having a
common pmf or pdf f(x; v) where the unknown parameter 
 takes the
values θ ∈ Θ (⊆ ℜ). We wish to test a null hypothesis H0 : v ∈ Θ0 against
an alternative hypothesis H1 :  ∈ Θ1 where Θ0 ∩ Θ1 = ϕ, Θ0 ∪ Θ1 ⊆ Θ.
Given = θ, we first write down the likelihood function 
Θ and obtain the (minimal) sufficient statistic T ≡ T(X) which is assumed

494
10. Bayesian Methods
real valued. Let us continue to denote the prior of  and the pmf or pdf of T
given  = θ by h(θ) and g(t; θ) respectively.
Next, one obtains the posterior distribution k(θ; t) as in (10.2.3) and pro-
ceeds to evaluate the posterior probability of the null and alternative spaces
Θ0, Θ1 respectively as follows:
We assume that both α0, α1 are positive and view these as the posterior
evidences in favor of H0, H1 respectively.
We refrain from giving more details. One may refer to Ferguson (1967) and
Berger (1985).
Example 10.6.1 (Example 10.3.5 Continued) Let X1, ..., X5 be iid N(θ, 4)
given that 
 = θ where 
(∈ ℜ) is the unknown population mean. Let us
suppose that the prior distribution of v on the space Θ = ℜ is N(3, 1). We wish
to test a null hypothesis H0 :  < 1 against an alternative hypothesis H1 :  >
2.8. Consider the statistic 
 which is minimal sufficient for θ given
that  = θ. Suppose that the observed value of T is t = 6.5.
With µ = (6.5--
4 + 3) (5-4 + 1)-1 ≈ 2. 0556 and 
 the
posterior distribution of  turns out to be 
 Let us denote a random
variable Y which is distributed as 
 and Z = (Y – µ)/σ0.
Now, from (10.6.1), we have α0 = P(Y < 1) = P(Z < -1. 5834) ≈ .0 5 6665
and α1 = P(Y > 2.8) = P(Z > 1. 1166) ≈ . 13208. Thus, the Bayes test from
(10.6.2) will reject H0. !
So far, we have relied heavily upon conjugate priors. But, in some situa-
tions, a conjugate prior may not be available or may not seem very appealing.
The set of examples in the next section will highlight a few such scenarios.
10.7 Examples with Non-Conjugate Priors
The following examples exploit some specific non-conjugate priors. Certainly
these are not the only choices of such priors. Even though the chosen priors
are non-conjugate, we are able to derive analytically the posterior and Bayes
estimate.
Example 10.7.1 Let X be N(θ, 1) given that  
=θ where 
 is the
unknown population mean. Consider the statistic T = X which is minimal

10. Bayesian Methods
495
sufficient for θ given that  = θ. We have been told that  is positive and so
there is no point in assuming a normal prior for the parameter . For simplic-
ity, let us suppose that the prior distribution of 
 on the space Θ = ℜ+ is
exponential with known mean α–1(> 0). Now, let us first proceed to deter-
mine the marginal pdf m(x). The joint distribution of (v, X) is given by
Recall the standard normal pdf 
 and the df
 with y ∈ ℜ. Thus, for all x ∈ ℜ, the marginal pdf of X
can be written as
Now, combining (10.7.1)-(10.7.2), we obtain the posterior pdf of v given
that X = x as follows: For all x ∈ ℜ, θ ∈ ℜ+,
In this example, the chosen prior is not a conjugate one and yet we have been
able to derive the expression of the posterior pdf in a closed form. In the
Exercise 10.7.1 we ask the reader to check by integration that k(θ; x) is
indeed a pdf on the positive half of the real line. Also refer to the Exercises
10.7.2-10.7.3. !
Example 10.7.2 (Example 10.7.1 Continued) Let X be N(θ, 1) given that
 = θ where v is the unknown population mean. Consider the statistic T = X
which is minimal sufficient for θ given that  = θ. We were told that  was
positive and we took the exponential distribution with the known mean α–1(>
0) as our prior for .
Let us recall the standard normal pdf 
 and the
df 
 for y ∈ ℜ. Now, from the Example 10.7.1, we know
that the posterior distribution of v is given by k(θ; x) = {Φ ([x – α])}–1 φ(θ –
[x – α]) for θ > 0, –∞ < x < ∞.

496
10. Bayesian Methods
In view of the Theorem 10.4.2, under the squared error loss function, the
Bayes estimate of  would be the mean of the posterior distribution. In other
words, one can write the Bayes estimate 
 of  as
It is easy to check that I2 = x – α. Next, we rewrite I1 as
Now, combining (10.7.4)-(10.7.5), we get
as the Bayes estimate of . Also refer to the Exercises 10.7.2-10.7.3. !
Example 10.7.3 (Example 10.7.2 Continued) Let X be N(θ, 1) given that
 = θ where  is the unknown population mean. Consider the statistic T = X
which is minimal sufficient for θ given that  = θ. We were told that v was
positive and we supposed that the prior distribution of  was exponential with
the known mean α–1(> 0). Under the squared error loss function, the Bayes
estimate of  turned out to be 
 given by (10.7.6). Observe that the poste-
rior distribution’s support is ℜ+ and thus 
 the mean of this posterior distri-
bution, must be positive. But, notice that the observed data x may be larger or
smaller than α. Can we check that the expression of 
 will always lead to a
positive estimate? The answer will be in the affirmative if we verify the fol-
lowing claim:
Note that 
Φ(x) = φ(x) and 
φ(x) = –xφ(x) for all x ∈ ℜ. Thus, one has
p(x) = Φ(x) which is positive for all x ∈ ℜ so that the function p(x) ↑ in x.
Next, let us consider the behavior of p(x) when x is near negative infin-
ity. By appealing to L’Hôpital’s rule from (1.6.29), we observe that
 Hence, 
 Combine

10. Bayesian Methods
497
this with the fact that p(x) is increasing in x(∈ ℜ) to validate (10.7.7). In other
words, the expression of 
 is always positive. Also refer to the Exercises
10.7.2-10.7.3. !
In the three previous examples, we had worked with the normal likelihood
function given  = θ. But, we were told that v was positive and hence we
were forced to put a prior only on (0, ∞). We experimented with the exponen-
tial prior for  which happened to be skewed to the right. In the case of a
normal likelihood, some situations may instead call for a non-conjugate but
symmetric prior for . Look at the next example.
Example 10.7.4 Let X be N(θ, 1) given that  = θ where (∈ ℜ) is the
unknown population mean. Consider the statistic T = X which is minimal
sufficient for θ given that  = θ. We have been told that (i)  is an arbitrary
real number, (ii)  is likely to be distributed symmetrically around zero, and
(iii) the prior probability around zero is little more than what it is likely with a
normal prior. With the N(0, 1) prior on , we note that the prior probability of
the interval (–.01, .01) amounts to 7.9787 × 10–3 whereas with the Laplace
prior pdf h(θ) = ½e–|θ|I(θ ∈ ℜ), the prior probability of the same interval
amounts to 9.9502 × 10–3. So, let us start with the Laplace prior h(θ) for .
From the apriori description of  alone, it does not follow however that the
chosen prior distribution is the only viable candidate. But, let us begin some
preliminary investigation anyway.
One can verify that the marginal pdf m(x) of X can be written as
Next, one may verify that the posterior pdf of 
 given that X = x can be
expressed as follows: For all x ∈ ℜ and θ ∈ ℜ,
where b(x = {1 – Φ(x + 1)}exp{(x + 1)2/2}+F(x – 1)exp{(x – 1)2/2}. The
details are left out as the Exercise 10.7.4. Also refer to the Exercises 10.7.5-
10.7.6. !
10.8 Exercises and Complements
10.2.1 Suppose that X1, ..., Xn are iid with the common pdf f(x; θ) =
θe–θxI(x > 0) given that v = θ where v(> 0) is the unknown parameter.

498
10. Bayesian Methods
Assume the prior density h(θ) = ae–αθI(θ > 0) where α (> 0) is known. Derive
the posterior pdf of v given that T = t where 
10.2.2 (Example 10.2.1 Continued) Suppose that X1, ..., X5 are iid with the
common pdf f(x; θ) = θe-θxI(x > 0) given that 
 = θ where 
(> 0) is the
unknown parameter. Assume the prior density h(θ) = 1/8e–θ/8I(θ > 0). Draw
and compare the posterior pdf’s of  given that 
 with t = 15, 40,
50.
10.2.3 Suppose that X1, ..., Xn are iid Poisson(θ) given that  = θ where
v(> 0) is the unknown population mean. Assume the prior density h(θ) = e–
θI(θ > 0). Derive the posterior pdf of 
 given that T = t where T =
10.2.4 (Example 10.2.3 Continued) Suppose that X1, ..., X10 are iid
Poisson(θ) given that = θ where (> 0) is the unknown population mean.
Assume the prior density h(θ) = ¼e-θ/4I(θ > 0). Draw and compare the poste-
rior pdf’s of  given that with t = 30, 40, 50.
10.3.1 Let X1, X2 be independent, X1 be distributed as N(θ, 1) and X2 be
distributed as N(2θ, 3) given that  = θ where (∈ ℜ) is the unknown param-
eter. Consider the minimal sufficient statistic T for θ given that  = θ. Let us
suppose that the prior distribution of  on the space Θ = ℜ is N(5, τ2) where
τ(> 0) is a known number. Derive the posterior distribution of v given that T
= t, t ∈ ℜ. {Hint: Given  = θ, is the statistic T normally distributed?}
10.3.2 Suppose that X1, ..., Xn are iid Exponential(θ) given that 
 = θ
where the parameter v(> 0) is unknown. We say that 
 has the inverted
gamma prior, denoted by IGamma(α, β), whenever the prior pdf is given by
where α, β are known positive numbers. Denote the sufficient statistic T =
 given that  = θ.
(i)
Show that the prior distribution of  is IGamma(α, β) if and only if
 has the Gamma(α, β) distribution;
(ii)
Show that the posterior distribution of v given T = t turns out to be
IGamma(n + α, {t + β-1}-1).
10.3.3 Suppose that X1, ..., Xn are iid N(0, θ2) given that  = θ where the
parameter v(> 0) is unknown. Assume that 
2 has the inverted gamma prior
IGamma(α, β), where α, β are known positive numbers. Denote the suffi-
cient statistic 
 given that  = θ. Show that the posterior distribu-
tion of  given T = t is an appropriate inverted gamma distribution.

10. Bayesian Methods
499
10.3.4 Suppose that X1, ..., Xn are iid Uniform(0, θ) given that  = θ where
the parameter (> 0) is unknown. We say that v has the Pareto prior, denoted
by Pareto(α, β), when the prior pdf is given by
where α, β are known positive numbers. Denote the sufficient statistic T =
Xn:n, the largest order statistic, given that 
 = θ. Show that the posterior
distribution of v given T = t turns out to be Pareto(max(t, α), n + β).
10.3.5 (Exercise 10.3.4 Continued) Let X1, ..., Xn be iid Uniform(0, aθ)
given that 
 = θ where the parameter 
(> 0) is unknown, but a(> 0) is
assumed known. Suppose that  has the Pareto(α, β) prior where α, β are
known positive numbers. Denote the sufficient statistic T = Xn:n, the largest
order statistic, given that 
 = θ. Show that the posterior distribution of 
given T = t is an appropriate Pareto distribution.
10.3.6 Let X1, ..., Xn be iid Uniform(–θ, θ) given that 
 = θ where the
parameter 
(> 0) is unknown. Suppose that 
 has the Pareto(α, β) prior
where α, β are known positive numbers. Denote the minimal sufficient statis-
tic T = |X|n:n, the largest order statistic among |X1|, ..., |Xn|, given that  = θ.
Show that the posterior distribution of  given T = t is an appropriate Pareto
distribution. {Hint: Can this problem be reduced to the Exercise 10.3.4?}
10.3.7 Let X1, X2, X3 be independent, X1 be distributed as N(θ, 1), X2 be
distributed as N(2θ, 3), and X3 be distributed as N(θ, 3) given that 
 = θ
where 
(∈ ℜ) is the unknown parameter. Consider the minimal sufficient
statistic T for θ given that  = θ. Let us suppose that the prior distribution of
 on the space Θ = ℜ is N(2, τ2) where τ(> 0) is a known number. Derive the
posterior distribution of  given that T = t, t ∈ ℜ. {Hint: Given  = θ, is the
statistic T normally distributed?}
10.3.8 We denote 
 and suppose that X1, ..., Xn are
iid N(θ1, θ 2
2) given that  = θ where the parameters 
1(∈ ℜ), 
2(∈ ℜ+) are
assumed both unknown. Consider the minimal sufficient statistic T = 
for θ given that  = θ. We are given the joint prior distribution of 
 as
follows:
where h½(θ1) stands for the pdf of the 
 distribution and 
 stands
for the pdf of the IGamma(2, 1) distribution. The form of the inverted gamma
pdf was given in the Exercise 10.3.3.

500
10. Bayesian Methods
(i)
Given that  = θ, write down the joint pdf of 
 by taking the
product of the two separate pdf’s because these are independent,
and this will serve as the likelihood function;
(ii)
Multiply the joint pdf from part (i) with the joint prior pdf 
Then, combine the terms involving 
 and the terms in-
volving only 
 From this, conclude that the joint posterior pdf k(θ;
t) of  given that 
 that is 
 can be viewed
as 
 Here, 
 stands for the pdf (in
the variable θ1) of the 
 distribution with
 
 and 
 stands for
the pdf (in the variable θ 2
2) of the IGamma(α0, β0) with α0 = 2 + ½n,
β-1
0 = 
(iii)
From the joint posterior pdf of  given that 
 inte-
grate out θ1 and thus show that the marginal posterior pdf of is the
same as that of the IGamma(α0, β0) distribution as in part (ii);
(iv)
From the joint posterior pdf of  given that 
 inte-
grate out 
 and thus show that the marginal posterior pdf of θ1 is
the same as that of an appropriate Student’s t distribution.
10.4.1 (Example 10.3.2 Continued) Let X1, ..., Xn be iid Poisson(θ) given
that  = θ where v(> 0) is the unknown population mean. We suppose that
the prior distribution of  on the space Θ = (0, ∞) is Gamma(α, β) where α(>
0) and β(> 0) are known numbers. Under the squared error loss function,
derive the expression of the Bayes estimate 
 for . {Hint: Use the form of
the posterior from (10.3.3) and then appeal to the Theorem 10.4.2.}
10.4.2 (Exercise 10.3.1 Continued) Let X1, X2 be independent, X1 be dis-
tributed as N(θ, 1) and X2 be distributed as N(2θ, 3) given that  = θ where
(∈ ℜ) is the unknown parameter. Let us suppose that the prior distribution
of  on the space Θ = ℜ is N(5, τ2) where τ(> 0) is a known number. Under
the squared error loss function, derive the expression of the Bayes estimate
 for . {Hint: Use the form of the posterior from Exercise 10.3.1 and then
appeal to the Theorem 10.4.2.}
10.4.3 (Exercise 10.3.2 Continued) Let X1, ..., Xn be iid Exponential(θ)
given that  = θ where the parameter (> 0) is unknown. Assume that 
has the inverted gamma prior IGamma(α, β) where α, β are known posi-
tive numbers. Under the squared error loss function, derive the expression of

10. Bayesian Methods
501
the Bayes estimate 
 for v. {Hint: Use the form of the posterior from Exer-
cise 10.3.2 and then appeal to the Theorem 10.4.2.}
10.4.4 (Exercise 10.3.3 Continued) Let X1, ..., Xn be iid N (0, θ2) given that
v = θ where the parameter 
(> 0) is unknown. Assume that  has the in-
verted gamma prior IGamma(α, β) where α, β are known positive numbers.
Under the squared error loss function, derive the expression of the Bayes
estimate 
 for . {Hint: Use the form of the posterior from Exercise 10.3.3
and then appeal to the Theorem 10.4.2.}
10.4.5 (Exercise 10.3.4 Continued) Let X1, ..., Xn be iid Uniform (0, θ)
given that  = θ where the parameter v(> 0) is unknown. Suppose that  has
the Pareto(α, β) prior pdf h(θ) = βαβ θ-(β+1) I(α < θ < ∞) where α, β are
known positive numbers. Under the squared error loss function, derive the
expression of the Bayes estimate 
 for . {Hint: Use the form of the poste-
rior from Exercise 10.3.4 and then appeal to the Theorem 10.4.2.}
10.4.6 (Exercise 10.3.5 Continued) Let X1, ..., Xn be iid Uniform (0, aθ)
given that 
 = θ where the parameter 
(> 0) is unknown, but a(> 0) is
assumed known. Suppose that  has the Pareto(α, β) prior where α, β are
known positive numbers. Under the squared error loss function, derive the
expression of the Bayes estimate 
 for . {Hint: Use the form of the poste-
rior from Exercise 10.3.5 and then appeal to the Theorem 10.4.2.}
10.4.7 (Exercise 10.3.6 Continued) Let X1, ..., Xn be iid Uniform(–θ, θ)
given that  = θ where the parameter (> 0) is unknown. Suppose that v has
the Pareto(α, β) prior where α, β are known positive numbers. Under the
squared error loss function, derive the expression of the Bayes estimate 
for . {Hint: Use the form of the posterior from Exercise 10.3.6 and then
appeal to the Theorem 10.4.2.}
10.4.8 A soda dispensing machine is set up so that it automatically fills the
soda cans. The actual amount of fill must not vary too much from the target
(12 fl. ounces) because the overfill will add extra cost to the manufacturer
while the underfill will generate complaints from the customers hampering
the image of the company. A random sample of the fills for 15 cans gave
=0.14. Assuming a N (12, θ2) distribution for the actual fills
given that v = θ(> 0) and the IGamma(10, 10) prior for 
2, obtain the Bayes
estimate of the true population variance under the squared error loss.
10.4.9 Ten automobiles of the same make and model were driven by the
drivers with similar road habits and the gas mileage for each was recorded
over a week. The summary results included 
 miles per gallon. As-
sume a N (θ, 4) distribution for the actual gas mileage given that  = θ (∈ ℜ)
and the N (20, 6) prior for . Construct a 90% HPD credible interval for the

502
10. Bayesian Methods
true average gas mileage per gallon, . Give the Bayes estimate of the true
average gas mileage per gallon  under the squared error loss.
10.4.10 (Fubini’s Theorem) Suppose that a function of two real variables
g(x1, x2) is either non-negative or integrable on the space χ = χ1 × χ2(⊆ ℜ ×
ℜ). That is, for all (x1, x2) ∈ χ, the function g(x1, x2) is either non-negative or
integrable. Then, show that the order of the (two-dimensional) integrals can
be interchanged, that is, one can write
{Note: This is a hard result to verify. It is stated here for reference purposes
and completeness. Fubini’s Theorem was used in the proof of the Theorem
10.4.1 for changing the order of two integrals.}
10.5.1 (Exercise 10.4.2 Continued) Let X1, X2 be independent, X1 be dis-
tributed as N (θ, 1) and X2 be distributed as N (2θ, 3) given that  = θ where
(∈ ℜ) is the unknown parameter. Let us suppose that the prior distribution
of  on the space Θ = ℜ is N (5, τ2) where τ(> 0) is a known number. With
fixed 0 < α < 1, derive the 100(1 – α)% HPD credible interval for . {Hint:
Use the form of the posterior from the Exercise 10.3.1.}
10.5.2 (Exercise 10.4.3 Continued) Let X1, ..., Xn be iid Exponential(θ)
given that  = θ where the parameter (> 0) is unknown. Suppose that  has
the IGamma(1, 1) prior pdf h(θ) = θ–2 exp {–1/θ}I(θ > 0). With fixed 0 < α <
1, derive the 100(1 – α)% HPD credible interval for . {Hint: Use the form of
the posterior from the Exercise 10.3.2.}
10.5.3 (Exercise 10.4.4 Continued) Let X1, ..., Xn be iid N (0, θ2) given that
 = θ where the parameter v(> 0) is unknown. Suppose that 
 has the
IGamma(1, 1). With fixed 0 < α < 1, derive the 100(1 – α)% HPD credible
interval for 
2. {Hint: Use the form of the posterior from the Exercise 10.3.3.}
10.5.4 (Exercise 10.4.5 Continued) Let X1, ..., Xn be iid Uniform(0, θ)
given that  = θ where the parameter (> 0) is unknown. Suppose that v has
the Pareto(τ, β) prior, that is the prior pdf is given by h(θ) = βτβθ–(β+1) I(τ < θ
< ∞) where τ, β are known positive numbers. With fixed 0 < α < 1, derive the
100(1 - a)% HPD credible interval for . {Hint: Use the form of the posterior
from the Exercise 10.3.4.}
10.5.5 (Exercise 10.4.6 Continued) Let X1, ..., Xn be iid Uniform(0, aθ)
given that  = θ where the parameter (> 0) is unknown, but a(> 0) is
assumed known. Suppose that  has the Pareto(τ, β) prior where τ, β are

10. Bayesian Methods
503
known positive numbers. With fixed 0 < α < 1, derive the 100(1 – α)% HPD
credible interval for . {Hint: Use the form of the posterior from the Exercise
10.3.5.}
10.5.6 (Exercise 10.4.7 Continued) Let X1, ..., Xn be iid Uniform(–θ, θ)
given that  = θ where the parameter (> 0) is unknown. Suppose that  has
the Pareto(τ, β) prior where τ, β are known positive numbers. With fixed 0 <
α < 1, derive the 100(1 – α)% HPD credible interval for . Under the squared
error loss function, derive the expression of the Bayes estimate 
 for 
.
{Hint: Use the form of the posterior from the Exercise 10.3.6.}
10.5.7 Suppose that X has its pdf f(x; θ) = 2θ–2 (θ – x)I(0 < x < θ) given
that  = θ where the parameter (> 0) is unknown. Suppose that  has the
Gamma(3, 2) distribution.
(i)
Show that the posterior pdf k(θ x) = ¼ (θ – x)e–(θ–x)/2× I (0 < x < θ);
(ii)
With fixed 0 < α < 1, derive the 100(1 – α)% HPD credible interval
for 
.
10.5.8 (Exercise 10.3.7 Continued) Let X1, X2, X3 be independent, X1 be
distributed as N (θ, 1), X2 be distributed as N (2θ, 3), and X3 be distributed as
N (θ, 3) given that  = θ where  (∈ ℜ) is the unknown parameter. Consider
the minimal sufficient statistic T for θ given that  = θ. Let us suppose that
the prior distribution of  on the space Θ = ℜ is N (2, 9). Suppose that the
following data has been observed:
Derive the 95% HPD credible interval for . Under the squared error loss
function, derive the expression of the Bayes estimate 
 for . {Hint: First
find the posterior along the lines of the Exercise 10.3.7.}
10.6.1 (Example 10.6.1 Continued) Let X1, ..., X10 be iid N (θ, 4.5) given
that  = θ where  (∈ ℜ) is the unknown parameter. Let us suppose that the
prior distribution of  on the space Θ = ℜ is N (4, 2). Consider the statistic
 which is minimal sufficient for θ given that  = θ. Suppose that
the observed value of T is t = 48.5. Use the Bayes test from (10.6.2) to
choose between a null hypothesis H0 :  < 3 against an alternative hypothesis
H1 :  ≥ 5.
10.6.2 (Example 10.3.3 Continued) Let X1, X2, X3 be iid Bernoulli(θ) given
that  = θ where  is the unknown probability of success, 0 < v < 1. Let us
assume that the prior distribution of v on the space Θ = (0, 1) is described
as Beta(2, 4). Consider the statistic 
 which is minimal suffi-
cient for θ given that  = θ. Suppose that the observed value of T is t =
2. Use the Bayes test from (10.6.2) to choose between a null hypothesis

504
10. Bayesian Methods
H0 :  ≤ .3 against an alternative hypothesis H1 :  ≥ .5. {Hint: Use integration
by parts when evaluating a0, a1 from (10.6.1). Review (1.6.28) as needed.}
10.6.3 (Example 10.3.4 Continued) Let X1, ..., X4 be iid Poisson(θ) given
that v = θ where (> 0) is the unknown population mean. Let us suppose that
the prior distribution of  on the space Θ = (0, ∞) is Gamma(2, 3). Consider
the statistic 
 which is minimal sufficient for θ given that  = θ.
Suppose that the observed value of T is t = 1. Use the Bayes test from (10.6.2)
to choose between a null hypothesis H0 :  ≤ 1.5 against an alternative hy-
pothesis H1 :  ≥ 2.5. {Hint: Use integration by parts when evaluating α0, α1
from (10.6.1). Review (1.6.28) as needed.}
10.7.1 (Example 10.7.1 Continued) Using the definition of a pdf, show
that the function k(θ; x) given in (10.7.3) is indeed a probability density func-
tion of .
10.7.2 Let X1, ..., Xn be iid N (θ 1) given that  = θ where  (∈ ℜ+) is the
unknown parameter. Let us suppose that the prior pdf of  on the space Θ =
ℜ+ is given by h(θ) = αe–αθ I(θ > 0) where α(> 0) is a known number. Con-
sider 
 the sample mean, which is minimal sufficient for θ given that 
= θ.
(i)
Derive the marginal pdf m(t) of T for t ∈ ℜ;
(ii)
Derive the posterior pdf k(θ t) for v given that T = t, for θ > 0 and
–∞ < t < ∞;
(iii)
Under the squared error loss function, derive the Bayes estimate 
and show, as in the Example 10.7.3, that 
 can take only positive
values.
{Hint: Proceed along the lines of the Examples 10.7.1-10.7.2.}
10.7.3 (Exercise 10.7.2 Continued) Let X1, ..., X10 be iid N (θ, 1) given that
 = θ where  (∈ ℜ+) is the unknwon parameter. Let us suppose that the
prior pdf of  on the space Θ = ℜ+ is given by h(θ) = αe-αθ I (θ > 0) where
α(> 0) is a known number. Suppose that the observed value of the sample
mean was 5.92. Plot the posterior pdf k(θ; t) assigning the values α = ½, ¼
and 1/8. Comment on the empirical behaviors of the corresponding posterior
pdf and the Bayes estimates of .
10.7.4 (Example 10.7.4 Continued) Verify the expressions of m(x) and k(θ
x) given by (10.7.8) and (10.7.9) respectively. Under the squared error loss
function, derive the form of the Bayes estimate 
.
10.7.5 Let X1, ..., Xn be iid N (θ, 1) given that  = θ where  (∈ ℜ) is the
unknown parameter. Let us suppose that the prior pdf of  on the space Θ =
ℜ is given by h(θ) = ½αe-α|θ| I (θ ∈ ℜ) where α(> 0) is a known number.
Consider 
 the sample mean, which is minimal sufficient for

10. Bayesian Methods
505
θ given that  = θ.
(i)
Derive the marginal pdf m(t) of T for t ∈ ℜ;
(ii)
Derive the posterior pdf k(θ t) for v given that T = t, for θ > 0 and
–∞ < t < ∞;
(iii)
Under the squared error loss function, derive the Bayes estimate 
and show, as in the Example 10.7.3, that 
 can take only positive
values.
{Hint: Proceed along the Example 10.7.4.}
10.7.6 (Exercise 10.7.5 Continued) Let X1, ..., X10 be iid N (θ, 1) given that
 = θ where  (∈ ℜ) is the unknown parameter. Let us suppose that the prior
pdf of  on the space Θ = ℜ is given by h(θ) = ½αe–α|θ| I (θ ∈ ℜ) where α(>
0) is a known number. Suppose that the observed value of the sample mean
was 5.92. Plot the posterior pdf k(θ t) assigning the values α = ½, ¼ and 1/8.
Comment on the empirical behaviors of the corresponding posterior pdf and
the Bayes estimates of .

This page intentionally left blank

507
11
Likelihood Ratio and Other Tests
11.1 Introduction
In Chapter 8, a theory of UMP level a tests was developed for a simple null
hypothesis against a lower- or upper-sided alternative hypothesis. But, we
have mentioned that even in a one-parameter problem, sometimes a UMP
level a test does not exist when choosing between a simple null hypothesis
against a two-sided alternative hypothesis. Recall the situation from Section
8.5.1 in the case of testing the mean of a normal distribution with known
variance when the alternative hypothesis was two-sided. One may also recall
the Exercises 8.5.2, 8.5.4 and 8.5.5 in this context. In these situations, the
likelihood ratio tests provide useful methodologies. This general approach to
construct test procedures for composite null and alternative hypotheses was
developed by Neyman and Pearson (1928a,b,1933a,b).
We start with iid real valued random variables X1, ..., Xn having a common
pdf f(x; θ) where the unknown parameter θ consists of p(≥ 1) components
(θ1, ..., θp) ∈ Θ(⊆ ℜp). We wish to test
with a given level α where Θ1 = Θ – Θ0, 0 < α < 1. First, let us write down the
likelihood function:
Then, we look at 
 which is interpreted as the best evidence in favor
of the null hypothesis H0. On the other hand, 
 is interpreted as the
overall best evidence in favor of θ without regard to any restrictions. Now,
the likelihood ratio (LR) test statistic is defined as
whereas the LR test is implemented as follows:

508
11. Likehood Ratio and Other Tests
Note that small values of Λ are associated with the small values of 
relative to 
 If the best evidence in favor of the null hypothesis ap-
pears weak, then the null hypothesis is rejected. That is, we reject H0 for
significantly small values of Λ.
It is easy to see that one must have 0 < Λ < 1 because in the definition of
Λ, the supremum in the numerator (denominator) is taken over a smaller
(larger) set Θ0(Θ). The cut-off number k ∈ (0, 1) has to be chosen in such a
way that the LR test from (11.1.3) has the required level α.
For simplicity, we will handle only a special kind of null hypothesis. Let us
test
where 
 is a known and fixed value of the (sub-) parameter θ1. One may be
tempted to say that the hypothesis H0 is a simple null hypothesis. But, actually
it may not be so. Even though H0 specifies a fixed value for a single compo-
nent of θ, observe that the other components of ? remain unknown and arbi-
trary.
How should we evaluate 
 First, in the expression of L(θ), we
must plug in the value in the place of 
. Then, we maximize the likelihood
function L( , θ2, ..., θp) with respect to the (sub-) parameters θ2, ..., θp by
substituting their respective MLE’s when we know that θ1 = 
 On the other
hand, the 
 is found by plugging in the MLE’s of all the components
θ1, ..., θp in the likelihood function. In the following sections, we highlight
these step by step derivations in a variety of situations.
Section 11.2 introduces LR tests for the mean and variance of a normal
population. In Section 11.3, we discuss LR tests for comparing the means
and variances of two independent normal populations. In Section 11.4, under
the assumption of bivariate normality, test procedures are given for the popu-
lation correlation coefficient ? and for comparing the means as well as vari-
ances.
11.2 One-Sample Problems
We focus on a single normal population and some LR tests associated
with it. With fixed α ∈ (0,1), first a level α LR test is derived for a speci-
fied population mean against the two-sided alternative hypothesis, and come
up with the customary two-sided Z-test (t-test) when the population vari-
ance is assumed known (unknown). Next, we obtain a level α LR test for

11. Likehood Ratio and Other Tests
509
a specified population variance against the two-sided alternative hypothesis,
and come up with the customary two-sided ?2-test assuming that the popula-
tion mean is unknown.
11.2.1
LR Test for the Mean
Suppose that X1, ..., Xn are iid observations from the N(µ, σ2) population
where µ ∈ ℜ, σ ∈ ℜ+. We assume that µ is unknown. Given α ∈ (0,1),
consider choosing between a null hypothesis H0 : µ = µ0 and a two-sided
alternative hypothesis H1 : µ ≠ µ0 with level α where µ0 is a fixed real number.
We address the cases involving known σ or unknown σ separately. As usual,
 respectively denote the
sample mean and variance.
Variance Known
Since σ is known, we have θ = µ, Θ0 = {µ0} and Θ = ℜ. In this case, H0
is a simple null hypothesis. The likelihood function is given by
Observe that
since Θ0 has the single element µ0. On the other hand, one has
Note that for any real number c, we can write
and hence by combining (11.2.2)-(11.2.3) we obtain the likelihood ratio
Intuitively speaking, one may be inclined to reject H0 if and only if 
L(µ) is small or 
 is large. But for some data, 
 and

510
11. Likehood Ratio and Other Tests
 may be small or large at the same time. Thus, more formally, one
rejects H0 if and only if Λ is small. Thus, we decide as follows:
where k(> 0) is a generic constant. That is, we reject H0 if and only if 
is too large (> 
) or too small (< – 
). The implementable form of the level
α LR test will look like this:
Figure 11.2.1. Two-Sided Standard Normal Rejection Region
See the Figure 11.2.1. The level of the two-sided Z-test (11.2.7) can be evalu-
ated as follows:
which is α since 
 is distributed as N(0, 1) if µ = µ0.
From Section 8.5.1, recall that no UMP test exists for this problem.
Variance Unknown
We have θ = (µ, σ2), Θ0 = {(µ0, σ2) : µ0 is fixed, σ ∈ ℜ+} and Θ = {(µ, σ2)
: µ ∈ ℜ, σ ∈ ℜ+}. In this case, H0 is not a simple null hypothesis. We assume
that the sample size n is at least two. The likelihood function is given by

11. Likehood Ratio and Other Tests
511
Observe that
On the other hand, one has
Now, we combine (11.2.4) and (11.2.10)-(11.2.11) to express the likelihood
ratio
Now, we reject H0 if and only if ? is small. Thus, we decide as follows:
where k(> 0) is a generic constant. That is, we reject H0 if and only if 
when properly scaled becomes too large or too small. The implementable
form of the level α LR test would then look like this:
Figure 11.2.2. Two-Sided Student’s tn–1 Rejection Region

512
11. Likehood Ratio and Other Tests
See the Figure 11.2.2. The level of the two-sided t test (11.2.14) can be
evaluated as follows:
which is α since 
 has the Student’s t distribution with n – 1
degrees of freedom if µ = µ0.
In principle, one may think of a LR test as long as one starts with
the likelihood function. The observations do not need to be iid.
Look at the Exercises 11.2.6 and 11.2.9.
Example 11.2.1 In a recent meeting of the association for the commuting
students at a college campus, an issue came up regarding the weekly average
commuting distance (µ). A question was raised whether the weekly average
commuting distance was 340 miles. Ten randomly selected commuters were
asked about how much (X) each had driven to and from campus in the imme-
diately preceding week. The data follows:
351.9 357.5 360.1 370.4 323.6 332.1 346.6 355.5 351.0 348.4
One obtains 
 miles and s = 13.4987 miles. Assume normality for
the weekly driving distances. We may like to test H0 : µ = 340 against H1 : µ ≠
340 at the 10% level. From (11.2.14), we have the observed value of the test
statistic:
With α = .10 and 9 degrees of freedom, one has t9,.05 = 1.8331. Since |tcalc|
exceeds t9,.05, we reject the null hypothesis at the 10% level. In other words, at
the 10% level, we conclude that the average commuting distance per week is
significantly different from 340 miles. !
11.2.2
LR Test for the Variance
Suppose that X1, ..., Xn are iid observations from the N(µ, σ2) population
where µ ∈ ℜ, σ ∈ ℜ+. We assume that both µ and σ are unknown. Given
α ∈ (0, 1), we wish to find a level α LR test for choosing between a null
hypothesis H0 : σ = σ0 and a two-sided alternative hypothesis H1 : σ ≠ σ0
where σ0 is a fixed positive real number. In this case, H0 is not a simple
null hypothesis. As usual, we denote the sample mean 
and the sample variance 
 We have

11. Likehood Ratio and Other Tests
513
θ = (µ, σ2), Θ0 = {(µ, 
) : µ ∈ ℜ, σ0 is fixed} and Θ = {(µ, σ2) : µ ∈ ℜ, σ ∈
ℜ+}.
The likelihood function is again given by
Now, observe that
On the other hand, one has 
from (11.2.11) where 
 Now, we combine this with
(11.2.17) to obtain the likelihood ratio
Now, one rejects H0 if and only if ? is small. Thus, we decide as follows:
where k(> 0) is a generic constant.
Figure 11.2.3. Plot of the g(u) Function
In order to express the LR test in an implementable form, we proceed
as follows: Consider the function g(u) = ue1-u for u > 0 and investigate

514
11. Likehood Ratio and Other Tests
its behavior to check when it is small (< k). We note that g(1) = 1 and g (u) =
{(1 - u)/u}g(u) which is positive (negative) when u < 1 (u > 1). Hence, the
function g(u) is strictly increasing (decreasing) on the left (right) hand side of
u = 1. Thus, g(u) is going to be “small” for both very small and very large
values of u(> 0). This feature is also clear from the plot of the function g(u)
given in the Figure 11.2.1. Thus, we rewrite the LR test (11.2.19) as follows:
as long a, b as are chosen so that the test has level α.
Figure 11.2.4. Two-Sided 
 Rejection Region
Recall that 
 has a Chisquare
distribution with n – 1 degrees of freedom if σ = σ0 and hence a level α LR
test can be expressed as follows:
That is, we reject H0 if and only if 
 when properly scaled becomes too
large or too small. See the Figure 11.2.4. The case when µ is known has
been left as the Exercises 11.2.2-11.2.3. Also look at the related Exercise
11.2.4.
Recall from the Exercise 8.5.5 that no UMP level α test exists
for testing H0 versus H1 even if µ is known.
Example 11.2.2 In a dart-game, the goal is to throw a dart and hit the
bull’s eye at the center. After the dart lands on the board, its distance (X)

11. Likehood Ratio and Other Tests
515
from the center is measured in inches. A player made 7 attempts to hit the
bull’s eye and the observed x values were recorded as follows:
2.5, 1.2, 3.0, 2.3, 4.4, 0.8, 1.6
Assume a normal distribution for X. We wish to test H0 : σ = 1 against H1 : σ
≠ 1 at 5% level. One obtains 
 inches and s = 1.2164 inches. From
(11.2.21), we have the observed value of the test statistic:
With α = .05 and 6 degrees of freedom, one has 
 and
 Since 
 lies between the two numbers 1.2373 and
14.449, we accept H0 or conclude that there is not enough evidence to reject
H0 at 5% level. !
Example 11.2.3 A preliminary mathematics screening test was given to a
group of twenty applicants for the position of actuary. This group’s test scores
(X) gave 
 and s = 15.39. Assume a normal distribution for X. The
administrator wished to test H0 : σ = 12 against H1 : σ ≠ 12 at 10% level. From
(11.2.21), we have the observed value of the test statistic:
With α = .10 and 19 degrees of freedom, one has χ2
19,.95 = 10.117 and χ2
19,.05 =
30.144. Since 
 lies outside of the interval (10.117, 30.144), one should
reject at 10% level. !
11.3 Two-Sample Problems
We focus on two independent normal populations and some associated likeli-
hood ratio tests. With fixed α ∈ (0, 1), first a level α LR test is derived for the
equality of means against a two-sided alternative hypothesis when the com-
mon population variance is unknown and come up with the customary two-
sided t-test which uses the pooled sample variance. Next, we derive a level a
LR test for the equality of variances against a two-sided alternative hypothesis
when the population means are unknown and come up with the customary
two-sided F-test.
11.3.1
Comparing the Means
Suppose that the random variables Xi1, ..., Xini are iid N(µi, σ2), i = 1, 2,
and that the X1j’s are independent of the X2j’s. We assume that all three

516
11. Likehood Ratio and Other Tests
parameters are unknown and θ = (µ1, µ2, σ) ∈ ℜ×ℜ×ℜ+. Given α ∈ (0, 1), we
wish to find a level α LR test for choosing between a null hypothesis H0 : µ1 =
µ2 and a two-sided alternative hypothesis H1 : µ1 ≠ µ2. With ni ≥ 2, let us denote
for i = 1, 2. Here, 
 is the pooled estimator of σ2.
Since H0 specifies that the two means are same, we have Θ0 = {(µ, µ, σ2):
µ ∈ ℜ, σ ∈ ℜ+}, and Θ = {(µ1, µ2, σ2) : µ1 ∈ ℜ, µ2 ∈ ℜ, σ ∈ ℜ+}. The
likelihood function is given by
 
Thus, we can write
One should check that the maximum likelihood estimates of µ, σ2 obtained
from this restricted likelihood function turns out to be
Hence, from (11.3.3)-(11.3.4) we have
On the other hand, one has

11. Likehood Ratio and Other Tests
517
Now, we combine (11.3.5)-(11.3.6) to express the likelihood ratio as
with  from (11.3.4) and reject H0 if and only if ? is small. Thus, we decide as
follows:
where k(> 0) is a generic constant. Again, let us utilize (11.2.4) and write
which implies that
In other words, the “small” values of 
 will correspond to the “large” values of 
 Thus, we can rewrite the test (11.3.8) as follows:
where k(> 0) is an appropriate number.
Figure 11.3.1. Two-Sided Student’s tv Rejection Region
Where the Degree of Freedom Is v = n1 + n2 - 2

518
11. Likehood Ratio and Other Tests
From Example 4.5.2 recall that 
 has the
Student’s t distribution with n1 + n2 – 2 degrees of freedom. Thus, in view of
(11.3.10), the implementable form of a level α LR test would be:
See the Figure 11.3.1. Note that the LR test rejects H0 when 
 is
sizably different from zero with proper scaling.
Look at the Exercises 11.3.2-11.3.3 for a LR test of the equality
of means in the case of known variances.
Look at the Exercises 11.3.4-11.3.6 for a LR test to choose between
H0 : µ1 – µ2 = D versus H1 : µ1 – µ2 ≠ D.
Example 11.3.1 Weekly salaries (in dollars) of two typical high-school
seniors, Lisa and Mike, earned during last summer are given below:
Lisa:
234.26, 237.18, 238.16, 259.53, 242.76, 237.81, 250.95, 277.83
Mike: 187.73, 206.08, 176.71, 213.69, 224.34, 235.24
Assume independent normal distributions with unknown average weekly sala-
ries, µL for Lisa and µM for Mike, but with common unknown variance σ2. At
5% level we wish to test whether the average weekly salaries are same for
these two students, that is we have to test H0 : µL = µM versus H1 : µL ≠ µM.
One has
so that the pooled sample variance
From (11.3.11), we find the observed value of the test statistic:
With α = .05 and 12 degrees of freedom, we have t12,.025 = 2.1788. Since
|tcalc| exceeds t12,.025, we reject the null hypothesis at 5% level. At 5% level,
we conclude that the average weekly salaries of Lisa and Mike were signifi-
cantly different. !

11. Likehood Ratio and Other Tests
519
11.3.2
Comparing the Variances
Suppose that the random variables Xi1, ..., Xini are iid N(µi, 
), ni ≥ 2, i = 1,
2, and that the X1j’s are independent of the X2j’s. We assume that all four
parameters are unknown, (µi, σi) ∈ ℜ × ℜ+, i = 1, 2. With fixed α ∈ (0, 1),
we wish to construct a level α LR test for choosing between a null hypoth-
esis H0 : σ1 = σ2 and a two-sided alternative hypothesis H1 : σ1 ≠ σ2. Let us
denote θ = (µ1, 
, and for i = 1, 2,
Since H0 specifies that the two variances are same, we can write Θ0 = {(µ1,
µ2, σ2, σ2) : µ1 ∈ ℜ, µ2 ∈ ℜ, s ∈ ℜ+}, and Θ = {(µ1, µ2, 
 : µ1, ∈ ℜ, µ2
∈ ℜ, σ1 ∈ ℜ+, σ2 ∈ ℜ+}. The likelihood function is given by
for all (µ1, µ2, 
 ∈ ℜ × ℜ × ℜ+ × ℜ+. Thus, we can write
Now, consider the restricted likelihood function from (11.3.14). One should
check that the maximum likelihood estimates of µ1, µ2, σ2 obtained from this
restricted likelihood function turns out to be
Hence, from (11.3.14)-(11.3.15) we have

520
11. Likehood Ratio and Other Tests
On the other hand, one has
Now, we combine (11.3.16)-(11.3.17) to express the likelihood ratio as
where a ≡ a(n1, n2), b = b(n1, n2) are positive numbers which depend on n1,
n2 only. Now, one rejects H0 if and only if ∧ is small. Thus, we decide as
follows:
or equivalentl
y,
where k(> 0) is a generic constant.
In order to express the LR test in an implementable form, we proceed as
follows: Consider the function g(u) = un1/2(u + b) –n1+n2)/2 for u > 0 and inves-
tigate its behavior in order to check when it is small (< k). Note that g′(u) =
½u(n–2)/2 (u +b)–(n1+n2+2)/2{n1b – n2u} which is positive (negative) when u <
(>)n1b/n2. Hence, g(u) is strictly increasing (decreasing) on the left (right)
hand side of u = n1b/n2. Thus, g(u) is going to be “small” for both very small
or very large values of u(> 0).
Next, we rewrite the LR test (11.3.19) as follows:

11. Likehood Ratio and Other Tests
521
where the numbers c, d are chosen in such a way that the test has level α.
Figure 11.3.2. Two-Sided Fn1-1,n2-1 Rejection Region
Recall that 
 has the F distribution with the degrees of freedom n1 – 1, n2
– 1 when σ1 = σ2, that is under the null hypothesis H0. Hence, a level α LR test
can be written as follows:
Thus, this test rejects H0 when 
 is sizably different from one. See the
Figure 11.3.2.
Look at the Exercises 11.3.9-11.3.10 for a LR test of the
equality of variances in the case of known means.
Look at the Exercise 11.3.11 for a LR test to choose between
H0 : σ1/σ2 = D(> 0) versus H1 : σ1/σ2 ≠ D.
Example 11.3.2 Over a period of 6 consecutive days, the opening prices
(dollars) of two well known stocks were observed and recorded as follows:
Stock #1: 39.09, 39.70, 41.77, 38.96, 41.42, 42.26
Stock #2: 42.33, 39.16, 42.10, 40.92, 46.47, 45.02
Let us make a naive assumption that the two stock prices went up or down
independently during the period under study. The stock prices gave rise to
and the question we want to address is whether the variabilities in the
opening prices of the two stocks are the same at 10% level. That is, we
want to test H0 : σ1 = σ2 versus the two-sided alternative hypothesis

522
11. Likehood Ratio and Other Tests
H1 : σ1 ≠ σ2. Assume normality. From (11.3.21), we find the observed value
of the test statistic:
With α = .10, one has F5,5,.05 = 5.0503 and F5,5,.95 = 
 .19801. Since Fcalc
lies between the two numbers. 19801 and 5.0503, we conclude at 10% level
that the two stock prices were equally variable during the six days under
investigation. !
The problem of testing the equality of means of two independent
normal populations with unknown and unequal variances is hard.
It is referred to as the Behrens-Fisher problem. For some ideas
and references, look at both the Exercises 11.3.15 and 13.2.10.
11.4 Bivariate Normal Observations
We have discussed LR tests to check the equality of means of two indepen-
dent normal populations. In some situations, however, the two normal popu-
lations may be dependent. Recall the Example 9.3.3. Different test procedures
are used in practice in order to handle such problems.
Suppose that the pairs of random variables (X1i, X2i) are iid bivariate nor-
mal, N2(µ1, µ2, 
 ρ), i = 1, ..., n(≥ 2). Here we assume that all five
parameters are unknown, (µl, σl) ∈ ℜ × ℜ+, l = 1, 2 and –1 < ρ < 1. Test
procedures are summarized for the population correlation coefficient ρ and
for comparing the means µ1, µ2 as well as the variances 
.
11.4.1
Comparing the Means: The Paired Difference t Method
With fixed α ∈ (0, 1), we wish to find a level α test for a null hypothesis H0 :
µ1 = µ2 against the upper-, lower-, or two-sided alternative hypothesis H1.
The methodology from the Section 11.3.1 will not apply here. Let us denote
Observe that Y1, ..., Yn are iid N(µ1 - µ2, σ2) where σ2 = 
 – 2ρσ1σ2.
Since the mean µ1 – µ2 and the variance σ2 of the common normal distribu-
tion of the Y’s are unknown, the two-sample problem on hand is reduced

11. Likehood Ratio and Other Tests
523
to a one-sample problem in terms of the pivot
Under the null hypothesis H0 : µ1 = µ2, the statistic U has the Student’s t
distribution with n – 1 degrees of freedom.
Upper-Sided Alternative Hypothesis
We test H0 : µ1 = µ2 versus H1 : µ1 > µ2. See the Figure 11.4.1. Along the
lines of Section 8.4, we can propose the following upper-sided level α test:
Figure 11.4.1. Upper-Sided Student’s tn-1 Rejection Region
Lower-Sided Alternative Hypothesis
We test H0 : µ1 = µ2 versus H1 : µ1 < µ2. See the Figure 11.4.2. Along the
lines of Section 8.4, we can propose the following lower-sided level α test:
Figure 11.4.2. Lower-Sided Student’s tn-1 Rejection Region

524
11. Likehood Ratio and Other Tests
Two-Sided Alternative Hypothesis
We test H0 : µ1 = µ2 versus H1 : µ1 ≠ µ2. See the Figure 11.4.3. Along the
lines of Section 11.2.1, we can propose the following two-sided level α test:
Figure 11.4.3. Two-Sided Student’s tn-1 Rejection Region
Example 11.4.1 In a large establishment, suppose that X1i, X2i respectively
denote the job performance score before and after going through a week-long
job training program for the ith employee, i = 1, ..., n(≥ 2). We may assume
that these employees are picked randomly and independently of each other
and want to compare the average job performance scores in the population,
before and after the training. Eight employee’s job performance scores (out
of 100 points) were recorded as follows.
Y
Y
ID #
X1
X2
X1 – X2
ID #
X1
X2
X1 – X2
1
70
80
–10
5
89
94
–5
2
85
83
2
6
78
86
–8
3
67
75
–8
7
63
69
–6
4
74
80
–6
8
82
78
4
Assume a bivariate normal distribution for (X1, X2). The question we wish to
address is whether the training program has been effective. That is, we want
to test H0 : µ1 = µ2 versus H1 : µ1 < µ2, say, at the 1% level. From the 8
observed values of Y, we get the sample mean and variance  = –4.625, s2 =
24.8393. From (11.4.4), under H0 we find the observed value of the test
statistic:

11. Likehood Ratio and Other Tests
525
With α = .01 and 7 degrees of freedom, one has t7,.01 = 2.9980. But, since ucalc
does not go below –t7,.01, we do not reject the null hypothesis at 1% level. In
other words, at 1% level, we conclude that the job training has not been
effective. !
11.4.2
LR Test for the Correlation Coefficient
With fixed α ∈ (0, 1), we wish to construct a level α LR test for a null
hypothesis H0 : ρ = 0 against a two-sided alternative hypothesis H1 : ρ ≠ 0. We
denote θ = (µ1, µ2, 
 ρ) and write Θ0 = {(µ1, µ2, 
 0) : µ1 ∈ ℜ, µ2 ∈
ℜ, σ1 ∈ ℜ+, σ2 ∈ ℜ+}, and Θ = {(µ1, µ2, 
 ρ) : µ1 ∈ ℜ, µ2 ∈ ℜ, σ1 ∈ ℜ+,
σ2 ∈ ℜ+, ρ ∈ (-1, 1)}. The likelihood function is given by
for all θ ∈ Θ. We leave it as Exercise 11.4.5 to show that the MLE’s for µ1, µ2,
 and ρ are respectively given by 
 and 
These stand for the customary sample means, sample variances (not unbi-
ased), and the sample correlation coefficient. Hence, from (11.4.6) one has
Under the null hypothesis, that is when ? = 0, the likelihood function happens
to be
We leave it as the Exercise 11.4.4 to show that the MLE’s for µ1, µ2, 
 and 
are respectively given by 
 These again stand for the customary sample means and
sample variances (not unbiased). Hence, from (11.4.8) one has

526
11. Likehood Ratio and Other Tests
Now, one rejects H0 if and only if Λ is small. Thus, we decide as follows:
or equivalently
where k(> 0) is a generic constant. Note that r2/(1 - r2) is a one-to-one func-
tion of r2. It is easy to see that this test rejects H0 when r is sizably different
from zero.
Figure 11.4.4. Two-Sided Student’s tn-2 Rejection Region
Now, recall from (4.6.10) that 
 has the Student’s t dis-
tribution with n – 2 degrees of freedom when ρ = 0. This is why we as-
sumed that n was at least three. The derivation of this sampling distribution
was one of the earliest fundamental contributions of Fisher (1915). See the
Figure 11.4.4. Now, from (11.4.10), a level α LR test can be expressed as
follows:
Upper-Sided Alternative Hypothesis
We test H0 : ρ = 0 versus H1 : ρ > 0. See the Figure 11.4.5. Along the lines
of (11.4.11), we can propose the following upper-sided level α test:

11. Likehood Ratio and Other Tests
527
Figure 11.4.5. Upper-Sided Student’s tn-2 Rejection Region
Lower-Sided Alternative Hypothesis
We test H0 : θ = 0 versus H1 : ρ < 0. See the Figure 11.4.6. Along the lines
of (11.4.11), one can also propose the following lower-sided level α test:
Figure 11.4.6. Lower-Sided Student’s tn-2 Rejection Region
Example 11.4.2 (Example 11.4.1 Continued) Consider the data on (X1, X2)
from Example 11.4.1 for 8 employees on their job performance scores before
and after the training. Assuming a bivariate normal distribution for (X1, X2),
we wish to test whether the job performance scores before and after the
training are correlated. At 10% level, first we may want to test H0 : ρ = 0
versus H1 : ρ ≠ 0. For the observed data, one should check that r = .837257.
From (11.4.11), we find the observed value of the test statistic:

528
11. Likehood Ratio and Other Tests
With α = .10 and 6 degrees of freedom, one has t6,.05 = 1.9432. Since |tcalc|
exceeds t6,.05, we reject the null hypothesis at 10% level and conclude that the
job performance scores before and after training appear to be significantly
correlated. !
11.4.3
Test for the Variances
With fixed α ∈ (0, 1), we wish to construct a level α test for the null hypoth-
esis H0 : σ1 = σ2 against the upper-, lower-, or two-sided alternative hypoth-
esis H1. The methodology from Section 11.3.2 does not apply. Let us denote
Observe that (Y1i, Y2i) are iid bivariate normal, N2(ν1, ν2, 
 ρ*), i = 1, ...,
n(≥ 3) where ν1 = µ1 + µ2, ν2 = µ1 – µ2, 
 and Cov(Y1i, Y2i) = 
 so that ρ* = (
)/(τ1τ2). Of
course all the parameters ν1, ν2, 
 ρ* are unknown, (νl, τl) ∈ ℜ × ℜ+, l =
1, 2 and –1 < ρ* < 1.
Now, it is clear that testing the original null hypothesis H0 : σ1 = σ2 is
equivalent to testing a null hypothesis H0 : ρ* = 0 whereas the upper-, lower-
, or two-sided alternative hypothesis regarding σ1, σ2 will translate into an
upper-, lower-, or two-sided alternative hypothesis regarding ρ*. So, a level
α test procedure can be derived by mimicking the proposed methodologies
from (11.4.11)-(11.4.13) once r is replaced by the new sample correlation
coefficient τ* obtained from the transformed data (Y1i, Y2i), i = 1, ..., n(≥ 3).
Upper-Sided Alternative Hypothesis
We test H0 : σ1 = σ2 versus H1 : σ1 > σ2. See the Figure11.4.5. Along the
lines of (11.4.12), we can propose the following upper-sided level α test:
Lower-Sided Alternative Hypothesis
We test H0 : σ1 = σ2 versus H1 : σ1 < σ2. See the Figure 11.4.6. Along

11. Likehood Ratio and Other Tests
529
the lines of (11.4.13), we can propose the following lower-sided level α test.
Two-Sided Alternative Hypothesis
We test H0 : σ1 = σ2 versus H1 : σ1 ≠ σ2. See the Figure 11.4.4. Along the
lines of (11.4.11), we can propose the following two-sided level α test.
Example 11.4.3 (Example 11.4.1 Continued) Consider that data on (X1,
X2) from the Example 11.4.1 for the 8 employees on their job performance
scores before and after the training. Assuming the bivariate normal distribu-
tion for (X1, X2), we may like to test whether the job performance scores after
the training are less variable than those taken before the training. At the 1%
level, we may want to test H0 : σ1 = σ2 versus H1 : σ1 > σ2 or equivalently test
H0 : ρ* = 0 versus H1 : ρ* > 0 where ρ* is the population correlation coeffi-
cient between Y1, Y2. For the observed data, one has
Y1
Y2
Y1
Y2
ID #
X1 + X2
X1 – X2
ID #
X1 + X2
X1 – X2
1
150
–10
5
183
–5
2
168
2
6
164
–8
3
142
–8
7
132
–6
4
154
–6
8
160
4
One should check that the sample correlation coefficient between Y1, Y2 is r*
= .34641. From (11.4.15), we find the observed value of the test statistic:
With α = .01 and 6 degrees of freedom, one has t6,.01 = 3.1427. Since tcalc does
not exceed t6,.01, we do not reject the null hypothesis at 1% level and conclude
that the variabilities in the job performance scores before and after the training
appear to be same. !
11.5 Exercises and Complements
11.2.1 Verify the result given in (11.2.4).

530
11. Likehood Ratio and Other Tests
11.2.2 Suppose that X1, ..., Xn are iid N(0, σ2) where σ(> 0) is the un-
known parameter. With preassigned α ∈ (0, 1), derive a level α LR test for the
null hypothesis H0 : σ2 = 
 against an alternative hypothesis H1 : 
in the implementable form. {Note: Recall from the Exercise 8.5.5 that no
UMP level α test exists for testing H0 versus H1}.
11.2.3 Suppose that X1, ..., Xn are iid N(µ, σ2) where σ (∈ ℜ+) is the
unknown parameter but µ(∈ ℜ) is assumed known. With preassigned α ∈ (0,
1), derive a level α LR test for a null hypothesis H0 : 
 against an
alternative hypothesis H1 : 
 in the implementable form. {Note: Recall
from the Exercise 8.5.5 that no UMP level a test exists for testing H0 versus
H1}.
11.2.4 Suppose that X1, X2 are iid N(µ, σ2) where µ(∈ ℜ), σ(∈ ℜ+) are
both assumed unknown parameters. With preassigned α ∈ (0, 1), reconsider
the level α LR test from (11.2.21) for choosing between a null hypothesis H0
: 
 against an alternative hypothesis H1 : 
 Show that the
same test can be expressed as follows: Reject H0 if and only if |X1 – X2| >
11.2.5 Suppose that X1, ..., Xn are iid having the common exponential pdf
f(x; θ) = θ–1 exp{–x/θ}I(x > 0) where θ(> 0) is assumed unknown. With
preassigned α ∈ (0, 1), derive a level α LR test for a null hypothesis H0 : θ =
θ0(> 0) against an alternative hypothesis H1 : θ ≠ θ0 in the implementable form.
{Note: Recall from the Exercise 8.5.4 that no UMP level a test exists for
testing H0 versus H1}.
11.2.6 Suppose that X1 and X2 are independent random variables respec-
tively distributed as N(µ, σ2), N(3µ, 2σ2) where µ ∈ ℜ is the unknown param-
eter and σ ∈ ℜ+ is assumed known. With preassigned α ∈ (0, 1), derive a level
α LR test for H0 : µ = µ0 versus H1 : µ ≠ µ0 where µ0 (∈ ℜ) is a fixed number,
in the implementable form. {Hint: Write down the likelihood function along
the line of (8.3.31) and then proceed directly as in Section 11.2.1.}
11.2.7 Suppose that X1, ..., Xn are iid having the Rayleigh distribution with
the common pdf f(x; θ) = 2θ–1 xexp(–x2/θ)I(x > 0) where θ(> 0) is the un-
known parameter. With preassigned α ∈ (0, 1), derive a level α LR test for H0
: θ = θ0 versus H1 : θ ≠ θ0 where θ0(∈ ℜ+) is a fixed number, in the
implementable form.
11.2.8 Suppose that X1, ..., Xn are iid having the Weibull distribution with
the common pdf f(x; a) = a–1bxb–1 exp(–xb/a)I(x > 0) where a(> 0) is an
unknown parameter but b(> 0) is assumed known. With preassigned a ∈ (0,
1), derive a level α LR test for H0 : a = a0 versus H1 : a ≠ a0 where a0 is a
positive number, in the implementable form.

11. Likehood Ratio and Other Tests
531
11.2.9 Let us denote X′ = (X1, X2) where X is assumed to have the bivari-
ate normal distribution, N2(µ, µ, 1, 2, 
). Here, we consider µ(∈ ℜ) as the
unknown parameter. With preassigned α ∈ (0, 1), derive a level α LR test for
H0 : µ = µ0 versus H1 : µ ≠ µ0 where µ0 is a real number, in the implementable
form. {Hint: Write down the likelihood function along the line of (8.3.33) and
proceed accordingly.}
11.2.10 Let X1, ..., Xn be iid positive random variables having the common
lognormal pdf f(x; µ) = 
 exp {-[log(x) - µ]2/(2s2)} with x > 0, –∞ <
µ < ∞ 0 < σ < ∞. Here, µ is the only unknown parameter. With preassigned α
∈ (0, 1), derive a level α LR test for H0 : µ = µ0 versus H1 : µ ≠ µ0 where µ0
is a real number, in the implementable form.
11.2.11 Suppose that X1, ..., Xn are iid having the common Laplace pdf f(x;
b) = ½b–1 exp( – |x – a| /b)I(x ∈ ℜ) where b(> 0) is an unknown parameter but
α(∈ ℜ) is assumed known. With preassigned α ∈ (0, 1), derive a level α LR
test for a null hypothesis H0 : b = b0(> 0) against the alternative hypothesis H1
: b ≠ b0 in the implementable form. {Hint: Use the Exercise 11.2.4.}
11.2.12 Suppose that X1, ..., Xn are iid having the common negative expo-
nential pdf f(x; µ) = σ–1 exp{–(x – µ)/σ}I(x > µ) where µ(∈ ℜ) is an unknown
parameter but σ(∈ ℜ+) is assumed known. With preassigned α ∈ (0, 1),
derive a level α LR test for H0 : µ = µ0 versus H1 : µ ≠ µ0 where µ0 is a real
number, in the implementable form.
11.2.13 Suppose that X1, ..., Xn are iid random variables having the com-
mon pdf f(x; θ) = θ–1x(1-θ)/1I(0 < x < 1) with 0 < θ < ∞. Here, θ is the unknown
parameter. With preassigned α ∈ (0, 1), derive a level α LR test for H0 : θ = θ0
versus H1 : θ ≠ θ0 where θ0 is a positive number, in the implementable form.
11.2.14 A soda dispensing machine automatically fills the soda cans. The
actual amount of fill must not vary too much from the target (12 fl. ounces)
because the overfill will add extra cost to the manufacturer while the underfill
will generate complaints from the customers. A random sample of the fills for
15 cans gave a standard deviation of .008 ounces. Assuming a normal distri-
bution for the fills, test at 5% level whether the true population standard de-
viation is different from .01 ounces.
11.2.15 Ten automobiles of the same make and model were used by driv-
ers with similar road habits and the gas mileage for each was recorded over a
week. The summary results were  = 22 miles per gallon and s = 3.5 miles
per gallon. Test at 10% level whether the true average gas mileage per gallon
is any different from 20.
11.2.16 The waiting time (in minutes) for a passenger at a bus stop

532
11. Likehood Ratio and Other Tests
is assumed to have an exponential distribution with mean θ (> 0). The waiting
times for ten passengers were recorded as follows in one afternoon:
6.2
5.8
4.5
6.1
4.6
4.8
5.3
5.0
3.8
4.0
Test at 5% level whether the true average waiting time is any different from
5.3 minutes. {Hint: Use the test from Exercise 11.2.5.}
11.3.1 Verify the expressions of the MLE’s in (11.3.4).
11.3.2 Let the random variables Xi1, ..., Xini be iid N(µi, 
), i = 1, 2, and
that the X1j’s be independent of the X2j’s. Here we assume that µ1, µ2 are
unknown and (µ1, µ2) ∈ ℜ × ℜ but (σ1, σ2) ∈ ℜ+ × ℜ+ are assumed known.
With preassigned α ∈ (0, 1), derive a level α LR test for H0 : µ1 = µ2 versus H1
: µ1 ≠ µ2 in the implementable form. {Hint: The LR test rejects H0 if and only
if 
11.3.3 Let the random variables Xi1, ..., Xini be iid N(µi, σ2), i = 1, 2, and
that the X1j’s be independent of the X2j’s. Here we assume that µ1, µ2 are
unknown and (µ1, µ2) ∈ ℜ × ℜ but σ ∈ ℜ+ is assumed known. With preas-
signed α ∈ (0, 1), derive a level α LR test for H0 : µ1 = µ2 versus H1 : µ1 ≠ µ2
in the implementable form. {Hint: The LR test rejects H0 if and only if
11.3.4 Let the random variables Xi1, ..., Xini be iid N(µi, σ2), i = 1, 2, and that the
X1j’s be independent of the X2j’s. Here we assume that µ1, µ2, σ are all unknown and
(µ1, µ2) ∈ ℜ × ℜ, σ ∈ ℜ+. With preassigned α ∈ (0, 1) and a real number D, show
that a level α LR test for H0 : µ1 – µ2 = D versus H1 : µ1 – µ2 ≠ D would reject H0
if and only if 
 
 {Hint: Re-
peat the techniques from Section 11.3.1.}
11.3.5 (Exercise 11.3.2 Continued) Let the random variables Xi1, ..., Xini be
iid N(µi, 
), i = 1, 2, and that the X1j’s be independent of the X2j’s. Here we
assume that µ1, µ2 are unknown and (µ1, µ2) ∈ ℜ×ℜ but (σ1, σ2) ∈ ℜ+ × ℜ+
are assumed known. With preassigned α ∈ (0, 1) and a real number D, derive
a level α LR test for H0 : µ1 – µ2 = D versus H1 : µ1 – µ2 ≠ D in the
implementable form. {Hint: The LR test rejects H0 if and only if
11.3.6 (Exercise 11.3.3 Continued) Let the random variables Xi1, ...,
Xini be iid N(µi, σ2), i = 1, 2, and that the X1j’s be independent of the X2j’s.
Here we assume that µ1, µ2 are unknown and (µ1, µ2) ∈ ℜ × ℜ but σ ∈ ℜ+
is assumed known. With preassigned α ∈ (0, 1) and a real number D,
derive a level α LR test for H0 : µ1 – µ2 = D versus H1 : µ1 – µ2 ≠ D

11. Likehood Ratio and Other Tests
533
in the implementable form. {Hint: The LR test rejects H0 if and only if
11.3.7 Two types of cars were compared for the braking distances. Test
runs were made for each car in a driving range. Once a car reached the stable
speed of 60 miles per hour, the brakes were applied. The distance (feet) each
car travelled from the moment the brakes were applied to the moment the car
came to a complete stop was recorded. The summary statistics are shown
below:
Car
Sample Size
s
Make A
nA = 12
37.1
3.1
Make B
nB = 10
39.6
4.3
Assume that the elapsed times are distributed as N(µA, σ2) and N(µB, σ2)
respectively for the make A and B cars with all parameters unknown. Test at
5% level whether the average braking distances of the two makes are signifi-
cantly different.
11.3.8 Verify the expressions of the MLE’s in (11.3.15).
11.3.9 Let the random variables Xi1, ..., Xini be iid N(0, 
), i = 1, 2, and that
the X1j’s be independent of the X2j’s. Here we assume that (µ1, µ2) ∈ ℜ × ℜ are
known but (σ1, σ2) ∈ ℜ+ × ℜ+ are unknown. With preassigned α ∈ (0, 1),
derive a level α LR test for H0 : σ1 = σ2 versus H1 : σ1 ≠ σ2 in the implementable
form.
11.3.10 Let the random variables Xi1, ..., Xini be iid N(µi, 
), i = 1, 2, and
that the X1j’s be independent of the X2j’s. Here we assume that µ1, µ2) ∈ ℜ ×
ℜ are known but (σ1, σ2) ∈ ℜ+ × ℜ+ are unknown. With preassigned α ∈ (0,
1), derive a level α LR test for H0 : σ1 = σ2 versus H1 : σ1 ≠ σ2 in the
implementable form.
11.3.11 Let the random variables Xi1, ..., Xini be iid N(µi, 
), i = 1, 2, and
that the X1j’s be independent of the X2j’s. Here we assume that µ1, µ2, s1, s2 are
all unknown and (µ1, µ2) ∈ ℜ2, (σ1, σ2) ∈ ℜ+2. With preassigned α ∈ (0, 1)
and a positive number D, show that the level α LR test for H0 : σ1/σ2 = D
versus H1 : σ1/σ2 ≠ D, would reject H0 if and only if > 
 Fn1 - 1,n2 - 1,α/
2 or < 
 Fn1 - 1,n2 -1,1 - α/2. {Hint: Repeat the techniques from Section
11.3.2.}
11.3.12 Let the random variables Xn1, ..., Xini be iid Exponential(θi),
i = 1, 2, and that the X1j’s be independent of the X2j’s. Here we assume
that θ1, θ2 are unknown and (θ1, θ2) ∈ ℜ+ × ℜ+. With preassigned α ∈
(0, 1), derive a level α LR test for H0 : θ1 = θ2 versus H1 : θ1 ≠ θ2 in the

534
11. Likehood Ratio and Other Tests
implementable form. {Hint: Proceed along Section 11.3.2. With 0 < c < d <
∞, a LR test rejects H0 if and only if 
 < c or > d.}
11.3.13 Two neighboring towns wanted to compare the variations in the
time (minutes) to finish a 5k-run among the first place winners during each
town’s festivities such as the heritage day, peach festival, memorial day, and
other town-wide events. The following data was collected recently by these
two towns:
Town A (xA):
18
20
17
22
19
18
20
18
17
Town B (xB):
20
17
25
24
18
23
Assume that the performances of the first place winners are independent and
that the first place winning times are normally distributed within each town.
Then, test at 1% level whether the two town’s officials may assume σA = σB.
11.3.14 Let the random variables Xi1, ..., Xini be iid N(µi, ∞2), ni ≥ 2, i = 1,
2, 3 and that the X1j’s, X2j’s and X2j’s be all independent. Here we assume that
µ1, µ2, µ3, ∞ are all unknown and (µ1, µ2, µ3) ∈ ℜ3, ∞ ∈ ℜ+. With preassigned
α ∈ (0, 1), show that a level α LR test for H0 : µ1 + µ2 = 2µ3 versus H1 : µ1 +
µ2 
≠ 
2µ3 
would 
reject 
H0 
if 
and 
only 
if
 where 
is understood to be the corresponding pooled sample variance based on n1 + n2
+ n3 – 3 degrees of freedom. {Hint: Repeat the techniques from Section
11.3.1. Under H0, while writing down the likelihood function, keep µ1, µ2 but
replace µ3 by ½(µ1 + µ2) and maximize the likelihood function with respect to
µ1, µ2 only.}
11.3.15 (Behrens-Fisher problem) Let Xi1, ..., Xini be iid N(µi, 
) random
variables, ni ≥ 2, i = 1, 2, and that the X1j’s be independent of the X2j’s. Here we
assume that µ1, µ2, σ1, σ2 are all unknown and (µ1, µ2) ∈ ℜ2, (σ1, σ2) ∈ ℜ+2, σ1
≠ σ2. Let 
 respectively be the sample mean and variance, i = 1, 2. With
preassigned α ∈ (0, 1), we wish to have a level a test for H0 : µ1 = µ2 versus H1
: µ1 ≠ µ2 in the implementable form. It may be natural to use the test statistic
Ucalc where 
 Under H0, the statistic Ucalc
has approximately a Student’s  distribution with 
 Obtain a two-sided approximate t test based on
Ucalc. {Hint: This is referred to as the Behrens-Fisher problem. Its develop-
ment, originated from Behrens (1929) and Fisher (1935, 1939), is histori-
cally rich. Satterthwaite (1946) obtained the approximate distribution of Ucalc
under H0, by matching “moments.” There is a related confidence inter-
val estimation problem for the ratio µ1/µ2 which is referred to as the

11. Likehood Ratio and Other Tests
535
Fieller-Creasy problem. Refer to Creasy (1954) and Fieller (1954). Both these
problems were elegantly reviewed by Kendall and Stuart (1979) and Wallace
(1980). In the Exercise 13.2.10, we have given the two-stage sampling tech-
nique of Chapman (1950) for constructing a fixed-width confidence interval
for µ1 – µ2 with the exact confidence coefficient at least 1 – α.}
11.4.1 A nutritional science project had involved 8 overweight men of
comparable background which included eating habits, family traits, health
condition and job related stress. An experiment was conducted to study the
average reduction in weight for overweight men following a particular regi-
men of nutritional diet and exercise. The technician weighed in each individual
before they were to enter this program. At the conclusion of the study which
took two months, each individual was weighed in again. It was believed that
the assumption of a bivariate normal distribution would be reasonable to use
for (X1, X2).
Test at 5% level whether the true average weights taken before and after
going through the regimen are significantly different. At 10% level, is it pos-
sible to test whether the true average weight taken after going through the
regimen is significantly lower than the true average weight taken before going
through the regimen? The observed data is given in the adjoining table.
ID# of
Weight (x1, pounds)
Weight (x2, pounds)
Individual
Before Study
After Study
1
235
220
2
189
175
3
156
150
4
172
160
5
165
169
6
180
170
7
170
173
8
195
180
{Hint: Use the methodology from Section 11.4.1.}
11.4.2 Suppose that the pairs of random variables (X1i, X2i) are iid bivariate
normal, N2(µ1, µ2, 
 ρ), i = 1, ..., n(≥ 2). Here we assume that µ1, µ2 are
unknown but 
 ρ are known where (µl, σl) ∈ ℜ × ℜ+, l = 1, 2 and –1 < ρ
< 1. With fixed α ∈ (0, 1), construct a level α test for a null hypothesis H0 : µ1
= µ2 against a two-sided alternative hypothesis H1 : µ1 ≠ µ2 in the implementable
form. {Hint: Improvise with the methodology from Section 11.4.1.}
11.4.3 Suppose that the pairs of random variables (X1i, X2i) are iid
bivariate normal, N2(µ1, µ2, σ2, σ2, ρ), i = 1, ..., n(≥ 2). Here we assume

536
11. Likehood Ratio and Other Tests
that all the parameters µ1, µ2, σ2 and ρ are unknown where (µ1, µ2) ∈ ℜ × ℜ,
σ ∈ ℜ+ and –1 < ρ < 1. With fixed α ∈ (0, 1), construct a level α test for a null
hypothesis H0 : µ1 ≠ µ2 against a two-sided alternative hypothesis H1 : µ1 ≠ µ2
in the implementable form. {Hint: Improvise with the methodology from Sec-
tion 11.4.1. Is it possible to have the t test based on 2(n – 1) degrees of
freedom?}
11.4.4 Suppose that the pairs of random variables (X1i, X2i) are iid bivariate
normal, N2(µ1, µ2, 
 0), i = 1, ..., n(≥ 2). Here we assume that all the
parameters µ1, µ2, 
 and 
 are unknown where (µl, σl) ∈ ℜ × ℜ+, l = 1, 2.
Show that the MLE’s for µl, 
 are respectively given by 
 and
 l = 1, 2. {Hint: Consider the likelihood function
from (11.4.8) and then proceed by taking its natural logarithm, followed by its
partial differentiation.}
11.4.5 Suppose that the pairs of random variables (X1i, X2i) are iid bivariate
normal, N2(µ1, µ2, 
 ρ), i = 1, ..., n(≥ 2). Here we assume that all the
parameters µ1, µ2, 
 and ρ are unknown where (µl, σl) ∈ ℜ × ℜ+, l = 1,
2, –1 < ρ < 1. Consider the likelihood function from (11.4.6) and then pro-
ceed by taking its natural logarithm, followed by its partial differentiation.
(i)
Simultaneously solve ∂logL/∂µl = 0 to show that 
 is the MLE of µl,
l = 1, 2;
(ii)
Simultaneously solve ∂logL/ 
 = 0, ∂logL/∂ρ = 0, l = 1, 2 and
show that the MLE’s for 
 and ρ are respectively 
 and the sample correlation coefficient,
11.4.6 A researcher wanted to study whether the proficiency in two spe-
cific courses, sophomore history (X1) and calculus (X2), were correlated.
From the large pool of sophomores enrolled in the two courses, ten students
were randomly picked and their midterm grades in the two courses were
recorded. The data is given below.
Student Number:
1
2
3
4
5
6
7
8
9
10
History Score (X1):
80
75
68
78
80
70
82
74
72
77
Calculus Score (X2): 90
85
72
92
78
87
73
87
74
85
Assume that (X1, X2) has a bivariate normal distribution in the population. Test
whether ρX1,X2 can be assumed zero with α = .10.
11.4.7 In what follows, the data on systolic blood pressure ( X1) and

11. Likehood Ratio and Other Tests
537
age ( X2) for a sample of 10 women of similar health conditions are given.
ID #
X1
X2
ID #
X1
X2
1
122
41
6
144
44
2
148
52
7
138
51
3
146
54
8
138
56
4
162
60
9
145
49
5
135
45
10
144
58
At 5% level, test whether the population correlation coefficient ρX1,X2 is sig-
nificantly different from zero. Assume the bivariate normality of (X1, X2).
11.4.8 The strength of the right and left grips, denoted by X1 and X2 re-
spectively, were checked for 12 auto accident victims during routine thera-
peutic exams at a rehab center. The observed values of X1 and X2 are both
coded scores between zero and ten. Here, a low (high) value indicates signifi-
cant weakness (strength) in the grip. Assume the bivariate normality of (X1,
X2). The data is given in the enclosed table.
ID #
X1
X2
ID #
X1
X2
ID #
X1
X2
1
6.2
6.8
5
3.7
2.8
9
7.7
6.4
2
5.3
4.9
6
5.4
6.2
10
4.9
7.8
3
7.1
7.6
7
5.0
5.8
11
6.5
6.5
4
7.8
6.9
8
8.2
7.9
12
5.2
6.0
(i)
At 1% level, test whether the mean grip strengths in the two arms
are same;
(ii)
At 5% level, test whether the variabilities of the strengths in the two
arms are same;
(iii)
At 1% level, test if the right and left hand grip’s strengths are sig-
nificantly correlated.

This page intentionally left blank

12
Large-Sample Inference
12.1 Introduction
In the previous chapters we gave different approaches of statistical infer-
ence. Those methods were meant to deliver predominantly exact answers,
whatever be the sample size n, large or small. Now, we summarize approxi-
mate confidence interval and test procedures which are meant to work when
the sample size n is large. We emphasize that these methods allow us to con-
struct confidence intervals with approximate confidence coefficient 1 − α or
construct tests with approximate level α.
Section 12.2 gives some useful large-sample properties of a maximum
likelihood estimator (MLE). In Section 12.3, we introduce large-sample con-
fidence interval and test procedures for (i) the mean µ of a population having
an unknown distribution, (ii) the success probability p in the Bernoulli distri-
bution, and (iii) the mean λ of a Poisson distribution. The variance stabilizing
transformation is introduced in Section 12.4 and we first exhibit the two
customary transformations 
 and 
 used respectively in the case
of a Bernoulli(p) and Poisson(λ) population. Section 12.4.3 includes Fisher’s
tanh-1(ρ) transformation in the context of the correlation coefficient ρ in a
bivariate normal population.
12.2
The Maximum Likelihood Estimation
In this section, we provide a brief introductory discussion of some of the
useful large sample properties of the MLE. Consider random variables X1, ...,
Xn which are iid with a common pmf or pdf f(x; θ) where x ∈ χ ⊆ ℜ and θ ∈
Θ ⊆ ℜ. Having observed the data X = X, recall that the likelihood function is
given by
We denote it as a function of θ alone because the observed data X = (x1, ..., xn)
is held fixed.
We make some standing assumptions. These requirements are, in spirit,
similar to those used in the derivation of Cramér-Rao inequality.
539

540
12. Large-Sample Inference
A1: The expressions 
are assumed finite for all x ∈ χ and for all θ in an interval around the true
unknown value of θ.
A2: Consider the three integrals 
 and
 The first two integrals amount to zero whereas the third
integral is positive for the true unknown value of θ.
A3: For every θ in an interval around the true unknown value of θ,
 < a(x) such that Eθ[a(X1)] < b where b is a constant
which is independent of θ.
The assumptions A1-A3 are routinely satisfied by many standard distribu-
tions, for example, binomial, Poisson, normal and exponential. In order to
find the MLE for θ, one frequently takes the derivative of the likelihood func-
tion and then solve the likelihood equation:
One will be tempted to ask: Does this equation necessarily have any solution?
If so, is the solution unique? The assumptions A1-A3 will guarantee that we
can answer both questions in the affirmative. For the record, we state the
following results:
In other words, the MLE of q will stay close to the unknown but true
value of q with high probability when the sample size n is sufficient large.
In other words, a properly normalized version of the MLE of θ will converge
(in distribution) to a standard normal variable when the sample size n is large.
In a variety of situations, the asymptotic variance of the MLE 
 coin-
cides with 1/I (θ). One may recall that 1/I (θ) is the Cramér-Rao lower bound
(CRLB) for the variance of unbiased estimators of θ. What we are claiming
then is this: In many routine problems, the variance of 
 the MLE of θ,
has asymptotically the smallest possible value. This phenomenon was referred
to as the asymptotic efficiency property of the MLE by Fisher (1922,1925a).

12. Large-Sample Inference
541
Fisher (1922,1925a) gave the foundations of the likelihood theory. Cramér
(1946a, Chapter 33) pioneered the mathematical treatments under full gener-
ality with assumptions along the lines of A1-A3. One may note that the as-
sumptions A1-A3 give merely sufficient conditions for the stated large sample
properties of an MLE.
Many researchers have derived properties of MLE’s comparable to those
stated in (12.2.3)-(12.2.4) under less restrictive assumptions. The reader may
look at some of the early investigations due to Cramér (1946a,b), Neyman
(1949), Wald (1949a), LeCam (1953,1956), Kallianpur and Rao (1955) and
Bahadur (1958). This is by no means an exhaustive list. In order to gain
historical as well as technical perspectives, one may consult Cramér (1946a),
LeCam (1986a,b), LeCam and Yang (1990), and Sen and Singer (1993). Ad-
mittedly, any detailed analysis is way beyond the scope of this book.
We should add, however, that in general an MLE  may not be consistent
for θ. Some examples of inconsistent MLE’s were constructed by Neyman
and Scott (1948) and Basu (1955b). But, under mild regularity conditions, the
result (12.2.3) shows that an MLE  is indeed consistent for the parameter θ
it is supposed to estimate in the first place. This result, in conjunction with the
invariance property (Theorem 7.2.1), make the MLE’s very appealing in prac-
tice.
Example 12.2.1 Suppose that X1, ..., Xn are iid with the common Cauchy
pdf f(x; θ) = π-1 {1 + (x − θ)2}-1 I(x ∈ ℜ) where θ(∈ ℜ) is the unknown
parameter. The likelihood function from (12.2.1) is then given by
Now, the MLE of θ is a solution of the equation 
 = 0 which
amounts to solving the equation
for θ. Having observed the data x1, ..., xn, one would solve the equation (12.2.5)
numerically with an iterative approach. Unfortunately, no analytical expres-
sion for the MLE 
 is available.
We leave it as Exercise 12.2.1 to check that the Fisher information I (θ) in
a single observation is 2. Then, we invoke the result from (12.2.4) to con-
clude that
even though we do not have any explicit expression for 
 !

542
12. Large-Sample Inference
Example 12.2.2 (Example 12.2.1 Continued) Consider the enclosed data
from a Cauchy population with pdf π-1 {1 + (x − θ)2}-1 I(x ∈ ℜ), θ ∈ ℜ. The
sample size is 30 which is regarded large for practical purposes. In order to
find the maximum likelihood estimate  of θ, we solved the likelihood equa-
tion (12.2.5) numerically. We accomplished this with the help of MAPLE. For
the observed data, the solution turns out to be  = 1.4617. That is, the MLE
of θ is 1.4617 and its approximate variance is 2n-1 with n = 30.
2.23730
2.69720
-.50220
-.11113
2.13320
0.04727
0.51153
2.57160
 1.48200
-.88506
2.16940
-27.410
-1.1656
1.78830
28.0480
-1.7565
-3.8039
 3.21210
5.03620
 2.60930
2.77440
2.66690
-.23249
8.71200
1.95450
0.87362
 -10.305
3.03110
 2.47850
1.03120
In view of (12.2.6), an approximate 99% confidence interval for θ is con-
structed as 
 which simplifies to 1.4617 ± .66615. We may add
that the data was simulated with θ = 2 and the true value of ? does belong to
the confidence interval. !
Example 12.2.3 (Example 12.2.2 Continued) Suppose that a random sample
of size n = 30 is drawn from a Cauchy population having its pdf f(x; q) = π-1
{1 = (x - θ)2}-1 I(x ∈ ℜ) where q (∈ ℜ) is the unknown parameter. We are told
that the maximum likelihood estimate of θ is 5.84. We wish to test a null
hypothesis H0 : q = 5 against an alternative hypothesis H1 : θ ≠ 5 with approxi-
mate 1% level. We argue that approximately 
2533 but za/2 = 2.58. Since |zcalc| exceeds za/2, we reject H0 at the approximate
level a. That is, at the approximate level 1%, we have enough evidence to
claim that the true value of θ is different from 5.!
Exercises 12.2.3-12.2 4 are devoted to a Logistic distribution with
the location parameter θ. The situation is similar to what we have
encountered in the case of the Cauchy population.
12.3 Confidence Intervals and Tests of Hypothesis
We start with one-sample problems and give confidence intervals and tests
for the unknown mean µ of an arbitary population having a finite variance.
Next, such methodologies are discussed for the success probability p
in Bernoulli trials and the mean λ in a Poisson distribution. Then, these

12. Large-Sample Inference
543
techniques are extended for analogous two-sample problems. These method-
ologies rely heavily upon the central limit theorem (CLT) introduced in Chap-
ter 5.
12.3.1
The Distribution-Free Population Mean
Suppose that X1, ..., Xn are iid real valued random variables from a population
with the pmf or pdf f(x; θ) where the parameter vector θ is unknown or the
functional form of f is unknown. Let us assume that the variance σ2 = σ2(θ)
of the population is finite and positive which in turn implies that the mean µ ≡
µ(θ) is also finite. Suppose that µ and σ are both unknown. Let
 be respectively the sample
mean and variance for n ≥ 2. We can apply the CLT and write: As n → ∞,
From (12.3.1) what we conclude is this: For large sample size n, we should
be able to use the random variable 
 as an approximate pivot
since its asymptotic distribution is N(0, 1) which is free from both µ, s.
Confidence Intervals for the Population Mean
Let us first pay attention to the one-sample problems. With preassigned α
∈ (0, 1), we claim that
which leads to the confidence interval
with approximate confidence coefficient 1 − α. Recall that zα/2 is the upper
100(α/2)% point of the standard normal distribution. See the Figure 12.3.1.
Figure 12.3.1. Standard Normal PDF: The Shaded Area Between
−zα/2 and zα/2 is 1 − α

544
12. Large-Sample Inference
In Chapter 9 we had a similar confidence interval for the unknown mean with
exact confidence coefficient 1 − α. That methodology worked for all sample
sizes, large or small, but at the expense of the assumption that the population
distribution was normal. The present confidence interval from (12.3.3) works
for large n, whatever be the population distribution as long as s is positive and
finite. We do realize that the associated confidence coefficient is claimed only
approximately 1 − α.
Next, let us briefly discuss the two-sample problems. Suppose that the
random variables Xi1, ..., Xini are iid from the ith population having a common
pmf or pdf fi(xi; θi) where the parameter vector θi is unknown or the func-
tional form of fi itself is unknown, i = 1, 2. Let us assume that the variance
 of the ith population is finite and positive which in turn implies
that its mean µi ≡ µi(θi) is also finite, i = 1, 2. Assume that  µiσ2
i are unknown,
i = 1, 2. Let us also suppose that the X1j’s are independent of the X2j’s and
denote the sample mean 
 and sample variance
  2. In this case, one can
invoke the following form of the CLT: If n1 → ∞, n2 → 8 such that n1/n2 → d
for some 0 < δ < ∞, then
Thus, using Slutsky’s Theorem, we can immediately conclude: If n1 → ∞, n2
→ ∞ such that n1/n2 → δ for some 0 < δ, < ∞, then
For large sample sizes n1 and n2, we should be able to use the random variable
 as an approximate pivot
since its asymptotic distribution is N(0, 1) which is free from µ1, µ2, σ1 and
σ2. With preassigned α ∈ (0, 1), we claim that
which leads to the confidence interval

12. Large-Sample Inference
545
with approximate confidence coefficient 1 − α.
Tests of Hypotheses for the Population Mean
Let us first pay attention to the one-sample problems. With preassigned α
∈ (0, 1), we wish to test a null hypothesis H0 : µ = µ0 at an approximate level
α where µ0 is a specified real number.. The alternative hypotheses will be
given shortly. One considers the test statistic
 approximately distributed as N(0,1)    (12.3.8)
for large n when µ = µ0.
Upper-Sided Alternative Hypothesis
Here we test H0 : µ = µ0 versus H1 : µ > µ0. Along the lines of Section 8.4,
we can propose the following upper-sided approximate level a test:
Reject H0 in favour of H1 : µ > µ0 if and only if U > Zα
 Figure 12.3.2. Upper-Sided Standard Normal Rejection Region
Lower-Sided Alternative Hypothesis
Here we test H0 : µ = µ0 versus H1 : µ < µ0. Along the lines of Section 8.4,
we can propose the following lower-sided approximate level α test:
Figure  12.3.3. Lower-Sided Standard Normal Rejection Region

546
12. Large-Sample Inference
Two-Sided Alternative Hypothesis
Here we test H0 : µ = µ0 versus H1 : µ ≠ µ0. Along the lines of Section
11.2.1, we can propose the following two-sided approximate level α test:
Figure 12.3.4. Two-Sided Standard Normal Rejection Region
Next, let us briefly address the two-sample problems. With preassigned α
∈ (0, 1), we wish to test a null hypothesis H0 : µ1 = µ2 with an approximate
level α. The specific alternative hypotheses will be given shortly. Using (12.3.5),
one considers the test statistic
for large n1, n2 when µ1 = µ2.
Upper-Sided Alternative Hypothesis
Here we test H0 : µ1 = µ2 versus H1 : µ1 > µ2. See, for example, the Figure
12.3.2. Along the lines of Section 8.4, we can propose the following upper-
sided approximate level α test:
Lower-Sided Alternative Hypothesis
Here we test H0 : µ1 = µ2 versus H1 : µ1 < µ2. See, for example, the Figure
12.3.3. Along the lines of Section 8.4, we can propose the following lower-
sided approximate level α test:

12. Large-Sample Inference
547
Two-Sided Alternative Hypothesis
Here we test H0 : µ1 = µ2 versus H1 : µ1 ≠ µ2. See, for example, the Figure
12.3.4. Along the lines of Section 11.2.1, we can propose the following two-
sided approximate level α test:
In practice, a sample of size thirty or more is considered
large. Refer to pages 258-261.
Example 12.3.1 The management of a rehab center for patients with acute
illnesses wants to estimate the average number of days its patients typically
spends with them. A random sample of 40(= n) patient’s records gave
 days and sn = 5.6 days. The administrator of the center now wants
to look at a 90% confidence interval for the average (µ) number of days of
stay. We do not have any specifics regarding the population distribution of the
number of days these patients spend in the center. But, the sample size n is
large, and thus we invoke (12.3.3). An approximate 90% confidence interval
for µ would be 
  that is (19.2435, 22.1565). The ad-
ministrator would conclude that the average stay at the center is between 19.2
and 22.2 days with approximate 90% confidence. !
Example 12.3.2 A local department store wanted to estimate the average
account balance owed by the store’s typical charge card holders. From a list
of charge card accounts, the store manager drew a random sample of 50(= n)
accounts for closer inspection. The 50 accounts with charge card balance
gave 
 and sn = $12.67. The manager wished to look at a 95%
confidence interval for the average (µ) amount owed per card. Again we do
not have any specifics regarding the population distribution for the amount
owed per card. But, the sample size n is large, and thus we invoke (12.3.3).
An approximate 95% confidence interval for µ would be
  that is (101.77, 108.79). The manager concluded
that the average amount owed per charge card at the store was between
$101.77 and $108.79 with approximate 95% confidence. !
Example 12.3.3 Two instructors were teaching different but large sec-
tions of the first-year undergraduate statistics service course from the same
text book. After the mid-term examinations in both sections, the accompa-
nied data was compiled from the randomly selected students’ grades. Based
on this data, is it possible to conclude at an approximate 1% level that the
students in the second section outscored the students in the first section on

548
12. Large-Sample Inference
the average? In Section #i, suppose that the unknown population average
exam grade is µi, i = 1, 2. We wish to test H0 : µ1 = µ2 versus H1 : µ1 < µ2 at
an approximate 1% level.
Instructor #1
Instructor #2
n1 = 55
n2 = 40
 = 72.4
 = 74.4
s1n1 = 5.29
s2n2 = 3.55
Now, we have
We find that z.01 = 2.33 and hence in view of (12.3.14), we fail to reject H0 at
an approximate 1% level since wcalc > -z.01. In other words, the students’
average mid-term exam performances in the two sections appear to be the
same at the approximately 1% level. !
Remark 12.3.1 In general, suppose that one has a consistent estimator
 for θ such that 
 as n → ∞, with some continu-
ous function b(θ) > 0, and for all θ ∈ Θ. Then, using Slutsky’s Theorem, we
can claim that 
 is an approximate 1 − α confidence in-
terval for θ if n is large. Look at the following example.
Example 12.3.4 In (12.3.3) we have given an approximate 1 − α confi-
dence interval for µ if n is large when the population distribution is unspeci-
fied. Now, suppose that we wish to construct an approximate 1 − α confi-
dence interval for µ2. Using Mann-Wald Theorem from Chapter 5, we claim
that 
 as n → ∞. Refer to Example 5.3.7. Thus,
an approximate 1 − α confidence interval for µ2 can be constructed
 when n is large. !
12.3.2
The Binomial Proportion
Suppose that X1, ..., Xn are iid Bernoulli(p) random variables where 0 < p < 1
is the unknown parameter. The minimal sufficient estimator of p is the sample
 mean which is same as the sample proportion 
 of the number of 1’s,
that is the proportion of successes in n independent replications. We can
immediately apply the CLT and write:

12. Large-Sample Inference
549
We use 
 as an approximate pivot since its asymp-
totic distribution is N(0, 1) which is free from p.
Confidence Intervals for the Success Probability
Let us first pay attention to the one-sample problems. With preassigned α ∈
(0, 1), we claim that
which leads to the confidence interval
with approximate confidence coefficient 1 − α. Recall that za/2 is the upper
100(α/2)% point of the standard normal distribution. See, for example, the
Figure 12.3.1.
A different confidence interval for p is given in Exercise 12.3.8.
Next, let us briefly discuss the two-sample problems. Suppose that the
random variables Xi1, ..., Xini are iid from the ith population having the
Bernoulli(pi) distribution where 0 < pi < 1 is unknown, i = 1, 2. We suppose
that the X1j’s are independent of the X2j’s and denote the sample 
mean obtained from the ith population, i = 1, 2. In this case, one invokes the
following CLT: If n1 → ∞, n2 → ∞ such that n1/n2 → δ for some 0 < δ < ∞,
then
For large sample sizes n1 and n2, we should be able to use the random variable
 as an
approximate pivot since its asymptotic distribution is N(0, 1) which is free
from p1 and p2. With preassigned α ∈ (0, 1), for large n1 and n2, we claim that
which leads to the confidence interval

550
12. Large-Sample Inference
for (p1 - p2) with approximate confidence coefficient 1 − α.
Tests of Hypotheses for the Success Probability
Let us first pay attention to the one-sample problems. With preassigned α
∈ (0, 1), we test a null hypothesis H0 : p = p0 with an approximate level α. The
specific alternative hypotheses will be given shortly. Here, 0 < p0 < 1 is a
fixed. We consider the test statistic
for large n when p = p0.
Upper-Sided Alternative Hypothesis
Here we test H0 : p = p0 versus H1 : p > p0. See, for example, the Figure
12.3.2. We can propose the following upper-sided approximate level a test:
Lower-Sided Alternative Hypothesis
Here we to test H0 : p = p0 versus H1 : p < p0. See, for example, the Figure
12.3.3. We can propose the following lower sided approximate level α test:
Two-Sided Alternative Hypothesis
Here we test H0 : p = p0 versus H1 : p < p0. See, for example, the Figure
12.3.4. We can propose the following two sided approximate level a test:
Remark 12.3.2 In the case of the upper- or lower-sided alternative
hypothesis, one should in fact prefer to use the corresponding randomized
UMP tests discussed in Chapter 8 instead of the proposed approximate
tests given by (12.3.23) and (12.3.24). On the other hand, when n is
large, finding the cut-off points for implementing the UMP test may be-
come too cumbersome. Hence, the approximate tests in proposed (12.3.23)
and (12.3.24) are also used in practice whenever n is large.
Next, let us briefly address the two-sample problems. With preassigned
α ∈ (0, 1), we test a null hypothesis H0 : p1 = p2 with the approximate level
α. The specific alternative hypotheses will be given shortly. Under the null
hypothesis H0 we have p1 = p2 = p where 0 < p < 1 is unknown and

12. Large-Sample Inference
551
we can write the variance of 
  Now, the
common p can be estimated by the pooled estimator, namely,
Thus, we consider the test statistic
for large n1, n2 when p1 = p2.
Upper-Sided Alternative Hypothesis
Here we test H0 : p1 = p2 versus H1 : p1 > p2. See, for example, the Figure
12.3.2. We can propose the following upper-sided approximate level a test:
Lower-Sided Alternative Hypothesis
Here we test H0 : p1 = p2 versus H1 : p1 < p2. See, for example, the Figure
12.3.3. We can propose the following lower-sided approximate level a test:
Two-Sided Alternative Hypothesis
Here we test H0 : p1 = p2 versus H1 : p1 ≠ p2. See, for example, the Figure
12.3.4. We can propose the following two-sided approximate level a test:.
In practice, a sample of size thirty or more is considered large.
Example 12.3.5 The members of a town council wished to estimate the
percentage of voters in favor of computerizing the local library’s present
cataloging system. A committee obtained a random sample of 300 voters,
of whom 175 indicated that they favored the proposed computerization. Let
p denote the proportion of voters in the town who favored the proposed
computerization. Then,  = 175/300 = .58333. Since n = 300 is large, in
view of (12.3.18), an approximate 90% confidence interval for p will be

552
12. Large-Sample Inference
 which simplifies to .58333±.04682. The committee
concluded with approximate 90% confidence that between 53.7% and 63%
of the voters in the town favored the proposed computerization project.!
Example 12.3.6 The manager of a local chain of fitness centers claimed
that 65% of their present members would renew membership for the next
year. A close examination of a random sample of 100 present members showed
that 55 renewed membership for the next year. At 1% level, is there sufficient
evidence that the manager is off the mark? Let p denote the proportion of
present members who would renew membership. The manager’s null hy-
pothesis is going to be H0 : p = .65 but what should be the appropriate alterna-
tive hypothesis? It is reasonable to assume that the manager does not believe
that p is likely to be larger than .65? Because if he did, he would have claimed
a higher value of p to bolster the image. So, we formulate the alternative
hypothesis as H1 : p < .65. Now, we have 
 55/100 = .55 and z.01 = 2.33.
Since n = 100 is large, in view of (12.3.24), we obtain
At an approximate 1% level, we do not reject H0 since ucalc > −z.01, that is we
do not have sufficient evidence to claim that the manager’s belief is entirely
wrong. !
Example 12.3.7 The two locations of a department store wanted to com-
pare proportions of their satisfied customers. The store at location #1 found
that out of 200 randomly picked customers, 155 expressed satisfaction,
whereas the store at location #2 found that out of 150 randomly picked cus-
tomers, 100 expressed satisfaction. Let pi stand for the population proportion
of satisfied customers for the store at location #i, i = 1, 2. The question we
want to address is whether the proportions of satisfied customers at the two
locations are same at 5% level. We consider testing H0 : p1 = p2 versus H1 : p1
≠ p2 with α = .05 so that z.025 = 1.96. Since the sample sizes are large, we
apply the test procedure from (12.3.30). From (12.3.26), observe that 
=
(155+100)  /(200 + 150) ≈ .72857. Thus, in view of (12.3.27), one has
which exceeds z.025. Thus, we reject H0 : p1 = p2 in favor of H1 : p1 ≠ p2 at an
approximate 5% level. In other words, the proportions of satisfied customers
at the two locations appear to be significantly different at an approximate 5%
level. !

12. Large-Sample Inference
553
12.3.3
The Poisson Mean
Suppose that X1, ..., Xn are iid Poisson(λ) where 0 < λ < ∞ is the unknown
parameter. The minimal sufficient estimator of ? is the sample mean 
 which
is denoted by 
 We immediately apply the CLT to write:
We will use 
 as an approximate pivot since its asymptotic
distribution is N(0, 1) which is free from λ.
Confidence Intervals for the Mean
Let us first pay attention to the one-sample problems. With preassigned α
∈ (0, 1), we claim that
which leads to the confidence interval
with an approximate confidence coefficient 1 − α. Recall that zα/2 is the upper
100(α/2)% point of the standard normal distribution. See, for example, the
Figure 12.3.1.
A different confidence interval for λ is given in the Exercise 12.3.14.
Next, let us briefly discuss the two-sample problems. Suppose that the
random variables Xi1, ..., Xini are iid from the ith population having the Poisson(λi)
distribution where 0 < λi < ∞ is unknown, i = 1, 2. We suppose that the X1j’s
are independent of the X2j’s and denote the sample mean 
 obtained
from the ith population, i = 1, 2. In this case, one invokes the following CLT:
If n1 → ∞, n2 → 8 such that n1/n2 → δ for some 0 < δ < ∞, then
For large sample sizes n1 and n2, we should use the random variable
 as an approximate pivot

554
12. Large-Sample Inference
since its asymptotic distribution is N(0, 1) which is free from λ1, λ2. With
preassigned α ∈ (0, 1), we claim that
which leads to the confidence interval
for (λ1 − λ2) with an approximate confidence coefficient 1 − α.
Tests of Hypotheses for the Mean
Let us first pay attention to the one-sample problems. With preassigned α
∈ (0, 1), we test a null hypothesis H0 : λ = λ0 with an approximate level α
where λ0(> 0) is a fixed number. The specific alternative hypotheses will be
given shortly. We consider the test statistic
 approximately distriuted as N (0, 1)  (12.3.37)
for large n when λ = λ0.
Upper-Sided Alternative Hypothesis
Here we test H0 : λ = λ0 versus H1 : λ > λ0. See, for example, the Figure
12.3.2. We can propose the following upper-sided approximate level α test:
Lower-Sided Alternative Hypothesis
Here we test H0 : λ = λ0 versus H1 : λ < λ0. See, for example, the Figure
12.3.3. We can propose the following lower-sided approximate level α test:
Two-Sided Alternative Hypothesis
Here we test H0 : λ = λ0 versus H1 : λ ≠ λ0. See, for example, the Figure
12.3.4. We can propose the following two-sided approximate level α test:

12. Large-Sample Inference
555
The Remark 12.3.2 again holds in essence in the case of the Poisson prob-
lem. Also, one can easily tackle the associated two-sample problems in the
Poisson case. We leave out the details as exercises.
In practice, a sample of size thirty or more is considered large.
Exercise 12.3.8 Customer arrivals at a train station’s ticket counter have a
Poisson distribution with mean λ per hour. The station manager wished to
check whether the rate of customer arrivals is 40 per hour or more at 10%
level. The data was collected for 120 hours and we found that 5,000 custom-
ers arrived at the ticket counter during these 120 hours. With α = .10 so that
z.10 = 1.28, we wish to test H0 : λ = 40 versus H1 : λ > 40. We have  = 5000/
120 ≈ 41.6667. Now, in view of (12.3.38), we obtain
 ≈ 2.8868 which is larger than z.10. Thus,
we reject H0 : λ = 40 in favor of H1 : λ > 40 at approximate 10% level. That is,
it appears that customers arrive at the ticket counter at a rate exceeding 40 per
hour. !
12.4 The Variance Stabilizing Transformations
Suppose that we have a sequence of real valued statistics {Tn; n ≥ 1} such
that n½(Tn − θ) 
 N(0, σ2) as n → ∞. Then, the Mann-Wald Theorem from
Section 5.3.2 will let us conclude that
if g(.) is a continuous real valued function and g′(θ) is finite and nonzero.
When σ2 itself involves θ, one may like to determine an appropriate function
g(.) such that for large n, the approximate variance of the associated trans-
formed statistic g(Tn) becomes free from the unknown parameter θ. Such a
function g(.) is called a variance stabilizing transformation.
In the case of both the Bernoulli and Poisson problems, discussed in the
previous section, the relevant statistic Tn happened to be the sample mean 
and its variance was given by p(1 − p)/n or λ/n as the case may be. In what
follows, we first exhibit the variance stabilizing transformation g(.) in the
case of the Bernoulli and Poisson problems. We also include the famous arctan
transformation due to Fisher, obtained in the context of asymptotic distribu-
tion of the sample correlation coefficient in a bivariate normal population.

556
12. Large-Sample Inference
12.4.1
The Binomial Proportion
Let us go back to Section 12.3.2. Suppose that X1, ..., Xn are iid Bernoulli(p)
random variables where 0 < p < 1 is the unknown parameter. The minimal
sufficient estimator of p is the sample mean 
 which is same as the sample
proportion 
 of successes in n independent runs. One has, 
 and
 /n. Even though 
 as
n → ∞, it will be hard to use 
 as a pivot to construct
tests and confidence intervals for p. Look at the Exercise 12.3.8. Since the
normalizing constant in the denominator depends on the unknown parameter
p, the “power” calculations will be awkward too.
We invoke Mann-Wald Theorem from (12.4.1) and require a suitable func-
tion g(.) such that the asymptotic variance of 
 becomes free
from p. In other words, we want to have
that is 
 We substitute p = sin2(θ) to write
From (12.4.3), it is clear that the transformation 
should be
looked at carefully. Let us now consider the asymptotic distribution of
We apply (12.4.1) with g(p) = 
 to claim that
since we have 
 That is, for large n, we
can consider the pivot
which is approximately N(0,1) (12.4.4)
In the literature, for moderate values of n, the following fine-tuned approxi-
mation is widely used:

12. Large-Sample Inference
557
See Johnson and Kotz (1969, Chapter 3, Section 8.1) for a variety of other
related approximations.
For large n, one may use either (12.4.4) or (12.4.5) to derive an approxi-
mate 100(1 − α)% confidence interval for p. Also, in order to test the null
hypothesis H0 : p = p0, for large n, one may use the test statistic
.
to come up with an approximate level α test against an appropriate alternative
hypothesis. The details are left out for brevity.
Example 12.4.1 (Example 12.3.5 Continued) Let p denote the proportion
of voters in the town who favored the proposed computerization. We wish to
test a null hypothesis H0 : p = .60 against an alternative hypothesis H1 : p < .60.
In a random sample of 300 voters, 175 indicated that they favored the pro-
posed computerization. Now, 
 = 175/300 = .58333 and we have
 ≈ −.588 in view of
(12.4.6), since n = 300 is large. Now, one has z.01 = 2.33 and thus at 1% level,
we fail to reject H0. In other words, we conclude that at an approximate 1%
level, we do not have enough evidence to validate the claim that less than sixty
percent of the voters are in favor of computerizing the library’s present cata-
loging system. !
Example 12.4.2 (Example 12.3.6 Continued) The manager of a local chain
of fitness centers claimed that 65% of their present members would renew
memberships for the next year. A random sample of 100 present members
showed that 55 have renewed memberships for the next year. At 1% level, is
there sufficient evidence to claim that the manager is off the mark? Let p
denote the proportion of present members who would renew membership.
The null hypothesis is H0 : p = .65 and the alternative hypothesis is H1 : p <
.65. We have  = 55/100 = .55 and z.01 = 2.33. Since n = 100 is large, in view
of (12.4.6), we obtain 
 ≈ −
2.0453. At an approximate 1% level, we do not reject H0 since zcalc > −z.01. So,
we do not have sufficient evidence to claim that the manager’s belief is en-
tirely wrong. !
Example 12.4.3 (Examples 12.3.5 and 12.4.1 Continued) In a random
sample of 300 voters, 175 indicated that they favored the proposed com-
puterization. Let p denote the proportion of voters in the whole town who
favor the proposed computerization. Then, 
 = .58333. Since n =
300 is large, in view of (12.4.4), an approximate 90% confidence interval
for 
 which reduces to
.86912 ± .04749. We conclude that 
 lies between .82163 and

558
12. Large-Sample Inference
.91661, and hence p lies between .5362 and .6297. We report that between
53.62% and 62.97% of the voters in the town favor the proposed computer-
ization. !
In Examples 12.4.1-12.4.3, the sample size n was so large that
the results obtained after variance stabilization looked nearly
identical to what was found in Section 12.3.1.
Example 12.4.4 (Examples 12.3.5 and 12.4.1 Continued) Suppose that in
a random sample of 30 voters, 18 indicated that they favored the proposed
computerization. Let p denote the proportion of voters in the town who fa-
vored the proposed computerization. Then, 
  Since n = 30 is
large, in view of (12.4.4), an approximate 90% confidence interval for
 will be 
 which reduces to .88608 ±
.15017. In other words, we conclude that 
 lies between .73591
and 1.03625, and hence p lies between .45059 and .74046. We report that
among the voters, between 45.06% and 74.05% favor the proposed comput-
erization.
On the other hand, in view of (12.3.18), an approximate 90% confidence
interval for p will be 
 which simplifies to .6±.14713.
We conclude that among the voters, between 45.29% and 74.71% in the
town favor the proposed computerization. Now, one can feel some difference
in the two approximations. !
For moderate values of n, an approximate 100(1 − α)% confidence inter-
val for p based on (12.4.4) is expected to fare a little better than that based on
(12.3.18). An approximate 100(1 − α)% confidence interval for
p would be (bL, bU) where 
 which are based on (12.4.4). Another
approximate 100(1 − α)% confidence interval for p would be (aL, aU) where
 which are based on
(12.3.18). Since aL, bL may fall below zero or aU, bU may exceed unity, instead
of the intervals (aL, aU) and (bL, bU) we may respectively consider the confi-
dence interval estimators
In Table 12.4.1, we exhibit these two respective intervals based on
simulated data from the Bernoulli(p) population with 10 replications when

12. Large-Sample Inference
559
α = .05, p = .6, .7 and n = 30.
Table 12.4.1. Comparison of Ten Approximate 95% Confidence
Intervals Based on (12.3.18) and (12.4.4)
p = .6 and z = z.025 = 1.96
 p = .7 and z = z.025 = 1.96
#
aL
 aU
 bL
 bU
aL
 aU
 bL
 bU
1 .3893
.7440
.3889
.7360
.5751
.8916 .5637
.8734
2 .4980
.8354
.4910
.8212
.4609
.8058 .4561
.7937
3
.3211
.6789
.3249
.6751
.4980
.8354 .4910
.8212
4 .4980
.8354
.4910
.8212
.6153
.9180 .6016
.8980
5 .3893
.7440
.3889
.7360
.6153
.9180 .6016
.8980
6
.3211
.6789
.3249
.6751
.5751
.8916 .5637
.8734
7 .3548
.7119
.3565
.7060
.6569
.9431 .6409
.9211
8 .4247
.7753
.4221
.7653
.6153
.9180 .6016
.8980
9 .4609
.8058
.4561
.7937
.6153
.9180 .6016
.8980
10 .4609
.8058
.4561
.7937
.4980
.8354 .4910
.8212
Based on this exercise, it appears that the intervals (bL, bU) obtained via
variance stabilizing transformation are on the whole a little “tighter” than the
intervals (aL, aU). The reader may perform large-scale simulations in order to
compare two types of confidence intervals for a wide range of values of p
and n.
12.4.2
The Poisson Mean
Suppose that X1, ..., Xn are iid Poisson(λ) random variables where 0 < λ <
∞ is the unknown parameter. The minimal sufficient estimator of λ is 
denoted by 
  One has 
 Even
though
 as n → ∞, it will be hard to use
 as a pivot o construct tests and confidence intervals for λ.
See the Exercise 12.3.14. Since the normalizing constant in the denominator
depends on the unknown parameter λ, the “power” calculations will be awk-
ward too.
We may invoke Mann-Wald Theorem from (12.4.1) and require a suitable
function g(.) such that the asymptotic variance of 
 becomes
free from λ. In other words, we want to have
that is

560
12. Large-Sample Inference
From (12.4.8), it is clear that we should look at the transformation 
carefully and consider the asymptotic distribution of 
  In
view of (12.4.1) with 
  we can claim that
 as → ∞.
That is, for large n, we consider the pivot
 which is approximately N(0, 1).           (12.2.9)
See Johnson and Kotz (1969, Chapter 4, Section 7) for a variety of other
related approximations.
For large n, one may use (12.4.9) to derive an approximate 100(1 − α)%
confidence interval for λ. Also, in order to test a null hypothesis H0 : λ = λ0,
for large n, one may use the test statistic
to come up with an approximate level α test against an appropriate alternative
hypothesis. The details are left out for brevity.
Example 12.4.5 A manufacturer of 3.5" diskettes measured the quality
of its product by counting the number of missing pulse (X) on each. We
are told that X follows a Poisson(λ) distribution where λ(> 0) is the un-
known average number of missing pulse per diskette and that in order to
stay competitive, the parameter λ should not exceed .009. A random sample
of 1000 diskettes were tested which gave rise to a sample mean . Is there
sufficient evidence to reject H0 : λ = .009 in favor of H1 : λ > .009 approxi-
mately at 5% level? In view of (12.4.10), we obtain
which exceeds z.05 = 1.645. Hence, we reject H0 approximately at 5%
level. Thus, at an approximate 5% level, there is sufficient evidence that
the defective rate of diskettes is not meeting the set standard.!
12.4.3 The Correlation Coefficient
Suppose that the pairs of random variables (X1i, X2i) are iid bivariate nor-
mal, N2(µ1, µ2, 
  ρ), i = 1, ..., n(≥ 4). It will be clear later from

12. Large-Sample Inference
561
(12.4.19) why we have assumed that the sample size is at least four. Let all
five parameters be unknown with (µl, σl) ∈ ℜ×ℜ+ ,l = 1,2 and −1 < ρ < 1. In
Section 11.4.2, α level a likelihood ratio test was given for H0 : ρ = 0 against
H1 : ρ ≠ 0. In Exercise 11.4.5, one verified that the MLE’s for µ1, µ2,
and ρ were respectively given by
These stand for the customary sample means, sample variances (not unbi-
ased), and the sample correlation coefficient.
The level α LR test for H0 : ρ = 0 against H1 : ρ ≠ 0 is this:
We recall that under H0, the pivot 
 has the Student’s t
distribution with n − 2 degrees of freedom.
Figure 12.4.1. Two-Sided Student’s tn-2 Rejection Region
But, now suppose that we wish to construct an approximate 100(1 − α)%
confidence interval for ρ. In this case, we need to work with the non-null
distribution of the sample correlation coefficient rn. The exact distribution of
rn, when ρ ≠ 0, was found with ingenious geometric techniques by Fisher
(1915). That exact distribution being very complicated, Fisher (1915) pro-
ceeded to derive the following asymptotic distribution when ρ ≠ 0:
For a proof of (12.4.12), one may look at Sen and Singer (1993, pp. 134-136)
among other sources.
Again, one should realize that a variance stabilizing transformation may be
useful here. We invoke Mann-Wald Theorem from (12.4.1) and require a

562
12. Large-Sample Inference
suitable function g(.) such that the asymptotic variance of 
becomes free from ρ. In other words, we want to have
that is g(ρ) = k ∫ 1/(1 − ρ2)dρ. Hence, we rewrite
From (12.4.14), it is clear that we should look at the transformations
and consider the asymptotic distribution of 
Now, in view of (12.4.1) we claim that
since with 
, one has g′(ρ) = 1/(1 − ρ2). That is, for large n,
we should consider the pivot
where Un and ξ were defined in (12.4.15).
One may verify that the transformations given in (12.4.15) can be equiva-
lently stated as
which are referred to as Fisher’s Z transformations introduced in 1925.
Fisher obtained the first four moments of tanh-1(rn) which were later up-
dated by Gayen (1951). It turns out that the variance of tanh-1(rn) is approxi-
mated better by 1/n - 3 rather than 1/n when n is moderately large. Hence, in
applications, we use the pivot
whatever be ρ, − 1 < ρ < 1. We took n at least four in order to make 
meaningful.
For large n, we use (12.4.19) to derive an approximate 100(1 − α)%
confidence interval for ρ. Also, to test a null hypothesis H0 : ρ = ρ0, for large
n, one uses the test statistic

12. Large-Sample Inference
563
and comes up with an approximate level a test against an appropriate alterna-
tive hypothesis. The details are left out for brevity.
Example 12.4.6 (Examples 11.4.1-11.4.2 Continued) Consider the vari-
ables (X1, X2), from Example 11.4.1 for employees on their job performance
scores before and after the training. Assuming a bivariate normal distribution
for (X1, X2), we want to test H0 : ρ = 0.7 versus H1 : ρ > 0.7 at an approximate
5% level. From a set of observed data for 85 employees, we found rn = .757.
Thus, we have tanh-1(r) = .98915, tanh-1(ρ0) = .8673, and hence from (12.4.20),
we obtain:
Now, with α = ..05, one has z.05 = 1.645. Since zcalc does not exceed z.05, we
do not reject the null hypothesis at an approximate 5% level. We conclude that
the correlation coefficient ρ between the job performance scores does not
appear to be significantly larger than 0.7.!
Example 12.4.7 A psychologist wanted to study the relationship between
the age (X1, in years) and the average television viewing time per day (X2, in
minutes) in a large group of healthy children between the ages 4 and 12 years.
From the study group, 42 children were randomly picked. For each child, the
television viewing time was recorded every day during the two preceding
weeks and X2 is the average of the 14 recorded numbers. From this set of 42
observations on the pair of random variables (X1, X2), we found rn = .338.
Assuming a bivariate normal distribution for (X1, X2), we wish to obtain an
approximate 95% confidence interval for ρ, the population correlation coeffi-
cient. From (12.4.19), we claim that tanh-1(rn) ± 
 is an approximate 95%
confidence interval for tanh-1(ρ). We have tanh-1(rn) = .35183 and so the inter-
val (0.03798, 0.66568) for tanh-1(ρ) has an approximate 95% confidence. In
other words, the interval (.03796,. 58213) for ρ has an approximate 95%
confidence. !
12.5  Exercises and Complements
12.2.1 (Example 12.2.1 Continued) Suppose that a random variable X has
the Cauchy pdf f(x; θ) = π-1{1 + (x − θ)2}-1 I(x ∈ ℜ) where θ(∈ ℜ) is the
unknown parameter. Show that the Fisher information, I (θ) = 2. {Hint: With
y = x−θ, first write I (θ) = π-1 
 Evaluate the integral by
substituting y = tan(z).}

564
12. Large-Sample Inference
12.2.2 Suppose that 
 is the MLE of θ(∈ ℜ − {0} which satis-
fies both (12.2.3)-(12.2.4). Then, find the asymptotic distribution of
12.2.3 Suppose that a random variable X has the Logistic distribution with
its pdf f(x; θ) = e−(x−y) {1 + e−(x−y)} −2 I (x ∈ ℜ) where θ(∈ ℜ) is the unknown
parameter. Show that the Fisher information, I (θ) = 1/3. {Hint: With y = x −
θ, express I (θ) as
Evaluate this integral by substituting u = {1 + e−y}−1.}
12.2.4 (Exercise 12.3.3 Continued) Suppose that X1, ..,Xn are iid random
variables having the common Logistic pdf f(x; θ) = e−(x−θ){1 + e−(x−θ)} −2 I(x ∈
ℜ) where θ(∈ ℜ) is the unknown parameter.
(i)
Write down the likelihood equation (12.2.2) in this situation. Is it
possible to obtain an analytical expression of the MLE, 
 ≡ 
(X) where X = (X1, ...,Xn)? If not, how could one find the esti
mate 
 (X) for θ having obtained the data X?
(ii)
Show that the MLE, 
 is consistent for θ;
(iii)
Show that asymptotically (as n → ∞), 
 is dis
tributed as N(0, 3). {Hint: Use Exercise 12.2.3.}
12.3.1 A coffee dispensing machine automatically fills the cups placed
underneath. The average (µ) amount of fill must not vary too much from the
target (4 fl. ounces) because the overfill will add extra cost to the manufac-
turer while the underfill will generate complaints from the customers. A ran-
dom sample of fills for 35 cups gave  = 3.98 ounces and S = .008 ounces.
Test a null hypothesis H0 : µ = 4 versus an alternative hypothesis H1 : µ ≠ 4 at
an approximate 1% level.
12.3.2 (Exercise 12.3.1 Continued) Obtain an approximate 95% confi-
dence interval for the average (µ) amount of fill per cup.
12.3.3 Sixty automobiles of the same make and model were tested by
drivers with similar road habits, and the gas mileage for each was recorded
over a week. The summary results were  = 19.28 miles per gallon and s =
2.53 miles per gallon. Construct an approximate 90% confidence interval for
the true average (µ) gas mileage per gallon.
12.3.4 A company has been experimenting with a new liquid diet. The
investigator wants to test whether the average (µ) weight loss for individu-
als on this diet is more that five pounds over the initial two-week period.
Fifty individuals with similar age, height, weight, and metabolic structure

12. Large-Sample Inference
565
were started on this diet. After two weeks, the weight reduction for each
individual was recorded which gave the sample average 5.78 pounds and
standard deviation 1.12 pounds. Test a null hypothesis H0 : µ = 5 versus an
alternative hypothesis H1 : µ > 5 at an approximate 5% level.
12.3.5 A receptionist in a medical clinic thought that a patient’s average
(µ) waiting time to see one of the doctors exceeded twenty minutes. A ran-
dom sample of sixty patients visiting the clinic during a week gave the average
waiting time  = 28.65 minutes and standard deviation s = 2.23 minutes. Can
the receptionist’s feeling regarding the average waiting time be validated at an
approximate 5% level?
12.3.6 (Exercise 12.3.5 Continued) Obtain an approximate 95% confi-
dence interval for the average (µ) waiting time for patients visiting the clinic.
12.3.7 A large tire manufacturing company has two factories A and B. It is
believed that the employees in Factory A are paid less monthly salaries on the
average than the employees in Factory B even though these employees had
nearly similar jobs and job-related performances. The local union of employ-
ees in Factory. A randomly sampled 35 employees from each factory and
recorded each individual’s monthly gross salary. The summary data follows:
Factory A
Factory B
n1 = 35
n2 = 35
1n1 = $2854.72
2n2 = $3168.27
s1n1 = $105.29
s2n2 = $53.55
At an approximate 5% level, test whether the data validates the belief that
employees in Factory A are paid less on the average than employees in Fac-
tory B.
12.3.8 Suppose that X1, ..,Xn are iid Bernoulli(p) random variables where 0
< p < 1 is the unknown parameter. The minimal sufficient estimator of p is the
sample proportion 
 of successes in n independent replications. One ob-
serves that 
 is asymptotically standard normal. Hence,
for large n, we have
Now, solve the quadratic equation in p to derive an alternative (to that given in
(12.3.18)) approximate 100(1 − α)% confidence interval for p.
12.3.9 A critic of the insurance industry claimed that less than 30% of
the women in the work-force in a city carried employer-provided health

566
12. Large-Sample Inference
benefits. In a random sample of 2000 working women in the city, we ob-
served 20% with employer-provided health benefits. At an approximate 5%
level, test the validity of the critic’s claim.
12.3.10 A die has been rolled 100 times independently and the face 6 came
up 25 times. Suppose that p stands for the probability of a six in a single trial.
At an approximate 5% level, test a null hypothesis H0 : p = 1/6 versus an
alternative hypothesis H1 : p ≠ 1/6.
12.3.11 (Exercise 12.3.10 Continued) A die has been rolled 1000 times
independently and the face 6 came up 250 times. Suppose that p stands for
the probability of a six in a single trial. At an approximate 5% level, test a null
hypothesis H0 : p = 1/6 versus an alternative hypothesis H1 : p ≠ 1/6. Also,
obtain an approximate 99% confidence interval for p.
12.3.12 We looked at two specific brands (A and B) of refrigerators in the
market and we were interested to compare the percentages (pA and pB) requir-
ing service calls during warranty. We found that out of 200 brand A refrigera-
tors, 15 required servicing, whereas out of 100 brand B refrigerators, 6 re-
quired servicing during the warranty. At an approximate 5% level, test a null
hypothesis H0 : pA = pB versus an alternative hypothesis H1 : pA ≠ pB.
12.3.13 (Exercise 12.3.12 Continued) Obtain an approximate 90% confi-
dence interval for pA − pB.
12.3.14 Suppose that X1, ..., Xn are iid Poisson(λ) random variables where
0 < λ < ∞ is the unknown parameter. The minimal sufficient estimator of λ is
the sample mean 
 which is denoted by 
 One observes immediately that
 is also asymptotically standard normal. Hence, for large n,
we 
have 
 
≈ 
1 
− 
α 
so 
that
 ≈ 1 − α. Now, solve the quadratic equation in
λ to derive an alternative (to that given in (12.3.33)) approximate 100(1 −
α)% confidence interval for λ.
12.3.15 Derive an approximate 100(1−α)% confidence interval for λ1−λ2
in the two-sample situation for independent Poisson distributions.
12.3.16 Derive tests for the null hypothesis H0 : λ1 = λ2 in the two-sample
situation for independent Poisson distributions with approximate level α, when
the alternative hypothesis is either upper-, lower-, or two-sided respectively.
12.3.17 (Sign Test) Suppose that X1, ..., Xn are iid continuous random
variables with a common pdf f(x; θ) which is symmetric around x = θ
where x ∈ ℜ, θ ∈ ℜ. That is, the parameter θ is the population median
assumed unknown, and f is assumed unknown too. The problem is to test

12. Large-Sample Inference
567
H0 : θ = 0 versus H1 : θ > 0 with given level α ∈ (0, 1). Define Yi = I(Xi > 0),
that is Yi = 1 or 0 according as Xi > 0 or Xi ≤ 0, i = 1, ..., n.
(i)
Show that Y1, ..., Yn are iid Bernoulli(p) where p = ½ or p > ½
according as θ = 0 or θ > 0 respectively whatever be f;
(ii)
Argue that one can equivalently test H0 : p = ½ versus H1 : p > ½
with the help of the observations Y1, ..., Yn;
(iii)
Propose an appropriate level a test from Section 12.3.1.
{Note: The associated Bernoulli parameter test is called a “Sign Test” be-
cause 
 counts the number of positive observations among the X’s. A
sign test is called nonparametric or distribution-free since the methodology
depending on Y’s does not use an explicit form of the function f.}
12.3.18 (Comparison of a Sign Test with a z Test in a Normal Distri-
bution) Suppose that X1, ..., Xn are iid N(µ, σ2) random variables where µ ∈
ℜ is assumed unknown, but σ ∈ ℜ+ is known. The problem is to test H0 : µ =
0 versus H1 : µ > 0 with given level α ∈ (0, 1). Define Yi = I(Xi > 0), that is Yi
= 1 or 0 according as Xi > 0 or Xi ≤ 0, i = 1, ..., n. The UMP level a test is
expressed as follows:
Whereas α level a sign test from Exercise 12.3.17 will look like this for large
n:
Now, address the following problems.
(i)
Compare the powers of Test #1 and Test #2 when n = 30, 40,
50, α = .05 and µ = .03, .02, .01 (which are close to µ = 0);
(ii)
Let us fix n = 50. Evaluate the sample sizes n1, n2, n3 such that
the Test #2 with sample size n = n1, n = n2, n = n3 has respec
tively the same power as that of Test #1, used with n = 50, at µ
= .03, .02, .01;
(iii)
How does n1, n2, n3 compare with n = 50? Any comments?
{Note: Such investigations eventually lead to a notion called the Bahadur
efficiency. Refer to Bahadur (1971).}
12.4.1 Suppose that X1, ..., Xn are iid with the common N(µ, σ2) distribu-
tion where the parameters µ ∈ ℜ, σ ∈ ℜ+. Thus, 
 is asymptoti-
cally (as n → ∞) distributed as N(0, 2σ4) where S2
n is the sample variance.
Find the variance stabilizing transformation of S2
n.
12.4.2 Suppose that X1, ..., Xn are iid with the common exponential distri-
bution having the mean β (> 0). Thus, 
 is asymptotically (as n →
∞) distributed as N(0, β2) where 
 is the sample mean. Find the variance
stabilizing transformation of 
.

568
12. Large-Sample Inference
12.4.3 Suppose that X1, ..., Xn are iid with the common N(θ, θ2) distribu-
tion where the parameter θ ∈ ℜ+. Let us denote 
  Derive the
asymptotic (as n → ∞) distribution 
  Find the variance stabiliz-
ing transformation of Tn.
12.4.4 Verify the result given by (12.4.18).
12.4.5 A researcher wanted to study the strength of the correlation coef-
ficient (ρ) between the proficiency in the two specific courses, namely first-
year physics (X1) and calculus (X2), in a college campus. From the large pool
of first-year students enrolled in these two courses, thirty eight students were
randomly picked and their midterm grades in the two courses were recorded.
From this data, we obtained the sample correlation coefficient r = .663. As-
sume that (X1, X2) has a bivariate normal distribution in the population. Test
whether α can be assumed to exceed 0.5 with an approximate level a = .10.
12.4.6 The data on systolic blood pressure ( X1) and age ( X2) for a sample
of 45 men of similar health conditions was collected. From this data, we
obtained the sample correlation coefficient r = .385. Obtain an approximate
95% confidence interval for the population correlation coefficient ρ. Assume
bivariate normality of (X1, X2) in the population.
12.4.7 The strength of the right and left grips, denoted by X1 and X2 re-
spectively, were tested for 120 auto accident victims during routine therapeu-
tic exams in a rehab center. The observed values of X1 and X2 were both
coded between zero and ten, a low (high) value indicating significant weak-
ness (strength) in the grip. Assume bivariate normality of (X1, X2) in the popu-
lation. The data gave rise to the sample correlation coefficient r = .605. Test
whether ρ can be assumed 0.5 with an approximate level α = .10.

13
Sample Size Determination:
Two-Stage Procedures
13.1 Introduction
Consider a sequence of iid observations X1, X2, ... from a N(µ, σ2) population
where both parameters µ ∈ ℜ, σ ∈ ℜ+ are unknown. Having recorded a fixed
number of observations X1, ..., Xn, n ≥ 2, and with preassigned σ ∈ (0, 1), the
customary 100(1 − α)% confidence interval for the population mean µ is
given by
Here, 
 and tn−1,α/2 stands for
the upper 100(α/2)% point of the Student’s t distribution with (n − 1) degrees
of freedom.
 Figure 13.1.1. The Area on the Right (or Left) of tn−1,α/2
(or −tn−1,α/2) Is α/2
The width of the confidence interval Jn is given by
Note that the width Wn is a random variable even though n is
held fixed. It is so because Sn is a random variable.
569

570
13. Sample Size Determination: Two-Stage Procedures
Now, recall that the support for the random variable Sn is the whole posi-
tive half of the real line, that is Sn has a positive density at the point s if and
only if s > 0.
How wide can the customary confidence interval Jn be?
First, one should realize that P{Wn > k} > 0 for any fixed k > 0, that is
there is no guarantee at all that the width Wn is necessarily going to be “small”.
In Table 13.1.1, we present a summary from a simple simulated exercise.
Using MINITAB Release 12.1, we generated n random samples from a N(0,
σ2) population where we let n = 5, 10, α = .05 and σ = 1, 2, 5. Since the
distribution of the sample standard deviation Sn, and hence that of Wn, is free
from the parameter µ, we fixed the value µ = 0 in this illustration. With a fixed
pair of values of n and σ, we gathered a random sample of size n, one hundred
times independently, thereby obtaining the confidence interval Jn for the pa-
rameter µ each time. Consequently, for a fixed pair of values of n and σ, we
came up with one hundred values of the random variable Wn, the width of the
actually constructed 95% confidence interval for µ. Table 13.1.1 lists some
descriptive statistics found from these one hundred observed values of Wn.
Table 13.1.1. Simulated Description of 95% Confidence Interval’s
Width Wn Given by (13.1.2): 100 Replications
n=5
minimum = 2.841, maximum = 24.721, mean = 11.696,
standard deviation = 4.091, median = 11.541
σ = 5
n = 10
minimum = 2.570, maximum = 10.606, mean = 6.661,
standard deviation = 1.669, median = 6.536
 n = 5
minimum = 1.932, maximum = 9.320, mean = 4.918,
standard deviation = 1.633, median = 4.889
σ = 2
n = 10
minimum = 1.486, maximum = 5.791, mean = 2.814,
standard deviation = 0.701, median = 2.803
 n = 5
minimum = 0.541, maximum = 5.117, mean = 2.245,
standard deviation = 0.863, median = 2.209
σ = 1
n = 10
minimum = 0.607, maximum = 2.478, mean = 1.394,
standard deviation = 0.344, median = 1.412

13. Sample Size Determination: Two-Stage Procedures
571
From Table 13.1.1, it is obvious that for a fixed value of σ, the random
width Wn goes down when n increases and for a fixed value of n, the same
goes down when σ decreases. But, it may feel awkward when we realize that
for any fixed pair of n and σ, an experimenter may get stuck with a confi-
dence interval Jn whose observed width Wn is relatively “large”.
Hence, we may like to construct a confidence interval J for µ such that
for all µ and σ2.
An interval estimator J satisfying both requirements in (13.1.3)
is called a fixed-width confidence interval for µ.
Dantzig (1940) had shown that no matter what fixed-sample-size proce-
dure one uses, that is regardless of what the fixed sample size n is, one can
not find a fixed-width confidence interval estimator for µ from the random
sample X1, ..., Xn. In other words, we can not simultaneously control the
width of the confidence interval for µ and its confidence coefficient if the
sample size n is held fixed in advance. A general non-existential result along
this line is found in Ghosh et al. (1997, Section 3.7).
We can, however, design an experiment by determining the appropriate
sample size as a positive integer valued random variable to solve this problem.
Stein (1945,1949) gave an ingenious method of determining the required
sample size by sampling in two steps. This method of data collection is called
two-stage sampling or double sampling.
This sampling technique falls in the area of Sequential Analysis. Abraham
Wald was the leading architect who laid the foundation of sequential sam-
pling, an approach that was essential to curtail quality control inspections of
shipments of weapons during the World War. In the early 1940’s, Wald and
his collaborators created the mathematical foundation in this area. Wald’s classic
book on this subject, Sequential Analysis, appeared in 1947. In its introduc-
tion, Wald credited P. C. Mahalanobis by noting that some of the key ideas
from Mahalanobis (1940) may be considered the “forerunner” of sequential
analysis. Mahalanobis (1940) introduced the fundamental idea of taking pilot
samples before conducting large-scale surveys. The method of two-stage sam-
pling or double sampling relies heavily on pilot samples.
The area of sequential analysis is both vast and rich. Sequential sampling
strategies are applied in areas including (i) clinical trials [Armitage (1973),
Whitehead (1986,1991)], (ii) reliability, life-tests and quality control [Basu

572
13. Sample Size Determination: Two-Stage Procedures
(1991)], and (iii) survival analysis [Gardiner and Susarla (1991), Gardiner et.
al. (1986)].
To achieve a balanced perspective, the reader may browse through the
books of Wald (1947), Bechhofer et al. (1968), Ghosh (1970), Armitage (1973),
Sen (1981), Woodroofe (1982), Siegmund (1985), Whitehead (1983),
Mukhopadhyay and Solanky (1994), Balakrishnan and Basu (1995), and Ghosh
et al. (1997) as well as the Handbook of Sequential Analysis edited by Ghosh
and Sen (1991). The Sequential Analysis journal provides the most up-to-date
account in this important area. We hope that the cited sources will guide
interested readers to consider many other important and relevant references.
There are important reasons why we include such topics to end this book.
One point to note is that many results derived earlier come together here as
important tools and machineries needed for technical analyses. But, there is
another point to note. From previous chapters, a reader may get an impres-
sion that all reasonable statistical problems can be handled by the classical
fixed-sample-size analyses and we show otherwise.
We recall Example 7.3.2 where we showed that it was impossible to find
any unbiased estimator of p-1 when the data consisted of iid Bernoulli(p) ob-
servations X1, ..., Xn with predetermined n. But, in Example 7.3.3 we showed
how inverse binomial sampling method could provide an unbiased estimator
of p-1, a parametric function which is very important in statistical applications
in ecology.
In the present chapter, we touch upon two statistical problems with very
realistic goals which can not be solved if the sample size is fixed in advance,
but these problems can be solved with the help of two-stage sampling strate-
gies. The emphasis is to show that there is a wide world out there beyond the
classical fixed-sample-size analysis.
Neither a fixed-width confidence interval satisfying both requirements
in (13.1.3) nor the bounded risk point estimation problem can be
solved by any fixed-sample-size method. A Stein type two-stage
methodology provides an exact solution for both problems.
In Section 13.2, we give the construction of a fixed-width confidence
interval for the unknown mean of a normal population whose variance is
completely unknown. This is accomplished (Theorem 13.2.2) by the Stein
two-stage methodology. In Section 13.3, we briefly address a point estima-
tion problem by requiring a preassigned upper bound on the mean square
error.

13. Sample Size Determination: Two-Stage Procedures
573
13.2 The Fixed-Width Confidence Interval
Let us begin with a N(µ, σ2) population where µ(∈ ℜ), σ(∈ ℜ+) are assumed
unknown. We wish to construct a fixed-width confidence interval J for µ
such that both requirements in (13.1.3) are met. That is, the length of J has to
be 2d and Pµ,σ
2{µ ∈ J} ≥ 1 − α for all µ, σ2 where d(> 0), 0 < α < 1 are
preassigned.
Having recorded the observations X1, ..., Xn, let us closely examine the
fixed-width confidence interval
One may recall that φ(.) and Φ(.) respectively stand for the pdf and df of a
standard normal random variable, that is 
 and
 z ∈ ℜ. Next, we have
if and only if
Had the population variance σ2 been known, we would have taken n = (C) +
1 and constructed the confidence interval Jn.
The notation (u) stands for the largest integer < u.
For example, if somehow we knew that C = 27.02 or 23 or 102.801, then n
will respectively be 28 or 23 or 103.
But, the magnitude of C is unknown because σ2 is unknown. We really do
not know how many observations to take so that the fixed-width confidence
interval Jn from (13.2.1) will work. In this literature, the sample size C, pre-
tending that it is an integer, is referred to as the optimal fixed sample size
required to solve the problem, had σ2 been known.
13.2.1
Stein’s Sampling Methodology
Since the optimal fixed sample size C is unknown, Stein (1945,1949) for-
mulated the following two-stage sampling strategy. One starts with a pre-
liminary set of observations X1, ..., Xm where the pilot sample size m(≥ 2)
is predetermined. This step is referred to as the first stage of sampling.

574
13. Sample Size Determination: Two-Stage Procedures
The pilot observations are utilized to estimate the population variance σ2 by
the corresponding sample variance 
 Next, let us define a stopping vari-
able,
which is an observable positive integer valued random variable. In the expres-
sion of C, we have replaced σ2 and z2
α/2 respectively by the sample variance
 and 
,α/2. Thus, the expression 
,α/2 
/d2, used in the definition of
N, is viewed as an estimator of C and hence we view N as an estimator of C.
Observe that the stopping variable N depends upon d, α, m and the first-stage
observations X1, ..., Xm through 
 .
If N = m, then we like to think that we have started with too many pilot
observations and hence we do not take any other observations in the second
stage. But, if N > m, then we sample the difference (N - m) in the second stage
to come up with (N - m) new observations Xm+1, ..., XN. After combining
observations from the two-stages, the data consists of X1, ..., XN whether any
observations are drawn in the second stage or not.
Next, on the basis of N and X1, ..., XN, one finds the sample mean
 and proposes the fixed-width confidence interval
This is what is referred to as the Stein two-stage sampling methodology.
13.2.2 Some Interesting Properties
We should check whether the confidence coefficient associated with the in-
terval estimator JN is at least 1 − α, the preassigned goal. Theorem 13.2.2
settles this query in the affirmative. We first summarize some preliminary but
interesting results.
Theorem 13.2.1 For the stopping variable N defined by (13.2.4), for all
fixed µ ∈ ℜ, σ ∈ ℜ+, d ∈ ℜ+ and α ∈ (0, 1), one has:
(i)
Pµ,σ
2{N < ∞} = 1;
(ii)
(iii)
  is distributed as N(0, 1). Also, the ran
dom variables Q and 
 are independent;
(iv)
  is distributed as the Student’s t random vari
able with m − 1 degrees of freedom;
(v)
 is an unbiased estimator of µ with its variance given by σ2
Eµ,σ
2 [1/N].

13. Sample Size Determination: Two-Stage Procedures
575
Proof From the definition of N in (13.2.4), one can verify that
which is referred to as the basic inequality. Now, let us proceed with the
proof.
(i) Observe that one has
But, with 
 for n ≥ m, we can write
Now, 
 and one can verify using L’Hôpital’s rule from (1.6.29)
that 
 Hence, from (13.2.8) we conclude that 
Pµ,σ
2{Y > an} = 0. Then, combining this with (13.2.7), we conclude that
Pµ,σ
2{N = ∞} = 0 so that one has Pµ,σ
2{N < ∞} = 1.
(ii) We take expectations throughout the basic inequality in (13.2.6) and
use the fact that  
  We leave the details out as Exercise 13.2.1.
(iii) For any fixed x ∈ ℜ, let us write

576
13. Sample Size Determination: Two-Stage Procedures
The last step in (13.2.9) is valid because (a) the random variable I(N = n) is
determined only by 
, (b) 
 and 
 are independent for all n = m, and
hence the two random variables 
 and I(N = n) are independent for all n ≥
m. Refer to Exercise 13.2.2. Now, we can rewrite (13.2.9) as
in view part (i). Hence, 
 is distributed as N(0, 1).
Observe that given 
, we have a fixed value of N. Hence, given 
, we
claim that the conditional distribution of 
 But,
this conditional distribution does not involve the given value of the condition-
ing variable 
. Thus, 
 and 
 are independently distributed.
(iv) We combine facts from part (iii) and Definition 4.5.1 to conclude that
 is distributed as Student’s t with m − 1 degrees of free-
dom.
(v) By the Theorem 3.3.1, part (i), we can express 
 as
which reduces to µ since Pµ,σ2 (N < ∞) = 1. In the same fashion, one can
verify that Eµ,σ2 
 = µ2+σ2 Eµ,σ
2[1/N] and the expression of 
 can
be found easily. The details are left out as Exercise 13.2.3. !
Now we are in a position to state a fundamental result which is due to
Stein (1945, 1949).
Theorem 13.2.2 For the stopping variable N defined by (13.2.4) and the
fixed-with confidence interval 
 for the population
mean µ, we have:
for all fixed µ ∈ ℜ, σ ∈ ℜ+, d ∈ ℜ+ and α ∈ (0, 1).

13. Sample Size Determination: Two-Stage Procedures
577
Proof Let us express the confidence coefficient associated with the fixed-
width confidence interval JN as
But, from the basic inequality (13.2.6) for N, we see that dN½/Sm ≥ tm−1,α/2
w.p.1, and hence the event 
 /Sm ≤ tm−,α/2 implies the event
 /Sm ≤ dN½/Sm. That is, the set A where 
 /Sm ≤ tm−
1,α/2 holds is a subset of the set B where 
 /Sm ≤ dN½/Sm holds.
Thus, we can claim that
Next, one combines (13.2.12)-(13.2.13) to write
since tm−1,α/2 is the upper 100(α/2)% point of the Student’s t distribution with
df m − 1. In (13.2.14), we have used the result from Theorem 13.2.1, part
(iv). !
Example 13.2.1 (Simulation) With the help of a computer, we generated
a normal population with µ = 2 and σ = 5. Let us pretend that we do not know
µ, σ. We consider α = .05, that is we require a 95% fixed-width confidence
interval for µ. We also fixed some d values (given in the accompanying Table
13.2.1) and took m = 5, 10. The two-stage estimation procedure (13.2.4)-
(13.2.5) is implemented as follows:
For the ith independent replication, we start with a new set of obser-
vations xi1, ..., xim from the computer-generated population and obtain
 which lead to an
observed value, N = ni. Then, the pilot data xi1, ..., xim is updated in order
to arrive at the final set of data xi1, ..., xini, i = 1, ..., 5000(= k, say).
Then, we compute 
which are respectively the unbiased estimates for E(N) and its variance.
During the ith independent replication, we also record pi = 1 (or 0) ac-
cording as µ = 2 ∈ (∉) to the observed fixed-width confidence interval
 where 
 Then, we obtain
 which are respectively the unbi-
ased estimates for the coverage probability and its variance. The Table 13.2.1
summarizes our findings.

578
13. Sample Size Determination: Two-Stage Procedures
From this table, some features are immediately noticed. First, the   val-
ues have always exceeded the target confidence coefficient 0.95 except in
one case. In the case m = 10, C = 50 we have  = .9458 with its estimated
standard error .0037. Since k is very large, an approximate 95% confidence
interval for the true coverage probability (p) in this case can be taken to be
 which amounts to (.9336, .9531). Note that the target coverage
probability, that is 0.95, lies between the two confidence bounds 0.9336 and
0.9531.
Table 13.2.1. Moderate Sample Performance of Stein’s
 Two-Stage Procedure (13.2.4)−(13.2.5) with α = .05:
5000 Replications
d
C
s( )
s( )
m = 5, z.025 = 1.96, t4,.025 = 2.776
1.789
30
62.32
0.825
0.9882
0.0041
1.386
50
103.03
1.023
0.9788
0.0043
0.980
100
199.20
1.765
0.9867
0.0041
m = 10, z.025 = 1.96, t9,.025 = 2.262
1.789
30
40.48
 0.264
0.9577
0.0029
1.386
50
67.22
0.416
0.9458
0.0037
0.980
100
135.16
 0.915
0.9524
0.0032
Next, we observe that the  values are sizably larger than the correspond-
ing C values, a feature which validates the fact that the Stein procedure over-
samples significantly. But, the extent of over-sampling falls substantially when
we choose m = 10 instead of m = 5. Compared with C, over-sampling amounts
to approximately 100% and 33% respectively when m = 5 and m = 10. We
may add that the ratio 
 approximately reduces to 2.006 and
1.332 respectively when m = 5 and m = 10. Since 
 and E(N)
≈ 
 the amount of over-sampling we have experienced
should be quite expected. More elaborate exercises with computer simula-
tions are left as class projects. !
The Theorem 13.2.2 shows that the two-stage procedure of Stein solved
a fundamental problem of statistical inference which could not be tackled by
any fixed-sample-size methodology.
On the other hand, there are several important issues one can raise in
this context. The choice of m, the pilot sample size, plays a crucial role in
the performance of the two-stage estimation procedure (13.2.4)-(13.2.5).

13. Sample Size Determination: Two-Stage Procedures
579
Moshman (1958) discussed a couple of approaches to determine a reasonable
choice of the pilot sample size m. But, whether one selects a small or large
value of m, it is known that the final sample size N tends to significantly
overestimate C on the average. We may add that even when C is very large,
the final sample size N tends to significantly overestimate C on the average.
Starr (1966) systematically investigated these other related issues.
But, there is no denying of the fact that the Stein procedure solved a fun-
damental problem of statistical inference which could not otherwise be solved
by any fixed-sample-size design. One may refer to Cox (1952), Chatterjee
(1991), Mukhopadhyay (1980, 1991), Ghosh and Mukhopadhyay (1976, 1981)
and Ghosh et al. (1997, Chapter 6), among other sources, to appreciate a
fuller picture of different kinds of developments in this vein.
If one can assume that the unknown parameter σ has a known lower
bound σL (> 0), then the choice of 
 becomes clear. In this
situation, Stein’s two-stage fixed-width confidence interval procedure, when
appropriately modified, becomes very “efficient” as shown recently by
Mukhopadhyay and Duggan (1997, 1999).
The exact confidence coefficient for Stein’s two-stage confidence
interval JN from (13.2.5) is given in the Exercise 13.2.4.
The two-sample fixed-width confidence problem for the difference
of means of independent normal populations with unknown and
unequal variances is left as Exercise 13.2.10. This is referred to
as the Behrens-Fisher problem. For ideas and references,
 look at both the Exercises 11.3.15 and 13.2.10.
13.3 The Bounded Risk Point Estimation
Again, we begin with a N(µ, σ2) population where µ ∈ ℜ, σ ∈ ℜ+ are assumed
unknown. Having recorded a fixed number of observations X1, ..., Xn, the
customary point estimator for µ is the sample mean 
 As
before, let us denote 
 and us sup-
pose that the loss incurred in estimating µ by 
 is measured by the squared
error loss function,
The associated risk is given by

580
13. Sample Size Determination: Two-Stage Procedures
which remains unknown since σ2 is assumed unknown. We recall that this is
the frequentist risk discussed earlier in Section 10.4.
It is clear that based on the data X1, ..., Xn we may estimate Rn (µ,
 ) by
 In Table 13.3.1, we present a summary from a simple simu-
lated exercise. Using MINITAB Release 12.1, we generated n random samples
from a N(0, σ2) population where we let a = 1, n = 5, 10 and σ = 5, 2, 1. Since
the distribution of the sample variance 
  and hence that of 
 is free from
the parameter µ, we fixed the value µ = 0 in this illustration. With a fixed pair
of values of n and σ, we gathered a random sample of size n, one hundred
times independently, thereby obtaining an observed value of 
 each time.
Consequently, for a fixed pair of values of n and s, we came up with one
hundred values of the random variable 
.  Table 13.3.1 lists some descrip-
tive statistics derived from these one hundred observed values of 
 .
Table 13.3.1. Simulated Description of the Estimated
Values 
 : 100 Replications
n = 5
minimum = .232, maximum = 16.324, mean = 4.534
standard deviation = 3.247, median = 3.701
σ = 5
 n = 10
minimum = .457, maximum = 9.217, mean = 2.597
standard deviation = 1.443, median = 2.579
n = 5
minimum = .0569, maximum = 3.3891, mean = .7772
standard deviation = .6145, median = .6578
σ = 2
 n = 10
minimum = .1179, maximum = 1.0275, mean = .3927
standard deviation = .1732, median = .3608
n = 5
minimum = .0135, maximum = .7787, mean = .1904
standard deviation = .1281, median = .1684
σ = 1
 n = 10
minimum = .02843, maximum = .24950, mean = .10113
standard deviation = .05247, median = .09293
From Table 13.3.1, it is obvious that for a fixed value of σ, the estimated
risk 
 goes down when n increases and for a fixed value of n, the same goes
down when σ decreases. But, it may feel awkward when we realize that for
any fixed pair n and σ, an experimenter may get stuck with an estimated risk
which is relatively “large”.

13. Sample Size Determination: Two-Stage Procedures
581
Hence, we may like to estimate µ by 
 in such a way that the risk,
This formulation is described as the bounded risk point estimation approach.
The requirement in (13.3.3) will guarantee that the risk does not exceed a
preassigned number ω. It is a very useful goal to achieve, particularly when ω
is chosen small.
But, there is a problem here. Even such a simplistic goal can not be at-
tained by any fixed-sample-size procedure. Refer to a general non-existential
result found in Ghosh et al. (1997, Section 3.7). In Section 13.3.1, we dis-
cuss a Stein type two-stage sampling technique for this problem. In Section
13.3.2, we show that the bounded risk point estimation problem can be solved
by adopting the proposed methodology.
13.3.1
The Sampling Methodology
The risk, Rn(µ, 
) is bounded from above by ω, a preassigned positive
number, if and only if the sample size n is the smallest integer = aσ2 /ω = n*,
say. Had ω been known, we would take a sample of size n = (n*) + 1 and
estimate µ by 
 obtained from the observations X1, ..., Xn. But, the fact is
that n* is unknown since σ remains unknown. Since no fixed-sample-size
solution exists, we propose an appropriate two-stage procedure for this prob-
lem. This methodology was discussed in Rao (1973, pp. 486-488). One may
refer to Ghosh and Mukhopadhyay (1976) for more details.
One starts with initial observations X1, ..., Xm where the pilot sample size
m(≥ 4) is predetermined. The basic idea is to begin the experiment with such
m which is hopefully “small” compared with n*. In real life though, one starts
with some reasonably chosen m, but it is impossible to check whether m is
small compared with n* because n* is unknown in the first place.
In any case, let the experiment begin with a pilot sample size m(≥ 4) and
the observations X1, ..., Xm. The data gathered from the first stage of sam-
pling are then utilized to estimate the population variance σ2 by the sample
variance 
 Let us denote
and define the stopping variable

582
13. Sample Size Determination: Two-Stage Procedures
which is a positive integer valued random variable. Observe that N depends on
a, ω, m and the first-stage observations X1, ..., Xm through 
 . The expres-
sion bm
 /ω used in the definition of N is viewed as an estimator of n*, the
optimal fixed-sample size had σ2 been known. Since σ2 is unknown, we have
replaced it by the sample variance 
 and a by bm in the expression of n*.
If N = m, then perhaps we have started with too many pilot observations to
begin with and hence we do not see a need to take any other observations in
the second stage. But, if N > m, then we sample the difference (N − m) in the
second stage which means that we take (N − m) new observations Xm+1, ...,
XN in the second stage. After combining the observations obtained from the
two-stages, the data consists of X1, ..., XN whether any observations are
drawn in the second stage or not, that is the stopping variable N is the final
sample size.
On the basis of N and X1, ..., XN, we obtain the sample mean, 
 = and propose to estimate the population mean µ by 
 .
13.3.2
Some Interesting Properties
We first summarize some preliminary but yet interesting results.
Theorem 13.3.1 For the stopping variable N defined by (13.3.5), for all
fixed µ ∈ ℜ, σ ∈ ℜ+;, a ∈ ℜ+ and ω ∈ ℜ+;, one has:
(i)
Pµ,σ2{N < ∞} = 1;
(ii)
bm σ2 ω−1 ≤ Eµ,σ
2 = m + bmσ2ω−1;
(iii)
Q ≡ 
 is distributed as N(0, 1). Also, the
random variables Q and 
 are independent;
(iv)
 is distributed as the Student’s t random vari
able with m − 1 degrees of freedom;
(v)
 is an unbiased estimator of µ.
Its proof is similar to that of Theorem 13.2.1 and so the details are left out
as Exercise 13.3.1.
Now, let us prove a fundamental result in this connection which shows
that the bounded risk point estimation problem for the mean of a normal
population, having an unknown variance, can be solved with the help of a
properly designed two-stage sampling strategy.
Theorem 13.3.2 For the stopping variable N defined by (13.3.5) and the
proposed estimator 
 for the population mean µ, we have the associated
risk:
for all fixed µ ∈ ℜ, σ ∈ ℜ+, a ∈ ℜ+ and ω ∈ ℜ+.

13. Sample Size Determination: Two-Stage Procedures
583
Proof Let us first express the risk function Eµ,s
2[L(µ, 
)] associated
with 
 as follows:
Now, since Eµ,σ
2 [(
 − µ)2] = σ2n−1 for every fixed n and Pµ,σ
2(N) < ∞) = 1,
from (13.3.6) we obtain
Next, from the definition of N in (13.3.5), we note that N ≥ bm
 /ω w.p.1.
Hence, we can write:
where Y = (m − 1) 
/σ2 which is distributed as 
  Now, we combine
the facts 
 and
Γ(½(m − 1)) = (½(m − 3))Γ(½(m − 3)) with (13.3.7)-(13.3.8) to obtain:
Thus, the proof is complete.
Again, the choice of m, the pilot sample size, plays a crucial role in the
performance of the two-stage estimation procedure (13.3.5). Whether one
selects a “small” or “large” value of m, the final sample size N will tend to
overestimate n* on the average. But, there is no denying of the fact that a Stein
type two-stage procedure has solved a fundamental problem of statistical
inference which could not otherwise be solved by any fixed-sample-size strat-
egy.
If one can assume that the unknown parameter σ has a known lower
bound σL(> 0), then the choice of m(≈ 
 /ω) becomes quite apparent. In
this situation, a Stein type two-stage bounded risk point estimation procedure,
when appropriately modified, becomes very “efficient” as shown recently by
Mukhopadhyay and Duggan (1999).

584
13. Sample Size Determination: Two-Stage Procedures
13.4 Exercises and Complements
13.2.1 Use the basic inequality from (13.2.6) to prove Theorem 13.2.1,
part (ii).
13.2.2 Let X1, ..., Xn be iid N(µ, σ2 random variables. Use Helmert’s trans-
formation from Chapter 4 (Example 4.4.9) to show that the sample mean 
and the sample variance 
 are distributed independently for all fixed n ≥ m(≥
2). This result has been utilized in the proof of Theorem 13.2.1, part (iii).
13.2.3 For the two-stage procedure (13.2.4), show that Vµ,σ2 
= σ2Eµ,σ2
[1/N]. Also evaluate Eµσ2 [(
 − µ)k] for k = 3, 4. {Hint: Use the fact that the
two random variables 
 and I(N = n) are independent for all n ≥ m.}
13.2.4 Consider the two-stage fixed-width confidence interval JN from
(13.2.5) for the unknown population mean µ. Show that
{Hint: Use the fact that the two random variables 
 and I(N = n) are inde-
pendent for all n ≥ m.}
13.2.5 Let X1, ..., Xn be iid random variables  with the common pdf f(x; µ,
σ) = σ−1 exp{−(x − µ)/σ}I(x > µ) where both the parameters µ(∈ ℜ) and σ(∈
ℜ+) are assumed unknown. Recall that the MLE for µ is Xn:1, the smallest
order statistic. Having two preassigned numbers d(> 0) and α ∈ (0, 1), sup-
pose that we consider a fixed-width confidence interval Jn = [Xn:1 − d, Xn:1] for
µ. Note that for the upper limit in the interval Jn, there may not be any point in
taking Xn:1 +d, because Xn:1 > µ w.p.1. In other words, Xn:1 is a “natural” upper
limit for µ. We also require that the confidence coefficient be at least 1 − α.
(i)
Show that Pµ,σ{µ ∈ Jn} = 1 − e−nd/σ;
(ii)
Hence, show that Pµ,σ{µ ∈ Jn} = 1 − α if and only if n is the
smallest integer ≥ aσ/d = C, say, where a = − log(α). The
optimal fixed sample size, had the scale parameter σ been known,
is then C, assuming that C is an integer.
{Hint: Refer to the Example 4.4.12.}
13.2.6 (Exercise 13.2.5 Continued) Whatever be the sample size n, a
fixed-width confidence interval Jn given in Exercise 13.2.5 can not be
constructed such that Pµ,σ{µ ∈ Jn} ≥ 1 − α for all fixed d, α, µ and σ.
Ghurye (1958) proposed the following Stein type two-stage sampling
strategy. Let the experiment begin with the sample size m(≥ 2) and initial
observations X1, ..., Xm. This is the first stage of sampling. These pi-

13. Sample Size Determination: Two-Stage Procedures
585
lot observations are then utilized to estimate the scale parameter σ by Tm = (m
− 1)−1 
 (Xi − Xm:1). Next, define the stopping variable,
which is a positive integer valued random variable. Here, N estimates C. Re-
call that F2,2m−2,α is the upper 100α% point of the F2,2m−2 distribution. This
procedure is implemented along the lines of Stein’s two-stage scheme. Fi-
nally, having obtained N and X1, ..., XN, we construct the fixed-width confi-
dence interval JN = [XN:1 − d, XN:1] for µ.
(i)
Write down the corresponding basic inequality along the lines
of (13.2.6). Hence, show that d−1 F2,2m−2,ασ = Eµ,σ[N] = m
+d-1 F2,2m−2,aσ;
(ii)
Show that Xn:1 and I(N = n) are independent for all n ≥ m;
(iii)
Show that Q ≡ N(XN:1 − µ)/σ is distributed as the standard ex
ponential random variable.
{Hint: From Chapter 4, recall that n(Xn:1 − µ)/σ and 2(m − 1)Tm/σ are
respectively distributed as the standard exponential and 
 random vari-
ables, and these are also independent.}
13.2.7 (Exercise 13.2.6 Continued) Show that Pµ,σ{µ ∈ JN} ≥ 1 − α for all
fixed d, α, µ and σ. This result was proved in Ghurye (1958). {Hint: First
show that N(XN:1 − µ)/Tm is distributed as F2,2m−2 and then proceed as in the
proof of Theorem 13.2.2.}
13.2.8 (Exercise 13.2.6 Continued) Verify the following expressions.
(i)
Eµ,σ[XN:1] = µ + σEµ,σ[1/N];
(ii)
Eµ,σ[(XN:1 − µ)2] = 2σ2Eµ,σ[1/N2];
(iii)
Pµ,σ{µ ∈ JN} = 1 − Eµ,σ[e−Nd/σ].
{Hint: Use the fact that the two random variables Xn:1 and I(N = n) are
independent for all n ≥ m.}
13.2.9 Let the random variables X11, X12, ... be iid N(µi,σ2), i = 1, 2, and that
the X1j’s be independent of the X2j’s. We assume that all three parameters are
unknown and (µ1, µ2, σ) ∈ ℜ × ℜ × ℜ+. With preassigned d(> 0) and α ∈ (0, 1),
we wish to construct a 100(1 − α)% fixed-width confidence interval for µ1 −
µ2(= µ, say). Having recorded Xi1, ..., Xin with n ≥ 2, let us denote
 for i = 1, 2 and the
pooled sample variance 
  Consider the confidence interval
 for µ1 − µ2. Let us also denote θ = (µ1,
µ2, σ).
(i)
Obtain the expression for Pθ{µ ∈ Jn} and evaluate the optimal fixed
sample size C, had σ been known, such that with n = (C) +1, one
has P?{µ ∈ Jn} ≥ 1 − α;

586
13. Sample Size Determination: Two-Stage Procedures
(ii)
Propose an appropriate Stein type two-stage stopping variable N for
which one will be able to conclude that Pθ{µ ∈ JN} ≥ 1 – α for all
fixed θ,d, α. Here, N should be defined using the pooled sample
variance S2
Pm obtained from pilot samples.
{Note: Analogous problems for the negative exponential populations were
discussed by Mauromaustakos (1984) and Mukhopadhyay and
Mauromoustakos (1987).}
13.2.10 (Behrens-Fisher problem) Let Xi1, ..., Xini be iid N(µi, 
) random
variables, ni ≥ 2, i = 1, 2, and that the X1j’s be independent of the X2j’s. Here
we assume that µ1, µ2, σ1, σ2 are all unknown and (µ1, µ2) ∈ ℜ2, (σ1, σ2) ∈
ℜ+2, σ1 ≠ σ2. Let 
 respectively be the sample mean and variance, i =
1, 2. With preassigned numbers α ∈ (0, 1) and d(> 0), we wish to construct
a confidence interval J for µ1 – µ2 such that (i) the length of J is 2d as well as
(ii) Pθ{µ1 - µ2 ∈ J} ≠ 1 – a for all fixed θ = (µ1, µ2, σ1, σ2) and d, α.
(i)
Let us denote a fixed-width confidence interval Jn = [{ 
 – 
}
± d] for µ1 - µ2 where n = (n1, n2). Find the expression for P?{µ1 -
µ2 ∈ Jn} with fixed n;
(ii)
Define h such that P{W1 – W2 ≤ h} = 1 – ½α, where W1, W2 are iid
Student’s tm–1 variables with some fixed m ≥ 2. Let m be the pilot
sample size from both populations. We define Ni = max {m, <h2 
/d2> + 1}, and sample the difference Ni – m in the second stage
from the ith population, i = 1, 2. Now, with N = (n1, n2), we propose
the fixed-width confidence interval 
  for
µ1 – µ2. Show that Pθ{µ1 – µ2 ∈ JN} ≥ 1 – α for all fixed θ = (µ1, µ2,
σ1, σ2) and d, α.
{Hint: It is a hard problem. This exact and elegant solution is due to
Chapman (1950). In the Exercise 11.3.15, we cited the original papers of
Behrens (1929), Fisher (1935,1939), Creasy (1954) and Fieller (1954). Ghosh
(1975) tabulated the h values for a range of values of m and a. The analogous
two-sample problem for the negative exponential distributions was developed
in Mukhopadhyay and Hamdy (1984). Section 6.7.1 in Ghosh et al. (1997)
gave a detailed account of the related sequential literature.}
13.3.1 Prove Theorem 13.3.1 along the lines of the proof of Theorem
13.2.1.
13.3.2 Let us consider a sequence of iid observations X1, X2, ... from
a N(µ, σ2) population where the parameters µ ∈ ℜ and σ ∈ ℜ+ are both
assumed unknown. Having recorded the observations X1, ..., Xn, the
customary point estimator for the population mean µ is the sample mean

13. Sample Size Determination: Two-Stage Procedures
587
13. Sample Size Determination: Two-Stage Procedures
587
. We denote 
 for n ≥ 2
and θ = (µ, σ2). Suppose that the loss incurred in estimating µ by 
 is given
by the function 
 where a and t are fixed and known
positive numbers.
(i)
Derive the associated fixed-sample-size risk Rn(µ, 
) which is
given by Eθ[L(µ, 
)];
(ii)
The experimenter wants to bound the risk Rn(µ, 
) from above
by ω, a preassigned positive number. Find an expression of n*, the
optimal fixed-sample-size, if σ2 were known.
13.3.3 (Exercise 13.3.2 Continued) For the bounded risk point estimation
problem formulated in Exercise 13.3.2, propose an appropriate two-stage stop-
ping variable N along the lines of (13.3.5) and estimate µ by the sample mean
. Find a proper choice of “bm” required in the definition of N by proceeding
along (13.3.9) in order to claim that Eθ[L(µ, 
)] ≤ ω for all θ, a and t. Is it
true that bm > 1 for all positive numbers t? Show that 
 is unbiased for µ.
13.3.4 Let the random variables Xi1, Xi2, ... be iid N(µi, σ2), i = 1, 2, and
that the X1j’s be independent of the X2j’s. We assume that all three parameters
are unknown and (µ1, µ2, σ2) ∈ ℜ × ℜ × ℜ+. Having recorded Xi1, ..., Xin with
n ≥ 2, let us denote 
 = (n – 
for i = 1, 2 and the pooled sample variance 
 = 
. The custom-
ary estimator of µ1 – µ2 is taken to be  
 – 
 Let us also denote θ = (µ1,
µ2, σ2). Suppose that the loss incurred in estimating µ1 – µ2 by 
 is
given by the function L(µ1 – µ2, 
) = a | { 
} – {µ1 – µ2}|t
where a and t are fixed and known positive numbers.
(i)
Derive the fixed-sample-size risk Rn(µ1 - µ2, 
) which is
given by E?[L(µ1 – µ2, 
)];
(ii)
The experimenter wants to bound Rn(µ1 – µ2, 
) from
above by ω, α preassigned positive number. Find the the expression
of n*, the optimal fixed-sample-size, had σ2 been known.
13.3.5 (Exercise 13.3.4 Continued) For the bounded risk point esti-
mation problem formulated in Exercise 13.3.4, propose an appropriate
two-stage stopping variable N along the lines of (13.3.5) depending on
 and estimate µ1 – µ2 by 
 Find a proper choice of “bm”
required in the definition of N by proceeding along (13.3.9) in order to
claim that Eθ[L(µ1 – µ2, 
)] = θ for all θ, α and t. Is it true that
bm > 1 for all positive numbers t? Show that 
 is unbiased for µ1 – µ2.

588
13. Sample Size Determination: Two-Stage Procedures
13.3.6 Let X1, X2, ... be iid random variables with the common pdf f(x; µ,
σ) = σ–1 exp{ – (x – µ)/σ}I(x > µ) where both the parameters µ(∈ ℜ) and σ(∈
ℜ+) are assumed unknown. Having recorded X1, ..., Xn for n ≥ 2, we estimate
µ and σ by Xn:1 and Tn = (n – 1)–1 
. Let us denote θ = (µ, σ).
Suppose that the loss incurred in estimating µ by Xn:1 is given by the function
L(µ, Xn:1) = a(Xn:1 - µ)t where a and t are fixed and known positive numbers.
(i)
Derive the associated fixed-sample-size risk Rn(µ, Xn:1) which is
given by Eθ[L(µ, Xn:1)];
(ii)
The experimenter wants to have the risk Rn(µ, Xn:1) bounded from
above by ω, a preassigned positive number. Find the expression of
n*, the optimal fixed-sample-size, had the scale parameter σ been
known.
13.3.7 (Exercise 13.3.6 Continued) For the bounded risk point estimation
problem formulated in Exercise 13.3.6, propose an appropriate two-stage stop-
ping variable N along the lines of (13.3.5) and estimate µ by the smallest order
statistic XN:1. Find the proper choice of “bm” required in the definition of N by
proceeding along (13.3.9) in order to claim that Eθ[L(µ, XN:1)] ≤ ω for all θ, a
and t. Is it true that bm > 1 for all positive numbers t? Obtain the expressions
for Eθ[XN:1] and V?[XN:1].
13.3.8 Let the random variables Xi1, Xi2, ... be iid having the common
pdf f(x; µi, σ), i = 1, 2, where f(x; µ, σ) = σ-1 exp{-(x - µ)/σ}I(x > µ).
Also let the X1j’s be independent of the X2j’s. We assume that all three
parameters are unknown and (µ1, µ2, σ) ∈ ℜ × ℜ × ℜ+. Having recorded
Xi1, ..., Xin with n ≥ 2, let us denote 
 =
 i = 1, 2, and the pooled estimator UPn = ½
{U1n + U2n} for the scale parameter σ. The customary estimator of µ1 – µ2
is 
 Let us denote θ = (µ1, µ2, σ). Suppose that the loss in-
curred in estimating µ1 – µ2 by 
is given by the function
 where a and t
are fixed and known positive numbers.
(i)
Derive the associated fixed-sample-size risk Rn (µ1 – µ2, 
 –
) which is given by Eθ [L (µ1 – µ2, 
)];
(ii)
The experimenter wants to have the risk Rn (µ1 – µ2, 
 
)
bounded from above by ω, α preassigned positive number. Find the
expression of n*, the optimal fixed-sample-size, had s been known.

13. Sample Size Determination: Two-Stage Procedures
589
13. Sample Size Determination: Two-Stage Procedures
589
13.3.9 (Exercise 13.3.8 Continued) For the bounded risk point estima-
tion problem formulated in Exercise 13.3.8, propose an appropriate two-
stage stopping variable N along the lines of (13.3.5) depending upon UPm
and estimate µ1 – µ2 by 
 Find the proper choice of “bm” re-
quired in the definition of N by proceeding along (13.3.9) in order to claim
that Eθ [L (µ1 – µ2, 
] ≤ ω for all θ, a and t. Is it true that bm >
1 for all positive numbers t? Obtain the expressions for Eθ [
]
and Vθ [
].

This page intentionally left blank

591
14
Appendix
The first section includes a summary of frequently used abbreviations and
some notation. The Section 14.2 provides brief biographical notes on some of
the luminaries mentioned in the book. In the end, a few standard statistical
tables are included. These tables were prepared with the help of MAPLE.
14.1 Abbreviations and Notation
Some of the abbreviations and notation are summarized here.
χ
space of the values x for the random variable X
ℜ
real line; (–8, 8)
ℜ+
positive half of ℜ; (0, ∞)
∈; x ∈ A
belongs to; x belongs to the set A
A(⊂) ⊆ B
A is a (proper) subset of B
!
denotes end of an example
!
denotes end of proof in theorems
¿
denotes end of proof of lengthy subcases
ϕ
empty set
lhs
left hand side
rhs
right hand side
log(a), a > 0
natural (base e) logarithm of a
pmf; PMF
probability mass function
pdf; PDF
probability density function
df; DF
distribution function
cdf; CDF
cumulative distribution function
mgf; MGF
moment generating function
iid
independent and identically distributed
product of a1, a2, ..., an
w.p. 1
with probability one
x, X, θ
vectors of dimension more than one
a ≡ b
a, b are equivalent
Γ(α), α > 0
gamma function; 

592
14. Appendix
beta(α, β), α > 0, β > 0
beta function; G(a)G(ß)/G(α + β)
stands for f(b) – f(a)
 or dg(t)/dt
first derivative of g(t) with respect to t
second derivative of g(t) with respect to t
first partial derivative of g(s,t) with respect
to t
second partial derivative of g(s,t) with
respect to t
I(A) or IA
indicator function of the set A
I(θ)
Fisher information about θ
Xn converges to a in probability as n → ∞
Xn converges to X in law (or distribution)
as n → ∞
a(x) ~ b(x)
a(x)/b(x) → 1 as x → ∞
Chi-square distribution with v degrees of
freedom
φ(.), Φ(.)
standard normal pdf and cdf respectively
sample mean from X1, ..., Xn
S2 or 
sample variance from X1,..., Xn; divisor n – 1
Xn:i
ith order statistic in Xn:1 ≤ Xn:2 ≤ ... ≤ Xn:n
det(A)
determinant of a square matrix A
A′
transposed matrix obtained from Am×n
Ik×k
identity matrix of order k × k
WLLN
weak law of large numbers
SLLN
strong law of large numbers
CLT
central limit theorem
<u>
largest integer < u
a ≈ b
a and b are approximately same
p.d.
positive definite
n.d.
negative definite
p.s.d.
positive semi definite
B(θ)
bias of an estimator
MSE
mean squared error
MLE
maximum likelihood estimator
CRLB
Cramér-Rao lower bound
UMVUE
uniformly minimum variance unbiased esti-
mator
(U)MP test
(uniformly) most powerful test

14. Appendix
593
(U)MPU test
(uniformly) most powerful unbiased test
LR
test likelihood ratio test
a ∝ b
a and b are proportional
za
upper 100a% point of N(0, 1)
upper 100a% point of 
tv,α
upper 100a% point of tv
Fv1,v2,α
upper 100a% point of Fv1,v2
14.2 A Celebration of Statistics: Selected
Biographical Notes
In this section, we add brief biographical notes for selected luminaries in
statistics mentioned in this textbook. Each biography is prepared with the help
of available published materials. In each case, we have cited the original
sources.
The aim is to let the readers appreciate that the field of statistics has devel-
oped, and it continues to flourish, because it attracted some of the best minds
of the twentieth century. What we present here is sketchy. The space is lim-
ited. The list is certainly not exhaustive.
The journal Statistical Science, published by the Institute of Mathemati-
cal Statistics, routinely prints conversation articles of eminent statisticians.
These articles are filled with historical materials. A broader sense of his-
tory will emerge from the additional conversation articles in Statistical
Science: H. Akaike [Findley and Parzen (1995)], T. W. Anderson [De-
Groot (1986b)], G. A. Barnard [DeGroot (1988)], M. Bartlett [Olkin
(1989)], H. Bergström [Råde (1997)], H. Chernoff [Bather(1996)], D.
R. Cox [Reid (1994)], H. Daniels [Whittle (1993)], F. N. David [Laird
(1989)], J. Doob [Snell (1997)], D. J. Finney [MacNeill (1993)], J. Gani
[Heyde(1995)], B. V. Gnedenko [Singpurwalla and Smith (1992)], I. J.
Good [Banks (1996)], S. S. Gupta [McDonald (1998)], M. Hansen [Olkin
(1987)], D. Kendall [Bingham (1996)], L. Kish [Frankel and King (1996)],
C. C. Li [Chen and Tai (1998)], D. V. Lindley [Smith (1995)], S. K.
Mitra [Mukhopadhyay (1997)], Y. V. Prokhorov [Shepp (1992)], F.
Proschan [Hollander and Marshall (1995)], I. R. Savage [Sampson and
Spencer (1999)], E. Seiden [Samuel-Cahn (1992)], H. Solomon [Switzer
(1992)], G. Watson [Beran and Fisher (1998)]. Bellhouse and Genest

594
14. Appendix
(1999) as well as the discussants included an account of the rich history of
statistics in Canada.
The interview articles of C. W. Dunnett [Liaison (1993)] and I. Olkin
[Press (1989)] are filled with historical remarks about colleagues, lives and
careers. These articles were included in the edited Festschrift Volumes re-
spectively An interview of H. Robbins [Page (1989)] appeared in “Herbert
Robbins: Selected Papers” (T. L. Lai and D. Siegmund eds.) which is a de-
light to read.
Morris H. DeGroot was born on June 8, 1931 in Scranton, Pennsylvania.
The dedication and memorial article in Statist. Sci. (1991, 6, 4-14), Biography
of Morris H. DeGroot, portrays the life and work of DeGroot, and it is rich in
history. DeGroot had a key role in creating the new journal, Statistical Sci-
ence, and he served as its Executive Editor during its formative years. DeGroot
was also responsible for several early interview articles. DeGroot died on
November 2, 1989.
The Archives of Films of Distinguished Statisticians in the American Sta-
tistical Association has taped lectures and interviews of some of the eminent
statisticians, including: T. W. Anderson, D. Blackwell, R. C. Bose, G. Box, H.
Chernoff, W. G. Cochran, D. R. Cox, H. Cramér, E. W. Deming, F. Graybill,
I. J. Good, M. Hansen, R. V. Hogg, S. Hunter, O. Kempthorne, E. L. Lehmann,
J. Neyman, F. Mosteller, G. Noether, I. Olkin, E. J. G. Pitman, C. R. Rao, H.
Robbins, E. L. Scott, J. W. Tukey, and M. Zelen. These are wonderful films,
filled with fascinating stories. A local Chapter of the American Statistical As-
sociation (ASA) or other scholarly societies may consider renting some of the
films from the ASA’s Archives for their meetings.
The edited volume of Pearson and Kendall (1970) includes very valuable
selected articles and hence it will undoubtedly serve as an excellent resource.
One may also consult the monographs and articles of Aldrich (1997), Ander-
son (1996), Craig (1986), David (1998), Edwards (1997a,b), Fienberg
(1992,1997), Ghosh et al. (1999), Halmos (1985), Hogg (1986), LeCam
(1986b), Shafer (1986), Stigler (1980,1986,1989,1991,1996) and Zabell (1989)
for a better appreciation of the international statistical heritage.
With due respect to all contributors, both past and present, in our young
field of statistics, we present the selected biographies. We earnestly hope that
the lives and careers of these fascinating individuals, and many others who
unfortunately could not be mentioned here in detail, will continue to inspire,
nurture and touch the souls of the future generations of statisticians.
LET THE CELEBRATION BEGIN
R. R. Bahadur: Raghu Raj Bahadur was born in Delhi, India on April

14. Appendix
595
30, 1924. He completed his school and college education in Delhi. He received
Ph.D. in statistics from the University of North Carolina at Chapel Hill. He
was in the first batch of Ph.D. advisees of Herbert Robbins.
During 1956-1961, Bahadur was a professor in the Research and Training
School of the Indian Statistical Institute, Calcutta. In 1961, he left the Insti-
tute to join University of Chicago, a position he held ever since.
Bahadur’s contributions on large deviation theory, sufficiency, MLE, com-
parisons of tests, sample quantiles, sequential decisions, transitive sufficiency,
are truly noteworthy. He was a master in his unique approach to unveil the
inner beauty in some of the hardest problems. Anything he published is con-
sidered a jewel by many statisticians. The phrases Bahadur Efficiency, Bahadur
Slope and Bahadur Representation of Quantiles have become household words
in statistics. At the 1974 inauguration ceremony of the Delhi campus of the
Indian Statistical Institute, Jerzy Neyman referred to Bahadur as the brightest
of the two hundred and fifty stars of Indian origin shining in U.S.A. in the
field of statistics.
In September, 1969, Bahadur gave the NSF-CBMS lectures in the Depart-
ment of Statistics of Florida State University, which led to his monograph,
Some Limit Theorems in Statistics (1971, SIAM). This monograph is a classic
in large sample theory. An international symposium was arranged in Delhi in
December 1988 to honor the memory of R. C. Bose. Bahadur edited the
symposium volume, Probability, Statistics and Design of Experiments (1990,
Wiley Eastern).
Bahadur received many honors in U.S.A. and India. He received
Guggenheim Fellowship in 1968. He was an elected Fellow of the Indian
National Science Academy and the Institute of Mathematical Statistics. Bahadur
became President (1974) of the Institute of Mathematical Statistics and gave
the Wald Lecture. Bahadur was very friendly, unassuming and always ap-
proachable. He cared about the people around him, students and colleagues
alike.
Bahadur died on June 7, 1997 after long illness. In the obituary of Bahadur,
Stigler (1997) wrote: “He was extremely modest in demeanor and uncomfort-
able when being honored, ... .” Stigler’s article and Bahadur’s own little write-
up, Remarks on Transitive Sufficiency, which was included in Ghosh et al.
(1992) provide more details on this one of a kind statistician’s life and work.
D. Basu: Debabrata Basu was born on July 5, 1924 in Dhaka, India
(now Bangladesh). His father was the Head of the Department of Math-
ematics at Dhaka University. He went through the undergraduate and Mas-
ters’ degree programs at this university. During this period, he was charmed
by the lectures of the famous number theorist, T. Vijayraghavan. Basu

596
14. Appendix
moved to Calcutta in 1948.
In 1950, Basu enrolled as the first research scholar under the guidance of
C. R. Rao in Indian Statistical Institute (ISI) and completed his Ph.D. thesis
within two years. In 1953, he went to University of California, Berkeley, on a
Fulbright fellowship where he came in close contact with Jerzy Neyman. In
1955, when R. A. Fisher visited ISI, Basu got the first-hand taste of Fisherian
inference. With his uncanny ability to produce counter-examples, quite often
spontaneously, Basu started raising fundamental philosophical issues. He felt
uncomfortable with Fisherian doctrine as well as the Neyman-Pearsonian ap-
proach. Eventually, he became an ardent Bayesian. He vigorously gave voice
to the staunchest criticisms of many frequentist ideas.
Basu always gave inspiring lectures. He travelled extensively all over the
world. During 1976-1986, he remained as a professor in the department of
statistics in Florida State University, Tallahassee. Currently, he is a Professor
Emeritus in ISI and Florida State University.
Basu is an elected Fellow of both the Institute of Mathematical Statistics
and the American Statistical Association. He is highly regarded for his pen-
etrating essays on the foundations of statistical inference. Ghosh (1988) ed-
ited a special volume containing most of Basu’s critical essays on Statistical
Information and Likelihood. He also made fundamental contributions on in-
ferences for finite population sampling. A Festschrift volume, Current Issues
in Statistical Inference: Essays in Honor of D. Basu, was presented to him on
his 65th birthday. This special volume was edited by Ghosh and Pathak (1992).
Basu’s article about himself, Learning from Counterexamples: At the In-
dian Statistical Institute in the Early Fifties, was included in Ghosh et al.
(1992). It mentions many interesting stories and encounters with Fisher,
Neyman, Wald and Rao. Basu has been a dedicated bridge player and he loves
gardening. Unfortunately, his health has been failing for sometime.
D. Blackwell: David Blackwell was born on April 24, 1919, in Centralia,
Illinois. He got into the University of Illinois and received the A.B. (1938),
A.M. (1939), and Ph.D. (1941) degrees, all in mathematics.
Blackwell entered college to become an elementary school teacher. He
came in contact with J. L. Doob at the University of Illinois. Blackwell wrote
his Ph.D. thesis under the guidance of Doob, but the thesis topic was neither
in the area of statistics nor probability.
After receiving the Ph.D. degree in mathematics, Blackwell was not sure
whether he would be interested in statistics. According to his own admis-
sion [DeGroot (1986a)], during the period 1941-1944, it was very hard for
him to land a suitable job. He accepted a faculty position in mathematics at

14. Appendix
597
Howard University, Washington D.C., and in 1944 he met A. Girshick when
Blackwell went to attend a Washington chapter meeting of the American Sta-
tistical Association where Girshick gave a lecture on sequential analysis.
Blackwell was impressed.
In the meantime, Blackwell came across the works of A. Wald and his
collaborators at Columbia University. In the early 1950’s, he started thinking
about a Bayesian formulation of sequential experiments. Lengthy collaborations
between Blackwell and Girshick followed which led to the Blackwell-Girshick
classic, Theory of Games and Statistical Decisions (1954, Wiley). Blackwell
also wrote an elementary textbook, Basic Statistics (1969, McGraw-Hill).
Blackwell stayed with the faculty in Howard University for ten years and
in 1954, he joined the department of statistics at the University of California,
Berkeley, after some persuasion from J. Neyman. He has been in Berkeley
ever since.
Blackwell has made fundamental contributions in several areas including
sufficiency and information, game theory and optimal statistical decisions. He
became President (1955) of the Institute of Mathematical Statistics, President
of the Bernoulli Society, and Vice President of the American Statistical Asso-
ciation, the American Mathematical Society, and the International Statistical
Institute. He has delivered the Fisher Lecture and has received prestigious
awards and honorary degrees. The interview article [DeGroot (1986a)] with
Blackwell gives a much broader perspective of his life and career. Blackwell is
a Professor Emeritus at Berkeley.
H. Cramér: Harald Cramér was born on September 25, 1893, in
Stockholm, Sweden. He studied mathematics and chemistry in Stockholm
University where he received the Ph.D. (1917) degree in pure mathematics.
His research career spanned over seven decades while his first publication
appeared in 1913 which dealt with a problem on biochemistry of yeast. In
1920, he published on the theory of prime numbers.
During 1917-1929, Cramér was in the faculty of mathematics in Stockholm
University, where he became a professor of mathematical statistics and actu-
arial science in 1929 and continued in that position until 1958. He became the
President of Stockholm University in 1950 and the Chancellor of the entire
Swedish University System in 1958. He retired from the Swedish University
System in 1961, but his professional career marched on for the next two
decades. He travelled extensively, including multiple visits to the University of
North Carolina at Chapel Hill, Princeton, Copenhagen, Paris, Helsinki, and the
Indian Statistical Institute in Calcutta.
Among Cramér’s many path-breaking contributions, lies his seminal book,
Mathematical Methods of Statistics (1945 from Stockholm; 1946 from Prince-

598
14. Appendix
ton University Press). During the next several decades, this book stood as the
gold standard of what mathematical statistics ought to be. The Mathematical
Methods of Statistics has been translated in other languages including Rus-
sian, Polish, Spanish, and Japanese. Cramér also wrote other important books
and monographs including the one on stochastic processes, co-authored with
R. Leadbetter (1967, Wiley).
Cramér’s contributions in Markov and other stochastic processes, prob-
ability theory, and large sample theory, in particular, have been legendary in
how they influenced the research methods for a very long time. Cramér’s
(1942) paper on harmonic analysis has been included in the Breakthroughs in
Statistics, Volume I [Johnson and Kotz (1992)]. The Cramér-Rao inequality
and the Cramér-Rao Lower Bound are two household phrases.
Cramér had close professional ties with H. Hotelling, R. A. Fisher, A. N.
Kolmogorov, P. Lévy, W. Feller, J. Neyman, P. C. Mahalanobis, D. G. Kendall,
J. L. Doob, and C. R. Rao, among others. He was a superb lecturer and a
great story-teller. He could grab any audience’s attention as soon as he walked
into a lecture hall. He was always charming, friendly and unassuming.
Cramér received many honors including the Guy Medal in Gold from the
Royal Statistical Society in 1972. He received several honorary degrees in-
cluding a D.Sc. degree from the Indian Statistical Institute.
Cramér’s (1976) lengthy article beautifully portrayed his recollections on
the development of probability theory. (D. G.) Kendall (1983) wrote a charm-
ing tribute to Cramér on the celebration of his 90th birthday. Wegman (1986)
detailed Cramér’s many personal recollections. In the interview article of H.
Bergström [Råde (1997)], some of Cramér’s major contributions are men-
tioned.
Cramér died on October 5, 1985. The legacy of his long, vigorous and
distinguished career continues to nurture the growth of mathematical statis-
tics.
B. de Finetti: Bruno de Finetti was born on June 13, 1906 in Innsbruck,
Austria. He first joined the faculty of mathematics at the University of Milan,
but eventually settled down in Rome. He was one of the strongest proponents
of the subjectivistic interpretation of probability which is the life-line of the
Bayesian doctrine. His books, de Finetti (1972,1974), on subjective probabil-
ity are master pieces. de Finetti introduced the concept of exchangeability
which profoundly influenced the modern Bayesian school. de Finetti’s (1937)
article has been included in the Breakthroughs in Statistics Volume I [Johnson
and Kotz (1992)]. His autobiographical piece, Probability and My Life, in-
cluded in Gani (1982), is filled with fascinating stories. de Finetti passed away
in Rome on July 20, 1985.

14. Appendix
599
W. Feller: William Feller was born on July 7, 1906, in Zagreb, Yugoslavia
(now Croatia). After finishing Masters degree there in 1925, he worked in the
University of Göttingen during 1925-1928 where he received Ph.D. degree in
1926 at a tender age of twenty. Feller knew David Hilbert and Richard Cou-
rant from Göttingen. He spent 1934-1939 at the University of Stockholm
where he came to know Harald Cramér.
In 1939, Feller joined Brown University at Providence as an associate pro-
fessor. He became the first Executive Editor of Mathematical Reviews and
continuously gave extraordinary service to the profession for six long years in
that position. In 1944, Feller became a U.S. citizen. Then, he moved to Cornell
University and finally in 1950, he joined Princeton University as the presti-
gious Eugene Huggins Professor.
Feller’s research covered many areas including calculus, geometry, func-
tional analysis, and probability theory. He was keenly interested in mathemati-
cal genetics. Some of his best known papers in probability theory made fun-
damental contributions in and around the classical limit theorems. These in-
cluded the Central Limit Theorem, the Law of the Iterated Logarithm, and the
Renewal Theory. Feller’s versatility as a first-rate mathematician was recog-
nized and highly respected by eminent mathematicians including F. Hausdorff,
A. N. Kolmogorov, G. H. Hardy, J. E. Littlewood, A. Y. Khinchine, P. Lévy
and P. Erdös. During 1950-1962, Feller went on to break new grounds on the
theory of diffusion which was earlier developed by Kolmogorov.
Feller’s seminal works, An Introduction to Probability Theory and Its Ap-
plications, Volume 1 (1950; third edition 1968) and Volume 2 (1956; second
edition in 1971), both published by Wiley, are still regarded as classics in
probability theory.
Feller received many honors. He was a member of the U.S. National Acad-
emy of Sciences, a past Governor of Mathematical Association of America,
and President (1946) of the Institute of Mathematical Statistics.
Feller was to receive the U.S. National Medal of Science, the highest honor
an American scientist can receive, in a White House ceremony on February
16, 1970. But the world of mathematics was stunned in grief to learn that
William Feller passed away on January 14, 1970, in New York.
By action of the Council of the Institute of Mathematical Statistics, the
1970 volume of the Annals of Mathematical Statistics was dedicated to the
memory of William Feller. Its opening article “William Feller 1906-1970” pre-
pared by the Editors of the Annals of Mathematical Statistics, described the
life and career of this one of a kind giant among mathematicians.
R. A. Fisher: Ronald Aylmer Fisher was born on February 17, 1890
in North London. When he was six years old, Fisher studied astronomy

600
14. Appendix
and later he took to mathematics. During early upbringing in a private school,
he developed extraordinary skills in geometrical ideas. His uncanny depth in
geometry first became vivid in his derivation [Fisher (1915)] of the distribu-
tion of the sample correlation coefficient.
Fisher went to Cambridge in 1909 to study mathematics and physics where
he came across K. Pearson’s (1903) paper on the theory of evolution. He was
very influenced by this paper and throughout his life, Fisher remained deeply
committed to both genetics and evolution. Fisher was regarded as a first-rate
geneticist of his time. He received B.A. from Cambridge in 1912, having passed
the Mathematical Tripos Part II as a Wrangler. On the outbreak of war, Fisher
volunteered for the military services in August, 1914, but he was disappointed
by his rejection on account of poor eye sight.
Fisher received both M.A. (1920) and Sc.D. (1926) degrees from Cam-
bridge. During 1919-1933, he was a statistician in the Rothamsted Experi-
mental Station, Harpenden. During 1933-1943, he was the Galton Professor
of Eugenics in the University College, London. In the University College, he
became the Editor of Annals of Eugenics and during 1943-1957, he was the
Arthur Balfour Professor of Genetics in Cambridge. During 1959-1962, Fisher
lived in South Australia as a Research Fellow at the CSIRO Division of Math-
ematical Statistics, University of Adelaide.
The fundamental concepts of likelihood, sufficiency, ancillarity, condition-
ality, maximum likelihood estimation, consistency, and efficiency were fully
developed by Fisher from ground zero. He had built a logical theory of scien-
tific inference based on the information gathered from data. In the areas of
correlations, partial correlations, directional data, multivariate analysis, dis-
criminant analysis, factor analysis, principal components, design of experi-
ments, modeling, anthropology, genetics, for example, Fisher gave the foun-
dations. Anderson (1996) reviewed Fisher’s fundamental contributions in the
area of multivariate analysis. Efron (1998) explained Fisher’s influence and
legacy in the twenty first century.
Fisher (1930) developed the notion of fiducial probability distributions for
unknown parameters which were supposedly generated by inverting the dis-
tribution of appropriate pivotal random variables. He was very fond of fiducial
probability and applied this concept vigorously whenever he had an opportu-
nity to do so, for example in Fisher (1935,1939). The articles of Buehler
(1980), Lane (1980) and Wallace (1980) provide important perspectives of
fiducial inference.
Fisher was an ardent frequentist. On many occasions, he fought tooth
and nail to defend against any hint or allegation from the Bayesians that
somewhere he tacitly used the Bayes’s Theorem without mentioning it.

14. Appendix
601
The Bayesians argued that Fisher’s theory of fiducial probability was not valid
without fully subscribing to the Bayesian principles. But, Fisher vehemently
defended his position that he never relied upon Bayesian arguments to develop
the concept of fiducial probability. He never caved in.
Fisher had close ties with W. S. Gosset who is perhaps better known
under the pseudonym “Student” than under his own name. Gosset graduated
(1899) with a first class degree in chemistry from New College in Oxford,
then joined the famous Guinness Brewery in Dublin as a brewer, and stayed
with this brewing firm for all his life, ultimately becoming the Head Brewer in
a new installation operated by the Guinness family at Park Royal, London in
1935. Gosset needed and developed statistical methods for small sample sizes
which he would then apply immediately to ascertain relationships between the
key ingredients in beer. Gosset’s path-breaking 1908 paper gave the founda-
tion of the t-distribution where he derived the probable error of a correlation
coefficient in 1908 and made several forceful conjectures, most of these be-
ing proven true later by Fisher (1915). For both Gosset and Fisher, making
scientific inferences and using small-sample statistics for experimental data
went hand-in-hand on a daily basis. The theory of statistics was essential for
applying statistics. There was no line drawn between the two. Fisher created
the foundation of this discipline. Rao (1992a) called Fisher “The founder of
modern statistics.” Fisher regarded Gosset very highly and he once described
Gosset as “Faraday of statistics.” Box (1987) pointed out various connec-
tions and collaborations among Guinness, Gosset and Fisher.
Fisher’s first book, Statistical Methods for Research Workers, was out in
1925. The Genetic Theory of Natural Selection, first appeared in 1930. The
Design of Experiments, appeared in 1935. The Statistical Tables for Biologi-
cal, Agricultural and Medical Research of R. A. Fisher and F. Yates was out
in 1938. Fisher’s three other books, The Theory of Inbreeding, Contributions
to Mathematical Statistics, followed by Statistical Methods and Scientific
Inference, respectively appeared in 1949, 1950 and 1956. These books have
served as landmarks in the history of the statistical science.
Fisher received many honors. The list includes, Fellow of the Royal Soci-
ety (1929), Honorary Fellow of the Indian Statistical Institute (1937), Royal
Medal (1938), Darwin Medal (1948), and Copley Medal (1955) of the Royal
Society, Guy Medal in Gold of the Royal Statistical Society (1946). He be-
came a Foreign Associate of the U.S. National Academy of Sciences (1948).
He was Knighted in 1952.
An extra-ordinary body of now well-known developments in modern
statistical theory and methods originated in the work of Fisher. One encoun-

602
14. Appendix
ters Fisherian thoughts and approaches practically in all aspects of statistics.
Fisher (1922) has been included in the Breakthroughs in Statistics, Volume I
[Johnson and Kotz (1992)]. Also, Fisher (1925b, 1926) have been included in
the Breakthroughs in Statistics, Volume II [Johnson and Kotz (1993)].
Early in life, Fisher got into some conflicts with Karl Pearson on statistical
issues. For example, Fisher criticized (K.) Pearson’s method of moments as
he pushed forward the MLE’s. (K.) Pearson did not take these criticisms
lightly. One will notice that in the team formed for the cooperative project
[Soper et al. (1917)] studying the distribution of the sample correlation coef-
ficient, under the leadership of (K.) Pearson, the young Fisher was not in-
cluded. This happened in spite of the fact that Fisher was right there and he
earned quite some fame for his brilliant paper of 1915 on the distribution of
the sample correlation coefficient. Fisher felt hurt as he was left out of this
project. He never published again in Biometrika with (K.) Pearson as its Edi-
tor. Refer to DasGupta (1980) for some historical details.
After Karl Pearson resigned from Galton Chair in 1933, his department
was split into two separate departments. R. A. Fisher was appointed as new
Galton Professor, and (E. S.) Pearson became the Reader and Head of a
separate statistics department. Fisher did not like this arrangement very much.
Neyman and (E. S.) Pearson (1928a,b) approached inference problems to
build statistical tools for experimenters to choose between two classes of
models. These culminated later into path-breaking contributions, Neyman and
(E. S.) Pearson (1333a,b). Fisher criticized the Neyman-Pearson approach
claiming that his estimation theory, along with the likelihood and sufficiency,
was quite adequate, and that the work of Neyman and Pearson on testing of
hypotheses was misguided. This controversy between Fisher and Neyman-
Pearson was never settled. Some of the letters exchanged between |Neyman
and Fisher have been included in Bennett (1990). In the 1961 paper, Silver
jubilee of my dispute with Fisher, Neyman sounded more conciliatory.
Fisher’s work was compiled as, Collected Papers of R. A. Fisher, Vol-
umes 1-5, edited by J. H. Bennett (in 1971-1974) at the University of Adelaide.
An interesting feature of this compilation is that many articles were reprinted
with updated commentaries from Fisher himself.
Fisher’s daughter, Joan Fisher Box, wrote (1978) a fascinating biography,
R. A. Fisher: The Life of a Scientist. In Bennett’s (1990) edited volume
containing many of Fisher’s correspondences, one will discover, for ex-
ample, interesting exchanges between him and G. A. Barnard, W. G. Cochran,
H. Hotelling, H. Jeffreys, P. C. Mahalanobis, J. Neyman, C. R. Rao, L. J.

14. Appendix
603
Savage, S. S. Wilks and J. Tukey. The article of Savage (1976), the volume
edited by Fienberg and Hinkley (1980), and the articles of Barnard (1992),
Karlin (1992), Rao (1992a) honoring Fisher’s centenary, deserve attention.
D. Basu’s article, Learning from Counterexamples: At the Indian Statisti-
cal Institute in the Early Fifties, about himself [Ghosh et al. (1992)] and the
interview article of Robbins [Page (1989)] mention interesting stories about
Fisher. Both (M. G.) Kendall (1963) and Neyman (1967) also gave much
historical perspectives.
Fisher travelled almost everywhere. At the request of P. C. Mahalanobis,
for example, he frequently visited the Indian Statistical Institute. Sometimes,
Fisher went to plead with the President as well as the Prime Minister of India
when Mahalanobis faced serious threat of cuts in Federal funding. Fisher
visited University of North Carolina at Chapel Hill and Iowa State University at
Ames a couple of times each. Many of these visits are colorfully portrayed in
his biography [Box (1978)].
Hinkley (1980b) opened with the sentence, “R. A. Fisher was without
question a genius.” In the published interview article [Folks (1995)], O.
Kempthorne mentioned giving a seminar at Ames with the title, “R. A. Fisher
= 1.4 of Gauss.” Fisher remained vigorously productive throughout his life.
He was always invigorating, totally engulfed in the creation of new ideas, and
forcefully challenging the status-quo in science and society.
He died peacefully on July 29, 1962 in Adelaide, South Australia. E. A.
Cornish, in his address [reprinted in Bennett (1990), pp. xvi-xviii] at Fisher’s
funeral on August 2, 1962 in Adelaide, had said, “He must go down in history
as one of the great men of this our twentieth century.” Nobody should ever
argue with this assertion.
A. N. Kolmogorov: Andrei Nikolaevitch Kolmogorov was born on April
25, 1903 during a journey to his mother’s home. His mother died during
childbirth. His father perished during the offensive by Dynikin in 1919. An
aunt brought up Kolmogorov in a village near Volga.
Kolmogorov went to Moscow University in 1920 to study mathematics.
In 1922, he constructed the first example of an integrable function whose
Fourier series diverged almost everywhere, and then gave the example of an
integrable function whose Fourier series diverged everywhere. He became an
instant international celebrity. Kolmogorov wrote his first paper, jointly with
Khinchine, which included the famous Three Series Theorem and Kolmogorov
Inequality.
Kolmogorov found a job as a secondary school teacher. He then became
a doctoral student under the supervision of N. N. Luzin. During this appren-
ticeship, one was normally required to complete fourteen different courses
in mathematics but one could substitute any final examination by submit-

604
14. Appendix
ting some independent project in the assigned field. Before the final examina-
tions, Kolmogorov submitted fourteen original research papers written in the
required fields. Buried in these papers, there were new discoveries which
included some of his fundamental contributions on the Strong Law of Large
Numbers, the Central Limit Theorem, and the Law of the Iterated Logarithm.
His 1931 paper, Markov Processes with Continuous States in Continuous Time,
laid the foundation of the stochastic diffusion theory which continues to stand
as a landmark, just like many of Kolmogorov’s other discoveries. We recall
that during 1950-1962, W. Feller broke new grounds on the theory of diffu-
sion and Kolmogorov was delighted.
Kolmogorov’s monograph, Foundations of the Theory of Probability, first
appeared in German in 1933 whose English translation became available in 1950.
This immediately became the manifesto in the axiomatic development in prob-
ability. The world recognizes Kolmogorov as the guiding light and a true pioneer
in probability theory. His 1933 paper on the empirical determination of a distri-
bution has been included in the Breakthroughs in Statistics, Volume II [Johnson
and Kotz (1993)]. Kolmogorov also wrote other important books and mono-
graphs including, Introductory Real Analysis, coauthored with S. V. Fomin (1968
Russian edition, 1970 English edition by Prentice-Hall).
Kolmogorov travelled extensively and enjoyed exchanging ideas with col-
leagues. He energetically listened to anyone around him, young and experi-
enced alike. He had ties with H. Cramér, D. G. Kendall, W. Feller, M. Fréchet,
J. Hadamard, P. Lévy, E. B. Dynkin, J. L. Doob, B. V. Gnedenko, A. Rényi, J.
Neyman, P. C. Mahalanobis, J. B. S. Haldane, Yu. V. Linnik and C. R. Rao,
among others.
Kolmogorov received many honors, including honorary D.Sc. degrees from
the University of Paris, University of Stockholm, Indian Statistical Institute,
and Hungary. He became a foreign member of the Polish Academy of Sci-
ences and the GDR Academy of Sciences, as well as a honorary member of
the London Mathematical Society, the Royal Statistical Society, the Romanian
Academy, and the Hungarian Academy. In 1967, he became a member of the
U. S. National Academy of Sciences.
The two articles, (D. G.) Kendall (1991) and Shiryaev (1991), provide
fascinating commentaries on the life and accomplishments of Kolmogorov.
The article of Shiryaev, the first Ph.D. student and later a close associate
of Kolmogorov, is particularly rich in its portrayal of the human elements.
Kolmogorov enjoyed telling stories and jokes. He also loved mountaineer-
ing, skiing, hiking and swimming. The July 1989 issue of the Annals of
Probability and the September 1990 issue of the Annals of Statistics, both
published by the Institute of Mathematical Statistics, devoted a total of

14. Appendix
605
195 pages to Kolmogorov’s life and contributions, plus a list of his 518 pub-
lications.
He constantly strived toward the development of a rigorous curriculum in
mathematics in schools. He loved teaching, playing, and listening to the young
children in the secondary school where he taught mathematics for many de-
cades. Subsequently, this school has been named after Kolmogorov. He passed
away on October 20, 1987.
E. L. Lehmann: Erich L. Lehmann was born on November 20, 1917, in
Strasbourg, France. He was raised in Germany and he is of Jewish ancestry.
When the Nazis came to power in 1933, his family decided to settle in Switzer-
land where he attended high school. Originally he had his mind set on studying
the German literature. But, his father suggested the route of mathematics in-
stead. In his interview article [DeGroot (1986c)], Lehmann said that he did not
really know what mathematics might entail, but he agreed with his father any-
way. In the article, he mentioned that he did not argue with his parents on the
choice of a career path. He went to study mathematics in Zurich.
The war broke out in Europe. Lehmann arrived in New York in 1940 and
went on to receive the M.A. (1942) and Ph.D. (1946) degrees in mathematics
from the University of California, Berkeley. Since then, he has been in the
faculty at Berkeley. For his Ph.D. thesis, related to the UMPU tests, Lehmann
got advice from J. Neyman, G. Pólya and P. L. Hsu. The interview [DeGroot
(1986c)] and his two articles, Lehmann (1993,1997), tell many stories about
his life and career.
Lehmann nurtured and developed ideas within the context of the theory of
tests of hypothesis in ways which at times had been different from Neyman’s
original approach. Neyman did not approve such departure from his theory.
Lehmann started preparing the manuscript for the book, Testing Statistical
Hypotheses. Neyman demanded to see the manuscript before it was
published&exl; Since then, Lehmann was not asked to teach a course on
testing of statistical hypotheses at Berkeley as long as Neyman was around.
These episodes caused unhappy strains in the relationship between Lehmann
and Neyman. One may refer to DeGroot (1986c, pp. 246-247).
The 1959 (Wiley) classic, Testing Statistical Hypotheses, followed by its
second edition (1986, Wiley), has been on the “must reading list” practically
in all statistics Ph.D. programs. Lehmann’s famous Notes on the Theory of
Estimation has been available in Berkeley since 1951. The Theory of Point
Estimation first appeared in 1983 (Wiley), a second edition (coauthored
with G. Casella) of which came out in 1998 (Springer-Verlag). Lehmann
also wrote few other books including the Nonparametrics: Statistical Meth-
ods Based on Ranks (1975, Holden-Day).

606
14. Appendix
Lehmann’s contributions, particularly on sufficiency, minimal sufficiency,
completeness, unbiased tests, nonparametric inference, have been far reach-
ing. The papers of Lehmann and Scheffé (1950, 1955, 1956, Sankhya-;) on
completeness, similar regions and unbiased estimation have profoundly influ-
enced the research and the understanding of statistical inference.
He was Editor of the Annals of Mathematical Statistics during 1953-1955.
He received many honors, including President (1961) of the Institute of Math-
ematical Statistics, the Guggenheim Fellowship awarded three times, and
elected (1978) member of the U. S. National Academy of Sciences. In Febru-
ary 1985, Lehmann received a honorary doctorate degree from the University
of Leiden. Lehmann stays active in research and he frequently travels to con-
ferences.
P. C. Mahalanobis: Prasanta Chandra Mahalanobis, without whom the
Indian statistical movement would probably never have been initiated, was
born on June 29, 1893 in Calcutta. His parents’ family-friends and relatives
were in the forefront of the nineteenth century awakening in India. After
receiving B.Sc. (Honors, 1912) degree in Physics from the University of
Calcutta, Mahalanobis went for a casual visit to London in the summer of
1913. He was awarded senior scholarships from King’s College where he
studied physics. Upon his return to India, he joined as a lecturer of physics in
Presidency College, Calcutta in 1915. Mahalanobis was an eminent physicist
of his time.
Because of his analytical mind, he was often called upon by influential
family-friends to help analyze data for important government as well as uni-
versity related projects and reports. Mahalanobis started reading some of the
old issues of Biometrika, the copies of which he happened to bring along
from his trip to Cambridge.
Within the Physics Department in Presidency College, Mahalanobis initiated
a Statistical Laboratory in the late 1920’s. This Statistical Laboratory eventually
grew up to be the famed Indian Statistical Institute (ISI) on April 28, 1932.
Mahalanobis’s energy and vision led to a phenomenal growth of a newly created
discipline in India. Other brilliant individuals joined hands with him including,
not in any particular order, S. N. Roy, R. C. Bose, S. S. Bose, C. R. Rao, K.
Kishen, K. R. Nair, D. B. Lahiri, G. Kallianpur, D. Basu, R. R. Bahadur, S. R. S.
Varadhan, S. K. Mitra, G. P. Patil, J. Sethuraman and R. G. Laha.
In 1933, Sankhya-;: The Indian Journal of Statistics was started with
Mahalanobis as its Editor. He continued to edit the journal until his death.
Due to his initiative and involvement, ISI played a key role in formulating
India’s second five-year economic plan after independence. The large-scale
surveys were initiated and the National Sample Survey Organization was

14. Appendix
607
created. National Statistical Service was established and Mahalanobis suc-
ceeded in obtaining recognition of Statistics as a discipline, separate from
mathematics, by the Indian Science Congress. Apart from these and host of
other accomplishments of national importance, Mahalanobis also wrote five
books and 210 papers.
Mahalanobis (1932) first constructed statistical tables for the F distribu-
tion since these were directly needed for the analyses of variance in field
experiments. Fisher, on the other hand, popularized tables for ½ log(F), namely
Fisher’s Z. Snedecor’s (1934) book apparently overlooked Mahalanobis’s earlier
contribution and independently provided the F tables saying simply that they
were computed from Fisher’s table.
Mahalanobis’s early papers on meteorology, anthropology, economic plan-
ning, flood control, psychology, multivariate analysis, design of experiments
and sample surveys are particularly noteworthy. He was interested in all as-
pects of science, including biology, geology, genetics, botany, and speech
recognition. He was also a literary figure and critic of his time.
In large-scale surveys, Mahalanobis (1940) pioneered the idea of gathering
samples in successive stages to increase efficiency. The method of using pilot
samples before running the main survey was developed in the same paper.
Wald (1947, p. 2) commented that this fundamental contribution may be re-
garded as a forerunner of Sequential Analysis. Mahalanobis’s D2, a measure
of distance between two populations, is a fundamental concept in multivariate
analysis.
Figure 14.2.1. P. C. Mahalanobis Centennial Postage Stamp
Issued by the Government of India, 1993
Mahalanobis received the highest honors awarded by the Government
of India including the title Padmabibhusan, which means that the awardee

608
14. Appendix
adds glory even to the flower Lotus. During the centennial year 1993, the
Government of India issued a postage stamp with Mahalanobis’s picture on
it. This is how the masses of India celebrated statistics and quantitative
literacy!
Mahalanobis became President (1950) of the Indian Science Congress.
Many other academies from India and all over the world bestowed honors
upon him, including the Fellowship of the Royal Society, the Foreign Mem-
bership of the U.S.S.R. Academy of Sciences, and the Statistical Advisor to
the Cabinet of Indian Government since 1949, Gold Medal from the Czecho-
slovak Academy of Sciences. He received a number of honorary degrees
including a Honorary Ph.D. (1956) from the University of Calcutta and the
Deshikottama (Honorary D.Litt., 1961) from Visva Bharati, the University
founded by Tagore, the Indian poet and Nobel Laureate.
Mahalanobis was instrumental in opening up the continuous flow of scien-
tific exchanges and visits, particularly with U.S.A. and many East European
Countries. He was able to create a vigorous statistical infrastructure in India
at all imaginable levels. He was a true ambassador of statistical science.
Additional details can be found in a series of commemorative articles which
appeared in Sankhya-;, Series A and B, 1973. The published interviews of Rao
[DeGroot (1987)] and Mitra [Mukhopadhyay (1997)] as well as the essays
written by Rao (1992b) and Hansen (1987) would be very informative. One
may also look at the entry [Rao (1968)] on P. C. Mahalanobis in the Interna-
tional Encyclopedia of Statistics. The recent article of Ghosh et al. (1999)
also gives many historical details on Mahalanobis’s contributions.
In June, 1972, Mahalanobis was hospitalized for some surgery. He was
recovering well. On June 27, like every other day, from his hospital bed he
cleared some files and wrote letters. He gave dictations on some policy mat-
ters regarding the Institute’s affairs and its future. Perhaps he felt that the end
was near&exl; This superbly powerful, dynamic and productive visionary’s
life came to a halt in Calcutta on June 28, 1972, exactly one day shy of his
79th birthday.
J. Neyman: Jerzy Neyman was born on April 16, 1894 in Bendery, Rus-
sia. His parents were Polish. He received early education from governesses,
alternately French and German, and this helped in his proficiency in several
languages. When he was twelve, his father passed away and his family moved,
ultimately settling in Kharkov.
Neyman entered University of Kharkov in 1912 to study physics and
mathematics. S. N. Bernstein, a Russian probabilist, was among his teach-
ers. Bernstein mentioned Karl Pearson’s (1892) monograph, Grammar of

14. Appendix
609
Science, to Neyman. This monograph had a life-long influence on Neyman.
Very early in college, Neyman was keenly interested in the theory of Lebesgue
measure. But, life-style was uncertain and difficult during those days of wars.
Poland and Russia started fighting about territories. Because of his Polish
background, Neyman was arrested as an enemy alien and he spent a few
weeks in jail. But, he was needed in the University to teach, and hence he was
ultimately let go.
At the age of 27, Neyman first had the opportunity to visit Poland in an
exchange of prisoners of war and in Warsaw, he met the Polish mathemati-
cian W. Sierpi ski who encouraged him and thought highly of his research.
Ultimately Sierpi ski helped Neyman to get a job as a statistician in the Na-
tional Institute of Agriculture. Here, he was to take meteorological observa-
tions and help with agricultural experiments.
Neyman’s doctoral thesis (1923) in the University of Warsaw had dealt
with probabilistic considerations in agricultural trials. In 1924, he went to the
University College London for a year to study under Karl Pearson where he
met three other statisticians: R. A. Fisher, E. S. Pearson, and W. S. Gosset
(“Student”). Some of Neyman’s papers were known to the people at the
University College because their English translations were already available.
Neyman spent the following year in Paris with a Rockefeller Fellowship, and
became acquainted with H. Lebesgue, J. Hadamard, and E. Borel.
In late 1920’s, Neyman focused more upon mathematical statistics as well
as applications in economics, insurance, biology and industry. His collabora-
tion with E. S. Pearson started around 1925. Since early 1920’s, (E. S.) Pearson
began developing his own philosophy of statistical methods and inference. He
also started to appreciate and build practical statistical models, particularly
useful for industrial applications. During 1928-1938, extensive collaborations
took place between (E. S.) Pearson and Neyman.
Neyman and (E. S.) Pearson (1928a,b) approached inference problems to
build statistical tools for experimenters to choose between two classes of
models. These culminated later into path-breaking contributions, Neyman and
(E. S.) Pearson (1333a,b). In the latter papers, likelihood ratio tests in the
multi-parameter cases were fully developed. Neyman and Pearson (1933a)
has been included in the Breakthroughs in Statistics, Volume I [Johnson and
Kotz (1992)]. Neyman-Pearson’s formulation of optimal tests ultimately evolved
into optimal decision functions for more general statistical problems in the
hands of Wald (1949b,1950).
Fisher criticized the Neyman-Pearson approach claiming that his estima-
tion theory, along with the likelihood and sufficiency, was quite adequate, and
that the work of Neyman and Pearson on testing of hypotheses was mis-

610
14. Appendix
guided. This controversy between Fisher and Neyman-Pearson was never
settled. Some of the letters exchanged between Neyman and Fisher have been
included in Bennett (1990). In his 1961 paper, Silver jubilee of my dispute
with Fisher, Jerzy Neyman sounded more conciliatory.
Neyman made fundamental contributions in survey sampling in 1934 and
later developed the notion of estimation via confidence sets in 1937. Neyman
(1934) has been included in the Breakthroughs in Statistics, Volume II [Johnson
and Kotz (1993)]. In all three fundamental areas of contributions, namely
tests of hypotheses-survey sampling-confidence sets, Neyman characteristi-
cally started by formulating the problem from what used to be very vague
concepts and then slowly building a logical structure within which one could
carry out scientific inferences.
His research arena was too vast for us to discuss fully. His contributions
have been far reaching. Many of his discoveries have become a part of the
statistical folklore. The phrases like Neyman-Pearson Lemma, Neyman Struc-
ture, Optimal C(α) Tests, Neyman Factorization, Neyman Accuracy, are parts
of statistical vocabulary.
Neyman came to visit U.S.A. for six weeks in 1937, lecturing in universities
and the Department of Agriculture, Washington D.C., and at that time he was
invited to join the University of California at Berkeley as a professor of statistics
to build a separate statistical unit. The offer came at a time when he was men-
tally ready to move away from the uncertainty in Europe and the oppression of
Hitler. Neyman arrived in Berkeley in August, 1938, and the rest is history.
In Berkeley, Neyman built the prestigious Statistical Laboratory and De-
partment of Statistics and he was the key attraction. He initiated and contin-
ued to organize the Berkeley Symposia where the brightest researchers as-
sembled to exchange ideas. Many path-breaking discoveries during 1950’s-
1970’s appeared in the highly acclaimed Proceedings of the Berkeley Sympo-
sium in Statistics and Probability. Neyman actively edited the Berkeley Sym-
posia volumes, sometimes with the help of his younger colleagues.
Neyman travelled extensively. When he did not travel, he hosted distin-
guished visitors at Berkeley. He had close ties with the leaders of centers of
learning in other countries. For example, Neyman had close ties with E. S.
Pearson, H. Cramér, A. N. Kolmogorov and P. C. Mahalanobis, and they were
very friendly. On a number of occasions, he visited (E. S.) Pearson in the
University College, Cramér in Stockholm, and Mahalanobis in the Indian Sta-
tistical Institute. Interesting details regarding Neyman’s visits to India can be
found in a series of Mahalanobis commemorative articles which had appeared
in Sankhya-, Series A and B, 1973.
Neyman’s smiling face was unmistakable whenever he walked into a room

14. Appendix
611
full of people. He encouraged bright and upcoming researchers. He was very
approachable and he often said, “I like young people.” Neyman had a very
strong personality, but he also had a big heart.
Neyman received many honors and awards for his monumental contribu-
tions to statistical science. He was elected to the U.S. National Academy of
Sciences. He became a foreign member of both the Polish and the Swedish
Academies of Science and a Fellow of the Royal Society. He received the Guy
Medal in Gold from the Royal Statistical Society. He was awarded several
honorary doctorate degrees, for example, from the University of Chicago, the
University of California at Berkeley, the University of Stockholm, the Univer-
sity of Warsaw, and the Indian Statistical Institute.
In 1968, Neyman was awarded the highest honor an American scientist
could receive. He was awarded the U.S. National Medal of Science from the
White House.
The article of LeCam and Lehmann (1974), prepared in celebration of
Neyman’s 80th birthday, described and appraised the impact of his contribu-
tions. The biographical book of Reid (1982), Neyman from Life, portrays a
masterful account of his life and work. One may also look at the entry [Scott
(1985)] on Jerzy Neyman, included in the Encyclopedia of Statistical Sci-
ences. Neyman died on August 5, 1981 in Berkeley, California.
E. S. Pearson: Egon Sharpe Pearson was born on August 11, 1895 in
Hampstead, England. His father was Karl Pearson. Egon Pearson finished his
school education in Oxford. He moved to Trinity College, Cambridge, and
obtained First Class in Part I of Mathematical Tripos. Due to ill-health and
time-out needed for war service, his B.A. degree from Cambridge was de-
layed and it was completed in 1920.
The Biometrika’s first issue came out in October 1901 when it was founded
by W. F. R. Weldon, F. Galton, and K. Pearson. From a very young age, Egon
Pearson saw how this journal was nurtured, and he grew emotionally at-
tached to it. In 1921, he joined his father’s internationally famous department
at the University College London as a lecturer of statistics.
Egon Pearson became the Assistant Editor of Biometrika in 1924. After
Karl Pearson resigned from the Galton Chair in 1933, his department was split
into two separate departments and R. A. Fisher was appointed as the new
Galton Professor whereas Egon Pearson became the Reader and Head of a
separate statistics department. Fisher did not like this arrangement very much.
Egon Pearson became a professor in 1935 and the Managing Editor of
Biometrika in 1936. He remained associated with the University College Lon-
don until his retirement in 1960.
Since early 1920’s, Egon Pearson started developing his own philoso-

612
14. Appendix
phy of statistical methods and inference. He began to appreciate and build
practical statistical models, particularly useful for industrial applications. On
either count, philosophically he started drifting away from his “father’s foot-
steps.” It was quite some challenge for the younger Pearson to form his own
strong ideological base and research interests under the “shadow” of Karl
Pearson. During 1928-1938, extensive collaborations took place between Egon
Pearson and Jerzy Neyman whom he first met in 1924 when Neyman came
to visit the Galton Laboratory.
Neyman and (E. S.) Pearson (1928a, b) approached inference problems to
build statistical tools for experimenters to choose between two classes of
models. These culminated later into path-breaking contributions, Neyman and
(E. S.) Pearson (1333a,b). In the latter papers, likelihood ratio tests in the
multi-parameter cases were fully developed. Fisher criticized the Neyman-
Pearson approach claiming that his estimation theory, along with the likeli-
hood and sufficiency, was quite adequate, and that the work of Neyman and
Pearson on testing of hypotheses was misguided. This controversy between
Fisher and Neyman-Pearson was never settled.
Neyman and (E. S.) Pearson (1933a) has been included in the Break-
throughs in Statistics, Volume I [Johnson and Kotz (1992)]. Egon Pearson’s
(1966) own account of his collaborations with Neyman was recorded in the
article, The Neyman-Pearson Story: 1926-1934. Egon Pearson’s interests and
enthusiasm in editing statistical tables have been particularly noteworthy. With
H. O. Hartley, the revised Biometrika Tables for Statisticians, Volume 1 and 2,
were respectively published (Cambridge University Press) in 1954 and 1972.
Some of these tables are indispensable even today.
Egon Pearson received many honors. In 1935, he received the Weldon
Prize and Medal, and became President (1955) of the Royal Statistical Society
for a two-year period. In 1955, he was awarded the Guy Medal in Gold by the
Royal Statistical Society.
E. S. Pearson died on June 12, 1980 in Midhurst, Sussex. The opening
articles of Bartlett (1981) and Tippett (1981) in Biometrika detailed Egon
Pearson’s productive life and career. The obituary article which appeared in
the J. Roy. Statist. Soc. , Ser. A (1981, 144, pp. 270-271) is also very informa-
tive. The article, Egon S. Pearson (August 11, 1895-June 12, 1980): An ap-
preciation, written by Jerzy Neyman (1981) ended with the following sen-
tence: “The memories of cooperation with Egon during the decade 1928-
1938 are very dear to me.”
K. Pearson: Karl Pearson was born on March 27, 1857 in London. He
attended University College School and the Kings College, Cambridge as a
scholar. He got his degree in 1879 securing the Third Wrangler posi-
tion in Mathematical Tripos. He was appointed to the Chair of Applied

14. Appendix
613
Mathematics at the University College London in 1885 where he stayed through-
out his career. In 1911, he moved to the Chair of Eugenics which was newly
established.
Karl Pearson was an established mathematician with interests in other fields,
including physics, law, genetics, history of religion and literature. Around
1885, he started formulating problems arising from observational studies and
systematically developed analysis of data. W. F. R. Weldon and F. Galton
influenced his thinking process significantly. At one time, he was consumed
by Galton’s Natural Inheritance from 1889.
Karl Pearson published an astonishingly large volume of original papers in
genetics, evolution, biometry, eugenics, anthropology, astronomy, and other
areas. His contributions on moments, correlations, association, system of
frequency curves, probable errors of moments and product moments, and
Chi-square goodness-of-fit, among his other discoveries, have become a part
of the folklore in statistics. Stigler (1989) throws more light on the invention
of correlation. He created the preliminary core of statistics. He was the nucleus
in the movement of systematic statistical thinking when it was essentially
unheard of. (K.) Pearson (1900) has been included in the Breakthroughs in
Statistics, Volume II [Johnson and Kotz (1993)].
Weldon, Galton and (K.) Pearson started the journal, Biometrika, whose
first issue came out in October 1901. Karl Pearson edited the journal until
1936. He resigned from the Galton Chair in 1933 and his department was split
into two separate departments. His son, Egon Pearson became the Reader
and Head of a separate statistics department whereas R. A. Fisher was ap-
pointed as the new Galton Professor.
K. Pearson received many honors, including the election (1896) to the
Royal Society and was awarded the Darwin Medal in 1898. Egon Pearson’s
(1938) monograph, Karl Pearson: An Appreciation of Some Aspects of his
Life and Work, is fascinating to read. One may also look at the entry [David
(1968)] on Karl Pearson, included in the International Encyclopedia of Sta-
tistics for more information. Karl Pearson died on April 27, 1936 in
Coldharbour, Surrey.
C. R. Rao: Calyampudi Radhakrishna Rao was born on September
10, 1920, in Karnataka, India. He received M.A. (1940) in mathematics
from the Andhra University, another M.A. (1943) in statistics from the
University of Calcutta, Ph.D. (1948) and Sc.D. (1965) from the Univer-
sity of Cambridge. R. A. Fisher was Rao’s adviser for his Ph.D. thesis in
Cambridge. Fisher told Rao to find his own research problem. Rao worked
by himself on discriminant analysis and classification problems related to
his work on anthropology at the museum in King’s College. Rao (1992b)
recalled that when he showed his completed thesis to Fisher, he was told

614
14. Appendix
simply that “The problem was worth investigating.” Rao’s life, career, and
accomplishments have been documented in an interview article [DeGroot
(1987)]. Rao’s (1992b) article, Statistics as a Last Resort, has many interest-
ing stories and important historical details.
After securing M.A. degree in mathematics, Rao started looking for a job
without much success. The war broke out. Eventually he came to Calcutta to
be interviewed for a position of “a mathematician for the army service unit”
and there he met an individual who was sent to Calcutta to receive “training in
statistics” from the Indian Statistical Institute (ISI), founded earlier by P. C.
Mahalanobis. He was told that statistics was a subject for the future. Rao
went to visit ISI and he immediately decided to join to get “training in statis-
tics.” Mahalanobis admitted him to the Training Section of ISI starting Janu-
ary 1, 1941. The rest has been history.
After Rao returned to Calcutta in August, 1948 from Cambridge with a
Ph.D. degree, he became a full professor in ISI. For the next thirty years, Rao
was a key figure for nurturing the Institute’s programs, goals, and aspira-
tions. He has advised Ph.D. dissertations of many distinguished statisticians
and probabilists, including D. Basu, V. S. Varadarajan, S. R. S. Varadhan, K.
R. Parthasarathy, and DesRaj.
During 1949-1963, Rao was Head of the Division of Theoretical Statistics
in ISI. He became Director of the Research and Training School in ISI in
1963. In 1976, he gave up his administrative position in ISI, but continued as
the Jawaharlal Nehru Professor, a special chair created for Rao by the Prime
Minister of India, Indira Gandhi. After the death of Mahalanobis, the Chief of
ISI, Rao held the position of the Director and Secretary of ISI.
In the summer of 1978, Rao came for a casual visit to the University of
Pittsburgh and he was invited to give a university-wide popular lecture. The
day after his lecture, he was offered a position which was totally “unex-
pected” as Rao (1992b) recalled. He started a new career at the University of
Pittsburgh in 1979. In 1988, he moved to the University of Pennsylvania as
Professor and holder of the Eberly Chair in Statistics. He is also Director of
the Center for Multivariate Analysis. He has been the Editor or Co-Editor of
Sankhy&abar;, The Indian Journal of Statistics for many years since 1964.
Rao has made legendary contributions practically in all areas of statis-
tics. Many contributions, for example, on sufficiency, information, maxi-
mum likelihood, estimation, tests, multivariate analysis, discriminant analy-
sis, linear models, linear algebra, generalized inverses of matrices, MINQE
theory, design of experiments and combinatorics are path-breaking. Through
the last five decades, many of his discoveries have been incorporated as
standard material in courses and curriculum in mathematics, statistics, proba-

14. Appendix
615
bility, electrical engineering, medical statistics, to name a few. The Cramér-
Rao inequality, Cramér-Rao lower bound, Rao-Blackwellization, and Rao’s
Score, for example, have become household phrases in these and other fields.
Rao’s (1945) paper has been included in the Breakthroughs in Statistics, Vol-
ume I [Johnson and Kotz (1992)].
Rao also made penetrating discoveries in nonparametric inference, higher
order efficiencies, estimating equations, principal component and factor analy-
sis. His book, Linear Statistical Inference, essentially took off from where
Cramér’s (1946a) classic text had left. This phenomenally successful work
first appeared in 1965 (Wiley), with its second edition out in 1973 (Wiley),
and it has been translated in many languages including Russian, German, Japa-
nese, Czech, Polish, and Chinese.
In applied areas, for example, in anthropometry, psychometry, economet-
rics, quality control, genetics and statistical ecology, Rao’s contributions have
been equally far-reaching and deep. His book, Advanced Statistical Methods
in Biometric Research, was published in 1952 (Wiley) and it included the most
modern statistical analyses of the day. He introduced and developed a com-
prehensive theory of what are customarily called the weighted distributions
which takes into account the visibility bias in sampling from wild-life popula-
tions, for example. The methodology of weighted distributions has played
fundamental roles in areas including statistical ecology and environmental
monitoring.
In mathematics also, Rao is considered a leader. We may simply mention
the areas, for example, the characterization problems in probability, differen-
tial geometry, and linear algebra. He has been in the forefront of these areas
over many decades. Much of the theory of the generalized inverses of matri-
ces was developed, unified and made popular by Rao himself and jointly with
S. K. Mitra and other colleagues. The monograph, Generalized Inverse of
Matrices and its Applications, jointly written by Rao and Mitra (1971, Wiley)
has been a landmark in this area. Rao’s another monograph, Characterization
Problems of Mathematical Statistics, written jointly with A. Kagan and Yu. V.
Linnik (1972 Russian edition, 1973 Wiley edition) was very highly acclaimed.
The recent article of Ghosh et al. (1999) supplies many details on Rao’s
phenomenal contributions. He has written or coauthored more than one dozen
books, over three hundred research papers, and edited many special volumes.
For his legendary contributions, Rao has received many honors from all over
the world. He has received more than a dozen honorary doctorate degrees
from prestigious Universities and Institutes spread all over the globe.
Rao is a Fellow of the Royal Society and King’s College, a Fellow of the

616
14. Appendix
Indian National Science Academy, a Honorary Member of the International
Statistical Institute, to name a few. He became President (1977) of the Insti-
tute of Mathematical Statistics. He is an elected member of the U. S. National
Academy of Sciences.
Rao is very modest and has a charming personality. He is a captivating
story-teller. His hobby is photography. At Penn State, Rao continues to keep
himself busy, always working-guiding-sharing ideas with colleagues as well
as students, and travelling both far and near.
L. J. Savage: Leonard Jimmie Savage was born in Detroit, Michigan on
November 20, 1917. He made phenomenal contributions in the area of foun-
dations of statistics, particularly in Bayesian inference. Savage’s work has
bolstered the Bayesian movement many fold over a period of several decades.
Savage (1976) gave a penetrating overview of the body of works of R. A.
Fisher on statistical inference. His 1954 monograph, The Foundations of Sta-
tistics, has been a landmark in statistics. Lindley (1980) beautifully synthe-
sized Savage’s contributions in statistics and probability.
Savage died in New Haven, Connecticut on November 1, 1971. Savage’s
(1981) volume of articles will attest to his profound influence and legacy in
statistics and subjective probability theory. Many well-known personalities, in-
cluding F. J. Anscombe, D. A. Berry, B. de Finetti, L. Dubins, S. E. Fienberg,
M. Friedman, W. Kruskal, D. V. Lindley, F. Mosteller, W. A. Wallis and A.
Zellner, have written about Savage and his contributions. These and other refer-
ences are included in Savage (1981). Sampson and Spencer’s (1999) interview
article on I. Richard Savage, the brother of L. J. Savage, gives many personal
as well as professional accounts of Jimmie Savage’s life and work.
H. Scheffé: Henry Scheffé was born on April 11, 1907, in New York
City. His parents were German.
In 1925, Scheffé became a student at the Polytechnic Institute of Brook-
lyn and afterward he worked as a technical assistant at Bell Telephone Labo-
ratories. In 1928, he went to attend the University of Wisconsin to study
mathematics. He had taken one statistics related course during this period.
Scheffé received B.A. (1931) in mathematics with high honors, followed by
Ph.D. (1936), both from the University of Wisconsin. His Ph.D. thesis dealt
with asymptotic solutions of differential equations (1936, Trans. Amer. Math.
Soc., 40, 127-154).
After finishing the Ph.D. degree, Scheffé taught in a number of places. In
1941, he joined Princeton University with the expectation of doing research
in a more fertile area such as mathematical statistics instead of mathemat-
ics. There, he came to know T. W. Anderson, W. G. Cochran, W. J. Dixon,
A. M. Mood, F. Mosteller, J. Tukey and S. S. Wilks, among others. Up un-

14. Appendix
617
til 1948, he also taught at Syracuse University and UCLA. During this period,
he spent part of his time as a Guggenheim Fellow at the University of Califor-
nia, Berkeley, where he began collaborations with E. L. Lehmann. During
1948-1953, he was an associate professor of mathematical statistics at Co-
lumbia University.
In 1953, Scheffé moved to the University of California, Berkeley, as a
professor of statistics and remained there until retirement in 1974. During
1965-1968, he became the Chairman of the statistics department at Berkeley.
Scheffé wrote the authoritative book, The Analysis of Variance, which
was published in 1959 (Wiley). The papers of Lehmann and Scheffé (1950,
1955, 1956, Sankhya-) on completeness, similar regions and unbiased estima-
tion have had a tremendous influence on research in statistics. Scheffé’s fun-
damental contributions in the area of multiple comparisons have provided
essential tools used by statisticians everywhere. His contributions in the area
of the Behrens-Fisher problem are also particularly noteworthy.
Scheffé received many honors. He was President (1954) of the Institute
of Mathematical Statistics, Vice President (1954-1956) of the American Sta-
tistical Association, and he received the Fulbright Research Award (1962-
1963).
Scheffé enjoyed reading novels. He was fond of bicycling, swimming,
snorkeling and backpacking. He loved nature and enjoyed travelling. Later in
life, he learned to play the recorder and played chamber music with friends.
After his retirement from Berkeley in 1974, he joined the University of Indiana
in Bloomington for three years as a professor of mathematics. He returned to
Berkeley in June, 1977 to start the process of revising his book, The Analysis
of Variance. Daniel and Lehmann (1979) noted that this was not to be - Scheffé
died on July 5, 1977 from the injuries he sustained in a bicycle accident
earlier that day.
By action of the Council of the Institute of Mathematical Statistics, the
1979 volume of the Annals of Statistics was dedicated to the memory of
Henry Scheffé. Its opening article, prepared by Daniel and Lehmann (1979)
detailed his life and career.
C. Stein: Charles Stein was born on March 22, 1920, in Brooklyn, New
York. He received B.S. (1940) in mathematics from the University of Chi-
cago. He served in the U. S. Army Air Force during 1942-1946 and became a
Captain. He earned Ph.D. (1947) in mathematical statistics from Columbia
University, and joined the faculty of Statistical Laboratory at the University of
California, Berkeley.
In 1949-1950, Stein was a National Research Council Fellow in Paris.
He was an associate professor at the University of Chicago during 1951-

618
14. Appendix
1953. Since 1953, he has been a professor in the department of statistics at
Stanford University.
In an interview article [DeGroot (1986d)], Stein told that he had always
intended to be a mathematician. After studying some works of Wald, he pub-
lished the landmark paper [Stein (1945)] on a two-stage sampling strategy for
testing the mean of a normal population whose power did not depend on the
unknown population variance.
Stein received encouragement from senior researchers including A. Wald,
J. Neyman and K. J. Arrow. He started to generalize some of Wald’s work on
most stringent tests. In the interview article [DeGroot (1986d)], Stein men-
tioned that G. Hunt pointed it out to him that what he was doing was group
theory. He did not realize this in the beginning. Eventually, HuntStein Theorem
became a household phrase within the realm of invariance where Stein made
fundamental contributions.
Stein’s best known result is perhaps his proof of the inadmissibility [Stein
(1956)] of the sample mean vector 
 as an estimator of the mean vector µ in
the Np(µ, I) population when the dimension p is three or higher, under the
quadratic loss function. Later, James and Stein (1961) gave an explicit esti-
mator which had its risk smaller than that of  
, for all µ, under the quadratic
loss function. The dominating estimator has come to be known as the James-
Stein estimator. In this area, one frequently encounters a special identity which
is called the Stein Identity. James and Stein (1961) has been included in the
Breakthroughs in Statistics, Volume I [Johnson and Kotz (1992)]. Berger (1985)
gave an elegant exposition of this problem and its generalizations. DeGroot
(1986d) portrayed a delightful account of Stein’s life and career. Stein re-
mains busy and active in research.
“Student” (W. S. Gosset): William Sealy Gosset was born in 1876, the
oldest child of a Colonel in the Royal Engineers. He was a pioneer in the
development of statistical methods for design and analysis of experiments. He
is perhaps better known under the pseudonym “Student” than under his own
name. In most of his papers, he preferred to use the pseudonym “Student”
instead of his given name.
Following his father’s footsteps, Gosset entered the Royal Military Acad-
emy, Woollwich, to become a Royal Engineer himself. But, he was rejected
on account of poor eyesight. He graduated (1899) with a first class degree in
chemistry from New College in Oxford, and then joined the famous Guinness
Brewery in Dublin as a brewer. He stayed with this brewing firm for all his
life, ultimately becoming the Head Brewer in a new installation operated by the
Guinness family at Park Royal, London in 1935.
Gosset needed and developed statistical methods for small sample sizes
which he would then apply immediately to ascertain relationships between

14. Appendix
619
the key ingredients in beer. His path-breaking 1908 paper, included in the
Breakthroughs in Statistics, Volume II [Johnson and Kotz (1993)], gave the
foundation of the t-distribution. Gosset derived the probable error of a corre-
lation coefficient in 1908 and made several forceful conjectures, most of
these being proven true later by Fisher (1915). Fisher once described Gosset
as “Faraday of statistics.”
The monographs and articles of (E. S.) Pearson (1938), Reid (1982), David
(1968), Barnard (1992), Karlin (1992), Rao (1992a) would provide more de-
tails on the life and work of Gosset. Box’s (1978) fascinating biography, R. A.
Fisher: The Life of a Scientist, points out early diverse interactions among
(K.) Pearson, Gosset, and Fisher. Box (1987) portrayed the connections and
collaborations among Guinness, Gosset and Fisher. Additionally, both (M. G.)
Kendall (1963) and Neyman (1967) presented much historical perspectives.
One may also take a look at the entry [Irwin (1968)], William Sealy Gosset,
included in the International Encyclopedia of Statistics for more information.
Gosset died in October, 1937.
A. Wald: Abraham Wald was born on October 31, 1902, in Cluj, Roma-
nia. He was raised in an intellectual environment within his family. But shock-
ingly, his parents, sisters, one brother who was considered intellectually gifted,
their spouses and children, and other relatives, perished in German crematoria
and concentration camps. As the son of an orthodox Jew, he faced many
obstacles and was barred to pursue many interests as a child and young adult.
After graduating from the University of Cluj, Wald was finally admitted to
study mathematics at the University of Vienna. He came to know K. Menger
and K. Hahn in Vienna. Initially, Wald began to work on geometry. One of his
proofs was later incorporated into the seventh edition of David Hilbert’s clas-
sic text, Grundlagen der Geometrie. But, it was clear that there was practi-
cally no prospect of getting an academic position in Austria in spite of Wald’s
brilliance.
Menger advised him to move into some area of applied mathematics. He
met O. Morgenstern and started working on mathematical economics. He
made fundamental contributions in econometrics through a series of papers
and a book. The three brilliant scholars, O. Morgenstern, J. von Neumann
and A. Wald later emigrated to U.S.A., and in 1944, von Neuman and
Morgenstern’s famous book, Theory of Games and Economic Behavior, ap-
peared. These collaborations led to a fuller growth of statistical decision theory,
culminating into Wald’s (1950) classic book, Statistical Decision Functions.
Wald came to U.S.A. in the summer of 1938 as a Fellow of the Cowles
Commission for Research in Economics. Hotelling arranged for Wald’s re-

620
14. Appendix
lease from the Fellowship of the Cowles Commission to enable him to accept
a Fellowship from the Carnegie Corporation and join Columbia University.
Wald quickly learned statistical theory by attending lectures of Hotelling at
Columbia. In Columbia, Wald had a long period of collaborations with J.
Wolfowitz. Together, they wrote a number of fundamental papers in sequen-
tial analysis and multiple decision theory.
Wald’s lecture notes on the topics he taught at Columbia were lucid but
rigorous and challenging. These notes have taught and given inspirations to
the next several generations of aspiring statisticians. During this period, he
wrote a number of important papers including the fundamental one on estima-
tion and testing [Wald (1939)], even before he came to know the details of
statistical theory. Wald gave the foundation to the theory of statistical decision
functions. This deep area is probably one of his greatest contributions. The
sequential analysis is another vast area where he made legendary contribu-
tions. Wald’s (1947) book, Sequential Analysis, has always been regarded as
a classic. The two papers, Wald (1945,1949b), have been included in the
Breakthroughs in Statistics, Volume I [Johnson and Kotz (1992)].
At the invitation of Mahalanobis, in November, 1950, Wald left for a visit
to the Indian Statistical Institute, Calcutta, and meet colleagues and students
there. After the official obligations of the trip was taken care of, Wald along
with his wife left on December 13, 1950, on an Air India flight for a visit to
the picturesque southern India. Unfortunately, the plane got lost in the fog and
crashed into the peaks of the Nilgiri mountains. All passengers aboard that
flight, including Wald and his wife, perished. This unfortunate accident shook
the world of mathematics, statistics and economics. The world lost a true
genius at a tender age of 48.
D. Basu’s article, Learning from Counterexamples: At the Indian Statisti-
cal Institute in the Early Fifties, included in Ghosh et al. (1992), recalls inter-
esting stories about Wald at the time of his visit to the Institute in November-
December, 1950. These are important stories because very little is otherwise
known about what Wald was doing or thinking during the last few days of his
life.
Wald became President (1948) of the Institute of Mathematical Statis-
tics. At the Abraham Wald Memorial Session held on September 7, 1951,
during the Minneapolis meeting of the Institute of Mathematical Statistics,
J. Wolfowitz had said, “The personal loss will be felt by his numerous
friends, but all must mourn for the statistical discoveries yet unmade which
were buried in the flaming wreckage on a mountain side in South India
and which will slowly and painfully have to be made by others.” By action
of the Council of the Institute of Mathematical Statistics, the 1952 volume

14. Appendix
621
of the Annals of Mathematical Statistics was dedicated to the memory of
Abraham Wald. Its opening articles, prepared by Wolfowitz (1952), Menger
(1952), and Tintner (1952) detailed the life and career of Wald.
Epilogue
It will be safe to say that we have already included too many citations in
this section. Yet, it is possible, however, that we have left out the references
of some important historical volumes or documents. But, for a reader who is
just beginning to get interested in the history of statistics, a very long list of
references will probably do more harm than good. As a starter, the following
narrower list will hopefully serve a reader better:
R. A. Fisher, The Life of a Scientist, by Joan Fisher Box (1978)
Neyman from Life, by Constance Reid (1982)
The Making of Statisticians, edited by Joe Gani (1982)
American Contributions to Mathematical Statistics in the Nineteenth Cen-
tury, edited by Steve Stigler (1980)
The History of Statistics: The Measurement of Uncertainty Before 1900,
by Steve Stigler (1986)
Glimpses of India’s Statistical Heritage, edited by J. K. Ghosh, S. K. Mitra,
and K. R. Parthasarathy (1992)
Enjoy and Celebrate Statistics by Learning Its History
14.3 Selected Statistical Tables
This section provides some of the standard statistical tables. These were pre-
pared with the help of MAPLE.
Tables 14.3.1a-14.3.1b correspond to the distribution function of a stan-
dard normal distribution. Tables 14.3.2, 14.3.3 and 14.3.4 respectively corre-
spond to the percentage points of the Chi-square, Student’s t, and F distribu-
tion. One may look at Lindley and Scott (1995) or other sources for more
extensive sets of tables like these.
14.3.1
The Standard Normal Distribution Function
In the Tables 14.3.1a and 14.3.1b, the first column and row respectively
designate the “first” and “second” decimal points of z. Let us suppose that

622
14. Appendix
Z stands for the standard normal random variable. Look at the Figures 14.3.1-
14.3.2.
Figure 14.3.1. N(0, 1) DF Φ(z) Is the Shaded Area When z < 0
Figure 14.3.2. N(0, 1) DF Φ(z) Is the Shaded Area When z > 0
Now, consider the following examples:

14. Appendix
623

624
14. Appendix
Table 14.3.1a. Values of the N(0, 1) Distribution
Function: 
z
.00
.01
.02
.03
.04
0.0
.50000
.50399
.50798
.51197
.51595
0.1
.53983
.54380
.54776
.55172
.55567
0.2
.57926
.58317
.58706
.59095
.59483
0.3
.61791
.62172
.62552
.62930
.63307
0.4
.65542
.65910
.66276
.66640
.67003
0.5
.69146
.69497
.69847
.70194
.70540
0.6
.72575
.72907
.73237
.73565
.73891
0.7
.75804
.76115
.76424
.76730
.77035
0.8
.78814
.79103
.79389
.79673
.79955
0.9
.81594
.81859
.82121
.82381
.82639
1.0
.84134
.84375
.84614
.84849
.85083
1.1
.86433
.86650
.86864
.87076
.87286
1.2
.88493
.88686
.88877
.89065
.89251
1.3
.90320
.90490
.90658
.90824
.90988
1.4
.91924
.92073
.92220
.92364
.92507
1.5
.93319
.93448
.93574
.93699
.93822
1.6
.94520
.94630
.94738
.94845
.94950
1.7
.95543
.95637
.95728
.95818
.95907
1.8
.96407
.96485
.96562
.96638
.96712
1.9
.97128
.97193
.97257
.97320
.97381
2.0
.97725
.97778
.97831
.97882
.97932
2.1
.98214
.98257
.98300
.98341
.98382
2.2
.98610
.98645
.98679
.98713
.98745
2.3
.98928
.98956
.98983
.99010
.99036
2.4
.99180
.99202
.99224
.99245
.99266
2.5
.99379
.99396
.99413
.99430
.99446
2.6
.99534
.99547
.99560
.99573
.99585
2.7
.99653
.99664
.99674
.99683
.99693
2.8
.99744
.99752
.99760
.99767
.99774
2.9
.99813
.99819
.99825
.99831
.99836
3.0
.99865
.99869
.99874
.99878
.99882
3.1
.99903
.99906
.99910
.99913
.99916
3.2
.99931
.99934
.99936
.99938
.99940
3.3
.99952
.99953
.99955
.99957
.99958
3.4
.99966
.99968
.99969
.99970
.99971
3.5
.99977
.99978
.99978
.99979
.99980

14. Appendix
625
Table 14.3.1b. Values of the N(0, 1) Distribution
Function: 
z
.05
.06
.07
.08
.09
0.0
.51994
.52392
.52790
.53188
.53586
0.1
.55962
.56356
.56749
.57142
.57535
0.2
.59871
.60257
.60642
.61026
.61409
0.3
.63683
.64058
.64431
.64803
.65173
0.4
.67364
.67724
.68082
.68439
.68793
0.5
.70884
.71226
.71566
.71904
.72240
0.6
.74215
.74537
.74857
.75175
.75490
0.7
.77337
.77637
.77935
.78230
.78524
0.8
.80234
.80511
.80785
.81057
.81327
0.9
.82894
.83147
.83398
.83646
.83891
1.0
.85314
.85543
.85769
.85993
.86214
1.1
.87493
.87698
.87900
.88100
.88298
1.2
.89435
.89617
.89796
.89973
.90147
1.3
.91149
.91309
.91466
.91621
.91774
1.4
.92647
.92785
.92922
.93056
.93189
1.5
.93943
.94062
.94179
.94295
.94408
1.6
.95053
.95154
.95254
.95352
.95449
1.7
.95994
.96080
.96164
.96246
.96327
1.8
.96784
.96856
.96926
.96995
.97062
1.9
.97441
.97500
.97558
.97615
.97670
2.0
.97982
.98030
.98077
.98124
.98169
2.1
.98422
.98461
.98500
.98537
.98574
2.2
.98778
.98809
.98840
.98870
.98899
2.3
.99061
.99086
.99111
.99134
.99158
2.4
.99286
.99305
.99324
.99343
.99361
2.5
.99461
.99477
.99492
.99506
.99520
2.6
.99598
.99609
.99621
.99632
.99643
2.7
.99702
.99711
.99720
.99728
.99736
2.8
.99781
.99788
.99795
.99801
.99807
2.9
.99841
.99846
.99851
.99856
.99861
3.0
.99886
.99889
.99893
.99896
.99900
3.1
.99918
.99921
.99924
.99926
.99929
3.2
.99942
.99944
.99946
.99948
.99950
3.3
.99960
.99961
.99962
.99964
.99965
3.4
.99972
.99973
.99974
.99975
.99976
3.5
.99981
.99981
.99982
.99983
.99983

626
14. Appendix
14.3.2
Percentage Points of the Chi-Square Distribution
The Table 14.3.2 provides the lower 100γ% point 
 for the 
 distribu-
tion for different values of v and γ. See the Figure 14.3.3.
Figure 14.3.3. The Shaded Area Is the Probability γ
Suppose that X has the 
 distribution with some appropriate v. Now, con-
sider the following examples:
Remark 14.3.1 The percentage points of the 
 distribution can be easily
found using the standard normal Table 14.3.1. Observe that
For example, when γ = .95, one has 
 Thus, from the
Table 14.3.1b we find that 
In the Chi-square Table 14.3.2, we have 
Remark 14.3.2 The percentage points of the 
 distribution can be easily
found using direct integrals of the exponential pdf. From (5.4.16), observe
that

14. Appendix
627
For example, when γ = .95, one has 
 In the
Chi-square Table 14.3.2, we have 
Table 14.3.2. Lower 100γ% 
 Point for the 
Distribution: 
v
γγγγγ = .01
γγγγγ = .025
γγγγγ = .05
γγγγγ = .95
γγγγγ = .975
γγγγγ = .99
1
.00016
.00098
.00393
3.8415
5.0239
6.6349
2
.02010
.05064
.10259
5.9915
7.3778
9.2103
3
.11490
.21570
.35185
7.8147
9.3484
11.345
4
.29711
.48442
.71072
9.4877
11.143
13.277
5
.55430
.83121
1.1455
11.070
12.833
15.086
6
.87209
1.2373
1.6354
12.592
14.449
16.812
7
1.2390
1.5643
2.1673
14.067
16.013
18.475
8
1.6465
2.1797
2.7326
15.507
17.535
20.090
9
2.0879
2.7004
3.3251
16.919
19.023
21.666
10
2.5582
3.2470
3.9403
18.307
20.483
23.209
11
3.0535
3.8157
4.5748
19.675
21.920
24.725
12
3.5706
4.4038
5.2260
21.026
23.337
26.217
13
4.1069
5.0088
5.8919
22.362
24.736
27.688
14
4.6604
5.6287
6.5706
23.685
26.119
29.141
15
5.2293
6.2621
7.2609
24.996
27.488
30.578
16
5.8122
6.9077
7.9616
26.296
28.845
32.000
17
6.4078
7.5642
8.6718
27.587
30.191
33.409
18
7.0149
8.2307
9.3905
28.869
31.526
34.805
19
7.6327
8.9065
10.117
30.144
32.852
36.191
20
8.2604
9.5908
10.851
31.410
34.170
37.566
21
8.8972
10.283
11.591
32.671
35.479
38.932
22
9.5425
10.982
12.338
33.924
36.781
40.289
23
10.196
11.689
13.091
35.172
38.076
41.638
24
10.856
12.401
13.848
36.415
39.364
42.980
25
11.524
13.120
14.611
37.652
40.646
44.314
26
12.198
13.844
15.379
35.563
41.923
45.642
27
12.879
14.573
16.151
40.113
43.195
46.963
28
13.565
15.308
16.928
41.337
44.461
48.278
29
14.256
16.047
17.708
42.550
45.731
49.590
30
14.953
16.791
18.493
43.780
46.980
50.890

628
14. Appendix
14.3.3
Percentage Points of the Student’s t Distribution
The Table 14.3.3 provides the lower 100γ% point tv,1–γ for the Student’s tv
distribution for different values of v and γ. See the Figure 14.3.4.
Figure 14.3.4. The Shaded Area Is the Probability γ
Suppose that X has the Student’s tv distribution with some appropriate v.
Now, consider the following examples:
Remark 14.3.3 The percentage points of the t1 distribution can be easily
found by directly integrating its pdf which happens to be the same as the
Cauchy pdf. Observe that
For example, when γ = .95, one has t1,.05 = tan (π(.95 - ½)) ≈ 6.3138. In the t
Table 14.3.3, we have t1,.05 = 6.314.

14. Appendix
629
Table 14.3.3. Lower 100?% Point tv,1−γ for the Student’s tv
Distribution: P(tv ≤ tv,1-γ) = γ
v
γγγγγ = .90
γγγγγ = .95
γγγγγ = .975
γγγγγ = .99
γγγγγ = .995
1
3.0777
6.3140
12.706
31.821
63.657
2
1.8856
2.9200
4.3027
6.9646
9.9248
3
1.6377
2.3534
3.1824
4.5407
5.8409
4
1.5332
2.1318
2.7764
3.7469
4.6041
5
1.4759
2.0150
2.5706
3.3649
4.0321
6
1.4398
1.9432
2.4469
3.1427
3.7074
7
1.4149
1.8946
2.3646
2.9980
3.4995
8
1.3968
1.8595
2.3060
2.8965
3.3554
9
1.3830
1.8331
2.2622
2.8214
3.2498
10
1.3722
1.8125
2.2281
2.7638
3.1693
11
1.3634
1.7959
2.2010
2.7181
3.1058
12
1.3562
1.7823
2.1788
2.6810
3.0545
13
1.3502
1.7709
2.1604
2.6503
3.0123
14
1.3450
1.7613
2.1448
2.6245
2.9768
15
1.3406
1.7531
2.1314
2.6025
2.9467
16
1.3368
1.7459
2.1199
2.5835
2.9208
17
1.3334
1.7396
2.1098
2.5669
2.8982
18
1.3304
1.7341
2.1009
2.5524
2.8784
19
1.3277
1.7291
2.0930
2.5395
2.8609
20
1.3253
1.7247
2.0860
2.5280
2.8453
21
1.3232
1.7207
2.0796
2.5176
2.8314
22
1.3212
1.7171
2.0739
2.5083
2.8188
23
1.3195
1.7139
2.0687
2.4999
2.8073
24
1.3178
1.7109
2.0639
2.4922
2.7969
25
1.3163
1.7081
2.0595
2.4851
2.7874
26
1.3150
1.7056
2.0555
2.4786
2.7787
27
1.3137
1.7033
2.0518
2.4727
2.7707
28
1.3125
1.7011
2.0484
2.4671
2.7633
29
1.3114
1.6991
2.0452
2.4620
2.7564
30
1.3104
1.6973
2.0423
2.4573
2.7500
35
1.3062
1.6896
2.0301
2.4377
2.7238
40
1.3031
1.6839
2.0211
2.4233
2.7045
45
1.3006
1.6794
2.0141
2.4121
2.6896
50
1.2987
1.6759
2.0086
2.4033
2.6778
100
1.2901
1.6602
1.9840
2.3642
2.6259

630
14. Appendix
14.3.4
Percentage Points of the F Distribution
The Table 14.3.4 provides the lower 100γ% point Fν1, ν2, 1-γ for the Fν1, ν2 distri-
bution for different values of ν1, ν2 and γ. See the Figure 14.3.5.
Figure 14.3.5. The Shaded Area Is the Probability γ
Suppose that X has the Fν1, ν2 distribution with some appropriate ν1, ν2. Now,
consider the following examples:
In confidence interval and testing of hypothesis problems, one may need the
values of Fν1, ν2, 1-γ where γ is small. In such situations, one needs to recall the
fact that if X has the Fν1, ν2 distribution, then Y = 1/X has the Fν2, ν1 distribution.
So, suppose that for some small value of γ, we wish to find the positive
number b such that P{X ≤ b} = δ. But, observe that P{Y ≥ 1/b} = P{X ≤ b}
= δ, so that P{Y ≤ 1/b} = 1 – δ. Now, one can obtain the value of a(=1/b)
from the Table 14.3.4 with γ = 1 – δ and the degrees of freedom reversed.
Look at the following example. We wish to determine the positive number
b such that with ν1 = 5, ν2 = 8, P{X ≤ b} = .05. In the Table 14.3.4, for F8,5
we find the number 4.8183 which corresponds to γ = .95 (= 1 – .05). That is,
b = 
 ≈ .20754.
Remark 14.3.4 For the F distribution, computing its percentage points
when the numerator degree of freedom is two can be fairly painless. We had
actually shown a simple way to accomplish this in Chapter 5. One can apply

14. Appendix
631
(5.4.17) with α = 1 – γ and verify that
For example, when γ = .90, .95, .975, .99 and .995, this formula provides the
values F2,2,1-γ = 9.0, 19.0, 39.0, 99.0 and 199.0 respectively which match with
the corresponding entries in the Table 14.3.4. Also, for example, this formula
provides the values F2,5,.05 = 5.7863 and F2,8,.025 = 6.0596 whereas the corre-
sponding entries in the Table 14.3.4 are 5.7861 and 6.0595 respectively.

632
14. Appendix
Table 14.3.4. Lower 100γ% Point Fν 1, ν 2,1-γ for the Fν1, ν2 Distribution:
P(Fν1, ν2 ≤ Fν1, ν2, 1-γ) = γ Where ν1, ν2 are Respectively the
Numerator and Denominator Degrees of Freedom
ν2
ν1
γ
1
2
5
8
10
15
20
.90
39.863
8.5263
4.0604
3.4579
3.2850
3.0732 2.9747
.95
161.45
18.513
6.6079
5.3177
4.9646
4.5431 4.3512
1
.975
647.79
38.506
10.007
7. 5709 6. 9367 6. 1995 6. 1995
.99
4052. 2 98. 503 16. 258 11. 259 10. 044 8. 6831 8. 0960
.995
16211
198. 50 22. 785 14. 688 12. 826 10. 798 9. 9439
.90
49. 500
9.0000
3. 7797 3. 1131 2. 9245 2. 6952 2. 5893
.95
199. 50
19.000
5. 7861 4. 4590 4. 1028 3. 6823 3. 4928
2
.975
799. 50
39.000
8. 4336 6. 0595 5. 4564
4. 765 4. 4613
.99
4999.5
99.000
13. 274 8. 6491 7. 5594 6. 3589 5. 8489
.995
20000
199.00
18. 314 11. 042 9. 4270 7. 7008 6. 9865
.90
57. 240 9. 2926
3. 453
2. 7264 2. 5216
2. 273 2. 1582
.95
230. 16 19. 296 5. 0503 3. 6875 3. 3258 2. 9013 2. 7109
5
.975
921. 85 39. 298 7. 1464 4. 8173 4. 2361 3. 5764 3. 2891
.99
5763. 6 99. 299 10. 967 6. 6318 5. 6363 4. 5556 4. 1027
.995
23056
199. 30 14. 940 8. 3018 6. 8724 5. 3721 4. 7616
.90
59. 439 9. 3668 3. 3393 2. 5893 2. 3772 2. 1185 1. 9985
.95
238. 88 19. 371 4. 8183 3. 4381 3. 0717 2. 6408 2. 4471
8
.975
956. 66 39. 373 6. 7572 4. 4333 3. 8549 3. 1987 2. 9128
.99
5981. 1 99. 374 10. 289 6. 0289 5. 0567 4. 0045 3. 5644
.995
23925
199. 37 13. 961 7. 4959 6. 1159 4. 6744 4. 0900
.90
60.195
9. 3916 3. 2974
2. 538
2. 3226 2. 0593 1. 9367
.95
241. 88 19. 396 4. 7351 3. 3472 2. 9782 2. 5437 2. 3479
10
.975
968. 63 39. 398 6. 6192 4. 2951 3. 7168 3. 0602 2. 7737
.99
6055. 8 99. 399 10. 051 5. 8143 4. 8491 3. 8049 3. 3682
.995
24224
199. 40 13. 618 7. 2106 5. 8467 4. 4235 3. 8470
.90
61. 220 9. 4247 3. 2380 2. 4642 2. 2435 1. 9722 1. 8449
.95
245. 95 19. 429 4. 6188 3. 2184 2. 8450 2. 4034 2. 2033
15
.975
984. 87 39. 431 6. 4277 4. 1012 3. 5217 2. 8621 2. 5731
.99
6157. 3 99. 433 9. 7222 5. 5151 4. 5581 3. 5222 3. 0880
.995
24630
199. 43 13. 146 6. 8143 5. 4707 4. 0698 3. 5020
.90
61. 740 9. 4413 3. 2067 2. 4246 2. 2007 1. 9243 1. 7938
.95
248. 01 19. 446 4. 5581 3. 1503 2. 7740 2. 3275 2. 1242
20
.975
993. 10 39. 448 6. 3286 3. 9995 3. 4185 2. 7559 2. 4645
.99
6208. 7 99. 449 9. 5526 5. 3591 4. 4054 3. 3719 2. 9377
.995
24836
199. 45 12. 903 6. 6082
5. 274
3. 8826 3. 3178

References
Abramowitz, M. and Stegun, I.A. (1972). Handbook of Mathematical
Functions (edited volume). Dover Publications, Inc., New York.
Aitken, A.C. and Silverstone, H. (1942). On the estimation of statistical
parameters. Proc. Roy. Soc. Edinburgh, Ser. A, 61, 186-194.
Aldrich, J. (1997). R. A. Fisher and the making of maximum likelihood
1912-1922. Statist. Sci., 3, 162-176.
Anderson, T.W. (1996). R. A. Fisher and multivariate analysis. Statist.
Sci., 11, 20-34.
Anscombe, F.J. (1949). The statistical analysis of insect counts based on
the negative binomial distribution. Biometrika, 5, 165-173.
Armitage, J.V. and Krishnaiah, P.R. (1964). Tables for Studentized largest
chi-square and their applications. Report ARL 64-188, Aerospace research
Laboratories, Wright-Patterson Air Force Base, Ohio.
Armitage, P. (1973). Sequential Medical Trials, second edition. Blackwell
Scientific Publications, Oxford.
Bahadur, R.R. (1954). Sufficiency and statistical decision functions. Ann.
Math. Statist., 25, 423-462.
Bahadur, R.R. (1957). On unbiased estimates of uniformly minimum vari-
ance. Sankhya-, 18, 211-224.
Bahadur, R.R. (1958). Examples of inconsistency of maximum likelihood
estimates. Sankh-ya-, 20, 207-210.
Bahadur, R.R. (1971). Some Limit Theorems in Statistics. NSF-CBMS Mono-
graph No. 4. Society for Industrial and Applied Mathematics, Philadelphia.
Bahadur, R.R. (1990). Probability Statistics and Design of Experiments
(edited volume). Festschrift in Honor of the Late Professor R. C. Bose. Wiley
Eastern Ltd., New Delhi.
Balakrishnan, N. and Basu, A.P. (1995). The Exponential Distributions:
Theory, Methods and Applications (edited volume). Gordon and Breach,
Amsterdam.
Banks, D.L. (1996). A conversation with I. J. Good. Statist. Sci., 11, 1-19.
Barankin, E.W. and Maitra, A. (1963). Generalization of the Fisher-Darmois-
Koopman-Pitman theorem on sufficient statistics. Sankhya-, Ser. A, 25, 217-
244.
Barnard, G. (1992). Review of “Statistical Inference and Analysis: Se-
lected Correspondence of R. A. Fisher (Edited by J.H. Bennett)”. Statist. Sci.,
7, 5-12.
Barndorff-Nielson, G. (1978). Information and Exponential Families in
Statistical Theory. John Wiley & Sons, Inc., New York.
633

634
References
Bartlett, M.S. (1981). Egon Sharpe Pearson, 1895-1980. Biometrika, 68,
1-12.
Basu, A.P. (1991). Sequential methods in reliability and life testing. Hand-
book of Sequential Analysis (B.K. Ghosh and P.K. Sen, eds.), Chapter 25, pp.
581-592. Marcel Dekker, Inc., New York.
Basu, D. (1955a). On statistics independent of a complete sufficient statis-
tic. Sankhya-, 15, 377-380.
Basu, D. (1955b). An inconsistency of the method of maximum likeli-
hood. Ann. Math. Statist., 26, 144-145.
Basu, D. (1958). On statistics independent of sufficient statistics. Sankhya-
, 20, 223-226.
Basu, D. (1964). Recovery of ancillary information. Contributions to Sta-
tistics, the 70th Birthday Festschrift Volume Presented to P. C. Mahalanobis.
Pergamon Press, Oxford.
Bather, J. (1996). A conversation with Herman Chernoff. Statist. Sci., 11,
335-350.
Bayes, T. (1783). An essay towards solving a problem in the doctrine of
chances. Phil. Trans. Roy. Soc., 53, 370-418.
Bechhofer, R.E., Kiefer, J. and Sobel, M. (1968). Sequential Identifica-
tion and Ranking Procedures. University of Chicago Press, Chicago.
Behrens, W.V. (1929). Ein betrag zur fehlerberechnung bei weniger
beobachtungen. Landwirtschaftliche Jahrbücher, 68, 807-837.
Bellhouse, D.R. and Genest, C. (1999). A history of the Statistical Society
of Canada: The formative years. Statist. Sci., 14, 80-125.
Bennett, J.H. (1971-1974). Collected Works of R. A. Fisher, Volumes 1-5
(edited volumes). University of Adelaide, South Australia.
Bennett, J.H. (1990). Statistical Inference and Analysis: Selected Corre-
spondence of R. A. Fisher (edited volume). Oxford University Press, Oxford.
Beran, R.J. and Fisher, N.I. (1998). A conversation with Geoff Watson.
Statist. Sci., 13, 75-93.
Berger, J.O. (1985). Statistical Decision Theory and Bayesian Analysis,
second edition. Springer-Verlag, Inc., New York.
Berger, J.O., Boukai, B. and Wang, Y. (1997). Unified frequentist and Baye-
sian testing of a precise hypothesis (with discussions by D.V. Lindley, T.A.
Louis and D. V. Hinkley). Statist. Sci., 12, 133-160.
Bingham, N.H. (1996). A conversation with David Kendall. Statist. Sci.,
11, 159-188.
Blackwell, D. (1947). Conditional expectation and unbiased sequential es-
timation. Ann. Math. Statist., 18, 105-110.
Blackwell, D. and Girshick, M.A. (1954). Theory of Games and Statistical

References
635
Decisions. John Wiley & Sons, Inc., New York. Reprinted (1979) by the
Dover Publications, Inc., New York.
Box, J.F. (1978). R. A. Fisher, The Life of a Scientist. John Wiley & Sons,
Inc., New York.
Box, J.F. (1987). Guinness, Gosset, Fisher, and small samples. Statist.
Sci., 2, 45-52.
Brown, L.D. (1964). Sufficient statistics in the case of independent ran-
dom variables. Ann. Math. Statist., 35, 1456-1474.
Buehler, R. (1980). Fiducial inference. R. A. Fisher: An Appreciation (S.E.
Fienberg and D.V. Hinkley, eds.), pp. 109-118. Springer-Verlag, Inc., New
York.
Chapman, D.G. (1950). Some two-sample tests. Ann. Math. Statist., 21,
601-606.
Chatterjee, S.K. (1991). Two-stage and multistage procedures. Handbook
of Sequential Analysis (B.K. Ghosh and P.K. Sen, eds.), Chapter 2, pp. 21-
45. Marcel Dekker, Inc., New York.
Chen, T.T. and Tai, J.J. (1998). A conversation with C. C. Li. Statist. Sci.,
13, 378-387.
Chung, K.L. (1974). A Course in Probability Theory. Academic Press,
New York.
Cornish, E.A. (1954). The multivariate t-distribution associated with a set
of normal sample deviates. Austr. J. Physics, 7, 531-542.
Cornish, E.A. (1962). The multivariate t-distribution associated with the
general multivariate normal distribution. Technical Paper No. 13, Div. Math.
Statist., CSIRO, Australia.
Cox, D.R. (1952). Estimation by double sampling. Biometrika, 39, 217-
227.
Craig, C.C. (1986). Early days in statistics at Michigan. Statist. Sci., 1,
292-293.
Cramér, H. (1942). On harmonic analysis in certain functional spaces.
Arkiv Mat. Astron. Fysik, 28B. Reprinted in Breakthroughs in Statistics Vol-
ume I (S. Kotz and N. L. Johnson, eds.), 1992. Springer-Verlag, Inc., New
York.
Cramér, H. (1946a). Mathematical Methods of Statistics. Princeton Uni-
versity Press, Princeton.
Cramér, H. (1946b). A contribution to the theory of statistical estimation.
Skand. Akt. Tidskr., 29, 85-94.
Cramér, H. (1976). Half a century with probability theory: Some personal
recollections. Ann. Probab., 4, 509-546.
Creasy, M.A. (1954). Limits for the ratios of means. J. Roy. Statist. Soc.,
Ser. B, 16, 186-194.
Cressie, N., Davis, A.S., Folks, J.L., and Policello II, G.E. (1981). The

636
References
moment-generating function and negative integer moments. Amer. Statist.,
35, 148-150.
Daniel, C. and Lehmann, E.L. (1979). Henry Scheffé 1907-1977. Ann.
Statist., 7, 1149-1161.
Dantzig, G.B. (1940). On the non-existence of tests of Student’s hypoth-
esis having power functions independent of σ. Ann. Math. Statist., 11, 186-
192.
Darmois, G. (1945). Sur les lois limites de la dispersion de certaines esti-
mations. Rev. Int. Statist. Inst., 13, 9-15.
DasGupta, S. (1980). Distributions of the correlation coefficient. R. A.
Fisher: An Appreciation (S.E. Fienberg and D.V. Hinkley, eds.), pp. 9-16.
Springer-Verlag, Inc., New York.
DasGupta, S. and Perlman, M. (1974). Power of the noncentral F-test:
Effect of additional variates on Hotelling’s T2-test. J. Amer. Statist. Assoc., 69,
174-180.
David, F.N. (1968). Karl Pearson. International Encyclopedia of Statistics
(W.H. Kruskal and J.M. Tanur eds.), 1, pp. 653-655. Collier Macmillan Pub-
lishing Co., Inc., London. Reprinted (1978) by the Free Press, New York.
David, H.A. (1998). Statistics in U.S. universities in 1933 and the estab-
lishment of the statistical laboratory at Iowa State. Statist. Sci., 13, 66-74.
de Finetti, B. (1937). Foresight: Its logical laws, its subjective sources.
Translated and reprinted, Studies in Subjective Probability (H. Kyberg and H.
Smokler, eds.), 1964, pp. 93-158. John Wiley & Sons, Inc., New York. Re-
printed in Breakthroughs in Statistics Volume I (S. Kotz and N. L. Johnson,
eds.), 1992. Springer-Verlag, Inc., New York.
de Finetti, B. (1972). Probability, Induction, and Statistics. John Wiley &
Sons, Inc., New York.
de Finetti, B. (1974). Theory of Probability, Volumes 1 and 2. John Wiley
& Sons, Inc., New York.
DeGroot, M.H. (1986a). A conversation with David Blackwell. Statist.
Sci., 1, 40-53.
DeGroot, M.H. (1986b). A conversation with T. W. Anderson. Statist.
Sci., 1, 97-105.
DeGroot, M.H. (1986c). A conversation with Erich L. Lehmann. Statist.
Sci., 1, 243-258.
DeGroot, M.H. (1986d). A conversation with Charles Stein. Statist. Sci.,
1, 454-462.
DeGroot, M.H. (1987). A conversation with C. R. Rao. Statist. Sci., 2, 53-67.
DeGroot, M.H. (1988). A conversation with George A. Barnard. Statist.
Sci., 3, 196-212.

References
637
Dunnett, C.W. (1955). A multiple comparison procedure for comparing
several treatments with a control. J. Amer. Statist. Assoc., 50, 1096-1121.
Dunnett, C.W. and Sobel, M. (1954). A bivariate generalization of Student’s
t-distribution, with tables for certain special cases. Biometrika, 41, 153-169.
Dunnett, C.W. and Sobel, M. (1955). Approximations to the probability
integral and certain percentage points of a multivariate analogue of Student’s
t distribution. Biometrika, 42, 258-260.
Edwards, A.W.F. (1997a). Three early papers on efficient estimation. Statist.
Sci., 12, 35-47.
Edwards, A.W.F. (1997b). What did Fisher mean by “Inverse Probability”
in 1912-1922? Statist. Sci., 12, 177-184.
Efron, B.F. (1975). Defining the curvature of a statistical problem (with
applications to second order efficiency). Ann. Statist., 3, 1189-1242.
Efron, B.F. (1978). The geometry of exponential families. Ann. Statist., 6,
367-376.
Efron, B.F. (1998). R. A. Fisher in the 21st century (with discussions by
D.R. Cox, R. Kass, O. Barndorff-Nielsen, D.V. Hinkley, D.A.S. Fraser and
A.P. Dempster). Statist. Sci., 13, 95-122.
Feller, W. (1968). An Introduction to Probability and Its Applications,
Volume 1, third edition. John Wiley & Sons, Inc., New York.
Feller, W. (1971). An Introduction to Probability and Its Applications,
Volume 2, second edition. John Wiley & Sons, Inc., New York.
Ferguson, T.S. (1967). Mathematical Statistics. Academic Press, Inc.,
New York.
Fieller, E.C. (1954). Some problems in interval estimation. J. Roy. Statist.
Soc., Ser. B, 16, 175-185.
Fienberg, S.E. (1992). A brief history of statistics in three and one-half
chapters: A review essay. Statist. Sci., 7, 208-225.
Fienberg, S.E. (1997). Introduction to R. A. Fisher on inverse probability
and likelihood. Statist. Sci., 12, 161.
Fienberg, S.E. and Hinkley, D.V. (1980). R. A. Fisher: An Appreciation (ed-
ited volume). Lecture Notes in Statistics No. 1. Springer-Verlag, Inc., New York.
Findley, D.F. and Parzen, E. (1995). A conversation with Hirottugu Akaike.
Statist. Sci., 10, 104-117.
Finney, D.J. (1941). The distribution of the ratio of estimates of the two
variances in a sample from a normal bivariate population. Biometrika, 30,
190-192.
Fisher, R.A. (1912). On an absolute criterion for fitting frequency curves.
Messeng. Math., 42, 155-160.
Fisher, R.A. (1915). Frequency distribution of the values of the corre-

638
References
lation coefficients in samples from an indefinitely large population. Biometrika,
10, 507-521.
Fisher, R.A. (1920). A mathematical examination of the methods of deter-
mining the accuracy of an observation by the mean error, and by the mean
square error. Monthly Notices Roy. Astronom. Soc., 80, 758-770.
Fisher, R.A. (1921). On the “probable error” of a coefficient of correlation
deduced from a small sample. Metron, 1, 3-32.
Fisher, R.A. (1922). On the mathematical foundations of theoretical statis-
tics. Phil. Trans. Roy. Soc., A222, 309-368. Reprinted in Breakthroughs in
Statistics Volume I (S. Kotz and N. L. Johnson, eds.), 1992. Springer-Verlag,
New York.
Fisher, R.A. (1925a). Theory of statistical estimation. Proc. Camb. Phil.
Soc., 22, 700-725.
Fisher, R.A. (1925b). Statistical Methods for Research Workers. Oliver
and Boyd, Edinburgh. Reprinted (1973) by Hafner, New York. Reprinted in
Breakthroughs in Statistics Volume II (S. Kotz and N. L. Johnson, eds.),
1993. Springer-Verlag, Inc., New York.
Fisher, R.A. (1926). The arrangement of field experiments. J. Min. Agric.
G. Br., 33, 503-515. Reprinted in Breakthroughs in Statistics Volume II (S.
Kotz and N. L. Johnson, eds.), 1993. Springer-Verlag, Inc., New York.
Fisher, R.A. (1928). Moments and product moments of sampling distribu-
tions. Proc. London Math. Soc., 30, 199-238.
Fisher, R.A. (1930). Inverse probability. Proc. Camb. Phil. Soc., 26, 528-
535.
Fisher, R.A. (1934). Two new properties of mathematical likelihood. Proc.
Roy. Soc., Ser. A, 144, 285-307.
Fisher, R.A. (1935). The fiducial argument in statistical inference. Ann.
Eugenics, 6, 391-398.
Fisher, R.A. (1939). The comparison of samples with possibly unequal
variances. Ann. Eugenics, 9, 174-180.
Fisher, R.A. (1956). Statistical Methods and Scientific Inference. Oliver
and Boyd, Edinburgh and London.
Folks, J.L. (1981). Ideas of Statistics. John Wiley & Sons, Inc., New
York.
Folks, J.L. (1995). A conversation with Oscar Kempthorne. Statist. Sci.,
10, 321-336.
Frankel, M. and King, B. (1996). A conversation with Leslie Kish. Statist.
Sci., 11, 65-87.
Fréchet, M. (1943). Sur l’extension de certaines evaluations statistiques
de petits echantillons. Rev. Int. Inst. Statist., 11, 182-205.
Gani, J. (1982). The Making of Statisticians (edited volume). Springer-
Verlag, Inc., New York.
Gardiner, J.C. and Susarla, V. (1991). Time-sequential estimation. Hand-

References
639
book of Sequential Analysis (B.K. Ghosh and P.K. Sen, eds.), Chapter 27, pp.
613-631. Marcel Dekker, Inc., New York.
Gardiner, J.C., Susarla, V. and van Ryzin, J. (1986). Time-sequential esti-
mation of the exponential mean under random withdrawals. Ann. Statist., 14,
607-618.
Gauss, C.F. (1821). Theoria combinationis observationum erroribus mini-
mis obnoxiae. Its English translation is available in Gauss’s publications (1803-
1826).
Gayen, A.K. (1951). The frequency distribution of the product moment
correlation coefficient in random samples of any size drawn from non-normal
universes. Biometrika, 38, 219-247.
Ghosh, B.K. (1970). Sequential Tests of Statistical Hypotheses. Addison-
Wesley, Reading.
Ghosh, B.K. (1973). Some monotonicity theorems for χ2, F and t distribu-
tions with applications. J. Roy. Statist. Soc., Ser. B, 35, 480-492.
Ghosh, B.K. (1975). On the distribution of the difference of two t-vari-
ables. J. Amer. Statist. Assoc., 70, 463-467.
Ghosh, B.K. and Sen, P.K. (1991). Handbook of Sequential Analysis (ed-
ited volume). Marcel Dekker, Inc., New York.
Ghosh, J.K. (1988). Statistical Information and Likelihood: A Collection
of Critical Essays by Dr. D. Basu (edited volume). Lecture Notes in Statistics
No. 45. Springer-Verlag, Inc., New York.
Ghosh, J.K., Maiti, P., Rao, T.J., and Sinha, B.K. (1999). Evolution of
statistics in India. International Statist. Rev., 67, 13-34.
Ghosh, J.K., Mitra, S.K., and Parthasarathy, K.R. (1992). Glimpses of
India’s Statistical Heritage. Wiley Eastern, Ltd., New Delhi.
Ghosh, M. and Mukhopadhyay, N. (1976). On two fundamental problems
of sequential estimation. Sankhya-, Ser. A, 38, 203-218.
Ghosh, M. and Mukhopadhyay, N. (1981). Consistency and asymptotic
efficiency of two-stage and sequential estimation procedures. Sankhya-, Ser.
A, 43, 220-227.
Ghosh, M., Mukhopadhyay, N. and Sen, P.K. (1997). Sequential Estima-
tion. John Wiley & Sons, Inc., New York.
Ghosh M. and Pathak, P.K. (1992). Current Issues in Statistical Inference:
Essays in Honor of D. Basu (edited volume). Institute of Mathematical Statis-
tics Lecture Notes-Monograph Series, U.S.A.
Ghurye, S.G. (1958). Note on sufficient statistics and two-stage proce-
dures. Ann. Math. Statist., 29, 155-166.
Gleser, L.J. and Healy, J.D. (1976). Estimating the mean of a normal dis-
tribution with known coefficient of variation. J. Amer. Statist. Assoc., 71,
977-981.
Grams, W.F. and Serfling, R.J. (1973). Convergence rates for U-statistics

640
References
and related statistics. Ann. Statist., 1, 153-160.
Haldane, J.B.S. (1956). The estimation and significance of the logarithm
of a ratio of frequencies. Ann. Human Genetics, 20, 309-311.
Halmos, P.R. (1946). The theory of unbiased estimation. Ann. Math. Stat-
ist., 17, 34-43.
Halmos, P.R. (1985). I want to Be a Mathematician: An Automathography.
Springer-Verlag, Inc., New York.
Halmos, P.R. and Savage, L.J. (1949). Application of the Radon-Nikodym
theorem to the theory of sufficient statistics. Ann. Math. Statist., 20, 225-
241.
Hansen, M.H. (1987). Some history and reminiscences on survey sam-
pling. Statist. Sci., 2, 180-190.
Hewett, J. and Bulgren, W.G. (1971). Inequalities for some multivariate f-
distributions with applications. Technometrics, 13, 397-402.
Heyde, C. (1995). A conversation with Joe Gani. Statist. Sci., 10, 214-
230.
Hinkley, D.V. (1980a). Fisher’s development of conditional inference. R.
A. Fisher: An Appreciation (S.E. Fienberg and D.V. Hinkley, eds.), pp. 101-
108. Springer-Verlag, Inc., New York.
Hinkley, D.V. (1980b). R. A. Fisher: Some introductory remarks. R. A.
Fisher: An Appreciation (S.E. Fienberg and D.V. Hinkley, eds.), pp. 1-5.
Springer-Verlag, Inc., New York.
Hipp, C. (1974). Sufficient statistics and exponential families. Ann. Stat-
ist., 2, 1283-1292.
Hochberg, Y. and Tamhane, A.C. (1987). Multiple Comparison Proce-
dures. John Wiley & Sons, Inc., New York.
Hoeffding, W. (1948). A class of statistics with asymptotically normal
distributions. Ann. Math. Statist., 19, 293-325.
Hogg, R.V. (1986). On the origins of the Institute of Mathematical Statis-
tics. Statist. Sci., 10, 285-291.
Hollander, M. and Marshall, A.W. (1995). A conversation with Frank
Proschan. Statist. Sci., 1, 118-133.
Irwin, J.O. (1968). William Sealy Gosset. International Encyclopedia of
Statistics (W.H. Kruskal and J.M. Tanur eds.), 1, pp. 409-413. Collier Macmillan
Publishing Co., Inc., London. Reprinted (1978) by the Free Press, New York.
James, W. and Stein, C. (1961). Estimation with quadratic loss. Proc.
Fourth Berkeley Symp. Math. Statist. Probab., 1, 361-379. University of Cali-
fornia Press, Berkeley.
Jeffreys, H. (1957). Scientific Inference. Cambridge University Press,
London.
Johnson, N.L. and Kotz, S. (1969). Distributions in Statistics: Discrete
Distributions. John Wiley & Sons, Inc., New York.

References
641
Johnson, N.L. and Kotz, S. (1970). Distributions in Statistics: Continuous
Univariate Distributions-2. John Wiley & Sons, Inc., New York.
Johnson, N.L. and Kotz, S. (1972). Distributions in Statistics: Continuous
Multivariate Distributions. John Wiley & Sons, Inc., New York.
Johnson, N.L. and Kotz, S. (1992). Breakthroughs in Statistics Volume I
(edited volume). Springer-Verlag, Inc., New York.
Johnson, N.L. and Kotz, S. (1993). Breakthroughs in Statistics Volume II
(edited volume). Springer-Verlag, Inc., New York.
Kallianpur, G. and Rao, C.R. (1955). On Fisher’s lower bound to asymp-
totic variance of a consistent estimate. Sankhy a, 15, 331-342.
Karlin, S. (1992). R. A. Fisher and evolutionary theory. Statist. Sci., 7, 5-
12.
Karlin, S. and Rubin, H. (1956). The theory of decision procedures for
distributions with monotone likelihood ratio. Ann. Math. Statist., 27, 272-
299.
Kendall, D.G. (1983). A tribute to Harald Cramér. J. Roy. Statist. Soc., Ser.
A, 146, 211-212.
Kendall, D.G. (1991). Kolmogorov as I remember him. Statist. Sci., 6,
303-312.
Kendall, M.G. (1963). Ronald Aylmer Fisher, 1890-1962. Biometrika, 50,
1-16.
Kendall, M.G. and Stuart, A. (1979). The Advanced Theory of Statistics,
Volume II: Inference and Relationship, fourth edition. Macmillan, New York.
Khan, R.A. (1968). A note on estimating the mean of a normal distribution
with known coefficient of variation. J. Amer. Statist. Assoc., 63, 1039-1041.
Kimbal, A.W. (1951). On dependent tests of significance in the analysis of
variance. Ann. Math. Statist., 22, 600-602.
Kolmogorov, A.N. (1933). On the empirical determination of a distribu-
tion. G. 1st. Ital. Attuari, 4, 83-91. Reprinted in Breakthroughs in Statistics
Volume II (S. Kotz and N. L. Johnson, eds.), 1993. Springer-Verlag, Inc.,
New York.
Kolmogorov, A.N. (1950a). Unbiased estimates (in Russian). Izvestia Acad.
Nauk. USSR, 14, 303-326. (Amer. Math. Soc. Translations No. 90).
Kolmogorov, A.N. (1950b). Foundations of the Theory of Probability (Ger-
man edition, 1933). Chelsea, New York.
Krishnaiah, P.R. and Armitage, J.V. (1966). Tables for multivariate t-distri-
bution. Sankhya-, Ser. B, 28, 31-56.
Laird, N.M. (1989). A conversation with F. N. David. Statist. Sci., 4, 235-
246.
Lane, D.A. (1980). Fisher, Jeffreys, and the nature of inference. R. A.

642
References
Fisher: An Appreciation (S.E. Fienberg and D.V. Hinkley, eds.), pp. 148-160.
Springer-Verlag, Inc., New York.
LeCam, L. (1953). On some asymptotic properties of maximum likelihood
estimates and related Bayes’s estimates. Univ. California Publ. Statist., 1,
277-330.
LeCam, L. (1956). On the asymptotic theory of estimation and testing
hypotheses. Proc. Third Berkeley Symp. Math. Statist. Probab., 1, 129-156.
University of California Press, Berkeley.
LeCam, L. (1986a). Asymptotic Methods in Statistical Decision Theory.
Springer-Verlag, Inc., New York.
LeCam, L. (1986b). The central limit theorem around 1935 (with discus-
sions by H.F. Trotter, J.L. Doob and D. Pollard). Statist. Sci., 1, 78-96.
LeCam, L. and Lehmann, E.L. (1974). J Neyman: On the occasion of his
80th birthday. Ann. Statist., 2, vii-xiii.
LeCam, L. and Yang, G.C. (1990). Asymptotics in Statistics: Some Basic
Concepts. Springer-Verlag, Inc., New York.
Lehmann, E.L. (1951). Notes on the Theory of Estimation. University of
California Press, Berkeley.
Lehmann, E.L. (1983). Theory of Point Estimation. John Wiley & Sons,
Inc., New York.
Lehmann, E.L. (1986). Testing Statistical Hypotheses, second edition. John
Wiley & Sons, Inc., New York.
Lehmann, E.L. (1993). Mentors and early collaborators: Reminiscences
from the years 1940-1956 with an epilogue. Statist. Sci., 8, 331-341.
Lehmann, E.L. (1997). Testing Statistical Hypotheses: The story of a book.
Statist. Sci., 12, 48-52.
Lehmann, E.L. and Casella, G. (1998). Theory of Point Estimation, sec-
ond edition. Springer-Verlag, Inc., New York.
Lehmann, E.L. and Scheffé, H. (1950). Completeness, similar regions and
unbiased estimation-Part I. Sankhy a, 10, 305-340.
Lehmann, E.L. and Scheffé, H. (1955). Completeness, similar regions and
unbiased estimation-Part II. Sankhy a, 15, 219-236.
Lehmann, E.L. and Scheffé, H. (1956). Corrigenda: Completeness, similar
regions and unbiased estimation-Part I. Sankhy a, 17, 250.
Liaison (1993). A conversation with Charles W. Dunnett. Multiple Com-
parisons, Selection, and Applications in Biometry, A Festschrift in Honor of
Charles W. Dunnett (F. Hoppe, ed.), pp. 1-10. Marcel Dekker, Inc., New York.
Lindley, D.V. (1980). L. J. Savage - His work in probability and statistics.
Ann. Statist., 8, 1-24.
Lindley, D.V. and Scott, W.F. (1995). New Cambridge Statistical Tables,
second edition. Cambridge University Press, Cambridge.
Lukacs, E. (1960). Characteristic Functions. Charles Griffin & Co., Ltd.,

References
643
London.
MacNeill, I. (1993). A conversation with David J. Finney. Statist. Sci., 8,
187-201.
Mahalanobis, P.C. (1932). Auxiliary tables for Fisher’s Z-test in analysis
of variance. (Statistical notes for agricultural workers, No. 3). Indian J. Agric.
Sci., 2, 679-693.
Mahalanobis, P.C. (1940). A sample survey of acreage under jute in Ben-
gal, with discussion on planning of experiments. Proc. Second Indian Statist.
Conference. Statistical Publishing Society, Calcutta.
Mauromaustakos, A. (1984). A three-stage estimation for the negative ex-
ponential. Masters thesis, Department of Statist., Oklahoma State Univ.,
Stillwater.
McDonald, G.C. (1998). A conversation with Shanti Gupta. Statist. Sci.,
13, 291-305.
Menger, K. (1952). The formative years of Abraham wald and his work in
geometry. Ann. Math. Statist., 32, 14-20.
Moshman, J. (1958). A method for selecting the size of the initial sample
in Stein’s two-sample procedure. Ann. Math. Statist., 29, 667-671.
Mukhopadhyay, N. (1980). A consistent and asymptotically efficient two-
stage procedure to construct fixed-width confidence interval for the mean.
Metrika, 27, 281-284.
Mukhopadhyay, N. (1991). Sequential point estimation. Handbook of Se-
quential Analysis (B.K. Ghosh and P.K. Sen, eds.), Chapter 10, pp. 245-267.
Marcel Dekker, Inc., New York.
Mukhopadhyay, N. (1997). A conversation with Sujit Kumar Mitra. Stat-
ist. Sci., 12, 61-75.
Mukhopadhyay, N. and Duggan, W.T. (1997). Can a two-stage procedure
enjoy second-order properties? Sankhya-, Ser. A, 59, 435-448.
Mukhopadhyay, N. and Duggan, W.T. (1999). On a two-stage procedure
having second-order properties with applications. Ann. Inst. Statist. Math,
51, in press.
Mukhopadhyay, N. and Hamdy, H.I. (1984). On estimating the difference
of location parameters of two negative exponential distributions. Canad. J.
Statist., 12, 67-76.
Mukhopadhyay, N. and Mauromoustakos, A. (1987). Three-stage estima-
tion for the negative exponential distributions. Metrika, 34, 83-93.
Mukhopadhyay, N. and Solanky, T.K.S. (1994). Multistage Selection and
Ranking Procedures. Marcel Dekker, Inc., New York.
Neyman, J. (1934). On two different aspects of the representative method.
J. Roy. Statist. Soc., 97, 558-625. Reprinted in Breakthroughs in Statistics
Volume II (S. Kotz and N. L. Johnson, eds.), 1993. Springer-Verlag, Inc.,
New York.

644
References
Neyman, J. (1935a). Sur un teorema concernente le considette statistiche
sufficienti. Giorn. 1st. Ital. Att., 6, 320-334.
Neyman, J. (1935b). On the problem of confidence intervals. Ann. Math.
Statist., 6, 111-116.
Neyman, J. (1937). Outline of a theory of statistical estimation based on
the classical theory of probability. Phil. Trans. Roy. Soc., Ser. A, 236, 333-
380.
Neyman, J. (1949). Contribution to the theory of the X2 test. Proc. First
Berkeley Symp. Math. Statist. Probab., 1, 239-273. University of California
Press, Berkeley.
Neyman, J. (1961). Silver jubilee of my dispute with Fisher. J. Oper. Res.
Soc. (Japan), 3, 145-154.
Neyman, J. (1967). R. A. Fisher: An appreciation. Science, 156, 1456-
1460.
Neyman, J. (1981). Egon S. Pearson (August 11, 1895-June 12, 1980):
An appreciation. Ann. Statist., 9, 1-2.
Neyman, J. and Pearson, E.S. (1928a). On the use and interpretation of
certain test criteria for purposes of statistical inference, Part I. Biometrika,
20A, 175-240.
Neyman, J. and Pearson, E.S. (1928b). On the use and interpretation of
certain test criteria for purposes of statistical inference, Part II. Biometrika,
20A, 263-294.
Neyman, J. and Pearson, E.S. (1933a). On the problem of the most effi-
cient tests of statistical hypotheses. Phil. Trans. Roy. Soc., Ser. A, 231, 289-
337. Reprinted in Breakthroughs in Statistics Volume I (S. Kotz and N. L.
Johnson, eds.), 1992. Springer-Verlag, Inc., New York.
Neyman, J. and Pearson, E.S. (1933b). The testing of statistical hypoth-
eses in relation to probabilities a priori. Proc. Camb. Phil. Soc., 24, 492-510.
Neyman, J. and Scott, E.L. (1948). Consistent estimates based on partially
consistent observations. Econometrica, 16, 1-32.
Olkin, I. (1987). A conversation with Morris Hansen. Statist. Sci., 2, 162-179.
Olkin, I. (1989). A conversation with Maurice Bartlett. Statist. Sci., 4, 151-
163.
Page, W. (1989). An interview of Herbert Robbins. Herbert Robbins: Se-
lected Papers (T.L. Lai and D. Siegmund, eds.), pp. xix-xli. Springer-Verlag,
Inc., New York. Reprinted from the College Mathematics Journal (1984), 15.
Pearson, E.S. (1938). Karl Pearson: An Appreciation of Some Aspects of
His Life and Work. Cambridge University Press.
Pearson, E.S. (1966). The Neyman-Pearson story: 1926-1934. Research
Papers in Statistics: Festschrift for J. Neyman (F.N. David, ed.). John

References
645
Wiley & Sons, Inc., New York.
Pearson, E.S. and Kendall, M.G. (1970). Studies in the History of Statis-
tics and Probability (edited volume). Charles Griffin & Co., Ltd., London.
Pearson, K. (1900). On the criterion that a given system of deviations
from the probable in the case of a correlated system of variables is such that
it can be reasonably supposed to have arisen from random sampling. Phil.
Mag., 5:50, 157-172. Reprinted in Breakthroughs in Statistics Volume II (S.
Kotz and N. L. Johnson, eds.), 1993. Springer-Verlag, Inc., New York.
Pearson, K. (1902). On the systematic fitting of curves to observations
and measurements - part I. Biometrika, 1, 265-303.
Pearson, K. (1903). Mathematical contributions to the theory of evolution
XII: On a generalized theory of alternative inheritance, with a special refer-
ence to Mendel’s Laws. Phil. Trans., 203A, 53-87.
Petersen, C.G.L. (1896). The yearly immigration of young plaice into the
Limfjord from the German Sea. Rep. Danish Bio. Statist., 6, 1-48.
Press, S.J. (1989). A conversation with Ingram Olkin. Contributions to
Probability and Statistics, Essays in Honor of Ingram Olkin (L.J. Gleser,
M.D. Perlman, S.J. Press, and A.R. Sampson, eds.), pp. 7-33. Springer-
Verlag, Inc., New York.
Råde, L. (1997). A conversation with Harald Bergström. Statist. Sci., 12, 53-60.
Ramachandran, B.R. (1967). Advanced Theory of Characteristic Func-
tions. Statistical Publishing Society, Calcutta.
Rao, C.R. (1945). Information and accuracy attainable in the estimation of
statistical parameters. Bull. Calcutta Math. Soc., 37, 81-91. Reprinted in Break-
throughs in Statistics Volume I (S. Kotz and N. L. Johnson, eds.), 1992.
Springer-Verlag, Inc., New York.
Rao, C.R. (1947). Minimum variance and the estimation of several param-
eters. Proc. Camb. Phil. Soc., 43, 280-283.
Rao, C.R. (1968). P.C. Mahalanobis. International Encyclopedia of Sta-
tistics (W.H. Kruskal and J.M. Tanur, eds.), 1, pp. 571-576. Collier Macmillan
Publishing Co., Inc., London. Reprinted (1978) by the Free Press, New York.
Rao, C.R. (1973). Linear Statistical Inference and Its Applications, sec-
ond edition. John Wiley & Sons, Inc., New York.
Rao, C.R. (1992a). R. A. Fisher: The founder of modern statistics. Statist.
Sci., 7, 5-12.
Rao, C.R. (1992b). Statistics as a last resort. Glimpses of India’s Statisti-
cal Heritage (Ghosh, J.K., Mitra, S.K., and Parthasarathy, K.R., eds.), pp.
153-213. Wiley Eastern, Ltd., New Delhi.
Reid, C. (1982). Neyman from Life. Springer-Verlag, Inc., New York.
Reid, N. (1994). A conversation with Sir David Cox. Statist. Sci., 9,

646
References
439-455.
Reid, N. (1995). The roles of conditioning in inference (with discussions
by G. Casella, A.P. Dawid, T.J. DiCiccio, V.P. Godambe, C. Goutis, B. Li,
B.C. Lindsay, P. McCullagh, L.A. Ryan, T.A. Severini and M.T. Wells). Stat-
ist. Sci., 10, 138-157.
Sampson, A.R. and Spencer, B. (1999). A conversation with I. Richard
Savage. Statist. Sci., 14, 126-148.
Samuel-Cahn, E. (1992). A conversation with Esther Seiden. Statist. Sci.,
7, 339-357.
Satterthwaite, F.E. (1946). An approximate distribution of estimates of
variance components. Biometrics Bulletin, 2, 110-114.
Savage, L.J. (1954). The Foundations of Statistics. John Wiley & Sons,
Inc., New York. Reprinted (1972) by Dover Publications, Inc., New York.
Savage, L.J. (1976). On rereading R. A. Fisher. Ann. Statist., 4, 441-500.
Savage, L.J. (1981). The Writings of Leonard Jimmie Savage - A Memorial
Selection. Amer. Statist. Assoc. and Inst. Math. Statist., Washington, D.C.
Scheffé, H. and Tukey, J.W. (1944). A formula for sample sizes for popu-
lation tolerance limits. Ann. Math. Statist., 15, 217.
Scott, E.L. (1985). Jerzy Neyman. Encyclopedia of Statistical Sciences, 6
(S. Kotz and N.L. Johnson, eds.), pp. 215-223. John Wiley & Sons, Inc.,
New York.
Sen, P.K. (1981). Sequential Nonparametrics. John Wiley & Sons, Inc.,
New York.
Sen, P.K. and Ghosh, M. (1981). Sequential point estimation of estimable
parameters based on U-statistics.. Sankhy a, Ser. A, 43, 331-344.
Sen, P.K. and Singer, J.O. (1993). Large Sample Methods in Statistics.
Chapman & Hall, Inc., New York.
Serfling, R.J. (1980). Approximation Theorems of Mathematical Statis-
tics. John Wiley & Sons, Inc., New York.
Shafer, G. (1986). Savage revisited (with discussions by D.V. Lindley, A.P.
Dawid, P.C. Fishburn, R.M. Dawes and J.W. Pratt). Statist. Sci., 1, 463-501.
Shepp, L. (1992). A conversation with Yuri Vasilyevich Prokhorov. Statist.
Sci., 7, 123-130.
Shiryaev, A.N. (1991). Everything about Kolmogorov was unusual ... .
Statist. Sci., 6, 313-318.
Siegmund, D. (1985). Sequential Analysis: Tests and Confidence Inter-
vals. Springer-Verlag, Inc., New York.
Singpurwalla, N.D. and Smith, R.L. (1992). A conversation with Boris
Vladimirovich Gnedenko. Statist. Sci., 7, 273-283.
Smith, A. (1995). A conversation with Dennis Lindley. Statist. Sci., 10,
305-319.

References
647
Snedecor, G.W. (1934). Calculation and Interpretation of Analysis of Vari-
ance and Covariance. Collegiate Press, Ames.
Snell, J.L. (1997). A conversation with Joe Doob. Statist. Sci., 12, 301-
311.
Soper, H.E. (1913). On the probable error of the correlation coefficient to
a second approximation. Biometrika, 9, 91-115.
Soper, H.E., Young, A.W., Cave, B.M., Lee, A. and Pearson, K. (1917).
On the distribution of the correlation coefficient in small samples. A coopera-
tive study. Biometrika, 11, 328-413.
Starr, N. (1966). The performance of a sequential procedure for fixed-
width interval estimate. Ann. Math. Statist., 37, 36-50.
Stein, C. (1945). A two sample test for a linear hypothesis whose power is
independent of the variance. Ann. Math. Statist., 16, 243-258.
Stein, C. (1949). Some problems in sequential estimation (abstract).
Econometrica, 17, 77-78.
Stein, C. (1956). Inadmissibility of the usual estimator of the mean of a
multivariate normal distribution. Proc. Third Berkeley Symp. Math. Statist.
Probab., 1, 197-206. University of California Press, Berkeley.
Stigler, S.M. (1980). American Contributions to Mathematical Statistics
in the Nineteenth Century (edited volume). Arno Press, New York.
Stigler, S.M. (1986). The History of Statistics: The Measurement of Un-
certainty Before 1900. Harvard University Press, Cambridge.
Stigler, S.M. (1989). Francis Galton’s account of the invention of correla-
tion. Statist. Sci., 4, 77-86.
Stigler, S.M. (1991). Stochastic simulation in the nineteenth century. Stat-
ist. Sci., 6, 89-97.
Stigler, S.M. (1996). The history of statistics in 1933. Statist. Sci., 11,
244-252.
Stigler, S.M. (1997). Raghu Raj Bahadur 1924-1997. Inst. Math. Statist.
Bul., 26, 357-358.
“Student” (W.S. Gosset) (1908). The probable error of a mean. Biometrika,
6, 1-25. Reprinted in Breakthroughs in Statistics Volume II (S. Kotz and N. L.
Johnson, eds.), 1993. Springer-Verlag, Inc., New York.
Switzer, P. (1992). A conversation with Herbert Solomon. Statist. Sci., 7,
388-401.
Tintner, G. (1952). Abraham Wald’s contributions to econometrics. Ann.
Math. Statist., 32, 21-28.
Tippett, L.H.C. (1981). Egon Sharpe Pearson, 1895-1980: Appreciation
by L.H.C. Tippett. Biometrika, 68, 7-12.
Tong, Y.L. (1990). The Multivariate Normal Distribution. Springer-Verlag,
Inc., New York.
Unni, K. (1978). The theory of estimation in algebraic and analytic ex-

648
References
ponential families with applications to variance components models. Unpub-
lished Ph.D. Dissertation, Indian Statistical Institute, Calcutta.
Wald, A. (1939). Contributions to the theory of statistical estimation and
testing hypotheses. Ann. Math. Statist., 10, 299-326.
Wald, A. (1945). Sequential tests of statistical hypotheses. Ann. Math.
Statist., 16, 117-196. Reprinted in Breakthroughs in Statistics Volume I (S.
Kotz and N. L. Johnson, eds.), 1992. Springer-Verlag, Inc., New York.
Wald, A. (1947). Sequential Analysis. John Wiley & Sons, Inc., New York.
Reprinted (1973) by Dover Publications, Inc., New York.
Wald, A. (1949a). Note on the consistency of maximum likelihood esti-
mate. Ann. Math. Statist., 20, 595-601.
Wald, A. (1949b). Statistical decision functions. Ann. Math. Statist., 29,
165-205. Reprinted in Breakthroughs in Statistics Volume I (S. Kotz and N. L.
Johnson, eds.), 1992. Springer-Verlag, Inc., New York.
Wald, A. (1950). Statistical Decision Functions. John Wiley & Sons, Inc.,
New York.
Wallace, D.L. (1980). The Behrens-Fisher and Fieller-Creasy problems.
R. A. Fisher: An Appreciation (S.E. Fienberg and D.V. Hinkley, eds.), pp.
119-147. Springer-Verlag, Inc., New York.
Wegman, E.J. (1986). Some personal recollections of Harald Cramér on
the development of statistics and probability. Statist. Sci., 1, 528-535.
Whitehead, J. (1983). The Design and Analysis of Sequential Clinical
Trials. Ellis Horwood, Chichester.
Whitehead, J. (1986). On the bias of maximum likelihood estimation fol-
lowing a sequential test. Biometrika, 73, 573-581.
Whitehead, J. (1991). Sequential methods in clinical trials. Handbook of
Sequential Analysis (B.K. Ghosh and P.K. Sen, eds.), Chapter 26, pp. 593-
611. Marcel Dekker, Inc., New York.
Whittle, P. (1993). A conversation with Henry Daniels. Statist. Sci., 8,
342-353.
Wolfowitz, J. (1952). Abraham Wald, 1902-1950. Ann. Math. Statist., 32,
1-13.
Woodroofe, M. (1982). Nonlinear Renewal Theory is Sequential Analysis.
NSF-CBMS Monograph No. 39. Society for Industrial and Applied Math-
ematics, Philadelphia.
Zabell, S. (1989). R. A. Fisher on the history of inverse probability (with
discussions by R.L. Plackett and G.A. Barnard). Statist. Sci., 4, 247-263.
Zehna, P.W. (1966). Invariance of maximum likelihood estimators. Ann.
Math. Statist., 37, 744.
Zinger, A.A. (1958). The independence of quasi-polynomial statistics and
analytical properties of distributions. Theory Probab. Appl., 3, 247-265.

Index
A
Analysis of variance 200
Helmert transformation for
normal distribution 197-199,
236, 325, 336
Independent unit splitting of
sample variance 200
Ancillarity 309-311
Conditional inference 317-318
Location family 314-316
Location-scale family 314-316
Pivotal approach 446
Confidence interval 446-447
Recovery of information 312-
313, 316
Scale family 314-316
Ancillary statistic; see Ancillarity
Approximate confidence interval
542
Binomial distribution
One-sample case 548-549
Two-sample case 549-550
Variance stabilizing
transformation 556-557
Correlation coefficient
Variance stabilizing
transformation 562-563
Distribution-free
One-sample case 543-544
Two-sample case 544-545
Poisson distribution
One-sample case 553
Two-sample case 553-554
Variance stabilizing
transformation 559-560
Approximation of distributions
Binomial distribution to normal
distribution 277, 548-549, 556-
557
Binomial distribution to Poisson
distribution 34-35
Chi-square distribution to
normal distribution 264
F distribution to Chi-square
distribution 265, 267-270
Poisson distribution to normal
distribution 553-554, 559-560
Student’s t distribution to
normal distribution 264-267
Tail comparison with
normal distribution 46-47
Asymptotic distribution; see
Approximate distribution
Axioms of probability 6-7
B
Bahadur efficiency 567
Basu’s Theorem 324
Applications 325-327, 337-338
Complete sufficiency 320-323
Bayes estimator 478, 485-487
Bayesian methods 477
Bayes point estimate 485-487
Bayes risk 486
Bayesian risk 486
Frequentist risk 485-486
Bayes test 493-494
Credible intervals 478
Contrast with confidence
intervals 492-493
Credible sets 488
Highest posterior density
489-492
Posterior distribution 479-480
Prior distribution 479
Conjugate prior 481-484
Inverse gamma 498-500
Non-conjugate prior
494-497, 504-505
Pareto distribution 502-503
Test 493-494
Bayes’s Theorem 14
649

650
Index
Applications 14-15, 53, 478-479
False negative 15
False positive 15
Posterior probability 14
Prior probability 14
Bayes test of hypothesis 493-494
Behrens-Fisher problem 534, 579,v586
Fixed-width confidence
interval 586-589
Test for means 534-535
Two-stage sampling 586
Bernoulli distribution 33, 35, 64
Moments 71, 72
Bernstein-Chernoff inequality 146
Best linear unbiased estimator
(BLUE); see Unbiased estimator
Beta distribution 48, 480-483
Beta function 31
Beta integral 31
Bias of an estimator 351
Binomial distribution 33, 35, 37,
    59-60
Factorial moments 97
Moment generating function
80-81
Moments 72, 73
Normal approximation 277
Recursive formula 60
Reproductive property 180
Binomial Theorem 17
Applications 17-18, 54, 72, 80
Bivariate normal distribution 131-
138, 169-170, 204-206, 217-
217, 522, 535-536
Conditional distributions 134
Conditional means 134
Conditional variances 134
Density function 131, 214
Independence 139-141
Marginal distributions 131-133
Multivariate normal
distribution 212-214
Regression 218
Reproductive property 216-218
Test procedures
Comparing means 522-525
Comparing variances
528-529
Correlation coefficient
525-528
Bonferroni inequality 52
Applications 157, 451-471
Borel sigma-algebra; see Borel
sigma-field
Borel sigma-field 6, 51
Sample space 6
Sets 3, 4, 51, 52
Bounded risk 579, 581-582
C
Calculus reviews 28
Extrema of a function
Maximum 31
Minimum 31
Two variables 31, 32
Integral
Beta function 31
Beta integral 31
By parts 32
Dominated convergence
theorem 274
Fubini’s Theorem 502
Gamma function 30
Gamma integral 30
Interchanging derivative and
integral 502
Monotone convergence
theorem 71, 92, 274
Jacobian of transformation
   193-195
L’Hôpital’s rule 32
Leibnitz’s rule 29
Triangular inequality 32
Cauchy distribution 47
Tail comparison with normal
distribution 47
Cauchy-Schwarz inequality 149
Applications 122, 150-151,
156-157, 174-175, 368
Central absolute moment inequality
  159
Applications 158
Central limit theorem (CLT) 242,

Index
651
  257
Applications 259-263, 543-555
565-567
Sample mean 258
Sample variance 262
Central moments 77; see also
Moment of a distribution
Existence 77-79, 85, 93
Moment problem 87-88
Non-existence 77-79
Chi-square distribution 44; see also
Gamma distribution
Asymptotic property 264
Density 42, 44
Moment generating function
(mgf) 85-86
Moments 76-77, 85-86
Reproductive property 192
Statistical table 626-627
Combinations; see Counting rules
Complete statistic 318-320
Basu’s Theorem 324
Exponential family 322
Minimal sufficiency 320-323
Sufficiency 320-323
Complete sufficient statistic; see
Complete statistic
Compound distribution 113-115
Concave function 152-156
Jensen’s inequality 154
Conditional distribution 102-103,
106, 108-109, 115-119, 479
Conditional expectation 109, 129,
487
Conditional inference 317-318
Ancillarity 309-311
Recovery of information 312-
313, 316
Conditional probability 9
Bayes’s Theorem 14, 15, 53,
478-479
Independence of events 10, 11, 53
Conditional variance 109, 112
Confidence coefficient 442
Interpretation 451-452
Joint confidence 451, 468-469,
471, 473, 475
Multiple comparisons 463-469
Confidence interval 441, 569
Accuracy measures 452-455,
569-571
Approximate 542
Binomial 548-550, 556-558,
563
Correlation coefficient 563
Poisson 553-554, 559-560, 566
Variance stabilizing transforma-
tions 555-563
Confidence coefficient 442
Interpretation 451-452
Contrast with credible interval 492-
493
Coverage probability 441
Distribution-free approximate
Comparing means 544
Estimation of mean 543
Fixed-width 571-574, 584-587
Inversion of a test 444
Joint confidence intervals 451, 468-
469, 471, 473, 475
Lower 441, 469
Multiple comparisons 463-469, 475-
476
One-sample problem 441, 444-446,
448-451
Paired difference t 459
Pivotal approach 446-447
Sample size determination 569, 573-
574, 586-588
Behrens-Fisher problem 579,
586
Simultaneous confidence
intervals; see Joint confidence
intervals
Two-sample problem
Comparing locations 457-458
Comparing means 456, 476
Comparing scales 461-462, 476
Comparing variances 460, 476
Two-stage procedure 573-574
Behrens-Fisher problem 586
One sample problem 573
Uniform distribution 448, 462
Upper 441, 469
Using for tests of hypotheses

652
Index
455
Confidence region 463
Mean vector 463-465
Multiple comparisons 465-469,
475-476
Multivariate F distribution 468-
469
Multivariate t distribution 466
Continuous random variables 23
Continuous uniform distribution 37,
38
Convergence notions 241
Central limit theorem (CLT)
257
In distribution or law 253
In probability 242
Weak law of large numbers
(WLLN) 241-242
Convergence results 264-270
Central limit theorem (CLT)
257
Sample mean 258
Sample variance 262
Chi-square distribution 264
Dominated convergence
theorem 274
F distribution 265
Khinchine’s WLLN 245, 270
Sample variance 251-252
Mann-Wald Theorem 261, 275
Monotone convergence theo-
rem 71, 92, 274
Multivariate F distribution 279-
280
Multivariate t distribution 279-
280
Probability density function
F distribution 209-211, 267-
268
t distribution 207-209, 265-
266
Percentage points
F distribution 268-270
t distribution 266-267
Slutsky’s Theorem 257, 260-
263
Weak law of large numbers
(WLLN) 241-242
Weak WLLN 242-244, 270
Sample variance 251-252,
Convex function 152-156
Jensen’s inequality 154
Convolutions 185-187
Cornish-Fisher expansion 267-268
F percentage point 269-270
t percentage point 267-268
Correlation coefficient 121
Confidence interval 563
Sample correlation 216
Tests of hypotheses 560-563
Zero correlation and indepen-
dence 139-141, 171-172
Counting rules 16
Combinations 16
Permutations 16
Covariance 119
Covariance inequality 150; see also
Cauchy-Schwarz inequality
Applications 122-123, 368
Cramér-Rao lower bound (CRLB)
366
Attainment 368-369
Non-attainment 369-371
Non-iid case 374-375
Cramér-Rao inequality 366
Information 368
Minimum variance unbiased
estimator (MVUE) 369-370
Non-iid case 374-375
Sufficiency 375-377
Credible interval 478, 488
Contrast with confidence
intervals 492-493
Highest posterior density (HPD)
489-492
Credible sets 488
Critical function 399
Critical region 397-399
Cumulative distribution function
(cdf); see Distribution function
(df)
Curved exponential family 144, 335
D
DeMorgan’s Law 5, 11, 51
Derivative of integral 29, 30

Index
653
Leibnitz’s rule 29
Discrete random variables 18
Discrete uniform distribution 37
Disjoint sets 5, 6; see also Mutually
exclusive
Dispersion matrix 214, 218
Distribution; see Probability distri-
bution
Distribution-free confidence
interval; see confidence interval
Distribution-free approximate test
545-547
Location
Sign test 566
Bahadur efficiency 567
Mean 543-546
Distribution function (df) 2, 19-20
Continuity points 25, 56, 57
Continuous case 24, 25
Convolutions 185-187
Discontinuity points 24, 25, 26,
56, 57
Countable set 26, 27
Discrete case 19, 20, 21, 22
Order statistics 182-184
Right continuous 22
Dominated convergence theorem
274
Double exponential distribution; see
Laplace distribution
E
Estimation
Bayes 478, 485-487, 489-492
Maximum likelihood estimator
(MLE) 345-350, 539-542
Invariance property 350
Moments estimator 342-344
Point estimation 341
Bayes risk 478, 485-487
Bias 351
Bounded risk 579-580, 587-
589
Lehmann-Scheffé Theorems
296, 371
Mean square error 352
Risk function 352
Squared error loss 352
Sufficiency 282, 294-295,
300
Two-stage sampling 581-582
Estimator
Bayes 478, 485-487
Best linear unbiased estimator
(BLUE) 357-358
Complete; see Complete
statistic
Confidence interval 441, 569
Accuracy measures
452-455, 569-571
Fixed-width 571-574, 584-
587
Two-stage sampling
573-574, 586
Consistent 380-382
Credible interval 478, 488
Maximum likelihood estimator
(MLE) 345, 539-542
Asymptotic normality 540
Consistency 540-541
Efficiency 540
Inconsistency 541
Invariance property 350
Method of moment 342-345
Minimum variance unbiased
estimator; see Uniformly
minimum variance unbiased
estimator (UMVUE) 341
Rao-Blackwellization; see Rao-
Blackwellized estimator
Rao-Blackwellized estimator
360-365, 372
Rao-Blackwell Theorem 359
Risk function 579
Bounded risk 580
Squared error loss 352
Sufficient; see Sufficiency and
Sufficient statistic
Unbiased estimator 351
Non-existence 353-354
Uniformly minimum variance
unbiased estimator (UMVUE)
356
Non-existence 379
Under incompleteness 377-

654
Index
379
Uniqueness 371
Events
Conditional probability 2, 9-12
Disjoint; see Mutually exclusive
Favorable 10
Independence of events 10, 53
Mutually 11
Pairwise 11
Mutually exclusive 4
Partition 7, 51, 397
Probability 1-3, 6-8, 21, 65-66
Calculus of probability 2,
9-12
Probability axioms 6-7
Probability scheme 7, 9
Simple 6
Expectation of a random variable;
    see Moment of a distribution
Conditional 109, 129, 487
Exponential distribution 43, 201-
   202; see also Gamma distribution
Memoryless property 44
Moment generating function
76-77, 85
Moments 86
Standard exponential 43
Exponential family 141, 172-173,
   300
Completeness 322
Curved exponential 144, 335
One-parameter 141-144
Multi-parameter 144-145
Sufficiency 300
F
Factorial moments 88-89
F distribution 48, 209-210
Asymptotic property 265
Density 48
Moments 211
Statistical table 630, 632
Fiducial distribution; see Historical
notes
Fieller-Creasy problem 535, 586
Fisher information; see
Information
Fixed-width confidence interval
   571-574, 584-587
Two-stage sampling 573-574,
586
Frequentist concept 1-3; see also
Historical notes
Probability 2, 8, 21, 65-66
Risk 351-354
Fubini’s Theorem 502
G
Gamma distribution 42, 482-484
Density 42
Moment generating function
84-85
Moments 76-77
Reproductive property 192
Gamma function 30
Gamma integral 30
Geometric distribution 35
Moments 76, 77
Probability generating function
(pgf) 97
Reproductive property 227
Negative binomial distribution
227
H
Helmert transformation 197-199
   236, 325, 336
Normal distribution 38-40
Helmert variables 198
Normal distribution 197-199,
236
Histogram 189, 258-260
Simulation 187-189, 258-260
Historical notes
Analysis of variance 199-200,
601, 607, 616
Ancillarity 310, 316-317, 600-
603
Bayesian concepts 478, 596-
598, 601, 616
Behrens-Fisher problem 534-
535, 579, 586
Characterization of normal

Index
655
distribution 200-201, 615
Completeness 319, 605-606,
616
Confidence interval 441, 467,
602, 609-612
Conditional inference 310, 316-
317, 600-603, 614-616
Cornish-Fisher expansions 267-
268
Correlation coefficient 121, 217
Decision theory 478, 597, 619-
620
Fiducial distribution 441, 478,
600-603
Frequentist concepts 6, 37, 39,
46, 88, 159, 598-600, 607,
609-612, 614-620
Information 301, 596-597, 614-
616
Limit theorem 595, 597-599,
604
Maximum likelihood estimation
540-541, 600-601, 609-612,
614-616
Point estimation 342-343, 345,
351, 356, 358, 365-366, 381-
382, 387-388, 605, 614-616
Probability theory 597-599,
603-604, 615
Sample correlation coefficient
217, 602
Probability distribution 217,
526, 561
Selected brief biographies 593-
621
Sequential analysis 571-572,
597, 607, 619-620
Student’s t distribution 601,
618-619
Subjectivist probability 14, 477,
598, 616
Sufficiency 281, 289, 596, 600,
605-606, 609-610, 614-616
Tests of hypotheses 395, 428,
507, 567, 595, 602, 605-606,
609-612, 614-616
Hölder’s inequality 156-157
Hypergeometric distribution 56
Hypothesis 395
Alternative 395
Lower-sided 417
One-sided 396, 404-406,
418, 422-423
Two-sided 396, 425-428
Null 395
Simple 396
Power 399
Power function 399-400
Type I error 396-398
Type II error 396-398
Hypotheses testing; see Tests of
hypotheses
I
Independence of events 10
Mutually 11
Pairwise 11
Independence of random variables
   125-126, 167
Zero correlation 139-141
Inequality
Bernstein-Chernoff 146
Bonferroni 157
Applications 157, 175,
451-471
Cauchy-Schwarz 149
Applications 150-151,
156-157, 174-175, 368
Central absolute moment 159
Applications 158
Covariance 150; see also
Cauchy-Schwarz inequality
Applications 122-123, 150-151,
156-157, 368
Cramér-Rao 366
Applications 369-370, 374-375
Non-iid case 374-375
Hölder’s 156
Applications 157-157
Information; see Cramér-Rao
inequality
Jensen’s 154
Applications 155-156
Concave function 152-153,
156, 174-175
Convex function 152-153,

656
Index
156, 174-175
Lyapunov’s 156
Markov 145
Applications 146-149, 173-
174
Tchebysheff’s 148
Applications 148-149, 157,
174-175, 243
Triangular inequality 32
Applications 246-247
Information 300-304, 540, 563-564
Conditional inference 316-318
Ancillarity 309-311
Inequality; see Cramér-Rao
inequality
Matrix 305-308
One-parameter case 301-304
Recovery of information 312-
313, 316
Sufficiency via information
303-304
Two-parameter case 305-309
Integral
Beta integral 31
By parts 32, 61, 504
By substitution 39-42, 58, 62-
63, 74-76, 82-83, 563
Fubini’s Theorem 502
Gamma integral 30
Invariance property 350
Maximum likelihood estimator
(MLE) 345, 539-542
Inverse gamma distribution 498-500
Inverse of a matrix 224
Partitioned matrix 226
J
Jacobian of transformation 193-
195, 198
Jensen’s inequality 154
Applications 155-156
Concave function 152-156
Convex function 152-156
Joint confidence coefficient 451,
468-469, 471, 473, 475
Joint confidence intervals 451, 468-
469, 471, 473, 475
Joint confidence region; see Joint
confidence intervals
Joint distribution 101, 107-108
Joint sufficiency 291-293
K
Karlin-Rubin Theorem 422
Applications 423-424
Khinchine’s WLLN 245
Applications 245, 270-271
Sample variance 251-252
Kolmogorov axioms 6-7
L
Large-sample properties
Maximum likelihood estimator
(MLE) 354, 539-542
Asymptotic normality 540
Consistency 540-541
Efficiency 540
Laplace distribution 62, 94, 173
Moments 76
Lehmann-Scheffé Theorems 296,
   371
Complete statistic 318-320
Minimal sufficient statistic 320
   323
Minimum variance unbiased
estimator (UMVUE) 356
Non-existence 379
Uniqueness 371
Sufficient statistic 320-323
Leibnitz’s rule 29, 185
Derivative of integral 29, 30
Level of a test 399
L’Hôpital’s rule 32
Derivative of integral 32
Likelihood function 288, 345, 477,
   539
Likelihood equation 540
Maximum likelihood estimator
(MLE) 345-350, 539-542
Minimal sufficiency 294-295,
300
Neyman factorization 288-289
Sufficiency 282, 288-294

Index
657
Likelihood ratio (LR) 403-413, 416,
   507
Monotone Likelihood ratio
property (MLR) 420
Applications 421-424, 434
   436
Test; see Likelihood ratio test
Likelihood ratio (LR) test 507
Bivariate normal distribution
522
Comparing means 522-525
Comparing variances 528-
529
Correlation coefficient 525-
528
One-sample problem 508
Normal mean 509-512
Normal variance 512-515
Two-sample problem 515
Comparing means 515-518,
532-533
Comparing variances 519-
522, 533
Location family of distributions
314-316
Pivotal approach 446-451
Confidence interval 446
Location-scale family of distribu-
tions 314-316
Pivotal approach 446-451
Confidence interval 446
Logistic distribution 564
Lognormal distribution 45
Moment generating function
(mgf)
Non-existence 95
Moments 94
L’Hôpital’s rule 32
Loss function
Squared error loss 352-354,
579
Bayes risk 486
Frequentist Risk 352-354,
485-486, 579
Lyapunov’s inequality 156
M
Mann-Wald Theorem 261
Applications 262-263, 277-278
Marginal distribution 101-103
Markov inequality 145-148
Applications 146-148
Matrix 178, 224-226
Determinant 224
Dispersion 214, 218
Information matrix 305-308
Inverse 224
Partitioned 226
Jacobian 195-196, 198
Negative definite (n.d.) 225-226
Non-singular 224
Orthogonal 198, 225
Partitioned 215, 224-226
Determinant 226
Inverse 215, 226
Positive definite (p.d.) matrix
225-226
Positive semi definite (p.s.d.)
matrix 225, 305
Rank of a matrix 224
Maximum likelihood estimator
(MLE) 345-350, 539-542
Invariance property 350
Large-sample properties
Asymptotic normality 540
Consistency 540-541
Efficiency 540
Median of a distribution 28, 62, 63
Mean squared error 351-354
Method of moment estimator 342-
344
Minimal sufficiency 294-295, 300
Basu’s Theorem 324
Complete statistic 320-323
Lehmann-Scheffé Theorems
296
Neyman factorization 288-289
Neyman-Pearson Lemma 402
Minimum variance unbiased
estimator; see Unbiased estima-
tor
Moment generating function (mgf)
79, 254-256
Determination of a

658
Index
distribution 79, 80, 86-88, 95-
96, 190-192
Determination of moments 79
Moment of a distribution 65, 68, 77
Central moments 77
Expectation 66, 68
Expected value 66, 77
Factorial moments 88-89
Finiteness 77-79, 85, 93
Mean 66, 77
Moment problem 87-88
Non-existence 77-79
Standard deviation 68
Variance 67, 77
Moment estimator; see Method of
moment estimator
Moment problem 87-88
Monotone convergence theorem 71,
92, 274
Monotone likelihood ratio (MLR)
property 420
Applications 421, 434-435
Exponential family 421
Karlin-Rubin Theorem 422
Uniformly most powerful
(UMP) test 422
Lower-sided alternative 435-
436
Upper-sided alternative 422-
424
Most powerful (MP) test; see Tests
of hypotheses
Multinomial distribution 103-104,
160-161
Conditional pmf’s 106
Covariance 124-125
Marginals 105
Means 106
Moment generating function
160
Variances 106
Multinomial Theorem 104-105
Multiple comparisons 465-469, 475-
476
Joint confidence coefficient
451, 468-469, 471, 473, 475
Joint confidence intervals 451,
468-469, 471, 473, 475
Multivariate F distribution 219-
220, 468-469
Multivariate normal
distribution 212-215
Multivariate t distribution 218-
219, 466
Multivariate distributions 99
Covariance 119-121
Continuous case 107
Bivariate normal distribution
131-136
Conditional pmf’s 108-109,
115-119
Marginal pmf’s 108-109
Normal distribution
Correlation coefficient 121-123,
139
Independence 139-141
Discrete case 101
Conditional pmf’s 102-103
Marginal pmf’s 100-103
Multinomial distribution 104,
105, 124-125
F distribution 219-220
Convergence 279-280
Density 220
Normal distribution; see
Multivariate normal distribution
t distribution 218-219
Convergence 279-280
Density 219
Multivariate normal distribution 212-
214
Bivariate normal distribution
131-138, 169-170, 204-206,
215-217
Regression 138
Conditional distributions 134
Conditional means 134
Density function 131, 214
Conditional variances 134
Percentage points 138
Sampling distributions 216-218
Multivariate random variables 99
Conditional distribution 102-
103, 106, 108-109, 115-119,
479

Index
659
Conditional expectation 109
129, 487
Conditional variance 109, 112
Continuous case 107
Correlation coefficient 121-123,
169-171
Covariance 119
Discrete case 101
F distribution 219-220
Convergence 279-280
Density 220
Independence 125-127, 129-
131, 169-171
Joint distribution 101, 107-108
Marginal distribution 100-103,
108-109, 214
Normal distribution; see
Multivariate normal distribution
t distribution 218-219
Convergence 279-280
Density 219
Mutually independent events 11
N
Negative binomial distribution 36,
227
Negative definite (n.d.) matrix 225-
226
Negative exponential distribution 49,
64
Confidence intervals 448, 457-
458, 461-462, 470-471, 473,
476
Transformation with spacings
202-203
Neyman factorization 288-289
Likelihood function 288
Sufficiency 282-284
Neyman-Pearson Lemma 402
Applications 404-410
Discrete situations 410- 412
Dependent observations 416
Non-identical distribution 416
Not involving parameters
413-415
Non-exponential family Examples
143-145
Nonparametric tests; see Distribu-
tion-free approximate tests
Non-sufficient statistics 286-288,
297
Normal distribution 38-40
Helmert transformation 197-
199, 236, 325, 336
Independence of sample mean
and variance 199, 325-327
Moment generating function
82-84
Moments 73-75
Reproductive property 192
Standard normal distribution
40-41, 45, 63
Absolute moments 93
Moments 73-76, 82-84, 93
Statistical table 621, 624-625
Tail comparison with Cauchy
distribution 47
Tail comparison with
Student’s
t distribu-
tion 46-47
Normal marginal distribution
Multivariate normal distribution
100-103, 108-109, 214
Non-multivariate normal joint
distribution 136-138
O
Order statistics 182-184, 201-203
Exponential distribution 201-
202
Negative exponential distribu-
tion 202-203
Non-iid random samples 184,
230
Uniform distribution 181-184
Orthogonal matrix 198, 225
Determinant 225
Inverse 225

660
Index
P
Paired difference t methods
Confidence interval 459
Test of hypothesis 522-525
Pairwise independent events 11
Parameter 33-36, 38, 42-45, 48-50,
282-284, 286-288, 314, 351
Parametric function 350-351, 356,
366, 371, 374-375
Parameter space 283-288, 341
Pareto distribution 502-503
Partitioned matrix 215, 224-226
Determinant 226
Inverse 215, 226
Partition of sample space 7, 51, 397
Permutations; see Counting rules
Pivotal approach 446-447
Point estimation; see Estimation and
Estimator
Poisson distribution 34
Factorial moments 97
Moment generating function
81-82
Moments 73
Recursive formula 60
Reproductive property 217
Pooled sample variance 209, 278,
280
Positive definite (p.d.) matrix 225-
226
Posterior distribution 479-480
Bayesian methods 477-478,
485-487, 489-494
Power function of a test 399-400
Power of a test 399
Prior distribution 479
Bayesian methods 477-479,
485-487, 489-494
Conjugate 481-484
Non-conjugate 494-497, 504-
505
Probability axioms 6, 7
Scheme 7, 9
Probability concept
Conditional 9
Fiducial; see Historical notes
Frequentist; see Historical
notes
Relative frequency 1-3, 8, 21,
65-66
Subjectivist; see Historical
notes
Probability density function (pdf) 2,
24
Exponential family 141, 172-
173
One-parameter 141-144
Multi-parameter 144-145
Probability distribution 18, 19, 22,
23
Discrete 18, 19
Bernoulli distribution 33, 64
Binomial distribution 33
Geometric distribution 35,
227
Hypergeometric distribution
56
Negative binomial distribution
36, 227
Poisson distribution 34
Uniform distribution 37
Continuous 22, 23, 27
Beta distribution 48
Cauchy distribution 47
Tail comparison with
normal distribution 46
Chi-square distribution 44;
see also Gamma
distribution and Statistical
table
Curved exponential family
144
Double exponential distribu-
tion; see Laplace distribution
Exponential distribution 43;
see also Gamma distribution
Memoryless property 44
Moment generating
function 85
Standard exponential 43
Transformation with
spacings 201-203
Exponential family 141, 172-
173

Index
661
Curved exponential 144, 335
One-parameter 141-144
Multi-parameter 144-145
F distribution 48, 209-211
Asymptotic distribution 265,
267-270
Density 48
Moments 211
Statistical table 630, 632
Gamma distribution 42; see also
Chi-square distribution
Moments 76-77
Density 42
Laplace distribution 62, 94
Logistic distribution 564
Lognormal distribution 45, 62
Moment generating
function non-existence 95
Moments 94
Multivariate; see Multivariate
distributions
Negative exponential
distribution 49, 64
Transformation with
spacings 202-203
Normal distribution 38, 39, 40
Absolute moments 93
Independence of sample
mean and variance 199, 325-
327
Moments 73-76, 82-84, 93
Standard normal
distribution 40, 41, 45, 63
Tail comparison with
Cauchy distribution 47
Tail comparison with
Student’s t distribution 46-47
Pareto distribution 502-503
Posterior 479-480
Prior 479, 481-484, 494-497, 504-
505
Rayleigh distribution 50, 58, 91, 94
Student’s t distribution 45-46, 207-
209
Asymptotic distribution 264-
267
Density 45
Moments 208
Statistical table 628-629
Tail comparison with normal
distribution 46-47
Support of a distribution 66
Triangular distribution 229
Weibull distribution 50, 58, 91,
94
Uniform distribution 37, 38
Probability generating function (pgf)
88-89
Factorial moments 88-89
Probability mass function (pmf) 2,
19
Exponential family 142
One-parameter 143
Multi-parameter 144
Probability of an event
Additive rule 2, 11-12
Conditional probability 2, 9, 10-
12
Bayes’s Theorem 14-15, 53
False negative 15
False positive 15
Independence 10
Multiplicative rule 2, 11-12
Relative frequency 1-3, 8, 21,
65-66
Probability scheme 7, 9
P-value of a test; see Tests of
hypotheses
R
Random experiment 1, 6, 51
Randomized MP test 410
Binomial distribution 410-411
Poisson distribution 411-413
Random variable 2
Continuous 2
Discrete 2, 18
Expectation; see Moment of a
distribution
Conditional 109, 129, 487
Independence 125-126, 139-
141
Standard deviation; see

662
Index
Moment of a distribution
Variance; see Moment of a
distribution
Range 183-184
Triangular distribution 229
Uniform distribution 38, 184,
228-229
Rao-Blackwellization; see Rao
   Blackwellized estimator
Rao-Blackwellized estimator 360
   365, 372, 387-390
Rao-Blackwellized statistic; see
   Rao-Blackwellized estimator
Rao-Blackwell Theorem 359
Complete statistic 318-320
Uniformly minimum variance
unbiased estimator (UMVUE)
356
Non-existence 379
Under incompleteness 377-
379
Uniqueness 371
Rayleigh distribution 50, 58, 173,
   329, 384
Rejection region; see Tests of
   hypotheses
Relative frequency 1-3
Probability concept 2, 8, 21,
65-66
Risk function
Bayes risk 486
Point estimation 485-489
Bounded risk 579, 581-582
Two-stage sampling 569, 586
Frequentist risk 1-3, 351-354
S
Sample size determination 569
Bounded risk point estimation
579-581
Confidence intervals 569-573
Test of hypotheses 438
Two-stage procedure 573-574,
581-582
Sample space 6, 7
Partition 7, 51, 397
Sampling distribution 187-189
Beta distribution 196, 211-212
Bivariate normal distribution
206, 215-216, 238
F distribution 209-211
Importance of independence
Chi-square distribution 221-222
F distribution 223-224
Normal distribution 220-221
Student’s t distribution 223
Multivariate normal
distribution 218, 238
Normal distribution 192, 199
Characterizing property 200
Sample correlation coefficient
Exact distribution 217; also
see Historical notes
Simulation 187-189, 258-260
Histogram 189
Student’s t distribution 207-211
One-sample 208
Two-sample 211
Scale family of distributions 314
   316
Pivotal approach 446-451
Confidence interval 446
Sequential analysis 571-572, 620
Set operations 3
Complement 4
DeMorgan’s Law 5, 11, 51
Disjoint; see Mutually exclusive
Intersection 4
Laws 4
Limit infimum 5
Limit supremum 5
Mutually exclusive 4, 8, 52
Proper subset 3
Subset 3
Symmetric difference 4, 5, 52
Union 4
Venn diagram 4, 51
Sigma-algebra 6
Borel; see Sigma-field
Sigma-field 6
Borel 6, 7, 9

Index
663
Sign test 566-567
Simulation 187-189, 258-260, 452,
   570-571, 578, 580-581
Histogram 187-189, 258-260
Simultaneous confidence intervals
   451
Joint confidence coefficient
451
Multiple comparisons 463-469
Size of a test 399
Slutsky’s Theorem 257
Applications 260-263, 272
Standard deviation of a random
variable 68
Standard normal distribution 40-41,
   45, 63
Absolute moments 93
Moments 73-76, 82-84, 93
Statistical table 621, 624-625
Tail comparison with Cauchy
distribution 47
Tail comparison with
Student’s t distribution 46-47
Statistic 281-283
Ancillary 309-313, 317-318
Complete 318-320
Information 300-308, 316-318
Minimal sufficient 294-295,
300
Lehmann-Scheffé Theorems
296
Non-sufficient 286-288, 297
Rao-Blackwellized 360-365,
372
Sufficient 282, 284
Via conditioning 284
Via information 303-304
Via Neyman factorization
288-
289
Statistical tables
Chi-square distribution 626-627
F distribution 630, 632
Standard normal distribution
621, 624-625
Student’s t distribution 628-629
Stein’s two-stage sampling; see
Two-stage sampling
Stirling’s formula 30, 59
Student’s t distribution 45, 207-209
Asymptotic distribution 264-
267
Central moments 208
Density 45
Statistical table 628-629
Tail comparison with normal
distribution 46-47
Subjectivist probability; see
   Historical notes
Sufficient statistic 282, 284
Basu’s Theorem 324-325
Completeness 318-320
Exponential family 300
Completeness 322
Distribution 300
Information 300-304
Conditional inference 316
318
Matrix 305-308
Recovery of information
316-318
Joint sufficiency 291-293
Via conditioning 284
Via information 303-304
Via Neyman factorization 288-
289
Sufficiency 282
Exponential family 300
Joint sufficiency 291-293
Minimal sufficiency 294-295,
300
Lehmann-Scheffé Theorems
296
Neyman factorization; see
Sufficient statistic
Support of a distribution 66
T
Tchebysheff’s inequality 148
Applications 148-149, 157,
174-175
t distribution; see Student’s t
distribution
Tests of hypotheses 395

664
Index
Alternative hypothesis 395
Lower-sided 417
One-sided 396, 404-406,
418,422-423
Two-sided 396, 425-428
Approximate 542
Binomial distribution 550-
552, 557
Correlation coefficient 560-
562
Poisson distribution 553-554,
560
Upper-sided 417
Variance stabilizing transfor
mations 555-563
Bayes test 493-494
Behrens-Fisher problem 534,
579
Test for means 534-535
Composite hypothesis 396
Critical function 399
Critical region 397-399
Discrete cases 410
Randomized test 410-413
Level of a test 399
Likelihood ratio (LR) test 507
Most powerful (MP) test 400
Randomized 410
Unbiasedness 428-429
Neyman-Pearson Lemma 402
Non-iid cases 416-417, 433-
434, 437, 439
Cases without parameter
413-416, 432, 434
Nonparametric; see
Distribution-free test
Null hypothesis 395
Simple 396
Paired difference t method 459,
522-525
Power 399, 431-433
Power function 399-400, 430-
431
P-value 419-420
Randomized MP test 410
Binomial distribution 410-411
Poisson distribution 411-413
Rejection region 397-399
Sample size determination 438
Confidence intervals 569-573
Two-stage procedure 573
574, 581-582
Size of a test 399
Sufficient statistic 413
Test for mean 404-406, 418,
422
Test for variance 418-419, 423
Test function 399
Type I error 396-398, 423-425,
429-430
Type II error 396-398, 429-430
Uniformly most powerful
(UMP) test 417
Binomial distribution 419
Exponential family 421-422
Karlin-Rubin Theorem 422
Lower-sided 418-419
One-sided 396, 404-406,
418,422-423
Monotone likelihood ratio
(MLR) property 420, 422-
425, 435-436
Two-sided existence 426-428
Two-sided non-existence
425-426
Upper sided 418
Uniformly most powerful
unbiased (UMPU) test 428
Transformations 177
Helmert for normal
distribution 197-199, 236, 325,
336
Analysis of variance 200
Jacobian 193-195
Not one-to-one 194, 205-206
One-to-one 192-193, 195-199,
201-205
Order statistics 201-202, 202-
203
Spacings for exponential 201-
202
Spacings for negative
exponential 202-203
Variance stabilizing 555

Index
665
Arc sine 556-557
Arc tanh 562
Square root 559-560
Triangular distribution 229
Triangular inequality 32
Applications 246-247
Two-sample problems
Comparing locations and means
456-458, 476, 534-535, 585-
587
Comparing scales and variances
460-462, 476
Two-stage sampling 569
Behrens-Fisher problem 579,
586
Bounded risk point estimation
579, 581-582
Fixed-width confidence interval
571, 573-574
Modified version 579
Second-order properties 579
Normal distribution mean
One-sample problem 573-574
Two-sample problem 585-
587
Negative exponential distribu-
tion location
One-sample problem 584-585
Two-sample problem 588-
589
U
Unbiased estimator; see Estimation
and Estimator
Unbiased test 428
Most powerful (MP) test 428-
429
Uniformly most powerful
(UMP) test 428
Uniform distribution
Continuous
Moments 73
Range 228-229
Triangular distribution 229
Discrete 37
V
Variance covariance matrix; see
Dispersion matrix
Variance of an estimator; see
Moment of a distribution
Conditional 109, 112
Variance of a random variable 67,
77
Variance stabilizing transformation
555
Arc sine 555-557
Arc tanh 562
Square root 559-560
Venn diagram of sets 4, 51
W
Weak law of large numbers
(WLLN); see Convergence
results
Weibull distribution 50, 58, 173,
329, 384
WLLN; see Convergence results


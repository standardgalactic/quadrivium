www.allitebooks.com

Professional Microsoft® SQL Server ® 2012 Administration
Published by
John Wiley & Sons, Inc.
10475 Crosspoint Boulevard
Indianapolis, IN 46256
www.wiley.com
Copyright © 2012 by John Wiley & Sons, Inc., Indianapolis, Indiana
Published simultaneously in Canada
ISBN: 978-1-118-10688-4
ISBN: 978-1-118-28684-5 (ebk)
ISBN: 978-1-118-28218-2 (ebk)
ISBN: 978-1-118-28388-2 (ebk)
Manufactured in the United States of America
10 9 8 7 6 5 4 3 2 1
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, 
electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 
of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization 
through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Rosewood Drive, Danvers, 
MA 01923, (978) 750-8400, fax (978) 646-8600. Requests to the Publisher for permission should be addressed to the 
Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, 
fax (201) 748-6008, or online at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or warranties with 
respect to the accuracy or completeness of the contents of this work and speciﬁ cally disclaim all warranties, including 
without limitation warranties of ﬁ tness for a particular purpose. No warranty may be created or extended by sales or 
promotional materials. The advice and strategies contained herein may not be suitable for every situation. This work 
is sold with the understanding that the publisher is not engaged in rendering legal, accounting, or other professional 
services. If professional assistance is required, the services of a competent professional person should be sought. Neither 
the publisher nor the author shall be liable for damages arising herefrom. The fact that an organization or Web site is 
referred to in this work as a citation and/or a potential source of further information does not mean that the author or the 
publisher endorses the information the organization or Web site may provide or recommendations it may make. Further, 
readers should be aware that Internet Web sites listed in this work may have changed or disappeared between when this 
work was written and when it is read.
For general information on our other products and services please contact our Customer Care Department within the 
United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with 
standard print versions of this book may not be included in e-books or in print-on-demand. If this book refers to media 
such as a CD or DVD that is not included in the version you purchased, you may download this material at 
http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.
Library of Congress Control Number: 2012933629
Trademarks: Wiley, the Wiley logo, Wrox, the Wrox logo, Wrox Programmer to Programmer, and related trade dress 
are trademarks or registered trademarks of John Wiley & Sons, Inc. and/or its afﬁ liates, in the United States and other 
countries, and may not be used without written permission. Microsoft and SQL Server are registered trademarks of 
Microsoft Corporation. All other trademarks are the property of their respective owners. John Wiley & Sons, Inc., is not 
associated with any product or vendor mentioned in this book.
www.allitebooks.com

CONTENTS
INTRODUCTION 
xxxvii
CHAPTER 1: SQL SERVER 2012 ARCHITECTURE 
1
SQL Server 2012 Ecosystem 
1
New Important Features in 2012  
2
Production DBA 
2
Development DBA 
2
Business Intelligence DBA and Developer 
3
SQL Server Architecture 
4
Database Files and Transaction Log 
4
SQL Native Client 
5
Standard System Databases  
6
Schemas 
8
Synonyms 
8
Dynamic Management Objects 
9
SQL Server 2012 Data Types  
10
Editions of SQL Server 
17
Edition Overview 
17
Licensing 
18
Summary 
20
CHAPTER 2: INSTALLING SQL SERVER 2012 BEST PRACTICES 
21
Planning the System 
22
Hardware Options 
22
Software and Install Options 
27
Installing SQL Server 
29
New Installs 
30
Side-by-Side Installs 
30
Upgrades 
30
Unattended Installs 
30
Attended Installations 
36
Installing Analysis Services 
40
Multidimensional and Data Mining Mode (UDM Mode) 
41
Tabular Mode 
42
Installing PowerPivot for SharePoint 
43
Burning in the System 
45
 
 
 
 
www.allitebooks.com

xx
CONTENTS
Post-Install Conﬁ guration 
45
Conﬁ guring SQL Server Settings for Performance 
46
tempdb 
47
Conﬁ guring SQL Server Setting for Security 
49
Best Practices Analyzer (BPA) 
50
SQL Server Conﬁ guration Manager 
50
Back It Up 
51
Uninstalling SQL Server 
51
Uninstalling Reporting Services 
51
Uninstalling Analysis Services 
51
Uninstalling the SQL Server Database Engine 
52
Troubleshooting a Failed Install 
52
Summary 
52
CHAPTER 3: UPGRADING SQL SERVER 2012 BEST PRACTICES 
53
Why Upgrade to SQL Server 2012? 
53
Risk Mitigation — the Microsoft Contribution 
54
Independent Software Vendors and SQL Community Contributions 
54
Upgrading to SQL Server 2012 
55
In-Place Upgrading 
55
Side-by-Side Upgrade 
57
In-Place Upgrade versus Side-By-Side Upgrade Considerations 
58
Pre-Upgrade Steps and Tools 
58
Pre-Upgrade Steps 
58
Pre-Upgrade Tools 
59
Backward Compatibility 
67
Unsupported and Discontinued Features in SQL Server 2012 
67
SQL Server 2012 Deprecated Database Features 
67
Other SQL Server 2012 Changes Af ecting Behavior 
68
SQL Server Component Considerations 
68
Upgrading Full-Text Catalog  
68
Upgrading Reporting Services 
68
Upgrading to 64-Bit 
69
Post-Upgrade Checks 
69
Poor Query Performance After Upgrade 
69
Summary 
70
CHAPTER 4:  MANAGING AND TROUBLESHOOTING 
THE DATABASE ENGINE 
71
Conﬁ guration and Administration Tools 
71
SQL Server Conﬁ guration Manager 
72
Startup Parameters 
73
www.allitebooks.com

xxi
CONTENTS
Startup Stored Procedures 
77
Partially Contained Databases 
78
Troubleshooting Tools 
79
Dedicated Administrator Connection 
79
Rebuilding the System Databases 
81
Management Studio 
82
Reports 
82
Conﬁ guring SQL Server in Management Studio 
85
Filtering Objects 
90
Error Logs 
90
Activity Monitor 
91
Monitoring Processes in T-SQL 
96
sp_who and sp_who2 
96
sys.dm_exec_connections 
97
sys.dm_exec_sql_text 
97
Multiserver Management 
98
Central Management Servers and Server Groups 
98
SQL Server Utility 
99
Trace Flags 
99
Getting Help from Support 
101
SQLDumper.exe 
101
SQLDiag.exe 
102
Summary 
104
CHAPTER 5: AUTOMATING SQL SERVER 
105
Maintenance Plans 
106
Maintenance Plan Wizard 
106
Maintenance Plan Designer 
109
Automating SQL Server with SQL Server Agent  
111
Jobs 
112
Schedules 
117
Operators 
118
Alerts 
121
SQL Server Agent Security 
126
Service Account 
126
Access to SQL Agent 
126
SQL Server Agent Proxies 
127
Conﬁ guring SQL Server Agent 
130
General Properties 
131
Advanced Properties 
132
Alert System Properties 
133
Job System Properties 
133
www.allitebooks.com

xxii
CONTENTS
Connection Properties 
134
History Properties 
134
Database Mail 
134
Architecture 
135
Security 
135
Conﬁ guration 
136
Archiving 
140
Multiserver Administration 
140
Using Token Replacement 
140
Event Forwarding 
143
Using WMI 
143
Multiserver Administration — Using Master and Target Servers 
145
Summary 
146
CHAPTER 6: SERVICE BROKER IN SQL SERVER 2012 
147
Asynchronous Messaging 
147
SQL Service Broker Overview 
148
SQL Server Service Broker Versus Other Message Queues 
148
Conﬁ guring SQL Server Service Broker 
149
Enabling  
149
Message Types 
151
Contracts 
151
Queues 
152
Services 
153
Routes 
154
Priorities 
156
Conversation Groups 
156
Using SQL Server Service Broker 
157
Sending Messages 
157
Receiving Messages 
160
Sending Messages Between Databases 
161
Sending Messages Between Instances 
162
External Activation 
163
Summary 
165
CHAPTER 7: SQL SERVER CLR INTEGRATION 
167
Introduction to the CLR 
167
SQL Server as a .NET Runtime Host 
169
Application Domains 
170
T-SQL versus CLR 
170
Enabling CLR Integration 
171
Creating CLR Assemblies 
172
The Non-Visual Studio Way 
172
www.allitebooks.com

xxiii
CONTENTS
Using Microsoft SQL Server Data Tools 
174
Securing CLR 
176
Performance Monitoring 
177
Windows System Monitor 
177
SQL Proﬁ ler 
178
Dynamic Management Views (DMVs) 
179
CLR Integration Design Goals 
180
Summary 
180
CHAPTER 8: SECURING THE DATABASE INSTANCE 
181
Authentication Types 
181
SQL Authentication 
182
Windows Authentication 
183
SQL Versus Windows Authentication 
183
Authorizing Securables 
184
Server Securables 
185
Database Securables 
189
Permission Chains 
190
Cross Database Permission Chains 
191
Row Level Security 
193
Summary 
194
CHAPTER 9: CHANGE MANAGEMENT 
197
Creating Solutions and Projects 
198
Creating a Connection 
199
Creating a Project Query 
200
Policy-Based Management 
200
Policy-Based Management Overview 
201
Policy-Based Management Step by Step 
202
Scripting Policy-Based Management 
209
Policy-Based Management Implementation 
210
DDL Trigger Syntax 
212
Database Triggers 
212
Server Triggers 
217
Trigger Views 
218
Scripting Overview 
218
sqlcmd 
219
PowerShell 
223
Creating Change Scripts 
225
Data-Tier Applications 
225
SQL Server Data Tools 
229
Version Tables 
229
Summary 
231
www.allitebooks.com

xxiv
CONTENTS
CHAPTER 10:  CONFIGURING THE SERVER FOR OPTIMAL 
PERFORMANCE 
233
What Every DBA Needs to Know About Performance 
234
The Performance Tuning Cycle 
234
Deﬁ ning Good Performance 
235
Focus on What’s Most Important 
236
What the Developer DBA Needs to Know About Performance 
237
Users 
237
SQL Statements 
237
Data Usage Patterns 
238
Robust Schema 
238
What the Production DBA Needs to Know About Performance 
238
Optimizing the Server 
239
Hardware Management 
241
CPU 
241
x64 
242
Cache 
242
Hyper-threading 
243
Multicore 
244
System Architecture 
246
Memory 
248
Physical Memory 
248
Physical Address Space 
248
Virtual Memory Manager 
249
The Page File 
249
Page Faults 
250
I/O 
251
Network 
252
Disks 
252
Storage Considerations  
255
Designing a Storage System 
257
Large Storage System Considerations: SAN Systems 
262
Server Conﬁ guration 
264
Fragmentation 
269
Summary 
271
CHAPTER 11: OPTIMIZING SQL SERVER 2012 
273
Application Optimization 
273
Deﬁ ning a Workload 
274
System Harmony Is the Goal 
274
The Silent Killer: I/O Problems 
274
www.allitebooks.com

xxv
CONTENTS
SQL Server I/O Process Model 
275
Database File Placement 
275
tempdb Considerations 
276
Table and Index Partitioning 
279
Why Consider Partitioning? 
280
Creating a Partition Function 
281
Creating Filegroups 
284
Creating a Partition Scheme 
284
Creating Tables and Indexes 
285
Data Compression 
290
Row Compression 
290
Page Compression 
291
Estimating Space Savings 
293
Monitoring Data Compression 
295
Data Compression Considerations 
295
CPU Considerations 
296
Cache Coherency 
297
Ai  nity Mask 
297
Max Degree of Parallelism (MAXDOP) 
300
Ai  nity I/O Mask 
301
Memory Considerations and Enhancements 
302
Tuning SQL Server Memory 
302
64-bit Versions of SQL Server 2012 
305
Data Locality 
306
Max Server Memory 
307
Index Creation Memory Option 
307
Minimum Memory per Query 
308
Resource Governor 
309
The Basic Elements of Resource Governor 
309
Using Resource Governor from SQL Server 2012 
Management Studio 
313
Monitoring Resource Governor 
314
Summary 
315
CHAPTER 12: MONITORING YOUR SQL SERVER 
317
The Goal of Monitoring 
318
Determining Your Monitoring Objectives 
318
Establishing a Baseline 
318
Comparing Current Metrics to the Baseline 
319
Choosing the Appropriate Monitoring Tools 
319
Performance Monitor 
321
CPU Resource Counters 
322
www.allitebooks.com

xxvi
CONTENTS
Disk Activity 
324
Memory Usage 
330
Performance Monitoring Tools 
333
Monitoring Events 
335
The Default Trace 
337
system_health Session 
338
SQL Trace 
338
Event Notiﬁ cations 
352
SQL Server Extended Events 
355
Monitoring with Dynamic Management Views and Functions 
376
What’s Going on Inside SQL Server? 
377
Viewing the Locking Information 
380
Viewing Blocking Information 
380
Index Usage in a Database 
381
Indexes Not Used in a Database 
382
View Queries Waiting for Memory Grants 
383
Connected User Information 
384
Filegroup Free Space 
384
Query Plan and Query Text for Currently Running Queries 
385
Memory Usage 
385
Buf er Pool Memory Usage 
385
Monitoring Logs 
386
Monitoring the SQL Server Error Log 
386
Monitoring the Windows Event Logs 
387
Management Data Warehouse 
387
System Data Collection Sets 
388
Viewing Data Collected by the System Data Collection Sets 
388
Creating Your Own Data Collection Set 
390
Examining the Data You Collected 
392
SQL Server Standard Reports 
393
System Center Management Pack 
395
SQL Server Best Practice Analyzer 
396
System Center Advisor 
396
Summary 
397
CHAPTER 13: PERFORMANCE TUNING T-SQL 
399
Physical Query Processing Part One: Compilation 
and Recompilation 
399
Compilation 
400
Recompilation 
401
Tools and Commands for Recompilation Scenarios 
408
www.allitebooks.com

xxvii
CONTENTS
Parser and Algebrizer 
410
Optimization 
412
Physical Query Processing Part Two: Execution 
417
Database I/O Information 
418
Working with the Query Plan 
419
Estimated Execution Plan 
420
Actual Execution Plan 
424
Index Access Methods 
427
Fragmentation 
438
Statistics 
439
Join Algorithms 
440
Data Modiﬁ cation Query Plan 
443
Query Processing Enhancements on Partitioned Tables and Indexes 
444
Gathering Query Plans for Analysis with SQL Trace 
446
Summary 
447
CHAPTER 14: INDEXING YOUR DATABASE 
449
Noteworthy Index-Related Features in SQL Server 
449
What’s New for Indexes in SQL Server 2012 
450
Index Features from SQL Server 2008R2, SQL Server 2008, 
and SQL Server 2005  
452
Partitioned Tables and Indexes 
455
Understanding Indexes 
455
Creating Indexes 
458
Why Use Both Partitioned Tables and Indexes? 
459
Creating Partitioned Tables 
460
Index Maintenance 
461
Monitoring Index Fragmentation 
462
Cleaning Up Indexes 
462
Improving Query Performance with Indexes  
464
Database Tuning Advisor 
468
Too Many Indexes? 
469
Summary 
471
CHAPTER 15: REPLICATION 
473
Replication Overview 
473
Replication Components 
474
Replication Types 
476
Replication Enhancements in SQL Server 2012 
478
Replication Models 
478
Single Publisher, One or More Subscribers 
478

xxviii
CONTENTS
Multiple Publishers, Single Subscriber 
480
Multiple Publishers Also Subscribing 
481
Updating Subscriber 
482
Peer-to-Peer 
483
Implementing Replication 
484
Setting Up Snapshot Replication 
484
Setting Up Distribution 
484
Implementing Snapshot Replication 
487
Implementing Transactional and Merge Replication 
497
Peer-to-Peer Replication 
498
Setting Up Peer-to-Peer Replication  
498
Conﬁ guring Peer-to-Peer Replication 
499
Scripting Replication 
502
Monitoring Replication 
502
Replication Monitor 
502
Performance Monitor 
505
Replication DMVs 
505
sp_replcounters 
506
Summary 
507
CHAPTER 16: CLUSTERING SQL SERVER 2012 
509
Clustering and Your Organization 
510
What Clustering Can Do 
510
What Clustering Cannot Do 
511
Choosing SQL Server 2012 Clustering for the Right Reasons 
512
Alternatives to Clustering 
512
Clustering: The Big Picture 
514
How Clustering Works 
515
Clustering Options 
518
Upgrading SQL Server Clustering 
520
Don’t Upgrade 
520
Upgrading Your SQL Server 2012 Cluster In Place 
520
Rebuilding Your Cluster  
521
Back-Out Plan 
523
Which Upgrade Option Is Best? 
523
Getting Prepared for Clustering 
523
Preparing the Infrastructure 
523
Preparing the Hardware 
524
Clustering Windows Server 2008 
527
Before Installing Windows 2011 Clustering 
527
Installing Windows Server 2008 Failover Clustering 
528
Preparing Windows Server 2008 for Clustering 
531

xxix
CONTENTS
Clustering Microsoft Distributed Transaction Coordinator 
532
Clustering SQL Server 2012 
534
Step by Step to Cluster SQL Server 
534
Installing the Service Pack and Cumulative Updates 
540
Test, Test, and Test Again 
540
Managing and Monitoring the Cluster 
542
Troubleshooting Cluster Problems 
543
How to Approach Windows Failover Clustering 
Troubleshooting 
544
Doing It Right the First Time 
544
Gathering Information 
544
Resolving Problems 
545
Working with Microsoft 
545
Summary 
546
CHAPTER 17: BACKUP AND RECOVERY 
547
Types of Failure 
548
Hardware Failure 
548
Data Modiﬁ cation Failure 
548
Software Failure 
550
Local Disasters 
550
Making Plans 
551
Backup/Recovery Plan 
551
Disaster Recovery Planning 
554
Creating the Disaster Recovery Plan 
556
Maintaining the Plan 
558
Overview of Backup and Restore 
559
How Backup Works 
559
Copying Databases 
562
Backup Compression 
570
Comparing Recovery Models 
571
Choosing a Model 
573
Switching Recovery Models 
574
Backing Up History Tables 
575
Permissions Required for Backup and Restore 
576
Backing Up System Databases 
577
Full-Text Backup 
578
Verifying the Backup Images 
578
How Restore Works 
579
Preparing for Recovery 
581
Recoverability Requirements 
581
Data Usage Patterns 
582

xxx
CONTENTS
Maintenance Time Window 
583
Other High-Availability Solutions 
584
Developing and Executing a Backup Plan 
585
Using SQL Server Management Studio 
585
Database Maintenance Plans 
589
Using Transact-SQL Backup Commands 
591
Managing Backups 
593
Backup and Restore Performance 
594
Performing Recovery 
594
Restore Process 
594
SQL Server Management Studio Restore 
599
T-SQL Restore Command 
602
Restoring System Databases 
602
Archiving Data 
604
SQL Server Table Partitioning 
604
Partitioned View 
605
Summary 
606
CHAPTER 18: SQL SERVER 2012 LOG SHIPPING 
607
Log Shipping Deployment Scenarios 
608
Log Shipping to Create a Warm Standby Server 
608
Log Shipping as a Disaster Recovery Solution 
609
Log Shipping as a Report Database Solution 
610
Log-Shipping Architecture 
611
Primary Server 
611
Secondary Server 
611
Monitor Server 
612
Log Shipping Process 
612
System Requirements 
613
Network 
613
Identical Capacity Servers 
613
Storage 
614
Software 
614
Deploying Log Shipping 
614
Initial Conﬁ guration 
614
Deploying with Management Studio 
616
Deploying with T-SQL Commands 
624
Monitoring and Troubleshooting 
624
Monitoring with Management Studio 
625
Monitoring with Stored Procedures 
626
Troubleshooting Approach 
626

xxxi
CONTENTS
Managing Changing Roles 
627
Synchronizing Dependencies 
627
Switching Roles from the Primary to Secondary Servers 
630
Switching Between Primary and Secondary Servers 
632
Redirecting Clients to Connect to the Secondary Server 
632
Database Backup Plan 
633
Integrating Log Shipping with Other High-Availability Solutions 
634
SQL Server 2012 Data Mirroring 
634
Windows Failover Clustering 
635
SQL Server 2012 Replication 
635
Removing Log Shipping 
636
Removing Log Shipping with Management Studio 
636
Removing Log Shipping with T-SQL Commands 
636
Log-Shipping Performance 
637
Upgrading to SQL Server 2012 Log Shipping 
638
Minimum Downtime Approach 
638
With Downtime Approach 
638
Deploy Log Shipping Approach 
639
Summary 
639
CHAPTER 19: DATABASE MIRRORING 
641
Overview of Database Mirroring 
641
Operating Modes of Database Mirroring 
643
Database Mirroring In Action 
645
Preparing the Endpoints 
646
Preparing the Database for Mirroring 
652
Initial Synchronization Between Principal and Mirror 
653
Establishing the Mirroring Session 
653
High-Safety Operating Mode Without Automatic Failover 
655
High-Safety Operating Mode with Automatic Failover 
655
High-Performance Operating Mode 
657
Database Mirroring and SQL Server 2012 Editions 
658
Database Mirroring Catalog Views 
658
sys.database_mirroring 
658
sys.database_mirroring_witnesses 
660
sys.database_mirroring_endpoints 
660
Database Mirroring Role Change 
661
Automatic Failover 
661
Manual Failover 
664
Forced Failover 
666
Database Availability Scenarios 
667
Principal Is Lost 
667
 
 
 
 

xxxii
CONTENTS
Mirror Is Lost 
668
Witness Is Lost 
669
Mirror and Witness Are Lost 
669
Monitoring Database Mirroring 
670
Monitoring Using System Monitor 
670
Monitoring Using Database Mirroring Monitor 
672
Setting Thresholds on Counters and Sending Alerts 
676
Troubleshooting Database Mirroring 
678
Troubleshooting Setup Errors 
678
Troubleshooting Runtime Errors 
679
Automatic Page Repair 
680
Preparing the Mirror Server for Failover 
681
Hardware, Software, and Server Conﬁ guration 
681
Database Availability During Planned Downtime 
682
SQL Job Conﬁ guration on the Mirror 
684
Database TRUSTWORTHY Bit on the Mirror 
684
Client Redirection to the Mirror 
684
Mirroring Multiple Databases 
685
Database Mirroring and Other High-Availability Solutions 
686
Database Mirroring versus Clustering 
687
Database Mirroring versus Transactional Replication 
687
Database Mirroring versus Log Shipping 
687
Database Mirroring Versus Availability Groups 
688
Mirroring Event Listener Setup 
688
Database Snapshots 
692
Summary 
693
CHAPTER 20:  INTEGRATION SERVICES ADMINISTRATION 
AND PERFORMANCE TUNING 
695
A Tour of Integration Services 
696
Integration Services Uses 
696
The Main Parts of Integration Services 
697
Project Management and Change Control 
699
Administration of the Integration Services Service 
699
An Overview of the Integration Services Service 
700
Conﬁ guration 
700
Event Logs 
704
Monitoring Activity 
705
Administration of Integration Services Packages in 
Package Deployment Model 
706
Using Management Studio for Package Management 
706
Deployment 
709

xxxiii
CONTENTS
Administration of Integration Services Packages 
in Project Deployment Model 
712
Conﬁ guring the SSIS Catalog 
712
Deploying Packages 
714
Conﬁ guring Packages 
716
Execution and Scheduling 
719
Running Packages in SQL Server Data Tools 
720
Running Packages with the SQL Server Import 
and Export Wizard 
720
Running Packages with DTExec 
720
Running Packages with DTExecUI (Package 
Deployment Model) 
721
Running Packages with the Execute Package Tool 
(Project Deployment Model) 
722
Scheduling Execution with SQL Server Agent 
723
Running Packages with T-SQL 
725
Applying Security to Integration Services 
725
An Overview of Integration Services Security 
725
Securing Packages in Package Deployment Model 
726
Summary 
728
CHAPTER 21:  ANALYSIS SERVICES ADMINISTRATION 
AND PERFORMANCE TUNING 
729
Tour of Analysis Services 
730
MOLAP Components 
731
Tabular Model Components 
732
Analysis Services Architectural Components 
732
Administering Analysis Services Server 
733
Server Properties 
734
Required Services 
735
Analysis Services Scripting Language 
736
Administering Analysis Services Databases 
737
Deploying Analysis Services Databases 
737
Processing Analysis Services Objects 
741
Backing Up and Restoring Analysis Services Databases 
745
Synchronizing Analysis Services Databases 
748
Analysis Services Performance Monitoring and Tuning 
749
Monitoring Analysis Services Events 
749
Creating Traces for Replay 
750
Using Flight Recorder for After-the-Fact Analysis 
751
Management of Analysis Services MOLAP Model Storage 
752
Storage Modes 
752

xxxiv
CONTENTS
Partition Conﬁ guration 
753
Designing Aggregations in the MOLAP Model 
755
Applying Security to Analysis Services in the MOLAP Model 
758
Server Role 
758
Database Role 
758
Database Role Permissions 
760
Applying Security to Analysis Services in the Tabular Model 
762
Summary 
763
CHAPTER 22:  SQL SERVER REPORTING SERVICES 
ADMINISTRATION 
  765
SQL Server Reporting Services Conﬁ guration Manager 
765
The Service Account 
768
The Web Service URL 
769
Reporting Services Databases 
771
The Report Manager URL 
773
E-mail Settings 
774
Execution Account 
775
Encryption Keys 
776
Scale-out Deployment 
777
Report Server Properties 
778
General Properties Page 
779
Execution Properties Page 
780
History Properties Page 
781
Logging Properties Page 
781
Security Properties Page 
782
Advanced Properties Page 
783
The Report Execution Log 
784
Report Builder 
786
Report Manager 
795
Managing Report Manager 
796
Managing Reports 
803
Summary 
814
CHAPTER 23: SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION 
815
Components of Integration 
815
PowerPivot  
816
Reporting Services 
818
Power View 
819
Service Application Architecture 
820
Data Refresh 
820

xxxv
CONTENTS
Using Data Connections in Excel 
820
PerformancePoint Data Refresh 
826
Visio Services Data Refresh 
827
PowerPivot Data Refresh 
829
Summary 
836
CHAPTER 24:  SQL AZURE ADMINISTRATION 
AND CONFIGURATION 
837
Introduction to SQL Azure 
837
SQL Azure Architecture 
838
Client Layer 
838
Services Layer 
838
Platform Layer 
839
Infrastructure Layer 
839
Conﬁ guring SQL Azure 
839
Server and Database Provisioning 
840
Throttling and Load Balancing  
844
Conﬁ guring SQL Azure Firewalls 
845
Connecting to SQL Azure 
847
Administering SQL Azure 
848
Creating Logins and Users 
848
Assigning Access Rights 
850
Working with SQL Azure 
850
Backups with SQL Azure 
852
Object Explorer for SQL Azure 
852
What’s Missing in SQL Azure 
854
Summary 
855
CHAPTER 25: ALWAYSON AVAILABILITY GROUPS 
857
Architecture 
858
Availability Group Replicas and Roles 
858
Availability Modes 
859
Types of Failover Supported 
859
Allowing Read-Only Access to Secondary Replicas 
860
Availability Group Example 
862
Conﬁ gure a New Availability Group 
862
Conﬁ gure an Existing Availability Group 
870
Availability Group Failover Operation 
872
Suspend an Availability Database 
873
Resume an Availability Database  
874
Client Application Connections 
874

xxxvi
CONTENTS
Active Secondary for Secondary Read-Only 
875
Read-Only Access Behavior 
876
Secondary Replica Client Connectivity 
876
Performance 
878
Backup on the Secondary Replica 
879
Evaluate Backup Replicas Metadata 
880
AlwaysOn Group Dashboard 
881
Monitoring and Troubleshooting 
883
Summary 
884
INDEX 
885
www.allitebooks.com

INTRODUCTION
SQL SERVER 2012 REPRESENTS A SIZABLE jump forward in scalability, performance, and usability 
for the DBA, developer, and business intelligence (BI) developer. It is no longer unheard of to have 
40-terabyte databases running on a SQL Server. SQL Server administration used to just be the job 
of a database administrator (DBA), but as SQL Server proliferates throughout smaller companies, 
many developers have begun to act as administrators and BI developers as well. In addition, some of 
the new features in SQL Server are more developer-centric, and poor conﬁ guration of these features 
can result in poor performance. SQL Server now enables all roles through signiﬁ cantly improved 
data tools experiences, better security integration, and drastic improvements in data integration, 
administration, availability, and usability. Professional Microsoft SQL Server2012 Administration 
is a comprehensive, tutorial-based book to get you over the learning curve of how to conﬁ gure and 
administer SQL Server 2012.
WHO THIS BOOK IS FOR
Whether you’re an administrator or developer using SQL Server, you can’t avoid wearing a DBA 
hat at some point. Developers often have SQL Server on their own workstations and must provide 
guidance to the administrator about how they’d like the production conﬁ gured. Oftentimes, they’re 
responsible for creating the database tables and indexes. Administrators or DBAs support the 
production servers and often inherit the database from the developer.
This book is intended for developers, DBAs, and casual users who hope to administer or may 
already be administering a SQL Server 2012 system and its business intelligence features, such as 
Integration Services. This book is a professional book, meaning the authors assume that you know 
the basics about how to query a SQL Server and have some rudimentary concepts of SQL Server. 
For example, this book does not show you how to create a database or walk you through the 
installation of SQL Server using the wizard. Instead, the author of the installation chapter provides 
insight into how to use some of the more advanced concepts of the installation. Although this book 
does not cover how to query a SQL Server database, it does cover how to tune the queries you’ve 
already written.
HOW THIS BOOK IS STRUCTURED
This book follows the same basic path of previous editions, with one major change. The author team 
has been selected speciﬁ cally to focus on their areas of expertise. The authors are the same people 
seen at major conferences and delivering top-tier services for topics such as performance tuning, 
business intelligence, database design, high availability, PowerShell, and even SQL Azure! This 
approach has led to unprecedented focus on quality and content with even better access to folks at 
Microsoft to drive the content in this new release of SQL Server. Hundreds of Connect items were 

xxxviii
INTRODUCTION
ﬁ led and resolved as a direct result of the work of this author team pushing for higher quality for 
you. Connect is the primary method for industry professionals and SQL Server MVPs to provide bug 
reports and vote on feature requests from Microsoft. It’s a great outlet for improving the product.
This edition of the book covers all the same great information covered in the previous edition, but 
with loads of new content added for SQL Server 2012, which includes numerous new features to 
improve the DBA’s life. In short, the new version of SQL Server focuses on improving your efﬁ ciency, 
the scale of your server, and the performance of your environment, so you can do more in much less 
time, and with fewer resources and people. The following is a brief description of each chapter. 
Chapter 1: SQL Server 2012 Architecture — The book starts off with a review of the new 
architecture changes and focuses on the overall components that make up SQL Server 2012.
Chapter 2: Installing SQL Server 2012 Best Practices — This chapter reviews the different 
ways to install SQL Server 2012 and covers best practices for the process. 
Chapter 3: Upgrading SQL Server 2012 Best Practices — This chapter covers upgrading to 
SQL Server 2012 and best practices to keep in mind while upgrading. Choosing the best 
upgrade method, requirements, and beneﬁ ts of upgrading are also covered. 
Chapter 4: Managing and Troubleshooting the Database Engine — This chapter focuses 
on the database engine and working through challenges as they arise. It also covers 
management and tools appropriate for the task.
Chapter 5: Automating SQL Server — This chapter focuses on automation throughout the 
SQL Server 2012 world including jobs, PowerShell, and other ways to automate. 
Chapter 6: Service Broker in SQL Server 2012 — Service Broker is a great tool to handle 
messaging inside the database. This chapter covers setup, operations, and management of 
Service Broker.
Chapter 7: SQL Server CLR Integration — SQL Server and .NET work together inside the 
Common Language Runtime. This chapter focuses on integrating .NET and the CLR with 
SQL Server, including assemblies and other options. 
Chapter 8: Securing the Database Instance — Security is critical in the database engine. 
This chapter helps you outline and implement your security plan.
Chapter 9: Change Management — Managing change is paramount to operational stability. 
This chapter focuses on features in SQL Server that support change management. 
Chapter 10: Conﬁ guring the Server for Optimal Performance — Conﬁ guring and setting 
up your server properly is important for maximizing application and database performance. 
This chapter discusses storage, server options, and other settings critical to system 
performance.
Chapter 11: Optimizing SQL Server 2012 — This chapter covers topics that help the reader 
review and analyze performance. It also focuses on settings and conﬁ guration items that 
improve SQL Server performance. 
Chapter 12: Monitoring Your SQL Server — SQL Server is critically important to make 
sure you keep performance where it needs to be. This chapter covers the important aspects 
and tools used to monitor SQL Server 2012.

INTRODUCTION
xxxix
Chapter 13: Performance Tuning T-SQL — Writing efﬁ cient and effective T-SQL is 
important to have good application performance and scalability. This chapter explains how 
to optimize your T-SQL to make it more efﬁ cient. It focuses on how SQL Server’s engine 
and internals read and execute your queries. You then learn how to take advantage of areas 
where this process can be tweaked and best practices can be leveraged.
Chapter 14: Indexing Your Database — Indexing is critical to successful database 
performance. This chapter discusses considerations and strategies for effective indexing for 
your database. 
Chapter 15: Replication — Replication is a key feature in SQL Server for keeping tables 
and databases in sync and supporting applications. This chapter will cover the types of 
replication, how to set them up, and the pros and cons of each.
Chapter 16: Clustering SQL Server 2012 — Clustering has been improved again in SQL 
2012 and this chapter takes the reader through the setup, conﬁ guration, and testing of your 
clustered conﬁ guration. 
Chapter 17: Backup and Recovery — Backup and recovery is critical to the success of a 
continuity plan and operational achievement. This chapter outlines the options in SQL 
Server for backups and recoveries, and provides recommendations to make the most of these 
features. 
Chapter 18: SQL Server 2012 Log Shipping — This chapter goes through setup, 
conﬁ guration, and administration of log shipping.
Chapter 19: Database Mirroring — There is more functionality in this release for 
availability than ever before. This chapter covers new and existing features to help you keep 
your systems online for your organization.
Chapter 20: Integration Services Administration and Performance Tuning — Integration 
is the key to making sure systems stay in sync. This chapter focuses on administering and 
tuning this great feature in SQL Server.
Chapter 21: Analysis Services Administration and Performance Tuning — Analysis Services 
is the Online Analytical Processing (OLAP) product of choice and cannot be ignored by 
data administrators. This chapter helps you get prepared.
Chapter 22: SQL Server Reporting Services Administration — Reporting Services is often 
administered by the DBA and this book prepares you no matter what your role to handle 
those Reporting Services challenges.
Chapter 23: SQL Server 2012 SharePoint 2010 Integration — SharePoint is a bigger part of 
SQL Server than ever. This chapter covers what you need to know about how SharePoint 
2010 integrates with SQL Server so you can be prepared to interact with that team or take 
on some SharePoint database administration responsibilities yourself.
Chapter 24: SQL Azure Administration and Conﬁ guration — This chapter introduces the 
reader to SQL Server Azure and gets you up and running on this exciting new cloud platform. 
Chapter 25: AlwaysOn Availability Groups — This chapter focuses on the availability 
group feature in Always On. These groups allow you to control instances and servers 
as groups and assign prioritization and additional ﬂ exibility to how failover and high 
availability are handled in your environment.





2  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
PowerPivot for SharePoint enable users to be closer to the data at all times and to seek and 
deliver intelligence more rapidly than ever. 
Integration and collaboration: New integrations for reporting services, PowerPivot, and 
claims authentication in SharePoint 2010 provide a strong foundation for the signiﬁ cant focus 
on self-service in this release. The new BI semantic model approach extends into the cloud as 
well with reporting services now in SQL Azure and more features promised to come.
NEW IMPORTANT FEATURES IN 2012 
There are a number of new things that you will be excited about, depending on your role and 
how you use SQL Server. This section touches on the features you should be checking out and 
getting your hands on. Many of these features are quick to get up and running, which is exciting 
for those readers who want to begin delivering impact right away. 
Production DBA
Production DBAs are a company’s insurance policy that the production database won’t go down. 
If the database does go down, the company cashes in its insurance policy in exchange for a 
recovered database. The Production DBA also ensures that the server performs optimally and 
promotes database changes from development to quality assurance (QA) to production. New 
features include the following:
AlwaysOn: Availability functionality including availability groups and the ability to ﬁ le 
over databases in groups that mimic applications. This includes new readable secondary 
servers, a big enhancement.
FileTable: Additional ﬁ le-based data storage
Extended Events: A new functionality built into SQL Server 2012 that provides lightweight 
and extensive tracing capability
Improved functionality and stability in SQL Server Management Studio (now in Visual 
Studio 2010 shell)
Distributed replay capabilities
Improved debugging functionality including expression support and breakpoint validation. 
Columnstore indexes for optimizing large data volumes
Improved statistics algorithm for very large databases
Improved compression and partitioning capabilities
Development DBA
Since the release of SQL Server 2000, there has been a trend away from full-time Production DBAs, 
and the role has merged with that of the Development DBA. The trend may have slowed, though, with 
laws such as Sarbanes-Oxley, which require a separation of power between the person developing 
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

the change and the person implementing the change. In a large organization, a Production DBA may 
fall into the operations department, which consists of the network of administrators and Windows-
support administrators. In other instances, a Production DBA may be placed in a development group. 
This removes the separation of power that is sometimes needed for regulatory reasons. 
Development DBAs play a traditional role in an organization. They wear more of a developer’s hat 
and are the development staff’s database experts and representatives. This administrator ensures 
that all stored procedures are optimally written and that the database is modeled correctly, both 
physically and logically. The development DBA also may be the person who writes the migration 
processes to upgrade the database from one release to the next. The Development DBA typically does 
not receive calls at 2:00 A.M like the Production DBA might for failed backups or similar problems. 
Things development DBAs should be excited about in this new release include the following: 
New TSQL and spatial functionality
SQL Server data tools: A new TSQL development environment integrated with Visual 
Studio 
New DAX expression language that provides Excel-like usability with the power of 
multidimensional capabilities
New tabular model for Analysis Services: Provides in-memory OLAP capabilities in a quick 
time to value format
The Development DBA typically reports to the development group and receives requests from a 
business analyst or another developer. In a traditional sense, Development DBAs should never have 
modiﬁ cation access to a production database. They should, however, have read-only access to the 
production database to debug in a time of escalation.
Business Intelligence DBA and Developer
The Business Intelligence (BI) DBA is a new role that has evolved due to the increased capabilities 
of SQL Server. In SQL Server 2012, BI grew to be an incredibly important feature set that many 
businesses could not live without. The BI DBA or developer is an expert at these features. This release is 
a treasure trove of new BI functionality including new enhancements to Reporting Services Integration, 
data exploration tools such as Power View, and a dramatic set of enhancements that make PowerPivot 
easier and more accessible than ever. Additionally, the new Tabular model in SSAS delivers the ability to 
create new PowerPivot-like “in memory” BI projects to SharePoint for mass user consumption. 
Development BI DBAs specialize in the best practices, optimization, and use of the BI toolset. In 
a small organization, a Development BI DBA may create your SSIS packages to perform Extract 
Transform and Load (ETL) processes or reports for users. In a large organization, developers create 
the SSIS packages and SSRS reports. The Development BI DBA is consulted regarding the physical 
implementation of the SSIS packages and Analysis Services (SSAS) cubes. Development BI DBAs 
may be responsible for the following types of functions: 
Model\consult regarding Analysis Services cubes and solutions
Create reports using Reporting Services
Create\consult around ETL using Integration Services
Develop deployment packages to be sent to the Production DBA
➤
➤
➤
➤
➤
➤
➤
➤
New Important Features in 2012  ❘  3

4  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
These responsibilities, coupled with these following new features make for an exciting time for the 
BI-oriented folks:
Rapid data discovery with Power View and PowerPivot
Managed Self-Service BI with SharePoint and BI Semantic Model
Credible, consistent data with Data Quality Services and Master Data Management 
capabilities
Robust DW solutions with Parallel Data Warehouse and Reference Architectures
SQL SERVER ARCHITECTURE
Many people just use SQL Server for its classic use: to store data. This release of SQL Server focuses 
on expanding the capabilities that were introduced in SQL Server 2008 R2, which was largely a 
self-service business intelligence and SharePoint feature release. The additional functionality in 
SQL Server 2012 not only enables but encourages users to go beyond simply storing data in SQL 
Server; this release can now be the center of an entire data strategy. New tools such as Power View 
and PowerPivot quickly integrate on top of SQL Server and can provide an easy user interface (UI) 
for SQL Server and other systems’ data. This section covers the primary ﬁ le types in SQL Server 
2012, ﬁ le management, SQL Client, and system databases. It also covers an overview of schemas, 
synonyms, and Dynamic Management Objects. Finally, it also goes into the new SQL Server 2012 
data types.
Database Files and Transaction Log
The architecture of database and transaction log ﬁ les remains relatively unchanged from prior 
releases. Database ﬁ les serve two primary purposes depending on their type. Data ﬁ les hold the 
data, indexes, and other data support structure within the database. Log ﬁ les hold the data from 
committed transactions to ensure consistency in the database. 
Database Files
A database may consist of multiple ﬁ legroups. Each ﬁ legroup must contain one or more physical data 
ﬁ les. Filegroups ease administrative tasks for a collection of ﬁ les. Data ﬁ les are divided into 8KB 
data pages, which are part of 64KB extents. You can specify how full each data page should be with 
the ﬁ ll factor option of the create/alter index T-SQL command. In SQL Server 2012 Enterprise 
Edition, you continue to have the capability to bring your database partially online if a single ﬁ le is 
corrupt. In this instance, the DBA can bring the remaining ﬁ les online for reading and writing, and 
users receive an error if they try to access the other parts of the database that are ofﬂ ine. 
In SQL 2000 and before, the largest row you could write was 8060 bytes. The exceptions to this limit 
are text, ntext, image, varchar(max), varbinary(max), and nvarchar(max) columns, which 
may each be up to 2 gigabytes and are managed separately. Beginning with SQL 2005, the 8KB limit 
applies only to those columns of ﬁ xed length. The sum of ﬁ xed-length columns and pointers for other 
column types must still be less than 8060 bytes per row. However, each variable-length column may 
be up to 8KB in size allowing for a total row size of well over 8060 bytes. If your actual row size 
➤
➤
➤
➤
www.allitebooks.com

exceeds 8060 bytes, you may experience some performance degradation because the logical row must 
now be split across multiple physical 8060-byte rows.
Transaction Log
The purpose of the transaction log is to ensure that all committed transactions are persisted in the 
database and can be recovered, either through rollback or point in time recovery. The transaction 
log is a write-ahead log. As you make changes to a database in SQL Server, the data is written to 
the log, and then the pages that need to be changed are loaded into memory (speciﬁ cally into the 
write buffer portion of the buffer pool). The pages are then dirtied by having the changes written 
to them. Upon checkpoint, the dirty pages are written to disk, making then now clean pages which 
no longer need to be part of the write buffer. This is why you may see your transaction log grow 
signiﬁ cantly in the middle of a long-running transaction even if your recovery model is set to simple. 
(Chapter 17, “Backup and Recovery” covers this in much more detail.)
SQL Native Client
The SQL Native Client is a data-access method that shipped with SQL Server 2005 and was 
enhanced in 2012 and is used by both OLE DB and ODBC for accessing SQL Server. The SQL 
Native Client simpliﬁ es access to SQL Server by combining the OLE DB and ODBC libraries into a 
single access method. The access type exposes these features in SQL Server: 
Database mirroring
Always On readable secondary routing
Multiple Active Result Sets (MARS)
Snapshot isolation
Query notiﬁ cation
XML data type support
User-deﬁ ned data types (UDTs)
Encryption
Performing asynchronous operations
Using large value types
Performing bulk copy operations
Table-value parameters
Large CLR user-deﬁ ned types
Password expiration
In these features, you can use the feature in other data layers such as Microsoft Data Access 
Components (MDAC), but it takes more work. MDAC still exists, and you can use it if you don’t 
need some of the new functionality of SQL Server 2008\2012. If you develop a COM-based 
application, you should use SQL Native Client; and if you develop a managed code application 
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
SQL Server Architecture ❘ 5



8  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
model Database
model is a system database that serves as a template when SQL Server creates a new database. As 
each database is created, SQL Server copies the model database as the new database. The only time 
this does not apply is when you restore or attach a database from a different server. 
If a table, stored procedure, or database option should be included in each new database that you 
create on a server, you may simplify the process by creating the object in model. When the new 
database is created, model is copied as the new database, including the special objects or database 
settings you have added to the model database. If you add your own objects to model, it should be 
included in your backups, or you should maintain a script that includes the changes.
msdb Database
msdb is a system database that contains information used by SQL Server agent, log shipping, SSIS, 
and the backup and restore system for the relational database engine. The database stores all the 
information about jobs, operators, alerts, and job history. Because it contains this important system-
level data, you should back up this database regularly.
Schemas
Schemas enable you to group database objects together. You may want to do this for ease of 
administration because you can apply security to all objects within a schema. Another reason to 
use schemas is to organize objects so that the consumers may ﬁ nd the objects they need easily. For 
example, you may create a schema called HumanResource and place all your employee tables and 
stored procedures into it. You could then apply security policies on the schema to allow appropriate 
access to the objects contained within it.
When you refer to an object, you should always use the two-part name. The dbo schema is the default 
schema for a database. An Employee table in the dbo schema is referred to as dbo.Employee. Table names 
must be unique within a schema. You could create another table called Employee in the HumanResources 
schema. It would be referred to as HumanResources.Employee. This table actually exists in the 
AdventureWorks sample database for SQL Server 2012. (All SQL Server 2012 samples must be 
downloaded and installed separately from wrox.com.) A sample query using the two-part name follows: 
SELECT BusinessEntityID, JobTitle
FROM HumanResources.Employee
Prior to SQL 2005, the ﬁ rst part of the two-part name was the user name of the object owner. The 
problem with that implementation was related to maintenance. If a user who owned objects were 
to leave the company, you could not remove that user login from SQL Server until you ensured that 
all the objects owned by the user were changed to a different owner. All the code that referred to 
the objects had to be changed to refer to the new owner. By separating ownership from the schema 
name, SQL 2005 through 2012 removes this maintenance problem.
Synonyms
A synonym is an alias, or alternative name, for an object. This creates an abstraction layer between 
the database object and the consumer. This abstraction layer enables you to change some of the 
physical implementation and isolate those changes from the consumer. The following example is 


10  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
sys.dm_io_virtual_file_stats(DB_ID(‘AdventureWorks’), 
FILE_ID(‘AdventureWorks_Data’))
Many new DMV’s and DMF’s exist in SQL Server 2012. These views focus on improved insight 
into new and existing areas of functionality and include the following:
AlwaysOn Availability Groups Dynamic Management Views and Functions  
Change Data Capture Related Dynamic Management Views  
Change Tracking Related Dynamic Management Views 
Common Language Runtime Related Dynamic Management Views  
Database Mirroring Related Dynamic Management Views  
Database-Related Dynamic Management Views  
Execution-Related Dynamic Management Views and Functions  
SQL Server Extended Events Dynamic Management Views 
FileStream and FileTable Dynamic Management Views  
Full-Text Search and Semantic Search Dynamic Management Views and Functions  
Index-Related Dynamic Management Views and Functions  
I/O-Related Dynamic Management Views and Functions  
Object-Related Dynamic Management Views and Functions  
Query Notiﬁ cations Related Dynamic Management Views  
Replication-Related Dynamic Management Views  
Resource Governor Related Dynamic Management Views  
Security-Related Dynamic Management Views and Functions  
Server-Related Dynamic Management Views and Functions  
Service Broker Related Dynamic Management Views  
Spatial Data Related Dynamic Management Views and Functions  
SQL Server Operating System Related Dynamic Management Views  
Transaction-Related Dynamic Management Views and Functions  
SQL Server 2012 Data Types 
Data types are the foundation of table creation in SQL Server. As you create a table, you must assign 
a data type for each column. This section covers some of the more commonly used data types in 
SQL Server. Even if you create a custom data type, it must be based on a standard SQL Server data 
type. For example, you may create a custom data type (Address) by using the following syntax, but 
notice that it based on the SQL Server standard varchar data type: 
CREATE TYPE Address
FROM varchar(35) NOT NULL
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤



SQL Server Architecture ❘ 13
Exact Numeric Data Types
Numeric data types consist of bit, tinyint, smallint, int, bigint, numeric, decimal, money, 
float, and real. Each of these data types stores different types of numeric values. The ﬁ rst data 
type, bit, stores only a null, 0 or a 1, which in most applications translates into true or false. Using 
the bit data type is perfect for on and off ﬂ ags, and it occupies only a single byte of space. Table 1-2 
shows other common numeric data types. 
TABLE 1.2: Exact Numeric Data Types
DATA TYPE
DESCRIPTION
STORAGE SPACE
bit
0, 1, or Null
1 byte for each 8 columns 
of this data type
tinyint
Whole numbers from 0 to 255
1 bytes
smallint
Whole numbers from −32,768 to 32,767
2 bytes
int
Whole numbers from −2,147,483,648 to 
2,147,483,647
4 bytes
bigint
Whole numbers from −9,223,372,036,854,775,808 
to 9,223,372,036,854,775,807
8 bytes
numeric(p,s) or 
decimal(p,s)
Numbers from −1,038 + 1 through 1,038 −1
Up to 17 bytes
money
−922,337,203,685,477.5808 to 
922,337,203,685,477.5807
8 bytes
smallmoney
−214,748.3648 to 214,748.3647
4 bytes
Numeric data types, such as decimal and numeric can store a variable number of digits to the 
right and left of the decimal place. Scale refers to the number of digits to the right of the decimal. 
Precision deﬁ nes the total number of digits, including the digits to the right of the decimal place. 
For example, 14.88531 would be a numeric(7,5) or decimal(7,5). If you were to insert 14.25 
into a numeric(5,1) column, it would be rounded to 14.3.
Approximate Numeric Data Types
The data types float and real are included in this group. They should be used when ﬂ oating-point 
data must be represented. However, because they are approximate, not all values can be represented 
exactly.
The n in the float(n) is the number of bits used to store the mantissa of the number. SQL Server 
uses only two values for this ﬁ eld. If you specify between 1 and 24, SQL uses 24. If you specify 
between 25 and 53, SQL uses 53. The default is 53 when you specify float(), with nothing in 
parenthesis.

www.allitebooks.com

SQL Server Architecture ❘ 15
The datetime data type is 8 bytes and stores from January 1, 1753, through December 31, 9999, to 
the nearest 3.33 millisecond.
SQL Server 2012 has four new date-related data types: datetime2, dateoffset, date, and time. 
You can ﬁ nd examples using these data types in SQL Server Books Online.
The datetime2 data type is an extension of the datetime data type, with a wider range of dates. 
Time is always stored with hours, minutes, and seconds. You can deﬁ ne the datetime2 data 
type with a variable parameter at the end — for example, datetime2(3). The 3 in the preceding 
expression means to store fractions of seconds to three digits of precision, or .999. Valid values are 
between 0 and 7, with a default of 3.
The datetimeoffset data type is just like the datetime2 data type, with the addition of the 
time offset. The time offset is + or – up to 14 hours, and contains the UTC offset so that you can 
rationalize times captured in different time zones.
The date data type stores the date only, a long-requested piece of functionality. Alternatively, the 
time data type stores the time only. The time data type also supports the time(n) declaration, so 
you can control granularity of the fractional seconds. As with datetime2 and datetimeoffset, n 
can be between 0 and 7.
Table 1-5 shows the date/time data types with a short description and the amount of storage 
required.
TABLE 1-5: Date and Time Data Types
DATA TYPE
DESCRIPTION
STORAGE SPACE
Date
January 1, 1 to December 31, 9999
3 bytes
Datetime
January 1, 1753 to December 31, 9999, Accurate 
to nearest 3.33 millisecond
8 bytes
Datetime2(n)
January 1, 1 to December 31, 9999 N between 
0 and 7 speciﬁ es fractional seconds
6 to 8 bytes
Datetimeoffset(n)
January 1, 1 to December 31, 9999 N between 
0 and 7 speciﬁ es fractional seconds +- of set
8 to 10 bytes
SmalldateTime
January 1, 1900 to June 6, 2079, Accurate to 
1 minute
4 bytes
Time(n)
Hours:minutes:seconds.9999999 N between 
0 and 7 speciﬁ es fractional seconds
3 to 5 bytes
Other System Data Types
Table 1-6 shows several other data types, which you have not yet seen. 


Editions of SQL Server ❘ 17
EDITIONS OF SQL SERVER
SQL Server 2012 is available in several editions, and the features available to you in each edition 
vary widely. The editions you can install on your workstation or server also vary based on the 
operating system. The editions of SQL Server range from SQL Express on the lowest end to 
Enterprise Edition on the highest. 
Edition Overview
There are three major editions of SQL Server 2012. There are additional, smaller editions typically 
not recommended for production environments so they are not covered here. For more details on 
these, see www.microsoft.com/sqlserver. Figures 1-1 and 1-2 describe the new editions releasing 
with SQL Server 2012. 
FIGURE 1-1
SQL Server 2012 offers three principal SKUs:
Enterprise Edition contains all the new SQL Server 2012 capabilities, including high 
availability and performance updates that make this a mission critical-ready database. This 
edition contains all the BI functionality as well. 
Business Intelligence Edition is new in SQL Server 2012. The BI Server Edition offers the 
full suite of powerful BI capabilities in SQL Server 2012, including PowerPivot and Power 
View. One major focus of this edition is empowering end users with BI functionality. This 
is ideal for projects that need advanced BI capabilities but don’t require the full OLTP 
performance and scalability of Enterprise Edition. The new BI Edition is inclusive of the 
standard edition and still contains the basic OLTP offerings. 
Standard Edition remains as it is today, designed for departmental use with limited scale. It 
has basic database functionality and basic BI functionality. New features are now included 
in Standard such as compression.
➤
➤
➤

18  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
Figure 1-2 provides you with an at-a-glance view of some of the key features in SQL Server 2012 by 
edition.
FIGURE 1-2
The basic functionality is kept in the Standard Edition. Key enterprise BI features are included in the 
BI SKU. When you need to go to high end data warehousing and enterprise level high availability, 
the Enterprise Edition is the right edition for you.
Figure 1-2 provides a more detailed look at the SQL Server 2012 Editions and related capabilities. 
As you can see, for the Standard Edition, there is a 16 core maximum. For the Business Intelligence 
Edition, there is a 20 core maximum for database use and up to the OS Maximum for BI. The 
Enterprise Edition can be used up the maximum number of cores in the operating system.
Both the BI Edition and Enterprise Edition offer the full premium BI capabilities of SQL Server 
2012, including Enterprise Data Management, Self-Server BI, and Corporate BI features. Enterprise 
Edition adds mission critical and Tier 1 database functionality with the maximum scalability, 
performance, and high availability. With the Enterprise Edition under Software Assurance, 
customers also get unlimited virtualization, with the ability to license unlimited virtual machines. 
Licensing
SQL Server 2012 has signiﬁ cant licensing changes. This new licensing scheme can impact your 
environment if you do not run Software Assurance on those licenses. This section is an overview of 
the changes, not an exhaustive licensing discussion. See your Microsoft account manager for more 
details on these changes. 
SQL Server 2012 pricing and licensing better aligns to the way customers purchase database and 
BI products. The new pricing and licensing offers a range of beneﬁ ts for customers including the 
following:

Editions of SQL Server ❘ 19
SQL Server 2012 offers market leading Total Cost of Ownership (TCO):
SQL Server 2012 continues to be the clear high-value leader among major vendors. 
This is exempliﬁ ed through the pricing model; features included in non-enterprise 
editions and world class support no matter the edition. 
Customers with Software Assurance get signiﬁ cant beneﬁ ts and ways to help ease 
the transition to new licensing models. These beneﬁ ts include access to upgrades 
and enhanced support options during your license term. 
Customers with Enterprise Agreements have the easiest transition to SQL Server 
2012 and realize the greatest cost-savings.
SQL Server 2012 is cloud-optimized:
SQL Server 2012 is the most virtualization-friendly database with expanded vir-
tualization licensing, the ﬂ exibility to license per VM, and excellent support for 
Hyper-V.
Customers also have the ability to support hybrid scenarios that span on-premises, 
private cloud, and public clouds through better integration with SQL Azure.
SQL Server 2012 pricing and licensing is designed to enable customers to pay as they grow:
The new streamlined editions are aligned to database and BI needs.
For datacenter scenarios, licensing is better aligned to hardware capacity.
For BI, licensing is aligned to user-based access, which is the way most customers 
are accustomed to purchasing BI.
CPU Core (Not Processor) Licenses
With the release of SQL Server 2012, Microsoft switched to a per core processing model. Don’t fret 
because for most of your servers the costs should stay the same. The CPU Core licenses (available 
only for the Standard and Enterprise Edition) are sold in two core “packs.” So a quad core CPU 
needs two of these packs per socket. These license packs cost half of what a SQL Server 2008 R2 
CPU license cost. The catch here is that you must purchase at least 4 cores per CPU. 
For example:
Two sockets with 2 cores each, you need 4 license “packs” (8 core licenses).
Two sockets with 4 cores each, you need 4 license “packs” (8 core licenses).
Two sockets with 6 cores each, you need 6 license “packs” (12 core licenses).
Two sockets with 8 cores each, you need 8 license “packs” (16 core licenses).
Virtualized SQL Server and Host Based Licensing
When you run a virtualized SQL Server, you must license at least four cores for the VM. If you have 
more than four virtual CPUs on the VM, you must have a CPU Core license for each virtual CPU 
that you have assigned to the VM.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

20  ❘  CHAPTER 1  SQL SERVER 2012 ARCHITECTURE
SQL Server 2012 still includes host-based licensing for those customers with Software Assurance 
and an Enterprise Agreement. The host-based licensing works just like it did before: you purchase 
enough Enterprise Edition CPU Core licenses for the host, and you can run as many virtual 
machines running SQL Server as you want. This will likely be the preferred way for many of you. 
For those customers not running with Software Assurance or an Enterprise Agreement, they will 
want to contact your Microsoft Representative or reseller since host based licensing is not available 
for those customers.
As you can see, this is a lot of change. The pricing has changed too but is not discussed here due 
to the wide variations depending on your individual agreements with Microsoft. Microsoft has 
tried hard to make sure the cost does not go up dramatically for many of you, so don’t fret, but be 
judicious and check this out with your Microsoft Account teams.
SUMMARY
The architecture for SQL Server 2012 has advancements under the covers that will improve 
performance, increase developer efﬁ ciency and system availability, and decrease overall operating 
cost. It is important to focus on your current and future roles and understand the types of features 
and editions that will apply to your situation and organizational needs. 


22  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
 7. 
Do post-install conﬁ gurations if necessary.
 8. 
Clean up for deployment and go! 
This chapter focuses on the plan and the actual install.
PLANNING THE SYSTEM
The ﬁ rst step before initiating a SQL Server installation involves proper planning. A successful 
SQL Server installation starts with a good game plan. As the old proverb goes: “Failing to plan is 
planning to fail.”
Some of the necessary planning includes the following tasks and considerations:
Baseline of current workload
Estimated growth in workload
Minimum hardware and software requirements
Proper storage system sizing and I/O requirements
SQL Server Edition
SQL Server collation, ﬁ le locations, and tempdb sizing
Service account selection
Database maintenance and backup plans
Minimum uptime and response time service levels
Disaster recovery strategy
This list provides just a few of the things to keep in mind when deploying, upgrading, or migrating 
a SQL Server 2012 instance. The next sections cover some of these considerations and best practices 
in more detail.
Hardware Options
Choosing the right hardware conﬁ guration may not always be straightforward. Microsoft 
provides minimum hardware requirements to support a SQL Server 2012 installation, but as the 
word minimum implies, these are only the minimum requirements but not necessarily the most 
appropriate. Ideally, you want to provision hardware that exceeds the minimum requirements to 
meet current and future resource requirements.
This is the reason why you need to create a baseline of current resource requirements and also to 
estimate future needs. Having the necessary hardware to meet future requirements can not only save 
you money, but also avoid downtime required to carry out hardware upgrades.
To guarantee a smooth installation process and predictable performance, become familiar with the 
minimum hardware requirements provided by Microsoft, as listed in Table 2-1.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

Planning the System ❘ 23
Processors
SQL Server 2012 instances that experience a high number of transactions and a high number of 
concurrent connections beneﬁ t from as much processing power as there is available. Processing 
power comes in the form of high clock-speed processors and a large number of available processors. 
Multiple slightly slower processors perform better than a single fast processor. For example, two 
1.6 GHz processors perform faster than a single 3.2 GHz processor.
Newer processor models offer multiple cores in a single physical socket. These multicore processors 
have many advantages including space and power consumption savings. Multicore processors enable 
you to run more than one instance of SQL Server 2012 within the same physical server, either as 
named instances or as virtual machines. In other words, you can run as many SQL Server 2012 
servers as your hardware and licensing allows with a single physical server. Space is drastically 
reduced in your data center as multiple physical servers can be consolidated into a single physical 
server. This consolidation enables you to reduce your power bill because you have fewer servers 
physically connected to the power grid.
Core-Based Licensing has changed for SQL Server 2012. In the new SQL Server 2012 Core-
Based Licensing model, each core in a multicore processor is now required to be licensed. This 
change applies both to physical server and virtual machines. For more details on SQL Server 2012 
Licensing, refer to the “Licensing” section of Chapter 1, “SQL Server 2012 Architecture.” 
TABLE 2-1: SQL Server 2012 Minimum Hardware Requirements
COMPONENT
REQUIREMENT
Processor
64-bit installations:
Speed: 1.4 Ghz or higher
AMD Opteron, Athlon 64, Intel Pentium IV, Xeon with Intel EM64T support
or
32-bit installations:
Speed: 1.0 Ghz or higher
Pentium III compatible
Memory
1GB (512MB Express Edition)
Storage
Database Engine and data ﬁ les, Replication, Full-Text Search, and Data Quality 
Services: 811 MB
Analysis Services and data ﬁ les: 345 MB
Reporting Services and Report Manager: 304 MB
Integration Services: 591 MB
Master Data Services: 243 MB
Client Components (other than SQL Server Books Online components and 
Integration Services tools): 1,823 MB
SQL Server Books Online Components to view and manage help content1: 375 KB
 
 
 
 

24  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
Memory
Memory is an important resource for optimal performance of SQL Server 2012. Well-designed 
database systems make proper use of available memory by reading as much as possible from cached 
data pages in memory buffers. 
Memory needs to be allocated both for the SQL Server instance and the operating system. As much 
as possible you should avoid installing memory-intensive applications in the same Windows server as 
your SQL Server instance.
A good starting point to decide how much memory you need is to factor the number of data pages of 
each of the databases hosted on the SQL Server instance along with query execution statistics such 
as minimum, maximum, and average memory utilization for a typical workload. The goal is to allow 
SQL Server to keep in cache as many data pages and execution plans in memory as possible to avoid 
costly data page reads from disk and execution plan compilations.
You also need to be aware of memory limitations imposed by speciﬁ c SQL Server editions. For 
example, SQL Server 2012 Enterprise Edition supports up to 2 TB of RAM; Standard Edition 
supports up to 64 GB of RAM; and Express Edition supports up to 1 GB of RAM.
Storage
The storage system of a SQL Server 2012 instance requires special considerations because slow 
performing storage can bring database performance to a halt. When planning for storage for your 
SQL Server 2012 databases, consider your availability, reliability, throughput, and scalability 
requirements. 
To test and validate your storage system’s performance, you need to gather important metrics such 
as maximum number of I/O requests per second (IOPS), throughput (MBPS) and I/O latency. 
Table 2-2 lists these three key metrics along with a brief description.
TABLE 2-2: Key Storage Metrics
METRIC
DESCRIPTION
I/O requests per 
second (IOps)
Number of concurrent requests the storage system can handle in one 
second. You want this number to be high, usually between 150 to 250 IOPS 
for a single 15k rpm SAS drive and between 1,000 to 1,000,000 IOPS for 
enterprise SSDs and SANs depending on conﬁ guration and manufacturer.
Throughput (MBps)
Size of data the storage system can read or write in one second. You want 
this number to be high.
I/O latency (ms)
Time delay between I/O operations. You want this number to be zero or 
close to zero. 
You can gather these key metrics by using free tools such as SQLIO, SQLIOSim, IOMeter, and 
CrystalDiskMark. The explanation on how to use these tools is beyond the scope of this chapter but 
you can ﬁ nd good documentation about them at http://msdn.microsoft.com/en-us/library/
cc966412.aspx.
www.allitebooks.com

Planning the System ❘ 25
There are the two main types of storage used in SQL Server installations: DAS and SAN. The 
following sections explain these types in detail.
Direct-Attached Storage (DAS)
Direct Attached Storage (DAS) is the simplest storage option to understand. In this type of storage, 
the disk drives are located within the server box enclosure and attached directly to a disk controller. 
Alternatively, they can also be located in an external enclosure attached directly through a cable to 
a Host Bus Adapter (HBA). No additional equipment is required such as switches.
The main advantage of DAS is that they are easier to implement and maintain at a lower cost. The 
main disadvantage is limited scalability. Although, in recent years DAS storage systems have been 
catching up with features found only in higher-end SAN units, they are still bound by limitations 
such as the number of disk drives and volume size they can scale up to and manage, the number of 
servers they can be attached to, and the distance between the storage unit and a server. 
Server connectivity and distance are in particular the biggest differentiators because DAS requires 
direct physical links between the storage unit and the servers, limiting the number of servers that 
can be attached simultaneously and the distance separating the storage unit and a server, usually 
just a couple feet long. 
Storage Area Network (SAN)
Storage area networks (SAN) are specialized networks that interconnect storage devices made 
available to servers as directly attached storage volumes. This network of storage devices is 
interconnected through high-speed dedicated Fibre Channel (FC) devices known as fabric switches, 
or through the iSCSI protocol using regular Ethernet switches.
One of the great advantages of SANs is the ability to span over a large geographical area, typically 
through TCP/IP routing using dedicated wide area network (WAN) connections. This enables 
organizations to implement features such as storage replication between distant data centers in their 
Disaster Recovery efforts. 
In addition, SANs offer the best reliability and scalability features for mission-critical database 
systems. Well- architected SANs offer much better throughput and reduced I/O latency then Direct 
Attached Storage (DAS). SANs can also scale up to handle many more disk arrays than DAS. 
The main disadvantage of SANs is the higher cost and complexity to implement and maintain SANs. 
Choosing the Right Type of Storage
The type of storage for your SQL Server installation depends on your speciﬁ c needs. As you learned 
from the brief preceding comparison, DAS is a lot less expensive and easier to conﬁ gure and 
maintain than a SAN; however, SANs offer many performance, availability, and scalability beneﬁ ts.
A key element to take into consideration when choosing a storage system is the disk drive 
technology used in your storage system and how these disk drives are pooled together. Both DAS 
and SANs use an array of disk drives that are usually conﬁ gured to create a storage pool that can 
then be presented to a server as a single entity. 

26  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
In the next section, you learn about the different disk drive technologies and how they can be 
pooled together through RAID levels.
Disk Drives
As previously discussed, an important consideration involves the amount of throughput necessary to 
support your IO requirements. To satisfy large throughput requirements, you often need to spread 
reads and writes over a large number of fast-spinning disk drives. 
Spreading these IO operations means storing small chunks of the data on each of the disk drives that 
are lumped together. In this type of distributed storage, no single disk drive contains the complete 
data. Therefore, a failure on one disk means total data loss. This is the reason you should always 
consider reliability with any decision for storage systems. To avoid data loss due to a disk failure, 
special arrangement of disks called disk arrays or RAID can be conﬁ gured to satisfy both throughput 
and reliability. Choosing the right disk RAID level is a key decision that can impact overall 
server performance. Table 2-3 describes the most common disk RAID levels used for SQL Server 
environments.
TABLE 2-3: Commonly Used RAID Levels
RAID LEVEL
DESCRIPTION
RAID 0
Also known as a stripe set or striped volume. Two or more disks lumped together 
to form a larger volume. No fault tolerance. Fast read/writes.
RAID 1
Also known as mirrored drives. Data written identically to two drives. One drive can 
fail with no data loss. Slower writes. Only half of total raw storage available.
RAID 1+0
Also known as RAID 10. Mirrored sets in a striped set. Better write performance 
and fault tolerance. Only half of total raw storage available.
RAID 0+1
Striped sets in a mirrored set. Less fault tolerant than RAID 1+0. Good write 
performance.
RAID 5
Tolerates one drive failure. Writes are distributed among drives. Faster reads, slow 
writes. Some raw storage space lost. 
RAID 6
Tolerates two drive failures. Faster reads, slower writes than RAID 5 due to added 
overhead of parity calculations. Similar raw storage loss as RAID 5.
In recent years, a faster type of disk drive technology has become more and more popular as it has 
become more affordable and reliable. This faster type of disk drive is called Solid State Drives (SSD) 
and involves drives that have no moving parts. SSD offer as much as 100x better read-and-write 
throughput than spinning disk drives. SQL Server can reap the beneﬁ ts of faster read-and-write 
operations, especially for databases with high IO demand.
Adoption of SSD drives has grown in the last couple of years due in part to improved reliability and 
cost reduction. Several SAN storage system vendors also offer SSD drive arrays.


28  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
You need to understand the locale requirements, sorting, case, and accent sensitivity of the data in your 
organization and your customers to determine which collation to use. The code page that a Windows 
Server uses can be found under Control Panel Á Regional Settings. The code page selected for the 
Windows Server may not always be the code page required for you SQL Server instance. 
Following are two types of collations: SQL Server and Windows.
SQL Server Collation
SQL Server collations affect the code page used to store data in char, varchar, and text columns. 
They affect how comparisons and sorting are done on these data types. For example, if you were to 
create a SELECT statement such as the following against a case-sensitive collation database, it would 
not return the employee named Jose stored in proper case as shown here.
 SELECT FROM Employees
WHERE EmployeeFirstName=’JOSE’
Results: <none>
SELECT FROM Employees
WHERE EmployeeFirstName=’Jose’
Results: Jose
Windows Collation
Windows collations use rules based on the chosen Windows locale for the operating system. The 
default behavior is that comparisons and sorting follow the rules used for a dictionary in the 
associated language. You may specify binary, case, accent, Kana, and width sensitivity. The key 
point is that Windows collations ensure that single-byte and double-byte character sets behave the 
same way in sorts and comparisons.
Case-Sensitivity
Your collation is either case-sensitive or not. Case-sensitive means that U is different from u. This is 
true for everything in the region to which the collation applies (in this case, master, model, resource, 
tempdb, and msdb). This is true for all the data in those databases. Here is the gotcha: Think about 
what the data in those databases actually is; it includes data in all the system tables, which means 
object names are also case-sensitive.
Sort Order
The collation you choose also affects sorting. Binary orders (Latinl_General_BIN, for instance) 
sort based on the bit value of the character; they are case-sensitive. Consider the following select 
statement for a table that contains employee names for Mary, Tom, mary, and tom, as seen in 
Figure 2-1. If you choose a dictionary sort order (Latinl_General_CS_AI, for instance), the 
previous statement would yield the following result set shown in Figure 2-2. 

Installing SQL Server ❘ 29
FIGURE 2-1
FIGURE 2-2
Service Accounts
Service accounts are an important part of your security model. When choosing service accounts, 
consider the principle of least privilege. Service accounts should only have the minimum required 
permissions to operate. Separate service accounts should be used for each service to track what each 
service is doing individually. Service accounts should always be assigned strong passwords. You can 
choose from several service account options:
Windows or Domain account: This active directory or Windows account that you create is 
the preferred account type for SQL Server services needing network access. 
Local System account: This highly privileged account should not be used for services 
because it acts as the computer on the network and has no password. A compromised 
process using the Local System account can also compromise your database system.
Local Service account: This special, preconﬁ gured account has the same permissions as 
members of the Users group. Network access is done as a null session with no credentials. 
This account is unsupported.
Network Service account: The same as the Local Service account, except that network 
access is allowed, credentialed as the computer account. Do not use this account for SQL 
Server or SQL Agent Service accounts. 
Local Server account: This local Windows account that you create is the most secure 
method you can use for services that do not need network access. 
You should use dedicated Windows or domain accounts for production systems. Some organizations 
chose to create a single domain account for all their SQL Server instances, whereas others choose to 
create individual domain accounts for each service. Either one works as long as the service account 
passwords are strong and secured.
INSTALLING SQL SERVER
In this section you learn about the different types of installations: new installs, side-by-side installs, 
and upgrades. You also learn to perform unattended and attended installs using the graphical user 
interface (GUI), the Command Prompt, Conﬁ guration Files, and PowerShell scripts. More details 
about upgrades are covered in Chapter 3, “Upgrading SQL Server 2012 Best Practices.” 
➤
➤
➤
➤
➤

30  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
New Installs
A new install occurs when you have a clean slate and no other SQL Server components are on the 
server. Check the directories and the Registry to ensure that you have a clean system and that you 
have no remnants of previous SQL installs. 
Side-by-Side Installs
SQL Server also supports a side-by-side install. A side-by-side install occurs when you have multiple 
instances of SQL Server on a single server. SQL Server 2012 supports multiple instances of the 
Database Engine, Reporting Services, and Analysis Services on the same box. It also runs side by 
side with previous versions of SQL Server. If the existing instance is a default instance, your new 
install must be a named instance because only one default instance can exist on each server.
The biggest issue with side-by-side installs is memory contention. Make sure you set up your 
memory so that each instance does not try to acquire the entire physical memory. IO contention can 
also become an issue if database ﬁ les from the different instances share the same storage resources.
Upgrades
If SQL Server components exist on the box, you can upgrade the existing instance. In this case, 
you install SQL Server on top of the existing instance, also known as in-place upgrade. To upgrade 
to SQL Server 2012 from a previous version of SQL Server, you launch the Upgrade Wizard from 
the SQL Server Installation Center using the Upgrade from SQL Server 2005, SQL Server 2008, or 
SQL Server 2008 R2 shortcut under the Installation tab.
Unattended Installs
SQL Server 2012 enables you to perform unattended installations via command-line parameters 
or a conﬁ guration ﬁ le. Unattended installations enable you to install SQL Server with the exact 
same conﬁ guration on multiple servers with little or no user interaction during the setup process. 
All screen entries and dialog responses are made automatically using stored information in the 
conﬁ guration ﬁ le or by passing them as command-line parameters.
Unattended Installation From the Command Line
To perform a new SQL Server 2012 installation from the command line, follow these steps.
 1. 
Launch the Command Prompt window with elevated Administrator privileges by 
right-clicking on the Command Prompt executable and selecting Run as Administrator. 
The Command Prompt window opens.
 2. 
In the Command line type the following command, and press Enter:
D:\setup.exe /ACTION=install /QS /INSTANCENAME=”MSSQLSERVER” /
IACCEPTSQLSERVERLICENSETERMS=1 
/FEATURES=SQLENGINE,SSMS 
/SQLSYSADMINACCOUNTS=”YourDomain\Administrators”


32  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
Notice in Figure 2-3 the path to ConfigurationFile.ini. At this point, the Conﬁ guration 
File has been created, and all options and parameter values speciﬁ ed in previous screens 
have been recorded.
 4. 
Open Windows Explorer, and navigate to the folder where the Conﬁ guration File has been 
created. Click the Cancel button to stop the SQL Server 2012 Setup Wizard.
 5. 
Locate ConfigurationFile.ini and copy it to a folder that can be referenced for 
unattended installs. For example, use a shared folder \\fileserver\myshare. 
 6. 
Open ConfigurationFile.ini to modify it. Make the following changes to prepare the ﬁ le 
for unattended installs.
Set QUIET = “True”
Set SQLSYSADMINACCOUNTS = “YourDomain\Administrators”
Set IACCEPTSQLSERVERLICENSETERMS = “True”
Remove ADDCURRENTUSERASSQLADMIN
Remove UIMODE 
After you have customized the Conﬁ guration File for unattended installations, use the Command 
Prompt to execute Setup.exe and specify the path to the Conﬁ guration File. The following 
command line script exempliﬁ es the syntax.
D:\Setup.exe /ConfigurationFile=\\fileserver\myshare\ConfigurationFile.ini
➤
➤
➤
➤
➤
FIGURE 2-3


34  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
LISTING 2-1 (continued)
}
$command = $path + ‘setup.exe /Action=”Install”’
}
#Accept the license agreement - required for command line installs
$command += ‘ /IACCEPTSQLSERVERLICENSETERMS’
#Use the QuietSimple mode (progress bar, but not interactive)
$command += ‘ /QS’
#Set the features to be installed
$command += ‘ /FEATURES=SQLENGINE,CONN,BC,SSMS,ADV_SSMS’
#Set the Instance Name
$command += (‘ /INSTANCENAME=”{0}”’ -f $InstanceName)
#Set License Key only if a value was provided, 
#else install Evaluation edition
if ($LicenseKey -ne $null -and $LicenseKey -ne “”)
{
 $command += (‘ /PID=”{0}”’ -f $LicenseKey)
}
#Check to see if a service account was specified
if ($ServiceAccount -ne $null -and $ServiceAccount -ne “”)
{
#Set the database engine service account
$command += (‘ /SQLSVCACCOUNT=”{0}” /SQLSVCPASSWORD=”{1}”
/SQLSVCSTARTUPTYPE=”Automatic”’ -f 
$ServiceAccount, $ServicePassword)
#Set the SQL Agent service account
$command += (‘ /AGTSVCACCOUNT=”{0}” /AGTSVCPASSWORD=”{1}”
 /AGTSVCSTARTUPTYPE=”Automatic”’ -f 
$ServiceAccount, $ServicePassword)
}
else
{
#Set the database engine service account to Local System
$command += ‘ /SQLSVCACCOUNT=”NT AUTHORITY\SYSTEM” 
/SQLSVCSTARTUPTYPE=”Automatic”’
#Set the SQL Agent service account to Local System
$command += ‘ /AGTSVCACCOUNT=”NT AUTHORITY\SYSTEM”
 /AGTSVCSTARTUPTYPE=”Automatic”’
}
#Set the server in SQL authentication mode if SA password was provided
if ($SaPassword -ne $null -and $SaPassword -ne “”)
{
$command += (‘ /SECURITYMODE=”SQL” /SAPWD=”{0}”’ -f $SaPassword)
}
#Add current user as SysAdmin
$command += (‘ /SQLSYSADMINACCOUNTS=”{0}”’ -f
 [Security.Principal.WindowsIdentity]::GetCurrent().Name)
#Set the database collation
$command += (‘ /SQLCOLLATION=”{0}”’ -f $SqlCollation)
#Enable/Disable the TCP Protocol
if ($NoTcp)
{
 $command += ‘ /TCPENABLED=”0”’
}
www.allitebooks.com

Installing SQL Server ❘ 35
else
{
 $command += ‘ /TCPENABLED=”1”’
}
#Enable/Disable the Named Pipes Protocol
if ($NoNamedPipes)
{
 $command += ‘ /NPENABLED=”0”’
}
else
{
 $command += ‘ /NPENABLED=”1”’
}
if ($PSBoundParameters[‘Debug’]) 
{
 Write-Output $command
}
else
{
 Invoke-Expression $command
}
}
After you download Listing 2-1 from the Wrox companion website, save it to a folder, for example 
c:\scripts. Because this is a ﬁ le that you download from the Internet, you may be required to 
right-click the ﬁ le and unblock it. When downloaded, execute this function by following these steps.
 1. 
Launch the PowerShell command line with elevated Administrator privileges by right-
clicking the PowerShell executable and selecting Run as Administrator. The PowerShell 
command line opens.
 2. 
Verify that you can run and load unsigned PowerShell scripts and ﬁ les. In the PowerShell 
command line, type get-executionpolicy to verify the current execution policy. If it is not 
set to RemoteSigned you need to change it to this value by executing the following command:
Set-ExecutionPolicy RemoteSigned
 3. 
Next, load the PowerShell function in the script ﬁ le by executing the following command:
. c:\scripts\Install-Sql2012.ps1
Notice the . and blank space before the script ﬁ le path. The . and blank space is a required 
character to dot-source the script ﬁ le.
 4. 
Verify that the function has been loaded by issuing the following command:
get-command Install-Sql2012
A single row is returned showing CommandType Function and Name Install-Sql2012.
 5. 
At this point you are ready to invoke the PowerShell function you just loaded. Invoke the 
Install-Sql2012 as follows:
Install-Sql2012 -Param1 Param1Value -Param2 Param2Value ..


Installing SQL Server ❘ 37
To initiate an attended SQL Server 2012 install, follow these steps.
 1. 
Launch Setup.exe from your SQL Server 2012 installation media. The Installation Center 
opens, as shown in Figure 2-5.
FIGURE 2-5
 2. 
Click the Installation tab on the left, and then click the ﬁ rst option on the right titled New 
SQL Server Standard Installation or Add Feature to an Existing Installation The SQL Server 
2012 Setup Wizard launches.
 3. 
The Setup Support Rules runs to identify problems that may occur during the Setup Support 
Files installation. When this step ﬁ nishes click OK. The Install Setup Files process initiates.
 4. 
After the Install Setup Files completes, a second set of Setup Support Rules need to be 
checked. Click OK to continue.
 5. 
Depending on the installation media and your license agreement, you may be prompted 
to select the SQL Server Edition and to enter a Product Key on the next screen. Click OK to 
continue.
 6. 
The License Agreement screen opens. Accept the terms and click Next.

38  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
 7. 
The Setup Role screen opens. Select the SQL Server Feature Installation option, and click 
Next.
 8. 
The Feature Selection screen opens. Select Database Engine Services and Management 
Tools -Basic, and click Next. Figure 2-6 shows the list of features available to install in 
SQL Server 2012.
FIGURE 2-6
 9. 
The Installation Rules screen opens to determine if anything can cause the installation 
process to lock. Click Next.
 10. 
The Instance Conﬁ guration opens. In this screen you can decide to install the instance as 
a default instance or as a named instance. You can also provide an instance ID and change 
the default root directory. Click Next.
 11. 
The Disk Space Requirements screen opens and displays a summary of space to be used by 
the features selected. Click Next.
 12. 
The Server Conﬁ guration screen opens. Provide the service accounts under which SQL 
Server Database Engine, SQL Server Agent, and SQL Server Browser run under and 
the Collation to be used by SQL Server 2012. Click Next. Figure 2-7 shows the Server 
Conﬁ guration screen.



Installing Analysis Services ❘ 41
New in SQL Server 2012 is the option to install Analysis Services in either of two modes:
 Multidimensional and Data Mining Mode (UDM mode)
Tabular Mode
The option to choose between these two modes is available in the Analysis Services Conﬁ guration 
screen in SQL Server 2012 Setup, as shown in Figure 2-9.
➤
➤
FIGURE 2-9
In the next sections, both Analysis Services modes are described brieﬂ y.
Multidimensional and Data Mining Mode (UDM Mode)
Analysis Services Multidimensional and Data Mining mode (UDM Mode), installs the 
traditional Analysis Services engine available since SQL Server 2005. This engine is based on the 
Uniﬁ ed Dimensional Model (UDM). 
The UDM serves as an intermediate layer between one or more data sources and consolidates all 
the business rules. UDM acts as the hub of the multidimensional model, enabling users to query, 
aggregate, drill down, and slice and dice large Analysis Services databases with blink-of-an-eye 
response times.

42  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
The Analysis Services UDM mode supports MOLAP, ROLAP, and HOLAP storage and processing 
modes described in the next sections.
MOLAP (Multidimensional OLAP)
In MOLAP storage mode, data is stored and aggregated in one or more partitions of the 
multidimensional database. This storage mode is designed to maximize query performance. Data 
stored in MOLAP storage mode is current as of the last processing time.
ROLAP (Relational OLAP)
In ROLAP storage mode, data is not stored within the Analysis Services database. If queries cannot 
be satisﬁ ed from the query cache, it uses internal indexed views to retrieve data from the relational 
data source. Query times can be much slower than MOLAP, but data is more real time as its source 
transactional system.
HOLAP (Hybrid OLAP)
In HOLAP storage mode, some portions of the multidimensional database are stored in MOLAP, 
and some portions are retrieved directly from its relational source. Queries that access more 
aggregated data are satisﬁ ed from its multidimensional store as in MOLAP. Drill-down queries are 
retrieved directly from its relational data sources as in ROLAP.
Tabular Mode
The Analysis Services Tabular Mode is based on a new database engine called the VertiPaq 
engine. The Vertipaq engine is a column-based database that enables high levels of compression 
due to the reduced need to store discrete values along with advanced compression algorithms. 
The ability to compress large amounts of data enables the Vertipaq engine to store, retrieve, and 
manipulate data from RAM memory much faster than the traditional disk-based Analysis Services 
engine.
Tabular Mode supports the new Semantic Model called Business Intelligence Semantic Model 
(BISM). The development environment is similar to that of the PowerPivot add-in for Excel. The 
PowerPivot add-in for Excel runs a scaled down version of the same Vertipaq engine that Analysis 
Services Tabular Mode uses.
Similar to the traditional Analysis UDM engine, Analysis Services Tabular Mode supports querying 
from its in-memory storage or directly from the relational data source or a hybrid of both. These 
query mode options are available in the BISM model properties. The four available query mode 
options include the following:
 1. 
Vertipaq
 2. 
Direct Query
 3. 
InMemorywithDirectQuery
 4. 
DirectQueryWithInMemory 
These four query mode options are described in the next sections.

Installing PowerPivot for SharePoint ❘ 43
Vertipaq Query Mode
In Vertipaq Query Mode, data is stored and queried from in-memory stored datasets. Little or no 
latency is involved as disk IO latency costs are minimized or eliminated. Because of its in-memory 
dataset access, complex calculations and sorting operations are almost instantaneous. 
Vertipaq Query Mode is similar to MOLAP storage in the traditional Analysis Services engine in 
that it holds point-in-time (PIT) data. If data changes in its relation data source, its in-memory 
storage needs to be refreshed with the new datasets.
Direct Query Mode
In Direct Query Mode, also known as pass-through mode, queries are processed by its data 
source,  in most cases a relational database. The Direct Query Mode advantage over Vertipaq 
Query Mode resides on it is capability to provide real-time datasets over larger data volumes that 
cannot ﬁ t in-memory. 
Similar to ROLAP in the traditional Analysis Services engine, Direct Query Mode is designed for 
real-time access requirements of the data source.
InMemoryWithDirectQueryMode
In this query mode, unless otherwise speciﬁ ed by the connection strings from the client, queries use 
the dataset stored in cache by default. It supports the ability for the client to switch to real-time data.
DirectQueryWithInMemoryMode
Opposite to InMemoryWithDirectQuery Mode, unless otherwise speciﬁ ed by the connection strings 
from the client, queries use the relational data source by default. It supports the ability for the client 
to switch to cached data.
INSTALLING POWERPIVOT FOR SHAREPOINT
PowerPivot for SharePoint Mode is a feature role option available since SQL Server 2008 R2. 
The PowerPivot for SharePoint option shown installs a version of the new Analysis Services 
Vertipaq engine to support server-side processing and management of PowerPivot workbooks that 
you publish to SharePoint 2010.
PowerPivot for SharePoint must be installed in a server joined to a domain, SharePoint 2010 
Enterprise with Service Pack 1 and the instance name PowerPivot to be available on the server where 
it is installed.
To install PowerPivot for SharePoint follow these steps:
 1. 
Launch Setup.exe from your SQL Server 2012 installation media. The Installation 
Center opens.
 2. 
Click the Installation tab on the left, and then click the ﬁ rst option on the right titled New 
SQL Server Standard Installation or Add Feature to an Existing Installation. The SQL 
Server 2012 Setup Wizard launches.

44  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
 3. 
The Setup Support Rules runs to identify problems that may occur during the setup support 
ﬁ les installation. When this step ﬁ nishes, click OK. The Install Setup Files process initiates.
 4. 
When the Install Setup Files completes, a second set of Setup Support Rules need to be 
checked. Click OK to continue.
 5. 
Depending on the installation media and your license agreement, you may be prompted to 
select the SQL Server Edition and to enter a Product Key on the next screen. Click OK to 
continue.
 6. 
The License Agreement screen opens. Accept the terms and click Next.
 7. 
The Setup Role screen opens. Select the PowerPivot for SharePoint option. You can also 
install an instance of SQL Server Database Services with this installation by checking the 
check box below, as shown in Figure 2-10. 
FIGURE 2-10
 8. 
Click Next. The Feature Selection screen opens. The options are preselected and displayed 
for informational purposes. 
 9. 
Click Next. The Installation Rules screen opens to determine if anything can cause the 
installation process to fail. Click Next.
 10. 
The Instance Conﬁ guration screen opens. You cannot change the instance name because it 
needs to be PowerPivot. Only the instance ID can be modiﬁ ed. Click Next.
www.allitebooks.com

 11. 
The Disk Space Requirements screen opens and displays a summary of space to be used by 
the features selected. Click Next.
 12. 
The Server Conﬁ guration screen opens. In this screen you provide the service account under 
which the Analysis Services Engine runs. A domain account is required. Click Next.
 13. 
The Analysis Services Conﬁ guration screen opens. Add domain accounts that require 
administrative permissions of the Analysis Services instance.
 14. 
Click next until you reach the Ready to Install screen. Review the installation summary 
page, and click Install.
This ﬁ nalizes the installation steps of PowerPivot for SharePoint.
BURNING IN THE SYSTEM
Before you move a system into common use, you should “burn it in.” This means that you should 
stress the server. It is not unusual for a server to work in production for months or years with a 
hardware problem that existed at the time the server was deployed. Many of these failures do not 
show up when the server is under a light load but become immediately evident when you push the 
server hard.
You can use several free tools to burn in and stress test a database server to ensure that the storage 
system is ready to handle required IO workloads, memory pressure, and CPU processing power 
demand. Some of these tools include the following:
SQLIOSim: Free tool by Microsoft designed to generate similar SQL Server IO read and 
write patterns. This tool is great to test IO intensive operations such as DBCC CHECKDB 
and Bulk insert, delete, and update operations. SQLIOSim replaces SQLIOStress. You can 
download SQLIOSim at http://support.microsoft.com/kb/231619.
IOMeter: Another free tool great for running a stress test with capability to simulate 
concurrent application workloads.
Prime95: This free tool was designed to ﬁ nd Mersenne Prime numbers and is CPU and 
RAM memory-intensive. It can be customized to stress test CPU and memory workloads for 
sustained periods of time.
You can search online for several other free and paid applications that can perform an initial burn 
in and stress test. Some server and server component manufacturers also provide tools to benchmark 
and stress test your equipment.
POST-INSTALL CONFIGURATION
After you install SQL Server 2012, you need to conﬁ gure additional settings and complete the tasks 
necessary to have a production-ready server. Some of these settings, including max server memory, 
parallelism threshold, and network packet size, are meant for ﬁ ne-tuning the SQL Server instance for 
optimal performance. Other settings and tasks, typically changing default port, login auditing, and 
disabling SA account, are geared toward securing, auditing, and monitoring a SQL Server instance.
➤
➤
➤
Post-Install Conﬁ guration ❘ 45


Post-Install Conﬁ guration ❘ 47
The minimum server memory setting does not need to be changed unless the operating system 
constantly requests memory resources for other applications sharing the same memory space. You 
want to avoid releasing too much memory to the operating system because that could potentially 
starve a SQL Server instance from memory.
On the other hand, the maximum server memory sets limits for the maximum amount of memory a 
SQL Server instance can allocate. A value set too high can potentially starve an operating system from 
memory resources. The maximum server memory value should not equal or exceed the total amount 
of available server memory. This value should be at least 1GB less than the total server memory.
Network Packet Size
The default network packet size for SQL Server 2012 is 4,096 bytes. Setting a packet size larger 
than the default size can improve performance for SQL Server instances that experience a large 
number of bulk operations or that transfer large volumes of data.
If Jumbo Frames are supported and enabled by the server’s hardware and the network 
infrastructure, increase the network packet size to 8,192 bytes.
Instant File Initialization
Each time a database ﬁ le is created or needs to grow, it is ﬁ rst zero-ﬁ lled by the operating system 
before the new space is made available for writing. This operation can be costly because all write 
operations are blocked until zero-ﬁ ll completes. To avoid these types of blocks and waits, you can 
enable instant ﬁ le initialization by adding the SQL Server service account to the list of users in the 
Perform Volume Maintenance Tasks policy under User Rights Assignment in the Local Policies of 
the server’s Security Settings.
tempdb
One of the most important system databases that require special consideration and planning 
is tempdb. Over the years, tempdb has taken on more responsibility than it had in the past. 
Historically, tempdb has been used for internal processes such as some index builds and table variable 
storage, as well as temporary storage space by programmers. The following is a partial list of some of 
the uses for tempdb:
Bulk load operations with triggers 
Common table expressions
DBCC operations
Event notiﬁ cations
Indexe rebuilds, including SORT_IN_TEMPDB, partitioned index sorts, and online index 
operations
Large object type variables and parameters
Multiple active result set operations
Query notiﬁ cations
Row versioning
➤
➤
➤
➤
➤
➤
➤
➤
➤


Post-Install Conﬁ guration ❘ 49
Conﬁ guring SQL Server Settings for Security
SQL Server 2012 provides system settings that can be optimized for a more controlled and secure 
environment. Some of the most important security settings are discussed in the following sections.
SA Account
The SysAdmin (SA) account is a default system account with top level privileges in SQL Server. 
Because it is a well-known account, it is the target of a large number of exploit attacks. To eliminate 
exposure to these types of attacks, always assign a strong password that only you know and never 
gets used. Secure the password in a vault, and disable the SA account.
TCP/IP Ports
SQL Server uses the default TCP/IP port 1433 to communicate with clients. Named SQL Server 
instances on the other hand are dynamically assigned TCP/IP ports upon service startup. For 
hacking prevention and ﬁ rewall conﬁ guration purposes, you may need to change default ports and 
control the port numbers over which named SQL Server instances communicate.
SQL Server 2012 includes a tool called SQL Server Conﬁ guration Manager (discussed in more 
detail later in this section) to manage SQL Server services and their related network conﬁ gurations. 
You can ﬁ nd the SQL Server Conﬁ guration Manager under the Microsoft SQL Server 2012\
Configuration Tools folder in the Start menu. Figure 2-12 shows the TCP/IP Properties dialog 
box in SQL Server Conﬁ guration Manager where you can change the default 1433 port.
FIGURE 2-12

50  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
Service Packs and Updates
After a freshly installed SQL Server instance, you must review available updates. SQL Server 
updates may be available in the form of hotﬁ xes, cumulative updates, and service packs. Carefully 
review all updates before applying them to avoid negatively impacting your applications. You 
absolutely want to install security ﬁ xes marked as critical to protect your database systems against 
known threats, worms, and vulnerabilities. Do not enable automatic updates on production SQL 
Server instances. Test all updates in a controlled testing environment before you apply them in 
production.
Additional SQL Server Settings
Additional SQL Server settings and properties are available through SQL Server Management 
Studio (SSMS) and the sp_configure System Stored Procedure.
For a complete list and description of all SQL Server conﬁ guration options available through the 
sp_configure System Stored Procedure, visit http://msdn.microsoft.com/en-us/library/
ms188787(v=sql.110).aspx.
Chapter 4, “Managing and Troubleshooting the Database Engine,” covers SQL Server 
conﬁ gurations in more detail.
Best Practices Analyzer (BPA)
The Microsoft SQL Server 2012 Best Practices Analyzer (BPA) is a free diagnostic tool that gathers 
information about installed SQL Server 2012 instances, determines if the conﬁ gurations align to 
recommended best practices, outlines settings that do not align to these recommendations, indicates 
potential problems, and recommends solutions.
BPA can help you identify potential issues for your SQL Server 2012 installation. It is always best to 
catch conﬁ guration issues before a server goes into production to avoid unplanned downtime.
You can download the Best Practices Analyzer for SQL Server 2012 at http://technet.microsoft
.com/en-us/sqlserver/bb671430.
SQL Server Conﬁ guration Manager
The SQL Server Conﬁ guration Manager tool enables you to specify SQL Server Services options 
and whether these services are automatically or manually started after Windows starts. SQL Server 
Conﬁ guration Manager enables you to conﬁ gure services settings such as service accounts, network 
protocols, and ports SQL Server listens on.
The SQL Server Conﬁ guration Manager can be accessed under Conﬁ guration Tools in the 
Microsoft SQL Server 2012 Program Menu folder. It is also available under Services and 
Applications in Computer Management Console.

Back It Up
Backup plans should be on your checklist after completing a SQL Server installation. You must 
deﬁ ne backup schedules and backup storage locations for system and user databases. In addition, if 
using encryption, you need to back up the encryption key. 
Always create your backup ﬁ les in a shared network drive or backup device and never on the same 
server being backed up. Consider keeping redundant copies of your backups and make sure that 
backups are secured and immediately available in case of a disaster.
Databases should be backed up in full or incrementally. Depending on the database recovery mode 
log, backups should also be part of your backup schedule to restore from the log if necessary. Refer 
to Chapter 17, “Backup and Recovery” for more details.
Deﬁ ne your backup retention policy to avoid storing unnecessary historical backups. Routinely 
restore backups to ensure they successfully restore at any given time to avoid surprises when facing a 
disaster. Remember, a good backup is as good as its last restore.
UNINSTALLING SQL SERVER
In some circumstances you need to uninstall a SQL Server instance completely due to problems 
such as incompatibility issues with a newer version or for licensing consolidation purposes. You can 
uninstall SQL Server using Programs and Features under Control Panel. During the uninstall process, 
you can choose to remove all or some of the features installed for a speciﬁ c instance. If more than one 
instance is installed, the uninstall process prompts you to select the instance you want to remove.
Additional components and requirements installed during Setup may not be uninstalled and need to 
uninstalled separately.
Uninstalling Reporting Services
When you uninstall Reporting Services, you need to do manual cleanup on some items, as described 
in this section. Before the uninstall, though, you need to gather some information. Make sure 
you know which databases are used by this instance of Reporting Services. You can obtain this 
information using the Reporting Services Conﬁ guration tool. Discover which directory this instance 
of Reporting Services is installed in by running SQL Server Conﬁ guration Manager. You also need 
to discover which directory the Reporting Services usage and log ﬁ les uses.
Uninstalling Reporting Services does not delete the ReportServer databases. You must manually 
delete these, or a new Reporting Services instance can reuse them.
Uninstalling Analysis Services
Uninstalling Analysis Services also requires some manual cleanup. You should always gather some 
information prior to the uninstall. Discover which directory this instance of Analysis Services is 
installed in by running SQL Server Conﬁ guration Manager.
Although the normal uninstall does not leave any databases behind, it does leave all the 
Analysis Services log ﬁ les. The default location is the Analysis Services install directory or the 
Uninstalling SQL Server ❘ 51

52  ❘  CHAPTER 2  INSTALLING SQL SERVER 2012 BEST PRACTICES
alternative location you previously discovered. To delete them, simply delete the appropriate 
directories.
Uninstalling the SQL Server Database Engine
As with other services, log ﬁ les are not deleted when you uninstall the SQL Server Database Engine. 
To delete them, simply delete the appropriate directories. You may need to separately remove the 
MS SQL Server Native Client, and you may ﬁ nd that some directories remain and must be manually 
removed as well. 
If you have no other instances of SQL Server on your machine, instead of deleting only the 100 
directory, under Program Files you can delete the entire MS SQL server directory. The .NET 
Framework is also left on the machine. If you want to remove it, do so from the Programs and 
Features in Control Panel, but make sure no other applications use it. 
TROUBLESHOOTING A FAILED INSTALL
Failed installations may occur most commonly due to failed setup support and installation rules. 
During setup a series of rules are checked to identify issues that may prevent a successful SQL 
Server installation. When a rule failure is detected, it must be corrected before continuing. A rules 
error report link and description is always provided during attended installs, and error log ﬁ les are 
generated for later review.
A detailed report is always available when a failure occurs. These reports provide valuable 
information to help you identify the root of the problem. In many cases, you can ﬁ x these failures by 
installing missing features or applications.
Error reports can be retrieved from the %Program Files%\Microsoft SQL Server\110\Setup 
Bootstrap\Log folder. Each installation attempt generates a time-stamped folder with detailed 
information stored in a log ﬁ le that can help you troubleshoot any errors. For a complete list and 
description of the log ﬁ les generated during Setup, visit http://msdn.microsoft.com/en-us/
library/ms143702(v=sql.110).aspx.
SUMMARY
As you may conclude, installing SQL Server 2012 is generally quite simple and can be performed 
with little or no user interaction. Planning before you initiate an installation is key to a successful 
deployment. A successful SQL Server 2012 install starts with a good plan and a good deﬁ nition of 
requirements. These requirements should deﬁ ne hardware and software requirements, prerequisites, 
authentication, collation, service accounts, ﬁ le locations, and so on.
A successful SQL Server 2012 install does not conclude after the Setup Wizard completes. Several post-
installation tasks require multiple default conﬁ guration settings to be modiﬁ ed such as max memory, 
parallelism thresholds, TCP/IP ports, patches, and more. You can use SQL Server Management Studio 
and SQL Server Conﬁ guration Manager to change these default conﬁ guration options. Database 
servers need to be burned in and stress tested to avoid unexpected behavior under heavy load. 
If you plan to upgrade, Chapter 3 offers some good advice in that area.


54  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Built-in encryption capabilities
Reduced operating system patching with support for Windows Server Core
Accelerated I/O performance with new compression capabilities
Default Schema for Groups
SQL Server Audit for all editions
Contained database authentication
User-deﬁ ned server roles
Risk Mitigation — the Microsoft Contribution
As with all previous versions of SQL Server, the SQL team took extraordinary steps to ensure 
that the quality of SQL Server 2012 is as high-grade as possible. The speciﬁ c steps of the software 
engineering cycle are beyond the scope of this book, but a few points are highlighted here, 
considering public knowledge about the daily build process. 
Today, a daily process produces x86, x64, and Itanium versions of SQL Server 2012 code (called 
builds) that have gone through a battery of tests. This process is utilized for both the development 
of new releases and the development of service packs for SQL Server 2012. These tests are a 
convergence of in-house build tests, customer-captured workloads, and Trustworthy Computing 
processes. Microsoft Research worked on bringing innovations to Microsoft’s products. In the areas 
of software development, the Microsoft research team is an essential contributor to the software 
engineering and testing processes. It improves the test harness with enhancements in several areas, 
including threat modeling, testing efﬁ ciencies, and penetration analysis.
In addition, many customer-captured workloads are also part of the software testing harness. These 
workloads are acquired through an assortment of programs such as the Customer Playback program 
and various lab engagements, including SQL Server 2012 compatibility labs.
The daily builds are tested against this gathered information, and out of this process come 
performance metrics, security metrics, and bugs. Bugs are subsequently ﬁ led, assigned, prioritized, 
and tracked until resolution. After a bug is ﬁ xed, its code goes through security testing as part of the 
software engineering process. This happens before the code is checked back into the software tree for 
the next testing cycle. This rigorous development and quality assurance process helps ensure that the 
shipped product is reliable and ready for production environments. The bottom line is that the old 
adage, “Wait until the ﬁ rst service pack to upgrade,” is no longer true for SQL Server 2012.
Independent Software Vendors and SQL Community 
Contributions
Starting with SQL Server 2005 and continuing with SQL Server 2012, the concept of community 
technology preview (CTP) was adopted. The November 2010 CTP was the ﬁ rst of several such 
releases, in addition to Release Candidate (RC) releases. The decision to adopt this snapshot in 
time of code (or build) resulted in hundreds of thousands of CTP and RC downloads, providing 
➤
➤
➤
➤
➤
➤
➤

unprecedented access to updated code to both independent software vendor (ISV) and SQL 
community testing. At the time of this writing, Microsoft has published ten case studies detailing 
successful implementations of SQL Server 2012. This type of access to beta code was leveraged as 
a means to identify additional bugs, conducting additional testing of software ﬁ xes, and driving 
additional improvements based on community feedback.
UPGRADING TO SQL SERVER 2012
Chapter 2 covers the installation guidelines, so this section mainly focuses on upgrade strategies 
and considerations for the SQL Server 2012 database component. A smooth upgrade requires a 
good plan. When you devise an upgrade plan, you need to break down the upgrade process into 
individual tasks. This plan should have sections for pre-upgrade tasks, upgrade tasks, and post-
upgrade tasks: 
Your pre-upgrade tasks consider SQL Server 2012 minimum hardware and software 
requirements. You should have an inventory of your applications that access the server, 
database-collation requirements, server dependencies, and legacy-systems requirements such 
as data-access methods. Your list should include database consistency checks and backup 
of all databases. Plans should be in place for testing the upgrade process and applications. 
You should have a thorough understanding of backward-compatibility issues and identify 
workarounds or ﬁ xes. You should also use the SQL Server 2012 Upgrade Advisor, as 
described later in this chapter, to assist in identifying and resolving these issues.
The upgrade execution process is a smooth execution of your well-documented and 
rehearsed plan. To reiterate the importance of this step, ensure you make a backup of all the 
databases before you execute the upgrade process.
Post-upgrade tasks consist of reviewing the upgrade process, bringing the systems 
back online, monitoring, and testing the system. You need to perform speciﬁ c database 
maintenance before releasing the system to the user community. These and other 
recommended steps are outlined later in the chapter. Run your database in backward-
compatibility mode after the upgrade to minimize the amount of change to your 
environment. Update the database-compatibility mode as part of a follow-up upgrade 
process and enable new SQL Server 2012 features.
As part of deciding your upgrade strategy, consider both in-place (upgrade) and side-by-side 
migration methods for upgrading.
In-Place Upgrading
The in-place server upgrade is the easier but riskier of the two options. This is an all-or-nothing 
approach to upgrading; meaning that after you initiate the upgrade there is no simple rollback 
procedure. This type of upgrade has the added requirement of greater upfront testing to avoid 
using a complex back-out plan. The beneﬁ t of this approach is that you don’t need to worry about 
users and logins remaining in sync, and database connectivity changes are not be required for 
applications. In addition, SQL Server Agent jobs migrate during the upgrade process.
➤
➤
➤
Upgrading to SQL Server 2012  ❘  55


Upgrading to SQL Server 2012 ❘ 57
Following are the advantages of an in-place upgrade: 
Fast, easy, and automated (best for small systems).
No additional hardware required.
Applications retain same instance name.
Preserves SQL Server 2008 (or 2005) functionality automatically.
The disadvantages of an in-place upgrade are as follows: 
Downtime incurred because the entire SQL Server instance is ofﬂ ine during upgrade.
No support for component-level upgrades.
Complex rollback strategy.
Backward-compatibility issues must be addressed for that SQL instance.
In-place upgrade is not supported for all SQL Server components.
Large databases require substantial rollback time.
Additionally, if you would like to change editions as a part of your upgrade, you must be aware of 
some limitations. You can upgrade SQL Server 2005 and 2008 Enterprise, Developer, Standard, and 
Workgroup editions to different editions of SQL Server 2012. However, SQL Server 2005 and 2008 
Express Editions may only be upgraded to SQL Server 2012 Express Edition. If this is of interest to 
you, see SQL Server 2012 Books Online (BOL), “Version and Edition Upgrades,” under the section 
“Upgrading to SQL Server 2012.” 
Side-by-Side Upgrade
In a side-by-side upgrade, SQL Server 2012 installs either along with SQL Server 2008 (or 2005) as 
a separate instance or on a different server. This process is essentially a new installation followed 
by a database migration. You may want to select this option as part of a hardware refresh or 
migration to a new platform, such as Itanium or x64. Because of the backup and restore times 
involved in a back-out scenario, if you have a sizable database, this is deﬁ nitely the option to use.
As part of this method, you can simply back up the databases from the original server and then 
restore them to the SQL Server 2012 instance. Other options are to manually detach your database 
from the old instance and reattach it to the new instance, use log shipping, or database mirroring. 
You can also leverage the Copy Database Wizard to migrate your databases to the new server. 
Although this approach provides for a good recovery scenario, it has additional requirements 
beyond those of the in-place upgrade, such as maintaining the original server name, caring for 
application connectivity, and keeping users and their logins in sync. 
Following are the arguments in favor of a side-by-side upgrade: 
More granular control over upgrade component-level process (database, Analysis Services, 
and others)
Ability to run SQL Servers side-by-side for testing and veriﬁ cation
Ability to gather real matrix for upgrade (outage window)
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

58  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Rollback strategy because original server is still intact
Best for large databases because restore time could be sizable
The arguments against a side-by-side upgrade are as follows: 
Does not preserve SQL Server 2008 (or 2005) functionality.
Issue of instance name for connecting applications.
In-Place Upgrade versus Side-By-Side Upgrade Considerations
Consider numerous factors before selecting an upgrade strategy. Your strategy should include the 
need for a component-level upgrade, the ability to roll back in case of failure, the size of your 
databases, and the need for partial upgrade. Your top priorities might depend upon if you can upgrade 
to new hardware, facilitate a change of strategy such as a server consolidation, and manage a small 
server outage window for the upgrade. Table 3-1 shows a summary of the two upgrade methods.
➤
➤
➤
➤
TABLE 3-1: In-Place and Side-by-Side Upgrade Comparison
PROCESS
IN-PLACE UPGRADE
SIDE-BY-SIDE UPGRADE
Number of resulting instances
One
Two
Data ﬁ le transfer
Automatic
Manual
SQL Server instance conﬁ guration
Automatic
Manual
Supporting upgrade utility
SQL Server setup
Various migration and data 
transfer methods
PRE-UPGRADE STEPS AND TOOLS
Now that you understand the reasons and options for upgrading to SQL Server 2012, you can move 
on to choosing your upgrade tools to assist in the upgrade process and performing pre-upgrade steps. 
Prior to the upgrade process, you can take preventative measures to avoid common upgrade issues, 
such as running out of disk space or executing startup stored procedures during the upgrade. There 
are also a number of tools that can help identify potential upgrade issues in your environment. The 
two most useful tools to aid in this process are the SQL Server Upgrade Advisor and the SQL Server 
Upgrade Assistant. These tools both provide pre-upgrade analysis and help you gain conﬁ dence that 
your upgrade will run successfully. Upgrade Assistant uses workload testing to test post-upgrade 
application behavior, while Upgrade Advisor performs in-place analysis of your databases for 
potential compatibility issues. 
Pre-Upgrade Steps
There are a number of steps to take prior to performing the upgrade process. These precautions and 
preventative measures help eliminate nasty surprises during upgrade. 
Set your data and log ﬁ les to autogrow during the upgrade process.  
➤



Pre-Upgrade Steps and Tools ❘ 61
Using the Upgrade Advisor
When installed, the Upgrade Advisor presents you with two choices, Upgrade Advisor Analysis 
Wizard and Upgrade Advisor Report Viewer. Launch the Upgrade Advisor Analysis Wizard to 
run the tool. As shown in Figure 3-3, you simply select a server and the components to analyze for 
upgrade, or you can click the Detect button, which starts the inspection process that selects the 
components installed on your system.
After you select the components for testing, the next decision is to select the databases that you 
would like to have evaluated for upgrade, as shown in Figure 3-4. The best part of this process 
is that you have the option to analyze SQL Proﬁ ler trace and SQL batch ﬁ les to help make this a 
comprehensive analysis. That is, by adding these ﬁ les to the evaluation process, Upgrade Advisor 
evaluates not only the database but its trace workload and SQL scripts as well. By evaluating this 
additional information, Upgrade Advisor evaluates not only the database as it exists right now, but 
also information about past database usage and behavior contained in the trace and batch ﬁ les. All 
you need to do is select the path to the directory where your trace ﬁ les or your batch ﬁ les are located.
FIGURE 3-3
FIGURE 3-4
After you complete conﬁ guration of the components that you want to evaluate, you will be 
prompted to begin the analysis. If you have any questions during the conﬁ guration steps, the Help 
button brings up an Upgrade Advisor-speciﬁ c Book Online (UABOL) that is rich in information and 
guides you through the options. As the component-level analysis completes, a green, yellow, or red 
dialog box indicates the outcome of the test.
When the test completes, you can view the discovered issues via the Upgrade Advisor Report 
Viewer. The reports, as shown in Figure 3-5, are presented in an interface similar to a Web browser. 
You can analyze the information by ﬁ ltering the report presented by server, instance, or component, 
or issue type. How to interpret the results of this report is discussed later in this chapter.

62  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Scripting the Upgrade Advisor
If you have a server farm or just prefer scripting, a command-line capability is also available. 
With the UpgradeAdvisorWizardCmd utility, you can conﬁ gure the tool via an XML 
conﬁ guration ﬁ le and receive results as XML ﬁ les. The following parameters can be passed to the 
UpgradeAdvisorWizardCmd utility: 
Command-line help
The conﬁ guration path and ﬁ lename
The SQL Server login and password for SQL Server (if SQL Server authentication is used, 
rather than Windows authentication)
An optional ﬂ ag to indicate whether to output reports in a comma-separated value (CSV) 
format
The conﬁ guration ﬁ le exposes all capabilities and parameters discussed in the wizard section. 
The results from a command-line execution can still be viewed in the Report Viewer, via XML 
documents or Excel if you use the CSV option. For example, the following XML document 
➤
➤
➤
➤
FIGURE 3-5

Pre-Upgrade Steps and Tools ❘ 63
from Upgrade Advisor reﬂ ects the choices of analyzing all databases, Analysis Services, and SSIS 
packages on a server named SQL12Demo and an instance named SQL2012: 
<Configuration>
   <Server>SQL12Demo</Server>
   <Instance>SQL2012</Instance>
  <Components>
     <SQLServer>
        <Databases>
          <Database>*</Database>
        </Databases>
     </SQLServer>
   </Components>
 </Configuration>
You can modify the ﬁ le in an XML editor, such as Visual Studio, and save the ﬁ le with a new 
ﬁ lename. Then, you can use the new ﬁ le as input to the command line of Upgrade Advisor. For 
example, the following code snippet displays the command prompt entry required to run the 
command line of Upgrade Advisor using Windows authentication. The conﬁ guration ﬁ le already 
contains names for a remote server named SQL2012 and an instance named SQL2012, and the 
PATH environment variable contains the path to the Upgrade Wizard: 
 C:\>UpgradeAdvisorWizardCmd -ConfigFile “SQL2012Config.xml”
From the command prompt, you can also install or remove the Upgrade Advisor application. From 
there you can control the install process with or without the UI. You can also conﬁ gure the install 
path and process-logging options.
For more information on the Upgrade Advisor’s conﬁ guration ﬁ les, see the Upgrade Advisor Help 
section “UpgradeAdvisorWizardCmd Utility.”
Resolving Upgrade Issues
The Upgrade Advisor’s report contains a wealth of information. The key is to understand how this 
information appears, what you need to resolve, and when. As shown previously in Figure 3-5, the 
ﬁ rst column indicates the importance of a ﬁ nding or a recommendation, the second column tells you 
when you need to address it, and the Description column tells you about the issue. Approach this 
analysis by ﬁ rst categorizing the information by Importance and When to Fix the items. Speciﬁ cally, 
the sum of the indicators should dictate whether you need to address issues before or after the 
upgrade process. Table 3-2 provides recommendations of when to address these issues. 
TABLE 3-2: When to Address Upgrade Issues
IMPORTANCE
WHEN TO FIX
OUR RECOMMENDATION
Red
Before
Resolve Before Upgrade
Red
Anytime
Resolve Before Upgrade
Red
After
Resolve After Upgrade
Yellow
Anytime
Resolve After Upgrade
Yellow
After
Resolve After Upgrade
Yellow
Advisory
Resolve After Upgrade

64  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Issues that have been ﬂ agged with an Importance of Red, and a When to Fix of Before or Anytime 
should be addressed before starting an upgrade process. Typically, these issues require remediation 
because of SQL Server 2012 functionality changes, such as discontinued features. You can usually 
resolve the remaining issues after the upgrade process because they either have a workaround 
within the upgrade process or do not affect it at all. If you expand the error in question, additional 
information appears, as shown in Figure 3-6.
FIGURE 3-6
The Show Affected Objects link shows the exact objects ﬂ agged by the Upgrade Advisor process as 
affected, whereas the Tell Me More About This Issue and How to Resolve It link takes you to the 
corresponding section of the Upgrade Advisor Books Online (UABOL). The UABOL describes the 
conditions and provides guidance about corrective action to address the issue. The UABOL is a true 
gem because it provides guidance for problem resolution in areas beyond the scope of the tools (such 
as replication, SQL Server Agent, and Full-Text Search).
The This Issue Has Been Resolved check mark is for your personal tracking of resolved issues. 
This metadata check mark is in place to support remediation processes by enabling the report to be 
viewed by ﬁ ltered status of resolved issues or pre-upgrade (unresolved) issues.
If you prefer command-line scripting, the viewer is nothing more than an XSLT transformation 
applied to the XML result ﬁ le located in your My Documents\SQL Server 2012 Upgrade Advisor Reports\ 
directory. You can ﬁ nd individual component results and conﬁ guration ﬁ les in each server’s name-based 
directories. You can even export viewer-based reports to other output formats such as CSV or text.

Pre-Upgrade Steps and Tools ❘ 65
Upgrade Assistant for SQL Server 2012 (UAFS)
UAFS was ﬁ rst developed for use in the SQL Server 2005 application-compatibility lab engagements 
run as part of the Microsoft Ascend (SQL 2005 customer training) and Touchdown (SQL 2005 
partner training) programs. The purpose of these labs was to help customers analyze their SQL 
Server 2000 (or 7.0) applications to understand the impact of upgrading to SQL Server 2005 and 
to provide guidance on any changes that may be necessary to successfully migrate both their 
database and their application. The labs helped improve the quality of SQL Server 2005 by running 
upgrades and performing impact analysis on real customer workloads against SQL Server 2005. 
The labs were run by Microsoft personnel and staffed by partners such as Scalability Experts. 
Nearly 50 labs were run worldwide, and hundreds of SQL Server 2000 applications were tested 
using this tool. A new version of UAFS was developed speciﬁ cally for SQL Server 2008 and has 
been updated for 2012; you can download it free from www.scalabilityexperts.com/tools/
downloads.html.
From a conceptual standpoint, the difference between this tool and Upgrade Advisor is that UAFS 
naturally encompasses the essence of a true upgrade and testing methodology. By reviewing the results 
of a SQL Server 2008 (or 2005) workload against the results of the same workload run against SQL 
Server 2012, you can identify upgrade blockers and application-coding changes that may be required. 
For SQL Server 2012, the UAFS tool supports upgrading from SQL Server 2008 and 2005.
The following sections walk through an overview of this process to show details of the steps 
contained in the UAFS, as shown in Figure 3-7. By using the UAFS, you can back up all databases 
and users and capture a subset of production workload. You can then restore the databases and 
users you just backed up and process the captured workload. The goal is to develop a new output 
ﬁ le, also known as a baseline. You then upgrade the test server to SQL Server 2012 and rerun the 
workload to capture a SQL Server 2012 reference output for comparison.
FIGURE 3-7

66  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Capturing the Environment
You should establish your baseline by backing up all SQL Server 2008 (or 2005) systems and user 
databases from your server. Following this step, you need to start capturing your trace ﬁ le to 
avoid gaps in the process. When you capture a trace ﬁ le, it needs to be a good representation of 
the workloads that characterize your environment. To do this, you might need to create an artiﬁ cial 
workload that better represents the workloads of the application over time. While capturing the 
trace ﬁ le, it’s a good idea to avoid multiserver operations such as linked server calls or bulk-copy 
operation dependencies. Be aware that there is a performance cost while tracing. The sum of the 
trace ﬁ les and database backups represent a repeatable and reusable workload called a playback.
Setting Up the Baseline Server
Now that you have captured the playback, you can set up the baseline system to use for the 
remainder of the test. Load this server with SQL Server 2008 R2 SP1, 2008 SP2, or SQL Server 
2005 SP2, with the minimum requirement for upgrading to 2012. In reality, it should be identical 
to the source system in collation and patching level. The tool then checks your server for this 
matching. If necessary, you are prompted to patch or rebuild the master database. It then restores 
your databases in the correct order so that your DB IDs match to production. (This also includes 
padding the DB creation process to accomplish this.) Finally, SSUA re-creates your logins and 
ensures that the IDs match production because all this is necessary to run the trace ﬁ le. The next 
step in the process is to run the Upgrade Advisor as described earlier. When the environment has 
been remediated, you can then proceed to the next step to replay the trace.
Running the SQL Proﬁ ler Trace
When you run the trace, ﬁ rst the statistics update on all databases. The replay tool then uses 
the API and runs all the queries within the trace ﬁ le in order. This tool is a single-threaded replay, 
but blocking can occur. If the trace appears to run slowly or stop, you may want to check SQL 
Server blocking; and if it does not clear up by itself, you need to kill the blocking processes. The 
output from this step generates a trace-output ﬁ le for comparison in the ﬁ nal analysis.
Upgrading to SQL Server 2012
Now you are ready to upgrade to SQL Server 2012. You have two options. You can use SSUA to 
restore the state of the SQL Server 2008 (or 2005) to its baseline and then upgrade in-place to 
SQL Server 2012, or you can migrate the SQL 2008 (or 2005) databases to an existing SQL Server 
2012 instance. As discussed earlier in this chapter, the decision to perform an in-place or side-by-
side upgrade is based on a number of factors speciﬁ c to your environment. You do not measure 
performance metrics, so these servers don’t need to be identical. You measure workload behavior 
between two versions of SQL Server. After restoring the baseline on a SQL Server 2012 platform, 
go through the Running the SQL Proﬁ ler Trace step again, but this time on SQL Server 2012. The 
output from this step generates the other trace-output ﬁ le for comparison in the ﬁ nal analysis.
Final Analysis
After completing all these processes, you will reach the ﬁ nal steps to compare the output ﬁ les by 
ﬁ ltering and comparing all batches in both trace ﬁ les for discrepancies. The Report Viewer shows 
one error condition at a time by showing the last correct step, the error step, and the next correct 
sequences of the batch ﬁ les. When a condition has been identiﬁ ed, it can be ﬁ ltered from the error-

reviewing process to enable the DBA to focus on identifying new error conditions. After the SQL 
Server 2012 upgrade completes, change the database compatibility mode to 110, and run your 
application to validate that it works in SQL Server 2012 compatibility mode. This ensures that 
no application behavior differences exist when your application runs on the database compatibility 
level of 110.
BACKWARD COMPATIBILITY
This section covers major product changes to SQL Server 2012 classiﬁ ed in one of three categories: 
unsupported, discontinued, or affecting the way SQL Server 2008 or 2005 behaves today. Although 
the Upgrade Advisor tool highlights these conditions if they are relevant to your environment, you 
should read this section to learn about these changes.
Unsupported and Discontinued Features in SQL Server 2012
From time to time, to move a technology forward, trade-offs must be made. From SQL 2008 to SQL 
Server 2012, the following lists some of the features no longer available: 
The system stored procedures sp_ActiveDirectory_Obj, sp_ActiveDirectory_SCP, and 
sp_ActiveDirectory_Start.
sp_configure options user instance timeout and user instances enabled.
Support for the VIA protocol. 
SQL Mail (Use Database Mail instead.)
The creation of new remote servers using sp_addserver (Use Linked Servers instead.)
Database compatibility level 80.
RESTORE {DATABASE | LOG} … WITH DBO_ONLY (Use the WITH RESTRICTED USER clause 
instead.)
This is a limited list of discontinued features. For a complete list of discontinued and deprecated 
features, go to http://msdn.microsoft.com/en-us/library/cc280407(v=SQL.110).aspx.
SQL Server 2012 Deprecated Database Features
These features are no longer available as of the SQL Server 2012 release or the next scheduled 
release of the product. Following are some of the features scheduled for deprecation; try to replace 
these features over time with the recommended items: 
SOAP/HTTP endpoints created with CREATE ENDPOINT and ALTER ENDPOINT (They have 
been replaced with Windows Communication Framework (WCF) or ASP.NET.)
The compatibility level 90 will not be available after SQL Server 2012.
Encryption using RC4 or RC4_128, is scheduled to be removed in the next version. 
Consider moving to another encryption algorithm such as AES.  
Not ending T-SQL statements with a semicolon will no longer be supported in a future 
version of SQL Server.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
Backward Compatibility ❘ 67
 
 
 
 

68  ❘  CHAPTER 3  UPGRADING SQL SERVER 2012 BEST PRACTICES
Other SQL Server 2012 Changes Af ecting Behavior
The behavior changes in the following features could adversely affect migration to SQL Server 2012: 
If you create a new job by copying the script from an existing job, the new job might 
inadvertently affect the existing job. This is because the parameter @schedule_uid should 
not be duplicated. Manually delete it in the script for the new job.
Deterministic scalar-valued CLR user-deﬁ ned functions and deterministic methods of 
CLR user-deﬁ ned types are now foldable. This seeks to enhance performance when these 
functions or methods are called more than once with the same arguments. However, if a 
nondeterministic function or method has been marked deterministic in error, it can create 
unexpected results.
When a database with a partitioned index upgrades, there may be a difference in the 
histogram data for these indexes. This is because SQL Server 2012 uses the default sampling 
algorithm to generate statistics rather than a full scan.
 Using sqlcmd.exe with XML Mode behaves differently in SQL Server 2012.
For additional behavior changes, see SQL Server 2012 Books Online or go to http://msdn
.microsoft.com/en-us/library/cc707785(v=SQL.110).aspx.
SQL SERVER COMPONENT CONSIDERATIONS
This section discusses individual components, along with any respective considerations that you 
need to evaluate during an upgrade process. Components not covered here are covered later in the 
book in their respective chapters.
Upgrading Full-Text Catalog 
During the upgrade process, all databases with Full-Text Catalog are marked Full-Text disabled. 
This is because of the potential time involved in rebuilding the catalog. Before you upgrade your 
Full-Text Search environment, you should familiarize yourself with some of the enhancements. The 
database attach and detach processes also result in the Full-Text Catalog being marked as Full-Text 
Disabled. You can read SQL Server 2012 Books Online to learn about additional behavior.
Upgrading Reporting Services
Reporting Services 2008 and Reporting Services 2005 support upgrading to Reporting Services 
2012. Reporting Services 2012 supports a Report database on SQL Server. Prior to upgrading, 
run the SQL Server 2012 Upgrade Advisor and follow its recommendations, guidance on 
possible mitigation options, and steps. Then, before executing the upgrade, back up the database, 
applications, conﬁ gurations ﬁ les, and the encryption key.
Following are two ways to upgrade Reporting Services: 
In-place upgrade: You can accomplish this by executing the SQL Server 2012 setup.exe; 
select the older Reporting Services to upgrade with Reporting Services 2012. This has the 
➤
➤
➤
➤
➤




72  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
administration screen. It also reduces the options that a hacker or, more likely, a malicious user 
can use to penetrate your system. As a result of these security measures, there is even more reason 
to invest time in conﬁ guring SQL Server 2012 towards your speciﬁ c needs. In this section, you 
learn how to conﬁ gure SQL Server for your speciﬁ c environment and security needs in a few ways: 
by using SQL Server Conﬁ guration Manager, startup parameters, startup stored procedures, and 
partially contained databases. SQL Server Conﬁ guration Manager is the best place to start.
SQL Server Conﬁ guration Manager
The SQL Server Conﬁ guration Manager conﬁ gures the SQL Server services much like the Services 
applet in the Control Panel, but it has much more functionality than the applet. For example, the 
program can also change what ports SQL Server listens on and what protocols each instance uses. 
You can open the program (see Figure 4-1) from Start Á All Programs Á Microsoft SQL Server 
2012 Á Conﬁ guration Tools Á SQL Server Conﬁ guration Manager.
FIGURE 4-1
To start conﬁ guring SQL server to ﬁ t your environment’s needs, follow these steps: 
 1. 
Select Start Á Microsoft SQL Server 2012 Á Conﬁ guration Tools Á SQL Server 
Conﬁ guration Manager Á SQL Server Services. 
 2. 
To conﬁ gure an individual service such as SQL Server, double-click the service name to 
open the service Properties page. In the Log On tab, you can conﬁ gure which account starts 
SQL Server. You should start SQL Server with a regular domain user account with minimal 
rights. The account should not have the privilege to Log on Locally, for example. There is 
no reason for the account to be a local or domain administrator in SQL Server 2012.
 3. 
Next, create a non-expiring password so your SQL Server doesn't fail to start when the 
password expires. If the SQL Server services do not need to communicate outside the 
instance's machine, you could start the service with the Local System account, but the 
account may have more local rights than you want. (This is discussed more in Chapter 8, 
“Securing the Database Instance.”) 




76  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
If you have a server that uses few extended stored procedures, distributed queries, or 
automation objects, you may want to use this switch with a value of less than 256 to reduce 
the amount of memory reserved for these objects. The memory would then be available 
for the general memory pool.
–m switch: This switch puts SQL Server in single-user mode (sometimes called master 
recovery mode) and suspends the CHECKPOINT process, which writes data from disk to the 
database device. This switch is useful when you want to recover the master database from a 
backup or perform other emergency maintenance procedures. 
-k switch: This switch is used to inﬂ uence the checkpoint frequency. It forces the 
regeneration of the system master key if one exists. Use this with extreme care, and only 
if directed by PSS. 
-s switch: This switch starts a named instance of SQL Server. When you start SQL Server 
from the command prompt, the default instance starts unless you switch to the appropriate 
BINN directory for the instance and provide the -s switch. For example, if your instance 
is named SQL2012, you should be in the C:\Program Files\Microsoft SQL Server\
MSSQL11.SQL2012\MSSQL\Binn directory and issue the following command:  
sqlserver.exe -sSQL2012
-c switch: This switch enables you to decrease startup time when starting SQL Server from 
a command prompt by taking advantage of the fact that the SQL Server Database Engine 
does not start as a service when starting from the command prompt. This switch bypasses 
the Service Control Manager, which is unnecessary in this situation.  
You can obtain a complete list of switches by using the -? switch, as shown in Table 4-1 (complete 
details appear in the SQL Server documentation): 
 sqlservr.exe -?
➤
➤
➤
➤
TABLE 4-1 : Startup Parameters
SWITCH
PURPOSE
-d
Deﬁ nes the data ﬁ le for the master database
-l
Deﬁ nes the log ﬁ le for the master database
-T
Enables you to start given trace ﬂ ags for all connections of a SQL Server instance
-f
Places SQL Server in minimal mode an enables a single connection
-g
Reserves additional memory outside SQL Server’s main memory pool for extended 
stored procedures
-m
Puts SQL Server in single-user mode (master recovery mode) and suspends the 
CHECKPOINT process
-k
Forces the regeneration of the system master key if one exists
-s
Speciﬁ es which named instance to start 
-c
Bypasses the Service Control Manager, which is unnecessary when starting SQL 
Server from a command prompt
-?
Returns a complete list of switches







Management Studio ❘ 83
the server level is the Server Dashboard, which is shown in Figure 4-4. The Server Dashboard report 
gives you a wealth of information about your SQL Server 2012 instance, including:
The edition and version of SQL Server you run
Anything for that instance not conﬁ gured to the default SQL Server settings
The I/O and CPU statistics by type of activity (for example, ad hoc queries, Reporting 
Services, and so on)
High-level conﬁ guration information such as whether the instance is clustered
➤
➤
➤
➤
FIGURE 4-4
Most of the statistical information includes only data gathered since the last time you started SQL 
Server. For example, the Server Dashboard provides a few graphs that show CPU usage by type 
of query. This graph is not historical; it shows you the CPU usage only for the period of time that 
SQL Server has been online. Use caution when extrapolating information from this aspect of the 
Server Dashboard. Always keep in mind that what you see is a time-sensitive snapshot of server 
performance, not its entire history.  

84  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
Database Reports
Database reports operate much like server-level reports. Select them by right-clicking the database 
name in the Object Explorer window in Management Studio. With these reports, you can see 
information that pertains to the database you selected. For example, you can see all the transactions 
currently running against a database, users being blocked, or disk utilization for a given database, 
as shown in Figure 4-5.
Object Explorer Details
The Object Explorer Details Pane provides a wealth of information in a consolidated GUI. You can 
access this feature in two ways. From the View menu, select Object Explorer Details. Alternatively, 
F7 opens the pane. If you have highlighted a node in Object Explorer, F7 opens the details for that 
object, as shown in Figure 4-6. The Synchronize button on the Object Explorer Pane synchronizes 
Object Explorer to Object Explorer Details. 
The pane can be ﬁ ltered and customized to your precise needs. Clicking on a column header sorts by 
that column, and right-clicking on the column header area gives you the opportunity to ﬁ lter which 
details display. 
FIGURE 4-5



Management Studio ❘ 87
Processors
In the Processors page, you can restrict the SQL Server Engine to use named processors and assign 
some or all those processors to I/O or threading operations. This is useful typically if you have 
multiple CPUs and more than one instance of SQL Server. You may have one instance that uses 
four processors and the other instance use the other four processors. In some cases, when you have 
a large number of concurrent connections to your SQL Server, you may want to set the Maximum 
Worker Threads option. Conﬁ gure this to 0 (the default) to enable SQL Server to automatically and 
dynamically ﬁ nd the best number of threads to enable on the processor. These threads can manage 
connections and other system functions, such as performing CHECKPOINTs. Generally, leaving this 
setting alone gives you optimal performance.
You can select the SQL Server Priority option to force Windows to assign a higher priority to the 
SQL Server process. Tweaking this setting may be tempting, but you should adjust it only after 
thorough testing because it may starve other system threads. Use of this option is not generally 
recommended unless directed by PSS.
Use the Lightweight Pooling option only on the rare occasion when the processor is highly utilized 
and context switching is extreme. Chapter 12, “Monitoring Your SQL Server,” contains more 
information on lightweight pooling and how it works.
FIGURE 4-8



90  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
Filtering Objects
You can also ﬁ lter objects in Management Studio by following a few easy steps, which is useful 
when you begin to have dozens of objects: 
 1. 
Select the node of the tree that you want to ﬁ lter, and click the Filter icon in the Object 
Explorer. 
 2. 
The Object Explorer Filter Settings dialog (shown in Figure 4-9) opens; here you can ﬁ lter 
by name, schema, or when the object was created. 
 3. 
Use the Operator drop-down box to select how you want to ﬁ lter, and then type the name 
in the Value column.
FIGURE 4-9
Error Logs
As you probably have already experienced, when something goes wrong with an application, one of 
the factors that you must consider is the database. It is up to the DBA to support the troubleshooting 
effort and to conﬁ rm that the database isn’t the source of the problem. The ﬁ rst thing the DBA 
typically does is connect to the server and look at the SQL Server instance error logs and then the 
Windows event logs.
In SQL Server 2012, you can quickly look through the logs in a consolidated manner using 
Management Studio. To view the logs, right-click SQL Server Logs under the Management tree, 
and select View Á SQL Server and Windows Log. This opens the Log File Viewer screen. From this 
screen, you can check and uncheck log ﬁ les that you want to bring into the view. You can consolidate 
logs from SQL Server, Agent, and the Windows Event Files, as shown in Figure 4-10.

Management Studio ❘ 91
In some situations, you may want to merge the logs from several machines into a single view to 
determine what’s causing an application problem. To do this, click the Load Log button, and browse 
to your .LOG ﬁ le. That ﬁ le could be a Windows error log that has been output to .LOG format 
or a SQL log from a different instance. For example, you can use this to consolidate all the SQL 
logs from every instance on a single server to give you a holistic view of all the physical machines’ 
problems.
Activity Monitor
The Activity Monitor gives you a view of current connections on an instance. You can use the 
monitor to determine whether you have any processes blocking other processes. To open the 
Activity Monitor in Management Studio, right-click the Server in the Object Explorer, and then 
select Activity Monitor.
The tool is a comprehensive way to view who connects to your machine and what they do. The top 
section shows four graphs (Show Processor Time, Waiting Tasks, Database I/O, and Batch Requests/
Sec) that are commonly used performance counters for the server. There are four lists under the 
graphs: Processes, Resource Waits, Data File I/O, and Recent Expensive Queries. In all these lists, 
you can also apply ﬁ lters to show only certain hosts, logins, or connections using greater than a 
given number of resources. You can also sort by a given column by clicking the column header.
FIGURE 4-10

92  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
On the Process Info page (shown in Figure 4-11), you can see each login connecting to your machine 
(also called a Server Process ID, or SPID). It’s easy to miss how much information is in this window. 
You can slide left to right to see loads of important data about each connection. When debugging, 
most of the following columns are useful:
FIGURE 4-11
Session ID: The unique number assigned to a process connected to SQL Server. This is also 
called a SPID. An icon next to the number represents what happens in the connection. If 
you see an hourglass, you can quickly tell that the process is waiting on or is being blocked 
by another connection. 
User Process Flag: Indicates whether internal SQL Server processes are connected. 
These processes are ﬁ ltered out by default. You can change the value to see the SQL Server 
internal processes by clicking the drop-down and selecting the appropriate value.
Login: The login to which the process is tied.
Database: The current database context for the connection.
Task State: Indicates whether the user is active or sleeping. 
Done: Completed.
Pending: The process is waiting for a worker thread.
Runnable: The process has previously been active, has a connection, but has no 
work to do.
Running: The process is currently performing work.
Suspended: The process has work to do, but it has been stopped. You can ﬁ nd addi-
tional information about why the process is suspended in the Wait Type column.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤


94  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
This query was intentionally not committed. In other words, there is a BEGIN TRAN com-
mand but no ROLLBACK or COMMIT command. This means that the rows deleted from 
Production.ProductCostHistory are still locked exclusively.
 2. 
Next, without closing the ﬁ rst window, open a new query window, and run the following 
query:
SELECT * FROM Production.ProductCostHistory;
This query should hang up and wait on the DELETE from Production.ProductCostHistory 
because the ﬁ rst transaction locks row one. Do not close either window. At the top of 
each query window, your session ID displays in parentheses, and at the bottom your login 
displays. If you cannot see the SPID in the tab at the top, hover your mouse over the tab, 
and a small window pops up showing the entire tab title, which includes the SPID. While 
the query windows are open, go ahead and explore the Activity Monitor to see what these 
connections look like (Figure 4-12).
FIGURE 4-12
 3. 
Open the Activity Monitor and note that one connection has a task state of Suspended. 
This is the query that is trying to do the SELECT. You can conﬁ rm this by comparing the 
session ID of the suspended session with the session ID of your query. Your wait type will 
be LCK_M_S, which means you are waiting on a shared lock for reading. If you hover the 
mouse over the Wait Resource column value, you can see more detailed information about 
the locks, including the object IDs of the resources. In the Blocked By column, you can 
also see the session ID of the process that blocks you, and it should match the SPID of your 
ﬁ rst query. You have the option to kill the blocking process; to do so, select the row for the 
blocking process, right-click, and choose Kill Process. 








102  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
This directory may vary, though, based on where you installed the SQL Server tools. To create a 
dump ﬁ le for support, go to a command prompt, and access the C:\Program Files\Microsoft 
SQL Server\110\Shared directory. As with many command-line utilities, you can see the options 
by running the following command, and get more details about them from the SQL Server 
documentation: 
 SQLdumper.exe -?
 
When you are at the command line, you can create a full dump or a minidump. If a minidump is 
less than a megabyte, a full dump may run 110MB on your system. To create a full dump, use the 
following command: 
 Sqldumper.exe <ProcessID> 0 0x1100
<ProcessID> is the Process ID of your SQL instance. This outputs the full dump to the same 
directory that you’re in. The ﬁ lename is called SQLDmpr0001.mdmp if this is the ﬁ rst time you’ve run 
the SQLDumper.exe program. Filenames are sequentially named after each execution. You cannot 
open the dump ﬁ le in a text editor such as Notepad. Instead, you need advanced troubleshooting 
tools such as Visual Studio or one of the PSS tools. A more practical dump would be a minidump, 
which contains most of the essential information that product support needs. To create a minidump, 
use the following command: 
 Sqldumper.exe <ProcessID> 0 0x0120
You can view the SQLDUMPER_ERRORLOG.log ﬁ le to determine whether there were any errors 
when you created the dump ﬁ le or whether a dump occurred. You need to be a local Windows 
administrator to run SQLDumper.exe or be logged in with the same account that starts the SQL 
Server service.
SQLDiag.exe
A tool that’s slightly less of a black box than SQLDumper.exe is SQLDiag.exe. This tool consolidates 
and collects information about your system from several sources: 
Windows System Monitor (sysmon)
Windows event logs
SQL Server Proﬁ le traces
SQL Server error logs
Information about SQL Server blocking
SQL Server conﬁ guration information
Because SQLDiag.exe gathers so much diagnostic information, you should run it only when you’re 
requested to or when you prepare for a call with support. The SQL Server Proﬁ ler trace ﬁ les alone 
can quickly grow large, so prepare to output these ﬁ les to a drive that has a lot of space. The process 
also uses a sizable amount of processing power as it runs. You can execute the tool from a command 
prompt or as a service; you can use the -? switch to see available switches.
➤
➤
➤
➤
➤
➤


104  ❘  CHAPTER 4  MANAGING AND TROUBLESHOOTING THE DATABASE ENGINE
 5. 
With the SQLDiag.exe now complete, go to the C:\Temp directory to zip the contents up 
and send them to Microsoft. In the directory, you can ﬁ nd a treasure chest of information 
for a support individual, including the following: 
##files.txt: A list of ﬁ les in the C:\Program Files\Microsoft SQL 
Server\110\Tools\binn directory, with their creation date. Use this to determine 
whether you’re running a patch that support has asked to be installed.
##envvars.txt: A list of all the environment variables for the server.
SERVERNAME__sp_sqldiag_Shutdown.OUT: A consolidation of the instance’s SQL 
logs and the results from a number of queries.
log_XX.trc: A series of Proﬁ ler trace ﬁ les of granular SQL Server activities being 
performed.
SERVERNAME_MSINFO32.TXT: A myriad details about the server system and 
hardware.
These ﬁ les are not only useful to support individuals. You also may want to consider running this 
on a regular basis to establish a baseline of your server during key times (before patches, monthly, 
or whatever your metric is). If you decided to do this, you wouldn’t want the Proﬁ ler part of 
SQLDiag.exe to run for more than a few seconds. You can gather useful baseline information if the 
tool periodically runs in snapshot mode. This mode performs the same functions just described but 
exits immediately after it gathers the necessary information. The following command uses the /X 
switch to run SQLDiag.exe in snapshot mode and the /N switch (with 2 as the option) to create a 
new directory for each run of SQLDiag.exe: 
 sqldiag /OC:\temp\baseline /X /N 2
The ﬁ rst directory created is called baseline_0000, and each new one is named sequentially after 
that. Many corporations choose to run this through SQL Agent or Task Manager on the ﬁ rst of the 
month or before key changes to have an automatic baseline of their server and instance.
SUMMARY
One of the most important things to remember when using SQL server is managing and 
troubleshooting your SQL Server. The key concepts for doing so are proper conﬁ guration, 
ongoing monitoring, and efﬁ cient troubleshooting. Methods for conﬁ guration include SQL Server 
Conﬁ guration Manager, startup parameters, and startup stored procedures. It is also important to use 
Management Studio and T-SQL to actively monitor your databases. The DAC provides an excellent 
way to connect to your instance and troubleshoot when issues arise. When you’ve exhausted your 
troubleshooting options, know how to provide the information that product support needs by using 
SQLDumper.exe and SQLDiag.exe. In the next chapter, you learn ways to automate SQL server. 
➤
➤
➤
➤
➤


106  ❘  CHAPTER 5  AUTOMATING SQL SERVER
MAINTENANCE PLANS
Maintenance Plans are a quick-and-easy way to automate routine maintenance tasks in SQL Server. 
They are no more than a user interface on top of regular SQL Server Agent jobs. However, the 
tasks in a plan aren’t equivalent to job steps because Maintenance Plans are built using SQL Server 
Integration Services (SSIS), and these are then run as a single SSIS job step in a job that maps to the 
Maintenance Plan name. For routine maintenance tasks, Maintenance Plans may be all that you 
need to automate on many SQL Servers.
There are two ways to create and maintain Maintenance Plans. The quick and easy way is to use the 
Maintenance Plan Wizard, and the manual way is to use the Maintenance Plan Designer.
Maintenance Plan Wizard
This section walks you through the steps to create a backup using the Maintenance Plan Wizard: 
 1. 
First, launch the wizard, which lives on the context 
menu on the Maintenance Plans node in the Object 
Explorer in SQL Server Management Studio. Select the 
Maintenance Plans Wizard menu item to launch the ﬁ rst 
page of the wizard. Figure 5-1 shows the menu selection 
to start the Maintenance Plan Wizard.
You can opt to not show this page again and then select 
Next. This brings up the Select Plan Properties page, 
where you can set some of the Plan options.
 2. 
On this page, as shown in Figure 5-2, specify a name 
and description for the plan and select the scheduling 
options.
 3. 
Select Next to move to the Select Maintenance Tasks screen where you can choose the tasks 
you want the plan to perform. For this example select the Back Up Database (Full) option, 
as shown in Figure 5-3.
FIGURE 5-1
FIGURE 5-2
FIGURE 5-3

 4. 
Select Next to move to the Select 
Maintenance Task Order screen, as 
shown in Figure 5-4. If you selected 
multiple tasks on the previous page, 
you can reorder them here to run in the 
order you want. In this example you 
have only a single task, so click Next.
 5. 
The next page is the Deﬁ ne Back Up 
Database (Full) Task screen, as shown 
in Figure 5-5. On this page select the 
details for the backup task. If you 
selected a different task on the Select 
Maintenance Tasks screen, you need 
to supply the details for that task. In 
the case of multiple tasks, this step 
presents a separate page for each task 
you selected in your plan.
Maintenance Plans  ❘  107
FIGURE 5-4
FIGURE 5-5
 6. 
Figure 5-6 shows the dialog where you can select the databases you want to back up. This 
ﬁ gure shows just one database to back up.
 7. 
On the next page (shown in Figure 5-7), select the reporting options for the plan: write a log 
to a speciﬁ c location, send an e-mail, or both.

108  ❘  CHAPTER 5  AUTOMATING SQL SERVER
FIGURE 5-8
FIGURE 5-9
FIGURE 5-7
FIGURE 5-6
 8. 
Select Next to go to the ﬁ nal page of the wizard, where you can conﬁ rm your selections (see 
Figure 5-8). 
 9. 
Click Finish to create your plan. While the plan is being created, a status page will show 
you the progress on each step of the plan’s creation, as shown in Figure 5-9. 

Maintenance Plans ❘ 109
The new plan now displays in the Object Explorer under the Maintenance Plans node and can be 
manually run by using the menu from that node.
You should have noticed along the way that the Maintenance Plan Wizard can perform only a limited 
number of tasks, but these are some of the most important routine maintenance activities on the 
server. Using this wizard enables you to automate many of the essential tasks needed on a SQL Server.
To explore more details about the plan you just created, look at the job created for this plan in 
the SQL Server Agent node, under the Jobs node. The job will be named <Your plan name>.
subplan_1, so in this example the job name is Basic Backup.Subplan_1.
Maintenance Plan Designer
Now that you’ve used the Wizard to create a basic backup job, it’s time to learn how to use the 
Designer to achieve the same task: 
 1. 
Right-click on the Management Node in Object 
Explorer, and this time select the New Maintenance 
Plan item. This will open the New Maintenance Plan 
dialog, which you can see in Figure 5-10. Enter a new 
plan name, Basic Backup 2, so you don’t conﬂ ict with 
the plan created using the Wizard. Click OK. 
Figure 5-11 shows the Plan Designer dialog that 
appears. You see two new windows inside Management Studio. The Maintenance Plan Tasks 
toolbox appears as a pane below the Object Explorer. The Plan Designer window appears on 
the right side of the screen.
FIGURE 5-10
FIGURE 5-11
 2. 
To create the basic backup task, click on the Back Up Database Task in the toolbox and 
drag it onto the designer’s surface. After doing this, your designer will look like Figure 5-12.

110  ❘  CHAPTER 5  AUTOMATING SQL SERVER
FIGURE 5-13
 3. 
At this point you have created the basic 
Backup task, but haven’t deﬁ ned what 
the backup task needs to do. To specify 
the same parameters as you did when 
using the Wizard, edit the properties of 
the Back Up Database Task by double-
clicking the task on the designer. This 
opens the task properties screen, shown 
in Figure 5-13. 
 4. 
This is the same dialog you completed 
using the Wizard, so select the same 
database to back up, and the same 
options you selected when using the 
Wizard. When you have ﬁ nished mak-
ing these changes, click OK to return 
to the designer. This time the Back 
Up Database Task no longer has the 
red warning sign, but now looks like 
Figure 5-14 (indicating the database you 
selected).
FIGURE 5-12
 
 
 
 


112  ❘  CHAPTER 5  AUTOMATING SQL SERVER
Jobs
A great reason to use SQL Server Agent is to create tasks you can schedule to complete work 
automatically, such as backing up a database. A SQL Server Agent job contains the deﬁ nition of 
the work to be done. The job itself doesn’t do the work but is a container for the job steps, which 
is where the work is done. A job has a name, a description, an owner, a category, and a job can be 
enabled or disabled. Jobs can be run in several ways: 
By attaching the job to one or more schedules
In response to one or more alerts
By executing sp_start_job
Manually via SQL Server Management Studio
Job Steps
A job consists of one or more job steps. The job steps are where the work is actually done. Each job 
step has a name and a type. Be sure to give your jobs and job steps good descriptive names that can 
be useful when they appear in error and logging messages. You can create a number of different 
types of job steps: 
PowerShell Job: Enables you to execute PowerShell scripts as part of a Job.
ActiveX Script: Enables you to execute VBScript, JScript, or any other installable scripting 
language.
Operating System commands (CmdExec): Enables you to execute command prompt items. 
You can execute bat ﬁ les or any of the commands that would be contained in a bat or cmd 
ﬁ le.
SQL Server Analysis Services command: Enables you to execute an XML for Analysis 
(XMLA) command. This must use the Execute method, which enables you to select data 
and administer and process Analysis Services objects.
SQL Server Analysis Services Query: Enables you to execute a Multidimensional Expression 
(MDX) against a cube. MDX queries enable you to select data from a cube.
SQL Server SSIS Package Execution: Enables you to execute an SSIS package. You can 
assign variable values, conﬁ gurations, and anything else you need to execute the package. 
This can save a great amount of time if you already have complex SSIS packages created, 
and want to execute them from a SQL Agent Job step. 
Transact-SQL Script (T-SQL): Enables you to execute T-SQL scripts. T-SQL scripts do 
not use SQL Server Agent Proxy accounts, described later in this chapter. If you are not 
a member of the sysadmin ﬁ xed-server role, the T-SQL step can run using your user 
credentials within the database. When members of the sysadm ﬁ xed-server role create 
T-SQL job steps, they may specify that the job step should run under the security context 
of a speciﬁ c database user. If they specify a database user, the step executes as the speciﬁ ed 
user; otherwise, the step executes under the security context of the SQL Server Agent 
Service account.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤


114  ❘  CHAPTER 5  AUTOMATING SQL SERVER
 2. 
From the menu select Manage Job Categories. The dialog box shown in Figure 5-15 
appears.
 3. 
Select Add, and the dialog shown in Figure 5-16 appears. 
FIGURE 5-15
FIGURE 5-16
 4. 
Enter the new Job Category name into the Name ﬁ eld, and select OK.
As trivial as it might seem, give some thought to organizing your jobs before creating your 
categories. You may be surprised how quickly the number of jobs on your server grows, making it 
difﬁ cult to ﬁ nd the correct job.
Job Step Logging
Each time a job is run, job history is created. Job history tells you when the job started, when it 
completed, and if it was successful. Each job step may be conﬁ gured for logging and history as well. 
All the logging setup for a job step is on the Advanced Tab of the job step properties. The Advanced 
Tab of the Job Step Properties is shown in Figure 5-17 and the key options that affect logging are 
discussed in the following list.


116  ❘  CHAPTER 5  AUTOMATING SQL SERVER
do so under the SQL Server Agent properties, as 
shown in Figure 5-18.
Job Notiﬁ cations
You can conﬁ gure SQL Server Agent to notify you 
when a job completes, succeeds, or fails. To do so, 
follow these steps: 
 1. 
In the Job Properties dialog, choose 
Notiﬁ cations to see the dialog box shown 
in Figure 5-19.
 2. 
A job can send a notiﬁ cation via e-mail, 
pager, and Net Send. A job can also write 
to the Windows Application Event Log. As 
Figure 5-19 shows, there is a line in the 
dialog box for each of these delivery 
methods. Place a check beside the delivery method you want; you may choose multiple methods. 
FIGURE 5-18
FIGURE 5-19
 3. 
Click the drop-down menu for each option and choose an operator to notify. An operator 
enables you to deﬁ ne the e-mail address for the delivery. (Operator setup is described later 
in this chapter.) 
 4. 
Choose the event that should trigger the notiﬁ cation. It can be when the job completes, 
when the job fails, or when the job succeeds. You may not want to be notiﬁ ed at all for 
some jobs such as routine maintenance like Index Maintenance. However, for mission-critical 


118  ❘  CHAPTER 5  AUTOMATING SQL SERVER
One item that is sorely lacking in SQL Server Agent’s arsenal is the capability to link jobs together 
so that one begins as the other ends. You can still make this happen though by adding a ﬁ nal step 
in one job that executes the second job. You can do this using sp_start_job. However using this 
approach puts all the navigation inside job steps. Navigation between Jobs should not be happening 
at the Job Step level; it should be outside at the job level. Some third-party tools do a good job of 
this. However, if you want to do it on your own, it is likely to be difﬁ cult to maintain.
Operators
An operator is a SQL Server Agent object that contains a friendly name and some contact 
information. Operators can be notiﬁ ed on completion of SQL Server Agent jobs and when alerts 
occur. (Alerts are covered in the next section.) You may want to notify operators who can ﬁ x 
problems related to jobs and alerts, so they may go about their business to support the business. 
You may also want to automatically notify management when mission-critical events occur, such as 
failure of the payroll cycle.
You should deﬁ ne operators before you begin deﬁ ning alerts. This enables you to choose the 
operators you want to notify as you are deﬁ ning the alert, saving you some time. To create a new 
operator, follow these steps: 
 1. 
Expand the SQL Server Agent Node in the Object Explorer in SQL Server Management Studio. 
 2. 
From there, right-click Operators and select New Operator. The New Operator dialog 
shown in Figure 5-20 appears, and here you can create a new operator. The operator name 
must be unique and fewer than 128 characters.
FIGURE 5-20

Automating SQL Server with SQL Server Agent ❘ 119
Operator Notiﬁ cations
Jobs enable you to notify a single operator for three different send types: 
E-mail: To use e-mail or pager notiﬁ cations, Database Mail must be set up and enabled, and 
SQL Server Agent must be conﬁ gured. For e-mail notiﬁ cations, you can provide an e-mail 
address. You may provide multiple e-mail addresses separated by semicolons. This could 
also be an e-mail group deﬁ ned within your e-mail system. If you want to notify many 
people, it is better to deﬁ ne an e-mail group in your e-mail system. This enables you to 
change the list of people notiﬁ ed without having to change every job.
Pager: For pager notiﬁ cations, you also provide an e-mail address. SQL Server Agent does 
not come equipped with paging. You must purchase paging via e-mail capabilities from 
a third-party provider. SQL Server Agent merely sends the e-mail to the pager address. 
Your pager software does the rest. Some pager systems require additional conﬁ guration 
characters to be sent around the Subject, CC, or To line. This can be set up in SQL Server 
Agent Conﬁ guration, covered at the end of this chapter.
Notice that there is a Pager on Duty Schedule associated with the Pager E-mail Name. This 
applies only to pagers. You can set up an on-duty schedule for paging this operator and then 
set this operator to be notiﬁ ed regarding an alert or job completion. When the job completes 
or the alert occurs, the operator will be paged only during her pager on-duty schedule.
Net Send: You can also use Net Send to notify an operator. To use Net Send, Windows 
Messaging Service must be running on the same server as SQL Agent. Additionally, you 
must provide the name of the workstation for this operator, and a Message dialog box 
pops up on her workstation. Out of these three, Net Send is the least reliable method of 
notiﬁ cation because the message is only available for a short period of time. If the operator 
is not at his desk at the time when the Net Send arrives, or the target server is ofﬂ ine or 
unavailable for any reason, the message will not be delivered.
Notiﬁ cations from alerts can reach multiple operators. This provides you with several convenient 
options. For example, you can create an operator for each shift (First Shift Operators, Second 
Shift Operators, and Third Shift Operators), set up a group e-mail and a group page address for 
each of the shifts, set up the pager-duty schedule to match each shift’s work schedule, and add all 
three operators to each alert. If an alert set up like this occurs at 2:00 a.m., then only the third-shift 
operators will be paged. If the alert occurs at 10:00 a.m., then only the ﬁ rst-shift operators will 
be paged.
There are several limitations of the schedule. Notice that the weekday schedule must be the same 
every day; although, you can specify a different schedule for Saturday and Sunday. Additionally, 
there is nothing to indicate company holidays or vacations. You can disable operators, perhaps 
because they are on vacation, but you cannot schedule the disablement in advance. 
Failsafe Operator
What happens if an alert occurs and no operators are on duty, according to their pager on-duty 
schedule? Unless you specify a failsafe operator, no one would be notiﬁ ed. The failsafe operator is 
a security measure that enables an alert notiﬁ cation (not job notiﬁ cation) to be delivered for pager 
➤
➤
➤

120  ❘  CHAPTER 5  AUTOMATING SQL SERVER
notiﬁ cations (not e-mail or Net Send) that could not be sent. Failures to send pager notiﬁ cations 
include the following: 
None of the speciﬁ ed operators are on duty.
SQL Server Agent cannot access the appropriate tables in msdb.
To designate an operator as the Failsafe Operator, perform the following steps: 
 1. 
Select the properties of SQL Server Agent. 
 2. 
Select the Alert system tab as shown in Figure 5-21. 
 3. 
In the Fail-safe operator section select Enable fail-safe operator. 
➤
➤
FIGURE 5-21
The failsafe operator is used only when none of the speciﬁ ed pager notiﬁ cations could be made or 
msdb is not available. If you have three pager operators associated with a speciﬁ c alert, and one of 
them is notiﬁ ed but two of them failed, the failsafe operator will not be notiﬁ ed.
You can indicate whether the failsafe operator will be notiﬁ ed using any or all of the three 
notiﬁ cation methods discussed in the previous section. However, a failsafe operator can be notiﬁ ed 
only if a pager notiﬁ cation cannot be successfully delivered, in which case the failsafe operator can 
be notiﬁ ed via e-mail, pager, Net Send, or a combination of these methods. 
Because the failsafe operator is a security mechanism, you may not delete an operator identiﬁ ed as 
failsafe. First, you must either disable the failsafe setup for SQL Agent or choose a different failsafe 
operator. Then you can delete the operator. Disabling an operator deﬁ ned as failsafe can prevent any 
normal alerts or job notiﬁ cations from being sent but cannot restrict this operator’s failsafe notiﬁ cations.





Automating SQL Server with SQL Server Agent ❘ 125
 2. 
Adjust this actual value so that you are not notiﬁ ed too quickly. If you have set up your log 
to be what you believe is large enough, you might instead want to notify on autogrowths.
WMI Event Alerts
Windows Management Instrumentation (WMI) is a tremendously powerful mechanism, but is also 
the least understood of all the alerting technologies.
SQL Server 2005 introduced the WMI Provider for Server Events which translates WMI Query 
Language (WQL) queries for events into event notiﬁ cations in a speciﬁ c database. For more 
information on using event notiﬁ cations see Chapter 12, “Monitoring Your SQL Server.”
To create a WMI event alert, select WMI event alert as the Type for the alert, validate the 
namespace is correct, and enter your WQL query.
Alert Responses
As was previously discussed, you can respond to an alert by starting a SQL Server Agent job or 
notifying one or more operators. You set this up on the Response tab of the Create Alert dialog 
box. To execute this job, simply check the check box and choose an existing job or create a new job. 
To notify an operator, check the appropriate box, and select the operators you want to notify by 
choosing one or more of the notiﬁ cation methods. For alerts, it is nice to have an operator for each 
shift you must cover, with the pager on duty set up appropriately, as discussed in the “Operators” 
section earlier in the chapter.
As you think about how you might best use this in your enterprise, imagine a scenario such as the 
transaction log getting full. You could set up a performance alert to notify operators when the log is 
actually full and run a job that grows the log. You could set up an alert that backs up the log when 
it becomes 80 percent full.
The scenario might play out as follows. You are having lunch and your pager goes off, notifying you 
that the log is 70 percent full. A job runs automatically that tries to back up the log to free space. In 
a couple of minutes you get a page telling you that the job completed successfully. After a few more 
potato chips, your pager goes off yet again — the log is now 80 percent full. The prior log backup 
did not free up any space. There might be a long-running transaction. The log backup job runs 
again, and you are notiﬁ ed upon its completion. You ﬁ nish your lunch with no other pages. This 
means the last log backup freed up some space and you are now in good shape.
Your pager may have gone off again, telling you that the log is nearly full, and has either been 
extended with autogrow, or a job to extend the log has run and extended the transaction log onto 
an emergency log disk. It’s probably time for you to get back to work, but the automation you have 
brought to the system has already been ﬁ ghting this problem while you ate your lunch, notifying you 
of each step. With some thoughtful consideration, you might account for many planned responses 
such as this, making your life easier and operations tighter.
The Alert Options page in the Create Alert dialog box enables you to do several things: 
Specify when to include more detailed information in the notiﬁ cation. Sometimes the error 
text of the message might be long. Additionally, you may have a limit on the amount of data 
that can be presented on your devices. Some pagers limit you to as few as 32 characters. 
You should not include the error text for those message types that cannot handle the extra 
text, which are most commonly pagers.
➤

126  ❘  CHAPTER 5  AUTOMATING SQL SERVER
Add information to the notiﬁ cation. The dialog includes a large text box labeled Additional 
Notiﬁ cation Message to Send. You can type any text here, and it will be included in the 
notiﬁ cation message. Perhaps something such as Get Up, Come In, and Fix This Problem 
Immediately might be appropriate. 
Delay the time between responses. At the bottom of the dialog, you can set a delay between 
responses. The default value for this is 0. Imagine a scenario in which an alert goes off 
many times during a short period. Perhaps a program is repeatedly executing raiserror or 
a performance condition alert is going wild. The performance condition alerts that run to 
alert of limited resources are especially vulnerable to this problem. You run low on memory, 
which causes an alert or job to run, which uses more memory. This causes the alert to ﬁ re 
again, using more memory, repeatedly. You are paged repeatedly as well.
You can right-click any of the SQL Server Agent objects and create a script that can drop or create 
the object. If you want the same object to exist on many servers, you can script it out, change the 
server name, and load it onto a different server. This means you would have to keep operators, jobs, 
alerts, and proxies in sync between multiple servers, which could be painful and error prone. Event 
forwarding can also simplify your life when you administer many servers. Multiserver jobs and 
event forwarding are covered later in the section “Multiserver Administration.”
SQL SERVER AGENT SECURITY
SQL Server Agent security is more ﬁ ne-grained than ever. This section covers not only the service 
account, but also security issues such as who can create, see, and run SQL Server Agent jobs. SQL 
Server 2012 enables multiple, separate proxy accounts to be afﬁ liated with each job step. These proxy 
accounts are associated with SQL logins, which provide excellent control for each type of job step.
Service Account
The SQL Server Agent service account should be a domain account if you plan to take advantage of 
Database Mail or require any network connectivity. The account should map to a login that is also a 
member of the sysadmin ﬁ xed-server role.
Access to SQL Agent
After the installation, only members of the sysadmin ﬁ xed-server role have access to SQL Server 
Agent objects. Others cannot even see the SQL Server Agent object in the Object Explorer of 
Management Studio. To give other users access to SQL Agent, you must add them to one of three 
ﬁ xed database roles in the msdb database: 
SQLAgentUserRole
SQLAgentReaderRole
SQLAgentOperatorRole
The roles are listed in order of increased capability, with SQLAgentOperator having the highest 
capability. Each higher role includes the permissions associated with the lower roles, so it is not 
necessary to assign a user to more than one role. 
➤
➤
➤
➤
➤




130  ❘  CHAPTER 5  AUTOMATING SQL SERVER
 4. 
Give the proxy a name that provides information about its security level or its intended use. 
Then associate a credential with the proxy. The proxy provides the permissions associated 
with its credential when it is used. Provide a more detailed description of what the proxy 
enables and how it should be used and when.
 5. 
Then select the subsystems that can use the proxy. A proxy can be associated with many 
subsystems. 
 6. 
Create a list of users (principles) who may use this proxy. This is done on the Principles 
page. A principle can be a Server Role, a SQL Login, or an msdb role.
 7. 
Now assume you have created the two proxies for the CmdExec subsystem (refer to 
Figure 5-24). Your SQL login is associated with both proxies. You want to create a job that 
contains a CmdExec job step. When you add the job step, open the drop-down labeled Run 
As, which contains a list of all the proxies you are allowed to use for your job step. Each 
proxy has its own permissions. Choose the proxy that contains the permissions you need for 
your job step, and you should be ready to go.
CONFIGURING SQL SERVER AGENT
Now that you have learned how things work in SQL Agent, you can take on the conﬁ guration task. 
You already know about some of the conﬁ guration options, so now you can go through the different 
pages to conﬁ gure the SQL Server Agent properties.
FIGURE 5-26

Conﬁ guring SQL Server Agent ❘ 131
To start conﬁ guration, right-click the SQL Server Agent node in Management Studio, and choose 
Properties. 
General Properties
The General page appears, as shown in Figure 5-27. Review each section on this page and consider 
the following: 
Check the two top check boxes: Auto Restart SQL Server If It Stops Unexpectedly and Auto 
Restart SQL Server Agent If It Stops Unexpectedly. The Service Control Manager watches 
both of these services and automatically restarts them if they fail.
Usually you leave the error-log location at the default; however, you can change it if you 
want. If you need some additional logging, check Include Execution Trace Messages. 
Execution Trace Messages provide detailed information on SQL Agent operation which is 
written to the SQL Agent Error Log. Enabling this option increases the space used in the 
SQL Agent Log, so Agent log size is something to consider when enabling this option.
➤
➤
FIGURE 5-27
To get a Net Send when errors are logged, enter a workstation name in the Net Send 
Recipient text box. Of course, Windows Messaging Service must be running on the server 
for Net Sends to occur.
➤

132  ❘  CHAPTER 5  AUTOMATING SQL SERVER
Advanced Properties
Choose the Advanced Page on the top left, which brings up the dialog shown in Figure 5-28. There 
are several options from which to choose on this page: 
The top section, SQL Server Event Forwarding, enables you to forward your events from 
one server to another. You can set up operators and alerts on a single server and then 
have the other servers forward their events to the single server. To use this capability, 
you also need to understand how to use SQL Server Agent tokens, which are covered in 
the section “Using Token Replacement” later in this chapter. If you want to employ this 
capability, check the box labeled Forward Events to a Different Server. Then select the 
server name. You can forward all events or only unhandled events. An unhandled event is 
one that does not have an alert deﬁ ned for it. You also select how severe the error must be 
before it can be forwarded. For example, you may not want anything less than a severity 
16 error (Miscellaneous User Error) to be forwarded. Whether you forward severity 16 
errors depends on whether you have application-deﬁ ned errors that specify notiﬁ cations. 
If you plan to use this capability, you also need to understand how to use SQL Server 
Agent tokens, which are covered in the section “Using Token Replacement” later in 
this chapter.
The second section is Idle CPU Condition. Recall that you can create a schedule that runs 
when the CPU becomes idle. This is where you deﬁ ne what idle means. The default is CPU 
utilization at less than 10 percent for 10 minutes.
➤
➤
FIGURE 5-28

Conﬁ guring SQL Server Agent ❘ 133
Alert System Properties
The next page is for the Alert System, as shown in Figure 5-29.
If you plan to use Database Mail, you do the setup 
here. Although you may have many mail proﬁ les 
in Database Mail, SQL Server Agent uses only one 
proﬁ le. Choose the mail system and proﬁ le.
The second section is for pager e-mails. If your 
pager system requires special control characters 
in the To, CC, or Subject line, you may add those 
characters here in front of the item (preﬁ x) or 
after the item (sufﬁ x). As you make changes, 
you can see the effect in the small box below 
your data-entry section. You may also choose 
to include or exclude the body of the e-mail for pagers by indicating your selection in the 
appropriate check box.
The third section enables you to provide failsafe operator information. Please use this if you are 
doing any notiﬁ cations. It is too easy to change a schedule in such a way that results in no one 
being notiﬁ ed, so don’t get caught. Enable this section, choose an operator, and indicate how the 
failsafe messages should be delivered (by e-mail, pager, Net Send, or some combination of these).
The last check box enables you to specify whether you want to have tokens replaced in 
jobs run from alerts. Details of token replacement are covered in the section “Multiserver 
Administration” later in this chapter.
Job System Properties
The Job System page is next, as shown in Figure 5-30.
➤
➤
➤
➤
FIGURE 5-29
FIGURE 5-30

134  ❘  CHAPTER 5  AUTOMATING SQL SERVER
In the ﬁ rst section, you can specify the shut-down period (in seconds) in the Shutdown time-
out interval list. For instance, suppose you are trying to shut down SQL Agent, and jobs 
are running. You can specify how long SQL Server Agent should wait for jobs to complete 
before killing them and shutting down.
The second section is only available if you administer a SQL Server 2000 Agent. This 
enables you to set the backward compatible nonadministrator proxy. SQL 2000 allowed 
only one proxy. SQL Server 2005, 2008, and 2012 allow many proxies, so this is not 
necessary when administering SQL Server 2005, 2008, and 2012 Agents.
Connection Properties
The Connection page is one that most users do not need. SQL Server Agent connects to SQL Server, 
by default, using the server name, the default port, the SQL Server Agent Service account, and the 
highest-matching protocol between the client conﬁ guration and the protocols enabled for SQL 
Server. There are several circumstances in which you may want to alter these defaults: 
Your server has multiple network cards, and you want to specify a particular IP or port.
You want to connect using a speciﬁ c protocol (IP, for instance).
You want SQL Server Agent to connect to the server using a login different from the service 
account login.
To create an alias for SQL Server, follow these steps: 
 1. 
Open Conﬁ guration Manager. 
 2. 
Expand the SQL Native Client Conﬁ guration, right-click Aliases, and choose New Alias. 
 3. 
Set up the alias to suit your connectivity needs. 
 4. 
On the SQL Server Agent Connection page, enter the alias name and the connection infor-
mation you want SQL Server Agent to use. Although SQL Server authentication is allowed, 
it is not recommended.
History Properties
The last page is the History page, shown previously in Figure 5-18. Here you can limit the size of 
the job history log to a ﬁ xed number of rows, and the Maximum Job History Rows per Job option 
is a lifesaver. Imagine a job that runs repeatedly. It could be a job scheduled by a user to run every 
second, or it could be a job that runs from an alert that occurs repeatedly. In any case, the log 
entries from this job could ﬁ ll up your entire job history, and you would have no history information 
for any other jobs. That could leave you in a tough spot if any other job needed debugging. This is 
exactly the situation that Maximum Job History Rows per Job is intended to prevent. The default is 
100 rows, but you can change it based on your needs.
DATABASE MAIL
Database Mail appeared with SQL Server 2005 and was a welcome replacement for SQLMail; 
Database Mail and SQLMail both enable you to notify operators via e-mail and to send e-mails 
via stored procedures, but Database Mail is more secure, more reliable, and does not rely on MAPI. 
➤
➤
➤
➤
➤




138  ❘  CHAPTER 5  AUTOMATING SQL SERVER
 2. 
Click Next on the Welcome page. The next page in the wizard is the Select Conﬁ guration 
page, and is shown previously in Figure 5-32. Check the top radio button to indicate you are 
setting up Database Mail for the ﬁ rst time. Click Next.
 3. 
If you haven’t previously enabled Database Mail, you receive a message box asking if you 
want to enable the Database Mail feature. Choose Yes and continue.
 4. 
This brings you to the New Proﬁ le dialog box, as shown in Figure 5-33. To continue, you 
need to add at least one mail account. Click the Add button to display the New Database 
Mail Account dialog, as shown in Figure 5-34. Here you provide the information needed to 
communicate with an SMTP server. Choose a name and description for this account.
FIGURE 5-33
FIGURE 5-34

E-mails sent from this account will be tagged from the e-mail address and display name 
that you set in this section. If the recipients reply to the e-mail, the reply will be sent to the 
address you supply in the Reply e-mail text box. 
 5. 
In the Server name text box, provide the name of the SMTP server. This is usually in the 
form of smtp.myserver.com. Do not provide the complete URL, such as http://smtp
.myserver.com. Database Mail does this for you. If you check the box labeled This Server 
Requires a Secure Connection (SSL), the URL created will be https://smtp.myserver.com. 
The default port number of 25 will sufﬁ ce unless you have changed the SMTP port number.
 6. 
Provide the SMTP login information in the SMTP authentication section. Not all SMTP 
servers require authentication; some require only a known sender e-mail, and others require 
nothing. After supplying the needed information, click OK to return to the New Proﬁ le dia-
log, and then click Next.
 7. 
The next page is the Manage Proﬁ le Security dialog, as shown in Figure 5-35. Here you set 
up public and private proﬁ les. Check the Public check box next to a proﬁ le to make it pub-
lic. You may also want to set this as a default proﬁ le.
 8. 
Click Next to move to the Conﬁ gure System Parameters page. On this page you can change 
the values for system parameters such as retry attempts, maximum ﬁ le size, prohibited 
extensions, and logging level. The default values work well in most cases.
 9. 
Click Next to view the Complete the Wizard page, where you have a last chance to conﬁ rm 
the selections you made before they are applied.
 10. 
Click Finish to apply the changes you made and view progress and a completion report as 
each set of changes is made.
 11. 
To ensure things are working properly, you should send a test e-mail. In SQL Server 
Management Studio, expand Management, right-click Database Mail, and choose Send Test 
E-Mail. You are prompted to enter an e-mail address. Send the mail and wait for receipt.
FIGURE 5-35
Database Mail ❘ 139


Multiserver Administration ❘ 141
The following is a list of tokens that you can use in any job: 
(DATE): Current Date (YYYYMMDD).
(INST): Instance name of the SQL Server. This token is empty for the default instance.
(JOBID): SQL Server Agent job ID.
(MACH): Computer name where the job is run.
(MSSA): Master SQLServerAgent service name.
(OSCMD): Preﬁ x for the program used to run CmdExec job steps.
(SQLDIR): SQL Server’s install directory. The default install directory is C:\Program 
  Files\Microsoft SQL Server\MSSQL.
(STEPCT): The number of times this step has executed. You could use this in looping code to 
  terminate the step after a speciﬁ c number of iterations. This count does not include retries on 
  failure. This is updated on each step run during the job, such as a real-time counter.
(STEPID): The job step ID.
(SVR): The server name of the computer running SQL Server, including the instance name.
(TIME): Current time (HHMMSS).
(STRTTM): The job’s start time (HHMMSS).
(STRTDT): The job’s start date (YYYYMMDD).
The following is a list of tokens that can be used only in a job that has been started from an alert. If 
these tokens are included in a job started any other way, the job throws an error: 
(A-DBN): Database name where the alert occurred
(A-SVR): Server name where the alert occurred
(A-ERR): Error number associated with the alert
(A-SEV): Error severity associated with the alert
(A-MSG): Message text associated with the alert
The following token is available for use only on jobs run as the result of a WMI alert (see the “Using 
WMI” section later in the chapter). 
(WMI(property)): Provides the value for the WMI property named property. 
$(WMI(DatabaseName)) returns the value of the DatabaseName property for the WMI alert 
that caused the job to run.
All these tokens must be used with escape macros. The purpose of this change is to increase the 
security related to the use of tokens from unknown sources. Consider the following token, which 
you might have included in a T-SQL job step: 
 Print ‘Error message: $(A-MSG)’
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

142  ❘  CHAPTER 5  AUTOMATING SQL SERVER
The T-SQL job step runs as the result of a user error (raiserror). A malicious user could raise an 
error like this one: 
 Raiserror(‘’’;Delete from dbo.Employee’,16,1)
The error returned would be:
 ‘;Delete from dbo.Employee
The print message would be:
 Print ‘Error message:’;Delete from dbo.Employee
If this happens, it means you have just been attacked with a SQL injection attack. The delete 
statement runs if the T-SQL job step has permission.
To combat an attack such as this, you must add an escape macro. Because the print statement uses 
single quotes, a SQL injection attack closes out the single quote and then insert its own SQL. To 
prevent this attack, you can double-quote any quote that comes in via the token. The escape macro 
ESCAPE_SQUOTE does exactly that. It is used like this: 
 Print ‘Error message: $(ESCAPE_SQUOTE(A-MSG))’
Continuing the example, you end up with the following: 
 Print ‘Error message:’’;Delete from dbo.Employee
You then get an error due to the unmatched quote, and the step fails, keeping you safe.
The following is a list of escape macros: 
$(ESCAPE_SQUOTE(token)): Doubles single quotes (‘) in the replacement string.
$(ESCAPE_DQUOTE(token)): Doubles double quotes (“) in the replacement string.
$(ESCAPE_RBRACKET(token)): Doubles right brackets (]) in the replacement string.
$(ESCAPE_NONE(token)): The token replacement is made without changes. This is used for 
backward compatibility only.
You can also use these values directly if you ensure proper data types. The SQL script-looping job 
with tokens contains the following code that terminates a job step after it has executed ﬁ ve times. 
The top line converts the STEPCT token to an integer so it can be used in a comparison. Then 
the JOBID token for this job is converted to a binary 16 and passed to the sp_stop_job stored 
procedure, which can take the job ID of the job you want to stop: 
 IF Convert(int,$(ESCAPE_NONE(STEPCT))) >5
   BEGIN
   DECLARE @jobid binary(16)
   SELECT @jobid =Convert(Uniqueidentifier,$(ESCAPE_NONE(JOBID)))
   EXEC msdb.dbo.sp_stop_job @job_id = @jobid
   END
Imagine how you might use the alert-based tokens. You could create a SQL performance alert that 
ﬁ res when the <any database> transaction log becomes greater than 80 percent full. Create a job 
with a T-SQL step like this: 
➤
➤
➤
➤

Multiserver Administration ❘ 143
 DECLARE @a varchar(100)
 SELECT @a =’BACKUP LOG $(ESCAPE_SQUOTE(A-DBN))
   TO DISK = “\\UNCName\Share\$(ESCAPE_SQUOTE(A-DBN))\log.bak”’
 SELECT @a
 BACKUP LOG $(ESCAPE_SQUOTE(A-DBN))
   TO DISK = ‘\\UNCName\Share\\$(ESCAPE_SQUOTE(A-DBN))\log.bak’
Here UNCName is the name of the server where you want the backup to be stored and Share 
is the share on the server. Make sure the job runs when the alert occurs. If the alert ﬁ res for 
NorthAmerica, the backup command looks like this: 
 BACKUP LOG NorthAmerica TO DISK = \\UNCName\Share\\NorthAmerica\log.bak
You have to create the directory ﬁ rst and grant appropriate permissions to the proxy you use. You 
could create a CMDExec step, which creates the directory on-the-ﬂ y. Now, a single log backup job 
can back up any transaction log. You might improve the name of the directory you create in the 
CMDExec step by adding the date and time to the ﬁ lename.
Event Forwarding
Where events and alerts are concerned, you can create operators and alerts on a single system and 
then have the other systems forward their events to your central alert-handling SQL Server, which 
responds to those alerts as necessary. 
Designating a server to forward events to is done on the Advanced Page of the SQL Server Agent 
properties dialog (refer to Figure 5-28). Check Forward Events to a Different Server then you can 
specify the server to forward events to.
You can conﬁ gure which events will be forwarded using by choosing from the options under Events. 
Here you can choose between Unhandled Events and All Events. If you choose All Events, you can 
then add a ﬁ lter on the severity level If Event has Severity at or Above. You can set up operators on 
your master event management system. Create the jobs that respond to the alerts. Then create alerts 
on the single master event management system to handle the event. The jobs you create can take 
advantage of SQL Server Agent tokens and know on which server and database the original event 
occurred.
Using WMI
Windows Management Instrumentation (WMI) is a set of functions embedded into the kernel of 
Microsoft Operating Systems and Servers, including SQL Server. The purpose of WMI is to enable 
local and remote monitoring and management of servers. It is a standards-based implementation 
that incorporates the Distributed Management Task Force’s (DMTF) Web-Based Enterprise 
Management (WBEM) and Common Information Model (CIM) speciﬁ cations.
WMI is a big initiative and probably warrants an entire book of its own. What you need to know 
most is that WMI has many events for SQL Server. Search for WMI to start in Books Online, and 
you can discover the many, many events. You can create alerts on these events. Included are Data 
Deﬁ nition Language (DDL) events that occur when databases are created or dropped and when 
tables are created or dropped, for example.


Multiserver Administration ❘ 145
directory. To ﬁ nd out more, Microsoft has an entire subsection of its website devoted to WMI. Just 
search for WMI on www.microsoft.com.
Multiserver Administration — Using Master and Target Servers
SQL Server enables you to set up a master server (MSX). The master server can send jobs to be 
run on one or more target servers (TSX), but the master server may not also be a target server 
that receives jobs from another master server. The target servers receive and run jobs from a single 
master server, in addition to their own local jobs. You may have multiple master servers in your 
environment, but a target server is associated with a single master server. This is a simple two-level 
hierarchy; a server is a master server, a target server, or neither. The language used to describe the 
process is military in character: You enlist target servers to add them, and they defect to go away.
Setting up servers is easy. Simply follow these steps: 
 1. 
In SSMS, right-click the SQL Server Agent node, select Multiserver Administration, and 
choose Make This a Master. 
 2. 
After the initial dialog box, you see a box where you can provide the e-mail address, pager 
address, and Net Send location to set up a master server operator. Fill in these ﬁ elds appro-
priately. This operator will be set up on the master server and all target servers. This is the 
only operator who can be notiﬁ ed from multiserver jobs.
 3. 
The next dialog box enables you to choose all the target servers. The list includes the serv-
ers that you have registered in SSMS. Choose the servers that you want to be targets of 
this master, and click Next. You may add additional registrations by clicking the Add 
Connection button. 
 4. 
Close this dialog box. SQL checks to ensure that the SQL versions of the master and targets 
are compatible. If the versions are not compatible, drop the target from the list and then 
continue. Later, you can upgrade the target or master, so the versions are the same.
 5. 
Go to the next dialog box and use the wizard to create a login on the target, if necessary, 
and grant it login rights to the master server. Target servers must connect to the master 
server to share job status information. After you complete the setup, refresh your SQL 
Server Agent nodes and see the change. There will be a note on the master server (MSX) 
and a note on the target server.
Now you can create jobs to be used at multiple target servers. Notice on the MSX that the Jobs node 
is divided into two sections: local jobs and multiserver jobs. To create a job, follow these steps. 
 1. 
Right-click multiserver jobs, and select New Job to create a simple job.
 2. 
Create a simple job on the MSX server and have it run at one or many TSX servers. While 
doing this, be sure to go to the notiﬁ cations page. The only operator you can notify is 
MSXOperator.
Creating multiserver jobs is a nice way to manage a larger implementation without having to buy 
additional third-party products. No one on the TSX box can mess up your jobs. Use SSMS to 
connect to the target server as an administrator and look at the job properties for the job you 
just created and downloaded from the MSX. You can see the job, you can see the job history, and 
you can even run the job. You cannot delete the job, change the schedule, change the steps, or 
anything else. This job does not belong to you; it belongs to the MSX.

146  ❘  CHAPTER 5  AUTOMATING SQL SERVER
As you begin to think about how you might use this, be sure you consider the implications of a 
single job running on multiple servers. Any reference to directories, databases, and so on must be 
valid for all the TSXs where this job runs. You can create a single backup share that all the backups 
can use, for instance.
Because a job can start another job, you could also create a master job that has a single step that 
starts another job. This other job is created on each TSX and is speciﬁ c to each TSX. This enables 
you to perform some customization, if necessary. To create a master job, perform the following steps:
 1. 
Back in SSMS, right-click the SQL Server Agent node on the master server. 
 2. 
Choose Multi Server Administration. Here you can add target servers and manage target 
servers. 
 3. 
Choose Manage Target Servers. In this dialog box, you can monitor the status of every-
thing. When you create a job for a target server, the job is automatically downloaded to the 
target server. If the unread instructions count does not go down to 0, poll the target server. 
This wakes it up to accept the instructions. 
 4. 
Click the relevant tab to see the details of downloaded instructions. This shows you details 
of when jobs are downloaded and updated.
 5. 
Using the Post Instructions button in the Target Server Status dialog, you can synchronize 
clocks between the servers, defect target servers, set polling intervals, and start jobs. You 
can also start the job directly from the Jobs node on the MSX or the TSX.
 6. 
Job histories can be viewed on the MSX for the job, just like any other job, but you cannot 
see job-step details. To get the step details, view the job history from the TSX.
 7. 
You can defect TSXs from the TSX SQL Server Agent node or from the Manage Target Servers 
dialog on the MSX. When all the TSXs have been defected, the MSX is no longer an MSX.
SUMMARY
Automating SQL Server is one of the most important things you can learn to make your life 
and your business run smoothly and easily. Maintenance plans take away a lot of the work of 
automating routine maintenance activities, and are a great way to get started with automating 
common maintenance tasks. Additionally, SQL Server Agent provides many features and services 
to make your life easier. Just creating a few simple backup jobs that notify operators can automate 
many normal tasks. If you want to be fancy, go ahead, but do some planning ﬁ rst, especially when 
considering multiserver jobs.
Using Alerts is a great way to automate notiﬁ cations about signiﬁ cant activities occurring on 
your database systems. You can use the pager notiﬁ cations and the related on-duty schedules for 
regular e-mail or pagers. This is a good way to ensure that the correct people are notiﬁ ed. If you 
have many operators for alert notiﬁ cations, consider creating e-mail groups and ofﬂ oading some of 
the notiﬁ cation work to your e-mail server. Start small, and take your time. As you become more 
comfortable with Maintenance Plans and SQL Server Agent, you can spread your wings and ﬂ y. 


148  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
banking applications where it handles nightly batch processing of transaction data, in social media 
handling friend requests on MySpace as well as a variety of other applications where the command 
needs to be completed, just not at the exact time that the command was issued.
SQL Service Broker Overview
SQL Server Service Broker provides an extremely ﬂ exible framework, which enables the creation 
of a variety of objects that can send messages within a database. The messages aren’t limited to 
being sent within the database, though. Messages can also be sent from database to database. The 
databases that the messages are sent between can exist on the same database instance, different 
instances within a company’s data center, or between two SQL Server instances across the Internet. 
Messages are sent within the context of a conversation, which this chapter discusses later.
When messages are sent, they can be received and processed on a schedule via the SQL Server Agent 
(or another job scheduler), on demand by a Windows application or service, or automatically when 
they are received by setting an activation stored procedure. Activation stored procedures are created 
on the queue and are triggered automatically when messages are received by the receiving queue. 
Activation procedures can be conﬁ gured to run a single thread or multiple threads running in 
parallel with up to 62,767 parallel threads.
Messages can be sent in a single direction or bidirectionally as needed. Typically messages are sent 
in only one direction; however, there are situations in which after the initial message is processed 
you might want to send a conﬁ rmation message back to be processed on the sending side.
SQL Server Service Broker introduces three new commands that are used to send and receive 
messages, all of which are discussed in more detail later in this chapter. The ﬁ rst is the CREATE 
CONVERSATION DIALOG command that creates the conversation on which messages are then sent. The 
second is the SEND command that is used to send messages on the previously created conversation. 
The third is the RECEIVE command that is used to remove messages from the queue for processing.
The beauty of SQL Server Service Broker is that messages are processed once, and only once and 
in the order sent, provided the messages are sent within the same conversation. If messages are sent 
within different conversations, they may not be processed in the order they are sent.
You can use Service Broker to send messages of just about any size, from a blank message (not to be 
confused with a NULL message even though a blank message has a NULL value) to a message that 
ﬁ lls the XML data type that caps at 1 billion bytes of Unicode data.
SQL Server Service Broker Versus Other Message Queues
A variety of message queuing technologies are available. Microsoft makes two: SQL Server Service 
Broker and Microsoft Message Queue (MSMQ). There are several third-party technologies that 
function similar to MSMQ that you can also use.
The big difference between SQL Server Service Broker and other queuing technologies is that 
SQL Server Service Broker stores the messages in queue within the database, and other queuing 
technologies store their data outside of the database. Because SQL Server Service Broker stores its 
messages within the database, this makes the queues transactionally consistent with the data that 
the queues back up and restore along with the database. The upside to this is that if the database 


150  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
selecting the Options page; then under Other Options, scroll down to the Service Broker section, 
as shown in Figure 6-1. The Broker Enabled setting is the only setting that can be conﬁ gured by 
selecting from either True or False.
FIGURE 6-1
You can also enable SQL Server Service Broker using T-SQL by using the ALTER DATABASE statement. 
There are two important ﬂ ags when using this method. The ﬁ rst is the NEW_BROKER ﬂ ag that conﬁ gures 
SQL Server Service Broker for the ﬁ rst time. The second is the ENABLE_BROKER ﬂ ag that turns SQL 
Server Service Broker on if it is disabled, as shown in the following code. If SQL Server Service Broker 
has never been enabled and the ENABLE_BROKER ﬂ ag is used, it has the same effect as if the NEW_
BROKER ﬂ ag was used.
ALTER DATABASE sample_database
SET ENABLE_BROKER
You see if SQL Server Service Broker is enabled by looking at the values of the is_broker_enabled 
and service_broker_guid columns of the sys.databases system catalog view. The is_broker_
enabled column is a bit ﬁ eld with a value of either 0 or 1. The service_broker_guid column 
contains a unique guid value that represents the speciﬁ c instance of SQL Server Service Broker 
within the speciﬁ c database. When the guid within the service_broker_guid column is all zeros, 
this indicates that the SQL Server Service Broker has never been enabled for that database before.

After SQL Server Service Broker has been enabled for a speciﬁ c database, the SQL Server Service 
Broker objects can be created, and messages can begin being sent.
Message Types
The ﬁ rst object type that you need to create when conﬁ guring SQL Server Service Broker is a 
message type. Message types validate that the data within a message is the correct, expected format. 
You can use four different validation options: 
NONE: Any data can be placed within the body of the message that is sent. When no value is 
speciﬁ ed for the VALIDATION option, the default value of NONE is used.
EMPTY: Only messages that are empty can be sent.
WELL_FORMED_XML: Only messages consisting of well-formed XML documents can be sent.
VALID_XML WITH SCHEMA COLLECTION: Only XML documents that ﬁ t with the speciﬁ ed 
XML schema can be used. The XML schema to use with the VALID_XML WITH SCHEMA 
COLLECTION option requires that the XML schema already exists within the database by 
using the CREATE XML SCHEMA COLLECTION command. 
Beyond the validation option, the CREATE MESSAGE TYPE command has only two other options. 
The ﬁ rst is the name of the message type, which must ﬁ t within the standard SQL Server object 
naming rules. The second option is the AUTHORIZATION option, which sets the owner of the message 
type when it is created. When the person creating the message type is a member of the sysadmin 
ﬁ xed server role or the db_owner ﬁ xed database role, then the value speciﬁ ed for AUTHORIZATION 
can be any valid database user or role. When the person creating the message type is not a member 
of the sysadmin ﬁ xed server role or the db_owner ﬁ xed database role, then the value speciﬁ ed 
for AUTHORIZATION must be that user, or another user that the user in question has the rights to 
impersonate. If no value is speciﬁ ed for the AUTHORIZATION parameter, the message type belongs to 
the current user. The following code snippet shows the creation of a message type.
CREATE MESSAGE TYPE YourMessageType
AUTHORIZATION dbo
VALIDATION = WELL_FORMED_XML
Contracts
The second object type to create is a contract. Contracts deﬁ ne the message types that are used 
within a single conversation. Contracts, similar to message types, have a couple of parameters that 
need to be speciﬁ ed when using the CREATE CONTRACT statement. These include the following:
 The name of the contract must follow the standard SQL Server object naming rules. 
The AUTHORIZATION value must be a user or role that exists within the database.
The CREATE CONTRACT requires a speciﬁ c list of message types that are bound to the contract, 
and each message type can only be used by a speciﬁ c member of the conversation. There are three 
options available for the user who can use each message type: 
INITIATOR: The SQL Server Service Broker SERVICE (SERVICEs are discussed later in this 
chapter) who initiated the conversation.
➤
➤
➤
➤
➤
➤
➤
Conﬁ guring SQL Server Service Broker ❘ 151


The third option is the POISON_MESSAGE_HANDLING, which can be ON or OFF. When 
poison message handling is enabled on a queue, which is the default, it causes the queue to 
automatically disable after ﬁ ve consecutive transaction rollbacks. When poison message 
handling is disabled, the message handling must be handled within the application.
The fourth option is the activation stored procedure conﬁ guration, which is made up of four 
child settings: 
STATUS: Under the ACTIVATION setting, STATUS is used to enable or disable the 
 activation procedure. When ACTIVATION is disabled, it stops only new threads of 
the activated stored procedure from being spawned; already running threads are left 
running.
PROCEDURE_NAME: This parameter is the name of the stored procedure that should 
be activated.
MAX_QUEUE_READERS: The number of threads that should be spawned, each of 
which calls the activated stored procedure.
EXECUTE AS: This parameter speciﬁ es the username that the procedure should be run as. 
The values that can be speciﬁ ed are SELF, OWNER, or any valid user within the database.
The following code shows the various options that can be speciﬁ ed. Two queues should be used 
when sending messages within an application: one queue as the source queue, and one queue as the 
destination queue. This way, when conversations are closed, the acknowledgments that are sent 
automatically are not sent to the same queue that has the production work load in it. This becomes 
especially important for high load workloads because a large amount of time may be spent processing 
these acknowledgments instead of processing the production workload that needs to be done.
CREATE QUEUE YourQueue_Source
WITH STATUS=ON,
     RETENTION=OFF,
     ACTIVATION
         (STATUS=OFF,
          PROCEDURE_NAME=dbo.MySourceActivationProcedure,
          MAX_QUEUE_READERS=1,
          EXECUTE AS OWNER),
     POISION_MESSAGE_HANDLING=ON;
When creating a queue that has an activated stored procedure, you can conﬁ gure the activated 
procedure when the queue is created, as shown in the previous code snippet. However, the 
stored procedure must exist before the queue can be created using this method. Because of this, the 
queue is often created without conﬁ guring the ACTIVATION settings. Instead, the stored procedure 
is created and the queue is altered using the ALTER QUEUE command to set the activation settings. 
The end result of creating a queue while enabling the activation settings would be the same if the 
queue was created without enabling the activation settings. In either case the queue would call 
the stored procedure when messages were received in the queue.
Services
Another object type you create is called a service. Although poorly named, SQL Server Service 
Broker Services play an important role. Service Broker Services are objects that are conﬁ gured via 
➤
➤
➤
➤
➤
➤
Conﬁ guring SQL Server Service Broker ❘ 153


Routes have a variety of parameters that you can conﬁ gure via the CREATE ROUTE statement. These 
include the following: 
Name of the route: The name of the route follows the normal object naming rules. 
Name of the service: The name of the service to which the route should apply is another 
parameter. You can either specify the name of the speciﬁ c service, or you can omit the service 
name from the CREATE ROUTE statement, which causes the route to apply to all 
services. When specifying the service name as part of the CREATE ROUTE statement, 
the service name is always case-sensitive, ignoring the databases collation setting. The 
reason for this is that the SQL Server does a binary compare of the route’s service setting 
and the service name within the database. Because uppercase and lowercase characters have 
different binary values, if a single character does not match, the route will not apply.
BROKER_INSTANCE: The BROKER_INSTANCE is an optional parameter that tells the route to 
which database on the server to send the messages. The BROKER_INSTANCE value can be 
queried from the sys.databases catalog view on the instance that hosts the database to 
which the route is pointing. If the BROKER_INSTANCE value is not speciﬁ ed, the SQL Service 
Broker attempts to identify the destination database on the instance based on matching the 
destination service name with the service names in the databases on the remote instance.
LIFETIME: This is also an optional parameter. The LIFETIME parameter tells the SQL 
Service Broker for how many seconds the route should be active. When the lifetime of the 
route has expired, the route will be ignored. If the LIFETIME is omitted or a value of NULL 
is speciﬁ ed, the route will never expire.
ADDRESS: The ADDRESS parameter is a required parameter that tells the SQL Service 
Broker how to contact the remote database. This parameter can specify an IP address, a 
network name, or a fully qualiﬁ ed domain name followed by the TCP port number of the 
service broker endpoint that must be created on the remote instance in the format of 
TCP://ServerName:PortNumber. If the destination database is located on the same 
instance as the source database, then the ADDRESS parameter can be speciﬁ ed as LOCAL. 
If the parameter is speciﬁ ed as TRANSPORT, then the SQL Service Broker attempts to identify 
which remote instance to connect to based on the name of the service.
MIRROR_ADDRESS: This optional parameter conﬁ gures the route to support database 
mirroring if the destination database is conﬁ gured for database mirroring. If the destination 
database is conﬁ gured for database mirroring and the MIRROR_ADDRESS is not speciﬁ ed and 
the database were to failover to the mirror instance, the messages would not be delivered 
until the database failed back to the instance speciﬁ ed in the ADDRESS parameter. The value 
of the MIRROR_ADDRESS parameter should be speciﬁ ed in the same format at the ADDRESS 
parameter.
The following code snippet shows the use of the various parameters when using the CREATE ROUTE 
statement.
CREATE ROUTE ExpenseRoute
    WITH SERVICE_NAME = ‘MyService’,
    BROKER_INSTANCE = ‘53FA2363-BF93-4EB6-A32D-F672339E08ED’,
    ADDRESS = ‘TCP://sql2:1234’,
    MIRROR_ADDRESS = ‘TCP://sql4:4567’ ;
➤
➤
➤
➤
➤
➤
Conﬁ guring SQL Server Service Broker ❘ 155

156  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
Priorities
SQL Server Service Broker priorities assign priorities to conversations to force speciﬁ c conversations 
to always be processed before lower priority conversations. This can be important in high load 
environments in which some messages need to be processed before others. The conversation priority 
is assigned by matching the name of the contract, the source service name, and the destination 
service name to what was conﬁ gured in the Service Broker Priority.
Because you don’t speciﬁ cally set a conversation’s priority when the conversation is created, it is 
wise to create multiple contracts, all of which use the same message types and are conﬁ gured to be 
used for the speciﬁ ed services. Using priorities to create a high priority conversation and a lower 
priority conversation could be done by creating a contract with the name “ContractLow” and 
a second contract named “ContractHigh.” Then a priority could be named that triggers on the 
ContractHigh, which has a high priority level assigned.
Creating SQL Server Service Broker priorities is done via T-SQL using the CREATE BROKER PRIORITY 
statement. This statement accepts ﬁ ve different values, including the name of the priority. The next 
three values enable you to specify the name of a service, or you can specify the special value of ANY, 
which causes that priority to be applied to any conversation. The last parameter is the priority level 
that will be used for the conversations to which this priority will be applied. The priority can be any 
whole number inclusively between the numbers 1 and 10. Conversations that do not have a speciﬁ c 
priority applied to them are assigned the priority of 5. The usage of the CREATE BROKER PRIORITY 
statement is shown in the following code snippet, in which a message sent to any service using the 
contract name MyHighPriority would be given the priority of 8 instead of the default of 5.
CREATE BROKER PRIORITY HighPriority
FOR CONVERSATION
SET ( CONTRACT_NAME = MyHighPriority ,
      LOCAL_SERVICE_NAME = ANY ,
      REMOTE_SERVICE_NAME = N’ANY’ ,
      PRIORITY_LEVEL = 8
)
Conversation Groups
Conversation groups control the order that messages are processed when those messages are sent to 
different services. This is done by putting the conversations sent from the same service to different 
services into a single group. This in-order processing is done through a process called conversation 
locks, which ensures that messages within the conversations in the same conversation group are 
processed “exactly once in order” or EOIO. This conversation locking is done automatically 
whenever a message is sent or received on the conversation group.  
By default each conversation is put into its own conversation group unless a conversation group is 
speciﬁ ed when the conversation is created. You can specify the conversation group into which the 
new conversation should be placed in two ways. You can do this by specifying the conversation 
group that should be used, or by specifying the handle of the conversation that the new conversation 
should be grouped with.

There is no speciﬁ c command to create a new conversation group. When a new conversation is 
started, a new group is created automatically, and it is assigned a new GUID value as its identiﬁ er. 
To assign new conversations to a speciﬁ c conversation group, a new GUID value simply needs to be 
assigned as the RELATED_CONVERSATION_GROUP parameter for the BEGIN DIALOG CONVERSATION 
statement, which is covered later in this chapter.
If you want to query the conversation group that the next message to be processed is a memory of, 
you can do this using the GET CONVERSATION GROUP statement. To use this statement, specify a 
variable that the next conversation group will be placed into, as well as the name of the queue to get 
the conversation group from, as shown in the following code.
DECLARE @conversation_group_id AS UNIQUEIDENTIFIER;
GET CONVERSATION GROUP @conversation_group_id
FROM YourQueue;
USING SQL SERVER SERVICE BROKER
Sending and receiving messages through SQL Server Service Broker is a basic task. Instead of using 
INSERT to put messages into the queue, like you would with a table, and SELECT to pull messages 
from the queue, you use the SEND statement to send messages and the RECEIVE statement to pull 
messages from the queue.
Sending Messages
You can send messages using the SEND command. The SEND command accepts only two parameters: 
the conversation ID and the body of the message, as shown in the following code snippet. You can 
get the conversation ID from the BEGIN DIALOG CONVERSATION command, as shown in the previous 
code snippet used to assign priorities.
DECLARE @message_body AS XML, @dialog_handle as UNIQUEIDENTIFIER
SET @message_body = (SELECT * 
     FROM sys.all_objects as object 
     FOR XML AUTO, root(‘root’))
BEGIN DIALOG CONVERSATION @dialog_handle
     FROM SERVICE [YourSourceService]
     TO SERVICE ‘YourDestinationService’
     ON CONTRACT [YourContract];
SEND ON CONVERSATION @dialog_handle
MESSAGE TYPE YourMessageType
(@message_body)
GO
The BEGIN DIALOG CONVERSATION command accepts several parameters, some of which are required 
and some of which are optional. The ﬁ rst three parameters are required and are the source and 
Using SQL Server Service Broker ❘ 157

158  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
destination services that you send the message and the contract from and to, respectively. Optionally, 
after the destination service name, you can specify the service broker GUID of the destination 
database or CURRENT DATABASE. The default value is CURRENT DATABASE which causes messages to be 
routed to the same database within which the user is running the code to send the message. The third 
parameter is the contract that deﬁ nes the message types used to send the messages.
The rest of the parameters are all optional. The ﬁ rst two can be either the RELATED_CONVERSATION 
or the RELATED_CONVERSATION_GROUP that both specify a certain conversation group that relates the 
new conversation to another conversation. The RELATED_CONVERSATION accepts the conversation 
ID from another pre-existing conversation. The RELATED_CONVERSATION_GROUP accepts a speciﬁ c 
conversation group ID that the new conversation would then be a member of.
The next parameter is the LIFETIME of the conversation, which speciﬁ es the amount of time that 
the conversation remains open. The LIFETIME is the number of seconds until the conversation closes 
automatically. The LIFETIME value is expressed as an integer data type with the default being the 
maximum value of the INT data type, which is 2^31-1 (2,147,483,647).
The last parameter is the ENCRYPTION parameter, which speciﬁ es whether the messages within 
the conversation should be encrypted while in transmission to another instance of SQL Server. 
This parameter accepts only ON or OFF, and encryption is ON by default. When sending 
messages between database instances, it is highly recommended that encryption be ON. When 
ENCRYPTION is ON and messages are sent within the same instance while the data isn’t actually 
encrypted, the database master key and the certiﬁ cates needed for the encryption are required for 
the conversation to successfully begin and to send the message.
The code shown in the previous example is a great start, but it isn’t good for high-performance 
SQL Server Service Broker workloads. This is because the cost of creating a conversation for each 
message is expensive. When working with high-load systems that send hundreds of thousands 
or millions of messages per day, you want to reuse conversations sending multiple messages 
per conversation to reduce the overhead of sending messages. You can easily do this by logging 
the conversation handle (the value of the @dialog_handle value that is set in the BEGIN DIALOG 
CONVERSATION command) to a table so that it can be retrieved by future sessions. A table like this is 
shown in following code snippet.
CREATE TABLE dbo.SSB_Settings
([Source] sysname NOT NULL,
[Destination] sysname NOT NULL,
[Contract] sysname NOT NULL,
[dialog_handle] uniqueidentifier
CONSTRAINT PK_SSB_Setting PRIMARY KEY ([Source], [Destination], [Contract])
One key requirement for using multiple messages per conversation is that there needs to be a way for 
the sending side of the conversation to tell the receiving side of the conversation that there will be no 
more messages sent over that conversation. An easy way to do this is to have an additional message 
type within the database that is speciﬁ cally used as a trigger on the destination side so that it knows 
when to end the conversation. In the following code, you can notice that a message type named 
EndOfConversation is used to trigger the remote side to close the conversation.
In a high-load environment, a stored procedure could be used to decide if a new conversation should 
be created, as well as storing the value as needed. Listing 6-1 shows the send_sequence value 


160  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
LISTING 6-1 (continued)
          AND [Contract] = @Contract;
     IF @@ROWCOUNT = 0
          INSERT INTO dbo.SSB_Settings 
           ([Source], [Destination], [Contract], [dialog_handle])
          VALUES
           (@Source, @Destination, @Contract, @dialog_handle);
END;
/*Send the message*/
SEND ON CONVERSATION @dialog_handle
MESSAGE TYPE @MessageType
(@XML);
/*Verify that the conversation handle is still the one logged in the table. 
  If not then mark this conversation as done.*/
IF (SELECT dialog_handle 
    FROM dbo.SSB_Settings  
    WHERE [Source] = @Source 
        AND [Destination] = @Destination 
        AND [Contract] = @Contract) <> @dialog_handle 
    SEND ON CONVERSATION @dialog_handle
        MESSAGE TYPE EndOfConversation;
GO
Receiving Messages
Receiving messages is done using the RECEIVE command. The RECEIVE command is written much 
like a SELECT statement where the person writing the statement can specify the columns that should 
be returned, and the queue is speciﬁ ed as the FROM statement, as shown in the following code snippet. 
After the data has been received into a variable, anything that needs to be done with it can be done.
DECLARE @dialog_handle UNIQUEIDENTIFIER, @message_body XML
RECEIVE TOP (1) @dialog_handle = conversation_handle, 
     @message_body = CAST(message_body as XML)
FROM YourDestinationQueue
/*Do whatever needs to be done with your XML document*/
END CONVERSATION @dialog_handle
There should also be a second set of code, usually conﬁ gured as an activated stored procedure on 
the source queue to read received all messages, and to then end the conversation of all messages 
that are received.
As with the code shown in the Sending Messages section, the basic code in this previous example is 
not the most efﬁ cient way to receive data. It is more efﬁ cient to receive multiple messages at once, 
and to receive the message body from the queue as the raw binary, and then convert it to XML (or 
whatever data type it was sent as) after it has been removed from the queue. The following code 
shows how to receive multiple messages in a single statement.


162  ❘  CHAPTER 6  SERVICE BROKER IN SQL SERVER 2012
Sending Messages Between Instances
One of the most powerful features of SQL Server’s Service Broker is its capability to send messages 
between databases on different instances, which run on different physical (or virtual) servers. 
Conﬁ guring SQL Service Broker to send messages between instances is effectively the same as 
conﬁ guring SQL Service Broker to send messages between databases on the same SQL Server 
instance. The major difference is the steps needed to conﬁ gure the authorization of communications 
between the instances. These steps are outlined in the following list; they should be done on both of 
the instances that will be exchanging SQL Service Broker Messages:
 1. 
First, conﬁ gure the Database Master Key in the master database.
 2. 
Then, conﬁ gure the Database Master Key in the application database.
 3. 
Next, create a certiﬁ cate in each database. 
 4. 
Exchange the certiﬁ cates between the databases.
 5. 
Now create SQL Service Broker Endpoints on each instance.
 6. 
Finally, conﬁ gure routes to connect to the remote instances SQL Service Broker Endpoint.
After you complete these steps, messages can route between the two databases.
Database Master Key
Before you begin using SQL Service Broker between instances, you must enable the database master 
key for both databases by using the CREATE MASTER KEY statement on both databases. If this has 
already been done, you do not need to do this again. Creating the master key is quite simple, as 
shown in the following code snippet, because the command accepts only a single parameter, which 
is the password used to secure the database master key.  
The database master key is a symmetric key used to protect all the other keys within the database, 
including other symmetric keys, certiﬁ cates, and asymmetric keys.
CREATE MASTER KEY ENCRYPTION BY PASSWORD = ‘YourSecurePassword1!’
After you create the database master key, back it up using the BACKUP MASTER KEY statement so 
that the master key can be recovered if a database failure occurs. Securely store the backup of the 
database master key at an offsite location.
Creating Certiﬁ cates 
When using certiﬁ cate authentication between the endpoints, you must create certiﬁ cates in the 
master databases of the instances that exchange messages. You can create certiﬁ cates using the 
CREATE CERTIFICATE statement within the master database, as shown in the following code 
snippet. When creating the certiﬁ cates on each instance, assign a unique name to each one. The 
easiest way to do this is to include the server and instance name in the name of the certiﬁ cate.
CREATE CERTIFICATE MyServiceBrokerCertificate
WITH SUBJECT = ‘Service Broker Certificate’, 
     START_DATE = ‘1/1/2011’,
     EXPIRY_DATE = ‘12/31/2099’

Exchanging Certiﬁ cates 
Once you create the certiﬁ cates, you need to exchange them. You can exchange certiﬁ cates between 
instances by backing up the certiﬁ cate using the BACKUP CERTIFICATE statement, as shown in the 
following code, on the machine that has the certiﬁ cate. You then need to restore the certiﬁ cate to 
the remote instance using the CREATE CERTIFICATE statement, as shown in the second code snippet.
BACKUP CERTIFICATE MyServiceBrokerCertificate 
     TO FILE=’C:\MyServiceBrokerCertificate.cer’
CREATE CERTIFICATE MyServiceBrokerCertificate
FROM FILE=’c:\MyServiceBrokerCertificate.cer’
SQL Service Broker Endpoints
Endpoints enable users or other SQL Servers to connect to the SQL Server instance that the endpoint 
is created on. For SQL Server instances to send messages to another instance, you must create 
endpoints on each SQL Server instance. You can create endpoints using the CREATE ENDPOINT 
statement, as shown in the following code. Each SQL Server Instance can have only one Service 
Broker endpoint; even if multiple instances send messages to a single server, all communication must 
be done through a single endpoint. Service Broker endpoints support a variety of authentication 
techniques including NTLM-, KERBEROS-, and CERTIFICATE-based authentication, as well as several 
combinations of those three authentication techniques. When doing cross-instance authentication 
for SQL Service Broker, messaging CERTIFICATE authentication is recommended because it removes 
the dependency on Active Directory.  
USE master
GO 
CREATE ENDPOINT ServiceBrokerEndpoint
STATE = STARTED
AS TCP (LISTENER_PORT = 1234, LISTENER_IP=ALL)
FOR SERVICE_BROKER
(AUTHENTICATION = CERTIFICATE MyServiceBrokerCertificate, 
     ENCRYPTION = REQUIRED ALGORITHM RC4);
GO
You can also conﬁ gure encryption on the endpoint with the encryption being either DISABLED, 
SUPPORTED, or REQUIRED. Encryption is supported using both the RC4 or AES algorithms as well as 
combinations of both algorithms speciﬁ ed as AES RC4 and RC4 AES.
When conﬁ guring the SQL Server Service Broker endpoint, a speciﬁ c TCP port, separate from the 
default SQL Server TCP port that the instance is listening on, needs to be speciﬁ ed. You must also 
specify the IP address that the endpoint should be listening on. In the preceding code example, TCP 
port 1234 is used to listen on, and the endpoint can listen on all IP addresses that are conﬁ gured on 
the server that the instance runs on. If the endpoint should listen only on a speciﬁ c IP address, the 
IPv4 or IPv6 address should be speciﬁ ed where the LISTENER_IP setting is speciﬁ ed.
External Activation
External activation is different from the normal activated stored procedures, which are available via 
the CREATE QUEUE or the ALTER QUEUE statements. External Activation runs as a separate Windows 
Using SQL Server Service Broker ❘ 163


SUMMARY
Although SQL Server Service Broker is quite complex to set up, it is an extremely powerful tool to 
use when you require asynchronous messaging. The SQL Server Service Broker is a ﬂ exible solution 
that can enable you to send messages within the database, from database to database within the 
same SQL Server instance, or from database to database between servers even if the servers are 
located next to each other or are half a world apart.
As you have seen through this chapter, you need to conﬁ gure a variety of objects. Although SQL 
Server Service Broker may at ﬁ rst appear to be quite complex, after some time working with it, the 
system becomes quite easy to use, and all the pieces begin to make sense. 
Summary ❘ 165



Once you understand the beneﬁ ts of CLR, the question then arises of how do you know when to use 
it, especially over T-SQL? The recommendation handed down by Microsoft is to use “CLR-based 
programming for logic that cannot be fully expressed declaratively in T-SQL, and to complement 
the expressive power of the T-SQL query language.” 
The key word in that sentence is “complement.” The CLR should never be considered a replacement 
(except for extended stored procedures); rather the CLR should be used to quickly and efﬁ ciently 
solve the programming challenges that the T-SQL language cannot easily solve, such as string 
parsing, complex mathematics, CPU-centric workloads, and accessing external resources. You 
frequently see the CLR used when creating user-deﬁ ned functions, such as scalar UDFs.
What makes this decision a little more difﬁ cult is that the T-SQL language continues to be improved, 
and many of the things that you couldn’t easily do in T-SQL is now quite easy and efﬁ cient in T-SQL. 
So the answer to the question of which to use becomes, “it depends.” Do some testing. Write the 
solution in both T-SQL and .NET to see which one performs better. The more you use it, the better 
feel you can get as to which method should be used for a given situation. It should be clear that the 
CLR rarely out-performs T-SQL when it comes to data access from within SQL Server.
With that introduction, now turn to how CLR is integrated into the database engine.
SQL Server as a .NET Runtime Host
A runtime host is deﬁ ned as any process that loads the .NET runtime and runs code in a managed 
environment. In SQL Server 2008, the database programming model improved signiﬁ cantly by 
hosting the .NET Framework 3.5. With CLR integration, also called the SQLCLR, SQL Server 
enables .NET programmers to write stored procedures, user-deﬁ ned functions, and triggers in any 
.NET-compatible language, especially C# and VB .NET.
With Visual Studio 2010, you can add user-deﬁ ned types and aggregates to the list of CLR objects 
you can create with SQL Server 2008 and SQL Server 2008 R2. This functionality has been around 
for quite a while and has been the “go-to” method for deploying SQLCLR objects into SQL Server.
Alternatively, Microsoft created a set of tools for Visual Studio called SQL Server Data Tools 
(SSDT), which offers SQLCLR support. SSDT provides an integrated environment for database 
developers to easily create or edit database objects. SSDT is available as a free component targeted 
for SQL Server 2005, SQL Server 2008, SQL Server 2008R2, SQL Server 2012, and SQL Azure. 
SSDT will be updated every 4 to 6 months in coordination with SQL Azure releases. You need at 
least Visual Studio 2010 SP1 to work with SSDT.
To ﬁ nd out more, visit http://msdn.microsoft.com/en-us/data/tools.aspx.
One of the great beneﬁ ts of the SQLCLR is that any .NET code that SQL Server runs is completely 
isolated from SQL Server itself. .NET code runs within the SQL Server process space, but SQL 
Server uses a construct in .NET called the Application Domain (AppDomain) to completely isolate 
and separate all resources that the .NET code uses from the resources that SQL Server uses. The 
AppDomain, which is discussed shortly, protects SQL Server from all malicious use of system 
resources. It should be noted that SQL Server manages its own thread scheduling, synchronization 
and locking, and memory management, which adds to the security and performance of the SQLCLR. 
Introduction to the CLR  ❘  169
 
 
 
 

170  ❘  CHAPTER 7  SQL SERVER CLR INTEGRATION
Application Domains
The primary design goal of placing assemblies in application domains is to achieve scalability, 
security, and the isolation goals needed. Application domains have existed for quite a while to 
provide a form of isolation between applications. This is necessary to ensure that code running in 
one application cannot and does not affect other unrelated applications. You can see this type of 
isolation between operating systems and runtime environments. 
The isolation boundaries created by the application domains also help with the security and 
reliability needed for application development, especially for isolating applications running on 
the same computer. When multiple applications exist on the same computer, each application is 
loaded into separate processes, accomplishing the needed isolation because memory addresses are 
process-related. 
Similarly, SQL Server isolates code between databases by using application domains. As such, 
Application Domains exist for each database that allows you to create, load, and register an 
assembly and call methods and functions within that database, independent of other assemblies 
registered in other databases. 
You can have multiple assemblies per database and one assembly can discover other assemblies at 
execution time using the .NET Framework reﬂ ection application programming interfaces. 
T-SQL versus CLR
With the integration of the CLR in SQL Server, the line that separates what is commonly known as 
the Business Logic Tier and the Data Tier just got a little fuzzier. That certainly is not meant to be 
taken in a negative tone; it just means that you need to do a little more homework when choosing 
where to do what. 
Choosing where to put middle tier logic and database access logic was fairly easy. It is not so 
obvious now with the CLR integrated into SQL Server, but with that comes added functionality and 
ﬂ exibility that can enhance your applications.
Choosing between T-SQL and managed code is not a cut-and-dry decision. T-SQL does some things 
phenomenally well, and managed code does other things well. But that doesn’t mean you should 
throw all data retrieval functionality into a T-SQL stored procedure.
Best practices state that when doing data retrieval, T-SQL is the way to go. Leave the data 
manipulation and CPU-intensive functions and procedures to the managed code side of things, 
especially if there is complex logic being processed on the returned data, such as complex 
mathematical calculations or string parsing. Obviously, you need to consider other things, but every 
situation is different, and more research is required to ﬁ nd the best plan.
The other thing to take into consideration is where the code will be executed. Is the client the 
best place to put certain logic, or will that same logic perform better on the server? Multi-tier 
applications typically have a data layer where much of the data logic is handled, adding the option 
of scalability at this layer. With SQL Server, however, both T-SQL and managed code can be run on 
the server. This brings the added beneﬁ t of server processing power and shortens the gap between 
data and code.

Introduction to the CLR ❘ 171
Don’t discount the client because workstation computers are well powered and can handle a lot of 
the application processing without drastic performance degradation. This means that a lot of the 
application processing can be ofﬂ oaded to the client, freeing up the server for other tasks.
From a performance viewpoint, the CLR is much faster at returning ﬁ le information from the 
operating system than a T-SQL approach, simply because OLE Automation has more overhead 
than the different methods used in CLR. However, raw speed shouldn’t be the only consideration. 
Important considerations include ease of development and ease of maintenance and might take 
precedence over speed, depending on the situation. 
As you can see, there are many pros and cons of each method. Keep in mind that managed code can 
run on either the client or the server; T-SQL can run only on the server.
Enabling CLR Integration
By default, CLR is disabled. This means that you cannot execute any .NET code until you purposefully 
enable the CLR. Not everyone can enable the CLR; only members of the sysadmin and serveradmin 
server roles can do so, or any user granted the server-level permission ALTER SETTINGS.
Enabling CLR is as simple as running a query. There are a couple dozen “advanced” SQL Server 
settings that can be changed or modiﬁ ed only via T-SQL. They are not found in any Properties 
dialog; they are only accessible via the sp_configure option. The syntax for enabling CLR is the 
following:
EXEC sp_configure ‘clr enabled’, 1
GO
RECONFIGURE
GO
Don’t enable the CLR yet though; there are still a few more things to consider.
There is another advanced setting called lightweight pooling that, when enabled, prevents the 
CLR execution. Per the MSDN documentation, CLR execution is not supported under lightweight 
pooling. The lightweight pooling option provides a means to reduce the system overhead 
associated with excessive context switching. Per Books Online (BOL):
When excessive context switching is present, lightweight pooling can provide 
better throughput by performing the context switching inline, thus helping to 
reduce user/kernel ring transitions.
BOL recommends that you disable one or the other (setting the value to 0). You cannot have both 
options enabled (option set to 1). Features that rely on the CLR but do not work properly when 
lightweight pooling is enabled include the hierarchy data type, replication, and Policy-based 
management.
Finally, enabling the CLR is not an “enable it, leave it” option. When enabled, you should closely 
monitor the SQL Server for several error messages in the error log, including:
Failed Virtual Allocate Bytes: FAIL_VIRTUAL_RESERVE <size> 
Failed Virtual Allocate Bytes: FAIL_VIRTUAL_COMMIT <size>
➤
➤


Creating CLR Assemblies ❘ 173
the assembly code is activated and running. The managed code is executed from the server 
and thus running as part of the user connection, or within the user context. At this point, 
the SqlPipe is accessible. SQLPipe is the SQL Server component that enables managed 
stored procedures to return results back to the caller. Results from a query execution are 
sent back to the client via the caller’s pipe. This is really no different for CLR database 
objects, in that results are sent back to the client via the methods associated with the 
SqlPipe object; send and ExecuteAndSend.
 2. 
After you add the preceding code, save the ﬁ le as MyFirstSqlClr.cs in a directory of your 
choice. (However, remember where you saved this ﬁ le.) In this example, the syntax is C#.
 3. 
Next, compile the assembly. By default, SQL Server installs the .NET Framework 
distribution ﬁ les, which include csc.exe and vbc.exe, the command line compilers for C# 
and VB .NET. These exist in the following location (for .NET 4.0):
C:\Windows\Microsoft.NET\Framework\v4.0.30319
Open a command prompt and navigate to the directory where you previously saved your 
.cs ﬁ le and type in the command, as shown in Figure 7-1.
FIGURE 7-1
 4. 
Use the following syntax to compile your code into an assembly:
csc /target:library myﬁ rstsqlclr.cs
The /target option is the parameter that speciﬁ es to compile the code into an assembly.
 5. 
After the program has been compiled into a library, you should see a new ﬁ le in the 
directory with the same name but with a .dll extension. This is the ﬁ le that will be loaded 
into SQL Server.
 6. 
Open up an instance of SQL Server Management Studio and open a new query window. 
Select the appropriate data in which you want to load the assembly, and type in the 
following T-SQL. (This example assumes that you have a Temp directory in the root of your 
C drive (C:\temp), if not you need to change the FROM location to where your assembly is 
located):
CREATE ASSEMBLY myﬁ rstsqlclr
FROM ‘c:\temp\myﬁ rstsqlclr.dll’ 
WITH PERMISSION_SET = SAFE
The preceding code loads the assembly into SQL Server and creates a SQL Server assembly 
reference using the location of the compiled assembly created in the previous step. Notice 

174  ❘  CHAPTER 7  SQL SERVER CLR INTEGRATION
also the PERMISSION_SET option. This option sets the security level of the assembly. We 
discuss these options in more details shortly.
 7. 
Next, create a T-SQL stored procedure reference to the assembly previously created called 
DoIt. Type in the following T-SQL in the query windows and execute it:
CREATE PROCEDURE DoIt
@i nchar(50) OUTPUT
AS
EXTERNAL NAME myﬁ rstsqlclr.FirstSQLCLRProc.FirstSQLCLR
After the procedure has been created, you can your newly 
created assembly and stored procedure by looking in Object 
Explorer, as shown in Figure 7-2.
 8. 
With your stored procedure created, you can run it like any 
other stored procedure by using standard T-SQL. In the 
query window, type the following T-SQL and execute it:
DECLARE @var nchar(50)
EXEC DoIt @var out
PRINT @var
You should see the following results in the Results window:
Hello world!
My First SQLCLR Assembly!
This isn’t the only way to add assemblies; take a look at another 
tool that enables you to easily add assemblies.
Using Microsoft SQL Server Data Tools
Creating and deploying SQLCLR objects using SQL Server Data Tools is an easy way to create and 
edit database objects. This section shows a quick example on how to use SSDT to create and deploy 
a SQLCLR stored procedure. This example assumes that CLR has been enabled.
 1. 
First, add a new window to the Visual Studio IDE. From the view menu, select Other 
Windows Á Data Tools Operations. This adds the Data Tools Operations windows to the 
bottom of the Visual Studio IDE. The Data Tools Operations shows the progress of many 
of the database operations done the SSDT, such as results from publishing objects to a 
database or deploying a database.
 2. 
Now install the SQL Server Data Tools by going to the following URL: http://msdn
.microsoft.com/en-us/data/tools.aspx
 3. 
When you arrive at the SQL Server Data Tools page, click the Get It icon near the top of 
the page. This takes you to the download page for the Microsoft SQL Server Data Tools 
download page. As of this writing, SSDT is in CTP4, supporting SQL Server 2005, 2008, 
2008R2, and 2012. 
FIGURE 7-2

Creating CLR Assemblies ❘ 175
 4. 
On the download page, click the Download SQL Server Data Tools link in the middle of the 
page, which launches the Web Platform Installer with the SSDT component selected. Click 
Install on the Web Platform Installer for SSDT. The install takes several minutes.
 5. 
When installed, open Visual Studio, and from the View menu, select SQL Server Object 
Explorer (SSOE), which opens the SQL Server Object Explorer dockable window on the left 
of the Visual Studio IDE. 
 6. 
In SSOE, click the Add SQL Server button to register a new SQL Server instance in SSOE. 
In the Connect to Server dialog, enter the server name and login credentials, and then click 
Connect. This registers a new SQL Server in SSOE. 
 7. 
With a new server registered, expand the server node; then expand the databases node. 
Select the database you want to deploy the SQLCLR assembly to, and then right-click that 
database. From the context menu, select Create New Project, which opens the Create New 
Project dialog. In this dialog, type in a project name and location for the new project; then 
select the Create New Solution check box and the Create Directory for Solution check box. 
Click Start, which creates a new Database Project associated to the database selected in the 
SQL Server Object Explorer.
 8. 
After you create the new project, right-click the new project, and from the context menu, 
select Properties, which opens the property pages for this project.
 9. 
On the Project Settings tab, change the Target Platform to SQL Server 2012. Notice that 
you can also set the platform to several versions of SQL Server, including SQL Server 2008, 
2008 R2, and SQL Azure. You can set the permission levels on this tab as well as the target 
framework (.NET 4.0, .NET 3.5, and earlier). Save the project and close the properties page.
 10. 
Right-click the project again in Solution Explorer, and from the context menu, select Add 
Á New Item. In the Add New Item dialog, select the SQLCLR C# from the list of installed 
templates; then select the SQLCLR C# Stored Procedure item. Keep the default name, and 
click OK. A new C# SQLCLR class will be added to the project and open ready for you to 
add code to.
In the class, modify the method to return a string along with the code used in the 
Non-Visual Studio example:
public static void SqlStoredProcedure1 (out string text)
{
    SqlContext.Pipe.Send(“Hello world!” + Environment.NewLine);
    text = “My First SQLCLR Assembly!”;
}
 11. 
Save the class and then build the project to ensure no compile errors exist. With the project 
compiled, the next step is to deploy the new CLR stored procedure. Right-click the project 
in Solution Explorer, and from the context menu, select Publish, which opens the Publish 
Database dialog.
 12. 
In the Publish Database dialog, click the Edit button to set the target server and database. 
When set, click the Publish button on the Publish Database dialog. This packages the 

176  ❘  CHAPTER 7  SQL SERVER CLR INTEGRATION
contents of the database project and deploys them to the selected database. In this case, 
all you have is a SQLCLR stored procedure so that will be the only thing deployed to your 
database.
The output of the publish displays in the Data Tools Operations window. Publish progress, 
messages, and errors display in this window. The publish of the CLR stored procedure should take 
only a minute, and when published, you should see it in the list of assemblies in SQL Server Object 
Explorer in Visual Studio.
When published, you can execute it just as you did in step 7 of the example in the section “The Non-
Visual Studio Way” by calling the CREATE PROCEDURE statement and referencing the new assembly 
you published. At that point you can call the stored procedure the same as you did in step 8.
As you can see, it is easier to use the SQL Server Data Tools to create and publish SQLCLR 
assemblies because it provides a much easier and more efﬁ cient way to work with database objects, 
including CLR objects.
SECURING CLR
There is certainly a potential security risk by implementing the CLR in SQL Server. As such, 
Microsoft wanted to ensure that every measure was taken to ensure a secure hosting environment 
for the CLR, and includes the following goals:
SQL Server should not be compromised by the running of managed code within SQL Server.
No unauthorized access to user data or other user code by managed code should be 
permitted. 
Mechanisms should be in place to restrict user code from accessing any resources outside of 
SQL Server.
Unauthorized access to system resources by managed code should be enforced.
To assist in upholding these goals, the CLR supports a security model called Control Access 
Security (CAS) for managed code. In this model, permissions are given to assemblies based on the 
identity of the code. The set of permissions that can be granted to the assemblies by the SQL Server 
host policy level are determined by the permission that is set and speciﬁ ed during the creation of the 
assembly in SQL Server. SQLCLR supports three permission sets:
SAFE: Only local data access and internal computations are allowed. If no permission is 
speciﬁ ed during the assembly creation, SAFE permission is applied by default. No access to 
external system resources such as ﬁ les or the registry exists.
EXTERNAL_ACCESS: Same permission as SAFE but with the added ability to access external 
resources such as the ﬁ le system, registry, networks, and environment variables.
UNSAFE: Unrestricted access to all resources within SQL Server and outside SQL Server. 
This is the least secure and should rarely be used.
➤
➤
➤
➤
➤
➤
➤

Performance Monitoring ❘ 177
PERFORMANCE MONITORING
As stated earlier, using the CLR should not be considered an “enable it, leave it” option. When 
enabled, you need to make sure it performs as you expect and does what you need it to do. To help 
with this monitoring, several tools and options are available to you: Windows System Monitor, SQL 
Proﬁ ler, and Dynamic Management Views (DMVs).
Windows System Monitor
You can use Windows System Monitor (PerfMon.exe) to monitor CLR activities for SQL Server. 
Use the counter in the .NET CLR group in System Monitor but select the sqlserver instance when 
you monitor CLR counters for SQL Server, as shown in Figure 7-3. The following counters are 
extremely helpful in understanding the health and activity of the programs running in a SQL-hosted 
environment.
FIGURE 7-3
.NET CLR Memory: Provides detailed information about the types of CLR heap memory, 
and garbage collection. These counters can be used to monitor CLR memory usage and 
to ﬂ ag alerts if the memory used gets too large. If the code is copying a lot of data into 
memory, you may have to check the code and take a different approach to reduce memory 
consumption, or add more memory.
.NET CLR Loading: SQL Server isolates code between databases by using AppDomain. 
This set of counters enables monitoring of the number of AppDomains and the number 
➤
➤

178  ❘  CHAPTER 7  SQL SERVER CLR INTEGRATION
of assemblies loaded in the system. You can use this counter to determine loaded CLR 
assemblies. You can also use some of the DMVs for AppDomain monitoring. 
.NET CLR Exceptions: The Exceptions/Sec counter provides you with a good idea of 
how many exceptions the code generates. The values vary from application to application 
because sometimes developers use exceptions to test application functionality, so you 
should monitor over time to set the baseline and go from there. As this number increases, 
performance decreases.
Figure 7-4 shows the .NET CLR Memory:# Induced GC object, which displays the peak number of 
times garbage collection was performed due to an explicit call to GC.Collect. It is a good idea to let 
the garbage collection tune the frequency of its collection.
➤
FIGURE 7-4
For more information on the different .NET counters, please see http://msdn.microsoft.com/
en-us/library/w8f5kw2e(v=VS.100).aspx.
SQL Proﬁ ler
SQL Proﬁ ler has only a single event that you can use to monitor CLR assemblies. It is called 
Assembly Load and tells you via an Assembly Load Succeeded message when an assembly loads. 
If the load of the assembly fails, a message displays that indicates which assembly failed to load 
and provides error code information. Figure 7-5 shows the selection of the Assembly Load event. 
This event can be useful when troubleshooting a query that uses CLR such as a slow running server 
running CLR queries.

Performance Monitoring ❘ 179
Dynamic Management Views (DMVs)
Dynamic management views (DMVs) return server state information that can be used to monitor 
the health of a server instance, diagnose problems, and tune performance. Following are four DMVs 
that pertain to the SQLCLR:
sys.dm_clr_appdomains: Returns a row for each application domain in the server.
sys.dm_clr_loaded_assemblies: Returns a row for each managed user assembly loaded 
into the server address space.
sys.dm_clr_properties: Returns a row for each property related to SQL Server CLR 
integration, including the version and the state of the hosted CLR.
sys.dm_clr_tasks: Returns a row for all CLR tasks currently running.
Although these four DMVs provide great information, they don’t provide the performance tuning 
information you need. To provide more insight into the operation and execution of CLR assemblies, 
you can use the following DMVs to give you that information for CLR assemblies:
sys.dm_exec_query_stats
sys.dm_exec_cached_plans
Both of the preceding DMVs apply to the same executed query. The sys.dm_exec_cached_plans 
DMV can be used to view a cached query plan for a CLR query, whereas the 
sys.dm_exec_query_stats DMV contains a row per query statement within the cached plan. 
With these DMVs you can gather aggregate performance statistics for cached query plans. This 
information can help determine how any queries in your assemblies are performing.
➤
➤
➤
➤
➤
➤
FIGURE 7-5

180  ❘  CHAPTER 7  SQL SERVER CLR INTEGRATION
CLR Integration Design Goals
When integrating the SQLCLR in your architecture, you should keep its design goals in mind: 
Performance: Best practice states that a CLR assembly should not be used to query data. 
This is what T-SQL is meant to do. CLR assemblies should not spend their time accessing 
data. Send the data you want worked on to the assembly instead of having the assembly pull 
it from the SQL Server.
Scalability: SQL Server and the SQL handle memory management differently. SQL Server 
is more cooperative and supports a nonpreemptive threading model. The CLR supports 
a preemptive threading model and does not differentiate between physical and virtual 
memory. These differences present an interesting challenge when building systems that 
need to scale. Thus, as you design your database architecture, do so in such a way that 
the scalability and the integrity of the system are not compromised by user code calling 
unnecessary APIs for threading and memory directly.
Security: Have a security plan and a set of rules for implementing SQLCLR, and stick to 
those rules. Ensure that any managed code follows the rules of SQL Server authentication 
and authorization. Not every piece of managed code needs access to external resources 
either.
Reliability: The key with reliability is that any user code should not be allowed to execute 
any operations that compromise the integrity of the database and Database Engine, 
including not overwriting internal data structures or memory buffers.
SUMMARY
The SQLCLR integration opens up a new world of capabilities for data querying and processing. 
None of this is truly new though; it has been around for more than 6 years. Thus, the purpose of 
this chapter is to simply provide a look at why you should consider using the SQLCLR, what to 
expect, and some guidance for how to use it.
A great addition to working with the SQLCLR is the SQL Server Data Tools, which provide an 
easy way to work with database objects. You can work with different types of SQLCLR objects, 
including stored procedures and user-deﬁ ned functions. Regardless of the type of CLR object you 
create, the key is to understand why you use SQLCLR over T-SQL. 
The SQLCLR isn’t a replacement for T-SQL, but complements the great functionality of T-SQL very 
well. It is up to you to decide how to use the SQLCLR.
Managed Code has its place within SQL Server. In fact, the SQL Server engine uses the CLR 
for some of its data types, such as the XML data type and the geographic data types. There are 
certainly some beneﬁ ts for using the SQLCLR, and the more you work with it the more you will 
ﬁ nd places where you can use it to improve your applications.
➤
➤
➤
➤






186  ❘  CHAPTER 8  SECURING THE DATABASE INSTANCE
Impersonate: The Impersonate privilege grants the granted user the ability to use the EXECUTE 
AS syntax specifying the grantee login the ability to execute code as the grantee login.
View Deﬁ nition: The View Deﬁ nition privilege grants the granted user the ability to view 
the conﬁ guration of the grantee login.
Instance-Wide Settings
Thirty-one privileges can be granted to a speciﬁ c login. Table 8-1 shows these privileges and their 
meaning.
TABLE 8-1: Instance Privileges and Meanings
PRIVILEGE NAME
PRIVILEGE DEFINITION
Administrator bulk 
options
Enables the user to bulk insert data into the SQL Server instance using 
the BULK INSERT statement, the bcp command-line application, and the 
OPENROWSET(BULK) operation.
Alter any availability 
group
Grants the user the right to alter or failover any Always On availability 
group. By granting this privilege, the login is also granted to the Create 
Availability Group privilege.
Alter any connection
Grants the user the right to kill any user connection.
Alter any credential
Grants the user the right to alter any credential within the database instance.
Alter any database
Grants the user the right to change the database options for any database 
within the database instance. By having this right granted, the user is also 
granted the Create Any Database privilege.
Alter any endpoint
Grants the user the right to alter any endpoint that has been created on 
the SQL Server instance. By having this right granted, the user is also 
granted the Create Any Endpoint privilege.
Alter any event 
notiﬁ cation
Grants the user the right to alter any event notiﬁ cation that has been 
created within the SQL Server instance. By having this right granted, the 
login is also granted the Create Trace Event Notiﬁ cation privilege.
Alter any linked 
server
Grants the user the right to alter any linked server within the SQL Server 
instance.
Alter any login
Grants the user the right to alter any login within the instance.
Alter any server audit
Grants the user the right to change any server audit speciﬁ cation.
Alter any server role
Grants the user the right to change the user-deﬁ ned server roles within 
the SQL Server instance.
Alter resources
Grants the user the right to change system resources.
Alter server state
Grants the user the right to change the server state. By having this right 
granted, the login is also granted the View Server State right.
➤
➤

Authorizing Securables ❘ 187
PRIVILEGE NAME
PRIVILEGE DEFINITION
Alter settings
Grants the user the right to change instance-wide settings.
Alter trace
Grants the user the right to change other user’s proﬁ ler and server side traces.
Authenticate server
Grants the user the right to authenticate against the SQL Server instance.
Connect SQL
Grants the user the right to connect to the SQL Server instance.
Control server
Grants a superset of instance level rights: Administrator bulk options, Alter 
Any Availability Group, Alter Any Connection, Alter Any Credential, 
Alter Any Database, Alter Any Endpoint, Alter Any Event Notiﬁ cation, Alter 
Any Linked Server, Alter Any Login, Alter Any Server Audit, Alter Any 
Server Role, Alter Resources, Alter Server State, Alter Settings, Alter 
Trace, Authenticate Server, Connect SQL, External Access Assembly, 
Shutdown, Unsafe Assembly, and View Any Deﬁ nition.
Create any database
Enables the user to create a new database or to restore a database from 
backup.
Create availability 
group
Enables the user to create a new Always On availability group.
Create DDL event 
notiﬁ cation
Grants the user the privilege to create a DDL trigger.
Create endpoint
Grants the user the privilege to create a SQL Server endpoint.
Create server role
Grants the user the privilege to create a user deﬁ ned server role.
Create trace event 
notiﬁ cation
Grants the user the privilege to create a trace event notiﬁ cation.
External access 
assembly
Grants the user the privilege to create an assembly that requires the 
external access setting.
Shutdown
Grants the user the privilege to shut down the SQL Server instance by 
using the SHUTDOWN T-SQL statement.
Unsafe assembly
Grants the user the privilege to create an assembly that requires the 
unsafe setting.
View any database
Grants the user the privilege to view the deﬁ nition of any database within 
the SQL Server instance.
View any deﬁ nition
Grants the user the privilege to view the deﬁ nition of any object within the 
SQL Server instance. By granting this right, the login is also granted the 
View Any Database privilege.
View server state
Grants the user the privilege to view the server state objects. These server 
state objects include the SQL Servers dynamic management views and 
functions.


Authorizing Securables ❘ 189
members of the role the right to bulk insert data into the database. In other previous versions of 
SQL Server, bulk loading data into the database required being a member of the most powerful 
ﬁ xed server role, the sysadmin, a role that provides the ability to perform any action against any 
database without restriction. Other ﬁ xed server roles grant various rights to the members of the 
roles and are discussed in the following list:
The dbcreator ﬁ xed server role grants the user the right to create databases. 
The diskadmin ﬁ xed server role grants the user the rights to manage the physical database 
ﬁ les. 
The setupadmin ﬁ xed server role grants the user the rights to add and remove linked 
servers.  
The processadmin grants the rights to kill other users’ processes within the SQL Server 
instance.  
The securityadmin ﬁ xed server role enables the members of the role to GRANT, DENY, 
and REVOKE all server-level permissions as well as any database-level permissions for the 
databases which they have rights to.  
The serveradmin ﬁ xed server role enables the members to change any server-wide 
conﬁ guration option as well as use the SHUTDOWN command to shut down the SQL Server 
instance. 
The ﬁ nal ﬁ xed server role is the public role, which grants no rights; all logins on the 
instance are members of the public role.
Database Securables
Objects of various kinds exist within each SQL Server database, all of which have their own 
permissions. These permissions grant speciﬁ c users rights to those objects so that the users can 
perform the functions needed to complete their tasks. It is a best practice to grant the users who 
will be using the database the minimum permissions needed to complete her job. This is done so 
that the users don’t have rights to objects or data within the database that they don’t need. An 
added beneﬁ t of this practice is to prevent someone who breaks into the database from gaining 
access to more secure data. 
Database Permissions
Permissions can be granted against the database itself. Some of these rights are speciﬁ c to the 
database level while some cascade down to objects within the database such as tables, views, and 
stored procedures. 
Tables and Views
When dealing with tables and views, ten different rights can be granted to speciﬁ c users or user-
deﬁ ned database roles. These are listed in Table 8-2.
➤
➤
➤
➤
➤
➤
➤

190  ❘  CHAPTER 8  SECURING THE DATABASE INSTANCE
TABLE 8-2: Rights for Tables and Views
RIGHT
DEFINITION
Alter
Grants the user the ability to change the schema of the object.
Control
Grants the user all other rights on the object.
Delete
Grants the user the ability to delete data from the object.
Insert
Grants the user the ability to insert data into the table.
References
Grants the user the ability to create a foreign key on the table. This right 
does not apply to views.
Select
Grants the user the ability to select the data from the object.
Take Ownership
Grants the user the ability to change the ownership of the object.
Update
Grants the user the ability to change data within the table.
View Change 
Tracking
Grants the user the ability to view the change tracking information for the 
object in question.
View Deﬁ nition
Grants the user the ability to view the schema design of the object.
Stored Procedures and Functions
Stored procedures, functions, and most other database objects within SQL Server contain only ﬁ ve 
permissions that can be granted to users or roles, and are listed in Table 8-3.
TABLE 8-3: Rights for Stored Procedures, Functions, and Most Other Database Objects
PERMISSION
DEFINITION
Alter
Enables the user to change the schema of the database object the right was 
granted to
Control
Grants the user the other rights to the object
Execute
Enables the user to execute the object
Take Ownership
Enables the user to change the owner of the object
View Deﬁ nition
Enables the user to view the schema of the object without having the ability to 
change the object
Permission Chains
Database permissions are chained from higher level code objects to lower level objects referenced 
within the object. If a table and stored procedure existed, as shown in the following code snippet, 

Authorizing Securables ❘ 191
and a user were granted rights to execute the stored procedure, the permissions chain would enable 
the user the right to query the table, but only from within the context of the stored procedures. Any 
queries run from outside the stored procedure do not work unless the user has been granted speciﬁ c 
rights to the table.
CREATE USER SampleUser WITHOUT LOGIN 
GO
CREATE TABLE dbo.Users
(UserId INT IDENTITY (1,1) PRIMARY KEY,
UserName varchar(100),
Password varchar(100))
go
CREATE PROCEDURE dbo.SignIn
     @UserName varchar(100),
     @Password varchar(100)
AS
SELECT UserId
FROM Users
WHERE UserName = @UserName
     and Password = @Password
GO
GRANT EXEC ON dbo.SignIn to SampleUser
GO
Permissions chains work only within the context of the same execution of the parent object. This 
is a fancy way to say that the permission chain does not work with dynamic SQL. When using 
dynamic SQL, the user must be granted rights to the speciﬁ c objects called within the dynamic SQL.
Permissions chaining works within all native SQL Server objects including stored procedures; scalar 
functions; table valued functions and views and chains down to other stored procedures; 
scalar functions, table valued functions, views, tables, and service broker objects. Permission 
chaining is enabled by default and cannot be disabled. Permission chaining is what enables the best 
practice of only granting the minimum set of permissions needed because it only requires execution 
rights on the stored procedures that are called and not the base objects.
Cross Database Permission Chains
Cross database permission chaining is a newer feature of SQL Server, introduced in SQL Server 
2000 Service Pack 3a. Cross database permission chaining is an extension of traditional permission 
chaining, but it enables the permissions chaining to apply between databases. Without cross 
database permission chaining, accessing objects in a second database using the three-part name of 
the object would require that the user have the necessary rights on the object. Looking at the script 
shown in the following code snippet, for the stored procedure to work as written, the user would 
need to have the SELECT privilege.
USE master
GO
CREATE DATABASE Sample1
GO
CREATE DATABASE Sample2

192  ❘  CHAPTER 8  SECURING THE DATABASE INSTANCE
GO
USE Sample1
GO
CREATE TABLE dbo.Users
(UserId INT IDENTITY (1,1) PRIMARY KEY,
UserName varchar(100),
Password varchar(100))
go
USE Sample2
GO 
CREATE TABLE dbo.Users
(UserId INT IDENTITY (1,1) PRIMARY KEY,
UserName varchar(100),
Password varchar(100))
go
CREATE PROCEDURE dbo.VerifyUsers
AS
SELECT *
FROM dbo.Users b
WHERE NOT EXISTS (SELECT * FROM Sample1.dbo.Users a WHERE a.UserId = b.UserId)
GO
Cross database chaining is a database level setting which is 
disabled by default on all databases. To enable it, the cross database 
setting needs to be enabled on both the database that the procedure 
exists in and the database that the table exists in. You can see if cross 
database chaining is enabled in a couple ways. The easiest is to query 
the sys.databases catalog view, speciﬁ cally looking at the is_db_
chaining_on column that returns a 0 when cross database chaining is 
disabled and a 1 when cross database chaining is enabled, as shown in 
Figure 8-1.
The status of the database chaining can also be viewed by looking at the properties window of the 
database. This can be done by connecting to the database instance in the object explorer. 
 1. 
Navigate to Databases; then select the database in question. 
 2. 
Right-click the database and select Properties from the context menu that opens. 
 3. 
Select the options page, and scroll down in the Other Options section looking at the 
Miscellaneous section, as shown in Figure 8-2. 
 4. 
Within the Miscellaneous section, the Cross-Database Ownership Chaining Enabled 
ﬁ eld is visible, although grayed out. View the setting from the screen shown in Figure 8-2; 
although, you must use T-SQL to change this setting using the ALTER DATABASE statement.
FIGURE 8-1

Row Level Security ❘ 193
As stated, enabling cross database chaining requires using the T-SQL ALTER DATABASE statement 
so that the DB_CHAINING option is enabled, as shown in the following code snippet. The ALTER 
DATABASE statement must be used against both databases as shown in the following code snippet so 
that the stored procedure shown in the previous code snippet works as written.
ALTER DATABASE Sample1 SET DB_CHAINING ON
GO 
ALTER DATABASE Sample2 SET DB_CHAINING ON
GO 
One additional right must be granted for cross database chaining to correctly work. The user must 
be added as a user within the second database. The user does not need any additional rights within 
the database except to exist as a user within the second database either by creating a user that maps 
to the same login that the ﬁ rst database user is mapped to or by enabling the guest user within the 
second database. Enabling the guest user is not recommended because that would grant other users 
who may not need rights to the database rights that they do not need.
ROW LEVEL SECURITY
Row level security is something people are always asking about because Oracle has the concept for 
virtual private databases that enables the DBA to specify which rows the user can access. 
FIGURE 8-2


Summary ❘ 195
This includes properly securing the network, the Windows operating system that hosts the database, 
the instance, the database permissions, and the application that needs to be secured to ensure that 
SQL Inject attacks are not be successful. 
With each release of Microsoft SQL Server, the face of security within the platform changes and 
administrators must adapt. With the release of SQL Server 2012, a powerful new tool has been 
added in the user deﬁ ned server roles that enables a much more ﬂ exible security model.


198  ❘  CHAPTER 9  CHANGE MANAGEMENT
CREATING SOLUTIONS AND PROJECTS
Visual Studio projects are generally a foreign concept to DBAs because the idea originated in 
the programming world. For SQL Server administrators, the Visual Studio concepts and tools 
are extended to SQL Server Data Tools and Management Studio as well. Solutions and projects 
enable you to separate your objects and resources into separate units to help in management 
and organization. Using the hierarchy of Visual Studio, ﬁ rst you create a solution, which can 
contain many projects, which can contain many ﬁ les for the project. For example, you may 
have a solution called LoanApplication that contains two projects: one for your C# program 
and another for the DDL to create the database. In the business intelligence world, these 
solutions help you group all the related ﬁ les, such as SSRS reports, SSAS cubes, and the SSIS 
packages to load the data. This separation of projects within solutions also aids in isolating 
changes using source control, especially when different teams or developers work on separate 
projects in the same solution. This is possible as each project and its related objects can be 
checked in and versioned individually using source control. You learn about source control 
later in this chapter.
Inside Management Studio, you can also create solutions and projects to hold your scripts 
and connections for easy access. When you double-click a script, it automatically implements the 
connection associated with the script. By placing script ﬁ les in a project, you also can easily store 
ﬁ les in Source Safe or another supported source control repository. This enables you to check code in 
and out, allowing for a collaborative development environment, wherein only one DBA or developer 
can be editing the code at one time. This requirement is discussed later in this chapter, but version 
controlling your scripts is often a requirement for change management. You could also check all 
your administrative scripts into Source Safe and share them with other administrators. The next few 
sections walk you through how to create the various pieces of a project that you can then check in to 
a version control system, but ﬁ rst, you’ll need to learn how to create the project itself in Management 
Studio. 
 1. 
Select File Á New Á Project, and then select SQL Server Scripts as your project template. 
 2. 
Call the project ProjectExampleScripts and name the solution AdminBookExamples 
(see Figure 9-1).
 3. 
To ﬁ nish creating a new solution and project, click OK. Three folders are created in the 
project: Connections, Queries, and Miscellaneous. Your database connections reside in the 
Connections folder, and all the queries that you’d like to keep together in the project are in 
the Queries folder. The Miscellaneous folder holds ﬁ les that don’t have a .SQL extension 
but that you’d like to keep in the project, such as a readme.txt ﬁ le that may describe 
instructions for using the project.


200  ❘  CHAPTER 9  CHANGE MANAGEMENT
There are other ways to create this connection. If you create a query ﬁ rst, you’ll be prompted for 
a connection prior to query creation. After you save this query, it also creates a connection in the 
Connections folder. Typically it is easier to create the connection as you create both the query and 
connection at same time.
Creating a Project Query
The Queries folder holds all the queries for your project. To create a query in the project, follow 
these steps: 
 1. 
Right-click the Queries folder, and select New Query. You are prompted to conﬁ rm the 
connection. The default query name is SQLQuery1.sql.
 2. 
Type whatever you want in the Query window, and select File Á Save.
 3. 
Rename the query SampleQuery1.sql by right-clicking it and selecting Rename. Click Save 
All to save the queries and the project.
 4. 
With the query now saved, you’re ready to begin using it. Close the Query window, and 
double-click the query ﬁ le again. This time, the query opens automatically and does not 
prompt you for a connection. You can see what connection is associated with the query by 
right-clicking it and selecting Properties. You can then continue to add queries until your 
project is complete.
Again, generally the main point of creating an entire project and solution is to have integration 
with a source control system. If you use a source control system such as Source Safe, each time you 
add or delete a new script, the project control ﬁ les are checked out. Individual ﬁ les are checked out 
automatically as you open and edit them.
After you create your solution and projects within SQL Server Data Tools, if you have source 
control tools installed, you can then check in the solution or project to your repository. 
The details to use source control are beyond the scope of this book, but generally with a source 
control system such as Visual Source Safe, you can right-click on a project or solution and 
choose to link the item to source control from there. When the items are linked to source 
control, you can check items in and out for editing. When checking items in, you can specify 
version numbers and notes for other team members. This not only helps you properly manage 
changes, but you also have a built-in form of documentation to help support the code 
throughout its life. 
POLICY-BASED MANAGEMENT
As mentioned in the opening paragraph to this chapter, many changes in government regulations 
have forced administrators everywhere to change the way they manage their environments. One 
of the major changes that DBAs have experienced is the need to implement and enforce policies 
departments have in writing regarding their database environments. The real challenge though 
is how do you enforce paper policies? Windows administrators have had the power of Active 

Policy-Based Management ❘ 201
Directory (AD) and policies for quite some time, but unfortunately those AD policies did not 
extend to the database world. Instead, database administrators have had to rely on either rolling 
their own solutions or spending money on third-party software to help monitor various parts 
of their environment, which even then was not necessarily equivalent to enforcing department 
data policies.
With the release of SQL Server 2008, database administrators ﬁ nally had the power of policy 
enforcement bestowed upon them in the form of a new feature called Policy-Based Management. 
In SQL Server 2012 that story continues and is even a part of some major features such as 
AlwaysOn Availability Groups. Policy-Based Management gives administrators the ability to 
create policies that can do things such as prevent objects in the database from being created if 
they don’t use a certain naming standard. You can also create a policy that quickly and easily 
checks to see (and change) the settings such as recovery model across multiple databases. Imagine 
switching hundreds of databases in a development environment to Simple recovery model with the 
single click of a button! In addition, you can use PowerShell to extend the power of Policy-Based 
Management for automation, which enables you to manage your environment in a predictable and 
scalable way.
Policy-Based Management Overview
The ﬁ rst questions most DBAs ask when the SQL development team rolls out a new feature are 
“What does it do?” and “Why should we care?” Policy-Based Management enables a DBA to 
declare her intent regarding how a speciﬁ c server should act and then apply that intent to one or 
more target servers. The short answer to “What does it do?” is that it enables declarative, scalable, 
and repeatable SQL Server management.
Policy-Based Management begins with creating an overall policy that encapsulates several 
pieces: facets, targets, and conditions. First you start with an item called a facet. Facets expose 
individual elements of the SQL Server system so that they can be evaluated and compared 
to compliance criteria. The list of facets is loosely based on the objects in the SQL Server 
Management Object (SQL-SMO) hierarchy, which is the underlying object model for SQL 
Server Management Studio. Most facets are derived from combinations or subsets of SMO 
objects and other server state information. Facets present properties relevant to regular SQL 
Server management tasks.
For example, the Database Maintenance facet exposes several properties essential to the 
maintenance of individual databases, yet there is not a corresponding SMO object implementing 
this facet. Facets do not map directly to SMO objects, nor do they correspond to SQL Server 
Management Studio elements. Facets are a completely new representation of the SQL Server 
internal state.
Facets are listed in alphabetical order, making them easy to ﬁ nd. SQL Server Management Studio 
organizes the SMO objects in hierarchical order, which makes more sense as part of a GUI. Facet 
properties correspond to properties or combinations of properties of the underlying SMO objects. 
For example, a Stored Procedure object has facets for the 19 different properties, as shown in 
Figure 9-3.

202  ❘  CHAPTER 9  CHANGE MANAGEMENT
Properties can be compared using a combination of AND and OR logic to deﬁ ne a wanted state. This 
end state is referred to as a condition. Conditions, which are always expressed as a Boolean value, 
can detect and optionally enforce wanted conﬁ gurations. One key limitation of a condition is that 
it can check the properties of only a single facet at a time. This is why the same property may 
appear in multiple facets. For example, Database Status appears in the Database Facet and the 
Database Maintenance Facet because it is necessary to completely evaluate conditions in both areas. 
Although you can evaluate only one facet at a time, you can choose multiple properties from a single 
facet to make a more ﬂ exible policy.
Policy-Based Management Step by Step
One of the ﬁ rst challenges DBAs face when inheriting a system is ﬁ guring out the backup and 
restore strategy. Because DBAs tend to inherit systems far more often than they develop them from 
scratch, rapidly learning about these systems is critical to the DBAs’ ability to meet the fundamental 
requirements of their position. Looking at this situation from a change management perspective, Policy-
Based Management can really help in this respect. Take a look at the following example and you can 
see how to create a policy that enforces naming conventions on stored procedures created in a database. 
Example: Using Policy-Based Management to Enforce Stored 
Procedure Naming Conventions
One development best practice for stored procedures when creating a stored procedure is preﬁ xing 
the name with usp_ to identify it as a user-created stored procedure. Some folks actually tend to 
name their stored procedures with sp_ to identify it as a stored procedure, but the problem with this 
FIGURE 9-3

Policy-Based Management ❘ 203
is that there are system stored procedures that follow this naming convention. This can become an 
issue if you name your user stored procedure sp_Addusers, for instance, because the system would 
then try to look for the stored procedure inside the system databases ﬁ rst because it assumes it’s a 
system stored procedure. On a busy system, this could cause a performance bottleneck. Also, by 
enforcing a naming convention for user stored procedures, management of those objects is easier, 
and you explicitly know what a user-created object is based on name.
In this example you want to ensure that when someone is attempting to create an object in your 
database, speciﬁ cally a stored procedure, that it follows the required naming convention. To 
accomplish this you create a policy using a condition that uses the Stored Procedure facet to enforce 
this naming convention. If the object does not pass the policy, you set it so that the object won’t get 
created at all in the database, and instead displays an error to the user. If it does pass, you can go 
ahead and allow the object to be created. You also see how to alternatively conﬁ gure the policy to 
allow the change to occur but log it for documentation purposes. 
 1. 
In SQL Server Management Studio, under the Policy-Based Management node under 
Management, right-click the Conditions folder, and select Create New Policy. Name the 
new policy Stored Procedure Naming Policy. 
 2. 
From the Check Condition drop-down menu, click the down arrow, and select Create New 
Condition. This opens the Create New Condition box (see Figure 9-4). Name the condition 
Stored Procedure Naming Conventions. 
 3. 
From the Facet drop-down, select Stored Procedure. 
 4. 
After you select your facet, select the properties to create the condition from. The properties 
for each facet are located under the Field column. When you click the ﬁ eld, the drop-down 
automatically populates with all the properties for that particular facet. From the drop-
down select the @Name option (see Figure 9-5).
FIGURE 9-4





208  ❘  CHAPTER 9  CHANGE MANAGEMENT
LISTING 9-2 (continued)
     FROM [Person].[Contact]
GO
In this version of the script, you changed the name of the stored procedure from 
sp_SampleProc to usp_SampleProc to fall in line with your naming convention. This time 
when you run the code, you get a success message, and your stored procedure object is created 
in the database.
Although this particular policy is an active one, meaning when you enable it and it is 
actively triggered by an action, you also can evaluate it manually. To do this, simply 
right-click the policy in SSMS, and select Evaluate from the context menu. This evaluates 
the policy against the instance in which the policy is currently stored. This particular policy, 
evaluates against all existing stored procedures in all user databases. Because the objects already 
exist, the policy obviously can’t prevent them from being created like it does for new code. 
The upside is that the results of the policy evaluation (see Figure 9-8) can give the 
administrator a comprehensive list of objects to investigate and follow up on for remediation. 
The results can be exported in XML format for reporting purposes by clicking the Export 
Results button. 
FIGURE 9-8


210  ❘  CHAPTER 9  CHANGE MANAGEMENT
LISTING 9-4 (continued)
@facet=N’StoredProcedure’, @object_set_id=@object_set_id OUTPUT
Select @object_set_id
Declare @target_set_id int
EXEC msdb.dbo.sp_syspolicy_add_target_set @object_set_name=N’Stored Procedure
Naming Policy_ObjectSet’, @type_skeleton=N’Server/Database/StoredProcedure’
, @type=N’PROCEDURE’, @enabled=True, @target_set_id=@target_set_id OUTPUT
Select @target_set_id
EXEC msdb.dbo.sp_syspolicy_add_target_set_level @target_set_id=@target_set_id,
@type_skeleton=N’Server/Database/StoredProcedure’,
@level_name=N’StoredProcedure’, @condition_name=N’’
, @target_set_level_id=0
EXEC msdb.dbo.sp_syspolicy_add_target_set_level @target_set_id=@target_set_id,
@type_skeleton=N’Server/Database’
, @level_name=N’Database’, @condition_name=N’’, @target_set_level_id=0
GO
Declare @policy_id int
EXEC msdb.dbo.sp_syspolicy_add_policy @name=N’Stored Procedure Naming Policy’,
@condition_name=N’Stored Procedure Naming Conventions’, @policy_category=N’’,
@description=N’’, @help_text=N’’
, @help_link=N’’, @schedule_uid=N’00000000-0000-0000-0000-000000000000’, 
@execution_mode=1, @is_enabled=True
, @policy_id=@policy_id OUTPUT, @root_condition_name=N’’, @object_set=N’Stored 
Procedure Naming Policy_ObjectSet’
Select @policy_id
GO
This script contains no XML. Because a policy contains only one condition, all its elements are 
deterministic and can be mapped to a well-deﬁ ned set of parameters. 
Policy-Based Management Implementation
Experienced DBAs often ask an extra question beyond just “What does it do?” They want to 
know “How can I apply this in my environment today?” It is important to know if the actual 
implementation of a feature is complete, secure, and scalable before you bet your systems (and your 
job) on it. This section covers how you can implement Policy-Based Management in your enterprise 
environment today.
You’ve already learned a bit about the On-Demand and Scheduled modes. These run directly 
from SQL Server Management Studio and from the SQL Agent, respectively. The other 
mode type is the On-Change event, which you used in the Stored Procedure Naming example. 
On-Change is precisely what the name implies: It captures a change event inside a target SQL 
Server and either logs the change event or prevents the change from occurring. SQL Server 
Policy-Based Management uses DDL triggers to either log or rollback a change event to implement 
On-Change policy evaluation.

Policy-Based Management ❘ 211
On-Change works only for deterministic event-driven events in which something changes inside 
the SQL Server. The earlier example for Stored Procedure Naming used the On-Change mode 
because it speciﬁ cally triggered a DDL event. In addition to raising a DDL event, all the properties 
within the chosen facet had to support the same set of events to use the On-Change:Prevent option. 
Refer to the previously mentioned blog post located here: http://blogs.msdn.com/b/sqlpbm/
archive/2008/05/24/facets.aspx.
The speciﬁ c trigger used to evaluate a policy in On-Change mode can be either a DDL trigger or a 
server trigger, depending on the speciﬁ c facet used to build the condition. Although Policy-Based 
Management greatly simpliﬁ es dynamic management for 
the DBA, it does have its limitations, such as certain facets 
not supporting certain modes. If you need to build a more 
customized solution that patterns Policy-Based Management’s 
behavior, you can create your own DDL triggers. These steps are 
detailed later in the “DDL Trigger Syntax” section.
The next step in scalability is to use an item called a Central 
Management Server (CMS). A CMS is the mechanism a DBA 
uses to apply actions, including policies, from one server and 
have those actions repeated across a designated list of servers. 
Setting up a CMS is no more challenging than using the 
Registered Server list in SQL Server Management Studio. It even 
looks similar (see Figure 9-9).
Registering a Central Management Server is rather simple:
 1. 
Open the Registered Servers by going to the View menu in SSMS and selecting Registered 
Servers. Alternatively, you can use the keyboard shortcut of Ctrl+Alt+G.
 2. 
Expand the Database Engine node. You see a folder labeled Central Management Servers; 
right-click it and select Register Central Management Server from the context menu.
 3. 
In the Server name box, supply the name of a SQL Server instance, which must be SQL 
Server 2008 or higher. Click Save to complete registration.
One difference between the two registration types is that when you save the credentials for 
member servers, you are telling the Central Management Server to save those registrations in its 
msdb database, rather than keeping that information stored on your local workstation. Another 
major difference between the two is that CMS supports only Windows authentication.
When you apply the policy to the Central Management Server, you are actually applying it 
across all the servers registered in the selected evaluation group, for example, say you create two 
server groups in your CMS: one called DEV and one called PROD. If you apply a policy on the 
PROD group, it evaluates only policies against servers under that group. However, if you try to 
evaluate policies against the CMS itself, it evaluates all the groups registered underneath it. You 
can also use the Central Management Server to execute T-SQL commands across multiple servers 
at the same time.
You may be asking yourself, “How do I automate this process?” Luckily there’s already a great 
open-source project freely available on Codeplex called the Enterprise Policy Management 
FIGURE 9-9

212  ❘  CHAPTER 9  CHANGE MANAGEMENT
Framework (EPMF). The EPMF uses PowerShell with your CMS to evaluate polices against your 
enterprise on a scheduled basis. In addition to this, EPMF comes with built-in reports that display 
the policy health state of your environment. These reports enable you to see at a glance how far 
from compliance your enterprise is based on your policies. Setting up and conﬁ guring EPMF is 
beyond the scope of this chapter, but you can ﬁ nd the project and all its documentation at http://
epmframework.codeplex.com.
DDL TRIGGER SYNTAX
The syntax for a DDL (Data Deﬁ nition Language) trigger is much like a DML (Data Manipulation) 
trigger, but with a DDL trigger, instead of monitoring an INSERT statement, you monitor a CREATE 
event such as a CREATE TABLE statement, for example. A DDL trigger is useful if you’re monitoring 
for events such as objects being created. Going back to the previous PBM example that enforced 
naming conventions of new stored procedures being created, that policy is actually a DDL trigger in 
action! The generic syntax looks like this: 
 CREATE TRIGGER <trigger_name>
 ON { ALL SERVER | DATABASE }
 [ WITH <ddl_trigger_option> [ ,…n ] ]
 { FOR | AFTER } { event_type | event_group } [ ,…n ]
 AS { sql_statement  [ ; ] [ …n ] | EXTERNAL NAME < method specifier >  [ ; ] }
Most of this syntax you probably recognize from DML triggers, so we’ll focus mostly on the DDL-
speciﬁ c syntax. There are two scopes you can specify in a DDL trigger: ALL_SERVER or DATABASE. 
As the names imply, the ALL_SERVER scope monitors all server-level events, and the DATABASE 
option monitors database-level events.
The other important conﬁ gurable part of the syntax is after the FOR clause. After the FOR clause, 
you specify what you’d like to monitor in the database or on the server with the DDL trigger option. 
This varies depending on what level of DDL trigger you have. The upcoming sections will break 
down these examples.
Database Triggers
Database DDL triggers are executed when you create, drop, or alter an object at a database level, 
such as a user, table, stored procedure, Service Broker queue, or view, to name a few. If you want to 
trap all database DDL events, you would use the trigger option in the earlier mentioned syntax of 
FOR DDL_DATABASE_LEVEL_EVENTS. The events are hierarchical, and the top-level database trigger 
types are shown in the following list. 
DDL_TRIGGER_EVENTS
DDL_FUNCTION_EVENTS
DDL_SYNONYM_EVENTS
DDL_SSB_EVENTS
DDL_DATABASE_SECURITY_EVENTS
➤
➤
➤
➤
➤

DDL Trigger Syntax ❘ 213
DDL_EVENT_NOTIFICATION_EVENTS
DDL_PROCEDURE_EVENTS
DDL_TABLE_VIEW_EVENTS
DDL_TYPE_EVENTS
DDL_XML_SCHEMA_COLLECTION_EVENTS
DDL_PARTITION_EVENTS
DDL_ASSEMBLY_EVENTS
To create a trigger that would audit for any stored procedure change, deletion, or creation, you 
could use a CREATE TRIGGER statement such as the following: 
 CREATE TRIGGER ChangeWindow
 ON DATABASE
 FOR DDL_PROCEDURE_EVENTS
 AS
 -- Trigger statement here
Under the trigger types mentioned in the list, you can get much more granular on certain events by 
using the event type after the FOR keyword. For example, rather than trap all events when any type 
of table event occurs, you can narrow it down to raise only the DDL event when a table is dropped 
by using the DROP_TABLE trigger option. To monitor for any DROP TABLE, CREATE TABLE, or ALTER 
TABLE statement issued, you could use the following code:  
 CREATE TRIGGER ChangeWindow
 ON DATABASE
 FOR CREATE_TABLE, DROP_TABLE, ALTER_TABLE
 AS
 -- Trigger statement here
Finally, you can monitor all changes by using the DDL_DATABASE_LEVEL_EVENTS event type: 
 CREATE TRIGGER ChangeWindow
 ON DATABASE
 FOR DDL_DATABASE_LEVEL_EVENTS
 AS
 -- Trigger statement here
Another important function in your DDL trigger toolbox is the EVENTDATA()system function. 
The EVENTDATA()system function is raised whenever a DDL trigger is ﬁ red at any level; it outputs 
the event type, the user who executed the query, and the exact syntax the user ran. The function 
outputs this data in XML format, as shown here: 
 <EVENT_INSTANCE>
   <EventType>CREATE_USER</EventType>
   <PostTime>2006-07-09T12:50:16.103</PostTime>
   <SPID>60</SPID>
   <ServerName>TORCHWOOD</ServerName>
➤
➤
➤
➤
➤
➤
➤

214  ❘  CHAPTER 9  CHANGE MANAGEMENT
   <LoginName>TARDIS\jsegarra</LoginName>
   <UserName>dbo</UserName>
   <DatabaseName>AdventureWorks</DatabaseName>
   <ObjectName>jorge</ObjectName>
   <ObjectType>SQL USER</ObjectType>
   <DefaultSchema>jorge</DefaultSchema>
   <SID>q7ZPUruGyU+nWuOrlc6Crg==</SID>
   <TSQLCommand>
     <SetOptions ANSI_NULLS=“ON” ANSI_NULL_DEFAULT=“ON” ANSI_PADDING=“ON”
 QUOTED_IDENTIFIER=“ON” ENCRYPTED=“FALSE” />
     <CommandText>CREATE USER [jorge] FOR LOGIN [jorge] WITH DEFAULT_SCHEMA
 =
 [dbo]</CommandText>
   </TSQLCommand>
 </EVENT_INSTANCE>
You can then either pull all the data from the EVENTDATA()function and log it into a table as an 
XML data type, or pull selective data out using an XPath query. To do an XML XPath query in 
SQL Server, you would specify the path to the XML node. In DDL triggers, the key elements from 
the EVENTDATA()function are as follows: 
EventType: The type of event that caused the trigger.
PostTime: The time the event occurred.
SPID: The SPID of the user who caused the event.
ServerName: The name of the instance on which the event occurred.
LoginName: The login name that performed the action that triggered the event.
UserName: The username that performed the action that triggered the event.
DatabaseName: The name of the database in which the event occurred.
ObjectType: The type of object that was modiﬁ ed, deleted, or created.
ObjectName: The name of the object that was modiﬁ ed, deleted, or created.
TSQLCommand: The T-SQL command that was executed to cause the trigger to be run.
To pull out selective data, you could use code like the following to do an XPath query. You would 
ﬁ rst pass in the fully qualiﬁ ed element name, such as /EVENT_INSTANCE/TSQLCommand, and the 
[1] in the following code means to pull out the ﬁ rst record. Because there is only one record in 
the EVENTDATA()function, you always pull only the ﬁ rst record. The EVENTDATA()function is only 
available to you in the scope of the trigger. If you were to run the following query outside the trigger, 
it would return NULL: 
 CREATE TRIGGER RestrictDDL
 ON DATABASE
 FOR DDL_DATABASE_LEVEL_EVENTS
 AS
 
 EXECUTE AS USER = ‘DBO’
 
 DECLARE @errordata XML
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤


216  ❘  CHAPTER 9  CHANGE MANAGEMENT
LISTING 9-6 (continued)
 SET @errordata = EVENTDATA()
 
 INSERT dbo.DDLAudit
        (LoginName,
         UserName,
         PostDateTime,
         EventType,
         DDLOp)
 VALUES   (SYSTEM_USER, ORIGINAL_LOGIN(), GETDATE(),
    @errordata.value(‘(/EVENT_INSTANCE/EventType)[1]’, ‘varchar(100)’),
    @errordata.value(‘(/EVENT_INSTANCE/TSQLCommand)[1]’, ‘varchar(2500)’) )
 
 IF DATEPART(hh,GETDATE()) > 7 AND DATEPART(hh,GETDATE()) < 20
 BEGIN
 
   RAISERROR (‘You can only perform this change between 8PM and 7AM. 
Please try this
 change again or contact Production support for an override.’, 16, -1)
   ROLLBACK
 END
In this code, you trap the login used and the original login to check for context switching 
(EXECUTE AS). With the trigger now created, test it by running a simple DDL command such 
as the following: 
 CREATE table TriggerTest
 (Column1 int)
If you executed this command after 7:00 A.M. and before 8:00 P.M., you would receive the 
following error, and the CREATE statement would roll back. If you looked at the tables in 
Management Studio, you should not see the TriggerTest table. 
  (1 row(s) affected)
 Msg 50000, Level 16, State 1, Procedure ChangeWindow, Line 22
 You can not perform this action on a production database. Please contact the
 production DBA department for change procedures.
 Msg 3609, Level 16, State 2, Line 2
 The transaction ended in the trigger. The batch has been aborted.
If you were to run the statement before 7:00 A.M. or after 8:00 P.M., you see only (1 row(s) 
affected), meaning that the change was logged, but you successfully performed the action. You 
can test this by changing the server’s time in Windows. After you try to create a few tables or make 
changes, select from the DDLAudit table to see the audited records.
There is little performance effect to this type of trigger because usually DDL events rarely happen. 
You can ﬁ nd the trigger in Management Studio by selecting the individual database, and then 
selecting Programmability Á Database Triggers. You can create a script of the trigger to modify an 
existing trigger, or you can delete the trigger by using Management Studio.


218  ❘  CHAPTER 9  CHANGE MANAGEMENT
LISTING 9-7 (continued)
 BEGIN
   RAISERROR (‘This change can only be performed 
by the server owner, Jorge Segarra.
 Please contact him at extension x4444 to follow the procedure.’, 16, -1)
 ROLLBACK
 
 END
If users other than the TARDIS\JSEGARRA login attempted a login change, they would receive the 
following error. You can issue a permission context switch (EXECUTE AS LOGIN) to test out the 
trigger in your development environment. 
Msg 50000, Level 16, State 1, Procedure PreventChangeTrigger, Line 9
This change can only be performed by the server owner, Jorge Segarra. 
Please contact
him at extension x1701 to follow the procedure.
Msg 3609, Level 16, State 2, Line 1
The transaction ended in the trigger. The batch was aborted.
DDL server triggers can be found in Management Studio under Server Objects Á Triggers. Like 
the database triggers, you can script only the trigger for modiﬁ cations, and delete the trigger, from 
Management Studio.
TRIGGER VIEWS
The Management Studio interface is still slightly lacking in what you can accomplish with DDL 
triggers, so a DBA must often use T-SQL as a management interface. One of the nice views available 
to show you all the database-level DDL triggers is sys.triggers; for server-level triggers, you can 
use sys.server_triggers. Between these two views, you can quickly see what triggers your server 
has installed on it with a query like this one: 
 SELECT type, name, parent_class_desc FROM sys.triggers
 WHERE parent_class_desc = ‘DATABASE’
 UNION
 SELECT type, name, parent_class_desc FROM sys.server_triggers
 WHERE parent_class_desc = ‘SERVER’
SCRIPTING OVERVIEW
Change management is all about creating a reproducible and auditable way to deploy and manage 
your changes. This is impossible to do properly when a DBA executes T-SQL scripts through a 
Management Studio environment. Most non-Windows system administrators use shell scripts to 
create a repeatable change management process for their OS and database server environments. This 
administrative practice has the broadest acceptance and the lowest risk. Scripting leverages both 

Scripting Overview ❘ 219
native OS and application capabilities, plus it works consistently regardless of the target system, at 
least outside the Windows environment. Just as experienced SQL DBAs have T-SQL script libraries, 
DBAs working with open-source and UNIX-variant systems have shell script libraries that just keep 
getting larger as they collect more and more useful scriptlets. Thanks to sqlcmd and PowerShell, you 
have the same scripting capabilities on the Windows platform, only better.
sqlcmd
The main way to create a repeatable change management system is by using sqlcmd ﬁ les that 
encapsulate that T-SQL logic into a set of output logs. These logs provide an audit trail for your 
deployment activities. This way, you know that the change will deploy to each of your environments 
(test, QA, Production, etc.) and provide predictable results.
sqlcmd is a replacement for isql and osql. SQL 2005 launched sqlcmd as the ﬁ rst major attempt 
to create a scripting environment that could cross the boundary between T-SQL and the outside 
world, and it is still a great mechanism for mixing SQL and operating system scripts for database 
schema deployments. (PowerShell, covered next, takes this a step further, enabling you to script 
object behavior for scalable server conﬁ guration deployments.)
You can use either of two modes to execute a sqlcmd command: at a command line or in 
Management Studio. If you are a SQL Server 2000 DBA, the transition to sqlcmd will be an easy one; 
sqlcmd is similar to osql, with some additional switches to simplify your daily job. You probably 
won’t be familiar with executing sqlcmd commands from Management Studio, though. Using 
Management Studio to execute these types of commands gives you a great deal of control and replaces 
many of the old extended stored procedures such as xp_cmdshell. This section covers both solutions.
Executing sqlcmd from the Command Prompt
Executing sqlcmd from the command prompt enables you to run any query from a command 
prompt. More important, it enables you to wrap these queries into a packaged install batch ﬁ le 
for deployments, making it easy for anyone to install the database. You can use many switches 
in sqlcmd, most of which have only a specialized use. The following are some of the important 
switches (they are all case sensitive): 
-U: Username.
-P: Password.
-E: Use Windows authentication. If you use this switch, you do not need to pass in the -U 
and -P switches.
-S: Instance name to connect to.
-d: Database name to start in.
-i: Input ﬁ le that contains the query to run.
-o: Output ﬁ le to which you want to log the output of the query.
-Q: Pass in the query with the -Q switch instead of an input ﬁ le. 
➤
➤
➤
➤
➤
➤
➤
➤


Scripting Overview ❘ 221
To create an initialization ﬁ le, follow these steps: 
 1. 
Create a T-SQL ﬁ le called C:\users\demo\documents\initexample.sql with Notepad or 
another text editor. 
This ﬁ le is going to run a series of commands after you ﬁ rst run sqlcmd. The :setvar state-
ment can be used to set user variables that can be employed later in the script with the 
$ sign. This example initialization script creates three variables. One holds the database 
name, one holds the 60-second timeout, and the last one holds the server name. You use the 
variables later in the script by using $(variablename). 
 :setvar  DBNAME AdventureWorks
 :setvar sqlcmdlogintimeout 60
 :setvar server “localhost”
 :connect $(server) -l $(sqlcmdlogintimeout)
 SELECT VersionofSQL = @@VERSION;
 SELECT ServerName = @@SERVERNAME ;
 2. 
At a command prompt, type the following command to set the sqlcmdini environment 
variable: 
 SET sqlcmdini=c:\users\demo\documents\initexample.sql
This sets the environment variable for the user’s proﬁ le. After you execute this, each time 
the sqlcmd program is executed, the initexample.sql script executes before handing con-
trol to the user.
With the environment variable now set, just run sqlcmd from a command prompt (see 
Figure 9-10). After you run this, you see the version of SQL Server and the server name 
before you’re given control to run any query against the database.
FIGURE 9-10



224  ❘  CHAPTER 9  CHANGE MANAGEMENT
PowerShell relies to a considerable degree on elements called cmdlets to do the heavy lifting. 
Cmdlets take an object model and present it like a disk drive. Natively, PowerShell can traverse and 
explore event logs, registry hives, ﬁ le systems, and pretty much anything else built into the Windows 
operating system. For SQL Server, the SQL Development team wrote a custom cmdlet that presents 
the SQL-SMO object model as a drive. The cmdlet also has some extra smarts regarding how it 
presents data to the console, just to make your job a bit easier.
One neat trick of PowerShell is that it has a built-in set of aliases that map its native verb-noun syntax 
into legacy commands. dir works just as well as get-childitem; and for the folks who believe 
that ls is the proper command for enumerating a folder, that works, too. The arguments for all 
commands must still be in PowerShell format. Trying to match that level of compatibility would have 
been impossibly complex. Figure 9-12 provides a visual of the server object and a list of its members. 
FIGURE 9-12
FIGURE 9-13
Because Server is an object, the cmdlet lists its properties and methods. If your current directory 
is a collection (folder), PowerShell instead lists all the members of the collection. You can cd to the 
database folder (collection) and list all the items in it (see Figure 9-13).


226  ❘  CHAPTER 9  CHANGE MANAGEMENT
The Data-tier Application feature enables developers to build their database projects for tier 2 and 
tier 3 databases and quickly and easily encapsulate objects and changes into a single DAC ﬁ le. You 
then pass this DAC ﬁ le along to your administrators to easily deploy the database application. This 
process not only makes deployments easy but it also simpliﬁ es database upgrades because you can 
use DACs to alter and upgrade existing registered Data-tier Applications.
Extracting a Data-Tier Application (DAC)
To begin extracting a DAC, the ﬁ rst thing you need to do is extract your database deﬁ nitions (that 
is, objects and users) and save them into a Data-tier Application DAC ﬁ le. 
 1. 
In Object Explorer, right-click the database you want to use for your database application. 
From the context menu, go to Tasks; then select the Extract Data-Tier Application option. 
The ﬁ rst screen that displays is an introduction screen; click Next to proceed.
 2. 
At the next screen, set the properties for the DAC. Properties you can set include 
Application Name, Version Level, Application Description and Location of the resulting 
DAC package ﬁ le (also referred to as a dacpac). Keep the default values and click Next. The 
wizard validates your selections and checks the databases to ensure the database contains 
all the supported elements for a DAC. 
 3. 
After the validation check is complete, a Validation Summary screen displays. Here 
you review any objects that may not be supported. You can also choose to save the 
results of the validation check by clicking the Save Report button. Click Next to 
initiate the extraction process and build the DAC package. When the package is built, 
click Finish.
Deploying a Data-tier Application
Now that you’ve built your DAC, you need to deploy it! 
 1. 
First, connect to a new instance in Management Studio. Right-click the instance, and from 
the context menu select Deploy Data-tier Application. Click Next to skip the Introduction 
screen.
 2. 
At the Select Package screen, click the Browse button. Navigate to the area on the ﬁ le system 
where you saved your DAC package, select your package, and click the Open button. After 
you select your DAC package, you notice that the metadata you provided in the Properties 
of the extraction process is loaded under the DAC details section. Click Next to proceed 
with the deployment.
 3. 
The next screen is the Update Conﬁ guration screen. Here you can make modiﬁ cations 
such as the database name and the location of the data and log ﬁ les. The data and log ﬁ le 
location defaults are the same conﬁ gured ones set on the SQL Server instance. Click Next to 
proceed to the Summary screen.
 4. 
At the Summary screen, review your selections and click Next to begin the deployment 
process. As the deployment occurs, you see the various action steps and their result. When 

Data-Tier Applications ❘ 227
the deployment is complete, you can review the various results by clicking their link. You 
can choose to save the results by clicking the Save Report button. 
 5. 
After you review the results, click Finish. Now when you look at your instance in Object 
Explorer, you can notice your new database has been created in the Databases node and 
your new DAC has been added under Data-tier Applications under the Management node 
(see Figure 9-14).
FIGURE 9-14
Upgrading a Data-tier Application
After your database application has been deployed, you can assume you’ve now gone through a new 
development life cycle and it is time to deploy new changes made to the database.
 1. 
First, on the development database extract your DAC again, repeating the steps outlined in 
the previous section. However, this time make a few changes at the Set Properties screen. 
Leave the Application Name the same but now change the version level to 2.0.0.0. 
 2. 
In the Save to DAC package ﬁ le section, a red exclamation point appears to the right of the 
Browse button. This is to let you know that there is already an existing DAC ﬁ le with that 

228  ❘  CHAPTER 9  CHANGE MANAGEMENT
name. You can choose to create a new DAC with a new name or simply check the box for 
Overwrite Existing File. For this example, check the Overwrite Existing File box and then 
click Next.
 3. 
Like before, it validates the database objects for the DAC, and after the validation checks 
are done, you are taken to the Validation and Summary screen. Review your validations, 
and click Next to build your DAC. After the DAC is built, click Finish.
 4. 
Now in SSMS, connect to your second instance. Expand the management node, the Data-
tier Application node, and right-click the DAC application you deployed previously. From 
the context menu, select Upgrade Data-tier Application, as shown in Figure 9-15.
FIGURE 9-15
 5. 
The ﬁ rst screen that displays is an Introduction screen; click Next to proceed. 
 6. 
At the Select Package screen, click the Browse button. Navigate to the location on the 
ﬁ le system where you saved the DAC package; select the package, and click Open. After 
you select the DAC package, the metadata you supplied earlier is populated in the DAC 
details section. Click Next to proceed to the validation step.
 7. 
In addition to the DAC validations, this step also performs a comparison of the existing DAC 
application to the new one. When the checks are done, the Detect Change screen displays 
what changes were detected between DAC versions. Click Next to proceed.

Data-Tier Applications ❘ 229
 8. 
The Options screen gives you the option to gracefully rollback any changes caused by the 
DAC upgrade if something were to go wrong. To do so, check the box for Rollback on 
Failure, and click Next.
 9. 
At the Review Upgrade Plan screen, a list displays with all the actions the DAC upgrade 
is about to perform. From here you can also save your change script. To save your change 
script, click the Save Script button. When you are ready to deploy the changes, click Next 
to proceed to the Summary screen. At the Summary screen, review your changes, and click 
Next to begin the upgrade process.
 10. 
After the changes are applied, you see the results of all the individual upgrade steps. Click 
Finish to complete the process. 
SQL Server Data Tools
Visual Studio Team System for Database Professionals, formerly known as “Data Dude,” has 
now been replaced in SQL Server 2012 with SQL Server Data Tools (SSDT). One of the features 
of the tool is that it enables change detection and simple deployment to multiple environments. 
This is especially important in database deployment situations because developers rarely write 
comprehensive change scripts. Developers often “hand-tweak” a data type or add a column that 
never makes it into the schema creation script. SQL Server Data Tools compares the actual state of 
two databases and writes scripts to reconcile the two.
Think of your production system as version 1.0, and development as version 1.1. When you script 
the production schema (or back it up as a baseline) you can use it as the starting point. The scripts 
can then repeatedly and reliably take the database to version 1.1. You can use the scripts to deploy 
to your quality testing environment and then use the exact same scripts to deploy to production. 
Because the output is T-SQL scripts, you can use Visual Source Safe (or the version management 
software of your choice) to have controllable versions of your database. As a bonus, SQL Server 
Data Tools creates the reverse scripts that take a version 1.1 database back down to version 1.0. 
Rollback has typically been an “Oops, let’s restore” operation at best. Now it can be a predictable 
part of your deployment repertoire.
Another useful feature of SQL Server Data Tools is its capability to refactor a database. 
Refactoring can be as simple as changing a column name or as complex as breaking off a 
column into its own table. Many applications start with a ﬁ eld for Address and later needing 
Billing Address, Shipping Address, and Mailing Address ﬁ elds. SQL Server Data Tools not only 
enables such changes, but also updates every stored procedure and view that referenced the 
original column. You may have to make logic changes, but SQL Server Data Tools does the heavy 
lifting to ﬁ nd all the column references and updates where it can. Of course, that assumes all your 
data access code is in stored procedures and you don’t do ad hoc SQL directly to the database 
tables. Following best design practices makes things easier in the long run in ways you cannot 
always predict.
Version Tables
An important practice when deploying changes is adding a version number to your changes, much 
as application developers do in their own version controlling. A table that the authors used for years 


Summary ❘ 231
OneOff: In some cases, customization code may be required. For example, suppose 
Application A has a number of customized versions of Build 2.1.1. In those cases, you could 
have 2.1.1.0–1 to indicate a customization (1 being for Client B, for example). This ﬁ eld is 
used only in specialized situations.
DateInstalled: The date this application was installed in the environment. This is set to 
getdate()by default, which set it to the current date and time.
InstalledBy: The name of the installer or creator of the service pack.
Description: Description of the service pack. This can be used in an environment in which 
multiple clients share one database — for example, “Upgrade Application A 2.5.1.2 for 
Client B.”
To insert a new change record into the table, use the following syntax: 
 INSERT INTO DB_VERSION  SELECT 1, 5, 0, 2, NULL, getdate(),‘Jorge Segarra’,’Script
 to promote zip code changes’
Ensure that the deploying DBA knows she must place the lines of code to create the table and insert 
the version information into it at the top of each command ﬁ le you create. A standard template 
script ﬁ le with ﬁ ll-in-the-blank ﬁ elds makes this task easier and more consistent. A last use for the 
table is to run a check before you perform a database upgrade. Before applying the database install 
for version 1.1, you can run an IF statement against the table to ensure that version 1.0 is installed. 
If it isn’t installed, you can throw an error and stop the script from executing.
SUMMARY
SQL Server 2012 has many ways to help administrators deal with changes and management. One 
important feature, Policy-Based Management, helps bring your environment into compliance and 
enforce standardization. Using tools like Visual Studio to create database projects that integrate 
into Source Safe, and using the Data-tier Application feature to quickly and easily deploy database 
applications, enables you to manage changes and standardize deployment practices. The last key for 
creating a change management process is using sqlcmd and PowerShell for a smooth deployment 
by using scripts. In the next chapter, you learn how to properly conﬁ gure a server for optimum 
performance before installing SQL Server 2012. 
➤
➤
➤
➤


234  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
that enables the application to continue to perform. As the system grows in terms of data, users, and 
functionality, it needs to grow in ways that keep the system operating optimally.
Similarly, Production DBAs need to understand performance so that the system they maintain starts 
out performing well and then continues to do so throughout the system’s life cycle. Several different 
elements factor into this, from getting the server set up correctly, to monitoring the system as it 
starts working, to implementing a full monitoring strategy to keep the system operating optimally.
The three most important pillars to deliver high performance around scalability, response time, 
reliability, and usability are as follows: 
Knowing what your system can deliver in terms of CPU, memory, input/output (I/O)
Finding the bottlenecks
Knowing your target (how many, how fast)
This chapter discusses all these issues and addresses the most pressing questions about performance 
and conﬁ guring a server.
WHAT EVERY DBA NEEDS TO KNOW ABOUT PERFORMANCE
This chapter lays out a lot of speciﬁ c hardware recommendations that can enable you to improve 
the performance of your system. However, shaving milliseconds off a transaction time isn’t always 
worth the amount of time and hardware budget you spend to accomplish that goal. Frequently, good 
planning up front is worth more than clever optimization later. Always keep the following three 
things in mind regarding performance:
The Performance Tuning Cycle
Deﬁ ning good performance
Focusing on what is most important
The Performance Tuning Cycle
Too often performance and optimization are 
tacked on at the end. Performance tuning is 
an iterative process and ideally starts at the 
beginning of the design process. Obtaining 
favorable performance starts with conﬁ guring 
the server, and continues with designing an 
efﬁ cient schema and specifying tuned SQL 
statements, which leads to ideal index selection. 
The monitoring and analysis of performance 
can then feed back to changes to the server 
conﬁ guration, schema design, or any other 
point in this process. Figure 10-1 illustrates 
this design system.
➤
➤
➤
➤
➤
➤
Determine
Requirements
Design
Schema
Deﬁne SQL
Statements
Understand Data
 Distirbution
Deﬁne
Indexes
Tuning
Specify
Hardware
Set Up Server
Conﬁgure
Server
FIGURE 10-1

You start with your ﬁ rst best guess about how you think your application is going to work and what 
resources you think it’s going to use and plan accordingly. Most often, it’s only a best guess because 
you don’t know yet exactly how the application is going to work.
In the case of a new application, you don’t have an existing system to measure. In the best case, you 
have some metrics from either an existing user base or management predictions about who the users 
will be, what they do on a daily basis, and how that would impact the new application.
In the case of an existing system that you are either moving to a new server or to which you are 
adding functionality, you can measure speciﬁ c metrics on system resource usage and use those as a 
starting point. Then you can add information about any new functionality including the answers to 
questions such as the following: 
Will this increase the user base? 
Will it increase the processing load on the server? 
Will it change the data volume?
All this information enables you to make a good estimate of the new system’s impact on resources. 
Even before you implement the new system, while testing is taking place you have a great 
opportunity to start evaluating your estimates against the actual resource requirements and the 
performance you get from your test servers.
Deﬁ ning Good Performance
The fundamental question that every DBA has to answer before reﬁ ning a system is simple: Does the 
system in question have good performance now? Without either a speciﬁ c target or some baseline 
to compare against, you will never know. Planning, sizing, testing, and monitoring can provide you 
with the information you need to start answering this question. You can break this process down 
into three steps: 
 1. 
Start by identifying your critical targets for CPU, memory, and I/O.
 2. 
Then create a baseline.
 3. 
Finally, after deploying, monitor your critical measurements.
For example, consider how performance requirements can vary in the case of an online store. 
Here, response time to users is critical to keep them shopping. On a database such as this, there 
are likely to be clearly deﬁ ned response times for the most important queries, and there may be a 
broad requirement that is deﬁ ned by the management of the business that no query can take longer 
than 2 or 3 seconds. On a different database server that delivers management reports on warehouse 
inventory levels, there may be an expectation that these reporting queries take some time to gather 
the right information, so response times as long as a few minutes may be acceptable. Although, 
there may still be some queries that have much shorter response time requirements. In yet another 
database, the key performance criterion might be the time it takes to back up the database, or the 
time to load or unload data.
After the critical targets have been identiﬁ ed, the current system needs to be measured to create a 
baseline. This subject is extensive but is covered in more detail in the excellent Wrox publication 
➤
➤
➤
What Every DBA Needs to Know About Performance  ❘  235


One of the ﬁ rst things you notice in the diagram is the number of elements in the bigger picture. 
When the user calls the help desk and complains of poor performance, ﬁ nding the culprit involves 
a lot of possible candidates, so there may be a lot of time spent identifying which piece of the 
complex system architecture might be guilty. Unfortunately, a large, complex system needs multiple 
support personnel all focusing on their piece of the puzzle. For example, the Firewall and Security 
Server is supported by the network team, the Web Server and Application Server is supported by 
the application team, the Authentication is handled by the Windows team and the SQL Server 
is supported by the Database Administration team. The important thing to take away from this 
exercise is an understanding of both the big picture and how to zero in on what’s important for you 
as a DBA.
WHAT THE DEVELOPER DBA NEEDS TO KNOW ABOUT 
PERFORMANCE
Good performance is built on a solid foundation upon which the rest of your application can 
be implemented; for a SQL database this foundation is a well-designed database schema. The 
performance tuning rules to follow are less simple than traditional concepts such as “normalize to 
the nth form.” Instead, they require that you have a solid understanding of the use of the system, 
including the usage pattern, the SQL statements, and the data. The optimal schema for an online 
transaction processing (OLTP) system may be less preferable for a decision support system (DSS), or 
for a data warehousing (DW) system.
Users
You ﬁ rst need to know who is going to use the system: the number of users and their concurrency, 
peak usage level, and what they are going to do. The users usually fall into different groups based 
on either job function or feature usage. For an e-commerce-based system, for example, the user 
groups might be browsers, purchasers, order trackers, customers needing help, and others. For a 
sales analytics system, the user groups may be primarily analysts reading the data with report tools 
such as PerformancePoint Server, Power View, or Excel, and perhaps running reporting for the sales 
team. The e-commerce-based system example is an OLTP database workload optimized for fewer 
and faster reads, updates, and writes requests, whereas the sales analytics system example is a DSS 
database workload optimized for large queries to generate reports.
SQL Statements
After determining the different groups of users, you need to understand what they do, which SQL 
statements will be run, and how often each of these is run for a user action. In the e-commerce 
example, a browser might arrive at the site, which invokes the home page, for example, requiring 
20 or 30 different stored procedures or SQL statements to be executed. When users click something 
on the home page, each action taken can require another set of stored procedures to be executed to 
return the data for the next page. So far it looks like everything has been read-only, but for 
ASP.NET pages, there may be the issue of session state, which may be kept in a SQL database. If 
that’s the case, you may already have seen a lot of write activity just to get to this stage.
What the Developer DBA Needs to Know About Performance ❘ 237

238  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Data Usage Patterns
The ﬁ nal part of the picture is the data in the database. You need an understanding of the total data 
volume in each table, including how that data gets there and how it changes over time. For the e-
commerce example, the main data elements of the site are the catalog of items available for sale. The 
catalog could come directly from the suppliers’ websites through an Internet portal. After this data 
is initially loaded, it can be refreshed with updates as suppliers change their product line and 
as prices vary. The overall volume of data won’t change much unless you add or remove items 
or suppliers.
What will change, hopefully quickly, is the number of registered users, any click-tracking you do 
based on site personalization, the number of orders placed, the number of line items sold, and the 
number of orders shipped. Of course, you hope that you can sell a lot of items, which results in a lot 
of new data growth every day.
A sound knowledge of the data, its distribution, and how it changes helps you ﬁ nd potential 
hot spots, which could be either frequently retrieved reference data, frequently inserted data, or 
frequently updated data. All these could result in bottlenecks that might limit performance.
Robust Schema
An understanding of all the preceding pieces — users, SQL statements, and data — needs to come 
together to help implement a well-designed and well-performing application. If the foundation stone 
of your database schema is not solid, anything you build on top of that foundation is going to be 
unstable. Although you may achieve something that’s acceptable, you are unlikely to achieve an 
optimal solution.
How does all this information help you tune the server? You need to understand where the hot spots 
are in the data to enable the physical design to be implemented in the most efﬁ cient manner. If you are 
going through the process of designing a logical data model, you shouldn’t care about performance 
issues. Only when you are ready to design the physical model do you take this information into 
account and modify the design to incorporate your knowledge of data access patterns.
WHAT THE PRODUCTION DBA NEEDS TO KNOW ABOUT 
PERFORMANCE
The production DBA’s life is considerably different from that of the developer DBA in that a 
production DBA is dealing with a system that someone else may have designed, built, and handed 
over, either as a new or as an already-running system. The production DBA may also face challenges 
with performance on old systems running legacy applications on outdated hardware. In this case, 
the scenario changes from designing an efﬁ cient system to making the system you have been given 
work as well as possible on that limited hardware.
The starting point for this process must be an understanding of what the hardware can deliver; what 
hardware resources the system needs; and what the expectations of the users are in terms of user 
response time. The key elements to understand the hardware are processor speed, type, and cache 
size. Additionally you need to know how much memory there is and what the bus speed is. Finally 

it is important to determine how many I/O disks there are, how they are conﬁ gured, and how many 
network interface cards (NICs) exist. 
The next step is determining how each component of the system is required to perform. Are there 
any performance-related service-level agreements (SLAs) that have been implemented between the 
business and the DBA team? If any performance guidelines are speciﬁ ed anywhere, are you meeting 
them, exceeding them, or failing them? In all cases, you should also know the trend. Have you 
been maintaining the status quo, getting better, or, as is most often the case, slowly getting worse? 
The production DBA needs to understand all this, and then know how to identify bottlenecks and 
resolve them to get the system performing at the required level again.
The tools that the production DBA uses to perform these tasks may include the following: 
Task Manager: Gives a quick, high-level view of server performance and use of resources.
System Performance Monitor (or Reliability and Performance Monitor in Windows 
2008 and Windows 7) (Perfmon): Provides a more detailed view of Windows server 
performance and per-instance SQL Server speciﬁ c counters.
SQL Server Management Data Warehouse (MDW): The MDW is a relational database that 
collects and stores Perfmon and Data Collector outputs for retrieval when the DBA needs to 
troubleshoot a system issue.
SQL Server Management Studio (SSMS): Enables long-running transactions to be analyzed 
and bottlenecks found and resolved. SSMS enables the DBA to run queries against 
DMV’s and Extended Events to gather this data.
Dynamic Management Views (DMVs): These are system objects that contain server state 
information that can be used to diagnose problems and monitor the health of a SQL Server.
Extended Events: This is a light weight monitoring system that collects data about the 
performance of the SQL Server. This data can be viewed through the Session UI that is 
new for SQL Server 2012.
Chapter 13, “Performance Tuning T-SQL,” covers these tools in more detail.
Optimizing the Server
The rest of this chapter covers optimizing the server. This includes the hardware and operating 
system conﬁ guration to provide SQL Server with the best environment in which to execute. You 
should consider three key resources any time you discuss optimization or performance:
CPU
Memory
I/O
Starting with the CPU, there aren’t a lot of options to play with here other than the number and 
type of processors. This part of the chapter focuses on understanding the different processor 
attributes so that you can make the right purchasing decisions.
➤
➤
➤
➤
➤
➤
➤
➤
➤
What the Production DBA Needs to Know About Performance ❘ 239
 
 
 
 




of 30–100 microseconds (µs) and 50–1024GB respectively. Enterprise speciﬁ cation Hard Disk 
Drives will have latency ﬁ gures in the range of 2–50 milliseconds (ms) and will range in size from 
80–2048GB. The performance of your SQL Server, therefore, is extremely dependent on the size 
of the cache available. Processor manufacturers offer a large variety of models that have a range 
of L2 and L3 sizes. The high performance nature of the cache means that it is very costly. This is 
reﬂ ected in the cost of the processors that contain large amounts of cache. However, as SQL Server 
is commonly used in a mission- and business-critical manner, you should purchase the fastest 
processor with the biggest cache memory for your server. In addition, if a compromise needs to be 
made, it is always easier and cheaper to upgrade RAM than it is to upgrade a CPU.
Introducing cache to store small, frequently used pieces of data during the central processing process 
was a complex resolution to the memory issue. But, it has increased the performance of all systems.
Hyper-threading
Hyper-threading (ofﬁ cially Hyper-Threading Technology) is Intel proprietary technology that works by 
duplicating certain sections of the physical processor core to improve parallelization of computations. 
What this means is that, for each physical core present, two logical cores appear to the operating 
system. Although the system schedules multiple threads to be executed by the processor, the shared 
resources may cause certain threads to wait for other ones to complete prior to execution. 
There is actually only one question about hyper-threading that you need to ask: Should you run 
with hyper-threading enabled or disabled? This is a difﬁ cult question to answer though, and a 
one-size-ﬁ ts-all approach will not work. Any answer must involve customers testing their individual 
system. The items you need to consider to ﬁ nd this answer are outlined in this section.
One of the most important factors when considering hyper-threading is to understand the maximum 
theoretical performance beneﬁ t that you might get from hyper-threading. Intel’s documentation 
on hyper-threading reveals that the maximum theoretical performance gain from hyper-threading 
is 30 percent. Many customers running with hyper-threading enabled for the ﬁ rst time expect to 
see double the performance because they see two processors. You should understand that hyper-
threading can give you only a maximum performance increase of 1.3 times non-hyper-threading 
performance at best, and in practice it may be closer to 1.1 to 1.15 times. This knowledge helps put 
any decision about hyper-threading back into perspective.
In some cases, hyper-threading, at least theoretically, won’t provide any beneﬁ t. For example, in 
any database workload where the code runs a tight loop entirely from cache, hyper-threading won’t 
help because there is only a single execution engine. This scenario could result in degraded performance 
because the operating system tries to schedule activity on a processor that isn’t physically there.
Another scenario in which hyper-threading can directly affect SQL Server performance is when 
a parallel plan might be chosen. One of the things a parallel plan does is split the work across the 
available processors with the assumption that each processor can complete the same amount of 
work in the given time. In a hyper-threading–enabled scenario, any thread that’s not currently 
executing is stalled until the other thread on that processor completes.
No one can yet tell you whether hyper–threading can help or hurt your performance when running 
your workload. Plenty of theories abound about how it might impact different theoretical workloads 
exist, but no one has yet come up with prescriptive guidance that deﬁ nitively says turn it on here 
CPU ❘ 243

244  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
and turn it off over there. Unfortunately, that leaves customers with the burden to ﬁ gure it out for 
themselves. You can consider a few things that can help you make your decision:
Hyper-threading itself has been evolving, and although there isn’t any concrete evidence of 
changes in how it is implemented on different Intel processors, feedback from production 
deployments seems to indicate that it’s getting better with each generation of processors. 
The point here is that hyper-threading on an older server may not perform as it would on a 
server with the latest generation of processors.
The new Microsoft SQL Server 2012 licensing policy, which is done per core, should also 
be taken into consideration. Please see Chapter 1 for a breakdown of the new licensing 
changes. In this context it is important to note that having hyper-threading enabled does 
not change the number of cores that are licensed when SQL Server is installed on a physical 
machine. When a virtualization environment (VMWare or Hyper-V) is used, however, 
hyper-threaded processors could be presented to the guest machine as a full CPU core. In 
this case, those would need to be licensed for SQL Server 2012.
The most important thing to consider about hyper-threading is the following: The 
maximum theoretical performance improvement with hyper-threading is only 30 percent 
compared to non-hyper-threaded systems. In practice, this can actually be a maximum of 
only just 10–15 percent. Moreover, for many customers this may be difﬁ cult to measure 
without performing a benchmark to compare it in their production environment and under 
consistent database workload.
To measure this benchmark, a comparative test needs to be performed that includes the following steps: 
 1. 
First, refer to your speciﬁ c system’s documentation to ﬁ nd out how to turn hyper-threading 
off and do so. 
 2. 
Now, run your benchmark test a number of times and get an average execution time. 
 3. 
Then, turn hyper-threading on again. 
 4. 
Run the benchmark test the same number of times again and compare results. What you 
will be looking for is the average run time of each test to be quicker with hyper-threading 
turned on. You should then calculate how much quicker it is, and use that information to 
make a decision on the value of hyper-threading.
Multicore
One of the biggest challenges facing multiprocessor system designers is how to reduce the latency 
caused by the physical limitations of the speed of light and the distance between processors and 
memory. One solution is to put multiple processors on a single chip. This is what a multicore system 
does, and it provides more potential for better performance than a single-core system because of this 
reduced latency between processors and memory. The big question is do you need it?
The answer to this depends on how many processors you need and how much you are willing to 
pay. If you need a 16-processor system, you can get a faster comparable system for less money by 
purchasing a dual-socket 8-core system, rather than a quad-socket quad-core system.
For the latest information on this issue, check out what the hardware vendors are doing with their 
Transaction Processing Council numbers (see www.tpc.org). The TPC results are a great way to 
➤
➤
➤

compare hardware, although some hardware vendors may not publish results for the systems you 
want to compare.
Another factor to consider is scalability. Rather than purchase a straightforward dual-socket, 
10-core system, you can purchase a dual-socket system with one quad-core processor capable of 
being upgraded to 10-core processors in the future. This way, you can defer the expense of adding 
10-core processors when you need to add more processing power.
Before continuing, it’s worth deﬁ ning some clear terminology here to avoid confusion when 
discussing multicore systems: 
The socket is the physical socket into which you plug the processor. Before multicore systems 
arrived, there used to be a direct one-to-one relationship between sockets and execution units.
A core is equivalent to an execution unit, or what you would previously have considered to 
be a processor. With a multicore processor there will be two or more of these per socket.
A thread in this context is not the same as the thread you might create in your program, or 
the operating system threads; it is relevant only in the context of hyper-threading. A hyper-
threading thread is not a new execution unit but a new pipeline on the front of an existing 
execution unit. Refer to the previous section “Hyper-threading” for more details about how 
this is implemented.
Now look at a speciﬁ c example to see what this actually means. Figure 10-4 shows a single-socket, 
single-core processor with no hyper-threading, which results in one thread.
Figure 10-5 shows a single-socket, multicore processor with hyper-threading, which results in four 
threads This is licensed as 2 cores in the new licensing model (with a minimum of 4 cores per socket).
➤
➤
➤
1 Thread
1 Core
1 Socket
FIGURE 10-4
1st Thread
2nd Thread
1st Core
3rd Thread
4th Thread
2nd Core
1 Socket
FIGURE 10-5
CPU ❘ 245

246  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Figure 10-6 shows a dual-socket, dual-core processor with hyper-threading, which results in 8 
threads, licensed as 4 cores in the new licensing model (with a minimum of 4 cores per socket) for a 
total of 8 core licenses.
1st Thread
2nd Thread
1st Core
3rd Thread
4th Thread
2nd Core
1 Socket
5th Thread
6th Thread
3rd Core
7th Thread
8th Thread
4th Core
1 Socket
FIGURE 10-6
To summarize: A socket is a physical processor, a core is an execution unit within a physical 
processor, and a thread is a pipeline through the core.
System Architecture
Another key purchasing decision must be made about the machine architecture. For most modern 
systems you need to consider the options of Symmetric Multi-Processing (SMP) versus Non-Uniﬁ ed 
Memory Architecture (NUMA) systems. This isn’t something you actually need to conﬁ gure, but 
rather something you should understand as an option when considering the purchase of one type of 
system versus another.
Symmetric Multiprocessing
An SMP system is a system in which all the processors are connected to a common system bus in a 
symmetric manner (see Figure 10-7). For most cases, this architecture works very well for smaller 
systems where the physical distance between resources is short, the number of resources is small, 
and the demand for those resources is low.


248  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
The operating system must also be NUMA-aware to schedule threads and allocate memory local 
to the node, which is more efﬁ cient than allocating nonlocal memory or scheduling a thread to run 
on a different node. Either of these actions incurs an overhead of node memory access, either to 
fetch data or to fetch the thread’s data from the old node and transfer it to the new node. Intel Xeon 
and AMD Opteron use different architecture implementations to access memory. Intel uses a Front 
Side Bus (FSB) whereby the sockets are connected through the bus to the external controller to the 
memory; as a result, all sockets are equal distance to the memory. AMD uses an integrated memory 
controller on each socket to connect to its local memory and the other sockets with their own 
memory by means of the HyperTransport link. This non-uniform memory arrangement is referred 
to as NUMA and the data latency depends on where the data requested by the CPU core is located 
in memory. For example, if the data is on a directly connected memory bank, access is fast; if it is 
on a remote memory bank that is on another socket, it can incur some latency. Although in the case 
of the Intel architecture, the FSB delivers equal-distance memory to each CPU core; the inefﬁ ciency 
with this approach is FSB contention, which Intel reduces by implementing larger caches.
MEMORY
The second hardware subsystem to consider is memory — speciﬁ cally, memory on the server, 
including some of the issues associated with memory, the options you can use, and how they can 
impact the performance of the server. Following is a basic introduction to operating system memory. 
Then, you’ll jump straight into the details of how to conﬁ gure a server for different memory 
conﬁ gurations.
Physical Memory
Physical memory is the RAM you install into the server. You are probably already familiar with 
memory in the form of Dynamic Inline Memory Modules (DIMM’s) that go into desktop PCs and 
servers. This is physical memory, or RAM. This memory is measured in megabytes, gigabytes, 
or, if you are lucky, terabytes, as the latest editions of Windows Server 2008 R2 Datacenter and 
Enterprise Editions can now support systems with 2TB of RAM. Future editions of the operating 
system will increase this number as customers demand increasingly powerful systems to solve 
increasingly complex business problems.
Physical Address Space
The physical address space is the set of addresses that the processor uses to access anything on its 
bus. Much of this space is occupied by memory, but some parts of this address space are reserved for 
things such as mapping hardware buffers, and interface-speciﬁ c memory areas such as video RAM. 
On a 32-bit processor, this was limited to a total of 4GB of memory addresses. On 32-bit Intel 
server processors with PAE, the address bus was 36 bits, which enabled the processor to handle 
64GB of memory addresses. You might assume that on a 64-bit processor the address bus would be 
64 bits, but because there isn’t a need for systems that can address 18 exabytes (EB) of memory yet, 
or the capability to build a system that large, manufacturers have limited the address bus to 48 bits, 
which is enough to address 256TB of memory. The architecture enables an extension of this to 52 
bits in the future, which would enable systems up to 4PB of memory.

Virtual Memory Manager
The Virtual Memory Manager (VMM) is the part of the operating system that manages all the 
physical memory and shares it between all the processes that need memory on the system. Its job 
is to provide each process with memory when it needs it, although the physical memory is actually 
shared between all the processes running on the system at the same time.
The VMM does this by managing the virtual memory for each process, and when necessary it takes 
back the physical memory behind virtual memory, and puts the data that resided in that memory 
into the page ﬁ le so that it is not lost. When the process needs to use that memory again, the VMM 
retrieves the data from the page ﬁ le, ﬁ nds a free page of memory (either from its list of free pages 
or from another process), writes the data from the page ﬁ le into memory, and maps the new 
page back into the processed virtual address space. The resulting delay or interruption is called a 
page fault. To determine whether SQL Server or another process is the cause of excessive paging, 
monitor the Process: Page Faults/sec counter for the SQL Server process instance. Please see 
the “Page Fault” section of this chapter for a more in-depth look at these. 
On a system with enough RAM to give every process all the memory it needs, the VMM doesn’t need to 
do much other than hand out memory and clean up after a process is done with it. On a system without 
enough RAM to go around, the job is a little more involved. The VMM must do some work to provide 
each process with the memory it needs when it needs it. It does this by using the page ﬁ le to store 
data in pages that a process isn’t using or that the VMM determines it can remove from the process.
The Page File
The page ﬁ le is a disk ﬁ le that the computer uses to increase the amount of physical storage for 
virtual memory. In other words, when the memory in use by all of the existing processes exceeds 
the amount of available RAM, the Windows operating system takes pages of one or more virtual 
address spaces and moves them to the page ﬁ le that resides on physical disk. This frees up RAM for 
other uses. These “paged out” pages are stored in one or more page ﬁ les that are located in the root 
of a disk partition. There can be one such page ﬁ le on each partition.
On a server running SQL Server, the objective is to try to keep SQL Server running using just the 
available physical memory. SQL Server itself goes to great lengths to ensure that it doesn’t over-
allocate memory, and tries to remain within the limits of the physical memory available.
Given this basic objective of SQL Server, in most cases there is limited need for a page ﬁ le. However, 
a frequently asked question is “Is there a recommended size for the page ﬁ le?” The answer to this 
question, of course, is “it depends.” It depends on the amount of RAM installed and what virtual 
memory will be required above and beyond SQL Server. A general guideline is to conﬁ gure a page 
ﬁ le total of 1.5 to 2 times the amount of RAM installed in the server.
However, in large systems with a large amount of RAM (more than 128GB), this may not be 
possible due to the lack of system drive space. Some good guidelines to adhere to in these cases are 
outlined in the following: 
Conﬁ gure an 8GB page ﬁ le on the system drive.
Make sure that the Windows operating system startup parameters are conﬁ gured to capture 
a kernel dump in the event of a failure. Please see this article from Microsoft Support on 
how to conﬁ gure this setting: http://support.microsoft.com/kb/307973.
➤
➤
Memory ❘ 249

250  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Optional: Conﬁ gure multiple page ﬁ les (on disk volumes other than the system volume) that 
will be available for the OS to utilize if a larger page ﬁ le is desired. It is recommended to use 
one additional page ﬁ le of 200GB for each 256GB of RAM installed in the server.
In some cases, SQL Server and the OS might not cooperate well on sharing the available memory, 
and you may start to see system warnings about low virtual memory. If this occurs, you will ideally 
add more RAM to the server, reconﬁ gure SQL to use less memory, or increase the size of the page 
ﬁ le. It may be better to reconﬁ gure SQL Server to remain within the available physical memory than 
it is to increase the size of the page ﬁ le. Reducing paging always results in better performance. If 
paging occurs, for best performance, the page ﬁ le should be on fast disks that have minimal disk 
usage activity, and the disks should be periodically defragmented to ensure that the page ﬁ le is 
contiguous on the disks, reducing the disk head movement and increasing performance. The metric 
in the Windows System Monitor to measure page ﬁ le usage is Paging ﬁ le: %Usage, which should be 
less than 70 percent.
Page Faults
Page faults are generally problematic for SQL Server, but not all page faults are the same. Some 
are unavoidable, and some have limited impact on performance, whereas others can cause severe 
performance degradation and are the kind you want to avoid.
SQL Server is designed to work within the available physical memory to avoid the bad kind of page 
faults. Unfortunately, the System Performance Monitor page fault counter doesn’t indicate whether 
you are experiencing the benign or bad kind of page fault, which means it doesn’t tell you 
whether you are experiencing good or bad performance.
Soft Page Faults
The most common kind of page fault you will experience is the soft page fault. These occur when a 
new page of memory is required. Anytime SQL Server wants to use more memory, it asks the VMM 
for another page of memory. The VMM then issues a soft page fault to bring that memory into SQL 
Server’s virtual address space. This actually happens the ﬁ rst time SQL Server tries to use the page 
and not when SQL Server ﬁ rst asks for it. For the programmers among you, this means that SQL 
Server is calling VirtualAlloc to commit a page of memory. The page fault occurs only when 
SQL Server tries to write to the page the ﬁ rst time.
Hard Page Faults
Hard page faults are the ones you want to try to avoid. A hard page fault occurs when SQL Server tries 
to access a page of its memory that has been paged out to the page ﬁ le. When this happens, the VMM 
has to step in and take some action to get the needed page from the page ﬁ le on disk, ﬁ nd an empty 
page of memory, read the page from disk, write it to the new empty page, and then map the new page 
into SQL Server’s address space. All the while, the SQL Server thread has been waiting. Only when the 
VMM has replaced the missing page of memory can SQL Server continue with what it was doing.
Why Page Faults Are Problematic
If you look back to the section “Optimizing the Server,” which discusses the relative difference 
in speed between the CPU, memory, and disks, you can see that disk speeds can be as slow as 
➤

200MB/sec, whereas memory speeds are likely to be approximately 5-12GB/sec. Whenever a hard 
page fault occurs, the thread on which it incurs remains waiting for a relatively long time before 
it can continue. If the thread running your query incurs a series of hard page faults, your query 
appears to run slowly.
The resolution to these hard page faults is to add more memory to the server, reduce the memory 
used by other applications on the server, or tune your SQL Server’s SQL statements to reduce the 
amount of memory they consume.
System performance counters that identify page faults are Pages Input/sec (shows the rate of pages 
read), and Pages Output/sec (shows the rate of pages written). See Chapter 13 for a more detailed 
discussion of the performance counters.
I/O
I/O conﬁ guration is actually too big a subject to cover in one chapter; it requires a book of its own. 
This section introduces you to some of the I/O options available and then walks through several 
scenarios to provide some insight into how to make the right storage conﬁ guration decisions.
I/O encompasses both network I/O and hard disk I/O. In most cases with SQL Server, you are 
primarily concerned with disk I/O because that’s where the data resides. However, you also need to 
understand the effect that poor network I/O can have as a bottleneck to performance.
Conﬁ guring I/O for a server storage system is perhaps the place where you have the most options, 
and it can have the largest impact on the performance of your SQL Server system. When you 
turn off your computer, the only thing that exists is the data stored on your hard drive. When 
you turn the power on, the processor starts running, the OS is loaded, and SQL Server is started; all 
this happens by reading data and code from the disk.
This basic concept is true for everything that happens on a computer. Everything starts its life 
on the disk and has to be read from the disk into memory and from there through the various 
processor caches before it reaches the processor and can be used as either code or data. Any results 
the processor arrives at must be written back to disk to persist any system event, for example, 
shutdown, failure, maintenance, and so on.
SQL Server is sensitive to disk performance, more so than many applications, because of the manner 
in which it manages large amounts of data in user databases. Many applications have the luxury 
of loading all their data from disk into memory and then running for long periods of time without 
having to access the disk again. SQL Server strives for that model because it is by far the fastest way 
to get anything done. Unfortunately, when the requested operation requires more data than can ﬁ t 
into memory, SQL Server must do some shufﬂ ing around to keep going as fast as it can, it starts to 
ﬂ ush the write buffer and it must start writing that data back to disk, so it can use that memory 
to generate some new results.
At some point in the life of SQL Server data, every piece of data that SQL server uses must be read 
from disk and changed data must be written back to disk.
Memory ❘ 251

252  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Network
Referring back to Figure 10-2, you can see that the network is a key component in any SQL Server 
system. The network is the link over which SQL Server receives all its requests to do something, 
and by which it sends all its results back to the client. In most cases, today’s high-speed networks 
provide enough capacity to enable a SQL Server system to use all its other resources (CPU, memory, 
and disk) to their maximum before the network becomes a bottleneck.
In some systems the type of work done on the SQL Server is relatively small compared to the 
number of requests sent to the server, or to the amount of data returned to the client. In either 
of these cases, the network can be a bottleneck. Network bottlenecks can occur anywhere in the 
network. They can be on the NIC of the client, where the client is an application server that’s 
serving the database server with hundreds of thousands of requests per second. Bottlenecks can 
occur on the fabric of the network between the server and the client (application server, web server, 
or the user’s workstation). This network fabric can consist of many pieces of network infrastructure, 
from the simplest system in which two machines connect over a basic local area network (LAN) 
to the most complex network interconnected systems in either the Internet or a global corporate 
wide-area network (WAN). In these larger, more complex interconnected systems, much of the 
network can be beyond your control and may introduce bandwidth or latency issues outside of 
acceptable limits. In these cases you can do little more than investigate, document, and report 
your ﬁ ndings.
The parts of networking that you examine here are those over which you may have direct control, 
and all are on the SQL Server system. You can make the assumption that the remainder of the 
network fabric is up to the job of supporting the number of requests received, and of passing the 
results back to the client in a timely manner.
One thing, in particular, to be aware of is the speed and duplex settings for the network. It is, 
unfortunately, easy to cause a duplex mismatch. The result of which is that the network will work 
much slower than its normal rate. The standard setting is for the network to be set to full duplex. 
This means that communications are transmitted in both directions simultaneously. This requires 
the use of approved cabling. Current networks operate at a rate of 1GB/s but 10GB/s networks are 
becoming more and more prevalent.
Disks
The other area of I/O is disk I/O. With earlier versions of SQL Server, disks were quite simple, 
leaving you with limited options. In most cases you had only a couple of disks to deal with. Now, 
enterprise systems have the option to use Storage Area Network (SAN) or Network Attached 
Storage (NAS) storage, and some may use external disk subsystems using some form of Redundant 
Array of Independent Drives (RAID), and most likely using an SCSI interface that enables you to 
build disk subsystems with hundreds if not thousands of disks.
There are various interfaces that are used in disk storage systems:
Advanced Technology Attachment (ATA): Also known as Integrated Drive Electronics (IDE) 
which refers not only to the connector and interface deﬁ nition but also to the fact that the 
drive controller is integrated into the drive. Parallel ATA (PATA) interfaces allow the data 
➤

to be transferred between the motherboard and the disk using a parallel stream up to a 
current maximum of 133MB/s. Serial ATA has been developed to overcome the architectural 
limitations of the parallel interface and can transmit data at speeds up to 3GB/s.
Small Computer Systems Interface (SCSI): This is a set of standards that was developed to 
connect and transfer data between computers and a myriad of peripheral devices including 
disks. This was also using the parallel stream of data transfer with speeds up to a maximum 
of 640MB/s. Serial Attached SCSI (SAS) has been developed as an evolution of the SCSI 
standard and utilizes a serial data stream for speeds of up to 4800MB/s.
Now consider some of the basic physics involved in disk performance. You need to understand 
the fundamental differences between different types of disks, as they explain differences in 
performance. This in turn helps you make an informed decision about what kind of disks to use. 
Table 10-1 demonstrates example values, under ideal conditions, of the typical, fundamental disk 
latency information. 
➤
Rotational latency is the time it takes the disk to make a half rotation. This ﬁ gure is given for just 
half a rotation rather than a full rotation because on average the disk makes only a half rotation to 
ﬁ nd the sector you want to access. It is calculated quite simply as rotational speed in RPM divided 
by 60 (to give revolutions per second) and then divided by the number of rotations per second.
Track-to-track latency is the time it takes the disk head to move from one track to another. In the 
table, the number for 5,400 and 7,200 RPM disks is considerably slower than for the 10,000 and 
15,000 RPM disks. This indicates that the slower disks are more likely to be ATA disks rather than 
SCSI disks. ATA disks have considerably slower internals than SCSI disks, which accounts for the 
considerable difference in price and performance between the two disk types.
Seek time is the magic number disk manufacturers publish, and it’s their measure of rotational latency 
and track-to-track latency. This is calculated in the preceding table as the sum of rotational 
latency and track-to-track latency.
Data transfer rate is the average rate of throughput. This is usually calculated as the amount of data 
that can be read from a single track in one revolution. So for a modern disk with a data density of 
approximately 1MB/track, and for a 7,200 RPM disk, rotating at 120 revs per second, that equates 
TABLE 10-1: Example Hard Disk Drive Latency
DISK 
ROTATIONAL 
SPEED
ROTATIONAL 
LATENCY
TRACK-
TO-TRACK 
LATENCY
SEEK 
TIME
DATA 
TRANSFER 
RATE
TRANSFER 
TIME FOR 
8KB
TOTAL 
LATENCY
5,400 RPM
5.5 ms
6.5 ms
12 ms
90MB/sec
88 μs
12.1 ms
7,200 RPM
4.1 ms
6.5 ms
10.7 ms
120MB/sec
66 μs
10.8 ms
10,000 RPM
3 ms
1.5 ms
4.5 ms
166MB/sec
48 μs
4.6 ms
15,000 RPM
2 ms
1.5 ms
3.5 ms
250MB/sec
32 μs
3.5 ms
Memory ❘ 253

254  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
to approximately 1MB per track at 120 revolutions per sec=120MB/sec of data that can be read 
from the disk.
Transfer time for 8KB is the time it takes to transfer a given amount of data at that transfer rate. To 
see how long it takes to transfer an 8KB block, you simply divide the amount of data you want to 
move by the transfer rate, so 8KB/120MB=66 micro seconds (or µs).
Total latency for a given amount of data is the sum of rotational latency, track-to-track latency, and 
disk transfer time.
Back in Table 10-1 you can see that for a single 8KB transfer, the largest amount of time is spent 
moving the head to the right position over the disk’s surface. Once you are there, reading the data is 
a tiny percentage of the time.
Latency limits the disk’s capability to service random read requests. Sequential read requests have 
an initial latency and are then limited by the disk’s capability to read the data from the disk surface 
and get each I/O request back through the many layers, before reading the next sector from the 
disk’s surface.
Disk write requests are often buffered to disk cache, which increases the SQL Server write 
performance but should be used only if the storage array implements a battery backup to protect 
that cache in case of a power failure. Otherwise, in a power failure, the database can become 
corrupted. Before using cache, check with your storage array vendor to verify that it has a battery 
backup and can guarantee writing the cache to disk if a power failure occurs. A typical storage 
array enables the cache to be conﬁ gured as a range between read-and-write cache. As the database 
transaction log performs synchronous writes, it would beneﬁ t from having a greater amount of 
write cache assigned in the storage array.
Throughput in MB/sec is a measure of how many bytes the disk can transfer to or from its surface 
in a second. This is usually quoted as a theoretical number based on the disk’s bus.
Throughput in IOs/sec or IOPS is a measure of how many I/Os the disk can service per second. 
This is also typically quoted as a theoretical number based on the disk’s bus.
The database workload determines which will have a greater impact on SQL Server performance, 
MB/sec throughput or IOPS. From a disk I/O point of view, an OLTP database workload that does 
small, random reads and writes will be gated by the IOPS. Such data requests ask for few rows 
per request per user (selects, deletes, inserts, or updates), and the disk’s capability to seek the 
request data in the disk and return it determines the user’s response time. Conversely, an OLAP or 
a reporting database with large sequential reads will be gated by the MB/sec throughput and will 
beneﬁ t most from reducing data fragmentation, making the data physically contiguous on disk. 
These are reads where the disk ﬁ nds the starting disk data request and pulls data sequentially, 
and the disk capability to return the data helps determines the user’s response time. However, in 
an OLAP and reporting database, the database workload not only typically returns more data 
per request than the OLTP, but also the queries may be more complex and have more table joins, 
therefore requiring more system CPU work to deliver the ﬁ nal dataset to the users. As a result, the 
user response time must include that in addition to the MB/sec throughput.

Storage Considerations 
After all that talk about the different pieces of a storage system, it’s time to get serious about 
ﬁ guring out how best to conﬁ gure your storage system. This is a challenging task because there is 
no single simple way to conﬁ gure storage that’s going to suit every purpose. SQL Server systems can 
be required to do dramatically different things, and each implementation could require a radically 
different storage conﬁ guration to suit its peculiar I/O requirements. The following sections offer a 
set of guidelines, and then provide the details showing how you can ﬁ gure out what works best for 
you. These guidelines can also be used during discussions with the teams responsible for the storage.
Use the Vendors’ Expertise
The vendors of each piece of hardware should be the people you turn to for expertise regarding 
how to conﬁ gure their hardware. They may not necessarily know how to conﬁ gure it best for 
SQL Server, however, so this is where you must convey SQL Server’s requirements in a manner 
the hardware vendor can understand. This is best done by quoting speciﬁ c ﬁ gures for reads versus 
writes, sequential versus random I/O, block sizes, I/Os per second, MB/sec for throughput, and 
minimum and maximum latency ﬁ gures. This information can help the vendor provide you with the 
optimal settings for their piece of the hardware, be it disks, an array controller, ﬁ ber, networking, 
or some other piece of the storage stack.
Every System Is Dif erent
Each SQL Server system may have different I/O requirements. Understand this and don’t try to use 
a cookie-cutter approach to I/O conﬁ guration (unless you have already done the work to determine 
that you do have SQL systems with the exact same IO requirements.)
Simple Is Better
It is an age-old engineering concept that simpler solutions are easier to design, easier to build, 
easier to understand, and hence easier to maintain. In most cases, this holds true for I/O design as 
well. The simpler solutions invariably work faster, are more robust and reliable, and require less 
maintenance than more complex designs. Unless you have a compelling, speciﬁ c reason to use a 
complex storage design, keep it simple.
More Disks
More disks are invariably faster than fewer disks. For example, if you have to build a 4TB volume, 
it’s going to deliver higher performance if it’s built from a lot of small disks (ten 400GB disks) versus 
a few larger disks (two 2TB disks). This is true for various reasons. First, smaller disks are usually 
faster than larger disks. Second, you stand a better chance to use more spindles to spread read-and-
write trafﬁ c over multiple disks when you have more disks in the array. This gives you throughput 
that in some cases is the sum of individual disk throughput. For example, if those 400GB disks and 
2TB disks all delivered the same throughput, which was, say 20MB/sec, you would sum that for the 
two 2TB disks to achieve just 40MB/sec. However, with the ten smaller disks, you would sum that 
to arrive at 200MB/sec, or ﬁ ve times more.
Memory ❘ 255

256  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Faster Disks
Not surprisingly, faster disks are better for performance than slower disks. However, this doesn’t 
just mean rotational speed, which is but one factor of overall disk speed. What you are looking for 
is some indicator of the disk’s capability to handle the I/O characteristics of the workload you are 
speciﬁ cally interested in. Unfortunately, disk manufacturers rarely, if ever, provide any information 
other than rotational speed and theoretical disk bus speeds. For example, you often see a 10K or 
15K RPM SCSI disk rated as delivering a maximum throughput of 300MB/sec because it’s on an 
SCSI 320 bus. However, if you hook up that disk and start running some tests using a tool such 
as SQLIO, the chances are good that for small-block-size, non-queued random reads, you will be 
lucky to get much more than 2–4MB/sec from that disk. Even for the fastest I/O types, large-block 
sequential I/O, you will rarely get more than 60–70MB/sec. That’s a long way from 300MB/sec. 
You can download the SQLIO Disk Subsystem Benchmark tool from www.microsoft.com/
download/en/details.aspx?id=20163.
Here is a very good tutorial to follow when starting out with SQLIO: 
http://sqlserverpedia.com/wiki/SAN_Performance_Tuning_with_SQLIO.
Cache – Read and Write
Embedded on the controller board of every hard drive is a set of memory that is called the disk 
buffer or disk cache. This cache acts as a buffer between the disk and the attached system and is 
used to store the data that is being read from or written to the disk. The size of this cache ranges 
from 8 to 64MB. In the same way, disk controllers, whether they are internal to the server or 
external as part of a SAN, also have a cache that is used to store read and written data. The size of 
controller cache can vary from 512MB to 512GB. Following are the many uses for disk cache: 
Read-ahead/read-behind: When a read operation is sent to the disk, the disk may read 
unrequested data it deems SQL Server is going to need at a later date.
Write acceleration: The disk controller may signal to SQL Server that the write operation 
has succeeded immediately after receiving the data, even though the data has not actually 
been written to the disk. The data is then stored in the write cache and written later. This 
does introduce the risk of data loss that could occur if the hard drive loses power before 
the data is physically written. To combat this very risk, most disk array controllers have 
a battery backup to ensure that all outstanding write operations complete even if the 
enclosure loses main power.
Input/Output speed balancing: The rate of read and write operations sent to the disk may 
ﬂ uctuate during the normal course of operation. To ensure that the requests get serviced in 
a timely manner, the cache is used to store data waiting to be transferred in and out.
In most conﬁ gurations, the read and write cache shares the same set of memory and the CPU of the disk 
(or array) controls the nuances of that arrangement. In some cases, certain OLTP database conﬁ gurations 
can have their read cache disabled on the storage to free up more memory for the write cache.
Test
Testing is an absolutely essential part of any conﬁ guration, optimization, or performance tuning 
exercise. Too often you speak with customers who have been convinced that black is white based on 
absolutely nothing more than a gut feeling, or some half-truth overheard in a corridor conversation.
➤
➤
➤

Until you have some test results in your hand, you don’t truly know what the I/O subsystem is 
doing. Forget all that speculation, which is nothing more than poorly informed guesswork, and 
start testing your I/O systems to determine what’s actually going on. IOMeter is a commonly used 
tool for I/O subsystem measurement and characterization to baseline an I/O subsystem. It is both a 
workload generator and a measurement tool that can emulate a disk or network I/O load. It can be 
found at www.iometer.org. Here is a good tutorial to follow when starting out with IOMeter: 
www.techrepublic.com/article/test-storage-system-performance-with-iometer/5735721.
A commonly used disk performance metric is disk latency, which is measured by Windows System 
Performance Monitor using the Avg Sec/Read, Avg Sec/Write, and Avg Sec/Transfer counters. 
Target disk latencies are as follows: 
Database Transaction Log: less than 5ms, ideally 0ms
OLTP Data: less than 10ms
Decision Support Systems (OLAP and Reporting) Data: less than 25ms
After you have set up the system and tested it, you need to keep monitoring it to ensure that you are 
aware of any changes to the workload or I/O subsystem performance as soon as it starts. If you have 
a policy of monitoring, you can also build a history of system performance that can be invaluable 
for future performance investigations. Trying to track down the origins of a slow-moving trend can 
be hard without a solid history of monitoring data. See Chapter 13, “Performance Tuning T-SQL” 
for more speciﬁ cs on what and how to monitor.
Designing a Storage System
There are important steps you need to take when designing a storage system. The following sections 
introduce each of the different parts of a storage system, providing some guidance on key factors, 
and offering recommendations where they are appropriate.
The ﬁ rst questions you must to answer when designing a new storage system are about the disks: 
How many disks do you need?
What size should they be?
How fast should they be?
A Word About Space
The ﬁ rst thing you need to determine is how much space you need. After you have sized the data, 
you need to factor in index space, growth, room for backups, and recovery. All these factors can 
increase the amount of space required. 
How Many Disks
After the total space requirement has been set, you need to decide on the total throughput (what 
level of IOPS performance) that is required. From there, consider the redundancy desired and, 
therefore, what RAID level is needed to satisfy the space and I/O pattern requirement. Use those 
details to determine the disk layout and spindle count. 
➤
➤
➤
➤
➤
➤
Memory ❘ 257

258  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Cost
Much as everyone would like to have an unlimited budget, that’s unlikely to be the case in most 
environments, so the ideal design must be modiﬁ ed to bring it in line with the available budget. This 
often results in less than optimal designs, but it’s a necessary factor when designing any system.
Desired I/O Characteristics
The ﬁ rst thing you need to consider here are the I/O characteristics of the operations you are going 
to perform in SQL Server. This can be a complex issue to resolve, and in many cases the easiest 
way to determine this is through testing and monitoring an existing system so that you can identify 
the exact types and volume of I/O that each major feature or function requires.
If you don’t have an existing system, you might use the information in Table 10-2 as a guideline to 
get you started. 
TABLE 10-2: I/O Characteristics
OPERATION
RANDOM/SEQUENTIAL
READ/WRITE
SIZE RANGE
Create Database
Sequential
Write
512KB (Only the log 
ﬁ le is initialized in SQL 
Server 2012 — see 
featured note.)
Backup Database
Sequential
Read/Write
Multiple of 64KB 
(up to 4MB).
Restore Database
Sequential
Read/Write
Multiple of 64KB 
(up to 4MB).
DBCC – CHECKDB
Sequential
Read
8KB–64KB.
Alter Index - on - 
rebuild (Read Phase)
Sequential
Read
See “Read Ahead” 
in Books Online.
Alter Index - on - 
rebuild (Write Phase)
Random or Sequential
Write
Any multiple of 
8KB up to 128KB.
Sys.dm_db_index_
physical_stats
Sequential
Read
8KB–64KB.
Insert / Update / 
Delete
Random or Sequential
Write
64KB–512KB
SELECT
Random or Sequential
Read
64KB–512KB
TempDB
Random or Sequential
Read/Write
8KB–64KB
Transaction Log
Sequential
Write
60KB


260  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
This increases bandwidth during read operations because multiple sections of the entire chunk of 
data are able to be read in parallel. However, RAID 0 does not implement any error checking and 
there is a higher risk of corruption. This is not recommended for any SQL Server volume.
RAID 1 — Mirroring without Striping or Parity (2 disks)
With RAID 1, one disk is mirrored onto another — meaning a two disks are needed to be 
conﬁ gured in the RAID set. This is fast because reads can (but not always) occur from both disks 
and writes incur minimal performance reduction. It provides redundancy from a disk failure but 
increases storage costs because usage capacity is 50 percent of the available disk drives. For storage 
cost reasons, backups, data loads, and read-only database operations may not require this level of 
protection.
RAID 10 — Striping with Mirroring (minimum 4 disks)
RAID 10 (also known as RAID 1+0) is a mirrored set in a stripped set with a minimum of 
four disks. There will always be an even number of disks in the set. This is normally the fastest 
arrangement available. RAID 5 (discussed next) is faster during read operations when the same 
number of disks is used in the set. Reads can occur from multiple disks, and writes incur minimal 
performance reduction. It also provides redundancy — it can survive more than one disk failure 
provided that the disk failures are not in the same mirrored set — but increases storage costs, as 
usage capacity is 50 percent of the available disk drives. Database systems that require the most I/O 
read/write performance and redundancy should be deployed on RAID 10. For storage cost reasons, 
backups, data loads, and read-only database operations may not require this level of protection. 
RAID 0+1 is an alternative to RAID 1+0 in that it creates a second striped set to mirror the ﬁ rst 
striped set as opposed to RAID 1+0 which creates a striped set from a series of mirrored drives.
RAID 5 — Striping with Parity (minimum 3 disks)
Raid 5 is striping with parity with a minimum of three disks. During writes it must calculate the 
data parity — for example, for each write operation in a three disk array, it writes data across two 
disks and parity across the third disk. The RAID ﬁ rmware distributes the parity blocks across 
all the disks in the RAID set to avoid a write hotspot. There is a performance penalty to calculating 
the parity and therefore RAID 5 is not a good choice for databases that must handle a signiﬁ cant 
amount of writes. Another downside of RAID 5 is that, in the event of a disk failure, performance 
can be seriously degraded while rebuilding the array with the new disk. If running with a failed disk, 
performance can also suffer because parity needs to be calculated per each read to return the data. 
During read operations, RAID 5 may perform faster than some other RAID type as multiple 
disks are able to serve the data in parallel. As a result, RAID 5 is efﬁ cient for predominantly 
read databases such as decision support systems (DSS). In addition, RAID 5 is more cost-effective 
than RAID 1 or RAID 10 because the disk space of one single drive is required for parity storage for 
each RAID 5 set, whereas for RAID 1 or RAID 10 it requires 50 percent of the disk space 
for redundancy.
RAID 6 — Striping with Double Parity (minimum 4 disks)
A RAID 6 set contains a minimum of four disks and distributes two copies of the parity across the 
disks. This provides enhanced fault tolerance as two drive failures could occur without destroying 



The beneﬁ ts of using SAN storage over local storage include the following:
Availability: SAN storage is generally more reliable than local storage and can reduce the 
costs associated with downtime due to hardware failure.
Management: A central SAN can reduce the time spent on managing multiple individual 
servers’ storage. Increasing the storage within a server requires new drives to be installed 
manually (a process which requires downtime) whereas assigning additional space from a 
SAN can be done remotely and may not require a reboot.
Improved disaster recovery: Centralizing data backup can improve recovery time while 
reducing overall costs. Using snapshots and data replication ensures that copies of the data 
are available if disaster strikes.
Space utilization: In general, local storage is underutilized because much more disk is 
purchased than is required. 
Any discussion on SQL Server storage conﬁ guration must include information on the concepts 
involved in conﬁ guring an external storage array.
Disk Virtualization
SAN systems present many challenges when you place large numbers of fast, high-capacity disks 
in a single unit. One of these challenges is how to provide the fastest storage to the largest number 
of people. You know that to increase disk performance, you need to stripe across as many spindles 
as possible, but many users of the SAN may require as little as 500GB or less of storage. If your 
SAN system is ﬁ lled with 300GB disks, you could deliver that 500GB using three or four disks. 
Unfortunately, delivering 500GB from just four disks isn’t going to the performance required. Step 
back and consider that the SAN itself may have thousands of disks. If you could take a small piece of a 
larger number of disks, you could build the same 500GB chunk of storage, but it would be much faster.
Now consider the theoretical situation in which you can take a 5GB chunk from each of 100 disks 
that are 300GB each. When you combine all those 5GB chunks together, you end up with 500GB 
of raw disk space. Sure, you wasted a little bit of space for the overhead of managing 100 disks, 
but now you have a 500GB chunk of storage that has the potential to deliver I/O at the rate of the 
sum of 100 disks; or if each disk were capable of 80MB/sec for sequential 64K reads, you would 
have a combined I/O throughput that runs at approximately 8000MB/sec. In practice, you won’t 
get anywhere near that theoretical I/O rate, but you can see considerably higher I/O rates than you 
would if you just combined three or four disks.
Disk virtualization is the technique that some SAN vendors use to present chunks of storage to a 
system. SAN vendors that offer storage virtualization have their own unique method, but they are 
all based on the same basic concept: Slice each disk up into slices, or chunklets, recombine these 
small slices into larger chunks of storage, and present these virtualized chunks to each application. 
This process can be performed by following these steps: 
 1. 
Start with a single disk and create multiple slices on the disk.
 2. 
Do the same thing across multiple disks.
 3. 
Group a collection of these disk-slices together using some level of RAID and present it to 
the server as a LUN.
➤
➤
➤
➤
Memory ❘ 263
 
 
 
 

264  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
Logical Unit Numbers
A logical unit number (LUN) is, technically, the number used to deﬁ ne a physical device addressed 
by the SCSI or Fiber Channel protocol. The term is also used to refer to the logical disk that is 
created on a SAN.
When considering the LUNs that the storage is going to present to the OS, you have to answer two 
questions: How big should each LUN be, and how many should you have? The starting point for 
answering these questions is determining how much storage you need. If you have differing storage 
requirements, you also need to know how much of each different type you require. Different types 
of storage might range from high speed, high reliability for data and log; low speed, high reliability 
for archive data; high speed, low reliability for backup staging (before writing to tape), and low 
speed, low reliability for “other” storage that doesn’t have tight performance criteria or present 
reliability concerns.
LUN Size
The next factor to be considered is LUN size. For a SAN-based system, or large local storage array, 
this equates to how big you make each chunk of storage to be presented to the operating system. 
These LUN-sized chunks of storage are how the OS sees the storage. By now, you may know from 
virtualization that the OS has no way to know how many disks, or how much from each disk, each 
LUN represents.
A number of factors can inﬂ uence your decision about LUN size. For instance, you need to consider 
how the storage is to be mounted in the OS. If you mount each LUN as its own volume, you can 
make them a little larger. You want to stay well below this limit due to backup, restore, and startup 
times. On startup, the OS runs a basic check of each volume. If an error is found on the volume, 
the OS runs a Check Disk (CHKDSK). If the volume is large, the time taken to run CHKDSK can 
become long — in some cases, extremely long — which can start to have a large impact on system 
startup time. For large enterprise servers with less than one server restart scheduled per year, this 
isn’t a major problem, except when it’s time to set up everything — installing the OS and drivers 
and conﬁ guring the server. Many reboots may be required. If each reboot takes 30–45 minutes, 
restarting the server eight or more times becomes a two day labor, rather than something you do 
while getting another coffee reﬁ ll.
On Windows 2008, the maximum size of Master Boot Record (MBR) volume is 2TB. GBT volumes 
have a maximum size of 9+ zettabytes. One ZB is equivalent to 1 million petabytes. 
Number of LUNs
If there are no clear criteria that set an ideal size for each LUN, there may be factors that dictate a 
speciﬁ c number of LUNs. The simplest way to determine the number of LUNs you need is to decide 
how you want the data laid out on the physical disks in the storage array. As a starting point, you 
need one LUN for tempdb, one LUN for data ﬁ les and one LUN for database log ﬁ les. This is in 
addition to the system volume as no data, log, or tempdb ﬁ les should reside on the system volume.
Server Conﬁ guration
After spending a lot of time conﬁ guring the storage, you still must conﬁ gure the server. The main 
conﬁ guration options on the server are related to the number, placement, and conﬁ guration of the 

host bus adapter (HBA), and then you’re into the details of the operating system — speciﬁ cally, the 
device manager and the ﬁ le system. The HBA is the I/O interface that connects a host system to 
the SCSI bus or SAN. Moreover, HBA cards are available in a variety of different throughputs (such 
as 4, 8 or 16GB/s), and some servers may have a limited number of faster PCI slots among all their 
available PCI slots. You want to put the HBA in a PCI slot that can enable it to run at its maximum 
throughput. Check the server documentation.
Disk Adapters
The disk adapter is the interface card that you plug into the PCI bus inside your server to enable 
the PCI bus to connect to the ATA, SCSI, iSCSI, or ﬁ ber channel cabling required to connect to the 
disks. Several factors should be considered with disk adapters.
Number of Adapters
When determining the number of disk adapters, you ﬁ rst must consider how many disks you need to 
support. On some interfaces, such as ATA and SCSI, there are physical limits to the number of disks 
allowed per bus: two disks per ATA bus (one master, one slave) and either 4, 8, or 16 per SCSI bus 
(depending on which version of SCSI is in use and length of cable). If you need an ATA-based system 
with eight disks, you need to have enough disk adapters to provide four ATA buses. If you need an 
SCSI-based system with 32 disks, and you use a version of SCSI that supports 16 disks per bus, you 
would need two, or possibly, four buses.
One adapter can provide more than one bus. You might ﬁ nd an ATA adapter that implements two 
or maybe four ATA buses, so your eight-disk system might need only a single adapter. In the case of 
SCSI adapters, many are multi-bus, so you should deliver two SCSI buses from a single adapter card.
At a minimum, one dual-port adapter or two single port adapters should be used to connect the 
server to the SAN switch.
Multipath I/O
Multipath I/O (MPIO) solutions use redundant physical path components — adapters, cables, and 
switches — to create logical “paths” between the server and the storage device. If one or more of 
these components fails, causing the path to fail, multipathing logic uses an alternative path for I/O 
so that applications can still access their data.
Keeping a highly available solution requires redundancy in each of the following components: 
Application availability through server clustering such as Microsoft Clustering Service 
provides redundancy among computers so that if a computer has a system failure, the 
running services — for example, SQL Server 2012 — can failover to another computer 
in that failover cluster. Microsoft Clustering Services is included with Windows 2008 
Enterprise and Datacenter Editions. Moreover, multiple NIC card teaming can be 
implemented if a network card failure occurs.
Storage redundancy through RAID enables you to conﬁ gure drives for performance and 
fault-tolerance to provide protection from a disk failure.
Storage availability through multipathing enables multiple interfaces to the same storage 
unit (LUN) to deliver on the storage network components’ redundancy. Multipathing 
➤
➤
➤
Memory ❘ 265

266  ❘  CHAPTER 10  CONFIGURING THE SERVER FOR OPTIMAL PERFORMANCE
manages the redundant I/O connections so that read/write requests are instantaneously 
rerouted in case of an I/O path failure. Microsoft MPIO with the Microsoft Device-Speciﬁ c 
Module (DSM) provides a tightly integrated Windows System multipathing solution that 
can be deployed in failover mode, whereby one I/O path is implemented as a failover 
partner to another, or it can be deployed in a dynamic load balancing mode whereby the 
I/O workload is balanced across all the available I/O paths based on an algorithm — for 
example, round-robin. Within either mode, if one path — for example, the ﬁ ber cable or 
the Host Bus Adapter (HBA) — were to fail, the I/O would continue through the surviving 
I/O paths. Other MPIO workload routing algorithms include: round robin with subset, 
least queue depth, weighted paths, and least blocks. For a detailed description of these 
algorithms, please see the MPIO Policies article that can be found here: http://technet
.microsoft.com/en-us/library/dd851699.aspx.
Moreover, MPIO in dynamic load balancing mode enables you to performance scale the I/O 
across many HBAs, enabling more total throughput I/O to ﬂ ow to the I/O subsystem. This 
is not supported by Microsoft MPIO. An additional, third party driver is required to do this.
Furthermore, if the I/O subsystem is connected through the server by way of a fabric switch, 
redundant fabric switches should be deployed to prevent it from becoming a single point 
of failure. 
Figure 10-9 shows an example HP MPIO implementation. Following is a description of each of the 
common MPIO parameters that can be found in this package:
FIGURE 10-9
LUN Identiﬁ er: A value used by the storage array to identify a particular LUN.
Drive Letter: Assigned by the user.
Disk Number: Assigned by Windows. Disk 0 is normally the C: drive, for example.
Status: Can be good or degraded, failed, and so on. The terminology depends on the vendor.
Number of Active Paths: Number of active paths to the LUN.
Load Balancing Policy: Indicates the load balancing policy for the LUN, which can be 
as follows: 
➤
➤
➤
➤
➤
➤

Round Robin: The I/O requests are distributed across all active paths to the device 
in a round-robin manner.
Shortest Queue Requests: Each I/O request is routed to the active path with the least 
number of outstanding requests.
Shortest Queue Bytes: Each I/O request is routed to the active path with the least 
number of outstanding data bytes.
Shortest Queue Service Time: Each I/O request is routed to the active path where 
the total outstanding time for pending I/O requests is the least.
No Load Balance: All I/O requests are routed through a chosen active path.
ALB: Indicates whether the adaptive load balance is enabled or disabled for the LUN. 
(This is an HP–speciﬁ c feature.)
Placement
In larger servers, the physical placement of the adapter card in the PCI slots can have an impact on 
performance. Placement can affect performance in two ways. On a large system, the physical distance 
between the furthest PCI slot and the CPU can increase latency and reduce overall bandwidth.
On some systems, the PCI slots are not all of the same type. It is increasingly common for systems 
to have a few PCI-X or fast 64-bit PCI slots, with the remainder being slower slots. Placing a high-
speed 64-bit, PCI-X, or PCI-Express disk adapter into a slow-speed PCI slot (some won’t physically 
ﬁ t, but some will) can force the whole bus to run at a considerably slower speed than expected. 
Check the server documentation or ask the vendor to identify the faster PCI slots.
Firmware
All disk adapters have ﬁ rmware that can be upgraded. Even when you ﬁ rst purchase a new disk 
adapter, you should check the vendor’s website to make sure you have the highest ﬁ rmware version 
which is supported on your storage array, which may not be the highest available ﬁ rmware version. 
Vendors change their ﬁ rmware to ﬁ x bugs, improve performance and match speciﬁ c drivers, so you 
must always conﬁ rm that you have the latest ﬁ rmware to ensure optimal performance.
Drivers
Even though the disk adapter probably comes with drivers, there is a good chance that they are 
outdated by the time you purchase the adapter, so even before installing anything you should check 
the vendor’s website for the highest driver version which is supported in your conﬁ guration. This 
may not be the newest driver version.
Conﬁ guration
Many disk adapters have options that can be set to change the way they work in different environments, 
or for different types of disks to which they may be attached. For ATA and SCSI adapters, this 
conﬁ guration is usually minimal, unless the adapter includes RAID. In most cases the conﬁ guration 
involves ﬁ ber channel options on HBA’s. One conﬁ guration that should be considered is the 
HBA queue depth. This setting speciﬁ es the maximum number of I/O operations that can be in 
the HBA queue before it no longer accepts more I/O commands. If this setting is misconﬁ gured, 
➤
➤
➤
➤
➤
➤
Memory ❘ 267




The best way to avoid problems is to follow these steps: 
 1. 
Install the OS.
 2. 
Defragment the disk.
 3. 
Install any applications (SQL Server).
 4. 
Defragment the disk.
 5. 
Create data and log ﬁ les at maximum size.
 6. 
Check for fragmentation and defragment if necessary.
 7. 
Disable autogrow.
 8. 
Routinely defragment the disk to clean up fragmentation caused by other applications. This 
preserves the free space should you ever need to add more SQL data or log ﬁ les.
In most cases, the operating system’s disk defragmenter does a great job and is all you need. In 
some situations, however, you may need to consider purchasing a third-party disk defragmentation 
utility. Some beneﬁ ts of using a third-party tool include increased speed of operation, background 
processing, multiple simultaneous volume defragmentation, and the ability to schedule 
defragmentation operations.
SUMMARY
Performance tuning can be tricky when a system has been operating sub-optimally for an extended 
period of time. The performance issues can be multiplied when the underlying system has not been 
designed in an optimal way from the outset.
Before starting to make decisions about a system’s hardware layout, there are multiple areas of the 
system that need to be discussed: user interaction with the system; data usage patterns; number and 
type of SQL statements that will be run against the system; schema design; and more. 
There are a number of hardware decisions that need to be made when conﬁ guring a server for 
optimal performance. The central processing unit of the server plays a large role in determining if 
the system will perform acceptably under the expected workload. CPU components such as cache, 
hyper-threading, multi-core and system architecture need to be investigated and the available 
options need to be weighed. You should also consider the memory and the various technologies that 
are at work within the memory. These technologies include physical and virtual address spaces, the 
virtual memory manager, and the page ﬁ le.
The slowest part of a system is I/O. Care should be taken to choose between the myriad of options 
when designing network and storage subsystems. Some of the questions that need to be answered 
include: How fast should the network be? How much storage space do I need? How many disks? 
What type of disks? How fast do the disks need to be? SAN or NAS? What RAID should I use? 
Storage adapter cards? Allocation unit size? and so on. 
You should now have many tools to choose from when conﬁ guring a SQL Server for 
optimal performance.
Summary ❘ 271


274  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
Deﬁ ning a Workload
A prerequisite to tuning any database environment is a thorough understanding of basic database 
principles. Two critical principles are the logical and physical structure of the data and the inherent 
differences in the application of the database. For example, different demands are made by an 
online transaction processing (OLTP) environment than are made by a decision support (DSS) 
environment. A DSS environment often needs a heavily optimized I/O subsystem to keep up with 
the massive amounts of data retrieval (or reads) it performs. An OLTP transactional environment 
needs an I/O subsystem optimized for more of a balance between read-and-write operations.
In most cases, SQL Server testing to scale with the actual demands on the application while in 
production is not possible. As a preferred practice, you need to set up a test environment that 
best matches the production system and then use a load generator such as Quest Benchmark 
Factory or Idera SQLscaler to simulate the database workload of the targeted production data-tier 
environment. This technique enables ofﬂ ine measuring and tuning the database system before you 
deploy it into the production environment.
Further, the use of this technique in a test environment enables you to compartmentalize speciﬁ c 
pieces of the overall solution to be tested individually. As an example, using the load-generator 
approach enables you to reduce unknowns or variables from a performance-tuning equation by 
addressing each component on an individual basis (hardware, database, and application).
System Harmony Is the Goal
Scaling an application and its database is dependent on the 
harmony of the memory, disk I/O, network, and processors, as 
shown in Figure 11-1. A well-designed system balances these 
components and should enable the system (the application 
and data tiers) to sustain a run rate of greater than 80 percent 
processor usage. Run rate refers to the optimal CPU utilization 
when the entire system is performing under heavy load. When 
a system is balanced properly, this can be achieved as all sub-
systems are performing efﬁ ciently.
Of these resource components, a processor bottleneck on a 
well-tuned system (application\database) is the least problematic 
to diagnose because it has the simplest resolution: add more 
processors or upgrade the processor speed/technology. There 
are, however, increased licensing costs associated with adding 
more processors and CPU cores.
THE SILENT KILLER: I/O PROBLEMS
Customers often complain about their SQL Server performance and point to the database because 
the processors aren’t that busy. After a discussion and a little elbow grease, frequently the culprit 
is an I/O bottleneck. The confusion comes from the fact that disk I/O is inversely proportional to 
FIGURE 11-1
Component Interactions. . . 
W   O   R   K   L   O   A   D
CPU
Memory
Disk I/O
Network





Table and Index Partitioning ❘ 279
Another great tool is the sys.dm_db_task_space_usage DMV, which provides insight into tempdb’s 
space consumption on a per-task basis. Keep in mind that once the task is complete, the counters reset 
to zero. In addition, you should monitor the per disk Avg. Sec/Read and Avg. Sec/Write as follows: 
Less than 10 milliseconds (ms) = Very good
Between 10–20 ms = Borderline
Between 20–50 ms = Slow, needs attention
Greater than 50 ms = Serious IO bottleneck
If you have large tempdb usage requirements, read the Q917047, “Microsoft SQL Server I/O 
subsystem requirements for tempdb database” at http://support.microsoft.com/kb/917047 or 
look in SQL Server 2012 Books Online (BOL) for “Optimizing tempdb Performance.”
Hopefully this section has impressed upon you that in SQL Server 2012, more capacity planning is 
required to optimize tempdb for performance.
TABLE AND INDEX PARTITIONING
Simply stated, partitioning is the breaking up of a large object, such as a table, into smaller, 
manageable pieces. A row is the unit on which partitioning is based. Unlike DPVs, all partitions 
must reside within a single database.
➤
➤
➤
➤
FIGURE 11-2










Table and Index Partitioning ❘ 289
[CK_SalesOrderHeaderOLD_ORDERDATE] CHECK  ([ORDERDATE]>=(‘2011-01-01
00:00:00’) AND [ORDERDATE]<(‘2012-01-01 00:00:00’));
Before you start to do table partition operations, you should verify that you have data in the partitioned 
table and no data in the nonpartitioned table. The following query returns which partitions have data 
and how many rows: 
SELECT $partition.OrderDateRangePFN(OrderDate) AS ‘Partition Number’, 
min(OrderDate) AS ‘Min Order Date’,
max(OrderDate) AS ‘Max Order Date’,
count(*) AS ‘Rows In Partition’
FROM SalesOrderHeader 
GROUP BY $partition.OrderDateRangePFN(OrderDate);
You should also verify that no data has been inserted into the SalesOrderHeaderOLD 
nonpartitioned table; this query should return no data: 
SELECT * FROM [SalesOrderHeaderOLD]
To simply switch the data from partition 4 into the SalesOrderHeaderOLD table, execute the 
following command: 
ALTER TABLE SalesOrderHeader
SWITCH PARTITION 4 TO SalesOrderHeaderOLD;
To switch the data from SalesOrderHeaderOLD back to partition 4, execute the following command: 
ALTER TABLE SalesOrderHeaderOLD  
SWITCH TO SalesOrderHeader PARTITION 4;
To merge a partition range, execute the following command: 
ALTER PARTITION FUNCTION OrderDateRangePFN()
MERGE RANGE (‘2011-01-01 00:00:00’);
You may want to split a partition range. The split range area should be empty of data because if 
data were available, SQL Server would need to physically (I/O) move the data across the range. For 
a large table, the data movement can take time and resources. Therefore, if the range is not empty, 
the split is not instantaneous. To split a range, you must add a new ﬁ legroup for the range before 
splitting it, like so: 
ALTER PARTITION SCHEME OrderDatePScheme NEXT USED [Primary];
ALTER PARTITION FUNCTION OrderDateRangePFN()
SPLIT RANGE (‘2013-01-01 00:00:00’);
This section discussed what partitioning is and why it should be considered as a performance option 
or as a manageability and scalability feature. You ran through the steps to create the partition 
function and the partition scheme. Remember that ﬁ legroup creation needs to be performed prior 
to these steps. The section ended with an outline of the various options used when creating tables 
and indexes that use the partition scheme. Next you will learn about Data Compression and how it 
pertains to increasing efﬁ ciency in a database.


Data Compression ❘ 291
A 1-byte column with a 1-byte value has no compression, but NULL or 0 values take no bytes. 
Compression does take a few bits of metadata overhead to store per value. For example, an Integer 
data type column storing a 1-byte value can have a 75 percent compression ratio. To create a new 
compressed table with row compression, use the CREATE TABLE command, as follows: 
 USE AdventureWorks
 GO
 CREATE TABLE [Person].[AddressType_Compressed_Row](
  [AddressTypeID] [int] IDENTITY(1,1) NOT NULL,
  [Name] [dbo].[Name] NOT NULL,
  [rowguid] [uniqueidentifier] ROWGUIDCOL  NOT NULL,
  [ModifiedDate] [datetime] NOT NULL,
 )
 WITH (DATA_COMPRESSION=ROW)
 GO
To change the compression setting of a table, use the ALTER TABLE command: 
 USE AdventureWorks
 GO
 ALTER TABLE Person.AddressType_Compressed_Row REBUILD
 WITH (DATA_COMPRESSION=ROW)
 GO
Page Compression
Page compression includes row compression and then implements two other compression operations: 
Preﬁ x compression: For each page and each column, a preﬁ x value is identiﬁ ed that 
can be used to reduce the storage requirements. This value is stored in the Compression 
Information (CI) structure for each page. Then, repeated preﬁ x values are replaced by a 
reference to the preﬁ x stored in the CI.
Dictionary compression: Searches for repeated values anywhere in the page, which are 
replaced by a reference to the CI.
When page compression is enabled for a new table, new inserted rows are row compressed until 
the page is full, and then page compression is applied. If afterward there is space in the page for 
additional new inserted rows, they are inserted and compressed; if not, the new inserted rows 
go onto another page. When a populated table is compressed, the indexes are rebuilt. Using page 
compression, nonleaf pages of the indexes are compressed using row compression. To create a 
new compressed table with page compression, use the following commands: 
 USE AdventureWorks
 GO
 CREATE TABLE [Person].[AddressType_Compressed_Page](
  [AddressTypeID] [int] IDENTITY(1,1) NOT NULL,
  [Name] [dbo].[Name] NOT NULL,
  [rowguid] [uniqueidentifier] ROWGUIDCOL  NOT NULL,
  [ModifiedDate] [datetime] NOT NULL,
 )
 WITH (DATA_COMPRESSION=PAGE)
 GO
➤
➤

292  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
To change the compression setting of a table, use the ALTER TABLE command: 
 USE AdventureWorks
 GO
 ALTER TABLE Person.AddressType_Compressed_Page REBUILD
 WITH (DATA_COMPRESSION=PAGE)
 GO
Moreover, on a partitioned table or index, compression can be applied or altered on individual 
partitions. The following code shows an example of applying compression to a partitioned table and 
index: 
 USE AdventureWorks
 GO
 
 CREATE PARTITION FUNCTION [TableCompression](Int)
 AS RANGE RIGHT
 FOR VALUES (1, 10001, 12001, 16001);
 GO
 
 CREATE PARTITION SCHEME KeyRangePS
 AS
 PARTITION [TableCompression]
 TO ([Default], [Default], [Default], [Default], [Default])
 GO
 
 CREATE TABLE PartitionTable
 (KeyID int,
 Description varchar(30))
 ON KeyRangePS (KeyID)
 WITH
 (
 DATA_COMPRESSION = ROW ON PARTITIONS (1),
 DATA_COMPRESSION = PAGE ON PARTITIONS (2 TO 4)
 )
 GO
 
 CREATE INDEX IX_PartTabKeyID
  ON PartitionTable (KeyID)
 WITH (DATA_COMPRESSION = ROW ON PARTITIONS(1),
 DATA_COMPRESSION = PAGE ON PARTITIONS (2 TO 4 ) )
 GO
Table partition operations on a compression partition table have the following behaviors: 
Splitting a partition: Both partitions inherit the original partition setting.
Merging partitions: The resultant partition inherits the compression setting of the 
destination partition.
Switching partitions: The compression setting of the partition and the table to switch must 
match.
Dropping a partitioned clustered index: The table retains the compression setting.
➤
➤
➤
➤

Data Compression ❘ 293
In addition, data compression can be managed from SQL Server 2012 Management Studio in 
Object Explorer by choosing the table or index to data compress. For example, to compress a table, 
follow these steps: 
 1. 
Choose the table and then right-click. From the pop-up menu, choose Storage Á Manage 
Compression.
 2. 
On the Data Compression Wizard that opens, click Next, and in the Select Compression 
Type dialog of the Data Compression Wizard, select the Compression Type drop-down to 
change the compression (None, Row, or Page). Figure 11-7 shows the Select Compression 
Type dialog.
 3. 
After making a Compression Type change, to see the estimated space savings, click the 
Calculate button. Then, click Next to complete the wizard.
Estimating Space Savings
Prior to enabling data compression, you can evaluate the estimated compression cost-savings. 
For example, if a row is more than 4KB and the whole value precision is always used for the 
data type, there may not be much compression savings. The sp_estimate_data_compression_
savings stored procedure creates a sample data set in tempdb and evaluates the compression 
space savings, returning the estimated table and sample savings. It can evaluate tables, clustered 
indexes, nonclustered indexes, index views, and table and index partitions for either page or row 
compression. Moreover, this stored procedure can estimate the size of a compressed table, index, 
or partition in the uncompressed state. This stored procedure performs the same cost-saving 
calculation that was performed by the Data Compression Wizard shown in Figure 11-7 when 
clicking the Calculate button.
FIGURE 11-7

294  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
The syntax of the sp_estimate_data_compression_savings stored procedure follows: 
 sp_estimate_data_compression_savings
 [ @schema_name = ] ‘schema_name’ 
 , [ @object_name = ] ‘object_name’
 , [@index_id = ] index_id
 , [@partition_number = ] partition_number  
 , [@data_compression = ] ‘data_compression’
 [;]
In this code: 
@schema_name is the name of the schema that contains the object.
@object_name is the name of the table or index view that the index is on.
@index_id is the ID number of the index. Specify NULL for all indexes in a table or view.
@partition_number is the partition number of the object; it can be NULL or 1 for 
nonpartitioned objects.
@data_compression is the type of compression to evaluate; it can be NONE, ROW, or PAGE.
Figure 11-8 shows an example of estimating the space savings for the SalesOrderDetail table in 
the AdventureWorks database using page compression.
➤
➤
➤
➤
➤
FIGURE 11-8

Data Compression ❘ 295
As shown in the result information in Figure 11-8: 
index_id identiﬁ es the object. In this case, 1 = clustered index (includes table); 2 and 3 are 
nonclustered indexes.
Size_with_current_compression_setting is the current size of the object.
Size_with_requested_compression_setting is the compressed size of the object
In this example, the clustered index, including the table, is 9952KB but when page-compressed, it is 
4784KB, saving a space of 5168KB.
Monitoring Data Compression
For monitoring data compression at the SQL Server 2012 instance level, two counters are available 
in the SQL Server:Access Method object that is found in Windows Performance Monitor: 
Page compression attempts/sec counts the number of page compression attempts per second.
Pages compressed/sec counts the number of pages compressed per second.
The sys.dm_db_index_operational_stats dynamic management function includes the page_
compression_attempt_count and page_compression_success_count columns which are used 
to obtain page compression statistics for individual partitions. It is important to take note of these 
metrics because failures of attempted compression operations waste system resources. If the ratio of 
attempts to successes gets too high then there may be performance impacts that could be avoided by 
removing compression. In addition, the sys.dm_db_index_physical_stats dynamic management 
function includes the compressed_page_count column, which displays the number of pages 
compressed per object and per partition.
To identify compressed objects in the database, you can view the data_compression column 
(0=None, 1=Row, 2=Page) of the sys.partitions catalog view. From SQL Server 2012 
Management Studio in Object Explorer, choose the table and right-click; then, from the pop-up menu 
choose Storage Á Manage Compression for the Data Compression Wizard. For detailed information 
on data compression, refer to “Data Compression” in SQL Server 2012 Books Online, which can be 
found here: http://msdn.microsoft.com/en-us/library/cc280449(v=SQL.110).aspx.
Data Compression Considerations
When deciding whether to use data compression, keep the following items in mind: 
Data compression is available with SQL Server 2008 Enterprise and Developer Editions 
only.
Enabling and disabling table or clustered index compression can rebuild all non-clustered 
indexes.
Data compression cannot be used with sparse columns.
Large objects (LOB) that are out-of-row are not compressed.
➤
➤
➤
➤
➤
➤
➤
➤
➤


CPU Considerations ❘ 297
Hardware support for hot-add CPU.
Supported by the 64-bit edition of Windows Server 2008 Datacenter or the Windows Server 
2008 Enterprise Edition for Itanium-Based Systems operating system.
Supported by SQL Server 2012 Enterprise Edition.
SQL Server 2012 must not be conﬁ gured for soft NUMA. For more information about soft 
NUMA, search for the following topics online: “Understanding Non-Uniform Memory 
Access” and “How to Conﬁ gure SQL Server to Use Soft-NUMA.”
Once you have met these requirements, execute the RECONFIGURE command to have SQL Server 
2012 recognize the new dynamically added CPU.
Cache Coherency
For reasons of data integrity, only one processor can update any piece of data at a time; other 
processors that have copies in their caches can have their local copy “invalidated” and thus 
must be reloaded. This mechanism is referred to as cache coherency, which requires that all the 
caches are in agreement regarding the location of all copies of the data and which processor 
currently has permission to perform the update. Supporting coherency protocols is one of the 
major scaling problems in designing big SMPs, particularly if there is a lot of update trafﬁ c. 
Cache coherency protocols were better supported with the introduction of NUMA and CMP 
architectures.
SQL Server 2012 has been optimized to take advantage of NUMA advancements exposed by both 
Windows and the hardware itself. As discussed in the “Memory Considerations and Enhancements” 
section, SQLOS is the technology that the SQL Server leverages to exploit these advances.
Ai  nity Mask
The afﬁ nity mask conﬁ guration option restricts a SQL Server instance to running on a subset 
of the processors. If SQL Server 2012 runs on a dedicated server, allowing SQL Server to use all 
processors can ensure best performance. In a server consolidation or multiple-instance environment, 
for more predictable performance, SQL Server may be conﬁ gured on dedicated hardware resources 
that afﬁ nitize processors by SQL Server instance.
SQL Server Processor Aﬃ  nity Mask
SQL Server’s mechanism for scheduling work requests is handled through a data structure 
concept called a scheduler. The scheduler is created for each processor assigned to SQL Server 
through the afﬁ nity mask conﬁ guration setting at startup. Worker threads (a subset of the 
max worker threads conﬁ guration setting) are dynamically created during a batch request 
and are evenly distributed between each CPU node and load-balanced across its schedulers (see 
Figure 11-10).
➤
➤
➤
➤


CPU Considerations ❘ 299
A worker thread continues to execute on its processor until it is forced to wait for a resource, such 
as locks or I/O, to become available. If the time slice expires, the thread voluntarily yields, at which 
time the scheduler selects another worker thread to begin execution. If it cannot proceed without 
access to a resource such as disk I/O, it sleeps until the resource is available. When access to that 
resource is available, the process is placed on the run queue before being put back on the processor. 
When the Windows kernel transfers control of the processor from an executing process to another 
that is ready to run, this is referred to as a context switch.
Context Switching
Context switching is expensive because of the associated housekeeping required to move from one 
running thread to another. Housekeeping refers to the maintenance of keeping the context or the 
set of processor register values and other data that describes the process state. The Windows kernel 
loads the context of the new process, which then starts to execute. When the process taken off 
the processor next runs, it resumes from the point at which it was taken off the processor. This is 
possible because the saved context includes the instruction pointer. In addition to this user mode 
time, context switching can take place in the Windows operating system (OS) for privileged 
mode time.
The total processor time is equal to the privileged mode time plus the user mode time.
Privileged Mode
Privileged mode is a processing mode designed for operating system components and hardware-
manipulating drivers. It enables direct access to hardware and all memory. Privileged time includes 
time-servicing interrupts and deferred process calls (DPCs).
User Mode
User mode is a restricted processing mode designed for applications such as SQL Server, Exchange, 
and other application and integral subsystems. The goal of performance tuning is to maximize user 
mode processing by reducing privileged mode processing. This can be monitored with the Processor: 
% Privileged Time counter, which displays the average busy time as a percentage of the sample time. 
A value above 15 percent may indicate that a disk array is being heavily used or that there is a high 
volume of network trafﬁ c requests. In some rare cases, a high rate of privileged time might even be 
attributed to a large number of interrupts generated by a failing device.
Priority Boost
The Priority Boost option is an advanced option under SQL Server. If you are using the sp_configure 
system stored procedure to change the setting, you can change Priority Boost only when Show 
Advanced Options is set to 1. The setting takes effect after the server is restarted. By enabling the 
Priority Boost option, SQL Server runs at a priority base of 13 in the Windows System scheduler, 
rather than its default of 7. On a dedicated server, this might improve performance, although it 
can also cause priority imbalances between SQL Server functions and operating system functions, 
leading to instability. Improvements in SQL Server 2012 and Windows make the use of this 
option unnecessary. 
 
 
 
 


CPU Considerations ❘ 301
Ai  nity I/O Mask
The afﬁ nity I/O mask feature, as shown in Figure 11-11, was introduced with SP1 of SQL Server 
2000. This option deﬁ nes the speciﬁ c processors on which SQL Server I/O threads can execute. The 
afﬁ nity I/O mask option has a default setting of 0, indicating that SQL Server threads are allowed 
to execute on all processors. The performance gains associated with enabling the afﬁ nity I/O 
mask feature are achieved by grouping the SQL threads that perform all I/O tasks (no-data cache 
retrievals — speciﬁ cally, physical I/O requests) on dedicated resources. This keeps I/O processing 
and related data in the same cache systems, maximizing data locality and minimizing unnecessary 
bus trafﬁ c.
When using afﬁ nity masks to assign processor resources to the operating system, either SQL 
Server processes (non-I/O) or SQL Server I/O processes, you must be careful not to assign multiple 
functions to any individual processor.
You should consider SQL I/O afﬁ nity when there is high privileged time on the processors that is 
not afﬁ nitized to SQL Server. For example, consider a 32-processor system running under load with 
30 of the 32 processors afﬁ nitized to SQL Server, leaving two processors to the Windows OS and 
other non-SQL activities. If the Processor: % Privileged time is high (greater than 15 percent), 
SQL I/O afﬁ nity can be conﬁ gured to help reduce the privileged-time overhead in the processors 
assigned to SQL Server.
FIGURE 11-11

302  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
The following steps outline the SQL I/O afﬁ nity procedure for determining what values to set: 
 1. 
Add I/O capability until there is no I/O bottleneck (disk queue length has been eliminated) 
and all unnecessary processes have been stopped.
 2. 
Add a processor designated to SQL I/O afﬁ nity.
 3. 
Measure the CPU utilization of these processors under heavy load.
 4. 
Increase the number of processors until the designated processors are no longer peaked. For 
a non-SMP system, select processors that are in the same cell node.
MEMORY CONSIDERATIONS AND ENHANCEMENTS
Because memory is fast relative to disk I/O, using this system resource effectively can have a large 
impact on the system’s overall ability to scale and perform well. 
SQL Server 2012 memory architecture and capabilities vary greatly from those of previous versions 
of SQL Servers. Changes include the ability to consume and release memory-based, internal-server 
conditions dynamically using the AWE mechanism. In SQL Server 2000, all memory allocations above 
4GB were static. Additional memory enhancements include the introduction of hierarchical memory 
architecture to maximize data locality and improve scalability by removing a centralized 
memory manager. SQL Server 2012 has resource monitoring, dynamic management views (DMVs), 
and a common caching framework. All these concepts are discussed throughout this section.
Tuning SQL Server Memory
The following performance counters are available from the System Performance Monitor. The 
SQL Server Cache Hit Ratio signiﬁ es the balance between servicing user requests from data in the 
data cache and having to request data from the I/O subsystem. Accessing data in RAM (or data 
cache) is exponentially faster than accessing the same information from the I/O subsystem; thus, the 
wanted state is to load all active data in RAM. Unfortunately, RAM is a limited resource. A wanted 
cache hit ratio average should be well over 90 percent. This does not mean that the SQL Server 
environment would not beneﬁ t from additional memory. A lower number signiﬁ es that the system 
memory or data cache allocation is below the wanted size.
Another reliable indicator of instance memory pressure is the SQL Server:Buffer Manager:Page-
life-expectancy (PLE) counter. This counter indicates the amount of time that a buffer page remains 
in memory, in seconds. The ideal number for PLE varies with the size of the RAM installed in your 
particular server and how much of that memory is used by the plan cache, Windows OS, and so 
on. The rule of thumb nowadays is to calculate the ideal PLE number for a speciﬁ c server using the 
following formula: MaxSQLServerMemory(GB) x 75. So, for a system that has 128GB of RAM and 
has a MaxServerMemory SQL setting of 120GB, the “minimum PLE before there is an issue” value 
is 9000. This can give you a more realistic value to monitor PLE against than the previous yardstick 
of 300. Be careful not to under-allocate total system memory because it forces the operating system 
to start moving page faults to a physical disk. A page fault is a phenomenon that occurs when the 
operating system goes to a physical disk to resolve memory references. The operating system may 
incur some paging, but when excessive paging takes places, it uses disk I/O and CPU resources, 



Memory Considerations and Enhancements ❘ 305
As shown in Figure 11-14, the CPU node is a subset 
of memory nodes and provides for logical grouping 
for CPUs.
A CPU node is also a hierarchical structure designed to 
provide logical grouping for CPUs. The purpose is to 
localize and load-balance related workloads to a CPU 
node. On an SMP system, all CPUs would be grouped 
under a single CPU node, whereas on a NUMA-based 
system, there would be as many CPU nodes as 
the system supported. The relationship between a CPU 
node and a memory node is explicit. There can be many 
CPU nodes to a memory node, but there can never be more than one memory node to a CPU node. 
Each level of this hierarchy provides localized services to the components that it manages, resulting 
in the capability to process and manage workloads in such a way as to exploit the scalability of 
whatever hardware architecture SQL Server runs on. SQLOS also enables services such as dynamic 
afﬁ nity, load-balancing workloads, dynamic memory capabilities, Dedicated Admin Connection 
(DAC), and support for partitioned resource management capabilities.
SQL Server 2012 leverages the common caching framework (also part of SQLOS) to achieve 
ﬁ ne-grain control over managing the increasing number of cache mechanisms (Cache Store, 
User Store, and Object Store). This framework improves the behavior of these mechanisms by 
providing a common policy that can be applied to internal caches to manage them in a wide range 
of operating conditions. For additional information about these caches, refer to SQL Server 2012 
Books Online.
SQL Server 2012 also features a memory-tracking enhancement called the Memory Broker, which 
enables the tracking of OS-wide memory events. Memory Broker manages and tracks the dynamic 
consumption of internal SQL Server memory. Based on internal consumption and pressures, it 
automatically calculates the optimal memory conﬁ guration for components such as buffer pool, 
optimizer, query execution, and caches. It propagates the memory conﬁ guration information back 
to these components for implementation. SQL Server 2012 also supports dynamic management of 
conventional, locked, and large-page memory, as well as the hot-add memory feature mentioned 
earlier. 
The Windows policy Lock Pages in Memory is granted by default to the local administrative 
accounts but can be explicitly granted to other user accounts. To ensure that memory runs as 
expected, it needs this privilege to enable SQL Server to manage which pages are ﬂ ushed out of 
memory and which pages are kept in memory.
Hot-add memory provides the ability to introduce additional memory in an operational server 
without taking it ofﬂ ine. In addition to OEM vendor support, Windows Server 2008 and SQL Server 
2012 Enterprise Edition are required to support this feature. Although a sample implementation 
script is provided in the following section, refer to BOL for additional implementation details.
64-bit Versions of SQL Server 2012
As SQL Server 2012 is only found in an x64 variety, you are in luck. SQL Server 2012 64-bit 
supports 1TB of RAM, and all of it is native addressable memory so there’s no need for /3GB or 
Memory Node
CPU Node
CPU Node
FIGURE 11-14

306  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
/PAE switches in your boot.ini ﬁ le. The mechanism used to manage AWE memory in 32-bit 
systems can be used to manage memory on 64-bit systems. Speciﬁ cally, this mechanism ensures 
that SQL Server manages what is ﬂ ushed out of its memory. To enable this feature, the SQL 
Server service account requires the Lock Pages in Memory privilege. To grant this access to 
the SQL Service account, use the Windows Group Policy tool (gpedit.msc) and perform the 
following steps: 
 1. 
On the Start menu, click Run. In the Open box, type gpedit.msc. The Group Policy 
dialog box opens.
 2. 
On the Group Policy console, expand Computer Conﬁ guration, and then expand 
Windows Settings.
 3. 
Expand Security Settings, and then expand Local Policies.
 4. 
Select the User Rights Assignment folder.
 5. 
The policies will be displayed in the details pane. In this pane, double-click Lock pages in 
memory.
 6. 
In the Local Security Policy Setting dialog box, click Add.
 7. 
In the Select Users or Groups dialog box, add the account that will be used to run 
sqlservr.exe.
You need to identify the type of application driving the database and verify that it can 
beneﬁ t from a large SQL Server data-cache allocation. In other words, is it memory-friendly? 
Simply stated, a database that does not need to keep its data in memory for an extended length 
of time cannot beneﬁ t from a larger memory allocation. For example, a call-center application in 
which no two operators handle the same customer’s information and where no relationship exists 
between customer records has no need to keep data in memory because data won’t be reused. In 
this case, the application is not deemed memory-friendly; thus, keeping customers’ data in memory 
longer than required would not beneﬁ t performance. Another type of inefﬁ cient memory use 
occurs when an application stores excessive amounts of data, beyond what is required by an 
operation — for example, a table scan. This type of operation suffers from the high cost of data 
invalidation. Larger amounts of data than are necessary are read into memory and thus must be 
ﬂ ushed out.
Data Locality
Data locality is the concept of having all relevant data available to the processor on its local NUMA 
node while it’s processing a request. All memory within a system is available to any processor on any 
NUMA node. This introduces the concepts of near memory and far memory. Near memory is the 
preferred method because it is accessed by a processor on the same NUMA node. As shown in 
Figure 11-15, accessing far memory is expensive because the request must leave the NUMA node and 
traverse the system interconnect crossbar to get the NUMA node that holds the required information in 
its memory.



Resource Governor  ❘ 309
RESOURCE GOVERNOR
Resource Governor is a SQL Server technology that limits the amount of resources that can be 
allocated to each database workload from the total resources available to SQL Server 2012. When 
enabled, Resource Governor classiﬁ es each incoming session and determines to which workload 
group the session belongs. Each workload group is then associated to a resource pool that limits 
those groups of workloads. Moreover, Resource Governor protects against runaway queries on 
the SQL Server and unpredictable workload execution, and sets workload priority. For Resource 
Governor to limit resources, it must differentiate workloads as follows: 
Classiﬁ es incoming connections to route them to a speciﬁ c workload group
Monitors resource usage for each workload group
Pools resources to set limits on CPU and memory to each speciﬁ c resource pool
Identiﬁ es workloads and groups them together to a speciﬁ c pool of resources
Sets workload priority within a workload group
The constraints on Resource Governor are as follows: 
It applies only to the SQL Server Relational Engine and not to Analysis Services, Report 
Services, or Integration Services.
It cannot monitor resources between SQL Server instances. However, you can use Windows 
Resource Manager (part of Windows) to monitor resources across Windows processes, 
including SQL Server.
It can limit CPU bandwidth and memory.
A typical OLTP workload consists of small and fast database operations that individually 
take a fraction of CPU time and may be too miniscule to enable Resource Governor to apply 
bandwidth limits.
The Basic Elements of Resource Governor
The main elements of Resource Governor include resource pools, workload groups, and 
classiﬁ cation support, explained in the following sections.
Resource Pools
The resource pools are the physical resources of the SQL Server. During SQL Server installation, 
two pools are created: internal and default. The resources that can be managed are min and max 
CPU and min and max memory. By default, the Resource Governor is disabled; to enable it, use the 
ALTER RESOURCE GOVERNOR RECONFIGURE command. You can disable it again with the command 
ALTER RESOURCE GOVERNOR DISABLE.
The internal pool is used for SQL Server’s own internal functions. It contains only the internal 
workload group; it cannot be altered, and it is not restricted.
➤
➤
➤
➤
➤
➤
➤
➤
➤

310  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
The default pool is the predeﬁ ned user resource pool. It contains the default workload group and 
can contain user-deﬁ ned workload groups. It can be altered but cannot be created or dropped.
You can create user-deﬁ ned pools using the CREATE RESOURCE POOL DDL statement or by using 
SQL Server Management Studio. Moreover, you can modify a pool by using the ALTER RESOURCE 
POOL command, and delete it by using the DROP RESOURCE POOL command. You may deﬁ ne any 
number of resource pools as you want, up to a maximum of 20. This includes the internal and 
default pools.
The following syntax creates a resource pool for Resource Governor: 
CREATE RESOURCE POOL pool_name
[ WITH
          ( [ MIN_CPU_PERCENT = value ]
     [ [ , ] MAX_CPU_PERCENT = value ] 
     [ [ , ] CAP_CPU_PERCENT = value ] 
     [ [ , ] AFFINITY {SCHEDULER = AUTO | (Scheduler_range_spec) | NUMANODE = (NUMA_
node_range_spec)}]      [ [ , ] MIN_MEMORY_PERCENT = value ]
     [ [ , ] MAX_MEMORY_PERCENT = value ] )
]  [;]
Please note that CAP_CPU_PERCENT and AFFINITY are new options available in SQL Server 2012. 
Please see the Books Online topic for more information which can be found here: http://msdn
.microsoft.com/en-us/library/bb934024(v=SQL.110).aspx.
Workload Groups
A workload group is a container for similar sessions according to the deﬁ ned classiﬁ cation rules and 
applies the policy to each session of the group. It also contains two predeﬁ ned workload groups: 
internal and default. The internal workload group relates to the internal resource pool and cannot 
be changed. The default workload group is associated with the default resource pool and is the 
group used when no session classiﬁ cation user-deﬁ ned function exists, when a classiﬁ cation failure 
occurs, or when a classiﬁ cation user-deﬁ ned function returns NULL.
You can create the user-deﬁ ned workload group by using the CREATE WORKLOAD GROUP command, 
modify it by using the ALTER WORKLOAD GROUP command, and drop it by using the DROP WORKLOAD 
GROUP command.
The following syntax creates a workload group for Resource Governor: 
 CREATE WORKLOAD GROUP group_name
 [ WITH
     ( [ IMPORTANCE = { LOW | MEDIUM | HIGH } ]
            [ [ , ] REQUEST_MAX_MEMORY_GRANT_PERCENT = value ]
            [ [ , ] REQUEST_MAX_CPU_TIME_SEC = value ]
            [ [ , ] REQUEST_MEMORY_GRANT_TIMEOUT_SEC = value ]
            [ [ , ] MAX_DOP = value ]
            [ [ , ] GROUP_MAX_REQUESTS = value ] )
  ]
 [ USING { pool_name | “default” } ] [ ; ]



Resource Governor  ❘ 313
ALTER RESOURCE GOVERNOR WITH (CLASSIFIER_FUNCTION= dbo.rgclassifier_v1);
COMMIT TRAN;
 5. 
Finally, to have these new changes take effect or to enable Resource Governor, run the 
following command: 
ALTER RESOURCE GOVERNOR RECONFIGURE;
Using Resource Governor from SQL Server 2012 
Management Studio
From inside SQL Server 2012 Management Studio in Object Explorer, Resource Governor is in the 
Management node. By default, Resource Governor is disabled. To enable it, right-click Resource 
Governor and then click Enable. Moreover, from Object Explorer inside the Management node, 
right-click Resource Governor, and then click Properties to add, delete, or modify properties of the 
resource pools and workload groups or to add or remove a user-deﬁ ned classiﬁ cation function and 
enable Resource Governor. From the Resource Governor Properties dialog, you can see that two 
resource pools, Internal and Default, with two corresponding workload groups, are visible. These 
are created by SQL Server 2012 during installation. No classiﬁ cation function is available until one 
is created. If Resource Governor is enabled, it uses the default system-deﬁ ned rules and the default 
pool, as conﬁ gured in Figure 11-18.
After running the script in Listing 11-6 to create the three user-deﬁ ned resource pools, workload 
groups, and the user-deﬁ ned classiﬁ cation function, and after enabling the Resource Governor, the 
properties should match what is shown in Figure 11-19.
FIGURE 11-18

314  ❘  CHAPTER 11  OPTIMIZING SQL SERVER 2012
Monitoring Resource Governor
SQL Server 2012 includes two performance objects to collect workload group and pool statistics for 
each SQL Server instance: 
SQLServer: Resource Pool Stats for resource-speciﬁ c statistics: Please see this 
article for more information: http://msdn.microsoft.com/en-us/library/
cc645958(v=SQL.110).aspx
SQLServer: Workload Group Stats for workload-speciﬁ c statistics: Please see this 
article for more information: http://msdn.microsoft.com/en-us/library/
cc627354(v=SQL.110).aspx
Moreover, there are Resource Governor Dynamic Management Views (DMVs) that return 
information speciﬁ c to resource statistics, as indicated in the following table. 
RESOURCE GOVERNOR DYNAMIC MANAGEMENT VIEWS
DESCRIPTION
sys.dm_resource_governor_configuration
For current in-memory conﬁ guration state
sys.dm_resource_governor_resource_pools
For resource pool state, conﬁ guration, 
and statistics
sys.dm_resource_governor_workload_groups
For workload group statisticvs and in-
memory conﬁ guration
➤
➤
FIGURE 11-19 



318  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
fragmentation. I did a few things to monitor resource usage and help ﬁ nd the key pain points. After 
you understood the pain points, you can take a few steps to get ahead of the curve on ﬁ xing things.
Now that you’ve seen the value in monitoring SQL Server, you can learn how to do this for yourself.
THE GOAL OF MONITORING
The goal when monitoring databases is to see what’s going on inside SQL Server — namely, how 
effectively SQL Server uses the server resources (CPU, Memory, and I/O). You want this information 
so that you can see how well the system performs. The data needs to be captured over time to enable 
you to build a proﬁ le of what the system normally looks like: How much of what resource do you 
use for each part of the system’s working cycle? From the data collected over time you can start to 
build a baseline of “normal” activity. That baseline enables you to identify abnormal activities that 
might lead to issues if left unchecked.
Abnormal activity could be an increase in a speciﬁ c table’s growth rate, a change in replication 
throughput, or a query or job taking longer than usual or using more of a scarce server resource than 
you expected. Identifying these anomalies before they become an issue that causes your users to call and 
complain makes for a much easier life. Using this data, you can identify what might be about to break, 
or where changes need to be made to rectify the root cause before the problem becomes entrenched.
Sometimes this monitoring is related to performance issues such as slow-running queries or deadlocks, 
but in many cases the data points to something that you can change to avoid problems in the future.
This philosophy is about the equivalent of “an apple a day keeps the doctor away” — preventative 
medicine for your SQL Server.
Determining Your Monitoring Objectives
Before you start monitoring you must ﬁ rst clearly identify your reasons for monitoring. These 
reasons may include the following: 
Establish a baseline.
Identify new trends before they become problems.
Monitor database growth.
Identify daily, weekly, and monthly maintenance tasks.
Identify performance changes over time.
Audit user activity.
Diagnose a speciﬁ c performance problem.
Establishing a Baseline
Monitoring is extremely important to help ensure the smooth running of your SQL Server systems. 
However, just monitoring by itself, and determining the value of a key performance metric at any point 
in time, is not of great value unless you have a sound baseline to compare the metric against. Are 50 
transactions per second good, mediocre, or bad? If the server runs at 75 percent CPU, is that normal? 
➤
➤
➤
➤
➤
➤
➤

Choosing the Appropriate Monitoring Tools ❘ 319
Is it normal for this time of day, on this day of the week, during this month of the year? With a baseline 
of the system’s performance, you immediately have something to compare the current metrics against.
If your baseline shows that you normally get 30 transactions per second, then 50 transactions 
per second might be good; the system can process more transactions. However, it may also be an 
indication that something else is going on, which has caused an increase in the transactions. What your 
baseline looks like depends on your system, of course. In some cases it might be a set of Performance 
Monitor logs with key server resource and SQL counters captured during several periods of signiﬁ cant 
system activity, or stress tests. In another case, it might be the results of analysis of a SQL Proﬁ ler 
trace captured during a period of high activity. The analysis might be as simple as a list of the stored 
procedure calls made by a particular application, with the call frequency.
To determine whether your SQL Server system performs optimally, take performance measurements 
at a regular interval over time, even when no problem occurs, to establish a server performance 
baseline. How many samples and how long each needs to be are determined by the nature of the 
workload on your servers. If your servers have a cyclical workload, the samples should aim to query 
at multiple points in several cycles to allow a good estimation of min, max, and average rates. If the 
workload is uniform, then fewer samples over shorter periods can provide a good indication of min, 
max, and average rates. At a minimum, use baseline performance to determine the following: 
Peak and off-peak hours of operation
Query or batch response time
Another consideration is how often the baseline should be recaptured. In a system that is rapidly 
growing, you may need to recapture a baseline frequently. When current performance has changed by 
15 to 25 percent compared to the old baseline, it is a good point to consider recapturing the baseline.
Comparing Current Metrics to the Baseline
A key part of comparing current metrics to those in the baseline is determining an acceptable limit 
from the baseline outside of which the current metric is not acceptable and which ﬂ ags an issue that 
needs investigating.
What is acceptable here depends on the application and the speciﬁ c metric. For example, a metric 
looking at free space in a database ﬁ legroup for a system with massive data growth and an aggressive 
archiving strategy might set a limit of 20 percent free space before triggering some kind of alert. On a 
different system with little database growth, that same metric might be set to just 5 percent.
You must make your own judgment of what is an acceptable limit for deviation from the baseline 
based on your knowledge of how your system is growing and changing.
CHOOSING THE APPROPRIATE MONITORING TOOLS
After you deﬁ ne your monitoring goals, you should select the appropriate tools for monitoring. The 
following list describes the basic monitoring tools: 
Performance Monitor: Performance Monitor is a useful tool that tracks resource use on 
Microsoft operating systems. It can monitor resource usage for the server and provide 
information speciﬁ c to SQL Server either locally or for a remote server. You can use it to 
➤
➤
➤

320  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
capture a baseline of server resource usage, or it can monitor over longer periods of time to 
help identify trends. It can also be useful for ad hoc monitoring to help identify any resource 
bottlenecks responsible for performance issues. You can conﬁ gure it to generate alerts when 
predetermined thresholds are exceeded.
Extended Events: Extended Events provide a highly scalable and conﬁ gurable architecture 
to enable you to collect information to troubleshoot issues with SQL Server. It is a 
lightweight system with a graphical UI that enables new sessions to be easily created.
Extended Events provides the system_health session. This is a default health session 
that runs with minimal overhead, and continuously collects system data that may help 
you troubleshoot your performance problem without having to create your own custom 
Extended Events session.
While exploring the Extended Events node in SSMS, you may notice an additional default 
session, the AlwaysOn_health session. This is an undocumented session created to provide 
health monitoring for Availability Groups.
SQL Proﬁ ler: This tool is a graphical application that enables you to capture a trace of 
events that occurred in SQL Server. All SQL Server events can be captured by this tool into 
the trace. The trace can be stored in a ﬁ le or written to a SQL Server table. 
SQL Proﬁ ler also enables the captured events to be replayed. This makes it a valuable 
tool for workload analysis, testing, and performance tuning. It can monitor a SQL Server 
instance locally or remotely. You can also use the features of SQL Proﬁ ler within a custom 
application, by using the Proﬁ ler system stored procedures. 
SQL Proﬁ ler has been deprecated in SQL Server 2012, so you should plan on moving away 
from using this tool, and instead use Extended events for trace capture activities, and 
Distributed Replay for replaying events.
SQL Trace: SQL Trace is the T-SQL stored procedure way to invoke a SQL Server trace 
without needing to start up the SQL Proﬁ ler application. It requires a little more work to 
set up, but it’s a lightweight way to capture a trace; and because it’s scriptable, it enables the 
automation of trace capture, making it easy to repeatedly capture the same events.
With the announcement of the deprecation of SQL Server proﬁ ler, you should start moving 
all your trace based monitoring to Extended Events.
Default trace: Introduced with SQL Server 2005, the default trace is a lightweight trace that 
runs in a continuous loop and captures a small set of key database and server events. This is 
useful in diagnosing events that may have occurred when no other monitoring was in place.
Activity Monitor in SQL Server Management Studio: This tool graphically displays the 
following information: 
Processes running on an instance of SQL Server
Resource Waits
Data File IO activity
Recent Expensive Queries
Dynamic management views and functions: Dynamic management views and functions 
return server state information that you can use to monitor the health of a server instance, 
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

Performance Monitor ❘ 321
diagnose problems, and tune performance. These are one of the best tools added to SQL 
Server 2005 for ad hoc monitoring. These views provide a snapshot of the exact state of 
SQL Server at the point they are queried. This is extremely valuable, but you may need 
to do a lot of work to interpret the meaning of some of the data returned, because they 
often provide just a running total of some internal counter. You need to add quite a bit of 
additional code to provide useful trend information. There are numerous examples in the 
“Monitoring with Dynamic Management Views and Functions” section later in this chapter 
that show how to do this.
System Stored procedures: Some system-stored procedures provide useful information for 
SQL Server monitoring, such as sp_who, sp_who2, sp_lock, and several others. These 
stored procedures are best for ad hoc monitoring, not trend analysis.
Utility Control Point (UCP): The Utility Control Point is a management construct 
introduced in SQL Server 2008 R2. It adds to the Data Collection sets of the Management 
Data Warehouse and includes reports on activity within a SQL Server Utility. The SQL 
Server Utility is another addition for SQL Server 2008 R2, and is a management container 
for server resources that can be monitored using the UCP.
Standard Reports: The standard reports that ship with SQL Server are a great way to 
get a look into what’s happening inside SQL Server without needing to dive into DMVs, 
Extended Events, and the default Trace. 
SQL Server Best Practice Analyzer: A set of rules implemented in Microsoft Baseline 
Conﬁ guration Analyzer (MBCA) that implement checks for SQL Server best practices. The 
tool is available as a download, and has a ﬁ xed set of rules. Also see System Center Advisor.
System Center Advisor: An extension of the SQL Server Best Practice Analyzer, this is a 
cloud-based utility to analyze your SQL Servers and provide feedback on their conﬁ guration 
and operation against the set of accepted best practices for conﬁ guring and operating SQL 
Server.
System Center Management Pack: SQL Server has had a management pack for some time 
now, but it’s not well known, or used by DBAs. The Management Pack enables you to create 
exception-driven events that drive operator interaction with SQL Server to resolve speciﬁ c 
issues. The rest of the chapter discusses these tools in detail.
PERFORMANCE MONITOR
Performance Monitor, also known as Perfmon, or System Monitor, is the User Interface that most 
readers will become familiar with as their interface with Performance Monitoring. Performance 
Monitor is a Windows tool that’s found in the Administrative Tools folder on any Windows PC, 
or Server. It has the ability to graphically show performance counter data as a graph (the default 
setting) or as a histogram, or in a textual report format.
Performance Monitor is an important tool because not only does it inform you about how SQL 
Server performs, it is also the tool that indicates how Windows performs. Performance Monitor 
provides a huge set of counters, but don’t be daunted. This section covers a few of them, but there is 
likely no one who understands all of them.
➤
➤
➤
➤
➤
➤
 
 
 
 


Performance Monitor ❘ 323
The ﬁ rst step to ﬁ nd the cause of the bottleneck is to identify that the bottleneck is a CPU resource 
issue. The following counters can help you do this: 
Object: Processor - Counter: % Processor Time: This counter determines the percentage 
of time each processor is busy. There is a _Total instance of the counter that for 
multiprocessor systems measures the total processor utilization across all processors in 
the system. On multiprocessor machines, the _Total instance might not show a processor 
bottleneck when one exists. This can happen when queries execute that run on either a 
single thread or fewer threads than there are processors. This is often the case on OLTP 
systems, or where MAXDOP has been set to less than the number of processors available. 
In this case, a query can be bottlenecked on the CPU as it’s using 100 percent of the single 
CPU it’s scheduled to run on, or in the case of a parallel query as it’s using 100 percent of 
multiple CPUs, but in both cases other idle CPUs are available that this query is not using. 
If the _Total instance of this counter is regularly at more than 80 percent, that’s a good 
indication that the server is reaching the limits of the current hardware. Your options 
here are to buy more or faster processors, or optimize the queries to use less CPU. See 
Chapter 11, “Optimizing SQL Server 2012,” for a detailed discussion on hardware.
Object: System - Counter: Processor Queue Length: The processor queue length is a 
measure of how many threads sit in a ready state waiting on a processor to become 
available to run them. Interpreting and using this counter is an advanced operating system 
performance-tuning option needed only when investigating complex multithreaded code 
problems. For SQL Server systems, processor utilization can identify CPU bottlenecks much 
more easily than trying to interpret this counter.
Object: Processor - Counter: % Privileged Time: This counter indicates the percentage of 
the sample interval when the processor was executing in kernel mode. On a SQL Server 
system, kernel mode time is time spent executing system services such as the memory 
manager, or more likely, the I/O manager. In most cases, privileged time equates to time 
spent reading and writing from disk or the network. 
It is useful to monitor this counter when you ﬁ nd an indication of high CPU usage. If 
this counter indicates that more than 15 percent to 20 percent of processor time is spent 
executing privileged code, you may have a problem, possibly with one of the I/O drivers, 
or possibly with a ﬁ lter driver installed by antivirus software scanning the SQL data or 
log ﬁ les.
Object: Process - Counter: % Processor Time - Instance: sqlservr: This counter measures 
the percentage of the sample interval during which the SQL Server Process uses the available 
processors. When the Processor % Processor Time counter is high, or you suspect a CPU 
bottleneck, look at this counter to conﬁ rm that it is SQL Server using the CPU, and not 
some other process.
Object: Process - Counter: % Privileged Time - Instance: sqlservr: This counter measures 
the percentage of the sample that the SQL Server Process runs in kernel mode. This will 
be the kernel mode portion of the total %ProcessorTime shown in the previous counter. 
As with the previous counter, this counter is useful when investigating high CPU usage on 
the server to conﬁ rm that it is SQL Server using the processor resource, and not some other 
process.
➤
➤
➤
➤
➤





328  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
from the disk over the sample interval. This is an average over the sample period, so with long 
sample periods it may average out to big peaks and troughs in throughput. Over a short sample 
period, this may ﬂ uctuate dramatically, as it sees the results of one or two larger I/Os ﬂ ooding the 
I/O subsystem. This information is useful to determine whether the I/O subsystem is approaching 
its capacity. 
As with the other Disk IO counters, Disk Read Bytes/Sec and Disk Write Bytes/Sec can be used in 
isolation to compare against the theoretical throughput for the number and type of disks in the 
I/O subsystem, but it is more useful when it can be compared against a baseline of the maximum 
throughput available from the I/O subsystem.
Monitoring I/O Latency
The Object: Physical Disk - Counter: Avg. Disk Sec/Write and Object: Physical Disk - Counter: Avg. 
Disk Sec/Read provide information on how long each read-and-write operation is taking. These two 
counters show average latency. It is an average taken over every I/O issued during the sample period. 
This information is extremely useful and can be used independently to determine how well the 
I/O subsystem deals with the current I/O load. Ideally, these counters should be below 5–10 
milliseconds (ms). On larger data warehouse or decision support systems, it is acceptable for 
the values of these counters to be in the range of 10–20 ms. Sustained values over 50 ms are an 
indication that the I/O subsystem is heavily stressed, and that a more detailed investigation of I/O 
should be undertaken. 
These counters show performance degradations before queuing starts. These counters should also 
be used with the following disk queue length counters to help diagnose I/O subsystem bottlenecks.
Monitoring I/O Queue Depth
The Object: Physical Disk - Counter: Avg. Disk Write Queue Length and Object: Physical Disk - 
Counter: Avg. Disk Read Queue Length provide information on the read-and-write queue depth. 
These two counters show the average queue depth over the sample period. Disk queue lengths 
greater than 2 for a single physical disk indicate that there may be an I/O subsystem bottleneck. 
Correctly interpreting these counters is more challenging when the I/O subsystem is a RAID array, 
or when the disk controller has built-in caching and intelligence. In these cases, the controller will 
have its own queue, which is designed to absorb and buffer, and effectively hide from this counter, 
any queuing going on at the disk level. For these reasons, monitoring these counters is less 
useful than monitoring the latency counters. If these counters do show queue lengths consistently 
greater than 2, it’s a good indication of a potential I/O subsystem bottleneck.
Monitoring Individual Instances Versus Total
In multidisk systems with several disks, monitoring all the preceding counters for all available disks 
provides a mass of ﬂ uctuating counters to monitor. In some cases, monitoring the _Total instance, 
which combines the values for all instances, can be a useful way to detect I/O problems. The 
scenario in which this doesn’t work is when I/O to different disks has different characteristics. In 
this case, the _Total instance shows a reasonably good average number, although some disks may 
sit idle and others melt from all the I/O requests they service.

Performance Monitor ❘ 329
Monitoring Transfers Versus Read and Write
One thing you may have noticed in the list of counters is that the transfer counters are missing. This 
is because the transfer counters average out the read-and-write activity. For a system that is heavy 
on one kind of I/O at the expense of the other (reads versus writes), the transfer counters do not 
show an accurate picture of what happens.
In addition, read I/O and write I/O usually have different characteristics, and different performance 
than the underlying storage. Monitoring a combination of two potentially disparate values doesn’t 
provide a meaningful metric.
Monitoring %Disk Counters
Another set of disk counters missing from this list are all the %Disk counters. Although these 
counters can provide interesting information, there are enough problems with the results (that is, the 
total percentage can often exceed 100) that these counters don’t provide a useful detailed metric.
If you can afford to monitor all the counters detailed in the preceding sections, you can have a much 
more complete view of what’s going on with your system.
If you want a few simple metrics that provide a good approximate indication of overall I/O subsystem 
activity, then the %Disk Time, %Disk Read Time, and %Disk Write time counters can provide that.
Isolating Disk Activity Created by SQL Server
All the counters you should monitor to ﬁ nd disk bottlenecks have been discussed. However, you 
may have multiple applications running on your servers, and one of those other applications could 
cause a lot of disk I/O. To conﬁ rm that the disk bottleneck is being caused by SQL Server, you 
should isolate the disk activities created by SQL Server. Monitor the following counters to determine 
whether the disk activity is caused by SQL server: 
SQL Server: Buffer Manager: Page reads/sec
SQL Server: Buffer Manager: Page writes/sec
Sometimes your application is too big for the hardware you have, and a problem that appears to 
be related to disk I/O may be resolved by adding more RAM. Make sure you do a proper analysis 
before making a decision. That’s where trend analysis is helpful because you can see how the 
performance problem evolved.
Is Disk Performance the Bottleneck?
With the help of the disk counters, you can determine whether you have disk bottlenecks in your 
system. Several conditions must exist for you to make that determination, including a sustained rate 
of disk activity well above your baseline, persistent disk queue length longer than two per disk, and 
the absence of a signiﬁ cant amount of paging. Without this combination of factors, it is unlikely 
that you have a disk bottleneck in your system.
Sometimes your disk hardware may be faulty, and that could cause a lot of interrupts to the CPU. 
Another possibility could be that a processor bottleneck is caused by a disk subsystem, which 
can have a systemwide performance impact. Make sure you consider this when you analyze the 
performance data. 
➤
➤


Performance Monitor ❘ 331
Monitoring Available Memory 
The Object: Memory - Counter: Available Mbytes reports how many megabytes of memory are 
currently available for programs to use. It is the best single indication that there may be a memory 
bottleneck on the server.
Determining the appropriate value for this counter depends on the size of the system you monitor. If 
this counter routinely shows values less than 128MB, you may have a serious memory shortage.
On a server with 4GB or more of physical memory (RAM), the operating system can send a low 
memory notiﬁ cation when available memory reaches 128MB. At this point, SQL releases some of its 
memory for other processes to use.
Ideally, aim to have at least 256MB to 500MB of Available MBytes. On larger systems with more 
than 16GB of RAM, this number should be increased to 500MB–1GB. If you have more than 64GB 
of RAM on your server, increase this to 1–2GB. 
Monitoring SQL Server Process Memory Usage
Having used the Memory – Counter: Available Mbytes to determine that a potential memory 
shortage exists; the next step is to determine which processes use the available memory. As your 
focus is on SQL Server, you hope that it is SQL Server that uses the memory. However, you should 
always conﬁ rm that this is the case.
The usual place to look for a process’s memory usage is in the Process object under the instance for 
the process. For SQL Server, these counters are detailed in the following list: 
Object: Process – Instance: sqlserver - Counter: Virtual Bytes: This counter indicates the 
size of the virtual address space allocated by the process. Virtual address space (VAS) is used 
by a lot of processes that aren’t related to memory performance. This counter is of value 
when looking for the root cause of SQL Server out-of-memory errors. If running on a 32-bit 
system, virtual address space is limited to 2GB (except when the /3GB switch is enabled). 
In this environment, if virtual bytes approach 1.5GB to 1.7GB, that is about as much space 
as can be allocated. At this point the root cause of the problem is a VAS limitation issue, 
and the resolution is to reduce SQL Server’s memory usage, enable AWE, boot using /3GB, 
or move to a 64-bit environment. On a 64-bit system this counter is of less interest because 
VAS pressure is not going to occur. To learn more about AWE, refer to Chapter 11. 
This counter includes the AWE window, but it does not show how much physical memory is 
reserved though AWE.
Object: Process – Instance: sqlservr – Counter: Working Set: This counter indicates the size of 
the working set for the SQL Server process. The working set is the total set of pages currently 
resident in memory, as opposed to being paged to disk. It can provide an indication of 
memory pressure when this is signiﬁ cantly lower than the Private Bytes for the process. 
This counter does not include AWE memory allocations.
Object: Process – Instance: sqlservr – Counter: Private Bytes: The Private Bytes counter 
tells you how much memory this process has allocated that cannot be shared with other 
processes — that is, it’s private to this process. To understand the difference between this 
➤
➤
➤


Performance Monitor ❘ 333
and the memory usage proﬁ le for your applications. Having a good baseline is useful, as 
the values for this counter can be compared to the baseline to determine whether there is a 
current memory issue, or whether the value is part of the expected behavior of the system. 
This counter should be read with the Page Life Expectancy counter.
Page Life Expectancy: This counter provides an indication of the time, in seconds, that 
a page is expected to remain in the buffer pool before being ﬂ ushed to disk. The current 
Best Practices from Microsoft state that values above 300 are generally considered okay. 
Values approaching 300 are a cause for concern. Values below 300 are a good indication 
of a memory shortage. These Best Practices are now getting a bit dated however. They 
were written when a large system might have 4 dual or quad core processors, and 16GB of 
RAM. Today’s commodity hardware comes with a 2P system with 10, 12, 16+ cores, and 
the ability to have 256GB or more of memory. Therefore, those best practice numbers are 
less relevant. Because of this, you should interpret this counter in conjunction with other 
counters to understand if there really is memory pressure.
Read this counter with the Free Pages counter. You should expect to see Free Pages drop 
dramatically as the page life expectancy drops below 300. When considered together, Free 
Pages and Page Life expectancy provide an indication of memory pressure that may result in 
a bottleneck.
Scripting Memory Counters with Logman
Following is a Logman script that can create a counter log of the memory counters discussed in this 
section (see the “Logman” section later in the chapter for more information): 
Logman create counter “Memory Counters” -si 05 -v nnnnnn -o
“c:\perflogs\Memory Counters” -c “\Memory\Available MBytes”
“\Process(sqlservr)\Virtual Bytes” “\Process(sqlservr)\Working Set”
“\Process(sqlservr)\Private Bytes” “\SQLServer:Buffer Manager\Database
pages“ ”\SQLServer:Buffer Manager\Target pages“ ”\SQLServer:Buffer
Manager\Total pages“ ”\SQLServer:Memory Manager\Target Server Memory (KB)”
“\SQLServer:Memory Manager\Total Server Memory (KB)”
Resolving Memory Bottlenecks
The easy solution to memory bottlenecks is to add more memory; but previously stated, tuning your 
application always comes ﬁ rst. Try to ﬁ nd queries that are memory-intensive, for instance queries 
with large worktables — such as hashes for joins and sorts — to see if you can tune them. You can 
learn more about tuning T-SQL queries in Chapter 13 “Performance Tuning T-SQL.”
In addition, refer to Chapter 10 to ensure that you have conﬁ gured your server properly. If you 
are running a 32-bit machine and after adding more memory you are still running into memory 
bottlenecks, then look into a 64-bit system.
Performance Monitoring Tools
A few tools are well hidden in the command-line utilities that have shipped with Windows operating 
systems for some time. Two of these that are extremely valuable when using Performance Monitor 
are Logman and Relog.
➤

334  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Logman
Logman is a command-line way to script performance monitoring counter logs. You can create, 
alter, start, and stop counter logs using Logman.
You have seen several examples earlier in this chapter of using Logman to create different counter 
logs. Following is a short command-line script ﬁ le to start and stop a counter collection: 
REM start counter collection
logman start “Memory Counters”
timeout /t 5
REM add a timeout for some short period
REM to allow the collection to start
REM do something interesting here
REM stop the counter collection
logman stop “Memory Counters”
timeout /t 5
REM make sure to wait 5 to ensure its stopped
Complete documentation for Logman is available through the Windows help system.
Logman Script for I/O Counters
The following script can create a new counter log called IO Counters and collect samples for 
every counter previously detailed, for all instances and with a 5-second sample interval, and write 
the log to c:\perflogs\IO Counters, appending a six-digit incrementing sequence number to 
each log: 
Logman create counter “IO Counters” -si 05 -v nnnnnn -o “c:\perflogs\IO
Counters“ -c ”\PhysicalDisk(*)\Avg. Disk Bytes/Read“ ” \PhysicalDisk(*)\Avg.
Disk Bytes/Write“ ”\PhysicalDisk(*)\Avg. Disk Read Queue Length”
“\PhysicalDisk(*)\Avg. Disk sec/Read” “\PhysicalDisk(*)\Avg. Disk sec/Write”
“\PhysicalDisk(*)\Avg. Disk Write Queue Length” “\PhysicalDisk(*)\Disk Read
Bytes/sec“ ”\PhysicalDisk(*)\Disk Reads/sec“ ”\PhysicalDisk(*)\Disk Write
Bytes/sec“ ”\PhysicalDisk(*)\Disk Writes/sec”
After running this script, run the following command to conﬁ rm that the settings are as expected: 
logman query “IO Counters”
Relog
Relog is a command-line utility that enables you to read a log ﬁ le and write selected parts of it to a 
new log ﬁ le.
You can use it to change the ﬁ le format from blg to csv. You can use it to resample data and turn a 
large ﬁ le with a short sample period into a smaller ﬁ le with a longer sample period. You can also use 
it to extract a short period of data for a subset of counters from a much larger ﬁ le.
Complete documentation for Relog is available through the Windows help system.

Monitoring Events ❘ 335
MONITORING EVENTS
Events are ﬁ red at the time of some signiﬁ cant occurrence within SQL Server. Using events enables you 
to react to the behavior at the time it occurs, and not have to wait until some later time. SQL Server 
generates many different events and has several tools available to monitor some of these events.
The following list describes the different features you can use to monitor events that happened in 
the Database Engine: 
system_health Session: The system_health session is included by default with SQL Server, 
starts automatically when SQL Starts, and runs with no noticeable performance impact. It 
collects a minimal set of system information that can help resolve performance issues.
Default Trace: Initially added in SQL Server 2005, this is perhaps one of the best kept 
secrets in SQL Server. It’s virtually impossible to ﬁ nd any documentation on this feature. 
The default trace is basically a ﬂ ight data recorder for SQL Server. It records the last 5MB 
of key events. The events it records were selected to be lightweight, yet valuable when 
troubleshooting a critical SQL event.
SQL Trace: This records speciﬁ ed events and stores them in a ﬁ le (or ﬁ les) that you can use 
later to analyze the data. You have to specify which Database Engine events you want to 
trace when you deﬁ ne the trace. Following are two ways to access the trace data: 
Using SQL Server Proﬁ ler, a graphical user interface
Through T-SQL system stored procedures
SQL Server Proﬁ ler: This exploits all the event-capturing functionality of SQL Trace and 
adds the capability to trace information to or from a table, save the trace deﬁ nitions as 
templates, extract query plans and deadlock events as separate XML ﬁ les, and replay trace 
results for diagnosis and optimization. Another option, and perhaps least understood, is 
using a database table to store the trace. Storing the trace ﬁ le in a database table enables the 
use of T-SQL queries to perform complex analysis of the events in the trace.
Event notiﬁ cations: These send information to a Service Broker service about many of the 
events generated by SQL Server. Unlike traces, event notiﬁ cations can be used to perform 
an action inside SQL Server in response to events. Because event notiﬁ cations execute 
asynchronously, these actions do not consume any resources deﬁ ned by the immediate 
transaction, meaning, for example, that if you want to be notiﬁ ed when a table is altered 
in a database, then the ALTER TABLE statement would not consume more resources or be 
delayed because you have deﬁ ned event notiﬁ cation.
Extended Events: These were new with SQL Server 2008 and extend the Event Notiﬁ cation 
mechanism. They are built on the Event Tracing for Windows (ETW) framework. Extended 
Events are a different set of events from those used by Event Notiﬁ cations and can be used 
to diagnose issues such as low memory conditions, high CPU use, and deadlocks. The logs 
created when using SQL Server Extended Events can also be correlated with other ETW 
logs using tracerpt.exe. See the topic on Extended Events in SQL Server Books Online for 
more references to information on using ETW and tracerpt.exe. For more details, see the 
section “SQL Server Extended Event Notiﬁ cation” later in this chapter.
➤
➤
➤
➤
➤
➤
➤
➤

336  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Following are a number of reasons why you should monitor events that occur inside your SQL Server: 
Find the worst-performing queries or stored procedures: You can do this using either 
Extended Events or through SQL Proﬁ ler / SQL Trace. To use SQL Proﬁ ler, you can ﬁ nd 
a trace template on this book’s website at www.wrox.com, which you can import into your 
SQL Server Proﬁ ler to capture this scenario. This includes the Showplan Statistics Proﬁ le, 
Showplan XML, and Showplan XML Statistics Proﬁ le under Performance event groups. 
These events are included because after you determine the worst-performing queries, you 
need to see what query plan was generated by them. Just looking at the duration of the T-SQL 
batch or stored procedure does not get you anywhere. Consider ﬁ ltering the trace data by 
setting some value in the Duration column to retrieve only those events that are longer than 
a speciﬁ c duration so that you minimize your dataset for analysis.
Audit user activities: You can either use the new SQL Audit capabilities in Extended Events 
to create a SQL Audit, or create a trace with Audit Login events. If you choose the latter, 
select the EventClass (the default), EventSubClass, LoginSID, and LoginName data 
columns; this way you can audit user activities in SQL Server. You may add more events 
from the Security Audit event group or data columns based on your need. You may someday 
need this type of information for legal purposes in addition to your technical purposes.
Identify the cause of a deadlock: You can do this using Extended Events. Much of the 
information needed is available in the system_health session that runs by default on every 
instance of SQL Server. You look into how to do that in more detail later in this chapter.
You can also do this the “old way” by setting the startup trace ﬂ ags for tracing 
deadlocks. SQL Trace doesn’t persist between server cycles unless you use SQL Job to 
achieve this. You can use startup trace ﬂ ag 1204 or 1222 (1222 returns more verbose 
information than 1204 and resembles an XML document) to trace a deadlock anytime 
it happens on your SQL Server. Refer to Chapter 4, “Managing and Troubleshooting the 
Database Engine,” to learn more about these trace ﬂ ags and how to set them. To capture 
deadlock information using SQL Trace, you need to capture these events in your trace: 
Start with Standard trace template and add the Lock event classes (Lock: Deadlock 
graph, Lock: Deadlock, or Lock: Deadlock Chain). If you specify the Deadlock graph 
event class, SQL Server Proﬁ ler produces a graphical representation of the deadlock.
Collect a representative set of events for stress testing: For some benchmarking, you want to 
reply to the trace generated. SQL Server provides the standard template TSQL_Replay to capture 
a trace that can be replayed later. If you want to use a trace to replay later, make sure that you 
use this standard template because to replay the trace, SQL Server needs some speciﬁ c events 
captured, and this template does just that. Later in this chapter you see how to replay the trace.
Create a workload to use for the Database Engine Tuning Adviser: SQL Server Proﬁ ler 
provides a predeﬁ ned Tuning template that gathers the appropriate Transact-SQL events in 
the trace output, so it can be used as a workload for the Database Engine Tuning Advisor.
Take a performance baseline: Earlier you learned that you should take a baseline and 
update it at regular intervals to compare with previous baselines to determine how your 
application performs. For example, suppose you have a batch process that loads some data 
once a day and validates it, does some transformation, and so on, and puts it into your 
warehouse after deleting the existing set of data. After some time there is an increase in data 
volume and suddenly your process starts slowing down. You would guess that an increase in 
➤
➤
➤
➤
➤
➤


338  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Objects: These events capture information around User object activity, speciﬁ cally Create, 
Delete, and Alter on any user object. If you need to know when a particular object was 
created, altered, or deleted, this could be the place to go look.
Security Audit: This captures events for the major security events occurring in SQL Server. 
There is quite a comprehensive list of sub events (not listed here).If you’re looking for 
security based information, then this should be the ﬁ rst place you go looking.
Server: The server category contains just one event, Server Memory Change. This event 
indicates when SQL Server memory usage increases or decreases by 1MB, or 5% of max 
server memory, whichever is larger.
You can see these categories by opening one of the default trace ﬁ les in SQL Server Proﬁ ler and 
examining the trace ﬁ le properties. By default, you don’t have permission to open the trace ﬁ les 
while they live in the Logs folder, so either copy the ﬁ le to another location, or alter the permissions 
on the ﬁ le that you want to open in proﬁ ler.
When you open the trace ﬁ le properties, you see that for each category all event columns are 
selected for all the events in the default trace.
system_health Session
The system_health session is a default extended events session that is created for you by SQL Server. 
It is very lightweight and has minimal impact on performance. With previous technologies like SQL 
Server Proﬁ ler and the default trace, customers have worried about the performance impact of running 
these monitoring tools. Extended Events and the system_health session mitigate those concerns.
The system_health session contains a wealth of information that can help diagnose issues with SQL 
Server. The following is a list of some of the information collected by this session.
SQL text and Session ID for sessions that:
Have a severity ≥ 20
Encounter a memory related error
Have waited on latches for ≥ 15 seconds
Have waited on Locks for ≥ 30 seconds
Deadlocks
Nonyielding scheduler problems
SQL Trace
As mentioned earlier, you have two ways to deﬁ ne the SQL Trace: using T-SQL system stored 
procedures and SQL Server Proﬁ ler. This section ﬁ rst explains the SQL Trace architecture; then you 
study an example to create the server-side trace using the T-SQL system stored procedure.
Before you start, you need to know some basic trace terminology: 
Event: The occurrence of an action within an instance of the Microsoft SQL Server 
Database Engine or the SQL Server Database Engine, such as the Audit: Logout event, 
which happens when a user logs out of SQL Server.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤





Monitoring Events ❘ 343
You can specify the maximum size of the trace ﬁ le before it creates another ﬁ le to add the trace data 
using the @maxfilesize parameter, in MB. In this case you have speciﬁ ed 10MB, which means 
that when the trace ﬁ le size exceeds 10MB, SQL Trace creates another ﬁ le and starts adding data 
there. Use this option because if you create one big ﬁ le, it’s not easy to move it around; and if you 
have multiple ﬁ les, then you can start looking at the older ﬁ les while trace is writing to the new ﬁ le. 
In addition, if disk space issues arise while gathering the trace data, you can move ﬁ les to different 
drives or servers.
You can optionally specify the trace stop time using the @stoptime parameter, which is of the 
datetime type.
The @filecount parameter speciﬁ es the maximum number of trace ﬁ les to be maintained with the 
same base ﬁ lename. Refer to Books Online for a detailed description of this parameter. 
Now look at how to set up the events and choose the data columns for those events. The stored 
procedure sp_trace_setevent can do that job with the following steps:
 1. 
The ﬁ rst parameter you use is the traceid, which you got from the sp_trace_create 
stored procedure. 
 2. 
The second parameter you use, @eventid, is the internal ID of the event you want to trace. 
The ﬁ rst call of the stored procedure speciﬁ es 14, which is the Audit Login event. 
 3. 
In the third parameter, specify which data column you want to capture for the event 
indicated. In this case, you have set @columnid to 8, which is the data column HostName. 
Call this stored procedure for each data column you want for a particular event. Call this 
stored procedure multiple times for @eventid 14 because you want multiple data columns. 
 4. 
The last parameter you use is @ON, which is a bit parameter that speciﬁ es whether you 
want to turn the event on or off. As mentioned earlier, the sp_trace_setevent article in 
SQL Server Books Online documents the internal ID number for each event and each data 
column.
 5. 
Once the event is established, set the ﬁ lter on it. Use the stored procedure sp_trace_
setfilter to set the ﬁ lter on a particular event and the data column. The article 
sp_trace_setfilter in BOL documents the internal ID number for the @comparison_
operator and @logical_operator parameters. In this case, you want only the trace 
generated by the application name SQL Server Management Studio – Query.
 6. 
To start the trace use the stored procedure sp_trace_setstatus. You can specify the trace 
ID you want to take action on with the option 0, 1, or 2. Because you want to start the 
trace, you have speciﬁ ed 1. If you want to stop it, specify 0. If you specify 2, it closes 
the speciﬁ ed trace and deletes its deﬁ nition from the server.
 7. 
You’re all set to run the server-side trace. You speciﬁ ed the @datetime option to stop the 
trace. You need to change the datetime value as your needs dictate. Make sure that if you 
specify the UNC path for the trace ﬁ le, the SQL Server service account has write access to 
the share. Run the script now.





348  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
When you set up the trace using Proﬁ ler, if you choose Showplan XML or Showplan Statistics 
Proﬁ le or Showplan XML for Query Compile, a tab shows up in the Trace Properties dialog, as 
shown in Figure 12-9.
FIGURE 12-9
Also shown in Figure 12-9 is a Deadlock XML option to store the deadlock graph in an XML 
document, which you can view later in SQL Management Studio or Proﬁ ler. This option is enabled 
only if you choose the Deadlock Graph event.
You can also use SET SHOWPLAN_XML ON before you execute the query, which can give you an 
estimated execution plan in XML without executing it. You can also use SET STATISTICS XML ON, 
which can give you an execution plan in XML format, as shown in Figure 12-10. Click the link in 
the XML Showplan to open an XML editor within SQL Server Management Studio.
If you want to see the graphical execution plan from this XML document, you can save the 
document with a .sqlplan extension. Open that ﬁ le in SQL Server Management Studio, and you 
get the graphical execution plan. Figure 12-11 shows the graphical execution plan generated from 
the XML document.


350  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
FIGURE 12-12
FIGURE 12-13


352  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
The database IDs on the target ideally should be the same as those on the source. However, 
if they are not the same, matching can be performed based on the database name if it is 
present in the trace, so make sure that you have the DatabaseName data column selected in 
the trace.
The default database on the target server for a login should be the same as on the source 
when the trace was taken.
Replaying events associated with missing or incorrect logins results in replay errors, but the 
replay operation continues.
Distributed Replay
New for SQL Server 2012 is Distributed Replay, a tool for replaying traces from multiple machines. 
While SQL proﬁ ler can replay a trace, it can only do so from a single machine. Distributed replay 
can also replay traces, but can do so from a pool of machines. Because of this, distributed replay 
provides a more scalable solution than SQL proﬁ ler, and is better at simulating mission critical 
workloads.
Now that there are two tools for replaying traces, the question becomes when to use which tool. 
As a general rule, you should use SQL proﬁ ler for all trace replays, and always for replaying traces 
against Analysis Services. You only need to resort to distributed replay if the concurrency in the 
captured trace is so high that a single server cannot sufﬁ ciently simulate the load you want to put 
onto the target server. 
A Distributed Replay system known as a Distributed Replay Utility consists of a number of 
different servers, the Admin tool, the controller, a number of clients, and the target SQL Server.
Because Distributed Replay can replay a trace from multiple servers, you need to do a little 
additional work on the Trace ﬁ le before it can be used in a distributed replay. Speciﬁ cally you have 
to pre-process the trace ﬁ le and spilt it into multiple streams of commands that are replayed from 
the different client servers in the distributed replay utility.
Performance Considerations When Using Trace
SQL Server tracing incurs no overhead unless it captures an event, and most events need few 
resources. Proﬁ ler can become expensive as you add events, and increase the amount of event 
data captured for each event. Normally, you see a maximum of from 10-20 percent overhead. If 
you see more than this, or even if this level of overhead is impacting the production system, either 
reduce the number of events, reduce the amount of data, or use an alternate approach. Most of the 
performance hit results from a longer code path; the actual resources that the trace needs to capture 
event data aren’t particularly CPU-intensive. In addition, to minimize the performance hit, you can 
deﬁ ne all your traces as server-side traces, avoiding the overhead of producing rowsets to send to the 
Proﬁ ler client.
Event Notiﬁ cations
Event notiﬁ cations are special database objects that send messages to the Service Broker service 
(see Chapter 7, “SQL Server CLR Integration,” for details on the Service Broker) with information 
➤
➤
➤



Monitoring Events ❘ 355
 5. 
When a table is created in the StoreEvent database, you get the message in the queue 
NotifyQueue, so create a table and run the following script to see what’s in the queue: 
 SELECT CAST(message_body AS xml)
 FROM NotifyQueue
 6. 
Following is what the ﬁ nal XML message in the queue looks like: 
 <EVENT_INSTANCE>
   <EventType>CREATE_TABLE</EventType>
   <PostTime>2012-09-23T21:53:14.463</PostTime>
   <SPID>56</SPID>
   <ServerName>CIPHER</ServerName>
   <LoginName>REDMOND\ketanp</LoginName>
   <UserName>dbo</UserName>
   <DatabaseName>StoreEvent</DatabaseName>
   <SchemaName>dbo</SchemaName>
   <ObjectName>TestTable1</ObjectName>
   <ObjectType>TABLE</ObjectType>
   <TSQLCommand>
     <SetOptions ANSI_NULLS=“ON” ANSI_NULL_DEFAULT=“ON” ANSI_PADDING=“ON”
 QUOTED_IDENTIFIER=“ON” ENCRYPTED=“FALSE” />
     <CommandText>CREATE TABLE TestTable1 (col1 int, col2 varchar(100), col3
 xml)
 </CommandText>
   </TSQLCommand>
 </EVENT_INSTANCE>
You can take some action with this event if you create a stored procedure and have it activated when 
a message arrives in the queue. You create the serverwide event in the same way. For a full list of the 
events for which you can be notiﬁ ed, you can query the sys.event_notification_event_types 
view. Refer to the script Metadata_EventNotification.sql to get the catalog view list that stores 
the metadata about event notiﬁ cations.
SQL Server Extended Events
SQL Server Extended Events (XEvents) was a completely new feature for SQL Server 2008. 
Extended Events have been enhanced in SQL Server 2012 with increased event coverage and a new 
GUI interface in SSMS. Extended Events provide a deep insight into SQL Server internals and are 
designed to enable faster diagnosis of issues with SQL Server. They provide the capability to act 
either synchronously or asynchronously to SQL Events and are designed to be extremely lightweight 
and highly scalable. The system_health session is a lighter weight and more powerful version of 
the default_trace. Extended events are also lighter weight and more ﬂ exible and scalable than 
SQL Server Trace and SQL Proﬁ ler.
Extended events are designed to replace some of the older monitoring technologies such as SQL 
Server Proﬁ ler. All the events and columns available in SQL Server Proﬁ ler are available through 
Extended Events.

356  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
XEvent Objects
This section introduces the new objects in XEvents. The object hierarchy for XEvent objects is 
shown in Figure 12-14.
FIGURE 12-14
Extended
Events
Module
Package
Event
Keyword
Channel
Action
Target
Module
The Module object is equivalent to the binary that contains the events. Module is equivalent to 
SQLServr.exe, or MyDll.dll if you were to write your own code and load it into SQL Server. The 
only place you see the module is as an attribute of the package in the DMV sys.dm_xe_packages.
Package
A package is a container object within the module. The packages that come with SQL Server can 
be seen in the DMV sys.dm_xe_packages. The following code lists the contents of this DMV, the 
results of which are shown in Table 12-1.
 
 
 
 

Monitoring Events ❘ 357
select name, description
from sys.dm_xe_packages
TABLE 12-1: sys.dm_xe_packages 
NAME
DESCRIPTION
package0
Default package; contains all standard types, maps, compare operators, 
actions, and targets
sqlos
Extended events for SQL operating system
XeDkPkg
Extended events for SQLDK binary
sqlserver
Extended events for Microsoft SQL Server
SecAudit
Security Audit Events
Ucs
Extended events for Uniﬁ ed Communications Stack
Sqlclr
Extended events for SQL CLR
Filestream
Extended events for SQL Server FILESTREAM and FileTable
sqlserver
Extended events for Microsoft SQL Server
As with modules, you won’t be creating any packages unless you write your own code and create 
your own new events.
Event
Events are the ﬁ rst “real” objects in the hierarchy. An event represents an occurrence of a signiﬁ cant 
activity within SQL Server. To get a better understanding of events, take a look at some of the 
events available. You can ﬁ nd these in the DMV sys.dm_xe_objects and they have a type of 
‘event’. The following code outputs a list of event types: 
select name
from sys.dm_xe_objects
where object_type =‘event’
order by name
This returns a list of 618 different events. This is quite an increase from the 254 event types that 
were originally available in SQL Server 2008. A select few are listed here: 
checkpoint_begin
checkpoint_end
lock_acquired
lock_deadlock
lock_released
locks_lock_waits
sp_statement_completed

358  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
sp_statement_starting
sql_statement_completed
sql_statement_starting
wait_info
wait_info_external
All events have two additional attributes: Keyword and Channel. The Keyword for an event is a way 
to group events based on who ﬁ res the event, so keywords are memory, broker, server, and so on. 
The Channel for an event reﬂ ects who might be interested in the event. Following are four channels 
in SQL Server 2012: 
debug
analytical
operational
administration
To see the Channel and Keyword for the events requires that you join several of the XEvent DMVs, 
as in the following code: 
 select p.name as package_name
 , k.event
 , k.keyword
 , c.channel
 , k.description
 from (
 select c.object_package_guid as event_package
 , c.object_name as event
 , v.map_value as keyword
 , o.description
 from sys.dm_xe_object_columns as c inner join sys.dm_xe_map_values as v
   on c.type_name = v.name
   and c.column_value = v.map_key
   and c.type_package_guid = v.object_package_guid
 inner join sys.dm_xe_objects as o
   on o.name = c.object_name
   and o.package_guid = c.object_package_guid
 where c.name = ‘keyword’
 ) as k inner join (
 select c.object_package_guid as event_package
 , c.object_name as event
 , v.map_value as channel
 , o.description
 from sys.dm_xe_object_columns as c inner join sys.dm_xe_map_values as v
   on c.type_name = v.name
   and c.column_value = v.map_key
   and c.type_package_guid = v.object_package_guid
 inner join sys.dm_xe_objects as o
   on o.name = c.object_name
   and o.package_guid = c.object_package_guid
 where c.name = ‘channel’
 ) as c
➤
➤
➤
➤

Monitoring Events ❘ 359
 on
 k.event_package = c.event_package and k.event = c.event
 inner join sys.dm_xe_packages as p on p.guid = k.event_package
 order by keyword
 , channel
 , event
Table 12-2 shows a few of the events, including their keywords and channels. 
TABLE 12-2: Select Extended Events
PACKAGE NAME
EVENT
KEYWORD
CHANNEL
DESCRIPTION
sqlserver
broker_activation_
task_aborted
broker
Admin
Broker 
activation task 
aborted
sqlserver
broker_activation_
task_started
broker
Analytic
Broker 
activation task 
started
sqlserver
change_tracking_
cleanup
change_
tracking
Debug
Change Tracking 
Cleanup
sqlserver
app_domain_ring_
buffer_recorded
clr
Debug
AppDomain ring 
buf er recorded
sqlserver
cursor_manager_
cursor_end
cursor
Analytic
Cursor manager 
cursor end
sqlserver
checkpoint_begin
database
Analytic
Checkpoint has 
begun
sqlserver
database_started
database
Operational
Database 
started
sqlserver
deadlock_monitor_
state_transition
deadlock_
monitor
Debug
Deadlock 
Monitor state 
transition
sqlserver
error_reported
errors
Admin
Error has been 
reported
sqlserver
trace_print
errors
Debug
Trace message 
published
sqlserver
assert_fired
exception
Debug
Assert ﬁ red
sqlos
dump_exception_
routine_executed
exception
Debug
Dump exception 
routine executed
continues

360  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
PACKAGE NAME
EVENT
KEYWORD
CHANNEL
DESCRIPTION
sqlserver
sql_statement_
starting
execution
Analytic
SQL statement 
starting
sqlserver
databases_log_
file_size_changed
io
Analytic
Database log ﬁ le 
size changed
sqlserver
file_read
io
Analytic
File read
sqlserver
flush_file_buffers
io
Debug
FlushFileBuf ers 
called
Action
Actions are what you want to happen when an event ﬁ res. They are invoked synchronously on 
the thread that ﬁ red the event. The available actions are stored in the DMV sys.dm_xe_objects 
with an object_type = ‘action’. The action enables you to do things such as correlate a 
plan_handle, and T-SQL stack with a speciﬁ c event. This kind of ﬂ exibility creates an incredibly 
powerful framework that exceeds anything that SQL Trace and SQL Proﬁ ler could do.
The following query returns all the actions available.
 select name
 from sys.dm_xe_objects
 where object_type = ‘action’
 order by name
This query returns 50 actions, some of which are listed here: 
attach_activity_id
attach_activity_id_xfer
callstack
collect_cpu_cycle_time
collect_system_time
create_dump_all_thread
create_dump_single_thread
database_context
database_id
debug_break
plan_handle
session_id
sos_context
sql_text
transaction_id
tsql_stack
Predicate
A predicate is a ﬁ lter that is applied to the event right before the event is published. A Boolean 
expression, it can be either local or global and can store state.
TABLE 12-2 (continued)

Monitoring Events ❘ 361
Predicates are stored in the DMV sys.dm_xe_objects and can be seen using the following T-SQL: 
 select name, description
 from sys.dm_xe_objects
 where object_type = ‘pred_compare’
 order by name
 -- 77 rows
 
 select name, description
 from sys.dm_xe_objects
 where object_type = ‘pred_source’
 order by name
 -- 44rows
Table 12-3 shows a few of the pred_compare objects.
TABLE 12-3: Selected pred_compare objects 
NAME
DESCRIPTION
divides_by_uint64
Whether a uint64 divides another with no remainder
equal_ansi_string
Equality operator between two ANSI string values
greater_than_equal_float64
Greater than or equal operator between two 64-bit 
double values
greater_than_i_sql_ansi_string
Greater than operator between two SQL ANSI string 
values
less_than_ansi_string
Less than operator between two ANSI string values
less_than_equal_i_unicode_
string_ptr
Less than or equal operator between two UNICODE 
string pointer values
less_than_int64
Less than operator between two 64-bit signed int values
not_equal_ptr
Inequality operator between two generic pointer values
Table 12-4 lists some of the pred_source objects.
TABLE 12-4: Selected pred_source objects 
NAME
DESCRIPTION
Counter
Counts the number of times evaluated.
cpu_id
Gets the current CPU ID.
current_thread_id
Gets the current Windows thread ID.
database_id
Gets the current database ID.
node_affinity
Gets the current NUMA node ai  nity.
continues

362  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
NAME
DESCRIPTION
partitioned_counter
Per-CPU partitioned counter. The value is aggregated and 
approximate.
scheduler_address
Gets the current scheduler address.
scheduler_id
Gets the current scheduler ID.
session_id
Gets the current session ID.
system_thread_id
Gets the current system thread ID.
task_address
Gets the current task address.
task_elapsed_quantum
Gets the time elapsed since quantum started.
task_execution_time
Gets the current task execution time.
transaction_id
Gets the current transaction ID.
worker_address
Gets the current worker address.
Target
A target is a way to deﬁ ne what you want to happen to the events you monitor. The ﬁ fteen targets 
deﬁ ned for SQL Server 2012 are shown in the following table. Like the other XEvent objects, they 
can be found in the DMV sys.dm_xe_objects with an object_ type = ‘target’.
The following T-SQL code returns the list of targets from sys.dm_xe_objects: 
 select name, description
 from sys.dm_xe_objects
 where object_type = ‘target’
 order by name
The ﬁ fteen different targets can be seen in Table 12-5.
TABLE 12-5: Targets
NAME
DESCRIPTION
asynchronous_router
Route events to asynchronous listeners.
asynchronous_security_audit_event
_log _target
Asynchronous security audit NT event log target.
asynchronous_security_audit_file 
_target
Asynchronous security audit ﬁ le target.
asynchronous_security_audit
_security _log_target
Asynchronous security audit NT security log target.
TABLE 12-4 (continued)

Monitoring Events ❘ 363
NAME
DESCRIPTION
etw_classic_sync_target
Event Tracing for Windows (ETW) Synchronous Target.
Event_counter
Counts the number of occurrences of each event in 
the event session.
Event_file
Saves the event data to an XEL ﬁ le, which can be 
archived and used for later analysis and review. You 
can merge multiple XEL ﬁ les to view the combined 
data from separate event sessions.
Event_stream
Asynchronous live stream target.
histogram
Aggregates event data based on a speciﬁ c event 
data ﬁ eld or action associated with the event. The 
histogram enables you to analyze distribution of 
the event data over the period of the event session.
pair_matching
Pairing target.
ring_buffer
Asynchronous ring buf er target.
router
Route events to listeners.
synchronous_security_audit_event
_log _target
Synchronous security audit NT event log target.
synchronous_security_audit_file 
_target
Synchronous security audit ﬁ le target.
synchronous_security_audit
_security _log_target
Synchronous security audit NT security log target.
Event Session
The event session is where all the objects detailed earlier are brought together to actually do 
something. You create the event session to deﬁ ne which of those objects you want to use to perform 
your event capture.
Event sessions are created using the DDL CREATE EVENT SESSION syntax. This one statement 
enables you to deﬁ ne all the objects you need to create a new event session. The only thing you 
cannot do is start the session. For transaction consistency, the session must be created ﬁ rst. When it 
has been created, it can be started using the ALTER EVENT SESSION syntax: 
 ALTER EVENT SESSION <session name>  STATE = START
Listing 12-4 shows an example of code to create a new event session that gathers sql_text, and the 
tsql_stack for any SQL statement that has a duration > 30 ms. It then writes the output to the xml 
ﬁ le speciﬁ ed in the target speciﬁ cation, and ﬂ ushes results from memory to the ﬁ le every second.


Monitoring Events ❘ 365
To see which sessions are currently active, use the following queries: 
 select 
* from sys.dm_xe_sessions
 
 select 
* from sys.dm_xe_session_events
Catalog Views
Following are several of the catalog views that expose information about XEvents: 
server_event_sessions
server_event_session_targets
server_event_session_fields
server_event_session_events
server_event_session_actions
DMVs
You have already seen some of the Extended Event DMVs in action. For completeness, here is the full list: 
sys.dm_xe_map_values: Returns a mapping of internal numeric keys to human-readable text.
sys.dm_xe_object_columns: Returns the schema information for all the objects.
sys.dm_xe_objects: Returns a row for each object exposed by an event package. Objects 
can be one of the following: 
Events: Indicate points of interest in an execution path. All events contain 
information about a point of interest.
Actions: Run synchronously when events ﬁ re. An action can append run-time data 
to an event.
Targets: Consume events, either synchronously on the thread that ﬁ res the event or 
asynchronously on a system-provided thread.
Predicate sources: Retrieve values from event sources for use in comparison operations. 
Predicate comparisons compare speciﬁ c data types and return a Boolean value.
Types: Encapsulate the length and characteristics of the byte collection, which is 
required to interpret the data.
sys.dm_xe_packages: Lists all the packages registered with the extended events engine.
sys.dm_xe_session_event_actions: Returns information about event session actions. 
Actions are executed when events are ﬁ red. This management view aggregates statistics 
about the number of times an action has run and the total run time of the action.
sys.dm_xe_session_events: Returns information about session events. Events are discrete 
execution points. Predicates can be applied to events to stop them from ﬁ ring if the event 
does not contain the required information.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

366  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
sys.dm_xe_session_object_columns: Shows the conﬁ guration values for objects that are 
bound to a session.
sys.dm_xe_session_targets: Returns information about session targets.
sys.dm_xe_sessions: Returns information about an active extended events session. This 
session is a collection of events, actions, and targets.
Working with Extended Event Sessions
There are several ways that you can create, modify, display, and analyze sessions and session data.
You can manipulate extended events using DDL in T-SQL as you saw in some of the previous examples. 
There are additional T-SQL examples on creating Extended Event Sessions later in this section.
There are two graphical user interfaces that you can use with Extended Events: The New Session 
Wizard and the New Session UI.
New Session Wizard
The new session wizard guides you through the creation of a new session with the following steps:
 1. 
Launch it from SQL Server Management Studio. Open the Management node, then the 
Extended Events Node, and then the Session Node. Right click on Sessions, and choose 
New Session Wizard. This launches the New Session Wizard and displays the Introduction 
page as shown in Figure 12-15. 
➤
➤
➤
FIGURE 12-15

Monitoring Events ❘ 367
 2. 
Select Next to move onto the set session properties page. Here you provide a session name, 
and select if you want the session to start up each time the server starts (see Figure 12-16). 
For this example, enter the name chapter_12_test.
FIGURE 12-16
 3. 
Select Next to move onto the Choose template page. Here you choose a predeﬁ ned template 
for the events in the template, or you can select to not use a template, and manually select 
events. In this example, select “Use this event session template” which populates the list 
of event session templates shown in Figure 12-17. For this example select the “Query Wait 
Statistics” template and select Next.
 4. 
The Next page in the Wizard is the Select Events To Capture page (see Figure 12-18). 
Because you chose to use a template, this is already populated with the events from the 
template. You can see the event selected in the template in the selected events box. Because 
you used a template, you don’t need to do anything here.

368  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
FIGURE 12-17
FIGURE 12-18

Monitoring Events ❘ 369
 5. 
Select Next to move onto the Capture Global Fields page. This page is shown in 
Figure 12-19. Again, because you selected a template, a number of global ﬁ elds are already 
preselected. The preselected ﬁ elds are those with a checkbox next to them. In Figure 12-19 
you can see client_app_name, and database_id are checked. Scrolling down shows the other 
ﬁ elds that are selected from the template.
FIGURE 12-19
 6. 
Select Next to move onto the Set Session Event Filters page (see Figure 12-20). Here you 
can select any ﬁ lters (also known as predicates) that would restrict the amount of data to be 
captured. For this example you are not going to apply any ﬁ lters.
 7. 
Select Next to move onto the Specify Session Data Storage page (see Figure 12-21). Here you 
can specify where you want the data to be collected. The two options are to save data to a 
ﬁ le for alter analysis, or to put it into a ring buffer. For this example, select Save data to a 
ﬁ le for later analysis (event_ﬁ le target), and leave the default values for ﬁ lename, max ﬁ le 
size, enable ﬁ le rollover, and max number of ﬁ le.

370  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
FIGURE 12-20
FIGURE 12-21

Monitoring Events ❘ 371
 8. 
Select Next to move onto the Summary page. This page shown in Figure 12-22 provides a 
summary of the selections made throughout the wizard. This provides one last opportunity 
to conﬁ rm the values selected before the wizard applies these settings and creates the new 
event session.
FIGURE 12-22
 9. 
Select Finish and the wizard creates the new event session. If it creates the event session 
successfully, you see the success page and have the option to start the event session 
immediately and watch live data as it is captured. The Create Event Session success screen is 
shown in Figure 12-23. Select both options: start the event session immediately, and watch 
live data on screen as shown in Figure 12-23.

372  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
 10. 
Select Close to close the wizard, start the event session, and watch live data being 
captured. The wizard closes, and SSMS displays a new tab showing the Live Data for the 
new session.
New Session UI
The New Session UI is launched from SQL Server Management Studio. To start using this interface, 
perform the following steps: 
 1. 
Open the Management node, then the Extended Events Node, and then the Session Node. 
Right click on Sessions, and choose New Session. This opens the New Session UI on 
the General Page. Enter a session name of chapter_12_test2, and select the Connection 
Tracking template. Select to start the event session at server startup. You see a page similar 
to that shown in Figure 12-24.
 2. 
Select the Events Page to see which events have been pre selected with this template (see 
Figure 12-25). Because you selected a template, there is no need to change anything here.
FIGURE 12-23

Monitoring Events ❘ 373
FIGURE 12-24
FIGURE 12-25
 3. 
Select the Data Storage page to deﬁ ne how the data is going to be stored. The default here 
is to store data into a ring_buffer target. To change this, select Add which adds a new line 
to the list of targets, with a drop down for “Please choose a target type.” Expand this drop 
down and you see the full set of targets, as shown in Figure 12-26.

374  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
FIGURE 12-26
 4. 
Select event_ﬁ le. This adds a set of target speciﬁ c properties below the list of targets. For 
this example, the default values are acceptable, so there is no need to change them.
 5. 
Select the Advanced page to specify advanced settings for the New Session. The Advanced 
page is shown in Figure 12-27. These settings are acceptable for this example.
FIGURE 12-27

Monitoring Events ❘ 375
 6. 
At this point you have examined all the options for creating the new session. To create 
the new session, select OK. The UI disappears, and if you go look in SSMS under 
Management Á Extended Events Á Sessions, you see that a new session called chapter_12_
test2 has been created. Because of the options you selected, it is not currently running. 
 7. 
To start the new session, right click it, and select Start Session. To view live data for the 
session, right click it and select Watch Live Data.
Editing a Session
To edit a session, select it in SSMS right click its node, and select Properties. This brings up the 
same set of pages seen in the New Session UI, but this time preloaded with the session info, and 
with some options disabled. Select properties on the chapter_12_test2 session to display the dialog 
seen in Figure 12-28. 
FIGURE 12-28
Using this dialog, you can edit many of the session properties. There are additional options 
throughout the session UI that have not been discussed here. Most of these options are 
self explanatory, but it is recommended to explore these in conjunction with the available 
documentation in Books Online.

376  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
MONITORING WITH DYNAMIC MANAGEMENT 
VIEWS AND FUNCTIONS
Dynamic management views (DMVs) and dynamic management functions (DMFs) are a godsend to 
the DBA. They provide plenty of information about server and database state. DMVs, and DMF’s 
are designed to give you a window into what’s going on inside SQL Server. They return server state 
information that you can use to monitor the health of a server instance, diagnose problems, and 
tune performance. Following are two types of DMVs and DMFs: 
Server-scoped dynamic management views and functions
Database-scoped dynamic management views and functions
All DMVs and functions exist in the sys schema and follow the naming convention dm_* 
respectively. To view the information from a server-scoped DMV, you have to grant the SERVER 
VIEW STATE permission to the user. For database-scoped DMVs and functions, you have to grant 
the VIEW DATABASE STATE permission to the user. After you grant the VIEW STATE permission, 
that user can see all the views; to restrict the user, deny the SELECT permission on the dynamic 
management views or functions that you do not want the user to access. The following example 
grants the VIEW SERVER STATE permission to the user Aish: 
 GRANT VIEW SERVER STATE TO [MyDom\Aish]
If you want the user [MyDom\Aish] to be restricted from viewing information in the view 
sys.dm_os_wait_stats, you need to DENY SELECT as follows: 
 DENY SELECT ON sys.dm_os_wait_stats TO [MyDom\Aish]
DMVs and DMFs are generally divided into the following categories: 
Always On Availability Group
Change Data Capture–related
Change Tracking–related
CLR-related
Database mirroring–related 
Database-related 
Execution-related 
Filestream and FileTable
Full-Text-Search and Semantic Search
Index-related 
I/O-related 
Object related
Query notiﬁ cations related
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤










386  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
LISTING 12-19 (continued)
    INNER JOIN 
    (
        SELECT object_name(object_id) AS name 
            ,index_id ,allocation_unit_id
        FROM sys.allocation_units AS au
            INNER JOIN sys.partitions AS p 
                ON au.container_id = p.hobt_id 
                    AND (au.type = 1 OR au.type = 3)
        UNION ALL
        SELECT object_name(object_id) AS name   
            ,index_id, allocation_unit_id
        FROM sys.allocation_units AS au
            INNER JOIN sys.partitions AS p 
                ON au.container_id = p.partition_id 
                    AND au.type = 2
    ) AS obj 
        ON bd.allocation_unit_id = obj.allocation_unit_id
WHERE database_id = db_id()
GROUP BY name, index_id 
ORDER BY cached_pages_count DESC;
MONITORING LOGS
Another aspect of monitoring that is frequently overlooked is monitoring the various log ﬁ les 
available. SQL Server writes its own error log, and then there are the Windows Event logs, and you 
may ﬁ nd events logged in the Application, Security, or System Event logs.
Traditionally, the SQL Server and Windows Event logs have been viewed through separate 
applications: Windows Logs through the Windows Event Viewer, and SQL Logs through a text 
editor. The SQL Server Management Studio Log File viewer enables you to combine both sets of 
logs into a combined view. There are root level nodes for SQL Server, SQL Server Agent, Database 
Mail, and Windows NT that enable you to do this.
Monitoring the SQL Server Error Log
The SQL Server Error log is the location where SQL Server writes all its error information, and also 
a lot of additional informational messages about how it is working and what it is doing.
The error log is a text ﬁ le written to the C:\Program Files\Microsoft SQL Server\MSSQL11
.MSSQLSERVER\MSSQL\Log folder. A new log ﬁ le is opened each time the SQL Server process starts. 
SQL Server keeps seven log ﬁ les: the current one is called simply errorlog, and the oldest one is 
called errorlog.6.
The error log contains a lot of useful information. It is deﬁ nitely the place to go looking for 
deadlock information after the relevant deadlock trace ﬂ ags have been enabled.

Management Data Warehouse ❘ 387
Anytime a signiﬁ cant issue occurs, the ﬁ rst place to search for additional information should be the 
SQL Server Error Log. Additionally, both Event Notiﬁ cations and Extended Events can be used if 
additional data is required to help troubleshoot a particular issue. 
Monitoring the Windows Event Logs
Three Windows event logs may hold entries of relevance to a SQL Server event: 
Application event log
Security event log
System event log
These event logs contain additional event information about the server environment, other processes/
applications operating on the server, and also additional information about the SQL Server process 
that may not be logged into the SQL Server Error log. These logs should be another place that you 
go to look for additional information about any issues that arise with SQL Server.
MANAGEMENT DATA WAREHOUSE
New to SQL Server 2008 was the Management Data Warehouse (MDW) and Data Collection. This 
was a new framework for data collection, storage, and reporting.
SQL Server 2008 R2 added the SQL Utility, the Utility Control Point (UCP), and the Utility 
Management Data Warehouse (UMDW) as a new location for data storage. The SQL Utility is a 
container for “managed instances” of SQL Server. The UMDW is a destination for data collection. 
The UCP data collector sets differ from the MDW in the granularity of data being collected. The 
MDW data collector is focused on troubleshooting individual queries. The UCP data collector set 
focuses on higher level system resource usage and is there to help the DBA determine when a SQL 
instance is under- or over-utilized. The UCP also provides additional reporting capabilities that were 
not available in SQL Server 2008 RTM.
For SQL Server 2012, you get the basics of Data Collection, a few Data Collection sets, the 
Management Data Warehouse, and some reports.
Following is a list of the basic concepts: 
Data provider: A data provider is a source of data to be captured. SQL Server 2012 has four 
data providers: 
SQL Trace
Performance Monitor Counters
T-SQL
Query Activity
Collection item: A collection item is a speciﬁ c item of data to be collected. This might be a 
single performance monitor counter, a SQL Trace event, or a T-SQL query of a DMV.
➤
➤
➤
➤
➤
➤
➤
➤
➤

388  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Collection set: A collection set is a logical grouping of collection items that are collected 
together. This might be all the performance counters monitoring disk I/O, or all the SQL 
trace events to look for long-running queries.
Management Data Warehouse (MDW): The Management Data Warehouse is where the 
items in each collection set are stored. It is the repository of historical data that you have 
collected. This can be either an MDW, or a UMDW. MDW data collections can be written 
to a UMDW, but UMDW data collections cannot be written to an MDW. 
Target servers: The target servers are the systems that you want to monitor. Ideally, the MDW 
should be on a separate server. If it’s on one of the target servers, you run the risk of recording 
activity about the data collection, rather than the target server you are actually interested in.
Data collection process: Data collection is performed by a series of SQL Agent jobs running 
SSIS packages that perform the data collection. Data is then captured based on the schedule 
deﬁ ned for each collection set. It is then cached and written only to the MDW when the 
current buffer is full. This helps optimize I/O to the MDW.
You should expect to consume between 200–400MB/day for each server being monitored. These 
ﬁ gures come from using the basic set of data collection sets, with 200MB per day for an idle server, 
and 400MB per day for a busy server.
System Data Collection Sets
Three system data collection sets ship with SQL Server 2012: 
Disk Usage: The Disk Usage system collector set collects disk usage performance counters. 
It is helpful for monitoring disk usage. The collected data is cached and then uploaded to 
the warehouse every 6 hours, where it is retained for 90 days.
Query Activity: The Query Activity system collection set captures query activity on the 
target server. It collects data from the server every 15 minutes and helps you identify 
the most interesting queries running on a server without having to run a Proﬁ ler trace. It 
captures the top three queries from several different resource usage categories.
Server Activity: The Server Activity system collection set collects a set of performance counters. 
Wait Statistics, Scheduler, Performance Counters, and Memory Counters are collected every 60 
seconds. The active sessions and requests are collected every 10 seconds. The data is uploaded 
to the warehouse every 5 minutes and is deleted from the warehouse after 14 days.
Viewing Data Collected by the System Data Collection Sets
Along with the system data collection sets is a set of reports that displays the history collected in the 
Management Data Warehouse for each of these data collection sets. To access these reports in SQL 
Server Management Studio, follow these steps: 
 1. 
Open Object Explorer and select Management Á Data Collection Á System Data Collection 
Sets. Under the System Data Collection Sets node, you see the three system data collection 
sets listed. 
➤
➤
➤
➤
➤
➤
➤

Management Data Warehouse ❘ 389
 2. 
To see the reports, right-click on a data 
collection set node (for example, Disk 
Usage) and select the reports item from 
the menu.
 3. 
Then select Reports Á Historical Á 
Disk Usage Summary. Figure 12-29 
shows this navigation path. The Disk 
Usage report displays, showing the 
history of disk usage data stored in the 
Management Data Warehouse. It look 
something like the report shown in 
Figure 12-30.
 4. 
Click a database name to see the 
detailed report for that database, as 
shown in Figure 12-31.
FIGURE 12-29
FIGURE 12-30
FIGURE 12-31

390  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Creating Your Own Data Collection Set
After seeing the system data collection sets, the next step is to set up your own custom data 
collection sets. Currently, there is no wizard or easy user interface to handle this process, so you 
need to write T-SQL to execute the steps required. Fortunately, there are only a couple of simple 
steps. The hardest part is determining what data you want to collect, where it needs to come from, 
and what schedule you want to capture the data on.
For this example you create a custom collection set to execute a T-SQL query that queries the max 
ID from an example table that has a high insertion rate. This information enables you to report on 
insertion rates over a period of time.
You ﬁ rst need to create the sample table, which can live in either an existing database or a new database 
that you create. In this example, the T-SQL to create a 50MB database in the default location is included 
with the table-creation code. The steps needed to complete this are described along with the code. 
 1. 
Create the sample database and table: 
 CREATE DATABASE [ch12_samples] ON  PRIMARY
 ( NAME = N‘ch12_samples’
 , FILENAME = N’C:\Program Files\Microsoft SQL
  Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\ch12_samples.mdf’
 , SIZE = 51200KB
 , MAXSIZE = UNLIMITED
 , FILEGROWTH = 1024KB )
  LOG ON
 ( NAME = N‘ch12_samples_log’
 , FILENAME = N’C:\Program Files\Microsoft SQL
 Server\MSSQL11.MSSQLSERVER\MSSQL\DATA\ch12_samples_log.ldf’
 , SIZE = 10240KB
 , MAXSIZE = 2048GB
 , FILEGROWTH = 10%)
 GO
 
 create table Sales (
 ID int identity (1,1) not null,
 sku int not null,
 quantity int not null
 )
 go
 
 -- insert some sales
 insert sales (sku, quantity) values (1,1)
 2. 
Create the collection set to get the max (id)every hour, and keep this for 45 days in the 
Management Data Warehouse: 
-- Create the collection set
 -- Make sure this runs in msdb as that’s where the DC SPs live.
 use msdb
 GO
 
 -- Find the uid for the schedule you want to use which is every 60 minutes
 declare @schedule_uid uniqueidentifier

Management Data Warehouse ❘ 391
 select @schedule_uid = (select schedule_uid
   from sysschedules_localserver_view
     where name=N‘CollectorSchedule_Every_60min’)
 
 -- Create a new custom collection set
 declare @collection_set_id int
 exec dbo.sp_syscollector_create_collection_set
     @name = N‘Sample insertion rate’,
     @schedule_uid = @schedule_uid,  -- 60 minutes
     @collection_mode = 1, -- Set collection mode to non cached,
 ie collection and upload are on the same schedule
     @days_until_expiration = 45, -- Keep data for 45 days
     @description = N’Sample max(id) so we can
 determine hourly insertion rates’,
     @collection_set_id = @collection_set_id output
 
 select @collection_set_id as collection_set_id
    
 declare @paramters xml
 declare @collection_item_id int
 declare @collection_type_uid uniqueidentifier
 
 -- Create the XML parameters for the collection item
 select @paramters = convert(xml,
     N’<TSQLQueryCollector>
         <Query>
           <Value>select max(id) as max_id from sales</Value>
           <OutputTable>max_sales_id</OutputTable>
         </Query>
         <Databases>
             <Database>Ch13_samples</Database>
         </Databases>
       </TSQLQueryCollector>‘)
 
 -- Find the Collector type you want to use which is TSQL
 select @collection_type_uid  = collector_type_uid
 from syscollector_collector_types
 where name = ‘Generic T-SQL Query Collector Type’
 
 -- Create the new collection item
 exec dbo.sp_syscollector_create_collection_item
     @collection_set_id = @collection_set_id,
     @collector_type_uid = @Collection_type_uid,
     @name = ‘Sales max ID’,
     @frequency = 60,
     @parameters = @paramters,
     @collection_item_id = @collection_item_id output;
 
 -- report the ID that just got created
 select @collection_item_id as collection_item_id
 
 -- start the collection set
 exec dbo.sp_syscollector_start_collection_set
     @Collection_set_id = @collection_set_id

392  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Now you have created a new custom snapshot 
that contains the max ID from the sales table, 
sampled over time. This enables you to report 
on the growth of records in the sales table.
Examining the Data You 
Collected
The data collected is stored in the 
Management Data Warehouse. From the 
preceding example, there is now a new 
custom snapshot table created, called 
custom_snapshots.max_sales_id, as 
shown in Figure 12-32.
The table you created has some additional 
columns, not deﬁ ned in the data you selected 
for the snapshot. These are database_name, 
collection_time, and snapshot_id. In 
addition, the collection time is stored in a 
datimeoffset column. This was a new data 
type in SQL Server 2008 and when queried, 
returns the date/time as a UTC time.
Following is the T-SQL to retrieve the data stored by the collection set you just created: 
 -- Find the data you recorded
 -- Switch to the MDW
 use mgmt_dw
 go
 
 -- Query the custom snapshot
 select 
* from custom_snapshots.max_sales_id
Table 12-6 shows the results of this query after the collector has been running for a few minutes 
with no inserts to the table. 
TABLE 12-6: Custom Snapshots
MAX_ID
DATABASE_NAME
COLLECTION_TIME
SNAPSHOT_ID
1
Ch12_samples
9/23/2012 1:59:44 AM +00:00
93
1
Ch12_samples
9/23/2012 2:00:21 AM +00:00
97
To fully leverage the data stored in the Management Data Warehouse, consider creating SQL Server 
Reporting Services reports to display the data.
FIGURE 12-32

SQL Server Standard Reports ❘ 393
SQL SERVER STANDARD REPORTS
One of the best kept secrets in SQL Server is the standard reports that started shipping with the SQL 
Server 2005 Performance dashboard reports. Since then, each edition of SQL Server has added to 
the standard reporting capabilities, until today whence a comprehensive set of reports that provide a 
great deal of detailed information on what’s going on inside SQL Server has come about.
The standard reports are accessible through SQL Server Management Studio. Starting with the 
Server Node in SSMS, right-click the server node, then Reports, and then Standard Reports to see 
the list of reports for the SQL Server Instance. The list of standard reports for the server node is 
shown in Figure 12-33.
FIGURE 12-33
As you navigate through the various nodes in the Object Explorer, different reports are available 
at different key nodes. In some cases, no standard reports exist and just a custom report node 

394  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
Unfortunately, these are not documented anywhere, so you have to ﬁ nd your own way around 
the various nodes in the Object Explorer by clicking nodes and looking in the Reports menu to 
see where there are any Standard Reports. Figure 12-35 shows one more location where there are 
standard reports, which is under Security Á Logins.
FIGURE 12-34
that’s empty can be found. In other cases, such as when you select a speciﬁ c database, a long list of 
standard reports is available. Figure 12-34 shows the standard reports for a database.

System Center Management Pack ❘ 395
SYSTEM CENTER MANAGEMENT PACK
All the monitoring discussed so far has been interactive in nature and has been based around 
activities that a single DBA executes against a small number of SQL Servers.
As DBAs must cover more and more databases, they need to change from an interactive monitoring 
mode to an exception-driven monitoring model. This is where the System Center suite, and speciﬁ cally 
System Center Operations Manager comes in. System Center Operations Manager (SCOM) provides 
a product that delivers exception and performance monitoring that can gather a broad set of 
performance, health, and exception data from a large number of servers. SCOM then consolidates the 
data and presents a high-level rollup of your entire datacenter health.
FIGURE 12-35

396  ❘  CHAPTER 12  MONITORING YOUR SQL SERVER
This approach lets a relatively small team manage or operate a large number of servers, knowing 
that any time something needs to happen, an alert or exception will be raised that lets them react to 
the relevant activity.
SQL Server 2012 has a new management pack that integrates both with the current shipping 
version of System Center Operations Manager 2007, and also with the next version, System Center 
Operations Manager 2012.
SCOM 2012 provides some considerable enhancements to SCOM functionality that make the 
upgrade worthwhile. These are primarily the new dashboards that provide a consolidated view of 
SQL Server health.
SQL SERVER BEST PRACTICE ANALYZER
The SQL Server Best Practice Analyzer is a tool introduced around the SQL Server 2005 time frame to 
help you know if your SQL Server instance meets currently accepted best practices. It was originally a 
stand-alone tool developed by the SQL Server Release Services team but has since evolved into a set of 
rules implemented in the Microsoft Baseline Conﬁ guration Analyzer (MBCA) framework.
SQL Server Best Practice Analyzer (BPA) is a stand-alone tool installed to each instance on which 
you want to run it. Once you install it, you then run the BPA and point it at a given SQL instance, 
provide credentials for it to connect, and it scans the SQL Instance and compares its conﬁ guration 
with the set of rules included in that version of the SQL BPA.
Any exceptions to the rules are reported in the output, and each exception will include some basic 
text, and a link to an online resource providing more information about the rule that was infringed.
Something important to remember about SQL BPA is that these are general purpose best practices, 
and in some cases it’s perfectly acceptable that your SQL Server instance doesn’t meet the relevant 
best practice. However every exception should be considered, and you should ensure that you 
understand why your instance doesn’t meet the relevant rule.
Something else to consider is that just because your instance doesn’t ﬁ re any exceptions does not 
mean that everything is ﬁ ne. There are plenty of speciﬁ c best practices that are not incorporated 
into the SQL BPA, so a clean report doesn’t necessarily mean you’re optimally conﬁ gured. However 
it is a great place to start.
SYSTEM CENTER ADVISOR
System Center Advisor (SCA) is the natural evolution of SQL Server Best Practice Analyzer. One 
of the challenges with SQL BPA is that the set of rules are encoded into each version for BPA that’s 
released, and knowledge about best practices can change more quickly than new versions can be 
released. SQL BPA is also a tool that needs to be manually executed on each server.
System Center Advisor is the result of a lot of effort by folks who work with SQL Server and the 
SQL teams within PSS to deliver a more effective tool for validating SQL Server conﬁ gurations.

Summary ❘ 397
System Center Advisor is a cloud-based conﬁ guration monitoring tool that can continuously 
monitor a large number of servers and provide online analysis of the results. One of the beneﬁ ts of 
being cloud-based is that new best practices can be incorporated into the validation checks quickly, 
and with no activity on you, the end user’s part. The System Center Advisor team can introduce a 
new rule with minimal effort, and every server being monitored by SCA can immediately gain the 
beneﬁ t of being checked against the new rule.
SUMMARY
Monitoring SQL Server regularly and gathering performance data is key to helping identify 
performance problems. Increasingly, today’s DBAs need to cover more systems than ever before and 
must spread their net widely. The tools and techniques introduced in this chapter help the DBAs do 
that to move from a hands-on approach to an event-driven approach. 
Performance Monitor enables the DBA to monitor resource usage for a server, and helps 
troubleshoot performance issues with server and SQL resource usage.
The SQL Server Dynamic Management Views and Functions provide a deep insight into what’s 
going on inside SQL Server, and the samples provided help illustrate how to use some of the DMVs 
and DMFs to troubleshoot speciﬁ c issues.
The SQL Trace architecture with SQL Proﬁ ler, SQL Trace, and now Distributed Replay provide 
tools for capturing, analyzing, and replaying SQL Server events.
Event Notiﬁ cations provides a framework to execute actions outside SQL Server asynchronously in 
response to events occurring inside the server.
Extended Events provides a scalable framework for capturing data when speciﬁ c events occur 
within SQL Server. This is a powerful framework that enables complex data collection to assist with 
troubleshooting SQL Server issues.
Data Collection sets, the Management Data Warehouse, and Utility MDW are a mechanism to 
collect performance related data and store it in a data warehouse. The Data Collector framework is 
a powerful way to store large amounts of performance data in a data warehouse for analysis should a 
performance issue occur, or just for trend analysis to see how load and performance is changing over 
time.
The System Center Operations Manager and the SQL Server 2012 Management Pack provide a 
central management interface for a data center operations team to monitor the health of hundreds of 
SQL Servers, and react to critical events when they occur.
SQL Server Best Practice Analyzer tells you when your SQL Server instance is in compliance with 
the established Best Practices.
System Center Advisor provides a cloud based service for best practices and for patch and update 
checking to help you keep your SQL Servers current with the very latest knowledge around SQL 
Server Best Practices.
In the next chapter you learn how to performance tune T-SQL. 



concepts of forced autoparam and simple autoparam are described later in the chapter.) If a match 
is found, the plan is sent to the algebrizer (explained later in this section), which creates a logical, or 
parse, tree for input to the optimizer (also explained later in this section). The logical tree or parse 
tree is generated by the process that checks whether the T-SQL is written correctly. The parse tree 
represents the logical steps necessary to execute the query. The plan is then cached in the plan cache. 
Of course, not all the plans are cached; for example, when you create a stored procedure with WITH 
RECOMPILE, the plan is not cached.
Recompilation
Sometimes a cached plan needs to be recompiled because it is not valid for some reason. Suppose 
that a batch has been compiled into a collection of one or more query plans. SQL Server 2012 
checks for validity (correctness) and optimality of that query plan before it begins executing any 
of the individual query plans. If one of the checks fails, the statement corresponding to the query 
plan or the entire batch is compiled again, and a possibly different query plan is produced. Such 
compilations are known as recompilations.
From SQL Server 2005 onward, if a statement in a batch causes the recompilation, then only 
that statement recompiles, not the whole batch. This statement-level recompilation has some 
advantages. In particular, it results in less CPU time and memory use during batch recompilations 
and obtains fewer compile locks. Prior to this change, if you had a long stored procedure, it was 
sometimes necessary to break it into small chunks just to reduce compile time.  
The reasons for recompilation can be broadly classiﬁ ed in two categories: 
Correctness
Plan optimality
Correctness
If the query processor decides that the cache plan would produce incorrect results, it recompiles that 
statement or batch. There are multiple reasons why the plan would produce incorrect results — 
changes to the table or view referenced by the query, changes to the indexes used by the execution 
plan, updates to the statistics, an explicit call to sp_recompile, or executing a stored procedure 
using the WITH RECOMPILE option. The following sections investigate how the changes that 
can occur to the schemas of objects cause recompilation as well as how SET options can cause 
recompilation.
Schemas of Objects
Your query batch could be referencing many objects such as tables, views, user-deﬁ ned functions 
(UDFs), or indexes; and if the schemas of any of the objects referenced in the query have changed 
since your batch was last compiled, your batch must be recompiled for statement-correctness 
reasons. Schema changes could include many things, such as adding an index on a table or in an 
indexed view, or adding or dropping a column in a table or view.
Since the release of SQL Server 2005, manually dropping or creating statistics on a table causes 
recompilation. Recompilation is triggered when the statistics are updated automatically, too. 
➤
➤
Physical Query Processing Part One: Compilation and Recompilation  ❘  401



404  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
In this formula, colmodctr (snapshot) is the value stored at the time the query plan was generated, 
and colmodctr (current) is the current value. If the difference between these counters as shown in 
the formula is greater or equal to recompilation threshold (RT), then recompilation happens for that 
statement. RT is calculated as follows for permanent and temporary tables. The n refers to a table’s 
cardinality (the number of rows in the table) when a query plan is compiled.
For a permanent table, the formula is as follows: 
If n <= 500, RT = 500
If n > 500, RT = 500 + 0.20 * n
For a temporary table, the formula is as follows: 
If n < 6, RT = 6
If 6 <= n <= 500, RT = 500
If n > 500, RT = 500 + 0.20 * n
For a table variable, RT does not exist. Therefore, recompilations do not happen because of changes 
in cardinality to table variables. Table 13-1 shows how the colmodctr is modiﬁ ed in SQL Server 
2005 and later through different data manipulation language (DML) statements. 
TABLE 13-1: colmodctr Modiﬁ cation
STATEMENT
COLMODCTR
INSERT
All colmodctr += 1 (colmodctr is incremented by 1 for each column in the 
table for each insert).
DELETE
All colmodctr += 1
UPDATE
If the update is to non-key columns, colmodctr+= 1 for all of the updated 
columns. If the update is to key columns, colmodctr+= 2 for all the columns.
BULK INSERT
Like n INSERTs. All colmodctr += n (n is the number of rows bulk inserted).
TABLE 
TRUNCATION
Like n DELETEs. All colmodctr += n (n is the table’s cardinal).
You can see the physical query process in action by following the code in Listing 13-2 and 
performing the steps associated with it. The goal is to determine whether the stored procedure 
plan is reused. Listing 13-2 also uses SQL Proﬁ ler and the dynamic management view (DMV) sys
.dm_exec_cached_plans to examine some interesting details. To determine whether a compiled 
plan is reused, you must monitor the events SP:CacheMiss, SP:CacheHit, and SP:CacheInsert 
under the Stored Procedures event class. Figure 13-2 shows these stored procedure plan compilation 
events in SQL Proﬁ ler.


406  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
 3. 
Connect the SQL Proﬁ ler to your designated machine, and start it after selecting the events 
(refer to Figure 13-2). Now execute the stored procedure TestCacheReUse as follows: 
USE AdventureWorks
GO
EXEC dbo.TestCacheReUse
In SQL Server Proﬁ ler you can ﬁ nd the SP:CacheMiss and SP:CacheInsert events, as 
shown in Figure 13-3.
FIGURE 13-3
The SP:CacheMiss event indicates that the compiled plan is not found in the plan cache 
(refer to Figure 13-3). The stored procedure plan is compiled and inserted into the plan 
cache indicated by SP:CacheInsert, and then the procedure TestCacheReUse is executed.
 4. 
Execute the same procedure again. This time SQL Server 2012 ﬁ nds the query plan in the 
plan cache, as shown in Figure 13-4.
FIGURE 13-4

Physical Query Processing Part One: Compilation and Recompilation ❘ 407
The plan for the stored procedure TestCacheReUse was found in the plan cache, which is  
why you see the event SP:CacheHit (refer to Figure 13-4).
 5. 
The DMV sys.dm_exec_cached_plans also provides information about the plans that are 
currently cached, along with some other information. Open the script DMV_CachePlanInfo
.sql from the solution QueryPlanReUse, shown here; the syntax in the following query is 
valid only when the database is in level 90 compatibility mode or higher: 
SELECT  bucketid, (SELECT Text FROM sys.dm_exec_sql_text(plan_handle)) AS
SQLStatement, usecounts,size_in_bytes, refcounts
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype = ‘Compiled Plan’
  AND objtype = ‘proc’
 6. 
Run this script; you should see output similar to what is shown in Figure 13-5.
FIGURE 13-5
This script uses the dynamic management function (DMF) sys.dm_exec_sql_text to get the SQL 
text for the plan_handle. Refer to Figure 13-5 to see that the compiled plan for the stored procedure 
TestCacheReUse that you executed earlier is cached. The column UseCounts in the output shows 
how many times this plan has been used since its inception. The ﬁ rst inception of the plan for this 
stored procedure was created when the SP:CacheInsert event happened (refer to Figure 13-3). You 
can also see the number of bytes consumed by the cache object (refer to Figure 13-5). In this case, the 
cache plan for the stored procedure TestCacheReUse has consumed 32KB in the plan cache. If you 
run DBCC FREEPROCCACHE now, and then run the query in the DMV_cachePlanInfo.sql script, 
you notice that the rows returned are 0 because DBCC FREEPROCCACHE cleared the procedure cache.



410  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
Another system DMV to investigate is sys.dm_exec_query_stats. This view can be used to 
return performance statistics for all queries, aggregated across all executions of those queries. The 
following query will return the top 50 CPU consuming queries for a speciﬁ c database (for example, 
AdventureWorks): 
-- Top 50 CPU Consuming queries
USE AdventureWorks
GO
SELECT TOP 50
  DB_NAME(DB_ID()) AS [Database Name],
  qs.total_worker_time / execution_count AS avg_worker_time,
  SUBSTRING(st.TEXT, (qs.statement_start_offset / 2) + 1,
    ((CASE qs.statement_end_offset
      WHEN -1 THEN DATALENGTH(st.TEXT)
      ELSE qs.statement_end_offset
    END - qs.statement_start_offset) / 2) + 1)
  AS statement_text, *
  FROM
    sys.dm_exec_query_stats AS qs
    CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) AS st
  ORDER BY avg_worker_time DESC;
DBCC FREEPROCCACHE
The DBCC FREPROCCACHE command clears the cached query plan and execution context. Use 
the full command only in a development or test environment. Avoid running it in a production 
environment because this could clear ALL the procedure caches on the entire server and cause 
all subsequent queries to be recompiled. This could lead to severe performance problems. This 
command can be run with parameters to target a speciﬁ c SQL handle, plan handle, or resource 
pool as shown here: 
DBCC FREEPROCCACHE [ ( { plan_handle | sql_handle | pool_name } ) ] 
[ WITH NO_INFOMSGS ]
See the article in BOL for more information at http://msdn.microsoft.com/en-us/library/
ms174283(v=sql.110).aspx.
DBCC FLUSHPROCINDB (db_id)
The DBCC FLUSHPROCINDB (db_id) command is the same as DBCC FREEPROCCACHE except it 
clears only the cached plan for a given database. The recommendation for use is the same as DBCC 
FREEPROCCACHE.
Parser and Algebrizer
Parsing is the process to check the syntax and transform the SQL batch into a parse tree. 
Parsing includes, for example, whether a non-delimited column name starts with a digit. 

Physical Query Processing Part One: Compilation and Recompilation ❘ 411
Parsing does not check whether the columns you have listed in a WHERE clause actually exist 
in any of the tables you have listed in the FROM clause. That is taken care of by the binding 
process (algebrizer). Parsing turns the SQL text into logical trees. One logical tree is created 
per query.
The algebrizer component was added in SQL Server 2005. This component replaced the 
normalizer in SQL Server 2000. The output of the parser — a parse tree — is the input to the 
algebrizer. The major function of the algebrizer is binding, so sometimes the entire algebrizer 
process is referred as binding. The binding process checks whether the semantics are correct. 
For example, if you try to JOIN table A with trigger T, then the binding process errors this out 
even though it may be parsed successfully. The following sections cover other tasks performed 
by the algebrizer.
Name Resolution
The algebrizer performs the tasks to check whether every object name in the query (the parse tree) 
actually refers to a valid table or column that exists in the system catalog, and whether it is visible in 
the query scope.
Type Derivation
The algebrizer determines the type for each node in the parse tree. For example, if you issue a UNION 
query, the algebrizer ﬁ gures out the type derivation for the ﬁ nal data type. (The columns’ data types 
could be different when you union the results of queries.)
Aggregate Binding
The algebrizer binds the aggregate to the host query and makes its decisions based on query syntax. 
Consider the following query in the AdventureWorks database: 
 SELECT s.CustomerID
 FROM Sales.SalesOrderHeader s
 GROUP BY s.CustomerID
 HAVING EXISTS(SELECT * FROM Sales.Customer c
 WHERE c.TerritoryID > COUNT(s.SalesPersonID))
In this query, although the aggregation is done in the inner query that counts the ContactID, 
the actual operation of this aggregation is performed in the outer query. For example, in the 
query plan shown in Figure 13-6, you can see that the aggregation is done on the result from the 
SalesOrderHeader table although the aggregation is performed in the inner query. The outer 
query is converted to something like this:
SELECT COUNT(s.SalesPersonID)
 FROM Sales.SalesOrderHeader s
 GROUP BY s.SalesPersonID

412  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
Grouping Binding
Consider the following query: 
 SELECT s.CustomerID, SalesPersonID, COUNT(s.SalesOrderID)
 FROM Sales.SalesOrderHeader s
 GROUP BY s.CustomerID, s.SalesPersonID
If you do not add the CustomerID and SalesPersonID columns in the GROUP BY list, the query 
does error out. The grouped queries have different semantics than the nongrouped queries. All 
nonaggregated columns or expressions in the SELECT list of a query with GROUP BY must have a 
direct match in the GROUP BY list. The process to verify this via the algebrizer is known as 
grouping binding.
Optimization
Optimization is probably the most complex and important piece to processing your queries. The 
logical tree created by the parser and algebrizer is the input to the optimizer. The optimizer needs 
the logical tree, metadata about objects involved in the query, such as columns, indexes, statistics, 
and constraints, and hardware information. The optimizer uses this information to create the 
compiled plan, which is made of physical operators. The logical tree includes logical operators that 
describe what to do, such as “read table,” “join,” and so on. The physical operators produced by 
the optimizer specify algorithms that describe how to do, such as “index seek,” “index scan,” “hash 
join,” and so on. The optimizer tells SQL Server how to exactly carry out the steps to get the results 
efﬁ ciently. Its job is to produce an efﬁ cient execution plan for each query in the batch or stored 
procedure. Figure 13-7 shows this process graphically.
FIGURE 13-6


414  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
optimizer chooses a parallel plan for your queries that uses multiple CPUs, which typically uses 
more resources than the serial plan but offers faster results. Of course, the optimizer cannot always 
come up with the best plan, which is why you have a job — for query tuning.
Optimization Flow
The ﬂ owchart in Figure 13-8 explains the steps involved in optimizing a query. These steps are 
simpliﬁ ed for explanation purposes. (The state-of-the-art optimization engine written by the SQL 
Server development team isn’t oversimpliﬁ ed.)
End optimization
Simpliﬁcation
Cost alternative
For each phase
Phase allowed?
Exploration
Implementation
FIGURE 13-8
The input to the optimizer is a logical tree produced by the algebrizer (refer to Figure 13-8). The 
query optimizer is a transformation-based engine. These transformations are applied to fragments 
of the query tree. Three kinds of transformation rules are applied: simpliﬁ cation, exploration, and 
implementation. The following sections discuss each of these transformations.
Simpliﬁ cation
The simpliﬁ cation process creates an output tree that is more optimized and returns results faster 
than the input tree. For example, it might push the ﬁ lter down in the tree, reduce the group by 

Physical Query Processing Part One: Compilation and Recompilation ❘ 415
columns, reduce redundant or excessive items in the tree, or perform other transformations. 
Figure 13-9 shows an example of simpliﬁ cation transformation (ﬁ lter pushing).
GROUP BY
C_CustKey
C_Name, N_Name
Group By
Join
C_NationKey =
N_NationKey
C_Custkey =
O_CustKey
O_OrderPriority =
‘1_URGENT’
Join
Filter
Nation
Orders
Customer
Aggregate reduction
GROUP BY
C_CustKey
N_Name
Group By
Join
C_NationKey =
N_NationKey
C_Custkey =
O_CustKey
O_OrderPriority =
‘1_URGENT’
Join
Filter
Nation
Orders
Customer
FIGURE 13-10
You can see that the logical tree on the left has the ﬁ lter after the join (refer to Figure 13-9). The 
optimizer pushes the ﬁ lter further down in the tree to ﬁ lter the data out of the Orders table with 
a predicate on O_OrderPriority. This optimizes the query by performing the ﬁ ltering early in 
the execution.
Figure 13-10 is another example of the simpliﬁ cation transformation (aggregate reduction). It differs 
from Figure 13-9 because it includes reducing the number of Group By columns in the execution plan.
GROUP BY
C_CustKey,
C_Name, N_Name
GROUP BY
C_CustKey,
C_Name, N_Name
C_NationKey =
N_NationKey AND
C_Custkey =
O_CustKeyAND
O_OrderPriority =
‘1-URGENT’
Group By
Filter
Join
Join
Orders
Customer
Nation
Group By
Join
C_NationKey =
N_NationKey
C_Custkey =
O_CustKey
O_OrderPriority =
‘1_URGENT’
Join
Filter
Nation
Orders
Customer
Filter pushing
FIGURE 13-9

416  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
The C_Name column is removed from the Group By clause because it contains the column C_
Custkey (refer to Figure 13-7 for the T-SQL statement), which is unique on the Customer table, so 
there is no need to include the C_Name column in the Group By clause. 
Exploration
As mentioned previously, SQL Server uses a cost-based optimizer implementation. Therefore, during 
the exploration process, the optimizer looks at alternative options to come up with the cheapest plan 
(see Figure 13-11). It makes a global choice using the estimated cost.
The optimizer explores the options related to which table should be used for inner versus outer 
joins. This is not a simple determination because it depends on many things, such as the size of the 
table, the available indexes, statistics, and operators higher in the tree. It is not a clear choice like 
the examples you saw in the simpliﬁ cation transformation.
Implementation
The third transformation is implementation. Figure 13-12 shows an example.
Join
Customer
Nation
 Hash Join
Implementation
Nation
Customer
FIGURE 13-12
Join
Customer
Nation
Join
Nation
Customer
FIGURE 13-11
As explained earlier, the implantation transforms the “what to do” part into the “how to do” part. 
In this example, the JOIN logical operation transforms into a HASH JOIN physical operation. The 
query cost is derived from physical operators based on model-of-execution algorithms (I/O and 
CPU) and estimations of data distribution, data properties, and size. Needless to say, the cost also 
depends on hardware such as the number of CPUs and the amount of memory available at the time 
of optimization.
Refer back to Figure 13-8. If the optimizer compared the cost of every valid plan and chose the least 
costly one, the optimization process could take a long time, and the number of valid plans could be 
huge. Therefore, the optimization process is divided into three search phases. As discussed earlier, 
a set of transformation rules is associated with each phase. After each phase, SQL Server 2012 

Physical Query Processing Part Two: Execution ❘ 417
evaluates the cost of the cheapest query plan to that point. If the plan is cheap enough, then the 
optimizer stops there and chooses that query plan. If the plan is not cheap enough, the optimizer 
runs through the next phase, which has an additional set of rules that are more complex.
The ﬁ rst phase of the cost-based optimization, Phase 0, contains a limited set of rules. These rules 
are applied to queries with at least four tables. Because JOIN reordering alone generates many valid 
plans, the optimizer uses a limited number of join orders in Phase 0 and considers only hash and 
loop joins in this phase. In other words, if this phase ﬁ nds a plan with an estimated cost below 0.2 
(internal cost unit), the optimization ends there.
The second phase, Phase 1, uses more transformation rules and different join orders. The best plan 
that costs less than 1.0 would cause optimization to stop in this phase. Until Phase 1, the plans are 
nonparallel (serial query plans).
Consider this: What if you have more than one CPU in your system? In that case, if the cost of the 
plan produced in Phase 1 is more than the cost threshold for parallelism (see sp_configure for this 
parameter; the default value is 5), then Phase 1 is repeated to ﬁ nd the best parallel plan. Then the 
cost of the serial plan produced earlier is compared with the new parallel plan, and the next phase, 
Phase 2 (the full optimization phase), is executed for the cheaper of the two plans.
PHYSICAL QUERY PROCESSING PART TWO: EXECUTION
After compilation, the execution engine takes over; it generates the execution plan, it copies 
the plan into its executable form and executes the steps in the query plan to produce the 
wanted result. If the same query or stored procedure is executed again, the compilation phase 
is skipped, and the execution engine uses the same cached plan to start the execution. Once 
that is complete, the data needs to be accessed and there are a myriad of ways to perform that 
index access. There is always room for tuning, but you must have enough data to start with. 
Therefore, you need to take a baseline of the system’s performance and compare against that 
baseline so that you know where to start. Chapter 12, “Monitoring Your SQL Server,” has 
details on getting the baseline. Just because a process is slow doesn’t mean that you have to 
start tuning SQL statements. First you need to do many basic things, such as configure the 
SQL Server 2012 database, and make sure tempdb and the log files are on their own drives. It 
is important to get the server configuration correct. See Chapter 10, “Configuring the Server 
for Optimal Performance,” and Chapter 11, “Optimizing SQL Server 2012,” for details about 
configuring your server and database for optimal performance. There is also a white paper 
on tempdb that you should read. This document is for SQL Server 2005, but it’s useful for 
SQL Server 2012 as well and can be found at: www.microsoft.com/technet/prodtechnol/
sql/2005/workingwithtempdb.mspx.
Don’t overlook the obvious. For example, suppose you notice that performance is suddenly 
quite bad on your server. If you have done a baseline and that doesn’t show much change in 
performance, then it is unlikely that the sudden performance change was caused by an application 
in most cases, unless a new patch for your application caused some performance changes. In that 
case, look at your server conﬁ guration to see if that changed recently. Sometimes you merely run 


Physical Query Processing Part Two: Execution ❘ 419
The counters can be explained as such:
DatabaseName: Name of the database
DatabaseFile_Type: Data ﬁ le or Log ﬁ le for the associated database name
IO_Read_MB: Number of bytes read from the database ﬁ le
IO_Write_MB: Number of bytes written to the database ﬁ le
IO_TOTAL_MB: Total number of bytes transferred to/from the database ﬁ le
IO_STALL_Seconds: Time, in seconds, that users waited for I/O to be completed on the ﬁ le
IO_STALL_Pct: IO stall percentage relative to the entire query
The counters show the value from the time the instance of SQL Server starts, but it gives you a 
good idea of which database is hammered. This query gives you a good starting point to further 
investigate the process level (stored procedures, ad-hoc T-SQL statements, and so on) for the 
database in question.
Next, look at how to gather the query plan and analyze it. You also learn about the different tools 
you need in this process.
Working with the Query Plan
Looking at the query plan is the ﬁ rst step to take in the process of query tuning. The SQL Server query 
plan comes in different ﬂ avors: textual, graphical, and, with SQL Server 2012, XML format. Showplan 
describes any of these query plan ﬂ avors. Different types of Showplans have different information. SQL 
Server 2012 can produce a plan with operators only, with cost information, and with XML format, which 
can provide some additional runtime details. Table 13-2 summarizes the various Showplan formats. 
➤
➤
➤
➤
➤
➤
➤
FIGURE 13-13


Physical Query Processing Part Two: Execution ❘ 421
GROUP BY sh.CustomerID, st.Name
HAVING SUM(sh.SubTotal) > 2000.00
GO
SET SHOWPLAN_TEXT OFF
GO
When you SET the SHOWPLAN_TEXT ON, the query does not execute; it just produces the estimated 
plan. The textual plan is shown next.
The output tells you that there are seven operators: Filter, Hash Match (Inner Join), Hash Match 
(Aggregate), Clustered Index Scan, Merge Join, Clustered Index Scan, and Index Scan, which are 
shown in bold in the query plan here. 
SELECT sh.CustomerID, st.Name, SUM(sh.SubTotal) AS SubTotal
FROM Sales.SalesOrderHeader sh
   JOIN Sales.Customer c
     ON c.CustomerID = sh.CustomerID
   JOIN Sales.SalesTerritory st
     ON st.TerritoryID = c.TerritoryID
   GROUP BY sh.CustomerID, st.Name   HAVING SUM(sh.SubTotal) > 2000.00
StmtText
  |--Filter(WHERE:([Expr1006]>(2000.00)))
       |--Hash Match(Inner Join, HASH:([st].[TerritoryID])=([c].[TerritoryID]), 
RESIDUAL:([AdventureWorks].[Sales].[SalesTerritory].[TerritoryID] 
as [st].[TerritoryID]=[AdventureWorks].[Sales].[Customer].[TerritoryID] 
as [c].[TerritoryID]))
            |--Clustered Index 
Scan(OBJECT:([AdventureWorks].[Sales].[SalesTerritory].
[PK_SalesTerritory_TerritoryID] AS [st]))
            |--Hash Match(Inner Join, HASH:([sh].[CustomerID])=([c].[CustomerID]))
                 |--Hash Match(Aggregate, HASH:([sh].[CustomerID]) DEFINE:
([Expr1006]=SUM([AdventureWorks].[Sales].[SalesOrderHeader].[SubTotal] 
as [sh].[SubTotal])))
                 |    |--Clustered Index Scan(OBJECT:
([AdventureWorks].[Sales].[SalesOrderHeader].
[PK_SalesOrderHeader_SalesOrderID] AS [sh]))
                 |--Index Scan(OBJECT:
([AdventureWorks].[Sales].[Customer].[IX_Customer_TerritoryID] 
AS [c]))
It may be easier to view the plan output in SSMS since the formatting here doesn’t look that good. 
You can run this query in the AdventureWorks database. In this plan all you have are the operators’ 
names and their basic arguments. For more details on the query plan, other options are available, 
which you explore soon.
To analyze the plan you read branches in inner levels before outer ones (bottom to top), and 
branches that appear in the same level from top to bottom. You can tell which branches are inner 
and which are outer based on the position of the pipe (|) character. When the plan executes, the 
general ﬂ ow of the rows is from the top down and from right to left. An operator with more 
indentation produces rows consumed by an operator with less indentation and produces rows for 



424  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
XML is the richest format of the Showplan. It contains some unique information not available 
in other Showplan formats. The XML Showplan contains the size of the plan in cache (the 
CachedPlanSize attributes) and parameter values for which the plan has been optimized 
(the Parameter sniffing element). When a stored procedure compiles for the ﬁ rst time, the 
values of the parameters supplied with the execution call optimize the statements within that stored 
procedure. This process is known as parameter snifﬁ ng. Also available is some runtime information, 
which is unique to the XML plan and described further in the section, “Actual Execution Plan” 
later in this chapter.
You can write code to parse and analyze the XML Showplan. This is probably the greatest 
advantage it offers because this task is hard to achieve with other forms of the Showplan.
Refer to the white paper at http://msdn.microsoft.com/en-us/library/ms345130.aspx 
for information on how you can extract the estimated execution cost of a query from its XML 
Showplan using CLR functions. This document is for SQL Server 2005 but still applies. You can use 
this technique to ensure that users can submit only those queries costing less than a predetermined 
threshold to a server running SQL Server, thereby ensuring it is not overloaded with costly, 
long-running queries.
Graphical Estimated Showplan
You can view a graphical estimated plan in Management Studio. To access the plan, either use the 
shortcut key Ctrl+L or select Query Á Display Estimated Execution plan. You can also select the 
button indicated in Figure 13-15.
FIGURE 13-15
If you use any of these options to display the graphical estimated query plan, it displays the plan as 
soon as compilation is completed because compilation complexity can vary according to the number 
and size of the tables. Right-clicking the graphical plan area in the Execution Plan tab reveals 
different zoom options and properties for the graphical plan. 
Actual Execution Plan
This section describes the options you can use to get the actual execution plan (SET 
STATISTICS XML ON|OFF and SET STATISTICS PROFILE ON|OFF) using the graphical actual 
 
 
 
 

Physical Query Processing Part Two: Execution ❘ 425
execution plan option in Management Studio. You can get the actual execution plan using SQL 
Trace as well; see the section, “Gathering Query Plans for Analysis with SQL Trace” later in 
this chapter.
SET STATISTICS XML ON|OFF
Two kinds of runtime information are in the XML Showplan: per SQL statement and per thread. If 
a statement has a parameter, the plan contains the parameterRuntimeValue attribute, which shows 
the value of each parameter when the statement was executed. The degreeOfParallelism attribute 
shows the actual degree of parallelism. The degree of parallelism shows the number of concurrent 
threads working on the single query. The compile time value for degree of parallelism is always half 
the number of CPUs available to SQL Server unless two CPUs are in the system. In that case, the 
value will be 2 as well.
The XML plan may also contain warnings. These are events generated during compilation or 
execution time. For example, missing statistics are a compiler-generated event. One important 
feature in SQL Server 2012 (originally added in SQL Server 2005) is the USE PLAN hint. This 
feature requires the plan hint in XML format, so you can use the XML Showplan. Using the 
USE PLAN hint, you can force the query to be executed using a certain plan. For example, suppose 
you ﬁ nd that a query runs slowly in the production environment but faster in the preproduction 
environment. You also ﬁ nd that the plan generated in the production environment is not optimal 
for some reason. In that case, you can use the better plan generated in the preproduction 
environment and force that plan in the production environment using the USE PLAN hint. For more 
details on how to implement it, refer to the Books Online (BOL) topic, “Using the USE PLAN 
Query Hint.”
There are new warnings that have been included in SQL Server 2012 execution plan. These two 
messages speciﬁ cally warn against implicit (or, sometimes, explicit) type conversions that cause the 
available index not to be used. The ﬁ rst warning message is:
 Type conversion in expression ColumnExpression may affect 
“CardinalityEstimate” in query plan choice 
This means that there is a type conversion on a certain column that will affect the 
CardinalityEstimate. The other warning message is:
Type conversion in expression ColumnExpression may affect 
“SeekPlan” in query plan choice. 
This means that the type conversion that is happening on a certain column will cause the index not 
to be used.
SET STATISTICS PROFILE ON|OFF
The SET STATISTICS PROFILE ON|OFF option is better than the previous option, which you 
should use it for query analysis and tuning. Run the following query using the code shown in 
Listing 13-7. If you don’t want to mess with your AdventureWorks database, you can back up and 
restore the AdventureWorks database with a different name. If you do that, be sure to change the USE 
DatabaseName line in the script.  


Physical Query Processing Part Two: Execution ❘ 427
Graphical Actual Execution Plan
You can use Management Studio to view the graphical actual execution plan. Again, either use the 
shortcut Ctrl+M, select Query Á Include Actual Execution plan, or select the button indicated in 
Figure 13-17.
FIGURE 13-17
If you use any of these options to display the graphical actual query plan, nothing happens. After 
query execution, the actual plan displays in a separate tab.
Index Access Methods
There are a number of index access methods that SQL Server uses to retrieve data needed to build 
execution plans. In this section you get to explore the ways that these methods are different from 
each other and how the index access methods may affect the performance of the query. You can use 
this knowledge when you tune the query and decide whether it uses the correct index access method 
and take the appropriate action.
In addition, you should make a copy (Backup and Restore, or take a snapshot) of the 
AdventureWorks database on your machine so that if you drop or create indexes on it, 
the original AdventureWorks database remains intact. When you restore the database, call 
it AW_2 for these examples.
Table Scan
A table scan involves a sequential scan of all data pages belonging to the table. Run the following 
script in the AW_2 database: 
SELECT * INTO dbo.New_SalesOrderHeader
FROM Sales.SalesOrderHeader
After you run this script to make a copy of the SalesOrderHeader table, run the following script: 
SELECT SalesOrderID, OrderDate, CustomerID
FROM dbo.New_SalesOrderHeader
Because this is a heap, this statement causes a table scan. If you want, you can always get the actual 
textual plan using SET STATISTICS PROFILE ON. Figure 13-18 displays the graphical plan.






Physical Query Processing Part Two: Execution ❘ 433
Table ‘New_SalesOrderHeader’. Scan count 1, logical reads 108, 
physical reads 0, read-ahead reads 0, lob logical reads 0, 
lob physical reads 0, lob read-ahead reads 0.
The query plan is shown in Figure 13-21.
FIGURE 13-21
As you can see from the statistics I/O result, only 108 logical reads were done to fulﬁ ll this query. 
The results returned by the clustered index query and the covering non-clustered index query are 
identical (number of columns and number of rows), but there were 794 logical reads in the clustered 
index and only 108 logical reads in the non-clustered scan because the non-clustered index has 
covered the query and served the data from its leaf level.
The clustered index leaf level contains the full data rows (all columns), whereas the non-clustered 
index has only one key column and two included columns. That means the row size is smaller for a 
non-clustered index, and the smaller row size can hold more data and requires less I/O.
Covering Non-Clustered Index Scan (Ordered)
If you run the query found in Listing 13-11 with OrderDate in the ORDER BY clause, the optimizer 
chooses the covering non-clustered index. The query plan would be exactly the same as shown in 
Figure 13-21, except that you see Ordered = True in the information box. The statistics IO 
information is also the same as for the non-ordered covering non-clustered index scan. Of course, 
an ordered index scan is not only used when you explicitly request the data sorted; the optimizer 
can also choose to sort the data if the plan uses an operator that can beneﬁ t from sorted data.




Physical Query Processing Part Two: Execution ❘ 437
This index access method ﬁ rst performs a seek operation on the ﬁ rst key (43696 in this case) and 
then performs an ordered partial scan at the leaf level, starting from the ﬁ rst key in the range 
and continuing until the last key (45734). Because the leaf level of the clustered index is actually the 
data rows, no lookup is required in this access method.
Look at Figure 13-25 to understand the I/O cost for this index access method. To read at least a 
single leaf page, the number of seek operations required is equal to the number of levels in the index.
FIGURE 13-24
Pointerto
root
Index depth
(3 in our case)
Clustered
Index
Leaf
Pages
FIGURE 13-25

438  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
How do you ﬁ nd the level in the index? Run the INDEXPROPERTY function with the IndexDepth 
property: 
 SELECT INDEXPROPERTY (OBJECT_ID(‘New_SalesOrderHeader’), ‘IXCU_SalesOrderID’,
 ‘IndexDepth’)
In this case, the index depth is 3, and of course the last level is the leaf level where the data resides. 
As shown in Figure 13-25, the cost of the seek operation (three random reads in this case because 
that is the depth of the index) and the cost of the ordered partial scan within the leaf level to get the 
data (in this case 53, according to the read-ahead reads) add up to 56 logical reads, as indicated in 
the statistics IO information. As you can see, an ordered partial scan typically incurs the bulk 
of the query’s cost because it involves most of the I/O to scan the range (53 in this case). As 
mentioned earlier, index fragmentation plays an important role in ordered partial scan operations, 
so when there is high fragmentation in the index, the disk arm needs to move a lot, which results in 
degraded performance.
The query plan shown in Figure 13-24 is called a trivial plan, which means that there is no better 
plan than this and the plan does not depend on the selectivity of the query. As long as you have a 
predicate on the SalesOrderID columns, no matter how many rows are sought, the plan is always 
the same unless you have a better index that the query optimizer can choose from. 
Fragmentation
Throughout this chapter, there have been a number of places in which fragmentation was 
mentioned. Most speciﬁ cally, in the difference between ordered and unordered index scans, you 
looked at how fragmentation can affect the performance of the query due to the order of pages and 
extents in a table. The following section elaborates on this. The two types of fragmentation are 
logical scan fragmentation and average page density. Logical scan fragmentation is the percentage 
of out-of-order pages in the index in regard to their physical order, rather than their logical order, in 
the linked list. This fragmentation has a substantial impact on ordered scan operations like the one 
shown in Figure 13-24. This type of fragmentation has no impact on operations that do not rely on 
an ordered scan, such as seek operations, unordered scans, or lookup operations.
The average page density is the percentage of pages that are full. A low percentage (fewer pages full) 
has a negative impact on the queries that read the data because these queries end up reading more 
pages than they could, were the pages better populated. The upside of having free space in pages 
is that insert operations in these pages do not cause page splits, which are expensive and lead to 
fragmentation. In short, free space in pages is bad for a data warehouse type of system (more read 
queries), whereas it is good for an OLTP system that involves many data modiﬁ cation operations. 
However, you need to remember that this is a balancing act between free space and page splits.
Rebuilding the indexes and specifying the proper ﬁ ll factor based on your application reduces or 
removes the fragmentation. Using appropriate data types, that is, char(2) for state, can also be 
the difference between good performance and page splits when they are updated from empty to 
populated. You can use the following DMF to ﬁ nd out both types of fragmentation in your index. 
Be aware that querying this DMF can affect performance because it reads data from both the leaf 
and nonleaf levels depending on the value (LIMITED, SAMPLED or DETAILED) that is speciﬁ ed for 
the ﬁ nal parameter of the DMF. For example, to ﬁ nd out the fragmentation for indexes on the 
New_SalesOrderHeader table, run the following query: 

Physical Query Processing Part Two: Execution ❘ 439
 SELECT 
* FROM sys.dm_db_index_physical_stats (DB_ID(),
 OBJECT_ID(‘dbo.New_SalesOrderHeader’), NULL, NULL, NULL)
Look for the avg_fragmentation_in_percent column for logical fragmentation. Ideally, it should be 0, 
which indicates no logical fragmentation. For average page density, look at the avg_page_space_used_
in_percent column. It shows the average percentage of available data storage space used in all pages. 
SQL Server 2005 added a feature to build the indexes online — an ONLINE option is added 
to the CREATE and ALTER INDEX statements. This Enterprise Edition feature enables you to 
create, drop, and rebuild the index online. See Chapter 14, “Indexing Your Database,” for more 
details. Following is an example of rebuilding the index IXNC_SalesOrderID on the 
New_SalesOrderHeader table: 
ALTER INDEX IXNC_SalesOrderID ON dbo.New_SalesOrderHeader 
REBUILD WITH (ONLINE = ON)
Statistics
SQL Server 2012 collects statistical information about the distribution of values in one or more 
columns of a table or indexed view. The uniqueness of data found in a particular column is known as 
cardinality. High-cardinality refers to a column with values that are unique, whereas low-cardinality 
refers to a column that has many values that are the same. The two main types of statistics are single-
column or multicolumn. . Each statistical object includes a histogram that displays the distribution of 
values in the ﬁ rst column of the list of columns contained in that statistical object. 
The query optimizer uses these statistics to estimate the cardinality and thus the selectivity of 
expressions. When those are calculated, the sizes of intermediate and ﬁ nal query results are 
estimated. Good statistics enable the optimizer to accurately assess the cost of different query plans 
and choose a high-quality plan. All information about a single statistics object is stored in several 
columns of a single row in the sysindexes table and in a statistics binary large object (statblob) 
kept in an internal-only table.
If your execution plan has a large difference between the estimated row count and the actual row 
count, the ﬁ rst things you should check are the statistics on the join columns and the column in the 
WHERE clause for that table. (Be careful with the inner side of loop joins; the row count should match 
the estimated rows multiplied by the estimated executions.) Make sure that the statistics are current. 
One way to verify this is to check the UpdateDate, Rows, and Rows Sampled columns returned by 
DBCC SHOW_STATISTICS. You must keep up-to-date statistics. Up-to-date statistics are a reﬂ ection 
of the data, not the age of the statistics. With no data changes, statistics can be valid indeﬁ nitely. 
You can use the following views and command to get details about statistics:
To see how many statistics exist in your table, you can use the sys.stats view. 
To view which columns are part of the statistics, you can use the sys.stats_columns view. 
To view the histogram and density information, you can use DBCC SHOW_STATISTICS. For 
example, to view the histogram information for the IXNC_SalesOrderID index on the New_
SalesOrderHeader table, run the following command: 
 DBCC SHOW_STATISTICS (‘dbo.New_SalesOrderHeader’, ‘IXNC_SalesOrderID’)
➤
➤
➤


Physical Query Processing Part Two: Execution ❘ 441
Hash Join
The hash join has two inputs like every other join: the build input (outer table) and the probe input 
(inner table). The query optimizer assigns these roles so that the smaller of the two inputs is the 
build input. A variant of the hash join (hash aggregate physical operator) can do duplicate removal 
and grouping, such as SUM (OrderQty) GROUP BY TerritoryID. These modiﬁ cations use only one 
input for both the build and probe roles.
The following query is an example of a hash join, and the graphical execution plan is shown in 
Figure 13-27:
--Hash Match
 
 SELECT p.Name As ProductName, ps.Name As ProductSubcategoryName
 FROM Production.Product p
 JOIN Production.ProductSubcategory ps
   ON p.ProductSubcategoryID = ps.ProductSubcategoryID
 ORDER BY p.Name,  ps.Name
FIGURE 13-27
As discussed earlier, the hash join ﬁ rst scans or computes the entire build input and then builds 
a hash table in memory if it ﬁ ts the memory grant. (In Figure 14-27, it is the Production
.ProductSubCategory table.) Each row is inserted into a hash bucket according to the hash value 
computed for the hash key, so building the hash table needs memory. If the entire build input is 
smaller than the available memory, all rows can be inserted into the hash table. (You see what 
happens if there is not enough memory shortly.) This build phase is followed by the probe phase. 
The entire probe input (refer to Figure 14-27; it is the Production.Product table) is scanned or 
computed one row at a time, and for each probe row (from the Production.Product table), the 
hash key’s value is computed, the corresponding hash bucket (the one created from the Production
.ProductSubCategory table) is scanned, and the matches are produced. This strategy is called an 
in-memory hash join.
If you’re talking about the AdventureWorks database running on your laptop with 1GB of RAM, 
you won’t have the problem of not ﬁ tting the hash table in memory. In the real world, however, 
with millions of rows in a table, there might not be enough memory to ﬁ t the hash table. If the build 
input does not ﬁ t in memory, a hash join proceeds in several steps. This is known as a grace hash 
join. In this hash join strategy, each step has a build phase and a probe phase. Initially, the entire 
build and probe inputs are consumed and partitioned (using a hash function on the hash keys) into 
multiple ﬁ les. Using the hash function on the hash keys guarantees that any two joining records 
must be in the same pair of ﬁ les. Therefore, the task of joining two large inputs has been reduced 
to multiple, but smaller, instances of the same tasks. The hash join is then applied to each pair of 
partitioned ﬁ les. If the input is so large that the preceding steps need to be performed many times, 




Physical Query Processing Part Two: Execution ❘ 445
Partition-Aware Seek Operation
The partition-aware SEEK operation is best understood through an example. Suppose you have a 
table called Sales with columns WeekID, GeographyID, ProductID, and SalesAmount. This table 
is partitioned on WeekID, which means each partition contains sales for a week. Suppose also that 
this table has a clustered index on GeographyID. Now say you want to perform a query like the 
following: 
 SELECT * FROM Sales WHERE GeographyID = 4 AND WeekID < 10
The partition boundaries for table Sales are deﬁ ned by the following partition function: 
 CREATE PARTITION FUNCTION myRangePF1 (int) AS RANGE LEFT FOR VALUES (3, 7, 10);
In SQL Server 2012, when the query optimizer starts processing this query, it inserts PartitionID 
(which is hidden to represent the partition number in a partitioned table) as a leading column in 
the SEEK or SCAN operation. The query optimizer looks at the clustered index on GeographyID 
with composite columns, with PartitionID as the leading column (PartitionID, GeographyID). 
First, SQL Server 2012 determines which partitions it needs to look at during the ﬁ rst-level SEEK 
operation. In this case, because this table is partitioned on WeekID, and the predicate in the query 
is WeekID < 10, SQL Server 2012 can ﬁ nd all the partitions that have WeekID less than 10 ﬁ rst 
(see Figure 13-31). Here SQL Server 2012 ﬁ nds three partitions that have WeekID less than 10, so it 
performs a second-level SEEK operation within those three partitions to ﬁ nd the records that have 
GeographyID = 4.
Parallel Query Execution Strategy for Partitioned Objects
SQL Server 2012 has implemented a new way to use parallelism to improve query performance 
when you access the partitioned table. If the number of threads is less than the number of partitions, 
the query processor assigns each thread to a different partition, initially leaving one or more 
partitions without an assigned thread. When a thread ﬁ nishes executing on a partition, the query 
processor assigns it to the next partition until each partition has been assigned a single thread. This 
is the only case in which the query processor reallocates threads to other partitions.
If the number of threads is equal to the number of partitions, the query processor assigns one thread 
to each partition. When a thread ﬁ nishes, it is not reallocated to another partition.
1  1  1
PartitionedID
Based on WeekID < 10
partitions found using
partition-Aware SEEK
Operation
2  2  2
3  3  3
4  4 4
1   2  3   4
GeographyID
2   4  5   6
3   4  7   8
4   5  6   9
1   2  3
WeekID
4  5  6 7
8  9  10
11  12
FIGURE 13-31

446  ❘  CHAPTER 13  PERFORMANCE TUNING T-SQL
What if SQL Server 2012 has more threads than the number of partitions it actually needs to access 
to get the data? Refer to Figure 13-31. You have data in three partitions, and SQL Server has 10 
threads to access the partitions. SQL Server uses all the available threads to access those partitions, 
but how can it assign those threads to each partition? It assigns three threads to each partition, and 
the remaining thread is assigned to one partition. Therefore, out of three partitions, two have six 
threads working on them (three threads each), and one partition has four threads working on it. If 
you have only one partition to access, then all the available threads are assigned to that partition — 
in this case, 10. When a thread ﬁ nishes its execution, it is not reassigned to another partition.
Remember the following key points to achieve better query performance on partitioned tables when 
you access large amounts of data: 
Use more memory to reduce I/O cost if your performance data proves that your system is 
I/O bound.
Take advantage that multicore processors are on a commodity server because SQL Server 
2012 can also use it for parallel query processing capabilities.
Make sure you have a clustered index on partitioned tables so that the query processor can 
take advantage of index scanning optimizations done in SQL Server 2012.
If you aggregate a large amount of data from a partitioned table, make sure you have 
enough tempdb space on your server. See the article “Capacity Planning for tempdb” in 
BOL for more information on how to monitor tempdb space usage.
Execution Plans for Partitioned Heaps
As mentioned earlier, PartitionID is always a leading column to seek for a particular partition 
or range of partitions, even if the partitioned table is a heap. In SQL Server 2012, a partitioned 
heap is treated as a logical index on the partition ID. Partition elimination on a partitioned heap is 
represented in an execution plan as a Table Scan operator with a SEEK predicate on partition ID. 
The following example shows the Showplan information provided: 
 |-- Table Scan (OBJECT: ([db].[dbo].[Sales]),
    SEEK: ([PtnId1001]=[Expr1011]) ORDERED FORWARD)
As you can see, even if it looks like a table scan, it still seeks on PartitionID.
Gathering Query Plans for Analysis with SQL Trace
Earlier you examined the query plans with different option such as SET STATISTICS PROFILE ON 
and SET STATISTICS XML ON. This technique does not work when you want to gather the query 
plans in your production environment. You have to use SQL Trace to get the query plan for analysis. 
See Chapter 12 for details about how to create server-side traces and import the trace data into a 
database table. You should create a server-side trace to capture events. In addition, import the data 
into a database table for analysis, rather than in SQL Proﬁ ler because SQL Proﬁ ler doesn’t offer as 
many options to play with data.
Please make sure that you set ﬁ lters for your criteria when you gather the data with a server-side 
trace because the ﬁ le size can grow quickly. In addition, depending on whether you prefer a textual 
➤
➤
➤
➤

Summary ❘ 447
execution plan or XML, you can check one of the events Showplan Statistics Proﬁ le or Showplan 
XML Statistics Proﬁ le.
SUMMARY
In this chapter you learned how to do Query parsing, compiling, and optimization are all key 
performance tuning strategies in SQL Server 2012. Additionally, knowing how to read the query 
plan is imperative. When you read the query plan (using STATISTICS PROFILE, for example) the 
most important columns you want to look at are Rows, Executes, and EstimatedRows. If you see 
a big discrepancy between Rows (the actual row count) and EstimatedRows, remove that small 
query from your main query and start your analysis there. Not every performance problem with a 
query stems from bad statistics or cardinality estimations. In the real world, where users are less 
experienced, most performance problems result from user errors (lack of indexes and such). Check 
the statistics on the columns in the JOIN and WHERE clauses.
Another important part of Performance tuning includes the various index access methods and 
join algorithms. Normally, I/O is the slowest process, so your goal in query tuning is to reduce the 
number of I/Os and balance the data modiﬁ cation operation (in an OLTP system), and for that 
knowledge of the index, access methods and join algorithms are vital. Tuning is not easy, but with 
patience and attention to details, you can get to the root of the problem. Of course, make sure that 
your server and disk conﬁ guration are done properly. New features starting in SQL Server 2008 
help facilitate the performance tuning you will perform, such as the MERGE and the query-processing 
enhancements on partitioned tables and indexes. Now that you know about conﬁ guring your server, 
optimizing SQL Server, and tuning queries, you can move on to learn about indexing your database 
in the next chapter. 


450  ❘  CHAPTER 14  INDEXING YOUR DATABASE
What’s New for Indexes in SQL Server 2012
SQL Server 2012 introduces a new type of index called columnstore index based on the Vertipaq engine 
acquisition. Additionally, online index operations such as index build and rebuild containing LOB 
columns are now supported. The next two sections describe these two new enhancements in detail.
Columnstore Index
Columnstore index is the new type of index introduced in SQL Server 2012. It is a column-based 
non-clustered index geared toward increasing query performance for workloads that involve large 
amounts of data, typically found in data warehouse fact tables. 
This new type of index stores data column-wise instead of row-wise, as indexes currently do. For 
example, consider an Employee table containing employee data, as shown in Table 14-1.
TABLE 14-1: Sample Employee Table
FIRSTNAME
LASTNAME
HIREDATE
GENDER
Adam
Jorgensen
5/9/2008
Male
Sherri
McDonald
7/1/2009
Female
Brian
McDonald
09/15/2009
Male
Jose
Chinchilla
1/10/2010
Male
Tim
Murphy
7/1/2009
Male
Tim
Moolic
6/1/2008
Male
In a row-based index, the data in the Employee table is stored in one or more data pages, as shown 
in Figure 14-1.
FIGURE 14-1
In a column-based index, the data in the Employee table is stored in separate pages for each of the 
columns, as shown in Figure 14-2.

Noteworthy Index-Related Features in SQL Server ❘ 451
FIGURE 14-2
Performance advantages in columnstore indexes are possible by leveraging the VertiPaq compression 
technology, which enables large amounts of data to be compressed in-memory. This in-memory 
compressed store reduces the number of disk reads and increases buffer cache hit ratios because only 
the smaller column-based data pages that need to satisfy a query are moved into memory.
For wide tables, such as those commonly found in data warehouses, columnstore indexes come in 
handy as you essentially reduce the amount and size of data needed to be accessed for any given 
query. For example, consider the following query:
SELECT 
FirstName,
LastName, 
FROM EmployeeTable
WHERE HireDate >= ‘1/1/2010’
A column-store index is more efﬁ cient for this example because only one smaller-sized (compressed) 
data page is needed to satisfy the query. In this case, the columnstore index for the HireDate 
column satisﬁ es the WHERE clause. A row-based index is not as efﬁ cient because it may need to load 
one or more larger-sized data pages into memory and read the entire rows, including columns not 
needed to satisfy the query. A larger-sized data page and additional unnecessary columns increases 
data size, memory usage, disk reads, and overall query time. Imagine if this table had 20 or more 
columns!
Columnstore indexes have some requirements and limitations, as shown in Table 14-2.
TABLE 14-2: Requirements and Limitations of Columnstore Index
DESCRIPTION
REQUIREMENT/LIMITATION
No. of columnstore indexes per table
1
Index record size limit of 900 bytes
No limit/Not applicable.
Index limit of 16 key columns 
No limit/Not applicable.
Table partitioning support
Yes, as a partition aligned index.
continues

452  ❘  CHAPTER 14  INDEXING YOUR DATABASE
DESCRIPTION
REQUIREMENT/LIMITATION
Can be combined with row-based 
indexes?
Yes, if clustered index, all columns must be present in 
columnstore index.
Update, Delete, Insert, Merge 
supported?
No, columnstore indexes are read-only but workarounds 
exist. Refer to Books Online: Best Practices: Updating 
Data in a Columnstore Index.
Data types that can be included in a 
columnstore index
Char, varchar except varchar(max), nchar, nvarchar except 
nvarchar(max), decimal and numeric except with precision 
greater than 18 digits, int, bigint, smallint, tinyint, ﬂ oat, 
real, bit, money, smallmoney, all date and time data types 
except datetimeof set with scale greater than 2.
Data types that cannot be included in 
a columnstore index.
Binary, varbinary, ntext, text, image, varchar(max), 
nvarchar(max), uniqueidentiﬁ er, rowversion, timestamp, 
sql_variant, decimal and numeric with precision greater 
than 18 digits, datetimeof set with scale greater than 2, 
CLR types including hierarchyid and spatial types, xml.
Following is the basic syntax to create a columnstore index:
CREATE COLUMNSTORE INDEX idx_cs1
ON EmployeeTable (FirstName, LastName, HireDate, Gender)
You can also create columnstore indexes using SQL Server Management Studio. Simply navigate to 
the Indexes section of the table, and select New Index Á Non-Clustered Columnstore Index.
Online Index Operations with LOB Columns
In previous versions of SQL Server, online index operations were not possible on indexes with 
columns deﬁ ned as large object data types. LOB datatypes include image, text, ntext, varchar(max), 
nvarchar(max), varbinary(max), and xml datatypes.
SQL Server 2012 supports building and rebuilding indexes that contain LOB columns while 
keeping the index available for read and write operations. A new reference consistency and sharing 
mechanism is employed during the online rebuild process to keep track of the data referenced by the 
old index and the new index.
Index Features from SQL Server 2008R2, SQL Server 2008, 
and SQL Server 2005 
While the core functionality of index hasn’t changed, there have been a number of enhancements in 
the last few releases of SQL Server. This section highlights all the index-related features introduced 
in previous versions of SQL Server that are now part of SQL Server 2012.
TABLE 14-2 (continued)

Noteworthy Index-Related Features in SQL Server ❘ 453
SQL Server 2008 introduced index-related features: 
Support for up to 15K partitions: In SQL Server 2008 Service Pack 2 and SQL Server 2008 
R2 Service Pack 1 (SP1), the limit of 999 table partitions increased to 15,000.
Filtered indexes and statistics: You can use a predicate to create ﬁ ltered indexes and 
statistics on a subset of rows in the table. Prior to SQL Server 2008, indexes and statistics 
were created on all the rows in the table. Now you can include a WHERE predicate in the 
index or statistics you create to limit the number of rows to be included in the indexes or 
stats. Filtered indexes and statistics are especially suitable for queries that select from well-
deﬁ ned subsets of data; columns with heterogeneous categories of values; and columns with 
distinct ranges of values. 
Compressed storage of tables and indexes: SQL Server 2008 added support for on-disk 
storage compression in both row and page format for tables, indexes, and indexed views. 
Compression of partitioned tables and indexes can be conﬁ gured independently for each 
partition. Chapter 12, “Monitoring Your SQL Server,” covers this topic in detail. 
Spatial indexes: SQL Server 2008 introduced support for spatial data and spatial indexes. 
Spatial data in this context represents geometric objects or physical location. SQL Server 
supports two spatial data types: geography and geometry. A spatial column is a table 
column that contains data of a spatial data type, such as geometry or geography. A spatial 
index is a type of extended index that enables you to index a spatial column. SQL Server 
uses the NET CLR (Common Language Runtime) to implement this data type. Refer to the 
topic “Working with Spatial Indexes (Database Engine)” in Books Online (BOL) for details.
SQL Server 2005 introduced these index-related features: 
Partitioned tables and indexes: Beginning with SQL Server 2005, you can create tables on 
multiple partitions and indexes on each partition. This enables you to manage operations on 
large datasets, such as loading and unloading a new set of data, more efﬁ ciently by indexing 
just the new partition, rather than having to re-index the whole table. You can ﬁ nd a lot 
more information about partitioned tables and indexes later in this chapter. 
Online index operations: Online index operations were added as an availability feature in 
SQL Server 2005. They enable users to continue to query against a table while indexes are 
built or rebuilt. The main scenario for using this new feature is when you need to make 
index changes during normal operating hours. The new syntax for using online index 
operations is the addition of the ONLINE = ON option with the CREATE INDEX, ALTER 
INDEX, DROP INDEX, and ALTER TABLE operations. 
Parallel index operations: Parallel index operations are another useful feature from SQL 
Server 2005. They are available only in Enterprise Edition and only apply to systems 
running on multiprocessor machines. The key scenario for using this feature is when you 
need to restrict the amount of CPU resources that index operations consume. This might be 
either for multiple index operations to coexist, or more likely when you need to allow 
other tasks to complete while performing index operations. They enable a DBA to specify 
the MAXDOP for an index operation. This is useful on large systems, enabling you to limit the 
maximum number of processors used in index operations. It’s effectively a MAXDOP speciﬁ cally 
for index operations, and it works with the server-conﬁ gured MAXDOP setting. The new 
➤
➤
➤
➤
➤
➤
➤

454  ❘  CHAPTER 14  INDEXING YOUR DATABASE
syntax for parallel index operations is the MAXDOP = n option, which can be speciﬁ ed on 
CREATE INDEX, ALTER INDEX, DROP INDEX (for clustered indexes only), ALTER TABLE ADD 
(constraint), ALTER TABLE DROP (clustered index), and CONSTRAINT operations. 
Asynchronous statistics update: This is a performance SET option -AUTO UPDATE 
STATISTICS_ASYNC. When this option is set, outdated statistics are placed on a queue 
and are automatically updated by a worker thread later. The query that generated the 
autoupdate request continues before the stats are updated. Asynchronous statistics updates 
cannot occur if any data deﬁ nition language (DDL) statements such as CREATE, ALTER, or 
DROP occur in the same transaction.
Full-text indexes: Beginning with SQL Server 2005, Full-Text Search supports the creation 
of indexes on XML columns. It was also upgraded to use MSSearch 3.0, which includes 
additional performance improvements for full-text index population. It also means that 
there is now one instance of MSSearch for each SQL Server instance.  
Nonkey columns in non-clustered indexes: With SQL Server 2005 and SQL Server 2008, 
nonkey columns can be added to a non-clustered index. This has several advantages. It 
enables queries to retrieve data faster because the query can now retrieve everything it needs 
from the index pages without having to do a bookmark lookup into the table to read the 
data row. The nonkey columns are not counted in the limits for the non-clustered index 
number of columns (16 columns) or key length (900 bytes). The new syntax for this option 
is INCLUDE (column Name, ... ), which is used with the CREATE INDEX statement.
Index lock granularity changes: In SQL Server 2005, the CREATE INDEX and ALTER INDEX 
T-SQL statements were enhanced by the addition of new options to control the locking that 
occurs during the index operation. ALLOW _ROW_LOCKS and ALLOW_PAGE_LOCKS specify the 
granularity of the lock to be taken during the index operation. 
Indexes on XML columns: This type of index on the XML data in a column enables the 
Database Engine to ﬁ nd elements within the XML data without having to shred the XML 
each time. 
Dropping and rebuilding large indexes: The Database Engine was modiﬁ ed in SQL Server 
2005 to treat indexes occupying more than 128 extents in a new, more scalable way. If 
a drop or rebuild is required on an index larger than 128 extents, the process is broken 
down into logical and physical stages. In the logical phase, the pages are simply marked as 
deallocated. After the transaction commits, the physical phase of deallocating the pages 
occurs. The deallocation takes place in batches, occurring in the background, thereby 
avoiding taking locks for a long period of time. 
Indexed view enhancements: Indexed views have been enhanced in several ways. They 
can now contain scalar aggregates and some user-deﬁ ned functions (with restrictions). In 
addition, the query optimizer can now match more queries to indexed views if the query 
uses scalar expressions, scalar aggregates, user-deﬁ ned functions, interval expressions, and 
equivalency conditions. 
Version Store: Version Store provides the basis for the row-versioning framework used 
by Online Indexing, Multiple Active Result Sets (MARS), triggers, and the new row-
versioning-based isolation levels. 
➤
➤
➤
➤
➤
➤
➤
➤


456  ❘  CHAPTER 14  INDEXING YOUR DATABASE
Row-based Indexes
A row-based index is a traditional index in which data is stored as rows in data pages. These 
indexes include the following:
Clustered Indexes 
Clustered indexes store and sort data based on the key column(s). There can only be one clustered 
index per table because data can be sorted in only one order. A clustered index is created by default 
when a table deﬁ nition includes a primary key constraint.
Non-clustered Indexes 
Non-clustered indexes contain index key values and row locators that point to the actual data row. 
If there is no clustered index, the row locator is a pointer to the row. When there is a clustered index 
present, the row locator is the clustered index key for the row. 
Non-clustered indexes can be optimized to satisfy more queries, improve query response times, and 
reduce index size. The two most important of these optimized non-clustered indexes are described 
in the next two sections.
Covering Indexes
Covering indexes are non-clustered indexes that include nonkey columns in the leaf level. These 
types of indexes improve query performance, cover more queries, and reduce IO operations as 
the columns necessary to satisfy a query are included in the index itself either as key or nonkey 
columns. Covering indexes can greatly reduce bookmark lookups. 
The ability to include nonkey columns enables indexes to be more ﬂ exible by including columns 
with data types not supported as index key columns Nonkey columns also enable indexes to extend 
beyond the 16 key column limitation. Nonkey columns do not count towards the 900 byte index key 
size limit. 
Filtered Indexes 
Filtered indexes can take a WHERE clause to indicate which rows are to be indexed. Since you index 
only a portion of rows in a table, you can create only a non-clustered ﬁ ltered index. If you try to 
create a ﬁ ltered clustered index, SQL Server returns a syntax error. 
Why do you need a non-clustered index with a subset of data in a table? A well-designed ﬁ ltered 
index can offer the following advantages over full-table indexes: 
Improved query performance and plan quality: If the index is deep (more pages because 
of more data), traversing an index takes more I/O and results in slow query performance. 
If you have a large table and you know that there are more user queries on a well-deﬁ ned 
subset of data, creating a ﬁ ltered index makes queries run faster because less I/O will be 
performed since the number of pages is less for the smaller amount of data in that ﬁ ltered 
index. Moreover, stats on the full table may be less accurate compared to ﬁ ltered stats with 
less data, which also helps improve query performance. 
➤

Partitioned Tables and Indexes ❘ 457
Reduced index maintenance costs: Maintaining a ﬁ ltered index is less costly than 
maintaining a full index because of smaller data size. Obviously, it also reduces the cost of 
updating statistics because of the smaller size of the ﬁ ltered index. As mentioned earlier, 
you must know your user queries and what kind of data they query often to create a well-
deﬁ ned ﬁ ltered index with a subset of that data. If the data outside of the ﬁ ltered index is 
modiﬁ ed frequently, it won’t cost anything to maintain the ﬁ ltered index. That enables you 
to create many ﬁ ltered indexes when the data in those indexes is not modiﬁ ed frequently. 
Reduced index storage costs: Less data, less space. If you don’t need a full-table non-clustered 
index, creating a smaller dataset for a non-clustered ﬁ ltered index takes less disk space.
Column-based Indexes 
Column-based indexes are a new type of index introduced in SQL Server 2012 in which 
only column data is stored in the data pages. These indexes are based on the Vertipaq engine 
implementation, which is capable of high compression ratios and handles large data sets in memory. 
How Indexes are Used by SQL Server?
A good understanding of how indexes are used by SQL Server is also important in a good index 
design. In SQL Server, the Query Optimizer component determines the most cost-effective option 
to execute a query. The Query Optimizer evaluates a number of query execution plans and selects 
the execution plan with the lowest cost.
The execution plan selected by the Query Optimizer may or may not make efﬁ cient use of indexes, or 
it may not use indexes at all. The following sections describe how execution plans can use indexes.
Table Scan
Indexes are not required by SQL Server to retrieve data requested by a query. In the absence of 
indexes or if determined to be least cost effective, SQL server scans every row of a table until the 
query is satisﬁ ed. This is known as a table scan. As you may suspect, table scans can bring forth 
expensive IO operations for large tables. SQL Server has to read every single data page until it ﬁ nds 
the data that satisﬁ es the query. A table scan can take from a couple of seconds to several minutes. 
Some users may even experience time-outs by applications with short response-time thresholds.
Table scans generally occur when there is no clustered indexed available; in other words, when the 
table is a heap.
Index Scan and Index Seek
An index scan is similar to a table scan in that SQL Server has to read every single data page in 
the index until it ﬁ nds the data that satisﬁ es the query. Index scans can be both IO and memory 
intensive operations.
An index seek on the other hand, is a more efﬁ cient way of retrieving data because only data pages 
and rows that satisfy the query are read. Index seeks result in less data pages read, hence reducing 
IO and memory consumption.
Depending on how selective a query is, meaning what percentage of the total number of rows in a 
table is requested, SQL Server Query Optimizer can choose to do an index scan rather than an index 
➤
➤

458  ❘  CHAPTER 14  INDEXING YOUR DATABASE
seek. The tipping point at which an index scan is preferred by the SQL Server Query Optimizer is 
not always a deﬁ nitive percentage. There are many factors such as parallelism settings, memory 
availability, and number of rows that contribute in the decision for the more cost-effective option.
Bookmark Lookup
It is quite common to see queries that require additional columns than the ones included in a 
non-clustered index. To retrieve these additional columns, SQL Server needs to retrieve additional 
data pages to cover all requested columns. Bookmark lookups can become expensive operations 
when dealing with a large number of rows because more data pages need to be retrieved from disk 
and loaded into memory.
To avoid excessive bookmark lookup operations, the required columns that need to be covered by 
the query can be included in the index deﬁ nition. These types of indexes are known as covering 
indexes.
Creating Indexes
At this point you should be familiar with the different types of indexes and how they are used in 
execution plans. This understanding is crucial to design and ﬁ ne tune indexes to improve query 
performance.
Indexes are created manually using T-SQL commands or by using a graphical user interface such as 
SQL Server Management Studio. SQL Server 2012 also includes a tool called the Database Engine 
Tuning Advisor (DTA) that suggests and generates missing indexes for you. This tool is discussed 
later in this chapter.
To create an index using T-SQL commands, perform the following steps:
 1. 
Open SQL Server Management Studio and connect to the SQL server instance.
 2. 
Open a new query window and follow one of the sample syntaxes provided in the following list:
To create a clustered index you use the CREATE CLUSTERED INDEX T-SQL 
command as follows:
CREATE CLUSTERED INDEX idx_EmployeeID
ON EmployeeTable (EmployeeID)
To create a non-clustered index you use the CREATE NONCLUSTERED INDEX T-SQL 
command. NONCLUSTERED is the default index type and can be omitted:
CREATE NONCLUSTERED INDEX idx_LastName
ON EmployeeTable (LastName)
Or
CREATE INDEX idx_LastName
ON EmployeeTable (LastName)
To create a covering index you use the CREATE NONCLUSTERED INDEX T-SQL 
command along with the INCLUDE keyword as follows:
➤
➤
➤
 
 
 
 

Partitioned Tables and Indexes ❘ 459
CREATE NONCLUSTERED INDEX idx_LastName
ON EmployeeTable (LastName)
INCLUDE (FirstName, HireDate)
To create a ﬁ ltered index you use the CREATE NONCLUSTERED INDEX T-SQL 
command along with the WHERE keyword as follows:
CREATE NONCLUSTERED INDEX idx_GenderFemale
ON EmployeeTable (Gender)
WHERE Gender = ‘Female’
Why Use Both Partitioned Tables and Indexes?
Partitioned tables are a way to spread a single table over multiple partitions, and while doing so 
each partition can be on a separate ﬁ legroup. Following are several reasons for doing this: 
Faster and easier data loading: If your database has a large amount of data to load, you 
might want to consider using a partitioned table. “A large amount of data,” doesn’t mean 
a speciﬁ c amount of data, but any case in which the load operation takes longer than is 
acceptable in the production cycle. A partitioned table enables you to load the data to an 
empty table that’s not in use by the “live” data, so it has less impact on concurrent live 
operations. Clearly, there will be an impact on the I/O subsystem, but if you also have 
separate ﬁ legroups on different physical disks, even this has a minimal impact on overall 
system performance. After the data is loaded to the new table, you can perform a switch 
to add the new table to the live data. This switch is a simple metadata change that quickly 
executes, which is why partitioned tables are a great way to load large amounts of data with 
limited impact to users who touch the rest of the data in the table. 
Faster and easier data deletion or archival: For the same reasons, partitioned tables also 
help you to delete or archive data. If your data is partitioned on boundaries that are also the 
natural boundaries on which you add or remove data, the data is considered to be aligned. 
When your data is aligned, deleting or archiving data is as simple as switching a table out 
of the current partition, after which you can unload or archive it at your leisure. There is a 
bit of a catch to this part: With archiving, you often want to move the old data to slower or 
different storage. The switch operation is so fast because all it does is change metadata. It 
doesn’t move any data around, so to actually move the data from the ﬁ legroup where it lived 
to the old, slow disk archival ﬁ legroup, you need to move the data, but you move it when the 
partition isn’t attached to the existing partitioned table. Therefore, although this may take 
quite some time, it can have a minimal impact on any queries executing against the live data. 
Faster queries: You are probably interested in an opportunity to get faster queries. When 
querying a partitioned table, the query optimizer can eliminate searching through partitions 
that it knows won’t hold any results. This is referred to as partition elimination. This works 
only if the data in the partitioned table or index is aligned with the query. That is, the data 
must be distributed among the partitions in a way that matches the search clause on the 
query. You learn more details about this as you consider how to create a partitioned table. 
SQL Server 2008 offers some improvements for parallel query processing enhancements on 
partitioned tables and indexes. Refer to the section “Query Processing Enhancements on 
Partitioned Tables and Indexes” in Chapter 13 for details on this topic. 
➤
➤
➤
➤

460  ❘  CHAPTER 14  INDEXING YOUR DATABASE
Sliding windows: A sliding window is basically what was referred to earlier in the discussion 
about adding new data and then deleting or archiving old data. What you did was ﬁ ll a 
new table, switch it into the live table, and then switch an existing partition out of the live 
table for archival or deletion. It’s kind of like sliding a window of new data into the current 
partitioned table, and then sliding an old window of data out of the partitioned table.
Creating Partitioned Tables
Table partitioning requires SQL Server 2012 Enterprise Edition. There are also some expectations 
about the hardware in use, in particular the storage system; although these are implicit 
expectations, and you can store the data anywhere you want. You just won’t get the same 
performance beneﬁ ts you would get if you had a larger enterprise storage system with multiple disk 
groups dedicated to different partitions.
SQL Server 2012 supports up to 15,000 partitions by default and is fully supported in 64-bit 
systems. In 32-bit systems, it is possible to create more than 1,000 table or index partitions, but it is 
not fully supported.
To create a partitioned table or index, perform the following steps: 
 1. 
Specify how the table or index is partitioned by the partitioning column, and the range of 
values included for each partition. Only one partitioning column can be speciﬁ ed. For example, 
to create four partitions based on a DateKey column, you execute the following command:
CREATE PARTITION FUNCTION DateKeyRange_PF (int)
AS RANGE LEFT FOR VALUES (20021231, 20031231, 20041231);
Either LEFT or RIGHT boundaries can be speciﬁ ed in the partition function. If no partition 
boundary is speciﬁ ed LEFT is used as default. Table 14-3 describes the partitions created by 
the preceding partition function.
TABLE 14-3: Partition Results for DateKeyRange Function.
PARTITION NO.
DESCRIPTION
1
All records with DateKey <= 20021231
2
Records between Datekey>20021231 and Datekey<=20031231
3
Records between Datekey>20031231 and Datekey<=20041231
4
All records with DateKey > 20041231
 2. 
To determine the partition number where a record will be placed based on the DateKey 
column value, use the $PARTITION function as follows:
SELECT ‘20010601’ DateKey, $PARTITION.DateKeyRange_PF(20010601) 
PartitionNumber
UNION
SELECT ‘20030601’ DateKey, $PARTITION.DateKeyeRange_PF(20030601) 
PartitionNumber
UNION
➤

Index Maintenance ❘ 461
SELECT ‘20040601’ DateKey, $PARTITION.DateKeyRange_PF(20040601) 
PartitionNumber
UNION
SELECT ‘20050601’ DateKey, $PARTITION.DateKeyeRange_PF(20050601)
The results of the $PARTITION function are shown in Figure 14-3.
FIGURE 14-3
 3. 
Now create a partition scheme. For example, create a partition scheme with four ﬁ legroups 
that can be used to hold the four partitions deﬁ ned in the DateKeyRange_PF partition 
function as follows:
CREATE PARTITION SCHEME DateKeyRange_ps
AS PARTITION DateKeyRange_PF
TO (FileGroup1, FileGroup2, FileGroup3, FileGroup4, FileGroup5)
INDEX MAINTENANCE
Another important task of database administrators is to monitor existing index health and identify 
where new indexes are needed. Every time data is inserted, updated, or deleted in SQL Server tables, 
indexes are accordingly updated. 
Over time, the distribution of data in data pages can become unbalanced. Some data pages become 
loosely ﬁ lled, whereas others are ﬁ lled to the maximum. Too many loosely ﬁ lled data pages create 
performance issues as more data pages need to be read to retrieve the requested data. 
On the other hand, pages ﬁ lled close to their maximum may create page splits when new data is 
inserted or updated. When page splits occur, about half of the data is moved to a newly created data 
page. This constant reorganization consumes resources and creates data page fragmentation.
The goal is to store as much data into the smallest number of data pages with room for growth 
to prevent excessive page splits. This delicate balance can be achieved by ﬁ ne-tuning the index ﬁ ll 
factor. For more information on ﬁ ne-tuning the index ﬁ ll factor, refer to Books Online at http://
msdn.microsoft.com/en-us/library/ms177459(v=SQL.110).aspx.

462  ❘  CHAPTER 14  INDEXING YOUR DATABASE
Monitoring Index Fragmentation
You can monitor index fragmentation through the provided Data Management Views (DMVs) 
available in SQL Server 2012. One of the most useful DMVs is sys.dm_db_index_physical_
stats, which provides average fragmentation information for each index.
For example, you can query the sys.dm_db_index_physical_stats DMV as follows:
SELECT index_id,avg_fragmentation_in_percent
FROM sys.dm_db_index_physical_stats
(
DB_ID(‘AdventureWorks’), 
OBJECT_ID(‘AdventureWorks’),
NULL, NULL, ‘DETAILED’
)
Figure 14-4 shows the results of this query.
FIGURE 14-4
From execution results of this DMV, you can observe indexes with high fragmentation. Indexes with 
high defragmentation percentages need to be defragmented to avoid performance issues. Heavily 
fragmented indexes are stored and accessed inefﬁ ciently by SQL Server depending on the type 
of fragmentation, internal or external. External fragmentation means that data pages are not stored 
in logical order. Internal fragmentation means that pages store much less data then they can hold. 
Both types of fragmentation cause query execution to take longer.
Cleaning Up Indexes
Index cleanup should always be part of all database maintenance operations. You need to perform 
these index cleanup tasks on a regular basis, depending on how fragmented indexes become due to 

Index Maintenance ❘ 463
changes to the data. If your indexes become highly fragmented, you can defragment it through one 
of the operations described here:
Reorganize an index: 
Reorders and compacts leaf level pages
Performs index reordering online, no long-term locks
Good for indexes with low fragmentation percentages
Rebuild an index:
Drops and re-creates the index
Reclaims disk space
Reorders and compacts rows in contiguous pages
Online index rebuilt option available in Enterprise Edition
Better for highly fragmented indexes
Table 14-2 lists the general syntax for index operations for the DimCustomer table.
TABLE 14-2: Index /operations Syntax for DimCustomer Table
OPERATION
SYNTAX
Create Index
CREATE INDEX IX_CustomerAlternateKey ON
DimCustomer_CustomerAlternateKey
Reorganize Index
ALTER INDEX IX_DimCustomer_CustomerAlternateKey ON 
DimCustomer REORGANIZE
Rebuild Index
ALTER INDEX IX_DimCustomer_CustomerAlternateKey ON 
DimCustomer REBUILD
Drop Index
DROP INDEX IX_DimCustomer_CustomerAlternateKey FROM 
DIMCustomer
Indexes may become heavily fragmented over time. Deciding whether to reorganize or rebuild 
indexes depends in part on their level of fragmentation and your maintenance window. Generally 
accepted fragmentation thresholds to perform an index rebuild range between 20 percent and 30 
percent. If your index fragmentation level is below this threshold, performing a reorganize index 
operation may be good enough. 
But, why not just rebuild indexes every time? You can, if your maintenance window enables you to 
do so. Keep in mind that index rebuild operations take longer to complete, time during which locks 
are placed and all inserts, updates, and deletions have to wait. If you are running SQL Server 2012 
Enterprise Edition, you can take advantage of online index rebuild operations. Unlike standard 
rebuild operations, online index operations allow for inserts, updates, and deletions during the time 
the index is being rebuilt.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

464  ❘  CHAPTER 14  INDEXING YOUR DATABASE
IMPROVING QUERY PERFORMANCE WITH INDEXES 
SQL Server 2012 includes several Dynamic Management Views (DMVs) that enable you to ﬁ ne-tune 
queries. DMVs are useful to surface execution statistics for a particular query such as the number of 
times it has been executed, number of reads and writes performed, amount of CPU time consumed, 
index query usage statistics, and so on.
You can use the execution statistics obtained through DMVs to ﬁ ne-tune a query by refactoring 
the T-SQL code to take advantage of parallelism and existing indexes, for example. You can also 
use them to identify missing indexes, indexes 
not utilized, and identify indexes that require 
defragmentation.
For example, explore the existing indexes 
in the FactInternetSales table from the 
AdventureWorks database. As shown in Figure 14-5, 
the FactInternetSales table has been indexed 
fairly well.
To illustrate the query tuning process, run through 
a series of steps to generate execution statistics that 
can surface through DMVS:
 1. 
Drop the existing ProductKey and OrderDateKey indexes from the FactInternet Sales 
table as follows:
USE [AdventureWorks] 
GO
-- Drop ProductKey index
IF  EXISTS (SELECT * FROM sys.indexes 
WHERE object_id = OBJECT_ID(N’[dbo].[FactInternetSales]’) AND 
name = N’IX_FactInternetSales_ProductKey’) 
DROP INDEX [IX_FactInternetSales_ProductKey] ON [dbo].[FactInternetSales]
GO
-- Drop OrderDateKeyIndex
IF  EXISTS (SELECT * FROM sys.indexes WHERE object_id = OBJECT_ID(N’[dbo].[FactInter
netSales]’) 
AND name = N’IX_FactInternetSales_OrderDateKey’)
DROP INDEX [IX_FactInternetSales_OrderDateKey] ON [dbo].[FactInternetSales]
GO
 2. 
Execute the following script three times, like so:
/*** Internet_ResellerProductSales ***/
SELECT 
 D.[ProductKey],
 D.EnglishProductName,    
 Color,
 Size,
 Style, 
FIGURE 14-5

Improving Query Performance with Indexes ❘ 465
 ProductAlternateKey,
 sum(FI.[OrderQuantity]) InternetOrderQuantity,
 sum(FR.[OrderQuantity]) ResellerOrderQuantity,
 sum(FI.[SalesAmount]) InternetSalesAmount,  
 sum(FR.[SalesAmount]) ResellerSalesAmount 
     
FROM [FactInternetSales] FI
 INNER JOIN DimProduct D 
  ON FI.ProductKey = D.ProductKey
 INNER JOIN FactResellerSales FR 
  ON FR.ProductKey = D.ProductKey
WHERE 
 FI.OrderDateKey BETWEEN 20000101 AND 20041231
 AND FR.OrderDateKey BETWEEN 20000101 AND 20041231
GROUP BY 
 D.[ProductKey],
 D.EnglishProductName,
 Color,
 Size,
 Style,
 ProductAlternateKey    
Figure 14-6 displays T-SQL script executed along with results and an execution time of 
11 seconds. Your execution results may vary depending on the resources available to your 
machine.
FIGURE 14-6

466  ❘  CHAPTER 14  INDEXING YOUR DATABASE
 3. 
Run the following script to analyze the execution statistics of the previous query.
SELECT TOP 10 
 SUBSTRING(qt.TEXT, (qs.statement_start_offset/2)+1,
 ((CASE qs.statement_end_offset WHEN -1 THEN DATALENGTH(qt.TEXT)
 ELSE qs.statement_end_offset
 END - qs.statement_start_offset)/2)+1) QueryText,
 qs.last_execution_time,
 qs.execution_count ,
 qs.last_logical_reads,
 qs.last_logical_writes,
 qs.last_worker_time,
 qs.total_logical_reads, 
 qs.total_logical_writes, 
 qs.total_worker_time,
 qs.last_elapsed_time/1000000 last_elapsed_time_in_S,
 qs.total_elapsed_time/1000000 total_elapsed_time_in_S,
 qp.query_plan
FROM 
 sys.dm_exec_query_stats qs
 CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) qt
 CROSS APPLY sys.dm_exec_query_plan(qs.plan_handle) qp
ORDER BY 
 qs.last_execution_time DESC,
 qs.total_logical_reads DESC
Figure 14-7 shows the execution statistics reported mainly by the sys.dm_exec_query_
stats DMV.
FIGURE 14-7
From this DMV you can observe that there was a large number of reads and long period of 
time the processor was busy executing the query. Keep these baseline numbers in mind. At 
the end of this example, you can reduce these numbers.
 4. 
Query the sys.dm_db_missing_index_details DMV to check if missing indexes are 
reported like so: 
SELECT 
 DatabaseName = DB_NAME(database_id,)
 [Number Indexes Missing] = count(*) 


468  ❘  CHAPTER 14  INDEXING YOUR DATABASE
   ALLOW_ROW_LOCKS = ON, 
   ALLOW_PAGE_LOCKS = ON
  ) ON [PRIMARY]
GO
 6. 
Execute the Internet_ResellerProductSales query deﬁ ned in step 2 three more times. 
Figure 14-9 shows that execution time went down to 8 seconds! That is a 28% improvement 
in query execution simply by adding the appropriate index.
FIGURE 14-9
DATABASE TUNING ADVISOR
One of the more useful tools available for database administrators since SQL Server 2005 is the 
Microsoft Database Engine Tuning Advisor (DTA). DTA enables you to analyze a database for 
missing indexes and other performance-tuning recommendations such as partitions and indexed 
views. DTA accepts the following types of workloads:
SQL script ﬁ les (*.sql)
Trace ﬁ les (*.trc)
XML ﬁ les (*.xml)
Trace table
Plan Cache (new)
Figure 14-10 displays the Database Engine Tuning Advisor’s workload selection screen, including 
the new Plan Cache option. One of the great beneﬁ ts of the Database Tuning Advisor to database 
administrators and SQL Server developers is the ability to rapidly generate database performance 
improvement recommendations without knowing the underlying database schema, data structure, 
usage patterns, or even the inner workings of the SQL Server Optimizer.
➤
➤
➤
➤
➤

Too Many Indexes? ❘ 469
In addition, new in SQL Server 2012, the plan cache can also be used as part of a DTA workload. 
This new workload option eliminates the need to manually generate a workload for analysis, such as 
trace ﬁ les. 
TOO MANY INDEXES?
The saying “Too much of a good thing is not always good” holds true when discussing indexes. Too 
many indexes create additional overhead associated with the extra amount of data pages that the 
Query Optimizer needs to go through. Also, too many indexes require too much space and add to 
the time it takes to accomplish maintenance tasks.
Still, the Data Tuning Wizard typically recommends a large number of indexes, especially when 
analyzing a workload with many queries. The reason behind this is because queries are analyzed on 
an individual basis. It is a good practice to incrementally apply indexes as needed, always keeping a 
baseline to compare if the new index improves query performance.
SQL Server 2012 provides several Dynamic Management Views (DMV) to obtain index usage 
information. Some of these DMVs include:
sys.dm_db_missing_index_details: Returns detailed information about a missing index.
sys.dm_db_missing_index_columns: Returns information about the table columns that 
are missing an index.
➤
➤
FIGURE 14-10

470  ❘  CHAPTER 14  INDEXING YOUR DATABASE
sys.dm_db_missing_index_groups: Returns information about a speciﬁ c group of missing 
indexes.
sys.dm_db_missing_index_group_stats: Returns summary information about missing 
index groups.
sys.dm_db_index_usage_stats: Returns counts of different types of index operations and 
the time each type of operation was last performed.
sys.dm_db_index_operational_stats: Returns current low-level I/O, locking, latching, 
and access method activity for each partition of a table or index in the database.
sys.dm_db_index_physical_stats: Returns size and fragmentation information for the 
data and indexes of the speciﬁ ed table or view.
For example, to obtain a list of indexes that have been used and those that have not been used by user 
queries, query the sys.dm_db_index_usage_stats DMV. From the list of indexes that have been 
used you can obtain important statistics that help you ﬁ ne tune your indexes. Some of this information 
includes index access patterns such index scans, index seeks, and index bookmark lookups.
To obtain a list of indexes that have been used by user queries, execute the following script:
SELECT 
 SO.name Object_Name,
 SCHEMA_NAME(SO.schema_id) Schema_name,
 SI.name Index_name, 
 SI.Type_Desc, 
 US.user_seeks,
 US.user_scans, 
 US.user_lookups, 
 US.user_updates  
FROM sys.objects AS SO
 JOIN sys.indexes AS SI
  ON SO.object_id = SI.object_id 
 INNER JOIN sys.dm_db_index_usage_stats AS US
  ON SI.object_id = SI.object_id   
  AND SI.index_id = SI.index_id
WHERE 
 database_id=DB_ID(‘AdventureWorks’)
 SO.type = ‘u’
 AND SI.type IN (1, 2) 
 AND (US.user_seeks > 0 OR US.user_scans > 0 OR US.user_lookups > 0 );
To obtain a list of indexes that have not been used by user queries, execute the following script:
SELECT
 SO.Name TableName,
 SI.name IndexName, 
 SI.Type_Desc IndexType
 US.user_updates 
FROM sys.objects AS SO 
 INNER JOIN sys.indexes AS SI
  ON SO.object_id = SI.object_id 
 LEFT OUTER JOIN sys.dm_db_index_usage_stats AS US    
➤
➤
➤
➤
➤

Summary ❘ 471
  ON SI.object_id = US.object_id   
  AND SI.index_id = US.index_id
WHERE  
 database_id=DB_ID(‘AdventureWorks’)
 SO.type = ‘u’ 
 AND SI.type IN (1, 2) 
 AND (US.index_id IS NULL) 
 OR  (US.user_seeks = 0 AND US.user_scans = 0 AND US.user_lookups = 0 );
Indexes that are not used by user queries should be dropped, unless they have been added to support 
mission critical work that occurs at speciﬁ c points in time, such as monthly or quarterly data extracts 
and reports. Unused indexes add overhead to insert, delete, and update operations as well as index 
maintenance operations. Index usage statistics are initialized to empty when the SQL Server service 
restarts. The database is detached or shutdown when the AUTO_CLOSE property is turned on.
SUMMARY
The newest index type introduced in SQL Server 2012 is the columnstore index. Columnstore 
indexes are column-based, non-clustered indexes that store data based on discrete values found in 
a column. This new type of index has greater advantages over regular row-based indexes. These 
advantages include smaller-sized indexes and faster retrieval of data.
An important part of indexing your database includes creating partitions and indexes, along with 
advanced indexing techniques such as ﬁ ltered indexes using the WHERE keyword and covering indexes 
using the INCLUDED keyword. Reorganizing and rebuilding indexes is an important maintenance 
operation to reduce and eliminate index fragmentation. SQL Server 2012 Database Tuning Advisor 
has been enhanced and can now help you tune your databases based on the plan cache.
You can put the ﬁ nishing touches on your indexed database by tuning a query, which you can 
accomplish by utilizing the data from Data Management Views (DMVs). It is also important to 
remember the beneﬁ ts of ﬁ nding indexes that are not used by user queries and removing them. 


474  ❘  CHAPTER 15  REPLICATION
which ones to include in the current month’s magazine. The selected set of articles is then published 
in a publication. After a monthly publication is printed, it is shipped out via various distribution 
channels to subscribers all over the world.
SQL Server replication uses similar terminology. The pool from which a publication is formed 
can be considered a database. Each piece selected for publication is an article; it can be a table, 
a stored procedure, or another database object. Like a magazine publisher, replication also 
needs a distributor to deliver publications, keep track of delivery status, and track a history of 
synchronization to maintain data consistency. Depending on the kind of replication model you 
choose, articles from a publication can either be stored as ﬁ les in a folder to which both publisher 
and subscriber(s) have access, or as records in tables in a distribution database synchronously or 
asynchronously. Regardless of how publications are delivered, replication always needs a distributor 
database to keep track of delivery status. Depending on the capacity of the publication server, the 
distributor database can be located on the Publisher, the Subscriber, or on another server that might 
be dedicated purely to serving as the distribution database.
Conceptually, however, there are differences between SQL Server replication and a magazine 
publishing company, the biggest being the contributor-like role the subscriber can sometimes take 
on. For example, in some replication models, a subscriber or subscribers can update articles and 
have them propagated back to the publisher or other subscribers. In the peer-to-peer replication 
model, each participant of replication acts both as publisher and subscriber so that changes made in 
different databases replicate back and forth between multiple servers. 
Replication Components
Now that you have an idea of how replication works in comparison to magazine publishing, it is 
time to examine what these terms and functions mean in direct relation to the SQL Server. SQL 
Server replication is comprised of several key components which are grouped into the following 
areas: replication roles, replication data, replication agents, and replication internal components.
Replication Roles
Following are three key roles in replication: 
Publisher: The publisher is the server, or database instance, that is the source, or master, for 
the articles being published.
Distributor: The distributor is the intermediary in the act of publishing and subscribing, 
and in some types of replication is the medium whereby the data gets from the publisher to 
the subscriber. The distributor stores the data to be replicated from the Publisher, and also 
stores the location of snapshots. The distributor is a database that can live on the publishing 
server, its own dedicated server, or the subscriber.
Subscriber: The subscriber is the server, or database instance, that is the destination, for the 
articles being published. In some replication models the subscriber can also be a publisher. 
The subscriber can republish articles when they need to be sent onto another subscriber in 
the Updating Subscriber model. The subscriber can also republish articles in a peer-to-peer 
model (explained in the “Replication Types” section).
➤
➤
➤


476  ❘  CHAPTER 15  REPLICATION
Queue Reader Agent: The Queue Reader Agent is used to read messages stored in a 
SQL Server queue, or a Microsoft Message Queue. It then applies those messages to the 
Publisher. The Queue Reader Agent is used in either snapshot or transactional replication 
publications, which allow queued updating (see the section on “Replication Types” for 
details on snapshot and transactional replication).
Replication Maintenance Jobs
Replication uses a number of additional SQL Agent Jobs to perform maintenance. 
Agent History Cleanup: This job removes replication agent history that is stored in 
the distribution database. This job is scheduled to run every ten minutes. 
Distribution Cleanup: This job removes transactions from the distribution 
database after they are no longer needed. This job is scheduled to run every ten 
minutes.
Expired Subscription Cleanup: This job determines when a snapshot has expired, 
and will remove it. This job is scheduled to run once a day at 1 A.M.
Reinitialize Failed Subscriptions: This job looks for subscriptions that have failed, and 
marks them for re-initialization. This job is not enabled by default, so it can either 
be run manually when required, or you can create a custom schedule to suite your 
needs.
Replication Agent Monitor: This job monitors the execution of the SQL Agents and 
writes to the Windows event log when a job step fails. This job is scheduled to run 
every 10 minutes.
Replication Types
SQL Server 2012 provides three types of replication. The three types are as follows: 
Snapshot replication
Transactional replication
Merge replication
Peer-to-Peer replication and Oracle Publishing replication are variations of the three types of 
replication previously listed and are also discussed in this section.
Snapshot Replication
As its name implies, snapshot replication takes a snapshot of a publication and makes it available to 
subscribers. When the snapshot is applied on the subscribing database, the articles at the subscriber, 
such as tables, views, and stored procedures, are dropped and re-created. Snapshot replication is a 
one-shot deal; there is no continuous stream of data from the publisher to the subscriber. The data 
at the publisher at the time the snapshot is taken is applied to the subscriber.
➤
➤
➤
➤
➤
➤
➤
➤
➤










486  ❘  CHAPTER 15  REPLICATION
 8. 
Next, deﬁ ne what the wizard should do from the Wizard Actions screen, shown in 
Figure 15-10. Choose to either conﬁ gure the distribution database or create scripts. If you 
want to create scripts, select both options.
 9. 
The next page of the wizard displays a list of actions the SQL Server can perform. Conﬁ rm 
that the list of actions is what you expect and select Finish. If it’s not correct, go back and 
ﬁ x the incorrect settings. You then see a screen indicating the progress the wizard has made 
at executing the actions you selected.
 10. 
Figure 15-11 shows the wizard actions completed successfully. If any errors are reported, 
investigate and resolve each one before attempting to rerun the wizard.
FIGURE 15-8
FIGURE 15-9
FIGURE 15-10
FIGURE 15-11


488  ❘  CHAPTER 15  REPLICATION
 2. 
You see the Publication Wizard welcome screen; click Next. On the Publication Database 
screen, pick a database for publication. Select Publisher as your database to create a 
publication, and click Next.
 3. 
Figure 15-14 shows the Publication Type screen where you select the type of replication. In 
this case, select Snapshot Publication and click Next to continue.
 4. 
Next is the Articles screen where you select the tables in this article. By default, nothing is 
selected, so you have to expand the Tables node and pick the tables you want to publish. 
Once you expand, select the Cars table you created earlier for publication and its children, 
as shown in Figure 15-15. You can see that the Cars table under the sales schema is selected. 
In other scenarios if necessary, you can pick and choose columns of the table for publication 
by unchecking the box next to the column name. 
FIGURE 15-14
FIGURE 15-15
You can also set properties of articles that you choose to publish. These properties affect 
how the article is published and some behaviors when it is synchronized with the subscriber. 
Don’t change the default properties here; however, they can be useful in other situations. 
Click Next to continue.
 5. 
On the Filter Table Rows page, the wizard gives you an option to ﬁ lter out rows. Click the 
Add button to display the Add Filter page.
 6. 
As mentioned earlier, because you want to replicate data to the Chinese market, you need to 
ﬁ lter the cars by country. To do this add a WHERE clause to the ﬁ lter statement to match the 
following SQL text as shown in Figure 15-16. 
SELECT <published_columns> 
FROM [Sales].[Cars] 
WHERE [Country] = ‘China’ 

Implementing Replication ❘ 489
After you click OK, the ﬁ lter is applied. This returns you to the previous screen, which now 
has a ﬁ lter deﬁ ned, as shown in Figure 15-17.
FIGURE 15-16
FIGURE 15-17
 7. 
Figure 15-18 shows the Snapshot Agent 
screen, where you deﬁ ne how the snapshot 
should be created, and when it should 
be scheduled to run. In this case, don’t 
schedule it or create one immediately; 
instead, in the example, you will invoke it 
manually by running a SQL Server Agent 
job yourself. Click Next to continue.
 8. 
The next screen is the Snapshot Agent 
Security screen, where you can specify the 
accounts used to run the snapshot agent 
job and to connect to the Publisher. As 
mentioned earlier, different replication 
models call for different agents to be 
run. Click the Security Settings button to 
conﬁ gure the accounts needed for your 
replication model. 
It is convenient to have a dedicated domain account with a secure password that doesn’t 
need to be changed often for this purpose. However, if you don’t have that access, you can 
choose to impersonate the SQL Server Agent account, as mentioned previously. Even though 
you will do so for this exercise, it is a best practice to always use a dedicated account. Enter 
FIGURE 15-18

490  ❘  CHAPTER 15  REPLICATION
the account details on the Snapshot Agent Security screen, shown in Figure 15-19. After the 
account is set, click OK.
 9. 
Figure 15-20 shows the Wizard Actions screen, where you can specify whether you want the 
wizard to create the publication or script the creation. For now leave these settings on the 
default, and just create the publication.
FIGURE 15-19
FIGURE 15-20
 10. 
The next screen is the wizard action 
conﬁ rmation screen. Here you can assign 
a name to this publication. Enter the name 
Pub_Cars_China in the Publication Name 
text box. Review the actions speciﬁ ed here, 
and if anything is incorrect go back and ﬁ x 
it. When everything is correct, click Finish.
 11. 
The next screen (see Figure 15-21) conﬁ rms 
your actions, which in this case are to 
Create the Publication and then add the 
articles, (of which we have just one). The 
status should indicate success showing that 
you have now successfully created a new 
publication. If any errors are reported, 
investigate and resolve them before 
attempting to execute the actions again.
The rest of the process is quite similar to the distributor creation documented earlier. After you 
click Finish, the snapshot publication is created. Again, you can use Object Explorer to see the 
publication and SQL Server jobs created during this process.
FIGURE 15-21

Implementing Replication ❘ 491
At this point no ﬁ les or folders have been created, and the snapshot has not been created because no 
one is subscribed to the snapshot. To get SQL Server to do anything further, you have to subscribe to 
the new publication. When there is a subscriber, SQL Server creates all the necessary ﬁ les. You can ﬁ nd 
these in the shared folder deﬁ ned when the distributor was set up earlier. The default location is 
C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\ReplData. If you 
speciﬁ ed a different location during setup, you can always retrieve the current location by expanding 
the Replication node in SQL Server management Studio Object Explorer, expanding Local Publications, 
and selecting the relevant publication. Right-click and select properties. On the Snapshot page (see 
Figure 15-22) you see a section titled Location of Snapshot Files that shows you the current location.
FIGURE 15-22
Setting Up a Subscription to the Snapshot Publication
Now that a snapshot publication is created, you can subscribe to it 
from a different server: 
 1. 
If you are not already connected to it, connect to the 
subscription server using SQL Server Management Studio, 
and expand the Replication node in Object Explorer. 
Right-click Local Subscription and select New 
Subscriptions (see Figure 15-23).
 2. 
You see the welcome screen of the New Subscription 
Wizard. Click Next to choose the publication you want to 
subscribe to. 
FIGURE 15-23

492  ❘  CHAPTER 15  REPLICATION
This example has the Publisher and Subscriber on the same server, so the page should 
already be populated with the available publications. In a real scenario in which the 
Publisher and Subscribers are on different servers, you need to ﬁ nd the Publishing 
server. Do this by selecting <Find SQL Server Publisher…> from the drop-down list 
(see Figure 15-24). You see the typical Connect to Server window that is common in 
SQL Server Management Studio. To connect to the server where you set up the snapshot 
publication earlier, select the Publishing server, and click Connect. You then see the 
Publication screen, as shown in Figure 15-25. Expand the database (Publisher) and 
select the publication you just created by the name you gave it: Pub_Cars_China. Click 
Next to continue.
FIGURE 15-24
FIGURE 15-25
 3. 
The next screen is the Distribution Agent Location screen, shown in Figure 15-26. Here, 
select the location for the distribution agents to execute. This can be either at the distributor 
for a push subscription or at the subscriber for a pull subscription. Make your subscription 
a push subscription by selecting the ﬁ rst option, Run All Agents at the Distributor, and click 
Next.
 4. 
The next screen is the Subscribers screen, (see Figure 15-27) where you can set the 
subscriber properties. Select the Server where the subscriber will be; in this case it is 
the current server. Then select the subscription database. If you want to add additional 
subscribers, you can do so on this page as well, by using the Add Subscriber button. Select 
Next to move to the next screen.

Implementing Replication ❘ 493
 5. 
Figure 15-28 shows the Distribution Agent Security screen, where you see the familiar screen 
used for setting agent security settings. Click the ellipsis (…) to set up the security options. 
If you want to specify a different domain account for Distribution Agent Security, ﬁ ll out 
account information here. For this example, select the options shown in Figure 15-29.
FIGURE 15-26
FIGURE 15-27
FIGURE 15-28
FIGURE 15-29

494  ❘  CHAPTER 15  REPLICATION
 6. 
Specify the synchronization schedule. Because this is just your ﬁ rst test example, make it 
simple and set it to run on demand. Click the drop-down and select Run on Demand Only. 
As shown in Figure 15-30. Click Next to continue.
 7. 
Figure 15-31 shows the Initialize Subscriptions screen where you specify when the snapshot 
is initialized. Check the box to select initialization; and in the drop-down, choose to 
initialize immediately.
FIGURE 15-30
FIGURE 15-31
 8. 
Next is the Wizard Actions screen shown earlier (refer to Figure 15-21), where you can 
choose to create the subscription and script the creation. Make your selection and click 
Next to continue.
 9. 
From the conﬁ rmation screen that appears, conﬁ rm that the actions are as you expect. If 
not, then go back and ﬁ x anything before executing the wizard actions you selected. When 
you are satisﬁ ed, click Finish.
 10. 
The next screen shows the actions and their status. You should see three actions, 
with the last of these indicating a warning. Clicking the message for this action should 
reveal a message indicating the snapshot is not available. Although you marked the 
subscription to initialize immediately, it has to wait until you manually run the snapshot 
agent to create a snapshot that is available to be applied. You can see this in Figure 15-32. 
If this step shows any errors, investigate and resolve before attempting to rerun the wizard.




498  ❘  CHAPTER 15  REPLICATION
Merge Replication
For merge replication, all articles must have a unique identiﬁ er column with a unique index and the 
ROWGUIDCOL property. If they don’t have it, SQL Server adds one for you. Just like the transactional 
replication with an updateable subscription, an INSERT statement without the column list will fail.
Additional agents are used for transactional and merge replication, such as the Log Reader agent 
and Queue Reader agent. The agents require a domain account to run under. The domain account 
can be their own or shared with other agents. How you choose to implement that depends on your 
company’s security policy.
PEER-TO-PEER REPLICATION
In peer-to-peer replication, every participant is both a publisher and a subscriber. It is suitable for 
cases in which user applications need to read or modify data at any of the databases participating 
in the setup. It provides an interesting alternative for load-balancing and high-availability scenarios. 
This feature is available only in the Enterprise Edition of SQL Server. Oracle calls this kind of 
replication multimaster, whereas DB2 calls it update anywhere.
Consider the following when evaluating and setting up peer-to-peer replication: 
It is designed for a small number of participating databases. A good rule-of-thumb number 
is less than 10. If you use more than that, you are likely to encounter performance issues.
Peer-to-peer replication handles conﬂ ict resolution but in a different way from Merge 
replication. Starting with SQL Server 2008, peer-to-peer replication causes a critical error 
to occur when it detects a conﬂ ict. This critical error causes the Distribution Agent to fail. 
Because of this, you should design your application so that you do not have conﬂ icts.
Peer-to-peer does not support data ﬁ ltering. That would defeat its purpose because 
everybody is an equal partner here for high availability and load balancing.
Applications can scale out read operations across multiple databases, and databases are 
always online. Participating nodes can be added or removed for maintenance.
As mentioned previously, peer-to-peer replication is available only in the Enterprise Edition; 
however, for your testing purposes, it is available in Developer Edition of SQL Server 2012.
Setting Up Peer-to-Peer Replication 
From an implementation standpoint, the process to setup peer to-peer replication is similar to 
setting up snapshot, transactional, or merge replication. However, there are some differences, as you 
will see in the following sections as you walk through an example to set up peer-to-peer replication.
 1. 
To start out, begin with one database, back it up, and restore it on all other participating 
databases. This way, you start from a clean and consistent slate.
 2. 
All nodes in the topology need a distributor, so set one here. Although you can use a single 
distribution database for all publications, this is not recommended for a production system, 
but for the purpose of these examples it is ﬁ ne. The examples here use a local distribution 
database, one on each Publishing server.
➤
➤
➤
➤
➤

Peer-to-Peer Replication ❘ 499
 3. 
After the distributor is set, create a publication just like a regular transactional replication 
publication, except that there is now a new replication type to choose from: peer-to-peer 
publication. Choose this new option, as the Publication Type on the Publication Type page 
which was previously shown in Figure 15-14.
 4. 
After selecting the replication type of Peer-to-Peer Publication, complete the new 
Publication wizard, which is similar to the steps used to create the snapshot publication 
previously.
 5. 
After the new peer-to-peer Publication is created, you must create additional 
peer-to-peer publications to act as peers in the topology. This example has two separate 
servers, one with a peer-to-peer publication 
called NorthAmerica publication, the other 
with a peer-to-peer publication called 
Europe. You can use the same sample code 
from the previous snapshot replication 
example to create a single table to be 
replicated in each of these systems.
 6. 
Proceed with the rest of the setup 
by right-clicking the publication 
and selecting Conﬁ gure Peer-to-Peer 
Topology. The resulting dialog box 
is shown in Figure 15-33. This 
option is only available for a peer-to-peer 
publication. This feature is only 
available in SQL Server 2012 Enterprise 
Edition. Click Next to continue.
Conﬁ guring Peer-to-Peer Replication
After setting up peer to peer replication a welcome screen appears. From here you can begin 
conﬁ guring the peer-to-peer topology: 
 1. 
Select which peer-to-peer publication 
will conﬁ gure the topology. For this 
example, choose to conﬁ gure the topology 
from the local publication, which is 
NorthAmerica.
 2. 
After choosing NorthAmerica, click 
Next, and you see the Conﬁ gure Topology 
page, which presents you with a designer 
surface that looks like the screen shown in 
Figure 15-34. To add nodes to the topology, 
right-click the designer surface, and select 
Add a New Peer Node.
 3. 
You now see the standard SQL Server 
Management Studio server connection 
FIGURE 15-33
FIGURE 15-34

500  ❘  CHAPTER 15  REPLICATION
dialog, where you can specify which server the peer is on. In this example everything is on 
the same server, so specify “.” as a reference to the local server.
 4. 
Next you see the Add a New Peer Node dialog, as shown in Figure 15-35. Select the Europe 
database, as shown in the ﬁ gure.
 5. 
Select OK, and the node is added to the topology designer, as shown in Figure 15-36. It is 
hard to know which node is which from just looking at the display surface. Fortunately, 
extensive tips are available when you hover your mouse over each node.
FIGURE 15-35
FIGURE 15-36
 6. 
The next step is to connect the nodes in the topology. To connect the two nodes, right click 
on either node on the designer and select either Connect to ALL displayed nodes, or Add a 
new peer connection. For this example, select Connect to ALL displayed nodes because you 
have only two nodes displayed. This adds a two-way arrow between the two nodes.
Since there are only two nodes in this particular example, you could have done this 
previously by selecting Connect to ALL displayed nodes (refer to Figure 15-35), but you can 
perform this step earlier (and should) only when you have only two nodes. 
 7. 
Select Next and the wizard moves onto the Log reader Agent Security page. Set the 
security information by clicking the ellipsis ( “…”) button, and choose Run under the SQL 
Server Agent service account option. Click Next and the Distribution Agent Security page 
appears.
 8. 
On the Distribution Agent Security page, you need to provide security information for 
the distribution agents on all nodes in the topology. For this example, select the same 

Peer-to-Peer Replication ❘ 501
 9. 
The next page in the wizard is the New Peer Initialization page, where you can specify how 
the new peer should be initialized. In this case, the DBs were created manually, and the 
same data was loaded into each database, so you can select the ﬁ rst option. Select Next to 
move onto the next page.
 10. 
The Complete the Wizard page displays; here you see all the choices made in the wizard. If 
these all appear to be correct, then select Finish to start applying the actions selected in the 
wizard.
 11. 
The ﬁ nal page in the wizard shows the actions as they are executed and reports any 
warnings or errors with the process. If there are any actions that report other than success, 
you need to investigate each and resolve the reported issue before attempting to rerun the 
topology wizard.
FIGURE 15-37
options for both servers: Run Under the SQL Server Agent Service Account, and choose By 
Impersonating the Process Account for how you connect to the distributor and subscriber, 
as shown in Figure 15-37. Select Next to move onto the next page.

502  ❘  CHAPTER 15  REPLICATION
You have just completed the build out of a peer-to-peer topology. You can alter the topology at any 
time by re-running the Conﬁ gure topology wizard and adding additional nodes, removing existing 
nodes, and changing connections.
SCRIPTING REPLICATION
For many the preferred interface for managing replication is the graphical interface provided in 
SQL Server Management Studio in the various replication wizards. However, DBAs want to create 
a script that can be checked into a source code control system such as Visual Source Safe or Visual 
Studio Team Foundation Server.
To script replication, you have two options. You can use either the various replication wizards, which 
provide a scripting option, or use SQL Server Management Studio and script individual objects.
You can save the scripts into your source code control system, where you can version-control 
them and use them to deploy a replication conﬁ guration to development, test, QA, preproduction, 
and production systems. See Chapter 9, “Change Management” for more information on using 
Visual Studio Team Server Database Edition (VSTS DB Pro) and Team Server Source Control 
(TSSC).
MONITORING REPLICATION
You can use three tools for monitoring replication: Replication Monitor, Performance Monitor, and 
the replication-related DMVs. This section discusses these three tools in detail.
Replication Monitor
Replication Monitor is built into SQL Server Management Studio. It shows the status of replication 
and can be used to make conﬁ guration changes. It can also show the current latency and the 
number of outstanding commands. It does not show the transaction or command rates; nor does it 
show any history of what happened at any time before the current snapshot of data was read. 
You can invoke the Replication Monitor from Management Studio.
 1. 
First, navigate to either the context menu of the top level replication node, or the context 
menu of any server listed under Publications or Subscriptions.
 2. 
From here choose the Launch Replication Monitor option. Figure 15-38 shows the 
Replication Monitor after it has been started when running on a server with a local replication 
conﬁ guration. If you start Replication Monitor from a computer without a local replication 
conﬁ guration, you need to specify the servers with the replication topology to see any 
information.

Monitoring Replication ❘ 503
Following are a few of the key options in the Replication Monitor that you can choose from once 
invoked: 
You can see the new SQL Server 2012 Distributor view by selecting the root node 
Replication Monitor. Doing this displays in the right pane the options to add a publisher or 
to Switch to Distributor View. Selecting this second option changes the display to present 
nodes based on their distributor. This example uses a local distributor, so this doesn’t 
change the display. In a more complex conﬁ guration with multiple replication streams using 
multiple distributors, this would change the ownership of publications and subscriptions 
to live under their parent distributor. To switch back, select the root node again, and select 
Switch to Publisher Group View.
To look at subscription status (see Figure 15-39), double-click any entry of the All Subscriptions 
tab to bring up a window where you can easily view reports and the status of publication to 
distribution, distribution to subscription, and undistributed commands. It is a user-friendly tool 
that provides a good overview of all your replication status information. 
Tracer tokens are a way to measure the current performance of replication. The Tracer 
Tokens tab is shown in Figure 15-40. Think of a tracer token as a dummy record that the 
Replication Monitor uses to gauge the performance of your replication model. It can give 
you a good idea of latency between your publisher, distributor, and subscriber.
➤
➤
➤
FIGURE 15-38

504  ❘  CHAPTER 15  REPLICATION
FIGURE 15-39
FIGURE 15-40

Monitoring Replication ❘ 505
Performance Monitor
You can monitor several replication-related Performance Monitor objects and counters in SQL 
Server: 
SQLServer: Replication Agent: Has a single counter called Running that shows the number 
of replication agents running on the server
SQL Server: Replication Log Reader: Shows the statistics for the Log Reader, with counters 
for Commands per second, Transactions per second, and Latency
SQL Server: Replication Dist: Shows the statistics for the distributor with the same counters 
as for the Log Reader
SQL Server: Replication Merge: Shows the statistics for the Merge agent, with counters for 
Conﬂ icts per second, Downloaded changes per second, and Updates per second
SQL Server: Replication Snapshot: Shows the statistics for the Snapshot agent, with counters 
for Delivered commands per second and Delivered transactions per second
When considering which performance counters to use, your replication type is the key factor to 
consider. For snapshot replication, look at the Replication Snapshot counters. For merge replication, 
look at the Replication Merge counters. For transactional replication, look at the Replication Log 
Reader and Replication Dist counters.
Here is some more information on the counters you should monitor for transactional replication: 
Object: SQL Server:Replication Log Reader - Counter: LogReader: Delivered Cmds/Sec and 
Tx/Sec: These two counters are repeated for each publication; they display the commands 
or transactions read per second and indicate how many commands or transactions are read 
by the Log Reader per second. If this counter increases, it indicates that the Tx rate at the 
publisher has increased.
Object: SQL Server: Replication Dist. - Counter: Dist:Delivered Cmds/Sec and Tx/Sec: 
These two counters are repeated for each subscription and display the commands or 
transactions per second delivered to the subscriber. If this number is lower than the Log 
Reader delivered number, it is an indication that commands may be backing up on the 
distribution database. If it is higher than the Log Reader rate and there is already a backlog 
of commands, it might indicate that replication is catching up.
Replication DMVs
Following are four replication-related DMVs in every SQL Server database: 
sys.dm_repl_articles: Contains information about each article being published. It 
returns data from the database being published and returns one row for each object being 
published in each article. The syntax is as follows: 
 select 
* from sys.dm_repl_articles
➤
➤
➤
➤
➤
➤
➤
➤

506  ❘  CHAPTER 15  REPLICATION
sys.dm_repl_schemas: Contains information about each table and column being 
published. It returns data from the database being published and returns one row for each 
column in each object being published. The syntax is as follows: 
 select 
* from sys.dm_repl_schemas
sys.dm_repl_tranhash: Contains information about the hashing of transactions. The 
syntax is as follows: 
 select 
* from sys.dm_repl_tranhash
sys.dm_repl_traninfo: Contains information about each transaction in a transactional 
replication. The syntax is as follows: 
 select 
* from sys.dm_repl_traninfo
These DMVs show information about what is being published from a speciﬁ c database. They 
cannot help you monitor what’s going on with replication. For that, you have to either run the 
sp_replcounters system stored procedure (discussed in the next section) or go digging into the 
distribution database. It’s much easier to just execute the sp_replcounters stored procedure than 
dig into the inner workings of replication in the MSDB or distribution databases; although, it may not 
have all the information you want to monitor. If you do need more information, you will need to 
search the distribution database. This is currently undocumented territory, but a good place to start 
is the source code for the replication stored procedures. Use these to begin determining where the 
data you want is located, and take it from there.
sp_replcounters
The sp_replcounters replication system stored procedure returns information about the 
transaction rate, latency, and ﬁ rst and last log sequence number (LSN) for each publication on a 
server. Run it on the publishing server. Calling this stored procedure on a server that is acting as the 
distributor or subscribing to publications from another server does not return any data.
Table 15-1 is an example of the output of this stored procedure. The results have been split over two 
lines for clarity. 
TABLE 15-1: Example Output From sp_replcounters
DATABASE
REPLICATED TRANSACTIONS
REPLICATION 
RATE 
TRANS/SEC
REPLICATION 
LATENCY (SEC)
Publisher
178
129.0323
0.016
➤
➤
➤
REPLBEGINLSN
REPLNEXTLSN
0x0002FACC003458780001
0x0002FACC003459A1003D

Summary ❘ 507
SUMMARY
Replication is an important technology within SQL Server for moving data between servers. 
The various types of replication include snapshot, transactional, and merge — and the different 
topologies that can be used with replication include Single Publishers, Multiple Publishers, Updating 
Subscribers, and Peer to Peer. You can monitor replication using Replication Monitor, Performance 
Monitor, and relevant performance counters. Additionally, some of the replication DMVs and 
system stored procedures help to identify the root cause of issues when they occur.
Along with log shipping, database mirroring, and clustering, SQL Server 2012 provides many 
features to satisfy customers needs for load balancing, high availability, disaster recovery, and scaling. 







Clustering: The Big Picture ❘ 515
How Clustering Works
In this section, you consider active and passive nodes, the shared disk array, the quorum, public and 
private networks, and the cluster server. Then, you learn how a failover works.
Active Nodes Versus Passive Nodes
A Windows Failover Cluster can support up to sixteen nodes; however, most clustering deployment 
is only two nodes. A single SQL Server 2012 instance can run on only a single node at a time; and 
should a failover occur, the failed instance can failover to another node. Clusters of three or more 
physical nodes should be considered when you need to cluster many SQL Server instances. Larger 
clusters are discussed later in this chapter.
In a two-node Windows Failover Cluster with SQL Server, one of the physical nodes is considered 
the active node, and the second one is the passive node for that single SQL Server instance. It 
doesn’t matter which of the physical servers in the cluster is designated as active or passive, but you 
should speciﬁ cally assign one node as the active and the other as the passive. This way, there is no 
confusion about which physical server is performing which role at the current time.
When referring to an active node, this particular node is currently running a SQL Server instance 
accessing that instance’s databases, which are located on a shared disk array.
When referring to a passive node, this particular node is not currently running the SQL Server. When 
a node is passive, it is not running the production databases, but it is in a state of readiness. If the 
active node fails and a failover occurs, the passive node automatically runs production databases and 
begins serving user requests. In this case, the passive node has become active, and the formerly active 
node becomes the passive node (or the failed node, if a failure occurs that prevents it from operating). 
Shared Disk Array
Standalone SQL Server instances usually store their databases on local disk storage or nonshared 
disk storage; clustered SQL Server instances store data on a shared disk array. Shared means that all 
nodes of the Windows Failover Cluster are physically connected to the shared disk array, but only 
the active node can access that instance’s databases. To ensure the integrity of the databases, both 
nodes of a cluster never access the shared disk at the same time.
Generally speaking, a shared disk array can be an iSCSI, a ﬁ ber-channel, SAS connected, a RAID 1, a 
RAID 5, or a RAID 10 disk array housed in a standalone unit, or a SAN. This shared disk array must 
have at least two logical disk partitions. One partition is used for storing the clustered instance’s SQL 
Server databases, and the other is used for the quorum drive, if a quorum drive is used. Additionally, 
you need a third logical partition if you choose to cluster MSDTC. 
The Quorum
When both cluster nodes are up and running and participating in their respective active and 
passive roles, they communicate with each other over the network. For example, if you change a 
conﬁ guration setting on the active node, this conﬁ guration is propagated automatically, and quickly, 
to the passive node, thereby ensuring synchronization. 
 
 
 
 

516  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
As you might imagine, though, you can make a change on the active node and have it fail before the 
change is sent over the network and made on the passive node. In this scenario, the change is never 
applied to the passive node. Depending on the nature of the change, this could cause problems, even 
causing both nodes of the cluster to fail.
To prevent this change from happening, a Windows Failover Cluster employs a quorum. A quorum 
is essentially a log ﬁ le, similar in concept to database logs. Its purpose is to record any change made 
on the active node. This way, should any recorded change not get to the passive node because the 
active node has failed and cannot send the change to the passive node over the network, the passive 
node, when it becomes the active node, can read the quorum log ﬁ le to ﬁ nd out what the change 
was. The passive node can then make the change before it becomes the new active node. If the state 
of this drive is compromised, your cluster may become inoperable. 
In effect, each cluster quorum can cast one “vote,” where the majority of total votes (based on the 
number of these cluster quorums that are online) determine whether the cluster continues running on 
the cluster node. This prevents more than one cluster node attempting to take ownership of the same 
SQL Server instance. The voting quorums are cluster nodes or, in some cases, a disk witness or ﬁ le 
share witness. Each voting cluster quorum (with the exception of a ﬁ le share witness) contains a copy 
of the cluster conﬁ guration. The cluster service works to keep all copies synchronized at all times. 
Following are the four supported Windows Failover Cluster quorum modes:
Node Majority: Each node that is available and in communication can vote. The cluster 
functions only with a majority of the votes.
Node and Disk Majority: Each node plus a designated disk in the cluster storage (the 
“disk witness”) can vote, whenever they are available and in communication. The cluster 
functions only with a majority of the votes.
Node and File Share Majority: Each node plus a designated ﬁ le share created by the 
administrator (the “ﬁ le share witness”) can vote, whenever they are available and in 
communication. The cluster functions only with a majority of the votes.
No Majority: Disk Only: The cluster has a quorum if one node is available and in 
communication with a speciﬁ c disk in the cluster storage. Only the nodes that are also 
in communication with that disk can join the cluster. The disk is the single point of failure, 
so use highly reliable storage. A quorum drive is a logical drive on the shared disk array 
dedicated to storing the quorum and as a best practice should be around 1GB of fault 
tolerant disk storage. 
With two-node clusters Disk only is the most often used quorum conﬁ guration, commonly known 
as the quorum disk. The quorum conﬁ guration can be switched after the cluster has been deployed 
based on the number of clustered nodes and user requirements. While in clusters with greater than 
two nodes, the other three quorum modes are more commonly used. 
Public and Private Networks
Each node of a cluster must have at least two network cards to be a fully supported installation. 
One network card is connected to the public network, and the other network card will be connected 
to a private cluster network.
➤
➤
➤
➤

Clustering: The Big Picture ❘ 517
The public network is the network to which the client applications connect. This is how 
they communicate to a clustered SQL Server instance using the clustered IP address and 
clustered SQL Server name. It is recommended to have two teamed network cards for the 
public network for redundancy and to improve availability. 
The private network is used solely for communications between the clustered nodes. It is 
used mainly for the heartbeat communication. Two forms of communications are executed:
LooksAlive: Veriﬁ es that the SQL Server service runs on the online node every 5 
seconds by default
IsAlive: Veriﬁ es that SQL Server accepts connections by executing 
sp_server_diagnostics.
This health detection logic determines if a node is down and the passive node then takes over the 
production workload. 
The SQL Server Instance
Surprisingly, SQL Server client applications don’t need to know how to switch communicating from 
a failed cluster node to the new active node or anything else about speciﬁ c cluster nodes (such as 
the NETBIOS name or IP address of individual cluster nodes). This is because each clustered SQL 
Server instance is assigned a Network name and IP address, which client applications use to connect 
to the clustered SQL Server. In other words, client applications don’t connect to a node’s speciﬁ c 
name or IP address but instead to the cluster SQL network name or cluster SQL IP address that 
stays consistent and fails over. Each clustered SQL Server will belong to a Failover Cluster Resource 
Group that contains the following resources that will fail together:
SQL Server Network Name
IP Address
One or more shared disks
SQL Server Database Engine service
SQL Server Agent
SQL Server Analysis Services, if installed in the same group
One ﬁ le share resource, if the FILESTREAM feature is installed
How a Failover Works
Assume that a single SQL Server 2012 instance runs on the active node of a cluster and that a 
passive node is available to take over when needed. At this time, the active node communicates with 
both the database and the quorum on the shared disk array. Because only a single node at a time 
can access the shared disk array, the passive node does not access the database or the quorum. In 
addition, the active node sends out heartbeat signals over the private network, and the passive node 
monitors them, so it can take over if a failover occurs. Clients are also interacting with the active 
node via the clustered SQL Server name and IP address while running production workloads.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

518  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
Now assume that the active node stops working because of a power failure. The passive node, 
which is monitoring the heartbeats from the active node, notices that the heartbeats stopped. After 
a predetermined delay, the passive node assumes that the active node has failed and initiates a 
failover. As part of the failover process, the passive node (now the active node) takes over control of 
the shared disk array and reads the quorum, looking for any unsynchronized conﬁ guration changes. 
It also takes over control of the clustered SQL Server name and IP address. In addition, as the node 
takes over the databases, it has to perform a SQL Server startup and recover the databases.
The time this takes depends on many factors, including the performance of the hardware and the 
number of transactions that might have to be rolled forward or back during the database recovery 
process. When the recovery process is complete, the new active node announces itself on the 
network with the clustered SQL Server name and IP address, which enables the client applications to 
reconnect and begin using the SQL Server 2012 instance after this minimal interruption.
Clustering Options
Up to this point, simple two-node, active/passive clusters running a single SQL Server instance have 
been discussed. However, this is only one of many options you have when clustering SQL Server. 
Two other popular options include active/active clustering and multi-node clustering. Additionally, 
it is available to cluster multiple instances of SQL Server on the same server. The following sections 
discuss these alternatives in detail. 
Active/Active Cluster
The examples so far have described what is called an active/passive cluster. This is a two-node 
cluster in which there is only one active instance of SQL Server 2011. Should the active node fail, the 
passive node takes over the single instance of SQL Server 2011, becoming the active node.
To save hardware costs, some organizations like to conﬁ gure an active/active cluster. Like active/
passive, this is also a two-node cluster, but instead of only a single SQL Server instance running, 
there are two instances, one on each physical node of the cluster.
The advantage of an active/active cluster is that you make better use of the available hardware. Both 
nodes of the cluster are in use instead of just one, as in an active/passive cluster. The disadvantage 
is that when a failover occurs, both SQL Server instances are running on a single physical server, 
which can reduce performance of both instances where memory may need to be readjusted to ensure 
that each has adequate memory. To help overcome this problem, both of the physical servers can 
be oversized to better meet the needs of both instances should a failover occur. Chances are good, 
however, that the perfect balance will not be met and there will be some performance slowdown 
when failing over to the other node. In addition, if you have an active/active cluster running two 
SQL Server instances, each instance needs its own logical disk on the shared disk array. Logical 
disks cannot be shared among SQL Server instances.
In the end, if you want to save hardware costs and don’t mind potential application slowdowns, use 
an active/active two-node cluster.
Multi-node Clusters
If you think you will be adding even more clustered SQL Server 2012 instances in the future you 
may want to consider a third option: multi-node clusters. For the more conservative, a three-node 


520  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
Clustering Multiple Instances of SQL Server on the Same Server
As indicated in the types of clustering discussed previously, a single SQL Server instance can run 
on a single physical server, however, this is not a requirement. SQL Server Enterprise Edition can 
actually support up to 25 SQL instances on a single clustered conﬁ guration. This is a restriction of 
drive letter limitations though so you need mount points to achieve this. The effectiveness of this 
depends on the business requirements, the capacity of the hardware, SLAs, and the expertise of the 
IT organization managing it.
The purpose of clustering is to boost high availability. Adding many SQL Server instances to a 
cluster adds complexity, and complexity can increases risk and failover points in the solution. But 
complexity can also be managed depending on the IT expertise to support it; speak to your IT 
support when considering this option.
UPGRADING SQL SERVER CLUSTERING
If your organization is like many, it probably already has some older versions of SQL Server clusters 
in production. If so, it is time to decide how to upgrade them to SQL Server 2012. Your available 
options include the following: 
Don’t upgrade.
Perform an in-place SQL Server 2012 upgrade.
Rebuild your cluster from scratch, and then install SQL Server 2012 clustering. Or leave the 
Windows Failover Cluster intact, if it is a Windows 2008 R2, but install a new SQL Server 
2012 on it and migrate the databases.
This section considers each of these three options.
Don’t Upgrade
This is an easy decision. Not upgrading is simple and doesn’t cost anything. Just because a new 
version of SQL Server comes out doesn’t mean you have to upgrade. If your current SQL Server 
cluster is running satisfactory, it may not be worth the costs and upgrade work. A properly 
conﬁ gured Windows 2008 cluster is stable running SQL Server 2005, 2008, and 2008 R2. 
On the other hand, SQL Server 2012 offers scalability, ease of use, capabilities and reliability, and 
new functionality of which you may want to take advantage. Before you upgrade, do the research 
to determine whether the new features of SQL Server 2012 are what you need. Otherwise, you can 
choose to stay on the current SQL Server version, provided that it is still supported by Microsoft.
Upgrading Your SQL Server 2012 Cluster In Place
Before talking about how to upgrade a Windows Failover Cluster to SQL Server 2012, ﬁ rst consider 
what operating system you currently run. If you are on Windows Server 2008 with the latest service pack 
or R2, you are in good shape and an in-place upgrade to SQL Server 2012 should not be a problem. 
If, however, you still run Windows Server 2003, you should deploy Windows 2008 (or R2). 
Upgrading from Windows 2003 to Windows 2008 Failover Cluster is not supported; a total rebuild 
is required. 
➤
➤
➤


522  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
will be down, there is also the added risk of unexpected problems. For example, you might make an 
installation mistake halfway through the upgrade and have to start over. Because of the uncertainty 
involved, you should ﬁ rst estimate how much time you think the upgrade will take under good 
circumstances, and then double that estimate as the size of your requested downtime window. This 
way, your users are prepared.
Whether you upgrade using new hardware or old hardware, you have to consider two additional issues: 
Will you reuse your current clustered SQL Server name and IP address or select new ones? 
How will you move your data from the previous cluster to the new cluster?
The clients that access your current SQL Server cluster do so using the cluster’s SQL Server name 
and IP address. If you want the clients to continue using the same clustered SQL Server name and IP 
address, you need to reuse the old clustered SQL Server name and IP address in the new cluster. This 
is the most common approach because it is generally easier to change a single clustered SQL Server 
name and IP address than to reconﬁ gure dozens, if not hundreds, of clients that access the SQL 
Servers on the Windows Failover cluster.
If you upgrade using old hardware, reusing the former clustered SQL Server name and IP address is 
not an issue because the old cluster is brought down and then the new one is brought up, so there 
is never a time when the clustered SQL Server name and IP address are on two clusters at the same 
time (which won’t work).
If you upgrade by using new hardware, you need to assign a clustered SQL Server name and IP 
address for testing, but you won’t use the old ones because they are currently in use. In this case, 
you need to use a temporary clustered SQL Server name and IP address for testing; when you are 
ready for the actual changeover from the old cluster to the new cluster, you need to follow these 
general steps: 
 1. 
Back up the data.
 2. 
Remove SQL Server clustering from the old cluster or turn off the old cluster.
 3. 
On the new cluster, change the clustered SQL Server name and IP address from the old 
cluster.
 4. 
Restore the data.
How you move the data from the old cluster to the new cluster depends on both the size of the 
databases and somewhat on whether you use old hardware or new hardware. 
Regardless of the option you choose, before you proceed, back up all the databases. Remember, 
identify any objects in the System databases such as SQL jobs, SSIS packages and logins, and re-create 
them in the new Windows Failover cluster. If you use old hardware, all you have to do is backup or 
detach the user databases. When the cluster rebuild is complete, restore or reattach the user databases. 
If you move to new hardware or change the database ﬁ le locations, you should ﬁ rst backup or 
detach user databases. Next, move these to the new cluster or new database location. Then when the 
cluster rebuild completes, restore or reattach the user databases. 
➤
➤


524  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
staff, but it is your responsibility to ensure that all these are in place before you begin building your 
SQL Server 2012 cluster: 
Your network must have at least one Active Directory server and ideally two for redundancy.
Your network must have at least one DNS server and ideally two for redundancy.
Your network must have available switch ports for the public network cards used by the 
nodes of the cluster. Be sure to set them to match the manually set network card settings 
used in the nodes of the cluster. SQL Server 2012 supports nodes in different subnets, but 
for SQL Server versions prior to SQL Server 2012 all the nodes of a cluster must be on the 
same subnet.
You must secure IP addresses for all the public network cards. Windows 2008 does support 
DHCP but you should use static IPs.
You must decide how you will conﬁ gure the private heartbeat network. Choose between 
using a direct network card-to-network card connection using a cross over cable (only 
possible with a two-node cluster), or use a hub or switch. The hub or switch should be 
different than the one supporting the public network for redundancy.
You need to secure IP addresses for the private network cards. Generally you should use a 
private network subnet such as 10.0.0.0–10.255.255.255, 172.16.0.0–172.31.255.255, or 
192.168.0.0–192.168.255.255. Remember, this is a private network seen only by the nodes 
of the cluster. Again, Windows 2008 enables DHCP to claim these IP addresses.
Ensure that you have proper electrical power for the new cluster nodes and shared disk array.
Ensure that battery backup power is available to all the nodes in your cluster and your 
shared disk array.
If you don't already have one, create a SQL Server service account to be used by the SQL 
Server services running on the cluster. This must be a domain account with the password set 
to never expire.
If you don't already have one, create a cluster service account to be used by the Windows 
Clustering service. This must be a domain account with the password set to never expire.
Determine a name for your Windows Failover Cluster and secure an IP address for it. This 
name will be used for management of the cluster after it is created.
Determine a name for your SQL Server 2012 cluster and secure an IP address for it. This 
will be the name that clients will connect to.
These preparations will come back into play during the installation process.
Preparing the Hardware
Hardware typically presents certain issues, often taking the most time to research and conﬁ gure. 
Part of the reason for this is that there are many hardware options, some of which work, some of 
which don’t. Unfortunately, there is no complete resource you can use to help you sort through 
this. Each vendor offers different hardware, and the available hardware is always changing, along 
with new and updated hardware drivers, making this entire subject a moving target with no easy 
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤


526  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
Preparing the Hardware
As a DBA, you may not be the one who installs the hardware. In any case, here are the general steps 
most people follow when building cluster hardware: 
 1. 
Install and conﬁ gure the hardware for each node in the cluster as if it will be running as a 
standalone server. This includes installing the latest approved drivers and ﬁ rmware.
 2. 
After the hardware is installed, install the operating system and the latest service pack, 
along with any additional required drivers. Then run Windows Update.
 3. 
Conﬁ gure the nodes to the public network. To make things easy to identify, name the 
network used for public connections public network.
 4. 
Conﬁ gure the private heartbeat network. To make things easy to identify, name the private 
heartbeat network private network.
 5. 
Set up and conﬁ gure the shared disk array, iSCSI targets, or SAN. 
 6. 
Install and conﬁ gure the iSCSI or ﬁ ber cards in each of the nodes, and install the latest 
drivers. In Windows 2008, you can also use iSCSI. In this case, you need to set up the iSCSI 
initiators to connect to the drives.
 7. 
One at a time, connect each node to the shared disk array, iSCSI drive, or SAN, following the 
instructions for your speciﬁ c hardware. It is critical that you do this one node at a time. In 
other words, only one node at a time should be physically on and connected to the shared disk 
array or SAN and conﬁ gured. After that node is conﬁ gured, turn it off and turn the next node 
on and conﬁ gure it, and so on, one node at a time. If you do not follow this procedure, you 
risk corrupting the disk conﬁ guration on your nodes, requiring you to start the process over.
 8. 
Use Disk Administrator to conﬁ gure and format the drives on the shared disk array. You 
need at minimum two logical drives on the shared disk array: one for storing your SQL 
Server databases, and the other for the quorum drive. The data drive must be big enough 
to store all the required data and the quorum drive must be at least 500MB (which is the 
smallest size that an NTFS volume can efﬁ ciently operate). When conﬁ guring the shared 
drives using Disk Administrator, each node of the cluster is required to use the same drive 
letter when referring to the drives on the shared disk array or SAN. For example, you might 
want to assign your data drive as drive “F:” on all the nodes, and assign the quorum drive 
letter “Q:” on all the nodes.
 9. 
When all the hardware is put together, ensure that there are no problems before you begin 
installing clustering services by checking the Windows Event Viewer. Although you may do 
some diagnostic hardware testing before you install the operating system, you need to wait 
until after installing the operating system before you can fully test the hardware. 
 10. 
Ensure that you can ping the cluster nodes over the public and then the private networks. 
Likewise, ensure that you can ping the domain controller and DNS server to verify that they 
are available. 
 11. 
Write a ﬁ le in the shared disks by one node, which should be visible by the other node. 
 12. 
Additionally, verify that the drive letters correspond between nodes and that there is not a 
mismatch in drive letters from the shared disks between nodes.  

Clustering Windows Server 2008 ❘ 527
After all the hardware has been conﬁ gured and tested, you are ready to install Windows Failover 
clustering.
CLUSTERING WINDOWS SERVER 2008
Before you can install SQL Server 2012 on the cluster, you must ﬁ rst install Windows Server 2008 
Failover Cluster services. After it is successfully installed and tested, you can cluster SQL Server. 
This section takes a high-level, step-by-step approach to installing and conﬁ guring a Windows 
Failover Cluster.
Before Installing Windows 2011 Clustering
To install Windows Failover clustering, you need to perform a series of important steps. This 
is especially important if you didn’t build the cluster nodes because you want to ensure everything is 
working correctly before you begin the actual cluster installation. When they are completed, you can 
install Windows 2008 Failover clustering. Following are the steps you must perform: 
 1. 
Ensure that all the physical nodes are working properly and are conﬁ gured identically 
(hardware, software, and drivers).
 2. 
Verify that none of the physical nodes have been conﬁ gured as a domain controller or for 
any other critical services such as Exchange. Also, ensure that you have multiple domain 
controllers in your environment.
 3. 
Verify that all drives are NTFS and are not compressed.
 4. 
Ensure that the public and private networks are properly installed and conﬁ gured.
 5. 
SQL Server failover cluster installation supports Local Disk only for installing the tempdb 
ﬁ les. Ensure that the path speciﬁ ed for the tempdb data and log ﬁ les is valid on all the 
cluster nodes. During failover, if the tempdb directories are not available on the failover 
target node, the SQL Server resource will fail to come online.
 6. 
Verify that you have disabled NetBIOS for all private network cards.
 7. 
Verify that there are no network shares on any of the shared drives.
 8. 
If you intend to use SQL Server encryption, install the server certiﬁ cate with the fully 
qualiﬁ ed DNS name of the clustered SQL Server on all cluster nodes.
 9. 
Check all the error logs to ensure that there are no problems. If there are, resolve them 
before proceeding with the cluster installation.
 10. 
Add the SQL Server and clustering service accounts to the Local Administrators group of all 
the cluster nodes.
 11. 
Verify that no antivirus software has been installed on the cluster nodes. Antivirus 
software can reduce the availability of a cluster. If you want to check for possible viruses 
on a Windows Failover Cluster, run scans on the cluster nodes remotely from another 
computer.

528  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
These are many things to check, but each is important. If skipped, any one of these steps could 
prevent your cluster from installing or working properly.
Installing Windows Server 2008 Failover Clustering
Now that all your physical nodes and shared disk array, iSCSI device, or SAN is ready, you are ready 
to install a Windows 2008 Failover Cluster. This section describes the process from beginning to end.
To begin, you must enable the clustering feature for each of the Windows 2008 node candidates. 
In Windows 2008 the decision was made to reduce the Windows surface area by disabling the 
feature by default. To enable the failover clustering feature, from the Server Manager, open the 
Add Features Wizard. Check Failover Clustering, and the wizard completes the installation for 
you (shown in Figure 16-1). This doesn’t actually cluster your machines but installs the necessary 
components on your server to manage the cluster and perform the failover. This step needs to be 
repeated on each physical node participating on the Windows Failover Cluster.
FIGURE 16-1
Validating the Windows Failover Cluster
Before you create the cluster, you want to validate that the cluster physical nodes run on supported 
hardware. You can tell if this is true if the nodes by pass the cluster validation. To do this, open the 
Failover Cluster Management tool, and click Validate a Conﬁ guration. This opens the Validate a 
Cluster Conﬁ guration Wizard, which runs a full diagnosis of your cluster prior to the installation. 


530  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
deployed a Windows 2003 Failover Cluster, you will be amazed when you see how easy these steps 
are to cluster Windows 2008: 
 1. 
From within the Failover Cluster Management, click Create a Cluster. This opens the 
Create Cluster Wizard. The ﬁ rst screen after the welcome screen asks you which physical 
nodes will participate in the cluster. Type in each physical node and click Add, as shown in 
Figure 16-3. More physical nodes can be added later if need be. 
FIGURE 16-3
 2. 
Assign the Windows cluster an IP address, and specify which network you want the cluster 
to use, as shown in Figure 16-4. In the ﬁ gure, you can see that “SQL2012Cluster” was the 
Windows cluster name used for management. That name is too long for NetBIOS though so 
it is shortened for customers using the NetBIOS network protocol.
FIGURE 16-4

Clustering Windows Server 2008 ❘ 531
The cluster then begins to be created and conﬁ gured (see Figure 16-5) and you will be notiﬁ ed when 
the build process is done. This is a much shorter process than it was in Windows 2003; it takes no 
more than 2 minutes in a test environment.
FIGURE 16-5
Preparing Windows Server 2008 for Clustering
Before you can install SQL Server, there is one small step you need to perform, and that is to 
prepare the cluster for SQL Server. In the previous section, you clustered Windows but didn’t tell 
Windows which are the shared disks. This is because Microsoft now deploys a minimalist approach 
to clustering during the installation, doing the absolute minimum it needs to get it clustered and 
then enabling you to add the necessary components later.
To prepare the cluster for SQL Server, perform the following steps from the Failover Cluster 
Management tool: 
 1. 
Under Navigate, click the Storage shortcut. 
 2. 
From the Actions pane, click Add Disk.
 3. 
Add any disks that you want to be visible to the Windows Failover Cluster.
 4. 
Click OK (see Figure 16-6).

532  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
CLUSTERING MICROSOFT DISTRIBUTED 
TRANSACTION COORDINATOR
To coordinate transactions across SQL Servers in a clustered environment, you will need to leverage 
Microsoft Distributed Transaction Coordinator (MSDTC) to make it highly available. 
Windows 2003 supported only one cluster MSDTC resource; as a result, all applications across 
the cluster needed to use the single MSDTC instance. However, when a MSTDC is highly utilized, 
it can become a bottleneck. On Windows 2008 and later, for better performance, you can install 
multiple MSDTC instances on a single Windows Failover Cluster as shown in the following steps. 
The ﬁ rst MSDTC instance installed becomes the default instance but can be changed from the 
Component Services Management Console (dcomcnfg).
 1. 
Launch the Component Services Management Console from the Administration folder or by 
executing dcomcnfg from a command prompt.  
FIGURE 16-6
If you cannot see the shared disks, they are either already added as a disk resource or the disks are 
not visible to all nodes in the cluster. This might indicate that the masking is incorrect or there’s a 
communication problem of some sort.
 
 
 
 

Clustering Microsoft Distributed Transaction Coordinator ❘ 533
 2. 
Expand Computers and then right-click My Computer. 
 3. 
Click Properties, click the MSDTC tab, and select the default coordinator. 
If multiple MSDTC instances exist, SQL Server uses the following rules to identify the MSDTC 
instance to be chosen in priority order:
 1. 
Use the MSDTC instance installed to the local SQL Server group.
 2. 
Use the mapped MSDTC instance. To create a mapping, execute the following at the 
command prompt: 
 msdtc –tmMappingSet –name <MappingName> 
-service <SQLServerServiceName> -clusterResource <MSDTCResourceName>
<MappingName> is any chosen name to identify the mapping. 
<SQLServerServiceName> is the service name from the SQL Server instance such as 
MSSQLServer or MSSQL$<InstanceName>.
<MSDTCResourceName> is the MSDTC resource name to map. 
 3. 
Use the cluster’s default MSDTC instance.
 4. 
Use the local node MSDTC instance.
SQL Server automatically uses its local group MSDTC instance if it exists; otherwise, it uses the 
default instance. If the local group MSDTC instance fails, you need to tell SQL Server to use another 
MSDTC instance; it does not automatically use the default instance. To create a cluster MSDTC 
instance, use the Failover Cluster Management tool.  
 1. 
Identify a small shared disk to store the MSDTC log ﬁ le. It is best to put the log ﬁ le on its own 
shared disk or mount point to protect it from any other service that may corrupt the disk.   
 2. 
If the MSDTC resource is installed within a SQL Server Resource Group, it can share the IP 
and clustered SQL Server name of that group. 
 3. 
The default MSDTC should be installed on its own Cluster Resource Group to avoid a 
failover of MSDTC, which in turn fails other services contained in that own resource group. 
When installed on its own resource group, it needs its own IP and network name in addition 
to a shared disk. 
After creating the MSDTC resource(s), you need to enable MSDTC network access to allow 
MSDTC to access network resources and for applications to access it. To do so perform these steps:  
 1. 
Launch the Component Services Management Console from the Administration folder or by 
executing dcomcnfg from a command prompt. 
 2. 
Expand Computers, and then right-click My Computer. Expand Distributed Transaction 
Coordinator and then expand <Your instance of MSDTC>.
 3. 
Right-click the Properties of your MSDTC instance.
 4. 
Under Security Settings, select the Network DTC Access tab and Allow Inbound and Allow 
Outbound check boxes, and click OK. 
➤
➤
➤

534  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
CLUSTERING SQL SERVER 2012
The procedure to install a SQL Server instance onto a Windows Failover Cluster is one of the easiest 
parts of getting your SQL Server cluster up and running. The SQL Server 2012 setup program is 
used for the install and does the heavy lifting for you. All you have to do is make a few (but critically 
important) decisions, and then sit back and watch the installation complete. The setup program even 
goes to the trouble to verify that your nodes are all properly conﬁ gured; and if not, it suggests how to 
ﬁ x any problems before the installation begins.
When the installation process does begin, the setup program identiﬁ es the physical node, and 
after you give it the go-ahead to install on each one, it does so, all automatically. SQL Server 2012 
binaries are installed on the local drive of each node, and the system databases are stored on the 
shared disk array you designate.
The next section shows the step-by-step instructions for installing a SQL Server 2012 instance in 
a Windows Failover Cluster. The assumption for this example is that you will install this instance 
in a two-node active/passive cluster. Even if you install in a two-node active/active or a multi-node 
cluster, the steps in this section are applicable. The only difference is that you have to run SQL 
Server 2012 setup on every node you want to install SQL Server, and you have to specify a different 
logical drive on the shared disk array for each SQL Server instance.
Step by Step to Cluster SQL Server
To begin installing your SQL Server on the cluster, you need the installation media (DVD or 
ISO). You can either install it directly from the media or copy the install ﬁ les from the media to 
the current active node of the cluster, and run the setup program from there. The general process 
for SQL Server installation is the same as a normal, non-clustered installation; this is covered in 
Chapter 2. Therefore, the following steps outline the differences from a normal non-clustered 
installation.
 1. 
To begin installation, run Setup.exe. The prerequisite components (which you 
learned about in Chapter 2, “SQL Server 2012 Installation Best Practices”) install ﬁ rst. 
It is a good idea to run these prerequisites on each of the nodes prior to clustering because 
doing so can save time and enable you to debug visually if something goes wrong during 
the installation. 
 2. 
When you get to the SQL Server Installation Center, click New SQL Server Failover Cluster 
Installation. In this screen you can also add new nodes to the cluster after the installation, 
as shown in Figure 16-7. 
For the most part, the cluster installation is exactly the same as the standalone SQL Server 
installation with the exception of just a few screens. When you get to the Setup Support 
Rules page, you notice a number of new rules that are checked (see Figure 16-8). For 
example, a check is made to determine whether MSDTC is clustered. Although SQL Server 
can work without MSDTC clustered, some features such as distributed transactions for 
linked servers or SSIS will not work without it.

Clustering SQL Server 2012 ❘ 535
FIGURE 16-8
FIGURE 16-7

536  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
 3. 
In the Instance Conﬁ guration dialog, specify the clustered SQL Server Network Name 
for your SQL Server instance, as shown in Figure 16-9. This is the name that all the client 
applications use to connect in their connection strings. The name of the directory where 
your program ﬁ les will be copied is also based on this name by default.
FIGURE 16-9
 4. 
Specify the cluster resource group that will group all your SQL Server resources, such as 
your network name, IP, and SQL Server services, as shown in Figure 16-10. 
A nice improvement since SQL Server 2008 is that you can have SQL Server use 
multiple drives in a cluster during the installation. In SQL Server 2005, you had to 
conﬁ gure this behavior afterward, and you weren’t able to back up to another disk 
other than the main SQL Server disk until you ﬁ xed the dependencies. In SQL Server 
2012, simply check which clustered drive resources you want SQL Server to access, as 
shown in Figure 16-11.

Clustering SQL Server 2012 ❘ 537
FIGURE 16-10
FIGURE 16-11
 5. 
With the drive conﬁ guration complete, specify the network that SQL Server will use to 
communicate with client applications, and choose an IP address for each of the networks. 
This IP address should have been obtained earlier from a network administrator. Notice in 
Figure 16-12 that Windows 2008 can also use DHCP.

538  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
 6. 
Then for Server Conﬁ guration, specify the service accounts and collation conﬁ guration as 
shown in Figure 16-13.
FIGURE 16-12
FIGURE 16-13


540  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
After each SQL Server instance that has been clustered, the resource group should contain the 
following resources: 
Network name
IP address
One or more shared disks
SQL Server Database Engine service
SQL Server Agent service
SQL Server Analysis Services service, if installed. As a best practice, install it on its own 
resource group to avoid a failure in one resource to affect the other. 
One ﬁ le share resource, if the FILESTREAM feature is installed. 
After the clustered SQL Server installation completes, evaluate the resources’ dependencies to 
identify what other resources in the group must be online before a particular resource can be 
brought online for. For example, SQL Server Agent depends on SQL Server being online, and SQL 
Server depends on the shared disks and clustered SQL Server name and IP address. The complete 
Windows Failover Cluster installation should look like Figure 16-14. 
Installing the Service Pack and Cumulative Updates
After you install and cluster SQL Server, your next step is to install any available SQL Server service 
pack and cumulative updates, which you can download from Windows Update .You can perform 
a rolling upgrade as described in the “Upgrading Your SQL Server 2012 Cluster In-Place” section. 
Installing a service pack and/or a cumulative update is fairly straightforward because they are 
cluster-aware. 
Test, Test, and Test Again
After you cluster SQL Server on the nodes, you need to thoroughly test the installation, just as you 
did after ﬁ rst installing Windows 2008 Failover Cluster. For example, check the Windows Event 
Viewer for any error messages, and validate that you can failover the SQL Server across nodes and 
fails back. However, not only do you want to test the SQL Server cluster, but you also want to test 
how your client applications “react” to failovers. Because of this, the following testing section is 
similar to the one you previously read but has been modiﬁ ed to meet the more complex needs of the 
additional client applications testing you need to perform.
The following is a series of tests you can perform to verify that your SQL Server 2012 cluster and 
its client applications work properly during failover situations. After you perform each test, verify 
whether you get the expected results (a successful failover), and be sure you check the Windows log 
ﬁ les for any possible problems. If you ﬁ nd a problem during one test, resolve it before proceeding to 
the next test.
Preparing for the Testing
As with your previous cluster testing, identify a workstation with the Failover Cluster Management 
tool to interact with your cluster during testing.
➤
➤
➤
➤
➤
➤
➤

Clustering SQL Server 2012 ❘ 541
To be prepared, you need to test each client application that will be accessing your clustered SQL 
Server to see what happens should a failover occur. Some client applications deal with clustering 
failovers by reconnecting, whereas others that are not designed with reconnect logic just fail to 
reconnect. You must determine beforehand how each client application responds.
To do this, ﬁ rst identify all the client applications; there may be dozens. Each of these has to be 
reconﬁ gured to use the clustered SQL Server name (and IP address) for the new clustered SQL Server 
instance. In addition, for the client applications to work, you need to have the appropriate databases 
restored or installed on the new cluster. This is necessary if you want a highly available cluster solution.
After you have each of your client applications connected to the SQL Server instance, you are ready 
for testing. Keep in mind that you are testing multiple things, including the Windows 2008 Cluster, 
the clustered SQL Server, and the client applications.
Moving Resource Groups Between Nodes
The easiest test to perform is to use the Failover Cluster Management tool to manually move the 
SQL Server resource group from the active node to a passive node, and then back again. To do this, 
follow these steps: 
 1. 
Go to the resource group that contains SQL Server, right click to select Move This Service, 
and specify where you’d like to move the resource group. This initiates the resource group 
move from your active node to the designated passive node.
 2. 
After this happens, check the Failover Cluster Management tool and each of the client 
applications. Each should continue to operate as if no failover had occurred. The Failover 
Cluster Management tool should pass this test easily. The clients are another story; you 
must check each client application to see if it reconnected. If not, you need to determine why 
not, which is not always easy. Most client applications that stop working after a failover do 
so because of no reconnect logic for. For example, they reconnect if you exit and restart the 
client application.
 3. 
When the resource group has been successfully moved from the active node to a passive 
node, use the same procedure to move the group back to the original node; and as before, 
check the Failover Cluster Management tool, the client applications, and the event logs to 
see if there were any problems. If you noticed any Windows Failover Cluster or SQL Server 
problems because of the failover test, you need to resolve them before proceeding. If you 
have a client applications problem, you can continue with your testing and try to resolve it 
later. In most cases, if a client application fails this ﬁ rst test, it will fail all the other tests.
Manually Failing Over Nodes by Turning Them Of 
To validate that the failover from node to node is taking place and that each node can take over the 
SQL Server, you can perform a failover test by manually turning nodes off. 
 1. 
Turn off the active node. When this happens, watch the failover in the Failover Cluster 
Management tool and the client applications. As before, check for any problems. 

542  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
 2. 
Next, turn on the node and wait until it boots back up successfully. Then turn off the now 
current active node. Again, watch the failover in the Failover Cluster Management tool and 
the client applications, and check for problems. 
 3. 
Turn the node back on when done.
Manually Failing Over Nodes by Disconnecting the Public 
Network Connections
You can also manually failover nodes by turning off public network connections. 
 1. 
Unplug the public network connection from the active node. This causes a failover to a 
passive node, which you can watch in the Failover Cluster Management tool and the client 
applications. Check for any problems. 
 2. 
Now plug the public network connection back into the server and unplug the public 
network connection from the now active node. This causes a failover to the current passive 
node, which you can watch in the Failover Cluster Management tool. Watch the failover in 
the Failover Cluster Management tool and the client applications, and check for problems. 
 3. 
When the testing is complete, plug the network connection back into the node.
Manually Failing Over Nodes by Breaking the Shared Array Connection
You can perform a third manual failover test by breaking a shared array connection. 
 1. 
From the active node, remove the shared disk array connection. This can cause a failover 
that you can watch in the Failover Cluster Management tool and client applications. Check 
for any problems. 
 2. 
Next, reconnect the connection from the now active node, and remove the shared disk 
array connection. Watch the failover in the Failover Cluster Management tool and the client 
applications, and check for problems. 
 3. 
When done, reconnect the connection. If you run into problems in any of these tests, resolve 
them before continuing.
MANAGING AND MONITORING THE CLUSTER
After you have your clustered SQL Server up, running, and tested, you are ready to deploy it into 
production. This may involve creating new databases, moving databases from older servers to 
this one, setting up SQL jobs, and so on. In most cases, managing SQL Server on a cluster is the 
same as managing it as a standalone SQL Server instance. The key thing to keep in mind is that 
whenever you access your cluster with any of your SQL Server 2012 administrative tools, such as 
Management Studio, you access it using its SQL cluster network name and IP address; but if you use 
any of the operating system tools, such as System Monitor, you need to use the SQL cluster network 
or IP address of the node to monitor (which is usually the active node).
In most cases, as a DBA, you probably will administer SQL Server 2012 using Management Studio, 
but sometimes you need to access the individual nodes of the cluster. If you have easy access to the 

Troubleshooting Cluster Problems ❘ 543
cluster, you can always log on locally; if you prefer remote administration, you can use Remote 
Desktop (Terminal Services) to access the individual nodes.
When DBAs begin to administer their ﬁ rst Windows Failover cluster, they get a little confused as to 
where SQL Server actually runs. Keep in mind that a clustered SQL Server instance consists of an 
active node which is running a SQL Server and the passive node which is not running a SQL Server. 
At any one time, a SQL Server instance runs on the active node only, so when you need to look at 
the nodes directly, generally you want to look at the active node. If you don’t know which node is 
currently active, you can ﬁ nd out by using the Failover Cluster Management tool.
When you log into the active node (or connect to it remotely using Remote Desktop) and then bring 
up Windows Explorer (a routine task), you can access the SQL Server shared data disks; but if you 
log on to the passive node, you cannot access the SQL Server shared data disks. This is because 
drives can be accessed from only a single SQL Server node at a time. 
If you access your cluster through Remote Desktop (Terminal Services), be aware of a couple of 
odd behaviors. For example, if you use Remote Desktop to connect to the cluster using the SQL 
cluster network name or IP address, you will connect to the active node, just as if you used Remote 
Desktop to connect to the active node directly (using its network name and IP address); but if a 
failover should occur and you use Remote Desktop to access the cluster using the SQL cluster 
network name and IP address, Remote Desktop gets a little confused, especially if you use Windows 
Explorer. For example, you may discover that your data drives no longer appear to be accessible, 
even though they actually are. To resolve this problem, you may need to log out of Remote Desktop 
and reconnect after the failover.
TROUBLESHOOTING CLUSTER PROBLEMS
Troubleshooting cluster-related issues require a lot of fortitude, persistence, and experience, and 
a support contract with Microsoft Technical Support. The problem is that clustering is somewhat 
complex and requires that you know hardware, shared disk array, hardware drivers, operating 
systems, clustering services, and SQL Server. Any problem you have could be caused by any one of 
them, and identifying the exact cause of a problem is often difﬁ cult.
Another reason cluster troubleshooting is difﬁ cult is that the feedback you get, in the form of 
messages or logs, is not easy to understand, assuming you get any feedback at all. And when you do 
get feedback, the resources for identifying and remedying problems are minimal.
Because of all this, if you have a Windows Failover Cluster, you should plan to purchase Microsoft 
Technical Support. This is a good investment, and one that will pay for itself. The authors have 
used Microsoft Technical Support many times, and in most cases it assisted adequately. You don’t 
need to automatically call support as soon as you have a problem; always try to identify and resolve 
problems on your own if you can. But at some point, especially if your cluster is down and you need 
assistance getting it back up, you need to recognize when you can’t resolve the problem by yourself 
and when you need outside help.
The next section includes some general advice to get you started when you need to identify and 
resolve cluster-related problems. 

544  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
How to Approach Windows Failover Clustering Troubleshooting
The discussion about how to install clustering in this chapter emphasized the importance of testing 
each task after it is performed and only proceeding to the next step if everything works. This 
methodical approach helps you more easily identify causes the problem as soon as possible after it 
happens. For example, if things work correctly but you perform a task and the test fails, you can 
fairly assume that what you just did is directly or indirectly responsible for the problem, making 
problem identiﬁ cation easier. If you don’t perform regular testing and don’t notice a problem until 
after many tasks have been performed, identifying the cause of a problem or problems is much more 
difﬁ cult. Therefore, the best way to troubleshoot problems is to perform incremental testing. This 
also makes it much easier if you have a detailed installation plan that you can follow, helping to 
ensure that you perform all the necessary steps (including testing at appropriate places).
Doing It Right the First Time
You can save a lot of troubleshooting problems by preventing them. Here’s how: 
Be sure that all the hardware for the physical nodes and shared disk array passes the Cluster 
Validation.
Ensure that you use the latest hardware and software drivers and service packs.
Create a detailed installation plan that you can use as your guide for the installation and for 
disaster recovery should the need arise.
Learn as much as you can about Windows Failover Cluster before you begin your 
installation. Many cluster problems are user-created because the people responsible guessed 
instead of knowing for sure what they needed to do.
Develop a Windows Failover Cluster runbook identifying the state, conﬁ guration, and 
instructions on running each cluster to establish a consistent and stable system.
Gathering Information
To help identify the cause of a problem, you often need a lot of information. Unfortunately, the 
information you need may not exist, or it may be scattered about in many different locations, or it may 
be downright misleading. In any event, to troubleshoot problems, you need to ﬁ nd as much information 
as you can. To try to combat these issues, use the following guidelines and resources when gathering 
information to troubleshoot a cluster problem: 
Know what is supposed to happen. If you expect a speciﬁ c result, and you are not getting 
it, be sure that you fully understand what is supposed to happen, and exactly what is 
happening. In other words, know the difference between the two.
Know what happened directly before the problem occurred. This is much easier if you test 
incrementally, as described earlier.
Know whether the problem is repeatable. Not all problems can be easily repeated, but if 
they can, the repetition can provide useful information.
➤
➤
➤
➤
➤
➤
➤
➤

Troubleshooting Cluster Problems ❘ 545
Take note of any onscreen error messages. Be sure that you take screen captures of any 
messages for reference. Some DBAs have the habit of clicking OK after an error message 
without recording its exact content. Often, the exact content of a message is helpful if you 
need to search the Internet to learn more about it.
View logs. There are a variety of logs you can view, depending on how far along you are 
in the cluster setup process. These include the three operating system logs: the cluster log 
(located at c:\windows\cluster\cluster.log); the SQL Server 2012 Setup log ﬁ les 
(located at %ProgramFiles%\Microsoft SQL Server\110\Setup Bootstrap\LOG\
Summary.txt); and the SQL Server 2012 log ﬁ les. There can be a variety of error messages 
in the log; once you identify the error, you can perform a web search to see if anyone else 
has had this error and if there are any suggestions around on how to resolve. 
Perform an Internet search for the error message. If the error messages you identify aren’t 
obvious (are they ever?), search on the Internet, including newsgroups.
The more information you can gather about a problem, the better position you are to resolve the 
problem.
Resolving Problems
Many cluster problems are due to the complexity of the software involved, and it is sometimes faster 
to just rebuild the cluster from scratch, including the operating system. This is especially true if 
you have tried to install Windows Failover Cluster or clustered SQL Server and the setup process 
aborted during setup and did not setup itself cleanly.
When you build a new Windows Failover cluster, rebuilding it to resolve problems is usually 
an option because time is not an issue. However, suppose you have a clustered SQL Server in 
production and it fails where neither node works. Since time now becomes an issue, you should 
bring in Microsoft.
Working with Microsoft
Operating a clustered SQL Server without having a Microsoft Technical Support contract is like 
operating a car without insurance. You can do it, but if you have any unexpected problems, you will 
be sorry you went without.
Generally, there are two main reasons to call Microsoft Technical Support for clustering issues. 
The ﬁ rst situation would be when it’s a noncritical issue that you just can’t ﬁ gure out for yourself. 
In this case, you will be assigned an engineer, and over a period of several days, you work with that 
engineer to resolve that problem. This may involve running diagnostics to gather information about 
your cluster so the Microsoft support engineer can diagnose it.
The second reason to call is because your production cluster is down, and there are no obvious 
solutions that you know of to get it quickly back up. Generally, in this case, call Microsoft Technical 
Support as soon as you can to get the problem ticket started. In addition, you should emphasize to 
the technical call screener (the ﬁ rst person who answers the phone) and the support engineer you is 
assigned to that you are facing a production down situation and that you want to declare a critical 
situation (critsit). This tells Microsoft that your problem is top priority, and you will get special 
➤
➤
➤

546  ❘  CHAPTER 16  CLUSTERING SQL SERVER 2012
attention. When you declare a critsit, the person on the phone will validate that it is a critsit because 
it causes a chain of events to happen within Microsoft Technical Support; but if your production 
cluster is down, you need to emphasize the serious nature of your problem. If it is, you can get 
immediate help with your problem until your Windows Failover Cluster is resolved.
SUMMARY
This chapter represents only the tip of the iceberg when it comes to covering everything the DBA 
should know about clustering SQL Server 2012. It is important to understand the basics around 
installing, conﬁ guring, testing monitoring, troubleshooting and maintaining a Windows 2008 
Failover Cluster running SQL Server 2012. To successfully install a Windows Failover Cluster, start 
with identifying the hardware and verify that it meets the cluster prerequisites. Follow step-by-step 
to conﬁ gure the hardware to be clustered. All of these steps must be conﬁ gured as described; any 
variation will likely prevent the cluster to install successfully. You should run the Cluster Validation 
to verify that the cluster conﬁ guration can be clustered; if any errors are identiﬁ ed, address 
them before proceeding. Then, you may go ahead and create the cluster. Determine the need for 
MSDTC(s) and install them. Using the SQL Server 2012 setup, install the SQL Server instance on 
the cluster. You can have more than one SQL Server instance on a node, or have many nodes each 
running SQL Server instances supported by a passive node or even without a passive node. Run 
the SQL Server 2012 setup to install additional SQL Server cluster instances. Clustering can be 
combined to offer high availability and disaster recovery by deploying a geographically dispersed 
cluster where a cluster is established with nodes that live across two data centers which may even be 
on different subnets. There are different Windows Failover Cluster deployed conﬁ gurations possible 
that you can learn more about by reading clustering articles, the SQL Server 2012 Books Online, 
and any additional information from the Microsoft’s website and elsewhere on the Internet. 





Making Plans ❘ 551
MAKING PLANS
You (or your company) should have a plan for high availability, backup/recovery, and disaster 
recovery. There may be a plan for each, or they may all be included in a single document. In a small 
company, by default this task may fall directly on your shoulders. For larger companies, you may 
be part of a team that plans for these events. In all cases, a risk/beneﬁ t analysis must be done. You 
must consider the likelihood of the event occurring, the cost of the downtime and other potential 
costs associated with the event, and the cost of the solution. Your ﬁ rst job will be the research 
and documentation regarding the risks, costs, and beneﬁ ts. Management will then decide which 
events to plan for and make the risk decision. The DBA is not the risk decision-maker — that’s 
management’s job. However, it is your job to ensure that management understands the issues and 
that a decision is made. It is then your job to implement whatever plan is necessary and to test the 
plan on a regular basis.
In this section you learn the basics of a backup/recovery plan and a disaster recovery plan. 
There is enough information about recovery plans to ﬁ ll up entire books, so consider this a basic 
introduction. High availability is not covered here but you can ﬁ nd information on it in Chapters 
15, 16, 18, and 19.
Backup/Recovery Plan
You or your team has the primary responsibility for the backup/recovery plan, including the 
following: 
Analyze business requirements.
Categorize databases by recovery criteria.
Document the plan.
Validate, implement, and test the plan.
Establish a failed backup notiﬁ cation policy.
Maintain the plan.
Analyze Business Requirements
First, you must gather some requirements for your plan. You need to have answers to the following 
questions: 
Who are the stakeholders/owners for each application or database? You need to determine 
the people from whom you should ask questions. You must also identify the decision-
makers who can approve your ﬁ nal plan.
What is the purpose for this database? Knowing a database’s purpose, such as whether it 
is for a data mart, a data warehouse, or the general ledger accounting database, gives you 
great insight into what might be necessary for your plan.
What is the acceptable downtime for the application/database in the case of an error, 
such as a disk drive error? This is also known as the Recovery Time Objective (RTO). 
➤
➤
➤
➤
➤
➤
➤
➤
➤


Making Plans ❘ 553
What is the size and growth of the database? If this database has not been implemented, 
you are not likely to get great information, but get the best you can. For all implemented 
databases, your normal monitoring procedures can provide this information. You should 
also work in concert with any project teams that develop new projects around this database, 
which may change the answers to this and any of the other questions. For more information 
on estimating the size of your database, read the “Estimating the size of a database” topic in 
Books Online at http://msdn.microsoft.com/en-us/library/ms187445.aspx.
What is the maintenance window for this database? Certain maintenance tasks that 
must be done on the database affect the online response. Items such as Database Console 
Commands (DBCCs), index maintenance, and potentially backups are among these items. 
Your plan must ensure that your work stays within the maintenance window.
What are the budget constraints for this database? Often, there is no speciﬁ c answer to 
this question. Instead, evaluating the risks and the costs is a negotiation process with the 
business. However, a speciﬁ c budget exists; you certainly need to know what it is.
What is the notiﬁ cation plan for this database? When errors occur for which you must 
restore, who should be notiﬁ ed? How should you notify them? In addition, how can you 
know when a database error occurs?
Categorize Databases by Recovery Criteria
If you have many databases, you can do yourself a favor by categorizing the databases into groups. 
Then you can have a plan for each group. You might categorize by the following criteria: 
Criticality: Is this database mission-critical?
Size: Large databases need more time for backup/restore than small databases. However, 
you can mitigate the time with ﬁ legroup backups or other interim measures. Filegroup 
backups are covered later in this chapter.
Volatility: Databases with a larger volume of data change need a different plan than inactive 
databases.
Once you categorize, you can name the groups something useful, like the following: 
Mission-Critical Large
Mission-Critical Small
Business-Critical 
Moderate Impact
Low / No Impact / Noncritical
For each category, choose the following: 
Recovery model: 
Full: Used when no data loss is acceptable.
Bulk Logged: Used when your database is using the Bulk Logged recovery model. 
Simple: Used when we can afford to lose data between full backups.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

554  ❘  CHAPTER 17  BACKUP AND RECOVERY
Backup plan: All groups need a periodic full database backup. Depending on the category 
and recovery model, choose between differential backups, ﬁ le/ﬁ legroup backups, and log 
backups. Also choose whether to use compression and which backup media to use.
Backup frequency: How often should each of the backups be executed?
Backup security policy: The Backup security policy will detail how long backups are 
retained, as well as how they are electronically and/or physically secured. For tape media, 
what is the rotation policy? How can you implement offsite storage? If you use disk-to-disk 
backups (using an external D2D device), or on-disk backups (writing backups to a separate 
set of local disks), how can you secure access to the backup ﬁ le store?
Disaster Recovery Planning
Disaster recovery requires considerable planning. A local disaster can cause severe ﬁ nancial loss to 
the organization. To reduce this, the organization must quickly execute the disaster recovery (DR) 
plan to bring its systems online. It requires a robust disaster recovery plan and periodic DR drills to 
ensure that everything works as planned. Often, organizations have well-intended disaster recovery 
plans, but they have never tested them for readiness; then, in a real disaster, the plan does not go 
smoothly. DR planning is based on the speciﬁ c organization’s business requirements, but some 
general areas need to be addressed to put any plan into action.
Use project management software, such as Microsoft Project, whereby people, resources, 
hardware, software, and tasks and their completion can be input to provide a systematic 
approach to managing the tasks, resources, and critical paths.
Develop a checklist of detailed steps for recovery. More information on what should be on 
this checklist is covered in the next section.
Disaster recovery solutions with Windows failover clusters are commonly used to provide hardware 
redundancy within the data center site and can be conﬁ gured across data centers by using a 
geographically dispersed Windows Server Failover Cluster. Prior to Windows Server 2008, this 
required expensive SAN based solutions. Windows Server 2008 eased the restrictions around 
subnets, and heartbeat latency, which now makes it easier to implement a geographically dispersed 
cluster. However, this is still an extremely complicated, expensive option. Moreover, AlwaysOn 
Availability groups, database mirroring, replication, and log shipping can all be inexpensively 
deployed as alternate disaster recovery solutions. 
Some organizations have a standby disaster recovery site available to take over all operations or, 
at the least, the mission-critical operations. A few of these organizations failover to the disaster 
recovery site periodically to validate that their plan can work in a real disaster. Others may not have 
a disaster recovery plan but an agreement with another organization that offers disaster recovery 
capabilities.
If you are going to implement a Disaster Recovery Site, then you need compatible hardware at the 
DR Site. If this hardware is not already available, you need a plan to acquire the hardware required 
to bring the organization online quickly. Document the current necessary hardware. For computers, 
consider the number and type of CPUs and speed, Intel versus AMD, hyper-threading, number of 
cores, disk drive capacity, RAID level, and the amount of physical memory required. Preferably, try 
➤
➤
➤
➤
➤

Making Plans ❘ 555
to acquire the exact hardware to minimize surprises. For the storage subsystem, consider the disk 
space requirements, the number of LUNs required, and the RAID level. For the network, acquire 
a similar network infrastructure to maintain the same performance. Some questions to ask include 
the following: 
How quickly can these computers be made available? Who will deliver or pick up the 
hardware?
Will the computers be preconﬁ gured, or will the DR team need to conﬁ gure them? Who 
will provide the expertise, and what is the availability for that staff resource?
Will the storage be preconﬁ gured with the LUNs and RAID levels, or will the DR team 
need to conﬁ gure it? Who will provide the expertise, and what is the availability for that 
staff resource?
Who will acquire the network equipment, and who will have the expertise to set it up and 
conﬁ gure it?
Will the DR site have Internet access — to download service packs, for hotﬁ xes, and for 
e-mail?
Make a detailed list of all software required, any hotﬁ xes, and service packs. Take an inventory of 
how each is going to be available to the DR team. Make sure that the software is at the required 
version level and that licensing keys are available and valid. Determine who is responsible to make 
available the software and the contact information for that staff member. If the software is in a 
certain location, know who has the physical keys and what access they have. In this scenario, 24/7 
access is required. In the event of a disaster, you need a list of staff resources to be contacted, which 
must be current and therefore periodically updated. Know who is responsible to maintain this list 
and where it will be found during a disaster. You also need to know the chain of command and who 
is onsite and offsite.
Additionally, you should create a detailed plan of the onsite roles required to execute the plan and 
who will ﬁ ll those roles. Ensure that there is a backup resource in case a staff resource is missing. 
Determine how many staff are needed in each role, how they will arrive at the site, and who will be 
the overall project manager or lead to escalate any issues. Assign and record who has the passwords 
and what logins are required to make the systems available for business.
As mentioned previously, the DR site must be accessible 24/7 and conveniently located. As larger DR 
deployment can take days to execute, the site should have beds for staff to take naps and easy access 
to food and transportation. Identify who has the key to access the remote site; if that person is not 
available, who is the designated replacement? Can resources remotely access the site if they must 
work from a remote location, and what is required to have remote access turned on? Are the backups 
for all the databases available at the DR site, or who is responsible to bring them there? If that staff 
resource is unavailable, who is the designated replacement?
To ensure that the DR plan will work during a real disaster and to reduce loss, as a best practice, 
periodically simulate a disaster drill and put the DR planning in action to identify any steps that 
were not taken into account, how quickly the organization can be expected to be online again, and 
areas that can be streamlined to speed the process. Most importantly, ensure that the plan will 
execute as expected, smoothly and quickly. To get the most effect from this simulated scenario, 
everyone should approach it as if it were a real disaster and take all actions exactly as planned.
➤
➤
➤
➤
➤

556  ❘  CHAPTER 17  BACKUP AND RECOVERY
Creating the Disaster Recovery Plan
To create your Disaster Recovery plan, start by collecting some important information. First, 
interview the business owners of any applications that use databases under your control. You need 
to ﬁ nd out from them what their requirements are in the event of a disaster. This information 
enables you to start categorizing the databases under your control into different classes of disaster 
recovery options. This categorization might be as simple as Databases That Need Disaster Recovery, 
and Databases That Don’t Need Disaster Recovery. Or, if everyone needs disaster recovery, but 
some need to be able to continue running while other systems are okay with hours, days, or weeks 
of down time before they are online at the disaster recovery site, this could be another way to 
categorize the databases.
You will also need to document the following information: 
Contact list
Decision tree
Recovery success criteria
Location of keys, backups, software, and hardware
Infrastructure documentation
You should have a contact list of management people who can declare and activate the emergency 
callout. All necessary contact information must be available. Make sure that you know how 
to contact people in the event of an emergency and that relevant staff are potentially waiting for 
a call, email, text, page, and so on. You also need to document alternate contact points in the 
event that people are out of phone coverage, on a ﬂ ight somewhere, with a dead phone battery, or 
otherwise un-contactable. Having that alternate contact point could make the difference between 
ﬁ nding someone to help, and not being able to get your Disaster recovery plan started. The 
procedures for responding must also be documented (such as who makes calls).
You should also have a contact list for everything else. This list should include vendors, technicians, 
off-site storage people, service people, parts suppliers, transportation companies, and so on — 
everyone! The list should also include backups when primaries cannot be contacted.
Departments should have fallback operational procedures; although, this might not be in your 
documents. It is more likely this would be in departmental documentation. However, both plans 
should sync and make sense when put together.
The decision tree speciﬁ es what you need to do based on the circumstances. You may have a 
separate decision tree for disaster recovery than the one you use for normal recovery. When a 
disaster strikes and mission-critical processes are down, things can get stressful. The decision 
tree prompts you, so you mostly just follow the plan, instead of making on-the-ﬂ y mistakes. The 
decision tree must be logical, clear, and easy to understand and follow because you will be using it 
under duress. Keep it simple to the point where it can be completed while still encompassing enough 
to cover most scenarios. 
The decision tree should classify database loss scenarios, such as natural disasters — for example, 
hurricane, earthquake, and so on — that can affect a wide area. You may not rely on locally 
➤
➤
➤
➤
➤

Making Plans ❘ 557
held backups, software, or hardware. The tree must cover single-location loss — power loss, ﬁ re, 
explosion; you might recover to a site close to the affected location.
The speciﬁ c decision tree for your purposes should include normal recovery scenarios, such as single 
server/data corruption or single database loss, disk drive, controller, memory failure, user error, and 
application failure. You should work to recover the missing data quickly, while not affecting other 
systems.
Another item you might include in your decision tree is loss of performance or service. In these 
cases, the database is not damaged but inaccessible or slow. You should debug and correct this 
quickly. Refer to Chapter 13 for guidance on performance tuning T-SQL.
The decision tree should also prioritize recovery steps. Thinking about these steps can enable you 
to make a plan that minimizes downtime and maximizes parallel work between all the players who 
need to be involved.
The decision tree should identify the most critical databases so that recovery can be completed in 
the best order for the business. Don’t forget about dependencies between databases.
Critical processes should be identiﬁ ed in the decision tree: SQL Server Integration Services (SSIS), 
SQL Server Analysis Services (SSAS), SQL Agent, Extract, Transform, Load (ETL) processes, and so 
on. What should be done for each of these processes? For instance, you might need to stop a process 
scheduler until the restore is complete.
You may also include security failures, whereby the database health is okay, but security is 
compromised due to malicious software, individual access, virus attack, and so on. You should 
discover security breaches and protect data. Refer to Chapter 8 for guidance on securing the 
database instance.
The recovery success criteria can be layered and may include service-level agreements (SLAs). When 
talking about SLAs, two terms come up very frequently and are discussed in the following list:
Recovery Time Objective (RTO): Recovery Time Objective is the amount of time that you 
have to restore a particular system to operation in the event of a disaster. An RTO of 8 
hours means that the affected system must be operational after a disaster within 8 hours 
from the disaster occurring.
Recovery Point Objective (RPO): Recovery Point Objective is a measure of the amount of 
data that can be lost from a system in the event of a disaster. An RPO of 8 hours means that 
it is acceptable to lose any data entered in the 8 hour period before a disaster.
The ﬁ rst measure of success could be that you have met the SLA on RTO, such that the system is 
online again within the RTO period. The second level of success is that the RPO has been met, and 
that no more than the acceptable amount of data as deﬁ ned by the RPO has been lost. Be sure you 
understand and document these criteria; you are likely to be measured by them.
Your documentation should include everything you need to get the job done. The location of keys 
or keyholders and the ability to contact and arrange for offsite storage people to deliver backups 
is important. Hardware, or access to and a prompt response from hardware people, as well as 
software support and availability of the software disks is also necessary. You should also have easy 
access to the levels of software, service packs, and driver levels for your server.
➤
➤

558  ❘  CHAPTER 17  BACKUP AND RECOVERY
Any other infrastructure documentation you might need should also be included, such as naming 
conventions, DNS, or network information — anything you need to get the job done.
List recovery steps in the correct order based on business needs. Don’t assume you will think calmly 
and make the best decisions on-the-ﬂ y when bad things happen. Make all the basic decisions in 
advance, and when the bad thing happens, engage your brain, but follow the documented process.
Validating, Implementing, and Testing the Plan
This is often not nearly as difﬁ cult as getting the proper information from the business — so don’t 
worry. If you can get the true needs of the business, then you can usually implement your backup/
restore plan. Of course, no plan is good if it doesn’t work, and you won’t know whether it works 
until you test it.
The people who need to implement this plan should practice it regularly through testing. You should 
plan for regular testing. Test at some well known period, maybe every 6 months. Also test anytime 
changes are made to the systems that require changes to plans. Make sure that any new staff 
understands the plans. Ideally, have new staff training that includes at least a discussion of these 
plans, and preferably include a practice run through for code Disaster scenarios. Anytime there is a 
signiﬁ cant percentage change in staff since the last planned test, run another test.  
Make sure that testing includes setting up secondary servers, completing the restores, and making 
the applications/data available. You should simulate failures as well, and practice the responses 
to them. Simulated failures might be loss of the most recent full backup or a transaction log. You 
should consider that the restore sight might be in a different time zone, or a database might be 
much, much larger than it used to be. What happens if key people do not respond or your contact 
list is inaccurate? What if access cards no longer work or keys are not available?
Failed Backup Notiﬁ cation Policy
Because the success of your plan depends on successful backups, you need a plan to receive 
notiﬁ cation when backups fail. Remember you don’t have a backup until it’s copied to a separate 
server, restored, and checked to be valid, so routinely restoring backups and validating should be an 
integral part of your planning.
You should also use DBCC commands to ensure that you back up a healthy database. In addition, 
the database backups should occur after other normal maintenance tasks, such as database 
shrinking and index maintenance. This is because when you have to restore from a database backup, 
you are restoring a database ready to go and not a database in need of additional maintenance.
Maintaining the Plan
Following are four steps to successfully maintain the plan: 
Communicate the plan: For effective communication, the documentation should be publicly 
available within your company; and IT, the business users, and management should be 
informed of its content and location.
Establish a policy to periodically rehearse the plan: Some companies carry out a drill every 
other year; others do this annually. You might have a complete companywide call out once 
➤
➤

Overview of Backup and Restore ❘ 559
a year and an IT test more frequently. In any case, you must test the plan on a schedule. 
Infrastructure changes and software and hardware updates can all conspire to make your 
plan unusable. You may have worked hard to do all the steps up to this point, and you may 
have done them all perfectly, but when disaster strikes, you will be measured solely by your 
ability to actually recover, not by how ﬁ ne a plan you may have. Rehearsal is the only way 
to guarantee success.
Establish a policy to periodically validate the plan: The policy to periodically validate the 
plan centers around the changing business environment. The business may have changed 
processes, or the ability or willingness to absorb risk, or some other factor that may have 
invalidated your plan. You should revalidate the information you gathered from the business 
and reassess your plan on a scheduled basis. In addition, be aware of new projects and how 
they may affect your planning.
Revise the plan as needed: The last requirement is to keep the plan up to date. Revisit the 
plan, and revise whenever needed. It is fairly common to come up with a plan and then let it 
grow stale on the shelf. This often renders the plan useless or, even worse, dangerous to use.
OVERVIEW OF BACKUP AND RESTORE
Before you can effectively formulate a backup and restore plan, you need to know how backup and 
recovery work on a mechanical level. SQL Server has several different backup and restore processes 
that you can use, depending on the needs of your organization. This section examines how backup 
and restore work and helps you choose the best plan for your needs.
How Backup Works
Database backup is a procedure that safeguards your organization’s investment to reduce the 
amount of data loss. A database backup is the process of making a point-in-time copy of the data 
and transaction log into an image on either disks or tapes. SQL Server implements versatile backup 
processes that can be used separately or together to produce the optimal backup strategy required 
by an organization. Moreover, SQL Server can perform the database backup while it is online and 
available to users. In addition, it supports up to 64 concurrent backup devices. The following types 
of backup are available: 
Full backup: This is a copy of all data in the database, including the transaction log. Using 
this backup type, you can restore the database to the point in time when the backup was 
taken. It is the most basic of the backups and is often required prior to any of the other 
backup types. When restoring from a full database backup, all the database ﬁ les are 
restored without any other dependencies, the database is available, and it is transactionally 
consistent.
Partial backup: This is a way to back up only those parts of the database that change. This 
reduces the size of the backup and the time it takes to backup and restore. It is a copy of 
the primary ﬁ legroup and read/write ﬁ legroups. To take advantage of this type of backup, 
you need to group together the tables that change into a set of ﬁ legroups and the tables that 
are static or history in a different set of ﬁ legroups. The ﬁ legroups containing historical data 
➤
➤
➤
➤


Overview of Backup and Restore ❘ 561
File differential backup: This is a copy of the ﬁ le or ﬁ legroup of all extents modiﬁ ed since the 
last ﬁ le or ﬁ legroup backup. A transaction-log backup is required after this backup for read/
write ﬁ les or ﬁ legroups. Moreover, after the restore, you need to restore the transaction log 
as well. Using the ﬁ le backup and ﬁ le differential backup methods increases the complexity of 
the restore procedures. Furthermore, it may take longer to restore the complete database.
Copy-only backup: This can be made for the database or transaction log. The copy-only 
backup does not interfere with the normal backup restore procedures. A normal full database 
backup resets the differential backups made afterward, whereas a copy-only backup does not 
affect the next differential backup; it still contains the changes since the last full backup. A 
copy-only backup of the transaction log does not truncate the log or affect the next normal 
transaction log backup. Copy-only backups are useful when you want to make a copy of the 
database for testing or development purposes without affecting the restore process. Copy-
only backups are not supported in SSMS and must be done via T-SQL.
The transaction log in SQL Server is a main component for a relational database system that 
maintains the ACID properties for transactions: atomicity, consistency, isolation, and durability. SQL 
Server implements the write ahead logging (WAL) protocol, which means that the transaction-log 
records are written to a stable media prior to the data being written to disk and before SQL Server 
sends an acknowledgment that the data has been permanently committed. Stable media is usually 
a directly attached disk drive, but it can be any device that guarantees that on power loss, no data 
will be lost. Even on direct attached systems, this can be a challenge; as disk drives implement write 
caches, RAID controllers, even at the simplest level, also implement caches, which either need to 
be write-disabled, or battery-backed. Any external storage system such as a SAN system must also be 
checked to conﬁ rm that the cache is battery-backed and will guarantee the consistency of any 
written log records during a power failure. There is a new trend for using solid state storage 
devices, which can take many forms. If you leverage these devices, you need to ensure that they 
either deliver guarantees around writes if a power failure occurs, or that they are used in places where 
write cache performance is not an issue, such as if used for tempdb, where all data is deleted in the 
event of a system restart. An increasingly common trend on high-performance systems that need the 
highest levels of Transaction Log write performance is to place the Transaction Log on solid state 
storage. Although this is great from a performance perspective, you must also guarantee that the log 
records can survive a power outage.
The SQL Server database engine expects the Transaction Log to be consistent on restart; if it is not, 
it will identify the database as corrupted because the data consistency of the database cannot be 
determined.
In addition, when a data modiﬁ cation occurs, SQL Server generates a new log sequence number 
(LSN) used on restart to identify the consistency of the data while performing database recovery. 
The LSN is used when restoring the transaction log; SQL Server uses it to determine the sequences 
of each transaction log restored. If, for example, a transaction-log backup from a backup log chain 
is not available, that is known as a broken log chain, which prevents a transaction-log recovery past 
that point. Backing up the transaction log to point-in-time recovery is a critical part of a backup 
strategy. A DBA can perform three types of transaction-log backup: 
Pure transaction-log backup: This is when there have not been any bulk-logged operations 
performed on the database. That is, every data modiﬁ cation performed is represented in the 
➤
➤
➤



564  ❘  CHAPTER 17  BACKUP AND RECOVERY
Scripting Wizard
SSMS provides a scripting wizard that enables you 
to generate scripts for selected objects within the 
database. This is a great way to keep a record of 
the scripts needed to regenerate a database and can 
be a good way to get a version check on any recent 
changes to the DB.
Use this with a source code control system to 
determine the delta between previously generated 
scripts and scripts currently generated to ﬁ nd 
changes to objects in the database.
Import/Export Wizard
SSMS provides Import/Export capabilities which 
are accessed from the Tasks item on the database 
context menu (see Figure 17-1).
As you can see, you can drive this from either end. 
In the following example you walk through an export driven from the source database.
 1. 
Select Export Data to start the wizard. The wizard welcome screen appears as shown in 
Figure 17-2.
 2. 
Select Next in the welcome screen and you are taken to the Choose a Data Source page, 
shown in Figure 17-3. Enter the server name and authentication method for this server, 
and then you can select the database you wish to export data from.
FIGURE 17-1
FIGURE 17-2
FIGURE 17-3

Overview of Backup and Restore ❘ 565
 3. 
The next page in the wizard enables you to choose a Destination (see Figure 17-4). Again, 
choose a server, authentication method, and then database name. The wizard defaults to 
using the default database for the login used to authenticate against this server. 
 4. 
On this page choose Table Copy to use a table copy, or Query to write a query to select 
data. In this example, choose the ﬁ rst option to copy data from one or more tables or 
views (see Figure 17-5). 
FIGURE 17-4
FIGURE 17-5
 5. 
The wizard now presents you with a list of 
all the tables and views that are available. 
In the example database, there are only a 
small number of tables that you can see in 
Figure 17-6. Select the tables and views to be 
exported.
 6. 
You now have the option to run the package 
immediately, or to save the SSIS package. 
If you choose to save the package, you can 
specify if you want to save it to SQL Server, 
or the ﬁ le system. You can also deﬁ ne 
a package protection level to secure the 
package contents. You can see these options 
FIGURE 17-6

566  ❘  CHAPTER 17  BACKUP AND RECOVERY
in Figure 17-7. The penultimate stage of the wizard (shown in Figure 17-8) shows you the 
selections you have made.
FIGURE 17-7
FIGURE 17-8
 7. 
Select Finish to execute the selections 
made during the wizard. In this 
case it executes one SSIS package to 
export the selected tables from the 
Source Server, creates the new tables 
in the destination server, and imports 
the data to the newly created tables. 
The wizard then reports on the 
status of the package execution. 
Figure 17-9 shows everything 
completed successfully.  
One thing to note is that if you get errors here 
and you go back to make changes and re-run 
the package, you may encounter additional 
errors caused because the wizard doesn’t 
attempt any cleanup if it fails part way 
through.
The most obvious errors you might encounter 
are when tables are created, but the data 
FIGURE 17-9


568  ❘  CHAPTER 17  BACKUP AND RECOVERY
 2. 
On the next page select your source server, as shown in Figure 17-12.
 3. 
Select the destination server as shown in Figure 17-13.
FIGURE 17-12
FIGURE 17-13
 4. 
On the next page choose how you would like the wizard to make the transfer. You can 
choose between Detach/Attach, or using SMO. Figure 17-14 exempliﬁ es choosing the SMO 
method.
 5. 
Select which databases to copy. You can choose to either copy or move the selected 
databases, and this choice is on a per database basis, so you can move some databases, and 
copy others if that’s what you want to do. Figure 17-15 shows the People database being 
selected to be copied.
FIGURE 17-14
FIGURE 17-15

Overview of Backup and Restore ❘ 569
 6. 
Now specify what the new database will be called, and which ﬁ les will be created. By 
default these options are pre-populated with the information from the source database. 
There are other options available on this page (see Figure 17-16) that determine what action 
to take if the destination database exists. 
 7. 
Next specify which objects outside the selected databases you want to include with the copy, 
as shown in Figure 17-17.
FIGURE 17-16
FIGURE 17-17
 8. 
Conﬁ gure the SSIS package that will perform the database copy, as shown in Figure 17-18.
 9. 
Now schedule the package. At this point you can choose to run the package immediately, 
or schedule it for execution later. In Figure 17-19 runs the package immediately. Be sure to 
conﬁ rm all the choices made before running the package (see Figure 17-20).
FIGURE 17-18
FIGURE 17-19





574  ❘  CHAPTER 17  BACKUP AND RECOVERY
model in an OLAP or Report database where there is no daily modiﬁ cation activity, as there is limited 
data loss risk if the databases are backed up right after any nightly data load. There is no chance of data 
loss throughout the day, as nothing would have changed. In addition, some data loss may be acceptable, 
as the OLAP and Reporting databases can be reloaded from the OLTP data source whenever needed.
Simple Recovery Model
The simple recovery model does not save the transaction log; instead, the checkpoint process 
truncates it. Therefore, no one has to maintain the transaction log. This recovery model is commonly 
used for development, read-only, and test systems for which transaction-log backups are not required. 
If there is data loss, a new copy of the data can be reloaded from the OLTP data source. If the DBA 
switches to this recovery model from one of the others, the transaction- log continuity is broken 
because there is no way to back up the transaction log. In this recovery model, there is no point-in-
time recovery because the DBA cannot back up the transaction log. Therefore, any restore would be 
from the previous full, and any differential, backups.
Switching Recovery Models
SQL Server provides complete ﬂ exibility to switch among the recovery models. However, be aware 
of the limitations when switching among them, as switching can result in data loss during recovery. 
The following list outlines the limitations of switching recovery models: 
Switching from full to bulk-logged: Because bulk-logged database operations may be 
performed, a transaction-log backup is recommended at a minimum so that the DBA can 
recover to this last transaction log if the tail transaction log is not available. To change to 
this recovery model, use this command: 
 ALTER DATABASE < db_name> SET RECOVERY BULK_LOGGED
Switching from full to simple: Because the transaction-log continuity is broken by this 
recovery model, a transaction-log backup is recommended, at minimum, before the switch. 
After the recovery model switch, transaction-log backups and point-in-time recovery are 
disallowed. To change to this recovery model, use the following command: 
 ALTER DATABASE < db_name> SET RECOVERY SIMPLE
Switching from bulk-logged to full: Because bulk-logged database operations may have been 
performed and to minimize potential data loss if the tail transaction log is not accessible, a 
transaction-log backup is recommended after the switch. To change to this recovery model, 
use the following command: 
 ALTER DATABASE < db_name> SET RECOVERY FULL
Switching from bulk-logged to simple: In this recovery model there is a greater chance of 
data loss in case of a database failure, so at a minimum, a transaction-log backup is highly 
recommended before the switch. To change to this recovery model, use the following command: 
 ALTER DATABASE < db_name> SET RECOVERY SIMPLE
➤
➤
➤
➤
 
 
 
 

Overview of Backup and Restore ❘ 575
Switching from simple to full: To enable the full recovery model to start to apply 
transaction-log backups, a full, differential, ﬁ le, or ﬁ legroup backup is required after the 
switch. To change to this recovery model, use the following command: 
 ALTER DATABASE < db_name> SET RECOVERY FULL
Switching from simple to bulk-logged: To enable the bulk-logged recovery model to start to 
apply transaction-log backups, a full, differential, ﬁ le, or ﬁ legroup backup is required after 
the switch. To change to this recovery model, use the following command: 
 ALTER DATABASE < db_name> SET RECOVERY BULK_LOGGED
The recovery model is conﬁ gured for each database. You can also switch the recovery model from 
SQL Server Management Studio by opening the Database Properties and choosing Options, as 
shown in Figure 17-21.
➤
➤
FIGURE 17-21
Backing Up History Tables
SQL Server maintains the backup history for the server in the MSDB database in a group of tables 
from which it can identify the backup available for a database. In the Restore dialog, SQL Server 
presents the restores available for the database. The tables are as follows: 
Backupfile: A row for each data or log ﬁ le backed up
Backupfilegroup: A row for each ﬁ legroup in a backup set
➤
➤


Overview of Backup and Restore ❘ 577
To restore a database, a user must have at minimum the following permissions: 
Server role: dbcreater
DB role: db_owner
Backing Up System Databases
SQL Server system databases are critical to the operation of each SQL Server instance. These 
databases are not often modiﬁ ed, but they contain important information that needs to be backed up. 
After creating a new SQL Server instance, develop a backup plan to perform a full backup of all the 
system databases, except for tempdb. SQL Server re-creates tempdb every time it is restarted because 
it does not contain any data to recover. Backing up these system databases takes only minutes, so 
there is no excuse for not having a proper backup. You could often schedule these backups nightly 
and do extra backups to your local hard drive before and after any changes you make. That keeps 
you safe until the current night’s normal backup.
Master
The master database contains the login information: metadata about each database for the SQL 
instance. Moreover, it contains SQL Server conﬁ guration information. For example, the database is 
altered every time you do the following: 
Add, remove, or modify a database level setting.
Add or delete a user database.
Add or remove a ﬁ le or ﬁ legroup in a user database.
Add, remove, or modify a login's security.
Modify a SQL Server serverwide conﬁ guration.
Add or remove a logical backup device.
Conﬁ gure distributed queries or remote procedure calls (RPC).
Add, modify, or remove a linked server or remote login.
Although these modiﬁ cations occur infrequently, when they do, consider doing a full database 
backup. If a backup is not performed, you stand to lose the modiﬁ cations if a previous backup of the 
master is restored. Moreover, as a precautionary measure, before and after adding any service pack 
or hotﬁ x, perform a new backup of the master database.
MSDB
The msdb database contains SQL jobs, backup jobs, schedules, operators, and backup and restore 
histories and can contain Integration Services packages and other items. If you create a new job 
or add a new Integration Services package and msdb were to fail, the previous backup would not 
contain these new jobs and would need to be recreated.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

578  ❘  CHAPTER 17  BACKUP AND RECOVERY
tempdb
tempdb cannot be backed up. Because it is re-created every time SQL Server is restarted, no data in 
it needs to be recovered.
Model
Typically, the model database changes even less frequently than the other system databases. Model 
is the template database used when a new database is created. If you want a certain database object 
to be present in every new database, such as a stored procedure or table, place it in Model. In these 
cases, it should be backed up; otherwise, any Model modiﬁ cations will be lost and need to be re-
created. In addition, keep scripts of any changes you make to the model, just to add another layer of 
safety.
Full-Text Backup
Full-text search performs fast querying of unstructured data using keywords based on the words in 
a particular language. It is primarily used to search char, varchar, and nvarchar ﬁ elds. Prior to 
querying, the full-text index must be created by a population or crawl process, during which 
full-text search performs a breakdown of the keywords and stores them in the full-text index. 
Each full-text index is then stored in a full-text catalog. Then a catalog is stored in a ﬁ legroup. 
Unlike previous versions of SQL Server, in SQL Server 2012 a full backup includes the full-text 
indexes. In SQL 2005, full-text indexes were part of a catalog that existed in a ﬁ legroup with a 
physical path and was simply treated as a database ﬁ le. SQL Server 2012 treats all the catalog as 
a virtual object–simply a collection of full-text indexes. Full-text indexes are now stored and treated 
like other indexes for the purpose of backups. To backup all the full-text indexes, you must discover 
the ﬁ les that contain any full-text index and then backup the ﬁ le or ﬁ legroup.
The end result is that backups for full-text indexes are completely incorporated into the standard 
backup architecture. You can place full text indexes on separate ﬁ legroups and the Primary 
ﬁ legroup, or allow them to live in the same ﬁ legroup as the base table. This improvement makes 
administration easier than in prior releases.
Verifying the Backup Images
With any backup solution, a critical operation is verifying the backup images that they will restore. 
Often, a DBA may be meticulously doing backups, but along the way the database becomes 
corrupted, and every backup from that point on is not usable. Plan on doing periodic restores to 
verify recoverability. In addition, perform database consistency checks to validate the database 
structures. Use the RESTORE VERIFYONLY T-SQL command to perform validation checks on the 
backup image. It does not restore the backup, but it performs validation checks, including the 
following: 
Conﬁ rms the backup set is readable.
Page ID.
If the backup were created WITH CHECKSUMS, then it will validate it.
Checks destination devices for sufﬁ cient space.
➤
➤
➤
➤



Preparing for Recovery ❘ 581
After the redo phase is the undo phase, where any transactions that did not complete are rolled 
back. Depending on the amount of work and the length of the transactions at the time before 
shutdown, this phase can take some time. For example, if the DBA was in the middle of deleting 
10 million rows, SQL Server is required to roll back all those rows during recovery. SQL Server 
does make the database available to users while in the undo phase, but users should expect some 
performance impact while in the redo phase.
PREPARING FOR RECOVERY
To mitigate the risk and extent of data loss, one of the DBA’s most important tasks is database backup 
and planning for recovery. You need to develop a backup plan that minimizes data loss and can be 
implemented within the maintenance window of time allowed. Choose the best SQL Server backup 
capabilities to achieve the preferred backup plan — one that meets the continuity and data loss 
requirements for the business. You must also set up the backup procedure and monitor it every day to 
ensure that it works successfully. That includes validating that the database backup restores properly.
An organization may be current with its backups and assume that it has the necessary backups to 
restore the database, only to ﬁ nd that the database was corrupted and some of the recent database 
backups will not restore. Cases like these can go undiscovered for months until someone needs to 
restore a database and ﬁ nds out that it is not recoverable. To reduce this risk, run the database-
consistency checks against each database and design a process to test the recoverability of the 
database backup. In addition, send database backups offsite to protect them in case of a local 
disaster, but keep local copies of recent backups in case you need to perform a quick restore.
Another critical task is disaster recovery planning. If the organization data center were to be 
completely destroyed, you should quickly deploy a new data center with minimum data loss and 
minimum business disruption. Disaster recovery planning is not complete until a team periodically 
simulates a data center failure and proceeds through the test drill to deploy a new data center.
Recoverability Requirements
Any backup planning should start with the end goal in mind: the recoverability requirements. You 
have already covered the planning in the previous section, “Making Plans,” but following are a few 
more things you might also consider: 
Perhaps only part of the database must be online. You can consider a piecemeal restore 
to reduce your restore time, especially on larger databases for which a restore can take a 
long time. Determine what parts of the database must be available, and arrange the data 
into ﬁ legroups so that you can recover the most critical ﬁ legroups ﬁ rst. Archived data or 
reporting data is less critical and can be recovered last.
The organization may allocate newer, redundant hardware and RAID array with a high-
availability solution to mitigate downtime. A company might also consider faster and more 
backup devices to quickly restore the database.
Determine how easy or difﬁ cult it would be to re-create lost data for each database. For 
some databases, data can be easily re-created by extracting data from another system or 
➤
➤
➤

582  ❘  CHAPTER 17  BACKUP AND RECOVERY
from ﬂ at-ﬁ le data loads. Typically, decision-support databases use ETL tools to extract 
data; for example, if some unrecoverable data loss occurred, the ETL tool can be executed 
to reload the data.
What is the acceptable downtime in a media failure, such as a failed disk drive? As disk 
technology continues to become less expensive, most organizations deploy databases 
on a fault-tolerant disk array that reduces the exposure of one of the disk drives failing, 
causing the database to become unavailable. For example, on a RAID 5 set, loss of a 
single drive can cause a noticeable performance slowdown. If a second drive in the same 
RAID 5 were to fail, the data would be lost. To mitigate this risk, have spare drives in the 
disk array system and get a service-level agreement from the hardware provider to deliver 
and install the drives. Another common scenario is a department inside the organization 
deploying a database in a less than ideal hardware environment. With time, the database 
becomes mission critical to that department, but it lives under the DBA's radar with 
no accountability. The DBA should attempt to identify all database sources within the 
organization and develop a recovery plan.
Determine which databases have any external dependencies on other databases, requiring 
both databases to be restored for users to perform their daily activity. Determine whether 
there is any linked server(s), external application(s), or mainframe connectivity on which a 
database has dependencies.
Identify the available hardware that can be allocated for redeployment and where it is 
located.
Identify the staff required for backup, restore, and disaster recovery. They need to 
understand the disaster recovery procedures and where they ﬁ t in these procedures. Record 
when all staff members are available, their contact numbers, the chain of communication, 
and the responsibility of each member. Determine the chain of command and ﬁ nd out, if 
the lead is unavailable, whether backup members have the expertise to carry out the duties 
for backup, restore, and disaster recovery. Find out the expertise of the staff and what 
additional training they might need to support the environment. Identify any training 
classes that may be beneﬁ cial.
Finally, document any information about stored SQL jobs, linked servers, and logins that 
may be needed when the database is restored onto another database server.
Data Usage Patterns
Part of your recovery plan should include analyzing how your data is used in a typical scenario. 
Determine for each database how often the data is modiﬁ ed. You ll require different backup 
strategies for a database that may have a data load once a day than for others that may be read-only 
or some that change every minute. Separate the tables that are modiﬁ ed from read-only tables. Each 
type can be placed on different ﬁ legroups and a backup plan developed around it.
Identify the usage pattern of the databases during the day to determine the backup strategy to use. 
For example, during high activity, a DBA may schedule more frequent differential or transaction-log 
backups, whereas full backups may be performed during off-peak hours.
➤
➤
➤
➤
➤

Preparing for Recovery ❘ 583
Determine the disk space used by the transaction log during peak times and the log’s performance. 
For example, during peak times, the transaction log may ﬁ ll the available disk drive allocated to 
it. Moreover, during peak times, the number of disks allocated for the transaction log may not be 
adequate to sustain the database’s performance. The database recovery model setting affects both 
disk space and performance.
For a database in the full recovery model, consider switching to bulk-logged mode during bulk 
operations to improve performance, as that will incur minimal transaction logging. Prior to the 
start of the bulk operations, you should at minimum perform a transactional or differential backup 
to guard against the risk of a data-drive failure when the tail transaction log may not be accessible.
Also consider how the database is to be used. If the database is mission critical, apply redundancy 
around the hardware. Start with a highly redundant storage system, using RAID10, and then add on 
additional hardware capabilities as you can afford them, up to and including a completely duplicate 
hot standby system using a failover cluster. Identify what level of data loss the company can afford, 
and plan to back up the transaction log to meet the time requirement. Also use the full recovery 
model so that you can get recovery to the point of failure.
Maintenance Time Window
Sometimes, the backup strategy is dictated by the maintenance time window available to perform 
database defragmentation, backups, statistics updates, and other maintenance activities. To keep 
enhancing the customer experience, organizations demand more timely information and give 
customers greater access to information, and customers are therefore more dependent on having this 
information. This presents a challenge to create the best customer experience, mitigate the risk of 
data loss, and enable quick restores if the database system fails.
The task of the DBA is to ﬁ nd the best backup strategy to meet the organization’s business 
requirements. Usually, the maintenance time window is limited. SQL Server implements various 
backup options that can be used in combination to meet these requirements. The following are some 
of the challenges you face when designing a backup strategy: 
Available backup time may be limited in a mission-critical, highly available database. 
Organizations often have SLAs and must ﬁ nish their maintenance by a certain time when 
users are back on the system. If the backup takes longer, it may delay other database 
activities, which might not ﬁ nish by the time users log in to the system, costing a company 
opportunity loss.
There may be a large number of databases to back up during the maintenance time window. 
You can try to optimize your time for all available backup media by performing concurrent 
backups within the capacity of the database server.
A growing database puts pressure on the maintenance window. Additional backup devices, 
higher-performance database servers, and faster I/O may be needed to relieve the pressure. 
Sometimes the maintenance time window can be increased, but oftentimes it cannot. You 
may need to consider a SAN copy solution to speed the backup process.
Other database activities are likely performed on all the databases in the database server 
(for example, database-consistency checking, defragmentation, update statistics, and 
➤
➤
➤
➤

584  ❘  CHAPTER 17  BACKUP AND RECOVERY
perhaps data loads). As the database grows, these other activities may take longer to 
perform, too.
Software updates, security patches, service packs, and database structure updates may need 
to ﬁ t within this maintenance time window.
Full-text catalogs may need to be processed.
As more organizations see the beneﬁ t of decision-support systems such as SQL Server 
Analysis Services, the analysis services database may need to be processed during this time.
To meet these requirements, a small database can use a full database backup every night. However, 
as the database becomes larger, that may not be possible. A good next step is to perform a full 
database backup on the weekend and nightly full differential backups. As the database becomes 
larger, consider moving read-only and read/write data to different ﬁ legroups, and then use full 
partial backups during the weekend and partial differential backups at night. As the database 
continues to grow, consider nightly backup of individual ﬁ les. 
Other High-Availability Solutions
When your database has been deployed in a high-availability solution, such as AlwaysOn, failover 
clustering, log shipping, or data mirroring, it may require additional backup considerations: 
If you use the AlwaysOn technology new in SQL Server 2012, you can modify your backup 
plans. For example, one reason to create a secondary replica might be to ofﬂ oad the IO load 
from backups from the Primary Replica to the Secondary. In this model, you would not 
take backups from the Primary Replica, or other Secondary Replicas, but have a dedicated 
Secondary Replica speciﬁ cally for taking backups from. You can specify a Replica to be the 
preferred location to run backups using the BACKUP_PRIORITY conﬁ guration setting for the 
availability group. For more information on the new AlwaysOn technologies see Chapter 25, 
“AlwaysOn Availability Groups.”
In log shipping, the transaction log is backed up by the log-shipping process. No other 
transaction-log backup should be permitted, as that will break the log chain and prevent 
any additional transaction log restores on the standby server. If that occurred, you would 
need to reconﬁ gure log shipping.
In data mirroring, if the mirror server is down, the principal server transaction log queues 
all modiﬁ cations to be sent to the mirror in the transaction log. The transaction log cannot 
be truncated past the point where it has not sent data modiﬁ cations to the mirror server.
A failover cluster is a single database, so there are no special considerations. However, if 
the failover cluster is integrated with log shipping or data mirroring, the transaction-log 
limitations already mentioned apply.
Any use of replication requires you to make a detailed backup recovery plan that includes 
the synchronization of the source database, the distribution database, and the subscribers. 
Replication can introduce a new level of complexity to the backup/recovery plan. Although 
you can recover a replicated database from a backup, additional criteria must be met to 
make this successful. You should consider if using backups is a reasonable solution for 
recovering the subscriber databases. In the case of small databases, it may be easier and 
more reliable to simply regenerate and apply a new snapshot from the publisher.
➤
➤
➤
➤
➤
➤
➤
➤


586  ❘  CHAPTER 17  BACKUP AND RECOVERY
Tape: Requires that a local tape drive be present on the database server 
File: Requires a valid disk destination
You do not need to use backup devices when backing up to disk because the location is hard-coded. 
Instead, create unique backup ﬁ lenames that include the database name, the backup type, and some 
date/time information to make the name unique. This is much more ﬂ exible than using a backup 
device. To perform a database backup from SQL Server Management Studio, follow these steps: 
 1. 
Select the database you want to back up, right-click, and choose Tasks Á Backup. The Back 
Up Database dialog appears, as shown in Figure 17-23.
➤
➤
FIGURE 17-23
 2. 
In the Source area of this dialog, conﬁ gure the following: 
Database: Choose the database to back up.
Recovery model: This value is grayed out because it cannot be changed. This is in 
full recovery model. If it were simple recovery model, the transaction log could not 
➤
➤


588  ❘  CHAPTER 17  BACKUP AND RECOVERY
 6. 
In the Overwrite Media section, you can choose to back up to the existing media set, in 
which case you have to conﬁ gure these options: 
Append to the existing backup set: Preserves the existing backups by appending to 
that media set. This is the default.
Overwrite all existing backup sets: Erases all the existing backups and replaces 
them with the current backup. This overwrites all existing backup sets unless the 
Check Media Set Name and Backup Set Expiration box is checked.
Alternatively, you can choose to back up to a new media set and erase all existing backup 
sets, which erases all backups in the media and begins a media set, according to your 
speciﬁ cations.
 7. 
The Reliability section of this dialog has three check boxes that are all good recommended 
practices because a backup is of no value if it is not recoverable. Check these boxes: 
Verify Backup When Finished: After the backup ﬁ nishes, SQL Server conﬁ rms that 
all volumes are readable.
Perform Checksum Before Writing to Media: SQL Server does a checksum prior to 
writing to media, which can be used during recovery to verify that the backup was 
not tampered with. There is a performance penalty with this operation.
➤
➤
➤
➤
FIGURE 17-24

Developing and Executing a Backup Plan ❘ 589
Continue on Error: Backup should continue to run after encountering an error such 
as a page checksum error or torn page.
 8. 
The Transaction Log section of this dialog contains options that only apply during 
transaction-log backups. If you are performing a transaction log backup, select whichever of 
these is appropriate to the log backup you are trying to accomplish: 
Truncate the Transaction Log: During normal transaction-log backups, it is 
common practice to manage the size of the transaction log and to truncate it after it 
has been backed up to a backup media.
Back Up the Tail of the Log and Leave the Database in the Restoring State: This 
option is useful when the data ﬁ les of the database are not accessible. (For example, 
the physical drives have failed but the transaction log in separate physical drives 
is still accessible.) As a result, during database recovery, apply this as the last 
transaction-log backup to recover right to the point of failure.
 9. 
The Tape Drive section of the dialog contains check boxes to let you specify how to handle 
the tape. The two options include:
Unload the tape after backup
Rewind the tape before unloading
 10. 
In the Compression section of this dialog, specify one of three compression options for the 
backup: 
Use the Default Server Setting
Compress Backup
Do Not Compress Backup
 11. 
Click OK and the backup process executes.
Database Maintenance Plans
Another approach to executing the backup plan is to develop database maintenance plans for each 
database, schedule them, and have SQL Server e-mail you a backup history report. 
The purpose of the database maintenance plan is ease of use and reuse. A database maintenance plan 
is beneﬁ cial because it includes many of the normal maintenance actions you must do for a database, 
but grouped all together, executed on a schedule, with history and reporting. You can create a plan 
manually or use the wizard, which walks you through a series of dialogs.
To create maintenance plans for one or more databases from SQL Server Management Studio, 
choose the folder Management Á Maintenance Plans, and then right-click and choose New 
Maintenance Plan. After naming the maintenance plan, you go to the maintenance plan design 
screen and perform the following steps: 
Note that you may need to display the Maintenance Plan Tasks toolbox, as this does not show up 
by default anymore. To do this press CTRL+ALT+X, or use the View Á Toolbox menu option. Now 
perform the following steps:
➤
➤
➤
➤
➤
➤
➤
➤

590  ❘  CHAPTER 17  BACKUP AND RECOVERY
 1. 
Choose the Back Up Database Task, and 
drag it to the Designer.
 2. 
Right-click the Back Up Database 
Task, and choose Edit to open the 
Backup Database Task dialog, as shown 
in Figure 17-25.
 3. 
In the Connection ﬁ eld, choose Local 
Server Connection, or if this maintenance 
plan is to back up databases on another 
server, choose New Connection and 
provide the connection information.
 4. 
In the Database(s) ﬁ eld, choose one or 
more databases. You can choose more 
than one database if they have identical 
backup requirements.
 5. 
In the Backup Component ﬁ eld, choose 
either Database or Files and Filegroups. If 
you choose Files and Filegroups, you need to 
specify which ﬁ les or ﬁ legroups to back up.
 6. 
You may optionally choose an expiration date for the backup set. This prevents accidental 
overwrites.
 7. 
In the Back Up To ﬁ eld, choose either Disk or Tape.
 8. 
You can choose a list of hard-coded ﬁ les to backup your databases to or have the 
maintenance plan create an automatically named backup ﬁ le for each database. 
 
a. 
If you choose Back Up Databases Across one or More Files:
1. 
Click the Add button to conﬁ gure the backup location. For disk, provide the 
full path to the ﬁ lename or the disk backup device. For tape, provide the tape 
location or the tape backup device. You can use more than one ﬁ le or backup 
device. If more than one is chosen, all the databases will be backed up across 
them, up to the 64 backup devices that SQL Server supports.
2. 
On the If Backup Files Exist ﬁ eld, select whether to append to the existing 
backup ﬁ le or to overwrite; the default is Append.
 
b. 
If you choose Create a Backup File for Every Database:
1. 
Choose the Create a Backup File for Every Database option.
2. 
Select the root directory for the backups.
3. 
Choose a backup ﬁ le extension.
 9. 
Click the Verify Backup Integrity check box as a recommended practice.
 10. 
For transaction log backups, you can optionally choose to back up the tail of the log.
FIGURE 17-25




594  ❘  CHAPTER 17  BACKUP AND RECOVERY
machine account) change. These certiﬁ cates must be maintained or the database will not be 
restorable or the data will not be accessible.
Set up a logistical procedure to promptly move a copy of each backup to the offsite location 
to prevent it from being destroyed in a disaster.
BACKUP AND RESTORE PERFORMANCE
SQL Server supports 64 backup devices and uses multiple backup devices in parallel to back up 
and restore for faster throughput. The backup devices should be on a different controller from the 
database for better throughput. For disk devices, consider the RAID level used for fault tolerance 
and performance. Using RAID 5 on drives used to store backups is a bad idea because the additional 
overhead of calculating parity can reduce IO throughput, and therefore slow down backups. 
RAID 10 is the preferred choice for write performance, especially if your RAID controller has the 
intelligence to split writes across both sides of the mirror. This can dramatically increase write 
throughput. Work with your storage vendor to get recommendations for your storage hardware. 
In many cases, disk based backups are written to large slow disks because of the cost savings from 
using cheaper, large capacity disks. This immediately has a performance impact on the ability of backup 
to write to these disks. This is just something that backups have to live with, as very few companies are 
willing to spend large amounts of money on a high performance disk subsystem to store backups.
A combination of full, differential, and transaction-log backups can improve performance by 
reducing the amount of data that needs to be read from the database and written to the backup 
device. If you take a full backup of a 5 TB database every day, that’s a lot of data to be backing 
up so often. If only a small percentage of the DB changes every day, then taking a full backup 
once a week (on the weekend, or other slack period perhaps), with daily differential backups can 
dramatically reduce the amount of data being read and written during the week.
Network bandwidth can become an issue when backing up to a network device or other server. 
Ideally backups should use a dedicated network with enough bandwidth to satisfy all of the backup, 
and restore throughput needs.
PERFORMING RECOVERY
Recovery is the action of restoring a database, and bringing it back to a consistent state. This 
section explains the various methods of recovery, through both Management Studio and T-SQL. 
You also learn how to restore the system databases.
Restore Process
It is a DBA’s task to ensure that backups are consistently taken and validated to restore. Each 
backup sequence is labeled and stored to enable quick identiﬁ cation to restore a database. These 
restore procedures include the following: 
Full Database Restore
Transaction-Log Restore
➤
➤
➤




598  ❘  CHAPTER 17  BACKUP AND RECOVERY
restore. This means that after a snapshot restore, you must take a full backup (or ﬁ le backup) before 
attempting to take any log backups
To create a database snapshot, use the following syntax: 
 CREATE DATABASE NorthAmerica_dbss9AM ON ( NAME = NorthAmerica_Data
, FILENAME =ÐC:\Program Files\Microsoft SQL Server
\MSSQL11.MSSQLSERVER\MSSQL\Data\NorthAmerica_data
To restore from a snapshot, use this syntax: 
USE MASTER
RESTORE DATABASE NorthAmerica 
FROM DATABASE_SNAPSHOT=ÐNorthAmerica_dbss9AMÐ
Beginning with SQL 2005, there has been a much-improved page-level reporting structure available. 
Page errors are now logged in the suspect_pages table in MSDB. Along with the ability to log page 
errors, the SQL team has provided the DBA with the ability to restore suspect pages. SQL Server 
can restore pages while the database remains online and available, even the ﬁ legroup and ﬁ le that 
contains the suspect pages. Other versions of SQL Server allow only ofﬂ ine restore of suspect pages.
Only data pages can be restored, which excludes allocation pages, full-text indexes, the transaction log, 
and the database and ﬁ le boot pages. Page restore also does not work with the simple recovery model.
The restoration process for page restore is just like that of a ﬁ le restore, except you provide the page 
numbers you want to restore. The syntax follows: 
RESTORE DATABASE <dbname> 
PAGE = ‘<file:page>,…’
FROM <backup file or device>
WITH NORECOVERY
You then restore any differential backup and then log backups with NORECOVERY. Then you create a 
normal log backup and restore it: 
 BACKUP LOG <dbname> TO <filename>
 RESTORE LOG <dbname> FROM <filename> WITH RECOVERY
You can identify suspect pages from the suspect_pages table in msdb, the SQL error log, SQL 
event traces, some DBCC commands, and Windows Management Instrumentation (WMI) provider 
for server events. Page restores can be a great thing–the ability to restore pages quickly without 
having to restore the whole database. This is especially useful when you have hardware failures like 
controller or disk drive intermittent failures.
History Tables Restore
The msdb database maintains restore metadata tables, which are restored as part of msdb database 
restore. The following list details the meta data tables, and what each contains: 
dbo.restorefile: Contains one row for each restored ﬁ le, including ﬁ les restored 
indirectly by ﬁ legroup name
➤

Performing Recovery ❘ 599
dbo.restorefilegroup: Contains one row for each restored ﬁ legroup
dbo.restorehistory: Contains one row for each restore operation
SQL Server Management Studio Restore
To restore a database from SQL Server Management Studio, perform the following steps: 
 1. 
Choose the Database folder, right-click the database of your choice, and choose Tasks Á 
Restore Á Database. The Restore Database dialog, as shown in Figure 17-26, exposes the 
restore capability.
➤
➤
FIGURE 17-26
 2. 
In the Restore Database dialog, in the Destination for Restore area, select from the 
following options: 
To Database: Choose the name of an existing database or type the database name.
To a Point in Time: For a transaction log restore, choosing a stop time for the 
restoration is equivalent to STOPAT in the Restore Log command. A point in time is 
commonly used when a database is being restored because of a user or application 
data modiﬁ cation error and you have identiﬁ ed the time when the error occurred. 
Therefore, you want to stop the restoration before the error. This option is not 
possible for the Simple recovery model because the transaction log is truncated.
➤
➤

600  ❘  CHAPTER 17  BACKUP AND RECOVERY
 3. 
In the Source for Restore area of this dialog, choose between the following options: 
From Database: The name of the database to restore; this information is retrieved 
from the backup history tables in msdb.
From Device: Choose either the backup device or the backup ﬁ le name to restore 
from. This may be used when restoring a database onto another SQL Server 2012 
instance and there is no restore data in the backup tables in msdb.
 4. 
Next, select the backup sets to restore from the list at the bottom of the dialog. When 
selecting the restore source, it populates this ﬁ eld with the backup sets available for the 
database. It also provides an option to choose which backup sets to restore.
 5. 
From the Restore Database dialog, select the Options page, and you'll be taken to the dialog 
shown in Figure 17-27.
➤
➤
FIGURE 17-27
 6. 
Choose from the following options in the Restore Options section of this dialog: 
Overwrite the Existing Database: Use this check box when the database you want 
to restore already exists in the SQL Server instance. Checking this box overwrites 
the existing database; this is equivalent to the REPLACE option in the Restore 
Database command.
➤

Performing Recovery ❘ 601
Preserve the Replication Settings: Use this check box when you restore a publisher 
database; it is equivalent to the PRESERVE_REPLICATION option in the Restore 
Database command.
Prompt Before Restoring Each Backup: Use this check box when you swap tapes 
that contain backup sets.
Restrict Access to the Restored Database: Use this check box when you need to 
perform additional database operations or validation before allowing users to 
access the database. This option limits database access to members of db_owner, 
dbcreator, or sysadmin and is equivalent to the RESTRICTED_USER option in the 
Restore Database command.
Restore the Database Files As: Here you can choose to restore the database in 
another directory and with a different ﬁ lename. For example, if a new database 
copy has been created in the same directory, you need to change the ﬁ lename of the 
restored database. This is equivalent to the MOVE option in the Restore Database 
command. If the ﬁ lenames are not changed, SQL Server generates the following error: 
   Restore failed for Server ‘Server1’. 
 (Microsoft.SqlServer.SmoExtended)System.Data.SqlClient.SqlError:
   Exclusive access could not be obtained because the database is in
   use.(Microsoft.SqlServer.Smo)
 7. 
In the Recovery State section of this dialog, select one of these options: 
Restore with RECOVERY: The default setting recovers the database, which means 
that no more backup images can be restored and the database becomes available 
to users. If additional backup images need to be restored, such as a full database 
restore followed by several transaction logs, the recovery should be performed after 
the last step because after recovery, no additional backup images can be restored 
without starting the restore over. This is equivalent to the WITH RECOVERY option in 
the Restore Database command.
Restore with NORECOVERY: After a backup image is restored, the database is 
not recovered to enable additional backup images to be applied, such as a database 
differential or a transaction log backup. Moreover, the database is not user accessible 
while in NORECOVERY. This state is used on the mirror server in data mirroring and is 
one of the states available on the secondary server in log shipping. This is equivalent 
to the WITH NORECOVERY option in the Restore Database command.
Restore with STANDBY: After a backup image has been restored, the database 
is left in a state in which it allows additional backup images to be restored while 
allowing read-only user access. In this state for the database to maintain data 
consistency, the undo and uncommitted transactions are saved in the standby ﬁ le 
to allow preceding backup images to commit them. Perhaps you plan to apply 
additional backup images and want to validate the data before each restore. 
Oftentimes, this option is used on the secondary server in log shipping to allow 
users access for reporting. This is equivalent to the WITH STANDBY option in the 
Restore Database command.
➤
➤
➤
➤
➤
➤
➤



604  ❘  CHAPTER 17  BACKUP AND RECOVERY
performed to the model and msdb need to be redeployed. The syntax to rebuild the master 
database is as follows: 
 start /wait setup.exe /qn INSTANCENAME=<InstanceName> REINSTALL=SQL_Engine
 10. 
Then attach the user databases.
If only the model or msdb databases are damaged, you can restore them from a current backup. 
If a backup is not available, then you have to execute Setup.exe, which re-creates all the system 
databases. Typically, model and msdb reside in the same disk system with the master, and if a 
disk-array failure occurred, most likely all three would be lost. To mitigate disk failure, consider 
using a RAID array where master, model, and msdb reside. tempdb does not need to be restored 
because it is automatically re-created by SQL Server at startup. tempdb is a critical database and is a 
single point of failure for the SQL Server instance; as such, it should be deployed on a fault-tolerant 
disk array.
ARCHIVING DATA
Archiving a large amount of data from large tables can be challenging. For example, selecting 
millions of rows from a billon-row table, copying them, and then deleting them is a long-running 
delete process that may escalate to a table lock and reduce concurrency, which is not acceptable, 
unless no one will be using the table. A commonly used procedure is to periodically delete a small 
number of rows to improve table concurrency because the smaller number of rows may take page 
locks and use an index for faster access, completing faster.
An efﬁ cient procedure to archive large amounts of data is to use a sliding time window table 
partitioning scheme. There are two approaches to this solution: using SQL Server table partitioning 
or using a partitioned view.
SQL Server Table Partitioning
SQL Server supports table partitioning, whereby a table can be carved into as many as 15,000 
pieces, with each residing on its own ﬁ legroup. Each ﬁ legroup can be independently backed up. 
Different ﬁ legroups can also be located on different storage; for example, current data can be held 
on fast disks, possibly even on solid state disks. Older/archive data can then be moved to larger, 
slower disks and more easily deleted when the data is no longer needed. The deletion of a partition 
can be achieved extremely quickly, and with virtually no impact to queries against the current data. 
Look at a partitioning example in which each partition contains one month’s data. With table 
partitioning, a new empty partition is created when the next monthly data becomes available. Then 
the oldest partition can be switched out into a table and moved to an archive table monthly. The 
basic steps to create a table partition are as follows: 
 1. 
Create a partition function that describes how you want the data partitioned.
 2. 
Create a partition schema that maps the pieces to the ﬁ legroups.
 3. 
Create one or more tables using the partition scheme.

Archiving Data ❘ 605
Following is an example of creating a partition table using a monthly sliding window: 
--Create partition function
 CREATE PARTITION FUNCTION [OrderDateRangePFN](datetime)
 AS RANGE RIGHT
 FOR VALUES (N‘2009-01-01 00:00:00’
, N‘2009-02-01 00:00:00’
, N‘2009-03-01 00:00:00’
,N‘2009-04-01 00:00:00’);
 --Create partition scheme
 CREATE PARTITION SCHEME [OrderDatePScheme]
 AS PARTITION [OrderDateRangePFN]
 TO ([filegroup1], [filegroup2], [filegroup3], [filegroup4], [filegroup5]);
 --Create partitioned table SalesOrderHeader
 CREATE TABLE [dbo].[SalesOrderHeader](
   [SalesOrderID] [int] NULL,
   [RevisionNumber] [tinyint] NOT NULL,
   [OrderDate] [datetime] NOT NULL,
   [DueDate] [datetime] NOT NULL,
   [ShipDate] [datetime] NULL,
   [Status] [tinyint] NOT NULL
 ) ON [OrderDatePScheme]([OrderDate]);
This example places each partition on a different ﬁ legroup. Splitting and merging partitions requires 
data movement. You can achieve high-speed splits and merges without table locking or reducing 
concurrency if you place the partitions on the same ﬁ legroup. When partitions are on the same 
ﬁ legroup, switching out a partition or merging is only a schema change and occurs quickly. There 
are several other smaller restrictions for high-speed partitioning, but the ﬁ legroup restriction is 
more important.
Partitioned View
This technique has been available since earlier versions of SQL Server. It uses a partition view to 
group independent, identical tables together (for example, a new table for each month). Following is 
the procedure: 
 1. 
Create individual, identical tables with a check constraint to limit the data that can reside in 
each.
 2. 
Create a view to unite all these tables together.
 3. 
Load the data through the partition view. SQL Server evaluates the table constraint to insert 
the data in the correct table.
 4. 
Before the next date period, create a new table with the date period constraint and include it 
as part of the view deﬁ nition. Then load the current data through the view.
 5. 
To archive, remove the oldest table from the view deﬁ nition and then archive it. Each table 
can be placed in its own ﬁ legroup and backed up individually.
This technique does not have the 15,000-partition limitation, but it requires more management 
because each table is independent and managed.

606  ❘  CHAPTER 17  BACKUP AND RECOVERY
SUMMARY
Backup and recovery are the last defenses to recover an organization data asset when everything 
else fails. The backup and restore functionality must guarantee that many years of customer 
information, buying patterns, ﬁ nancial data, and inventory can be recovered. SQL Server 2012 is 
a scalable and highly available RDBMS solution supporting some of the largest databases with the 
highest number of concurrent users running mission-critical applications. These key backup and 
restore functionalities ensure that it can support a larger database with less management. If you 
followed along throughout this chapter, you should now both understand the details needed to 
create a robust plan for backing up your company’s data and possess one or more documents that 
constitute your recovery plan. 


608  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
LOG SHIPPING DEPLOYMENT SCENARIOS
Log shipping takes advantage of the transaction-log backup and restores functionalities of SQL 
Server. The two log-shipping SQL Server partners can be located next to each other for high 
availability or across a distance for disaster recovery. The only distance restriction for the two SQL 
Servers is that they share connectivity that enables the secondary SQL Server to copy the transaction 
log and restore it. There are three different scenarios in which log shipping can be deployed: 
Warm standby server: Maintains a backup database copy in the same physical location to 
protect from a primary server failure
Disaster recovery solution: Two servers are geographically separated in case the local area 
where the primary server resides suffers from a disaster
Reporting solution: The secondary server is used to satisfy the reporting needs
Log Shipping to Create a Warm Standby Server
A warm standby server involves creating a full backup and periodic transaction log backups at the 
primary server, and then applying those backups, in sequence, to the standby server. The standby 
server is left in a read-only state between restores. When the standby server needs to be made available 
for use, any outstanding transaction log backups, including the backup of the active transaction log 
from the primary server, are applied to the standby server and the database is recovered. A common 
log-shipping scenario is creating a warm standby server whereby the log-shipping secondary server 
is located close to the primary server. If the primary server goes down for planned or unplanned 
downtime, the secondary server takes over and maintains business continuity. Then, the DBA may 
choose to failback to the primary server when the primary server becomes available. 
It is simple to conﬁ gure a warm standby server with log shipping because it uses the dependable 
transaction log backup, operating system copy ﬁ le, and transaction log restore. In most warm 
standby scenarios, you should conﬁ gure the log-shipping jobs to execute at a shorter interval to 
maintain the secondary server closely in sync with the primary server, to reduce the amount of time 
to switch roles, and to reduce data loss. Additionally, to further limit data loss, if the active portion 
of the primary server’s transaction log is available, the secondary server would be restored to the 
point in time of the failed primary server.
However, in some situations, the active portion of the transaction log may not be available when 
the storage where the transaction log resided on is not accessible, or some transaction log ﬁ les that 
were in transit may not have made it to the secondary server, causing some data loss. In a typical 
role-switch scenario, you would recover all in-transit transaction logs and the active portion of 
the transaction log before recovering the secondary server. Users would also need to be redirected 
because log shipping, unlike Windows failover clustering, has no automatic user redirect.
Sometimes, when performing a failback, log shipping is used in place of Windows failover clustering 
because it is a less expensive solution; for example, clustering requires a shared disk system that 
an organization may not own. Log shipping does not have such hardware requirements, so an 
organization may already own hardware that is not failover — cluster-compatible that can be used 
for log shipping. Moreover, in log shipping, the primary and secondary databases exist on separate 
servers. This is a shared-nothing environment. Windows failover clustering uses one shared disk 
system with a single copy of your database on the shared disk, which could become corrupted.
➤
➤
➤




612  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
backup folder, where it was placed by the primary server, and to restore the transaction log backup. 
The secondary server is conﬁ gured with two SQL Agent jobs: one to copy the transaction-log ﬁ le 
from the shared backup folder and the other to restore the transaction log. This server should have 
similar performance speciﬁ cations to those of the primary server to maintain a consistent user 
experience during failover. Log shipping can also support multiple secondary servers at a time. For 
instance, one can be setup for warm-standby and another for reporting and another may even have 
a transaction-log with a time delay. 
The secondary database can be conﬁ gured using either the NORECOVERY or STANDBY recovery option:
STANDBY: This provides users read-only access to the log shipped database between 
transaction log restores. This means you can ofﬂ oad read-only access to a secondary server. 
However, for a transaction log restore to succeed, no read-only access is allowed.
NORECOVERY: The database will not be available for users for read-only access.
Monitor Server
Having a monitor server as part of log shipping is optional, but recommended. The monitor server 
should be a different physical server to prevent it from becoming a single point of failure from the 
primary or secondary server. Any SQL Server version including SQL Server Express can be conﬁ gured 
as a monitor server. When the monitor server participates in log shipping, it manages the jobs that 
produce monitoring information, such as the last time the transaction log was backed up on the primary 
server, the last transaction log that was restored on the secondary server, and the time deltas between 
the processes. The monitor server can also send alerts to page or e-mail the operator when log-shipping 
thresholds are crossed. A single monitor server can monitor multiple log-shipping environments. 
Having a separate physical monitor server is recommended. Deploying a monitor server on the 
primary or secondary server has the risk that if that server were to fail, you would lose monitoring 
server capabilities as well. Without the monitor server, log shipping will continue to operate and 
monitoring can be performed using DMVs. 
LOG SHIPPING PROCESS
SQL Agent is used on the participating servers to execute the processes that implement log shipping. 
There are three main processes: 
Back up the transaction log on the primary server: A SQL Agent job on the primary server 
backs up the transaction log at a user-conﬁ gurable time interval to a ﬁ le in a backup folder. By 
default, the ﬁ lename is time-stamped to provide uniqueness — for example, databasename_
yyyymmddhhmmss.trn. By default, the backup job is named LSBackup_databasename, and 
it executes an operating system command to back up the transaction log: 
 “C:\Program Files\Microsoft SQL Server\110\Tools\Binn\sqllogship.exe”
 -Backup 0E5D9AA6-D054-45C9-9C6B-33301DD934E2 -server SQLServer1
Copy the transaction log to the secondary server: A SQL Agent job on the secondary server 
uses UNC or a shared drive to access the backup folder on the primary server to copy the 
transaction-log ﬁ le to a local folder on the secondary server. By default, the copy job is 
➤
➤
➤
➤
 
 
 
 


614  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
Storage
Unlike a Windows failover cluster that requires a shared-disk infrastructure, log shipping has no such 
requirements. On the contrary, to mitigate the risk of storage failure becoming a single point of failure, 
the primary and secondary servers should not share the same disk system. In a disaster recovery scenario 
conﬁ guration, the primary and secondary servers would be located at a distance from each other and 
would be unlikely to share the same disk system. Plan the disk space requirements for the backup share 
that holds the transaction-log backups to avoid running out of disk space. Moreover, when identifying 
the performance speciﬁ cation for the disk systems, consider the log-shipping I/O activities.
When deploying log shipping with Management Studio, the SQL Server service and the SQL Agent 
account (or its proxy running the backup job) must have read-and-write permission to the backup 
folder. If possible, this folder should reside on a fault-tolerant disk system so that if a drive is lost, all 
the transaction log ﬁ les are not lost. 
Software
Three SQL Server editions are supported for log shipping: 
SQL Server 2012 Enterprise Edition
SQL Server 2012 Standard Edition
SQL Server 2012 Business Intelligence Edition
Monitor server can be any edition including SQL Express
The log-shipping servers are required to have identical case-sensitivity settings, and the log-shipping 
databases must use either the full or bulk-logged recovery model.
DEPLOYING LOG SHIPPING
Before you can begin the log-shipping deployment process, you need to do some initial 
conﬁ guration. Then you have a choice regarding how you want to deploy: using the SQL 
Server 2012 Management Studio or using T-SQL scripts. Typically, a DBA uses SQL Server 
2012 Management Studio to conﬁ gure log shipping and then generates SQL scripts for future 
redeployment. Both procedures are covered here.
Initial Conﬁ guration
Prior to deploying Log Shipping, some speciﬁ c directories are needed for log shipping to copy the 
transaction log ﬁ les and SQL conﬁ guration to prepare the Log Shipping process to execute. To 
conﬁ gure your shared directories for log shipping, perform the following steps: 
 1. 
First create a backup folder that the primary server can access; share it, and ensure that 
it is accessible by the secondary server. For example, you could use the folder 
c:\primaryBackupLog, which is also shared as a UNC path: \\primaryserver\
primaryBackupLog. Use the UNC when you are accessing to the share from a remote 
server; when the share is local, you can access either by UNC or by the directory letter. The 
primary server’s SQL Agent account must have read-and-write permission to the folder, 
➤
➤
➤
➤


616  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
To use T-SQL, open a SQL query window and use the ALTER DATABASE command to change the 
recovery model. For example, to change the AdventureWorks database to Full, use this T-SQL: 
 USE master;
 GO
 ALTER DATABASE AdventureWorks 
 SET RECOVERY FULL;
 GO
Deploying with Management Studio
To deploy log shipping with SQL Server Management Studio, perform the following steps: 
 1. 
Start by opening the database to be conﬁ gured and select the database properties; then select 
Transaction Log Shipping. Click the check box that reads Enable This as a Primary Database 
in a Log Shipping Conﬁ guration, as shown in Figure 18-3.
FIGURE 18-3
 2. 
Next, click the Backup Settings button. The Transaction Log Backup Settings dialog 
appears, as shown in Figure 18-4.

Deploying Log Shipping ❘ 617
 3. 
Here, you need to provide the network path to the backup folder and the local path if the 
folder is local to the primary server. If the folder is local to the primary server, log shipping 
uses the local path. Remember that the SQL Server service and the SQL Agent account or its 
proxy running the backup job must have read read-and and-write permission to this folder. 
If possible, this folder should reside on a fault-tolerant disk system so that if a drive is lost, 
all the transaction log ﬁ les are not lost.
Typically you can delete transaction-log backup ﬁ les that have been applied and are older 
than the value in the Delete Files Older Than ﬁ eld to control the folder size containing older 
transaction backup log ﬁ les. However, for an additional level of protection, if the business 
requires point-in-time recovery, leave the ﬁ les there until the OS backup program backs them 
up to another storage device, provided that a full database backup is also available to apply 
these transaction logs. The default setting is 72 hours.
 4. 
In the Alert if No Backup Occurs Within ﬁ eld, choose a value based on the business 
requirements. The amount of data your organization can stand to lose determines the 
transaction backup interval setting or how critical the data is. The alert time also depends 
on the transaction backup interval setting. For example, if the business requires a highly 
available secondary server, where the transaction log is backed up every couple of minutes, 
FIGURE 18-4

618  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
this setting should be conﬁ gured to send an alert if the job fails to run within that interval. 
The default setting is 1 hour.
 5. 
Click the Schedule button to display the Job Schedule Properties page, and set up a schedule 
for the transaction-log backup job. The important setting is the “Occurs Every Field,” 
which defaults to 15 minutes. This setting can be conﬁ gured to once every minute for higher 
availability. However, the time interval should be appropriately set to allow the previous 
transaction-log backup job to complete. This value is the degree to which the primary 
and secondary servers are in sync. When you’re done here, click OK on the Job Schedule 
Properties page; then click OK in the Transaction Log Backup Settings dialog to return to 
the Database Properties page for Transaction Log Shipping.
 6. 
Click Add to display the Secondary Database Settings dialog to set up a secondary (standby) 
server, as shown in Figure 18-5.
FIGURE 18-5
Then click Connect and choose the Secondary Server instance. Then choose an existing 
database or a new database name. On the Initialize Secondary Database tab as shown in 
Figure 18-6, there are three options to choose (described in the following list).

Deploying Log Shipping ❘ 619
These options answer the question, “Do you want the Management Studio to restore a 
backup into the secondary database? 
Yes, generate a full backup of the primary database and restore it into the second-
ary database (and create the secondary database if it does not exist): The Restore 
Options setting enables you to set the database folder locations for the data and the 
log ﬁ les. If this is not set, the default database locations are used.
Yes, restore an existing backup of the primary database into the secondary database 
(and create the secondary database if it does not exist): The Restore Options setting 
enables you to set database folder locations for the data and the log ﬁ les. You also 
specify the network location of the backup ﬁ le you want to restore.
No, the secondary database is initialized: The option means that the database has 
already been created. The transaction logs preceding the database restore must be 
available to enable log shipping to work. For example, the log sequence number 
(LSN) of the primary server and the secondary server databases must match. Also, 
the secondary database must be in either NORECOVERY or STANDBY mode to allow 
additional transaction-log ﬁ les to be applied.
 7. 
Next, restore the secondary database from the primary backup; the new database ﬁ le 
path and name created will be the same as the primary database. You cannot alter the 
ﬁ lename, but you can change the path and specify path names in the dialog box by clicking 
the Restore Options button. For the examples in this section, the primary and secondary 
➤
➤
➤
FIGURE 18-6

620  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
databases are on the same Windows server but different SQL Server 2012 instances. The 
secondary database AdventureWorks was created with the same ﬁ lenames as those for the 
primary database AdventureWorks but placed in a different directory. 
 8. 
On the Copy Files tab as shown in Figure 18-7, in the “Destination folder for the copied 
ﬁ les directory” textbox, type the destination folder (e.g., c:\secondaryBackupDest). 
The Delete Copied Files After option controls the folder size after the transaction log is 
restored on the secondary server’s database. Any ﬁ les older than the speciﬁ ed time are 
deleted. The default is 72 hours.
FIGURE 18-7
 9. 
Click the Schedule button to set up a schedule for the transaction-log-ﬁ le copy job. The 
important setting is the Occurs Every ﬁ eld, which defaults to 15 minutes. You can reduce 
this time to have the secondary closer in data sync with the primary. Click OK when you 
ﬁ nish to return to the Secondary Database Settings page.

Deploying Log Shipping ❘ 621
You have two options for the On Database State When Restoring Backups ﬁ eld: 
No Recovery mode: The secondary database is left in NORECOVERY mode, which 
enables the server to restore additional transactional-log backups but doesn’t enable 
user access.
Standby mode: The secondary database enables read-only operations to be per-
formed in the database, such as reporting. However, as mentioned previously, the 
restore process needs exclusive access to the secondary database; if users are access-
ing the database, the restore process cannot complete.
For the Delay Restoring Backups at Least setting, the default is 0 minutes. Typically, you 
would change this setting if your organization wants to keep the secondary database around 
in case of a primary database’s data corruption or unintended data deletions. This delay 
may prevent the secondary database from restoring the corrupted transaction-log ﬁ le.
The Alert if No Restore Occurs Within setting defaults to 45 minutes and should be set to 
the tolerance level of the business. 
➤
➤
FIGURE 18-8
 10. 
Click the Restore Transaction Log tab as shown in Figure 18-8.



624  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
Deploying with T-SQL Commands
Another deployment option is to use the actual T-SQL commands to conﬁ gure log shipping. Even 
if you choose to use the SQL Server Management Studio to conﬁ gure log shipping, saving the 
generated command script enables you to quickly reconﬁ gure the server to expedite a disaster 
recovery scenario while avoiding any user-induced errors. The following T-SQL commands are 
equivalent to the steps you took in SQL Server Management Studio.
On the primary server, execute the following stored procedures in the MSDB: 
master.dbo.sp_add_log_shipping_primary_database: Conﬁ gures the primary database 
for a log-shipping conﬁ guration; this conﬁ gures the log-shipping backup job.
msdb.dbo.sp_add_schedule: Creates a schedule for the log-shipping conﬁ guration.
msdb.dbo.sp_attach_schedule: Links the log-shipping job to the schedule.
msdb.dbo.sp_update_job: Enables the transaction-log backup job.
master.dbo.sp_add_log_shipping_alert_job: Creates the alert job and adds the job 
ID in the log_shipping_monitor_alert table. This stored procedure enables the alert 
notiﬁ cations.
On the secondary server, execute the following stored procedures: 
master.dbo.sp_add_log_shipping_secondary_primary: Sets up the primary 
information, adds local and remote monitor links, and creates copy and restore jobs on the 
secondary server for the speciﬁ ed primary database
msdb.dbo.sp_add_schedule: Sets the schedule for the copy job
msdb.dbo.sp_attach_schedule: Links the copy job to the schedule
msdb.dbo.sp_add_schedule: Sets the schedule for the restore job
msdb.dbo.sp_attach_schedule: Links the restore job to the schedule
master.dbo.sp_add_log_shipping_secondary_database: Sets up secondary databases 
for log shipping
msdb.dbo.sp_update_job: Enables the copy job
msdb.dbo.sp_update_job: Enables the transaction-log restore job
Back on the primary server, execute this stored procedure in the MSDB: 
master.dbo.sp_add_log_shipping_primary_secondary: Adds an entry for a secondary 
database on the primary server
MONITORING AND TROUBLESHOOTING
Log shipping has monitoring capabilities to identify the progress of the backup, copy, and restore jobs. 
Additionally, monitoring helps to determine whether the backup, copy, or restore jobs are out of sync 
with the secondary server. A few indicators that something has gone wrong include a job that has not 
made any progress or a job that has failed, both of which will be discussed later in this section. 
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

Monitoring and Troubleshooting ❘ 625
There are two approaches to monitor the progress of the log-shipping operation: using the 
Transaction Log Shipping Status report, performed through Management Studio or executing the 
master.dbo.sp_help_log_shipping_monitor stored procedure. Either method can help you 
determine whether the secondary server is out of sync with the primary server, and the time delta 
between the two. Using Management Studio, you can visually see the operation; but using the stored 
procedure enables setup of a recurring batch SQL Agent job command to monitor the log shipping 
operation to send alerts. With both of these methods, you can also determine which jobs are not 
making any progress and the last transaction-log backup, copy, and restore ﬁ lename processed on 
the secondary server.
To inform you of any errors found, log shipping also performs alerts jobs that periodically check if a 
preset threshold has been exceeded by executing the sys.sp_check_log_shipping_monitor_alert 
stored procedure. If the threshold has been exceeded, the stored procedure raises an alert that is 
returned to the log shipping monitoring status. You can choose to modify the log-shipping alert jobs 
to capture the alert and notify you using SQL Agent. 
In log shipping, if a monitor server is deployed, these alerts reside on the monitor server that reports 
on the transaction-log backup, copy ﬁ le, and restore transaction log. If not, the primary server 
manages the alert job for the transaction-log backup, and the secondary server manages the alert 
job for the copy ﬁ le and restore transaction log. If the monitoring server is present, the primary and 
secondary servers will not deploy alert jobs.
The following is an example error that results if the transaction-log backup process has exceeded 
the preset threshold of 30 minutes: 
Executed as user: NT AUTHORITY\SYSTEM. The log shipping primary database
SQLServer1.AdventureWorks has backup threshold of 30 minutes and has not
performed a backup log operation for 60 minutes. Check agent log and log shipping
monitor information. [SQLSTATE 42000](Error 14420). This step failed.
The next example shows an error that results if the restore transaction-log process has exceeded the 
preset threshold of 30 minutes: 
Executed as user: NT AUTHORITY\SYSTEM. The log shipping secondary database
SQLServer2.AdventureWorks has restore threshold of 30 minutes and is out of
sync. No restore was performed for 60 minutes. Restored latency is 15 minutes. Check 
agent log and log shipping monitor information. [SQLSTATE 42000](Error 14421).
The step failed.
As an alternative, you can set up an alert for when errors 14420 or 14221 are raised; SQL Agent 
sends an alert to the operator.
Monitoring with Management Studio
The Transaction Log Shipping Status report displays monitoring information from Management 
Studio. This report executes the sp_help_log_shipping_monitor stored procedure. When 
executed on the primary server, it reports on the transaction-log backup details; when executed on 
the secondary server, it reports on the copy and transaction-log restore details. When the monitor 
server is conﬁ gured, the report executed from the monitor server produces a consolidated report of 

626  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
the transaction-log backup, copy, and transaction-log restore details in one report. To access the 
Transaction Log Shipping Status report: 
 1. 
Connect to the primary, secondary, or monitor server. The monitor server is the most useful 
option because it has the consolidated log-shipping detail data.
 2. 
If the Object Explorer is not visible, select View Á Object Explorer.
 3. 
Right-click the server node in the Object Explorer and select Reports Á Standard Reports Á 
Transaction Log Shipping Status.
Figure 18-11 shows an example of a Transaction Log Shipping Status report executed from the 
monitor server, showing the details for all log-shipping activities with alerts.
FIGURE 18-11
Monitoring with Stored Procedures
Executing the sp_help_log_shipping_monitor stored procedure in the master database from a 
SQL query window produces log-shipping status details, similar to the Transaction Log Shipping 
Status report. If you execute it from the primary server, it returns detailed information on the 
transaction-log backup job. If you execute it from the secondary server, it returns information on 
the copy and transaction-log restore jobs. If it is executed from the monitor server, it returns a 
consolidated detail result of the transaction-log backup, copy, and transaction-log restore, as the 
monitor server is visible to all log-shipping processes.
For additional log-shipping operational and troubleshooting detail information, the log-shipping 
tables can be queried using the log-shipping stored procedures found in the msdb database. For more 
information, see SQL Server 2012 Books Online.
Troubleshooting Approach
As mentioned previously, log shipping consists of three basic operations: backing up the transaction 
log, copying the ﬁ le, and restoring the transaction log. Troubleshooting this process is simply a 
matter of identifying which operation is not functioning. You can use both of the log-shipping 
monitoring capabilities to identify the problem. 
For example, say the restore transaction-log ﬁ le shows that no new ﬁ les have been restored in the 
last 60 minutes. You need to look at the log-shipping job history on the secondary server ﬁ rst 
under the SQL Agent and the Windows Event Viewer to determine the actual error message. If, 
for instance, the copy ﬁ le job is failing, it may be because the network is down. If the restore 


628  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
resources available during the failover. The following sections describe common log-shipping 
dependencies and their solutions.
Login and Database Users
When developing a process to synchronize SQL Server logins with the secondary server and 
database you should begin by setting it up as a recurring SQL job that runs at certain scheduled 
intervals. In a planned failover, you should run these SQL jobs before failover to update the 
secondary server with the most current access information. Following are the steps: 
 1. 
Develop an Integration Services (SSIS) package to Transfer logins. Open SQL Server 
Business Intelligence Development Studio, and start a new Integration Services project.
 2. 
In the Solution Explorer, name the SSIS project Transfer Logins, and rename the SSIS 
Package to Transfer Logins.
 3. 
Click the Toolbox, and drag the Transfer Logins Task into the package.
 4. 
Right-click the Transfer Logins Task, and choose Edit.
 5. 
Click Logins. You see the dialog 
shown in Figure 18-12.
 6. 
For SourceConnection, enter a new 
connection to the primary server.
 7. 
For DestinationConnection, enter 
a new connection for the secondary 
server.
 8. 
For LoginsToTransfer, choose 
AllLoginsFromSelectedDatabases.
 9. 
For DatabasesList, choose the log-
shipping database.
 10. 
In the Options section, in the 
IfObjectExists entry, choose 
what to do if the login exists, such 
as FailTask, Override, or Skip. 
If the secondary server is hosting 
other databases, you may encounter 
duplicate logins.
 11. 
Save the package, and choose Build Á Build SSIS Transfer Logins to compile the package.
 12. 
From Microsoft SQL Server 2012 Management Studio, connect to the Integration Services 
of the primary server.
 13. 
Under Stored Packages, choose MSDB; right-click, and choose Import Package.
FIGURE 18-12


630  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
 2. 
Provide the connection information and the jobs to transfer. 
 3. 
Then compile and import the package into SQL Server 2012 or as an SSIS ﬁ le system. 
 4. 
Finally, schedule and execute it as a SQL Agent job to periodically synchronize changes with 
the secondary server. The frequency depends on how frequent jobs are changed that then 
need to be synchronized. 
Other Database Dependencies
Make a list of all dependencies on which the log-shipping database relies. These can be distributed 
queries/linked servers, encrypted data, certiﬁ cates, user-deﬁ ned error messages, event notiﬁ cations 
and WMI events, extended stored procedures, server conﬁ guration, full-text engine, permissions 
on objects, replication settings, Service Broker applications, startup procedures, triggers, CLR 
procedures, and database mail. Develop a plan to synchronize these to the secondary server.
Switching Roles from the Primary to Secondary Servers
If the primary server were to fail, the secondary server needs to assume the role of the primary 
server. This is called role switching. There are two potential types of role switches: planned failover 
and unplanned failover. A planned failover is most likely when the DBA needs to perform some 
type of maintenance, usually scheduled at a time of low business activity or during a maintenance 
window — for example, switching roles to apply a service pack on the primary server. An 
unplanned failover is when the DBA switches roles for business continuity because the primary 
server becomes unavailable.
Planned Failover
For a planned failover, identify a time when the primary server has little or no activity. During 
a time like this, it is likely that the secondary server will not have restored all the transaction 
logs from the primary server, and transaction logs will be in the process of being copied across 
by the SQL Agent job that the restore SQL Agent job has not completed. Additionally, the active 
transaction log may contain records that have not been backed up. Before you start your planned 
failover, you must completely synchronize the secondary server and the active transaction log must 
be restored on the secondary server. The steps to do that are as follows: 
 1. 
Stop and disable the primary server transaction log backup job.
 2. 
Execute the log-shipping copy, and restore jobs to reinstate the remainder of the transaction 
logs. Use the log-shipping monitoring tool or report to verify that the entire set of 
transaction-log backups has been copied and restored on the secondary server. A manual 
option is to copy all transaction-log backups that have not been copied from the primary 
server backup folder to the secondary server folder. Then restore each transaction log in 
sequence to the secondary server. 
 3. 
Stop and disable the secondary server’s copy and restore jobs.
 4. 
Execute the Sync Secondary Server Access Information job to synchronize database 
dependencies and then disable it.

Managing Changing Roles ❘ 631
 5. 
Back up the active transaction log from the primary to the secondary server with 
NORECOVERY: 
 USE MASTER;
 BACKUP LOG <Database_Name> TO DISK =
 ‘C:\primaryBackupLog\<Database_Name>.trn’ 
 WITH NORECOVERY;
This accomplishes two goals: 
It backs up the active transaction log from the primary server so that it can be 
restored to the secondary server to synchronize the secondary database.
It changes the old primary server database to NORECOVERY mode to enable transac-
tion logs from the new primary server to be applied without initializing the database 
by a restore, as the log chain would not have been broken.
 6. 
Copy the backup log ﬁ les to the secondary server. On the secondary server, restore the tail 
of the transaction log and then recover the database: 
 RESTORE LOG <Database_Name>  
 FROM DISK =‘c:\secondaryBackupDest\Database_name.trn’WITH RECOVERY;
 7. 
If the active transaction log is not accessible, the database can be recovered without it: 
 RESTORE DATABASE <Database_Name>  WITH RECOVERY;
 8. 
On the new primary server, execute the Resolve Logins job to synchronize the logins. The 
secondary server’s database becomes the primary server’s database and starts to accept data 
modiﬁ cations.
 9. 
Redirect all applications to the new primary server.
 10. 
Conﬁ gure log shipping from the new primary server to the secondary server. The secondary 
server (the former primary server) is already in NORECOVERY mode. During log-shipping 
conﬁ guration, in the Secondary Database Settings dialog box, choose No, The Secondary 
Database Is Initialized.
 11. 
When you ﬁ nish conﬁ guring log shipping, the new primary server executes the transaction-
log backup job, and the secondary server copies and restores the transaction-log ﬁ les. Set 
up and enable all SQL jobs that were synchronizing from the old primary server (e.g., to 
synchronize the logins and database users to the old primary server).
Unplanned Failover
If the primary server becomes unavailable in an unplanned situation, some data loss is probable. 
This could be because the active transaction log cannot be backed up or some of the transaction-
log backup may not be reachable. Therefore, in an unplanned failover, you have to verify that the 
last copy and restore transaction logs have been restored by using the log-shipping monitoring and 
reporting functions. If the active transaction-log backup is accessible, it should be restored to bring 
the secondary server in synchronization with the primary server up to the point of failure. Then 
restore the secondary database with RECOVERY.
➤
➤

632  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
After assessing the damage and ﬁ xing the old primary server, you will most likely need to 
reconﬁ gure log-shipping conﬁ guration because the active transaction log may not have been 
accessible, or the log chain may have been broken. When you conﬁ gure log shipping, in the 
Secondary Database Settings dialog, choose either to restore from a previous database backup or to 
generate a new database backup to restore. You may choose to switch roles to promote the original 
primary server, which is discussed next.
Switching Between Primary and Secondary Servers
In a planned failover, after performing the steps to switch the primary and secondary server 
in a server change whereby the Log Shipping jobs have been deployed to both primary and secondary 
servers, you can switch between primary and secondary servers by following these steps: 
 1. 
Stop and disable the primary server’s transaction-log backup job.
 2. 
Verify that all the transaction-log backups have been copied and restored, either manually 
or by executing the SQL jobs.
 3. 
Execute the Sync Logins job.
 4. 
Stop and disable the transaction-log copy and restore jobs on the secondary server.
 5. 
Back up the active Transaction log on the primary server with NORECOVERY.
 6. 
Restore the active transaction-log backup on the secondary server.
 7. 
Restore the secondary server’s database with RECOVERY.
 8. 
Execute the Resolve Logins job.
 9. 
Enable the transaction-log backup on the new primary server to log-ship to the secondary 
server.
 10. 
Enable the secondary server transaction-log copy and restore jobs.
 11. 
Enable synchronization of the logins and database users.
 12. 
Enable any other SQL jobs.
Redirecting Clients to Connect to the Secondary Server
After switching roles, the client connections need to be redirected to the secondary server with 
minimal disruptions to users. Log shipping does not provide any client-redirect capability, so you 
need to choose another approach. The approach you choose may depend on the infrastructure 
and who controls it as well as the number of clients that need to be redirected, the required 
availability of the application (such as a service-level agreement [SLA]), and the application activity. 
At minimum, users will experience a brief interruption as the client applications are redirected. 
The following sections discuss a few common approaches to redirecting client connections to the 
secondary server.
Application Coding
The application can be developed as failover-aware with the capability to connect to the secondary 
server either with automatic retry or by manually changing the server name. The application logic 

Database Backup Plan ❘ 633
would connect to the primary server ﬁ rst, but if it is unavailable, and after the retry logic has 
run unsuccessfully, the application can attempt to connect to the secondary server if it has been 
promoted to a primary server. 
After the secondary database has been recovered however, it may not necessarily be ready to serve 
user requests. For example, you may need to run several tasks or jobs ﬁ rst, such as running the 
Resolve Logins task. To overcome this circumstance, the database may need to be put into single-
user mode to prevent other users from connecting while you perform tasks or jobs. Therefore, the 
application logic must handle this situation in which the primary server is no longer available and 
the secondary server is not yet accessible. 
Network Load Balancing
Use a network load balancing solution, either Windows Network Load Balancing (NLB) or a 
hardware solution. With this solution, the application connects using the load balancing network 
name or IP address, and the load balancing solution directs the application to the database server. 
Therefore, in a failover scenario, the application continues to connect to the network load balancer’s 
network name or IP address, while the load balancer is updated manually or by script with the new 
primary server network name and IP address. Then clients are redirected. NLB is included with 
certain versions of Microsoft Windows. Conﬁ guration is straightforward and can act as the cross-
reference to direct applications to the current primary server.
Domain Name Service (DNS)
DNS provides name-to-IP address resolution and can be used to redirect clients to the new primary 
server. If you have access to the DNS server, you can modify the IP address to redirect client 
applications after a failover, either by script or by using the Windows DNS management tool. DNS 
acts as a cross-reference for the client applications because they continue to connect to the same 
name, but the DNS modiﬁ cation redirects the database request to the new primary server.
SQL Client Aliasing
SQL Aliasing is another method that can be used to redirect clients to the new primary server. To 
conﬁ gure aliasing, go to the SQL Server Conﬁ guration Manager, under the SQL Native Client 
Conﬁ guration. This method may be less favorable if many client applications connect directly to 
the database server because the alias would have to be created at each client computer, which may 
not be feasible. It is more feasible if the client applications connect to a Web or application server 
that then connects to the database server. Then the SQL client alias can be applied on the Web or 
application server, and all the clients would be redirected to the new primary server. 
DATABASE BACKUP PLAN
Regardless of the high-availability solution, a database backup plan is strongly recommended to protect 
data from corruption or user error. A secondary database is not enough since a corruption in the 
primary database can cause corruption on the secondary database as well. There are two routes you 
can take when choosing a backup plan: a full database back up and a differential database backup. 
 
 
 
 

634  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
A full database backup copies all the data and the transaction log to a backup device, which is 
usually a tape or disk drive. It is used in case of an unrecoverable failure so that the database can be 
restored to that point in time at which it was last backed up. A full database restore is also required 
as a starting point for differential or transaction-log restores. The backup should be stored offsite so 
that the database is not lost in the event of a disaster.
A differential database backup copies all modiﬁ ed extents in the database since the last full database 
backup. An extent is a unit of space allocation that consists of eight database pages that SQL Server 
uses to allocate space for database objects. Differential database backups are smaller than the 
full database backup, except in certain scenarios where the database is active and every extent is 
modiﬁ ed since the last full backup. To restore from a differential database backup, a full database 
backup is required prior to the differential backup.
Full or differential database backup operations will not break log shipping, provided no transaction-
log operations are performed that change it. However, the inverse is possible in that log shipping can 
impact some backup and recovery plans in the following ways: 
Another transaction-log backup cannot be performed in addition to log shipping because 
that breaks the log chain for the log-shipping process. SQL Server will not prevent an 
operator from creating additional transaction-log backup jobs on a log-shipping database.
A transaction-log backup that truncates the transaction log will break the log chain, and log 
shipping will stop functioning.
If you change the database to the simple recovery model, the transaction log will truncate 
by the SQL Server and log shipping will stop functioning.
INTEGRATING LOG SHIPPING WITH OTHER HIGH-AVAILABILITY 
SOLUTIONS
Log shipping can be deployed along with other Microsoft high-availability solutions because log 
shipping provides disaster recovery while the other solution provides high availability. There are 
three main solutions with which you can integrate log shipping: 
Data mirroring: Maintains a remote site if the data-mirroring pair becomes unavailable. 
Windows failover clustering: Maintains a remote disaster recovery site if the local Windows 
failover cluster becomes unavailable. 
Replication: Maintains a highly available replication publisher.
SQL Server 2012 Data Mirroring
A scenario in which log shipping can be best integrated with a SQL Server 2012 data mirroring 
solution would be if an organization deployed local data mirroring and log shipping from the 
principal server to a remote location. However, during a data-mirroring role switch, log shipping 
does not automatically switch roles. Therefore manual steps must be taken to enable the former 
mirror, which would now be the principal, to start to log ship its transaction log to the secondary 
server by deploying log shipping from the new principal server.
➤
➤
➤
➤
➤
➤


636  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
Additionally, the following should be taken into consideration when using transactional replication 
with log shipping. 
For replication to continue to work after the role switch, the primary and secondary server 
conﬁ gurations must be identical after the switch. 
For transactional replication, to prevent the subscribers from having data that has to be 
shipped to the secondary server, the primary server should be conﬁ gured to use the sync 
with backup parameter in the transaction log backup command of log shipping, whereby 
a transaction is not replicated to the subscribers until a transaction log backup has been 
performed. This does introduce a latency penalty whereby the subscribers are not quite in 
real time because replication needs to wait for log shipping. This latency can be reduced by 
decreasing the interval to perform the transaction log backup. Without sync with backup, 
there is a possibility that the subscriber’s data may not match with the publisher during a 
log-shipping role switch, and data loss may occur.
In merge replication, after the role switch, the merge publisher may synchronize any changes 
lost during the role switch by merge replicating with each subscriber. A more common high-
availability solution to prevent a single point of failure for the publisher is to conﬁ gure a Windows 
failover cluster so that in case of a failure, replication fails over to the other cluster node. 
REMOVING LOG SHIPPING
Before deleting the log-shipping database, you need to remove log shipping from it. When you 
remove log shipping, all schedules, jobs, history, and error information are deleted. Recall that there 
are two ways to remove log shipping: with Management Studio and with T-SQL. You may want to 
script the log-shipping conﬁ guration before deleting to quickly redeploy log shipping in the future.
Removing Log Shipping with Management Studio
To use Management Studio to remove log shipping, follow these steps: 
 1. 
Choose the primary server’s database properties.
 2. 
Under Select a Page, choose Transaction Log Shipping.
 3. 
Clear the Enable This as a Primary Database in a Log Shipping Conﬁ guration check box, 
and click OK.
 4. 
If necessary, choose to remove a secondary server from the primary server’s database properties. 
Under Secondary Databases, choose the secondary server instance, and click Remove.
 5. 
To remove a monitor server instance, uncheck the Use a Monitor Server Instance check box.
Removing Log Shipping with T-SQL Commands
To remove log shipping with T-SQL, issue this command on the primary server: 
 Use Master;
 sp_delete_log_shipping_primary_secondary  @primary_database, @secondary_server,
 @secondary_database;
➤
➤
➤

Log-Shipping Performance ❘ 637
This command deletes secondary information on the primary server from the msdb.dbo.log_
shipping_primary_secondaries table.
On the secondary server, issue this command: 
 Use Master;
 sp_delete_log_shipping_secondary_database @secondary_database;
This command deletes the secondary information on the secondary server and its jobs by executing 
the sys.sp_delete_log_shipping_secondary_database_internal stored procedure.
Back on the primary server, issue this command: 
 Use Master;
 sp_delete_log_shipping_primary_database @database;
This command deletes the log-shipping information from the primary server and its jobs, removes 
monitor info and the monitor, and deletes the msdb.dbo.log_shipping_primary_databases table.
Then, if desirable, you can delete the secondary database.
LOG-SHIPPING PERFORMANCE
A performing log shipping solution is critical to provide faster failover to support the warm standby 
requirements, such as service level agreements (SLA). To perform, the following key areas must be 
tuned and optimized to handle the additional capacity of log shipping:  
Networking is required for copying the transaction log ﬁ les from the primary to the 
secondary server. 
IO on both the ﬁ le share on the primary server (where the transaction log ﬁ les are getting 
backed up) and on the secondary server (that is receiving the transaction log ﬁ les) and then 
applying them needs to have the read/write throughput. Consider placing the log-shipping 
backup directory on a separate disk drive from the database ﬁ les. Additionally, backup 
compression improves IO performance. As part of ongoing administration, monitor the I/O 
performance counters for any bottlenecks (for example, the average second/read and average 
second/write should preferably be less than 10ms). 
To keep the secondary server more closely in sync with the primary server, maintain shorter 
database transactions, when possible. A database transaction is log shipped only after it has 
been committed on the primary database.
Perform database administration activities, such as index defragmentation, during a period 
of lower activity. Depending on the level of fragmentation, the transaction-log ﬁ le will be 
larger and take longer to back up, copy, and restore increasing latency.
To maintain performance after failover, both the primary and secondary servers should be 
of identical hardware and server conﬁ gurations. 
➤
➤
➤
➤
➤


Summary ❘ 639
allocated downtime to upgrade the SQL Server 2005/2008 servers. This is the simpler approach 
because it does not involve going through the failover process. 
 1. 
Verify that the secondary server is in sync with the primary server by applying all the 
transaction-log backups and the active transaction log from the primary server. 
 2. 
Stop and disable the log-shipping jobs; then do an in-place upgrade on the secondary server, 
followed by an upgrade to the primary server. 
 3. 
Reconﬁ gure SQL Server 2012 log shipping using the same shared folders. Additionally, 
in the Secondary Database Settings, on the Initialize Secondary Database page, choose No, 
the Secondary Database Is Initialized. SQL Server 2012 log shipping starts shipping the 
transaction log without having to take a database backup and restore on the secondary 
server.
Deploy Log Shipping Approach
The deploy log shipping approach is more feasible when the log-shipping databases are small and 
can quickly be backed up, copied, and restored on the secondary server which has already been 
upgraded to SQL Server 2012. It is similar to the downtime approach, but instead of verifying and 
waiting for synchronization of the secondary database, after the upgrade and during the SQL Server 
2012 log-shipping conﬁ guration, on the Initialize Secondary Database page, choose Yes, Generate a 
Full Backup of the Primary Database. The log-shipping process performs a database backup on the 
primary server and restores the backup onto the secondary server to start the log shipping. 
SUMMARY
Log shipping is a simple, inexpensive, dependable SQL Server high-availability solution with a 
long track record. As a disaster recovery solution, it has been deployed to maintain a secondary 
server over a distance for business continuity to protect from a local disaster, power-grid failure, 
and network outages. Log shipping can ship the log anywhere in the world. Its only limitation is 
the network bandwidth capacity to transfer the transaction log ﬁ le in a timely manner. It can be 
combined with a reporting solution when an organization requires point-in-time reporting, rather 
than real time. Moreover, it has been deployed as an alternative to Windows failover clustering to 
provide a local, high-availability solution, where it is less expensive to implement because it does not 
require a shared disk system.
The main challenge in deploying log shipping is that it does not provide any automatic client 
redirect; therefore, an operator needs to have an approach for it. In addition, the role switch requires 
some user intervention to ensure that all the transaction logs have been restored to the secondary 
server. Switching the roles back also requires some manual intervention. Another challenge is that 
log shipping is at the user database level that does not copy new logins, SQL jobs, Integration 
Services, and linked servers from the primary server. However, using Integration Services’ Tasks and 
SQL Agent jobs, it can be accomplished.
Regardless of the risks and challenges, many organizations have successfully used log shipping by 
scripting all the processes, such as role switching, login synchronization, and client’s redirects, and 
can quickly failover with minimal manual downtime. 

640  ❘  CHAPTER 18  SQL SERVER 2012 LOG SHIPPING
Furthermore, as log shipping involves two or more physical separate servers that do not have any 
shared components (like a Windows failover cluster does), you can achieve patch-management 
independence whereby one server is patched, the roles are switched, and then the other server 
is patched.
AlwaysOn Availability Groups is the newest high-availability technology that may eventually 
replace log shipping. However, log shipping is a time-tested, high-availability solution that many 
large, strategic customers with mission-critical applications depend on every day to provide local 
disaster recovery. 





Database Mirroring In Action ❘ 645
How long that delay might be depends on many factors, such as server capacity, database workload, 
network latency, application architecture, disk throughput, and more. An application with a lot 
of small transactions has more impact on response time than one with long transactions because 
transactions wait for acknowledgment from the mirror, and the wait time adds proportionately 
more to the response time of short transactions.
If you choose the SAFETY OFF option, you are setting up database mirroring in high performance 
mode, or asynchronous mirroring mode. In this mode, the log transfer process is the same as in 
synchronous mode, but the principal does not wait for acknowledgment from the mirror that the log 
buffer is hardened to the disk on a commit. As soon as step 3 in Figure 19-2 occurs, the transaction 
is committed on the principal. The database is synchronized after the mirror server catches up to 
the principal server. Because the mirror server is busy keeping up with the principal server, if the 
principal suddenly fails, you may lose data, however it will only be that which hasn’t been sent 
to the mirror. In this operating mode, there is minimal impact on response time or transaction 
throughput because it does not wait for the mirror to commit. Note three important terms in 
database mirroring which are: 
Send queue: While sending the log records from the principal to the mirror, if the log 
records can’t be sent at the rate at which they are generated, a queue builds up at the 
principal, in the database transaction log which is known as the send queue. The send 
queue does not use extra storage or memory. It exists entirely in the transaction log of the 
principal. It refers to the part of the log that has not yet been sent to the mirror.
Redo queue: While applying log records on the mirror, if the log records can’t be applied 
at the rate at which they are received, a queue builds up at the mirror in the database 
transaction log which is known as the redo queue. Like the send queue, the redo queue 
does not use extra storage or memory. It exists entirely in the transaction log of the mirror. 
It refers to the part of the hardened log that remains to be applied to the mirror database to 
roll it forward. Mostly, a single thread is used for redo, but SQL Server Enterprise Edition 
implements parallel redo — that is, a thread for every four processing units (equivalent to 
four cores).
Stream compression: When transferring data across the partners, data mirroring uses 
stream data compression, which reduces network utilization and can achieve at least a 12.5 
percent compression ratio. In the data mirror scenario, the principal compresses the data 
before sending it to the mirror, and the mirror uncompresses the data before applying it. 
This slightly increases the CPU utilization on both the principal and the mirror to compress 
and uncompress the data, while reducing network utilization. It is especially useful for 
database workloads that incur a great deal of data modiﬁ cation throughout the day, 
reducing the amount of network resources that data mirroring uses.
DATABASE MIRRORING IN ACTION
Now that you understand transaction safety, you can look at an example to better understand the 
operating modes and other mirroring concepts. You need to designate three SQL Server instances 
for this example: one principal server, one mirror server, and one witness server. In this example, 
as shown in Figure 19-3, you set up high-safety mode with automatic failover. This example 
➤
➤
➤

646  ❘  CHAPTER 19  DATABASE MIRRORING
assumes that all three SQL server instances are on the network and run under the same domain 
account, which is Administrator on the SQL Server instance and has access to the other SQL Server 
instances. 
5.    Create Certiﬁcate on the Witness
6.    Create Endpoint on Witness
9.    Create Login and Grant Connect on Witness
17.   Setup Witness Server
Principal Server
Mirror Server
Witness Server
1.    Create Certiﬁcate on the Principal
2.   Create Endpoint on Principal 
7.   Create Login and Grant Connect on  Principal
10.  Backup the AdventureWorks database on Principal
12.  Insert and modify data on Principal
13.  Backup Transaction Log on Principal
15.  Setup Principal Server
3.    Create Certiﬁcate on the Mirror
4.    Create Endpoint on Mirror
8.    Create Login and Grant Connect on Mirror
11.    Restore the AdventureWorks database on Mirror
14.   Setup Mirror Server
16.   Restore Transaction Log on Mirror
FIGURE 19-3
Preparing the Endpoints
For database-mirroring partners to connect to each other, they must trust each other. This trust is 
established by means of Transmission Control Protocol (TCP) endpoints. Therefore, on each partner 
you have to create the endpoint using the T-SQL statement CREATE ENDPOINT and grant the connect 
permission on these endpoints using the GRANT CONNECT ON ENDPOINT statement. The endpoint 
concept is exactly the same as discussed in Chapter 8, “Securing the Database Instance” so the rules 
are the same. The only difference is that instead of creating an endpoint for Service Broker, here you 


648  ❘  CHAPTER 19  DATABASE MIRRORING
The BACKUP CERTIFICATE statement backs up the public key certiﬁ cate for this private key.
 4. 
Now create the endpoint on the principal server. Connect to the principal server, and 
execute the CreateEndPointOnPrincipal.sql script: 
 --Check If Mirroring endpoint exists
 IF NOT EXISTS(SELECT * FROM sys.endpoints WHERE type = [type_desc]
 CREATE ENDPOINT DBMirrorEndPoint
 STATE = STARTED AS TCP (LISTENER_PORT = port_num)
 FOR DATABASE_MIRRORING ( AUTHENTICATION = CERTIFICATE PrincipalServerCert,
                          ENCRYPTION = REQUIRED
                         ,ROLE = ALL
                        )
code snippet CreateEndPointOnPrincipal.sql
This code creates the endpoint DBMirrorEndPoint, and you have speciﬁ ed the 
PrincipalServerCert certiﬁ cate to use for authentication. You also speciﬁ ed ROLE=ALL, 
which indicates that this server can act as either the principal, mirror, or witness server. 
If you want this server to act only as the witness, you can specify WITNESS as a parameter. 
You can also specify the PARTNER option, which indicates that the server can act as either 
the principal or the mirror, but not the witness.
 5. 
Now create the certiﬁ cates on both the mirror and the witness servers. Connect to the 
mirror server and execute the CreateCertOnMirror.sql script: 
USE MASTER
 GO
 IF NOT EXISTS(SELECT 1 FROM sys.symmetric_keys where name =
 ‘##MS_DatabaseMasterKey##’)
 CREATE MASTER KEY ENCRYPTION BY PASSWORD = ‘23%&weqÐyzYu3000!’GO
 
 IF NOT EXISTS (select 1 from sys.databases where
 [is_master_key_encrypted_by_server] = 1)
 ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY
 GO
 
 IF NOT EXISTS (SELECT 1 FROM sys.certificates WHERE name = ‘MirrorServerCert’)
 CREATE  CERTIFICATE MirrorServerCert
 WITH SUBJECT = ‘Mirror Server Certificate’
 GO
 
 BACKUP CERTIFICATE MirrorServerCert TO FILE = ‘<your folder>\MirrorServerCert.cer’
code snippet CreateCertOnMirror.sql
 6. 
Next, while connected to the mirror server, execute the CreateEndPointOnMirror.sql script: 
 --Check If Mirroring endpoint exists
 IF NOT EXISTS(SELECT * FROM sys.endpoints WHERE type = 4)
 CREATE ENDPOINT DBMirrorEndPoint
 STATE=STARTED AS TCP (LISTENER_PORT = port_num

Database Mirroring In Action ❘ 649
 FOR DATABASE_MIRRORING ( AUTHENTICATION = CERTIFICATE MirrorServerCert,
                          ENCRYPTION = REQUIRED
                         ,ROLE = ALL
                        )
code snippet CreateEndPointOnMirror.sql
 7. 
Connect to the witness server, and execute the CreateCertOnWitness.sql script: 
 USE MASTER
 GO
 IF NOT EXISTS(SELECT 1 FROM sys.symmetric_keys where name =
 ‘##MS_DatabaseMasterKey##’)
 CREATE MASTER KEY ENCRYPTION BY PASSWORD = ‘23%&weqÐyzYu3000!’GO
 
 IF NOT EXISTS (select 1 from sys.databases where
 [is_master_key_encrypted_by_server] = 1)
 ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY
 GO
 
 IF NOT EXISTS (SELECT 1 FROM sys.certificates WHERE name = ‘WitnessServerCert’)
 CREATE  CERTIFICATE WitnessServerCert
 WITH SUBJECT = ‘Witness Server Certificate’
 GO
 
 BACKUP CERTIFICATE WitnessServerCert 
 TO FILE = ‘<your folder>\WitnessServerCert.cer’
code snippet CreateCertOnWitness.sql
 8. 
Finally, while connected to the witness server, execute the CreateEndPointOnWitness.sql 
script: 
 --Check If Mirroring endpoint exists
 IF NOT EXISTS(SELECT * FROM sys.endpoints WHERE type = 4)
 CREATE ENDPOINT DBMirrorEndPoint
 STATE=STARTED AS TCP (LISTENER_PORT = port_num)
 FOR DATABASE_MIRRORING 
( AUTHENTICATION = CERTIFICATE WitnessServerCert, ENCRYPTION
 = REQUIRED
                         ,ROLE = ALL
                        )
code snippet CreateEndPointOnWitness.sql
Because all the partners can talk to each other, each partner needs permission to connect to the 
others. To do that, you have to create logins on each server and associate the logins with certiﬁ cates 
from the other two servers and grant connect permission to that user on the endpoint.
 1. 
First, copy the certiﬁ cates created in the previous scripts with the BACKUP 
CERTIFICATE command to the other two servers. For example, copy the certiﬁ cate 

650  ❘  CHAPTER 19  DATABASE MIRRORING
PrincipalServerCert.cer on the principal from the new folder you created earlier to the 
similar folders on both the witness and mirror servers.
 2. 
Connect to the principal server and execute the Principal_CreateLoginAndGrant.sql 
script: 
 USE MASTER
 GO
 
 --For Mirror server to connect
 IF NOT EXISTS(SELECT 1 FROM sys.syslogins WHERE name = ‘MirrorServerUser’)
 CREATE LOGIN MirrorServerUser WITH PASSWORD = ‘32sdgsgyÐ%$!’
 IF NOT EXISTS(SELECT 1 FROM sys.sysusers WHERE name = ‘MirrorServerUser’)
 CREATE USER MirrorServerUser;
 
 IF NOT EXISTS(SELECT 1 FROM sys.certificates WHERE name = ‘MirrorDBCertPub’)
 CREATE CERTIFICATE MirrorDBCertPub  AUTHORIZATION MirrorServerUser
 FROM FILE = ‘<your folder>\MirrorServerCert.cer’
 
 GRANT CONNECT ON ENDPOINT::DBMirrorEndPoint TO MirrorServerUser
 GO
 
 --For Witness server to connect
 IF NOT EXISTS(SELECT 1 FROM sys.syslogins WHERE name = ‘WitnessServerUser’)
 CREATE LOGIN WitnessServerUser WITH PASSWORD = ‘32sdgsgyÐ%$!’
 IF NOT EXISTS(SELECT 1 FROM sys.sysusers WHERE name = ‘WitnessServerUser’)
 CREATE USER WitnessServerUser;
 
 IF NOT EXISTS(SELECT 1 FROM sys.certificates WHERE name = ‘WitnessDBCertPub’)
 CREATE CERTIFICATE WitnessDBCertPub  AUTHORIZATION WitnessServerUser
 FROM FILE = ‘<your folder>\WitnessServerCert.cer’
 
 GRANT CONNECT ON ENDPOINT::DBMirrorEndPoint TO WitnessServerUser
 GO
code snippet Principal_CreateLoginAndGrant.sql
This script creates two users on the principal server: MirrorServerUser and 
WitnessServerUser. These users are mapped to the certiﬁ cates from the mirror and the 
witness. After that, you granted connect permission on the endpoint, so now the mirror 
and the witness server have permission to connect to the endpoint on the principal server. 
Perform the same steps on the mirror server and witness server.
 3. 
Next, connect to the mirror server, and execute the Mirror_CreateLoginAndGrant
.sql script. Finally, connect to the witness server, and execute the Witness_
CreateLoginAndGrant.sql script.
Now you have conﬁ gured the endpoints on each server, using certiﬁ cates. If you want to use the 
Windows authentication, the steps to conﬁ gure the endpoints are a bit easier than using certiﬁ cates. 
Simply do the following on each server (this example is for the principal): 
 IF NOT EXISTS(SELECT * FROM sys.endpoints WHERE type = 4)
 CREATE ENDPOINT DBMirrorEndPoint
 
 
 
 

Database Mirroring In Action ❘ 651
 STATE = STARTED AS TCP (LISTENER_PORT = 5022)
 FOR DATABASE_MIRRORING ( AUTHENTICATION = WINDOWS, ROLE = ALL)
 
 GRANT CONNECT ON ENDPOINT::DBMirrorEndPoint TO
 [MyDomain\MirrorServerServiceAccount]
 GO
 
 GRANT CONNECT ON ENDPOINT::DBMirrorEndPoint TO
 [MyDomain\WitnessServerServiceAccount]
 GO
code snippet Mirror_CreateLoginAndGrant.sql
code snippet Witness_CreateLoginAndGrant.sql
Of course, you have to change the logins appropriately. In Windows authentication mode, each 
server uses the service account under which it is running to connect to the other partners, so you 
have to grant connect permission on the endpoint to the service account of SQL Server 2012. 
Additionally, you can use SQL Server 2012 Management Studio to conﬁ gure data mirroring with 
Windows authentication. Right-click the database you want to mirror, and choose Tasks, then Á 
Mirror. A wizard starts, as shown in Figure 19-4. Click the Conﬁ gure Security button, which takes 
you through the steps to conﬁ gure database mirroring. The wizard tries to use 5022 as the default 
TCP port for database mirroring, but you can change it if you want.
FIGURE 19-4




Database Mirroring In Action ❘ 655
This script assumes that you have copied the transaction log in the C:\ drive on the mirror 
server. If you put the transaction log somewhere else, substitute that folder location, and 
then run the script. Now the principal and mirror databases are in sync.
 6. 
Connect to the mirror server and execute the SetupMirrorServer.sql script again. 
 7. 
Then connect to the principal server, and execute the SetupPrincipalServer.sql script. 
It should succeed now because the transaction log has been restored, and you have just 
established the mirroring session.
code Snippet SetupMirrorServer.sql
code Snippet SetupPrincipalServer.sql
When you execute the SetupPrincipalServer.sql or SetupMirrorServer.sql scripts, you may 
get the following type of error: 
 Database mirroring connection error 4 ‘An error occurred 
  while receiving data: ‘64(The specified network name 
  is no longer available.)’.’ for TCP://YourMirrorServer:5022’.  
  Error: 1443, Severity: 16, State: 2.
It’s possible that the ﬁ rewall on the mirror or principal server is blocking the connection on the port 
speciﬁ ed. Go to Windows Firewall and Advanced Security to verify that the port numbers chosen 
for database mirroring are not blocked. 
High-Safety Operating Mode Without Automatic Failover
When you establish the mirroring session, the transaction safety is set to FULL by default, so the 
mirroring session is always established in high-safety operating mode without automatic failover. In 
this operating mode, a witness is not set up, so automatic failover is not possible. Because the witness 
server is not present in this operating mode, the principal doesn’t need to form a quorum to serve the 
database. If the principal loses its quorum with the mirror, it still keeps serving the principal database 
and the mirror transactions are queued on the principal and send once the mirror becomes available.
Next you will learn how to change it to automatic failover.
High-Safety Operating Mode with Automatic Failover
Automatic failover means that if the principal database (or the server hosting it) fails, the database 
mirroring will failover to the mirror server, and the mirror server will assume the principal role 
and serve the database. However, you need a third server, the witness, for automatic failover to 
the mirror. The witness just sits there as a third party and is used by the mirror to verify that the 
principal is really down, providing a “2 out of 3” condition for automatic failover. No user action is 
necessary to failover to the mirror if a witness server is present.
Witness Server
If you choose the SAFETY FULL option, you have an option to set up a witness server (refer to 
Figure 19-1). The presence of the witness server in high-safety mode determines whether you can 


Database Mirroring In Action ❘ 657
A database in its data mirroring session must be in one of these three quorum event types to serve as 
the principal database. 
High-Performance Operating Mode
By default, the SAFETY is ON when you establish the mirroring session, so to activate the high-
performance operating mode, you have to turn the safety OFF, like so: 
 USE Master
 ALTER DATABASE AdventureWorks SET PARTNER SAFETY OFF
There is minimal impact on transaction throughput and response time in this mode. The log 
transfer to the mirror works the same way as in high-safety mode, but because the principal 
doesn’t wait for hardening the log to disk on the mirror, it’s possible that if the principal goes down 
unexpectedly, you may lose data.
You can conﬁ gure the witness server in high-performance mode, but because you cannot do 
automatic failover in this mode, the witness will not provide any beneﬁ ts. Therefore, don’t deﬁ ne 
a witness when you conﬁ gure database mirroring in high-performance mode. You can remove the 
witness server by running the following command if you want to change the operating mode to high 
performance with no witness: 
 USE Master
 ALTER DATABASE AdventureWorks SET WITNESS OFF
If you conﬁ gure the witness server in a high-performance mode session, the enforcement of quorum 
means the following: 
If the mirror server is lost, then the principal must be connected to the witness. Otherwise, 
the principal server takes its database ofﬂ ine until either the witness server or the mirror 
server rejoins the session.
If the principal server is lost, then forcing failover to the mirror requires that the mirror 
server be connected to the witness.
The only way you can failover to the mirror in this mode is by running the following command 
on the mirror when the principal server is disconnected from the mirroring session. This is called 
forced failover. Refer to Table 19-1 for the different failover types. 
 USE MASTER
 ALTER DATABASE AdventureWorks SET PARTNER FORCE_SERVICE_ALLOW_DATA_LOSS
The forced failover causes an immediate recovery of the mirror database, which may cause data loss. 
This mode is best used for transferring data over long distances (for disaster recovery to a remote site) 
or for mirroring an active database for which some potential data loss is acceptable. For example, 
you can use high-performance mode for mirroring your warehouse. Then, you can use a database 
snapshot as discussed in the “Database Snapshot” section of this chapter to create a snapshot on the 
mirror server to enable a reporting environment from the mirrored warehouse.
➤
➤

658  ❘  CHAPTER 19  DATABASE MIRRORING
DATABASE MIRRORING AND SQL SERVER 2012 EDITIONS
Now that you understand the operating modes of database mirroring, it is a good idea to keep 
in mind which of these operating modes is available on your edition of SQL Server. Table 19-2 
summarizes which features of database mirroring are available in which SQL Server 2012 editions.
TABLE 19-2: SQL Server 2012 Features Supported by Edition
DATABASE MIRRORING 
FEATURE
ENTERPRISE 
EDITION
DEVELOPER 
EDITION
STANDARD 
EDITION
BUSINESS  
INTELLIGENCE 
EDITION
SQL 
EXPRESS 
EDITION
Partner (principal or 
mirror)
√
√
√
√
Witness
√
√
√
√
√
Safety = FULL
√
√
√
√
Safety = OFF
√
√
Available during UNDO 
after failover
√
√
√
√
Parallel REDO
√
√
The SQL Express edition can be used only as a witness server in a mirroring session. Some other 
features such as high-performance operating mode, you need the Enterprise or Developer edition.
SQL Server 2012 may use multiple threads to roll forward the log in the redo queue on the mirror 
database. This is called parallel redo. If the mirror server has fewer than ﬁ ve CPUs, SQL Server uses 
only a single thread for redo. Parallel redo is optimized by using one thread per four CPUs. This 
feature is available only in the Enterprise or Developer editions. 
DATABASE MIRRORING CATALOG VIEWS
In the example so far, you have set up the mirroring session but you still haven’t learned how to get 
information about the mirroring conﬁ guration. Therefore, before continuing with failover scenarios 
and other topics, you need to learn how you can get that information. The following sections 
describe the catalog views you can use to get information about database mirroring. SQL Server 
2012 Books Online describes every column in database mirroring catalog views. 
The following shows you when and how you should use these views.
sys.database_mirroring
The most important view to monitor mirroring state, safety level, and witness status (when present) 
is sys.database_mirroring. See the following query, which you can execute on either partner 
(principal or mirror), and you will get the same results: 

Database Mirroring Catalog Views ❘ 659
 SELECT
  DB_NAME(database_id) AS DatabaseName
 ,mirroring_role_desc
 ,mirroring_safety_level_desc
 ,mirroring_state_desc
 ,mirroring_safety_sequence
 ,mirroring_role_sequence
 ,mirroring_partner_instance
 ,mirroring_witness_name
 ,mirroring_witness_state_desc
 ,mirroring_failover_lsn
 ,mirroring_connection_timeout
 ,mirroring_redo_queue
 FROM sys.database_mirroring
 WHERE mirroring_guid IS NOT NULL
You use this query often when you establish the mirroring session. If you run this query after 
you establish the mirroring session in the example scenario, you see output similar to the result in 
Table 19-3. Of course, some values will be different based on your server names, and so on.
TABLE 19-3: sys.database_mirroring view
METADATA COLUMN IN SELECT LIST
PRINCIPAL VALUES: 
YOURPRINCIPALSERVER
MIRROR VALUES: 
YOURMIRRORSERVER
DatabaseName
AdventureWorks
AdventureWorks
mirroring_role_desc
PRINCIPAL
MIRROR
mirroring_safety_level_desc
FULL
FULL
mirroring_state_desc
SYNCHRONIZED
SYNCHRONIZED
mirroring_safety_sequence
1
1
mirroring_role_sequence
3
3
mirroring_partner_instance
SQLServer1\SQLServer2
SQLServer1
mirroring_witness_name
TCP://SQLServer1.
myCluster.local:5024
TCP://SQLServer1.
myCluster.local:5024
mirroring_witness_state_desc
CONNECTED
CONNECTED
mirroring_failover_lsn
1157000000027300001
1157000000027300001
mirroring_connection_timeout
10
10
mirroring_redo_queue
NULL
NULL
Some values in Table 19-3 are self-explanatory, but others require some additional explanation: 
mirroring_safety_sequence gives a count of how many times the safety has changed 
(from FULL to OFF and back) since the mirroring session was established.
mirroring_role_sequence gives a count of how many times failover has happened since 
the mirroring session was established.
➤
➤

660  ❘  CHAPTER 19  DATABASE MIRRORING
mirroring_failover_lsn gives the log sequence number of the latest transaction that is 
guaranteed to be hardened to permanent storage on both partners. In this case, because there 
is little database activity, both of these numbers are the same if you use the ModifyData.sql 
script in a WHILE loop. (If you change the script, be sure not to use an inﬁ nite loop!)
mirroring_connection_timeout gives the mirroring connection timeout, in seconds. This 
is the number of seconds to wait for a reply from a partner or witness before considering 
them unavailable. The default timeout value is 10 seconds. If it is null, then the database is 
inaccessible or not mirrored. It can be set by using this command: 
ALTER DATABASE [database] SET PARTNER TIMEOUT [seconds]
mirroring_redo_queue displays the maximum amount of the transaction log to be redone 
at the mirror in megabytes when the mirroring_redo_queue_type is not set to unlimited. 
When the maximum is reached, the principal transaction log temporarily stalls to enable 
the mirror to catch up; therefore, it can limit the failover time. When the mirroring_redo_
queue_type is set to unlimited which is the default, data mirroring does not inhibit the 
redo_queue. It can be set by using this command:
 ALTER DATABASE <database> SET PARTNER REDO_QUEUE <UNLIMITED or MB>
sys.database_mirroring_witnesses
If you have a witness established for your mirroring session, then sys.database_mirroring_
witnesses returns data mirroring information with the following query: 
 SELECT * FROM sys.database_mirroring_witnesses
You can execute this query on the witness server to list:
principal and mirror server names 
the database name 
safety level for all the mirroring sessions 
You get the following multiple rows from this query if the same witness server acts as a witness for 
more than one mirroring session. 
role_sequence_number column displays how many times failover has happened between 
mirroring partners since the mirroring session was established.
is_suspended column displays whether database mirroring is suspended. If this column 
value is 1, then mirroring is currently suspended.
sys.database_mirroring_endpoints
The following query returns information about database mirroring endpoints such as port number, 
whether encryption is enabled, authentication type, and endpoint state: 
 SELECT
  dme.name AS EndPointName
 ,dme.protocol_desc
➤
➤
➤
➤
➤
➤
➤
➤

Database Mirroring Role Change ❘ 661
 ,dme.type_desc AS EndPointType
 ,dme.role_desc AS MirroringRole
 ,dme.state_desc AS EndPointStatus
 ,te.port AS PortUsed
 ,CASE dme.is_encryption_enabled 
       WHEN  1 THEN ‘Yes’
       ELSE ‘No’
  END AS Is_Encryption_Enabled
 ,dme.encryption_algorithm_desc
 ,dme.connection_auth_desc
 FROM sys.database_mirroring_endpoints dme
 JOIN sys.tcp_endpoints te
   ON dme.endpoint_id = te.endpoint_id
This query uses the sys.tcp_endpoints view because the port information is not available in the 
sys.database_mirroring_endpoints catalog view.
DATABASE MIRRORING ROLE CHANGE
In the example so far, you have learned how to establish and monitor a database mirroring session. 
You have likely noticed that if you try to query the AdventureWorks database on your mirror server, 
you get an error like the following: 
 Msg 954, Level 14, State 1, Line 1
 The database “AdventureWorks” cannot be opened. It is acting as a mirror database.
You cannot access the mirrored database, so how do you switch the roles of the mirroring partners? You 
can failover to the mirror server in three ways (also described earlier): 
Automatic failover
Manual failover
Forced Failover
The failover types depend on which transaction safety is used (FULL or OFF) and whether a witness 
server is present.
Automatic Failover
Automatic failover is a database mirroring feature in high-availability mode (SAFETY FULL when 
a witness server is present). When a failure occurs on the principal, automatic failover is initiated. 
Because you have set up database mirroring in high-availability mode with a witness server, you are 
ready to do automatic failover. The following events occur in an automatic failover scenario: 
 1. 
The failure occurs: The principal database becomes unavailable. This could be the result of 
a power failure, a hardware failure, a storage failure, or some other reason.
 2. 
The failure is detected: The failure is detected by the mirror and the witness. Both partners 
and witness continually ping each other to identify their presence. Of course, it is more than 
just a simple ping, detecting things such as whether the SQL Server is available, whether the 
principal database is available, and so on. A timeout is speciﬁ ed for the ping, which is set to 
➤
➤
➤

662  ❘  CHAPTER 19  DATABASE MIRRORING
10 seconds by default when you set up the database mirroring session. You can change the 
timeout using the ALTER DATABASE as described earlier. 
If the principal does not respond to the ping message within the timeout period, it is 
considered to be down, and failure is detected. You should leave the default setting for 
timeout at 10 seconds, or at least do not change it to less than 10 seconds because under 
heavy load and sporadic network conditions, false failures may occur, and your database 
will start failing over back and forth.
 3. 
A complete redo is performed on the mirror: The mirror database has been in the restoring 
state until now, continuously redoing the log (rolling it forward onto the database). When 
failure is detected, the mirror needs to recover the database. In order to do that, the mirror 
needs to redo the remaining log entries in the redo queue.
 4. 
The failover decision is made: The mirror now contacts the witness server to form a quorum 
and determines whether the database should failover to the mirror. In the high-safety mode 
with automatic failover, the witness must be present for automatic failover. The decision 
takes about 1 second, so if the principal comes back up before step 3 is complete, that 
failover is terminated.
 5. 
The mirror becomes the principal: The redo continues while the failover decision is being 
made. After both the witness and the mirror have formed a quorum on the failover decision, 
the database is recovered completely. The mirror’s role is switched to principal; recovery is 
run (this involves setting up various database states and rolling back any in-ﬂ ight system 
transactions and starting up rollback of user transactions); then the database is available to 
the clients; and normal operations can be performed. 
 6. 
Undo: There may be uncommitted transactions, the transactions sent to the mirror while 
the principal was available but not committed before the principal went down in the 
transaction log of the new principal, which are rolled back.
Normally, the time taken to failover in this operating mode is short, usually seconds, but that mostly 
depends on the redo phase. If the mirror is already caught up with the principal before the principal 
has gone down, the redo phase will not introduce time lag. The time to apply the redo records 
depends on the redo queue length and the redo rate on the mirror. The failover will not happen if the 
mirroring_state is not synchronized. There are some performance counters available, which are 
examined in the “Performance Monitoring Database Mirroring” section. From these counters, you 
can estimate the time it will take to apply the transaction log for the redo queue on the mirror server.
To measure the actual time, you can use the SQL Proﬁ ler to trace the event. See Chapter 13, 
“Performance Tuning T-SQL,” to learn more about running traces with SQL Proﬁ ler, which you use 
here to measure the actual time it takes to failover: 
 1. 
Open SQL Proﬁ ler.
 2. 
Connect to the mirror server.
 3. 
Choose the Database Mirroring State Change event under the Database events group.
 4. 
Choose the TextData and StartTime columns.
 5. 
Start the trace.
 6. 
Stop the SQL Server service on the principal server. Soon, automatic failover happens.

Database Mirroring Role Change ❘ 663
Two columns in the trace are of interest: TextData provides the description of the database 
mirroring state change event. StartTime represents the timestamp at which time the event took 
place. Figure 19-5 shows the SQL Proﬁ ler trace of the events.
FIGURE 19-5
In the event Synchronized Mirror with Witness, the mirror informs the witness server that the 
connection to the principal is lost, as shown in the step 4 event in Figure 19-2. Then the mirror fails 
over to become the principal; this means that the principal will be running without a partner and 
if it were to fail, the database will be ofﬂ ine. You also see a message similar to the following in the 
mirror server SQL error log: 
 The mirrored database “AdventureWorks” is changing roles from “MIRROR” to
 “PRINCIPAL” due to Auto Failover.
In the StartTime column, the actual failover time for this automatic failover was approximately 7 
seconds.
The duration of the failover depends upon the type of failure and the load on the database. 
Under load, it takes longer to failover than in a no-load situation. You see messages similar to the 
following in the SQL error log: 
 The mirrored database “AdventureWorks” is changing roles from “MIRROR” to
 “PRINCIPAL” due to Failover from partner.
 Starting up database ‘AdventureWorks’.
 Analysis of database ‘AdventureWorks’ (9) is 81% complete (approximately 0
 seconds
 remain).


Database Mirroring Role Change ❘ 665
 2. 
To see what’s happening behind the scenes, start SQL Proﬁ ler, connect it to the mirror server, 
and select the event Database Mirroring State Change under the Database event group. 
 3. 
Then run the trace. Two columns in the trace are of interest: TextData and StartTime. 
 4. 
To see activities on the principal, start another instance of SQL Proﬁ ler and connect it to 
the principal.
 5. 
Now connect to your principal server and execute the following command: 
ALTER DATABASE AdventureWorks SET PARTNER FAILOVER
You have just forced a manual failover from the principal to the mirror. You can also use SQL 
Server 2012 Management Studio to execute a manual failover as follows: 
 1. 
Right-click the principal database, and select Tasks Á Mirror to access the dialog shown in 
Figure 19-6. 
 2. 
Click the Failover button to get a failover conﬁ rmation dialog.
 3. 
Click OK again.
FIGURE 19-6
You can use manual failover for planned downtime, migrations and upgrading, as discussed in the 
section “Preparing the Mirror Server for Failover.”


Database Availability Scenarios ❘ 667
DATABASE AVAILABILITY SCENARIOS
So far you have learned about database mirroring operating modes and how to failover in different 
operating modes. This section describes what happens to the database availability of clients when 
the server is lost. The server might be lost not only because the power is off but also because of a 
communication link failure or some other reason; the point is that the other server in the mirroring 
session cannot communicate. Several different scenarios exist. To keep matters clear, you use three 
server names in this section: ServerA (principal), ServerB (mirror) and ServerC (witness).
Principal Is Lost
If the principal server is lost, the failover scenario depends on the transaction safety (FULL or OFF) 
and whether a witness is present.
Scenario 1: Safety FULL with a Witness
In this scenario (covered in further detail in the automatic failover section), the mirror forms a 
quorum with the witness because the principal is lost. Automatic failover will happen (of course, 
certain conditions must be met, as mentioned earlier), the mirror becomes the new principal server, 
and the database will be available on the new principal.
In this situation, before the failure, ServerA was the principal, ServerB was the mirror, and ServerC 
was the witness. ServerA now fails. After failover, ServerB becomes the principal and serves the 
database. However, because there is no mirror server after failover (because ServerA is down), 
ServerB runs exposed, and the mirroring state is DISCONNECTED. If ServerA becomes operational, then 
it automatically assumes the mirror role, except that the session is suspended until a RESUME is issued.
If SAFETY is FULL and you have conﬁ gured a witness, to serve the database, at least two servers need 
be available to form a quorum. In this scenario, if ServerA fails, then ServerB becomes the principal 
and serves the database; but now if ServerC (witness) goes down, ServerB cannot serve the database.
Scenario 2: Safety FULL Without a Witness
In this operating mode, SAFETY is FULL, but automatic failover is not possible. Therefore, if ServerA 
(principal) fails, the database is unavailable to the clients. You need to manually perform several 
steps to force the database to become available again.
In this situation, before the failure, ServerA was the principal, ServerB was the mirror, and there 
was no witness. ServerA is now lost, so the database is unavailable to clients. To make the database 
available, you need to execute the following commands on the mirror: 
 ALTER DATABASE <database_name> SET PARTNER OFF
 RESTORE DATABASE <database_name> WITH RECOVERY
These commands bring the database on ServerB online, making it available again. When ServerA 
comes online, you have to reestablish the mirroring session. However, an alternative in which you 
do not have to reestablish the mirroring session does exist. Execute the following command on 
ServerB (which is still the mirror after ServerA becomes unavailable): 
 ALTER DATABASE <database_name> SET PARTNER FORCE_SERVICE_ALLOW_DATA_LOSS

668  ❘  CHAPTER 19  DATABASE MIRRORING
That brings the database online on ServerB, which becomes the principal server. When ServerA 
comes online, it automatically assumes the mirror role. However, the mirroring session is 
suspended, meaning no transaction logs will be sent from ServerB to ServerA. You’ll need to resume 
the mirroring session by starting to send the transaction logs from ServerB to ServerA. To do so, 
execute the following command: 
 ALTER DATABASE <database_name> SET PARTNER RESUME
Whether you choose to break the mirroring session or force failover, you lose any transactions that 
were not sent to the mirror at the time of failure.
Scenario 3: SAFETY OFF
When SAFETY is OFF, the witness doesn’t add any value, you do not need to conﬁ gure a witness in 
that case. If ServerA (principal) is lost in this scenario, the database becomes unavailable. Then you 
have to force failover for the database to become available again.
In this scenario, before the failure, ServerA was the principal and ServerB was the mirror. ServerA 
now fails and the database is not available to clients. You can manually failover to ServerB by using 
the following command: 
 ALTER DATABASE <database_name> SET PARTNER FORCE_SERVICE_ALLOW_DATA_LOSS
However, the SAFETY is OFF, so any transactions that were not received by the mirror at the 
time of the principal failure will be lost. Therefore, manual failover with SAFETY is OFF involves 
acknowledging the possibility of data loss. When ServerA becomes operational again, it 
automatically assumes the mirror role, but the mirroring session is suspended. You can resume the 
mirroring session again by executing the following command: 
 ALTER DATABASE <database_name> SET PARTNER RESUME
Mirror Is Lost
If the mirror fails, then the principal continues functioning, so the database continues to be 
available to the clients. The mirroring state is DISCONNECTED, and the principal runs exposed which 
means without a failover partner. You can use the sys.database_mirroring catalog view to see 
the mirroring state on the principal server.
When the mirror goes down, you have to be a little careful and take steps to ensure that the principal 
continues to serve the database without any issue. The mirroring state changes to DISCONNECTED, and 
as long as the state is DISCONNECTED the transaction log space cannot be reused, even if you back up 
the transaction log. If your transaction log ﬁ les keep growing and reach their maximum size limit, or 
your disk runs out of space, the database can no longer process transactions. Then you may want to 
do the following:
Make sure you have plenty of disk space for the transaction log to grow on the principal, 
and be sure to bring the mirror server online before you run out of disk space.
Break the database mirroring session using the following command: ALTER DATABASE 
<database_name> SET PARTNER OFF.
➤
➤

Database Availability Scenarios ❘ 669
The issue here is that when your mirror server becomes operational, you have to reestablish the 
mirroring session by a backup and restore of the principal database and then by reestablishing 
the mirror partnership.
For a large database, the backup and restore operations can take some time; consider the following 
best practices:
Break the database mirroring session using the following command: ALTER DATABASE 
<database_name> SET PARTNER OFF.
Make a note of the time when the mirror went down, and make sure the job that backs up 
the transaction log is running on the principal.
When the mirror comes back online, apply all the transaction logs on the mirror database. 
The ﬁ rst transaction log you should apply is the one you backed up after the mirror went 
down. Be sure to apply the transaction log on the mirror database with the NORECOVERY 
option. That way, you don’t have to back up the entire database and restore it on the mirror. 
Of course, you have to perform other steps to reestablish the mirroring session because the 
session was broken.
Witness Is Lost
If the witness is lost, the database mirroring session continues functioning without interruption, and the 
database will be available. Automatic failover will not happen. When the witness comes back online, it 
automatically joins the mirroring session, of course, as a witness. With SAFETY FULL, if the witness is 
lost and then the mirror or the principal is lost, the database becomes unavailable to the clients.
Mirror and Witness Are Lost
Assuming that you have conﬁ gured the mirroring session with a witness, if the mirror server is 
unavailable, the principal continues and the database is available, but it is running exposed. If the 
witness is also lost, then the principal becomes isolated and cannot serve the clients. Even though 
the principal database is running, it is not available to the clients because it cannot form a quorum. 
You want to consider locating the witness in a third datacenter to prevent a datacenter failover 
bringing down both the partner and the witness. If you try to access the database, you get the 
following message: 
 Msg 955, Level 14, State 1, Line 1
 Database <database_name> is enabled for Database Mirroring, 
but neither the partner
 nor witness server instances are available: the database cannot be opened.
With both the mirror and the witness lost, the only way you can bring the database online to serve 
clients is by breaking the mirroring session with the following command on the principal: 
 ALTER DATABASE <database_name> SET PARTNER OFF
When the mirror becomes available, you can reestablish the mirroring session. To do so, you may 
have to back up and restore the database on the mirror, but if you want to avoid that step, refer 
➤
➤
➤
➤

670  ❘  CHAPTER 19  DATABASE MIRRORING
to the third option in the “Mirror Is Lost” section. When the witness becomes available, it can 
join in again as a witness, but you must reestablish the mirroring session for the mirror ﬁ rst, then 
reestablishing the witness partnership.
MONITORING DATABASE MIRRORING
You can monitor database mirroring in different ways, based on what information you want to track. 
For basic information about the database mirroring state, safety level, and witness status, you can 
use catalog views. Refer to the “Database Mirroring Catalog Views” section earlier in this chapter 
for more details about catalog views. To monitor the performance of database mirroring, SQL Server 
2012 provides a set of System Monitor performance objects. There is also a Database Mirroring 
Monitor GUI available with SQL Server 2012 Management Studio, which you can access if your 
database is mirrored. This section discusses both the key System Monitor counters and the GUI.
Monitoring Using System Monitor
The object SQL Server: Database Mirroring has plenty of counters to monitor database mirroring 
performance. You can use these counters to monitor the database mirroring activities on each 
partner, and the trafﬁ c between them, as shown in Figure 19-7. You can do this for each database 
instance, so if you mirror more than one database on a server, select the database you want to 
monitor from the list box. The key counters are described in the following sections.
FIGURE 19-7

Monitoring Database Mirroring ❘ 671
Counters for the Principal
The following System Monitor counters are used on the principal server: 
Log Bytes Sent/Sec: The rate at which the principal sends transaction log data to the mirror.
Log Send Queue KB: The total kilobytes of the log that have not been sent to the mirror 
server yet. As the transaction log data is sent from the principal to the mirror, the Log Send 
Queue is depleted, growing again as new transactions are recorded into the log buffer on 
the principal.
Transaction Delay: The delay (in milliseconds) spent waiting for commit acknowledgment 
from the mirror. This counter reports the total delay for all the transactions in process 
at the time. You can determine the average delay per transaction by dividing this counter by 
the Transactions/Sec counter. For high-performance mode, this counter is zero because the 
principal doesn’t wait for the mirror to harden the transaction log to disk. 
You can do a simple exercise using the ModifyData.sql script to get a feel for how this 
counter reports on the transaction delay: 
 
1. 
Start the System Monitor, and add this counter on your principal.
 
2. 
On the principal, execute the ModifyData.sql script, and note the average for this 
counter. It should show a value greater than 0.
 
3. 
On the principal, execute this command: 
          ALTER DATABASE AdventureWorks SET SAFETY OFF 
This puts database mirroring in high performance mode.
 
4. 
Execute the ModifyData.sql script again. You should now notice that this counter is 0.
Transaction/Sec: You can ﬁ nd this counter in the SQL Server: Databases object, which 
measures database throughput and shows how many transactions are processed in a second. 
This counter gives you an idea of how fast the log ﬁ le will grow if your mirror is down, the 
mirror state is DISCONNECTED, and you need to expand the log ﬁ le. Be sure to choose the 
database instance you are interested in for this counter.
Log Bytes Flushed/Sec: This counter is under the SQL Server: Databases object, which 
indicates how many bytes are written to disk (log hardening) per second on the principal. 
This is the log-generation rate of your application. These are also the bytes sent to the 
mirror when it is ﬂ ushed to the disk on the principal. In normal operating conditions, the 
Log Bytes Flushed/Sec and Log Bytes Sent/Sec counters should show the same value. If you 
refer to Figure 19-2, the activity labeled 2 happens at the same time, which is exactly what 
the System Monitor indicates.
Counters for the Mirror
The following System Monitor counters are on the mirror server under the object SQL Server: 
Database Mirroring: 
Redo Bytes/Sec: The rate at which log bytes are rolled forward (replayed) to the data pages, 
per second, from the redo queue.
➤
➤
➤
➤
➤
➤

672  ❘  CHAPTER 19  DATABASE MIRRORING
Redo Queue KB: This counter shows the total KB of the transaction log still to be applied 
to the mirror database (rolled forward). You learn later how to calculate the estimated time 
the mirror takes to redo the logs using this counter and the Redo Bytes/Sec counter. The 
failover time depends on how big this queue is and how fast the mirror can empty this 
queue.
Log Bytes Received/Sec: The rate at which log bytes are received from the principal. If the 
mirror can keep up with the principal to minimize the failover time, then ideally the Log 
Bytes Received/Sec and Redo Bytes/Sec counter shows the same average value, which means 
that the Redo Queue KB is zero. Whatever bytes the principal sends are immediately rolled 
forward to the data pages on the mirror, and there is no redo left, so the mirror is ready to 
failover right away.
Counters for the Principal and the Mirror
From a DBA standpoint, you want to know approximately how far the mirror is behind the 
principal; and after the mirror catches up, how long would it take to redo the transaction log so that 
it can failover. To do this, a calculation is required. 
To calculate the estimated time for the mirror to catch up (in seconds) with the principal, you can 
use the Log Send Queue counter from the principal and the Log Bytes Received/Sec counter on the 
mirror. Moreover, you can use the Log Bytes Send/Sec counter on the principal instead of Log Bytes 
Received/Sec on the mirror. Use their average values and calculate as follows: 
 Estimated time to catch up (in seconds) = (Log Send Queue)/(Log Bytes Received
 /sec)
To calculate the estimated time for the mirror to replay the transaction log (redo) to get ready for 
failover, you can use the Redo Queue KB counter (this counter gives you KB value, so convert it 
to bytes) and the Redo Bytes/Sec counter on the mirror. Use their average values and calculate as 
follows: 
 Estimated time to redo (in seconds) = (Redo Queue)/(Redo Bytes/sec)
In addition to the calculation, you can also use the Database Mirroring Monitor that ships with 
SQL Server 2012. This option provides all the same information that a calculation does. 
Monitoring Using Database Mirroring Monitor
The SQL Server 2012 Database Mirroring Monitor will monitor the database mirroring activities. 
This makes the DBA’s life easier. You can access this tool by right-click any user database in Object 
Explorer in SQL Management Studio on any registered SQL Server 2012 server and select Tasks Á 
Launch Database Mirroring Monitor. You can monitor all your mirroring sessions from it.
You have to register the mirrored database in the Database Mirroring Monitor before you can 
use it. To do so, click Action Á Register Mirrored Database, and follow the wizard. You can give 
the wizard either the principal server name or the mirror server name; it will ﬁ gure out the other 
partner. Figure 19-8 shows the Database Mirroring Monitor with a registered mirrored database.
➤
➤

Monitoring Database Mirroring ❘ 673
You can monitor the key counters mentioned in the previous section using this GUI. You can also 
set alerts for these counters to receive an e-mail or take an action if any counter exceeds a set 
threshold, as covered later.
If you set up mirroring using the SQL Server 2012 Management Studio, it creates the SQL job called 
Database Mirroring Monitor Job, which runs every 1 minute by default to refresh these counters. 
This data is stored in the msdb.dbo.dbm_monitor_data table. You can change the job schedule if 
wanted. If you set up database mirroring using the scripts provided here, you can create the SQL job 
using the following command to refresh the counters: 
 sp_dbmmonitoraddmonitoring [ update_period ]
By default, the [ update_period ] is 1 minute. You can specify a value between 1 and 120, in 
minutes. If you don’t create this job, just press F5 when you are at the Database Mirroring Monitor 
screen. It calls the sp_dbmmonitorresults stored procedure to refresh the data (adding a row for 
new readings) in the msdb.dbo.dbm_monitor_data table. Actually, sp_dbmmonitorresults calls 
another stored procedure in the msdb database called sp_dbmmonitorupdate to update the status 
table and calculate the performance matrix displayed in the UI. If you press F5 more than once in 
15 seconds, it won’t refresh the data again in the table.
Look at the Status tab details in Figure 19-9. The Status area is where the server instance names, 
their current role, mirroring state, and witness connection status (if there is a witness) comes from 
the sys.database_mirroring catalog view. If you click the History button, you get a history 
of the mirroring status and other performance counters. The mirroring status and performance 
history is kept for 7 days (168 hours) by default in the msdb.dbo.dbm_monitor_data table. If 
FIGURE 19-8
 
 
 
 

674  ❘  CHAPTER 19  DATABASE MIRRORING
you want to change the retention period, you can use the sp_dbmmonitorchangealert stored 
procedure, like this: 
 EXEC sp_dbmmonitorchangealert AdventureWorks, 5, 8, 1
This example changes the retention period to 8 hours for the AdventureWorks database where 5 is 
the alter_id and 1 is enabled. You can refer to SQL 2012 Books Online for a description of this 
stored procedure. 
FIGURE 19-9
Following are detailed explanations of the different counters that are available in the Status tab.
Principal Log: Unsent Log: This counter provides the same value as the Log Send Queue 
KB counter on the principal. Unsent Log reads the last value, so if you want to compare 
the Performance Monitor and this counter, look at its last value. You can run the script 
ModifyData.sql from earlier in the chapter after suspending the mirroring session using 
the following command: 
 ALTER DATABASE AdventureWorks SET PARTNER SUSPEND
You can see this counter value start to go up.
Principal Log: Oldest Unsent Transaction: This counter gives you the age, in hh:mm:ss 
format, of the oldest unsent transaction waiting in the Send Queue. It indicates that the 
mirror is behind the principal by that amount of time.
Principal Log: Time to Send Log (Estimated): The estimated time the principal instance 
requires to send the log currently in the Send Queue to the mirror server. Because the rate 
➤
➤
➤

Monitoring Database Mirroring ❘ 675
of the incoming transaction can change, this counter can provide an estimate only. This 
counter also provides a rough estimate of the time required to manually failover. If you 
suspend the mirroring, you notice that this counter shows a value of “Inﬁ nite,” which 
means that because you are not sending any transaction logs to the mirror, the mirror never 
catches up.
Principal Log: Current Send Rate: This counter provides the rate at which the transaction 
log is sent to the mirror, in KB/Sec. This is the same as the Performance Monitor counter 
Log Bytes Sent/Sec. The counter Current Send Rate provides the value in KB, whereas the 
counter Log Bytes Sent/Sec provides the value in bytes. When Time to Send Log is inﬁ nite, 
this counter shows a value of 0 KB/Sec because no log is being sent to the mirror server.
Principal Log: Current Rate of New Transaction: The rate at which new transactions are 
coming in per second. This is the same as the Transaction/sec counter in the Database object.
Mirror Log: Unrestored Log: This counter is for the mirror server. It provides the amount 
of log in KB waiting in the Redo Queue yet to be restored. This is the same as the Redo 
Queue KB counter for the mirror server. If this counter is 0, the mirror is keeping up with 
the principal and can failover immediately if required.
Mirror Log: Time to Restore Log: This counter provides an estimate, in minutes, of how 
long the mirror will take to replay the transactions waiting in the Redo Queue. You saw this 
calculation earlier; this is the estimated time that the mirror requires before failover can occur.
Mirror Log: Current Restore Rate: The rate at which the transaction log is restored into the 
mirror database, in KB/Sec.
Mirror Committed Overhead: This counter measures the delay (in milliseconds) spent 
waiting for a commit acknowledgment from the mirror. This counter is the same as the 
Transaction Delay on the principal. It is relevant only in high-safety mode. For high-
performance mode, this counter is zero because the principal does not wait for the mirror to 
harden the transaction log to disk.
Time to Send and Restore All Current Log (Estimated): This counter measures the time 
needed to send and restore all of the log that has been committed at the principal at the 
current time. This time may be less than the sum of the values of the Time to Send Log 
(Estimated) and Time to Restore Log (Estimated) ﬁ elds, because sending and restoring can 
operate in parallel. This estimate does predict the time required to send and restore new 
transactions committed at the principal while working through backlogs in the Send Queue.
Witness Address: The fully qualiﬁ ed domain name of the witness, with the port number 
assigned for that endpoint.
Operating Mode: The operating mode of the database mirroring session. It can be one of 
the following: 
High performance (asynchronous)
High safety without automatic failover (synchronous)
High safety with automatic failover (synchronous)
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

676  ❘  CHAPTER 19  DATABASE MIRRORING
Setting Thresholds on Counters and Sending Alerts
You can set warnings for different mirroring thresholds so that you receive alerts based on the 
threshold you have set. Figure 19-10 shows the Warnings tab of the Database Mirroring Monitor. 
FIGURE 19-10
Following are instructions on how to set thresholds for the different warnings.
 1. 
Start by clicking the Set Thresholds button on the Warnings tab, which opens the dialog 
shown in Figure 19-11. Here, you can set the threshold for counters on both the principal 
and mirror servers individually so that you can either keep the thresholds the same or vary 
them based on your requirements.
 2. 
For this example, select the check box for the ﬁ rst warning, Warn if the Unsent Log Exceeds 
the Threshold, and set the threshold value to 100KB, just for the principal server.
 3. 
Next, under the Alert folder in SQL Server Agent in SQL Server 2012 Management 
Studio, add a new alert. You see a dialog similar to the one shown in Figure 19-12. Type 
the alert name, and select the database name for which you need this alert — in this case, 
AdventureWorks. 
 4. 
SQL Server raises the error when the threshold you set is exceeded. Enter 32042 for the 
error number. Refer to the section “Using Warning Thresholds and Alerts on Mirroring 
Performance Metrics” in SQL 2012 Books Online to get the error numbers for other events. 
 5. 
Now click Response on the left pane and ﬁ ll out the operator information to specify who 
will receive the alert when the threshold is exceeded.

Monitoring Database Mirroring ❘ 677
FIGURE 19-11
FIGURE 19-12


Troubleshooting Database Mirroring ❘ 679
Make sure you have identiﬁ ed the correct fully qualiﬁ ed names of the partners and witness 
(if any) in the ALTER DATABASE command.
The most common types of set up errors have to do with the data mirroring partners not being 
able to network with each other or the mirror database not being ready to accept transactions. For 
example, you may get the following error while setting up database mirroring: 
Database mirroring connection error 4
 ‘An error occurred while receiving data: ‘64
 The specified network name is no longer available.)’.’ 
 for ‘TCP://yourMirrorServer:5023’. 
 Error: 1443, Severity: 16, State: 2.
This error could indicate that the ﬁ rewall on the mirror server or the principal server is blocking the 
connection on the speciﬁ ed port. Check whether the ﬁ rewall is blocking that communication port.
Finally, a mirror database that is not in the correct state to accept transactions can also produce an 
error while setting up database mirroring on the principal server: 
Msg 1412, Level 16, State 0, Line 1 The remote copy of database “AdventureWorks” has 
not been rolled forward 
 to a point in time that is encompassed in the local copy 
of the database log.
This indicates that the database on the mirror server is not rolled forward enough to establish the 
mirroring session. You may have backed up some transaction logs while backing up and restoring 
the database, so you have to restore these transaction logs to the mirror server with the NORECOVERY 
option to synchronize the mirror with the principal. See the RestoreLogOnMirror.sql script for an 
example.
Troubleshooting Runtime Errors
If your database mirroring setup is done correctly but you get errors afterward, the ﬁ rst thing you 
want to look at is the sys.database_mirroring catalog view. Check the status of the mirroring 
state. If it is SUSPENDED, check the SQL error log for more details. You may have added a data ﬁ le 
or log ﬁ le on the principal for which you do not have the exact same path on the mirror, which will 
cause a Redo error to occur on the mirror, causing the database session to be suspended. You see an 
error similar to the following in the mirror server error log: 
 Error: 5123, Severity: 16, State: 1.
 CREATE FILE encountered operating system error 3(The system cannot find the path 
 specified.) while attempting to open or create the physical file
 ‘<your folder>\AdventureWorks_1.ndf’.
If you get this error, perform the following steps: 
 1. 
Create the folder on the mirror and then resume the mirroring session using the following 
command: 
 ALTER DATABASE AdventureWorks SET PARTNER RESUME
➤

680  ❘  CHAPTER 19  DATABASE MIRRORING
 2. 
If you have added a data ﬁ le and you do not have that drive on the mirror, delete the data 
ﬁ le from the principal. If pages are allocated on the ﬁ le, empty the data ﬁ le before you can 
delete it. Then resume the mirroring session again.
If you cannot connect to the principal database even though your server is online, it is most likely 
because safety is set to FULL, and the principal server cannot form a quorum because both the 
witness and mirror are lost. This can happen, for example, if your system is in high-safety mode 
with the witness, and the mirror and witness have become disconnected from the principal. You can 
force the mirror server to recover, using the following command on the mirror: 
 ALTER DATABASE AdventureWorks SET PARTNER FORCE_SERVICE_ALLOW_DATA_LOSS
After that, because both the old principal and the witness are not available, the new principal 
cannot serve the database, so turn the SAFETY to OFF using the following command: 
 ALTER DATABASE AdventureWorks SET PARTNER SAFETY OFF
Ensure that there is sufﬁ cient disk space on the mirror for both redo (free space on the data drives) 
and log hardening (free space on the log drive).
Automatic Page Repair
On SQL Server Enterprise and Developer Editions, database mirroring can automatically correct 
error numbers 823 and 824, which are caused by data cyclic redundancy check (CRC) errors 
raised while the server is attempting to read a page. When a mirror partner cannot read a page, it 
asynchronously requests a copy from its partner; if the page requested is successfully applied, the 
page is repaired and the error is resolved. During the repair, the actual data is preserved. It does not 
repair control type pages, such as allocation pages. The page repair operation varies depending on 
whether the principal or mirror is requesting the page.
Principal Request Page
The principal identiﬁ es a page read error, marks the page with an 829 error (restore pending), 
inserts a row into the suspect_pages table in MSDB with the error status, and requests the page 
from the mirror. If the mirror is successful in reading the page, it returns the page to the principal 
who applies it. After the page is repaired, the principal marks the page as restored — that is, 
event_type = 5 in the suspect_pages table. Then any deferred transactions are resolved.
Mirror Request Page
The mirror identiﬁ es a page read error, marks the page with an 829 error (restore pending), and 
inserts a row into the suspect_pages table in MSDB with the error status. It requests the page 
from the principal and sets the mirror session in a SUSPENDED state. If the principal is successful in 
reading the page, it returns the page to the mirror. After the page is applied at the mirror, the mirror 
resumes the data mirroring session and marks the page as restored in the suspect_pages table — 
that is, event_type = 5.

Preparing the Mirror Server for Failover ❘ 681
The sys.dm_db_mirroring_auto_page_repair DMV view displays corrupted pages in the data 
mirroring environment. This catalog view returns a maximum of 100 rows per database of every 
automatic page repair attempt; as it reaches this maximum, the next entry replaces the oldest entry. 
Use the following command to execute the catalog view: 
 SELECT * FROM sys.dm_db_mirroring_auto_page_repair
This will return six columns. One column in particular, error_type column, indicates errors 
encountered that are to be corrected. Additionally, the page_status column indicates where the 
program is in the process of repairing that page. The rest of the columns are self-explanatory.
PREPARING THE MIRROR SERVER FOR FAILOVER
When you set up mirroring, your intentions are clear that in the event of failover, the mirror 
takes on the full load. For the mirror to be fully functional as a principal, you have to do some 
conﬁ gurations on the mirror server. Database mirroring is a database-to-database failover solution 
only; the entire SQL Server instance is not mirrored. If you want to implement full SQL Server 
instance failover, then you should consider Windows failover clustering, discussed in Chapter 16, 
“Clustering SQL Server 2012.”
When preparing your mirror server for failover, the ﬁ rst place to start is with your hardware and 
software. 
Hardware, Software, and Server Conﬁ guration
Your mirror server hardware should be identical (CPU, memory, storage, and network capacity) to 
that of the principal if you want your mirror server to handle the same load as your principal. You 
may argue that if your principal is a 16-core, 64-bit server with 32GB of RAM, having identical 
mirror hardware is a costly solution if your principal is not going down often and such expensive 
hardware is sitting idle. If your application is not critical, then you may want to have a smaller 
server just for failover and taking the load for some time, but it is arguable that if the application is 
not that critical, you do not need to have such extensive hardware on the primary either.
Moreover, with such huge hardware, the process on the server must be heavy, so if you failover, 
your mirror should handle that load even if it is for a short time. In addition, you have to consider 
the business cost versus the hardware cost and then make the decision. You can also use your mirror 
server for noncritical work so that even though it is a mirror, it can be used for some other work. 
Of course, you have to plan that out properly to make good use of your mirror server. Provided 
that your database server performance characteristics can be supported within the virtual server 
maximum performance capacity, data mirroring can be deployed on virtual servers.
Make sure that you have the same operating system version, service packs, and patches on both 
servers. During a rolling upgrade (as discussed in the “Database Availability During Planned 
Downtime” section), service packs and patch levels can be temporarily different.
You need to have the same edition of SQL Server on both partners. If you use a witness, then you 
don’t need the same edition on it. You can use a smaller server for the witness because it doesn’t 

682  ❘  CHAPTER 19  DATABASE MIRRORING
carry any actual load — it is used only to form a quorum. Of course, the availability of the witness 
server is critical for automatic failover. Refer to the table in the section “Database Mirroring and 
SQL Server 2012 Editions” for more details on which editions support which database mirroring 
features.
Make sure you have an identical directory structure for the SQL Server install and database ﬁ les on 
both the partners. If you add a database ﬁ le to a volume/directory and that volume/directory does 
not exist on the mirror, the mirroring session will be suspended immediately.
On both principal and mirror, make sure that all the SQL Server conﬁ gurations are identical (for 
example, tempdb size, trace ﬂ ags, startup parameters, memory settings, and degree of parallelism).
All SQL Server logins on the principal must also be present on the mirror server; otherwise, your 
application cannot connect if a failover occurs. You can use SQL Server 2012 Integration Services, 
with the “Transfer Logins task, to copy logins and passwords from one server to another. (Refer to 
the section “Managing Changing Roles” in Chapter 18, “SQL Server 2012 Log Shipping,” for more 
information.) Moreover, you still need to set the database permission for these logins. If you transfer 
these logins to a different domain, then you have to match the security identiﬁ er (SID) — that is, the 
unique name that Windows uses to identify a speciﬁ c user name or group.
On the principal, many other objects may exist and be needed for that application(for example, 
SQL jobs, SQL Server Integration Services packages, linked server deﬁ nitions, maintenance plans, 
supported databases, SQL Mail or Database Mail settings, and DTC settings). You also have to 
transfer all these objects to the mirror server.
If you use SQL Server authentication, you have to resolve the logins on the new principal server 
after failover. You can use the sp_change_users_login stored procedure to resolve these logins. 
Be sure to have a process in place so that when you make any changes to any conﬁ guration on the 
principal server, you repeat or transfer the changes on the mirror server.
After you set up your mirror, failover the database and let your application run for some time on the 
new principal because that is the best way to ensure that all the settings are correct. Try to schedule 
this task during a slow time of the day, and deliver the proper communication procedures prior to 
testing failover.
Database Availability During Planned Downtime
This section describes the steps required to do a rolling upgrade technique to perform a software 
and hardware upgrade while keeping the database online for applications. The steps to perform a 
rolling upgrade vary based on database mirroring session transaction safety conﬁ guration which is: 
SAFETY FULL or SAFETY OFF.
Safety Full Rolling upgrade
Assuming that you have conﬁ gured the mirroring session with SAFETY FULL, if you have to perform 
software and hardware upgrades, perform the following steps in order: 
 1. 
Perform the hardware and software changes on the mirror server ﬁ rst. If you have to restart 
the SQL Server 2012 or the server itself, you can do so. As soon as the server comes back 

Preparing the Mirror Server for Failover ❘ 683
up again, the mirroring session will be established automatically and the mirror will start 
synchronizing with the principal. The principal is exposed while the mirror database is 
down, so if you have a witness conﬁ gured, make sure that it is available during this time; 
otherwise, the principal will be running in isolation and cannot serve the database because 
it cannot form a quorum.
 2. 
After the mirror is synchronized with the principal, you can failover using the following 
command: 
ALTER DATABASE <database_name> SET PARTNER FAILOVER
The application now connects to the new principal. The “Client Redirection to the Mirror” 
section covers application redirection when the database is mirrored. Open and in-ﬂ ight 
transactions during failover will be rolled back at this point. If that is not tolerable for 
your application, you can stop the application for the brief failover moment and restart the 
application after failover completes.
 3. 
Now perform the hardware or software upgrade on your old principal server. When you 
ﬁ nish with upgrades and the database becomes available on the old principal, it assumes 
the mirror role, the database mirroring session is automatically established, and it starts 
synchronizing.
 4. 
If you have a witness set up, perform the hardware or software upgrade on that server.
 5. 
At this point, your old principal acts as a mirror. You can fail back to your old principal 
now that all your upgrades are done. If you have identical hardware on the new principal, 
you may leave it as is so that you don’t have to stop the application momentarily; but if your 
hardware is not identical, consider switching back to your original principal.
Safety Of  Rolling Upgrade
If you have conﬁ gured the database mirroring with SAFETY OFF, you can still use the rolling 
upgrade technique by following these steps: 
 1. 
Perform the hardware and software changes on the mirror ﬁ rst. See the preceding section 
for more details.
 2. 
On the principal server, change the SAFETY to FULL using this command: 
ALTER DATABASE <database_name> SET SAFETY FULL
Plan this activity during off-peak hours to reduce the mirror server synchronization time.
 3. 
After the mirror is synchronized, you can perform the failover to the mirror.
 4. 
Perform the hardware and software upgrade on the old principal. After the old principal 
comes back up, it assumes the mirror role and starts synchronizing with the new principal.
 5. 
When synchronized, you can fail back to your original principal.
If you use mirroring just to make a redundant copy of your database, you may not want to failover 
for planned downtime because you may not have properly conﬁ gured all the other settings on the 


Mirroring Multiple Databases ❘ 685
You can write the connection string in several ways, but here is one example, specifying ServerA as 
the principal, ServerB as the mirror, and AdventureWorks as the database name: 
 “Data Source=ServerA;Failover Partner=ServerB;Initial Catalog=AdventureWorks;
 Integrated Security=True;”
The failover partner in the connection string is used as an alternative server name if the connection 
to the initial principal server fails. If the connection to the initial principal server succeeds, the 
failover partner name is not used, but the driver stores the failover partner name that it retrieves 
from the principal server in the client-side cache.
Assume a client is successfully connected to the principal, and a database mirroring failover 
(automatic, manual, or forced) occurs. The next time the application attempts to use the connection, 
the ADO.NET or SQL Native Client driver detects that the connection to the old principal has failed 
and automatically retries connecting to the new principal as speciﬁ ed in the failover partner name. 
If successful, and a new mirror server is speciﬁ ed for the database mirroring session by the new 
principal, the driver retrieves the new partner failover server name and places it in its client cache. If 
the client cannot connect to the alternative server, the driver tries each server alternatively until the 
login timeout period is reached.
The advantage of using the database mirroring client support built into ADO.NET and the SQL 
Native Client driver is that you do not need to recode the application, or place special codes in the 
application, to handle a database mirroring failover.
If you do not use the ADO.NET or SQL Native Client automatic redirection, you can use other 
techniques that enable your application to failover. For example, you could use network load 
balancing (NLB) to redirect connections from one server to another while remaining transparent 
to the client application that is using the NLB virtual server name. On failover, you can reconﬁ gure 
NLB to divert all client applications using the database to the new principal by tracking the 
mirroring state change event to; then change the NLB conﬁ guration to divert the client applications 
to the new principal server. Additionally, you could consider writing your own redirection code and 
retry logic. Moreover, the Domain Name System (DNS) service provides a name-to-IP resolution 
that can be used to redirect clients to the new principal server. Provided that the administrator has 
access to the DNS server either by script or by using the Windows DNS management tool to modify 
the IP address to redirect client applications during a failover, DNS acts as the cross-reference 
for the client applications, as they will continue to connect to the same name; after the DNS 
modiﬁ cation, it redirects the database request to the new principal server.
MIRRORING MULTIPLE DATABASES
As discussed previously, it is possible to mirror multiple databases on the same server. You can 
either use the same server for a mirror partner or use a different mirror server for each database. 
I recommend using the same mirror server for mirroring multiple databases from a principal server. 
That way, maintenance is reduced, and the system is less complex; otherwise, you have to perform 
all the steps mentioned in “Hardware, Software, and Server Conﬁ guration” on each mirror server.


Database Mirroring and Other High-Availability Solutions ❘ 687
AlwaysOn: Server 2012 has Availability Groups in SQL Server 2012. Availability Groups is 
discussed in detail in Chapter 25, “AlwaysOn Availability Groups.”
Now see how database mirroring compares with these other technologies.
Database Mirroring versus Clustering
Obviously, the biggest difference between database mirroring and a Window failover cluster 
solution is the level at which each provides redundancy. Database mirroring provides protection 
at the database level, as you have seen, whereas a cluster solution provides protection at the SQL 
Server instance level.
As discussed in the section “Mirroring Multiple Databases,” if your application requires multiple 
database dependencies, clustering is probably a better solution than mirroring. If you need 
to provide availability for one database at a time, mirroring is a good solution and has many 
advantages compared to clustering — for example: ease of conﬁ guration.
Unlike clustering, database mirroring does not require shared storage hardware and does not have a 
single failure point with the shared storage. Database mirroring brings the standby database online 
faster than any other SQL Server high-availability technology and works well in ADO.NET and 
SQL Native Access Client for client-side redirect.
Another important difference is that in database mirroring, the principal and mirror servers are 
separate SQL Server instances with distinct names, whereas a SQL Server instance on a cluster gets 
one virtual server name and IP address that remains the same no matter which node of the cluster 
hosts that SQL Server instance.
You can use database mirroring within a cluster to create a hot standby for a cluster SQL Server 
2012 database. If you do, be aware that because a cluster failover is longer than the timeout value 
on database mirroring, a high-availability mode mirroring session will react to a cluster failover as 
a failure of the principal server. It would then put the cluster node into a mirroring state. You can 
increase the database mirroring timeout value by using following command: 
 ALTER DATABASE <database_name> SET PARTNER TIMEOUT <integer_value_in_seconds>
Database Mirroring versus Transactional Replication
The common process between database mirroring and transactional replication is reading the 
transaction log on the originating server. Although the synchronization mechanism is different, 
database mirroring directly initiates I/O to the transaction log ﬁ le to transfer the log records.
Transactional replication can be used with more than one subscriber, whereas database mirroring 
is a one-database-to-one-database solution. You can read nearly real-time data on the subscriber 
database, whereas you cannot read data on the mirrored database unless you create a database 
snapshot, which is a static, point-in-time snapshot of the database.
Database Mirroring versus Log Shipping
Database mirroring and log shipping both rely on moving the transaction log and restoring it. In 
database mirroring, the mirror database is constantly in a recovering state, which is why you cannot 
➤

688  ❘  CHAPTER 19  DATABASE MIRRORING
query the mirrored database. In log shipping, the database is in standby mode, so you can query 
the database if the log is not being restored at the same time. In addition, log shipping supports the 
bulk-logged recovery model, whereas mirroring supports only the full recovery model.
If your application relies on multiple databases for its operation, you may want to consider log 
shipping for failover. Sometimes it is a bit tedious to set up log shipping going the other way once a 
failover has occurred, whereas mirroring is easy in that aspect.
In the high-performance mode, there is a potential for data loss if the principal fails and the mirror 
is recovered using a forced failover recovery. If you are log shipping the old principal, and the 
transaction log ﬁ le of the old principal is undamaged, you can make a “tail of the log” backup from 
the principal to get the last set of log records from the transaction log. If the standby log-shipping 
database has had every other transaction log backup applied to it, you can then apply the “tail of 
the log” backup to the standby server and not lose any of the old principal’s data. You can then 
compare the data in the log-shipping standby server with the remote database and potentially copy 
missing data to the remote server.
You can use log shipping and mirroring together if you like. You can use log shipping to ship the log 
to a remote site for disaster recovery and have a database-mirroring, high-availability conﬁ guration 
locally.
Database Mirroring Versus Availability Groups
Database mirroring and AlwaysOn both rely on moving the transaction log records and restoring it. 
AlwaysOn is a new high-availability feature of SQL Server 2012 with similar functionality to data 
mirroring but has the following additional capabilities:
AlwaysOn relies on Windows Failover clustering for a virtual IP/virtual name for client 
connection. But unlike Windows Failover clustering, it doesn’t require a shared disk 
resource but instead uses nonshared disks such as data mirroring.
AlwaysOn implements Availability Groups that can contain one or more databases to 
failover as a single failover group; data mirroring failover is a single database.
AlwaysOn can support up to ﬁ ve failover partners in a combination of synchronous or 
asynchronous modes; instead data mirroring supports two partners. 
AlwaysOn supports read-only replica databases kept in sync that can be used for reporting 
or database backup while in data mirroring; the mirrored database is only accessible using a 
database snapshot.
MIRRORING EVENT LISTENER SETUP
This section provides steps you can use to take some action when the database mirroring 
session changes state (for example, from disconnected to synchronizing or from synchronized to 
suspended). You can perform the following steps to conﬁ gure an alert for mirroring state change 
events and take some action on these events.
➤
➤
➤
➤

Mirroring Event Listener Setup ❘ 689
 1. 
Right-click the Alert folder under SQL Server Agent in SQL Server 2012 Management 
Studio and select New Alert. The dialog shown in Figure 19-13 appears.
FIGURE 19-13
 2. 
Type the name of the event, and select the event type WMI Event Alert from the drop-
down menu. The namespace is automatically ﬁ lled out for you. In the query ﬁ eld, type the 
following query: 
 SELECT * FROM DATABASE_MIRRORING_STATE_CHANGE
 3. 
In this example, the alert is ﬁ red for all database mirroring state change events for all 
the databases mirrored on this server. If you want to be alerted about a speciﬁ c database 
mirroring state change event for a speciﬁ c database, add a WHERE clause to the SELECT 
statement: 
 SELECT * FROM DATABASE_MIRRORING_STATE_CHANGE WHERE State = 8 AND
 DatabaseName = ‘AdventureWorks’
This statement listens only for the “automatic failover” state change (state = 8) for the 
AdventureWorks database. Table 19-4 lists all the database mirroring state change events, 
so that you can use it to build the WHERE clause to listen to speciﬁ c events. 

690  ❘  CHAPTER 19  DATABASE MIRRORING
TABLE 19.4: DATABASE_MIRRORING_STATE_CHANGE State
STATE
NAME
DESCRIPTION
0
Null Notiﬁ cation
Occurs brieﬂ y when a mirroring session is started.
1
Synchronized 
Principal with Witness
Occurs on the principal when the principal and mirror are 
connected and synchronized and the principal and witness 
are connected. For a mirroring conﬁ guration with a witness, 
this is the normal operating state.
2
Synchronized 
Principal without 
Witness
Occurs on the principal when the principal and mirror are 
connected and synchronized but the principal does not have 
a connection to the witness. For a mirroring conﬁ guration 
without a witness, this is the normal operating state.
3
Synchronized Mirror 
with Witness
Occurs on the mirror when the principal and mirror are 
connected and synchronized and the mirror and witness are 
connected. For a mirroring conﬁ guration with a witness, this is 
the normal operating state.
4
Synchronized Mirror 
without Witness
This state occurs on the mirror when the principal and mirror 
are connected and synchronized but the mirror does not have 
a connection to the witness. For a mirroring conﬁ guration 
without a witness, this is the normal operating state.
5
Connection with 
Principal Lost
Occurs on the mirror server instance when it cannot connect 
to the principal.
6
Connection with 
Mirror Lost
Occurs on the principal server instance when it cannot 
connect to the mirror.
7
Manual Failover
Occurs on the principal server instance when the user fails 
over manually from the principal, or on the mirror server 
instance when a forced service is executed at the mirror.
8
Automatic Failover
Occurs on the mirror server instance when the operating 
mode is high safety with automatic failover (synchronous) and 
the mirror and witness server instances cannot connect to the 
principal server instance.
9
Mirroring Suspended
Occurs on either partner instance when the user suspends 
(pauses) the mirroring session, or when the mirror server 
instance encounters an error. It also occurs on the mirror 
server instance following a forced service command. When the 
mirror comes online as the principal, mirroring is automatically 
suspended.

Mirroring Event Listener Setup ❘ 691
STATE
NAME
DESCRIPTION
10
No Quorum
If a witness is conﬁ gured, this state occurs on the principal or 
mirror server instance when it cannot connect to its partner or 
to the witness server instance.
11
Synchronizing Mirror
Occurs on the mirror server instance when there is a backlog 
of unsent log. The status of the session is Synchronizing.
12
Principal Running 
Exposed
Occurs on the principal server instance when the operating 
mode is high safety (synchronous) and the principal cannot 
connect to the mirror server instance.
13
Synchronizing 
Principal
Occurs on the principal server instance when there is 
a backlog of unsent log. The status of the session is 
Synchronizing.
 4. 
Now select the Response page, as shown in Figure 19-14. This dialog enables you to specify 
what you want SQL Server 2012 to do if the event occurs. In Figure 19-14, you want to 
execute the SQL job. 
FIGURE 19-14

692  ❘  CHAPTER 19  DATABASE MIRRORING
 5. 
In the TestEventChange SQL job, you can actually add the following script to store the 
history of database mirroring state change events in a table. Create this table ﬁ rst in some 
other database, such as msdb: 
 CREATE TABLE dbo.MirroringStateChanges
 (
  EventTime varchar(max) NULL
 ,EventDescription varchar(max) NULL
 ,NewState int NULL
 ,DatabaseName varchar(max) NULL
 )
 6. 
Add the following script as a job step to insert into this table: 
 INSERT INTO dbo.MirroringStateChanges
 (
  [EventTime]
 ,[EventDescription]
 ,[NewState]
 ,[DatabaseName]
 )
 VALUES
 (
  $(ESCAPE_NONE(WMI(StartTime)))
 ,$(ESCAPE_NONE(WMI(TextData)))
 ,$(ESCAPE_NONE(WMI(State)))
 ,$(ESCAPE_NONE(WMI(DatabaseName)))
  )
You can change the database mirroring state using the ALTER DATABASE command to test this alert. 
Additionally, the database mirroring state change is logged to Event Viewer under the Application 
events as something like the following: 
 The mirrored database “AdventureWorks” is changing roles from “MIRROR” to
 “PRINCIPAL” due to Failover from partner
DATABASE SNAPSHOTS
As you have probably ﬁ gured out by now, the mirror database is in NORECOVERY mode, so you 
cannot query the mirror database. If you want to read data from the mirror database to be used 
for reporting, SQL Server 2012 (Enterprise Edition and Developer Edition) has a feature called 
Database Snapshots, ﬁ rst introduced in SQL Server 2005. A database snapshot is a point-in-time, 
read-only, static view of a database (the source database). This feature comes in handy for reading 
the mirror database. Multiple database snapshots can exist but they always reside on the same SQL 
Server instance as the database. Each database snapshot is transactionally consistent with the source 
database at the point in time of the snapshot’s creation. A snapshot persists until it is explicitly 
dropped by the database owner.
Using this feature, you can create a snapshot on the mirror database. Then, you can read the 
database snapshot as you would read any other database. The database snapshot operates at a 

Summary ❘ 693
data-page level. Before a page of the source database is modiﬁ ed for the ﬁ rst time after the database 
snapshot, the original page is copied from the source database to the snapshot ﬁ le. This process is 
called a copy-on-write operation. The snapshot stores the original page, preserving the data records 
as they existed when the snapshot was created. Subsequent updates to records in a modiﬁ ed page 
on the source database do not affect the data contents of the snapshot. In this way, the snapshot 
preserves the original pages for all data records that have ever been modiﬁ ed since the snapshot was 
taken. Even if you modify the source data, the snapshot will still have the same data from the time 
when it was created. (See the topic “Database Mirroring and Database Snapshots” in SQL Server 
2012 Books Online for more information.)
The following example shows how to create a snapshot on the AdventureWorks database: 
 CREATE DATABASE AdventureWorks_Snapshot ON
 (NAME = AdventureWorks, FILENAME = ‘<your folder>\ADW_Mirroring_
 snapshot_Data1.SS’)
 AS SNAPSHOT OF AdventureWorks
Because new data changes will be continuous on the mirrored database, if you want to read more 
recent data not in the database snapshot after you have created it, you need to drop the database 
snapshot and re-create it. You can drop the snapshot in the same manner as you would drop a 
database: 
 DROP DATABASE AdventureWorks_Snapshot
Generating a database snapshot has some performance impact on the mirror server, so evaluate the 
impact if you want to create many snapshots on multiple databases on a server. Most important, 
from a database mirroring perspective, having too many snapshots on a mirror database can slow 
down the redo and cause the database to fall more and more behind the principal, potentially 
resulting in huge failover times.
In addition, prepare an area of disk space as big as the size of the source database because as data 
changes on the source database, the snapshot will start copying the original pages to the snapshot 
ﬁ le, and it will start growing. Additionally, you may want to evaluate SQL Server replication as an 
alternative reporting solution.
SUMMARY
Database mirroring provides a database redundancy solution using the log-transfer mechanism. The 
transaction log records are sent to the mirror transaction log as soon as the log buffer is written to 
the disk on the principal. Mirroring can be conﬁ gured in either high-performance mode or high-safety 
mode. In high-safety mode, if the principal fails, the mirror server automatically becomes a new 
principal and recovers its database. Understanding application behavior in terms of log-generation 
rate, number of concurrent connections, and size of transactions is important to achieve the best 
performance. Network bandwidth plays an important role in a database mirroring environment. When 
used with a high-bandwidth and low-latency network, database mirroring can provide a reliable, 
high-availability solution against planned and unplanned downtime. 



A Tour of Integration Services ❘ 697
volume and complex enrichment and transformation situations such as these is restartability. SSIS 
includes checkpoints to handle rerunning a package from a task or container within the control ﬂ ow 
so that you can elegantly handle various types of errors that may occur during these complex data-
loading scenarios.
Also important in data warehouse loads is the ability to source a particular destination from many 
different tables or ﬁ les. In the database world, this is referred to as denormalization, and SSIS 
packages can easily merge data into a single dataset and load the destination table in a single process 
without the need to stage or land the data at each step of the process.
You often require the management or partitioning of history within your data warehouses to review 
the state of activity at a certain point in time. This history management creates complex updating 
scenarios, and SSIS handles this with the assistance of the Slowly Changing Dimension Wizard. 
This wizard dynamically creates and conﬁ gures a set of data transformation tasks used to manage 
inserting and updating records, updating related records, and adding new columns to tables to 
support this history management.
Often, businesses receive data from outside of their systems and need to perform data-quality 
routines to standardize and clean the data before loading it into their systems. SSIS can handle 
most data situations from heterogeneous databases or ﬂ at ﬁ les. This is commonly the case when 
different areas of the business use different standards and formats for the information or when 
the data is being purchased, such as with address data. Sometimes the data formats are different 
because the platforms from which they originate differ from the intended destination. In these cases, 
SSIS includes a rich set of data-transformation tasks to perform a wide range of data-cleaning, 
converting, and enriching functions. You can replace values or get descriptions from code values by 
using exact or fuzzy lookups within SSIS. Identifying records that may be duplicates by using SSIS 
grouping transformations helps to successfully remove them before loading the destination.
The ability to dynamically adjust the data transformations being performed is a common scenario 
within businesses. Often, data needs to be handled differently based on certain values it may 
contain or even based upon the summary or count of values in a given set of records. SSIS includes 
a rich set of transformations that are useful for splitting or merging data based upon data values, 
applying different aggregations or calculations based on different parts of a dataset, and loading 
different parts of the data into different locations. SSIS containers speciﬁ cally support evaluating 
expressions, enumerating across a set of information, and performing workﬂ ow tasks based on 
results of the data values.
Lastly, you commonly have operational administrative functions that require automation. SSIS 
includes an entire set of tasks devoted to these administrative functions. You can use tasks 
speciﬁ cally designed to copy SQL Server objects or facilitate the bulk loading of data. You also have 
access in SSIS to a SQL Management Objects (SMO) enumerator to perform looping across your 
servers to perform administrative operations on each server in your environment. When complete, 
you can also schedule all your SSIS packages and solutions using SQL Server Agent jobs.
The Main Parts of Integration Services
In SQL Server 2012, Integration Services has introduced two models that can impact your team’s 
development and you as an administrator: package deployment model and project deployment 

698  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
model. The decision on which model to use is made when you ﬁ rst create the project and can 
always be changed later but should not be changed lightly. While you can use both models in 
your environment interchangeably, you should try to guide your development towards the project 
deployment model since it turns on all the new features of 2012. 
Package deployment model was the only model a DBA and developer had prior to SQL Server 
2012. This model has you deploy a package by itself, and the package’s project is just an arbitrary 
container that doesn’t do a lot. Packages run in this model can be deployed to the MSDB database 
or the server’s ﬁ le system. There is also an SSIS service in this model that monitors the execution of 
packages. Packages can be conﬁ gured externally at runtime with conﬁ guration ﬁ les or entries in a 
conﬁ guration table.
Project deployment model is a new model to SQL Server 2012 in which the project that contains 
the packages is more important in previous SQL Server editions. In this model, parameters can be 
passed into the project or package to reconﬁ gure the package at runtime. Projects take a more vital 
role in this model because you deploy the entire project at a time, and you cannot deploy individual 
packages. When you deploy packages, they are added to the SSIS catalog database and can be 
executed in T-SQL or through PowerShell.  
Moving forward in SQL Server 2012, you should use the new project deployment model because it 
offers a more robust conﬁ guration, logging, and management infrastructure. The package deployment 
model remains available for backward compatibility but should not be used as your ﬁ rst choice.
The package deployment model contains many important components including the service, 
runtime engine and components, object model, and dataﬂ ow engine and components. The following 
sections cover each of these components. In the package deployment model, the components have 
not changed in SQL Server 2012.
Integration Services Service
The component of architecture within Integration Services, responsible for monitoring packages as 
they execute and managing the storage of packages, is the SSIS service. Its primary job is to cache the 
data providers, monitor which packages are being executed, and monitor which packages are stored 
in the package store. This service is used only in the package deployment model. 
Integration Services Runtime Engine and Runtime Components
The SSIS runtime engine works across both deployment models and is responsible for saving the 
layout and design of the packages, running the packages, and providing support for all additional 
package functionality such as transactions, breakpoints, conﬁ guration, connections, event handling, 
and logging. The speciﬁ c executables that make up this engine include packages, containers, and 
tasks. You can ﬁ nd three default constraints within SSIS: success, completion, and failure.
Integration Services Object Model
The managed application programming interface (API) used to access SSIS tools, command-line 
utilities, and custom applications in the SSIS object model. Although this object model isn’t discussed 
in detail, it is a major component of Integration Services.

Administration of the Integration Services Service ❘ 699
Integration Services Data Flow Engine and Data Flow Components
Within an SSIS package’s control ﬂ ow, a Data Flow Task creates instances of the data ﬂ ow engine. 
This engine is responsible for providing the in-memory data movement from sources to destinations. 
In addition, this engine performs the requested transformations to enrich the data for the purposes 
you specify. Three primary components make up the data ﬂ ow engine: sources, transformations, and 
destinations. The sources provide connectivity to, and extract data from, a wide range of sources 
such as database tables or views, ﬁ les, spreadsheets, and even XML ﬁ les. The destinations permit 
the insert, update, and deletion of information on a similar wide range of destinations. Lastly, the 
transformations enable you to modify the source data before loading it into a destination using 
capabilities such as lookups, merging, pivoting, splitting, converting, and deriving information.
Project Management and Change Control
One of the areas needing an entirely different mindset from the previous version of SQL Server 
involves how DBAs interact with the development team. In BI, the line between development and 
DBA has blurred and the two must now work closer together. The shared view of development 
by administrators and developers alike is enacted through the SQL Server Data Tools (SSDT), 
previously named Business Intelligence Development Studio (BIDS). SSDT is a Visual Studio shell 
that optionally installs when you install SQL Server. For Integration Services, SSIS solutions and 
projects are created in the SSDT environment. Generally, the conﬁ guration of the SSDT solutions 
and projects is handled by the developers; however, administrators are called upon to help conﬁ gure 
various aspects of these solutions. The administration and management of Integration Services 
is primarily performed within SQL Server Management Studio. Often, moving the Integration 
Services solutions from environment to environment means changing dynamic information within 
the package and setting up any information referenced by the packages. Examples of these elements 
include Package Conﬁ guration settings, referenced XML or conﬁ guration ﬁ les, and solution data 
sources, which are all covered later in this chapter.
When a developer clicks Save or executes the package in SSDT, the old version of the package 
is immediately overwritten in the ﬁ le system. To remedy this, you should integrate the SSIS 
development environment into a source control system — such as Visual SourceSafe, for example. 
After such integration, when a package is saved you can always roll back to an earlier release of the 
package. You can use any type of source control system that integrates with Visual Studio.
ADMINISTRATION OF THE INTEGRATION SERVICES SERVICE
Now that you have a better understanding of the parts of Integration Services, you can take 
a look at the various administrative aspects of Integration Services, including the details needed 
to become comfortable working with the components. You start with a review of the Integration 
Services service and then look at various conﬁ guration elements of the service. Next, you look 
at how you can adjust properties of the SSIS service using either the Windows Services Snap-In or 
the SQL Server Conﬁ guration Manager. Understanding how you can modify Windows Firewall 
follows, and then you look at the management and conﬁ guration of event logs and performance 
monitoring.
 
 
 
 

700  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
An Overview of the Integration Services Service
The Integration Services service is a Windows service used to manage SSIS packages deployed in 
the package deployment model. Accessed through SQL Server Management Studio, it provides the 
following management capabilities: 
Starting and stopping local and remote packages
Monitoring local and remote packages
Importing and exporting packages from different sources
Managing the package store
Customizing storage folders
Stopping running packages when service stops
Viewing the Windows Event Log
Connecting to multiple SSIS server instances
To be clear, you don’t need this service for designing or executing packages. The primary purpose 
of this service is to manage packages within Management Studio. One side beneﬁ t to having the 
service running is that the SSIS Designer in SSDT can use the service to cache the objects used in the 
designer, thus enhancing the designer’s performance.
Conﬁ guration
The conﬁ guration of the Integration Services service includes viewing and possibly modifying the 
XML ﬁ le responsible for the runtime conﬁ guration of the service, setting service properties using 
either the Windows Services Snap-In or SQL Server Conﬁ guration Manager, and, potentially, 
conﬁ guring Windows Firewall to permit access by Integration Services.
XML Conﬁ guration File
The MsDtsSrvr.ini.xml ﬁ le responsible for the conﬁ guration of the Integration Services service 
is located in <SQL Server Drive>\Program Files\Microsoft SQL Server\110\DTS\Binn 
by default. You can also move this ﬁ le to a new location by changing the HKEY_LOCAL_MACHINE\
SOFTWARE\Microsoft\ Microsoft SQL Server\100\SSIS\ServiceConfigFile Registry key. 
This ﬁ le includes settings for specifying whether running packages stop when the service stops, 
a listing of root folders to display in the Object Explorer of Management Studio, and settings for 
specifying which folders in the ﬁ le system are managed by the service.
You can change the conﬁ guration ﬁ lename and location. You can obtain this information by 
Management Studio from the Windows registry key HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\
MSDTS\ServiceConfigFile. As with most Registry key changes, you should back up the Registry 
before making any changes, and you need to restart the service after making changes for them to 
take effect.
One example of a conﬁ guration change that must be made is when you connect to a named instance 
of SQL Server. The following example shows the modiﬁ cation for handling a named instance 
➤
➤
➤
➤
➤
➤
➤
➤

Administration of the Integration Services Service ❘ 701
(MyServerName\MyInstanceName). This is because the default conﬁ guration for the SSIS service 
always points to “.” and must be conﬁ gured for a clustered or named instance. 
 <?xml version=“1.0” encoding=“utf-8”?>
 <DtsServiceConfiguration xmlns:xsd=“http://www.w3.org/2001/XMLSchema”
 xmlns:xsi=“http://www.w3.org/2001/XMLSchema-instance”>
   <StopExecutingPackagesOnShutdown>true</StopExecutingPackagesOnShutdown>
   <TopLevelFolders>
     <Folder xsi:type=“SqlServerFolder”>
       <Name>MSDB</Name>
       <ServerName>MyServerName\MyInstanceName</ServerName>
     </Folder>
     <Folder xsi:type=“File systemFolder”>
       <Name>File System</Name>
       <StorePath>..\Packages</StorePath>
     </Folder>
   </TopLevelFolders> 
 </DtsServiceConfiguration>
There isn’t a lot to conﬁ gure in this ﬁ le but it has some interesting uses. The ﬁ rst conﬁ guration line 
tells the packages how to react if the service stops. By default, packages that the service runs stop 
upon the service stopping or being failed over. You could also conﬁ gure the packages to continue to 
run until they complete after the service stops by changing the StopExecutingPackagesOnShutDown 
property to False: 
   <StopExecutingPackagesOnShutdown>false</StopExecutingPackagesOnShutdown>
Other common conﬁ guration ﬁ le change scenarios include adding additional paths from which 
to display packages other than the default SSIS package store path of C:\Program Files\SQL 
Server\110\Packages and creating a centralized folder structure for multiple servers by storing the 
service conﬁ guration ﬁ le in a central ﬁ le share.
Creating a Central SSIS Server
Many enterprise companies have so many packages that they decide to separate the service from SQL 
Server and place it on its own server. When you do this, you must still license the server just as if it 
were running SQL Server. The advantages of this are that your SSIS packages do not suffocate the 
SQL Server’s memory during a large load, and you have a central spot to manage. The disadvantages 
are that now you must license the server separately, and you add another layer of complexity when 
you debug packages. When you create a dedicated server, you create a fantastic way to easily scale 
packages by adding more memory to your central server, but you also create an added performance 
hit, as all remote data must be copied over the network before entering the data ﬂ ow buffer.
To create a centralized SSIS hub, you need to modify only the MsDtsSrvr.ini.xml ﬁ le and restart 
the service. The service can read a UNC path such as \\ServerName\Share and can point to 
multiple remote servers. Mapped drives are not recommended because the account that starts the 
SSIS service would need to be aware of the drive and could create an unnecessary dependency on 
that account. In the following example, the service enumerates packages from three servers, one of 
which is local and another that is a named instance. After restarting the service, you see a total of 

702  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
six folders to expand in Management Studio. The Management Studio aspect of SSIS is covered in 
detail later in this chapter. 
 <?xml version=“1.0” encoding=“utf-8” ?>
 <DtsServiceConfiguration xmlns:xsd=“http://www.w3.org/2001/XMLSchema” 
 xmlns:xsi=“http://www.w3.org/2001/XMLSchema-instance”>
   <StopExecutingPackagesOnShutdown>true</StopExecutingPackagesOnShutdown>
  <TopLevelFolders>
 <Folder xsi:type=“SqlServerFolder”>
   <Name>Server A MSDB</Name>
   <ServerName>localhost</ServerName>
   </Folder>
   <Name>Server B MSDB</Name>
   <ServerName>SQLServerB</ServerName>
   </Folder>
 <Folder xsi:type=“File systemFolder”>
   <Name>Server A File System</Name>
   <StorePath>P:\Packages</StorePath>
   </Folder>
 <Folder xsi:type=“File systemFolder”>
   <Name>Server B File System</Name>
   <StorePath>\\SQLServerB\Packages</StorePath>
   </Folder>
   </TopLevelFolders>
   </DtsServiceConfiguration>
Your next issue is how to schedule packages when using a centralized SSIS hub. You can schedule 
your packages through SQL Server Agent or through a scheduling system such as Task Scheduler 
from Windows. You already pay for a license of SQL Server, so it’s better to install SQL Server 
on your server and use Agent because it gives you much more ﬂ exibility. You can also store 
conﬁ guration tables and logging tables on this SQL Server to centralize its processing as well. Both 
scheduling mechanisms are covered later in this chapter.
Each time you make a change to the conﬁ guration ﬁ le, you need to stop and start the SSIS service, 
as described in the following sections. Now that you know how to conﬁ gure the MsDtsSrvr.ini
.xml ﬁ le responsible for the conﬁ guration of the Integration Services service, you need to know how 
to set the service’s properties.
Setting Service Properties Using the Windows Services Snap-In
As with any other Windows service, the Integration Services service has properties that dictate how 
it is to be started. Speciﬁ cally, you can manage the following from the Windows Services Snap-In: 
Conﬁ gure the startup type as Manual, Automatic, or Disabled.
Request that the service is started, stopped, or restarted.
Establish how the computer reacts to service failures.
View or modify a listing of dependent services (none are set up by default).
➤
➤
➤
➤

Administration of the Integration Services Service ❘ 703
To view and modify SSIS services properties using the Windows Services Snap-in, follow these steps: 
 1. 
Open the Services Snap-In from Control 
Panel Á Administrative Tools (or using 
the Category view from Performance and 
Maintenance Á Administrative Tools).
 2. 
Locate and right-click SQL Server Integration 
Services in the list of services.
 3. 
Select Properties to view the currently applied 
settings.
 4. 
On the General tab, you can view or change the 
Startup type (Automatic, Manual, or Disabled). 
When set to either Manual or Automatic, you 
can change the Service status to Start, Stop, or 
Resume.
 5. 
On the Log On tab (see Figure 20-1), you can 
view or alter the account used to start and run 
the service. By default, this runs under the NT 
Service\MsDtsServer110 account but most 
people use a domain service account.
 6. 
On the Recovery tab, you can conﬁ gure how the server responds to failures of the service 
by setting the First, Second, and Subsequent failures options to either Take No Action 
(the default), Restart the Service, Run a Program, or Restart the Computer. You can also 
instruct the service to reset the failure count after a certain number of days.
 7. 
You can modify the list of services on which the SSIS service depends (none by default) 
and view the list of services dependent on the SSIS service (none by default) on the 
Dependencies tab.
Setting Service Properties Using SQL Server Conﬁ guration Manager
As with using the Windows Services Snap-In, you can also conﬁ gure a limited set of Integration 
Services service properties using the SQL Server Conﬁ guration Manager. Speciﬁ cally, you can both 
conﬁ gure the logon information used by the service and establish the startup model of the service.
Follow these steps to view and modify SSIS Services properties using the SQL Server Conﬁ guration 
Manager: 
 1. 
Open the SQL Server Conﬁ guration Manager from All Programs Á Microsoft SQL Server 
2012 Á Conﬁ guration Tools.
 2. 
On the list of services on the right side, right-click SQL Server Integration Services and 
select Properties.
 3. 
On the Log On tab, you can view or alter the account used to start and run the service. 
By default, this runs under the NT Service\MsDtsServer110account. This resembles 
Figure 20-1.
FIGURE 20-1

704  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
 4. 
On the Service tab, you can view or change the Startup type (Automatic, Manual, or 
Disabled).
Now that you are comfortable setting up the service properties for the Integration Services service 
using either the Windows Services Snap-In or the SQL Server Conﬁ guration Manager, you next 
learn how you can modify Windows Firewall to permit access to Integration Services.
Conﬁ guring Windows Firewall for Access
You’ll probably ﬁ nd that your service requires modiﬁ cations to be made to the Windows Firewall 
system to provide consistent access to Integration Services. The Windows Firewall system controls 
access to speciﬁ c computer resources primarily by limiting access to preconﬁ gured ports. You 
cannot modify the port number used by Integration Services because it works using only port 135.
In addition to using the user interface mentioned in this section, you can also script out this process 
from the command line by running the following (the line is wrapped here but everything should be 
on a single command when you type it): 
 netsh firewall add portopening protocol=TCP port=135 name=“RPC (TCP/135)”
 model=ENABLE scope=SUBNET
 
 netsh firewall add allowedprogram program=“%ProgramFiles%\Microsoft SQL
 Server\100\DTS\Binn\MsDtsSrvr.exe” name=“SSIS Service” scope=SUBNET
To conﬁ gure the Windows Firewall to permit Integration Services access, follow these steps: 
 1. 
From the Control Panel, open the Windows Firewall.
 2. 
Select the Exceptions tab and click Add Program.
 3. 
In the Add Program dialog, click Browse and select C:\Program Files\Microsoft SQL 
Server\100\DTS\Binn\MsDtsSrvr.exe. You should also use the Change Scope option 
to detail the computers that have access to the program by specifying a custom list of IP 
addresses, subnets, or both. The resulting exception is shown in the Windows Firewall 
dialog.
 4. 
Alternatively, you can open the port instead of allowing the executable to have full reign of 
your network. To do this, click Add Port.
 5. 
In the Add Port dialog, type a meaningful description such as RPC(TCP/135) Integration 
Services, type 135 in the Port Number box, and select TCP as the protocol. You should 
also use the Change Scope option to indicate which computers have access to the port by 
specifying a custom list of IP addresses, subnets, or both.
That covers a substantial amount of the conﬁ guration required for the Integration Services service. 
The next focus is event logging.
Event Logs
Integration Services records events raised by packages during their execution in logs. The SSIS log 
providers can write log entries to text ﬁ les, SQL Server Proﬁ ler, SQL Server, Windows Event Log, 

Administration of the Integration Services Service ❘ 705
or XML ﬁ les. To perform logging, SSIS packages and tasks must have logging enabled. Logging 
can occur at the package, container, and task level, and you can specify different logs for packages, 
containers, and tasks.
To record the events raised, a log provider must be selected and a log added for the package. You 
can create these logs only at the package level, and a task or container must use one of the logs 
created for the package. After you conﬁ gure the logs within packages, you can view them either 
using Windows Event Viewer or within SQL Server Management Studio.
To view SSIS event logs using the Windows Event Viewer, follow these steps: 
 1. 
Open the Event Viewer from Control Panel Á Administrative Tools (or use the Category 
view from Performance and Maintenance Á Administrative Tools).
 2. 
Within the Event Viewer dialog, click Application.
 3. 
After the Application snap-in displays, locate an entry in the Source column valued at 
SQLISService110 or SQLISPackage110. The SQLISPackage110 source logs would be 
generated from the package logs, and the SQLISService110 source logs would be simple 
messages from the SSIS service.
 4. 
Right-click the entry, and select Event Properties to display descriptive information about 
the entry.
To view these events in SQL Server Management Studio, follow these steps: 
 1. 
Open Management Studio, and connect to the target Integration Services server.
 2. 
In Object Explorer, right-click Integration Services (the topmost node) and click View Logs.
 3. 
Select SQL Server Integration Services from the Select Logs section.
 4. 
You can see the details for an event displayed in the lower pane by clicking an event in the 
upper pane.
Monitoring Activity
Part of the performance monitoring of the Integration Services service includes conﬁ guring the 
logging of performance counters. These counters enable you to view and understand the use of 
resources consumed during the execution of SSIS packages. Speciﬁ cally, the logging encompasses 
event-resource usage, whereas packages perform the Data Flow Tasks.
Begin by focusing on some of the more insightful counters, including the following (at a server-level):
Rows Read: Provides the number of rows read from all data sources during package 
execution
Buffers in Use: Details the number of pipeline buffers (memory pools) in use throughout the 
package pipeline
Buffers Spooled: Speciﬁ es the number of buffers used to handle the data ﬂ ow processes 
The Buffers Spooled counter is important because it is a good indicator of when your machine runs 
out of physical memory or runs out of virtual memory during data ﬂ ow processing. The importance 
➤
➤
➤

706  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
of using buffers rather than spooling to disk is the difference between a package with 20 minutes 
execution time versus 20 hours in some cases. Each time you see a buffer spooled, a 10MB buffer 
has been written to disk.
One example of how these performance counters can be used includes ensuring that your server 
running the SSIS packages has enough memory. One of the bottlenecks in any transformation 
process includes input/output operations, whereby data is staged to disk during the transformations. 
Integration Services was designed to optimize system resources when transforming data between a 
source and destination, including attempting to perform these transformations in memory, rather 
than having to stage data to disk and incur I/O performance penalties. You should expect to see the 
value of the Buffers Spooled counter remain at zero (0) when only memory is being used during the 
transformation processes being performed by the SSIS packages. When you observe that the Buffers 
Spooled counter is normally valued higher than zero (0), it’s a good indication that more memory is 
needed on the server processing the SSIS packages.
SQL Server Proﬁ ler enables you to analyze the data operations and query plans generated for 
various data ﬂ ow pipeline activities. You can use this information to reﬁ ne indexes or apply other 
optimization techniques to the data sources your SSIS solution uses.
ADMINISTRATION OF INTEGRATION SERVICES PACKAGES 
IN PACKAGE DEPLOYMENT MODEL
Now that you’ve learned about the various aspects of Integration Services service administration, 
this section provides an overview of SSIS package elements and administration, and then you look at 
various ways to create packages. Next, you look at the management of the developed SSIS packages. 
When you understand how to create and manage packages, you can move on to the deployment, 
execution, and scheduling of SSIS packages and solutions.
Using Management Studio for Package Management
As discussed earlier in the “Integration Services Service” section, packages are managed primarily 
via Management Studio and its connection to the Integration Services service. Upon connecting 
to the service, you see two main folders: Running Packages and Stored Packages. The packages 
displayed are stored in either the msdb database sysssispackages table or the ﬁ le system folders 
speciﬁ ed in the Integration Services service conﬁ guration ﬁ le.
The main uses of Management Studio include monitoring running packages and managing the 
packages stored within the Integration Services environment. 
You can see information regarding currently executing packages within the Running Packages 
folder. Information about these packages displays on the Summary page, whereas you can obtain 
information about a particular executing package by clicking the package under the Running 
Packages folder and viewing the Summary page. You can stop the execution of a package listed 
within this folder by right-clicking the package and selecting Stop.
You can make changes to the storage of packages by adding custom folders and by copying packages 
from one type of storage to another using the Import and Export utilities. You can conﬁ gure the 

Administration of Integration Services Packages in Package Deployment Model ❘ 707
logical folders displayed within the MSDB folder in Management Studio by right-clicking on a given 
folder or by altering the sysssispackagefolders table within the msdb database. The root folders 
in this table are those in which the parentfolderid column contains null values. You can add 
values to this table to add logical folders, bearing in mind that the folderid and parentfolderid 
columns are the key values used to specify the folder hierarchy. In addition, you can conﬁ gure the 
default folders in the ﬁ le system that Management Studio displays. This is discussed in the “XML 
Conﬁ guration File” section earlier in this chapter. Importing and exporting packages are discussed 
in the “Deployment” section of this chapter.
The main management tasks you can perform on packages within Management Studio include the 
following: 
Creating new Object Explorer folders to display packages saved in either the ﬁ le system or 
SQL Server (msdb database sysssispackages table)
Importing and exporting packages
Running packages
Deleting packages
Renaming packages
Using the DTUtil Package Management Utility
Other than using Management Studio to manage packages, you also have the assistance of a 
command prompt utility named DTUtil. The primary reason you need to understand DTUtil is 
that this utility permits you to manage packages using schedulers or batch ﬁ les. As with using 
Management Studio, DTUtil enables you to copy, delete, move, sign, and even verify whether the 
server contains speciﬁ ed packages.
Using this utility, you include either the /SQL, /FILE, or /DTS options to specify where the packages 
that you want to manage are located. You use options (parameters) to specify particular behavior 
you want to use when running the utility. The options start with either a slash (/) or a minus sign (-) 
and can be added to the command line in any sequence.
You receive exit codes that let you know when something is wrong with your syntax or arguments, 
or you simply have an invalid combination of options. When everything is correct, DTUtil returns 
exit code 0 and displays the message The Operation Completed Successfully. The following 
other exit codes may be returned: 
1 — Failed
4 — Cannot locate package
5 — Cannot load package
6 — Cannot resolve the command
You must observe the following syntactical rules when you create the commands: 
Values for options must be strings and must be enclosed in quotation marks or contain no 
whitespace.
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤


Administration of Integration Services Packages in Package Deployment Model ❘ 709
Using similar steps, you can export packages. The one notable difference is that you right-click the 
package to be exported and select Export, rather than right-click the target folder and select Import. 
You can also perform these import and export operations using the DTUtil command-line utility.
Deployment
When Integration Services packages and solutions have been developed either on local computers or 
on development servers, they need to be deployed to test on production servers. Usually, you start 
the deployment process after you ensure that the packages run successfully within SSDT.
You deploy your packages or solutions via one of the following methods: 
Creating a package deployment utility and using the Package Installer Wizard
Using import or export package utilities in Management Studio
Saving or moving copies of packages in the ﬁ le system
Executing the DTUtil Package Management Utility
Often, the modiﬁ cations made to your Integration Services solution dictate which deployment 
method and tools to use. For example, if you modify only a single package out of a 30-package 
solution, using the import package utility within Management Studio or saving or moving copies 
of packages in the ﬁ le system might be simpler than deploying the entire solution using the Package 
deployment utility and Package Installer Wizard.
You can further categorize these four options for deployment into automated and manual. Using 
a Package deployment utility with the Package Installer Wizard would be best categorized as an 
automated deployment method, whereas the other options represent manual deployment methods. 
The following sections take a detailed look at each of these deployment methods.
Automated Package Deployment
A common way to deploy packages involves using the Package deployment utility. This utility builds 
your SSIS packages, package conﬁ gurations, and any supporting ﬁ les into a special deployment 
folder located within the bin directory for the Integration Services project. In addition, this utility 
creates a special executable ﬁ le named ProjectName.SSISDeploymentManifest and places it 
within this deployment folder. After creating the deployment utility, you then execute the manifest 
ﬁ le to install the packages.
This deployment method relies upon two separate steps. First, you create a deployment utility 
that contains all the ﬁ les needed for deployment. Second, you use the Package Installer Wizard to 
perform the deployment of these ﬁ les to a target deployment server.
Using the Package Deployment Utility
The following steps walk you through using the Package deployment utility to deploy your 
Integration Services solution: 
 1. 
Open SQL Server Data Tools and an Integration Services solution.
 2. 
Right-click your solution or project (the topmost node) in Solution Explorer, and select 
Properties.
➤
➤
➤
➤

710  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
 3. 
On the [Solution/Project Name] Property Pages dialog, select the Deployment Utility 
section.
 4. 
Within the Deployment Utility section of the Property Pages dialog, set the value of the 
CreateDeploymentUtility to True (see Figure 20-2).
 5. 
Optionally, you can conﬁ gure the deployment to enable conﬁ guration changes by setting 
the AllowConfigurationChanges value to True. This option enables updating the 
conﬁ guration of key elements of your packages that would be machine or environment 
dependent, such as server names or database initial catalogs that are both properties 
of database connection managers.
FIGURE 20-2
 6. 
Build your project as normal. The build process creates the ProjectName
.SSISDeploymentManifest ﬁ le and copies the packages to the bin/Deployment folder or 
whatever folder was speciﬁ ed for the DeploymentOutputPath on the project’s Property Page 
in the Deployment Utility section.
Because this utility copies all solution ﬁ les as part of the process, you can deploy additional ﬁ les, 
such as a Readme ﬁ le, with the project by simply placing these ﬁ les in the Miscellaneous folder of the 
Integration Services project.
Using the Package Installer Wizard
After you create an SSISDeploymentManifest ﬁ le using the Package deployment utility, you can 
install the packages by using the Package Installer Wizard, which can be started by clicking the 
SSISDeploymentMainfest ﬁ le. This wizard runs the DTSInstall.exe program and copies the 
packages and any conﬁ guration to a designated location.

Administration of Integration Services Packages in Package Deployment Model ❘ 711
Using the Package Installer Wizard provides you with some useful functionality that you 
either can’t ﬁ nd or is hard to achieve using the manual deployment methods. For example, you 
may choose either a ﬁ le-based or SQL-based deployment. Your ﬁ le-based dependencies will always 
be installed to the ﬁ le system. Another important, as well as useful, capability of this deployment 
process includes the ability to modify conﬁ gurations for use on the target deployment server. This 
enables you to update the values of the conﬁ guration properties, such as server name, as part of the 
wizard.
Following are the steps you need to take to ensure a successful deployment of your packages using 
the Package Installer Wizard: 
 1. 
Use Windows Explorer to browse to the ﬁ le path location in which the 
SSISDeploymentManifest ﬁ le was created (usually the solution or project location 
/bin/Deployment).
 2. 
After creating the ﬁ les within the 
Deployment folder, copy the Deployment 
folder and all its ﬁ les to a target 
deployment server.
 3. 
On the target deployment server, open 
the Deployment folder, and double-
click the SSISDeploymentManifest ﬁ le 
to launch the Package Installer Wizard 
(DTSInstall.exe).
 4. 
On the Deploy SSIS Packages page, 
select whether you want to deploy your 
packages to the ﬁ le system or to SQL 
Server (see Figure 20-3). Optionally, you 
can also have the packages validated 
after they have been installed.
 5. 
On the Select Installation Folder page, either provide a folder path for a ﬁ le system 
deployment or provide a server name and the appropriate server credentials for a SQL 
Server deployment.
 6. 
For a SQL Server deployment, on the Select Installation Folder page, provide a folder 
path for the package dependencies that require storing within the ﬁ le system. If you 
accept the default property here, you could be moving the location of your package 
conﬁ guration ﬁ les. 
 7. 
Optionally, if the package includes conﬁ gurations and you set the 
AllowConfigurationChanges value to true when the deployment manifest was 
created, the Conﬁ gure Packages page displays so that you can update the values for the 
conﬁ gurations.
 8. 
Optionally, if you requested validation of the packages, the Packages Validation page 
displays so that you can review the validation results.
FIGURE 20-3

712  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Import or Export Package Deployment
Earlier in the chapter, you looked at using the import and export functionality, which enables you to 
add or copy packages from one storage location and format to another storage location and format. 
One obvious use of this functionality is to deploy packages after development and testing have been 
completed.
An interesting beneﬁ t this approach may yield involves the capability of the import or export to 
change storage formats (for example, from ﬁ le system folders to the SQL server msdb database). This 
alteration of storage formats may be useful for disaster recovery, as a further safeguard for your 
Integration Services solutions, by saving them in various storage formats and locations.
File Save/Move Package Deployment
Probably the simplest way to get packages deployed involves copying them out of the Visual Studio 
project bin directory and copying it to the target server. This method does not have any of the more 
useful capabilities, but it can work quite well for smaller-scale Integration Services solutions. One 
distinct capability missing from this deployment method is the ability to deploy to SQL Server.
DTUtil Package Deployment
As with using Management Studio, DTUtil enables you to copy or move packages. As previously 
stressed in this chapter, the beneﬁ t of using DTUtil is that the commands created can be scheduled 
or run later. Therefore, using these capabilities, you could schedule the deployment of your packages 
to another server simply by using DTUtil copy or move commands to move the modiﬁ ed packages to 
a target server.
The following example demonstrates how you can use a DTUtil copy command for deployment: 
 dtutil /DTS srcPackage.dtsx /COPY SQL;destPackage
ADMINISTRATION OF INTEGRATION SERVICES PACKAGES 
IN PROJECT DEPLOYMENT MODEL
Much of this chapter to this point has focused on the package deployment model, which was the 
only way to operate in SQL Server 2005 and 2008. In SQL Server 2012, you can now administer 
and deploy packages as a project, which is a group of packages and how most developers operate. 
This section shows how to conﬁ gure the SSIS catalog and then deploy packages to it.
Conﬁ guring the SSIS Catalog
In the package deployment model, you can deploy to the MSDB database or the ﬁ le system of the 
server. With the project deployment model, you can deploy only to the SSIS catalog, which exists 
inside the database instance. This means that you also no longer need the Integration Services 
Windows service. The packages execute in the context of the database instance, and if the database 
engine were to fail, SSIS also fails or fails over in a cluster.

Administration of Integration Services Packages in Project Deployment Model ❘ 713
Creating the catalog also creates a database called SSISDB. There is only one catalog per database 
instance, and you can use this database to query to ﬁ nd some metadata about your packages. You 
can also read some of the tables in this database to gather operational data about which packages 
have failed recently from some of the logs.
With the catalog now created, it is time to conﬁ gure it. To do so, right-click on the previously 
created catalog and select Properties. This opens the catalog Properties dialog box (shown in 
Figure 20-5). In this dialog box, you can choose the level of logging and the amount of days you’re 
FIGURE 20-4
Before you can deploy your project, you need an SSIS catalog to deploy to. To do this, open 
Management Studio and follow these steps:
 1. 
Connect to the database engine in Management Studio.
 2. 
Right-click on Integration Services Catalogs, and select Create Catalog. This opens the 
Create Catalog dialog box (shown in Figure 20-4). In the dialog box, you must type a 
password that generates a key for encryption. Common Language Runtime (CLR) is also 
turned on here to execute packages via T-SQL. Click OK to create the catalog.

714  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Deploying Packages
The simplest way to deploy packages is with the project deployment model with the Integration 
Services Deployment Wizard. You can launch it from SSDT by right-clicking the project and 
selecting Deploy or by selecting the wizard in the SQL Server 2012 Á Integration Services program 
group. The wizard asks you a few questions to complete the deployment. The ﬁ rst screen asks which 
server you want to deploy to and which path (shown in Figure 20-6). 
FIGURE 20-5
retaining the logs. You can set the level of logging to be basic (default), verbose, performance 
or none. If you choose verbose or performance, you will see SSIS performance issues with SSIS 
packages. Those two options are only for temporary debugging of the package. 
You can also select how many versions of the project will be kept as you deploy. This enables you to 
rollback to a previous release of the project. By default, 10 versions of packages are kept.
 
 
 
 

Administration of Integration Services Packages in Project Deployment Model ❘ 715
FIGURE 20-6
After you select a valid path, select Next to deploy the package. You can also now ﬁ nd an .ispac 
ﬁ le in your project’s Deployment folder in Windows Explorer. If you send this one ﬁ le to the 
administrator, he can double-click it to reopen this same wizard. The ﬁ le contains all package and 
conﬁ guration information necessary for the entire project. 
If the folder does not exist on the server you want to deploy, click Browse and you can create it. To 
deploy, you must have a folder. The folder acts as a container for one or more projects and helps 
control the conﬁ guration of your environment later.


Administration of Integration Services Packages in Project Deployment Model ❘ 717
versus production variables. Environments contain a collection of variables that you can create that 
hold the conﬁ guration for the project or package. To do so, perform the following steps:  
 1. 
Open Management Studio, and expand the Integration Services Catalogs node. 
 2. 
Under the node expand your folder, right-click the Environments node, and select Create 
Environment. 
 3. 
For this example, create one environment called Client A and another called Client B. After 
each environment is created, right-click the environment and select Properties, which opens 
the Environment Properties dialog box (Figure 20-8). 
 4. 
Go to the Variables tab and create a new string variable called ServerName with the value 
of ServerA. Repeat this step for the Client B environment but use ServerB for its value. You 
can also click Sensitive if you want to encrypt the value.
FIGURE 20-8
Using Environments
To use the environments, you must ﬁ rst allow the project to see the environments and then follow 
these steps: 
 1. 
Right-click the project in the folder and select Conﬁ gure.
 2. 
Then, go to the References tab, and add a reference to any environment you want to be used 
by this project by clicking the Add button (shown in Figure 20-9). 

718  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
 3. 
With the references now created, you can reconﬁ gure the package to use them. In the 
Conﬁ gure Project screen go to the Parameters tab (shown in Figure 20-10). Here, you see 
a list of parameters and connection managers and can conﬁ gure them outside the package 
easily. In Figure 20-10 the ServerName parameter is underlined. This means that it actually 
references the environment variable you created in the last section. 
FIGURE 20-9
FIGURE 20-10

Execution and Scheduling ❘ 719
 4. 
To change the value of any parameter, select the ellipsis button next to the parameter. 
This opens the Set Parameter Value dialog box (shown in Figure 20-11). In this screen you 
can change the parameter’s value to any value you want by selecting the Edit Value radio 
box or change it to an environment’s value by selecting the environment’s variable in the 
drop-down box. 
FIGURE 20-11
EXECUTION AND SCHEDULING
Thus far, you have looked at ways to create, manage, and deploy Integration Services solutions. This 
section focuses on the ways in which you can execute and schedule execution of these solutions. As 
you have seen with other package and solution administrative tasks, the execution of packages can 
be performed using different tools. Speciﬁ cally, you can execute packages from the following: 
SQL Server Data Tools
SQL Server Import and Export Wizard (when run from Management Studio)
DTExec package execution command-line utility
DTExecUI package execution utility
Execute Package Tool
SQL Server Agent jobs
T-SQL (for project deployment model packages)
➤
➤
➤
➤
➤
➤
➤

720  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Which tool you should use often depends on factors such as in which stage of the package life cycle 
you are presently working. For example, the SSIS Designer within SSDT is a logical choice for 
package execution during development due to the features designed to assist in development (such as 
visually displaying package execution progress by changing the background color of tasks).
Running Packages in SQL Server Data Tools
Probably the ﬁ rst executions of packages will occur within SSDT because this is the development 
environment used to create your Integration Services solutions. Within SSDT, you simply either right-
click the package and then select Execute Package or press the F5 function key (or the Start button 
on the menu bar). The best way to execute packages from SSDT is by right-clicking the package and 
selecting Execute Package. Executing this way can prevent other packages from executing in case of 
misconﬁ guration. 
Running Packages with the SQL Server Import 
and Export Wizard
When you use the Import and Export Wizard from Management Studio, you have an option 
to execute the package immediately. This provides an opportunity to both relocate and execute 
packages in one administrative step.
Running Packages with DTExec
The primary use of DTExec is to enable you to run packages either from the command line, from 
a script, or using a scheduling utility. All conﬁ guration and execution features are available using 
this command. You can also load and run packages from SQL Server, the SSIS service, and the ﬁ le 
system.
The following additional syntactical rules must be followed when you create these commands: 
All command options start with a slash (/) or a minus sign (−).
Arguments are enclosed in quotation marks when they contain any whitespace.
Values that contain single quotation marks are escaped by using double quotation marks 
within quoted strings.
The general syntax for the DTExec commands is as follows: 
 Dtexec /option value
Following is an example that shows running a sample package called CaptureDataLineage.dtsx. 
The /FILE is pointing to a package stored on the ﬁ le system in package deployment model. The /
CONNECTION switch is changing a connection manager at runtime. 
Dtexec /FILE “C:\Program Files\Microsoft SQL Server\110\Samples\Integration
Services\Package Samples\CaptureDataLineage
Sample\CaptureDataLineage\CaptureDataLineage.dtsx “ /CONNECTION
“ (local).AdventureWorks “; “\ “Data Source=(local);Initial
➤
➤
➤

Execution and Scheduling ❘ 721
Catalog=AdventureWorks;Provider=SQLNCLI.1;Integrated Security=SSPI;Auto
Translate=False;\ “ “  /REPORTING
E
Whenever you execute a package using DTExec, one of the following exit codes may be returned: 
0 — Successful execution
1 — Failed
3 — Canceled by User
4 — Unable to Find Package
5 — Unable to Load Package
6 — Syntax Not Correct
There are numerous options you can use to alter how the package execution is run. Some examples 
include /Decrypt, which sets the package password used to secure information within the package, 
and /Set, which you use to assign values to SSIS variables at runtime. The options are processed in 
the order in which they are speciﬁ ed. When using the /Set and /ConfigFile commands, the values 
are also processed in the order in which they are speciﬁ ed. Neither options nor arguments (except 
passwords) are case-sensitive.
Running Packages with DTExecUI (Package 
Deployment Model)
You can conﬁ gure the various options you need to run packages using the graphical equivalent 
to the DTExec utility: the DTExecUI utility. With the wizard that this utility uses to gather details 
regarding the package execution, you can better understand many of the options and see the syntax 
required to run the package execution. To use this wizard, complete the following steps: 
 1. 
Launch the DTExecUI utility by double-clicking a ﬁ le with a .dtsx extension or from inside 
Management Studio by right-clicking a package and selecting Run. 
 2. 
Then select the options that you need to run the package along the left side of the utility 
pages and conﬁ gure the options in the main part of the page. When you ﬁ nish, you can view 
the last page, which shows you the command line needed to execute the package with the 
options you selected.
 3. 
After you complete the various pages and review the command line that will be submitted, 
click the Execute button. This submits the command line to the Integration Services engine 
by using the DTExecUI utility. Be careful when you use this utility in a 64-bit environment 
because this utility runs in Windows on Win32, not on Win64. Thus, for 
64-bit environments, you should use the 64-bit version of the DTExec utility at the 
command prompt or use SQL Server Agent.
The main reason you should become more familiar with both the DTExec and DTExecUI utilities 
is that they are useful for testing your packages and ultimately validating the proper command line 
that you may schedule using the SQL Server Agent.
➤
➤
➤
➤
➤
➤

722  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Running Packages with the Execute Package Tool 
(Project Deployment Model)
In the project deployment model, package execution and conﬁ guration are even easier. To execute 
packages in this model, complete the following steps: 
 1. 
Right-click the package in Management Studio, and select Execute. This opens the Execute 
Package dialog box (shown in Figure 20-12). 
 2. 
If your package requires an environment variable, check the box, and select your 
environment that you want to execute under. Doing this reconﬁ gures the package to run 
under the collection of environment variables you created earlier. 
 3. 
 If you need to change the connection managers, use the Connection Manager tab. This 
changes only the connection for one execution. In the Advanced tab, you can conﬁ gure the 
package to run in 32-bit mode and the logging levels. 
FIGURE 20-12
 4. 
Click OK and the package executes asynchronously, meaning that the package execution 
runs in T-SQL in the background. 
 5. 
To view the status of the package execution, open the operational reports by right-clicking 
the SSIS catalog and selecting Reports Á Standard Reports Á  Integration Services 

Execution and Scheduling ❘ 723
Dashboard. When opened, you can drill into the package execution by viewing the 
Overview report. This report (shown in Figure 20-13) enables you to see what parameters 
were passed to the package and performance-related information. 
FIGURE 20-13: Execution Dashboard
Scheduling Execution with SQL Server Agent
You need the ability to automate the execution of your Integration Services packages. Although 
many popular scheduling tools are available to accomplish this automation, here you look at how 
SQL Server Agent can assist in automating execution.
You start by creating a job and then including at least one step of the SQL Server Integration 
Services Packages type. You can also conﬁ gure other job options. One option you may conﬁ gure 
includes job notiﬁ cations to send e-mail messages when the job completes, succeeds, or fails. 
Another job option you may conﬁ gure includes job alerts to send notiﬁ cations for SQL Server event 
alerts, performance condition alerts, or WMI event alerts. Much of this conﬁ guration can be done 
through environments setup by the DBA.
To set up SQL Server Agent to execute a package, follow these steps: 
 1. 
Open Management Studio, and connect to a SQL Server.
 2. 
In Object Explorer, expand the SQL Server Agent.

724  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
 3. 
Within the SQL Server Agent section of Object Explorer, right-click the Jobs folder, and 
select New Job.
 4. 
On the General page of the New Job dialog, provide a name, owner, category, and 
description for the job.
 5. 
On the Steps page of the New Job dialog, click the New button along the bottom.
 6. 
On the New Job Step dialog, provide a step name, and select SQL Server Integration 
Services Packages type. In addition, conﬁ gure the SSIS-speciﬁ c tabbed sections with the 
information required to run your package. This SSIS section is almost identical to the 
options you provided when using the DTExecUI utility or the Package Execution dialog box. 
You have a package source that you set to SSIS Catalog for the project deployment model 
(shown in Figure 20-14) or for the package deployment model you have SQL Server, ﬁ le 
system, or SSIS package store. Next, you provide the package you want to schedule. When 
you select the Command Line tab, you can review the detailed command line that will 
be submitted by the SQL Server Agent to execute the package. You may want to compare 
this to the command-line values generated by the DTExecUI utility while you were testing 
package execution.
FIGURE 20-14

Applying Security to Integration Services ❘ 725
 7. 
On the Advanced page of the New Job Step dialog, you can specify actions to perform when 
the step completes successfully, the number of retry attempts, the retry interval, and actions 
to perform should the step fail. After accepting the step conﬁ guration by pressing OK, the 
Step page of the New Job dialog shows your new step. After adding multiple steps, you can 
reorder the steps on this page.
 8. 
After accepting the step conﬁ guration, from the New Job dialog you can optionally 
conﬁ gure execution schedules, alerts, notiﬁ cations, and target servers.
Running Packages with T-SQL
To execute a package in T-SQL, you can use the stored procedures in the catalog schema. First, you 
must create an execution using the catalog.create_execution stored procedure. This creates 
a unique identiﬁ er, (GUID), that you then call and execute using the catalog.start_execution 
stored procedure. You can also set parameters of the package using the catalog.set_execution_
parameter_value stored procedure. A complete execution can be seen in the following code snippet:
Declare @execution_id bigint
EXEC [SSISDB].[catalog].[create_execution] @package_name=N’2-OtherFeature.dtsx’, @
execution_id=@execution_id OUTPUT, 
@folder_name=N’EDW’, @project_name=N’ExpeditionDenali’, 
@use32bitruntime=False, @reference_id=Null
Select @execution_id
DECLARE @var0 sql_variant = N’localhost’
EXEC [SSISDB].[catalog].[set_execution_parameter_value] @execution_id,  @object_
type=30, @parameter_name=N’ServerName’, 
@parameter_value=@var0
DECLARE @var1 smallint = 1
EXEC [SSISDB].[catalog].[set_execution_parameter_value] @execution_id,  @object_
type=50, @parameter_name=N’LOGGING_LEVEL’, 
@parameter_value=@var1
EXEC [SSISDB].[catalog].[start_execution] @execution_id
GO
APPLYING SECURITY TO INTEGRATION SERVICES
You have now looked at most of the important package administrative tasks, including creating, 
managing, deploying, and executing Integration Services solutions. In addition, you have reviewed 
the major Integration Services service administrative tasks. This section describes the detailed 
security options available within Integration Services.
An Overview of Integration Services Security
Integration Services, like all of SQL Server, uses layers of security that rely on different mechanisms 
to ensure the integrity of both the design of packages and the administration and execution of 

726  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
packages. For the package deployment model, SSIS security is found on both the client and the 
server, implemented with features such as the following: 
Package-protection levels to encrypt or remove sensitive information from the package
Package-protection levels with passwords to protect all or just sensitive information
Restricting access to packages with roles
Locking down ﬁ le locations where packages may be stored
Signing packages with certiﬁ cates
Within packages, Integration Services generally deﬁ nes sensitive data as information such as 
passwords and connection strings. You cannot deﬁ ne what should and should not be considered 
sensitive by SSIS unless you do so within a custom-developed task.
Integration Services deﬁ nes sensitive information as the following: 
Connection string password (Sensitive) or Whole connection string (All)
Task-generated XML nodes tagged as sensitive by SSIS
Variables marked as sensitive by SSIS
For the project deployment model, much of this complexity goes away. As you deploy the package to 
the catalog database, the database handles the encryption and then you secure the package with roles.
Securing Packages in Package Deployment Model
The two primary ways in which you secure packages within Integration Services include setting 
package-protection levels and conﬁ guring appropriate database SSIS roles. The following sections 
look at these two security implementations.
Package Protection Levels
Many organizations have sensitive information in the SSIS package and want to control where that 
information resides within the organization. Your packages may contain passwords from your 
environment that if executed by the wrong individual may produce data ﬁ les that could be sensitive.
These security concerns are addressed in Integration Services through the use of package protection 
levels. First, you can ensure that sensitive information that would provide details about where your 
information resides, such as connection strings, can be controlled by using EncryptSensitive 
package protection levels. Second, you can control who can open or execute a package by using 
EncryptAll package passwords.
The following package protection levels are at your disposal within Integration Services: 
Do not save sensitive.
Encrypt (all/sensitive) with User Key.
Encrypt (all/sensitive) with Password.
Rely on server storage for encryption (SQL storage only).
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤

Applying Security to Integration Services ❘ 727
The package protection levels are ﬁ rst assigned using SSDT. You can update these levels after 
deployment or during import or export of the package using Management Studio. In addition, you 
can alter the package protection levels when packages are copied from SSDT to any other location 
in which packages are stored. This is a nice compromise between development and administration 
because developers can conﬁ gure these levels to suit their rapid development requirements, and 
administrators can follow up and revise these levels to meet production security standards.
Database Integration Services Roles for Package Deployment Model
If you deploy your packages to SQL Server (msdb database), you need to protect these packages 
within the database. Like traditional databases, this security is handled by using database roles. 
Three ﬁ xed database-level roles can be applied to the msdb database to control access to packages: 
db_dtsadmin, db_dtsltduser, and db_dtsoperator.
You apply these roles to packages within Management Studio, and these assignments are saved 
within the msdb database, in the sysssispackages table within the readerrole, writerrole, and 
ownersid columns. As the column names imply, you can view the roles that have read access to 
a particular package by looking at the value of the readerrole column, the roles that have write 
access to a particular package by looking at the value of the writterrole column, and the role that 
created the package by looking at the value of the ownersid column.
Follow these steps to assign a reader and writer role to packages for packages in the package 
deployment model: 
 1. 
Open Management Studio, and connect to an Integration Services server.
 2. 
In Object Explorer, expand the Stored Packages folder, and expand the subfolder to assign 
roles.
 3. 
Right-click the subfolder to assign roles.
 4. 
In the Packages Roles dialog, select a reader role in the Reader Role list and a writer role in 
the Writer Role list.
For packages in the project deployment model, the conﬁ guration steps are almost identical. The only 
difference is you conﬁ gure the project, not the packages. You can right-click the project and select 
Properties and then go to the Permissions tab. Because packages in this model are all stored in the 
database instance, you can also run them with SQL authenticated logins. 
You may also create user-deﬁ ned roles if the default execute and update actions for existing roles do 
not meet your security needs. To deﬁ ne these roles, you connect to a SQL Server instance and open 
the Roles node within the msdb database. In the Roles node, right-click the database roles, and select 
New Database Role. After a new role has been added to the msdb database, you must restart the 
SSIS service before you can use the role.
These database Integration Services roles help to conﬁ gure your msdb database sysssispackages 
table with package security options for reading and writing to speciﬁ c packages. By applying this 
level of security, you provide security at the server, database, and table levels. Again, the security 
discussed within this section applies only when you save your packages within SQL Server (msdb 
database). 

728  ❘  CHAPTER 20  INTEGRATION SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Database Integration Services Roles for Project Deployment Model
If you deploy packages using the project deployment model to the SSIS catalog, you can secure the 
packages at a project-level. To do so, perform the following steps: 
 1. 
To conﬁ gure permissions, right-click on the project in Management Studio and select 
Properties. 
 2. 
Go to the Permissions tab (shown in Figure 20-15) and click Browse to grant rights to a new 
role or SQL Server user that’s in the catalog database called SSISDB. One important note is 
that by default any user in the DBO role of the SSISDB database will automatically have full 
rights to your project (see Figure 20-14) unless you revoke those rights. 
FIGURE 20-15
You can grant a user read rights if you just want them to be able to see the package and perhaps 
scan the metadata from the packages in the project. Execute rights enables the user or role to run a 
package in the project and Modify rights enables the user to overwrite the project or conﬁ gure it. 
SUMMARY
You are now familiar with many of the various administrative functions related to Integration 
Services. There are now two modes for deployment in SSIS: the package and project deployment 
models. The project deployment model only enables you to deploy the entire project and has most of 
the new 2012 features associated with it. The package deployment model is similar to what you’ve 
had in SQL Server 2005 and 2008. Packages in the project deployment model can be conﬁ gured 
with environments easily so you can run the package with a set of variables for a give customer. In 
Chapter 21, “Analysis Services Administration and Performance Tuning,” you learn about similar 
administrative functions related to Reporting Services. 



Advanced calculations that offer better support and performance than RDBMS engine 
capabilities
Advanced data mining techniques to predict future activities based on the historical data in 
your database
So what is the DBA role within SSAS? If you take a look at a traditional DBA, Table 22-1 shows 
some of what a DBA does in SSAS:
➤
➤
Tour of Analysis Services  ❘  731
Now that you have an overview of the MOLAP and tabular models, the next section focuses on a 
deeper dive into each of the models.
MOLAP Components
In the 2012 release of Analysis Services, the Multidimensional OLAP model (MOLAP model) is 
the cube. The MOLAP model cube combines dimensions and fact tables into a single navigable 
view for users to do self-service analytics against. Following is a look at the composition of the 
MOLAP model: 
Data source view: At the heart of the MOLAP model is the logical data schema that 
represents the data from the source in a familiar and standard manner. This schema is 
known as the data source view (DSV), and it isolates the cube from changes made to the 
underlying sources of data.
Dimensional model: This model provides the framework from which the cube is designed. 
Included are the measures (facts) that users need to gain measurable insight into their 
business and the dimensions that users employ to constrain or limit the measurements to 
useful combinations of factors.
Calculations (expressions): Often, a cube needs to be enhanced with additional calculations 
to add the necessary business value that it is expected to achieve. The calculations within 
the MOLAP model are implemented by writing Multi Dimensional Expression (MDX) 
language code snippets. MDX is to the cube what SQL is to the database. In other words, 
MDX is what you use to get information from a cube to respond to various user requests.
➤
➤
➤
TABLE 22-1: Mapping your DBA to SSAS
SQL SERVER DBA SKILL
SSAS SKILL COMPARISON
Creating logins
Creating roles
Creating indexes
Conﬁ guring aggregations
Partitioning tables
Partitioning measure groups
Backing up a database
Backing up an SSAS database
 
 
 
 

732  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Familiar and abstracted model: Many additional features enhance the end-users’ analysis 
experience by making their reporting and navigation through the cube more natural. Again, 
like calculations, the model is often enhanced to include features not found in the data 
sources from which the cube was sourced. Features such as language translations, aliasing 
of database names, perspectives to reduce information overload, or Key Performance 
Indicators (KPIs) to quickly summarize data into meaningful measurements are all part of 
the MOLAP model.
Administrative conﬁ guration: With the cube designed and developed, the administrative 
aspects of the MOLAP model come to the forefront. Often, administrative tasks such as 
conﬁ guring the security to be applied to the cube or devising a partitioning scheme to 
enhance both query and processing performance are applied to the MOLAP model.
Tabular Model Components
A Tabular model has very similar components to the MOLAP model except simpliﬁ ed. Instead of 
starting with a model and working backwards, the user creates a Tabular model by ﬁ rst importing 
data. Then, the user creates the model with the data she imported. The data can be imported from 
a variety of sources like Excel, ﬂ at ﬁ les, or nearly any OLE DB or ODBC compliant data source. 
Following are some of the elements you’ll ﬁ nd in a Tabular model:
Connections: A list of data connections required to make the cube
Tables: Contains the actual data that the cube is built on 
Roles: The DBA mechanism to secure the cube or data in the cube
Analysis Services Architectural Components
Now that you understand the basics about the MOLAP and the Tabular models, it’s time to turn 
to the components that make up Analysis Services. The Analysis Services server (msmdsvr.exe 
application) is implemented as a Microsoft Windows service and consists of a query processor (for 
MDX queries and DMX data-mining queries), an XMLA listener, and XML for Analysis. The 
following list describes these components in greater detail: 
Query processor: The query processor parses and processes statements similarly to 
the query processing engine within SQL Server. This processor is also responsible for the 
caching of objects, storage of MOLAP model objects and their data, processing 
calculations, handling server resources, and managing transactions.
XMLA listener: This listener component facilitates and manages communications between 
various clients and the Analysis Services server. The port conﬁ guration for this listener is 
located in the msmdsrv.ini ﬁ le, which is located in the C:\Program Files\Microsoft 
SQL Server\MSAS11.MSSQLSERVER\OLAP\Config folder by default. A value of 0 in this ﬁ le 
under the <Port> tag simply indicates that SSAS is conﬁ gured to listen on the default TCP/
IP port of 2383 for the default instance of SSAS and 2382 for other instances of SSAS. 
SSAS named instances can use a non-default port. The SQL Server Browser keeps track of 
the ports on which each named instance listens and performs any redirection required when 
a client does not specify the port number along with the named instance. You should use a 
ﬁ rewall to restrict user access to Analysis Services ports from the Internet.
➤
➤
➤
➤
➤
➤
➤


734  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
One thing you might notice in Figure 21-1 is the two similar databases in the localhost Tabular 
instance. This is because as you open the development tools a temporary workspace is created while 
you modify the cube in the SSDT. The database name for this temporary workspace ends with the 
user’s name and a GUID. This applies only to the Tabular models because the MOLAP model is 
developed ofﬂ ine and then deployed to the server.
Server Properties
The server properties covered in this section are important for conﬁ guring the behavior of the 
SSAS server instance. To review and adjust the server properties, perform the following steps: 
 1. 
Open SQL Server Management Studio.
 2. 
Connect to the Analysis Services server using the Object Explorer.
 3. 
Right-click the server (the topmost node) and choose Properties.
By going to the Properties window, you can see dozens of properties that can help you tune your 
SSAS instance. You can see more properties by checking Show Advanced Properties. This section 
covers some of the important properties to SSAS.
Log Properties
If you’re trying to diagnose why a query is taking longer than anticipated, this set of properties will 
help you troubleshoot performance. The log properties control how and where logging takes place. 
This property group includes details related to error logging, exception logging, the ﬂ ight recorder, 
query logging, and tracing. Some examples include the QueryLog\QueryLogConnectionString and 
QueryLog\QueryLogTableName properties, which direct the server to where the query logging is 
persisted (database and table). The QueryLog\QueryLogConnectionString property speciﬁ es the 
SQL Server instance and database that hold the Analysis Services query log. By default, after you 
specify this and set the CreateQueryLog Table property to True, SQL Server begins to log every 
tenth query to the table. If you have an active Analysis Services instance, you may want to decrease 
this setting to every hundredth query.
This query log can later be used to tune your SQL Server by using a Usage Based Optimization tool, 
whereby you tune the Analysis Services cube based on queries used in the past.
Memory Properties
The memory properties dictate how the server utilizes system memory resources. The 
LowMemoryLimit represents a threshold percentage of total physical memory, at which point the 
server attempts to perform garbage collection for unused resources to free more resources. The 
default value is conﬁ gured at 65 percent of total physical memory. The TotalMemoryLimit tells 
the server how much of the total physical memory of the server hardware should be made available 
for use by Analysis Services. This limit is conﬁ gured to 80 percent of all server memory by default. 

Administering Analysis Services Server ❘ 735
Essentially, this means that Analysis Services can take between 65 to 80 percent of your server’s 
memory resource and not give it back after it crosses 65 percent.
In the Tabular model, the VertipaqMemoryLimit property identiﬁ es at what point the SSAS will 
start allowing paging. This paging occurs only if the VertipaqPagingPolicy property is set to 1 
(advanced property only available if you check the Show Advanced Properties checkbox). Paging is 
turned on by default. 
Network Properties
The network properties are a group of properties to control the network communication resources 
used by the server. Most notable are the settings that dictate whether the listener uses IPv4 or IPv6 
protocols and whether the server permits the use of Binary XML for requests or responses. In 
Windows 2008 R2 and Windows 7, IPv6 is enabled by default.
OLAP Properties
The OLAP properties control how the server performs processing of the server objects (cubes, 
dimensions, and aggregations). Along with the processing properties, this section includes 
conﬁ guration properties for the way the server processes queries. Some of these query-processing 
properties are useful for simulating many testing scenarios. For example, you could adjust the 
IndexUseEnabled, UseDataSlice, and AggregationUseEnabled properties to benchmark different 
query-handling scenarios to determine whether some of these optimizations can provide the wanted 
performance enhancement.
Security Properties
The security properties are responsible for controlling how the server handles permissions. 
Examples of these properties include RequireClientAuthentication, which conﬁ gures whether 
clients connecting to the server require authentication. By setting BuiltInAdminsAreServerAdmins 
to False, local server administrators are not implicitly given administrator rights to your SSAS 
instance. Both the local administrators and the service account are given escalated rights to Analysis 
Services by default because of this property.
Required Services
The Windows services required by Analysis Services include SQL Server Analysis Services, SQL 
Server Agent (only if you want to schedule processing of jobs), and SQL Server Browser. The 
SQL Server Browser service supports the Analysis Services redirector used when clients connect to 
named instances.
Commonly, the logon account used by any service should be one that has the least number of 
privileges required to function properly. More often than not, an account that has network rights 
is required, and this account would need to be granted access rights on the remote resources in 
addition to conﬁ guring the account to be used by the service.

736  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Analysis Services Scripting Language
Now consider how many of your administrative tasks can be automated by using the built-in 
scripting language as a DBA. The Analysis Services Scripting Language, or ASSL, is a language that 
will automate administrative tasks for Analysis Services. This language is based on XML and is 
what client applications use to get information from Analysis Services.
The scripting language has two distinct parts. The ﬁ rst part deﬁ nes the objects and server 
properties that are part of the server, including the objects used to develop solutions (measures and 
dimensions). The other part requests the server to perform actions, such as processing objects or 
performing batch operations.
It is important to focus on the scripting language components that help you manage the Analysis 
Services server. Start by looking at some examples of how you can use the language to process 
objects. Processing enables you to ﬁ ll objects with data so that they may be used by end users 
for business analyses. Some of the objects you can process include cubes, databases, dimensions, 
and partitions. To perform this processing using the scripting language, you use the language’s 
Process command.
An example of a script that would process the AdventureWorks Employee dimension follows: 
 <Batch xmlns=“http://schemas.microsoft.com/analysisservices/2003/engine”>
 <Parallel>
 <Process xmlns:xsd=“http://www.w3.org/2001/XMLSchema”
 xmlns:xsi=“http://www.w3.org/2001/XMLSchema-instance”
 xmlns:ddl2=“http://schemas.microsoft.com/analysisservices/2003/engine/2”
 xmlns:ddl2_2=“http://schemas.microsoft.com/analysisservices/2003/engine/2/2”
 xmlns:ddl100_100=“http://schemas.microsoft.com/analysisservices/2012/engine/10
 0/100”>
 <Object>
 <DatabaseID>Adventureworks DW</DatabaseID>
 <DimensionID>Dim Employee</DimensionID>
 </Object>
 <Type>ProcessUpdate</Type>
 <WriteBackTableCreation>UseExisting</WriteBackTableCreation>
     </Process>
   </Parallel>
 </Batch>
You can script many of the actions that you can conﬁ gure in SQL Management Studio. 
For example, you can generate the example script shown here by right-clicking the AdventureWorks 
cube and selecting the Process Menu option. This displays the Process Cube dialog (see Figure 21-2). 
From this dialog, click the Script button located along the top under the title bar, and then select the 
location in which you want to generate the script.
Don’t worry about the options in this screen yet; the “Processing Analysis Services Objects” section 
discusses them.

Administering Analysis Services Databases ❘ 737
ADMINISTERING ANALYSIS SERVICES DATABASES
Now that you understand more about the Analysis Services server, look at the administrative tasks 
needed for the databases that are ultimately deployed and run on the Analysis Services server. The 
primary tasks associated with managing the Analysis Services databases include deployment to the 
server, processing Analysis Services objects, performing disaster recovery activities such as backup 
and restore operations, and synchronizing databases to copy entire databases.
Deploying Analysis Services Databases
Obviously, without deploying databases, there is no value to running an Analysis Services. Through 
deployment of Analysis Services databases to the server, changes to the design of the database are 
applied to the server.
When performing administrative tasks, you can either use Management Studio to affect changes 
directly in a database in what is commonly referred to as online mode, or you can work within 
SSDT to affect changes via a Build and Deploy process commonly referred to as ofﬂ ine mode. More 
speciﬁ c to database deployment, you have the following options: 
Deploy changes directly from SSDT.
Script changes and deploy from within Management Studio.
Make incremental deployments using the Deployment Wizard.
Process changes using the Synchronize Database Wizard.
➤
➤
➤
➤
FIGURE 21-2

738  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Many of these options are useful only in speciﬁ c circumstances and as such are not given much attention 
in this chapter. The most useful and complete method to deploy the databases is to use the Deployment 
Wizard. Alternatively, the next best tool to assist with deployment is the Synchronize Database Wizard.
The main advantage of the Deployment Wizard is that it is the only deployment method that applies 
the database project deﬁ nition to production or any environment and enables you to keep many 
of the production database conﬁ guration settings, such as security and partitioning. This is 
important because neither direct deployment from SSDT nor scripting from Management Studio 
permits the deployment to maintain existing conﬁ guration settings.
The following steps show how the Deployment Wizard operates so you can understand how 
valuable it is for handling deployment: 
 1. 
From the Start menu under Microsoft SQL Server 2012 Á Analysis Services, launch the 
Deployment Wizard.
 2. 
On the Specify Source Analysis Services Database page, enter a full path to an Analysis 
Services database. This ﬁ le should be provided to you by the SSAS developer, or you can 
ﬁ nd it under the SSAS project folder. This one ﬁ le contains all the metadata for the cube, 
security, and partitions. It does not contain any data.
 3. 
On the Installation Target page, indicate the server to which the database should be deployed, 
along with the wanted database name. (It defaults to the ﬁ lename of the database.) If you 
don’t like the default database name, you can type over it, as shown in Figure 21-3. 
FIGURE 21-3
 4. 
On the Specify Options for Partitions and Roles page, indicate which conﬁ guration options 
(Partitions and Roles) should be maintained on the deployment target database and thus 
not overwritten by this deployment (see Figure 21-4). This screen is especially useful if you 

Administering Analysis Services Databases ❘ 739
FIGURE 21-4
make changes in Management Studio to roles or partitions and do not want the developer’s 
ﬁ les to overwrite your own conﬁ guration.
 5. 
On the Specify Conﬁ guration Properties page, select which conﬁ guration settings from 
the current conﬁ guration ﬁ le (.configsettings) should be applied to the target database. 
These settings provide a useful way to redirect items such as data source connection 
strings to point to production sources, rather than those used for development and 
testing. The Retain check boxes at the top provide an elegant way to manage updates of 
previous deployments because they disable overwriting of both the conﬁ guration and the 
optimization setting (see Figure 21-5). On this screen, you can also change the source for 
your data and the target of where your physical ﬁ les will be stored. 
 6. 
On the Select Processing Options page, enter the desired processing method and change any 
writeback table options that the developer may have set. To support a robust deployment, 
you may also select the option to include all processing in a single transaction that can roll 
back all changes should any part of the deployment fail. The Default processing method 
enables Analysis Services to review the modiﬁ cations to be applied and determine the 
optimal processing needed (see Figure 21-6).

740  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
FIGURE 21-5
FIGURE 21-6

Administering Analysis Services Databases ❘ 741
 7. 
On the Conﬁ rm Deployment page is an option to script the entire deployment. This 
option is useful when either the person running the Deployment Wizard is not authorized 
to perform the actual deployment or the deployment needs to be scheduled so as not to 
interfere with other activities.
Processing Analysis Services Objects
Now that you understand how to deploy Analysis Services databases, you must add data to these 
objects by processing them. In addition, if the cubes need to be updated to reﬂ ect development 
changes made after the initial deployment, you need to reprocess them. Last, when data sources 
have changes made to their information, you need to perform, minimally, an incremental 
reprocessing of the cube to ensure that you have up-to-date data within the Analysis Services 
solution. Developers often build and test designs locally; the local schema from SSDT must ﬁ rst be 
deployed to the server before performing any processing.
Processing a MOLAP Model
The Analysis Services MOLAP model objects that require processing include measure groups, 
partitions, dimensions, cubes, mining models, mining structures, and databases. The processing 
is hierarchical — that is, processing an object that contains any other objects also processes those 
objects. For example, take a database that includes one or more cubes, and these cubes contain 
one or more dimensions, so processing the database would also process all the cubes contained 
within that database and all the dimensions contained in or referenced by each of the cubes. For 
the Tabular model, you simply reprocess the tables fully (not incrementally). The following sections 
cover how to process individual objects. If your SSAS database is smaller (roughly less than 20 
gigabytes), then you could potentially process the entire database and still be online prior to your 
users needing the data. This could then be done with fully processing the database (essentially a 
wipe and load) with much less complexity to you. 
Processing Dimensions
Analysis Services processes dimensions by simply running queries that return data from the data 
source tables for the dimensions. This data is then organized into the hierarchies and ultimately into 
map ﬁ les that list all the unique hierarchical paths for each dimension. Processing dimensions can 
be optimized primarily through strategic indexes on the primary key and other key attributes. Prior 
to processing your cube or partition, you must ﬁ rst process the dimension if you are processing 
items selectively.
Processing Cubes
The cube contains both measure groups and partitions and is combined with dimensions to give the 
cube a data deﬁ nition. You can process a cube by issuing queries to get fact-table members and the 
related measure values such that each path of dimensional hierarchies includes a value.
Processing Partitions
Just as in database partitioning, the goal of Analysis Services partitioning is to improve query response 
times and administrator processing durations by breaking large data sets into smaller ﬁ les — typically, 


Administering Analysis Services Databases ❘ 743
on the database can also be processed (see Figure 21-7). A common architectural design 
employed in data warehousing involves the use of shared dimensions. These dimensions 
can be shared across the organization and allow for low maintenance and uniformity. The 
Process Affected Objects setting can therefore have a profound impact when you use an 
architecture involving shared dimensions because it may force reprocessing of many other 
databases in which the shared dimensions are used.
FIGURE 21-7
 5. 
You can also conﬁ gure sophisticated dimension key error handling (see Figure 21-8). For 
example, you can conﬁ gure the options to use a custom error conﬁ guration, which converts 
key errors to an unknown record, rather than terminating the processing. In addition, you 
can specify error limits and what action to take when those limits have been exceeded. Last, 
you can choose to handle speciﬁ c error conditions such as “key not found” or duplicate keys 
by reporting and continuing to process, by ignoring the error and continuing to process, 
and by reporting and stopping the processing. Using these settings typically means you 
have data issues and you should go back to the ETL to ﬁ x such issues because you’re simply 
masking issues. 

744  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Processing a Tabular Model
Although the action is the same to process a Tabular model as it is to process a MOLAP model, what 
happens behind the scenes is different. To process a Tabular model, right-click the object you want 
to process, and select Process in Management Studio (see Figure 21-9). Doing this goes back to the 
original data sources (SQL Server, Access, Excel, and so on) and wipes and loads the SSAS tables. 
FIGURE 21-8
FIGURE 21-9


746  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
storage options are covered a bit later, you need to know that available options to be included in the 
backup are the following: 
metadata that deﬁ nes all the objects 
aggregations calculated 
source data used to populate the objects
Now let’s review performing these back-up functions for Analysis Services databases. Again, you 
can use Management Studio to assist with the setup and conﬁ guration of these tasks and script the 
results to permit scheduling: 
 1. 
Open Management Studio, and connect to an Analysis Services server.
 2. 
Right-click an Analysis Services database, and select Backup. The Backup Database dialog 
appears. Here you can conﬁ gure the details of the backup such as applying compression, 
where the backup ﬁ le should be located, or whether the ﬁ le should be encrypted 
(see Figure 21-11). Storage types are covered later, but you get a clear statement of what 
information is part of the backup at the bottom of this dialog. Basically, the backup is 
backing up only the Analysis Services information (partitions, metadata, source data, and 
aggregations) available to a database based on the storage type.
➤
➤
➤
FIGURE 21-11
 3. 
Optionally, you can script the backup by pressing the Script button along the top of the dialog. 
The resulting script looks like the following example (including the password not shown here):
<Backup xmlns=“http://schemas.microsoft.com/analysisservices/2003/engine”>
   <Object>
     <DatabaseID>Adventureworks DW</DatabaseID>

Administering Analysis Services Databases ❘ 747
   </Object>
   <File>Adventureworks DW.abf</File>
   <Password>password</Password>
 </Backup>
Now that you have a backup of an Analysis Services database, turn your attention to the recovery 
of Analysis Services databases. Recovery takes a previously created backup ﬁ le (named with an 
.abf ﬁ le extension) and restores it to an Analysis Services database. Several options are available 
during this process: 
Using the original database name (or specifying a new database name).
Overwriting an existing database.
Including existing security information (or skipping security).
Changing the restoration folder for each partition.
Following are the steps needed to perform a recovery of the database: 
 1. 
Open Management Studio, and connect to an Analysis Services server.
 2. 
Right-click an Analysis Services database, and select Restore. The Restore Database dialog 
appears; here you can conﬁ gure the restoration details, such as including security or 
overwriting an existing database (see Figure 21-12).
➤
➤
➤
➤
FIGURE 21-12

748  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
 3. 
Optionally, you can script the restore by clicking the Script button along the top of the 
dialog. The resulting script would look like the following example (including, again, a poor 
practice of including the password):
 <Restore xmlns=“http://schemas.microsoft.com/analysisservices/2003/engine”>
   <File>C:\Program Files\Microsoft SQL
 Server\MSAS10.MSSQLSERVER\OLAP\Backup\Adventureworks DW.abf</File>
   <DatabaseName>Adventureworks DW</DatabaseName>
   <AllowOverwrite>true</AllowOverwrite>
   <Password>password</Password>
   <DbStorageLocation
 xmlns=“http://schemas.microsoft.com/analysisservices/2012/engine/100/100”>C:\
 Program Files\Microsoft SQL Server\MSAS10.MSSQLSERVER\OLAP\Data\
</DbStorageLocation>
 </Restore>
You can also click on the Partitions tab to change the storage location of each of the partitions. 
Synchronizing Analysis Services Databases
Another important activity to perform involves synchronizing Analysis Services databases from 
one server to another. This is usually done as a mechanism for deploying from a test or quality-
assurance server to a production server. This feature is attractive for this purpose because users 
can continue to browse the production cubes while the synchronization takes place. When 
the synchronization and processing completes, a user is automatically redirected to the newly 
synchronized copy of the database, and the older version is removed from the server. This differs 
greatly from what happens when you perform a deployment because part of the deployment usually 
involves processing of dimensions and cubes. As you may recall, certain types of processing of 
Analysis Services objects require that the cube be taken ofﬂ ine, making it unavailable to users until 
the processing completes.
As with many other database tasks, you can run the synchronization immediately from the wizard, 
or you can save the results of the selections to a script ﬁ le for later execution or scheduling.
To synchronize an Analysis Services database between servers, follow these steps: 
 1. 
Open Management Studio, and connect to the target Analysis Services server.
 2. 
On this target server, right-click the databases folder, and select Synchronize.
 3. 
On the Select Databases to Synchronize page, specify the source server and database; the 
destination server is hard-coded to the server from which you launched the synchronization.
 4. 
If applicable, on the Specify Locations for Local Partitions page, the source folder displays 
the folder name on the server that contains the local partition, whereas the destination folder 
can be changed to reﬂ ect the folder into which you want the database to be synchronized.
 5. 
If applicable, on the Specify Locations for Remote Partitions page, you can modify both 
the destination folder and server to reﬂ ect where you want the database to be synchronized. 
In addition, if the location has remote partitions contained in that location that need to be 
included in the synchronization, you must place a check beside the Sync option.

Analysis Services Performance Monitoring and Tuning ❘ 749
 6. 
On the Specify Query Criteria page, enter a value for the security deﬁ nitions and indicate 
whether compression should be used. The security options include copying all deﬁ nitions 
and membership information, skipping membership information but including the security 
deﬁ nitions, and ignoring all security and membership details.
 7. 
On the Select Synchronization Method page, you can either run the synchronization 
immediately or script to a ﬁ le for later use in scheduling the synchronization.
Processing is one of those topics where the lines between development and production DBAs blur. 
The same line becomes blurred with performance tuning a cube. The developer will likely perform 
the base performance tuning but will rely on the DBA to create long term performance tuning 
through aggregations. These are covered in the next section.
ANALYSIS SERVICES PERFORMANCE MONITORING AND TUNING
Successful use of Analysis Services requires continual monitoring of how user queries and other 
processes perform and make the required adjustments to improve their performance. The main tools 
for performing these tasks include the SQL Proﬁ ler, performance counters, and the Flight Recorder.
Monitoring Analysis Services Events
There are now two ways to monitor SQL Server and Analysis Services: SQL Server Proﬁ ler and 
xEvents. Proﬁ ler will eventually be removed from SQL Server in a future release, making xEvents 
the future. The main issue holding xEvents back for now is the user interface for xEvents needs 
some more development before it’s as easy to create as Proﬁ ler. The interface for xEvents resembles 
T-SQL for SQL Server or XMLA for Analysis Services. The events you trap are the same whether 
you use xEvents or Proﬁ ler.
Chapter 12 provides detailed coverage of how to use SQL Proﬁ ler, so here you learn what is important 
about using this tool for monitoring your Analysis Services events. The capabilities related to using 
SQL Proﬁ ler for Analysis Services were vastly improved in the 2005 release and are now quite useful 
for this purpose. With SQL Server Proﬁ ler, you can review what the server does during processing and 
query resolution. Especially important is the ability to record the data generated by proﬁ ling to either 
a database table or ﬁ le to review or replay it later to get a better understanding of what happened. 
You can also now either step through the events that were recorded or replay them as they originally 
occurred. Last, you can place the events side by side with any machine or SSAS performance counters 
to spot trends affecting performance.
The main focus here is tracing the Analysis Services server activity and investigating the 
performance of the MDX queries submitted to the server to process user requests for information. 
The useful event categories include the following: 
Command events provide insight into the actual types of statements issued to perform actions.
Discovery events detail requests for metadata about server objects, including the Discovery 
Server State events (such as open connections).
➤
➤

750  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Error and Warning events show any errors or warnings being thrown by the instance 
of SSAS.
Query events trap queries being passed into SSAS.
Because of all the detail that a trace returns, use the Column Filter button to display only the 
activities sent to a speciﬁ c Analysis Services database. You can also use these traces to replay against 
other servers to see how your server will scale. This is covered in the next section.
Creating Traces for Replay
Traces are important because they enable you to determine various elements of status information 
for Analysis Services through certain counters. To start a trace, you must open Performance 
Monitor by either selecting Performance in Administrative Tools from the Control Panel or by 
typing PerfMon at the command prompt. Two types of counters are used within Performance 
Monitor: predeﬁ ned counters and user-deﬁ nes counters. The predeﬁ ned counters measure statistics 
for your server and process performance, whereas user-deﬁ ned counters are used to analyze events 
that may occur. Command-line tools such as LodCtr, LogMan, ReLog, TypePerf, and UnloadCtr 
also capture performance metrics.
To get a better idea of how to conﬁ gure these traces for replaying queries submitted to your Analysis 
Services server, start a trace of your own. 
 1. 
First open SQL Proﬁ ler and selecting File Á New Trace. When prompted, specify the 
Analysis Services server to connect to, and conﬁ gure trace properties to resemble what is 
shown in Figure 21-13.
➤
➤
FIGURE 21-13



Management of Analysis Services MOLAP model Storage ❘ 753
is query performance because all information needed to respond to queries is available 
without having to access the source data. Periodic processing is required to update the data 
stored within the structure, and this processing can be either incremental or full. As a result 
of processing the cube, data latency is introduced with this storage mode. Also as a result of 
this structure, storage requirements become much more important due to the volume 
of information that the system requires.
Relational OLAP: Indexed views within the data source of the ROLAP structure store the 
aggregations, whereas a copy of the source data is not stored within Analysis Service. With 
this mode, any queries that the query cache cannot answer must be passed on to the data 
source. That makes this storage mode slower than MOLAP or HOLAP (covered in the next 
bullet). The beneﬁ t is that users can view data in real or near-real time, and because a copy 
of the source data is not being stored within the structure, the storage requirements are 
lower than MOLAP.
Hybrid OLAP: As you might have guessed, the HOLAP storage mode is a combination of 
multidimensional OLAP and relational OLAP. This storage mode stores aggregations but 
does not store a copy of the source data. As a result, queries that access the aggregated 
values perform well, but those that do not have access perform slower. Also as a result, this 
storage mode requires far less storage space than MOLAP.
Partition Conﬁ guration
When you need to conﬁ gure partitions for your cubes to speed up access to your data, you have 
two primary tasks. First, you have the conﬁ guration of the storage of the partition, and second, 
you have the optimization of the partition by conﬁ guring aggregations. Partitions should be 
created for measure groups that are either larger than 4 gigabytes in size or about 15 million rows 
roughly. Otherwise, the cube performance will suffer. Aggregations are precalculated summaries 
of data, primarily employed so that query response time is made faster because the cube partition 
has prepared and saved the data in advance of its use. These aggregations are discussed in the next 
section but they are involved heavily with partitions as well. 
You should understand that in Analysis Services, storage is conﬁ gured separately for each partition 
of each measure group in a cube. This enables you to optimize your cube query strategies for each 
partition. An example would be to keep the current year’s data in one partition optimized for more 
detailed and narrow queries, while keeping older data in another partition optimized for broader 
aggregated queries.
You conﬁ gure your cube storage using either SSDT or, after deployment, Management Studio. 
Often, developers do not need to be involved with partitioning or conﬁ guration of storage; this 
scenario is a better ﬁ t for using Management Studio. The downside to this situation is that the 
Visual Studio project will not be updated to reﬂ ect the current storage settings, and you must take 
care during deployment that your selections made in Management Studio are not overwritten. 
Speciﬁ cally, you want to ensure that during deployment, when the Specify Options for Partitions 
and Roles page displays, you indicate that Partitions and Security should be maintained on the 
deployment target database and thus not overwritten by this deployment (refer to Figure 23-4).
➤
➤

754  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
When you deploy a cube for the ﬁ rst time, a measure group is set up to be entirely contained within 
a single partition, spanning the entire fact table used for the measures. With the BI and Enterprise 
Editions, you can change that to deﬁ ne multiple partitions by setting the StorageMode property 
for each partition. With Standard Edition, you can create up to three partitions. To set the storage 
options in SQL Server Management Studio for a MOLAP model, follow these steps: 
 1. 
Open Management Studio, and connect to the target Analysis Services server.
 2. 
In Object Explorer, right-click the cube that contains the partition, and click Properties 
to set storage options. If you have more than one partition, right-click a partition and 
click Properties.
 3. 
Select the Proactive Caching page.
 4. 
Select the Standard Setting radio button to accept the default storage settings for the 
storage type speciﬁ ed by the slider bar. Now you may move the slider bar to change the storage 
type from MOLAP to HOLAP and to ROLAP (see Figure 21-15). For the purpose of this 
example, select Scheduled MOLAP, and then click Options.
FIGURE 21-15
 5. 
Click the Options button to display the Storage Options dialog, as shown in Figure 21-16. 
This screen enables you to quickly conﬁ gure how the cube will be processed automatically. 

Management of Analysis Services MOLAP model Storage ❘ 755
If you check Enable Proactive Caching (available only in Enterprise Edition), the cube 
turns on the proactive caching option. For scheduled MOLAP, check Update the Cache 
Periodically to automatically build the cube once a day. With proactive caching, the cube 
remains online during the update. You can also have SSAS automatically detect changes in 
the underlying data warehouse and process as soon as the change is made, if you prefer to 
have more real-time analytics. Proactive caching is for SQL Server Enterprise Edition only 
in MOLAP model mode and does not scale if millions of rows are being updated every hour. 
Most people instead create a processing task in an SSIS package or a SQL Server Agent job 
to refresh their cube.
FIGURE 21-16
Designing Aggregations in the MOLAP model
Again, the primary role of aggregations is to precalculate summaries of the cube data so that user 
queries may be quickly answered. When a query cannot use an aggregation because it does not 
exist, Analysis Services must query the lowest level of details it has stored and sum the values. 
Aggregations are stored in a cube in cells at the intersection of the selected dimensions.


Management of Analysis Services MOLAP model Storage ❘ 757
 4. 
On the Select Partitions to Modify dialog, specify any partitions that are to be evaluated. 
You can either select all partitions for the measure group, or you can select combinations of 
individual partitions.
 5. 
On the Specify Query Criteria dialog, you can view query statistics for the selected measure 
group partition, including the total number of queries and the average response time for 
processing the queries. Optionally, you can set some limits to ﬁ lter the queries that you 
would like the optimization to consider, including an interesting option for ﬁ ltering the 
queries by users. Presumably, one notable use of this option would be to enable you to 
ensure that your executives’ queries are delivering the best response time.
 6. 
On the Review the Queries That Will Be Optimized dialog, you can view the speciﬁ c 
dimension member combinations under the client request column, the occurrences of those 
combinations, and the average duration of those combinations. At this point, you also have 
a column of check boxes beside each row that enables you to indicate that you do not want 
some of the suggested queries optimized.
 7. 
Under the Review Aggregation Usage page, select the default options. This screen gives you 
full control over how each dimension is aggregated.
 8. 
Specify the counts of various cube objects on the Specify Object Counts dialog by clicking 
the Count button.
 9. 
On the Set Aggregations Options dialog, 
specify how long the aggregations should 
be designed. Options to consider include 
designing aggregations until a speciﬁ ed 
amount of storage has been used, until a 
speciﬁ ed percentage of performance gain 
has been reached, or until you decide to 
stop the optimization process (see 
Figure 21-17). Because you base this 
optimization on real query statistics, 
you should consider optimizing until a 
performance gain of approximately 30 
percent has been attained. This translates 
loosely to optimizing 30 percent of the 
queries to use the aggregations, which is 
about 52KB in this example. If you build 
aggregations on a system that does not 
have them already, 30 percent is a good starting point. For Usage Based Optimization, you 
probably want to see this number at 100 percent, meaning that you want to tune all the 
queries that you ﬁ ltered on the previous screen.
 10. 
Last, the Completing the Wizard dialog displays, which you can use to review the partitions 
affected by the aggregation design and to indicate whether you would like the affected 
partitions to be processed immediately.
In Proﬁ ler, you can see whether aggregations are being used easily on a query-by-query basis. In 
the Progress Report End event class, you see an aggregation being read, versus a partition. There’s 
FIGURE 21-17

758  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
also an event you can trap in Proﬁ ler called Get Data from Aggregation that can show when an 
aggregation is being read from. 
You now have designed the storage of your Analysis Services cubes, set up partitions, and designed 
aggregations. In the next section, you learn how to apply security to Analysis Services.
APPLYING SECURITY TO ANALYSIS SERVICES IN THE 
MOLAP MODEL
Security within Analysis Services involves designing Active Directory user permissions to selected 
cubes, dimensions, cells, mining models, and data sources. Analysis Services relies on Microsoft 
Windows to authenticate users, and only authenticated users who have rights within Analysis 
Services can establish a connection to Analysis Services.
After a user connects to Analysis Services, the permissions that user has within Analysis Services are 
determined by the rights assigned to the Analysis Services roles to which that user belongs, either 
directly or through membership in a Windows role. The two roles available in Analysis Services are 
server roles and database roles.
Server Role
The server role permits unrestricted access to the server and all the objects contained on the server. 
This role also allows its members to administer security by assigning permissions to other users. By 
default, all members of the Administrators local group are members of the server role in Analysis 
Services and have serverwide permissions to perform any task. You conﬁ gure this access by using 
Management Studio, Business Intelligence Development Studio, or an XMLA script.
To add additional Windows users or groups to this Server role, follow these steps: 
 1. 
Open Management Studio, and connect to an Analysis Services server.
 2. 
Right-click the server node in Object Explorer, and choose Properties.
 3. 
On the Analysis Server Properties dialog, select the Security page. Note again that no users 
or groups are included automatically. Although it’s not shown in the dialog, only members 
of the local Windows Administrators group are automatically assigned this server role.
 4. 
Click the Add button, and add users and groups with the standard Windows Select Users 
and Groups dialog.
 5. 
After adding the users and groups, you can remove the local Administrators from the server 
role by selecting the General page and clicking the Show Advanced (ALL) Properties check 
box. Then set the Security\BuiltinAdminsAreServerAdmins property to False.
Database Role
Within Analysis Services, you can set up multiple database roles. Only the members of the server 
role are permitted to create these database roles within each database, grant administrative or user 
permissions to these database roles, and add Windows users and groups to these roles.

Applying Security to Analysis Services in the MOLAP model ❘ 759
These database roles have no administrative capabilities unless they are granted Full Control, 
otherwise known as administrator rights, or a more limited set of administrator rights (such as 
Process the Database, Process One or More Dimensions, and Read Database Metadata).
In summary, reading Analysis Services data is only available to members of the server role and 
members of a database role that have Full Control unless they have speciﬁ c rights to read a given 
dimension. Other users can get this access only if their database role expressly grants permissions to 
the objects in Analysis Services (dimensions, cubes, and cells).
You set up database roles using either SSDT or Management Studio. In SSDT, you use the Role 
Designer, whereas in Management Studio you use the Create Role dialog. When using Management 
Studio for setting up these roles, the changes do not require that you deploy the database because 
they are made in online mode.
Follow these steps to add a database role to an Analysis Services database using Management Studio: 
 1. 
Open Management Studio, and connect to an Analysis Services server.
 2. 
Right-click the Roles folder located in one of the databases, and select New Role.
 3. 
On the Create Role dialog (see Figure 21-18), enter Data Admin as the role name, and check 
the Full control (Administrator) check box. This automatically checks every other box. By 
checking this, you’ve given any user in the role full control of your database.
FIGURE 21-18
 4. 
Select the Membership page, and add a Windows user account.


Applying Security to Analysis Services in the MOLAP model ❘ 761
When access has been modiﬁ ed for the dimensions, you need to deﬁ ne the speciﬁ c attribute 
hierarchies and members within the dimension to which role members are allowed access. If you 
forget to do this, the role will not have permission to view any attribute hierarchies within the 
dimension, nor any of their members. For example, you can permit a sales manager access to only 
the Customer promotions in the Dimension Data page and selecting the Promotion dimension 
from the Dimension combo box. Then, select the Promotion Category as the attribute hierarchy and 
check only the Customer member. You also need to select Deselect All Members (see Figure 21-20) 
so that as new members are added, the user is automatically denied rights to those members.
FIGURE 21-20
You may also encounter a security conﬁ guration that requires even more sophistication, and for that 
you have the Advanced tab on the Dimension Data page of the Create Role dialog. This tab enables 
the creation of complex combinations of allowed and denied listings of dimension members, along 
with conﬁ guration of default members for your role. Here is where administrators may need to 
work with developers to understand the multidimensional expression language (MDX) syntax that 
would be required to conﬁ gure these advanced security options. If you do want there to always be a 
ﬁ lter on the Customer promotion, select Enable Visual Totals (see Figure 21-21). By doing this, even 
if the promotion dimension has not been used in the query, the member is ﬁ ltered out automatically. 
Previously you would have to use the Promotion Category attribute to see the security ﬁ lter.

762  ❘  CHAPTER 21  ANALYSIS SERVICES ADMINISTRATION AND PERFORMANCE TUNING
Applying Security to Analysis Services in the Tabular Model
The Tabular model operates much the same way as the MOLAP model for security but has fewer 
options and less ﬂ exibility. You can specify database-level permissions, add the users to the role, 
and then specify any members that you want to ﬁ lter. To create a new Tabular role, follow these 
following steps:
 1. 
Open Management Studio, and connect to an Analysis Services server in Tabular mode.
 2. 
Right-click the Roles folder located in one of the databases, and select New Role.
 3. 
In the General tab, name the role. If you want users to be administrators, check Full 
Control. If you want users to have selective rights, check Read.
 4. 
In the membership tab, select the Active Directory groups or users that you want to inherit 
this role.
 5. 
You can apply a ﬁ lter to ensure your users of this role can see only certain rows by using 
a Data Analysis Expression (DAX) Filter in the Row Filters tab. A DAX ﬁ lter is actually a 
simple language to learn and would look like Figure 21-22, which allows the user to see 
only U.S.-based sales. 
FIGURE 21-21

Summary ❘ 763
SUMMARY
You covered a lot of ground in understanding the various administrative functions related to Analysis 
Services. There are two types of models in SSAS: MOLAP and Tabular model. Administration 
for the new SQL Server 2012 Tabular model is similar to the MOLAP model, but the processing 
involves refreshing the entire table. Aggregations are used to performance tune a cube based on 
query patterns. Various security administration tasks include creating database roles and assigning 
granular permissions to the roles. Now that you’ve learned about administering the Business 
Integration services, you can move on to Chapter 22, which discusses how to administer the 
Reporting Services (SSRS) in SQL Server 2012. 
FIGURE 21-22



version number. The name of the database containing the reporting items is also shown. In the 
example shown in Figure 22-3, the default of ReportServer was taken. 
You can see the service runs in Native mode. SQL Server Reporting Services can also run in 
SharePoint Integrated mode. Starting with 2012, conﬁ guration of Reporting Services running 
in SharePoint Integrated mode should be done through the SharePoint console, not the SSRS 
conﬁ guration manager.
In SharePoint integrated mode, SharePoint manages all aspects of reporting. Reports are uploaded 
to and stored in SharePoint. Security is controlled within SharePoint. In addition, features of 
SharePoint libraries such as version control and alerts are supported with reports. 
There are some disadvantages to running SSRS in integrated mode however; items such as custom 
security extensions, the ability to manage reports within Report Manager, and the ability to use 
the Reporting Services Conﬁ guration Manager are not supported when running in SharePoint 
integrated mode. 
FIGURE 22-3
SQL Server Reporting Services Conﬁ guration Manager ❘ 767

768  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
A discussion about how to conﬁ gure with SharePoint console is outside the scope of this chapter, 
but you can ﬁ ne more information on this topic at http://msdn.microsoft.com/en-us/library/
bb326356.aspx. 
Finally, you occasionally have changes that prompt you to recycle SQL Server Reporting Services. 
You can do so by starting and stopping the service, as shown in Figure 22-3. 
The Service Account
The Report Server service account is the user account under which Reporting Services run. 
Although initially conﬁ gured during the installation of SQL Server, it can be updated or modiﬁ ed 
on this screen (shown in Figure 22-4). It is under the service account that the Report Server web 
service, Report Manager, and background process tasks all run. 
FIGURE 22-4
Should you decide to change the account, you have two options: 
Built-in Account: With Windows Server 2008 and previous versions, you may opt to use 
one of the built-in accounts, such as Network Service, Local System, or Local Service. Of 
these, Microsoft recommends the use of the Network Service account. 
➤


770  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
All these actions require you to use an application to interface to the web service. Some of the 
applications from Microsoft that can interact with the web service are Report Builder, SQL Server 
Data Tools, and Report Manager. You may also write your own application that interacts with the 
web service using one of the many .Net languages available.
To customize the web service URL for this instance of SSRS, use the Web Service URL page, as 
shown in Figure 22-5. The default URL is the name of the server, followed by the port number and 
then the virtual directory ReportServer, all of which are conﬁ gurable.
You can have more than one Web Service URL exposed by Reporting Services. To do so, use the 
Advanced button and add additional URLs to this server. Reporting Services also supports secure 
connections via SSL. Simply specify the SSL certiﬁ cate and port number. 
One of the ﬁ rst places you’ll likely use the web service URL is within SQL Server Data Tools (SSDT, 
also known as Business Intelligence Developer Studio [BIDS] in former versions of SQL Server). 
From within SSDT you can deploy your reports to the report server. To do so you ﬁ rst must go to 
the properties page for the report project. One of the properties is the target server; it is the Web 
Service URL you use for this property.
FIGURE 22-5

Figure 22-6 shows an example of what you 
see if you try to navigate to the web service 
URL from within your web browser. It is a 
simple listing of the reports, with links that 
enable you to open or drill down into folders. 
This can be a good way to validate the web 
service URLs you use in your applications 
but it is a terrible interface for the average 
user. Fortunately, Microsoft provides a 
complete solution for users to interface with 
SSRS in the form of Report Manager, which 
is covered in-depth in the “Report Manager” 
section later in this chapter. 
Reporting Services Databases
Reporting Services requires two databases to do its work. You can create these during the 
installation of Reporting Services or afterward using the Reporting Services Conﬁ guration 
Manager. By default, these databases are named ReportServer and ReportServerTempDB, although 
these may be renamed during their creation. Figure 22-7 shows the conﬁ guration screen for setting 
up these databases.
FIGURE 22-6
FIGURE 22-7
SQL Server Reporting Services Conﬁ guration Manager ❘ 771

772  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
In Figure 22-8, you can see these databases are visible inside SQL 
Server Management Studio. 
The ReportServer database stores all the reports, as well as data 
source information, logins, subscriptions, and more. All sensitive 
data is encrypted using the encryption keys, a topic covered in the 
“Encryption Keys” section of this chapter. 
As its name implies, ReportServerTempDB holds temporary information for Reporting Services. 
Cached reports are one example of an entity that is stored. If you had a report that was used by 
many users — perhaps a morning status report — you could have that report generated and cached 
so that the report accesses only the source database once. Users executing the report would see the 
cached report, instead of having to completely regenerate it on each execution. When the cache time 
expires, the report is ﬂ ushed from the ReportServerTempDB cache. 
Important distinctions exist between the two databases you need to be aware of. It’s vital that the 
ReportServer database be backed up because it contains all your information about the reports 
hosted on this instance of SQL Server Reporting Services. If a restore is needed, you can restore this 
database much as you would any other. After restoring it, you need to restore the encryption keys 
for the encrypted information to be decipherable by Reporting Services.
The ReportServerTempDB is quite the opposite. All data within the temporary database can 
be deleted without permanent damage to your SSRS installation. It would require, however, 
all reports with caching enabled to be rerun and cached because the cache would be lost if 
ReportServerTempDB goes away. 
For disaster recovery purposes you have two options. First, you can chose to back up ReportServerTempDB 
along with ReportServer. However, the temporary database typically grows fairly large and can 
consume a lot of time and disc space in backups. For that reason many users go with the option to 
generate a script to create ReportServerTempDB. If a recovery occurs then the script is run, and if 
necessary any reports that need to be cached can be executed. To create a script, simply right-click the 
ReportServerTempDB database name, select Script Database As Á Create To Á File, and save the output. 
The names ReportServer and ReportServerTempDB are the default names for these databases. If multiple 
instances of SSRS are installed on the same server, the default names are appended with an underscore 
and then the instance name. As a best practice, you should retain these names because that is what most 
SQL IT Professionals are accustomed to. You can, though, change these if the need arises.
To change the database, simply click the Change Database button. This brings up a wizard that 
provides two options: connect to an existing database or create a new database. After you select 
your option, the wizard walks you through a series of questions common to both choices. You select 
the server for the databases, the name of the database, the credentials to use, and so forth. When 
done, Reporting Services now uses the database you indicated in the wizard. 
Although it’s not uncommon to have the ReportServer and ReportServerTempDB databases on 
the same SQL Server that Reporting Services runs on, it is not required. You can elect to put the 
reporting databases on a separate server and have only SSRS run on the report server. 
This ﬂ exibility is commonly used with a scale-out deployment. You can implement two topologies 
in this scenario. In the ﬁ rst, the ReportServer and ReportServerTempDB databases reside on a server 
FIGURE 22-8

containing a SQL Server database engine. Then, two servers are created that run only SSRS. Both 
point to the ReportServer and ReportServerTempDB databases on the database’s ﬁ rst server.
The second is a slight variation that has only two servers. The ﬁ rst server holds both the databases 
and SSRS; the second runs only SSRS and points back to the ﬁ rst server for the ReportServer and 
ReportServerTempDB databases. 
Although these are the most common two setups, you are not limited to only two SSRS servers 
in a scale-out situation. You could simply conﬁ gure the additional servers to point to the central 
ReportServer and ReportServerTempDB databases. See the “Scale-Out Deployment” section later in 
this chapter for more information. 
In a single server environment, if you chose to install and conﬁ gure Reporting Services during the 
install of SQL Server, you generally won’t need to alter the information in Figure 22-7. You need to 
understand how the reporting databases are used though, and how they can be conﬁ gured within 
your server environment.
The Report Manager URL
SQL Server Reporting Services ships with an interface called Report Manager, which enables users 
to upload, conﬁ gure, and run reports. Through Report Manager you can apply security to a speciﬁ c 
user or to a group. Permissions can vary from as basic as having only the ability to run certain 
reports, to having full administrative control over the report server. 
By default the URL is the name of the server, followed by the default port of 80, followed by the 
virtual directory name of Reports. This URL is conﬁ gurable and may be changed on the page as 
seen in Figure 22-9. 
FIGURE 22-9
SQL Server Reporting Services Conﬁ guration Manager ❘ 773

774  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
As with the Web Service URL, you can specify multiple URLs using the Advanced button of the 
Report Services Conﬁ guration Manager. For example, you may want to have one standard http style 
URL for internal use and a Secure Socket Layer (SSL) version of the address (https) for external use. 
Report Manager is a big topic and is covered in-depth in the “Report Manager” section later in 
this chapter. 
E-mail Settings
One of the options Reporting Services provides is the ability to set up e-mail–based subscriptions for 
reports. Users may elect to have reports automatically generated and e-mailed to them. To support 
this, Reporting Services must have access to an e-mail account. Figure 22-10 shows where to enter 
your e-mail information. 
As with most accounts used with servers, you must ensure the account has a non-expiring password 
and that it has rights to e-mail the various attachment types supported by SQL Server Reporting 
Services. You need to take care with this ability however. Some reports can become quite large 
and ﬂ ood your e-mail servers with attachments. Consider having reports generated and stored in a 
central repository and instead e-mail links to the reports. 
FIGURE 22-10


776  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Encryption Keys
Reporting Services requires a good deal of conﬁ dential information to do its job. Credentials, 
connection strings, and the like must be stored in a secure manner. This kind of sensitive data is 
stored in the Reporting Services report database, but before it is stored it is encrypted using an 
encryption key. Encryption keys are managed using the screen shown in Figure 22-12. 
It is vital that this encryption key be backed up. If the Reporting Services database needs to be 
restored, either from a crash or moving the instance to a new server, you need to restore the 
encryption keys. Without doing so all the conﬁ dential information stored is unusable, and you are 
faced with the laborious task of re-creating all the credentials manually. 
After you restore the report database, you can restore the encryption keys through the dialog, as 
shown in Figure 22-12. When restored Reporting Services again has the capability to properly 
decrypt the stored credentials, thus restoring the server to full functionality. 
FIGURE 22-12
There may be occasions in which you want to change the security credentials. For example, you 
might have lost the backup of the encryption key which would present a potential security issue. 
Or you may have corporate rules requiring periodic refreshes of all your encryption keys. The 
change command in Figure 22-12 creates a new security key. Of course after changing the key, be 
sure to back it up. 
You may also have the need to remove any sensitive data stored in the report database. You can use 
the Delete function to remove any conﬁ dential information stored by Reporting Services. 


778  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
 6. 
Simply select the new server, and then click the Add Server button. After it has joined, you 
can verify it by going to the Report Services or Report Manager URLs speciﬁ ed when you 
conﬁ gured the new server. 
Reporting Services also supports being installed on a network load- balanced cluster. If you do 
so, you must conﬁ gure a few additional items. For more information, see the Books Online article 
“Conﬁ gure a Report Server on a Network Load Balancing Cluster.”
REPORT SERVER PROPERTIES
After you have SQL Server Reporting Services installed and 
conﬁ gured, you may want to alter several properties. To do so, perform 
the following steps: 
 1. 
Start by launching SQL Server Management Studio. In the 
Object Explorer, click the Connect menu button; then in the 
list select Reporting Services, as shown in Figure 22-14.
 2. 
In the connection dialog, ensure the server is correct as well as the authentication method; 
then click the Connect button. 
FIGURE 22-13
FIGURE 22-14

 3. 
When you connect, the Reporting 
Services server appears in Object 
Explorer. To access the properties, 
right-click the server name, and select 
Properties, as shown in Figure 22-15.
General Properties Page
The server properties dialog has a number of 
pages, listed on the left side, that enable you 
to explore and alter the settings within the 
Reporting Services server. Start by looking 
at the General tab, as shown in Figure 22-16. The General tab has a variety of options which are 
explained in the following list:
FIGURE 22-15
FIGURE 22-16
The Name property is the name of the server displayed in Report Manager. As you’ll see in 
the “Report Manager” section later in this chapter, the Name you enter into this property 
is used as the site title in Report Manager. Select a name that would be meaningful to the 
users, such as the name of your company or perhaps the department or application the SSRS 
server will be used by.
The Version and Edition properties are read-only and simply display the information about 
the version of SQL Server Reporting Services. 
➤
➤
Report Server Properties ❘ 779

780  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Authentication Mode is another read-only property, which indicates the types of 
authentication modes that will be accepted by this SSRS server. 
The URL is the http address for the web service under which SSRS can be accessed. The 
URL is set in the Reporting Services Conﬁ guration Manager, as shown in the previous 
section of this chapter. 
Moving down the dialog, you see a check box to enable a My Reports folder for each user. 
Within the Report Manager each user can have her own folder to work with. Named My 
Reports within Report Manager, a user can have full control to upload new reports, update 
existing ones, schedule them to run or execute them at will, and more. 
If you choose to enable the My Reports feature, the Select the Role to Apply to Each My 
Reports Folder drop-down becomes enabled. This sets the permissions users have within 
their My Reports folder. Permissions are discussed more in the upcoming section on Report 
Manager. 
The ﬁ nal option on the dialog in Figure 22-16 enables or disables the ability to download 
a special ActiveX print control. This special control enables features such as print preview, 
controlling page margins, and other items common to print dialogs. 
Execution Properties Page
The Execution page, as shown in Figure 22-17, controls how long a report can run before Reporting 
Services halts execution of the report. It has two choices; the ﬁ rst is Do Not Timeout Report 
Execution. Although this may seem like a good option, it is quite dangerous. If a report goes awry, 
it can remain in memory using resources until the Reporting Services server is ﬁ nally restarted. 
➤
➤
➤
➤
FIGURE 22-17

A better option is the default Limit Report Execution to the Following Number of Seconds. By 
default reports can run up to 1800 seconds (30 minutes) before SSRS halts execution of the report. 
In most situations this will be plenty of time, but it is not uncommon to have some reports that take 
longer to run. If so, you can increase the allowed run time through this setting. 
History Properties Page
For any report, users can elect to have Reporting Services take a snapshot of that report and store 
it for historical purposes. Users can see each time a report runs, along with a copy of the report 
including the data associated with that particular run of the report. 
Although this is a useful feature for auditing purposes, it can cause the ReportServer database to swell 
quickly if done on many reports. Through the History Properties Page, as shown in Figure 22-18, you 
can control how many historical copies of reports are maintained. 
By default, SQL Server Report Services retains all snapshots of a report in history. You can instead 
select to limit the number of copies by choosing that option and setting a value for the number of 
copies to retain. 
FIGURE 22-18
Logging Properties Page
Metrics are invaluable to a DBA in managing SQL Server. SSRS provides a rich set of information 
through its logging mechanism. Data such as the name of the report, who ran the report, how 
long the report took to execute, when the report ran, and more are exposed through views in the 
ReportServer database. 
Report Server Properties ❘ 781

782  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
The next section examines the views in Report Builder, using them to build a useful report. For 
now, understand that you can control whether logging is done using the Logging Properties Page, as 
shown in Figure 22-19. 
This dialog has two basic options. The ﬁ rst determines whether SQL Server Reporting Services does 
logging. The second option sets the number of days for which logs are retained. The default is 60 days.
Security Properties Page
There are two options for working with security at the Reporting Services server level, as shown 
in Figure 22-20. The ﬁ rst speciﬁ es whether a report is allowed to connect to its data source using 
the security credentials of the user trying to run the report. When disabled, users can either supply 
credentials manually (the report must have the credentials stored with it) or no authorization needs 
to be given to allow access to the source data. 
FIGURE 22-19

FIGURE 22-20
The second option, Enable Ad Hoc Reporting, sets whether a user can perform ad hoc queries from 
a Report Builder report when new reports are automatically generated when a user clicks data. 
Leaving this enabled can be a security risk. Turning it off mitigates denial-of-service attacks from 
hackers attempting to overload the report server. 
Advanced Properties Page
The ﬁ nal page of the Reporting Services server properties is the Advanced Page. As you can see in 
Figure 22-21, this page provides a single place to conﬁ gure all the properties. Many of these may 
also be set on the previous pages. For example, the ﬁ rst line, EnableMyReports, may also be set on 
the General Properties Page. 
Report Server Properties ❘ 783

784  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
For details on each individual option, see Books Online at http://technet.microsoft.com/en-
us/library/bb934303(SQL.110).aspx.
THE REPORT EXECUTION LOG
In the previous section you saw the option to turn execution logging on and off. In this section, you 
see what is in the report execution log. To get started, follow these steps: 
 1. 
Open SQL Server Management Studio, and connect to the Database Engine where the 
ReportServer database is stored. By default the database has the name ReportServer, and 
that’s how you can refer to it here. However, if you changed the name during the install or 
using the Reporting Services Conﬁ guration Manager, select the database you named. 
 2. 
Within the Views branch are three views directly related to logging: ExecutionLog, 
ExecutionLog2, and ExecutionLog3. Each contains the same basic information — the ID 
of the report, the server instance the report ran on, how long it took to run the report, who 
ran the report, and so forth. The second and third versions of the view extend the amount 
of information returned. Figure 22-22 shows an example of the three views, with the ﬁ rst 
one expanded to show its columns. 
FIGURE 22-21



 1. 
To start you ﬁ rst need to obtain a copy of Report Builder. It’s a free download from 
Microsoft at http://www.microsoft.com/download/en/details.aspx?displaylang=
en&id=6116. Alternatively, you can launch Report Builder from within Report Manager. 
For standard users this can be a simple way to access Report Builder. As an IT Professional, 
however, downloading and installing allows you to create reports and run them independent 
of Report Manager. 
 2. 
After downloading Report Builder, 
begin the install process. Most of the 
questions are simple, and you can take 
the defaults. The one screen you should 
be aware of is shown in Figure 22-24. 
It asks for the URL to the SSRS web 
service. This is optional, but if you 
ﬁ ll it in now it can make deployment 
easier later. If you’ve forgotten the 
address, you can look it up using the 
Reporting Services Conﬁ guration 
Manager’s Web Service URL, discussed 
in that section earlier in this chapter.  
After ﬁ lling in the URL to use as the 
default, or deciding to leave it blank, 
you can ﬁ nish the install wizard. 
 3. 
Now that you have Report Builder 
installed, it’s time to create your ﬁ rst 
report. Before you launch the application 
though, there’s something important 
you need to be aware of. If you run 
Report Builder on a Windows server, 
you get an Insufﬁ cient Rights error 
when you try to preview the report. To 
prevent this, simply run Report Builder 
in administrator mode. Open the Start 
Menu and navigate to the Report 
Builder menu item. Right-click Report 
Builder, and select Run as Administrator, 
as shown in Figure 22-25.
After Report Builder ﬁ res up and connects 
to the Reporting Services server, the opening 
screen displays (Figure 22-26). You can pick 
from quite a variety of options. In addition to 
creating a new report, you can also create a 
new dataset. A dataset is just what it sounds like; it deﬁ nes a set of data that will be returned from 
a server. Think of it as the combination of a SQL statement along with the connection string needed 
FIGURE 22-24
FIGURE 22-25
Report Builder ❘ 787

788  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
to talk to the server. When you create a dataset, it can be shared across several reports. You also 
have the ability to open a report that already exists, and a short cut to view reports you recently 
edited. Now create a new report. 
Within the options for creating a new report, you can create several types of reports. The Table or 
Matrix report creates a traditional text based report. The Chart Wizard creates a report containing 
one or more charts. If you need to display data geographically, the Map Wizard is for you. Should 
you want a blank slate to add report parts to manually, you can create a blank report. 
For this simple over, use the Table or Matrix Wizard. To create a report in this manner, perform the 
following steps: 
 1. 
Select the Table or Matrix Wizard, as shown in Figure 22-26. 
FIGURE 22-26
 2. 
On the next screen of the wizard, shown in Figure 22-27, you are asked what dataset you 
want to use to populate your new report. To deﬁ ne where you want to get the data from, 
you need to deﬁ ne the data source, as shown in Figure 22-28. 

FIGURE 22-27
FIGURE 22-28
Report Builder ❘ 789

790  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
 3. 
Start by giving the data source a descriptive name. Because this report gets its data from the 
reporting services event log, call it ReportServer. You can leave the connection type at 
the default of SQL Server; be aware though you can create reports from a wide variety of 
data sources. 
 4. 
Next deﬁ ne the connection string. To make it easy, you can click the Build button to 
bring up a dialog that lets you pick the server, username, and database name all from 
drop-downs. 
 5. 
For this report, pick the server where you installed the Reporting Services ReportServer 
database, and use the ReportServer database as the source. It’s always a good idea to test 
the connection by clicking the Test Connection button. When complete, your dialog should 
resemble the one shown in Figure 22-28. Click OK to move to the next stage. 
 6. 
You are now asked what connection you want to use for the new data source. The data 
connection you just created should appear in the list; select it and click Next. 
 7. 
The next screen in the dialog, as shown in Figure 22-29, provides an easy way for end users 
to drill down into the available tables and views and select the columns they want to appear 
on the report. This is a great option for those unaccustomed to writing SQL. As a DBA 
you don’t have that limitation, so instead you can use the query you created in the previous 
section on the execution log. If you saved the query, you may simply click the Import button 
and pick the .SQL ﬁ le you saved the query to. If not, you can paste or type it in. (Refer to 
Listing 22-1 for the query). 
FIGURE 22-29

 8. 
After entering the query, it is a good idea to run it, just to validate that there were no typos 
or other errors. To test, simply click the red exclamation mark at the top of the dialog. 
Figure 22-30 shows the dialog with the query entered and tested. If the results look correct, 
click Next to continue. 
FIGURE 22-30
 9. 
Next you are asked to arrange the ﬁ elds in the order you want them. After selecting the 
ﬁ elds to display on the report, you can group them by rows and columns. A common need 
for this report might be to group by instance name. Or, you may want to group by the 
report name, as you do in this example. 
 10. 
Drag the ReportName ﬁ eld into the area box Row groups. Next, drag TimeStart, TimeEnd, 
TimeDataRetrievalMilliseconds, TimeProcessingMilliseconds, TimeRenderingMilliseconds, 
ByteCount, and RowCount ﬁ elds into the Values box. You are not required to use every 
ﬁ eld in your data source; for example, you can leave the other ﬁ elds in the Available box. 
When your screen resembles the one shown in Figure 22-31, click Next.
Report Builder ❘ 791
 
 
 
 

792  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
 11. 
You are now asked how you want the report to be laid out (see Figure 22-32). You can 
choose to have totals, and if so you can indicate where you want them displayed. You can 
also allow the user to interact with the report by expanding and collapsing row and column 
groups. For this report, seeing the total execution time for a report might be useful, so 
leave the subtotals and grand totals displayed. There probably won’t be much use in having 
expanding/collapsing groups, so uncheck that option and continue. 
FIGURE 22-31
FIGURE 22-32

 12. 
The ﬁ nal step in the wizard gives you the opportunity to stylize the report. The styles, 
shown in the left column on Figure 22-33, apply certain colorations and fonts to the report. 
If you don’t want any special style, pick the Generic option for plain black text on a white 
background. For this report use the default of Ocean, but you can select one of the other 
colors if you want. When you have a color you like, click Finish to generate the report. 
Now that the report has been generated, take just a moment to look around Report Builder. 
Figure 22-34 shows the generated report inside the Report Builder application. 
Across the top is the toolbar. There are three tabs: Home, Insert, and View. The Home toolbar 
enables you to adjust basic things such as fonts, borders, formatting numbers, and the like. The 
Insert tab enables you to add additional components to the report. You can add data-driven items 
such as charts and graphs, tables, static items such as text boxes and lines, and subreports and 
report parts. The View tab is simple; it acts as a place to toggle the display of various areas of the 
Report Builder application. You can hide or display the ruler across the top, as well as the Row and 
Column groups at the bottom. You can also hide the Report Data area to the left. 
In the Report Data area, you can add images, new data sources, or grab ﬁ elds that you didn’t 
originally put on the report. You can also add parameters, so users can narrow down the data they 
want to see, as well as access built-in ﬁ elds such as the page number or report run time. 
FIGURE 22-33
Report Builder ❘ 793

794  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Now that the report is completed, you can test it before deploying it to your server. On the Home 
tab, click the Run button. The report executes, and you should see results similar to those in 
Figure 22-35. (If you didn’t run Report Builder as an administrator, this is where you would see 
the insufﬁ cient rights error.)
FIGURE 22-34
FIGURE 22-35

Assuming everything went well, you should 
now save your hard work. Clicking the Save 
icon in the upper-left corner of Report Builder 
opens your save dialog. From here, you can 
either save to the local hard drive or to the 
Reporting Services server. 
To save to the server, use the web services 
URL as described in the section, “Reporting 
Services Conﬁ guration Manager.” Give the 
report a good name; for this example use 
Reporting Services Execution Log.rdl, 
as shown in Figure 22-36. 
Report Builder is a powerful tool; you just 
scratched the surface of what it can do. For 
more information on Report Builder, see 
John Wiley’s Professional SQL Server 2012 
Reporting Services (Turley et al., 2012). Explore its capabilities; it can be a useful tool to you as 
a DBA. With it you can create reports to diagnose the health of the server and have those reports 
generated and sent to you every day, as you see in the next section on Report Manager. 
REPORT MANAGER
In the previous section you learned how to use Report Builder to generate reports. After a report 
generates, it is ready to be passed to SQL Server Reporting Services so it can be managed. You need 
to use the Report Manager tool to do this. 
The Report Manager is a web interface that both IT professionals and end users can use to manage 
and execute their collection of reports. You get to the Report Manager by opening Internet Explorer 
(or similar web browser) and going to the URL speciﬁ ed on the Report Manager URL page of the 
Reporting Services Conﬁ guration Manager. From here you can do a lot, but speciﬁ cally you can 
perform three main types of tasks:
From within SQL Server Data Tools you can organize reports into report projects. You can 
then deploy these reports from within SQL Server Data Tools to the Reporting Services 
server via the web services URL designated in the Reporting Services Conﬁ guration 
Manager. 
As described in the previous section, you can save reports from Report Builder directly to 
the Reporting Services server via the web service URL. 
Report Manager has the capability to upload a report to it from a disc drive. 
As mentioned earlier, SQL Server Data Tools is too big a subject to cover here. In the previous 
section you saw how to save a report from Report Builder; in this section you take a look at 
uploading and managing a report within Report Manager. Before that though, you need to see how 
to manage Report Manager. 
➤
➤
➤
FIGURE 22-36
Report Manager ❘ 795

796  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Managing Report Manager
Report Manager has the same quirk as Report Builder; for all the features to work correctly, you 
must run it in administrator mode. Just as you did with Report Builder, ﬁ nd the Internet Explorer 
icon on your Start menu, right-click it, and select Run as Administrator. 
When IE is open, navigate to the Report Manager URL. Your screen should be similar to the one 
shown in Figure 22-37; although, don’t be alarmed if it doesn’t match exactly. The Figure examples 
in this section have a few extra reports and folders for demo purposes, in addition to the Reporting 
Services Execution Log you created in the previous section on Report Builder.  
The Navigation Menu
In the upper-right corner of Figure 22-37 is a navigation menu. The Home option brings you back 
to the home page, as shown in Figure 22-37. My Subscriptions brings users to their private report 
storage area. If My Subscriptions is turned off, this does not appear to the users.
The Site Settings menu option is for SSRS administrators only and is not visible to anyone but 
designated admins. The available settings, as shown in Figure 22-38, are a subset of the properties 
you can set from within SQL Server Management Studio. (See the section “Report Server 
Properties” for more information.) The Help option provides some basic help for the Report 
Manager. 
FIGURE 22-37

In the Site Settings, you should change the server name to something appropriate for your 
environment, if you have not done so already. Typically, this is the name of your company, but it 
may also be a departmental name. For test or development servers, it’s a good practice to show that 
in the name of the server. 
After you change the name of your server, or any of the other properties, you must click the Apply 
button, as shown in Figure 22-39. This is true not only for this page, but also for all the pages in 
Report Manager. Anywhere you make a change, you must click the Apply button, or your changes 
will be lost. 
FIGURE 22-38
FIGURE 22-39
Report Manager ❘ 797

798  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
So far you’ve been looking at the General 
page of Site Settings, as indicated by the page 
menu on the left. Under it is another 
important page, Security. Through it you 
can add or edit users (or groups of users). 
In the screen snippet shown in Figure 22-40, 
you can see the two users listed for this 
server and the roles they have. 
To add a new user, simply click the New Role Assignment button. A new page displays (see 
Figure 22-41); start by entering the Active Directory user or group name, and then check the role 
they should have. Choose from the two following User options: 
System User role: This is fairly straightforward. It allows the designated users to see but 
not alter system properties and shared schedules. They are also allowed to launch Report 
Builder. 
System Administrator: This should be given out only with careful thought. System 
Administrators can change any of the properties, add/remove users, reports, and so on. This 
role should be reserved for DBAs or similar IT Professionals. 
➤
➤
FIGURE 22-40
The Schedules page is used to establish a shared schedule for capturing snapshots of reports. Say 
you have three reports that have important ﬁ nancial information about the company. These reports 
may run several times a day, independent of each other. 
For auditing purposes you need to run all three reports at the same time every day and store a 
snapshot of these. To accomplish this, you can ﬁ rst set up a shared schedule. Simply click the New 
Schedule menu option, ﬁ ll out what time of day to run, what days to run, and when to start and 
optionally stop running the report and save it. Figure 22-42 shows one schedule named Daily 
Corporate Snapshot. Remember this as you see where to use this later in the chapter. 
FIGURE 22-41
FIGURE 22-42

The Task Menu
Returning to the home screen (refer to Figure 22-37) there is a more prominent task menu across 
the screen. Starting from the left is the New Folder option. Report Manager enables reports to be 
organized into logical folders, similar to the way folders are used on a hard drive. When you move 
into a folder, the second line in the title will be updated to show the current folder name. 
Figure 22-43 reﬂ ects the new name of your 
SSRS server and shows that you have moved 
into the Accounting Reports folder. This 
example also lets the user know there are no 
reports in the folder, a much better option 
than showing nothing and making the user 
wonder. 
Next to the New Folder menu option is New Data Source. When you created your report in Report 
Builder, you started by building a data set. In creating that data set, the ﬁ rst thing you were asked 
was the data source. For your report you created the source and stored it within the report. 
You had the option to use a shared data source. You can create a shared data source through the 
New Data Source option in Report Manager. Then you simply point to it from Report Builder. 
Shared data sources make management much easier. If you have a group of reports that all connect 
to the same database, and that database is moved to a new server, the connection has to be updated 
only once, rather than for each report. Shared data sources also facilitate report development. You 
can set up a testing SSRS server, on which the data source points to a test database. When the report 
has passed all its tests, it can be uploaded to the production server that has a shared data source 
with the same name, but pointing to the 
production database. No update to the report 
would be required. 
Setting up a data source, although 
straightforward, does require some 
knowledge about how SSRS works and how 
your reports will be used. As Figure 22-44 
shows, you should start things by giving your 
data source a descriptive name. You can add 
an optional description to add clarity. 
The home screen (refer to Figure 22-37), is 
displayed in Tile Mode. This is the mode most 
users use to interact with Report Manager. 
(There is a second mode, Details View, which 
is covered later.) You can suppress the display 
of a data source by checking Hide in Tile 
View, and it is common to do so. This reduces 
the number of items in Report Manager, 
therefore increasing simplicity for the users. 
FIGURE 22-43
FIGURE 22-44
Report Manager ❘ 799

800  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Moving down the page, you next see the Enable This Data Source option. As its name implies, you 
can disable and re-enable this data source from use. 
The data source type is the next option. SSRS can access a rich set of data sources beyond SQL 
Server. SQL Server Analysis Services, SQL Azure, OLE DB, and Oracle are just a few of the many 
sources available to SSRS.
The connection string can vary by the data source type. For SQL Server, it takes the form of 
Data Source= followed by the name of the server. If the instance is not the default one, you also need 
to add the name of the SQL Server instance. That will be followed by a semicolon, Initial Catalog=, 
and the name of the database to get data from. Note the spaces between the words Data Source and 
Initial Catalog, which need to be there. 
Thus, to connect to the reporting execution log for your demo test server, the connection string 
looks like 
Data Source=WIN-TEV63B8OEN9;Initial Catalog=ReportServer 
The next section, Connect Using, has four options. These have a big impact on how the report can 
be used on the server. 
Credentials Supplied by the User Running the Report
With this option, users are prompted each time they run the report to enter their creden-
tials. The text of the prompt may be speciﬁ ed, and there is an option to use the supplied cre-
dentials as Windows credentials. Most users ﬁ nd having to enter their credentials each time 
a report is run annoying, at the very least. Thus this option is rarely used. 
A good example of where it might be useful though is in a facility where large numbers 
of temporary works are brought in for short time periods. Setting up Active Directory 
accounts for all those users is not practical. This option becomes even more beneﬁ cial when 
the workers share a common PC, for example on a manufacturing ﬂ oor production ﬂ oor 
where maintenance is done. A handful of IDs could be created and shared among a group 
of workers. When maintenance people run the report, they enters a generic credential for 
all maintenance people, and the report displays only the data that job role is allowed to see. 
Likewise, a shift supervisor would see only data they are allowed, and so on. 
When this method of authentication is used, the report cannot be set up for unattended 
execution. You see more on unattended execution in a moment.  
Credentials Stored Securely in the Report Server
With this option you must enter a speciﬁ c set of credentials to run the report with. This may 
be SQL Server ID or an Active Directory ID. If an Active Directory ID is used you should 
also check the Use as Windows Credentials option. 
By default the data source cannot see the Windows user as the person making the request, 
but rather the ID supplied here. This is good when you don’t need user-speciﬁ c security 
around the data being accessed. If, however, you do require the data source to know who 
the Windows user is, you can check the ﬁ nal option Impersonate the Authenticated User. 
SSRS passes the Windows user ID through to the data source, allowing it to return data 
based on the user’s ID (or Active Directory group membership). 
This is one of the two options, which enables you to run a report in unattended execution 
mode. 
➤
➤

Windows Integrated Security
With the Windows Integrated Security option, SSRS can automatically detect the Windows 
credentials of the user accessing SSRS and pass them along to the data source. This option 
cannot allow a report to be run in unattended execution mode. 
Credentials Are Not Required
There are some situations in which credentials are not required, or not even usable. A good 
example is a report generated from an XML ﬁ le. XML has no concept of authentication. 
This mode enables unattended execution.
There are two basic methods for executing a report. With the ﬁ rst method, the report is executed 
on demand by a user. Commonly it is done via Report Manager, but it may also occur with reports 
launched from an application, such as an ASP.Net website. In this situation, Reporting Services 
knows who the user is and can pass their credentials to the data source. Refer to this mode as 
attended execution.
In the second method, it is SQL Server Report Services that launches the report. This occurs when 
a report is scheduled to be run or a snapshot is due to be created on a speciﬁ c schedule. Thus the 
name unattended execution mode. 
In this light the connection methods make sense. To run unattended, the credentials for the data 
source must be stored on the server or must not be required. 
After you ﬁ ll out all the information, be sure to click OK to save the shared Data Source. 
You can set these same options for an individual report. You see where to do that in the next 
section; however, the connection options are identical for both shared and report speciﬁ c data 
sources. 
The third item in the task menu is Report Builder. This launches Report Builder for the user, and 
if Report Builder has not yet been installed, it installs it as well. See the “Report Builder” section 
earlier in this chapter for more information on how to use Report Builder. 
The Folder Settings task enables you to set security for the current folder. The operation is similar 
to the site settings you saw in Figures 23-40 and 23-41, but speciﬁ c to the current folder and by 
default any subfolders. The security options are slightly different though, as shown in Figure 22-45. 
➤
➤
FIGURE 22-45
Report Manager ❘ 801

802  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Microsoft did a great job on the screen shown in Figure 22-45, spelling out what rights each role 
has; thus they don’t need much more explanation. A user may have more than one role, as needed. 
As with most security, it’s best to start out with the most minimal right, Browser, and increase 
privileges as need. 
In addition to saving reports directly to the server, Report Builder also enables you to save reports 
to a hard drive. This is a great option for reports that were built to run once and then discarded. 
Some companies have tight security settings around their report servers. Users who want to develop 
reports must save them locally and then send the completed report (in the form of an RDL ﬁ le) to IT 
where it can be validated. 
To get the ﬁ le into Report Manager, the Upload File task is used. It brings up a simple web page 
where a user can upload a report. When updating an existing report, the uploader must also check 
the Overwrite check box. This is a safety mechanism so that existing reports won’t be overwritten 
accidentally. 
In the section on deﬁ ning a data source, you saw an option Hide in Tile View (refer to Figure 22-44). 
Although tile view is the most common way to look at reports, there is another option, Details view, 
which can be accessed by picking the last option in the task menu, appropriately named Details 
View, as shown in Figure 22-46. 
FIGURE 22-46
Details view provides additional information, such as the last run date for the report, the last 
modiﬁ ed date, and who did the modiﬁ cation. It also lists all objects in this folder, which might be 
hidden. This is how you can get to an item you may have hidden previously. 
In addition, two more menu options appear in the task bar on the left: Delete and Move. When you 
check one or more of the objects, these buttons will be enabled, allowing you permanently delete the 
selected items or move them to another folder within the same Report Manager site. 
So far you’ve seen how to manage the Report Manager, as well as manage the folders, data sources, 
and reports that reside in Report Manager. In the next section you see how to manage a report. 

Managing Reports
Whether you are in Tile or Detail view, you begin by hovering over 
any report. A yellow border surrounds the report, and a yellow 
drop-down button appears on the right. Clicking that button 
opens up the report management menu, as shown in Figure 22-47.
Some of the options are self-explanatory. Move and Delete 
behave like they do in detail view. Download enables you to 
save the report or data source to your hard drive. Edit in Report 
Builder launches Report Builder and loads the selected report 
for you, ready to edit. 
The main way to manage a report is through the Manage web 
pages, which you see in a moment. Subscribe, View Report 
History, and Security are shortcuts to pages contained in the Manage web pages; you look at those 
in context of the Manage pages. 
The last option is Create Linked Report. This is analogous to a shortcut within Windows. It creates 
a clone of the entry for the report, but not the report itself. You can then change all the properties 
(which you’ll see momentarily) in the Manage web pages. Thus you run the same report with two 
different sets of parameters, schedules, snapshots, and the like. 
Creating a linked report is easy. After clicking the menu option, all you need to tell it is what you want 
the link to be named, and what folder you want to put it in. It defaults to the current folder, but you can 
change it if you want. From there you can alter any of the properties in the Manage area, as you see next.
Properties 
When you open the Manage web page, you are greeted with the Properties page, as shown in 
Figure 22-48. By now you should be familiar with most of these properties and commands, having 
seen them in other places. The only new one here is Replace, which is just a shortcut to the Upload 
command. The only difference is Upload lacks the Overwrite if Exists check box. SSRS assumes if 
you are at this point, it’s a safe assumption you want to overwrite, so it doesn’t bother to ask. 
With all the pages in the Manage area, be sure to click the Apply button when you ﬁ nish. If you move 
to another page within Manage web pages without pressing Apply ﬁ rst, you will lose your changes.
FIGURE 22-47
FIGURE 22-48
Report Manager ❘ 803


Subscriptions 
Subscriptions are a useful tool in SSRS; they enable reports to be generated and delivered to the 
users without their intervention. This is an ideal solution for long running reports, or when a report 
will be run on a regular basis, such as daily. 
When entering the Subscriptions page for the ﬁ rst time, there won’t be any subscriptions listed. You 
have two choices to create a subscription: New Subscription and New Data-Driven Subscription.
New Subscription
New subscriptions are run on a timed basis. When you select this option, you will be asked what 
delivery method you want, and when you want it to run (see Figure 22-50).
Assuming you set up an e-mail address in the Reporting Services Conﬁ guration Manager’s E-mail page, 
you can choose between two delivery methods: E-Mail and Windows File Share. Figure 22-50 shows 
the E-Mail option and gives you the chance to set basic details about the mail: what format should the 
report be in, who should it go to, and more. The Windows File Share is similar; in it you set the path to 
save to, the format of the report, and the credentials used to access the ﬁ le share.
On the lower half of the New Subscription page, you specify when you want the report to run. The 
ﬁ rst option enables you to set a custom schedule for this speciﬁ c report. The second option enables 
you to use a shared schedule. Shared schedules are set up in the Site Settings area, as discussed 
earlier in this chapter in the “Managing the Report Manager — The Navigation Menu” section.
FIGURE 22-50
Report Manager ❘ 805


         , ‘Brians File’
         , ‘IMAGE’
         , ‘Hi Brian, here is your report.’
         , ‘Reporting Services Execution Log’)
     , (   ‘2’
         , ‘Adam Jorgensen’
         , ‘bogusaddress@somedomain.com’
         , ‘\\WIN-TEV63B8OEN9\FileShare’
         , ‘Adams Data’
         , ‘MHTML’
         , ‘Greetings Adam, here is your report’
         , ‘Reporting Services Execution Log’
       )
     , (   ‘3’
         , ‘Robert Cain’
         , ‘arcanecode@gmail.com’
         , ‘\\WIN-TEV63B8OEN9\FileShare’
         , ‘Roberts Stuff’
         , ‘PDF’
         , ‘Hi Mom!’
         , ‘Reporting Services Execution Log’
       );
Now that you have data to work with, you can set up a data-driven subscription. On the 
subscriptions page select the New Data-Driven Subscription option. Now walk through a series of 
steps needed to set up the subscription. 
 1. 
The ﬁ rst step is illustrated in Figure 22-51. After giving your subscription a name, you are 
asked about your delivery option; you can pick from e-mail or a Windows File Share. In 
the previous section on regular nondata-driven subscriptions, you used e-mail, so for this 
example use Windows File Share. The ﬁ nal option is to indicate the data source. 
FIGURE 22-51
Report Manager ❘ 807

808  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
 2. 
Because in the previous step the data 
source select was for this report only, 
you are now prompted for connection 
information, as shown in Figure 22-52. 
If you select a shared data source, this 
step would instead give you the chance 
to select an existing shared data source.
One annoyance: Each time perform-
ing this step, you must ﬁ ll out the 
password. It won’t retain it, so if you 
return to edit this subscription later, 
be sure to have the password. 
 3. 
Now provide the appropriate SQL 
query to apply to the database 
speciﬁ ed in the previous step (see Figure 22-53). In addition to the query, you can override 
the default different time-out if you have a long running query. Finally, you should always 
use the Validate button to ensure the SQL query you entered is valid. 
FIGURE 22-52
FIGURE 22-53
Because the screen is not big enough to see the full query, it is displayed here in Listing 22-4. 
When you run this query it returns the following information on the ﬁ le share: where to put 
the output ﬁ le, which ﬁ lename to use, and what format the ﬁ le should be in. Some date math 


810  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
The path must be in UNC naming format: \\SERVERNAME\\FILESHARENAME. In this 
example the UNC path to your ﬁ le share (stored in the table, refer to Listing 22-3) was 
\\WIN-TEV63BOEN9\FileShare. If you had picked e-mail as your delivery option, then this 
step would have ﬁ elds oriented toward sending e-mails. It would map the same way as you 
did with the ﬁ le share in Figure 22-54.
 5. 
If you had put any parameters on your report, now is the time to map values from your SQL 
query to the report parameters. This can be an effective manner in which to reuse reports 
multiple ways. 
For example, a report could be created for each manager that would list the employees and total 
number of hours worked for the week. In the subscription table for that report, the manager’s 
name, e-mail address, and department name 
could be stored. The data-driven subscription 
could map the department name to a param-
eter in the report, so only people in that 
department are on the instance of the report 
being e-mailed to the associated manager. 
However, for the Report Execution Log 
report you created, there were no param-
eters. Figure 22-55 shows you that and 
allows you to move on to the next step. 
 6. 
Next you specify when the report should 
execute (see Figure 22-56). The ﬁ rst option, 
When the Report Data Is Updated on the 
Server, is just as it implies. SSRS monitors 
the report data for updates and ﬁ res when 
it sees changes. This is useful when you 
have data updated on a limited, infrequent 
basis on an unpredictable schedule. 
The other two options set the report to 
run on a time schedule. The lower option 
enables you to reuse an existing schedule 
(see the information on Site Settings in 
the “Navigation Menu” section earlier 
in this chapter). When either the ﬁ rst or 
last option is selected, the Finish button is 
enabled, and you are done. 
If you select the middle option, you have 
one more step where you must set up the 
schedule for this subscription. 
 7. 
In the ﬁ nal step of setting up a data-driven 
subscription, you simply need to supply 
the information on when and how often 
the report should execute. Figure 22-57 
shows there is a lot of ﬂ exibility when 
conﬁ guring the schedule. 
FIGURE 22-55
FIGURE 22-56
FIGURE 22-57

That was a lot of effort but well worth it. Data-driven subscriptions can be valuable when you 
need to run the same report many times, each time with different values for the parameters and 
processing options. Figure 22-58 shows the listing of existing subscriptions. 
FIGURE 22-58
Processing Options
Figure 22-59 shows you a lot of different processing options, but they can be summarized as a 
choice between caching and snapshots. The report cache is for storing temporary copies of the 
report, whereas the snapshots can be retained for a speciﬁ ed duration. Users can see previous 
snapshots of reports, whereas cached reports are lost forever when they expire. 
FIGURE 22-59
Report Manager ❘ 811
 
 
 
 

812  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
Of course, you can elect not to cache or snapshot reports at all, as the ﬁ rst option in Figure 22-59 
shows. When you choose to neither cache nor snapshot the report, each time a user executes the 
report SSRS makes a call to the database to get the report data. 
With the cache options, when a user runs a report, SSRS ﬁ rst checks to see if it is in the cache. If so, 
it returns the report from the cache instead of going to the source database. If not, it executes the 
report and adds it to the cache. The difference in the two options is simply when the cache expires: 
either after a set time or on a speciﬁ c schedule. 
The second option shown in Figure 22-59 enables the report to be rendered from a snapshot. As 
mentioned earlier, snapshots are copies of the report and its associated data that is stored long term. 
With this option you may also set a schedule for snapshots to be generated. 
The ﬁ nal option enables you to override the standard settings for report timeouts for this speciﬁ c 
report. 
Cache Refresh Options
A report can get into the report cache in two ways. The ﬁ rst was discussed brieﬂ y in the previous 
section: processing options. When a report is run, if it is marked for caching but is not in the cache, 
it is executed and then added to the cache. 
The second way is to set up a cache refresh plan, as shown in Figure 22-60. When you set up a new 
refresh plan, you are taken to a simple web page where you are asked if you want to create a speciﬁ c 
schedule or reuse an existing one. 
Using a cache refresh plan is a good way to ensure the reports in the cache stay fresh, while at the 
same time reducing the amount of time users must wait for the report. 
FIGURE 22-60
Report History
This section talked about snapshots and the capability of SSRS to store copies of a report as it 
executes. To view past snapshots of a report, use the Report History screen, as shown in Figure 22-61. 

The Report History provides a listing of every snapshot of the report. To view one, simply click the 
last run date/time (refer to Figure 22-61). You also have the option to generate a new snapshot by 
clicking the New Snapshot button. This causes the report to execute immediately and be stored as a 
snapshot.
Snapshot Options
When you use snapshots you must manage them. Through the Snapshot Options, as shown in 
Figure 22-62, you can establish a schedule for automatically creating new snapshots. You can also 
determine how many snapshots should be retained. 
FIGURE 22-61
FIGURE 22-62
Security
Security is the ﬁ nal section of the Manage web pages. This enables you to ﬁ ne-tune access for this 
speciﬁ c report. The dialog and actions are identical to those for the folders, as seen in Figure 22-40. 
Report Manager ❘ 813

814  ❘  CHAPTER 22  SQL SERVER REPORTING SERVICES ADMINISTRATION
SUMMARY
SQL Server Reporting Services offers a rich toolset around managing both the server and the reports 
it contains. The Reporting Services Conﬁ guration Manager enables you to conﬁ gure critical options 
such as the databases SSRS needs to do its job, encryption keys, URLs, and more. 
Using the properties settings in SQL Server Management Studio, you can ﬁ ne-tune your instance of 
Reporting Services. Properties such as execution timeouts, logging, history, and security are set via 
SQL Server Management Studio. 
Report Builder provides an easy-to-understand yet full-featured way to create the reports you’ll 
house in SSRS. It can even be useful to you, the DBA, in creating reports from SQL Server 
management data, such as the SSRS execution log. 
Report Manager is the tool of choice for managing and executing your reports. You can conﬁ gure 
reports to run on schedules and be placed into a cache for reuse. You can also store and track them 
as snapshots. 
With the understanding acquired here, the DBA will be well equipped to manage Reporting Services. 




818  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
Reporting Services
Reporting Services continues to be an integral part of the Microsoft Business Intelligence stack. 
With SQL Server 2012 a massive overhaul occurred on the Reporting Services SharePoint 
integration side. The main enhancement comes by way of a new SharePoint Service application for 
Reporting Services. Anyone who has conﬁ gured Reporting Services for SharePoint integrated mode 
can appreciate this change. 
Prior to SQL Server 2012
With SQL Server 2008 R2 and previous editions, Reporting Services was required to be conﬁ gured 
as a part of the SQL Server installation and then integrated into SharePoint. This was accomplished 
by ﬁ rst creating an instance of Reporting Services, conﬁ gured to run in SharePoint integrated mode 
instead of native mode. 
One issue, although easily ﬁ xed, that would often arise is the need for the Reporting Services 
database to be switched from a native mode conﬁ guration to a SharePoint integrated conﬁ guration. 
This change had to be made through the Reporting Services Conﬁ guration Manager. Consequently, 
other settings would need to be managed here as well including the report server URL, the execution 
account, and email settings. Meanwhile all reports would be managed on the SharePoint side.
After the Windows Service was correctly conﬁ gured, additional setup needed to be completed on the 
SharePoint Central Administration side. This ﬁ nal step was in place to tell SharePoint where to look 
for the Reporting Services instance as well as the authentication for connecting to it. At this point 
reports were ready to be deployed and viewed inside SharePoint.
SQL Server 2012 and Beyond
A lot has changed with SQL Server 2012; Reporting Services is now consolidated in with the rest of 
the SharePoint shared services. The biggest requirements for setting up Reporting Services 2012 in 
SharePoint integrated mode is SharePoint 2010 Service Pack 1. The ﬁ rst major difference in setup is 
now two options need to be selected on the Feature Selection page of the SQL Server installation: 
Reporting Services — SharePoint and Reporting Services Add-In for SharePoint Products. Following 
the completion of the installation, all further setup is handled within SharePoint.  
Much like conﬁ guring other service applications such as Excel Services, in SharePoint 2010 Central 
Administration click on Manage Service Applications, click on New in the top left corner, and select 
SQL Server Reporting Services Service Application. All the settings that would previously have been 
entered in the Reporting Services Conﬁ guration manager can now be entered inside SharePoint. 
This includes specifying an application pool, a corresponding security account, and the location of 
the Reporting Services database.
While undoubtedly users can appreciate the ease provided by a consolidated installation and 
conﬁ guration process, this new approach provides even more beneﬁ ts. As mentioned previously, the 
reporting databases have their location speciﬁ ed inside SharePoint Central Administration. This is 
advantageous because they can easily be placed on any server, including on the same instance as the 
SharePoint content databases, without the need to go outside the SharePoint environment. A Service 
Application is the name of an application that runs inside SharePoint. It is easy to set up multiple 
service applications for Reporting Services for use in the farm. When this happens each gets its own 
set of databases on the backend; they are not shared. 

Components of Integration ❘ 819
Improvements to SharePoint Integrated Mode in SQL Server 2012
Now that all the Reporting Services pieces for SharePoint integrated mode are contained 
inside SharePoint, the maintenance model will be simpliﬁ ed. Scale out can also be handled on 
the SharePoint side, rather than worrying about scaling Reporting Services and SharePoint.
Among the other enhancements is the dramatically improved report performance. Part of the 
basis for this improvement is that SharePoint no longer has to go to a separate Reporting Services 
server for information — it is all managed by SharePoint.
For farms with multiple servers, it is no longer required to conﬁ gure the rsreportserver.config 
ﬁ le on each server because this information is stored in the conﬁ guration database. Set the values 
once and all the machines can pick up the values. For information on how this affects licensing, visit 
the Microsoft Web site because the pricing model is always subject to change.
Power View
Power View is a new feature introduced with SQL Server 2012 in combination with SharePoint 
2010 to provide interactive ad-hoc reporting in real time. Power View works with Developer, 
Enterprise, and Business Intelligence versions of SQL Server 2012 and requires an Enterprise edition 
of SharePoint 2010 and the Reporting Services add-in for SharePoint to be enabled. 
What Is Power View
Power View provides a new innovative way to interact with your data by way of Silverlight renderings. 
These reports are developed and fed from PowerPivot workbooks deployed to a SharePoint PowerPivot 
gallery or the new tabular model deployed to an Analysis Services (SSAS) 2012 instance.
This is not meant to be a replacement for traditional Reporting Services reports or report builder. 
Power View is meant for ad-hoc data exploration, which is to say moving through your data and 
looking at how it relates across your business. The traditional ﬁ le created by the current Reporting 
Services tools is a report deﬁ nition ﬁ le (RDL) and cannot be edited or viewed in Power View. Because 
Power View is also used only inside SharePoint, it is not applicable for use in every situation depending 
on the reporting needs. Each of these technologies has its place in the reporting environment.
Presentation Layer
Unlike a traditional Reporting Services report, Power View is an “always on” presentation type that 
doesn’t require you to preview reports. The data is always live. There is also the added advantage of 
exporting Power View reports to PowerPoint where each individual report becomes its own slide. The 
full-screen viewing and PowerPoint slides work in the same way. Both enable interaction with data 
such as applying ﬁ lters that the builder added or using visualizations. However, neither enables further 
development. In this state, the data is interactive, but you cannot add new ﬁ lters or visualizations.
Some of the visualizations available in Power View follow:
Table/Matrix
Chart
Bubble Chart
Scatter Chart
➤
➤
➤
➤

820  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
Cards
Tiles
Creation
You initiate all Power View report creation from tabular model elements, xlsx ﬁ les, or connection 
ﬁ les pointing to BISM models, within SharePoint 2010 document libraries or PowerPivot Galleries. 
Clicking the arrow next to an applicable element can reveal an option for Create Power View Report. 
Selecting this option opens the Power View Designer. You can edit existing reports by clicking the 
drop-down arrow next to a report and selecting Edit in Power View. 
When saving Power View reports, they are in RDLX format. The option to save corresponding 
images is also available. When exporting to PowerPoint, if you do not use the option to save images, 
only placeholders appear.
Service Application Architecture
Service applications refer to the new service application architecture in SharePoint 2010. Deep 
SharePoint architecture discussions are beyond the scope of this book. For more details see http://
technet.microsoft.com/en-us/library/cc560988.aspx. 
DATA REFRESH
As a SQL professional your job focuses on the ability to get data in and out quickly and efﬁ ciently. 
This is important for the performance of your data environment and the applications serviced by it. 
You’ve already seen a signiﬁ cant amount of DBA related items such as PowerPivot and Power View 
implementations in the previous section, but more is added here. This section focuses on managing 
and controlling data refresh with your reports and data in SharePoint. There are many possible data 
sources, so it is best to focus on those that typically come out of SQL Server, such as the following.
Excel Services
PerformancePoint Services
Visio Services
PowerPivot
Using Data Connections in Excel
Every Excel workbook that uses external data contains a connection to a data source. Connections 
consist of everything required to establish communications with and retrieve data from an external 
data source. These requirements include the following:
Connection string, which speciﬁ es which server to connect to and how to connect to it
Query, which is a string that speciﬁ es what data to retrieve
Any other speciﬁ cs required to get the data such as impersonation, proxy mode etc. 
Embedded and linked connections
➤
➤
➤
➤
➤
➤
➤
➤
➤
➤



Data Refresh ❘ 823
Secure Store Service
Secure Store is a SharePoint Server 2010 service application used to store encrypted credentials in 
a database for use by applications to authenticate to other applications. In this case, Excel Services 
uses Secure Store to store and retrieve credentials for use in authenticating to external data sources.
If you choose the SSS option, you must then specify the application ID of a Secure Store target 
application. The speciﬁ ed target application serves as a lookup used to retrieve the appropriate set 
of credentials. Each target application can have permissions set so that only speciﬁ c users or groups 
can use the stored credentials.
When provided with an application ID, Excel Services retrieves the credentials from the Secure Store 
database for the user who accesses the workbook (either through the browser, or using Excel Web 
Services). Excel Services then uses those credentials to authenticate to the data source and retrieve data.
For information about how to use Secure Store with Excel Services, see http://technet
.microsoft.com/en-us/library/ff191191.aspx.
None
When you select the None option, no credential retrieval occurs, and no special action is taken for 
authentication for the connection. Excel Services does not try to delegate credentials and does not 
try to retrieve credentials stored for the user from the Secure Store database. Instead, Excel Services 
impersonates the unattended service account and passes the connection string to the data provider 
that handles authentication.
The connection string may specify a username and password to connect to the data source or may 
specify that the Windows identity of the user or computer issuing the request be used to connect 
to the data source. In either case, the unattended account is impersonated ﬁ rst, and then the data 
source connection is made. The connection string and the provider determine the authorization 
method. Additionally, authorization can be based on either the credentials found in the connection 
string or the impersonated unattended account’s Windows identity. 
Excel Services Security and External Data
Excel Services manages workbooks and external data connections by using the following:
Trusted ﬁ le locations: Locations designated by an administrator from which Excel Services 
can load workbooks
Trusted data connection libraries: SharePoint Server 2010 data connection libraries that 
have been explicitly trusted by an administrator from which Excel Services can load data 
connection ﬁ les
Trusted data providers: Data providers that have been explicitly trusted by an administrator
Unattended service account: A low-privileged account that Excel Services can impersonate 
when it makes data connections
Trusted File Locations
Excel Services loads workbooks only from trusted ﬁ le locations. A trusted ﬁ le location is a 
SharePoint Server location, network ﬁ le share, or Web folder address that the administrator has 
➤
➤
➤
➤

824  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
explicitly enabled workbooks to be loaded from. These directories are added to a list that is internal 
to Excel Services. This list is known as the trusted ﬁ le locations list.
Trusted locations can specify a set of restrictions for workbooks loaded from them. All workbooks 
loaded from a trusted location adhere to the settings for that trusted location. Following is a short 
list of the trusted location settings that affect external data:
Allow External Data: Deﬁ nes how external data can be accessed.
No Data Access Allowed (Default): Only connection ﬁ les in a trusted SharePoint Server 
2010 data connection library are allowed.
Warn on Refresh: Deﬁ nes whether to show the query refresh warnings.
Stop When Refresh on Open Fails: Deﬁ nes whether to fail the workbook load if external 
data does not refresh when the workbook opens. This is used in scenarios in which the 
workbook has cached data results that will change depending on the identity of the user 
viewing the workbook. The objective is to hide these cached results and make sure that any 
user who views the workbook can see only the data speciﬁ c to that user. In this case, if the 
workbook is set to refresh on open and the refresh fails, the workbook does not display. 
External Data Cache Lifetime: Deﬁ nes external data cache expiration times. Data is shared 
among many users on the server to improve scale and performance, and these cache life-
times are adjustable. This accommodates scenarios in which query execution should be kept 
to a minimum because the query might take a long time to execute. In these scenarios, the 
data often changes only daily, weekly, or monthly instead of by the minute or every hour.
Trusted Data Connection Libraries and Managed Connections
A data connection library is a SharePoint Server 2010 library designed to store connection ﬁ les, 
which can then be referenced by Ofﬁ ce 2010 applications, such as Excel and Microsoft Visio. Excel 
Services loads only connection ﬁ les from trusted SharePoint Server 2010 data connection libraries. 
A trusted data connection library is a library that the server administrator has explicitly added to 
an internal trusted list. Data connection libraries enable you to centrally manage, secure, store, and 
reuse data connections.
Managing Connections
Because workbooks contain a link to the ﬁ le in a data connection library, if something about 
the connection changes (such as a server name or a Secure Store application ID), only a single 
connection ﬁ le must be updated instead of potentially many workbooks. The workbooks can obtain 
the connection changes automatically the next time that they use that connection ﬁ le to refresh data 
from Excel or Excel Services.
Securing Connections
The data connection library in a SharePoint library supports all the permissions that SharePoint 
Server 2010 does, including per-folder and per-item permissions. The advantage that this provides 
on the server is that a data connection library can become a locked-down data connection store that 
is highly controlled. Many users may have read-only access to it. This enables them to use the data 
connections, but they can be prevented from adding new connections. By using access control lists 
➤
➤
➤
➤
➤

Data Refresh ❘ 825
(ACLs) with the data connection library, and letting only trusted authors upload connections, the 
data connection library becomes a store of trusted connections. 
You can conﬁ gure Excel Services to load connection ﬁ les only from data connection libraries 
explicitly trusted by the server administrator and block loading of any embedded connections. In 
this conﬁ guration, Excel Services uses the data connection library to apply another layer of security 
around data connections.
You can use data connection libraries together with the new Viewer role in SharePoint Server 2010 
that enables those connections to refresh workbooks rendered in a browser by Excel Services. If the 
Viewer role is applied, users cannot access the connection ﬁ le contents from a client application, 
such as Excel. Therefore, the connection ﬁ le contents are protected but still can be used for 
workbooks refreshed on the server.
Storing Connections
Storing data connections is another important role for document libraries. These data connections 
are stored as objects like documents or images in a library and can be accesses by services and 
reports throughout the farm depending on permissions. 
Reusing Connections
Users can reuse connections created by other users and create different reports that use the same 
data source. You can have the IT department or a business intelligence expert create connections, 
and other users can reuse them without understanding the details about data providers, server 
names, or authentication. The location of the data connection library can even be published to 
Ofﬁ ce clients so that the data connections display in Excel or in any other client application that 
uses the data connection library. 
Trusted Data Providers
Excel Services uses only external data providers on the Excel Services trusted data providers list. 
This is a security mechanism that prevents the server from using providers that the administrator 
does not trust. 
Unattended Service Account
Excel Services runs under a highly privileged account. Because Excel Services has no control over 
the data provider and does not directly parse provider-speciﬁ c connection strings, using this account 
for the purposes of data access would be a security risk. To lessen this risk, Excel Services uses an 
unattended service account. This is a low-privileged account that is impersonated by Excel Services 
if any of the following conditions are true:
Any time that it tries a connection where the None authentication option is selected.
Whenever the Secure Store Service (SSS) option is selected and the stored credentials are not 
Windows credentials.
If the None option is selected and the unattended account does not have access to the data 
source, Excel Services impersonates the unattended service account and uses information 
stored in the connection string to connect to the data source.
➤
➤
➤

826  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
If the None option is selected and the unattended account has access to the data source, 
a connection is successfully established using the credentials of the unattended service 
account. Use caution when you design solutions that intentionally use this account to con-
nect to data. This is a single account that potentially can be used by every workbook on the 
server. Any user can open a workbook with an authentication setting of None using Excel 
Services to view that data by using the server. In some scenarios, this might be needed. 
However, Secure Store is the preferred solution for managing passwords on a per-user or 
per-group basis.
If the SSS option is selected and the stored credentials are not Windows credentials, Excel 
Services impersonates the unattended service account and then attempts to connect to the 
data source by using the stored credentials.
If the Windows Authentication option is selected, or if the SSS option is selected and the 
stored credentials are Windows credentials, then the unattended service account is not used. 
Instead, Excel Services impersonates the Windows identity and attempts to connect to the 
data source.
PerformancePoint Data Refresh
In PerformancePoint Services you must create a connection to the data source or sources you want 
to use in your dashboard. All data used in PerformancePoint Services is external data, living in data 
repositories outside of PerformancePoint. After you establish a data connection, you can use the 
data in the various PerformancePoint feature areas.
PerformancePoint supports both tabular data sources including SharePoint Lists, Excel Services, 
SQL Server tables and Excel workbooks; as well as multidimensional (Analysis Services) data 
sources; and also supports PowerPivot for Excel. 
Tabular Data Sources
A user can create a data connection to SharePoint Lists, Excel Services, SQL Server tables, or 
Excel workbooks. For these kinds of data sources, you can view a sample of the data from the 
Dashboard Designer tool and set speciﬁ c properties for the data depending how you want the data 
to be interpreted within PerformancePoint. For example, you can indicate which datasets should 
be treated as a dimension; you can specify if a dataset is to be treated as a dimension or a fact; or 
if you do not want the data to be included, you can select Ignore. If you decide to set the value as 
a fact, you can indicate how those numbers should be aggregated in PerformancePoint Services. 
You can also use datasets that have time values within PerformancePoint Services and use the 
PerformancePoint Services time intelligence features to set time parameters and create dashboard 
ﬁ lters.
SharePoint Lists
You can use data contained in a SharePoint List on a SharePoint Site in PerformancePoint Services 
by creating a SharePoint List data source in Dashboard Designer. Data from SharePoint Lists can 
only be read but not modiﬁ ed. Modiﬁ cation to SharePoint List data must be done from SharePoint. 
Users may connect to any kind of SharePoint List.
➤
➤
➤

Data Refresh ❘ 827
Excel Services
Data in Excel ﬁ les published to Excel Services on a SharePoint Site can be used in PerformancePoint 
Services by creating an Excel Services data source. Supported published data can be read only 
in PerformancePoint Services. Published parameter values can be modiﬁ ed from the Dashboard 
Designer. If you use an Excel Services parameter in calculating a KPI, it is easy to make additional 
changes. PerformancePoint Services supports the following Excel Services components: Named 
Ranges, Tables, and Parameters.
SQL Server Tables
You can create a data source connection to a SQL Server database and use the data within 
PerformancePoint Services. Tables and views are supported data sources within PerformancePoint 
Services. 
Excel Workbooks
You may use the content of an actual Excel ﬁ le stored in PerformancePoint as a data source in 
PerformancePoint Services by creating an Excel Workbook data source connection and selecting 
only the data to be used. The original Excel ﬁ le will be independent from the PerformancePoint 
copy. PerformancePoint Services 2010 supports Excel 2007 and Excel 2010 workbooks as data 
sources. 
Multidimensional Data Sources
Use data residing in a SQL Server Analysis Services multidimensional cube in PerformancePoint 
Services by creating a data connection to the source. PerformancePoint Services enables you to 
map the wanted time dimension and the required level of detail for its hierarchies to the internal 
PerformancePoint Services Time Intelligence.
PowerPivot for Excel
In PerformancePoint Services you can use a PowerPivot model as a data source to build 
your PerformancePoint Services dashboards. To use PowerPivot as a data source within a 
PerformancePoint Services dashboard, you must have PerformancePoint Services activated on a 
SharePoint Server 2010 farm and have PowerPivot for SharePoint installed. After a PowerPivot 
model has been created by using the PowerPivot add-in for Excel, this Excel ﬁ le must be uploaded 
or published to a SharePoint site that has PowerPivot services enabled. Create the data source 
connection in Dashboard Designer using the Analysis Services data source template.
Visio Services Data Refresh
The Visio Graphics Service can connect to data sources. These include SharePoint lists, Excel 
workbooks hosted on the farm, databases such as Microsoft SQL Server, and custom data sources. 
You can control access to speciﬁ c data sources by explicitly deﬁ ning the data providers trusted and 
conﬁ guring them in the list of trusted data providers.
When Visio Services loads a data connected Web drawing, the service checks the connection 
information that is stored in the Web drawing to determine whether the speciﬁ ed data provider is a 


Data Refresh ❘ 829
Secure Store Service: In this security model the Visio Graphics Service uses the Secure Store 
Service to map the user’s credentials to a different credential that has access to the database. 
The Secure Store Service supports individual and group mappings for both Integrated Windows 
authentication and other forms of authentication such as SQL Server Authentication. This 
gives administrators more ﬂ exibility in deﬁ ning one-to-one, many-to-one, or many-to-many 
relationships. This authentication model can be used only by drawings that use an Ofﬁ ce Data 
Connection (ODC) ﬁ le to specify the connection. The ODC ﬁ le speciﬁ es the Secure Store target 
application that can be used for credential mapping. The ODC ﬁ les must be created by using 
Microsoft Excel. 
Unattended Service Account: For ease of conﬁ guration the Visio Graphics Service provides 
a special conﬁ guration where an administrator can create a unique mapping associating all 
users to a single account by using a Secure Store Target Application. This mapped account, 
known as the unattended service account, must be a low-privilege Windows domain 
account that is given access to databases. The Visio Graphics Service impersonates this 
account when it connects to the database if no other authentication method is speciﬁ ed. 
This approach does not enable personalized queries against a database and does not provide 
auditing of database calls. This authentication method is the default authentication method 
used when you connect to SQL Server databases: If no ODC ﬁ le is used in the Visio Web 
drawing that speciﬁ es a different authentication method, then Visio Services uses the cre-
dentials speciﬁ ed by the unattended account to connect to the SQL Server database. 
In a larger server farm, it is likely that Visio drawings use a mix of the authentication methods 
described here. Consider the following:
Visio Services supports usage of both the Secure Store Service and the unattended service 
account in the same farm. In Web drawings connected to SQL Server data that do not use 
ODC ﬁ les, the unattended account is required and always used.
If Integrated Windows authentication is selected, and authentication to the data source fails, 
Visio Services does not attempt to render the drawing using the unattended service account.
Integrated Windows authentication can be used together with the Secure Store by conﬁ gur-
ing drawings to use an ODC ﬁ le that speciﬁ es a Secure Store target application for those 
drawings that require speciﬁ c credentials.
PowerPivot Data Refresh
PowerPivot data refresh is a scheduled server-side operation that queries external data sources to 
update embedded PowerPivot data in an Excel workbook stored in a content library. 
Data refresh is a built-in feature of PowerPivot for SharePoint, but using it requires that you run 
speciﬁ c services and timer jobs in your SharePoint farm. Additional administrative steps, such as 
installing data providers and checking database permissions, are often required for data refresh to 
succeed.
After you ensure that the server environment and permissions are conﬁ gured, data refresh is ready 
to use. To use data refresh, a SharePoint user creates a schedule on a PowerPivot workbook that 
speciﬁ es how often data refresh occurs. Creating the schedule is typically done by the workbook 
➤
➤
➤
➤
➤

830  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
owner or author who published the ﬁ le to SharePoint. This person creates and manages the data 
refresh schedules for the workbooks that he or she owns. The following sections provide a list of 
steps you should follow in order to perform a successful PowerPivot data refresh.
Step 1: Enable Secure Store Service and Generate a Master Key 
PowerPivot data refresh depends on Secure Store Service to provide credentials used to run data 
refresh jobs and to connect to external data sources that use stored credentials. 
If you installed PowerPivot for SharePoint using the New Server option, the Secure Store Service is 
conﬁ gured for you. For all other installation scenarios, you must manually create and conﬁ gure a 
service application and generate a master encryption key for Secure Store Service. This is completed 
by performing the following steps: 
 1. 
In Central Administration, in Application Management, click Manage service applications.
 2. 
In the Service Applications Ribbon, in Create, click New.
 3. 
Select Secure Store Service. 
 4. 
In the Create Secure Store Application page, enter a name for the application. 
 5. 
In Database, specify the SQL Server instance that will host the database for this service 
application. The default value is the SQL Server Database Engine instance that hosts the 
farm conﬁ guration databases.
 6. 
In Database Name, enter the name of the service application database. The default value is 
Secure_Store_Service_DB_<guid>. The default name corresponds to the default name of 
the service application. If you entered a unique service application name, follow a similar 
naming convention for your database name so that you can manage them together.
 7. 
In Database Authentication, the default is Windows Authentication. If you choose SQL 
Authentication, refer to the SharePoint administrator guide for guidance on how to use the 
authentication type in your farm.
 8. 
In Application Pool, select Create new application pool. Specify a descriptive name that can 
help other server administrators identify how the application pool is used. 
 9. 
Select a security account for the application pool. Specify a managed account to use. This 
should be a domain user account. 
 10. 
Accept the remaining default values, and then click OK. The service application appears 
alongside other managed services in the farm’s service application list. 
 11. 
Click the Secure Store Service application from the list.
 12. 
In the Service Applications Ribbon, click Manage.
 13. 
In Key Management, click Generate New Key.
 14. 
Enter and then conﬁ rm a pass phrase. The pass phrase will be used to add additional secure 
store shared service applications.
 15. 
Click OK.


832  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
PowerPivot System Service and stores the username and password entered by the person deﬁ ning the 
schedule, PowerPivot System Service uses that target application rather than creating a new one.
The primary advantage to use this credential option is ease of use and simplicity. Advance work 
is minimal because target applications are created for you. Also, running data refresh under the 
credentials of the schedule owner (who is most likely the person who created the workbook) 
simpliﬁ es permission requirements downstream. Most likely, this user already has permissions on 
the target database. When data refresh runs under this person’s Windows user identity, any data 
connections that specify “current user” work automatically. 
The disadvantage is limited management capability. Although target applications are created 
automatically, they are not deleted automatically or updated as account information changes. 
Password expiration policies might cause these target applications to become out of date. Data 
refresh jobs that use expired credentials will start to fail. If alerting is conﬁ gured, DBA’s can get an 
email or test alert. When this occurs, schedule owners need to update their credentials by providing 
current username and password values in a data refresh schedule. A new target application is 
created at that point. Over time, as users add and revise credential information in their data refresh 
schedules, you might have a large number of auto-generated target applications on your system. 
Currently, there is no way to determine which of these target applications are active or inactive, nor 
is there a way to trace a speciﬁ c target application back to the data refresh schedules that use it. In 
general, you should leave the target applications alone because deleting them might break existing 
data refresh schedules. Deleting a target application still in use causes data refresh to fail with the 
message Target Application Not Found appearing in the data refresh history page of the workbook.
If you choose to disable this credential option, you can safely delete all of the target applications 
that were generated for PowerPivot data refresh.
Step 3: Create Target Applications to Store Credentials Used in Data Refresh 
When Secure Store Service is conﬁ gured, SharePoint administrators can create target applications 
to make stored credentials available for data refresh purposes, including the PowerPivot unattended 
data refresh account or any other account used to either run the job or connect to external data 
sources.
Recall from the previous section that you need to create target applications for certain credential 
options to be usable. Speciﬁ cally, you must create target applications for the PowerPivot unattended 
data refresh account, plus any additional stored credentials that you expect would be used in data 
refresh operations. 
Step 4: Conﬁ gure the Server for Scalable Data Refresh 
By default, each PowerPivot for SharePoint installation supports both on-demand queries and 
scheduled data refresh. 
For each installation, you can specify whether the Analysis Services server instance supports both 
query and scheduled data refresh, or is dedicated to a speciﬁ c type of operation. If you have multiple 
installations of PowerPivot for SharePoint in your farm, consider dedicating a server for just data 
refresh operations if you ﬁ nd that jobs are delayed or failing.


834  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
used during data refresh, the account used to run the data refresh job becomes the current 
user. As such, this account needs read permissions on any external data source accessed via 
a trusted connection.
Did You Enable the PowerPivot Unattended Data Refresh Account? If yes, then you should 
grant that account read permissions on data sources accessed during data refresh. The rea-
son why this account needs read permissions is because in a workbook that uses the default 
authentication options, the unattended account will be the current user during data refresh. 
Unless the schedule owner overrides the credentials in the connection string, this account 
needs read permissions on any number of data sources actively used in your organization. 
Are You Using Credential Option 2: Allowing the Schedule Owner to Enter a Windows 
Username and Password? Typically, users who create PowerPivot workbooks already 
have sufﬁ cient permissions because they have already imported the data. If these users 
subsequently conﬁ gure data refresh to run under their own Windows user identity, their 
Windows user account, which already has rights on the database, will be used to retrieve 
data during data refresh. Existing permissions should be sufﬁ cient.
Are You Using Credential Option 3: Using a Secure Store Service Target Application 
to Provide a User Identity for Running Data Refresh Jobs? Any account used to run a 
data refresh job needs read permissions, for the same reasons as those described for the 
PowerPivot unattended data refresh account.
Step 7: Enable Workbook Upgrade for Data Refresh 
By default, workbooks created using the SQL Server 2008 R2 version of PowerPivot for Excel 
cannot be conﬁ gured for scheduled data refresh on a Microsoft SQL Server 2012 version of 
PowerPivot for SharePoint. If you host newer and older versions of PowerPivot workbooks in your 
SharePoint environment, you must upgrade SQL Server 2008 R2 workbooks ﬁ rst before they can be 
scheduled for automatic data refresh on the server. 
Step 8: Verify Data Refresh Conﬁ guration 
To verify data refresh, you must have a PowerPivot workbook published to a SharePoint site. You 
must have Contribute permissions on the workbook and permissions to access any data sources 
included in the data refresh schedule.
When you create the schedule, select the Also Refresh as Soon as Possible check box to run data 
refresh immediately. You can then check the data refresh history page of that workbook to verify 
that it ran successfully. Recall that the PowerPivot Data Refresh timer job runs every minute. It can 
take at least that long to get conﬁ rmation that data refresh succeeded.
Be sure to try all the credential options you plan to support. For example, if you conﬁ gured the 
PowerPivot unattended data refresh account, verify that data refresh succeeds using that option. 
If data refresh fails, refer to the Troubleshooting PowerPivot Data Refresh page on the TechNet wiki 
for possible solutions. This can be found at http://technet.microsoft.com.
➤
➤
➤

Data Refresh ❘ 835
Modify Conﬁ guration Settings for Data Refresh 
Each PowerPivot service application has conﬁ guration settings that affect data refresh operations. 
This section explains the two major ways to modify those settings. 
Reschedule the PowerPivot Data Refresh Timer Job 
Scheduled data refresh is triggered by a PowerPivot Data Refresh timer job that scans schedule 
information in the PowerPivot service application database at 1-minute intervals. When data 
refresh is scheduled to begin, the timer job adds the request to a processing queue on an available 
PowerPivot server.
You can increase the length of time between scans as a performance tuning technique. You can also 
disable the timer job to temporarily stop data refresh operations while you troubleshoot problems. 
The default setting is 1 minute, which is the lowest value you can specify. This value is 
recommended because it provides the most predictable outcome for schedules that run at arbitrary 
times throughout the day. For example, if a user schedules data refresh for 4:15 P.M., and the timer 
job scans for schedules every minute, the scheduled data refresh request will be detected at 4:15 
P.M. and processing will occur within a few minutes of 4:15 P.M.
If you raise the scan interval so that it runs infrequently (for example, once a day at midnight), all 
the data refresh operations scheduled to run during that interval are added to the processing queue 
all at once, potentially overwhelming the server and starving other applications of system resources. 
Depending on the number of scheduled refreshes, the processing queue for data refresh operations 
might build up to such an extent that not all jobs can complete. Data refresh requests at the end of 
the queue might be dropped if they run into the next processing interval. 
To adjust the timer job schedule you can perform the following steps:
 1. 
In Central Administration, click Monitoring.
 2. 
Click Review Job Deﬁ nitions.
 3. 
Select the PowerPivot Data Refresh Timer Job. 
 4. 
Modify the schedule frequency to change how often the timer job scans for data refresh 
schedule information. 
Disable the Data Refresh Timer Job 
The PowerPivot data refresh timer job is a farm-level timer job that is either enabled or disabled for 
all PowerPivot server instances in the farm. It is not tied to a speciﬁ c Web application or PowerPivot 
service application. You cannot disable it on some servers to force data refresh processing to other 
servers in the farm.
If you disable the PowerPivot data refresh timer job, requests that were already in the queue will 
be processed, but no new requests will be added until you reenable the job. Requests that were 
scheduled to occur in the past are not processed. 

836  ❘  CHAPTER 23  SQL SERVER 2012 SHAREPOINT 2010 INTEGRATION
Disabling the timer job has no effect on feature availability in application pages. There is no 
way to remove or hide the data refresh feature in web applications. Users who have Contribute 
permissions or above can still create new schedules for data refresh operations, even if the timer job 
is permanently disabled.
Summary
SharePoint 2010 will continue to be an integral part of business intelligence within SQL Server 
2012. Many enhancements make setup and maintenance much easier. The setup and conﬁ guration 
of PowerPivot along with Reporting Services being consolidated to a shared service lead the way 
in helping to make SharePoint a better tool. The addition of new features such as Power View will 
undoubtedly enhance the SharePoint experience for everyone who will begin using the new BISM 
analysis service format and continue using PowerPivot. In the end SharePoint 2010 together with 
SQL Server 2012 is getting better, more user-friendly, and more intelligent. 
 
 
 
 



Conﬁ guring SQL Azure ❘ 839
Provisioning: Creates and provisions the databases you specify either through the Azure 
platform portal or SQL Server Management Studio
Billing and Metering: Handles the usage-based metering and billing on individual Azure 
platform accounts.
Connection Routing: Handles all the connections routed between applications and the 
physical servers where the data resides
Again, this layer is speciﬁ c to SQL Azure simply due to the need to route connections, meter 
and track usage, and provide billing around database usage. Additionally, this layer provides the 
database creation and provisioning functionality.
Platform Layer
This layer includes the physical servers and services that support the Services layer. It is the Platform 
layer that contains the many SQL instances of SQL Server with each instance managed by the SQL 
Azure Fabric. 
The key part of this layer is the SQL Azure Fabric, a distributed computing system that is installed on 
each physical SQL Server and is made up of tightly integrated networks, servers, and storage. The SQL 
Azure Fabric provides the automatic failover, load balancing, and automatic replication between servers.
Infrastructure Layer
The Infrastructure layer represents the physical IT administration of the actual hardware and 
operating systems that support the Services layer.
The key to understanding the SQL Azure architecture is to remember the responsibilities of the Service 
layer; connections aren’t connecting directly to the physical 
SQL Server. When connecting to SQL Server, connections 
are made to a physical server. In SQL Azure, connections are 
made through a TDS endpoint via the Services layer, which 
routes the connection to the physical server behind the Services 
layer. Figure 24-2 shows the differences between on-premises 
and SQL Azure connections.
You can see an example of this in the differences between server names of an on-premises SQL 
Server and SQL Azure. For example, when connecting to an on-premises SQL Server, the server 
name typically is the name of the physical server. In SQL Azure, the servername is a Fully Qualiﬁ ed 
DNS Name (FQDN), which follows the format of server.database.windows.net. 
The server portion of the FQDN is a unique 10-digit, randomly generated set of characters. The 
entire string must be used when making a connection to SQL Azure.
CONFIGURING SQL AZURE
After you create your Azure account, you are ready to start working with SQL Azure. First, you 
need to create your SQL Azure server and database and learn the different ways to work with SQL 
Azure. One of these is the Azure Management Portal, a web-based Azure management tool that 
➤
➤
➤
SQL Server
A Machine
SQL Azure
Server
A TDS
Endpoint
FIGURE 24-2

840  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
enables you to manage all aspects of the Azure platform, including your hosted services (web roles 
and worker roles), storage accounts, and the Azure AppFabric (Service Bus and Access Control). The 
following sections walk you through these processes. 
Server and Database Provisioning
The only way to provision (create) a new server is through the Azure Management Portal. You can 
access Azure Management Portal, shown in Figure 24-3, at http://Windows.Azure.Com.
Within the Azure Management Portal, you have the ability to manage all aspects of your SQL 
Azure subscription. Notice also (see Figure 24-3) that each account can have multiple subscriptions, 
each subscription can have multiple servers, and each server can contain multiple databases.
FIGURE 24-3
Creating a new SQL Azure server is as simple as selecting the appropriate subscription and clicking 
the Create button on the toolbar, which opens the Create Server Wizard. The Create Server Wizard 
guides you through a few steps necessary to create your SQL Azure server.
 1. 
The ﬁ rst step in the wizard is to select a region where you would like to host your SQL 
Azure server (see Figure 24-4). Picking the appropriate region in which to host your 
SQL Azure is crucial. There are six regions to choose from: Microsoft currently has two 
datacenters in the United States, two in Europe, and two in Asia. Technically, you should 
pick the datacenter closest to where you live.

Conﬁ guring SQL Azure ❘ 841
However, there are cases where that might 
not be true. For example, a company in 
Australia picked an Asia datacenter 
thinking that it would get better perfor-
mance by selecting a datacenter closest to 
it. It soon discovered that it actually got 
better performance by selecting the South 
Central U.S. datacenter because the pipe 
between Australia and the United States 
was much bigger than the pipe between 
Australia and Asia. While it is important 
to take those things into consideration, 
most of the times the best way to choose 
is to start with the datacenter closest to 
you and test it thoroughly.
 2. 
After you choose the appropriate region, 
select Next. The next page of the wizard asks you to create a login and password for the 
SQL Azure server, as shown in Figure 24-5. The login and password entered here is the 
server-level principle for the SQL Azure server you are creating. This login is equivalent to 
the SQL Server Administrator account (SA) for your on-premises server. 
 3. 
After you enter your login and password, click Next. The next page of the wizard asks you 
to specify ﬁ rewall rules on your SQL Azure server (see Figure 24-6). The ﬁ rewall rules can 
be speciﬁ ed at any time, and will be discussed in the next section. Go ahead and click Finish 
on the Create Server wizard. 
FIGURE 24-4
You have now successfully created your SQL Azure server. But before you begin learning how to 
work with it, you must take note of some important information. 
FIGURE 24-5
FIGURE 24-6

842  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
Server Name and Databases
After your server has been created, in the center section of the portal the Items List section displays 
the list of all the SQL Azure servers in your Azure subscription, including the one you just created. 
In the Navigation pane the name of the new server appears in the list of servers beneath your 
selected subscription, as shown in Figure 24-7. 
FIGURE 24-7
Just in case there is any confusion, click the subscription name in the Navigation pane. The center 
of the portal window, the Items List section, lists all the SQL Azure servers that pertain to your 
selected subscription. The list contains four columns:
Server Name
Administrator User
Region
Fully Qualiﬁ ed Server Name
Notice that the ﬁ rst part of the Fully Qualiﬁ ed Server Name is the same as the Server Name. When 
you hear people refer to the SQL Azure server, they are referring to the value in the ﬁ rst column; 
although, using the last column (Fully Qualiﬁ ed Server Name) as the server name reference isn’t 
harmful. In fact, when specifying the “server name” for an application connection string, it is the 
value of the Fully Qualiﬁ ed Server Name that needs to be used, like so:
 servername.database.windows.net
➤
➤
➤
➤

Conﬁ guring SQL Azure ❘ 843
You need to know this server name (Fully Qualiﬁ ed Server Name) when the topic of connecting 
to SQL Azure is discussed later in the chapter, so highlight and copy to the clipboard the entire 
SQL Azure server. The Fully Qualiﬁ ed Server Name is found in the Fully Qualiﬁ ed DNS Name 
(FQDN) area of the Properties pane. The Properties pane appears when you select the server 
in the Navigation pane. Even though you are not connecting to a physical computer, your 
SQL Azure server still behaves similarly to that of an on-premises SQL Server, meaning that a SQL 
Azure server contains a logical group of databases and acts as the central administrative point for 
multiple databases. As such, you can create many of the same objects that you can with on-premises 
databases such as tables, views, stored procedures, and indexes. Again, in the Properties pane of the 
portal, use the mouse to highlight the complete server name and copy that to the clipboard (Ctrl+C) 
as this will be used shortly.
With your server name still selected in the Navigation pane, a couple pieces of information need 
to be highlighted. When the SQL Azure server is created, the master database is automatically 
provisioned. This database is read-only and contains conﬁ guration and security information 
for your databases. You can see the master database in the list of databases shown previously 
in Figure 24-7. 
On the Server Information page, you also have the ability to manage your ﬁ rewall rules by 
selecting the Firewall Rules button. Here you can create, modify, or delete the rules necessary to 
allow access to your SQL Azure server. Firewall Rules is the ﬁ rst level of security for SQL Azure 
and are critical in protecting your data. Conﬁ guring SQL Azure Firewall Rules are discussed later in 
this chapter.
Creating a New Database
As of this writing, SQL Azure supports two database “editions” and several database sizes based 
on the edition. SQL Azure includes the Web and Business database editions. The Web database 
edition includes database sizes of 1GB and 5GB. With the Business database edition, you can select 
from a 10GB, 20GB, 30GB, 40GB, 50GB, and 150GB database. There really is no difference in the 
databases between editions, except for size. Meaning, the functionality of the database is the same 
regardless of the edition you choose. To create a new database, follow these steps: 
 1. 
Select your server and click the blue Create 
button on the toolbar that opens the Create 
Database dialog, shown in Figure 24-8. In 
this dialog, specify the name of the database 
and then select the database Edition and the 
associated Maximum size.
 2. 
Click OK on the Create Database dialog to 
ﬁ rst create your new database. You are then 
returned to the Azure Management Portal 
with your new database selected and the 
Database Information displayed, which contains information and links on how to develop 
and deploy applications with SQL Azure and connection information. Figure 24-9 displays 
the Database Information for this example.
FIGURE 24-8

844  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
 3. 
Figure 24-9 also displays a connections strings section on the right side of the page in 
the Properties pane. Click the ellipse button next to View here and you see a dialog 
in which connection strings for ADO.NET, ODBC, and PHP are already deﬁ ned. Copy 
the appropriate connection string for your development environment and paste it into 
your application. Change the password when you paste the connection string into your 
application and you are all set. 
Throttling and Load Balancing 
Throttling is SQL Azure’s mechanism for ensuring that one subscriber’s application or code (stored 
procedure or T-SQL) does not seize all the resources. Since SQL Azure works behind the scenes to 
provide a high-performing database, it uses a load balancer mechanism to help ensure that a server 
is not in a continuous state of throttling. To fully understand throttling and load balancing, let’s 
ﬁ rst take a step back and look at how, and where, SQL Azure creates new databases.
The goal for Microsoft SQL Azure is to maintain, currently, 99.9 percent availability for the 
subscriber’s database. This high availability is achieved through several methods. First, Microsoft uses 
commodity hardware that can be quickly and easily replaced in the case of hardware failure. Second, 
and more importantly, Microsoft implements the automatic management of database replicas. When 
you create a database, you actually get three databases; one primary and two secondary. These 
databases are always in sync, automatically, without any interaction from the end user. 
You, your application, or anyone else cannot access the secondary databases directly, but they 
are there and for an important purpose: if for any reason your primary database should become 
FIGURE 24-9


846  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
You could have developed a great application, designed a great database, set the proper connection 
string, and so on; but unless you deﬁ ne the appropriate ﬁ rewall rules, it will have been all for naught. 
The SQL Azure Firewall is the ﬁ rst level of security to help protect your data and prevent unwanted 
access to your SQL Azure server. All access to your SQL Azure server is rejected and blocked until 
you specify which computers have permission. Connection attempts coming from the Internet, and 
even within Azure itself, cannot reach your SQL Azure server until you specify who has access.
Firewall rules are IP address-based, and only acceptable addresses or ranges can be deﬁ ned. Firewall 
rules are deﬁ ned via the Azure Management Portal (refer to Figure 24-7) in the Server Information 
section. 
To conﬁ gure a ﬁ rewall rule, click the Firewall Rules button in the Management Portal in the Server 
Information section, which displays any deﬁ ned ﬁ rewall rules. Underneath the ﬁ rewall rules you see 
three buttons: Add, Update, and Delete, as shown in Figure 24-10.
To add a new ﬁ rewall rule, simply click the Add button, which displays the Add Firewall Rule 
dialog, as shown in Figure 24-11. In this dialog, simply give the new rule a unique name and provide 
the IP Address you would like to give access to SQL Azure. Alternatively, you can provide an IP 
Address range to allow multiple IP Addresses access.
FIGURE 24-10
FIGURE 24-11
Notice also (refer to Figure 24-11), that you can easily add your IP Address because the bottom of 
the dialog displays it. You can simply highlight, copy, and paste the IP Address into the Start and End 
Range boxes (if you have added only your single IP address). After you add your IP address, click OK. 
You then see your ﬁ rewall rule added to the management portal, as shown in Figure 24-12. At this 
point, you can connect to your SQL Azure via your applications and SQL Server Management Studio.
FIGURE 24-12


848  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
Now you are connected to your SQL Azure server via SSMS. Next the logical administration aspect 
of working with SQL Azure, including creating logins and users, is discussed.
ADMINISTERING SQL AZURE
One of the big misconceptions about SQL Azure is that it is nothing like SQL Server. The truth 
is that both SQL Server and SQL Azure use the same authorization model, with users and roles 
created in each database and associated to the user logins. SQL Server has ﬁ xed serverwide roles 
such as serveradmin, securityadmin, and dbcreated, which do not exist in SQL Azure. They 
don’t need to though because of the logical administration aspect of SQL Azure. Instead, SQL 
Azure has a loginmanager role to create logins and a dbmanager role to create and manage 
databases. These roles can be assigned to users only in the master database. 
Creating Logins and Users
SQL Azure provides the same set of security principles available in SQL Server authentication, which 
you can use to authorize and secure your data. In SQL Azure, logins are used to authenticate access to 
SQL Azure at the server level. Database users are used to grant access to SQL Azure at the database 
level, and database roles are used to group users and grant access to AQL Azure at the database level.
Creating a New Login
Creating a login is nearly identical to SQL Server except that you cannot create a login based on 
Windows credentials. Thus, all logins are SQL logins for SQL Authentication. The following steps 
outline how to do this. 
 1. 
In SQL Server Management Studio, open a new query window and connect to the master 
database using the administrator account created earlier. In the query window, type and run 
the following command:
CREATE LOGIN AzureTest WITH PASSWORD = ‘T3stPwd001’
FIGURE 24-14


850  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
If you attempt to create a user without ﬁ rst creating the login account, you receive a message stating 
that the user is not a valid login. Loginless users are not allowed in SQL Azure.
Assigning Access Rights
The next step in the Administration process is to assign the newly created user account access rights. 
To allow the AzureTest account to have unlimited access to the selected user database, you need to 
assign the user to the db_owner group:
EXEC sp_addrolemember ‘db_owner’, ‘AzureTest’
At this point, the AzureTest user can create tables, views, stored procedures, and more. But just like 
SQL Server, you can get granular with your permissions. You can grant and revoke permissions 
and grant insert, update, delete, create, and delete privileges just like you can with SQL Server. In 
SQL Server, user accounts are automatically assigned to the public role. This is not the case in SQL 
Azure because the public role cannot be assigned to user accounts for enhanced security. As a result, 
speciﬁ c access rights must be granted to use a user account.
WORKING WITH SQL AZURE
Working in SQL Server Management Studio has provided a nice and easy way to create databases 
and tables and do a lot of the management and maintenance of SQL Server through a great user 
interface. However, some of the better user interface features aren’t automatically available. There 
is still a way to enjoy them, though, if you are up for writing some code. To do so, perform the 
following steps:
 1. 
Right click the database node in Object Explorer and select New Database from the 
Context menu. A query window appears. Figure 24-17 shows this query window with the 
syntax required to create a SQL Azure database in SQL Server Management Studio. 
FIGURE 24-16

Working with SQL Azure ❘ 851
 2. 
You don’t get the Create Database dialog next, but instead, you get to write the Create 
Database statement yourself. As you can see in Figure 24-17, the syntax isn’t all that 
difﬁ cult. Specify the name of the database, the edition, and the size. If you leave out the 
edition and size, you simply get a 1GB Web edition database. 
FIGURE 24-17
Similarly, you can write code to create tables and views, and when working with permissions. 
Figure 24-18 shows the result of right-mouse-clicking the Tables node in Object Explorer and 
selecting New Table from the Context menu.
FIGURE 24-18

852  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
Even though the user-interface features aren’t quite there yet, that doesn’t mean that the 
functionality isn’t there. You just need to write code (same T-SQL syntax you are used to) to 
accomplish most of the things you need to do.
Backups with SQL Azure
One of the biggest questions potential Azure customers have is that of backups. Currently, backups 
with the on-premises SQL Server do not exist in SQL Azure. Instead, a feature called Database 
Copy enables you to make a transactionally consistent copy of your primary SQL Azure database 
into another SQL Azure database. The syntax for this is simple:
CREATE DATABASE DB2 AS COPY OF DB1
You can also copy databases between servers using the same syntax; you just need to be admins on 
both servers. However, administering two servers does entail paying for two databases, and more 
important, this makes it difﬁ cult to do daily backups and keep a history.
The other backup option is to use SQL Azure Data Sync Services. SQL Azure Data Sync Services 
is currently available via the SQL Azure Labs site (https://datasync.azure.com/SADataSync
.aspx) but does require you to sign up. SQL Azure Data Sync Services is a beautiful interim solution 
and is quite easy to set up and conﬁ gure. Microsoft understands that there is a need for a better and 
more efﬁ cient backup solution and is working hard at providing this functionality.
Object Explorer for SQL Azure
When you connect to SQL Azure in SQL Server Management Studio, you notice that a few nodes 
are missing — speciﬁ cally the Server Objects, Replication, and SQL Server Agent nodes. The 
following sections explain why these aren’t there (yet) and what you might expect down the road.
Server Objects
Looking at the subnodes in Server Objects, you see the following:
Backup Devices
SOAP Endpoints
Linked Servers
Triggers
There are a few speciﬁ c reasons why these items are not included in SQL Azure. 
Backup Devices: First, because you don’t have access to the physical hardware that SQL 
Azure runs on, you don’t need to create a backup device and won’t have access to it anyway. 
Second, except for the current backup solutions discussed earlier, there isn’t any other way 
to back up your databases. Until Microsoft provides more functional backup capabilities, 
the Backup Devices node doesn’t need to be there.
SOAP Endpoints: SOAP endpoints go back to SQL Server 2005 and enable you to 
essentially create web services that expose database access over HTTP. Microsoft has let it 
➤
➤
➤
➤
➤
➤

Working with SQL Azure ❘ 853
be known that this feature will be deprecated in future releases of SQL Server. SQL Azure 
has a better solution for SOAP Endpoints: OData. With just a few clicks you can expose 
your SQL Azure data via the OData protocol. The OData Service for SQL Azure provides a 
simple, no-code solution for providing an OData endpoint through an open HTTP protocol.
Linked Servers: These are typically used to handle distributed queries. Distributed queries 
aren’t supported in SQL Azure, but something even better will appear that makes the 
distributed queries a thing of the past. Microsoft announced in late 2010 a technology 
called SQL Azure Federation; the ability to partition, or shard, your data to improve the 
scalability and throughput of your database. 
Sharding is the process of breaking an application’s logical database into smaller chunks 
of data and then distributing those chunks of data across multiple physical databases to 
achieve application scalability. In sharding, one or more tables within a database are split 
by row and portioned out across multiple databases. This partitioning can be done with no 
downtime, and client applications can continue accessing data during sharding operations 
with no interruption in service.
There are quite a few blog posts and articles available on MSDN that drill into detail about 
this, but with SQL Azure Federation, Linked Servers aren’t needed.
Triggers: Enabling these in a shared environment creates a whole new level of complexity, 
such as potential security risks and possible performance issues. 
Replication
Due to SQL Azure Data Sync Services, replication just isn’t needed in SQL Azure. As mentioned 
earlier, there are three copies of every database you create: the primary and two replicas. SQL 
Azure keeps these in sync for you and automatically provides the high availability and redundancy 
you need.
If you need to “replicate” data from one database to another database, SQL Azure Data Sync Services 
provides an easy-to-use, wizard-driven interface that provides multi-direction data synchronization 
between two SQL Azure databases (primary to primary) or between SQL Azure and an on-premise 
SQL Server database. Therefore, replication just isn’t necessary in SQL Azure. 
SQL Server Agent
SQL Server agent doesn’t exist for SQL Azure, but there are many articles on the web that explain 
how to use a Windows Azure Worker Role to mimic the functionality of the SQL Server Agent.
The SQL Server agent is a Microsoft Windows Service that executes scheduled admin tasks called 
jobs and provides alerting capabilities. A good look at a Windows Azure Worker Role reveals that 
a Windows Azure Worker Role is basically a Windows Service in the cloud. Worker Roles are roles 
used to perform background tasks and long running/intermittent tasks. 
What makes Worker Roles great is that their structure is close to that of a Windows Service, 
including the starting, stopping, and conﬁ guration concepts that you see in a Windows Service. You 
can ﬁ nd a great blog post about this here: http://blogs.msdn.com/b/sqlazure/archive/2010/0
7/30/10044271.aspx.
➤
➤
 
 
 
 

854  ❘  CHAPTER 24  SQL AZURE ADMINISTRATION AND CONFIGURATION
WHAT’S MISSING IN SQL AZURE
As you start working with SQL Azure, or, if you are already familiar with SQL Azure, you will at 
some point wonder why SQL Azure doesn’t have a speciﬁ c feature or functionality you are looking 
for, such as Full-Text Search. For example, if you were to do a feature-to-feature comparison you 
would quickly see that at least the following features are not present in SQL Azure:
SQLCLR
Full-Text Search
Replication
SQL Agent
Encryption (TDE)
This is not a complete list, but the complete list is not a long one either. It is not publicly known the 
reason for the lack of certain features and functionality in SQL Azure. However, this section can at 
least provide information on some of the items in the above list.
The SQLCLR is in fact “partially” supported. For example, the XML and Spatial data types are 
actually CLR data types, and you will certainly ﬁ nd these data types in SQL Azure. What SQL 
Azure doesn’t support for SQLCLR is the ability to create assemblies in managed code (see Chapter 
7, “SQL Server CLR Integration”) and deploy those to SQL Azure. It is unknown if and when that 
will be supported.
Replication really isn’t needed, because of the existence of SQL Azure Data Sync Services, 
mentioned earlier in the chapter. SQL Azure Data Sync Services works far better and is far easier 
to conﬁ gure and use than Replication. Data Sync Services is built entirely on top of the Sync 
Framework and therefore includes a much richer data synchronization platform for moving data, 
including conﬂ ict handling and status reporting.
Microsoft is working hard at including encryption but there are certain problems they need to solve 
before you will see it included. The big issue really is how to support multiple levels of encryption in 
a shared environment. Remember that SQL Azure is a shared environment, and as such, databases 
are spread out over multiple instances. For example, my database and your database could 
potentially be located on the same server. In this scenario, the problem arises when I use one level of 
encryption and you use another. 
These are several examples of missing features and depending on the feature, it will elicit a different 
response. For example, SQL Azure has SQL Azure Reporting Services, but where is the rest of the 
Business Intelligent (BI) stack of Analysis Services and Integration Services? Microsoft is working on 
them, but when and how they will be included is yet to be seen.
The moral of this story is that some features you just won’t see because it doesn’t make sense to 
include them, as is the case in Replication. With other features you just need to be patient.
➤
➤
➤
➤
➤

Summary ❘ 855
SUMMARY
The goal of this chapter was to provide a solid overview of how to conﬁ gure and administer SQL 
Azure, including creating your SQL Azure server, creating databases, and discussing what happens 
behind the scenes to provide the great high-availability and failover needed in a cloud-based 
solution. 
The logical administration of SQL Azure plays an important role in understanding topics such as 
creating a SQL Azure Server and associated databases. While difference between on-premise SQL 
Server and SQL Azure do exist, SQL Azure still maintains power and ﬂ exibility as SQL Server “in 
the cloud.” 



Optionally, you can conﬁ gure one or more secondary replicas to support read-only access 
to secondary databases, and you can conﬁ gure any secondary replica to permit backups on 
secondary databases.
Every availability replica is assigned an initial role — either the primary role or the secondary role, 
which is inherited by the availability databases of that replica. The role of a given replica determines 
whether it hosts read-write databases or read-only databases. The primary replica is assigned the 
primary role and hosts read-write databases, which are known as primary databases. At least one 
secondary replica is assigned the secondary role. A secondary replica hosts read-only databases, 
known as secondary databases.
For the replicas to participate in Availability Groups they must be deployed as a Windows Failover 
Cluster (WFC); therefore, to implement an Availability Group, you must ﬁ rst enable Windows 
Failover Cluster in each server participating in the Availability Group. Inside of a WFC, an 
Availability Group is a cluster resource with its own network name and cluster IP address used for 
application clients to connect to the Availability Group. However, the major difference from a SQL 
Server cluster installation is that an Availability Group does not require a shared disk; that is, in this 
conﬁ guration, each server does not share its disks with the other server. 
Availability Modes
Each availability replica contains a property that determines its availability mode in relationship to 
the transaction record data movement that it receives, which determines how current its data is in 
relationship to the primary replica that is moving the data to all the other replicas. Two modes of 
availability are supported:
Asynchronous-commit mode: The primary replica commits transactions without waiting 
for acknowledgment that an asynchronous-commit secondary replica has hardened 
the transaction log. Asynchronous-commit mode minimizes transaction latency on the 
secondary replica databases but enables them to lag behind the primary replica databases, 
increasing the probably in case of a failure or possible data loss. 
Synchronous-commit mode: The primary replica waits for a synchronous-commit from 
the secondary replicas to acknowledge that it has ﬁ nished hardening the log. Synchronous-
commit mode ensures that when a given secondary replica database is synchronized with 
the primary replica database, committed transactions are fully protected. This protection 
comes at the cost of increased request latency and increases the user transaction times 
because the latency is included in the overall database response time. 
Types of Failover Supported
During failover, the primary and secondary replicas are interchangeable where one of the secondary 
replicas becomes the primary, and the former primary becomes the secondary replica. The new 
primary brings its Availability Group databases online and makes its databases available for read/
write operations. The former primary that is now the secondary replica then begins to receive 
transactional data from the new primary replicate after the failover. If the failover is caused by a 
failure to the former primary such that it became unavailable for a time, when the former primary 
returns online it automatically joins in its Availability Group and takes a secondary replica role. 
➤
➤
Architecture  ❘  859

860  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
There are three modes of failover: automatic, manual, and forced (with possible data loss). Support 
is based on the availability mode conﬁ gured, as shown in Table 25-1.
The following list explains each supported failover mode from Table 25-1 in greater detail.
Manual failover (without data loss): A manual failover occurs after a database 
administrator issues a manual-failover command and causes a synchronized secondary 
replica to transition to be the primary replica role (with guaranteed data protection) and 
the primary replica to transition as the secondary replica role. The database administrator 
can specify which secondary replica to failover to. A manual failover requires that both the 
primary replica and the target secondary replica run under synchronous-commit mode, and 
the secondary replica must already be synchronized. 
Automatic failover (without data loss): An automatic failover occurs in response to a 
failure that causes a synchronized secondary replica to transition to the primary role 
(with guaranteed data protection). When the former primary replica becomes available, it 
transitions to the secondary role. Automatic failover requires that both the primary replica 
and the target secondary replica run under synchronous-commit mode with the failover 
mode set to Automatic. In addition, the secondary replica must already be synchronized.
Manual (possible data loss): Under asynchronous-commit mode, the only form of failover 
is manual forced failover (with possible data loss). A forced failover should only be used 
for disaster recovery as there is a good risk of data loss. Manual is the only form of failover 
possible when the target secondary replica is not synchronized with the primary replica, 
even for synchronous-commit mode. 
Allowing Read-Only Access to Secondary Replicas
The main purpose of the Availability Group is to deliver high availability and disaster recovery, but 
in addition, a secondary replica can be conﬁ gured for read-only database access. In such a scenario, 
read-only workloads are ofﬂ oaded on the secondary replica databases, optimizing the primary replica 
to support read/write mission critical workloads. For example, if you want to run reports, rather than 
running them on the primary replica and overburdening it, you can select one of the secondary replicas 
to run the reports. You can also execute database backups from the secondary replica; full database, ﬁ le, 
and ﬁ legroup backups are supported but differential backup is not supported. Transaction log backup is 
supported in the secondary replica. Consider the following capabilities of read-only secondary replicas:
The secondary replica’s data is maintained to near real time with the primary replica. Real 
time depends on the network latency between the primary and secondary, the redo operation, 
locking on the secondary databases, and activities running on the secondary server.
➤
➤
➤
➤
TABLE 25-1: Supported Failover Modes
AVAILABILITY MODE
FAILOVER MODE SETTING
SECONDARY REPLICA FAILOVER SUPPORTED
Synchronous 
Manual 
Manual (without data loss)
Synchronous
Automatic
Automatic/Manual (without data loss)
Asynchronous 
Manual 
Manual (Possible data loss)

Read-only on the secondary replica applies to all the Availability Group databases.
To greatly reduce locking in the secondary read-only replica, the secondary replica 
databases are conﬁ gured by default in snapshot isolation to avoid read activities blocking 
the redo log updating the data. Snapshot isolation adds a 14-byte row identiﬁ er to maintain 
row version and maintains last committed row(s) in tempdb as the redo operation updates 
the replica databases.
For optimizing the read-only workload, any index required by the secondary replica or 
database statistics needs to be created in the primary replica database and transferred to 
all replicas by the transactional log data movement because the secondary is read-only. 
However, to support read-only workload, SQL Server 2012 does have the capability to 
create temporary database statistics that are stored in tempdb. These temporary statistics 
will be lost when the SQL Server instance is restarted or when the secondary replica is 
promoted to the primary during a failover. 
Similar to data mirroring, Availability Groups support automatic page repair. Each 
availability replica tries to automatically recover from corrupted pages on a local database 
by resolving certain types of errors that prevent reading a data page. If a secondary replica 
cannot read a page, the replica requests a fresh copy of the page from the primary replica. 
If the primary replica cannot read a page, the replica broadcasts a request for a fresh copy 
to all the secondary replicas and gets the page from the ﬁ rst to respond. If this request 
succeeds, the unreadable page is replaced by the copy, which usually resolves the error.
A client application can connect to a read-only replica. You can connect to a secondary 
replica that supports read-only access in one of two ways: either directly by specifying 
the SQL Server instance name in the connection string, or by using the Availability 
Group Listener name and leveraging read-only routing to reconnect to the next available 
secondary read-only replica. For more detailed information, see the “Secondary Replica 
Client Connectivity” section in this chapter. Access to the secondary replica is based on the 
Availability Group property, as shown in Table 25-2.
➤
➤
➤
➤
➤
TABLE 25-2: Setting for the Secondary Replicas to Support Read Operations
READABLE 
SECONDARY
DESCRIPTION
No
The secondary replica does not allow read operations; any connections to the 
secondary replica will be denied. 
Read-intent 
only
The secondary replica allows read operations but it accepts only connections with 
application Intent READONLY. This READONLY intent is a new connection property 
available as part of new TDS protocol. Older client applications or client applications 
that do not specify READONLY property for application intent cannot connect to the 
read-only secondary replica. To connect to the secondary replica, in the connection 
string, you must provide the following: ApplicationIntent = ReadOnly.
continues
Architecture ❘ 861


properties. Under AlwaysOn High Availability, check the Enable AlwaysOn Availability 
Groups check box, as shown in Figure 25-2.
Now you are ready to create the Availability Group. 
 1. 
Choose one of the SQL Servers as the primary replica; this SQL Server must have the 
database(s) that will be included in the Availability Group. In this example, it is SQLAG01 
and the AdventureWorks database.
 2. 
In the Microsoft SQL Server Management Studio, click on to the SQL Server, in this 
example SQLAG01, then right-click and choose New Availability Group Wizard, as shown 
in Figure 25-3. 
 3. 
In the Introduction, click Next and in the Specify Name, choose a name, for example AG1, 
as shown in Figure 25-4. 
FIGURE 25-2
FIGURE 25-3
Availability Group Example ❘ 863

864  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
 4. 
Click Next. Here on the Select Databases, click the check box for the databases, for 
example, AdventureWorks. If more than one database meets prerequisites, it will appear 
in the list for you to choose, as shown in Figure 25-5. For a database to be eligible to meet 
prerequisites, a database must:
Be a user database. System databases cannot belong to an Availability Group.
Be a read-write database. Read-only databases cannot be added to an Availability 
Group. 
Be a multiuser database.
Not use AUTO_CLOSE.
Use the full recovery model.
Possess a full database backup.
Reside on the SQL Server instance where you are creating the Availability Group 
and be accessible to the server instance. 
Not belong to another Availability Group.
Not be conﬁ gured for Database Mirroring.
➤
➤
➤
➤
➤
➤
➤
➤
➤
FIGURE 25-4

FIGURE 25-5
 5. 
Click Next. On the Specify Replicas, under the Replicas tab, choose any secondary replica 
that you want to participate in the Availability Group, as shown in Figure 25-6. Table 25-3 
also provides a description for each option available in this step. Click Next again. 
FIGURE 25-6
Availability Group Example ❘ 865

866  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
 6. 
On the Specify Replicas, under the Backup Preferences tab, specify where backups should 
occur from the following options as shown in Figure 25-7: 
Prefer Secondary: Speciﬁ es that backups should occur on a secondary replica except 
when the primary replica is the only replica online. In that case, the backup should 
occur on the primary replica. This is the default option.
Secondary Only: Speciﬁ es that backups should never be performed on the primary 
replica. If the primary replica is the only replica online, then the backup should 
not occur.
Primary: Speciﬁ es that the backups should always occur on the primary replica. 
This option supports creating differential backups, which are not supported when 
backup is run on a secondary replica.
➤
➤
➤
TABLE 25-3: Specify Replicas Option Descriptions for Step 5.
INITIAL ROLE
IDENTIFY THE INITIAL PRIMARY OR SECONDARY REPLICAS.
Automatic Failover 
(Up to 2)
If you want to conﬁ gure automatic failover, choose up to two 
availability replica to be automatic failover partners where one of 
the partners chosen must be the initial primary replica. Both of these 
replicas use the synchronous-commit availability mode and only 
two replicas are supported in automatic failover. 
Synchronous Commit 
(Up to 3)
If you selected Automatic Failover (Up to 2) for the two replica 
partners, Synchronous Commit (Up to 3) is automatically selected 
for them. 
Here you have the option to choose additional replicas to use 
synchronous-commit mode with only planned manual failover. Only 
three replicas are supported in synchronous-commit mode. Leave the 
checkbox blank if you want the replica to use asynchronous-commit 
availability mode. Then this replica will support only forced manual 
failover (with possible data loss). 
Readable Secondary
No: No direct connections are allowed to the secondary databases 
of this replica. They are not available for read access. This is the 
default setting.
Read-intent only: Only direct read-only connections are allowed to 
secondary databases of this replica. The secondary database(s) are 
all available for read access.
Yes: All connections are allowed to secondary databases of this replica, 
but only for read access. The secondary database(s) are all available for 
read access. 

 7. 
On Specify Replicas, under the Listener tab, choose Create An Availability Group Listener. 
Provide a Listener DNS Name, which is the network name that client applications use to 
connect to this Availability Group.
 8. 
By default SQL Server listens on port 1433. If you changed the port number to a non-default 
value, provide that port number value. 
 9. 
Then choose either a DHCP or Static IP for the Availability Group Listener. In this example, 
a Static IP was given to the Availability Group Listener.
 10. 
If you want, you can skip and create the Availability Group Listener later. For this 
example, the Listener is AG1_Listener with port number 1433, as shown in Figure 25-8. 
Click Next.  
FIGURE 25-7
Any Replica: Speciﬁ es that you prefer for backup jobs to ignore the role of the avail-
ability replicas when choosing the replica to perform backups. Backup jobs might 
evaluate other factors such as backup priority of each availability replica in combi-
nation with its operational state and connected state.
➤
Availability Group Example ❘ 867

868  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
 11. 
On the Select Initial Data Synchronization, choose Full. For Specify a Shared Network 
Location accessible by all replicas, choose a shared folder accessible by all participating 
replicas, for example \\SQLAG01\SQLBackups, as shown in Figure 25-9. Also, you can 
choose Skip Initial Data Synchronization and manually restore to each secondary replica. 
FIGURE 25-8
FIGURE 25-9

 12. 
Click Next to run the validation, as shown in Figure 25-10. Then click Next on the 
Summary. Click Finish to create the AG1 Availability Group. 
FIGURE 25-10
 13. 
When you ﬁ nish, you should be able to go to the Windows Failover Cluster Manager to see 
that the Availability Group has been created, as shown in Figure 25-11.
FIGURE 25-11
Availability Group Example ❘ 869

870  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
When a SQL Server login is added to the primary replica, the same login must be synchronized to 
exist on each secondary replica so that in a failover, that login will be available to authenticate. An 
Availability Group is not supported for System databases where the logins exists. However, new 
with SQL Server 2012, login data can be stored in the user database with the Contained Database 
feature, which means that any logins created will be synchronized as data by the Availability Group. 
If the database has been conﬁ gured with Containment Type set to Partial, as shown in Figure 25-12, 
the user-login relationship can be conﬁ gured and authenticated at database level rather than on the 
SQL Server’s System database. Detailed information on contained databases can be found in 
Chapter 4, “Managing and Troubleshooting the Database Engine.”
FIGURE 25-12
Conﬁ gure an Existing Availability Group
Now you have an Availability Group deployed and are relying on it for your high availability, 
disaster recovery, and reporting needs. At some point, a new requirement may arise if one of the 
following situations occurs:
The application is upgraded and an additional, new database is created that has 
dependencies to the other availability databases inside the Availability Group.
The company has increased reporting requirements and needs to deploy another read-only 
database to ofﬂ oad the work on the primary replica.
➤
➤

A disaster recovery site is identiﬁ ed and you are asked to deploy a replica there.
Backups are running on the primary replica and are impacting its performance and you 
have been asked to deploy a replica to perform backups.  
You have consolidated several databases from the Availability Group and need to remove 
one of them.
You want to remove a replica as it is no longer needed, or it has been replaced with another 
replica running on faster hardware.  
For situations like those in the preceding list, Availability Groups deliver the ﬂ exibility to start with 
at least a two-replica Availability Group, then deploy additional replicas and availability databases 
as your needs change. This section covers how to deploy additional replicas with a maximum of 
ﬁ ve or additional availability databases. You can remove replicas and availability databases when 
they are no longer needed as part of the Availability Group. When performing these operations in 
a production environment, consider doing so during your maintenance window and not during 
normal production time.   
Add/remove replica 
Anytime you want to add a replica, follow these steps:
 1. 
Join the new replica to the Windows Cluster group 
from Windows Failover Cluster Manager; see 
Chapter 16, “Clustering SQL Server 2012,” for 
help on doing so. 
 2. 
Enable AlwaysOn Availability Group for that SQL 
Server instance; refer to Figure 25-2. 
 3. 
In SQL Server Management Studio, on the Availability 
Groups folder, right-click the Availability Group for 
example AG1, and choose Add Replica as shown 
in Figure 25-13. Then follow the wizard to add a 
new replica.
To remove a replica, simply choose the replica, right-click, and Delete. 
To remove an Availability Group, choose the Availability Group; then right-click and choose Delete.
Add/remove a Database
Anytime you want to add an availability database, follow these steps:
 1. 
In SQL Server Management Studio, on the Availability Groups folder, right-click the 
Availability Group, for example AG1, and choose Add Database as shown in Figure 25-14. 
 2. 
Then, follow the wizard to add a new database. 
➤
➤
➤
➤
FIGURE 25-13
Availability Group Example ❘ 871
 
 
 
 


If you failed over outside of the automatic failover set of the Availability Group, adjust the 
quorum votes of the Windows Failover Cluster nodes to reﬂ ect your new Availability Group 
conﬁ guration (see “Chapter 16 “Clustering SQL Server 2012” for quorum settings).
If you failed over outside of the synchronous-commit failover set, consider adjusting the 
availability mode and failover mode on the new primary replica and on remaining secondary 
replicas, to reﬂ ect your desired synchronous-commit and automatic failover conﬁ guration.
For each scenario, you must manually resume each suspended database individually. On resuming, a 
secondary database initiates data synchronization with the corresponding primary database. When 
the former primary replica becomes available, it switches to the secondary replica role, becoming a 
secondary replica, and immediately suspends its now-secondary databases. Under the asynchronous-
commit mode, the accumulated unsent log is a possibility on any of the new secondary databases. 
Resuming a new secondary database causes it to discard unsent log records and to roll back any 
changes that were never received by the now-primary replica. 
If an availability replica that failed will not be returning to the availability replica or will return too 
late for you to delay transaction log truncation on the new primary database, consider removing the 
failed replica from the Availability Group.
Suspend an Availability Database
During a performance bottleneck, you may decide to suspend a secondary availability database. 
An availability database that is suspended means no transaction record data movement occurs and 
the transaction log on the primary replica keeps growing and cannot be truncated. If you suspend a 
secondary database on a secondary availability replica, only 
the local secondary database is suspended. If the suspend 
database is on the primary replica, transaction record 
data movement is suspended to all secondary databases 
on every secondary replica. When a secondary database is 
suspended, its database state is changed to SUSPENDED and 
it begins to fall behind the primary database. The primary 
database remains available, and if you have only one 
secondary replica, the primary database runs exposed.
To suspend a database using SQL Server Management 
Studio, follow these steps:
 1. 
In SQL Server Management Studio, connect 
to the SQL Server instance that hosts the 
availability replica on which you want to 
suspend a database.
 2. 
Expand the AlwaysOn High Availability, and 
then the Availability Groups folders. 
 3. 
Expand the Availability Databases folder, 
right-click the database, and click Suspend Data 
Movement, as shown in Figure 25-15.
 4. 
In the Suspend Data Movement dialog box, click OK.
➤
➤
FIGURE 25-15
Availability Group Example ❘ 873

874  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
To suspend a database using Transact-SQL, follow these steps: 
 1. 
Connect to the SQL Server instance that hosts the replica whose database you want to suspend. 
 2. 
Suspend the secondary database by using the following ALTER DATABASE statement:
    ALTER DATABASE database_name SET HADR SUSPEND;
Additionally, an availability database can be suspended by using the following PowerShell commands.
 1. 
Change directory (cd) to the server instance that hosts the replica whose database you 
want to suspend. 
 2. 
Use the Suspend-SqlAvailabilityDatabase cmdlet to suspend the Availability Group. 
Resume an Availability Database 
After a database has been suspended, when the replica is resumed, the initial state is SYNCHRONIZING 
until it catches up. The primary database eventually resumes all its secondary databases that were 
suspended as the result of suspending the primary database. To resume a database using SQL Server 
Management Studio, follow these steps:
 1. 
In SQL Server Management Studio, connect to the server instance that hosts the availability 
replica on which you want to resume a database.
 2. 
Expand the AlwaysOn High Availability, and then the Availability Groups folders. 
 3. 
Expand the Availability Databases folder, right-click the database, and click Resume 
Data Movement. 
 4. 
In the Resume Data Movement dialog box, click OK.
To resume a database using Transact-SQL, follow these steps:
 1. 
Connect to the server instance that hosts the database you want to resume. 
 2. 
Resume the secondary database by using the following ALTER DATABASE statement like so:
    ALTER DATABASE database_name SET HADR RESUME;
Additionally, an availability database can be resumed by using the following PowerShell commands:
 1. 
Change directory (cd) to the server instance that hosts the replica whose database you want 
to resume. 
 2. 
Use the Resume-SqlAvailabilityDatabase cmdlet to resume the Availability Group. 
Client Application Connections
To support failover, two options are available for client applications to connect to the primary replica: 
using the network name assigned to the Availability Group Listener or using the database-mirroring 
connection strings. When you create and conﬁ gure an Availability Group, you create the Availability 
Group Listener, which creates a network name and IP address in the Windows Server Failover Cluster 
(refer to Figure 25-8). Then when a failure occurs and the primary replica becomes unavailable, the 

secondary replica becomes the new primary replica with the network name in connection strings; the 
network name automatically redirects client applications to the new primary replica. You can provide 
client connectivity to the primary replica of a given Availability Group by using one of the two 
aforementioned methods, described in greater detail in the following two sections:
Availability Group Listener Network Name 
When the Availability Group fails over and the client applications have the network name in the 
connection strings, the network name directs connections to the new primary replica. You must 
create a network name that is unique in the domain for each Availability Group. As you create a 
network name, the IP address is assigned to the network name. Only the TCP protocol is supported 
for using a network name and, optionally, an IP to connect to an Availability Group. In the 
connection strings that your client applications use to connect to the databases in the Availability 
Group, specify the Availability Group network name rather than the server name; then applications 
can connect directly to the current primary replica. Following are examples of client application 
connection strings:
Server=tcp:MynetworkName;Database=AdventureWorks;IntegratedSecurity=SSPI
Server=tcp:MynetworkName,1433;Database=AdventureWorks;IntegratedSecurity=SSPI
Database-Mirroring Connection Strings
While performing a migration from database mirroring to an Availability Group, if an Availability 
Group contains only two availability replicas and is not conﬁ gured to allow read-access to the 
secondary replica, client applications can connect to the primary replica by using database 
mirroring connection strings. This approach can be useful while migrating an existing client 
application from database mirroring to Availability Group, as long as you limit the Availability 
Group to two availability replicas. However, before you add additional availability replicas, you 
need to create an Availability Group Listener network name for the Availability Group and update 
your client application to use it. When using database mirroring connection strings, the client 
can use either SQL Server Native Client or .NET Framework Data Provider for SQL Server. The 
connection string provided by a client application must minimally supply the name of one server 
instance, the initial partner name, to identify the server instance that initially hosts the availability 
primary replica to which you intend to connect. Optionally, the connection string can also supply 
the name of the secondary replica, which is the failover partner name, to identify the server instance 
that initially hosts the secondary replica as the failover partner name. 
ACTIVE SECONDARY FOR SECONDARY READ-ONLY
This section covers the secondary replica capabilities offered by Availability Groups in relation to 
running reports and read-only queries on the secondary replicas. Availability Groups enable read-
only secondary replicas. The primary purpose of secondary replicas is for high-availability and 
disaster recovery, but secondary replicas can be conﬁ gured to ofﬂ oad read-only queries for running 
reports from the primary replica. Then, the primary replica can better deliver the mission critical 
Active Secondary for Secondary Read-Only ❘ 875


two ways: either directly by specifying the SQL Server instance name in the connection string, or 
by using the Availability Group Listener name and leveraging read-only routing to reconnect to the 
next available secondary read-only replica. 
Read-only routing route connections that come into an Availability Group Listener change over 
to a secondary replica that is conﬁ gured to enable read-only workloads. Read-only routing works 
only if the connection string references an Availability Group Listener. An incoming connection 
referencing an Availability Group Listener name can automatically be routed to a read-only replica 
if the following are true:
The application intent of the incoming connection is set to read-only. 
The connection access supported on the secondary replica is set to read-only.
The READ_ONLY_ROUTING_URL for each replica is set by the CREATE or ALTER 
AVAILABILILTY GROUP Transact-SQL command, as part of the SECONDARY_ROLE replica 
options. You must set this option before conﬁ guring the read-only routing list.
The READ_ONLY_ROUTING_LIST is set for each replica in the CREATE AVAILABILITY GROUP 
or ALTER AVAILABILITY GROUP Transact-SQL command, as part of the PRIMARY_ROLE 
replica options. 
The READ_ONLY_ROUTING_LIST can contain one or more routing targets. You can conﬁ gure 
multiple routing targets, and routing will take place in the order targets are speciﬁ ed in the 
routing list.
The following example demonstrates modifying an existing Availability Group for read-only 
routing support: 
ALTER AVAILABILITY GROUP [AG1]
 MODIFY REPLICA ON
N’SQLAG01’ WITH 
(SECONDARY_ROLE (READ_ONLY_ROUTING_URL = N’TCP://SQLAG01. contoso.com:1433’));
ALTER AVAILABILITY GROUP [AG1]
 MODIFY REPLICA ON
N’SQLAG02’ WITH 
(SECONDARY_ROLE (READ_ONLY_ROUTING_URL = N’TCP://SQLAG02. contoso.com:1433’));
ALTER AVAILABILITY GROUP [AG1] 
MODIFY REPLICA ON
N’SQLAG01’ WITH 
(PRIMARY_ROLE (READ_ONLY_ROUTING_LIST=(‘SQLAG02’,’SQLAG01’)));
Then, based on the application intent speciﬁ ed in the connection string, (read-write or read-only 
property), the client application will be directed to either the read-write or read-only replica. In this 
example, the application intent connection string property is read-only and the connection will be 
routed to the read-only secondary replica.
Server=tcp:AGListener,1433;Database=AdventureWorks;IntegratedSecurity=SSPI;Applicati
onIntent=ReadOnly
➤
➤
➤
➤
➤
Active Secondary for Secondary Read-Only ❘ 877

878  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
Performance
To support concurrency and avoid blocking to prevent readers and writers from blocking each 
other, snapshot isolation is enabled by default on the secondary replicas’ databases. When read-only 
secondary replicas are enabled, the primary databases add 14 bytes of overhead on deleted, modiﬁ ed, 
or inserted data rows to store pointers to row versions on the secondary replica database(s). The row 
versioning structure is copied to the secondary databases, which generate row versions as needed 
by read-only workloads. Row versioning increases data storage in both the primary and secondary 
replicas. Also, as the 14-byte overhead is added to data rows, page splits may occur.
For running read-only workload, often indexes may need to be created to support these read-only 
queries. As the read-only secondary databases cannot create a new index, any indexes must be 
created in the primary replica and have the data movement apply them to the secondary replicas. 
Statistics on columns of tables and index views are transferred from the primary replica to the 
secondary replicas through the transaction record data movement, but the read-only workload in 
the secondary replica may require additional statistics. This is addressed where temporary statistics 
are allowed to be created by SQL Server on the tempdb of each secondary replica. If you need to 
create statistics yourself, they must be created in the primary replica. There are a few actions to keep 
in mind when creating these statistics: 
To delete temporary statistics, use the DROP STATISTICS Transact-SQL statement.
Monitor statistics using the sys.stats and sys.stats_columns catalog views where sys
.stats contains a column, which is_temporary, to indicate which statistics are permanent 
and which are temporary.
Temporary statistics will be automatically dropped if the secondary replica is restarted as 
tempdb is re-created or a failover event occurs and the replicas roles are switched.
The read-only secondary replicas are usually behind the primary replica, in most cases by seconds, 
as secondary replicas are impacted by the following:
The length of the database transaction 
The workload on the secondary replica; the timely transaction record data movement 
processing may be reduced if the replica is busy running read-only workload. 
The network latency between the primary and secondary replicas
When the workload is IO-intensive, that is, when data is transferred to the secondary 
replicas as an example of IO-intensive workloads, it can create index or bulk copy 
operations. 
In addition, a query in the secondary replica will see data from the primary replica after the redo 
thread in the secondary replica has applied that data. The steps for data to arrive to a query on the 
secondary replica require the following operations (see Figure 25-16):
 1. 
Data arrives to the secondary replica, and it is applied and hardened to the transaction log. 
 2. 
An acknowledgment is sent to the primary replica.
➤
➤
➤
➤
➤
➤
➤



identify the preferred location of where your database backup would run, you can easily determine 
by running this query: 
SELECT automated_backup_preference, automated_backup_preference_desc 
FROM sys.availability_groups;
To interpret the returned values from the query, see Table-26-5 for a description of the automated_
backup_preference and automated_backup_preference_desc columns.
TABLE 25-5: Values for the automated_backup_preference and automated_backup_preference_desc
automated_backup_preference
0 = Performing backups on the primary replica is 
preferable.
1 = Performing backups on a secondary replica is 
preferable.
2 = Performing backups on a secondary replica is 
preferable, but performing backups on the primary 
replica is acceptable if no secondary replica is 
available for backup operations.
3 = No preference about whether backups are performed 
on the primary replica or on a secondary replica.
automated_backup_preference_desc
PRIMARY
SECONDARY_ONLY
SECONDARY
NONE
To determine the priority of a given availability replica to run the database backup relative to the 
other replicas, it can easily be determined by running this query, where backup_priority represents 
the priority for performing backups on this replica relative to the other replicas in the same 
Availability Group. The value is an integer in the range of 0–100.
SELECT backup_priority FROM sys.availability_replicas;
ALWAYSON GROUP DASHBOARD
To monitor AlwaysOn Availability Groups, availability replicas, and availability databases in 
Microsoft SQL Server 2012, you can use the dashboard or data management views. From the 
dashboard, you can determine the health and performance of each Availability Group. To access the 
Availability Group Dashboard, follow these steps:
 1. 
In SQL Server Management Studio, connect to the instance of SQL Server on which you want 
to run Availability Group Dashboard, which is either the primary or a secondary replica.
AlwaysOn Group Dashboard ❘ 881

882  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
 2. 
Expand the AlwaysOn High Availability, and the Availability Groups folders. 
 3. 
Right-click on your Availability Group name, in this example AG1, and then click Show 
Dashboard as shown in Figure 25-18.
FIGURE 25-18
TABLE 25-6: Dashboard important values
Availability 
Group State
Displays the state of health for the Availability Group.
Role
The current role of the availability replica values is Primary or Secondary.
Primary Instance
Name of the server instance that is hosting the primary replica of the 
Availability Group.
Failover Mode
Displays the failover mode for which the replica is conﬁ gured. The possible 
failover mode values are: 
Automatic: indicates that one or more replicas is in automatic-failover mode
Manual: indicates that no replica is in automatic-failover mode
Table 25-6 gives an explanation of the key parameters that the dashboard displays.

Additionally, instead of using the AlwaysOn Group Dashboard, you can choose to use the 
sys.dm_hadr_availability_replica_states dynamic management view to query for the preceding 
information. 
MONITORING AND TROUBLESHOOTING
Like any solution, you need to monitor an Availability Group as part of regular operations to 
identify any issues with the Availability Group that may prevent the data from been synchronized 
with the secondary replicas or if failover to a secondary replica fails. To monitor Availability 
Groups, availability replicas, and availability databases using SQL Server Management Studio, 
follow these steps:
 1. 
In SQL Server Management Studio, connect to the instance of SQL Server on which you 
want to monitor an Availability Group and click the server name.
 2. 
Expand AlwaysOn High Availability, and the Availability Groups folders. 
 3. 
The Object Explorer Details pane displays every Availability Group for which the connected 
server instance hosts a replica. For each Availability Group, the Server Instance (Primary) 
column displays the name of the server instance currently hosting the primary replica. To 
display more information about a given Availability Group, select it in Object Explorer. 
The Object Explorer Details pane then displays the Availability Replicas and Availability 
Databases for the Availability Group.
To perform detail monitoring and troubleshooting, use the DMVs provided in Table 25-7 to 
identify, verify, and to then troubleshoot an Availability Group.
Synchronization 
State
Indicates whether a secondary replica is currently synchronized with 
primary replica; values are: 
Not Synchronized: database is not synchronized or has not yet been joined 
to the Availability Group
Synchronized: the database is synchronized with the primary database on 
the current primary replica, if any, or on the last primary replica.
NULL (Unknown): This value occurs when the local server instance cannot 
communicate with the WSFC failover cluster (that is the local node is not 
part of WSFC quorum).
TABLE 25-7: Monitoring and Troubleshooting for Availability Groups
sys.availability_groups
Returns a row for each Availability Group for which the local 
instance of SQL Server hosts an availability replica.
sys.dm_hadr_availability_
group_states
Returns a row for each Availability Group that possesses an 
availability replica on the local instance of SQL Server. 
continues
Monitoring and Troubleshooting ❘ 883

884  ❘  CHAPTER 25  ALWAYSON AVAILABILITY GROUPS
SUMMARY
AlwaysOn Availability Groups provide a robust high-availability solution that supports up to ﬁ ve 
partners. This solution can be conﬁ gured for synchronization and asynchronization modes with no 
shared disk infrastructure using a network name to seamlessly failover client applications across the 
replicas. In addition, you can use the secondary replicas to perform backup and to run read-only 
operations, therefore ofﬂ oading the primary replica. The Availability Group functionality leverages 
the best of breed between Windows Failover Clustering and SQL Server Data Mirroring and more 
to deliver a feature set to support mission critical applications throughout the enterprise. 
sys.availability_replicas
Returns a row for every availability replica in each availability 
group for which the local instance of SQL Server hosts an 
availability replica.
sys.dm_hadr_availability_
replica_states
Returns a row showing the state of each local availability 
replica and a row for each remote availability replica in the 
same availability group.
sys.dm_hadr_database_
replica_states
Returns a row for each database that is participating in any 
Availability Group for which the local instance of SQL Server 
is hosting an availability replica.
sys.dm_hadr_database_
replica_cluster_states
Returns a row containing information to provide detail into the 
health of the availability databases in each Availability Group 
on the Windows Server Failover Clustering (WSFC) cluster.
sys.availability_group_
listener_ip_addresses
Returns a row for every IP address that is currently online for 
an Availability Group listener.
sys.availability_
group_listeners
For a given Availability Group, returns either zero rows 
indicating that no network name is associated with the 
Availability Group, or returns a row for each availability-group 
listener conﬁ guration in the WSFC cluster.
sys.dm_tcp_listener_states
Returns a row containing dynamic-state information for each 
TCP listener.
TABLE 25-7 (continued)


886
ALTER MASTER KEY ADD 
ENCRYPTION BY SERVICE 
MASTER KEY, 563
Alter permission, 185, 188–190
ALTER QUEUE, 152–153, 163
ALTER RESOURCE GOVERNOR, 
309, 311, 313
ALTER RESOURCE POOL, 310
ALTER SETTINGS, 171
ALTER TABLE, 213, 289, 291, 
292, 335, 453
AlwaysOn, 514, 857. See also 
Availability Groups
AlwaysOn_health, 320
AMD processors, 23, 242, 248, 554
AMO (Analysis Management 
Objects), 742
A-MSG token, 140
Analysis Management Objects 
(AMO), 742
Analysis Services (SSAS), 729–763
after-the-fact analysis, 
751–752
architectural components, 
732–733
ASSL, 733, 736–737
BI stack, 233, 695, 815, 818, 
854
cubes
MDX, 111, 731–732, 749, 
751–752, 761
MOLAP model, 730, 
731–732
proactive caching, 742
Process Cube dialog, 
736–737
processing, 741–742
Tabular model, 730, 732
databases
backup and restoring, 
745–748
deploying, 739–741
synchronizing, 748–749
DBA’s role, 731
events, monitoring, 749–750
Flight Recorder, 751–752
installation, 40–43
memory, 733
MOLAP model
aggregations, 753, 
755–758
calculations, 731
components, 731–732
deﬁ ned, 42, 730
dimensional model, 731
DSV, 731
expressions, 731
partition conﬁ guration, 
753–755
processing, 741–744
roles, 758–762
security, 758–762
storage modes, 752–753
Tabular model v., 730, 
732
overview, 730–731
performance tuning, 749–752
query processor, 732
server
administration, 733–734
properties, 734–735
Tabular model
components, 732
MOLAP v., 730, 732
processing, 744–745
query mode options, 
42–43
roles, 732
security, 762–763
tables, 732
traces for replay, 750–751
UDM Mode, 41–42
HOLAP, 42, 752–754
ROLAP, 42, 752–754
uninstalling, 51–52
Windows services, 735
XML for Analysis, 111, 
732–733, 742, 749, 758
XMLA listener, 732
Analysis Services Scripting 
Language (ASSL), 733, 
736–737
ANSI_NULL_DFLT_ON, 402
ANSI_NULLS, 402–403
@AnsiNullsStatus, 204
ANSI_PADDING, 402
ANSI_WARNINGS, 402
ANY, 154, 156
ApexSQL, 225
app_domain_ring_buffer_
recorded, 359
application boundary, 78–79
application domains 
(AppDomains), 169–170, 
177–178
application failure, 549
application optimization, 
273–274
ApplicationName, 339, 340
approximate numeric data types, 
13–14
architecture
Analysis Services, 732–733
Availability Groups, 
858–862
CPUs, 246–248
Database Mail, 133–134
log shipping, 611–612
NUMA, 246–248, 296–297
CC-NUMA, 296, 304
data locality, 306–307
interleave, 304
soft, 297, 304
service applications, 820
SQL Azure, 838–839
SQL Server 2012, 1–20
SQL Trace, 339–340, 397
web-based, 236–237
Windows Server System 
Reference Architecture, 236
archiving
backup/recovery, 604–605
Database Mail, 138–139
article, replication data, 474–475
Ascend, Microsoft, 65
A-SEV token, 140
assemblies, SQL CLR, 172–176
Assembly Load, 178–179
assert_fired, 359
ASSL. See Analysis Services 
Scripting Language
A-SVR token, 140
asynchronous messaging, 147–149
asynchronous operations, I/O 
process model, 275
asynchronous statistics updates, 
454
asynchronous-commit mode, 859
ATA (Advanced Technology 
Attachment), 252–253
Attach/Detach feature, 563
attended installations, 36–40
authentication
Excel Services data 
connections, 821–823
Kerberos, 769, 822, 828
SQL Server, 182–184
Visio Services, 828–829
Windows, 183–184, 822
AUTHORIZATION parameter, 151, 
154
ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY – AUTHORIZATION parameter

887
autogrow, 48, 58, 270, 277, 278, 317
autogrow fragmentation, 270
automatic failover
Availability Groups, 860
database mirroring, 661–664
automatic page repair, 680–681, 861
automation, 105–145. See also 
multiserver administration; 
SQL Server Agent
Database Mail, 133–139
EPMF, 211–212
Integration Services, 697
Maintenance Plans, 106–110
Designer, 108–110
Wizard, 106–108
package deployment, 709–712
SQL Server Agent, 110–133
SQL Server installations, 36
autoparam, 400, 401
Availability Groups (AlwaysOn 
Availability Groups), 857–884
architecture, 858–862
asynchronous-commit mode, 
859
availability modes, 859
clustering v., 514
contained databases, 858, 870
Dashboard, 881–883
example, 862–875
client application 
connections, 874–875
conﬁ gure, 862–872
failover operation, 
872–873
resume availability 
database, 874
suspend availability 
database, 873
failover, 859–860
forced failover, 857, 860, 872
log shipping v., 640, 687–688
monitoring, 883–884
overview, 857–858
permissions, 188
primary replicas, 858–859
restore considerations, 584
roles, 858–859
secondary replicas
backup, 879–881
client connectivity, 
876–877
deﬁ ned, 858–859
performance, 878–879
purpose, 875
read-only access, 860–862, 
876
synchronous-commit mode, 859
troubleshooting, 883–884
availability modes, 859
Available Mbytes, memory, 331
average page density, 438–439
Avg. Disk Sec/Read, 328
Avg. Disk Sec/Write, 328
Avg. Sec/Read, 257, 279
Avg. Sec/Transfer, 257
Avg. Sec/Write, 257, 279
AWE memory, 302, 306, 332
Azure Management Portal, 839–840
Azure platform, 837, 839, 840, 845. 
See also SQL Azure
B
backups
Analysis Services databases, 
745–748
compression, 570–571, 607, 
637–638
copying databases, 562–570
deﬁ ned, 559
history tables, 575–576
log shipping, 633–634
managing, 593–594
performance, 594
permissions, 576–577
plan development/execution
database maintenance 
plans, 589–591
Management Studio, 
585–591
T-SQL backup commands, 
591–593
secondary replicas, 879–881
SQL Azure, 852
types, 559–562
verify backup images, 578–579
backup and recovery, 547–606
archiving data, 604–605
partitioned views, 604–605
table partitioning, 604–605
failures, 548–550
application, 548
data modiﬁ cation, 548–549
hardware, 548
software, 550
too much privileges, 549
T-SQL user errors, 548–
549
planning, 551–554
analyze business 
requirements, 551–553
categorize databases 
by recovery criteria, 
553–554
recovery, 594–604
data usage patterns, 
582–583
deﬁ ned, 580, 594
disaster recovery plan, 
554–559, 609–610
disaster recovery site, 550, 
554–556, 634, 871
high-availability solutions, 
584
maintenance time window, 
583–584
models, 571–575
preparations, 581–585
requirements, 581–582
restore v., 580
restore, 579–581
Analysis Services 
databases, 745–748
performance, 594
permissions, 576–577
recovery v., 580
system databases, 602–604
T-SQL restore command, 
602
SQL Server installation process, 
51
Backup Database dialog, 586–589, 
745–746
Backup Device/Device Throughput 
Bytes/sec, 571
Backup Set area, Back Up Database 
dialog, 587
backward compatibility, 67–68
balance, of resources, 239–241, 274
balanced tree, 429
baseline
current metrics v., 319
establishing, 318–319
recapturing, 319
references, 235–236
UAFS, 65–66
basic volumes, 269
BCP command-line application, 186, 
351, 549, 563, 572
autogrow – BCP command-line application

888
BEGIN DIALOG 
CONVERSATION, 154, 157, 
158
behavior changes, SQL Server 
2012, 68
Best Practices Analyzer (BPA), 50, 
321, 396–397
BI (Business Intelligence) stack, 
233, 695, 815, 818, 854. 
See also Analysis Services; 
Integration Services; 
Reporting Services
BIDS (Business Intelligence 
Development Studio), 69, 628, 
699, 742, 756, 758, 770, 786. 
See also SQL Server Data 
Tools
big picture, web-based architecture, 
236–237
binary, 14
binary data types, 14
binding, algebrizer, 411–412
BISM (Business Intelligence 
Semantic Model), 2, 4, 42, 
820, 836
bit, 13
blank report, Report Builder, 788
blocking information query, 
380–381
bookmark lookup operations, 458
bottlenecks
CPU resource issues, 322–324
data usage patterns, 238
disk, 329–330
disk controllers, 27
disk I/O, 274–275, 324
global resource management, 
303
memory, 333
network I/O, 252
stored procedure naming 
conventions, 203
BPA (Best Practices Analyzer), 50, 
321, 396, 397
broken log chain, 561
broker_activation_task_
aborted, 359
broker_activation_task_
started, 359
BROKER_INSTANCE, 155
brute force attacks, 184
buffer, disk, 256
Buffer Cache Hit Ratio, 302, 332
buffer cache hit ratios, 451
Buffer Manager - Counter: 
Database Pages, 332
Buffer Manager - Counter: Target 
Pages, 332
Buffer Manager - Counter: Total 
Pages, 332
Buffer Manager: Page Life 
Expectancy, 302, 333
Buffer Manager: Page reads/sec, 
329
Buffer Manager: Page writes/sec, 
329
buffer pool memory usage query, 
385–386
Buffers in Use, 705
Buffers Spooled, 705–706
Build, db_version table, 230
builds, 54
built-in service accounts, Report 
Server, 768
bulk logged recovery model, 572, 
573–574
bulk transaction-log backup, 562
bulkadmin ﬁ xed server role, 
188–189
burn in/stress test, 45
Business database edition, SQL 
Azure, 843–844
business intelligence DBAs
role, 3–4, 233
SQL Server 2012 features, 3–4
Business Intelligence Development 
Studio (BIDS), 69, 628, 699, 
742, 756, 758, 770, 786. See 
also SQL Server Data Tools
Business Intelligence Edition, SQL 
Server, 17–18
Business Intelligence Semantic 
Model (BISM), 2, 4, 42, 820, 
836
Business Intelligence (BI) stack, 
233, 695, 815, 818, 854. 
See also Analysis Services; 
Integration Services; 
Reporting Services
business requirements, backup/
recovery plan, 551–553
C
-c switch, 76
cache
Buffer Cache Hit Ratio, 302, 
332
buffer cache hit ratios, 451
cache coherency, 247, 
296–297, 304
CPU performance, 242–243, 
297
DBCC FREEPROCCACHE, 
407, 410
disk, 256
L1, L2, L3, 242–243
proactive caching, 742, 
754–755
read/write, 256
report cache, 811–812
sys.dm_exec_cached_
plans, 179, 404, 407, 409
sys.syscacheobjects, 
409
Cache Refresh Options, Manage 
web pages, 812
cache refresh plan, reports, 812
calculations, MOLAP, 731
capacity planning, tempdb, 
277–279, 446
car company example, 484. See also 
snapshots
cardinality, 404, 439, 447
CAS (Control Access Security), 176
case sensitivity
collations, 28
PowerShell, 225
service name, routes, 155
sqlcmd switches, 219, 223
tokens, 139
catalog views
database mirroring, 658–661
sys.database_
mirroring, 658–660
sys.database_
mirroring_endpoints, 
660–661
sys.database_
mirroring_witnesses, 
660
XEvents, 365
CC-NUMA, 296, 304
cellular multiprocessing (CMP), 
296–297
central management servers (CMS), 
98–99, 211–212
certiﬁ cates, Service Broker, 
162–163
BEGIN DIALOG CONVERSATION – certiﬁ cates, Service Broker
 
 
 
 

889
change management, 197–231. See 
also Policy-Based Management
challenges, 197
Data-tier Applications, 
225–229
DDL triggers, 212–218
projects/solutions, 198–200
scripting, 218–225
SSDT, 229
version tables, 229–231
change_tracking_cleanup, 
359
ChangeWindow, 215–216
changing roles
database mirroring, 661–666
log shipping, 627–633
channels, events, 358–360
char, 12
character data types, 12
Chart Wizard, 788
checkpoint_begin, 359
Cherry, Denny, 183
CIM (Common Information 
Model), 142
classiﬁ cation, Resource Governor, 
311–312
cleaning up indexes, 462–463
client application connections, 
Availability Group example, 
874–875
Client layer, SQL Azure 
architecture, 838
client-side traces, 340
clouds. See also SQL Azure
cloud-optimized SQL Server 
2012, 19
System Center Advisor, 321, 
396–397
CLR. See SQL CLR
clustered index scan
ordered, 431–432
unordered, 429–431
clustered indexes
clustered index seek with 
ordered partial scan, 
436–438
deﬁ ned, 456
clustering (Windows Failover 
Clustering), 509–546
active/active, 518
alternatives, 512–514
big picture, 514–520
capabilities, 510–511
database mirroring v., 514, 
686–687
deﬁ ned, 510–512
Failover Cluster Management 
tool, 528, 530–531, 533, 
539–543, 869, 871
failover mechanics, 517–518
geographically dispersed, 509, 
511–512, 546, 550, 554, 
608, 635
installation, 528–531
cumulative updates/service 
pack, 540
pre-installation steps, 
527–528
SQL Server instance, 
534–540
licensing, 519
limitations, 511–512
log shipping
comparison, 513, 608–
609, 614
integration, 635
monitoring, 542–543
MSDTC, 532–533
multi-node, 518–519
options, 518–519
pre-installation steps, 527–528
preparations, 523–527
hardware, 524–527
network infrastructure, 
523–524
public/private networks, 
516–517
quorum, 515–516
quorum drive, 515–516, 
525–526
reasons for, 512
rebuilding, 521–523
recovery considerations, 584
testing, 540–542
third-party clustering 
solutions, 514
troubleshooting, 543–546
two-node, 515–516, 518
upgrading, 520–523
clusters. See clustering
cmdlets, 224–225
CMP (cellular multiprocessing), 
296–297
CMS (central management servers), 
98–99, 211–212
cold backup server, 512–513
collaboration features, SQL Server 
2012, 2
collations, 27–29
collection items, 387
collection sets
creating, 390–392
deﬁ ned, 388, 397
system data, 388
Disk Usage, 388–389
Query Activity, 387–388
reports, 388–389
Server Activity, 388
viewing, 388–389
Colmodctr, 403–404
column-based indexes, 450–451, 
457, 471
columns
nonkey, 454, 456
statistics, 439
XML, indexes, 454
columnstore indexes, 449, 
450–452, 471
comma-separated list, contracts, 
154
command line
parameters, 31
sqlcmd, 219–221
unattended installation, SQL 
Server, 30–31
Common Information Model 
(CIM), 142
Common Language Runtime. See 
SQL CLR
community technology preview 
(CTP), 54–55, 174
compilation. See query compilation
compliance, 197, 201, 207, 212, 
231. See also Policy-Based 
Management
Component Services Management 
Console, 532–533
compression, 290–296
backups, 570–571, 607, 637, 
638
cost savings, 293–294
dictionary, 291
monitoring, 295
page compression, 291–293
partitioned tables/indexes, 453
preﬁ x, 291
row compression, 290–291
space savings, 293–295
stream, 645
change management – compression

890
compression (continued)
VertiPaq engine, 42–43, 450, 
451, 457, 816
when to use, 295–296
Compression section, Back Up 
Database dialog, 589
CONCAT_NULLS_YIELDS_NULL, 
402
conditions, PBM, 202, 204–205, 
209
conﬁ guration
Availability Group example, 
862–872
Database Mail, 135–138
disk adapters, 267–268
Integration Services service, 
700–704
MOLAP partition 
conﬁ guration, 753–755
Service Broker, 149–157
SQL Server Agent, 129–133
conﬁ guration ﬁ le
unattended installation, SQL 
Server, 31–32
XML, Integration Services 
service, 700–701
Conﬁ guration Manager (Reporting 
Services)
databases, 771–773
deﬁ ned, 765, 814
e-mail settings, 774
encryption keys, 776
Execution Account, 775, 818
launching, 766
opening screen, 766–768
reasons for use, 765–766, 814
Report Manager URL, 
773–774
Scale-out Deployment, 
777–778
server properties dialog, 
778–784
service accounts, 768–769
SQL Server Conﬁ guration 
Manager v., 765
web service URL, 769–771, 
774, 787, 795
Conﬁ guration Manager (SQL 
Server)
deﬁ ned, 50, 72–73
Reporting Services 
Conﬁ guration Manager v., 
765
Windows Services Snap-In, 
699, 700, 702–704
conﬁ guration tools, 71–79
partially contained databases, 
78–79
startup parameters, 73–76
startup stored procedures, 
77–78
ConfigurationFile.ini, 32
:CONNECT, 222
Connect permission, 185
Connect Using option, 800–801
connected users query, 384
connecting to SQL Azure, 847–848
Connection page
Server Properties screen, 88
SQL Server Agent 
conﬁ guration, 132–133
connections
projects, 199–200
Tabular model, 732
Connections folder, 198–200
constant folding, 402
contact list, disaster recovery plan, 
556
contained databases
Availability Groups, 858, 870
deﬁ ned, 78
log-shipping dependencies, 
627–630
partially, 78–79
contained users, 182–183
context switching, 299
contracts, 151–152, 154
Control Access Security (CAS), 176
Control permission, 185, 188, 190
conversations. See also Service 
Broker
conversation groups, 156–157
conversation locks, 156
priorities, 156
reusable, 158–160
Copy Database Wizard, 57, 
567–570
copy phase, restore, 580
copying databases, 562–570
copy-only backup, 561
core, 245–246
correctness, recompilations, 
401–403
correlating traces, 349–351
cost threshold for parallelism, 89, 
417
counters. See also Performance 
Monitor; speciﬁ c counters
CPU resources, 322–324
database mirroring, 670–672
% disk, 329
disk I/O, 324–330
Integration Services service, 
705–706
logical disk, 325–327
memory, 302–303, 331–333
mirroring thresholds, 676–678
.NET CLR Exceptions, 178
.NET CLR Loading, 177–178
.NET CLR Memory, 177, 178
performance condition alerts, 
122–124
physical disk, 325–327
replication-related, 505
transfer, 329
covering indexes, 456
covering non-clustered index scan
ordered, 433
unordered, 432–433
CPUs
AMD processors, 23, 242, 
248, 554
balance of resources, 239–241, 
274
cache, 242–243, 297
Core licenses, 19
critical target, 235
hot-added, 296–297
hyper-threading, 243–244
Intel processors, 23, 242, 244, 
248, 554
multicore processor systems, 
244–246
nodes, 297, 298, 305
NUMA, 246, 247–248
performance tuning, 239–248, 
296–302
resource counters, 322–324
64-bit platform, 69, 242, 
305–306
SMP, 246–247
SQL Server 2012 
requirements, 23
system architecture, 246–248
x64 platform, 242, 305–306
CREATE CERTIFICATE, 162
Create Cluster Wizard, 530–531
Create Database dialog, 843–844
CREATE ENDPOINT, 67, 163, 646
compression – CREATE ENDPOINT

891
CREATE MASTER KEY, 162
CREATE PROCEDURE, 175
CREATE QUEUE, 163
CREATE RESOURCE POOL, 310
CREATE ROUTE, 155
Create Server Wizard, 840–843
CREATE SERVICE, 154
CREATE USER, 183
CreateCertOnPrincipal.
sql, 647–649
critical targets, performance 
tuning, 235
CRM (Customer Resource 
Management) application, 
481–482
cross database permission chains, 
191–193
CrystalDiskMark, 24
CTP (community technology 
preview), 54–55, 174
cubes. See also Analysis Services
MDX, 111, 731–732, 
749, 751, 752, 761
MOLAP model, 730–732
proactive caching, 742
Process Cube dialog, 736–737
processing, 741–742
Tabular model, 730, 732
cumulative updates, clusters, 540
Current Rate of New Transaction, 
675
Current Restore Rate, 675
Current Send Rate, 675
currently running queries, 377
current_user(), 194
cursor, 16
CURSOR_CLOSE_ON_COMMIT, 
402
cursor_manager_cursor_
end, 359
custom data types, 16
Customer Playback program, 54
Customer Resource Management 
(CRM) application, 481–482
custom_snapshots.max_
sales_id, 392
D
-d switch, 74, 76, 219–220
DAC (Data-tier Applications), 
225–229, 567
DAC (Dedicated Administrator 
Connection), 79–81
DAS (direct-attached storage), 25, 
27
Dashboard, Availability Group, 
881–883
data archiving. See archiving
data collection sets
creating, 390–392
deﬁ ned, 388, 397
system data, 388
Disk Usage, 388–389
Query Activity, 387–388
reports, 388–389
Server Activity, 388
viewing, 388–389
data columns, 339
data compression, 290–296
backup compression, 570–571, 
607, 637–638
cost savings, 293–294
dictionary, 291
monitoring, 295
page compression, 291–293
preﬁ x, 291
row compression, 290–291
space savings, 293–295
stream, 645
VertiPaq engine, 42–43, 450, 
451, 457, 816
when to use, 295–296
data connection libraries, 824–825
data connections, Excel Services, 
820–826
Data Deﬁ nition Language. See DDL
Data File(s) Size(KB), 278
data ﬂ ow engine, Integration 
Services, 699
data locality, 306–307
data loss, acceptable, 552
data marts, 551, 696
data mirroring. See database 
mirroring
data modiﬁ cation failures, 548–549
data modiﬁ cation query plan, 
443–444
data providers, 387, 821. See also 
Excel Services; Performance 
Monitor; Query Activity; SQL 
Trace; T-SQL
data refresh
Excel Services, 820–826
PerformancePoint Services, 
826–827
PowerPivot, 829–836
Visio Services, 827–829
data source type, 799–800
data source view (DSV), 731
data sources, Report Manager, 
799–800
Data Sources page, Manage web 
pages, 804
Data Sync Services, SQL Azure, 
853
Data Tools. See SQL Server Data 
Tools
data transfer rate, 253–254
data types, 10–16. See also speciﬁ c 
data types
binary, 14
character, 12
custom, 16
date, 14–15
numeric, 13–14
system, 15–16
time, 14–15
data usage patterns
performance tuning SQL 
Server, 238, 271
recovery plan, 582–583
data warehouses
Analysis Services, 755
columnstore indexes, 449, 
450–452, 471
disk controllers, 27
Enterprise Edition, 18
fragmentation, 438
Integration Services, 696–697
I/O latency monitoring, 328
Management Data Warehouse
deﬁ ned, 239, 388, 397
UCP v., 99, 321, 387
UMDW, 387–388, 397
MAXDOP value, 300
NTFS allocation unit size, 
269
OLTP v., 237, 274, 438
PowerPivot, 816
shared dimensions, 743
SQL Server Service Broker, 147
Tabular model, 730
Utility Management Data 
Warehouse, 387–388, 397
database administrators. See DBAs
CREATE MASTER KEY – database administrators

892
database availability scenarios, 
667–670. See also database 
mirroring
database category, events, 337
database connections
projects, 199–200
Tabular model, 732
Database Engine (SQL Server). See 
also performance tuning
uninstalling, 52
database ﬁ les
deﬁ ned, 4–5
location, I/O performance, 
275–276
database I/O information, 418–419
Database Mail, 133–139
account, 136
architecture, 133–134
archiving, 138–139
conﬁ guration, 135–138
Conﬁ guration Wizard, 134, 
135–136
SQL Mail v., 59, 67, 70, 133, 
682
database maintenance plans, 
backups, 589–591
database master key, 158, 162, 563
database mirroring, 641–693
automatic page repair, 
680–681
catalog views, 658–661
clustering v., 514, 686, 687
counters, 670–672
database snapshots, 692–693
diagram, 642
endpoints, 646–652
establish mirroring session, 
653–655
event listener setup, 678, 684, 
688–692
failover
automatic, 661–664
forced, 643, 657, 661, 
666, 688
manual, 664–665
mirror server preparation, 
681–685
log shipping
comparison, 686, 687–688
integration, 634
log-transfer process, 645, 657, 
693
mirror server
database availability 
scenarios, 668–669, 
670
deﬁ ned, 642
lost, 668–670
overview, 646
prepare for failover, 
681–685
principal server/mirror 
server synchronization, 
653
monitoring
Database Mirroring 
Monitor, 672–678
Performance Monitor, 
670–672
multiple databases, 685–686
operating modes, 643–645
high-performance, 
643–645, 657
high-safety operating 
mode with automatic 
failover, 643–645, 
655–657
high-safety operating 
mode without 
automatic failover, 
643–645, 655
SQL Server 2012 editions, 
658
overview, 641–643
partners, 642
prepare database, 652
principal server
database availability 
scenarios, 667–668
deﬁ ned, 642
lost, 667–668
overview, 646
principal server/mirror 
server synchronization, 
653
quorums, 656–657
re-mirroring, 643
role changing, 661–666
SQL Proﬁ ler, 678
SQL Server 2012 editions, 658
state change events, 678, 684, 
688–692
sys.database_
mirroring, 658–660
sys.database_
mirroring_endpoints, 
660–661
sys.database_
mirroring_witnesses, 
660
thresholds, on counters, 
676–678
transactional replication v., 
686, 687
witness server
database availability 
scenarios, 669–670
deﬁ ned, 643
lost, 669–670
overview, 646
SAFETY FULL, 
667–668
Database Mirroring Monitor, 
672–678
database reports, 84
database schemas. See schemas
database securables, 189–190
Database Settings page, Server 
Properties screen, 88
database shrinking, 48, 558
database snapshots
mirroring, 692–693
restore, 597–598
Database Tuning Advisor (DTA), 
280, 455, 458, 468–469
database-level DDL triggers, 
212–217
DatabaseMail.exe, 133, 134, 
135
databases. See also backups; 
backup and recovery; speciﬁ c 
databases
Analysis Services
backup and restoring, 
745–748
deploying, 737–741
synchronizing, 748–749
contained databases feature
Availability Groups, 858, 
870
deﬁ ned, 78
log-shipping dependencies, 
627–630
partially, 78–79
copying, 562–570
DDL triggers monitor database 
changes (example), 215–217
database availability scenarios – databases

893
partially contained, 78–79
provisioning, SQL Azure, 
840–844
Reporting Services, 771–773
resource, 6–7, 409
roles, MOLAP, 758–759
sample, 40
system databases
backing up, 577–578
rebuilding, 81–82
restoring, 602–604
standard, 6–8
Databases/Backup/Restore 
Throughput/sec, 571
database-scoped DMVs/DMFs, 376
databases_log_file_size_
changed, 360
database_started, 359
data-driven subscriptions, 
806–811
datasets, 787–788
Data-tier Applications (DAC), 
225–229, 567
date, 14–15
date data types, 14–15
DATE token, 139
DateInstalled, db_version 
table, 231
DateKeyRange, 460–461
datetime, 14–15
datetime2, 15
datetimeoffset, 15
DBAs (database administrators)
accidental, 233
Analysis Services, 731
business intelligence
role, 3–4, 233
SQL Server 2012 features, 
3–4
developer
performance tuning, 
237–238
role, 2–3, 233–234
SQL Server 2012 features, 
2–3
developer/business intelligence
role, 3–4
SQL Server 2012 features, 
3–4
hybrid, 233
performance tuning, 
234–237
production
performance tuning, 
238–239
role, 2, 233, 234
SQL Server 2012 features, 
2
SQL development team v., 
201, 699
web-based architecture big 
picture, 236–237
DBCC FlUSHPROCINDB, 410
DBCC FREEPROCCACHE, 407, 410
DBCC SHOW_STATISTICS, 439
dbcreator ﬁ xed server role, 189, 
577, 601, 848
DBMail. See Database Mail
dbo schema, 8
db_version table, 230–231
dcomcnfg, 532–533
DDL (Data Deﬁ nition Language)
events, 353
DDL triggers, 212–218
database-level, 212–217
event notiﬁ cations v., 353
monitor database changes 
(example), 215–217
server-level, 217–218
syntax, 212
DDLAudit, 215–216
DDL_DATABASE_LEVEL_
EVENTS, 143, 212–215
DDL_TABLE_EVENTS, 353
deadlock_monitor_state_
transition, 359
deadlocks
Extended Events, 336
trace ﬂ ags, 101, 336
decimal, 13
decision support environment 
(DSS), 237, 260, 274
decision tree, 556–557
Dedicated Administrator 
Connection (DAC), 79–81
default resource pool, 309–310
default route, 154
default trace, 320, 335, 337–338, 
355
default workload group, 310
defragmentation, 270–271
degreeOfParallelism, 425
delete option, Report Manager task 
menu, 802
Delete permission, 190
Delivered Cmds/Sec, Tx/Sec, 505
denormalization, 697
DENY, 184
dependencies, synchronizing, 
627–630
deployment
Data-tier Application, 
226–227
deploy log shipping approach, 
upgrade, 639
log shipping
initial conﬁ guration, 
614–616
Management Studio, 
616–623
scenarios, 608–610
T-SQL commands, 624
packages, 709–712
Deployment Wizard, 714, 737, 
738–741
deprecation
features, SQL Server 2012, 67
ntext, 345
SET options, 402
SOAP Endpoints, 853
SQL Proﬁ ler, 320, 345, 352, 
397
sysprocesses, 378
text, 345
Description, db_version 
table, 231
Designer, Maintenance Plans, 
108–110
Destination area
Back Up Database dialog, 587
Restore Database dialog, 599
destinations, data ﬂ ow engine, 
699
Detach/Attach feature, 563
Details View, 799
developer DBAs
performance tuning, 237–238
role, 2–3, 233–234
SQL Server 2012 features, 2–3
developer/business intelligence 
DBAs
role, 3–4
SQL Server 2012 features, 3–4
development team, SQL, 201, 224, 
414, 699
dictionary compression, 291
differential backup, 560
DimCustomer table, 463
dimensional model, 731
Databases/Backup/Restore Throughput/sec – dimensional model

894
dimensions
processing, 741
shared, 743
Direct Query Mode, 43
direct-attached storage (DAS), 25, 
27
DirectQueryWithInMemory Mode, 
43
disaster recovery
site, 550, 554–556, 634, 871
SQL Azure, 838
disaster recovery plan, 554–559
creating, 556–558
log shipping, 609–610
maintaining, 558–559
disasters, 551
discontinued/unsupported features, 
67. See also deprecation
disk adapters, 265–268
disk arrays, 26, 515. See also RAID
disk bottlenecks, 329–330
disk buffer, 256
disk cache, 256
disk controllers, 27
% disk counters, 329
disk drives, 26–27
disk I/O
bottlenecks, 274–275, 324
counters, 324–330
network I/O v., 251
performance tuning, 252–254
disk latency information. See 
latency
Disk Read Bytes/Sec, 327–328
Disk Read Queue Length, 328
Disk Reads/Sec, 327
Disk Usage system data collection 
set, 388, 389
disk virtualization, 263
Disk Write Bytes/Sec, 327–328
Disk Write Queue Length, 328
Disk Writes/Sec, 327
Distributed Management Task 
Force speciﬁ cations, 142
distributed partitioned views 
(DPVs), 279–280
Distributed Replay tool, 320, 352, 
397
Distribution agent, 475, 492, 493, 
498, 500
Distribution Cleanup, 476
distributor role, 474
DMFs (dynamic management 
functions)
categories, 376–377
database-scoped, 376
deﬁ ned, 9–10, 320–321, 397
monitoring, 376–386
new, 10
server-scoped, 376
sys.dm_exec_sql_text, 
97–98, 409, 467
types, 376
DMOs (Dynamic Management 
Objects), 9–10, 97, 98
DMVs (dynamic management 
views)
Availability Groups, 883–884
categories, 376–377
database-scoped, 376
deﬁ ned, 9–10, 239, 320–321, 
397
monitoring, 376–386
new, 10
queries
blocking information, 
380–381
buffer pool memory usage, 
385–386
connected users, 384
currently running, 377
index usage stats, 381–382
indexes not used, 382–383
locking information, 380
memory usage, 385
resource usage, 378–379
wait stats, 380
waiting for memory 
grants, 383
who is waiting, 379
recompilation issues, 409–410
server-scoped, 376
SQL CLR, 179
sys.dm_exec_
connections, 97
types, 376
XEvent, 365–366
DNS (Domain Name System)
clustering infrastructure, 524, 
526–527
FQDN, 839, 843
Listener DNS Name, 867
log shipping, 633
management tool, 633, 685
documentation, infrastructure, 558
Domain Name System (DNS)
clustering infrastructure, 524, 
526–527
FQDN, 839, 843
Listener DNS Name, 867
log shipping, 633
management tool, 633, 685
domain password policies, 182
domain policies, 182–183
Domain service account, 29
DOS/command shell commands, 
225
downtime, acceptable, 551–552
downtime approach, log shipping 
upgrade, 638–639
DPVs (distributed partitioned 
views), 279–280
DRAC, 241
drivers, disk adapters, 267
dropping/rebuilding large indexes, 
454
DSS (decision support 
environment), 237, 260, 274
DSV (data source view), 731
DTA (Database Tuning Advisor), 
280, 455, 458, 468–469
DTExec, 720–721
DTExecUI, 721
DTUtil package management 
utility, 707–708, 709, 712
dump_exception_routine_
executed, 359
dynamic management functions. 
See DMFs
Dynamic Management Objects 
(DMOs), 9–10, 97, 98. See 
also DMFs; DMVs
dynamic management views. See 
DMVs
dynamic volumes, 269
E
-E switch, 219, 220
ecosystem, SQL Server 2012, 1–2
editions, SQL Server, 17–18, 658
e-mail. See also Database Mail; 
Service Broker
notiﬁ cations, 117
pager, 118, 131
settings, Reporting Services, 
774
Employee table example, 450–451
dimensions – Employee table example

895
EMPTY, 151
encryption
keys, Reporting Services, 776
SQL Azure, 854
ENCRYPTION parameter, 158
endpoints
CREATE ENDPOINT, 67, 163, 
646
database mirroring, 646–652
permissions, 185
Service Broker, 163, 646
SOAP, 67, 852–853
Enterprise Edition, SQL Server, 
17–18
Enterprise Policy Management 
Framework (EPMF), 211–212
environments
using, 716–719
shared, SQL Azure, 845, 853, 
854
EOIO (exactly once in order), 156
EPMF (Enterprise Policy 
Management Framework), 
211–212
error logs
deﬁ ned, 90–91
monitoring, 386–387
error messages, SQL Server event 
alerts, 120–122
error_reported, 359
errors/warnings category, events, 
337
escape macros, 140–142
ESCAPE_DQUOTE, 141
ESCAPE_NONE, 141
ESCAPE_RBRACKET, 141
ESCAPE_SQUOTE, 141
estimated execution plan, 420–424
ETL (Extract Transform and Load), 
3, 147, 557, 582, 696, 743
evaluation modes, 206
events. See also Extended Events
alerts v., 119–120
Analysis Services, monitoring, 
749–750
channels, 358–360
default trace, 337–338
deﬁ ned, 338, 357
keywords, 358–360
traces, 339
XEvent, 357–360
event alerts, 119–125
SQL Server, 120–122
SQL Server performance 
condition, 122–124
WMI, 124
event forwarding, 142
event listener setup, database 
mirroring, 678, 684, 688–692
event logs
Integration Services service, 
704–705
monitoring, 387
event monitoring, 335–375
default trace feature, 320, 335, 
337–338, 355
reasons for using, 338–339
system_health session, 
320, 335–336, 338, 355
tools, 335
event notiﬁ cations, 352–355. See 
also Service Broker
creating, 353–355
deﬁ ned, 335, 352–353, 397
SQL Trace v., 353
event sessions, 363–365
Event Viewer, Windows, 386, 526, 
540, 622, 626, 692, 705
EVENTDATA(), 213–215
exact numeric data types, 13
exactly once in order (EOIO), 156
Excel Services
data refresh, 820–826
PerformancePoint, 827
PowerPivot, 816
security, 823–826
unattended service account, 
825–826
exception-driven monitoring model, 
395
EXECUTE AS, 153, 186, 216, 218
Execute Package Tool, 719, 722–
723
Execute permission, 190
executing packages, 714–723
DTExec, 720–721
DTExecUI, 721
Execute Package Tool, 719, 
722–723
Import and Export Wizard, 
720
scheduling, 723–725
scheduling, SQL Server Agent, 
723–725
SSDT, 720
T-SQL, 725
Execution Account, Reporting 
Services, 775, 818
execution logs, report, 784–786
execution plan (query processing)
actual, 424–427
estimated, 420–424
Execution Properties page, Report 
Server, 780–781
execution unit, 245
Expired Subscription Cleanup, 476
exploration, query optimization, 
416
export wizard. See Import and 
Export Wizard
export/import packages, 708–709
expressions, MOLAP, 731
Extended Events (XEvents). See 
also events
actions, 356, 360
catalog views, 365
deadlocks, 336
deﬁ ned, 239, 320, 335, 355, 397
DMVs, 365–366
events, 357–360
modules, 356
New Session UI, 366, 372–375
New Session Wizard, 366–372
object hierarchy, 356
predicates, 360–362
sessions, 366–375
SQL Proﬁ ler v., 355
system_health session, 
320, 335–336, 338, 355
targets, 356, 362–363
extended stored procedures. See 
also SQL CLR
Failed Virtual 
Allocate Bytes, 172
–g switch, 75–76
log shipping, 615
removal, 168
sp_addextendedproc, 59
sp_dropextendedproc, 59
SQL CLR v., 168, 169
sqlcmd v., 219
External Activation service, 
163–164
external fragmentation, 270–271
EXTERNAL_ACCESS, 176
Extract Transform and Load (ETL), 
3, 147, 557, 582, 696, 743
EMPTY – Extract Transform and Load (ETL)


897
preparation, clustering, 
524–527
SQL Server 2012 installation, 
22–27
vendors’ expertise, 255
Harinath, S., 730
harmony, of resources, 239–241, 
274
hash join, 441–442
HBAs (host bus adapters), 25, 
265–268
Health Insurance Portability and 
Accountability Act (HIPAA), 
197
heaps, partitioned, 446
heartbeat network, private, 517, 
518, 524, 526
help, product support, 101–104
Hide in Tile View, 799, 802
hierarchy
Visual Studio, 198
XEvents objects, 356
Hierarchyid, 16
high-availability systems. See 
also Availability Groups; 
clustering; database mirroring; 
log shipping
clustering v., 511
compression settings, 296
recovery plan, 584
SQL Azure, 838
high-performance operating mode, 
643–645, 657
high-safety operating mode
with automatic failover, 643–
645, 655–657
without automatic failover, 
643–645, 655
deﬁ ned, 644
synchronous mirroring mode, 
644
HIPAA (Health Insurance 
Portability and Accountability 
Act), 197
History page, SQL Server Agent 
conﬁ guration, 133
History Properties page, Report 
Server, 751
history tables
backup, 575–576
restore, 598–599
HOLAP (hybrid OLAP), 42, 
752–754
host bus adapters (HBAs), 25, 265, 
266, 267, 268
host-based licensing, 20
hot-added CPUs, 296–297
hot-added memory, 303, 305
hybrid DBAs, 233
hybrid OLAP (HOLAP), 42, 752, 
753, 754
hyper-threading, 243–244
Hyper-V, 19, 244, 510
I
-i switch, 219, 220
IDE (Integrated Drive Electronics), 
252–253
identical capacity servers, log 
shipping, 613
Idera, 225, 274
iLO, 241
Impersonate permission, 186
implementation transformation, 
query optimization, 416–417
IMPLICIT_TRANSACTIONS, 402
Import and Export Wizard, 564–
567, 706, 719–720
import/export packages, 708–709
independent software vendors, SQL 
Server upgrade, 54–55
index access methods, 427–438
clustered index scan
ordered, 431–432
unordered, 429–431
clustered index seek with 
ordered partial scan, 
436–438
covering non-clustered index 
scan
ordered, 433
unordered, 432–433
non-clustered index seek with 
ordered partial scan and 
lookups, 434–436
table scans, 427–429
index alignment, 286
Index Allocation Map, 428
index creation memory option, 
307–308
index scans, 457
index seeks
clustered index seek with 
ordered partial scan, 
436–438
deﬁ ned, 457–458
non-clustered index seek with 
ordered partial scan and 
lookups, 434–436
Index Tuning Wizard, 455. See also 
Database Tuning Advisor
index usage stats query, 381–382
indexed views, 454
indexes, 449–471
asynchronous statistics 
updates, 454
cleaning up, 462–463
column-based, 450–451, 457, 
471
columnstore, 449, 450–452, 
471
covering, 456
creating, 285–289, 458–459
DTA, 468–469
15K partitions, 453
ﬁ ll factor, 4, 438, 461
ﬁ ltered, 453, 456–457
fragmentation, monitoring, 
462
full-text, 454, 578
index-related features, 449–455
large, dropping/rebuilding, 454
lock granularity changes, 454
maintenance, 461–463
new features, 449–452
non-clustered, 456
nonkey columns, 454, 456
online index operations, 47, 
276, 450, 452, 453, 463
parallel index operations, 
453–454
partitioned tables/indexes, 
455–461
compression, 453
query processing 
enhancements, 444–
446
reasons for using, 459–
460
table creation, 460–461
partitioning, 279–289
query performance, 464–468
row-based, 456–457
too many, 469–471
understanding, 455–458
XML columns, 254
indexes not used query, 382–383
infrastructure documentation, 558
Harinath – infrastructure documentation

898
Infrastructure layer, SQL Azure 
architecture, 839
initialization ﬁ le, 220–221
InMemoryWithDirectQuery Mode, 
43
in-place upgrading, 55–58
Insert permission, 190
INST token, 139
Installation Center, 30, 37, 43, 534
installation process (SQL Server 
2012), 21–52. See also 
upgrade process
Analysis Services, 40–43
attended installs, 36–40
automation, 36
backup, 51
burn in/stress test, 45
collations, 27–29
failed, 52
hardware options, 22–27
planning stage, 22
post-install conﬁ guration, 
45–51
PowerPivot for SharePoint, 
43–45, 816–817
Setup Wizard, 21, 31–32, 
36–37, 40, 43, 52
stress test/burn in, 45
types, 30
unattended installs, 30–36
uninstalling SQL Server, 
51–52
InstalledBy, db_version 
table, 231
/INSTANCENAME, 81
instances (SQL Server instances). 
See also AdventureWorks 
sample database
clustering, 517, 534–540
database mirroring example, 
645–646
security, 181–195
Service Broker messages 
between instances, 162–163
instant ﬁ le initialization, 47, 595
int, 13
Integrated Drive Electronics (IDE), 
252–253
integrated mode, SharePoint, 767, 
768, 818–819
integration. See also SharePoint/
SQL Server integration
features, SQL Server 2012, 2
log shipping/ clustering, 635
log shipping/ data mirroring, 
634
log shipping/ replication, 
635–636
SharePoint/SQL Server 2012, 
815–836
SQL CLR, 167–180
Integration Services (SSIS), 695–
728. See also packages
BI stack, 233, 695, 815, 818, 854
components, 697–699
data ﬂ ow engine, 699
object model, 698
overview, 696–699
runtime engine, 698
security, 725–728
Transfer Jobs Task, 629–630
Transfer Logins, 628–
629, 635, 682
usage scenarios, 696–697
Integration Services Deployment 
Wizard, 714–716
Integration Services service
administration, 699–706
central SSIS server, 701–702
conﬁ guration, 700–704
counters, 705–706
event logs, 704–705
overview, 700–704
performance monitoring, 
705–706
purpose, 698
setting properties, 702–704
Windows Firewall, 704
XML conﬁ guration ﬁ le, 
700–701
Intel processors, 23, 242, 244, 248, 
554. See also hyper-threading
interactive monitoring model, 395
interleave NUMA, 304
internal resource pool, 309
internal workload group, 310
I/O. See also disk I/O; network I/O
afﬁ nity I/O mask feature, 
301–302
balance of resources, 239–241, 
274
critical target, 235
database ﬁ le location, 275–276
database I/O information, 
418–419
MPIO, 265–267
performance tuning, 239–241, 
251–254, 271
process model, 275
tempdb considerations, 
276–279
I/O characteristics, 258–259
I/O latency (ms), 24, 328
I/O requests per second (IOps), 24
IOMeter, 24, 45, 257
iSCSI, 25, 262, 265, 268, 510, 515, 
525, 526, 528
isolation, storage systems, 261–262
isql, 219
Itaninum, 54, 57, 242, 297
J
job steps, 111–115, 139–141
Job System page, SQL Server Agent 
conﬁ guration, 132
job tokens, 139–142
JOBID token, 139
jobs, 111–116
history, 113–115, 133
notiﬁ cations, 115–116
schedules, 116–117
joins, 440–442
K
-k switch, 76
Kerberos, 769, 822, 828
keywords, events, 358–360
Kimball, Ralph, 730
Kumaravel, A., 223
L
-l switch, 74, 76
L1, L2, L3 cache, 242–243
latency
I/O latency (ms), 24, 328
types, 253–254
LDF data ﬁ les, 275
least privilege principle, 29, 39, 136
licensing
central Integration Services 
server, 701–702
clustering, 519
multicore systems, 245–246
SQL Server 2012, 18–20, 244
Infrastructure layer, SQL Azure architecture –licensing

899
LIFETIME, 155, 158
lightweight pooling option, 87, 171, 
300
lightweight thread, 300
lightweight trace, 320
LIKE, 204
load balancing
clustering, 511
SQL Azure, 844–845
LOB columns, online index 
operations, 452
local disasters. See disaster recovery 
plan
Local Server service account, 29
Local System service account, 29
locality, data, 306–307
location
database ﬁ les, 275–276
disk adapters, 267
tempdb, 48, 276–277
lock granularity changes, indexes, 
454
Lock Pages in Memory, 305–306
locking condition, Activity 
Monitor, 93–95
locking information query, 380
Log Bytes Flushed/Sec, 671
Log Bytes Received/Sec, 672
Log Bytes Sent/Sec, 671
Log File Viewer, 90, 386
Log File(s) Size(KB), 278
Log File(s) Used (KB), 278
Log Reader agent, 475, 477, 484, 
497, 498, 500
Log Send Queue KB, 671
log sequence number (LSN), 506, 
561, 619, 660
log shipping, 607–640
architecture, 611–612
Availability Groups v., 640, 
687–688
challenges, 639
changing roles, 627–633
clustering
comparison, 513, 608–
609, 614
integration, 635
database backup plan, 633–
634
database mirroring
comparison, 686–688
integration, 634
deployment
initial conﬁ guration, 
614–616
Management Studio, 
616–623
scenarios, 608–610
T-SQL commands, 624
identical capacity servers, 613
monitor server, 612
monitoring, 624–626
Management Studio, 
625–626
stored procedures, 626
performance, 637–638
primary server, 611
processes, 612–613
removing, 636–637
report database solution, 610
restore considerations, 584
role switching, 630–632
secondary server, SQL Server 
Agent, 611–612, 615, 622, 
626
synchronizing dependencies, 
627–630
system requirements, 613–614
troubleshooting, 624–625, 
626–627
upgrading, 638–639
warm standby server, 608–610
Log Shipping Monitor Settings 
page, 622–623
Logging Properties page, Report 
Server, 781–782
logical disk counters, 325–327
logical scan fragmentation, 
438–439
logical trees, 401, 410–412, 
414, 415
logical unit numbers (LUNs), 264, 
276
logins. See also users
permissions, 185–187
SQL Azure, 848–849
users v., 185
Logman, 333–334, 750
LogReader: Delivered Cmds/Sec, 
Tx/Sec, 505
logs
error, 90–91
failed installs, 52
job steps, 113–115
monitoring, 386–387
quorum v., 515–516
report execution, 784–786
transaction logs
backups, 561–562
deﬁ ned, 5
WAL protocol, 5, 561, 580
write-ahead, 5, 561
log-transfer process, database 
mirroring, 645, 657, 693
Loopback Adapter Address, 80
lost
mirror server, 668–670
principal server, 667–668
witness server, 669–670
LSN (log sequence number), 506, 
561, 619, 660
LUNs (logical unit numbers), 264, 
276
M
-m switch, 76
Macanic, Adam, 96
MACH token, 139
magazine publishing process 
analogy, 473–474
maintenance
index, 461–463
replication maintenance jobs, 
476
Maintenance Plans, 106–110
Designer, 108–110
Wizard, 106–108
maintenance time window, 583–
584
MajorVersion, db_version 
table, 230
Manage web pages (Report 
Manager), 803–813
Cache Refresh Options, 812
Data Sources page, 804
Processing Options, 811–812
Properties page, 803
Report History, 812–813
Security section, 813
Snapshot Options, 813
Subscriptions page, 805–811
managed connections, Excel 
Services, 824
Management Data Warehouse 
(MDW)
deﬁ ned, 239, 388, 397
UCP v., 99, 321, 387
UMDW, 387–388, 397
LIFETIME – Management Data Warehouse (MDW)

900
management processor, 241
Management Studio (SSMS)
Activity Monitor, 91–95
Copy Database Wizard, 
567–570
database management plans, 
589–591
Dedicated Administrator 
Connection, 80
deﬁ ned, 239
error logs, 90–91
ﬁ ltering objects, 90
Import/Export Wizard, 
564–567
Log File Viewer, 90, 386
log shipping
deployment, 616–623
monitoring, 625–626
removing, 636
New Session UI, 372–375
Object Explorer Details Pane, 
84–85
package management, 706–709
projects/solutions, 198–200
Replication Monitor, 496, 
502–504
Report Server properties, 
778–784
reports
database, 84
server-level, 82–83
Resource Governor, 313–314
restore database, 599–601
scripting wizard, 564
Server Properties screen, 
85–89
solutions/projects, 198–200
sqlcmd execution, 222–223
manual failover
Availability Groups, 860
database mirroring, 664–665
Map Wizard, 788
MAPI, 133
mapped drives, 593, 701
MARS (Multiple Active Record 
sets), 5, 276, 454
master database
backing up, 577
deﬁ ned, 7
mirroring, 643
master key
database, 158, 162, 563
-k switch, 76
master servers (MSX), 143–145
master.dbo.sp_add_log_
shipping_alert_job, 624
master.dbo.sp_add_log_
shipping_primary_
database, 624
master.dbo.sp_add_log_
shipping_primary_
secondary, 624
master.dbo.sp_add_log_
shipping_secondary_
database, 624
master.dbo.sp_add_log_
shipping_secondary_
primary, 624
master.dbo.sp_help_log_
shipping_monitor, 625
Matrix wizard. See Table or Matrix 
Wizard
Max Degree of Parallelism 
(MAXDOP), 88–89, 300, 323, 
453–454
max server memory, 307
Max Worker Threads, 297, 300
maximum server memory setting, 
46–47
maximum theoretical performance 
beneﬁ t, hyper-threading, 
243–244
MAX_QUEUE_READERS, 153
MBps. See throughput
MDAC (Microsoft Data Access 
Components), 5
MDF data ﬁ les, 275
MDW. See Management Data 
Warehouse
MDX (Multidimensional 
Expression), 111, 731–732, 
749, 751–752, 761
memory
Analysis Services, 733
Available Mbytes, 331
AWE, 302, 306, 332
balance of resources, 239–241, 
274
bottlenecks, 333
cache
Buffer Cache Hit Ratio, 
302, 332
buffer cache hit ratios, 451
cache coherency, 247, 296, 
297, 304
CPU performance, 242–
243, 297
DBCC FREEPROCCACHE, 
407, 410
disk, 256
L1, L2, L3, 242–243
proactive caching, 742, 
754–755
read/write, 256
report cache, 811–812
sys.dm_exec_
cached_plans, 179, 
404, 407, 409
sys.
syscacheobjects, 
409
counters, 302–303, 331–333
critical target, 235
data locality, 306–307
far, 306–307
hot-added, 303, 305
index creation memory option, 
307–308
max server memory, 307
Minimum Memory per Query 
option, 308
monitoring, 330–333
near, 306–307
NUMA, 246, 247–248, 
296–297
CC-NUMA, 296, 304
data locality, 306–307
interleave, 304
soft, 297, 304
page ﬁ le, 249–250
performance tuning, 46–47, 
239–241, 248–251, 302–308
physical address space, 248
RAM, 248
side-by-side installs, 30
64-bit platform, 69, 242, 
305–306
SMP systems, 247
SQL Server 2012 
requirements, 23, 24
VMM, 249–251
Memory Manager - Counter: 
Target Server Memory (KB), 
332
Memory Manager - Counter: Total 
Server Memory (KB), 332
memory nodes, 304–305
management processor – memory nodes

901
Memory page, Server Properties 
screen, 86–87
Memory: Pages/sec, 303
memory usage query, 385
Merge agent, 475, 477, 505
merge join, 442
merge replication, 477, 497–498
Mersenne Prime numbers, 45
Message Queue (MSMQ), 148–149, 
476
message queuing technologies, 148–
149. See also Service Broker
messages, Service Broker
receiving, 160–161
sending, 157–160, 161–163
types, 151
metrics, storage systems, 24
Microsoft Ascend, 65
Microsoft Data Access Components 
(MDAC), 5
Microsoft Distributed Transaction 
Coordinator. See MSDTC
Microsoft Product Support Services 
(PSS), 6, 76, 87, 101, 102, 396
Microsoft Research, 54
Microsoft Technical Support, 543, 
545–546
Microsoft Touchdown, 65
minimum downtime approach, log 
shipping upgrade, 638
Minimum Memory per Query 
option, 308
minimum server memory setting, 
46–47
MinorVersion, db_version 
table, 230
mirroring. See database mirroring
Mirror Committed Overhead, 675
mirror database, 642
mirror server
database availability scenarios, 
668–669, 670
deﬁ ned, 642
lost, 668–670
overview, 646
prepare for failover, 681–685
principal server/mirror server 
synchronization, 653
MIRROR_ADDRESS, 155
mirroring thresholds, counters, 
676–678
mirroring_connection_
timeout, 659, 660
mirroring_failover_lsn, 
659, 660
mirroring_redo_queue, 659, 
660
mirroring_role_sequence, 
659
mirroring_safety_
sequence, 659
Miscellaneous folder, 198
missing features, SQL Azure, 854
model database
backing up, 578
deﬁ ned, 8, 48
mirroring, 643
modules, XEvents, 356
MOLAP (Multidimensional 
OLAP). See also Tabular 
model
aggregations, 753, 755–758
calculations, 731
components, 731–732
deﬁ ned, 42, 730
dimensional model, 731
DSV, 731
expressions, 731
partition conﬁ guration, 
753–755
processing, 741–744
roles
database role permissions, 
760–762
database roles, 758–759
server role, 758
security, 758–762
storage modes, 752–753
Tabular model v., 730, 732
money, 13
monitor server, log shipping, 612
monitoring, 317–397
Analysis Services events, 
749–750
Availability Groups, 883–884
AWE memory, 332
baseline performance
current metrics v., 319
establishing, 318–319
clusters, 542–543
data compression, 295
DDL triggers monitor database 
changes (example), 215–217
disk I/O, 324–330
DMVs/DMFs, 376–386
exception-driven monitoring 
model, 395
goal, 318
index fragmentation, 462
interactive monitoring model, 
395
log shipping, 624–626
logs, 386–387
memory usage, 330–333
performance tuning, 236–237
reasons, 318
replication, 502–506
Resource Governor, 314
T-SQL, 96–98
value, 317–318
monitoring tools. See also DMFs; 
DMVs; Performance Monitor; 
SQL Proﬁ ler
Activity Monitor, 91–95, 320
SCA, 321, 396–397
SCOM, 278, 395–397
SQL Server Best Practices 
Analyzer, 50, 321, 396–397
System Center Management 
Pack, 321, 395–396
types, 319–321, 397
UCP, 99, 321, 387
move option, Report Manager task 
menu, 802
MPIO (multipath I/O), 265–267
ms. See I/O latency
msdb database
backing up, 577
deﬁ ned, 8
mirroring, 643
msdb.dbo.dbm_monitor_
data, 673
msdb.dbo.sp_add_schedule, 
624
msdb.dbo.sp_attach_
schedule, 624
msdb.dbo.sp_update_job, 
624
MSDTC (Microsoft Distributed 
Transaction Coordinator)
clustering, 532–533
transactional replication, 497
MSMQ (Message Queue), 148–149, 
476
MSSA token, 139
MSX (master servers), 143–145
multicolumn statistics, 439
Memory page, Server Properties screen – multicolumn statistics


903
Tabular model v., 732
ROLAP, 42, 752, 753, 754
self-service reporting, 730
Oldest Unsent Transaction, 674
OLTP. See online transaction 
processing
On Change, 206, 210
On Change: Prevent, 206, 211
On Demand, 206, 210
On Schedule, 206, 210
OneOff, db_version table, 231
ONLINE = ON option, 453
online analytical processing. See 
OLAP
online index operations, 47, 276, 
450, 452–453, 463
online mode, 737, 759
online transaction processing 
(OLTP)
DSS v., 237, 274
fragmentation, 438
OPEN MASTER KEY 
DECRYPTION BY 
PASSWORD, 563
Operating Mode, 675
operating modes, database 
mirroring, 643–645
high-performance, 643–645, 
657
high-safety operating mode
with automatic failover, 
643–645, 655–657
without automatic 
failover, 643–645, 
655
SQL Server 2012 editions, 
658
operators, 117–119
Failsafe Operator, 118–119, 
131
notiﬁ cations, 117–118
optimization, query compilation, 
400, 412–417. See also 
performance tuning
Oracle, 193, 473, 800
Oracle Publishing replication, 476, 
478
ordered covering non-clustered 
index scan, 433
OSCMD token, 140
osql, 219
Overwrite Media section, Back Up 
Database dialog, 588
P
-P switch, 219–220
package deployment model
deﬁ ned, 698
package management, 
Management Studio, 
706–709
package security, 726–728
Package Installer Wizard, 175, 
709–711
packages (Integration Services), 
706–725. See also Integration 
Services
automated deployment, 
709–712
deployment, 709–712
DTUtil, 707–709, 712
environments, 716–719
executing/running, 714–723
DTExec, 720–721
DTExecUI, 721
Execute Package Tool, 
719, 722–723
Import and Export 
Wizard, 720
scheduling, SQL Server 
Agent, 723–725
SSDT, 720
T-SQL, 725
export, 708–709
import, 708–709
package deployment model
deﬁ ned, 698
package management, 
Management Studio, 
706–709
package security, 726–728
project deployment model
deﬁ ned, 698
package conﬁ guration, 
716–719
package deployment, 
714–716
protection levels, 726–727
security, 726–728
packages (XEvents), 356–357
page compression, 291–293
Page compression attempts/sec, 295
page faults, 249–251
page ﬁ le, 249–250
Page Life Expectancy (PLE), 302, 
333
pager notiﬁ cations, 117–118, 131
Pages compressed/sec, 295
paging activity, 330
parallel ATA, 252–253
parallel index operations, 
453–454
parallel query execution strategy, 
partitioned tables/indexes, 
445–446
parallel redo, 645, 658
parallelism
cost threshold, 89, 417
degreeOfParallelism, 
425
hyper-threading, 243
Max Degree of Parallelism, 
88–89, 300, 323, 453–454
parameter snifﬁ ng, 424
parameters, command-line, 31
Pareto Principle, 273
parse trees, 401, 410–412, 414–415
parsing, query compilation, 
410–412
partial backup, 559–560
partial database restore, 596
partial differential backup, 560
partially contained databases, 
78–79
partition conﬁ guration, MOLAP, 
753–755
partition elimination, 446, 459
partition function, 281–284
partition scheme, 284–285
partition-aware SEEK operation, 
445
partitioned heaps, 446
partitioned tables/indexes, 455–
461. See also indexes
archiving data, 604–605
compression, 453
query processing 
enhancements, 444–446
reasons for using, 459–460
table creation, 460–461
partitioned views
archiving data, 604, 605
DPVs, 279–280
partitioning, 279–289
Analysis Services, 741–742
deﬁ ned, 268, 279–280
ﬁ legroups, 284
high-level process, 281
impact, 281
Oldest Unsent Transaction – partitioning

904
partitioning (continued)
indexes creation, 285–289
reasons for using, 280–281
sliding-window scenario, 282, 
285, 460
start sector alignment, 
268–269
table creation, 285–289, 
292–293
telephone company trouble-
ticketing system, 282–283
partner-to-partner quorum, 656
partners, database mirroring, 642
passive nodes, 510, 515
passwords
domain password policies, 182
SA, 39, 49, 81
service accounts, 29
SQL Server authentication, 
182
Windows authentication, 
183–184
patch-management solutions, 550
PBM. See Policy-Based 
Management
peer-to-peer replication, 477, 483, 
498–502
performance. See also performance 
tuning
backup/restore, 594
features, SQL Server 2012, 1
log-shipping, 637–638
query performance
indexes, 464–468
post-upgrade checks, 
69–70
requirements, 234
restore/backup, 594
secondary replicas, 878–879
SQL CLR, 180
traces, 352
performance condition alerts, SQL 
Server, 122–124
performance counters. See counters
Performance Monitor (Perfmon, 
Windows System Monitor). 
See also counters
data provider, 387
database mirroring, 670–672
deﬁ ned, 319–322, 397
log data, traces and, 349–351
Logman, 333–334, 750
Relog, 334, 750
replication, 505
SQL CLR, 177–178
performance tuning. See also query 
processing
Analysis Services, 749–752
T-SQL, 399–447
performance tuning SQL Server, 
233–315
afﬁ nity mask, 297–300
baseline, 235–236
challenges, 271
CPUs, 239–248, 296–302
balance of resources, 
239–241, 274
cache, 242–243, 297
critical target, 235
hyper-threading, 243–244
multicore processor 
systems, 244–246
NUMA, 246–248, 
296–297
resource counters, 322–
324
SMP, 246–247
system architecture, 
246–248
x64 platform, 242
critical targets, 235
data compression, 290–296
backup compression, 570–
571, 607, 637–638
cost savings, 293–294
dictionary, 291
monitoring, 295
page compression, 291–
293
preﬁ x, 291
row compression, 290–
291
space savings, 293–295
VertiPaq engine, 42–43, 
450–451, 457, 816
when to use, 295–296
data usage patterns, 238, 271
database schema, 237–238
DBAs, 234–237
developer, 237–238
production, 238–239
fragmentation, 269–271
goal, 273
hardware
considerations, 238–239, 
271
management, 241
tools, 239
harmony of resources, 239–
241, 274
I/O, 239–241, 251–254, 271
afﬁ nity I/O mask feature, 
301–302
balance of resources, 
239–241, 274
critical target, 235
database ﬁ le location, 
275–276
process model, 275
tempdb considerations, 
276–279
memory, 46–47, 239–241, 
248–251, 302–308
monitoring, 236–237
performance tuning cycle, 
234–235
prerequisites, 274
server conﬁ guration, 264–269
SQL statement types, 237, 271
storage systems, 255–264
tempdb database, 47–48
user interaction, 237, 271
web-based architecture, 
236–237
workload deﬁ ning, 274
PerformancePoint Services, 
826–827
permission chains, 190–193
permissions (privileges, rights)
Availability Groups, 188
backups, 576–577
database, 189–190
endpoints, 185
functions, 190
logins, 185–187
restoring databases, 
576–577
SQL CLR, 176
SQL Server Agent subsystem, 
127–129
stored procedures, 190
tables, 189–190
too much privileges, 549
user deﬁ ned server roles, 188
views, 189–190
Permissions page, Server Properties 
screen, 89
PERMISSION_SET, 174
physical address space, 248
partitioning – physical address space

905
physical disk counters, 325–327
physical memory (RAM), 248. See 
also memory
placement
database ﬁ les, 275–276
disk adapters, 267
tempdb, 48, 276–277
plan optimality, recompilations, 
403–408
planning stage
SQL Server installation, 22
SQL Server upgrade, 55
Platform layer, SQL Azure 
architecture, 839
PLE (Page Life Expectancy), 302, 
333
Point of Service (POS) application, 
480–481
POISON_MESSAGE_HANDLING, 
153
Policy-Based Management (PBM), 
200–212
compliance, 197, 201, 207, 
212, 231
implementation, 210–212
overview, 201–202
scripting, 209–210
stored procedure naming 
conventions example, 
202–208
ports
Dedicated Administrator 
Connection, 80–81
TCP/IP, 49, 732
POS (Point of Service) application, 
480–481
post-install conﬁ guration, 45–51
post-upgrade checks, 69–70
PowerPivot for Excel, 827
PowerPivot for SharePoint
conﬁ guration, 817
data refresh, 829–836
install, 43–45, 816–817
PowerShell
features, 223–225
Professional Windows 
PowerShell Programming: 
Snap-Ins, Cmdlets, Hosts 
and Providers (Kumaravel, 
et al.), 223 
PowerView, 819–820
scripted installation, SQL Server, 
33–36
pred_compare objects, 361
predicates, XEvent object, 360–362
pred_source objects, 361–362
preﬁ x compression, 291
presentation layer, PowerView, 
819–820
pre-upgrade steps/tools, 55, 58–67
PreventChangeTrigger, 
217–218
primary replicas, 858–859
primary roles, 859
primary server, log shipping, 611. 
See also log shipping
Prime95, 45
principal database, 642
principal server
deﬁ ned, 642
lost, 667–668
overview, 646
principal server/mirror server 
synchronization, 653
priorities, Service Broker, 156
Priority Boost option, 299–300
Private Bytes, 331–332
private heartbeat network, 517–518, 
524, 526
private/public networks, clusters, 
516–517
privileged mode, 299
% Privileged Time, 299, 301, 
323–324
% Privileged Time - Instance 
sqlserver, 323
privileges. See permissions
proactive caching, 742, 754–755
problematic page faults, 250–251
PROCEDURE_NAME, 153
Process Cube dialog, 736–737
processadmin ﬁ xed server role, 
189
processing
MOLAP model, 741–744
Tabular model, 744–745
Processing Options, Manage web 
pages, 811–812
% Processor Time, 323
% Processor Time - Instance 
sqlserver, 323
Processor Queue Length, 323
processors. See CPUs
Processors page, Server Properties 
screen, 87
product support help, 101–104
Product Support Services 
(Microsoft PSS), 6, 76, 87, 
101, 102, 396
production DBAs
performance tuning, 238–239
role, 2, 233–234
SQL Server 2012 features, 2
Professional Microsoft SQL Server 
Analysis Services 2012 with 
MDX (Harinath, et al.), 730
Professional SQL Server 2005 
Performance Tuning (Wrox), 
548
Professional SQL Server 2008 
Internals and Troubleshooting 
(Wrox), 236
Professional SQL Server 2012 
Reporting Services (Turley, et 
al.), 795
Professional Windows PowerShell 
Programming: Snap-Ins, 
Cmdlets, Hosts and Providers 
(Kumaravel, et al.), 223
project deployment model. See also 
packages
deﬁ ned, 698
packages
conﬁ guration, 716–719
deployment, 714–716
project queries, 200
projects/solutions, 198–200
Properties page, Manage web 
pages, 803
protection levels, package, 
726–727
proxies, SQL Server Agent, 
127–129
PSS (Microsoft Product Support 
Services), 6, 76, 87, 101–102, 
396
public role, 189, 850
publication, replication 
data, 475
public/private networks, clusters, 
516–517
Publish Database dialog, 175
publisher role, 474
publishing magazine analogy, 
473–474
Pull subscription, 475, 492
pure transaction-log backup, 
561–562
Push subscription, 475, 492
physical disk counters – Push subscription
 
 
 
 


907
switching, 574–575
preparations, 581–585
requirements, 581–582
restore v., 580
success criteria, 557
Recovery Point Objective (RPO), 
552
Recovery State section, Restore 
Database dialog, 601
Recovery Time Objective (RTO), 
551, 557
Red-Gate, 225
redo
parallel redo, 645, 658
phase, restore, 580
redo queue, 645, 658, 660, 
662, 671, 675
Redo Bytes/Sec, 671
Redo Queue KB, 672
Redundant Array of Independent 
Drives. See RAID
refactoring, 229, 464
References permission, 190
rehearsal, disaster recovery plan, 
558–559
reinitialize failed subscriptions, 476
RELATED_CONVERSATION_
GROUP, 157, 158
relational OLAP (ROLAP), 42, 
752, 753, 754
reliability
LUNs, 264
RAID, 26
SANs, 25
SQL CLR, 180
SQL Server 2012, 53
SSDs, 26–27
storage systems, 24, 26
Reliability and Performance 
Monitor. See Performance 
Monitor
Reliability section, Back Up 
Database dialog, 588–589
Relog, 334, 750
re-mirroring, 643
removing log shipping, 636–637
replay
Distributed Replay tool, 320, 
352, 397
traces, 351–352, 750–751
replicas, 858–859
primary, 858–859
secondary
backup, 879–881
client connectivity, 
876–877
deﬁ ned, 858–859
performance, 878–879
purpose, 875
read-only access, 860–
862, 876
replication, 473–507
clustering v., 513
components, 474
counters, 505
data, 475
enhancements, 478
log shipping/replication 
integration, 635–636
magazine publishing process 
analogy, 473–474
maintenance jobs, 476
merge, 477, 497–498
models, 478–483
monitoring, 502–506
multimaster, 498
Multiple Publisher model, 
480–481
Multiple Publishers also 
Subscribing model, 481–482
Oracle Publishing, 476, 478
overview, 473–478
peer-to-peer, 477, 483, 
498–502
Performance Monitor, 505
recovery considerations, 
584–585
roles, 474–475
scripting, 502
Single Publisher model, 
478–479
snapshot, 476–477, 484–497
sp_replcounters, 506
SQL Azure, 853, 854
transactional, 477, 497–498, 
686, 687
types, 476
update anywhere, 498
Updating Subscriber model, 
482–483
Replication Agent Monitor, 476
replication agents, 475–476
Distribution, 475, 492, 493, 
498, 500
Log Reader, 475, 477, 484, 
497, 498, 500
Merge, 475, 477, 505
Queue Reader, 476, 484, 498
Snapshot, 475, 484, 489–490, 
494, 496, 505
Replication Management Objects 
(RMO), 478
Replication Monitor, 496, 
502–504
Report Builder, 786–795
blank report, 788
Chart Wizard, 788
datasets, 787–788
deﬁ ned, 786, 814
download/install, 787
Map Wizard, 788
Professional SQL Server 2012 
Reporting Services, 795
RDL ﬁ les, 786, 802, 819
Report Manager task menu, 801
SQL Server Data Tools v., 786
Table or Matrix Wizard, 
788–795
report cache, 811–812
report database solution, log 
shipping, 610
Report Deﬁ nition Language (RDL), 
786, 802, 819
report execution log, 784–786
Report History, Manage web pages, 
812–813
Report Manager, 795–813
data sources, 799–800
deﬁ ned, 765, 773, 795, 814
Manage web pages, 803–813
Cache Refresh Options, 
812
Data Sources page, 804
Processing Options, 
811–812
Properties page, 803
Report History, 812–813
Security section, 813
Snapshot Options, 813
Subscriptions page, 
805–811
managing, 796–802
home screen, 796, 799
navigation menu, 796
Site Settings menu option, 
796–798
task menu, 799–802
Report Manager URL, 773–774, 
778, 795–796
Recovery Point Objective (RPO) – Report Manager URL

908
Report Server
properties (server properties 
dialog), 778–784
Advanced Properties page, 
783–784
Execution Properties page, 
780–781
General Properties page, 
779–780
History Properties page, 
751
Logging Properties page, 
781–782
Security Properties page, 
782–783
service account, 768–769
virtual account, 769
Report Viewer, 61–62, 66, 769
reporting, self-service, 730
Reporting Services (SSRS), 765–
814. See also Report Builder; 
Report Manager
BI stack, 233, 695, 815, 818, 854
Conﬁ guration Manager
databases, 771–773
deﬁ ned, 765, 814
e-mail settings, 774
encryption keys, 776
Execution Account, 775, 
818
launching, 766
opening screen, 766–768
reasons for use, 765–766, 
814
Report Manager URL, 
773–774
Scale-out Deployment, 
777–778
server properties dialog, 
778–784
service accounts, 768–769
SQL Server Conﬁ guration 
Manager v., 765
web service URL, 769–
771, 774, 787, 795
data-driven subscriptions, 
806–811
native mode, 767, 818
SharePoint integrated mode, 
767–768, 818–819
subscriptions, 805–811
uninstalling, 51
upgrading, 68–69
reports
cache refresh plan, 812
database, 84
managing, 803–813
server-level, 82–83
standard, 321, 392–395
system data collection sets, 
388–389
ReportServer, 771–773
ReportServerTempDB, 771–773
reprocessing, 741–743
resource database, 6–7, 409
Resource Governor, 309–315
Resource Pool Stats, 314
resource pools, 309–310
resource usage query, 378–379
resources, balance/harmony, 239–
241, 274
restore, 579–581. See also backup 
and recovery
Analysis Services databases, 
745–748
performance, 594
permissions, 576–577
recovery v., 580
system databases, 602–604
T-SQL restore command, 602
Restore Database dialog, 596–601
RESTORE FILELISTONLY, 576
RESTORE HEADERONLY, 576
RESTORE LABELONLY, 576
RESTORE VERIFYONLY, 578–579
resume availability database, 
Availability Group example, 
874
retrieving
trace data from ﬁ le, 345
trace metadata, 344
reusable conversations, 158–160
Revision, db_version table, 
230
REVOKE, 184
rights. See permissions
risk mitigation, SQL Server 
upgrade, 54
RMO (Replication Management 
Objects), 478
Rocket Division, 510
ROLAP (relational OLAP), 42, 
752–754
role changing
database mirroring, 661–666
log shipping, 627–633
role switching, 630–632
roles
Availability Groups, 858–859
MOLAP model
database role permissions, 
760–762
database roles, 758–759
server role, 758
package deployment model, 
727
project deployment model, 
728
replication, 474–475
Tabular model, 732, 
762–763
Worker Roles, SQL Azure, 
847, 853
ROLLBACK, 215
rotational latency, 253
routes, 154–155
row compression, 290–291
row level security, 193–194
row-based indexes, 456–457
Rows Read, 705
rowversion, 16
RPO (Recovery Point Objective), 
552, 557
RSA, 241
RTO (Recovery Time Objective), 
551, 557
running packages, 714–723
DTExec, 720–721
DTExecUI, 721
Execute Package Tool, 719, 
722–723
Import and Export Wizard, 
720
scheduling, SQL Server Agent, 
723–725
SSDT, 720
T-SQL, 725
runtime engine, Integration 
Services, 698
runtime host, 169
S
-S switch, 219
-s switch, 76
SA (SysAdmin) account, 49
SAFE, 176
SAFETY FULL, 644, 655, 661, 
667, 669, 682–683
Report Server – SAFETY FULL

909
with witness, 667
without witness, 667–668
SAFETY OFF, 644–645, 657, 666, 
668, 671, 680, 683
SalesOrderDetails table, 
294–295
sample databases, 40
sample time determination, 322
SANs (storage area networks)
beneﬁ ts, 262
deﬁ ned, 25, 262
/SAPW, 81
Sarbanes-Oxley Act (SOX), 197
SAS (serial attached SCSI), 253
SCA (System Center Advisor), 321, 
396–397
scalability. See also partitioning
application optimization, 273
CMS, 211
DAS, 25
memory architecture, 302
MOLAP model, 730
multicore systems, 245
partitioning, 280, 289
SANs, 25
SQL Azure Federation, 853
SQL CLR, 168, 180
SQL Server Enterprise Edition, 
18
tempdb, 277
Scalability Experts, 65
Scale-out Deployment, Reporting 
Services, 777–778
scatter-gather, I/O process model, 
275
SCC (System Conﬁ guration 
Checker), 56
schedulers, 297–298, 303–304
schedules, 116–117
scheduling package execution, SQL 
Server Agent, 723–725
SCHEMA_ID(), 11
schemas
dbo, 8
deﬁ ned, 8
performance tuning SQL 
Server, 237–238
query recompilations, 401–
402
SCOM (System Center Operations 
Manager), 278, 395–396, 397
ScriptDom, T-SQL, 60
scripting. See also PowerShell
ASSL, 733, 736–737
change management, 218–225
PBM, 209–210
replication, 502
Upgrade Advisor, 62–63
Scripting Host, 225
scripting wizard, 564
SCSI (small computer systems 
interface), 253. See also 
iSCSI
Secondary Database Settings 
dialog, 618–620, 631, 638, 
639
secondary replicas
backup, 879–881
client connectivity, 876–877
deﬁ ned, 858–859
performance, 878–879
purpose, 875
read-only access, 860–862, 
876
secondary roles, 859
secondary server, log shipping, 
611–612, 615, 622, 626
sector alignment, 268–269
Secure Sockets Layer (SSL), 135–
137, 770, 774
Secure Store, 823
Securing SQL Server (Cherry), 183
security. See also permissions
Analysis Services
MOLAP model, 758–762
Tabular model, 762–763
Database Mail, 134–135
Distribution Agent security 
screen, 493, 500
Excel Services, 823–826
Integration Services, 725–728
row level, 193–194
settings, SQL Server 2012, 
49–50
Snapshot Agent Security 
screen, 489–490
SQL CLR, 176, 180
SQL Server Agent, 125–129
SQL Server instance, 181–195
security audit category, events, 338
security contexts, job steps, 112
security identiﬁ er (SID), 183, 682
Security page, Server Properties 
screen, 88
Security Properties page, Report 
Server, 782–783
Security section, Manage web 
pages, 813
securityadmin ﬁ xed server role, 
189, 848
seek time, 253
Select permission, 190–191
self-service
reporting, 730
SQL Server 2012 features, 1–2
SEND, 157
send queue, 645, 671, 674–675
sending alerts, database mirroring, 
676–678
sending messages, Service Broker, 
157–160
between databases, 161
between instances, 162–163
@sent_before, 139
@sent_status, 139
serial ATA, 253
serial attached SCSI (SAS), 253
Server Activity system data 
collection set, 388
Server Catalog, Windows, 241
server category, events, 338
server conﬁ guration, performance 
tuning, 264–269
Server Dashboard report, 83
server groups, 98–99, 211, 533
Server Objects node, SQL Azure, 
852–853
server properties dialog. See Report 
Server
Server Properties screen, 85–89
server provisioning, SQL Azure, 
840–844
server roles
ﬁ xed, 188–189
MOLAP model, 758
user deﬁ ned, 188
ServerA, ServerB, ServerC. See 
database mirroring
serveradmin ﬁ xed server role, 
171, 189, 848
server-level DDL triggers, 
217–218
server-level reports, 82–83
server-scoped DMVs/DMFs, 376
server-side traces
client-side traces v., 340
creating, 340–344
server-wide permissions, 185–189
services, Service Broker, 153–154
SAFETY OFF– services, Service Broker 

910
service accounts
Execution Account v., 775, 
818
Report Server, 768–769
SQL Server, 29, 39
SQL Server Agent, 125
unattended, Excel Services, 
825–826
virtual, 769
service application architecture, 
820
Service Broker, 147–165
asynchronous messaging, 
147–149
conﬁ guration, 149–157
contracts, 151–152, 154
conversation groups, 156–157
enabling, 149–151
endpoints, 163, 646
event notiﬁ cations, 352–355
External Activation service, 
163–164
messages
receiving, 160–161
sending, 157–160, 
161–163
types, 151
MSMQ v., 148–149
overview, 148
priorities, 156
queues, 152–153
routes, 154–155
services, 153–154
using, 157–164
service pack, clusters, 540
service-level agreements. See SLAs
Services Console, 111
Services layer, SQL Azure 
architecture, 838–839
Session UI. See New Session UI
Session Wizard. See New Session 
Wizard
sessions, Extended Events, 366–375
SET options, recompilations, 
402–403
SET SHOWPLAN_ALL, 420–422
SET SHOWPLAN_TEXT, 420–422
SET SHOWPLAN_XML, 423–424
SET STATISTICS PROFILE 
ON/OFF, 425–427
SET STATISTICS XML ON/
OFF, 425
Setup Wizard, 21, 31–32, 36–37, 
40, 43, 52
setupadmin ﬁ xed server role, 189
:SETVAR, 223
sharding, 853
shared dimensions, 743
shared disk array, 515. See also 
RAID
shared environment platform, SQL 
Azure, 845, 853–854
SharePoint integrated mode, 767, 
768, 818–819
SharePoint/SQL Server integration, 
815–836
components, 815–820
data refresh, 820–836
Excel Services, 820–826
PerformancePoint 
Services, 826–827
PowerPivot, 829–836
Visio Services, 827–829
PowerPivot for SharePoint
conﬁ guration, 817
data refresh, 829–836
install, 43–45, 816–817
PowerView, 819–820
service application 
architecture, 820
Showplan
formats, 420
graphical actual, 427
graphical estimated, 424
SET SHOWPLAN_ALL, 
420–422
SET SHOWPLAN_TEXT, 
420–422
SET SHOWPLAN_XML, 
423–424
SET STATISTICS 
PROFILE ON/OFF, 
425–427
SET STATISTICS XML 
ON/OFF, 425
shrink operations, database, 48, 
558
SID (security identiﬁ er), 183, 682
side-by-side installs, 30
side-by-side upgrading, 57–58
Silverlight, 817, 819
simple autoparam, 400–401
Simple Mail Transfer Protocol. See 
SMTP
simple recovery model, 573–574
simple storage system design, 255
simpliﬁ cation transformation, 
query optimization, 414–416
single disk
multiple partition, 326
single partition, 325
Single Publisher model, 478–480
single-column statistics, 439
64-bit platform, 69, 242, 305–306
SLAs (service-level agreements), 
239, 259, 282, 449, 520, 557, 
582–583, 613, 632, 637
sliding-window scenario, 282, 285, 
460
Slowly Changing Dimension 
Wizard, 697
small computer systems interface 
(SCSI), 253. See also iSCSI
smalldateTime, 15
smallint, 13
smallmoney, 13
SMP (Symmetric Multi-Processing), 
246–247, 296
SMTP (Simple Mail Transfer 
Protocol), 133–137
Snap-In, Windows Services, 
699–700, 702–704
snapshots
database snapshots
mirroring, 692–693
restore, 597–598
Snapshot agent, 475, 484, 489, 
490, 494, 496, 505
Snapshot Options, Manage 
web pages, 813
snapshot replication, 476–477, 
484–497
SOAP
endpoints, 67, 852–853
XML/A, 111, 732–733, 742, 
749, 758
socket, 245–246
soft page faults, 250
soft-NUMA, 297, 304
software
failure, 550
independent software 
vendors, SQL Server 
upgrade, 54–55
log shipping, 614
solid state drives (SSDs), 26–27, 
242, 262, 330, 561, 604
solutions/projects, 198–200
service accounts – solutions/projects

911
sort order, collations, 28–29
Source area
Back Up Database dialog, 
586–587
Restore Database dialog, 600
Source Safe, 197–200, 229, 231, 
502
sources, data ﬂ ow engine, 699
SOX (Sarbanes-Oxley Act), 197
sp_ preﬁ x, 202–203
space savings, compression, 
293–295
sp_addextendedproc, 59
sp_addmessage, 122
SPADE, 36
sp_altermessage, 122
sp_configure
enable contained databases, 
78–79
SQL Server conﬁ guration 
options, 50
startup stored procedures, 77
using, 89
sp_dbmmonitoradd
monitoring, 673
sp_dbmmonitorresults, 673
sp_dropextendedproc, 59
speciﬁ c account, Report Server, 769
sp_estimate_data_
compression_savings, 
293–295
sp_help_log_shipping_
monitor, 625–626
SPID column, 339–340
sp_procoption, 78
sp_replcounters, 506
sp_send_dbmail, 133–134
sp_SimpleNamingProc, 207
sp_start_job, 111, 117
sp_trace_create, 340, 342–
343
sp_trace_setevent, 340, 
342–343
sp_trace_setfilter, 340, 343
sp_trace_setstatus, 340, 343
sp_updatejob, 126, 624
sp_update_schedule, 126
sp_updatestats, 70
sp_who, 96
sp_who2, 96
sp_whoisactive, 96
SQL Agent. See SQL Server Agent
SQL Azure, 837–855
access rights, 850
administering, 848–850
architecture, 838–839
Azure Management Portal, 
839–840
Azure platform, 837, 
839–840, 845
backups, 852
BI stack, 854
Business database edition, 
843–844
Client layer, 838
conﬁ guring, 839–848
connecting to, 847–848
Data Sync Services, 853
database provisioning, 840–
844
disaster recovery, 838
encryption, 854
ﬁ rewalls, 843, 845–847
full-text search, 854
high availability, 838
Infrastructure layer, 839
introduction, 837–839
load balancing, 844–845
logins, 848–849
missing features, 854
Object Explorer, 852–853
Platform layer, 839
replication, 853–854
Server Objects node, 852–853
server provisioning, 840–844
Services layer, 838–839
shared environment platform, 
845, 853–854
SQL Server Agent, 853–854
throttling, 844–845
T-SQL, 837
users, 848, 849–850
Web database edition, 843–
844
Worker Roles, 847, 853
working with, 850–853
SQL CLR (SQL Common Language 
Runtime, SQLCLR), 167–180
assemblies, 172–176
beneﬁ ts, 168–169
custom data types, 16
design goals, 180
DMVs, 179
enabling, 171–172
extended stored procedures v., 
168–169
introduction, 167–169
Performance Monitor, 177–
178
permission sets, 176
security, 176, 180
SQL Azure, 854
SQL Proﬁ ler, 178–179
SSDT
assemblies, 174–176
support, 169
T-SQL v., 168–171
SQL development team, 201, 224, 
414, 699
SQL Mail, 59, 67, 70, 133, 682. See 
also Database Mail
SQL Native Client, 5–6, 52, 133, 
633, 684–685, 875
SQL Nexus Tool, 103
SQL Proﬁ ler (SQL Server Proﬁ ler), 
345–352
deﬁ ned, 320, 335
deprecation, 320, 345, 352, 
397
Distributed Replay v., 320, 
352, 397
monitor database mirroring, 
678
recompilation scenarios, 408
SQL CLR, 178–179
XEvents v., 355
SQL Server
authentication, 182–184
Best Practices Analyzer, 50, 
321, 396–397
collations, 28
event alerts, 120–122
performance condition alerts, 
122–124
Utility, 99, 321
SQL Server 2012
architecture, 1–20
automating, 105–145
editions, 17–18, 658
installation process, 21–52
Analysis Services, 40–43
attended installs, 36–40
automation, 36
backup, 51
burn in/stress test, 45
collations, 27–29
failed, 52
hardware options, 22–27
planning stage, 22
sort order, collations – SQL Server 2012

912
SQL Server 2012 (continued)
post-install conﬁ guration, 
45–51
PowerPivot for SharePoint 
option, 43–45
Setup Wizard, 21, 31–32, 
36–37, 40, 43, 52
stress test/burn in, 45
types, 30
unattended installs, 30–36
uninstalling SQL Server, 
51–52
licensing, 18–20, 244
.NET runtime host, 169
new features, 1–4
optimization, 273–315
security settings, 49–50
SharePoint/SQL Server 2012 
integration, 815–836
uninstalling, 51–52
upgrade process, 53–70
backward compatibility, 
67–68
beneﬁ ts, 53–54
CTP, 54–55
execution process, 55
full-text catalogs, 68
independent software 
vendors, 54–55
in-place, 55–58
overview, 30
plan, 55
post-upgrade checks, 
69–70
pre-upgrade steps/tools, 
55, 58–67
reasons for upgrading, 
53–54
Reporting Services, 68–69
risk mitigation, 54
side-by-side, 57–58
upgrading, 53–70
SQL Server Agent, 110–133. See 
also log shipping
access, 125–126
alerts, 119–125
conﬁ guration, 129–133
jobs, 111–116
log shipping, secondary server, 
611–612, 615, 622, 626
operators, 117–119
proxies, 127–129
schedules, 116–117
scheduling package execution, 
723–725
security, 125–129
service account, 125
SQL Azure, 853, 854
subsystems, 127–129
Transfer Jobs Task, 629–630
SQL Server Browser, 38, 732, 735
SQL Server Data Tools (SSDT)
BIDS v., 69, 628, 699, 742, 
756, 758, 770, 786
change management, 229
Integration Services, 699
new feature, 3
Report Builder v., 786
running packages, 720
SQL CLR
assemblies, 174–176
support, 169
web service URL, 770–771
SQL Server instances. See instances
SQL Server Management Object 
(SQL-SMO), 201, 224
SQL Server operating system 
(SQLOS), 303
SQL Server Proﬁ ler. See SQL 
Proﬁ ler
SQL statement types, performance 
tuning SQL Server, 237, 271
SQL Trace, 338–345. See also 
traces
architecture, 339–340, 397
data provider, 387
deﬁ ned, 320, 335
event notiﬁ cations v., 353
query plans for analysis, 
446–447
terminology, 338–339
SQLAgentOperatorRole, 126
SQLAgentReaderRole, 126
SQLAgentUserRole, 126
SQLCLR. See SQL CLR
sqlcmd, 219–223
command prompt, 219–221
Dedicated Administrator 
Connection, 80
Management Studio, 222–223
switches, 219–220
SqlContext, 172–173
SQLDiag.exe, 102–104
SQLDIR token, 140
SQLDumper.exe, 101–102
SQLIO, 24, 256
SQLIOSim, 24, 45, 548
SQLIOStress, 45, 548
SQLOS (SQL Server operating 
system), 303
SqlPipe, 172–173
SQL-SMO (SQL Server 
Management Object), 201, 
224
sql_statement_starting, 
360
/SQLSYSADMINACCOUNTS, 81
SQL_Variant, 16
SSAS. See Analysis Services
SSDs (solid state drives), 26–27, 
242, 262, 330, 561, 604
SSDT. See SQL Server Data Tools
SSIS. See Integration Services
SSL (Secure Sockets Layer), 135, 
136, 137, 770, 774
SSMS. See Management Studio
SSRS. See Reporting Services
Standard Edition, SQL Server, 
17–18
standard reports, 321, 392–395
STANDBY, 612
start sector alignment, 268–269
startup parameters, 73–76
startup stored procedures, 77–78
StarWind, 510
state change events, database 
mirroring, 678, 684, 688–692
statement-level recompilation, 401
statistics
columns, 439
ﬁ ltered indexes, 453
STATUS, 152–153
STEPCT token, 140
STEPID token, 140
storage alignment, 286–289
storage area networks. See SANs
storage modes, 752–753. See also 
MOLAP
HOLAP, 42, 752–754
ROLAP, 42, 752–754
storage systems. See also data 
compression; RAID; SANs
choosing, 25–26
DAS, 25, 27
designing, 257–262
disk drives, 26–27
isolation, 261–262
log shipping, 614
metrics, 24
SQL Server 2012 – storage systems

913
performance tuning 
considerations, 255–257
SAN, 262–264
SQL Server 2012 
requirements, 23–27
SSDs, 26–27, 242, 262, 330, 
561, 604
testing, 256–257
stored procedures
activated, 148, 153, 160, 163
extended stored procedures
Failed Virtual 
Allocate Bytes, 
172
–g switch, 75–76
log shipping, 615
removal, 168
sp_addextendedproc, 
59
sp_
dropextendedproc, 
59
SQL CLR v., 168–169
sqlcmd v., 219
log shipping monitoring, 626
naming conventions example, 
202–208
permissions, 190
startup, 77–78
system stored procedures
deﬁ ned, 321
naming conventions 
example, 202–208
T-SQL, server-side traces, 
340–344
usp_ preﬁ x, 202, 208
stream compression, 645
stress test/burn in, 45
STRTDT token, 139–140
STRTTM token, 140
subscriber role, 474–475
subscriptions
data-driven, 806–811
deﬁ ned, 475
push/pull, 475, 492
reinitialize failed 
subscriptions, 476
Reporting Services, 805–811
Subscriptions page, Manage web 
pages, 805–811
subsystems, SQL Server Agent, 
127–129
support help, 101–104
suser_sname(), 194
suspend availability database, 
Availability Group example, 
873
SVR token, 140
switches. See also speciﬁ c switches
rebuilding system databases, 
81
sqlcmd, 219–220
startup parameters, 73–76
switching roles, primary/secondary 
servers, 630–632
Symmetric Multi-Processing (SMP), 
246–247, 296
Synchronize Database Wizard, 737, 
738
synchronizing
Analysis Services databases, 
748–749
dependencies, log shipping, 
627–630
synchronous mirroring mode. See 
high-safety operating mode
synchronous-commit mode, 859
synonyms, 8–9
SysAdmin (SA) account, 49
sysadmin ﬁ xed server role, 189
sys.availability_group_
listener_ip_addresses, 
884
sys.availability_group_
listeners, 884
sys.availability_groups, 
883
sys.availability_
replicas, 884
sys.conversation_
endpoints, 159
sys.database_mirroring, 
658–660
sys.database_mirroring_
endpoints, 660–661
sys.database_mirroring_
witnesses, 660
sys.dm_clr_appdomains, 179
sys.dm_clr_loaded_
assemblies, 179
sys.dm_clr_properties, 179
sys.dm_clr_tasks, 179
sys.dm_db_index_
operational_stats, 295, 
470
sys.dm_db_index_physical_
stats, 258, 295, 462, 470
sys.dm_db_index_usage_
stats, 470
sys.dm_db_missing_index_
columns, 469
sys.dm_db_missing_index_
details, 466, 467, 469
sys.dm_db_missing_index_
groups, 470
sys.dm_db_missing_index_
group_stats, 470
sys.dm_db_task_space_
usage, 279
sys.dm_db_uncontained_
entities, 79
sys.dm_exec_cached_plans, 
179, 404, 407, 409
sys.dm_exec_connections, 
97
sys.dm_exec_query_memory_
grants, 383
sys.dm_exec_query_stats, 
97, 98, 179, 410, 466
sys.dm_exec_sql_text, 
97–98, 409, 467
sys.dm_hadr_availability_
group_states, 883
sys.dm_hadr_availability_
replica_states, 884
sys.dm_hadr_database_
replica_cluster_
states, 884
sys.dm_hadr_database_
replica_states, 884
sys.dm_io_virtual_file_
stats, 9, 418
sys.dm_repl_articles, 505
sys.dm_repl_schemas, 506
sys.dm_repl_tranhash, 506
sys.dm_repl_traninfo, 506
sys.dm_resource_governor_
configuration, 314
sys.dm_resource_governor_
resource_pools, 314
sys.dm_resource_governor_
workload_groups, 314
sys.dm_tcp_listener_
states, 884
sys.dm_xe_map_values, 365
sys.dm_xe_object_columns, 
365
stored procedures – sys.dm_xe_object_columns

914
sys.dm_xe_objects, 357, 360, 
361, 362, 365
sys.dm_xe_packages, 356–
357, 365
sys.dm_xe_session_event_
actions, 365
sys.dm_xe_session_events, 
365
sys.dm_xe_sessions, 365, 366
sys.dm_xe_session_
targets, 366
sysmail_allitems, 138
sysmail_delete_mailitems, 
138
sysmail_mailattachments, 
138
sysprocesses, 378
sys.server_triggers, 218
sys.sp_check_log_
shipping_monitor_
alert, 625
sys.syscacheobjects, 409
sys.tcp_endpoints, 661
system architecture, CPUs, 246–
248
System Center Advisor (SCA), 321, 
396–397
System Center Management Pack, 
321, 395–396
System Center Operations Manager 
(SCOM), 278, 395–397
System Conﬁ guration Checker 
(SCC), 56
system data collection sets, 388
Disk Usage, 388–389
Query Activity, 387–388
reports, 388–389
Server Activity, 388
viewing, 388–389
system data types, 15–16
system databases
backing up, 577–578
rebuilding, 81–82
restoring, 602–604
standard, 6–8
system DMVs, recompilation issues, 
409–410
system master key, 76
System Monitor. See Performance 
Monitor
System Performance Monitor. See 
Performance Monitor
system stored procedures
deﬁ ned, 321
naming conventions example, 
202–208
system_health session, 320, 
335–336, 338, 355
sys.triggers, 218
T
-T switch, 75–76
Table, 16
Table or Matrix Wizard, Report 
Builder, 788–795
table scans, 427–429, 457
tables. See also partitioned tables/
indexes
creating, 285–289
DimCustomer, 463
FactInternetSales, 
464–468
history tables
backup, 575–576
restore, 598–599
partitioning, 279–289, 
292–293
permissions, 189–190
SalesOrderDetails, 
294–295
sys.syscacheobjects, 
409
Tabular model, 732
tabular data, PerformancePoint, 
826–827
Tabular model. See also MOLAP
components, 732
MOLAP v., 730, 732
processing, 744–745
query mode options, 42–43
roles, 732, 762–763
security, 762–763
tables, 732
tail transaction-log backup, 562
Take Ownership permission, 185, 
188, 190
Tape Drive section, Back Up 
Database dialog, 589
Target Server Memory (KB), 332
target servers (TSX), 143–145
targets
performance tuning, 235
XEvent object, 356, 362–363
Task Manager, 101, 104, 239
task menu, Report Manager
Connect Using option, 800–
801
data source type, 799–800
Delete option, 802
Folder Settings task, 801–802
move option, 802
Report Builder option, 801
TCP endpoints. See endpoints
TCP/IP
database mirroring, 647
ports, 49, 732
XML/A messages, 733
Team Server Source Control, 502
Technical Support, Microsoft, 543, 
545–546
telephone company trouble-
ticketing system, 282–283
tempdb
backing up, 577–578
capacity planning, 277–279, 
446
deﬁ ned, 7
I/O performance, 276–279
isolation, 262
mirroring, 643
performance tuning, 47–48
placement/location, 48, 
276–277
pre-upgrade steps, 59
uses, 47
white paper, 417
templates, 339
TestCacheReuse, 405–408
testing
clusters, 540–542
QA/test systems, 550
storage systems, 256–257
text, 12, 345
theoretical performance beneﬁ t, 
hyper-threading, 243–244
third-party clustering solutions, 514
threads
deﬁ ned, 245–246
ﬁ bers v., 300
Max Worker Threads, 297, 
300
worker, 297–298
throttling, SQL Azure, 844–845
throughput
I/Os, 324
MBps, 24, 324, 327–328
Tile Mode, 799
time, 15
sys.dm_xe_objects – time

915
time data types, 14–15
Time to Restore Log, 675
Time to Send and Restore All 
Current Log, 675
Time to Send Log, 674
TIME token, 140
timestamp, 16
tinyint, 13
tokens, 139–142
too many indexes, 469–471
too much privilege, 549
total latency, 254
Total Server Memory (KB), 332
Touchdown program, 65
trace ﬂ ags, 99–101, 336
trace_print, 359
tracer tokens, 503
traces. See also SQL Proﬁ ler; SQL 
Trace
correlating, 349–351
default trace feature, 320, 335, 
337–338, 355
Distributed Replay tool, 320, 
352, 397
event notiﬁ cations v., 335
events, 339
performance considerations, 
352
replaying, 351–352, 750–751
retrieving
trace data from ﬁ le, 345
trace metadata, 344
terminology, 338–339
track-to-track latency, 253
Transaction Delay, 671
transaction logs. See also log 
shipping
backups, 561–562
deﬁ ned, 5
log shipping process, 612–613
Transaction Log Backup 
Settings, 609, 616–618
Transaction Log section, Back 
Up Database dialog, 589
transaction-log restore, 
595–596
Transaction Processing Council 
numbers, 244
transactional replication, 477, 
497–498, 686, 687
Transaction/Sec, 671
Transact-SQL. See T-SQL
transfer counters, 329
Transfer Jobs Task, 629–630
Transfer Logins, 628–629, 
635, 682
transfer time, 254
transformations, data ﬂ ow engine, 
699
troubleshooting
Availability Groups, 883–884
clusters, 543–546
database mirroring, 678–681
failed install, 52
log shipping, 624–627
tools
Dedicated Administrator 
Connection, 79–81
rebuilding system 
databases, 81–82
web-based architecture big 
picture, 236–237
trouble-ticketing system, telephone 
company, 282–283
trusted data connection libraries, 
824–825
trusted ﬁ le locations, Excel 
Services, 823–824
Trustworthy Computing, 54
T-SQL (Transact-SQL). See also 
query processing
backup commands, 591–593
CMS feature, 98–99
data provider, 387
deployment commands, 624
FactInternetSales, 
464–467
index creation, 458–459
job steps, 112, 139–141
log shipping removal, 
636–637
monitoring processes, 96–98
package execution, 725
performance tuning, 399–447
restore command, 602
ScriptDom, 60
Service Broker conﬁ guration, 
149–157
SQL Azure, 837
SQL CLR v., 168–171
stored procedures, server-side 
traces, 340–344
user errors, 548–549
TSQL_Replay, 336, 351
TSX (target servers), 143–145
Turley, P., 795
two-node clusters, 515–516, 518
type derivation, algebrizer, 411
TYPE_ID(), 11
TYPE_NAME(), 11
U
-U switch, 219–220
UAFS (Upgrade Assistant), 65–67
UCP (Utility Control Point), 99, 
321, 387
UDM Mode (Multidimensional and 
Data Mining mode), 41–42. 
See also MOLAP
HOLAP, 42, 752–754
ROLAP, 42, 752–754
UMDW (Utility Management Data 
Warehouse), 387–388, 397
unattended installs, 30–36
unattended service account, Excel 
Services, 825–826
UNC name, 114, 341–343, 592, 
593, 612, 614, 701, 810
undo phase, restore, 580–581
uninstalling SQL Server, 51–52
Uniqueidentifier, 16
UNIX-based systems, 219, 225
unordered clustered index scan, 
429–431
unordered covering non-clustered 
index scan, 432–433
Unrestored Log, 675
UNSAFE, 176
Unsent Log, 674
unsupported/discontinued features, 
67. See also deprecation
update anywhere replication, 498
Update permission, 190
Updating Subscriber model, 
482–483
Upgrade Advisor, 55, 58–64, 521
Analysis Wizard, 61
Report Viewer, 61–62, 66
Upgrade Assistant (UAFS), 65–67
upgrade process (SQL Server 2012), 
53–70
backward compatibility, 
67–68
beneﬁ ts, 53–54
CTP, 54–55
Data-tier Application, 
227–229
time data types – upgrade process (SQL Server 2012)


917
database availability scenarios, 
669–670
deﬁ ned, 643
lost, 669–670
overview, 646
SAFETY FULL, 667–668
witness-to-partner quorum, 656
wizards
Aggregation Design Wizard, 
756
Chart Wizard, 788
Copy Database Wizard, 57, 
567–570
Create Cluster Wizard, 
530–531
Create Server Wizard, 840–
843
Database Mail Conﬁ guration 
Wizard, 134–136
Deployment Wizard, 714, 
737–741
Import and Export Wizard, 
564–567, 706, 719–720
Index Tuning Wizard, 455
Integration Services 
Deployment Wizard, 
714–716
Maintenance Plan Wizard, 
106–108
Map Wizard, 788
New Session Wizard, 366–372
Package Installer Wizard, 175, 
709–711
scripting wizard, 564
Setup Wizard, 21, 31–32, 
36–37, 40, 43, 52
Slowly Changing Dimension 
Wizard, 697
Synchronize Database Wizard, 
737, 738
Table or Matrix Wizard, 
788–795
Upgrade Advisor Analysis 
Wizard, 61
Usage Based Optimization 
Wizard, 734, 756–757
Validate a Cluster 
Conﬁ guration Wizard, 
528–529
WMI (Windows Management 
Instrumentation)
event alerts, 124
using, 142–143
WMI Query Language (WQL), 
124, 142–143
WMIproperty token, 140
Worker Roles, SQL Azure, 847, 853
workers threads, 297–298
Working Set, 331
workload, performance tuning SQL 
Server, 274
Workload Group Stats, 314
workload groups, 310–311
WQL (WMI Query Language), 
124, 142–143
write ahead logging (WAL) 
protocol, 5, 561, 580
write-ahead logs, 5, 561
write/read cache, 256
Wrox
Professional SQL Server 2005 
Performance Tuning, 548
Professional SQL Server 
2008 Internals and 
Troubleshooting, 236
WSUS (Windows Server Update 
Services), 60, 550
X–Y–Z
x64 platform, 242, 305–306
XE_alter_long_running_
queries.sql, 364–365
XE_long_running_queries.
sql, 364
XEvents. See Extended Events
XML
columns, indexes, 454
conditions, 209
conﬁ guration ﬁ le, Integration 
Services service, 700–701
XML data type, 16, 209
XML for Analysis (XML/A), 111, 
732–733, 742, 749, 758
XMLA listener, 732
xp_cmdshell, 219
witness-to-partner quorum – XMLA listener

